Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0026041666666666665,"Dynamic graph representation learning is an important task with widespread ap-
plications. Previous methods on dynamic graph learning are usually sensitive
to noisy graph information such as missing or spurious connections, which can
yield degenerated performance and generalization. To overcome this challenge,
we propose a Transformer-based dynamic graph learning method named Dynamic
Graph Transformer (DGT) with spatial-temporal encoding to effectively learn
graph topology and capture implicit links. To improve the generalization ability,
we introduce two complementary self-supervised pre-training tasks and show that
jointly optimizing the two pre-training tasks results in a smaller Bayesian error rate
via an information-theoretic analysis. We also propose a temporal-union graph
structure and a target-context node sampling strategy for an efÔ¨Åcient and scalable
training. Extensive experiments on real-world datasets illustrate that DGT presents
superior performance compared with several state-of-the-art baselines."
INTRODUCTION,0.005208333333333333,"1
INTRODUCTION"
INTRODUCTION,0.0078125,"In recent years, graph representation learning has been recognized as a fundamental learning problem
and has received much attention due to its widespread use in various domains, including social
network analysis (Kipf & Welling, 2017; Hamilton et al., 2017), trafÔ¨Åc prediction (Cui et al., 2019;
Rahimi et al., 2018), knowledge graphs (Wang et al., 2019a;b), drug discovery (Do et al., 2019;
Duvenaud et al., 2015), and recommendation systems (Berg et al., 2017; Ying et al., 2018). Most
existing graph representation learning work focuses on static graphs. However, real-world graphs
are intrinsically dynamic where nodes and edges can appear and disappear over time. This dynamic
nature of real-world graphs motivates dynamic graph representation learning methods that can model
temporal evolutionary patterns and accurately predict node properties and future edges."
INTRODUCTION,0.010416666666666666,"Recently, several attempts (Sankar et al., 2018; Pareja et al., 2020; Goyal et al., 2018) have been
made to generalize graph learning algorithms from static graphs to dynamic graphs by Ô¨Årst learning
node representations on each static graph snapshot then aggregating these representations from the
temporal dimension. However, these methods are vulnerable to noisy information such as missing
or spurious links. This is due to the ineffective message aggregation over unrelated neighbors from
noisy connections. The temporal aggregation makes this issue severe by further carrying the noise
information over time. Over-relying on graph structures makes the model sensitive to noisy input
and can signiÔ¨Åcantly affect downstream task accuracy. A remedy is to consider the input graph
as fully connected and learn a graph topology by assigning lower weights to task-irrelevant edges
during training (Devlin et al., 2019). However, completely ignoring the graph structure makes the
optimization inefÔ¨Åcient because the model has to estimate the underlying graph structure while learn
model parameters at the same time. To resolve the above challenges, we propose a Transformer-based
dynamic graph learning method named Dynamic Graph Transformer (DGT) that can ‚Äúleverage
underlying graph structures‚Äù and ‚Äúcapture implicit edge connections‚Äù to balance this trade-off."
INTRODUCTION,0.013020833333333334,"Transformers (Vaswani et al., 2017), designed to automatically capture the inter-dependencies between
tokens in a sequence, have been successfully applied in several domains such as Natural Language
Processing (Devlin et al., 2019; Brown et al., 2020) and Computer Vision (Dosovitskiy et al., 2020; Liu
et al., 2021). We summarize the success of Transformers into three main factors, which can also help
resolve the aforementioned challenges in dynamic graph representation learning: (1) fully-connected
self-attention: by modeling all pair-wise node relations, DGT can capture implicit edge connections,"
INTRODUCTION,0.015625,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.018229166666666668,"thus become robust to graphs with noisy information such as missing links; (2) positional encoding:
by generalizing positional encoding to the graph domain using spatial-temporal encoding, we can
inject both spatial and temporal graph evolutionary information as inductive biases into DGT to
learn a graph‚Äôs evolutionary patterns over time; (3) self-supervised pre-training: by optimizing two
complementary pre-training tasks, DGT presents a better performance on the downstream tasks."
INTRODUCTION,0.020833333333333332,"Though powerful, training Transformers on large-scale graphs is non-trivial due to the quadratic
complexity of the fully connected self-attention on the graph size (Zaheer et al., 2020; Wang et al.,
2020). This issue is more severe on dynamic graphs as the computation cost grows with the number
of time-steps in a dynamic graph (Pareja et al., 2020; Sankar et al., 2018). To make the training
scalable and independent of both the graph size and the number of time-steps, we Ô¨Årst propose a
temporal-union graph structure that aggregates graph information from multiple time-steps into
a uniÔ¨Åed meta-graph; we then develop a two-tower architecture with a novel target-context node
sampling strategy to model a subset of nodes with their contextual information. These approaches
improve DGT‚Äôs training efÔ¨Åciency and scalability from both the temporal and spatial perspectives."
INTRODUCTION,0.0234375,"To this end, we summarize our contributions as follows: (1) a two-tower Transformer-based method
named DGT with spatial-temporal encoding that can capture implicit edge connections in addition to
the input graph topology; (2) a temporal-union graph data structure that efÔ¨Åciently summarizes the
spatial-temporal information of dynamic graphs and a novel target-context node sampling strategy
for large-scale training; (3) two complementary pre-training tasks that can facilitate performing
downstream tasks and are proven beneÔ¨Åcial using information theory; and (4) a comprehensive
evaluation on real-world datasets with ablation studies to validate the effectiveness of DGT."
PRELIMINARIES AND RELATED WORKS,0.026041666666666668,"2
PRELIMINARIES AND RELATED WORKS"
PRELIMINARIES AND RELATED WORKS,0.028645833333333332,"In this section, we Ô¨Årst deÔ¨Åne dynamic graphs, then review related literature on dynamic graph
representation learning and Transformers on graphs."
PRELIMINARIES AND RELATED WORKS,0.03125,"Dynamic graph deÔ¨Ånition. The nodes and edges in a dynamic graph may appear and disappear
over time. In this paper, we deÔ¨Åne a dynamic graph as a sequence of static graph snapshots with a
temporal order G ‚âú{G1, . . . , GT }, where the t-th snapshot graph Gt(V, Et) is an undirected graph
with a shared node set V of all time steps and an edge set Et. We also denote its adjacency matrix
as At. Our goal is to learn a latent representation of each node at each time-step t, such that the
learned representation can be used for any speciÔ¨Åc downstream task such as link prediction or node
classiÔ¨Åcation. Please notice that the shared node set V is not static and will be updated when new
snapshot graph arrives, which is the same as Sankar et al. (2018); Pareja et al. (2020).
Dynamic graph representation learning. Previous dynamic graph representation learning methods
usually extend static graph algorithms by further taking the temporal information into considera-
tion. They can mainly be classiÔ¨Åed into three categories: (1) smoothness-based methods learn a
graph autoencoder to generate node embeddings on each graph snapshot and ensure the temporal
smoothness of the node embeddings across consecutive time-steps. For example, DYGEM (Goyal
et al., 2018) uses the learned embeddings from the previous time-step to initialize the embeddings
in the next time-step. DYNAERNN applies RNN to smooth node embeddings at different time-
steps; (2) Recurrent-based methods capture the temporal dependency using RNN. For example,
GCRN (Seo et al., 2018) Ô¨Årst computes node embeddings on each snapshot using GCN (Defferrard
et al., 2016), then feeds the node embeddings into an RNN to learn their temporal dependency.
EVOLVEGCN (Pareja et al., 2020) uses RNN to estimate the GCN weight parameters at different
time-steps; (3) Attention-based methods use self-attention mechanism for both spatial and temporal
message aggregation. For example, DYSAT (Sankar et al., 2018) propose to use the self-attention
mechanism for temporal and spatial information aggregation. TGAT (Xu et al., 2020) encodes the
temporal information into the node feature, then applies self-attention on the temporal augmented
node features. However, smoothness-based methods heavily rely on the temporal smoothness and are
inadequate when nodes exhibit vastly different evolutionary behaviors, recurrent-based methods scale
poorly when the number of time-steps increases due to the recurrent nature of RNN, attention-based
methods only consider the self-attention on existing edges and are sensitive to noisy graphs. In
contrast, DGT leverages Transformer to capture the spatial-temporal dependency between all nodes
pairs, does not over-relying on the given graph structures, and is less sensitive to noisy edges.
Graph Transformers.
Recently, several attempts have been made to leverage Transformer for"
PRELIMINARIES AND RELATED WORKS,0.033854166666666664,Under review as a conference paper at ICLR 2022
PRELIMINARIES AND RELATED WORKS,0.036458333333333336,"ùùç!,#,$
$%& ' 1 2 3
4 5"
PRELIMINARIES AND RELATED WORKS,0.0390625,"ùùç("",$,"")=2
ùùç("",&,"")=1"
PRELIMINARIES AND RELATED WORKS,0.041666666666666664,"‚Ä¶
ùùç(&,',$)=3
ùùç((,',$)=4"
PRELIMINARIES AND RELATED WORKS,0.044270833333333336,"ùùì(),*)"
PRELIMINARIES AND RELATED WORKS,0.046875,"(3) Spatial Distance 
Encoding generation (Section 4.3)"
PRELIMINARIES AND RELATED WORKS,0.049479166666666664,(2) Temporal Connection
PRELIMINARIES AND RELATED WORKS,0.052083333333333336,Encoding generation (Section 4.3)
PRELIMINARIES AND RELATED WORKS,0.0546875,"ùùì("",$)=1
ùùì("",&)=1"
PRELIMINARIES AND RELATED WORKS,0.057291666666666664,"‚Ä¶
ùùì(&,')=2
ùùì((,')=1 1 2 3
4 1 3
4 5 ùìñùüè ùìñùüê"
PRELIMINARIES AND RELATED WORKS,0.059895833333333336,(1) Temporal union graph (Section 4.1)
PRELIMINARIES AND RELATED WORKS,0.0625,"Temporal 
Connection"
PRELIMINARIES AND RELATED WORKS,0.06510416666666667,"Spatial 
Distance
ùê¥),* +, ùê¥),*"
PRELIMINARIES AND RELATED WORKS,0.06770833333333333,"-.
ùìñ/0120"
PRELIMINARIES AND RELATED WORKS,0.0703125,"(4) Target node driven context 
node sampling (Section 4.2) 1 2 3
4 5
?"
PRELIMINARIES AND RELATED WORKS,0.07291666666666667,"Context node 
Sampling"
PRELIMINARIES AND RELATED WORKS,0.07552083333333333,"Target nodes 1 2
3
4 5"
PRELIMINARIES AND RELATED WORKS,0.078125,Target nodes
PRELIMINARIES AND RELATED WORKS,0.08072916666666667,Context node ùêá$%& (() ùêá%*% (()
PRELIMINARIES AND RELATED WORKS,0.08333333333333333,(5) Dynamic Graph Transformer (Section 4.4)
PRELIMINARIES AND RELATED WORKS,0.0859375,"Task: Predict whether edge 1,5 in ùìñùüë"
PRELIMINARIES AND RELATED WORKS,0.08854166666666667,"WeightAverage
ùêûùùç!,#,ùíï )* ùíï%ùüè ùüê ùêá$%& (‚Ñì./) ùêá%*%"
PRELIMINARIES AND RELATED WORKS,0.09114583333333333,"(‚Ñì./)
Product"
PRELIMINARIES AND RELATED WORKS,0.09375,ùêÄ-.[ùí±343; ùí±536]
PRELIMINARIES AND RELATED WORKS,0.09635416666666667,Softmax
PRELIMINARIES AND RELATED WORKS,0.09895833333333333,"ùêÄ+,[ùí±343; ùí±536] ùêá%*%"
PRELIMINARIES AND RELATED WORKS,0.1015625,"(‚Ñì)
Query Key"
PRELIMINARIES AND RELATED WORKS,0.10416666666666667,Product Value FFN ùêá%*% (‚Ñì./) ùêá$%&
PRELIMINARIES AND RELATED WORKS,0.10677083333333333,"(‚Ñì./)
Product"
PRELIMINARIES AND RELATED WORKS,0.109375,ùêÄ-.[ùí±536; ùí±343]
PRELIMINARIES AND RELATED WORKS,0.11197916666666667,Softmax
PRELIMINARIES AND RELATED WORKS,0.11458333333333333,"ùêÄ+,[ùí±536; ùí±343] ùêá$%&"
PRELIMINARIES AND RELATED WORKS,0.1171875,"(‚Ñì)
Query Key"
PRELIMINARIES AND RELATED WORKS,0.11979166666666667,Product Value FFN
PRELIMINARIES AND RELATED WORKS,0.12239583333333333,"Linear
Projection"
PRELIMINARIES AND RELATED WORKS,0.125,"Linear
Projection"
PRELIMINARIES AND RELATED WORKS,0.12760416666666666,"ùêûùùì(!,#) /0 ùëë ùë°= 1 ùê∑' () ùëë"
PRELIMINARIES AND RELATED WORKS,0.13020833333333334,"ùêûùùç!,#,ùíï )* ùíï%ùüè ùüê ùë°= 2"
PRELIMINARIES AND RELATED WORKS,0.1328125,"Figure 1: Overview of using DGT for link prediction. Given snapshot graphs {G1, G2} as input, (1)
we Ô¨Årst generate the temporal union graph with the considered max shortest path distance Dmax = 5,
and its associated (2) temporal connection encoding and (3) spatial distance encoding. Then, the
encodings are mapped into ATC
i,j, ASD
i,j for each node pairs (i, j) using a fully connected layer. To
predict whether an edge exists in future graph G3, we Ô¨Årst (4) sample target nodes and context nodes,
and then apply (5) DGT to encode target nodes and context nodes separately."
PRELIMINARIES AND RELATED WORKS,0.13541666666666666,"graph representation learning. For example, GRAPHORMER (Ying et al., 2021) and GRAPHTRANS-
FORMER (Dwivedi & Bresson, 2020) use scaled dot-product attention (Vaswani et al., 2017) for
message aggregation and generalizes the idea of positional encoding to graph domains. GRAPH-
BERT (Zhang et al., 2020) Ô¨Årst samples an egocentric network for each node, then order all nodes
into a sequence based on node importance, and feed into the Transformer. However, GRAPHORMER
is only feasible to small molecule graphs and cannot scale to large graphs due to the signiÔ¨Åcant
computation cost of full attention; GRAPHTRANSFORMER only considers the Ô¨Årst-hop neighbor
aggregation, which makes it sensitive to noisy graphs; GRAPHBERT does not leverage the graph
topology and can perform poorly when graph topology is important. In contrast, DGT encodes the
input graph structures as an inductive bias to guide the full-attention optimization, which balances
the trade-offs between noisy input robustness and efÔ¨Åciently learning an underlying graph structure.
A detailed comparison is deferred to Appendix D."
METHOD,0.13802083333333334,"3
METHOD"
METHOD,0.140625,"In this section, we Ô¨Årst introduce the temporal union-graph (in Section 3.1) and our sampling strategy
(in Section 3.2) that can reduce the overall complexity from the temporal and spatial perspectives
respectively. Then, we introduce our spatial-temporal encoding technique (in Section 3.3), describe
the two-tower transformer architecture design, and explain how to integrate the spatial-temporal
encoding to DGT (in Section 3.4). Figure 1 illustrates the overall DGT design."
TEMPORAL-UNION GRAPH GENERATION,0.14322916666666666,"3.1
TEMPORAL-UNION GRAPH GENERATION"
TEMPORAL-UNION GRAPH GENERATION,0.14583333333333334,"One major challenge of applying Transformers on graph representation learning is its signiÔ¨Åcant
computation and memory overhead. In Transformers, the computation cost of self-attention is
O(|E| d) and its memory cost is O(|E| + |V| d). When using full attention, the computation graph
is fully connected with |E| = |V|2, where the overall complexity is quadratic in the graph size.
On dynamic graphs, this problem can be even more severe if one naively extends the static graph
algorithm to a dynamic graph, e.g., Ô¨Årst extracting the spatial information of each snapshot graph
separately, then jointly reasoning the temporal information on all snapshot graphs (Sankar et al.,
2018; Pareja et al., 2020). By doing so, the overall complexity grows linearly with the number of
time-steps T, i.e., with O(|V|2Td) computation and O(|V|2T + |V|Td) memory cost. To reduce the
dependency of the overall complexity on the number of time-steps, we propose to Ô¨Årst aggregate
dynamic graphs G = {G1, . . . , GT } into a temporal-union graph Gunion(V, E‚Ä≤) then employ DGT on
the generated temporal-union graph, where E‚Ä≤ = Unique{(i, j) : (i, j) ‚ààEt, t ‚àà[T]} is the set of"
TEMPORAL-UNION GRAPH GENERATION,0.1484375,Under review as a conference paper at ICLR 2022
TEMPORAL-UNION GRAPH GENERATION,0.15104166666666666,"all possible unique edges in G. As a result, the overall complexity of DGT does not grow with the
number of time-steps. Details on how to leverage spatial-temporal encoding to recover the temporal
information of edges are described in Section 3.3."
TARGET NODE DRIVEN CONTEXT NODE SAMPLING,0.15364583333333334,"3.2
TARGET NODE DRIVEN CONTEXT NODE SAMPLING"
TARGET NODE DRIVEN CONTEXT NODE SAMPLING,0.15625,"Although the temporal-union graph can alleviate the computation burden from the temporal dimension,
due to the overall quadratic complexity of self-attention with respect to the input graph size, scaling
the training of Transformer to real-world graphs is still non-trivial. Therefore, a properly designed
sampling strategy that makes the overall complexity independent with graph sizes is necessary. Our
goal is to design a sub-graph sampling strategy that ensures a Ô¨Åxed number of well-connected nodes
and a lower computational complexity. To this end, we propose to Ô¨Årst sample a subset of nodes that
we are interested in as target nodes, then sample their common neighbors as context nodes."
TARGET NODE DRIVEN CONTEXT NODE SAMPLING,0.15885416666666666,"Let target nodes Vtgt ‚äÜV be the set of nodes that we are interested in and want to compute its node
representation. For example, for the link prediction task, Vtgt are the set of nodes that we aim to
predict whether they are connected. Then, the context nodes Vctx ‚äÜ{N(i) | ‚àÄi ‚ààVtgt} are sampled
as the common neighbors of the target nodes. Notice that since context nodes Vctx are sampled as
the common neighbors of the target nodes, they can provide local structure information for nodes in
the target node set. Besides, since two different nodes in the target node set can be far apart with a
disconnected neighborhood, the neighborhood of two nodes can provide an approximation of the
global view of the full graph. During the sampling process, to control the randomness involved
in the sampling process, Vctx are chosen as the subset of nodes with the top-K joint Personalized
PageRank (PPR) score (Andersen et al., 2006) to nodes in Vtgt, where PPR score is a node proximity
measure that captures the importance of two nodes in the graph. More speciÔ¨Åcally, our joint PPR
sampler proceeds as follows: First, we compute the approximated PPR vector œÄ(i) ‚ààRN for all
node i ‚ààVtgt, where the j-th element in œÄ(i) can be interpreted as the probability of a random
walk to start at node i and end at node j. We then compute the approximated joint PPR vector
ÀÜœÄ(Vtgt) = P"
TARGET NODE DRIVEN CONTEXT NODE SAMPLING,0.16145833333333334,"i‚ààVtgt œÄ(i) ‚ààRN. Finally, we select K context nodes where each node j ‚ààVctx has the
top-K joint PPR score in ÀÜœÄ(Vtgt). In practice, we select the context node size the same as the target
node size, i.e., K = |Vtgt|."
SPATIAL-TEMPORAL ENCODING,0.1640625,"3.3
SPATIAL-TEMPORAL ENCODING"
SPATIAL-TEMPORAL ENCODING,0.16666666666666666,"Given the temporal-union graph, our next step is to translate the spatial-temporal information from
snapshot graphs to the temporal-union graph Gunion, which can be recognized and leveraged by
Transformers. Notice that most classical GNNs either over-rely on the given graph structure by only
considering the Ô¨Årst- or higher-order neighbors for feature aggregation (Ying et al., 2021), or directly
learn graph adjacency without using the given graph structure (Devlin et al., 2019). On the one
hand, over-relying on the graph structure makes the model fails to capture the inter-relation between
nodes that are not connected in the labeled graph, and could be very sensitive to the noisy edges
due to human-labeling errors. On the other hand, completely ignoring the graph structure makes the
optimization problem challenging because the model has to iteratively learn model parameters and
estimate the graph structure. To avoid the above two extremes, we present two simple but effective
designs of encodings, i.e., temporal connection encoding and spatial distance encoding, and provide
details on how to integrate them into DGT."
SPATIAL-TEMPORAL ENCODING,0.16927083333333334,"Temporal connection encoding. Temporal connection (TC) encoding is designed to inform DGT if
an edge (i, j) exists in the t-th snapshot graph. We denote ETC = [eTC
2t‚àí1, eTC
2t ]T
t=1 ‚ààR2T √ód as the
temporal connection encoding lookup-table where d represents the hidden dimension size, which
is indexed by a function œà(i, j, t) indicating whether an edge (i, j) exists at time-step t. More
speciÔ¨Åcally, we have œà(i, j, t) = 2t if (i, j) ‚ààGt, œà(i, j, t) = 2t ‚àí1 if (i, j) Ã∏‚ààGt and use this value
as an index to extract the corresponding temporal connection embedding from the look-up table for
next-step processing. Note that during pre-training or the training on Ô¨Årst few time-steps, we need
to mask-out certain time-steps to avoid leaking information related to the predicted items (e.g., the
temporal reconstruction task in Section. 4.1). In these cases, we set œà(i, j, t‚Ä≤) = √ò where t‚Ä≤ denotes
the time-step we mask-out, and skip the embedding extraction at time t‚Ä≤.
Spatial distance encoding. Spatial distance (SD) encoding is designed to provide DGT a global
view of the graph structure. The success of Transformer is largely attributed to its global receptive"
SPATIAL-TEMPORAL ENCODING,0.171875,Under review as a conference paper at ICLR 2022
SPATIAL-TEMPORAL ENCODING,0.17447916666666666,"Ô¨Åeld due to its full attention, i.e., each token in the sequence can attend independently to other
tokens and process its representations. Computing full attention requires the model to explicitly
capturing the positions dependency between tokens, which can be achieved by either assigning each
position an absolute positional encoding or encode the relative distance using relative positional
encoding. However, for graphs, the design of unique node positions is not mandatary because a
graph is not changed by the permutation of its nodes. To encode the global structural information
of a graph in the model, inspired by (Ying et al., 2021), we adopt a spatial distance encoding
that measures the relative spatial relationship between any two nodes in the graph, which is a
generalization of the classical Transformer‚Äôs positional encoding to the graph domain. Let Dmax be
the maximum shortest path distance (SPD) we considered, where Dmax is a hyper-parameter that
can be smaller than the graph diameter. More speciÔ¨Åcally, given any node i and node j, we deÔ¨Åne
œÜ(i, j) = min{SPD(i, j), Dmax} as the SPD between the two nodes if SPD(i, j) < Dmax and
otherwise as Dmax. Let ESD = [eSD
1 , . . . , eSD
Dmax] ‚ààRDmax√ód as the spatial distance lookup-table
which is indexed by the œÜ(i, j), where œÜ(i, j) is used to select the spatial distance encoding eSD
œÜ(i,j)
that provides the spatial distance information of two nodes.
Integrate spatial-temporal encoding. We integrate temporal connection encoding and spatial
distance encoding by projecting them as a bias term in the self-attention module. SpeciÔ¨Åcally, to
integrate the spatial-temporal encoding of node pair (i, j) to DGT, we Ô¨Årst gather all its associated
temporal connection encodings on different time-steps as {eTC
œÜ(i,j,t)}T
t=1. Then, we apply weight
average on all encodings over the temporal axis and projected the temporal averaged encoding as
a scalar by ATC
i,j = Linear
 
WeightAverage({eTC
œÜ(i,j,t)}T
t=1)

‚ààR, where the aggregation weight is
learned during training. Similarly, to integrate the spatial distance encoding, we project the spatial
distance encoding of node pair (i, j) as a scalar by ASD
i,j = Linear(eSD
œÜ(i,j)) ‚ààR. Then, ATC
i,j and ASD
i,j
are used as the bias term to the self-attention, which we describe in detail in Section 3.4."
GRAPH TRANSFORMER ARCHITECTURE,0.17708333333333334,"3.4
GRAPH TRANSFORMER ARCHITECTURE"
GRAPH TRANSFORMER ARCHITECTURE,0.1796875,"As shown in Figure 1, each layer in DGT consists of two towers (i.e., the target node tower and
the context node tower) to encode the target nodes and the context nodes separately. The same set
of parameters are shared between two towers. The two-tower structure is motivated by the fact
that nodes within each group are sampled independently but there exist neighborhood relationships
between inter-group nodes. Only attending inter-group nodes help DGT better capture this context
information without fusing representations from irrelevant nodes. The details are as follows:"
GRAPH TRANSFORMER ARCHITECTURE,0.18229166666666666,"‚Ä¢ First, we compute the self-attentions that are used to aggregate information from target nodes to
context nodes (denote as ‚Äúctx‚Äù) and from context nodes to target nodes (denote as ‚Äútgt‚Äù). Let deÔ¨Åne
H(‚Ñì)
ctx ‚ààR|Vctx|√ód as the ‚Ñì-th layer output of the context-node tower and H(‚Ñì)
tgt ‚ààR|Vtgt|√ód as the ‚Ñì-th
layer output of the target-node tower. Then, the ‚Ñìth layer self-attention is computed as"
GRAPH TRANSFORMER ARCHITECTURE,0.18489583333333334,"A(‚Ñì)
ctx =
(LN(H(‚Ñì‚àí1)
ctx
)W(‚Ñì)
Q )(LN(H(‚Ñì‚àí1)
tgt
)W(‚Ñì)
K )‚ä§
‚àö"
GRAPH TRANSFORMER ARCHITECTURE,0.1875,"d
, A(‚Ñì)
tgt =
(LN(H(‚Ñì‚àí1)
tgt
)W(‚Ñì)
Q )(LN(H(‚Ñì‚àí1)
ctx
)W(‚Ñì)
K )‚ä§
‚àö d
,"
GRAPH TRANSFORMER ARCHITECTURE,0.19010416666666666,"where LN(H) stands for applying layer normalization on H and W(‚Ñì)
Q , W(‚Ñì)
K are weight matrices.
‚Ä¢ Then, we integrate spatial-temporal encoding as a bias term to self-attention as follows"
GRAPH TRANSFORMER ARCHITECTURE,0.19270833333333334,"P(‚Ñì)
ctx = A(‚Ñì)
ctx + ATC[Vctx; Vtgt] + ASD[Vctx; Vtgt], P(‚Ñì)
tgt = A(‚Ñì)
tgt + ATC[Vtgt; Vctx] + ASD[Vtgt; Vctx],
where ATC[VA; VB], ASD[VA; VB] denote the the matrix form of the projected temporal connection
and spatial distance self-attention bias with row indexed by VA and columns indexed by VB.1"
GRAPH TRANSFORMER ARCHITECTURE,0.1953125,"‚Ä¢ After that, we use the normalized P(‚Ñì)
ctx and P(‚Ñì)
tgt to propagate information between two towers, i.e.,"
GRAPH TRANSFORMER ARCHITECTURE,0.19791666666666666,"Z(‚Ñì)
ctx = Softmax(P(‚Ñì)
ctx )LN(H(‚Ñì‚àí1)
tgt
)W(‚Ñì)
V +H(‚Ñì‚àí1)
ctx
, Z(‚Ñì)
tgt = Softmax(P(‚Ñì)
tgt )LN(H(‚Ñì‚àí1)
ctx
)W(‚Ñì)
V +H(‚Ñì‚àí1)
tgt
.
‚Ä¢ Finally, a residual connected feed-forward network is applied to the aggregated message to produce
the Ô¨Ånal output"
GRAPH TRANSFORMER ARCHITECTURE,0.20052083333333334,"H(‚Ñì)
ctx = FFN(LN(Z(‚Ñì)
ctx )) + Z(‚Ñì)
ctx , H(‚Ñì)
tgt = FFN(LN(Z(‚Ñì)
tgt )) + Z(‚Ñì)
tgt ,
where FFN(¬∑) denotes the multi-layer feed-forward network. The Ô¨Ånal layer output of the target
node tower H(L)
tgt will be used to compute the loss deÔ¨Åned in Section 4."
GRAPH TRANSFORMER ARCHITECTURE,0.203125,"1Given a matrix A ‚ààRm√ón, the element at the i-th row and j-th column is denoted as Ai,j, the submatrix
formed from row Irow = {a1, . . . , ar} and columns Icol = {b1, . . . , bs} is denoted as A [Irow; Icol]."
GRAPH TRANSFORMER ARCHITECTURE,0.20572916666666666,Under review as a conference paper at ICLR 2022
GRAPH TRANSFORMER ARCHITECTURE,0.20833333333333334,DyGraphTransformer
GRAPH TRANSFORMER ARCHITECTURE,0.2109375,DyGraphTransformer
GRAPH TRANSFORMER ARCHITECTURE,0.21354166666666666,"Context node 
Sampling 1 3
4 5 1 2"
GRAPH TRANSFORMER ARCHITECTURE,0.21614583333333334,"3
4
ùí¢! ùí¢"" 1 2 3
4 5
?"
GRAPH TRANSFORMER ARCHITECTURE,0.21875,"Target nodes 1 2
3
4 5"
GRAPH TRANSFORMER ARCHITECTURE,0.22135416666666666,"Target nodes ùí±!""!"
GRAPH TRANSFORMER ARCHITECTURE,0.22395833333333334,"Context node ùí±#!$ 1 3
2
4 5"
GRAPH TRANSFORMER ARCHITECTURE,0.2265625,"Target nodes ùí±!""!"
GRAPH TRANSFORMER ARCHITECTURE,0.22916666666666666,"Context node $ùí±!""! ùêá%&% (() #ùêá%&% (()"
GRAPH TRANSFORMER ARCHITECTURE,0.23177083333333334,"Context node 
Sampling"
GRAPH TRANSFORMER ARCHITECTURE,0.234375,"‚Ñí*+,-(ùöØ)"
GRAPH TRANSFORMER ARCHITECTURE,0.23697916666666666,"‚Ñí.,/01(ùöØ)"
GRAPH TRANSFORMER ARCHITECTURE,0.23958333333333334,ùí¢23+43
GRAPH TRANSFORMER ARCHITECTURE,0.2421875,"Figure 2: Overview of the pre-training. Given snapshot graphs {G1, G2} as input, we Ô¨Årst generate
the temporal union graph. Then, we sample the target node Vtgt and two different set of context nodes
Vctx, eVctx. After that, we apply DGT on {Vtgt, Vctx} and {Vtgt, eVctx} to output H(L)
tgt and eH(L)
tgt . To
this end, we optimize Lview(Œò) by maximizing the similarity between H(L)
tgt and eH(L)
tgt , and optimize
Lrecon(Œò) by recovering snapshot graphs using H(L)
tgt ."
DYNAMIC GRAPH TRANSFORMER LEARNING,0.24479166666666666,"4
DYNAMIC GRAPH TRANSFORMER LEARNING"
DYNAMIC GRAPH TRANSFORMER LEARNING,0.24739583333333334,"Transformer usually requires a signiÔ¨Åcant amount of supervised data to guarantee their generalization
ability on unseen data. However, existing dynamic graph datasets are relatively small and may
not be sufÔ¨Åcient to train a powerful Transformer. To overcome this challenge, we propose to Ô¨Årst
pre-train DGT with two complementary self-supervised objective functions (in Section 4.1). Then,
we Ô¨Åne-tine DGT using the supervised objective function (in Section 4.2). Notice that the same set
of snapshot graphs but different objective functions are used for pre-training and Ô¨Åne-tuning. Finally,
via an information-theoretic analysis, we show that the representation can have a better generalization
ability on downstream tasks by optimizing our pre-training losses (in Section 4.3)."
PRE-TRAINING,0.25,"4.1
PRE-TRAINING"
PRE-TRAINING,0.2526041666666667,"We introduce a temporal reconstruction loss Lrecon(Œò) and a multi-view contrastive loss Lview(Œò) as
our self-supervised object functions. Then, our overall pre-taining loss is deÔ¨Åned as Lpre-train(Œò) =
Lrecon(Œò)+Œ≥Lview(Œò), where Œ≥ is a hyper-parameter that balances the importance of two pre-taining
tasks as illustrated in Figure 2."
PRE-TRAINING,0.2552083333333333,"Temporal reconstruction loss. To ensure that the spatial-temporal encoding is effective and can
inform DGT the temporal dependency between multiple snapshot graphs, we introduce a temporal
reconstruction loss as our Ô¨Årst pre-training objective. Our goal is to reconstruct the t-th graph snapshot
Gt‚Äôs structure using all but except the t-th graph snapshot (i.e., G \ Gt). Let ¬ØH(L)
tgt (t) denote the target-
node tower‚Äôs Ô¨Ånal layer output computed on G \ Gt. To decode the graph structure of graph snapshot
Gt, we use a fully connected layer as the temporal structure decoder that takes ¬ØH(L)
tgt (t) as input and
output E(t) = Linear( ¬ØH(L)
tgt (t)) ‚ààR|Vtgt|√ód with ei(t) ‚ààRd denotes the i-th row of E(t). Then, the
temporal reconstruction loss is deÔ¨Åned as Lrecon(Œò) = PT
t=1 LinkPredLoss({ei(t)}i‚ààVtgt, Vtgt, Et),
where œÉ(¬∑) is Sigmoid function and"
PRE-TRAINING,0.2578125,"LinkPredLoss({xi}i‚ààS, S, E) ‚âú
X i,j‚ààS 
‚àí
X"
PRE-TRAINING,0.2604166666666667,"(i,j)‚ààE
log(œÉ(x‚ä§
i xj)) ‚àí
X"
PRE-TRAINING,0.2630208333333333,"(i,j)Ã∏‚ààE
log(1 ‚àíœÉ(x‚ä§
i xj))

. (1)"
PRE-TRAINING,0.265625,"Multi-view contrastive loss.
Recall that Vctx is constructed by deterministically selecting the
common neighbors of Vtgt with the top-K PPR score. Then, we introduce eVctx as the subset of the
common neighbors of Vtgt randomly sampled with sampling probability of each node proportional
to its PPR score. Since a different set of context nodes are provided for the same set of target
nodes, {Vtgt, eVctx} provides an alternative view of {Vtgt, Vctx} when computing the representation
for nodes in Vtgt. Notice that although the provided context nodes are different, since they have
the same target nodes, it is natural to expect the calculated representation have high similarity.
We denote H(L)
tgt
and eH(L)
tgt
as the Ô¨Ånal layer model output that are computed on {Vtgt, Vctx} and
{Vtgt, eVctx}. To this end, we introduce our second self-supervised objective function as Lview(Œò) =
‚à•H(L)
tgt ‚àíStopGrad( eH(L)
tgt )‚à•2
F + ‚à•StopGrad(H(L)
tgt ) ‚àíeH(L)
tgt ‚à•2
F, where StopGrad denotes stop gradient.
Note that optimizing Lview(Œò) alone without stopping gradient results in a degenerated solution (Chen
& He, 2021; Tian et al., 2021)."
PRE-TRAINING,0.2682291666666667,Under review as a conference paper at ICLR 2022
FINE-TUNING,0.2708333333333333,"4.2
FINE-TUNING"
FINE-TUNING,0.2734375,"To apply the pre-trained model for downstream tasks, we choose to Ô¨Ånetune the pre-trained model
with downstream task objective functions. Here, we take link prediction as an example. Our goal is
to predict the existence of a link at time T + 1 using information up to time T. Let H(L)
tgt ({Gj}t
j=1)
denote the Ô¨Ånal layer output of DGT using snapshot graphs {Gj}t
j=1. Then, the link prediction loss
is deÔ¨Åned as LLinkPred(Œò) = PT ‚àí1
t=1 LinkPredLoss(H(L)
tgt ({Gj}t
j=1), Vtgt, Et+1), where LinkPredLoss
is deÔ¨Åned in Eq. 1."
ON THE IMPORTANCE OF PRE-TRAINING FROM INFORMATION THEORY PERSPECTIVE,0.2760416666666667,"4.3
ON THE IMPORTANCE OF PRE-TRAINING FROM INFORMATION THEORY PERSPECTIVE"
ON THE IMPORTANCE OF PRE-TRAINING FROM INFORMATION THEORY PERSPECTIVE,0.2786458333333333,"In this section, we show that our pre-training objectives can improve the generalization error under
mild assumptions and results in a better performance on downstream tasks. Let X denote the
input random variable, S as the self-supervised signal (also known as a different view of input
X), and ZX = f(X), ZS = f(S) as the representations that are generated by a deterministic
mapping function f. In our setting, we have the sampled sub-graph of temporal-union graph
Gunion induced by node {Vtgt, Vctx} as input X, the sampled subgraph of Gunion induced by node
{Vtgt, eVctx} as self-supervised signal S, and DGT as f that computes the representation of X, S by
ZX = f(X), ZS = f(S). Besides, we introduce the task-relevant information as Y , which refers to
the information that is required for downstream tasks. For example, when the downstream task is
link prediction, Y can be the ground truth graph structure about which we want to reason. Notice
that in practice we have no access to Y during pre-training and it is only introduced as the notation
for analysis. Furthermore, let H(A) denote entropy, H(A|B) denote conditional entropy, I(A; B)
denote mutual information, and I(A; B|C) denote conditional mutual information. More details and
preliminaries on information theory are deferred to Appendix B."
ON THE IMPORTANCE OF PRE-TRAINING FROM INFORMATION THEORY PERSPECTIVE,0.28125,"In the following, we study the generalization error of the learned representation ZX under the
binary classiÔ¨Åcation setting. We choose Bayes error rate (i.e., the lowest possible test error rate
a binary classiÔ¨Åer can achieve) as our evaluation metric, which can be formally deÔ¨Åned as Pe =
1 ‚àíE[maxy P(Y = y|ZX)]. Before proceeding to our result, we make the following assumption on
input X, self-supervised signal S, and task-relevant information Y .
Assumption 1. We assume the task-relevant information is shared between the input random variable
X, self-supervised signal S, i.e., we have I(X; Y |S) = 0 and I(S; Y |X) = 0."
ON THE IMPORTANCE OF PRE-TRAINING FROM INFORMATION THEORY PERSPECTIVE,0.2838541666666667,"We argue the above assumption is mild because input X and self-supervised signal S are two different
views of the data, therefore they are expected to contain task-relevant information Y . In Proposition 1,
we make connections between the Bayes error rate and pre-training losses, which explains why the
proposed pre-training losses are helpful for downstream tasks. We defer the proofs to Appendix B."
ON THE IMPORTANCE OF PRE-TRAINING FROM INFORMATION THEORY PERSPECTIVE,0.2864583333333333,"Proposition 1. We can upper bound Bayes error rate by Pe ‚â§1 ‚àíexp(‚àíH(Y ) + I(ZX; X) ‚àí
I(ZX; X|Y )), and reduce the upper bound of Pe by (1) maximizing the mutual information I(ZX; X)
between the learned representation ZX and input X, which can be achieved by minimizing temporal
reconstruction loss Lrecon(Œò), and (2) minimizing the task-irrelevant information between the learned
representation ZX and input X, which can be achieved by minimizing our multi-view loss Lview(Œò)."
ON THE IMPORTANCE OF PRE-TRAINING FROM INFORMATION THEORY PERSPECTIVE,0.2890625,"The Proposition 1 suggests that if we can create a different views S of our input data X in a way
such that both X and S contain the task-relevant information Y , then by jointly optimizing our two
pre-training losses can result in the representation ZX with a lower Bayes error rate Pe. Our analysis
is based on the information theory framework developed in (Tsai et al., 2020), in which they show
that using contrastive loss between ZX and S (i.e., maximizing I(ZX; S)), predicting S from ZX
(i.e., minimizing H(S|ZX)), and predicting ZX from S (i.e., minimizing H(ZX|S)) can result in a
smaller Bayes error rate Pe."
EXPERIMENTS,0.2916666666666667,"5
EXPERIMENTS"
EXPERIMENTS,0.2942708333333333,"We evaluate DGT using dynamic graph link prediction, which has been widely used in (Sankar et al.,
2018; Goyal et al., 2018) to compare its performance with a variety of static and dynamic graph
representation learning baselines. Besides, DGT can also be applied to other downstream tasks such
as node classiÔ¨Åcation. We defer node classiÔ¨Åcation results to Appendix A.3."
EXPERIMENTS,0.296875,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.2994791666666667,Table 1: Comparing DGT with baselines using Micro- and Macro-AUC on real-world datasets.
EXPERIMENTS,0.3020833333333333,"Method
Metric
Enron
RDS
UCI
Yelp
ML-10M"
EXPERIMENTS,0.3046875,"NODE2VEC
Micro-AUC
82.42 ¬± 2.03
81.10 ¬± 0.87
81.41 ¬± 0.60
68.93 ¬± 0.33
90.50 ¬± 0.83
Macro-AUC
81.35 ¬± 2.93
82.85 ¬± 0.86
81.39 ¬± 0.76
67.38 ¬± 0.49
89.48 ¬± 0.62"
EXPERIMENTS,0.3072916666666667,"GRAPHSAGE
Micro-AUC
82.39 ¬± 3.01
85.49 ¬± 0.96
79.85 ¬± 2.62
62.36 ¬± 1.01
86.31 ¬± 0.97
Macro-AUC
83.41 ¬± 2.94
86.64 ¬± 0.89
78.45 ¬± 2.01
58.36 ¬± 0.91
90.23 ¬± 0.90"
EXPERIMENTS,0.3098958333333333,"DYNAERNN
Micro-AUC
74.00 ¬± 1.24
80.56 ¬± 0.77
79.29 ¬± 1.90
71.54 ¬± 0.83
87.01 ¬± 0.88
Macro-AUC
74.36 ¬± 1.35
80.16 ¬± 0.91
83.81 ¬± 1.25
72.29 ¬± 0.58
89.04 ¬± 0.67"
EXPERIMENTS,0.3125,"DYNGEM
Micro-AUC
66.46 ¬± 0.74
79.29 ¬± 1.01
76.36 ¬± 0.83
69.43 ¬± 1.09
79.80 ¬± 0.88
Macro-AUC
68.46 ¬± 1.14
81.94 ¬± 1.97
78.22 ¬± 0.99
69.93 ¬± 0.78
84.86 ¬± 0.49"
EXPERIMENTS,0.3151041666666667,"DYSAT
Micro-AUC
83.81 ¬± 1.55
83.89 ¬± 0.92
83.10 ¬± 0.99
69.00 ¬± 0.22
88.91 ¬± 0.87
Macro-AUC
83.73 ¬± 1.61
83.60 ¬± 0.68
86.32 ¬± 1.46
69.42 ¬± 0.25
90.63 ¬± 0.91"
EXPERIMENTS,0.3177083333333333,"EVOLVEGCN
Micro-AUC
73.83 ¬± 1.23
85.35 ¬± 0.87
85.81 ¬± 0.50
68.99 ¬± 0.67
92.79 ¬± 0.21
Macro-AUC
75.77 ¬± 1.57
86.53 ¬± 0.76
84.18 ¬± 0.72
69.41 ¬± 0.26
93.45 ¬± 0.19"
EXPERIMENTS,0.3203125,"DGT
Micro-AUC
87.32 ¬± 0.87
88.77 ¬± 0.50
87.91 ¬± 0.32
73.39 ¬± 0.21
95.30 ¬± 0.36
Macro-AUC
87.82 ¬± 0.89
89.77 ¬± 0.46
88.49 ¬± 0.43
74.31 ¬± 0.23
96.16 ¬± 0.22"
EXPERIMENT SETUP,0.3229166666666667,"5.1
EXPERIMENT SETUP"
EXPERIMENT SETUP,0.3255208333333333,"Datasets. We select Ô¨Åve real-world datasets of various sizes and types in our experiments. The
detailed data statistics can be accessed at Table 11 in Appendix C.2. Graph snapshots are created by
splitting the data using suitable time windows such that each snapshot has an equitable number of
interactions. In each snapshot, the edge weights are determined by the number of interactions.
Link prediction task. To compare the performance of DGT with baselines, we follow the evaluation
strategy in (Goyal et al., 2018; Zhou et al., 2018; Sankar et al., 2018) by training a logistic regression
classiÔ¨Åer taking two node embeddings as input for dynamic graph link prediction. SpeciÔ¨Åcally, we
learn the dynamic node representations on snapshot graphs {G1, . . . , GT } and evaluate DGT by
predicting links at GT +1. For evaluation, we consider all links in GT +1 as positive examples and an
equal number of sampled unconnected node pairs as negative examples. We split 20% of the edge
examples for training the classiÔ¨Åer, 20% of examples for hyper-parameters tuning, and the rest 60%
of examples for model performance evaluation following the practice of existing studies (e.g., (Sankar
et al., 2018)). We evaluate the link prediction performance using Micro- and Macro-AUC scores,
where the Micro-AUC is calculated across the link instances from all the time-steps while the Macro-
AUC is computed by averaging the AUC at each time-step. During inference, all nodes in the testing
set (from 60% edge samples in GT +1) are selected as the target nodes. To scale the inference of the
testing sets of any sizes, we compute the full attention by Ô¨Årst splitting all self-attentions into multiple
chunks then iteratively compute the self-attention in each chunk (as shown in Figure 7). Since only a
Ô¨Åxed number of self-attention is computed at each iteration, we signiÔ¨Åcantly reduce DGT ‚Äôs inference
memory consumption. We also repeat all experiments three times with different random seeds.
Baselines.
We compare with several state-of-the-art methods as baselines including both static
and dynamic graph learning algorithms. For static graph learning algorithms, we compare against
NODE2VEC (Grover & Leskovec, 2016) and GRAPHSAGE (Hamilton et al., 2017). To make
the comparison fair, we feed these static graph algorithms the same temporal-union graph used in
DGT rather than any single graph snapshots. For dynamic graph learning algorithms, we compare
against DYNAERNN (Goyal et al., 2020), DYNGEM (Goyal et al., 2018), DYSAT (Sankar et al.,
2018), and EvolveGCN (Pareja et al., 2020). We use the ofÔ¨Åcial implementations for all baselines
and select the best hyper-parameters for both baselines and DGT. Notice that we only compare with
dynamic graph algorithms that takes a set of temporal ordered snapshot graph as input, and leave
the study on other dynamic graph structure (e.g., continuous time-step algorithms (Xu et al., 2020;
Rossi et al., 2020)) as a future direction. More details on experiment conÔ¨Ågurations are deferred to
Appendix C and more results (Figure 4, Table 2 and 3) are deferred to Appendix A.1."
EXPERIMENT RESULTS,0.328125,"5.2
EXPERIMENT RESULTS"
EXPERIMENT RESULTS,0.3307291666666667,"Table 1 indicates the state-of-the-art performance of our approach on link prediction tasks, where
DGT achieves a consistent 1% ‚àº3% Macro-AUC gain on all datasets. Besides, DGT is more stable
when using different random seeds observed from a smaller standard deviation of the AUC score.
Furthermore, to better understand the behaviors of different methods from a Ô¨Åner granularity, we
compare the model performance at each time-step in Figure 4 and observe that the performance of
DGT is relatively more stable than other methods over time. Besides, we additionally report the
results of dynamic link prediction evaluated only on unseen links at each time-step. Here, we deÔ¨Åne
unseen links as the ones that Ô¨Årst appear at the prediction time-step but are not in the previous graph
snapshots. From Table 2, we Ô¨Ånd that although all methods achieve a lower AUC score, which may"
EXPERIMENT RESULTS,0.3333333333333333,Under review as a conference paper at ICLR 2022
EXPERIMENT RESULTS,0.3359375,Figure 3: Comparison of the Micro- and Macro-AUC score of DGT with and without pre-training.
EXPERIMENT RESULTS,0.3385416666666667,"be due to the new link prediction is more challenging, DGT still achieves a consistent 1% ‚àº3%
Macro-AUC gain over baselines. Moreover, we compare the training time and memory consumption
with several baselines in Table 3 and shows that DGT maintains a good scalability."
ABLATION STUDIES,0.3411458333333333,"5.3
ABLATION STUDIES"
ABLATION STUDIES,0.34375,"We conduct ablation studies to further understand DGT and present the details in Appendix A.2.
The effectiveness of pre-training. We compare the performance of DGT with and without pre-
training. As shown in Figure 3, DGT ‚Äôs performance is signiÔ¨Åcantly improved if we Ô¨Årst pre-train
it with the self-supervised loss with a Ô¨Åne-tuning on downstream tasks. When comparing the AUC
scores at each time-step, we observe that the DGT with no pre-training has a relatively lower
performance but a larger variance. This may be due to the vast number of training parameters in
DGT , which potentially requires more data to be trained well. The self-supervised pre-training
alleviate this challenge by utilizing additional unlabeled input data.
Comparing two-tower to single-tower architecture. In Table 4, we compare the performance of
DGT with single- and two-tower design where a single-tower means a full-attention of over all pairs
of target and context nodes. We observe that the two-tower DGT has a consistent performance gain
(0.5% Micro- and Macro-AUC) over the single-tower on Yelp and ML-10M. This may be due to that
the nodes within the target or context node set are sampled independently while inter-group nodes are
likely to be connected. Only attending inter-group nodes helps DGT better capturing these contextual
information without fusing representations from irrelevant nodes.
Comparing K-hop attention with full attention. To better understand full-attention, we compare
it with sparson and ions such as 1-hop and 3-hop attention. These variants are evaluated based on
the single-tower DGT to include all node pairs into consideration. Table 5 shows the results where
we observe that the full attention presents a consistent performance gain around 1% ‚àº3% over the
other two variants. This demonstrates the beneÔ¨Åts of full-attention when modeling implicit edge
connections in graphs with a larger receptive Ô¨Åelds comparing to its K-hop counterparts.
The effectiveness of spatial-temporal encoding.
In Table 6, we conduct an ablation study by
independently removing two encodings to validate the effectiveness of spatial-temporal encoding. We
observe that even without any encoding (i.e., ignoring the spatial-temporal graph topologies), due to
full attention, DGT is still very competitive comparing with the state-of-the-art baselines in Table 1.
However, we also observe a 0.6% ‚àº4.6% performance gain when adding the spatial connection and
temporal distance encoding, which empirically shows their effectiveness.
The effectiveness of stacking more layers. When stacking more layers, traditional GNNs usually
suffer from the over-smoothing (Zhao & Akoglu, 2020; Yan et al., 2021) and result in a degenerated
performance. We study the effect of applying more DGT layers and show results in Table 7. In
contrast to previous studies, DGT has a relatively stable performance and does not suffer much
from performance degradation when the number of layers increases. This is potentially due to that
DGT only requires a shallow architecture since each individual layer is capable of modeling longer-
range dependencies due to full attention. Besides, the self-attention mechanism can automatically
attend importance neighbors, therefore alleviate the over-smoothing and bottleneck effect."
CONCLUSION,0.3463541666666667,"6
CONCLUSION"
CONCLUSION,0.3489583333333333,"In this paper, we introduce DGT for dynamic graph representation learning, which can efÔ¨Åciently
leverage the graph topology and capture implicit edge connections. To further improve the general-
ization ability, two complementary pre-training tasks are introduced. To handle large-scale dynamic
graphs, a temporal-union graph structure and a target-context node sampling strategy are designed
for an efÔ¨Åcient and scalable training. Extensive experiments on real-world dynamic graphs show that
DGT presents signiÔ¨Åcant performance gains over several state-of-the-art baselines. Potential future
directions include exploring GNNs on continuous dynamic graphs and studying its expressive power."
CONCLUSION,0.3515625,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.3541666666666667,"Reproducibility Statement. We summarize the hardware speciÔ¨Åcation and environment used in
experiments in Appendix C.1, details on all datasets in Appendix C.2, details on baseline hyper-
parameters tuning in Appendix C.3, details on DGT hyper-parameters in Appendix C.4, and the
proof of theory in Appendix B. We plan to release our code afterwards or being requested by the
reviewers during the review phase."
REFERENCES,0.3567708333333333,REFERENCES
REFERENCES,0.359375,"Reid Andersen, Fan Chung, and Kevin Lang. Local graph partitioning using pagerank vectors. In
Foundations of Computer Science, 2006."
REFERENCES,0.3619791666666667,"Rianne van den Berg, Thomas N Kipf, and Max Welling. Graph convolutional matrix completion. In
International Conference on Knowledge Discovery & Data Mining, 2017."
REFERENCES,0.3645833333333333,"Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,
Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya
Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural
Information Processing Systems, 2020."
REFERENCES,0.3671875,"Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Conference on
Advances in Neural Information Processing Systems, 2021."
REFERENCES,0.3697916666666667,Thomas M. Cover and Joy A. Thomas. Elements of Information Theory. 2006.
REFERENCES,0.3723958333333333,"Zhiyong Cui, Kristian Henrickson, Ruimin Ke, and Yinhai Wang. TrafÔ¨Åc graph convolutional
recurrent neural network: A deep learning framework for network-scale trafÔ¨Åc learning and
forecasting. IEEE Transactions on Intelligent Transportation Systems, 2019."
REFERENCES,0.375,"Micha¬®el Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on
graphs with fast localized spectral Ô¨Åltering. In Advances in Neural Information Processing Systems,
2016."
REFERENCES,0.3776041666666667,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. In Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, 2019."
REFERENCES,0.3802083333333333,"Kien Do, Truyen Tran, and Svetha Venkatesh. Graph transformation policy network for chemical
reaction prediction. In International Conference on Knowledge Discovery & Data Mining, 2019."
REFERENCES,0.3828125,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An
image is worth 16x16 words: Transformers for image recognition at scale.
arXiv preprint
arXiv:2010.11929, 2020."
REFERENCES,0.3854166666666667,"David Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, Rafael G¬¥omez-Bombarelli, Tim-
othy Hirzel, Al¬¥an Aspuru-Guzik, and Ryan P. Adams. Convolutional networks on graphs for
learning molecular Ô¨Ångerprints. In Advances in Neural Information Processing Systems, 2015."
REFERENCES,0.3880208333333333,"Vijay Prakash Dwivedi and Xavier Bresson. A generalization of transformer networks to graphs.
arXiv preprint arXiv:2012.09699, 2020."
REFERENCES,0.390625,"Meir Feder and Neri Merhav. Relations between entropy and error probability. IEEE Transactions on
Information theory, 40, 1994."
REFERENCES,0.3932291666666667,"Palash Goyal, Nitin Kamra, Xinran He, and Yan Liu. Dyngem: Deep embedding method for dynamic
graphs. CoRR, 2018."
REFERENCES,0.3958333333333333,"Palash Goyal, Sujit Rokka Chhetri, and Arquimedes Canedo. dyngraph2vec: Capturing network
dynamics using dynamic graph representation learning. Knowledge-Based Systems, 187, 2020."
REFERENCES,0.3984375,Under review as a conference paper at ICLR 2022
REFERENCES,0.4010416666666667,"Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In International
Conference on Knowledge Discovery and Data Mining, 2016."
REFERENCES,0.4036458333333333,"William L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large
graphs. In Advances in Neural Information Processing Systems, 2017."
REFERENCES,0.40625,"Ziniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou Sun. Heterogeneous graph transformer. In
WWW ‚Äô20: The Web Conference 2020, Taipei, Taiwan, April 20-24, 2020, 2020."
REFERENCES,0.4088541666666667,"Thomas N. Kipf and Max Welling. Semi-supervised classiÔ¨Åcation with graph convolutional networks.
In International Conference on Learning Representations, 2017."
REFERENCES,0.4114583333333333,"Srijan Kumar, Xikun Zhang, and Jure Leskovec. Predicting dynamic embedding trajectory in temporal
interaction networks. In International Conference on Knowledge Discovery & Data Mining, 2019."
REFERENCES,0.4140625,"Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining
Guo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint
arXiv:2103.14030, 2021."
REFERENCES,0.4166666666666667,Kevin P. Murphy. Probabilistic Machine Learning: An introduction. 2022.
REFERENCES,0.4192708333333333,"Aldo Pareja, Giacomo Domeniconi, Jie Chen, Tengfei Ma, Toyotaro Suzumura, Hiroki Kanezashi,
Tim Kaler, Tao Schardl, and Charles Leiserson. Evolvegcn: Evolving graph convolutional networks
for dynamic graphs. In Conference on ArtiÔ¨Åcial Intelligence, volume 34, 2020."
REFERENCES,0.421875,"Afshin Rahimi, Trevor Cohn, and Timothy Baldwin. Semi-supervised user geolocation via graph
convolutional networks. In Proceedings of the Association for Computational Linguistics, 2018."
REFERENCES,0.4244791666666667,"Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, Federico Monti, and Michael
Bronstein. Temporal graph networks for deep learning on dynamic graphs. arXiv preprint
arXiv:2006.10637, 2020."
REFERENCES,0.4270833333333333,"Aravind Sankar, Yanhong Wu, Liang Gou, Wei Zhang, and Hao Yang. Dynamic graph representation
learning via self-attention networks. arXiv preprint arXiv:1812.09430, 2018."
REFERENCES,0.4296875,"Youngjoo Seo, Micha¬®el Defferrard, Pierre Vandergheynst, and Xavier Bresson. Structured sequence
modeling with graph convolutional recurrent networks. In International Conference on Neural
Information Processing, 2018."
REFERENCES,0.4322916666666667,"Yuandong Tian, Xinlei Chen, and Surya Ganguli. Understanding self-supervised learning dynamics
without contrastive pairs. arXiv preprint arXiv:2102.06810, 2021."
REFERENCES,0.4348958333333333,"Yao-Hung Hubert Tsai, Yue Wu, Ruslan Salakhutdinov, and Louis-Philippe Morency. Self-supervised
learning from a multi-view perspective. arXiv preprint arXiv:2006.05576, 2020."
REFERENCES,0.4375,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information
Processing Systems, 2017."
REFERENCES,0.4401041666666667,"Hongwei Wang, Fuzheng Zhang, Mengdi Zhang, Jure Leskovec, Miao Zhao, Wenjie Li, and
Zhongyuan Wang. Knowledge-aware graph neural networks with label smoothness regular-
ization for recommender systems. In International Conference on Knowledge Discovery & Data
Mining, 2019a."
REFERENCES,0.4427083333333333,"Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with
linear complexity. arXiv preprint arXiv:2006.04768, 2020."
REFERENCES,0.4453125,"Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. KGAT: knowledge graph
attention network for recommendation. In International Conference on Knowledge Discovery &
Data Mining, 2019b."
REFERENCES,0.4479166666666667,"Da Xu, Chuanwei Ruan, Evren K¬®orpeoglu, Sushant Kumar, and Kannan Achan. Inductive repre-
sentation learning on temporal graphs. In International Conference on Learning Representations,
2020."
REFERENCES,0.4505208333333333,Under review as a conference paper at ICLR 2022
REFERENCES,0.453125,"Yujun Yan, Milad Hashemi, Kevin Swersky, Yaoqing Yang, and Danai Koutra. Two sides of the
same coin: Heterophily and oversmoothing in graph convolutional neural networks. arXiv preprint
arXiv:2102.06462, 2021."
REFERENCES,0.4557291666666667,"Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen,
and Tie-Yan Liu. Do transformers really perform bad for graph representation? arXiv preprint
arXiv:2106.05234, 2021."
REFERENCES,0.4583333333333333,"Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and Jure Leskovec.
Graph convolutional neural networks for web-scale recommender systems. In International
Conference on Knowledge Discovery & Data Mining, 2018."
REFERENCES,0.4609375,"Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J. Kim. Graph transformer
networks. In Advances in Neural Information Processing Systems, 2019."
REFERENCES,0.4635416666666667,"Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago
Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for
longer sequences. In Advances in Neural Information Processing Systems, 2020."
REFERENCES,0.4661458333333333,"Jiawei Zhang, Haopeng Zhang, Congying Xia, and Li Sun. Graph-bert: Only attention is needed for
learning graph representations. arXiv preprint arXiv:2001.05140, 2020."
REFERENCES,0.46875,"Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. In International
Conference on Learning Representations, 2020."
REFERENCES,0.4713541666666667,"Dawei Zhou, Lecheng Zheng, Jiawei Han, and Jingrui He. A data-driven graph generative model
for temporal interaction networks. In International Conference on Knowledge Discovery & Data
Mining, 2020."
REFERENCES,0.4739583333333333,"L. Zhou, Y. Yang, X. Ren, F. Wu, and Y. Zhuang. Dynamic Network Embedding by Modelling
Triadic Closure Process. In Conference on ArtiÔ¨Åcial Intelligence, 2018."
REFERENCES,0.4765625,Under review as a conference paper at ICLR 2022
REFERENCES,0.4791666666666667,Appendix
REFERENCES,0.4817708333333333,Table of Contents
REFERENCES,0.484375,"A More experiment results
14
A.1
Link prediction results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
A.2
Ablation study results. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
A.3
Node classiÔ¨Åcation results . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
A.4
Results on noisy dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
A.5
Comparison with continuous-graph learning algorithms
. . . . . . . . . . . . .
18"
REFERENCES,0.4869791666666667,"B
Pre-training can reduce the irreducible error
18
B.1
Preliminary on information theory . . . . . . . . . . . . . . . . . . . . . . . . .
18
B.2
Proof of Proposition 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20"
REFERENCES,0.4895833333333333,"C Experiment conÔ¨Åguration
21
C.1
Hardware speciÔ¨Åcation and environment
. . . . . . . . . . . . . . . . . . . . .
21
C.2
Details on datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
C.3
Details on baseline hypter-parameters tuning . . . . . . . . . . . . . . . . . . .
22
C.4
Additional details on experiment conÔ¨Åguration . . . . . . . . . . . . . . . . . .
23"
REFERENCES,0.4921875,"D Comparison of different graph Transformer
23
D.1
Positional encoding types . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
D.2
Graph Transformer with other applications . . . . . . . . . . . . . . . . . . . .
24"
REFERENCES,0.4947916666666667,Under review as a conference paper at ICLR 2022
REFERENCES,0.4973958333333333,"A
MORE EXPERIMENT RESULTS"
REFERENCES,0.5,"A.1
LINK PREDICTION RESULTS"
REFERENCES,0.5026041666666666,"In this section, we provide the remained Ô¨Ågures and tables in Section 5."
REFERENCES,0.5052083333333334,"Comparison of AUC score at different time steps. In Figure 4, we compare the AUC score of
DGT with baselines on Enron, UCI, Yelp, and ML-10M dataset. We can observe that DGT can
consistently outperform baselines on Enron and ML-10M dataset at all time steps, but has a relatively
lower AUC score at certain time steps on the UCI and Yelp dataset. Besides, the performance of
DGT is relatively more stable than baselines on different time steps."
REFERENCES,0.5078125,"Figure 4: Comparison of DGT with baselines across multiple time steps, where the Macro-AUC
score is reported in the box next to the curves."
REFERENCES,0.5104166666666666,"Comparision of AUC score on new link prediction task.
In Table 2, we report dynamic link
prediction result evaluated only on the new links at each time step, where a link that appears at
the current snapshot but not in the previous snapshot is considered as a new link. This experiment
can provide an in-depth analysis of the capabilities of different methods in predicting unseen links.
As shown in Table 2, all methods achieve a lower AUC score, which is expected because new link
prediction is more challenging. However, DGT still achieves consistent gains of 1 ‚àº3% Macro-AUC
over baselines, thus illustrate its effectiveness in accurately temporal context for new link prediction."
REFERENCES,0.5130208333333334,"Table 2: Comparison of Micro- and Macro-AUC on real-world datasets restricted to new edges.
Method
Metric
Enron
RDS
UCI
Yelp
ML-10M"
REFERENCES,0.515625,"NODE2VEC
Micro-AUC
76.45 ¬± 1.93
75.62 ¬± 1.42
75.31 ¬± 0.83
68.83 ¬± 0.29
88.92 ¬± 0.79
Macro-AUC
76.01 ¬± 2.39
76.25 ¬± 0.85
75.82 ¬± 0.96
68.00 ¬± 0.51
88.01 ¬± 0.50"
REFERENCES,0.5182291666666666,"GRAPHSAGE
Micro-AUC
74.36 ¬± 2.88
80.21 ¬± 0.87
76.56 ¬± 1.91
61.97 ¬± 1.00
85.18 ¬± 0.89
Macro-AUC
74.42 ¬± 2.64
79.99 ¬± 0.78
75.94 ¬± 1.88
58.49 ¬± 0.89
89.31 ¬± 0.93"
REFERENCES,0.5208333333333334,"DYNAERNN
Micro-AUC
61.81 ¬± 1.89
68.43 ¬± 1.13
77.39 ¬± 2.10
70.82 ¬± 0.93
86.89 ¬± 0.75
Macro-AUC
62.62 ¬± 1.91
68.18 ¬± 1.23
81.82 ¬± 1.71
71.56 ¬± 0.77
89.45 ¬± 0.53"
REFERENCES,0.5234375,"DYNGEM
Micro-AUC
59.42 ¬± 1.42
72.43 ¬± 1.62
74.72 ¬± 0.73
69.23 ¬± 1.76
77.18 ¬± 1.96
Macro-AUC
61.61 ¬± 1.91
74.49 ¬± 2.21
76.34 ¬± 0.78
70.67 ¬± 1.32
82.62 ¬± 0.49"
REFERENCES,0.5260416666666666,"DYSAT
Micro-AUC
75.78 ¬± 1.89
76.28 ¬± 1.34
81.18 ¬± 1.09
69.12 ¬± 0.21
88.21 ¬± 0.64
Macro-AUC
76.92 ¬± 1.81
76.87 ¬± 1.21
83.43 ¬± 1.57
69.20 ¬± 0.20
88.98 ¬± 0.87"
REFERENCES,0.5286458333333334,"EVOLVEGCN
Micro-AUC
67.82 ¬± 1.71
78.36 ¬± 0.91
81.99 ¬± 0.73
68.73 ¬± 0.64
90.91 ¬± 0.32
Macro-AUC
69.39 ¬± 1.89
79.18 ¬± 1.01
82.18 ¬± 0.76
68.63 ¬± 0.30
91.45 ¬± 0.29"
REFERENCES,0.53125,"DGT
Micro-AUC
81.99 ¬± 0.90
82.78 ¬± 0.56
85.78 ¬± 0.99
73.32 ¬± 0.22
93.01 ¬± 0.23
Macro-AUC
81.32 ¬± 0.89
82.89 ¬± 0.52
86.21 ¬± 0.56
73.88 ¬± 0.22
93.56 ¬± 0.21"
REFERENCES,0.5338541666666666,Under review as a conference paper at ICLR 2022
REFERENCES,0.5364583333333334,"Computation time and memory consumption. In Table 3, we compare the memory consumption
and epoch time on the last time step of ML-10M and Yelp dataset. We chose the last time step of
these two datasets because its graph size is relatively larger than others, which can provide a more
accurate time and memory estimation. The memory consumption is record by nvidia-smi and
the time is recorded by function time.time(). During pre-training, DGT samples 256 context
node and 256 context node at each iteration. During Ô¨Åne-tuning, DGT Ô¨Årst 256 positive links (links
in the graph) and sample 2, 560 negative links (node pairs that do not exist in the graph), then treat
all nodes in the sampled node pairs at target nodes and sample the same amount of context nodes.
Notice that although the same sampling size hyper-parameter is used, since the graph size and the
graph density are different, the actual memory consumption and time are also different. For example,
since the Yelp dataset has more edges with more associated nodes for evaluation than ML-10M, the
memory consumption and time are required on Yelp than on ML-10M dataset."
REFERENCES,0.5390625,"Table 3: Comparison of the epoch time and memory consumption of DGT with baseline methods
on the last time step of ML-10M and Yelp dataset using the neural architecture conÔ¨Åguration
summarized in Section C.4."
REFERENCES,0.5416666666666666,"Dataset
Method
Memory consumption
Epoch time
Total time"
REFERENCES,0.5442708333333334,ML-10M
REFERENCES,0.546875,"DYSAT
9.2 GB
97.2 Sec
4276.8 Sec (45 epochs)
EVOLVEGCN
13.6 GB
6.9 Sec
821.1 Sec (120 epochs)
DGT (Pre-training)
6.5 GB
38.9 Sec
986.5 Sec (89 epochs)
DGT (Fine-tuning)
10.1 GB
2.98 Sec
62.2 Sec (22 epochs) Yelp"
REFERENCES,0.5494791666666666,"DYSAT
5.4 GB
29.4 Sec
4706.4 Sec (160 epochs)
EVOLVEGCN
7.5 GB
19.14 Sec
1091.2 Sec (57 epochs)
DGT (Pre-training)
21.3 GB
11.8 Sec
413.5 Sec (34 epochs)
DGT (Fine-tuning)
21.3 GB
21.41 Sec
521.6 Sec (23 epochs)"
REFERENCES,0.5520833333333334,"A.2
ABLATION STUDY RESULTS."
REFERENCES,0.5546875,"In this section, we provide missing the tables in Section 5.3, where discussion on the results are
provided in Section 5.3."
REFERENCES,0.5572916666666666,"Compare two-tower to single-tower architecture. In Table 4, we compare the Micro-AUC score
and Macro-AUC score of DGT with one-tower2 and two-tower structure on UCI, Yelp, and ML-10M
datasets."
REFERENCES,0.5598958333333334,"Table 4: Comparison of the Micro- and Macro-AUC of DGT using single-tower and two-tower model
architecture on the real-world datasets."
REFERENCES,0.5625,"Method
Metric
UCI
Yelp
ML-10M"
REFERENCES,0.5651041666666666,"Single-tower
Micro-AUC
87.86 ¬± 0.60
72.95 ¬± 0.20
94.80 ¬± 0.81
Macro-AUC
88.27 ¬± 0.68
73.81 ¬± 0.21
95.49 ¬± 0.57"
REFERENCES,0.5677083333333334,"Two-tower
Micro-AUC
87.91 ¬± 0.32
73.39 ¬± 0.21
95.30 ¬± 0.36
Macro-AUC
88.49 ¬± 0.43
74.31 ¬± 0.23
96.16 ¬± 0.22"
REFERENCES,0.5703125,2The node representation H(‚Ñì) in the single-tower DGT is computed by
REFERENCES,0.5729166666666666,H(‚Ñì) = FFN(LN(Z(‚Ñì))) + Z(‚Ñì)
REFERENCES,0.5755208333333334,Z(‚Ñì) = Softmax
REFERENCES,0.578125,"(LN(H(‚Ñì‚àí1))W(‚Ñì)
Q )(LN(H(‚Ñì‚àí1))W(‚Ñì)
K )‚ä§
‚àö"
REFERENCES,0.5807291666666666,"d
+ ATC + ASD !"
REFERENCES,0.5833333333333334,"LN(H(‚Ñì‚àí1))W(‚Ñì)
V
+ H(‚Ñì‚àí1). (2)"
REFERENCES,0.5859375,Under review as a conference paper at ICLR 2022
REFERENCES,0.5885416666666666,"Compare K-hop attention with full attention. In Table 4, we compare the performance of ‚Äúsingle-
tower DGT using full-attention‚Äù, ‚Äúsingle-tower DGT using 1-hop attention‚Äù, and ‚Äúsingle-tower
DGT using 3-hop attention‚Äù on the UCI, Yelp, and ML-10M dataset."
REFERENCES,0.5911458333333334,"Table 5: Comparison of the Micro- and Macro-AUC of full attention and K-hop attention using the
single-tower architecture on the real-world datasets."
REFERENCES,0.59375,"Method
Metric
UCI
Yelp
ML-10M"
REFERENCES,0.5963541666666666,"Full attention
Micro-AUC
87.86 ¬± 0.60
72.95 ¬± 0.20
94.80 ¬± 0.81
Macro-AUC
88.27 ¬± 0.68
73.81 ¬± 0.21
95.49 ¬± 0.57"
"-HOP NEIGHBOR
MICRO-AUC",0.5989583333333334,"1-hop neighbor
Micro-AUC
84.62 ¬± 0.31
71.33 ¬± 0.43
91.88 ¬± 0.73
Macro-AUC
85.10 ¬± 0.15
71.45 ¬± 0.45
92.18 ¬± 0.44"
"-HOP NEIGHBOR
MICRO-AUC",0.6015625,"3-hop neighbor
Micro-AUC
87.01 ¬± 0.89
71.19 ¬± 0.22
91.83 ¬± 0.92
Macro-AUC
87.48 ¬± 0.88
72.31 ¬± 0.22
92.33 ¬± 0.82"
"-HOP NEIGHBOR
MICRO-AUC",0.6041666666666666,"The effectiveness of spatial-temporal encoding.
In Table 6, we validate the effectiveness of
spatial-temporal encoding by independently removing the temporal edge coding and spatial distance
encoding."
"-HOP NEIGHBOR
MICRO-AUC",0.6067708333333334,"Table 6: Comparison of the Micro- and Macro-AUC of with and without spatial-temporal encoding
on the real-world datasets."
"-HOP NEIGHBOR
MICRO-AUC",0.609375,"Method
Metric
UCI
Yelp
ML-10M"
"-HOP NEIGHBOR
MICRO-AUC",0.6119791666666666,"With both encoding
Micro-AUC
87.91 ¬± 0.32
73.39 ¬± 0.21
95.30 ¬± 0.36
Macro-AUC
88.49 ¬± 0.43
74.31 ¬± 0.23
96.16 ¬± 0.22"
"-HOP NEIGHBOR
MICRO-AUC",0.6145833333333334,"Without any encoding
Micro-AUC
83.27 ¬± 0.29
72.82 ¬± 0.37
91.81 ¬± 0.43
Macro-AUC
83.87 ¬± 0.47
73.80 ¬± 0.38
92.59 ¬± 0.35"
"-HOP NEIGHBOR
MICRO-AUC",0.6171875,"Only temporal connective encoding
Micro-AUC
84.78 ¬± 0.31
73.36 ¬± 0.26
94.51 ¬± 0.37
Macro-AUC
84.60 ¬± 0.42
74.31 ¬± 0.25
95.43 ¬± 0.29"
"-HOP NEIGHBOR
MICRO-AUC",0.6197916666666666,"Only spatial distance encoding
Micro-AUC
87.01 ¬± 0.46
72.98 ¬± 0.32
92.34 ¬± 0.40
Macro-AUC
87.99 ¬± 0.47
73.90 ¬± 0.36
93.13 ¬± 0.33"
"-HOP NEIGHBOR
MICRO-AUC",0.6223958333333334,"The effect of the number of layers. In Table 7, we compare the Micro-AUC score and Macro-AUC
score of DGT with a different number of layers on the UCI, Yelp, and ML-10M datasets."
"-HOP NEIGHBOR
MICRO-AUC",0.625,"Table 7: Comparison of the Micro- and Macro-AUC score of DGT with different number of layers
on the real-world datasets."
"-HOP NEIGHBOR
MICRO-AUC",0.6276041666666666,"Method
Metric
UCI
Yelp
ML-10M"
"LAYERS
MICRO-AUC",0.6302083333333334,"2 layers
Micro-AUC
87.89 ¬± 0.43
74.30 ¬± 0.21
94.99 ¬± 0.21
Macro-AUC
88.31 ¬± 0.53
74.29 ¬± 0.23
96.08 ¬± 0.15"
"LAYERS
MICRO-AUC",0.6328125,"4 layers
Micro-AUC
87.42 ¬± 0.36
73.39 ¬± 0.21
95.30 ¬± 0.36
Micro-AUC
88.35 ¬± 0.37
74.31 ¬± 0.23
96.16 ¬± 0.22"
"LAYERS
MICRO-AUC",0.6354166666666666,"6 layers
Micro-AUC
87.91 ¬± 0.32
74.30 ¬± 0.20
95.35 ¬± 0.28
Micro-AUC
88.49 ¬± 0.43
74.28 ¬± 0.22
96.11 ¬± 0.18"
"LAYERS
MICRO-AUC",0.6380208333333334,Under review as a conference paper at ICLR 2022
"LAYERS
MICRO-AUC",0.640625,"A.3
NODE CLASSIFICATION RESULTS"
"LAYERS
MICRO-AUC",0.6432291666666666,"In this section, we show that although DGT is orginally designed for the link prediction task, the
learned representation of DGT can be also applied to binary node classiÔ¨Åcation. We evaluate
DGT on Wikipedia and Reddit dataset, where dataset statistic is summarized in Table 11. The
snapshot is created in a similar manner as the link prediction task. As shown in Table 8 and Figure 5,
DGT performs around 0.7% better than all baselines on the Wikipedia dataset and around 0.7%
better than EVOLVEGCN on Reddit dataset. However, the results DGT on the Reddit dataset is
slightly lower than DYSAT. This is potentially due to DGT is less in favor of a dense graph, e.g.,
Reddit dataset, with very dense graph structure information encoded by spatial-temporal encodings."
"LAYERS
MICRO-AUC",0.6458333333333334,"Table 8: Comparison of the Micro- and Macro-AUC score of DGT with different number of layers
on the real-world datasets for binary node classiÔ¨Åcation task."
"LAYERS
MICRO-AUC",0.6484375,"Method
Metric
Wikipedia
Reddit"
"LAYERS
MICRO-AUC",0.6510416666666666,"DYSAT
Micro-AUC
94.69 ¬± 0.46
87.35 ¬± 0.28
Macro-AUC
94.74 ¬± 0.66
87.36 ¬± 0.30"
"LAYERS
MICRO-AUC",0.6536458333333334,"EVOLVEGCN
Micro-AUC
92.31 ¬± 0.68
84.72 ¬± 0.89
Micro-AUC
92.36 ¬± 0.85
84.79 ¬± 0.88"
"LAYERS
MICRO-AUC",0.65625,"DGT (without pre-training)
Micro-AUC
92.90 ¬± 0.84
82.37 ¬± 0.78
Micro-AUC
92.94 ¬± 0.62
84.41 ¬± 0.82"
"LAYERS
MICRO-AUC",0.6588541666666666,"DGT (with pre-training)
Micro-AUC
95.49 ¬± 0.66
85.48 ¬± 0.43
Micro-AUC
95.55 ¬± 0.65
85.50 ¬± 0.44"
"LAYERS
MICRO-AUC",0.6614583333333334,"Figure 5: Comparison of DGT with baselines across multiple time steps, where the Macro-AUC
score is reported in the box next to the curves"
"LAYERS
MICRO-AUC",0.6640625,"A.4
RESULTS ON NOISY DATASET"
"LAYERS
MICRO-AUC",0.6666666666666666,"In this section, we study the effect of noisy input on the performance of DGT using UCI and Yelp
datasets. We achieve this by randomly selecting 10%, 20%, 50% of the node pairs and changing
their connection status either from connected to not-connected or from not-connected to connected.
As shown in Table 9, although the performance of both using full-attention and 1-hop attention
decreases as the noisy level increases, the performance of using full-attention aggregation is more
stable and robust as the noisy level changes. This is because 1-hop attention relies more on the given
structure, while full-attention only take the give structure as a reference and learns the ‚Äúground truth‚Äù
underlying graph structure by gradient descent update."
"LAYERS
MICRO-AUC",0.6692708333333334,"Table 9: Comparison of the Macro-AUC score of DGT and its variants with input graph with different
noisy level."
"LAYERS
MICRO-AUC",0.671875,"Method
10%
20%
50%"
"LAYERS
MICRO-AUC",0.6744791666666666,"UCI
DGT (1-hop attention)
Micro-AUC
82.97 ¬± 0.56
81.23 ¬± 0.78
77.85 ¬± 0.66
Macro-AUC
83.01 ¬± 0.61
82.10 ¬± 0.60
78.43 ¬± 0.67"
"LAYERS
MICRO-AUC",0.6770833333333334,"DGT (Full aggregation)
Micro-AUC
86.98 ¬± 0.51
86.10 ¬± 0.57
84.36 ¬± 0.49
Macro-AUC
86.12 ¬± 0.57
85.93 ¬± 0.59
85.51 ¬± 0.51"
"LAYERS
MICRO-AUC",0.6796875,"Yelp
DGT (1-hop attention)
Micro-AUC
70.00 ¬± 0.20
68.55 ¬± 0.21
65.32 ¬± 0.22
Macro-AUC
69.94 ¬± 0.20
68.45 ¬± 0.23
65.61 ¬± 0.15"
"LAYERS
MICRO-AUC",0.6822916666666666,"DGT (Full aggregation)
Micro-AUC
70.99 ¬± 0.20
71.74 ¬± 0.19
70.93 ¬± 0.21
Macro-AUC
71.64 ¬± 0.18
71.67 ¬± 0.21
69.93 ¬± 0.21"
"LAYERS
MICRO-AUC",0.6848958333333334,Under review as a conference paper at ICLR 2022
"LAYERS
MICRO-AUC",0.6875,"A.5
COMPARISON WITH CONTINUOUS-GRAPH LEARNING ALGORITHMS"
"LAYERS
MICRO-AUC",0.6901041666666666,"In this section, we compare snapshot graph-based methods against continuous graph-based learning
algorithm on the UCI, Yelp, and ML-10M dataset. For the continuous graph learning algorithm, we
choose JODIE Kumar et al. (2019) and TGAT Xu et al. (2020) as the baseline. As shwon in Table 10,
JODIE and TGAT suffer from signiÔ¨Åcant performance degradation. This is because they are designed
to leverage the edge features and Ô¨Åne-grained timestamp information for link prediction, however,
these information is lacking on existing snapshot graph datasets."
"LAYERS
MICRO-AUC",0.6927083333333334,"Please note that we compare with continuous graph algorithm only for the sake of completeness.
However, since snapshot graph-based methods and continuous graph-based methods require different
input graph structures, different evaluation strategies, and are designed under different settings,
directly comparing two sets of methods cannot provide much meaningful interpretation. For example,
existing works Kumar et al. (2019); Xu et al. (2020) on a continuous graph select the training and
evaluation set by taking the Ô¨Årst 80% of links in the dataset for training and taking the rest for
evaluation. In other words, the training and evaluation samples can be arbitrary close and might even
come from the same time step. However, in the snapshot graph, the training and evaluation set is
selected by taking the links in the previous T ‚àí1 snapshot graphs for training and evaluating on
the T-th snapshot graph. That is, the training and evaluation samples never come from the same
time step. Besides, since the time steps in the continuous graph are Ô¨Åne-grained than snapshot
graphs, continuous graph methods suffer from performance degradation when applied on the snapshot
graph dataset due to lack of Ô¨Åne-grained timestamp information. Due to the aforementioned reasons,
existing continuous graph learning methods (e.g., Jodie, TGAT) only compare with other continuous
graph methods on the continuous datasets, similarly, existing snapshot graph learning methods
(e.g., DySAT, EvolveGCN, DynAERNN, DynGEM) also only considers other snapshot graph based
methods as their baseline for a comparison."
"LAYERS
MICRO-AUC",0.6953125,"Table 10: Comparison of the Micro- and Macro-AUC score of DGT , JODIE on the real-world
datasets."
"LAYERS
MICRO-AUC",0.6979166666666666,"Method
Metric
UCI
Yelp
ML-10M"
"LAYERS
MICRO-AUC",0.7005208333333334,"DGT
Micro-AUC
87.91 ¬± 0.32
73.39 ¬± 0.21
95.30 ¬± 0.36
Macro-AUC
88.49 ¬± 0.43
74.31 ¬± 0.23
96.16 ¬± 0.22"
"LAYERS
MICRO-AUC",0.703125,"JODIE
Micro-AUC
57.99 ¬± 0.34
59.85 ¬± 0.32
62.84 ¬± 0.47
Macro-AUC
57.21 ¬± 0.37
61.01 ¬± 0.44
61.30 ¬± 0.46"
"LAYERS
MICRO-AUC",0.7057291666666666,"TGAT
Micro-AUC
48.15 ¬± 0.45
51.95 ¬± 0.39
52.15 ¬± 0.51
Macro-AUC
49.02 ¬± 0.43
52.78 ¬± 0.40
51.15 ¬± 0.50"
"LAYERS
MICRO-AUC",0.7083333333333334,"B
PRE-TRAINING CAN REDUCE THE IRREDUCIBLE ERROR"
"LAYERS
MICRO-AUC",0.7109375,"B.1
PRELIMINARY ON INFORMATION THEORY"
"LAYERS
MICRO-AUC",0.7135416666666666,"In this section, we recall preliminaries on information theory, which are helpful to understand the
proof in the following section. More details can be found in books such as Murphy (2022); Cover &
Thomas (2006)."
"LAYERS
MICRO-AUC",0.7161458333333334,"Entropy. Let X be a discrete random variable, X as the sample space, and x as outcome. We deÔ¨Åne
the probability mass function as p(x) = Pr(X = x), x ‚ààX. Then, the entropy for a discrete random
variable X is deÔ¨Åned as"
"LAYERS
MICRO-AUC",0.71875,"H(X) = ‚àí
X"
"LAYERS
MICRO-AUC",0.7213541666666666,"x‚ààX
p(x) log p(x),
(3)"
"LAYERS
MICRO-AUC",0.7239583333333334,"where we use log base 2. The joint entropy of two random variables X, Y is deÔ¨Åned as"
"LAYERS
MICRO-AUC",0.7265625,"H(X, Y ) = ‚àí
X"
"LAYERS
MICRO-AUC",0.7291666666666666,"x‚ààX,y‚ààY
p(x, y) log p(x, y).
(4)"
"LAYERS
MICRO-AUC",0.7317708333333334,Under review as a conference paper at ICLR 2022
"LAYERS
MICRO-AUC",0.734375,"ùêª(ùëã)
ùêª(ùëå)"
"LAYERS
MICRO-AUC",0.7369791666666666,"ùêª(ùëã|ùëå)
ùêª(ùëå|ùëã)
ùêº(ùëã;ùëå)"
"LAYERS
MICRO-AUC",0.7395833333333334,"ùêª(ùëã, ùëå)"
"LAYERS
MICRO-AUC",0.7421875,"Figure 6: Relationship between entropy and mutual information (Figure 2.2 of Cover & Thomas
(2006))."
"LAYERS
MICRO-AUC",0.7447916666666666,"The conditional entropy of Y given X is the uncertainty we have in Y after seeing X, which is
deÔ¨Åned as"
"LAYERS
MICRO-AUC",0.7473958333333334,"H(Y |X) =
X"
"LAYERS
MICRO-AUC",0.75,"x‚ààX
p(x)H(Y |X = x)"
"LAYERS
MICRO-AUC",0.7526041666666666,"= H(X, Y ) ‚àíH(X).
(5)"
"LAYERS
MICRO-AUC",0.7552083333333334,"Notice that we have H(Y |X) = 0 if Y = f(X), where f a deterministic mapping."
"LAYERS
MICRO-AUC",0.7578125,"Mutual information. Mutual information is a special case of KL-divergence, which is a measure of
distance between two distributions. The KL-divergence between p(x), q(x) is deÔ¨Åned as"
"LAYERS
MICRO-AUC",0.7604166666666666,"KL(p‚à•q) =
X"
"LAYERS
MICRO-AUC",0.7630208333333334,"x‚ààX
p(x) log p(x)"
"LAYERS
MICRO-AUC",0.765625,"q(x).
(6)"
"LAYERS
MICRO-AUC",0.7682291666666666,"Then, the mutual information I(X; Y ) between random variable X, Y is deÔ¨Åned as follows"
"LAYERS
MICRO-AUC",0.7708333333333334,"I(X; Y ) = KL
 
p(x, y)‚à•p(x)p(y)

=
X"
"LAYERS
MICRO-AUC",0.7734375,"x‚ààX,y‚ààY
log p(x, y)"
"LAYERS
MICRO-AUC",0.7760416666666666,"p(x)p(y),
(7)"
"LAYERS
MICRO-AUC",0.7786458333333334,"and we have I(X; Y ) = 0 if X, Y are independent. Notice that we use I(X; Y ) instead of I(X, Y )
represent the mutual information between X and Y . Besides, I(X; Y, Z) represent the mutual
information between X and (Y, Z)."
"LAYERS
MICRO-AUC",0.78125,"Based on the above deÔ¨Ånition on entropy and mutual information, we have the following relation
between entropy and mutual information:
I(X; Y ) = H(X) ‚àíH(X|Y )
= H(Y ) ‚àíH(Y |X)
= H(X) + H(Y ) ‚àíH(X, Y ),
(8)"
"LAYERS
MICRO-AUC",0.7838541666666666,"and the following relation between conditional entropy and conditional mutual information:
I(X; Y |Z) = H(X|Z) ‚àíH(X|Y, Z)
= H(Y |Z) ‚àíH(Y |X, Z)
= H(X|Z) + H(Y |Z) ‚àíH(X, Y |Z),
(9)"
"LAYERS
MICRO-AUC",0.7864583333333334,"where conditional mutual information I(X; Y |Z) can be think of as the reduction in the uncertainty
of X due to knowledge of Y when Z is given."
"LAYERS
MICRO-AUC",0.7890625,A Ô¨Ågure showing the relation between mutual information and entropy is provided in Figure 6.
"LAYERS
MICRO-AUC",0.7916666666666666,"Data processing inequality. Random variables X, Y, Z are said to form a Markov chain X ‚ÜíY ‚Üí
Z if the joint probability mass function can be written as P(x, y, z) = p(x)p(y|x)p(z|y). Suppose
random variable X, Y, Z forms a Markov chain X ‚ÜíY ‚ÜíZ, then we have I(X; Y ) ‚â•I(X; Z)."
"LAYERS
MICRO-AUC",0.7942708333333334,Under review as a conference paper at ICLR 2022
"LAYERS
MICRO-AUC",0.796875,"Bayes error and entropy. In the binary classiÔ¨Åcation setting, Bayes error rate is the lowest possible
test error rate (i.e., irreducible error), which can be formally deÔ¨Åned as"
"LAYERS
MICRO-AUC",0.7994791666666666,"Pe = E

1 ‚àímax
y
p(Y = y|X)

,
(10)"
"LAYERS
MICRO-AUC",0.8020833333333334,"where Y denotes label and X denotes input. Feder & Merhav (1994) derives an upper bound showing
the relation between Bayes error rate with entropy:
‚àílog(1 ‚àíPe) ‚â§H(Y |X).
(11)
The above inequality is used as the foundation of our following analysis."
"LAYERS
MICRO-AUC",0.8046875,"B.2
PROOF OF PROPOSITION 1"
"LAYERS
MICRO-AUC",0.8072916666666666,"In the following, we utilize the analysis framework developed in Tsai et al. (2020) to show the
importance of two pre-training loss functions."
"LAYERS
MICRO-AUC",0.8098958333333334,"By using Eq. 11, we have the following inequality:
‚àílog(1 ‚àíPe) ‚â§H(Y |ZX)
(12)
By rearanging the above inequality, we have the following upper bound on the Bayes error rate"
"LAYERS
MICRO-AUC",0.8125,"Pe ‚â§1 ‚àí
1
exp
 
H(Y |ZX)
"
"LAYERS
MICRO-AUC",0.8151041666666666,"=
(a) 1 ‚àí
1
exp
 
H(Y ) ‚àíI(ZX; Y )
"
"LAYERS
MICRO-AUC",0.8177083333333334,"=
(b) 1 ‚àí
1
exp
 
H(Y ) ‚àíI(ZX; X) + I(ZX; X|Y )
, (13)"
"LAYERS
MICRO-AUC",0.8203125,"where equality (a) is due to I(ZX; Y ) = H(Y ) ‚àíH(Y |ZX), equality (b) is due to I(ZX; Y ) =
I(ZX; X) ‚àíI(ZX; X|Y ) + I(ZX; Y |X) and I(ZX; Y |X) = 0 because ZX = f(X) is a determin-
istic mapping given input X. Our goal is to Ô¨Ånd the deterministic mapping function f to generate
ZX that can maximize I(ZX; X) ‚àíI(ZX; X|Y ), such that the upper bound on the right hand side
of Eq. 13 is minimized. We can achieve this by:"
"LAYERS
MICRO-AUC",0.8229166666666666,"‚Ä¢ Maximizing the mutual information I(ZX; X) between the representation ZX to the input X.
‚Ä¢ Minimizing the task-irrelevant information I(ZX; X|Y ), i.e., the mutual information between the
representation ZX to the input X given task-relevant information Y ."
"LAYERS
MICRO-AUC",0.8255208333333334,"In the following, we Ô¨Årst show that minimizing Lrecon(Œò) can maximize the mutual information
I(ZX; X), then we show that minimizing Lview(Œò) can minimize the task irrelevant information
I(ZX; X|Y )."
"LAYERS
MICRO-AUC",0.828125,"Maximize mutual information I(ZX; X). By the relation between mutual information and entropy
I(ZX; X) = H(X) ‚àíH(X|ZX), we know that maximizing the mutual information I(ZX; X) is
equivalent to minimizing the conditional entropy H(X|ZX). Notice that we ignore H(X) because it
is only dependent on the raw feature and is irrelevant to feature representation ZX. By the deÔ¨Ånition
of conditional entropy, we have"
"LAYERS
MICRO-AUC",0.8307291666666666,"H(X|ZX) =
X"
"LAYERS
MICRO-AUC",0.8333333333333334,"zx‚ààZX
p(zx)H(X|ZX = zx) =
X"
"LAYERS
MICRO-AUC",0.8359375,"zx‚ààZX
p(zx)
X"
"LAYERS
MICRO-AUC",0.8385416666666666,"x‚ààX
‚àíp(x|zx) log p(x|zx) =
X zx‚ààZX X"
"LAYERS
MICRO-AUC",0.8411458333333334,"x‚ààX
‚àíp(x, zx) log p(x|zx)"
"LAYERS
MICRO-AUC",0.84375,"= EP(X,ZX)
h
‚àílog P(X|ZX)
i"
"LAYERS
MICRO-AUC",0.8463541666666666,"= min
QŒ∏ EP(X,ZX)
h
‚àílog QŒ∏(X|ZX)
i
‚àíKL

P(X|ZX)‚à•QŒ∏(X|ZX)
"
"LAYERS
MICRO-AUC",0.8489583333333334,"‚â§min
QŒ∏ EP(X,ZX)
h
‚àílog QŒ∏(X|ZX)
i (14)"
"LAYERS
MICRO-AUC",0.8515625,Under review as a conference paper at ICLR 2022
"LAYERS
MICRO-AUC",0.8541666666666666,"where QŒ∏(¬∑|¬∑) is a variational distribution with Œ∏ represent the parameters in QŒ∏ and KL denotes
KL-divergence."
"LAYERS
MICRO-AUC",0.8567708333333334,"Therefore,
maximizing mutual information I(ZX; X) can be achieved by minimizing
EPX,ZX [‚àílog QŒ∏(X|ZX)]. By assuming QŒ∏ as the categorical distribution and Œ∏ as a neural
network, minimizing EPX,ZX [‚àílog QŒ∏(X|ZX)] can be think of as introducing a neural network
parameterized by Œ∏ to predict the input X from the learned representation ZX by minimizing the
binary cross entropy loss."
"LAYERS
MICRO-AUC",0.859375,"Minimize the task irrelevant information I(ZX; X|Y ). Recall that in our setting, input X is the
node features of {Vtarget, Vcontext} and the subgraph induced by {Vtarget, Vcontext}. The self-supervised
signal S is node features of {Vtarget, eVcontext} and the subgraph induced by {Vtarget, eVcontext}. Therefore,
it is natural to make the following mild assumption on the input random variable X, self-supervised
signal S, and task relevant information Y ."
"LAYERS
MICRO-AUC",0.8619791666666666,"Assumption 2. We assume tall task-relevant information is shared between the input random variable
X, self-supervised signal S, i.e., we have I(X; Y |S) = 0 and I(S; Y |X) = 0."
"LAYERS
MICRO-AUC",0.8645833333333334,"In the following, we show that minimizing I(ZX; X|Y ) can be achieved by minimizing H(ZX|S).
From data processing inequality, we have I(X; Y |S) ‚â•I(ZX; Y |S) ‚â•0. From Assumption 2,
we have I(X; Y |S) = 0, therefore we know I(ZX; Y |S) = 0. By the relation between mutual
information and entropy, we have
I(ZX; X|Y ) = H(ZX|Y ) ‚àíH(ZX|X, Y )
=
(a) H(ZX|Y )"
"LAYERS
MICRO-AUC",0.8671875,"= H(ZX|S, Y ) + I(ZX; S|Y )
= H(ZX|S) ‚àíI(ZX; Y |S) + I(ZX; S|Y )
=
(b) H(ZX|S) + I(ZX; S|Y )"
"LAYERS
MICRO-AUC",0.8697916666666666,"‚â§
(c)
H(ZX|S) + I(X; S|Y ), (15)"
"LAYERS
MICRO-AUC",0.8723958333333334,"where equality (a) is due to H(ZX|X, Y ) = 0 since ZX = f(X) and f is a deterministic mapping,
equality (b) is due to I(ZX, Y |S) = 0, and inequality (c) is due to data processing inequality."
"LAYERS
MICRO-AUC",0.875,"From Eq. 14, we know that
H(ZX|S) = EP(S,ZX)[‚àílog P(ZX|S)]"
"LAYERS
MICRO-AUC",0.8776041666666666,"‚â§min
Q‚Ä≤
œÜ
EP(S,ZX)
h
‚àílog Q‚Ä≤
œÜ(ZX|S)
i
.
(16)"
"LAYERS
MICRO-AUC",0.8802083333333334,"By assuming Q‚Ä≤
œÜ as the Gaussian distribution and œÜ as a neural network, minimizing
EPS,ZX [‚àílog QœÜ(ZX|S)] can be think of as introducing a neural network parameterized by œÜ
that take S as input and output ZS = NeuralNetworkœÜ(S), then minimize the mean-square error
between ZX and ZS."
"LAYERS
MICRO-AUC",0.8828125,"C
EXPERIMENT CONFIGURATION"
"LAYERS
MICRO-AUC",0.8854166666666666,"C.1
HARDWARE SPECIFICATION AND ENVIRONMENT"
"LAYERS
MICRO-AUC",0.8880208333333334,"We run our experiments on a single machine with Intel i9-10850K, Nvidia RTX 3090 GPU, and
32GB RAM memory. The code is written in Python 3.7 and we use PyTorch 1.4 on CUDA 10.1 to
train the model on the GPU."
"LAYERS
MICRO-AUC",0.890625,"C.2
DETAILS ON DATASETS"
"LAYERS
MICRO-AUC",0.8932291666666666,"We summarize the dataset statistic in Table 11. More speciÔ¨Åcally, we prepare snapshot graphs
following the procedure as described in Sankar et al. (2018). In the following, we provide brief
descriptions of each dataset."
"LAYERS
MICRO-AUC",0.8958333333333334,Under review as a conference paper at ICLR 2022
"LAYERS
MICRO-AUC",0.8984375,"Table 11: Statistics of the datasets used in our experiments.
Enron
DRS
UCI
Yelp
ML-10M
Wikipedia
Reddit
Nodes
143
167
1, 809
6, 569
20, 537
9, 227
11, 000
Edges
2, 347
1, 521
16, 822
95, 361
43, 760
157, 474
672, 447
Time steps
16
100
13
16
13
11
11"
"LAYERS
MICRO-AUC",0.9010416666666666,"‚Ä¢ Enron dataset3: Enron is a public available social network dataset which contains data from about
150 users. We only consider the email communications between Enron employees to generate the
dynamic dataset, and use a 2 month sliding window to construct 16 snapshots.
‚Ä¢ RDS dataset4: This is a publicly available social network dataset that contains an email commu-
nication network between employees of a mid-sized manufacturing company Radoslaw. Nodes
represent employees and edges represent individual emails between two users. The snapshots are
created using a window size of 3 days.
‚Ä¢ UCI dataset5: This is a publicly available social network dataset that contains private messages
sent between users on an online social network platform at the University of California, Irvine over
6 months. The snapshots are created using a window size of 10 days.
‚Ä¢ Yelp dataset6: This is a public available rating dataset which contains user-business rating in
Arizona. We only consider user-business pairs that have at least 15 interactions. The snapshots are
created using a window size of 6 months.
‚Ä¢ ML-10M7: This is a publicly available rating dataset that contains tagging behavior of MovieLens
users, with the tags applied by a user on her rated movies. The snapshots are created using a
window size of 3 months.
‚Ä¢ Wikipedia8: This is a publicly available interaction graph, where users and pages are nodes, and
an interaction represents a user editing a page. The snapshots are created using a window size of 3
days.
‚Ä¢ Reddit9: This is a publicly available interaction graph, where users and subreddits are nodes, and
interaction occurs when a user writes a post to the subreddit. The snapshots are created using a
window size of 3 days. The data is from a pre-existing, publicly available dataset collected by a
third party."
"LAYERS
MICRO-AUC",0.9036458333333334,"C.3
DETAILS ON BASELINE HYPTER-PARAMETERS TUNING"
"LAYERS
MICRO-AUC",0.90625,We tune the hyper-parameters of baselines following their recommended guidelines.
"LAYERS
MICRO-AUC",0.9088541666666666,"‚Ä¢ NODE2VEC10: We use the default setting as introduced in Grover & Leskovec (2016). More
speciÔ¨Åcally, for each node we use 10 random walks of length 80, context window size as 10.
The in-out hyper-parameter p and return hyper-parameter q are selected by grid-search in range
{0.25, 0.5, 1, 2, 5} on the validation set.
‚Ä¢ GRAPHSAGE11: We use the default setting as introduced in Hamilton et al. (2017). More specif-
ically, we train two layer GNN with neighbor sampling size 25 and 10. The neighbor aggre-
gation is selected by grid-search from ‚Äúmean-based aggregation‚Äù, ‚ÄúLSTM-based aggregation‚Äù,
‚Äúmax-pooling aggregation‚Äù, and ‚ÄúGCN-based aggregation‚Äù on the validation set. In practice, GCN
aggregator performs best on Enron, RDS, and UCI, and max-pooling aggregator performs best on
Yelp and ML-10M."
"LAYERS
MICRO-AUC",0.9114583333333334,"3https://www.cs.cmu.edu/Àúenron/
4https://nrvis.com/download/data/dynamic/ia-radoslaw-email.zip
5http://konect.cc/networks/opsahl-ucsocial/
6https://www.yelp.com/dataset
7https://grouplens.org/datasets/movielens/10m/
8http://snap.stanford.edu/jodie/wikipedia.csv
9http://snap.stanford.edu/jodie/reddit.csv
10https://github.com/aditya-grover/node2vec
11https://github.com/williamleif/GraphSAGE"
"LAYERS
MICRO-AUC",0.9140625,Under review as a conference paper at ICLR 2022
"LAYERS
MICRO-AUC",0.9166666666666666,"‚Ä¢ DYNGEM and DYNAERNN12: We use the default setting as introduced in Goyal et al. (2018)
and Goyal et al. (2020). The scaling and regularization hyper-parameters is selected by grid-search
in range Œ± ‚àà{10‚àí6, 10‚àí5}, Œ≤ ‚àà{0.1, 1, 2, 5}, and ŒΩ1, ŒΩ2 ‚àà{10‚àí6, 10‚àí4} on the validation set.
‚Ä¢ DYSAT13: We use the default setting and model architecture as introduced in Sankar et al. (2018).
The co-occurring positive node pairs are sampled by running 10 random walks of length 40 for each
node. The negative sampling ratio is selected by grid-search in the range {0.01, 0.1, 1}, number of
the self-attention head is selected in the range {8, 16}, and the feature dimension is selected in the
range {128, 256} on the validation set.
‚Ä¢ EVOLVEGCN14: We use the default setting and model architecture as introduced in Pareja et al."
"LAYERS
MICRO-AUC",0.9192708333333334,"(2020). We train both EvolveGCN-O and EvolveGCN-H and report the architecture with the best
performance on the validation set. In practice, EvolveGCN-O performs best on UCI, Yelp, and
ML-10M, EvolveGCN-H performs best on Enron and RDS."
"LAYERS
MICRO-AUC",0.921875,"C.4
ADDITIONAL DETAILS ON EXPERIMENT CONFIGURATION"
"LAYERS
MICRO-AUC",0.9244791666666666,"We select a single set of hyper-parameters for each dataset using grid search. More speciÔ¨Åcally, we
select the negative sampling ratio (i.e., number of positive edge/number of negative edge) in the range
{0.01, 0.1, 1}, number of the self-attention head is selected in the range {8, 16}, feature dimension
is selected in the range {128, 256}, number of layers in the range {2, 4, 6}, maximum shortest path
distance Dmax in the range {2, 3, 5} on the validation set, and the hyper-parameter that balances the
importance of two pre-taining tasks Œ≥ = 1. The hyper-parameter for each dataset is summarized in
Table 12."
"LAYERS
MICRO-AUC",0.9270833333333334,"Table 12: Hyper-parameters used in DGT for different datasets. ‚Äú-‚Äù stands for hyper-parameters that
are not required."
"LAYERS
MICRO-AUC",0.9296875,"Enron
RDS
UCI
Yelp
ML-10M
Wiki
Reddit
Self-attention head
16
16
8
8
8
8
8
Hidden dimension
256
256
128
128
128
128
128
Number of layer
2
2
6
4
4
2
2
Hidden feature dropout out ratio
0.5
0.5
0.5
0.5
0.5
0.5
0.5
Self-attention dropout ratio
0.1
0.1
0.1
0.5
0.5
0.1
0.1
Negative sampling ratio
50
10
10
10
10
‚àí
‚àí
Maximize shortest path distance
5
5
5
5
5
5
5
Mini-batch size
512
512
512
512
512
800
800"
"LAYERS
MICRO-AUC",0.9322916666666666,"During training, we set the target node size as the largest number our GPU (Nvidia RTX 3090)
can Ô¨Åt, and sample the context nodes the same size as target nodes. In practice, we found that a
mini-batch size of 512 works well on all datasets. During the evaluation, we set all validation and
testing nodes as target nodes during evaluation. Notice that since gradients are not required during
evaluation, therefore we can obtain all the self-attention values by Ô¨Årst splitting all self-attentions
into multiple chunks, then we iterative compute the self-attention in each chunk on GPU as shown in
Figure 7. Since every iteration only a Ô¨Åxed number of self-attention are computed and computing the
self-attention is the most memory-consuming operation, DGT can inference validation and testing
set of any size."
"LAYERS
MICRO-AUC",0.9348958333333334,"D
COMPARISON OF DIFFERENT GRAPH TRANSFORMER"
"LAYERS
MICRO-AUC",0.9375,"In this section, we provide details on the comparison of different graph Transformers as summarized
in Table 13, provide detailed information on different positional encoding type in Section D.1, and
discussion on othe application of graph Transformers in Section D.2."
"LAYERS
MICRO-AUC",0.9401041666666666,"12https://github.com/palash1992/DynamicGEM
13https://github.com/aravindsankar28/DySAT
14https://github.com/IBM/EvolveGCN"
"LAYERS
MICRO-AUC",0.9427083333333334,"Under review as a conference paper at ICLR 2022 ùë£! ùë£"" ùë£# ùë£$"
"LAYERS
MICRO-AUC",0.9453125,"ùë£#
ùë£!
ùë£"" ùë£% ùë£$
ùë£% ùíóùüè ùíóùüê ùíóùüë ùë£$ ùë£% ùíóùüè ùíóùüê ùíóùüë ùë£$"
"LAYERS
MICRO-AUC",0.9479166666666666,"ùë£#
ùë£!
ùë£"" ùë£% ùíóùüí
ùíóùüì ùë£! ùë£"" ùë£# ùíóùüí"
"LAYERS
MICRO-AUC",0.9505208333333334,"ùë£#
ùë£!
ùë£"" ùíóùüì ùíóùüí
ùíóùüì"
"LAYERS
MICRO-AUC",0.953125,"Iteration 1
Iteration 2
Iteration 4"
"LAYERS
MICRO-AUC",0.9557291666666666,Attention computed at current iteration
"LAYERS
MICRO-AUC",0.9583333333333334,"Attention previously computed 
and stored in memory"
"LAYERS
MICRO-AUC",0.9609375,"ùíóùüë
ùíóùüè
ùíóùüê
ùë£$
ùë£% ‚Ä¶"
"LAYERS
MICRO-AUC",0.9635416666666666,"ùíóùíä
Node selected at current iteration"
"LAYERS
MICRO-AUC",0.9661458333333334,Attention needs to be computed
"LAYERS
MICRO-AUC",0.96875,"Figure 7: Suppose we want to compute the pairwise self-attention between nodes {v1, . . . , v5}. We
can Ô¨Årst split all self-attentions into multiple chunks, and iteratively compute the self-attention value
in each chunk. For example, at the Ô¨Årst iteration, we Ô¨Årst compute the self attention between node
{v1, . . . , v3} (in blue) and store it in memory (in yellow). Then, at the second iteration, we compute
the attention between node {v1, . . . , v3} and node {v4, v5}."
"LAYERS
MICRO-AUC",0.9713541666666666,"Table 13: Comparison of different graph Transformers.
Method
GRAPHORMER
GRAPHTRANSFORMER
GRAPHBERT
DGT (ours)
Mini-batch
No
No
Yes
Yes
Graph type
Static
Static
Static
Dynamic
Attention type
Full-attention
1-hop attention
Full-attention
Full-attention"
"LAYERS
MICRO-AUC",0.9739583333333334,"Encoding type
Centrality Encoding
WL-based
Temporal Connection
Spatial Encoding
Laplacian Eigenvector
Intimacy-based
Spatial Distance
Edge Encoding
Hop-based"
"LAYERS
MICRO-AUC",0.9765625,"D.1
POSITIONAL ENCODING TYPES"
"LAYERS
MICRO-AUC",0.9791666666666666,"In the following, we summarize the positional encoding of different methods."
"LAYERS
MICRO-AUC",0.9817708333333334,"(1) GRAPHORMER (Ying et al., 2021):"
"LAYERS
MICRO-AUC",0.984375,"‚Ä¢ Centrality Encoding: the in-degree and out-degree information of each node. Then, they add the
degree information to the original node feature.
‚Ä¢ Spatial Encoding: shortest path distance between two nodes. Then, they add the shortest path
distance as a bias term to the self-attention.
‚Ä¢ Edge Encoding: the summation of all egde features on the shortest path. Then, they add the
shortest path distance as a bias term to the self-attention."
"LAYERS
MICRO-AUC",0.9869791666666666,"(2) GRAPHTRANSFORMER (Dwivedi & Bresson, 2020) proposes to use the Laplacian Eigenvectors.
Then, they add the Eigenvectors to the original node feature."
"LAYERS
MICRO-AUC",0.9895833333333334,"(3) GRAPHBERT (Zhang et al., 2020):"
"LAYERS
MICRO-AUC",0.9921875,"‚Ä¢ Weisfeiler-Lehman Absolute Role Embedding: the node label generated by the Weisfeiler-Lehman
(WL) algorithm according to the structural roles of each node in the graph data. Then, they add
the labelled node information to the original node feature.
‚Ä¢ Intimacy based Relative Positional Embedding: the placement orders of the serialized node
list ordered by the Personalized PageRank score of each node. Then, they add the node order
information to the original node feature.
‚Ä¢ Hop based Relative Distance Embedding: shortest path distance between a node to the center
node of the sampled subgraph. Then, they add the shortest path distance as a bias information to
the original node feature."
"LAYERS
MICRO-AUC",0.9947916666666666,"D.2
GRAPH TRANSFORMER WITH OTHER APPLICATIONS"
"LAYERS
MICRO-AUC",0.9973958333333334,"Transformers are also applied to other graph types or downstream tasks. For example, HGT Hu et al.
(2020) and GTNS Yun et al. (2019) propose a Ô¨Årst-order graph transformer to solve the heterogeneous
graph representation learning problem, TAGGEN Zhou et al. (2020) is working on synthetic graph
generation problems, which are out of the scope of this paper."
