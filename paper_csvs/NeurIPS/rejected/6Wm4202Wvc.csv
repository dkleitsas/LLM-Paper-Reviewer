Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0010438413361169101,"As deep learning models become larger and more expensive, many practitioners
1"
ABSTRACT,0.0020876826722338203,"turn to fine-tuning APIs. These web services allow fine-tuning a model between
2"
ABSTRACT,0.003131524008350731,"two parties: the client that provides the data, and the server that hosts the model.
3"
ABSTRACT,0.0041753653444676405,"While convenient, these APIs raise a new concern: the data of the client is at
4"
ABSTRACT,0.005219206680584551,"risk of privacy breach during the training procedure. This challenge presents
5"
ABSTRACT,0.006263048016701462,"an important practical case of vertical federated learning, where the two parties
6"
ABSTRACT,0.007306889352818371,"perform parameter-efficient fine-tuning (PEFT) of a large model. In this study, we
7"
ABSTRACT,0.008350730688935281,"systematically search for a way to fine-tune models over an API while keeping the
8"
ABSTRACT,0.009394572025052192,"labels private. We analyze the privacy of LoRA, a popular approach for parameter-
9"
ABSTRACT,0.010438413361169102,"efficient fine-tuning when training over an API. Using this analysis, we propose
10"
ABSTRACT,0.011482254697286013,"P3EFT, a multi-party split learning algorithm that takes advantage of existing PEFT
11"
ABSTRACT,0.012526096033402923,"properties to maintain privacy at a lower performance overhead. To validate our
12"
ABSTRACT,0.013569937369519834,"algorithm, we fine-tune DeBERTa-v2-XXLarge, Flan-T5 Large and LLaMA-2 7B
13"
ABSTRACT,0.014613778705636743,"using LoRA adapters on a range of NLP tasks. We find that P3EFT is competitive
14"
ABSTRACT,0.015657620041753653,"with existing privacy-preserving methods in multi-party and two-party setups while
15"
ABSTRACT,0.016701461377870562,"having higher accuracy.
16"
INTRODUCTION,0.017745302713987474,"1
Introduction
17"
INTRODUCTION,0.018789144050104383,"One of the main reasons behind deep learning success is its ability to transfer knowledge between
18"
INTRODUCTION,0.019832985386221295,"tasks [34]. When training a model for any particular problem, it is common to reuse previously
19"
INTRODUCTION,0.020876826722338204,"trained models from other, related problems. In the past, this was typically done by downloading
20"
INTRODUCTION,0.021920668058455117,"pre-trained model weights from public hubs, then fine-tuning the said models on the downstream
21"
INTRODUCTION,0.022964509394572025,"task. However, as models grow larger and more compute-intensive, fine-tuning them locally becomes
22"
INTRODUCTION,0.024008350730688934,"an increasingly difficult task. Furthermore, many recent models are not released, but instead made
23"
INTRODUCTION,0.025052192066805846,"available as proprietary services.
24"
INTRODUCTION,0.026096033402922755,"When a model cannot be fine-tuned locally, many practitioners opt instead for the so-called fine-tuning
25"
INTRODUCTION,0.027139874739039668,"APIs [27, 16, 6, 26]. These APIs are web services that host one or several pre-trained models and
26"
INTRODUCTION,0.028183716075156576,"allow clients to perform limited fine-tuning. More specifically, APIs usually allow their clients to run
27"
INTRODUCTION,0.029227557411273485,"parameter-efficient fine-tuning (PEFT), such as LoRA [15] or Prefix-tuning [21]. These techniques
28"
INTRODUCTION,0.030271398747390398,"allow adapting a model to a dataset while training a relatively small number of additional weights,
29"
INTRODUCTION,0.031315240083507306,"which is particularly important for large language or image generation models that have billions of
30"
INTRODUCTION,0.032359081419624215,"parameters.
31"
INTRODUCTION,0.033402922755741124,"Although the fine-tuning APIs can be convenient, they also introduce new risk in terms of data privacy.
32"
INTRODUCTION,0.03444676409185804,"When a client uses such API to train on sensitive data, they need to ensure that their data will stay
33"
INTRODUCTION,0.03549060542797495,"private [7]. This is particularly important when dealing with patient’s medical records, personal
34"
INTRODUCTION,0.03653444676409186,"user data or trade secrets [24, 19]. The two main threats to data privacy are that the API provider
35"
INTRODUCTION,0.037578288100208766,"obtains the private data and that a third party intercepts data in transit. Therefore, data privacy is
36"
INTRODUCTION,0.038622129436325675,"not guaranteed even if the API provider is trusted. Several recent works propose LLM fine-tuning
37"
INTRODUCTION,0.03966597077244259,"protocols that establish a certain level of privacy for multi-party fine-tuning [42, 7, 22]. Unfortunately,
38"
INTRODUCTION,0.0407098121085595,"these algorithms work for a narrow class of fine-tuning algorithms or assume that a client can run
39"
INTRODUCTION,0.04175365344467641,"LLM training locally using an obfuscated version of the model, provided by a remote server [42].
40"
INTRODUCTION,0.04279749478079332,"As a result, these algorithms are impractical for our use case of fine-tuning over an API. The few
41"
INTRODUCTION,0.04384133611691023,"algorithms that are suitable for API fine-tuning guarantee the privacy of input tokens [22], meaning
42"
INTRODUCTION,0.04488517745302714,"that the attacker can infer private training labels.
43"
INTRODUCTION,0.04592901878914405,"In this work, we seek to alleviate this problem by designing a two-party fine-tuning protocol that
44"
INTRODUCTION,0.04697286012526096,"performs standard parameter-efficient fine-tuning with privacy guarantees. We formulate our protocol
45"
INTRODUCTION,0.04801670146137787,"as a special case of split learning (or vertical federated learning), where one side (server) holds the
46"
INTRODUCTION,0.049060542797494784,"pre-trained model and the other (client) has private training data. More specifically, we focus on the
47"
INTRODUCTION,0.05010438413361169,"privacy of client’s training labels. While input privacy is often crucial, there are scenarios where
48"
INTRODUCTION,0.0511482254697286,"input data is publicly available, such as social media user pages. In these cases, labels could include
49"
INTRODUCTION,0.05219206680584551,"ad clicks (known to the social network) or financial information (known to a bank that matches social
50"
INTRODUCTION,0.05323590814196242,"profiles to its internal records). This example further justifies the use of LLMs, as social media pages
51"
INTRODUCTION,0.054279749478079335,"often contain substantial amounts of text, and LLMs excel at processing long-context data.
52"
INTRODUCTION,0.055323590814196244,"Instead of developing a specific privacy-preserving architecture, we seek algorithms that can work
53"
INTRODUCTION,0.05636743215031315,"with popular existing models and PEFT algorithms. Furthermore, our approach relies on the properties
54"
INTRODUCTION,0.05741127348643006,"of parameter-efficient fine-tuning. Notably, since the adapters are compact, both parties can maintain
55"
INTRODUCTION,0.05845511482254697,"multiple sets of adapters and swap between them with relative ease. This allows us to design a
56"
INTRODUCTION,0.059498956158663886,"PEFT-specific algorithm that can solve its task more effectively than general split learning strategies
57"
INTRODUCTION,0.060542797494780795,"[18].
58"
INTRODUCTION,0.061586638830897704,"We summarize our main contributions as follows:
59"
INTRODUCTION,0.06263048016701461,"• We analyze Low-Rank Adaptation, a common parameter-efficient fine-tuning algorithm,
60"
INTRODUCTION,0.06367432150313153,"from the perspective of label privacy in the split learning setup. We observe that, despite
61"
INTRODUCTION,0.06471816283924843,"fine-tuning less than 0.1% of model parameters, PEFT algorithms leak client’s training
62"
INTRODUCTION,0.06576200417536535,"labels against simple attacks that work for modern pretrained transformers.
63"
INTRODUCTION,0.06680584551148225,"• Based on our analysis, we formulate a framework for privacy-preserving parameter-efficient
64"
INTRODUCTION,0.06784968684759916,"fine-tuning (P3EFT). This framework leverages the properties of PEFT to obfuscate the
65"
INTRODUCTION,0.06889352818371608,"gradients and parameters communicated during fine-tuning with little impact on the fine-
66"
INTRODUCTION,0.06993736951983298,"tuned model quality.
67"
INTRODUCTION,0.0709812108559499,"• To verify the practical viability of P3EFT, we conduct experiments on popular real-world
68"
INTRODUCTION,0.0720250521920668,"PEFT workloads1. Specifically, we fine-tune DeBERTa-v2-XXL [13], Flan-T5-Large [4]
69"
INTRODUCTION,0.07306889352818371,"and LLaMA-2 7B [35] on a set of standard language understanding problems. We find that,
70"
INTRODUCTION,0.07411273486430063,"compared to prior split learning algorithms, P3EFT can maintain label privacy throughout
71"
INTRODUCTION,0.07515657620041753,"training with a significantly smaller accuracy drop.
72"
BACKGROUND,0.07620041753653445,"2
Background
73"
FEDERATED LEARNING AND SPLIT LEARNING,0.07724425887265135,"2.1
Federated learning and split learning
74"
FEDERATED LEARNING AND SPLIT LEARNING,0.07828810020876827,"Privacy preservation in machine learning has been a subject of active study within several frameworks.
75"
FEDERATED LEARNING AND SPLIT LEARNING,0.07933194154488518,"An important branch of privacy-preserving learning methods is federated learning, or FL [24], which
76"
FEDERATED LEARNING AND SPLIT LEARNING,0.08037578288100208,"can be broadly described as an approach allowing several parties to train a model jointly without
77"
FEDERATED LEARNING AND SPLIT LEARNING,0.081419624217119,"sharing their private data. In particular, vertical federated learning [12, 43] targets the scenario where
78"
FEDERATED LEARNING AND SPLIT LEARNING,0.0824634655532359,"different features (including the label) of each training instance are kept by different parties.
79"
FEDERATED LEARNING AND SPLIT LEARNING,0.08350730688935282,"One of the most popular approaches to vertical FL for neural networks is split learning [10, 37],
80"
FEDERATED LEARNING AND SPLIT LEARNING,0.08455114822546973,"where each party stores its part of the overall model. To train the model in such an approach, it is
81"
FEDERATED LEARNING AND SPLIT LEARNING,0.08559498956158663,"only necessary to transfer the intermediate activations and the gradients between layers, while the
82"
FEDERATED LEARNING AND SPLIT LEARNING,0.08663883089770355,"data itself is stored at the premises of the participant hosting each layer. In this work, we focus on
83"
FEDERATED LEARNING AND SPLIT LEARNING,0.08768267223382047,"the two-party formulation of split learning, where one side stores the features for each example and
84"
FEDERATED LEARNING AND SPLIT LEARNING,0.08872651356993737,"another one stores the labels.
85"
FEDERATED LEARNING AND SPLIT LEARNING,0.08977035490605428,1The code is available at github.com/anonymousauthor56/P3EFT
FEDERATED LEARNING AND SPLIT LEARNING,0.09081419624217119,"Recent works have investigated the setting of two-party split learning from the label leakage per-
86"
FEDERATED LEARNING AND SPLIT LEARNING,0.0918580375782881,"spective [38, 28]: because the label party needs to pass the gradients of the loss function to the
87"
FEDERATED LEARNING AND SPLIT LEARNING,0.09290187891440502,"non-label party, it is possible for the latter party to deduce the labels by inspecting the gradients or
88"
FEDERATED LEARNING AND SPLIT LEARNING,0.09394572025052192,"activations or by hijacking the training procedure. Li et al. [18] provide a set of attack methods that
89"
FEDERATED LEARNING AND SPLIT LEARNING,0.09498956158663883,"allow recovering private labels and propose a defense mechanism that injects noise into the gradients;
90"
FEDERATED LEARNING AND SPLIT LEARNING,0.09603340292275574,"however, they test the approach on pretraining smaller models, and we study finetuning large models
91"
FEDERATED LEARNING AND SPLIT LEARNING,0.09707724425887265,"on private downstream data.
92"
PARAMETER-EFFICIENT FINETUNING,0.09812108559498957,"2.2
Parameter-efficient finetuning
93"
PARAMETER-EFFICIENT FINETUNING,0.09916492693110647,"The majority of large neural networks today are not trained with a specific task in mind: instead, they
94"
PARAMETER-EFFICIENT FINETUNING,0.10020876826722339,"are pretrained on a general objective and then adapted for the downstream problem. Importantly, the
95"
PARAMETER-EFFICIENT FINETUNING,0.10125260960334029,"growth in the size of foundation models has led to the increased popularity of parameter-efficient
96"
PARAMETER-EFFICIENT FINETUNING,0.1022964509394572,"finetuning (PEFT) methods that adapt the model to a given task by training a small number of
97"
PARAMETER-EFFICIENT FINETUNING,0.10334029227557412,"task-specific parameters. There are several prominent approaches to parameter-efficient finetuning,
98"
PARAMETER-EFFICIENT FINETUNING,0.10438413361169102,"ranging from trainable prompts [21, 11], to residual adapters [14, 29]. We focus on Low-Rank
99"
PARAMETER-EFFICIENT FINETUNING,0.10542797494780794,"Adaptation (or LoRA, 15), one of the most popular PEFT methods that adds extra parameters to each
100"
PARAMETER-EFFICIENT FINETUNING,0.10647181628392484,"weight matrix in the form of a low-rank factorization (see Appendix C for a more detailed description).
101"
PARAMETER-EFFICIENT FINETUNING,0.10751565762004175,"Such formulation allows LoRA adapters to be merged into the original weights after finetuning; this
102"
PARAMETER-EFFICIENT FINETUNING,0.10855949895615867,"ability, combined with the simplicity of the method, has made LoRA a broadly popular approach in
103"
PARAMETER-EFFICIENT FINETUNING,0.10960334029227557,"multiple domains. Still, the approach we propose can be applied to any PEFT method.
104"
PARAMETER-EFFICIENT FINETUNING,0.11064718162839249,"Several recent lines of work explore the problem of fine-tuning LLMs with privacy guarantees [44, 31].
105"
PARAMETER-EFFICIENT FINETUNING,0.11169102296450939,"Zhao et al. [46] analyze the viability of prompt tuning for federated learning, and Zhang et al. [45], Liu
106"
PARAMETER-EFFICIENT FINETUNING,0.1127348643006263,"et al. [23] study PEFT algorithms in the setting of horizontal federated learning, that is, where multiple
107"
PARAMETER-EFFICIENT FINETUNING,0.11377870563674322,"users train a shared model on their local private data. Another, more relevant research direction
108"
PARAMETER-EFFICIENT FINETUNING,0.11482254697286012,"considers private fine-tuning in a vertical federated learning scenario, where participants hold different
109"
PARAMETER-EFFICIENT FINETUNING,0.11586638830897704,"model layers [22, 40]. Most of these studies leverage the idea of differential privacy to prove an
110"
PARAMETER-EFFICIENT FINETUNING,0.11691022964509394,"upper bound on how much information is leaked [8]. Unfortunately, these upper bounds are typically
111"
PARAMETER-EFFICIENT FINETUNING,0.11795407098121086,"loose and do not match practical observations for real models. Furthermore, the majority of these
112"
PARAMETER-EFFICIENT FINETUNING,0.11899791231732777,"studies only guarantees privacy of specific parts of the training procedure: for instance, Li et al.
113"
PARAMETER-EFFICIENT FINETUNING,0.12004175365344467,"[22] only protects the input features, and not labels or model parameters. Finally, Xiao et al. [42]
114"
PARAMETER-EFFICIENT FINETUNING,0.12108559498956159,"presents an alternative algorithm that protects client data by running the entire fine-tuning on client
115"
PARAMETER-EFFICIENT FINETUNING,0.12212943632567849,"side by emulating the server-side model layers. While this approach is more holistic, it assumes that
116"
PARAMETER-EFFICIENT FINETUNING,0.12317327766179541,"clients can run fine-tuning locally, which makes it impractical for many real-world users of LLM
117"
PARAMETER-EFFICIENT FINETUNING,0.12421711899791232,"fine-tuning APIs. The primary distinction between our work and these studies is that we investigate
118"
PARAMETER-EFFICIENT FINETUNING,0.12526096033402923,"parameter-efficient adaptation in the setting of split learning: we aim to finetune a model without
119"
PARAMETER-EFFICIENT FINETUNING,0.12630480167014613,"disclosing the labels of examples to the model provider.
120"
PRIVACY-PRESERVING PARAMETER-EFFICIENT FINE-TUNING,0.12734864300626306,"3
Privacy-preserving parameter-efficient fine-tuning
121"
PRIVACY-PRESERVING PARAMETER-EFFICIENT FINE-TUNING,0.12839248434237996,"In this section, we analyze the privacy of parameter-efficient fine-tuning and propose a protocol for
122"
PRIVACY-PRESERVING PARAMETER-EFFICIENT FINE-TUNING,0.12943632567849686,"two-party parameter-efficient fine-tuning with the desired privacy guarantees. We begin by analyzing
123"
PRIVACY-PRESERVING PARAMETER-EFFICIENT FINE-TUNING,0.1304801670146138,"the privacy of API fine-tuning with popular PEFT algorithms in Sections 3.1 and 3.2. Then, in
124"
PRIVACY-PRESERVING PARAMETER-EFFICIENT FINE-TUNING,0.1315240083507307,"Section 3.3, we formulate a protocol for privately computing gradients over fine-tuning APIs. Finally,
125"
PRIVACY-PRESERVING PARAMETER-EFFICIENT FINE-TUNING,0.1325678496868476,"we formulate the full P3EFT protocol in Section 3.4.
126"
SETUP,0.1336116910229645,"3.1
Setup
127"
SETUP,0.13465553235908143,"To analyze the privacy of API fine-tuning, we first need to formulate a common framework for
128"
SETUP,0.13569937369519833,"this type of APIs and develop private learning protocols. This step is important, because existing
129"
SETUP,0.13674321503131523,"fine-tuning APIs greatly vary in what they offer to the client: from closed APIs that require users to
130"
SETUP,0.13778705636743216,"submit their full training data [27] to more flexible APIs where clients can run individual training
131"
SETUP,0.13883089770354906,"steps [20, 2, 30]. Similarly to most existing works on split learning, we focus on the latter type of
132"
SETUP,0.13987473903966596,"APIs that allows clients to run individual forward and backward passes over a remote model. Thus,
133"
SETUP,0.1409185803757829,"a client can use these APIs to obtain the training gradients for their PEFT adapters, then update
134"
SETUP,0.1419624217118998,"adapters locally with any optimization method. In our work, we adopt this archetype of fine-tuning
135"
SETUP,0.1430062630480167,"API as it offers sufficient flexibility to develop privacy-preserving algorithms.
136"
SETUP,0.1440501043841336,"We formulate fine-tuning over an API for two or more parties: a client, and one or several servers.
137"
SETUP,0.14509394572025053,"The client owns a training dataset with inputs X and labels Y . In turn, each server has the same
138"
SETUP,0.14613778705636743,"pre-trained model h(xi, θ) ∈Rd. Note that the parameters θ denote not the pre-trained model
139"
SETUP,0.14718162839248433,"Step: 0
Step: 1000
Step: 4000
Step: 16000"
SETUP,0.14822546972860126,"Step: 0
Step: 1000
Step: 4000
Step: 16000"
SETUP,0.14926931106471816,"Figure 1: A visualization of top-2 principal components of gradients (top) and activations (bottom)
from different fine-tuning steps (left to right). Color indicates the training labels (binary)."
SETUP,0.15031315240083507,"weights, but the trainable adapter weights for a certain PEFT algorithm. A model can encode an input
140"
SETUP,0.151356993736952,"xi ∈X and produce a d-dimensional vector of activations that depend on the learned adapter weights
141"
SETUP,0.1524008350730689,"θ.
142"
SETUP,0.1534446764091858,"To allow fine-tuning, a server offers two API methods:
143"
SETUP,0.1544885177453027,"1. forward(x, θ) →h(x, θ) that computes model activations on input x using adapter weights
144"
SETUP,0.15553235908141963,"θ;
145"
SETUP,0.15657620041753653,"2. backprop(x, θ, gh) →gθ that receives gradients of an arbitrary loss function w.r.t. model
146"
SETUP,0.15762004175365343,"activations gh =
∂L(h(x,θ))"
SETUP,0.15866388308977036,"∂h(x,θ)
and returns the gradients w.r.t. adapter parameters, gθ =
147"
SETUP,0.15970772442588727,"∂L(h(x,θ))"
SETUP,0.16075156576200417,"∂θ
.
148"
SETUP,0.1617954070981211,"We further assume that both forward(·) and backprop(·) APIs are stateless and deterministic, i.e.
149"
SETUP,0.162839248434238,"calling the same API method multiple times (or on multiple servers) with the same inputs produces
150"
SETUP,0.1638830897703549,"identical results. Thus, if the model uses dropout or any other form of non-determinism, we assume
151"
SETUP,0.1649269311064718,"that clients provide the random seed as a part of x.
152"
SETUP,0.16597077244258873,"To fine-tune a model with this API, a client can initialize adapters locally, alongside with a small
153"
SETUP,0.16701461377870563,"task-specific head2, then train both adapters and the head. For each training batch (x, y) ∈D, a client
154"
SETUP,0.16805845511482254,"calls forward(x, θ) to compute feature representations, then predicts with local “head” and computes
155"
SETUP,0.16910229645093947,"task-specific loss function L. After that, a client performs backward pass: first, it computes gradients
156"
SETUP,0.17014613778705637,w.r.t. local head inputs gh = ∂L
SETUP,0.17118997912317327,"∂h , then passes those gradients to a remote server via backprop(x, θ, gh)
157"
SETUP,0.1722338204592902,API call to compute gradients w.r.t. ∂L
SETUP,0.1732776617954071,"∂θ . Finally, a client updates both θ and local “head” parameters
158"
SETUP,0.174321503131524,"using the optimizer of choice.
159"
SETUP,0.17536534446764093,"Before building more advanced algorithms, let us analyze the privacy of client’s labels under standard
160"
SETUP,0.17640918580375783,"fine-tuning. We consider an “honest, but curious” attacker model. This means that the server will
161"
SETUP,0.17745302713987474,"faithfully run the forward and backprop computations as requested by the client without changing
162"
SETUP,0.17849686847599164,"the results. Furthermore, we assume that servers are independent and do not communicate client’s
163"
SETUP,0.17954070981210857,"data between each other. However, a server can recover client’s labels by performing arbitrary
164"
SETUP,0.18058455114822547,"computations using any information it receives from the client. When training in this way, a client
165"
SETUP,0.18162839248434237,"does not directly communicate training labels to the server. However, it communicates inputs, adapter
166"
SETUP,0.1826722338204593,"parameters, and gradients. Furthermore, the server communicates input representations that can be
167"
SETUP,0.1837160751565762,"intercepted by a third party.
168"
LABEL LEAKAGE OF STANDARD SPLIT LEARNING,0.1847599164926931,"3.2
Label Leakage of Standard Split Learning
169"
LABEL LEAKAGE OF STANDARD SPLIT LEARNING,0.18580375782881003,"In Figure 1, we train a DeBERTa-v2-XXL model on the SST-2 [32] sentiment classification dataset.
170"
LABEL LEAKAGE OF STANDARD SPLIT LEARNING,0.18684759916492694,"The top row depicts the gradients gh communicated by the client when calling backprop(·) at different
171"
LABEL LEAKAGE OF STANDARD SPLIT LEARNING,0.18789144050104384,"training stages. In the bottom row, we similarly track activations h(x, θ) that server may compute
172"
LABEL LEAKAGE OF STANDARD SPLIT LEARNING,0.18893528183716074,"based on the specified x, θ. We defer further additional figures and details to Section 4.1.
173"
LABEL LEAKAGE OF STANDARD SPLIT LEARNING,0.18997912317327767,"As we can see, both gradients and activations are arranged in such a way that simple k-means
174"
LABEL LEAKAGE OF STANDARD SPLIT LEARNING,0.19102296450939457,"clustering would reveal which objects have the same label. The training activations (bottom row) do
175"
LABEL LEAKAGE OF STANDARD SPLIT LEARNING,0.19206680584551147,2A linear layer that predicts class logits or regression target.
LABEL LEAKAGE OF STANDARD SPLIT LEARNING,0.1931106471816284,"SERVER 
CLIENT"
LABEL LEAKAGE OF STANDARD SPLIT LEARNING,0.1941544885177453,"n model instances with unique LoRA weights 𝜃 
activations 
weights 
weighted activations 
model head"
LABEL LEAKAGE OF STANDARD SPLIT LEARNING,0.1951983298538622,local layers
LABEL LEAKAGE OF STANDARD SPLIT LEARNING,0.19624217118997914,"h1 
W1"
LABEL LEAKAGE OF STANDARD SPLIT LEARNING,0.19728601252609604,"h2 
W2"
LABEL LEAKAGE OF STANDARD SPLIT LEARNING,0.19832985386221294,"hn 
Wn"
LABEL LEAKAGE OF STANDARD SPLIT LEARNING,0.19937369519832984,"𝜃1 
→ 
→
 → 
 ⊙ 
="
LABEL LEAKAGE OF STANDARD SPLIT LEARNING,0.20041753653444677,"𝜃2
→
→
 → 
 ⊙ 
="
LABEL LEAKAGE OF STANDARD SPLIT LEARNING,0.20146137787056367,"𝜃n
→
→
→
⊙ 𝚺= h’ 𝓛"
LABEL LEAKAGE OF STANDARD SPLIT LEARNING,0.20250521920668058,"… 
… 
 … 
 … 
 … 
 …"
LABEL LEAKAGE OF STANDARD SPLIT LEARNING,0.2035490605427975,forward
LABEL LEAKAGE OF STANDARD SPLIT LEARNING,0.2045929018789144,"last layer
activations
from n models"
LABEL LEAKAGE OF STANDARD SPLIT LEARNING,0.2056367432150313,backward
LABEL LEAKAGE OF STANDARD SPLIT LEARNING,0.20668058455114824,"weighted
gradients
to n models …"
LABEL LEAKAGE OF STANDARD SPLIT LEARNING,0.20772442588726514,Figure 2: An intuitive illustration of the proposed fine-tuning protocol.
LABEL LEAKAGE OF STANDARD SPLIT LEARNING,0.20876826722338204,"not reveal labels right away (at least not against this attack). However, they gradually “leak” private
176"
LABEL LEAKAGE OF STANDARD SPLIT LEARNING,0.20981210855949894,"label information during training. Informally, it appears that the training gradients gradually pull
177"
LABEL LEAKAGE OF STANDARD SPLIT LEARNING,0.21085594989561587,"apart the feature representations for each label, until eventually they turn into separate clusters. From
178"
LABEL LEAKAGE OF STANDARD SPLIT LEARNING,0.21189979123173278,"an information-theoretic perspective, knowing just one vector of gradients or trained activations
179"
LABEL LEAKAGE OF STANDARD SPLIT LEARNING,0.21294363256784968,"allows the attacker to learn all but one bit3 of information about client’s private labels.
180"
LABEL LEAKAGE OF STANDARD SPLIT LEARNING,0.2139874739039666,"To summarize, leaving any one data source unprotected (gradients, activations or parameters) would
181"
LABEL LEAKAGE OF STANDARD SPLIT LEARNING,0.2150313152400835,"already compromise label privacy. However, we found that gradients and activations require different
182"
LABEL LEAKAGE OF STANDARD SPLIT LEARNING,0.2160751565762004,"means of protection.
183"
PRIVACY-PRESERVING BACKPROPAGATION,0.21711899791231734,"3.3
Privacy-preserving backpropagation
184"
PRIVACY-PRESERVING BACKPROPAGATION,0.21816283924843424,"In this section, we formulate an algorithm for “anonymizing” the gradients communicated over a
185"
PRIVACY-PRESERVING BACKPROPAGATION,0.21920668058455114,"single training step with arbitrary PEFT type. Several prior works approach this by modifying the
186"
PRIVACY-PRESERVING BACKPROPAGATION,0.22025052192066805,"training objective or model architecture. However, when dealing with a real-world PEFT workload
187"
PRIVACY-PRESERVING BACKPROPAGATION,0.22129436325678498,"with optimized hyperparameters, changing the model or loss function often results in reduced model
188"
PRIVACY-PRESERVING BACKPROPAGATION,0.22233820459290188,"accuracy4. Thus, we seek an algorithm that preserves both model and training objective.
189"
PRIVACY-PRESERVING BACKPROPAGATION,0.22338204592901878,"We design our algorithm based on an observation that backpropagation is conditionally lin-
190"
PRIVACY-PRESERVING BACKPROPAGATION,0.2244258872651357,"ear in output gradients, even when the model itself is nonlinear. Formally, if we take a model
191"
PRIVACY-PRESERVING BACKPROPAGATION,0.2254697286012526,"h(·, ·), a fixed set of trainable parameters θ and input samples x, the backprop function5 computes
192"
PRIVACY-PRESERVING BACKPROPAGATION,0.2265135699373695,"backprop(x, θ,
∂L
∂h(x,θ)) = ∂L"
PRIVACY-PRESERVING BACKPROPAGATION,0.22755741127348644,"∂θ . For convenience, we shorten it to backprop(x, θ, gh) = gθ, where
193"
PRIVACY-PRESERVING BACKPROPAGATION,0.22860125260960334,"gh =
∂L
∂h(x,θ) represents the gradients of some objective function with respect to model activations
194"
PRIVACY-PRESERVING BACKPROPAGATION,0.22964509394572025,"(outputs), and gθ = ∂L"
PRIVACY-PRESERVING BACKPROPAGATION,0.23068893528183715,"∂θ are gradients of the same objective function w.r.t. trainable parameters. In
195"
PRIVACY-PRESERVING BACKPROPAGATION,0.23173277661795408,"this notation, backprop is linear in terms of gh for any fixed x, θ.
196"
PRIVACY-PRESERVING BACKPROPAGATION,0.23277661795407098,"This becomes self-evident if we view backprop as multiplying gh by the Jacobian of model outputs
197"
PRIVACY-PRESERVING BACKPROPAGATION,0.23382045929018788,"w.r.t. trainable parameters, ∂h(x,θ)"
PRIVACY-PRESERVING BACKPROPAGATION,0.2348643006263048,"∂θ
. If x, θ are constant, the Jacobian is also constant, and backprop
198"
PRIVACY-PRESERVING BACKPROPAGATION,0.2359081419624217,"is a linear operator:
199"
PRIVACY-PRESERVING BACKPROPAGATION,0.23695198329853862,"backprop(x, θ,
∂L
∂h(x, θ)) = ∂L"
PRIVACY-PRESERVING BACKPROPAGATION,0.23799582463465555,"∂θ =
∂L
∂h(x, θ) × ∂h(x, θ)"
PRIVACY-PRESERVING BACKPROPAGATION,0.23903966597077245,"∂θ
.
(1)"
PRIVACY-PRESERVING BACKPROPAGATION,0.24008350730688935,"This observation allows us to design a private backpropagation protocol.
To illustrate
200"
PRIVACY-PRESERVING BACKPROPAGATION,0.24112734864300625,"this protocol, let us first consider a distributed API with two identical independent servers
201"
PRIVACY-PRESERVING BACKPROPAGATION,0.24217118997912318,"that offer backprop API. Then, for arbitrary vector z, we can rewrite backprop(x, θ, gh) as
202"
PRIVACY-PRESERVING BACKPROPAGATION,0.24321503131524008,"backprop(x, θ, gh+z)+backprop(x, θ, gh−z).
203"
PRIVACY-PRESERVING BACKPROPAGATION,0.24425887265135698,"During API fine-tuning, we obtain backprop(x, θ, gh + z) using an API call to server 1, whereas the
204"
PRIVACY-PRESERVING BACKPROPAGATION,0.2453027139874739,"second term backprop(x, θ, gh −z) translates to an API call to server 2. Note that neither of two
205"
PRIVACY-PRESERVING BACKPROPAGATION,0.24634655532359082,"servers has access to the true gradient gh: they only receive the sum [gh + z]. If we sample a large
206"
PRIVACY-PRESERVING BACKPROPAGATION,0.24739039665970772,"noise vector z (Var(z) ≫∥gh∥2
2), this sum also becomes dominated by noise. However, when both
207"
PRIVACY-PRESERVING BACKPROPAGATION,0.24843423799582465,"API calls finish, a client can sum the results to recover the true gradient of the loss with respect to
208"
PRIVACY-PRESERVING BACKPROPAGATION,0.24947807933194155,"parameters.
209"
PRIVACY-PRESERVING BACKPROPAGATION,0.25052192066805845,"If both requests are processed by the same server, it can obviously recover gh by adding up gradients
210"
PRIVACY-PRESERVING BACKPROPAGATION,0.25156576200417535,"from both calls, which leads us to the final step. Instead of generating a single noise vector, a client
211"
PRIVACY-PRESERVING BACKPROPAGATION,0.25260960334029225,"3The missing bit corresponds to attacker not knowing which cluster corresponds to label “1”.
4We validate this empirically in 4.2.
5This is the same as the backprop API defined in Section 3.1."
PRIVACY-PRESERVING BACKPROPAGATION,0.2536534446764092,"needs to generate (privately) a set of m > 1 random vectors ˆg1
h, . . . , ˆgm
h and scalars α1, . . . , αm such
212"
PRIVACY-PRESERVING BACKPROPAGATION,0.2546972860125261,"that
213 gh = m
X"
PRIVACY-PRESERVING BACKPROPAGATION,0.255741127348643,"i=1
αi · ˆgi
h.
(2)"
PRIVACY-PRESERVING BACKPROPAGATION,0.2567849686847599,"Then, for each ˆgi
h, client computes backprop(x, θ, ˆgi
h) as m parallel API calls. Once this is done,
214"
PRIVACY-PRESERVING BACKPROPAGATION,0.2578288100208768,"client recovers
215 gθ = m
X"
PRIVACY-PRESERVING BACKPROPAGATION,0.2588726513569937,"i=1
αi · backprop(x, θ, ˆgi
h).
(3)"
PRIVACY-PRESERVING BACKPROPAGATION,0.2599164926931106,"Note that the client does not reveal α1, . . . , αm to anyone.
216"
PRIVACY-PRESERVING BACKPROPAGATION,0.2609603340292276,"The resulting procedure is formulated in Algorithm 1. This algorithm is conceptually similar to
217"
PRIVACY-PRESERVING BACKPROPAGATION,0.2620041753653445,"the secure aggregation protocol for conventional (horizontal) federated learning [1]. This protocol
218"
PRIVACY-PRESERVING BACKPROPAGATION,0.2630480167014614,"allows clients to average their local vector with peers while keeping each individual vector provably
219"
PRIVACY-PRESERVING BACKPROPAGATION,0.2640918580375783,"private. Similarly to our scheme, clients perturb the vector in such a way that the average of perturbed
220"
PRIVACY-PRESERVING BACKPROPAGATION,0.2651356993736952,"vectors remains the same. Unlike Bonawitz et al. [1], our protocol privately backpropagates through
221"
PRIVACY-PRESERVING BACKPROPAGATION,0.2661795407098121,"a server-hosted model by leveraging the conditional linearity of the backpropagation operator.
222"
PRIVACY-PRESERVING BACKPROPAGATION,0.267223382045929,Algorithm 1 private_backprop — Privacy-Preserving Backpropagation (from the client’s perspective)
PRIVACY-PRESERVING BACKPROPAGATION,0.26826722338204595,"1: Input: x inputs, θ adapter weights, gh gradients w.r.t. activations, m > 1 - number of passes
2: ˆg1
h, . . . , ˆgm
h , α1, . . . , αm = obfuscate(gh, m)
▷2
3: for j = 1, . . . , m do
4:
ˆgj
θ = backprop(x, θ, ˆgj
h)
▷computed by server
5: end for
6: gθ = Pm
j=1 αj · ˆgj
θ
7: Return: gθ"
PRIVACY-PRESERVING BACKPROPAGATION,0.26931106471816285,"The private backpropagation algorithm can allow client to safely compute gradients once, but, in
223"
PRIVACY-PRESERVING BACKPROPAGATION,0.27035490605427975,"practice, client usually needs to run many consecutive steps. This creates an additional vector of
224"
PRIVACY-PRESERVING BACKPROPAGATION,0.27139874739039666,"attack: if the same server receives two sets of parameters θt, θt+1 , they could potentially recover gθ
225"
PRIVACY-PRESERVING BACKPROPAGATION,0.27244258872651356,"by inverting the optimizer.
226"
PRIVACY-PRESERVING BACKPROPAGATION,0.27348643006263046,"In the simplest case, if the server somehow knows that the client computes θt+1 = θt −η · gθ, then
227"
PRIVACY-PRESERVING BACKPROPAGATION,0.2745302713987474,"they can compute gθ = (θt −θt+1)/η. While gθ does not necessarily leak private labels, a server
228"
PRIVACY-PRESERVING BACKPROPAGATION,0.2755741127348643,"could, in some cases, use gθ to recover gh, either fully (e.g. if Jacobian is invertible), or partially.
229"
PRIVACY-PRESERVING BACKPROPAGATION,0.2766179540709812,"The client has two ways to prevent this attack. The first one is to ensure that no single server runs
230"
PRIVACY-PRESERVING BACKPROPAGATION,0.2776617954070981,"backprop on two consecutive steps. This is easy to do in decentralized systems where there are
231"
PRIVACY-PRESERVING BACKPROPAGATION,0.278705636743215,"many potential servers. However, even when there is a single server, they could be required to set up
232"
PRIVACY-PRESERVING BACKPROPAGATION,0.2797494780793319,"multiple trusted execution environments [25]. A more risky alternative is to ensure that the gradients
233"
PRIVACY-PRESERVING BACKPROPAGATION,0.2807933194154488,"cannot be reversed from consecutive parameters: randomize initial optimizer statistics or add noise to
234"
PRIVACY-PRESERVING BACKPROPAGATION,0.2818371607515658,"parameters. This solution is easier, but it can slow down training in some cases.
235"
PRIVACY-PRESERVING BACKPROPAGATION,0.2828810020876827,"To summarize, we formulated a procedure that allows a client to compute gradients privately for any
236"
PRIVACY-PRESERVING BACKPROPAGATION,0.2839248434237996,"given model and PEFT type. Furthermore, since Equation 3 recovers true gradients, this obfuscation
237"
PRIVACY-PRESERVING BACKPROPAGATION,0.2849686847599165,"method does not affect the training dynamics. However, as we have shown in Section 3.1, gradients
238"
PRIVACY-PRESERVING BACKPROPAGATION,0.2860125260960334,"are not the only source of privacy leakage.
239"
FULL FINE-TUNING,0.2870563674321503,"3.4
Full fine-tuning
240"
FULL FINE-TUNING,0.2881002087682672,"The other major attack vector are training activations. As the model fits to training data, it’s
241"
FULL FINE-TUNING,0.28914405010438415,"intermediate activations h(x, θ) allow attackers to recover labels, e.g. by clustering (see Figure 1).
242"
FULL FINE-TUNING,0.29018789144050106,"To combat this issue, we take advantage of the fact that PEFT has few trainable parameters. Instead
243"
FULL FINE-TUNING,0.29123173277661796,"of learning just one set of trainable parameters, a client creates n independent adapter sets θ1, ..., θn.
244"
FULL FINE-TUNING,0.29227557411273486,"Note that this does not require n unique servers: a single server can run multiple sets of adapters.
245"
FULL FINE-TUNING,0.29331941544885176,"Furthermore, a client can alternate between using different servers for the same adapters. During
246"
FULL FINE-TUNING,0.29436325678496866,"forward pass, the outputs of different adapters are mixed together using randomized mixing weights
247"
FULL FINE-TUNING,0.2954070981210856,"W ∈Rn,d:
248"
FULL FINE-TUNING,0.2964509394572025,"h′(x, θ1, . . . , θn) = n
X"
FULL FINE-TUNING,0.2974947807933194,"i=1
Wi ⊙h(x, θi)
(4)"
FULL FINE-TUNING,0.2985386221294363,"249
Overall, we design this model in such a way the combined model h′ can predict the labels, but the
250"
FULL FINE-TUNING,0.29958246346555323,"adapters h(x, θi) do not allow predicting these labels without knowing the mixing weights W. The
251"
FULL FINE-TUNING,0.30062630480167013,"mixing weights are generated such that initial activations h′(x, . . . ) are equal to mean h(x, ·) for all
252"
FULL FINE-TUNING,0.30167014613778703,"x. To achieve this, we generate W as follows: first, we generate n · (n −1)/2 d-dimensional random
253"
FULL FINE-TUNING,0.302713987473904,"vectors ξi,j ∈Rd∀i ∈[1, n], j ∈[i + 1, n]. Then, we add them up in the following way:
254 W =  

"
FULL FINE-TUNING,0.3037578288100209,"1
ne + ξ1,2 + ξ1,3 + · · · + ξ1,n
−ξ1,2 + 1"
FULL FINE-TUNING,0.3048016701461378,"ne + ξ2,3 + · · · + ξ2,n
. . .
−ξ1,n −ξ2,n −ξ3,n −· · · + 1 ne "
FULL FINE-TUNING,0.3058455114822547,"


(5)"
FULL FINE-TUNING,0.3068893528183716,"Here, e stands for a vector of all ones. The purpose of these mixing weights is to ensure that the
255"
FULL FINE-TUNING,0.3079331941544885,"gradients w.r.t. individual h(x, θi) are obfuscated, but the averaged model behaves the same as
256"
FULL FINE-TUNING,0.3089770354906054,"regular PEFT adapter. To illustrate this, consider n=2 identical LoRA adapters θ1, θ2. During the
257"
FULL FINE-TUNING,0.31002087682672236,"first training step h(x, θ1) = h(x, θ2). Therefore,
258"
FULL FINE-TUNING,0.31106471816283926,"h′(x, θ1, . . . , θn) = (1/2e + ξ1,2) ⊙h(x, θ1) + (1/2e −ξ1,2) ⊙h(x, θ2) = h(x, θ1)
(6)"
FULL FINE-TUNING,0.31210855949895616,"However, the two adapters will learn different functions as they receive different gradients. From the
259"
FULL FINE-TUNING,0.31315240083507306,"first update on, h′ will be equal to an average of adapter predictions.
260"
FULL FINE-TUNING,0.31419624217118997,"Finally, to ensure that individual adapters h(x, θ) do not accidentally “learn to leak” labels, we
261"
FULL FINE-TUNING,0.31524008350730687,"maintain this over the course of training with a privacy regularizer inspired by [9]. This ensures that
262"
FULL FINE-TUNING,0.3162839248434238,"it is impossible to predict labels from individual adapters h(x, θi). Intuitively, on each training step,
263"
FULL FINE-TUNING,0.3173277661795407,"client fits n linear “heads” that learn to predict labels y from h(x, θi), then performs an adversarial
264"
FULL FINE-TUNING,0.31837160751565763,"update of θi to prevent the “head” from predicting y. Formally, each of n “heads” minimize the same
265"
FULL FINE-TUNING,0.31941544885177453,"objective function as the full model. For instance, if the full model solves multi-class classification,
266"
FULL FINE-TUNING,0.32045929018789143,"each head is trained to minimize cross-entropy:
267"
FULL FINE-TUNING,0.32150313152400833,"η∗
i = arg min
ηi X"
FULL FINE-TUNING,0.32254697286012524,"x,y∈D
−y · log
e⟨ηij,h(x,θi)⟩
P"
FULL FINE-TUNING,0.3235908141962422,"k e⟨ηik,h(x,θi)⟩,
(7) 268"
FULL FINE-TUNING,0.3246346555323591,"where y is one-hot encoding of the correct class.
269"
FULL FINE-TUNING,0.325678496868476,"The whole adversarial update takes place locally on client’s side, using the same h(x, θ) it uses for the
270"
FULL FINE-TUNING,0.3267223382045929,"main training objective. The resulting procedure appears complicated but it typically takes negligible
271"
FULL FINE-TUNING,0.3277661795407098,"time compared to running the large pre-trainied model h(x, θ). Furthermore, since adversarial “heads”
272"
FULL FINE-TUNING,0.3288100208768267,"are linear, minimizing the objective above is done with standard logistic regression solver.
273"
FULL FINE-TUNING,0.3298538622129436,"To summarize, our approach combines the two proposed ideas: we use the private backpropagation
274"
FULL FINE-TUNING,0.33089770354906056,"algorithm from Section 3.3 to protect the gradients, then trains a mixture of adapters in such a way
275"
FULL FINE-TUNING,0.33194154488517746,"that obfuscates learned activatons leaking labels. The resulting procedure is described in Algorithm 2.
276"
FULL FINE-TUNING,0.33298538622129437,"In the next section, we will evaluate the efficacy of P3EFT on popular NLP benchmarks.
277"
EXPERIMENTS,0.33402922755741127,"4
Experiments
278"
EXPERIMENTS,0.33507306889352817,"The main goal of our study is to find a practical method of private fine-tuning that would scale to
279"
EXPERIMENTS,0.33611691022964507,"large models. Because our approach leverages parameter-efficient fine-tuning techniques, we evaluate
280"
EXPERIMENTS,0.33716075156576203,"P3EFT with fine-tuning Transformer models on popular NLP benchmarks that these techniques were
281"
EXPERIMENTS,0.33820459290187893,"designed for.
282"
EXPERIMENTS,0.33924843423799583,"To that end, we chose three pre-trained models: DeBERTa-XXLarge [13], Flan-T5-Large [4] and
283"
EXPERIMENTS,0.34029227557411273,"LLaMA-2 7B [35]. We train these models on several datasets from the GLUE benchmark [39]:
284"
EXPERIMENTS,0.34133611691022964,"SST-2 [32], MNLI [41] and QNLI.
285"
PRIVACY OF GRADIENTS AND ACTIVATIONS,0.34237995824634654,"4.1
Privacy of gradients and activations
286"
PRIVACY OF GRADIENTS AND ACTIVATIONS,0.34342379958246344,"For this experiment, we train DeBERTa-XXLarge on SST-2 dataset using LoRA adapters with
287"
PRIVACY OF GRADIENTS AND ACTIVATIONS,0.3444676409185804,"hyperparameters from [15]. First, we train the model locally and track model activations h and
288"
PRIVACY OF GRADIENTS AND ACTIVATIONS,0.3455114822546973,"gradients w.r.t. those activations. We apply principal component analysis to them and plot the first
289"
PRIVACY OF GRADIENTS AND ACTIVATIONS,0.3465553235908142,"Step: 0
Step: 1000
Step: 4000
Step: 16000"
PRIVACY OF GRADIENTS AND ACTIVATIONS,0.3475991649269311,"Step: 0
Step: 1000
Step: 4000
Step: 16000"
PRIVACY OF GRADIENTS AND ACTIVATIONS,0.348643006263048,"Figure 3: Gradients of cross-entropy w.r.t. LoRA parameters for DeBERTa-v2-XXLarge. The top
row corresponds to normal backpropagation and the bottom row uses privacy-preserving backprop."
PRIVACY OF GRADIENTS AND ACTIVATIONS,0.3496868475991649,"2 dimensions in Figure 1. Similarly, we visualize gradients of individual per-sample loss functions
290"
PRIVACY OF GRADIENTS AND ACTIVATIONS,0.35073068893528186,"w.r.t. LoRA parameters θ in Figure 3 (top row). The results suggest that a hypothetical attacker could
291"
PRIVACY OF GRADIENTS AND ACTIVATIONS,0.35177453027139877,"easily recover private labels by performing K-Means clustering over any data source: activations,
292"
PRIVACY OF GRADIENTS AND ACTIVATIONS,0.35281837160751567,"gradients with respect to activations, or individual gradients with respect to parameters.
293"
PRIVACY OF GRADIENTS AND ACTIVATIONS,0.35386221294363257,"Next, we run the same experiment using privacy-preserving backpropagation as defined in Section 3.3.
294"
PRIVACY OF GRADIENTS AND ACTIVATIONS,0.35490605427974947,"We use n = 2 with the noise variance set to 1000. As expected, we observed the same learning curve
295"
PRIVACY OF GRADIENTS AND ACTIVATIONS,0.3559498956158664,"as with normal training. However, instead of sending gradients w.r.t. activations to the server, a
296"
PRIVACY OF GRADIENTS AND ACTIVATIONS,0.3569937369519833,"client uses specially crafted random noise vectors that are not informative. In Figure 3 (bottom) we
297"
PRIVACY OF GRADIENTS AND ACTIVATIONS,0.35803757828810023,"plot the same kind of individual gradients as in the top row, except that we visualize the gradients
298"
PRIVACY OF GRADIENTS AND ACTIVATIONS,0.35908141962421714,"computed by the first of the two servers. Finally, we train XGBoost [3] with default hyperparameters
299"
PRIVACY OF GRADIENTS AND ACTIVATIONS,0.36012526096033404,"to predict labels given the noisy gradients (pre-PCA): the resulting classifier is able to fit the training
300"
PRIVACY OF GRADIENTS AND ACTIVATIONS,0.36116910229645094,"data perfectly, but has at most 50.4% accuracy on a balanced test set.
301"
MAIN FINE-TUNING EXPERIMENTS,0.36221294363256784,"4.2
Main fine-tuning experiments
302"
MAIN FINE-TUNING EXPERIMENTS,0.36325678496868474,"Next, we evaluated the entire P3EFT algorithm. To control tasks and model type, we examined
303"
MAIN FINE-TUNING EXPERIMENTS,0.36430062630480164,"DeBERTa and Flan-T5 across all four datasets mentioned above, in addition to evaluating LLaMA on
304"
MAIN FINE-TUNING EXPERIMENTS,0.3653444676409186,"SST2 and QNLI datasets. For each setup, we compare against three baselines:
305"
MAIN FINE-TUNING EXPERIMENTS,0.3663883089770355,"• Without LoRAs. In this baseline, the client gathers h activations at the beginning (with no
306"
MAIN FINE-TUNING EXPERIMENTS,0.3674321503131524,"adapters), then proceeds to train local “head” layers using these activations. This method cannot
307"
MAIN FINE-TUNING EXPERIMENTS,0.3684759916492693,"leak information about training labels except for what is stored in X.
308"
MAIN FINE-TUNING EXPERIMENTS,0.3695198329853862,"• Regular fine-tuning (Regular FT) refers to training a single LoRA adapter normally. This baseline
309"
MAIN FINE-TUNING EXPERIMENTS,0.3705636743215031,"represents an upper bound on model accuracy, but lacks privacy.
310"
MAIN FINE-TUNING EXPERIMENTS,0.37160751565762007,"• Distance Correlation (DC). Our re-implementation of the distance correlation defense formulated
311"
MAIN FINE-TUNING EXPERIMENTS,0.37265135699373697,"in [33] for Transformer models.
312"
MAIN FINE-TUNING EXPERIMENTS,0.3736951983298539,"For each algorithm, we evaluated a task-specific metric (accuracy or F1), as well as the privacy
313"
MAIN FINE-TUNING EXPERIMENTS,0.3747390396659708,"leakage value for the 3 following measures:
314"
MAIN FINE-TUNING EXPERIMENTS,0.3757828810020877,"• Spectral attack AUC — a measure of vulnerability to an attack proposed in [33], measured as
315"
MAIN FINE-TUNING EXPERIMENTS,0.3768267223382046,"classifier ROC AUC: lower value corresponds to better privacy.
316"
MAIN FINE-TUNING EXPERIMENTS,0.3778705636743215,"• Norm attack AUC — vulnerability to a variant of attack proposed in [18], measured as classifier
317"
MAIN FINE-TUNING EXPERIMENTS,0.37891440501043844,"ROC AUC (lower is better). Despite the initial proposal of this approach for attacking gradients,
318"
MAIN FINE-TUNING EXPERIMENTS,0.37995824634655534,"we observed that it is also well-suited for attacking activations.
319"
MAIN FINE-TUNING EXPERIMENTS,0.38100208768267224,"• K-means accuracy — vulnerability to clusterization attack, measured in the percentage of correctly
320"
MAIN FINE-TUNING EXPERIMENTS,0.38204592901878914,"clustered activations, lower is better.
321"
MAIN FINE-TUNING EXPERIMENTS,0.38308977035490605,"For all setups, we report the worst (least private) value among these metrics throughout the entire
322"
MAIN FINE-TUNING EXPERIMENTS,0.38413361169102295,"training period as a measure of privacy leakage, because it is the worst possible scenario that matters
323"
MAIN FINE-TUNING EXPERIMENTS,0.38517745302713985,"from the client’s perspective. For DC and P3EFT, we specify the values for the best configuration in
324"
MAIN FINE-TUNING EXPERIMENTS,0.3862212943632568,"terms of the utility-privacy trade-off. See details in Appendix A. We also report adjusted standard
325"
MAIN FINE-TUNING EXPERIMENTS,0.3872651356993737,"deviations for the two privacy aware algorithms: P3EFT and DC. To do so, we run the full training
326"
MAIN FINE-TUNING EXPERIMENTS,0.3883089770354906,"procedure from scratch with 3 random seeds.
327"
MAIN FINE-TUNING EXPERIMENTS,0.3893528183716075,"Table 1: Accuracy and privacy metrics.
DeBERTa XXLarge."
MAIN FINE-TUNING EXPERIMENTS,0.3903966597077244,"Dataset
Without Regular
DC
P3EFT
LoRAs
FT"
MAIN FINE-TUNING EXPERIMENTS,0.3914405010438413,"SST2 acc
82.9
96.9
96.6±0.4 96.5±0.2
leak 53.9
99.1
93.3±6.8 62.6±2.6"
MAIN FINE-TUNING EXPERIMENTS,0.3924843423799583,"QNLI acc
72.6
96.0
95.8±0.3 95.6±0.5
leak 51.5
99.1
85.0±11.6 74.6±11.1"
MAIN FINE-TUNING EXPERIMENTS,0.3935281837160752,"MNLI acc
49.2
91.9
—
86.9±0.5
leak 34.2
91.5
—
37.4±0.7"
MAIN FINE-TUNING EXPERIMENTS,0.3945720250521921,"Table 2: Accuracy and privacy metrics.
Flan-T5-Large."
MAIN FINE-TUNING EXPERIMENTS,0.395615866388309,"Dataset
Without Regular
DC
P3EFT
LoRAs
FT"
MAIN FINE-TUNING EXPERIMENTS,0.3966597077244259,"SST2 acc
92.8
96.1
95.0±0.1 96.1±0.1
leak 55.8
98.3
68.1±5.0 74.1±3.0"
MAIN FINE-TUNING EXPERIMENTS,0.3977035490605428,"QNLI acc
83.2
95.3
95.2±0.1 94.7±0.0
leak 58.7
98.9
67.0±1.2 63.0±0.8"
MAIN FINE-TUNING EXPERIMENTS,0.3987473903966597,"MNLI acc
73.9
90.5
89.8±0.1 90.1±0.1
leak 34.6
85.9
45.6±0.8 40.0±1.1"
MAIN FINE-TUNING EXPERIMENTS,0.39979123173277664,"The results for DeBERTa are presented in Table 1. To improve reproducibility, we reuse the hyperpa-
328"
MAIN FINE-TUNING EXPERIMENTS,0.40083507306889354,"rameters from original paper, with the exception of the LoRA dropout value. We disable dropout
329"
MAIN FINE-TUNING EXPERIMENTS,0.40187891440501045,"because it interferes with the mixing weights (5). In preliminary experiments, we observed that with
330"
MAIN FINE-TUNING EXPERIMENTS,0.40292275574112735,"dropout enabled, both our algorithm and DC begin to perform significantly worse.
331"
MAIN FINE-TUNING EXPERIMENTS,0.40396659707724425,"We use n = 2 adapter sets for P3EFT for all datasets and adhered to the same approach for the
332"
MAIN FINE-TUNING EXPERIMENTS,0.40501043841336115,"other models as well. Overall, P3FT achieves nearly the same accuracy as traditional (non-private)
333"
MAIN FINE-TUNING EXPERIMENTS,0.40605427974947805,"fine-tuning, outperforming the DC-based algorithm in terms of accuracy given the same privacy level.
334"
MAIN FINE-TUNING EXPERIMENTS,0.407098121085595,"On the MNLI dataset, we could not find the hyperparameters for DC that ensure stable training while
335"
MAIN FINE-TUNING EXPERIMENTS,0.4081419624217119,"maintaining privacy. Meanwhile, P3EFT maintains consistent performance on this task with a slight
336"
MAIN FINE-TUNING EXPERIMENTS,0.4091858037578288,"drop in quality.
337"
MAIN FINE-TUNING EXPERIMENTS,0.4102296450939457,"Table 2 a reports evaluation for the Flan-T5 base model[4]. For this model, we adapt the exact same
338"
MAIN FINE-TUNING EXPERIMENTS,0.4112734864300626,"hyperparameters as in the previous evaluation with DeBERTa-XXLarge. Compared to DeBERTa,
339"
MAIN FINE-TUNING EXPERIMENTS,0.4123173277661795,"these results are more closely matched. Both both our algorothm and DC consistently solve all three
340"
MAIN FINE-TUNING EXPERIMENTS,0.4133611691022965,"tasks, but P3EFT slightly outperforms DC in terms of privacy.
341"
MAIN FINE-TUNING EXPERIMENTS,0.4144050104384134,Table 3: Accuracy and privacy metrics for LLaMA-2 7B.
MAIN FINE-TUNING EXPERIMENTS,0.4154488517745303,"Dataset
Without
Regular
DC
P3EFT
LoRAs
FT"
MAIN FINE-TUNING EXPERIMENTS,0.4164926931106472,"SST2
acc
94.6
97.4
97.1±0.1
95.8±0.1
leak
59.1
99.3
83.6±10.6
68.9±2.6"
MAIN FINE-TUNING EXPERIMENTS,0.4175365344467641,"QNLI
acc
77.0
95.0
95.2±0.1
94.7±0.2
leak
53.3
85.5
66.6±4.1
62.9±0.8"
MAIN FINE-TUNING EXPERIMENTS,0.418580375782881,"To evaluate how our algorithm scales to larger models, we also fine-tune Llama-2 7B [35] on
342"
MAIN FINE-TUNING EXPERIMENTS,0.4196242171189979,"SST2 [32] and QNLI [39] datasets. For these evaluations, we use LoRA hyperparameters that Hu
343"
MAIN FINE-TUNING EXPERIMENTS,0.42066805845511485,"et al. [15] used when fine-tuning GPT-3, with several changes inspired by Dettmers et al. [5]. Namely,
344"
MAIN FINE-TUNING EXPERIMENTS,0.42171189979123175,"we use the NF4 weight format, apply LoRA to both attention and MLP layers with rank 16. We
345"
MAIN FINE-TUNING EXPERIMENTS,0.42275574112734865,"fine-tune both tasks with maximum context length of 512 and weight decay 0.01. Table 3 summarizes
346"
MAIN FINE-TUNING EXPERIMENTS,0.42379958246346555,"our results: for QNLI, P3EFT achieves somewhat better privacy-accuracy trade-off. On SST2, P3EFT
347"
MAIN FINE-TUNING EXPERIMENTS,0.42484342379958245,"shows similarly favorable trade-offs while DC struggles to preserve privacy.
348"
CONCLUSION AND DISCUSSION,0.42588726513569936,"5
Conclusion and Discussion
349"
CONCLUSION AND DISCUSSION,0.42693110647181626,"In this work, we analyze privacy-preserving fine-tuning of large neural networks in the context of
350"
CONCLUSION AND DISCUSSION,0.4279749478079332,"parameter-efficient fine-tuning and the two-party split learning setting. We show that while standard
351"
CONCLUSION AND DISCUSSION,0.4290187891440501,"fine-tuning suffers from label leakage even in the parameter-efficient case, it is possible to leverage
352"
CONCLUSION AND DISCUSSION,0.430062630480167,"the efficiency of PEFT to alter the procedure without any significant performance drawbacks. We test
353"
CONCLUSION AND DISCUSSION,0.4311064718162839,"the resulting method, named P3EFT, on a range of pretrained language models and multiple datasets,
354"
CONCLUSION AND DISCUSSION,0.4321503131524008,"showing that it is competitive with a strong baseline in terms of label privacy while having higher
355"
CONCLUSION AND DISCUSSION,0.4331941544885177,"task performance.
356"
CONCLUSION AND DISCUSSION,0.4342379958246347,"In future work, it is natural to explore how this approach can be extended to establish holistic privacy
357"
CONCLUSION AND DISCUSSION,0.4352818371607516,"in both labels and inputs. This problem can be approached from two directions: either adapt the
358"
CONCLUSION AND DISCUSSION,0.4363256784968685,"ideas of P3EFT for input privacy, or combine it with an existing work like [22]. Another important
359"
CONCLUSION AND DISCUSSION,0.4373695198329854,"direction for future research is exploring the privacy of the long-term client-provider interaction. In a
360"
CONCLUSION AND DISCUSSION,0.4384133611691023,"typical real-world use case of API fine-tuning, a client performs multiple training runs on overlapping
361"
CONCLUSION AND DISCUSSION,0.4394572025052192,"data and hyperparameters. This could open additional attacks vectors that combine information from
362"
CONCLUSION AND DISCUSSION,0.4405010438413361,"multiple training runs.
363"
REFERENCES,0.44154488517745305,"References
364"
REFERENCES,0.44258872651356995,"[1] Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan McMahan,
365"
REFERENCES,0.44363256784968685,"Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for
366"
REFERENCES,0.44467640918580376,"privacy-preserving machine learning. In proceedings of the 2017 ACM SIGSAC Conference on
367"
REFERENCES,0.44572025052192066,"Computer and Communications Security, pages 1175–1191, 2017.
368"
REFERENCES,0.44676409185803756,"[2] Alexander Borzunov, Dmitry Baranchuk, Tim Dettmers, Max Ryabinin, Younes Belkada, Artem
369"
REFERENCES,0.44780793319415446,"Chumachenko, Pavel Samygin, and Colin Raffel. Petals: Collaborative inference and fine-tuning
370"
REFERENCES,0.4488517745302714,"of large models. arXiv preprint arXiv:2209.01188, 2022. URL https://arxiv.org/abs/
371"
REFERENCES,0.4498956158663883,"2209.01188.
372"
REFERENCES,0.4509394572025052,"[3] Tianqi Chen and Carlos Guestrin. XGBoost: A scalable tree boosting system. In Proceedings of
373"
REFERENCES,0.4519832985386221,"the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
374"
REFERENCES,0.453027139874739,"KDD ’16, pages 785–794, New York, NY, USA, 2016. ACM. ISBN 978-1-4503-4232-2. doi:
375"
REFERENCES,0.45407098121085593,"10.1145/2939672.2939785. URL http://doi.acm.org/10.1145/2939672.2939785.
376"
REFERENCES,0.4551148225469729,"[4] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li,
377"
REFERENCES,0.4561586638830898,"Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu,
378"
REFERENCES,0.4572025052192067,"Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav
379"
REFERENCES,0.4582463465553236,"Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H.
380"
REFERENCES,0.4592901878914405,"Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling
381"
REFERENCES,0.4603340292275574,"instruction-finetuned language models, 2022. URL https://arxiv.org/abs/2210.11416.
382"
REFERENCES,0.4613778705636743,"[5] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient
383"
REFERENCES,0.46242171189979125,"finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023.
384"
REFERENCES,0.46346555323590816,"[6] Dreambooth API. Dreambooth API – Easily finetune Stable Diffusion and generate customised
385"
REFERENCES,0.46450939457202506,"AI images — dreamboothapi.ai. https://dreamboothapi.ai/, 2023. [Accessed 28-09-
386"
REFERENCES,0.46555323590814196,"2023].
387"
REFERENCES,0.46659707724425886,"[7] Haonan Duan, Adam Dziedzic, Nicolas Papernot, and Franziska Boenisch. Flocks of stochastic
388"
REFERENCES,0.46764091858037576,"parrots: Differentially private prompt learning for large language models. arXiv preprint
389"
REFERENCES,0.46868475991649267,"arXiv:2305.15594, 2023.
390"
REFERENCES,0.4697286012526096,"[8] Cynthia Dwork. Differential privacy. In International colloquium on automata, languages, and
391"
REFERENCES,0.4707724425887265,"programming, pages 1–12. Springer, 2006.
392"
REFERENCES,0.4718162839248434,"[9] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation.
393"
REFERENCES,0.47286012526096033,"In Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference
394"
REFERENCES,0.47390396659707723,"on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 1180–
395"
REFERENCES,0.47494780793319413,"1189, Lille, France, 07–09 Jul 2015. PMLR. URL https://proceedings.mlr.press/v37/
396"
REFERENCES,0.4759916492693111,"ganin15.html.
397"
REFERENCES,0.477035490605428,"[10] Otkrist Gupta and Ramesh Raskar. Distributed learning of deep neural network over multiple
398"
REFERENCES,0.4780793319415449,"agents. Journal of Network and Computer Applications, 116:1–8, 2018. ISSN 1084-8045.
399"
REFERENCES,0.4791231732776618,"doi: https://doi.org/10.1016/j.jnca.2018.05.003. URL https://www.sciencedirect.com/
400"
REFERENCES,0.4801670146137787,"science/article/pii/S1084804518301590.
401"
REFERENCES,0.4812108559498956,"[11] Karen Hambardzumyan, Hrant Khachatrian, and Jonathan May. WARP: Word-level Ad-
402"
REFERENCES,0.4822546972860125,"versarial ReProgramming.
In Proceedings of the 59th Annual Meeting of the Associa-
403"
REFERENCES,0.48329853862212946,"tion for Computational Linguistics and the 11th International Joint Conference on Natural
404"
REFERENCES,0.48434237995824636,"Language Processing (Volume 1: Long Papers), pages 4921–4933, Online, August 2021.
405"
REFERENCES,0.48538622129436326,"Association for Computational Linguistics.
doi: 10.18653/v1/2021.acl-long.381.
URL
406"
REFERENCES,0.48643006263048016,"https://aclanthology.org/2021.acl-long.381.
407"
REFERENCES,0.48747390396659707,"[12] Stephen Hardy, Wilko Henecka, Hamish Ivey-Law, Richard Nock, Giorgio Patrini, Guillaume
408"
REFERENCES,0.48851774530271397,"Smith, and Brian Thorne. Private federated learning on vertically partitioned data via entity
409"
REFERENCES,0.48956158663883087,"resolution and additively homomorphic encryption, 2017.
410"
REFERENCES,0.4906054279749478,"[13] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced
411"
REFERENCES,0.49164926931106473,"bert with disentangled attention. In International Conference on Learning Representations,
412"
REFERENCES,0.49269311064718163,"2021. URL https://openreview.net/forum?id=XPZIaotutsD.
413"
REFERENCES,0.49373695198329853,"[14] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,
414"
REFERENCES,0.49478079331941544,"Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning
415"
REFERENCES,0.49582463465553234,"for NLP. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th
416"
REFERENCES,0.4968684759916493,"International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning
417"
REFERENCES,0.4979123173277662,"Research, pages 2790–2799. PMLR, 09–15 Jun 2019. URL https://proceedings.mlr.
418"
REFERENCES,0.4989561586638831,"press/v97/houlsby19a.html.
419"
REFERENCES,0.5,"[15] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
420"
REFERENCES,0.5010438413361169,"Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In
421"
REFERENCES,0.5020876826722338,"International Conference on Learning Representations, 2022. URL https://openreview.
422"
REFERENCES,0.5031315240083507,"net/forum?id=nZeVKeeFYf9.
423"
REFERENCES,0.5041753653444676,"[16] Hugging Face. AutoTrain — huggingface.co. https://huggingface.co/autotrain, 2023.
424"
REFERENCES,0.5052192066805845,"[Accessed 28-09-2023].
425"
REFERENCES,0.5062630480167014,"[17] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
426"
REFERENCES,0.5073068893528184,"arXiv:1412.6980, 2014.
427"
REFERENCES,0.5083507306889353,"[18] Oscar Li, Jiankai Sun, Xin Yang, Weihao Gao, Hongyi Zhang, Junyuan Xie, Virginia Smith,
428"
REFERENCES,0.5093945720250522,"and Chong Wang. Label leakage and protection in two-party split learning. In International
429"
REFERENCES,0.5104384133611691,"Conference on Learning Representations, 2022. URL https://openreview.net/forum?
430"
REFERENCES,0.511482254697286,"id=cOtBRgsf2fO.
431"
REFERENCES,0.5125260960334029,"[19] Qinbin Li, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, Yuan Li, Xu Liu, and Bingsheng He.
432"
REFERENCES,0.5135699373695198,"A survey on federated learning systems: Vision, hype and reality for data privacy and protection.
433"
REFERENCES,0.5146137787056367,"IEEE Transactions on Knowledge and Data Engineering, 2021.
434"
REFERENCES,0.5156576200417536,"[20] Shen Li, Pritam Damania, Luca Wehrstedt, and Rohan Varma. PyTorch RPC: Distributed Deep
435"
REFERENCES,0.5167014613778705,"Learning Built on Tensor-Optimized Remote Procedure Calls. In Proceedings of Machine
436"
REFERENCES,0.5177453027139874,"Learning and Systems 5 (MLSys), 2023.
437"
REFERENCES,0.5187891440501043,"[21] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In
438"
REFERENCES,0.5198329853862212,"Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the
439"
REFERENCES,0.5208768267223383,"11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),
440"
REFERENCES,0.5219206680584552,"pages 4582–4597, Online, August 2021. Association for Computational Linguistics. doi:
441"
REFERENCES,0.5229645093945721,"10.18653/v1/2021.acl-long.353. URL https://aclanthology.org/2021.acl-long.353.
442"
REFERENCES,0.524008350730689,"[22] Yansong Li, Zhixing Tan, and Yang Liu. Privacy-preserving prompt tuning for large language
443"
REFERENCES,0.5250521920668059,"model services. ArXiv, abs/2305.06212, 2023. URL https://api.semanticscholar.org/
444"
REFERENCES,0.5260960334029228,"CorpusID:258588141.
445"
REFERENCES,0.5271398747390397,"[23] Xiao-Yang Liu, Rongyi Zhu, Daochen Zha, Jiechao Gao, Shan Zhong, and Meikang Qiu.
446"
REFERENCES,0.5281837160751566,"Differentially private low-rank adaptation of large language model using federated learning.
447"
REFERENCES,0.5292275574112735,"arXiv preprint arXiv:2312.17493, 2023.
448"
REFERENCES,0.5302713987473904,"[24] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Ar-
449"
REFERENCES,0.5313152400835073,"cas. Communication-Efficient Learning of Deep Networks from Decentralized Data. In Aarti
450"
REFERENCES,0.5323590814196242,"Singh and Jerry Zhu, editors, Proceedings of the 20th International Conference on Artifi-
451"
REFERENCES,0.5334029227557411,"cial Intelligence and Statistics, volume 54 of Proceedings of Machine Learning Research,
452"
REFERENCES,0.534446764091858,"pages 1273–1282. PMLR, 20–22 Apr 2017. URL https://proceedings.mlr.press/v54/
453"
REFERENCES,0.535490605427975,"mcmahan17a.html.
454"
REFERENCES,0.5365344467640919,"[25] Nvidia. Nvidia confidential computing. https://www.nvidia.com/en-us/data-center/
455"
REFERENCES,0.5375782881002088,"solutions/confidential-computing, 2023. [Accessed 28-09-2023].
456"
REFERENCES,0.5386221294363257,"[26] OctoAI. Fine-tuning Stable Diffusion — docs.octoai.cloud. https://docs.octoai.cloud/
457"
REFERENCES,0.5396659707724426,"docs/fine-tuning-stable-diffusion, 2023. [Accessed 28-09-2023].
458"
REFERENCES,0.5407098121085595,"[27] OpenAI.
OpenAI Platform — platform.openai.com.
https://platform.openai.com/
459"
REFERENCES,0.5417536534446764,"docs/guides/fine-tuning, 2023. [Accessed 28-09-2023].
460"
REFERENCES,0.5427974947807933,"[28] Dario Pasquini, Giuseppe Ateniese, and Massimo Bernaschi. Unleashing the tiger: Inference
461"
REFERENCES,0.5438413361169102,"attacks on split learning. In Proceedings of the 2021 ACM SIGSAC Conference on Computer and
462"
REFERENCES,0.5448851774530271,"Communications Security, CCS ’21, page 2113–2129, New York, NY, USA, 2021. Association
463"
REFERENCES,0.545929018789144,"for Computing Machinery. ISBN 9781450384544. doi: 10.1145/3460120.3485259. URL
464"
REFERENCES,0.5469728601252609,"https://doi.org/10.1145/3460120.3485259.
465"
REFERENCES,0.5480167014613778,"[29] Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych.
466"
REFERENCES,0.5490605427974948,"Adapterfusion: Non-destructive task composition for transfer learning, 2021.
467"
REFERENCES,0.5501043841336117,"[30] Yuma Rao, Jacob Steeves, Ala Shaabana, Daniel Attevelt, and Matthew McAteer. Bittensor: A
468"
REFERENCES,0.5511482254697286,"peer-to-peer intelligence market, 2021.
469"
REFERENCES,0.5521920668058455,"[31] Weiyan Shi, Ryan Shea, Si Chen, Chiyuan Zhang, Ruoxi Jia, and Zhou Yu. Just fine-tune twice:
470"
REFERENCES,0.5532359081419624,"Selective differential privacy for large language models. arXiv preprint arXiv:2204.07667,
471"
REFERENCES,0.5542797494780793,"2022.
472"
REFERENCES,0.5553235908141962,"[32] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew
473"
REFERENCES,0.5563674321503131,"Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a
474"
REFERENCES,0.55741127348643,"sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural
475"
REFERENCES,0.558455114822547,"Language Processing, pages 1631–1642, Seattle, Washington, USA, October 2013. Association
476"
REFERENCES,0.5594989561586639,"for Computational Linguistics. URL https://www.aclweb.org/anthology/D13-1170.
477"
REFERENCES,0.5605427974947808,"[33] Jiankai Sun, Xin Yang, Yuanshun Yao, and Chong Wang. Label leakage and protection from
478"
REFERENCES,0.5615866388308977,"forward embedding in vertical federated learning. arXiv preprint arXiv:2203.01451, 2022.
479"
REFERENCES,0.5626304801670147,"[34] Chuanqi Tan, Fuchun Sun, Tao Kong, Wenchang Zhang, Chao Yang, and Chunfang Liu. A
480"
REFERENCES,0.5636743215031316,"survey on deep transfer learning. In Vˇera K˚urková, Yannis Manolopoulos, Barbara Hammer,
481"
REFERENCES,0.5647181628392485,"Lazaros Iliadis, and Ilias Maglogiannis, editors, Artificial Neural Networks and Machine
482"
REFERENCES,0.5657620041753654,"Learning – ICANN 2018, pages 270–279, Cham, 2018. Springer International Publishing. ISBN
483"
REFERENCES,0.5668058455114823,"978-3-030-01424-7.
484"
REFERENCES,0.5678496868475992,"[35] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,
485"
REFERENCES,0.5688935281837161,"Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open
486"
REFERENCES,0.569937369519833,"foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.
487"
REFERENCES,0.5709812108559499,"[36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
488"
REFERENCES,0.5720250521920668,"Gomez, Ł ukasz Kaiser, and Illia Polosukhin.
Attention is all you need.
In I. Guyon,
489"
REFERENCES,0.5730688935281837,"U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, ed-
490"
REFERENCES,0.5741127348643006,"itors, Advances in Neural Information Processing Systems, volume 30. Curran Associates,
491"
REFERENCES,0.5751565762004175,"Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/
492"
REFERENCES,0.5762004175365344,"3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.
493"
REFERENCES,0.5772442588726514,"[37] Praneeth Vepakomma, Otkrist Gupta, Tristan Swedish, and Ramesh Raskar. Split learning for
494"
REFERENCES,0.5782881002087683,"health: Distributed deep learning without sharing raw patient data, 2018.
495"
REFERENCES,0.5793319415448852,"[38] Praneeth Vepakomma, Otkrist Gupta, Abhimanyu Dubey, and Ramesh Raskar. Reducing
496"
REFERENCES,0.5803757828810021,"leakage in distributed deep learning for sensitive health data. 05 2019.
497"
REFERENCES,0.581419624217119,"[39] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.
498"
REFERENCES,0.5824634655532359,"Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv
499"
REFERENCES,0.5835073068893528,"preprint arXiv:1804.07461, 2018.
500"
REFERENCES,0.5845511482254697,"[40] Yiming Wang, Yu Lin, Xiaodong Zeng, and Guannan Zhang. Privatelora for efficient privacy
501"
REFERENCES,0.5855949895615866,"preserving llm. arXiv preprint arXiv:2311.14030, 2023.
502"
REFERENCES,0.5866388308977035,"[41] Adina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge corpus
503"
REFERENCES,0.5876826722338204,"for sentence understanding through inference. arXiv preprint arXiv:1704.05426, 2017.
504"
REFERENCES,0.5887265135699373,"[42] Guangxuan Xiao, Ji Lin, and Song Han. Offsite-tuning: Transfer learning without full model.
505"
REFERENCES,0.5897703549060542,"arXiv preprint arXiv:2302.04870, 2023.
506"
REFERENCES,0.5908141962421712,"[43] Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated machine learning: Concept
507"
REFERENCES,0.5918580375782881,"and applications. ACM Trans. Intell. Syst. Technol., 10(2), jan 2019. ISSN 2157-6904. doi:
508"
REFERENCES,0.592901878914405,"10.1145/3298981. URL https://doi.org/10.1145/3298981.
509"
REFERENCES,0.593945720250522,"[44] Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A Inan, Gautam Kamath,
510"
REFERENCES,0.5949895615866388,"Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz, Sergey Yekhanin, and
511"
REFERENCES,0.5960334029227558,"Huishuai Zhang. Differentially private fine-tuning of language models. In International
512"
REFERENCES,0.5970772442588727,"Conference on Learning Representations, 2022. URL https://openreview.net/forum?
513"
REFERENCES,0.5981210855949896,"id=Q42f0dfjECO.
514"
REFERENCES,0.5991649269311065,"[45] Zhuo Zhang, Yuanhang Yang, Yong Dai, Qifan Wang, Yue Yu, Lizhen Qu, and Zenglin
515"
REFERENCES,0.6002087682672234,"Xu. FedPETuning: When federated learning meets the parameter-efficient tuning methods of
516"
REFERENCES,0.6012526096033403,"pre-trained language models. In Findings of the Association for Computational Linguistics:
517"
REFERENCES,0.6022964509394572,"ACL 2023, pages 9963–9977, Toronto, Canada, July 2023. Association for Computational
518"
REFERENCES,0.6033402922755741,"Linguistics. doi: 10.18653/v1/2023.findings-acl.632. URL https://aclanthology.org/
519"
REFERENCES,0.6043841336116911,"2023.findings-acl.632.
520"
REFERENCES,0.605427974947808,"[46] Haodong Zhao, Wei Du, Fangqi Li, Peixuan Li, and Gongshen Liu. Fedprompt: Communication-
521"
REFERENCES,0.6064718162839249,"efficient and privacy preserving prompt tuning in federated learning, 2023.
522"
REFERENCES,0.6075156576200418,"A
Hyperparameters search
523"
REFERENCES,0.6085594989561587,"In P3EFT and Distance Correlation methods resulting loss L function can be viewed in the form
524"
REFERENCES,0.6096033402922756,"L = Lm + α · Lr,"
REFERENCES,0.6106471816283925,"where Lm - main task loss, Lr - regularizer and α is a coefficient that controls the tradeoff between
525"
REFERENCES,0.6116910229645094,"these two losses. The selection of this coefficient affects the final performance of the model. Therefore,
526"
REFERENCES,0.6127348643006263,"to find the best configurations for both methods, we iterated through this hyperparameter using a grid
527"
REFERENCES,0.6137787056367432,"search.
528"
REFERENCES,0.6148225469728601,"We started with α = 1 and then altered it with a multiplicative step of 10
1
2 . Values were discarded if
529"
REFERENCES,0.615866388308977,"the quality did not exceed that achieved by solely training the classifier without LoRA. This criterion
530"
REFERENCES,0.6169102296450939,"was adopted because such outcomes would suggest the method’s inability to outperform training
531"
REFERENCES,0.6179540709812108,"scenarios in which the server does not engage with the labels whatsoever. Additionally, we excluded
532"
REFERENCES,0.6189979123173278,"values that led to unstable training. By this, we mean instances where, although the model initially
533"
REFERENCES,0.6200417536534447,"trained on the primary task, at some point, the regularizer began contributing significantly more,
534"
REFERENCES,0.6210855949895616,"and the utility value dropped to the starting value. We observed this issue for the DC method with
535"
REFERENCES,0.6221294363256785,"DeBERTa on the MNLI. From the remaining values, we aimed to choose the one that offered the
536"
REFERENCES,0.6231732776617954,"lowest privacy leakage. The final hyperparameter values for P3EFT can be found in the Table 4 and
537"
REFERENCES,0.6242171189979123,"for DC in the Table 5.
538"
REFERENCES,0.6252609603340292,"Table 4: Regularization parameter α for the P3EFT method. The values in the table represent powers
of the 10
1
2 ."
REFERENCES,0.6263048016701461,"SST2
QNLI
MNLI"
REFERENCES,0.627348643006263,"DeBERTa XXLarge
1
1
1"
REFERENCES,0.6283924843423799,"Flan-T5-Large
-1
1
1"
REFERENCES,0.6294363256784968,"LLaMA-2 7B
0
0
—"
REFERENCES,0.6304801670146137,"Table 5: Regularization parameter α for the DC method. The values in the table represent powers of
the 10
1
2 ."
REFERENCES,0.6315240083507306,"SST2
QNLI
MNLI"
REFERENCES,0.6325678496868476,"DeBERTa XXLarge
0
-1
—"
REFERENCES,0.6336116910229646,"Flan-T5-Large
2
-1
0"
REFERENCES,0.6346555323590815,"LLaMA-2 7B
-1
-1
—"
REFERENCES,0.6356993736951984,"B
Formal algorithm definition
539"
REFERENCES,0.6367432150313153,"Below, we define the full P3EFT algorithm. In Algorithm 2, main_loss is the task-specific objective
540"
REFERENCES,0.6377870563674322,"e.g. cross-entropy; reg_loss is the adversarial regularizer described in Section 3.4. We denote
541"
REFERENCES,0.6388308977035491,"client-side model ""head"" as f(h, ψt), where ψ represent trainable head parameters. Finally, opt_step
542"
REFERENCES,0.639874739039666,"function performs a single gradient descent step with a task-specific optimizer, typically Adam [17].
543"
REFERENCES,0.6409185803757829,Algorithm 2 P3EFT - full training algorithm
REFERENCES,0.6419624217118998,"1: Input: dataset D = {X, Y }, n > 1 number of adapters, α ≥0 - regularizing weight, m > 1
number of obfuscated gradients
2: Initialize head ψ0, mixing weights Wi and adapters θ0
i , i = 1, n
3: for t = 0, 1, . . . , T −1 do
4:
Sample batch {xt, yt}
5:
for i = 1, . . . , n do
6:
ht
i = h(xt, θt
i)
▷by server
7:
li = reg_loss(ht
i, yt)
▷by client
8:
end for
9:
h′ = Pn
i=1 Wi ⊙ht
i
10:
l = main_loss(f(h′, ψt), yt)
11:
L = l + α · Pn
i=1 li
12:
for i = 1, . . . , n do
13:
gh = ∂L/∂ht
i
▷Client performs partial backprop
14:
gt
i = private_backprop(x, θt
i, gh, m)
15:
θt+1
i
= opt_step(θt
i, gt
i, t)
16:
end for
17:
ψt+1 = opt_step(ψt, ∂l/∂ψt, t)
18: end for
19: Return: ψT , θT
1 , . . . , θT
M"
REFERENCES,0.6430062630480167,"C
Informal description of LoRA fine-tuning
544"
REFERENCES,0.6440501043841336,"For convenience, we provide a brief summary of fine-tuning with LoRA [15]. This PEFT method
545"
REFERENCES,0.6450939457202505,"was originally designed for fine-tuning large pre-trained language models on downstream NLP tasks.
546"
REFERENCES,0.6461377870563675,"These language models are typically based on the Transformer architecture [36], where most trainable
547"
REFERENCES,0.6471816283924844,"parameters are allocated to linear layers in multi-head self-attention and feedforward blocks.
548"
REFERENCES,0.6482254697286013,"In the first stage of LoRA fine-tuning, user augments the model with adapters. To do so, a user goes
549"
REFERENCES,0.6492693110647182,"over linear layers in transformer blocks and adds two trainable matrices, A and B that affect this
550"
REFERENCES,0.6503131524008351,"layer’s forward pass. Let Wi × x + bi be the original layer with n inputs and m hidden units. Here,
551"
REFERENCES,0.651356993736952,"Wi ∈Rm×n is a pre-trained weight matrix, bi ∈Rm is a pre-trained intercept vector and x ∈Rn
552"
REFERENCES,0.6524008350730689,"represents a vector of inputs to this particular layer. During the forward pass, a layer with LoRA
553"
REFERENCES,0.6534446764091858,"adapter computes Wi × x + bi + Bi × Ai × x, or equivalently, (Wi + B × A) × x + bi. Here, Ai
554"
REFERENCES,0.6544885177453027,"and Bi are two newly added matrices that constitute a LoRA adapter.
555"
REFERENCES,0.6555323590814196,"The adapter matrices A ∈Rr×n and B ∈Rm×r have a very small intermediate dimension r. For
556"
REFERENCES,0.6565762004175365,"instance, when training GPT-3 with LoRA adapters, authors use 1 ≤r ≤64, whereas the main
557"
REFERENCES,0.6576200417536534,"weight dimensions are m = n = 12288. The first matrix A is initialized with small random normal
558"
REFERENCES,0.6586638830897703,"values, and the second matrix B is initialized at zeros. That way, initial A and B do not affect the
559"
REFERENCES,0.6597077244258872,"model predictions.
560"
REFERENCES,0.6607515657620042,"Once all adapters are initilized, the user trains all Ai and Bi matrices of the model, while keeping
561"
REFERENCES,0.6617954070981211,"the rest of the weights frozen. This way, only a small faction (less than 1%) of model weights are
562"
REFERENCES,0.662839248434238,"updated. Once the training is over, the learned adapters Ai and Bi can be merged into the main
563"
REFERENCES,0.6638830897703549,"weights (Wi := Wi + Ai × Bi) or used separately.
564"
REFERENCES,0.6649269311064718,"LoRA adapters are designed with two objectives in mind: i) to allow fine-tuning models in limited
565"
REFERENCES,0.6659707724425887,"GPU memory and ii) to allow inferencing many fine-tuned models using one inference server. When
566"
REFERENCES,0.6670146137787056,"fine-tuning, LoRA achieves small memory footprint due to the fact that user does not need to compute
567"
REFERENCES,0.6680584551148225,"gradients (or optimizer statistics) for billions of main model parameters. During inference, a server
568"
REFERENCES,0.6691022964509394,"can keep a library of several adapters for different tasks and swap between them on demand.
569"
REFERENCES,0.6701461377870563,"D
Informal description of LoRA fine-tuning
570"
REFERENCES,0.6711899791231732,"We used NVIDIA A100 GPUs for all the experiments. Experiments with DeBERTA [13] and Flan-T5
571"
REFERENCES,0.6722338204592901,"[4] on SST2 [32] were conducted on the single GPU, while experiments on MNLI [41] and QNLI
572"
REFERENCES,0.673277661795407,"require 4 A100. LLaMA-2 [35] expetiments were carried out on the node of 8 A100.
573"
REFERENCES,0.6743215031315241,"All the experiments last 12-24 hours. However, it is possible to speed up some of them using more
574"
REFERENCES,0.675365344467641,"GPUs, as well as conduct them on a smaller number of GPUs using technics to save GPU memory
575"
REFERENCES,0.6764091858037579,"(see parameters in our code).
576"
REFERENCES,0.6774530271398748,"NeurIPS Paper Checklist
577"
CLAIMS,0.6784968684759917,"1. Claims
578"
CLAIMS,0.6795407098121086,"Question: Do the main claims made in the abstract and introduction accurately reflect the
579"
CLAIMS,0.6805845511482255,"paper’s contributions and scope?
580"
CLAIMS,0.6816283924843424,"Answer: [Yes]
581"
CLAIMS,0.6826722338204593,"Justification: See our main contributions in 1 and Section 5.
582"
CLAIMS,0.6837160751565762,"Guidelines:
583"
CLAIMS,0.6847599164926931,"• The answer NA means that the abstract and introduction do not include the claims
584"
CLAIMS,0.68580375782881,"made in the paper.
585"
CLAIMS,0.6868475991649269,"• The abstract and/or introduction should clearly state the claims made, including the
586"
CLAIMS,0.6878914405010439,"contributions made in the paper and important assumptions and limitations. A No or
587"
CLAIMS,0.6889352818371608,"NA answer to this question will not be perceived well by the reviewers.
588"
CLAIMS,0.6899791231732777,"• The claims made should match theoretical and experimental results, and reflect how
589"
CLAIMS,0.6910229645093946,"much the results can be expected to generalize to other settings.
590"
CLAIMS,0.6920668058455115,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
591"
CLAIMS,0.6931106471816284,"are not attained by the paper.
592"
LIMITATIONS,0.6941544885177453,"2. Limitations
593"
LIMITATIONS,0.6951983298538622,"Question: Does the paper discuss the limitations of the work performed by the authors?
594"
LIMITATIONS,0.6962421711899791,"Answer: [Yes]
595"
LIMITATIONS,0.697286012526096,"Justification: We discuss potential limitations of the method in Section 1, Section 3 and
596"
LIMITATIONS,0.6983298538622129,"Section 5.
597"
LIMITATIONS,0.6993736951983298,"Guidelines:
598"
LIMITATIONS,0.7004175365344467,"• The answer NA means that the paper has no limitation while the answer No means that
599"
LIMITATIONS,0.7014613778705637,"the paper has limitations, but those are not discussed in the paper.
600"
LIMITATIONS,0.7025052192066806,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
601"
LIMITATIONS,0.7035490605427975,"• The paper should point out any strong assumptions and how robust the results are to
602"
LIMITATIONS,0.7045929018789144,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
603"
LIMITATIONS,0.7056367432150313,"model well-specification, asymptotic approximations only holding locally). The authors
604"
LIMITATIONS,0.7066805845511482,"should reflect on how these assumptions might be violated in practice and what the
605"
LIMITATIONS,0.7077244258872651,"implications would be.
606"
LIMITATIONS,0.708768267223382,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
607"
LIMITATIONS,0.7098121085594989,"only tested on a few datasets or with a few runs. In general, empirical results often
608"
LIMITATIONS,0.7108559498956158,"depend on implicit assumptions, which should be articulated.
609"
LIMITATIONS,0.7118997912317327,"• The authors should reflect on the factors that influence the performance of the approach.
610"
LIMITATIONS,0.7129436325678496,"For example, a facial recognition algorithm may perform poorly when image resolution
611"
LIMITATIONS,0.7139874739039666,"is low or images are taken in low lighting. Or a speech-to-text system might not be
612"
LIMITATIONS,0.7150313152400835,"used reliably to provide closed captions for online lectures because it fails to handle
613"
LIMITATIONS,0.7160751565762005,"technical jargon.
614"
LIMITATIONS,0.7171189979123174,"• The authors should discuss the computational efficiency of the proposed algorithms
615"
LIMITATIONS,0.7181628392484343,"and how they scale with dataset size.
616"
LIMITATIONS,0.7192066805845512,"• If applicable, the authors should discuss possible limitations of their approach to
617"
LIMITATIONS,0.7202505219206681,"address problems of privacy and fairness.
618"
LIMITATIONS,0.721294363256785,"• While the authors might fear that complete honesty about limitations might be used by
619"
LIMITATIONS,0.7223382045929019,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
620"
LIMITATIONS,0.7233820459290188,"limitations that aren’t acknowledged in the paper. The authors should use their best
621"
LIMITATIONS,0.7244258872651357,"judgment and recognize that individual actions in favor of transparency play an impor-
622"
LIMITATIONS,0.7254697286012526,"tant role in developing norms that preserve the integrity of the community. Reviewers
623"
LIMITATIONS,0.7265135699373695,"will be specifically instructed to not penalize honesty concerning limitations.
624"
THEORY ASSUMPTIONS AND PROOFS,0.7275574112734864,"3. Theory Assumptions and Proofs
625"
THEORY ASSUMPTIONS AND PROOFS,0.7286012526096033,"Question: For each theoretical result, does the paper provide the full set of assumptions and
626"
THEORY ASSUMPTIONS AND PROOFS,0.7296450939457203,"a complete (and correct) proof?
627"
THEORY ASSUMPTIONS AND PROOFS,0.7306889352818372,"Answer: [NA]
628"
THEORY ASSUMPTIONS AND PROOFS,0.7317327766179541,"Justification: We include no proofs.
629"
THEORY ASSUMPTIONS AND PROOFS,0.732776617954071,"Guidelines:
630"
THEORY ASSUMPTIONS AND PROOFS,0.7338204592901879,"• The answer NA means that the paper does not include theoretical results.
631"
THEORY ASSUMPTIONS AND PROOFS,0.7348643006263048,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
632"
THEORY ASSUMPTIONS AND PROOFS,0.7359081419624217,"referenced.
633"
THEORY ASSUMPTIONS AND PROOFS,0.7369519832985386,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
634"
THEORY ASSUMPTIONS AND PROOFS,0.7379958246346555,"• The proofs can either appear in the main paper or the supplemental material, but if
635"
THEORY ASSUMPTIONS AND PROOFS,0.7390396659707724,"they appear in the supplemental material, the authors are encouraged to provide a short
636"
THEORY ASSUMPTIONS AND PROOFS,0.7400835073068893,"proof sketch to provide intuition.
637"
THEORY ASSUMPTIONS AND PROOFS,0.7411273486430062,"• Inversely, any informal proof provided in the core of the paper should be complemented
638"
THEORY ASSUMPTIONS AND PROOFS,0.7421711899791231,"by formal proofs provided in appendix or supplemental material.
639"
THEORY ASSUMPTIONS AND PROOFS,0.7432150313152401,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
640"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.744258872651357,"4. Experimental Result Reproducibility
641"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7453027139874739,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
642"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7463465553235908,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
643"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7473903966597077,"of the paper (regardless of whether the code and data are provided or not)?
644"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7484342379958246,"Answer: [Yes]
645"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7494780793319415,"Justification: We describe all the technical details in the Section 4 and in the README of
646"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7505219206680585,"our attached code.
647"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7515657620041754,"Guidelines:
648"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7526096033402923,"• The answer NA means that the paper does not include experiments.
649"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7536534446764092,"• If the paper includes experiments, a No answer to this question will not be perceived
650"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7546972860125261,"well by the reviewers: Making the paper reproducible is important, regardless of
651"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.755741127348643,"whether the code and data are provided or not.
652"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7567849686847599,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
653"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7578288100208769,"to make their results reproducible or verifiable.
654"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7588726513569938,"• Depending on the contribution, reproducibility can be accomplished in various ways.
655"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7599164926931107,"For example, if the contribution is a novel architecture, describing the architecture fully
656"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7609603340292276,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
657"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7620041753653445,"be necessary to either make it possible for others to replicate the model with the same
658"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7630480167014614,"dataset, or provide access to the model. In general. releasing code and data is often
659"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7640918580375783,"one good way to accomplish this, but reproducibility can also be provided via detailed
660"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7651356993736952,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
661"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7661795407098121,"of a large language model), releasing of a model checkpoint, or other means that are
662"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.767223382045929,"appropriate to the research performed.
663"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7682672233820459,"• While NeurIPS does not require releasing code, the conference does require all submis-
664"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7693110647181628,"sions to provide some reasonable avenue for reproducibility, which may depend on the
665"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7703549060542797,"nature of the contribution. For example
666"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7713987473903967,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
667"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7724425887265136,"to reproduce that algorithm.
668"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7734864300626305,"(b) If the contribution is primarily a new model architecture, the paper should describe
669"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7745302713987474,"the architecture clearly and fully.
670"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7755741127348643,"(c) If the contribution is a new model (e.g., a large language model), then there should
671"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7766179540709812,"either be a way to access this model for reproducing the results or a way to reproduce
672"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7776617954070981,"the model (e.g., with an open-source dataset or instructions for how to construct
673"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.778705636743215,"the dataset).
674"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7797494780793319,"(d) We recognize that reproducibility may be tricky in some cases, in which case
675"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7807933194154488,"authors are welcome to describe the particular way they provide for reproducibility.
676"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7818371607515657,"In the case of closed-source models, it may be that access to the model is limited in
677"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7828810020876826,"some way (e.g., to registered users), but it should be possible for other researchers
678"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7839248434237995,"to have some path to reproducing or verifying the results.
679"
OPEN ACCESS TO DATA AND CODE,0.7849686847599165,"5. Open access to data and code
680"
OPEN ACCESS TO DATA AND CODE,0.7860125260960334,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
681"
OPEN ACCESS TO DATA AND CODE,0.7870563674321504,"tions to faithfully reproduce the main experimental results, as described in supplemental
682"
OPEN ACCESS TO DATA AND CODE,0.7881002087682673,"material?
683"
OPEN ACCESS TO DATA AND CODE,0.7891440501043842,"Answer: [Yes]
684"
OPEN ACCESS TO DATA AND CODE,0.7901878914405011,"Justification: We provide attached code.
685"
OPEN ACCESS TO DATA AND CODE,0.791231732776618,"Guidelines:
686"
OPEN ACCESS TO DATA AND CODE,0.7922755741127349,"• The answer NA means that paper does not include experiments requiring code.
687"
OPEN ACCESS TO DATA AND CODE,0.7933194154488518,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
688"
OPEN ACCESS TO DATA AND CODE,0.7943632567849687,"public/guides/CodeSubmissionPolicy) for more details.
689"
OPEN ACCESS TO DATA AND CODE,0.7954070981210856,"• While we encourage the release of code and data, we understand that this might not be
690"
OPEN ACCESS TO DATA AND CODE,0.7964509394572025,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
691"
OPEN ACCESS TO DATA AND CODE,0.7974947807933194,"including code, unless this is central to the contribution (e.g., for a new open-source
692"
OPEN ACCESS TO DATA AND CODE,0.7985386221294363,"benchmark).
693"
OPEN ACCESS TO DATA AND CODE,0.7995824634655533,"• The instructions should contain the exact command and environment needed to run to
694"
OPEN ACCESS TO DATA AND CODE,0.8006263048016702,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
695"
OPEN ACCESS TO DATA AND CODE,0.8016701461377871,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
696"
OPEN ACCESS TO DATA AND CODE,0.802713987473904,"• The authors should provide instructions on data access and preparation, including how
697"
OPEN ACCESS TO DATA AND CODE,0.8037578288100209,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
698"
OPEN ACCESS TO DATA AND CODE,0.8048016701461378,"• The authors should provide scripts to reproduce all experimental results for the new
699"
OPEN ACCESS TO DATA AND CODE,0.8058455114822547,"proposed method and baselines. If only a subset of experiments are reproducible, they
700"
OPEN ACCESS TO DATA AND CODE,0.8068893528183716,"should state which ones are omitted from the script and why.
701"
OPEN ACCESS TO DATA AND CODE,0.8079331941544885,"• At submission time, to preserve anonymity, the authors should release anonymized
702"
OPEN ACCESS TO DATA AND CODE,0.8089770354906054,"versions (if applicable).
703"
OPEN ACCESS TO DATA AND CODE,0.8100208768267223,"• Providing as much information as possible in supplemental material (appended to the
704"
OPEN ACCESS TO DATA AND CODE,0.8110647181628392,"paper) is recommended, but including URLs to data and code is permitted.
705"
OPEN ACCESS TO DATA AND CODE,0.8121085594989561,"6. Experimental Setting/Details
706"
OPEN ACCESS TO DATA AND CODE,0.8131524008350731,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
707"
OPEN ACCESS TO DATA AND CODE,0.81419624217119,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
708"
OPEN ACCESS TO DATA AND CODE,0.8152400835073069,"results?
709"
OPEN ACCESS TO DATA AND CODE,0.8162839248434238,"Answer: [Yes]
710"
OPEN ACCESS TO DATA AND CODE,0.8173277661795407,"Justification: See Section 4, Appendix A.
711"
OPEN ACCESS TO DATA AND CODE,0.8183716075156576,"Guidelines:
712"
OPEN ACCESS TO DATA AND CODE,0.8194154488517745,"• The answer NA means that the paper does not include experiments.
713"
OPEN ACCESS TO DATA AND CODE,0.8204592901878914,"• The experimental setting should be presented in the core of the paper to a level of detail
714"
OPEN ACCESS TO DATA AND CODE,0.8215031315240083,"that is necessary to appreciate the results and make sense of them.
715"
OPEN ACCESS TO DATA AND CODE,0.8225469728601252,"• The full details can be provided either with the code, in appendix, or as supplemental
716"
OPEN ACCESS TO DATA AND CODE,0.8235908141962421,"material.
717"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.824634655532359,"7. Experiment Statistical Significance
718"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8256784968684759,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
719"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.826722338204593,"information about the statistical significance of the experiments?
720"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8277661795407099,"Answer: [Yes]
721"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8288100208768268,"Justification: We report error bars in main results.
722"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8298538622129437,"Guidelines:
723"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8308977035490606,"• The answer NA means that the paper does not include experiments.
724"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8319415448851775,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
725"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8329853862212944,"dence intervals, or statistical significance tests, at least for the experiments that support
726"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8340292275574113,"the main claims of the paper.
727"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8350730688935282,"• The factors of variability that the error bars are capturing should be clearly stated (for
728"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8361169102296451,"example, train/test split, initialization, random drawing of some parameter, or overall
729"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.837160751565762,"run with given experimental conditions).
730"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8382045929018789,"• The method for calculating the error bars should be explained (closed form formula,
731"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8392484342379958,"call to a library function, bootstrap, etc.)
732"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8402922755741128,"• The assumptions made should be given (e.g., Normally distributed errors).
733"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8413361169102297,"• It should be clear whether the error bar is the standard deviation or the standard error
734"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8423799582463466,"of the mean.
735"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8434237995824635,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
736"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8444676409185804,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
737"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8455114822546973,"of Normality of errors is not verified.
738"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8465553235908142,"• For asymmetric distributions, the authors should be careful not to show in tables or
739"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8475991649269311,"figures symmetric error bars that would yield results that are out of range (e.g. negative
740"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.848643006263048,"error rates).
741"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8496868475991649,"• If error bars are reported in tables or plots, The authors should explain in the text how
742"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8507306889352818,"they were calculated and reference the corresponding figures or tables in the text.
743"
EXPERIMENTS COMPUTE RESOURCES,0.8517745302713987,"8. Experiments Compute Resources
744"
EXPERIMENTS COMPUTE RESOURCES,0.8528183716075156,"Question: For each experiment, does the paper provide sufficient information on the com-
745"
EXPERIMENTS COMPUTE RESOURCES,0.8538622129436325,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
746"
EXPERIMENTS COMPUTE RESOURCES,0.8549060542797495,"the experiments?
747"
EXPERIMENTS COMPUTE RESOURCES,0.8559498956158664,"Answer: [Yes]
748"
EXPERIMENTS COMPUTE RESOURCES,0.8569937369519833,"Justification: See Appendix D.
749"
EXPERIMENTS COMPUTE RESOURCES,0.8580375782881002,"Guidelines:
750"
EXPERIMENTS COMPUTE RESOURCES,0.8590814196242171,"• The answer NA means that the paper does not include experiments.
751"
EXPERIMENTS COMPUTE RESOURCES,0.860125260960334,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
752"
EXPERIMENTS COMPUTE RESOURCES,0.8611691022964509,"or cloud provider, including relevant memory and storage.
753"
EXPERIMENTS COMPUTE RESOURCES,0.8622129436325678,"• The paper should provide the amount of compute required for each of the individual
754"
EXPERIMENTS COMPUTE RESOURCES,0.8632567849686847,"experimental runs as well as estimate the total compute.
755"
EXPERIMENTS COMPUTE RESOURCES,0.8643006263048016,"• The paper should disclose whether the full research project required more compute
756"
EXPERIMENTS COMPUTE RESOURCES,0.8653444676409185,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
757"
EXPERIMENTS COMPUTE RESOURCES,0.8663883089770354,"didn’t make it into the paper).
758"
CODE OF ETHICS,0.8674321503131524,"9. Code Of Ethics
759"
CODE OF ETHICS,0.8684759916492694,"Question: Does the research conducted in the paper conform, in every respect, with the
760"
CODE OF ETHICS,0.8695198329853863,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
761"
CODE OF ETHICS,0.8705636743215032,"Answer: [Yes]
762"
CODE OF ETHICS,0.8716075156576201,"Justification: We confirm the NeurIPS Code of Ethics
763"
CODE OF ETHICS,0.872651356993737,"Guidelines:
764"
CODE OF ETHICS,0.8736951983298539,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
765"
CODE OF ETHICS,0.8747390396659708,"• If the authors answer No, they should explain the special circumstances that require a
766"
CODE OF ETHICS,0.8757828810020877,"deviation from the Code of Ethics.
767"
CODE OF ETHICS,0.8768267223382046,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
768"
CODE OF ETHICS,0.8778705636743215,"eration due to laws or regulations in their jurisdiction).
769"
BROADER IMPACTS,0.8789144050104384,"10. Broader Impacts
770"
BROADER IMPACTS,0.8799582463465553,"Question: Does the paper discuss both potential positive societal impacts and negative
771"
BROADER IMPACTS,0.8810020876826722,"societal impacts of the work performed?
772"
BROADER IMPACTS,0.8820459290187892,"Answer: [Yes]
773"
BROADER IMPACTS,0.8830897703549061,"Justification: We describe a potential social impact in Introduction.
774"
BROADER IMPACTS,0.884133611691023,"Guidelines:
775"
BROADER IMPACTS,0.8851774530271399,"• The answer NA means that there is no societal impact of the work performed.
776"
BROADER IMPACTS,0.8862212943632568,"• If the authors answer NA or No, they should explain why their work has no societal
777"
BROADER IMPACTS,0.8872651356993737,"impact or why the paper does not address societal impact.
778"
BROADER IMPACTS,0.8883089770354906,"• Examples of negative societal impacts include potential malicious or unintended uses
779"
BROADER IMPACTS,0.8893528183716075,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
780"
BROADER IMPACTS,0.8903966597077244,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
781"
BROADER IMPACTS,0.8914405010438413,"groups), privacy considerations, and security considerations.
782"
BROADER IMPACTS,0.8924843423799582,"• The conference expects that many papers will be foundational research and not tied
783"
BROADER IMPACTS,0.8935281837160751,"to particular applications, let alone deployments. However, if there is a direct path to
784"
BROADER IMPACTS,0.894572025052192,"any negative applications, the authors should point it out. For example, it is legitimate
785"
BROADER IMPACTS,0.8956158663883089,"to point out that an improvement in the quality of generative models could be used to
786"
BROADER IMPACTS,0.8966597077244259,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
787"
BROADER IMPACTS,0.8977035490605428,"that a generic algorithm for optimizing neural networks could enable people to train
788"
BROADER IMPACTS,0.8987473903966597,"models that generate Deepfakes faster.
789"
BROADER IMPACTS,0.8997912317327766,"• The authors should consider possible harms that could arise when the technology is
790"
BROADER IMPACTS,0.9008350730688935,"being used as intended and functioning correctly, harms that could arise when the
791"
BROADER IMPACTS,0.9018789144050104,"technology is being used as intended but gives incorrect results, and harms following
792"
BROADER IMPACTS,0.9029227557411273,"from (intentional or unintentional) misuse of the technology.
793"
BROADER IMPACTS,0.9039665970772442,"• If there are negative societal impacts, the authors could also discuss possible mitigation
794"
BROADER IMPACTS,0.9050104384133612,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
795"
BROADER IMPACTS,0.906054279749478,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
796"
BROADER IMPACTS,0.907098121085595,"feedback over time, improving the efficiency and accessibility of ML).
797"
SAFEGUARDS,0.9081419624217119,"11. Safeguards
798"
SAFEGUARDS,0.9091858037578288,"Question: Does the paper describe safeguards that have been put in place for responsible
799"
SAFEGUARDS,0.9102296450939458,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
800"
SAFEGUARDS,0.9112734864300627,"image generators, or scraped datasets)?
801"
SAFEGUARDS,0.9123173277661796,"Answer: [No]
802"
SAFEGUARDS,0.9133611691022965,"Justification: We do not describe the safeguards
803"
SAFEGUARDS,0.9144050104384134,"Guidelines:
804"
SAFEGUARDS,0.9154488517745303,"• The answer NA means that the paper poses no such risks.
805"
SAFEGUARDS,0.9164926931106472,"• Released models that have a high risk for misuse or dual-use should be released with
806"
SAFEGUARDS,0.9175365344467641,"necessary safeguards to allow for controlled use of the model, for example by requiring
807"
SAFEGUARDS,0.918580375782881,"that users adhere to usage guidelines or restrictions to access the model or implementing
808"
SAFEGUARDS,0.9196242171189979,"safety filters.
809"
SAFEGUARDS,0.9206680584551148,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
810"
SAFEGUARDS,0.9217118997912317,"should describe how they avoided releasing unsafe images.
811"
SAFEGUARDS,0.9227557411273486,"• We recognize that providing effective safeguards is challenging, and many papers do
812"
SAFEGUARDS,0.9237995824634656,"not require this, but we encourage authors to take this into account and make a best
813"
SAFEGUARDS,0.9248434237995825,"faith effort.
814"
LICENSES FOR EXISTING ASSETS,0.9258872651356994,"12. Licenses for existing assets
815"
LICENSES FOR EXISTING ASSETS,0.9269311064718163,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
816"
LICENSES FOR EXISTING ASSETS,0.9279749478079332,"the paper, properly credited and are the license and terms of use explicitly mentioned and
817"
LICENSES FOR EXISTING ASSETS,0.9290187891440501,"properly respected?
818"
LICENSES FOR EXISTING ASSETS,0.930062630480167,"Answer: [Yes]
819"
LICENSES FOR EXISTING ASSETS,0.9311064718162839,"Justification: We use open datasets from GLUE benchmark and open-sourced models and
820"
LICENSES FOR EXISTING ASSETS,0.9321503131524008,"cite them in our work.
821"
LICENSES FOR EXISTING ASSETS,0.9331941544885177,"Guidelines:
822"
LICENSES FOR EXISTING ASSETS,0.9342379958246346,"• The answer NA means that the paper does not use existing assets.
823"
LICENSES FOR EXISTING ASSETS,0.9352818371607515,"• The authors should cite the original paper that produced the code package or dataset.
824"
LICENSES FOR EXISTING ASSETS,0.9363256784968684,"• The authors should state which version of the asset is used and, if possible, include a
825"
LICENSES FOR EXISTING ASSETS,0.9373695198329853,"URL.
826"
LICENSES FOR EXISTING ASSETS,0.9384133611691023,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
827"
LICENSES FOR EXISTING ASSETS,0.9394572025052192,"• For scraped data from a particular source (e.g., website), the copyright and terms of
828"
LICENSES FOR EXISTING ASSETS,0.9405010438413361,"service of that source should be provided.
829"
LICENSES FOR EXISTING ASSETS,0.941544885177453,"• If assets are released, the license, copyright information, and terms of use in the
830"
LICENSES FOR EXISTING ASSETS,0.94258872651357,"package should be provided. For popular datasets, paperswithcode.com/datasets
831"
LICENSES FOR EXISTING ASSETS,0.9436325678496869,"has curated licenses for some datasets. Their licensing guide can help determine the
832"
LICENSES FOR EXISTING ASSETS,0.9446764091858038,"license of a dataset.
833"
LICENSES FOR EXISTING ASSETS,0.9457202505219207,"• For existing datasets that are re-packaged, both the original license and the license of
834"
LICENSES FOR EXISTING ASSETS,0.9467640918580376,"the derived asset (if it has changed) should be provided.
835"
LICENSES FOR EXISTING ASSETS,0.9478079331941545,"• If this information is not available online, the authors are encouraged to reach out to
836"
LICENSES FOR EXISTING ASSETS,0.9488517745302714,"the asset’s creators.
837"
NEW ASSETS,0.9498956158663883,"13. New Assets
838"
NEW ASSETS,0.9509394572025052,"Question: Are new assets introduced in the paper well documented and is the documentation
839"
NEW ASSETS,0.9519832985386222,"provided alongside the assets?
840"
NEW ASSETS,0.9530271398747391,"Answer: [NA]
841"
NEW ASSETS,0.954070981210856,"Justification: We do not release any of the assets
842"
NEW ASSETS,0.9551148225469729,"Guidelines:
843"
NEW ASSETS,0.9561586638830898,"• The answer NA means that the paper does not release new assets.
844"
NEW ASSETS,0.9572025052192067,"• Researchers should communicate the details of the dataset/code/model as part of their
845"
NEW ASSETS,0.9582463465553236,"submissions via structured templates. This includes details about training, license,
846"
NEW ASSETS,0.9592901878914405,"limitations, etc.
847"
NEW ASSETS,0.9603340292275574,"• The paper should discuss whether and how consent was obtained from people whose
848"
NEW ASSETS,0.9613778705636743,"asset is used.
849"
NEW ASSETS,0.9624217118997912,"• At submission time, remember to anonymize your assets (if applicable). You can either
850"
NEW ASSETS,0.9634655532359081,"create an anonymized URL or include an anonymized zip file.
851"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.964509394572025,"14. Crowdsourcing and Research with Human Subjects
852"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.965553235908142,"Question: For crowdsourcing experiments and research with human subjects, does the paper
853"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9665970772442589,"include the full text of instructions given to participants and screenshots, if applicable, as
854"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9676409185803758,"well as details about compensation (if any)?
855"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9686847599164927,"Answer: [NA]
856"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9697286012526096,"Justification: We do not involve crowdsourcing.
857"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9707724425887265,"Guidelines:
858"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9718162839248434,"• The answer NA means that the paper does not involve crowdsourcing nor research with
859"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9728601252609603,"human subjects.
860"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9739039665970772,"• Including this information in the supplemental material is fine, but if the main contribu-
861"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9749478079331941,"tion of the paper involves human subjects, then as much detail as possible should be
862"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.975991649269311,"included in the main paper.
863"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9770354906054279,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
864"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9780793319415448,"or other labor should be paid at least the minimum wage in the country of the data
865"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9791231732776617,"collector.
866"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9801670146137788,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
867"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9812108559498957,"Subjects
868"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9822546972860126,"Question: Does the paper describe potential risks incurred by study participants, whether
869"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9832985386221295,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
870"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9843423799582464,"approvals (or an equivalent approval/review based on the requirements of your country or
871"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9853862212943633,"institution) were obtained?
872"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9864300626304802,"Answer: [NA]
873"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9874739039665971,"Justification: Our paper does not involve research with human subjects
874"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.988517745302714,"Guidelines:
875"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9895615866388309,"• The answer NA means that the paper does not involve crowdsourcing nor research with
876"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9906054279749478,"human subjects.
877"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9916492693110647,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
878"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9926931106471816,"may be required for any human subjects research. If you obtained IRB approval, you
879"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9937369519832986,"should clearly state this in the paper.
880"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9947807933194155,"• We recognize that the procedures for this may vary significantly between institutions
881"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9958246346555324,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
882"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9968684759916493,"guidelines for their institution.
883"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9979123173277662,"• For initial submissions, do not include any information that would break anonymity (if
884"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9989561586638831,"applicable), such as the institution conducting the review.
885"
