Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.00303951367781155,"Classiﬁers in machine learning are often reduced to single dimensional quantities,
such as test error or loss. Here, we initiate a much richer study of classiﬁers by
considering the entire joint distribution of their inputs and outputs. We present
both new empirical behaviors of standard classiﬁers, as well as quantitative conjec-
tures which capture these behaviors. Informally, our conjecture states: the output
distribution of an interpolating classiﬁer matches the distribution of true labels,
when conditioned on certain subgroups of the input space. For example, if we
mislabel 30% of dogs as cats in the train set of CIFAR-10, then a ResNet trained
to interpolation will in fact mislabel roughly 30% of dogs as cats on the test set
as well, while leaving other classes unaffected. This conjecture has implications
for the theory of overparameterization, scaling limits, implicit bias, and statistical
consistency. Further, it can be seen as a new kind of generalization, which goes
beyond measuring single-dimensional quantities to measuring entire distributions."
INTRODUCTION,0.0060790273556231,"1
INTRODUCTION"
INTRODUCTION,0.00911854103343465,"In learning theory, when we study how well a classiﬁer “generalizes”, we usually consider a single
metric – its test error (Shalev-Shwartz & Ben-David, 2014). However, there could be many different
classiﬁers with the same test error that differ substantially in, say, the subgroups of inputs on which
they make errors. Reducing classiﬁers to a single number misses these rich aspects of their behavior.
In this work, we propose formally studying the entire joint distribution of classiﬁer inputs and outputs.
That is, the distribution (x, f(x)) for samples from the distribution x ∼D for a classiﬁer f(x). This
distribution reveals many structural properties of the classiﬁer beyond test error (such as where the
errors occur). In fact, we discover new behaviors of modern classiﬁers that can only be understood in
this framework. As an example, consider the following experiment (Figure 1).
Experiment 1. Consider a binary classiﬁcation version of CIFAR-10, where CIFAR-10 images x
have binary labels Animal/Object. Take 50K samples from this distribution as a train set, but
apply the following label noise: ﬂip the label of cats to Object with probability 30%. Now train
a WideResNet f to 0 train error on this train set. How does the trained classiﬁer behave on test
samples? Options below:"
INTRODUCTION,0.0121580547112462,"(1) The test error is low across all classes, since there is only 3% overall label noise in the train set."
INTRODUCTION,0.015197568389057751,"(2) Test error is “spread” across the animal class. After all, the classiﬁer is not explicitly told what a
cat or a dog is, just that they are all animals."
INTRODUCTION,0.0182370820668693,"(3) The classiﬁer misclassiﬁes roughly 30% of test cats as “objects”, but all other animals are largely
unaffected."
INTRODUCTION,0.02127659574468085,"The reality is closest to option (3) as shown in Figure 1. The left panel shows the joint density of
train inputs x with train labels Object/Animal. The right panel shows the classiﬁer predictions
f(x) on test inputs x."
INTRODUCTION,0.0243161094224924,"There are several notable things about this experiment. First, the error is localized to cats in the test
set as it was in the train set, even though no explicit cat labels were provided. The interpolating
model is thus sensitive to subgroup-structures in the distribution. Second, the amount of error on
the cat class is close to the noise applied on the train set. Thus, the behavior of the classiﬁer on the
train set generalizes to the test set in a stronger sense than just average error. Speciﬁcally, when"
INTRODUCTION,0.02735562310030395,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.030395136778115502,"Figure 1: The setup and result of Experiment 1. The CIFAR-10 train set is labeled as either Animals
or Objects, with label noise affecting only cats. A WideResNet-28-10 is then trained to 0 train error
on this train set, and evaluated on the test set. Full experimental details in Appendix C.2"
INTRODUCTION,0.03343465045592705,"conditioned on a subgroup (cat), the distribution of the true labels is close to that of the classiﬁer
outputs. Third, this is not the behavior of the Bayes-optimal classiﬁer, which would always output
the maximum-likelihood label instead of reproducing the noise in the distribution. The network
is thus behaving poorly from the perspective of Bayes-optimality, but behaving well in a certain
distributional sense (which we will formalize soon)."
INTRODUCTION,0.0364741641337386,"Now, consider a seemingly unrelated experimental observation. Take an AlexNet trained on ImageNet,
a 1000-way classiﬁcation problem with 116 varieties of dogs. AlexNet only achieves 56.5% test
accuracy on ImageNet. However, it at least classiﬁes most dogs as some variety of dog (with 98.4%
accuracy), though it may mistake the exact breed. In this work, we show that both of these experiments
are examples of the same underlying phenomenon. We empirically show that for an interpolating
classiﬁer, its classiﬁcation outputs are close in distribution to the true labels — even when conditioned
on many subsets of the domain. For example, in Figure 1, the distribution of p(f(x)|x = cat) is close
to the true label distribution of p(y|x = cat). We propose a formal conjecture (Feature Calibration),
that predicts which subgroups of the domain can be conditioned on for the above distributional
closeness to hold."
INTRODUCTION,0.03951367781155015,"These experimental behaviors could not have been captured solely by looking at average test error,
as is done in the classical theory of generalization. In fact, they are special cases of a new kind of
generalization, which we call “Distributional Generalization”."
DISTRIBUTIONAL GENERALIZATION,0.0425531914893617,"1.1
DISTRIBUTIONAL GENERALIZATION"
DISTRIBUTIONAL GENERALIZATION,0.04559270516717325,"Informally, Distributional Generalization states that the outputs of classiﬁers f on their train sets
and test sets are close as distributions (as opposed to close in just error). That is, the following joint
distributions1 are close:
(x, f(x))x∼TestSet ≈(x, f(x))x∼TrainSet
(1)"
DISTRIBUTIONAL GENERALIZATION,0.0486322188449848,"The remainder of this paper is devoted to making the above statement precise, and empirically
checking its validity on real-world tasks. Speciﬁcally, we want to formally deﬁne the notion of
approximation (≈), and understand how it depends on the problem parameters (the type of classiﬁer,
number of train samples, etc). We focus primarily on interpolating methods, where we formalize
Equation (1) through our Feature Calibration Conjecture."
OUR CONTRIBUTIONS AND ORGANIZATION,0.05167173252279635,"1.2
OUR CONTRIBUTIONS AND ORGANIZATION"
OUR CONTRIBUTIONS AND ORGANIZATION,0.0547112462006079,"We discover new empirical behaviors of interpolating classiﬁers, and we propose quantitative conjec-
tures to characterize these behaviors."
OUR CONTRIBUTIONS AND ORGANIZATION,0.057750759878419454,"• In Section 3, we introduce a “Feature Calibration” conjecture, which uniﬁes our experimental
observations. Roughly, Feature Calibration says that the outputs of classiﬁers match the
statistics of their training distribution when conditioned on certain subgroups."
OUR CONTRIBUTIONS AND ORGANIZATION,0.060790273556231005,"• In Section 4, we experimentally stress test our Feature Calibration conjecture across various
settings in machine learning, including neural networks, kernel machines, and decision trees.
This highlights the universality of our results across machine learning."
OUR CONTRIBUTIONS AND ORGANIZATION,0.06382978723404255,"1These distributions also include the randomness in sampling the train and test sets, and in training the
classiﬁer, as we deﬁne more precisely in Section 3."
OUR CONTRIBUTIONS AND ORGANIZATION,0.0668693009118541,Under review as a conference paper at ICLR 2022
OUR CONTRIBUTIONS AND ORGANIZATION,0.06990881458966565,"• In Section 5, we relate our results to classical generalization, by deﬁning a new notion of
Distributional Generalization which extends and subsumes the classical notion."
OUR CONTRIBUTIONS AND ORGANIZATION,0.0729483282674772,"• Finally, in Section 5.2 we informally discuss how Distributional Generalization can be
applied even for non-interpolating methods."
OUR CONTRIBUTIONS AND ORGANIZATION,0.07598784194528875,"Our results extend our scientiﬁc understanding of of interpolating methods, and introduce a new type
of generalization exhibited across many methods in machine learning."
RELATED WORK AND SIGNIFICANCE,0.0790273556231003,"1.3
RELATED WORK AND SIGNIFICANCE"
RELATED WORK AND SIGNIFICANCE,0.08206686930091185,"Our work has connections to, and implications for many existing research programs in deep learning."
RELATED WORK AND SIGNIFICANCE,0.0851063829787234,"Implicit Bias and Overparameterization. There has been a long line of recent work towards
understanding overparameterized and interpolating methods, since these pose challenges for classical
theories of generalization (e.g. Zhang et al. (2016); Belkin et al. (2018a;b; 2019); Liang & Rakhlin
(2018); Nakkiran et al. (2020); Schapire et al. (1998); Breiman (1995); Soudry et al. (2018); Gunasekar
et al. (2018)). The “implicit bias” program here aims to answer: Among all models with 0 train error,
which model is actually produced by SGD? Most existing work seeks to characterize the exact implicit
bias of models under certain (sometimes strong) assumptions on the model, training method or the
data distribution. In contrast, our conjecture applies across many different interpolating models (from
neural nets to decision trees) as they would be used in practice, and thus form a sort of “universal
implicit bias” of these methods. Moreover, our results place constraints on potential future theories
of implicit bias, and guide us towards theories that better capture practice."
RELATED WORK AND SIGNIFICANCE,0.08814589665653495,"Benign Overﬁtting. Most prior works on interpolating classiﬁers attempt to explain why training to
interpolation “does not harm” the the model. This has been dubbed “benign overﬁtting” (Bartlett
et al., 2020) and “harmless interpolation” (Muthukumar et al., 2020), reﬂecting the widely-held
belief that interpolation does not harm the decision boundary of classiﬁers. In contrast, we ﬁnd that
interpolation actually does “harm” classiﬁers, in predictable ways: ﬁtting the label noise on the train
set causes similar noise to be reproduced at test time. Our results thus indicate that interpolation
can signiﬁcantly affect the decision boundary of classiﬁers, and should not be considered a purely
“benign” effect."
RELATED WORK AND SIGNIFICANCE,0.0911854103343465,"Classical Generalization and Scaling Limits. Our framework of Distributional Generalization is
insightful even to study classical generalization, since it reveals much more about models than just
their test error. For example, statistical learning theory attempts to understand if and when models
will asymptotically converge to Bayes optimal classiﬁers, in the limit of large data (“asymptotic
consistency” Shalev-Shwartz & Ben-David (2014); Wasserman (2013)). In deep learning, there
are at least two distinct ways to scale model and data to inﬁnity together: the underparameterized
scaling limit, where data-size ≫model-size always, and the overparameterized scaling limit, where
data-size ≪model-size always. The underparameterized scaling limit is well-understood: when
data is essentially inﬁnite, neural networks will converge to the Bayes-optimal classiﬁer (provided
the model-size is large enough, and the optimization is run for long enough, with enough noise
to escape local minima). On the other hand, our work suggests that in the overparameterized
scaling limit, models will not converge to the Bayes-optimal classiﬁer. Speciﬁcally, our Feature
Calibration Conjecture implies that in the limit of large data, interpolating models will approach a
sampler from the distribution. That is, the limiting model f will be such that the output f(x) is a
sample from p(y|x), as opposed to the Bayes-optimal f ∗(x) = argmaxy p(y|x). This claim— that
overparameterized models do not converge to Bayes-optimal classiﬁers— is unique to our work as
far as we know, and highlights the broad implications of our results."
RELATED WORK AND SIGNIFICANCE,0.09422492401215805,"Locality and Manifold Learning. Our intuition for the behaviors in this work is that they arise due to
some form of “locality” of the trained classiﬁers, in an appropriate embedding space. For example, the
behavior observed in Experiment 1 would be consistent with that of a 1-Nearest-Neighbor classiﬁer
in a embedding that separates the CIFAR-10 classes well. This intuition that classiﬁers learn good
embeddings is present in various forms in the literature, for example: the so-called called “manifold
hypothesis,” that natural data lie on a low-dimensional manifold (Narayanan & Mitter, 2010; Sharma
& Kaplan, 2020), as well as works on local stiffness of the loss landscape (Fort et al., 2019), and
works showing that overparameterized neural networks can learn hidden low-dimensional structure"
RELATED WORK AND SIGNIFICANCE,0.0972644376899696,Under review as a conference paper at ICLR 2022
RELATED WORK AND SIGNIFICANCE,0.10030395136778116,"in high-dimensional settings (Gerace et al., 2020; Bach, 2017; Chizat & Bach, 2020). It is open to
more formally understand connections between our work and the above."
RELATED WORK AND SIGNIFICANCE,0.1033434650455927,"Uncertainty Estimation. Since the appearance of the current work on arXiv, it has been directly
built on by other authors. The work of Jiang et al. (2021) investigates our conjectures further, and
extends them to develop a method for out-of-distribution uncertainty estimation. This highlights the
fundamental nature and importance of our results, since they have already been used in a practical
application. A full discussion of related works is in Appendix A."
PRELIMINARIES,0.10638297872340426,"2
PRELIMINARIES"
PRELIMINARIES,0.1094224924012158,"Notation. We consider joint distributions D on x ∈X and discrete y ∈Y = [k]. Let S =
{(xi, yi)}n
i=1 ∼Dn denote a train set of n iid samples from D. Let A denote the training procedure
(including architecture and training algorithm for neural networks), and let f ←TrainA(S) denote
training a classiﬁer f on train-set S using procedure A. We consider classiﬁers which output hard
decisions f : X →Y. Let NNS(x) = xi denote the nearest-neighbor to x in train-set S, with
respect to a distance metric d. Our theorems will apply to any distance metric, and so we leave
this unspeciﬁed. Let NN(y)
S (x) denote the nearest-neighbor estimator itself, that is, NN(y)
S (x) := yi
where xi = NNS(x)."
PRELIMINARIES,0.11246200607902736,"Experimental Setup. Brieﬂy, we train all classiﬁers to interpolation (to 0 train error). Neural
networks (MLPs and ResNets (He et al., 2016)) are trained with SGD. Interpolating decision trees
are trained using the growth rule from Random Forests (Breiman, 2001). For kernel classiﬁcation,
we consider kernel regression on one-hot labels and kernel SVM, with small or 0 of regularization
(which is often optimal (Shankar et al., 2020)). Full experimental details are provided in Appendix B."
PRELIMINARIES,0.11550151975683891,"Distributional Closeness. We formalize distributional closeness using the notion of Integral Prob-
ability Metrics (M¨uller, 1997), which we review here. For two distributions P, Q over X × Y, let
a “test” (or “distinguisher”) be a function T : X × Y →[0, 1] which accepts a sample from either
distribution, and is intended to classify the sample as either from distribution P or Q. For any set
C ⊆{T : X × Y →[0, 1]} of tests, we say distributions P and Q are “ε-indistinguishable up to
C-tests” if they are close with respect to all tests in class C. That is,"
PRELIMINARIES,0.11854103343465046,"P ≈C
ε Q ⇐⇒sup
T ∈C"
PRELIMINARIES,0.12158054711246201,"E
(x,y)∼P[T(x, y)] −
E
(x,y)∼Q[T(x, y)]
 ≤ε
(2)"
PRELIMINARIES,0.12462006079027356,"Total-Variation distance is equivalent to closeness in all tests, i.e. C = {T : X × Y →[0, 1]}, but we
consider closeness for restricted families of tests C. P ≈ε Q denotes ε-closeness in TV-distance."
FEATURE CALIBRATION CONJECTURE,0.1276595744680851,"3
FEATURE CALIBRATION CONJECTURE"
DISTRIBUTIONS OF INTEREST,0.13069908814589665,"3.1
DISTRIBUTIONS OF INTEREST"
DISTRIBUTIONS OF INTEREST,0.1337386018237082,"We ﬁrst deﬁne three key distributions that we will use in stating our formal conjecture. For a given
data distribution D over X × Y and training procedure TrainA, we consider the following three
distributions over X × Y:"
DISTRIBUTIONS OF INTEREST,0.13677811550151975,"1. Source D: (x, y) where x, y ∼D."
DISTRIBUTIONS OF INTEREST,0.1398176291793313,"2. Train Dtr: (xtr, f(xtr)) where S ∼Dn, f ←TrainA(S), (xtr, ytr) ∼S"
DISTRIBUTIONS OF INTEREST,0.14285714285714285,"3. Test Dte: (x, f(x)) where S ∼Dn, f ←TrainA(S), x, y ∼D"
DISTRIBUTIONS OF INTEREST,0.1458966565349544,"The source distribution D is simply the original distribution. To sample once from the Train Dis-
tribution Dtr, we ﬁrst sample a train set S ∼Dn, train a classiﬁer f on it, then output (xtr, f(xtr))
for a random train point xtr ∈S. That is, Dtr is the distribution of input and outputs of a trained
classiﬁer f on its train set. To sample once from the Test Distribution Dte, we do this same proce-
dure, but output (x, f(x)) for a random test point x. That is, the Dte is the distribution of input and
outputs of a trained classiﬁer f at test time. The only difference between the Train Distribution and"
DISTRIBUTIONS OF INTEREST,0.14893617021276595,Under review as a conference paper at ICLR 2022
DISTRIBUTIONS OF INTEREST,0.1519756838905775,"Test Distribution is that the point x is sampled from the train set or the test set, respectively.2 For
interpolating classiﬁers, f(xtr) = ytr on the train set, and so the Source and Train distributions are
equivalent: D ≡Dtr. (Note that these deﬁnitions, crucially, involve randomness from sampling the
train set, training the classiﬁer, and sampling a test point)."
FEATURE CALIBRATION,0.15501519756838905,"3.2
FEATURE CALIBRATION"
FEATURE CALIBRATION,0.1580547112462006,"We now formally describe the Feature Calibration Conjecture. At a high level, we argue that the
distributions Dte and D are statistically close for interpolating classiﬁers if we ﬁrst “coarsen” the
domain of x by some partition L : X →[M] in to M parts. That is, for certain partitions L, the
following distributions are statistically close:"
FEATURE CALIBRATION,0.16109422492401215,"(L(x), f(x))x∼D ≈ε (L(x), y)x∼D"
FEATURE CALIBRATION,0.1641337386018237,"We think of L as deﬁning subgroups over the domain— for example, L(x) ∈{dog, cat, horse...}.
Then, the above statistical closeness is essentially equivalent to requiring that for all subgroups
ℓ∈[M], the conditional distribution of classiﬁer output on the subgroup—p(f(x)|L(x) = ℓ) — is
close to the true conditional distribution: p(y|L(x) = ℓ)."
FEATURE CALIBRATION,0.16717325227963525,"The crux of our conjecture lies in deﬁning exactly which subgroups L satisfy this distributional
closeness, and quantifying the ε approximation. This is subtle, since it must depend on almost all
parameters of the problem. For example, consider a modiﬁcation to Experiment 1, where we use
a fully-connected network (MLP) instead of a ResNet. An MLP cannot properly distinguish cats
even when it is actually provided the real CIFAR-10 labels, and so (informally) it has no hope of
behaving differently on cats in the setting of Experiment 1, where the cats are not labeled explicitly
(See Figure C.2 for results with MLPs). Similarly, if we train the ResNet with very few samples from
the distribution, the network will be unable to recognize cats. Thus, the allowable partitions must
depend on the classiﬁer family and the training method, including the number of samples."
FEATURE CALIBRATION,0.1702127659574468,"We conjecture that allowable partitions are those which can themselves be learnt to good test
performance with an identical training procedure, but trained with the labels of the partition L instead
of y. To formalize this, we deﬁne a distinguishable feature: a partition of the domain X that is
learnable for a given training procedure. Thus, in Experiment 1, the partition into CIFAR-10 classes
would be a distinguishable feature for ResNets (trained with SGD with 50K or more samples), but
not for MLPs. The deﬁnition below depends on the training procedure A, the data distribution D,
number of train samples n, and an approximation parameter ε (which we think of as ε ≈0)."
FEATURE CALIBRATION,0.17325227963525835,"Deﬁnition 1 ((ε, A, D, n)-Distinguishable Feature). For a distribution D over X × Y, number of
samples n, training procedure A, and small ε ≥0, an (ε, A, D, n)-distinguishable feature is a
partition L : X →[M] of the domain X into M parts, such that training a model using A on n
samples labeled by L works to classify L with high test accuracy. Precisely, L is a (ε, A, D, n)-
distinguishable feature if:"
FEATURE CALIBRATION,0.1762917933130699,"Pr
S={(xi,L(xi)}x1,...,xn∼D
f←TrainA(S); x∼D"
FEATURE CALIBRATION,0.17933130699088146,[f(x) = L(x)] ≥1 −ε
FEATURE CALIBRATION,0.182370820668693,"This deﬁnition depends only on the marginal distribution of D on x, and not on the label distribution
pD(y|x). To recap, this deﬁnition is meant to capture a labeling of the domain X that is learnable for
a given training procedure A. It must depend on the architecture used by A and number of samples
n, since more powerful classiﬁers can distinguish more features. Note that there could be many
distinguishable features for a given setting (ε, A, D, n) — including features not implied by the class
label such as the presence of grass in a CIFAR-10 image. Our main conjecture follows."
FEATURE CALIBRATION,0.18541033434650456,"Conjecture 1 (Feature Calibration). For all natural distributions D, number of samples n, interpo-
lating training procedures A, and ε ≥0, the following distributions are statistically close for all
(ε, A, D, n)-distinguishable features L:"
FEATURE CALIBRATION,0.1884498480243161,"(L(x), f(x))
f←TrainA(Dn); x,y∼D
≈ε
(L(x), y)
x,y∼D
(3)"
FEATURE CALIBRATION,0.19148936170212766,"2Technically, these deﬁnitions require training a fresh classiﬁer for each sample, using independent train sets.
For practical reasons most of our experiments train a single classiﬁer f and evaluate it on the entire train/test set."
FEATURE CALIBRATION,0.1945288753799392,Under review as a conference paper at ICLR 2022
FEATURE CALIBRATION,0.19756838905775076,or equivalently:
FEATURE CALIBRATION,0.2006079027355623,"(L(x), by)
x,by∼Dte
≈ε
(L(x), y)
x,y∼D
(4)"
FEATURE CALIBRATION,0.20364741641337386,"This claims that the TV distance between the LHS and RHS of Equation (4) is at most ε, where ε is the
error of the distinguishable feature (in Deﬁnition 1). We claim that this holds for all distinguishable
features L “automatically” – we simply train a classiﬁer, without specifying any particular partition.
The formal statements of Deﬁnition 1 and Conjecture 1 may seem somewhat arbitrary, involving
many quantiﬁers over (ε, A, D, n). However, we believe these statements are natural: In addition
to extensive experimental evidence in Section 4, we also prove that Conjecture 1 is formally true as
stated for 1-Nearest-Neighbor classiﬁers in Theorem 1."
FEATURE CALIBRATION,0.2066869300911854,"3.3
FEATURE CALIBRATION FOR 1-NEAREST-NEIGHBORS"
FEATURE CALIBRATION,0.20972644376899696,"Here we prove that the 1-Nearest-Neighbor classiﬁer formally satisﬁes Conjecture 1, under mild
assumptions. We view this theorem as support for our (somewhat involved) formalism of Conjecture 1.
Indeed, without Theorem 1 below, it is unclear if our statement of Conjecture 1 can ever be satisﬁed
by any classiﬁer, or if it is simply too strong to be true. This theorem applies generically to a wide
class of distributions; the only assumption is a weak regularity condition. The proof of Theorem 1 is
straightforward, and provided in Appendix D – but this strong property of nearest-neighbors was not
know before, to our knowledge.
Theorem 1. Let D be a distribution over X × Y, and let n ∈N be the number of train samples.
Assume the following regularity condition holds: Sampling the nearest-neighbor train point to a
random test point yields (close to) a uniformly random test point. That is, suppose that for some
small δ ≥0, the distributions: {NNS(x)}S∼Dn
x∼D
≈δ
{x}x∼D. Then, Conjecture 1 holds. That is,"
FEATURE CALIBRATION,0.2127659574468085,"for all (ε, NN, D, n)-distinguishable partitions L, the following distributions are statistically close:"
FEATURE CALIBRATION,0.21580547112462006,"{(y, L(x))}x,y∼D
≈ε+δ
{(NN(y)
S (x), L(x)}S∼Dn
x,y∼D
(5)"
FEATURE CALIBRATION,0.2188449848024316,"3.4
LIMITATIONS: NATURAL DISTRIBUTIONS"
FEATURE CALIBRATION,0.22188449848024316,"Technically, Conjecture 1 is not fully speciﬁed, since it does not specify exactly which classiﬁers or
distributions obey the conjecture. We do not claim that all classiﬁers and distributions satisfy our
conjectures. Nevertheless, we claim our conjectures hold in all “natural” settings, which informally
means settings with real data and classiﬁers that are actually used in practice. The problem of
understanding what separates “natural distributions” from artiﬁcial ones is not unique to our work,
and lies at the heart of deep learning theory. Many theoretical works handle this by considering
simpliﬁed distributional assumptions (e.g. smoothness, well-separatedness, gaussianity), which are
mathematically tractable, but unrealistic in practice (Arora et al., 2019; Li et al., 2019; Allen-Zhu
et al., 2018). In contrast, we do not make unrealistic mathematical assumptions. This beneﬁt of
realism comes at the cost of mathematical formalism. We hope that as the theory of deep learning
evolves, we will better understand how to formalize the notion of “natural” in our conjectures."
FEATURE CALIBRATION,0.22492401215805471,"4
EXPERIMENTS: FEATURE CALIBRATION"
FEATURE CALIBRATION,0.22796352583586627,"We now give empirical evidence for our conjecture in a variety of settings in machine learning. In
each experiment, we consider a feature that is (veriﬁably) distinguishable, and then test our Feature
Calibration conjecture for this feature. Each of the experimental settings below highlights a different
aspect of interpolating classiﬁers, which may be of independent interest. Selected experiments are
summarized here, with full details and further experiments in Appendix C."
FEATURE CALIBRATION,0.23100303951367782,"Constant Partition: Consider the trivially-distinguishable constant feature: L(x) = 0 everywhere.
For this feature, Conjecture 1 reduces to the statement that the marginal distribution of class labels for
any interpolating classiﬁer is close to the true marginals p(y). To test this, we construct a variant of
CIFAR-10 with class-imbalance and train classiﬁers with varying levels of test errors to interpolation
on it. As shown in Figure 2B, the marginals of the classiﬁer outputs are close to the true marginals,
even for a classiﬁer that only achieves 37% test error."
FEATURE CALIBRATION,0.23404255319148937,Under review as a conference paper at ICLR 2022
FEATURE CALIBRATION,0.23708206686930092,"Figure 2: Feature Calibration. (A) Random confusion matrix on CIFAR-10, with a WideResNet28-
10 trained to interpolation. Left: Joint density of labels y and original class L on the train set. Right:
Joint density of classiﬁer predictions f(x) and original class L on the test set. These two joint
densities are close, as predicted by Conjecture 1. (B) Constant partition: The CIFAR-10 train set is
class-rebalanced according to the left panel distribution. The center and right panels show that both
ResNets and MLPs have the correct marginal distribution of outputs, even though the MLP has high
test error."
FEATURE CALIBRATION,0.24012158054711247,"Figure 3: Feature Calibration. (A) CIFAR-10 with p fraction of class 0 →1 mislabeled on the
train set. Plotting observed noise on classiﬁer outputs vs. applied noise on the train set. (B) Multiple
feature calibration on CelebA. (C) TV-distance between (L(x), f(x)) and (L(x), y) for a variant of
Experiment 1 with error on the distinguishable partitions (ε). The error was changed by changing the
number of samples n."
FEATURE CALIBRATION,0.24316109422492402,"Coarse Partition: Consider AlexNet trained on ILSVRC-2012 ImageNet (Russakovsky et al., 2015),
a 1000-class image classiﬁcation problem with 116 varieties of dogs. The network achieves only
56.5% accuracy on the test set. But it will at least classify most dogs as dogs (with 98.4% accuracy),
making L(x) ∈{dog, not-dog} a distinguishable feature. Moreover, as predicted by Conjecture 1,
the network is calibrated with respect to dogs: 22.4% of all dogs in ImageNet are Terriers, and indeed
the network classiﬁes 20.9% of all dogs as Terriers (though it has 9% error on which speciﬁc dogs
it classiﬁes as Terriers). See Appendix Table 2 for details, and related experiments on ResNets and
kernels in Appendix C."
FEATURE CALIBRATION,0.24620060790273557,"Class Partition: We now consider settings where the class labels are themselves distinguishable
features (eg: CIFAR-10 classes are distinguishable by ResNets). Here our conjecture predicts the
behavior of interpolating classiﬁers under structured label noise. As an example, we generate a
random spare confusion matrix and apply this to the labels of CIFAR-10 as shown in Figure 2A.
We ﬁnd that a WideResNet trained to interpolation outputs the same confusion matrix on the test
set as well (Figure 2B). Now, to test that this phenomenon is indeed robust to the level of noise, we
mislabel class 0 →1 with probability p in the CIFAR-10 train set for varying levels of p. We then
observe bp, the fraction of samples mislabeled by this network from 0 →1 in the test set (Figure 3A
shows p versus bp). The Bayes optimal classiﬁer for this distribution behaves as a step function (in
red), and a classiﬁer that obeys Conjecture 1 exactly would follow the diagonal (in green). The actual
experiment (in blue) is close to the behavior predicted by Conjecture 1. This experiment shows a
contrast with classical learning theory. While most existing theory focuses on whether classiﬁers
converge to the Bayes optimal solution, we show that interpolating classiﬁers behave “optimally” in a
different sense: they match the distribution of their train set. We discuss this further in Section 5. See
Appendix C.4 for more experiments, including other classiﬁers such as Decisions Trees."
FEATURE CALIBRATION,0.24924012158054712,Under review as a conference paper at ICLR 2022
FEATURE CALIBRATION,0.25227963525835867,"Multiple features: Conjecture 1 states that the network should be automatically calibrated for
all distinguishable features, without any explicit labels for them. To do this, we use the CelebA
dataset (Liu et al., 2015), containing images with many binary attributes per image. (“male”, “blond
hair”, etc). We train a ResNet-50 to classify one of the hard attributes (accuracy 80%) and conﬁrm that
the Feature Calibration holds for all the other attributes (Figure 3) that are themselves distinguishable."
FEATURE CALIBRATION,0.2553191489361702,"Quantitative predictions: We now test the quantitative predictions made by Conjecture 1. This
conjecture states that the TV-distance between the joint distributions (L(x), f(x)) and (L(x), y) is at
most ε, where ε is the error of the training procedure in learning L (see Deﬁnition 1). To test this, we
consider binary task similar to Experiment 1 where (Ship, Plane) are labeled as class 0 and
(Cat, Dog) are labeled as class 1, with p = 0.3 fraction of cats mislabeled to class 0. Then, we
train a network to interpolation on this task. To vary the error ε systematically, we train networks
with varying number of train samples. Networks with fewer samples have larger ε since they are
worse at classifying the distinguishable features of (Ship,Plane,Cat,Dog). Then, we use the
same setup to train networks on the binary task and measure the TV-distance between (L(x), f(x))
and (L(x), y) in this task. The results are shown in Figure 3C. As predicted, the TV distance on the
binary task is upper bounded by ε error on the 4-way classiﬁcation task."
DISTRIBUTIONAL GENERALIZATION,0.25835866261398177,"5
DISTRIBUTIONAL GENERALIZATION"
DISTRIBUTIONAL GENERALIZATION,0.2613981762917933,"In order to relate our results to the classical theory of generalization, we now propose a formal
deﬁnition of “Distributional Generalization”, which subsumes both Feature Calibration and classical
generalization. A trained model f obeys classical generalization (with respect to test error) if its error
on the train set is close to its error on the test distribution. We ﬁrst rewrite this using our deﬁnitions
below."
DISTRIBUTIONAL GENERALIZATION,0.26443768996960487,Classical Generalization (informal): Let f be a trained classiﬁer. Then f generalizes if:
DISTRIBUTIONAL GENERALIZATION,0.2674772036474164,"E
x∼TrainSet
by←f(x)
[1{by ̸= y(x)}] ≈
E
x∼TestSet
by←f(x)
[1{by ̸= y(x)}]
(6)"
DISTRIBUTIONAL GENERALIZATION,0.270516717325228,"Above, y(x) is the true class of x and by is the predicted class. The LHS of Equation 6 is the train
error of f, and the RHS is the test error. Using our deﬁnitions of Dtr, Dte from Section 3.1, and
deﬁning Terr(x, by) := 1{by ̸= y(x)}, we can write Equation 6 equivalently:"
DISTRIBUTIONAL GENERALIZATION,0.2735562310030395,"E
x,by∼Dtr
[Terr(x, by)] ≈
E
x,by∼Dte
[Terr(x, by)]
(7)"
DISTRIBUTIONAL GENERALIZATION,0.2765957446808511,"That is, classical generalization states that a certain function (Terr) has similar expectations on both the
Train Distribution Dtr and Test Distribution Dte. We can now introduce Distributional Generalization,
which is a property of trained classiﬁers. It is parameterized by a set of bounded functions (“tests”):
T ⊆{T : X × Y →[0, 1]}."
DISTRIBUTIONAL GENERALIZATION,0.2796352583586626,"Distributional Generalization: Let f be a trained classiﬁer. Then f satisﬁes Distributional Gener-
alization with respect to tests T if:"
DISTRIBUTIONAL GENERALIZATION,0.2826747720364742,"∀T ∈T :
E
x,by∼Dtr
[T(x, by)] ≈
E
x,by∼Dte
[T(x, by)]
(8)"
DISTRIBUTIONAL GENERALIZATION,0.2857142857142857,"This states that the train and test distribution have similar expectations for all functions in the family
T , which we can write as: Dtr ≈T Dte. For the singleton set T = {Terr}, this is equivalent to
classical generalization, but it may hold for much larger sets T . This deﬁnition of Distributional
Generalization, like the deﬁnition of classical generalization, is just deﬁning an object— not stating
when or how it is satisﬁed. Feature Calibration turns this into a concrete conjecture."
FEATURE CALIBRATION AS DISTRIBUTIONAL GENERALIZATION,0.2887537993920973,"5.1
FEATURE CALIBRATION AS DISTRIBUTIONAL GENERALIZATION"
FEATURE CALIBRATION AS DISTRIBUTIONAL GENERALIZATION,0.2917933130699088,"We can write our Feature Calibration Conjecture as a special case of Distributional Generalization,
for a certain family of tests T . Informally, for a given setting, the family T is all tests which take
input (x, y), but only depend on x via a distinguishable feature (Deﬁnition 1). For example, a test
of the form T(x, y) = g(L(x), y) where L is a distinguishable feature, and g is arbitrary. Formally,"
FEATURE CALIBRATION AS DISTRIBUTIONAL GENERALIZATION,0.2948328267477204,Under review as a conference paper at ICLR 2022
FEATURE CALIBRATION AS DISTRIBUTIONAL GENERALIZATION,0.2978723404255319,"Figure 4: Distributional Generalization for WideResNet on CIFAR-10. The confusion matrices
on the train set (top row) and test set (bottom row) remain close throughout training."
FEATURE CALIBRATION AS DISTRIBUTIONAL GENERALIZATION,0.3009118541033435,"for a given problem setting, suppose L is the set of (ε, A, D, n)-distinguishable features. Then
Conjecture 1 states that ∀L ∈L : (L(x), f(x)) ≈ε (L(x), y). This is equivalent to the statement"
FEATURE CALIBRATION AS DISTRIBUTIONAL GENERALIZATION,0.303951367781155,"Dte ≈T
ε D
(9)"
FEATURE CALIBRATION AS DISTRIBUTIONAL GENERALIZATION,0.3069908814589666,"where T is the set of functions T := {T : T(x, y) = g(L(x), y), L ∈L, g : X × Y →[0, 1]}.
For interpolating classiﬁers, we have D ≡Dtr, and so Equation (9) is equivalent to Dte ≈T
ε Dtr,
which is a statement of Distributional Generalization. Since any classiﬁer family will contain a large
number of distinguishable features, the set L may be very large. Hence, the distributions Dtr and Dte
can be thought of as being close as distributions."
BEYOND INTERPOLATING METHODS,0.3100303951367781,"5.2
BEYOND INTERPOLATING METHODS"
BEYOND INTERPOLATING METHODS,0.3130699088145897,"The previous sections focused on interpolating classiﬁers, which ﬁt their train sets exactly. Here we
informally discuss how to extend our results beyond interpolating methods. The discussion in this
section is not as precise as in previous sections, and is only meant to suggest that our abstraction of
Distributional Generalization can be useful in other settings. For non-interpolating classiﬁers, we
may still expect that they behave similarly on their test and train sets – that is, Dte ≈T Dtr for some
family of tests T . For example, the following is a possible generalization of Feature Calibration.
Conjecture 2 (Generalized Feature Calibration, informal). For trained classiﬁers f, the following
distributions are statistically close for many partitions L of the domain:"
BEYOND INTERPOLATING METHODS,0.3161094224924012,"(L(x), by)
x,by∼Dte
≈
(L(x), by)
x,by∼Dtr
(10)"
BEYOND INTERPOLATING METHODS,0.3191489361702128,"We do not yet understand how to state this conjecture formally, but we give experimental evidence in
its support. In Figure 4, we apply label noise from a random sparse confusion to the CIFAR-10 train
set. We then train a single WideResNet28-10, and measure its predictions on the train and test over
increasing SGD steps. The top row shows the confusion matrix of predictions f(x) vs true labels
L(x) on the train set, and the bottom row shows the corresponding confusion matrix on the test set.
As the network is trained for longer, it ﬁts more of the noise on the train set, and this noise is mirrored
almost identically on the test set. Full experimental details in Appendix B."
CONCLUSION,0.3221884498480243,"6
CONCLUSION"
CONCLUSION,0.3252279635258359,"This work initiates the study of a new kind of generalization— Distributional Generalization— which
considers the entire input-output behavior of classiﬁers, instead of just their test error. We presented
both new empirical behaviors, and new formal conjectures which characterize these behaviors.
Roughly, our conjecture states that the outputs of classiﬁers on the test set are “close in distribution”
to their outputs on the train set. These results build a deeper understanding of models used in practice,
and we hope our results inspire further work on distributional generalization in machine learning."
CONCLUSION,0.3282674772036474,Under review as a conference paper at ICLR 2022
REFERENCES,0.331306990881459,REFERENCES
REFERENCES,0.3343465045592705,"Madhu S Advani and Andrew M Saxe. High-dimensional dynamics of generalization error in neural
networks. arXiv preprint arXiv:1710.03667, 2017."
REFERENCES,0.3373860182370821,"Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized
neural networks, going beyond two layers. arXiv preprint arXiv:1811.04918, 2018."
REFERENCES,0.3404255319148936,"Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized
neural networks, going beyond two layers. In Advances in neural information processing systems,
pp. 6158–6169, 2019."
REFERENCES,0.3434650455927052,"Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. In International
Conference on Machine Learning, pp. 322–332, 2019."
REFERENCES,0.3465045592705167,"Susan Athey, Julie Tibshirani, Stefan Wager, et al. Generalized random forests. The Annals of
Statistics, 47(2):1148–1178, 2019."
REFERENCES,0.3495440729483283,"Francis Bach. Breaking the curse of dimensionality with convex neural networks. The Journal of
Machine Learning Research, 18(1):629–681, 2017."
REFERENCES,0.3525835866261398,"Peter L Bartlett, Philip M Long, G´abor Lugosi, and Alexander Tsigler. Benign overﬁtting in linear
regression. Proceedings of the National Academy of Sciences, 2020."
REFERENCES,0.3556231003039514,"Mikhail Belkin, Daniel J Hsu, and Partha Mitra. Overﬁtting or perfect ﬁtting? risk bounds for
classiﬁcation and regression rules that interpolate. In Advances in neural information processing
systems, pp. 2300–2311, 2018a."
REFERENCES,0.3586626139817629,"Mikhail Belkin, Siyuan Ma, and Soumik Mandal. To understand deep learning we need to understand
kernel learning. arXiv preprint arXiv:1802.01396, 2018b."
REFERENCES,0.3617021276595745,"Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning
practice and the classical bias–variance trade-off. Proceedings of the National Academy of Sciences,
116(32):15849–15854, 2019."
REFERENCES,0.364741641337386,"Leo Breiman. Reﬂections after refereeing papers for nips. The Mathematics of Generalization, pp.
11–15, 1995."
REFERENCES,0.3677811550151976,"Leo Breiman. Random forests. Machine learning, 45(1):5–32, 2001."
REFERENCES,0.3708206686930091,"Leo Breiman, Jerome Friedman, Charles J Stone, and Richard A Olshen. Classiﬁcation and regression
trees. CRC press, 1984."
REFERENCES,0.3738601823708207,"Niladri S Chatterji and Philip M Long. Finite-sample analysis of interpolating linear classiﬁers in the
overparameterized regime. arXiv preprint arXiv:2004.12019, 2020."
REFERENCES,0.3768996960486322,"Lenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks
trained with the logistic loss. arXiv preprint arXiv:2002.04486, 2020."
REFERENCES,0.3799392097264438,"Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml."
REFERENCES,0.3829787234042553,"Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. arXiv preprint
arXiv:1703.11008, 2017."
REFERENCES,0.3860182370820669,"Manuel Fern´andez-Delgado, Eva Cernadas, Sen´en Barro, and Dinani Amorim. Do we need hundreds
of classiﬁers to solve real world classiﬁcation problems? The journal of machine learning research,
15(1):3133–3181, 2014."
REFERENCES,0.3890577507598784,"Stanislav Fort, Paweł Krzysztof Nowak, Stanislaw Jastrzebski, and Srini Narayanan. Stiffness: A
new perspective on generalization in neural networks. arXiv preprint arXiv:1901.09491, 2019."
REFERENCES,0.39209726443769,Under review as a conference paper at ICLR 2022
REFERENCES,0.3951367781155015,"Mario Geiger, Stefano Spigler, St´ephane d’Ascoli, Levent Sagun, Marco Baity-Jesi, Giulio Biroli,
and Matthieu Wyart. Jamming transition as a paradigm to understand the loss landscape of deep
neural networks. Physical Review E, 100(1):012115, 2019."
REFERENCES,0.3981762917933131,"Federica Gerace, Bruno Loureiro, Florent Krzakala, Marc M´ezard, and Lenka Zdeborov´a. Gener-
alisation error in learning with random features and the hidden manifold model. arXiv preprint
arXiv:2002.09339, 2020."
REFERENCES,0.4012158054711246,"Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Linearized two-layers
neural networks in high dimension. arXiv preprint arXiv:1904.12191, 2019."
REFERENCES,0.40425531914893614,"Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and estimation.
Journal of the American statistical Association, 102(477):359–378, 2007."
REFERENCES,0.4072948328267477,"Sebastian Goldt, Madhu S Advani, Andrew M Saxe, Florent Krzakala, and Lenka Zdeborova.
Generalisation dynamics of online learning in over-parameterised neural networks. arXiv preprint
arXiv:1901.09085, 2019."
REFERENCES,0.41033434650455924,"Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in
terms of optimization geometry. In International Conference on Machine Learning, pp. 1832–1841.
PMLR, 2018."
REFERENCES,0.4133738601823708,"Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural
networks. arXiv preprint arXiv:1706.04599, 2017."
REFERENCES,0.41641337386018235,"Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The elements of statistical learning: data
mining, inference, and prediction. Springer Science & Business Media, 2009."
REFERENCES,0.4194528875379939,"Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani.
Surprises in high-
dimensional ridgeless least squares interpolation. arXiv preprint arXiv:1903.08560, 2019."
REFERENCES,0.42249240121580545,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770–778, 2016."
REFERENCES,0.425531914893617,"´Ursula H´ebert-Johnson, Michael Kim, Omer Reingold, and Guy Rothblum. Multicalibration: Cali-
bration for the (computationally-identiﬁable) masses. In International Conference on Machine
Learning, pp. 1939–1948, 2018."
REFERENCES,0.42857142857142855,"Tin Kam Ho. Random decision forests. In Proceedings of 3rd international conference on document
analysis and recognition, volume 1, pp. 278–282. IEEE, 1995."
REFERENCES,0.4316109422492401,"Rashidedin Jahandideh, Alireza Tavakoli Targhi, and Maryam Tahmasbi. Physical attribute prediction
using deep residual neural networks. arXiv preprint arXiv:1812.07857, 2018."
REFERENCES,0.43465045592705165,"Yiding Jiang, Vaishnavh Nagarajan, Christina Baek, and J Zico Kolter. Assessing generalization of
sgd via disagreement. arXiv preprint arXiv:2106.13799, 2021."
REFERENCES,0.4376899696048632,Alex Krizhevsky et al. Learning multiple layers of features from tiny images. 2009.
REFERENCES,0.44072948328267475,"Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998."
REFERENCES,0.44376899696048633,"Yuanzhi Li, Colin Wei, and Tengyu Ma. Towards explaining the regularization effect of initial large
learning rate in training neural networks. arXiv preprint arXiv:1907.04595, 2019."
REFERENCES,0.44680851063829785,"Tengyuan Liang and Alexander Rakhlin. Just interpolate: Kernel” ridgeless” regression can generalize.
arXiv preprint arXiv:1808.00387, 2018."
REFERENCES,0.44984802431610943,"Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
Proceedings of International Conference on Computer Vision (ICCV), December 2015."
REFERENCES,0.45288753799392095,"Song Mei and Andrea Montanari. The generalization error of random features regression: Precise
asymptotics and double descent curve. arXiv preprint arXiv:1908.05355, 2019."
REFERENCES,0.45592705167173253,Under review as a conference paper at ICLR 2022
REFERENCES,0.45896656534954405,"Nicolai Meinshausen. Quantile regression forests. Journal of Machine Learning Research, 7(Jun):
983–999, 2006."
REFERENCES,0.46200607902735563,"Alfred M¨uller. Integral probability metrics and their generating classes of functions. Advances in
Applied Probability, 29(2):429–443, 1997."
REFERENCES,0.46504559270516715,"Vidya Muthukumar, Kailas Vodrahalli, Vignesh Subramanian, and Anant Sahai. Harmless inter-
polation of noisy data in regression. IEEE Journal on Selected Areas in Information Theory,
2020."
REFERENCES,0.46808510638297873,"Elizbar A Nadaraya. On estimating regression. Theory of Probability & Its Applications, 9(1):
141–142, 1964."
REFERENCES,0.47112462006079026,"Vaishnavh Nagarajan and J. Zico Kolter. Uniform convergence may be unable to explain generaliza-
tion in deep learning, 2019."
REFERENCES,0.47416413373860183,"Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep
double descent: Where bigger models and more data hurt. In International Conference on Learning
Representations, 2020."
REFERENCES,0.47720364741641336,"Hariharan Narayanan and Sanjoy Mitter. Sample complexity of testing the manifold hypothesis. In
Advances in neural information processing systems, pp. 1786–1794, 2010."
REFERENCES,0.48024316109422494,"Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with
noisy labels. In Advances in neural information processing systems, pp. 1196–1204, 2013."
REFERENCES,0.48328267477203646,"Brady Neal, Sarthak Mittal, Aristide Baratin, Vinayak Tantia, Matthew Scicluna, Simon Lacoste-
Julien, and Ioannis Mitliagkas. A modern take on the bias-variance tradeoff in neural networks.
arXiv preprint arXiv:1810.08591, 2018."
REFERENCES,0.48632218844984804,"Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. Towards
understanding the role of over-parametrization in generalization of neural networks. arXiv preprint
arXiv:1805.12076, 2018."
REFERENCES,0.48936170212765956,"Alexandru Niculescu-Mizil and Rich Caruana. Predicting good probabilities with supervised learning.
In Proceedings of the 22nd international conference on Machine learning, pp. 625–632, 2005."
REFERENCES,0.49240121580547114,"Matthew A Olson and Abraham J Wyner. Making sense of random forest probabilities: a kernel
perspective. arXiv preprint arXiv:1812.05792, 2018."
REFERENCES,0.49544072948328266,"Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. 2017."
REFERENCES,0.49848024316109424,"F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research,
12:2825–2830, 2011."
REFERENCES,0.5015197568389058,"Taylor Pospisil and Ann B Lee. Rfcde: Random forests for conditional density estimation. arXiv
preprint arXiv:1804.05753, 2018."
REFERENCES,0.5045592705167173,"Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in
neural information processing systems, pp. 1177–1184, 2008."
REFERENCES,0.5075987841945289,"David Rolnick, Andreas Veit, Serge Belongie, and Nir Shavit. Deep learning is robust to massive
label noise. arXiv preprint arXiv:1705.10694, 2017."
REFERENCES,0.5106382978723404,"Jonas Rothfuss, Fabio Ferreira, Simon Walther, and Maxim Ulrich. Conditional density estimation
with neural networks: Best practices and benchmarks. arXiv preprint arXiv:1903.00954, 2019."
REFERENCES,0.513677811550152,"Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet
Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115
(3):211–252, 2015. doi: 10.1007/s11263-015-0816-y."
REFERENCES,0.5167173252279635,Under review as a conference paper at ICLR 2022
REFERENCES,0.5197568389057751,"Robert E Schapire. Theoretical views of boosting. In European conference on computational learning
theory, pp. 1–10. Springer, 1999."
REFERENCES,0.5227963525835866,"Robert E Schapire, Yoav Freund, Peter Bartlett, Wee Sun Lee, et al. Boosting the margin: A new
explanation for the effectiveness of voting methods. The annals of statistics, 26(5):1651–1686,
1998."
REFERENCES,0.5258358662613982,"Shai Shalev-Shwartz and Shai Ben-David.
Understanding machine learning: From theory to
algorithms. Cambridge university press, 2014."
REFERENCES,0.5288753799392097,"Vaishaal Shankar, Alex Fang, Wenshuo Guo, Sara Fridovich-Keil, Ludwig Schmidt, Jonathan Ragan-
Kelley, and Benjamin Recht. Neural kernels without tangents. arXiv preprint arXiv:2003.02237,
2020."
REFERENCES,0.5319148936170213,"Utkarsh Sharma and Jared Kaplan. A neural scaling law from the dimension of the data manifold.
arXiv preprint arXiv:2004.10802, 2020."
REFERENCES,0.5349544072948328,"Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit
bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):
2822–2878, 2018."
REFERENCES,0.5379939209726444,"Sunil Thulasidasan, Tanmoy Bhattacharya, Jeff Bilmes, Gopinath Chennupati, and Jamal Mohd-
Yusof. Combating label noise in deep learning using abstention. arXiv preprint arXiv:1905.10964,
2019."
REFERENCES,0.541033434650456,"Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau,
Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, St´efan J. van der Walt,
Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric
Jones, Robert Kern, Eric Larson, CJ Carey, ˙Ilhan Polat, Yu Feng, Eric W. Moore, Jake Vand erPlas,
Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R Harris,
Anne M. Archibald, Antˆonio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1. 0
Contributors. SciPy 1.0: Fundamental Algorithms for Scientiﬁc Computing in Python. Nature
Methods, 17:261–272, 2020. doi: https://doi.org/10.1038/s41592-019-0686-2."
REFERENCES,0.5440729483282675,"Larry Wasserman. All of statistics: a concise course in statistical inference. Springer Science &
Business Media, 2013."
REFERENCES,0.547112462006079,"Geoffrey S Watson. Smooth regression analysis. Sankhy¯a: The Indian Journal of Statistics, Series A,
pp. 359–372, 1964."
REFERENCES,0.5501519756838906,"Abraham J Wyner, Matthew Olson, Justin Bleich, and David Mease. Explaining the success of
adaboost and random forests as interpolating classiﬁers. The Journal of Machine Learning
Research, 18(1):1558–1590, 2017."
REFERENCES,0.5531914893617021,"Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017."
REFERENCES,0.5562310030395137,"Mohammad Yaghini, Bogdan Kulynych, and Carmela Troncoso. Disparate vulnerability: On the
unfairness of privacy attacks against machine learning. arXiv preprint arXiv:1906.00389, 2019."
REFERENCES,0.5592705167173252,"Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146,
2016."
REFERENCES,0.5623100303951368,"Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016."
REFERENCES,0.5653495440729484,"Liu Ziyin, Blair Chen, Ru Wang, Paul Pu Liang, Ruslan Salakhutdinov, Louis-Philippe Morency,
and Masahito Ueda.
Learning not to learn in the presence of noisy labels.
arXiv preprint
arXiv:2002.06541, 2020."
REFERENCES,0.5683890577507599,Under review as a conference paper at ICLR 2022
REFERENCES,0.5714285714285714,"A
FULL RELATED WORK"
REFERENCES,0.574468085106383,"Our work is inspired by the broader study of interpolating and overparameterized methods in machine
learning; a partial list of works in this theme includes Zhang et al. (2016); Belkin et al. (2018a;b;
2019); Liang & Rakhlin (2018); Nakkiran et al. (2020); Mei & Montanari (2019); Schapire et al.
(1998); Breiman (1995); Ghorbani et al. (2019); Hastie et al. (2019); Bartlett et al. (2020); Advani &
Saxe (2017); Geiger et al. (2019); Gerace et al. (2020); Chizat & Bach (2020); Goldt et al. (2019);
Arora et al. (2019); Allen-Zhu et al. (2019); Neyshabur et al. (2018); Dziugaite & Roy (2017);
Muthukumar et al. (2020); Neal et al. (2018)."
REFERENCES,0.5775075987841946,"Interpolating Methods. Many of the best-performing techniques on high-dimensional tasks are
interpolating methods, which ﬁt their train samples to 0 train error. This includes neural-networks
and kernels on images (He et al., 2016; Shankar et al., 2020), and random forests on tabular data
(Fern´andez-Delgado et al., 2014). Interpolating methods have been extensively studied both recently
and in the past, since we do not theoretically understand their practical success (Schapire et al., 1998;
Schapire, 1999; Breiman, 1995; Zhang et al., 2016; Belkin et al., 2018a;b; 2019; Liang & Rakhlin,
2018; Mei & Montanari, 2019; Hastie et al., 2019; Nakkiran et al., 2020). In particular, much of
the classical work in statistical learning theory (uniform convergence, VC-dimension, Rademacher
complexity, regularization, stability) fails to explain the success of interpolating methods (Zhang
et al., 2016; Belkin et al., 2018a;b; Nagarajan & Kolter, 2019). The few techniques which do apply
to interpolating methods (e.g. margin theory (Schapire et al., 1998)) remain vacuous on modern
neural-networks and kernels."
REFERENCES,0.5805471124620061,"Decision Trees. In a similar vein to our work, Wyner et al. (2017); Olson & Wyner (2018) investigate
decision trees, and show that random forests are equivalent to a Nadaraya–Watson smoother Nadaraya
(1964); Watson (1964) with a certain smoothing kernel. Decision trees (Breiman et al., 1984) are often
intuitively thought of as “adaptive nearest-neighbors,” since they are explicitly a spatial-partitioning
method (Hastie et al., 2009). Thus, it may not be surprising that decision trees behave similarly to 1-
Nearest-Neighbors. Wyner et al. (2017); Olson & Wyner (2018) took steps towards characterizing and
understanding this behavior – in particular, Olson & Wyner (2018) deﬁnes an equivalent smoothing
kernel corresponding to a random forest, and empirically investigates the quality of the conditional
density estimate. Our work presents a formal characterization of the quality of this conditional density
estimate (Conjecture 1), which is a novel characterization even for decision trees, as far as we know."
REFERENCES,0.5835866261398176,"Kernel Smoothing. The term kernel regression is sometimes used in the literature to refer to kernel
smoothers, such as the Nadaraya–Watson kernel smoother (Nadaraya, 1964; Watson, 1964). But in
this work we use the term “kernel regression” to refer only to regression in a Reproducing Kernel
Hilbert Space, as described in the experimental details."
REFERENCES,0.5866261398176292,"Label Noise. Our conjectures also describe the behavior of neural networks under label noise, which
has been empirically and theoretically studied in the past, though not formally characterized before
(Zhang et al., 2016; Belkin et al., 2018b; Rolnick et al., 2017; Natarajan et al., 2013; Thulasidasan
et al., 2019; Ziyin et al., 2020; Chatterji & Long, 2020). Prior works have noticed that vanilla
interpolating networks are sensitive to label noise (e.g. Figure 1 in Zhang et al. (2016), and Belkin
et al. (2018b)), and there are many works on making networks more robust to label noise via
modiﬁcations to the training procedure or objective (Rolnick et al., 2017; Natarajan et al., 2013;
Thulasidasan et al., 2019; Ziyin et al., 2020). In contrast, we claim this sensitivity to label noise is
not necessarily a problem to be ﬁxed, but rather a consequence of a stronger property: distributional
generalization."
REFERENCES,0.5896656534954408,"Conditional Density Estimation. Our density calibration property is similar to the guarantees of a
conditional density estimator. More speciﬁcally, Conjecture 1 states that an interpolating classiﬁer
samples from a distribution approximating the conditional density of p(y|x) in a certain sense.
Conditional density estimation has been well-studied in classical nonparametric statistics (e.g. the
Nadaraya–Watson kernel smoother (Nadaraya, 1964; Watson, 1964)). However, these classical
methods behave poorly in high-dimensions, both in theory and in practice. There are some attempts
to extend these classical methods to modern high-dimentional problems via augmenting estimators
with neural networks (e.g. Rothfuss et al. (2019)). Random forests have also been known to exhibit
properties similar to conditional density estimators. This has been formalized in various ways, often
only with asymptotic guarantees (Meinshausen, 2006; Pospisil & Lee, 2018; Athey et al., 2019)."
REFERENCES,0.5927051671732523,Under review as a conference paper at ICLR 2022
REFERENCES,0.5957446808510638,"No prior work that we are aware of attempts to characterize the quality of the resulting density
estimate via testable assumptions, as we do with our formulation of Conjecture 1. Finally, our
motivation is not to design good conditional density estimators, but rather to study properties of
interpolating classiﬁers — which we ﬁnd happen to share properties of density estimators."
REFERENCES,0.5987841945288754,"Feature Calibration (Conjecture 1) is also related to the concepts of calibration and multicalibra-
tion (Guo et al., 2017; Niculescu-Mizil & Caruana, 2005; H´ebert-Johnson et al., 2018). In our
framework, calibration is implied by Feature Calibration for a speciﬁc set of partitions L (determined
by level sets of the classiﬁer’s conﬁdence). However, we are not concerned with a speciﬁc set of
partitions (or “subgroups” in the algorithmic fairness literature) but we generally aim to characterize
for which partitions Feature Calibration holds. Moreover, we consider only hard-classiﬁcation deci-
sions and not conﬁdences, and we study only standard learning algorithms which are not given any
distinguished set of subgroups/partitions in advance. Our notion of distributional generalization is
also related to the notion of “distributional subgroup overﬁtting” introduced recently by Yaghini et al.
(2019) to study algorithmic fairness. This can be seen as studying distributional generalization for a
speciﬁc family of tests (determined by distinguished subgroups in the population)."
REFERENCES,0.601823708206687,"Locality and Manifold Learning. Our intuition for the behaviors in this work is that they arise
due to some form of “locality” of the trained classiﬁers, in an appropriate space. This intuition is
present in various forms in the literature, for example: the so-called called “manifold hypothesis,”
that natural data lie on a low-dimensional manifold (e.g. Narayanan & Mitter (2010); Sharma &
Kaplan (2020)), as well as works on local stiffness of the loss landscape (Fort et al., 2019), and
works showing that overparameterized neural networks can learn hidden low-dimensional structure
in high-dimensional settings (Gerace et al., 2020; Bach, 2017; Chizat & Bach, 2020). It is open to
more formally understand connections between our work and the above."
REFERENCES,0.6048632218844985,"Note about Proper Scoring Rules: If the loss function used in training is a strictly-proper scoring
rule such as cross-entropy, then we may expect that in the limit of a large-capacity network and
inﬁnite data, training on samples {(xi, yi)} will yield a good density estimate of p(y|x) at the softmax
layer. However, this is not what is happening in our experiments: First, our experiments consider the
hard-decisions, not the softmax outputs. Second, we observe Conjecture 1 even in settings without
proper scoring rules (kernel SVM and decision trees)."
REFERENCES,0.60790273556231,"B
EXPERIMENTAL DETAILS"
REFERENCES,0.6109422492401215,"Here we describe general background, and experimental details common to all sections. Then we
provide section-speciﬁc details below."
REFERENCES,0.6139817629179332,"B.1
DATASETS"
REFERENCES,0.6170212765957447,"We consider the image datasets CIFAR-10 and CIFAR-100 (Krizhevsky et al., 2009), MNIST (LeCun
et al., 1998), Fashion-MNIST (Xiao et al., 2017), CelebA (Liu et al., 2015), and ImageNet (Rus-
sakovsky et al., 2015). We normalize images to x ∈[0, 1]C×W ×H."
REFERENCES,0.6200607902735562,"We also consider tabular datasets from the UCI repository Dua & Graff (2017). For UCI data, we
consider the 121 classiﬁcation tasks as standardized in Fern´andez-Delgado et al. (2014). Some of these
tasks have very few examples, so we restrict to the 92 classiﬁcation tasks from Fern´andez-Delgado
et al. (2014) which have at least 200 total examples."
REFERENCES,0.6231003039513677,"B.2
MODELS"
REFERENCES,0.6261398176291794,"We consider neural-networks, kernel methods, and decision trees."
REFERENCES,0.6291793313069909,"B.2.1
DECISION TREES"
REFERENCES,0.6322188449848024,"We train interpolating decision trees using a growth rule from Random Forests (Breiman, 2001;
Ho, 1995):
selecting a split based on a random
√"
REFERENCES,0.6352583586626139,"d subset of d features, splitting based
on Gini impurity, and growing trees until all leafs have a single sample.
This is as imple-
mented by Scikit-learn Pedregosa et al. (2011) defaults with RandomForestClassifier
(n_estimators=1, bootstrap=False)."
REFERENCES,0.6382978723404256,Under review as a conference paper at ICLR 2022
REFERENCES,0.6413373860182371,"B.2.2
KERNELS"
REFERENCES,0.6443768996960486,"Throughout this work we consider classiﬁcation via kernel regression and kernel SVM. For M-class
classiﬁcation via kernel regression, we follow the methodology in e.g. Rahimi & Recht (2008);
Belkin et al. (2018b); Shankar et al. (2020). We solve the following convex problem for training:"
REFERENCES,0.6474164133738601,"α∗:= argmin
α∈RN×M ||Kα −y||2
2 + λαT Kα"
REFERENCES,0.6504559270516718,"where Kij = k(xi, xj) is the kernel matrix of the training points for a kernel function k, y ∈RN×M
is the one-hot encoding of the train labels, and λ ≥0 is the regularization parameter. The solution
can be written
α∗= (K + λI)−1y"
REFERENCES,0.6534954407294833,"which we solve numerically using SciPy linalg.solve (Virtanen et al., 2020). We use the
explicit form of all kernels involved. That is, we do not use random-feature approximations (Rahimi
& Recht, 2008), though we expect they would behave similarly."
REFERENCES,0.6565349544072948,The kernel predictions on test points are then given by
REFERENCES,0.6595744680851063,"gα(x) :=
X"
REFERENCES,0.662613981762918,"i∈[N]
αik(xi, x)
(11)"
REFERENCES,0.6656534954407295,"fα(x) := argmax
j∈[M]
gα(x)j
(12)"
REFERENCES,0.668693009118541,"where g(x) ∈RM are the kernel regressor outputs, and g(x) ∈[M] is the thresholded classiﬁcation
decision. This is equivalent to training M separate binary regressors (one for each label), and taking
the argmax for classiﬁcation. We usually consider unregularized regression (λ = 0), except in
Section 5.2."
REFERENCES,0.6717325227963525,"For kernel SVM, we use the implementation provided by Scikit-learn (Pedregosa et al., 2011)
sklearn.svm.SVC with a precomputed kernel, for inverse-regularization parameter C ≥0
(larger C corresponds to smaller regularization)."
REFERENCES,0.6747720364741642,Types of Kernels. We use the following kernel functions k : Rd × Rd →R≥0.
REFERENCES,0.6778115501519757,"• Gaussian Kernel (RBF): k(xi, xj) = exp(−||xi−xj||2
2
2eσ2
)."
REFERENCES,0.6808510638297872,"• Laplace Kernel: k(xi, xj) = exp(−||xi−xj||2 eσ
)."
REFERENCES,0.6838905775075987,"• Myrtle10 Kernel: This is the compositional kernel introduced by Shankar et al. (2020). We
use their exact kernel for CIFAR-10."
REFERENCES,0.6869300911854104,"For the Gaussian and Laplace kernels, we parameterize bandwidth by σ := eσ/
√"
REFERENCES,0.6899696048632219,"d. We use the
following bandwidths, found by cross-validation to maximize the unregularized test accuracy:"
REFERENCES,0.6930091185410334,• MNIST: σ = 0.15 for RBF kernel.
REFERENCES,0.6960486322188449,• Fashion-MNIST: σ = 0.1 for RBF kernel. σ = 1.0 for Laplace kernel.
REFERENCES,0.6990881458966566,"• CIFAR-10: Myrtle10 Kernel from Shankar et al. (2020), and σ = 0.1 for RBF kernel."
REFERENCES,0.7021276595744681,"B.2.3
NEURAL NETWORKS"
REFERENCES,0.7051671732522796,"We use 4 different neural networks in our experiments. We use a multi-layer perceptron, and three
different Residual networks."
REFERENCES,0.7082066869300911,"MLP: We use a Multi-layer perceptron or a fully connected network with 3 hidden layers with 512
neurons in each layer. A hidden layer is followed by a BatchNormalization layer and ReLU activation
function."
REFERENCES,0.7112462006079028,"WideResNet: We use the standard WideResNet-28-10 described in Zagoruyko & Komodakis (2016).
Our code is based on this repository."
REFERENCES,0.7142857142857143,"ResNet50: We use a standard ResNet-50 from the PyTorch library (Paszke et al., 2017)."
REFERENCES,0.7173252279635258,Under review as a conference paper at ICLR 2022
REFERENCES,0.7203647416413373,"MLP
ResNet18
WideResNet
ResNet50
Batchsize
128
128
128
32
Epochs
820
200
200
50"
REFERENCES,0.723404255319149,"Optimizer
Adam
(β1 = 0.9, β2 = 0.999)"
REFERENCES,0.7264437689969605,"SGD +
Momentum (0.9)"
REFERENCES,0.729483282674772,"SGD +
Momentum (0.9)
SGD"
REFERENCES,0.7325227963525835,"Learning rate
(LR) schedule
Constant LR = 0.001"
REFERENCES,0.7355623100303952,"Inital LR= 0.05
scale by 0.1 at
epochs (80, 120)"
REFERENCES,0.7386018237082067,"Inital LR= 0.1
scale by 0.2 at
epochs (80, 120, 160)"
REFERENCES,0.7416413373860182,"Initial LR = 0.001,
scale by 0.1
if training loss stagnant
for 2000 gradient steps
Data
Augmentation
Random ﬂips + RandomCrop(32, padding=4)"
REFERENCES,0.7446808510638298,"CIFAR-10 Error
∼37%
∼8%
∼4%
N/A"
REFERENCES,0.7477203647416414,"Table 1: Hyperparameters used to train the neural networks and their errors on the unmodiﬁed
CIFAR-10 dataset"
REFERENCES,0.7507598784194529,"ResNet18: We use a modiﬁcation of ResNet18 He et al. (2016) adapted to CIFAR-10 image sizes.
Our code is based on this repository."
REFERENCES,0.7537993920972644,"For Experiment 1 and Section 4, the hyperparameters used to train the above networks are given in
Table 1."
REFERENCES,0.756838905775076,Under review as a conference paper at ICLR 2022
REFERENCES,0.7598784194528876,"C
FEATURE CALIBRATION: APPENDIX"
REFERENCES,0.7629179331306991,"C.1
A GUIDE TO READING THE PLOTS"
REFERENCES,0.7659574468085106,All the experiments in support of Conjecture 1 involve various quantities which we enumaerate here
REFERENCES,0.7689969604863222,"1. Inputs x: Each experiment involves inputs from a standard dataset like CIFAR-10 or MNIST.
We use the standard train/test splits for every dataset."
REFERENCES,0.7720364741641338,"2. Distinguishable feature L(x): This feature depends only on input x. We consider various
features like the original classes itself, a superset of classes (as in coarse partition) or some
secondary attributes (like the binary attributes provided with CelebA)"
REFERENCES,0.7750759878419453,"3. Output labels y: The output label may be some modiﬁcation of the original labels. For
instance, by adding some type of label noise, or a constructed binary task as in Experiment 1"
REFERENCES,0.7781155015197568,"4. Classiﬁer family F: We consider various types of classiﬁers like neural networks trained
with gradient based methods, kernel and decision trees."
REFERENCES,0.7811550151975684,"In each experiment, we are interested in two joint densities (y, L(x)), which depends on our dataset
and task and is common across train and test, and (f(x), L(x)) which depends on the interpolating
classiﬁers outputs on the test set. Since y, L(x) and f(x) are discrete, we will look at their discrete
joint distributions. We sometimes refer to (y, L(x)) as the train joint density, as at interpolation
(y, L(x)) = (f(x), L(x)) for all training inputs x. We also refer to (f(x), L(x)) as the test density,
as we measure this only on the test set."
REFERENCES,0.78419452887538,"C.2
EXPERIMENT 1"
REFERENCES,0.7872340425531915,"Experimental details: We now provide further details for Experiment 1. We ﬁrst construct a dataset
from CIFAR-10 that obeys the joint density (y, L(x)) shown in Figure 1 left panel. We then train
a WideResNet-28-10 (WRN-28-10) on this modiﬁed dataset to zero training error. The network is
trained with the hyperparameters described in Table 1. We then observe the joint density (f(x), L(x))
on the test images and ﬁnd that the two joint densities are close as shown in Figure 5."
REFERENCES,0.790273556231003,We now consider a modiﬁcation of this experiment as follows:
REFERENCES,0.7933130699088146,"Experiment 2. Consider the following distribution over images x and binary labels y.
Sample x as a uniformly random CIFAR-10 image, and sample the label as p(y|x)
=
Bernoulli(CIFAR Class(x)/10). That is, if the CIFAR-10 class of x is k ∈{0, 1, . . . 9}, then the
label is 1 with probability (k/10) and 0 otherwise. Figure 5 shows this joint distribution of (x, y). As
before, train a WideResNet to 0 training error on this distribution."
REFERENCES,0.7963525835866262,"In this experiment too, we observe that the train and test joint densities are close as shown in Figure 5."
REFERENCES,0.7993920972644377,"Now, we repeat the same experiment, but with an MLP instead of WRN-28-10. The training procedure
is described in Table 1. This MLP has an error on 37% on the original CIFAR-10 dataset."
REFERENCES,0.8024316109422492,"Since this MLP has poor accuracy on the original CIFAR-10 classiﬁcation task, it does not form a
distinguishable partition for it. As a result, the train and test joint densities (Figure 6) do not match as
well as they did for WRN-28-10."
REFERENCES,0.8054711246200608,"C.3
CONSTANT PARTITION"
REFERENCES,0.8085106382978723,"Conjecture 1 states that the marginal distribution of class labels for any interpolating classiﬁer f(x)
is close to the true marginals p(y). To show this, we construct a dataset based on CIFAR-10 that has
class-imbalance. For class k ∈{0...9}, sample (k + 1) × 500 images from that class. This will give
us a dataset where classes will have marginal distribution p(y = ℓ) ∝ℓ+ 1 for classes ℓ∈[10], as
shown in Figure 2. We do this both for the training set and the test set, to keep the distribution D
ﬁxed."
REFERENCES,0.8115501519756839,"We then train a variety of classiﬁers (MLPs, RBF Kernel, ResNets) to interpolation on this dataset,
which have varying levels of test errors (9-41%). The class balance of classiﬁer outputs on the
(rebalanced) test set"
REFERENCES,0.8145896656534954,Under review as a conference paper at ICLR 2022
REFERENCES,0.817629179331307,"Figure 5: Distributional Generalization in Experiment 2.
Joint densities of the distributions
involved in Experiment 2.
The top panel shows the joint density of labels on the train set:
(CIFAR Class(x), y). The bottom panels shows the joint density of classiﬁer predictions on
the test set: (CIFAR Class(x), f(x)). Distributional Generalization claims that these two joint
densities are close."
REFERENCES,0.8206686930091185,"Figure 6: Joint density of (y, Class(x)), top, and (f(x), Class(x)), bottom, for test samples (x, y)
from Experiment 2 for an MLP."
REFERENCES,0.8237082066869301,"C.4
CLASS PARTITION"
REFERENCES,0.8267477203647416,"C.4.1
NEURAL NETWORKS AND CIFAR-10"
REFERENCES,0.8297872340425532,"We now describe details for the experiments in Figures 2A and 3A. A WRN-28-10 achieves an error
of 4% on CIFAR-10. Hence, the original labels in CIFAR-10 form a distinguishable partition for
this dataset. To demonstrate that Conjecture 1 holds, we consider different structured label noise on
the CIFAR-10 dataset. To do so, we apply a variety of confusion matrices to the data. That is, for a
confusion matrix C : 10 × 10 matrix, the element cij gives the joint density that a randomly sampled
image had original label j, but is ﬂipped to class i. For no noise, this would be an identity matrix."
REFERENCES,0.8328267477203647,"We begin by a simple confusion matrix where we ﬂip only one class 0 →1 with varying probability
p. Figure 7A shows one such confusion matrix for p = 0.4. We then train a WideResNet-28-10
to zero train error on this dataset. We use the hyperparameters described in B.2 We ﬁnd that the"
REFERENCES,0.8358662613981763,Under review as a conference paper at ICLR 2022
REFERENCES,0.8389057750759878,"classiﬁer outputs on the test set closely track the confusion matrix that was applied to the distribution.
Figure 7C shows that this is independent of the value of p and continues to hold for p = [0, 1]."
REFERENCES,0.8419452887537994,"Figure 7: Feature Calibration with original classes on CIFAR-10: We train a WRN-28-10 on
the CIFAR-10 dataset where we mislabel class 0 →1 with probability p. (A): Joint density of the
distinguishable features L (the original CIFAR-10 class) and the classiﬁcation task labels y on the
train set for noise probability p = 0.4. (B): Joint density of the original CIFAR-10 classes L and the
network outputs f(x) on the test set. (C): Observed noise probability in the network outputs on the
test set (the (1, 0) entry of the matrix in B) for varying noise probabilities p"
REFERENCES,0.8449848024316109,"To show that this is not dependent on the particular class used, we also show that the same holds for a
random confusion matrix. We generate a sparse confusion matrix as follows. We set the diagonal to
0.5. Then, for every class j, we pick any two random classes for and set them to 0.2 and 0.3. We
train a WRN-28-10 on it and report the test confusion matrix. The resulting train and test densities
are shown in Figure 2A. As expected, the train and test confusion matrices are close, and share the
same sparsity pattern."
REFERENCES,0.8480243161094225,"C.4.2
DECISION TREES"
REFERENCES,0.851063829787234,"Figure 8 shows a version of this experiment for decision trees on the molecular biology UCI task.
The molecular biology task is a 3-way classiﬁcation problem: to classify the type of a DNA splice
junction (donor, acceptor, or neither), given the sequence of DNA (60 bases) surrounding the junction.
We add varying amounts of label noise that ﬂips class 2 to class 1 with a certain probability, and we
observe that interpolating decision trees reproduce this same structured label noise on the test set."
REFERENCES,0.8541033434650456,"Similar results hold for decision trees; here we show experiments on two UCI tasks: wine and
mushroom."
REFERENCES,0.8571428571428571,"The wine task is a 3-way classiﬁcation problem: to identify the cultivar of a given wine (out of 3
cultivars), given 13 physical attributes describing the wine. Figure 9 shows an analogous experiment
with label noise taking class 1 to class 2."
REFERENCES,0.8601823708206687,"The mushroom task is a 2-way classiﬁcation problem: to classify the type of edibility of a mushroom
(edible vs poisonous) given 22 physical attributes (e.g. stalk color, odor, etc). Figure 10 shows an
analogous experiment with label noise ﬂipping class 0 to class 1."
REFERENCES,0.8632218844984803,"C.5
MULTIPLE FEATURES"
REFERENCES,0.8662613981762918,"Conjecture 1 states that the network should be automatically calibrated for all distinguishable features,
without any explicit labels for them. To verify this, we use the CelebA dataset (Liu et al., 2015),
containing images with various labelled binary attributes per-image (“male”, “blond hair”, etc). Some
of these attributes form a distinguishable feature for ResNet50 as they are learnable to high accuracy
(Jahandideh et al., 2018). We pick one of hard attributes as the target classiﬁcation task. We train a
ResNet-50 to predict the attribute {Attractive, Not Attractive}. We choose this attribute because a
ResNet-50 performs poorly on this task (test error ∼20%) and has good class balance. We choose
an attribute with poor generalization because the conjecture would hold trivially for if the network"
REFERENCES,0.8693009118541033,Under review as a conference paper at ICLR 2022
REFERENCES,0.8723404255319149,"Figure 8: Feature Calibration for Decision trees on UCI (molecular biology). We add label noise
that takes class 2 to class 1 with probability p ∈[0, 0.5]. The top row shows the confusion matrix
of the true class L(x) vs. the label y on the train set, for varying levels of noise p. The bottom row
shows the corresponding confusion matrices of the classiﬁer predictions f(x) on the test set, which
closely matches the train set, as predicted by Conjecture 1."
REFERENCES,0.8753799392097265,"Figure 9: Decision trees on UCI (wine). We add label noise that takes class 1 to class 2 with
probability p ∈[0, 0.5]. Each column shows the test and train confusion matrices for a given p. Note
that this decision trees achieve high accuracy on this task with no label noise (leftmost column).
We plot the empirical joint density of the train set, and not the population joint density of the train
distribution, and thus the top row exhibits some statistical error due to small-sample effects."
REFERENCES,0.878419452887538,"generalizes well. We initialize the network with a pretrained ResNet-50 from the PyTorch library
Paszke et al. (2017) and use the hyperparameters described in Section B.2 to train on this attribute.
We then check the train/test joint density with various other attributes like Male, Wearing Lipstick
etc. Note that the network is not given any label information for these additional attributes, but is
calibrated with respect to them. That is, the network says ∼30% of images that have ’heavy makeup’
will be classiﬁed as ’Attractive’, even if the network makes mistakes on which particular inputs it
chooses to do so. In this setting, the label distribution is deterministic, and not directly dependent on
the distinguishable features, unlike the experiments considered before. Yet, as we see in Figure 11,
the classiﬁer outputs are correctly calibrated for each attribute. Loosely, this can be viewed as the
network performing 1NN classiﬁcation in a metric space that is well separated for each of these
distinguishable features."
REFERENCES,0.8814589665653495,"C.6
COARSE PARTITION"
REFERENCES,0.8844984802431611,"We now consider cases where the original classes do not form a distinguishable partition for the
classiﬁer in consideration. That is, the classiﬁer is not powerful enough to obtain low error on the
original dataset, but can perform well on a coarser division of the classes."
REFERENCES,0.8875379939209727,Under review as a conference paper at ICLR 2022
REFERENCES,0.8905775075987842,"Figure 10: Decision trees on UCI (mushroom). We add label noise that takes class 0 to class 1 with
probability p ∈[0, 0.5]. Each column shows the test and train confusion matrices for a given p. Note
that this decision trees achieve high accuracy on this task with no label noise (leftmost column)."
REFERENCES,0.8936170212765957,"Figure 11: Feature Calibration for multiple features on CelebA: We train a ResNet-50 to perform
binary classiﬁcation task on the CelebA dataset. The top row shows the joint distribution of this task
label with various other attributes in the dataset. The bottom row shows the same joint distribution
for the ResNet-50 outputs on the test set. Note that the network was not given any explicit inputs
about these attributes during training."
REFERENCES,0.8966565349544073,"To verify this, we consider a division of the CIFAR-10 classes into Objects {airplane, automobile,
ship, truck} vs Animals {cat, deer, dog, frog}. An MLP trained on this problem has low error (∼8%),
but the same network performs poorly on the full dataset (∼37% error). Hence, Object vs Animals
forms a distinguishable partition with MLPs. In Figure 12a, we show the results of training an MLP
on the original CIFAR-10 classes. We see that the network mostly classiﬁes objects as objects and
animals as animals, even when it might mislabel a dog for a cat."
REFERENCES,0.8996960486322189,"We perform a similar experiment for the RBF kernel on Fashion-MNIST, with partition
{clothing, shoe, bag}, in Figure 12b."
REFERENCES,0.9027355623100304,"ImageNet experiment. In Table 2 we provide results of the terrier experiment in the body, for various
ImageNet classiﬁers. We use publicly available pretrained ImageNet models from this repository,
and use their evaluations on the ImageNet test set."
REFERENCES,0.9057750759878419,"C.7
DISCUSSION: PROPER SCORING RULES"
REFERENCES,0.9088145896656535,"Here we distinguish the density-estimation of Conjecture 1 from another setting where density
estimation occurs. If ℓ(bp, y) is a strictly-proper scoring rule3 on predicted distribution bp ∈∆(Y)"
REFERENCES,0.9118541033434651,3See Gneiting & Raftery (2007) for a survey of proper scoring rules.
REFERENCES,0.9148936170212766,Under review as a conference paper at ICLR 2022
REFERENCES,0.9179331306990881,"Model
AlexNet
ResNet18
ResNet50
BagNet8
BagNet32"
REFERENCES,0.9209726443768997,"ImageNet Accuracy
0.565
0.698
0.761
0.464
0.667
Accuracy on dogs
0.588
0.729
0.793
0.462
0.701
Accuracy on terriers
0.572
0.704
0.775
0.421
0.659
Accuracy for binary {dog/not-dog}
0.984
0.993
0.996
0.972
0.992
Accuracy on {terrier/not-terrier} among dogs
0.913
0.955
0.969
0.876
0.944"
REFERENCES,0.9240121580547113,"Fraction of real-terriers among dogs
0.224
0.224
0.224
0.224
0.224
Fraction of predicted-terriers among dogs
0.209
0.222
0.229
0.192
0.215"
REFERENCES,0.9270516717325228,"Table 2: ImageNet classiﬁers are calibrated with respect to dogs: All classiﬁers predict terrier for
roughly ∼22% of all dogs (last row), though they may mistake which speciﬁc dogs are terriers."
REFERENCES,0.9300911854103343,"(a) CIFAR10 + MLP
(b) Fashion-MNIST + RBF"
REFERENCES,0.9331306990881459,"Figure 12: Coarse partitions as distinguishable features: We consider a setting where the original
classes are not distinguishable, but a superset of the classes are distinguishable."
REFERENCES,0.9361702127659575,Under review as a conference paper at ICLR 2022
REFERENCES,0.939209726443769,"and sample y ∈Y, then the population minimizer of ℓ(F(x), y) is exactly the conditional density
F(x) = p(y|x). That is,"
REFERENCES,0.9422492401215805,"p(y|x) =
argmin
F :X→∆(Y)
E
(x,y)∼p[ℓ(F(x), y)]"
REFERENCES,0.9452887537993921,"This suggests that in the limit of large-capacity network and very large data (to approximate population
quantities), training neural nets with cross-entropy loss on samples (x, y) will yield a good density
estimate of p(y|x) at the softmax layer. However, this is not what is happening in our experiments.
First, our experiments consider the hard-thresholded classiﬁer, i.e. the argmax of the softmax layer. If
the softmax layer itself was close to p(y|x), then the classiﬁer itself will be close to argmaxy p(y|x)
– that is, close to the optimal classiﬁer. However, this is not the case (since the classiﬁers make
signiﬁcant errors). Second, we observe Conjecture 1 even in settings where we train with non-proper
scoring rules (e.g. kernel regression, where the classiﬁer does not output a probability)."
REFERENCES,0.9483282674772037,"D
NEAREST-NEIGHBOR PROOFS"
REFERENCES,0.9513677811550152,"D.1
FEATURE CALIBRATION PROPERTY"
REFERENCES,0.9544072948328267,"Proof of Theorem 1. Recall that L being an (ε, NN, D, n)-distinguishable partition means that
nearest-neighbors works to classify L(x) from x:"
REFERENCES,0.9574468085106383,"Pr
{xi,yi}∼Dn"
REFERENCES,0.9604863221884499,"S={(xi,L(xi)}
x,y∼D"
REFERENCES,0.9635258358662614,"[NN(y)
S (x) = L(x)] ≥1 −ε
(13)"
REFERENCES,0.9665653495440729,"Now, we have"
REFERENCES,0.9696048632218845,"{(NN(y)
S (x), L(x))}S∼Dn
x,y∼D ≡{(byi, L(x))}
S∼Dn
b
xi, b
yi←NNS(x)
x,y∼D (14)"
REFERENCES,0.9726443768996961,"≈ε {(byi, L( bxi))}
S∼Dn
b
xi, b
yi←NNS(x)
x,y∼D (15)"
REFERENCES,0.9756838905775076,"≈δ {(byi, L( bxi))} b
xi, b
yi∼D
(16)"
REFERENCES,0.9787234042553191,"Line (15) is by distinguishability, since Pr[L(x) ̸= L( bxi)] ≤ε. And Line (16) is by the regularity
condition."
REFERENCES,0.9817629179331308,"E
NON-INTERPOLATING CLASSIFIERS: APPENDIX"
REFERENCES,0.9848024316109423,"Here we give an additional example of distributional generalization: in kernel SVM (as opposed to
kernel regression, in the main text)."
REFERENCES,0.9878419452887538,Under review as a conference paper at ICLR 2022
REFERENCES,0.9908814589665653,"Figure 13: Distributional Generalization. Train (left) and test (right) confusion matrices for kernel
SVM on MNIST with random sparse label noise. Each row corrosponds to one value of inverse-
regularization parameter C. All rows are trained on the same (noisy) train set."
REFERENCES,0.993920972644377,Under review as a conference paper at ICLR 2022
REFERENCES,0.9969604863221885,"Figure 14: Distributional Generalization for WideResNet on CIFAR-10. We apply label noise
from a random sparse confusion to the CIFAR-10 train set. We then train a single WideResNet28-10,
and measure its predictions on the train and test sets over increasing train time (SGD steps). The
top row shows the confusion matrix of predictions f(x) vs true labels L(x) on the train set, and the
bottom row shows the corresponding confusion matrix on the test set. As the network is trained for
longer, it ﬁts more of the noise on the train set, and this behavior is mirrored almost identically on the
test set."
