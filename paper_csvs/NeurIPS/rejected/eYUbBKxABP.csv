Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.000835421888053467,"We study the legal challenges in automated decision-making by analysing conven-
1"
ABSTRACT,0.001670843776106934,"tional algorithmic fairness approaches and their alignment with anti-discrimination
2"
ABSTRACT,0.002506265664160401,"law in the United Kingdom and other jurisdictions based on English common law.
3"
ABSTRACT,0.003341687552213868,"By translating principles of anti-discrimination law into a decision-theoretic frame-
4"
ABSTRACT,0.004177109440267335,"work, we formalise discrimination and propose a new, legally informed approach
5"
ABSTRACT,0.005012531328320802,"to developing systems for automated decision-making. Our investigation reveals
6"
ABSTRACT,0.005847953216374269,"that while algorithmic fairness approaches have adapted concepts from legal theory,
7"
ABSTRACT,0.006683375104427736,"they can conflict with legal standards, highlighting the importance of bridging the
8"
ABSTRACT,0.007518796992481203,"gap between automated decisions, fairness, and anti-discrimination doctrine.
9"
INTRODUCTION,0.00835421888053467,"1
Introduction
10"
INTRODUCTION,0.009189640768588136,"Automated decision-making using predictive models is becoming increasingly important in many
11"
INTRODUCTION,0.010025062656641603,"areas of society, including lending [60, 100, 107], criminal justice [31, 14, 151], hiring [64, 59, 25],
12"
INTRODUCTION,0.01086048454469507,"and welfare eligibility [41, 56, 113]. Instances of large-scale failures, from disproportionately harming
13"
INTRODUCTION,0.011695906432748537,"vulnerable people in welfare eligibility assessments [113] to bias in consumer lending [80], highlight
14"
INTRODUCTION,0.012531328320802004,"the need for lawful implementation. Scrutiny of ML-based decisions is heightened by concerns about
15"
INTRODUCTION,0.013366750208855471,"replicating human biases and historical inequality [97, 41, 93].
16"
INTRODUCTION,0.014202172096908938,"Concerns about algorithmic bias have spurred research into fair ML. Early discourse on fairness
17"
INTRODUCTION,0.015037593984962405,"in ML was relatively narrow due to technical constraints [29, 61]. More recently, researchers have
18"
INTRODUCTION,0.015873015873015872,"developed formal definitions of fairness in algorithmic decisions and methods to measure fairness
19"
INTRODUCTION,0.01670843776106934,"in predictive models [49, 31, 27, 147, 87, 85]. Algorithmic fairness definitions generally measure
20"
INTRODUCTION,0.017543859649122806,"prediction disparities across groups with different legally protected characteristics [90, 136, 87, 17].
21"
INTRODUCTION,0.018379281537176273,"This research has resulted in several proposals, including statistical metrics to assess the fairness of
22"
INTRODUCTION,0.01921470342522974,"individual predictive models [136, 111, 22, 24], fairness for model auditing [70, 103, 63, 89, 98], and
23"
INTRODUCTION,0.020050125313283207,"fairness constraints on models [31, 148, 50, 145, 12].
24"
INTRODUCTION,0.020885547201336674,"These criteria simplify fairness into measurements of disparity that do not inherently map to unlawful
25"
INTRODUCTION,0.02172096908939014,"discrimination. The usefulness of these metrics in practice is limited as incomplete or even irrelevant
26"
INTRODUCTION,0.022556390977443608,"measures for legal investigations. There have been important efforts to bridge the gap between legal
27"
INTRODUCTION,0.023391812865497075,"and technical approaches to fair ML [81, 55, 51, 144, 139, 1, 46]. Lawyers have highlighted the
28"
INTRODUCTION,0.02422723475355054,"challenges of the narrow construction of fairness metrics focusing on disparity in predictions rather
29"
INTRODUCTION,0.02506265664160401,"than more nuanced definitions of discriminatory conduct and the broader context of the automated
30"
INTRODUCTION,0.025898078529657476,"decision-making process [55, 51, 144, 1]. We aim to contextualise and formalise legal concepts of
31"
INTRODUCTION,0.026733500417710943,"algorithmic discrimination beyond the narrow construction of statistical disparity.
32"
INTRODUCTION,0.02756892230576441,"The predominance of US analysis of fairness and discrimination in ML, lack of non-US ML
33"
INTRODUCTION,0.028404344193817876,"datasets [78], and the limited legal scholarship translating these concepts, has inadvertently fos-
34"
INTRODUCTION,0.029239766081871343,"tered a series of misconceptions that pervade the field. However, very few papers have engaged with
35"
INTRODUCTION,0.03007518796992481,"anti-discrimination laws outside of the United States [143, 139, 1, 140, 67, 76]. We aim to introduce
36"
INTRODUCTION,0.030910609857978277,"new principles and methods to deal with the issues identified in this literature. By avoiding the
37"
INTRODUCTION,0.031746031746031744,"nuanced legal realities of other jurisdictions, models designed to comply with US laws may breach
38"
INTRODUCTION,0.03258145363408521,"UK laws or those in comparable jurisdictions. Our paper addresses this gap by providing a rigorous
39"
INTRODUCTION,0.03341687552213868,"analysis of UK discrimination law, correcting some mischaracterisations, and establishing a more
40"
INTRODUCTION,0.034252297410192145,"accurate foundation for developing fair ML in the UK and its related jurisdictions.
41"
AUTOMATED DECISION-MAKING,0.03508771929824561,"1.1
Automated Decision-Making
42"
AUTOMATED DECISION-MAKING,0.03592314118629908,"Let xi ∈Rp be a vector of observed attributes for individual i. A decision-maker must choose a
43"
AUTOMATED DECISION-MAKING,0.036758563074352546,"decision a ∈A, where A is closed. Further, we assume that the decision-maker wants to decide
44"
AUTOMATED DECISION-MAKING,0.03759398496240601,"based on a future outcome yi ∈Y for individual i. Here, we assume Y = N, which can be relaxed.
45"
AUTOMATED DECISION-MAKING,0.03842940685045948,"Decision-making under uncertainty has long been studied in statistical decision theory [108, 32, 13,
46"
AUTOMATED DECISION-MAKING,0.03926482873851295,"99]. Let u(y, a) be a utility function that summarises the utility for the decision-maker. The optimal
47"
AUTOMATED DECISION-MAKING,0.040100250626566414,"decision is then
48"
AUTOMATED DECISION-MAKING,0.04093567251461988,"a⋆= arg max
a∈A X"
AUTOMATED DECISION-MAKING,0.04177109440267335,"y∈Y
u(a, y)p(y|a) .
(1)"
AUTOMATED DECISION-MAKING,0.042606516290726815,"The decision-maker usually neither knows yi nor p(y|a) at the time of the decision. Hence, the
49"
AUTOMATED DECISION-MAKING,0.04344193817878028,"decision must be based solely on xi. In an SML setting, a prediction model ˆp(y|x) is trained to
50"
AUTOMATED DECISION-MAKING,0.04427736006683375,"compute the predicted probability distribution (pmf) ˆπi = ˆp(y|xi) for individual i, with the support
51"
AUTOMATED DECISION-MAKING,0.045112781954887216,"on Y. Further, let ˆy(ˆπi) ∈Y be the classification made based on ˆπi. In simple settings, the decision
52"
AUTOMATED DECISION-MAKING,0.04594820384294068,"can be formulated as a decision function d(ˆπi) ∈A that is used to choose an appropriate action
53"
AUTOMATED DECISION-MAKING,0.04678362573099415,"based on ˆπi. In the binary y and a case, it reduces to a simple threshold τ, i.e., d(ˆπ) = I(ˆπ ≤τ),
54"
AUTOMATED DECISION-MAKING,0.047619047619047616,"where I is the indicator function and ˆπi = ˆp(y = 1 | xi). We often train a model ˆp(y|x) based
55"
AUTOMATED DECISION-MAKING,0.04845446950710108,"on previous data D = (y, X), drawn from a population p(y, x), where both xi and yi are known.
56"
AUTOMATED DECISION-MAKING,0.04928989139515455,"Replacing p(y|xi) with the predictive model ˆp(y|xi) in Eq. 1 gives an optimal decision.
57"
ALGORITHMIC FAIRNESS,0.05012531328320802,"1.2
Algorithmic Fairness
58"
ALGORITHMIC FAIRNESS,0.050960735171261484,"To define algorithmic fairness, we separate xi into protected and legitimate features xi = (xpi, xli);
59"
ALGORITHMIC FAIRNESS,0.05179615705931495,"we drop i to simplify notation. Here, xp ∈C indicates protected attributes, with C being the set of
60"
ALGORITHMIC FAIRNESS,0.05263157894736842,"different groups. Legally protected characteristics commonly identified in datasets include gender,
61"
ALGORITHMIC FAIRNESS,0.053467000835421885,"race, and age. Many fairness metrics aim to evaluate the fairness of an SML model for commonly
62"
ALGORITHMIC FAIRNESS,0.05430242272347535,"identified protected characteristics in datasets, including gender and race [49, 31, 82, 85].
63"
ALGORITHMIC FAIRNESS,0.05513784461152882,"Statistical parity, or demographic parity, is one of the central algorithmic fairness metrics [31, 136,
64"
ALGORITHMIC FAIRNESS,0.055973266499582286,"90, 82]. For statistical parity to hold, it requires that
65"
ALGORITHMIC FAIRNESS,0.05680868838763575,"Ex [ˆp(y|x) | xp] = Ex [ˆp(y|x)] ,
(2)"
ALGORITHMIC FAIRNESS,0.05764411027568922,"such that the model predictions, in expectation over x, need to be the same for the different groups [31,
66"
ALGORITHMIC FAIRNESS,0.05847953216374269,"136]. Given that the decision function d(π) is the same for the different groups, statistical parity results
67"
ALGORITHMIC FAIRNESS,0.059314954051796154,"in equal decisions for the different groups. However, we discuss later in this paper that, in practice,
68"
ALGORITHMIC FAIRNESS,0.06015037593984962,"statistical parity may exacerbate inequality or even result in unlawful discrimination [10, 74, 63].
69"
ALGORITHMIC FAIRNESS,0.06098579782790309,"Conditional statistical parity extends statistical parity to account for legitimate features xl. The
70"
ALGORITHMIC FAIRNESS,0.061821219715956555,"model predictions should only differ across protected groups to the extent that the difference is
71"
ALGORITHMIC FAIRNESS,0.06265664160401002,"conditional on legitimate factors [31, 136, 23]. This can be formalised as,
72"
ALGORITHMIC FAIRNESS,0.06349206349206349,"Ex [ˆp(y|x) | xl, xp] = Ex [ˆp(y|x) | xl] ,
(3)"
ALGORITHMIC FAIRNESS,0.06432748538011696,"so that, conditional on legitimate features xl, there should not be any difference in predictions between
73"
ALGORITHMIC FAIRNESS,0.06516290726817042,"groups given by the protected attribute. Below, we discuss the legitimacy of variables that correlate
74"
ALGORITHMIC FAIRNESS,0.06599832915622389,"to protected attributes [34, 77].
75"
ALGORITHMIC FAIRNESS,0.06683375104427736,"Other similar group comparison metrics have been proposed, such as error parity, balanced clas-
76"
ALGORITHMIC FAIRNESS,0.06766917293233082,"sification rate, and equalised odds [49, 31, 136, 90, 82, 30]. Also, more individual approaches
77"
ALGORITHMIC FAIRNESS,0.06850459482038429,"to parity have considered whether otherwise identical individuals are treated differently if they
78"
ALGORITHMIC FAIRNESS,0.06934001670843776,"have different protected attributes [34, 68]. Finally, ideas from causal inference and counterfactual
79"
ALGORITHMIC FAIRNESS,0.07017543859649122,"analysis have also been proposed to measure outcome consistency for individuals across protected
80"
ALGORITHMIC FAIRNESS,0.07101086048454469,"groups [75, 69, 106, 149, 26, 142, 92, 6]
81"
ANTI-DISCRIMINATION LAW,0.07184628237259816,"1.3
Anti-Discrimination Law
82"
ANTI-DISCRIMINATION LAW,0.07268170426065163,"The algorithmic fairness literature largely identifies statistical disparities in predicted outcomes for
83"
ANTI-DISCRIMINATION LAW,0.07351712614870509,"binary marginalised groups. Legally, discrimination is both broader and more detailed. Not all
84"
ANTI-DISCRIMINATION LAW,0.07435254803675856,"actions perceived as discriminatory are unlawful, and some non-obvious actions may be prohibited.
85"
ANTI-DISCRIMINATION LAW,0.07518796992481203,"Anti-discrimination law only applies to select duty-bearers in certain conditions [65]. Individuals’
86"
ANTI-DISCRIMINATION LAW,0.07602339181286549,"friendship choices being based on race are not legally regulated, despite sometimes seeming unfair [36,
87"
ANTI-DISCRIMINATION LAW,0.07685881370091896,"65]. It only applies to protected attributes. An algorithm that rejects a loan application because the
88"
ANTI-DISCRIMINATION LAW,0.07769423558897243,"applicant uses an Android phone rather than an iOS device may seem unfair because it does not
89"
ANTI-DISCRIMINATION LAW,0.0785296574770259,"reflect the true default risk but is a proxy for the applicant’s income [2, 76]. However, in isolation, this
90"
ANTI-DISCRIMINATION LAW,0.07936507936507936,"would not be unlawful discrimination under UK law because poverty is not a protected attribute [96].
91"
ANTI-DISCRIMINATION LAW,0.08020050125313283,"The prohibition on discrimination traces its legal roots to the Universal Declaration of Human Rights,
92"
ANTI-DISCRIMINATION LAW,0.0810359231411863,"which established equality and freedom from discrimination as fundamental human rights, further
93"
ANTI-DISCRIMINATION LAW,0.08187134502923976,"advanced in several international treaties [132, 88], and enacted as legislation worldwide spurred by
94"
ANTI-DISCRIMINATION LAW,0.08270676691729323,"the Civil Rights Movement [86, 65]. The United Kingdom implemented several anti-discrimination
95"
ANTI-DISCRIMINATION LAW,0.0835421888053467,"laws in the 20th century [118, 119, 116], which were consolidated in the Equality Act 2010 [40].
96"
ANTI-DISCRIMINATION LAW,0.08437761069340016,"The Equality Act protects “age; disability; gender reassignment; marriage and civil partnership;
97"
ANTI-DISCRIMINATION LAW,0.08521303258145363,"pregnancy and maternity; race; religion or belief; sex; sexual orientation” [40, s 4]. Algorithmic
98"
ANTI-DISCRIMINATION LAW,0.0860484544695071,"fairness literature has often oversimplified these protected characteristics as simply identifying visible
99"
ANTI-DISCRIMINATION LAW,0.08688387635756056,"traits when each has complex social meanings [57]. One complexity is, for example, the difference
100"
ANTI-DISCRIMINATION LAW,0.08771929824561403,"between a person with a protected attribute by biological fact or by identifying with a protected
101"
ANTI-DISCRIMINATION LAW,0.0885547201336675,"group [73]. UK anti-discrimination law distinguishes between direct discrimination and indirect
102"
ANTI-DISCRIMINATION LAW,0.08939014202172096,"discrimination. While analogous to the US disparate treatment and disparate impact doctrine, there
103"
ANTI-DISCRIMINATION LAW,0.09022556390977443,"are important distinctions, meaning they should not be so easily elided [1].
104"
ANTI-DISCRIMINATION LAW,0.0910609857978279,"Direct discrimination occurs when an individual is treated less favourably than another based on
105"
ANTI-DISCRIMINATION LAW,0.09189640768588136,"a protected characteristic [40, s 13]. To establish direct discrimination, it is necessary to identify
106"
ANTI-DISCRIMINATION LAW,0.09273182957393483,"the specific protected characteristic involved, demonstrate the less favourable treatment (by real or
107"
ANTI-DISCRIMINATION LAW,0.0935672514619883,"hypothetical comparison), and prove that this treatment was caused “but for” the protected attribute.
108"
ANTI-DISCRIMINATION LAW,0.09440267335004177,"The intention of the decision-maker is not required or necessary [123, 131].
109"
ANTI-DISCRIMINATION LAW,0.09523809523809523,"Indirect discrimination refers to a policy, criterion, or practice (PCP) that disproportionately
110"
ANTI-DISCRIMINATION LAW,0.0960735171261487,"disadvantages a group with a particular protected attribute compared to those without [40, s 19]. To
111"
ANTI-DISCRIMINATION LAW,0.09690893901420217,"prove indirect discrimination, one must identify such a PCP, show that it puts a group defined by its
112"
ANTI-DISCRIMINATION LAW,0.09774436090225563,"protected attribute at a particular disadvantage compared to those without such attribute, and evaluate
113"
ANTI-DISCRIMINATION LAW,0.0985797827903091,"whether it is justifiable as a proportionate means of achieving a legitimate aim.
114"
ANTI-DISCRIMINATION LAW,0.09941520467836257,"English common law is either in force or is the dominant influence in 80 legal systems that govern
115"
ANTI-DISCRIMINATION LAW,0.10025062656641603,"approximately 2.8 billion people, not including the US [28]. UK anti-discrimination law is very similar
116"
ANTI-DISCRIMINATION LAW,0.1010860484544695,"to numerous Commonwealth and common law jurisdictions, including Australia [3], Canada [20],
117"
ANTI-DISCRIMINATION LAW,0.10192147034252297,"India [47], New Zealand [91], South Africa [110], and the pending bill in Bangladesh [9]. European
118"
ANTI-DISCRIMINATION LAW,0.10275689223057644,"Union law also has broadly the same principles and discrimination case law evolved in parallel during
119"
ANTI-DISCRIMINATION LAW,0.1035923141186299,"the UK’s membership [45]. It is increasingly important to gain a nuanced understanding of unlawful
120"
ANTI-DISCRIMINATION LAW,0.10442773600668337,"discrimination in AI systems as new laws aim to prevent future harms [44, 16].
121"
CONTRIBUTIONS AND LIMITATIONS,0.10526315789473684,"1.4
Contributions and Limitations
122"
CONTRIBUTIONS AND LIMITATIONS,0.1060985797827903,"This paper makes four core contributions at the intersection of automated decision-making, fairness,
123"
CONTRIBUTIONS AND LIMITATIONS,0.10693400167084377,"and anti-discrimination doctrine.
124"
CONTRIBUTIONS AND LIMITATIONS,0.10776942355889724,"1. We formalise critical aspects of anti-discrimination doctrine into decision-theoretic formalism.
125"
CONTRIBUTIONS AND LIMITATIONS,0.1086048454469507,"2. We analyse the legal role of the data-generating process (DGP) and develop the DGP as a
126"
CONTRIBUTIONS AND LIMITATIONS,0.10944026733500417,"theoretical framework to formalise the legitimacy of the prediction target y and the features x in
127"
CONTRIBUTIONS AND LIMITATIONS,0.11027568922305764,"supervised models for automated decisions.
128"
CONTRIBUTIONS AND LIMITATIONS,0.1111111111111111,"3. Further, we consider the legal and practical effects of approximating the DGP in supervised
129"
CONTRIBUTIONS AND LIMITATIONS,0.11194653299916457,"models. We propose conditional estimation parity as a new, legally informed target.
130"
CONTRIBUTIONS AND LIMITATIONS,0.11278195488721804,"4. Finally, we provide recommendations on creating SML models that minimise the risk of unlawful
131"
CONTRIBUTIONS AND LIMITATIONS,0.1136173767752715,"discrimination in automated decision-making.
132"
CONTRIBUTIONS AND LIMITATIONS,0.11445279866332497,"Our paper is formally limited to analysing and providing novel recommendations for the UK. While
133"
CONTRIBUTIONS AND LIMITATIONS,0.11528822055137844,"we discuss related jurisdictions that are functionally similar and based on English common law,
134"
CONTRIBUTIONS AND LIMITATIONS,0.1161236424394319,"specific legal advice should be followed with respect to different jurisdictions. Accountability varies
135"
CONTRIBUTIONS AND LIMITATIONS,0.11695906432748537,"by jurisdiction and context, which is why our paper underscores the importance of careful, informed
136"
CONTRIBUTIONS AND LIMITATIONS,0.11779448621553884,"classification by experts with appropriate legal advice.
137"
AUTOMATED DECISIONS AND DISCRIMINATION,0.11862990810359231,"2
Automated Decisions and Discrimination
138"
LEGITIMACY OF TRUE DIFFERENCES,0.11946532999164577,"2.1
Legitimacy of True Differences
139"
LEGITIMACY OF TRUE DIFFERENCES,0.12030075187969924,"In SML, it is crucial to differentiate unlawful discrimination from mere statistical disparities and con-
140"
LEGITIMACY OF TRUE DIFFERENCES,0.12113617376775271,"cepts of algorithmic fairness. While formal equality may map to statistical parity, anti-discrimination
141"
LEGITIMACY OF TRUE DIFFERENCES,0.12197159565580618,"laws in the UK and related jurisdictions aim to achieve substantive equality. Despite the general
142"
LEGITIMACY OF TRUE DIFFERENCES,0.12280701754385964,"rule that individuals should not receive less favourable treatment based on their protected attributes,
143"
LEGITIMACY OF TRUE DIFFERENCES,0.12364243943191311,"courts acknowledge that treating all groups the same can actually disadvantage a protected group
144"
LEGITIMACY OF TRUE DIFFERENCES,0.12447786131996658,"and minimise important structural and true differences [137, 143]. Therefore, substantive equal-
145"
LEGITIMACY OF TRUE DIFFERENCES,0.12531328320802004,"ity may sometimes require legitimate differential treatment because of the true differences among
146"
LEGITIMACY OF TRUE DIFFERENCES,0.1261487050960735,"individuals [137, 52, 144, 139].
147"
LEGITIMACY OF TRUE DIFFERENCES,0.12698412698412698,"For instance, insurance decisions that might otherwise be construed as discriminatory – specifically
148"
LEGITIMACY OF TRUE DIFFERENCES,0.12781954887218044,"concerning gender reassignment, marriage, civil partnership, pregnancy, and sex discrimination –
149"
LEGITIMACY OF TRUE DIFFERENCES,0.1286549707602339,"are permissible if they are based on reliable actuarial data and executed reasonably [40, Sch 9. s
150"
LEGITIMACY OF TRUE DIFFERENCES,0.12949039264828738,"20]. Financial services can also “use age as a criterion for pricing risk, as it is a key risk factor
151"
LEGITIMACY OF TRUE DIFFERENCES,0.13032581453634084,"associated with for example, medical conditions, ability to drive, likelihood of making an insurance
152"
LEGITIMACY OF TRUE DIFFERENCES,0.1311612364243943,"claim and the ability to repay a loan” [117, para. 7.6]. These exemptions highlight legal recognition
153"
LEGITIMACY OF TRUE DIFFERENCES,0.13199665831244778,"that certain group distinctions, particularly those involving risk assessment, are relevant and necessary
154"
LEGITIMACY OF TRUE DIFFERENCES,0.13283208020050125,"for the equitable operation of such services. Similar statutory exemptions are found in other similar
155"
LEGITIMACY OF TRUE DIFFERENCES,0.1336675020885547,"anti-discrimination laws, including the European Union [43, art 2], Australia [8, s 30-47], Canada
156"
LEGITIMACY OF TRUE DIFFERENCES,0.13450292397660818,"[20, s 15], New Zealand [91, s 24-60] and South Africa [110, s 14].
157"
TRUE DATA GENERATING PROCESS,0.13533834586466165,"2.2
True Data Generating Process
158"
TRUE DATA GENERATING PROCESS,0.1361737677527151,"Therefore, an important aspect from the legal perspective that is overlooked in the existing literature
159"
TRUE DATA GENERATING PROCESS,0.13700918964076858,"is the distinction between a “true data-generating” process (DGP) and the estimated model ˆp(y|x).
160"
TRUE DATA GENERATING PROCESS,0.13784461152882205,"To formalise, we assume that there exists a true DGP, D ∼p(y, x), where Di = (yi, xi). Further, we
161"
TRUE DATA GENERATING PROCESS,0.13868003341687551,"use p(y|xtrue
i
) to denote the true probability (pmf) for individual i, given the true features xtrue
i
.
162"
TRUE DATA GENERATING PROCESS,0.13951545530492898,"We make multiple observations on the role of the “true” model and its use in connecting predictive
163"
TRUE DATA GENERATING PROCESS,0.14035087719298245,"modelling and legal reasoning.
164"
TRUE DATA GENERATING PROCESS,0.14118629908103592,"First, understanding the limits of predictive models is crucial to explore inherent uncertainties and
165"
TRUE DATA GENERATING PROCESS,0.14202172096908938,"limitations in predictions. The true model is, in practice, never observed or known. When developing
166"
TRUE DATA GENERATING PROCESS,0.14285714285714285,"ˆp(y|x), the target is often to select the model with the best predictive performance, which is closely
167"
TRUE DATA GENERATING PROCESS,0.14369256474519632,"connected to the role of the true DGP [15, 134, 135, 133]. For this reason, the “true” model may
168"
TRUE DATA GENERATING PROCESS,0.14452798663324978,"include features in xtrue
i
that are not observed in the data, sometimes referred to as an M-open setting
169"
TRUE DATA GENERATING PROCESS,0.14536340852130325,"when the “true” model is not included in the set of candidate models [15, 135].
170"
TRUE DATA GENERATING PROCESS,0.14619883040935672,"Second, we assume that p(y|xi) is a probability distribution over Y, introducing some level of
171"
TRUE DATA GENERATING PROCESS,0.14703425229741018,"aleatoric uncertainty in the true underlying process [95, 58, 114]. This means that perfect prediction
172"
TRUE DATA GENERATING PROCESS,0.14786967418546365,"of yi may not be possible, even with knowledge of the true DGP. The distinction between aleatoric
173"
TRUE DATA GENERATING PROCESS,0.14870509607351712,"and epistemic uncertainty is important from a legal perspective. The reason is simple: the uncertainty
174"
TRUE DATA GENERATING PROCESS,0.14954051796157058,"coming from estimation is the (legal) responsibility of the modeller, while the aleatoric uncertainty
175"
TRUE DATA GENERATING PROCESS,0.15037593984962405,"can instead be considered a true underlying general risk.
176"
TRUE DATA GENERATING PROCESS,0.15121136173767752,"Third, the true DGP connects to judicial legal reasoning. Courts must engage theoretically with
177"
TRUE DATA GENERATING PROCESS,0.15204678362573099,"legal and normative conceptions of what is justifiable and what constitutes unlawful discrimination.
178"
TRUE DATA GENERATING PROCESS,0.15288220551378445,"Judges consider legitimacy, proportionality, and necessity when evaluating actions, and hypothetical
179"
TRUE DATA GENERATING PROCESS,0.15371762740183792,"alternatives, that led to less favourable treatment. Although, courts are not oracles. Discrimination
180"
TRUE DATA GENERATING PROCESS,0.1545530492898914,"case law may not pinpoint what the perfect decision should have been. However, courts will engage
181"
TRUE DATA GENERATING PROCESS,0.15538847117794485,"in a similar theoretical process of reasoning about the decision-making process to the true DGP to
182"
TRUE DATA GENERATING PROCESS,0.15622389306599832,"understand whether the actions were justified or unlawful. We explain legal reasoning within this
183"
TRUE DATA GENERATING PROCESS,0.1570593149540518,"framework throughout the paper and in a real-world case on unlawful discrimination in algorithmic
184"
TRUE DATA GENERATING PROCESS,0.15789473684210525,"decision-making (see Appendix A).
185"
ESTIMATION PARITY,0.15873015873015872,"2.3
Estimation Parity
186"
ESTIMATION PARITY,0.1595655806182122,"Legally, distinguishing between a true difference and an estimated one is important. We approximate
187"
ESTIMATION PARITY,0.16040100250626566,"the true DGP with a model ˆp(y|x) based on training data when training an SML model. The
188"
ESTIMATION PARITY,0.16123642439431912,"approximation introduces estimation error
189"
ESTIMATION PARITY,0.1620718462823726,"ϵi = ˆπi −πi = ˆp(yi|xi) −p(yi|xtrue
i
) .
(4)"
ESTIMATION PARITY,0.16290726817042606,"Algorithmic fairness literature often assumes the absence of estimation error [see e.g., 49] or assumes
190"
ESTIMATION PARITY,0.16374269005847952,"that the true causal structure is known [150, 68, 26, 21]. In practice, this is rarely the case. Hence,
191"
ESTIMATION PARITY,0.164578111946533,"it is crucial, both practically and legally, to distinguish between the true underlying probabilities
192"
ESTIMATION PARITY,0.16541353383458646,"πi and the estimated probabilities ˆπi. While the true underlying probability may sometimes be
193"
ESTIMATION PARITY,0.16624895572263992,"defensible (Section 2.2), introducing an estimation error that disadvantages individuals based on
194"
ESTIMATION PARITY,0.1670843776106934,"protected attributes invokes discrimination liability.
195"
ESTIMATION PARITY,0.16791979949874686,"As the model will try to approximate the true data-generating process, modellers’ expectations are
196"
ESTIMATION PARITY,0.16875522138680032,"difficult to ascertain. The law is unlikely to set a deterministic standard that any adverse effects of
197"
ESTIMATION PARITY,0.1695906432748538,"estimation will make a modeller liable. The modeller should try to approximate the true model as
198"
ESTIMATION PARITY,0.17042606516290726,"much as possible [see 4, 141, 135, 133, for discussions on model misspecification]. However, where
199"
ESTIMATION PARITY,0.17126148705096073,"an estimation disparity reaches a threshold for discriminatory effects, the legal evaluation would
200"
ESTIMATION PARITY,0.1720969089390142,"require analysing the steps taken to test and mitigate estimation disparity (even though the intent is
201"
ESTIMATION PARITY,0.17293233082706766,"immaterial).
202"
ESTIMATION PARITY,0.17376775271512113,"The potential bias in training data presents a risk that the estimation model will introduce bias against
203"
ESTIMATION PARITY,0.1746031746031746,"individuals with protected attributes (Section 2.6). Historical discriminatory lending practices, for
204"
ESTIMATION PARITY,0.17543859649122806,"example, could be perpetuated through biased training data [18, 104]. Such biased estimations
205"
ESTIMATION PARITY,0.17627401837928153,"may introduce biased outcomes that are not reflective of true differences, potentially leading to
206"
ESTIMATION PARITY,0.177109440267335,"discriminatory outcomes. Therefore, we introduce “Conditional Estimation Parity” to formalise the
207"
ESTIMATION PARITY,0.17794486215538846,"legal context of estimation.
208"
ESTIMATION PARITY,0.17878028404344193,"Conditional Estimation Parity is the difference in estimation error between groups with a protected
209"
ESTIMATION PARITY,0.1796157059314954,"attribute, given legitimate features, i.e.,
210"
ESTIMATION PARITY,0.18045112781954886,"Ex[ϵ | xp, xl] = Ex[ϵ | xl] .
(5)"
ESTIMATION PARITY,0.18128654970760233,"Reducing the error in Eq. 4 is expected to diminish the risk of conditional estimation disparity.
211"
ESTIMATION PARITY,0.1821219715956558,"However, assessing conditional estimation parity is complex due to inherent challenges in evaluating
212"
ESTIMATION PARITY,0.18295739348370926,"estimation error.
213"
ESTIMATION PARITY,0.18379281537176273,"It is crucial to examine both mathematical and legal causal theories of why certain differences are
214"
ESTIMATION PARITY,0.1846282372598162,"legitimate bases to make classification distinctions [71]. We examine the mathematical basis for
215"
ESTIMATION PARITY,0.18546365914786966,"identifying statistical disparities in the context of unlawful discrimination. In Section 2.5, 2.7, and 2.6
216"
ESTIMATION PARITY,0.18629908103592313,"we consider the causal relationships between legitimate differentiation and unlawful discrimination.
217"
STATISTICAL DISPARITIES AND PRIMA FACIE DISCRIMINATION,0.1871345029239766,"2.4
Statistical Disparities and Prima Facie Discrimination
218"
STATISTICAL DISPARITIES AND PRIMA FACIE DISCRIMINATION,0.18796992481203006,"To initiate a claim for discrimination, a claimant must establish a prima facie case [37, 40, s 136].
219"
STATISTICAL DISPARITIES AND PRIMA FACIE DISCRIMINATION,0.18880534670008353,"Sufficient evidence must be produced to show that unlawful discrimination may have occurred,
220"
STATISTICAL DISPARITIES AND PRIMA FACIE DISCRIMINATION,0.189640768588137,"including by showing discriminatory effects or harm against an individual or group caused by the
221"
STATISTICAL DISPARITIES AND PRIMA FACIE DISCRIMINATION,0.19047619047619047,"decision-maker’s action [37, 65]. Statistical evidence can be used to prove less favourable treatment
222"
STATISTICAL DISPARITIES AND PRIMA FACIE DISCRIMINATION,0.19131161236424393,"or particular disadvantage, but by design, it shows correlations, and “a correlation is not the same
223"
STATISTICAL DISPARITIES AND PRIMA FACIE DISCRIMINATION,0.1921470342522974,"as a causal link” [130, para. 28]. We explain the threshold for legal causation at the trial stage in
224"
STATISTICAL DISPARITIES AND PRIMA FACIE DISCRIMINATION,0.19298245614035087,"Section 2.5. Although, at this stage, a mere correlation between the adverse effect on the person
225"
STATISTICAL DISPARITIES AND PRIMA FACIE DISCRIMINATION,0.19381787802840433,"and the decision-maker’s action will suffice [65]. The size of the disparity is relevant. Smaller
226"
STATISTICAL DISPARITIES AND PRIMA FACIE DISCRIMINATION,0.1946532999164578,"disparities are less likely to trigger legal inquiry under anti-discrimination laws [127]. Courts will
227"
STATISTICAL DISPARITIES AND PRIMA FACIE DISCRIMINATION,0.19548872180451127,"compare statistical evidence showing the different effects and outcomes between a disadvantaged
228"
STATISTICAL DISPARITIES AND PRIMA FACIE DISCRIMINATION,0.19632414369256473,"group compared to a group without the protected attribute. The significance of the statistical disparity
229"
STATISTICAL DISPARITIES AND PRIMA FACIE DISCRIMINATION,0.1971595655806182,"hinges on the specifics of the case [127, 124]. The thresholds for statistical significance are flexible
230"
STATISTICAL DISPARITIES AND PRIMA FACIE DISCRIMINATION,0.19799498746867167,"and often resisted by courts to avoid excessive dependence on data [138]. The UK has specifically
231"
STATISTICAL DISPARITIES AND PRIMA FACIE DISCRIMINATION,0.19883040935672514,"avoided thresholds like those used to measure statistically significant disparity in the US [105, 10].
232"
STATISTICAL DISPARITIES AND PRIMA FACIE DISCRIMINATION,0.1996658312447786,"Statistical disparities, as identified through algorithmic fairness metrics, may indicate a reason to
233"
STATISTICAL DISPARITIES AND PRIMA FACIE DISCRIMINATION,0.20050125313283207,"consider whether discrimination has arisen. However, without taking context and potential true and
234"
STATISTICAL DISPARITIES AND PRIMA FACIE DISCRIMINATION,0.20133667502088554,"legitimate differences into account, these disparities hold little legal weight (see Section 2.1). We can
235"
STATISTICAL DISPARITIES AND PRIMA FACIE DISCRIMINATION,0.202172096908939,"formalise this as the legal target being to minimise the conditional estimation disparity
236"
STATISTICAL DISPARITIES AND PRIMA FACIE DISCRIMINATION,0.20300751879699247,"ω = ||Ex [ϵi | xl, xp] −Ex [ϵi | xl] ||2,
(6)"
STATISTICAL DISPARITIES AND PRIMA FACIE DISCRIMINATION,0.20384294068504594,"where || · ||2 is the euclidean norm. This target generalises the idea of minimising conditional
237"
STATISTICAL DISPARITIES AND PRIMA FACIE DISCRIMINATION,0.2046783625730994,"statistical parity. If we assume true conditional statistical parity, i.e.
238"
STATISTICAL DISPARITIES AND PRIMA FACIE DISCRIMINATION,0.20551378446115287,"Ex

p(yi|xtrue
i
) | xl, xp

= Ex

p(yi|xtrue
i
) | xl

,
(7)"
STATISTICAL DISPARITIES AND PRIMA FACIE DISCRIMINATION,0.20634920634920634,"then the target in Eq. 6 will be reduced to minimise the conditional statistical parity (see Eq. 3).
239"
STATISTICAL DISPARITIES AND PRIMA FACIE DISCRIMINATION,0.2071846282372598,"Although, this is only true as long as there are no true differences.
240"
STATISTICAL DISPARITIES AND PRIMA FACIE DISCRIMINATION,0.20802005012531327,"Hence, if true statistical parity does not hold, it is explained by true differences between groups. If
241"
STATISTICAL DISPARITIES AND PRIMA FACIE DISCRIMINATION,0.20885547201336674,"there is a true difference, such as age in financial services, forcing conditional statistical parity would
242"
STATISTICAL DISPARITIES AND PRIMA FACIE DISCRIMINATION,0.2096908939014202,"harm the protected group, most likely resulting in unlawful discrimination. This result aligns with
243"
STATISTICAL DISPARITIES AND PRIMA FACIE DISCRIMINATION,0.21052631578947367,"previous observations about the risks of forcing parity metrics [31, 144, 54]. Courts may need to be
244"
STATISTICAL DISPARITIES AND PRIMA FACIE DISCRIMINATION,0.21136173767752714,"more flexible in the type of statistical data they consider to establish a prima facie case by considering
245"
STATISTICAL DISPARITIES AND PRIMA FACIE DISCRIMINATION,0.2121971595655806,"non-comparative adverse effects in their assessment. Therefore, deferring to conditional estimation
246"
STATISTICAL DISPARITIES AND PRIMA FACIE DISCRIMINATION,0.21303258145363407,"parity provides an avenue for a contextually informed assessment.
247"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.21386800334168754,"2.5
Legal Causation and the Utility Function
248"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.214703425229741,"To lawyers, causation is the relationship between an act, i.e., an action or decision, and its effect,
249"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.21553884711779447,"which requires two questions: (1) factually, but for the act, would the consequences have occurred;
250"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.21637426900584794,"(2) is the act a substantial cause of the consequence to apply responsibility. We are concerned with
251"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.2172096908939014,"the first question. Direct discrimination “requires a causal link between the less favourable treatment
252"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.21804511278195488,"and the protected characteristic”; indirect discrimination “requires a causal link between the PCP and
253"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.21888053467000834,"the particular disadvantage suffered by the group and individual” [130, para. 25]. In an algorithmic
254"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.2197159565580618,"context, this causal link requires asking whether i would have received the same action or decision
255"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.22055137844611528,"a, but for their protected attribute xp or the PCP that indirectly relates to their protected attribute
256"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.22138680033416874,"xp [122, 123]. For instance, whether an individual would have suffered the disadvantage but for the
257"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.2222222222222222,"protected attribute would be discriminatory regardless of the decision-maker’s intention [1]. This is a
258"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.22305764411027568,"notable distinction from certain aspects of US discrimination doctrine.
259"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.22389306599832914,"From a decision-theoretic perspective, the protected attribute xp can affect the decision a either
260"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.2247284878863826,"through the utility function u(a, y) or through the model ˆp(y|x). Discrimination may occur if
261"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.22556390977443608,"the utility function in Eq. 1 differs for different groups defined by the protected attribute. Such a
262"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.22639933166248954,"difference would mean that an individual or whole group with a protected attribute is treated less
263"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.227234753550543,"favourably than those without a protected attribute given the same model ˆp(y|x). Such a difference in
264"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.22807017543859648,"the utility function would risk unlawful discrimination. Specifically, if u(a, y) is changed for different
265"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.22890559732664995,"persons, either directly based on a protected attribute or indirectly has the effect of disproportionately
266"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.2297410192147034,"disadvantaging a group with a protected characteristic without justification (see Section 2.6).
267"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.23057644110275688,"Having different ˆp(y|x), on the other hand, would mean that there is a legal causation between the
268"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.23141186299081035,"decision a and xp. This might either be motivated by true differences (see Section 2.1) or a result of
269"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.2322472848788638,"conditional estimation disparity. In the latter case, this might be a case of legal causation, i.e., that the
270"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.23308270676691728,"model is poor, and hence, the modelling has resulted in disadvantaging a protected group. Therefore,
271"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.23391812865497075,"we can view the causal structure of ˆp(y|x) as central to avoiding unlawful discrimination. However,
272"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.23475355054302421,"not considering causal structures could lead to conditional estimation disparity, and potentially result
273"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.23558897243107768,"in unlawful discrimination.
274"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.23642439431913115,"Legal causation focuses on the legal causal link between xp and the decision a. In addition, legal
275"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.23725981620718462,"causation is less formal than common definitions of causal effects in ML. Courts, at least outside of
276"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.23809523809523808,"the US, are effects-orientated, and a wide range of forms of a “legal causal link” could be identified
277"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.23893065998329155,"[109, 65]. Much of the causal-based fairness literature formulates “causation” on the true causal
278"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.23976608187134502,"model structure in ˆp(y|x), i.e., the study of the causal effect of x, due to outside interventions on y
279"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.24060150375939848,"[101, 10, 149, 21]. However, this formulation is not the same as that of legal causation.
280"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.24143692564745195,"In this discussion, the parallels to other discrimination studies become evident in how it would
281"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.24227234753550542,"affect automated decision-making, particularly taste-based and statistical discrimination. Taste-based
282"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.24310776942355888,"discrimination[11], could arise if only the utility function u(a, y) unjustifiably disfavours a group
283"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.24394319131161235,"based on protected attributes xp. Statistical discrimination, on the other hand, arises when decision-
284"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.24477861319966582,"makers use group-level statistics as proxies for individual characteristics due to imperfect information
285"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.24561403508771928,"[7, 102]. Statistical discrimination parallels the disadvantaging of a group due to having different
286"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.24644945697577275,"ˆp(x|y). While these types of discrimination are generally prohibited, statistical discrimination can be
287"
LEGAL CAUSATION AND THE UTILITY FUNCTION,0.24728487886382622,"legally permissible in some circumstances (see Section 2.1).
288"
LEGITIMATE AIM AND Y,0.24812030075187969,"2.6
Legitimate aim and y
289"
LEGITIMATE AIM AND Y,0.24895572263993315,"Decision-makers must consider the legitimacy of using an SML model by explicitly defining its
290"
LEGITIMATE AIM AND Y,0.24979114452798662,"purpose and the outcome variable y. In algorithm design, social implications should be considered [87,
291"
LEGITIMATE AIM AND Y,0.2506265664160401,"57, 63]. Additionally, this aligns the model’s use with legal expectations.
292"
LEGITIMATE AIM AND Y,0.25146198830409355,"If the court believes sufficient evidence of discrimination exists, the burden shifts to the respondent to
293"
LEGITIMATE AIM AND Y,0.252297410192147,"disprove allegations of unlawful discrimination [38]. Indirect discrimination can be justified if the
294"
LEGITIMATE AIM AND Y,0.2531328320802005,"PCP is a proportionate means of achieving a legitimate aim [40, s 19(2)(d)]. Identifying a legitimate
295"
LEGITIMATE AIM AND Y,0.25396825396825395,"aim is closely connected to the choice of y, the unknown entity used for decision-making. If the
296"
LEGITIMATE AIM AND Y,0.2548036758563074,"choice of y is legitimate based on context and the benefit outweighs any potential harm, there is a
297"
LEGITIMATE AIM AND Y,0.2556390977443609,"lower risk of unlawful discrimination [35].
298"
LEGITIMATE AIM AND Y,0.25647451963241436,"The legitimacy of the aim depends on the decision-makers’ raison d’être [65]. In Homer, the Court
299"
LEGITIMATE AIM AND Y,0.2573099415204678,"established a legitimate aim must “correspond to a real need and the means used must be appropriate
300"
LEGITIMATE AIM AND Y,0.2581453634085213,"with a view to achieving the objective and be necessary to that end” [128, 35, 39]. In lending, it is
301"
LEGITIMATE AIM AND Y,0.25898078529657476,"a legitimate aim to protect the repayment of their loans or at least secure their loans. In fact, “the
302"
LEGITIMATE AIM AND Y,0.2598162071846282,"mortgage market could not survive without that aim being realised” [126, para. 79].
303"
LEGITIMATE AIM AND Y,0.2606516290726817,"For a legitimate y to be an exception to indirect discrimination, the PCP must be a proportionate
304"
LEGITIMATE AIM AND Y,0.26148705096073516,"means of achieving the legitimate y [40, s 19(2)(d)]. To be proportionate, it must be an appropriate
305"
LEGITIMATE AIM AND Y,0.2623224728487886,"means of achieving the legitimate aim and (reasonably) necessary to do so [128]. Such analysis
306"
LEGITIMATE AIM AND Y,0.2631578947368421,"will turn on the facts of each case. However, it will require evaluating whether the design choices
307"
LEGITIMATE AIM AND Y,0.26399331662489556,"were “appropriate with a view to achieving the objective and be necessary” by weighing the need
308"
LEGITIMATE AIM AND Y,0.264828738512949,"against the seriousness of detriment to the disadvantaged group [39, para. 151]. This will require
309"
LEGITIMATE AIM AND Y,0.2656641604010025,"considering whether non-discriminatory alternatives were available [128]. Measures to improve
310"
LEGITIMATE AIM AND Y,0.26649958228905596,"accuracy, maximise benefits over costs, minimise estimation error, or condition for protected attributes
311"
LEGITIMATE AIM AND Y,0.2673350041771094,"may all be relevant considerations for whether the modeller’s choices were proportionate means of
312"
LEGITIMATE AIM AND Y,0.2681704260651629,"achieving a legitimate y.
313"
LEGITIMATE AIM AND Y,0.26900584795321636,"If the estimated outcome ˜y approximates the true outcome y, this can lead to biased predictions. Let
314"
LEGITIMATE AIM AND Y,0.2698412698412698,"γi = ||p(˜yi|xtrue
i
) −p(yi|xtrue
i
)||2 ,
(8)"
LEGITIMATE AIM AND Y,0.2706766917293233,"then, if the expectation of γ condition on xl shows a disparity, i.e.,
315"
LEGITIMATE AIM AND Y,0.27151211361737676,"Ex[γ | xp, xl] ̸= Ex[γ | xl] ,
(9)"
LEGITIMATE AIM AND Y,0.2723475355054302,"it suggests the use of ˜y is inappropriate and might be discriminatory.
316"
LEGITIMATE AIM AND Y,0.2731829573934837,"To illustrate with an example, if a bank’s training data is outdated or sourced from a different country,
317"
LEGITIMATE AIM AND Y,0.27401837928153716,"it may not accurately represent the current population relevant to the model. This discrepancy can
318"
LEGITIMATE AIM AND Y,0.27485380116959063,"lead to biased estimations, particularly if the data reflects historical prejudices. For instance, the
319"
LEGITIMATE AIM AND Y,0.2756892230576441,"model might unjustly associate certain demographics with higher default risk, not because of true
320"
LEGITIMATE AIM AND Y,0.27652464494569756,"differences but biased historical data [as warned in 33].
321"
LEGITIMATE X,0.27736006683375103,"2.7
Legitimate x
322"
LEGITIMATE X,0.2781954887218045,"One of the more crucial aspects of SML for automated decision-making is the choice of features x.
323"
LEGITIMATE X,0.27903091060985796,"The aim and y will help inform the choice of features to include in the model. We can separate three
324"
LEGITIMATE X,0.27986633249791143,"types of features from a legal perspective: features with protected attributes xp, legitimate features xl,
325"
LEGITIMATE X,0.2807017543859649,"and non-legitimate or illegitimate features xn. The distinction between xl and xn depends on whether
326"
LEGITIMATE X,0.28153717627401836,"the feature can be considered legitimately related to y (see Section 2.5). Causal fairness literature
327"
LEGITIMATE X,0.28237259816207183,"has engaged with questions of discriminatory variables through the lens of proxy discrimination
328"
LEGITIMATE X,0.2832080200501253,"[69, 115]. Proxy discrimination has a specific legal meaning under UK law that relates to direct
329"
LEGITIMATE X,0.28404344193817876,"discrimination, unlike much of the US literature on proxy discrimination that relates to indirect forms
330"
LEGITIMATE X,0.28487886382623223,"of discrimination. Here, we explain the UK legal implications of such causal relationships between
331"
LEGITIMATE X,0.2857142857142857,"variables and we provide a real-world example in Appendix A.
332"
DIRECT DISCRIMINATION AND REMOVING XP,0.28654970760233917,"2.7.1
Direct Discrimination and Removing xp
333"
DIRECT DISCRIMINATION AND REMOVING XP,0.28738512949039263,"Direct discrimination in automated decisions may arise when members of, or an entire protected
334"
DIRECT DISCRIMINATION AND REMOVING XP,0.2882205513784461,"group, is affected. Where a model ˆp(y|x) uses a protected attribute xp, and there is a difference
335"
DIRECT DISCRIMINATION AND REMOVING XP,0.28905597326649957,"in predictions between the protected groups defined by xp, this risk arises. Models have directly
336"
DIRECT DISCRIMINATION AND REMOVING XP,0.28989139515455303,"used protected characteristics, giving rise to direct discrimination [94, see discussion in Appendix].
337"
DIRECT DISCRIMINATION AND REMOVING XP,0.2907268170426065,"Direct discrimination may arise when a feature is an exact proxy for a protected attribute. In Lee v
338"
DIRECT DISCRIMINATION AND REMOVING XP,0.29156223893065997,"Ashers, Lady Hale explained that the risk of direct discrimination also arises if a decision is based on
339"
DIRECT DISCRIMINATION AND REMOVING XP,0.29239766081871343,"a feature that “is not the protected characteristic itself but some proxy for it” [129]. Therefore, direct
340"
DIRECT DISCRIMINATION AND REMOVING XP,0.2932330827067669,"discrimination can arise even where xp has been removed because there is a feature which is an exact
341"
DIRECT DISCRIMINATION AND REMOVING XP,0.29406850459482037,"proxy that is “indissociable” or has an “exact correspondence” to xp [130, 129]. Formally, we can
342"
DIRECT DISCRIMINATION AND REMOVING XP,0.29490392648287383,"define an exact proxy as a feature ˜xp with a perfect or almost perfect correlation with xp [115].
343"
DIRECT DISCRIMINATION AND REMOVING XP,0.2957393483709273,"UK courts have accepted that an exact proxy would be pregnancy because “pregnancy is unique
344"
DIRECT DISCRIMINATION AND REMOVING XP,0.29657477025898077,"to the female sex” [121, 125]. If a model uses pregnancy or maternity leave as a feature, collected
345"
DIRECT DISCRIMINATION AND REMOVING XP,0.29741019214703424,"from CV information, for example, it would have the effect of using an exact proxy ˜xp that could
346"
DIRECT DISCRIMINATION AND REMOVING XP,0.2982456140350877,"hypothetically be the basis for a direct discrimination claim.
347"
DIRECT DISCRIMINATION AND REMOVING XP,0.29908103592314117,"Given the relevance of xp to direct discrimination, modellers have been encouraged to remove pro-
348"
DIRECT DISCRIMINATION AND REMOVING XP,0.29991645781119464,"tected attributes when designing ML models [105, 62, 48]. These claims are usually based on the US
349"
DIRECT DISCRIMINATION AND REMOVING XP,0.3007518796992481,"Equal Protection Clause, which subjects classifications based on certain protected characteristics, such
350"
DIRECT DISCRIMINATION AND REMOVING XP,0.30158730158730157,"as race, to strict scrutiny [146]. The focus on excluding certain data inputs is one form of discrimina-
351"
DIRECT DISCRIMINATION AND REMOVING XP,0.30242272347535504,"tion prevention [146, 46], but not under UK law. Further, simply removing protected characteristics
352"
DIRECT DISCRIMINATION AND REMOVING XP,0.3032581453634085,"reduces accuracy and utility [150, 66], and does not remove the risk of discrimination [34, 79, 72].
353"
DIRECT DISCRIMINATION AND REMOVING XP,0.30409356725146197,"This reasoning connects to the true DGP. If a protected attribute like gender is inherent in the DGP,
354"
DIRECT DISCRIMINATION AND REMOVING XP,0.30492898913951544,"removing it does not eliminate discrimination but instead may introduce it. Taking a gender-neutral
355"
DIRECT DISCRIMINATION AND REMOVING XP,0.3057644110275689,"approach to recidivism predictions may have the adverse effect of discrimination against women who
356"
DIRECT DISCRIMINATION AND REMOVING XP,0.3065998329156224,"would otherwise have received lower risk scores [31]. In Loomis, the Court accepted that in recidivism
357"
DIRECT DISCRIMINATION AND REMOVING XP,0.30743525480367584,"algorithms, “if the inclusion of gender promotes the accuracy, it serves the interests of institutions and
358"
DIRECT DISCRIMINATION AND REMOVING XP,0.3082706766917293,"defendants, rather than a discriminatory purpose” [112, 766]. Hence, if the inclusion of xp improves
359"
DIRECT DISCRIMINATION AND REMOVING XP,0.3091060985797828,"the accuracy and benefits the protected group, it may avoid the risk of discriminatory purposes. There
360"
DIRECT DISCRIMINATION AND REMOVING XP,0.30994152046783624,"is an absence of any legal guidance in the UK on the relationship between true probabilities and
361"
DIRECT DISCRIMINATION AND REMOVING XP,0.3107769423558897,"protected attributes in automated decision-making. Pending further legal guidance, it is important to
362"
DIRECT DISCRIMINATION AND REMOVING XP,0.3116123642439432,"carefully consider whether including xp is relevant to promote accuracy and conditional estimation
363"
DIRECT DISCRIMINATION AND REMOVING XP,0.31244778613199664,"parity. Removing protected attributes often ignores the true probabilities for the legitimate differences
364"
DIRECT DISCRIMINATION AND REMOVING XP,0.3132832080200501,"between protected groups, affecting the lawfulness of its outcomes.
365"
DIRECT DISCRIMINATION AND REMOVING XP,0.3141186299081036,"Therefore, removing xp will not avoid liability for unlawful direct discrimination by itself. Even
366"
DIRECT DISCRIMINATION AND REMOVING XP,0.31495405179615704,"if a model ignores xp, in practice, it may rely on other data points acting as proxies with “exact
367"
DIRECT DISCRIMINATION AND REMOVING XP,0.3157894736842105,"correspondence” to a protected characteristic ˜xp. Importantly, this diverges from US law and
368"
DIRECT DISCRIMINATION AND REMOVING XP,0.316624895572264,"highlights that intention is immaterial to UK direct discrimination [Cf. e.g., 5, 115]. UK law focuses
369"
DIRECT DISCRIMINATION AND REMOVING XP,0.31746031746031744,"on the discriminatory effects rather than a formalistic view of whether xp is considered or not.
370"
DEFINING XL AND XN,0.3182957393483709,"2.7.2
Defining xl and xn
371"
DEFINING XL AND XN,0.3191311612364244,"Indirect discrimination may arise if a PCP appears to apply equally to everyone but disadvantages
372"
DEFINING XL AND XN,0.31996658312447784,"members of a protected group. Both forms of discrimination can arise using an exact proxy or a
373"
DEFINING XL AND XN,0.3208020050125313,"weak proxy in a PCP. Therefore, identifying legitimate features is challenging when many features
374"
DEFINING XL AND XN,0.3216374269005848,"correlate to protected groups. We define non-legitimate features xn as features not legitimate in
375"
DEFINING XL AND XN,0.32247284878863824,"the context of the true DGP (Section 2.2). In practice, this means a non-legitimate feature is one
376"
DEFINING XL AND XN,0.3233082706766917,"that, if included, would not contribute to the predictive performance of the optimal model, i.e., the
377"
DEFINING XL AND XN,0.3241436925647452,"one with the lowest estimation error (Section 2.3). Therefore, xn would not improve the predictive
378"
DEFINING XL AND XN,0.32497911445279865,"performance if a modeller had the true features.
379"
DEFINING XL AND XN,0.3258145363408521,"For example, hair length strongly correlates to gender in many cultural contexts but is unlikely to
380"
DEFINING XL AND XN,0.3266499582289056,"contribute to the consumers’ true default risk. Boyarskaya et al. explain the absence of a “causal
381"
DEFINING XL AND XN,0.32748538011695905,"story” between hair length and loan repayment because hair length would not be part of a true model
382"
DEFINING XL AND XN,0.3283208020050125,"for the risk of default [19]. Therefore, hair length is an example of xn in a lending context.
383"
DEFINING XL AND XN,0.329156223893066,"For comparison, the legitimacy of zip codes illustrates the nuanced nature of legitimate features.
384"
DEFINING XL AND XN,0.32999164578111945,"While a zip code may correlate with race in some contexts, it might be a legitimate variable in
385"
DEFINING XL AND XN,0.3308270676691729,"other situations. For example, in an application for home insurance covering flood risk, zip codes
386"
DEFINING XL AND XN,0.3316624895572264,"are invaluable proxies for granular information such as geographical features, land topography and
387"
DEFINING XL AND XN,0.33249791144527985,"historical flooding. Therefore, in the best model for property flood insurance decisions, zip code will
388"
DEFINING XL AND XN,0.3333333333333333,"improve the predictive performance as a legitimate proxy for data within the true DGP. However, in a
389"
DEFINING XL AND XN,0.3341687552213868,"university application, there should be no predictive or causal relationship to merit for acceptance. In
390"
DEFINING XL AND XN,0.33500417710944025,"such cases, zip code likely acts as a proxy for race or the unprotected characteristic of socio-economic
391"
DEFINING XL AND XN,0.3358395989974937,"status and would be xn. So, in some circumstances, the zip code would be legitimate xl, but in others,
392"
DEFINING XL AND XN,0.3366750208855472,"it may not be xn. It will also be relevant to consider whether a less discriminatory feature is available,
393"
DEFINING XL AND XN,0.33751044277360065,"i.e., one with less correlation to a protected attribute that is equally predictive.
394"
DEFINING XL AND XN,0.3383458646616541,"As explored in Appendix A, in lending, information about income and debts are likely to be legitimate
395"
DEFINING XL AND XN,0.3391812865497076,"features xl. Credit scores can be a proxy for a person’s financial position, as well as protected
396"
DEFINING XL AND XN,0.34001670843776105,"attributes [18, 60]. However, the complexity of calculating credit scores means it is more valuable for
397"
DEFINING XL AND XN,0.3408521303258145,"inferring income, debt repayments, and history of credit. Credit scores, or related features, would
398"
DEFINING XL AND XN,0.341687552213868,"have a material impact on the true model for default, and then would be a legitimate feature xl.
399"
DEFINING XL AND XN,0.34252297410192145,"Given that nearly, all features may contain some information on protected attributes, even legitimate
400"
DEFINING XL AND XN,0.3433583959899749,"factors [30], this approach explains the need to assess the strength of this dependence and whether the
401"
DEFINING XL AND XN,0.3441938178780284,"feature contributes significantly to the model’s prediction and can be argued to be part of a true DGP.
402"
FEATURE CONSTRUCTION FROM X,0.34502923976608185,"2.7.3
Feature construction from x
403"
FEATURE CONSTRUCTION FROM X,0.3458646616541353,"The distinction between xl and xn also gives rise to problems in automatic feature construction, such
404"
FEATURE CONSTRUCTION FROM X,0.3467000835421888,"as using deep neural networks. If features are constructed automatically using a combination of xl
405"
FEATURE CONSTRUCTION FROM X,0.34753550543024225,"and xn, indirect and direct discrimination are risks. As an example, an applicant’s resume contains
406"
FEATURE CONSTRUCTION FROM X,0.3483709273182957,"legitimate features xl for recruitment prediction. However, the detailed granularity of many resumes
407"
FEATURE CONSTRUCTION FROM X,0.3492063492063492,"also gives rise to the problem of non-legitimate information, such as maternity leave or women-only
408"
FEATURE CONSTRUCTION FROM X,0.35004177109440265,"sports or other information that may contain information on other protected attributes. Hence, there
409"
FEATURE CONSTRUCTION FROM X,0.3508771929824561,"needs to be an active choice of only including legitimate features xl from available data in the model.
410"
CONCLUSIONS,0.3517126148705096,"3
Conclusions
411"
CONCLUSIONS,0.35254803675856305,"Minimising unlawful discrimination in automated decision-making requires a nuanced and contextual
412"
CONCLUSIONS,0.3533834586466165,"approach. While it is beyond our scope to offer specific legal advice, our findings underscore several
413"
CONCLUSIONS,0.35421888053467,"key considerations to identify and mitigate potential discrimination effectively:
414"
CONCLUSIONS,0.35505430242272346,"1. Assess data legitimacy. Carefully examine if the data, both the target variable (y) and features (x),
415"
CONCLUSIONS,0.3558897243107769,"are legitimate for the specific context (Sections 2.6 and 2.7). Legal analysis should inform what is
416"
CONCLUSIONS,0.3567251461988304,"legitimate in a specific setting.
417"
CONCLUSIONS,0.35756056808688386,"2. Build an accurate model. Strive to approximate the true DGP p(y|x), using only legitimate
418"
CONCLUSIONS,0.3583959899749373,"features xl. Reasonable, necessary, and proportionate steps must be taken to minimise estimation
419"
CONCLUSIONS,0.3592314118629908,"error and aim for estimation parity (Section 2.3). This may entail model inference, interrogating
420"
CONCLUSIONS,0.36006683375104426,"social biases in the data, and scrutinising the estimated model.
421"
CONCLUSIONS,0.3609022556390977,"3. Evaluate statistical disparity. Given the best model ˆp(y|x), assess for conditional statistical parity
422"
CONCLUSIONS,0.3617376775271512,"by examining outcomes across groups with protected characteristics (Section 2.4). If a model’s
423"
CONCLUSIONS,0.36257309941520466,"performance improves by including protected attributes, consider:
424"
CONCLUSIONS,0.3634085213032581,"(a) Identify whether conditional statistical parity is unattainable or undesirable based on true
425"
CONCLUSIONS,0.3642439431913116,"group differences. This requires stringent analysis into whether differences stem from prior
426"
CONCLUSIONS,0.36507936507936506,"injustice or legitimate variation.
427"
CONCLUSIONS,0.3659147869674185,"(b) Incorporate further legitimate features xl that could minimise statistical disparities by “ex-
428"
CONCLUSIONS,0.366750208855472,"plaining away” the performance gained by the protected attribute with legitimate features.
429"
CONCLUSIONS,0.36758563074352546,"(c) Avoid using the model due to unmitigated discrimination risks.
430"
CONCLUSIONS,0.3684210526315789,"While these guidelines cannot guarantee lawful automated decisions, they provide meaningful
431"
CONCLUSIONS,0.3692564745196324,"recommendations and abstractions to help identify and mitigate unlawful discrimination risks.
432"
CONCLUSIONS,0.37009189640768586,"In conclusion, this work bridges a critical gap between the technical aspects of automated decisions
433"
CONCLUSIONS,0.37092731829573933,"and the complexities of anti-discrimination law. By translating these nuanced legal concepts into
434"
CONCLUSIONS,0.3717627401837928,"decision theory, we underscore the importance of accurately modelling true data-generating processes
435"
CONCLUSIONS,0.37259816207184626,"and the innovative concept of estimation parity. This interdisciplinary approach enhances the
436"
CONCLUSIONS,0.37343358395989973,"understanding of automated decision-making and sets a foundation for future research that aligns
437"
CONCLUSIONS,0.3742690058479532,"technological advancements with legal and ethical standards.
438"
REFERENCES,0.37510442773600666,"References
439"
REFERENCES,0.37593984962406013,"[1] Jeremias Adams-Prassl, Reuben Binns, and Aislinn Kelly-Lyth. Directly Discriminatory Algorithms. The
440"
REFERENCES,0.3767752715121136,"Modern Law Review, 86(1):144–175, 2022.
441"
REFERENCES,0.37761069340016706,"[2] Nikita Aggarwal. The Norms of Algorithmic Credit Scoring. Cambridge Law Journal, 80(1):42–73,
442"
REFERENCES,0.37844611528822053,"2021.
443"
REFERENCES,0.379281537176274,"[3] AHRC. A Quick Guide to Australian Discrimination Laws. Technical report, Australian Human Rights
444"
REFERENCES,0.38011695906432746,"Commission, 2014.
445"
REFERENCES,0.38095238095238093,"[4] Hirotogu Akaike. Information theory and an extension of the maximum likelihood principle. 1973.
446"
REFERENCES,0.3817878028404344,"[5] Larry Alexander and Kevin Cole. Discrimination by Proxy. Constitutional Commentary, 14:453–463,
447"
REFERENCES,0.38262322472848787,"1997.
448"
REFERENCES,0.38345864661654133,"[6] Jose Manuel Alvarez and Salvatore Ruggieri. Counterfactual Situation Testing: Uncovering Discrimi-
449"
REFERENCES,0.3842940685045948,"nation under Fairness given the Difference. In Proceedings of the 3rd ACM Conference on Equity and
450"
REFERENCES,0.38512949039264827,"Access in Algorithms, Mechanisms, and Optimization, Boston, MA, USA„ 2023. ACM.
451"
REFERENCES,0.38596491228070173,"[7] Kenneth Arrow. The Theory of Discrimination. In Discrimination in Labor Markets, pages 3–33.
452"
REFERENCES,0.3868003341687552,"Princeton University Press, 1971.
453"
REFERENCES,0.38763575605680867,"[8] Australian Parliament. Sex Discrimination Act 1984.
454"
REFERENCES,0.38847117794486213,"[9] Bangladesh Parliament. Anti-Discrimination Bill 2022.
455"
REFERENCES,0.3893065998329156,"[10] Solon Barocas and Andrew Selbst. Big Data’s Disparate Impact. California Law Review, 104(3):671–732,
456"
REFERENCES,0.39014202172096907,"2016.
457"
REFERENCES,0.39097744360902253,"[11] Gary Becker. The Economics of Discrimination. University of Chicago Press, 1957.
458"
REFERENCES,0.391812865497076,"[12] Ruben Becker, Gianlorenzo D’Angelo, and Sajjad Ghobadi. On the cost of demographic parity in
459"
REFERENCES,0.39264828738512947,"influence maximization, June 2023.
460"
REFERENCES,0.39348370927318294,"[13] James Berger. Statistical Decision Theory and Bayesian Analysis. New York: Springer, 1985.
461"
REFERENCES,0.3943191311612364,"[14] Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth. Fairness in Criminal
462"
REFERENCES,0.39515455304928987,"Justice Risk Assessments: The State of the Art. Sociological Methods & Research, 50(1):3–44, 2018.
463"
REFERENCES,0.39598997493734334,"[15] José M Bernardo and Adrian FM Smith. Bayesian theory. John Wiley & Sons, 1994.
464"
REFERENCES,0.3968253968253968,"[16] J. R. Biden. Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial
465"
REFERENCES,0.39766081871345027,"Intelligence. The White House, 2023. Executive Order 14110.
466"
REFERENCES,0.39849624060150374,"[17] Reuben Binns. Fairness in Machine Learning: Lessons from Political Philosophy. Proceedings of
467"
REFERENCES,0.3993316624895572,"Machine Learning Research, 81:149–159, 2018.
468"
REFERENCES,0.40016708437761067,"[18] Harold Black, Robert L. Schweitzer, and Lewis Mandell. Discrimination in Mortgage Lending. The
469"
REFERENCES,0.40100250626566414,"American Economic Review, 68(2):186–191, 1978.
470"
REFERENCES,0.4018379281537176,"[19] Margarita Boyarskaya, Solon Barocas, Hanna Wallach, and Michael Carl Tschantz. What Is a Proxy and
471"
REFERENCES,0.40267335004177107,"Why Is It a Problem? Proceedings of the Conference on Fairness, Accountability, and Transparency,
472"
REFERENCES,0.40350877192982454,"2022.
473"
REFERENCES,0.404344193817878,"[20] Canadian Parliament. Human Rights Act. R.S.C. (c.H-6), 1985.
474"
REFERENCES,0.4051796157059315,"[21] Alycia N. Carey and Xintao Wu. The causal fairness field guide: perspectives from social and formal
475"
REFERENCES,0.40601503759398494,"sciences. Frontiers in Big Data, 5:892837, 2022.
476"
REFERENCES,0.4068504594820384,"[22] Alycia N. Carey and Xintao Wu. The statistical fairness field guide: perspectives from social and formal
477"
REFERENCES,0.4076858813700919,"sciences. AI and Ethics, 3(1):1–23, 2023.
478"
REFERENCES,0.40852130325814534,"[23] Alessandro Castelnovo, Riccardo Crupi, Greta Greco, Daniele Regoli, Ilaria Giuseppina Penco, and
479"
REFERENCES,0.4093567251461988,"Andrea Claudio Cosentini. A clarification of the nuances in the fairness metrics landscape. Scientific
480"
REFERENCES,0.4101921470342523,"Reports, 12(1), 2022.
481"
REFERENCES,0.41102756892230574,"[24] Simon Caton and Christian Haas. Fairness in Machine Learning: A Survey. ACM Computing Surveys,
482"
REFERENCES,0.4118629908103592,"2023. Just Accepted.
483"
REFERENCES,0.4126984126984127,"[25] Zhisheng Chen. Ethics and discrimination in artificial intelligence-enabled recruitment practices. Human-
484"
REFERENCES,0.41353383458646614,"ities and Social Sciences Communications, 10(1), 2023.
485"
REFERENCES,0.4143692564745196,"[26] S. Chiappa. Path-specific counterfactual fairness. In Proceedings of the AAAI Conference on Artificial
486"
REFERENCES,0.4152046783625731,"Intelligence, pages 7801–7808, 2019.
487"
REFERENCES,0.41604010025062654,"[27] Alexandra Chouldechova. Fair Prediction with Disparate Impact: A Study of Bias in Recidivism
488"
REFERENCES,0.41687552213868,"Prediction Instruments. Big Data, 5(2):153–163, 2017.
489"
REFERENCES,0.4177109440267335,"[28] CIA. Legal System - The World Factbook. Technical report, Central Intelligence Agency.
490"
REFERENCES,0.41854636591478694,"[29] Nancy S. Cole. Bias in Selection. Journal of Educational Measurement, 10(4):237–255, 1973.
491"
REFERENCES,0.4193817878028404,"[30] Sam Corbett-Davies, Johann D. Gaebler, Hamed Nilforoshan, Ravi Shroff, and Sharad Goel. The Measure
492"
REFERENCES,0.4202172096908939,"and Mismeasure of Fairness. Journal of Machine Learning Research, 24(312):1–117, 2023.
493"
REFERENCES,0.42105263157894735,"[31] Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. Algorithmic Decision
494"
REFERENCES,0.4218880534670008,"Making and the Cost of Fairness. In Proceedings of the 23rd ACM SIGKDD International Conference on
495"
REFERENCES,0.4227234753550543,"Knowledge Discovery and Data Mining, pages 797–806, Halifax, Canada, 2017.
496"
REFERENCES,0.42355889724310775,"[32] M. H. DeGroot. Optimal Statistical Decisions. McGraw-Hill, 1970.
497"
REFERENCES,0.4243943191311612,"[33] DFS. Report on Apple Card Investigation. New York State Department of Financial Services, 2021.
498"
REFERENCES,0.4252297410192147,"[34] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Rich Zemel. Fairness Through
499"
REFERENCES,0.42606516290726815,"Awareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference, pages
500"
REFERENCES,0.4269005847953216,"214–226, 2012.
501"
REFERENCES,0.4277360066833751,"[35] ECJ. C-170/84, Bilka Kaufhaus GmbH v Weber von Hartz. European Court of Justice, 1986. ECR 1607.
502"
REFERENCES,0.42857142857142855,"[36] Elizabeth Emens. Intimate Discrimination: The State’s Role in the Accidents of Sex and Love. Harvard
503"
REFERENCES,0.429406850459482,"Law Review, 22(5):1307–1402, 2009.
504"
REFERENCES,0.4302422723475355,"[37] England and Wales Court of Appeal. Igen ltd v Wong. [2005] EWCA Civ 142.
505"
REFERENCES,0.43107769423558895,"[38] England and Wales Court of Appeal. Madarassy v Nomura International plc. [2007] EWCA Civ 33.
506"
REFERENCES,0.4319131161236424,"[39] England and Wales Court of Appeal. Secretary of State for Defence v Elias. (2006) IRLR 934.
507"
REFERENCES,0.4327485380116959,"[40] Equality Act. 2010 (UK).
508"
REFERENCES,0.43358395989974935,"[41] Virginia Eubanks. Automating Inequality. St. Martin’s Press, 2018.
509"
REFERENCES,0.4344193817878028,"[42] European Court of Justice. C-236/09 Association belge des Consommateurs Test-Achats ASBL v Conseil
510"
REFERENCES,0.4352548036758563,"des ministres. (2011) ECR I-00773.
511"
REFERENCES,0.43609022556390975,"[43] European Parliament. Directive 2002/73/EC of the European Parliament and of the Council of 23
512"
REFERENCES,0.4369256474519632,"September 2002 amending Council Directive 76/207/EEC on the implementation of the principle of equal
513"
REFERENCES,0.4377610693400167,"treatment for men and women as regards access to employment, vocational training and promotion, and
514"
REFERENCES,0.43859649122807015,"working conditions. 2002. OJ L 269.
515"
REFERENCES,0.4394319131161236,"[44] European Parliament. Amendments adopted by the European Parliament on 14 June 2023 on the proposal
516"
REFERENCES,0.4402673350041771,"for a regulation of the European Parliament and of the Council on laying down harmonised rules on
517"
REFERENCES,0.44110275689223055,"artificial intelligence (Artificial Intelligence Act) and amending certain Union legislative acts. 2023.
518"
REFERENCES,0.441938178780284,"(COM(2021)0206 – C9-0146/2021 – 2021/0106(COD).
519"
REFERENCES,0.4427736006683375,"[45] European Union. Charter of Fundamental Rights of the European Union. 2009. OJ 2012/C 326/02.
520"
REFERENCES,0.44360902255639095,"[46] Talia Gillis. The Input Fallacy. Minnesota Law Review, 106:1175, 2022.
521"
REFERENCES,0.4444444444444444,"[47] Government of India. Constitution of India, 1950.
522"
REFERENCES,0.4452798663324979,"[48] Przemyslaw A. Grabowicz, Nicholas Perello, and Aarshee Mishra. Marrying Fairness and Explainability
523"
REFERENCES,0.44611528822055135,"in Supervised Learning. In Proceedings of the Conference on Fairness, Accountability, and Transparency,
524"
REFERENCES,0.4469507101086048,"page 1905–1916, Seoul, Republic of Korea, 2022.
525"
REFERENCES,0.4477861319966583,"[49] Moritz Hardt, Eric Price, and Nathan Srebro. Equality of Opportunity in Supervised Learning. In
526"
REFERENCES,0.44862155388471175,"Proceedings of the 30th Conference on Neural Information Processing Systems, Barcelona, Spain, 2016.
527"
REFERENCES,0.4494569757727652,"[50] Hoda Heidari, Michele Loi, Krishna P. Gummadi, and Andreas Krause. A moral framework for under-
528"
REFERENCES,0.4502923976608187,"standing fair ml through economic models of equality of opportunity. In Proceedings of the Conference
529"
REFERENCES,0.45112781954887216,"on Fairness, Accountability, and Transparency, page 181–190, Atlanta, GA, 2019.
530"
REFERENCES,0.4519632414369256,"[51] Deborah Hellman. Measuring Algorithmic Fairness. Virginia Law Review, 106(4):811–866, 2020.
531"
REFERENCES,0.4527986633249791,"[52] Deborah Hellman. Sex, Causation, and Algorithms: How Equal Protection Prohibits Compounding Prior
532"
REFERENCES,0.45363408521303256,"Injustice. Washington University Law Review, 98:481–523, 2020.
533"
REFERENCES,0.454469507101086,"[53] Anne Hellum, Ingunn Ikdahl, Vibeke Strand, and Eva-Maria Svensson. Nordic Equality and Anti-
534"
REFERENCES,0.4553049289891395,"Discrimination Laws in the Throes of Change: Legal developments in Sweden, Finland, Norway, and
535"
REFERENCES,0.45614035087719296,"Iceland. Routledge, 2023.
536"
REFERENCES,0.4569757727652464,"[54] Corinna Hertweck, Christoph Heitz, and Michele Loi. On the Moral Justification of Statistical Parity. In
537"
REFERENCES,0.4578111946532999,"Proceedings of the Conference on Fairness, Accountability, and Transparency, pages 747–757, 2021.
538"
REFERENCES,0.45864661654135336,"[55] Daniel E Ho and Alice Xiang. Affirmative algorithms: The legal grounds for fairness as awareness.
539"
REFERENCES,0.4594820384294068,"University of Chicago Law Review Online, pages 134–154, 2020.
540"
REFERENCES,0.4603174603174603,"[56] Sally Ho and Garance Burke. An Algorithm that Screens for Child Neglect Raises Concerns. Associated
541"
REFERENCES,0.46115288220551376,"Press, 2022.
542"
REFERENCES,0.4619883040935672,"[57] Lily Hu and Issa Kohler-Hausmann. What’s sex got to do with machine learning? In Proceedings of the
543"
REFERENCES,0.4628237259816207,"Conference on Fairness, Accountability, and Transparency, page 513, Barcelona, Spain, 2020.
544"
REFERENCES,0.46365914786967416,"[58] Eyke Hüllermeier and Willem Waegeman. Aleatoric and epistemic uncertainty in machine learning: an
545"
REFERENCES,0.4644945697577276,"introduction to concepts and methods. Machine Learning, 110(3):457–506, March 2021.
546"
REFERENCES,0.4653299916457811,"[59] Anna Lena Hunkenschroer and Alexander Kriebitz. Is AI Recruiting (un)ethical? A Human Rights
547"
REFERENCES,0.46616541353383456,"Perspective on the Use of AI for Hiring. AI and Ethics, 3(1):199–213, 2022.
548"
REFERENCES,0.467000835421888,"[60] Mikella Hurley and Julius Adebayo. Credit Scoring in the Era of Big Data. Yale Journal of Law and
549"
REFERENCES,0.4678362573099415,"Technology, 18(1):148–216, 2017.
550"
REFERENCES,0.46867167919799496,"[61] Ben Hutchinson and Margaret Mitchell. 50 Years of Test (Un)fairness: Lessons for Machine Learning. In
551"
REFERENCES,0.46950710108604843,"Proceedings of the Conference on Fairness, Accountability, and Transparency, pages 49–58, Atlanta, GA,
552"
REFERENCES,0.4703425229741019,"2019.
553"
REFERENCES,0.47117794486215536,"[62] James E. Johndrow and Kristian Lum. An Algorithm For Removing Sensitive Information: Application
554"
REFERENCES,0.47201336675020883,"To Race-independent Recidivism Prediction. The Annals of Applied Statistics, 13(1):pp. 189–220, 2019.
555"
REFERENCES,0.4728487886382623,"[63] Maximilian Kasy and Rediet Abebe. Fairness, Equality, and Power in Algorithmic Decision-Making. In
556"
REFERENCES,0.47368421052631576,"Proceedings of the Conference on Fairness, Accountability, and Transparency, pages 576–586, 2021.
557"
REFERENCES,0.47451963241436923,"[64] Aislinn Kelly-Lyth. Challenging Biased Hiring Algorithms. Oxford Journal of Legal Studies, 41(4):899–
558"
REFERENCES,0.4753550543024227,"928, 2021.
559"
REFERENCES,0.47619047619047616,"[65] Tarunabh Khaitan. A Theory of Discrimination Law. Oxford University Press, 2015.
560"
REFERENCES,0.47702589807852963,"[66] Fereshte Khani and Percy Liang. Removing Spurious Features Can Hurt Accuracy and Affect Groups
561"
REFERENCES,0.4778613199665831,"Disproportionately. In Proceedings of the Conference on Fairness, Accountability, and Transparency,
562"
REFERENCES,0.47869674185463656,"page 196–205, 2021.
563"
REFERENCES,0.47953216374269003,"[67] Elif Kiesow Cortez and Nestor Maslej. Adjudication of Artificial Intelligence and Automated Decision-
564"
REFERENCES,0.4803675856307435,"Making Cases in Europe and the USA. European Journal of Risk Regulation, 14(3):457–475, 2023.
565"
REFERENCES,0.48120300751879697,"[68] Niki Kilbertus, Adria Gascon, Matt Kusner, Michael Veale, Krishna Gummadi, and Adrian Weller. Blind
566"
REFERENCES,0.48203842940685043,"Justice: Fairness with Encrypted Sensitive Attributes. In Proceedings of the 35th International Conference
567"
REFERENCES,0.4828738512949039,"on Machine Learning, pages 2630–2639, Stockholm, Sweden, 2018.
568"
REFERENCES,0.48370927318295737,"[69] Niki Kilbertus, Mateo Rojas-Carulla, Giambattista Parascandolo, Moritz Hardt, Dominik Janzing, and
569"
REFERENCES,0.48454469507101083,"Bernhard Schölkopf. Avoiding discrimination through causal reasoning. In Advances in Neural Informa-
570"
REFERENCES,0.4853801169590643,"tion Processing Systems, volume 30, page 656–666, 2017.
571"
REFERENCES,0.48621553884711777,"[70] Pauline Kim. Auditing Algorithms for Discrimination. University of Pennsylvania Law Review Online,
572"
REFERENCES,0.48705096073517123,"166(1), 2017.
573"
REFERENCES,0.4878863826232247,"[71] Barbara Kiviat. The Moral Affordances of Construing People as Cases: How Algorithms and the Data
574"
REFERENCES,0.48872180451127817,"They Depend on Obscure Narrative and Noncomparative Justice. Sociological Theory, 41(3):175–200,
575"
REFERENCES,0.48955722639933164,"2023.
576"
REFERENCES,0.4903926482873851,"[72] Jon Kleinberg, Jens Ludwig, Sendhil Mullainathan, and Cass Sunstein. Discrimination in the Age of
577"
REFERENCES,0.49122807017543857,"Algorithms. Journal of Legal Analysis, 10:113–174, 2019.
578"
REFERENCES,0.49206349206349204,"[73] Issa Kohler-Hausmann and Robin Dembroff. Supreme Confusion About Causality at the Supreme Court.
579"
REFERENCES,0.4928989139515455,"City University of New York Law Review, 25(1):57–92, 2022.
580"
REFERENCES,0.49373433583959897,"[74] Joshua Kroll, Joanna Huey, Solon Barocas, Edward Felten, Joel Reidenberg, David Robinson, and Harlan
581"
REFERENCES,0.49456975772765244,"Yu. Accountable Algorithms. University of Pennsylvania Law Review, 165(3):633, 2017.
582"
REFERENCES,0.4954051796157059,"[75] Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual Fairness. Advances in
583"
REFERENCES,0.49624060150375937,"Neural Information Processing Systems, 30:4069–4079, 2017.
584"
REFERENCES,0.49707602339181284,"[76] Katja Langenbucher. Consumer Credit in The Age of AI – Beyond Anti-Discrimination Law. Law
585"
REFERENCES,0.4979114452798663,"Working Paper No. 663/2022, 2023.
586"
REFERENCES,0.49874686716791977,"[77] Finn Lattimore, Simon O’Callaghan, Zoe Paleologos, Alistair Reid, Edward Santow, Holli Sargeant,
587"
REFERENCES,0.49958228905597324,"and Andrew Thomsen. Using Artificial Intelligence to Make Decisions: Addressing the Problem of
588"
REFERENCES,0.5004177109440268,"Algorithmic Bias. Technical Paper, Australian Human Rights Commission, 2020.
589"
REFERENCES,0.5012531328320802,"[78] Tai Le Quy, Arjun Roy, Vasileios Iosifidis, Wenbin Zhang, and Eirini Ntoutsi. A survey on datasets for
590"
REFERENCES,0.5020885547201337,"fairness-aware machine learning. WIREs Data Mining and Knowledge Discovery, 12(3):e1452, 2022.
591"
REFERENCES,0.5029239766081871,"[79] Zachary Lipton, Julian McAuley, and Alexandra Chouldechova. Does Mitigating ML’s Impact Disparity
592"
REFERENCES,0.5037593984962406,"Require Treatment Disparity? Advances in Neural Information Processing Systems, 31, 2018.
593"
REFERENCES,0.504594820384294,"[80] Emmanuel Martinez and Lauren Kirchner. The Secret Bias Hidden in Mortgage-Approval Algorithms.
594"
REFERENCES,0.5054302422723476,"The Markup, https://perma.cc/U6W9-MECE, 2021.
595"
REFERENCES,0.506265664160401,"[81] Sandra G. Mayson. Bias In, Bias Out. Yale Law Journal, 128(8):2122–2473, 2019.
596"
REFERENCES,0.5071010860484545,"[82] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A Survey on
597"
REFERENCES,0.5079365079365079,"Bias and Fairness in Machine Learning. ACM Computing Surveys, 54(6), 2021.
598"
REFERENCES,0.5087719298245614,"[83] Ministry of Justice, Finland. Government Porposal for the Equality Act and Related Laws HE 19/2014 vp
599"
REFERENCES,0.5096073517126148,"(Hallituksen esitys eduskunnalle yhdenvertaisuuslaiksi ja eräiksi siihen liittyviksi laeiksi).
600"
REFERENCES,0.5104427736006684,"[84] Ministry of Justice, Finland. Non-Discrimination Act (Yhdenvertaisuuslaki) (1325/2014).
601"
REFERENCES,0.5112781954887218,"[85] Shira Mitchell, Eric Potash, Solon Barocas, Alexander D’Amour, and Kristian Lum. Algorithmic Fairness:
602"
REFERENCES,0.5121136173767753,"Choices, Assumptions, and Definitions. Annual Review of Statistics and Its Application, 8(1):141–163,
603"
REFERENCES,0.5129490392648287,"2021.
604"
REFERENCES,0.5137844611528822,"[86] Sophia Moreau. What Is Discrimination? Philosophy & Public Affairs, 38(2):143–179, 2010.
605"
REFERENCES,0.5146198830409356,"[87] Deirdre Mulligan, Joshua Kroll, Nitin Kohli, and Richmond Wong. This Thing Called Fairness: Disci-
606"
REFERENCES,0.5154553049289892,"plinary Confusion Realizing a Value in Technology. In Proceedings of the ACM on Human-Computer
607"
REFERENCES,0.5162907268170426,"Interaction, volume 3, pages 1–36, 2019.
608"
REFERENCES,0.5171261487050961,"[88] Mpoki Mwakagali. International Human Rights Law and Discrimination Protections. Brill, 2018.
609"
REFERENCES,0.5179615705931495,"[89] Jakob Mökander. Auditing of AI: Legal, Ethical and Technical Approaches. Digital Society, 2(3):49,
610"
REFERENCES,0.518796992481203,"2023.
611"
REFERENCES,0.5196324143692564,"[90] Arvind Narayanan. Tutorial: 21 Fairness Definition and their Politics. Proceedings of the Conference on
612"
REFERENCES,0.52046783625731,"Fairness, Accountability, and Transparency, 2018.
613"
REFERENCES,0.5213032581453634,"[91] New Zealand Parliament. Human Rights Act, 1993.
614"
REFERENCES,0.5221386800334169,"[92] Hamed Nilforoshan, Johann D Gaebler, Ravi Shroff, and Sharad Goel. Causal conceptions of fairness
615"
REFERENCES,0.5229741019214703,"and their consequences. In International Conference on Machine Learning, pages 16848–16887. PMLR,
616"
REFERENCES,0.5238095238095238,"2022.
617"
REFERENCES,0.5246449456975772,"[93] Safiya Umoja Noble. Algorithms of Oppression. New York University Press, 2018.
618"
REFERENCES,0.5254803675856308,"[94] Finland National Non-Discrimination and Equality Tribunal. Decision 216/2017. 2018.
619"
REFERENCES,0.5263157894736842,"[95] Tony O’Hagan. Dicing with the Unknown. Significance, 1(3):132–133, 2004.
620"
REFERENCES,0.5271512113617377,"[96] OHCHR. Banning Discrimination on Grounds of Socioeconomic Disadvantage: An Essential Tool in the
621"
REFERENCES,0.5279866332497911,"Fight Against Poverty. Thematic Report A/77/157, Special Rapporteur on Extreme Poverty and Human
622"
REFERENCES,0.5288220551378446,"Rights, United Nations Office of the High Commissioner for Human Rights, 2022.
623"
REFERENCES,0.529657477025898,"[97] Cathy O’Neil. Weapons of Math Destruction: How Big Data Increases Inequality and Threatens
624"
REFERENCES,0.5304928989139516,"Democracy. Penguin Books, 2016.
625"
REFERENCES,0.531328320802005,"[98] Cathy O’Neil, Holli Sargeant, and Jacob Appel. Explainable Fairness in Regulatory Algorithmic Auditing,
626"
REFERENCES,0.5321637426900585,"2023.
627"
REFERENCES,0.5329991645781119,"[99] Giovanni Parmigiani and Lurdes Inoue. Decision Theory. Wiley, 2010.
628"
REFERENCES,0.5338345864661654,"[100] Frank Pasquale. The Black Box Society. Harvard University Press, 2019.
629"
REFERENCES,0.5346700083542189,"[101] Judea Pearl. An Introduction to Causal Inference. The International Journal of Biostatistics, 6(2), 2010.
630"
REFERENCES,0.5355054302422724,"[102] Edmund Phelps. The Statistical Theory of Racism and Sexism. The American Economic Review,
631"
REFERENCES,0.5363408521303258,"62(4):659–661, 1972.
632"
REFERENCES,0.5371762740183793,"[103] Inioluwa Deborah Raji, Andrew Smart, Rebecca N. White, Margaret Mitchell, Timnit Gebru, Ben
633"
REFERENCES,0.5380116959064327,"Hutchinson, Jamila Smith-Loud, Daniel Theron, and Parker Barnes. Closing the AI Accountability Gap:
634"
REFERENCES,0.5388471177944862,"Defining an End-to-End Framework for Internal Algorithmic Auditing. In Proceedings of the Conference
635"
REFERENCES,0.5396825396825397,"on Fairness, Accountability, and Transparency, page 33–44, Barcelona, Spain, 2020.
636"
REFERENCES,0.5405179615705932,"[104] Lisa Rice and Deidre Swesnik. Discriminatory Effects of Credit Scoring on Communities of Color.
637"
REFERENCES,0.5413533834586466,"Suffolk University Law Review, 46(935):935–966, 2013.
638"
REFERENCES,0.5421888053467001,"[105] Andrea Romei and Salvatore Ruggieri. A multidisciplinary survey on discrimination analysis. The
639"
REFERENCES,0.5430242272347535,"Knowledge Engineering Review, 29(5):582–638, 2014.
640"
REFERENCES,0.543859649122807,"[106] Chris Russell, Matt J Kusner, Joshua Loftus, and Ricardo Silva. When Worlds Collide: Integrating
641"
REFERENCES,0.5446950710108605,"Different Counterfactual Assumptions in Fairness. In Advances in Neural Information Processing Systems,
642"
REFERENCES,0.545530492898914,"volume 30, page 6417–6426, 2017.
643"
REFERENCES,0.5463659147869674,"[107] Holli Sargeant. Algorithmic decision-making in financial services: economic and normative outcomes in
644"
REFERENCES,0.5472013366750209,"consumer credit. AI and Ethics, 3(4):1295–1311, 2023.
645"
REFERENCES,0.5480367585630743,"[108] Leonard Savage. The Foundations of Statistics. Operations Research, 4(2):254–258, 1956.
646"
REFERENCES,0.5488721804511278,"[109] Patrick Shin. Is there a unitary concept of discrimination?
In Deborah Hellman and Sophia Rei-
647"
REFERENCES,0.5497076023391813,"betanz Moreau, editors, Philosophical foundations of discrimination law, page 172. Oxford University
648"
REFERENCES,0.5505430242272348,"Press, 2013.
649"
REFERENCES,0.5513784461152882,"[110] South African Parliament. Promotion of Equality and Prevention of Unfair Discrimination Act, 2000.
650"
REFERENCES,0.5522138680033417,"[111] Till Speicher, Hoda Heidari, Nina Grgic-Hlaca, Krishna P. Gummadi, Adish Singla, Adrian Weller, and
651"
REFERENCES,0.5530492898913951,"Muhammad Bilal Zafar. A unified approach to quantifying algorithmic unfairness: Measuring individual
652"
REFERENCES,0.5538847117794486,"& group unfairness via inequality indices. In Proceedings of the 24th International Conference on
653"
REFERENCES,0.5547201336675021,"Knowledge Discovery & Data Mining, page 2239–2248, London, United Kingdom, 2018.
654"
REFERENCES,0.5555555555555556,"[112] Supreme Court of Wisconsin. State v. Loomis. 881 N.W.2d 749, 2016.
655"
REFERENCES,0.556390977443609,"[113] Adrien Sénécat. The use of opaque algorithms facilitates abuses within public services. Le Monde, 2023.
656"
REFERENCES,0.5572263993316625,"[114] Anique Tahir, Lu Cheng, and Huan Liu. Fairness through aleatoric uncertainty. In Proceedings of the 32nd
657"
REFERENCES,0.5580618212197159,"International Conference on Information and Knowledge Management, page 2372–2381, Birmingham,
658"
REFERENCES,0.5588972431077694,"United Kingdom, 2023.
659"
REFERENCES,0.5597326649958229,"[115] Michael Carl Tschantz. What is proxy discrimination? In Proceedings of the Conference on Fairness,
660"
REFERENCES,0.5605680868838764,"Accountability, and Transparency, pages 1993–2003, Seoul, Republic of Korea, June 2022.
661"
REFERENCES,0.5614035087719298,"[116] UK Parliament. Disability Discrimination Act 1995.
662"
REFERENCES,0.5622389306599833,"[117] UK Parliament. Explanatory Memorandum to the Equality Act 2010 (Age Exceptions Order). 2012.
663"
REFERENCES,0.5630743525480367,"[118] UK Parliament. Race Relations Act 1965.
664"
REFERENCES,0.5639097744360902,"[119] UK Parliament. Sex Discrimination Act 1975.
665"
REFERENCES,0.5647451963241437,"[120] United Kingdom Employment Appeals Tribunal.
Dziedziak v Future Electronics Ltd.
[2012]
666"
REFERENCES,0.5655806182121972,"UKEAT/0270/11.
667"
REFERENCES,0.5664160401002506,"[121] United Kingdom Employment Appeals Tribunal. O’Neil v Governors of St Thomas More Roman Catholic
668"
REFERENCES,0.5672514619883041,"School. (1996) IRLR 372.
669"
REFERENCES,0.5680868838763575,"[122] United Kingdom House of Lords. Equal Opportunities Commission, R (on the application of) v Birming-
670"
REFERENCES,0.568922305764411,"ham City Council. (1989) 1 AC 1155.
671"
REFERENCES,0.5697577276524645,"[123] United Kingdom House of Lords. James v Eastleigh Borough Council. (1990) 2 AC 751.
672"
REFERENCES,0.570593149540518,"[124] United Kingdom House of Lords. Secretary of State For Employment, Ex Parte Seymour Smith and
673"
REFERENCES,0.5714285714285714,"Another, R v. (2000) 1 All ER 857.
674"
REFERENCES,0.5722639933166249,"[125] United Kingdom House of Lords. Webb v EMO Air Cargo (UK) Ltd (No. 2). (1995) IRLR 645.
675"
REFERENCES,0.5730994152046783,"[126] United Kingdom Supreme Court. Akerman-Livingstone v Aster Communities Ltd. (2015) 1 AC 1399.
676"
REFERENCES,0.5739348370927319,"[127] United Kingdom Supreme Court. Essop v Home Office (UK Border Agency). (2017) IRLR 558.
677"
REFERENCES,0.5747702589807853,"[128] United Kingdom Supreme Court. Homer v Chief Constable of West Yorkshire Police. (2012) IRLR 601.
678"
REFERENCES,0.5756056808688388,"[129] United Kingdom Supreme Court. Lee v Ashers. (2018) AC 413.
679"
REFERENCES,0.5764411027568922,"[130] United Kingdom Supreme Court. R (Coll) v Secretary of State for Justice. (2017) 1 WLR 2093.
680"
REFERENCES,0.5772765246449457,"[131] United Kingdom Supreme Court. R (on the application of E) v JFS Governing Body. (2009) 1 WLR
681"
REFERENCES,0.5781119465329991,"2353.
682"
REFERENCES,0.5789473684210527,"[132] United Nations. Universal Declaration of Human Rights. 1948.
683"
REFERENCES,0.5797827903091061,"[133] Aki Vehtari, Andrew Gelman, and Jonah Gabry. Practical bayesian model evaluation using leave-one-out
684"
REFERENCES,0.5806182121971596,"cross-validation and waic. Statistics and computing, 27:1413–1432, 2017.
685"
REFERENCES,0.581453634085213,"[134] Aki Vehtari and Jouko Lampinen. Bayesian model assessment and comparison using cross-validation
686"
REFERENCES,0.5822890559732665,"predictive densities. Neural computation, 14(10):2439–2468, 2002.
687"
REFERENCES,0.5831244778613199,"[135] Aki Vehtari and Janne Ojanen. A survey of Bayesian predictive methods for model assessment, selection
688"
REFERENCES,0.5839598997493735,"and comparison. Statistics Surveys, 6:142 – 228, 2012.
689"
REFERENCES,0.5847953216374269,"[136] Sahil Verma and Julia Rubin. Fairness Definitions Explained. In Proceedings of the International
690"
REFERENCES,0.5856307435254804,"Workshop on Software Fairness, pages 1–7, Gothenburg, Sweden, 2018.
691"
REFERENCES,0.5864661654135338,"[137] Marc De Vos. The European Court of Justice and the March Towards Substantive Equality in European
692"
REFERENCES,0.5873015873015873,"Union Anti-Discrimination Law. International Journal of Discrimination and the Law, 20(1):62–87,
693"
REFERENCES,0.5881370091896407,"2020.
694"
REFERENCES,0.5889724310776943,"[138] Sandra Wachter, Brent Mittelstadt, and Chris Russell. Why Fairness Cannot Be Automated: Bridging the
695"
REFERENCES,0.5898078529657477,"Gap Between EU Non-discrimination Law and AI. Computer Law & Security Review, 41:105567, 2021.
696"
REFERENCES,0.5906432748538012,"[139] Sandra Wachter, Brent Daniel Mittelstadt, and Chris Russell. Bias Preservation in Machine Learning:
697"
REFERENCES,0.5914786967418546,"The Legality of Fairness Metrics Under EU Non-Discrimination Law. West Virginia Law Review,
698"
REFERENCES,0.5923141186299081,"123(3):735–790, 2021.
699"
REFERENCES,0.5931495405179615,"[140] Hilde Weerts, Raphaële Xenidis, Fabien Tarissan, Henrik Palmer Olsen, and Mykola Pechenizkiy.
700"
REFERENCES,0.5939849624060151,"Algorithmic unfairness through the lens of eu non-discrimination law: Or why the law is not a decision
701"
REFERENCES,0.5948203842940685,"tree. In Proceedings of the Conference on Fairness, Accountability, and Transparency, page 805–816,
702"
REFERENCES,0.595655806182122,"Chicago, IL, USA, 2023. ACM.
703"
REFERENCES,0.5964912280701754,"[141] Halbert White. Maximum likelihood estimation of misspecified sodels. Econometrica, 50(1):1–25, 1982.
704"
REFERENCES,0.5973266499582289,"[142] Yongkai Wu, Lu Zhang, Xintao Wu, and Hanghang Tong.
Pc-fairness: A unified framework for
705"
REFERENCES,0.5981620718462823,"measuring causality-based fairness. In Advances in Neural Information Processing Systems, volume 32,
706"
REFERENCES,0.5989974937343359,"page 3404–3414, 2019.
707"
REFERENCES,0.5998329156223893,"[143] Raphaële Xenidis. Tuning EU equality law to algorithmic discrimination: Three pathways to resilience.
708"
REFERENCES,0.6006683375104428,"Maastricht Journal of European and Comparative Law, 27(6):736–758, 2020.
709"
REFERENCES,0.6015037593984962,"[144] Alice Xiang. Reconciling Legal and Technical Approaches to Algorithmic Bias. Tennessee Law Review,
710"
REFERENCES,0.6023391812865497,"88(3):649, 2021.
711"
REFERENCES,0.6031746031746031,"[145] Renzhe Xu, Peng Cui, Kun Kuang, Bo Li, Linjun Zhou, Zheyan Shen, and Wei Cui. Algorithmic decision
712"
REFERENCES,0.6040100250626567,"making with conditional fairness. Proceedings of the 26th International Conference on Knowledge
713"
REFERENCES,0.6048454469507101,"Discovery & Data Mining, 2020.
714"
REFERENCES,0.6056808688387636,"[146] Crystal Yang and Will Dobbie. Equal Protection Under Algorithms: A New Statistical and Legal
715"
REFERENCES,0.606516290726817,"Framework. Michigan Law Review, 119:291, 2020.
716"
REFERENCES,0.6073517126148705,"[147] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna Gummadi. Fairness
717"
REFERENCES,0.6081871345029239,"beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment. In
718"
REFERENCES,0.6090225563909775,"Proceedings of the 26th International Conference on World Wide Web, pages 1171–1180, Perth, Australia,
719"
REFERENCES,0.6098579782790309,"2017.
720"
REFERENCES,0.6106934001670844,"[148] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez-Rodriguez, and Krishna P. Gummadi. Fairness
721"
REFERENCES,0.6115288220551378,"constraints: A flexible approach for fair classification. Journal of Machine Learning Research, 20(75):1–
722"
REFERENCES,0.6123642439431913,"42, 2019.
723"
REFERENCES,0.6131996658312447,"[149] Junzhe Zhang and Elias Bareinboim. Fairness in decision-making — the causal explanation formula.
724"
REFERENCES,0.6140350877192983,"Proceedings of the AAAI Conference on Artificial Intelligence, 32(1), April 2018.
725"
REFERENCES,0.6148705096073517,"[150] Lu Zhang, Yongkai Wu, and Xintao Wu. A Causal Framework for Discovering and Removing Direct and
726"
REFERENCES,0.6157059314954052,"Indirect Discrimination. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial
727"
REFERENCES,0.6165413533834586,"Intelligence, pages 3929–3935, Melbourne, Australia, 2017.
728"
REFERENCES,0.6173767752715121,"[151] Miri Zilka, Holli Sargeant, and Adrian Weller. Transparency, Governance and Regulation of Algorithmic
729"
REFERENCES,0.6182121971595655,"Tools Deployed in the Criminal Justice System: A UK Case Study. In Proceedings of the Conference on
730"
REFERENCES,0.6190476190476191,"AI, Ethics, and Society, page 880–889, Oxford, United Kingdom, 2022.
731"
REFERENCES,0.6198830409356725,"A
Case Study
732"
REFERENCES,0.620718462823726,"Overview of Finnish Anti-Discrimination Law
733"
REFERENCES,0.6215538847117794,"Finnish anti-discrimination law bears many similarities to UK and EU laws. We briefly set out the
734"
REFERENCES,0.6223893065998329,"relevant provisions that show the similarities to the Equality Act set out in Section 1.3.
735"
REFERENCES,0.6232247284878863,"Section 8(1) of the Non-Discrimination [84] defines the protected characteristics as:1
736"
REFERENCES,0.6240601503759399,"No one may be discriminated against on the basis of age, origin, nationality, language, religion,
737"
REFERENCES,0.6248955722639933,"belief, opinion, political activity, trade union activity, family relationships, state of health,
738"
REFERENCES,0.6257309941520468,"disability, sexual orientation or other personal characteristics. Discrimination is prohibited,
739"
REFERENCES,0.6265664160401002,"regardless of whether it is based on a fact or assumption concerning the person him/herself or
740"
REFERENCES,0.6274018379281537,"another.
741"
REFERENCES,0.6282372598162071,"Section 3(1) of the Non-Discrimination Act provides that: “Provisions on prohibition of discrimination
742"
REFERENCES,0.6290726817042607,"based on gender and the promotion of gender equality are laid down in the Act on Equality between
743"
REFERENCES,0.6299081035923141,"Women and Men (609/1986).” The Non-Discrimination Act can be applied in cases of multiple
744"
REFERENCES,0.6307435254803676,"discrimination, even if gender is one of the grounds of discrimination [83, 84, s 3(1)].
745"
REFERENCES,0.631578947368421,"It is worth noting that this definition is broader than in the UK Equality Act. Some protected
746"
REFERENCES,0.6324143692564745,"characteristics are outlined more explicitly; for example, a person discriminated against on the basis
747"
REFERENCES,0.633249791144528,"of language may be able to bring a claim based on racial discrimination [120]. Unlike many Nordic
748"
REFERENCES,0.6340852130325815,"countries, the Equality Act does not explicitly protect political activity, trade union activity, and does
749"
REFERENCES,0.6349206349206349,"not include “or other personal characteristics” [53].
750"
REFERENCES,0.6357560568086884,"Direct discrimination is defined in Section 10:2
751"
REFERENCES,0.6365914786967418,"Discrimination is direct if a person, on the grounds of personal characteristics, is treated less
752"
REFERENCES,0.6374269005847953,"favourably than another person was treated, is treated or would be treated in a comparable
753"
REFERENCES,0.6382623224728488,"situation.
754"
REFERENCES,0.6390977443609023,"Indirect discrimination is defined in Section 13:3
755"
REFERENCES,0.6399331662489557,"Discrimination is indirect if an apparently neutral rule, criterion or practice puts a person at a
756"
REFERENCES,0.6407685881370092,"disadvantage compared with others as on the grounds of personal characteristics, unless the rule,
757"
REFERENCES,0.6416040100250626,"criterion or practice has a legitimate aim and the means for achieving the aim are appropriate
758"
REFERENCES,0.6424394319131161,"and necessary.
759"
REFERENCES,0.6432748538011696,"Section 11(1) defines justifications for different treatment as:4
760"
REFERENCES,0.6441102756892231,"Different treatment does not constitute discrimination if the treatment is based on legislation and
761"
REFERENCES,0.6449456975772765,"it otherwise has an acceptable objective and the measures to attain the objective are proportionate.
762"
REFERENCES,0.64578111946533,"Overview of Finnish National Non-Discrimination and Equality Tribunal Decision 216/2017
763"
REFERENCES,0.6466165413533834,"The first case regarding automated decision-making and discrimination was in Finland. The person,
764"
REFERENCES,0.647451963241437,"referred to as A, was denied credit for online purchases based on a credit rating system employed
765"
REFERENCES,0.6482873851294904,"by a bank. Person A reported the case to the Non-Discrimination Ombudsman (Yhdenvertaisuus-
766"
REFERENCES,0.6491228070175439,"valtuutettu), who brought the case before the National Non-Discrimination and Equality Tribunal
767"
REFERENCES,0.6499582289055973,"(Yhdenvertaisuus- ja tasa-arvolautakunta). The Tribunal found that the bank’s statistical scoring
768"
REFERENCES,0.6507936507936508,"model resulted in direct discrimination based on multiple protected characteristics and was not
769"
REFERENCES,0.6516290726817042,"1Official translation from Finnish, although only legally binding in Swedish (not included) and Finnish:
“Syrjinnän kielto Ketään ei saa syrjiä iän, alkuperän, kansalaisuuden, kielen, uskonnon, vakaumuksen, mielipiteen,
poliittisen toiminnan, ammattiyhdistystoiminnan, perhesuhteiden, terveydentilan, vammaisuuden, seksuaalisen
suuntautumisen tai muun henkilöön liittyvän syyn perusteella. Syrjintä on kielletty riippumatta siitä, perustuuko
se henkilöä itseään vai jotakuta toista koskevaan tosiseikkaan tai oletukseen”
2“Syrjintä on välitöntä, jos jotakuta kohdellaan henkilöön liittyvän syyn perusteella epäsuotuisammin kuin
jotakuta muuta on kohdeltu, kohdellaan tai kohdeltaisiin vertailukelpoisessa tilanteessa.”
3“Syrjintä on välillistä, jos näennäisesti yhdenvertainen sääntö, peruste tai käytäntö saattaa jonkun muita
epäedullisempaan asemaan henkilöön liittyvän syyn perusteella, paitsi jos säännöllä, perusteella tai käytännöllä
on hyväksyttävä tavoite ja tavoitteen saavuttamiseksi käytetyt keinot ovat asianmukaisia ja tarpeellisia.”
4“Erilainen kohtelu ei ole syrjintää, jos kohtelu perustuu lakiin ja sillä muutoin on hyväksyttävä tavoite ja
keinot tavoitteen saavuttamiseksi ovat oikeasuhtaisia.”"
REFERENCES,0.6524644945697577,"justified by an acceptable objective achieved by proportionate measures. Consequently, the Tri-
770"
REFERENCES,0.6532999164578112,"bunal prohibited the bank from continuing this practice and imposed a conditional fine to enforce
771"
REFERENCES,0.6541353383458647,"compliance.
772"
REFERENCES,0.6549707602339181,"The decision-making system in question is for online store financing, which is a purchase-bound, fast
773"
REFERENCES,0.6558061821219716,"and automated credit type very different from regular consumer credit. The credit applied for by the
774"
REFERENCES,0.656641604010025,"consumer in each situation is also always bound to the purchase and its value, which means that it is
775"
REFERENCES,0.6574770258980785,"more difficult, or even impossible, to undertake detailed requests for information and background
776"
REFERENCES,0.658312447786132,"checks. The individual investigation of the creditworthiness of customers using personal information
777"
REFERENCES,0.6591478696741855,"and documents, such as salary and tax certificates, may not be suitable for this type of credit.
778"
REFERENCES,0.6599832915622389,"Decision-making Model and Data
779"
REFERENCES,0.6608187134502924,"The company made credit decisions based on data from the internal records of the credit company,
780"
REFERENCES,0.6616541353383458,"information from the credit file, and the score from the company’s internal scoring system.
781"
REFERENCES,0.6624895572263994,"The bank’s scoring system assessed creditworthiness. The scoring system used population statistics
782"
REFERENCES,0.6633249791144528,"and personal attributes to calculate the percentage of people in certain groups with bad credit history
783"
REFERENCES,0.6641604010025063,"and awarded points proportionate to how common bad credit records were in the group in question.
784"
REFERENCES,0.6649958228905597,"The variables used included race, first language, age, and place of residence. The company did not
785"
REFERENCES,0.6658312447786132,"require or investigate the applicant’s income or financial situation.
786"
REFERENCES,0.6666666666666666,"True Data Generating Process and Estimation Error
787"
REFERENCES,0.6675020885547202,"The bank’s scoring model was based on statistical correlations calculated population and groups,
788"
REFERENCES,0.6683375104427736,"including gender, language, age and place of residence, meaning the model is more or less ˆp(y|xp).
789"
REFERENCES,0.6691729323308271,"This model cannot be said to have attempted to model the true underlying data-generating process
790"
REFERENCES,0.6700083542188805,"and instead relied on data that was available regarding protected attributes. It is reasonable to expect
791"
REFERENCES,0.670843776106934,"that the bank was aware of other legitimate factors that could explain the credit score. Therefore, the
792"
REFERENCES,0.6716791979949874,"model introduces epistemic uncertainty stemming from the lack of information that could have been
793"
REFERENCES,0.672514619883041,"used to make better predictions, i.e. reasonable legitimate features xl.
794"
REFERENCES,0.6733500417710944,"By solely using the data available, rather than identifying what data would be best to reduce estimation
795"
REFERENCES,0.6741854636591479,"error, the modellers built an automated decision-making system that unlawfully discriminated. We
796"
REFERENCES,0.6750208855472013,"now evaluate how the Tribunal came to those conclusions about the legitimacy of y and x for such a
797"
REFERENCES,0.6758563074352548,"model.
798"
REFERENCES,0.6766917293233082,"Legitimate y
799"
REFERENCES,0.6775271512113618,"The bank argued that the “different treatment does not constitute discrimination if the treatment
800"
REFERENCES,0.6783625730994152,"is based on legislation and has an otherwise acceptable objective and the measures to attain the
801"
REFERENCES,0.6791979949874687,"objective are proportionate.” The Tribunal agreed that “the provision of credit to customers is a
802"
REFERENCES,0.6800334168755221,"business, the purpose of which is to gain profit” and that “the investigation of creditworthiness is as
803"
REFERENCES,0.6808688387635756,"such based on law and that it has the acceptable and justified objective as defined in section 11 of the
804"
REFERENCES,0.681704260651629,"Non-Discrimination Act”. Therefore, creditworthiness assessment is a legitimate y.
805"
REFERENCES,0.6825396825396826,"However, the Tribunal clarified that “the individual assessment required by the legislation means
806"
REFERENCES,0.683375104427736,"expressly the assessment of an individual’s credit behaviour, credit history, income level and assets,
807"
REFERENCES,0.6842105263157895,"and not the extension of the impact of models formed on the basis of probability assessments created
808"
REFERENCES,0.6850459482038429,"with statistical methods using the behaviour and characteristics of others, to the individual applying
809"
REFERENCES,0.6858813700918964,"for the credit in the credit decision in such a way that assessment is solely based on such models.”
810"
REFERENCES,0.6867167919799498,"Therefore, to be appropriate and necessary to achieve that aim, the model must consider legitimate
811"
REFERENCES,0.6875522138680034,"features xl.
812"
REFERENCES,0.6883876357560568,"Protected, Legitimate, and Non-Legitimate Variables x
813"
REFERENCES,0.6892230576441103,"Four protected attributes were used as variables in this model xp: age, language, other personal
814"
REFERENCES,0.6900584795321637,"characteristic (place of residence), and gender.
815"
REFERENCES,0.6908939014202172,"The Tribunal acknowledged that age may be a legitimate variable if it had been used in the assessment
816"
REFERENCES,0.6917293233082706,"of creditworthiness mainly when applied to young persons. However, it was not justified in this
817"
REFERENCES,0.6925647451963242,"assessment, given the age of the credit applicant.
818"
REFERENCES,0.6934001670843776,"The Tribunal agreed with the position under European law that gender is prohibited from being used
819"
REFERENCES,0.6942355889724311,"as an actuarial factor in financial services [42].
820"
REFERENCES,0.6950710108604845,"Therefore, these features did not contribute to the accuracy of the model’s prediction in a way
821"
REFERENCES,0.695906432748538,"that could be argued as part of the true DGP. Therefore, in this case, these xp variables are also
822"
REFERENCES,0.6967418546365914,"non-legitimate variables xn.
823"
REFERENCES,0.697577276524645,"As explained by the Tribunal, to achieve the legitimate y of undertaking an individual assessment
824"
REFERENCES,0.6984126984126984,"of creditworthiness and ability to repay, the model should have considered, for example, income,
825"
REFERENCES,0.6992481203007519,"expenditure, debt, assets, security and guarantee liabilities, employment and type of employment
826"
REFERENCES,0.7000835421888053,"contract (i.e., permanent or temporary). These features would have been legitimate variables xl by
827"
REFERENCES,0.7009189640768588,"improving the predictive performance of the model to achieve more accurate decisions.
828"
REFERENCES,0.7017543859649122,"Conditional Estimation Parity
829"
REFERENCES,0.7025898078529658,"Using the legitimate variables identified above, we can now consider conditional estimation parity,
830"
REFERENCES,0.7034252297410192,"the difference in estimation error between groups with a protected attribute, given legitimate features.
831"
REFERENCES,0.7042606516290727,"Reducing the error in Eq. 4 is expected to diminish the risk of conditional estimation disparity.
832"
REFERENCES,0.7050960735171261,"However, assessing conditional estimation parity is complex due to inherent challenges in evaluating
833"
REFERENCES,0.7059314954051796,"estimation error.
834"
REFERENCES,0.706766917293233,"Judges engage this type of reasoning through statistical or theoretical means. In this case, the
835"
REFERENCES,0.7076023391812866,"Ombudsman brought evidence of the effects of the protected characteristics xp on the true prediction.
836"
REFERENCES,0.70843776106934,"Person A was negatively affected by his age. He was in the age group of 31-40 years old, but if he had
837"
REFERENCES,0.7092731829573935,"been at least 51 years old, he would have received a higher score sufficient for the credit application.
838"
REFERENCES,0.7101086048454469,"If person A spoke Swedish as his first language, he would have received a sufficient score for granting
839"
REFERENCES,0.7109440267335004,"the loan. Finnish-speaking residents received a lower score compared to Swedish-speaking residents.
840"
REFERENCES,0.7117794486215538,"Further, ethnic minorities with an official first language other than Finnish or Swedish were put in an
841"
REFERENCES,0.7126148705096074,"unfavourable position.
842"
REFERENCES,0.7134502923976608,"A would have earned more points based on his residential area if he had lived in a population
843"
REFERENCES,0.7142857142857143,"centre. The bank’s statistical method, which is based on a grid of residential areas, gave A the lowest
844"
REFERENCES,0.7151211361737677,"score because he lives in a sparsely populated area that has not yielded any statistically significant
845"
REFERENCES,0.7159565580618212,"information.
846"
REFERENCES,0.7167919799498746,"Gender impacted the model, where women received a higher score than men. The Tribunal agreed
847"
REFERENCES,0.7176274018379282,"that if the person A had been a woman, he would have been granted the credit.
848"
REFERENCES,0.7184628237259816,"Conclusions
849"
REFERENCES,0.7192982456140351,"This case study demonstrates the intersection between judicial reasoning and our formalisation. To
850"
REFERENCES,0.7201336675020885,"avoid liability for unlawful multiple direct discrimination in this algorithmic decision-making process,
851"
REFERENCES,0.720969089390142,"the company should have:
852"
REFERENCES,0.7218045112781954,"1. Assessed data legitimacy. While the Tribunal agreed with the target variable (y) as a legitimate
853"
REFERENCES,0.722639933166249,"aim, they did not believe the features (x) were legitimate for the specific context (Section 2.7).
854"
REFERENCES,0.7234753550543024,"2. Built an accurate model. The bank did not strive to approximate the true DGP p(y|x), and did not
855"
REFERENCES,0.7243107769423559,"use legitimate features xl. Reasonable, necessary, and proportionate steps should have been taken
856"
REFERENCES,0.7251461988304093,"to minimise estimation error and aim for estimation parity (Section 2.3).
857"
REFERENCES,0.7259816207184628,"3. Evaluate differences. The bank should have considered whether there were true and legitimate
858"
REFERENCES,0.7268170426065163,"differences based on protected characteristics and whether they could have been “explained away”
859"
REFERENCES,0.7276524644945698,"by legitimate features (xl) to minimise statistical disparities.
860"
REFERENCES,0.7284878863826232,"These recommendations should be used to help identify and mitigate unlawful discrimination within
861"
REFERENCES,0.7293233082706767,"the specific context of each jurisdiction.
862"
REFERENCES,0.7301587301587301,"NeurIPS Paper Checklist
863"
CLAIMS,0.7309941520467836,"1. Claims
864"
CLAIMS,0.731829573934837,"Question: Do the main claims made in the abstract and introduction accurately reflect the
865"
CLAIMS,0.7326649958228906,"paper’s contributions and scope?
866"
CLAIMS,0.733500417710944,"Answer: [Yes]
867"
CLAIMS,0.7343358395989975,"Justification: The abstract and introduction clearly state the claims, contributions, assump-
868"
CLAIMS,0.7351712614870509,"tions and limitations of the paper.
869"
CLAIMS,0.7360066833751044,"Guidelines:
870"
CLAIMS,0.7368421052631579,"• The answer NA means that the abstract and introduction do not include the claims
871"
CLAIMS,0.7376775271512114,"made in the paper.
872"
CLAIMS,0.7385129490392648,"• The abstract and/or introduction should clearly state the claims made, including the
873"
CLAIMS,0.7393483709273183,"contributions made in the paper and important assumptions and limitations. A No or
874"
CLAIMS,0.7401837928153717,"NA answer to this question will not be perceived well by the reviewers.
875"
CLAIMS,0.7410192147034252,"• The claims made should match theoretical and experimental results, and reflect how
876"
CLAIMS,0.7418546365914787,"much the results can be expected to generalize to other settings.
877"
CLAIMS,0.7426900584795322,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
878"
CLAIMS,0.7435254803675856,"are not attained by the paper.
879"
LIMITATIONS,0.7443609022556391,"2. Limitations
880"
LIMITATIONS,0.7451963241436925,"Question: Does the paper discuss the limitations of the work performed by the authors?
881"
LIMITATIONS,0.746031746031746,"Answer: [Yes]
882"
LIMITATIONS,0.7468671679197995,"Justification: Section 1.4 sets out the limitations of this paper.
883"
LIMITATIONS,0.747702589807853,"Guidelines:
884"
LIMITATIONS,0.7485380116959064,"• The answer NA means that the paper has no limitation while the answer No means that
885"
LIMITATIONS,0.7493734335839599,"the paper has limitations, but those are not discussed in the paper.
886"
LIMITATIONS,0.7502088554720133,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
887"
LIMITATIONS,0.7510442773600668,"• The paper should point out any strong assumptions and how robust the results are to
888"
LIMITATIONS,0.7518796992481203,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
889"
LIMITATIONS,0.7527151211361738,"model well-specification, asymptotic approximations only holding locally). The authors
890"
LIMITATIONS,0.7535505430242272,"should reflect on how these assumptions might be violated in practice and what the
891"
LIMITATIONS,0.7543859649122807,"implications would be.
892"
LIMITATIONS,0.7552213868003341,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
893"
LIMITATIONS,0.7560568086883876,"only tested on a few datasets or with a few runs. In general, empirical results often
894"
LIMITATIONS,0.7568922305764411,"depend on implicit assumptions, which should be articulated.
895"
LIMITATIONS,0.7577276524644946,"• The authors should reflect on the factors that influence the performance of the approach.
896"
LIMITATIONS,0.758563074352548,"For example, a facial recognition algorithm may perform poorly when image resolution
897"
LIMITATIONS,0.7593984962406015,"is low or images are taken in low lighting. Or a speech-to-text system might not be
898"
LIMITATIONS,0.7602339181286549,"used reliably to provide closed captions for online lectures because it fails to handle
899"
LIMITATIONS,0.7610693400167085,"technical jargon.
900"
LIMITATIONS,0.7619047619047619,"• The authors should discuss the computational efficiency of the proposed algorithms
901"
LIMITATIONS,0.7627401837928154,"and how they scale with dataset size.
902"
LIMITATIONS,0.7635756056808688,"• If applicable, the authors should discuss possible limitations of their approach to
903"
LIMITATIONS,0.7644110275689223,"address problems of privacy and fairness.
904"
LIMITATIONS,0.7652464494569757,"• While the authors might fear that complete honesty about limitations might be used by
905"
LIMITATIONS,0.7660818713450293,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
906"
LIMITATIONS,0.7669172932330827,"limitations that aren’t acknowledged in the paper. The authors should use their best
907"
LIMITATIONS,0.7677527151211362,"judgment and recognize that individual actions in favor of transparency play an impor-
908"
LIMITATIONS,0.7685881370091896,"tant role in developing norms that preserve the integrity of the community. Reviewers
909"
LIMITATIONS,0.7694235588972431,"will be specifically instructed to not penalize honesty concerning limitations.
910"
THEORY ASSUMPTIONS AND PROOFS,0.7702589807852965,"3. Theory Assumptions and Proofs
911"
THEORY ASSUMPTIONS AND PROOFS,0.77109440267335,"Question: For each theoretical result, does the paper provide the full set of assumptions and
912"
THEORY ASSUMPTIONS AND PROOFS,0.7719298245614035,"a complete (and correct) proof?
913"
THEORY ASSUMPTIONS AND PROOFS,0.772765246449457,"Answer: [NA]
914"
THEORY ASSUMPTIONS AND PROOFS,0.7736006683375104,"Justification: The paper presents formalisations, which all include relevant assumptions and
915"
THEORY ASSUMPTIONS AND PROOFS,0.7744360902255639,"formatting, but no theoretical results.
916"
THEORY ASSUMPTIONS AND PROOFS,0.7752715121136173,"Guidelines:
917"
THEORY ASSUMPTIONS AND PROOFS,0.7761069340016709,"• The answer NA means that the paper does not include theoretical results.
918"
THEORY ASSUMPTIONS AND PROOFS,0.7769423558897243,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
919"
THEORY ASSUMPTIONS AND PROOFS,0.7777777777777778,"referenced.
920"
THEORY ASSUMPTIONS AND PROOFS,0.7786131996658312,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
921"
THEORY ASSUMPTIONS AND PROOFS,0.7794486215538847,"• The proofs can either appear in the main paper or the supplemental material, but if
922"
THEORY ASSUMPTIONS AND PROOFS,0.7802840434419381,"they appear in the supplemental material, the authors are encouraged to provide a short
923"
THEORY ASSUMPTIONS AND PROOFS,0.7811194653299917,"proof sketch to provide intuition.
924"
THEORY ASSUMPTIONS AND PROOFS,0.7819548872180451,"• Inversely, any informal proof provided in the core of the paper should be complemented
925"
THEORY ASSUMPTIONS AND PROOFS,0.7827903091060986,"by formal proofs provided in appendix or supplemental material.
926"
THEORY ASSUMPTIONS AND PROOFS,0.783625730994152,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
927"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7844611528822055,"4. Experimental Result Reproducibility
928"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7852965747702589,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
929"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7861319966583125,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
930"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7869674185463659,"of the paper (regardless of whether the code and data are provided or not)?
931"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7878028404344194,"Answer: [NA]
932"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7886382623224728,"Justification: The paper does not include experiments.
933"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7894736842105263,"Guidelines:
934"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7903091060985797,"• The answer NA means that the paper does not include experiments.
935"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7911445279866333,"• If the paper includes experiments, a No answer to this question will not be perceived
936"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7919799498746867,"well by the reviewers: Making the paper reproducible is important, regardless of
937"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7928153717627402,"whether the code and data are provided or not.
938"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7936507936507936,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
939"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7944862155388471,"to make their results reproducible or verifiable.
940"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7953216374269005,"• Depending on the contribution, reproducibility can be accomplished in various ways.
941"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7961570593149541,"For example, if the contribution is a novel architecture, describing the architecture fully
942"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7969924812030075,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
943"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.797827903091061,"be necessary to either make it possible for others to replicate the model with the same
944"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7986633249791144,"dataset, or provide access to the model. In general. releasing code and data is often
945"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7994987468671679,"one good way to accomplish this, but reproducibility can also be provided via detailed
946"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8003341687552213,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
947"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8011695906432749,"of a large language model), releasing of a model checkpoint, or other means that are
948"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8020050125313283,"appropriate to the research performed.
949"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8028404344193818,"• While NeurIPS does not require releasing code, the conference does require all submis-
950"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8036758563074352,"sions to provide some reasonable avenue for reproducibility, which may depend on the
951"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8045112781954887,"nature of the contribution. For example
952"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8053467000835421,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
953"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8061821219715957,"to reproduce that algorithm.
954"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8070175438596491,"(b) If the contribution is primarily a new model architecture, the paper should describe
955"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8078529657477026,"the architecture clearly and fully.
956"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.808688387635756,"(c) If the contribution is a new model (e.g., a large language model), then there should
957"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8095238095238095,"either be a way to access this model for reproducing the results or a way to reproduce
958"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.810359231411863,"the model (e.g., with an open-source dataset or instructions for how to construct
959"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8111946532999165,"the dataset).
960"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8120300751879699,"(d) We recognize that reproducibility may be tricky in some cases, in which case
961"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8128654970760234,"authors are welcome to describe the particular way they provide for reproducibility.
962"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8137009189640768,"In the case of closed-source models, it may be that access to the model is limited in
963"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8145363408521303,"some way (e.g., to registered users), but it should be possible for other researchers
964"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8153717627401837,"to have some path to reproducing or verifying the results.
965"
OPEN ACCESS TO DATA AND CODE,0.8162071846282373,"5. Open access to data and code
966"
OPEN ACCESS TO DATA AND CODE,0.8170426065162907,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
967"
OPEN ACCESS TO DATA AND CODE,0.8178780284043442,"tions to faithfully reproduce the main experimental results, as described in supplemental
968"
OPEN ACCESS TO DATA AND CODE,0.8187134502923976,"material?
969"
OPEN ACCESS TO DATA AND CODE,0.8195488721804511,"Answer: [NA]
970"
OPEN ACCESS TO DATA AND CODE,0.8203842940685045,"Justification: The paper does not include experiments.
971"
OPEN ACCESS TO DATA AND CODE,0.8212197159565581,"Guidelines:
972"
OPEN ACCESS TO DATA AND CODE,0.8220551378446115,"• The answer NA means that paper does not include experiments requiring code.
973"
OPEN ACCESS TO DATA AND CODE,0.822890559732665,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
974"
OPEN ACCESS TO DATA AND CODE,0.8237259816207184,"public/guides/CodeSubmissionPolicy) for more details.
975"
OPEN ACCESS TO DATA AND CODE,0.8245614035087719,"• While we encourage the release of code and data, we understand that this might not be
976"
OPEN ACCESS TO DATA AND CODE,0.8253968253968254,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
977"
OPEN ACCESS TO DATA AND CODE,0.8262322472848789,"including code, unless this is central to the contribution (e.g., for a new open-source
978"
OPEN ACCESS TO DATA AND CODE,0.8270676691729323,"benchmark).
979"
OPEN ACCESS TO DATA AND CODE,0.8279030910609858,"• The instructions should contain the exact command and environment needed to run to
980"
OPEN ACCESS TO DATA AND CODE,0.8287385129490392,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
981"
OPEN ACCESS TO DATA AND CODE,0.8295739348370927,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
982"
OPEN ACCESS TO DATA AND CODE,0.8304093567251462,"• The authors should provide instructions on data access and preparation, including how
983"
OPEN ACCESS TO DATA AND CODE,0.8312447786131997,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
984"
OPEN ACCESS TO DATA AND CODE,0.8320802005012531,"• The authors should provide scripts to reproduce all experimental results for the new
985"
OPEN ACCESS TO DATA AND CODE,0.8329156223893066,"proposed method and baselines. If only a subset of experiments are reproducible, they
986"
OPEN ACCESS TO DATA AND CODE,0.83375104427736,"should state which ones are omitted from the script and why.
987"
OPEN ACCESS TO DATA AND CODE,0.8345864661654135,"• At submission time, to preserve anonymity, the authors should release anonymized
988"
OPEN ACCESS TO DATA AND CODE,0.835421888053467,"versions (if applicable).
989"
OPEN ACCESS TO DATA AND CODE,0.8362573099415205,"• Providing as much information as possible in supplemental material (appended to the
990"
OPEN ACCESS TO DATA AND CODE,0.8370927318295739,"paper) is recommended, but including URLs to data and code is permitted.
991"
OPEN ACCESS TO DATA AND CODE,0.8379281537176274,"6. Experimental Setting/Details
992"
OPEN ACCESS TO DATA AND CODE,0.8387635756056808,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
993"
OPEN ACCESS TO DATA AND CODE,0.8395989974937343,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
994"
OPEN ACCESS TO DATA AND CODE,0.8404344193817878,"results?
995"
OPEN ACCESS TO DATA AND CODE,0.8412698412698413,"Answer: [NA]
996"
OPEN ACCESS TO DATA AND CODE,0.8421052631578947,"Justification: The paper does not include experiments.
997"
OPEN ACCESS TO DATA AND CODE,0.8429406850459482,"Guidelines:
998"
OPEN ACCESS TO DATA AND CODE,0.8437761069340016,"• The answer NA means that the paper does not include experiments.
999"
OPEN ACCESS TO DATA AND CODE,0.8446115288220551,"• The experimental setting should be presented in the core of the paper to a level of detail
1000"
OPEN ACCESS TO DATA AND CODE,0.8454469507101086,"that is necessary to appreciate the results and make sense of them.
1001"
OPEN ACCESS TO DATA AND CODE,0.8462823725981621,"• The full details can be provided either with the code, in appendix, or as supplemental
1002"
OPEN ACCESS TO DATA AND CODE,0.8471177944862155,"material.
1003"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.847953216374269,"7. Experiment Statistical Significance
1004"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8487886382623224,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
1005"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.849624060150376,"information about the statistical significance of the experiments?
1006"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8504594820384294,"Answer: [NA] .
1007"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8512949039264829,"Justification: The paper does not include experiments.
1008"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8521303258145363,"Guidelines:
1009"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8529657477025898,"• The answer NA means that the paper does not include experiments.
1010"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8538011695906432,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
1011"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8546365914786967,"dence intervals, or statistical significance tests, at least for the experiments that support
1012"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8554720133667502,"the main claims of the paper.
1013"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8563074352548037,"• The factors of variability that the error bars are capturing should be clearly stated (for
1014"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8571428571428571,"example, train/test split, initialization, random drawing of some parameter, or overall
1015"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8579782790309106,"run with given experimental conditions).
1016"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.858813700918964,"• The method for calculating the error bars should be explained (closed form formula,
1017"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8596491228070176,"call to a library function, bootstrap, etc.)
1018"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.860484544695071,"• The assumptions made should be given (e.g., Normally distributed errors).
1019"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8613199665831245,"• It should be clear whether the error bar is the standard deviation or the standard error
1020"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8621553884711779,"of the mean.
1021"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8629908103592314,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
1022"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8638262322472848,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
1023"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8646616541353384,"of Normality of errors is not verified.
1024"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8654970760233918,"• For asymmetric distributions, the authors should be careful not to show in tables or
1025"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8663324979114453,"figures symmetric error bars that would yield results that are out of range (e.g. negative
1026"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8671679197994987,"error rates).
1027"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8680033416875522,"• If error bars are reported in tables or plots, The authors should explain in the text how
1028"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8688387635756056,"they were calculated and reference the corresponding figures or tables in the text.
1029"
EXPERIMENTS COMPUTE RESOURCES,0.8696741854636592,"8. Experiments Compute Resources
1030"
EXPERIMENTS COMPUTE RESOURCES,0.8705096073517126,"Question: For each experiment, does the paper provide sufficient information on the com-
1031"
EXPERIMENTS COMPUTE RESOURCES,0.8713450292397661,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
1032"
EXPERIMENTS COMPUTE RESOURCES,0.8721804511278195,"the experiments?
1033"
EXPERIMENTS COMPUTE RESOURCES,0.873015873015873,"Answer: [NA] .
1034"
EXPERIMENTS COMPUTE RESOURCES,0.8738512949039264,"Justification: The paper does not include experiments.
1035"
EXPERIMENTS COMPUTE RESOURCES,0.87468671679198,"Guidelines:
1036"
EXPERIMENTS COMPUTE RESOURCES,0.8755221386800334,"• The answer NA means that the paper does not include experiments.
1037"
EXPERIMENTS COMPUTE RESOURCES,0.8763575605680869,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
1038"
EXPERIMENTS COMPUTE RESOURCES,0.8771929824561403,"or cloud provider, including relevant memory and storage.
1039"
EXPERIMENTS COMPUTE RESOURCES,0.8780284043441938,"• The paper should provide the amount of compute required for each of the individual
1040"
EXPERIMENTS COMPUTE RESOURCES,0.8788638262322472,"experimental runs as well as estimate the total compute.
1041"
EXPERIMENTS COMPUTE RESOURCES,0.8796992481203008,"• The paper should disclose whether the full research project required more compute
1042"
EXPERIMENTS COMPUTE RESOURCES,0.8805346700083542,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
1043"
EXPERIMENTS COMPUTE RESOURCES,0.8813700918964077,"didn’t make it into the paper).
1044"
CODE OF ETHICS,0.8822055137844611,"9. Code Of Ethics
1045"
CODE OF ETHICS,0.8830409356725146,"Question: Does the research conducted in the paper conform, in every respect, with the
1046"
CODE OF ETHICS,0.883876357560568,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
1047"
CODE OF ETHICS,0.8847117794486216,"Answer: [Yes]
1048"
CODE OF ETHICS,0.885547201336675,"Justification: Our research rigorously addresses the ethical code outlined by the conference,
1049"
CODE OF ETHICS,0.8863826232247285,"particularly focusing on issues related to safety, security, discrimination, and fairness. We
1050"
CODE OF ETHICS,0.8872180451127819,"have proactively identified and discussed potential harmful outcomes, particularly those
1051"
CODE OF ETHICS,0.8880534670008354,"involving discrimination and misuse in the contexts of legal and ethical standards. Further-
1052"
CODE OF ETHICS,0.8888888888888888,"more, we provide recommendations to mitigate these risks, underscoring our commitment
1053"
CODE OF ETHICS,0.8897243107769424,"to the responsible development and application of technology that respects human rights
1054"
CODE OF ETHICS,0.8905597326649958,"and societal values. The paper does not contain research involving human subjects or
1055"
CODE OF ETHICS,0.8913951545530493,"participants, it does not conduct experiments or have data-related concerns.
1056"
CODE OF ETHICS,0.8922305764411027,"Guidelines:
1057"
CODE OF ETHICS,0.8930659983291562,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
1058"
CODE OF ETHICS,0.8939014202172096,"• If the authors answer No, they should explain the special circumstances that require a
1059"
CODE OF ETHICS,0.8947368421052632,"deviation from the Code of Ethics.
1060"
CODE OF ETHICS,0.8955722639933166,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
1061"
CODE OF ETHICS,0.8964076858813701,"eration due to laws or regulations in their jurisdiction).
1062"
BROADER IMPACTS,0.8972431077694235,"10. Broader Impacts
1063"
BROADER IMPACTS,0.898078529657477,"Question: Does the paper discuss both potential positive societal impacts and negative
1064"
BROADER IMPACTS,0.8989139515455304,"societal impacts of the work performed?
1065"
BROADER IMPACTS,0.899749373433584,"Answer: [Yes]
1066"
BROADER IMPACTS,0.9005847953216374,"Justification: Our research contributes to bridging the gap between legal standards and
1067"
BROADER IMPACTS,0.9014202172096909,"algorithmic fairness, aiming to enhance the integrity and fairness of automated decision-
1068"
BROADER IMPACTS,0.9022556390977443,"making systems. This has significant implications for improving equity in critical areas
1069"
BROADER IMPACTS,0.9030910609857978,"where algorithmic decisions are increasingly prevalent. We also the risks of unfair treatment
1070"
BROADER IMPACTS,0.9039264828738512,"based on model or data biases or misinterpretation of the legal doctrines we study. We
1071"
BROADER IMPACTS,0.9047619047619048,"explore the potential for unintended consequences even when the technology functions
1072"
BROADER IMPACTS,0.9055973266499582,"as intended, such as the reinforcement of existing societal biases under the guise of legal
1073"
BROADER IMPACTS,0.9064327485380117,"compliance. To mitigate these risks, we propose specific safeguards to prevent unlawful
1074"
BROADER IMPACTS,0.9072681704260651,"discrimination in systems.
1075"
BROADER IMPACTS,0.9081035923141186,"Guidelines:
1076"
BROADER IMPACTS,0.908939014202172,"• The answer NA means that there is no societal impact of the work performed.
1077"
BROADER IMPACTS,0.9097744360902256,"• If the authors answer NA or No, they should explain why their work has no societal
1078"
BROADER IMPACTS,0.910609857978279,"impact or why the paper does not address societal impact.
1079"
BROADER IMPACTS,0.9114452798663325,"• Examples of negative societal impacts include potential malicious or unintended uses
1080"
BROADER IMPACTS,0.9122807017543859,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
1081"
BROADER IMPACTS,0.9131161236424394,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
1082"
BROADER IMPACTS,0.9139515455304928,"groups), privacy considerations, and security considerations.
1083"
BROADER IMPACTS,0.9147869674185464,"• The conference expects that many papers will be foundational research and not tied
1084"
BROADER IMPACTS,0.9156223893065998,"to particular applications, let alone deployments. However, if there is a direct path to
1085"
BROADER IMPACTS,0.9164578111946533,"any negative applications, the authors should point it out. For example, it is legitimate
1086"
BROADER IMPACTS,0.9172932330827067,"to point out that an improvement in the quality of generative models could be used to
1087"
BROADER IMPACTS,0.9181286549707602,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
1088"
BROADER IMPACTS,0.9189640768588136,"that a generic algorithm for optimizing neural networks could enable people to train
1089"
BROADER IMPACTS,0.9197994987468672,"models that generate Deepfakes faster.
1090"
BROADER IMPACTS,0.9206349206349206,"• The authors should consider possible harms that could arise when the technology is
1091"
BROADER IMPACTS,0.9214703425229741,"being used as intended and functioning correctly, harms that could arise when the
1092"
BROADER IMPACTS,0.9223057644110275,"technology is being used as intended but gives incorrect results, and harms following
1093"
BROADER IMPACTS,0.923141186299081,"from (intentional or unintentional) misuse of the technology.
1094"
BROADER IMPACTS,0.9239766081871345,"• If there are negative societal impacts, the authors could also discuss possible mitigation
1095"
BROADER IMPACTS,0.924812030075188,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
1096"
BROADER IMPACTS,0.9256474519632414,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
1097"
BROADER IMPACTS,0.9264828738512949,"feedback over time, improving the efficiency and accessibility of ML).
1098"
SAFEGUARDS,0.9273182957393483,"11. Safeguards
1099"
SAFEGUARDS,0.9281537176274018,"Question: Does the paper describe safeguards that have been put in place for responsible
1100"
SAFEGUARDS,0.9289891395154553,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
1101"
SAFEGUARDS,0.9298245614035088,"image generators, or scraped datasets)?
1102"
SAFEGUARDS,0.9306599832915622,"Answer: [NA] .
1103"
SAFEGUARDS,0.9314954051796157,"Justification: The paper does not release data or models.
1104"
SAFEGUARDS,0.9323308270676691,"Guidelines:
1105"
SAFEGUARDS,0.9331662489557226,"• The answer NA means that the paper poses no such risks.
1106"
SAFEGUARDS,0.934001670843776,"• Released models that have a high risk for misuse or dual-use should be released with
1107"
SAFEGUARDS,0.9348370927318296,"necessary safeguards to allow for controlled use of the model, for example by requiring
1108"
SAFEGUARDS,0.935672514619883,"that users adhere to usage guidelines or restrictions to access the model or implementing
1109"
SAFEGUARDS,0.9365079365079365,"safety filters.
1110"
SAFEGUARDS,0.9373433583959899,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
1111"
SAFEGUARDS,0.9381787802840434,"should describe how they avoided releasing unsafe images.
1112"
SAFEGUARDS,0.9390142021720969,"• We recognize that providing effective safeguards is challenging, and many papers do
1113"
SAFEGUARDS,0.9398496240601504,"not require this, but we encourage authors to take this into account and make a best
1114"
SAFEGUARDS,0.9406850459482038,"faith effort.
1115"
LICENSES FOR EXISTING ASSETS,0.9415204678362573,"12. Licenses for existing assets
1116"
LICENSES FOR EXISTING ASSETS,0.9423558897243107,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
1117"
LICENSES FOR EXISTING ASSETS,0.9431913116123642,"the paper, properly credited and are the license and terms of use explicitly mentioned and
1118"
LICENSES FOR EXISTING ASSETS,0.9440267335004177,"properly respected?
1119"
LICENSES FOR EXISTING ASSETS,0.9448621553884712,"Answer: [NA] .
1120"
LICENSES FOR EXISTING ASSETS,0.9456975772765246,"Justification: The paper does not use existing assets.
1121"
LICENSES FOR EXISTING ASSETS,0.9465329991645781,"Guidelines:
1122"
LICENSES FOR EXISTING ASSETS,0.9473684210526315,"• The answer NA means that the paper does not use existing assets.
1123"
LICENSES FOR EXISTING ASSETS,0.948203842940685,"• The authors should cite the original paper that produced the code package or dataset.
1124"
LICENSES FOR EXISTING ASSETS,0.9490392648287385,"• The authors should state which version of the asset is used and, if possible, include a
1125"
LICENSES FOR EXISTING ASSETS,0.949874686716792,"URL.
1126"
LICENSES FOR EXISTING ASSETS,0.9507101086048454,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
1127"
LICENSES FOR EXISTING ASSETS,0.9515455304928989,"• For scraped data from a particular source (e.g., website), the copyright and terms of
1128"
LICENSES FOR EXISTING ASSETS,0.9523809523809523,"service of that source should be provided.
1129"
LICENSES FOR EXISTING ASSETS,0.9532163742690059,"• If assets are released, the license, copyright information, and terms of use in the
1130"
LICENSES FOR EXISTING ASSETS,0.9540517961570593,"package should be provided. For popular datasets, paperswithcode.com/datasets
1131"
LICENSES FOR EXISTING ASSETS,0.9548872180451128,"has curated licenses for some datasets. Their licensing guide can help determine the
1132"
LICENSES FOR EXISTING ASSETS,0.9557226399331662,"license of a dataset.
1133"
LICENSES FOR EXISTING ASSETS,0.9565580618212197,"• For existing datasets that are re-packaged, both the original license and the license of
1134"
LICENSES FOR EXISTING ASSETS,0.9573934837092731,"the derived asset (if it has changed) should be provided.
1135"
LICENSES FOR EXISTING ASSETS,0.9582289055973267,"• If this information is not available online, the authors are encouraged to reach out to
1136"
LICENSES FOR EXISTING ASSETS,0.9590643274853801,"the asset’s creators.
1137"
NEW ASSETS,0.9598997493734336,"13. New Assets
1138"
NEW ASSETS,0.960735171261487,"Question: Are new assets introduced in the paper well documented and is the documentation
1139"
NEW ASSETS,0.9615705931495405,"provided alongside the assets?
1140"
NEW ASSETS,0.9624060150375939,"Answer: [NA] .
1141"
NEW ASSETS,0.9632414369256475,"Justification: The paper does not release new assets.
1142"
NEW ASSETS,0.9640768588137009,"Guidelines:
1143"
NEW ASSETS,0.9649122807017544,"• The answer NA means that the paper does not release new assets.
1144"
NEW ASSETS,0.9657477025898078,"• Researchers should communicate the details of the dataset/code/model as part of their
1145"
NEW ASSETS,0.9665831244778613,"submissions via structured templates. This includes details about training, license,
1146"
NEW ASSETS,0.9674185463659147,"limitations, etc.
1147"
NEW ASSETS,0.9682539682539683,"• The paper should discuss whether and how consent was obtained from people whose
1148"
NEW ASSETS,0.9690893901420217,"asset is used.
1149"
NEW ASSETS,0.9699248120300752,"• At submission time, remember to anonymize your assets (if applicable). You can either
1150"
NEW ASSETS,0.9707602339181286,"create an anonymized URL or include an anonymized zip file.
1151"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9715956558061821,"14. Crowdsourcing and Research with Human Subjects
1152"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9724310776942355,"Question: For crowdsourcing experiments and research with human subjects, does the paper
1153"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9732664995822891,"include the full text of instructions given to participants and screenshots, if applicable, as
1154"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9741019214703425,"well as details about compensation (if any)?
1155"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.974937343358396,"Answer: [NA] .
1156"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9757727652464494,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
1157"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9766081871345029,"Guidelines:
1158"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9774436090225563,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1159"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9782790309106099,"human subjects.
1160"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9791144527986633,"• Including this information in the supplemental material is fine, but if the main contribu-
1161"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9799498746867168,"tion of the paper involves human subjects, then as much detail as possible should be
1162"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9807852965747702,"included in the main paper.
1163"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9816207184628237,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
1164"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9824561403508771,"or other labor should be paid at least the minimum wage in the country of the data
1165"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9832915622389307,"collector.
1166"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9841269841269841,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
1167"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9849624060150376,"Subjects
1168"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.985797827903091,"Question: Does the paper describe potential risks incurred by study participants, whether
1169"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9866332497911445,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
1170"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9874686716791979,"approvals (or an equivalent approval/review based on the requirements of your country or
1171"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9883040935672515,"institution) were obtained?
1172"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9891395154553049,"Answer: [NA] .
1173"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9899749373433584,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
1174"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9908103592314118,"Guidelines:
1175"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9916457811194653,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1176"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9924812030075187,"human subjects.
1177"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9933166248955723,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
1178"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9941520467836257,"may be required for any human subjects research. If you obtained IRB approval, you
1179"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9949874686716792,"should clearly state this in the paper.
1180"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9958228905597326,"• We recognize that the procedures for this may vary significantly between institutions
1181"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9966583124477861,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
1182"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9974937343358395,"guidelines for their institution.
1183"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9983291562238931,"• For initial submissions, do not include any information that would break anonymity (if
1184"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9991645781119465,"applicable), such as the institution conducting the review.
1185"
