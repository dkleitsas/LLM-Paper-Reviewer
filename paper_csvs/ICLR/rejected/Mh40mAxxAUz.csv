Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0040650406504065045,"Differential Privacy (DP) is the de facto standard for reasoning about the privacy
guarantees of a training algorithm. Despite the empirical observation that DP re-
duces the vulnerability of models to existing membership inference (MI) attacks,
a theoretical underpinning as to why this is the case is largely missing in the lit-
erature. In practice, this means that models need to be trained with differential
privacy guarantees that greatly decrease their accuracy. In this paper we provide
a tighter bound on the accuracy of any membership inference adversary when a
training algorithm provides ϵ-DP. Our bound informs the design of a novel privacy
ampliﬁcation scheme, where an effective training set is sub-sampled from a larger
set prior to the beginning of training, to greatly reduce the bound on MI accuracy.
As a result, our scheme enables ϵ-DP users to employ looser differential privacy
guarantees when training their model to limit the success of any MI adversary; this
in turn ensures that the model’s accuracy is less impacted by the privacy guarantee.
Finally, we discuss implications of our MI bound on machine unlearning."
INTRODUCTION,0.008130081300813009,"1
INTRODUCTION"
INTRODUCTION,0.012195121951219513,"Differential Privacy (DP) (Dwork, 2006) is employed extensively to reason about privacy guarantees
in a variety of different settings (Dwork, 2008). Recently, DP started being used to give privacy
guarantees for the training data of deep neural networks (DNNs) learned through stochastic gradient
descent (Abadi et al., 2016). However, even though DP gives privacy guarantees and bounds the
worst case privacy leakage, it is not immediately clear how these guarantees bound the accuracy of
known existing forms of privacy infringement attacks."
INTRODUCTION,0.016260162601626018,"At the time of writing, the most practical attack on the privacy of DNNs is Membership Inference
(MI) (Shokri et al., 2017), where an attacker predicts whether or not a model used a particular data
point for training (note that this is quite similar to the hypothetical adversary at the core of the
game instantiated in the deﬁnition of DP); membership inference attacks saw a strong interest by
the community, and several improvements and renditions were proposed since its inception (Sablay-
rolles et al., 2019; Choquette-Choo et al., 2021; Maini et al., 2021; Hu et al., 2021). Having privacy
guarantees would desirably defend against MI, and in fact current literature highlights that DP does
indeed give an upper bound on the performance of MI adversaries (Yeom et al., 2018; Erlingsson
et al., 2019; Sablayrolles et al., 2019; Jayaraman et al., 2020)."
INTRODUCTION,0.02032520325203252,"In this paper, we ﬁrst propose a tighter bound on MI accuracy for training algorithms that provide
ϵ-DP. Our approach uses a lemma about an equivalence of certain sets of datasets. Furthermore, in
obtaining our bound, we also show how this bound can beneﬁt from a form of privacy ampliﬁcation
where the training dataset itself is sub-sampled from a larger dataset. Ampliﬁcation is a technique
pervasively found in work improving the analysis of DP learners like DP-SGD (Abadi et al., 2016),
and we observe the effect of our ampliﬁcation on lowering MI accuracy is signiﬁcantly stronger than
the effect of batch sampling, a common privacy ampliﬁcation scheme for training DNNs."
INTRODUCTION,0.024390243902439025,"Our bound also has consequences for the problem of unlearning (or data forgetting in ML) intro-
duced by Cao & Yang (2015). In particular the MI accuracy on the point to be unlearned is a popular
measure for how well a model has unlearned (Baumhauer et al., 2020; Graves et al., 2020; Golatkar
et al., 2020b;a). However empirical veriﬁcation of the MI accuracy can be open ended, as it is sub-
jective to the attack employed. Theoretical bounds on all MI attacks, such as the one proposed in
this work, circumvent this issue; a bound on the accuracy of MI attacks, in particular the probability
a data point was used in the training dataset, indicates a limitation for any entity to discern if the"
INTRODUCTION,0.028455284552845527,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.032520325203252036,"model had trained on the data point. In the case when this is sufﬁciently low (where sufﬁciently is
deﬁned apriori), one can then claim to have unlearned by achieving a model sufﬁciently likely to
have not come from training with the data point. Our analysis shows that, if dataset subsampling is
used, one can unlearn under this deﬁnition by training with a relatively large ϵ (and thus have less
cost to performance)."
INTRODUCTION,0.036585365853658534,"To summarize, our contributions are:"
INTRODUCTION,0.04065040650406504,• We present a tighter general bound on MI accuracy for ϵ-DP;
INTRODUCTION,0.044715447154471545,"• We further demonstrate how to lower this bound using a novel privacy ampliﬁcation scheme
built on dataset subsampling;"
INTRODUCTION,0.04878048780487805,"• We discuss the beneﬁts of such bounds to Machine Unlearning as a rigorous way to use MI
as a metric for unlearning."
BACKGROUND,0.052845528455284556,"2
BACKGROUND"
DIFFERENTIAL PRIVACY,0.056910569105691054,"2.1
DIFFERENTIAL PRIVACY"
DIFFERENTIAL PRIVACY,0.06097560975609756,"Differential privacy (DP) (Dwork, 2006) bounds how different the outputs of a function on adjacent
inputs can be in order to give privacy guarantees for the inputs. More formally a function F is ϵ-DP
if for all adjacent inputs x and x′ (i.e. inputs with hamming distance 1) we have for all sets S in the
output space:"
DIFFERENTIAL PRIVACY,0.06504065040650407,"P(F(x) ∈S) ≤eϵP(F(x′) ∈S)
(1)"
DIFFERENTIAL PRIVACY,0.06910569105691057,"We also have a more relaxed notion of (ϵ, δ)-DP where in the same setup as above, but with a
parameter δ ∈(0, 1], we have P(F(x) ∈S) ≤eϵP(F(x′) ∈S) + δ. Notably, (ϵ, δ)-DP is used for
functions where it is more natural to work with ℓ2 metrics on the input space, which has to do with
how DP guarantees are obtained."
DIFFERENTIAL PRIVACY,0.07317073170731707,"To achieve DP guarantees one usually introduces noise to the output of the function F. The amount
of noise is calibrated to the maximal ℓ2 or ℓ1 difference between all possible outputs of the function
on adjacent datasets (also called sensitivity). Signiﬁcant progress was achieved on minimizing the
amount of noise needed for a given sensitivity (Balle & Wang, 2018), and on how DP guarantees
scale when composing multiple DP functions (Dwork et al., 2010; Kairouz et al., 2015)."
DIFFERENTIAL PRIVACY,0.07723577235772358,"Abadi et al. (2016) demonstrated a method to make the ﬁnal model returned by mini-batch SGD
(ϵ, δ)-DP with respect to its training dataset by bounding the sensitivity of gradient updates during
mini-batch SGD and introducing Gaussian noise to each update. This approach became the de-facto
standard for DP guarantees in DNNs. However, the adoption is still greatly limited because of an
observed trade-off between privacy guarantees and model utility. At the time of writing there is still
no feasible ways to learn with low ϵ and high accuracy, and past work (Jagielski et al., 2020) have
suggested that DP-analysis may be too loose and provides more privacy than is expected."
DIFFERENTIAL PRIVACY,0.08130081300813008,"However, more recently Nasr et al. (2021) showed (using statistical tests and stronger MI adver-
saries) that the current state of the art approaches to achieving (ϵ, δ)-DP bounds for deep learning
are tight, in contrast to Jagielski et al. (2020) results suggesting that they were loose. That is, there
is not much more improvement to be gained by studying how to improve the (ϵ, δ)-DP bound from
a given amount of noise or improving composition rules. Facing this, future improvements in DP
training would lie in understanding the guarantees that DP bounds provide against the performance
of relevant privacy attacks. This would allow us to be more informed about the guarantees required
during training to defeat practical attacks and enable the use of looser guarantees if one is only
interested in defending against a speciﬁc set of attacks. 1."
DIFFERENTIAL PRIVACY,0.08536585365853659,"1It is worth noting that Nasr et al. (2021) showed that current analytic upper bounds on DP guarantees are
tight, measuring them empirically with various strong privacy adversaries. Although results do suggest that
bounds match, the paper did not investigate how DP guarantees limit performance of the adversary."
DIFFERENTIAL PRIVACY,0.08943089430894309,Under review as a conference paper at ICLR 2022
MEMBERSHIP INFERENCE,0.09349593495934959,"2.2
MEMBERSHIP INFERENCE"
MEMBERSHIP INFERENCE,0.0975609756097561,"Shokri et al. (2017) introduced a MI attack against DNNs, which leveraged shadow models (models
with the same architecture as the target model) trained on similar data in order to train a classiﬁer
which, given the outputs of a model on a data point, predicts if the model was trained on that
data point or not. Since the introduction of this initial attack, the community has proposed several
improved and variations of the original MI attack (Yeom et al., 2018; Salem et al., 2018; Sablayrolles
et al., 2019; Truex et al., 2019; Jayaraman et al., 2020; Maini et al., 2021), such as an attack that
only look at predicted labels of the target model (Choquette-Choo et al., 2021)."
MEMBERSHIP INFERENCE,0.1016260162601626,"MI attacks are currently the main practical threat to the privacy of a user’s data used to train DNNs.
Especially in the context of DP DNN learning, it would be beneﬁcial to know how DP bounds
translate to bounds on the accuracy of MI attacks. This would provide an understanding on what sort
of DP guarantees an entity requires to ensure a sufﬁciently low maximum accuracy of an adversary
trying to discern their training data from their deployed model. The bounds would hence help in
guiding decisions regarding setting these parameters and their corresponding guarantees."
MEMBERSHIP INFERENCE,0.10569105691056911,"In our paper we will work with the following (abstract) notion of MI in giving our bounds. In
particular we deﬁne our MI adversary as a function f which takes a set of models S and a data
point x∗to deterministically output either 1 or 0, corresponding to whether x∗was in the dataset
D used to obtain the models in S or not respectively. Note the generality of this adversary, as we
do not consider how the adversary obtains the function, i.e. it can have arbitrary strength. Any such
adversary will then satisfy the upper and lower bounds we derive later in the paper."
MEMBERSHIP INFERENCE,0.10975609756097561,"With that deﬁnition of our adversary, we have the following deﬁnition of positive and negative MI
accuracy (which is what we focus on in this paper); note that here D is the training dataset and
P(x∗∈D|S) is the probability a data point x∗was in the training dataset used to obtain the models
in the set S (i.e. this is a probability over datasets). We explain more about where the randomness
is introduced (in particular the probability involved in obtaining a training dataset) in Section 3.1."
MEMBERSHIP INFERENCE,0.11382113821138211,"Deﬁnition 1 (MI accuracy). The positive accuracy of f(x∗, S), the accuracy if f outputs 1 which
we deﬁne as A(f(x∗, S) = 1), is P(x∗∈D|S) and the negative accuracy, the accuracy if f outputs
0 which we deﬁne as A(f(x∗, S) = 0), is P(x∗/∈D|S) = 1 −P(x∗∈D|S)"
PREVIOUS BOUNDS,0.11788617886178862,"2.3
PREVIOUS BOUNDS"
PREVIOUS BOUNDS,0.12195121951219512,"Before giving bounds on MI accuracy, we have to formally deﬁne the attack setting. Two of the
main bounds (Yeom et al., 2018; Erlingsson et al., 2019) focused on an experimental setup ﬁrst
introduced by Yeom et al. (2018). To summarize the experiment, and in particular the situation
the adversary is operating in, an adversary f is given a datapoint x∗that is 50% likely to have
been used to train a model S or not. The adversary then either predicts 1 if they think it was
used, or 0 otherwise. Let b = 1 or 0 indicate if the datapoint was or was not used for training
respectively. We say the adversary was correct if their prediction matches b. We then deﬁne the
adversary’s advantage as improvement in accuracy over the 50% baseline of random guessing, or
more speciﬁcally 2(A(f) −0.5) where A(f) is the accuracy of f."
PREVIOUS BOUNDS,0.12601626016260162,"For such an adversary operating in a scenario where data is equally likely to be included or not in
the training dataset, Yeom et al. (2018) showed that they could bound the advantage of the adversary
by eϵ −1 when training with ϵ-DP. In other words, they showed that they could bound the accuracy
of the MI adversary by eϵ/2. Their proof used the fact that the true positive rate (TPR) and false
positive rate (FPR) of their adversary could be represented as expectations over the different data
points in a dataset, and from that introduced the DP condition to obtain their MI bound, noting that
MI advantage is equivalent to TPR - FPR."
PREVIOUS BOUNDS,0.13008130081300814,"Erlingsson et al. (2019) improved on the bound developed by Yeom et al. (2018) for an adversary
operating under the same condition by utilizing a proposition given by Hall et al. (2013) on the
relation between TPR and FPR for an (ϵ, δ)-DP function. Using these facts, Erlingsson et al. (2019)
bounded the membership advantage by 1−e−ϵ+δe−ϵ, which is equivalent to bounding the accuracy
of the adversary by 1 −e−ϵ/2 when δ = 0 (i.e. in ϵ-DP). This is, to the best of our knowledge, the
previous state-of-the-art bound for high ϵ."
PREVIOUS BOUNDS,0.13414634146341464,Under review as a conference paper at ICLR 2022
PREVIOUS BOUNDS,0.13821138211382114,"Other work have considered more general setups where the probability of sampling a datapoint in
the dataset can vary, similar to what we consider in Section 3.1. For ϵ-DP, Sablayrolles et al. (2019)
bounded the probability of a datapoint x∗being used in the training set of a model (i.e., the accuracy
of an attacker who predicted the datapoint was in the dataset of the model) by Px∗(1) + ϵ"
WHERE,0.14227642276422764,"4 where
Px∗(1) is the probability of the datapoint being in the dataset. This is, to the best of our knowledge,
the previous state-of-the-art bound for low ϵ (when reduced to case of Yeom et al. (2018) by setting
Px∗(1) = 0.5 as to compare with Erlingsson et al. (2019))."
WHERE,0.14634146341463414,"Finally, Jayaraman et al. (2020) bounded the positive predictive value of an attacker (i.e. its preci-
sion) on a model trained with (ϵ, δ)-DP when the FPR is ﬁxed. It is worth noting that although the
considered setup is similar to works covered above, it assumes an unbalanced sampling procedure.
Similarly, Jayaraman et al. further bounded membership advantage under the experiment described
by Yeom et al. (2018) for a ﬁxed FPR. Erlingsson et al. (2019) followed a similar argument for their
bound but were also able to remove the need for an explicit knowledge of the FPR."
UNLEARNING,0.15040650406504066,"2.4
UNLEARNING"
UNLEARNING,0.15447154471544716,"Having bounds on membership inference is particularly relevant to machine unlearning for DNNs.
Machine unlearning was ﬁrst introduced by Cao & Yang (2015), who described a setting where
it is important for the model to be able to forget certain training data points and focused on the
cases where there were efﬁcient analytic solutions. It was then extended to DNNs by Bourtoule
et al. (2019) with the deﬁnition that a model has unlearned a data point if after the unlearning, the
distribution of models returned is identical to the one that would result from not training with the
data point at all. This deﬁnition was also stated earlier by Ginart et al. (2019) for other classes of
machine learning models."
UNLEARNING,0.15853658536585366,"Given that unlearning is interested in removing the impact a data point had on the model, further
work employed MI accuracy on the data point to be unlearned as a metric for how well the model
had forgotten it after using some proposed unlearning method (Baumhauer et al., 2020; Graves et al.,
2020; Golatkar et al., 2020b;a). Yet, empirical estimates on the membership status of a datapoint
are subjective to the concrete MI attacks employed – indeed it may be possible that there exists a
stronger practical attack."
UNLEARNING,0.16260162601626016,"Analytic bounds to MI attacks, on the other hand, resolve the subjectivity issue of MI as a metric for
unlearning as they bound the success of any adversary. In particular one could give the following
deﬁnition of an unlearning guarantee from a formal MI positive accuracy bound:
Deﬁnition 2 (B-MI Unlearning Guarantee). An algorithm is B-MI unlearnt for x∗if P(ˆx∗/∈
D|S) ≥B, i.e. the probability of x∗not being in the training dataset is greater than B."
UNLEARNING,0.16666666666666666,"Therefore, our result bounding positive MI accuracy has direct consequences on the ﬁeld of machine
unlearning, which we further elaborate in Section 6"
THE BOUND,0.17073170731707318,"3
THE BOUND"
THE SCENARIO,0.17479674796747968,"3.1
THE SCENARIO"
THE SCENARIO,0.17886178861788618,"We now proceed to formalize the scenario under which our adversary operates. Here, our scenario is
more general than the one introduced by Yeom et al. (2018) and, as we note later, a speciﬁc instance
of it can be reduced to their setup. In particular we formalize how an entity samples data into the
training dataset, and proceed with our analysis from there."
THE SCENARIO,0.18292682926829268,"Our intuition here is that one can imagine the existence of some large data superset containing all
the data points an entity could have in their training dataset. Yet, any one of these datapoints only
has some probability of being sampled into the training dataset. For example, this larger dataset
could consist of all the users that gave an entity access to their data, and the probability comes from
the entity randomly sampling the data to use in their training dataset. This randomness can be a
black-box such that not even the entity knows what data was used to train. In essence, this is the
scenario Jayaraman et al. (2020) considers, though in their case, the larger dataset is implicit and
takes the form of a distribution. We can then imagine that the adversary (or perhaps an arbitrator in
an unlearning setup) knows the larger dataset and tries to infer whether a particular data point was"
THE SCENARIO,0.18699186991869918,Under review as a conference paper at ICLR 2022
THE SCENARIO,0.1910569105691057,"used in the training dataset. The particular MI attack that we analyze and bound is based on this
scenario."
THE SCENARIO,0.1951219512195122,"Speciﬁcally, let the individual training datasets D be constructed by sampling from a ﬁnite count-
able set where all datapoints are unique and sampled independently, i.e. from some larger set
{x1, · · · , xN}. That is if D = {x1, x2, · · · , xn} then the probability of sampling D is P(D) =
Px1(1)Px2(1) · · · Pxn(1)Pxn+1(0) · · · PxN (0), where Pxi(1) is probability of drawing xi into the
dataset and Pxi(0) is the probability of not."
THE SCENARIO,0.1991869918699187,"We deﬁne D as the set of all datasets. Let now Dx∗be the set of all datasets that contain a particular
point x∗∈{x1, · · · , xN}, that is Dx∗= {D s.t x∗∈D}. Similarly let Dx∗′ be the set of all
datasets that do not contain x∗, i.e. Dx∗′ = {D′ s.t x∗/∈D′}. Note D = Dx∗∪Dx∗′ by the
simple logic that any dataset has or does not have x∗in it. We then have the following lemma (see
Appendix A for the proof)."
THE SCENARIO,0.2032520325203252,Lemma 1. Dx∗and Dx∗′ are in bijective correspondence with P(D) Px∗(0)
THE SCENARIO,0.2073170731707317,Px∗(1) = P(D′) for D ∈Dx∗
THE SCENARIO,0.21138211382113822,and D′ ∈Dx∗′ that map to each other under the bijective correspondence.
THE SCENARIO,0.21544715447154472,"Once some dataset D is obtained, we call H the training function which takes in D and outputs a
model M as a set of weights in the form of a real vector. Recall that H is ϵ-DP if for all adjacent
datasets D and D′ and any set of model(s) S in the output space of H (i.e. some weights) we have:
P(H(D) ∈S) ≤eϵP(H(D′) ∈S). It should be noted that from now on we assume that the
set S has a non-zero probability to be produced by H. This is sensible as we are not interested in
membership inference attacks on sets of models that have 0 probability to come from training; note
also if P(H(D) ∈S) = 0, then P(H(D′) ∈S) = 0 for all adjacent D′ as 0 ≤P(H(D′) ∈S) ≤
eϵP(H(D) ∈S) = 0, and thus the probability is 0 for all countable datasets as we can construct any
dataset by removing and adding a data point (which does not change the probability if it is initially
0) countably many times."
MAIN RESULT,0.21951219512195122,"3.2
MAIN RESULT"
MAIN RESULT,0.22357723577235772,"We now proceed to use Lemma 1 to bound the positive and negative accuracy of MI, as stated in
Deﬁnition 1, for a training function H that is ϵ-DP under the data-sampling scenario deﬁned earlier.
Our approach differs from those we discussed in Section 2.3 in that we now focus on the deﬁnition
of the conditional probability P(x∗∈D|S) as a quotient; ﬁnding a bound then reduces to ﬁnding a
way to simplify the quotient with the ϵ-DP deﬁnition, which we achieve using Lemma 1."
MAIN RESULT,0.22764227642276422,"What follows are the technical results, with Section 4, 5, and 6 discussing the main consequences of
the bound."
MAIN RESULT,0.23170731707317074,"Theorem 1 (DP bounds MI positive accuracy). If f is a MI attack applied to a set of models S and
it predicts if x∗was in the datasets used to obtain them, and the training process H is DP with ϵ, its"
MAIN RESULT,0.23577235772357724,"accuracy is upper-bounded by

1 + e−ϵPx∗(0)"
MAIN RESULT,0.23983739837398374,"Px∗(1)
−1
and lower bounded by

1 + eϵPx∗(0)"
MAIN RESULT,0.24390243902439024,"Px∗(1)
−1
,where"
MAIN RESULT,0.24796747967479674,Px∗(1) is the probability of drawing x∗into the dataset.
MAIN RESULT,0.25203252032520324,See Appendix A for the proof.
MAIN RESULT,0.25609756097560976,By the deﬁnition of negative accuracy of f we have the following corollary:
MAIN RESULT,0.2601626016260163,"Corollary 1 (DP bounds MI negative accuracy). If f is an MI attack applied to a set of models
S and it predicts if x∗is not in it, and the training process H is DP with ϵ, then the accuracy is"
MAIN RESULT,0.26422764227642276,"upper-bounded by

1 + e−ϵPx∗(1)"
MAIN RESULT,0.2682926829268293,"Px∗(0)
−1
and lower-bounded by

1 + eϵPx∗(1)"
MAIN RESULT,0.27235772357723576,"Px∗(0)
−1
, where Px∗(1) is
the probability of drawing x∗into the dataset."
MAIN RESULT,0.2764227642276423,See Appendix A for the proof.
MAIN RESULT,0.2804878048780488,"Note that in the case Px∗(1) = Px∗(0) = 0.5, the bounds given by Theorem 1 and Corollary 1 are
identical. Therefore, as f(x∗, S) must output either 0 or 1, we have a more general claim that the
attack accuracy (maximum of positive or negative accuracy) is always bounded by the same values
given by Theorem 1."
MAIN RESULT,0.2845528455284553,Under review as a conference paper at ICLR 2022
MAIN RESULT,0.2886178861788618,"4
THE EFFECT OF Px∗(1)"
MAIN RESULT,0.2926829268292683,"We now focus on the effect of Px∗(1) which we discuss in two subsections below. The ﬁrst ex-
plains how the privacy ampliﬁcation, i.e. lowering of our MI positive accuracy bound, we observe
from decreasing Px∗(1) is fundamentally different than the privacy ampliﬁcation on MI from batch
sampling. The second subsection outlines the practical consequences of this for a defender."
A NEW PRIVACY AMPLIFICATION FOR MI,0.2967479674796748,"4.1
A NEW PRIVACY AMPLIFICATION FOR MI"
A NEW PRIVACY AMPLIFICATION FOR MI,0.3008130081300813,"Our bound given by Theorem 1 can be reduced by decreasing Px∗(1) or ϵ. Furthermore, we have
batch sampling, which is the probability for a data point to be used in the batch for a given training
step, reduces MI positive accuracy as it reduces ϵ. So dataset sub-sampling (Px∗(1)) and batch
sampling both decrease our bound, and we term their effect ”privacy ampliﬁcation” (for MI) as they
decrease privacy infringement (analogous to how ”privacy ampliﬁcation” for DP refers to methods
that reduce privacy loss). We now ask the question, is the effect of dataset sub-sampling and batch
sampling different?"
A NEW PRIVACY AMPLIFICATION FOR MI,0.3048780487804878,"Before proceeding, it is useful to get a sense of the impact Px∗(1) has on our bound. We plot
Px∗(1) against our positive MI accuracy bound given by Theorem 1 in Figure 3 for different ϵ (see
Appendix B). Notably, for a speciﬁc case when Px∗(1) is small, we get that the positive accuracy is
bounded by 6.9% for ϵ = 2 and Px∗(1) = 0.01 (i.e. 1%)."
A NEW PRIVACY AMPLIFICATION FOR MI,0.3089430894308943,"We now turn to comparing the effect of batch sampling to the effect of Px∗(1) (note
Px∗(0)/Px∗(1) = (1−Px∗(1))/Px∗(1)). First it is worth noting that the two ampliﬁcation methods
are mostly independent, i.e. decreasing Px∗(0)/Px∗(1) mostly places no restriction on the sam-
pling rate for batch sizes (with some exception) 2. Nevertheless we can ignore this restriction
for the time being as we are interested in their independent mathematical behaviours. Including
q for DP batch privacy ampliﬁcation, we can compare its impact to Px∗(1) by looking at the term
e−qϵ0Px(0)/Px(1) in the bound given by Theorem 1; the goal is to maximize this to make the upper
bound as small as possible. In particular, we see that decreasing q increases this term by O(e−t)
where as decreasing Px∗(1) increases this term by O((1 −t)/t), which is slower than O(e−t) up to
a point, then faster (do note that we are looking at the order as the variable t decreases). Figure 1
plots this relation, however note that the speciﬁc values are subject to change with differing con-
stant. Nevertheless what does not change with the constants are the asymptotic behaviours, and in
particular we see limt→0 O((1 −t)/t) = ∞where as limt→0 O(e−t) = constant."
A NEW PRIVACY AMPLIFICATION FOR MI,0.3130081300813008,"Thus we can conclude the effects of data sampling and batch sampling are different to our bound,
and hence data sampling presents a new privacy ampliﬁcation scheme for MI positive accuracy. As
a last remark, note these same comparison holds more generally when comparing the effect of ϵ and
Px∗(1);"
A NEW PRIVACY AMPLIFICATION FOR MI,0.3170731707317073,"0.0
0.2
0.4
0.6
0.8
1.0
t 0 1 2 3 4 5"
A NEW PRIVACY AMPLIFICATION FOR MI,0.32113821138211385,Amplifciation
A NEW PRIVACY AMPLIFICATION FOR MI,0.3252032520325203,Comparison of Privacy Amplification
A NEW PRIVACY AMPLIFICATION FOR MI,0.32926829268292684,"DP Amplification
Our Amplification"
A NEW PRIVACY AMPLIFICATION FOR MI,0.3333333333333333,"Figure 1: Comparing the DP ampliﬁcation observed by decreasing batch probability (given by e−t)
to the ampliﬁcation we observe from decreasing Px(1) (given by (1 −t)/t)"
A NEW PRIVACY AMPLIFICATION FOR MI,0.33739837398373984,"2We say ”mostly” as this is true upto a point. In particular we have the expectation of the training dataset
size decreases with smaller dataset sampling probabilities, and thus the lowest possible batch sampling rate
1/n, where n in the training set size, increases in expectation"
A NEW PRIVACY AMPLIFICATION FOR MI,0.34146341463414637,Under review as a conference paper at ICLR 2022
USEFULNESS FOR A DEFENDER,0.34552845528455284,"4.2
USEFULNESS FOR A DEFENDER"
USEFULNESS FOR A DEFENDER,0.34959349593495936,"We now explain one course of action a defender can take in light of this new privacy ampliﬁcation
for MI. In particular note that an upper bound on Px∗(1) translates to an upper bound on the relation
found in Theorem 1 (as the bound is monotonically increasing with Px∗(1)); hence one can, in
practice, focus on giving smaller upper-bounds on Px∗(1) to decrease MI positive accuracy."
USEFULNESS FOR A DEFENDER,0.35365853658536583,"A possible approach to this is as follows. Say a user (the defender) is given some sample D ⊂
{x1, · · · , xN} drawn with some unknown distribution. In particular they do not know the individual
probabilites for the points being in D. However say that from D they obtain D′ by sampling any
point from D with probability T. Then the probability for any point x∗∈{x1, · · · , xN} being in
D′ is bounded by T as the true probability is the probability x∗∈D (which is ≤1) times T. Hence
if the user trains with D′ they will have Px∗(1) ≤T and thus can use our bound to give guarantees
on how low the MI positive accuracy is."
USEFULNESS FOR A DEFENDER,0.35772357723577236,"This does come with some drawbacks. In general one wants to train with more data, but by further
sampling with probability T we reduce our expected training dataset size. Thus a user will have
to make the decision between how low they can make T (in conjunction with the ϵ parameter they
choose) compared to how small a dataset they are willing to train on. We leave this type of decision
making for future work."
DISCUSSION,0.3617886178861789,"5
DISCUSSION"
OUR BOUND IS TIGHTER THAN EARLIER RESULTS,0.36585365853658536,"5.1
OUR BOUND IS TIGHTER THAN EARLIER RESULTS"
OUR BOUND IS TIGHTER THAN EARLIER RESULTS,0.3699186991869919,"We ﬁrst compare our main technical result, the bound on a MI adversary’s success, with the two
key baselines we identiﬁed earlier (Sablayrolles et al., 2019; Erlingsson et al., 2019). The setup
described in Section 3.1 is equivalent to the MI experiment deﬁned by Yeom et al. (2018) when
Pxi(1) = 0.5 ∀xi ∈{x1, · · · , xN}. That is, if the training dataset was constructed by sampling
data points from a larger dataset by a coin ﬂip, then when the adversary is given any data point
from the larger dataset to test there is a 50% chance it was in the training set or not. Furthermore,
recall that when Px∗(1) = 0.5, then Theorem 1 reduces to bounds on the overall accuracy of any MI
attack f (as the bounds on positive and negative accuracy are the same). We can thus compare the
upper-bound on MI accuracy that we achieved with the current tightest bounds given by Erlingsson
et al. (2019) and Sablayrolles et al. (2019) for ϵ-DP; we stated these bounds earlier in Section 2.3,
where for the latter bound we also set Px∗(1) = 0.5."
OUR BOUND IS TIGHTER THAN EARLIER RESULTS,0.37398373983739835,"Our bound, and these two previous bounds, are depicted in Figure 2, where we see the bound given
in Theorem 1 is always tighter than both of the previous bounds. In particular, we see that it is
closer to the one introduced by Sablayrolles et al. for small ϵ and closer to the one deﬁned by
Erlingsson et al. for large ϵ. Notably, for ϵ = 1, we bound the accuracy of an MI attack by 73.1%
whereas Erlingsson et al. bound it by 81.6%, and Sablayrolles et al. by 75%. For ϵ = 2, we bound
MI accuracy by 88% whereas Erlingsson et al. bound it by 95% and Sablayrolles et al. by 100%."
OUR BOUND IS TIGHTER THAN EARLIER RESULTS,0.3780487804878049,"5.2
ANALYSIS OF 1D LOGISTIC REGRESSION"
OUR BOUND IS TIGHTER THAN EARLIER RESULTS,0.3821138211382114,"We now motivate future work on MI bounds by illustrating how our approach of bounding the
positive accuracy (deﬁned as P(x∗|S)) of a MI adversary for ϵ-DP does not immediately extend to
(ϵ, δ)-DP. Such an extension would be desirable so one can capture the success of MI adversaries
against training algorithms that only provide relaxed guarantees of DP, as opposed to the ε-DP
guarantees we studied. In particular, we give a practical counter-example that shows for a speciﬁc
MI attack on an (ϵ, δ)-DP logistic regression that there is no bound on the positive accuracy of the
adversary, unlike what follows from our bound for ε-DP. Nevertheless, we demonstrate how one can
still tailor bounds on general MI accuracy for this speciﬁc attack and remark how this bound is much
tighter than what our theorem states for any MI adversary in the tighter ϵ-DP conditions."
POSITIVE ACCURACY IS NOT BOUNDED,0.3861788617886179,"5.2.1
POSITIVE ACCURACY IS NOT BOUNDED"
POSITIVE ACCURACY IS NOT BOUNDED,0.3902439024390244,"Consider a set of two (scalar) points {x1, x2} which are drawn into the training set D with Px1(1) =
1 and Px2(1) = 0.5; that is x1 is always in the training set, and x2 has a 50% chance of being in"
POSITIVE ACCURACY IS NOT BOUNDED,0.3943089430894309,Under review as a conference paper at ICLR 2022
POSITIVE ACCURACY IS NOT BOUNDED,0.3983739837398374,"the training set. Let model M be a single dimensional logistic regression without bias deﬁned as
M(x) = wx initialized such that for cross-entropy loss L, ∇L|x1 ≈0 (i.e. set x1 = {(106, 1)}
and the weights w = 106 so that M(x) = 106x and thus the softmax output of the model is
approximately 1 on x1 and thus gradient is approximately 0). Conversely set x2 such that the
gradient on it is less than −1 (i.e., for the above setup set x2 = {(106, 0)})."
POSITIVE ACCURACY IS NOT BOUNDED,0.4024390243902439,"Now, train the model to (1, 10−5)-DP following Abadi et al. (2016) with η = 1, sampling rate of
100%, a maximum gradient norm of 1, for one step. Note these are just parameters which we are
allowed to change under the DP analysis, and we found the noise we would need for (1, 10−5)-DP
is 4.0412. Then consider running a MI attack on the above setup where if for some threshold α the
ﬁnal weights W are s.t if W ≤α one classiﬁes those weights as having come from the dataset with
x2, otherwise not. Do note that here we use W for the ﬁnal weights as opposed to w to emphasize
that we are now talking about a random variable. The intuition for this attack is that if the dataset
only contain x1 then the weights do not change, but if the dataset contains x2 we know the resulting
gradient is negative (by construction) and thus decreases the weights (before noise)."
POSITIVE ACCURACY IS NOT BOUNDED,0.4065040650406504,"By the earlier setup on how training datasets are constructed note that D = {x1} or D = {x1, x2},
and we will denote these D1 and D2 respectively. Note that if M trained on D1 following the sug-
gested data points and initial weights, we have the distribution of ﬁnal weight WD1 = N(106, σ) =
N(106, 4.0412) where σ denotes the noise needed for (1, 10−5)-DP as stated earlier. Similarly
WD2 = N(106 −1, 4.0412), since the maximum gradient norm is set to 1."
POSITIVE ACCURACY IS NOT BOUNDED,0.4105691056910569,For the above MI attack we can then bound the positive accuracy as a function of α by:
POSITIVE ACCURACY IS NOT BOUNDED,0.4146341463414634,"P(D2|W ≤α) =
P(WD2 ≤α) ∗P(D2)
P(WD2 ≤α) ∗P(D2) + P(WD1 ≤α) ∗P(D1)
(2)"
POSITIVE ACCURACY IS NOT BOUNDED,0.4186991869918699,"=
φ(WD2, α) ∗0.5
φ(WD1, α) ∗0.5 + φ(WD2, α) ∗0.5
(3)"
POSITIVE ACCURACY IS NOT BOUNDED,0.42276422764227645,"where φ(W, α) is the (Gaussian) cumulative function of random variable W upto α."
POSITIVE ACCURACY IS NOT BOUNDED,0.4268292682926829,"We plot this in Figure 4a, and unlike Theorem 1, note how it is not bounded by anything less than
1 and goes to 1 as the threshold α decreases (i.e. ∀m ∈[0, 1) ∃α s.t S = (−∞, α] yields positive
accuracy greater than m)."
MI ACCURACY IS BOUNDED,0.43089430894308944,"5.2.2
MI ACCURACY IS BOUNDED"
MI ACCURACY IS BOUNDED,0.4349593495934959,"The previous section showed that for (ϵ, δ)-DP the positive accuracy of our adversary is not bounded.
However, as we will show in this section, this does not mean the overall accuracy is not bounded.
Speciﬁcally, for the same attack and setup as in Section 5.2.1, note that we have a bound on the
general accuracy of this speciﬁc attack given by:
P(D1|W ≥α)∗P(W ≥α)+P(D2|W ≤α)∗P(W ≤α) = (1−φ(WD1, α))∗0.5+φ(WD2, α)∗0.5
(4)"
MI ACCURACY IS BOUNDED,0.43902439024390244,"We illustrate this in Figure 4b and observe that it is bounded by 54.9% for α = 106 −0.5. Do note
that 54.9% is signiﬁcantly less than 73.1% which is what our bound gives for ϵ = 1-DP, and ϵ = 1-
DP is a tighter DP condition than (ϵ = 1, δ = 10−5)-DP which is what is depicted in Figure 4b.
This illustrates how the bound can be further tightened with a better understanding of the (worst
case scenario) weight distribution and the nature of the attack. We leave this to future work."
MI ADVANTAGE,0.44308943089430897,"5.3
MI ADVANTAGE"
MI ADVANTAGE,0.44715447154471544,"Previous work, particularly Yeom et al. (2018) and Erlingsson et al. (2019), focused on membership
advantage, which is essentially an improvement in accuracy of f over the random guess of 50% of
drawing a point in the training dataset. More speciﬁcally, if we let A(f) denote the accuracy of f,
then membership advantage is computed as 2(A(f) −0.5). We can generalize this to ask what the
membership advantage of the positive accuracy of f compared to the baseline Px∗(1) is."
MI ADVANTAGE,0.45121951219512196,"Theorem 1 gives us an upper bound on the positive accuracy and thus an upper bound on the positive
advantage of f denoted as Ad(f):"
MI ADVANTAGE,0.45528455284552843,"Ad(f) = 2 (A(f(x∗, S) = 1) −Px∗(1)) ≤2"
MI ADVANTAGE,0.45934959349593496,"
1 + e−ϵPx∗(0)"
MI ADVANTAGE,0.4634146341463415,Px∗(1)
MI ADVANTAGE,0.46747967479674796,"−1
−Px∗(1) ! (5)"
MI ADVANTAGE,0.4715447154471545,Under review as a conference paper at ICLR 2022
MI ADVANTAGE,0.47560975609756095,"We plotted this advantage as a function of Px∗(1) for different ﬁxed ϵ in Figure 6 (see Appendix B).
We observe that the advantage clearly depends on Px∗(1), and in fact for different ϵ, the Px∗(1)
resulting in the maximum advantage changes. In particular, Px∗(1) = 0.5 is not close to the advan-
tage for large ϵ, which shows how the ﬁxed experiment proposed by Yeom et al. (2018) does not
necessarily give the maximum advantage an adversary could have."
MI ADVANTAGE,0.4796747967479675,"However, it should be noted that higher advantage here does not mean a higher upper bound on
accuracy; as we already saw in Figure 3, the upper bound on accuracy increases monotonically with
Px∗(1), in contrast to the bump observed with membership advantage. This serves to contrast the
study of advantage and the study of accuracy for future work."
IMPORTANCE TO DATA DELETION,0.483739837398374,"6
IMPORTANCE TO DATA DELETION"
IMPORTANCE TO DATA DELETION,0.4878048780487805,"The ability for an entity to decrease the ability for an arbitrator to attribute a data point to a model also
has consequences for machine unlearning and data deletion requests as mentioned in Section 2.4."
IMPORTANCE TO DATA DELETION,0.491869918699187,"In particular if P(x∗∈D|S) is sufﬁciently low, that is the likelihood of S coming from x∗is low,
then an entity could claim that they do not need to delete the users data since their model is most
likely independent of that data point as it most likely came from a model without it: i.e. leading
to plausible deniability. Note we deﬁned this type of unlearning in Section 2.4 as a B-MI unlearn-
ing guarantee. This is similar to the logic presented by Sekhari et al. (2021) where unlearning is
presented probablistically in terms of (ϵ, δ)-unlearning."
IMPORTANCE TO DATA DELETION,0.4959349593495935,"We also observe an analogous result to Sekhari et al. (2021) where we can only handle a maximum
number of deletion requests before no longer having sufﬁciently low probability. To be exact, let
us say we do not need to undergo any unlearning process given a set of data deletion request ˆx∗if
P(ˆx∗/∈D|S) ≥B for some B (i.e. we are working with probability of not having that set of data
in our training set which we want to be high). Note that we sampled data independently, thus if
ˆx∗= {x∗
1, x∗
2, · · · , x∗
m}, then P(ˆx∗/∈D|S) = P(x∗
1 /∈D|S) · · · P(x∗
m /∈D|S)."
IMPORTANCE TO DATA DELETION,0.5,"Now, for simplicity, assume the probability of drawing all points into the datasets are the same, so
that for all x∗
i we have the same bound given by Corollary 1, that is P(x∗
i /∈D|S) > L for some
L ≤1. Then we have P(ˆx∗/∈D|S) ≥Lm and so an entity does not need to unlearn if Lm ≥B,
i.e. if m ≤ln 1/B"
IMPORTANCE TO DATA DELETION,0.5040650406504065,ln 1/L = ln B
IMPORTANCE TO DATA DELETION,0.508130081300813,"ln L . This gives a bound on how many deletion requests the entity can avoid in
terms of the lower bound given in Corollary 1."
IMPORTANCE TO DATA DELETION,0.5121951219512195,"In particular, note that if {x1 · · · xN} is the larger set of data points an entity is sampling from, and"
IMPORTANCE TO DATA DELETION,0.516260162601626,"Px(1) = c/N ∀x ∈{x1 · · · xN}, then the lower bound given by Corollary 1 is

1 + e−ϵ c N
1−c N"
IMPORTANCE TO DATA DELETION,0.5203252032520326,"−1
.
Sekhari et al. (2021) showed that with typical DP the deletion requests grow linearly with the size of
the training (in the above case c represents the expected training set size). We thus compare a linear
line w.r.t to c to
ln B
ln L(c) (where L is given in the earlier expression for the bound from Corollary 1)
in Figure 7 (see Appendix B) to observe their respective magnitude: we ﬁx B = 0.8,N = 10000 and
ϵ = 1 as we are interested in general trends. We observe that our deletion capacity is signiﬁcantly
higher for low expected training set sizes and is marginally lower than a linear trend for larger
training set sizes."
CONCLUSION,0.524390243902439,"7
CONCLUSION"
CONCLUSION,0.5284552845528455,"In this work, we provide a tighter bound on MI accuracy against ML models trained with ϵ-DP. Our
bound highlights that intricacies of dataset construction are of great importance for model vulnera-
bility to MI attacks. Indeed, based on our ﬁndings, we develop a privacy ampliﬁcation scheme that
just requires one to sub-sample their training dataset from larger pool of possible data points."
CONCLUSION,0.532520325203252,"Based on our results, entities training their ML models with DP can employ looser privacy guar-
antees (and thereby preserve their models’ accuracy better) while still limiting the success of MI
attacks. Finally, our bound, and more generally bounds on positive MI accuracy, can also be applied
to handle unlearning requests when doing machine unlearning if unlearning is deﬁned by achieving
a model with low probability of having come that data point."
CONCLUSION,0.5365853658536586,Under review as a conference paper at ICLR 2022
REFERENCES,0.540650406504065,REFERENCES
REFERENCES,0.5447154471544715,"Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and
Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC
conference on computer and communications security, pp. 308–318, 2016."
REFERENCES,0.5487804878048781,"Borja Balle and Yu-Xiang Wang. Improving the gaussian mechanism for differential privacy: Ana-
lytical calibration and optimal denoising. In International Conference on Machine Learning, pp.
394–403. PMLR, 2018."
REFERENCES,0.5528455284552846,"Thomas Baumhauer, Pascal Sch¨ottle, and Matthias Zeppelzauer. Machine unlearning: Linear ﬁltra-
tion for logit-based classiﬁers. arXiv preprint arXiv:2002.02730, 2020."
REFERENCES,0.556910569105691,"Lucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin
Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. Machine unlearning. arXiv preprint
arXiv:1912.03817, 2019."
REFERENCES,0.5609756097560976,"Yinzhi Cao and Junfeng Yang. Towards making systems forget with machine unlearning. In 2015
IEEE Symposium on Security and Privacy, pp. 463–480. IEEE, 2015."
REFERENCES,0.5650406504065041,"Christopher A Choquette-Choo, Florian Tramer, Nicholas Carlini, and Nicolas Papernot. Label-only
membership inference attacks. In International Conference on Machine Learning, pp. 1964–1974.
PMLR, 2021."
REFERENCES,0.5691056910569106,"Cynthia Dwork. Differential privacy. In International Colloquium on Automata, Languages, and
Programming, pp. 1–12. Springer, 2006."
REFERENCES,0.573170731707317,"Cynthia Dwork. Differential privacy: A survey of results. In International conference on theory and
applications of models of computation, pp. 1–19. Springer, 2008."
REFERENCES,0.5772357723577236,"Cynthia Dwork, Guy N Rothblum, and Salil Vadhan. Boosting and differential privacy. In 2010
IEEE 51st Annual Symposium on Foundations of Computer Science, pp. 51–60. IEEE, 2010."
REFERENCES,0.5813008130081301,"´Ulfar Erlingsson, Ilya Mironov, Ananth Raghunathan, and Shuang Song. That which we call private.
arXiv preprint arXiv:1908.03566, 2019."
REFERENCES,0.5853658536585366,"Antonio Ginart, Melody Y Guan, Gregory Valiant, and James Zou. Making ai forget you: Data
deletion in machine learning. arXiv preprint arXiv:1907.05012, 2019."
REFERENCES,0.5894308943089431,"Aditya Golatkar, Alessandro Achille, and Stefano Soatto.
Eternal sunshine of the spotless net:
Selective forgetting in deep networks. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 9304–9312, 2020a."
REFERENCES,0.5934959349593496,"Aditya Golatkar, Alessandro Achille, and Stefano Soatto. Forgetting outside the box: Scrubbing
deep networks of information accessible from input-output observations. In European Conference
on Computer Vision, pp. 383–398. Springer, 2020b."
REFERENCES,0.5975609756097561,"Laura Graves, Vineel Nagisetty, and Vijay Ganesh. Amnesiac machine learning. arXiv preprint
arXiv:2010.10981, 2020."
REFERENCES,0.6016260162601627,"Rob Hall, Alessandro Rinaldo, and Larry Wasserman. Differential privacy for functions and func-
tional data. The Journal of Machine Learning Research, 14(1):703–727, 2013."
REFERENCES,0.6056910569105691,"Hongsheng Hu, Zoran Salcic, Gillian Dobbie, and Xuyun Zhang. Membership inference attacks on
machine learning: A survey. arXiv preprint arXiv:2103.07853, 2021."
REFERENCES,0.6097560975609756,"Matthew Jagielski, Jonathan Ullman, and Alina Oprea.
Auditing differentially private machine
learning: How private is private sgd? arXiv preprint arXiv:2006.07709, 2020."
REFERENCES,0.6138211382113821,"Bargav Jayaraman, Lingxiao Wang, Katherine Knipmeyer, Quanquan Gu, and David Evans. Revis-
iting membership inference under realistic assumptions. arXiv preprint arXiv:2005.10881, 2020."
REFERENCES,0.6178861788617886,"Peter Kairouz, Sewoong Oh, and Pramod Viswanath. The composition theorem for differential
privacy. In International conference on machine learning, pp. 1376–1385. PMLR, 2015."
REFERENCES,0.6219512195121951,Under review as a conference paper at ICLR 2022
REFERENCES,0.6260162601626016,"Pratyush Maini, Mohammad Yaghini, and Nicolas Papernot. Dataset inference: Ownership resolu-
tion in machine learning. arXiv preprint arXiv:2104.10706, 2021."
REFERENCES,0.6300813008130082,"Milad Nasr, Shuang Song, Abhradeep Thakurta, Nicolas Papernot, and Nicholas Carlini. Adver-
sary instantiation: Lower bounds for differentially private machine learning.
arXiv preprint
arXiv:2101.04535, 2021."
REFERENCES,0.6341463414634146,"Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, Yann Ollivier, and Herv´e J´egou. White-
box vs black-box: Bayes optimal strategies for membership inference. In International Confer-
ence on Machine Learning, pp. 5558–5567. PMLR, 2019."
REFERENCES,0.6382113821138211,"Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz, and Michael Backes.
Ml-leaks: Model and data independent membership inference attacks and defenses on machine
learning models. arXiv preprint arXiv:1806.01246, 2018."
REFERENCES,0.6422764227642277,"Ayush Sekhari, Jayadev Acharya, Gautam Kamath, and Ananda Theertha Suresh. Remember what
you want to forget: Algorithms for machine unlearning. arXiv preprint arXiv:2103.03279, 2021."
REFERENCES,0.6463414634146342,"Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference at-
tacks against machine learning models. In 2017 IEEE Symposium on Security and Privacy (SP),
pp. 3–18. IEEE, 2017."
REFERENCES,0.6504065040650406,"Stacey Truex, Ling Liu, Mehmet Emre Gursoy, Lei Yu, and Wenqi Wei. Demystifying membership
inference attacks in machine learning as a service. IEEE Transactions on Services Computing,
2019."
REFERENCES,0.6544715447154471,"Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. Privacy risk in machine learn-
ing: Analyzing the connection to overﬁtting. In 2018 IEEE 31st Computer Security Foundations
Symposium (CSF), pp. 268–282. IEEE, 2018."
REFERENCES,0.6585365853658537,"A
PROOFS"
REFERENCES,0.6626016260162602,"Lemma 1 Note that for a given D = {x1, · · · , xn} ∈Dx∗, D′ = D/x∗∈Dx∗′ is unique (i.e. the
map by removing x∗is injective) and similarly for a given D′ ∈Dx∗′ D = D′ ∪x∗is unique
(i.e. the map by adding x∗is injective). Thus, we have injective maps running both ways which are
the inverses of each other. As a consequence, we have Dx∗and Dx∗′ are in bijective correspondence."
REFERENCES,0.6666666666666666,"Now if the larger set of datapoints is {x1 · · · xn−1, x∗, xn · · · xN} letting D = {x1 · · · xn−1} ∪x∗
and D′ = {x1 · · · xn−1} be any pair of datasets that map to each other by the above bijective
map, then note P(D) = Px1(1)Px2(1) · · · Pxn−1(1)Px∗(1) · · · Pxn+1(0) · · · PxN (0) and P(D′) =
Px1(1)Px2(1) · · · Pxn−1(1)Px∗(0) · · · Pxn+1(0) · · · PxN (0). In particular we have P(D) Px∗(0)"
REFERENCES,0.6707317073170732,"Px∗(1) =
P(D′)."
REFERENCES,0.6747967479674797,Theorem 1 The positive accuracy of f is:
REFERENCES,0.6788617886178862,"A(f(x∗, S) = 1) = P(x∗|S) = P"
REFERENCES,0.6829268292682927,"D∈Dx∗P(H(D) ∈S)P(D)
P"
REFERENCES,0.6869918699186992,"D∈D P(H(D) ∈S)P(D)
(6)"
REFERENCES,0.6910569105691057,"By the observation D
=
Dx∗∪Dx∗′ we have that the denominator can be split into
P"
REFERENCES,0.6951219512195121,D∈Dx∗P(H(D) ∈S)P(D) + P
REFERENCES,0.6991869918699187,"D′∈D′
x∗P(H(D′) ∈S)P(D′)."
REFERENCES,0.7032520325203252,"By Lemma 1, we can replace the D′ ∈D′
x∗in the second sum by D ∈Dx∗and replace
P(D′) by P(D) Px∗(0)"
REFERENCES,0.7073170731707317,"Px∗(1). For P(H(D′) ∈S) note by H being ϵ-DP we have P(H(D′) ∈S) ≥
e−ϵP(H(D) ∈S) and so with the previous replacements we have that the denominator is greater
than (1 + e−ϵPx∗(0)"
REFERENCES,0.7113821138211383,Px∗(1) ) · P
REFERENCES,0.7154471544715447,D∈Dx∗P(H(D) ∈S)P(D).
REFERENCES,0.7195121951219512,"Thus, the accuracy of f is ≤
1"
REFERENCES,0.7235772357723578,"1+
e−ϵPx∗(0)"
REFERENCES,0.7276422764227642,"Px∗(1)
(i.e. the upper bound)."
REFERENCES,0.7317073170731707,Under review as a conference paper at ICLR 2022
REFERENCES,0.7357723577235772,"0
1
2
3
4
5
Epsilon 0.5 0.6 0.7 0.8 0.9 1.0"
REFERENCES,0.7398373983739838,Accuracy Bound
REFERENCES,0.7439024390243902,Different MI Bounds
REFERENCES,0.7479674796747967,"Our Bound
Erligsson Bound
Sablayrolles Bound"
REFERENCES,0.7520325203252033,"Figure 2: Comparing the upper bound to MI performance we achieved to that given by Erlingsson
et al. (2019) and Sablayrolles et al. (2019) (note Px∗(1) = 0.5 here). In particular note we are tighter
for all ϵ."
REFERENCES,0.7560975609756098,"If instead we used the fact that P(H(D′) ∈S) ≤eϵP(H(D) ∈S), we would ﬁnd that the accuracy
of f is ≥
1
1+
eϵPx∗(0)"
REFERENCES,0.7601626016260162,"Px∗(1)
(i.e. the lower bound)."
REFERENCES,0.7642276422764228,"Corollary 1 Immediately follows from Theorem 1 and Deﬁntion 1, as if A(f(x∗, S) = 1) ≤
1
1+
eϵPx∗(0)"
REFERENCES,0.7682926829268293,"Px∗(1)
then A(f(x∗, S) = 0) = 1 −A(f(x∗, S) = 1) ≤1 −
1
1+
eϵPx∗(0)"
REFERENCES,0.7723577235772358,"Px∗(1)
=
1"
REFERENCES,0.7764227642276422,"1+
e−ϵPx∗(1)"
REFERENCES,0.7804878048780488,"Px∗(0)
."
REFERENCES,0.7845528455284553,"Similarly, we get A(f(x∗, S) = 0) = 1 −A(f(x∗, S) = 1) ≥1 −
1"
REFERENCES,0.7886178861788617,"1+
e−ϵPx∗(0)"
REFERENCES,0.7926829268292683,"Px∗(1)
=
1
1+
eϵPx∗(1)"
REFERENCES,0.7967479674796748,Px∗(0)
REFERENCES,0.8008130081300813,"B
FIGURES"
REFERENCES,0.8048780487804879,"0.0
0.2
0.4
0.6
0.8"
REFERENCES,0.8089430894308943,Probability of drawing x * 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.8130081300813008,Positive Accuracy Bound
REFERENCES,0.8170731707317073,Positive Accuracy Bound vs Probability
REFERENCES,0.8211382113821138,"Pos Acc Bound:  = 0.1
Pos Acc Bound:  = 1
Pos Acc Bound:  = 2
Pos Acc Bound:  = 5"
REFERENCES,0.8252032520325203,Figure 3: Our upper bound on MI positive accuracy as a function of Px∗(1)
REFERENCES,0.8292682926829268,"C
TABLES"
REFERENCES,0.8333333333333334,"Paper
Analytic Form
Type
Yeom et al. (2018)
eϵ/2
General
Erlingsson et al. (2019)
1 −e−ϵ/2
General
Sablayrolles et al. (2019)
Px∗(1) + ϵ/4
Positive Accuracy"
REFERENCES,0.8373983739837398,Our Work
REFERENCES,0.8414634146341463,"
1 + e−ϵPx∗(0)"
REFERENCES,0.8455284552845529,"Px∗(1)
−1"
REFERENCES,0.8495934959349594,Positive Accuracy
REFERENCES,0.8536585365853658,Table 1: Bounds found in prior work.
REFERENCES,0.8577235772357723,Under review as a conference paper at ICLR 2022
REFERENCES,0.8617886178861789,"100
75
50
25
0
25
50
75
100
Threshold
+1e6 0.5 0.6 0.7 0.8 0.9 1.0"
REFERENCES,0.8658536585365854,Positive Accuracy
REFERENCES,0.8699186991869918,Positive Accuracy vs Threshold
REFERENCES,0.8739837398373984,(a) Positive accuracy as a function of the threshold
REFERENCES,0.8780487804878049,"10.0
7.5
5.0
2.5
0.0
2.5
5.0
7.5
10.0
Threshold
+1e6 0.50 0.51 0.52 0.53 0.54 0.55"
REFERENCES,0.8821138211382114,Accuracy
REFERENCES,0.8861788617886179,Accuracy vs Threshold
REFERENCES,0.8902439024390244,(b) Accuracy as a function of the threshold
REFERENCES,0.8943089430894309,Figure 4: Impact of threshold on positive accuracy and accuracy.
REFERENCES,0.8983739837398373,"0.0
0.2
0.4
0.6
0.8
1.0"
REFERENCES,0.9024390243902439,Probability of drawing x * 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.9065040650406504,Sablayrolles Pos Accuracy Bound
REFERENCES,0.9105691056910569,Sablayrolles Pos Accuracy Bound vs Probability
REFERENCES,0.9146341463414634,Accuracy Bound:  = 0.1
REFERENCES,0.9186991869918699,Our Bound:  = 0.1
REFERENCES,0.9227642276422764,Accuracy Bound:  = 1.0
REFERENCES,0.926829268292683,Our Bound:  = 1.0
REFERENCES,0.9308943089430894,Accuracy Bound:  = 2.0
REFERENCES,0.9349593495934959,Our Bound:  = 2.0
REFERENCES,0.9390243902439024,"Figure 5: Sablayrolles et al. (2019) upper bound on MI positive accuracy as a function of Px∗(1)
compared to our bound. Note that we are still tighter for all probabilities."
REFERENCES,0.943089430894309,"0.0
0.2
0.4
0.6
0.8
Probability of drawing x* 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75"
REFERENCES,0.9471544715447154,Advantage Bound
REFERENCES,0.9512195121951219,MI Advantage vs Probability
REFERENCES,0.9552845528455285,MI Advantage:  = 0.1
REFERENCES,0.959349593495935,MI Advantage:  = 1
REFERENCES,0.9634146341463414,MI Advantage:  = 3
REFERENCES,0.967479674796748,MI Advantage:  = 5
REFERENCES,0.9715447154471545,Figure 6: MI advantage
REFERENCES,0.975609756097561,Under review as a conference paper at ICLR 2022
REFERENCES,0.9796747967479674,"0
25
50
75
100
125
150
175
200
Expected Training Set Size 0 2000 4000 6000 8000 10000 12000"
REFERENCES,0.983739837398374,Deletion Capacity
REFERENCES,0.9878048780487805,Delection Capacity vs. Expected Training Set Size
REFERENCES,0.991869918699187,"Our Trend
Sekhari Trend"
REFERENCES,0.9959349593495935,"Figure 7: Comparing our deletion capacity trend to the trend Sekhari et al. (2021) describes. In
particular our number of deletions degrades with training size while theirs increasing."
