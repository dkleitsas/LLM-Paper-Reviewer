Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002638522427440633,"Federated learning (FL) allows multiple clients with (private) data to collabora-
tively train a common machine learning model without sharing their private train-
ing data. In-the-wild deployment of FL faces two major hurdles: robustness to
poisoning attacks and communication efﬁciency. To address these concurrently,
we propose Federated Supermask Learning (FSL). FSL server trains a global sub-
network within a randomly initialized neural network by aggregating local subnet-
works of all collaborating clients. FSL clients share local subnetworks in the form
of rankings of network edges; more useful edges have higher ranks. By shar-
ing integer rankings, instead of ﬂoat weights, FSL restricts the space available to
craft effective poisoning updates, and by sharing subnetworks, FSL reduces the
communication cost of training. We show theoretically and empirically that FSL
is robust by design and also signiﬁcantly communication efﬁcient; all this with-
out compromising clients’ privacy. Our experiments demonstrate the superiority
of FSL in real-world FL settings; in particular, (1) FSL achieves similar perfor-
mances as state-of-the-art FedAvg with signiﬁcantly lower communication costs:
for CIFAR10, FSL achieves same performance as Federated Averaging while re-
ducing communication cost by ∼35%. (2) FSL is substantially more robust to
poisoning attacks than state-of-the-art robust aggregation algorithms."
INTRODUCTION,0.005277044854881266,"1
INTRODUCTION"
INTRODUCTION,0.0079155672823219,"Federated Learning (FL) is an emerging AI technology, where mutually untrusted clients (e.g., An-
droid devices) collaborate to train a shared model, called the global model, without explicitly sharing
their local training data. FL training involves a server (e.g., Google server) which collects model
updates from selected FL clients in each round of training, and uses them to update the global model.
FL, although highly promising, faces multiple challenges (Kairouz et al., 2019; Li et al., 2020b) to
its practical deployment, in particular, communication efﬁciency and robustness, which are the focus
of our work. Privacy preservation is another major challenge to FL, but is orthogonal to our work."
INTRODUCTION,0.010554089709762533,"We present Federated Supermask Learning (FSL), a novel approach to perform FL while concur-
rently achieving the two goals of robustness and communication efﬁciency. FSL is built on a novel
learning paradigm called supermasks (Zhou et al., 2019; Ramanujan et al., 2020), which allows it to
reduce communication costs while achieving signiﬁcantly higher robustness. Speciﬁcally, in FSL,
clients collaborate to ﬁnd a subnetwork within a randomly initialized neural network which we call
the supernetwork (this is in contrast to conventional FL where clients collaborate to train a neural
network). The goal of training in FSL is to collaboratively identify a supermask, which is a binary
mask of 1’s and 0’s, that is superimposed on the random neural network (the supernetwork) to obtain
the ﬁnal subnetwork. The subnetwork is then used for downstream tasks, e.g., image classiﬁcation,
hence it is equivalent to the global model in conventional FL. Note that in entire FSL training,
weights of the supernetwork do not change."
INTRODUCTION,0.013192612137203167,"More speciﬁcally, each FSL client computes the importance of the edges of the supernetwork based
on their local data. The importance of the edges is represented as a ranking vector. Each FSL
client will use the edge popup algorithm (Ramanujan et al., 2020) and their data to compute their
local rankings (the edge popup algorithm aims at learning which edges in a supernetwork are more
important over the other edges by minimizing the loss of the subnetwork on their local data). Each
client then will send their local edge ranking to the server. Finally, the FSL server uses a novel
voting mechanism to aggregate client rankings into a supermask, which represents which edges of
the random neural network (the supernetwork) will form the global subnetwork."
INTRODUCTION,0.0158311345646438,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.018469656992084433,"Intuitions on FSL’s robustness: In traditional FL algorithms, clients send large-dimension model
updates ∈Rd (real numbers) to the server, providing malicious clients a signiﬁcant ﬂexibility in
fabricating malicious updates. By contrast, FSL clients merely share the rankings of the edges of
the supernetwork, i.e., integers ∈[1, d], where d is the size of the supernetwork. Therefore, as we
will show both theoretically and empirically, FSL provides robustness by design and reduces the
impact of untargeted poisoning attacks. Furthermore, unlike most existing robust FL frameworks,
FSL does not require any knowledge about the percentages of malicious clients."
INTRODUCTION,0.021108179419525065,"Intuitions on FSL’s communication efﬁciency:
In FSL, the clients and the server communicate
just the rankings of the edges in the supernetwork, i.e., a permutation of indices in [1, d]. Ranking
vectors are generally signiﬁcantly smaller than the global model. This, as we will show, signiﬁ-
cantly reduces the upload and download communication in FSL compared to Federated Averaging
(FedAvg) (McMahan et al., 2017), where clients communicate model parameters, each of 32/64 bits."
INTRODUCTION,0.023746701846965697,"Empirical results: We experiment with three datasets in real-world heterogeneous FL settings and
show that: (1) FSL achieves similar performance (e.g., model accuracy) as state-of-the-art FedAvg
but with signiﬁcantly reduced communication costs: for CIFAR10, the accuracy and communication
cost per client are 85.4% and 40.2MB for FedAvg, while 85.3% and 26.2MB for FSL. (2) FSL is
highly robust to poisoning attacks as compared to state-of-the-art robust aggregation algorithms:
from 85.4% in the benign setting, 10% malicious clients reduce the accuracy of FL to 56.3% and
58.8% with Trimmed-Mean (Xie et al., 2018; Yin et al., 2018) and Multi-Krum (Blanchard et al.,
2017), respectively, while FSL’s performance only decreases to 79.0%."
INTRODUCTION,0.026385224274406333,"We also compare FSL with two communication reduction methods, SignSGD (Bernstein et al.,
2019) and TopK (Alistarh et al., 2018a) and show that FSL produces comparable communication
costs and model accuracies. For instance, on CIFAR10, FSL, SignSGD, and TopK achieve 85.3%,
79.1%, and 82.1% test accuracy, respectively, when the corresponding communication costs (down-
load and upload) are 26.2MB, 20.73MB, and 30.79MB. On the other hand, FSL offers a signiﬁcantly
superior robustness. For instance, on CIFAR10, 10% (20%) malicious clients reduce the accuracy
of SignSGD to 39.7% (10.0%), but FSL’s accuracy decreases to only 79.0% (69.5%). TopK is in-
compatible with existing robust aggregation algorithms, hence uses Average aggregation and is as
vulnerable as FedAvg, especially in the real-world heterogeneous settings."
RELATED WORKS,0.029023746701846966,"2
RELATED WORKS"
RELATED WORKS,0.0316622691292876,"Supermask Learning:
Modern neural networks have a very large number of parameters. These
networks are generally overparameterized (Dauphin & Bengio, 2013; Denil et al., 2013), i.e., they
have more parameters than they need to perform a particular task, e.g., classiﬁcation. The lottery
ticket hypothesis (Frankle & Carbin, 2019) states that a fully-trained neural network, i.e., supernet-
work, contains sparse subnetworks, i.e., subsets of all neurons in supernetwork, which can be trained
from scratch (i.e., by training same initialized weights of the subnetwork) and achieve performances
close to the fully trained supernetwork. The lottery ticket hypothesis allows for massive reductions
in the sizes of neural networks. (Ramanujan et al., 2020) offer a complementary conjecture that an
overparameterized neural network with randomly initialized weights contains subnetworks which
perform as good as the fully trained network."
RELATED WORKS,0.03430079155672823,"Poisoning Attacks and Defenses for Federated Learning (FL): FL involves mutually untrusting
clients. Hence, a poisoning adversary may own or compromise some of the FL clients, called
malicious clients, with the goal of mounting a targeted or untargeted poisoning attack. In a targeted
attack, the goal is to reduce the utility of the model on speciﬁc test inputs, while in the untargeted
attack, the goal is to reduce the utility for all (or most) test inputs. It is shown (Blanchard et al.,
2017) that even a single malicious client can mount an effective untargeted attack on FedAvg."
RELATED WORKS,0.036939313984168866,"In order to make FL robust to the presence of such malicious clients, the literature has designed
various robust aggregation rules (AGR) (Blanchard et al., 2017; Mhamdi et al., 2018; Yin et al.,
2018; Chang et al., 2019), which aim to remove or attenuate the updates that are more likely to be
malicious according to some criterion. For instance, Multi-krum (Blanchard et al., 2017) repeatedly
removes updates that are far from the geometric median of all the updates, and Trimmed-mean (Xie
et al., 2018; Yin et al., 2018) removes the largest and smallest values of each update dimension and
calculates the mean of the remaining values. Unfortunately, these robust AGRs are not very effective
in non-convex FL settings and multiple works have demonstrated strong targeted (Wang et al., 2020;"
RELATED WORKS,0.0395778364116095,Under review as a conference paper at ICLR 2022
RELATED WORKS,0.04221635883905013,"Bhagoji et al., 2019) and untargeted attacks (Shejwalkar & Houmansadr, 2021; Fang et al., 2020) on
them."
RELATED WORKS,0.044854881266490766,"Communication Cost of FL: In many real-world applications of FL, it is essential to minimize
the communication between FL server and clients. Especially in cross-device FL, the clients (e.g.,
mobile phones and wearable devices) have limited resources and communication can be a major
bottleneck. There are two major types of communication reduction methods: (1) Qunatization
methods reduce the resolution of (i.e., number of bits used to represent) each dimension of a client
update. For instance, SignSGD (Bernstein et al., 2019) uses the sign (1 bit) of each dimension of
model updates. (2) Sparsiﬁcation methods propose to use only a subset of all the update dimensions.
For instance, in TopK (Aji & Heaﬁeld, 2017; Alistarh et al., 2018a), only the largest K% update
dimensions are sent to the server in each FL round. We note that, communication reduction methods
primarily focus on and succeed at reducing upload communication (client →server), but they use
the entire model in download communication (server →client)."
PRELIMINARIES,0.047493403693931395,"3
PRELIMINARIES"
FEDERATED LEARNING,0.05013192612137203,"3.1
FEDERATED LEARNING"
FEDERATED LEARNING,0.052770448548812667,"In FL (McMahan et al., 2017; Kairouz et al., 2019; Koneˇcn`y et al., 2016), N clients collaborate to
train a global model without directly sharing their data. In round t, the service provider (server)
selects n out of N total clients and sends them the most recent global model θt. Each client trains
a local model for E local epochs on their data starting from the θt using stochastic gradient descent
(SGD). Then the client send back the calculated gradients (▽k for kth client) to the server. The
server then aggregates the collected gradients and updates the global model for the next round. FL
can be either cross-device or cross-silo (Kairouz et al., 2019). In cross-device FL, N is large (from
few thousands to billions) and only a small fraction of clients is chosen in each FL training round,
i.e., n ≪N. By contrast, in cross-silo FL, N is moderate (up to 100) and all clients are chosen in
each round, i.e., n = N. In this work, we evaluate the performance of FSL and other FL baselines
for cross-device FL under realistic production FL settings."
EDGE-POPUP ALGORITHM,0.055408970976253295,"3.2
EDGE-POPUP ALGORITHM"
EDGE-POPUP ALGORITHM,0.05804749340369393,"Algorithm 1 Edge-popup (EP) algorithm: it ﬁnds
a subnetwork of size k% of the entire network θ"
EDGE-POPUP ALGORITHM,0.06068601583113457,"1: Input: number of local epochs E, training
data D, initial weights θw and scores θs, sub-
network size k%, learning rate η
2: for e ∈[E] do
3:
B ←Split D in B batches
4:
for batch b ∈[B] do
5:
EP FORWARD (θw, θs, k, b)
6:
θs = θs −η∇ℓ(θs; b)
7:
end for
8: end for
9: return θs"
EDGE-POPUP ALGORITHM,0.0633245382585752,"10: function EP FORWARD(θw, θs, k, b)
11:
m ←sort(θs)
12:
t ←int((1 −k) ∗len(m))
13:
m[: t] = 0
14:
m[t :] = 1
15:
θp = θw ⊙m
16:
return θp(b)
17: end function"
EDGE-POPUP ALGORITHM,0.06596306068601583,"The edge-popup (EP) algorithm (Ramanujan
et al., 2020) is a novel optimization method to
ﬁnd supermasks within a large, randomly ini-
tialized neural network, i.e., a supernetwork,
with performances close to the fully trained su-
pernetwork. EP algorithm does not train the
weights of the network, instead only decides
the set of edges to keep and removes the rest
of the edges (i.e., pop).
Speciﬁcally, EP al-
gorithm assigns a positive score to each of the
edges in the supernetwork. On forward pass, it
selects top k% edges with highest scores, where
k is the percentage of the total number of edges
in the supernetwork that will remain in the ﬁ-
nal subnetwork. On the backward pass, it up-
dates the scores with the straight-through gradi-
ent estimator (Bengio et al., 2013). Algorithm 1
presents EP algorithm; we defer further details
to Appendix D."
EDGE-POPUP ALGORITHM,0.06860158311345646,"4
FEDERATED SUPERMASK LEARNING: DESIGN"
EDGE-POPUP ALGORITHM,0.0712401055408971,"In this section, we provide the design of our federated supermask learning (FSL) algorithm. FSL
clients collaborate (without sharing their local data) to ﬁnd a subnetwork within a randomly ini-
tialized, untrained neural network called the supernetwork. Algorithm 2 describes FSL’s training.
Training a global model in FSL means to ﬁrst ﬁnd a unanimous ranking of supernetwork edges and
then use the subnetwork of the top ranked edges as the ﬁnal output. We detail a round of FSL train-
ing and depict it in Figure 1, where we use a supernetwork with six edges ei∈[0,5] to demonstrate a"
EDGE-POPUP ALGORITHM,0.07387862796833773,Under review as a conference paper at ICLR 2022
EDGE-POPUP ALGORITHM,0.07651715039577836,"single FSL round and consider three clients Cj∈[1,3] who aim to ﬁnd a subnetwork of size k=50%
of the original supernetwork."
EDGE-POPUP ALGORITHM,0.079155672823219,Algorithm 2 Federated Supermask Learning (FSL)
EDGE-POPUP ALGORITHM,0.08179419525065963,"1: Input: number of FL rounds T, number of local epochs E, number of selected users in each
round n, seed SEED, learning rate η, subnetwork size k%
2:
Server: Initialization
3: θs, θw ←Initialize random scores and weights of global model θ using SEED
4: R1
g ←ARGSORT(θs)
▷Sort the initial scores and obtain initial rankings
5: for t ∈[1, T] do
6:
U ←set of n randomly selected clients out of N total clients
7:
for u in U do
8:
Clients: Calculating the ranks
9:
θs, θw ←Initialize scores and weights using SEED
10:
θs[Rt
g] ←SORT(θs)
▷sort the scores based on the global ranking
11:
S ←Edge-PopUp(E, Dtr
u , θw, θs, k, η)
▷Client u uses Algorithm1 to train a
supermask on its local training data
12:
Rt
u ←ARGSORT(S)
▷Ranking of the client
13:
end for
14:
Server: Majority Vote"
EDGE-POPUP ALGORITHM,0.08443271767810026,"15:
Rt+1
g
←VOTE(Rt
u∈U)
▷Majority vote aggregation
16: end for
17: function VOTE(R{u∈U} ):
18:
V ←ARGSORT(R{u∈U})
19:
A ←SUM(V )
20:
return ARGSORT(A)
21: end function"
EDGE-POPUP ALGORITHM,0.0870712401055409,"4.1
SERVER: INITIALIZATION PHASE (ONLY FOR ROUND t = 1)"
EDGE-POPUP ALGORITHM,0.08970976253298153,"In the ﬁrst round, the FSL server chooses a random seed SEED to generate initial random weights
θw and scores θs for the global supernetwork θ; note that, θw, θs, and SEED remain constant during
the entire FSL training. Next, the FSL server shares SEED with FSL clients, who can then locally
reconstruct the initial weights θw and scores θs using SEED. Figure 1- 1 depicts this step."
EDGE-POPUP ALGORITHM,0.09234828496042216,"Recall that, the goal of FSL training is to ﬁnd the most important edges in θw without changing the
weights. Unless speciﬁed otherwise, both server and clients use the Singed Kaiming Constant algo-
rithm (Ramanujan et al., 2020) to generate random weights and the Kaiming Uniform algorithm (He
et al., 2015) to generate random scores. However, in Appendix C.1, we also explore the impacts
of different initialization algorithms on the performance of FSL. We use the same seed to initialize
weights and scores."
EDGE-POPUP ALGORITHM,0.09498680738786279,"At the beginning, the FSL server ﬁnds the global rankings of the initial random scores (Algorithm 2
line 4), i.e., R1
g = ARGSORT(θs). We deﬁne rankings of a vector as the indices of elements of vector
when the vector is sorted from low to high, which is computed using ARGSORT function (argsort)."
EDGE-POPUP ALGORITHM,0.09762532981530343,"4.2
CLIENTS: CALCULATING THE RANKS (FOR EACH ROUND t)"
EDGE-POPUP ALGORITHM,0.10026385224274406,"In the tth round, FSL server randomly selects n clients among total N clients, and shares the global
rankings Rt
g with them. Each of the selected n clients locally reconstructs the weights θw’s and
scores θs’s using SEED (Algorithm 2 line 9). Then, each FSL client reorders the random scores
based on the global rankings, Rt
g (Algorithm 2 line 10); we depict this in Figure 1- 2a ."
EDGE-POPUP ALGORITHM,0.10290237467018469,"Next, each of the n clients uses reordered θs and ﬁnds a subnetwork within θw using Algorithm 1;
to ﬁnd a subnetwork, they use their local data and E local epochs (Algorithm 2 line 11). Note that,
each iteration of Algorithm 1 updates the scores θs. Then client u computes their local rankings Rt
u
using the ﬁnal updated scores (S) and ARGSORT(.), and sends Rt
u to the server. Figure 1- 2a shows
how each of the selected n clients reorders the random scores using global rankings. For instance,
the initial global rankings for this round are Rt
g = [2, 3, 0, 5, 1, 4], meaning that edge e4 should get
the highest score (s4 = 1.2), and edge e2 should get the lowest score (s2 = 0.2)."
EDGE-POPUP ALGORITHM,0.10554089709762533,Under review as a conference paper at ICLR 2022
EDGE-POPUP ALGORITHM,0.10817941952506596,Figure 1: A single FSL round with three clients and network of 6 edges.
EDGE-POPUP ALGORITHM,0.11081794195250659,"Figure 1- 2b shows, for each client, the scores and rankings they obtained after ﬁnding their local
subnetwork. For example, rankings of client C1 are Rt
1 = [4, 0, 2, 3, 5, 1], i.e., e4 is the least impor-
tant and e1 is the most important edge for C1. Considering desired subnetwork size to be 50%, C1
uses edges {3,5,1} in their ﬁnal subnetwork."
EDGE-POPUP ALGORITHM,0.11345646437994723,"4.3
SERVER: MAJORITY VOTE (FOR EACH ROUND t)
The server receives all the local rankings of the selected n clients, i.e., Rt
{u∈U}. Then, it performs a
majority vote over all the local rankings using VOTE(.) function. Note that, for client u, the index i
represents the importance of the edge Rt
u[i] for Cu. For instance, in Figure 1- 2b , rankings of C1 are
Rt
1 = [4, 0, 2, 3, 5, 1], hence the edge e4 at index=0 is the least important edge for C1, while the edge
e1 at index=5 is the most important edge. Consequently, VOTE(.) function assigns reputation=0 to
edge e4, reputation=1 to e0, reputation=2 to e2, and so on. In other words, for rankings Rt
u of Cu
and edge ei, VOTE(.) computes the reputation of ei as its index in Rt
u. Finally, VOTE(.) computes
the total reputation of ei as the sum of reputations from each of the local rankings. In Figure 1- 2b ,
reputations of e0 are 1 in Rt
1, 1 in Rt
2, and 0 in Rt
3, hence, the total reputation of e0 is 2. We depict
this in Figure 1- 3 ; here, the ﬁnal total reputations for edges e{i∈[0,5]} are A = [2, 12, 3, 11, 8, 9].
Finally, the server computes global rankings Rt+1
g
to use for round t + 1 by sorting the ﬁnal total
reputations of all edges, i.e., Rt+1
g
= ARGSORT(A)."
EDGE-POPUP ALGORITHM,0.11609498680738786,"Note that, all FSL operations that involve sorting, reordering, and voting are performed in a layer-
wise manner. For instance, the server computes global rankings Rt
g in round t for each layer sepa-
rately, and consequently, the clients selected in round t reorder their local randomly generated scores
θs for each layer separately."
EDGE-POPUP ALGORITHM,0.11873350923482849,"5
FEDERATED SUPERMASK LEARNING: SALIENT FEATURES"
EDGE-POPUP ALGORITHM,0.12137203166226913,"In this section, we discuss the two salient features of FSL that are instrumental for any distributed
learning algorithm to be practical: robustness to poisoning attacks and communication efﬁciency."
ROBUSTNESS OF FSL TO POISONING ATTACKS,0.12401055408970976,"5.1
ROBUSTNESS OF FSL TO POISONING ATTACKS
FSL is a distributed learning algorithm with mutually untrusting clients. Hence, a poisoning adver-
sary may own or compromise some of FSL clients, called malicious clients, and mount a targeted
or untargeted poisoning attack. In our work, we consider the untargeted attacks as they are more
severe than targeted attacks and can cause denial-of-service for all collaborating clients (Shejwalkar
et al., 2021) and show that FSL is secure against such poisoning attacks by design."
ROBUSTNESS OF FSL TO POISONING ATTACKS,0.1266490765171504,Under review as a conference paper at ICLR 2022
ROBUSTNESS OF FSL TO POISONING ATTACKS,0.12928759894459102,"Intuition on FSL’s robustness: Existing FL algorithms, including robust FL algorithms, are shown
to be vulnerable to targeted and untargeted poisoning attacks (Shejwalkar et al., 2021) where mali-
cious clients corrupt the global model by sharing malicious model updates. One of the key reasons
behind the susceptibility of existing algorithms is that their model updates can have arbitrary values.
For instance, to manipulate vanilla FedAvg, malicious clients send very large updates (Blanchard
et al., 2017), and to manipulate Multi-krum and Trimmed-mean, (Fang et al., 2020; Shejwalkar &
Houmansadr, 2021) propose to perturb a benign update in a speciﬁc malicious direction. On the
other hand, in FSL, clients must send a permutation of indices ∈[1, nℓ] for each layer. Hence, FSL
signiﬁcantly reduces the space of the possible malicious updates that an adversary can craft. Ma-
jority voting in FSL further reduces the chances of successful attack. Intuitively, this makes FSL
design robust to poisoning attacks. Below, we make this intuition more concrete."
ROBUSTNESS OF FSL TO POISONING ATTACKS,0.13192612137203166,"The worst-case untargeted poisoning attack on FSL: Here, the poisoning adversary aims to
reduce the accuracy of the ﬁnal global FSL subnetwork on most test inputs. To achieve this, the
adversary should replace the high ranked edges with low ranked edges in the ﬁnal subnetwork. For
the worst-case analysis of FSL, we assume a very strong adversary (i.e., threat model): 1) each of the
malicious clients has some data from benign distribution; 2) malicious clients know the entire FSL
algorithm and its parameters; 3) malicious clients can collude. Under this threat model we design
a worst case attack on FSL (Algorithm 3 in Appendix A.1), which executes as follows: First, all
malicious clients compute rankings on their benign data and use VOTE(.) algorithm to compute an
aggregate rankings. Finally, each of the malicious clients uses the reverse of the aggregate rankings
to share with the FSL server in given round. The adversary should invert the rankings layer-wise
as the FSL server will aggregate the local rankings per layer too, and it is not possible to mount a
model-wise attack."
ROBUSTNESS OF FSL TO POISONING ATTACKS,0.1345646437994723,"Now we justify why the attack in Algorithm 3 is the worst case attack on FSL for the strong threat
model we consider. Note that, FSL aggregation, i.e., VOTE(.), computes the reputations using
clients’ rankings and sums the reputations of each network edge. Therefore, the strongest poi-
soning attack would want to reduce the reputation of good edges. This can be achieved following
the aforementioned procedure of Algorithm 3 to reverse the rankings computed using benign data."
ROBUSTNESS OF FSL TO POISONING ATTACKS,0.13720316622691292,"0.6
0.7
0.8
0.9
1.0
p 0.0 0.2 0.4 0.6 0.8 1.0"
ROBUSTNESS OF FSL TO POISONING ATTACKS,0.13984168865435356,Vote Failure Prob Upperbound
ROBUSTNESS OF FSL TO POISONING ATTACKS,0.1424802110817942,"α = 10
α = 20
α = 30
α = 40"
ROBUSTNESS OF FSL TO POISONING ATTACKS,0.14511873350923482,"Figure 2: Upper bound on the failure
probability of VOTE(.) function in FSL."
ROBUSTNESS OF FSL TO POISONING ATTACKS,0.14775725593667546,"Theoretical analysis of robustness of FSL algorithm:
In this section, we prove an upper bound on the failure
probability of robustness of FSL, i.e., the probability that
a good edge will be removed from the ﬁnal subnetwork
when malicious clients mount the worst case attack."
ROBUSTNESS OF FSL TO POISONING ATTACKS,0.1503957783641161,"Following the work of Bernstein et al. (2019), we make
two assumptions in order to facilitate a concrete robust-
ness analysis of FSL: a) each malicious client has access
only to its own data, and b) we consider a simpler VOTE(.)
function, where the FSL server puts an edge ei in the ﬁ-
nal subnetwork if more than half of the clients have ei (a
good edge) in their local subnetworks. In other words,
the rankings that each client sends to the server is just a
bit mask showing that each edge should or should not be
in the ﬁnal subnetwork. The server makes a majority vote
on the bit masks, and if an edge has more than half votes,
it will be in the global subnetwork. Our VOTE(.) mecha-
nism has more strict robustness criterion, as it uses more nuanced reputations of edges instead of bit
masks. Hence, the upper bound on failure probability in this section also applies to the FSL VOTE(.)
function."
ROBUSTNESS OF FSL TO POISONING ATTACKS,0.15303430079155672,"The probability that our voting system fails is the probability that more than half of the votes
do not include ei in their subnetworks. The upper bound on the probability of failure would be 1/2
q"
ROBUSTNESS OF FSL TO POISONING ATTACKS,0.15567282321899736,"np(1−p)
(n(p+α(1−2p)−1/2))2 , where n is the number of clients being processed, p shows the probabil-
ity that a benign client puts ei in their top ranks, and α is the fraction of malicious clients. Due to
space limitations, we defer the detailed proof to Appendix A.2. Figure 2 shows the upper bound on
the failure of VOTE(.) for different values of α and p. We note that, the higher the probability p, the
higher the robustness of FSL."
ROBUSTNESS OF FSL TO POISONING ATTACKS,0.158311345646438,Under review as a conference paper at ICLR 2022
ROBUSTNESS OF FSL TO POISONING ATTACKS,0.16094986807387862,"5.2
FSL’S COMMUNICATION COSTS"
ROBUSTNESS OF FSL TO POISONING ATTACKS,0.16358839050131926,"In FL, and especially in the cross-device setting, clients have limited communication bandwidth.
Hence, FL algorithms must be communication efﬁcient. We discuss here the communication cost
of FSL algorithm. In the ﬁrst round, the FSL server only sends one seed of 32 bits to all the FSL
clients, so they can construct the random weights (θw) and scores (θs). In a round t, each of selected
FSL clients receives the global rankings Rt
g and sends back their local rankings Rt
u. The rankings
are a permutation of the indices of the edges in each layer, i.e., of [0, nℓ−1]∀ℓ∈[L] where L is the
number of layers and nℓis the number of parameters in ℓth layer."
ROBUSTNESS OF FSL TO POISONING ATTACKS,0.1662269129287599,"We use the naive approach to communicate layer-wise rankings, where each FSL client exchanges
a total of P"
ROBUSTNESS OF FSL TO POISONING ATTACKS,0.16886543535620052,"ℓ∈[L] nℓ× log(nℓ) bits. Because, for the layer ℓ, the client receives and sends nℓranks
where each one is encoded with log(nℓ) bits. On the other hand, a client exchanges P"
ROBUSTNESS OF FSL TO POISONING ATTACKS,0.17150395778364116,"ℓ∈[L] nℓ× 32
bits in FedAvg, when 32 bits are used to represent each of nℓweights in layer ℓ. In Appendix E, we
compare theoretical communication costs of various FL algorithms."
ROBUSTNESS OF FSL TO POISONING ATTACKS,0.1741424802110818,"Sparse-FSL: Here, we propose Sparse-FSL, a simple extension of FSL to further reduce the com-
munication cost. In Sparse-FLS, a client sends only the most important ranks of their local rankings
to the server for aggregation. For instance, in Figure 1, client C1 sends Rt
1 = [4, 0, 2, 3, 5, 1] in case
of FSL. But in sparse-FSL, with sparsity set to 50%, client C1 sends just the top 3 rankings, i.e.,
sends R′t
1 = [3, 5, 1]. For each client, the sparse-FSL server assumes 0 reputation for all of the edges
not included in the client’s rankings, i.e., in Figure 1, sparse-FSL server will assign reputation=0 for
edges e4, e0, and e2. Then the server uses VOTE(.) to compute total reputations of all edges and
then sort them to obtain the ﬁnal aggregate global rankings, i.e., Rt+1
g
, to use for subsequent rounds.
We observe in out experiments, that sparse-FSL performs very close to FSL, even with sparsity as
low as 10%, while also signiﬁcantly reducing the communication cost. Due to space limitation, we
defer the communication cost comparison of FSL with FedAvg, SingSGD, and LotteryFL (Li et al.,
2020a) to Appendix E."
EXPERIMENTS,0.17678100263852242,"6
EXPERIMENTS
In this section, we investigate the utility, robustness, and communication cost of our FSL frame-
work. We use MNIST, CIFAR10, and FEMNIST data and distribute them in non-iid fashion (using
Dirichlet distribution) among 1000, 1000, and 3400 clients respectively. At the end of the training,
we calculate the test accuracy of all the clients on the ﬁnal global model, and we report the mean
and standard deviation of all clients’ test accuracies in our experiments. We provide further details
of experimental setup in Appendix B. In addition to FSL, we also evaluate Sparse-FSL in different
settings. We use SFSL top x% to denote a Sparse-FSL clients who sends top x% of ranks in each
round."
COMMUNICATION COST ANALYSIS,0.17941952506596306,"6.1
COMMUNICATION COST ANALYSIS
In FSL, both clients and server communicate just the edge ranks instead of weight parameters.
Thus, FSL reduces both upload and download communication cost. Table 1 illustrates the utility,
i.e., the accuracy on test data, and communication cost of FSL and state-of-the-art quantization,
i.e., SignSGD (Bernstein et al., 2019), and sparsiﬁcation, i.e., TopK (Alistarh et al., 2018b; Aji &
Heaﬁeld, 2017) communication-reduction methods."
COMMUNICATION COST ANALYSIS,0.1820580474934037,"FSL versus SignSGD: We note that, FSL is signiﬁcantly more accurate than SignSGD. For in-
stance, on CIFAR10, distributed non-iid among 1000 clients, FSL achieves 85.3% while SignSGD
achieves 79.1% , or on FEMNIST, FSL achieves 84.2% while SignSGD achieves 79.3%. This is
because, FSL clients send more nuanced information via rankings of their subnetworks compared to
SignSGD, where clients just send the signs of their model updates."
COMMUNICATION COST ANALYSIS,0.18469656992084432,"SignSGD in FL reduces only the upload communication, but for efﬁciency reasons, the server sends
all of the weight parameters (each of 32 bits) to the newly selected clients. Hence, SignSGD has
very efﬁcient upload communication, but very inefﬁcient download communication. For instance,
on CIFAR10, for both upload and download, FSL achieves 13.1MB each while SignSGD achieves
0.63MB and 20.1MB, respectively."
COMMUNICATION COST ANALYSIS,0.18733509234828497,"FSL versus TopK: We compare FSL with TopK with K ∈{10, 50}%. FSL is more accurate
than Topk for MNIST and CIFAR10: on CIFAR10, FSL accuracy is 85.3%, while TopK accuracies
are 82.1% and 77.8% with K=50% and K=10%, respectively. Similar to SignSGD, Topk is more
efﬁciently reduces upload cost, but has very high download communication cost. Therefore, the"
COMMUNICATION COST ANALYSIS,0.18997361477572558,Under review as a conference paper at ICLR 2022
COMMUNICATION COST ANALYSIS,0.19261213720316622,"Table 1: Comparing the utility (test accuracy) and communication cost of FedAvg, FSL (in bold),
SignSGD and, TopK and Sparse-FSL (SFSL) with different percentages of sparsity (in bold)."
COMMUNICATION COST ANALYSIS,0.19525065963060687,"Dataset
Algorithm
Accuracy (STD)
Upload (MB)
Download (MB)"
COMMUNICATION COST ANALYSIS,0.19788918205804748,"MNIST + LeNet
1000 clients"
COMMUNICATION COST ANALYSIS,0.20052770448548812,"FedAvg
98.8 (3.1)
6.20
6.20
FSL
98.8 (3.2)
4.05
4.05
SFSL Top 50%
98.2 (3.8)
2.03
4.05
SFSL Top 10%
89.5 (9.2)
0.40
4.05
SignSGD
97.2 (4.6)
0.19
6.20
TopK 50%
98.8 (3.2)
3.29
6.20
TopK 10%
98.7 (3.2)
0.81
6.20"
COMMUNICATION COST ANALYSIS,0.20316622691292877,"CIFAR10 + Conv8
1000 clients"
COMMUNICATION COST ANALYSIS,0.20580474934036938,"FedAvg
85.4 (11.2)
20.1
20.1
FSL
85.3 (11.3)
13.1
13.1
SFSL Top 50%
77.6 (13.0)
6.5
13.1
SFSL Top 10%
27.5 (14.4)
1.3
13.1
SignSGD
79.1 (13.6)
0.63
20.1
TopK 50%
82.1 (11.8)
10.69
20.1
TopK 10%
77.8 (13.0)
2.64
20.1"
COMMUNICATION COST ANALYSIS,0.20844327176781002,"FEMNIST + LeNet
3400 clients"
COMMUNICATION COST ANALYSIS,0.21108179419525067,"FedAvg
85.8 (10.2)
6.23
6.23
FSL
84.2 (10.7)
4.06
4.06
SFSL Top 50%
75.2 (12.7)
2.03
4.06
SFSL Top 10%
59.2 (15.0)
0.40
4.06
SignSGD
79.3 (12.4)
0.19
6.23
TopK 50%
85.7 (9.9)
3.31
6.23
TopK 10%
85.5 (10.0)
0.81
6.23"
COMMUNICATION COST ANALYSIS,0.21372031662269128,"combined upload and download communication cost per client per round is 26.2MB for FSL and
30.79MB for TopK with K=50%, and TopK still has worse performance."
COMMUNICATION COST ANALYSIS,0.21635883905013192,"Communication cost reduction due to Sparse-FSL (SFSL): We now evaluate SFSL explained
in Section 5.2. In SFSL with top 50% ranks, clients send the top 50% of their ranks to the server,
which reduces the upload bandwidth consumption by half. Please note that the download cost of
SFSL is the same as FSL since the FSL server should send all the global rankings to the selected
clients in each round. We note from Table 1 that, by sending fewer ranks, SFSL reduces upload
communication at a small cost of performance. For instance, on CIFAR10, SFLS with top 50%
reduces the upload communication by 50% at the cost reducing accuracy from 85.4% to 77.6%."
SECURITY ANALYSIS,0.21899736147757257,"6.2
SECURITY ANALYSIS"
SECURITY ANALYSIS,0.22163588390501318,"We compare FSL with state-of-the-art robust aggregation rules (AGRs): Mkrum (Blanchard et al.,
2017), and Trimmed-mean (Xie et al., 2018; Yin et al., 2018). Table 2 gives the performances of
robust AGRs, SignSGD, and FSL with different percentages of malicious clients. Here, we make
a rather impractical assumption in favor of the robust AGRs: we assume that the server knows the
exact % of malicious clients in each FL round. FSL does not require this knowledge."
SECURITY ANALYSIS,0.22427440633245382,"FSL achieves higher robustness than state-of-the-art robust AGRs: We note from Table 2 that,
FSL is more robust to the presence of malicious clients who try to poison the global model compared
to Multi-Krum, Trimmed-mean, and SignSGD for both 10% and 20% malicious clients rates. For
instance, on CIFAR10, 10% malicious clients can decrease the accuracy of FL models to 56.3%,
58.8%, and 39.8% for Trimmed-mean, Multi-Krum, and SignSGD respectively; 20% malicious
clients can decrease the accuracy of the FL models to 20.5%, 25.6%, 10.0% for Trimmed-mean,
Multi-Krum, and SignSGD respectively. On the other hand, FSL performance decreases to 79.0%
and 69.5% for 10% and 20% attacking ratio respectively."
SECURITY ANALYSIS,0.22691292875989447,"We make similar observations for MNIST and FEMNIST datasets: for FEMNIST, 10% (20%) mali-
cious clients reduce accuracy of the global model from 85.8% to 72.7% (56.2%) for Trimmed-Mean,
to 80.9% (23.7%) for Multi-krum, and 76.7% (55.1%) for SignSGD, while FSL accuracy decreases
to 83.0% (65.8%). We omit evaluating TopK, because even a single malicious client (Blanchard
et al., 2017) can jeopardize its accuracy."
SECURITY ANALYSIS,0.22955145118733508,Under review as a conference paper at ICLR 2022
SECURITY ANALYSIS,0.23218997361477572,"Table 2: Comparing the robustness of various FL algorithms: FSL and SFSL (in bold) outperform
state-of-the-art robust AGRs and SignSGD against strongest of untargeted poisoning attacks."
SECURITY ANALYSIS,0.23482849604221637,"Dataset
AGR
No malicious
10% malicious
20% malicious"
SECURITY ANALYSIS,0.23746701846965698,"MNIST + LeNet
1000 clients"
SECURITY ANALYSIS,0.24010554089709762,"FedAvg
98.8 (3.2)
10.0 (10.0)
10.0 (10.0)
Trimmed-mean
98.8 (3.2)
95.1 (7.7)
87.6 (9.5)
Multi-krum
98.8 (3.2)
98.6 (3.3)
97.9 (4.1)
SignSGD
97.2 (4.6)
96.6 (5.0)
96.2 (5.6)
FSL
98.8 (3.1)
98.8 (3.1)
98.7 (3.3)
SFSL Top 50%
98.2 (3.8)
97.04 (4.4)
95.1 (7.8)"
SECURITY ANALYSIS,0.24274406332453827,"CIFAR10 + Conv8
1000 clients"
SECURITY ANALYSIS,0.24538258575197888,"FedAvg
85.4 (11.2)
10.0 (10.1)
10.0 (10.1)
Trimmed-mean
84.9 (11.0)
56.3 (16.0)
20.5 (13.2)
Multi-krum
84.7 (11.3)
58.8 (15.8)
25.6 (14.4)
SignSGD
79.1 (12.8)
39.7 (15.9)
10.0 (10.1)
FSL
85.3 (11.3)
79.0 (12.4)
69.5 (14.8)
SFSL Top 50%
77.6 (13.0)
41.7 (15.4)
39.7 (15.2)"
SECURITY ANALYSIS,0.24802110817941952,"FEMNIST + LeNet
3400 clients"
SECURITY ANALYSIS,0.25065963060686014,"FedAvg
85.8 (10.2)
6.3 (5.8)
6.3 (5.8)
Trimmed-mean
85.2 (11.0)
72.7 (15.7)
56.2 (20.3)
Multi-krum
85.2 (10.9)
80.9 (12.2)
23.7 (12.8)
SignSGD
79.3 (12.4)
76.7 (13.2)
55.1 (14.9)
FSL
84.2 (10.7)
83.0 (10.9)
65.8 (17.8)
SFSL Top 50%
75.2 (12.7)
70.5 (14.4)
60.39 (14.8)"
MISCELLANEOUS DISCUSSIONS,0.2532981530343008,"6.3
MISCELLANEOUS DISCUSSIONS"
MISCELLANEOUS DISCUSSIONS,0.2559366754617414,"Due to space limitations, we defer a detailed discussion of ablation studies of FSL to Appendix C
and below give their important takeaways."
MISCELLANEOUS DISCUSSIONS,0.25857519788918204,"Initialization matters in FSL: In FSL, the weight parameters are randomly initialized at the start
and remain ﬁxed throughout the training. An appropriate initialization is instrumental to the success
of FSL, since the clients are trying to ﬁnd the most important weight parameters. We study efﬁcacy
of three initializing strategies that use three different distributions: Glorot Normal, Kaiming Normal,
and Singed Kaiming Constant. Table 5 shows the results. We observe from Table 5 that, Singed
Kaiming Constant initialization achieves the best results that are closest to FedAvg."
MISCELLANEOUS DISCUSSIONS,0.2612137203166227,"Varying the sparsity of edge-popup algorithm in FSL: Figure 4 illustrates how the performance
of FSL varies with the sizes of local subnetworks that the clients share with the server. In other
words, when we vary the sparsity k% of edge popup algorithm during local subnetwork search
k ∈[10, 20, 30, 40, 50, 60, 70, 80, 90]%. Interestingly we note that, FSL performs the worst when
clients use all (k=100%) or none (k=0%) of the edges. This is because, it is difﬁcult to ﬁnd a
subnetwork with small number of edges. While using all of the edge essentially results in using a
random neural network. As we can see FSL with k ∈[40, 70]%, gives the best performances for all
the three datasets. Hence, we set k=50% by default in our experiments."
CONCLUSIONS,0.2638522427440633,"7
CONCLUSIONS"
CONCLUSIONS,0.26649076517150394,"We designed a novel collaborative learning algorithm, called Federated Supermask Learning (FSL),
to address the issues of robustness to poisoning and communication efﬁciency in existing FL algo-
rithms. We argue that a core reason for the susceptibility of existing FL algorithms to poisoning
is the use of arbitrary values in their model updates. Hence, in FSL, we use ranks of edges of a
randomly initialized neural network contributed by collaborating clients to ﬁnd a global ranking
and then use a subnetwork based only on the top edges. Use of rankings in a ﬁxed range restricts
the space available to poisoning adversaries to craft malicious updates, and also allows FSL to use
sophisticated communication reduction methods. We show, both theoretically and empirically, that
ranking based collaborative learning can effectively mitigate the robustness issue as well as reduce
the communication costs involved."
CONCLUSIONS,0.2691292875989446,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.2717678100263852,"8
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.27440633245382584,"We have attached our implementation code and experiment notebooks as supplementary materi-
als. There are separate experiment notebooks showing the process of FSL training for MNIST,
CIFAR10, and FEMNIST (results that we used in Table 1 and Table 2). We have explained all the
hyperparameters including optimizer, learning rate, momentum, weight decay, and batch size in Ap-
pendix B.1 for both FSL and FedAvg (including using robust AGRs) settings. We also explained the
model architectures we used in our experiments in Table 4 explaining different layers with number
of parameters inside each. We also explained how we distributed MNIST and CIFAR10 samples
among 1,000 clients using Dirichlet distribution to make them non-iid in Appendix B.3."
REPRODUCIBILITY STATEMENT,0.2770448548812665,"We propose Federated Supermask Learning (FSL) where clients ﬁnd a subnetwork within a ran-
domly initialized neural network, without training the weights of the network. Our extensive evalu-
ation demonstrated FSL can provide better communication cost and more robustness to untargeted
poisoning attacks compared to existing FL compressors and Byzantine-robust aggregation rules."
REFERENCES,0.2796833773087071,REFERENCES
REFERENCES,0.28232189973614774,"Alham Fikri Aji and Kenneth Heaﬁeld. Sparse communication for distributed gradient descent.
In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,
EMNLP, 2017."
REFERENCES,0.2849604221635884,"Dan Alistarh, Torsten Hoeﬂer, Mikael Johansson, Nikola Konstantinov, Sarit Khirirat, and C´edric
Renggli. The convergence of sparsiﬁed gradient methods. In Advances in Neural Information
Processing Systems 31: NeurIPS, 2018a."
REFERENCES,0.287598944591029,"Dan Alistarh, Torsten Hoeﬂer, Mikael Johansson, Nikola Konstantinov, Sarit Khirirat, and C´edric
Renggli. The convergence of sparsiﬁed gradient methods. In Advances in Neural Information
Processing Systems, pp. 5973–5983, 2018b."
REFERENCES,0.29023746701846964,"argsort.
Torch.argsort.
URL https://pytorch.org/docs/stable/generated/
torch.argsort.html."
REFERENCES,0.2928759894459103,"Yoshua Bengio, Nicholas L´eonard, and Aaron Courville.
Estimating or propagating gradients
through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013."
REFERENCES,0.2955145118733509,"Jeremy Bernstein, Jiawei Zhao, Kamyar Azizzadenesheli, and Anima Anandkumar. signsgd with
majority vote is communication efﬁcient and fault tolerant. In 7th International Conference on
Learning Representations, ICLR, 2019."
REFERENCES,0.29815303430079154,"Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo. Analyzing federated
learning through an adversarial lens. In International Conference on Machine Learning, pp. 634–
643, 2019."
REFERENCES,0.3007915567282322,"Peva Blanchard, Rachid Guerraoui, Julien Stainer, et al. Machine learning with adversaries: Byzan-
tine tolerant gradient descent. In Advances in Neural Information Processing Systems, pp. 119–
129, 2017."
REFERENCES,0.3034300791556728,"Sebastian Caldas, Peter Wu, Tian Li, Jakub Koneˇcn´y, H. Brendan McMahan, Virginia Smith, and
Ameet Talwalkar. LEAF: A benchmark for federated settings. CoRR, abs/1812.01097, 2018.
URL http://arxiv.org/abs/1812.01097."
REFERENCES,0.30606860158311344,"Francesco Paolo Cantelli. Sui conﬁni della probabilita. In Atti del Congresso Internazionale dei
Matematici: Bologna del 3 al 10 de settembre di 1928, pp. 47–60, 1929."
REFERENCES,0.3087071240105541,"Hongyan Chang, Virat Shejwalkar, Reza Shokri, and Amir Houmansadr. Cronus: Robust and het-
erogeneous collaborative learning with black-box knowledge transfer, 2019."
REFERENCES,0.3113456464379947,"Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andr´e van Schaik.
EMNIST: extending
MNIST to handwritten letters.
In 2017 International Joint Conference on Neural Networks,
IJCNN, 2017."
REFERENCES,0.31398416886543534,Under review as a conference paper at ICLR 2022
REFERENCES,0.316622691292876,"Yann N Dauphin and Yoshua Bengio.
Big neural networks waste capacity.
arXiv preprint
arXiv:1301.3583, 2013."
REFERENCES,0.31926121372031663,"Misha Denil, Babak Shakibi, Laurent Dinh, Marc’Aurelio Ranzato, and Nando de Freitas. Predicting
parameters in deep learning. In Proceedings of the 26th International Conference on Neural
Information Processing Systems-Volume 2, pp. 2148–2156, 2013."
REFERENCES,0.32189973614775724,"Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Zhenqiang Gong. Local model poisoning attacks
to byzantine-robust federated learning. In Srdjan Capkun and Franziska Roesner (eds.), 29th
USENIX Security Symposium, USENIX Security, 2020."
REFERENCES,0.3245382585751979,"Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. ICLR, 2019."
REFERENCES,0.32717678100263853,"Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward neural
networks. In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence
and Statistics, AISTATS, 2010."
REFERENCES,0.32981530343007914,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing
human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE international
conference on computer vision, pp. 1026–1034, 2015."
REFERENCES,0.3324538258575198,"Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data
distribution for federated visual classiﬁcation. arXiv preprint arXiv:1909.06335, 2019."
REFERENCES,0.33509234828496043,"Peter Kairouz, H Brendan McMahan, Brendan Avent, Aur´elien Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances
and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019."
REFERENCES,0.33773087071240104,"Jakub Koneˇcn`y, H Brendan McMahan, Felix X Yu, Peter Richt´arik, Ananda Theertha Suresh, and
Dave Bacon.
Federated learning: Strategies for improving communication efﬁciency.
arXiv
preprint arXiv:1610.05492, 2016."
REFERENCES,0.3403693931398417,Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
REFERENCES,0.34300791556728233,"Ang Li, Jingwei Sun, Binghui Wang, Lin Duan, Sicheng Li, Yiran Chen, and Hai Li. Lotteryﬂ:
Personalized and communication-efﬁcient federated learning with lottery ticket hypothesis on
non-iid datasets. CoRR, 2020a. URL https://arxiv.org/abs/2008.03371."
REFERENCES,0.34564643799472294,"Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges,
methods, and future directions. IEEE Signal Processing Magazine, 37(3):50–60, 2020b."
REFERENCES,0.3482849604221636,"Heiko Ludwig, Nathalie Baracaldo, Gegi Thomas, and Yi Zhou. IBM federated learning: an enter-
prise framework white paper V0.1. CoRR, abs/2007.10987, 2020."
REFERENCES,0.35092348284960423,"H Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efﬁcient learning of deep networks from decentralized data. Proceedings of the
20 th International Conference on Artiﬁcial Intelligence and Statistics, 2017."
REFERENCES,0.35356200527704484,"El Mahdi El Mhamdi, Rachid Guerraoui, and S´ebastien Rouault. The hidden vulnerability of dis-
tributed learning in byzantium. In Proceedings of the 35th International Conference on Machine
Learning, ICML, 2018."
REFERENCES,0.3562005277044855,"Matthias Paulik, Matt Seigel, and Henry Mason and. Federated evaluation and tuning for on-device
personalization: System design & applications. CoRR, abs/2102.08503, 2021. URL https:
//arxiv.org/abs/2102.08503."
REFERENCES,0.35883905013192613,"Vivek Ramanujan, Mitchell Wortsman, Aniruddha Kembhavi, Ali Farhadi, and Mohammad Raste-
gari. What’s hidden in a randomly weighted neural network? In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 11893–11902, 2020."
REFERENCES,0.36147757255936674,"Sashank J Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Koneˇcn`y,
Sanjiv Kumar, and Hugh Brendan McMahan. Adaptive federated optimization. In International
Conference on Learning Representations, 2020."
REFERENCES,0.3641160949868074,Under review as a conference paper at ICLR 2022
REFERENCES,0.36675461741424803,"Virat Shejwalkar and Amir Houmansadr. Manipulating the byzantine: Optimizing model poisoning
attacks and defenses for federated learning. In Proceedings of the 28th Network and Distributed
System Security Symposium, (NDSS), 2021."
REFERENCES,0.36939313984168864,"Virat Shejwalkar, Amir Houmansadr, Peter Kairouz, and Daniel Ramage. Back to the drawing board:
A critical evaluation of poisoning attacks on federated learning. arXiv preprint arXiv:2108.10241,
2021."
REFERENCES,0.3720316622691293,"Hongyi Wang, Kartik Sreenivasan, Shashank Rajput, Harit Vishwakarma, Saurabh Agarwal, Jy-
yong Sohn, Kangwook Lee, and Dimitris Papailiopoulos. Attack of the tails: Yes, you really can
backdoor federated learning. arXiv preprint arXiv:2007.05084, 2020."
REFERENCES,0.37467018469656993,"Mitchell Wortsman, Vivek Ramanujan, Rosanne Liu, Aniruddha Kembhavi, Mohammad Rastegari,
Jason Yosinski, and Ali Farhadi. Supermasks in superposition. In Advances in Neural Information
Processing Systems 33: NeurIPS, 2020."
REFERENCES,0.37730870712401055,"Cong Xie, Oluwasanmi Koyejo, and Indranil Gupta. Generalized byzantine-tolerant sgd. arXiv
preprint arXiv:1802.10116, 2018."
REFERENCES,0.37994722955145116,"Dong Yin, Yudong Chen, Kannan Ramchandran, and Peter L. Bartlett. Byzantine-robust distributed
learning: Towards optimal statistical rates. In Jennifer G. Dy and Andreas Krause (eds.), Pro-
ceedings of the 35th International Conference on Machine Learning, ICML, 2018."
REFERENCES,0.38258575197889183,"Hattie Zhou, Janice Lan, Rosanne Liu, and Jason Yosinski. Deconstructing lottery tickets: Zeros,
signs, and the supermask. In Advances in Neural Information Processing Systems 32: NeurIPS,
2019."
REFERENCES,0.38522427440633245,Under review as a conference paper at ICLR 2022
REFERENCES,0.38786279683377306,"A
MISSING DETAILS OF ROBUSTNESS OF FSL"
REFERENCES,0.39050131926121373,"A.1
FSL WORST CASE POISONING ATTACK ALGORITHM"
REFERENCES,0.39313984168865435,Algorithm 3 shows the rankings poisoning attack explained in Section 5.
REFERENCES,0.39577836411609496,Algorithm 3 FSL Poisoning
REFERENCES,0.39841688654353563,"1: Input: number of malicious clients M, number of malicious local epochs E′, seed SEED, global
ranking Rt
g, learning rate η, subnetwork size k%
2: function CRAFTMALICIOUSUPDATE(M, SEED, Rt
g, E′, η, k):
3:
for mu ∈[M] do
▷For all the malicious clients
4:
Malicious Client Executes:
5:
θs, θw ←Initialize scores and weights using SEED
6:
θs[Rt
g] ←SORT(θs)
7:
S ←Edge-PopUp(E′, Dtr
u , θw, θs, k, η)
8:
Rt
mu ←ARGSORT(S)
▷Ranking of the malicious client
9:
end for
10:
Aggregation:"
REFERENCES,0.40105540897097625,"11:
Rt
m ←VOTE(Rt
mu∈[M])
▷Majority vote aggregation"
REFERENCES,0.40369393139841686,"12:
return REVERSE(Rt
m)
▷reverse the ranking
13: end function"
REFERENCES,0.40633245382585753,"A.2
THEORETICAL ANALYSIS OF ROBUSTNESS OF FSL"
REFERENCES,0.40897097625329815,"In this section, we detail the proof of robustness of FSL. In other words, we prove an upper bound
on the failure probability of robustness of FSL, i.e., the probability that a good edge will be re-
moved from the ﬁnal subnetwork when malicious clients mount the worst case attack. Inspired from
SignSGD (Bernstein et al., 2019), for this proof, We assume a simpler VOTE(.) function where if
more than half of the clients add an edge ei to their subnetworks, then the FSL server adds it to the
ﬁnal global subnetwork. We also assume that the malicious clients cannot collude in our proof."
REFERENCES,0.41160949868073876,"Assume that edge ei is a good edge, i.e., having ei in the ﬁnal subnetwork improves the performance
of the ﬁnal subnetwork. Let Z be the random variable that represents the number of clients who vote
for the edge ei to be in the ﬁnal subnetwork, i.e., the number of clients whose local subnetwork of
size k% of the entire supernetwork (Algorithm 2 line 11) contains ei. Therefore, Z ∈[0, n] where
n is the number of clients being processed in a given FSL round."
REFERENCES,0.41424802110817943,"Let G and B be the random variable that represent the number of benign and malicious clients that
vote for edge ei, respectively; the malicious clients inadvertently exclude the good edge ei in their
local subnetwork based on their benign training data."
REFERENCES,0.41688654353562005,"There are total of αn malicious clients, where α is the fraction of malicious clients that B of them
decides that ei is a bad edge and should not be removed. Each of the malicious clients computes
the subnetwork on its own benign training data, so B of them do not conclude that ei is a good
edge. Hence, Z = G + B. We can say that G and B have binomial distribution , i.e., G ∼
binomial([(1 −α)n, p] and B ∼binomial([αn, 1 −p] where p is the probability that a benign client
has this edge in their local subnetwork and α is the fraction of malicious clients. Note that the
probability that our voting in simpliﬁed FSL fails is P[failure] = P[Z <= n"
REFERENCES,0.41952506596306066,"2 ], i.e., when more than
half of the clients vote against ei, i.e., they do not include ei in their local subnetworks. We can ﬁnd
the mean and variance of Z as follows:"
REFERENCES,0.42216358839050133,"E[Z] = (1 −α)np + αn(1 −p)
(1)"
REFERENCES,0.42480211081794195,"V ar[Z] = (1 −α)np(1 −p) + αnp(1 −p) = np(1 −p)
(2)"
REFERENCES,0.42744063324538256,"Cantelli (1929) provides an inequality where for a random variable X with mean µ and variance σ2
we have P[µ −X >= λ] <=
1
1+ λ2"
REFERENCES,0.43007915567282323,"σ2 . Using this inequality, we can write:"
REFERENCES,0.43271767810026385,Under review as a conference paper at ICLR 2022
REFERENCES,0.43535620052770446,"0
5
10
15
20
FSL Round
0 5 10 15 20 25"
REFERENCES,0.43799472295514513,Number of Votes
REFERENCES,0.44063324538258575,"malicious ratio=10%
malicious ratio=20%"
REFERENCES,0.44327176781002636,"malicious ratio=30%
malicious ratio=40%"
REFERENCES,0.44591029023746703,"Figure 3: Empirical robustness bounds of FSL. Bars are showing the number of votes for a particular
edge in ﬁrst layer of LeNet (trained on MNIST) with selecting 25 clients in each round and no
presence of malicious client. The adversary tries to ﬂip one good edge from the global model. The
horizontal lines are showing the thresholds for each malicious ratio that if the number of votes is
less than them, the adversary can change the decision about this edge."
REFERENCES,0.44854881266490765,P[Z <= n
REFERENCES,0.45118733509234826,"2 ] = P[E[Z] −Z >= E[Z] −n/2] <=
1"
REFERENCES,0.45382585751978893,1 + (E[z]−n/2)2
REFERENCES,0.45646437994722955,"var[Z]
(3)"
REFERENCES,0.45910290237467016,"because 1 + x2 >= 2x, we have:"
REFERENCES,0.46174142480211083,P[Z <= n
REFERENCES,0.46437994722955145,2 ] <= 1/2 s
REFERENCES,0.46701846965699206,"V ar[Z]
(E[Z] −n/2)2 = 1/2 s"
REFERENCES,0.46965699208443273,"np(1 −p)
(np −αnp + αn −αnp −n/2)2
(4) = 1/2 s"
REFERENCES,0.47229551451187335,"np(1 −p)
(n(p + α(1 −2p) −1/2))2"
REFERENCES,0.47493403693931396,"What this means is that the probability that the simpliﬁed VOTE(.) fails is upper bounded as in (4).
We show the effect of the different values of α and p in Figure 2. We can see from Figure 2,
if the benign clients can train better supermasks (better chance that a good edge ended in their
subnetwork), the probability that the attackers succeed is lower (more robustness). VOTE(.) in FSL
(Section 4.3) is more sophisticated and puts more constraints on the malicious clients, hence the
about upper bound also applies to FSL."
REFERENCES,0.47757255936675463,"We show the theoretical relationship between upper bound on the failure of VOTE(.) for different
values of malicious rate (α) in Figure 2. To validate our theoretical bounds, we measure the least
number of malicious clients that the adversary needs to control to remove a good edge from the
global subnetwork in Figure 3. In this ﬁgure, we report the votes of one particular edge in the ﬁrst
layer of LeNet (trained on MNIST) where there are 288 edges in the ﬁrst layer (Table B.1 shows the
number of edges in each layer). We consider this edge as a good edge since we observe that it would
be in the ﬁnal global subnetwork (at FSL round 20) if there were no malicious clients. In this ﬁgure,
the bars are showing the number of votes this edge received to be in the global subnetwork for that
FSL round. The horizontal lines are showing the thresholds that if the number of votes is less than
them, the adversary can change the decision about this good edge. We are selecting 25 clients in
each round, so we consider 2 × α × 25 for thresholds. For instance, when we assume there are
20% malicious clients (on average there are 5 malicious among 25 selected clients) that means that
5 votes of benign votes decreases and 5 votes is added to malicious votes, so the threshold would be
10."
REFERENCES,0.48021108179419525,Under review as a conference paper at ICLR 2022
REFERENCES,0.48284960422163586,"Table 3: Model architectures. We use identical architecture to those Ramanujan et al. (2020);
Wortsman et al. (2020) used."
REFERENCES,0.48548812664907653,"Architecture
Layer Name
Number of parameters"
REFERENCES,0.48812664907651715,"LeNet + MNIST
(Wortsman et al., 2020)"
REFERENCES,0.49076517150395776,"Convolution(32) + Relu
288
Convolution(64) + Relu
18432
MaxPool(2x2)
-
FC(128) + Relu
1605632
FC(10)
1280"
REFERENCES,0.49340369393139843,"Conv8 + CIFAR10
(Ramanujan et al., 2020)"
REFERENCES,0.49604221635883905,"Convolution(64) + Relu
1728
Convolution(64) + Relu
36864
MaxPool(2x2)
-
Convolution(128) + Relu
73728
Convolution(128) + Relu
147456
MaxPool(2x2)
-
Convolution(256) + Relu
294912
Convolution(256) + Relu
589824
MaxPool(2x2)
-
Convolution(512) + Relu
1179648
Convolution(512) + Relu
2359296
MaxPool(2x2)
-
FC(256) + Relu
524288
FC(256) + Relu
65536
FC(10)
2560"
REFERENCES,0.49868073878627966,"LeNet + FEMNIST
(Wortsman et al., 2020)"
REFERENCES,0.5013192612137203,"Convolution(32) + Relu
288
Convolution(64) + Relu
18432
MaxPool(2x2)
-
FC(128) + Relu
1605632
FC(62)
7936
B
MISSING DETAILS OF EXPERIMENTAL SETUP"
REFERENCES,0.503957783641161,"B.1
DATASETS AND MODEL ARCHITECTURES"
REFERENCES,0.5065963060686016,"MNIST is a 10-class class-balanced classiﬁcation task with 70,000 gray-scale images, each of size
28 × 28. We experiment with LeNet architecture given in Table 4. For local training in each FSL/FL
round, each client uses 2 epochs. For training weights (experiments with FedAvg, SignSGD, TopK),
we use SGD optimizer with learning rate of 0.01, momentum of 0.9, weight decay of 1e-4, and
batch size 8. For training supermasks (experiments with FSL), we use SGD with learning rate of
0.4, momentum 0.9, weight decay 1e-4, and batch size 8."
REFERENCES,0.5092348284960422,"CIFAR10 (Krizhevsky & Hinton, 2009) is a 10-class classiﬁcation task with 60,000 RGB images
(50,000 for training and 10,000 for testing), each of size 32 × 32. We experiment with a VGG-like
architecture given in Table 4, which is identical to what Ramanujan et al. (2020) used. For local
training in each FSL/FL round, each client uses 5 epochs. For training weights (experiments with
FedAvg, SignSGD, TopK), we use SGD with learning rate of 0.1, momentum of 0.9, weight decay
of 1e-4, and batch size of 8. For training supermasks (experiments with FSL), we optimize SGD
with learning rate of 0.4, momentum of 0.9, weight decay of 1e-4, and batch size of 8."
REFERENCES,0.5118733509234829,"FEMNIST (Caldas et al., 2018; Cohen et al., 2017) is a character recognition classiﬁcation task
with 3,400 clients, 62 classes (52 for upper and lower case letters and 10 for digits), and 671,585
gray-scale images. Each client has data of their own handwritten digits or letters. We use LeNet
architecture given in Table 4. For local training in each FSL/FL round, each client uses 2 epochs.
For training weights (experiments with FedAvg, SignSGD, TopK), we use SGD with learning rate
of 0.15, momentum of 0.9, weight decay of 1e-4, and batch size of 10. For training supermasks
(experiments with FSL), we optimize SGD with learning rate of 0.2, momentum of 0.9, weight
decay of 1e-4, and batch size of 10."
REFERENCES,0.5145118733509235,Under review as a conference paper at ICLR 2022
REFERENCES,0.5171503957783641,"B.2
HYPERPARAMETERS TUNING"
REFERENCES,0.5197889182058048,"We optimize the hyperparameters based on FSL and other baselines independently. The hyperpa-
rameters that we used in our experiments are tuned in scenario with no malicious clients. Table B.1
shows the performance of FSL and other baselines on CIFAR10 (distributed over 1000 users using
Dirichlet distribution) for different values of hyperparameters when there are 10% malicious clients
among the clients. This table shows the robustness of FSL still persists even if we change the hyper-
parameters. We reported mean of accuracies and standard deviation of accuracies for all the clients
at the ﬁnal FSL round."
REFERENCES,0.5224274406332454,"B.3
NON-IID DATA DISTRIBUTION"
REFERENCES,0.525065963060686,"Using Dirichlet Distribution:
Considering the heterogeneous data in the real-word cross-device
FL, similar to previous works (Reddi et al., 2020; Hsu et al., 2019), we distribute MNIST and
CIFAR10 among 1,000 clients in a non-iid fashion using Dirichlet distribution with parameter β =
1. Note that increasing β results in more iid datasets. Next, we split datasets of each client into
training (80%) and test (20%). At the end of the FL rounds, we calculate the test accuracy of each
client for its test data, and we report the average of test accuracies of all the clients. We run all the
experiments for 2000 global rounds of FSL and FL and select 25 clients in each round."
REFERENCES,0.5277044854881267,"Assigning only two classes to each client:
McMahan et al. (2017) used a more extreme hetero-
geneous data assignment. For assignment of MNIST and CIFAR10 among 1000 clients using this
Non-iid-ness method, we sort all the training and validation data inside MNIST and CIFAR10, then
partition them into 2000 shards. This means that each shards of training MNIST has 30 images and
each CIFAR10 shard has 25 images. Then we assign two random shards to each client resulting
in each client have at most data of two classes, and in CIFAR10 experiments, each client has 50
training images, and 10 test images, and in MNIST experiments, each client has 60 training images
and 10 test images. We only use this assignment in Section C.3."
REFERENCES,0.5303430079155673,"B.4
BASELINE FL ALGORITHMS"
REFERENCES,0.5329815303430079,"Federated averaging
In non-adversarial FL settings, i.e., without any malicious clients, the
dimension-wise Average (FedAvg) (Koneˇcn`y et al., 2016; McMahan et al., 2017) is an effective
AGR. In fact, due to its efﬁciency, Average is the only AGR implemented by FL applications in
practice (Ludwig et al., 2020; Paulik et al., 2021)."
REFERENCES,0.5356200527704486,"SignSGD is a quantization method used in distributed learning to compress each dimension of
updates into 1 bit instead of 32 or 64 bits. To achieve this, in SignSGD (Bernstein et al., 2019)
the clients only send the sign of the gradient updates to the server, and the server runs a majority
vote on them. SignSGD is designed for distributed learning where all the clients participate in
each round, so all the clients are aware of the most updated weight parameters of the global model.
However, using SignSGD in FL just provides beneﬁt in upload bandwidth, but to achieve good
overall performance of the global model, the server should send all the weight parameters (each of
32 bits) to the newly selected clients in each round. This makes SignSGD very efﬁcient in upload
cost, but it is as inefﬁcient as FedAvg in download."
REFERENCES,0.5382585751978892,"TopK is a sparsiﬁcation method used in distributed learning that transmits only a few elements in
each model update to the server. In TopK (Aji & Heaﬁeld, 2017; Alistarh et al., 2018a), the clients
ﬁrst sort the absolute values of their local gradient updates, and send the Top K% largest gradients
update dimensions to the server for aggregation. TopK suffers from the same problem as SignSGD:
for performance reasons, the server should send the entire updated model weights to the new selected
clients."
REFERENCES,0.5408970976253298,"B.5
MODEL POISONING ATTACK FOR ROBUSTNESS EVALUATIONS"
REFERENCES,0.5435356200527705,"To evaluate robustness of various FL algorithms, we use state-of-the-art model poisoning attack
proposed by Shejwalkar & Houmansadr (2021) in our robustness experiments. The attack proposes
a general FL poisoning framework and then tailors it to speciﬁc FL settings. It ﬁrst computes an
average ∇b of the available benign updates and perturbs it in a dynamic, data-dependent malicious
direction ω to compute the ﬁnal poisoned update ∇′ = ∇b + γω. DYN-OPT ﬁnds the largest γ that"
REFERENCES,0.5461741424802111,Under review as a conference paper at ICLR 2022
REFERENCES,0.5488126649076517,"Table 4: Performance of FSL with different hyperparameters trained on CIFAR10 (distributed over
1000 clients using Dirichlet distribution)."
REFERENCES,0.5514511873350924,"Method
hyperparameter
value
Test Accuracy with 10% malicious FSL"
REFERENCES,0.554089709762533,batch size
REFERENCES,0.5567282321899736,"6
78.4 (12.6)
8
79.0 (12.4)
16
76.4 (13.6)"
REFERENCES,0.5593667546174143,local epochs
REFERENCES,0.5620052770448549,"2
79.8 (12.2)
5
79.0 (12.4)
10
78.2 (12.6)"
REFERENCES,0.5646437994722955,learning rate
REFERENCES,0.5672823218997362,"0.1
73.5 (13.4)
0.2
82.4 (12.1)
0.3
83.11 (11.8)
0.4
79.0 (12.4)
0.5
77.5 (13.1)"
REFERENCES,0.5699208443271768,"FedAvg
-
-
10.0 (10.1)"
REFERENCES,0.5725593667546174,"TopK
-
-
10.0 (10.1)"
REFERENCES,0.575197889182058,FedAvg + Trimmed-mean
REFERENCES,0.5778364116094987,batch size
REFERENCES,0.5804749340369393,"6
55.5 (14.5)
8
56.3 (16.0)
16
37.7 (15.6)"
REFERENCES,0.58311345646438,local epochs
REFERENCES,0.5857519788918206,"2
41.0 (15.4)
5
56.3 (16.0)
10
21.0 (9.9)"
REFERENCES,0.5883905013192612,learning rate
REFERENCES,0.5910290237467019,"0.01
34.0 (15.5)
0.05
38.3 (15.3)
0.1
56.3 (16.0)
0.15
10.0 (10.0)
0.2
10.0 (10.0)"
REFERENCES,0.5936675461741425,FedAvg + Multi-Krum
REFERENCES,0.5963060686015831,batch size
REFERENCES,0.5989445910290238,"6
19.0 (12.5)
8
58.8 (15.8)
16
36.7 (14.8)"
REFERENCES,0.6015831134564644,local epochs
REFERENCES,0.604221635883905,"2
46.1 (15.9)
5
58.8 (15.8)
10
24.3 (11.7)"
REFERENCES,0.6068601583113457,learning rate
REFERENCES,0.6094986807387863,"0.01
15.3 (11.7)
0.05
50.0 (16.2)
0.1
58.8 (15.8)
0.15
15.4 (11.9)
0.2
10.0 (10.0)"
REFERENCES,0.6121372031662269,SignSGD
REFERENCES,0.6147757255936676,batch size
REFERENCES,0.6174142480211082,"6
33.1 (15.6)
8
39.7 (15.9)
16
10.2 (10.1)"
REFERENCES,0.6200527704485488,local epochs
REFERENCES,0.6226912928759895,"2
10.2 (10.5)
5
39.7 (15.9)
10
41.5 (16.0)"
REFERENCES,0.6253298153034301,learning rate
REFERENCES,0.6279683377308707,"0.01
44.2 (15.8)
0.05
41.9 (15.5)
0.1
39.7 (15.9)
0.15
35.8 (15.3)
0.2
10.2 (10.1)"
REFERENCES,0.6306068601583114,Under review as a conference paper at ICLR 2022
REFERENCES,0.633245382585752,"successfully circumvents the target AGR. DYN-OPT is much stronger, because unlike STAT-OPT,
it ﬁnds the largest γ and uses a dataset tailored ω."
REFERENCES,0.6358839050131926,"C
MISSING EXPERIMENTS"
REFERENCES,0.6385224274406333,"C.1
FSL: INITIALIZATION MATTERS"
REFERENCES,0.6411609498680739,"Table 5: Comparing the performance of FSL with different random weight initialization algorithms
with the performance of vanilla FedAvg for cross-device setting. Using Singed Kaiming Constant
as weight initialization gives the best performance for all the datasets."
REFERENCES,0.6437994722955145,"Dataset
Metric
Algorithm
FedAvg
FSL
Winit ∼
-
XN
NK
UK"
REFERENCES,0.6464379947229552,"MNIST
LeNet
N=1000"
REFERENCES,0.6490765171503958,"Mean
98.8
96.6
98.7
98.8
STD
3.1
5.2
3.2
3.1
Min
75.0
57.1
75.0
75.0
Max
100
100
100
100"
REFERENCES,0.6517150395778364,"CIFAR10
Conv8
N=1000"
REFERENCES,0.6543535620052771,"Mean
85.4
63.6
82.0
85.3
STD
11.2
15.6
11.9
11.3
Min
33.3
0
0
33.3
Max
100
100
100
100"
REFERENCES,0.6569920844327177,"FEMNIST
LeNet
N=3400"
REFERENCES,0.6596306068601583,"Mean
85.8
69.2
82.9
84.2
STD
10.2
14.2
11.1
10.7
Min
10.0
0
14.3
7.1
Max
100
100
100
100"
REFERENCES,0.662269129287599,"In FSL, the weight parameters are ﬁxed throughout the FSL protocol and they are initialized ran-
domly at the beginning of the protocol. It is very important to appropriately initialize the weights
since the clients will ﬁnd the subnetworks within these weights. We use three different distribution
for initializing the weight parameters as follows:"
REFERENCES,0.6649076517150396,"Glorot Normal (Glorot & Bengio, 2010) where we denote by XN. Previous work Zhou et al. (2019)
used this initialization to demonstrate that subnetworks of randomly weighted neural networks can
achieve impressive performance."
REFERENCES,0.6675461741424802,"Kaiming Normal (He et al., 2015) where we denote by Nk deﬁned as NK = N

0,
p"
REFERENCES,0.6701846965699209,"2/nℓ−1
"
REFERENCES,0.6728232189973615,where N shows normal distribution. nℓshows the number of parameters in the ℓth layer.
REFERENCES,0.6754617414248021,"Singed Kaiming Constant (Ramanujan et al., 2020) where all the wights are a constant σ but they
are assigned {+, −} randomly. This constant, σ, is the standard deviation of Kaiming Normal. We"
REFERENCES,0.6781002638522428,"show this initialization with UK as we are sampling from {−σ, +σ} where σ =
p"
REFERENCES,0.6807387862796834,"2/nℓ−1

."
REFERENCES,0.683377308707124,"Table 5 shows the results of running FSL for three datasets under the three aforementioned initializa-
tion algorithms. We compare FSL with FedAvg and report the mean, standard deviation, minimum,
and maximum of the accuracies for the clients’ local subnetwork (for FSL) and local models (for
FedAvg) at the end of FSL/FedAvg training. As we can see under three different random initial-
ization, using Signed Kaiming Normal (UK) results in better performance. We note from Table 5
that FSL with Signed Kaiming Normal (UK) initialization achieves performance very close to the
performance of FedAVg."
REFERENCES,0.6860158311345647,"Note that, since the FSL clients update scores in each round, unlike initialization of weights, initial-
ization of scores does not have signiﬁcant impact on the ﬁnal global subnetwork search. Therefore,
we do not explore different randomized initialization algorithms for scores and simply use Kaiming
Uniform initialization for scores."
REFERENCES,0.6886543535620053,Under review as a conference paper at ICLR 2022
REFERENCES,0.6912928759894459,"20
40
60
80
k % 96.5 97.0 97.5 98.0 98.5 99.0"
REFERENCES,0.6939313984168866,Test Accuracy (%)
REFERENCES,0.6965699208443272,"FedAvg
FSL"
REFERENCES,0.6992084432717678,(a) MNIST
REFERENCES,0.7018469656992085,"20
40
60
80
k % 20 40 60 80"
REFERENCES,0.7044854881266491,Test Accuracy (%)
REFERENCES,0.7071240105540897,"FedAvg
FSL"
REFERENCES,0.7097625329815304,(b) CIFAR10
REFERENCES,0.712401055408971,"20
40
60
80
k % 72.5 75.0 77.5 80.0 82.5 85.0"
REFERENCES,0.7150395778364116,Test Accuracy (%)
REFERENCES,0.7176781002638523,"FedAvg
FSL"
REFERENCES,0.7203166226912929,(c) FEMNIST
REFERENCES,0.7229551451187335,"Figure 4: Comparing performance of FSL for different subnetwork sizes. k (x-axis) shows the %
of weights that each client is including in its subnetwork, test accuracy (y-axis) shows the mean of
accuracies for all the clients on their test data. The chosen clients in each round send all the ranks to
the server. FSL with subnetworks of ∈[40%, 70%] result in better performances."
REFERENCES,0.7255936675461742,"Ramanujan et al. (2020) also considered these three initialization to ﬁnd the best subnetwork in
centralized machine learning setting. They also showed that using Singed Kaiming Normal gives
the best supermasks. Our results align with their conclusions, hence we use Singed Kaiming Normal
to initialize the weights and Kaiming Uniform to initialize the scores of global supernetwork."
REFERENCES,0.7282321899736148,"C.2
PERFORMANCES OF FSL WITH VARYING SIZES OF SUBNETWORKS"
REFERENCES,0.7308707124010554,"In FSL, each client uses Edge-Pop Algorithm (Ramanujan et al., 2020) and their local data to ﬁnd a
local subnetwork within a randomly initialized global network, which we call supernetwork. Edge-
Pop algorithm use parameter k which represents the % of all the edges in a supernetwork which will
remain in the ﬁnal subnetwork. For instance, k = 50% denotes that each client ﬁnds a subnetwork
within a supernetwork that has half the number of edges as in the supernetwork."
REFERENCES,0.7335092348284961,"Figure 4 illustrates how the performance of the global subnetwork in FSL varies with the size of
subnetwork; note that, all of the clients collaborate to ﬁnd the global subnetwork. We train nine
FSL models with k ∈{10, 20, 30, 40, 50, 60, 70, 80, 90}% and a FedAvg model (shown using a
horizontal line); FedAvg model updates all the weights, hence it is a supermask with k = 100%."
REFERENCES,0.7361477572559367,"C.3
PERFORMANCES OF FSL WITH DIFFERENT HETEROGENEOUS DATA DISTRIBUTION
METHODS"
REFERENCES,0.7387862796833773,"Table 6 shows the performances of FSL and FedAvg using different methods of non-iid assignment.
We distribute the data between 1000 clients with two methods: (I) Dirichlet distribution with β = 1
similar to (Reddi et al., 2020; Hsu et al., 2019) and (II) each client has data of 2 random classes sim-
ilar to (McMahan et al., 2017). In this table, we can see that FSL can achieve the same performance
of FedAvg in different heterogeneous data distributions."
REFERENCES,0.741424802110818,"D
MISSING DETAILS OF EDGE-POPUP AND FSL ALGORITHM"
REFERENCES,0.7440633245382586,"Suppose in a fully connected neural network, there are L layers and layer ℓ∈[1, L] has nℓneu-
rons, denoted by V ℓ= {V ℓ
1 , ..., V ℓ
nℓ}. If Iv and Zv denote the input and output for neuron v
respectively, then the input of the node v is the weighted sum of all nodes in previous layer, i.e.,
Iv = P"
REFERENCES,0.7467018469656992,"u∈V ℓ−1 WuvZu. Here, Wuv is the weight of the edge connecting u to v. Edge-popup
algorithm tries to ﬁnd subnetwork E, so the input for neuron v would be: Iv = P"
REFERENCES,0.7493403693931399,"(u,v)∈E WuvZu."
REFERENCES,0.7519788918205804,"Updating scores. Consider an edge Euv that connects two neurons u and v, Wuv be the weight of
Euv, and suv be the score assigned to the edge Euv by Edge-popup algorithm. Then the edge-popup
algorithm removes edge Euv from the supermask if its score suv is not high enough. Each iteration
of supermask training updates the scores of all edges such that, if having an edge Euv in subnetwork
reduces loss (e.g., cross-entropy loss) over training data, the score suv increases."
REFERENCES,0.7546174142480211,Under review as a conference paper at ICLR 2022
REFERENCES,0.7572559366754618,"Table 6: Comparing the performance of FSL and FedAvg for cross-device setting using two methods
of data assignment. We distribute the data between 1000 clients with two methods: (I) Dirichlet
distribution with β = 1 and (II) each client has data of 2 random classes."
REFERENCES,0.7598944591029023,"Dataset
Type of Non-IID
Metric
Algorithm
FedAvg
FSL"
REFERENCES,0.762532981530343,"MNIST
LeNet
N=1000"
REFERENCES,0.7651715039577837,Dirichlet Distribution β = 1
REFERENCES,0.7678100263852242,"Mean
98.8
98.8
STD
3.1
3.1
Min
75.0
75.0
Max
100
100"
REFERENCES,0.7704485488126649,Randomly 2 classes assigned to each client
REFERENCES,0.7730870712401056,"Mean
98.4
98.3
STD
4.3
4.1
Min
70.0
80.0
Max
100
100"
REFERENCES,0.7757255936675461,"CIFAR10
Conv8
N=1000"
REFERENCES,0.7783641160949868,Dirichlet Distribution β = 1
REFERENCES,0.7810026385224275,"Mean
85.4
85.3
STD
11.2
11.3
Min
33.3
33.3
Max
100
100"
REFERENCES,0.783641160949868,Randomly 2 classes assigned to each client
REFERENCES,0.7862796833773087,"Mean
70.6
70.9
STD
21.9
19.2
Min
0
10.0
Max
100
100"
REFERENCES,0.7889182058047494,"The algorithm selects top k% edges (i.e., ﬁnds a subnetwork with sparsity of k%) with highest
scores, so Iv reduces to Iv = P"
REFERENCES,0.7915567282321899,"u∈V ℓ−1 WuvZuh(suv) where h(.) returns 1 if the edge exists in top-
k% highest score edges and 0 otherwise. Because of existence of h(.), which is not differentiable,
it is impossible to compute the gradient of loss with respect to suv. Recall that, the edge-popup
algorithm use straight-through gradient estimator (Bengio et al., 2013) to compute gradients. In
this approach, h(.) will be treated as the identity in the backward pass meaning that the upstream
gradient (i.e.,
∂L
∂Iv ) goes straight-through h(). Now using chain rule, we can derive
∂L
∂Iv
∂Iv
∂suv =
∂L
∂Iv WuvZuwhere L is the loss to minimize. Then we can SGD with step size η to update scores as
suv ←−suv −η ∂L"
REFERENCES,0.7941952506596306,∂Iv ZuWuv.
REFERENCES,0.7968337730870713,"Ramanujan et al. (2020) proved that when edge (a, b) replaces (c, b) in layer ℓand the rest of the
subnetwork remains ﬁxed then the loss of the supermask learning decreases (provided the loss is
sufﬁciently smooth). Motivated by their proof, we can show when these two edges are swapped in
FSL, the loss decreases for FSL optimization too."
REFERENCES,0.7994722955145118,"Theorem 1: when edge (a, b) replaces (c, b) in layer ℓand the rest of the subnetwork remains ﬁxed
then the loss of the FSL optimization will decrease (provided the loss is sufﬁciently smooth)."
REFERENCES,0.8021108179419525,"proof. First, we know that the optimization problem of FSL is as follow:"
REFERENCES,0.8047493403693932,"min
Rg F(θw, Rg) = min
Rg N
X"
REFERENCES,0.8073878627968337,"i=1
λiLi(θw ⊙m) s.t.
(5)"
REFERENCES,0.8100263852242744,"m[Rg < t] = 0 and m[Rg >= t] = 1
(6)"
REFERENCES,0.8126649076517151,where λi shows the importance of the ith client in empirical risk minimization which λi = 1
REFERENCES,0.8153034300791556,"N gives
same importance to all the participating clients. m is the ﬁnal mask that contains the edges of top
ranks, and Li is the loss function for the ith client. θw ⊙m shows the subnetwork inside the random
θw that all clients unanimously vote for. In this optimization, the FSL clients try to minimize F by
ﬁnding the best global ranking Rg."
REFERENCES,0.8179419525065963,"We now wish to show F(θw, Rt+1
g
) < F(θw, Rt
g) when in FSL round t + 1, the edge (a, b) replaces
(c, b) in layer ℓand the rest of the subnetwork remains ﬁxed. Suppose global rank of edge (a, b) was
Rt
g[(a, b)] and global rank of edge (c, b) was Rt
g[(c, b)] in round t, so we have:"
REFERENCES,0.820580474934037,Under review as a conference paper at ICLR 2022
REFERENCES,0.8232189973614775,"Rt
g[(a, b)] < Rt
g[(c, b)]
(7)"
REFERENCES,0.8258575197889182,"Rt+1
g
[(a, b)] > Rt+1
g
[(c, b)]
(8)"
REFERENCES,0.8284960422163589,"where the order of all remaining global ranks remain ﬁxed, and only these two edges are swapped
in global ranking. Now let st,i
ab shows the score of weight wab in round t and client ith and st+1,i
ab
shows the updated score of it after local training. As in our majority vote, we are calculating the
sum of the reputation of edges we will have: N
X"
REFERENCES,0.8311345646437994,"i=1
st,i
ab < N
X"
REFERENCES,0.8337730870712401,"i=1
st,i
cb
(9) N
X"
REFERENCES,0.8364116094986808,"i=1
st+1,i
ab
> N
X"
REFERENCES,0.8390501319261213,"i=1
st+1,i
cb
(10)"
REFERENCES,0.841688654353562,We also know that Edge-popup algorithm updates the scores in the ith client as follow:
REFERENCES,0.8443271767810027,"st+1,i
ab
= st,i
ab −η ∂L"
REFERENCES,0.8469656992084432,"∂Ia
ZaWab
(11)"
REFERENCES,0.8496042216358839,"Based on 9, 10 and 11, we can say: N
X"
REFERENCES,0.8522427440633246,"i=1
st,i
ab − N
X"
REFERENCES,0.8548812664907651,"i=1
st,i
cb < N
X"
REFERENCES,0.8575197889182058,"i=1
st+1,i
ab
− N
X"
REFERENCES,0.8601583113456465,"i=1
st+1,i
cb
(12)"
REFERENCES,0.862796833773087,"We also know that: N
X i=1"
REFERENCES,0.8654353562005277,"
st+1,i
ab
−st,i
ab

= N
X i=1"
REFERENCES,0.8680738786279684,"
−η ∂Li"
REFERENCES,0.8707124010554089,"∂Iia
Zi
aWab"
REFERENCES,0.8733509234828496,"
(13) N
X i=1"
REFERENCES,0.8759894459102903,"
st+1,i
cb
−st,i
cb

= N
X i=1"
REFERENCES,0.8786279683377308,"
−η ∂Li"
REFERENCES,0.8812664907651715,"∂Iic
Zi
cWcb"
REFERENCES,0.8839050131926122,"
(14)"
REFERENCES,0.8865435356200527,"Based on 12, 13 and 14, we can say: N
X i=1 ∂Li"
REFERENCES,0.8891820580474934,"∂Iic
Zi
cWcb 
> N
X i=1 ∂Li"
REFERENCES,0.8918205804749341,"∂Iia
Zi
aWab"
REFERENCES,0.8944591029023746,"
(15)"
REFERENCES,0.8970976253298153,"So based on 15, and what Ramanujan et al. (2020) proved for each supermask training we can
show 16. We assume that loss is smooth and the input to the nodes that their edges are swapped are
close before and after the swap. N
X i=1"
REFERENCES,0.899736147757256," 
Li(θw ⊙mt+1)

< N
X i=1"
REFERENCES,0.9023746701846965," 
Li(θw ⊙mt)

(16)"
REFERENCES,0.9050131926121372,Under review as a conference paper at ICLR 2022
REFERENCES,0.9076517150395779,"0
2000
4000
6000
8000
10000
Number of Parameters 0 10 20 30 40"
REFERENCES,0.9102902374670184,Communicication Cost(KB)
REFERENCES,0.9129287598944591,"FSL(Lowerbound)
FedAvg
FSL"
REFERENCES,0.9155672823218998,"SFSL(50%)
SFSL(10%)
SignSGD"
REFERENCES,0.9182058047493403,"Figure 5: Communication cost Analysis. Please note that the download communication cost of all
SFSLs would be the same as FSL."
REFERENCES,0.920844327176781,"that means:
F(θw, Rt+1
g
) < F(θw, Rt
g)
(17)"
REFERENCES,0.9234828496042217,"E
MISSING DETAILS ABOUT COMMUNICATION COST COMPARISON"
REFERENCES,0.9261213720316622,"One of the features of the FSL training is its communication efﬁciency. In Section 5.2, we show that
if the FSL clients send and receive rankings, the communication cost will be P"
REFERENCES,0.9287598944591029,"ℓ∈[L] nℓ× log(nℓ)
bits per client. In this section, we are providing a lower bound on the FSL communication cost, and
then compare it with FedAvg and SignSGD."
REFERENCES,0.9313984168865436,"Lowerbound of communication cost of FSL: Since the FSL clients send and receive layer-wise
rankings of indices, i.e., integers ∈[0, nℓ−1], for layer ℓ, there are nℓ! possible permutations for
layer ℓ∈[L]. If we use the best possible compression method in FSL, an FSL client needs to
send and receive P"
REFERENCES,0.9340369393139841,"ℓ∈[L] log(nℓ!) bits. Therefore, the download and upload bandwidth for each FSL
client would be P"
REFERENCES,0.9366754617414248,ℓin[L] log (nℓ∗(nℓ−1) ∗... ∗2 ∗1) = P
REFERENCES,0.9393139841688655,"ℓ∈[L]
Pnℓ
i=1 log(i) bits. Please note
that in our experiment, FSL clients send and receive the rankings without any further compression,
and P"
REFERENCES,0.941952506596306,"ℓ∈[L]
Pnℓ
i=1 log(i) just shows a lower-bound of communication cost of FSL. In Section 6.1,
we measure the performance and communication cost of FSL with other existing FL compressors
signSGD (Bernstein et al., 2019) and TopK (Aji & Heaﬁeld, 2017; Alistarh et al., 2018a). In Fig-
ure 5, we compare the communication cost of one client per FL round for FedAvg, SignSGD, and
different variant of FSL for different number of parameters."
REFERENCES,0.9445910290237467,"Similar work in this domain is LotteryFL (Li et al., 2020a), a personalization framework that each
FL client learns a lottery ticket network (LTN) by pruning the base model using Lottery Ticket
hypothesis (Frankle & Carbin, 2019). In LotteryFL, each client sends and receives the update for
its subnetwork, and at the end, they have an extra step for personalization. FSL is different from
LotteryFL as the FSL clients ﬁnd subnetworks within a random and ﬁxed network and send the
ranks of their subnetwork edges instead of what LotteryFL clients do that train their weights and
ﬁnd a subnetwork by freezing some weights and send their actual model update. LotteryFL is based
on FedAvg that the clients can send any update to the server, which is vulnerable to the same attacks
that existed for FedAVG. In terms of communication cost, FSL is very close to LotteryFL as they
report 1.81x improvement over CIFAR10 which is close to FSL and SFSL(50%) which provide
1.53x, 3.09x improvement respectively over CIFAR10."
REFERENCES,0.9472295514511874,"F
ADDITIONAL COMPARISONS"
REFERENCES,0.9498680738786279,"Figure 6 is showing the learning curve of FSL for different numbers of local epochs for CIFAR10
experiment. On the x-ais we have accumulated communication cost: e × 13.1 × 2 × 25 MB where
e is the FSL round, 13.1MB is the cost of FSL per client, 2 is for download+upload cost, and 25
clients are selected in each round."
REFERENCES,0.9525065963060686,Under review as a conference paper at ICLR 2022
REFERENCES,0.9551451187335093,"Table 7: The effect of other settings on performance of FSL trained on CIFAR10 distributed over
1000 clients using Dirichlet distribution. The bold shows the value we used in our experiments."
REFERENCES,0.9577836411609498,"Method
hyperparameter
value
Test Accuracy with 10% malicious FSL"
REFERENCES,0.9604221635883905,Number of participants (n)
REFERENCES,0.9630606860158312,"15
84.8 (11.3)
25
85.3 (11.3)
50
84.9 (11.2)"
REFERENCES,0.9656992084432717,local epochs (E)
REFERENCES,0.9683377308707124,"2
82.2 (12.0)
5
85.3 (11.3)
10
83.5 (11.9)"
REFERENCES,0.9709762532981531,Non-iid degree (β)
REFERENCES,0.9736147757255936,"1
85.3 (11.3)
10
85.6 (11.1)
100
85.6 (10.9)"
REFERENCES,0.9762532981530343,"0
200
400
600
800
1000
1200
Accumulated Communicaiton cost (GB) 10 20 30 40 50 60 70 80"
REFERENCES,0.978891820580475,Test Accuracy (%)
REFERENCES,0.9815303430079155,"FSL (E=2)
FSL (E=5)
FSL (E=10)"
REFERENCES,0.9841688654353562,(a) CIFAR10 (Test Accuracy)
REFERENCES,0.9868073878627969,"0
200
400
600
800
1000
1200
Accumulated Communicaiton cost (GB) 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25"
REFERENCES,0.9894459102902374,Test Loss
REFERENCES,0.9920844327176781,"FSL (E=2)
FSL (E=5)
FSL (E=10)"
REFERENCES,0.9947229551451188,"(b) CIFAR10 (Test Loss)
Figure 6: Comparing performance of FSL for different local epochs."
REFERENCES,0.9973614775725593,"Table 7 is showing the effect of other settings on performance of FSL trained on CIFAR10 distributed
over 1000 clients using Dirichlet distribution. The bold shows the value we used in our experiments."
