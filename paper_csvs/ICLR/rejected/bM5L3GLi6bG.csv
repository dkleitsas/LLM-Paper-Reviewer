Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.010101010101010102,"Open set domain adaptation focuses on transferring the information from a richly
labeled domain called source domain to a scarcely labeled domain called target
domain while classifying the unseen target samples as one unknown class in an un-
supervised way. Compared with the close set domain adaptation, where the source
domain and the target domain share the same class space, the classiﬁcation of the
unknown class makes it easy to adapt to the realistic environment. Particularly,
after the recognition of the unknown samples, the robot can either ask for man-
ually labeling or further develop the classiﬁcation ability of the unknown classes
based on pre-stored knowledge. Inspired by this idea, in this paper we propose
a model for open set domain adaptation with zero-shot learning on the unknown
classes. We utilize adversarial learning to align the two domains while rejecting
the unknown classes. Then the knowledge graph is introduced to generate the
classiﬁers for the unknown classes with the employment of the graph convolution
network (GCN). Thus the classiﬁcation ability of the source domain is transferred
to the target domain and the model can distinguish the unknown classes with prior
knowledge. We evaluate our model on digits datasets and the result shows superior
performance."
INTRODUCTION,0.020202020202020204,"1
INTRODUCTION"
INTRODUCTION,0.030303030303030304,"In the last decades, deep learning models have shown good performance in various tasks, especially
in visual perception. The training of the deep learning network relies on plenty of labeled data.
However, most of the existing large labeled datasets are collected from the Internet. The images in
these datasets are normative and uniﬁed, which are different from the images relevant for a speciﬁc
application. Besides, depending on the application, the images may be obtained by different typed
of visual sensors or with a different perspective of sensors. It costs a lot to retrain the classiﬁcation
model in different situations. In some typical applications, the samples in the real world are hard
to gather or too large to label. Thus it is important to deal with the gap among domains. They
should be able to utilize the well-labeled samples in the source domain to classify the samples in the
unlabeled target domain which is related to domain adaptation. There are already some researches
on domain adaptation, such as Ganin & Lempitsky (2015), Long et al. (2015), Long et al. (2016),
and Wang & Deng (2018). The alignment of the domain gap makes the robot adapt well to dynamic
and unstructured environments."
INTRODUCTION,0.04040404040404041,"Except for the domain gap among different datasets, the variation of the classes also makes it hard
for the model to adapt to a new dataset. Depending on the application and the scale of different
datasets, the model may come across classes that are not contained in the source domains. With
the traditional domain adaptation methods, the unknown classes are mistakenly aligned due to the
absence of training samples of unknown classes in the source domain. The imbalance of the types
of classes brings over-ﬁtting problems and is not suitable for classiﬁcation in the open world. Thus
it is important for the robot to reject the unknown classes and only align the shared classes. This
problem is known as open set domain adaptation, which is ﬁrst proposed by Panareda Busto & Gall
(2017) and followed by for instance Saito et al. (2018), Busto et al. (2018), and Liu et al. (2019).
In the setting of the open set domain adaptation, the target domain contains both the classes of the
source domain and the additional new classes. The model not only aligns the target domain to the
source domain but also rejects the unknown classes."
INTRODUCTION,0.050505050505050504,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.06060606060606061,"Figure 1: An overview of the proposed domain adaptation with zero-shot learning. Close set domain
adaptation align the target domain to the source domain. Open set domain adaptation not only align
the domain gap, but also reject the unknown classes as one class. Open set domain adaptation
with zero-shot learning further gives detailed classiﬁcation on the unknown classes, which is more
complex and valuable."
INTRODUCTION,0.0707070707070707,"It is worth noting that previous open set domain adaptation methods typically classify all the addi-
tional new classes into one unknown class. However, the unknown class may contain classes that
are worth learning. It may be more valuable to detect the unknown classes in detail and develop the
ability to classify them with the former information. With the process of distinction and transfer-
ring, the model can expand its visual recognition ability with little labeled information. Since the
unknown classes are not included in the source domain, the model lacks the labeled information for
the new classes. Current open set domain adaptation methods can not give detailed classiﬁcation
on the unknown part with no labeled images. This problem is related to zero-shot learning. In the
zero-shot learning problem, complementary information is collected to transfer the knowledge from
the base classes to classify the unknown ones. Inspired by this, with the knowledge stored in the
knowledge graph, the classiﬁers of the unknown classes can be obtained in the target domain with
no labeled samples."
INTRODUCTION,0.08080808080808081,"Towards this end, we propose a generic model to align the gap between the labeled source domain
and the unlabeled target domain while classifying the unknown classes in the target domain. The
contributions of this paper mainly lies in tackling the following two difﬁculties."
INTRODUCTION,0.09090909090909091,"First, since the unknown classes are not contained in the source domain, we have no labeled samples
for supervised training. The lack of labeled data may cause overﬁtting problem of the model, which
means the model only classify the samples as the known classes and can not classify the unknown
ones. It is necessary to utilize complementary information to support the inference. Thus we employ
the knowledge graph to stores some prior knowledge of the known classes and the unknown classes,
which contains the structural relations between different classes, beyond the individual attribute
representation of each class. The structural information offers a bridge for the inference from the
known classes to the unknown ones. With the employment of the graph convolution network, the
information propagates among the graph and the unknown classes gather the information from their
neighbor to generate their classiﬁers. These inference classiﬁers work as the initial classiﬁers of the
classiﬁcation model."
INTRODUCTION,0.10101010101010101,"The second difﬁculty is how to adapt the inference classiﬁers to the target domain. Since we only
have labeled samples in the source domain, the inference classiﬁers are suitable to the source do-
main. It is not able to classify the unknown samples in the target domain because of the domain
gap. Thus we introduce adversarial learning to align the domain gap. The classiﬁcation model con-
sists of two modules, the feature generator, and the classiﬁer. Since the generator works to extracts
the features of the samples and the classiﬁer works to output the class probability, we train them
simultaneously in an adversarial way. The classiﬁer is trained to found a boundary for the unknown
classes while the generator is trained to make the samples far from the boundary. With adversarial
learning, the generator can deceive the classiﬁer to generate aligned features in both domains and re-
ject the unknown classes according to the unknown boundary. Thus the feature of the shared classes"
INTRODUCTION,0.1111111111111111,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.12121212121212122,"is aligned in both domains and the unknown classes are rejected as one class. With the adaptation
in both domain gap and class gap, our model is able to classify objects in the dynamic and complex
open world."
INTRODUCTION,0.13131313131313133,"Figure 2: An overview of our model for domain adaptation with zero-shot learning. We utilize a
knowledge graph to infer the classiﬁer of the unknown classes. With the employment of the graph
convolutional network, the model generates the initial classiﬁer for all classes and applies it to the
classiﬁcation part. The generator and the classiﬁer are trained in an adversarial way to align the
domain gap."
INTRODUCTION,0.1414141414141414,"We utilize the knowledge graph and the adversarial learning in a jointly trained framework. The
two parts work together to align the shared classes in two spaces while generating classiﬁers for
the unknown classes in the target domain. We further evaluate our method on digits datasets and
demonstrate its effectiveness."
RELATED WORKS,0.15151515151515152,"2
RELATED WORKS"
OPEN SET DOMAIN ADAPTATION,0.16161616161616163,"2.1
OPEN SET DOMAIN ADAPTATION"
OPEN SET DOMAIN ADAPTATION,0.1717171717171717,"Open set domain adaptation goes beyond traditional close set domain adaptation. It considers a
more realistic classiﬁcation task, in which the target domain contains unknown samples that are
not present in the source domain. Open set domain adaptation is ﬁrst proposed by Busto et al.
(2018). They measure the distance between the target sample and the center of the source class to
decide whether a target sample belongs to one of the source classes or the unknown class. However,
they require the source domain to have unknown samples as well. Later on, Saito et al. (2018)
propose open set back-propagation (OSBP) for source domain with no unknown samples. They
utilize adversarial learning to train the feature generator and classiﬁer. As the classiﬁer tries to set a
boundary for the unknown classes, the feature generator tries to deceive it. However, both of them
only separate the unknown classes in the target domain, but can not give detailed classiﬁcation on
the unknown ones. The learnable information in the unknown space deserves deep exploitation. We
have found few papers that consider the ﬁne-grained classiﬁcation of the unknown classes in open
set domain adaptation, we aim to ﬁll in the blanks."
ZERO-SHOT LEARNING,0.18181818181818182,"2.2
ZERO-SHOT LEARNING"
ZERO-SHOT LEARNING,0.1919191919191919,"Zero-shot learning aims at generating classiﬁers for unknown classes with no labeled samples. Sev-
eral pieces of research have been done on this area, such as Kipf & Welling (2016) Xian et al.
(2017). Due to the limitation of the available samples, some researchers extract complementary
information from the related known classes to support the inference of the unknown ones. Among
these methods, building the relationship between classes in form of a graph seems more reasonable.
The special geometry of graphs well shows the complicated relationship and the unknown classes
can gather adequate information from the known ones. Current zero-shot learning please refer to"
ZERO-SHOT LEARNING,0.20202020202020202,Under review as a conference paper at ICLR 2022
ZERO-SHOT LEARNING,0.21212121212121213,"knowledge graphs for inference. Wang et al. (2018) built an unweighted knowledge graph com-
bined with word embedding upon the graph convolutional network. With information propagation,
novel nodes generate predictive classiﬁers with common sense. Kampffmeyer et al. (2019) improve
upon this model and propose a dense graph propagation to prevent dilution of knowledge from dis-
tant nodes. Knowledge graphs intuitively present the stored information for the visual cognitive
development."
GRAPH CONVOLUTIONAL NETWORK,0.2222222222222222,"2.3
GRAPH CONVOLUTIONAL NETWORK"
GRAPH CONVOLUTIONAL NETWORK,0.23232323232323232,"Graph convolutional network (GCN) is a kind of graph neural network proposed by Kipf & Welling
(2016). GCN enables the nodes in the graph to share the intensity of statistics, which improves the
efﬁciency of sampling. GCN can also be employed in non-euclidean space. From the perspective
of the spatial domain, GCN iteratively aggregates neighborhood information. The propagation of
information on the graph further exploits the structural information on the graph. GCN is ﬁrst
introduced by Bruna et al. (2013). Then GCN is extended with a ﬁltering method based on the
recurrent Chebyshev polynomial. The improvement reduces the computational complexity a lot,
which is equivalent to common CNNs in image operating. Kipf & Welling (2016) further simplify
the framework to improve the scalability and robustness. They employed their model on the semi-
supervised learning on the graphs. Our model employs the framework of GCN to propagate the
complementary information among nodes for the inference of classiﬁers."
APPROACH,0.24242424242424243,"3
APPROACH"
PROBLEM DEFINITION,0.25252525252525254,"3.1
PROBLEM DEFINITION"
PROBLEM DEFINITION,0.26262626262626265,"In open set domain adaptation with zero-shot learning, we have a source domain Ds
=
{(xs
i, ys
i )}ns
i=1, which contains ns labeled samples, and a target domain Dt = {xt
j}nt
j=1, which
contains nt unlabeled samples. The class space in the source domain is Cs which we call known
classes. Cs also is shared by the class space of the target domain Ct. It is worth noting that Ct further
contains additional unknown classes Cu, that is Ct = Cs ∨Cu. The source domain is sampled from
the distribution qs, while the target domain is sampled from the distribution qt. In close set domain
adaptation, qs ̸= qt. In open set domain adaptation with zero-shot learning, we also deﬁne qs ̸= qs
t .
qs
t refers to the distribution of the known classes in the target domain. Note that the samples in the
target domain are all unlabeled and the samples in the source domain are all labeled."
CLASSIFIER INFERENCE MODULE,0.2727272727272727,"3.2
CLASSIFIER INFERENCE MODULE"
CLASSIFIER INFERENCE MODULE,0.2828282828282828,"With few labeled samples, the human can make good inferences on unfamiliar things with the
related information that they obtain from books. Our model also extracts the task-based knowl-
edge from a prestored knowledge graph. The inference graph is denoted as G = (V, E), where
V = {v1, v2, ..., vns, ..., vnt} is a node-set of all classes Ct. ns refers to the number of known
classes. nt −ns refers to the number of unknown classes. The nodes in the prestored knowledge
contain the attributes vi of different classes. E = {ei,j = (vi, vj)} is an edge set referring to the re-
lationship among the graph. The edges in the prestored knowledge graph are based on the similarity
of the attributes between different classes."
CLASSIFIER INFERENCE MODULE,0.29292929292929293,"Since we only have labeled data of the source domain. We ﬁrst train the recognition model on the
source domain Ds. Speciﬁcally, the pre-trained recognition model is denoted as C(F(·|θ)|w). The
model consists of two parts, feature extractor F(·|θ) and class classiﬁer C(·|w). θ and w indicate
the parameters of the model trained with Ds = {(x1, y1), ..., (xns, ys)}. The symbol xi refers to the
source images of the ith class while yi refers to their label. Feature extractor F(x|θ) takes an image
as input and ﬁgures out the feature vector of it as zi. The ﬁnal classiﬁcation score is computed as"
CLASSIFIER INFERENCE MODULE,0.30303030303030304,"[s1, s2, ..., sM] = [zT w1, zT w2, ..., zT wns]
(1)"
CLASSIFIER INFERENCE MODULE,0.31313131313131315,"Thus the inference of the classiﬁers on unknown classes is to inference the classiﬁcation weights ws
on the unknown classes with the inference graph."
CLASSIFIER INFERENCE MODULE,0.32323232323232326,"With the framework of the graph convolutional network, Our model propagates information among
nodes by exploring the class relationship. For one layer in GCN, a node aggregates information"
CLASSIFIER INFERENCE MODULE,0.3333333333333333,Under review as a conference paper at ICLR 2022
CLASSIFIER INFERENCE MODULE,0.3434343434343434,"from the neighbors connected to it. GCN can also be extended to multiple layers to perform a
deeper spread. Therefore, the unknown classes can utilize the information from the related known
classes and predict the classiﬁcation weights of their own. The mechanism of GCN is described as"
CLASSIFIER INFERENCE MODULE,0.35353535353535354,H(l+1) = ReLu( ˆD−1
CLASSIFIER INFERENCE MODULE,0.36363636363636365,2 ˆE ˆD−1
CLASSIFIER INFERENCE MODULE,0.37373737373737376,"2 H(l)U (l))
(2)"
CLASSIFIER INFERENCE MODULE,0.3838383838383838,"where H(l) denotes the output of the lth layer, while for the ﬁrst layer H0 = V . It uses Leaky ReLu
as the nonlinear activation function. To reserve the self-information of the nodes, self-loops are
added among the propagation, ˆE = E + I, where E ∈RN×N is the symmetric adjacency matrix
and I ∈RN×N represents identity matrix. Dii = P
j Eij normalizes rows in E to prevent the scale
of input modiﬁed by E. The matrix U l is the weight matrix of the lth layer, which GCN regulates
constantly to achieve better performance."
CLASSIFIER INFERENCE MODULE,0.3939393939393939,"Our model conducts two layers of GCN on the inference graph. Unknown classes learn the mecha-
nism of end-to-end learning from known classes through propagation. The inference graph is trained
to minimize the loss between the predicted classiﬁcation weights and the ground-truth weights. The
ground-truth weights refer to the classiﬁers of the known classes, which are extracted from the pre-
trained model on the source domain."
CLASSIFIER INFERENCE MODULE,0.40404040404040403,"LGCN = 1 M M
X"
CLASSIFIER INFERENCE MODULE,0.41414141414141414,"i=1
(winf
i
−wtrain
i
)2
(3)"
CLASSIFIER INFERENCE MODULE,0.42424242424242425,"where winf refers to the output of known classes on GCN. wtrain denotes the ground truth classiﬁers
of the known classes obtained from the pre-trained model. With the supervision of the known
classes, the unknown nodes in the inference graph can also generate classiﬁer weights of their own.
Finally, with the employment of GCN, the classiﬁer inference module not only generates predictive
classiﬁers of the unknown classes in the source domain but also provides more general classiﬁers of
the known ones."
DOMAIN ADAPTATION MODULE,0.43434343434343436,"3.3
DOMAIN ADAPTATION MODULE"
DOMAIN ADAPTATION MODULE,0.4444444444444444,"The classiﬁer inference module generates classiﬁers for the unknown classes. However, these clas-
siﬁers are suitable to the source domain, since the ground-truth classiﬁers are extracted from the
model trained on the labeled samples. Thus the domain adaptation module attempts to align the
domain gap between the source domain and target domain."
DOMAIN ADAPTATION MODULE,0.45454545454545453,"We apply the inference classiﬁers to the pre-trained classiﬁcation model. The number of the clas-
siﬁers of the pre-trained classiﬁcation model changes from ns to nt. As mentioned above, the
classiﬁcation model consists of two parts, the feature generator, and the classiﬁer. To align the do-
main gap, we employ adversarial learning on the classiﬁer and the feature generator. The classiﬁer
aims at setting a boundary for the unknown classes in the target domain. With the boundary, the
unknown classes can be picked out. The boundary refers to the proportion of the unknown classes
in the target domain.
pun = p(y = yun|xt) = t
(4) yun = nt
X"
DOMAIN ADAPTATION MODULE,0.46464646464646464,"i=ns+1
p(y = yi|xt)
(5)"
DOMAIN ADAPTATION MODULE,0.47474747474747475,"where yun refers to the proportion of unknown classes, t refers to the boundary. The feature gener-
ator tries to generate features that can deceive the classiﬁer. That is, the objective of the generator is
to maximize the error of the classiﬁer. To increase the error, the generator tries to generate features
far from the boundary. Besides, the classiﬁcation ability on the known classes should be reserved,
thus we also consider the classiﬁcation accuracy on the source domain during the training process.
We use a standard cross-entropy loss for this purpose."
DOMAIN ADAPTATION MODULE,0.48484848484848486,"Ls(xs, ys) = −log(p(y = ys|xs))
(6)
p(y = ys|xs) = (C(F(xs)))ys
(7)"
DOMAIN ADAPTATION MODULE,0.494949494949495,"With the cross-entropy loss, the model ensure the classiﬁcation accuracy on known classes. For
the boundary of the unknown classes, we follow the settings in the OSBP and utilize binary cross"
DOMAIN ADAPTATION MODULE,0.5050505050505051,Under review as a conference paper at ICLR 2022
DOMAIN ADAPTATION MODULE,0.5151515151515151,"entropy loss.
Ladv(xt) = −t log(pun) −(1 −t) log(1 −pun))
(8) pun = nt
X"
DOMAIN ADAPTATION MODULE,0.5252525252525253,"i=ns+1
p(y = yi|xt)
(9)"
DOMAIN ADAPTATION MODULE,0.5353535353535354,"The overall objective of our model is,"
DOMAIN ADAPTATION MODULE,0.5454545454545454,"min
C Ls(xs, ys) + Ladv(xt) + LGCN
(10)"
DOMAIN ADAPTATION MODULE,0.5555555555555556,"min
G Ls(xs, ys) −Ladv(xt) + LGCN
(11)"
DOMAIN ADAPTATION MODULE,0.5656565656565656,"With the domain adaptation module, the unknown classes in the target domain are separated and the
features of both domains are aligned. We also suggest iterating the classiﬁer inference module and
the domain adaptation module for better performance."
EXPERIMENT,0.5757575757575758,"4
EXPERIMENT"
EXPERIMENT,0.5858585858585859,"In this section, we perform experiments to evaluate the effectiveness of our model. Since there is no
related model on the open set domain adaptation problem with zero-shot learning, we demonstrate
our model with current zero-shot learning models and make a detailed analysis of the results."
DATASETS,0.5959595959595959,"4.1
DATASETS"
DATASETS,0.6060606060606061,"We test our model on three digits datasets. Compared to the traditional dataset on domain adaptation.
The digits datasets contain a fewer number of classes, which means the number of the known classes
is fewer. Thus the information that the unknown classes can utilize is fewer. The task turns to a
more difﬁcult zero-shot learning problem. The three digits datasets are MNIST, USPS, and SVHN.
MNIST is proposed by LeCun et al. (1998), which contains about seventy thousand images of the
white digits on a black background. USPS is proposed by Friedman et al. (2001), which contains
seven thousand grayscale images on the handwritten digits. SVHN is proposed by Netzer et al.
(2011), which contains six hundred thousand images of real-world street view house numbers. We
consider each dataset as a different domain. For the class space, we have two settings of unknown
classes. In the 3-way setting, the source domain contains seven classes (0-6), while the target domain
contains ten classes (0-9). While in the 2-way setting, the source domain contains seven classes (0-
7), while the target domain contains ten classes (0-9). In both settings, our goal is to align the known
classes in the target domain to the source domain and have the ability to classify the unknown ones."
SETTINGS,0.6161616161616161,"4.2
SETTINGS"
SETTINGS,0.6262626262626263,"We employ GCN of two layers for the inference of the classiﬁer. The ground truth of the classiﬁers
on the known classes is extracted from the original recognition model trained on the source domain.
The original model has almost the same structure as the ﬁnal classiﬁcation model, except for the
number of the classiﬁer. The classiﬁcation model is trained in 1200 epochs. We use the Adam Da
(2014) optimizer for training with the weight decay of 0.0005 and the learning rate of 0.001. The
boundary of unknown classes is set to 0.5 for better performance. The whole project is under the
framework of PyTorch Paszke et al. (2017)."
COMPARISON,0.6363636363636364,"4.3
COMPARISON"
COMPARISON,0.6464646464646465,"To test the performance of our model, we conduct experiments under several settings. However,
since there are few models that work on the domain adaptation with zero-shot learning, we compare
our model to zero-shot learning methods. Besides verifying the value of the inference classiﬁers, we
also compare our model to open set domain adaptation methods with random initialization unknown
classiﬁers. The results are shown in the following table."
COMPARISON,0.6565656565656566,"The comparison between our model and other exiting methods is reported in table 1. The task names
s2m, u2m, and m2u refer to the transfer tasks from SVHN to MNIST, USPS to MNIST, and MNIST
to USPS. The 2-way and 3-way settings mean the number of unknown classes is 2 and 3. The class"
COMPARISON,0.6666666666666666,Under review as a conference paper at ICLR 2022
COMPARISON,0.6767676767676768,"Figure 3: In the task setting m2u, the model is trained on the MNIST dataset with seven or eight
classes and transferred to the USPS dataset with ten classes."
COMPARISON,0.6868686868686869,"type setting all and unknown refers to the classiﬁcation accuracy on the overall 10 classes and the
unknown classes. The performance is evaluated by the average top-1 accuracy."
COMPARISON,0.696969696969697,"z-GCN proposed by Wang et al. (2018) is a zero-shot learning model with the employment of graph
knowledge. It only considers the classes gap while ignores the domain gap. However, as ﬁgure 3
shows, the domain gap between the MNIST and the USPS is large. Overﬁtting on the source domain
and the lack of labeled training images in the target domain affect the results a lot. In the s2m tasks
with the 2-way setting, the classiﬁcation accuracy of z-GCN on all classes is forty-eight, which is
twenty percent lower than our model. In the u2m and m2u tasks, the improvement of our model
is about ten percent as well. The result demonstrates that the adversarial learning employed by our
model is able to transfer the classiﬁcation ability from the source domain to the target domain with
no labeled data. The domain adaptation is important for the ﬂexibility of the models"
COMPARISON,0.7070707070707071,Table 1: Comparison results
COMPARISON,0.7171717171717171,"task
s2m
setting
2-way
3-way
classes
all
unknown
all
unknown
z-GCN
48.4%
6.2%
39.5%
13.2%
OSBP
58.2%
17.4%
54.0 %
24.3%
our
67.0%
38.2%
64.3%
46.4%
task
u2m
setting
2-way
3-way
classes
all
unknown
all
unknown
z-GCN
60.5%
9.5%
54.1%
10.4%
OSBP
61.4%
8.5%
62.3 %
12.4%
our
67.4%
26.5%
69.2%
24.1%
task
m2u
setting
2-way
3-way
classes
all
unknown
all
unknown
z-GCN
63.4%
8.6%
62.0%
12.3%
OSBP
51.3%
7.6%
42.2%
10.3%
our
73.6%
49.3%
68.2%
23.5%"
COMPARISON,0.7272727272727273,"Although z-GCN aims at the zero-shot problem, it is worth noticing that the accuracy of the unknown
classes in the target domain adaptation is around ten percent. The classiﬁcation accuracy is low on
the zero-shot learning problem. That is because the z-GCN only considers the gap in the class
space, but does not take the domain gap into consideration. With the training process, z-GCN only
infers the classiﬁer for the source domain. However, the testing is performed on the target domain.
This result shows the importance of domain adaptation in the zero-shot learning problem. With
the aligning of the domain gap, the accuracy of the unknown classes is increased by about twenty
percent in our model. In the s2m tasks, the improvement is about thirty percent on the unknown
classes. The high classiﬁcation accuracy on the unknown classes also conﬁrms that the domain"
COMPARISON,0.7373737373737373,Under review as a conference paper at ICLR 2022
COMPARISON,0.7474747474747475,"adaptation not only aligns the known classes in both domains but also extracted the features of the
images, which is domain invariant."
COMPARISON,0.7575757575757576,"To test the effectiveness of the inference classiﬁers, we further conduct experiments on the open set
domain adaptation method OSBP proposed by Saito et al. (2018). OSBP transfers the classiﬁcation
ability from the source domain to the target domain and rejects all of the unknown classes as one
class. Since we aim at classifying the unknown classes in detail, we expand the OSBP with randomly
initialized classiﬁers on the unknown classes. From the results in table 1, the classiﬁcation ability
on the unknown classes is about twenty percent lower than our method. However, we notice that the
OSBP still shows about ten percent accuracy on the unknown classes and sometimes even higher
than z-GCN. We owe the classiﬁcation accuracy on the unknown classes to the rejection mechanism.
Since OSBP has the ability to reject the unknown classes as one class, the detailed classiﬁcation in
the one class is much easier. The randomly initialized classiﬁer still has the possibility to classify
the two or three unknown classes properly."
COMPARISON,0.7676767676767676,"The accuracy on unknown classes for our method is about twenty percent higher than OSBP. Besides,
the accuracy on all classes still has about a ten percent increase. The result shows that the inference
from the word embedding to the unknown classiﬁer makes sense. Besides, the inaccurate classiﬁers
on the unknown classes confuse the classiﬁer on the known classes and result in a decrease in the
accuracy. Thus the outperformance of our model is not simply due to the increased number of the
classiﬁers. The inference of the classiﬁers based on the knowledge graph is valuable and credible. To
avoid randomness, we perform three different domain adaptation tasks. We also test the inﬂuence of
the number of the unknown classes, which is also shown in the above table. From the result showing
above, we can come to the conclusion that our model shows a good performance on open set domain
adaptation with zero-shot learning."
CONCLUSION,0.7777777777777778,"5
CONCLUSION"
CONCLUSION,0.7878787878787878,"In this paper, we propose a model on open set domain adaptation with zero-shot learning. Our model
not only makes good performance on the alignment of the domain gap but also can give detailed
classiﬁcation on the unknown classes. The ability of the further classiﬁcation on the unknown classes
improves the visual cognitive development ability of the robot, which is important for the robot
working in a realistic environment. The experiments show that our model has a good performance
on domain adaptation with zero-shot learning. In future work, we will test our model on the real
scenario dataset, which is more suitable to the task of the visual cognitive development robot."
REFERENCES,0.797979797979798,REFERENCES
REFERENCES,0.8080808080808081,"Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally
connected networks on graphs. Computer Science, 2013."
REFERENCES,0.8181818181818182,"Pau Panareda Busto, Ahsan Iqbal, and Juergen Gall. Open set domain adaptation for image and
action recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(2):413–
429, 2018."
REFERENCES,0.8282828282828283,"Kingma Da. A method for stochastic optimization. Computer Science, 2014."
REFERENCES,0.8383838383838383,"Jerome Friedman, Trevor Hastie, and Robert Tibshirani. The elements of statistical learning. Journal
of the Royal Statistical Society, 1(10):192–192, 2001."
REFERENCES,0.8484848484848485,"Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In
Proceedings of International Conference on Machine Learning, pp. 1180–1189, 2015."
REFERENCES,0.8585858585858586,"Michael Kampffmeyer, Yinbo Chen, Xiaodan Liang, Hao Wang, Yujia Zhang, and Eric P Xing.
Rethinking knowledge graph propagation for zero-shot learning. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 11487–11496, 2019."
REFERENCES,0.8686868686868687,"Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional net-
works. In Proceedings of International Conference on Learning Representations, 2016."
REFERENCES,0.8787878787878788,Under review as a conference paper at ICLR 2022
REFERENCES,0.8888888888888888,"Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998."
REFERENCES,0.898989898989899,"Hong Liu, Zhangjie Cao, Mingsheng Long, Jianmin Wang, and Qiang Yang. Separate to adapt:
Open set domain adaptation via progressive separation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 2927–2936, 2019."
REFERENCES,0.9090909090909091,"Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features with
deep adaptation networks. In Proceeding of International Conference on Machine Learning, pp.
97–105, 2015."
REFERENCES,0.9191919191919192,"Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Unsupervised domain adaptation
with residual transfer networks. In Proceedings of Conference on Neural Information Processing
Systems, 2016."
REFERENCES,0.9292929292929293,"Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. NIPS Workshop on Deep Learning
and Unsupervised Feature Learning, 2011."
REFERENCES,0.9393939393939394,"Pau Panareda Busto and Juergen Gall. Open set domain adaptation. In Proceedings of the IEEE
International Conference on Computer Vision, pp. 754–763, 2017."
REFERENCES,0.9494949494949495,"Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
Automatic differentiation in
pytorch. 2017."
REFERENCES,0.9595959595959596,"Kuniaki Saito, Shohei Yamamoto, Yoshitaka Ushiku, and Tatsuya Harada. Open set domain adap-
tation by backpropagation. In Proceedings of the European Conference on Computer Vision, pp.
153–168, 2018."
REFERENCES,0.9696969696969697,"Mei Wang and Weihong Deng. Deep visual domain adaptation: A survey. Neurocomputing, 312
(27):135–153, 2018."
REFERENCES,0.9797979797979798,"Xiaolong Wang, Yufei Ye, and Abhinav Gupta. Zero-shot recognition via semantic embeddings
and knowledge graphs. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 6857–6866, 2018."
REFERENCES,0.98989898989899,"Yongqin Xian, Bernt Schiele, and Zeynep Akata. Zero-shot learning-the good, the bad and the
ugly. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
4582–4591, 2017."
