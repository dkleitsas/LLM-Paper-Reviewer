Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.004608294930875576,"Existing works to compute trust as a numerical value mainly rely on ranking,
rating or assessments of agents by other agents. However, the concept of trust
is manifold, and should not be limited to reputation. Recent research in neuro-
science converges with Berg’s hypothesis in economics that trust is an encoded
function in the human brain. Based on this new assumption, we propose an ap-
proach where a trust level is learned by an overlay of any model-free off-policy
reinforcement learning algorithm. The main issues were i) to use recent ﬁnd-
ings on dopaminergic system and reward circuit to simulate trust, ii) to assess our
model with reliable and unbiased real life models. In this work, we address these
problems by extending Q-Learning to trust evaluation, and comparing our results
to a social science case study. Our main contributions are threefold. (1) We model
the trust-decision making process with a reinforcement learning algorithm. (2) We
propose a dynamic reinforcement of the trust reward inspired by recent ﬁndings of
neuroscience. (3) We propose a method to explore and exploit the trust space. Ex-
periments show that it is possible to ﬁnd a set of hyperparameters of our algorithm
that reproduces the Dunning-Kruger effect, whereby beginners are overconﬁdent
while experts tend to assess their conﬁdence level more accurately."
INTRODUCTION,0.009216589861751152,"1
INTRODUCTION"
INTRODUCTION,0.013824884792626729,"Trust seems to be one of the most overused words in the Information Technology world. Yet the
concept is struggling to ﬁnd a universal deﬁnition: “the term is never deﬁned. The usage is much
clearer than the concept itself” (Hunyadi, 2020). At the end of the day, the question of trust in the
digital world is rarely raised; scientists and industrialists prefer to focus on the effects of its erosion
- most often - or its improvement - more rarely - on related issues: digital identity, privacy, personal
datas, security, customer relationship, artiﬁcial intelligence, ﬁnancial services digitalisation."
INTRODUCTION,0.018433179723502304,"On the other hand, from a practical point of view - e.g. for industries and institutions - we observed
that there is a lack of tools to forecast the evolution of trust among stakeholders (customers, suppli-
ers, partners) according to the strategic choices of digital transformation. New job positions, such as
that of chief trust ofﬁcer, could be interested in this type of tool. This paper presents the preliminary
research that allows us to envision the building of such future applications."
INTRODUCTION,0.02304147465437788,"On the understanding of trust as a structure of a social system, we choose to overcome the very ﬁrst
question concerning its deﬁnition on the basis of scientiﬁc knowledge in sociology, psychology and
philosophy."
INTRODUCTION,0.027649769585253458,"According to social scientists, trust functions as a way to reduce complexity in a social system,
while allowing it to evolve towards richer interactions. It acts as a stabilising structure for societies
(Gambetta et al., 2000). Trust makes it possible to act in contexts where information is lacking, or
even unavailable (the relations between present and future events being indeterminable). Luhmann
(2000) has provided semantic distinction between trust and conﬁdence. Trust or interpersonal trust
is an assessment of risk by an individual who makes a rational choice. Trust allows the binding
of groups of people whose number does not exceed 100 to 150 (Aral, 2020; Harari, 2016; Dunbar,
1992) Conﬁdence allows an individual to act in spite of an inherent danger in the social system
because he has faith in institutionalised factors of the society, his culture, his knowledge. Conﬁdence
allows a social system to be uniﬁed at scale despite the oppositions of the groups within it."
INTRODUCTION,0.03225806451612903,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.03686635944700461,"In his recent work, the philosopher Hunyadi proposes a new uniﬁed theory of trust, which leads
to the following deﬁnition: “Trust is a bet on the behavioural expectations of things (e.g. that the
ground will support me when I walk), people (that the driver of the car I pass will obey the rules of
the road), and institutions (that the money I use for transactions will have some value).” He argues
that former theories - such as Luhmann’s - are discontinuous because the intrinsic mechanisms of
trust are not the same depending on the scale at which they are observed."
INTRODUCTION,0.041474654377880185,"Hunyadi’s theory seems both to overlap with Luhmann’s deﬁnition of interpersonal trust and to be
practical enough - e.g. the notion of a bet leads quite naturally to deﬁning a reward in the future - to
be simulated with reinforcement learning ."
INTRODUCTION,0.04608294930875576,"We want to develop a numerical model based on this new deﬁnition of trust - the contribution pre-
sented in this paper - and hypothesise that such a model would allow the emergence of other types of
trust (e.g. Luhmann trust) on a large scale, or even validate the trust continuum hypothesis proposed
by Hunyadi - our future works."
INTRODUCTION,0.05069124423963134,"The main contribution of this paper is a trust learning algorithm based on reinforcement learning.
We called this algorithm SelfQ-Trust because the trust level an agent has in itself is evaluated by
a Q-value calculated by the algorithm. Leveraging on recent ﬁndings about the reward circuit in
the human brain, we propose a dynamic reinforcement of the trust reward ; a kind of distributional
reinforcement learning applied on a vector of trust levels ; a method for explore and exploit the
trust space. We then evaluate the model using a recent dataset of psychological research on the
Dunning-Kruger effect on self-conﬁdence."
RELATED WORK,0.055299539170506916,"2
RELATED WORK"
RELATED WORK,0.059907834101382486,"The economics of internet platforms heavily depend on the quality of information and the security
of users (Barbosa et al., 2020). Trust models based on multi-agent systems (MAS) are used to
help users choose their interactions by avoiding those who are allegedly incompetent or malicious
(Ramchurn et al., 2004; Sabater & Sierra, 2005). Many algorithms assign each agent a score based
on the result of its past interactions with others (Jøsang et al., 2007). Both implicit (e.g. frequency,
duration, closeness of relationships) and explicit (e.g. comments and ratings from other agents,
payment informations) measures are used and the score is considered as a measure of the trust the
community can place in the agent (Ben-Naim et al., 2020). The applications cover almost all uses
of the Internet. Examples include: e-commerce (e.g. (Xiong & Liu, 2003)), collaborative economy
platforms (e.g. Airbnb (Alsheikh et al., 2019)), messaging (e.g. (Lien & Cao, 2014))."
RELATED WORK,0.06451612903225806,"In such a model, the concept of trust is often confused with that of reputation (Jøsang, 2007). To
the best of our knowledge, the theoretical frameworks are based on a cognitive model (Cho et al.,
2015) or on game theory (Wang et al., 2016). In cognitive models, an agent evaluates the trust it
places in another based on the latter’s underlying degree of belief. But as it does not have direct
access to the mental states of the latter, the trust decision is based on the observation of the outcome
of the interaction. The observation model takes into account the uncertainty that arises from the -
at least initial - unpredictability of the other agent. In this context, Bayesian networks are a logical
choice of knowledge representation (Esfandiari & Chandrasekharan, 2001). In game-theoretic work,
trust is the result of a game that involves a utility calculation based on knowledge of the history of
relationships. A well-known trust game used in econmics is that of Berg et al. (Berg et al., 1995).
The assessment of these models can be done theoretically by establishing a set of properties to be
achieved by the system (see e.g. (Cox, 2004)). The experimental approach consists in developing a
test bed where different trust systems compete (Sabater & Sierra, 2005)."
RELATED WORK,0.06912442396313365,"In economics, Zak & Knack (2001) links trust and economic growth based on game theory. The
hypothesis of the rational economic agent is to be reviewed. Indeed, if an agent in an economic
system were purely rational, then the trust game of Berg et al. would respect the Nash equilibrium,
which is not the case. Several attempts have been made in this direction, the most successful of
which use Bayesian models (e.g. (Tribus, 2016)). The stakes are very high. It is a question of taking
into account the realistic behaviour of social relations in a society, in order to deﬁne - in the long
term - theoretical frameworks and indicators capable of making economic decisions in uncertain
contexts."
RELATED WORK,0.07373271889400922,Under review as a conference paper at ICLR 2022
RELATED WORK,0.07834101382488479,"Previous work attempting to measure trust in MAS with Q-learning (see e.g. Vijaya Kumar &
Jeyapal (2014); Aref & Tran (2018)) has relied on explicit measures of the relationship - which
introduces a modelling bias. Meanwhile, the Berg’s hypothesis is being conﬁrmed by recent work
in neuroscience. Meyniel et al. (2015) argue that in humans, trust is not derived from a heuristic
process but rather is formed during their learning process. For those reasons, we have taken the
problem to be that of modelling trust as a human brain learning process by exploiting the latest
knowledge on the functioning of the reward circuit."
BACKGROUND,0.08294930875576037,"3
BACKGROUND"
TERMINOLOGY,0.08755760368663594,"3.1
TERMINOLOGY"
TERMINOLOGY,0.09216589861751152,"We consider a social system S deﬁned by an environment and a ﬁnite set of predeﬁned actions A.
A population I of individuals represented by a MAS whose trust evolution relative to S is to be
modelled. As deﬁned by Russell & Norvig (2020), “an agent is anything that can be viewed as
perceiving its environment through sensors and acting upon that environment through effectors”.
Any agent of I is deﬁned mathematically as an application f which maps every possible percepts
sequence (o1, ..., on) ∈P∗(O), where O is the set of possible observations of the environment by
the agent, to an action a ∈A that the agent can perform or to any other function (e.g., each sub-
function of an algorithm intrinsic to the agent) that affects its possible actions: f : P∗(O) →A."
TERMINOLOGY,0.0967741935483871,"In his doctoral work, Marsh (1992) was the ﬁrst to formalise trust as a computational concept.
He introduced a measure of trust Tx(y, α) ∈[−1, 1[ as the probability that an agent y will act if
Tx(y, α) ≥0 (respectively minus the probability it will not act, if Tx(y, α) < 0) in the situation α
to reach an outcome that another agent x presupposes to expect if it had a full trust in y, before y
acts. If Tx(y, α) > threshold(y, α) the agents cooperate."
TERMINOLOGY,0.10138248847926268,"Research in social sciences and information technology has extended the English terminology from
property law 1 2. Thus, according to Hardin (2002), a ”trustor” is an entity that trusts another entity,
the ”trustee”. According to Cofta (2007), a trustor can be a social actor (such as a person or an
institution) or a technical actor (such as a computer or a software), which acts on behalf of a social
actor. In the following, we will note f ∈I to refer to a trustor who is placing its trust in a trustee
g ∈I when it wants to perform an action chosen from a part of the available actions P(A)."
Q-LEARNING,0.10599078341013825,"3.2
Q-LEARNING"
Q-LEARNING,0.11059907834101383,"A single-agent reinforcement learning problem is commonly modelled by a Markov decision process
(MDP). A MDP is usually formalized by a tuple {S, A, T, R} where s ∈S, a ∈A and r ∈R
stands respectively for state, action and reward. T is a transition function deﬁned as a probability
distribution over the states. For any s ∈S, agent’s choice to perfom an action a ∈A occurs with the
probablility T(s, a, s′) ∈[0, 1] where s′ is the next state following s. It will result in the environment
entering the new state s′ and give a reward r = R(s, a). A policy π describes which action the agent
takes in each state. Formally, it is therefore a function S →A in the case of a deterministic policy
or S × A →[0; 1] in the stochastic case. The goal for the agent is to learn a policy that maximizes
the cumulative rewards received over its learning process."
Q-LEARNING,0.1152073732718894,"A common way to achieve the goal is to solve the Bellman equation applied to the total sum of
discounted rewards Qπ(s, a) reaped by the agent if it starts in state s and ﬁrst takes the action a,
before then applying the policy π ad inﬁnitum (γ states for the discount factor):"
Q-LEARNING,0.11981566820276497,"q∗(s, a) =
X"
Q-LEARNING,0.12442396313364056,"s′,r
p(s′, r|s, a)

r + γmax
a′∈A q∗(s′, a′)

(1)"
Q-LEARNING,0.12903225806451613,"Introduced by Watkins (1989), Q-learning is a numerical method of solving Equation 1. At each
time step t of the algorithm, agent chooses action At from state St using policy derived from Q (e.g."
Q-LEARNING,0.1336405529953917,"1trustor (Collins): (in property law) a person who sets up a trust transferring property to another person
2trustee (Collins): someone with legal control of money or property that is kept or invested for another
person, company, or organization"
Q-LEARNING,0.1382488479262673,Under review as a conference paper at ICLR 2022
Q-LEARNING,0.14285714285714285,"ϵ-greedy). Then it makes a step in environment, observe the new state St+1 and the reward Rt+1.
Q-value is updated by applying a temporal-difference method (TD) (Sutton, 1988) :"
Q-LEARNING,0.14746543778801843,"Q(St, At) ←Q(St, At) + α

Rt+1 + γmax
a∈AQ(St+1, a) −Q(St, At)

(2)"
Q-LEARNING,0.15207373271889402,"where Q is the learned action-value function which approximates the optimal action-value function
q∗(Watkins & Dayan, 1992), independent of the policy being followed and α ∈[0, 1] is a learning
rate setting the updating speed of Q-values at each time step. The part of the equation in brackets is
the difference between the reward estimated by the model and the reward actually received."
MODELLING THE DOPAMINERGIC SYSTEM,0.15668202764976957,"3.3
MODELLING THE DOPAMINERGIC SYSTEM"
MODELLING THE DOPAMINERGIC SYSTEM,0.16129032258064516,"Berg hypothesised that trust is a function that has been encoded in the human brain. Cognitive neu-
roscientists linked the functioning of the neurotransmitter dopamine to the theoretical computational
framework of reinforcement learning in the early 2000s (Holroyd & Coles, 2002). It is now estab-
lished that the release of dopamine in the reward circuit reveals a prediction error by the individual
(Glimcher, 2011). This surprise signal favours the learning of reward predictions and shapes the
individual’s future behaviour."
MODELLING THE DOPAMINERGIC SYSTEM,0.16589861751152074,"Predictions of reward were previously represented as a single scalar quantity. However, recent artiﬁ-
cial intelligence research on distributional reinforcement learning has inspired a new model in which
the brain represents possible future rewards not as a single mean, but rather as a probability distri-
bution, effectively representing multiple future outcomes simultaneously and in parallel (Bellemare
et al., 2017). Recordings from the mouse ventral tegmental area provided strong evidence for the
neural realisation of distributional reinforcement learning (Dabney et al., 2020). From a functional
point of view, an interesting recent hypothesis suggests that dopaminergic neurons encode both dif-
ferences between rewards and expectations in a goal-directed system. These prediction errors trigger
reward learning (Bogacz, 2020)."
METHODS,0.17050691244239632,"4
METHODS"
METHODS,0.17511520737327188,We propose the following deﬁnition based on Hunyadi’s work:
METHODS,0.17972350230414746,"Deﬁnition 1 (Trust) Trust is a bet made by an intelligent agent on the behavioural expectations it
has of another agent or of itself. Considering that an agent f, named trustor, trusts another agent g,
named trustee, we deﬁne a level of trust Tf(g) ∈[−1, 1] as a measure of the subjective certainty of
the trustor f that its bet will come true if the level is positive, and false if the level is negative."
PROBLEM STATEMENT,0.18433179723502305,"4.1
PROBLEM STATEMENT"
PROBLEM STATEMENT,0.1889400921658986,"We consider a population P of agents whose behaviour in a given environment is modelled by a
MDP {S, A, R}. The agents learn an optimal policy by means of any underlying model-free off-
policy reinforcement learning algorithm."
PROBLEM STATEMENT,0.1935483870967742,"The goal is to estimate the trust level Tf(g, s) ∈[−1, 1] that a trustor f ∈P can have in any agent
g ∈P which advises it to take the action a ∈A in state s ∈S following the introspection of its
MDP model. We require that: (1) the trust measurement algorithm runs in parallel with the MDP
learning algorithm and (2) it can be implemented with any model-free off-policy RL algorithm."
LEARNING PRINCIPLES,0.19815668202764977,"4.2
LEARNING PRINCIPLES"
LEARNING PRINCIPLES,0.20276497695852536,"The general explanations concern the multi-agent case, in which each agent assumes both the role
of trustor and that of trustee. For a better understanding of the basics of the method, simplifying
assumptions are made: the trust environment perceived by the agent is deterministic and the trust
actions deﬁned hereafter are discretised."
LEARNING PRINCIPLES,0.2073732718894009,"The trust level Tf(g, s) is learned by a trust model formally described by a MDP {S, ˆ
A, ˆRf,g},
where ˆ
A = J0, 2nK is the set of trust actions corresponding to trust levels { ˆa−n"
LEARNING PRINCIPLES,0.2119815668202765,"n }0≤ˆa≤2n and ˆRf,g :"
LEARNING PRINCIPLES,0.21658986175115208,Under review as a conference paper at ICLR 2022
LEARNING PRINCIPLES,0.22119815668202766,"S × ˆ
A →R is a reward function. By applying the Deﬁnition 1, we deﬁne the trust action ˆa as
follows: f trusts g to take action a chosen by g w.r.t. its underlying MDP model with trust level
T = ˆa−n"
LEARNING PRINCIPLES,0.22580645161290322,"n
means that the closer T is to 1, the greater f is certain that action a is optimal regarding
the underlying MDP and the closer T is to −1, the greater f is certain that action a isn’t optimal
regarding the underlying MDP."
LEARNING PRINCIPLES,0.2304147465437788,"Overall operation The trustor f asks any agent g for the action a it would have taken if it had been
in the same state s as f. This advice allows f to choose its action. f then learns its own model of
trust towards g thanks to a trust reward ˆr. g may be a trustee found in a queue of trusted agents or f
itself. In the latter case, the algorithm calculates the trust that f has in itself."
LEARNING PRINCIPLES,0.2350230414746544,"To do so, the algorithm learns a Q-function ˆQf,g : S × ˆ
A →R that calculates the quality of a
state–trust action combination. At all times, the trust that f places in g in state s can be derived from
the Q-function. Equation 3 is the application of Q-learning in the state-trust actions space. Applying
the Q-learning principle, the trust level estimated by the model is therefore the one obtained thanks
to the index of the trust action for the max of the Q-value. To report it in the interval [−1, 1], we
divide by n and subtract 1."
LEARNING PRINCIPLES,0.23963133640552994,"Tf(g, s) =
arg max
k
qk−1"
LEARNING PRINCIPLES,0.24423963133640553,"n
−1
with
ˆQf,g(s) = (qk)1≤k≤2n+1 the Q-value trust vector
(3)"
LEARNING PRINCIPLES,0.2488479262672811,"However, our initial attempts to make a simple instantiation of Q-learning in the state-trust action
space yielded poor results. We faced two main challenges: (1) a ﬁxed reward did not work for
learning a realistic trust model. (2) when reinforcement is applied to only one trust level, the model
quickly locks on the initial level. We used the following approach to answer these two issues."
LEARNING PRINCIPLES,0.2534562211981567,Using the neural realization of distributional reinforcement learning hypothesis.
LEARNING PRINCIPLES,0.25806451612903225,"Firstly, we hypothesised that trust is a process that is built up in the human brain. Secondly, we
deﬁne this process as a bet - therefore fundamentally reward-based. We therefore require that our
model takes into account the latest neuroscientiﬁc knowledge about the reward circuit in the human
brain. The reward prediction error (RPE) hypothesis of dopamine — the discrepancy between ob-
served and expected reward — has become central to research at the intersection of neuroscience
and computer science because RPE is precisely the signal that a RL system would need to update
reward expectations (Montague et al., 1996; Schultz et al., 1997). Dabney et al. ﬁndings provide
strong evidence to support the hypothesis that the brain represents future rewards not as a single
scalar quantity, but rather as a probability distribution."
LEARNING PRINCIPLES,0.2626728110599078,"We apply the hypothesis to our model of trust learning. We therefore require that (1) the reward
magnitude must be updated according to an observation of the result of the bet (”Dynamic rein-
forcement” section below) and (2) the bet made by the trustor in a trust relationship with a trustee
must give rise to several simultaneous rewards over all possible trust levels (”Distributional rein-
forcement learning” section below)."
LEARNING PRINCIPLES,0.2672811059907834,"Dynamic reinforcement. A way to vary the reward according to the experience of trust relationships
were to introduce a kind of attention mechanism. The general idea is (1) to increase the reward at the
end of an episode when a positive trust action leads to an improved score and conversely (2) to make
the reward proportional to the trust level of the trust action. For this purpose, we assume that the
agent is able to measure its performance and that of other agents in solving the underlying MDP. The
trustor uses this measure and the prior trust level it has towards its advisor to calculate the magnitude
of the trust reward. The magnitude of the trust reward ˆmf,g is regularly updated according to the
observed performance of the trustee g in ﬁnding the optimal policy of the underlying MDP. Each
time the trustor f chooses an action advised by a trustee g, it receives a trust reward ˆr depending on
the prior trust level T it places in g and the magnitude of the trust reward ˆmf,g:"
LEARNING PRINCIPLES,0.271889400921659,"ˆr = ˆmf,gT
(4)"
LEARNING PRINCIPLES,0.2764976958525346,"Distributional reinforcement learning. We tried to apply Equation 2 to the Q-value of trust level T
prior to the relationship and observed that the learned trust level quickly locks onto its initial level.We
chose to use a distributed learning design to overcome this problem, while taking advantage of the"
LEARNING PRINCIPLES,0.28110599078341014,Under review as a conference paper at ICLR 2022
LEARNING PRINCIPLES,0.2857142857142857,"distributed reinforcement brain hypothesis. We therefore apply the learning to all possible Q-values
of trust actions at the given state-action pair. The 2n trust levels Ti are learned by 2n Q-functions
ˆQi simultaneously and in parallel, as shown in Figure 1."
LEARNING PRINCIPLES,0.2903225806451613,"-1
-.8 -.6 -.4 -.2
0
.2
.4
.6
.8
1
Ti ˆQi ˆδi ˆQi i"
LEARNING PRINCIPLES,0.29493087557603687,"ˆQi(t +1)
ˆQi(t)"
LEARNING PRINCIPLES,0.2995391705069124,"δ( ˆα, ˆγ, ˆr)Xi"
LEARNING PRINCIPLES,0.30414746543778803,"Figure 1: Distributional trust learning.
In the example, before the trust action,
the trust level of the trustor towards the
trustee is 0.6, as given by the model in
gray. The trustor chooses an a priori
trust level in range [0.6 −ˆϵ, 0.6 + ˆϵ]
- say 0.2 (see Equation 7 in next sec-
tion). A Gaussian random variable X
centered on 0.2 is modulated by the dif-
ference between actual reward and ex-
pectation (in green). Added to the Q-
value, the result gives the model after
the trust action in gray, indicating that
the trust level drops from 0.6 to 0.4."
LEARNING PRINCIPLES,0.3087557603686636,"In the following, T is the a priori level of trust that the
trustee f has in the trustee g. At each time step of the
algorithm, a difference δ(ˆα, ˆγ, ˆr) between the reward es-
timated by the model and the reward actually received ˆr is
computed in the same way as in Equation 2. A Gaussian
random variable X centered on T is modulated by this
difference. Equation 5 is used to update the 2n functions
ˆQi corresponding to the 2n discrete trust levels:"
LEARNING PRINCIPLES,0.31336405529953915,"ˆQf,g(St) ←ˆQf,g(St) + δ(ˆα, ˆγ, ˆr)X
with
X ∈R4n+1 ∼N(T, 1)
(5)"
LEARNING PRINCIPLES,0.31797235023041476,"Exploration and exploitation of trust model.
The
trustor has to manage a trade-off between trusting other
agents without prior knowledge (trust exploration) and
using the trust model it has learned to take into account
other agents’ advices (trust exploitation). We use for that
an epsilon-greedy method where: (1) Trust exploration
amounts to the trustor choosing to have full trust in the
trustee; we hypothetize that trusting someone - or oneself
- in a ﬁrst relational experience - should it be self-oriented
- is choosing to systematically take into account the ad-
vice he/she gives. (2) Rather than just using the model
being learned, the exploitation involves an uncertainty on
the level of trust that the trustor grants to the trustee. In
fact, as it is stated in the survey of Cho et al., the uncer-
tainty on the a priori trust that one grants in an interper-
sonal relationship tends to fade with time. In the single-
agent case, only the phase of exploitation is played."
SINGLE-AGENT CASE,0.3225806451612903,"4.3
SINGLE-AGENT CASE"
SINGLE-AGENT CASE,0.3271889400921659,"SelfQ-Trust Algorithm (see Appendix A) focuses on the
single-agent case, with agent f assuming both roles of trustor and trustee. From a technical point
of view, the trust evaluation should be seen as an introspection function of the agent that evaluates
in real time the relevance of its learning algorithm’s prediction of an underlying MDP with respect
to its current state. In the single-agent case, the evaluation of trust amounts to the evaluation of
self-conﬁdence. For better clarity, we instantiate below Deﬁnition 1 on the single-agent case."
SINGLE-AGENT CASE,0.3317972350230415,"Deﬁnition 2 (Self-conﬁdence) Self-conﬁdence is a bet made by an intelligent agent on its future
action. We deﬁne a level of self-conﬁdence Tf ∈[−1, 1] as a measure of the subjective certainty of
the agent f that future action it will take is optimal with respect to the task it is currently solving if
the level is positive, and non optimal if the level is negative."
SINGLE-AGENT CASE,0.33640552995391704,"For the dynamic reinforcement, we update ˆmf,f at the end of each episode i w.r.t. the score vi the
agent achieved in the episode i. Equation 6 is a heuristic that weights the reward of the trust model
according to the score that the agent measures at the end of a learning episode of the underlying
MDP. The idea is as follows: we consider that the agent stores its scores in a memory list of ﬁxed-
size. If the agent beats the highest score it has stored in the last card(memory) episodes, one can
consider that it was right to trust itself over the past episode, driving to a positive reward for the
next episode. Conversely, if the agent does less well than its highest score, one can consider that it
was wrong to trust itself on the past episode, driving to a negative reward for the next episode. The
magnitude of the reward will be proportional to the difference with the highest score observed in the
memory."
SINGLE-AGENT CASE,0.34101382488479265,Under review as a conference paper at ICLR 2022
SINGLE-AGENT CASE,0.3456221198156682,"ˆmf,f = −
vi −
max
vj∈memory(vj)"
SINGLE-AGENT CASE,0.35023041474654376,"vi
(6)"
SINGLE-AGENT CASE,0.3548387096774194,"For the exploitation of trust model, Equation 7 allows for uncertainty in the level of trust T that the
agent grants to itself. The uncertainty is decreasing with time, i.e. it is depending on the probability
ˆϵ of choosing to explore the trust model. T = X"
SINGLE-AGENT CASE,0.35944700460829493,"n , with X ∼U (max(−n, n(Tf(f, s) −ˆϵ)), min(n, n(Tf(f, s) + ˆϵ)))
(7)"
EXPERIMENTS,0.3640552995391705,"5
EXPERIMENTS"
EXPERIMENTS,0.3686635944700461,"There are public quantitative surveys dealing with trust (e.g. interpersonal trust, conﬁdence in pub-
lic institutions (Fitzgerald et al., 2016; Ortiz-Ospina & Roser, 2016)). Nevertheless, the available
datasets are likely to be extremely biased the term is never deﬁned. Moreover, the interpretation of
general questions about trust can be very different from one individual to another."
EXPERIMENTS,0.37327188940092165,"However, there are studies in the social sciences and economics that give results on observable
properties of known systems. These studies can be used as test cases to assess the model. Among
them, we have chosen the so-called Dunning-Kruger effect (DKE) to assess the trust of an agent in
itself. Speciﬁcally, in their seminal paper, Kruger & Dunning (1999) put forward as an explanation a
metacognitive difﬁculty of the unqualiﬁed people which prevents them from accurately recognising
their incompetence and evaluating their real abilities. Conversely, the most qualiﬁed people would
tend to underestimate their level of competence and would wrongly think that tasks that are easy for
them are also easy for others. Dunning (2011) suggests that the effect is universal, as it has been
observed in very different areas of learning."
EXPERIMENTS,0.3778801843317972,"Experimental Setup. We implemented the SelfQ-Trust algorithm in Python; the code is available
at https://github.com/selfQtrust/code. Environment is a randomly generated maze
of dimension (n, n). The goal of the agent is to start from the upper left corner and ﬁnd the exit at
the lower right corner. The maze generation algorithm ensures that at least one path exists between
the entrance and the exit."
EXPERIMENTS,0.3824884792626728,"The agent learns to trust itself to select the optimal path in each state of the environment. In the
maze case, the state is the cell where the agent is located. Thus, during an episode, an agent learns
a sequence of trust levels, each of which is linked to a cell he visited. Using this sequence, we
calculate three metrics to aggregate the trust measures during the episode: the mean trust on all
states of the environment at the end of the episode (M1); the mean trust over all states the agent
passed through during the episode (M2); the max trust the agent had during the episode in the states
it passed through (M3)."
EXPERIMENTS,0.3870967741935484,"Simulation of the Dunning-Kruger Effect (DKE). In our ﬁrst experiments, we were looking at
whether the algorithm is able to reproduce the DKE effect. We trained a population of 1000 agents
over 3 mazes of dimension (6, 6), (10, 10), (30, 30). Computations of the three metrics M1, M2,
M3 are shown in Figure 2."
EXPERIMENTS,0.391705069124424,"The results show that the SelfQ-Trust algorithm reproduces the DKE trend: the agent’s self-
conﬁdence goes through a peak systematically present at the beginning of the training, then de-
creases abruptly to a more or less pronounced local minimum, to end up rising and then stabilising.
Moreover, the effect strength seems to increase with the complexity of the environment. The fol-
lowing experiment raises the question of the dependence of DKE on hyperparameters."
EXPERIMENTS,0.39631336405529954,"Sensitivity to learning hyperparameters. The learning model uses two pairs of learning hyper-
parameters: the underlying MDP that the agent uses to solve the maze problem uses an α learning
rate and a γ discount factor; the trust model uses a rate of trust learning ˆα and a discount factor of
trust learning ˆγ. The environment is the same maze of dimension (5, 5) for the whole experiment.
We trained a population of 50 agents on 60 episodes of MDP learning as well as on 60 episodes of
trust learning - we choosed those settings accordingly to Sanchez & Dunning (2018) experiments
on Overconﬁdence Effect. The second metric M2 is used to show the results in Figure 3."
EXPERIMENTS,0.4009216589861751,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.4055299539170507,"100
101
102"
EXPERIMENTS,0.41013824884792627,Episode (log scale) 0 20 40 60 80 100
EXPERIMENTS,0.4147465437788018,Self-Conﬁdence (%) M1
EXPERIMENTS,0.41935483870967744,Maze (6x6)
EXPERIMENTS,0.423963133640553,Maze (10x10)
EXPERIMENTS,0.42857142857142855,Maze (30x30)
EXPERIMENTS,0.43317972350230416,"100
101
102"
EXPERIMENTS,0.4377880184331797,Episode (log scale) 0 20 40 60 80 100 M2
EXPERIMENTS,0.4423963133640553,Maze (6x6)
EXPERIMENTS,0.4470046082949309,Maze (10x10)
EXPERIMENTS,0.45161290322580644,Maze (30x30)
EXPERIMENTS,0.45622119815668205,"100
101
102"
EXPERIMENTS,0.4608294930875576,Episode (log scale) 0 20 40 60 80 100 M3
EXPERIMENTS,0.46543778801843316,Maze (6x6)
EXPERIMENTS,0.4700460829493088,Maze (10x10)
EXPERIMENTS,0.47465437788018433,Maze (30x30)
EXPERIMENTS,0.4792626728110599,"Figure 2: DKE simulation. 1000 agents are trained on 500 synchronised episodes of the underlying
MDP and the trust model on each of the 3 mazes. Hyperparameters are set to: (α, γ, ˆα, ˆγ) =
(0.1, 0.99, 0.01, 0.99) for the 3 mazes. Estimate of the central tendency (mean estimator) and 95%
conﬁdence interval of self-conﬁdence computed by SelfQ-Trust vs. learning episode."
EXPERIMENTS,0.4838709677419355,"0
10
20
30
40
50
60
Episode −60 −40 −20 0 20 40 60"
EXPERIMENTS,0.48847926267281105,Self-Conﬁdence (%) α
EXPERIMENTS,0.4930875576036866,"0.01
0.02
0.05
0.1
0.2"
EXPERIMENTS,0.4976958525345622,"0
10
20
30
40
50
60
Episode 0 20 40 60 γ"
EXPERIMENTS,0.5023041474654378,"0.8
0.9
0.99
0.999"
EXPERIMENTS,0.5069124423963134,"0
10
20
30
40
50
60
Episode 0 20 40 60 ˆα"
EXPERIMENTS,0.511520737327189,"0.01
0.02
0.05
0.1
0.2"
EXPERIMENTS,0.5161290322580645,"0
10
20
30
40
50
60
Episode 0 20 40 60 ˆγ"
EXPERIMENTS,0.5207373271889401,"0.8
0.9
0.99
0.999"
EXPERIMENTS,0.5253456221198156,"Figure 3:
Sensitivity to learning hyperparameters α, γ, ˆα and ˆγ.
We set (α, γ, ˆα, ˆγ)
=
(0.1, 0.9, 0.1, 0.9) then for each of the ﬁgures we vary the hyperparameter studied in the set
{0.01, 0.02, 0.05, 0.1, 0.2} for α and ˆα and in the set {0.8, 0.9, 0.99, 0.999} for γ and ˆγ. Estimate
of the central tendency (mean estimator) and 95% conﬁdence interval of self-conﬁdence computed
by SelfQ-Trust vs. learning episode."
EXPERIMENTS,0.5299539170506913,"The experiment conﬁrms the DKE trend, regardless of the hyperparameters choosen. Moreover, the
magnitude of the trend appears to be correlated with α, ˆα and ˆγ on the experience. γ does not seem
to have much effect on the trust model (all curves are in the same statistical conﬁdence interval). We
wonder why the peak of conﬁdence - which we will later call the peak of overconﬁdence - almost
always occurs during the same short window of learning episodes?"
EXPERIMENTS,0.5345622119815668,"Sensitivity to the complexity of the environment. The complexity of the environment should be
narrowly assessed from the agent’s point of view. We thus calculate it by training a population of
agents with the underlying MDP learning algorithm - i.e. a simple Q-learning on a maze environment
in the case of the experiment. We randomly generate 1000 mazes of dimension (10, 10). We train an
agent on each over 1000 episodes with α = 0.1 and γ = 0.99. We deﬁne complexity as the smallest
number of steps the agent has taken in the environment during the training episodes."
EXPERIMENTS,0.5391705069124424,"We then train 600 agents with SelfQ-Trust on a subset of mazes according to the range of the
measured complexity values. The number of episodes of the underlying MDP training and that
of the trust model are equal to 60. Hyperparameters are set to: α = 0.1 , γ = 0.99, ˆα = 0.1 ,
ˆγ ∈{0.9, 0.99}. Results are in Figure 4."
EXPERIMENTS,0.543778801843318,"Figure 4(c) show that the beginner’s overall self-conﬁdence tends to increase with the complexity of
the task being learned. It may be hypothesised that, the more complicated the problem is to solve,
the more difﬁcult it will be to learn self-conﬁdence and the more biases in that learning - such as
overconﬁdence - will manifest themselves. Moreover, the closer the trust discount factor is to 1, the
greater the overconﬁdence effect. Shall the ˆγ factor of SelfQ-Trust algorithm be able to measure the
overconﬁdence effect studied by Sanchez & Dunning?"
EXPERIMENTS,0.5483870967741935,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.5529953917050692,"0
10
20
30
40
50
60
Episode 20 40 60 80"
EXPERIMENTS,0.5576036866359447,Self-Conﬁdence (%)
EXPERIMENTS,0.5622119815668203,(a) ( ˆγ = 0.9)
EXPERIMENTS,0.5668202764976958,complexity
EXPERIMENTS,0.5714285714285714,"18
28
38
48
58
68
78
88"
EXPERIMENTS,0.576036866359447,"0
10
20
30
40
50
60
Episode 20 40 60 80"
EXPERIMENTS,0.5806451612903226,(b) ( ˆγ = 0.99)
EXPERIMENTS,0.5852534562211982,complexity
EXPERIMENTS,0.5898617511520737,"18
28
38
48
58
68
78
88"
EXPERIMENTS,0.5944700460829493,"18
28
38
48
58
68
78
88
Complexity 20 40 60 80 (c) ˆγ"
EXPERIMENTS,0.5990783410138248,"0.9
0.99"
EXPERIMENTS,0.6036866359447005,"Figure 4: Sensitivity to environment complexity measured with the metric M2. 600 agents are
trained on 8 mazes of dimension (10, 10) and complexity (18, 28, 38, 48, 58, 68, 78, 88). (a,b) Self-
conﬁdence vs. episode with two different discount factors of trust. ˆγ = 0.9 in (a) and ˆγ = 0.99
in (b). (c) Self-conﬁdence tendency per agent vs. complexity in both cases. All curves show an
estimate of the central tendency (mean estimator) and 95% conﬁdence interval"
EXPERIMENTS,0.6082949308755761,"We also observe on Figure 4(a) and (b) that the overconﬁdence peak occurring at the outset of
learning is not exclusive. Depending on the trust discount factor and the level of complexity, one
or two more peaks appear afterwards. This repetition of overconﬁdence peaks at the beginning
of learning has not been reported in any of the social science research we have studied, and may
constitute a new hypothesis."
EXPERIMENTS,0.6129032258064516,"0
10
20
30
40
50
60
Episode 20 40 60 80"
EXPERIMENTS,0.6175115207373272,Self-Conﬁdence (%)
EXPERIMENTS,0.6221198156682027,Overconﬁdence Effect
EXPERIMENTS,0.6267281105990783,Experiments
EXPERIMENTS,0.631336405529954,"(a) SelfQ-Trust (our model)
(b) Experiment with humans"
EXPERIMENTS,0.6359447004608295,"Figure 5: SelfQ-Trust repro-
duces the results of an experi-
ment involving human volun-
teers."
EXPERIMENTS,0.6405529953917051,"Simulation of the Overconﬁdence Effect. One of most recent sci-
entiﬁc result on the DKE is the measure of the Overconﬁdence (OC)
effect by Sanchez & Dunning (Sanchez & Dunning, 2018). We ex-
tracted a dataset of the measure of the self-conﬁdence from their
paper and tuned the SelfQ-Trust hyperparameters to reproduce the
results of their Study 2. Using the hypermarameter sensitivity study,
we have indicated the trend. We then used a brute force approach
to ﬁnd suitable hyperparameters. Parameters are: Maze (10, 10) of
complexity = 88, 60 agents trained, 60 episodes of learning MDP
as trust model, α = 0.1, γ = 0.99, ˆα = 0.05, ˆγ = 0.99. Results
are shown in Figure 5."
CONCLUSION,0.6451612903225806,"6
CONCLUSION"
CONCLUSION,0.6497695852534562,"In this paper we provide a new deﬁnition of trust for computer sci-
ence. It is praxis-oriented allowing for the development of numeri-
cal models. We introduce SelfQ-Trust, a RL algorithm for measur-
ing self-conﬁdence of any human like entity that can be modelled by
an intelligent agent trying to solve a MDP. Our method is inspired
by the latest ﬁndings in neurosciences and RL and shows convinc-
ing results, in line with research on the Dunning-Kruger Effect in
social psychology. Moreover, we show that numerical simulations of our model allows to formulate
new hypotheses; e.g. the intensity of the overconﬁdence effect depends on the complexity of the
task to be learned."
CONCLUSION,0.6543778801843319,"Further experiments can be carried out to strengthen the evidence: (1) Justify the use of ”distribu-
tional reinforcement learning” with an experiment showing that a model with a ﬁxed trust reward
do not work. However, we need to study our empirical design in a more theoretical way, notably
using the Bellemare operator of distributional reinforcement learning theory. (2) An ablation study
to assess the need for dynamic reinforcement. (3) An ablation study on the exploitation of the trust
model, with and without time-decaying uncertainty on the prior trust level. (4) Experiments with
different base algorithms e.g., DQN and A2C would evaluate the usefulness of SelfQ-Trust in more
complex RL formulations, and whether the results hold. (5) Experiments involving human experi-
menters and serious games to further investigate the overall design hypothesis."
CONCLUSION,0.6589861751152074,Under review as a conference paper at ICLR 2022
REFERENCES,0.663594470046083,REFERENCES
REFERENCES,0.6682027649769585,"Sinan S Alsheikh, Khaled Shaalan, and Farid Meziane. Exploring the effects of consumers’ trust: a
predictive model for satisfying buyers’ expectations based on sellers’ behavior in the marketplace.
IEEE Access, 7:73357–73372, 2019."
REFERENCES,0.6728110599078341,"Sinan Aral. The hype machine. Currency, 2020."
REFERENCES,0.6774193548387096,"Abdullah Aref and Thomas Tran. A hybrid trust model using reinforcement learning and fuzzy
logic. Computational Intelligence, 34(2):515–541, 2018."
REFERENCES,0.6820276497695853,"Nat˜a M Barbosa, Emily Sun, Judd Antin, and Paolo Parigi.
Designing for trust: A behavioral
framework for sharing economy platforms. In Proceedings of The Web Conference 2020, pp.
2133–2143, 2020."
REFERENCES,0.6866359447004609,"Marc G Bellemare, Will Dabney, and R´emi Munos. A distributional perspective on reinforcement
learning. In International Conference on Machine Learning, pp. 449–458. PMLR, 2017."
REFERENCES,0.6912442396313364,"Jonathan Ben-Naim, Dominique Longin, and Emiliano Lorini. Formalization of cognitive-agent
systems, trust, and emotions, 2020."
REFERENCES,0.695852534562212,"Joyce Berg, John Dickhaut, and Kevin McCabe. Trust, reciprocity, and social history. Games and
economic behavior, 10(1):122–142, 1995."
REFERENCES,0.7004608294930875,"Rafal Bogacz. Dopamine role in learning and action inference. Elife, 9:e53262, 2020."
REFERENCES,0.7050691244239631,"Jin-Hee Cho, Kevin Chan, and Sibel Adali. A survey on trust modeling. ACM Computing Surveys
(CSUR), 48(2):1–40, 2015."
REFERENCES,0.7096774193548387,"Piotr Cofta. Trust, complexity and control: conﬁdence in a convergent world. John Wiley & Sons,
2007."
REFERENCES,0.7142857142857143,"James C Cox. How to identify trust and reciprocity. Games and economic behavior, 46(2):260–281,
2004."
REFERENCES,0.7188940092165899,"Will Dabney, Zeb Kurth-Nelson, Naoshige Uchida, Clara Kwon Starkweather, Demis Hassabis,
R´emi Munos, and Matthew Botvinick. A distributional code for value in dopamine-based rein-
forcement learning. Nature, 577(7792):671–675, 2020."
REFERENCES,0.7235023041474654,"Robin IM Dunbar. Neocortex size as a constraint on group size in primates. Journal of human
evolution, 22(6):469–493, 1992."
REFERENCES,0.728110599078341,"David Dunning. The dunning–kruger effect: On being ignorant of one’s own ignorance. In Advances
in experimental social psychology, volume 44, pp. 247–296. Elsevier, 2011."
REFERENCES,0.7327188940092166,"Babak Esfandiari and Sanjay Chandrasekharan. On how agents make friends: Mechanisms for
trust acquisition. In Proceedings of the fourth workshop on deception, fraud and trust in agent
societies, Montreal, Canada, pp. 27–34, 2001."
REFERENCES,0.7373271889400922,"Rory Fitzgerald et al. Ess round 8: European social survey round 8 data (2016). data ﬁle edition
2.1., 2016. https://www.kaggle.com/pascalbliem/european-social-survey-ess-8-ed21-201617."
REFERENCES,0.7419354838709677,"Diego Gambetta et al. Can we trust trust. Trust: Making and breaking cooperative relations, 13:
213–237, 2000."
REFERENCES,0.7465437788018433,"Paul W Glimcher. Understanding dopamine and reinforcement learning: the dopamine reward pre-
diction error hypothesis. Proceedings of the National Academy of Sciences, 108(Supplement 3):
15647–15654, 2011."
REFERENCES,0.7511520737327189,"Yuval Noah Harari. Homo Deus: A brief history of tomorrow. Random House, 2016."
REFERENCES,0.7557603686635944,"Clay B Holroyd and Michael GH Coles. The neural basis of human error processing: reinforcement
learning, dopamine, and the error-related negativity. Psychological review, 109(4):679, 2002."
REFERENCES,0.7603686635944701,"Mark Hunyadi. Au d´ebut est la conﬁance. Lectures, Publications rec¸ues, 2020."
REFERENCES,0.7649769585253456,Under review as a conference paper at ICLR 2022
REFERENCES,0.7695852534562212,"Audun Jøsang. Trust and reputation systems. In Foundations of security analysis and design IV, pp.
209–245. Springer, 2007."
REFERENCES,0.7741935483870968,"Audun Jøsang, Roslan Ismail, and Colin Boyd. A survey of trust and reputation systems for online
service provision. Decision support systems, 43(2):618–644, 2007."
REFERENCES,0.7788018433179723,"Justin Kruger and David Dunning. Unskilled and unaware of it: how difﬁculties in recognizing one’s
own incompetence lead to inﬂated self-assessments. Journal of personality and social psychology,
77(6):1121, 1999."
REFERENCES,0.783410138248848,"Che Hui Lien and Yang Cao. Examining wechat users’ motivations, trust, attitudes, and positive
word-of-mouth: Evidence from china. Computers in human behavior, 41:104–111, 2014."
REFERENCES,0.7880184331797235,"Niklas Luhmann. Familiarity, conﬁdence, trust: Problems and alternatives. Trust: Making and
breaking cooperative relations, 6:94–107, 2000."
REFERENCES,0.7926267281105991,"Stephen Marsh. Trust in distributed artiﬁcial intelligence. In European Workshop on Modelling
Autonomous Agents in a Multi-Agent World, pp. 94–112. Springer, 1992."
REFERENCES,0.7972350230414746,"Florent Meyniel, Daniel Schlunegger, and Stanislas Dehaene. The sense of conﬁdence during prob-
abilistic learning: A normative account. PLoS computational biology, 11(6):e1004305, 2015."
REFERENCES,0.8018433179723502,"P Read Montague, Peter Dayan, and Terrence J Sejnowski.
A framework for mesencephalic
dopamine systems based on predictive hebbian learning. Journal of neuroscience, 16(5):1936–
1947, 1996."
REFERENCES,0.8064516129032258,"Esteban
Ortiz-Ospina
and
Max
Roser.
Trust.
Our
World
in
Data,
2016.
https://ourworldindata.org/trust."
REFERENCES,0.8110599078341014,"Sarvapali D Ramchurn, Dong Huynh, and Nicholas R Jennings. Trust in multi-agent systems. The
knowledge engineering review, 19(1):1–25, 2004."
REFERENCES,0.815668202764977,Stuart Russell and Peter Norvig. Artiﬁcial intelligence: a modern approach. 2020.
REFERENCES,0.8202764976958525,"Jordi Sabater and Carles Sierra. Review on computational trust and reputation models. Artiﬁcial
intelligence review, 24(1):33–60, 2005."
REFERENCES,0.8248847926267281,"Carmen Sanchez and David Dunning. Overconﬁdence among beginners: Is a little learning a dan-
gerous thing? Journal of Personality and Social Psychology, 114(1):10, 2018."
REFERENCES,0.8294930875576036,"Wolfram Schultz, Peter Dayan, and P Read Montague. A neural substrate of prediction and reward.
Science, 275(5306):1593–1599, 1997."
REFERENCES,0.8341013824884793,"Richard S Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3
(1):9–44, 1988."
REFERENCES,0.8387096774193549,"Myron Tribus. Rational Descriptions, Decisions and Designs: Pergamon Uniﬁed Engineering Se-
ries. Elsevier, 2016."
REFERENCES,0.8433179723502304,"Anitha Vijaya Kumar and Akilandeswari Jeyapal. Self-adaptive trust based abr protocol for manets
using q-learning. The Scientiﬁc World Journal, 2014, 2014."
REFERENCES,0.847926267281106,"Yingjie Wang, Zhipeng Cai, Guisheng Yin, Yang Gao, Xiangrong Tong, and Qilong Han. A game
theory-based trust measurement model for social networks. Computational social networks, 3(1):
1–16, 2016."
REFERENCES,0.8525345622119815,"Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279–292, 1992."
REFERENCES,0.8571428571428571,Christopher John Cornish Hellaby Watkins. Learning from delayed rewards. 1989.
REFERENCES,0.8617511520737328,"Li Xiong and Ling Liu. A reputation-based trust model for peer-to-peer e-commerce communities.
In EEE International Conference on E-Commerce, 2003. CEC 2003., pp. 275–284. IEEE, 2003."
REFERENCES,0.8663594470046083,"Paul J Zak and Stephen Knack. Trust and growth. The economic journal, 111(470):295–321, 2001."
REFERENCES,0.8709677419354839,Under review as a conference paper at ICLR 2022
REFERENCES,0.8755760368663594,"A
ALGORITHM"
REFERENCES,0.880184331797235,"Algorithm 1: SelfQ-Trust estimating self-trust of an agent f learning to solve a MDP.
Data: trust learning rate ˆα ∈]0, 1], ˆϵ = 1"
REFERENCES,0.8847926267281107,"1 Initialize ˆQf(s) ∼N(µ, σ2) −1 for all s ∈S with e.g. µ = 0 and σ2 = card( ˆ
Qf (s))
4
= 2n+1 4"
REFERENCES,0.8894009216589862,2 Initialize intensity of trust reward ˆRf = 0
REFERENCES,0.8940092165898618,3 Initialize empty list of scores for each past episode (vj)1≤j≤i
FOREACH EPISODE I DO,0.8986175115207373,4 foreach episode i do
FOREACH EPISODE I DO,0.9032258064516129,"5
s ←s0; steps ←0"
FOREACH STEP OF EPISODE - UNTIL S IS ST DO,0.9078341013824884,"6
foreach step of episode - until s is sT do"
FOREACH STEP OF EPISODE - UNTIL S IS ST DO,0.9124423963133641,"7
Get action a w.r.t. MDP model f and state s"
FOREACH STEP OF EPISODE - UNTIL S IS ST DO,0.9170506912442397,"8
n ←uniform random number ∈[0, 1]"
FOREACH STEP OF EPISODE - UNTIL S IS ST DO,0.9216589861751152,"9
if n < ˆϵ then ˆa ←2n"
ELSE,0.9262672811059908,"10
else"
ELSE,0.9308755760368663,"11
T ←uniform random number
∈{ j"
ELSE,0.9354838709677419,"n}−n≤j≤n ∩[max(−1, Tf(f, s) −ˆϵ), min(1, Tf(f, s) + ˆϵ)]"
ELSE,0.9400921658986175,"12
Get trust action ˆa from T: ˆa ←n(1 + T)"
ELSE,0.9447004608294931,"13
Take action a in env; learn MDP"
ELSE,0.9493087557603687,"14
Learn trust: LearnTrust(ˆa, s, s′)"
ELSE,0.9539170506912442,"15
s ←s′"
ELSE,0.9585253456221198,"16
steps ←steps + 1"
ELSE,0.9631336405529954,"17
if i /∈{0, 1} then ˆRf ←−
vi−max
1≤j<i(vj)"
ELSE,0.967741935483871,"vi
18
Append −steps to (vi)"
ELSE,0.9723502304147466,"19
Decay ˆϵ"
ELSE,0.9769585253456221,"20 Function LearnTrust(ˆa, s, s′)"
ELSE,0.9815668202764977,"21
δ ←ˆα

( ˆa"
ELSE,0.9861751152073732,"n −1) ˆRf + ˆγ max
ˆa′∈ˆ
A
ˆ
Qf(s′, ˆa′) −ˆ
Qf(s, ˆa)
"
ELSE,0.9907834101382489,"22
Get X ∈R4n+1 ∼N(0, 1); Y ←(X2n−ˆa, ..., X4n−ˆa)"
ELSE,0.9953917050691244,"23
ˆ
Qf(s) ←ˆ
Qf(s) + δY"
