Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.004608294930875576,"It is attracting attention to the long-tailed recognition problem, a burning issue that
has become very popular recently. Distinctive from conventional recognition is that
it posits that the allocation of the training set is supremely distorted. Predictably, it
will pose challenges to the generalisation behaviour of the model. Approaches to
these challenges revolve into two groups: ﬁrstly, training-aware methods, with the
aim of enhancing the generalisability of the model by exploiting its potential in the
training period; and secondly, post-hoc correction, liberally coupled with training-
aware methods, which is intended to reﬁne the predictions to the extent possible in
the post-processing stage, offering the advantages of simplicity and effectiveness.
This paper introduces an alternative direction to do the post-hoc correction, which
goes beyond the statistical methods. Mathematically, we approach this issue from
the perspective of optimal transport (OT), yet, choosing the exact cost matrix
when applying OT is challenging and requires expert knowledge of various tasks.
To overcome this limitation, we propose to employ linear mapping to learn the
cost matrix without necessary conﬁgurations adaptively. Testing our methods
in practice, along with high efﬁciency and excellent performance, our method
surpasses all previous methods and has the best performance to date."
INTRODUCTION,0.009216589861751152,"1
INTRODUCTION"
INTRODUCTION,0.013824884792626729,"Classiﬁcation problems in the real world are generally challenged by the long-tailed label distribution,
i.e., having a small number of samples for a majority of labels, and a dominant number of samples for
a minority of labels (Van Horn & Perona, 2017; Buda et al., 2018; Liu et al., 2019). It is also known
as imbalanced recognition, which has been widely studied in the past decades (Cardie & Nowe, 1997;
Chawla et al., 2002; Qiao & Liu, 2009; Cui et al., 2019). These distribution biases pose a signiﬁcant
challenge to predictive modeling; conceivably, models often suffer from poor generalisation and
undesirable estimation bias (Cao et al., 2019; Kang et al., 2020; Zhou et al., 2020)."
INTRODUCTION,0.018433179723502304,"Recently, a renewed interest in the problem of long-tail recognition has emerged following the context
of neural networks, as numerous publications in the literature endeavour to resolve the problem albeit
in different ways including decouple (Kang et al., 2020), meta-learning (Ren et al., 2020; Wang
et al., 2020; Li et al., 2021), post-hoc correction (Tang et al., 2020; Hong et al., 2021), etc (Liu
et al., 2019; Cao et al., 2019; Tang et al., 2020). One of the representative methods of post-hoc
correction, Logit Adjustment Menon et al. (2021), provides a statistical correction to the prediction,
receiving widespread attention for its simplicity and validity. But the downside is that it is conducted
on individual samples, the rectiﬁed marginal distribution may not satisfy the desired distribution."
INTRODUCTION,0.02304147465437788,"Figuring out exact ﬂaws of Logit Adjustment, our explicit modeling of the problem mathematically
turns into an equational constraint, meanwhile to minimise the difference between reﬁned distribution
and the original one, this minimisation is motivated upon the inner-product similarity. A little further,
the resulting problem can be linked to OT. Drawing on this linkage, we develop it further by proposing"
INTRODUCTION,0.027649769585253458,Published as a conference paper at ICLR 2022
INTRODUCTION,0.03225806451612903,"a linear mapping to automatically learn cost matrix, thereby circumventing the requirement for expert
knowledge to conﬁgure this matrix. In summary, our contributions are as follows:"
INTRODUCTION,0.03686635944700461,"• We propose an alternative direction based on convex optimisation to do post-hoc correction,
which goes beyond previous direction from the statistical view."
INTRODUCTION,0.041474654377880185,"• Imposing marginal distributions to align ideal ones, we derive an optimisation problem tied
to OT that is solved using Sinkhorn. More further, for better learning of the cost matrix, we
present a linear mapping enabling elegant learning with one-layer network."
INTRODUCTION,0.04608294930875576,"• The experimental evidence shows the high efﬁciency and best performance on three bench-
marks. It veriﬁes that addressing the post-hoc problem via OT is helpful and effective."
PRELIMINARIES,0.05069124423963134,"2
PRELIMINARIES"
PRELIMINARIES,0.055299539170506916,"In this section, we begin with notational deﬁnition, followed by an introduction to the long-tailed
recognition problem. Finally, we brieﬂy review the OT and Logit Adjustment Menon et al. (2021)."
PRELIMINARIES,0.059907834101382486,"Notations:
In what follows, for two matrices X, Y
∈
RN×K, we denote ⟨X, Y ⟩
=
PN
n=1
PK
k=1 XnkYnk as the Frobenius dot-product. δ(·) stands for the Dirac function, p(·) repre-
sents the probability distribution. U(r, c) = {P ∈RN×K
+
|P 1K = r, P ⊺1N = c}, where 1N and
1K are N-dimension and K-dimension vector whose elements are all 1. r and c refer to the vectors
of size N and K, U(r, c) include all matrices with row and column sums r and c respectively."
PROBLEM FORMULATION,0.06451612903225806,"2.1
PROBLEM FORMULATION"
PROBLEM FORMULATION,0.06912442396313365,"Having a collection of training samples {(xs
n, ys
n)}Ns
n=1, validation samples {(xv
n, yv
n)}Nv
n=1 and test
samples {(xt
n, yn)t}Nt
n=1 for classiﬁcation with K labels and input x ∈Rd, long-tailed recognition
assumes that the class-prior distribution for training data p(ys) is different from that for validation
data p(yv) and test data p(yt). Speciﬁcally, long-tailed recognition means the distribution p(ys) is
highly skewed, that is, some classes have the dominant number of samples, while tailed labels own
a very small number of samples. We can use imbalance ratio to measure the skewness in training
data set, which can be deﬁned as R = Ns
max
Ns
min , where N s
max and N s
min denote the largest and smallest
number of samples in the training data set, respectively. In this paper, we assume that the marginal
distribution of the test set is known, we consider it as an implicit prior knowledge to be applied.
Stepping back, even if we do not know the marginal distribution of the test dataset in advance. There
are still ways to estimate the marginal distribution of the test dataset relatively precisely, such as
methods in Hendrycks et al. (2018); Azizzadenesheli et al. (2019)."
PROBLEM FORMULATION,0.07373271889400922,"Obviously, most models trained on imbalanced training data set would suffer from extremely limited
generalisation ability. Hence the ultimate goal is to learn a model that minimises the empirical risk:"
PROBLEM FORMULATION,0.07834101382488479,"J (Φ (xs
n) , ys
n) = 1 Ns Ns
X"
PROBLEM FORMULATION,0.08294930875576037,"n=1
L (Φ(xs
n), ys
n) ,
(1)"
PROBLEM FORMULATION,0.08755760368663594,"where Φ(xs
n) ∈RK denotes logits with associated sample, Φ(·) : Rd →RK represents the mapping
via neural networks, L stands for the loss function, typically cross entropy for classiﬁcation problem."
REMINDERS ON OPTIMAL TRANSPORT,0.09216589861751152,"2.2
REMINDERS ON OPTIMAL TRANSPORT"
REMINDERS ON OPTIMAL TRANSPORT,0.0967741935483871,"OT is used to calculate the cost of transporting one probability measure to another. We next present a
brief introduction to OT to help us better view the long-tailed problem from an OT perspective."
REMINDERS ON OPTIMAL TRANSPORT,0.10138248847926268,"For two random variables X and Y , we denote its corresponding probability measures as r and
c. Besides, C(X, Y ) : X × Y →R+ stands for cost function which measures the expense of
transporting X to Y . Based on these, we can deﬁne OT distance between X and Y as"
REMINDERS ON OPTIMAL TRANSPORT,0.10599078341013825,"d(r, c) =
min
π∈Π(r,c) Z"
REMINDERS ON OPTIMAL TRANSPORT,0.11059907834101383,"X×Y
C(x, y)π(x, y)dxdy,
(2)"
REMINDERS ON OPTIMAL TRANSPORT,0.1152073732718894,Published as a conference paper at ICLR 2022
REMINDERS ON OPTIMAL TRANSPORT,0.11981566820276497,"where Π (r, c) =
nR"
REMINDERS ON OPTIMAL TRANSPORT,0.12442396313364056,"Y π(x, y)dy = r,
R"
REMINDERS ON OPTIMAL TRANSPORT,0.12903225806451613,"X π(x, y)dx = c
o
is the joint probability measure with r
and c. When we extend the above to the discrete situation, we consider following discrete distributions: r = N
X"
REMINDERS ON OPTIMAL TRANSPORT,0.1336405529953917,"i=1
pi(xi)δ(xi)
c = K
X"
REMINDERS ON OPTIMAL TRANSPORT,0.1382488479262673,"j=1
pi(yj)δ(yj)
(3)"
REMINDERS ON OPTIMAL TRANSPORT,0.14285714285714285,"where pi(xi) and pi(yj) represent the probability mass to the sample xi and yj respectively. In this
context, OT distance can be expressed as:"
REMINDERS ON OPTIMAL TRANSPORT,0.14746543778801843,"dM(r, c) =
min
P ∈U(r,c)⟨P , M⟩.
(4)"
REMINDERS ON OPTIMAL TRANSPORT,0.15207373271889402,"where M stands for the cost matrix constructed by Mij = C(xi, yj). The goal of OT is to ﬁnd a
transportation matrix P that minimizes the distance dM(r, c)"
REMINDERS ON OPTIMAL TRANSPORT,0.15668202764976957,"As we can see, OT is a distance measure between two probability distributions under some cost
matrix (Villani, 2008). However, when we use network simplex or interior point methods to solve the
above optimisation problem, it often comes at the cost of heavy computational demands. To tackle
this issue, OT with entropy constraint is proposed to allow the optimisation at small computational
cost in sufﬁcient smoothness (Burges et al., 2013). By adding a Lagrangian multiplier to the entropy
constraint, the new formulation can be deﬁned as follows:"
REMINDERS ON OPTIMAL TRANSPORT,0.16129032258064516,"dλ
M(r, c) = ⟨P λ, M⟩
where
P λ = arg min
P ∈U(r,c)
⟨P , M⟩−λh(P ),
(5)"
REMINDERS ON OPTIMAL TRANSPORT,0.16589861751152074,"where λ ∈[0, +∞], h(P ) = −PN
n=1
PK
k=1 Pnk log Pnk, dλ
M(r, c) is also known as dual-Sinkhorn
divergence, besides, it can be calculated with matrix scaling algorithms for cheaper computational
demand. The following lemma guarantees the convergence and uniqueness of the solution."
REMINDERS ON OPTIMAL TRANSPORT,0.17050691244239632,"Lemma 1 For λ > 0, the solution P λ is unique and has the form P λ = diag(u)Kdiag(v), where
u and v are two non-negative vectors uniquely deﬁned up to a multiplicative factor and K = e−M/λ
is the element-wise exponential of −M/λ."
REMINDERS ON OPTIMAL TRANSPORT,0.17511520737327188,"The above lemma states the uniqueness of P λ (Sinkhorn, 1974), and P λ can be efﬁciently computed
via Sinkhorn’s ﬁxed point iteration u, v ←r./Kv, c./K⊺u."
A QUICK RECAP OF LOGIT ADJUSTMENT,0.17972350230414746,"2.3
A QUICK RECAP OF LOGIT ADJUSTMENT"
A QUICK RECAP OF LOGIT ADJUSTMENT,0.18433179723502305,"We give a brief introduction to Logit Adjustment (Menon et al., 2021; Hong et al., 2021). For the
model Φ(·), it is trained by the standard cross-entropy loss function on imbalanced training data set,
and evaluated on test data. In this algorithm, the test logit is adjusted as follows:"
A QUICK RECAP OF LOGIT ADJUSTMENT,0.1889400921658986,"Φ(xt
n) = Φ(xt
n) −log p(ys)
(6)"
A QUICK RECAP OF LOGIT ADJUSTMENT,0.1935483870967742,"This simple procedure is derived from the Bayes optimal rule. It is apparent that Logit Adjustment
involves a post hoc correction on an individual sample, which does not necessarily guarantee that the
marginal distribution of the whole dataset matches the desired distribution."
METHODOLOGY,0.19815668202764977,"3
METHODOLOGY"
METHODOLOGY,0.20276497695852536,"The ﬁrst part of this section explores post-hoc correction from an OT perspective, proceeds to the
automatic learning of the cost matrix via linear mapping. Lastly, we demonstrate how it can be
achieved simply with one-layer neural network."
POST-HOC CORRECTION FORMALISED FROM AN OT PERSPECTIVE,0.2073732718894009,"3.1
POST-HOC CORRECTION FORMALISED FROM AN OT PERSPECTIVE"
POST-HOC CORRECTION FORMALISED FROM AN OT PERSPECTIVE,0.2119815668202765,"Since Logit Adjustment applies adjustment at the individual sample level. It doesn’t assure that the
marginal distribution of the overall data set fulﬁls our desired distribution. In this respect, we clearly
put the constraint into an equation:"
POST-HOC CORRECTION FORMALISED FROM AN OT PERSPECTIVE,0.21658986175115208,"Y ⊺1N = µ,
(7)"
POST-HOC CORRECTION FORMALISED FROM AN OT PERSPECTIVE,0.22119815668202766,Published as a conference paper at ICLR 2022
POST-HOC CORRECTION FORMALISED FROM AN OT PERSPECTIVE,0.22580645161290322,"where Y ∈RN×K indicates the reﬁned prediction value in matrix form, µ represents the expected
distribution on the test set. Alternatively, it is desirable to preserve another characteristic of Y ,
namely, remaining almost as similar to the original prediction as possible. We consider inner-product
based similarity to measure this, which is a straightforward yet useful similarity measure."
POST-HOC CORRECTION FORMALISED FROM AN OT PERSPECTIVE,0.2304147465437788,"maximize
Y
⟨C( ˆZ), Y ⟩,
(8)"
POST-HOC CORRECTION FORMALISED FROM AN OT PERSPECTIVE,0.2350230414746544,"where ˆZ represents the original prediction in matrix form, C(·) denotes to some transformation to ˆZ,
it can be some simple function, like Logarithmic function log(z), exponential function zα. Here we
select −log(·) as the cost function. This choice was driven by the requirement that the cost matrix
must be positive deﬁnite, whereas the transformation of the original prediction by −log(·) satisﬁes
this condition. In addition, as log likelihood represents the local probability density of the associated
samples, it can also be used to substitute ˆZ for the similarity approximation. In brief, the resulting
numerical form can be put in formal terms as follows:"
POST-HOC CORRECTION FORMALISED FROM AN OT PERSPECTIVE,0.23963133640552994,"minimize
Y
⟨−log( ˆZ), Y ⟩
(9)"
POST-HOC CORRECTION FORMALISED FROM AN OT PERSPECTIVE,0.24423963133640553,"subject to
Y ⊺1N = µ,
Y 1K = 1N.
(10)"
POST-HOC CORRECTION FORMALISED FROM AN OT PERSPECTIVE,0.2488479262672811,"Extra constraint on Y is imposed simply cos the tuned estimation has to fulﬁl the basic probabilistic
requirement that its sum is one. Comparing Eq. (9-10) with Eq. (4), we can see that if we substitute
P with Y , and substitute r and c with 1N and µ respectively, we ﬁnd that the above optimisation
problem is actually a special case of OT."
POST-HOC CORRECTION FORMALISED FROM AN OT PERSPECTIVE,0.2534562211981567,"In preliminaries, the entropy regularised OT (EOT) is introduced. By adding entropy regularisation to
OT, the given equation can be solved efﬁciently by Sinkhorn algorithm. Speciﬁcally, the equation is"
POST-HOC CORRECTION FORMALISED FROM AN OT PERSPECTIVE,0.25806451612903225,"minimize
Y
⟨−log( ˆZ), Y ⟩+ λY log(Y )
(11)"
POST-HOC CORRECTION FORMALISED FROM AN OT PERSPECTIVE,0.2626728110599078,"subject to
Y ⊺1N = µ,
Y 1K = 1N."
POST-HOC CORRECTION FORMALISED FROM AN OT PERSPECTIVE,0.2672811059907834,The associated algorithmic ﬂow for solving Eq. (11) is outlined in detail in Algorithm 1.
POST-HOC CORRECTION FORMALISED FROM AN OT PERSPECTIVE,0.271889400921659,"Algorithm 1: Solve OT-related algorithm efﬁciently in the post-hoc correction via Sinkhorn
Algorithm."
POST-HOC CORRECTION FORMALISED FROM AN OT PERSPECTIVE,0.2764976958525346,"Input: Cost matrix M = −log( ˆZ), trade-off parameter λ, max number of iterations NT ,
iteration number t, error threshold ϵ, current error σ, row and column sums r = 1N and
c = µ, |·| denotes the vector norm.
Result: Reﬁned predictions Y"
POST-HOC CORRECTION FORMALISED FROM AN OT PERSPECTIVE,0.28110599078341014,"1 Initialise K = e−M/λ,uold = 1N, v = 1K, t = 0;"
POST-HOC CORRECTION FORMALISED FROM AN OT PERSPECTIVE,0.2857142857142857,2 while t ≤NT and δ ≤ϵ do
POST-HOC CORRECTION FORMALISED FROM AN OT PERSPECTIVE,0.2903225806451613,"3
u = r./Kv;"
POST-HOC CORRECTION FORMALISED FROM AN OT PERSPECTIVE,0.29493087557603687,"4
v = c./K⊺u;"
POST-HOC CORRECTION FORMALISED FROM AN OT PERSPECTIVE,0.2995391705069124,"5
σ = |uold −u|;"
POST-HOC CORRECTION FORMALISED FROM AN OT PERSPECTIVE,0.30414746543778803,"6
uold = u;"
POST-HOC CORRECTION FORMALISED FROM AN OT PERSPECTIVE,0.3087557603686636,"7
t = t + 1;"
END,0.31336405529953915,8 end
END,0.31797235023041476,9 Output Y = diag(u)Kdiag(v)
END,0.3225806451612903,"Assigning λ with 1, it is observed that we equate our objective function to the KL divergence, thus
illustrating the extensive nature of our approach. DARP (Kim et al., 2020a) has previously applied it
to long-tailed semi-supervised classiﬁcation."
END,0.3271889400921659,"Remark
We would like to illustrate the non-applicable scenarios of our method. Firstly, our method
requires a large number of samples for evaluation. This is because if the batch size is small, we can
not guarantee that the desired marginal distribution can be satisﬁed within the batch. In some online
scenarios, the sample-wise correction method is more suitable. In addition, our method assumes that
the marginal distribution is already known. We assume that it is consistent with a uniform distribution."
END,0.3317972350230415,Published as a conference paper at ICLR 2022
COST FUNCTION LEARNING VIA LINEAR MAPPING,0.33640552995391704,"3.2
COST FUNCTION LEARNING VIA LINEAR MAPPING"
COST FUNCTION LEARNING VIA LINEAR MAPPING,0.34101382488479265,"Simple functions are likely to be sub-optimal for the real data sets; this suggests the design of a better
cost function to better ﬁt the long-tailed recognition problem. However, manually designed cost
functions require expert knowledge in different domains. Thus, we propose to use a linear mapping to
automatically learn the cost function, which relieves the need to conﬁguration."
COST FUNCTION LEARNING VIA LINEAR MAPPING,0.3456221198156682,"More speciﬁcally, for predictions ˜Z generated by Softmax operation via leveraging linear transforma-
tion matrix W , W ∈RK×K is learned so that the following objective function is minimised."
COST FUNCTION LEARNING VIA LINEAR MAPPING,0.35023041474654376,"minimize
Y ∈R
−⟨Y , log ˜Z⟩+ λ⟨Y , log Y ⟩,
(12)"
COST FUNCTION LEARNING VIA LINEAR MAPPING,0.3548387096774194,"Y 1K = 1N
Y ⊺1N = µ,
˜Znk =
exp(W ⊺Φ (xn))k
PK
k exp(W ⊺Φ (xn))k
."
COST FUNCTION LEARNING VIA LINEAR MAPPING,0.35944700460829493,"The resulting formula can be illustrated using a simple one-layer network of weight parameter W ,
together with an error function in Eq. (12). We initialise W with an identity matrix and use small
learning rate to learn W . One could also absorb the term −log p(ys) as the ﬁxed bias parameter into
the network. Motivated by this description, we can use a general gradient descent algorithm, such as
SGD, to optimise the error function. Taking into account the implementation in practice, the direct
calculation of Eq. (12) can be done using the Sinkhorn iterations (Burges et al., 2013; Frogner et al.,
2015; Peyré et al., 2019) in mini-batch training efﬁciently. Besides, We term the proposed method as
OTLM (Optimal Transport via Linear Mapping), Figure 1 illustrates the overview of OTLM."
COST FUNCTION LEARNING VIA LINEAR MAPPING,0.3640552995391705,"A single layer feed-
forward network"
COST FUNCTION LEARNING VIA LINEAR MAPPING,0.3686635944700461,"Optimal Transport !, #$ %&"
COST FUNCTION LEARNING VIA LINEAR MAPPING,0.37327188940092165,Cost matrix
COST FUNCTION LEARNING VIA LINEAR MAPPING,0.3778801843317972,−()*(%&)
COST FUNCTION LEARNING VIA LINEAR MAPPING,0.3824884792626728,Refined prediction -
COST FUNCTION LEARNING VIA LINEAR MAPPING,0.3870967741935484,Input logits .(/)
COST FUNCTION LEARNING VIA LINEAR MAPPING,0.391705069124424,"Figure 1: Our proposed framework OTLM. The logit Φ (x) is fed into a single-layer feed-forward
network to infer ˜Z. The cost matrix M is set to −log ˜Z. By solving the optimal transport problem
via Sinkhorn algorithm, we can obtain the reﬁned prediction Y ."
COST FUNCTION LEARNING VIA LINEAR MAPPING,0.39631336405529954,"Compute the gradients w.r.t W
With the optimisation in Algorithm 1, it was conducted on the
overall data set with a comparatively large number of data. Quite a different scenario now, as mini-
batch training is more favoured when it comes to neural networks. For this reason, the optimisation
workﬂow in Algorithm 1, as pointed by Peyré et al. (2019); Viehmann (2019), will have a problem of
batch stablisation. To this end, we perform logarithmic transformation to step 3-4 in Algorithm 1."
COST FUNCTION LEARNING VIA LINEAR MAPPING,0.4009216589861751,"log u = log r −log (Mv) = log r −logsumexp
 1"
COST FUNCTION LEARNING VIA LINEAR MAPPING,0.4055299539170507,"λM −log v

(13)"
COST FUNCTION LEARNING VIA LINEAR MAPPING,0.41013824884792627,"log v = log c −log (M ⊺u) = log c −logsumexp
 1"
COST FUNCTION LEARNING VIA LINEAR MAPPING,0.4147465437788018,"λM −log u

.
(14)"
COST FUNCTION LEARNING VIA LINEAR MAPPING,0.41935483870967744,"with log-sum-exp operation logsumexp (x) = log(P
i exp xi). Assuming convergence is achieved in
the Sinkhorn’s loops, it is unnecessary for us manually to compute the derivative of the loss function
in Eq. (12) w.r.t W . As long as we make sure that the entire optimisation process is differentiable,
modern deep learning libraries can automatically implement end-to-end derivations, as it allows us to
differentiate between the results of Sinkhorn’s loops as a mere composition of elementary operations."
COST FUNCTION LEARNING VIA LINEAR MAPPING,0.423963133640553,"Performance degradation caused by imbalanced distributions can be addressed with meta-learning
based methods (Ren et al., 2020; Wang et al., 2020; Li et al., 2021). Recent works have illustrated neu-
ral networks can learn more meaningful representations from the validation data set {(xv
n, yv
n)}Nv
n=1.
We also take a validation data set to optimise the parameter W , but with the still signiﬁcant difference
in that we require no labelling information, thus avoiding a large expanse of labeling samples."
COST FUNCTION LEARNING VIA LINEAR MAPPING,0.42857142857142855,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.43317972350230416,"4
EXPERIMENTS"
EXPERIMENTS,0.4377880184331797,"In this section, we ﬁrst conduct experiments comparing our approach versus extant post-hoc correction
methods on three data sets, including CIFAR-100-LT (Cao et al., 2019), ImageNet-LT (Liu et al.,
2019), and iNaturalist (Horn et al., 2018) with varying backbones. Finally, we empirically make a
comparison of our algorithm with alternative cutting-edge long-tailed recognition methods. Observe
that, for one thing, a plausible coupling of our algorithm to any training-aware long-tailed recognition
method exists. As an illustration of the potency and strong generalisation of our approach, we
conducted post-hoc correction of the pseudo-predictions of both methods, which are fairly typical
for the training stage of all data sets: The ﬁrst is based on the cross entropy (CE), and the other is
the recently proposed RIDE (Wang et al., 2021), which is based on multi experts. We conducted
experiments in the appendix for semi-supervised learning, to some extent to imitate the online
situation. All our experiments are implemented in the PaddlePaddle deep learning platform."
DATA SETS AND BASELINES,0.4423963133640553,"4.1
DATA SETS AND BASELINES"
DATA SETS AND BASELINES,0.4470046082949309,"The baseline methods and data sets are brieﬂy described here, with implementation details placed in
appendix. We assume that the marginal distribution is uniform due to the characteristics of data sets."
DATA SETS AND BASELINES,0.45161290322580644,"Baselines: We compare our methods with (i) post-hoc correction methods including Logit Adjust-
ment (Menon et al., 2021), DARP (Kim et al., 2020a), (ii) state-of-the-art methods including Focal
Loss (Lin et al., 2017), LDAM (Cao et al., 2019), BBN (Zhou et al., 2020), Balanced Softmax (Ren
et al., 2020), Causal Norm (Tang et al., 2020), LADE (Hong et al., 2021), M2m (Kim et al., 2020b),
Decouple (Kang et al., 2020), LFME (Xiang et al., 2020), RIDE (Wang et al., 2021)"
DATA SETS AND BASELINES,0.45622119815668205,"Long-tailed data set: We take experiments on three data sets including CIFAR-100-LT, ImageNet-
LT, and iNaturalist. We build the imbalanced version of CIFAR-100 by downsampling samples per
class following the proﬁle in Liu et al. (2019); Kang et al. (2020) with imbalanced ratios 10, 50, and
100. For all the benchmarks, we evaluate the performance on the test data set using a model trained
on the training data set, and report the results using top-1 accuracy."
MAIN RESULTS,0.4608294930875576,"4.2
MAIN RESULTS"
MAIN RESULTS,0.46543778801843316,"Table 1 illustrates the results on CIFAR-100-LT data set. For CE-based methods, it shows that
Logit Adjustment is indeed a simple yet effective method for post-hoc correction. Remarkably, it
outperforms the baseline methods by 2.6%, 4.1%, and 4.1% under the imbalance ratio of 10, 50, and
100 respectively. With a quick look at the results based on the OT approach, the superior results stress
the advantages of our approach over the DARP and Logit adjustments. OT algorithm can further
improve the performance by 0.62%, 0.43%, 0.70%, for OTLM, it outperforms Logit Adjustment by
0.55%, 0.73%, 0.99% under the three imbalance ratios on CIFAR-100-LT data set. For RIDE-based
training-aware methods, there is also a 0.6% and 1.23% improvement in the accuracy of our method."
MAIN RESULTS,0.4700460829493088,"Table 1: Comparison on the top-1 accuracy with post-hoc correction methods on CIFAR-100-LT data
set using ResNet-32 backbone, ImageNet-LT and iNaturalist data set using ResNeXt-50-32x4d and
ResNet-50 backbones respectively. Better results are marked in bold, the small red font indicates the
increase in accuracy when our method is compared to Logit Adjustment and RIDE. ’-’ means the
results are not reported. CE indicates that a cross-entropy loss function is used in the training stage."
MAIN RESULTS,0.47465437788018433,"Data set
CIFAR-100
ImageNet-LT
iNaturalist
Imabalced Ratio
10
50
100
-
-
Baseline (CE)
59.00
45.50
41.00
43.10
65.00
Logit Adjustment+CE
61.63
49.62
45.11
51.62
69.44
DARP+CE
61.78
49.52
45.13
51.20
71.18
OT+CE
62.25+0.62
50.05+0.43
45.81+0.70
51.87+0.25
71.20+1.76
OTLM+CE
62.18+0.55
50.35+0.73
46.10+0.99
52.35+0.73
72.30+2.86
RIDE
-
-
50.20
57.50
72.90
OT+RIDE
-
-
50.82+0.62
58.45+0.95
75.85+2.95
OTLM+RIDE
-
-
51.43+1.23
58.71+1.21
76.12+3.22"
MAIN RESULTS,0.4792626728110599,Published as a conference paper at ICLR 2022
MAIN RESULTS,0.4838709677419355,"Table 1 also provides the results on ImageNet-LT and iNaturalist data sets. The fact again attracts
our attention that for CE-based training-aware methods, OT, which is based on convex optimisation,
consistently outperforms Logit Adjustment by a large margin (1.76%) on iNaturalist data set. The
accuracy boosting indicates the great potential of methods based on optimisation for post-hoc correc-
tion. Besides, not surprisingly, OTLM can further enhance the prediction accuracy, it outperforms
Logit Adjustment by 0.73% and 2.86% on ImageNet-LT and iNaturalist respectively. As for RIDE,
the gain in accuracy is also signiﬁcant, about 3% on iNaturalist."
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.48847926267281105,"4.3
COMPARISON WITH THE STATE-OF-THE-ART METHODS"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.4930875576036866,"Armed with a demonstration of the validity of OT and OTLM, we switched to a comparison of our
performance with existing methods that achieved the most advanced results on three benchmarks. As
shown in Table 2, in particular, on ImageNet-LT and iNaturalist data sets, the experimental results are
remarkable and impressive, our method outperforms RIDE by 1.0% and 3.0%, respectively. Since
iNaturalist is a very difﬁcult and ﬁne-grained data set consisting of 8,142 categories. These huge
performance gains come at a fraction of the cost, as we show in the following subsection."
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.4976958525345622,"Table 2: Comparison on the top-1 accuracy with state-of-the-arts on CIFAR-100-LT data set using
ResNet-32 backbone, on ImageNet-LT and iNaturalist data set using ResNeXt-50-32x4d and ResNet-
50 backbones respectively. −denotes the results are not reported, results underlined are the ones
being compared, best results are marked in bold, the small red font denotes performance gain."
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5023041474654378,"Data Set
CIFAR-100
ImageNet-LT
iNaturalist
Imabalced Ratio
10
50
100
−
−
Cross Entropy (CE)
55.7
45.5
38.3
44.4
61.7
Focal Loss
55.8
44.3
38.4
43.7
−
CB-Focal
−
−
−
−
61.1
OLTR
−
−
−
46.3
63.9
NCM
−
−
−
47.3
63.1
norm
−
−
−
49.4
69.3
cRT
−
−
−
49.6
67.6
LWS
−
−
−
49.9
69.5
LDAM
−
−
−
−
64.6
LDAM+DRW
58.7
46.6
42.0
−
68.0
BBN
59.1
47.0
42.6
−
69.3
Causal Norm
59.6
50.3
44.1
51.8
−
M2m
58.2
−
43.5
−
−
LFME
−
−
43.8
−
−
Balanced Softmax
61.6
49.9
45.1
−
−
LADE
61.7
50.5
45.4
51.9
70.0
RIDE (6 experts)
−
−
50.2
57.5
72.9
Logit Adjustment
61.6
49.6
45.1
51.6
69.4
DARP
61.8
49.5
45.1
51.2
71.2
OT+CE
62.3+0.7
50.1
45.8
51.9
71.2
OTLM+CE
62.2
50.4+0.8
46.1
52.4
72.3
OT+RIDE (6 experts)
−
−
50.8+0.6
58.5+1.0
75.9+3.0
OTLM+RIDE (6 experts)
−
−
51.4+1.2
58.7+1.2
76.1+3.2"
COMPUTATION COST OF OT AND OTLM,0.5069124423963134,"4.4
COMPUTATION COST OF OT AND OTLM"
COMPUTATION COST OF OT AND OTLM,0.511520737327189,"As we have highlighted, the additional computational cost of OT and OTLM is particularly small,
compared to that of training-aware methods. In Table 3, exact time of the evaluations on the ImageNet-
LT and iNaturalist data sets is provided. Please note, the time here are measured from the start of
each method to the ﬁnal best performance. For the comparison between OT and OTLM, because
OTLM runs on the GPU, one optimisation iteration of OTLM is in fact much faster than OT. Except
for OTLM, which was run on an NVidia card (V100), the results come from a 28-core machine
(2.20 Ghz Xeon). Firstly, we can observe that all post-hoc correction methods here are not quite
time-consuming. For comparison, ImageNet-LT and iNaturalist can be trained on 4 NVidia cards"
COMPUTATION COST OF OT AND OTLM,0.5161290322580645,Published as a conference paper at ICLR 2022
COMPUTATION COST OF OT AND OTLM,0.5207373271889401,"Table 3: Time for different methods to execute on the ImageNet-LT and iNaturalist data sets. The
times here are measured from the start of each method to the ﬁnal best performance. Their running
time are counted in seconds. Coupled with the prior results, an observation can be made that OT can
be the best coordinate to trade off performance and efﬁciency in post-hoc correction methods."
COMPUTATION COST OF OT AND OTLM,0.5253456221198156,"Data Set
ImageNet-LT
iNaturalist
Logit Adjustment
0.3
1.2
OT
32.7
82.5
OTLM
134.4
256.5"
COMPUTATION COST OF OT AND OTLM,0.5299539170506913,"(V100) for extremely long periods of time. Using ResNeXt-50-32x4d on the iNaturalist training data
set, for example, with a batch size of 256, the amount of training time (in seconds) to perform 1
iteration is approximately 850. Also, if one differs the performance of each method, we gather that
OT and OTLM provide the best balance of performance and running costs to match."
CONVERGENCE BEHAVIOR OF OTLM,0.5345622119815668,"4.5
CONVERGENCE BEHAVIOR OF OTLM"
CONVERGENCE BEHAVIOR OF OTLM,0.5391705069124424,"To verify that our adopted OTLM algorithm is really efﬁcient. With ResNeXt-50-32x4d as the
backbone, we plotted curves depicting the decreasing absolute error σ with the number of iterations t
on the ImageNet-LT validation data set. Image on the left in Figure 2 gives us a visualisation of the
optimisation process. It is found that the absolute error σ converges at around 60 iterations. In fact,
we discovered that 10 or even fewer iterations were enough to yield satisfactory performance."
CONVERGENCE BEHAVIOR OF OTLM,0.543778801843318,"Figure 2: The left image shows the convergence of OTLM: The absolute error σ decreases with
iterations on the ImageNet-LT validation data set (20k samples), and we used ResNet-50 as the
backbone to obtain the original predictions Z. The right image demonstrates the block in W , we
trained on CIFAR-100 to produce the original predictions with OTLM to minimise the difference
between the ideal and actual distributions. For ease of display, the values of the diagonal elements
are relatively large and we will replace these values with zeros."
DETAILED ANALYSIS ON DISTRIBUTION MATCHING,0.5483870967741935,"4.6
DETAILED ANALYSIS ON DISTRIBUTION MATCHING"
DETAILED ANALYSIS ON DISTRIBUTION MATCHING,0.5529953917050692,"Upon distribution ﬁtting, preferably the estimation bias is lessened, which implies that the KL
divergence distance between the reﬁned prediction and the target distribution is decreased. Table 4
displays the KL distances on the ImageNet-LT and iNaturalist data sets, and we can ﬁnd that OT
possesses a smaller KL distance than Logit Adjustment, which suggests that OT decreases the
estimation bias more than Logit Adjustment by a much larger margin."
VISUALISATION ON WEIGHT PARAMETER IN OTLM,0.5576036866359447,"4.7
VISUALISATION ON WEIGHT PARAMETER IN OTLM"
VISUALISATION ON WEIGHT PARAMETER IN OTLM,0.5622119815668203,"As a natural consequence, one wonders about the exact structure of this weight matrix W , and no
better way to present its results than through visualisation. Towards this end, we applied OTLM"
VISUALISATION ON WEIGHT PARAMETER IN OTLM,0.5668202764976958,Published as a conference paper at ICLR 2022
VISUALISATION ON WEIGHT PARAMETER IN OTLM,0.5714285714285714,"Table 4: KL distance between reﬁned probability distribution and target distribution computed on
ImageNet-LT and iNaturalist data sets. Smaller distance indicates two distributions are more similar."
VISUALISATION ON WEIGHT PARAMETER IN OTLM,0.576036866359447,"Data Set
ImageNet-LT
iNaturalist"
VISUALISATION ON WEIGHT PARAMETER IN OTLM,0.5806451612903226,"Logit Adjustment
1.78e-05
5.81e-06
DARP
8.49e-08
1.07e-08
OT
8.62e-10
4.72e-10"
VISUALISATION ON WEIGHT PARAMETER IN OTLM,0.5852534562211982,"to reﬁne the original predictions on CIFAR-100 data set with an imbalanced ratio R = 100 and
ResNet-32 backbone, and trained a one-layer model until it converged to extract the value of W ,
where the shape of is 100 × 100. We visualised a sub-block of size 20 × 20, the illustration is
presented on the right image in Figure 2. The visualised matrix is very dense, this non-sparsity
suggests that the OTLM manages to correct the predictions of the model in the training stage, whilst
minimising the distance between the desired and the actual distribution."
RELATED WORK,0.5898617511520737,"5
RELATED WORK"
RELATED WORK,0.5944700460829493,"Long-tailed recognition: The great majority of current methods applied to long-tailed recognition
can be divided into either training-aware and post-hoc correction methods. The training-aware
methods aim to improve the generalisation ability of models trained on imbalanced training data set.
Among them, re-sampling and re-weighting are two major methods. Re-sampling controls the class
numbers by the means of over-sampling (Chawla et al., 2002) and under-sampling (Barandela et al.,
2004; Drummond et al., 2003). The re-weighting approaches achieve balanced learning by giving
smaller weights to majority classes and larger weights to minority classes (Tang et al., 2008; Zadrozny
et al., 2003; Lin et al., 2017; Lee et al., 2017; Cui et al., 2019; Khan et al., 2019). Other approaches
have also been proposed to resolve the imbalanced problem. For example, Cao et al. introduces a
theoretically-principled loss function motivated by minimizing the generalisation error bound (Cao
et al., 2019), Kang et al. proposes to decouple the learning phases into representation learning and
classiﬁer tuning (Kang et al., 2020). Apart from training-aware methods, post-hoc correction also
offers us an avenue for long-tailed recognition problem, it can be applied to the normalisation of
classiﬁer weights (Kang et al., 2020; Zhang et al., 2019; Kim & Kim, 2020) or logits based on label
frequency (Provost, 2000; Collell et al., 2016). Among them, Logit Adjustment (Menon et al., 2021)
is a simple yet powerful method to use. Compared with Logit Adjustment, our approach performs the
post-hoc correction from the optimisation view."
RELATED WORK,0.5990783410138248,"Optimal transport: OT distance is a important family of distances for probability measures (Villani,
2008; Santambrogio, 2015). It has been extensively applied in different ﬁelds in machine learning.
Courty et al. (Courty et al., 2016) proposes a regularised unsupervised optimal transportation model
to perform the domain adaption. Other applications include 3D shape matching (Su et al., 2015),
generative model (Arjovsky et al., 2017; Salimans et al., 2018; Bunne et al., 2019; Deshpande
et al., 2019; Bhagoji et al., 2019; An et al., 2021), graph matching (Xu et al., 2019a;b), model
designs Kandasamy et al. (2018); Chizat & Bach (2018). To the best of our knowledge, we are the
ﬁrst to perform the post-hoc correction from the OT perspective."
CONCLUSION,0.6036866359447005,"6
CONCLUSION"
CONCLUSION,0.6082949308755761,"In this paper, we present two techniques for post-hoc corrections from an optimisation perspective.
Our approach reviews the basic mathematical formulation of the distribution alignment problem
and relates the resulting formulation to OT, which we ﬁnd, moreover, offers greater possibilities
and ﬂexibility to solve the problem. In order to avoid manual conﬁguration of the cost matrix, we
propose to learn the cost matrix automatically by linear mapping, which leads to further performance
improvements. The experimental results conﬁrm the superiority of our proposed approach. We
believe our work can open up more possibilities and imagination for post-hoc corrections. There
are potential challenges that remain to be addressed. These include, for example, how to apply OT
to correction of predictions under unknown marginal distribution. Meanwhile, an extension of our
approach to other applications where label shift is involved is considered possible."
CONCLUSION,0.6129032258064516,Published as a conference paper at ICLR 2022
REFERENCES,0.6175115207373272,REFERENCES
REFERENCES,0.6221198156682027,"Dongsheng An, Jianwen Xie, and Ping Li. Learning deep latent variable models by short-run MCMC
inference with optimal transport correction. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pp. 15415–15424, virtual, 2021."
REFERENCES,0.6267281105990783,"Martín Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial networks. In
Proceedings of the International Conference on Machine Learning (ICML), pp. 214–223, Sydney,
Australia, 2017."
REFERENCES,0.631336405529954,"Kamyar Azizzadenesheli, Anqi Liu, Fanny Yang, and Animashree Anandkumar. Regularized learning
for domain adaptation under label shifts. In Proceedings of International Conference on Learning
Representations (ICLR), New Orleans, LA, 2019."
REFERENCES,0.6359447004608295,"Ricardo Barandela, Rosa Maria Valdovinos, José Salvador Sánchez, and Francesc J. Ferri. The
imbalanced training sample problem: Under or over sampling? In Proceedings of the Joint IAPR
International Workshops on Structural, Syntactic, and Statistical Pattern Recognition (SSPR and
SPR), pp. 806, Lisbon, Portugal, 2004."
REFERENCES,0.6405529953917051,"Arjun Nitin Bhagoji, Daniel Cullina, and Prateek Mittal. Lower bounds on adversarial robustness
from optimal transport. In Advances in Neural Information Processing Systems (NeurIPS), pp.
7496–7508, Vancouver, BC, Canada, 2019."
REFERENCES,0.6451612903225806,"Paula Branco, Luís Torgo, and Rita P. Ribeiro. A survey of predictive modeling on imbalanced
domains. volume 49, pp. 1–50. ACM New York, NY, USA, 2016."
REFERENCES,0.6497695852534562,"Mateusz Buda, Atsuto Maki, and Maciej A. Mazurowski. A systematic study of the class imbalance
problem in convolutional neural networks. Neural Networks, 106:249–259, 2018."
REFERENCES,0.6543778801843319,"Charlotte Bunne, David Alvarez-Melis, Andreas Krause, and Stefanie Jegelka. Learning generative
models across incomparable spaces. In Proceedings of International Conference on Machine
Learning (ICML), pp. 851–861, Long Beach, CA, 2019."
REFERENCES,0.6589861751152074,"Christopher J. C. Burges, Léon Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger. Sinkhorn
distances: Lightspeed computation of optimal transport. In Advances in neural information
processing systems (NeurIPS), pp. 2292–2300, Lake Tahoe, Nevada, United States, 2013."
REFERENCES,0.663594470046083,"Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Aréchiga, and Tengyu Ma. Learning imbalanced
datasets with label-distribution-aware margin loss. In Advances in Neural Information Processing
Systems (NeurIPS), pp. 1565–1576, Vancouver, Canada, 2019."
REFERENCES,0.6682027649769585,"Claire Cardie and Nicholas Nowe. Improving minority class prediction using case-speciﬁc feature
weights. In Proceedings of the International Conference on Machine Learning (ICML, pp. 57–65,
Nashville, Tennessee, USA, 1997. Morgan Kaufmann."
REFERENCES,0.6728110599078341,"Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic
minority over-sampling technique. Journal of artiﬁcial intelligence research (JMLR), 16:321–357,
2002."
REFERENCES,0.6774193548387096,"Lénaïc Chizat and Francis R. Bach.
On the global convergence of gradient descent for over-
parameterized models using optimal transport. In Annual Conference on Neural Information
Processing Systems, (NeurIPS), pp. 3040–3050, Montréal, Canada, 2018."
REFERENCES,0.6820276497695853,"Guillem Collell, Drazen Prelec, and Kaustubh R. Patil. Reviving threshold-moving: a simple plug-in
bagging ensemble for binary and multiclass imbalanced data. arXiv preprint arXiv:1606.08698,
2016."
REFERENCES,0.6866359447004609,"Nicolas Courty, Rémi Flamary, Devis Tuia, and Alain Rakotomamonjy. Optimal transport for
domain adaptation. IEEE transactions on pattern analysis and machine intelligence (PAMI), 39(9):
1853–1865, 2016."
REFERENCES,0.6912442396313364,"Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge J. Belongie. Class-balanced loss based
on effective number of samples. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pp. 9268–9277, Long Beach, CA, USA, 2019."
REFERENCES,0.695852534562212,Published as a conference paper at ICLR 2022
REFERENCES,0.7004608294930875,"Ishan Deshpande, Yuan-Ting Hu, Ruoyu Sun, Ayis Pyrros, Nasir Siddiqui, Sanmi Koyejo, Zhizhen
Zhao, David A. Forsyth, and Alexander G. Schwing. Max-sliced wasserstein distance and its use
for gans. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 10648–10656, Long Beach, CA, USA, 2019."
REFERENCES,0.7050691244239631,"Chris Drummond, Robert C Holte, et al. C4. 5, class imbalance, and cost sensitivity: why under-
sampling beats over-sampling. In Workshop on learning from imbalanced datasets II, volume 11,
pp. 1–8. Citeseer, 2003."
REFERENCES,0.7096774193548387,"Charlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya-Polo, and Tomaso A. Poggio.
Learning with a wasserstein loss. In Advances in neural information processing systems (NeurIPS),
pp. 2053–2061, Montreal, Quebec, Canada, 2015."
REFERENCES,0.7142857142857143,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 770–778, Las Vegas, NV, USA, 2016."
REFERENCES,0.7188940092165899,"Dan Hendrycks, Mantas Mazeika, Duncan Wilson, and Kevin Gimpel. Using trusted data to train
deep networks on labels corrupted by severe noise. In Advances in neural information processing
systems (NeurIPS), pp. 10477–10486, Montréal, Canada, 2018."
REFERENCES,0.7235023041474654,"Youngkyu Hong, Seungju Han, Kwanghee Choi, Seokjun Seo, Beomsu Kim, and Buru Chang.
Disentangling label distribution for long-tailed visual recognition. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6626–6636, virtual, 2021."
REFERENCES,0.728110599078341,"Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alexander Shepard, Hartwig
Adam, Pietro Perona, and Serge J. Belongie. The inaturalist species classiﬁcation and detection
dataset. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 8769–8778, Salt Lake City, UT, USA, 2018."
REFERENCES,0.7327188940092166,"Chen Huang, Yining Li, Chen Change Loy, and Xiaoou Tang. Learning deep representation for
imbalanced classiﬁcation. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 5375–5384. IEEE Computer Society, 2016."
REFERENCES,0.7373271889400922,"Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabás Póczos, and Eric P. Xing.
Neural architecture search with bayesian optimisation and optimal transport. In Annual Conference
on Neural Information Processing Systems, (NeurIPS), pp. 2020–2029, Montréal, Canada, 2018."
REFERENCES,0.7419354838709677,"Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, and Yannis
Kalantidis. Decoupling representation and classiﬁer for long-tailed recognition. 2020."
REFERENCES,0.7465437788018433,"Salman Khan, Munawar Hayat, Syed Waqas Zamir, Jianbing Shen, and Ling Shao. Striking the right
balance with uncertainty. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 103–112, Long Beach, CA, USA, 2019."
REFERENCES,0.7511520737327189,"Byungju Kim and Junmo Kim. Adjusting decision boundary for class imbalanced learning. 8:
81674–81685, 2020."
REFERENCES,0.7557603686635944,"Jaehyung Kim, Youngbum Hur, Sejun Park, Eunho Yang, Sung Ju Hwang, and Jinwoo Shin. Distri-
bution aligning reﬁnery of pseudo-label for imbalanced semi-supervised learning. In Advances in
Neural Information Processing Systems (NeurIPS), pp. 1565–1576, Vancouver, Canada, 2020a."
REFERENCES,0.7603686635944701,"Jaehyung Kim, Jongheon Jeong, and Jinwoo Shin. M2m: Imbalanced classiﬁcation via major-
to-minor translation. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 13896–13905, Seattle, WA, USA, 2020b."
REFERENCES,0.7649769585253456,"Wonji Lee, Chi-Hyuck Jun, and Jong-Seok Lee. Instance categorization by support vector machines
to adjust weights in adaboost for imbalanced data classiﬁcation. Information Sciences, 381:92–103,
2017."
REFERENCES,0.7695852534562212,"Shuang Li, Kaixiong Gong, Chi Harold Liu, Yulin Wang, Feng Qiao, and Xinjing Cheng. Metasaug:
Meta semantic augmentation for long-tailed visual recognition. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5212–5221, virtual, 2021."
REFERENCES,0.7741935483870968,Published as a conference paper at ICLR 2022
REFERENCES,0.7788018433179723,"Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense ob-
ject detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 2999–3007, Venice,Italy, 2017."
REFERENCES,0.783410138248848,"Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X. Yu. Large-scale
long-tailed recognition in an open world. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pp. 2537–2546, Long Beach, CA, USA, 2019."
REFERENCES,0.7880184331797235,"Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain, Andreas Veit, and
Sanjiv Kumar. Long-tail learning via logit adjustment. In Proceedings of International Conference
on Learning Representations (ICLR), Virtual Event, Austria, 2021. OpenReview.net."
REFERENCES,0.7926267281105991,"Gabriel Peyré, Marco Cuturi, et al. Computational optimal transport: With applications to data
science. Foundations and Trends® in Machine Learning, 11(5-6):355–607, 2019."
REFERENCES,0.7972350230414746,"Foster Provost. Machine learning from imbalanced data sets 101. In Proceedings of the AAAI’2000
workshop on imbalanced data sets, volume 68, pp. 1–3. AAAI Press, 2000."
REFERENCES,0.8018433179723502,"Xingye Qiao and Yufeng Liu. Adaptive weighted learning for unbalanced multicategory classiﬁcation.
Biometrics, 65(1):159–168, 2009."
REFERENCES,0.8064516129032258,"Jiawei Ren, Cunjun Yu, Shunan Sheng, Xiao Ma, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Balanced
meta-softmax for long-tailed visual recognition. In Advances in neural information processing
systems (NeurIPS), virtual, 2020."
REFERENCES,0.8110599078341014,"Tim Salimans, Han Zhang, Alec Radford, and Dimitris N. Metaxas. Improving gans using opti-
mal transport. In Proceedings of International Conference on Learning Representations, ICLR,
Vancouver, BC, Canada, 2018. OpenReview.net."
REFERENCES,0.815668202764977,"Filippo Santambrogio. Optimal transport for applied mathematicians. Birkäuser, NY, 55(58-63):94,
2015."
REFERENCES,0.8202764976958525,"Richard Sinkhorn. Diagonal equivalence to matrices with prescribed row and column sums. ii.
Proceedings of the American Mathematical Society, 45(2):195–198, 1974."
REFERENCES,0.8248847926267281,"Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin Raffel, Ekin Dogus
Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with
consistency and conﬁdence. In Advances in Neural Information Processing Systems (NeurIPS),
virtual, 2020."
REFERENCES,0.8294930875576036,"Zhengyu Su, Yalin Wang, Rui Shi, Wei Zeng, Jian Sun, Feng Luo, and Xianfeng Gu. Optimal mass
transport for shape matching and comparison. IEEE transactions on pattern analysis and machine
intelligence (PAMI), 37(11):2246–2259, 2015."
REFERENCES,0.8341013824884793,"Kaihua Tang, Jianqiang Huang, and Hanwang Zhang. Long-tailed classiﬁcation by keeping the good
and removing the bad momentum causal effect. In Advances in Neural Information Processing
Systems (NeurIPS), virtual, 2020."
REFERENCES,0.8387096774193549,"Yuchun Tang, Yan-Qing Zhang, Nitesh V. Chawla, and Sven Krasser. Svms modeling for highly
imbalanced classiﬁcation. IEEE Trans. Syst. Man Cybern. Part B, 39(1):281–288, 2008."
REFERENCES,0.8433179723502304,"Grant Van Horn and Pietro Perona. The devil is in the tails: Fine-grained classiﬁcation in the wild.
arXiv preprint arXiv:1709.01450, 2017."
REFERENCES,0.847926267281106,"Thomas Viehmann. Implementation of batched sinkhorn iterations for entropy-regularized wasserstein
loss. arXiv preprint arXiv:1907.01729, 2019."
REFERENCES,0.8525345622119815,"Cédric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media,
2008."
REFERENCES,0.8571428571428571,"Renzhen Wang, Kaiqin Hu, Yanwen Zhu, Jun Shu, Qian Zhao, and Deyu Meng. Meta feature
modulator for long-tailed recognition. arXiv preprint arXiv:2008.03428, 2020."
REFERENCES,0.8617511520737328,Published as a conference paper at ICLR 2022
REFERENCES,0.8663594470046083,"Xudong Wang, Long Lian, Zhongqi Miao, Ziwei Liu, and Stella X. Yu. Long-tailed recognition
by routing diverse distribution-aware experts. In Proceedings of International Conference on
Learning Representations (ICLR), Virtual, 2021."
REFERENCES,0.8709677419354839,"Liuyu Xiang, Guiguang Ding, and Jungong Han. Learning from multiple experts: Self-paced
knowledge distillation for long-tailed classiﬁcation. In Proceedings of European Conference on
Computer Vision (ECCV), pp. 247–263, Glasgow,UK, 2020. Springer."
REFERENCES,0.8755760368663594,"Saining Xie, Ross B. Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual
transformations for deep neural networks. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pp. 6626–6636, Honolulu, HI, USA, 2021."
REFERENCES,0.880184331797235,"Hongteng Xu, Dixin Luo, and Lawrence Carin. Scalable gromov-wasserstein learning for graph
partitioning and matching. In Advances in Neural Information Processing Systems (NeurIPS), pp.
3046–3056, Vancouver, BC, Canada, 2019a."
REFERENCES,0.8847926267281107,"Hongteng Xu, Dixin Luo, Hongyuan Zha, and Lawrence Carin. Gromov-wasserstein learning for
graph matching and node embedding. In Proceedings of International Conference on Machine
Learning (ICML), pp. 6932–6941, Long Beach, California, USA, 2019b. PMLR."
REFERENCES,0.8894009216589862,"Bianca Zadrozny, John Langford, and Naoki Abe. Cost-sensitive learning by cost-proportionate
example weighting. In Third IEEE international conference on data mining (ICDM), pp. 435–442,
Melbourne, Florida, USA, 2003. IEEE."
REFERENCES,0.8940092165898618,"Junjie Zhang, Lingqiao Liu, Peng Wang, and Chunhua Shen. To balance or not to balance: A simple-
yet-effective approach for learning with long-tailed distributions. arXiv preprint arXiv:1912.04486,
2019."
REFERENCES,0.8986175115207373,"Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. Bbn: Bilateral-branch network with
cumulative learning for long-tailed visual recognition. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pp. 9719–9728, Seattle, WA, USA, 2020."
REFERENCES,0.9032258064516129,Published as a conference paper at ICLR 2022
REFERENCES,0.9078341013824884,"A
OPTIMAL TRANSPORT ON LONG-TAILED SEMI-SUPERVISED
CLASSIFICATION"
REFERENCES,0.9124423963133641,"Together, with the extension of our approach, we adapt it to semi-supervised long-tailed classiﬁca-
tion tasks. We bring our approach together with the advanced semi-supervised learning methods
FixMatch Sohn et al. (2020). Adhering to the conﬁguration in DARP Kim et al. (2020a), we also
create synthetically long-tailed variants of CIFAR-100 with imbalanced ratio R = 20. The sample
size of the unlabeled data is twice as large as the labelled data, with the same proportion of imbal-
ance between them. To assess the performance, we report two popular metrics: balanced accuracy
(bACC) Huang et al. (2016) and geometric mean scores (GM) Branco et al. (2016). Experimentally,
with the exception of an OT to align the marginal distribution of the unlabelled data, all settings are
consistent with DARP, and we set λ to 0.005. The results are reported in Table 5. It can be seen
that our approach also achieves much better performance on semi-supervised learning tasks. It can
somehow mimic the online scenarios. And the advantages of OT coping with such cases can be
demonstrated."
REFERENCES,0.9170506912442397,"Table 5: Comparison of classiﬁcation performance (bACC/GM) on CIFAR-100 with imbalanced
ratio R = 20, We report standard deviation and mean values for each evaluation metric. Better results
are marked in bold."
REFERENCES,0.9216589861751152,"Method
CIFAR-100
bACC
GM
DARP
54.9±0.05
46.4±0.41
OT
56.6±0.08
48.4±0.32"
REFERENCES,0.9262672811059908,"B
PARAMETER STUDY OF λ"
REFERENCES,0.9308755760368663,"We conduct an additional experiment on iNaturalist data set to study the effect of λ on the ﬁnal
performance. The value of λ is selected from {0.01, 05, 0.1, 1.0.2.0, 5.0}, all other experimental
settings are the same, including various hyperparameters, network structure, etc. As illustrated in
Figure 3, we can ﬁnd that OT achieves the best performance when λ = 1."
REFERENCES,0.9354838709677419,Figure 3: Visualisation of the accuracy change curve with λ on iNaturalist data set.
REFERENCES,0.9400921658986175,"C
IMPLEMENTATION DETAILS"
REFERENCES,0.9447004608294931,The speciﬁc implementation details for each data set under the different methods are described below.
REFERENCES,0.9493087557603687,"C.1
IMPLEMENTATION DETAILS OF CROSS ENTROPY"
REFERENCES,0.9539170506912442,"CIFAR-100-LT: During the training stage, following the recipe of (Tang et al., 2020; Hong et al.,
2021), we apply SGD with batch size 256 and weight decay 0.0005 to train a ResNet-32 (He et al.,
2016) model for 200 epochs, we employ the linear warm-up learning rate schedule for the ﬁrst ﬁve"
REFERENCES,0.9585253456221198,Published as a conference paper at ICLR 2022
REFERENCES,0.9631336405529954,"epochs. We also set the base learning rate to 0.2 and reduce it at epoch 120 and 160 by a factor of
100. During the post-hoc stage, we use Adam with batch size 10k and weight decay 0.001 to train the
one-layer feed-forward network for 20 iterations. We set λ to 0.1, T to 200, ∆to 0.001. The learning
rate is constant and set to 0.001."
REFERENCES,0.967741935483871,"ImageNet-LT: During the training stage, following the implementation of (Hong et al., 2021), we
train the ResNeXt-50-32x4d (Xie et al., 2021) for 90 epochs and perform a cosine learning rate
scheme with an initial learning rate of 0.05. SGD is also employed to optimise the neural network
with batch size 256, weight decay 0.0005, and momentum 0.9. During the post-hoc stage, we use
SGD with batch size 10k, weight decay 0.001 to train the one-layer feed-forward network for 100
iterations. The constant learning rate is 0.2. We set λ to 1.2, T to 7, ∆to 0.001."
REFERENCES,0.9723502304147466,"iNaturalist: During the training stage, ResNet-50 (He et al., 2016) is chosen as the backbone network.
We use SGD with momentum 0.9, batch size 256 to train the network. We utilise the cosine learning
rate schedule gradually decaying from 0.1 to 0. During the post-hoc stage, we use SGD with batch
size 23,826, weight decay 0.001, λ to 1.0, ∆to 0.001 and T to 10. The constant learning rate is set to
0.05. As iNaturalist has no test data set, we directly report the performance on validation data set."
REFERENCES,0.9769585253456221,"C.2
IMPLEMENTATION DETAILS OF RIDE"
REFERENCES,0.9815668202764977,"The training details we use in RIDE share the same as the original paper, there are 6 experts in our
RIDE on all three data sets."
REFERENCES,0.9861751152073732,"CIFAR-100-LT
We experimented with R = 100 and ResNet-32 as the base network. SGD is
utilised as our solver with momentum 0.9, batch size 128, and epoch 200. The learning rate is
initialised as 0.1 and decayed by 0.01 at epoch 120 and 160 respectively."
REFERENCES,0.9907834101382489,"ImageNet-LT
We take experiments with ResNeXt-50 as backbone with 100 epochs. We set the
initial learning rate to 0.1 and reduce it by 10 at epoch 60 and 80. SGD with batch size 256 and
momentum 0.9 is also adopted as the optimisation solver here."
REFERENCES,0.9953917050691244,"iNaturalist
We conduct experiments with ResNet-50 as backbone. The training details are the
same as ImageNet-LT except from batch size 512."
