Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.004784688995215311,"Unlike matrix completion, tensor completion does not have an algorithm that is
known to achieve the information-theoretic sample complexity rate. This paper
develops a new algorithm for the special case of completion for nonnegative tensors.
We prove that our algorithm converges in a linear (in numerical tolerance) number
of oracle steps, while achieving the information-theoretic rate. Our approach is
to define a new norm for nonnegative tensors using the gauge of a particular 0-1
polytope; integer linear programming can, in turn, be used to solve linear separation
problems over this polytope. We combine this insight with a variant of the Frank-
Wolfe algorithm to construct our numerical algorithm, and we demonstrate its
effectiveness and scalability through computational experiments using a laptop on
tensors with up to one-hundred million entries."
INTRODUCTION,0.009569377990430622,"1
Introduction"
INTRODUCTION,0.014354066985645933,"Tensors generalize matrices. A tensor ψ of order p is ψ ∈Rr1×···×rp, where ri is the dimension of
the tensor in the i-th index, for i = 1, . . . , p. Though related, many problems that are polynomial-time
solvable for matrices are NP-hard for tensors. For instance, it is NP-hard to compute the rank of a
tensor (Hillar and Lim, 2013). Tensor versions of the spectral norm, nuclear norm, and singular value
decomposition are also NP-hard to compute (Hillar and Lim, 2013; Friedland and Lim, 2014)."
INTRODUCTION,0.019138755980861243,"1.1
Past Approaches to Tensor Completion"
INTRODUCTION,0.023923444976076555,"Tensor completion is the problem of observing (possibly with noise) a subset of entries of a tensor and
then estimating the remaining entries based on an assumption of low-rankness. The tensor completion
problem is encountered in a number of important applications, including computer vision (Liu et al.,
2012; Zhang et al., 2019), regression with only categorical variables (Aswani, 2016), healthcare
(Gandy et al., 2011; Dauwels et al., 2011), and many other application domains (Song et al., 2019)."
INTRODUCTION,0.028708133971291867,"Although the special case of matrix completion is now well understood, the tensor version of this
problem has an unresolved tension. To date, no tensor completion algorithm has been shown to achieve
the information-theoretic sample complexity rate. Namely, for a tensor completion problem on a
rank k tensor with sample size n, the information-theoretic rate for estimation error is
p k · P"
INTRODUCTION,0.03349282296650718,"i ri/n
(Gandy et al., 2011). In fact, evidence suggests a computational barrier in which no polynomial-time
algorithm can achieve this rate (Barak and Moitra, 2016). One set of approaches has polynomial-time
computation but requires exponentially more samples than the information-theoretic rate (Gandy
et al., 2011; Mu et al., 2014; Barak and Moitra, 2016; Montanari and Sun, 2018), whereas another
set of approaches achieve substantially closer to the information-theoretic rate but (in order to attain
global minima) requires solving NP-hard problems that lack numerical algorithms (Chandrasekaran
et al., 2012; Yuan and Zhang, 2016, 2017; Rauhut and Stojanac, 2021)."
INTRODUCTION,0.03827751196172249,"However, algorithms that achieve the information-theoretic rate have been developed for a few special
cases of tensors. Completion of nonnegative rank-1 tensors can be written as a convex optimization
problem (Aswani, 2016). For symmetric orthogonal tensors, a variant of the Frank-Wolfe algorithm
has been proposed (Rao et al., 2015). Though this latter paper does not prove their algorithm achieves
the information-theoretic rate, this can be shown using standard techniques (Gandy et al., 2011). This
latter paper is closely related to the approach we take in this paper, with one of the key differences
being the design of a different separation oracle in order to support a different class of tensors."
INTRODUCTION,0.0430622009569378,"1.2
Contributions"
INTRODUCTION,0.04784688995215311,"This paper proposes a numerical algorithm for completion of nonnegative tensors that provably
converges to a global minimum in a linear (in numerical tolerance) number of oracle steps, while
achieving the information-theoretic rate. Nonnegative tensors are encountered in many applications.
For instance, image and video data usually consists of nonnegative tensors (Liu et al., 2012; Zhang
et al., 2019). Notably, the image demosaicing problem that must be solved by nearly every digital
camera (Li et al., 2008) is an instance of a nonnegative tensor completion problem, though it has not
previously been interpreted as such. Nonnegative tensor completion is also encountered in specific
instances of recommender systems (Song et al., 2019), healthcare applications (Gandy et al., 2011;
Dauwels et al., 2011), and statistical regression contexts (Aswani, 2016)."
INTRODUCTION,0.05263157894736842,"Our approach is to define a new norm for nonnegative tensors using the gauge of a specific 0-1
polytope that we construct. We prove this norm acts as a convex surrogate for rank and has low
statistical complexity (as measured by the Rademacher average for the tensor, viewed as a function
from a set of indices to the corresponding entry), but is NP-hard to approximate to an arbitrary
accuracy. Importantly, we prove that the tensor completion problem using this norm achieves the
information-theoretic rate in terms of sample complexity. However, the resulting tensor completion
problem is NP-hard to solve. Nonetheless, because our new norm is defined using a 0-1 polytope,
we can use integer linear optimization as a practical means to solve linear separation problems over
the polytope. We combine this insight with a variant of the Frank-Wolfe algorithm to construct our
numerical algorithm, and we demonstrate its effectiveness and scalability through experiments."
IMPLEMENTATION/METHODS,0.05741626794258373,"2
Norm for Nonnegative Tensors"
IMPLEMENTATION/METHODS,0.06220095693779904,"Consider a tensor ψ ∈Rr1×···×rp. To refer to a specific entry in the tensor we use the notation
ψx := ψx1,...,xp, where x = (x1, . . . , xp), and xi ∈[ri] denotes the value of the i-th index, with
[s] := {1, . . . , s}. Let ρ = P"
IMPLEMENTATION/METHODS,0.06698564593301436,"i ri, π = Q"
IMPLEMENTATION/METHODS,0.07177033492822966,"i ri, and R = [r1] × · · · × [rp]."
IMPLEMENTATION/METHODS,0.07655502392344497,"A nonnegative rank-1 tensor is ψx = Qp
k=1 θ(k)
xk , where θ(k) ∈Rrk
+ are nonnegative vectors indexed
by the different values of xk ∈[rk]. To simplify notation, we drop the superscript in θ(k)
xk and write
this as θxk when clear from the context. For a nonnegative tensor ψ, its nonnegative rank is"
IMPLEMENTATION/METHODS,0.08133971291866028,"rank+(ψ) = min{q | ψ = Pq
k=1 ψk, ψk ∈B∞for k ∈[q]},
(1)"
IMPLEMENTATION/METHODS,0.0861244019138756,where we define the ball of nonnegative rank-1 tensors whose maximum entry is λ ∈R+ to be
IMPLEMENTATION/METHODS,0.09090909090909091,"Bλ = {ψ : ψx = λ · Qp
k=1 θxk, θxk ∈[0, 1], for x ∈R}
(2)"
IMPLEMENTATION/METHODS,0.09569377990430622,"and B∞= limλ→∞Bλ.
A nonnegative CP decomposition is given by the summation ψ =
Prank+(ψ)
k=1
ψk, where ψk ∈B∞for k ∈[rank+(ψ)]."
IMPLEMENTATION/METHODS,0.10047846889952153,"2.1
Convex Hull of Nonnegative Rank-1 Tensors"
IMPLEMENTATION/METHODS,0.10526315789473684,Now for λ ∈R+ consider the finite set of points:
IMPLEMENTATION/METHODS,0.11004784688995216,"Sλ = {ψ : ψx = λ · Qp
k=1 θxk, θxk ∈{0, 1} for x ∈R}.
(3)"
IMPLEMENTATION/METHODS,0.11483253588516747,"Using standard techniques from integer optimization (Hansen, 1979; Padberg, 1989), the above set
of points can be rewritten as a set of linear constraints on binary variables: Sλ = {ψ : λ · (1 −p) +
λ · Pp
k=1 θxk ≤ψx, 0 ≤ψx ≤λ · θxk, θxk ∈{0, 1}, for k ∈[p], x ∈R}. Our first result is that the
convex hulls of the set of points Sλ and of the ball Bλ are equivalent.
Proposition 2.1. We have the relation that Cλ := conv(Bλ) = conv(Sλ)."
IMPLEMENTATION/METHODS,0.11961722488038277,"Proof. We prove this by showing the two set inclusions conv(Sλ) ⊆conv(Bλ) and conv(Bλ) ⊆
conv(Sλ). The first inclusion is immediate since by definition we have Sλ ⊂Bλ, and so we focus
on proving the second inclusion. We prove this by contradiction: Suppose conv(Bλ) ̸⊂conv(Sλ).
Then there exists a tensor ψ′ ∈Bλ with ψ′ ̸∈conv(Sλ). By the hyperplane separation theorem,
there exists φ ∈Rr1×···×rp and δ > 0 such that ⟨φ, ψ′⟩≥⟨φ, ψ⟩+ δ for all ψ ∈conv(Sλ), where
⟨·, ·⟩is the usual inner product that is defined as the summation of elementwise multiplication. Now
consider the multilinear optimization problem
max ⟨φ, ψ⟩"
IMPLEMENTATION/METHODS,0.12440191387559808,"s.t. ψx = λ · Qp
k=1 θxk,
for x ∈R
θxk ∈[0, 1],
for x ∈R
(4)"
IMPLEMENTATION/METHODS,0.1291866028708134,"Proposition 2.1 of (Drenick, 1992) shows there exists a global optimum ψ′′ of (4) with ψ′′ ∈Sλ.
By construction, we must have ⟨φ, ψ′′⟩≥⟨φ, ψ′⟩, which implies ⟨φ, ψ′′⟩≥⟨φ, ψ⟩+ δ for all
ψ ∈conv(Sλ). But this last statement is a contradiction since ψ′′ ∈Sλ ⊆conv(Sλ)."
IMPLEMENTATION/METHODS,0.1339712918660287,"Remark 2.2. This result has two useful implications. First, Cλ is a polytope since it is the convex hull
of a finite number of bounded points. Second, the elements of Sλ are the vertices of Cλ, since any
individual element cannot be written as a convex combination of the other elements."
IMPLEMENTATION/METHODS,0.13875598086124402,"We will call the set Cλ the nonnegative tensor polytope. A useful observation is that the following
relationships hold: Bλ = λB1, Sλ = λS1, and Cλ = λC1."
IMPLEMENTATION/METHODS,0.14354066985645933,"2.2
Constructing a Norm for Nonnegative Tensors"
IMPLEMENTATION/METHODS,0.14832535885167464,"Since the set of nonnegative tensors forms a cone (Qi et al., 2014), we need to use a modified
definition of a norm. A norm on a cone K is a function p : K →R+ such that for all x, y ∈K the
function has the following three properties: p(x) = 0 if and only if x = 0; p(γ · x) = γ · p(x) for
γ ∈R+; and p(x + y) ≤p(x) + p(y). The difference with the usual norm definition is subtle: The
second property here is required to hold for γ ∈R+, whereas in the usual norm definition we require
p(γ · x) = |γ| · p(x) for all γ ∈R."
IMPLEMENTATION/METHODS,0.15311004784688995,"We next use Cλ to construct a new norm for nonnegative tensors using a gauge (or Minkowski
functional) construction. Though constructing norms using a gauge is common in machine learning
(Chandrasekaran et al., 2012), the convex sets used in these constructions are symmetric about the
origin. Symmetry guarantees that scaling the ball eventually includes the entire space (Rockafellar
and Wets, 2009). However, in our case Cλ is not symmetric about the origin, and so without proof we
do not a priori know whether scaling C1 eventually includes the entire space of nonnegative tensors.
Thus we have to explicitly prove the gauge is a norm.
Proposition 2.3. The function defined as
∥ψ∥+ := inf{λ ≥0 | ψ ∈λC1}
(5)"
IMPLEMENTATION/METHODS,0.15789473684210525,"is a norm for nonnegative tensors ψ ∈Rr1×···×rp
+
."
IMPLEMENTATION/METHODS,0.16267942583732056,"Proof. We first prove the above function is finite. Consider any nonnegative tensor ψ ∈Rr1×···rp
+
,
and note there exists a decomposition ψ = Pπ
i=1 ψk with ψk ∈∥ψ∥max · B1 (Qi et al., 2016). (This
holds because we can choose each ψk to have a single non-zero value that corresponds to a different
entry of ψ.) Hence by Proposition 2.1, ψk ∈∥ψ∥max · C1. Recalling the decomposition of ψ, this
means ψ ∈π∥ψ∥max · C1 which by definition means ∥ψ∥+ ≤π∥ψ∥max. Thus ∥ψ∥+ must be finite."
IMPLEMENTATION/METHODS,0.1674641148325359,"Next we verify that the three properties of a norm on a cone are satisfied. To do so, we first observe
that by definition: C1 is convex; 0 ∈C1; and C1 is closed and bounded. Thus by Example 3.50
of (Rockafellar and Wets, 2009) we have {ψ : ∥ψ∥+ = 0} = {0}, which means the first norm
property holds. Also by Example 3.50 of (Rockafellar and Wets, 2009) we have λC1 ⊆λ′C1 for all
0 ≤λ ≤λ′, which means the second norm property holds. Last, note Example 3.50 of (Rockafellar
and Wets, 2009) establishes sublinearity of the gauge, and so the third norm property holds."
IMPLEMENTATION/METHODS,0.1722488038277512,"2.3
Convex Surrogate for Nonnegative Tensor Rank"
IMPLEMENTATION/METHODS,0.17703349282296652,"An important property of our norm ∥ψ∥+ is that it can be used as a convex surrogate for tensor rank,
whereas the max and Frobenius norms cannot."
IMPLEMENTATION/METHODS,0.18181818181818182,"Proposition 2.4. If ψ is a nonnegative tensor, then we have ∥ψ∥max ≤∥ψ∥+ ≤rank+(ψ) · ∥ψ∥max.
If rank+(ψ) = 1, then ∥ψ∥+ = ∥ψ∥max."
IMPLEMENTATION/METHODS,0.18660287081339713,"Proof. Consider any λ ≥0. If ψ ∈Sλ, then by definition ∥ψ∥max = λ. By the convexity of norms
we have that: if ψ ∈Cλ, then ∥ψ∥max ≤λ. This means that for all λ ≥0 we have Cλ ⊆Uλ := {ψ :
∥ψ∥max ≤λ} . Thus we have the relation inf{λ ≥0 | ψ ∈λU1} ≤inf{λ ≥0 | ψ ∈λC1}. But the
left side is ∥ψ∥max and the right side is ∥ψ∥+. This proves the left side of the inequality."
IMPLEMENTATION/METHODS,0.19138755980861244,"Next consider the case where rank+(ψ) = 1. By definition we have ψ ∈∥ψ∥max · B1. Thus
ψ ∈∥ψ∥max · C1, and so ∥ψ∥+ ≤∥ψ∥max. This proves the rank-1 case."
IMPLEMENTATION/METHODS,0.19617224880382775,"Last we prove the right side of the desired inequality. Consider any nonnegative tensor ψ. Recall its
nonnegative CP decomposition ψ = Prank+(ψ)
k=1
ψk with ψk ∈B∞. The triangle inequality gives"
IMPLEMENTATION/METHODS,0.20095693779904306,"∥ψ∥+ ≤Prank+(ψ)
k=1
∥ψk∥+ = Prank+(ψ)
k=1
∥ψk∥max,
(6)"
IMPLEMENTATION/METHODS,0.20574162679425836,"where the equality follows from the above-proved rank-1 case since rank+(ψk) = 1 by definition of
the CP decomposition. But ∥ψk∥max ≤∥ψ∥max since the tensors are nonnegative."
IMPLEMENTATION/METHODS,0.21052631578947367,"Remark 2.5. These bounds are tight. The lower and upper bounds are achieved by all nonnegative
rank-1 tensors. The identity matrix with k columns achieves the upper bound with rank+(ψ) = k."
IMPLEMENTATION/METHODS,0.215311004784689,"3
Measures of Norm Complexity"
IMPLEMENTATION/METHODS,0.22009569377990432,"3.1
Computational Complexity"
IMPLEMENTATION/METHODS,0.22488038277511962,"We next characterize the computational complexity of our new norm on nonnegative tensors.
Proposition 3.1. It is NP-hard to approximate the nonnegative tensor norm ∥· ∥+ to arbitrary
accuracy."
IMPLEMENTATION/METHODS,0.22966507177033493,"Proof. The dual norm is ∥φ∥◦= sup{⟨φ, ψ⟩| ∥ψ∥+ ≤1} for all tensors φ ∈Rr1×···×rp. Theorems
3 and 10 of (Friedland and Lim, 2016) show that approximation of the dual norm ∥· ∥◦is polynomial-
time reducible to approximation of the norm ∥· ∥+. We proceed by showing a polynomial-time
reduction to approximation of ∥· ∥◦. Without loss of generality assume p = 2 and d := r1 =
r2. The appendix of (Witsenhausen, 1986) proves that MAX CUT is polynomial-time reducible to
sup{⟨φ, ψ⟩| ψ ∈S1}. However, since the objective function is linear and the set S1 consists of the
vertices of C1, this means that optimization problem is equivalent to sup{⟨φ, ψ⟩| ψ ∈C1}. This is
the desired polynomial-time reduction of MAX CUT to approximation of the dual norm ∥· ∥◦because
C1 = {ψ : ∥ψ∥+ ≤1}. The result now follows by recalling that approximately solving MAX CUT to
arbitrary accuracy is NP-hard (Papadimitriou and Yannakakis, 1991; Arora et al., 1998)."
IMPLEMENTATION/METHODS,0.23444976076555024,"Corollary 3.2. Given K ∈R+ and ψ ∈Rr1×···×rp
+
, it is NP-complete to determine if ∥ψ∥+ ≤K."
IMPLEMENTATION/METHODS,0.23923444976076555,"Proof. The problem is NP-hard by reduction from the problem of Proposition 3.1. In particular, a
binary search can approximate the norm using this decision problem, (∥ψ∥+ ≤K)?, as an oracle.
By Proposition 2.4, the search can be initialized over the interval [0, π∥ψ∥max]. Furthermore, the
decision problem is in NP because we can use the (polynomial-sized) θ from (2) as a certificate."
IMPLEMENTATION/METHODS,0.24401913875598086,"3.2
Stochastic Complexity of Norm"
IMPLEMENTATION/METHODS,0.24880382775119617,"We next show that our norm ∥ψ∥+ has low stochastic complexity. Let X = {x⟨1⟩, . . . , x⟨n⟩}, and
suppose σi are independent and identically distributed (i.i.d.) Rademacher random variables (i.e.,
σi = ±1 with probability 1"
IMPLEMENTATION/METHODS,0.2535885167464115,"2) (Bartlett and Mendelson, 2002; Srebro et al., 2010). The Rademacher
complexity for a set of functions H is R(H) = E(suph∈H
1
n| Pn
i=1 σi · h(x⟨i⟩)|), and the worst case
Rademacher complexity of H is W(H) = supX Eσ(suph∈H
1
n| Pn
i=1 σi · h(x⟨i⟩)|). These notions
can be used to measure the complexity of sets of matrices (Srebro and Shraibman, 2005) or tensors
(Aswani, 2016). The idea is to interpret each nonnegative tensor as a function ψ : R →R+ from a
set of indices x ∈R to the corresponding entry of the tensor ψx. This complexity notion is useful for
the completion problem because it can be directly translated into generalization bounds."
IMPLEMENTATION/METHODS,0.2583732057416268,"Proposition 3.3. We have R(Cλ) ≤W(Cλ) ≤2λ
p ρ/n."
IMPLEMENTATION/METHODS,0.2631578947368421,"Proof. First note that from their definitions we get R(Cλ) ≤W(Cλ). Next define the set Pλ = {±ψ :
ψ ∈Sλ}, and recall that Cλ = conv(Sλ) by Proposition 2.1. This means W(Cλ) = W(Sλ) (Ledoux
and Talagrand, 1991; Bartlett and Mendelson, 2002). Next observe that"
IMPLEMENTATION/METHODS,0.2679425837320574,"W(Cλ) = W(Sλ) = supX Eσ

supψ∈Sλ
1
n
 Pn
i=1 σi · ψx⟨i⟩
"
IMPLEMENTATION/METHODS,0.2727272727272727,"= supX Eσ

maxψ∈Pλ
1
n · Pn
i=1 σi · ψx⟨i⟩

≤supX r√2 log #Pλ/n
(7)"
IMPLEMENTATION/METHODS,0.27751196172248804,"where in the second line we replaced the supremum with a maximum, since the set Pλ is finite, and"
IMPLEMENTATION/METHODS,0.2822966507177033,"used the Finite Class Lemma (Massart, 2000) with r = maxψ∈Pλ
qPn
i=1(ψx⟨i⟩)2 ≤λ√n. This
inequality on r is due to the fact that Pλ consists of tensors whose entries are from {−λ, 0, λ}. Thus
W(Cλ) ≤λ
p"
IMPLEMENTATION/METHODS,0.28708133971291866,(2 log 2) · (ρ + 1)/n. The result follows by noting that log 2 · (ρ + 1) ≤2ρ.
IMPLEMENTATION/METHODS,0.291866028708134,"Remark 3.4. Because ψ has π = O(rp) entries, the Rademacher complexity in a typical tensor
norm (e.g., max and Frobenius norms) will be O(
p"
IMPLEMENTATION/METHODS,0.2966507177033493,"π/n) = O(
p"
IMPLEMENTATION/METHODS,0.3014354066985646,"rp/n). However, the Rademacher
complexity in our norm ∥ψ∥+ is O(
p"
IMPLEMENTATION/METHODS,0.3062200956937799,"ρ/n) = O(
p"
IMPLEMENTATION/METHODS,0.31100478468899523,"rp/n), which is exponentially smaller."
IMPLEMENTATION/METHODS,0.3157894736842105,"4
Tensor Completion"
IMPLEMENTATION/METHODS,0.32057416267942584,"Suppose we have data (x⟨i⟩, y⟨i⟩) ∈R × R for i = 1, . . . , n. Let I = {i1, . . . , iu} ⊆[n] be any set
of points that specify all the unique x⟨i⟩for i = 1, . . . , n, meaning the set U = {x⟨i1⟩, . . . , x⟨iu⟩}
does not have any repeated points and x⟨i⟩∈U for all i = 1, . . . , n. The nonnegative tensor
completion problem using our norm ∥ψ∥+ is given by"
IMPLEMENTATION/METHODS,0.3253588516746411,"bψ ∈arg min
ψ
1
n
Pn
i=1

y⟨i⟩−ψx⟨i⟩
2"
IMPLEMENTATION/METHODS,0.33014354066985646,"s.t. ∥ψ∥+ ≤λ
(8)"
IMPLEMENTATION/METHODS,0.3349282296650718,"We will study the statistical properties of the above estimate and describe the elements of algorithm
to solve the above optimization problem. The purpose of defining the set U is that it is used in
constructing the algorithm used to solve the above problem."
IMPLEMENTATION/METHODS,0.3397129186602871,"4.1
Statistical Guarantees"
IMPLEMENTATION/METHODS,0.3444976076555024,"Though in the previous section we calculated a Rademacher complexity for nonnegative tensors in
Cλ viewed as functions, here we use an alternative approach to derive generalization bounds. The
reason is that generalization bounds using the Rademacher complexity are not tight here."
IMPLEMENTATION/METHODS,0.3492822966507177,"Our approach is based on the observation that the nonnegative tensor completion problem (8)
using our norm ∥ψ∥+ is equivalent to a convex aggregation problem (Nemirovski, 2000; Tsybakov,
2003; Lecué et al., 2013) for a finite set of functions. In particular, by Proposition 2.1 we have
{ψ : ∥ψ∥+ ≤λ} = Cλ = conv(Sλ). The implication is we can directly apply existing results for
convex aggregation to provide a tight generalization bound for the solution of (8).
Proposition 4.1 ((Lecué et al., 2013)). Suppose |y| ≤b almost surely. Given any δ > 0, with
probability at least 1 −4δ we have that"
IMPLEMENTATION/METHODS,0.35406698564593303,"E

(y −bψx)2
≤min
φ∈Cλ E

(y −φx)2
+ c0 · max

b2, λ2
· max

ζn, log(1/δ)"
IMPLEMENTATION/METHODS,0.3588516746411483,"n

,
(9)"
IMPLEMENTATION/METHODS,0.36363636363636365,"where c0 is an absolute constant and ζn = 
  2ρ"
IMPLEMENTATION/METHODS,0.3684210526315789,"n ,
if 2ρ ≤√n
r"
IMPLEMENTATION/METHODS,0.37320574162679426,"1
n log

e2ρ
√n

,
if 2ρ > √n
(10)"
IMPLEMENTATION/METHODS,0.37799043062200954,"Remark 4.2. We make two comments. First, note that ζn = O(
p"
IMPLEMENTATION/METHODS,0.3827751196172249,"ρ/n). Second, in some regimes ζn
can be considerably faster than the
p"
IMPLEMENTATION/METHODS,0.3875598086124402,ρ/n rate.
IMPLEMENTATION/METHODS,0.3923444976076555,"Generalization bounds under specific noise models, such as an additive noise model, follow as a
corollary to the above proposition combined with Proposition 2.4."
IMPLEMENTATION/METHODS,0.39712918660287083,"Corollary 4.3. Suppose φ is a nonnegative tensor with rank+(φ) = k and ∥φ∥max ≤µ. If
(x⟨i⟩, y⟨i⟩) are independent and identically distributed with |y⟨i⟩−φx⟨i⟩| ≤e almost surely and
Ey⟨i⟩= φx⟨i⟩. Then given any δ > 0, with probability at least 1 −4δ we have"
IMPLEMENTATION/METHODS,0.4019138755980861,"E

(y −bψx)2
≤e2 + c0 ·

µk + e)2 · max

ζn, log(1/δ)"
IMPLEMENTATION/METHODS,0.40669856459330145,"n

,
(11)"
IMPLEMENTATION/METHODS,0.41148325358851673,"where ζn is as in (10) and c0 is an absolute constant.
Remark 4.4. The above result achieves the information-theoretic rate when the rank k = O(1)."
IMPLEMENTATION/METHODS,0.41626794258373206,"4.2
Computational Complexity"
IMPLEMENTATION/METHODS,0.42105263157894735,"Though (8) is a convex optimization problem, our next result shows that solving it is NP-hard.
Proposition 4.5. It is NP-hard to solve the optimization problem (8) to an arbitrary accuracy."
IMPLEMENTATION/METHODS,0.4258373205741627,"Proof. Define the ball of radius δ > 0 centered at a nonnegative tensor ψ to be B(ψ, δ) = {φ : ∥φ −
ψ∥F ≤δ}. Next define W(C1, δ) = S"
IMPLEMENTATION/METHODS,0.430622009569378,"ψ∈C1 B(ψ, δ) and W(C1, −δ) = {ψ ∈C1 : B(ψ, δ) ⊆C1}.
The weak membership problem for C1 is that given a nonnegative tensor ψ and a δ > 0 decide
whether ψ ∈W(C1, δ) or ψ /∈W(C1, −δ). Theorem 10 of (Friedland and Lim, 2016) shows that
approximation of the norm ∥· ∥+ is polynomial-time reducible to the weak membership problem for
C1. Since Proposition 3.1 shows that approximation of the norm ∥· ∥+ is NP-hard, the result follows
if we can reduce the weak membership problem to (8)."
IMPLEMENTATION/METHODS,0.4354066985645933,"Suppose we are given inputs ψ and δ for the weak membership problem. Choose x⟨i⟩for i = 1, . . . , π
such that each element in R is enumerated exactly once. Next choose y⟨i⟩= ψx⟨i⟩. Finally, note if
we solve (8) and the minimum objective value is less than or equal to δ, then we have ψ ∈W(C1, δ);
otherwise we have ψ /∈W(C1, −δ). The result now follows since this was the desired reduction."
IMPLEMENTATION/METHODS,0.44019138755980863,"We note that the decision version of (8), that is ascertaining whether a given tensor ψ attains an
objective less than a given value while also satisfying ∥ψ∥+ ≤λ, is NP-complete. This follows
directly from Corollary 3.2 and Proposition 4.5."
IMPLEMENTATION/METHODS,0.4449760765550239,"4.3
Numerical Computation"
IMPLEMENTATION/METHODS,0.44976076555023925,"Although it is NP-hard, there is substantial structure that enables practical numerical computation
of global minima of (8). The key observation is that C1 is a 0-1 polytope, which implies the
linear separation problem on this polytope can be solved using integer linear optimization. Integer
optimization has well-established global algorithms that are guaranteed to solve the separation
problem for this polytope. This is a critical feature that enables the use of the Frank-Wolfe algorithm
or one of its variants to solve (8) to a desired numerical tolerance. In fact, the Frank-Wolfe variant
known as Blended Conditional Gradients (BCG) (Braun et al., 2019) is particularly well-suited for
calculating a solution to (8) for the following reasons:"
IMPLEMENTATION/METHODS,0.45454545454545453,"The first reason is that our problem has structure such that the BCG algorithm will terminate in a
linear (with respect to numerical tolerance) number of oracle steps. A sufficient condition for this
linear convergence is if the feasible set is a polytope and the objective function is strictly convex
over the feasible set. For our problem (8), the objective function can be made strictly convex over
the feasible set by an equivalent reformulation. Specifically, we use the equivalent reformulation in
which we change the feasible set from {ψ : ∥ψ∥+ ≤λ} = Cλ to ProjU(Cλ) where the projection is
done over the unique indices specified by the set U. In fact, this projection is trivial to implement
because it simply consists of discarding the entries of ψ that are not observed by the x⟨i⟩indices."
IMPLEMENTATION/METHODS,0.45933014354066987,"The second is that the BCG algorithm uses weak linear separation, which accommodates early-
termination of the associated integer linear optimization. Integer optimization software tends to find
optimal or near-optimal solutions considerably faster than certifying the optimality of such solutions.
Furthermore, a variety of tuning parameters (Berthold et al., 2018) can be used to accelerate the search
for good primal solutions when exact solutions are not needed. We also deploy a fast alternating
maximization heuristic in order to avoid integer optimization oracle calls where possible. Thus,
early-termination allows us to deploy a globally convergent algorithm with practical solution times."
IMPLEMENTATION/METHODS,0.46411483253588515,"Hence we use the BCG algorithm to compute global minima of (8) to arbitrary numerical tolerance.
For brevity we omit a full description of the algorithm, and instead focus on the separation oracle,"
IMPLEMENTATION/METHODS,0.4688995215311005,Algorithm 1: Weak Separation Oracle for Cλ
IMPLEMENTATION/METHODS,0.47368421052631576,"Input: linear objective c ∈Rr1×···×rp, point ψ ∈Cλ, accuracy K ≥1, gap estimate Φ > 0,
norm bound λ
Output: Either (1) vertex φ ∈Sλ with ⟨c, ψ −φ⟩≥Φ/K, or (2) false: ⟨c, ψ −φ⟩≤Φ for
all φ ∈Cλ"
IMPLEMENTATION/METHODS,0.4784688995215311,Algorithm 2: Alternating Maximization
IMPLEMENTATION/METHODS,0.48325358851674644,"Input: linear objective c ∈Rr1×···×rp, point ψ ∈Cλ, norm bound λ, incumbent (binary)
solution ˆθ ∈Sλ
Output: Best known solution θ"
IMPLEMENTATION/METHODS,0.4880382775119617,"θ ←ˆθ
z ←zM(θ)
for i = 1 to p do"
IMPLEMENTATION/METHODS,0.49282296650717705,for k = 1 to ri do
IMPLEMENTATION/METHODS,0.49760765550239233,"θ(i)
k
←1 −θ(i)
k
if zM(θ) > z then"
IMPLEMENTATION/METHODS,0.5023923444976076,"z ←zM(θ)
else"
IMPLEMENTATION/METHODS,0.507177033492823,"θ(i)
k
←1 −θ(i)
k
end if
end for
end for"
IMPLEMENTATION/METHODS,0.5119617224880383,"which is the main component specifically adapted to our application. The oracle is described in
Algorithm 1, where ⟨·, ·⟩is the dot product for tensors viewed as vectors; for notational simplicity
we state the oracle in terms of the original space, ignoring projection onto U. Output condition (1)
provides separation with some vertex φ, while (2) requires certification that no such vertex exists."
IMPLEMENTATION/METHODS,0.5167464114832536,"In implementing the weak separation oracle, we first attempt separation with our alternating maxi-
mization procedure. Described as Algorithm 2, it involves the following objective function:"
IMPLEMENTATION/METHODS,0.5215311004784688,zM(θ) := P
IMPLEMENTATION/METHODS,0.5263157894736842,"x∈R⟨cx, ψx −λ · Qp
k=1 θxk⟩
(12)"
IMPLEMENTATION/METHODS,0.5311004784688995,"The separation problem is thus treated as a multilinear binary optimization problem, and the algorithm
successively minimizes in each dimension. We apply this heuristic a fixed number of times, randomly
complementing entries in the incumbent each time. The procedure runs in polynomial-time, and
is not guaranteed to separate nor can it certify that such separation is impossible. However, in our
experiments it succeeds often, offering substantial practical speedups."
IMPLEMENTATION/METHODS,0.5358851674641149,"If alternating maximization fails to separate, then we solve the integer programming problem:"
IMPLEMENTATION/METHODS,0.5406698564593302,"max
φ,θ ⟨c, ψ −φ⟩"
IMPLEMENTATION/METHODS,0.5454545454545454,"s.t. λ · (1 −p) + λ · Pp
k=1 θxk ≤φx
x ∈R
0 ≤φx ≤λ · θxk
k ∈[p], x ∈R
θxk ∈{0, 1}
k ∈[p], x ∈R (13)"
IMPLEMENTATION/METHODS,0.5502392344497608,"Note that early termination is deployed: we stop whenever an incumbent solution is found such that
the objective is greater than Φ/K. If no such solution exists, then the integer programming solver is
guaranteed to (eventually) establish a dual bound zU such that ⟨c, ψ −φ⟩≤zU ≤Φ."
RESULTS/EXPERIMENTS,0.5550239234449761,"5
Numerical Experiments"
RESULTS/EXPERIMENTS,0.5598086124401914,"Here we present results that show the efficacy and scalability of our algorithm for nonnegative tensor
completion. Our experiments were conducted on a laptop computer with 8GB of RAM and an Intel
Core i5 2.3Ghz processor with 2-cores/4-threads. The algorithms were coded in Python 3. We used"
RESULTS/EXPERIMENTS,0.5645933014354066,"20
40
60
80
100 r 0.0 0.2 0.4 0.6 0.8 1.0 NMSE"
RESULTS/EXPERIMENTS,0.569377990430622,"BCG
ALS
SiLRTC
TNCP"
RESULTS/EXPERIMENTS,0.5741626794258373,"20
40
60
80
100 r 0 5 10 15 20 25 30"
RESULTS/EXPERIMENTS,0.5789473684210527,Time (s)
RESULTS/EXPERIMENTS,0.583732057416268,"BCG
ALS
SiLRTC
TNCP"
RESULTS/EXPERIMENTS,0.5885167464114832,Figure 1: Results for order-3 nonnegative tensors with size r × r × r and n = 500 samples.
RESULTS/EXPERIMENTS,0.5933014354066986,"4
5
6
7
8 p 0.0 0.2 0.4 0.6 0.8 1.0 NMSE"
RESULTS/EXPERIMENTS,0.5980861244019139,"BCG
ALS
SiLRTC
TNCP"
RESULTS/EXPERIMENTS,0.6028708133971292,"4
5
6
7
8 p 10
2 10
1 100 101 102 103"
RESULTS/EXPERIMENTS,0.6076555023923444,Time (s)
RESULTS/EXPERIMENTS,0.6124401913875598,"BCG
ALS
SiLRTC
TNCP"
RESULTS/EXPERIMENTS,0.6172248803827751,"Figure 2: Results for increasing order nonnegative tensors with size 10×p and n = 10, 000 samples."
RESULTS/EXPERIMENTS,0.6220095693779905,"10
2
10
1
100
101"
RESULTS/EXPERIMENTS,0.6267942583732058,Sample Percent 0.0 0.2 0.4 0.6 0.8 1.0 NMSE
RESULTS/EXPERIMENTS,0.631578947368421,"BCG
ALS
SiLRTC
TNCP"
RESULTS/EXPERIMENTS,0.6363636363636364,"10
2
10
1
100
101"
RESULTS/EXPERIMENTS,0.6411483253588517,"Sample Percent 10
1 100 101 102"
RESULTS/EXPERIMENTS,0.645933014354067,Time (s)
RESULTS/EXPERIMENTS,0.6507177033492823,"BCG
ALS
SiLRTC
TNCP"
RESULTS/EXPERIMENTS,0.6555023923444976,Figure 3: Results for nonnegative tensors with size 10×6 and increasing n samples.
RESULTS/EXPERIMENTS,0.6602870813397129,"10
2
10
1
100
101"
RESULTS/EXPERIMENTS,0.6650717703349283,Sample Percent 0.0 0.2 0.4 0.6 0.8 1.0 NMSE
RESULTS/EXPERIMENTS,0.6698564593301436,"BCG
ALS
SiLRTC
TNCP"
RESULTS/EXPERIMENTS,0.6746411483253588,"10
2
10
1
100
101"
RESULTS/EXPERIMENTS,0.6794258373205742,Sample Percent 100 101 102 103
RESULTS/EXPERIMENTS,0.6842105263157895,Time (s)
RESULTS/EXPERIMENTS,0.6889952153110048,"BCG
ALS
SiLRTC
TNCP"
RESULTS/EXPERIMENTS,0.69377990430622,Figure 4: Results for nonnegative tensors with size 10×7 and increasing n samples.
RESULTS/EXPERIMENTS,0.6985645933014354,"Gurobi v9.1 (Gurobi Optimization, LLC, 2021) to solve the integer programs (13). As a benchmark,
we use alternating least squares (ALS) – which is often called a “workhorse” for numerical tensor
problems (Kolda and Bader, 2009) – as well as two state-of-the-art methods implemented by the
PyTen package (Song et al., 2019), namely the simple low rank tensor completion (SiLRTC) algorithm
(Liu et al., 2012) and the trace norm regularized CP decomposition (TNCP) algorithm (Liu et al.,
2014). PyTen is available from https://github.com/datamllab/pyten under a GPL 2 license."
RESULTS/EXPERIMENTS,0.7033492822966507,"To minimize the impact of hyperparameter selection in our numerical results, we provided the ground
truth values when possible. For instance, in our nonnegative tensor completion formulation (8) we
chose λ to be the smallest value for which we could certify that ∥ψ∥+ ≤λ for the true tensor ψ. This
was accomplished by construction of the true tensor ψ. For ALS and TNCP, we used a nonnegative
rank k that was the smallest value for which we could certify that rank+(ψ) ≤k. This was also
accomplished by construction of the true tensor ψ. A last note is that ALS often works better when
used with L2 regularization (Navasca et al., 2008). The hyperparameter for the L2 regularization for
ALS was chosen in a way favorable to ALS in order to maximize its accuracy."
RESULTS/EXPERIMENTS,0.7081339712918661,"5.1
Experiments with Third-Order Tensors"
RESULTS/EXPERIMENTS,0.7129186602870813,"Our first set of results concerns tensors of order p = 3 with increasing dimensions. In each experiment,
the true tensor ψ was constructed by randomly choosing 10 points from S1 and then taking a random
convex combination. This construction ensures ∥ψ∥+ ≤1 and rank+(ψ) ≤10. We used n = 500
samples (with indices sampled with replacement). Each experiment was repeated 100 times, and the
results are shown in Figure 1. A table of these values and their standard error is found in the Appendix.
We measured accuracy using normalized mean squared error (NMSE) ∥bψ −ψ∥2
F /∥ψ∥2
F , which is a
more stringent measure than used in Corollary 4.3 because the statistical theoretical result does not
include normalization. The results show that our approach provides modestly higher accuracy but
requires more computation time. However, computation time remains on the order of seconds for the
various tensor sizes. We note that the NMSE value of approximately 0.48 that TNCP converges to is
the value achieved by a tensor that is identically the average of y⟨i⟩in all its entries."
RESULTS/EXPERIMENTS,0.7177033492822966,"5.2
Experiments with Increasing Tensor Order"
RESULTS/EXPERIMENTS,0.722488038277512,"Our second set of results concerns tensors with increasing order p, where each dimension takes the
value ri = 10 for i = 1, . . . , p. In each experiment, the true tensor was constructed by the method in
Sec. 5.1. We used n = 10, 000 samples (with indices sampled with replacement). Each experiment
was repeated 100 times, and the results are shown in Figure 2 with the full values and standard error
given in the Appendix. SiLRTC and TNCP were unable to run for tensors with 108 entries. The
results show that our approach provides much higher accuracy but requires more computation time.
However, computation remains on the order of minutes even for the largest tensor with 108 entries."
RESULTS/EXPERIMENTS,0.7272727272727273,"5.3
Experiments with Increasing Sample Size"
RESULTS/EXPERIMENTS,0.7320574162679426,"Our third and fourth set of results concerns tensors of size 10×6 and 10×7, respectively, where
experiments are conducted with increasing sample size, given in the table as percentage of total
entries. The true tensor ψ was constructed as in Sec. 5.1. We begin with sample percentage 0.01%
(with indices sampled with replacement), and extend each set of experiments’ sample size by one
order of magnitude. Each experiment was repeated 100 times, and the results are shown in Figures 3
and 4 with the full values and standard error given in the Appendix. Again, the numerical results
show that our approach provides substantially higher accuracy but requires more computation time.
The computation time remains on the order of minutes for most of the sampling schemes, excepting
the draw of 106 entries for a tensor with 107 entries (i.e., about 1.5 hours)."
CONCLUSION/DISCUSSION,0.7368421052631579,"6
Conclusion"
CONCLUSION/DISCUSSION,0.7416267942583732,"We defined a new norm for nonnegative tensors and used it to develop an algorithm for nonneg-
ative tensor completion that provably converges in a linear number of oracle steps and meets the
information-theoretic sample complexity rate. Its efficacy and scalability were demonstrated using
experiments. The next step is to generalize this approach to all tensors. In fact, our norm definitions,
optimization formulations, algorithm design, and theoretical analysis all extend to general tensors."
CONCLUSION/DISCUSSION,0.7464114832535885,Acknowledgements
CONCLUSION/DISCUSSION,0.7511961722488039,This material is based upon work partially supported by the NSF under grant CMMI-1847666.
REFERENCES,0.7559808612440191,References
REFERENCES,0.7607655502392344,"Arora, S., Lund, C., Motwani, R., Sudan, M., and Szegedy, M. (1998). Proof verification and the
hardness of approximation problems. Journal of the ACM (JACM), 45(3):501–555."
REFERENCES,0.7655502392344498,"Aswani, A. (2016). Low-rank approximation and completion of positive tensors. SIAM Journal on
Matrix Analysis and Applications, 37(3):1337–1364."
REFERENCES,0.7703349282296651,"Barak, B. and Moitra, A. (2016). Noisy tensor completion via the sum-of-squares hierarchy. In
Conference on Learning Theory, pages 417–445. PMLR."
REFERENCES,0.7751196172248804,"Bartlett, P. and Mendelson, S. (2002). Rademacher and gaussian complexities: Risk bounds and
structural results. J. Mach. Learn. Res."
REFERENCES,0.7799043062200957,"Berthold, T., Hendel, G., and Koch, T. (2018). From feasibility to improvement to proof: three phases
of solving mixed-integer programs. Optimization Methods and Software, 33(3):499–517."
REFERENCES,0.784688995215311,"Braun, G., Pokutta, S., Tu, D., and Wright, S. (2019). Blended conditonal gradients. In International
Conference on Machine Learning, pages 735–743. PMLR."
REFERENCES,0.7894736842105263,"Chandrasekaran, V., Recht, B., Parrilo, P. A., and Willsky, A. S. (2012). The convex geometry of
linear inverse problems. Foundations of Computational mathematics, 12(6):805–849."
REFERENCES,0.7942583732057417,"Dauwels, J., Garg, L., Earnest, A., and Pang, L. K. (2011). Handling missing data in medical
questionnaires using tensor decompositions. In 2011 8th International Conference on Information,
Communications Signal Processing, pages 1–5."
REFERENCES,0.7990430622009569,"Drenick, R. (1992). Multilinear programming: Duality theories. Journal of optimization theory and
applications, 72(3):459–486."
REFERENCES,0.8038277511961722,"Friedland, S. and Lim, L.-H. (2014). Computational complexity of tensor nuclear norm. Submitted."
REFERENCES,0.8086124401913876,"Friedland, S. and Lim, L.-H. (2016). The computational complexity of duality. SIAM Journal on
Optimization, 26(4):2378–2393."
REFERENCES,0.8133971291866029,"Gandy, S., Recht, B., and Yamada, I. (2011). Tensor completion and low-n-rank tensor recovery via
convex optimization. Inverse problems, 27(2):025010."
REFERENCES,0.8181818181818182,"Gurobi Optimization, LLC (2021). Gurobi Optimizer Reference Manual."
REFERENCES,0.8229665071770335,"Hansen, P. (1979). Methods of nonlinear 0-1 programming. In Annals of Discrete Mathematics,
volume 5, pages 53–70. Elsevier."
REFERENCES,0.8277511961722488,"Hillar, C. and Lim, L.-H. (2013). Most tensor problems are np-hard. J. ACM, 60(6):45:1–45:39."
REFERENCES,0.8325358851674641,"Kolda, T. and Bader, B. (2009). Tensor decompositions and applications. SIAM Review, 51(3):455–
500."
REFERENCES,0.8373205741626795,"Lecué, G. et al. (2013). Empirical risk minimization is optimal for the convex aggregation problem.
Bernoulli, 19(5B):2153–2166."
REFERENCES,0.8421052631578947,"Ledoux, M. and Talagrand, M. (1991). Probability in Banach Spaces: Isoperimetry and Processes.
Springer."
REFERENCES,0.84688995215311,"Li, X., Gunturk, B., and Zhang, L. (2008). Image demosaicing: A systematic survey. In Visual
Communications and Image Processing 2008, volume 6822, page 68221J. International Society
for Optics and Photonics."
REFERENCES,0.8516746411483254,"Liu, J., Musialski, P., Wonka, P., and Ye, J. (2012). Tensor completion for estimating missing values
in visual data. IEEE transactions on pattern analysis and machine intelligence, 35(1):208–220."
REFERENCES,0.8564593301435407,"Liu, Y., Shang, F., Jiao, L., Cheng, J., and Cheng, H. (2014). Trace norm regularized cande-
comp/parafac decomposition with missing data. IEEE transactions on cybernetics, 45(11):2437–
2448."
REFERENCES,0.861244019138756,"Massart, P. (2000). Some applications of concentration inequalities to statistics. Annales de la faculté
des sciences de Toulouse Sér. 6, 9(2):245–303."
REFERENCES,0.8660287081339713,"Montanari, A. and Sun, N. (2018). Spectral algorithms for tensor completion. Communications on
Pure and Applied Mathematics, 71(11):2381–2425."
REFERENCES,0.8708133971291866,"Mu, C., Huang, B., Wright, J., and Goldfarb, D. (2014). Square deal: Lower bounds and improved
relaxations for tensor recovery. In International conference on machine learning, pages 73–81.
PMLR."
REFERENCES,0.8755980861244019,"Navasca, C., De Lathauwer, L., and Kindermann, S. (2008). Swamp reducing technique for tensor
decomposition. In 2008 16th European Signal Processing Conference, pages 1–5. IEEE."
REFERENCES,0.8803827751196173,"Nemirovski, A. (2000). Topics in non-parametric statistics. Ecole d’Eté de Probabilités de Saint-
Flour, 28:85."
REFERENCES,0.8851674641148325,"Padberg, M. (1989). The boolean quadric polytope: some characteristics, facets and relatives.
Mathematical programming, 45(1-3):139–172."
REFERENCES,0.8899521531100478,"Papadimitriou, C. H. and Yannakakis, M. (1991). Optimization, approximation, and complexity
classes. Journal of computer and system sciences, 43(3):425–440."
REFERENCES,0.8947368421052632,"Qi, Y., Comon, P., and Lim, L.-H. (2014). Uniqueness of nonnegative tensor approximations. arXiv
preprint arXiv:1410.8129."
REFERENCES,0.8995215311004785,"Qi, Y., Comon, P., and Lim, L.-H. (2016). Uniqueness of nonnegative tensor approximations. IEEE
Transactions on Information Theory, 62(4):2170–2183."
REFERENCES,0.9043062200956937,"Rao, N., Shah, P., and Wright, S. (2015). Forward–backward greedy algorithms for atomic norm
regularization. IEEE Transactions on Signal Processing, 63(21):5798–5811."
REFERENCES,0.9090909090909091,"Rauhut, H. and Stojanac, Ž. (2021). Tensor theta norms and low rank recovery. Numerical Algorithms,
88(1):25–66."
REFERENCES,0.9138755980861244,"Rockafellar, R. and Wets, R. (2009). Variational Analysis. Springer."
REFERENCES,0.9186602870813397,"Song, Q., Ge, H., Caverlee, J., and Hu, X. (2019). Tensor completion algorithms in big data analytics.
ACM Transactions on Knowledge Discovery from Data (TKDD), 13(1):1–48."
REFERENCES,0.9234449760765551,"Srebro, N. and Shraibman, A. (2005). Rank, trace-norm and max-norm. In International Conference
on Computational Learning Theory, pages 545–560. Springer."
REFERENCES,0.9282296650717703,"Srebro, N., Sridharan, K., and Tewari, A. (2010). Smoothness, low noise and fast rates. In Advances
in Neural Information Processing Systems, pages 2199–2207."
REFERENCES,0.9330143540669856,"Tsybakov, A. B. (2003). Optimal rates of aggregation. In Learning theory and kernel machines,
pages 303–313. Springer."
REFERENCES,0.937799043062201,"Witsenhausen, H. (1986). A simple bilinear optimization problem. Systems & control letters,
8(1):1–4."
REFERENCES,0.9425837320574163,"Yuan, M. and Zhang, C.-H. (2016). On tensor completion via nuclear norm minimization. Foundations
of Computational Mathematics, 16(4):1031–1068."
REFERENCES,0.9473684210526315,"Yuan, M. and Zhang, C.-H. (2017). Incoherent tensor norms and their applications in higher order
tensor completion. IEEE Transactions on Information Theory, 63(10):6753–6766."
REFERENCES,0.9521531100478469,"Zhang, X., Wang, D., Zhou, Z., and Ma, Y. (2019). Robust low-rank tensor recovery with rectification
and alignment. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(1):238–255."
OTHER,0.9569377990430622,Checklist
OTHER,0.9617224880382775,1. For all authors...
OTHER,0.9665071770334929,"(a) Do the main claims made in the abstract and introduction accurately reflect the paper’s
contributions and scope? [Yes]
(b) Did you describe the limitations of your work? [Yes] See Sec. 5."
OTHER,0.9712918660287081,"(c) Did you discuss any potential negative societal impacts of your work? [N/A] This is
foundational research and not tied to particular applications.
(d) Have you read the ethics review guidelines and ensured that your paper conforms to
them? [Yes]
2. If you are including theoretical results..."
OTHER,0.9760765550239234,"(a) Did you state the full set of assumptions of all theoretical results? [Yes]
(b) Did you include complete proofs of all theoretical results? [Yes]
3. If you ran experiments..."
OTHER,0.9808612440191388,"(a) Did you include the code, data, and instructions needed to reproduce the main exper-
imental results (either in the supplemental material or as a URL)? [Yes] See Supple-
mental Material.
(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
were chosen)? [Yes] See Sec. 5.
(c) Did you report error bars (e.g., with respect to the random seed after running experi-
ments multiple times)? [Yes] See Appendix.
(d) Did you include the total amount of compute and the type of resources used (e.g., type
of GPUs, internal cluster, or cloud provider)? [Yes] See Sec. 5.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets..."
OTHER,0.9856459330143541,"(a) If your work uses existing assets, did you cite the creators? [Yes] See Sec. 5 for a
citation to the PyTen package.
(b) Did you mention the license of the assets? [Yes] See Sec. 5 for the license of the PyTen
package.
(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]"
OTHER,0.9904306220095693,"There is new code in the Supplemental Material.
(d) Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? [N/A]
(e) Did you discuss whether the data you are using/curating contains personally identifiable
information or offensive content? [N/A]
5. If you used crowdsourcing or conducted research with human subjects..."
OTHER,0.9952153110047847,"(a) Did you include the full text of instructions given to participants and screenshots, if
applicable? [N/A]
(b) Did you describe any potential participant risks, with links to Institutional Review
Board (IRB) approvals, if applicable? [N/A]
(c) Did you include the estimated hourly wage paid to participants and the total amount
spent on participant compensation? [N/A]"
