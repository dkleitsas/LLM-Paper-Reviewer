Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0021929824561403508,"This paper proposes a novel supervised contrastive learning framework, called
Self-Contrastive (SelfCon) Learning, that self-contrasts within multiple outputs
from the different levels of a multi-exit network. SelfCon learning does not require
additional augmented samples, which resolves the concerns of multi-viewed batch
(e.g., high computational cost and generalization error). Furthermore, we prove
that SelfCon loss guarantees the lower bound of label-conditional mutual informa-
tion between the intermediate and the last feature. In our experiments including
ImageNet-100, SelfCon surpasses cross-entropy and Supervised Contrastive (Sup-
Con) learning without the need for a multi-viewed batch. We demonstrate that the
success of SelfCon learning is related to the regularization effect associated with
the single-view and sub-network."
INTRODUCTION,0.0043859649122807015,"1
INTRODUCTION"
INTRODUCTION,0.006578947368421052,"Recent studies have studied the success of deep neural networks by investigating how neural networks
can encode representations with rich information (Tishby & Zaslavsky, 2015; Shwartz-Ziv & Tishby,
2017; Hjelm et al., 2018; Saxe et al., 2019a). Among the various approaches suggested, contrastive
loss functions, designed to maximize the lower bound of mutual information (MI) between the
target and context, have achieved considerable success in self-supervised representation learning Ô¨Årst
(Gutmann & Hyv√§rinen, 2010; Oord et al., 2018) and supervised learning recently (Khosla et al.,
2020; Gunel et al., 2020; Wang et al., 2021). The main objective of the contrastive loss functions in
supervision is to make representations from the same class closer and representations from different
classes farther. To this end, they deÔ¨Åne positive samples, i.e., augmented samples from the same
image or (augmented) images sharing the same class label, and negative samples, i.e., all other
samples, for every data batch."
INTRODUCTION,0.008771929824561403,"Contrasting two random augmented samples, often referred to as a multi-viewed batch, has shown
impressive results in representation learning (Chen et al., 2020; He et al., 2020; Grill et al., 2020;
Caron et al., 2020; Chen & He, 2020; Khosla et al., 2020), yet a multi-viewed batch causes the
following issues. (1) A multi-viewed batch doubles the batch size, which is a huge burden on
memory and computation. (2) Oracle needs to carefully choose the augmentation policies (Chen
et al., 2020; Tian et al., 2020; Caron et al., 2020; Kim et al., 2020). (3) A multi-viewed contrastive
task is domain-speciÔ¨Åc, because data-level augmentation requires speciÔ¨Åc domain knowledge such as
image cropping and Ô¨Çipping (Verma et al., 2021). In spite of these concerns, a supervised contrastive
learning framework (Khosla et al., 2020) still relies on multi-views, although they are not necessarily
needed with the help of usable label information."
INTRODUCTION,0.010964912280701754,"In this work, we propose a novel contrastive learning framework in supervision, called Self-
Contrastive (SelfCon) learning, that does not require additional augmented samples. Instead,
SelfCon uses a multi-exit framework (Teerapittayanon et al., 2016; Zhang et al., 2019a;b) where
sub-networks produce multiple outputs for the same input and self-contrasts within multiple out-
puts from the different levels of a single network. For training, SelfCon learning can use the
multi-viewed batch as well. However, the multi-exit framework already generates positive pairs from
a single image, which can replace the augmentation based multi-view. In Figure 1, we compare
SelfCon learning and Supervised Contrastive (SupCon, Khosla et al. (2020)) learning."
INTRODUCTION,0.013157894736842105,"SelfCon learning improves the classiÔ¨Åcation performance of the encoder network, owing to (1)
the increase of the lower bound of label-conditional MI between the intermediate and the last
features, and (2) the regularization effect from the single-view and sub-network. We theoreti-
cally demonstrate that SelfCon loss is the lower bound of label-conditional MI. Furthermore, unlike"
INTRODUCTION,0.015350877192982455,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.017543859649122806,Anchor
INTRODUCTION,0.019736842105263157,Positive
INTRODUCTION,0.021929824561403508,Negative
INTRODUCTION,0.02412280701754386,Backbone (ùë≠)
INTRODUCTION,0.02631578947368421,Sub (ùëÆ)
INTRODUCTION,0.02850877192982456,(a) SupCon
INTRODUCTION,0.03070175438596491,"(b) SelfCon with Multi-View
(c) SelfCon with Single-View"
INTRODUCTION,0.03289473684210526,"Figure 1: Comparison of learning framework in terms of augmentation and architecture. Both
SupCon (Khosla et al., 2020) and SelfCon use all the samples of the same ground-truth label as the
positive pairs. In all three methods, every output can be an anchor feature. SpeciÔ¨Åcally, in (b) and
(c), an anchor from the backbone network contrasts other features from the backbone, as well as the
features from the sub-network. Best seen in color."
INTRODUCTION,0.03508771929824561,"SupCon loss, it encourages the encoder to learn the label information that only intermediate features
can explain. Second, we empirically show that SelfCon learning reduces the generalization error with
the single-view and sub-network. The former prevents the encoder from overÔ¨Åtting to each instance,
and the latter regularizes the intermediate feature to be similar to the last feature."
INTRODUCTION,0.03728070175438596,The contributions of our paper can be summarized as follows:
INTRODUCTION,0.039473684210526314,"[S3] We propose Self-Contrastive learning, a novel supervised contrastive framework between the
multiple features from the different levels of a single network."
INTRODUCTION,0.041666666666666664,"[S4] SelfCon loss guarantees the lower bound of label-conditional MI between the intermediate and
the last features, and leads to increasing interaction information between the features and the label."
INTRODUCTION,0.043859649122807015,"[S5.1, S5.2] SelfCon learning achieved higher classiÔ¨Åcation accuracy for various benchmarks and
architectures than cross-entropy and SupCon loss. Our empirical study of MI estimation provides
evidence for superior performance."
INTRODUCTION,0.046052631578947366,"[S5.3, S5.4] We investigate the advantages of single-viewed batch in terms of the generalization error
and computation resources. We also identify that SelfCon learning beneÔ¨Åts from the sub-network
owing to the regularization effect, vanishing gradient, and ensemble prediction."
RELATED WORKS,0.04824561403508772,"2
RELATED WORKS"
CONTRASTIVE LEARNING,0.05043859649122807,"2.1
CONTRASTIVE LEARNING"
CONTRASTIVE LEARNING,0.05263157894736842,"As the cost of data labeling increases exponentially, there is an increased need for acquiring repre-
sentation under unsupervised scenarios. After Oord et al. (2018) proposed an InfoNCE loss (also
called a contrastive loss), contrastive learning-based algorithms showed a remarkable improvement
in performance (Chen et al., 2020; He et al., 2020; Grill et al., 2020; Caron et al., 2020; Chen &
He, 2020). Most previous works use the structure of a Siamese network, which is a weight-sharing
network applied on two or more inputs, with negative pairs (SimCLR (Chen et al., 2020), MoCo
(He et al., 2020)), momentum encoders (MoCo (He et al., 2020), BYOL (Grill et al., 2020)), online
clustering (SwAV (Caron et al., 2020)), or a stop-gradient operation (SimSiam (Chen & He, 2020)).
While the softmax form is frequently used for the contrastive loss (Chen et al., 2020; He et al.,
2020), recent state-of-the-art algorithms utilize an MSE loss (Grill et al., 2020; Chen & He, 2020),
or a cross-entropy loss (Caron et al., 2020; 2021). Khosla et al. (2020), inspired by the success
in self-supervised learning, propose a label-based contrastive loss in supervision, named SupCon
loss. Supervised contrastive learning has also been extended to semantic segmentation (Wang et al.,"
CONTRASTIVE LEARNING,0.05482456140350877,Under review as a conference paper at ICLR 2022
CONTRASTIVE LEARNING,0.05701754385964912,"2021) and language tasks (Gunel et al., 2020). While the SupCon loss utilizes the output features
from two randomly augmented images, we also contrast the features from different network paths
by introducing the multi-exit framework (Teerapittayanon et al., 2016; Zhang et al., 2019a;b). We
further propose a novel loss function that we can apply on the single-viewed batch."
MUTUAL INFORMATION,0.05921052631578947,"2.2
MUTUAL INFORMATION"
MUTUAL INFORMATION,0.06140350877192982,"Mutual information is a measure to quantify the information held in a random variable about the other
variable, and it has been used as a powerful tool to open the black box of deep neural networks (Tishby
& Zaslavsky, 2015; Shwartz-Ziv & Tishby, 2017; Saxe et al., 2019a). As it is difÔ¨Åcult to compute the
MI exactly (Paninski, 2003), several works have proposed variational MI estimators based on neural
networks, e.g., InfoNCE (Oord et al., 2018), MINE (Belghazi et al., 2018), NWJ (Nguyen et al., 2010),
ML-CPC (Song & Ermon, 2020), and SMILE (Song & Ermon, 2019). Recently, MI estimator-based
objectives have been proposed to improve the performance of contrastive learning (Hjelm et al., 2018;
Bachman et al., 2019; Song & Ermon, 2020; Wu et al., 2020b) and knowledge distillation (Tian et al.,
2019b; Ahn et al., 2019). Among previous approaches, we are highly motivated by those that aim to
increase the MI between the intermediate and the last features. DIM (Hjelm et al., 2018) and AMDIM
(Bachman et al., 2019) propose novel contrastive losses between the global features and local features
(i.e., all pixels or patches of the intermediate features), while we contrast the reÔ¨Åned local features
via sub-networks. VID (Ahn et al., 2019) makes the student learn the distribution of the activations
in the auxiliary teacher‚Äôs intermediate features. Our method differs from VID in that we self-distill
within the network itself. Zhang et al. (2019a;b) share a similar idea with the aforementioned works,
but they implicitly increase MI using Kullback‚ÄìLeibler (KL) divergence loss."
SELF-CONTRASTIVE LEARNING,0.06359649122807018,"3
SELF-CONTRASTIVE LEARNING"
SELF-CONTRASTIVE LEARNING,0.06578947368421052,"We propose a new supervised contrastive loss that maximizes the similarity of the outputs
from different network paths by introducing the multi-exit framework. We deÔ¨Åne the encoder
structure with F as a backbone network and G as a sub-network that shares the backbone‚Äôs parameters
up to some intermediate layer. T denotes those sharing layers that produce the intermediate feature.
The sub-network has additional non-sharing parameters attached after T . Note that F and G include
the projection head on the encoder. We highlight the positive and negative pairs with respect to an
anchor sample, following Figure 1."
SELF-CONTRASTIVE LEARNING,0.06798245614035088,"SupCon loss
To mitigate the weaknesses of cross-entropy, such as the reduced generalization
performance and the possibility of poor margins, Khosla et al. (2020) proposed a supervised version
of the contrastive loss that deÔ¨Ånes the positive pairs as every sample with the same ground-truth label.
We reformulate the SupCon loss function as follows:"
SELF-CONTRASTIVE LEARNING,0.07017543859649122,"Lsup = ‚àí
X i,p1"
SELF-CONTRASTIVE LEARNING,0.07236842105263158,"log
exp(F (xi) ‚Ä¢ F (xp1)/œÑ)
P p2"
SELF-CONTRASTIVE LEARNING,0.07456140350877193,exp(F (xi) ‚Ä¢ F (xp2)/œÑ) + P n
SELF-CONTRASTIVE LEARNING,0.07675438596491228,"exp(F (xi) ‚Ä¢ F (xn)/œÑ)
(1)"
SELF-CONTRASTIVE LEARNING,0.07894736842105263,"i ‚ààI ‚â°{1, . . . , 2B}
p‚àó‚ààPi‚àó‚â°{p ‚ààI \ {i}|yp = yi}
n ‚ààNi ‚â°{n ‚ààI|yn Ã∏= yi}"
SELF-CONTRASTIVE LEARNING,0.08114035087719298,"where ‚Ä¢ denotes the inner product, B is the batch size, and œÑ is the temperature to soften or harden the
softmax value. I denotes a set of indices of the multi-viewed batch that concatenates the original
B images and the augmented ones, i.e., xB+i is an augmented pair of xi. Pi‚àóand Ni are a set of
positive and negative pair indices with respect to an anchor i, respectively. Note that Eq. 1 is the
same as NT-Xent loss (Chen et al., 2020) when Pi‚àó‚â°{(i + B) mod 2B}. We dropped the dividing
term of sums for brevity."
SELF-CONTRASTIVE LEARNING,0.08333333333333333,"SelfCon loss
Besides minimizing the above loss, we aim to maximize the similarity between the
outputs from the backbone and the sub-network. To this end, we deÔ¨Åne SelfCon loss, which forms a
self-contrastive task for every output including the features from the sub-network. In Section 4, we
clarify the connection of this loss with the label-conditional MI between the intermediate and the last
features."
SELF-CONTRASTIVE LEARNING,0.08552631578947369,Under review as a conference paper at ICLR 2022
SELF-CONTRASTIVE LEARNING,0.08771929824561403,"Lself = ‚àí
X œâ,œâ1 X i,p1"
SELF-CONTRASTIVE LEARNING,0.08991228070175439,"log
exp(œâ(xi) ‚Ä¢ œâ1(xp1)/œÑ)
P œâ2  P p2"
SELF-CONTRASTIVE LEARNING,0.09210526315789473,exp(œâ(xi) ‚Ä¢ œâ2(xp2)/œÑ) + P n
SELF-CONTRASTIVE LEARNING,0.09429824561403509,"exp(œâ(xi) ‚Ä¢ œâ2(xn)/œÑ)

(2)"
SELF-CONTRASTIVE LEARNING,0.09649122807017543,"i ‚ààI ‚â°

{1, . . . , B} (SelfCon-S)
{1, . . . , 2B} (SelfCon-M)"
SELF-CONTRASTIVE LEARNING,0.09868421052631579,"p‚àó‚ààPi‚àó‚â°{p ‚ààI|yp = yi}
n ‚ààNi ‚â°{n ‚ààI|yn Ã∏= yi}"
SELF-CONTRASTIVE LEARNING,0.10087719298245613,"œâ, œâ1, œâ2 ‚àà‚Ñ¶= {F , G}, a function set of the backbone network and the sub-network. We exclude
an anchor sample from the positive set to avoid contrasting the same feature, i.e., Pi‚àó‚ÜêPi‚àó\ {i}
when œâ = œâ‚àó. While prevalent contrastive approaches (Khosla et al., 2020; Chen et al., 2020; He
et al., 2020; Grill et al., 2020; Caron et al., 2020) force a multi-viewed batch generated by data
augmentation, in SelfCon learning, the sub-network plays a role as the augmentation and provides an
alternative view on the feature space. Therefore, we formulate our SelfCon loss function with single-
viewed batch (SelfCon-S); I ‚â°{1, . . . , B} without the additional augmented samples. Besides, we
also deÔ¨Åne SelfCon with multi-viewed batch (SelfCon-M) loss, i.e., Lself with I ‚â°{1, . . . , 2B}."
SELF-CONTRASTIVE LEARNING,0.10307017543859649,"In the development of our SelfCon loss formulation, we can further use multiple sub-networks,
i.e., ‚Ñ¶= {F , G1, G2, . . . }. Appendix C provides the classiÔ¨Åcation performance of the expanded
network, which was comparable to that of the single sub-network in our experiments. Thus, we
simply use a single sub-network throughout our paper."
DISCUSSIONS,0.10526315789473684,"4
DISCUSSIONS"
DISCUSSIONS,0.1074561403508772,"Question 1: How does SelfCon loss maximize the lower bound of label-conditional MI between
the intermediate and the last features?"
DISCUSSIONS,0.10964912280701754,"We explain in the following propositions to answer Question 1. All the proofs can be found in
Appendix A.
Proposition 4.1. Let x and z be different samples that share the same class label c. Then, with some
discriminator function modeled by a neural network F and 2(K ‚àí1) negative sample size, SupCon
loss maximizes the lower bound of conditional MI between the output features of a positive pair,"
DISCUSSIONS,0.1118421052631579,"log(2K ‚àí1) ‚àíLsup(x, z; F , K) ‚â§I(F (x); F (z)|c).
(3)"
DISCUSSIONS,0.11403508771929824,"Proposition 4.2. Denote Lself-s as SelfCon loss with single-viewed batch. SelfCon-S loss maximizes
the lower bound of MI between the output features from the backbone and the sub-network,"
DISCUSSIONS,0.1162280701754386,"log(2K ‚àí1) ‚àíLself-s(x; {F , G}, K) ‚â§I(F (x); G(x)|c).
(4)"
DISCUSSIONS,0.11842105263157894,"Proposition 4.3. Let T (x) be the intermediate feature of the backbone network, which is also an
input to the auxiliary network path. Then the r.h.s. of Eq. 4 is upper-bounded by"
DISCUSSIONS,0.1206140350877193,"I(F (x); T (x)|c) = I(F (x); c|T (x)) ‚àíI(F (x); c)
|
{z
}
(‚ñ†)"
DISCUSSIONS,0.12280701754385964,"+ I(F (x); T (x))
|
{z
}
(‚ñ°) .
(5)"
DISCUSSIONS,0.125,"Proposition 4.1 and 4.2 can be derived from the exact bound of InfoNCE (Poole et al., 2019). SupCon
and SelfCon-S loss have 2(K ‚àí1) negative sample size because of the augmented negative pairs for
SupCon and the sub-network features for SelfCon-S. Note that SelfCon-M loss has a similar upper
bound as Eq. 3 and Eq. 4 (refer to Appendix A.2)."
DISCUSSIONS,0.12719298245614036,"In Proposition 4.3, (‚ñ†) is an interaction information (Yeung, 1991) that measures the inÔ¨Çuence
of T (x) on the amount of shared information between F (x) and c. SelfCon-S loss is related to
increasing this interaction information, so that the intermediate feature enhances the correlation
between the last feature and the label, which may result in improving the downstream classiÔ¨Åcation
tasks. In addition, (‚ñ°) implies that SelfCon loss increases the MI between the intermediate and the
last features in the backbone. T (x) is distilled from F (x) that has richer class-related information
and consequently, earlier layers are trained to produce better representation (Zhang et al., 2019a; Ahn
et al., 2019). We believe this is theoretical evidence for the improved performance of SelfCon. Also,
refer to Appendix A.4 for the connection to the classiÔ¨Åcation performance of the SelfCon loss."
DISCUSSIONS,0.12938596491228072,Under review as a conference paper at ICLR 2022
DISCUSSIONS,0.13157894736842105,"Table 1: The results of linear evaluation on ResNet-18 and ResNet-50 for various datasets. In
the supervised setting, we compare our SelfCon-M and SelfCon-S with cross-entropy (CE) loss,
supervised contrastive loss with multi-view (SupCon), and without multi-view (SupCon-S). Bold
type is for all the values of which the standard deviation range overlaps with that of the best accuracy."
DISCUSSIONS,0.1337719298245614,"ResNet-18
ResNet-50
Method
Single-View
CIFAR-10
CIFAR-100
Tiny-ImageNet
CIFAR-10
CIFAR-100
Tiny-ImageNet"
DISCUSSIONS,0.13596491228070176,"CE
‚úì
94.7¬±0.1
72.9¬±0.1
57.5¬±0.3
94.9¬±0.2
74.8¬±0.1
62.3¬±0.4
SupCon
94.7¬±0.2
73.0¬±0.0
56.9¬±0.4
95.6a"
DISCUSSIONS,0.13815789473684212,"¬±0.1
75.5a"
DISCUSSIONS,0.14035087719298245,"¬±0.2
61.6¬±0.2
SupCon-Sb
‚úì
94.9¬±0.0
73.9¬±0.1
58.4¬±0.3
95.8¬±0.1
76.7¬±0.1
62.0¬±0.2
SelfCon-M (ours)
95.0¬±0.1
74.9¬±0.1
59.2¬±0.0
95.5¬±0.1
76.9¬±0.1
63.0¬±0.2
SelfCon-S (ours)
‚úì
95.3¬±0.2
75.4¬±0.1
59.8¬±0.4
95.7¬±0.2
78.5¬±0.3
63.7¬±0.2"
DISCUSSIONS,0.1425438596491228,"aWe have re-implemented SupCon method (Khosla et al., 2020) and also run their ofÔ¨Åcial code for credibility,
but the accuracy was slightly lower than their reported numbers.
bSupCon-S sets I as {1, ..., B} in Eq. 1. Although Khosla et al. (2020) did not propose the version of the
single-view, we implemented SupCon-S since it is worth investigating the effect of multi-viewed batch."
DISCUSSIONS,0.14473684210526316,Question 2: Is it applicable to unsupervised representation learning?
DISCUSSIONS,0.14692982456140352,"Good representations should get rid of redundant input information and extract meaningful features
(Tishby & Zaslavsky, 2015). However, in Eq. 2, the backbone network can be an anchored function
where the contrastive features are those from the backbone as well as the sub-network. Unfortunately,
this might allow the last feature to follow the intermediate feature, learning redundant information
about the input. This could be why SelfCon learning might not work in an unsupervised environment
where there is no label information (refer to Appendix D.1). However, in Appendix D.2, we suggest
that blocking the backbone from following the sub-network, i.e., removing the term in Eq. 2 where
œâ = F , œâ‚àó= G, can mitigate the problem."
DISCUSSIONS,0.14912280701754385,"Supervised contrastive framework, which is the main interest of our paper, is away from the above
problem owing to the label information. Note that the unsupervised version of SelfCon-S loss
guarantees only the lower bound of (‚ñ°) in Proposition 4.3. Meanwhile, (‚ñ†), induced by the condition
on label, encourages the backbone network to be in accordance with label information. Thus, we
believe that jointly maximizing the lower bound of (‚ñ°+ ‚ñ†) offers an evidence for the success of
supervised SelfCon learning."
EXPERIMENT,0.1513157894736842,"5
EXPERIMENT"
EXPERIMENT,0.15350877192982457,"We presented the image classiÔ¨Åcation accuracy for standard benchmarks, such as CIFAR-10, CIFAR-
100 (Krizhevsky et al., 2009), Tiny-ImageNet (Le & Yang, 2015), ImageNet-100 (Tian et al., 2019a),"
EXPERIMENT,0.15570175438596492,"and ImageNet (Deng et al., 2009), and extensively analyzed the results. We summarized the mean and
standard deviation of top-1 accuracy over three random seeds for the reliability of the experimental
results."
EXPERIMENT,0.15789473684210525,"We used the optimal structure and position of the sub-network for all the network architectures (see
Appendix C). We compared three types of the sub-network: the same structure with non-sharing
layers of the backbone network (same), a structure with the number of non-sharing layers reduced in
half (small), and a simple fully-connected layer (fc). The overall performance was comparable or
better than the baselines. We observed a trend: in shallow networks the same structure was better,
while fc was better in deeper networks. Besides, the performance was consistently good when the
exit path is attached after the midpoint of the encoder (e.g., 2nd block in ResNet or 3rd block in VGG
architecture). Complete implementation details are in Appendix B."
REPRESENTATION LEARNING,0.1600877192982456,"5.1
REPRESENTATION LEARNING"
REPRESENTATION LEARNING,0.16228070175438597,"We measured the classiÔ¨Åcation accuracy on the representation learning protocol (Chen et al., 2020),
which consists of 2-stage training, (1) pretraining an encoder network and (2) Ô¨Åne-tuning on a linear
classiÔ¨Åer with the frozen encoder (called a linear evaluation)."
REPRESENTATION LEARNING,0.16447368421052633,Under review as a conference paper at ICLR 2022
REPRESENTATION LEARNING,0.16666666666666666,"Table 2:
The classiÔ¨Åcation accuracy on
ResNet-18 and ResNet-50 for ImageNet-
100. Parentheses indicate the performance of
ensemble prediction in the multi-exit frame-
work (refer to Section 5.4).
We reported
the results with one random seed. Note that
Acc@5 shows the same trend of Acc@1."
REPRESENTATION LEARNING,0.16885964912280702,"Method
ResNet-18
ResNet-50"
REPRESENTATION LEARNING,0.17105263157894737,"CE
81.1
82.3
SupCon
83.0
86.3
SupCon-S
83.5
86.8
SelfCon-M
83.9(85.9)
86.8(88.1)
SelfCon-S
84.5(86.0)
87.8(88.2)"
REPRESENTATION LEARNING,0.17324561403508773,"InfoNCE
   MINE
   NWJ"
REPRESENTATION LEARNING,0.17543859649122806,"MI Estimator
Test Acc."
REPRESENTATION LEARNING,0.17763157894736842,Test Accuracy (%) 70 72 74 76 78 80
REPRESENTATION LEARNING,0.17982456140350878,Mutual Information I(F(x);T(x)) 0 0.5 1.0 1.5 2.0
REPRESENTATION LEARNING,0.18201754385964913,"CE
SupCon SelfCon-M SelfCon-S"
REPRESENTATION LEARNING,0.18421052631578946,"Figure 2: Test accuracy and the estimated mu-
tual information of different methods. Mutual
information estimators are measured between the
intermediate and the last features."
REPRESENTATION LEARNING,0.18640350877192982,"The classiÔ¨Åcation accuracy is summarized in Table 1. Interestingly, the loss functions in the single-
viewed batch, such as SupCon-S and SelfCon-S, outperform their multi-view counterparts in all
settings. Furthermore, our SelfCon learning, which trains with the sub-network, shows higher
classiÔ¨Åcation accuracy than CE and SupCon. The effects of the sub-network are analyzed in Section
5.4. We also experimented on the ImageNet-100 benchmark, of which 100 classes were randomly
sampled (Tian et al., 2019a), to verify that SelfCon learning has the same effect on large scale datasets.
Table 2 presents the consistent performance improvement of SelfCon learning. Refer to Appendix G
for the ImageNet results."
MUTUAL INFORMATION ESTIMATION,0.18859649122807018,"5.2
MUTUAL INFORMATION ESTIMATION"
MUTUAL INFORMATION ESTIMATION,0.19078947368421054,"Minimizing the SelfCon loss is highly related to maximizing the MI between the intermediate and
the last features (Proposition 4.3). To empirically conÔ¨Årm this claim, we estimated MI using various
estimators: InfoNCE (Oord et al., 2018), MINE (Belghazi et al., 2018), and NWJ (Nguyen et al.,
2010). SpeciÔ¨Åcally, we extracted the features of the CIFAR-100 dataset from the pretrained ResNet-50
encoders and computed the estimators. Then we optimized a simple 3-layer Conv-ReLU network
with the MI estimator objectives. We consider the encoder output without the projection head, which
differs from Section 3."
MUTUAL INFORMATION ESTIMATION,0.19298245614035087,"In Figure 2, SelfCon-M and SelfCon-S both show high I(F (x); T (x)), which supports our claim,
while the values of CE and SupCon are lower. Recall that T (x) is the intermediate feature of the
backbone network, which is an input to the auxiliary network path. Although I(F (x); T (x)) does
not explicitly guarantee an increase in accuracy, our results imply that it has a positive correlation
with the test accuracy of the backbone network. We believe that the richer information in earlier
features makes the encoder output better representation because the intermediate feature is also the
input for the subsequent layers."
MUTUAL INFORMATION ESTIMATION,0.19517543859649122,"We also found that SelfCon learning increases the information between T (x) and the label, implying
that the intermediate feature is imbued with class-related knowledge, while the information between
T (x) and the input x is decreased (refer to Table 12 in Appendix E). This suggests that SelfCon
learning is in agreement with IB principle of training a deep neural network, i.e., Ô¨Åtting-compression
phase (Saxe et al., 2019a). Furthermore, refer to Appendix K for the additional empirical evidence of
correlation between MI and improved performance."
MUTUAL INFORMATION ESTIMATION,0.19736842105263158,"5.3
MULTI-VIEW VS. SINGLE-VIEW"
MUTUAL INFORMATION ESTIMATION,0.19956140350877194,"We compare the multi-view and single-view in terms of generalization error, efÔ¨Åciency, and batch
size."
MUTUAL INFORMATION ESTIMATION,0.20175438596491227,"Single-view reduces generalization error.
In Figure 3, SupCon shows higher train accuracy but
lower test accuracy than SupCon-S, and the same trend is observed with SelfCon-M and SelfCon-S.
Compared to single-view, multi-view from the augmented image makes the encoder amplify the
memorization of data and result in overÔ¨Åtting to each instance. Figure 4 shows that SelfCon-S
gradually enhanced the generalization ability, while SelfCon-M or SupCon achieved little gain in test
accuracy despite the fast convergence."
MUTUAL INFORMATION ESTIMATION,0.20394736842105263,Under review as a conference paper at ICLR 2022
MUTUAL INFORMATION ESTIMATION,0.20614035087719298,"Train Acc, -M
Train Acc, -S"
MUTUAL INFORMATION ESTIMATION,0.20833333333333334,"Test Acc, -M
Test Acc, -S"
MUTUAL INFORMATION ESTIMATION,0.21052631578947367,Train Accuracy (%) 95 96 97 98 99 100
MUTUAL INFORMATION ESTIMATION,0.21271929824561403,Test Accuracy (%) 90 92 94 96 98 100
MUTUAL INFORMATION ESTIMATION,0.2149122807017544,"SupCon
SelfCon"
MUTUAL INFORMATION ESTIMATION,0.21710526315789475,(a) CIFAR-10
MUTUAL INFORMATION ESTIMATION,0.21929824561403508,"Train Acc, -M
Train Acc, -S"
MUTUAL INFORMATION ESTIMATION,0.22149122807017543,"Test Acc, -M
Test Acc, -S"
MUTUAL INFORMATION ESTIMATION,0.2236842105263158,Train Accuracy (%) 90 92 94 96 98 100
MUTUAL INFORMATION ESTIMATION,0.22587719298245615,Test Accuracy (%) 70 72 74 76 78 80
MUTUAL INFORMATION ESTIMATION,0.22807017543859648,"SupCon
SelfCon"
MUTUAL INFORMATION ESTIMATION,0.23026315789473684,(b) CIFAR-100
MUTUAL INFORMATION ESTIMATION,0.2324561403508772,"Train Acc, -M
Train Acc, -S"
MUTUAL INFORMATION ESTIMATION,0.23464912280701755,"Test Acc, -M
Test Acc, -S"
MUTUAL INFORMATION ESTIMATION,0.23684210526315788,Train Accuracy (%) 70 80 90 100
MUTUAL INFORMATION ESTIMATION,0.23903508771929824,Test Accuracy (%) 45 50 55 60 65 70
MUTUAL INFORMATION ESTIMATION,0.2412280701754386,"SupCon
SelfCon"
MUTUAL INFORMATION ESTIMATION,0.24342105263157895,"(c) Tiny-ImageNet
Figure 3: Train accuracy and test accuracy of ResNet-18 for different views and loss functions.
The train and test accuracy are measured with a linear classiÔ¨Åer during the linear evaluation. The axis
on the left and right denotes the train accuracy and test accuracy, respectively."
MUTUAL INFORMATION ESTIMATION,0.24561403508771928,"Table 3: Memory (GiB / GPU) and computation time (sec / step) comparison. All numbers are
measured with ResNet-18 training on 8 RTX 2080 Ti GPUs and Intel i9-10940X CPU. Note that
FLOPS is for one sample. B stands for batch size. For the results of ResNet-50, see Appendix H."
MUTUAL INFORMATION ESTIMATION,0.24780701754385964,"Dataset (Image size)
Method
Params
FLOPS
B = 256
B = 512
B = 1024"
MUTUAL INFORMATION ESTIMATION,0.25,"Memory
Time
Memory
Time
Memory
Time"
MUTUAL INFORMATION ESTIMATION,0.25219298245614036,"CIFAR-100 (32x32)
SupCon
11.50 M
1.11 G
2.14
0.13
2.35
0.16
3.18
0.27
SelfCon-S
11.89 M
0.56 G
1.83
0.13
2.03
0.14
2.54
0.18"
MUTUAL INFORMATION ESTIMATION,0.2543859649122807,"Tiny-ImageNet (64x64)
SupCon
11.50 M
1.13 G
2.01
0.14
2.69
0.17
3.97
0.31
SelfCon-S
11.89 M
0.56 G
1.75
0.13
2.05
0.13
2.68
0.18"
MUTUAL INFORMATION ESTIMATION,0.2565789473684211,"ImageNet-100 (224x224)
SupCon
11.50 M
3.64 G
3.34
0.51
5.34
1.04
9.54
2.11
SelfCon-S
11.90 M
1.82 G
2.54
0.35
3.38
0.70
5.67
1.38"
MUTUAL INFORMATION ESTIMATION,0.25877192982456143,"Single-view is efÔ¨Åcient in terms of memory usage and computational cost.
In Table 3, we
compared SupCon and SelfCon-S to observe the efÔ¨Åciency of the single-viewed batch. SelfCon-S
requires a larger number of parameters owing to the extra sub-network but is more efÔ¨Åcient in memory
and computation. In both SupCon and SelfCon-S, the same batch size implies the same number of
anchor features; however, they differ in memory consumption due to the data augmentation of the
multi-viewed batch."
MUTUAL INFORMATION ESTIMATION,0.26096491228070173,"Table 4: CIFAR-100 results on ResNet-18
with various batch sizes. We omitted the stan-
dard deviation due to the lack of margin."
MUTUAL INFORMATION ESTIMATION,0.2631578947368421,Batch Size
MUTUAL INFORMATION ESTIMATION,0.26535087719298245,"Method
64
128
256
512
1024"
MUTUAL INFORMATION ESTIMATION,0.2675438596491228,"CE
74.9
74.9
74.1
73.3
72.9
SupCon
74.8
73.8
72.9
72.5
73.0
SupCon-S
73.6
75.3
75.0
74.0
73.9
SelfCon-M
75.8
76.5
75.9
75.0
74.9
SelfCon-S
74.0
76.6
77.0
75.8
75.4"
MUTUAL INFORMATION ESTIMATION,0.26973684210526316,"Multi-view is advantageous for small batch size.
In supervised learning, large batch size reduces the
generalization ability, which results in decreasing
the performance (You et al., 2017; Luo et al., 2018;
Wu et al., 2020a). We examined whether the perfor-
mance in a supervised contrastive framework is also
dependent on the batch size. Table 4 summarizes
the results. The multi-viewed method, e.g., SupCon,
which doubles the effective number of training data,
outperformed the single-viewed counterpart in 64-
batch experiments; the opposite was observed in
all other batch sizes. For small batch size such as
64, doubling the effective batch size can make the learning stable, but as batch size gets larger the
stabilizing effect from multi-views decreases and the necessity of regularization appears to be critical.
Hence, the optimal batch size of single-viewed methods was higher (SupCon-S vs. SupCon and
SelfCon-S vs. SelfCon-M) because single-view itself helps regularization (i.e., small batch with
single-view might lead to under-Ô¨Åtting). Also, note that SelfCon learning with sub-network still
surpasses the SupCon counterpart. Refer to Appendix I for the sensitivity study of the learning rates."
MUTUAL INFORMATION ESTIMATION,0.2719298245614035,"5.4
WHAT DOES THE SUB-NETWORK ACHIEVE?"
MUTUAL INFORMATION ESTIMATION,0.2741228070175439,"Regularization effect
SelfCon loss regularizes the sub-network to output the similar features to the
backbone network. It prevents the encoder from overÔ¨Åtting to data, and it is effective in multi-viewed
as well as single-viewed batch. In Figure 3, we conÔ¨Årmed the regularization effect (i.e., lower train
accuracy, but higher test accuracy) by comparing each bar of the same color. The strong regularization
of the sub-network helped SelfCon (-M, -S) outperform the SupCon counterparts. This trend is also
observed in Table 4 and Figure 4."
MUTUAL INFORMATION ESTIMATION,0.27631578947368424,Under review as a conference paper at ICLR 2022
MUTUAL INFORMATION ESTIMATION,0.27850877192982454,"SupCon
SelfCon-M
SelfCon-S"
MUTUAL INFORMATION ESTIMATION,0.2807017543859649,Accuracy (%) 60 70 80 90 100
MUTUAL INFORMATION ESTIMATION,0.28289473684210525,Epochs
MUTUAL INFORMATION ESTIMATION,0.2850877192982456,100 200 300 400 500 600 700 800 900 1000
MUTUAL INFORMATION ESTIMATION,0.28728070175438597,"Figure 4: CIFAR-100 accuracy
at different training epochs.
The solid line is for train accu-
racy and the dashed line is for test
accuracy. ResNet-18 is used."
MUTUAL INFORMATION ESTIMATION,0.2894736842105263,"Conv1
Conv2
Conv3
Conv4"
ST BLOCK,0.2916666666666667,1st Block
ND BLOCK,0.29385964912280704,2nd Block
RD BLOCK,0.29605263157894735,3rd Block
TH BLOCK,0.2982456140350877,4th Block
TH BLOCK,0.30043859649122806,"Conv1
Conv2
Conv3
Conv4 0 0.1 0.2 0.3 0.4"
TH BLOCK,0.3026315789473684,Gradient Norm
ST BLOCK,0.3048245614035088,1st Block
ND BLOCK,0.30701754385964913,2nd Block
RD BLOCK,0.3092105263157895,3rd Block
TH BLOCK,0.31140350877192985,4th Block
TH BLOCK,0.31359649122807015,"Figure 5: Gradient norm of each ResNet-18 block and con-
volutional layer. We computed gradients from the SupCon loss
(Left) and SelfCon-M loss (Right), both from the same initial-
ized model. All convolution layers in the block are named by
order."
TH BLOCK,0.3157894736842105,"Table 5: ClassiÔ¨Åcation accuracy with the classiÔ¨Åers after backbone, sub-network, and the en-
semble of them. The encoder is pretrained by SelfCon-S loss function."
TH BLOCK,0.31798245614035087,"ResNet-18
ResNet-50
Method
CIFAR-10
CIFAR-100
Tiny-ImageNet
CIFAR-10
CIFAR-100
Tiny-ImageNet"
TH BLOCK,0.3201754385964912,"Backbone
95.3¬±0.2
75.4¬±0.1
59.8¬±0.4
95.7¬±0.2
78.5¬±0.2
63.7¬±0.2
Sub-network
92.6¬±0.1
69.1¬±0.3
53.5¬±0.1
93.6¬±0.2
73.3¬±0.3
58.9¬±0.6
Ensemble
95.2¬±0.1
77.4¬±0.0
62.2¬±0.3
95.5¬±0.1
80.0¬±0.2
65.7¬±0.5"
TH BLOCK,0.3223684210526316,"Vanishing gradient
SelfCon learning can send more abundant information to the earlier layers
through the gradients Ô¨Çowing from the sub-networks. Previous works (Lee et al., 2015; Teerapit-
tayanon et al., 2016; Zhang et al., 2019a) point out that the success of the multi-exit framework is
owing to solving the vanishing gradient. We showed that the same argument applies to our SelfCon
learning. Note that the sub-network is positioned after the 2nd block of the backbone. In Figure 5, a
larger gradient Ô¨Çows up to the earlier layer in the SelfCon-M, while a large amount of the SupCon
loss gradient vanishes. In particular, there is a signiÔ¨Åcant difference in the gradient norm in the 2nd
block."
TH BLOCK,0.32456140350877194,"Ensemble with sub-network
The sub-network in SelfCon learning can also be used on downstream
tasks such as image classiÔ¨Åcation. In our previous experiments, we followed the linear evaluation
protocol, Ô¨Åne-tuning a classiÔ¨Åer with the frozen backbone network. The network pretrained on
SelfCon learning has an exit path, which is similarly allowed to be frozen and used as linear
evaluation. We thus demonstrated two additional types of linear evaluation scenarios: (1) Ô¨Åne-tuning
a classiÔ¨Åer after the sub-network output and (2) Ô¨Åne-tuning two classiÔ¨Åers after the backbone and
the sub-network and ensembling two classiÔ¨Åers‚Äô predictions. Table 5 indicates that the sub-network
could make appropriate, although not the best, predictions as the backbone did, while ensembling
their predictions is found to be the most powerful trick we have come up with."
TH BLOCK,0.3267543859649123,"Feature-level multi-view
One of the advantages of SelfCon learning is to relax the dependency on
multi-viewed batch. This is accomplished by the multi-views on the representation space made by the
parameters of the sub-network. In Figure 6, we visualized via Grad-CAM (Selvaraju et al., 2017) the
gradient of SelfCon-S loss with respect to the intermediate layer of the backbone network, right before
the exit path. Both networks focus on similar, but clearly different pixels of the same input image,
implying that the sub-network learns another view in the feature space. As multi-view in contrastive
learning requires domain-speciÔ¨Åc augmentation, recent studies have explored the domain-agnostic
way of augmentation (Lee et al., 2020; Verma et al., 2021). SelfCon might be an intriguing future
work in that auxiliary networks could be an efÔ¨Åcient substitute for data augmentation."
ABLATION STUDY,0.32894736842105265,"5.5
ABLATION STUDY"
ABLATION STUDY,0.33114035087719296,"Different encoder architectures
We experimented with other architectures: VGG-16 (Simonyan
& Zisserman, 2014) with Batch Normalization (BN) (Ioffe & Szegedy, 2015) and WRN-16-8
(Zagoruyko & Komodakis, 2016), and the results are presented in Table 6. The classiÔ¨Åcation accuracy"
ABLATION STUDY,0.3333333333333333,Under review as a conference paper at ICLR 2022
ABLATION STUDY,0.3355263157894737,"Figure 6: Visualizations for the feature-level multi-view generated by the sub-network. Along
with the original image, each map visualizes the gradients from the sub-network (Left) and the
backbone network (Right), respectively. We measured the gradient of the pretrained ResNet-18 with
SelfCon-S loss."
ABLATION STUDY,0.33771929824561403,"Table 6: The results of linear evaluation on WRN-16-8 and VGG-16 with BN for various
datasets. We tuned the best structure and position of the sub-network for each architecture. Appendix
C summarizes the implementation details."
ABLATION STUDY,0.3399122807017544,"WRN-16-8
VGG-16 wih BN
Method
Single-View
CIFAR-10
CIFAR-100
Tiny-ImageNet
CIFAR-10
CIFAR-100
Tiny-ImageNet"
ABLATION STUDY,0.34210526315789475,"CE
‚úì
94.6¬±0.1
73.6¬±0.6
56.5¬±0.5
93.8¬±0.3
71.2¬±0.2
60.7¬±0.1
SupCon
95.3¬±0.0
75.1¬±0.3
57.4¬±0.3
93.6¬±0.1
69.6¬±0.1
57.3¬±0.4
SupCon-S
‚úì
95.2¬±0.1
76.0¬±0.1
57.3¬±0.5
93.8¬±0.3
71.1¬±0.0
58.4¬±0.2
SelfCon-M (ours)
95.4¬±0.2
75.6¬±0.1
58.7¬±0.1
93.4¬±0.1
71.7¬±0.3
59.4¬±0.1
SelfCon-S (ours)
‚úì
95.5¬±0.0
76.6¬±0.1
59.3¬±0.2
93.5¬±0.1
72.0¬±0.0
60.7¬±0.1"
ABLATION STUDY,0.3442982456140351,"for WRN-16-8 showed a similar trend as that of ResNet architectures. However, for VGG-16 with
BN architecture, SupCon had lower performance than CE on every dataset. Although the contrastive
learning approach does not seem to result in signiÔ¨Åcant changes for the VGG-16 with BN encoders,
SelfCon-S was better than or comparable to CE."
ABLATION STUDY,0.34649122807017546,"Table 7: CIFAR-100 1-stage training re-
sults on ResNet architectures. ‚Ä† describes
a modiÔ¨Åcation to 1-stage training with
multi-exit framework. We omitted the stan-
dard deviation. Parentheses indicate the
sub-network‚Äôs accuracy."
ABLATION STUDY,0.34868421052631576,"Method
ResNet-18
ResNet-50"
ABLATION STUDY,0.3508771929824561,"CE
72.9
74.8
CE w/ Sub‚Ä†
73.5(69.2)
76.2(72.3)
SD‚Ä†
73.5(71.5)
76.1(73.3)
SelfCon-S‚Ä†
74.5(70.6)
76.8(72.6)
SelfCon-S (2-stage)
75.4(69.1)
78.5(73.3)"
-STAGE TRAINING,0.3530701754385965,"1-stage training
1-stage training framework, i.e., not
decoupling the encoder pretraining and linear evalua-
tion, on the single-viewed batch, is standard for super-
vised learning. With a multi-exit framework, Zhang
et al. (2019a) propose Self-Distillation (SD), which
distills logit information within the network itself, i.e.,
LSD = Œ±LCE + (1 ‚àíŒ±)LKL. We replaced the KL
divergence term with Lself-s in Eq. 2, and distilled the
features from the projection head, instead of the logits.
From Table 7, we observed that adding cross-entropy
loss to the sub-network (CE w/ Sub) improved the back-
bone network‚Äôs classiÔ¨Åcation performance. However,
the results of SD suggested that there is a saturation of
increase even when the classiÔ¨Åer of sub-network better converged. Meanwhile, we hypothesize that
feature distillation of SelfCon-S makes the encoder learn better representation than logit distillation
of classiÔ¨Åers. We would like to highlight that SelfCon loss in 2-stage training still demonstrated the
best classiÔ¨Åcation accuracy, as Khosla et al. (2020) argued that representation learning mitigates the
poor generalization performance of CE-based 1-stage training."
CONCLUSION,0.35526315789473684,"6
CONCLUSION"
CONCLUSION,0.3574561403508772,"We proposed SelfCon learning, which self-contrasts the features from the multiple levels of a network.
SelfCon learning is free from the issues that multi-viewed batch triggers. We found that SelfCon loss
maximizes the lower bound of label-conditional MI between the intermediate and the last features,
and it is related to improving the classiÔ¨Åcation performance. In addition, we analyzed why SelfCon is
better by exploring the effect of single-view and sub-network. We verify by extensive experiments,
including ImageNet-100 and ImageNet, that SelfCon-S loss outperforms CE and SupCon."
CONCLUSION,0.35964912280701755,Under review as a conference paper at ICLR 2022
REFERENCES,0.3618421052631579,REFERENCES
REFERENCES,0.36403508771929827,"Sungsoo Ahn, Shell Xu Hu, Andreas Damianou, Neil D Lawrence, and Zhenwen Dai. Variational
information distillation for knowledge transfer. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 9163‚Äì9171, 2019."
REFERENCES,0.36622807017543857,"Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing
mutual information across views. arXiv preprint arXiv:1906.00910, 2019."
REFERENCES,0.3684210526315789,"David Barber and Felix Agakov. The im algorithm: a variational approach to information maximiza-
tion. Advances in neural information processing systems, 16(320):201, 2004."
REFERENCES,0.3706140350877193,"Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron
Courville, and R Devon Hjelm. Mine: mutual information neural estimation. arXiv preprint
arXiv:1801.04062, 2018."
REFERENCES,0.37280701754385964,"Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
Unsupervised learning of visual features by contrasting cluster assignments. arXiv preprint
arXiv:2006.09882, 2020."
REFERENCES,0.375,"Mathilde Caron, Hugo Touvron, Ishan Misra, Herv√© J√©gou, Julien Mairal, Piotr Bojanowski, and
Armand Joulin. Emerging properties in self-supervised vision transformers. arXiv preprint
arXiv:2104.14294, 2021."
REFERENCES,0.37719298245614036,"Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International conference on machine learning, pp.
1597‚Äì1607. PMLR, 2020."
REFERENCES,0.3793859649122807,"Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. arXiv preprint
arXiv:2011.10566, 2020."
REFERENCES,0.3815789473684211,"Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment:
Learning augmentation strategies from data. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 113‚Äì123, 2019."
REFERENCES,0.38377192982456143,"Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated
data augmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition Workshops, pp. 702‚Äì703, 2020."
REFERENCES,0.38596491228070173,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248‚Äì255. Ieee, 2009."
REFERENCES,0.3881578947368421,"Zhiyuan Fang, Jianfeng Wang, Lijuan Wang, Lei Zhang, Yezhou Yang, and Zicheng Liu. Seed:
Self-supervised distillation for visual representation. arXiv preprint arXiv:2101.04731, 2021."
REFERENCES,0.39035087719298245,"Ziv Goldfeld, Ewout van den Berg, Kristjan H Greenewald, Igor Melnyk, Nam Nguyen, Brian
Kingsbury, and Yury Polyanskiy. Estimating information Ô¨Çow in deep neural networks. In ICML,
2019."
REFERENCES,0.3925438596491228,"Priya Goyal, Piotr Doll√°r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola,
Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017."
REFERENCES,0.39473684210526316,"Jean-Bastien Grill, Florian Strub, Florent Altch√©, Corentin Tallec, Pierre H Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi
Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. arXiv preprint
arXiv:2006.07733, 2020."
REFERENCES,0.3969298245614035,"Beliz Gunel, Jingfei Du, Alexis Conneau, and Ves Stoyanov. Supervised contrastive learning for
pre-trained language model Ô¨Åne-tuning. arXiv preprint arXiv:2011.01403, 2020."
REFERENCES,0.3991228070175439,Under review as a conference paper at ICLR 2022
REFERENCES,0.40131578947368424,"Michael Gutmann and Aapo Hyv√§rinen. Noise-contrastive estimation: A new estimation principle
for unnormalized statistical models. In Proceedings of the Thirteenth International Conference on
ArtiÔ¨Åcial Intelligence and Statistics, pp. 297‚Äì304. JMLR Workshop and Conference Proceedings,
2010."
REFERENCES,0.40350877192982454,"Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 9729‚Äì9738, 2020."
REFERENCES,0.4057017543859649,"R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam
Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation
and maximization. arXiv preprint arXiv:1808.06670, 2018."
REFERENCES,0.40789473684210525,"Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International conference on machine learning, pp. 448‚Äì456.
PMLR, 2015."
REFERENCES,0.4100877192982456,"Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron
Maschinot, Ce Liu, and Dilip Krishnan.
Supervised contrastive learning.
arXiv preprint
arXiv:2004.11362, 2020."
REFERENCES,0.41228070175438597,"Sungnyun Kim, Gihun Lee, Sangmin Bae, and Se-Young Yun. Mixco: Mix-up contrastive learning
for visual representation. arXiv preprint arXiv:2010.06300, 2020."
REFERENCES,0.4144736842105263,"Soroush Abbasi Koohpayegani, Ajinkya Tejankar, and Hamed Pirsiavash. Compress: Self-supervised
learning by compressing representations. arXiv preprint arXiv:2010.14713, 2020."
REFERENCES,0.4166666666666667,"Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009."
REFERENCES,0.41885964912280704,"Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7:7, 2015."
REFERENCES,0.42105263157894735,"Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu. Deeply-supervised
nets. In ArtiÔ¨Åcial intelligence and statistics, pp. 562‚Äì570. PMLR, 2015."
REFERENCES,0.4232456140350877,"Kibok Lee, Yian Zhu, Kihyuk Sohn, Chun-Liang Li, Jinwoo Shin, and Honglak Lee. I-mix: A
domain-agnostic strategy for contrastive representation learning. arXiv preprint arXiv:2010.08887,
2020."
REFERENCES,0.42543859649122806,"Ralph Linsker. An application of the principle of maximum information preservation to linear systems.
In Advances in neural information processing systems, pp. 186‚Äì194, 1989."
REFERENCES,0.4276315789473684,"Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv
preprint arXiv:1608.03983, 2016."
REFERENCES,0.4298245614035088,"Ping Luo, Xinjiang Wang, Wenqi Shao, and Zhanglin Peng. Towards understanding regularization in
batch normalization. arXiv preprint arXiv:1809.00846, 2018."
REFERENCES,0.43201754385964913,"XuanLong Nguyen, Martin J Wainwright, and Michael I Jordan. Estimating divergence functionals
and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory,
56(11):5847‚Äì5861, 2010."
REFERENCES,0.4342105263157895,"Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive
coding. arXiv preprint arXiv:1807.03748, 2018."
REFERENCES,0.43640350877192985,"Liam Paninski. Estimation of entropy and mutual information. Neural computation, 15(6):1191‚Äì1253,
2003."
REFERENCES,0.43859649122807015,"Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational
bounds of mutual information. In International Conference on Machine Learning, pp. 5171‚Äì5180.
PMLR, 2019."
REFERENCES,0.4407894736842105,"Andrew M Saxe, Yamini Bansal, Joel Dapello, Madhu Advani, Artemy Kolchinsky, Brendan D
Tracey, and David D Cox. On the information bottleneck theory of deep learning. Journal of
Statistical Mechanics: Theory and Experiment, 2019(12):124020, 2019a."
REFERENCES,0.44298245614035087,Under review as a conference paper at ICLR 2022
REFERENCES,0.4451754385964912,"Andrew M Saxe, Yamini Bansal, Joel Dapello, Madhu Advani, Artemy Kolchinsky, Brendan D
Tracey, and David D Cox. On the information bottleneck theory of deep learning. Journal of
Statistical Mechanics: Theory and Experiment, 2019(12):124020, 2019b."
REFERENCES,0.4473684210526316,"Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh,
and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based local-
ization. In Proceedings of the IEEE international conference on computer vision, pp. 618‚Äì626,
2017."
REFERENCES,0.44956140350877194,"Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via information.
arXiv preprint arXiv:1703.00810, 2017."
REFERENCES,0.4517543859649123,"Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014."
REFERENCES,0.45394736842105265,"Jiaming Song and Stefano Ermon. Understanding the limitations of variational mutual information
estimators. arXiv preprint arXiv:1910.06222, 2019."
REFERENCES,0.45614035087719296,"Jiaming Song and Stefano Ermon.
Multi-label contrastive predictive coding.
arXiv preprint
arXiv:2007.09852, 2020."
REFERENCES,0.4583333333333333,"Alessandro Sordoni, Nouha Dziri, Hannes Schulz, Geoff Gordon, Philip Bachman, and Remi Tachet
Des Combes. Decomposed mutual information estimation for contrastive representation learning.
In International Conference on Machine Learning, pp. 9859‚Äì9869. PMLR, 2021."
REFERENCES,0.4605263157894737,"Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. Branchynet: Fast inference
via early exiting from deep neural networks. In 2016 23rd International Conference on Pattern
Recognition (ICPR), pp. 2464‚Äì2469. IEEE, 2016."
REFERENCES,0.46271929824561403,"Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. arXiv preprint
arXiv:1906.05849, 2019a."
REFERENCES,0.4649122807017544,"Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive representation distillation. arXiv
preprint arXiv:1910.10699, 2019b."
REFERENCES,0.46710526315789475,"Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What
makes for good views for contrastive learning. arXiv preprint arXiv:2005.10243, 2020."
REFERENCES,0.4692982456140351,"Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In 2015
IEEE Information Theory Workshop (ITW), pp. 1‚Äì5. IEEE, 2015."
REFERENCES,0.47149122807017546,"Vikas Verma, Thang Luong, Kenji Kawaguchi, Hieu Pham, and Quoc Le. Towards domain-agnostic
contrastive learning. In International Conference on Machine Learning, pp. 10530‚Äì10541. PMLR,
2021."
REFERENCES,0.47368421052631576,"Wenguan Wang, Tianfei Zhou, Fisher Yu, Jifeng Dai, Ender Konukoglu, and Luc Van Gool. Exploring
cross-image pixel contrast for semantic segmentation. arXiv preprint arXiv:2101.11939, 2021."
REFERENCES,0.4758771929824561,"Jingfeng Wu, Wenqing Hu, Haoyi Xiong, Jun Huan, Vladimir Braverman, and Zhanxing Zhu. On the
noisy gradient descent that generalizes as sgd. In International Conference on Machine Learning,
pp. 10367‚Äì10376. PMLR, 2020a."
REFERENCES,0.4780701754385965,"Mike Wu, Chengxu Zhuang, Milan Mosse, Daniel Yamins, and Noah Goodman. On mutual in-
formation in contrastive learning for visual representations. arXiv preprint arXiv:2005.13149,
2020b."
REFERENCES,0.48026315789473684,"Raymond W Yeung. A new outlook on shannon‚Äôs information measures. IEEE transactions on
information theory, 37(3):466‚Äì474, 1991."
REFERENCES,0.4824561403508772,"Yang You, Igor Gitman, and Boris Ginsburg. Large batch training of convolutional networks. arXiv
preprint arXiv:1708.03888, 2017."
REFERENCES,0.48464912280701755,"Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146,
2016."
REFERENCES,0.4868421052631579,Under review as a conference paper at ICLR 2022
REFERENCES,0.48903508771929827,"Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, and Kaisheng Ma. Be your
own teacher: Improve the performance of convolutional neural networks via self distillation. In
Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 3713‚Äì3722,
2019a."
REFERENCES,0.49122807017543857,"Linfeng Zhang, Zhanhong Tan, Jiebo Song, Jingwei Chen, Chenglong Bao, and Kaisheng Ma. Scan:
A scalable neural networks framework towards compact and efÔ¨Åcient models. arXiv preprint
arXiv:1906.03951, 2019b."
REFERENCES,0.4934210526315789,Under review as a conference paper at ICLR 2022
REFERENCES,0.4956140350877193,Appendix
REFERENCES,0.49780701754385964,"A
PROOFS"
REFERENCES,0.5,"A.1
PROOF OF PROPOSITION 4.1"
REFERENCES,0.5021929824561403,"Proof. We simply extend the exact bound of InfoNCE (Poole et al., 2019; Sordoni et al., 2021). Here,
we consider the supervised setting where there are C training classes. Without loss of generality,
choose a class c out of C classes, and let x and z be different samples that share the same class label c.
The derivation for the multi-view (z being an augmented sample of x) is similar. For conciseness of
the proof, we consider that no other image in a batch shares the same class. We prove that minimizing
the SupCon loss (Khosla et al., 2020) maximizes the lower bound of conditional MI between two
samples x and z given the label c:"
REFERENCES,0.5043859649122807,"I(x; z|c) ‚â•log(2K ‚àí1) ‚àíLsup(x, z; F , K)
(6)"
REFERENCES,0.506578947368421,for some function F and hyperparameter K.
REFERENCES,0.5087719298245614,"We start from Barber and Agakov‚Äôs variational lower bound on MI (Barber & Agakov, 2004)."
REFERENCES,0.5109649122807017,"I(x; z|c) = Ep(x,z|c) log p(z|x, c)"
REFERENCES,0.5131578947368421,"p(z|c)
‚â•Ep(x,z|c) log q(z|x, c)"
REFERENCES,0.5153508771929824,"p(z|c)
(7)"
REFERENCES,0.5175438596491229,"where q is a variational distribution. Since q is arbitrary, we can set the sampling strategy as
follows. First, sample z1 from the proposal distribution œÄ(z|c) where c is a class label of x. Then,
sample (K ‚àí1) negative samples {z2, ¬∑ ¬∑ ¬∑ , zK} from the distribution P"
REFERENCES,0.5197368421052632,"c‚Ä≤Ã∏=c œÄ(z, c‚Ä≤), so that these
negative samples do not share the class label with x. We augment each negative sample by random
augmentation and concatenate with the original samples, i.e., {z2, ¬∑ ¬∑ ¬∑ , zK, zK+1, ¬∑ ¬∑ ¬∑ , z2K‚àí1},
where zK+i‚àí1 is the augmented sample from zi for 2 ‚â§i ‚â§K. We deÔ¨Åne the unnormalized density
of z1 given a speciÔ¨Åc set {z2, ¬∑ ¬∑ ¬∑ , z2K‚àí1} and x of label c is"
REFERENCES,0.5219298245614035,"q(z1|x, z2:(2K‚àí1), c) = œÄ(z1|c) ¬∑
(2K ‚àí1) ¬∑ eœàF (x,z1)"
REFERENCES,0.5241228070175439,"eœàF (x,z1) + P2K‚àí1
k=2
eœàF (x,zk)
(8)"
REFERENCES,0.5263157894736842,"where œà is often called a discriminator function (Hjelm et al., 2018), deÔ¨Åned as œàF (u, v) =
F (u) ¬∑ F (v) for some vectors u, v. By setting the proposal distribution as œÄ(z|c) = p(z|c), we
obtain the MI bound:"
REFERENCES,0.5285087719298246,"I(x; z|c) ‚â•Ep(x,z1|c) log q(z1|x, c)"
REFERENCES,0.5307017543859649,"p(z1|c)
(9)"
REFERENCES,0.5328947368421053,"= Ep(x,z1|c) log
Ep(z2:(2K‚àí1)|c)q(z1|x, z2:(2K‚àí1), c)"
REFERENCES,0.5350877192982456,"p(z1|c)
(10)"
REFERENCES,0.5372807017543859,"‚â•Ep(x,z1|c) """
REFERENCES,0.5394736842105263,"Ep(z2:(2K‚àí1)|c) log
p(z1|c) ¬∑
(2K‚àí1)¬∑eœàF (x,z1)"
REFERENCES,0.5416666666666666,"eœàF (x,z1)+P2K‚àí1
k=2
eœàF (x,zk)"
REFERENCES,0.543859649122807,p(z1|c) # (11)
REFERENCES,0.5460526315789473,"= Ep(x,z1|c)p(z2:(2K‚àí1)|c) log
eœàF (x,z1)"
REFERENCES,0.5482456140350878,"1
2K‚àí1
P2K‚àí1
k=1
eœàF (x,zk)
(12)"
REFERENCES,0.5504385964912281,"= log(2K ‚àí1) ‚àíLsup(x, z; F , K).
(13)"
REFERENCES,0.5526315789473685,"where the second inequality is derived from Jensen‚Äôs inequality. Because Eq. 12 is an expectation
with respect to the sampled x and z1, the case where the anchor is swapped to z1 is also being
considered."
REFERENCES,0.5548245614035088,Under review as a conference paper at ICLR 2022
REFERENCES,0.5570175438596491,"A neural network F (backbone in our framework) with L layers are formulated as F = fL ‚ó¶fL‚àí1 ‚ó¶
¬∑ ¬∑ ¬∑ ‚ó¶f1. Then, œàF (u, v) = F (u) ¬∑ F (v) = f1:L(u) ¬∑ f1:L(v). We deÔ¨Åne another discriminator
function as œà‚Ä†
F (u, v) = f(‚Ñì+1):L(u) ¬∑ f(‚Ñì+1):L(v). Obviously, the following equivalence holds:"
REFERENCES,0.5592105263157895,"œà‚Ä†
F (f1:‚Ñì(u), f1:‚Ñì(v)) = œàF (u, v).
(14)"
REFERENCES,0.5614035087719298,"Note that f1:‚Ñì(u) is the ‚Ñì-th intermediate feature of input u. Following the same procedure as in Eq.
9-13,"
REFERENCES,0.5635964912280702,"I(f1:‚Ñì(x); f1:‚Ñì(z)|c) ‚â•Ep(x,z1|c)p(z2:(2K‚àí1)|c) log
eœà‚Ä†
F (f1:‚Ñì(x),f1:‚Ñì(z1))"
REFERENCES,0.5657894736842105,"1
2K‚àí1
P2K‚àí1
k=1
eœà‚Ä†
F (f1:‚Ñì(x),f1:‚Ñì(zk))
(15)"
REFERENCES,0.5679824561403509,"= Ep(x,z1|c)p(z2:(2K‚àí1)|c) log
eœàF (x,z1)"
REFERENCES,0.5701754385964912,"1
2K‚àí1
P2K‚àí1
k=1
eœàF (x,zk)
(16)"
REFERENCES,0.5723684210526315,"= log(2K ‚àí1) ‚àíLsup(x, z; F , K)
(17)"
REFERENCES,0.5745614035087719,"From above, as the intermediate feature is arbitrary to the position, we can obtain a similar inequality:"
REFERENCES,0.5767543859649122,"I(f(‚Ñì+1):L(f1:‚Ñì(x)); f(‚Ñì+1):L(f1:‚Ñì(z))|c) = I(F (x); F (z)|c)
(18)"
REFERENCES,0.5789473684210527,"= I(f1:L(x); f1:L(z)|c)
(19)
‚â•log(2K ‚àí1) ‚àíLsup(x, z; F , K).
(20)"
REFERENCES,0.581140350877193,"A.2
PROOF OF PROPOSITION 4.2"
REFERENCES,0.5833333333333334,"Proof. In Section 4.1, we proved that SupCon loss maximizes the lower bound of conditional MI
between the output features of a positive pair. We can think of another scenario where the network F
now has a sub-network G. Assume that the sub-network has M > ‚Ñìlayers: G = gM ‚ó¶gM‚àí1‚ó¶¬∑ ¬∑ ¬∑‚ó¶g1.
As we discussed in the paper, the exit path is placed after the ‚Ñì-th layer, so regarding our deÔ¨Ånition of
the sub-network, G shares the same parameters with F up to ‚Ñì-th layer, i.e., g1 = f1, g2 = f2, ¬∑ ¬∑ ¬∑ ,
g‚Ñì= f‚Ñì. DeÔ¨Åne œàG(u, v) = G(u) ¬∑ G(v)"
REFERENCES,0.5855263157894737,"We introduce a discriminator function that measures the similarity between the outputs from the
backbone and the sub-network, œàF G(u, v) = F (u) ¬∑ G(v). Similarly, œàGF (u, v) = G(u) ¬∑ F (v).
Considering that the SelfCon-S loss has the anchored function of F and G, we obtain an upper
bound of two symmetric mutual information. Here, z1 = x because SelfCon-S loss is deÔ¨Åned on the
single-viewed batch and we assume that other images in a batch (i.e., z2, ¬∑ ¬∑ ¬∑ , zK) are sampled from
the different class label with x."
REFERENCES,0.5877192982456141,"I(F (x); G(x)|c) + I(G(x); F (x)|c)
(21)"
REFERENCES,0.5899122807017544,"‚â•E log
eœàF G(x,x)"
REFERENCES,0.5921052631578947,"1
2K‚àí1
 
eœàF G(x,x) + PK
k=2 eœàF G(x,zk) + PK
k=2 eœàF (x,zk)"
REFERENCES,0.5942982456140351,"+ E log
eœàGF (x,x)"
REFERENCES,0.5964912280701754,"1
2K‚àí1
 
eœàGF (x,x) + PK
k=2 eœàGF (x,zk) + PK
k=2 eœàG(x,zk)
(22)"
REFERENCES,0.5986842105263158,"= 2 log(2K ‚àí1) ‚àí2Lself-s(x; {F , G}, K)
(23)"
REFERENCES,0.6008771929824561,"Due to the symmetry of mutual information,"
REFERENCES,0.6030701754385965,"I(F (x); G(x)|c) ‚â•log(2K ‚àí1) ‚àíLself-s(x; {F , G}, K)
(24)"
REFERENCES,0.6052631578947368,Under review as a conference paper at ICLR 2022
REFERENCES,0.6074561403508771,"In addition, we can similarly bound the SelfCon-M loss. As the derivation of SupCon loss bound, only
consider the anchor x and its positive pair z1. When the anchored feature is F (x), the contrastive
features are: G(x), G(z), and F (z). By symmetry, when the anchored feature is G(x), the
contrastive features are: F (x), F (z), and G(z). As the derivation of the SupCon loss bound, we
assume the augmented negative samples, i.e., {z2, ¬∑ ¬∑ ¬∑ , zK, zK+1, ¬∑ ¬∑ ¬∑ , z2K‚àí1}."
REFERENCES,0.6096491228070176,"I(F (x); G(x)|c) + I(F (x); G(z)|c) + I(F (x); F (z)|c)
+I(G(x); F (x)|c) + I(G(x); F (z)|c) + I(G(x); G(z)|c)
(25) ‚â•1"
E LOG,0.6118421052631579,"3 E log
eœàF G(x,x) ¬∑ eœàF G(x,z1) ¬∑ eœàF (x,z1)"
E LOG,0.6140350877192983,"1
4K‚àí1"
E LOG,0.6162280701754386," 
eœàF G(x,x) + P2K‚àí1"
E LOG,0.618421052631579,"k=1
eœàF G(x,zk) + P2K‚àí1"
E LOG,0.6206140350877193,"k=1
eœàF (x,zk) + 1"
E LOG,0.6228070175438597,"3 E log
eœàGF (x,x) ¬∑ eœàGF (x,z1) ¬∑ eœàG(x,z1)"
E LOG,0.625,"1
4K‚àí1"
E LOG,0.6271929824561403," 
eœàGF (x,x) + P2K‚àí1"
E LOG,0.6293859649122807,"k=1
eœàGF (x,zk) + P2K‚àí1"
E LOG,0.631578947368421,"k=1
eœàG(x,zk)
(26) = 2"
E LOG,0.6337719298245614,"3 log(4K ‚àí1) ‚àí2Lself-m(x, z; {F , G}, K)
(27)"
E LOG,0.6359649122807017,"A.3
PROOF OF PROPOSITION 4.3"
E LOG,0.6381578947368421,"F (x) and G(x) are the output features from the backbone network and the sub-network, respectively.
Recall that T denotes the sharing layers between F and G. T (x) is the intermediate feature of the
backbone which is also an input to the auxiliary network path."
E LOG,0.6403508771929824,"Before proving the following Lemma, we would like to note that the usefulness of mutual infor-
mation should be carefully discussed on the stochastic mapping of a neural network. If a mapping
T (x) 7‚ÜíF (x) is a deterministic mapping, then the MI between T (x) and F (x) is degnerate
because I(T (x); F (x)) is either inÔ¨Ånite for continuous T (x) (conditional differential entropy is
‚àí‚àû) or a constant for discrete T (x) which is independent on the network‚Äôs parameters (equal to
H(T (x))). However, for studying the usefulness of mutual information in a deep neural network,
the map T (x) 7‚ÜíF (x) is considered as a stochastic parameterized channel. In many recent works
about information theory with DNN, they view the training via SGD is a stochastic process, and the
stochasticity in the training procedure lets us deÔ¨Åne the MI with stochastically trained representations
(Shwartz-Ziv & Tishby, 2017; Goldfeld et al., 2019; Saxe et al., 2019b; Goldfeld et al., 2019). Our
theoretical claim focuses on the SelfCon loss as a training loss optimized by SGD algorithm. There-
fore, analyzing the MI between the hidden representations while training with the SelfCon loss is
based on the information theory to understand DNN (Tishby & Zaslavsky, 2015)."
E LOG,0.6425438596491229,"Also, information theory in deep learning, especially in contrastive learning, is based on the InfoMax
principle (Linsker, 1989) which is about learning a neural network that maps a set of input to a set of
output to maximize the average mutual information between the input and output of a neural network,
subject to stochastic processes. This InfoMax principle is nowadays widely used for analyzing and
optimizing DNNs. Most works for contrastive learning based on maximizing mutual information
grounds on the InfoMax principle, and they are grounding on the stochastic mapping of an encoder.
Moreover, Poole et al. (2019) rigorously discussed the mutual information with respect to a stochastic
encoder. This is common practice in representation learning context where x is data and z is a
learned stochastic representation."
E LOG,0.6447368421052632,Lemma A.1.
E LOG,0.6469298245614035,"I(F (x); G(x)|c) ‚â§I(F (x); T (x)|c)
(28)"
E LOG,0.6491228070175439,"Proof. As F (x) and G(x) are conditionally independent given the intermediate representation T (x),
they formulate a Markov chain as follows: G ‚ÜîT ‚ÜîF . Under this relation, the following is"
E LOG,0.6513157894736842,Under review as a conference paper at ICLR 2022
E LOG,0.6535087719298246,satisÔ¨Åed:
E LOG,0.6557017543859649,"I(F (x); G(x)|c) = H(F (x)|c) ‚àíH(F (x)|G(x), c)
(29)
‚â§H(F (x)|c) ‚àíH(F (x)|T (x), G(x), c)
(30)"
E LOG,0.6578947368421053,"= H(F (x)|c) ‚àí
Z"
E LOG,0.6600877192982456,"t,f,g
p(t, f, g|c) log p(f|t, g, c)dtdfdg
(31)"
E LOG,0.6622807017543859,"= H(F (x)|c) ‚àí
Z"
E LOG,0.6644736842105263,"t,f
p(t, f|c) log p(f|t, c)dtdf
(32)"
E LOG,0.6666666666666666,"= H(F (x)|c) ‚àíH(F (x)|T (x), c)
(33)
= I(F (x); T (x)|c)
(34)"
E LOG,0.668859649122807,"Eq. 30 is from the property of conditional entropy, and Eq. 32 is due to the conditional independence
and marginalization of g."
E LOG,0.6710526315789473,"From Lemma A.1 and Eq. 24, we can prove the Proposition 4.3."
E LOG,0.6732456140350878,Proof.
E LOG,0.6754385964912281,"log(2K ‚àí1) ‚àíLself-s(x; {F , G}, K)
(35)
‚â§I(F (x); G(x)|c)
(36)
‚â§I(F (x); T (x)|c)
(37)
= I(F (x); T (x), c) ‚àíI(F (x); c)
(38)
= I(F (x); c|T (x)) ‚àíI(F (x); c)
|
{z
}
(‚ñ†)"
E LOG,0.6776315789473685,"+ I(F (x); T (x))
|
{z
}
(‚ñ°) (39)"
E LOG,0.6798245614035088,"Strictly speaking, SelfCon-S loss does not guarantee the lower bound of either (‚ñ†) or (‚ñ°) in Eq. 39.
However, SelfCon-S loss guarantees the label-conditional MI between the intermediate and the last
feature, which is (‚ñ†+ ‚ñ°)."
E LOG,0.6820175438596491,"A.4
CONNECTION BETWEEN SELFCON LOSS AND CLASSIFICATION PERFORMANCE"
E LOG,0.6842105263157895,"We used the variational inference to prove that SelfCon loss is the lower bound of label-conditional MI
between the intermediate and the last features; thus, we assumed a probabilistic model as Eq. 8. When
the anchor feature is similar to the negative pairs (i.e., different class representations, z2:(2K‚àí1)),
this model becomes a distribution with random mapping, and SelfCon loss cannot be optimized.
Therefore, representations of other classes should be farther to decrease the gap between SelfCon loss
and MI. After all, SelfCon loss has improved performance because it aims to increase the lower
bound of the label-conditional MI between F and G while increasing the distinction between
different class representations."
E LOG,0.6864035087719298,"B
IMPLEMENTATION DETAILS"
E LOG,0.6885964912280702,"Network architectures
We modiÔ¨Åed the architecture of networks according to the benchmarks.
For smaller scale of benchmarks (e.g., CIFAR-10, CIFAR-100, and Tiny-ImageNet) and the residual
networks (e.g., ResNet-18, ResNet-50, and WRN-16-8), we changed the kernel size and stride
of a convolution head to 3 and 1, respectively. We also excluded Max-Pooling on the top of the
ResNet architecture for the CIFAR datasets. Moreover, for VGG-16 with BN, the dimension of the
fully-connected layer was changed from 4096 to 512 for CIFAR and Tiny-ImageNet. MLP projection
head for contrastive learning consisted of two convolution layers with 128 dimensions and one ReLU
activation. For the architectures of sub-networks, refer to Appendix C."
E LOG,0.6907894736842105,Under review as a conference paper at ICLR 2022
E LOG,0.6929824561403509,"Representation learning
We refer to the technical improvements used in SupCon, i.e., a cosine
learning rate scheduler (Loshchilov & Hutter, 2016), an MLP projection head (Chen et al., 2020),
and the augmentation strategies (Cubuk et al., 2019): {ResizedCrop, HorizontalFlip, ColorJitter,
GrayScale}. ColorJitter and GrayScale are only used in the pretraining stage. We used 8 RTX 2080
Ti GPUs and set the batch size to 1024 for the pretraining and 512 for the linear evaluation. We used
the batch size of 512 for ResNet-18 and 256 for ResNet-50 when pretraining on the ImageNet-100
dataset. We trained the encoder and the linear classiÔ¨Åer for 400 epochs and 40 epochs, respectively,
in ImageNet-100 benchmark. Meanwhile, we trained 1000 epochs and 100 epochs, respectively, in
all other benchmarks."
E LOG,0.6951754385964912,"Every experiment used SGD with 0.9 momentum and weight decay of 1e-4 without Nesterov
momentum. All contrastive loss functions used temperature œÑ of 0.1. For a fair comparison to (Khosla
et al., 2020), we set the same learning rate of the encoder network as 0.5. Then, we linearly scaled the
learning rate according to the batch size (Goyal et al., 2017). For ImageNet-100 dataset, we used the
small learning rate of 0.0625. We used 5.0 as a learning rate of the linear classiÔ¨Åer for the residual
architecture, but it was robust to any value and converged in nearly 20 epochs. Meanwhile, for VGG
architecture, only a small learning rate of 0.1 converged."
-STAGE TRAINING,0.6973684210526315,"1-stage training
In the 1-stage training protocol, we trained the encoder network jointly with a
linear classiÔ¨Åer on the single-viewed batch. Most of the experimental settings were same as those of
representation learning, but we trained the encoder for 500 epochs on CIFAR and Tiny-ImageNet
dataset. We used the batch size of 512 and the learning rate of 0.8. For the cross-entropy result
of ImageNet-100 dataset, we trained for 90 epochs with learning rate decay of 0.1 after 30 and 60
epochs. Here, we used the batch size 512 and the learning rate 0.2."
-STAGE TRAINING,0.6995614035087719,"In the multi-exit framework, we used linear combination of loss functions for the backbone and
sub-network. We used only cross-entropy loss for the backbone network, and weighted linear
combinations of various loss functions (e.g., KL divergence and SelfCon-S) for the sub-network.
Self-Distillation (Zhang et al., 2019a) used the interpolation coefÔ¨Åcient Œ± of 0.5. For the 1-stage
version of SelfCon loss, we follow the coefÔ¨Åcient form in (Tian et al., 2019b): L = LCE + Œ≤Lself.
We set the coefÔ¨Åcient Œ≤ = 1.0 for ResNet-18 and Œ≤ = 0.8 for ResNet-50. Note that we used the
outputs from the projection head instead of the logits. We used temperature œÑ = 3.0 for SD and
œÑ = 0.1 for SelfCon loss."
-STAGE TRAINING,0.7017543859649122,"C
ABLATION STUDY ON SUB-NETWORK"
-STAGE TRAINING,0.7039473684210527,"The structure, position, and the number of sub-networks are important to the performance of SelfCon
learning. First, in order to Ô¨Ånd a suitable structure of the sub-network, the following three structures
were attached after the 2nd block of an encoder: (1) a simple fc, fully-connected, layer, (2) small
structure which reduced by half the number of layers in the non-sharing blocks, (3) same structure
which is same as the backbone‚Äôs non-sharing block structure. After we found the optimal structure,
we Ô¨Åxed the structure of sub-network and found which position is the best. For ResNet architectures,
there are three positions to attach; after the 1st, 2nd, and 3rd block. For VGG-16 with BN, there are
four positions and for WRN-16-8, there are two positions possible. Note that blocks are divided
based on the Max-Pooling layer in VGG-16 with BN."
-STAGE TRAINING,0.706140350877193,"Table 8 presents the ablation study results for ResNet-18 and ResNet-50, and Table 9 presents the
results for WRN-16-8 and VGG-16 with BN. We highlighted the selected structure and position in
Table 8 and Table 9. For the ImageNet-100 dataset, we used same structure positioned after the 2nd
block of ResNet-18."
-STAGE TRAINING,0.7083333333333334,"Obviously, there are many combinations of placing sub-networks, and Table 8 and Table 9 presented
an interesting result that some performance was the best when sub-networks are attached to all
blocks. It seems that increasing the number of positive and negative pairs by the various views
improves the performance. It is consistent with the argument of CMC (Tian et al., 2019a) that the
more views, the better the representation, but our SelfCon learning is much more efÔ¨Åcient in terms of
the computational cost and GPU memory usage. However, for the efÔ¨Åciency of the experiments and
better understanding of the framework, we stuck to a single sub-network in all experimental settings."
-STAGE TRAINING,0.7105263157894737,Under review as a conference paper at ICLR 2022
-STAGE TRAINING,0.7127192982456141,"Table 8: The results of SelfCon-S loss according to the structure and position of sub-network.
The classiÔ¨Åcation accuracy is for ResNet-18 (Left) and ResNet-50 (Right) on the CIFAR-100
benchmark."
-STAGE TRAINING,0.7149122807017544,"Position
Structure
1st Block
2nd Block
3rd Block
Accuracy"
-STAGE TRAINING,0.7171052631578947,"FC

75.3¬±0.1
Small

74.7¬±0.2
Same

74.5¬±0.0
FC

73.2¬±0.2
FC

75.4¬±0.1
FC

75.5¬±0.1
FC



74.5¬±0.1"
-STAGE TRAINING,0.7192982456140351,"Position
Structure
1st Block
2nd Block
3rd Block
Accuracy"
-STAGE TRAINING,0.7214912280701754,"FC

78.5¬±0.3
Small

78.1¬±0.2
Same

77.4¬±0.2
FC

77.0¬±0.2
FC

78.5¬±0.3
FC

77.4¬±0.1
FC



78.7¬±0.5"
-STAGE TRAINING,0.7236842105263158,"Table 9: The results of SelfCon-S loss according to the structure and position of sub-network.
The classiÔ¨Åcation accuracy is for WRN-16-8 (Left) and VGG-16 with BN (Right) on the CIFAR-100
benchmark."
-STAGE TRAINING,0.7258771929824561,"Position
Structure
1st Block
2nd Block
Accuracy"
-STAGE TRAINING,0.7280701754385965,"FC

74.4¬±1.2
Small

76.2¬±0.0
Same

76.6¬±0.1
Same

76.6¬±0.1
Same

76.5¬±0.2
Same


76.5¬±0.0"
-STAGE TRAINING,0.7302631578947368,"Position
Structure
1st Block
2nd Block
3rd Block
4th Block
Accuracy"
-STAGE TRAINING,0.7324561403508771,"FC

71.4¬±0.0
Small

71.5¬±0.4
Same

71.5¬±0.3
FC

70.9¬±0.1
FC

71.4¬±0.0
FC

72.0¬±0.0
FC

71.5¬±0.1
FC




72.5¬±0.1"
-STAGE TRAINING,0.7346491228070176,"D
EXTENSIONS OF SELFCON LEARNING"
-STAGE TRAINING,0.7368421052631579,"D.1
SELFCON IN UNSUPERVISED LEARNING"
-STAGE TRAINING,0.7390350877192983,"Although we have experimented only in the supervision, our motivation of contrastive learning with
a multi-exit framework can also be extended to unsupervised learning. We propose a SelfCon loss
function for the unsupervised scenario and present the linear evaluation performance of ResNet-18
architecture on CIFAR-100 dataset."
-STAGE TRAINING,0.7412280701754386,"Loss function
Under the unsupervised setting, Chen et al. (2020) proposed a simple framework
for contrastive learning of visual representations (SimCLR) with NT-Xent loss. SimCLR suggests a
contrastive task that contrasts the augmented pair among other images. Therefore, the objective of
SimCLR is exactly same as Eq. 1, while each sample has only one positive pair of its own augmented
image, i.e., P ‚â°{(i + B) mod 2B}. We denote this loss as Lsim."
-STAGE TRAINING,0.743421052631579,"We fomulate SelfCon loss in unsupervised setting as SimCLR, using a positive set without label
information. We formulate SelfCon loss with the multi-viewed and unlabeled batch (SelfCon-MU)
as follows:"
-STAGE TRAINING,0.7456140350877193,"Lself-mu = ‚àí
X œâ,œâ1 X i,p1"
-STAGE TRAINING,0.7478070175438597,"log
exp(œâ(xi) ‚Ä¢ œâ1(xp1)/œÑ)
P œâ2  P p2"
-STAGE TRAINING,0.75,exp(œâ(xi) ‚Ä¢ œâ2(xp2)/œÑ) + P n
-STAGE TRAINING,0.7521929824561403,"exp(œâ(xi) ‚Ä¢ œâ2(xn)/œÑ)
 (40)"
-STAGE TRAINING,0.7543859649122807,"i ‚ààI ‚â°{1, . . . , 2B}
p‚àó‚ààPi‚àó‚â°{i, (i + B) mod 2B}
n ‚ààNi ‚â°I \ Pi2"
-STAGE TRAINING,0.756578947368421,"As SelfCon loss in supervised setting, we exclude anchor sample from Pi‚àóand Ni to avoid contrasting
the same feature, i.e., Pi‚àó‚ÜêPi‚àó\ {i} when œâ = œâ‚àóand Ni ‚ÜêNi \ {i} when œâ = œâ2. Here, we
used œÑ = 0.5 for the unsupervised SelfCon loss and Lsim."
-STAGE TRAINING,0.7587719298245614,Under review as a conference paper at ICLR 2022
-STAGE TRAINING,0.7609649122807017,"Table 10: The results under the unsupervised scenario. We compared our SelfCon-MU and
SelfCon-SU loss with SimCLR in the unsupervised setting. For the comparison with supervised
learning, we also added the classiÔ¨Åcation accuracy of CE loss. We used ResNet-18 encoder and
CIFAR-100 dataset. Accuracy* denotes the accuracy of SelfCon learning with the anchors only from
the sub-network (see details in Appendix D.2)."
-STAGE TRAINING,0.7631578947368421,"Method
CE
SimCLR
SelfCon-MU
SelfCon-SU"
-STAGE TRAINING,0.7653508771929824,"Multi-view
-


"
-STAGE TRAINING,0.7675438596491229,"Accuracy
72.9¬±0.1
63.3¬±0.3
5.0¬±0.1
6.4¬±0.2
Accuracy*
-
-
64.6¬±0.1
12.8¬±0.1"
-STAGE TRAINING,0.7697368421052632,We also formulate SelfCon loss with the single-viewed and unlabeled batch (SelfCon-SU) as follows:
-STAGE TRAINING,0.7719298245614035,"Lself-su = ‚àí
X œâ,œâ1 X i,p1"
-STAGE TRAINING,0.7741228070175439,"log
exp(œâ(xi) ‚Ä¢ œâ1(xp1)/œÑ)
P œâ2  P p2"
-STAGE TRAINING,0.7763157894736842,exp(œâ(xi) ‚Ä¢ œâ2(xp2)/œÑ) + P n
-STAGE TRAINING,0.7785087719298246,"exp(œâ(xi) ‚Ä¢ œâ2(xn)/œÑ)
 (41)"
-STAGE TRAINING,0.7807017543859649,"i ‚ààI ‚â°{1, . . . , B}
p‚àó‚ààPi‚àó‚â°{i}
n ‚ààNi ‚â°I \ Pi2"
-STAGE TRAINING,0.7828947368421053,"Similarly, Ni ‚ÜêNi \ {i} when œâ = œâ2. For the positive set, since this loss is based on the
single-viewed batch, we have an empty positive set when œâ = œâ‚àó."
-STAGE TRAINING,0.7850877192982456,"Experimental results
All implementation details for unsupervised representation learning are
identical with those of supervised representation learning in Appendix B, except for temperature œÑ of
0.5 and linear evaluation learning rate of 1.0. We used a small sub-network attached after the 2nd
block. Table 10 shows the linear evaluation performance of unsupervised learning on ResNet-18
in CIFAR-100 dataset. However, we empirically found that the encoder failed to converge with
SelfCon-MU and SelfCon-SU loss."
-STAGE TRAINING,0.7872807017543859,"D.2
SELFCON WITH ANCHORS ONLY FROM THE SUB-NETWORK"
-STAGE TRAINING,0.7894736842105263,"We suspect that Eq. 40 and 41 allow the backbone network to follow the sub-network, which
makes the last feature learn more redundant information about the input variable, without any label
information. Thus, unsupervised loss function under the SelfCon framework needs to modiÔ¨Åed."
-STAGE TRAINING,0.7916666666666666,"When the anchor feature is from the backbone network, we remove the loss term which contrasts
the features of sub-network. Strictly speaking, it does not perfectly prevent the backbone from
following the sub-network, since there is no stop-gradient operation on the outputs of backbone
network when the outputs of sub-network are the anchors. However, we hypothesize that it helps
prevent the encoder from collapsing to the trivial solution by the contradiction of IB principle. We
conÔ¨Årmed the performance of revised loss functions in both unsupervised and supervised scenarios."
-STAGE TRAINING,0.793859649122807,Loss function
-STAGE TRAINING,0.7960526315789473,"Lself-mu* = Lsim ‚àíŒ±
 X œâ1 X i,p1"
-STAGE TRAINING,0.7982456140350878,"log
exp(G(xi) ‚Ä¢ œâ1(xp1)/œÑ)
P œâ2   P p2"
-STAGE TRAINING,0.8004385964912281,exp(G(xi) ‚Ä¢ œâ2(xp2)/œÑ) + P n
-STAGE TRAINING,0.8026315789473685,"exp(G(xi) ‚Ä¢ œâ2(xn)/œÑ)

 (42)"
-STAGE TRAINING,0.8048245614035088,"i ‚ààI ‚â°{1, . . . , 2B}
p‚àó‚ààPi‚àó‚â°{i, (i + B) mod 2B}
n ‚ààNi ‚â°I \ Pi2"
-STAGE TRAINING,0.8070175438596491,"We exclude anchor sample from Pi‚àóand Ni, and all notations are same as Eq. 40, except for the
coefÔ¨Åcient Œ± where we used 1.0. For the supervised setting, simply change Pi‚àóto {p ‚ààI|yp = yi}
and Lsim to Lsup. Note that Pi‚àó‚ÜêPi‚àó\ {i} when œâ‚àó= G and Ni ‚ÜêNi \ {i} when œâ2 = G. We
get rid of the situation that the anchor F (x) contrasts the positive pair in sub-network G(xp‚àó). Still,"
-STAGE TRAINING,0.8092105263157895,Under review as a conference paper at ICLR 2022
-STAGE TRAINING,0.8114035087719298,"Table 11: CIFAR-100 results with SelfCon extensions. Accuracy* denotes the accuracy of SelfCon
learning with the anchors only from the sub-network."
-STAGE TRAINING,0.8135964912280702,"Method
Architecture
Accuracy
Accuracy*"
-STAGE TRAINING,0.8157894736842105,"SelfCon-M
ResNet-18
74.9¬±0.1
74.9¬±0.2
SelfCon-S
75.4¬±0.1
75.6¬±0.1
SelfCon-M
ResNet-50
76.9¬±0.1
77.7¬±0.4
SelfCon-S
78.5¬±0.3
78.8¬±0.1"
-STAGE TRAINING,0.8179824561403509,"Table 12: The detailed results of mutual information estimation. x, y, T (x), and F (x) respec-
tively denotes the input variable, label variable, intermediate feature, and the last feature. Recall that
T (x) is the intermediate feature of the backbone network, which is an input to the auxiliary network
path. We summarized the average of estimated MI through multiple random seeds. We highlighted
the MI between the intermediate and the last features, which is the main concern of the SelfCon loss.
Bold type indicates the smallest values for I(x; T (x)) and the largest values for I(y; T (x)) and
I(F (x); T (x)), according to the IB principle."
-STAGE TRAINING,0.8201754385964912,"Estimator
MI
CE
SupCon
SelfCon-M
SelfCon-S"
-STAGE TRAINING,0.8223684210526315,"I(x; T (x))
0.509
0.344
0.231
0.236
I(y; T (x))
0.214
0.235
0.469
0.479
InfoNCE (Oord et al., 2018)"
-STAGE TRAINING,0.8245614035087719,"I(F (x); T (x))
0.288
0.303
0.530
0.553
I(x; T (x))
1.639
0.898
0.558
0.554
I(y; T (x))
0.398
0.627
1.486
1.680
MINE (Belghazi et al., 2018)"
-STAGE TRAINING,0.8267543859649122,"I(F (x); T (x))
0.734
0.762
1.726
2.123
I(x; T (x))
1.485
0.930
0.601
0.589
I(y; T (x))
0.390
0.546
1.450
1.389
NWJ (Nguyen et al., 2010)"
-STAGE TRAINING,0.8289473684210527,"I(F (x); T (x))
0.655
0.799
1.820
1.873"
-STAGE TRAINING,0.831140350877193,"the proposed loss function includes SimCLR loss (Lsim) or SupCon loss (Lsup) in the unsupervised
or supervised setting, respectively."
-STAGE TRAINING,0.8333333333333334,"Lself-su* = ‚àí
X i,p1"
-STAGE TRAINING,0.8355263157894737,"log
exp(G(xi) ‚Ä¢ F (xp1)/œÑ)
P p2"
-STAGE TRAINING,0.8377192982456141,exp(G(xi) ‚Ä¢ F (xp2)/œÑ) + P œâ2 P n
-STAGE TRAINING,0.8399122807017544,"exp(G(xi) ‚Ä¢ œâ2(xn)/œÑ)
(43)"
-STAGE TRAINING,0.8421052631578947,"i ‚ààI ‚â°{1, . . . , B}
p‚àó‚ààPi‚àó‚â°{i}
n ‚ààNi ‚â°I \ Pi2"
-STAGE TRAINING,0.8442982456140351,"For the supervised setting, we change the above equation as the equally-weighted linear combination
of SupCon-S loss and Eq. 43 with Pi‚àó‚â°{p ‚ààI|yp = yi}. Note that we also exclude contrasting the
anchor itself in SupCon-S loss term."
-STAGE TRAINING,0.8464912280701754,"Experimental results
In Table 10, we also reported the accuracy of SelfCon-MU and SelfCon-SU
loss according to Eq. 42 and 43. Surprisingly, in this case, SelfCon-MU outperformed SimCLR loss
(Chen et al., 2020), improving 1.3%p. Unfortunately, SelfCon-SU had not converged again, although
it improved the result in a small amount compared to Eq. 41. While SelfCon-MU has SimCLR loss
term which makes the backbone encoder still learn meaningful features, SelfCon-SU loss does not
have the anchor features from the backbone, which makes the backbone hard to be trained. Table
11 summarizes SelfCon-M and SelfCon-S loss, removing the anchors from the backbone in the
supervised setting, i.e., supervised version of Eq. 42 and 43. As we expected, these variants of
SelfCon-M and SelfCon-S further improved the classiÔ¨Åcation performance."
-STAGE TRAINING,0.8486842105263158,"E
DETAILS OF MI ESTIMATION"
-STAGE TRAINING,0.8508771929824561,"Table 12 summarizes the full details of mutual information estimation, measured on CIFAR-100
with the pretrained ResNet-50 encoders. Note that we considered the encoder output without the
projection head. We used three types of estimators: InfoNCE (Oord et al., 2018), MINE (Belghazi
et al., 2018), and NWJ (Nguyen et al., 2010). As expected, SelfCon-M and SelfCon-S loss exhibited"
-STAGE TRAINING,0.8530701754385965,Under review as a conference paper at ICLR 2022
-STAGE TRAINING,0.8552631578947368,"Figure 7: Qualitative examples for mitigating vanishing gradient. Along with the original image,
we visualized the gradient when training with SupCon (Left) and SelfCon-M loss (Right). Note that
all the gradients are from the same model checkpoint of ResNet-18."
-STAGE TRAINING,0.8574561403508771,"larger MI between the intermediate and the last feature of the backbone network than CE and SupCon
loss."
-STAGE TRAINING,0.8596491228070176,"F
QUALITATIVE EXAMPLES FOR VANISHING GRADIENT"
-STAGE TRAINING,0.8618421052631579,"In Figure 5, we have already showed that the sub-network solves the vanishing gradient problem
through the visualization for gradient norms of each layer. In Figure 7, we also visualized qualitative
examples using Grad-CAM (Selvaraju et al., 2017). We used the gradient measured on the last layer
in the 2nd block when the sub-network is attached after the 2nd block. In order to compare the absolute
magnitude of the gradient, it is normalized by the maximum and minimum values of the two methods,
SelfCon-M and SupCon. As in Figure 7, SelfCon learning led to a larger gradient via sub-networks,
and more clearly Grad-CAM highlighted the pixels containing important information in the images."
-STAGE TRAINING,0.8640350877192983,"G
IMAGENET RESULTS"
-STAGE TRAINING,0.8662280701754386,"Table 13: The classiÔ¨Åcation accuracy on ResNet-
18 for ImageNet."
-STAGE TRAINING,0.868421052631579,"Method
B
Epoch
Acc@1 / Acc@5
CE
256
90
69.9 / 89.4
SupCon
1024
400
70.9 / 89.6
SupCon-S
1024
400
69.2 / 89.2
SelfCon-M
1024
400
71.2 / 90.1
SelfCon-S
1024
400
70.3 / 89.6
SupCon
1024
800
71.7 / 90.3
SupCon-S
1024
800
69.7 / 89.6
SelfCon-M
1024
800
71.9 / 90.4
SelfCon-S
1024
800
70.8 / 89.9
SupCon
2048
800
71.9 / 90.3
SelfCon-S
2048
800
72.0 / 90.4"
-STAGE TRAINING,0.8706140350877193,"We experimented with the full-scale ImageNet
benchmark to enhance the reliability of Self-
Con learning on the large-scale dataset. Due to
the limited computational resources, we used
ResNet-18 architecture for both the pretraining
and linear evaluation. For the pretraining stage,
we experimented with various batch sizes and
epochs. Besides, we tuned the learning rate as
0.125 for 1024 batch size and 0.25 for 2048
batch size. In the linear evaluation stage, we
trained the linear classiÔ¨Åer with a batch size of
256, 40 epochs, and a learning rate of 6.0. Other
hyperparameters are the same as Appendix B.
For the cross-entropy result, we used the batch
size of 256 and the learning rate of 0.1. The
settings of epochs and the learning rate rule are
the same as the ImageNet-100 experiments."
-STAGE TRAINING,0.8728070175438597,"We summarized the experimental results in Table 13. For the experiment with 1024 batch size
and 400 epochs, it is still consistent that SelfCon-M showed better performance than SupCon, but
the multi-viewed methods outperformed the single-viewed counterparts. We consider the possible
reasons as follows. (1) Under-Ô¨Åtting issue due to the tremendous number of samples and relatively
small size of architecture. (2) InsufÔ¨Åcient number of training epochs (also refer to Figure 4). (3)
Relatively small batch size with respect to the number of classes (1000) in ImageNet (also refer
to Table 4). Therefore, we did two more ablation studies: (1) pretraining with 800 epochs and (2)
pretraining with a larger batch size (B = 2048)."
-STAGE TRAINING,0.875,"In the experiments with 800 epochs, the trend that multi-viewed methods outperformed the single-
viewed counterparts did not change. However, the classiÔ¨Åcation accuracy of each algorithm signiÔ¨Å-
cantly increased as the pretraining epochs became longer. On the other hand, for the larger batch"
-STAGE TRAINING,0.8771929824561403,Under review as a conference paper at ICLR 2022
-STAGE TRAINING,0.8793859649122807,"Table 14: Memory (GiB / GPU) and computation time (sec / step) comparison. All numbers are
measured with ResNet-50 training on 8 RTX 2080 Ti GPUs and Intel i9-10940X CPU. Note that
FLOPS is for one sample. B stands for batch size."
-STAGE TRAINING,0.881578947368421,"Dataset (Image size)
Method
Params
FLOPS
B = 256
B = 512
B = 1024"
-STAGE TRAINING,0.8837719298245614,"Memory
Time
Memory
Time
Memory
Time"
-STAGE TRAINING,0.8859649122807017,"CIFAR-100 (32x32)
SupCon
27.96 M
2.62 G
4.00
0.11
6.40
0.14
11.29
0.20
SelfCon-S
33.47 M
1.31 G
2.73
0.11
3.92
0.12
6.28
0.16"
-STAGE TRAINING,0.8881578947368421,"Tiny-ImageNet (64x64)
SupCon
27.96 M
2.63 G
4.41
0.21
6.71
0.26
11.84
0.36
SelfCon-S
33.47 M
1.32 G
2.98
0.21
4.21
0.23
6.82
0.27"
-STAGE TRAINING,0.8903508771929824,"size (i.e., B = 2048), we observed that SelfCon-S is better than SupCon. The difference seems to be
marginal, but it is noteworthy that the training time for SelfCon-S is more than twice shorter than
SupCon, and memory consumption is much lower. It is an important result in terms of the recent
research Ô¨Çow for the compression in contrastive learning (Koohpayegani et al., 2020; Fang et al.,
2021). From the result of SelfCon-S in the 2048 batch size, we are highly convinced that the trend
in ImageNet is the same as the other benchmarks. The experiments for the larger architecture (e.g.,
ResNet-50, ResNet-101) or the larger batch size (e.g., 4096, 6144) are left for the future work."
-STAGE TRAINING,0.8925438596491229,"H
MEMORY USAGE AND COMPUTATIONAL COST"
-STAGE TRAINING,0.8947368421052632,"In Table 14, we reported the computational cost for pretraining with ResNet-50. ImageNet-100
dataset result is not reported because ResNet-50 with batch size over 256 exceed the GPU limit. The
overall trend is similar to that in Table 3."
-STAGE TRAINING,0.8969298245614035,"I
SENSITIVITY STUDY FOR LEARNING RATE"
-STAGE TRAINING,0.8991228070175439,"Table 15: CIFAR-100 results on ResNet-
18 with various learning rates. Bold type
is for the best accuracy within each method."
-STAGE TRAINING,0.9013157894736842,Learning Rate
-STAGE TRAINING,0.9035087719298246,"Method
0.1
0.5
1.0
1.5"
-STAGE TRAINING,0.9057017543859649,"SupCon
71.4
73.0
73.4
73.7
SupCon-S
72.1
73.9
74.6
75.0
SelfCon-M
72.9
74.9
75.4
75.8
SelfCon-S
73.7
75.4
75.7
76.2"
-STAGE TRAINING,0.9078947368421053,"In Table 4, we experimented with the supervised con-
trastive algorithms with various batch sizes and con-
Ô¨Årmed that the classiÔ¨Åcation accuracy decreases in the
large batch size. We supposed that this trend is induced
by the regularization effect from the batch size. How-
ever, there could be a concern for using a sub-optimal
learning rate on the large batch size. We further studied
the sensitivity for the learning rate in a batch size of
1024 and summarized the results in Table 15."
-STAGE TRAINING,0.9100877192982456,"We concluded that the performance comparison in Ta-
ble 4 is consistent with hyperparameter tuning. The
experimental results supported that a larger learning rate than 0.5 may be a better choice but the trend
between all methods maintained in parallel with the learning rate of 0.5. Therefore, we stick to the
initial learning rate of 0.5 that Khosla et al. (2020) had used."
-STAGE TRAINING,0.9122807017543859,"J
ABLATION STUDY FOR DIFFERENT AUGMENTATION POLICIES"
-STAGE TRAINING,0.9144736842105263,"Table 16: CIFAR-100 results on ResNet-
18 with various augmentation policies."
-STAGE TRAINING,0.9166666666666666,Augmentation Policy
-STAGE TRAINING,0.918859649122807,"Method
Optimal
Standard
Simple"
-STAGE TRAINING,0.9210526315789473,"SupCon
74.3¬±0.1
73.0¬±0.0
72.0¬±0.3
SelfCon-S
72.5¬±0.0
75.4¬±0.1
74.2¬±0.2"
-STAGE TRAINING,0.9232456140350878,"We could think of the optimal scenario where we have
prior domain knowledge about a good augmentation
policy. Also, there could be the opposite scenario where
we do not know an optimal policy and just use the basic
augmentations. Hence, we investigated three different
augmentation policies as follows and compared the
performances of SupCon and SelfCon-S under these
scenarios."
-STAGE TRAINING,0.9254385964912281,"‚Ä¢ Optimal : We used RandAugment (Cubuk et al., 2020) for an optimal augmentation policy.
RandAugment randomly samples N out of 14 transformation choices (e.g., shear, translate, auto-
Contrast, and posterize) with M magnitude parameter. We used the optimized value of N = 2 and
M = 9 in (Cubuk et al., 2020). It is also known that SupCon performs better with RandAugment
policy (Khosla et al., 2020)."
-STAGE TRAINING,0.9276315789473685,Under review as a conference paper at ICLR 2022
-STAGE TRAINING,0.9298245614035088,"InfoNCE
   MINE
   NWJ"
-STAGE TRAINING,0.9320175438596491,"MI Estimator
Test Acc."
-STAGE TRAINING,0.9342105263157895,Test Accuracy (%) 70 71 72 73 74 75 76 77
-STAGE TRAINING,0.9364035087719298,Mutual Information I(F(x);T(x)) 0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0
-STAGE TRAINING,0.9385964912280702,SupCon
-STAGE TRAINING,0.9407894736842105,SelfCon-M*0.025
-STAGE TRAINING,0.9429824561403509,SelfCon-M*0.05
-STAGE TRAINING,0.9451754385964912,SelfCon-M*0.075
-STAGE TRAINING,0.9473684210526315,SelfCon-M*0.1
-STAGE TRAINING,0.9495614035087719,SelfCon-M*0.15
-STAGE TRAINING,0.9517543859649122,SelfCon-M
-STAGE TRAINING,0.9539473684210527,"Figure 8: Test accuracy and the estimated mutual information of different methods. SelfCon-
M*Œ± denotes SelfCon-M* loss with hyperparameter Œ±. We used ResNet-18 on CIFAR-100 dataset
for the measurements."
-STAGE TRAINING,0.956140350877193,"‚Ä¢ Standard: For standard augmentation, we used {RandomResizedCrop, RandomHorizontalFlip,
RandomColorJitter, RandomGrayscale}. This is the same basic policy we used in the paper."
-STAGE TRAINING,0.9583333333333334,"‚Ä¢ Simple: When we do not have domain knowledge, it might be difÔ¨Åcult to choose the appropriate
augmentation policies. We assumed a scenario where we might not know that the color would be
important in this visual recognition task. Therefore, we removed the color-related augmentation
policies from Standard policy, i.e., we only used {RandomResizedCrop, RandomHorizontalFlip}
for a simple augmentation policy."
-STAGE TRAINING,0.9605263157894737,"The results are presented in Table 16. Interestingly, when we apply Optimal augmentations, SupCon
outperformed SelfCon-S, but for Standard and Simple augmentations, SelfCon-S outperformed
SupCon. SupCon with the multi-viewed batch can beneÔ¨Åt more from the strong and optimized
augmentation policy because training each sample twice more encouraged the memorization. Mean-
while, SelfCon learning did not work well with RandAugment (Cubuk et al., 2020), as SupCon
degraded with the Stacked RandAugment (Tian et al., 2020) in their experiments. There would be
an optimal policy for SelfCon learning but, Ô¨Ånding an optimal policy, such as with RandAugment
or AutoAugment (Cubuk et al., 2019), is not a trivial process and needs a lot of computational cost.
SelfCon learning can relieve this concern and is more robust to the simple augmentation (i.e., weak
augmentation policy)."
-STAGE TRAINING,0.9627192982456141,"K
ADDITIONAL EXPERIMENT FOR CORRELATION BETWEEN SELFCON LOSS
AND MUTUAL INFORMATION"
-STAGE TRAINING,0.9649122807017544,"To clearly show the correlation between the mutual information and test accuracy, we experimented
with the interpolation between SupCon loss and SelfCon-M loss (SupCon loss is a special case of
SelfCon-M loss). However, the current formulation of Eq. 1 and Eq. 2 cannot make the exact
interpolation between SupCon and SelfCon-M because the SelfCon-M loss should have negative
pairs from different levels of a network (i.e., backbone and sub-network), but the SupCon loss cannot
produce those. Therefore, we should modify the SelfCon-M loss to a supervised version of Eq. 42.
In the supervised version of Eq. 42, we can break down the SupCon loss term and SelfCon-like loss"
-STAGE TRAINING,0.9671052631578947,Under review as a conference paper at ICLR 2022
-STAGE TRAINING,0.9692982456140351,"Table 17: The detailed results of mutual information estimation. Every notation is same as Table
12. We used ResNet-18 on CIFAR-100 dataset for the measurements."
-STAGE TRAINING,0.9714912280701754,"SupCon
SelfCon-M*
SelfCon-M*
SelfCon-M*
SelfCon-M*
SelfCon-M*
SelfCon-M"
-STAGE TRAINING,0.9736842105263158,"Estimator
MI
-
Œ± = 0.02
Œ± = 0.05
Œ± = 0.075
Œ± = 0.1
Œ± = 0.15
-"
-STAGE TRAINING,0.9758771929824561,"I(x; T (x))
0.285
0.296
0.277
0.299
0.293
0.232
0.203
I(y; T (x))
0.221
0.299
0.290
0.309
0.329
0.341
0.454
InfoNCE"
-STAGE TRAINING,0.9780701754385965,"I(F (x); T (x))
0.296
0.330
0.357
0.381
0.392
0.402
0.508
I(x; T (x))
0.758
0.843
0.719
0.744
0.665
0.697
0.508
I(y; T (x))
0.616
0.719
0.700
0.834
0.928
0.961
1.261
MINE"
-STAGE TRAINING,0.9802631578947368,"I(F (x); T (x))
0.845
0.919
0.937
1.032
1.140
1.050
1.760
I(x; T (x))
0.714
0.798
0.694
0.764
0.692
0.592
0.496
I(y; T (x))
0.467
0.594
0.641
0.808
0.799
0.843
1.287
NWJ"
-STAGE TRAINING,0.9824561403508771,"I(F (x); T (x))
0.747
0.835
0.914
1.039
1.086
1.101
1.593"
-STAGE TRAINING,0.9846491228070176,"term, with an interpolating parameter Œ±:"
-STAGE TRAINING,0.9868421052631579,"Lself-m‚àó= Lsup ‚àíŒ±
 X œâ1 X i,p1"
-STAGE TRAINING,0.9890350877192983,"log
exp(G(xi) ‚Ä¢ œâ1(xp1)/œÑ)
P œâ2   P p2"
-STAGE TRAINING,0.9912280701754386,exp(G(xi) ‚Ä¢ œâ2(xp2)/œÑ) + P n
-STAGE TRAINING,0.993421052631579,"exp(G(xi) ‚Ä¢ œâ2(xn)/œÑ)

 (44)"
-STAGE TRAINING,0.9956140350877193,"i ‚ààI ‚â°{1, . . . , 2B}
p‚àó‚ààPi‚àó‚â°{p ‚ààI|yp = yi}
n ‚ààNi ‚â°I \ Pi2"
-STAGE TRAINING,0.9978070175438597,"where Pi‚àó‚ÜêPi‚àó\ {i} when œâ‚àó= G and Ni ‚ÜêNi \ {i} when œâ2 = G. Therefore, if Œ± = 0,
Lself-m‚àóis equivalent to the SupCon loss and if Œ± = 1, Lself-m‚àóis almost equivalent to SelfCon-M
loss. Figure 8 describes the estimated mutual information and its relationship with classiÔ¨Åcation
performance via controlling the hyperparameter Œ±, and Table 17 summarizes the detailed estimation
values of the intermediate feature with respect to the input, label, and the last feature. We observed a
clear increasing trend of both MI and test accuracy as the contribution of SelfCon gets larger. Also,
the detailed MI estimation values in Table 17 imply the same interpretation as the IB principle and
Appendix E."
