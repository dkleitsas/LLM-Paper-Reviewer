Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002457002457002457,"We present a novel framework for Distributing Black-Box Optimization (DiBB).
DiBB can encapsulate any Black Box Optimization (BBO) method, making it of
particular interest for scaling and distributing modern Evolution Strategies (ES),
such as CMA-ES and its variants, which maintain a sampling covariance matrix
throughout the run. Due to high algorithmic complexity however, such methods
are unsuitable alone to address high-dimensional problems, e.g. for sophisticated
Reinforcement Learning (RL) control. This limits the applicable methods to simpler
ES, which trade off faster updates for lowered sample efﬁciency. DiBB overcomes
this limitation by means of problem decomposition, leveraging expert knowledge
in the problem structure such as a known topology for a neural network controller.
This allows to distribute the workload across an arbitrary number of nodes in a
cluster, while maintaining the feasibility of second order (covariance) learning on
high-dimensional problems. The computational complexity per node is bounded
by the (arbitrary) size of blocks of variables, which is independent of the problem
size."
INTRODUCTION,0.004914004914004914,"1
INTRODUCTION"
INTRODUCTION,0.007371007371007371,"Black Box Optimization (BBO) can be applied, by deﬁnition, to any problem independent of the
speciﬁc application (Audet & Hare, 2017). In principle, this provides a method that is applicable to
problems yet unsolved by the current state of the art. The most obvious catch lies in their compu-
tational requirements, which usually takes one of two forms: (i) simple black-box solvers suffers
from high sample complexity (i.e., they are data hungry), in exchange for smaller computational
requirements (both in memory and CPU time). On the other hand, (ii) sophisticated solvers such
as modern Evolution Strategies (ES; Hansen & Ostermeier 2001; Wierstra et al. 2014) have a high
internal computational cost per sample, in exchange for a wide set of properties (see Section 2) that
make for signiﬁcantly improved sample efﬁciency."
INTRODUCTION,0.009828009828009828,"In the ﬁrst case (simpler ES), the requirement for more samples can be mitigated—to some extent—
through the embarrassingly parallel evaluation of a large population of candidate solutions. This
approach however has its own limitations, as traversing a more complex ﬁtness landscape often
requires a disproportionate growth in the number of required samples as the dimensionality rises. In
the second case however (complex ES), algorithms relying on covariance matrix adaptation (CMA;
e.g. CMA-ES by Hansen 1996) have a quadratic complexity in the number of variables for processing
a sample. This strictly limits their application (within a sensible time frame) to problems within the
tens of thousands variables. This limit has restricted the applicability of these methods to rather simple
scenarios and applications so far. As a result, most successful applications of ES to high-dimensional
problems (e.g. training larger neural networks) rely on simpler ES, such as the type (i) described
above."
INTRODUCTION,0.012285012285012284,"Most of the per-sample complexity of type (ii) algorithms can be traced to the algorithm maintaining
and updating the covariance information among the set of variables, which helps the sample efﬁciency
of these algorithms (as in convergence speed) at the cost of longer run times (as in wall-clock speed),
eventually becoming intractable as the number of variables grows. Several algorithms (Ros & Hansen,
2008; Schaul et al., 2011) thereby implement an assumption of separability between the variables,
relinquishing covariance information altogether for a greatly enhanced wall-clock speed per sample.
Unsurprisingly, this comes at a cost in terms of convergence speed: these algorithms can require
several orders of magnitude more samples to ﬁnd comparable solutions, depending on the problem
structure (Hansen & Ostermeier, 2001)."
INTRODUCTION,0.014742014742014743,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.0171990171990172,Worker node A
INTRODUCTION,0.019656019656019656,Head node
INTRODUCTION,0.022113022113022112,"Object Store 
Communication array"
INTRODUCTION,0.02457002457002457,Reference solution:
INTRODUCTION,0.02702702702702703,A B C D E F
INTRODUCTION,0.029484029484029485,"DIBB 
main routine"
INTRODUCTION,0.03194103194103194,Copy of global
INTRODUCTION,0.0343980343980344,reference
INTRODUCTION,0.036855036855036855,"solution: 
A B C D E F"
INTRODUCTION,0.03931203931203931,"BLOCK 
WORKER"
INTRODUCTION,0.04176904176904177,"Complete
solutions: 
A B X X E F 
A B Y Z E F 
A B Y Y E F"
INTRODUCTION,0.044226044226044224,"FITNESS
EVALUATOR"
INTRODUCTION,0.04668304668304668,PyBullet simulator
INTRODUCTION,0.04914004914004914,"Walker 2D
Candidate"
INTRODUCTION,0.051597051597051594,samples:
INTRODUCTION,0.05405405405405406,"X X 
Y Z 
Y Y"
INTRODUCTION,0.056511056511056514,"Local
reference"
INTRODUCTION,0.05896805896805897,solution: C D
INTRODUCTION,0.06142506142506143,BBO instance
INTRODUCTION,0.06388206388206388,Worker node B
INTRODUCTION,0.06633906633906633,Copy of global
INTRODUCTION,0.0687960687960688,reference
INTRODUCTION,0.07125307125307126,"solution: 
A B C D E F"
INTRODUCTION,0.07371007371007371,"BLOCK 
WORKER"
INTRODUCTION,0.07616707616707617,"Complete
solutions: 
A B C D P T 
A B C D S P 
A B C D R T"
INTRODUCTION,0.07862407862407862,"FITNESS
EVALUATOR"
INTRODUCTION,0.08108108108108109,PyBullet simulator
INTRODUCTION,0.08353808353808354,Walker 2D
INTRODUCTION,0.085995085995086,Candidate
INTRODUCTION,0.08845208845208845,samples:
INTRODUCTION,0.09090909090909091,"P T 
S P 
R T"
INTRODUCTION,0.09336609336609336,"Local
reference"
INTRODUCTION,0.09582309582309582,solution: E F
INTRODUCTION,0.09828009828009827,BBO instance
INTRODUCTION,0.10073710073710074,"Figure 1: Architecture of the DiBB framework. This example shows an instantiation on a cluster
with 3 nodes, one head and two workers."
INTRODUCTION,0.10319410319410319,"The approach taken in this paper takes instead into account the fact that the correlation among
variables is not uniform for most complex problems. Rather, certain groups of variables will display
higher correlation between each other, with variables having lower correlation belonging instead into
different groups. This induces two different assumptions on the variable set: groups having high
intra-correlation support an assumption of partial correlation, while the low inter-group correlation
displays the parallel argument of partial separability. We can represent this using a block-diagonal
covariance matrix, which at the same time maintains full-covariance information within the variables
belonging to one such group, and discarding the less useful (and often negligible) inter-group
correlation altogether (Cuccu & Gomez, 2012). Such an approach allows leveraging covariance
information where it is most efﬁcient, while at the same time signiﬁcantly reducing the memory
footprint of the algorithm (based on the number and size of the groups), and even offering additional
opportunities for parallelization and distributed computing. We will use the term blocks to refer to
the groups of variable with high intra-correlation in the rest of the paper."
INTRODUCTION,0.10565110565110565,"BBO algorithms are designed to ignore blocks, since they need to work in any unknowable (black-
box) environment. Even in a gray-box setting, black-box algorithm engineering can be useful to
ensure that no illegitimate assumptions are made in their design. On the other hand, we aim to
proﬁt from the added information. Real applications often have a gray-box character, exposing a
certain level of expert knowledge about the correlation between variables. Let us take for example
the common application of Neuroevolution (Stanley et al., 2019), where evolutionary algorithms
(such as ES) are applied to learn the weights of a neural network. We know from the network’s
equation that the weights of the connections entering the same neuron (as opposed to entering
different neurons) are by necessity highly correlated, as they are composed in a linear fashion inside
the neuron prior to activation. The same reasoning can be made for the weights of connections
entering neurons belonging to the same layer, versus neurons in different layers, because outputs of a
layer are linearly combined in the next layer. This principle remains true as the network expands:
in complex applications such as those requiring deep networks, the correlation between weights of
connections in the early versus late layers will be by design much more limited than in the neuron or
layer examples above1."
INTRODUCTION,0.10810810810810811,"Based on the above insights, this paper proposes a new framework for Distributed Black Box
optimization, named DiBB, see Figure 1. We leverage the assumption of partial correlation by
partitioning the variables set into highly intra-correlated blocks; then, switching the assumption to
partial separability, we search each block with a different instance of the reference BBO algorithm,
independently. This makes DiBB particularly suited for neuroevolution applications requiring
larger/deep networks. In Section 2, we provide rigorous theoretical arguments for the sample
efﬁciency of this approach."
INTRODUCTION,0.11056511056511056,"From a practical perspective, this allows to search blocks asynchronously and even to distribute each
BBO instance on different nodes in a cluster. Varying the partition of the variables enables the user to
arbitrarily trade-off between wall-clock speed (smaller blocks) and sample efﬁciency (larger blocks),
while distributing the BBO instances decouples the complexity from the number of variables, as long"
THE FUNDAMENTAL ASSUMPTION OF PARTIAL SEPARABILITY ACROSS NETWORK LAYERS HOWEVER DOES NOT SEEM TO BE,0.11302211302211303,"1The fundamental assumption of partial separability across network layers however does not seem to be
very well studied in the literature, despite a considerable body of work on neural network loss landscapes (Fort
& Jastrzebski, 2019; Soltanolkotabi et al., 2018). This can be partially tracked to the limited availability of
algorithms making use of partial correlation information."
THE FUNDAMENTAL ASSUMPTION OF PARTIAL SEPARABILITY ACROSS NETWORK LAYERS HOWEVER DOES NOT SEEM TO BE,0.11547911547911548,Under review as a conference paper at ICLR 2022
THE FUNDAMENTAL ASSUMPTION OF PARTIAL SEPARABILITY ACROSS NETWORK LAYERS HOWEVER DOES NOT SEEM TO BE,0.11793611793611794,"as enough machines are available to run the BBO instances. A major challenge comes with solution
evaluation, as the variables constituting a sample are practically scattered across a network. This is
addressed in Section 3, by establishing a rather sparse communication scheme."
THE FUNDAMENTAL ASSUMPTION OF PARTIAL SEPARABILITY ACROSS NETWORK LAYERS HOWEVER DOES NOT SEEM TO BE,0.12039312039312039,"It is important to notice at this point that no assumption or claim has yet been made on the underlying
BBO algorithm of choice—nor will be made. While this paper explores the applicability of DiBB in
the context of modern, sophisticated ESs, DiBB can be applied to any BBO algorithm with minimal
interfaces."
THE FUNDAMENTAL ASSUMPTION OF PARTIAL SEPARABILITY ACROSS NETWORK LAYERS HOWEVER DOES NOT SEEM TO BE,0.12285012285012285,"We analyze the performance of our framework on the classic COCO benchmark, both on the BBOB
and BBOB large-scale suites, using the industry-standard CMA-ES as a reference, and as a block-
level optimizer within DiBB. The sample complexity of our approach typically (and as expected) sits
in between the full-covariance and the diagonal-covariance versions of CMA-ES, with few notable
exceptions where maintaining extra covariance information is actually deceptive/misleading due to
separability. Notably, on parallel hardward, DiBB is considerably faster in terms of wall clock time.
To demonstrate the scalability of our algorithms, we present results on training a 20-layer neural
network using 20 blocks on 20 machines, the largest neural network trained with neuroevolution to
date, to the best of our knowledge."
CONTRIBUTIONS,0.12530712530712532,"1.1
CONTRIBUTIONS"
CONTRIBUTIONS,0.12776412776412777,Our key contributions are as follows:
CONTRIBUTIONS,0.13022113022113022,"• We provide a novel way of parallelizing an ES, going beyond the parallel evaluation of
candidate solutions forming a population, by optimizing blocks of variables with multiple
ES instances."
CONTRIBUTIONS,0.13267813267813267,"• We exploit the block structure to add covariance matrix information at low cost where it
matters most, namely inside blocks of potentially highly correlated variables."
CONTRIBUTIONS,0.13513513513513514,"• The resulting DiBB framework is particularly well-suited for neuroevolution. However, it
can be applied to any BBO method."
CONTRIBUTIONS,0.1375921375921376,"• We demonstrate that also large and deep networks can be trained efﬁciently with neuroevo-
lution on highly parallel hardware."
A PRIMER ON EVOLUTION STRATEGIES,0.14004914004914004,"2
A PRIMER ON EVOLUTION STRATEGIES"
A PRIMER ON EVOLUTION STRATEGIES,0.14250614250614252,"ES are direct search methods, which optimize a black-box objective function f : Rd →R by
sampling candidate points from an adaptive Gaussian distribution. We brieﬂy review the types of ES
most relevant for our discussion. The classic variant is the (1+1)-ES (Rechenberg, 1973). Its central
algorithmic mechanism is step size adaptation, i.e., its ability to actively adapt the standard deviation
σ > 0 of its Gaussian sampling distribution N(m, σ2I) to the current needs. For about 20 years,
CMA-ES (Hansen & Ostermeier, 2001) is the gold standard in ES research. Many variants exist, such
as Natural Evolution Strategies (NES; Wierstra et al. 2014). Its most important mechanism going
beyond “simple” step-size adaptive ES is covariance matrix adaptation (CMA), which means that
not only the global step size σ, but also the full covariance matrix C of the Gaussian N(m, σ2C) is
adapted to the problem at hand."
A PRIMER ON EVOLUTION STRATEGIES,0.14496314496314497,"CMA-ES is a powerful optimizer; however, it was not designed for high-dimensional applications
with hundreds of thousands of variables. Its internal parameters are not tuned with such a regime in
mind, and learning a full covariance matrix with d(d+1)"
A PRIMER ON EVOLUTION STRATEGIES,0.14742014742014742,"2
parameters is inherently slow. Such problems
are best addressed by restricting C to a diagonal matrix (Ros & Hansen, 2008), to a diagonal plus
a low-rank matrix (Loshchilov, 2014; Akimoto et al., 2014; Loshchilov et al., 2018), or to a block-
diagonal matrix (Cuccu & Gomez, 2012). The number of parameters of the covariance matrix can
hence be chosen rather ﬂexibly in the range d to d(d+1) 2
."
ES FOR NEUROEVOLUTION,0.14987714987714987,"2.1
ES FOR NEUROEVOLUTION"
ES FOR NEUROEVOLUTION,0.15233415233415235,"The application of ES to machine learning problems and to RL in particular has a long history (Igel,
2003; Heidrich-Meisner & Igel, 2009). In 2017, the work of Salimans et al. (2017) sparked a renewed"
ES FOR NEUROEVOLUTION,0.1547911547911548,Under review as a conference paper at ICLR 2022
ES FOR NEUROEVOLUTION,0.15724815724815724,"interest in ES by showcasing how to successfully exploit the embarrassing parallel nature of objective
function evaluations in populations-based algorithms, considerably speeding-up the learning process.
This triggered a large body of work on neuroevolution based on ES, see e.g. Plappert et al. (2017);
Ha & Schmidhuber (2018); Chrabaszcz et al. (2018); Stanley et al. (2019) and references therein."
CONVERGENCE RATES AND COMPUTATIONAL COMPLEXITY,0.1597051597051597,"2.2
CONVERGENCE RATES AND COMPUTATIONAL COMPLEXITY"
CONVERGENCE RATES AND COMPUTATIONAL COMPLEXITY,0.16216216216216217,"Due to Taylor’s theorem, local optima of C2 functions in d-dimensional space are well approximated
(up to O(∥x −x∗∥3)) by convex quadratic functions f(x) = 1"
CONVERGENCE RATES AND COMPUTATIONAL COMPLEXITY,0.16461916461916462,"2(x −x∗)T H(x −x∗). The computa-
tional complexity of solving this problem with an ES to a ﬁxed target precision ε > 0 is of the form
O(d · κ(H) · log(1/ε)), where κ(H) denotes the condition number of the Hessian H (J¨agersk¨upper,
2006; Hansen et al., 2015). Hence, a step-size adaptive ES achieves linear convergence with rate
O
 
1/(d · κ(H))

."
CONVERGENCE RATES AND COMPUTATIONAL COMPLEXITY,0.16707616707616707,"The linear dependency on d is optimal for comparison-based optimization (Fournier & Teytaud, 2011),
but the dependency on H is sub-optimal. The advantage of maintaining covariance information is that
the factor κ(H) is improved to κ(HC∗), where C∗denotes the optimal covariance matrix available
to the ES. When approaching C∗= H−1, methods maintaining full covariance achieve the optimal
value κ(HC∗) = 1. In effect, as expected from a pseudo second order method, the convergence
rate is independent of H. A diagonal covariance matrix C∗acts as a diagonal pre-conditioner, with
varying effectiveness depending on the problem. Obviously, the block-diagonal and the low-rank
cases are in-between."
CONVERGENCE RATES AND COMPUTATIONAL COMPLEXITY,0.16953316953316952,"The OpenAI-ES (Salimans et al., 2017) features neither step size adaptation nor covariance adaptation.
Based on the NES framework of Wierstra et al. (2014), it leverages the ability of ES to estimate the
natural gradient of f from samples, and then applies the ADAM optimizer on top. In effect, this is
roughly comparable to using diagonal C."
CONVERGENCE RATES AND COMPUTATIONAL COMPLEXITY,0.171990171990172,"CMA has a price in terms of algorithm internal complexity, and in addition the adaptation process
is slow in terms of sample complexity. The above convergence rates measure time in terms of
objective function evaluations (sample complexity). When scaling up CMA to high dimensions, we
need to take the following concepts into consideration. Algorithm internal complexity refers to the
required (amortized) number of operations needed for creating a sample and for updating the internal
state—the covariance matrix in particular. Regarding sample complexity, we distinguish between the
number of samples needed to learn the covariance matrix, and the number of samples needed to solve
the problem."
CONVERGENCE RATES AND COMPUTATIONAL COMPLEXITY,0.17444717444717445,"Learning C with up to Θ(d2) parameters is sample-inefﬁcient. For example, a (small) network
with d = 104 weights results in d(d+1)"
CONVERGENCE RATES AND COMPUTATIONAL COMPLEXITY,0.1769041769041769,"2
≈5 · 107 parameters of the covariance matrix. Learning
these takes hundreds of millions of samples, each of which can be a full RL episode. This is
too slow for being useful. For larger d, even the storage of the full covariance matrix C becomes
prohibitive. Furthermore, performing computations with C scales at least linear with the number of its
parameters, which amounts to an internal complexity of Ω(d2) for full CMA. Since network evaluation
scales linearly with the number of weights d, CMA quickly becomes the computational bottleneck.
Therefore, a different trade-off between fast convergence and internal complexity is needed, which
can be realized for example with block-diagonal and low-rank structures, and combinations thereof."
IMPLICATIONS FOR NEURAL NETWORK TRAINING,0.17936117936117937,"2.3
IMPLICATIONS FOR NEURAL NETWORK TRAINING"
IMPLICATIONS FOR NEURAL NETWORK TRAINING,0.18181818181818182,"Diagonal, block-diagonal, and low-rank schemes successfully lower both internal and sample costs
of CMA signiﬁcantly. Therefore, they are key to the application of modern ES with CMA to RL."
IMPLICATIONS FOR NEURAL NETWORK TRAINING,0.18427518427518427,"In neural network training, weight spaces are often extremely high-dimensional. However, they also
come with a canonical structure, induced by the network topology. We can expect weights belonging
to different layers (and even weights belonging to different neurons within the same layer) to be
less correlated than weights within a layer. Hence, a block-diagonal covariance structure with one
block per layer (or per neuron) is a natural choice. It should be noted that there exist approaches for
identifying a problem decomposition automatically (Omidvar et al., 2013), if needed."
IMPLICATIONS FOR NEURAL NETWORK TRAINING,0.18673218673218672,Under review as a conference paper at ICLR 2022
IMPLICATIONS FOR NEURAL NETWORK TRAINING,0.1891891891891892,"If H has a block structure (f is separable), then sequentially optimizing all blocks in isolation is as
fast as optimizing the full problem. This is a direct consequence of the O(d) scaling of the sample
complexity discussed above, and lead to the following insight:"
IMPLICATIONS FOR NEURAL NETWORK TRAINING,0.19164619164619165,"Solving b independent sub-problems with k = d/b variables each in parallel results in a b-fold
speed-up over solving the full problem with d variables."
IMPLICATIONS FOR NEURAL NETWORK TRAINING,0.1941031941031941,"It is understood that this comes at a cost if the separability assumption is violated. However, the block
structure offers a further beneﬁt:"
IMPLICATIONS FOR NEURAL NETWORK TRAINING,0.19656019656019655,"Solving b independent sub-problems with k = d/b variables each with an ES featuring CMA results
in O(k2) sample and internal complexity for covariance learning, in contrast to O(d2) for full CMA.
If the number of blocks b scales linearly with the problem size d, or k ∈O(1), then CMA becomes
feasible for arbitrarily large problems."
IMPLICATIONS FOR NEURAL NETWORK TRAINING,0.19901719901719903,"These two insights offers a novel route towards highly parallel and at the same time more sample-
efﬁcient neuroevolution strategies. In addition to the embarrassingly parallel evaluation of a popula-
tion of candidate points, multiple blocks can be optimized in parallel. Given enough cores, higher
parallelism results in a b-fold speed-up. As an additional beneﬁt, CMA can be applied within each
block of variables. Provided that the problem has an (approximately) separable structure, CMA
results in improved sample efﬁciency, yielding a further speed-up."
METHOD,0.20147420147420148,"3
METHOD"
METHOD,0.20393120393120392,"The DiBB framework is originally inspired by Block-Diagonal Natural Evolution Strategies (BD-
NES; Cuccu & Gomez 2012), but extending the original design to be applicable to any BBO algorithm
without restrictions, providing at the same time a parallel and distributed implementation with limited
overhead. Here is a short summary of how to use DiBB:"
METHOD,0.20638820638820637,"1. The user deﬁnes a partition of the parameters, typically based on expert knowledge of the
application, e.g. for neuroevolution consider one block per (weights of connections entering
a) layer or one block per neuron.
2. Launching the method on a head node spawns the run control routine plus the object store
that maintains shared data. This follows typical best practices in distributed processing (Dean
& Ghemawat, 2008).
3. The control then launches one Block Worker (BW) for each parameter block on the avail-
able machines. The BW encapsulates the actual BBO algorithm. Each BW runs fully
asynchronously from the others, though contemporary. The BWs exchange information by
uploading to the head node the state of their search after each update (i.e. a generation in
ES).
4. In our implementation, each BW can spawn a pool of Fitness Evaluators (FE) on the same
machine. These are used to manage limited computational resources, such as available CPUs,
which becomes critical when a nontrivial objective function needs access to a signiﬁcant
portion of the available resources, such as the case with external software (e.g. physics
simulations of control tasks).
5. Alternatively, the user can request to evaluate trivial optimization functions on the BW
directly (either sequentially or with multi-threading), which is useful when the overhead of
maintaining a discrete pool of evaluators would be signiﬁcant."
METHOD,0.20884520884520885,"The Block Workers communicate with the head node in each generation, while generation cycles
are deﬁned asynchronously by each BW. Consider a problem with d variables x1, . . . , xd, and a BW
optimizing b variables xa, . . . , xa+b−1, denoted as the vector xB for short. The head node maintains
a reference solution ¯x ∈Rd, which fulﬁlls a two-fold purpose: it serves as an anytime estimate of the
state of the search (optimum), and it provides a unifying context to the BWs and the FEs."
METHOD,0.2113022113022113,"Intuitively, each BW is only aware of the variables in one block, and can only generate samples for
the corresponding variables. These incomplete samples need to be scored on the task to obtain the
gradient for solution improvement. The problem here is that the task expects a complete solution to
be evaluated: in our recurring example of neuroevolution, the sample could correspond to the weights
for a neuron, or layer, while only full networks can be evaluated on the task."
METHOD,0.21375921375921375,Under review as a conference paper at ICLR 2022
METHOD,0.21621621621621623,"We address this issue by leveraging once again our assumption of partial separability. After our
hypothesis of the correlation across blocks being negligible, we can evaluate each block in isolation
by inserting it in the context of the reference solution, obtained from the head node. Hypothesize for
a moment that there is no correlation inter-block—we address below the validity of this statement. In
this case, each layer can be scored fairly by constructing a full reference network, then swapping the
corresponding weights in the target layer for the block sample, and ﬁnally evaluating the resulting
complete network on the task. Different independent samples from a same block would receive fair
evaluation in this fashion as long as they are evaluated on the same reference network. This is in fact
constructed by assembling the per-block, partial reference solutions of each of the BBO instances
running on each block, and is updated in the head node by each block instance after each generation."
METHOD,0.21867321867321868,"More formally: at the start of each generation, the BW receives the current reference solution
¯x1, . . . , ¯xd from the head node.2 The block-level ES samples a population of candidate solutions
y1, . . . yλ ∈Rb. If b is small, then sampling from a Gaussian with full covariance matrix N(m, σ2C)
is feasible. The b-dimensions points are injected into the reference solution by constructing the
d-dimensional vectors x1, . . . xλ ∈Rd according to the following rule:"
METHOD,0.22113022113022113,"xj
i =
yj
i−a
if a ≤i ≤a + b
¯xi
otherwise"
METHOD,0.22358722358722358,"The vectors x1, . . . , xλ are passed to the (thread or node) pool of Fitness Evaluators. Once all
objective function values are computed, the ES updates its internal state, which contains its sampling
mean m and optionally further parameters like step size, evolution paths, and covariance matrix.
Finally, it sends the updated mean m back to the head node, which incorporates it into its reference
solution by overwriting xB with m."
METHOD,0.22604422604422605,"The last issue remaining is that of course we cannot expect the weights entering different neurons or
layers to be entirely separable; after all, they belong to the same network and they are thereby all
contributing to the ﬁnal output in its equation. This results in a problem of moving target, where the
score of a block sample depends on the global state of the search (in the form of the current complete
reference solution), which changes constantly as every BBO instance asynchronously sends an update
to the global reference state. Empirically we verify that the impact on the algorithm is only marginal:
after all, our hypothesis is relatively much stricter than in fully-separable implementations. This
is further mitigated when comparing with full-covariance algorithms by the fact that the latter will
still need to learn the lower covariance between lowly-correlated variables, which in turn effectively
lowers the sample efﬁciency of the theoretically superior algorithm in most complex real-world
scenario. In practice, we have seen no measurable advantage or disadvantage on either approach from
this perspective."
METHOD,0.2285012285012285,"The proposed setup directly reﬂects the added level of parallelism, compared to a standard ES.
Traditional implementations usually restrict parallelism to maintaining a pool of evaluators to speed
up the evaluation of independent samples using multiple cores. In our setup, a second level of
parallelism is established in terms of the Block Workers, which operate independently but for sparse
communication with the head node."
EXPERIMENTS,0.23095823095823095,"4
EXPERIMENTS"
EXPERIMENTS,0.2334152334152334,"This section describes the setup used to empirically assess the performance of DiBB. With our
experiments, we aim to address the following research questions:"
EXPERIMENTS,0.23587223587223588,"Q1: How well does the block-diagonal approach work, compared to diagonal and full CMA?
Q2: How does DiBB scale to a large number of machines and cores?
Q3: Is DiBB well-suited for neuroevolution applications?"
EXPERIMENTS,0.23832923832923833,"We assess the ﬁrst two questions on the standard COmparing Continuous Optimizers (COCO) Black
Box Optimization Benchmark (BBOB), using both the standard and the large-scale benchmark
suites. For answering the third question, we showcase a neuroevolution application in the challenging
OpenAI Gym 2D Walker environment."
EXPERIMENTS,0.24078624078624078,"2It would sufﬁce to send ¯x1, . . . , ¯xa−1, ¯xa+b, . . . , ¯xd to the BW, but the difference is usually negligible."
EXPERIMENTS,0.24324324324324326,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.2457002457002457,"Cluster setup.
We tested DiBB on a variety of hardware solutions. The COCO-BBOB experiments
were run on a cluster of 24 low-performance machines, each sporting an Intel(R) Core(TM) i7-2600
CPU @ 3.40GHz (4 cores/8 threads each), and 32 GB of RAM. As one can expect, these old machines
are far from state-of-the-art performance, which leaves a signiﬁcant margin of improvement for the
timings presented in our results."
EXPERIMENTS,0.24815724815724816,"Reference implementation.
Our experiments use our implementation of DiBB written in Python
and leveraging the Ray distributed computation library3. The code is released open source on GitHub4,
and available through Pypi5 for ease of adoption and extension. The cluster setup is simpliﬁed by
the included managing scripts. Running on a single machine with a single block and no Fitness
Evaluators roughly corresponds to running the underlying BBO algorithm alone (plus overhead).
Spawning multiple BWs and FEs automatically scales to the available resources. Deﬁning a set of
IPs in the managing script allows to easily distribute the computation."
COCO BBOB,0.25061425061425063,"4.1
COCO BBOB"
COCO BBOB,0.25307125307125306,"COCO provides multiple suites covering a broad range of test cases. The experiments presented in
this work are based on the BBOB and the BBOB large-scale suites."
COCO BBOB,0.25552825552825553,"BBOB.
The BBOB suite comes with 24 noise-free real-parameter single-objective benchmark
functions in dimensions d ∈{2, 3, 5, 10, 20, 40} (Hansen et al., 2009). The functions are divided into
ﬁve groups: separable functions (f1-f5), moderately conditioned functions (f6-f9), ill-conditioned
functions (f10-f14), multi-modal functions (f15-f19), and weakly structured multi-modal functions
(f20-f24). We expect the performance of DiBB to vary accordingly to our assumption of partial
separability."
COCO BBOB,0.257985257985258,"BBOB large-scale suite.
The BBOB large-scale suite contains the same 24 same functions
which are found in the BBOB suite, but scaling the available number of dimensions to d ∈
{20, 40, 80, 160, 320, 640}. This suite however introduces heuristics to decrease the computational
cost of a selection of functions in a large-scale setting. We refer the interested reader to Elhara et al.
(2019) for further details."
COCO BBOB,0.26044226044226043,"The BBOB suite provides a broad range of problems representative of many use-cases in the spectrum
of Black-Box Optimization. Particularly, and by design, several edge cases are present that are
unlikely to be encountered in real applications but provide compelling insights on the performance
and applicability of the tested algorithm. Particularly, BBOB problems are designed to be solved to
high precision to test for scale invariance; therefore d is not in the thousands. Scalability of d, and the
distinction between separable and non-separable problem classes, allows us to systematically evaluate
the effect of DiBB’s block structure on its performance. COCO greatly facilitates this process by
providing publicly available performance data for many SOTA algorithms.6"
COCO BBOB,0.2628992628992629,"We test the scaling of DiBB’s performance as we vary the problem dimensions and block size, from
the opposing perspectives of sample efﬁciency (i.e. convergence speed) and run time (i.e. wall-clock
speed). To this end, we ran the following experiments:"
COCO BBOB,0.26535626535626533,"1. Constant number of blocks, increasing d and block size, on the BBOB suite (bbob_fnb)
2. Constant block size, increasing d and number of blocks on the BBOB suite (bbob_fbs)
3. Constant number of blocks, increasing d and block size on the large-scale suite
(bbob_ls_fnb)
4. Constant block size, increasing d and number of blocks on the large-scale suite
(bbob_ls_fbs)"
COCO BBOB,0.2678132678132678,Details of the experimental setup are found in Appendix A.1.
COCO BBOB,0.2702702702702703,"3https://www.ray.io/ — a Python framework for distributed computing
4Link anonymized.
5Link anonymized.
6https://numbbo.github.io/data-archive/bbob/"
COCO BBOB,0.2727272727272727,Under review as a conference paper at ICLR 2022
COCO BBOB,0.2751842751842752,"0
2
4
6
log10(# f-evals / dimension) 0.0 0.2 0.4 0.6 0.8 1.0"
COCO BBOB,0.27764127764127766,"Fraction of function,target pairs"
COCO BBOB,0.2800982800982801,CMA-ES
COCO BBOB,0.28255528255528256,sepCMA
COCO BBOB,0.28501228501228504,"DiBB FNB
bbob-largescale f1-f5, 320-D
51 targets: 100..1e-08
15 instances"
COCO BBOB,0.28746928746928746,v0.0.0
COCO BBOB,0.28992628992628994,"0
2
4
6
log10(# f-evals / dimension) 0.0 0.2 0.4 0.6 0.8 1.0"
COCO BBOB,0.29238329238329236,"Fraction of function,target pairs CMA"
COCO BBOB,0.29484029484029484,sepCMA
COCO BBOB,0.2972972972972973,"DiBB FBS
bbob-largescale f1-f5, 320-D
51 targets: 100..1e-08
15 instances"
COCO BBOB,0.29975429975429974,v0.0.0
COCO BBOB,0.3022113022113022,"0
2
4
6
log10(# f-evals / dimension) 0.0 0.2 0.4 0.6 0.8 1.0"
COCO BBOB,0.3046683046683047,"Fraction of function,target pairs"
COCO BBOB,0.3071253071253071,sepCMA
COCO BBOB,0.3095823095823096,DiBB FNB
COCO BBOB,0.31203931203931207,"CMA-ES
bbob-largescale f10-f14, 320-D
51 targets: 100..1e-08
15 instances"
COCO BBOB,0.3144963144963145,v0.0.0
COCO BBOB,0.31695331695331697,"0
2
4
6
log10(# f-evals / dimension) 0.0 0.2 0.4 0.6 0.8 1.0"
COCO BBOB,0.3194103194103194,"Fraction of function,target pairs"
COCO BBOB,0.32186732186732187,sepCMA
COCO BBOB,0.32432432432432434,DiBB FBS
COCO BBOB,0.32678132678132676,"CMA
bbob-largescale f10-f14, 320-D
51 targets: 100..1e-08
15 instances"
COCO BBOB,0.32923832923832924,v0.0.0
COCO BBOB,0.3316953316953317,"Figure 2: ECDF plots of the performance of DiBB, separable and full CMA-ES in dimenion 320.
From left to right: f1-f5 with Exp 1/Exp 3, Exp 2/Exp 4, then f10-f14 with Exp 1/Exp 3, Exp 2/Exp 4."
COCO BBOB,0.33415233415233414,"We chose to focus on the group of separable (f1-f5) and ill-conditioned functions (f10-f14). The ﬁrst
group contains problems that can be solved easily by DiBB, while the second group is extremely hard
since it strongly violates the separability assumption."
COCO BBOB,0.3366093366093366,"Prototypical results of the COCO/BBOB experiments are found in ﬁgure 2 (full results in the ap-
pendix). The ECDF plots show the fraction of reached (precision) targets over dimension-normalized
time on a log-scale, so “higher is better”."
COCO BBOB,0.33906633906633904,"Sample Efﬁciency
On fully separable problems f1-f5, all algorithms perform roughly the same
in terms of sample complexity. In other words, covariance terms can be dropped at no cost, and
blocks can be optimized independently. This conﬁrms the theoretical predictions from Section 2.
Interestingly, in some cases DiBB (slightly) outperforms full CMA-ES, since off-diagonal covariance
terms can be detrimental when not needed."
COCO BBOB,0.3415233415233415,"For the ill-conditioned non-separable problems f10-f14, the performance of DiBB is close to CMA-ES
with diagonal covariance matrix and far worse than full CMA-ES, since the unmodeled terms with
(unrealistically high) correlations extremely close to ±1 dominate performance. Yet, unsurprisingly,
larger blocks result in better sample complexity. This ﬁnding conﬁrms the theoretical prediction that
the assumption of separability is crucial."
COCO BBOB,0.343980343980344,"Taken together, the two results imply that DiBB applied to a problem with block structure greatly
outperforms an ES with diagonal covariance matrix by being more sample-efﬁcient within each block,
while full CMA offers no further advantage. This answers question Q1."
COCO BBOB,0.3464373464373464,"Timings
Most of the experiments were run on the low-performance cluster described above. For 3
of the experiments however we tested the ﬂexibility of DiBB by switching to a new cluster of nodes
with Intel(R) Xeon(R) CPU E5-2620 v4 @ 2.10GHz processors (32 cores in total) and 128 GB RAM.
These were the 5d BBOB suite with one block (thus running on a single machine), the 40d BBOB
suite with 2 blocks (running on 3 machines: 1 head and 2 for the Block Workers), and the 40d BBOB
large-scale suite with one block (again single-machine run)."
COCO BBOB,0.3488943488943489,"Number of
dimensions
Number
of blocks
Block
size
Duration"
COCO BBOB,0.35135135135135137,"80
16
5
02h 47m 53s
160
16
10
04h 11m 15s
320
16
20
07h 01m 38s
640
16
40
12h 16m 02s"
COCO BBOB,0.3538083538083538,"Number of
dimensions
Number
of blocks
Block
size
Duration"
COCO BBOB,0.35626535626535627,"40
1
40
25h 13m 47s
80
2
40
43h 26m 57s
160
4
40
40h 21m 01s
320
8
40
44h 53m 29s
640
16
40
61h 20m 11s"
COCO BBOB,0.35872235872235875,"Table 1: Timing data for experiment 3 on the BBOB large-scale suite with a ﬁxed number of blocks
(left) and experiment 4 with a ﬁxed block size (right, 5× larger budget)."
COCO BBOB,0.36117936117936117,"Table 1 compares the runtimes of DiBB with different numbers of blocks, block sizes, and dimensions
on the BBOB large-scale suite. Several effects come together: in larger dimensions, function
evaluation time and communication overhead grow linearly. Due to the ﬁxed budget multiplier used
in COCO, the overall function evaluation budget also grows linearly. On the other hand, CMA’s
computational effort grows quadratically in the block size. We clearly observe that the runtime grows
far more benign when using a ﬁxed block size, which indicates that CMA overhead indeed quickly"
COCO BBOB,0.36363636363636365,Under review as a conference paper at ICLR 2022
COCO BBOB,0.36609336609336607,"becomes the dominating term. Hence, the block size should be kept tightly under control. This
answers question Q2."
COCO BBOB,0.36855036855036855,"4.2
PYBULLET WALKER 2D"
COCO BBOB,0.371007371007371,Although this paper introduces DiBB as a generic BBO
COCO BBOB,0.37346437346437344,"Figure 3:
The Walker 2D robot from
the Walker2DBulletEnv-v0 environ-
ment in OpenAI Gym. The goal is to make
the robot walk as far as possible without
falling. This sophisticated control task pro-
vides a signiﬁcant challenge in reinforce-
ment learning control, especially in its Py-
Bullet implementation. We tackle it using
DiBB wrapping LM-MA-ES to evolve a
neural network policy controller totaling
198 neurons and 11590 weights in direct
encoding."
COCO BBOB,0.3759213759213759,"framework, the original motivation behind its design
is rooted in scaling neuroevolution to large network
models. Therefore we present our results on a com-
plex reinforcement learning control task: the Walker
2D gym environment from PyBullet (Coumans & Bai,
2016–2021)."
COCO BBOB,0.3783783783783784,"This task, running on the PyBullet physics engine, is
about low-level control of a fairly basic 2D robot (see
Figure 3). The goal of the task is to learn a gait that
allows the robot to reach the farthest possible distance
in a limited time and without toppling. The 22 dimen-
sions of the observations correspond to a broad array
of sensors including positional and angular speed of
the joints, height above the ground, speed along the 3
axes (plus roll, pitch and yaw), and more. The action
space is composed of 6 (normalized) torque control
signals, which are sent to the motors in each joint."
COCO BBOB,0.3808353808353808,"The policy network is feed-forward and fully con-
nected, including 2 hidden layers of sizes [128, 64],
and using ReLU activation on all neurons. The output
layer is composed of 6 neurons with tanh activation to
match the normalized range of motor commands."
COCO BBOB,0.3832923832923833,"The network is trained with DiBB wrapping LM-MA-ES (Loshchilov et al., 2018), and reaches a
score of 1126 (average score over 100 runs, maximal episode length 1000) in 25 hours running on a
cluster of 4 machines (3 of which have 8 Intel(R) Core(TM) i7-3770 CPU @ 3.40GHz processors, the
fourth one having 32 Intel(R) Xeon(R) CPU E5-2620 v4 @ 2.10GHz processors). As a comparison
we used Random Weight Guessing, generating 1000 individuals, which reached a score of 42.These
are the ﬁrst results on this benchmark using neuroevolution to the authors’ knowledge, due to the
limited availability of sophisticated evolutionary methods scaling to that many dimensions. This
answers question Q3."
CONCLUSIONS,0.3857493857493858,"5
CONCLUSIONS"
CONCLUSIONS,0.3882063882063882,"We present a new framework that wraps any Black-Box Optimization (BBO) algorithm into a new
partially-separable variant. The variable set is partitioned into blocks with high intra-correlation
but low inter-correlation, based on expert knowledge on the problem structure that is commonly
available in real-world applications. Each block is searched independently by a separate instance of
the wrapped BBO algorithm, which allows to parallelize and distribute its computation across any
number of available machines. We name it DiBB for Distributing Black-Box optimization."
CONCLUSIONS,0.3906633906633907,"Its main advantage lies in bounding the overall computational complexity not in the total number
of variables, but in the (arbitrary, user-deﬁned) size of the largest block. The algorithm complexity
scales constantly with the number of blocks as long as more machines are available, but for a limited
communication overhead. Varying the number and size of the blocks allows the user to trade off
convergence speed (larger blocks giving more awareness of the relationship between variables) and
wall-clock speed (smaller blocks make for faster updates). DiBB is particularly well suite for the
purpose of scaling neuroevolution to larger network models, with (weights of connections entering)
neurons and layers being prime candidate for block grouping. Future work includes exploring the
dynamics of different BBO algorithms, and scaling to larger models—potentially non-differentiable,
as smoothness is not a requirement."
CONCLUSIONS,0.3931203931203931,Under review as a conference paper at ICLR 2022
REFERENCES,0.3955773955773956,REFERENCES
REFERENCES,0.39803439803439805,"Youhei Akimoto, Anne Auger, and Nikolaus Hansen. Comparison-based natural gradient optimization
in high dimension. In Genetic and Evolutionary Computation Conference, pp. 373–380. ACM,
2014."
REFERENCES,0.4004914004914005,"Charles Audet and Warren Hare. Derivative-free and blackbox optimization. Springer, 2017."
REFERENCES,0.40294840294840295,"Patryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter. Back to basics: Benchmarking canonical
evolution strategies for playing ATARI. Technical Report 1802.08842, arXiv.org, 2018."
REFERENCES,0.40540540540540543,"Erwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation for games, robotics
and machine learning. http://pybullet.org, 2016–2021."
REFERENCES,0.40786240786240785,"Giuseppe Cuccu and Faustino Gomez. Block diagonal natural evolution strategies. In International
Conference on Parallel Problem Solving from Nature, pp. 488–497. Springer, 2012."
REFERENCES,0.4103194103194103,"Jeffrey Dean and Sanjay Ghemawat. Mapreduce: simpliﬁed data processing on large clusters.
Commun. ACM, 51(1):107–113, 2008. doi: 10.1145/1327452.1327492. URL http://doi.
acm.org/10.1145/1327452.1327492."
REFERENCES,0.41277641277641275,"Ouassim Ait Elhara, Konstantinos Varelas, Duc Manh Nguyen, Tea Tuˇsar, Dimo Brockhoff, Nikolaus
Hansen, and Anne Auger. COCO: The Large Scale Black-Box Optimization Benchmarking
(bbob-largescale) Test Suite. arXiv preprint arXiv:1903.06396, 2019."
REFERENCES,0.4152334152334152,"Stanislav Fort and Stanislaw Jastrzebski. Large scale structure of neural network loss landscapes.
Advances in Neural Information Processing Systems, 32:6709–6717, 2019."
REFERENCES,0.4176904176904177,"Herv´e Fournier and Olivier Teytaud. Lower bounds for comparison based evolution strategies using
vc-dimension and sign patterns. Algorithmica, 59(3):387–408, 2011."
REFERENCES,0.4201474201474201,"David Ha and J¨urgen Schmidhuber. Recurrent world models facilitate policy evolution. Technical
Report 1809.01999, arXiv.org, 2018."
REFERENCES,0.4226044226044226,"Nikolaus Hansen. Adapting Arbitrary Normal Mutation Distributions in Evolution Strategies: The
Covariance Matrix Adaptation. In IEEE International Conference on Evolutionary Computation,
1996, pp. 312–317, 1996."
REFERENCES,0.4250614250614251,"Nikolaus Hansen. A global surrogate assisted CMA-ES. In Proceedings of the Genetic and Evolu-
tionary Computation Conference, pp. 664–672, Prague Czech Republic, July 2019. ACM. ISBN
978-1-4503-6111-8. doi: 10.1145/3321707.3321842. URL https://dl.acm.org/doi/10.
1145/3321707.3321842."
REFERENCES,0.4275184275184275,"Nikolaus Hansen and Andreas Ostermeier. Completely derandomized self-adaptation in evolution
strategies. Evolutionary Computation, 9(2):159–195, 2001."
REFERENCES,0.42997542997543,"Nikolaus Hansen, Steffen Finck, Raymond Ros, and Anne Auger. Real-Parameter Black-Box
Optimization Benchmarking 2009: Noiseless Functions Deﬁnitions. Technical Report RR-6869,
INRIA, 2009."
REFERENCES,0.43243243243243246,"Nikolaus Hansen, Dirk V Arnold, and Anne Auger. Evolution strategies. In Springer handbook of
computational intelligence, pp. 871–898. Springer, 2015."
REFERENCES,0.4348894348894349,"Verena Heidrich-Meisner and Christian Igel. Neuroevolution strategies for episodic reinforcement
learning. Journal of Algorithms, 64(4):152–168, 2009."
REFERENCES,0.43734643734643736,"Christian Igel. Neuroevolution for reinforcement learning using evolution strategies. In The 2003
Congress on Evolutionary Computation (CEC’03), volume 4, pp. 2588–2595. IEEE, 2003."
REFERENCES,0.4398034398034398,"Jens J¨agersk¨upper. How the (1+1)-ES using isotropic mutations minimizes positive deﬁnite quadratic
forms. Theoretical Computer Science, 361(1):38–56, 2006."
REFERENCES,0.44226044226044225,"Ilya Loshchilov. A Computationally Efﬁcient Limited Memory CMA-ES for Large Scale Optimiza-
tion. In Genetic and Evolutionary Computation Conference, pp. 397–404. ACM, 2014."
REFERENCES,0.44471744471744473,Under review as a conference paper at ICLR 2022
REFERENCES,0.44717444717444715,"Ilya Loshchilov, Tobias Glasmachers, and Hans-Georg Beyer. Large scale black-box optimization by
limited-memory matrix adaptation. IEEE Transactions on Evolutionary Computation, 99, 2018."
REFERENCES,0.44963144963144963,"Mohammad Nabi Omidvar, Xiaodong Li, Yi Mei, and Xin Yao. Cooperative co-evolution with
differential grouping for large scale optimization. IEEE Transactions on Evolutionary Computation,
18(3):378–393, 2013."
REFERENCES,0.4520884520884521,"Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y Chen, Xi Chen,
Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter space noise for exploration.
Technical Report 1706.01905, arXiv.org, 2017."
REFERENCES,0.45454545454545453,"Ingo Rechenberg.
Evolutionsstrategie: Optimierung technischer Systeme nach Prinzipien der
biologischen Evolution. Frommann-Holzboog, 1973."
REFERENCES,0.457002457002457,"Raymond Ros and Nikolaus Hansen. A Simple Modiﬁcation in CMA-ES Achieving Linear Time and
Space Complexity. In G¨unter Rudolph, Thomas Jansen, Nicola Beume, Simon Lucas, and Carlo
Poloni (eds.), Parallel Problem Solving from Nature – PPSN X, pp. 296–305, Berlin, Heidelberg,
2008. Springer Berlin Heidelberg. ISBN 978-3-540-87700-4."
REFERENCES,0.4594594594594595,"Tim Salimans, Jonathan Ho, Xi. Chen, and Ilya Sutskever. Evolution strategies as a scalable alternative
to reinforcement learning. Technical Report arXiv:1703.03864, arxiv.org, 2017."
REFERENCES,0.4619164619164619,"Tom Schaul, Tobias Glasmachers, and J¨urgen Schmidhuber. High dimensions and heavy tails
for natural evolution strategies. In Proceedings of the 13th annual conference on Genetic and
Evolutionary Computation (GECCO), pp. 845–852, 2011."
REFERENCES,0.4643734643734644,"Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization
landscape of over-parameterized shallow neural networks. IEEE Transactions on Information
Theory, 65(2):742–769, 2018."
REFERENCES,0.4668304668304668,"Kenneth O Stanley, Jeff Clune, Joel Lehman, and Risto Miikkulainen. Designing neural networks
through neuroevolution. Nature Machine Intelligence, 1(1):24–35, 2019."
REFERENCES,0.4692874692874693,"Konstantinos Varelas. Benchmarking Large Scale Variants of CMA-ES and L-BFGS-B on the Bbob-
Largescale Testbed. In Proceedings of the Genetic and Evolutionary Computation Conference
Companion, GECCO ’19, pp. 1937–1945, New York, NY, USA, 2019. Association for Computing
Machinery. ISBN 978-1-4503-6748-6. doi: 10.1145/3319619.3326893. URL https://doi.
org/10.1145/3319619.3326893."
REFERENCES,0.47174447174447176,"D. Wierstra, T. Schaul, T. Glasmachers, Y. Sun, J. Peters, and J. Schmidhuber. Natural evolution
strategies. Journal of Machine Learning Research, 15:949–980, 2014."
REFERENCES,0.4742014742014742,"A
APPENDIX"
REFERENCES,0.47665847665847666,"A.1
DETAILS OF THE COCO/BBOB EXPERIMENTS"
REFERENCES,0.47911547911547914,"For experiments 1 and 2, a budget multiplier of 105 was chosen and the IPOP restart mechanism was
used in accordance with Hansen (2019). To compare not only against CMA-ES but also the separable
version, we also ran sep-CMA-ES on the bbob suite with a budget multiplier of 104."
REFERENCES,0.48157248157248156,"To compare with CMA-ES and sep-CMA-ES, we ran experiments 3 and 4 without IPOP and we
sampled the initial candidate solutions uniformly at random from [−4, 4]d the same way as Varelas
(2019). Experiment 3 uses a budget multiplier of 104, and experiment 4 uses a budget multiplier of
5 · 104. The initial step size sigma0 was set to 2 in all of the experiments."
REFERENCES,0.48402948402948404,"The
data
for
CMA-ES
and
sep-CMA-ES
can
be
downloaded
via
cocopp
or
us-
ing
the
following
links
respectively:
https://numbbo.github.io/gforge/
data-archive/bbob-largescale/2019/CMA_Varelas_largescale.tgz
and
https://numbbo.github.io/gforge/data-archive/bbob-largescale/2019/
sepCMA_Varelas_largescale.tgz."
REFERENCES,0.4864864864864865,Under review as a conference paper at ICLR 2022
REFERENCES,0.48894348894348894,"0
2
4
6
log10(# f-evals / dimension) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.4914004914004914,"Fraction of function,target pairs"
REFERENCES,0.49385749385749383,sepCMA
REFERENCES,0.4963144963144963,DiBB FNB
REFERENCES,0.4987714987714988,CMA-ES
REFERENCES,0.5012285012285013,"best 2009
bbob f1-f5, 5-D
51 targets: 100..1e-08
15 instances"
REFERENCES,0.5036855036855037,v0.0.0
REFERENCES,0.5061425061425061,"0
2
4
6
log10(# f-evals / dimension) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.5085995085995086,"Fraction of function,target pairs"
REFERENCES,0.5110565110565111,sepCMA
REFERENCES,0.5135135135135135,DiBB FNB
REFERENCES,0.515970515970516,CMA-ES
REFERENCES,0.5184275184275184,"best 2009
bbob f10-f14, 5-D
51 targets: 100..1e-08
15 instances"
REFERENCES,0.5208845208845209,v0.0.0
REFERENCES,0.5233415233415234,"0
2
4
6
log10(# f-evals / dimension) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.5257985257985258,"Fraction of function,target pairs"
REFERENCES,0.5282555282555282,DiBB FBS
REFERENCES,0.5307125307125307,sepCMA
REFERENCES,0.5331695331695332,CMA-ES
REFERENCES,0.5356265356265356,"best 2009
bbob f1-f5, 5-D
51 targets: 100..1e-08
15 instances"
REFERENCES,0.538083538083538,v0.0.0
REFERENCES,0.5405405405405406,"0
2
4
6
log10(# f-evals / dimension) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.542997542997543,"Fraction of function,target pairs"
REFERENCES,0.5454545454545454,sepCMA
REFERENCES,0.547911547911548,DiBB FBS
REFERENCES,0.5503685503685504,CMA-ES
REFERENCES,0.5528255528255528,"best 2009
bbob f10-f14, 5-D
51 targets: 100..1e-08
15 instances"
REFERENCES,0.5552825552825553,v0.0.0
REFERENCES,0.5577395577395577,"0
2
4
6
log10(# f-evals / dimension) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.5601965601965602,"Fraction of function,target pairs"
REFERENCES,0.5626535626535627,sepCMA
REFERENCES,0.5651105651105651,CMA-ES
REFERENCES,0.5675675675675675,DiBB FNB
REFERENCES,0.5700245700245701,"best 2009
bbob f1-f5, 10-D
51 targets: 100..1e-08
15 instances"
REFERENCES,0.5724815724815725,v0.0.0
REFERENCES,0.5749385749385749,"0
2
4
6
log10(# f-evals / dimension) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.5773955773955773,"Fraction of function,target pairs"
REFERENCES,0.5798525798525799,sepCMA
REFERENCES,0.5823095823095823,DiBB FNB
REFERENCES,0.5847665847665847,CMA-ES
REFERENCES,0.5872235872235873,"best 2009
bbob f10-f14, 10-D
51 targets: 100..1e-08
15 instances"
REFERENCES,0.5896805896805897,v0.0.0
REFERENCES,0.5921375921375921,"0
2
4
6
log10(# f-evals / dimension) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.5945945945945946,"Fraction of function,target pairs"
REFERENCES,0.597051597051597,sepCMA
REFERENCES,0.5995085995085995,DiBB FBS
REFERENCES,0.601965601965602,CMA-ES
REFERENCES,0.6044226044226044,"best 2009
bbob f1-f5, 10-D
51 targets: 100..1e-08
15 instances"
REFERENCES,0.6068796068796068,v0.0.0
REFERENCES,0.6093366093366094,"0
2
4
6
log10(# f-evals / dimension) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.6117936117936118,"Fraction of function,target pairs"
REFERENCES,0.6142506142506142,sepCMA
REFERENCES,0.6167076167076168,DiBB FBS
REFERENCES,0.6191646191646192,CMA-ES
REFERENCES,0.6216216216216216,"best 2009
bbob f10-f14, 10-D
51 targets: 100..1e-08
15 instances"
REFERENCES,0.6240786240786241,v0.0.0
REFERENCES,0.6265356265356266,"0
2
4
6
log10(# f-evals / dimension) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.628992628992629,"Fraction of function,target pairs"
REFERENCES,0.6314496314496314,sepCMA
REFERENCES,0.6339066339066339,CMA-ES
REFERENCES,0.6363636363636364,DiBB FNB
REFERENCES,0.6388206388206388,"best 2009
bbob f1-f5, 20-D
51 targets: 100..1e-08
15 instances"
REFERENCES,0.6412776412776413,v0.0.0
REFERENCES,0.6437346437346437,"0
2
4
6
log10(# f-evals / dimension) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.6461916461916462,"Fraction of function,target pairs"
REFERENCES,0.6486486486486487,sepCMA
REFERENCES,0.6511056511056511,DiBB FNB
REFERENCES,0.6535626535626535,CMA-ES
REFERENCES,0.6560196560196561,"best 2009
bbob f10-f14, 20-D
51 targets: 100..1e-08
15 instances"
REFERENCES,0.6584766584766585,v0.0.0
REFERENCES,0.6609336609336609,"0
2
4
6
log10(# f-evals / dimension) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.6633906633906634,"Fraction of function,target pairs"
REFERENCES,0.6658476658476659,sepCMA
REFERENCES,0.6683046683046683,CMA-ES
REFERENCES,0.6707616707616708,DiBB FBS
REFERENCES,0.6732186732186732,"best 2009
bbob f1-f5, 20-D
51 targets: 100..1e-08
15 instances"
REFERENCES,0.6756756756756757,v0.0.0
REFERENCES,0.6781326781326781,"0
2
4
6
log10(# f-evals / dimension) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.6805896805896806,"Fraction of function,target pairs"
REFERENCES,0.683046683046683,sepCMA
REFERENCES,0.6855036855036855,DiBB FBS
REFERENCES,0.687960687960688,CMA-ES
REFERENCES,0.6904176904176904,"best 2009
bbob f10-f14, 20-D
51 targets: 100..1e-08
15 instances"
REFERENCES,0.6928746928746928,v0.0.0
REFERENCES,0.6953316953316954,"0
2
4
6
log10(# f-evals / dimension) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.6977886977886978,"Fraction of function,target pairs"
REFERENCES,0.7002457002457002,sepCMA
REFERENCES,0.7027027027027027,CMA-ES
REFERENCES,0.7051597051597052,DiBB FNB
REFERENCES,0.7076167076167076,"best 2009
bbob f1-f5, 40-D
51 targets: 100..1e-08
15 instances"
REFERENCES,0.7100737100737101,v0.0.0
REFERENCES,0.7125307125307125,"0
2
4
6
log10(# f-evals / dimension) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.714987714987715,"Fraction of function,target pairs"
REFERENCES,0.7174447174447175,sepCMA
REFERENCES,0.7199017199017199,DiBB FNB
REFERENCES,0.7223587223587223,CMA-ES
REFERENCES,0.7248157248157249,"best 2009
bbob f10-f14, 40-D
51 targets: 100..1e-08
15 instances"
REFERENCES,0.7272727272727273,v0.0.0
REFERENCES,0.7297297297297297,"0
2
4
6
log10(# f-evals / dimension) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7321867321867321,"Fraction of function,target pairs"
REFERENCES,0.7346437346437347,sepCMA
REFERENCES,0.7371007371007371,CMA-ES
REFERENCES,0.7395577395577395,DiBB FBS
REFERENCES,0.742014742014742,"best 2009
bbob f1-f5, 40-D
51 targets: 100..1e-08
15 instances"
REFERENCES,0.7444717444717445,v0.0.0
REFERENCES,0.7469287469287469,"0
2
4
6
log10(# f-evals / dimension) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7493857493857494,"Fraction of function,target pairs"
REFERENCES,0.7518427518427518,DiBB FBS
REFERENCES,0.7542997542997543,sepCMA
REFERENCES,0.7567567567567568,CMA-ES
REFERENCES,0.7592137592137592,"best 2009
bbob f10-f14, 40-D
51 targets: 100..1e-08
15 instances"
REFERENCES,0.7616707616707616,v0.0.0
REFERENCES,0.7641277641277642,"0
2
4
6
log10(# f-evals / dimension) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7665847665847666,"Fraction of function,target pairs"
REFERENCES,0.769041769041769,CMA-ES
REFERENCES,0.7714987714987716,sepCMA
REFERENCES,0.773955773955774,"DiBB FNB
bbob-largescale f1-f5, 80-D
51 targets: 100..1e-08
15 instances"
REFERENCES,0.7764127764127764,v0.0.0
REFERENCES,0.7788697788697788,"0
2
4
6
log10(# f-evals / dimension) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7813267813267813,"Fraction of function,target pairs"
REFERENCES,0.7837837837837838,DiBB FNB
REFERENCES,0.7862407862407862,sepCMA
REFERENCES,0.7886977886977887,"CMA-ES
bbob-largescale f10-f14, 80-D
51 targets: 100..1e-08
15 instances"
REFERENCES,0.7911547911547911,v0.0.0
REFERENCES,0.7936117936117936,"0
2
4
6
log10(# f-evals / dimension) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7960687960687961,"Fraction of function,target pairs CMA"
REFERENCES,0.7985257985257985,sepCMA
REFERENCES,0.800982800982801,"DiBB FBS
bbob-largescale f1-f5, 80-D
51 targets: 100..1e-08
15 instances"
REFERENCES,0.8034398034398035,v0.0.0
REFERENCES,0.8058968058968059,"0
2
4
6
log10(# f-evals / dimension) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.8083538083538083,"Fraction of function,target pairs"
REFERENCES,0.8108108108108109,sepCMA
REFERENCES,0.8132678132678133,DiBB FBS
REFERENCES,0.8157248157248157,"CMA
bbob-largescale f10-f14, 80-D
51 targets: 100..1e-08
15 instances"
REFERENCES,0.8181818181818182,v0.0.0
REFERENCES,0.8206388206388207,"0
2
4
6
log10(# f-evals / dimension) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.8230958230958231,"Fraction of function,target pairs"
REFERENCES,0.8255528255528255,CMA-ES
REFERENCES,0.828009828009828,sepCMA
REFERENCES,0.8304668304668305,"DiBB FNB
bbob-largescale f1-f5, 160-D
51 targets: 100..1e-08
15 instances"
REFERENCES,0.8329238329238329,v0.0.0
REFERENCES,0.8353808353808354,"0
2
4
6
log10(# f-evals / dimension) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.8378378378378378,"Fraction of function,target pairs"
REFERENCES,0.8402948402948403,DiBB FNB
REFERENCES,0.8427518427518428,sepCMA
REFERENCES,0.8452088452088452,"CMA-ES
bbob-largescale f10-f14, 160-D
51 targets: 100..1e-08
15 instances"
REFERENCES,0.8476658476658476,v0.0.0
REFERENCES,0.8501228501228502,"0
2
4
6
log10(# f-evals / dimension) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.8525798525798526,"Fraction of function,target pairs CMA"
REFERENCES,0.855036855036855,sepCMA
REFERENCES,0.8574938574938575,"DiBB FBS
bbob-largescale f1-f5, 160-D
51 targets: 100..1e-08
15 instances"
REFERENCES,0.85995085995086,v0.0.0
REFERENCES,0.8624078624078624,"0
2
4
6
log10(# f-evals / dimension) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.8648648648648649,"Fraction of function,target pairs"
REFERENCES,0.8673218673218673,sepCMA
REFERENCES,0.8697788697788698,DiBB FBS
REFERENCES,0.8722358722358723,"CMA
bbob-largescale f10-f14, 160-D
51 targets: 100..1e-08
15 instances"
REFERENCES,0.8746928746928747,v0.0.0
REFERENCES,0.8771498771498771,"0
2
4
6
log10(# f-evals / dimension) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.8796068796068796,"Fraction of function,target pairs"
REFERENCES,0.8820638820638821,CMA-ES
REFERENCES,0.8845208845208845,sepCMA
REFERENCES,0.8869778869778869,"DiBB FNB
bbob-largescale f1-f5, 320-D
51 targets: 100..1e-08
15 instances"
REFERENCES,0.8894348894348895,v0.0.0
REFERENCES,0.8918918918918919,"0
2
4
6
log10(# f-evals / dimension) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.8943488943488943,"Fraction of function,target pairs"
REFERENCES,0.8968058968058968,sepCMA
REFERENCES,0.8992628992628993,DiBB FNB
REFERENCES,0.9017199017199017,"CMA-ES
bbob-largescale f10-f14, 320-D
51 targets: 100..1e-08
15 instances"
REFERENCES,0.9041769041769042,v0.0.0
REFERENCES,0.9066339066339066,"0
2
4
6
log10(# f-evals / dimension) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.9090909090909091,"Fraction of function,target pairs CMA"
REFERENCES,0.9115479115479116,sepCMA
REFERENCES,0.914004914004914,"DiBB FBS
bbob-largescale f1-f5, 320-D
51 targets: 100..1e-08
15 instances"
REFERENCES,0.9164619164619164,v0.0.0
REFERENCES,0.918918918918919,"0
2
4
6
log10(# f-evals / dimension) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.9213759213759214,"Fraction of function,target pairs"
REFERENCES,0.9238329238329238,sepCMA
REFERENCES,0.9262899262899262,DiBB FBS
REFERENCES,0.9287469287469288,"CMA
bbob-largescale f10-f14, 320-D
51 targets: 100..1e-08
15 instances"
REFERENCES,0.9312039312039312,v0.0.0
REFERENCES,0.9336609336609336,"0
2
4
6
log10(# f-evals / dimension) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.9361179361179361,"Fraction of function,target pairs"
REFERENCES,0.9385749385749386,sepCMA
REFERENCES,0.941031941031941,"DiBB FNB
bbob-largescale f1-f5, 640-D
51 targets: 100..1e-08
15 instances"
REFERENCES,0.9434889434889435,v0.0.0
REFERENCES,0.9459459459459459,"0
2
4
6
log10(# f-evals / dimension) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.9484029484029484,"Fraction of function,target pairs"
REFERENCES,0.9508599508599509,sepCMA
REFERENCES,0.9533169533169533,"DiBB FNB
bbob-largescale f10-f14, 640-D
51 targets: 100..1e-08
15 instances"
REFERENCES,0.9557739557739557,v0.0.0
REFERENCES,0.9582309582309583,"0
2
4
6
log10(# f-evals / dimension) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.9606879606879607,"Fraction of function,target pairs"
REFERENCES,0.9631449631449631,sepCMA
REFERENCES,0.9656019656019657,"DiBB FBS
bbob-largescale f1-f5, 640-D
51 targets: 100..1e-08
15 instances"
REFERENCES,0.9680589680589681,v0.0.0
REFERENCES,0.9705159705159705,"0
2
4
6
log10(# f-evals / dimension) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.972972972972973,"Fraction of function,target pairs"
REFERENCES,0.9754299754299754,sepCMA
REFERENCES,0.9778869778869779,"DiBB FBS
bbob-largescale f10-f14, 640-D
51 targets: 100..1e-08
15 instances"
REFERENCES,0.9803439803439803,v0.0.0
REFERENCES,0.9828009828009828,"Figure 4: BBOB-COCO results of DiBB, separable and full CMA-ES. Rows correspond to dimen-
sions 5, 10, 20, 40, 80, 160, 320, and 640. Columns correspond to Exp 1/Exp 2 on separable functions,
ill-conditioned functions, as well as Exp 3/Exp 4 on separable and ill-conditioned functions."
REFERENCES,0.9852579852579852,Under review as a conference paper at ICLR 2022
REFERENCES,0.9877149877149877,"BBOB
d = 5
d = 10
d = 20
d = 40"
REFERENCES,0.9901719901719902,"Exp. 1
2 / 2,3 / 250K
2 / 5 / 500K
2 / 10 / 1M
2 / 20 / 2M
Exp. 2
1 / 5 / 500K
2 / 5 / 500K
4 / 5 / 500K
8 / 5 / 500K"
REFERENCES,0.9926289926289926,"large-scale
d = 80
d = 160
d = 320
d = 640"
REFERENCES,0.995085995085995,"Exp. 3
16 / 5 / 250K
16 / 10 / 500K
16 / 20 / 1M
16 / 40 / 2M
Exp. 4
2 / 40 / 400K
4 / 40 / 400K
8 / 40 / 400K
16 / 40 / 400K"
REFERENCES,0.9975429975429976,"Table 2: Experimental setup on the BBOB and the BBOB large-scale suites, in the format “number
of blocks / block size / # objective function evaluations per machine”. Experiments 1 and 3 scale the
block size (and correspondingly, the evaluation budget per compute node), while experiments 2 and 4
scale the number of blocks."
