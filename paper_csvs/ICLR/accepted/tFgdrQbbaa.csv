Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.001282051282051282,"Sequential training from task to task is becoming one of the major objects in deep
learning applications such as continual learning and transfer learning. Nevertheless,
it remains unclear under what conditions the trained model’s performance improves
or deteriorates. To deepen our understanding of sequential training, this study
provides a theoretical analysis of generalization performance in a solvable case of
continual learning. We consider neural networks in the neural tangent kernel (NTK)
regime that continually learn target functions from task to task, and investigate
the generalization by using an established statistical mechanical analysis of kernel
ridge-less regression. We ﬁrst show characteristic transitions from positive to
negative transfer. More similar targets above a speciﬁc critical value can achieve
positive knowledge transfer for the subsequent task while catastrophic forgetting
occurs even with very similar targets. Next, we investigate a variant of continual
learning which supposes the same target function in multiple tasks. Even for the
same target, the trained model shows some transfer and forgetting depending on the
sample size of each task. We can guarantee that the generalization error monotoni-
cally decreases from task to task for equal sample sizes while unbalanced sample
sizes deteriorate the generalization. We respectively refer to these improvement and
deterioration as self-knowledge transfer and forgetting, and empirically conﬁrm
them in realistic training of deep neural networks as well."
INTRODUCTION,0.002564102564102564,"1
INTRODUCTION"
INTRODUCTION,0.0038461538461538464,"As deep learning develops for a single task, it enables us to work on more complicated learning frame-
works where the model is sequentially trained on multiple tasks, e.g., transfer learning, curriculum
learning, and continual learning. Continual learning deals with the situation in which the learning
machine cannot access previous data due to memory constraints or privacy reasons. It has attracted
much attention due to the demand on applications, and fundamental understanding and algorithms
are being explored (Hadsell et al., 2020). One well-known phenomenon is catastrophic forgetting;
when a network is trained between different tasks, naive training cannot maintain performance on the
previous task (McCloskey & Cohen, 1989; Kirkpatrick et al., 2017)."
INTRODUCTION,0.005128205128205128,"It remains unclear in most cases under what conditions a trained model’s performance improves
or deteriorates in sequential training. Understanding its generalization performance is still limited
(Pentina & Lampert, 2014; Bennani et al., 2020; Lee et al., 2021). For single-task training, however,
many empirical and theoretical studies have succeeded in characterizing generalization performance
in over-parameterized neural networks and given quantitative evaluation on sample-size dependencies,
e.g., double descent (Nakkiran et al., 2020). For further development, it will be helpful to extend the
analyses on single-task training to sequential training on multiple tasks and give theoretical backing."
INTRODUCTION,0.00641025641025641,"To deepen our understanding of sequential training, this study shows a theoretical analysis of its
generalization error in the neural tangent kernel (NTK) regime. In more details, we consider the
NTK formulation of continual learning proposed by Bennani et al. (2020); Doan et al. (2021). By
extending a statistical mechanical analysis of kernel ridge-less regression, we investigate learning
curves, i.e., the dependence of generalization error on sample size or number of tasks. The analysis"
INTRODUCTION,0.007692307692307693,Published as a conference paper at ICLR 2022
INTRODUCTION,0.008974358974358974,"focuses on the continual learning with explicit task boundaries. The model learns data generated
by similar teacher functions, which we call target functions, from one task to another. All input
samples are generated from the same distribution in an i.i.d. manner, and each task has output samples
(labels) generated by its own target function. Our main contributions are summarized as follows.
First, we revealed characteristic transitions from negative to positive transfer depending on the target
similarity. More similar targets above a speciﬁc critical value can achieve positive knowledge transfer
(i.e., better prediction on the subsequent task than training without the ﬁrst task). Compared to
this, backward transfer (i.e., prediction on the previous task) has a large critical value, and subtle
dissimilarity between targets causes negative transfer. The error can rapidly increase, which clariﬁes
that catastrophic forgetting is literally catastrophic (Section 4.1)."
INTRODUCTION,0.010256410256410256,"Second, we considered a variant of continual learning, that is, learning of the same target function
in multiple tasks. Even for the same target function, the trained model’s performance improves
or deteriorates in a non-monotonic way. This depends on the sample size of each task; for equal
sample sizes, we can guarantee that the generalization error monotonically decreases from task to task
(Section 4.2 for two tasks & Section 5 for more tasks). Unbalanced sample sizes, however, deteriorates
generalization (Section 4.3). We refer to these improvement and deterioration of generalization as
self-knowledge transfer and forgetting, respectively. Finally, we empirically conﬁrmed that self-
knowledge transfer and forgetting actually appear in the realistic training of multi-layer perceptron
(MLP) and ResNet-18 (Section 5.1). Thus, this study sheds light on fundamental understanding and
the universal behavior of sequential training in over-parameterized learning machines."
RELATED WORK,0.011538461538461539,"2
RELATED WORK"
RELATED WORK,0.01282051282051282,"Method of analysis. As an analysis tool, we use the replica method originally developed for statistical
mechanics. Statistical mechanical analysis enables us typical-case evaluation, that is, the average
evaluation over data samples or parameter conﬁgurations. It sometimes provides us with novel insight
into what the worst-case evaluation has not captured (Abbaras et al., 2020; Spigler et al., 2020).
The replica method for kernel methods has been developed in Dietrich et al. (1999), and recently
in Bordelon et al. (2020); Canatar et al. (2021). These recent works showed excellent agreement
between theory and experiments on NTK regression, which enables us to quantitatively understand
sample-size dependencies including implicit spectral bias, double descent, and multiple descent."
RELATED WORK,0.014102564102564103,"Continual learning. Continual learning dynamics in the NTK regime was ﬁrst formulated by Bennani
et al. (2020); Doan et al. (2021), though the evaluation of generalization remains unclear. They derived
an upper bound of generalization via the Rademacher complexity, but it includes naive summation
over single tasks and seems conservative. In contrast, our typical-case evaluation enables us to newly
ﬁnd such rich behaviors as negative/positive transfer and self-knowledge transfer/forgetting. The
continual learning in the NTK regime belongs to so-called single-head setting (Farquhar & Gal, 2018),
and it allows the model to revisit the previous classes (target functions) in subsequent tasks. This is
complementary to earlier studies on incremental learning of new classes and its catastrophic forgetting
(Ramasesh et al., 2020; Lee et al., 2021), where each task includes different classes and does not allow
the revisit. Note that the basic concept of continual learning is not limited to incremental learning
but allows the revisit (McCloskey & Cohen, 1989; Kirkpatrick et al., 2017). Under the limited data
acquisition or resources of memory, we often need to train the same model with the same targets (but
different samples) from task to task. Therefore, the setting allowing the revisit seems reasonable."
PRELIMINARIES,0.015384615384615385,"3
PRELIMINARIES"
NEURAL TANGENT KERNEL REGIME,0.016666666666666666,"3.1
NEURAL TANGENT KERNEL REGIME"
NEURAL TANGENT KERNEL REGIME,0.017948717948717947,"We summarize conventional settings of the NTK regime (Jacot et al., 2018; Lee et al., 2019). Let us
consider a fully connected neural network f = uL given by"
NEURAL TANGENT KERNEL REGIME,0.019230769230769232,"ul = σwWlhl−1/
p"
NEURAL TANGENT KERNEL REGIME,0.020512820512820513,"Ml−1 + σbbl, hl = φ(ul)
(l = 1, ..., L),
(1)"
NEURAL TANGENT KERNEL REGIME,0.021794871794871794,"where we deﬁne weight matrices Wl ∈RMl×Ml−1, bias terms bl ∈RMl, and their variances σ2
w
and σ2
b. We set random Gaussian initialization Wl,ij, bl,i ∼N(0, 1) and focus on the mean squared
error loss: L(θ) = PN
µ=1 ∥yµ −f(xµ)∥2, where the training samples {xµ, yµ}N
µ=1 are composed of"
NEURAL TANGENT KERNEL REGIME,0.023076923076923078,Published as a conference paper at ICLR 2022
NEURAL TANGENT KERNEL REGIME,0.02435897435897436,"inputs xµ ∈RD normalized by ∥xµ∥2 = 1 and labels yµ ∈RC. The set of all parameters is denoted
as θ, and the number of training samples is N. Assume the inﬁnite-width limit for hidden layers
(Ml →∞), ﬁnite sample size and depth. The gradient descent dynamics with a certain learning rate
then converges to a global minimum sufﬁciently close to the random initialization. This is known as
the NTK regime, and the trained model is explicitly obtained as"
NEURAL TANGENT KERNEL REGIME,0.02564102564102564,"f (c)(x′) = f (c)
0 (x′) + Θ(x′, X)Θ(X)−1(y(c) −f (c)
0 (X))
(c = 1, ..., C).
(2)
We denote the NTK matrix as Θ, arbitrary input samples (including test samples) as x′, and the set
of training samples as X. The indices of f mean 0 for the model at initialization and c for the head
of the network. Entries of NTK Θ(x′, x) are deﬁned by ∇θf0(x′)∇θf0(x)⊤. We write Θ(X) as an
abbreviation for Θ(X, X). The trained network is equivalent to a linearized model around random
initialization (Lee et al., 2019), that is, f (c) = f (c)
0
+ ∇θf (c)
0 ∆θ with"
NEURAL TANGENT KERNEL REGIME,0.026923076923076925,"∆θ = θ −θ0 = C
X"
NEURAL TANGENT KERNEL REGIME,0.028205128205128206,"c=1
∇θf (c)
0 (X)⊤Θ(X)−1(y(c) −f (c)
0 (X)).
(3)"
NEURAL TANGENT KERNEL REGIME,0.029487179487179487,"While over-parameterized models have many global minima, the NTK dynamics implicitly select
the above θ, which corresponds to the L2 min-norm solution. Usually, we ignore f0 by taking the
average over random initialization. The trained model (2) is then equivalent to the kernel ridge-less
regression (KRR)."
NEURAL TANGENT KERNEL REGIME,0.03076923076923077,"NTK regime also holds in various architectures including ResNets and CNNs (Yang & Littwin,
2021), and the difference only appears in the NTK matrix. Although we focus on the fully connected
network in synthetic experiments, the following NTK formulation of sequential training and our
theoretical results hold in any architecture under the NTK regime."
NEURAL TANGENT KERNEL REGIME,0.03205128205128205,"NTK formulation of Continual learning. We denote the set of training samples in the n-th task as
(Xn, yn) (n = 1, 2, ..., K), a model trained in a sequential manner from task 1 to task n as fn and its
parameters as θn. That is, we train the network initialized at θn−1 for the n-th task and obtain fn.
Assume that the number of tasks K is ﬁnite. The trained model within the NTK regime is then given
as follows (Bennani et al., 2020; Doan et al., 2021):
fn(x′) = fn−1(x′) + Θ(x′, Xn)Θ(Xn)−1(yn −fn−1(Xn)),
(4)"
NEURAL TANGENT KERNEL REGIME,0.03333333333333333,"θn −θn−1 = ∇θf0(Xn)⊤Θ(Xn)−1(yn −fn−1(Xn)).
(5)
We omit the index c because each head is updated independently. The model fn completely ﬁts
the n-th task, i.e., yn = fn(Xn). The main purpose of this study is to analyze the generalization
performance of the sequentially trained model (4). At each task, the model has an inductive bias of
KRR in the function space and L2 min-norm solution in the parameter space. The next task uses
this inductive bias as the initialization of training. The problem is whether this inductive bias helps
improve the generalization in the subsequent tasks."
NEURAL TANGENT KERNEL REGIME,0.03461538461538462,"Remark (independence between different heads).
For a more accurate understanding of the
continual learning in the NTK regime, it may be helpful to remark on the heads’ independence, which
previous studies did not explicitly mention. As in Eq. (2), all heads share the same NTK, and f (c)"
NEURAL TANGENT KERNEL REGIME,0.035897435897435895,"depends only on the label of its class y(c). While the parameter update (3) includes information of all
classes, the c-th head can access only the information of the c-th class1. For example, suppose that
the n-th task includes all classes except 1, i.e., {2, ..., C}. Then, the model update on the the class 1
at the n-th task, i.e., f 1
n −f 1
n−1, becomes"
NEURAL TANGENT KERNEL REGIME,0.03717948717948718,"∇θf (1)
0 (x′)(θn −θn−1) = ∇θf (1)
0 (x′)
X"
NEURAL TANGENT KERNEL REGIME,0.038461538461538464,"c=2
∇θf (c)
0 (Xn)⊤Θ(Xn)−1(y(c)
n −f (c)
n−1) = 0"
NEURAL TANGENT KERNEL REGIME,0.03974358974358974,"because ∇θf (c)
0 ∇θf (c′)⊤
0
= 0 (c ̸= c′) in the inﬁnite-width limit (Jacot et al., 2018; Yang, 2019).
Thus, we can deal with each head independently and analyze the generalization by setting C = 1
without loss of generality. This indicates that in the NTK regime, interaction between different
heads do not cause knowledge transfer and forgetting. One may wonder if there are any non-trivial
knowledge transfer and forgetting in such a regime. Contrary to such intuition, we reveal that when
the subsequent task revisits previous classes (targets), the generalization shows interesting increase
and decrease."
NEURAL TANGENT KERNEL REGIME,0.041025641025641026,"1We use the term “class”, although the regression problem is assumed in NTK theory. Usually, NTK studies
solve the classiﬁcation problem by regression with a target y(c) = {0, 1}."
NEURAL TANGENT KERNEL REGIME,0.04230769230769231,Published as a conference paper at ICLR 2022
LEARNING CURVE ON SINGLE-TASK TRAINING,0.04358974358974359,"3.2
LEARNING CURVE ON SINGLE-TASK TRAINING"
LEARNING CURVE ON SINGLE-TASK TRAINING,0.04487179487179487,"To evaluate the generalization performance of (4), we extend the following theory to our sequential
training. Bordelon et al. (2020) obtained an analytical expression of the generalization for NTK
regression on a single task (2) as follows. Assume that training samples are generated in an i.i.d.
manner ( xµ ∼p(x)) and that labels are generated from a square integrable target function ¯f:"
LEARNING CURVE ON SINGLE-TASK TRAINING,0.046153846153846156,"¯f(x) = ∞
X"
LEARNING CURVE ON SINGLE-TASK TRAINING,0.047435897435897434,"i=0
¯wiψi(x),
yµ = ¯f(xµ) + εµ
(µ = 1, ..., N),
(6)"
LEARNING CURVE ON SINGLE-TASK TRAINING,0.04871794871794872,"where ¯wi are constant coefﬁcients and ε represents Gaussian noise with ⟨εµεν⟩= δµνσ2. We deﬁne
ψi(x) := √ηiφi(x) with basis functions φi(x) given by Mercer’s decomposition:
Z
dx′p (x′) Θ (x, x′) φi (x′) = ηiφi(x)
(i = 0, 1, . . . , ∞).
(7)"
LEARNING CURVE ON SINGLE-TASK TRAINING,0.05,"Here, ηi denotes NTK’s eigenvalue and we assume the ﬁnite trance of NTK P"
LEARNING CURVE ON SINGLE-TASK TRAINING,0.05128205128205128,"i ηi < ∞. We
set η0 = 0 in the main text to avoid complicated notations.
We can numerically compute
eigenvalues by using the Gauss-Gegenbauer quadrature. Generalization error is expressed by
E1 :=
DR
dxp(x)
  ¯f(x) −f ∗(x)
2E"
LEARNING CURVE ON SINGLE-TASK TRAINING,0.052564102564102565,"D where f ∗is a trained model and ⟨· · · ⟩D is the average
over training samples. Bordelon et al. (2020) derived a typical-case evaluation of the generalization
error by using the replica method: for a sufﬁciently large N, we have asymptotically"
LEARNING CURVE ON SINGLE-TASK TRAINING,0.05384615384615385,"E1 =
1
1 −γ ∞
X"
LEARNING CURVE ON SINGLE-TASK TRAINING,0.05512820512820513,"i=0
ηi ¯w2
i"
LEARNING CURVE ON SINGLE-TASK TRAINING,0.05641025641025641,"
κ
κ + Nηi"
LEARNING CURVE ON SINGLE-TASK TRAINING,0.057692307692307696,"2
+
γ
1 −γ σ2.
(8)"
LEARNING CURVE ON SINGLE-TASK TRAINING,0.05897435897435897,"Although the replica method takes a large sample size N, Bordelon et al. (2020); Canatar et al. (2021)
reported that the analytical expression (8) coincides well with empirical results even for small N.
The constants κ and γ are deﬁned as follows and characterize the increase and decrease of E1: 1 = ∞
X i=0"
LEARNING CURVE ON SINGLE-TASK TRAINING,0.06025641025641026,"ηi
κ + Nηi
, γ = ∞
X i=0"
LEARNING CURVE ON SINGLE-TASK TRAINING,0.06153846153846154,"Nη2
i
(κ + Nηi)2 .
(9)"
LEARNING CURVE ON SINGLE-TASK TRAINING,0.06282051282051282,"The κ is a positive solution of the ﬁrst equation and obtained by numerical computation. The γ
satisﬁes 0 < γ < 1 by deﬁnition. For N = αDl (α > 0, l ∈N, D ≫1), we can analytically solve it
and obtain more detailed evaluation. For example, a positive κ decreases to zero as the sample size
increases and E1 also decreases for σ2 = 0. For σ2 > 0, the generalization error shows multiple
descent depending on the decay of eigenvalue spectra. Since multiple descent is not a main topic of
this paper, we brieﬂy summarize it in Section A.5 of the Supplementary Materials."
LEARNING CURVES BETWEEN TWO TASKS,0.0641025641025641,"4
LEARNING CURVES BETWEEN TWO TASKS"
LEARNING CURVES BETWEEN TWO TASKS,0.06538461538461539,"In this section, we analyze the NTK formulation of continual learning (4) between two tasks (K = 2).
One can also regard this setting as transfer learning. We sequentially train the model from task A to
task B, and each one has a target function deﬁned by"
LEARNING CURVES BETWEEN TWO TASKS,0.06666666666666667,"¯fA(x) =
X"
LEARNING CURVES BETWEEN TWO TASKS,0.06794871794871794,"i
¯wA,iψi(x), ¯fB(x) =
X"
LEARNING CURVES BETWEEN TWO TASKS,0.06923076923076923,"i
¯wB,iψi(x), [ ¯wA,i, ¯wB,i] ∼N(0, ηi"
LEARNING CURVES BETWEEN TWO TASKS,0.07051282051282051,"
1
ρ
ρ
1"
LEARNING CURVES BETWEEN TWO TASKS,0.07179487179487179,"
).
(10)"
LEARNING CURVES BETWEEN TWO TASKS,0.07307692307692308,"The target functions are dependent on each other and belong to the reproducing kernel Hilbert
space (RKHS). By denoting the RKHS by H, one can interpret the target similarity ρ as the inner
product ⟨¯fA, ¯fB⟩H/(∥¯fA∥H∥¯fB∥H) = ρ. These targets have dual representation such as ¯f(x) =
P
i αiΘ(x′
i, x) with i.i.d. Gaussian variables αi (Bordelon et al., 2020), as summarized in Section
E.1. We generate training samples by yµ
A = ¯fA(xµ
A) + εµ
A　(µ = 1, ..., NA) and yµ
B = ¯fB(xµ
B) +
εµ
B　(µ = 1, ..., NB), although we focus on the noise-less case (σ = 0) in this section. Input samples
xµ
A and xµ
B are i.i.d. and generated by the same distribution p(x). We can measure the generalization
error in two ways: generalization error on subsequent task B and that on previous task A:"
LEARNING CURVES BETWEEN TWO TASKS,0.07435897435897436,"EA→B(ρ) =
Z
dxp(x)( ¯fB(x) −fA→B(x))2

,
(11)"
LEARNING CURVES BETWEEN TWO TASKS,0.07564102564102564,"Eback
A→B(ρ) =
Z
dxp(x)( ¯fA(x) −fA→B(x))2

,
(12)"
LEARNING CURVES BETWEEN TWO TASKS,0.07692307692307693,Published as a conference paper at ICLR 2022
LEARNING CURVES BETWEEN TWO TASKS,0.0782051282051282,"where we write f2 as fA→B to emphasize the sequential training from A to B. The notation Eback
A→B(ρ)
is referred to as backward transfer (Lopez-Paz & Ranzato, 2017). Large negative backward transfer
is known as catastrophic forgetting. We take the average ⟨· · · ⟩over training samples of two tasks
{DA, DB}, and target coefﬁcients ¯w. In fact, we can set ¯w as constants, and it is unnecessary to take
the average. To avoid complicated notation, we take the average in the main text. We then obtain the
following result.
Theorem 1. Using the replica method under sufﬁciently large NA and NB, for σ = 0, we have"
LEARNING CURVES BETWEEN TWO TASKS,0.07948717948717948,"EA→B(ρ) =
X i """
LEARNING CURVES BETWEEN TWO TASKS,0.08076923076923077,"2(1 −ρ) (1 −qA,i) +
q2
A,i
1 −γA #"
LEARNING CURVES BETWEEN TWO TASKS,0.08205128205128205,"EB,i,
(13)"
LEARNING CURVES BETWEEN TWO TASKS,0.08333333333333333,"Eback
A→B(ρ) =
X i """
LEARNING CURVES BETWEEN TWO TASKS,0.08461538461538462,"2(1 −ρ)(1 + FγB(qA,i, qB,i))η2
i +
q2
A,i
1 −γA
EB,i #"
LEARNING CURVES BETWEEN TWO TASKS,0.0858974358974359,",
(14)"
LEARNING CURVES BETWEEN TWO TASKS,0.08717948717948718,"where we deﬁne qA,i = κA/(κA + NAηi), qB,i = κB/(κB + NBηi), EB,i = q2
B,iη2
i /(1 −γB) and
FγB(a, b) = b(a −2) + b2(1 −a)/(1 −γB)."
LEARNING CURVES BETWEEN TWO TASKS,0.08846153846153847,"The constants κA and γA (κB and γB, respectively) are given by setting N = NA(NB) in (9). The
detailed derivation is given in Section A. Technically speaking, we use the following lemma:
Lemma 2. Denote the trained model (2) on single task A as fA = P
i w∗
A,iψi, and deﬁne the
following cost function: E =

P"
LEARNING CURVES BETWEEN TWO TASKS,0.08974358974358974,"i φi(w∗
A,i −ui)2"
LEARNING CURVES BETWEEN TWO TASKS,0.09102564102564102,"DA for arbitrary constants φi and ui. Using the
replica method under a sufﬁciently large NA, we have E =
X i"
LEARNING CURVES BETWEEN TWO TASKS,0.09230769230769231,"
( ¯wA,i −ui)2 −2 ¯wA,i( ¯wA,i −ui)qA,i +

¯w2
A,i + ηiNA"
LEARNING CURVES BETWEEN TWO TASKS,0.09358974358974359,"κ2
A
(EA + σ2)

q2
A,i 
φi,"
LEARNING CURVES BETWEEN TWO TASKS,0.09487179487179487,where EA denotes generalization error E1 on single task A.
LEARNING CURVES BETWEEN TWO TASKS,0.09615384615384616,"For example, we can see that EA is a special case of E with φi = ηi and u = ¯wA, and that EA→B(ρ)
is reduced to φi = q2
B,iη2
i /(1 −γB) and u = ¯wB after certain calculation."
LEARNING CURVES BETWEEN TWO TASKS,0.09743589743589744,"Spectral Bias. The generalization errors obtained in Theorem 1 are given by the summation over
spectral modes like the single-task case. The EB,i corresponds to the i-th mode of generalization
error (8) on single task B. The study on the single task (Bordelon et al., 2020) revealed that as the
sample size increases, the modes of large eigenvalues decreases ﬁrst. This is known as spectral
bias and clariﬁes the inductive bias of the NTK regression. Put the eigenvalues in descending order,
i.e., λi ≥λi+1. When D is sufﬁciently large and NB = αDl (α > 0, l ∈N), the error of each
mode is asymptotically given by EB,i = 0 (i < l) and η2
i (i > l). We see that the spectral bias also
holds in EA→B because it is a weighted sum over EB,i. In contrast, Eback
A→B includes a constant term
2(1 −ρ)η2
i . This constant term causes catastrophic forgetting, as we show later."
TRANSITION FROM NEGATIVE TO POSITIVE TRANSFER,0.09871794871794871,"4.1
TRANSITION FROM NEGATIVE TO POSITIVE TRANSFER"
TRANSITION FROM NEGATIVE TO POSITIVE TRANSFER,0.1,"We now look at more detailed behaviors of generalization errors obtained in Theorem 1. We ﬁrst
discuss the role of the target similarity ρ for improving generalization. Figure 1(a) shows the
comparison of the generalization between single-task training and sequential training. Solid lines
show theory, and markers show experimental results of trained neural networks in the NTK regime.
We trained the model (1) with ReLU activation, L = 3, and Ml = 4, 000 by using the gradient descent
over 50 trials. More detailed settings of our experiments are summarized in Section E. Because
we set NA = NB = 100, we have EA = EB. The point is that both EA→B(ρ) and Eback
A→B(ρ) are
lower than EA(= EB) for large ρ. This means that the sequential training degrades generalization if
the targets are dissimilar, that is, negative transfer. In particular, Eback
A→B(ρ) rapidly deteriorates for
the dissimilarity of targets. Note that both EA→B and Eback
A→B are linear functions of ρ. Figure 1(a)
indicates that the latter has a large slope. We can gain quantitative insight into the critical value of ρ
for the negative transfer as follows."
TRANSITION FROM NEGATIVE TO POSITIVE TRANSFER,0.10128205128205128,Knowledge transfer. The following asymptotic equation gives us the critical value for EA→B:
TRANSITION FROM NEGATIVE TO POSITIVE TRANSFER,0.10256410256410256,"EA→B(ρ)/EB ∼2(1 −ρ)
for NA ≫NB.
(15)"
TRANSITION FROM NEGATIVE TO POSITIVE TRANSFER,0.10384615384615385,Published as a conference paper at ICLR 2022
TRANSITION FROM NEGATIVE TO POSITIVE TRANSFER,0.10512820512820513,"ρ
(b)
(a)"
TRANSITION FROM NEGATIVE TO POSITIVE TRANSFER,0.1064102564102564,"Figure 1: (a) Transitions from positive to negative transfer caused by target similarity ρ. We set
NA = NB. (b) Learning curves show negative transfer (EA→B/EB) in a highly non-linear way
depending on unbalanced sample sizes. We changed NA and set NB = 103."
TRANSITION FROM NEGATIVE TO POSITIVE TRANSFER,0.1076923076923077,"A straightforward algebra leads to this (Section A.3). In the context of transfer learning, it is
reasonable that the target domain has a limited sample size compared to the ﬁrst task. For ρ > 1/2, we
have EA→B < EB, that is, previous task A contributes to improving the generalization performance
on subsequent task B (i.e., positive transfer). For ρ < 1/2, however, negative transfer appears."
TRANSITION FROM NEGATIVE TO POSITIVE TRANSFER,0.10897435897435898,"The following sufﬁcient condition for the negative transfer is also noteworthy. By evaluating EA→B >
EB, we can prove that for any NA and NB, the negative transfer always appears for
ρ < ρ∗:= √γA/(1 + √γA).
(16)
This is just a sufﬁcient condition and may be loose. For example, we asymptotically have the critical
target similarity ρ = 1/2 > ρ∗for NA ≫NB. Nevertheless, this sufﬁcient condition is attractive in
the sense that it clariﬁes the unavoidable negative transfer for the small target similarity."
TRANSITION FROM NEGATIVE TO POSITIVE TRANSFER,0.11025641025641025,"Backward transfer. The Eback
A→B(0) includes the constant term P"
TRANSITION FROM NEGATIVE TO POSITIVE TRANSFER,0.11153846153846154,"i η2
i independent of sample sizes.
Note that qA and qB decrease to zero for large sample sizes (Bordelon et al., 2020), and we have
Eback
A→B(ρ) ∼P"
TRANSITION FROM NEGATIVE TO POSITIVE TRANSFER,0.11282051282051282,"i η2
i (1 −ρ). In contrast, EA converges to zero for a large NA. Therefore, the
intersection between Eback
A→B(ρ) and EA reach ρ = 1 as NA and NB increase. This means that when
we have a sufﬁcient number of training samples, negative backward transfer (Eback
A→B(ρ) > EA) occurs
even for very similar targets. Figure 1(a) conﬁrms this result, and Figure 6 in Section A.6 shows more
detailed learning curves of backward transfer. Catastrophic forgetting seems literally “catastrophic”
in the sense that the backward transfer rapidly deteriorates by the subtle target dissimilarity."
SELF-KNOWLEDGE TRANSFER,0.1141025641025641,"4.2
SELF-KNOWLEDGE TRANSFER"
SELF-KNOWLEDGE TRANSFER,0.11538461538461539,"We have shown that target similarity is a key factor for knowledge transfer. We reveal that the sample
size is another key factor. To clarify the role of sample sizes, we focus on the same target function
(ρ = 1) in the following analysis. We refer to the knowledge transfer in this case as self-knowledge
transfer to emphasize the network learning the same function by the same head. As is the same in
ρ < 1, the knowledge obtained in the previous task is transferred as the network’s initialization for
the subsequent training and determines the eventual performance."
SELF-KNOWLEDGE TRANSFER,0.11666666666666667,"Positive transfer by equal sample sizes. We ﬁnd that positive transfer is guaranteed under equal
sample sizes, that is, NA = NB. To characterize the advantage of the sequential training, we compare
it with a model average: (fA + fB)/2, where fA (fB) means the model obtained by a single-task
training on A (B). Note that since the model is a linear function of the parameter in the NTK regime,
this average is equivalent to that of the trained parameters: (θA + θB)/2. After straightforward
calculation in Section D, the generalization error of the model average is given by
Eave = (1 −γB/2) EB.
(17)
Sequential training and model average include information of both tasks A and B; thus, it is interesting
to compare it with EA→B. We ﬁnd
Proposition 3. For ρ = 1 and any NA = NB,
EA→B(1) < Eave < EA = EB.
(18)"
SELF-KNOWLEDGE TRANSFER,0.11794871794871795,Published as a conference paper at ICLR 2022
SELF-KNOWLEDGE TRANSFER,0.11923076923076924,"The derivation is given in Section D. This proposition clariﬁes the superiority of sequential training
over single-task training and even the average. The ﬁrst task contributes to the improvement on the
second task; thus, we have positive transfer."
SELF-KNOWLEDGE TRANSFER,0.12051282051282051,"Negative transfer by unbalanced sample sizes. While equal sample size leads to positive transfer,
the following unbalanced sample sizes cause the negative transfer of self-knowledge:
EA→B(1)/EB ∼1/(1 −γA)
for NB ≫NA.
(19)
The derivation is based on Jensen’s inequality (Section A.3). While EA→B and EB asymptotically
decrease to zero for the large NB, their ratio remains ﬁnite. Because 0 < γA < 1, EA→B(1) > EB.
It indicates that the small sample size of task A leads to a bad initialization of subsequent training
and makes the training on task B hard to ﬁnd a better solution."
SELF-KNOWLEDGE TRANSFER,0.12179487179487179,"Figure 1(b) summarizes the learning curves which depend on sample sizes in a highly non-linear
way. Solid lines show theory, and markers show the results of NTK regression over 100 trials. The
ﬁgure shows excellent agreement between theory and experiments. Although we have complicated
transitions from positive to negative transfer, our theoretical analyses capture the basic characteristics
of the learning curves. For self-knowledge transfer (ρ = 1), we can achieve positive transfer at
NA/NB = 1, as shown in Proposition 3, and for large NA/NB, as shown in (15). In contrast,
negative transfer appears for small NA/NB, as shown in (19). For ρ < 1, large NA/NB produces
positive transfer for ρ < 1/2, as shown in (15)."
SELF-KNOWLEDGE TRANSFER,0.12307692307692308,"If we set a relatively large σ > 0, the learning curve may become much more complicated due
to multiple descent. Figure 5 in Section A.5 conﬁrms the case in which multiple descent appears
in EA→B. The curve shape is generally characterized by the interaction among target similarity,
self-knowledge transfer depending on sample size, and multiple descent caused by the noise."
SELF-KNOWLEDGE FORGETTING,0.12435897435897436,"4.3
SELF-KNOWLEDGE FORGETTING"
SELF-KNOWLEDGE FORGETTING,0.12564102564102564,"Figure 2:
Self-knowledge forget-
ting: unbalanced sample sizes de-
grade generalization."
SELF-KNOWLEDGE FORGETTING,0.12692307692307692,"We have shown in Section 4.1 that backward transfer is likely
to cause catastrophic forgetting for ρ < 1. We show that even
for ρ = 1, sequential training causes forgetting. That is, the
training on task B degrades generalization even though both
tasks A and B learn the same target."
SELF-KNOWLEDGE FORGETTING,0.1282051282051282,"We have Eback
A→B(1) = EA→B(1) by deﬁnition, and Proposi-
tion 3 tells us that EA→B < EA. Therefore, no forgetting
appears for equal sample sizes. In contrast, we have
EA→B(1)/EA ∼1/(1 −γB)
for NA ≫NB.
(20)
One can obtain this asymptotic equation in the same manner
as (19) since EA→B(1) is a symmetric function in terms of
indices A and B. Combining (20) with (15), we have EA <
EA→B(1) ≪EB. Sequential training is better than using
only task B, but training only on the ﬁrst task is the best. One
can say that the model forgets the target despite learning the
same one. We call this self-knowledge forgetting. In the
context of continual learning, many studies have investigated the catastrophic forgetting caused
by different heads (i.e., in the situation of incremental learning). Our results suggest that even the
sequential training on the same task and the same head shows such forgetting. Intuitively speaking,
the self-knowledge forgetting is caused by the limited sample size of task B. Note that we have
EA→B(1) = P"
SELF-KNOWLEDGE FORGETTING,0.1294871794871795,"i q2
A,iEB,i/(1 −γA). The generalization error of single-task training on task B
(EB,i) takes a large value for a small NB and this causes the deterioration of EA→B as well. Figure
2 conﬁrms the self-knowledge forgetting in NTK regression. We set NA = NB as the red line and
NB = 100 as the yellow line. The dashed line shows the point where forgetting appears."
LEARNING CURVES OF MANY TASKS,0.13076923076923078,"5
LEARNING CURVES OF MANY TASKS"
LEARNING CURVES OF MANY TASKS,0.13205128205128205,"We can generalize the sequential training between two tasks to that of more tasks. For simplicity, let us
focus on the self-knowledge transfer (ρ = 1) and equal sample sizes. Applying Lemma 2 recursively
from task to task, we can evaluate generalization deﬁned by En =

R
dxp(x)( ¯f(x) −fn(x))2 D."
LEARNING CURVES OF MANY TASKS,0.13333333333333333,Published as a conference paper at ICLR 2022
LEARNING CURVES OF MANY TASKS,0.1346153846153846,"Theorem 4. Assume that (i) (Xn, yn) (n = 1, ..., K) are given by the same distribution Xn ∼P(X)
and target yn = P"
LEARNING CURVES OF MANY TASKS,0.1358974358974359,"i ¯wiψ(Xn) + εn. (ii) sample sizes are equal: Nn = N. For n = 1, ..., K,"
LEARNING CURVES OF MANY TASKS,0.1371794871794872,"En+1 =
1
(1 −γ)2 ˜q⊤Qn−1˜q + Rn+1σ2,
Q := diag
 
q2
+
N
(1 −γ)κ2 ˜q˜q⊤,
(21)"
LEARNING CURVES OF MANY TASKS,0.13846153846153847,"where qi = κ/(κ + ηiN), ˜qi = ηiq2
i and diag(q2) denotes a diagonal matrix whose entries are q2
i .
The noise term Rn is a positive constant. In the noise-less case (σ = 0), the learning curve shows
monotonic decrease: En+1 ≤En. If all eigenvalues are positive, we have
En+1 < En
(n = 1, 2, ...).
(22)"
LEARNING CURVES OF MANY TASKS,0.13974358974358975,"See Section B for details of derivation. The learning curve (i.e., generalization error to the number
of tasks) monotonically decreases for the noise-less case. the monotonic decrease comes from
λmax(Q) < 1. This result means that the self-knowledge is transferred and accumulated from task to
task and contributes in improving generalization. It also ensures that no self-knowledge forgetting
appears. We can also show that Rn converges to a positive constant term for a large n and the
contribution of noise remains as a constant."
LEARNING CURVES OF MANY TASKS,0.14102564102564102,"KRR-like expression. The main purpose of this work was to address the generalization of the
continually trained model fn. As a side remark, we show another expression of fn:"
LEARNING CURVES OF MANY TASKS,0.1423076923076923,"[Θ (x′, X1) · · · Θ (x′, Xn)]  "
LEARNING CURVES OF MANY TASKS,0.14358974358974358,"Θ (X1)
O
· · ·
O"
LEARNING CURVES OF MANY TASKS,0.14487179487179488,"Θ (X2, X1)
Θ (X2)
...
...
...
O
Θ (Xn, X1)
· · ·
Θ (Xn)   −1  "
LEARNING CURVES OF MANY TASKS,0.14615384615384616,"y1
...
yn "
LEARNING CURVES OF MANY TASKS,0.14743589743589744,".
(23)"
LEARNING CURVES OF MANY TASKS,0.14871794871794872,"This easily comes from comparing the update (4) with a formula for the inversion of triangular block
matrices (Section C). One can see this expression as an approximation of KRR, where the upper
triangular block of NTK is set to zero. Usual modiﬁcation of KRR is diagonal, e.g., L2 regularization
and block approximation, and it seems that the generalization error of this type of model has never
been explored. Our result revealed that this model provides non-trivial sample size dependencies
such as self-knowledge transfer and forgetting."
EXPERIMENTS,0.15,"5.1
EXPERIMENTS"
EXPERIMENTS,0.15128205128205127,"Although Theorem 4 guarantees the monotonic learning curve for equal sizes, unbalanced sample
sizes should cause a non-monotonic learning curve. We empirically conﬁrmed this below."
EXPERIMENTS,0.15256410256410258,"Synthetic data.
First, we empirically conﬁrm En on synthetic data (10). Figure 3(a1) conﬁrms
the claim of Theorem 4 that the generalization error monotonically decreases for σ = 0 as the
task number increases. Dashed lines are theoretical values calculated using the theorem, and points
with error bars were numerically obtained by fn (4) over 100 trials. For σ > 0, the decrease was
suppressed. We set σ2 = {0, 10−5, 10−4, 10−3} and Ni = 100. Figure 3(a2) shows self-knowledge
forgetting. When the ﬁrst task has a large sample size, the generalization error by the second task can
increase for small subsequent sample sizes Ni. For smaller Ni, there was a tendency for the error to
keep increasing and taking higher errors than that of the ﬁrst task during several tasks. In practice,
one may face a situation where the model is initialized by the ﬁrst task training on many samples and
then trained in a continual learning manner under a memory constraint. The ﬁgure suggests that if
the number of subsequent tasks is limited, we need only the training on the ﬁrst task. If we have a
sufﬁciently large number of tasks, generalization eventually improves."
EXPERIMENTS,0.15384615384615385,"MLP on MNIST / ResNet-18 on CIFAR-10. We mainly focus on the theoretical analysis in the
NTK regime, but it will be interesting to investigate whether our results also hold in more practical
settings of deep learning. We trained MLP (fully-connected neural networks with 4 hidden layers)
and ResNet-18 with stochastic gradient descent (SGD) and cross-entropy loss. We set the number of
epochs sufﬁcient for the training error to converge to zero for each task. We conﬁrmed that they show
qualitatively similar results as in the NTK regime. We randomly divided the dataset into tasks without
overlap of training samples. Figures 3(b1,c1) show the monotonic decrease for an equal sample
size and that the noise suppressed the decrease. We set Ni = 500 and generated the noise by the
label corruption with a corruption probability {0, 0.2, ..., 0.8} (Zhang et al., 2017). The vertical axis
means the error, i.e., 1 −(Test accuracy [%])/100. Figures 3(b2,c2) show that unbalanced sample
sizes caused the non-monotonic learning curve, similar to NTK regression."
EXPERIMENTS,0.15512820512820513,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.1564102564102564,"(a1)
(b1)
(c1)"
EXPERIMENTS,0.1576923076923077,"(a2)
(b2)
(c2)"
EXPERIMENTS,0.15897435897435896,"NTK
MLP
ResNet-18"
EXPERIMENTS,0.16025641025641027,"Figure 3: (a1)-(c1) Learning curves for equal sample sizes. For noise-less case, they monotonically
decreased (orange lines). For noisy case, decrease was suppressed (grey lines; we plotted learning
curves for several σ2 and those with larger test errors correspond to larger σ2). We trained MLP
on MNIST and ResNet-18 on CIFAR-10. (a2)-(c2) Learning curves for unbalanced sample sizes
were non-monotonic (N1 = 4, 000 for NTK regression, N1 = 104 for SGD training of MLP and
ResNet-18). Numbers in the legend mean Nn
(n ≥2)."
DISCUSSION,0.16153846153846155,"6
DISCUSSION"
DISCUSSION,0.16282051282051282,"We provided novel quantitative insight into knowledge transfer and forgetting in the sequential
training of neural networks. Even in the NTK regime, where the model is simple and linearized,
learning curves show rich and non-monotonic behaviors depending on both target similarity and
sample size. In particular, learning on the same target shows successful self-knowledge transfer
or undesirable forgetting depending on the balance of sample sizes. These results indicate that the
performance of the sequentially trained model is more complicated than we thought, but we can still
ﬁnd some universal laws behind it."
DISCUSSION,0.1641025641025641,"There are other research directions to be explored. While we focused on reporting novel phenomena
on transfer and forgetting, it is also important to develop algorithms to achieve better performance. To
mitigate catastrophic forgetting, previous studies proposed several strategies such as regularization,
parameter isolation, and experience replay (Mirzadeh et al., 2020). Evaluating such strategies with
theoretical backing would be helpful for further development of continual learning. For example,
orthogonal projection methods modify gradient directions (Doan et al., 2021), and replay methods
allow the reuse of the previous samples. We conjecture that these could be analyzed by extending our
calculations in a relatively straightforward way. It would also be interesting to investigate richer but
complicated situations required in practice, such as streaming of non-i.i.d. data and distribution shift
(Aljundi et al., 2019). The current work and other theories in continual learning or transfer learning
basically assume the same input distribution between different tasks (Lee et al., 2021; Tripuraneni
et al., 2020). Extending these to the case with an input distribution shift will be essential for some
applications including domain incremental learning. Our analysis may also be extended to topics
different from sequential training. For example, self-distillation uses trained model’s outputs for the
subsequent training and plays an interesting role of regularization (Mobahi et al., 2020)."
DISCUSSION,0.16538461538461538,"While our study provides universal results, which do not depend on speciﬁc eigenvalue spectra or
architectures, it is interesting to investigate individual cases. Studies on NTK eigenvalues have made
remarkable progress, covering shallow and deep ReLU neural networks (Geifman et al., 2020; Chen
& Xu, 2020), skip connections (Belfer et al., 2021), and CNNs (Favero et al., 2021). We expect that
our analysis and ﬁndings will serve as a foundation for further understanding and development on
theory and experiments of sequential training."
DISCUSSION,0.16666666666666666,Published as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.16794871794871793,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.16923076923076924,"The main contributions of this work are theoretical claims, and we clearly explained their assumptions
and settings in the main text. Complete proofs of the claims are given in the Supplementary Materials.
For experimental results of training deep neural networks, we used only already-known models and
algorithms implemented in PyTorch. All of the detailed settings, including learning procedures and
hyperparameters, are clearly explained in Section E."
REPRODUCIBILITY STATEMENT,0.17051282051282052,ACKNOWLEDGMENTS
REPRODUCIBILITY STATEMENT,0.1717948717948718,"This work was funded by JST ACT-X Grant Number JPMJAX190A and JSPS KAKENHI Grant
Number 19K20366."
REFERENCES,0.17307692307692307,REFERENCES
REFERENCES,0.17435897435897435,"Alia Abbaras, Benjamin Aubin, Florent Krzakala, and Lenka Zdeborová. Rademacher complexity
and spin glasses: A link between the replica and statistical theories of learning. In Mathematical
and Scientiﬁc Machine Learning (MSML), pp. 27–54. PMLR, 2020."
REFERENCES,0.17564102564102563,"Rahaf Aljundi, Klaas Kelchtermans, and Tinne Tuytelaars. Task-free continual learning. In IEEE/CVF"
REFERENCES,0.17692307692307693,"Conference on Computer Vision and Pattern Recognition (CVPR), pp. 11254–11263, 2019."
REFERENCES,0.1782051282051282,"Yuval Belfer, Amnon Geifman, Meirav Galun, and Ronen Basri. Spectral analysis of the neural
tangent kernel for deep residual networks. arXiv preprint arXiv:2104.03093, 2021."
REFERENCES,0.1794871794871795,"Mehdi Abbana Bennani, Thang Doan, and Masashi Sugiyama. Generalisation guarantees for continual
learning with orthogonal gradient descent. arXiv preprint arXiv:2006.11942, 2020."
REFERENCES,0.18076923076923077,"Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan. Spectrum dependent learning curves
in kernel regression and wide neural networks. In International Conference on Machine Learning
(ICML), pp. 1024–1034. PMLR, 2020."
REFERENCES,0.18205128205128204,"Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan. Spectral bias and task-model align-
ment explain generalization in kernel regression and inﬁnitely wide neural networks. Nature
communications, 12(1):1–12, 2021."
REFERENCES,0.18333333333333332,Lin Chen and Sheng Xu. Deep neural tangent kernel and Laplace kernel have the same RKHS. In
REFERENCES,0.18461538461538463,"International Conference on Learning Representations (ICLR), 2020."
REFERENCES,0.1858974358974359,"Rainer Dietrich, Manfred Opper, and Haim Sompolinsky. Statistical mechanics of support vector
networks. Physical review letters, 82(14):2975, 1999."
REFERENCES,0.18717948717948718,"Thang Doan, Mehdi Abbana Bennani, Bogdan Mazoure, Guillaume Rabusseau, and Pierre Alquier.
A theoretical analysis of catastrophic forgetting through the NTK overlap matrix. In International
Conference on Artiﬁcial Intelligence and Statistics (AISTATS), pp. 1072–1080. PMLR, 2021."
REFERENCES,0.18846153846153846,Sebastian Farquhar and Yarin Gal. Towards robust evaluations of continual learning. arXiv preprint
REFERENCES,0.18974358974358974,"arXiv:1805.09733, 2018."
REFERENCES,0.191025641025641,"Alessandro Favero, Francesco Cagnetta, and Matthieu Wyart. Locality defeats the curse of dimen-
sionality in convolutional teacher-student scenarios. arXiv preprint arXiv:2106.08619, 2021."
REFERENCES,0.19230769230769232,"Amnon Geifman, Abhay Yadav, Yoni Kasten, Meirav Galun, David Jacobs, and Ronen Basri. On
the similarity between the Laplace and neural tangent kernels. In Advances in neural information
processing systems (NeurIPS), pp. 1451–1461, 2020."
REFERENCES,0.1935897435897436,"Raia Hadsell, Dushyant Rao, Andrei A Rusu, and Razvan Pascanu. Embracing change: Continual
learning in deep neural networks. Trends in cognitive sciences, 2020."
REFERENCES,0.19487179487179487,"Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in neural information processing systems (NeurIPS),
pp. 8571–8580, 2018."
REFERENCES,0.19615384615384615,Published as a conference paper at ICLR 2022
REFERENCES,0.19743589743589743,"James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming
catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114
(13):3521–3526, 2017."
REFERENCES,0.1987179487179487,"Jaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri, Jascha Sohl-Dickstein, and Jeffrey
Pennington. Wide neural networks of any depth evolve as linear models under gradient descent. In
Advances in neural information processing systems (NeurIPS), pp. 8572–8583, 2019."
REFERENCES,0.2,"Sebastian Lee, Sebastian Goldt, and Andrew Saxe. Continual learning in the teacher-student setup:
Impact of task similarity. In International Conference on Machine Learning (ICML), pp. 6109–
6119. PMLR, 2021."
REFERENCES,0.2012820512820513,David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. In
REFERENCES,0.20256410256410257,"Advances in Neural Information Processing Systems (NIPS), pp. 6467–6476, 2017."
REFERENCES,0.20384615384615384,"Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The
sequential learning problem. In Psychology of learning and motivation, volume 24, pp. 109–165.
Elsevier, 1989."
REFERENCES,0.20512820512820512,"Seyed Iman Mirzadeh, Mehrdad Farajtabar, Dilan Gorur, Razvan Pascanu, and Hassan Ghasemzadeh.
Linear mode connectivity in multitask and continual learning. In International Conference on
Learning Representations (ICLR), 2020."
REFERENCES,0.2064102564102564,"Hossein Mobahi, Mehrdad Farajtabar, and Peter L Bartlett. Self-distillation ampliﬁes regularization in
Hilbert space. In Advances in Neural Information Processing Systems (NeurIPS), pp. 3351–3361,
2020."
REFERENCES,0.2076923076923077,"Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep
double descent: Where bigger models and more data hurt. In International Conference on Learning
Representations (ICLR), 2020."
REFERENCES,0.20897435897435898,"Anastasia Pentina and Christoph Lampert.
A PAC-Bayesian bound for lifelong learning.
In
International Conference on Machine Learning (ICML), pp. 991–999. PMLR, 2014."
REFERENCES,0.21025641025641026,"Vinay Venkatesh Ramasesh, Ethan Dyer, and Maithra Raghu. Anatomy of catastrophic forgetting: Hid-
den representations and task semantics. In International Conference on Learning Representations
(ICLR), 2020."
REFERENCES,0.21153846153846154,"Stefano Spigler, Mario Geiger, and Matthieu Wyart. Asymptotic learning curves of kernel methods:
empirical data versus teacher–student paradigm. Journal of Statistical Mechanics: Theory and
Experiment, 2020(12):124001, 2020."
REFERENCES,0.2128205128205128,"Nilesh Tripuraneni, Michael Jordan, and Chi Jin. On the theory of transfer learning: The importance of
task diversity. In Advances in Neural Information Processing Systems (NeurIPS), pp. 7852–7862,
2020."
REFERENCES,0.2141025641025641,"Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior,
gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760,
2019."
REFERENCES,0.2153846153846154,"Greg Yang and Etai Littwin. Tensor programs IIb: Architectural universality of neural tangent kernel
training dynamics. In International Conference on Machine Learning (ICML), pp. 11762–11772.
PMLR, 2021."
REFERENCES,0.21666666666666667,"Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understand-
ing deep learning requires rethinking generalization. In International Conference on Learning
Representations (ICLR), 2017."
REFERENCES,0.21794871794871795,Published as a conference paper at ICLR 2022
REFERENCES,0.21923076923076923,Supplementary Materials
REFERENCES,0.2205128205128205,"A
SEQUENTIAL TRAINING BETWEEN TWO TASKS"
REFERENCES,0.22179487179487178,"A.1
NOTATIONS"
REFERENCES,0.2230769230769231,The kernel ridge(-less) regression is given by
REFERENCES,0.22435897435897437,"f ∗= arg min
f∈H 1
2λ N
X"
REFERENCES,0.22564102564102564,"µ=1
(f(xµ) −yµ)2 + 1"
REFERENCES,0.22692307692307692,"2⟨f, f⟩H
(S.1)"
REFERENCES,0.2282051282051282,"where N is the sample size and H is the reproducing kernel Hilbert space (RKHS) induced by the
neural tangent kernel Θ. We assume that the input samples xµ are generated from a probabilistic dis-
tribution p(x) and that the NTK has ﬁnite trace, i.e.,
R
dxp(x)Θ(x, x) < ∞. Mercer’s decomposition
of Θ is expressed by
Z
dx′p (x′) Θ (x, x′) φi (x′) = ηiφi(x)
(i = 0, 1, . . . , ∞).
(S.2)"
REFERENCES,0.22948717948717948,"In other words, we have Θ(x′, x) = P∞
i=0 ηiφi(x′)φi(x). Note that a function belonging to RKHS
is given by"
REFERENCES,0.23076923076923078,"f(x) = ∞
X"
REFERENCES,0.23205128205128206,"i=0
aiφi(x)
(S.3)"
REFERENCES,0.23333333333333334,"with ∥f∥2
H = P∞
i=0 a2
i /ηi < ∞. Then, using the orthonormal bases {φi}∞
i=0, the solution of the
regression is given by"
REFERENCES,0.23461538461538461,"f ∗(x) =
X"
REFERENCES,0.2358974358974359,"i
w∗
i ψi(x), w∗=
 
ΨΨ⊤+ λI
−1 Ψy
(S.4)"
REFERENCES,0.23717948717948717,"where ψi(x) := √ηiφi(x). Each column of Ψ is given by φ(xµ) (µ = 1, ..., N). We focus on the
NTK regime and take the ridge-less limit (λ →0)."
REFERENCES,0.23846153846153847,The sequentially trained model (4) between tasks A and B is written as
REFERENCES,0.23974358974358975,"fA→B(x) −fA(x) = Θ(x, XB)Θ(XB)−1 (yB −fA(XB)) ,
(S.5)"
REFERENCES,0.24102564102564103,"fA(x) = Θ(x, XA)Θ(XA)−1yA.
(S.6)
The model fA is trained on single task A and represented by"
REFERENCES,0.2423076923076923,"fA(x) = w∗⊤
A ψ(x)
(S.7)"
REFERENCES,0.24358974358974358,"with
w∗
A = lim
λ→0 argminwA HA(wA),
(S.8)"
REFERENCES,0.24487179487179486,"HA(wA) := 1 2λ NA
X µ=1"
REFERENCES,0.24615384615384617," 
w⊤
Aψ(xµ
A) −yµ
A
2 + 1"
REFERENCES,0.24743589743589745,"2∥wA∥2
2.
(S.9)"
REFERENCES,0.24871794871794872,"Eq. (S.9 ) is the objective function equivalent to (S.1 ) because ⟨fA, fA⟩H = ∥wA∥2
2. Similarly, we
can represent fA→B by the series expansion with the bases of RKHS. Note that the right-hand side of
(S.5 ) is equivalent to the kernel ridge-less regression on input samples XB and labels yB −fA(B).
We have"
REFERENCES,0.25,"fA→B(x) = (w∗
A + w∗
B)⊤ψ(x)
(S.10)"
REFERENCES,0.2512820512820513,"with
w∗
B = lim
λ→0 argminwB H(wB, w∗
A),
(S.11)"
REFERENCES,0.25256410256410255,"H(wB, w∗
A) := 1 2λ NB
X µ=1"
REFERENCES,0.25384615384615383," 
w⊤
Bψ(xµ
B) −(yµ
B −w∗⊤
A ψ(xµ
B))
2 + 1"
REFERENCES,0.2551282051282051,"2∥wB∥2
2.
(S.12)"
REFERENCES,0.2564102564102564,Published as a conference paper at ICLR 2022
REFERENCES,0.25769230769230766,"A.2
PROOF OF THEOREM 1."
REFERENCES,0.258974358974359,"A.2.1
KNOWLEDGE TRANSFER EA→B"
REFERENCES,0.2602564102564103,"First, we take the average over training samples of task B conditioned by task A, that is,"
REFERENCES,0.26153846153846155,"EA→B|A :=
Z
dxp(x)( ¯fB(x) −(w∗
A + w∗
B)⊤ψ(x))2"
REFERENCES,0.26282051282051283,"DB
(S.13)"
REFERENCES,0.2641025641025641,"=
Z
dxp(x)((w∗
B −( ¯wB −w∗
A))⊤ψ(x))2"
REFERENCES,0.2653846153846154,"DB
(S.14)"
REFERENCES,0.26666666666666666,where the set of task B’s training samples is denoted by DB. We have
REFERENCES,0.26794871794871794,"EA→B = ⟨EA→B|A⟩DA.
(S.15)"
REFERENCES,0.2692307692307692,"Note that the objective function H(wB, w∗
A) is transformed to"
REFERENCES,0.2705128205128205,"H(wB, w∗
A) = 1 2λ NB
X µ=1"
REFERENCES,0.2717948717948718,"
(wB −( ¯wB −w∗
A))⊤ψ(x(µ)
B )
2
+ 1"
REFERENCES,0.27307692307692305,"2∥wB∥2
2.
(S.16)"
REFERENCES,0.2743589743589744,"Comparing (S.14 ) and (S.16 ), one can see that the average over task B conditioned by task A is
equivalent to single-task training with the target function ( ¯wB −w∗
A)⊤ψ(x). Therefore, by using (8),
we immediately obtain"
REFERENCES,0.27564102564102566,"EA→B|A =
1
1 −γB X"
REFERENCES,0.27692307692307694,"i
ηi( ¯wB,i −w∗
A,i)2

κB
κB + NBηi"
REFERENCES,0.2782051282051282,"2
+
γB
1 −γB
σ2
(S.17) =:
X"
REFERENCES,0.2794871794871795,"i
φi( ¯wB,i −w∗
A,i)2 +
γB
1 −γB
σ2.
(S.18)"
REFERENCES,0.28076923076923077,"Next, we take the average of (S.18 ) over task A. Only ¯w∗
A depends on the task A and is determined by
the single-task training (S.8 ). This corresponds to Lemma 2 with u = ¯wB and φi = ηiq2
B,i/(1−γB).
We obtain"
REFERENCES,0.28205128205128205,"EA→B =
X i """
REFERENCES,0.2833333333333333,"( ¯wA,i −¯wB,i)2 −2 ¯wA,i( ¯wA,i −¯wB,i)qA,i + "
REFERENCES,0.2846153846153846,"¯wA,i +
1
1 −γA ηiNA κ2
A X"
REFERENCES,0.2858974358974359,"j
ηj ¯w2
A,jq2
A,j "
REFERENCES,0.28717948717948716,"q2
A,i"
REFERENCES,0.28846153846153844,"#
ηiq2
B,i
1 −γB"
REFERENCES,0.28974358974358977,"+ σ2
 
1
1 −γA"
REFERENCES,0.29102564102564105,"1
1 −γB NA κ2
A X"
REFERENCES,0.2923076923076923,"i
η2
i q2
A,iq2
B,i +
γB
1 −γB !"
REFERENCES,0.2935897435897436,".
(S.19)"
REFERENCES,0.2948717948717949,"Although this is a general result that holds for any ¯wA and ¯wB, it is a bit complicated and seems not
easy to give an intuitive explanation. Let us take the average over"
REFERENCES,0.29615384615384616,"[ ¯wA,i, ¯wB,i] ∼N(0, ηi"
REFERENCES,0.29743589743589743,"
1
ρ
ρ
1"
REFERENCES,0.2987179487179487,"
).
(S.20)"
REFERENCES,0.3,The generalization error is then simpliﬁed to
REFERENCES,0.30128205128205127,"EA→B(ρ) = ⟨EA→B⟩w
(S.21) =
X i """
REFERENCES,0.30256410256410254,"2(1 −ρ) (1 −qA,i) +
q2
A,i
1 −γA # EB,i"
REFERENCES,0.3038461538461538,"+ σ2
 
1
1 −γA"
REFERENCES,0.30512820512820515,"1
1 −γB NA κ2
A X"
REFERENCES,0.30641025641025643,"i
η2
i q2
A,iq2
B,i +
γB
1 −γB !"
REFERENCES,0.3076923076923077,",
(S.22)"
REFERENCES,0.308974358974359,"where we used ⟨w2
A,i⟩w = ⟨w2
B,i⟩w = ηi and ⟨wA,i, wB,i⟩w = ρ."
REFERENCES,0.31025641025641026,Published as a conference paper at ICLR 2022
REFERENCES,0.31153846153846154,"A.2.2
BACKWARD TRANSFER"
REFERENCES,0.3128205128205128,Backward transfer is measured by the prediction on the previous task A:
REFERENCES,0.3141025641025641,"Eback
A→B :=
Z
dxp(x)( ¯fA(x) −fA→B(x))2

(S.23)"
REFERENCES,0.3153846153846154,"=
Z
dxp(x)((w∗
B −( ¯wA −w∗
A))⊤ψ(x))2

.
(S.24)"
REFERENCES,0.31666666666666665,"First, we take the average over task B,"
REFERENCES,0.31794871794871793,"Eback
A→B|A =
Z
dxp(x)((w∗−( ¯wA −w∗
A))⊤ψ(x))2"
REFERENCES,0.3192307692307692,"DB
.
(S.25)"
REFERENCES,0.32051282051282054,"This corresponds to Lemma 2 with the replacement φi ←ηi and u ←¯wA −w∗
A. Recall that the
objective function of NTK regression was given by (S.16). The target ¯wA in Lemma 2 is replaced as
¯wA ←¯wB −w∗
A. We then have"
REFERENCES,0.3217948717948718,"Eback
A→B|A =
X i """
REFERENCES,0.3230769230769231,"( ¯wB,i −¯wA,i)2 −2( ¯wB,i −w∗
A,i)( ¯wB,i −¯wA,i)qB,i + 
"
REFERENCES,0.3243589743589744,"( ¯wB,i −w∗
A,i)2 +
1
1 −γB ηiNB"
REFERENCES,0.32564102564102565,"κ2
B
(
X"
REFERENCES,0.3269230769230769,"j=0
ηj( ¯wB,j −w∗
A,j)2q2
B,j + σ2) 
"
REFERENCES,0.3282051282051282,"q2
B,i # ηi"
REFERENCES,0.3294871794871795,(S.26)
REFERENCES,0.33076923076923076,"= γB
X"
REFERENCES,0.33205128205128204,"i
ηi( ¯wB,i −¯wA,i)2 +
X i"
REFERENCES,0.3333333333333333,"ηiq2
B,i
1 −γB"
REFERENCES,0.3346153846153846,"
wA,i −( ¯wB,i −1 −γB"
REFERENCES,0.33589743589743587,"qB,i
( ¯wB,i −¯wA,i))
2"
REFERENCES,0.3371794871794872,"+
γB
1 −γB
σ2.
(S.27)"
REFERENCES,0.3384615384615385,"Next, we take the average over DA. Since the ﬁrst and third terms of (S.27 ) are independent of
DA, we need to evaluate only the second term. The second term corresponds to Lemma 2 with
φi = ηiq2
B,i/(1 −γB) and ui = ¯wB,i −(1 −γB)/qB,i( ¯wB,i −¯wA,i). We obtain"
REFERENCES,0.33974358974358976,"Eback
A→B =
D
Eback
A→B|A
E DA"
REFERENCES,0.34102564102564104,"= γB
X"
REFERENCES,0.3423076923076923,"i
ηi( ¯wB,i −¯wA,i)2 +
X i """
REFERENCES,0.3435897435897436,"( ¯wA,i −¯wB,i)2(qB,i −(1 −γB))2"
REFERENCES,0.34487179487179487,"−2 ¯wA,i( ¯wA,i −¯wB,i)(qB,i −(1 −γB))qA,iqB,i + 
"
REFERENCES,0.34615384615384615,"¯w2
A,i +
1
1 −γA ηiNA"
REFERENCES,0.3474358974358974,"κ2
A
(
X"
REFERENCES,0.3487179487179487,"j
ηjw2
A,jq2
A,j + σ2) 
"
REFERENCES,0.35,"q2
B,iq2
B,i"
REFERENCES,0.35128205128205126,"#
ηi
1 −γB"
REFERENCES,0.3525641025641026,"+ σ2
 
1
1 −γA"
REFERENCES,0.35384615384615387,"1
1 −γB NA κ2
A X"
REFERENCES,0.35512820512820514,"i
η2
i q2
A,iq2
B,i +
γB
1 −γB !"
REFERENCES,0.3564102564102564,".
(S.28)"
REFERENCES,0.3576923076923077,Published as a conference paper at ICLR 2022
REFERENCES,0.358974358974359,"Finally, by taking the average over (S.20 ), we have"
REFERENCES,0.36025641025641025,"Eback
A→B(ρ) = ⟨Eback
A→B⟩w =
X i"
REFERENCES,0.36153846153846153,"
2γBη2
i (1 −ρ) + 2 (qB,i −(1 −γB))2
η2
i
1 −γB
(1 −ρ)"
REFERENCES,0.3628205128205128,"−2 η2
i (qB,i −1 + γB)qB,iqA,i"
REFERENCES,0.3641025641025641,"1 −γB
(1 −ρ) +
1
1 −γA"
REFERENCES,0.36538461538461536,"1
1 −γB
η2
i q2
A,iq2
B,i "
REFERENCES,0.36666666666666664,"+ σ2
 
1
1 −γA"
REFERENCES,0.367948717948718,"1
1 −γB NA κ2
A X"
REFERENCES,0.36923076923076925,"i
η2
i q2
A,iq2
B,i +
γB
1 −γB !"
REFERENCES,0.37051282051282053,"= 2(1 −ρ)
X"
REFERENCES,0.3717948717948718,"i
η2
i """
REFERENCES,0.3730769230769231,"1 + qB,i(qA,i −2) +
q2
B,i(1 −qA,i) 1 −γB #"
REFERENCES,0.37435897435897436,"+
1
1 −γA"
REFERENCES,0.37564102564102564,"1
1 −γB ×
X"
REFERENCES,0.3769230769230769,"i
η2
i q2
A,iq2
B,i + σ2
 
1
1 −γA"
REFERENCES,0.3782051282051282,"1
1 −γB NA κ2
A X"
REFERENCES,0.37948717948717947,"i
η2
i q2
A,iq2
B,i +
γB
1 −γB !"
REFERENCES,0.38076923076923075,".
(S.29)"
REFERENCES,0.382051282051282,"A.2.3
LEMMA 2"
REFERENCES,0.38333333333333336,"Lemma 2. Suppose training on single task A, the target of which is given by ¯f = P"
REFERENCES,0.38461538461538464,"i ¯wA,iψi, and
denote the trained model (2) as f ∗= P"
REFERENCES,0.3858974358974359,"i w∗
A,iψi. Deﬁne the following cost function: E = *X"
REFERENCES,0.3871794871794872,"i
φi(w∗
A,i −ui)2
+"
REFERENCES,0.38846153846153847,"DA
(S.30)"
REFERENCES,0.38974358974358975,"for arbitrary constants φi and ui. Using the replica method under a sufﬁciently large NA, we have E =
X i=0 """
REFERENCES,0.391025641025641,"( ¯wA,i −ui)2 −2 ¯wA,i( ¯wA,i −ui)qA,i + 
"
REFERENCES,0.3923076923076923,"¯w2
A,i +
1
1 −γA ηiNA"
REFERENCES,0.3935897435897436,"κ2
A
(
X"
REFERENCES,0.39487179487179486,"j=0
ηj ¯w2
A,jq2
A,j + σ2) 
"
REFERENCES,0.39615384615384613,"q2
A,i #"
REFERENCES,0.3974358974358974,"φi.
(S.31)"
REFERENCES,0.39871794871794874,"Proof. The process of derivation is similar to Canatar et al. (2021), but we have additional constants
φi and ui. They cause several differences in the detailed form of equations. Deﬁne"
REFERENCES,0.4,"Z[J] =
Z
dwA exp(−βHA(wA) + J βN"
REFERENCES,0.4012820512820513,"2 E(wA))
(S.32)"
REFERENCES,0.4025641025641026,"where E(wA) =

P"
REFERENCES,0.40384615384615385,"i φi(wA,i −ui)2"
REFERENCES,0.40512820512820513,DA. We omit the index A in the following. We have
REFERENCES,0.4064102564102564,"E = lim
β→∞
2
βN
∂
∂J ⟨log Z[J]⟩D"
REFERENCES,0.4076923076923077,"J=0
.
(S.33)"
REFERENCES,0.40897435897435896,"To evaluate ⟨log Z⟩, we use the replica method (a.k.a. replica trick):"
REFERENCES,0.41025641025641024,"⟨log Z⟩D = lim
n→0
1
n (⟨Zn⟩D −1)
(S.34)"
REFERENCES,0.4115384615384615,"The point of the replica method is that we ﬁrst calculate Zn for n ∈N then take the limit of n to zero
by treating it as a real number. In addition, we calculate the average ⟨Zn⟩D under a replica symmetric
ansätz and a Gaussian approximation by following the calculation procedure of the previous works
(Dietrich et al., 1999; Bordelon et al., 2020; Canatar et al., 2021)."
REFERENCES,0.4128205128205128,Published as a conference paper at ICLR 2022
REFERENCES,0.41410256410256413,We have
REFERENCES,0.4153846153846154,"⟨Zn⟩D =
Z
dWn exp  −β 2 n
X"
REFERENCES,0.4166666666666667,"a=1
∥wa∥2 + βJN 2 n
X"
REFERENCES,0.41794871794871796,"a=1
(wa −u)⊤Φ (wa −u) !"
REFERENCES,0.41923076923076924,"⟨Q⟩NA
{xµ,εµ} ,"
REFERENCES,0.4205128205128205,(S.35)
REFERENCES,0.4217948717948718,"Q := exp  −β 2λ n
X a=1"
REFERENCES,0.4230769230769231," 
(wa
A −¯wA)⊤ψ(xµ) −εµ2
!"
REFERENCES,0.42435897435897435,",
(S.36)"
REFERENCES,0.4256410256410256,"where we deﬁne dWn = Qn
a=1 dwa and Φ is a diagonal matrix whose diagonal entries given by φi.
We take the shift of wa →wa + ¯w. Then,"
REFERENCES,0.4269230769230769,"⟨Zn⟩D =
Z
dWn exp(−nβ"
REFERENCES,0.4282051282051282,2 ∥¯w∥2 + nβJN
REFERENCES,0.42948717948717946,"2
( ¯w −u)⊤Φ( ¯w −u))
|
{z
}
=: ¯
Z(J)"
REFERENCES,0.4307692307692308,"· exp(
X a
−β"
REFERENCES,0.43205128205128207,2 ∥wa∥2 + βJN
REFERENCES,0.43333333333333335,"2
wa⊤Φwa −βk⊤wa) ⟨Q⟩N
{xµ,εµ} ,
(S.37)"
REFERENCES,0.4346153846153846,"Q = exp(−β 2λ n
X"
REFERENCES,0.4358974358974359,"a=1
(wa⊤ψ(xµ) −εµ)2),
(S.38)"
REFERENCES,0.4371794871794872,"¯k := ¯w −JNΦ( ¯w −u).
(S.39)"
REFERENCES,0.43846153846153846,"First, let us calculate ⟨Q⟩{xµ,εµ}. This term is exactly the same as appeared in the previous works
(Bordelon et al., 2020; Canatar et al., 2021). To describe notations, we overview their derivation.
Deﬁne qa = wa⊤ψ (x) + ε, which are called order parameters. We approximate the probability
distribution of qa by a multivariate Gaussian:"
REFERENCES,0.43974358974358974,"P({qa}) =
1
p"
REFERENCES,0.441025641025641,"(2π)n det(C)
exp  −1 2 X"
REFERENCES,0.4423076923076923,"a,b
(qa −µa) [C−1]ab
 
qb −µb
"
REFERENCES,0.44358974358974357,",
(S.40)"
REFERENCES,0.44487179487179485,"with
µa := ⟨qa⟩{x,ε} =

wa⊤ψ(x)"
REFERENCES,0.4461538461538462,"x + ⟨ε⟩ε = √η0wa
0,
(S.41)"
REFERENCES,0.44743589743589746,"Cab :=

qaqb"
REFERENCES,0.44871794871794873,"{x,ε} = wa⊤⟨ψ(x)ψ(x)⊤⟩xwb + ⟨εaεb⟩ε = wa⊤Λwa + Σab,
(S.42)"
REFERENCES,0.45,"where Σab = σδab, ⟨· · · ⟩x denotes the average over p(x) and Λ is a diagonal matrix whose entries
are ηi. We have √η0wa
0 in (S.41 ) since η0 corresponds to the constant shift φ0(x) = 1. Training
samples are i.i.d. and we omitted the index µ. We then have"
REFERENCES,0.4512820512820513,"⟨Q⟩{x,ε} = exp  −1"
LOG DET,0.45256410256410257,"2 log det

I + β"
LOG DET,0.45384615384615384,"λC

−β"
LOG DET,0.4551282051282051,"2λµ⊤

I + β"
LOG DET,0.4564102564102564,"λC
−1
µ !"
LOG DET,0.4576923076923077,",
(S.43)"
LOG DET,0.45897435897435895,"where I denotes the identity matrix. Deﬁne conjugate variables {ˆµa, ˆCab} by the following identity:"
LOG DET,0.46025641025641023,"1 = C
Z  Y"
LOG DET,0.46153846153846156,"a≥b
dµadˆµadCabd ˆCab   × exp  −N
X"
LOG DET,0.46282051282051284,"a
ˆµa  
µa −wa
A,0
√η0

−N
X"
LOG DET,0.4641025641025641,"a≥b
ˆCab  
Cab −wa⊤Λwb −Σab
"
LOG DET,0.4653846153846154,",
(S.44)"
LOG DET,0.4666666666666667,where C denotes an uninteresting constant and we took the conjugate variables on imaginary axes.
LOG DET,0.46794871794871795,"Next, we perform the integral over Wn. Eq. (S.37 ) becomes
⟨Zn⟩D"
LOG DET,0.46923076923076923,"= C ¯Z(J)
Z
dWn
Y"
LOG DET,0.4705128205128205,"a≥b
dΩab exp(−N(
X"
LOG DET,0.4717948717948718,"a
ˆµaµa +
X"
LOG DET,0.47307692307692306,"a≥b
ˆCab(Cab −Σab))) exp[−NG −GS]"
LOG DET,0.47435897435897434,(S.45)
LOG DET,0.4756410256410256,Published as a conference paper at ICLR 2022
LOG DET,0.47692307692307695,where dΩab = dµadˆµadCabd ˆCab and deﬁne G = 1
LOG DET,0.4782051282051282,"2 log det

I + β"
LOG DET,0.4794871794871795,"λC

+ β"
LOG DET,0.4807692307692308,"2λµ⊤

I + β"
LOG DET,0.48205128205128206,"λC
−1
µ,
(S.46)"
LOG DET,0.48333333333333334,"exp(−GS) = exp(
X a
(−β"
LOG DET,0.4846153846153846,2 ∥wa∥2 + βJN
LOG DET,0.4858974358974359,"2
wa⊤Φwa −βk⊤wa)) × exp  N
X"
LOG DET,0.48717948717948717,"a
ˆµawa
A,0
√η0 + N
X"
LOG DET,0.48846153846153845,"a≥b
ˆCabwa⊤Λwb "
LOG DET,0.4897435897435897,".
(S.47)"
LOG DET,0.491025641025641,"We can represent
R
dWn exp(−GS) by
Z
dWn exp(−GS) =
Y i"
LOG DET,0.49230769230769234,"Z
dxi exp

−β"
LOG DET,0.4935897435897436,"2 x⊤
i ˆQixi −βb⊤
i xi"
LOG DET,0.4948717948717949,"
(S.48)"
LOG DET,0.49615384615384617,"where xi ∈Rn denotes a vector [w1
i , ..., wa
i , ..., wn
i ]⊤and i is the index of kernel’s eigenvalue mode
(i = 0, 1, ...). We deﬁned"
LOG DET,0.49743589743589745,ˆQi = (1 −φiJN)In −ηiN
LOG DET,0.4987179487179487,"β ( ˜C + diag( ˜C)),
(S.49)"
LOG DET,0.5,"bi = ¯ki1n
(i ≥1),
(S.50)"
LOG DET,0.5012820512820513,b0 = ¯k01n −N√η0
LOG DET,0.5025641025641026,"β
ˆµ,
(S.51)"
LOG DET,0.5038461538461538,"where In is an n × n identity matrix and 1n is an n-dimensional vector whose all entries are 1. The
GS term includes φ and c that are speciﬁc to our study. When φ = η and u = ¯w, it is reduced to
previous works (Bordelon et al., 2020; Canatar et al., 2021). Taking the integral over {xi}, we have
Z
dWn exp(−GS) = C
Y"
LOG DET,0.5051282051282051,"i=0
exp(β"
LOG DET,0.5064102564102564,"2 b⊤
i ˆQ−1
i bi)/
q"
LOG DET,0.5076923076923077,"det ˆQi.
(S.52)"
LOG DET,0.5089743589743589,"That is,"
LOG DET,0.5102564102564102,GS = 1 2 X
LOG DET,0.5115384615384615,"i=0
log det ˆQi −β 2 X"
LOG DET,0.5128205128205128,"i=0
b⊤
i ˆQ−1
i bi.
(S.53)"
LOG DET,0.514102564102564,REPLICA SYMMETRY AND SADDLE-POINT METHOD
LOG DET,0.5153846153846153,"Next, we carry out the integral (S.47 ) by the saddle-point method. Assume the replica symmetry:"
LOG DET,0.5166666666666667,"µ = µa,
r = Caa,
c = Ca̸=b,
(S.54)"
LOG DET,0.517948717948718,"ˆµ = ˆµa,
ˆr = ˆCaa,
ˆc = ˆCa̸=b.
(S.55)"
LOG DET,0.5192307692307693,The following three terms are the same as in the previous works:
LOG DET,0.5205128205128206,"det

I + β"
LOG DET,0.5217948717948718,"λC

=

1 + β"
LOG DET,0.5230769230769231,"λ (r −c)
n 
1 + n
βc
λ + β (r −c)"
LOG DET,0.5243589743589744,"
,
(S.56)"
LOG DET,0.5256410256410257,"
I + β"
LOG DET,0.5269230769230769,"λC
−1
=
1
1 + β"
LOG DET,0.5282051282051282,λ (r −c)
LOG DET,0.5294871794871795,"
I −
βc
λ + β (r −c) + nβc11⊤

,
(S.57)"
LOG DET,0.5307692307692308,"ˆµ⊤µ +
X"
LOG DET,0.532051282051282,"a≥b
ˆCab(Cab −Σab) = n

ˆµµ + ˆr(r −σ2) −1"
LOG DET,0.5333333333333333,"2 ˆq(q −σ2)

,
(S.58)"
LOG DET,0.5346153846153846,"where 11⊤denotes a matrix whose all entries are 1. Regarding the leading term of order n (n →0),"
LOG DET,0.5358974358974359,"log det

I + β"
LOG DET,0.5371794871794872,"λC

= n log

1 + β"
LOG DET,0.5384615384615384,"λ (r −c)

+ n
βC
λ + β (r −c),
(S.59)"
LOG DET,0.5397435897435897,"µ⊤

I + β"
LOG DET,0.541025641025641,"λC
−1
µ =
nµ2 1 + β"
LOG DET,0.5423076923076923,"λ (r −c)
.
(S.60)"
LOG DET,0.5435897435897435,Published as a conference paper at ICLR 2022
LOG DET,0.5448717948717948,Furthermore. we have
LOG DET,0.5461538461538461,ˆQi = (1 −φiJN −ηiN
LOG DET,0.5474358974358975,β (2ˆr −ˆc))In −ηiNˆc
LOG DET,0.5487179487179488,"β
1n1⊤
n .
(S.61)"
LOG DET,0.55,"After straightforward algebra, the leading terms become"
LOG DET,0.5512820512820513,log det ˆQi = n ln gi −nηiNˆc
LOG DET,0.5525641025641026,"giβ ,
(S.62)"
LOG DET,0.5538461538461539,"1⊤
n ˆQ−1
i 1n = n"
LOG DET,0.5551282051282052,"gi
,
(S.63) with"
LOG DET,0.5564102564102564,gi := 1 −φiJN −ηiN
LOG DET,0.5576923076923077,"β (2ˆr −ˆc).
(S.64)"
LOG DET,0.558974358974359,"Substituting these leading terms into (S.45 ), we obtain ⟨Zn⟩D = C
R
dθ exp(−nNS(θ)) with"
LOG DET,0.5602564102564103,S(θ) = −βJ
LOG DET,0.5615384615384615,2 ( ¯w −u)⊤Φ( ¯w −u)
LOG DET,0.5628205128205128,"=

ˆµµ + ˆr(r −σ2) −1"
LOG DET,0.5641025641025641,"2ˆc(c −σ2)

+ 1 2"
LOG DET,0.5653846153846154,"
log β (r −c) + c + µ2 r −c  + 1"
N,0.5666666666666667,2N X
N,0.5679487179487179,"i=0
(ln gi −(ηiNˆc"
N,0.5692307692307692,"β
+ β¯k2
i )/gi) −
1
2g0
(−2¯k0ˆµ√η0 + Nη0ˆµ2/β),
(S.65)"
N,0.5705128205128205,"where θ denotes a set of variables {r, ˆr, c, ˆc, µ, ˆµ}. Here, we take N ≫1 and use the saddle-point
method. We calculate saddle-point equations ∂S(θ∗)/∂θ = 0 and obtain"
N,0.5717948717948718,"ˆµ = −
µ
r −c,
(S.66)"
N,0.573076923076923,ˆr = 1 2
N,0.5743589743589743, c + µ2
N,0.5756410256410256,"(r −c)2 −
1
r −c"
N,0.5769230769230769,"
,
(S.67)"
N,0.5782051282051283,ˆc = c + µ2
N,0.5794871794871795,"(r −c)2 ,
(S.68)"
N,0.5807692307692308,µ = Nη0ˆµ
N,0.5820512820512821,"g0β
−
¯k0
g0"
N,0.5833333333333334,"√η0,
(S.69) r =
X"
N,0.5846153846153846,"i=0
(ηiNˆc"
N,0.5858974358974359,"β
+ β¯k2
i ) ηi"
N,0.5871794871794872,"g2
i β + η0N"
N,0.5884615384615385,"g2
0β (−2¯k0ˆµ√η0 + Nη0ˆµ2β) + σ2,
(S.70) c =
X"
N,0.5897435897435898,"i=0
(ηiNˆc"
N,0.591025641025641,"β
+ β¯k2
i ) ηi"
N,0.5923076923076923,"g2
i β −1 β X i=0"
N,0.5935897435897436,"ηi
gi
+ η0N"
N,0.5948717948717949,"g2
0β (−2¯k0ˆµ√η0 + Nη0ˆµ2β) + σ2.
(S.71)"
N,0.5961538461538461,"From (S.67 ) and (S.68 ),"
N,0.5974358974358974,"r −c = −
1
2ˆr −ˆc.
(S.72)"
N,0.5987179487179487,"From (S.70 ) and (S.71 ), we also have"
N,0.6,"β(r −c) =
X i=0"
N,0.6012820512820513,"ηi
gi
=: κ.
(S.73)"
N,0.6025641025641025,"Substituting (S.64 ) and (S.72 ) into (S.73 ), We obtain the implicit function for κ: 1 =
X i=0"
N,0.6038461538461538,"ηi
W(φi), W(φi) := κ(1 −φiJN) + ηiN.
(S.74)"
N,0.6051282051282051,Published as a conference paper at ICLR 2022
N,0.6064102564102564,"Using κ and W, the saddle-point equations become"
N,0.6076923076923076,ˆµ = −βµ
N,0.6089743589743589,"κ ,
(S.75)"
N,0.6102564102564103,ˆr = 1 2
N,0.6115384615384616,c + µ2
N,0.6128205128205129,"κ2
β2 −β κ"
N,0.6141025641025641,"
,
(S.76)"
N,0.6153846153846154,ˆc = β2
N,0.6166666666666667,"κ2 (c + µ2),
(S.77)"
N,0.617948717948718,"µ = −
√η0¯k0κ
Nη0 + W(φ0),
(S.78)"
N,0.6192307692307693,r = c + κ
N,0.6205128205128205,"β .
(S.79)"
N,0.6217948717948718,"Substituting back these quantities into (S.65 ), we obtain"
N,0.6230769230769231,S(θ∗) = −βJ
N,0.6243589743589744,2 ( ¯w −u)⊤Φ( ¯w −u) + 1
N,0.6256410256410256,2 ln κ + 1
N,0.6269230769230769,2N X
N,0.6282051282051282,"i=0
ln gi −β"
N,0.6294871794871795,2N X i=0
N,0.6307692307692307,"¯k2
i
gi"
N,0.632051282051282,"+
βη0κ
2W(φ0)"
N,0.6333333333333333,"¯k2
0
Nη0 + W(φ0) + β"
N,0.6346153846153846,"2κσ2 + const.
(S.80)"
N,0.6358974358974359,"Recall ⟨Zn⟩D ≈exp(−nNS(θ∗)). We have
⟨log Z⟩D = −NS(θ∗).
(S.81)"
N,0.6371794871794871,GENERALIZATION ERROR
N,0.6384615384615384,We evaluate
N,0.6397435897435897,"E = lim
β→∞
2
βN
∂
∂J ⟨log Z⟩D"
N,0.6410256410256411,"J=0
= −lim
β→∞
2
β
∂
∂J S(θ∗)

J=0
.
(S.82)"
N,0.6423076923076924,"Note that W, g, κ and ¯k depend on J. At the point of J = 0,
∂W(φi)"
N,0.6435897435897436,"∂J
= −φiNκ + ∂κ"
N,0.6448717948717949,"∂J ,
(S.83) ∂gi"
N,0.6461538461538462,∂J = −(φi + 1
N,0.6474358974358975,"κ2
∂κ
∂J ηi)N,
(S.84)"
N,0.6487179487179487,"∂κ
∂J = κ2N 1 −γ X i=0"
N,0.65,"ηiφi
(κ + ηiN)2 ,
(S.85) ∂¯ki"
N,0.6512820512820513,"∂J = −Nφi( ¯wi −ui).
(S.86)"
N,0.6525641025641026,"Substituting (S.83 -S.86 ) into (S.82 ), we obtain
E = X i=0 """
N,0.6538461538461539,"( ¯wi −ui)2 −2 ¯wi( ¯wi −ui)qi + 
"
N,0.6551282051282051,"¯w2
i +
1
1 −γ
ηiN"
N,0.6564102564102564,"κ2 (
X"
N,0.6576923076923077,"j=0
ηj ¯w2
jq2
j + σ2) 
 q2
i #"
N,0.658974358974359,"φi + ξ0,"
N,0.6602564102564102,"(S.87)
where ξ0 is"
N,0.6615384615384615,"ξ0 = η0 ¯w2
0"
N,0.6628205128205128,"""
κ
Nη0 + 2W0
W 2
0 (Nη0 + W0)2 −
1
W0(Nη0 + W0)"
N,0.6641025641025641,"  
N
1 −γ X"
N,0.6653846153846154,"i
ηiq2
i φi !"
N,0.6666666666666666,"−φ0Nκ2
Nη0 + 2W0
W 2
0 (Nη0 + W0)2 #"
N,0.6679487179487179,"+ 2η0
φ0κN ¯w0( ¯w0 −u0)"
N,0.6692307692307692,"W0(Nη0 + W0)
(S.88)"
N,0.6705128205128205,"with W0 = κ + Nη0. When η0 = 0, we have ξ0 = 0. Note that the additional term ξ0 is not speciﬁc
to our case but also appeared in the previous work (Canatar et al., 2021). We have ξ0 = O(1/η0) for
a large η0 and ξ0 is often negligibly small."
N,0.6717948717948717,Published as a conference paper at ICLR 2022
N,0.6730769230769231,"A.3
EQUATIONS FOR UNDERSTANDING NEGATIVE TRANSFER"
N,0.6743589743589744,"A.3.1
DERIVATION OF EQUATION 15"
N,0.6756410256410257,"First, let us evaluate the term including qA,i = κA/(κA + NAηi). Note that κA is a monotonically
decreasing function of NA because"
N,0.676923076923077,"∂κA
∂NA
= − X i"
N,0.6782051282051282,"ηi
(κA + NAηi)2 !−1 X i"
N,0.6794871794871795,"η2
i
(κA + NAηi)2 < 0,
(S.89)"
N,0.6807692307692308,"which comes from the implicit function theorem. Let us write κA at a ﬁnite NA = c as κA(c). We
then have"
N,0.6820512820512821,"qA,i ≤κA(c)"
N,0.6833333333333333,"NAηi
,
(S.90)"
N,0.6846153846153846,"for NA > c. Next, we evaluate"
N,0.6858974358974359,"EA→B(ρ) = 2(1 −ρ)EB −2(1 −ρ)
X"
N,0.6871794871794872,"i
qA,iEB,i +
X i"
N,0.6884615384615385,"q2
A,i
1 −γA
EB,i.
(S.91)"
N,0.6897435897435897,"The second term is negligibly small for a sufﬁciently large NA > c because
X"
N,0.691025641025641,"i
qA,iEB,i ≤κA(c) NA X i"
N,0.6923076923076923,"ηiq2
B,i
1 −γB
= κA(c)"
N,0.6935897435897436,"NA
κB.
(S.92)"
N,0.6948717948717948,Note that we have 1 −γ = κ P
N,0.6961538461538461,"i=0 ηi/(κ + ηiN)2. The third term is
X i"
N,0.6974358974358974,"q2
A,i
1 −γA
EB,i ≤
EA
1 −γB
=
1
1 −γB"
N,0.6987179487179487,"γA
1 −γA"
N,0.7,"κ2
A
NA
,
(S.93)"
N,0.7012820512820512,"where we used qB,i ≤1. It decreases to zero as NA increases. Thus, we have EA→B(ρ) ∼
2(1 −ρ)EB."
N,0.7025641025641025,"A.3.2
DERIVATION OF EQUATION 19"
N,0.7038461538461539,EA→B(1)
N,0.7051282051282052,"EB
=
1
1 −γA P"
N,0.7064102564102565,"i q2
A,iq2
B,iη2
i
P"
N,0.7076923076923077,"i q2
B,iη2
i
(S.94)"
N,0.708974358974359,"≥
1
1 −γA"
N,0.7102564102564103,"κA
κA + P"
N,0.7115384615384616,"i q2
B,iη3
i NA !2"
N,0.7128205128205128,(S.95)
N,0.7141025641025641,"where the second line comes from Jensen’s inequality. Using (S.90 ), we have
X"
N,0.7153846153846154,"i
q2
B,iη3
i ≤κB(c)2 N 2
B X"
N,0.7166666666666667,"i
ηi,
(S.96)"
N,0.717948717948718,"for NB ≥c. Since we assumed the ﬁnite trance of NTK (i.e., P"
N,0.7192307692307692,"i ηi < ∞), the left-hand side of
(S.96 ) is ﬁnite and converges to zero for a sufﬁciently large NB. Therefore, we have
EA→B(1)"
N,0.7205128205128205,"EB
≳
1
1 −γA
.
(S.97)"
N,0.7217948717948718,"In contrast, EA→B/EB ≤1/(1 −γA) since qA ≤1. Thus, the ratio is sandwiched by 1/(1 −γA)."
N,0.7230769230769231,"A.3.3
SUFFICIENT CONDITION FOR THE NEGATIVE TRANSFER"
N,0.7243589743589743,"Let us denote the i-th mode of EA→B as EA→B,i and that of EB as EB,i. From EA→B,i > EB,i,
we have"
N,0.7256410256410256,"f(qi) := 2(1 −ρ)(1 −qi) +
q2
i
1 −γ −1 > 0,
(S.98)"
N,0.7269230769230769,"where f(q) is a quadratic function and takes the minimum at q∗= (1 −ρ)(1 −γ). The condition
f(q) > 0 holds if and only if"
N,0.7282051282051282,"f(q∗) = −(1 −γ)ρ2 −2γρ + γ > 0.
(S.99)
Solving this, we ﬁnd ρ < √γ/(1 + √γ)."
N,0.7294871794871794,Published as a conference paper at ICLR 2022
N,0.7307692307692307,"A.4
PROOF OF PROPOSITION 3."
N,0.732051282051282,"For NA = NB, we have qA,i = qB,i = qi. Then,"
N,0.7333333333333333,"EA→B(1) =
1
(1 −γ)2
X"
N,0.7346153846153847,"i
q4
i η2
i
(S.100)"
N,0.735897435897436,and the generalization error of single-task training is given by
N,0.7371794871794872,"E =
1
1 −γ X"
N,0.7384615384615385,"i
q2
i η2
i .
(S.101)"
N,0.7397435897435898,"Model average is obtained in Section D and we have Eave = (1 −γ/2)E < E because 0 < γ < 1.
Thus, we only need to evaluate the relation between Eave and EA→B:"
N,0.7410256410256411,Eave −EA→B = (1 −γ
N,0.7423076923076923,"2 )
1
1 −γ X"
N,0.7435897435897436,"i
q2
i η2
i −
1
(1 −γ)2
X"
N,0.7448717948717949,"i
η2
i q4
i
(S.102)"
N,0.7461538461538462,= (1 −γ
N,0.7474358974358974,"2 )
1
1 −γ κ2 X"
N,0.7487179487179487,"i
a2
i −
κ2"
N,0.75,"(1 −γ)2
X"
N,0.7512820512820513,"i
a2
i q2
i
(S.103) =
κ2"
N,0.7525641025641026,(1 −γ)2N (X
N,0.7538461538461538,"i
N 2a3
i (2 −Nai) −1"
N,0.7551282051282051,2γ2(3 −γ) )
N,0.7564102564102564,"|
{z
}
=:F"
N,0.7576923076923077,(S.104)
N,0.7589743589743589,"where we deﬁned
ai :=
ηi
κ + ηiN ,
(S.105)"
N,0.7602564102564102,and used qi = 1−aiN and γ = N P
N,0.7615384615384615,"i a2
i . We can provide a lower bound of F by using the following
cubic function:
G(ai) := N 2a2
i (2 −Nai).
We have 0 ≤ai ≤1/N by deﬁnition. Let us consider a lower bound of G by using its tangent line at
ai = t/N (0 ≤t ≤1), that is, Nt(4 −3t)ai −2t2(1 −t). Deﬁne"
N,0.7628205128205128,"H(ai) := G(ai) −(Nt(4 −3t)ai −2t2(1 −t)).
We have H(0) = 2t2(1 −t) ≥0. Since H(1/N) = −2(t −1)2(t −1/2), we need t ≤1/2 to
guarantee H(ai) ≥0 for all 0 ≤ai ≤1/N. Here, note that H is a cubic function of ai and has two
ﬁxed points ai = t/N and (4 −3t)/(3N). We can see that for t ≤1/2, H has the local minimum at
ai = t/N and"
N,0.764102564102564,"H(t/N) = t2(2 −t) −(t2(4 −3t) −2t2(1 −t))
= 0.
(S.106)
Therefore, we have H(ai) ≥0 for all 0 ≤ai ≤1/N when the ﬁxed constant t satisﬁes t ≤1/2.
Thus, we have the lower bound of G:
G(ai) ≥Nt(4 −3t)ai −2t2(1 −t).
(S.107)
Using this lower bound, we have F =
X"
N,0.7653846153846153,"i
aiG(ai) −1"
N,0.7666666666666667,2γ2(3 −γ)
N,0.767948717948718,≥γt(4 −3t) −2t2(1 −t) −1
N,0.7692307692307693,"2γ2(3 −γ),
(S.108)"
N,0.7705128205128206,where we used γ = N P
N,0.7717948717948718,"i a2
i and P"
N,0.7730769230769231,"i ai = 1. By setting t = γ/2, we obtain F ≥γ2"
N,0.7743589743589744,2 (4 −3γ/2) −2γ2
N,0.7756410256410257,4 (1 −γ/2) −1
N,0.7769230769230769,2γ2(3 −γ)
N,0.7782051282051282,"= 0.
(S.109)
Therefore, we have Eave > EA→B."
N,0.7794871794871795,"A.5
MULTIPLE DESCENT"
N,0.7807692307692308,Published as a conference paper at ICLR 2022
N,0.782051282051282,"Figure 5: Theoretical values of EA→B(1). (Left) noise-less case (σ2 = 0), (Right) noisy case
(σ2 = 10−5). We set D = 100 and other settings are the same as in Figure 1. In noise-less case,
generalization error monotonically decrease as NA or NB increases. In contrast, when noise is added,
it shows multiple descents."
N,0.7833333333333333,"Figure 4: Typical behaviors of κ
and γ."
N,0.7846153846153846,"We overview the multiple descent of E1 investigated in Canatar
et al. (2021). It appears for σ > 0. Let us set NB = αDl
(α > 0, l ∈N, D ≫1), which is called the l-th learning
stage. We have EB,i = 0 (i < l), EB,l(α) and η2
i (i > l).
EB,l(α) is a function of α, and becomes a one-peak curve
depending on the noise and the decay of kernel’s eigenvalue
spectrum. Because each learning stage has a one-peak curve,
the whole learning curve shows multiple descents as the sam-
ple size increases. Roughly speaking, the appearance of the
multiple descent is controlled by γ. The γ is determined by the
kernel’s spectrum and sample size N. For example, previous
studies showed that if we assume the l-th learning stage, γ
is a non-monotonic function of α, the maximum of which is
γ = 1/(
p¯λl +
p¯λl + 1)2 for a constant ¯λl = P
k>l ¯ηk/¯ηl,
where ¯η is a normalized eigenvalue of the kernel (Canatar et al., 2021). This tells us that γ repeats
the increase and decrease depending on the increase of the learning stage as is shown in Figure 4.
Roughly speaking, this non-monotonic change of γ leads to the multiple descent."
N,0.7858974358974359,"In sequential training, EB,l(α) can similarly cause multiple descent as in single tasks because EA→B
is a weighted summation over EB,i. Figure 5 is an example in which noise causes multiple decreases
and increases depending on the sample sizes. We set ρ = 1 and obtained the theoretical values by
(S.22 ). While EA→B(1) is a symmetric function of indices A and B for σ = 0, it is not for σ > 0.
Depending on NA and NB, the learning “surface” becomes much more complicated than the learning
curve of a single task."
N,0.7871794871794872,"A.6
ADDITIONAL EXPERIMENT ON BACKWARD TRANSFER"
N,0.7884615384615384,"Figure 6 shows the learning curves of backward transfer. Solid lines show theory, and markers show
the mean and interquartile range of NTK regression over 100 trials. The ﬁgure shows excellent
agreement between theory and experiments. As is shown in Section 4.1, subtle target dissimilarity
(ρ < 1) causes negative backward transfer (i.e., catastrophic forgetting). For a large NA, Eback
A→B(ρ <
1) approaches a non-zero constant while EA approaches zero. Thus, we have Eback
A→B > EA even for
the target similarity close to 1. When sample sizes are unbalanced (NA ≫NB), the learning curve
shows negative transfer even for ρ = 1. This is the self-knowledge forgetting revealed in (20). The
ratio Eback
A→B(1)/EA takes a constant larger than 1, that is, 1/(1 −γB)."
N,0.7897435897435897,Published as a conference paper at ICLR 2022 ρ
N,0.791025641025641,"Figure 6: Negative backward transfer easily appears depending on target similarity and sample sizes.
We changed NA and set NB = 102. We set D = 20 and other experimental details are the same as
in Figure 1(b)."
N,0.7923076923076923,"B
PROOF OF THEOREM 4."
N,0.7935897435897435,"B.1
LEARNING CURVE OF MANY TASKS"
N,0.7948717948717948,Eq. (4) is written as
N,0.7961538461538461,"fn(x) = n
X"
N,0.7974358974358975,"k=1
Θ(x, Xk)Θ(Xk)−1(yk −fk−1(Xk)).
(S.110)"
N,0.7987179487179488,"Each term of this summation is equivalent to the kernel ridge-less regression on input samples Xk
and labels yk −fk−1(Xk). Therefore, we can represent fn by"
N,0.8,"fn(x) = n
X"
N,0.8012820512820513,"k=1
w∗⊤
k ψ(x)
(S.111)"
N,0.8025641025641026,"with the minimization of the objective function H:
w∗
n = lim
λ→0 argminwn H(wn, w∗
1:(n−1)),
(S.112)"
N,0.8038461538461539,"H(wn, w∗
1:(n−1)) = 1 2λ Nn
X µ=1 "
N,0.8051282051282052,"w⊤
n ψ(xµ) −(yµ
n − n−1
X"
N,0.8064102564102564,"k=1
w∗⊤
k ψ(xµ)) !2 + 1"
N,0.8076923076923077,"2∥wn∥2
2
(S.113) = 1 2λ Nn
X µ=1 "
N,0.808974358974359,"(wn −( ¯w − n−1
X"
N,0.8102564102564103,"k=1
w∗
k))⊤ψ(xµ) −εµ
n !2 + 1"
N,0.8115384615384615,"2∥wn∥2
2.
(S.114)"
N,0.8128205128205128,The generalization error is
N,0.8141025641025641,"En+1 :=
Z
dxp(x)( ¯f(x) −fn+1(x))2"
N,0.8153846153846154,"D
(S.115) ="
N,0.8166666666666667,"*Z
dxp(x)((w∗
n+1 −( ¯w − n
X"
N,0.8179487179487179,"k=1
w∗
k))⊤ψ(x))2
+"
N,0.8192307692307692,"D
(S.116)"
N,0.8205128205128205,"where D = {D1, ..., Dn}."
N,0.8217948717948718,"Since the data samples are independent of each other among different tasks, we can apply Lemma 2
sequentially from wn+1 to w1. First, we take the average over the (n + 1)-th task, that is,"
N,0.823076923076923,En+1|1:n =
N,0.8243589743589743,"*Z
dxp(x)((w∗
n+1 −( ¯w − n
X"
N,0.8256410256410256,"k=1
w∗
k))⊤ψ(x))2
+"
N,0.8269230769230769,"Dn+1
(S.117)"
N,0.8282051282051283,Published as a conference paper at ICLR 2022
N,0.8294871794871795,"This corresponds to Lemma 2 with φ = η and u = ¯wA = ¯w −Pn
k=1 w∗
k. We have"
N,0.8307692307692308,"En+1|1:n =
X"
N,0.8320512820512821,"i=1
φ1,i( ¯wi − n
X"
N,0.8333333333333334,"k=1
w∗
k,i)2 + R1σ2
(S.118) with"
N,0.8346153846153846,"φ1,i := ηiq2
n+1,i
1 −γn+1
, R1 :=
γn+1
1 −γn+1
.
(S.119)"
N,0.8358974358974359,"Here, κn, γn and qn,k are determined by Nn. Next, we take the average over the n-th task:"
N,0.8371794871794872,"En+1|1:(n−1) =

En+1|1:n"
N,0.8384615384615385,"Dn,εn
(S.120) = *X"
N,0.8397435897435898,"i
φ1,i(w∗
n,i −( ¯wi − n−1
X"
N,0.841025641025641,"k=1
w∗
k,i))2
+"
N,0.8423076923076923,"Dn
+ R1σ2
(S.121)"
N,0.8435897435897436,"This corresponds to Lemma 2 with φ = φ1 and u = ¯wA = ¯w −Pn−1
k=1 w∗
k. We obtain"
N,0.8448717948717949,"En+1|1:(n−1) =
X"
N,0.8461538461538461,"i=1
φ2,i( ¯w − n−1
X"
N,0.8474358974358974,"k=1
w∗
k)2 + R2σ2
(S.122) with"
N,0.8487179487179487,"φ2,i = φ1,iq2
n,i +
Nn
κ2n(1 −γn)ηiq2
n,i
X"
N,0.85,"j
ηjφ1,jq2
n,j,
(S.123)"
N,0.8512820512820513,"R2 = R1 +
Nn
κ2n(1 −γn) X"
N,0.8525641025641025,"j
ηjφ1,jq2
n,j.
(S.124)"
N,0.8538461538461538,"Similarly, we can take the averages from the (n −1)-th task to the ﬁrst task, and obtain"
N,0.8551282051282051,"En+1 =
X"
N,0.8564102564102564,"i=1
φn+1,i ¯w2
i + Rn+1σ2,
(S.125)"
N,0.8576923076923076,"φm+1,i = φm,iq2
n−m+1,i +
Nn−m+1
κ2
n−m+1(1 −γn−m+1)ηiq2
n−m+1,i  X"
N,0.8589743589743589,"j
ηjφm,jq2
n−m+1,j  ,"
N,0.8602564102564103,"(S.126)
for m = 1, ..., n, and"
N,0.8615384615384616,"Rn+1 = n
X m=1"
N,0.8628205128205129,"Nn−m+1
κ2
n−m+1(1 −γn−m+1) X"
N,0.8641025641025641,"j
ηjφm,jq2
n−m+1,j.
(S.127)"
N,0.8653846153846154,"They are general results for any Nn. By setting Nn = N for all n, we can obtain a simpler formulation.
In a vector representation, φn+1 is explicitly written as"
N,0.8666666666666667,"φn+1 =
1
1 −γ"
N,0.867948717948718,"
diag(q2) +
N
(1 −γ)κ2 ˜q˜q⊤
n
˜q.
(S.128)"
N,0.8692307692307693,"Finally, by taking the average over ¯wi ∼N(0, ηi) which is the one-dimensional case of (S.20 ), we
have"
N,0.8705128205128205,"En+1 =
1
(1 −γ)2 ˜q⊤

diag(q2) +
N
(1 −γ)κ2 ˜q˜q⊤
n−1
˜q + Rn+1σ2
(S.129) with"
N,0.8717948717948718,"Rn+1 =
N
κ2(1 −γ)2 n
X"
N,0.8730769230769231,"m=1
˜q⊤

diag(q2) +
N
(1 −γ)κ2 ˜q˜q⊤
m−1
˜q +
γ
1 −γ .
(S.130)"
N,0.8743589743589744,Published as a conference paper at ICLR 2022
N,0.8756410256410256,"B.2
MONOTONIC DECREASE OF En"
N,0.8769230769230769,"For σ2 = 0, we have"
N,0.8782051282051282,"En+1 =
1
(1 −γ)2 ˜q⊤Qn−1˜q.
(S.131)"
N,0.8794871794871795,"Note that Q is a positive semi-deﬁnite matrix. If the maximum eigenvalue is no greater than 1, En
is monotonically non-increasing with n. We denote the inﬁnite norm as ∥A∥∞= maxi
P"
N,0.8807692307692307,"j |Aij|.
Because the inﬁnite norm bounds the eigenvalues, we have"
N,0.882051282051282,"λ1(Q) ≤max
i X"
N,0.8833333333333333,"j
|Qij|
(S.132)"
N,0.8846153846153846,"= max
i"
N,0.8858974358974359,"
1 + N κ ηi"
N,0.8871794871794871," 
κ
κ + ηiN 2"
N,0.8884615384615384,"|
{z
}
:=G(ηi)"
N,0.8897435897435897,",
(S.133)"
N,0.8910256410256411,"where G(η) is monotonically decreasing for η ≥0 and G(0) = 1. Therefore, the largest eigenvalue
λ1(Q) is upper-bounded by 1, and En becomes monotonically decreasing. In particular. if ηi > 0
for all i, we have λ1(Q) < 1 and obtain En+1 < En."
N,0.8923076923076924,"For λ1(Q) < 1, it is also easy to check that the series (S.130 ) in Rn+1 converges to a constant for
large n."
N,0.8935897435897436,"C
DERIVATION OF KRR-LIKE EXPRESSION"
N,0.8948717948717949,"KRR-like expression comes from the inverse formula of a block triangular matrix. Let us write (23)
as"
N,0.8961538461538462,"fn(x′) = [Θ(x′, 1 : (n −1)) Θ(x′, n)]Z−1
n"
N,0.8974358974358975,"
y1:(n−1)
yn"
N,0.8987179487179487,"
,
(S.134)"
N,0.9,where Zn is a block triangular matrix recursively deﬁned by
N,0.9012820512820513,"Zn =

Zn−1
O
Θ(n, 1 : (n −1))
Θ(n)"
N,0.9025641025641026,"
.
(S.135)"
N,0.9038461538461539,"Then, (S.134 ) is transformed to"
N,0.9051282051282051,"[Θ(x′, 1 : (n −1)) Θ(x′, n)]

Z−1
n−1
O
−Θ(n)−1Θ(n, 1 : (n −1))Z−1
n−1
Θ(n)−1"
N,0.9064102564102564," 
y1:(n−1)
yn"
N,0.9076923076923077,"
(S.136)"
N,0.908974358974359,"= Θ(x′, 1 : (n −1))Z−1
n−1y1:(n−1) + Θ(x′, n)Θ(n)−1(yn −Θ(n, 1 : (n −1))Z−1
n−1y1:(n−1))
(S.137)"
N,0.9102564102564102,"= fn−1(x′) + Θ(x′, n)Θ(n)−1(yn −fn−1(Xn)).
(S.138)"
N,0.9115384615384615,"Thus, one can see that the KRR-like expression is equivalent to our continually trained model."
N,0.9128205128205128,"D
DERIVATION OF MODEL AVERAGE"
N,0.9141025641025641,"For clarity, we set σ = 0, NA = NB, and ρ = 1 (that is, ¯wA = ¯wB). Generalization error of a model
average is expressed by"
N,0.9153846153846154,Eave =
N,0.9166666666666666,"*Z
dxp(x)

¯fB(x) −fA(x) + fB(x) 2 2+"
N,0.9179487179487179,"D
(S.139) ="
N,0.9192307692307692,"*Z
dxp(x)"
N,0.9205128205128205,"
¯wB −wA + wB 2"
N,0.9217948717948717,"⊤
ψ(x) !2+"
N,0.9230769230769231,"D
(S.140) ="
N,0.9243589743589744,"*
¯wB −wA + wB 2"
N,0.9256410256410257,"⊤
Λ

¯wB −wA + wB 2 +"
N,0.926923076923077,"D
(S.141)"
N,0.9282051282051282,Published as a conference paper at ICLR 2022
N,0.9294871794871795,"We can evaluate Eave in a similar way to EA→B. First, take the average over DB. We deﬁne"
N,0.9307692307692308,Eave|A = 1
N,0.9320512820512821,"4

((wB −(2 ¯wB −wA))⊤Λ(wB −(2 ¯wB −wA))"
N,0.9333333333333333,"DB .
(S.142)"
N,0.9346153846153846,"This average corresponds to Lemma 2 with replacement from the target task A to B, φ ←η and
u ←2 ¯wB −wA. Then, we have"
N,0.9358974358974359,Eave|A = 1 4 X i=0
N,0.9371794871794872,"
(wA,i −¯wB,i)2 −2 ¯wB,i(wA,i −¯wB,i)qB,i +

¯w2
B,i + ηiNB"
N,0.9384615384615385,"κ2
B
EB"
N,0.9397435897435897,"
q2
B,i 
ηi"
N,0.941025641025641,(S.143) = 1 4 X
N,0.9423076923076923,"i=0
(wA,i −(1 + qB,i) ¯wB,i)2ηi + γB"
N,0.9435897435897436,"4 EB.
(S.144)"
N,0.9448717948717948,"Next, we take the average over DA. The ﬁrst term of (S.144 ) corresponds to Lemma 2 with
replacement φ ←η and u ←(1 + qB,i) ¯wB. Then, we get"
N,0.9461538461538461,"Eave = ⟨Eave|A⟩DA
(S.145) = 1 4 X"
N,0.9474358974358974,"i=0
((1 −qA,i) ¯wA,i −(1 + qB,i) ¯wB,i)2ηi + γA"
N,0.9487179487179487,4 EA + γB
N,0.95,"4 EB.
(S.146)"
N,0.9512820512820512,"For NA = NB, we have qA = qB, γA = γB, and EA = EB. In addition, since we focused on
¯wA = ¯wB, we have"
N,0.9525641025641025,"Eave =
X"
N,0.9538461538461539,"i=0
ηi ¯w2
B,iq2
B,i + γB"
EB,0.9551282051282052,"2 EB
(S.147)"
EB,0.9564102564102565,"=

1 −γB 2"
EB,0.9576923076923077,"
EB
(S.148)"
EB,0.958974358974359,"E
EXPERIMENTAL SETTINGS"
EB,0.9602564102564103,"E.1
DUAL REPRESENTATION"
EB,0.9615384615384616,"In dual representation, the targets (10) are given by"
EB,0.9628205128205128,"[αA,i, αB,i] ∼N(0,

1
ρ
ρ
1"
EB,0.9641025641025641,"
/P ′),
(S.149)"
EB,0.9653846153846154,"¯fA(x) = P ′
X"
EB,0.9666666666666667,"j
αA,jΘ(x′
j, x), ¯fB(x) = P ′
X"
EB,0.967948717948718,"j
αB,jΘ(x′
j, x)
(S.150)"
EB,0.9692307692307692,"for a sufﬁciently large P ′. We sampled x′ from the same input distribution p(x) and set P ′ = 104
in all experiments. It is easy to check that targets (10) and (S.150 ) are asymptotically equiv-
alent for a large P ′.
Because Θ(x′, x) = P"
EB,0.9705128205128205,"i ψi(x′)ψi(x), we have PP ′"
EB,0.9717948717948718,"j αA,jΘ(x′
j, x) =
P"
EB,0.9730769230769231,"i
PP ′"
EB,0.9743589743589743,"j (αA,jψi(x′
j))ψi(x) ∼P"
EB,0.9756410256410256,"i ¯wiψi(x), where ¯wi is sampled from N(0, ηi)."
EB,0.9769230769230769,"E.2
SUMMARIES ON EXPERIMENTAL SETUP"
EB,0.9782051282051282,"Figure 1(a). We trained the deep neural network (1) with ReLU, L = 3, and Ml = 4, 000 by the
gradient descent. We set a learning rate 0.5 and trained the network during 10,000 steps over 50 trials
with different random seeds. The target is given by (S.150 ) with C = 1, D = 10, and Gaussian input
samples xi ∼N(0, 1). We initialized the network with (σ2
w, σ2
b) = (2, 0), set NA = NB = 100,
and calculated the generalization error over 4, 000 samples. Each marker shows the mean and
interquartile range over the trials. To calculate the theoretical curves, we need eigenvalues ηi of the
NTK. We numerically compute them by the Gauss-Gegenbauer quadrature following the instruction
of Bordelon et al. (2020). Its implementation is also given by Canatar et al. (2021). Since the input"
EB,0.9794871794871794,Published as a conference paper at ICLR 2022
EB,0.9807692307692307,"samples are deﬁned on a hyper-sphere Sd−1 and the NTK is a dot product kernel (i.e., Θ(x′, x) can
be represented by Θ(x′⊤x)), the NTK can be decomposed into Gegenbauer polynomials:"
EB,0.982051282051282,"Θ(z) = ∞
X"
EB,0.9833333333333333,"i=0
ηiN(d, i)Qi(z),
(S.151)"
EB,0.9846153846153847,"where {Qi} are the Gegenbauer polynomials and N(d, i) are constants depending d and i. Since the
Gegenbauer polynomials are orthogonal polynomials, we have"
EB,0.985897435897436,"ηi = c
Z 1"
EB,0.9871794871794872,"−1
Θ(z)Qi(z)dτ(z),
(S.152)"
EB,0.9884615384615385,"for a certain constant c and dτ(z) = (1 −z2)(d−3)/2dz. The Gauss-Gegenbaur quadrature approxi-
mates this integral by ηi ≈c r
X"
EB,0.9897435897435898,"j=1
wjΘ(zj)Qi(zj),
(S.153)"
EB,0.9910256410256411,"where wj are constant weights and zj are the r roots of Qr(z). The deﬁnitions of constants (N(d, i),
c and wj) are given in Section 8 of Bordelon et al. (2020) and we set r = 1000 as is the same in this
previous work. We computed ηi (i = 1, .., 1000) and substituted them to the analytical expressions."
EB,0.9923076923076923,"Figure 1(b). This ﬁgure shows the results of NTK regression; (2) for the single task and (4) for
sequential training. We set NB = 4, 000, D = 50 and other settings were the same as in Figure 1(a)."
EB,0.9935897435897436,"Figure 2. This ﬁgure shows the results of NTK regression; we set D = 20, and other settings are the
same as Figure 1(a)."
EB,0.9948717948717949,"Figure 3. (NTK regression) This ﬁgure shows the results of NTK regression; We set D = 10 and
show the means and error bars over 100 trials. Other settings were the same as in Figure 1(a)."
EB,0.9961538461538462,"(MLP) We trained the deep neural network (1) with ReLU, L = 5, and Ml = 512 by SGD with a
learning rate 0.001, momentum 0.9, and mini-batch size 32 over 10 trials with Pytorch seeds 1-10. We
set the number of epochs to 150 and scaled the learning rate ×1/10 at the 100-th epoch, conﬁrming
that the training accuracy reached 1 for each task."
EB,0.9974358974358974,"(ResNet-18) We trained ResNet-18 by SGD with a learning rate 0.01, momentum 0.9, and mini-batch
size 128 over 10 trials with Pytorch seeds 1-10. We set the number of epochs to 150 and scaled the
learning rate ×1/10 at the 100-th epoch, conﬁrming that the training accuracy reached 1 for each
task."
EB,0.9987179487179487,"Note that the purpose of these experiments was not to achieve performance comparable to state-of-
the-art but to conﬁrm self-knowledge transfer and forgetting. Therefore, we did not apply any data
augmentation or regularization method such as weight decay and dropout."
