Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0018867924528301887,"This paper studies robust nonparametric regression, in which an adversarial attacker
1"
ABSTRACT,0.0037735849056603774,"can modify the values of up to q samples from a training dataset of size N. Our
2"
ABSTRACT,0.005660377358490566,"initial solution is an M-estimator based on Huber loss minimization. Compared
3"
ABSTRACT,0.007547169811320755,"with simple kernel regression, i.e. the Nadaraya-Watson estimator, this method
4"
ABSTRACT,0.009433962264150943,"can significantly weaken the impact of malicious samples on the regression per-
5"
ABSTRACT,0.011320754716981131,"formance. We provide the convergence rate as well as the corresponding minimax
6"
ABSTRACT,0.013207547169811321,"lower bound. The result shows that, with proper bandwidth selection, ℓ∞error is
7"
ABSTRACT,0.01509433962264151,"minimax optimal. The ℓ2 error is optimal if q ≲
q"
ABSTRACT,0.016981132075471698,"N/ ln2 N, but is suboptimal
8"
ABSTRACT,0.018867924528301886,"with larger q. The reason is that this estimator is vulnerable if there are many
9"
ABSTRACT,0.020754716981132074,"attacked samples concentrating in a small region. To address this issue, we propose
10"
ABSTRACT,0.022641509433962263,"a correction method by projecting the initial estimate to the space of Lipschitz
11"
ABSTRACT,0.024528301886792454,"functions. The final estimate is nearly minimax optimal for arbitrary q, up to a
12"
ABSTRACT,0.026415094339622643,"ln N factor.
13"
INTRODUCTION,0.02830188679245283,"1
Introduction
14"
INTRODUCTION,0.03018867924528302,"In the era of big data, it is common for some samples to be corrupted due to various reasons, such
15"
INTRODUCTION,0.03207547169811321,"as transmission errors, system malfunctions, malicious attacks, etc. The values of these samples
16"
INTRODUCTION,0.033962264150943396,"may be altered in any way, rendering many traditional machine learning techniques less effective.
17"
INTRODUCTION,0.035849056603773584,"Consequently, evaluating the effects of these corrupted samples, and making corresponding robust
18"
INTRODUCTION,0.03773584905660377,"strategies, have become critical tasks in the research community [1–10].
19"
INTRODUCTION,0.03962264150943396,"Among all types of data contamination, adversarial attack is of particular interest in recent years
20"
INTRODUCTION,0.04150943396226415,"[11–17], in which there exists a malicious adversary who aims at deteriorating our model performance.
21"
INTRODUCTION,0.04339622641509434,"With this goal, the attacker alters the values of some samples using a carefully designed strategy.
22"
INTRODUCTION,0.045283018867924525,"Compared with other types of undesired samples, such as accidental errors or noise, adversarial
23"
INTRODUCTION,0.04716981132075472,"samples are more challenging to deal with, since their values are altered deliberately instead of
24"
INTRODUCTION,0.04905660377358491,"randomly. Therefore, any learning models that can withstand adversarial attacks should also be
25"
INTRODUCTION,0.0509433962264151,"resilient to other corruptions.
26"
INTRODUCTION,0.052830188679245285,"Adversarial attack can be divided into poisoning attack [11–13], which manipulates training samples
27"
INTRODUCTION,0.05471698113207547,"to damage the model, and evasion attack [14–17], which modifies test samples to generate wrong
28"
INTRODUCTION,0.05660377358490566,"predictions. We focus on poisoning attack here. For classification problems, the labels can only
29"
INTRODUCTION,0.05849056603773585,"be altered within several discrete values, thus the impact of poisoning samples is relatively limited
30"
INTRODUCTION,0.06037735849056604,"[11, 18, 19]. However, regression problems are crucially different, since the response variable is
31"
INTRODUCTION,0.062264150943396226,"continuous and can be altered arbitrarily far away from its ground truth. Without proper handling,
32"
INTRODUCTION,0.06415094339622641,"even if only a tiny fraction of training samples are attacked, the model performance may drastically
33"
INTRODUCTION,0.0660377358490566,"deteriorate. Therefore, for regression problems, defense strategies against poisoning attack are
34"
INTRODUCTION,0.06792452830188679,"crucially needed.
35"
INTRODUCTION,0.06981132075471698,"Despite many previous works toward robust regression problems, most of them focus on parametric
36"
INTRODUCTION,0.07169811320754717,"models [13, 20–22]. For example, there are several robust techniques for linear models, such as
37"
INTRODUCTION,0.07358490566037736,"M-estimation [23], least median of squares [24], least trimmed squares [25], etc. However, for
38"
INTRODUCTION,0.07547169811320754,"nonparametric methods such as kernel [26] and k nearest neighbor estimator, defense strategies
39"
INTRODUCTION,0.07735849056603773,"against poisoning attack still need further exploration [27]. Actually, designing robust techniques is
40"
INTRODUCTION,0.07924528301886792,"indeed more challenging for nonparametric methods than parametric one. For parametric models,
41"
INTRODUCTION,0.08113207547169811,"the parameters are estimated using full dataset, while nonparametric methods have to rely on local
42"
INTRODUCTION,0.0830188679245283,"training data around the query point. Even if the ratio of attacked samples among the whole dataset is
43"
INTRODUCTION,0.08490566037735849,"small, the local anomaly ratio in the neighborhood of the query point can be large. As a result, the
44"
INTRODUCTION,0.08679245283018867,"estimated function value at such query point can be totally wrong. Despite such difficulty, in many
45"
INTRODUCTION,0.08867924528301886,"real scenarios, due to problem complexity or lack of prior knowledge, parametric models are not
46"
INTRODUCTION,0.09056603773584905,"always available. Therefore, we hope to explore effective schemes to overcome the robustness issue
47"
INTRODUCTION,0.09245283018867924,"of nonparametric regression.
48"
INTRODUCTION,0.09433962264150944,"In this paper, we provide a theoretical study about robust nonparametric regression problem under
49"
INTRODUCTION,0.09622641509433963,"poisoning attack. In particular, we hope to investigate the theoretical limit of this problem, and design
50"
INTRODUCTION,0.09811320754716982,"a method to achieve this limit. With this goal, we make the following contributions:
51"
INTRODUCTION,0.1,"Firstly, we propose and analyze an estimator that minimizes a weighted Huber loss, which can be
52"
INTRODUCTION,0.1018867924528302,"viewed as a combination of ℓ1 and ℓ2 loss functions, and thus achieves a tradeoff between consistency
53"
INTRODUCTION,0.10377358490566038,"and adversarial robustness. It was originally proposed in [28], but to the best of our knowledge,
54"
INTRODUCTION,0.10566037735849057,"it was not analyzed under adversarial setting. We show the convergence rate of both ℓ2 and ℓ∞
55"
INTRODUCTION,0.10754716981132076,"risk, under the assumption that the function to estimate is Lipschitz continuous, and the noise is
56"
INTRODUCTION,0.10943396226415095,"sub-exponential. An interesting finding is that if q ≲
q"
INTRODUCTION,0.11132075471698114,"N/ ln2 N, in which q is the maximum
57"
INTRODUCTION,0.11320754716981132,"number of attacked samples, then the convergence rate is not affected by adversarial samples, i.e. the
58"
INTRODUCTION,0.11509433962264151,"influence of poisoning samples on the overall risk is only up to a constant factor.
59"
INTRODUCTION,0.1169811320754717,"Secondly, we provide an information theoretic minimax lower bound, which indicates the underlying
60"
INTRODUCTION,0.11886792452830189,"limit one can achieve, with respect to q and N. The minimax lower bound without adversarial
61"
INTRODUCTION,0.12075471698113208,"samples can be derived using standard information theoretic methods [29]. Under adversarial attack,
62"
INTRODUCTION,0.12264150943396226,"the estimation problem is harder, thus the lower bound in [29] may not be tight enough. We design
63"
INTRODUCTION,0.12452830188679245,"some new techniques to derive a tighter one. The result shows that the initial estimator has optimal
64"
INTRODUCTION,0.12641509433962264,"ℓ∞risk. If q ≲
q"
INTRODUCTION,0.12830188679245283,"N/ ln2 N, then ℓ2 risk is also minimax optimal. Nevertheless, for larger q, the
65"
INTRODUCTION,0.13018867924528302,"ℓ2 risk is not optimal, indicating that this estimator is still not perfect. We then provide an intuitive
66"
INTRODUCTION,0.1320754716981132,"explanation of the suboptimality. Instead of attacking some randomly selected training samples, the
67"
INTRODUCTION,0.1339622641509434,"best strategy for the attacker is to focus their attack within a small region. With this strategy, majority
68"
INTRODUCTION,0.13584905660377358,"of training samples are altered here, resulting in wrong estimates. A simple remedy is to increase the
69"
INTRODUCTION,0.13773584905660377,"kernel bandwidth to improve robustness. Nevertheless, this will introduce additional bias in other
70"
INTRODUCTION,0.13962264150943396,"regions. It turns out that ℓ∞risk can be made optimal by adjusting the bandwidth, while ℓ2 risk is
71"
INTRODUCTION,0.14150943396226415,"always suboptimal. Actually, the drawback of the initial estimator is that it does not make full use of
72"
INTRODUCTION,0.14339622641509434,"the continuity of regression function, and thus unable to correct the estimation.
73"
INTRODUCTION,0.14528301886792452,"Finally, motivated by the issues of the initial method mentioned above, we propose a corrected
74"
INTRODUCTION,0.1471698113207547,"estimator. If the attack focuses on a small region, although the initial estimate fails here, the output
75"
INTRODUCTION,0.1490566037735849,"elsewhere is still reliable. With the assumption that the underlying function is continuous, the value at
76"
INTRODUCTION,0.1509433962264151,"such region can be inferred using the surrounding values. With such intuition, we propose a nonlinear
77"
INTRODUCTION,0.15283018867924528,"filtering method, which makes minimal adjustment to the estimated function in ℓ1 sense, to make it
78"
INTRODUCTION,0.15471698113207547,"Lipschitz continuous. The corrected estimate is then proved to be nearly minimax optimal up to only
79"
INTRODUCTION,0.15660377358490565,"a ln N factor.
80"
INTRODUCTION,0.15849056603773584,"The remainder of this paper is organized as follows. In section 2, we provide the problem statement
81"
INTRODUCTION,0.16037735849056603,"as well as the initial estimator by Huber loss minimization. The upper bound and the minimax
82"
INTRODUCTION,0.16226415094339622,"lower bound are shown in section 3. In section 4, we elaborate the corrected estimator, as well as
83"
INTRODUCTION,0.1641509433962264,"related theoretical analysis. Numerical simulation results are shown in section 5. Finally, we discuss
84"
INTRODUCTION,0.1660377358490566,"limitations and provide concluding remarks in section 6 and 7, respectively.
85"
THE INITIAL ESTIMATOR,0.16792452830188678,"2
The Initial Estimator
86"
THE INITIAL ESTIMATOR,0.16981132075471697,"Suppose X1, . . . , XN ∈Rd be N independently and identically distributed training samples, gen-
87"
THE INITIAL ESTIMATOR,0.17169811320754716,"erated from a common probability density function (pdf) f. For each sample Xi, we can receive a
88"
THE INITIAL ESTIMATOR,0.17358490566037735,"corresponding label Yi:
89"
THE INITIAL ESTIMATOR,0.17547169811320754,"Yi =

η(Xi) + Wi
if i /∈B
⋆
otherwise,
(1)"
THE INITIAL ESTIMATOR,0.17735849056603772,"in which η : Rd →R is the unknown underlying function that we would like to estimate. Wi is the
90"
THE INITIAL ESTIMATOR,0.1792452830188679,"noise variable. For i = 1, . . . , N, Wi are independent, with zero mean and finite variance. B is the
91"
THE INITIAL ESTIMATOR,0.1811320754716981,"set of indices of attacked samples. ⋆means some value determined by the attacker. For each normal
92"
THE INITIAL ESTIMATOR,0.1830188679245283,"sample Xi, the received label is Yi = η(Xi) + Wi. However, if a sample is attacked, then Yi can be
93"
THE INITIAL ESTIMATOR,0.18490566037735848,"arbitrary value determined by the attacker. The attacker can manipulate up to q samples, thus |B| ≤q.
94"
THE INITIAL ESTIMATOR,0.18679245283018867,"Our goal is opposite to the attacker. We hope to find an estimate ˆη that is as close to η as possible,
95"
THE INITIAL ESTIMATOR,0.18867924528301888,"while the attacker aims at reducing the estimation accuracy using a carefully designed attack strategy.
96"
THE INITIAL ESTIMATOR,0.19056603773584907,"We consider white-box setting here, in which the attacker has complete access to the ground truth η,
97"
THE INITIAL ESTIMATOR,0.19245283018867926,"Xi and Wi for all i ∈{1, . . . , N}, as well as our estimation algorithm. Under this setting, we hope
98"
THE INITIAL ESTIMATOR,0.19433962264150945,"to design a robust regression method that resists to any attack strategies.
99"
THE INITIAL ESTIMATOR,0.19622641509433963,"The quality of estimation is evaluated using ℓ2 and ℓ∞loss, which is defined as
100"
THE INITIAL ESTIMATOR,0.19811320754716982,"R2[ˆη]
=
E

sup
A
(ˆη(X) −η(X))2

,
(2)"
THE INITIAL ESTIMATOR,0.2,"R∞[ˆη]
=
E

sup
A
sup
x |ˆη(x) −η(x)|

,
(3)"
THE INITIAL ESTIMATOR,0.2018867924528302,"in which A denotes the attack strategy, X denotes a random test sample that follows a distribution
101"
THE INITIAL ESTIMATOR,0.2037735849056604,"with pdf f. Our analysis can be easily generated to ℓp loss with arbitrary p.
102"
THE INITIAL ESTIMATOR,0.20566037735849058,"The kernel regression, also called the Nadaraya-Watson estimator [26,30] is
103"
THE INITIAL ESTIMATOR,0.20754716981132076,"ˆηNW (x) =
PN
i=1 K
  x−Xi"
THE INITIAL ESTIMATOR,0.20943396226415095,"h

Yi
PN
i=1 K
  x−Xi"
THE INITIAL ESTIMATOR,0.21132075471698114,"h
 ,
(4)"
THE INITIAL ESTIMATOR,0.21320754716981133,"in which K is the Kernel function, h is the bandwidth that will decrease with the increase of sample
104"
THE INITIAL ESTIMATOR,0.21509433962264152,"size N. ˆη0(x) can be viewed as a weighted average of the labels around x. Without adversarial
105"
THE INITIAL ESTIMATOR,0.2169811320754717,"attack, such estimator converges to η [31]. However, (4) fails even if a tiny fraction of samples are
106"
THE INITIAL ESTIMATOR,0.2188679245283019,"attacked. The attacked labels can just set to be sufficiently large. As a result, ˆη0(x) could be far away
107"
THE INITIAL ESTIMATOR,0.22075471698113208,"from its truth.
108"
THE INITIAL ESTIMATOR,0.22264150943396227,"Now we build the estimator based on Huber loss minimization. Similar method was proposed in [28],
109"
THE INITIAL ESTIMATOR,0.22452830188679246,"but to the best of our knowledge, the performance under adversarial setting has not been analyzed.
110"
THE INITIAL ESTIMATOR,0.22641509433962265,"We elaborate this method for completeness and notation consistency. We use ˆη0 to denote the new
111"
THE INITIAL ESTIMATOR,0.22830188679245284,"estimator, which is designed as following:
112"
THE INITIAL ESTIMATOR,0.23018867924528302,"ˆη0(x) = arg min
|s|≤M N
X"
THE INITIAL ESTIMATOR,0.2320754716981132,"i=1
K
x −Xi h"
THE INITIAL ESTIMATOR,0.2339622641509434,"
ϕ(Yi −s),
(5)"
THE INITIAL ESTIMATOR,0.2358490566037736,"in which tie breaks arbitrarily if the minimum is not unique, and
113"
THE INITIAL ESTIMATOR,0.23773584905660378,"ϕ(u) =

u2
if
|u| ≤T
2T|u| −T 2
if
|u| > T
(6)"
THE INITIAL ESTIMATOR,0.23962264150943396,"is the Huber cost function.
114"
THE INITIAL ESTIMATOR,0.24150943396226415,"Here we have introduced two new parameters, namely, M and T. With M →∞and T →∞,
115"
THE INITIAL ESTIMATOR,0.24339622641509434,"function ϕ becomes simple square loss, and it is straightforward to show that the resulting estimator
116"
THE INITIAL ESTIMATOR,0.24528301886792453,"(5) reduces to the Nadaraya-Watson estimator(4). M is a constant hyperparameter that does not
117"
THE INITIAL ESTIMATOR,0.24716981132075472,"change with sample size N. By restricting |s| ≤M, we avoid the estimated value from being too
118"
THE INITIAL ESTIMATOR,0.2490566037735849,"large. It would be better if M is larger than the upper bound of |η(x)|, so that the estimation is
119"
THE INITIAL ESTIMATOR,0.2509433962264151,"not truncated too much. T balances accuracy and robustness. Smaller T ensures robustness while
120"
THE INITIAL ESTIMATOR,0.2528301886792453,"sacrificing consistency, and vice versa. To achieve better tradeoff, T need to increase with the training
121"
THE INITIAL ESTIMATOR,0.25471698113207547,"sample size N. The best rate of the growth of T with respect to N depends on the strength of the tail
122"
THE INITIAL ESTIMATOR,0.25660377358490566,"of the noise distribution. In our theoretical analysis, we will show that under sub-exponential noise,
123"
THE INITIAL ESTIMATOR,0.25849056603773585,"T ∼ln N is optimal.
124"
THE INITIAL ESTIMATOR,0.26037735849056604,"We would like to remark that apart from Huber loss minimization, there are other robust mean
125"
THE INITIAL ESTIMATOR,0.2622641509433962,"estimation methods, such as median-of-means (MoM) [32,33] and trimmed means [34,35]. However,
126"
THE INITIAL ESTIMATOR,0.2641509433962264,"it is not efficient to generalize these methods to nonparametric regression. For MoM, with up to q
127"
THE INITIAL ESTIMATOR,0.2660377358490566,"corrupted samples, it divides the data into at least 2q + 1 groups and then calculate the median of the
128"
THE INITIAL ESTIMATOR,0.2679245283018868,"means of values in each group. Under the regression setting, since the distribution of attacked samples
129"
THE INITIAL ESTIMATOR,0.269811320754717,"is unknown, we have to divide the data into 2q + 1 groups within the neighborhood of each query
130"
THE INITIAL ESTIMATOR,0.27169811320754716,"point. As a result, the accuracy with N training samples with q contaminated is only comparable
131"
THE INITIAL ESTIMATOR,0.27358490566037735,"to those with N/(2q + 1) clean samples, indicating that the MoM method is ineffective. Trimmed
132"
THE INITIAL ESTIMATOR,0.27547169811320754,"means method has similar problems. The threshold of the trimmed mean need to be set uniformly
133"
THE INITIAL ESTIMATOR,0.27735849056603773,"among the whole support, while the adversarial attack may focus on a small region. As a result,
134"
THE INITIAL ESTIMATOR,0.2792452830188679,"the parameter can not be tuned optimal everywhere. The nonconsistency at attacked region and the
135"
THE INITIAL ESTIMATOR,0.2811320754716981,"inefficiency at relatively cleaner regions are two problems that can not be avoided simultaneously.
136"
THE INITIAL ESTIMATOR,0.2830188679245283,"Consequently, these alternative approaches are less effective than the M-estimator based on Huber
137"
THE INITIAL ESTIMATOR,0.2849056603773585,"loss minimization.
138"
THE INITIAL ESTIMATOR,0.28679245283018867,"Finally, we comment on the computation of the estimator (5). Note that ϕ is convex, therefore the
139"
THE INITIAL ESTIMATOR,0.28867924528301886,"minimization problem in (5) can be solved by gradient descent. The derivative of ϕ is
140"
THE INITIAL ESTIMATOR,0.29056603773584905,ϕ′(u) =
THE INITIAL ESTIMATOR,0.29245283018867924,"(
2u
if
|u| ≤T
2T
if
u > T
−2T
if
u < −T. (7)"
THE INITIAL ESTIMATOR,0.2943396226415094,"Based on (5) and (7), s can be updated using binary search. Denote ϵ as the required precision, then
141"
THE INITIAL ESTIMATOR,0.2962264150943396,"the number of iterations for binary search should be O(ln(M/ϵ)). Therefore, the computational
142"
THE INITIAL ESTIMATOR,0.2981132075471698,"complexity is higher than kernel regression up to a ln(M/ϵ) factor.
143"
THEORETICAL ANALYSIS,0.3,"3
Theoretical Analysis
144"
THEORETICAL ANALYSIS,0.3018867924528302,"This section proposes the theoretical analysis of the initial estimator (5) under adversarial setting. To
145"
THEORETICAL ANALYSIS,0.30377358490566037,"begin with, we make some assumptions about the problem.
146"
THEORETICAL ANALYSIS,0.30566037735849055,"Assumption 1. (Problem Assumption) there exists a compact set X and several constants L, γ, fm,
147"
THEORETICAL ANALYSIS,0.30754716981132074,"fM, D, α, σ, such that the pdf f is supported at X, and
148"
THEORETICAL ANALYSIS,0.30943396226415093,"(a) (Lipschitz continuity) For any x1, x2 ∈X, |η(x1) −η(x2)| ≤L||x1 −x2||;
149"
THEORETICAL ANALYSIS,0.3113207547169811,"(b) (Bounded f and η) For all x ∈X, fm ≤f(x) ≤fM and |η(x)| ≤M, in which M is the
150"
THEORETICAL ANALYSIS,0.3132075471698113,"parameter used in (5);
151"
THEORETICAL ANALYSIS,0.3150943396226415,"(c) (Corner shape restriction) For all r < D, V (B(x, r) ∩X) ≥αvdrd, in which B(x, r) is the ball
152"
THEORETICAL ANALYSIS,0.3169811320754717,"centering at x with radius r, vd is the volume of d dimensional unit ball, which depends on the norm
153"
THEORETICAL ANALYSIS,0.31886792452830187,"we use;
154"
THEORETICAL ANALYSIS,0.32075471698113206,"(d) (Sub-exponential noise) The noise Wi is subexponential with parameter σ,
155"
THEORETICAL ANALYSIS,0.32264150943396225,"E[eλWi] ≤e
1
2 σ2λ2, ∀|λ| ≤1"
THEORETICAL ANALYSIS,0.32452830188679244,"σ ,
(8)"
THEORETICAL ANALYSIS,0.3264150943396226,"for i = 1, . . . , N.
156"
THEORETICAL ANALYSIS,0.3283018867924528,"(a) is a common assumption for smoothness. (b) assumes that the pdf is bounded from both below and
157"
THEORETICAL ANALYSIS,0.330188679245283,"above. (c) prevents the shape of the corner of the support from being too sharp. Without assumption
158"
THEORETICAL ANALYSIS,0.3320754716981132,"(c), the samples around the corner may not be enough, and the attacker can just attack the corner of
159"
THEORETICAL ANALYSIS,0.3339622641509434,"the support. (d) requires that the noise is sub-exponential. If the noise assumption is weaker, e.g.
160"
THEORETICAL ANALYSIS,0.33584905660377357,"only requiring the bounded moments of Wi up to some order, then the noise can be disperse. In this
161"
THEORETICAL ANALYSIS,0.33773584905660375,"case, it will be harder to distinguish adversarial samples from clean samples. More discussions are
162"
THEORETICAL ANALYSIS,0.33962264150943394,"provided in section 6.
163"
THEORETICAL ANALYSIS,0.34150943396226413,"We then make some restrictions on the kernel function K.
164"
THEORETICAL ANALYSIS,0.3433962264150943,"Assumption 2. (Kernel Assumption) the kernel need to satisfy: (a)
R
K(u)du = 1; (b)K(u) =
165"
THEORETICAL ANALYSIS,0.3452830188679245,"0, ∀||u|| > 1; (c) cK ≤K(u) ≤CK for two constants cK and CK.
166"
THEORETICAL ANALYSIS,0.3471698113207547,"(a) is actually not necessary, since from (5), the estimated value will not change if the kernel function
167"
THEORETICAL ANALYSIS,0.3490566037735849,"is multiplied by a constant factor. This assumption is only for convenience of proof. (b) and (c)
168"
THEORETICAL ANALYSIS,0.35094339622641507,"actually requires that the kernel need to be somewhat close to the uniform function in the unit ball.
169"
THEORETICAL ANALYSIS,0.35283018867924526,"Intuitively, if the attacker wants to modify the estimate at some x, the best way is to change the
170"
THEORETICAL ANALYSIS,0.35471698113207545,"response of sample i with large K((Xi −x)/h), in order to make strong impact on ˆη(x). To defend
171"
THEORETICAL ANALYSIS,0.35660377358490564,"against such attack, the upper bound of K should not be too large. Besides, to ensure that clean
172"
THEORETICAL ANALYSIS,0.3584905660377358,"samples dominate corrupted samples everywhere, the effect of each clean sample on the estimation
173"
THEORETICAL ANALYSIS,0.360377358490566,"should not be too small, thus K also need to be bounded from below in its support.
174"
THEORETICAL ANALYSIS,0.3622641509433962,"Furthermore, recall that (5) has three parameters, i.e. h, T and M. We assume that these three
175"
THEORETICAL ANALYSIS,0.3641509433962264,"parameters satisfy the following conditions.
176"
THEORETICAL ANALYSIS,0.3660377358490566,"Assumption 3. (Parameter Assumption) h, T, M need to satisfy (a)h > ln2 N/N; (b)T ≥4Lh +
177"
THEORETICAL ANALYSIS,0.36792452830188677,"16σ ln N; (c)M > sup
x∈X
|η(x)|.
178"
THEORETICAL ANALYSIS,0.36981132075471695,"(a) ensures that the number of samples whose distance to x less than h is not too small. Actually, for a
179"
THEORETICAL ANALYSIS,0.37169811320754714,"better tradeoff between bias and variance, h need to grow much faster than ln2 N/N. (b) requires that
180"
THEORETICAL ANALYSIS,0.37358490566037733,"T ∼ln N. Actually, the optimal growth rate of T depends on the distribution of noise. Recall that in
181"
THEORETICAL ANALYSIS,0.3754716981132076,"Assumption 1(d), we assume that the distribution of noise is sub-exponential. If we use sub-Gaussian
182"
THEORETICAL ANALYSIS,0.37735849056603776,"assumption instead, then it is enough for T ∼
√"
THEORETICAL ANALYSIS,0.37924528301886795,"ln N. If the noise is further assumed to be bounded,
183"
THEORETICAL ANALYSIS,0.38113207547169814,"then T can just be set to constant. (c) prevents the estimate from being truncated too much.
184"
THEORETICAL ANALYSIS,0.38301886792452833,"The upper bound of ℓ2 error is derived under these assumptions. Denote a ≲b if a ≤Cb for some
185"
THEORETICAL ANALYSIS,0.3849056603773585,"constant C that depends only on L, M, γ, fm, fM, D, α, σ, cK, CK.
186"
THEORETICAL ANALYSIS,0.3867924528301887,"Theorem 1. Under Assumption 1, 2 and 3,
187"
THEORETICAL ANALYSIS,0.3886792452830189,"E

sup
A
(ˆη0(X) −η(X))2

≲T 2q2"
THEORETICAL ANALYSIS,0.3905660377358491,"N 2hd + h2 +
1
Nhd .
(9)"
THEORETICAL ANALYSIS,0.39245283018867927,"The detailed proof of Theorem 1 is shown in section 2 in the supplementary material. From the proof,
188"
THEORETICAL ANALYSIS,0.39433962264150946,"it can also be observed that the effect of adversarial samples is higher when they concentrate at a
189"
THEORETICAL ANALYSIS,0.39622641509433965,"small region instead of distributing uniformly over the whole support. Denote Bh(x) as the ball
190"
THEORETICAL ANALYSIS,0.39811320754716983,"centering at x with radius h. Even if q/N is small, the proportion of attacked samples within B(x, h)
191"
THEORETICAL ANALYSIS,0.4,"for some x may be large, which may result in large error at x.
192"
THEORETICAL ANALYSIS,0.4018867924528302,"The next theorem shows the bound of ℓ∞error:
193"
THEORETICAL ANALYSIS,0.4037735849056604,"Theorem 2. Under Assumption 1, 2, 3, if K(u) is monotonic decreasing with respect to ∥u∥, then
194"
THEORETICAL ANALYSIS,0.4056603773584906,"E

sup
A
sup
x |ˆη0(x) −η(x)|

≲Tq"
THEORETICAL ANALYSIS,0.4075471698113208,"Nhd + h + ln N
√"
THEORETICAL ANALYSIS,0.40943396226415096,"Nhd .
(10)"
THEORETICAL ANALYSIS,0.41132075471698115,"The proof is in section 3 in the supplementary material. We then show the minimax lower bound,
195"
THEORETICAL ANALYSIS,0.41320754716981134,"which indicates the information theoretic limit of the adversarial nonparametric regression problem.
196"
THEORETICAL ANALYSIS,0.41509433962264153,"In general, it is impossible to design an estimator with convergence rate faster than the following
197"
THEORETICAL ANALYSIS,0.4169811320754717,"bound.
198"
THEORETICAL ANALYSIS,0.4188679245283019,"Theorem 3. Let F be the collection of f, η, PN that satisfy Assumption 1, in which PN is the
199"
THEORETICAL ANALYSIS,0.4207547169811321,"distribution of the noise W1, . . . , WN. Then
200"
THEORETICAL ANALYSIS,0.4226415094339623,"inf
ˆη
sup
(f,η,PN)∈F
E

sup
A
(ˆη(X) −η(X))2

≳
 q N  d+2"
THEORETICAL ANALYSIS,0.42452830188679247,"d+1 + N −
2
d+2 ,
(11)"
THEORETICAL ANALYSIS,0.42641509433962266,"and
201"
THEORETICAL ANALYSIS,0.42830188679245285,"inf
ˆη
sup
(f,η,PN)∈F
E

sup
A
sup
x |ˆη(x) −η(x)|

≳
 q N"
THEORETICAL ANALYSIS,0.43018867924528303,"
1
d+1 + N −
1
d+2 .
(12)"
THEORETICAL ANALYSIS,0.4320754716981132,"The proof is shown in section 4 in the supplementary material. In the right hand side of (11) and (12),
202"
THEORETICAL ANALYSIS,0.4339622641509434,"N −2/(d+2) is the standard minimax lower bound for nonparametric estimation [29], which holds
203"
THEORETICAL ANALYSIS,0.4358490566037736,"even if there are no adversarial samples. In the supplementary material, we only prove the lower
204"
THEORETICAL ANALYSIS,0.4377358490566038,"bound with the first term in the right hand side of (11).
205"
THEORETICAL ANALYSIS,0.439622641509434,"Compare Theorem 1, 2 and Theorem 3, we have the following findings. We claim that the upper and
206"
THEORETICAL ANALYSIS,0.44150943396226416,"lower bound nearly match, if these two bounds match up to a polynomial of ln N:
207"
THEORETICAL ANALYSIS,0.44339622641509435,"• From (10) and (12), with h ∼max{(q/N)1/(d+1), N −1/(d+2)} and T ∼ln N, the upper
208"
THEORETICAL ANALYSIS,0.44528301886792454,"and minimax lower bound of ℓ∞error nearly match.
209"
THEORETICAL ANALYSIS,0.44716981132075473,"• If q ≲
q"
THEORETICAL ANALYSIS,0.4490566037735849,"N/ ln2 N, from (9) and (11), let h ∼N −
2
d+2 , the upper and minimax lower bound
210"
THEORETICAL ANALYSIS,0.4509433962264151,"of ℓ2 match. In fact, in this case, the convergence rate of (5) is the same as ordinary kernel
211"
THEORETICAL ANALYSIS,0.4528301886792453,"regression without adversarial samples, i.e. h2 + 1/(Nhd). With optimal selection of h, the
212"
THEORETICAL ANALYSIS,0.4547169811320755,"rate becomes N −2/(d+2), which is just the standard rate for nonparametric statistics [29,38].
213"
THEORETICAL ANALYSIS,0.45660377358490567,"• The ℓ2 upper and lower bound no longer match if q ≳
q"
THEORETICAL ANALYSIS,0.45849056603773586,"N/ ln2 N. In this case, the optimal
214"
THEORETICAL ANALYSIS,0.46037735849056605,"h in (9) is h ∼(q ln N/N)2/(d+2), and resulting ℓ2 error is R2 ≲(q ln N/N)4/(d+2),
215"
THEORETICAL ANALYSIS,0.46226415094339623,"higher than the lower bound in (11).
216"
THEORETICAL ANALYSIS,0.4641509433962264,"This result indicates that the initial estimator (5) is optimal under ℓ∞, or under ℓ2 with small q.
217"
THEORETICAL ANALYSIS,0.4660377358490566,"However, under large number of adversarial samples, the ℓ2 error becomes suboptimal.
218"
THEORETICAL ANALYSIS,0.4679245283018868,"Now we provide an intuitive understanding of the suboptimality of ℓ2 risk with large q using a simple
219"
THEORETICAL ANALYSIS,0.469811320754717,"one dimensional example shown in Figure 1, with N = 10000, h = 0.05, M = 3, f(x) = 1 for
220"
THEORETICAL ANALYSIS,0.4716981132075472,"x ∈(0, 1), η(x) = sin(2πx), and the noise follows standard normal distribution N(0, 1). For each
221"
THEORETICAL ANALYSIS,0.47358490566037736,"x, denote qh(x), nh(x) as the number of attacked samples and total samples within (x −h, x + h),
222"
THEORETICAL ANALYSIS,0.47547169811320755,"respectively. For robust mean estimation problems, the breakdown point is 1/2 [39], which also
223"
THEORETICAL ANALYSIS,0.47735849056603774,"holds locally for nonparametric regression problem. Hence, if qh(x)/nh(x) > 1/2, the estimator
224"
THEORETICAL ANALYSIS,0.47924528301886793,"will collapse and return erroneous values even if we use Huber cost. In (a), q = 500, among
225"
THEORETICAL ANALYSIS,0.4811320754716981,"which 250 attacked samples are around x = 0.25, while others are around x = 0.75. In this case,
226"
THEORETICAL ANALYSIS,0.4830188679245283,"qh(x)/nh(x) < 1/2 over the whole support. The curve of estimated function is shown in Fig 1(b).
227"
THEORETICAL ANALYSIS,0.4849056603773585,"The estimate with (5) is significantly better than kernel regression. Then we increase q to 2000. In
228"
THEORETICAL ANALYSIS,0.4867924528301887,"this case, qh(x)/nh(x) > 1/2 around 0.25 and 0.75 (Fig 1(c)), thus the estimate fails. The estimated
229"
THEORETICAL ANALYSIS,0.48867924528301887,"function curve shows an undesirable spike (Fig 1(d)).
230"
THEORETICAL ANALYSIS,0.49056603773584906,"0.0
0.2
0.4
0.6
0.8
1.0 10 5 0 5 10"
THEORETICAL ANALYSIS,0.49245283018867925,Normal samples
THEORETICAL ANALYSIS,0.49433962264150944,attacked samples
THEORETICAL ANALYSIS,0.4962264150943396,"(a) Scatter plots with q =
500."
THEORETICAL ANALYSIS,0.4981132075471698,"0.0
0.2
0.4
0.6
0.8
1.0 3 2 1 0 1 2"
GROUND TRUTH,0.5,"3
Ground truth"
GROUND TRUTH,0.5018867924528302,Kernel regression
GROUND TRUTH,0.5037735849056604,Initial estimate
GROUND TRUTH,0.5056603773584906,Corrected estimate
GROUND TRUTH,0.5075471698113208,"(b) Estimated results with
q = 500."
GROUND TRUTH,0.5094339622641509,"0.0
0.2
0.4
0.6
0.8
1.0 10 5 0 5 10 15"
GROUND TRUTH,0.5113207547169811,Normal samples
GROUND TRUTH,0.5132075471698113,attacked samples
GROUND TRUTH,0.5150943396226415,"(c) Scatter plots with q =
2000."
GROUND TRUTH,0.5169811320754717,"0.0
0.2
0.4
0.6
0.8
1.0 3 2 1 0 1 2"
GROUND TRUTH,0.5188679245283019,"3
Ground truth"
GROUND TRUTH,0.5207547169811321,Kernel regression
GROUND TRUTH,0.5226415094339623,Initial estimate
GROUND TRUTH,0.5245283018867924,Corrected estimate
GROUND TRUTH,0.5264150943396226,"(d) Estimated results with
q = 2000."
GROUND TRUTH,0.5283018867924528,"Figure 1: A simple example with q = 500 and q = 2000. In (a) and (c), red dots are attacked samples,
while blue dots are normal samples. In (b) and (d), four curves correspond to ground truth η, the
result of kernel regression, initial estimate and corrected estimate, respectively. With q = 500, the
initial estimate (5) works well. However, with q = 2000, the initial estimate fails, while the corrected
regression works well."
GROUND TRUTH,0.530188679245283,"The above example shows that the best strategy for attacker is to focus on altering values at a small
231"
GROUND TRUTH,0.5320754716981132,"region. In this case, the local ratio of attacked samples surpasses the breakdown point, resulting in
232"
GROUND TRUTH,0.5339622641509434,"a wrong estimate. With such strategy and sufficient q, the initial estimator (5) fails to be optimal.
233"
GROUND TRUTH,0.5358490566037736,"Actually, (5) does not make full use of the continuity property of regression function η, and thus
234"
GROUND TRUTH,0.5377358490566038,"unable to detect and remove the spikes. A simple remedy is to increase h so that qh(x)/nh(x)
235"
GROUND TRUTH,0.539622641509434,"becomes smaller. However, this solution will introduce additional bias. In the next section, we design
236"
GROUND TRUTH,0.5415094339622641,"a corrected estimator to improve (5), which will close the gap between upper and minimax lower
237"
GROUND TRUTH,0.5433962264150943,"bound with q ≳
q"
GROUND TRUTH,0.5452830188679245,"N/ ln2 N.
238"
CORRECTED REGRESSION,0.5471698113207547,"4
Corrected Regression
239"
CORRECTED REGRESSION,0.5490566037735849,"In this section we propose and analyze a correction method to the initial estimator (5).
240"
CORRECTED REGRESSION,0.5509433962264151,"As has been discussed in section 3, the drawback of the initial estimator is that the continuity property
241"
CORRECTED REGRESSION,0.5528301886792453,"of η is not used. Consequently, an intuitive solution is to filter out the spike, and estimate η here using
242"
CORRECTED REGRESSION,0.5547169811320755,"values in surrounding locations. Linear filter does not work here since the profile of the regression
243"
CORRECTED REGRESSION,0.5566037735849056,"estimate will be blurred. Therefore, we propose a nonlinear filter as following. It conducts minimum
244"
CORRECTED REGRESSION,0.5584905660377358,"correction (in ℓ1 sense) to the initial result ˆη0, while ensuring that the corrected estimate is Lipschitz.
245"
CORRECTED REGRESSION,0.560377358490566,"Formally, given the initial estimate ˆη0(x), our method solves the following optimization problem
246"
CORRECTED REGRESSION,0.5622641509433962,"ˆηc = arg min
∥∇g∥∞≤L
∥ˆη0 −g∥1,
(13)"
CORRECTED REGRESSION,0.5641509433962264,"in which
247"
CORRECTED REGRESSION,0.5660377358490566,"∥∇g∥∞= max

∂g
∂x1"
CORRECTED REGRESSION,0.5679245283018868,", . . . ,

∂g
∂xd "
CORRECTED REGRESSION,0.569811320754717,"
.
(14)"
CORRECTED REGRESSION,0.5716981132075472,"In section 5 in the supplementary material, we prove that the solution to the optimization problem
248"
CORRECTED REGRESSION,0.5735849056603773,"(13) is unique.
249"
CORRECTED REGRESSION,0.5754716981132075,"(13) can be viewed as the projection of the output of initial estimator (5) into the space of Lipschitz
250"
CORRECTED REGRESSION,0.5773584905660377,"function. Here we would like to explain intuitively why we use ℓ1 distance instead of other metrics
251"
CORRECTED REGRESSION,0.5792452830188679,"in (13). Using the example in Fig.1(d) again, it can be observed that at the position of such spikes,
252"
CORRECTED REGRESSION,0.5811320754716981,"|η(x) −g(x)| can be large. Other metrics such as ℓ2 distance impose large costs here, thus somewhat
253"
CORRECTED REGRESSION,0.5830188679245283,"prevents the removal of spikes. Hence ℓ1 distance is preferred.
254"
CORRECTED REGRESSION,0.5849056603773585,"The estimation error of the corrected regression can be bounded by the following theorem.
255"
CORRECTED REGRESSION,0.5867924528301887,"Theorem 4. (1) Under the same conditions as Theorem 1,
256"
CORRECTED REGRESSION,0.5886792452830188,"E

sup
A
(ˆηc(X) −η(X))2

≲
q ln N N  d+2"
CORRECTED REGRESSION,0.590566037735849,"d+1
+ h2 + ln N"
CORRECTED REGRESSION,0.5924528301886792,"Nhd .
(15)"
CORRECTED REGRESSION,0.5943396226415094,"(2) Under the same conditions as Theorem 2,
257"
CORRECTED REGRESSION,0.5962264150943396,"E

sup
A
sup
x |ˆηc(x) −η(x)|

≲Tq"
CORRECTED REGRESSION,0.5981132075471698,"Nhd + h + ln N
√"
CORRECTED REGRESSION,0.6,"Nhd .
(16)"
CORRECTED REGRESSION,0.6018867924528302,"The proof is shown in section 6 in the supplementary material. Compared with Theorem 3, with
258"
CORRECTED REGRESSION,0.6037735849056604,"T ∼ln N and a proper h, the upper and lower bound nearly match.
259"
CORRECTED REGRESSION,0.6056603773584905,"Now we discuss the practical implementation. (13) can not be calculated directly for a continuous
260"
CORRECTED REGRESSION,0.6075471698113207,"function. Therefore, we find a approximate numerical solution instead. The detail of practical
261"
CORRECTED REGRESSION,0.6094339622641509,"implementation is shown in section 1 in the supplementary material.
262"
NUMERICAL EXAMPLES,0.6113207547169811,"5
Numerical Examples
263"
NUMERICAL EXAMPLES,0.6132075471698113,"In this section we show some numerical experiments. In particular, we show the curve of the growth
264"
NUMERICAL EXAMPLES,0.6150943396226415,"of mean square error over the attacked sample size q.
265"
NUMERICAL EXAMPLES,0.6169811320754717,"For each case, we generate N = 10000 training samples, with each sample follows uniform distribu-
266"
NUMERICAL EXAMPLES,0.6188679245283019,"tion in [0, 1]d. The kernel function is
267"
NUMERICAL EXAMPLES,0.620754716981132,"K(u) = 2 −|u|, ∀|u| ≤1.
(17)"
NUMERICAL EXAMPLES,0.6226415094339622,"We compare the performance of kernel regression, the median-of-means method, initial estimate,
268"
NUMERICAL EXAMPLES,0.6245283018867924,"and the corrected estimation under multiple attack strategies. For kernel regression, the output is
269"
NUMERICAL EXAMPLES,0.6264150943396226,"max(min(ˆηNW , M), −M), in which ˆηNW is the simple kernel regression defined in (4). We truncate
270"
NUMERICAL EXAMPLES,0.6283018867924528,"the result into [−M, M] for a fair comparison with robust estimators. For the median-of-means
271"
NUMERICAL EXAMPLES,0.630188679245283,"method, we divide the training samples into 20 groups randomly, and then conduct kernel regression
272"
NUMERICAL EXAMPLES,0.6320754716981132,"for each group and then find the median, i.e.
273"
NUMERICAL EXAMPLES,0.6339622641509434,"ˆηMoM = Clip(med({ˆη(1)
NW , . . . , ˆη(m)
NW }), [−M, M]).
(18)
For the initial estimator (5), the parameters are T = 1 and M = 3. The corrected estimate uses (3)
274"
NUMERICAL EXAMPLES,0.6358490566037736,"in the supplementary material. For d = 1, the grid count is m = 50. For d = 2, m1 = m2 = 20.
275"
NUMERICAL EXAMPLES,0.6377358490566037,"Consider that the optimal bandwidth need to increase with the dimension, in (4), the bandwidths of
276"
NUMERICAL EXAMPLES,0.6396226415094339,"all these four methods are set to be h = 0.03 for one dimensional distribution, and h = 0.1 for two
277"
NUMERICAL EXAMPLES,0.6415094339622641,"dimensional case.
278"
NUMERICAL EXAMPLES,0.6433962264150943,"The attack strategies are designed as following. Let q = 500k for k = 0, 1, . . . , 10.
279"
NUMERICAL EXAMPLES,0.6452830188679245,"Definition 1. There are three strategies, namely, random attack, one directional attack, and concen-
280"
NUMERICAL EXAMPLES,0.6471698113207547,"trated attack, which are defined as following:
281"
NUMERICAL EXAMPLES,0.6490566037735849,"(1) Random Attack. The attacker randomly select q samples among the training data to attack. The
282"
NUMERICAL EXAMPLES,0.6509433962264151,"value of each attacked sample is −10 or 10 with equal probability;
283"
NUMERICAL EXAMPLES,0.6528301886792452,"(2) One directional Attack. The attacker randomly select q samples among the training data to attack.
284"
NUMERICAL EXAMPLES,0.6547169811320754,"The value of all attacked samples are 10;
285"
NUMERICAL EXAMPLES,0.6566037735849056,"(3) Concentrated Attack. The attacker pick two random locations c1, c2 that are uniformly distributed
286"
NUMERICAL EXAMPLES,0.6584905660377358,"in [0, 1]d. For ⌊q/2⌋samples that are closest to c1, modify their values to 10. For ⌊q/2⌋samples that
287"
NUMERICAL EXAMPLES,0.660377358490566,"are closest to c2, modify their values to −10.
288"
NUMERICAL EXAMPLES,0.6622641509433962,"0
1000
2000
3000
4000
5000
q 0.1 0.2 0.3 0.4 0.5"
NUMERICAL EXAMPLES,0.6641509433962264,Root MSE
NUMERICAL EXAMPLES,0.6660377358490566,"kernel
MoM
initial
corrected"
NUMERICAL EXAMPLES,0.6679245283018868,"(a) Squared root of ℓ2 error, random
attack."
NUMERICAL EXAMPLES,0.6698113207547169,"0
1000
2000
3000
4000
5000
q 0.0 0.5 1.0 1.5 2.0 2.5 3.0"
NUMERICAL EXAMPLES,0.6716981132075471,Root MSE
NUMERICAL EXAMPLES,0.6735849056603773,"kernel
MoM
initial
corrected"
NUMERICAL EXAMPLES,0.6754716981132075,"(b) Squared root of ℓ2 error, one di-
rectional attack."
NUMERICAL EXAMPLES,0.6773584905660377,"0
1000
2000
3000
4000
5000
q 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6"
NUMERICAL EXAMPLES,0.6792452830188679,Root MSE
NUMERICAL EXAMPLES,0.6811320754716981,"kernel
MoM
initial
corrected"
NUMERICAL EXAMPLES,0.6830188679245283,"(c) Squared root of ℓ2 error, concen-
trated attack."
NUMERICAL EXAMPLES,0.6849056603773584,"0
1000
2000
3000
4000
5000
q 0.2 0.4 0.6 0.8 1.0 1.2"
NUMERICAL EXAMPLES,0.6867924528301886,Supremum Error
NUMERICAL EXAMPLES,0.6886792452830188,"kernel
MoM
initial
corrected"
NUMERICAL EXAMPLES,0.690566037735849,"(d) ℓ∞error, random attack."
NUMERICAL EXAMPLES,0.6924528301886792,"0
1000
2000
3000
4000
5000
q 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0"
NUMERICAL EXAMPLES,0.6943396226415094,Supremum Error
NUMERICAL EXAMPLES,0.6962264150943396,"kernel
MoM
initial
corrected"
NUMERICAL EXAMPLES,0.6981132075471698,"(e) ℓ∞error, one directional attack."
NUMERICAL EXAMPLES,0.7,"0
1000
2000
3000
4000
5000
q 0.5 1.0 1.5 2.0"
NUMERICAL EXAMPLES,0.7018867924528301,Supremum Error
NUMERICAL EXAMPLES,0.7037735849056603,"kernel
MoM
initial
corrected"
NUMERICAL EXAMPLES,0.7056603773584905,"(f) ℓ∞error, concentrated attack."
NUMERICAL EXAMPLES,0.7075471698113207,Figure 2: Comparison of ℓ2 and ℓ∞error between various methods for one dimensional distribution.
NUMERICAL EXAMPLES,0.7094339622641509,"For one dimensional distribution, let the ground truth be
289"
NUMERICAL EXAMPLES,0.7113207547169811,"η1(x) = sin(2πx).
(19)
For two dimensional distribution,
290"
NUMERICAL EXAMPLES,0.7132075471698113,"η(x) = sin(2πx1) + cos(2πx2).
(20)"
NUMERICAL EXAMPLES,0.7150943396226415,"The noise follows standard Gaussian distribution N(0, 1). The performances are evaluated using
291"
NUMERICAL EXAMPLES,0.7169811320754716,"square root of ℓ2 error, as well as ℓ∞error. The results are shown in Figure 2 and 3 for one and
292"
NUMERICAL EXAMPLES,0.7188679245283018,"two dimensional distributions, respectively. In these figures, each point is the average over 1000
293"
NUMERICAL EXAMPLES,0.720754716981132,"independent trials.
294"
NUMERICAL EXAMPLES,0.7226415094339622,"Figure 2 and 3 show that the simple kernel regression (blue dotted line) fails under poisoning attack.
295"
NUMERICAL EXAMPLES,0.7245283018867924,"The ℓ2 and ℓ∞error grows fast with the increase of q. Besides, traditional median-of-means does
296"
NUMERICAL EXAMPLES,0.7264150943396226,"not improve over kernel regression. Moreover, the initial estimator (5) (orange dash-dot line) shows
297"
NUMERICAL EXAMPLES,0.7283018867924528,"significantly better performance than kernel estimator under random and one directional attack, as
298"
NUMERICAL EXAMPLES,0.730188679245283,"are shown in Fig.2 and 3, (a), (b), (d), (e). However, if the attacked samples concentrate around some
299"
NUMERICAL EXAMPLES,0.7320754716981132,"centers, then the initial estimator fails. Compared with kernel regression, there is some but limited
300"
NUMERICAL EXAMPLES,0.7339622641509433,"improvement for (5). Finally, the corrected estimator (red solid line) performs well under all attack
301"
NUMERICAL EXAMPLES,0.7358490566037735,"strategies. Under random attack, the corrected estimator performs nearly the same as initial one. For
302"
NUMERICAL EXAMPLES,0.7377358490566037,"one directional attack, the corrected estimator performs better than the initial one with large q. Under
303"
NUMERICAL EXAMPLES,0.7396226415094339,"concentrated attack, the correction shows significant improvement. These results are consistent with
304"
NUMERICAL EXAMPLES,0.7415094339622641,"our theoretical analysis.
305"
NUMERICAL EXAMPLES,0.7433962264150943,"0
1000
2000
3000
4000
5000
q 0.1 0.2 0.3 0.4 0.5 0.6 0.7"
NUMERICAL EXAMPLES,0.7452830188679245,Root MSE
NUMERICAL EXAMPLES,0.7471698113207547,"kernel
MoM
initial
corrected"
NUMERICAL EXAMPLES,0.7490566037735849,"(a) Squared root of ℓ2 error, random
attack."
NUMERICAL EXAMPLES,0.7509433962264151,"0
1000
2000
3000
4000
5000
q 0.0 0.5 1.0 1.5 2.0 2.5 3.0"
NUMERICAL EXAMPLES,0.7528301886792453,Root MSE
NUMERICAL EXAMPLES,0.7547169811320755,"kernel
MoM
initial
corrected"
NUMERICAL EXAMPLES,0.7566037735849057,"(b) Squared root of ℓ2 error, one di-
rectional attack."
NUMERICAL EXAMPLES,0.7584905660377359,"0
1000
2000
3000
4000
5000
q 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25"
NUMERICAL EXAMPLES,0.7603773584905661,Root MSE
NUMERICAL EXAMPLES,0.7622641509433963,"kernel
MoM
initial
corrected"
NUMERICAL EXAMPLES,0.7641509433962265,"(c) Squared root of ℓ2 error, concen-
trated attack."
NUMERICAL EXAMPLES,0.7660377358490567,"0
1000
2000
3000
4000
5000
q 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25"
NUMERICAL EXAMPLES,0.7679245283018868,Supremum Error
NUMERICAL EXAMPLES,0.769811320754717,"kernel
MoM
initial
corrected"
NUMERICAL EXAMPLES,0.7716981132075472,"(d) ℓ∞error, random attack."
NUMERICAL EXAMPLES,0.7735849056603774,"0
1000
2000
3000
4000
5000
q 1 2 3 4 5"
NUMERICAL EXAMPLES,0.7754716981132076,Supremum Error
NUMERICAL EXAMPLES,0.7773584905660378,"kernel
MoM
initial
corrected"
NUMERICAL EXAMPLES,0.779245283018868,"(e) ℓ∞error, one directional attack."
NUMERICAL EXAMPLES,0.7811320754716982,"0
1000
2000
3000
4000
5000
q 1 2 3 4"
NUMERICAL EXAMPLES,0.7830188679245284,Supremum Error
NUMERICAL EXAMPLES,0.7849056603773585,"kernel
MoM
initial
corrected"
NUMERICAL EXAMPLES,0.7867924528301887,"(f) ℓ∞error, concentrated attack."
NUMERICAL EXAMPLES,0.7886792452830189,Figure 3: Comparison of ℓ2 and ℓ∞error between various methods for one dimensional distribution.
LIMITATIONS,0.7905660377358491,"6
Limitations
306"
LIMITATIONS,0.7924528301886793,"The major limitation is that for high dimensional feature distributions, the corrected estimator can be
307"
LIMITATIONS,0.7943396226415095,"computationally expensive, since the number of grids grows exponentially with the dimensionality.
308"
LIMITATIONS,0.7962264150943397,"Moreover, our theoretical results rely on Assumption 1. Nevertheless, it is not hard to generalize
309"
LIMITATIONS,0.7981132075471699,"these assumptions. For (a), we can use a local polynomial method to improve the convergence rate if
310"
LIMITATIONS,0.8,"η satisfies higher order of smoothness. (b) limits the feature distribution. Actually, our analysis can
311"
LIMITATIONS,0.8018867924528302,"be extended to heavy tail cases, in which the bandwidth can be made adaptive, such as [36,37]. In
312"
LIMITATIONS,0.8037735849056604,"order to achieve better tradeoff between bias and variance, in the regions with high pdf, bandwidth
313"
LIMITATIONS,0.8056603773584906,"h need to be smaller, and vice versa. Currently, we only focus on distributions without tails. (d)
314"
LIMITATIONS,0.8075471698113208,"requires that the noise is sub-exponential. Such restriction can also be extended to the case in which
315"
LIMITATIONS,0.809433962264151,"the noise is only assumed to have bounded moments. In this case, we can let T grow faster with N.
316"
LIMITATIONS,0.8113207547169812,"Despite that we are convinced that all these assumptions can be extended with some modification, the
317"
LIMITATIONS,0.8132075471698114,"current results focus on a simpler situation.
318"
CONCLUSION,0.8150943396226416,"7
Conclusion
319"
CONCLUSION,0.8169811320754717,"In this paper, we have provided a theoretical analysis of robust nonparametric regression problem
320"
CONCLUSION,0.8188679245283019,"under adversarial attack. In particular, we have derived the convergence rate of an M-estimator
321"
CONCLUSION,0.8207547169811321,"based on Huber loss minimization. We have also derived the information theoretic minimax lower
322"
CONCLUSION,0.8226415094339623,"bound, which is the underlying limit of robust nonparametric regression. The result shows that the
323"
CONCLUSION,0.8245283018867925,"initial estimator has minimax optimal ℓ∞risk. With q ≲
q"
CONCLUSION,0.8264150943396227,"N/ ln2 N, in which q is the number
324"
CONCLUSION,0.8283018867924529,"of adversarial samples, ℓ2 risk is also optimal. However, for large q, the initial estimator becomes
325"
CONCLUSION,0.8301886792452831,"suboptimal. In particular, if the attacker focus their attack around some centers, then the resulting
326"
CONCLUSION,0.8320754716981132,"estimate shows some undesirable spikes at these centers. Actually, the drawback of initial estimator is
327"
CONCLUSION,0.8339622641509434,"that it does not make full use of the continuity of regression function, and hence unable to detect spikes
328"
CONCLUSION,0.8358490566037736,"and correct the estimate. Motivated by such discussion, we have proposed a correction technique,
329"
CONCLUSION,0.8377358490566038,"which is a nonlinear filter that projects the estimated function into the space of Lipschitz functions.
330"
CONCLUSION,0.839622641509434,"Our theoretical analysis shows that the corrected estimator is minimax optimal even for large q.
331"
CONCLUSION,0.8415094339622642,"Numerical experiments validate our theoretical analysis.
332"
REFERENCES,0.8433962264150944,"References
333"
REFERENCES,0.8452830188679246,"[1] Natarajan, N., I. S. Dhillon, P. K. Ravikumar, et al. Learning with noisy labels. In Advances in
334"
REFERENCES,0.8471698113207548,"Neural Information Processing Systems, vol. 26. 2013.
335"
REFERENCES,0.8490566037735849,"[2] Van Rooyen, B., R. C. Williamson. A theory of learning with corrupted labels. J. Mach. Learn.
336"
REFERENCES,0.8509433962264151,"Res., 18(1):8501–8550, 2017.
337"
REFERENCES,0.8528301886792453,"[3] Jiang, L., Z. Zhou, T. Leung, et al. Mentornet: Learning data-driven curriculum for very deep
338"
REFERENCES,0.8547169811320755,"neural networks on corrupted labels. In International conference on machine learning, pages
339"
REFERENCES,0.8566037735849057,"2304–2313. PMLR, 2018.
340"
REFERENCES,0.8584905660377359,"[4] Liu, T., D. Tao. Classification with noisy labels by importance reweighting. IEEE Transactions
341"
REFERENCES,0.8603773584905661,"on pattern analysis and machine intelligence, 38(3):447–461, 2015.
342"
REFERENCES,0.8622641509433963,"[5] Gao, W., B.-B. Yang, Z.-H. Zhou. On the resistance of nearest neighbor to random noisy labels.
343"
REFERENCES,0.8641509433962264,"arXiv preprint arXiv:1607.07526, 2016.
344"
REFERENCES,0.8660377358490566,"[6] Menon, A., B. Van Rooyen, C. S. Ong, et al. Learning from corrupted binary labels via
345"
REFERENCES,0.8679245283018868,"class-probability estimation. In International conference on machine learning, pages 125–134.
346"
REFERENCES,0.869811320754717,"PMLR, 2015.
347"
REFERENCES,0.8716981132075472,"[7] Patrini, G., F. Nielsen, R. Nock, et al. Loss factorization, weakly supervised learning and label
348"
REFERENCES,0.8735849056603774,"noise robustness. In International Conference on Machine Learning, pages 708–717. PMLR,
349"
REFERENCES,0.8754716981132076,"2016.
350"
REFERENCES,0.8773584905660378,"[8] Van Rooyen, B., A. Menon, R. C. Williamson. Learning with symmetric label noise: The
351"
REFERENCES,0.879245283018868,"importance of being unhinged. In Advances in Neural Information Processing Systems, vol. 28.
352"
REFERENCES,0.8811320754716981,"2015.
353"
REFERENCES,0.8830188679245283,"[9] Wang, R., T. Liu, D. Tao. Multiclass learning with partially corrupted labels. IEEE transactions
354"
REFERENCES,0.8849056603773585,"on neural networks and learning systems, 29(6):2568–2580, 2017.
355"
REFERENCES,0.8867924528301887,"[10] Reeve, H., A. Kabán. Fast rates for a knn classifier robust to unknown asymmetric label noise.
356"
REFERENCES,0.8886792452830189,"In International Conference on Machine Learning, pages 5401–5409. PMLR, 2019.
357"
REFERENCES,0.8905660377358491,"[11] Biggio, B., B. Nelson, P. Laskov. Poisoning attacks against support vector machines. In
358"
REFERENCES,0.8924528301886793,"International Conference on Machine Learning. 2012.
359"
REFERENCES,0.8943396226415095,"[12] Xiao, H., B. Biggio, G. Brown, et al. Is feature selection secure against training data poisoning?
360"
REFERENCES,0.8962264150943396,"In International Conference on Machine Learning, pages 1689–1698. PMLR, 2015.
361"
REFERENCES,0.8981132075471698,"[13] Jagielski, M., A. Oprea, B. Biggio, et al. Manipulating machine learning: Poisoning attacks
362"
REFERENCES,0.9,"and countermeasures for regression learning. In 2018 IEEE symposium on security and privacy
363"
REFERENCES,0.9018867924528302,"(SP), pages 19–35. IEEE, 2018.
364"
REFERENCES,0.9037735849056604,"[14] Szegedy, C., W. Zaremba, I. Sutskever, et al. Intriguing properties of neural networks. In
365"
REFERENCES,0.9056603773584906,"International Conference on Learning Representations. 2014.
366"
REFERENCES,0.9075471698113208,"[15] Goodfellow, I. J., J. Shlens, C. Szegedy. Explaining and harnessing adversarial examples. In
367"
REFERENCES,0.909433962264151,"International Conference on Learning Representations. 2015.
368"
REFERENCES,0.9113207547169812,"[16] Madry, A., A. Makelov, L. Schmidt, et al. Towards deep learning models resistant to adversarial
369"
REFERENCES,0.9132075471698113,"attacks. In International Conference on Learning Representations. 2018.
370"
REFERENCES,0.9150943396226415,"[17] Mao, C., Z. Zhong, J. Yang, et al. Metric learning for adversarial robustness. In Advances in
371"
REFERENCES,0.9169811320754717,"Neural Information Processing Systems, vol. 32. 2019.
372"
REFERENCES,0.9188679245283019,"[18] Steinhardt, J., P. W. W. Koh, P. S. Liang. Certified defenses for data poisoning attacks. In
373"
REFERENCES,0.9207547169811321,"Advances in Neural Information Processing Systems, vol. 30. 2017.
374"
REFERENCES,0.9226415094339623,"[19] Koh, P. W., P. Liang. Understanding black-box predictions via influence functions. In Interna-
375"
REFERENCES,0.9245283018867925,"tional Conference on Machine Learning, pages 1885–1894. PMLR, 2017.
376"
REFERENCES,0.9264150943396227,"[20] Ribeiro, A. H., T. B. Schön. Overparameterized linear regression under adversarial attacks.
377"
REFERENCES,0.9283018867924528,"IEEE Transactions on Signal Processing, 71:601–614, 2023.
378"
REFERENCES,0.930188679245283,"[21] Lecué, G., M. Lerasle. Robust machine learning by median-of-means: theory and practice.
379"
REFERENCES,0.9320754716981132,"Annals of Statistics, 2020.
380"
REFERENCES,0.9339622641509434,"[22] Liu, C., B. Li, Y. Vorobeychik, et al. Robust linear regression against training data poisoning.
381"
REFERENCES,0.9358490566037736,"In Proceedings of the 10th ACM workshop on artificial intelligence and security, pages 91–102.
382"
REFERENCES,0.9377358490566038,"2017.
383"
REFERENCES,0.939622641509434,"[23] Huber, P. J. Robust Statistics. John Wiley & Sons, 1981.
384"
REFERENCES,0.9415094339622642,"[24] Rousseeuw, P. J. Least median of squares regression. Journal of the American statistical
385"
REFERENCES,0.9433962264150944,"association, 79(388):871–880, 1984.
386"
REFERENCES,0.9452830188679245,"[25] Rousseeuw, P. J., A. M. Leroy. Robust regression and outlier detection. John wiley & sons,
387"
REFERENCES,0.9471698113207547,"2005.
388"
REFERENCES,0.9490566037735849,"[26] Nadaraya, E. A. On estimating regression. Theory of Probability & Its Applications, 9(1):141–
389"
REFERENCES,0.9509433962264151,"142, 1964.
390"
REFERENCES,0.9528301886792453,"[27] Salibian-Barrera, M. Robust nonparametric regression: review and practical considerations.
391"
REFERENCES,0.9547169811320755,"arXiv preprint arXiv:2211.08376, 2022.
392"
REFERENCES,0.9566037735849057,"[28] Hall, P., M. Jones. Adaptive m-estimation in nonparametric regression. Annals of Statistics,
393"
REFERENCES,0.9584905660377359,"pages 1712–1728, 1990.
394"
REFERENCES,0.960377358490566,"[29] Tsybakov, A. B. Introduction to Nonparametric Estimation. Springer Series in Statistics, 2009.
395"
REFERENCES,0.9622641509433962,"[30] Watson, G. S. Smooth regression analysis. Sankhy¯a: The Indian Journal of Statistics, Series A,
396"
REFERENCES,0.9641509433962264,"pages 359–372, 1964.
397"
REFERENCES,0.9660377358490566,"[31] Devroye, L. P. The uniform convergence of the nadaraya-watson regression function estimate.
398"
REFERENCES,0.9679245283018868,"Canadian Journal of Statistics, 6(2):179–191, 1978.
399"
REFERENCES,0.969811320754717,"[32] Nemirovskij, A. S., D. B. Yudin. Problem complexity and method efficiency in optimization.
400"
REFERENCES,0.9716981132075472,"Wiley-Interscience Series in Discrete Mathematics, 1983.
401"
REFERENCES,0.9735849056603774,"[33] Ben-Hamou, A., A. Guyader. Robust non-parametric regression via median-of-means. arXiv
402"
REFERENCES,0.9754716981132076,"preprint arXiv:2301.10498, 2023.
403"
REFERENCES,0.9773584905660377,"[34] Bickel, P. J. On some robust estimates of location. The Annals of Mathematical Statistics, pages
404"
REFERENCES,0.9792452830188679,"847–858, 1965.
405"
REFERENCES,0.9811320754716981,"[35] Dhar, S., P. Jha, P. Rakshit. The trimmed mean in non-parametric regression function estimation.
406"
REFERENCES,0.9830188679245283,"Theory of Probability and Mathematical Statistics, 107:133–158, 2022.
407"
REFERENCES,0.9849056603773585,"[36] Herrmann, E. Local bandwidth choice in kernel regression estimation. Journal of Computational
408"
REFERENCES,0.9867924528301887,"and Graphical Statistics, 6(1):35–54, 1997.
409"
REFERENCES,0.9886792452830189,"[37] Zhao, P., L. Lai. Minimax rate optimal adaptive nearest neighbor classification and regression.
410"
REFERENCES,0.9905660377358491,"IEEE Transactions on Information Theory, 67(5):3155–3182, 2021.
411"
REFERENCES,0.9924528301886792,"[38] Krzyzak, A. The rates of convergence of kernel regression estimates and classification rules.
412"
REFERENCES,0.9943396226415094,"IEEE Transactions on Information Theory, 32(5):668–679, 1986.
413"
REFERENCES,0.9962264150943396,"[39] Andrews, D. F., F. R. Hampel. Robust estimates of location: Survey and advances, vol. 1280.
414"
REFERENCES,0.9981132075471698,"Princeton University Press, 2015.
415"
