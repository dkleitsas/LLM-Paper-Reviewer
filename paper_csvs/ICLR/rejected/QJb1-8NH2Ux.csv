Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.005154639175257732,"Making classiﬁers robust to adversarial examples is challenging. Thus, many de-
fenses tackle the seemingly easier task of detecting perturbed inputs.
We show a barrier towards this goal. We prove a general hardness reduction be-
tween detection and classiﬁcation of adversarial examples: given a robust detector
for attacks at distance ϵ (in some metric), we show how to build a similarly robust
(but inefﬁcient) classiﬁer for attacks at distance ϵ/2—and vice-versa.
Our reduction is computationally inefﬁcient, and thus cannot be used to build
practical classiﬁers. Instead, it is a useful sanity check to test whether empiri-
cal detection results imply something much stronger than the authors presumably
anticipated.
To illustrate, we revisit 14 empirical detector defenses published over the past
years. For 12/14 defenses, we show that the claimed detection results imply an
inefﬁcient classiﬁer with robustness far beyond the state-of-the-art— thus casting
some doubts on the results’ validity.
Finally, we show that our reduction applies in both directions: a robust classi-
ﬁer for attacks at distance ϵ/2 implies an inefﬁcient robust detector at distance ϵ.
Thus, we argue that robust classiﬁcation and robust detection should be regarded
as (near)-equivalent problems, if we disregard their computational complexity."
INTRODUCTION,0.010309278350515464,"1
INTRODUCTION"
INTRODUCTION,0.015463917525773196,"Building models that are robust to adversarial examples (Szegedy et al., 2014; Biggio et al., 2013) is
a major challenge and open-problem in machine learning. Due to the inherent difﬁculty in building
robust classiﬁers, researchers have attempted to build techniques to at least detect adversarial exam-
ples, a weaker task that is largely considered easier than robust classiﬁcation (Xu et al., 2018; Pang
et al., 2021; Sheikholeslami et al., 2021)."
INTRODUCTION,0.020618556701030927,"Yet, evaluating the robustness of empirical detector defenses is challenging. This is in part due to
a lack of strong evaluation guidelines and benchmarks—akin to those developed for robust classi-
ﬁers (Carlini et al., 2019; Croce et al., 2020)—as well as to a lack of long-standing comparative
baselines such as adversarial training (Madry et al., 2018)."
INTRODUCTION,0.02577319587628866,"To illustrate, consider the following (ﬁctitious) claims about two defenses against adversarial exam-
ples on CIFAR-10:"
INTRODUCTION,0.030927835051546393,"• defense A is a classiﬁer that achieves robust accuracy of 90% under ℓ∞-perturbations
bounded by ϵ = 4/255;
• defense B also has a “rejection” option, and achieves robust accuracy of 90% under ℓ∞-
perturbations bounded by ϵ = 8/255 (we say that defense B is robust for some example
if it classiﬁes that example correctly, and either rejects/detects or correctly classiﬁes all
perturbed examples at distance ϵ.)"
INTRODUCTION,0.03608247422680412,Which of these two (empirical) claims are you more likely to believe to be correct?
INTRODUCTION,0.041237113402061855,"Defense A claims much higher robustness than the current best result achieved with adversarial
training (Madry et al., 2018; Rebufﬁet al., 2021), the only empirical defense against adversarial
examples that has stood the test of time. Indeed, the state-of-the-art ℓ∞robustness for ϵ = 4/255"
INTRODUCTION,0.04639175257731959,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.05154639175257732,"on CIFAR-10 (without external data) is ≈79% (Rebufﬁet al., 2021). Thus, the claim of defense A
would likely be met with some initial skepticism and heightened scrutiny, as could be expected for
such a claimed breakthrough result."
INTRODUCTION,0.05670103092783505,"The claim of defense B is harder to assess, due to a lack of long-standing baselines for robust
detectors (many detection defenses have been shown to be broken (Carlini & Wagner, 2017; Tram`er
et al., 2020)). On one hand, detection of adversarial examples has largely been considered to be an
easier task than classiﬁcation (Xu et al., 2018; Pang et al., 2021; Sheikholeslami et al., 2021). On
the other hand, defense B claims robustness to perturbations that are twice as large as defense A
(ϵ = 8/255 vs. ϵ = 4/255)."
INTRODUCTION,0.061855670103092786,"In this paper, we show that the claims of defenses A and B are, in fact, equivalent! (up to computa-
tional efﬁciency.)"
INTRODUCTION,0.06701030927835051,"We prove a general hardness reduction between classiﬁcation and detection of adversarial examples.
Given a detector defense that achieves robust risk α for attacks at distance ϵ (under any metric), we
show how to build an explicit but inefﬁcient classiﬁer that achieves robust risk α for classifying
attacks at distance ϵ/2. The reverse implication also holds: a classiﬁer robust at distance ϵ/2 implies
an explicit but inefﬁcient robust detector at distance ϵ."
INTRODUCTION,0.07216494845360824,"To the authors knowledge, there is no known way of leveraging computational inefﬁciency to build
more robust models. We should thus be as “surprised” by the claim made by defense B as by the
claim made by defense A."
INTRODUCTION,0.07731958762886598,"Our reduction provides a way of assessing the plausibility of new robust detection claims, by con-
trasting them with results from the more mature literature on robust classiﬁcation. To illustrate, we
revisit 14 published detection defenses across three datasets, and show that in 12/14 cases the de-
fense’s robust detection claims would imply an inefﬁcient classiﬁer with robustness far superior to
the current state-of-the-art. Yet, none of these detection papers make the claim that their techniques
should imply such a breakthrough in robust classiﬁcation."
INTRODUCTION,0.08247422680412371,"Using our reduction, it is obvious that many detection defenses are claiming much stronger robust-
ness than we believe feasible with current techniques. And indeed, many of these defenses were
later shown to have overestimated their robustness (Carlini & Wagner, 2017; Tram`er et al., 2020)."
INTRODUCTION,0.08762886597938144,"Remarkably, we ﬁnd that for certiﬁed defenses, the state-of-the-art results for provable robust clas-
siﬁcation and detection perfectly match the results implied by our reduction. For example, Sheik-
holeslami et al. (2021) recently proposed a certiﬁed detector on CIFAR-10 with provable robust
error that is within 3% of the provable error of the inefﬁcient detector obtained by combining our
result with the state-of-the-art robust classiﬁer of Zhang et al. (2020a)."
INTRODUCTION,0.09278350515463918,"In summary, we prove that giving classiﬁers access to a detection option does not help robustness (or
at least, not much). Our work provides, to our knowledge, the ﬁrst example of a hardness reduction
between different approaches for robust machine learning. As in the case of computational com-
plexity, we believe that such reductions can be useful for identifying research questions or areas that
are unlikely to bear fruit (bar a signiﬁcant breakthrough)—so that the majority of the community’s
efforts can be redirected elsewhere."
INTRODUCTION,0.0979381443298969,"On a technical level, our reduction exposes a natural connection between robustness and error cor-
recting codes, which may be of independent interest."
"HARDNESS REDUCTIONS BETWEEN ROBUST CLASSIFIERS AND
DETECTORS",0.10309278350515463,"2
HARDNESS REDUCTIONS BETWEEN ROBUST CLASSIFIERS AND
DETECTORS"
"HARDNESS REDUCTIONS BETWEEN ROBUST CLASSIFIERS AND
DETECTORS",0.10824742268041238,"In this section, we prove our main result: a reduction between robust detectors and robust classiﬁers,
and vice-versa. We ﬁrst introduce some useful notation and deﬁne the (robust) risk of classiﬁers
with and without a detection option."
PRELIMINARIES,0.1134020618556701,"2.1
PRELIMINARIES"
PRELIMINARIES,0.11855670103092783,"We consider a classiﬁcation task with a distribution D over examples x ∈Rd with labels y ∈
[C]. A classiﬁer is a function f : Rd →[C]. A detector is a classiﬁer with an extra “rejection”"
PRELIMINARIES,0.12371134020618557,Under review as a conference paper at ICLR 2022
PRELIMINARIES,0.12886597938144329,"or ”detection” option ⊥, that indicates the absence of a classiﬁcation. We assume for simplicity
that classiﬁers and detectors are deterministic. Our results can easily be extended to randomized
functions as well. The binary indicator function 1{A} is 1 if and only if the predicate A is true."
PRELIMINARIES,0.13402061855670103,"We ﬁrst deﬁne a classiﬁer’s risk, i.e., its classiﬁcation error on unperturbed samples."
PRELIMINARIES,0.13917525773195877,"Deﬁnition 1 (Risk). Let f : Rd →[C] ∪{⊥} be a classiﬁer (optionally with a detection output ⊥).
The risk of f is the expected rate at which f fails to correctly classify a sample:"
PRELIMINARIES,0.14432989690721648,"R(f) :=
E
(x,y)∼D"
PRELIMINARIES,0.14948453608247422,"
1{f(x)̸=y}

(1)"
PRELIMINARIES,0.15463917525773196,"Note that for a detector, rejecting an unperturbed example sampled from the distribution D is counted
as an error."
PRELIMINARIES,0.15979381443298968,"For classiﬁers without a rejection option, we deﬁne the robust risk as the risk on worst-case adver-
sarial examples (Madry et al., 2018). Given an input x sampled from D, an adversarial example ˆx
is constrained to being within distance d(x, ˆx) ≤ϵ from x, where d is some distance measure."
PRELIMINARIES,0.16494845360824742,Deﬁnition 2 (Robust risk). Let f : Rd →[C] be a classiﬁer. The robust risk at distance ϵ is:
PRELIMINARIES,0.17010309278350516,"Rϵ
adv(f) :=
E
(x,y)∼D"
PRELIMINARIES,0.17525773195876287,"
max
d(x,ˆx)≤ϵ 1{f(ˆx)̸=y} 
(2)"
PRELIMINARIES,0.18041237113402062,"Thus, a sample (x, y) is robustly classiﬁed if and only if every point within distance ϵ of x (including
x itself) is correctly classiﬁed as y."
PRELIMINARIES,0.18556701030927836,"For a detector (a classiﬁer with an extra detection/rejection output), we analogously deﬁne the robust
risk with detection. The classiﬁer is now allowed to reject adversarial examples."
PRELIMINARIES,0.19072164948453607,"Deﬁnition 3 (Robust risk with detection). Let f : Rd →[C] ∪{⊥} be a classiﬁer with an extra
detection output ⊥. The robust risk with detection at distance ϵ is:"
PRELIMINARIES,0.1958762886597938,"Rϵ
adv-det(f) :=
E
(x,y)∼D"
PRELIMINARIES,0.20103092783505155,"
max
d(x,ˆx)≤ϵ 1{f(x)̸=y ∨f(ˆx)/∈{y,⊥}} 
(3)"
PRELIMINARIES,0.20618556701030927,"That is, a detector defense f is robust on a natural input x if and only if the defense classiﬁes the
natural input x correctly, and the defense either rejects or correctly classiﬁes every perturbed input
ˆx within distance ϵ from x. The requirement that the defense correctly classify natural examples
eliminates pathological defenses that reject all inputs."
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.211340206185567,"2.2
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION"
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.21649484536082475,"We are now ready to introduce our main result, a reduction from a robust detector for adversarial
examples at distance ϵ, to an inefﬁcient robust classiﬁer at distance ϵ/2. We later prove that this
reduction also holds in the reverse direction, thereby demonstrating the equivalence between robust
detection and classiﬁcation—up to computational hardness."
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.22164948453608246,"Theorem 4 (ϵ-robust detection implies inefﬁcient ϵ/2-robust classiﬁcation). Let d(·, ·) be an ar-
bitrary metric. Let f be a detector that achieves risk R(f) = α, and robust risk with detection
Rϵ
adv-det(f) = β. Then, we can construct an explicit (but inefﬁcient) classiﬁer g that achieves risk
R(g) ≤α and robust risk Rϵ/2
adv (g) ≤β."
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.2268041237113402,The classiﬁer g is constructed as follows on input x:
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.23195876288659795,"• Run the detector model y ←f(x). If the input is not rejected, i.e., y ̸= ⊥, then output the label
y that was predicted by the detector."
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.23711340206185566,"• Otherwise, ﬁnd an input x′ within distance ϵ/2 of x that is not rejected, i.e., d(x, x′) ≤ϵ/2 and
f(x′) ̸= ⊥. If such an input x′ exists, output the label y ←f(x′). Else, output a uniformly
random label y ∈[C]."
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.2422680412371134,"An intuitive illustration for our construction, and for the proof of the theorem (see below) is in
Figure 1."
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.24742268041237114,Under review as a conference paper at ICLR 2022 𝜖/2 𝜖 x x̂
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.25257731958762886,"Figure 1: Illustration of the construction of a robust classiﬁer from a robust detector in Theorem 4.
The outer blue circle represents all inputs at distance at most ϵ from the input x. For a detector f,
the areas in green correspond to correctly classiﬁed inputs, and ratcheted gray areas correspond to
rejected inputs. The detector f is thus robust on x up to distance ϵ. The classiﬁer g classiﬁes a
perturbed input ˆx, at distance ϵ/2 from x, by ﬁnding any input within distance ϵ/2 from ˆx (the red
dashed circle) that is not rejected by f. Such an input necessarily exists and is correctly labeled by
f. The classiﬁer g is thus robust on x up to distance ϵ/2."
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.25773195876288657,"Our construction can be viewed as an analog of minimum distance decoding in coding theory. We
can view a clean data point sampled from D as a codeword, and an adversarial example ˆx as a noisy
message with a certain number of errors (where the error magnitude is measured using an arbitrary
metric on Rd rather than the Hamming distance that is typically used for error correcting codes).
A standard result in coding theory states that if a code can detect α errors, then it can correct α/2
errors. This result follows from a “ball-packing” argument: if α errors can be detected, then any
two valid codewords must be at least at distance α from each other, and therefore α/2 errors can be
corrected via minimum distance decoding."
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.26288659793814434,"Proof of Theorem 4. First, note that the natural accuracy of our constructed classiﬁer g is at least as
high as that of the detector f, since g always mimics the output of f whenever f does not reject an
input sampled from D. Thus, R(g) ≤R(f) = α."
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.26804123711340205,"Now, for the sake of contradiction, consider an input (x, y) ∼D for which the constructed classiﬁer
g is not robust at distance ϵ/2. By construction, this means that there exists some input ˆx at distance
ϵ/2 from x such that ˆx is misclassiﬁed, i.e., g(ˆx) = ˆy ̸= y. We will show that the detector f is not
robust with detection for x either (for attacks at distance up to ϵ)."
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.27319587628865977,"By deﬁnition of the classiﬁer g, if g(ˆx) = ˆy ̸= y then either:"
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.27835051546391754,"• The detector f also misclassiﬁes ˆx, i.e., f(ˆx) = ˆy."
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.28350515463917525,So f is not robust with detection for x at distance ϵ.
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.28865979381443296,"• There exists an input x′ within distance ϵ/2 of x, such that the detector f misclassiﬁes x′,
i.e. f(x′) = ˆy."
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.29381443298969073,"Note that by the triangular inequality, d(x, x′) ≤d(x, ˆx) + d(ˆx, x′) ≤ϵ/2 + ϵ/2 = ϵ, and
thus f is not robust with detection for x at distance ϵ."
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.29896907216494845,"• The detector f rejects all inputs x′ within distance ϵ/2 of x (and thus g has output ˆy by
sampling a label at random)."
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.30412371134020616,"Since d(x, ˆx) ≤ϵ/2, this implies that the detector also rejects the clean input x, i.e.,
f(x) = ⊥, and thus f is not robust with detection for x."
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.30927835051546393,Under review as a conference paper at ICLR 2022
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.31443298969072164,"In summary, whenever the constructed classiﬁer g fails to robustly classify an input x up to dis-
tance ϵ/2, the detector f also fails to robustly classify x with detection up to distance ϵ. Taking
expectations over the entire distribution D concludes the proof."
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.31958762886597936,"Note that the classiﬁer g constructed in Theorem 4 is computationally inefﬁcient. Indeed, the second
step of the defense consists in ﬁnding a non-rejected input within some metric ball. If the original
detector f is a non-convex function (e.g., a deep neural network), then this step consists in solving
an intractable non-convex optimization problem. Our reduction is thus typically not suitable for
building a practical robust classiﬁer. Instead, it demonstrates the existence of an inefﬁcient but
explicit robust classiﬁer. We discuss the implications of this result more thoroughly in Section 3."
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.3247422680412371,"A corollary to our reduction is that many “information theoretic” results about robust classiﬁers can
be directly extended to robust detectors. For example, Tsipras et al. (2019) prove that there exists a
formal tradeoff between a classiﬁer’s clean accuracy and robust accuracy for certain natural tasks.
Since their result applies to any classiﬁer (including inefﬁcient ones), combining their result with our
reduction implies that a similar accuracy-robustness tradeoff exists for detectors. More precisely,
Tsipras et al. (2019) show that for certain classiﬁcation tasks and suitable choices of parameters
α, β, ϵ, any classiﬁer g which achieves risk R(g) ≤α must have robust risk at least Rϵ
adv(g) ≥β
against ℓ∞-perturbations bounded by ϵ. By our reduction, this implies that any detector f with
risk at most R(f) ≤α must also have robust risk with detection at least Rϵ/2
adv-det(f) ≥β against
ℓ∞-perturbations bounded by ϵ/2."
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.32989690721649484,"Similar arguments can be applied to show, for instance, that the increased data complexity of robust
generalization from Schmidt et al. (2018), or the tradeoff between robustness to multiple perturba-
tion types from Tram`er & Boneh (2019), also apply to robust detectors."
ROBUST DETECTION IMPLIES INEFFICIENT ROBUST CLASSIFICATION,0.33505154639175255,"Our reduction does not apply for “computational” hardness results that have been shown for robust
classiﬁcation. For example, Garg et al. (2020) and Bubeck et al. (2018) show (“unnatural”) distri-
butions where learning a robust classiﬁer is computationally hard—under standard cryptographic
assumptions. We cannot use Theorem 4 to conclude that learning a robust detector is hard for these
distributions, since the existence of such a detector would only imply an inefﬁcient robust classiﬁer
which does not contradict the results of Garg et al. (2020) or Bubeck et al. (2018)."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.3402061855670103,"2.3
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION"
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.34536082474226804,"A similar argument as in Theorem 4 can be used in the opposite direction, to show that a robust clas-
siﬁer at distance ϵ/2 implies an inefﬁcient robust detector at distance ϵ. Taken together, Theorem 4
and Theorem 5 show that robust detection and classiﬁcation are equivalent, up to a factor 2 in the
norm bound and up to computational constraints."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.35051546391752575,"Theorem 5 (ϵ/2 robust-classiﬁcation implies inefﬁcient ϵ-robust detection). Let d(·, ·) be an arbi-
trary metric. Let g be a defense that achieves robust risk Rϵ/2
adv (f) = β. Then, we can construct
an explicit (but inefﬁcient) defense f that achieves risk R(f) ≤β and robust risk with detection
Rϵ
adv-det(f) ≤β."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.3556701030927835,The defense f is constructed as follows on input x:
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.36082474226804123,• Run the classiﬁer y ←g(x).
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.36597938144329895,"• Find a perturbed input x′ withing distance ϵ/2 of x that is classiﬁed differently, i.e.,
d(x, x′) ≤ϵ/2 and g(x′) ̸= y. If such an input x′ exists, reject the input and output
⊥. Else, output the class y."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.3711340206185567,We provide the proof of Theorem 5 in Appendix A.
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.37628865979381443,"A main distinction between Theorem 4 and Theorem 5 is that the construction in Theorem 4 pre-
serves clean accuracy, but the construction in Theorem 5 does not. That is, the constructed robust
detector in Theorem 5 has clean accuracy that is equal to the robust classiﬁer’s robust accuracy."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.38144329896907214,"The construction in Theorem 5 can be efﬁciently (but approximately) instantiated by a certiﬁably
robust classiﬁer (Wong & Kolter, 2018; Raghunathan et al., 2018). These defenses can certify that
a classiﬁer’s output is constant for all points within some distance ϵ of the input. For an adversarial"
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.3865979381443299,Under review as a conference paper at ICLR 2022
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.3917525773195876,"example ˆx for g, the certiﬁcation always fails and thus the constructed detector f will reject ˆx. If g
is robust and the certiﬁcation succeeds, the detector f copies the output of g. However, a certiﬁed
defense may fail to certify a robust input (a false negative), and thus the detector f may reject more
inputs than with the “optimal” construction in Theorem 5. This reduction from a certiﬁed classiﬁer
to a detector is implicit in (Wong & Kolter, 2018, Section 3.1)."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.39690721649484534,"3
WHAT ARE DETECTION DEFENSES CLAIMING?"
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.4020618556701031,"We now survey 14 detection defenses, and consider the robust classiﬁcation performance that these
defenses implicitly claim (via Theorem 4). As we will see, in 12/14 cases, the defenses’ detection
results imply an inefﬁcient classiﬁer with far better robust accuracy than the state-of-the-art."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.4072164948453608,"Before presenting our experimental setup and the explicit results from the reduction, we ﬁrst discuss
how we believe these results should be interpreted."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.41237113402061853,"Interpreting our reduction.
Suppose that some detector defense claims a robust accuracy that
implies—via our reduction—an inefﬁcient classiﬁer with much higher robustness that the state-of-
the-art (e.g., the defense A described in the introduction of this paper)."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.4175257731958763,"A ﬁrst possible interpretation of our reduction is that this robust detector implies the existence of a
robust classiﬁer. This interpretation is rather weak however, since it is typically presumed that robust
classiﬁcation is possible, and that human perception is one concrete example of a robust classiﬁer.
The mere existence of a robust classiﬁer is thus typically already assumed to be true."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.422680412371134,"Our reduction yields a stronger result. It provides an explicit construction of an (inefﬁcient) robust
classiﬁer from a robust detector. The question then is whether we should expect the construction of
inefﬁcient robust classiﬁers to be easier than the construction of efﬁcient ones. That is, do we expect
that we can leverage computational inefﬁciency to build more robust classiﬁers that the current
state-of-the-art?"
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.42783505154639173,"We do not know of a positive answer to this question, and there is evidence to suggest that the answer
may be negative.1 For example, the work of Schmidt et al. (2018) proves that for a synthetic classiﬁ-
cation task between Gaussian distributions, building more robust classiﬁers requires additional data
regardless of the amount of computation power. Their results are corroborated by current state-of-
the-art robust classiﬁers based on adversarial training (Madry et al., 2018), which do not appear to
be limited by computational constraints. On CIFAR-10 for example, adversarial training achieves
100% robust training accuracy (Schmidt et al., 2018). Thus, it is unclear how computational inefﬁ-
ciency could be leveraged to build more robust classiﬁers using existing techniques."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.4329896907216495,"Candidate approaches could be to train much larger models (e.g., with an exponential number of
parameters), or to perform an exhaustive architecture search to ﬁnd more robust models. Yet, note
that the robust classiﬁer constructed in our reduction only uses its unbounded computational power
at inference time. That is, the classiﬁer that is built in Theorem 4 uses a trained detector model as a
subroutine (which is presumed to be efﬁcient), and then solves a non-convex optimization problem
at inference time. The classiﬁer built in our reduction is thus presumably weaker than a robust
classiﬁer that can be trained with unbounded computational power."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.4381443298969072,"To summarize, when a detector defense claims a certain robust accuracy, this implies the existence of
a concretely instantiatable robust classiﬁer with an inefﬁcient inference procedure. If this inefﬁcient
classiﬁer is much more robust than the current state-of-the-art, this does not necessarily mean that
the defense’s claims is wrong. But given how challenging robust classiﬁcation is proving to be,
we have reason to be skeptical of such a major breakthrough (even for inefﬁcient classiﬁers). To
compound this, many proposed detection defenses are quite simple, and reject adversarial inputs
based on some standard statistical test over a neural network’s features. It would thus be particularly
surprising if such simple techniques could yield robust classiﬁers, given that “simple” approaches to
adversarial robustness (denoising, compression, randomness, etc.) are ineffective (He et al., 2017)."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.44329896907216493,"1Some works have shown that for certain “unnatural distributions”, computational inefﬁciency is necessary
to build robust classiﬁers (Garg et al., 2020; Bubeck et al., 2018). Yet, since we presume that the human
perceptual system is robust to small perturbations on natural data (e.g., such as CIFAR-10), there must exist
some efﬁcient natural process to achieve robustness on such data."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.4484536082474227,Under review as a conference paper at ICLR 2022
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.4536082474226804,"As a result, it is not too surprising that a number of the detector defenses that we survey have already
been broken by stronger attacks (Carlini & Wagner, 2017; Tram`er et al., 2020). Our reduction would
have already suggested that such a break was likely to happen."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.4587628865979381,"Experimental setup.
We choose 14 detector defenses from the literature (see Table 1). Our se-
lection of these defenses was partially motivated by a pragmatic consideration on the easiness of
translating the defenses’ claims into a bound on the robust risk with detection Rϵ
adv-det. Indeed, some
defenses simply report a single AUC score for the detector’s performance, from which we cannot
derive a useful bound on the robust risk. We thus focus on defenses that either directly report a
robust error akin to Deﬁnition 3, or that provide concrete pairs of false-positive and false-negative
rates (e.g., a full ROC curve). In the latter case, we compute a “best-effort” bound on the robust risk
with detection2 as:
Rϵ
adv-det(f) ≤FPR + FNR + R(f) ,
(4)"
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.4639175257731959,"where FPR and FNR are the detector’s false-positive and false-negative rates for a ﬁxed detection
threshold, and R(f) is the defense’s standard risk (i.e., the test error on natural examples)."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.4690721649484536,"The above union bound in Equation (4) is quite pessimistic, as we may over-count examples that lead
to multiple sources of errors (e.g., a natural input that is misclassiﬁed and erroneously detected). The
true robustness claim made by these detector defenses might thus be stronger than what we obtain
from our bound. We encourage future detection papers to report their adversarial risk with detection,
Rϵ
adv-det, to facilitate direct comparisons with robust classiﬁers using our reduction."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.4742268041237113,"The 14 detector defenses use three datasets: MNIST, CIFAR-10 and ImageNet, and consider ad-
versarial examples under the ℓ∞or ℓ2 norms. Given a claim of robust detection at distance ϵ, we
contrast it to a state-of-the-art robust classiﬁcation result for distance ϵ/2:"
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.4793814432989691,"• On MNIST with ℓ∞attacks, we use the adversarially-trained TRADES classiﬁer (Zhang
et al., 2019) and measure robust error with the Square attack (Andriushchenko et al., 2020)."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.4845360824742268,"• On MNIST with ℓ2 attacks, we use the adversarially-trained classiﬁer from Tram`er &
Boneh (2019) and measure robust error with PGD (Madry et al., 2018)."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.4896907216494845,"• On CIFAR-10, for both ℓ∞and ℓ2 attacks we use the adversarially-trained classiﬁer of Re-
bufﬁet al. (2021) (trained without external data), and attack it using the APGD-CE attack
from AutoAttack (Croce & Hein, 2020)."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.4948453608247423,"• For ImageNet, for both ℓ∞and ℓ2 attacks we use adversarially-trained classiﬁers and PGD
attacks from Engstrom et al. (2019)."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.5,"We also consider two certiﬁed defenses for ℓ∞attacks on CIFAR-10: the robust classiﬁer of Zhang
et al. (2020a), and a recent certiﬁed detector of Sheikholeslami et al. (2021)."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.5051546391752577,"Results.
As we can see from Table 1, most defenses claim a detection performance that implies a
far greater robust accuracy than our current best robust classiﬁers. To illustrate with a concrete exam-
ple, the CIFAR-10 detector of Miller et al. (2019) claims to achieve robust accuracy with detection
of 75% for ℓ2 attacks with ϵ = 2.9. Using Theorem 4, this implies an inefﬁcient classiﬁer with
robust accuracy of 75% for ℓ2 attacks with ϵ = 2.9/2 = 1.45. Yet, the current state-of-the-art robust
accuracy for such a perturbation budget is only 30% (Rebufﬁet al., 2021). If this detector defense’s
robustness claim were correct, it would imply a remarkable breakthrough in robust classiﬁcation."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.5103092783505154,"Why do many of these defenses claim robust accuracies that appear “too good to be true”? A pri-
mary reason is that the vast majority of the above detector defenses do not consider evaluations
against adaptive attacks (Carlini et al., 2019; Athalye et al., 2018; Tram`er et al., 2020). That is,
these defenses show that they can detect some ﬁxed attacks, and thereafter conclude that the detec-
tor is robust against all attacks. As in the case of robust classiﬁers, such an evaluation is clearly
insufﬁcient! Some defenses do evaluate against adaptive adversaries, but fail to build a sufﬁciently
strong attack to reliably approximate the worst-case robust risk. Because of the lack of a strong
comparative baseline, it is not always immediately clear that these results are overly strong."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.5154639175257731,"2Many detector defenses report performance against a set of ﬁxed (non-adaptive) attacks. We interpret these
results as being an approximation of the worst-case risk."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.520618556701031,Under review as a conference paper at ICLR 2022
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.5257731958762887,"Table 1: For each detector defense, we compute a (best-effort) bound on the claimed robust risk
with detection Rϵ
adv-det using Equation (4), and report the complement (the robust accuracy with
detection), 1 −Rϵ
adv-det. For each detector’s robustness claim (at distance ϵ), we report the state-
of-the-art robust classiﬁcation accuracy for attacks at distance ϵ/2, denoted 1 −Rϵ/2
adv . Detection
defense claims that imply a higher robust classiﬁcation accuracy than the current state-of-the-art are
highlighted in red."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.5309278350515464,"Dataset
Defense
Norm
ϵ
1 −Rϵ
adv-det
1 −Rϵ/2
adv"
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.5360824742268041,"MNIST
Grosse et al. (2017)
ℓ∞
0.5
≥98%
94%"
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.5412371134020618,"Ma et al. (2018)
ℓ2
4.2
≥99%
72%
Raghuram et al. (2021)
ℓ2
8.9
≥74%
0%"
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.5463917525773195,CIFAR-10
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.5515463917525774,"Yin et al. (2020)
ℓ2
1.7
≥90%
66%
Feinman et al. (2017)
ℓ2
2.7
≥43%
36%
Miller et al. (2019)
ℓ2
2.9
≥75%
30%
Raghuram et al. (2021)
ℓ2
4.0
≥56%
10%"
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.5567010309278351,"Ma & Liu (2019)
ℓ∞
4/255
≥96%
85%
Roth et al. (2019)
ℓ∞
8/255
≥66%
79%
Lee et al. (2018)
ℓ∞
20/255
≥81%
59%
Li et al. (2019)
ℓ∞
26/255
≥80%
44%"
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.5618556701030928,ImageNet
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.5670103092783505,"Xu et al. (2018)
ℓ2
1.0
≥67%
54%"
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.5721649484536082,"Ma & Liu (2019)
ℓ∞
2/255
≥68%
55%
Jha et al. (2019)
ℓ∞
2/255
≥30%
55%
Hendrycks & Gimpel (2017)
ℓ∞
10/255
≥76%
30%
Yu et al. (2019)
ℓ∞
26/255
≥7%
5%"
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.5773195876288659,"For example, the recent work of Raghuram et al. (2021, ICML Long Talk) builds a detector on
MNIST with a FNR of ≤5% at a FPR of ≤20%, for adaptive ℓ2 attacks bounded by ϵ = 8.9.
Yet, this perturbation bound is much larger than the average distance between an MNIST image
and the nearest image from a different class! Thus, an attack within this perturbation bound can
trivially reduce the detector’s accuracy to chance. On CIFAR-10, the same detector achieves 95%
clean accuracy, and a FNR of ≤19% at a FPR of ≤20% for adaptive ℓ2 attacks bounded by ϵ = 4.
Using Equation (4), this yields a bound on the robust accuracy with detection of 1 −Rϵ
adv-det(f) ≥
1 −(5% + 19% + 20%) = 56%. In contrast, the best robust classiﬁer we are aware of for ℓ2
attacks bounded by ϵ = 2 achieves robust accuracy of only 10% (Rebufﬁet al., 2021). In summary,
the adaptive attack considered in this detector defense’s evaluation is highly unlikely to be good
approximation of a worst-case attack, and this defense can likely be broken by stronger attacks."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.5824742268041238,"Certiﬁably robust detection and classiﬁcation.
In Table 2, we look at the robust accuracy with
detection, and standard robust accuracy achieved by certiﬁed defenses (for which the claimed ro-
bustness numbers are necessarily mathematically correct)."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.5876288659793815,"We note that our reduction is not as meaningful in the case of certiﬁed defenses, since it is highly
plausible that computational inefﬁciency can be leveraged to build better certiﬁed classiﬁers. In-
deed, given any robust classiﬁer (e.g., an adversarially trained model), the classiﬁer’s robustness can
always be certiﬁed inefﬁciently (by enumerating over all points within an ϵ-ball). Thus, the existence
of an inefﬁcient classiﬁer with higher certiﬁed robustness than the state-of-the-art is to be expected."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.5927835051546392,"Nevertheless, we ﬁnd that existing results for certiﬁed classiﬁers and detectors perfectly match what
is implied by our reduction (up to ±2% error). For example, Zhang et al. (2020a) follow a long line
of results on robust classiﬁers and achieve 39% robust accuracy on CIFAR-10 for perturbations of
ℓ∞-norm below 4/255. Together with Theorem 5, this implies an inefﬁcient detector with 39% robust"
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.5979381443298969,Under review as a conference paper at ICLR 2022
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.6030927835051546,"Table 2: Certiﬁed robust accuracy 1 −Rϵ/2
adv for the defense of Zhang et al. (2020a), and certiﬁed
robust accuracy with detection 1 −Rϵ
adv-det for the defense of Sheikholeslami et al. (2021)."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.6082474226804123,"ϵ
1 −Rϵ
adv-det
1 −Rϵ/2
adv"
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.6134020618556701,"8/255
37%
39%"
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.6185567010309279,"16/255
32%
33%"
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.6237113402061856,"detection accuracy for perturbations of ℓ∞-norm below 8/255. The recent work of Sheikholeslami
et al. (2021) nearly matches that bound (37% robust accuracy with detection), with a defense that
has the advantage of being concretely efﬁcient."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.6288659793814433,"These results give additional credence to our thesis: with current techniques, robust classiﬁcation
is indeed approximately twice as hard (in terms of the perturbation bounds covered) than robust
detection."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.634020618556701,"Extensions and open problems.
The main open problem raised by our work is of course whether
it could be possible to show an efﬁcient reduction between classiﬁcation and detection of adversarial
examples, but this seems implausible (at least with our minimum distance decoding approach)."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.6391752577319587,"Another interesting question is whether a similar reduction can be shown for robustness to less
“structured” perturbations than ℓp balls and other metric spaces. For example, there has been a line
of research on defending against adversarial patches (Brown et al., 2017), using empirical (Hayes,
2018; Naseer et al., 2019; Chou et al., 2020) and certiﬁable techniques (Chiang et al., 2020; Zhang
et al., 2020b; Xiang et al., 2021). To use our result, we would have to deﬁne some metric to measure
the size of an adversarial patch’s perturbation. Yet, the size of a patch is typically deﬁned by the
number of contiguously perturbed pixels, which does not deﬁne a metric (in particular, it does not
satisfy the triangular inequality which our reduction relies on)."
ROBUST CLASSIFICATION IMPLIES INEFFICIENT ROBUST DETECTION,0.6443298969072165,"Finally, similar hardness reductions might exist between other candidate approaches for building
robust classiﬁers. For example, the question of whether (test-time) randomness can be leveraged to
build more robust models is also intriguing .Empirical defenses that use randomness can be noto-
riously hard to evaluate (Athalye et al., 2018; Tram`er et al., 2020), so a reduction similar to ours
might be useful in showing that we should not expect such approaches to bare fruit."
CONCLUSION,0.6494845360824743,"4
CONCLUSION"
CONCLUSION,0.654639175257732,"We have shown formal reductions between robust classiﬁcation with, and without, a detection op-
tion. Our results show that signiﬁcant progress on one of these two tasks implies similar progress
on the other—unless computational inefﬁciency can somehow be leveraged to build more robust
models. This raises the question on whether we should spend our efforts on studying both of these
tasks, or focus our efforts on a single one."
CONCLUSION,0.6597938144329897,"On one hand, the two tasks represent different ways of tackling a common goal, and working on
either task might result in new techniques or ideas that apply to the other task as well. On the
other hand, our reductions show that unless we make progress on both tasks, work on one of the
tasks can merely aim to match the robustness of our inefﬁcient constructions, whilst improving their
computational complexity."
CONCLUSION,0.6649484536082474,"We believe our reduction will serve as a useful sanity-check when assessing the claims of future de-
tector defenses. Detector defenses’ robustness evaluations have received less stringent scrutiny than
robust classiﬁers over the past years, perhaps in part due to a lack of strong comparative baselines.
Instead of having to wait until some detector defense’s claims pass the test-of-time, we show that
detection results can be directly contrasted against long-standing results for robust classiﬁcation."
CONCLUSION,0.6701030927835051,"When applying this approach to past detector defenses, we ﬁnd that many make robustness claims
that imply signiﬁcant breakthroughs in robust classiﬁcation. We believe our reduction could have
been useful in highlighting the suspiciously strong claims made by many of these defenses—before
they were explicitly broken by stronger attacks."
CONCLUSION,0.6752577319587629,Under review as a conference paper at ICLR 2022
ETHICS STATEMENT,0.6804123711340206,"Ethics statement.
Our paper demonstrates a fundamental barrier towards detecting against adver-
sarial examples, under the assumption that our current techniques are insufﬁcient to achieve strong
(inefﬁcient) robust classiﬁcation. We do not however explicitly break any existing defenses (our
results merely strongly suggest that many existing detector defenses’ claims are suspiciously high).
Our paper therefore cannot lead to any explicit harms, but aims to further our understanding of the
hardness of robust classiﬁcation and detection."
REPRODUCIBILITY STATEMENT,0.6855670103092784,"Reproducibility statement.
Our paper’s contribution is mainly of theoretical nature. Section 2
is self-contained and clearly states our assumptions, results and proofs (except for the proof of
Theorem 5 in Appendix A). The experiments in Section 3 use only public datasets and pre-trained
models, with clearly indicates hyper-parameters for all attacks that we evaluate."
REFERENCES,0.6907216494845361,REFERENCES
REFERENCES,0.6958762886597938,"Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein. Square at-
tack: a query-efﬁcient black-box adversarial attack via random search. In European Conference
on Computer Vision, 2020."
REFERENCES,0.7010309278350515,"Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of se-
curity: Circumventing defenses to adversarial examples. In International Conference on Machine
Learning, 2018."
REFERENCES,0.7061855670103093,"Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim ˇSrndi´c, Pavel Laskov, Gior-
gio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In Euro-
pean Conference on Machine Learning and Knowledge Discovery in Databases, pp. 387–402.
Springer, 2013."
REFERENCES,0.711340206185567,"Tom B Brown, Dandelion Man´e, Aurko Roy, Mart´ın Abadi, and Justin Gilmer. Adversarial patch.
arXiv preprint arXiv:1712.09665, 2017."
REFERENCES,0.7164948453608248,"S´ebastien Bubeck, Yin Tat Lee, Eric Price, and Ilya Razenshteyn. Adversarial examples from cryp-
tographic pseudo-random generators. arXiv preprint arXiv:1811.06418, 2018."
REFERENCES,0.7216494845360825,"Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten
detection methods. In AISec, pp. 3–14. ACM, 2017."
REFERENCES,0.7268041237113402,"Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris
Tsipras, Ian Goodfellow, and Aleksander Madry. On evaluating adversarial robustness. arXiv
preprint arXiv:1902.06705, 2019."
REFERENCES,0.7319587628865979,"Ping-yeh Chiang, Renkun Ni, Ahmed Abdelkader, Chen Zhu, Christoph Studer, and Tom Goldstein.
Certiﬁed defenses for adversarial patches. arXiv preprint arXiv:2003.06693, 2020."
REFERENCES,0.7371134020618557,"Edward Chou, Florian Tram`er, and Giancarlo Pellegrino.
Sentinet: Detecting physical attacks
against deep learning systems. In Workshop on Deep Learning Security, 2020."
REFERENCES,0.7422680412371134,"Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble
of diverse parameter-free attacks. In ICML, 2020."
REFERENCES,0.7474226804123711,"Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Nicolas Flammarion, Mung Chiang,
Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustness bench-
mark. arXiv preprint arXiv:2010.09670, 2020."
REFERENCES,0.7525773195876289,"Logan Engstrom, Andrew Ilyas, Hadi Salman, Shibani Santurkar, and Dimitris Tsipras. Robustness
(python library), 2019. URL https://github.com/MadryLab/robustness."
REFERENCES,0.7577319587628866,"Reuben Feinman, Ryan R Curtin, Saurabh Shintre, and Andrew B Gardner. Detecting adversarial
samples from artifacts. arXiv preprint arXiv:1703.00410, 2017."
REFERENCES,0.7628865979381443,"Sanjam Garg, Somesh Jha, Saeed Mahloujifar, and Mahmoody Mohammad. Adversarially robust
learning could leverage computational hardness. In Algorithmic Learning Theory, pp. 364–385.
PMLR, 2020."
REFERENCES,0.7680412371134021,Under review as a conference paper at ICLR 2022
REFERENCES,0.7731958762886598,"Kathrin Grosse, Praveen Manoharan, Nicolas Papernot, Michael Backes, and Patrick McDaniel. On
the (statistical) detection of adversarial examples. arXiv preprint arXiv:1702.06280, 2017."
REFERENCES,0.7783505154639175,"Jamie Hayes. On visible adversarial perturbations & digital watermarking. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 1597–1604, 2018."
REFERENCES,0.7835051546391752,"Warren He, James Wei, Xinyun Chen, Nicholas Carlini, and Dawn Song. Adversarial example
defenses: Ensembles of weak defenses are not strong. In USENIX Workshop on Offensive Tech-
nologies, 2017."
REFERENCES,0.788659793814433,"Dan Hendrycks and Kevin Gimpel. Early methods for detecting adversarial images. In International
Conference on Learning Representations, 2017."
REFERENCES,0.7938144329896907,"Susmit Jha, Sunny Raj, Steven Lawrence Fernandes, Sumit Kumar Jha, Somesh Jha, Gunjan Verma,
Brian Jalaian, and Ananthram Swami. Attribution-driven causal analysis for detection of adver-
sarial examples. arXiv preprint arXiv:1903.05821, 2019."
REFERENCES,0.7989690721649485,"Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple uniﬁed framework for detecting
out-of-distribution samples and adversarial attacks. In Advances in Neural Information Process-
ing Systems, 2018."
REFERENCES,0.8041237113402062,"Yingzhen Li, John Bradshaw, and Yash Sharma. Are generative classiﬁers more robust to adversarial
attacks? In International Conference on Machine Learning, pp. 3804–3814. PMLR, 2019."
REFERENCES,0.8092783505154639,"Shiqing Ma and Yingqi Liu. Nic: Detecting adversarial samples with neural network invariant
checking. In Proceedings of the 26th Network and Distributed System Security Symposium (NDSS
2019), 2019."
REFERENCES,0.8144329896907216,"Xingjun Ma, Bo Li, Yisen Wang, Sarah M Erfani, Sudanthi Wijewickrema, Grant Schoenebeck,
Dawn Song, Michael E Houle, and James Bailey. Characterizing adversarial subspaces using
local intrinsic dimensionality. In International Conference on Learning Representations, 2018."
REFERENCES,0.8195876288659794,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations, 2018."
REFERENCES,0.8247422680412371,"David Miller, Yujia Wang, and George Kesidis. When not to classify: Anomaly detection of attacks
(ada) on dnn classiﬁers at test time. Neural computation, 31(8):1624–1670, 2019."
REFERENCES,0.8298969072164949,"Muzammal Naseer, Salman Khan, and Fatih Porikli. Local gradients smoothing: Defense against
localized adversarial attacks. In 2019 IEEE Winter Conference on Applications of Computer
Vision (WACV), pp. 1300–1307. IEEE, 2019."
REFERENCES,0.8350515463917526,"Tianyu Pang, Huishuai Zhang, Di He, Yinpeng Dong, Hang Su, Wei Chen, Jun Zhu, and Tie-Yan
Liu. Adversarial training with rectiﬁed rejection. arXiv preprint arXiv:2105.14785, 2021."
REFERENCES,0.8402061855670103,"Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certiﬁed defenses against adversarial exam-
ples. In International Conference on Learning Representations, 2018."
REFERENCES,0.845360824742268,"Jayaram Raghuram, Varun Chandrasekaran, Somesh Jha, and Suman Banerjee. A general frame-
work for detecting anomalous inputs to dnn classiﬁers. In International Conference on Machine
Learning, 2021."
REFERENCES,0.8505154639175257,"Sylvestre-Alvise Rebufﬁ, Sven Gowal, Dan A Calian, Florian Stimberg, Olivia Wiles, and Tim-
othy Mann.
Fixing data augmentation to improve adversarial robustness.
arXiv preprint
arXiv:2103.01946, 2021."
REFERENCES,0.8556701030927835,"Kevin Roth, Yannic Kilcher, and Thomas Hofmann. The odds are odd: A statistical test for detecting
adversarial examples. In International Conference on Machine Learning, 2019."
REFERENCES,0.8608247422680413,"Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Adver-
sarially robust generalization requires more data. In Advances In Neural Information Processing
Systems, pp. 5019–5031, 2018."
REFERENCES,0.865979381443299,Under review as a conference paper at ICLR 2022
REFERENCES,0.8711340206185567,"Fatemeh Sheikholeslami, Ali Lotﬁ, and J Zico Kolter. Provably robust classiﬁcation of adversarial
examples with detection. In International Conference on Learning Representations, 2021."
REFERENCES,0.8762886597938144,"Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. In International Conference on
Learning Representations, 2014."
REFERENCES,0.8814432989690721,"Florian Tram`er and Dan Boneh. Adversarial training and robustness for multiple perturbations. In
Advances In Neural Information Processing Systems, 2019."
REFERENCES,0.8865979381443299,"Florian Tram`er, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to
adversarial example defenses. In Conference on Neural Information Processing Systems, 2020."
REFERENCES,0.8917525773195877,"Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry.
Robustness may be at odds with accuracy. In International Conference on Learning Representa-
tions, 2019."
REFERENCES,0.8969072164948454,"Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In International Conference on Machine Learning, pp. 5283–5292, 2018."
REFERENCES,0.9020618556701031,"Chong Xiang, Arjun Nitin Bhagoji, Vikash Sehwag, and Prateek Mittal. Patchguard: A provably ro-
bust defense against adversarial patches via small receptive ﬁelds and masking. In 30th {USENIX}
Security Symposium ({USENIX} Security 21), 2021."
REFERENCES,0.9072164948453608,"Weilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial examples in deep
neural networks. In Network and Distributed System Security Symposium, 2018."
REFERENCES,0.9123711340206185,"Xuwang Yin, Soheil Kolouri, and Gustavo K Rohde. Gat: Generative adversarial training for ad-
versarial example detection and robust classiﬁcation. In International Conference on Learning
Representations, 2020."
REFERENCES,0.9175257731958762,"Tao Yu, Shengyuan Hu, Chuan Guo, Wei-Lun Chao, and Kilian Q Weinberger. A new defense
against adversarial images: Turning a weakness into a strength. In Advances in Neural Informa-
tion Processing Systems, 2019."
REFERENCES,0.9226804123711341,"Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and Michael I. Jordan.
Theoretically principled trade-off between robustness and accuracy. In International Conference
on Machine Learning, 2019."
REFERENCES,0.9278350515463918,"Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Duane Boning, and Cho-Jui Hsieh. Towards
stable and efﬁcient training of veriﬁably robust neural networks. In International Conference on
Learning Representations, 2020a."
REFERENCES,0.9329896907216495,"Zhanyuan Zhang, Benson Yuan, Michael McCoyd, and David Wagner. Clipped bagnet: Defending
against sticker attacks with clipped bag-of-features. In 2020 IEEE Security and Privacy Work-
shops (SPW), pp. 55–61. IEEE, 2020b."
REFERENCES,0.9381443298969072,Under review as a conference paper at ICLR 2022
REFERENCES,0.9432989690721649,"A
PROOF OF THEOREM 5."
REFERENCES,0.9484536082474226,We recall Theorem 5:
REFERENCES,0.9536082474226805,"Theorem 5 (ϵ/2 robust-classiﬁcation implies inefﬁcient ϵ-robust detection). Let d(·, ·) be an arbi-
trary metric. Let g be a defense that achieves robust risk Rϵ/2
adv (f) = β. Then, we can construct
an explicit (but inefﬁcient) defense f that achieves risk R(f) ≤β and robust risk with detection
Rϵ
adv-det(f) ≤β."
REFERENCES,0.9587628865979382,The defense f is constructed as follows on input x:
REFERENCES,0.9639175257731959,• Run the classiﬁer y ←g(x).
REFERENCES,0.9690721649484536,"• Find a perturbed input x′ withing distance ϵ/2 of x that is classiﬁed differently, i.e.,
d(x, x′) ≤ϵ/2 and g(x′) ̸= y. If such an input x′ exists, reject the input and output
⊥. Else, output the class y."
REFERENCES,0.9742268041237113,"Proof of Theorem 5. Note that for any input (x, y) for which the classiﬁer g is robust at distance
ϵ/2, no input x′ above exists and so f(x) = y. Thus, the risk of f is at most the robust risk of g, so
R(f) ≤β."
REFERENCES,0.979381443298969,"Now, consider an input (x, y) ∼D for which f is not robust with detection at distance ϵ. That is,
either f(x) ̸= y, or there exists an input ˆx at distance d(x, ˆx) ≤ϵ such that f(ˆx) = ˆy /∈{y, ⊥}. We
will show that the defense g is not robust for x either (for attacks at distance up to ϵ/2.)"
REFERENCES,0.9845360824742269,"If f(x) ̸= y, then by the same argument as above it cannot be the case that g is robust at distance
ϵ/2 for x."
REFERENCES,0.9896907216494846,"So let us consider the case where f(ˆx) = ˆy /∈{y, ⊥}. By the deﬁnition of f, this means that for all
x′ at distance at most ϵ/2 from ˆx, we have g(x′) = ˆy. But, note that there exists a point x∗that is at
distance at most ϵ/2 from both ˆx and x. Since we must have g(x∗) = ˆy, we conclude that g is not
robust at distance ϵ/2 for x."
REFERENCES,0.9948453608247423,Taking expectations over the distribution D concludes the proof.
