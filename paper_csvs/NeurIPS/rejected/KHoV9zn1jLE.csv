Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001445086705202312,"We study unstable dynamics of stochastic gradient descent (SGD) and its impact
1"
ABSTRACT,0.002890173410404624,"on generalization in neural networks. We find that SGD induces an implicit
2"
ABSTRACT,0.004335260115606936,"regularization on the interaction between the gradient distribution and the loss
3"
ABSTRACT,0.005780346820809248,"landscape geometry. Moreover, based on the analysis of a concentration measure of
4"
ABSTRACT,0.0072254335260115606,"the batch gradient, we propose a more accurate scaling rule, Linear and Saturation
5"
ABSTRACT,0.008670520231213872,"Scaling Rule (LSSR), between batch size and learning rate.
6"
INTRODUCTION,0.010115606936416185,"1
Introduction
7"
INTRODUCTION,0.011560693641618497,"SGD plays an important role in the success of deep learning. However, we still do not fully understand
8"
INTRODUCTION,0.01300578034682081,"how SGD works from the perspectives of both optimization behavior and generalization performance.
9"
INTRODUCTION,0.014450867052023121,"To be specific, SGD is a stochastic approximation of full-batch gradient descent (GD), but SGD
10"
INTRODUCTION,0.015895953757225433,"generally yields better generalization with a small batch size [27, 23]. Moreover, GD is a discretization
11"
INTRODUCTION,0.017341040462427744,"of gradient flow (GF) with a finite learning rate, i.e., GF is a GD in the limit of vanishing learning
12"
INTRODUCTION,0.01878612716763006,"rate, but GD generally performs better with a large learning rate [2, 32, 28, 43]. There are some
13"
INTRODUCTION,0.02023121387283237,"scaling rules [25, 10, 15, 45, 54] on how to tune the learning rate for varying batch sizes, but they
14"
INTRODUCTION,0.02167630057803468,"fail when the batch size gets large [42, 38, 57, 43, 33]. Especially for a greater data-parallelism to
15"
INTRODUCTION,0.023121387283236993,"accelerate the training process, we require a more accurate scaling rule for the large-batch regime.
16"
INTRODUCTION,0.024566473988439308,"There has been many studies to understand the SGD dynamics and its impacts on generalization in
17"
INTRODUCTION,0.02601156069364162,"deep neural networks. While they provide some useful and intuitive explanations to help us understand
18"
INTRODUCTION,0.02745664739884393,"these properties of SGD, unfortunately, some results often rely on impractical assumptions or only
19"
INTRODUCTION,0.028901734104046242,"apply to a certain range of learning rates and batch sizes. For example, some approximate SGD as a
20"
INTRODUCTION,0.030346820809248554,"stochastic differential equation (SDE) in the limit of vanishing learning rate [34, 35, 29, 16, 30, 31,
21"
INTRODUCTION,0.031791907514450865,"18, 44, 4]. Therefore, in a practical finite learning rate regime, this may not properly describe the
22"
INTRODUCTION,0.033236994219653176,"SGD dynamics. Moreover, Yaida [52] raises some theoretical issues about the SDE approximation
23"
INTRODUCTION,0.03468208092485549,"and Li et al. [33] theoretically analyze a sufficient condition for the SDE approximation to fail.
24"
INTRODUCTION,0.036127167630057806,"In this paper, we aim to understand the dynamics and the implicit bias of SGD through the analysis of
25"
INTRODUCTION,0.03757225433526012,"the interaction between SGD and the loss landscape of a neural network with minimal assumptions.
26"
INTRODUCTION,0.03901734104046243,"To be specific, we investigate the unstable dynamics of SGD “at the edge of stability” [6] (Section
27"
INTRODUCTION,0.04046242774566474,"4.1-4.2). This investigation leads to a more refined characterization of the edge of stability by the
28"
INTRODUCTION,0.04190751445086705,"interaction-aware sharpness which extends the previous findings for full-batch GD to a general SGD.
29"
INTRODUCTION,0.04335260115606936,"Then, we introduce a concentration measure of the the batch gradient distribution of SGD. By doing
30"
INTRODUCTION,0.044797687861271675,"so, we find that SGD implicitly regularizes the interaction-aware sharpness and its regularization
31"
INTRODUCTION,0.046242774566473986,"effect is controlled by the ratio of the concentration measure to learning rate (Section 5.1). Finally,
32"
INTRODUCTION,0.0476878612716763,"we propose a more accurate scaling rule between batch size and learning rate, based on a novel
33"
INTRODUCTION,0.049132947976878616,"analysis of the implicit regularization and the concentration measure (Section 5.2). This can be
34"
INTRODUCTION,0.05057803468208093,"applied to any batch size including the large-batch regime where the previous scaling rules fail
35"
INTRODUCTION,0.05202312138728324,"[18, 38, 57, 42, 43, 46]. We name it Linear and Saturation Scaling Rule (LSSR).
36"
STOCHASTIC GRADIENT AND LOSS LANDSCAPE,0.05346820809248555,"2
Stochastic Gradient and Loss Landscape
37"
STOCHASTIC GRADIENT AND LOSS LANDSCAPE,0.05491329479768786,"In this section, we review some concepts required for further discussion. We also summarize the
38"
STOCHASTIC GRADIENT AND LOSS LANDSCAPE,0.05635838150289017,"notations in Appendix A for a quick reference. We often omit the dependence on some variables and
39"
STOCHASTIC GRADIENT AND LOSS LANDSCAPE,0.057803468208092484,"the subscript of the expectation operation when clear from the context.
40"
STOCHASTIC GRADIENT AND LOSS LANDSCAPE,0.059248554913294796,"For a learning task, we use a parameterized model (neural network) with model parameter θ ∈
41"
STOCHASTIC GRADIENT AND LOSS LANDSCAPE,0.06069364161849711,"Θ ⊂Rm. Then we train the model using training data D = {xi}n
i=1 and a loss function ℓ(x; θ).
42"
STOCHASTIC GRADIENT AND LOSS LANDSCAPE,0.06213872832369942,"We denote the (total) training loss by L(θ) ≡
1
n
Pn
i=1 ℓ(xi; θ) for training data D. At time step
43"
STOCHASTIC GRADIENT AND LOSS LANDSCAPE,0.06358381502890173,"t, we update the parameter θt using GD: θt+1 = θt −η∇θL(θt) with a learning rate η, or using
44"
STOCHASTIC GRADIENT AND LOSS LANDSCAPE,0.06502890173410404,"SGD: θt+1 = θt −ηgb(θt) with a mini-batch gradient gb(θt) ≡1 b
P"
STOCHASTIC GRADIENT AND LOSS LANDSCAPE,0.06647398843930635,"x∈Bt ∇θℓ(x; θt) ∈Rm for a
45"
STOCHASTIC GRADIENT AND LOSS LANDSCAPE,0.06791907514450866,"mini-batch Bt ⊂D of size b (1 ≤b ≤n).
46"
STOCHASTIC GRADIENT AND LOSS LANDSCAPE,0.06936416184971098,"Now, we are ready to introduce some important matrices, Cb, Sb, and H. First, we define the
47"
STOCHASTIC GRADIENT AND LOSS LANDSCAPE,0.0708092485549133,"covariance Cb(θ) ≡Var[gb(θ)] = E
h
(gb(θ) −E[gb(θ)]) (gb(θ) −E[gb(θ)])⊤i
∈Rm×m and the
48"
STOCHASTIC GRADIENT AND LOSS LANDSCAPE,0.07225433526011561,"second moment Sb(θ) ≡E[gb(θ)gb(θ)⊤] ∈Rm×m of the mini-batch gradient gb(θ) over batch
49"
STOCHASTIC GRADIENT AND LOSS LANDSCAPE,0.07369942196531792,"sampling for a batch size 1 ≤b ≤n.1 The covariance Cb and the second moment Sb satisfy not only
50"
STOCHASTIC GRADIENT AND LOSS LANDSCAPE,0.07514450867052024,"Cb = Sb −Sn but also the following equation [15, 29, 49]:
51"
STOCHASTIC GRADIENT AND LOSS LANDSCAPE,0.07658959537572255,"Cb = γn,b"
STOCHASTIC GRADIENT AND LOSS LANDSCAPE,0.07803468208092486,"b (S1 −Sn) = γn,b"
STOCHASTIC GRADIENT AND LOSS LANDSCAPE,0.07947976878612717,"b C1,
(1)"
STOCHASTIC GRADIENT AND LOSS LANDSCAPE,0.08092485549132948,"where γn,b = n−b"
STOCHASTIC GRADIENT AND LOSS LANDSCAPE,0.08236994219653179,"n−1 for sampling without replacement and γn,b = 1 for sampling with replacement.
52"
STOCHASTIC GRADIENT AND LOSS LANDSCAPE,0.0838150289017341,"We provide a self-contained proof of (1) in Appendix B.1. We note that, for sampling without
53"
STOCHASTIC GRADIENT AND LOSS LANDSCAPE,0.08526011560693642,"replacement, many previous works approximate γn,b ≈1 assuming b ≪n [18, 15, 46], but we
54"
STOCHASTIC GRADIENT AND LOSS LANDSCAPE,0.08670520231213873,"consider the whole range of 1 ≤b ≤n (0 ≤γn,b ≤1 with γn,1 = 1 and γn,n = 0). Second,
55"
STOCHASTIC GRADIENT AND LOSS LANDSCAPE,0.08815028901734104,"we define the Hessian H(θ) = ∇2
θL(θ) = Ex∼D[∇2
θℓ(x; θ)] ∈Rm×m and the operator norm (the
56"
STOCHASTIC GRADIENT AND LOSS LANDSCAPE,0.08959537572254335,"top eigenvalue) ∥H∥≡sup∥u∥=1 ∥Hu∥of H. We also denote the i-th largest eigenvalue and its
57"
STOCHASTIC GRADIENT AND LOSS LANDSCAPE,0.09104046242774566,"corresponding normalized eigenvector by λi ∈R and qi ∈Rm, respectively, for i = 1, · · · , m.
58"
STOCHASTIC GRADIENT AND LOSS LANDSCAPE,0.09248554913294797,"Therefore, with these matrices, we can write one of our goals as follows:
59"
STOCHASTIC GRADIENT AND LOSS LANDSCAPE,0.09393063583815028,"We aim to understand how the gradient distribution (Cb and Sb) and the loss landscape geometry
60"
STOCHASTIC GRADIENT AND LOSS LANDSCAPE,0.0953757225433526,"(H) interact with each other during SGD training.
61"
STOCHASTIC GRADIENT AND LOSS LANDSCAPE,0.0968208092485549,"We investigate this “interaction” in terms of matrix multiplication HSb. To be specific, we consider
62"
STOCHASTIC GRADIENT AND LOSS LANDSCAPE,0.09826589595375723,the trace tr(HSb) or its normalized one tr(HSb)
STOCHASTIC GRADIENT AND LOSS LANDSCAPE,0.09971098265895954,"tr(Sb) (will be denoted by ∥H∥Sb in Definition 2 later).
63"
RELATED WORK,0.10115606936416185,"3
Related Work
64"
RELATED WORK,0.10260115606936417,"Some studies investigate the interaction between the gradient distribution and the loss landscape
65"
RELATED WORK,0.10404624277456648,"geometry represented by tr(HSb) in the context of escaping efficiency [58, Section 3.1], stationarity
66"
RELATED WORK,0.10549132947976879,"[52, Section 2.2], and convergence [48, Section 3.1.1]. However, they require some additional
67"
RELATED WORK,0.1069364161849711,"assumptions like SDE approximation of SGD [58], the existence of a stationary-state distribution
68"
RELATED WORK,0.10838150289017341,"of the model parameter [52, Section 2.3.4], and strong convexity of the training loss function [48],
69"
RELATED WORK,0.10982658959537572,"respectively. In this paper, we provide a new insight into the interaction tr(HSb) without these
70"
RELATED WORK,0.11127167630057803,"assumptions.
71"
RELATED WORK,0.11271676300578035,"Convergence of full-batch GD (b = n) has been instead analyzed with an upper bound on the
72"
RELATED WORK,0.11416184971098266,"interaction tr(HSn) with further assumptions for the stable optimization, such as β-smoothness of
73"
RELATED WORK,0.11560693641618497,"1These two matrices Cb and Sb are also called the second central and non-central moments, respectively.
But to avoid confusion, we use the term “second moment” only for the non-central Sb."
RELATED WORK,0.11705202312138728,the objective and 0 < η < 2
RELATED WORK,0.11849710982658959,"β (e.g., η = 1"
RELATED WORK,0.1199421965317919,"β ) [39, 41, 37, 3]. 2 However, it may lose useful information
74"
RELATED WORK,0.12138728323699421,"of the interaction between H and Sn. Moreover, when we train a standard neural network with GD
75"
RELATED WORK,0.12283236994219653,"in practice, ∥H∥(≤β) increases in the early phase of training and the iterate enters the regime called
76"
RELATED WORK,0.12427745664739884,the edge of stability [6] where ∥H∥⪆2
RELATED WORK,0.12572254335260116,"η, i.e., η ⪆
2
∥H∥≥2"
RELATED WORK,0.12716763005780346,"β . This contradicts with the assumption
77"
RELATED WORK,0.12861271676300579,"for stable optimization and the iterate exhibits unstable behavior with a non-monotonically decreasing
78"
RELATED WORK,0.13005780346820808,"loss [51, 50, 6]. We further extend this discussion of unstable dynamics for GD to the case of SGD.
79"
RELATED WORK,0.1315028901734104,"From the generalization perspective, many studies focus on the implicit bias of SGD toward a better
80"
RELATED WORK,0.1329479768786127,"generalization [40, 56, 47, 20, 21, 1, 46]. There are mainly two factors known to correlate with the
81"
RELATED WORK,0.13439306358381503,"generalization performance: the batch gradient distribution during training [15, 18, 44, 58] and the
82"
RELATED WORK,0.13583815028901733,"sharpness of the loss landscape at the minimum [14, 23, 8, 22, 9, 26]. We provide a link between the
83"
RELATED WORK,0.13728323699421965,"batch gradient distribution and the sharpness that the model is implicitly regularized to have a low
84"
RELATED WORK,0.13872832369942195,"sharpness when the second moment of the batch gradient is large (see Section 5.1).
85"
OPTIMIZATION THROUGH LOSS LANDSCAPE,0.14017341040462428,"4
Optimization through Loss Landscape
86"
OPTIMIZATION THROUGH LOSS LANDSCAPE,0.1416184971098266,"We start by investigating the optimization behavior of SGD through the interaction between SGD and
87"
OPTIMIZATION THROUGH LOSS LANDSCAPE,0.1430635838150289,"the loss landscape without the stochastic differential equation (SDE) approximation.
88"
UNSTABLE OPTIMIZATION,0.14450867052023122,"4.1
Unstable Optimization
89"
UNSTABLE OPTIMIZATION,0.14595375722543352,"Using the second-order Taylor expansion, the change in total training loss Lt = L(θt) as the SGD
90"
UNSTABLE OPTIMIZATION,0.14739884393063585,"iterate moves from θt to θt+1 at time step t can be expressed as follows:
91"
UNSTABLE OPTIMIZATION,0.14884393063583815,Lt+1 −Lt = −η∇L⊤gb + η2
UNSTABLE OPTIMIZATION,0.15028901734104047,"2 g⊤
b Hgb + O(∥δt∥3),
(2)"
UNSTABLE OPTIMIZATION,0.15173410404624277,"where δt = θt+1 −θt = −ηgb. Thus, we obtain the expected loss difference as follows:
92"
UNSTABLE OPTIMIZATION,0.1531791907514451,E[Lt+1] −Lt = −η∇L⊤E[gb] + η2
UNSTABLE OPTIMIZATION,0.1546242774566474,"2 E[g⊤
b Hgb] + ϵ
(3)"
UNSTABLE OPTIMIZATION,0.15606936416184972,= −η∥∇L∥2 + η2
TR,0.157514450867052,"2 tr
 
E[Hgbg⊤
b ]

+ ϵ
(4)"
TR,0.15895953757225434,= −η tr(Sn) + η2
TR,0.16040462427745664,"2 tr(HSb) + ϵ
(5) = η2"
TR,0.16184971098265896,"2 tr(Sn)
tr(HSb)"
TR,0.16329479768786126,tr(Sn) −2 η
TR,0.16473988439306358,"
+ ϵ,
(6)"
TR,0.16618497109826588,"where ϵ = O(E[∥δt∥3]) and E[gb] = ∇L is used. For the moment, we make a minimal assumption
93"
TR,0.1676300578034682,"that the training loss is locally quadratic, i.e., ϵ = 0 near θt, but we will revisit this assumption later
94"
TR,0.16907514450867053,"(see Section 4.2). Then, the expected loss increases when the following instability condition is met:
95"
TR,0.17052023121387283,Definition 1 (Instability Condition).
TR,0.17196531791907516,tr(HSb)
TR,0.17341040462427745,tr(Sn) > 2
TR,0.17485549132947978,"η .
(7)"
TR,0.17630057803468208,We also define unstable regime U = {θ ∈Θ : tr(HSb)
TR,0.1777456647398844,"tr(Sn)
> 2"
TR,0.1791907514450867,"η} and stable regime S ≡Uc. For a
96"
TR,0.18063583815028902,"standard non-quadratic loss function, we will show in the following sections that the iterate tends not
97"
TR,0.18208092485549132,"to stay within the unstable regime U and operates near at the boundary ∂S of the stable regime S,
98"
TR,0.18352601156069365,"called the edge of stability [6]. Cohen et al. [6] mark the edge of stability with {θ ∈Θ : ∥H∥= 2 η}
99"
TR,0.18497109826589594,"for GD, but we mark with ∂S = {θ ∈Θ : tr(HSb)"
TR,0.18641618497109827,tr(Sn) = 2
TR,0.18786127167630057,"η} for both SGD and GD which provides a more
100"
TR,0.1893063583815029,"clear and generalized indication as shown in Figure 4 later. On the other hand, for a globally quadratic
101"
TR,0.1907514450867052,"loss, when the GD iterate satisfies the instability condition, it diverges within the unstable regime [6].
102"
TR,0.19219653179190752,"We emphasize that many studies on the convergence of GD usually consider the optimization within
103"
TR,0.1936416184971098,2L(θt+1)−L(θt) ≤∇L⊤(θt+1−θt)+ β
TR,0.19508670520231214,2 ∥θt+1−θt∥2 = −η∥∇L∥2+ βη2
TR,0.19653179190751446,2 ∥∇L∥2 = −η(1−βη
TR,0.19797687861271676,2 )∥∇L∥2
TR,0.1994219653179191,and thus the loss monotonically decreases when 0 < η < 2 β .
TR,0.20086705202312138,"0.00
0.75
1.50
2.25
3.00
tr(HSb)
tr(Sn) 1e2 2 0 2 4"
TR,0.2023121387283237,"(E[Lt + 1]
Lt)/tr(Sn)"
E,0.203757225433526,"1e
2"
E,0.20520231213872833,slope= 2 2
E,0.20664739884393063,2 = 100
E,0.20809248554913296,"CE, b = 128,
= 0.02 40 60 80 step"
E,0.20953757225433525,"0.00
0.75
1.50
2.25
3.00
tr(HSb)
tr(Sn) 1e2 2 0 2 4"
E,0.21098265895953758,"(E[Lt + 1]
Lt)/tr(Sn)"
E,0.21242774566473988,"1e
2"
E,0.2138728323699422,slope= 2 2
E,0.2153179190751445,2 = 100
E,0.21676300578034682,"CE, b = 256,
= 0.02 60 80 100 step"
E,0.21820809248554912,"0.00
0.75
1.50
2.25
3.00
||H||Sn
1e2 2 0 2 4"
E,0.21965317919075145,"(Lt + 1
Lt)/tr(Sn)"
E,0.22109826589595374,"1e
2"
E,0.22254335260115607,slope= 2 2
E,0.2239884393063584,2 = 100
E,0.2254335260115607,"CE, GD, 
= 0.02 110 115 120 step"
E,0.22687861271676302,"0.00
0.75
1.50
2.25
3.00
||H||Sn
1e2 2 0 2 4"
E,0.22832369942196531,"(Lt + 1
Lt)/tr(Sn)"
E,0.22976878612716764,"1e
2"
E,0.23121387283236994,slope= 2 2
E,0.23265895953757226,2 = 100
E,0.23410404624277456,"MSE, GD, 
= 0.02 250 260 270 step"
E,0.23554913294797689,"Figure 1: [An empirical validation of (6) for SGD (top) and (9) for GD (bottom)] In the early
phase, until the iterate enters the edge of stability, it validates (6) and (9) with the blue line with
the slope η2"
E,0.23699421965317918,2 and x-intercept 2
E,0.2384393063583815,"η. For GD (bottom), they are plotted after ∥H∥exceeds 2"
E,0.2398843930635838,"η after which
∥H∥Sn starts to increase from 0 to 2"
E,0.24132947976878613,"η in a few steps. For cross-entropy loss, we mark the end point
with ‘x’ when the iterate enters the unstable regime. We train 6CNN with η = 0.02."
E,0.24277456647398843,"the stable regime [39, 41, 37, 3], but GD mostly occurs at the edge of stability after a few steps of
104"
E,0.24421965317919075,"training. We will argue that this behavior is crucial for generalization in neural networks.
105"
E,0.24566473988439305,"For later use, we also define the interaction-aware sharpness as follows:
106"
E,0.24710982658959538,Definition 2 (interaction-aware sharpness).
E,0.24855491329479767,∥H∥Sb ≡tr(HSb)
E,0.25,"tr(Sb) .
(8)"
E,0.2514450867052023,"Here, tr(HSb) ≤∥H∥tr(Sb), i.e., ∥H∥Sb ≤∥H∥, and the equality holds only when every gb is
107"
E,0.25289017341040465,"aligned in the direction of the top eigenvector of H.
108"
E,0.2543352601156069,"Figure 1 (top row) empirically validates (6), showing the normalized loss difference E[Lt+1]−Lt"
E,0.25578034682080925,"tr(Sn)
109"
E,0.25722543352601157,against tr(HSb)
E,0.2586705202312139,"tr(Sn) in the early phase of training before entering the unstable regime. This result implies
110"
E,0.26011560693641617,"that the training loss L(θ) is approximately locally quadratic, i.e., ϵ ≈0, in the early phase. Especially,
111"
E,0.2615606936416185,"for full-batch GD (b = n), the instability condition can be rewritten as ∥H∥Sn > 2"
E,0.2630057803468208,"η and we have the
112"
E,0.26445086705202314,"following relationship between the loss difference Lt+1 −Lt and ∥H∥Sn from (6):
113"
E,0.2658959537572254,Lt+1 −Lt = η2
E,0.26734104046242774,"2 tr(Sn)

∥H∥Sn −2 η"
E,0.26878612716763006,"
+ ϵ.
(9)"
E,0.2702312138728324,Figure 1 (bottom row) shows ∥H∥Sn soars from 0 in a few steps after ∥H∥exceeds 2
E,0.27167630057803466,"η [6], satisfying
114"
E,0.273121387283237,"(9) approximately with ϵ ≈0, before the iterate enters the edge of stability. This result is consistent
115"
E,0.2745664739884393,"with the following Proposition for a quadratic training loss L. The proof is deferred to Appendix B.2.
116"
E,0.27601156069364163,"Proposition 4.1. For GD with a quadratic L, if ∥H∥> 2"
E,0.2774566473988439,η and 0 < λi < 2
E,0.27890173410404623,"η for all i ̸= 1, then
117"
E,0.28034682080924855,"| cos(q1, ∇L(θt))|, |q⊤
1 ∇L(θt)| and ∥H∥Sn increase to 1, ∞and ∥H∥, respectively, as t →∞.
118"
E,0.2817919075144509,"4.2
Non-quadraticity, Asymmetric Valleys and the Edge of Stability
119"
E,0.2832369942196532,"In the previous section, we have shown that the training loss is approximately locally quadratic before
120"
E,0.2846820809248555,"the iterate enters the edge of stability. However, after the iterate enters the edge of stability, i.e.,
121"
E,0.2861271676300578,tr(HSb)
E,0.2875722543352601,tr(Sn) reaches and exceeds 2
E,0.28901734104046245,"η, the step size is relatively large for the sharp loss landscape so that the
122"
E,0.2904624277456647,"iterate jumps across the valley [19], and the higher-order terms ϵ in (6) and (9) become non-negligible
123"
E,0.29190751445086704,"and cause a different behavior of the iterate than in the stable regime.
124"
E,0.29335260115606937,"0.00
0.75
1.50
2.25
3.00
tr(HSb)
tr(Sn) 1e2 2 0 2 4"
E,0.2947976878612717,"(E[Lt + 1]
Lt)/tr(Sn)"
E,0.29624277456647397,"1e
2"
E,0.2976878612716763,slope= 2 2
E,0.2991329479768786,2 = 100
E,0.30057803468208094,"CE, b = 128,
= 0.02 0 500 1000 1500 2000 step"
E,0.3020231213872832,"0.00
0.75
1.50
2.25
3.00
tr(HSb)
tr(Sn) 1e2 2 0 2 4"
E,0.30346820809248554,"(E[Lt + 1]
Lt)/tr(Sn)"
E,0.30491329479768786,"1e
2"
E,0.3063583815028902,slope= 2 2
E,0.3078034682080925,2 = 100
E,0.3092485549132948,"CE, b = 256,
= 0.02 0 500 1000 1500 2000 step"
E,0.3106936416184971,"0.00
0.75
1.50
2.25
3.00
||H||Sn
1e2 2 0 2 4"
E,0.31213872832369943,"(Lt + 1
Lt)/tr(Sn)"
E,0.31358381502890176,"1e
2"
E,0.315028901734104,slope= 2 2
E,0.31647398843930635,2 = 100
E,0.3179190751445087,"CE, GD, 
= 0.02 0 500 1000 1500 step"
E,0.319364161849711,"0.00
0.75
1.50
2.25
3.00
||H||Sn
1e2 2 0 2 4"
E,0.3208092485549133,"(Lt + 1
Lt)/tr(Sn)"
E,0.3222543352601156,"1e
2"
E,0.3236994219653179,slope= 2 2
E,0.32514450867052025,2 = 100
E,0.3265895953757225,"MSE, GD, 
= 0.02 0 500 1000 1500 step"
E,0.32803468208092484,Figure 2: [Non-quadraticity and overestimation] The normalized loss difference E[Lt+1]−Lt
E,0.32947976878612717,"tr(Sn)
against"
E,0.3309248554913295,tr(HSb)
E,0.33236994219653176,"tr(Sn) during training. After the iterate enters the edge of stability, it often shows a more gentle"
E,0.3338150289017341,slope than η2
E,0.3352601156069364,"2 , especially in the unstable regime."
E,0.33670520231213874,"Figure 2 shows empirical evidences for the non-quadraticity. After the SGD/GD iterate enters the
125"
E,0.33815028901734107,"edge of stability, when the instability condition tr(HSb)"
E,0.33959537572254334,tr(Sn) > 2
E,0.34104046242774566,"η is met, the normalized increase in the
126"
E,0.342485549132948,"loss
 E[Lt+1]−Lt"
E,0.3439306358381503,"tr(Sn)
 is often smaller than η2"
E,0.3453757225433526,"2
 tr(HSb)"
E,0.3468208092485549,tr(Sn) −2
E,0.34826589595375723,"η
 from (6) and (9) (blue line) when assuming
127"
E,0.34971098265895956,a locally quadratic function. This results in a gentle slope less than η2
E,0.3511560693641618,"2 .
128"
E,0.35260115606936415,"We hypothesise that due to this non-quadraticity of the training loss, the iterate is discouraged from
129"
E,0.3540462427745665,"staying within the unstable regime. Figure 3 demonstrates the asymmetric valley [12] that one side is
130"
E,0.3554913294797688,"sharp and the other is flat. In Figure 3 (left), we evaluate the directional sharpness ∥Hα∥Sn along
131"
E,0.3569364161849711,the gradient descent direction −η∇L(θ) where Hα ≡H(θ −αη∇L(θ)) for α ∈1
E,0.3583815028901734,"4 × [1, 2, 3, 4, 5],
132"
E,0.3598265895953757,"and compare ∥Hα∥Sn(θ) with ∥H∥Sn(θ). At the sharp side, it has a high ∥H∥Sn > 2"
E,0.36127167630057805,"η (blue) with
133"
E,0.3627167630057804,"the gradient ∇L and the top eigenvector q1(H) of the Hessian being highly aligned (cf. Prop. 4.1).
134"
E,0.36416184971098264,"However, when the loss landscape gets far from being quadratic, the Hessian and its top eigenvector
135"
E,0.36560693641618497,"can change abruptly, q1(Hα) would not always be aligned with q1(H) and ∇L(θ), and ∥Hα∥Sn
136"
E,0.3670520231213873,"tends to decrease. This would be a possible explanation for the tendency of decreasing and then
137"
E,0.3684971098265896,"oscillating ∥H∥Sn. See Appendix C.3 for detailed empirical evidences of the above arguments.
138"
E,0.3699421965317919,"Figure 3 (right) similarly shows that when the iterate is at a sharp side of the valley, it tends to jump
139"
E,0.3713872832369942,"to the other side of a flatter area, and vice versa.
140"
E,0.37283236994219654,"To summarize, we make the following observations for GD in order: (i) ∥H∥increases in the
141"
E,0.37427745664739887,"beginning (the progressive sharpening [6]), (ii) ∥H∥exceeds 2"
E,0.37572254335260113,"η, (iii) the gradient ∇L becomes more
142"
E,0.37716763005780346,"aligned with the top eigenvector q1(H) in a few steps, (iv) ∥H∥Sn reaches the threshold 2"
E,0.3786127167630058,"η and the
143"
E,0.3800578034682081,"iterate jumps across the valley, (v) ∥H∥Sn tends to decrease due to the non-quadraticity, and it repeats
144"
E,0.3815028901734104,"this process, while ∥H∥Sn oscillating around 2"
E,0.3829479768786127,"η. We observe a similar behavior with oscillating
145"
E,0.38439306358381503,tr(HSb)
E,0.38583815028901736,tr(Sn) around 2
E,0.3872832369942196,"η for SGD. It requires further investigation into the exact underlying mechanisms
146"
E,0.38872832369942195,"and we leave it as a future work.
147"
E,0.3901734104046243,"Remark (Experiments in Section 4). We report the experimental results using vanilla SGD/GD
148"
E,0.3916184971098266,"without momentum and weight decay, constant learning rate, and no data augmentation. We train a
149"
E,0.3930635838150289,"simple 6-layer CNN (6CNN, m = 0.51M) on CIFAR-10-8k where DATASET-n denotes a subset of
150"
E,0.3945086705202312,"DATASET with |D| = n and k=210 = 1024. See Appendix C.1-C.3 for the results from other datasets,
151"
E,0.3959537572254335,"learning rates and networks (ResNet-9 with m = 2.3M [13] and WRN-28-2 with m = 36M [55]).
152"
E,0.39739884393063585,"0.25
0.50
0.75
1.00
1.25
10
1 100 101"
E,0.3988439306358382,"||H ||Sn
||H||Sn"
E,0.40028901734104044,"||H||Sn < 2
||H||Sn > 2"
E,0.40173410404624277,"t
zero
min 0 -min L
L"
E,0.4031791907514451,"||H||Sn < 2
||H||Sn > 2"
E,0.4046242774566474,Figure 3: [Asymmetric valleys] Left: The ratio ∥Hα∥Sn
E,0.4060693641618497,"∥H∥Sn
where Hα = H(θ −αη∇L(θ)) for
α = 1"
E,0.407514450867052,"4 × [1, 2, 3, 4, 5] for each t during training. When ∥H∥Sn < 2"
E,0.40895953757225434,"η (red), ∥Hα∥Sn is usually larger
than ∥H∥Sn. On the other hand, when ∥H∥Sn > 2"
E,0.41040462427745666,"η (blue), ∥Hα∥Sn is usually smaller than ∥H∥Sn.
Right: The training loss difference along the gradient descent direction, for each θt. Each plot is
normalized and translated to have the same minimum value and the same zero where ∆L = 0. We
also plot the quadratic baseline (cyan dashed curve). When ∥H∥Sn < 2"
E,0.41184971098265893,"η (red), it usually becomes
sharper across the valley (right-shifted). On the other hand, when ∥H∥Sn > 2"
E,0.41329479768786126,"η (blue), it usually
becomes flatter across the valley (left-shifted). We train 6CNN using GD with η = 0.04."
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.4147398843930636,"5
Generalization through Implicit Regularization
153"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.4161849710982659,"In the previous section, we have empirically demonstrated that the SGD iterate is implicitly discour-
154"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.41763005780346824,"aged from staying within the unstable regime. Now, we are ready to further analyze this property
155"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.4190751445086705,"from the regularization perspective.
156"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.42052023121387283,"5.1
Implicit Interaction Regularization (IIR)
157"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.42196531791907516,"First, to understand the effect of batch size b on the gradient distribution, we define the following ρb:
158"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.4234104046242775,"Definition 3 (a concentration measure of the batch gradient). We define ρb as the ratio of the squared
159"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.42485549132947975,"norm of the total gradient ∥∇L∥2 to the expected squared norm of the batch gradients E[∥gb∥2], i.e.,
160"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.4263005780346821,ρb ≡∥∇L∥2
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.4277456647398844,E[∥gb∥2] = tr(Sn)
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.4291907514450867,"tr(Sb) .
(10)"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.430635838150289,"Here, we can write ∥∇L∥2 = ∥E[gb]∥2 and thus the ratio ρb = ∥E[gb]∥2"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.4320809248554913,"E[∥gb∥2] ≤1 is similar to the square
161"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.43352601156069365,"of the mean resultant length ¯R2
b ≡∥E[ gb"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.434971098265896,"∥gb∥]∥2 ≤1 of the batch gradient gb [36], especially when
162"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.43641618497109824,"std[∥gb∥] is small compared to E[∥gb∥] (see Appendix C.5 for empirical evidences). Both ρb and ¯R2
b
163"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.43786127167630057,"are concentration measures and have lower values when the batch gradients gb are more scattered.
164"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.4393063583815029,"Therefore, it is natural to expect that the ratio ρb is small for a small batch size b, and we will revisit
165"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.4407514450867052,"this in more detail in the following section (cf. (12)). We also note that ρn = ¯R2
n = 1.
166"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.4421965317919075,"Now, we can rewrite the instability condition tr(HSb)"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.4436416184971098,tr(Sn) > 2
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.44508670520231214,"η (multiplying both sides by ρb) as ∥H∥Sb >
167 2ρb"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.44653179190751446,"η . In other words, the interaction-aware sharpness ∥H∥Sb is implicitly regularized to be less than
168 2ρb"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.4479768786127168,"η . We name this Implicit Interaction Regularization (IIR).
169"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.44942196531791906,Definition 4 (Implicit Interaction Regularization (IIR)).
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.4508670520231214,∥H∥Sb ≤2ρb
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.4523121387283237,"η .
(11)"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.45375722543352603,We argue that the upper constraint 2ρb
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.4552023121387283,"η in IIR is crucial in determining the generalization performance.
170"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.45664739884393063,"With a low constraint, SGD strongly regularizes the interaction-aware sharpness ∥H∥Sb. We also
171"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.45809248554913296,"note that IIR affects not only the magnitude ∥H∥but also the directional interaction. In other words,
172"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.4595375722543353,"IIR discourages the batch gradients from aligning with the top eigensubspace of the Hessian that is
173"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.46098265895953755,"spanned by a few largest eigenvectors of the Hessian (cf. [11]).
174"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.4624277456647399,"0
500
1000
1500
2000
Step 0.0 0.5 1.0 1.5 2.0"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.4638728323699422,"ResNet-9, CE, GD, 
= 0.01"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.4653179190751445,train loss 0 100 200 300 400 500 600 700
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.4667630057803468,"800
||H||Sn
||H||"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.4682080924855491,"(a) GD, ResNet-9, cross-entropy, η = 0.01"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.46965317919075145,"0
500
1000
1500
2000
Step 0.0 0.5 1.0 1.5 2.0"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.47109826589595377,"ResNet-9, CE, GD, 
= 0.02"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.4725433526011561,train loss 0 50 100 150 200 250 300 350
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.47398843930635837,"400
||H||Sn
||H||"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.4754335260115607,"(b) GD, ResNet-9, cross-entropy, η = 0.02"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.476878612716763,"0
500
1000
1500
2000
Step 0.0 0.1 0.2 0.3 0.4"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.47832369942196534,"0.5
6CNN, MSE, GD, 
= 0.02"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.4797687861271676,train loss 0 25 50 75 100 125 150 175
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.48121387283236994,"200
||H||Sn
||H||"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.48265895953757226,"(c) GD, 6CNN, MSE, η = 0.02"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.4841040462427746,"0
125
250
375
500
Step 0.0 5.0 10.0 15.0 20.0 25.0 30.0"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.48554913294797686,||H||Sb
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.4869942196531792,b = 212
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.4884393063583815,b = 211
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.48988439306358383,b = 210
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.4913294797687861,b = 29
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.49277456647398843,b = 28
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.49421965317919075,b = 27
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.4956647398843931,b = 26
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.49710982658959535,b = 25
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.4985549132947977,b = 24
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5,b = 23
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5014450867052023,"(d) SGD, ResNet-9, cross-entropy, η = 0.08"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5028901734104047,"Figure 4:
[A clear indication of the edge of stability] (a)-(c): After a few steps of full-batch
training, ∥H∥(blue) hovers above 2"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5043352601156069,"η [6], but ∥H∥Sn (red, defined in (8)) oscillates around 2"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5057803468208093,"η (red
dashed horizontal line). The edge of stability is more evident in the latter (red). Curves are plotted
for every step. We train a model on CIFAR-10-8k (n = 213) using (a)/(b) cross-entropy loss with
η = 0.01/0.02, respectively, and (c) MSE with η = 0.02. (d): We plot curves ∥H∥Sb when trained
with various b’s. After a few steps (around 125), they reach the threshold which linearly increases as
b becomes larger when b ≪n = 213, and saturates to 2ρb η ≈2"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5072254335260116,"η when b is large. Curves are smoothed
for visual clarity. We use SGD with b ∈{23, · · · , 212} and η = 0.08."
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5086705202312138,"Figures 4(a)-4(c) show that, for GD (ρn = 1), the interaction-aware sharpness ∥H∥Sn (red) oscillates
175"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5101156069364162,around 2
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5115606936416185,"η and exhibits IIR. This result is consistent with Cohen et al. [6] that ∥H∥hovers above 2 η
176"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5130057803468208,"for GD. This is because, as mentioned earlier, 2"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5144508670520231,"η ≈∥H∥Sn ≤∥H∥and the equality holds only when
177"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5158959537572254,"the gradient ∇L and the top eigenvector q1 of H are aligned, but generally they are not. For this
178"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5173410404624278,"reason, IIR provides a tighter relation and more clearly identifies the edge of stability than Cohen
179"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5187861271676301,"et al. [6]. These results are also consistent with Prop. 4.1 that ∥H∥Sn suddenly increases from 0 to 2 η
180"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5202312138728323,in a few steps after ∥H∥exceeds 2
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5216763005780347,"η (see Appendix C.3-C.4 for more). Moreover, IIR also applies to
181"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.523121387283237,"a general SGD training with 1 ≤b ≤n. Figure 4(d) shows IIR for SGD with different batch sizes
182"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5245664739884393,"b ∈{23, · · · , 212}. The upper bound (2ρb/η according to (11)) of ∥H∥Sb is higher when using a
183"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5260115606936416,"larger batch size, but limited to less than 2/η (ρb ≤1). We will further discuss this behavior with an
184"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5274566473988439,"investigation of ρb in the following section.
185"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5289017341040463,"5.2
Linear and Saturation Scaling Rule (LSSR)
186"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5303468208092486,"The ratio b/η of batch size b to learning rate η has long been believed as an important factor in-
187"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5317919075144508,"fluencing the generalization performance, and the test accuracy has observed to be similar when
188"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5332369942196532,"trained with the same ratio b/η = b′/η′, i.e., b′ = kb and η′ = kη for k > 0. This is called the
189"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5346820809248555,"linear scaling rule (LSR) [25, 10, 18, 44, 57]. They argue that LSR holds because θt+k −θt =
190 −η"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5361271676300579,"b
Pk−1
i=0
P"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5375722543352601,x∈Bt+i ∇ℓ(x; θt+i) ≈−η
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5390173410404624,"b
Pk−1
i=0
P"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5404624277456648,"x∈Bt+i ∇ℓ(x; θt) = −η′ b′
P"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.541907514450867,"x∈Bt:t+k ∇ℓ(x; θt) as-
191"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5433526011560693,"suming ∇ℓ(θt+i) ≈∇ℓ(θt) for 0 ≤i < k, where Bt:t+k ≡∪k−1
i=0 Bt+i and |Bt:t+k| = kb = b′.
192"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5447976878612717,"However, the assumption is false and the gradient oscillates mostly with a negative cosine value
193"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.546242774566474,"cos(gb(θt), gb(θt+1)) < 0 between two consecutive gradients after entering the edge of stability
194"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5476878612716763,"20
22
24
26
28
210
212"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5491329479768786,"batch size (b) 2
8 2
6 2
4 2
2 20 22 24 f(b)"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5505780346820809,"LSSR: f(b) =
b
LSR: f(b) = b"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5520231213872833,"SRSR: f(b) =
b"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5534682080924855,3 4 5 6 7 8 9 10111213
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5549132947976878,log2(b)
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5563583815028902,"6
5
4
3
2
1
0
-1
-2
-3
-4
-5
-6
-7
-8
-9
-10"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5578034682080925,log2( )
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5592485549132948,"CIFAR-10-8k, ResNet-9 20 30 40 50 60 70"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5606936416184971,3 4 5 6 7 8 9 10111213
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5621387283236994,log2(b)
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5635838150289018,"6
5
4
3
2
1
0
-1
-2
-3
-4
-5
-6
-7
-8
-9
-10"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.565028901734104,log2( )
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5664739884393064,"CIFAR-100-8k, ResNet-9 5 10 15 20 25 30 35 40"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5679190751445087,3 4 5 6 7 8 9 101112
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.569364161849711,log2(b)
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5708092485549133,"4
3
2
1
0
-1
-2
-3
-4
-5
-6
-7
-8
-9
-10
log2( )"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5722543352601156,"STL-10-4k, ResNet-9 40 45 50 55 60 65 70"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5736994219653179,3 4 5 6 7 8 9 101112131415
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5751445086705202,log2(b)
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5765895953757225,"6
5
4
3
2
1
0
-1
-2
-3
-4
-5
-6
-7
-8
-9
-10"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5780346820809249,log2( )
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5794797687861272,"Tiny-ImageNet-32k, ResNet-9 2.5 5.0 7.5 10.0 12.5 15.0 17.5"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5809248554913294,"Figure 5: [Linear and Saturation Scaling Rule (LSSR)] Left: LSSR (red) in (12), LSR (black dotted
line) [10] and SRSR (blue dotted line) [15]. For LSSR, we can observe both linear and saturation
regions (n = 8k, ρ = 2−7). Right: Heatmaps of test accuracy for models trained with a large number
of pairs of (b, η) on CIFAR-10-8k , CIFAR-100-8k , STL-10-4k, and Tiny-ImageNet-32k (from
left to right, from top to bottom). It does not follow either LSR or SRSR, but LSSR. We also plot
f(b) = ρb (yellow dashed curve) for some ρ on each heatmap. Note that they are all log-log plots
and thus a slope of 1 means it is linear."
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5823699421965318,"(see Appendix C.3). Moreover, LSR fails when the batch size is large [18, 38, 57, 43, 46]. On the
195"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5838150289017341,"other hand, Krizhevsky [25], Hoffer et al. [15] propose the square root scaling rule (SRSR) with
196"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5852601156069365,"another ratio
√"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5867052023121387,"b/η to keep the covariance of the parameter update constant for b ≪n based on
197"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.588150289017341,"Var[ηgb] = η2Cb = γn,bη2"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5895953757225434,"b
C1 ≈η2"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5910404624277457,"b C1. However, Shallue et al. [42] show that both LSR and SRSR
198"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5924855491329479,"do not hold in general.
199"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5939306358381503,"Based on the analysis of IIR with a new ratio 2ρb/η in the previous section, we explore why LSR fails
200"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5953757225433526,"in the large-batch regime and provide a more accurate rule to explain the generalization performance
201"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.596820809248555,"of the models trained with various choices of batch size and learning rate pairs (b, η).
202"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5982658959537572,"To this end, we investigate the concentration measure ρb = tr(Sn)/ tr(Sb). By combining two
203"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.5997109826589595,"equations, Cb = Sb −Sn (by definition) and Cb = γn,b"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.6011560693641619,"b (S1 −Sn) in (1), we can obtain Sb =
204"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.6026011560693642,"Cb + Sn = γn,b"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.6040462427745664,"b S1 + (1 −γn,b"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.6054913294797688,"b )Sn. Therefore, we have tr(Sb) = γn,b"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.6069364161849711,"b
tr(S1) + (1 −γn,b"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.6083815028901735,"b ) tr(Sn),
205"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.6098265895953757,"which leads to the following equation:
206"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.611271676300578,ρb ≡tr(Sn)
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.6127167630057804,"tr(Sb) =
tr(Sn)
γn,b"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.6141618497109826,"b
tr(S1) + (1 −γn,b"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.615606936416185,"b ) tr(Sn) =
1
γn,b"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.6170520231213873,"b
1
ρ + (1 −γn,b"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.6184971098265896,"b )
|
{z
}
(∗) ≈"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.619942196531792,"(
b
γn,b ρ ≈bρ
if b is small
1
if b is large"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.6213872832369942,"(12)
from (10) where ρ = ρ1 = tr(Sn)/ tr(S1). Note that ρ is (much) smaller than 1 because ∇ℓ(xi)
207"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.6228323699421965,"has different direction for each xi and tr(Sn) = ∥∇L∥2 = ∥1 n
P"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.6242774566473989,"i ∇ℓ(xi)∥2 ≤1 n
P"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.6257225433526011,"i ∥∇ℓ(xi)∥2 =
208"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.6271676300578035,"tr(S1). In other words, 1/ρ is (much) larger than 1 (see Appendix C.5).
209"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.6286127167630058,"Figure 5 (left) demonstrates a new scaling rule with the ratio ρb/η, called the Linear and Saturation
210"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.630057803468208,"Scaling Rule (LSSR), with the two regimes that (i) ρb is almost linear when b ≪n (linear regime) and
211"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.6315028901734104,"(ii) ρb saturates when b is large (saturation regime), which are also shown in Figure 4(d). It depends
212"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.6329479768786127,"on which part of the denominator (∗) in (12) dominates the other. First, when b ≪n, then γn,b/b is
213"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.634393063583815,"not very small and the first term γn,b"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.6358381502890174,"b
1
ρ dominates the second term 1 −γn,b"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.6372832369942196,"b
since 1"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.638728323699422,"ρ ≫1. Second, as
214"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.6401734104046243,"b becomes large, γn,b/b ≈0 and the second term (≈1) dominates the first term. Thus, ρb saturates
215"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.6416184971098265,"to 1 and is not linearly related to b, and LSR is no longer valid. The above arguments also hold for
216"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.6430635838150289,"the batches sampled with replacement where the only modification is γn,b = 1, ∀b in (12). Figure 5
217"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.6445086705202312,"(right) empirically supports LSSR with the test accuracies when trained with various combinations of
218"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.6459537572254336,"pairs (b, η). To be specific, the optimal learning rate is almost linear when b is small, but it saturates
219"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.6473988439306358,"when b is large. We also plot f(b) = ρb (the yellow dashed curve) for some ρ. Note that Figure 8 of
220"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.6488439306358381,"Shallue et al. [42, Section 4.7] shows similar “linear and saturation” behaviors supportive of LSSR
221"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.6502890173410405,"on other datasets (see also Figure 7 of Zhang et al. [57, Section 4.3]).
222"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.6517341040462428,"Remark (Experiments in Section 5). We train models using vanilla SGD/GD without momentum and
223"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.653179190751445,"weight decay, constant learning rate, and no data augmentation. For Figure 5, we use subsets of the
224"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.6546242774566474,"datasets CIFAR-10 [24], CIFAR-100 [24], STL-10 [5], and Tiny-ImageNet (a subset of ImageNet [7]
225"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.6560693641618497,"with 3 × 64 × 64 images and 200 object classes). We use a large number of epochs (800) and batch
226"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.6575144508670521,"normalization [17] to achieve a zero training error even with a large b and a small η. However, in
227"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.6589595375722543,"the lower right corner (red area) of each heatmap in Figure 5 (right), when b is too large or η is too
228"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.6604046242774566,"small so that ∥θt+1 −θt∥= η∥gb∥is too small, it requires an exponentially large number of steps
229"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.661849710982659,"for the iterate to enter the edge of stability. Thus, in this case, the assumption in Goyal et al. [10],
230"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.6632947976878613,"∇ℓ(θt) ≈∇ℓ(θt+i) for 0 ≤i < k, approximately holds and the reasoning on LSR is valid. However,
231"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.6647398843930635,"this only holds for a non-practical (b, η) which shows a suboptimal performance. See Appendix
232"
GENERALIZATION THROUGH IMPLICIT REGULARIZATION,0.6661849710982659,"C.4-C.5 for the results from other networks and hyperparameters.
233"
DISCUSSION,0.6676300578034682,"6
Discussion
234"
DISCUSSION,0.6690751445086706,"We provide a new insight on the link between the batch gradient distribution and the sharpness of the
235"
DISCUSSION,0.6705202312138728,"loss landscape. In this section, we reconcile our arguments with some previous studies.
236"
DISCUSSION,0.6719653179190751,"Jastrz˛ebski et al. [18] explain the optimization behavior of SGD with the SDE approximation
237"
DISCUSSION,0.6734104046242775,"dθt = −∇L(θt)dt +
p η"
DISCUSSION,0.6748554913294798,"b C1/2
1
dW(t) of the SGD where W is an m-dimensional Brownian motion.
238"
DISCUSSION,0.6763005780346821,"Therefore, the same ratio η"
DISCUSSION,0.6777456647398844,b = η′
DISCUSSION,0.6791907514450867,"b′ leads to the same SDE, which implies LSR. Moreover, a large η b
239"
DISCUSSION,0.680635838150289,"implies a large diffusion in SDE, which has been linked with the escaping efficiency from a sharp
240"
DISCUSSION,0.6820809248554913,"local minimum in Zhu et al. [58]. We instead argue that a large second moment tr(Sb) (compared
241"
DISCUSSION,0.6835260115606936,"to tr(Sn)) and a large η lead to a low constraint 2ρb/η on the interaction-aware sharpness. We
242"
DISCUSSION,0.684971098265896,"emphasize that we do not model SGD with SDE and thus our argument is applicable to a practical
243"
DISCUSSION,0.6864161849710982,"learning rate regime.
244"
DISCUSSION,0.6878612716763006,"Wu et al. [49] empirically show that what is important for the generalization performance of a neural
245"
DISCUSSION,0.6893063583815029,"network is not the class to which the gradient distribution belongs, but the second moment of the
246"
DISCUSSION,0.6907514450867052,"distribution. This is consistent with our arguments with the interaction tr(HSb) and the concentration
247"
DISCUSSION,0.6921965317919075,"measure ρb = tr(Sn)/ tr(Sb), because they depend on the second moment Sb, not on the class of the
248"
DISCUSSION,0.6936416184971098,"gradient distribution.
249"
DISCUSSION,0.6950867052023122,"Recently, Li et al. [33] suggest a necessary condition that the “noise-to-signal ratio” needs to be
250"
DISCUSSION,0.6965317919075145,"large for LSR (and the SDE assumption) to hold. This is consistent with our result on the linear
251"
DISCUSSION,0.6979768786127167,"regime (where b and ρb are small) because the noise-to-signal ratio is approximately the inverse of the
252"
DISCUSSION,0.6994219653179191,"“signal-to-noise” ratio ρb = tr(Sn)/ tr(Sb), but defined for an equilibrium distribution. We provide
253"
DISCUSSION,0.7008670520231214,"not only the necessary condition but also the sufficient condition for LSR with a novel scaling rule
254"
DISCUSSION,0.7023121387283237,"LSSR applicable to every batch size including where LSR fails (the saturation regime).
255"
CONCLUSION,0.703757225433526,"7
Conclusion
256"
CONCLUSION,0.7052023121387283,"From an analysis of unstable dynamics of SGD (Section 4.1) and the instability condition (Definition
257"
CONCLUSION,0.7066473988439307,"1), we clearly mark the edge of stability (Figure 4) with the interaction-aware sharpness ∥H∥Sb
258"
CONCLUSION,0.708092485549133,"(Definition 2) and show the presence of the implicit regularization effect on the interaction between
259"
CONCLUSION,0.7095375722543352,"the gradient distribution and the loss landscape geometry (IIR) (Section 5.1, Definition 4). Moreover,
260"
CONCLUSION,0.7109826589595376,"introducing the concentration measure ρb of the batch gradient (Definition 3, (12)), we link the
261"
CONCLUSION,0.7124277456647399,"second moment of the gradient distribution and the sharpness of the loss landscape, and propose
262"
CONCLUSION,0.7138728323699421,"a new scaling rule called Linear and Saturation Scaling Rule (LSSR) (Section 5.2, Figure 5). Due
263"
CONCLUSION,0.7153179190751445,"to the simplicity of the analysis, we hope that our insights will motivate the future work toward
264"
CONCLUSION,0.7167630057803468,"understanding various learning tasks.
265"
REFERENCES,0.7182080924855492,"References
266"
REFERENCES,0.7196531791907514,"[1] David Barrett and Benoit Dherin. Implicit gradient regularization. In International Con-
267"
REFERENCES,0.7210982658959537,"ference on Learning Representations, 2021. URL https://openreview.net/forum?id=
268"
REFERENCES,0.7225433526011561,"3q5IqUrkcF.
269"
REFERENCES,0.7239884393063584,"[2] Nils Bjorck, Carla P Gomes, Bart Selman, and Kilian Q Weinberger. Understanding batch
270"
REFERENCES,0.7254335260115607,"normalization. Advances in neural information processing systems, 31, 2018.
271"
REFERENCES,0.726878612716763,"[3] Léon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
272"
REFERENCES,0.7283236994219653,"learning. Siam Review, 60(2):223–311, 2018.
273"
REFERENCES,0.7297687861271677,"[4] Pratik Chaudhari and Stefano Soatto. Stochastic gradient descent performs variational infer-
274"
REFERENCES,0.7312138728323699,"ence, converges to limit cycles for deep networks. In International Conference on Learning
275"
REFERENCES,0.7326589595375722,"Representations, 2018.
276"
REFERENCES,0.7341040462427746,"[5] Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsuper-
277"
REFERENCES,0.7355491329479769,"vised feature learning. In Proceedings of the fourteenth international conference on artificial
278"
REFERENCES,0.7369942196531792,"intelligence and statistics, pages 215–223. JMLR Workshop and Conference Proceedings, 2011.
279"
REFERENCES,0.7384393063583815,"[6] Jeremy Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter, and Ameet Talwalkar. Gradient descent
280"
REFERENCES,0.7398843930635838,"on neural networks typically occurs at the edge of stability. In International Conference on Learn-
281"
REFERENCES,0.7413294797687862,"ing Representations, 2021. URL https://openreview.net/forum?id=jh-rTtvkGeM.
282"
REFERENCES,0.7427745664739884,"[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-
283"
REFERENCES,0.7442196531791907,"scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern
284"
REFERENCES,0.7456647398843931,"recognition, pages 248–255. Ieee, 2009.
285"
REFERENCES,0.7471098265895953,"[8] Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize
286"
REFERENCES,0.7485549132947977,"for deep nets. In International Conference on Machine Learning, pages 1019–1028. PMLR,
287"
REFERENCES,0.75,"2017.
288"
REFERENCES,0.7514450867052023,"[9] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware min-
289"
REFERENCES,0.7528901734104047,"imization for efficiently improving generalization. In International Conference on Learning
290"
REFERENCES,0.7543352601156069,"Representations, 2021. URL https://openreview.net/forum?id=6Tm1mposlrM.
291"
REFERENCES,0.7557803468208093,"[10] Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola,
292"
REFERENCES,0.7572254335260116,"Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training
293"
REFERENCES,0.7586705202312138,"imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
294"
REFERENCES,0.7601156069364162,"[11] Guy Gur-Ari, Daniel A Roberts, and Ethan Dyer. Gradient descent happens in a tiny subspace.
295"
REFERENCES,0.7615606936416185,"arXiv preprint arXiv:1812.04754, 2018.
296"
REFERENCES,0.7630057803468208,"[12] Haowei He, Gao Huang, and Yang Yuan. Asymmetric valleys: Beyond sharp and flat local
297"
REFERENCES,0.7644508670520231,"minima. Advances in neural information processing systems, 32, 2019.
298"
REFERENCES,0.7658959537572254,"[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
299"
REFERENCES,0.7673410404624278,"recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
300"
REFERENCES,0.7687861271676301,"pages 770–778, 2016.
301"
REFERENCES,0.7702312138728323,"[14] Sepp Hochreiter and Jürgen Schmidhuber. Flat minima. Neural computation, 9(1):1–42, 1997.
302"
REFERENCES,0.7716763005780347,"[15] Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the
303"
REFERENCES,0.773121387283237,"generalization gap in large batch training of neural networks. Advances in neural information
304"
REFERENCES,0.7745664739884393,"processing systems, 30, 2017.
305"
REFERENCES,0.7760115606936416,"[16] Wenqing Hu, Chris Junchi Li, Lei Li, and Jian-Guo Liu. On the diffusion approximation of
306"
REFERENCES,0.7774566473988439,"nonconvex stochastic gradient descent. Annals of Mathematical Sciences and Applications, 4
307"
REFERENCES,0.7789017341040463,"(1), 2019.
308"
REFERENCES,0.7803468208092486,"[17] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
309"
REFERENCES,0.7817919075144508,"by reducing internal covariate shift. In International conference on machine learning, pages
310"
REFERENCES,0.7832369942196532,"448–456. PMLR, 2015.
311"
REFERENCES,0.7846820809248555,"[18] Stanisław Jastrz˛ebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua
312"
REFERENCES,0.7861271676300579,"Bengio, and Amos Storkey.
Three factors influencing minima in sgd.
arXiv preprint
313"
REFERENCES,0.7875722543352601,"arXiv:1711.04623, 2017.
314"
REFERENCES,0.7890173410404624,"[19] Stanisław Jastrz˛ebski, Zachary Kenton, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and
315"
REFERENCES,0.7904624277456648,"Amost Storkey. On the relation between the sharpest directions of DNN loss and the SGD
316"
REFERENCES,0.791907514450867,"step length. In International Conference on Learning Representations, 2019. URL https:
317"
REFERENCES,0.7933526011560693,"//openreview.net/forum?id=SkgEaj05t7.
318"
REFERENCES,0.7947976878612717,"[20] Stanisław Jastrz˛ebski, Maciej Szymczak, Stanislav Fort, Devansh Arpit, Jacek Tabor,
319"
REFERENCES,0.796242774566474,"Kyunghyun Cho*, and Krzysztof Geras*. The break-even point on optimization trajecto-
320"
REFERENCES,0.7976878612716763,"ries of deep neural networks. In International Conference on Learning Representations, 2020.
321"
REFERENCES,0.7991329479768786,"URL https://openreview.net/forum?id=r1g87C4KwB.
322"
REFERENCES,0.8005780346820809,"[21] Stanisław Jastrz˛ebski, Devansh Arpit, Oliver Astrand, Giancarlo B Kerg, Huan Wang, Caiming
323"
REFERENCES,0.8020231213872833,"Xiong, Richard Socher, Kyunghyun Cho, and Krzysztof J Geras. Catastrophic fisher explosion:
324"
REFERENCES,0.8034682080924855,"Early phase fisher matrix impacts generalization. In Marina Meila and Tong Zhang, editors,
325"
REFERENCES,0.8049132947976878,"Proceedings of the 38th International Conference on Machine Learning, volume 139 of Pro-
326"
REFERENCES,0.8063583815028902,"ceedings of Machine Learning Research, pages 4772–4784. PMLR, 18–24 Jul 2021. URL
327"
REFERENCES,0.8078034682080925,"https://proceedings.mlr.press/v139/jastrzebski21a.html.
328"
REFERENCES,0.8092485549132948,"[22] Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic
329"
REFERENCES,0.8106936416184971,"generalization measures and where to find them. In International Conference on Learning
330"
REFERENCES,0.8121387283236994,"Representations, 2020. URL https://openreview.net/forum?id=SJgIPJBFvH.
331"
REFERENCES,0.8135838150289018,"[23] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping
332"
REFERENCES,0.815028901734104,"Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp
333"
REFERENCES,0.8164739884393064,"minima. In 5th International Conference on Learning Representations, ICLR 2017, Toulon,
334"
REFERENCES,0.8179190751445087,"France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL
335"
REFERENCES,0.819364161849711,"https://openreview.net/forum?id=H1oyRlYgg.
336"
REFERENCES,0.8208092485549133,"[24] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Master’s
337"
REFERENCES,0.8222543352601156,"thesis, Department of Computer Science, University of Toronto, 2009.
338"
REFERENCES,0.8236994219653179,"[25] Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks. arXiv
339"
REFERENCES,0.8251445086705202,"preprint arXiv:1404.5997, 2014.
340"
REFERENCES,0.8265895953757225,"[26] Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. Asam: Adaptive sharpness-
341"
REFERENCES,0.8280346820809249,"aware minimization for scale-invariant learning of deep neural networks, 2021.
342"
REFERENCES,0.8294797687861272,"[27] Yann A LeCun, Léon Bottou, Genevieve B Orr, and Klaus-Robert Müller. Efficient backprop.
343"
REFERENCES,0.8309248554913294,"In Neural networks: Tricks of the trade, pages 9–48. Springer, 2012.
344"
REFERENCES,0.8323699421965318,"[28] Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari.
345"
REFERENCES,0.8338150289017341,"The large learning rate phase of deep learning: the catapult mechanism.
arXiv preprint
346"
REFERENCES,0.8352601156069365,"arXiv:2003.02218, 2020.
347"
REFERENCES,0.8367052023121387,"[29] Chris Junchi Li, Lei Li, Junyang Qian, and Jian-Guo Liu. Batch size matters: A diffusion
348"
REFERENCES,0.838150289017341,"approximation framework on nonconvex stochastic gradient descent. stat, 1050:22, 2017.
349"
REFERENCES,0.8395953757225434,"[30] Qianxiao Li, Cheng Tai, and E Weinan. Stochastic modified equations and adaptive stochastic
350"
REFERENCES,0.8410404624277457,"gradient algorithms. In International Conference on Machine Learning, pages 2101–2110.
351"
REFERENCES,0.8424855491329479,"PMLR, 2017.
352"
REFERENCES,0.8439306358381503,"[31] Qianxiao Li, Cheng Tai, and E Weinan. Stochastic modified equations and dynamics of
353"
REFERENCES,0.8453757225433526,"stochastic gradient algorithms i: Mathematical foundations. The Journal of Machine Learning
354"
REFERENCES,0.846820809248555,"Research, 20(1):1474–1520, 2019.
355"
REFERENCES,0.8482658959537572,"[32] Yuanzhi Li, Colin Wei, and Tengyu Ma. Towards explaining the regularization effect of initial
356"
REFERENCES,0.8497109826589595,"large learning rate in training neural networks. Advances in Neural Information Processing
357"
REFERENCES,0.8511560693641619,"Systems, 32, 2019.
358"
REFERENCES,0.8526011560693642,"[33] Zhiyuan Li, Sadhika Malladi, and Sanjeev Arora. On the validity of modeling sgd with stochastic
359"
REFERENCES,0.8540462427745664,"differential equations (sdes). Advances in Neural Information Processing Systems, 34, 2021.
360"
REFERENCES,0.8554913294797688,"[34] Stephan Mandt, Matthew Hoffman, and David Blei. A variational analysis of stochastic gradient
361"
REFERENCES,0.8569364161849711,"algorithms. In International conference on machine learning, pages 354–363. PMLR, 2016.
362"
REFERENCES,0.8583815028901735,"[35] Stephan Mandt, Matthew D Hoffman, and David M Blei. Stochastic gradient descent as
363"
REFERENCES,0.8598265895953757,"approximate bayesian inference. Journal of Machine Learning Research, 18:1–35, 2017.
364"
REFERENCES,0.861271676300578,"[36] Kanti V Mardia, Peter E Jupp, and KV Mardia. Directional statistics, volume 2. Wiley Online
365"
REFERENCES,0.8627167630057804,"Library, 2000.
366"
REFERENCES,0.8641618497109826,"[37] James Martens. New insights and perspectives on the natural gradient method. arXiv preprint
367"
REFERENCES,0.865606936416185,"arXiv:1412.1193, 2014.
368"
REFERENCES,0.8670520231213873,"[38] Dominic Masters and Carlo Luschi. Revisiting small batch training for deep neural networks.
369"
REFERENCES,0.8684971098265896,"arXiv preprint arXiv:1804.07612, 2018.
370"
REFERENCES,0.869942196531792,"[39] Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87.
371"
REFERENCES,0.8713872832369942,"Springer Science & Business Media, 2003.
372"
REFERENCES,0.8728323699421965,"[40] Behnam Neyshabur. Implicit regularization in deep learning. arXiv preprint arXiv:1709.01953,
373"
REFERENCES,0.8742774566473989,"2017.
374"
REFERENCES,0.8757225433526011,"[41] Mark Schmidt. Convergence rate of stochastic gradient with constant step size. 2014.
375"
REFERENCES,0.8771676300578035,"[42] Christopher J Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein, Roy Frostig, and
376"
REFERENCES,0.8786127167630058,"George E Dahl. Measuring the effects of data parallelism on neural network training. arXiv
377"
REFERENCES,0.880057803468208,"preprint arXiv:1811.03600, 2018.
378"
REFERENCES,0.8815028901734104,"[43] Samuel Smith, Erich Elsen, and Soham De. On the generalization benefit of noise in stochastic
379"
REFERENCES,0.8829479768786127,"gradient descent. In International Conference on Machine Learning, pages 9058–9067. PMLR,
380"
REFERENCES,0.884393063583815,"2020.
381"
REFERENCES,0.8858381502890174,"[44] Samuel L Smith and Quoc V Le. A bayesian perspective on generalization and stochastic
382"
REFERENCES,0.8872832369942196,"gradient descent. In International Conference on Learning Representations, 2018.
383"
REFERENCES,0.888728323699422,"[45] Samuel L Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc V Le. Don’t decay the learning
384"
REFERENCES,0.8901734104046243,"rate, increase the batch size. arXiv preprint arXiv:1711.00489, 2017.
385"
REFERENCES,0.8916184971098265,"[46] Samuel L Smith, Benoit Dherin, David Barrett, and Soham De. On the origin of implicit regular-
386"
REFERENCES,0.8930635838150289,"ization in stochastic gradient descent. In International Conference on Learning Representations,
387"
REFERENCES,0.8945086705202312,"2021. URL https://openreview.net/forum?id=rq_Qr0c1Hyo.
388"
REFERENCES,0.8959537572254336,"[47] Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The
389"
REFERENCES,0.8973988439306358,"implicit bias of gradient descent on separable data. The Journal of Machine Learning Research,
390"
REFERENCES,0.8988439306358381,"19(1):2822–2878, 2018.
391"
REFERENCES,0.9002890173410405,"[48] Valentin Thomas, Fabian Pedregosa, Bart Merriënboer, Pierre-Antoine Manzagol, Yoshua
392"
REFERENCES,0.9017341040462428,"Bengio, and Nicolas Le Roux. On the interplay between noise and curvature and its effect
393"
REFERENCES,0.903179190751445,"on optimization and generalization. In International Conference on Artificial Intelligence and
394"
REFERENCES,0.9046242774566474,"Statistics, pages 3503–3513. PMLR, 2020.
395"
REFERENCES,0.9060693641618497,"[49] Jingfeng Wu, Wenqing Hu, Haoyi Xiong, Jun Huan, Vladimir Braverman, and Zhanxing Zhu.
396"
REFERENCES,0.9075144508670521,"On the noisy gradient descent that generalizes as sgd. In International Conference on Machine
397"
REFERENCES,0.9089595375722543,"Learning, pages 10367–10376. PMLR, 2020.
398"
REFERENCES,0.9104046242774566,"[50] Lei Wu, Chao Ma, and Weinan E. How sgd selects the global minima in over-parameterized
399"
REFERENCES,0.911849710982659,"learning: A dynamical stability perspective. In Proceedings of the 32nd International Conference
400"
REFERENCES,0.9132947976878613,"on Neural Information Processing Systems, pages 8289–8298, 2018.
401"
REFERENCES,0.9147398843930635,"[51] Chen Xing, Devansh Arpit, Christos Tsirigotis, and Yoshua Bengio. A walk with sgd. arXiv
402"
REFERENCES,0.9161849710982659,"preprint arXiv:1802.08770, 2018.
403"
REFERENCES,0.9176300578034682,"[52] Sho Yaida. Fluctuation-dissipation relations for stochastic gradient descent. In International
404"
REFERENCES,0.9190751445086706,"Conference on Learning Representations, 2019. URL https://openreview.net/forum?
405"
REFERENCES,0.9205202312138728,"id=SkNksoRctQ.
406"
REFERENCES,0.9219653179190751,"[53] Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael W Mahoney. Pyhessian: Neural
407"
REFERENCES,0.9234104046242775,"networks through the lens of the hessian. In 2020 IEEE International Conference on Big Data
408"
REFERENCES,0.9248554913294798,"(Big Data), pages 581–590. IEEE, 2020.
409"
REFERENCES,0.9263005780346821,"[54] Yang You, Igor Gitman, and Boris Ginsburg. Scaling sgd batch size to 32k for imagenet training.
410"
REFERENCES,0.9277456647398844,"arXiv preprint arXiv:1708.03888, 2017.
411"
REFERENCES,0.9291907514450867,"[55] Sergey Zagoruyko and Nikos Komodakis.
Wide residual networks.
arXiv preprint
412"
REFERENCES,0.930635838150289,"arXiv:1605.07146, 2016.
413"
REFERENCES,0.9320809248554913,"[56] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
414"
REFERENCES,0.9335260115606936,"deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):
415"
REFERENCES,0.934971098265896,"107–115, 2021.
416"
REFERENCES,0.9364161849710982,"[57] Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George Dahl, Chris
417"
REFERENCES,0.9378612716763006,"Shallue, and Roger B Grosse. Which algorithmic choices matter at which batch sizes? insights
418"
REFERENCES,0.9393063583815029,"from a noisy quadratic model. Advances in neural information processing systems, 32, 2019.
419"
REFERENCES,0.9407514450867052,"[58] Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, and Jinwen Ma. The anisotropic noise in
420"
REFERENCES,0.9421965317919075,"stochastic gradient descent: Its behavior of escaping from sharp minima and regularization
421"
REFERENCES,0.9436416184971098,"effects. In International Conference on Machine Learning, pages 7654–7663. PMLR, 2019.
422"
REFERENCES,0.9450867052023122,"Checklist
423"
REFERENCES,0.9465317919075145,"(a) For all authors...
424"
REFERENCES,0.9479768786127167,"(a) Do the main claims made in the abstract and introduction accurately reflect the paper’s
425"
REFERENCES,0.9494219653179191,"contributions and scope? [Yes]
426"
REFERENCES,0.9508670520231214,"(b) Did you describe the limitations of your work? [Yes] We try to avoid theoretical
427"
REFERENCES,0.9523121387283237,"analysis based on impractical assumptions. Therefore, some of our claims are supported
428"
REFERENCES,0.953757225433526,"by experiments and may require further theoretical investigation.
429"
REFERENCES,0.9552023121387283,"(c) Did you discuss any potential negative societal impacts of your work? [N/A]
430"
REFERENCES,0.9566473988439307,"(d) Have you read the ethics review guidelines and ensured that your paper conforms to
431"
REFERENCES,0.958092485549133,"them? [Yes]
432"
REFERENCES,0.9595375722543352,"(b) If you are including theoretical results...
433"
REFERENCES,0.9609826589595376,"(a) Did you state the full set of assumptions of all theoretical results? [Yes] See Prop. 4.1.
434"
REFERENCES,0.9624277456647399,"(b) Did you include complete proofs of all theoretical results? [Yes] See Appendix B.
435"
REFERENCES,0.9638728323699421,"(c) If you ran experiments...
436"
REFERENCES,0.9653179190751445,"(a) Did you include the code, data, and instructions needed to reproduce the main experi-
437"
REFERENCES,0.9667630057803468,"mental results (either in the supplemental material or as a URL)? [Yes]
438"
REFERENCES,0.9682080924855492,"(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
439"
REFERENCES,0.9696531791907514,"were chosen)? [Yes]
440"
REFERENCES,0.9710982658959537,"(c) Did you report error bars (e.g., with respect to the random seed after running experi-
441"
REFERENCES,0.9725433526011561,"ments multiple times)? [N/A]
442"
REFERENCES,0.9739884393063584,"(d) Did you include the total amount of compute and the type of resources used (e.g.,
443"
REFERENCES,0.9754335260115607,"type of GPUs, internal cluster, or cloud provider)? [N/A] We do not propose any new
444"
REFERENCES,0.976878612716763,"algorithm which requires to report the computational cost.
445"
REFERENCES,0.9783236994219653,"(d) If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
446"
REFERENCES,0.9797687861271677,"(a) If your work uses existing assets, did you cite the creators? [Yes]
447"
REFERENCES,0.9812138728323699,"(b) Did you mention the license of the assets? [Yes]
448"
REFERENCES,0.9826589595375722,"(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]
449"
REFERENCES,0.9841040462427746,"(d) Did you discuss whether and how consent was obtained from people whose data you’re
450"
REFERENCES,0.9855491329479769,"using/curating? [Yes]
451"
REFERENCES,0.9869942196531792,"(e) Did you discuss whether the data you are using/curating contains personally identifiable
452"
REFERENCES,0.9884393063583815,"information or offensive content? [N/A]
453"
REFERENCES,0.9898843930635838,"(e) If you used crowdsourcing or conducted research with human subjects...
454"
REFERENCES,0.9913294797687862,"(a) Did you include the full text of instructions given to participants and screenshots, if
455"
REFERENCES,0.9927745664739884,"applicable? [N/A]
456"
REFERENCES,0.9942196531791907,"(b) Did you describe any potential participant risks, with links to Institutional Review
457"
REFERENCES,0.9956647398843931,"Board (IRB) approvals, if applicable? [N/A]
458"
REFERENCES,0.9971098265895953,"(c) Did you include the estimated hourly wage paid to participants and the total amount
459"
REFERENCES,0.9985549132947977,"spent on participant compensation? [N/A]
460"
