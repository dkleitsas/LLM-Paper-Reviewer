Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002004008016032064,"Explaining deep learning model inferences is a promising venue for scientiﬁc un-
derstanding, improving safety, uncovering hidden biases, evaluating fairness, and
beyond, as argued by many scholars. One of the principal beneﬁts of counterfac-
tual explanations is allowing users to explore “what-if” scenarios through what
does not and cannot exist in the data, a quality that many other forms of explana-
tion such as heatmaps and inﬂuence functions are inherently incapable of doing.
However, most previous work on generative explainability cannot disentangle im-
portant concepts effectively, produces unrealistic examples, or fails to retain rel-
evant information. We propose a novel approach, DISSECT, that jointly trains
a generator, a discriminator, and a concept disentangler to overcome such chal-
lenges using little supervision. DISSECT generates Concept Traversals (CTs),
deﬁned as a sequence of generated examples with increasing degrees of concepts
that inﬂuence a classiﬁer’s decision. By training a generative model from a clas-
siﬁer’s signal, DISSECT offers a way to discover a classiﬁer’s inherent “notion”
of distinct concepts automatically rather than rely on user-predeﬁned concepts.
We show that DISSECT produces CTs that (1) disentangle several concepts, (2)
are inﬂuential to a classiﬁer’s decision and are coupled to its reasoning due to
joint training (3), are realistic, (4) preserve relevant information, and (5) are stable
across similar inputs. We validate DISSECT on several challenging synthetic and
realistic datasets where previous methods fall short of satisfying desirable criteria
for interpretability and show that it performs consistently well. Finally, we present
experiments showing applications of DISSECT for detecting potential biases of a
classiﬁer and identifying spurious artifacts that impact predictions."
INTRODUCTION,0.004008016032064128,"1
INTRODUCTION"
INTRODUCTION,0.006012024048096192,"Explanation of the internal inferences of deep learning models remains a challenging problem that
many scholars deem promising for improving safety, evaluating fairness, and beyond [13, 16, 23,
49]. Many efforts in explainability methods have been working towards providing solutions for
this challenging problem. One way to categorize them is by the type of explanations, some post
hoc techniques focusing on the importance of individual features, such as saliency maps [19, 47,
76, 79], some on importance of individual examples [33, 34, 39, 87], some on importance of high-
level concepts [35]. There has been active research into the shortcomings of explainability methods
(e.g. [1, 30, 61, 70, 84]) and determining when attention can be used as an explanation [84]."
INTRODUCTION,0.008016032064128256,"While these methods focus on information that already exists in the data, either by weighting fea-
tures or concepts in training examples or by selecting important training examples, recent progress
in generative models [12, 29, 36, 40, 45] has lead to another family of explainability methods that
provide explanations by generating new examples or features [14, 32, 68, 73]. New examples or
features can be used to generate counterfactuals [82] allowing users to ask: what if this sample were
to be classiﬁed as the opposite class, and how would it differ? This way of explaining mirrors the"
INTRODUCTION,0.01002004008016032,Published as a conference paper at ICLR 2022
INTRODUCTION,0.012024048096192385,"Figure 1: Examples of explainability methods applied to a melanoma classiﬁer. Explanation by (a) heatmaps
(e.g. [19, 47, 76, 79]), (b) segmentation masks (e.g. [22, 69]), (c) sample retrieval (e.g. [72]), (d) counterfactual
generation (e.g. [68, 73]), (e) and multiple counterfactual generations such as DISSECT. Multiple counter-
factuals could highlight several different ways that changes in a skin lesion could reveal its malignancy and
overcome some of the blind spots of the other approaches. For example, they can demonstrate that large le-
sions, jagged borders, and asymmetrical shapes lead to melanoma classiﬁcation. They can also show potential
biases of the classiﬁer by revealing that surgical markings can spuriously lead to melanoma classiﬁcation."
INTRODUCTION,0.014028056112224449,"way humans reason, justify decisions [3], and learn [4, 6, 83]. Additionally, users across visual,
auditory, and sensor data domains found examples the most preferred means of explanations [31]."
INTRODUCTION,0.01603206412825651,"To illustrate the beneﬁts of counterfactual explanations, consider a dermatology task where an ex-
planation method is used to highlight why a certain sample is classiﬁed as benign/malignant (Fig. 1).
Explanations like heatmaps, saliency maps, or segmentation masks only provide partial information.
Such methods might hint at what is inﬂuential within the sample, potentially focusing on the lesion
area. However, they cannot show what kind of changes in color, texture, or inﬂammation could trans-
form the input at hand from benign to malignant. Retrieval-based approaches that provide examples
that show a concept are not enough for answering “what-if” questions either. A retrieval-based tech-
nique might show input samples of malignant skin lesions that have similarities to a benign lesion
in patient A, but from a different patient B, potentially from another body part or even a different
skin tone. Such examples do not show what this benign lesion in patient A would have to look like
if it were classiﬁed as malignant instead. On the other hand, counterfactuals depict how to modify
the input sample to change its class membership. A counterfactual explanation visualizes what a
malignant tumor could look like in terms of potential color or texture, or inﬂammation changes on
the skin. Better yet, multiple counterfactuals could highlight several different ways that changes in
a skin lesion could reveal its malignancy. For a classiﬁer that relies on surgical markings [86] as
well as meaningful color/texture/size changes to make its decision, a single explanation might fail
to reveal this ﬂaw resulting from dependence on spurious features, but multiple distinct explana-
tions shed light on this phenomenon. Multiple explanations are strongly preferred in several other
domains, such as education, knowledge discovery, and algorithmic recourse generation [17, 54]."
INTRODUCTION,0.018036072144288578,"In this work, we propose a generation-based explainability method called DISSECT that generates
Concept Traversals (CTs)–sequences of generated examples with increasing degrees of concepts’
inﬂuence on a classiﬁer’s decision. While current counterfactual generation techniques fail to sat-
isfy the most consistently agreed-upon properties desired for an explainability method simultane-
ously [50, 60, 68, 73, 74], DISSECT aims to overcome this challenge. CTs are generated by jointly
training a generator, a discriminator, and a CT disentangler to produce examples that (1) express one
distinct factor [50] at a time; (2) are inﬂuential [73, 74] to a classiﬁer’s decision and are coupled to
the classiﬁer’s reasoning, due to joint training; (3) are realistic [73]; (4) preserve relevant informa-
tion [50, 60, 68]; and (5) are stable across similar inputs [21, 50, 60]. Compared to other approaches
that require a human to identify concepts a priori and ﬁnd samples representing them, e.g. [9, 35],
DISSECT uncovers them automatically. We compare DISSECT with several baselines, some of
which have been optimized for disentanglement, some used extensively for explanation, and some
that fall in between. DISSECT is the only technique that performs well across all these dimensions.
Other baselines either have a hard time with inﬂuence, lack ﬁdelity, generate poor quality and un-
realistic samples, or do not disentangle properly. We evaluate DISSECT using 3D Shapes [7],
CelebA [44], and a new synthetic dataset inspired by real-world dermatology challenges. We show
that DISSECT outperforms prior work in addressing all of these challenges. We discuss applications
to detect a classiﬁer’s potential biases and identify spurious artifacts using simulated experiments."
INTRODUCTION,0.02004008016032064,"This paper makes ﬁve main contributions: 1) presents a novel counterfactual explanation approach
that manifests several desirable properties outperforming baselines; 2) demonstrates applications"
INTRODUCTION,0.022044088176352707,Published as a conference paper at ICLR 2022
INTRODUCTION,0.02404809619238477,"through experiments showcasing the effectiveness of this approach for detecting potential biases of
a classiﬁer; 3) presents a set of explainability baselines inspired by approaches used for generative
disentanglement; 4) translates desired properties commonly referred to in the literature across forms
of explanation into measurable quantities for benchmarking and evaluating counterfactual genera-
tion approaches; 5) releases a new synthetic dermatology dataset inspired by real-world challenges."
RELATED WORK,0.026052104208416832,"2
RELATED WORK"
RELATED WORK,0.028056112224448898,"Our method relates to the active research area of post hoc explainability methods. One way to
categorize them is by explanation form. While met with criticisms [1, 75], many feature-based
explainability methods are shown to be useful [47, 79], which assign a weight to each input feature
to indicate their importance in classiﬁcation [10, 57]. Example-based methods are another popular
category [33, 34, 39, 87] that instead assign importance weights to individual examples. More
recently, concept-based methods have emerged that attribute weights to concepts, i.e., higher-level
representations of features [22, 25, 35] such as “long hair”."
RELATED WORK,0.03006012024048096,"Our work leverages recent progress in generative modeling, where the explanation is presented
through several conditional generations [52, 56]. Efforts for the “discovery” of concepts are also
related to learning disentangled representations [12, 29, 35, 36], which has been shown to be chal-
lenging without additional supervision [45]. Recent ﬁndings suggest that weak supervision in the
following forms could make the problem identiﬁable: how many factors of variation have changed
[71], using labels for validation [46], or using sparse labels during training [46]. While some tech-
niques like Conditional Subspace VAE (CSVAE) [38] began to look into conditional disentangle-
ment by incorporating labels, their performance has not yet reached their unconditional counterparts.
Our work uses a new form of weak supervision to improve upon existing methods: the posterior
probability and gradient of the classiﬁer-under-test."
RELATED WORK,0.03206412825651302,"Many explainability methods have emerged that use generative models to modify existing exam-
ples or generate new examples [14, 15, 26, 32, 48, 68, 73]. Most of these efforts use pre-trained
generators and generate a new example by moving along the generator’s embedding. Some aim
to generate samples that would ﬂip the classiﬁer’s decision [15, 32], while others aim to modify
particular attribution of the image and observe the classiﬁer’s decision change [14]. More recent
work allows the classiﬁer’s predicted probabilities or gradients to ﬂow through the generator dur-
ing training [68], and visualize the continuous progression of a key differential feature [73]. This
progressive visualization has additional beneﬁts, as Cai et al. [9] showed that pathologists could
iterate more quickly using a tool that allowed them to progressively change the value of a concept,
compared to purely using examples. Additionally, continuous exaggeration of a concept facilitated
thinking more systematically about the speciﬁc features important for diagnosis."
RELATED WORK,0.03406813627254509,"Most of these approaches assume that there is only one path that crosses the decision boundary,
and they generate examples along that path. Our work leverages both counterfactual generation
techniques and ideas from the disentanglement literature to generate diverse sets of explanations via
multiple distinct paths inﬂuential to the classiﬁer’s decision, called Concept Traversals (CT). Each
CT is made of a sequence of generated examples that express increasing degrees of a concept."
METHODS,0.036072144288577156,"3
METHODS"
METHODS,0.03807615230460922,"Our key innovation is developing an approach to provide multiple potential counterfactuals. We
show how to successfully incorporate the classiﬁer’s signal while disentangling relevant concepts.
We review existing generative models for building blocks of our approach, and follow Singla et al.
[73] for DISSECT’s backbone. This choice is informed by the good conditional generation capa-
bility of [73] and being a strong interpretability baseline. We then introduce a novel component
for unsupervised disentanglement of explanations. Multiple counterfactual explanations could re-
veal several different paths that ﬂip the classiﬁcation outcome and shed light on different biases of
the classiﬁer. However, a single explanation might fail to reveal such information. Disentangling
distinct concepts without relying on predeﬁned user labels reduces the burden on the user side. Ad-
ditionally, it can surface concepts that the user might have missed otherwise. However, unsupervised
disentanglement is particularly challenging [45]. To address this challenge, we design a disentan-
gler network. The disentangler, along with the generator, indirectly enforces different concepts to"
METHODS,0.04008016032064128,Published as a conference paper at ICLR 2022
METHODS,0.04208416833667335,"be as distinct as possible. This section summarizes our ﬁnal design choices. For more details about
baselines, DISSECT, and ablations studies, see §A.1, §A.2, and §A.6, respectively."
METHODS,0.04408817635270541,"Notation. Without loss of generality, we consider a binary classiﬁer f : X →Y = {−1, 1} such
that f(x) = p(y = 1|x) where x ∼PX. We want to ﬁnd K latent concepts that contribute to the
decision-making of f. Given x, α ∈[0, 1], and k, we want to generate an image, ¯x, by perturbing
the latent concept k, such that the posterior probability f(¯x) = α. In addition, when conditioning on
x and a concept k, by changing α, we hope for a smooth change in the output image ¯x. This smooth-
ness resembles slightly changing the degree of a concept. Putting it together, we desire a generic gen-
eration function that generates ¯x, deﬁned as G(x, α, k; f) : X × [0, 1] × {0, 1, . . . , K −1} →X,
where f(G(x, α, k; f)) ≈α. The generation function G conditions on x and manipulates the con-
cept k such that a monotonic change in the f(G(x, 0, k)), . . . , f(G(x, 1, k)) is achieved."
ENCODER-DECODER ARCHITECTURE,0.04609218436873747,"3.1
ENCODER-DECODER ARCHITECTURE"
ENCODER-DECODER ARCHITECTURE,0.04809619238476954,"We realize the generic (conditional) generation process using an encoder-decoder framework. The
input x, is encoded into an embedding E(x), before feeding into the generator G, which serves
as a decoder. In addition to E(x), we have a conditional input c(α, k) for the kth latent concept
we are going to manipulate with level α. α is the desired prediction of f we want to achieve in the
generated sample by perturbing the kth latent concept. The generative process G can be implemented
by G(E(x), c(α, k)), where we can manipulate c for interpretation via conditional generation."
ENCODER-DECODER ARCHITECTURE,0.050100200400801605,"There are many advances in generative models, which can be adopted to train G(E(x), c(α, k)). For
example, Variational Autoencoders (VAE) explicitly designed for disentanglement [11, 29, 38, 41].
However, VAE-based approaches often generate blurry images which requires customized architec-
ture designs [63, 81]. In contrast, GANs [24] tend to generate high-quality images. Therefore, we
consider an encoder-decoder design with GANs [5, 55, 58, 64, 66, 89] to have ﬂexible controllability
and high quality generation for explainability."
ENCODER-DECODER ARCHITECTURE,0.052104208416833664,"There are different design choices for realizing the conditioning code c. A straightforward option is
multiplying a K-dimensional one-hot vector, which speciﬁes the desired concept k, with the proba-
bility α. However, conditioning on continuous variables is usually challenging for conditional GANs
compared with their discretized counterparts [52]. Therefore, we discretize α into [0, 1/N, . . . , 1],
and parametrize c as a binary matrix [0, 1](N+1)×K for N + 1 discrete values and K concepts, fol-
lowing Singla et al. [73]. Therefore, c(α, k) denotes that we set the elements (m, k) to be 1, where
m is an integer and m/N ≤α < (m + 1)/N. See § A.2 for more details."
ENCODER-DECODER ARCHITECTURE,0.05410821643286573,"We include the following elements in our objective function following Singla et al. [73], and further
modify them to support multiple concepts:"
ENCODER-DECODER ARCHITECTURE,0.056112224448897796,Interpreting classiﬁers. We align the generator outputs with the classiﬁer being interpreted 1:
ENCODER-DECODER ARCHITECTURE,0.05811623246492986,"Lf(G) = α log

f
 
G (x, c(α, k))

+

1 −α

log

1 −f
 
G (x, c(α, k))

.
(1)
Reconstruction. We adopt ℓ1 reconstruction loss in our encoder-decoder design [55, 89]. Given x ∼PX and
classiﬁer f, the ground-truth α for all concepts is f(x):"
ENCODER-DECODER ARCHITECTURE,0.06012024048096192,"Lrec(G) = 1 K K−1
X k=0"
ENCODER-DECODER ARCHITECTURE,0.06212424849699399,"x −G
 
x, c (f(x), k)

1.
(2)"
ENCODER-DECODER ARCHITECTURE,0.06412825651302605,"Cycle consistency. Deﬁne ¯xα,k = G(x, c(α, k)). We add a cycle consistency loss [89]:"
ENCODER-DECODER ARCHITECTURE,0.06613226452905811,"Lcyc(G) = 1 K K−1
X"
ENCODER-DECODER ARCHITECTURE,0.06813627254509018,"k=0
Eα"
ENCODER-DECODER ARCHITECTURE,0.07014028056112225,"""x −G
 
¯xα,k, c (f(x), k)

1 # .
(3)"
ENCODER-DECODER ARCHITECTURE,0.07214428857715431,"GAN loss. We use spectral normalization [53] with its hinge loss variant [52] to compare the distribution
x ∼PX and the generated data:2"
ENCODER-DECODER ARCHITECTURE,0.07414829659318638,LcGAN(D) = −Ex∼PX
ENCODER-DECODER ARCHITECTURE,0.07615230460921844,"
min

0, −1 + D(x)

−Ex∼PX Eα,k"
ENCODER-DECODER ARCHITECTURE,0.0781563126252505,"
min

0, −1 −D
 
G (x, c(α, k))

, (4)"
ENCODER-DECODER ARCHITECTURE,0.08016032064128256,"LcGAN(G) = −Ex∼PX Eα,k"
ENCODER-DECODER ARCHITECTURE,0.08216432865731463,"
D
 
G (x, c(α, k))

.
(5)"
ENCODER-DECODER ARCHITECTURE,0.0841683366733467,"1Note that G is trained having access to f’s signal. The LogLoss between the desired probability α and
the predicted probability of the perturbed image is minimized. Thus, the model supports ﬁdelity by design.
2One can use any GAN loss [2, 24, 27, 42, 53, 88]. Generated data comes from uniformly sampling k & α."
ENCODER-DECODER ARCHITECTURE,0.08617234468937876,Published as a conference paper at ICLR 2022
ENFORCING DISENTANGLEMENT,0.08817635270541083,"3.2
ENFORCING DISENTANGLEMENT"
ENFORCING DISENTANGLEMENT,0.09018036072144289,"Ideally, we want the K concepts to capture distinctively different qualities inﬂuential to the classiﬁer
being explained. Though G is capable of expressing K different concepts, the above formulation
does not enforce distinctness among them. To address this challenge, we introduce a concept disen-
tangler R inspired by the disentanglement literature [8, 11, 12, 29, 38, 41, 43]. Most disentangle-
ment measures quantify proxies of distinctness of changes across latent dimensions and similarity
of changes within a latent dimension [45]. This is usually achieved through latent code traversal,
keeping a particular dimension ﬁxed, or keeping all but one dimension ﬁxed. Thus, we design R to
encourage distinctness across concepts and similarity within a concept."
ENFORCING DISENTANGLEMENT,0.09218436873747494,"The K concepts can be viewed as “knobs” that can modify a sample along a conceptual dimension.
Without loss of generality, we assume modifying only one “knob” at a time. Given a pair of images
(x, x′), R tries to predict which concept k, k ∈{0, . . . , K −1}, has perturbed query x to produce
x′. To formalize this, let ¯xk,α = G(x, c(α, k)) and ˜xk = G(¯xk,α, c(f(x), k)). The loss function is"
ENFORCING DISENTANGLEMENT,0.09418837675350701,"Lr(G, R) = CE(ek, R(x, ¯xk)) + CE(ek, R(¯xk, ˜xk)),
(6)"
ENFORCING DISENTANGLEMENT,0.09619238476953908,"where CE is the cross entropy and ek is a one-hot vector of size K for the kth concept. Note that
while the kth “knob” can modify a sample query with α degree, α ∈[0, 1/N, . . . , 1], R tries to
solve a multi-class binary prediction problem. Thus, for any α, α ̸= f(x), R(x, ¯xk) is trained to
predict a one-hot vector for the kth concept. Furthermore, note that no external labels are required
to distinguish between concepts. Instead, by jointly optimizing R and G, we penalize the generator
G if any concept knob results in similar-looking samples to other concept knobs. Encouraging
noticeably distinct changes across concept knobs indirectly promotes disentanglement. In summary,
the overall objective function of our method is:"
ENFORCING DISENTANGLEMENT,0.09819639278557114,"min
G,R max
D λcGANLcGAN(D, G) + λfLf(D, G) + λrecLrec(G) + λrecLcyc(G) + λrLr(G, R).
(7)"
ENFORCING DISENTANGLEMENT,0.10020040080160321,We call our approach DISSECT as it disentangles simultaneous explanations via concept traversals.
EXPERIMENTS,0.10220440881763528,"4
EXPERIMENTS"
EXPERIMENTS,0.10420841683366733,"3D Shapes. We ﬁrst use 3D Shapes [7] purely for validation and demonstration purposes due
to the controllability of all its factors. It is a synthetic dataset available under Apache License
2.0 composed of 480K 3D shapes procedurally generated from 6 ground-truth factors of variation.
These factors are ﬂoor hue, wall hue, object hue, scale, shape, and orientation."
EXPERIMENTS,0.1062124248496994,"SynthDerm. Second, we create a new dataset, SynthDerm (Fig. 2). Real-world characteris-
tics of melanoma skin lesions in dermatology settings inspire the generation of this dataset [80].
These characteristics include whether the lesion is asymmetrical, its border is irregular or jagged,
is unevenly colored, has a diameter more than 0.25 inches, or is evolving in size, shape, or color
over time. These qualities are usually referred to as the ABCDE of melanoma [65]. We generate
SynthDerm algorithmically by varying several factors: skin tone, lesion shape, lesion size, lesion
location (vertical and horizontal), and whether there are surgical markings present. We randomly
assign one of the following to the lesion shape: round, asymmetrical, with jagged borders, or multi-
colored (two different shades of colors overlaid with salt-and-pepper noise). For skin tone values,
we simulate Fitzpatrick ratings [20]. Fitzpatrick scale is a commonly used approach to classify the
skin by its reaction to sunlight exposure modulated by the density of melanin pigments in the skin.
This rating has six values, where 1 represents skin with lowest melanin and 6 represents skin with
highest melanin. For our synthetic generation, we consider six base skin tones that similarly resem-
ble different amounts of melanin. We also add a small amount of random noise to the base color to
add further variety. SynthDerm includes more than 2,600 64x64 images."
EXPERIMENTS,0.10821643286573146,"CelebA. We also include the CelebA dataset [44] that is available for non-commercial research
purposes. This dataset includes “identities [of celebrities] collected from the Internet” [78], is re-
alistic, and closely resembles real-world settings where the attributes are nuanced and not mutually
independent. CelebA includes more than 200K images with 40 annotated face attributes, such as
smiling and bangs."
EXPERIMENTS,0.11022044088176353,Published as a conference paper at ICLR 2022 I II III IV V VI
EXPERIMENTS,0.11222444889779559,Benign
EXPERIMENTS,0.11422845691382766,Asymmetrical
EXPERIMENTS,0.11623246492985972,"Asymmetrical with 
surgical markings"
EXPERIMENTS,0.11823647294589178,Jagged borders
EXPERIMENTS,0.12024048096192384,Uneven colors
EXPERIMENTS,0.12224448897795591,"Larger than 0.25"""
EXPERIMENTS,0.12424849699398798,"Fitzpatrick Scale Samples
Fitzpatrick Scale Samples
Characteristics
Characteristics"
EXPERIMENTS,0.12625250501002003,"Figure 2: Illustration of SynthDerm dataset that we algorithmically generated. Fitzpatrick scale of skin
classiﬁcation based on melanin density and corresponding examples in the dataset are visualized."
BASELINES,0.1282565130260521,"4.1
BASELINES"
BASELINES,0.13026052104208416,"We consider several baselines. First, we modify a set of VAEs explicitly designed for disentan-
glement [8, 11, 29, 41] to incorporate the classiﬁer’s signal during their training processes and en-
courage the generative model to learn latent dimensions that inﬂuence the classiﬁer, i.e., learning
Important CTs. While these approaches are not naturally designed to explain an external classiﬁer,
they have been designed to ﬁnd distinct factors of variation. We refer to these baselines with a -
mod postﬁx, e.g. β-VAE-mod. Second, we include CSVAE [38] that aims to solve unsupervised
learning of features associated with a speciﬁc label using a low-dimensional latent subspace that can
be independently manipulated. CSVAE, without any modiﬁcation, is the closest alternative in the
literature that tries to discover both Important and Distinct CTs. Third, we include Explanation by
Progressive Exaggeration (EPE) [73], a GAN-based approach that learns to generate one series of
counterfactual and realistic samples that change the prediction of f, given data and the classiﬁer’s
signal. EPE is the most similar baseline to DISSECT in terms of design choices; however, it is
not designed to handle distinct CTs. We introduce EPE-mod by modifying EPE to allow learning
multiple pathways by making the generator conditional on the CT dimension (more details in §A.1)."
EVALUATION METRICS,0.13226452905811623,"4.2
EVALUATION METRICS"
EVALUATION METRICS,0.1342685370741483,"This section provides a brief summary of our evaluation metrics. We consider Importance, Real-
ism, Distinctness, Substitutability, and Stability, which commonly appear as desirable qualities in
explainability literature.3 See more in §A.3."
EVALUATION METRICS,0.13627254509018036,"Importance. Explanations should produce the desired outcome from the black-box classiﬁer f. Pre-
vious work has referred to this quality using different terms, such as importance [22], compatibility
with classiﬁer [73], and classiﬁcation model consistency [74]. While most previous methods have
relied on visual inspection, we introduce a quantitative metric to measure the gradual increase of the
target class’s posterior probability through a CT. Notably, we compute the correlation between α and
f(I(x, α, k; f)) introduced in § 3. For brevity, we refer to f(I(x, α, k; f)) as f(¯x) in the remain-
der of the paper. We also report the mean-squared error and the Kullback–Leibler (KL) divergence
between α and f(¯x). We also calculate the performance of f on the counterfactual explanations.
Speciﬁcally, we replace the test set of real images with their generated counterfactual explanations
and quantify the performance of the pre-trained black-box classiﬁer f on the counterfactual test set.
Better performance suggests that counterfactual samples are compatible with f and lie on the cor-
rect side of the classiﬁer’s boundary. Note that Importance scores can be calculated per concept and
provide a ranking across concepts, or they can be aggregated to summarize overal performance. See
§A.7 for more information about concept ranking using individial concept Importance scores.
Realism. We need the generated samples that form a CT to look realistic to enable users to identify
concepts they represent. It means the counterfactual explanations should lie on the data manifold.
This quality has been referred to as realism or data consistency [73]. We use Fr´echet Inception
Distance (FID) [28] as a measure of Realism. Additionally, inspired by [18], we train a post hoc
classiﬁer that predicts whether a sample is real or generated. Note that it is essential to do this step
post hoc and independent from the training procedure because relying on the discriminator’s accu-
racy in an adversarial training framework can be misleading [18].
Distinctness. Another desirable quality for explanations is to represent inputs with non-overlapping
concepts, often referred to as diversity [50]. Others have suggested similar properties such as co-"
EVALUATION METRICS,0.13827655310621242,"3Eq. 1 encourages Importance, Eq. 5 encourages Realism, Eq. 6 encourages Distinctness, Eq. 2 and Eq. 3
indirectly help with Stability. All the elements of Eq. 7 contribute to improving Substitutability."
EVALUATION METRICS,0.1402805611222445,Published as a conference paper at ICLR 2022
EVALUATION METRICS,0.14228456913827656,"𝑓(𝑥) = 0.00
𝑓(𝑥) = 1.00
𝑓(𝑥) = 1.00
𝑓(𝑥) = 1.00"
EVALUATION METRICS,0.14428857715430862,"𝑓(𝑥) = 1.00
𝑓(𝑥) = 1.00"
EVALUATION METRICS,0.1462925851703407,"𝑓(𝑥) = 0.00
𝑓(𝑥) = 1.00
Query
EPE"
EVALUATION METRICS,0.14829659318637275,"𝑓(𝑥) = 1.00
EPE-mod"
EVALUATION METRICS,0.15030060120240482,"𝑓(𝑥) = 1.00
DISSECT"
EVALUATION METRICS,0.1523046092184369,"𝑓(𝑥) = 1.00
𝑓(𝑥) = 1.00"
EVALUATION METRICS,0.15430861723446893,"𝐶𝑇*, ,-*.. N/A"
EVALUATION METRICS,0.156312625250501,"𝐶𝑇*, ,-*.."
EVALUATION METRICS,0.15831663326653306,"𝐶𝑇/, ,-*.."
EVALUATION METRICS,0.16032064128256512,"𝐶𝑇/, ,-*..
N/A"
EVALUATION METRICS,0.1623246492985972,"Query
EPE
EPE-mod
DISSECT"
EVALUATION METRICS,0.16432865731462926,"𝑓(𝑥) = 0.00
Query"
EVALUATION METRICS,0.16633266533066132,"𝑓(𝑥) = 1.00
EPE-mod"
EVALUATION METRICS,0.1683366733466934,"𝑓(𝑥) = 1.00
DISSECT"
EVALUATION METRICS,0.17034068136272545,"𝑓(𝑥) = 1.00
𝑓(𝑥) = 1.00"
EVALUATION METRICS,0.17234468937875752,"𝐶𝑇*, ,-*.."
EVALUATION METRICS,0.1743486973947896,"𝐶𝑇/, ,-*.."
EVALUATION METRICS,0.17635270541082165,𝑓(𝑥) = 1.00 N/A EPE
EVALUATION METRICS,0.17835671342685372,"Figure 3: Examples from 3D Shapes. EPE and EPE-mod converge to ﬁnding the same concept, despite
EPE-mod’s ability to express multiple pathways to switch classiﬁer outcomes from False to True. DISSECT
discovers the two ground-truth concepts: CT1 ﬂips the ﬂoor color to cyan and CT2 ﬂips the shape color to red."
EVALUATION METRICS,0.18036072144288579,"Table 1: Quantitative results on 3D Shapes. DISSECT performs signiﬁcantly better or on par with the
strongest baselines in all evaluation categories. Modiﬁed VAE variants perform poorly in terms of Impor-
tance, worse than CSVAE, and signiﬁcantly worse than EPE, EPE-mod, and DISSECT. CSVAE and other
VAE variants do not produce high-quality images, thus have poor Realism scores; meanwhile, EPE, EPE-mod,
and DISSECT generate realistic samples indistinguishable from real images. While the aggregated metrics
for Importance are useful for discarding VAE baselines with poor performance, they do not show a consistent
order across EPE, EPE-mod, and DISSECT. Our approach greatly improves Distinctness, especially compared
to EPE-mod. EPE is inherently incapable of doing this, and the extension EPE-mod does, but poorly. For the
Substitutability scores, note the classiﬁer’s precision, recall, and accuracy when training on actual data is 100%.
(*Some methods only generate samples with f(¯x) = 0.0. Correlation with a constant value is undeﬁned.)"
EVALUATION METRICS,0.18236472945891782,"Importance
Realism
Distinctness
Substitutability
Stability"
EVALUATION METRICS,0.1843687374749499,"↑R
↑ρ
↓KL
↓MSE
↑CF
Acc"
EVALUATION METRICS,0.18637274549098196,"↑CF
Prec"
EVALUATION METRICS,0.18837675350701402,"↑CF
Rec
↓Acc
↓Prec
↓Rec
↑Acc
↑Prec
(micro)"
EVALUATION METRICS,0.1903807615230461,"↑Prec
(macro)"
EVALUATION METRICS,0.19238476953907815,"↑Rec
(micro)"
EVALUATION METRICS,0.19438877755511022,"↑Rec
(macro)"
EVALUATION METRICS,0.1963927855711423,"↑Acc
Sub"
EVALUATION METRICS,0.19839679358717435,"↑Prec
Sub"
EVALUATION METRICS,0.20040080160320642,"↑Rec
Sub"
EVALUATION METRICS,0.20240480961923848,"↓CF
MSE"
EVALUATION METRICS,0.20440881763527055,"↓Prob
JSD
VAE-mod
0.1
0.3
inf
0.42
50.0
0
0
94.9
92.1
99.6
86.3
95.1
95.3
80.6
80.6
14.7
14.2
69.4
0.151
0.0000
β-VAE-mod
0.0
0.0
inf
0.42
50.0
0
0
99.5
99.0
100
90.7
92.2
92.2
91.0
91.0
42.5
8.2
19.8
0.197
0.0000
Annealed-VAE-mod
N/A*
N/A*
inf
0.42
50.0
0
0
100
100
100
34.0
53.2
49.2
1.20
1.2
35.8
14.4
48.1
0.175
0.0000
DIPVAE-mod
0.1
0.5
inf
0.41
52.3
100
4.6
100
100
100
97.2
96.7
96.9
96.5
96.5
19.0
19.0
100
0.127
0.0001
βTCVAE-mod
0.5
0.7
inf
0.42
50.0
0
0
100
100
100
100
100
100
100
100
36.4
21.3
87.3
0.143
0.0000
CSVAE
0.3
0.3
5.5
0.28
64.6
100
29.3
100
100
100
71.4
74.7
76.4
87.2
87.2
47.0
23.8
81.3
19.544
0.0274
EPE
0.8
0.8
1.54
0.09
98.4
100
96.7
50.1
0
0
-
-
-
-
-
99.2
95.9
100
0.134
0.0004
EPE-mod
0.9
0.7
2.2
0.08
99.7
100
99.4
49.3
0
0
45.3
49.6
49.8
30.3
30.3
91.0
100
52.5
0.128
0.0002
DISSECT
0.8
0.8
1.61
0.08
98.7
100
97.5
49.3
0
0
100
100
100
100
100
100
99.7
100
0.102
0.0003"
EVALUATION METRICS,0.20641282565130262,"herency, meaning examples of a concept should be similar but different from examples of other
concepts [22]. To quantify this in a counterfactual setup, we introduce a distinctness measure cap-
turing the performance of a secondary classiﬁer that distinguishes between CTs. We train a classiﬁer
post hoc that given a query image x, a generated image x′, and K number of CTs, predicts CTk that
has perturbed x to produce x′, k ∈{1, . . . , K}.
Substitutability. The representation of a sample in terms of concepts should preserve relevant in-
formation [50, 60]. Previous work has formalized this quality for counterfactual generation contexts
through a proxy called substitutability [68]. Substitutability measures an external classiﬁer’s perfor-
mance on real data when it is trained using only synthetic images.4 A higher substitutability score
suggests that explanations have retained relevant information and are of high quality.
Stability.
Explanations should be coherent for similar inputs, a quality known as stability
[21, 50, 60]. To quantify stability in counterfactual explanations, we augment the test set with
additive random noise to each sample x and produce S randomly augmented copies x
′′
i . Then, we
generate counterfactual explanations ¯x and ¯x
′′
i , respectively. We calculate the mean-squared dif-
ference between counterfactual images ¯x and ¯x
′′
i and the resulting Jensen Shannon distance (JSD)
between the predicted probabilities f(¯x) and f(¯x
′′
i )."
EVALUATION METRICS,0.20841683366733466,"4.3
CASE STUDY I: VALIDATING THE QUALITIES OF CONCEPT TRAVERSALS"
EVALUATION METRICS,0.21042084168336672,"Considering 3D Shapes [7], we deﬁne an image as “colored correctly” if the shape hue is red or
the ﬂoor hue is cyan. We train a classiﬁer to detect whether a sample image is “colored correctly”
or not. In this synthetic experiment, only these two independent factors contribute to the decision
of this classiﬁer. Given a not “colored correctly” query, we would like the algorithm to ﬁnd a CT
related to the shape color and another CT associated with the ﬂoor color–two different pathways that
lead to switching the classiﬁer outcome.5"
EVALUATION METRICS,0.2124248496993988,"4This metric has been used in other contexts outside of explainability and has been called Classiﬁcation
Accuracy Score (CAS) [62]. CAS is more broadly applicable than Fr´echet Inception Distance [28] & Inception
Score [67] that are only useful for GAN evaluation.
5In this scenario, these two ground-truth concepts do not directly apply to switching the classiﬁer outcome
from True to False. For example, if an image has a red shape and a cyan ﬂoor, both of these colors need to be
changed to switch the classiﬁcation outcome. We still observe that DISSECT ﬁnds CTs that change different
combinations of colors, but the baseline methods converge to the same CT. See §A.5.1 for more details."
EVALUATION METRICS,0.21442885771543085,Published as a conference paper at ICLR 2022
EVALUATION METRICS,0.21643286573146292,"EPE-mod
DISSECT
EPE-mod
DISSECT"
EVALUATION METRICS,0.218436873747495,"Query
FP III"
EVALUATION METRICS,0.22044088176352705,"EPE-mod
DISSECT Query"
EVALUATION METRICS,0.22244488977955912,"FP V
Query FP I"
EVALUATION METRICS,0.22444889779559118,Asymmetry
EVALUATION METRICS,0.22645290581162325,Border Color
EVALUATION METRICS,0.22845691382765532,Diameter Color
EVALUATION METRICS,0.23046092184368738,Surgical Markings
EVALUATION METRICS,0.23246492985971945,& Color
EVALUATION METRICS,0.23446893787575152,No change
EVALUATION METRICS,0.23647294589178355,No change
EVALUATION METRICS,0.23847695390781562,Border*
EVALUATION METRICS,0.24048096192384769,Border*
EVALUATION METRICS,0.24248496993987975,Color*
EVALUATION METRICS,0.24448897795591182,Color*
EVALUATION METRICS,0.24649298597194388,"Figure 4: Examples from SynthDerm comparing DISSECT with the strongest baseline, EPE-mod. We
illustrate three queries with different Fitzpatrick ratings [20] and visualize the two most prominent concepts
for each technique. We observe that EPE-mod converges on a single concept that only vaguely represents
meaningful ground-truth concepts. However, DISSECT successfully ﬁnds concepts describing asymmetrical
shapes, jagged borders, and uneven colors that align with the ABCDE of melanoma [65]. DISSECT also
identiﬁes concepts for surgical markings that spuriously impact the classiﬁer’s decisions."
EVALUATION METRICS,0.24849699398797595,"Table 2: Quantitative results on SynthDerm. DISSECT performs consistently best in all categories. For
anchoring the Substitutability scores, note that the precision, recall, and accuracy of the classiﬁer when training
on actual data is 97.7%, 100.0%, and 95.4%, respectively."
EVALUATION METRICS,0.250501002004008,"Importance
Realism
Distinctness
Substitutability
Stability"
EVALUATION METRICS,0.25250501002004005,"↑R
↑ρ
↓KL
↓MSE
↑CF
Acc"
EVALUATION METRICS,0.2545090180360721,"↑CF
Prec"
EVALUATION METRICS,0.2565130260521042,"↑CF
Rec
↓FID
↓Acc
↓Prec
↓Rec
↑Acc
↑Prec
(micro)"
EVALUATION METRICS,0.25851703406813625,"↑Prec
(macro)"
EVALUATION METRICS,0.2605210420841683,"↑Rec
(micro)"
EVALUATION METRICS,0.2625250501002004,"↑Rec
(macro)"
EVALUATION METRICS,0.26452905811623245,"↑Acc
Sub"
EVALUATION METRICS,0.2665330661322645,"↑Prec
Sub"
EVALUATION METRICS,0.2685370741482966,"↑Rec
Sub"
EVALUATION METRICS,0.27054108216432865,"↓CF
MSE"
EVALUATION METRICS,0.2725450901803607,"↓Prob
JSD
CSVAE
0.25
0.64
1.78
0.12
86.7
43.9
5.2
52.6
54.6
69.9
16.3
85.1
0
0
0
0
29.9
36.8
55.4
2.318
0.006
EPE
0.87
0.23
inf
0.03
80.7
55.1
86.9
23.8
50.1
0
0
-
-
-
-
-
74.4
83.8
60.7
0.111
0.001
EPE-mod
0.81
0.73
0.92
0.04
95.3
83.9
79.5
24.3
50.0
0
0
85.1
71.1
26.3
0.7
0.7
74.3
83.5
60.7
0.239
0.002
DISSECT
0.92
0.75
0.35
0.02
97.8
92.3
91.1
27.5
50.0
0
0
96.0
96.5
96.5
74.6
74.6
81.0
97.2
64.0
0.338
0.001"
EVALUATION METRICS,0.2745490981963928,"Tab. 1 summarizes the quantitative results on 3D Shapes. Most VAE variants perform poorly in
terms of Importance, CSVAE performs slightly better, and EPE, EPE-mod, and DISSECT perform
best. Our results suggest that DISSECT performs similarly to EPE that has been geared explic-
itly toward exhibiting Importance and its extension, EPE-mod. Additionally, DISSECT still keeps
Realism intact. Also, it notably improves the Distinctness of CTs compared to relevant baselines."
EVALUATION METRICS,0.27655310621242485,"Fig. 3 shows the examples illustrating the qualitative results for EPE, EPE-mod, and DISSECT.
Our results reveal that EPE converges to ﬁnding only one of these concepts. Similarly, both CTs
generated by EPE-mod converge to ﬁnding the same concept, despite being given the capability to
explore two pathways to switch the classiﬁer outcome. However, DISSECT ﬁnds the two distinct
ground-truth concepts through its two generated CTs. This is further evidence for the necessity
of the disentanglement loss to enforce the distinctness of discovered concepts. For brevity, three
sample queries are visualized (See more in §A.5.1). Note that given the synthetic nature of this
dataset, the progressive paths may not be as informative. Thus, we only visualize the far end of
the spectrum, i.e., the example that ﬂips the classiﬁcation outcome all the way. See §4.5 for the
progressive exaggeration of concepts in a more nuanced experiment."
EVALUATION METRICS,0.2785571142284569,"4.4
CASE STUDY II: IDENTIFYING SPURIOUS ARTIFACTS"
EVALUATION METRICS,0.280561122244489,"A “high performance” model could learn to make its decisions based on irrelevant features that only
happen to correlate with the desired outcome, known as label leakage [85]. One of the applica-
tions of DISSECT is to uncover such spurious concepts and allow probing a black-box classiﬁer.
Motivated by real-world examples that revealed classiﬁer dependency on surgical markings in iden-
tifying melanoma [86], we design this experiment. Given the synthetic nature of SynthDerm and
how it has been designed based on real-world characteristics of melanoma [65, 80], each sample
has a deterministic label of melanoma or benign. If the image is asymmetrical, has jagged borders,
has different colors represented by salt-and-pepper noise, or has a large diameter (i.e., does not ﬁt
in a 40×40 square), the sample is melanoma. Otherwise, the image represents the benign class.
Similar to in-situ dermatology images, melanoma samples have surgical markings more frequently
than benign samples. We train a classiﬁer to distinguish melanoma vs. benign lesion images."
EVALUATION METRICS,0.28256513026052105,"Given a benign query, we would like to produce counterfactual explanations that depict how to
modify the input sample to change its class membership. We want DISSECT to ﬁnd CTs that
disentangle meaningful characteristics of melanoma identiﬁcation in terms of color, texture, or shape
[65], and identify potential spurious artifacts that impact the classiﬁer’s predictions."
EVALUATION METRICS,0.2845691382765531,"Tab. 2 summarizes the quantitative results on SynthDerm. Our method performs consistently well
across all the metrics, signiﬁcantly boosting Distinctness and Substitutability scores and making
meaningful improvements on Importance scores. Our approach has higher performance compared"
EVALUATION METRICS,0.2865731462925852,"Published as a conference paper at ICLR 2022 𝐶𝑇! 𝐶𝑇"""
EVALUATION METRICS,0.28857715430861725,"𝑓(𝑥) = 0.0
Query Image"
EVALUATION METRICS,0.2905811623246493,"EPE-mod
DISSECT"
EVALUATION METRICS,0.2925851703406814,"𝑓(𝑥) = 0.00
𝑓(𝑥) = 0.03
𝑓(𝑥) = 0.38
𝑓(𝑥) = 0.89
𝑓(𝑥) = 1.00"
EVALUATION METRICS,0.29458917835671344,"𝑓(𝑥) = 0.00
𝑓(𝑥) = 0.02
𝑓𝑥= 0.37
𝑓(𝑥) = 0.90
𝑓(𝑥) = 1.00"
EVALUATION METRICS,0.2965931863727455,"𝑓(𝑥) = 0.00
𝑓(𝑥) = 0.07
𝑓(𝑥) = 0.38
𝑓(𝑥) = 0.81
𝑓(𝑥) = 1.00"
EVALUATION METRICS,0.2985971943887776,"𝑓(𝑥) = 0.00
𝑓(𝑥) = 0.08
𝑓(𝑥) = 0.46
𝑓(𝑥) = 0.75
𝑓(𝑥) = 1.00"
EVALUATION METRICS,0.30060120240480964,Correlated Ground-truth
EVALUATION METRICS,0.3026052104208417,Concepts:
EVALUATION METRICS,0.3046092184368738,"Bangs
Blond Hair
Classifying:"
EVALUATION METRICS,0.3066132264529058,Smiling
EVALUATION METRICS,0.30861723446893785,"Figure 5: Examples from CelebA. A biased classiﬁer has been trained to predict smile probability, where a
training dataset was sub-sampled so that smiling co-occurs only with “bangs” and “blond hair” attributes. EPE
does not support multiple CTs. EPE-mod converges on the same concept, despite having the ability to express
various pathways to change f(¯x) through CT1 and CT2. However, DISSECT discovers distinct pathways: CT1
mainly changes hair color to blond, and CT2 does not alter hair color but tries to add bangs."
EVALUATION METRICS,0.3106212424849699,"Table 3: Quantitative results on CelebA. DISSECT performs better than or on par with the baselines in all
categories. Notably, DISSECT greatly improves the Distinctness of CTs and achieves a higher Realism score,
suggesting disentangling CTs does not diminish the quality of generated images and may even improve them.
The classiﬁer’s precision, recall, accuracy when training on actual data is 95.4%, 98.6%, 92.7%, respectively."
EVALUATION METRICS,0.312625250501002,"Importance
Realism
Distinctness
Substitutability
Stability"
EVALUATION METRICS,0.31462925851703405,"↑R
↑ρ
↓KL
↓MSE
↑CF
Acc"
EVALUATION METRICS,0.3166332665330661,"↑CF
Prec"
EVALUATION METRICS,0.3186372745490982,"↑CF
Rec
↓FID
↓Acc
↓Prec
↓Rec
↑Acc
↑Prec
(micro)"
EVALUATION METRICS,0.32064128256513025,"↑Prec
(macro)"
EVALUATION METRICS,0.3226452905811623,"↑Rec
(micro)"
EVALUATION METRICS,0.3246492985971944,"↑Rec
(macro)"
EVALUATION METRICS,0.32665330661322645,"↑Acc
Sub"
EVALUATION METRICS,0.3286573146292585,"↑Prec
Sub"
EVALUATION METRICS,0.3306613226452906,"↑Rec
Sub"
EVALUATION METRICS,0.33266533066132264,"↓CF
MSE"
EVALUATION METRICS,0.3346693386773547,"↓Prob
JSD
CSVAE
0.00
0.00
1.25
0.284
50.2
50.2
34.1
99.7
100
99.5
199.4
15.1
0.0
0.0
0.0
0.0
52.8
53.0
98.6
23.722
0.039
EPE
0.85
0.91
0.28
0.060
99.2
99.9
98.5
13.2
49.9
32.0
0.3
-
-
-
-
-
93.0
95.4
91.2
0.411
0.003
EPE-mod
0.86
0.90
0.21
0.048
99.5
99.7
99.2
13.3
49.3
33.3
0.1
18.7
50.0
50.1
15.2
15.2
91.8
94.8
89.4
0.446
0.004
DISSECT
0.84
0.88
0.19
0.047
99.2
99.8
98.5
11.1
49.2
0.0
0.0
95.0
98.0
98.1
96.1
96.1
91.9
96.9
87.6
0.567
0.005"
EVALUATION METRICS,0.3366733466933868,"to EPE-mod and EPE baselines and substantially improves upon CSVAE. Our method’s high Dis-
tinctness and Substitutability scores show that DISSECT covers the landscape of potential concepts
very well and retains the variety seen in real images strongly better than all the other baselines."
EVALUATION METRICS,0.33867735470941884,"Fig. 4 illustrates a few examples to showcase DISSECT’s improvements over the strongest baseline,
EPE-mod. EPE-mod converges to ﬁnding a single concept that only vaguely represents meaningful
ground-truth concepts. However, DISSECT successfully ﬁnds concepts describing asymmetrical
shapes, jagged borders, and uneven colors that align with ABCDE of melanoma [65]. DISSECT
also identiﬁes surgical markings as a spurious concept impacting the classiﬁer’s decisions. Overall,
the qualitative results show that DISSECT uncovers several critical blind spots of baselines."
EVALUATION METRICS,0.3406813627254509,"4.5
CASE STUDY III: IDENTIFYING BIASES"
EVALUATION METRICS,0.342685370741483,"Another potential use case of DISSECT is to identify biases that might need to be rectiﬁed. Since
our approach does not depend on predeﬁned user concepts, it may help discover unkwown biases.
We design an experiment to test DISSECT in such a setting. We sub-sample CelebA to create a
training dataset such that smiling correlates with “blond hair” and “bangs” attributes. In particular,
positive samples either have blond hair or have bangs, and negative examples are all dark-haired
and do not have bangs. We use this dataset to train a purposefully biased classiﬁer. We employ
DISSECT to generate two CTs. Fig. 5 shows the qualitative results, which depict that DISSECT
automatically discovers the two biases, which other techniques fail to do. Tab. 3 summarizes the
quantitative results that replicate our ﬁnding from Tab. 1 in § 4.3 and Tab. 2 in § 4.4 in a real-world
dataset, conﬁrming that DISSECT quantitatively outperforms all the other baselines in Distinctness
without negatively impacting Importance or Realism."
CONCLUDING REMARKS,0.34468937875751504,"5
CONCLUDING REMARKS"
CONCLUDING REMARKS,0.3466933867735471,"We present DISSECT that successfully ﬁnds multiple distinct concepts by generating a series of
realistic-looking, counterfactually generated samples that gradually traverse a classiﬁer’s decision
boundary. We validate DISSECT via quantitative and qualitative experiments, showing signiﬁcant
improvement over prior methods. Mirroring natural human reasoning for explainability [3, 4, 6, 83],
our method provides additional checks-and-balances for practitioners to probe a trained model prior
to deployment, especially in high-stakes tasks. DISSECT helps identify whether the model-under-
test reﬂects practitioners’ domain knowledge and discover biases not previously known to users."
CONCLUDING REMARKS,0.3486973947895792,"One avenue for future explainability work is extending earlier theories [45, 71] that obtain disentan-
glement guarantees. While DISSECT uses an unsupervised approach for concepts discovery, it is
possible to modify the training procedure and the concept disentangler network to allow for concept
supervision, which can be another avenue for future work. Furthermore, extending this technique to
domains beyond visual data such as time series or text-based input can broaden this work’s impact.
While we provide an extensive list of qualitative examples in the Appendix, human-subject studies
to validate that CTs exhibit semantically meaningful attributes could further strengthen our ﬁndings."
CONCLUDING REMARKS,0.35070140280561124,Published as a conference paper at ICLR 2022
ETHICS STATEMENT,0.3527054108216433,"6
ETHICS STATEMENT"
ETHICS STATEMENT,0.35470941883767537,"We emphasize that our proposed method does not guarantee ﬁnding all the biases of a classiﬁer,
nor ensures semantic meaningfulness across all found concepts. Additionally, it should not replace
procedures for promoting transparency in model evaluation such as model cards [51]; instead, we
recommend it be used in tandem. We also acknowledge the limitations for Fitzpatrick ratings used
in SynthDerm dataset in capturing variability, especially for darker skin tones [59, 77], and would
like to consider other ratings for future."
REPRODUCIBILITY STATEMENT,0.35671342685370744,"7
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.3587174348697395,"The code for all the models and metrics is publicly available at https://github.com/
asmadotgh/dissect.Additionally, we released the new dataset, which is publicly available at
https://affect.media.mit.edu/dissect/synthderm. See §A.3.6 and §A.4 for ad-
ditional details about evaluation metrics, experiment setup, and hyperparameter selection."
REPRODUCIBILITY STATEMENT,0.36072144288577157,ACKNOWLEDGMENTS
REPRODUCIBILITY STATEMENT,0.3627254509018036,"We thank Ardavan Saeedi, David Sontag, Zachary Lipton, Weiwei Pan, Finale Doshi-Velez, Emily
Denton, Matthew Groh, Josh Belanich, James Wexler, Tom Murray, Kree Cole-Mclaughlin, Kim-
berly Wilber, Amirata Ghorbani, and Sonali Parbhoo for insightful discussions, Kayhan Batmanghe-
lich and Sumedha Singla for the baseline open-source code, and MIT Media Lab Consortium for
supporting this research."
REFERENCES,0.36472945891783565,REFERENCES
REFERENCES,0.3667334669338677,"[1] Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim.
Sanity checks for saliency maps. In Advances in Neural Information Processing Systems, pp.
9505–9515, 2018. 1, 3
[2] Martin Arjovsky, Soumith Chintala, and L´eon Bottou. Wasserstein generative adversarial net-
works. In International conference on machine learning, pp. 214–223. PMLR, 2017. 4
[3] Maximilian Augustin, Alexander Meinke, and Matthias Hein. Adversarial robustness on in-
and out-distribution improves explainability. arXiv preprint arXiv:2003.09461, 2020. 2, 9
[4] Sarah R Beck, Kevin J Riggs, and Sarah L Gorniak. Relating developments in children’s
counterfactual thinking and executive functions. Thinking & reasoning, 15(4):337–354, 2009.
2, 9
[5] David Berthelot, Thomas Schumm, and Luke Metz. Began: Boundary equilibrium generative
adversarial networks. arXiv preprint arXiv:1703.10717, 2017. 4
[6] Daphna Buchsbaum, Sophie Bridgers, Deena Skolnick Weisberg, and Alison Gopnik. The
power of possibility: Causal learning, counterfactual reasoning, and pretend play. Philosophi-
cal Transactions of the Royal Society B: Biological Sciences, 367(1599):2202–2212, 2012. 2,
9
[7] Chris Burgess and Hyunjik Kim. 3D shapes dataset, 2018. URL https://github.com/
deepmind/3dshapes-dataset. 2, 5, 7
[8] Christopher P Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Des-
jardins, and Alexander Lerchner.
Understanding disentangling in beta-VAE.
Advances in
neural information processing systems: Workshop on Learning Disentangled Representations,
2017. 5, 6, 15
[9] Carrie J Cai, Emily Reif, Narayan Hegde, Jason Hipp, Been Kim, Daniel Smilkov, Martin
Wattenberg, Fernanda Viegas, Greg S Corrado, Martin C Stumpe, et al. Human-centered tools
for coping with imperfect algorithms during medical decision-making. In Proceedings of the
2019 CHI Conference on Human Factors in Computing Systems, pp. 1–14, 2019. 2, 3
[10] Runjin Chen, Hao Chen, Jie Ren, Ge Huang, and Quanshi Zhang. Explaining neural networks
semantically and quantitatively. In Proceedings of the IEEE/CVF International Conference on
Computer Vision, pp. 9187–9196, 2019. 3"
REFERENCES,0.3687374749498998,Published as a conference paper at ICLR 2022
REFERENCES,0.37074148296593185,"[11] Tian Qi Chen, Xuechen Li, Roger B Grosse, and David K Duvenaud. Isolating sources of
disentanglement in variational autoencoders. In Advances in neural information processing
systems, pp. 2610–2620, 2018. 4, 5, 6, 15"
REFERENCES,0.3727454909819639,"[12] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Info-
GAN: Interpretable representation learning by information maximizing generative adversarial
nets. In Advances in neural information processing systems, pp. 2172–2180, 2016. 1, 3, 5"
REFERENCES,0.374749498997996,"[13] Henriette Cramer, Jean Garcia-Gathright, Aaron Springer, and Sravana Reddy. Assessing and
addressing algorithmic bias in practice. Interactions, 25(6):58–63, 2018. 1"
REFERENCES,0.37675350701402804,"[14] Emily Denton, Ben Hutchinson, Margaret Mitchell, and Timnit Gebru. Detecting bias with
generative counterfactual face attribute augmentation, 2019. 1, 3"
REFERENCES,0.3787575150300601,"[15] Amit Dhurandhar, Pin-Yu Chen, Ronny Luss, Chun-Chen Tu, Paishun Ting, Karthikeyan Shan-
mugam, and Payel Das. Explanations based on the missing: Towards contrastive explanations
with pertinent negatives. In Advances in neural information processing systems, pp. 592–603,
2018. 3"
REFERENCES,0.3807615230460922,"[16] Finale Doshi-Velez and Been Kim. Towards a rigorous science of interpretable machine learn-
ing. arXiv preprint arXiv:1702.08608, 2017. 1"
REFERENCES,0.38276553106212424,"[17] Michael Downs, Jonathan L Chu, Yaniv Yacoby, Finale Doshi-Velez, and Weiwei Pan. Cruds:
Counterfactual recourse using disentangled subspaces.
ICML Workshop on Human Inter-
pretability in Machine Learning, 2020. 2, 16"
REFERENCES,0.3847695390781563,"[18] Yanai Elazar and Yoav Goldberg. Adversarial removal of demographic attributes from text
data. In Conference on Empirical Methods in Natural Language Processing, pp. 11–21, 2018.
6, 20"
REFERENCES,0.3867735470941884,"[19] Dumitru Erhan, Yoshua Bengio, Aaron Courville, and Pascal Vincent. Visualizing higher-layer
features of a deep network. University of Montreal, 1341(3):1, 2009. 1, 2"
REFERENCES,0.38877755511022044,"[20] Thomas B Fitzpatrick. The validity and practicality of sun-reactive skin types i through vi.
Archives of dermatology, 124(6):869–871, 1988. 5, 8"
REFERENCES,0.3907815631262525,"[21] Amirata Ghorbani, Abubakar Abid, and James Zou. Interpretation of neural networks is fragile.
In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pp. 3681–3688,
2019. 2, 7, 20"
REFERENCES,0.3927855711422846,"[22] Amirata Ghorbani, James Wexler, James Y Zou, and Been Kim. Towards automatic concept-
based explanations. In Advances in Neural Information Processing Systems, volume 32, 2019.
2, 3, 6, 7, 19, 20"
REFERENCES,0.39478957915831664,"[23] Alyssa Glass, Deborah L McGuinness, and Michael Wolverton. Toward establishing trust
in adaptive agents. In Proceedings of the 13th international conference on Intelligent user
interfaces, pp. 227–236, 2008. 1"
REFERENCES,0.3967935871743487,"[24] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in
neural information processing systems, pp. 2672–2680, 2014. 4, 17"
REFERENCES,0.39879759519038077,"[25] Yash Goyal, Uri Shalit, and Been Kim.
Explaining classiﬁers with causal concept effect
(CaCE). arXiv preprint arXiv:1907.07165, 2019. 3"
REFERENCES,0.40080160320641284,"[26] Yash Goyal, Ziyan Wu, Jan Ernst, Dhruv Batra, Devi Parikh, and Stefan Lee. Counterfactual
visual explanations. In International Conference on Machine Learning, pp. 2376–2384, 2019.
3"
REFERENCES,0.4028056112224449,"[27] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville.
Improved training of wasserstein gans. arXiv preprint arXiv:1704.00028, 2017. 4"
REFERENCES,0.40480961923847697,"[28] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances
in neural information processing systems, pp. 6626–6637, 2017. 6, 7, 19, 20"
REFERENCES,0.40681362725450904,"[29] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew
Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning basic visual con-
cepts with a constrained variational framework. International Conference on Learning Repre-
sentations, 2(5):6, 2017. 1, 3, 4, 5, 6, 15"
REFERENCES,0.4088176352705411,Published as a conference paper at ICLR 2022
REFERENCES,0.41082164328657317,"[30] Sarthak Jain and Byron C Wallace. Attention is not explanation. In Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Tech-
nologies, volume 1, pp. 3543–3556, 2019. 1"
REFERENCES,0.41282565130260523,"[31] Jeya Vikranth Jeyakumar, Joseph Noor, Yu-Hsi Cheng, Luis Garcia, and Mani Srivastava. How
can i explain this to you? an empirical study of deep neural network explanation methods.
Advances in Neural Information Processing Systems, 33, 2020. 2"
REFERENCES,0.4148296593186373,"[32] Shalmali Joshi, Oluwasanmi Koyejo, Warut Vijitbenjaronk, Been Kim, and Joydeep Ghosh.
Towards realistic individual recourse and actionable explanations in black-box decision making
systems. arXiv preprint arXiv:1907.09615, 2019. 1, 3"
REFERENCES,0.4168336673346693,"[33] Rajiv Khanna, Been Kim, Joydeep Ghosh, and Sanmi Koyejo. Interpreting black box predic-
tions using Fisher kernels. In International Conference on Artiﬁcial Intelligence and Statistics,
pp. 3382–3390, 2019. 1, 3"
REFERENCES,0.4188376753507014,"[34] Been Kim, Cynthia Rudin, and Julie A Shah. The bayesian case model: A generative approach
for case-based reasoning and prototype classiﬁcation. In Advances in neural information pro-
cessing systems, pp. 1952–1960, 2014. 1, 3"
REFERENCES,0.42084168336673344,"[35] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas,
et al. Interpretability beyond feature attribution: Quantitative testing with concept activation
vectors (TCAV). In International Conference on Machine Learning, pp. 2673–2682, 2018. 1,
2, 3"
REFERENCES,0.4228456913827655,"[36] Hyunjik Kim and Andriy Mnih. Disentangling by factorising. In International Conference on
Machine Learning, pp. 2654–2663, 2018. 1, 3"
REFERENCES,0.4248496993987976,"[37] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International
Conference on Learning Representations, 2015. 19"
REFERENCES,0.42685370741482964,"[38] Jack Klys, Jake Snell, and Richard Zemel. Learning latent subspaces in variational autoen-
coders. In Proceedings of the 32nd International Conference on Neural Information Process-
ing Systems, pp. 6445–6455, 2018. 3, 4, 5, 6, 16"
REFERENCES,0.4288577154308617,"[39] Pang Wei Koh and Percy Liang. Understanding black-box predictions via inﬂuence functions.
In International Conference on Machine Learning, volume 70, pp. 1885–1894, 2017. 1, 3"
REFERENCES,0.4308617234468938,"[40] Tejas D Kulkarni, William F Whitney, Pushmeet Kohli, and Josh Tenenbaum. Deep convo-
lutional inverse graphics network. In Advances in neural information processing systems, pp.
2539–2547, 2015. 1"
REFERENCES,0.43286573146292584,"[41] Abhishek Kumar, Prasanna Sattigeri, and Avinash Balakrishnan.
Variational inference of
disentangled latent concepts from unlabeled observations.
In International Conference
on Learning Representations, 2018.
URL https://openreview.net/forum?id=
H1kG7GZAW. 4, 5, 6, 15"
REFERENCES,0.4348697394789579,"[42] Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnab´as P´oczos.
Mmd gan: Towards deeper understanding of moment matching network.
arXiv preprint
arXiv:1705.08584, 2017. 4"
REFERENCES,0.43687374749499,"[43] Zinan Lin, Kiran Thekumparampil, Giulia Fanti, and Sewoong Oh.
InfoGAN-CR and
ModelCentrality: Self-supervised model training and selection for disentangling GANs. In
Hal Daum´e III and Aarti Singh (eds.), Proceedings of the 37th International Conference
on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp.
6127–6139. PMLR, 13–18 Jul 2020. URL http://proceedings.mlr.press/v119/
lin20e.html. 5"
REFERENCES,0.43887775551102204,"[44] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the
wild. In IEEE international conference on computer vision, pp. 3730–3738, 2015. 2, 5"
REFERENCES,0.4408817635270541,"[45] Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard
Sch¨olkopf, and Olivier Bachem. Challenging common assumptions in the unsupervised learn-
ing of disentangled representations. In International Conference on Machine Learning, pp.
4114–4124, 2019. 1, 3, 5, 9, 15, 22"
REFERENCES,0.44288577154308617,"[46] Francesco Locatello, Michael Tschannen, Stefan Bauer, Gunnar R¨atsch, Bernhard Sch¨olkopf,
and Olivier Bachem. Disentangling factors of variations using few labels. In International
Conference on Learning Representations, 2019. 3"
REFERENCES,0.44488977955911824,Published as a conference paper at ICLR 2022
REFERENCES,0.4468937875751503,"[47] Scott M Lundberg and Su-In Lee. A uniﬁed approach to interpreting model predictions. In
Advances in neural information processing systems, 2017. 1, 2, 3"
REFERENCES,0.44889779559118237,"[48] Ronny Luss, Pin-Yu Chen, Amit Dhurandhar, Prasanna Sattigeri, Yunfeng Zhang, Karthikeyan
Shanmugam, and Chun-Chen Tu. Leveraging latent features for local explanations. In Pro-
ceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pp.
1139–1149, 2021. 3"
REFERENCES,0.45090180360721444,"[49] Charles T Marx, Richard Lanas Phillips, Sorelle A Friedler, Carlos Scheidegger, and Suresh
Venkatasubramanian. Disentangling inﬂuence: Using disentangled representations to audit
model predictions. arXiv preprint arXiv:1906.08652, 2019. 1"
REFERENCES,0.4529058116232465,"[50] David Alvarez Melis and Tommi Jaakkola. Towards robust interpretability with self-explaining
neural networks.
In Advances in Neural Information Processing Systems, pp. 7775–7784,
2018. 2, 6, 7, 20"
REFERENCES,0.45490981963927857,"[51] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben
Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model
reporting. In Proceedings of the conference on fairness, accountability, and transparency, pp.
220–229, 2019. 10"
REFERENCES,0.45691382765531063,"[52] Takeru Miyato and Masanori Koyama. cGANs with projection discriminator. In Interna-
tional Conference on Learning Representations, 2018.
URL https://openreview.
net/forum?id=ByS1VpgRZ. 3, 4, 17"
REFERENCES,0.4589178356713427,"[53] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normaliza-
tion for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018. 4"
REFERENCES,0.46092184368737477,"[54] Ramaravind K Mothilal, Amit Sharma, and Chenhao Tan. Explaining machine learning clas-
siﬁers through diverse counterfactual explanations. In Proceedings of the 2020 Conference on
Fairness, Accountability, and Transparency, pp. 607–617, 2020. 2"
REFERENCES,0.46292585170340683,"[55] Anh Nguyen, Jeff Clune, Yoshua Bengio, Alexey Dosovitskiy, and Jason Yosinski. Plug & play
generative networks: Conditional iterative generation of images in latent space. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4467–4477, 2017. 4"
REFERENCES,0.4649298597194389,"[56] Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with
auxiliary classiﬁer GANs. In International Conference on Machine Learning, volume 70, pp.
2642–2651, 2017. 3"
REFERENCES,0.46693386773547096,"[57] Jose Oramas, Kaili Wang, and Tinne Tuytelaars. Visual explanation by interpretation: Im-
proving visual feedback capabilities of deep neural networks. In International Conference on
Learning Representations, 2018. 3"
REFERENCES,0.46893787575150303,"[58] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Con-
text encoders: Feature learning by inpainting.
In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 2536–2544, 2016. 4"
REFERENCES,0.4709418837675351,"[59] Latrice C Pichon, Hope Landrine, Irma Corral, Yongping Hao, Joni A Mayer, and Kather-
ine D Hoerster. Measuring skin cancer risk in african americans: is the ﬁtzpatrick skin type
classiﬁcation scale culturally sensitive. Ethn Dis, 20(2):174–179, 2010. 10"
REFERENCES,0.4729458917835671,"[60] Gregory Plumb, Maruan Al-Shedivat, ´Angel Alexander Cabrera, Adam Perer, Eric Xing, and
Ameet Talwalkar. Regularizing black-box models for improved interpretability. Advances in
Neural Information Processing Systems, 33, 2020. 2, 7, 20"
REFERENCES,0.4749498997995992,"[61] Danish Pruthi, Mansi Gupta, Bhuwan Dhingra, Graham Neubig, and Zachary C Lipton. Learn-
ing to deceive with attention-based explanations. In Proceedings of the 58th Annual Meeting
of the Association for Computational Linguistics, pp. 4782–4793, 2020. 1"
REFERENCES,0.47695390781563124,"[62] Suman Ravuri and Oriol Vinyals.
Classiﬁcation accuracy score for conditional generative
models. In Advances in Neural Information Processing Systems, volume 32, pp. 12268–12279.
Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/
2019/file/fcf55a303b71b84d326fb1d06e332a26-Paper.pdf. 7, 20"
REFERENCES,0.4789579158316633,"[63] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-ﬁdelity images
with vq-vae-2. In Advances in Neural Information Processing Systems, volume 32. Curran
Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/
file/5f8e2fa1718d1bbcadf1cd9c7a54fb8c-Paper.pdf. 4"
REFERENCES,0.48096192384769537,Published as a conference paper at ICLR 2022
REFERENCES,0.48296593186372744,"[64] JH Rick Chang, Chun-Liang Li, Barnabas Poczos, BVK Vijaya Kumar, and Aswin C Sankara-
narayanan. One network to solve them all–solving linear inverse problems using deep projec-
tion models. In Proceedings of the IEEE International Conference on Computer Vision, pp.
5888–5897, 2017. 4
[65] Darrell S Rigel, Robert J Friedman, Alfred W Kopf, and David Polsky. Abcde—an evolving
concept in the early detection of melanoma.
Archives of dermatology, 141(8):1032–1034,
2005. 5, 8, 9
[66] Mihaela Rosca, Balaji Lakshminarayanan, David Warde-Farley, and Shakir Mohamed.
Variational approaches for auto-encoding generative adversarial networks.
arXiv preprint
arXiv:1706.04987, 2017. 4
[67] Tim Salimans, Ian J Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and
Xi Chen. Improved techniques for training GANs. In NIPS, 2016. 7, 19, 20
[68] Pouya Samangouei, Ardavan Saeedi, Liam Nakagawa, and Nathan Silberman. ExplainGAN:
Model explanation via decision boundary crossing transformations. In European Conference
on Computer Vision, pp. 666–681, 2018. 1, 2, 3, 7, 20
[69] Alberto Santamaria-Pang, James Kubricht, Aritra Chowdhury, Chitresh Bhushan, and Peter
Tu. Towards emergent language symbolic semantic segmentation and model interpretability. In
International Conference on Medical Image Computing and Computer-Assisted Intervention,
pp. 326–334. Springer, 2020. 2
[70] Soﬁa Serrano and Noah A Smith. Is attention interpretable? In Proceedings of the 57th Annual
Meeting of the Association for Computational Linguistics, pp. 2931–2951, 2019. 1
[71] Rui Shu, Yining Chen, Abhishek Kumar, Stefano Ermon, and Ben Poole. Weakly supervised
disentanglement with guarantees. In International Conference on Learning Representations,
2020. URL https://openreview.net/forum?id=HJgSwyBKvr. 3, 9, 15, 22
[72] Wilson Silva, Alexander Poellinger, Jaime S Cardoso, and Mauricio Reyes. Interpretability-
guided content-based medical image retrieval. In International Conference on Medical Image
Computing and Computer-Assisted Intervention, pp. 305–314. Springer, 2020. 2
[73] Sumedha Singla, Brian Pollack, Junxiang Chen, and Kayhan Batmanghelich. Explanation by
progressive exaggeration. In International Conference on Learning Representations, 2020.
URL https://openreview.net/forum?id=H1xFWgrFPS. 1, 2, 3, 4, 6, 17, 19, 21
[74] Sumedha Singla, Brian Pollack, Stephen Wallace, and Kayhan Batmanghelich. Explaining the
black-box smoothly-a counterfactual approach. arXiv preprint arXiv:2101.04230, 2021. 2, 6,
19
[75] Leon Sixt, Maximilian Granz, and Tim Landgraf. When explanations lie: Why modiﬁed bp
attribution fails. arXiv preprint arXiv:1912.09818, 2019. 3
[76] Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Vi´egas, and Martin Wattenberg. Smooth-
Grad: Removing noise by adding noise. ICML Workshop on Visualization for Deep Learning,
2017. 1, 2
[77] Marilyn S Sommers, Jamison D Fargo, Yadira Regueira, Kathleen M Brown, Barbara L
Beacham, Angela R Perfetti, Janine S Everett, and David J Margolis.
Are the ﬁtzpatrick
skin phototypes valid for cancer risk assessment in a racially and ethnically diverse sample
of women? Ethnicity & disease, 29(3):505, 2019. 10
[78] Yi Sun, Yuheng Chen, Xiaogang Wang, and Xiaoou Tang. Deep learning face representation
by joint identiﬁcation-veriﬁcation. In Proceedings of the 27th International Conference on
Neural Information Processing Systems-Volume 2, pp. 1988–1996, 2014. 5
[79] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In
International Conference on Machine Learning, volume 70, pp. 3319–3328, 2017. 1, 2, 3
[80] Hensin Tsao, Jeannette M Olazagasti, Kelly M Cordoro, Jerry D Brewer, Susan C Taylor,
Jeremy S Bordeaux, Mary-Margaret Chren, Arthur J Sober, Connie Tegeler, Reva Bhushan,
et al. Early detection of melanoma: reviewing the abcdes. Journal of the American Academy
of Dermatology, 72(4):717–723, 2015. 5, 8
[81] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation
learning. In Proceedings of the 31st International Conference on Neural Information Process-
ing Systems, pp. 6309–6318, 2017. 4"
REFERENCES,0.4849699398797595,Published as a conference paper at ICLR 2022
REFERENCES,0.48697394789579157,"[82] Sandra Wachter, Brent Mittelstadt, and Chris Russell. Counterfactual explanations without
opening the black box: Automated decisions and the gdpr. Harv. JL & Tech., 31:841, 2017. 1"
REFERENCES,0.48897795591182364,"[83] Deena S Weisberg and Alison Gopnik. Pretense, counterfactuals, and bayesian causal models:
Why what is not real really matters. Cognitive science, 37(7):1368–1381, 2013. 2, 9"
REFERENCES,0.4909819639278557,"[84] Sarah Wiegreffe and Yuval Pinter. Attention is not not explanation. In Proceedings of the 2019
Conference on Empirical Methods in Natural Language Processing and the 9th International
Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 11–20, 2019. 1"
REFERENCES,0.49298597194388777,"[85] Jenna Wiens, Suchi Saria, Mark Sendak, Marzyeh Ghassemi, Vincent X Liu, Finale Doshi-
Velez, Kenneth Jung, Katherine Heller, David Kale, Mohammed Saeed, et al. Do no harm: A
roadmap for responsible machine learning for health care. Nature medicine, pp. 1–4, 2019. 8"
REFERENCES,0.49498997995991983,"[86] Julia K Winkler, Christine Fink, Ferdinand Toberer, Alexander Enk, Teresa Deinlein, Rainer
Hofmann-Wellenhof, Luc Thomas, Aimilios Lallas, Andreas Blum, Wilhelm Stolz, et al. Asso-
ciation between surgical skin markings in dermoscopic images and diagnostic performance of
a deep learning convolutional neural network for melanoma recognition. JAMA dermatology,
155(10):1135–1141, 2019. 2, 8"
REFERENCES,0.4969939879759519,"[87] Chih-Kuan Yeh, Joon Kim, Ian En-Hsu Yen, and Pradeep K Ravikumar. Representer point
selection for explaining deep neural networks. In Advances in neural information processing
systems, 2018. 1, 3"
REFERENCES,0.49899799599198397,"[88] Junbo Zhao, Michael Mathieu, and Yann LeCun. Energy-based generative adversarial network.
arXiv preprint arXiv:1609.03126, 2016. 4"
REFERENCES,0.501002004008016,"[89] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image trans-
lation using cycle-consistent adversarial networks. In Proceedings of the IEEE international
conference on computer vision, pp. 2223–2232, 2017. 4"
REFERENCES,0.503006012024048,"A
APPENDIX"
REFERENCES,0.5050100200400801,"A.1
BASELINES"
REFERENCES,0.5070140280561122,"A.1.1
BASELINE 1: MULTI-MODAL EXPLAINABILITY THROUGH VAE-BASED
DISENTANGLEMENT"
REFERENCES,0.5090180360721442,"Disentanglement approaches have demonstrated practical success in learning representations that
correspond to factors for variation in data [71], though some gaps between theory and practice re-
main [45]. However, the extent to which these techniques can aid post hoc explainability in conjunc-
tion with an external model is not well understood. Thus, we consider a set of baseline approaches
based on VAEs explicitly designed for disentanglement: β-VAE [29], Annealed-VAE [8], β-TCVAE
[11], and DIPVAE [41]. We extend each of them to incorporate the classiﬁer’s signal during their
training processes for a fair comparison with DISSECT. Intuitively speaking, this encourages the
generative model to learn latent dimensions that could inﬂuence the classiﬁer, i.e., learning Inﬂuen-
tial CTs. Note that it is necessary to also cover data generative factors that are independent of the
external classiﬁer. This means that, along with having a good quality reconstruction and high likeli-
hood, we need to promote sparsity in sensitivity of the external classiﬁer to the latent dimensions of
the generative model."
REFERENCES,0.5110220440881763,"More formally, consider a vanilla VAE that has an encoder eθ with parameters θ, a decoder dφ
with parameters φ, and the M-dimensional latent code z with prior distribution p(z). Recall that x
denotes the input sample. The objective of a VAE is to minimize the loss:"
REFERENCES,0.5130260521042084,"Lvanilla VAE
θ,φ
= −Ez∼eθ(z|x)[log dφ(x|z)] + KL(eθ(z|x)||p(z))."
REFERENCES,0.5150300601202404,"We introduce an additional loss term for incorporating the black-box classiﬁer’s signal:
1/K PK
k=1 ∂f(x)/∂zk.
We impose this only for the ﬁrst K dimensions in the latent space6,"
REFERENCES,0.5170340681362725,"6Without loss of generality, the additional term can be applied to the ﬁrst K dimensions, and there is no
need to consider
 K
M

potential selections."
REFERENCES,0.5190380761523046,Published as a conference paper at ICLR 2022
REFERENCES,0.5210420841683366,"in other words, the number of desired CTs, K ≤M.
Minimizing this term provides CTks,
k ∈{1, 2, . . . , K}, with negative ∂f(x)/∂zk, with a high |∂f(x)/∂zk|. The ﬁnal loss is:"
REFERENCES,0.5230460921843687,"Lθ,φ = Lvanilla VAE
θ,φ
+ λ ∗
PK
k=1 ∂f(x)/∂zk K
,"
REFERENCES,0.5250501002004008,"where λ is a hyper-parameter. We apply this modiﬁcation to the four aforementioned VAE-based
approaches and refer to them with a -mod postﬁx, e.g. β-VAE-mod.7"
REFERENCES,0.5270541082164328,"Development process.
To promote discovering Important CTs,
we introduced Laux
=
PK
d=1 ∂f(x)/∂zd"
REFERENCES,0.5290581162324649,"K
, which incorporated the directional derivative of f with respect to the latent di-
mensions of interest into the loss function of VAE. Despite experimentation with many variants of
Laux, we observed two common themes."
REFERENCES,0.531062124248497,"First, a monotonic increase of f(¯x) through traversing one latent dimension and keeping the rest
static was hardly achieved. Second, while the purpose of Laux was to promote exerting Importance
only in the ﬁrst K dimensions of the latent space, ∂f(x)/∂zd for d ∈{K +1, K +2, · · · , M} were
impacted similarly. Having strongly correlated dimensions is a failure in achieving the very goal of
disentanglement approaches. Table 4 summarizes a subset of the variants of Laux studied."
REFERENCES,0.533066132264529,"Table 4: Summary of a subset of Laux iterations. The development goal is to make the ﬁrst K
dimensions of the latent space Important. In some iterations, we encouraged the remaining M −K
dimensions not to be Important to reduce potential correlation across latent dimensions."
REFERENCES,0.5350701402805611,"Laux
1"
REFERENCES,0.5370741482965932,"PK
k=1 ∂f(x)/∂zk K
2"
REFERENCES,0.5390781563126252,"PK
k=1 ∂f(x)/∂zk K
+"
REFERENCES,0.5410821643286573,"PM
d=K+1 |∂f(x)/∂zd| M−K
3"
REFERENCES,0.5430861723446894,"PK
k=1 ∂f(x)/∂zk K
+"
REFERENCES,0.5450901803607214,"PM
d=K+1[∂f(x)/∂zd]2 M−K
4"
REFERENCES,0.5470941883767535,"PM
d=K+1 |∂f(x)/∂zd| M−K
5"
REFERENCES,0.5490981963927856,"PM
d=K+1[∂f(x)/∂zd]2 M−K
6 P"
REFERENCES,0.5511022044088176,"k,d |∂f(x)/∂zd|/|∂f(x)/∂zk|"
REFERENCES,0.5531062124248497,"K∗(M−K)
where k ∈{1, 2, · · · , K}, d ∈{K + 1, K + 2, · · · , M} 7 P"
REFERENCES,0.5551102204408818,"k,d log(|∂f(x)/∂zd|/|∂f(x)/∂zk|)"
REFERENCES,0.5571142284569138,"K∗(M−K)
where k ∈{1, 2, · · · , K}, d ∈{K + 1, K + 2, · · · , M}"
REFERENCES,0.5591182364729459,"A.1.2
BASELINE 2: MULTI-MODAL EXPLAINABILITY THROUGH CONDITIONAL SUBSPACE
VAE"
REFERENCES,0.561122244488978,"Another relevant area of work is conditional generation. In particular, Conditional subspace VAE
(CSVAE) is a method aiming to solve unsupervised learning of features associated with a speciﬁc
label using a low-dimensional latent subspace that can be independently manipulated [38]. CSVAE
partitions the latent space into two parts: w learns representations correlated with the label, and z
covers the remaining characteristics for data generation. An assumption of independence between z
and w is made. To explicitly enforce independence in the learned model, we minimize the mutual
information between Y and Z. CSVAE has proven successful in providing counterfactual scenarios
to reverse unfavorable decisions of an algorithm, also known as algorithmic recourse [17]. To adjust
CSVAE to explain the decision-making of an external classiﬁer f, we treat the predictions of the
classiﬁer as the label of interest."
REFERENCES,0.56312625250501,"More formally, the generative model can be summarized as:"
REFERENCES,0.5651302605210421,"w|y ∼N(µy, σ2
y.I), y ∼Bern(p),"
REFERENCES,0.5671342685370742,"x|w, z ∼N(dφµ(w, z), σ2
ϵ .I), z ∼N(0, σ2
z.I)"
REFERENCES,0.5691382765531062,Conducting inference leads to the following objective function:
REFERENCES,0.5711422845691383,"7We build these baselines on top of the open-sourced implementations provided in https://github.
com/google-research/disentanglement_lib under Apache License 2.0."
REFERENCES,0.5731462925851704,Published as a conference paper at ICLR 2022
REFERENCES,0.5751503006012024,"M1 = ED(x,y)[−Eqφ(z,w|x,y)[log pθ(x|w, z)]+KL(qφ(w|x, y)||pγ(w|y))+KL(qφ(z|x, y)||p(z))−log p(y)]"
REFERENCES,0.5771543086172345,"M2 =Eqφ(z|x)D(x)[
Z"
REFERENCES,0.5791583166332666,"Y
qδ(y|z) log qδ(y|z) dy]"
REFERENCES,0.5811623246492986,"M3 =Eq(z|x)D(x,y)[qδ(y|z)]
min
θ,φ,γ β1M1 + β2M2; max
δ
β3M3"
REFERENCES,0.5831663326653307,"A.1.3
BASELINE 3: MULTI-MODAL EXPLAINABILITY THROUGH PROGRESSIVE
EXAGGERATION"
REFERENCES,0.5851703406813628,"Explanation by Progressive Exaggeration (EPE) [73] is a recent successful generative approach that
learns to generate one series of counterfactual and realistic samples that change the prediction of f,
given data and the classiﬁer’s signal. It is particularly relevant to our work as it explicitly optimizes
Inﬂuence and Realism. EPE is a type of Generative Adversarial Network (GAN) [24] consisting of
a discriminator (D) and a generator (G) that is based on Projection GAN [52]. It incorporates the
amount of desired perturbation α on the outcome of f as:"
REFERENCES,0.5871743486973948,"LcGAN(D) = −Ex∼pdata[min(0, −1 + D(x, 0))] −Ex∼pdata[min(0, −1 −D(G(x, α), α))]
(8)"
REFERENCES,0.5891783567134269,"LcGAN(G) = −Ex∼pdata[D(G(x, α), α)]
(9)
A Kullback–Leibler divergence (KL) term in the objective function between the desired perturbation
(α) and the achieved one (f(G(x, α))) promotes Importance [73]:"
REFERENCES,0.591182364729459,"Lf(D, G) = r(D, G(x, α)) + KL(α|f(G(x, α))),
(10)"
REFERENCES,0.593186372745491,"where the ﬁrst term is the likelihood ratio deﬁned in projection GAN [24], and [73] uses an ordinal-
regression parameterization of it."
REFERENCES,0.5951903807615231,"A reconstruction loss and a cycle loss promote self-consistency in the model, meaning that applying
a reverse perturbation or no perturbation should reconstruct the query sample:"
REFERENCES,0.5971943887775552,"Lrec(G) = ||x −G(x, f(x))||1
(11)"
REFERENCES,0.5991983967935872,"Lcyc(G) = ||x −G(G(x, α), f(x))||1.
(12)
Thus, the overall objective function of EPE is the following:"
REFERENCES,0.6012024048096193,"min
G
max
D λcGANLcGAN(D, G) + λfLf(D, G) + λrecLrec(G) + λrecLcyc(G),
(13)"
REFERENCES,0.6032064128256514,"where λcGAN, λf, and λrec are the hyper-parameters."
REFERENCES,0.6052104208416834,"Note that EPE only ﬁnds one pathway to switch the classiﬁer’s outcome. We argue that classiﬁers
learned from challenging and realistic datasets will have complex reasoning pathways that could
enhance model explainability if revealed. Decomposing this complexity is needed to make reasoning
comprehensible for humans. We thus create a more powerful baseline, an EPE-variant, EPE-mod.
EPE-mod learns multiple pathways by making the generator conditional on another variable: the
CT dimension. More formally, EPE-mod updates G(·, ·) to G(·, ·, k) in Eq. equation 8-equation 12,
while Eq. equation 13 remains unchanged. We compare DISSECT to both EPE and EPE-mod."
REFERENCES,0.6072144288577155,"A.2
DISSECT DETAILS"
REFERENCES,0.6092184368737475,"We build our proposed method on EPE-mod and further promote distinctness across CTs by adding
a disentangler network, R. The disentangler is a classiﬁer with K classes. Given a pair of (x, x′)
images, R tries to predict which CTk;k∈{1,...,K} has perturbed query x to produce x′. Note that
R can return close to 0 probability for all classes if x′ is just a reconstruction of x, indicating no
tweaked dimensions. The disentangler also penalizes the generator if any CTs use similar pathways
to cross the decision boundary. See the appendix for schematics of our method."
REFERENCES,0.6112224448897795,"To formalize this, let:
ˆxk = G(x, f(x), k)
¯xk = G(x, α, k)
˜xk = G(¯xk, f(x), k)."
REFERENCES,0.6132264529058116,Published as a conference paper at ICLR 2022
REFERENCES,0.6152304609218436,Discriminator
REFERENCES,0.6172344689378757,Real or Fake?
REFERENCES,0.6192384769539078,Generator 𝛼
REFERENCES,0.6212424849699398,Counterfactual
REFERENCES,0.6232464929859719,Discriminator
REFERENCES,0.625250501002004,Real or Fake?
REFERENCES,0.627254509018036,"CT 
dimension"
REFERENCES,0.6292585170340681,Generator 𝛼
REFERENCES,0.6312625250501002,"CT Dimension 1
CT Dimension 2"
REFERENCES,0.6332665330661322,Discriminator
REFERENCES,0.6352705410821643,"Real or Fake?
CT Dimension?"
REFERENCES,0.6372745490981964,"CT 
dimension"
REFERENCES,0.6392785571142284,"Generator
CT Disentangler 𝛼"
REFERENCES,0.6412825651302605,"CT Dimension 1
CT Dimension 2 EPE"
REFERENCES,0.6432865731462926,EPE-mod
REFERENCES,0.6452905811623246,DISSECT
REFERENCES,0.6472945891783567,"Figure 6: Simpliﬁed illustration of EPE, EPE-mod, and DISSECT. EPE-mod can be viewed as an
ablated version of DISSECT. Orange, Green, and Blue show elements related to the discrimina-
tor, generator, and CT disentangler, respectively. The novelty of DISSECT is the disentanglement
components."
REFERENCES,0.6492985971943888,"Note that ˆxk and ˜xk are reconstructions of x while ¯xk is perturbed to change the classiﬁer output
from f(x) to α. Therefore, x, ¯xk and ˜xk form a cycle, and k represents CTk. R(., .) is the predicted
probabilities of the perturbed concept given a pair of examples, which is a vector of size K, where
each element is a value in [0, 1]. We deﬁne the following cross entropy loss that is a function of both
R and G:"
REFERENCES,0.6513026052104208,"Lr(G, R) = CE(ek, R(x, ¯xk)) + CE(ek, R(¯xk, ˜xk))"
REFERENCES,0.6533066132264529,"= −Ex∼pdata K
X"
REFERENCES,0.655310621242485,"k=1
[eklogR(x, ¯xk) + eklogR(¯xk, ˜xk)]
(14)"
REFERENCES,0.657314629258517,Published as a conference paper at ICLR 2022
REFERENCES,0.6593186372745491,"Here, ek refers to a one-hot vector of size K where the k-th element is one and the remaining
elements are zero. This term enforces R to identify no change when receiving reconstructions of the
same image as input and utilizes the cycle and promotes determining the correct dimension when a
non-zero change has happened, either increasing or decreasing the outcome of f. In summary, the
overall objective function of our method is:"
REFERENCES,0.6613226452905812,"min
G,R max
D [λcGANLcGAN(D, G) + λfLf(D, G) + λrecLrec(G) + λrecLcyc(G)"
REFERENCES,0.6633266533066132,"+ λrLr(G, R)]
(15)"
REFERENCES,0.6653306613226453,"For this adversarial min-max optimization, we use the Adam optimizer [37]. See Figure 6 for a for
a visualization of DISSECT .We open-source our implementation at https://github.com/
asmadotgh/dissect under MIT license, which builds on top of the open-source implementa-
tion of EPE [73] 8."
REFERENCES,0.6673346693386774,"A.3
EVALUATION METRICS DETAILS"
REFERENCES,0.6693386773547094,"In this section, we provide more details about the evaluation metrics. First, we provide evidence
supporting these criteria as desirable qualities for explainability. Then, we summarize how these
qualities have been measured in prior work. Finally, we explain when we can use prior formulation
of these metrics as is, and when we need to modify them to make them applicable to the counterfac-
tual explanation case. We justify how these formulations capture the desired criteria."
REFERENCES,0.6713426853707415,"A.3.1
IMPORTANCE"
REFERENCES,0.6733466933867736,"Motivation.
Explanations should produce the desired outcome from the black-box classiﬁer f.
Previous work has referred to this quality using different terms, such as importance [22], compati-
bility with classiﬁer [73], and classiﬁcation model consistency [74]."
REFERENCES,0.6753507014028056,"Quantiﬁcation.
Most previous methods have relied on visual inspection. Singla et al. [74] have
plotted expected outcome from the classiﬁer, against the actual classiﬁer prediction of the gener-
ated explanations. Ghorbani et al. [22] have plotted prediction accuracy vs. adding or removal of
discovered concepts."
REFERENCES,0.6773547094188377,"We introduce a quantitative metric to measure the gradual increase of the target class’s posterior
probability through a CT. Notably, we compute the correlation between α and f(I(x, α, k; f)) in-
troduced in § 3. For brevity, we refer to f(I(x, α, k; f)) as f(¯x) in the remainder of the paper. We
also report the mean-squared error and the Kullback–Leibler (KL) divergence between α and f(¯x).
This is one way to summarize information in the plots used by Singla et al. [74] for Importance
evaluation."
REFERENCES,0.6793587174348698,"We also calculate the performance of f on the counterfactual explanations. Speciﬁcally, we replace
the test set of real images with their generated counterfactual explanations and quantify the perfor-
mance of the pre-trained black-box classiﬁer f on the counterfactual test set. Better performance
suggests that counterfactual samples are compatible with f and lie on the correct side of the classi-
ﬁer’s boundary. This is one way to summarize the information in the plots used by Ghorbani et al.
[22] for Importance evaluation."
REFERENCES,0.6813627254509018,"A.3.2
REALISM"
REFERENCES,0.6833667334669339,"Motivation.
We need the generated samples that form a CT to look realistic to enable users to
identify concepts they represent. It means the counterfactual explanations should lie on the data
manifold. This quality has been referred to as realism or data consistency [73]."
REFERENCES,0.685370741482966,"Quantiﬁcation.
A variety of inception scores have perviously been used to quantify visual quality
of generated images, such as Fr´echet Inception Distance [28] and Inception Score [67]."
REFERENCES,0.687374749498998,"8https://github.com/batmanlab/Explanation_by_Progressive_Exaggeration
available under MIT License."
REFERENCES,0.6893787575150301,Published as a conference paper at ICLR 2022
REFERENCES,0.6913827655310621,"Similar to prior work, we use FID [28]. However, since FID is based on an Inception V3 network
trained on ImageNet, it could be a better ﬁt for more realistic data that has similar distribution to Im-
ageNet. This is not the case, especially for 3d Shapes and SynthDerm in our work. Therefore,
we include an additional metric inspired by [18]. We train a post hoc classiﬁer that predicts whether
a sample is real or generated. We report performance of this classiﬁer as a proxy for Realism."
REFERENCES,0.6933867735470942,"A.3.3
DISTINCTNESS"
REFERENCES,0.6953907815631263,"Motivation.
Another desirable quality for explanations is to represent inputs with non-overlapping
concepts, often referred to as diversity [50]. Others have suggested similar properties such as co-
herency, meaning examples of a concept should be similar but different from examples of other
concepts [22]."
REFERENCES,0.6973947895791583,"Quantiﬁcation.
To quantify this in a counterfactual generation setup, we introduce a distinctness
measure capturing the performance of a secondary classiﬁer that distinguishes between CTs. We
train a classiﬁer post hoc that given a query image x and a generated image x′ and K number of
CTs, predicts CTk that has perturbed x to produce x′, k ∈{1, . . . , K}. This"
REFERENCES,0.6993987975951904,"A.3.4
SUBSTITUTABILITY"
REFERENCES,0.7014028056112225,"Motivation.
The representation of a sample in terms of concepts should preserve relevant infor-
mation [50, 60]. Previous work has formalized this quality for counterfactual generation contexts
through a proxy called substitutability [68]."
REFERENCES,0.7034068136272545,"Quantiﬁcation.
Substitutability measures an external classiﬁer’s performance on real data when
it is trained using only synthetic images. This metric has been used in other contexts outside of
explainability and has been called Classiﬁcation Accuracy Score (CAS) [62]. CAS is more broadly
applicable than Fr´echet Inception Distance [28] & Inception Score [67] that are only useful for
GAN evaluation. Furthermore, CAS can reveal information that none of these inception scores
successfully capture [62]. A higher substitutability score suggests that explanations have retained
relevant information and are of high quality."
REFERENCES,0.7054108216432866,We follow Samangouei et al. [68] for formulation of this metric.
REFERENCES,0.7074148296593187,"A.3.5
STABILITY"
REFERENCES,0.7094188376753507,"Motivation.
Explanations should be coherent for similar inputs, a quality known as stability [21,
50, 60]."
REFERENCES,0.7114228456913828,"Quantiﬁcation.
To quantify stability in counterfactual explanations, we augment the test set with
additive random noise to each sample x and produce S randomly augmented copies x
′′
i . Then,
we generate counterfactual explanations ¯x and ¯x
′′
i , respectively. We calculate the mean-squared
difference between counterfactual images ¯x and ¯x
′′
i and the resulting Jensen Shannon distance (JSD)
between the predicted probabilities f(¯x) and f(¯x
′′
i )."
REFERENCES,0.7134268537074149,"A.3.6
IMPLEMENTATION DETAILS"
REFERENCES,0.7154308617234469,"The VAE-based baselines support continuous values for latent dimensions zk. Also, we can directly
sample latent code values and produce CTk by keeping zj (j ̸= k) constant and monotonically
increasing zk values. However, to calculate the evaluation metrics comparably to EPE, EPE-mod,
and DISSECT, we do the following: We encode each query sample using the probabilistic encoder.
We set zj = µj, j ̸= k where µj is the mean of the ﬁtted Gaussian distribution for zj. For dimension
k, we produce N + 1 linearly spaced values between µp ± 2 ∗σp, where µp and σp are the mean
and standard deviation of the prior normal distribution, in our case 0.0 and 1.0 respectively. Note
that these different values for zk map out to α, α ∈{0, 1"
REFERENCES,0.717434869739479,"N , · · · , 1} in EPE, EPE-mod, and DISSECT
models. After this step, calculating all the metrics related to Importance, Realism, and Distinctness
is identical across all the models."
REFERENCES,0.7194388777555111,Published as a conference paper at ICLR 2022
REFERENCES,0.7214428857715431,"Table 5: Summary of hyper-parameter values. Discriminator optimization happens once every D
steps. Similarly, generator optimization happens once every G steps. λr is speciﬁc to DISSECT,
and K is speciﬁc to EPE-mod and DISSECT. All the remaining parameters are shared across EPE,
EPE-mod, and DISSECT. Note that samples used for evaluation are not included in the training
process."
REFERENCES,0.7234468937875751,"Preprocessing
Training
Evaluation Metrics"
REFERENCES,0.7254509018036072,"N
max
samples
per bin"
REFERENCES,0.7274549098196392,"λcGAN
λrec
λf
D
steps
G
steps
batch
size
epochs
K
λr
max
# samples
batch
size
epochs
hold-out
test
ratio
3D Shapes
3
5,000
1
100
1
1
5
32
300
2
10
10,000
32
10
0.25
SynthDerm
2
1,350
2
100
1
5
1
32
300
5
2
10,000
8
10
0.25
CelebA
10
5,000
1
100
1
1
5
32
300
2
2
10,000
32
10
0.25"
REFERENCES,0.7294589178356713,"A.4
EXPERIMENT SETUP AND HYPER-PARAMETER TUNING DETAILS"
REFERENCES,0.7314629258517034,"Experiments were conducted on an internal compute cluster at authors’ institution. Training and
evaluation of all models across the three datasets have approximately taken 1000 hours on a com-
bination of Nvidia GPUs including GTX TITAN X, GTX 1080 Ti, RTX 2080 Rev, and Quadro
K5200."
REFERENCES,0.7334669338677354,"We seeded the model’s parameters from [73] based on the reported values in their accompanying
open-sourced repository.9 We used the same parameters for 3D Shapes, except for the number
of bins, N, used for ordinal regression transformation of the classiﬁer’s posterior probability. The
largest number of bins that resulted in non-zero samples per bin, 3, was selected. We kept all the
parameters shared between EPE, EPE-mod, and DISSECT the same."
REFERENCES,0.7354709418837675,"Given the experiments’ design, we ﬁxed the number of dimensions K in DISSECT and EPE-mod
to 2. We experimented with a few values for λr, 1, 10, 20, 50. Based on manual inspection after 30k
training batches, λr was selected. Factors considered for selection included inspecting the perceived
quality of generated samples and the learning curves of LcGAN(D), LcGAN(G), Lcyc(G), Lrec(G),
and Lr(G, R)."
REFERENCES,0.7374749498997996,"For evaluation, we used a hold-out set including 10K samples. For post hoc evaluation classiﬁers
predicting Distinctness and Realism, 75% of the samples were used for training, and the results were
reported on the remaining 25%. See Table 5 for the summary of the hyper-parameter values."
REFERENCES,0.7394789579158316,"A.5
ADDITIONAL QUALITATIVE RESULTS"
REFERENCES,0.7414829659318637,"A.5.1
CASE STUDY I"
REFERENCES,0.7434869739478958,"Recall that considering 3D Shapes, we deﬁne an image as “colored correctly” if the shape hue
is red or the ﬂoor hue is cyan. Given a not “colored correctly” query, we recover a CT related to
the shape color and another CT associated with the ﬂoor color–two different pathways leading to
switching the classiﬁer outcome for that sample. See Figure 7 for additional qualitative examples
where classiﬁcation outcome is ﬂipped from False to True."
REFERENCES,0.7454909819639278,"However, these two ground-truth concepts do not directly apply to switching the classiﬁer outcome
from True to False in this scenario. For example, if an image has a red shape and a cyan ﬂoor,
both colors need to be changed to switch the classiﬁcation outcome. As shown in Figure 8, we still
observe that applying DISSECT to such cases results in two discovered CTs that change different
combinations of colors while EPE-mod converges to the same CT."
REFERENCES,0.7474949899799599,"A.5.2
CASE STUDY III"
REFERENCES,0.749498997995992,"Recall the biased CelebA experiment where smiling correlates with “blond hair” and “bangs” at-
tributes. Figure 9 shows additional qualitative samples, suggesting that DISSECT can recover and
separate the aforementioned concepts, which other techniques fail to do."
REFERENCES,0.751503006012024,"9https://github.com/batmanlab/Explanation_by_Progressive_Exaggeration
available under MIT License."
REFERENCES,0.7535070140280561,Published as a conference paper at ICLR 2022
REFERENCES,0.7555110220440882,"𝑓(𝑥) = 0.00
Query"
REFERENCES,0.7575150300601202,"𝑓(𝑥) = 1.00
EPE-mod"
REFERENCES,0.7595190380761523,"𝑓(𝑥) = 1.00
DISSECT"
REFERENCES,0.7615230460921844,"𝑓(𝑥) = 1.00
𝑓(𝑥) = 1.00"
REFERENCES,0.7635270541082164,"𝐶𝑇*, ,-*.."
REFERENCES,0.7655310621242485,"𝐶𝑇/, ,-*.."
REFERENCES,0.7675350701402806,"𝑓(𝑥) = 0.00
Query"
REFERENCES,0.7695390781563126,"𝑓(𝑥) = 1.00
EPE-mod"
REFERENCES,0.7715430861723447,"𝑓(𝑥) = 1.00
DISSECT"
REFERENCES,0.7735470941883767,"𝑓(𝑥) = 1.00
𝑓(𝑥) = 1.00"
REFERENCES,0.7755511022044088,"𝐶𝑇*, ,-*.."
REFERENCES,0.7775551102204409,"𝐶𝑇/, ,-*.."
REFERENCES,0.779559118236473,"𝑓(𝑥) = 0.00
Query"
REFERENCES,0.781563126252505,"𝑓(𝑥) = 1.00
EPE-mod"
REFERENCES,0.7835671342685371,"𝑓(𝑥) = 1.00
DISSECT"
REFERENCES,0.7855711422845691,"𝑓(𝑥) = 1.00
𝑓(𝑥) = 1.00"
REFERENCES,0.7875751503006012,"𝐶𝑇*, ,-*.."
REFERENCES,0.7895791583166333,"𝐶𝑇/, ,-*.."
REFERENCES,0.7915831663326653,"𝑓(𝑥) = 0.00
Query"
REFERENCES,0.7935871743486974,"𝑓(𝑥) = 1.00
EPE-mod"
REFERENCES,0.7955911823647295,"𝑓(𝑥) = 1.00
DISSECT"
REFERENCES,0.7975951903807615,"𝑓(𝑥) = 1.00
𝑓(𝑥) = 1.00"
REFERENCES,0.7995991983967936,"𝐶𝑇*, ,-*.."
REFERENCES,0.8016032064128257,"𝐶𝑇/, ,-*.."
REFERENCES,0.8036072144288577,"𝑓(𝑥) = 0.00
Query"
REFERENCES,0.8056112224448898,"𝑓(𝑥) = 1.00
EPE-mod"
REFERENCES,0.8076152304609219,"𝑓(𝑥) = 1.00
DISSECT"
REFERENCES,0.8096192384769539,"𝑓(𝑥) = 1.00
𝑓(𝑥) = 1.00"
REFERENCES,0.811623246492986,"𝐶𝑇,-*..1"
REFERENCES,0.8136272545090181,"𝐶𝑇/, ,-*.."
REFERENCES,0.8156312625250501,"𝑓(𝑥) = 0.00
Query"
REFERENCES,0.8176352705410822,"𝑓(𝑥) = 1.00
EPE-mod"
REFERENCES,0.8196392785571143,"𝑓(𝑥) = 1.00
DISSECT"
REFERENCES,0.8216432865731463,"𝑓(𝑥) = 1.00
𝑓(𝑥) = 1.00"
REFERENCES,0.8236472945891784,"𝐶𝑇*, ,-*.."
REFERENCES,0.8256513026052105,"𝐶𝑇/, ,-*.."
REFERENCES,0.8276553106212425,"Figure 7: Qualitative results on 3D Shapes when ﬂipping classiﬁcation outcome from “False” to
“True.” We observe that EPE-mod converges to ﬁnding the same concept, despite having the ability
to express multiple pathways to switch the classiﬁer outcome. However, DISSECT can discover the
two Distinct ground-truth concepts: CT1 ﬂips the ﬂoor color to cyan, and CT2 converts the shape
color to red."
REFERENCES,0.8296593186372746,"𝑓(𝑥) =  1.00
𝑓(𝑥) = 0.00
𝑓(𝑥) = 0.00"
REFERENCES,0.8316633266533067,"𝑓(𝑥) = 0.00
𝑓(𝑥) = 0.00"
REFERENCES,0.8336673346693386,"𝑓(𝑥) = 1.00
Query"
REFERENCES,0.8356713426853707,"𝑓(𝑥) = 0.00
EPE-mod"
REFERENCES,0.8376753507014028,"𝑓(𝑥) = 0.00
DISSECT"
REFERENCES,0.8396793587174348,"𝑓(𝑥) = 0.00
𝑓(𝑥) = 0.00"
REFERENCES,0.8416833667334669,"𝐶𝑇*, ,-*.."
REFERENCES,0.843687374749499,"𝐶𝑇*, ,-..."
REFERENCES,0.845691382765531,"𝐶𝑇/, ,-..."
REFERENCES,0.8476953907815631,"𝐶𝑇/, ,-*.."
REFERENCES,0.8496993987975952,"Query
EPE-mod
DISSECT"
REFERENCES,0.8517034068136272,"𝑓(𝑥) = 1.00
Query"
REFERENCES,0.8537074148296593,"𝑓(𝑥) = 0.00
EPE-mod"
REFERENCES,0.8557114228456913,"𝑓(𝑥) = 0.00
DISSECT"
REFERENCES,0.8577154308617234,"𝑓(𝑥) = 0.00
𝑓(𝑥) = 0.00"
REFERENCES,0.8597194388777555,"𝐶𝑇*, ,-*.."
REFERENCES,0.8617234468937875,"𝐶𝑇/, ,-*..
Shape: Red
Floor: Not Cyan"
REFERENCES,0.8637274549098196,"Shape: Not Red
Floor: Cyan"
REFERENCES,0.8657314629258517,"Shape: Red
Floor: Cyan"
REFERENCES,0.8677354709418837,"Figure 8: Qualitative results on 3D Shapes when ﬂipping classiﬁcation outcome from “True” to
“False.” We observe that EPE-mod converges to ﬁnding the same concept, despite having the ability
to express multiple pathways to switch the classiﬁer outcome. However, DISSECT is capable of
discovering Distinct paths to do so. Left: When the input query has a red shape, but the ﬂoor color
is not cyan, CT1 ﬂips the shape color to orange and CT2 ﬂips it to violet. Middle: When the input
query has a cyan ﬂoor, but the shape color is not red, CT1 ﬂips the ﬂoor color to lime, and CT2
converts it to magenta. Right: When the input query has a red shape and cyan ﬂoor, CT1 changes
the shape color to dark orange and ﬂoor color to lime, and CT2 ﬂips the shape color to violet and
ﬂoor color to magenta."
REFERENCES,0.8697394789579158,"A.5.3
LIMITATIONS"
REFERENCES,0.8717434869739479,"While we provide several qualitative examples, further conﬁrmation from human-subject studies to
validate that CTs exhibit semantically meaningful attributes could strengthen our ﬁndings."
REFERENCES,0.87374749498998,"The number of concepts to be discovered by DISSECT is a hyper parameter that can be selected
by the user. While Distinctness and Substitutability are designed to be globally evaluated across
all concepts, Importance, Realism, and Stability scores can be calculated for each concept. We
hypothesize that ranking the discovered concepts based on these individual scores can beneﬁt the
users. In particular, it can help them focus on more salient concepts and not be overwhelmed by less
informative concepts. Future user-studies are required to investigate these hypotheses."
REFERENCES,0.875751503006012,"We emphasize that our proposed method does not guarantee ﬁnding all the biases of a classiﬁer, nor
ensures semantic meaningfulness across all found concepts. One avenue for future explainability
work is extending earlier theories [45, 71] that obtain disentanglement guarantees."
REFERENCES,0.8777555110220441,Published as a conference paper at ICLR 2022
REFERENCES,0.8797595190380761,𝑓(𝑥) = 0.0 𝐶𝑇 𝐶𝑇) 𝐶𝑇*
REFERENCES,0.8817635270541082,"𝛼= 0.0
𝛼= 0.25
𝛼= 1.0
𝛼= 0.5
𝛼= 0.75"
REFERENCES,0.8837675350701403,"𝑓(𝑥) = 0.00
𝑓(𝑥) = 0.00
𝑓(𝑥) = 0.05
𝑓(𝑥) = 0.33
𝑓(𝑥) = 0.81"
REFERENCES,0.8857715430861723,"𝑓(𝑥) = 0.00
𝑓(𝑥) = 0.00
𝑓(𝑥) = 0.05
𝑓(𝑥) = 0.38
𝑓(𝑥) = 0.83 𝐶𝑇) 𝐶𝑇*"
REFERENCES,0.8877755511022044,"𝑓(𝑥) = 0.00
𝑓(𝑥) = 0.00
𝑓(𝑥) = 0.03
𝑓(𝑥) = 0.43
𝑓(𝑥) = 0.99"
REFERENCES,0.8897795591182365,"𝑓(𝑥) = 0.00
𝑓(𝑥) = 0.00
𝑓(𝑥) = 0.02
𝑓(𝑥) = 0.15
𝑓(𝑥) = 0.92"
REFERENCES,0.8917835671342685,"𝑓(𝑥) = 0.00
𝑓(𝑥) = 0.00
𝑓(𝑥) = 0.02
𝑓(𝑥) = 0.12
𝑓(𝑥) = 0.72"
REFERENCES,0.8937875751503006,𝑓(𝑥) = 0.0
REFERENCES,0.8957915831663327,"Query
Image EPE"
REFERENCES,0.8977955911823647,EPE-mod
REFERENCES,0.8997995991983968,DISSECT
REFERENCES,0.9018036072144289,"𝛼= 0.0
𝛼= 0.25
𝛼= 1.0
𝛼= 0.5
𝛼= 0.75"
REFERENCES,0.9038076152304609,"𝑓(𝑥) = 0.00
𝑓(𝑥) = 0.24
𝑓(𝑥) = 0.95
𝑓(𝑥) = 1.00
𝑓(𝑥) = 1.00"
REFERENCES,0.905811623246493,"𝑓(𝑥) = 0.00
𝑓(𝑥) = 0.26
𝑓𝑥= 0.96
𝑓(𝑥) = 1.00
𝑓(𝑥) = 1.00"
REFERENCES,0.9078156312625251,"𝑓(𝑥) = 0.00
𝑓(𝑥) = 0.41
𝑓(𝑥) = 0.84
𝑓(𝑥) = 0.97
𝑓(𝑥) = 1.00"
REFERENCES,0.9098196392785571,"𝑓(𝑥) = 0.00
𝑓(𝑥) = 0.06
𝑓(𝑥) = 0.38
𝑓(𝑥) = 0.85
𝑓(𝑥) = 1.00"
REFERENCES,0.9118236472945892,"𝑓(𝑥) = 0.00
𝑓(𝑥) = 0.08
𝑓(𝑥) = 0.43
𝑓(𝑥) = 0.89
𝑓(𝑥) = 1.00"
REFERENCES,0.9138276553106213,Correlated Ground-
REFERENCES,0.9158316633266533,truth Concepts:
REFERENCES,0.9178356713426854,"Bangs
Blond Hair"
REFERENCES,0.9198396793587175,Classifying:
REFERENCES,0.9218436873747495,Smiling
REFERENCES,0.9238476953907816,"Query
Image"
REFERENCES,0.9258517034068137,"Figure 9: Qualitative results on CelebA. A biased classiﬁer has been trained to predict smile prob-
ability, where the training dataset has been sub-sampled such that smiling co-occurs only with
“bangs” and “blond hair” attributes. EPE does not support multiple CTs. We observe that EPE-
mod converges to ﬁnding the same concept, despite having the ability to express several pathways to
change f(¯x) through CT1 and CT2. However, DISSECT can discover Distinct routes: CT1 mainly
changes hair color to blond, and CT2 does not alter hair color but focuses more on hairstyle and tries
to add bangs. Thus it identiﬁes two otherwise hidden biases."
REFERENCES,0.9278557114228457,"Table 6: Ablation experiment. Different rows show results for CelebA, with different λr values.
All the other variables are the same across experiments: λcGAN = 1, λrec = 100, λf = 1."
REFERENCES,0.9298597194388778,"Importance
Realism
Distinctness
Substitutability
Stability"
REFERENCES,0.9318637274549099,"λr
↑R
↑ρ
↓KL
↓MSE
↑CF
Acc"
REFERENCES,0.9338677354709419,"↑CF
Prec"
REFERENCES,0.935871743486974,"↑CF
Rec
↓Acc
↓Prec
↓Rec
↑Acc
↑Prec
(micro)"
REFERENCES,0.9378757515030061,"↑Prec
(macro)"
REFERENCES,0.9398797595190381,"↑Rec
(micro)"
REFERENCES,0.9418837675350702,"↑Rec
(macro)"
REFERENCES,0.9438877755511023,"↑Acc
Sub"
REFERENCES,0.9458917835671342,"↑Prec
Sub"
REFERENCES,0.9478957915831663,"↑Rec
Sub"
REFERENCES,0.9498997995991983,"↓CF
MSE"
REFERENCES,0.9519038076152304,"↓Prob
JSD
EPE-mod
0
0.859
0.904
0.212
0.048
99.465
99.749
99.180
49.26
33.333
0.118
18.704
50.008
50.108
15.217
15.237
91.810
94.824
89.434
0.446
0.004
DISSECT
0.01
0.813
0.868
0.329
0.067
98.4
99.8
97.0
49.8
0.0
0.0
36.1
53.6
54.5
47.3
47.4
98.4
95.1
85.2
0.796
0.004
DISSECT
0.1
0.866
0.915
0.246
0.046
99.448
99.829
99.065
49.64
36.364
0.799
82.304
93.145
93.210
82.385
82.394
81.122
97.291
66.236
0.453
0.003
DISSECT
1
0.821
0.877
0.309
0.060
98.640
99.785
97.490
49.480
0.000
0.000
96.024
97.594
97.595
97.730
97.732
90.524
92.367
89.524
0.613
0.004
DISSECT
2
0.843
0.882
0.188
0.047
99.165
99.843
98.485
49.16
0.0
0.0
94.980
98.042
98.108
96.056
96.053
91.938
96.937
87.559
0.567
0.005
DISSECT
10
0.044
0.110
2.528
0.324
52.122
52.174
50.945
98.32
98.917
97.740
96.012
98.129
98.132
97.162
97.163
43.033
29.333
5.293
0.414
0.002
DISSECT
100
0.207
0.144
1.416
0.270
59.76
83.096
24.505
99.68
99.841
99.524
96.328
97.582
97.582
98.170
98.170
54.863
63.840
34.266
0.180
0.001"
REFERENCES,0.9539078156312625,"A.6
ABLATION/SENSITIVITY EXPERIMENTS"
REFERENCES,0.9559118236472945,"EPE-mod can be viewed as the ablated version of DISSECT where λr = 0. For a more detailed anal-
yses of the inﬂuence of the disentanglement element, we conducted an experiment with different λr
while ﬁxing the other hyperparameters of the model. As seen in Table 6, increasing λr up to a point
improves Distinctness and retains performance on the other criteria of interest. Further increasing
λr hurts the quality of generated images, and thus negatively impacts Realism and Substitutability."
REFERENCES,0.9579158316633266,"A.7
INDIVIDUAL CONCEPT INFLUENCE"
REFERENCES,0.9599198396793587,"Note that metrics such as Distinctness and Substitutability are not meaningful for each concept.
However, Importance, Realism, and Stability can be calculated on each concept separately, or over
all concepts. Calculating such metrics for each concept can help us rank them with respect to that
criterion. For example, Importance scores per concept can tell us which concept is more important"
REFERENCES,0.9619238476953907,Published as a conference paper at ICLR 2022
REFERENCES,0.9639278557114228,"Table 7:
Individual Importance scores per concept discovered by DISSECT. Discovered
3D Shapes concepts: 1) Red shape, 2) Cyan ﬂoor. Discovered SynthDerm concepts: 1) Color,
2) Diameter, 3) Asymmetry, 4) Border, 5) Surgical markings and color. Discovered CelebA con-
cepts: 1) Blond hair, 2) Bangs."
REFERENCES,0.9659318637274549,Importance
REFERENCES,0.9679358717434869,"↑R
↑ρ
↓KL
↓MSE
↑CF
↑CF
↑CF
Acc
Prec
Rec"
D SHAPES,0.969939879759519,"3D Shapes
CT1
0.823
0.843
1.421
0.084
92.32
100.0
98.64
CT2
0.822
0.766
1.794
0.085
98.15
100.0
96.3"
D SHAPES,0.9719438877755511,SynthDerm
D SHAPES,0.9739478957915831,"CT1
0.908
0.747
0.383
0.021
97.615
91.886
89.671
CT2
0.895
0.746
0.423
0.023
97.215
90.191
88.294
CT3
0.935
0.75
0.276
0.015
98.26
93.61
93.037
CT4
0.929
0.749
0.303
0.016
98.195
93.244
92.923
CT5
0.919
0.748
0.349
0.018
97.935
92.605
91.507"
D SHAPES,0.9759519038076152,"CelebA
CT1
0.845
0.882
0.184
0.046
99.08
99.817
98.34
CT2
0.842
0.882
0.192
0.047
99.25
99.868
98.63"
D SHAPES,0.9779559118236473,"Table 8: Interaction between generation quality and explanation quality. Three different sizes have
been considered for the underlying GAN architecture: xxsmall, small, base. For comparison,
a GAN-only version of base is also included, where all the other components are disabled, i.e.
λr = 0, λrec = 0, and λf = 0."
D SHAPES,0.9799599198396793,"Importance
Realism
Distinctness"
D SHAPES,0.9819639278557114,"↑R
↑ρ
↓KL
↓MSE
↓FID
↓Acc
↓Prec
↓Rec
↑Acc
↑Prec
(micro)"
D SHAPES,0.9839679358717435,"↑Prec
(macro)"
D SHAPES,0.9859719438877755,"↑Rec
(micro)"
D SHAPES,0.9879759519038076,"↑Rec
(macro)
GAN-only
0.000
0.000
6.049
0.412
7.367
49.32
0.0
0.0
33.287
0.0
0.0
0.0
0.0"
D SHAPES,0.9899799599198397,"xxsmall
0.0
0.0
11.526
0.414
110.964
100.0
100.0
100.0
33.3
0.0
0.0
0.0
0.0
small
0.822
0.803
2.915
0.085
51.002
99.08
99.15
98.99
99.980
99.990
99.990
99.980
99.980
base
0.84
0.88
0.19
0.047
42.109
49.2
0.0
0.0
95.0
98.0
98.1
96.1
96.1"
D SHAPES,0.9919839679358717,"in the decision making of the classiﬁer. Table 7 summarizes the Importance scores per concept. As
expected, given the symmetric design of our experiments, all discovered concepts exhibit similar
Importance scores."
D SHAPES,0.9939879759519038,"A.8
INTERACTION BETWEEN GENERATION QUALITY AND EXPLANATION QUALITY"
D SHAPES,0.9959919839679359,"In this section, we evaluate DISSECT with different sizes of underlying GAN architectures. As
shown in Table 8, even with imperfect generation quality, DISSECT can still discover relevant con-
cepts with comparable performance in terms of Importance, Distinctness. This is despite signiﬁcant
impacts on the Realism scores. Perfect generation quality can further improve realism. However, a
reasonable generation quality is sufﬁcient for successful explanation with DISSECT."
D SHAPES,0.9979959919839679,"With the recent advances in generative modeling that have achieved great generation quality over
a majority of domains, it is timely to utilize them when available for generating realistic-looking
explanations. It is worth mentioning that a perfect generation alone is not sufﬁcient for good ex-
planations in terms of performance criteria other than Realism. For comparison, we have calculated
performance scores of a strong generative model, excluding the interpretability-related terms in the
objective function. See the results in Table 8 for the GAN-only row."
