Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0006514657980456026,"We focus on a class of reinforcement learning algorithms, Monte-Carlo Tree Search
1"
ABSTRACT,0.0013029315960912053,"(MCTS), in stochastic settings. While recent advancements combining MCTS with
2"
ABSTRACT,0.001954397394136808,"deep learning have excelled in deterministic environments, they face challenges
3"
ABSTRACT,0.0026058631921824105,"in highly stochastic settings, leading to suboptimal action choices and decreased
4"
ABSTRACT,0.003257328990228013,"performance. Distributional Reinforcement Learning (RL) addresses these chal-
5"
ABSTRACT,0.003908794788273616,"lenges by extending the traditional Bellman equation to consider value distributions
6"
ABSTRACT,0.004560260586319218,"instead of a single mean value, showing promising results in Deep Q Learning.
7"
ABSTRACT,0.005211726384364821,"In this paper, we bring the concept of Distributional RL to MCTS, focusing on
8"
ABSTRACT,0.005863192182410423,"modeling value functions as categorical and particle distributions. Consequently,
9"
ABSTRACT,0.006514657980456026,"we propose two novel algorithms: Categorical Thompson Sampling for MCTS
10"
ABSTRACT,0.0071661237785016286,"(CATS), which uses categorical distributions for Q values, and Particle Thompson
11"
ABSTRACT,0.007817589576547232,"Sampling for MCTS (PATS), which models Q values with particle-based distri-
12"
ABSTRACT,0.008469055374592834,"butions. Both algorithms employ Thompson Sampling to handle action selection
13"
ABSTRACT,0.009120521172638436,"randomness. Our contributions are threefold: We introduce a distributional frame-
14"
ABSTRACT,0.009771986970684038,"work for Monte-Carlo Planning to model uncertainty in return estimation. We
15"
ABSTRACT,0.010423452768729642,"prove the effectiveness of our algorithms by achieving a non-asymptotic problem-
16"
ABSTRACT,0.011074918566775244,"dependent upper bound on simple regret of order O(n−1), where n is the number
17"
ABSTRACT,0.011726384364820847,"of trajectories. We provide empirical evidence demonstrating the efficacy of our
18"
ABSTRACT,0.012377850162866449,"approach compared to baselines in both stochastic and deterministic environments.
19"
INTRODUCTION,0.013029315960912053,"1
Introduction
20"
INTRODUCTION,0.013680781758957655,"Online planning in Markov decision processes (MDPs) involves making real-time decisions based on
21"
INTRODUCTION,0.014332247557003257,"the current state of the environment. It requires balancing exploration and exploitation while handling
22"
INTRODUCTION,0.01498371335504886,"uncertainty and partial observability. Monte Carlo Tree Search (MCTS) is a highly effective online
23"
INTRODUCTION,0.015635179153094463,"planning method for tackling complex MDPs. MCTS has shown impressive performance in various
24"
INTRODUCTION,0.016286644951140065,"tasks, including traditional board games like Chess and Go, video games, and real-world challenges.
25"
INTRODUCTION,0.016938110749185668,"Notable successes include advancements in Chess (35) and Go (34; 36; 30), video game strategy (28),
26"
INTRODUCTION,0.01758957654723127,"robot assembly (16), robot path planning (15; 13), and autonomous driving (24).
27"
INTRODUCTION,0.018241042345276872,"Despite these achievements, current MCTS methods are primarily effective in deterministic environ-
28"
INTRODUCTION,0.018892508143322474,"ments, often overlooking the significant impact of randomness in real-world scenarios. In highly
29"
INTRODUCTION,0.019543973941368076,"stochastic and partially observable environments, conventional MCTS approaches face substantial
30"
INTRODUCTION,0.020195439739413682,"challenges due to widespread randomness and limited observability. This leads to compromised value
31"
INTRODUCTION,0.020846905537459284,"estimates, suboptimal decisions, and diminished overall performance. Therefore, there is a clear need
32"
INTRODUCTION,0.021498371335504887,"for improved methods capable of navigating the complexities of randomness and partial observability
33"
INTRODUCTION,0.02214983713355049,"in value estimation.
34"
INTRODUCTION,0.02280130293159609,"We now review related works to understand the advancements and limitations in these areas.
35"
INTRODUCTION,0.023452768729641693,"Related work In MCTS, value estimation methods and action selection rules are critical factors for
36"
INTRODUCTION,0.024104234527687295,"algorithm performance. Traditional value estimation methods, such as using empirical average mean
37"
INTRODUCTION,0.024755700325732898,"for value backup as in the Upper Confidence bounds applied to Trees method (UCT) (21), suffer from
38"
INTRODUCTION,0.025407166123778503,"underestimation of optimal values while maximum backup suffers from overestimation of optimal
39"
INTRODUCTION,0.026058631921824105,"values (9). The power mean estimator (12) offers a balanced solution by computing a mean between
40"
INTRODUCTION,0.026710097719869708,"the average and maximum values. In our approach, we also use power mean for value operator as
41"
INTRODUCTION,0.02736156351791531,"each V node stores the power mean of empirical means of succeeding Q-value nodes, eliminating the
42"
INTRODUCTION,0.028013029315960912,"need for V to be modeled as a distribution.
43"
INTRODUCTION,0.028664495114006514,"For action selection in MCTS, strategies from Multi-Armed Bandits (MAB) are commonly employed.
44"
INTRODUCTION,0.029315960912052116,"For instance, UCT extends the UCB1 strategy from bandits to the tree by computing confidence
45"
INTRODUCTION,0.02996742671009772,"intervals at each step. However, original UCT’s performance is hindered by the incorrect choice of
46"
INTRODUCTION,0.03061889250814332,"logarithmic bonus constant (32). Shah et al. (32) propose an adapted version of UCT incorporating a
47"
INTRODUCTION,0.031270358306188926,"polynomial bonus term instead of the ""logarithmic"" bonus term in UCT and show the non-asymtotic
48"
INTRODUCTION,0.031921824104234525,"convergence of rate O(n−1/2), with n is the number of rollout trajectories. On the other hand, our
49"
INTRODUCTION,0.03257328990228013,"method improves over this rate with theoretical guarantee of O(n−1). Although Thompson sampling
50"
INTRODUCTION,0.03322475570032573,"has been less explored in MCTS, some approaches like those by Bai et al. (1) and Bai et al. (2)
51"
INTRODUCTION,0.033876221498371335,"incorporate it for exploration. However, these methods lack convergence rate analysis. Furthermore,
52"
INTRODUCTION,0.03452768729641694,"in the article Bai et al. (1), authors model value functions as a mixture of Normal distributions, which
53"
INTRODUCTION,0.03517915309446254,"may lack the generality of complex real-world scenarios. Our approach adopts Thompson sampling
54"
INTRODUCTION,0.035830618892508145,"for action selection but introduces a novelty by modeling the uncertainty of action value estimates
55"
INTRODUCTION,0.036482084690553744,"over the tree as arbitrary categorical and particle-based distributions. This modification enhances our
56"
INTRODUCTION,0.03713355048859935,"ability to handle more generality in highly stochastic environments effectively.
57"
INTRODUCTION,0.03778501628664495,"Entropy regularization techniques in RL modify value and action selection functions to balance
58"
INTRODUCTION,0.038436482084690554,"exploration and exploitation, leading to improved value estimation (25; 17; 31; 18). Several works
59"
INTRODUCTION,0.03908794788273615,"have applied these techniques in MCTS. Maximum Entropy Tree Search (MENTS) (40) emphasizes
60"
INTRODUCTION,0.03973941368078176,"exploration by integrating MCTS with maximum entropy policy optimization. MENTS aims to
61"
INTRODUCTION,0.040390879478827364,"maximize cumulative rewards and policy entropy concurrently, regulated by a temperature parameter.
62"
INTRODUCTION,0.04104234527687296,"Dam et al. (14) extend MENTS by incorporating Relative and Tsallis entropy, leading to the RENTS
63"
INTRODUCTION,0.04169381107491857,"and TENTS algorithms. However, the effectiveness of MENTS/RENTS/TENTS hinges on the
64"
INTRODUCTION,0.04234527687296417,"temperature parameter, which may impede convergence. Furthermore, the value estimation converges
65"
INTRODUCTION,0.04299674267100977,"exponentially to the regularized value not the optimal one. In contrast, Painter et al. (27) utilize
66"
INTRODUCTION,0.04364820846905537,"a similar action selection approach but employ a maximum backup operator for value estimation.
67"
INTRODUCTION,0.04429967426710098,"Although their method exhibits exponential decay of simple regret, it heavily relies on the sensitivity
68"
INTRODUCTION,0.04495114006514658,"of the temperature parameter for Boltzmann Exploration, limiting its practicality.
69"
INTRODUCTION,0.04560260586319218,"Distributional Reinforcement Learning (RL) (6; 11; 22) addresses the randomness of the value
70"
INTRODUCTION,0.04625407166123779,"estimation by introducing a distributional perspective to the traditional Bellman equation. This
71"
INTRODUCTION,0.046905537459283386,"approach views the value function as a distribution rather than a single mean, providing a compre-
72"
INTRODUCTION,0.04755700325732899,"hensive understanding of uncertainties in rewards and the stochasticity from environments. Through
73"
INTRODUCTION,0.04820846905537459,"discretization (26), parameterization (6), and quantization (10), it allows for efficient and effective
74"
INTRODUCTION,0.048859934853420196,"approximation of value distributions, leading to improved performance in various RL tasks. However,
75"
INTRODUCTION,0.049511400651465795,"these results are only for learning not for planning.
76"
INTRODUCTION,0.0501628664495114,"Outline and contribution In this work, we integrate the distributional approach from reinforcement
77"
INTRODUCTION,0.050814332247557006,"learning (RL) into the planning framework to tackle the challenges of planning in stochastic environ-
78"
INTRODUCTION,0.051465798045602605,"ments. We focus on modeling value functions as categorical and particle distributions. Consequently,
79"
INTRODUCTION,0.05211726384364821,"we propose two novel algorithms: Categorical Thompson Sampling for MCTS (CATS) and Particle
80"
INTRODUCTION,0.05276872964169381,"Thompson Sampling for MCTS (PATS). CATS represents each Q value function as a categorical
81"
INTRODUCTION,0.053420195439739415,"distribution and uses Thompson Sampling for action selection to manage uncertainty. PATS models
82"
INTRODUCTION,0.054071661237785014,"each Q value function with a particle-based distribution, using a nuanced Thompson Sampling
83"
INTRODUCTION,0.05472312703583062,"approach to handle action selection randomness.
84"
INTRODUCTION,0.05537459283387622,"Our contributions are threefold:
85"
INTRODUCTION,0.056026058631921824,"(i)
In section 3, we introduce a distributional framework for planning to model uncertainty in
86"
INTRODUCTION,0.05667752442996743,"return estimation, enhancing the robustness of value estimation in stochastic environments.
87"
INTRODUCTION,0.05732899022801303,"(ii)
In section 4 Theorem 5 and Theorem 6, we prove the effectiveness of our algorithms by
88"
INTRODUCTION,0.057980456026058634,"achieving a non-asymptotic problem-dependent upper bound on simple regret of O(n−1),
89"
INTRODUCTION,0.05863192182410423,"which significantly improves upon the current state-of-the-art theoretical analysis of regret,
90"
INTRODUCTION,0.05928338762214984,"previously established at O(n−1/2) by Shah et al. (33).
91"
INTRODUCTION,0.05993485342019544,"(iii)
In section 5, we provide comprehensive empirical evidence demonstrating the efficacy of
92"
INTRODUCTION,0.06058631921824104,"our approach compared to baselines, showcasing competitive performance in stochastic
93"
INTRODUCTION,0.06123778501628664,"settings and the Atari benchmark.
94"
INTRODUCTION,0.06188925081433225,"In the next section, we describe the problem setting addressed in this paper.
95"
SETTING,0.06254071661237785,"2
Setting
96"
SETTING,0.06319218241042346,"In our study, We address the dynamics of an agent navigating an infinite-horizon discounted Markov
97"
SETTING,0.06384364820846905,"decision process (MDP), defined formally as M = ⟨S, A, R, P, γ⟩. Here, S represents the state
98"
SETTING,0.06449511400651466,"space, A denotes the set of actions, and R quantifies the Reward function of the MDP (R : S ×
99"
SETTING,0.06514657980456026,"A × S →R). Transition dynamics are governed by P(S × A →S), with γ ∈(0, 1] as the discount
100"
SETTING,0.06579804560260587,"factor. The agent interacts with the environment via a policy π ∈Π : S →A, guiding action
101"
SETTING,0.06644951140065146,"selection based on observed states. This yields an action-value function Qπ, indicating the expected
102"
SETTING,0.06710097719869706,"cumulative discounted reward from a state-action pair under π. The agent seeks the optimal policy
103"
SETTING,0.06775244299674267,"maximizing the action-value function, adhering to the Bellman equation (7), given by Q(s, a) ≜
104
R"
SETTING,0.06840390879478828,"S P(s′|s, a)[R(s, a, s′) + γ maxa′ Q(s′, a′)]ds for all states s and actions a. Upon acquiring the
105"
SETTING,0.06905537459283388,"optimal action-value function, we derive the optimal value function V (s) ≜maxa∈A Q(s, a) for all
106"
SETTING,0.06970684039087947,"states s in S.
107"
SETTING,0.07035830618892508,"Monte-Carlo tree search (MCTS) (20; 8) is a planning approach for complex Markov decision
108"
SETTING,0.07100977198697069,"processes (MDPs). It employs an iterative approach:
109"
SETTING,0.07166123778501629,"Selection: It begins by selecting an action using a specified strategy, followed by executing this action
110"
SETTING,0.07231270358306188,"through Monte Carlo simulation.
111"
SETTING,0.07296416938110749,"Expansion: Subsequently, it assesses the resulting state, either by recursively evaluating if it already
112"
SETTING,0.0736156351791531,"exists in the search tree or by inserting it into the tree.
113"
SETTING,0.0742671009771987,"Simulation: Or employing a rollout policy via simulations. This iterative process continues until
114"
SETTING,0.0749185667752443,"certain termination criteria are met, allowing traversal through the search tree.
115"
SETTING,0.0755700325732899,"Backpropagation: Finally, the outcomes of the simulations are propagated backward through the
116"
SETTING,0.0762214983713355,"chosen nodes to update their statistical metrics.
117"
SETTING,0.07687296416938111,"Simple Regret An MCTS algorithm dynamically gathers trajectories within an MDP starting from
118"
SETTING,0.07752442996742671,"an initial state s0. After processing t trajectories, it provides two outputs:
119"
SETTING,0.0781758957654723,"•
bat, a guess for the best action to take at state s0
120"
SETTING,0.07882736156351791,"•
bVt(s0) an estimator of the optimal value in s0,
121"
SETTING,0.07947882736156352,"where s0 is the state at the root node. The algorithm’s performance can be assessed by its convergence
122"
SETTING,0.08013029315960912,"rate r(t) of the simple regret, formulated as:
123"
SETTING,0.08078175895765473,"E [R(s0, t)] = E
h
V ⋆(s0) −bVt (s0)
i
≤r(t),"
SETTING,0.08143322475570032,"Here, R(s0, t) = V ⋆(s0) −bVt(s0) is the simple regret of the algorithm at the root node with V ⋆(s0)
124"
SETTING,0.08208469055374593,"representing the optimal value at state s0.
125"
SETTING,0.08273615635179153,"In this article, we analyze an MCTS algorithm employing a maximal planning horizon H and
126"
SETTING,0.08338762214983714,"a playout policy π0 with value V0. We define eV (sH) = V0(sH) recursively as follows: for all
127"
SETTING,0.08403908794788274,"h ≤H −1,
128"
SETTING,0.08469055374592833,"eQ(sh, a) = r(sh, a) + γ P"
SETTING,0.08534201954397394,"sh+1∈Ash P(sh+1|sh, a)eV (sh+1), eV (sh) = maxa eQ(sh, a),
(1)"
SETTING,0.08599348534201955,"where r(sh, a) defined formally as the mean intermediate reward at state sh after taking action a.
129"
SETTING,0.08664495114006515,"The primary objective of an MCTS algorithm is to estimate a tied rate r(t) by constructing estimates
130"
SETTING,0.08729641693811074,"of eQ(sh, a) and eV (sh) to ultimately estimate eQ(s0, a) and consequently Q⋆(s0, a). In practical im-
131"
SETTING,0.08794788273615635,"plementations of the MCTS algorithm, the maximal depth H can sometimes be set to +∞. However,
132"
SETTING,0.08859934853420195,"for theoretical analysis, the maximal depth H is crucial as we will analyze the algorithm that always
133"
SETTING,0.08925081433224756,"collects trajectories of length H.
134"
SETTING,0.08990228013029317,"Distributional Reinforcement Learning The mathematical framework used in reinforcement learn-
135"
SETTING,0.09055374592833876,"ing is based on the Bellman equation (37), which aims to find an agent to maximize the expected
136"
SETTING,0.09120521172638436,"utility Q value. However, the single expected value function cannot encapsulate the stochasticity in
137"
SETTING,0.09185667752442997,"the reward function and the dynamic of the environments. Recently, in the article (5), authors shed
138"
SETTING,0.09250814332247558,"light on the distributional perspective of the Bellman equation by modeling each Q value function as
139"
SETTING,0.09315960912052117,"a distribution instead of a single expected value. The main objective is to study the random return Q
140"
SETTING,0.09381107491856677,"at the state s, action a, and is defined recursively as
141"
SETTING,0.09446254071661238,"Q(s, a)
D= X(s, a) + γQ(s
′, a
′), V(s
′)
D= EπQ(s
′, π(·|s
′)),
(2)"
SETTING,0.09511400651465798,"where X(s, a) is the reward distribution at the state s, action a, Q(s, a) is the Q value distribution
142"
SETTING,0.09576547231270359,"at state s, action a, and Q(s
′, a
′) is the Q value distribution at state s
′, action a
′. s
′ distributed
143"
SETTING,0.09641693811074918,"according to P(·|s, a), a
′ distributed according to a policy π(·|s
′). A
D= B denotes that two random
144"
SETTING,0.09706840390879479,"variables A and B have equal probability laws.
145"
SETTING,0.09771986970684039,"This distributional approach offers a deeper understanding of uncertainty and variability, especially
146"
SETTING,0.098371335504886,"in complex, stochastic systems where traditional expected value representations may fail to capture
147"
SETTING,0.09902280130293159,"the true dynamics of the problem. which has been successfully used in Deep Q Learning (5).
148"
SETTING,0.0996742671009772,"Categorical Value Distribution Based on the distributional Bellman equation, In the article (5), au-
149"
SETTING,0.1003257328990228,"thors approximate the Q value distribution Q(s, a) as a discrete categorical distribution parametrized
150"
SETTING,0.10097719869706841,"by N ∈N, which denotes the number of atoms (N+1) at fixed-sized locations. This method effectively
151"
SETTING,0.10162866449511401,"divides the Q value function into a set of equally spaced atoms zi(s, a) = Qmin + i△z : 0 ≤i ≤N,
152"
SETTING,0.1022801302931596,"where Qmin and Qmax are respectively the minimum and maximum values at state s, action a. The
153"
SETTING,0.10293159609120521,size of each atom is set as △z := Qmax−Qmin
SETTING,0.10358306188925082,"N
.
154"
SETTING,0.10423452768729642,"This discrete distribution approach is highly expressive and computationally efficient, making it ideal
155"
SETTING,0.10488599348534201,"for practical applications. For instance, in the article (5), authors successfully used this representation
156"
SETTING,0.10553745928338762,"in Deep Q Learning (C51), showing promising results in several Atari games. In the next section, we
157"
SETTING,0.10618892508143322,"demonstrate how to apply this idea to MCTS.
158"
DISTRIBUTIONAL THOMPSON SAMPLING IN TREE SEARCH,0.10684039087947883,"3
Distributional Thompson Sampling in Tree Search
159"
DISTRIBUTIONAL THOMPSON SAMPLING IN TREE SEARCH,0.10749185667752444,"In this section, we introduce two novel distributional approaches for MCTS based on Thompson
160"
DISTRIBUTIONAL THOMPSON SAMPLING IN TREE SEARCH,0.10814332247557003,"sampling. The first method represents each Q-value node as a categorical distribution, while the
161"
DISTRIBUTIONAL THOMPSON SAMPLING IN TREE SEARCH,0.10879478827361563,"second uses particle-based distributions for greater flexibility. Both methods integrate Thompson
162"
DISTRIBUTIONAL THOMPSON SAMPLING IN TREE SEARCH,0.10944625407166124,"sampling for improved exploration and performance.
163"
DISTRIBUTIONAL MONTE-CARLO TREE SEARCH,0.11009771986970684,"3.1
Distributional Monte-Carlo Tree Search
164"
DISTRIBUTIONAL MONTE-CARLO TREE SEARCH,0.11074918566775244,"We leverage the success of distributional reinforcement learning (4; 3; 6) and apply this concept to
165"
DISTRIBUTIONAL MONTE-CARLO TREE SEARCH,0.11140065146579804,"MCTS. In MCTS, there are two types of nodes: V-nodes and Q-value nodes. Instead of treating each
166"
DISTRIBUTIONAL MONTE-CARLO TREE SEARCH,0.11205211726384365,"V value and Q value as a single expected value, we model these functions as distributions.
167"
DISTRIBUTIONAL MONTE-CARLO TREE SEARCH,0.11270358306188925,"Based on equation (2), we can derive
168"
DISTRIBUTIONAL MONTE-CARLO TREE SEARCH,0.11335504885993486,"Q(s, a)
D= X(s, a) + γV(s
′), V(s
′)
D= P"
DISTRIBUTIONAL MONTE-CARLO TREE SEARCH,0.11400651465798045,"a′∼¯π(.|s′) Q(s
′, a
′),
(3)"
DISTRIBUTIONAL MONTE-CARLO TREE SEARCH,0.11465798045602606,"with s
′ ∼P(·|s, a), where ¯π(.|s
′) is formally defined as the tree policy at state s′. We can model
169"
DISTRIBUTIONAL MONTE-CARLO TREE SEARCH,0.11530944625407166,"any Q distribution with equal law distributed as the sum of the distributions of the next reward and
170"
DISTRIBUTIONAL MONTE-CARLO TREE SEARCH,0.11596091205211727,"the Q distributions of the next states actions. We further model each V distribution, having equal
171"
DISTRIBUTIONAL MONTE-CARLO TREE SEARCH,0.11661237785016286,"probability law to the expectation of the chosen policy of the next Q-value distributions (3).
172"
DISTRIBUTIONAL MONTE-CARLO TREE SEARCH,0.11726384364820847,"Our method follows the same four basic steps of MCTS but is different in Value Backup and Action
173"
DISTRIBUTIONAL MONTE-CARLO TREE SEARCH,0.11791530944625407,"selection steps. We introduce two distinct methodologies: categorical-based and particle-based. In
174"
DISTRIBUTIONAL MONTE-CARLO TREE SEARCH,0.11856677524429968,"the categorical based approach, we parameterize each V value and Q value function in the tree as a
175"
DISTRIBUTIONAL MONTE-CARLO TREE SEARCH,0.11921824104234528,"categorical distribution. In contrast, in the particle-based approach, we model each value distribution
176"
DISTRIBUTIONAL MONTE-CARLO TREE SEARCH,0.11986970684039087,"as a set of sampling particles, representing the values observed during the tree planning. We provide
177"
DISTRIBUTIONAL MONTE-CARLO TREE SEARCH,0.12052117263843648,"a detailed explanation for the value backup and action selection of each method in the next section.
178"
VALUE BACKUP,0.12117263843648209,"3.2
Value Backup
179"
VALUE BACKUP,0.12182410423452769,"In this work, we employ two approaches to represent the Q value distribution.
180"
VALUE BACKUP,0.12247557003257328,"Categorical distribution: we represent each node in the tree as a categorical distribution. In each
181"
VALUE BACKUP,0.12312703583061889,"Q-value node, we: (1) store the empirical mean value of that Q-value node (same as in UCT), and
182"
VALUE BACKUP,0.1237785016286645,"(2) maintain a categorical distribution of the Q value function. To define a categorical distribution Q
183"
VALUE BACKUP,0.1244299674267101,"function, we require three essential pieces of information:
184"
VALUE BACKUP,0.1250814332247557,"•
The number of atoms (N+1): We choose a consistent number of atoms (N+1) that remains
185"
VALUE BACKUP,0.1257328990228013,"the same for all Q distributions along the tree.
186"
VALUE BACKUP,0.12638436482084692,"•
Minimum and maximum values (min and max): Each node in the tree may have different
187"
VALUE BACKUP,0.1270358306188925,"ranges for its minimum (Qmin)1 and maximum (Qmax) values, depending on its state/action
188"
VALUE BACKUP,0.1276872964169381,"in the environment. When a new Q-value node is added to the tree, we initially set Qmin
189"
VALUE BACKUP,0.12833876221498372,"to 0 (assuming we have scaled the reward range to [0, R]) and initialize Qmax to a small
190"
VALUE BACKUP,0.1289902280130293,"1Since reward is scaled in [0, R], Qmin is not updated in our setup."
VALUE BACKUP,0.12964169381107493,"Algorithm 1 CATS
SelectAction (sh)
(Sec 3.2)
for a ∈[A] do"
VALUE BACKUP,0.13029315960912052,"L(sh, a) ∼Dir(α0(sh, a), . . . , αN(sh, a))
ϕ(sh, a) = [z0(sh, a), . . . , zN(sh, a)]⊤L(sh, a)"
VALUE BACKUP,0.13094462540716612,"a = arg max
a "
VALUE BACKUP,0.13159609120521173,"ϕ(sh, a)"
VALUE BACKUP,0.13224755700325733,"return a
SimulateV (sh, t)
(Sec 3.2)
a =SelectAction (sh)
SimulateQ (sh, a, t)
Tsh(t) = Tsh(t) + 1
bQ(sh, a) = P"
VALUE BACKUP,0.13289902280130292,"i
zi(sh, a)pi(sh, a)"
VALUE BACKUP,0.13355048859934854,"bV (sh) =
P a"
VALUE BACKUP,0.13420195439739413,"Tsh,a(t)"
VALUE BACKUP,0.13485342019543975,"Tsh(t) bQp(sh, a)
 1 p"
VALUE BACKUP,0.13550488599348534,"SimulateQ (sh, a, t)
(Sec 3.2)
sh+1 ∼P(·|sh, a), rt(sh, a) ∼R(sh, a, sh+1)
if Node sh+1 not expanded then"
VALUE BACKUP,0.13615635179153093,Rollout(sh+1) else
VALUE BACKUP,0.13680781758957655,"SimulateV (sh+1, t)"
VALUE BACKUP,0.13745928338762214,"Tsh,a(t) = Tsh,a(t) + 1
Qt(sh, a) = rt(sh, a) + γ bV (sh+1)
if Qt(sh, a) ̸∈[Qmin(sh, a), Qmax(sh, a)] then"
VALUE BACKUP,0.13811074918566776,"Qmax(sh, a) = max{Qt(sh, a), Qmax(sh, a)}
Qmin(sh, a) = min{Qt(sh, a), Qmin(sh, a)}
△z = Qmax−Qmin"
VALUE BACKUP,0.13876221498371336,"N
zi(sh, a) = Qmin + i△z : 0 ≤i ≤N"
VALUE BACKUP,0.13941368078175895,"Update p(sh, a) = [p0(sh, a), . . . , pN(sh, a)]"
VALUE BACKUP,0.14006514657980457,"Algorithm 2 PATS
SelectAction (sh)
(Sec 3.2)
for a ∈[A] do"
VALUE BACKUP,0.14071661237785016,"L(sh, a) ∼Dir(α(sh, a))
ϕ(sh, a) = S(sh, a)⊤L(sh, a)"
VALUE BACKUP,0.14136807817589578,"a = arg max
a "
VALUE BACKUP,0.14201954397394137,"ϕ(sh, a)"
VALUE BACKUP,0.14267100977198696,"return a
SimulateV (sh, t)
(Sec 3.2)
a =SelectAction (sh)
SimulateQ (sh, a, t)
Ts(t) = Ts(t) + 1
bQ(sh, a) = P αt(sh, a)Qt(sh, a)"
VALUE BACKUP,0.14332247557003258,"bV (sh) =
P a"
VALUE BACKUP,0.14397394136807817,"Tsh,a(t)"
VALUE BACKUP,0.14462540716612377,"Tsh(t) bQp(s, a)
 1 p"
VALUE BACKUP,0.14527687296416938,"SimulateQ (sh, a, t)
(Sec 3.2)
sh+1 ∼P(·|sh, a), rt(sh, a) ∼R(sh, a, sh+1)
if Node sh+1 not expanded then"
VALUE BACKUP,0.14592833876221498,Rollout(sh+1) else
VALUE BACKUP,0.1465798045602606,"SimulateV (sh+1, t)"
VALUE BACKUP,0.1472312703583062,"Tsh,a(t) = Tsh,a(t) + 1
Qt(sh, a) = rt(sh, a) + γ bV (sh+1)
if Qt(s, a) ∈{S(sh, a)} then"
VALUE BACKUP,0.14788273615635178,"αt(sh, a) += 1 //αt(sh, a) : weight of Qt(sh, a)
else"
VALUE BACKUP,0.1485342019543974,"S(sh, a) := (S(sh, a), Qt(sh, a))
α(sh, a) := (α(sh, a), 1)"
VALUE BACKUP,0.149185667752443,"Figure 1: Comparing CATS (left) and PATS (right) The main distinction is in the Q value function
backup(SimulateQ) and action selection function (SelectAction); the two methods are identical in
other procedures. In CATS, we init (α0(s, a), . . . , αN(s, a)) = (1, . . . , 1) and in PATS, S(s, a) =
(1), α(s, a) = (∅) for each s, a."
VALUE BACKUP,0.1498371335504886,"number, e.g., Qmax = 0.001. Since the min and max values are unknown, we start with a
191"
VALUE BACKUP,0.1504885993485342,"small range, that will get updated accordingly to the scale of the observed values.
192"
VALUE BACKUP,0.1511400651465798,"•
Probabilistic parameterization: The probability of each atom (pi(s, a)) is determined based
193"
VALUE BACKUP,0.1517915309446254,"on the visitation count ratio. In detail, each atom stores statistical information about the
194"
VALUE BACKUP,0.152442996742671,"visitation count, and the probability of that atom will be calculated as the visitation count
195"
VALUE BACKUP,0.15309446254071662,"divide with the total visitation count of that Q-value node. When we backpropagate the
196"
VALUE BACKUP,0.15374592833876222,"rt(s, a) + γ bVt(s′) value to a specific node, we identify the atom whose value range includes
197"
VALUE BACKUP,0.1543973941368078,"the rt(s, a) + γ bVt(s′) value. At this point, we increase its visitation count.
198"
VALUE BACKUP,0.15504885993485343,"Additionally, as we backpropagate Monte-Carlo Q values over time, we empirically adjust the Qmin
199"
VALUE BACKUP,0.15570032573289902,"and Qmax values to account for the dynamic range of Q values observed in the tree. This dynamic
200"
VALUE BACKUP,0.1563517915309446,"scaling ensures that the atom locations are effectively rescaled to adapt to the changing conditions.
201"
VALUE BACKUP,0.15700325732899023,"This representation method allows us to encapsulate the knowledge gained through exploration in the
202"
VALUE BACKUP,0.15765472312703582,"form of categorical distributions, which helps in making informed decisions during the tree search.
203"
VALUE BACKUP,0.15830618892508144,"Paricle based distribution: We represent each Q value distribution as a collection of sampling
204"
VALUE BACKUP,0.15895765472312703,"particles, which encapsulate the observed values during tree planning. Initially, we maintain an empty
205"
VALUE BACKUP,0.15960912052117263,"set of particles for the Q value distribution, denoted as S(s, a). At time step t, upon receiving an
206"
VALUE BACKUP,0.16026058631921825,"intermediate reward Qt(s, a) = rt(s, a) + γ bVt(s′), with s′ ∼P(·|s, a), we add Qt(s, a) to the set
207"
VALUE BACKUP,0.16091205211726384,"S(s, a) if the particle does not already exist within it. If the particle Qt(s, a) already exists in S(s, a),
208"
VALUE BACKUP,0.16156351791530946,"we increase the visitation count ratio associated with that particle.
209"
VALUE BACKUP,0.16221498371335505,"Value function: The Q-value node is crucial in the tree because its representation influences action
210"
VALUE BACKUP,0.16286644951140064,"selection, as detailed in the next section. We now discuss modeling each V-value node. The V-value
211"
VALUE BACKUP,0.16351791530944626,"distribution is based on the expected outcomes of the chosen policy and the subsequent Q-distributions.
212"
VALUE BACKUP,0.16416938110749185,"Thus, the mean of the V-function corresponds to the tree policy’s expectation of the means of all
213"
VALUE BACKUP,0.16482084690553747,"succeeding Q-value nodes. The common approach is to use empirical average mean for the value
214"
VALUE BACKUP,0.16547231270358306,"backup, as in UCT (21). However, this approach underestimates the optimal value, while using the
215"
VALUE BACKUP,0.16612377850162866,"maximum value overestimates it (9). The power mean estimator (12) provides a balanced solution,
216"
VALUE BACKUP,0.16677524429967427,"falling between the average and maximum values. In our methods, each V node stores the power
217"
VALUE BACKUP,0.16742671009771987,"mean of the empirical means of all succeeding Q-value nodes, eliminating the need to model V as a
218"
VALUE BACKUP,0.16807817589576549,"distribution.
219"
VALUE BACKUP,0.16872964169381108,"bV (s) =
P a"
VALUE BACKUP,0.16938110749185667,"Ts,a(n)"
VALUE BACKUP,0.1700325732899023,"Ts(n) bQp(s, a)
 1"
VALUE BACKUP,0.17068403908794788,"p
, p ≥1,"
VALUE BACKUP,0.17133550488599347,"where Ts(n), Ts,a(n) are the number of visitations at s and s, a at timestep n respectively. Next, we
220"
VALUE BACKUP,0.1719869706840391,"show how to select actions in the tree based on the categorical distribution of Q-value nodes.
221"
ACTION SELECTION,0.17263843648208468,"3.3
Action Selection
222"
ACTION SELECTION,0.1732899022801303,"Thompson sampling has shown promising results in real bandit scenarios due to the randomness of
223"
ACTION SELECTION,0.1739413680781759,"action selection. Taking advantage of the established categorical based distribution and particle based
224"
ACTION SELECTION,0.1745928338762215,"distribution, we use the Thompson sampling method for action selection. We maintain a Dirichlet dis-
225"
ACTION SELECTION,0.1752442996742671,"tribution of parameter of the Q value distribution. We denote the Dirichlet distribution of parameters
226"
ACTION SELECTION,0.1758957654723127,"(α0, α1, . . . , αN) by Dir(α0, α1, . . . , αN), whose density function is given by Γ(PN
i=0 αi)
ΠN
i=0Γ(αi) ΠN
i=0xαi−1
i
227"
ACTION SELECTION,0.17654723127035832,"for (x0, . . . , xN) ∈[0, 1]N+1 such that PN
i=0 xi = 1.
228"
ACTION SELECTION,0.1771986970684039,"Categorical distribution: The probability mass function of the discrete categorical distribution at
229"
ACTION SELECTION,0.1778501628664495,"each Q-value node at state s, action a: p(s, a) = [p0(s, a), p1(s, a), . . . , pN(s, a)], where pi(s, a)
230"
ACTION SELECTION,0.17850162866449512,"represents the probability of selecting the i-th atom zi(s, a), N + 1 is the number of atoms. We main-
231"
ACTION SELECTION,0.1791530944625407,"tain a Dirichlet distribution Dir(α0(s, a), α1(s, a), . . . , αN(s, a)) as the prior for the Q-value node
232"
ACTION SELECTION,0.17980456026058633,"at state s, action a. At each time step t we sample Lt(s, a) ∼Dir(α0(s, a), α1(s, a), . . . , αN(s, a))
233"
ACTION SELECTION,0.18045602605863192,"and compute ϕt(s, a) = [z0(s, a), z1(s, a), . . . , zN(s, a)]⊤Lt(s, a). Then, the action at is selected
234"
ACTION SELECTION,0.18110749185667752,"as follows:
235"
ACTION SELECTION,0.18175895765472314,"at = arg max
a "
ACTION SELECTION,0.18241042345276873,"ϕt(s, a)"
ACTION SELECTION,0.18306188925081432,"After taking action at and get an intermediate reward Qt(s, at) = rt(s, at) + γ bVt(s′). The posterior
236"
ACTION SELECTION,0.18371335504885994,"is also a Dirichlet: Dir(α0(s, a), . . . , αt(s, a) + 1, . . . , αN(s, a)) with the intermediate reward at
237"
ACTION SELECTION,0.18436482084690553,"time step t: Qt(s, at) is in the range of the atom zt(s, a). We denote this mechanism as Categorical
238"
ACTION SELECTION,0.18501628664495115,"Thompson sampling for Tree Search (CATS) method.
239"
ACTION SELECTION,0.18566775244299674,"Paricle based distribution: In the particle-based approach, the prior Dirichlet distribution of the
240"
ACTION SELECTION,0.18631921824104233,"Q-value node at state s, action a is Dir(α(s, a)), with α(s, a) is initiated as [1]. Considering each Q
241"
ACTION SELECTION,0.18697068403908795,"value distribution at state s, action a has a set of particle {Qt(s, a)} with the corresponding weighted
242"
ACTION SELECTION,0.18762214983713354,"α(s, a) = {αt(s, a)} At each time step t we also sample Lt(s, a) ∼Dir(α(s, a)) and compute
243"
ACTION SELECTION,0.18827361563517916,"ϕt(s, a) = [1, Q0(s, a), Q1(s, a), . . . , QN(s, a)]⊤Lt(s, a). Then the action at is chosen as
244"
ACTION SELECTION,0.18892508143322476,"at = arg max
a "
ACTION SELECTION,0.18957654723127035,"ϕt(s, a)
	
."
ACTION SELECTION,0.19022801302931597,"After taking action at and get an intermediate reward Qt(s, at) = rt(s, at) + γ bVt(s′). We update
245"
ACTION SELECTION,0.19087947882736156,"αt(s, a) = αt(s, a) + 1 if Qt(s, at) is in the set {Qt(s, a)}. If not, we add Qt(s, at) to the set
246"
ACTION SELECTION,0.19153094462540718,"{Qt(s, a)} and add 1 to the set {αt(s, a)} = {αt(s, a), 1}.
247"
ACTION SELECTION,0.19218241042345277,"We call this method as Paricle Thompson sampling for Tree Search (PATS) method. Detailed
248"
ACTION SELECTION,0.19283387622149836,"pseudocode and a comparison of CATS and PATS can be seen in Fig 1. The two methods are identical
249"
ACTION SELECTION,0.19348534201954398,"in all procedures except for the Q value function backup (SimulateQ) and the action selection
250"
ACTION SELECTION,0.19413680781758957,"function (SelectAction).
251"
ACTION SELECTION,0.19478827361563517,"Remark 1. CATS and PATS both use similar action selection strategies within a bandit setting,
252"
ACTION SELECTION,0.19543973941368079,"specifically referring to Multinomial Thompson Sampling and Non-Parametric Thompson Sampling,
253"
ACTION SELECTION,0.19609120521172638,"respectively (29). While CATS action selection heavily depends strictly on Thompson Sampling
254"
ACTION SELECTION,0.196742671009772,"by maintaining parameters of posterior Q-value distribution, PATS is not based on the posterior
255"
ACTION SELECTION,0.1973941368078176,"sampling in the strict sense. At each step, it computes an average of the observed rewards with
256"
ACTION SELECTION,0.19804560260586318,"random weight and is a Non-Parametric approach. Furthermore, CATS maintains a fixed set of atoms,
257"
ACTION SELECTION,0.1986970684039088,"whereas in PATS, the number of particles increases depending on the observed Q values.
258"
ACTION SELECTION,0.1993485342019544,"In the next section, we provide a theoretical analysis of the convergence of simple regret for CATS
259"
ACTION SELECTION,0.2,"and PATS.
260"
ACTION SELECTION,0.2006514657980456,"Algorithm 3 CATS in Non-stationary bandits
Require: K arms; n: number of plays;
N + 1 support size of categorical distributions
Init (α0
a, . . . , αN
a ) = (1, . . . , 1) for each a ∈[K]
Main ()"
ACTION SELECTION,0.2013029315960912,"for t = 0,1,2,..., n do"
ACTION SELECTION,0.20195439739413681,for a ∈[A] do
ACTION SELECTION,0.2026058631921824,"La,t ∼Dir(α0
a, . . . , αN
a )
ϕa,t = [0, R(t)"
ACTION SELECTION,0.20325732899022803,"N , 2R(t)"
ACTION SELECTION,0.20390879478827362,"N
, · · · , R(t)]⊤Lt"
ACTION SELECTION,0.2045602605863192,"a = arg max
a  ϕa,t"
ACTION SELECTION,0.20521172638436483,"Pull arm a and observe reward
Ra,t = mR(t)"
ACTION SELECTION,0.20586319218241042,"N
where m ∈{0, 1, . . . N}
Update αm
a = αm
a + 1"
ACTION SELECTION,0.206514657980456,"Algorithm 4 PATS in Non-stationary bandits
Require: K arms; n: number of plays;
Init αa = (1); Sa = (1) for each a ∈[K]
Main ()"
ACTION SELECTION,0.20716612377850163,"for t = 0,1,2,. . ., n do"
ACTION SELECTION,0.20781758957654722,for a ∈[A] do
ACTION SELECTION,0.20846905537459284,"La,t ∼Dir(αa)
ϕa,t = S⊤
a La,t"
ACTION SELECTION,0.20912052117263843,"a = arg max
a  ϕa,t"
ACTION SELECTION,0.20977198697068403,"Pull arm a and observe reward Ra,t
if Ra,t ∈{Sa} then"
ACTION SELECTION,0.21042345276872965,"αt
a += 1 //αt
a : weight of Ra,t
else"
ACTION SELECTION,0.21107491856677524,"Sa := (Sa, Ra,t)
αa := (αa, 1)"
ACTION SELECTION,0.21172638436482086,"Figure 2: Comparing CATS (left) and PATS (right) in Non-stationary bandits.
4
Theoretical analysis
261"
ACTION SELECTION,0.21237785016286645,"Planning in MCTS involves making a sequence of decisions along the tree, where each internal node
262"
ACTION SELECTION,0.21302931596091204,"functions as a non-stationary bandit, with the empirical mean drifting due to the action selection
263"
ACTION SELECTION,0.21368078175895766,"strategy. Therefore, we first study the non-stationary multi-armed bandit settings using the action
264"
ACTION SELECTION,0.21433224755700325,"selections of CATS and PATS, examining the concentration properties of the power mean backup for
265"
ACTION SELECTION,0.21498371335504887,"each arm relative to the optimal arm. We then apply these results to MCTS.
266"
NON-STATIONARY MULTI-ARMED BANDIT,0.21563517915309446,"4.1
Non-stationary multi-armed bandit
267"
NON-STATIONARY MULTI-ARMED BANDIT,0.21628664495114006,"We consider a class of non-stationary multi-armed bandit (MAB) problems with K ≥1 arms. Let
268"
NON-STATIONARY MULTI-ARMED BANDIT,0.21693811074918568,"Ra,t denote the random reward obtained by playing arm a ∈[K] at the time step t bounded in [0, R].
269"
NON-STATIONARY MULTI-ARMED BANDIT,0.21758957654723127,"We consider bµa,n = 1"
NON-STATIONARY MULTI-ARMED BANDIT,0.2182410423452769,"n
Pn
t=1 Ra,t as the average rewards collected at arm a after n plays. We first
270"
NON-STATIONARY MULTI-ARMED BANDIT,0.21889250814332248,"define:
271"
NON-STATIONARY MULTI-ARMED BANDIT,0.21954397394136807,"Definition 1. A sequence of estimators (bVn)n≥1 is concentrated and convergent towards some limit
272"
NON-STATIONARY MULTI-ARMED BANDIT,0.2201954397394137,"V if the following two properties hold:
273"
NON-STATIONARY MULTI-ARMED BANDIT,0.22084690553745928,"(A) Concentration: For all n ≥1, for all ε > 0, ∃c > 0 that P

|bVn −V | > ε

≤cn−1ε−1.
274"
NON-STATIONARY MULTI-ARMED BANDIT,0.22149837133550487,"(B) Convergence: lim
n→∞E[bVn] = V .
275"
NON-STATIONARY MULTI-ARMED BANDIT,0.2221498371335505,"In that case, we write plim
n→∞
bVn = V .
276"
NON-STATIONARY MULTI-ARMED BANDIT,0.22280130293159608,"We assume that the reward sequence {Ra,t} , t ≥1 is a non-stationary process satisfying the
277"
NON-STATIONARY MULTI-ARMED BANDIT,0.2234527687296417,"convergence and concentration properties from Definition 1, by making the following assumption:
278"
NON-STATIONARY MULTI-ARMED BANDIT,0.2241042345276873,"Assumption 1. Consider K arms that for a ∈[K], let (bµa,n)n≥1 be a sequence of estimator satisfying
279"
NON-STATIONARY MULTI-ARMED BANDIT,0.2247557003257329,"plim
n→∞
bµa,n = µa."
NON-STATIONARY MULTI-ARMED BANDIT,0.2254071661237785,"The action selection of CATS and PATS follows closely as in Section 3.3 and pseudocode are shown
280"
NON-STATIONARY MULTI-ARMED BANDIT,0.2260586319218241,"in Fig. 2. Let us define bµn(p) =
PK
a=1
Ta(n)"
NON-STATIONARY MULTI-ARMED BANDIT,0.22671009771986972,"n
bµp
a,Ta(n)
 1"
NON-STATIONARY MULTI-ARMED BANDIT,0.2273615635179153,"p as the power mean value backup operator
281"
NON-STATIONARY MULTI-ARMED BANDIT,0.2280130293159609,"after n rounds. Here 1 ≤p < ∞is a constant. We denote Ta(n) is the number of visitations of the
282"
NON-STATIONARY MULTI-ARMED BANDIT,0.22866449511400652,"arm a.
283"
NON-STATIONARY MULTI-ARMED BANDIT,0.2293159609120521,"We define µ⋆= maxa∈[K]{µa} and assume that µ⋆is unique. Then, we establish the concentration
284"
NON-STATIONARY MULTI-ARMED BANDIT,0.22996742671009773,"and convergence properties of the power mean backup operator bµn(p) towards the optimal value µ⋆,
285"
NON-STATIONARY MULTI-ARMED BANDIT,0.23061889250814332,"as shown in Theorem 1 and Theorem 2, respectively for CATS and PATS.
286"
NON-STATIONARY MULTI-ARMED BANDIT,0.23127035830618892,"Theorem 1. For a ∈[K], let (bµa,n)n≥1 be a sequence of estimator satisfying plim
n→∞
bµa,n = µa and let
287"
NON-STATIONARY MULTI-ARMED BANDIT,0.23192182410423454,"µ⋆= max
a {µa}. Assume that all the estimators are bounded in [0, R]. We consider a bandit algorithm
288"
NON-STATIONARY MULTI-ARMED BANDIT,0.23257328990228013,"that selects each arm according to CATS once in each round n ≥K. Then, plim
n→∞
bµn(p) = µ⋆.
289"
NON-STATIONARY MULTI-ARMED BANDIT,0.23322475570032572,"Theorem 2. For a ∈[K], let (bµa,n)n≥1 be a sequence of estimator satisfying plim
n→∞
bµa,n = µa and let
290"
NON-STATIONARY MULTI-ARMED BANDIT,0.23387622149837134,"µ⋆= max
a {µa}. Assume that all the estimators are bounded in [0, R]. We consider a bandit algorithm
291"
NON-STATIONARY MULTI-ARMED BANDIT,0.23452768729641693,"that selects each arm according to PATS once in each round n ≥K. Then, plim
n→∞
bµn(p) = µ⋆.
292"
NON-STATIONARY MULTI-ARMED BANDIT,0.23517915309446255,"Detailed proofs of the two Theorems can be found in the appendix. Based upon these results we
293"
NON-STATIONARY MULTI-ARMED BANDIT,0.23583061889250814,"analyse the concentration properties for any internal node and convergence of the simple regret in the
294"
NON-STATIONARY MULTI-ARMED BANDIT,0.23648208469055373,"MCTS in the next section.
295"
MONTE-CARLO TREE SEARCH,0.23713355048859935,"4.2
Monte-Carlo Tree Search
296"
MONTE-CARLO TREE SEARCH,0.23778501628664495,"Before presenting the main results (Theorem 3 Theorem 4), we first show an important Lemma
297"
MONTE-CARLO TREE SEARCH,0.23843648208469057,"Lemma 1. Let (bVm,n)n≥1, m ∈[M], be a sequence of estimator satisfying plim
n→∞
bVm,n = Vm.
298"
MONTE-CARLO TREE SEARCH,0.23908794788273616,"Assume that there exists a constant L > 0 such that L = supremum{bVm,n}n≥1. Let Ri be an iid
299"
MONTE-CARLO TREE SEARCH,0.23973941368078175,"sequence with mean µ and Si be an iid sequence from a distribution p = (p1, . . . , pM) supported
300"
MONTE-CARLO TREE SEARCH,0.24039087947882737,"on {1, . . . , M}. Introducing the random variables N n
m = #|{i ≤n : Si = sm}|, we define the
301"
MONTE-CARLO TREE SEARCH,0.24104234527687296,"sequence of estimator
302"
MONTE-CARLO TREE SEARCH,0.24169381107491858,bQn = 1
MONTE-CARLO TREE SEARCH,0.24234527687296417,"n
Pn
i=1 Ri + γ PM
m=1
Nn
m
n bVm,Nn
m."
MONTE-CARLO TREE SEARCH,0.24299674267100976,"Then plim
n→∞
bQn = µ + PM
m=1 pmVm.
303"
MONTE-CARLO TREE SEARCH,0.24364820846905538,"The significance of Lemma 1 lies in demonstrating the concentration and convergence of an estimated
304"
MONTE-CARLO TREE SEARCH,0.24429967426710097,"Q value, conditioned on the concentration and convergence of a child V-value node. Here, bV·,n
305"
MONTE-CARLO TREE SEARCH,0.24495114006514657,"represents the value estimation at time step n, and Ri denotes an intermediate reward received by
306"
MONTE-CARLO TREE SEARCH,0.24560260586319219,"taking a specific action at a particular state.
307"
MONTE-CARLO TREE SEARCH,0.24625407166123778,"Next, we first start with Theorem 3 to show the convergence and concentration of any V-Node and
308"
MONTE-CARLO TREE SEARCH,0.2469055374592834,"Q-node in the tree for CATS.
309"
MONTE-CARLO TREE SEARCH,0.247557003257329,"Theorem 3. When we apply the CATS algorithm, we have
310"
MONTE-CARLO TREE SEARCH,0.24820846905537458,"(i) For any node sh at the depth hth in the tree, plim
n→∞
bQn(sh, ak) = eQ(sh, ak).
311"
MONTE-CARLO TREE SEARCH,0.2488599348534202,"(ii) For any node sh at the depth hth in the tree, plim
n→∞
bVn(sh) = eV (sh).
312"
MONTE-CARLO TREE SEARCH,0.2495114006514658,"We can derive a similar result for PATS as shown in Theorem 4.
313"
MONTE-CARLO TREE SEARCH,0.2501628664495114,"Theorem 4. When we apply the PATS algorithm, we have
314"
MONTE-CARLO TREE SEARCH,0.250814332247557,"(i) For any node sh at the depth hth in the tree, plim
n→∞
bQn(sh, ak) = eQ(sh, ak).
315"
MONTE-CARLO TREE SEARCH,0.2514657980456026,"(ii) For any node sh at the depth hth in the tree, plim
n→∞
bVn(sh) = eV (sh).
316"
MONTE-CARLO TREE SEARCH,0.2521172638436482,"The results of Theorems 4 and 4 demonstrate that, at any node in the tree, both the V-value and
317"
MONTE-CARLO TREE SEARCH,0.25276872964169383,"Q-value nodes are convergent and concentrated. These results are applicable to any power mean
318"
MONTE-CARLO TREE SEARCH,0.2534201954397394,"backup operator of V-value nodes with p ∈[1, +∞). Finally, we show important results in Theorem 5,
319"
MONTE-CARLO TREE SEARCH,0.254071661237785,"and Theorem 6, since they show the convergence of simple regret of CATS and PATS, respectively.
320"
MONTE-CARLO TREE SEARCH,0.2547231270358306,"Theorem 5. (Convergence of Simple Regret of CATS) We have at the root node s0,
321"
MONTE-CARLO TREE SEARCH,0.2553745928338762,"E
h
V ⋆(s0) −bVn (s0)
i ≤O(n−1)."
MONTE-CARLO TREE SEARCH,0.25602605863192185,"Theorem 6. (Convergence of Simple Regret of PATS) We have at the root node s0,
322"
MONTE-CARLO TREE SEARCH,0.25667752442996744,"E
h
V ⋆(s0) −bVn (s0)
i ≤O(n−1)."
MONTE-CARLO TREE SEARCH,0.25732899022801303,"Remark 2. These results demonstrate that both CATS and PATS share the same convergence rate
323"
MONTE-CARLO TREE SEARCH,0.2579804560260586,"for value estimation at the root node of O(n−1), which improves over the rate O(n−1/2) of Fixed-
324"
MONTE-CARLO TREE SEARCH,0.2586319218241042,"Depth-MCTS (33). Furthermore, Our finding more broadly applies to the power mean estimator with
325"
MONTE-CARLO TREE SEARCH,0.25928338762214986,"p ∈[1, +∞).
326"
EXPERIMENTS,0.25993485342019546,"5
Experiments
327"
EXPERIMENTS,0.26058631921824105,"We compare our methods with UCT (21), Fixed-Depth-MCTS (33), MENTS (40), RENTS,
328"
EXPERIMENTS,0.26123778501628664,"TENTS (14), BTS (27) and DNG (1) in a stochastic setting (SyntheticTree) to highlight the benefits of
329"
EXPERIMENTS,0.26188925081433223,"CATS and PATS in stochastic environments. Additionally, we test on 17 Atari games, comparing our
330"
EXPERIMENTS,0.2625407166123778,"algorithms with DQN (base network without planning) and other non-distributional planning methods
331"
EXPERIMENTS,0.26319218241042347,"(Power-UCT (12), MENTS (40), TENTS (14)) to demonstrate CATS and PATS’ competitiveness and
332"
EXPERIMENTS,0.26384364820846906,"put results in Appendix. In all settings, we use 100 atoms for CATS, and set the discount factor γ to
333"
EXPERIMENTS,0.26449511400651465,"0.99 for Atari, and γ to 1 for SyntheticTree.
334"
EXPERIMENTS,0.26514657980456025,"SyntheticTree: We evaluate CATS and PATS using the synthetic tree toy problem (14). This problem
335"
EXPERIMENTS,0.26579804560260584,"involves a tree with depth d and branching factor k. Each tree edge has a random value between 0
336"
EXPERIMENTS,0.2664495114006515,"and 1. Returns at the leaf nodes are simulated using Gaussian distributions with means equal to the
337"
EXPERIMENTS,0.2671009771986971,"sum of edge values from the root to the leaf, and a standard deviation of 0.5. Means are normalized
338"
EXPERIMENTS,0.26775244299674267,"between 0 and 1. An agent traverses the tree from the root, aiming to find the leaf node with the
339"
EXPERIMENTS,0.26840390879478826,"highest mean value. Internal nodes give zero reward, while leaf nodes provide a reward sampled
340"
EXPERIMENTS,0.26905537459283385,"from their Gaussian distribution. We introduce stochasticity into the environment by altering the
341"
EXPERIMENTS,0.2697068403908795,"transition probabilities: there is a 50% chance of moving to the intended node and a 50% chance of
342"
EXPERIMENTS,0.2703583061889251,"moving to a different node with equal probability. We conduct 25 experiments on five trees with five
343"
EXPERIMENTS,0.2710097719869707,"runs each, covering all combinations of branching factors k = {2, 4, 6, 8, 10, 12, 14, 16, 100, 200}
344"
EXPERIMENTS,0.2716612377850163,"and depths d = {1, 2, 3, 4}. We compute the value estimation error at the root node. Fig. 3 shows
345"
EXPERIMENTS,0.27231270358306187,"the convergence of the value estimations of CATS and PATS at the root node in the Synthetic Tree
346"
EXPERIMENTS,0.2729641693811075,environment which shows they archives faster convergence compared to other methods.
EXPERIMENTS,0.2736156351791531,0    500   1000
EXPERIMENTS,0.2742671009771987,# Simulations 0.0 0.5
EXPERIMENTS,0.2749185667752443,Value Estimation Error
EXPERIMENTS,0.2755700325732899,k=16  d=1
EXPERIMENTS,0.2762214983713355,0    500   1000
EXPERIMENTS,0.2768729641693811,# Simulations 0 1
EXPERIMENTS,0.2775244299674267,k=200  d=1
EXPERIMENTS,0.2781758957654723,0    500   1000
EXPERIMENTS,0.2788273615635179,# Simulations 0 1
EXPERIMENTS,0.27947882736156354,k=14  d=3
EXPERIMENTS,0.28013029315960913,0    500   1000
EXPERIMENTS,0.2807817589576547,# Simulations 0 1
EXPERIMENTS,0.2814332247557003,k=16  d=3
EXPERIMENTS,0.2820846905537459,0    500   1000
EXPERIMENTS,0.28273615635179156,# Simulations 0 2
EXPERIMENTS,0.28338762214983715,k=16  d=4
EXPERIMENTS,0.28403908794788274,0    500   1000
EXPERIMENTS,0.28469055374592833,# Simulations 0.0 0.5
EXPERIMENTS,0.2853420195439739,k=200  d=2
EXPERIMENTS,0.28599348534201957,"UCT
Power-UCT"
EXPERIMENTS,0.28664495114006516,"DNG
Fixed-Depth-MCTS"
EXPERIMENTS,0.28729641693811075,"MENTS
RENTS"
EXPERIMENTS,0.28794788273615635,"TENTS
BTS"
EXPERIMENTS,0.28859934853420194,"CATS
PATS"
EXPERIMENTS,0.28925081433224753,Figure 3: Performance of CATS and PATS in SyntheticTree.
EXPERIMENTS,0.2899022801302932,"347
348"
CONCLUSION,0.29055374592833877,"6
Conclusion
349"
CONCLUSION,0.29120521172638436,"To conclude, our work introduces Categorical Thompson Sampling for MCTS (CATS) and Particle
350"
CONCLUSION,0.29185667752442995,"Thompson Sampling for MCTS (PATS), distributional planning approaches specifically designed to
351"
CONCLUSION,0.29250814332247554,"tackle complexities arising from stochasticity. CATS uses a categorical distribution, while PATS uses
352"
CONCLUSION,0.2931596091205212,"a particle-based distribution to represent and model the uncertainty inherent in return outcomes. We
353"
CONCLUSION,0.2938110749185668,"also propose exploration strategies based on Thompson Sampling that leverage this distributional
354"
CONCLUSION,0.2944625407166124,"modeling. Our methods come with a rigorous theoretical convergence guarantee, achieving a simple
355"
CONCLUSION,0.29511400651465797,"regret polynomial decay of the order O(n−1), which improves over the O(n−1/2) rate of the fixed
356"
CONCLUSION,0.29576547231270356,"version of UCT (32). Empirical findings conclusively demonstrate the effectiveness of our approach
357"
CONCLUSION,0.2964169381107492,"in stochastic environments.
358"
REFERENCES,0.2970684039087948,"References
359"
REFERENCES,0.2977198697068404,"[1] A. Bai, F. Wu, and X. Chen. Bayesian mixture modelling and inference based thompson
360"
REFERENCES,0.298371335504886,"sampling in monte-carlo tree search. Advances in neural information processing systems, 26,
361"
REFERENCES,0.2990228013029316,"2013.
362"
REFERENCES,0.2996742671009772,"[2] A. Bai, F. Wu, Z. Zhang, and X. Chen. Thompson sampling based monte-carlo planning in
363"
REFERENCES,0.3003257328990228,"pomdps. the International Conference on Automated Planning and Scheduling, 24(1), 2014.
364"
REFERENCES,0.3009771986970684,"[3] M. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, and R. Munos. Unifying
365"
REFERENCES,0.301628664495114,"count-based exploration and intrinsic motivation. In Advances in neural information processing
366"
REFERENCES,0.3022801302931596,"systems, pages 1471–1479, 2016.
367"
REFERENCES,0.30293159609120524,"[4] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An
368"
REFERENCES,0.3035830618892508,"evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253–279,
369"
REFERENCES,0.3042345276872964,"2013.
370"
REFERENCES,0.304885993485342,"[5] M. G. Bellemare, W. Dabney, and R. Munos. A distributional perspective on reinforcement
371"
REFERENCES,0.3055374592833876,"learning. In International Conference on Machine Learning, 2016.
372"
REFERENCES,0.30618892508143325,"[6] M. G. Bellemare, W. Dabney, and R. Munos. A distributional perspective on reinforcement
373"
REFERENCES,0.30684039087947884,"learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,
374"
REFERENCES,0.30749185667752443,"pages 449–458. JMLR. org, 2017.
375"
REFERENCES,0.30814332247557,"[7] R. Bellman. The theory of dynamic programming. Technical report, Rand corp santa monica
376"
REFERENCES,0.3087947882736156,"ca, 1954.
377"
REFERENCES,0.30944625407166126,"[8] C. B. Browne, E. Powley, D. Whitehouse, S. M. Lucas, P. I. Cowling, P. Rohlfshagen, S. Tavener,
378"
REFERENCES,0.31009771986970686,"D. Perez, S. Samothrakis, and S. Colton. A survey of monte carlo tree search methods. IEEE
379"
REFERENCES,0.31074918566775245,"Transactions on Computational Intelligence and AI in games, 4(1):1–43, 2012.
380"
REFERENCES,0.31140065146579804,"[9] R. Coulom. Efficient selectivity and backup operators in monte-carlo tree search. In Interna-
381"
REFERENCES,0.31205211726384363,"tional conference on computers and games. Springer, 2006.
382"
REFERENCES,0.3127035830618892,"[10] W. Dabney, G. Ostrovski, D. Silver, and R. Munos. Implicit quantile networks for distributional
383"
REFERENCES,0.31335504885993487,"reinforcement learning. In International conference on machine learning, pages 1096–1105.
384"
REFERENCES,0.31400651465798046,"PMLR, 2018.
385"
REFERENCES,0.31465798045602605,"[11] W. Dabney, M. Rowland, M. G. Bellemare, and R. Munos. Distributional reinforcement learning
386"
REFERENCES,0.31530944625407165,"with quantile regression. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
387"
REFERENCES,0.31596091205211724,"[12] T. Dam, P. Klink, C. D’Eramo, J. Peters, and J. Pajarinen. Generalized mean estimation in
388"
REFERENCES,0.3166123778501629,"monte-carlo tree search. arXiv preprint arXiv:1911.00384, 2019.
389"
REFERENCES,0.3172638436482085,"[13] T. Dam, G. Chalvatzaki, J. Peters, and J. Pajarinen. Monte-carlo robot path planning. IEEE
390"
REFERENCES,0.31791530944625407,"Robotics and Automation Letters, 7(4):11213–11220, 2022.
391"
REFERENCES,0.31856677524429966,"[14] T. Q. Dam, C. D’Eramo, J. Peters, and J. Pajarinen. Convex regularization in monte-carlo tree
392"
REFERENCES,0.31921824104234525,"search. In International Conference on Machine Learning, pages 2365–2375. PMLR, 2021.
393"
REFERENCES,0.3198697068403909,"[15] S. Eiffert, H. Kong, N. Pirmarzdashti, and S. Sukkarieh. Path planning in dynamic environments
394"
REFERENCES,0.3205211726384365,"using generative rnns and monte carlo tree search. In 2020 IEEE International Conference on
395"
REFERENCES,0.3211726384364821,"Robotics and Automation (ICRA), pages 10263–10269. IEEE, 2020.
396"
REFERENCES,0.3218241042345277,"[16] N. Funk, G. Chalvatzaki, B. Belousov, and J. Peters. Learn2assemble with structured repre-
397"
REFERENCES,0.32247557003257327,"sentations and search for robotic architectural construction. In Conference on Robot Learning,
398"
REFERENCES,0.3231270358306189,"pages 1401–1411. PMLR, 2022.
399"
REFERENCES,0.3237785016286645,"[17] M. Geist, B. Scherrer, and O. Pietquin. A theory of regularized markov decision processes. In
400"
REFERENCES,0.3244299674267101,"International Conference on Machine Learning, pages 2160–2169, 2019.
401"
REFERENCES,0.3250814332247557,"[18] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy
402"
REFERENCES,0.3257328990228013,"deep reinforcement learning with a stochastic actor. In International Conference on Machine
403"
REFERENCES,0.32638436482084693,"Learning, pages 1861–1870, 2018.
404"
REFERENCES,0.3270358306188925,"[19] J. Honda and A. Takemura. An asymptotically optimal bandit algorithm for bounded support
405"
REFERENCES,0.3276872964169381,"models. In COLT, pages 67–79. Citeseer, 2010.
406"
REFERENCES,0.3283387622149837,"[20] L. Kocsis and C. Szepesvári. Bandit based monte-carlo planning. In Proceedings of the 17th
407"
REFERENCES,0.3289902280130293,"European Conference on Machine Learning, ECML’06, page 282–293, Berlin, Heidelberg,
408"
REFERENCES,0.32964169381107494,"2006. Springer-Verlag. ISBN 354045375X. doi: 10.1007/11871842_29. URL https://doi.
409"
REFERENCES,0.33029315960912053,"org/10.1007/11871842_29.
410"
REFERENCES,0.3309446254071661,"[21] L. Kocsis, C. Szepesvári, and J. Willemson. Improved monte-carlo search. Univ. Tartu, Estonia,
411"
REFERENCES,0.3315960912052117,"Tech. Rep, 1, 2006.
412"
REFERENCES,0.3322475570032573,"[22] B. Mavrin, H. Yao, L. Kong, K. Wu, and Y. Yu. Distributional reinforcement learning for
413"
REFERENCES,0.33289902280130296,"efficient exploration. In International conference on machine learning, pages 4424–4434.
414"
REFERENCES,0.33355048859934855,"PMLR, 2019.
415"
REFERENCES,0.33420195439739414,"[23] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves,
416"
REFERENCES,0.33485342019543973,"M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep rein-
417"
REFERENCES,0.3355048859934853,"forcement learning. Nature, 518(7540):529–533, 2015.
418"
REFERENCES,0.33615635179153097,"[24] S. Mo, X. Pei, and C. Wu. Safe reinforcement learning for autonomous vehicle using monte
419"
REFERENCES,0.33680781758957656,"carlo tree search. IEEE Transactions on Intelligent Transportation Systems, 23(7):6766–6773,
420"
REFERENCES,0.33745928338762216,"2021.
421"
REFERENCES,0.33811074918566775,"[25] G. Neu, A. Jonsson, and V. Gómez. A unified view of entropy-regularized markov decision
422"
REFERENCES,0.33876221498371334,"processes. arXiv preprint arXiv:1705.07798, 2017.
423"
REFERENCES,0.33941368078175893,"[26] T. Nguyen-Tang, S. Gupta, and S. Venkatesh. Distributional reinforcement learning via moment
424"
REFERENCES,0.3400651465798046,"matching. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages
425"
REFERENCES,0.34071661237785017,"9144–9152, 2021.
426"
REFERENCES,0.34136807817589576,"[27] M. Painter, M. Baioumy, N. Hawes, and B. Lacerda. Monte carlo tree search with boltzmann
427"
REFERENCES,0.34201954397394135,"exploration. Advances in Neural Information Processing Systems, 36, 2024.
428"
REFERENCES,0.34267100977198695,"[28] D. Perez, S. Samothrakis, and S. Lucas. Knowledge-based fast evolutionary mcts for general
429"
REFERENCES,0.3433224755700326,"video game playing. In 2014 IEEE Conference on Computational Intelligence and Games,
430"
REFERENCES,0.3439739413680782,"pages 1–8. IEEE, 2014.
431"
REFERENCES,0.3446254071661238,"[29] C. Riou and J. Honda. Bandit algorithms based on thompson sampling for bounded reward
432"
REFERENCES,0.34527687296416937,"distributions. In Algorithmic Learning Theory, pages 777–826. PMLR, 2020.
433"
REFERENCES,0.34592833876221496,"[30] J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A. Guez, E. Lock-
434"
REFERENCES,0.3465798045602606,"hart, D. Hassabis, T. Graepel, et al. Mastering atari, go, chess and shogi by planning with a
435"
REFERENCES,0.3472312703583062,"learned model. Nature, 588(7839):604–609, 2020.
436"
REFERENCES,0.3478827361563518,"[31] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization.
437"
REFERENCES,0.3485342019543974,"In International Conference on Machine Learning, pages 1889–1897, 2015.
438"
REFERENCES,0.349185667752443,"[32] D. Shah, Q. Xie, and Z. Xu. Non-asymptotic analysis of monte carlo tree search. In Abstracts
439"
REFERENCES,0.3498371335504886,"of the 2020 SIGMETRICS/Performance Joint International Conference on Measurement and
440"
REFERENCES,0.3504885993485342,"Modeling of Computer Systems, pages 31–32, 2020.
441"
REFERENCES,0.3511400651465798,"[33] D. Shah, Q. Xie, and Z. Xu. Nonasymptotic analysis of monte carlo tree search. Operation
442"
REFERENCES,0.3517915309446254,"Research, 70(6):3234–3260, 2022.
443"
REFERENCES,0.352442996742671,"[34] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser,
444"
REFERENCES,0.35309446254071664,"I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalch-
445"
REFERENCES,0.3537459283387622,"brenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis.
446"
REFERENCES,0.3543973941368078,"Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):
447"
REFERENCES,0.3550488599348534,"484–489, Jan. 2016. doi: 10.1038/nature16961.
448"
REFERENCES,0.355700325732899,"[35] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre,
449"
REFERENCES,0.35635179153094465,"D. Kumaran, T. Graepel, et al. Mastering chess and shogi by self-play with a general reinforce-
450"
REFERENCES,0.35700325732899024,"ment learning algorithm. arXiv preprint arXiv:1712.01815, 2017.
451"
REFERENCES,0.35765472312703583,"[36] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker,
452"
REFERENCES,0.3583061889250814,"M. Lai, A. Bolton, Y. Chen, T. Lillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, and
453"
REFERENCES,0.358957654723127,"D. Hassabis. Mastering the game of go without human knowledge. Nature, 550:354–, Oct.
454"
REFERENCES,0.35960912052117266,"2017. URL http://dx.doi.org/10.1038/nature24270.
455"
REFERENCES,0.36026058631921826,"[37] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press, 2018.
456"
REFERENCES,0.36091205211726385,"[38] H. Van Hasselt, A. Guez, and D. Silver. Deep reinforcement learning with double q-learning.
457"
REFERENCES,0.36156351791530944,"In Thirtieth AAAI conference on artificial intelligence, 2016.
458"
REFERENCES,0.36221498371335503,"[39] T. Weissman, E. Ordentlich, G. Seroussi, S. Verdu, and M. J. Weinberger. Inequalities for the l1
459"
REFERENCES,0.3628664495114006,"deviation of the empirical distribution. Hewlett-Packard Labs, Tech. Rep, 2003.
460"
REFERENCES,0.36351791530944627,"[40] C. Xiao, R. Huang, J. Mei, D. Schuurmans, and M. Müller. Maximum entropy monte-carlo
461"
REFERENCES,0.36416938110749186,"planning. In Advances in Neural Information Processing Systems, pages 9516–9524, 2019.
462"
REFERENCES,0.36482084690553745,"A
Outline
463"
REFERENCES,0.36547231270358305,"• Notations will be described in Section B.
464"
REFERENCES,0.36612377850162864,"• Supporting Lemmas are presented in Section C.
465"
REFERENCES,0.3667752442996743,"• The Convergence of CATS and PATS in Non-stationary multi-armed bandits is shown in
466"
REFERENCES,0.3674267100977199,"Section D.
467"
REFERENCES,0.36807817589576547,"• Section E presents the concentration and convergence guarantee of CATS and PATS in
468"
REFERENCES,0.36872964169381106,"MCTS.
469"
REFERENCES,0.36938110749185665,"• Section F discusses about Limitations and possible improvements.
470"
REFERENCES,0.3700325732899023,"• Experimental setup is provided in Section G.
471"
REFERENCES,0.3706840390879479,"• Additional Experimental results are shown in Section H.
472"
REFERENCES,0.3713355048859935,"B
Notations
473"
REFERENCES,0.3719869706840391,Table 1: List of all notations for Non-stationary Multi-arms bandit.
REFERENCES,0.37263843648208467,"Notation
Type
Description"
REFERENCES,0.3732899022801303,"K
N
Number of arms"
REFERENCES,0.3739413680781759,"Ta(t)
N
Number of visitations at arm a after t timesteps"
REFERENCES,0.3745928338762215,"µa
R
mean value of arm a"
REFERENCES,0.3752442996742671,"a⋆
A
optimal action"
REFERENCES,0.3758957654723127,"µ⋆
R
mean value of an optimal arm. We assume it is unique."
REFERENCES,0.37654723127035833,"bµn(p)
R
power mean estimator, with a constant p ∈[1, +∞)"
REFERENCES,0.3771986970684039,"bµa,n
R
mean estimator of arm a after n visitations"
REFERENCES,0.3778501628664495,"C
Supporting Lemmas
474"
REFERENCES,0.3785016286644951,"We start with a result of the following lemma which plays an important role in the analysis of our
475"
REFERENCES,0.3791530944625407,"MCTS algorithm.
476"
REFERENCES,0.37980456026058634,"Lemma 1. For m ∈[M], let (bVm,n)n≥1 be a sequence of estimator satisfying plim
n→∞
bVm,n = Vm.
477"
REFERENCES,0.38045602605863194,"Assume that there exists a constant L > 0 such that L = supremum{bVm,n}n≥1. Let Ri be an iid
478"
REFERENCES,0.3811074918566775,"sequence with mean µ and Si be an iid sequence from a distribution p = (p1, . . . , pM) supported
479"
REFERENCES,0.3817589576547231,"on {1, . . . , M}. Introducing the random variables N n
m = #|{i ≤n : Si = sm}|, we define the
480"
REFERENCES,0.3824104234527687,"sequence of estimator
481"
REFERENCES,0.38306188925081436,"bQn = 1 n n
X"
REFERENCES,0.38371335504885995,"i=1
Ri + γ M
X m=1"
REFERENCES,0.38436482084690554,"N n
m
n
bVm,Nn
m."
REFERENCES,0.38501628664495113,"Then there exists some constant c′ (which depends on pi (i=1,2,...,M), γ, µ) such that
482"
REFERENCES,0.3856677524429967,"plim
n→∞
bQn = µ + M
X"
REFERENCES,0.3863192182410424,"m=1
pmVm."
REFERENCES,0.38697068403908796,"Proof. Let p = (p1, p2, ...pM), p ∈△M where △M = {x ∈RM : PM
i=1 Ri = 1, Ri ≥0} is the
483"
REFERENCES,0.38762214983713356,"(M −1)-dimensional simplex. Let us study a random vector bpn = ( N n
1
n , Nn
2
n , ..., Nn
M
n ). Let us define
484"
REFERENCES,0.38827361563517915,Table 2: List of all notations for Monte-Carlo Tree Search.
REFERENCES,0.38892508143322474,"Notation
Type
Description"
REFERENCES,0.38957654723127033,"γ
R
Discount factor"
REFERENCES,0.390228013029316,"N
N
Number of atoms"
REFERENCES,0.39087947882736157,"sh
S
state at depth h"
REFERENCES,0.39153094462540716,"bVt(s)
R
Estimated Value function at state s after t visitations"
REFERENCES,0.39218241042345275,"Ts(t)
N
Number of visitations at state s after t timesteps"
REFERENCES,0.39283387622149835,"Ts,a(t)
N
Number of visitations at (s, a) after t timesteps"
REFERENCES,0.393485342019544,"T s′
s,a(t)
N
Number of visitations at (s, a) that goes to s′ after t timesteps"
REFERENCES,0.3941368078175896,"bQt(s, a)
R
Estimated Q Value function at state s action a after t visitations"
REFERENCES,0.3947882736156352,"Qmin(s, a)
R
Minimum value for the Q value distribution at state s, action a"
REFERENCES,0.39543973941368077,"Qmax(s, a)
R
Maximum value for the Q value distribution at state s, action a"
REFERENCES,0.39609120521172636,"R(s, a)
Reward distribution at state s action a"
REFERENCES,0.396742671009772,"V(s)
Value distribution at state s"
REFERENCES,0.3973941368078176,"Q(s, a)
Q Value distribution at state s action a"
REFERENCES,0.3980456026058632,"pi(s, a)
R
Probability of the ith atom at the Q Value distribution at state s action a"
REFERENCES,0.3986970684039088,"△z
R
Size of each atom"
REFERENCES,0.3993485342019544,"zi(s, a)
R
value of the atom ith at state s, action a."
REFERENCES,0.4,"Qt(s, a)
R
intermediate Q value at time t at (s, a)"
REFERENCES,0.4006514657980456,"V = (V1, V2, ...VM). Let bRn = 1"
REFERENCES,0.4013029315960912,"n
Pn
i=1 Ri, bVn = (bV1,N n
1 , bV2,N n
2 , ..., bVM,N n
M ), PM
i=1 N n
i = n, N n
i
485"
REFERENCES,0.4019543973941368,"is the number of times that population i was observed. We have bQn = bRn + γ
D
bpn, bVn
E
. Therefore,
486"
REFERENCES,0.4026058631921824,"P

bQn −
 
µ + γ ⟨p, V ⟩

≥ϵ

≤P

bRn −µ ≥1"
REFERENCES,0.40325732899022804,"2ϵ

+ P

γ
D
bpn, bVn
E
−γ ⟨p, Y ⟩≥1 2ϵ
"
REFERENCES,0.40390879478827363,≤exp{−2nϵ2
REFERENCES,0.4045602605863192,"4 } + P
 D
bpn, bVn
E
−⟨p, Y ⟩≥1"
REFERENCES,0.4052117263843648,"2γ ϵ
"
REFERENCES,0.4058631921824104,"|
{z
}
A ."
REFERENCES,0.40651465798045605,"To upper bound A, let us consider
D
bpn, bV
E
−⟨p, V ⟩=
D
(bpn −p), bVn
E
+
D
p, (bV −V )
E
. Then,
487"
REFERENCES,0.40716612377850164,"A ≤P
 D
(bpn −p), bVn
E
≥1"
REFERENCES,0.40781758957654723,"4γ ϵ
"
REFERENCES,0.4084690553745928,"|
{z
}
A1"
REFERENCES,0.4091205211726384,"+ P
 D
p, (bVn −V )
E
≥1"
REFERENCES,0.40977198697068407,"4γ ϵ
"
REFERENCES,0.41042345276872966,"|
{z
}
A2 ."
REFERENCES,0.41107491856677525,"By applying a Hölder inequality to bpn −p and bV , we obtain
488"
REFERENCES,0.41172638436482084,"D
(bpn −p), bVn
E
≤∥bpn −p ∥1∥bVn ∥∞=∥bpn −p ∥1 L,"
REFERENCES,0.41237785016286643,"with L is the supremum of bV . Then we can derive
489"
REFERENCES,0.413029315960912,"A1 = P
 D
(bpn −p), bVn
E
≥1"
REFERENCES,0.41368078175895767,"4γ ϵ

≤P

∥bpn −p ∥1 L ≥1"
REFERENCES,0.41433224755700326,"4γ ϵ
"
REFERENCES,0.41498371335504886,"= P

∥bpn −p ∥1≥
1
4γLϵ

."
REFERENCES,0.41563517915309445,"According to (39), we have for any M ≥2 and δ ∈[0, 1]
490"
REFERENCES,0.41628664495114004,"P

∥bpn −p ∥1≥ r"
REFERENCES,0.4169381107491857,"2M ln(2/δ) n 
≤δ."
REFERENCES,0.4175895765472313,"Define ϵ =
q"
REFERENCES,0.41824104234527687,2M ln(2/δ)
REFERENCES,0.41889250814332246,"n
, therefore δ = 2 exp{ −nϵ2"
REFERENCES,0.41954397394136805,"2M }, we have
491"
REFERENCES,0.4201954397394137,"P

∥bpn −p ∥1≥ϵ

≤2 exp{−nϵ2 2M }."
REFERENCES,0.4208469055374593,"Therefore,
492"
REFERENCES,0.4214983713355049,"A1 ≤P

∥bpn −p ∥1≥ϵ

≤2 exp{
−nϵ2"
REFERENCES,0.4221498371335505,32Mγ2L2 }.
REFERENCES,0.42280130293159607,"We also have
493"
REFERENCES,0.4234527687296417,"A2 = P

M
X"
REFERENCES,0.4241042345276873,"m=1
pm(bVm,Nn
m −Vm) ≥1"
REFERENCES,0.4247557003257329,"4γ ϵ
 ≤ M
X"
REFERENCES,0.4254071661237785,"m=1
E

P
 1 N n
m"
REFERENCES,0.4260586319218241,"Nn
m
X"
REFERENCES,0.42671009771986973,"t=1
Vm,t −Vm ≥
1
4γpm
ϵ
N n
m  ≤ M
X"
REFERENCES,0.4273615635179153,"m=1
E

c(N n
m)−1(
ϵ
4γpm
)−1

."
REFERENCES,0.4280130293159609,"Let us define an event E =

N n
m ≥npm 2"
REFERENCES,0.4286644951140065,"
. Therefore,
494 A2 ≤ M
X"
REFERENCES,0.4293159609120521,"m=1
E

c(npm"
REFERENCES,0.42996742671009774,"2
)−1(
ϵ
4γpm
)−1
 + M
X"
REFERENCES,0.43061889250814334,"m=1
E

P(N n
m < npm"
REFERENCES,0.43127035830618893,"2
)

= M
X"
REFERENCES,0.4319218241042345,"m=1
(c21+2γ1p−1+1
m
)n−1ϵ−1 + M
X"
REFERENCES,0.4325732899022801,"m=1
E

P(N n
m −pmn ≤−pmn 2
)
 ≤ M
X"
REFERENCES,0.43322475570032576,"m=1
(c23γ)n−1ϵ−1 + M
X"
REFERENCES,0.43387622149837135,"m=1
exp

−2n(pmn"
REFERENCES,0.43452768729641694,"2
)2
"
REFERENCES,0.43517915309446253,"We consider pm > 0 only since if pm = 0, pm(bVm,Nn
m −Vm) = 0, and has been eliminated.
495"
REFERENCES,0.4358306188925081,"Therefore,
496"
REFERENCES,0.4364820846905538,"A ≤A1 + A2 ≤2 exp{
−nϵ2"
REFERENCES,0.43713355048859937,"32Mγ2L2 } + M
X"
REFERENCES,0.43778501628664496,"m=1
(c23γ)n−1ϵ−1 + M
X"
REFERENCES,0.43843648208469055,"m=1
exp

−2n(pmn"
REFERENCES,0.43908794788273614,"2
)2

."
REFERENCES,0.43973941368078173,"That leads to
497"
REFERENCES,0.4403908794788274,"P

bQn −
 
µ + γ ⟨p, V ⟩

≥ϵ

≤exp{−2nϵ2 4 }"
REFERENCES,0.44104234527687297,"+ 2 exp{
−nϵ2"
REFERENCES,0.44169381107491856,"32Mγ2L2 } + M
X"
REFERENCES,0.44234527687296415,"m=1
(c23γ)n−1ϵ−1 + M
X"
REFERENCES,0.44299674267100975,"m=1
exp

−2n(pmn"
REFERENCES,0.4436482084690554,"2
)2

≤c
′n−1ϵ−1,"
REFERENCES,0.444299674267101,"with c
′ > 0 depends on c, M, pi. So that
498"
REFERENCES,0.4449511400651466,"P

bQn −
 
µ + γ ⟨p, V ⟩

≥ϵ

≤c
′n−1ϵ−1,"
REFERENCES,0.44560260586319217,"By following the same steps, we can derive
499"
REFERENCES,0.44625407166123776,"P

bQn −
 
µ + γ ⟨p, V ⟩

≤−ϵ

≤c
′n−1ϵ−1."
REFERENCES,0.4469055374592834,"Therefore, with n ≥1, ϵ > 0,
500"
REFERENCES,0.447557003257329,"P
  bQn −
 
µ + γ ⟨p, V ⟩
 ≥ϵ

≤c
′n−1ϵ−1."
REFERENCES,0.4482084690553746,"Furthermore,
501"
REFERENCES,0.4488599348534202,"bQn −
 
µ + γ ⟨p, V ⟩

= ( bRn −µ) +

γ
D
bpn, bVn
E
−γ ⟨p, Y ⟩
"
REFERENCES,0.4495114006514658,"= ( bRn −µ) + γ
 D
(bpn −p), bVn
E
+
D
p, (bV −V )
E "
REFERENCES,0.4501628664495114,"Therefore,
502"
REFERENCES,0.450814332247557,"⇒
E[ bQn] −
 
µ + γ ⟨p, V ⟩
 ≤
E[( bRn −µ)]
 + γ

|E[bpn −p]|
bVn
 + p
E[bV −V ]

"
REFERENCES,0.4514657980456026,"⇒
E[ bQn] −
 
µ + γ ⟨p, V ⟩
 ≤
E[( bRn −µ)]
 + γ

L |E[bpn −p]| + p
E[bV −V ]

"
REFERENCES,0.4521172638436482,"Also because lim
n→∞E[bVm,n] = Vm, lim
n→∞"
REFERENCES,0.4527687296416938,"b
Nn
m
n = pm, and E[( bRn −µ)] = 0 so that,
503"
REFERENCES,0.45342019543973944,"lim
n→∞E[ bQn] = µ + γ M
X"
REFERENCES,0.45407166123778503,"m=1
pmVm."
REFERENCES,0.4547231270358306,"That mean
504"
REFERENCES,0.4553745928338762,"plim
n→∞
bQn = µ + γ M
X"
REFERENCES,0.4560260586319218,"m=1
pmVm,"
REFERENCES,0.45667752442996745,"which concludes the proof.
505"
REFERENCES,0.45732899022801304,"Results from Lemma 1 is important as it shows the concentration for the Q value estimation given the
506"
REFERENCES,0.45798045602605864,"concentration of V value of the children nodes.
507"
REFERENCES,0.4586319218241042,"Lemma 2. Let consider non-negative variables x, y ∈R+, and a constant m that 0 ≤m ≤1. Then
508"
REFERENCES,0.4592833876221498,(x + y)m ≤xm + ym.
REFERENCES,0.45993485342019547,"Proof. With y = 0, or x = 0, the inequality (2) becomes correct. Let consider the case where
509"
REFERENCES,0.46058631921824106,"x > 0, y > 0, the inequality (2) can be written as
510 (x"
REFERENCES,0.46123778501628665,"y + 1)m ≤
x y"
REFERENCES,0.46188925081433224,"m
+ 1."
REFERENCES,0.46254071661237783,"Let us define a function
511"
REFERENCES,0.4631921824104234,"f(t) = (t + 1)m −tm −1, (t > 0)."
REFERENCES,0.4638436482084691,"We can see that
512"
REFERENCES,0.46449511400651466,"f
′(t) = m(t + 1)m−1 −mtm−1 = m
 
(t + 1)m−1 −tm−1
≤0 with m ∈[0, 1], t > 0,"
REFERENCES,0.46514657980456026,"because g(x) = xm−1 is a decreasing function with m ∈[0, 1], x > 0. Therefore,
513"
REFERENCES,0.46579804560260585,f(t) ≤f(0) = 0 with t > 0.
REFERENCES,0.46644951140065144,"So that,
514"
REFERENCES,0.4671009771986971,"(t + 1)m −tm −1 ≤0, (t > 0).
with t = x"
REFERENCES,0.4677524429967427,"y ≥0, we can derive the inequality (2).
515"
REFERENCES,0.46840390879478827,"We use Minkowski’s inequality as shown below
516"
REFERENCES,0.46905537459283386,"Lemma 3. (Minkowski’s inequality) Given p ≥1, {xi, yi} ∈R, i = 1, 2, ..., n, then we have the
517"
REFERENCES,0.46970684039087945,"following inequality
518 X"
REFERENCES,0.4703583061889251,"i
(|xi + yi|)p
! 1 p
≤ X"
REFERENCES,0.4710097719869707,"i
(|xi|)p
! 1 p
+ X"
REFERENCES,0.4716612377850163,"i
(|yi|)p
! 1 p
."
REFERENCES,0.4723127035830619,"Proof. This is a basic result.
519"
REFERENCES,0.47296416938110747,"Lemma 4. (Markov’s inequality) If X is a nonnegative random variable and a > 0, then the
520"
REFERENCES,0.4736156351791531,"probability that X is at least a is at most the expectation of X divided by a:
521"
REFERENCES,0.4742671009771987,"Pr(X > a) ≤E[X] a
."
REFERENCES,0.4749185667752443,"Proof. This is a well-known result.
522"
REFERENCES,0.4755700325732899,"D
Convergence of CATS and PATS in Non-stationary multi-armed bandits
523"
REFERENCES,0.4762214983713355,"We note that in an MCTS tree, each node is considered a non-stationary multi-armed bandit where
524"
REFERENCES,0.47687296416938113,"the average mean drifts due to the given action selection strategy. Therefore, we first study the
525"
REFERENCES,0.4775244299674267,"convergence of CATS and PATS in non-stationary multi-armed bandits where the action selection is
526"
REFERENCES,0.4781758957654723,"Thompson sampling, with the power mean backup operator at the root node. Detailed descriptions of
527"
REFERENCES,0.4788273615635179,"the CATS and PATS in Non-stationary multi-armed bandits settings can be found in the main article
528"
REFERENCES,0.4794788273615635,"in the Theoretical Analysis section.
529"
REFERENCES,0.48013029315960915,"We first establish the convergence and concentration properties for the power mean backup operator
530"
REFERENCES,0.48078175895765474,"in non-stationary bandits, detailed in Theorem 1 for CATS and Theorem 2 for PATS.
531"
REFERENCES,0.48143322475570033,"To achieve these results, we demonstrate that the expected payoff of the power mean backup operator
532"
REFERENCES,0.4820846905537459,decays polynomially at a rate of O( log n
REFERENCES,0.4827361563517915,"n ). This is supported by Lemma 7 for CATS and Lemma 8 for
533"
REFERENCES,0.48338762214983716,"PATS. Critical to this analysis are Lemma 5 and Lemma 6, which establish an upper bound of log(n)
534"
REFERENCES,0.48403908794788275,"for the expected number of suboptimal arm pulls.
535"
REFERENCES,0.48469055374592834,"We introduce some important definitions. F n
a represents the empirical cumulative distribution function
536"
REFERENCES,0.48534201954397393,"of arm a after n visitations, and Fa represents the cumulative distribution function of arm a. We
537"
REFERENCES,0.4859934853420195,"employ the following distance measure: If P and Q are two distributions characterized by parameters
538"
REFERENCES,0.4866449511400652,"p = (p0, p1, · · · , pN) and q = (q0, q1, · · · , qN) respectively, then the distance is defined as
539"
REFERENCES,0.48729641693811077,"d(P, Q) :=∥p −q ∥∞=
sup
i∈[0,N]
|pi −qi|"
REFERENCES,0.48794788273615636,"This
represents
the
L∞
distance
between
p
and
q
in
RN+1.
We
also
denotes
540"
REFERENCES,0.48859934853420195,"KL(P
∥
Q)
as
the
Kullback–Leibler
divergence
between
P
and
Q,
and
denote
541"
REFERENCES,0.48925081433224754,"Kinf(Fa, µ⋆)
=
infG:E[G]>µ⋆KL(Fa
∥
G).
In addition, we denote K(N)
inf (Fa, µ⋆)
=
542"
REFERENCES,0.48990228013029313,"inf

KL(Fa ∥G)
 the support of G ∈

0, R"
REFERENCES,0.4905537459283388,"N , 2R"
REFERENCES,0.49120521172638437,"N , · · · , R
	
, E[G] > µ⋆"
REFERENCES,0.49185667752442996,"
.
543"
REFERENCES,0.49250814332247556,"We see that the definition of Kinf(Fa, µ⋆) and K(N)
inf (Fa, µ⋆) is only difference in the support set.
544"
REFERENCES,0.49315960912052115,"We denote the true parameter of arm a by pa = (p0
a, p1
a, . . . , pN
a ) with pi
a = PrX∼Fa[X =
i
N ]. We
545"
REFERENCES,0.4938110749185668,"denote the parameter of the posterior distribution of arm a as αa = (α0
a, α1
a, . . . , αN
a ). Since
546"
REFERENCES,0.4944625407166124,"each arm a is non-stationary, we also denote the parameter of arm a after n visitations by
547"
REFERENCES,0.495114006514658,"pa(n) = (p0
a(n), p1
a(n), . . . , pN
a (n)) with pi
a(n) = PrX∼F n
a [X =
i
N ]. The parameter of the
548"
REFERENCES,0.49576547231270357,"posterior distribution of arm a denoted as αa(n) = (α0
a(n), α1
a(n), . . . , αN
a (n)) We first show the
549"
REFERENCES,0.49641693811074916,"results of an important Lemma 5. The proof follows closely to the Proof of Proposition 7 (29). The
550"
REFERENCES,0.4970684039087948,"only difference is that in our settings, we study non-stationary bandits.
551"
REFERENCES,0.4977198697068404,"Lemma 5. Consider Categorical Thompson Sampling(CATS) strategy applied to a non-stationary
552"
REFERENCES,0.498371335504886,"problem where the pay-off sequence satisfies Assumption 1. Let Ta(n) denote the number of plays of
553"
REFERENCES,0.4990228013029316,"arm a up to timestep n.
554"
REFERENCES,0.4996742671009772,"If a is the index of a suboptimal arm, Then for any ϵ0, ϵ1 ≥0, each sub-optimal arm a is played in
555"
REFERENCES,0.5003257328990228,"expectation at most
556"
REFERENCES,0.5009771986970684,"E[Ta(n)] ≤
(1 + ϵ0) log n"
REFERENCES,0.501628664495114,"K(N)
inf (Fa, µ⋆) −ϵ1
+ o(log n) + O(1),"
REFERENCES,0.5022801302931597,"Proof. We have ϕa,t = [0, R"
REFERENCES,0.5029315960912052,"N , 2R"
REFERENCES,0.5035830618892508,"N , · · · , R]⊤La,t, with La,t ∼Dir(α0
a(t), . . . , αN
a (t)).
557"
REFERENCES,0.5042345276872964,"To analyze the expectation associated with selecting a suboptimal arm a, we decompose it into two
558"
REFERENCES,0.504885993485342,"components:
559 E "" n
X"
REFERENCES,0.5055374592833877,"t=1
1(I(t) = a) # = E "" n
X"
REFERENCES,0.5061889250814332,"t=1
1(I(t) = a), ϕa,t ≥µ∗−ϵ1, d( bFI(t), FI(t)) ≤ϵ2) #"
REFERENCES,0.5068403908794789,"|
{z
}
A1 + E "" n
X"
REFERENCES,0.5074918566775244,"t=1
1(I(t) = a), ϕa,t < µ∗−ϵ1, d( bFI(t), FI(t)) > ϵ2) #"
REFERENCES,0.50814332247557,"|
{z
}
A2"
REFERENCES,0.5087947882736157,"We first find an upper bound for A1:
560 A1 = n
X t=1 n
X"
REFERENCES,0.5094462540716612,"m=1
1

I(t) = a, θk(t) ≥µ⋆−ϵ1; ∥
αa(t)
Tk(t) + N + 1 −pa(t) ∥∞≤ϵ2, Tk(t) = m
"
REFERENCES,0.5100977198697069,"We see that if the event
561

I(t) = a, θk(t) ≥µ⋆−ϵ1; ∥
αa(t)
Tk(t) + N + 1 −pa(t) ∥∞≤ϵ2, Tk(t) = m
"
REFERENCES,0.5107491856677524,"occurs at step t for a certain m ∈[1, n] , then Tk(t′) > Tk(t) = m for any t′ > t. Therefore, for any
562"
REFERENCES,0.511400651465798,"m ∈[n]
563 n
X"
REFERENCES,0.5120521172638437,"t=1
1

I(t) = a, θk(t) ≥µ⋆−ϵ1; ∥
αa(t)
Tk(t) + N + 1 −pa(t) ∥∞≤ϵ2, Tk(t) = m

≤1"
REFERENCES,0.5127035830618892,"We can bound for any m0 ∈[n]
564"
REFERENCES,0.5133550488599349,"A1 ≤m0 + n
X t=1 n
X"
REFERENCES,0.5140065146579804,"m=m0
E

1

I(t) = a, θk(t) ≥µ⋆−ϵ1; ∥
αa(t)
Tk(t) + N + 1 −pa(t) ∥∞≤ϵ2, Tk(t) = m
 ≤m0 + n
X t=1 n
X"
REFERENCES,0.5146579804560261,"m=m0
Pr
"
REFERENCES,0.5153094462540717,"θk(t) ≥µ⋆−ϵ1; ∥
αa(t)
Tk(t) + N + 1 −pa(t) ∥∞≤ϵ2, Tk(t) = m
 ≤m0 + n
X t=1 n
X"
REFERENCES,0.5159609120521172,"m=m0
Pr
"
REFERENCES,0.5166123778501629,θk(t) ≥µ⋆−ϵ1
REFERENCES,0.5172638436482084,"∥
αa(t)
Tk(t) + N + 1 −pa(t) ∥∞≤ϵ2, Tk(t) = m
"
REFERENCES,0.5179153094462541,"× Pr

∥
αa(t)
Tk(t) + N + 1 −pa(t) ∥∞≤ϵ2, Tk(t) = m

(4)"
REFERENCES,0.5185667752442997,"By applying results of Lemma 13 Appendix F (29), we have
565 Pr
"
REFERENCES,0.5192182410423453,θk(t) ≥µ⋆−ϵ1
REFERENCES,0.5198697068403909,"αa, Tk(t) = m
"
REFERENCES,0.5205211726384364,"≤C(m + N + 1)N/2 exp{−(m + N + 1)KL(Pαa(t) ∥P ∗
µ⋆−ϵ1)}"
REFERENCES,0.5211726384364821,"where P ∗
µ⋆−ϵ1 = arg minx:u⊤x≥µ⋆−ϵ1 KL(Pαa ∥x) and Pαa(t) =
1
n+N+1αa(t). And by definition
566"
REFERENCES,0.5218241042345277,"KL(Pαa(t) ∥P ∗
µ⋆−ϵ1) = Kinf(Pαa(t), µ⋆−ϵ1), therefore
567 Pr
"
REFERENCES,0.5224755700325733,θk(t) ≥µ⋆−ϵ1
REFERENCES,0.5231270358306189,"αa(t), Tk(t) = m
"
REFERENCES,0.5237785016286645,"≤C(m + N + 1)N/2 exp{−(m + N + 1)Kinf(Pαa(t), µ⋆−ϵ1)},"
REFERENCES,0.5244299674267101,where C = exp{1/12}
REFERENCES,0.5250814332247556,"Γ(N+1)

1
√ 2π"
REFERENCES,0.5257328990228013,"N
. On the other hand, Kinf(x, µ⋆−ϵ1) is continuous in x ∈[0, 1]N+1
568"
REFERENCES,0.5263843648208469,"on the probability simplex with respect to the L∞distance from ((19), Theorem 7) and Lemma 18 in
569"
REFERENCES,0.5270358306188925,"Appendix H (29). Therefore, for any ϵ3 > 0, there exists ϵ2 > 0 and constant C′ > 0 such that
570 Pr
"
REFERENCES,0.5276872964169381,θk(t) ≥µ⋆−ϵ1
REFERENCES,0.5283387622149837,"∥
αa(t)
Tk(t) + N + 1 −pa(t) ∥∞≤ϵ2, Tk(t) = m
"
REFERENCES,0.5289902280130293,"≤C′ exp{−(m + N + 1)(Kinf(pa, µ⋆−ϵ1) −ϵ3)}"
REFERENCES,0.529641693811075,"And because Pr

∥
αa(t)
Tk(t)+N+1 −pa(t) ∥∞≤ϵ2, Tk(t) = m

≤1. Therefore,
571"
REFERENCES,0.5302931596091205,"A1 ≤m0 + C′
1 n
X"
REFERENCES,0.5309446254071661,"t=1
exp{−(m + N + 1)(Kinf(pa, µ⋆−ϵ1) −ϵ3)}"
REFERENCES,0.5315960912052117,"≤m0 + C′
1T exp{−(m + N + 1)(Kinf(pa, µ⋆−ϵ1) −ϵ3)}
(5)"
REFERENCES,0.5322475570032573,"Choosing m0 =
log n
Kinf(pa,µ⋆−ϵ1)−ϵ3 −N −1, we have
572"
REFERENCES,0.532899022801303,"A1 ≤
log n
Kinf(pa, µ⋆−ϵ1) −ϵ3
−N −1 + C′
1"
REFERENCES,0.5335504885993485,"Furthermore, as from ((19), Theorem 7), it is proven that µ →Kinf(F, µ) is continuous for µ < 1,
573"
REFERENCES,0.5342019543973942,"when we scale reward from [0,1] to [0, R] therefore µ from [0,1] to [0, R]. We have µ →Kinf(F, µ)
574"
REFERENCES,0.5348534201954397,"is continuous for µ < R. Therefore, ∀ϵ4 > 0, ∃ϵ1 > 0, such that
575"
REFERENCES,0.5355048859934853,"|Kinf(pa, µ∗−ϵ1) −Kinf(pa, µ∗)| ≤ϵ4
⇒Kinf(pa, µ∗−ϵ1) −ϵ3 ≥Kinf(pa, µ∗) −ϵ3 −ϵ4
Therefore, ∀ϵ0 > 0
576"
REFERENCES,0.536156351791531,A1 ≤(ϵ0 + 1) log n
REFERENCES,0.5368078175895765,"Kinf(pa, µ⋆) −N −1 + C′
1"
REFERENCES,0.5374592833876222,"Also According to Proposition 8 (29), for any ϵ0 > 0 we have
577"
REFERENCES,0.5381107491856677,"A2 ≤O(1)
(6)"
REFERENCES,0.5387622149837134,"Combining inequality (5) and inequality (6) leads us to
578"
REFERENCES,0.539413680781759,E[Ta(n)] ≤(1 + ϵ0) log n
REFERENCES,0.5400651465798045,"K(N)
inf (Fa, µ⋆)
+ o(log n) + O(1)."
REFERENCES,0.5407166123778502,"Therefore which concludes the proof.
579"
REFERENCES,0.5413680781758957,"Lemma 6. Consider Particle Thompson Sampling(PATS) strategy applied to a non-stationary
580"
REFERENCES,0.5420195439739414,"problem where the pay-off sequence satisfies Assumption 1. Then for any ϵ0 ≥0. Let Ta(n) denote
581"
REFERENCES,0.542671009771987,"the number of plays of arm a up to timestep n. Then if a is the index of a suboptimal arm, then each
582"
REFERENCES,0.5433224755700325,"sub-optimal arm a is played in expectation at most
583"
REFERENCES,0.5439739413680782,"E[Ta(n)] ≤
log n
Kinf(Fa, µ⋆) −ϵ0
+ o(log n) + O(1)."
REFERENCES,0.5446254071661237,"Proof. In this Theorem, we use the Levy distance. Recall that the Levy distance between two
584"
REFERENCES,0.5452768729641694,"cumulative distribution functions F and G on [0, 1] is defined as
585"
REFERENCES,0.545928338762215,"DL(F, G) = inf{ϵ > 0 : ∀x ∈[0, 1], F(x −ϵ) −ϵ ≤G(x) ≤F(x + ϵ) + ϵ}."
REFERENCES,0.5465798045602606,"The proof follows the same steps as in Lemma 5. We also can derive
586 E "" n
X"
REFERENCES,0.5472312703583062,"t=1
1(I(t) = a) # = E "" n
X"
REFERENCES,0.5478827361563517,"t=1
1(I(t) = a), ϕa,t ≥µ∗−ϵ1, DL( bFI(t), FI(t)) ≤ϵ2) #"
REFERENCES,0.5485342019543974,"|
{z
}
B1 + E "" n
X"
REFERENCES,0.549185667752443,"t=1
1(I(t) = a), ϕa,t < µ∗−ϵ1, DL( bFI(t), FI(t)) > ϵ2) #"
REFERENCES,0.5498371335504886,"|
{z
}
B2
We can use the same ways of derivations as in Lemma 5, equation (4) to have the same bound
587"
REFERENCES,0.5504885993485342,"B1 ≤m0 + n
X t=1 n
X"
REFERENCES,0.5511400651465798,"m=m0
Pr
"
REFERENCES,0.5517915309446254,θk(t) ≥µ⋆−ϵ1
REFERENCES,0.552442996742671,"DL

bFa(t), Fa(t)

≤ϵ2, Tk(t) = m
"
REFERENCES,0.5530944625407166,"× Pr

DL

bFa(t), Fa(t)

≤ϵ2, Tk(t) = m

(7)"
REFERENCES,0.5537459283387622,"According to Lemma 15 in Appendix G.1 (29) on conditional probabilities, for any ν ∈(0, 1) we
588"
REFERENCES,0.5543973941368078,"have
589 Pr
"
REFERENCES,0.5550488599348534,θk(t) ≥µ⋆−ϵ1
REFERENCES,0.5557003257328991,"DL

bFa(t), Fa(t)

≤ϵ2, Tk(t) = m
 ≤1"
REFERENCES,0.5563517915309446,"ν exp

−n

Kinf( bFa(t), µ⋆−ϵ1) −ν
µ⋆−ϵ1
1 −(µ⋆−ϵ1) "
REFERENCES,0.5570032573289903,"Because Kinf(F, µ) is continuous in F with respect to the Levy distance from (19), Theorem 7, for
590"
REFERENCES,0.5576547231270358,"any ϵ3 > 0 there exists ϵ2 > 0 such that
591"
REFERENCES,0.5583061889250814,"DL( bFa(t), Fa) ≤ϵ2 ⇒
Kinf( bFa(t), µ⋆−ϵ1) −Kinf(Fa, µ⋆−ϵ1)
 ≤ϵ3"
REFERENCES,0.5589576547231271,"Therefore, ∀ν ∈(0, 1) and for any ϵ5 > 0, there exists ϵ1, ϵ2 > 0 such that
592 Pr
"
REFERENCES,0.5596091205211726,θk(t) ≥µ⋆−ϵ1
REFERENCES,0.5602605863192183,"DL

bFa(t), Fa(t)

≤ϵ2, Tk(t) = m
 ≤1 ν"
REFERENCES,0.5609120521172638,"
−m

Kinf(Fa, µ⋆−ϵ1) −ϵ3 −ν
µ⋆−ϵ1
1 −(µ⋆−ϵ1) "
REFERENCES,0.5615635179153095,"(Theorem 6 (19) )
≤
1
ν"
REFERENCES,0.5622149837133551,"
−m

Kinf(Fa, µ⋆)
ϵ1
1 −µ⋆
−ϵ3 −ν
µ⋆−ϵ1
1 −(µ⋆−ϵ1) "
REFERENCES,0.5628664495114006,"This implies that ∀ϵ0 > 0, there exists ν ∈(0, 1), ϵ1 > 0 and ϵ2 > 0 such that
593 Pr
"
REFERENCES,0.5635179153094463,θk(t) ≥µ⋆−ϵ1
REFERENCES,0.5641693811074918,"DL

bFa(t), Fa(t)

≤ϵ2, Tk(t) = m

≤1"
REFERENCES,0.5648208469055375,"ν exp {−m(Kinf(Fa, µ⋆) −ϵ0)}"
REFERENCES,0.5654723127035831,"Therefore, according to inequality (7) and the fact that
594"
REFERENCES,0.5661237785016286,"Pr

DL

bFa(t), Fa(t)

≤ϵ2, Tk(t) = m

≤1"
REFERENCES,0.5667752442996743,"we have
595"
REFERENCES,0.5674267100977198,"B1 ≤m0 + n
X t=1"
REFERENCES,0.5680781758957655,"1
ν exp {−m(Kinf(Fa, µ⋆) −ϵ0)}"
REFERENCES,0.5687296416938111,≤m0 + 1
REFERENCES,0.5693811074918567,"ν T exp {−m0(Kinf(Fa, µ⋆) −ϵ0)}"
REFERENCES,0.5700325732899023,"Choose m0 =
log n
Kinf(Fa,µ⋆)−ϵ0 we have
596"
REFERENCES,0.5706840390879478,"B1 ≤
log n
Kinf(Fa, µ⋆) −ϵ0
+ 1 ν"
REFERENCES,0.5713355048859935,"Also According to Proposition 10 (29), for any ϵ0 > 0 we have
597"
REFERENCES,0.5719869706840391,B2 ≤O(1)
REFERENCES,0.5726384364820847,"That leads us to
598"
REFERENCES,0.5732899022801303,"E[Ta(n)] ≤
log n
Kinf(Fa, µ⋆) −ϵ0
+ o(log n) + O(1),"
REFERENCES,0.5739413680781759,"which concludes the proof.
599"
REFERENCES,0.5745928338762215,"Lemma 7. Consider Categorical Thompson Sampling(CATS) strategy applied to a non-stationary
600"
REFERENCES,0.575244299674267,"problem where the pay-off sequence satisfies Assumption 1. Let us define the power mean estimator
601"
REFERENCES,0.5758957654723127,"bµn(p) as bµn(p) =
PK
a=1
Ta(n)"
REFERENCES,0.5765472312703583,"n
bµp
a,Ta(n)
 1"
REFERENCES,0.5771986970684039,"p , and δ⋆,n = µ⋆−µ⋆,n For any p ≥1, ϵ0 > 0, we have
602"
REFERENCES,0.5778501628664495,"|E[bµn(p)] −µ⋆| ≤|δ⋆,n| + R n K
X"
REFERENCES,0.5785016286644951,"a=1,a̸=a∗"
REFERENCES,0.5791530944625407,(1 + ϵ0) log n
REFERENCES,0.5798045602605864,"K(N)(Fa, µ⋆) + o(log n) + O(1)
"
REFERENCES,0.5804560260586319,"Proof. We observe that
603"
REFERENCES,0.5811074918566775,"|bµn(p) −µ⋆| ≤|bµn(p) −µ⋆,n| + |µ⋆−µ⋆,n| = |bµn(p) −µ⋆,n| + |δ⋆,n|"
REFERENCES,0.5817589576547231,"Furthermore,
604"
REFERENCES,0.5824104234527687,"bµa,Ta(n) ≤µa,n +
bµa,Ta(n) −µa,n
 .
(8)"
REFERENCES,0.5830618892508144,"Since µ⋆,n = maxa∈[K]{µa,n}, we have
605"
REFERENCES,0.5837133550488599,"bµn(p) −µ⋆,n = bµn(p) − K
X"
REFERENCES,0.5843648208469056,"a=1
Ta(n)µ⋆,n ≤ K
X a=1 Ta(n)"
REFERENCES,0.5850162866449511,"n
 
bµa,Ta(n)
p
! 1 p
− K
X a=1 Ta(n)"
REFERENCES,0.5856677524429967,"n
(µa,n)p
! 1 p ="
REFERENCES,0.5863192182410424,"PK
a=1 Ta(n)
 
bµa,Ta(n)
p 1"
REFERENCES,0.5869706840390879,"p −
PK
a=1 Ta(n) (µa,n)p 1 p n
1
p"
REFERENCES,0.5876221498371336,"Applying Minkowski’s inequality from Lemma 3, and the result of (8), we have
606"
REFERENCES,0.5882736156351791,"bµn(p) −µ⋆,n ≤"
REFERENCES,0.5889250814332248,"PK
a=1 Ta(n)
 
µa +
bµa,Ta(n) −µa,n
p 1"
REFERENCES,0.5895765472312704,"p −
PK
a=1 Ta(n) (µa,n)p 1 p n
1
p ≤"
REFERENCES,0.5902280130293159,"PK
a=1 Ta(n)
 bµa,Ta(n) −µa,n
p 1 p n
1
p"
REFERENCES,0.5908794788273616,"On the other hand,
607"
REFERENCES,0.5915309446254071,"µ⋆,n −bµn(p) = nµ⋆,n −nbµn(p)"
REFERENCES,0.5921824104234528,"n
= nµ⋆,n −(PK
a=1 Ta(n)µa,n) + PK
a=1 Ta(n)µa,n −nbµn(p)
n ="
REFERENCES,0.5928338762214984,"PK
a=1,a̸=a∗Ta(n) |µ⋆,n −µa,n| + PK
a=1 Ta(n)µa,n −nbµn(p) n ≤R K
X"
REFERENCES,0.593485342019544,"a=1,a̸=a∗ Ta(n) n
+ K
X a=1 Ta(n)"
REFERENCES,0.5941368078175896,"n
µa,n −bµn(p)
(9)"
REFERENCES,0.5947882736156351,"Because power mean is an increasing function of p, so that
608 K
X a=1 Ta(n)"
REFERENCES,0.5954397394136808,"n
µa,n ≤ K
X a=1 Ta(n)"
REFERENCES,0.5960912052117264,"n
(µa,n)p
!1/p ."
REFERENCES,0.596742671009772,"Furthermore, we observe that
609"
REFERENCES,0.5973941368078176,"µa,n ≤bµa,Ta(n) +
bµa,Ta(n) −µa,n
 .
So that, from equation (9) we have
610"
REFERENCES,0.5980456026058631,"µ⋆,n −bµn(p) ≤R K
X"
REFERENCES,0.5986970684039088,"a=1,a̸=a∗ Ta(n) n
+ K
X a=1 Ta(n)"
REFERENCES,0.5993485342019544,"n
(µa,n)p
!1/p"
REFERENCES,0.6,"−bµn(p) ≤R K
X"
REFERENCES,0.6006514657980456,"a=1,a̸=a∗ Ta(n) n +"
REFERENCES,0.6013029315960912,"PK
a=1 Ta(n)
 
bµa,Ta(n) +
bµa,Ta(n) −µa,n
p 1"
REFERENCES,0.6019543973941368,"p −
PK
a=1 Ta(n)
 
bµa,Ta(n)
p 1 p n
1
p"
REFERENCES,0.6026058631921825,"(Minkovski’s inequality)
≤
R K
X"
REFERENCES,0.603257328990228,"a=1,a̸=a∗ Ta(n) n
+"
REFERENCES,0.6039087947882736,"PK
a=1 Ta(n)
 bµa,Ta(n) −µa,n
p 1 p n
1
p"
REFERENCES,0.6045602605863192,"(Properties of Lp norm)
≤
R K
X"
REFERENCES,0.6052117263843648,"a=1,a̸=a∗ Ta(n) n
+"
REFERENCES,0.6058631921824105,"PK
a=1 Ta(n)
 bµa,Ta(n) −µa,n
 n
1
p = R K
X"
REFERENCES,0.606514657980456,"a=1,a̸=a∗ Ta(n) n
+"
REFERENCES,0.6071661237785017,"PK
a=1

PTa(n)
t
Ra,t −Ta(n)µa,n

 n
1
p"
REFERENCES,0.6078175895765472,"Therefore
611"
REFERENCES,0.6084690553745928,"|E[bµn(p) −µ⋆,n]| ≤R K
X"
REFERENCES,0.6091205211726385,"a=1,a̸=a∗"
REFERENCES,0.609771986970684,E[Ta(n)]
REFERENCES,0.6104234527687297,"n
+
E
hPK
a=1
PTa(n)
t
Ra,t −Ta(n)µa,n

i n
1
p = R K
X"
REFERENCES,0.6110749185667752,"a=1,a̸=a∗"
REFERENCES,0.6117263843648209,E[Ta(n)] n
REFERENCES,0.6123778501628665,"Please note that because we study non-stationary bandits, E[Pn
t Ra,t] = nµa,n, therefore,
612"
REFERENCES,0.613029315960912,"E
hPK
a=1
PTa(n)
t
Ra,t −Ta(n)µa,n

i"
REFERENCES,0.6136807817589577,"n
1
p
= 0"
REFERENCES,0.6143322475570032,"According to Lemma 5, we have
613"
REFERENCES,0.6149837133550489,"|E[bµn(p) −µ⋆,n]| ≤R K
X"
REFERENCES,0.6156351791530945,"a=1,a̸=a∗"
REFERENCES,0.61628664495114,"E[Ta(n)] n
≤R n K
X"
REFERENCES,0.6169381107491857,"a=1,a̸=a∗"
REFERENCES,0.6175895765472312,(1 + ϵ0) log n
REFERENCES,0.6182410423452769,"K(N)(Fa, µ⋆) + o(log n) + O(1)

,"
REFERENCES,0.6188925081433225,"which concludes the proof.
614"
REFERENCES,0.6195439739413681,"Lemma 8. Consider Particle Thompson Sampling(PATS) strategy applied to a non-stationary
615"
REFERENCES,0.6201954397394137,"problem where the pay-off sequence satisfies Assumption 1. Let us define the power mean estimator
616"
REFERENCES,0.6208469055374592,"bµn(p) as bµn(p) =
PK
a=1
Ta(n)"
REFERENCES,0.6214983713355049,"n
bµp
a,Ta(n)
 1"
REFERENCES,0.6221498371335505,"p , and δ⋆,n = µ⋆−µ⋆,n For any p ≥1, ϵ0 > 0, we have
617"
REFERENCES,0.6228013029315961,"|E[bµn(p)] −µ⋆| ≤|δ⋆,n| + R n K
X"
REFERENCES,0.6234527687296417,"a=1,a̸=a∗"
REFERENCES,0.6241042345276873,"
log n
Kinf(Fa, µ⋆) −ϵ0
+ o(log n) + O(1)
"
REFERENCES,0.6247557003257329,"Proof. Similar to Lemma 7, we can derive
618"
REFERENCES,0.6254071661237784,"|E[bµn(p) −µ⋆,n]| ≤|δ⋆,n| + R K
X"
REFERENCES,0.6260586319218241,"a=1,a̸=a∗"
REFERENCES,0.6267100977198697,"E[Ta(n)] n
."
REFERENCES,0.6273615635179153,"And according to Lemma 6, we have
619"
REFERENCES,0.6280130293159609,"|E[bµn(p) −µ⋆,n]| ≤R K
X"
REFERENCES,0.6286644951140065,"a=1,a̸=a∗"
REFERENCES,0.6293159609120521,"E[Ta(n)] n
≤R n K
X"
REFERENCES,0.6299674267100978,"a=1,a̸=a∗"
REFERENCES,0.6306188925081433,"
log n
Kinf(Fa, µ⋆) −ϵ0
+ o(log n) + O(1)

,"
REFERENCES,0.6312703583061889,"which concludes the proof.
620"
REFERENCES,0.6319218241042345,"Theorem 1. For a ∈[K], let (bµa,n)n≥1 be a sequence of estimator satisfying plim
n→∞
bµa,n = µa and
621"
REFERENCES,0.6325732899022801,"let µ⋆= max
a {µa}. Assume that all the estimators are bounded in [0, R]. We consider a bandit
622"
REFERENCES,0.6332247557003258,"algorithm that selects each arm according to CATS once in each round n ≥K.
623"
REFERENCES,0.6338762214983713,"Then, for all p ∈[1, ∞), the sequence of estimators
624"
REFERENCES,0.634527687296417,"bµn(p) = K
X a=1 Ta(n)"
REFERENCES,0.6351791530944625,"n
bµp
a,Ta(n) ! 1 p
,"
REFERENCES,0.6358306188925081,"where Ta(n) = Pn−1
t=1 1(at = a) is the number of selections of a prior to round n satisfies
625"
REFERENCES,0.6364820846905538,"plim
n→∞
bµn(p) = µ⋆."
REFERENCES,0.6371335504885993,"Proof. We first prove that lim
n→∞E[bµn(p)] = µ∗. According to the result of Lemma 7, we have
626"
REFERENCES,0.637785016286645,"|E[bµn(p)] −µ⋆| ≤|δ⋆,n| + R K
X"
REFERENCES,0.6384364820846905,"a=1,a̸=a∗"
REFERENCES,0.6390879478827362,E[Ta(n)] n
REFERENCES,0.6397394136807818,"≤|δ⋆,n| + R n K
X"
REFERENCES,0.6403908794788273,"a=1,a̸=a∗"
REFERENCES,0.641042345276873,(1 + ϵ0) log n
REFERENCES,0.6416938110749185,"K(N)(Fa, µ⋆) + o(log n) + O(1)
"
REFERENCES,0.6423452768729642,"with δ⋆,n = µ⋆−µ⋆,n, and because lim
n→∞µ∗,n = µ⋆, we can concludes that
627"
REFERENCES,0.6429967426710098,"lim
n→∞E[bµn(p)] = µ∗."
REFERENCES,0.6436482084690553,"Second, we prove that
628"
REFERENCES,0.644299674267101,"∀n ≥1, ∀ε > 0, ∃c > 0 that P (|bµn(p) −µ⋆| > ε) ≤cn−1ε−1."
REFERENCES,0.6449511400651465,"We observe that
629"
REFERENCES,0.6456026058631922,"|bµn(p) −µ⋆| ≤|bµn(p) −µ⋆,n| + |µ⋆−µ⋆,n| = |bµn(p) −µ⋆,n| + |δ⋆,n|
=⇒P(|bµn(p) −µ⋆| ≥ϵ) ≤P(|bµn(p) −µ⋆,n| ≥ϵ/2) + P(|δ⋆,n| ≥ϵ/2)."
REFERENCES,0.6462540716612378,"Because lim
n→n|δ⋆,n| = 0, therefore, ∃N0 > 0 such that ∀n ≥N0, we have |δ⋆,n| < ϵ/2 that means
630"
REFERENCES,0.6469055374592834,"∀n > N0, P(|δ⋆,n| ≥ϵ/2) = 0."
REFERENCES,0.647557003257329,"Next, according to Lemma 7,
631"
REFERENCES,0.6482084690553745,"|E[bµn(p)] −µ⋆,n| ≤R n K
X"
REFERENCES,0.6488599348534202,"a=1,a̸=a∗"
REFERENCES,0.6495114006514658,(1 + ϵ0) log n
REFERENCES,0.6501628664495114,"K(N)(Fa, µ⋆) + o(log n) + O(1)

= O(n−1),"
REFERENCES,0.650814332247557,"that leads to
632"
REFERENCES,0.6514657980456026,"P(|bµn(p) −µ⋆,n| ≥ϵ/2) ≤|E[bµn(p)] −µ⋆,n|"
REFERENCES,0.6521172638436482,"ϵ/2
= O(n−1) ϵ/2
."
REFERENCES,0.6527687296416939,"Therefore, ∃c > 0 such that
633"
REFERENCES,0.6534201954397394,"P(|bµn(p) −µ⋆,n| ≥ϵ/2) ≤cn−1ϵ−1,"
REFERENCES,0.654071661237785,"which means
634"
REFERENCES,0.6547231270358306,"∀n ≥N0, ∀ε > 0, ∃c > 0 that P (|bµn(p) −µ⋆| > ε) ≤cn−1ε−1."
REFERENCES,0.6553745928338762,"Now we see that |bµn(p) −µ⋆| ≤R. With ϵ ≥R, we have |bµn(p) −µ⋆| > ϵ ⇔|bµn(p) −µ⋆| > R,
635"
REFERENCES,0.6560260586319219,"therefore the inequality holds as
636"
REFERENCES,0.6566775244299674,P (|bµn(p) −µ⋆| > ε) = 0 ≤cn−1ε−1.
REFERENCES,0.657328990228013,"with 0 < ϵ < R, 1 ≤n < N0 ⇒nϵ < RN0 ⇒n−1ε−1 > 1/RN0. Therefore
637"
REFERENCES,0.6579804560260586,"∀C > 1/RN0 ⇒P (|bµn(p) −µ⋆| > ε) ≤1 < Cn−1ε−1,"
REFERENCES,0.6586319218241042,"which means
638"
REFERENCES,0.6592833876221499,"∀n ≥1, ∀ε > 0, ∃C > 0 that P (|bµn(p) −µ⋆| > ε) ≤Cn−1ε−1."
REFERENCES,0.6599348534201954,"That concludes the proof.
639"
REFERENCES,0.6605863192182411,"Theorem 2. For a ∈[K], let (bµa,n)n≥1 be a sequence of estimator satisfying plim
n→∞
bµa,n = µa and
640"
REFERENCES,0.6612377850162866,"let µ⋆= max
a {µa}. Assume that all the estimators are bounded in [0, R]. We consider a bandit
641"
REFERENCES,0.6618892508143323,"algorithm that selects each arm according to PATS once in each round n ≥K.
642"
REFERENCES,0.6625407166123779,"Then, for all p ∈[1, ∞), the sequence of estimators
643"
REFERENCES,0.6631921824104234,"bµn(p) = K
X a=1 Ta(n)"
REFERENCES,0.6638436482084691,"n
bµp
a,Ta(n) ! 1 p
,"
REFERENCES,0.6644951140065146,"where Ta(n) = Pn−1
t=1 1(at = a) is the number of selections of a prior to round n satisfies
644"
REFERENCES,0.6651465798045603,"plim
n→∞
bµn(p) = µ⋆."
REFERENCES,0.6657980456026059,"Proof. The proof follows the same steps as Theorem 1. We first prove that lim
n→∞E[bµn(p)] = µ∗.
645"
REFERENCES,0.6664495114006515,"According to the result of Lemma 8, we have
646"
REFERENCES,0.6671009771986971,"|E[bµn(p)] −µ⋆| ≤|δ⋆,n| + R K
X"
REFERENCES,0.6677524429967426,"a=1,a̸=a∗"
REFERENCES,0.6684039087947883,E[Ta(n)] n
REFERENCES,0.6690553745928339,"≤|δ⋆,n| + R n K
X"
REFERENCES,0.6697068403908795,"a=1,a̸=a∗"
REFERENCES,0.6703583061889251,"
log n
Kinf(Fa, µ⋆) −ϵ0
+ o(log n) + O(1)
"
REFERENCES,0.6710097719869706,"with δ⋆,n = µ⋆−µ⋆,n, and because lim
n→∞µ∗,n = µ⋆, we can concludes that
647"
REFERENCES,0.6716612377850163,"lim
n→∞E[bµn(p)] = µ∗."
REFERENCES,0.6723127035830619,"Second, we prove that
648"
REFERENCES,0.6729641693811075,"∀n ≥1, ∀ε > 0, ∃c > 0 that P (|bµn(p) −µ⋆| > ε) ≤cn−1ε−1."
REFERENCES,0.6736156351791531,"We observe that
649"
REFERENCES,0.6742671009771987,"|bµn(p) −µ⋆| ≤|bµn(p) −µ⋆,n| + |µ⋆−µ⋆,n| = |bµn(p) −µ⋆,n| + |δ⋆,n|
=⇒P(|bµn(p) −µ⋆| ≥ϵ) ≤P(|bµn(p) −µ⋆,n| ≥ϵ/2) + P(|δ⋆,n| ≥ϵ/2)."
REFERENCES,0.6749185667752443,"Because lim
n→n|δ⋆,n| = 0, therefore, ∃N0 > 0 such that ∀n ≥N0, we have |δ⋆,n| < ϵ/2 that means
650"
REFERENCES,0.6755700325732898,"∀n > N0, P(|δ⋆,n| ≥ϵ/2) = 0."
REFERENCES,0.6762214983713355,"Next, according to Lemma 8,
651"
REFERENCES,0.6768729641693811,"|E[bµn(p)] −µ⋆,n| ≤R n K
X"
REFERENCES,0.6775244299674267,"a=1,a̸=a∗"
REFERENCES,0.6781758957654723,"
log n
Kinf(Fa, µ⋆) −ϵ0
+ o(log n) + O(1)

= O(n−1),"
REFERENCES,0.6788273615635179,"that leads to
652"
REFERENCES,0.6794788273615635,"P(|bµn(p) −µ⋆,n| ≥ϵ/2) ≤|E[bµn(p)] −µ⋆,n|"
REFERENCES,0.6801302931596092,"ϵ/2
= O(n−1) ϵ/2
."
REFERENCES,0.6807817589576547,"Therefore, ∃c > 0 such that
653"
REFERENCES,0.6814332247557003,"P(|bµn(p) −µ⋆,n| ≥ϵ/2) ≤cn−1ϵ−1,"
REFERENCES,0.6820846905537459,"which means
654"
REFERENCES,0.6827361563517915,"∀n ≥N0, ∀ε > 0, ∃c > 0 that P (|bµn(p) −µ⋆| > ε) ≤cn−1ε−1."
REFERENCES,0.6833876221498372,"Now we see that |bµn(p) −µ⋆| ≤R. With ϵ ≥R, we have |bµn(p) −µ⋆| > ϵ ⇔|bµn(p) −µ⋆| > R,
655"
REFERENCES,0.6840390879478827,"therefore the inequality holds as
656"
REFERENCES,0.6846905537459284,P (|bµn(p) −µ⋆| > ε) = 0 ≤cn−1ε−1.
REFERENCES,0.6853420195439739,"with 0 < ϵ < R, 1 ≤n < N0 ⇒nϵ < RN0 ⇒n−1ε−1 > 1/RN0. Therefore
657"
REFERENCES,0.6859934853420195,"∀C > 1/RN0 ⇒P (|bµn(p) −µ⋆| > ε) ≤1 < Cn−1ε−1,"
REFERENCES,0.6866449511400652,"which means
658"
REFERENCES,0.6872964169381107,"∀n ≥1, ∀ε > 0, ∃C > 0 that P (|bµn(p) −µ⋆| > ε) ≤Cn−1ε−1."
REFERENCES,0.6879478827361564,"That concludes the proof.
659"
REFERENCES,0.6885993485342019,"E
Convergence of CATS and PATS in Monte-Carlo Tree Search
660"
REFERENCES,0.6892508143322476,"Based upon the results of CATS and PATS using power mean as the value backup operator on the
661"
REFERENCES,0.6899022801302932,"described non-stationary multi-armed bandit problem, we derive theoretical results for CATS in an
662"
REFERENCES,0.6905537459283387,"MCTS tree.
663"
REFERENCES,0.6912052117263844,"We derive Theorem 3 for CATS and Theorem 4 for PATS, which show concentration and convergence
664"
REFERENCES,0.6918566775244299,"for any internal node in the tree. These proofs utilize induction, leveraging the results of Lemma 7
665"
REFERENCES,0.6925081433224756,"for CATS and Lemma 8 for PATS, and Lemma 5 for CATS and Lemma 6 for PATS. Additionally, we
666"
REFERENCES,0.6931596091205212,"use Lemma 1, which demonstrates the concentration and convergence of an estimated Q-value based
667"
REFERENCES,0.6938110749185668,"on the child V-value node, applying it recursively throughout the tree.
668"
REFERENCES,0.6944625407166124,"Our main results, Theorem 5 for CATS and Theorem 5 for PATS, show that the simple regret
669"
REFERENCES,0.6951140065146579,"converges non-asymptotically at a rate of O(n−1).
670"
REFERENCES,0.6957654723127036,"Theorem 3. When we apply the CATS algorithm, we have
671"
REFERENCES,0.6964169381107492,"(i) For any node sh at the depth hth in the tree,
672"
REFERENCES,0.6970684039087948,"plim
n→∞
bQn(sh, ak) = eQ(sh, ak)."
REFERENCES,0.6977198697068404,"673
(ii) For any node sh at the depth hth in the tree,
674"
REFERENCES,0.698371335504886,"plim
n→∞
bVn(sh) = eV (sh)."
REFERENCES,0.6990228013029316,"Proof. We will prove this by induction on the depth D of the tree. If the tree only has depth (1).
675"
REFERENCES,0.6996742671009772,"The state at the root node is s0, let us assume that at time step t, after taking action ak, the MCTS tree
676"
REFERENCES,0.7003257328990228,"gets an intermediate reward rt(s0, ak) and traverses to the next state s1. Let us assume that R(s0, ak)
677"
REFERENCES,0.7009771986970684,"is the mean of the intermediate reward at state s0, after taking action ak. We recall the definition of
678"
REFERENCES,0.701628664495114,"eQ(s0, ak), with π0 is the rollout policy to estimate the newly added node at the leaf,
679"
REFERENCES,0.7022801302931596,"eQ(s0, ak) = R(s0, ak) + γ
X"
REFERENCES,0.7029315960912053,"s1∈As0
P(s1|s0, ak)eV (s1)"
REFERENCES,0.7035830618892508,"where eV (s1) is the value of the policy π0 at state s1, As0 is the set of feasible actions at state s0,
680"
REFERENCES,0.7042345276872964,"|As0| = M, P(s1|s0, ak) is the probability transition of taking action ak at state s0 to state s1. From
681"
REFERENCES,0.704885993485342,"((1)), we have
682"
REFERENCES,0.7055374592833876,"bQn(s0, ak) = 1 n n
X"
REFERENCES,0.7061889250814333,"t=1
rt(s0, ak) + γ
X"
REFERENCES,0.7068403908794788,"s1∼τ(s0,ak)"
REFERENCES,0.7074918566775245,"T s1
s0,ak(n)"
REFERENCES,0.70814332247557,"n
bVT s1
s0,ak (n)(s1)"
REFERENCES,0.7087947882736156,"(i) is a direct result of Lemma 1 with Xt is the intermediate reward rt(s0, ak) at time t, p =
683"
REFERENCES,0.7094462540716613,"(p1, p2, ...pM) ∼P(·|s0, ak), where P(·|s0, ak) is the probability transition dynamic of taking action
684"
REFERENCES,0.7100977198697068,"ak at state s0. For m ∈[M], each (bVm,t)t≥1 at time step t is the deterministic initial Value function
685"
REFERENCES,0.7107491856677525,"eV (s1). We have
686"
REFERENCES,0.711400651465798,"plim
n→∞
bVm,n(s1) = eV (s1), with s1 ∈{sm}, m = 1, 2, 3...M, where sm ∼τ(·|s0, ak)"
REFERENCES,0.7120521172638437,"(ii) Direct results from Theorem 1. In detail, we have from (i),
687"
REFERENCES,0.7127035830618893,"plim
n→∞
bQn(s0, ak) = eQ(s0, ak), with ak ∈As0"
REFERENCES,0.7133550488599348,"Because by definition:
688"
REFERENCES,0.7140065146579805,"eV (s0) = max
ak∈As0
eQ(s0, ak)"
REFERENCES,0.714657980456026,bVn(s0) =  X a∈As0
REFERENCES,0.7153094462540717,"Ts0,a(n) n"
REFERENCES,0.7159609120521173,"
bQTs0,a(n)(s0, a)
p
 "
P,0.7166123778501629,"1
p"
P,0.7172638436482085,"for some p ∈[1, +∞)"
P,0.717915309446254,"Then we have
689"
P,0.7185667752442997,"plim
n→∞
bVn(s0) = eV (s0)"
P,0.7192182410423453,"that concludes for (ii)
690"
P,0.7198697068403909,"Let us assume that with the tree of depth D, the theorem holds for all its children.
691"
P,0.7205211726384365,"Now let’s consider the tree with depth (D + 1). When we take one action at the root node at the state
692"
P,0.721172638436482,"s0, it comes to a subtree with depth (D). According to the induction assumption, the results hold for
693"
P,0.7218241042345277,"any internal node in the tree after we take the first action. We have s1 ∼τ(s0, ak). By the definition,
694"
P,0.7224755700325733,"eV (sH) = V0(sH) and, for all h ≤H −1,
695"
P,0.7231270358306189,"eQ(sh, a)
=
R(sh, a) + γ
X"
P,0.7237785016286645,"sh+1∈As
P(sh+1|sh, a)eV (sh+1)"
P,0.7244299674267101,"eV (sh)
=
max
a
eQ(sh, a)"
P,0.7250814332247557,"By the assumption of the induction the root node of a subtree with depth (D) at state s1 we have
696"
P,0.7257328990228012,"plim
n→∞
bVn(s1) = eV (s1)"
P,0.7263843648208469,"(i) Let’s apply Lemma 1 with {Xt} is the intermediate reward {rt(s0, ak)}, p = (p1, p2, ...pM) ∼
697"
P,0.7270358306188925,"P(·|s0, ak). For m ∈[M], each (bVm,t)t≥1 at time step t is the empirical Value function bVt(s1). We
698"
P,0.7276872964169381,"will have
699"
P,0.7283387622149837,"plim
n→∞
bQn(s0, ak) = eQ(s0, ak), with ak ∈As0"
P,0.7289902280130293,"(ii) follows the results of Theorem 1 as at the root node s0 of depth D + 1, with
700"
P,0.7296416938110749,"eV (s0) = max
ak∈As0
eQ(s0, ak)"
P,0.7302931596091206,bVn(s0) = X a∈As
P,0.7309446254071661,"Ts0,a(n) n"
P,0.7315960912052117,"
bQTs0,a(n)(s0, a)
p
! 1"
P,0.7322475570032573,"p
for some p ∈[1, +∞)"
P,0.7328990228013029,"And because
701"
P,0.7335504885993486,"plim
n→∞
bQn(s0, ak) = eQ(s0, ak), with ak ∈As0"
P,0.7342019543973941,"Then, we have
702"
P,0.7348534201954398,"plim
n→∞
bVn(s0) = eV (s0)."
P,0.7355048859934853,"that concludes for (ii)
703"
P,0.7361563517915309,"The results of Theorem 3 hold for any node in the tree with the tree of depth (D + 1). By induction,
704"
P,0.7368078175895766,"we can conclude the proof.
705"
P,0.7374592833876221,"Similarly we can derive the following Theorem
706"
P,0.7381107491856678,"Theorem 4. When we apply the PATS algorithm, we have
707"
P,0.7387622149837133,"(i) For any node sh at the depth hth in the tree,
708"
P,0.739413680781759,"plim
n→∞
bQn(sh, ak) = eQ(sh, ak)."
P,0.7400651465798046,"709
(ii) For any node sh at the depth hth in the tree,
710"
P,0.7407166123778501,"plim
n→∞
bVn(sh) = eV (sh)."
P,0.7413680781758958,"Proof. The proof follows the same steps as Theorem 3 by applying the results of Lemma 1 and
711"
P,0.7420195439739413,"Theorem 2.
712"
P,0.742671009771987,"Theorem 5. (Convergence of Expected Payoff of CATS) We have at the root node s0,
713"
P,0.7433224755700326,"E
hbVn (s0) −V ⋆(s0)

i
≤O(n−1)."
P,0.7439739413680782,"Proof. We prove the result by induction and use the results of Theorem 3 to prove this Theorem. Let
714"
P,0.7446254071661238,"us assume that the depth of the tree is D = 1, as the results of Lemma 7, we have
715"
P,0.7452768729641693,"E[bVn(s0)] −V ⋆(s0)
 ≤|δ⋆,n| + O(log n"
P,0.745928338762215,"n
) = |δ⋆,n| + O(n−1)."
P,0.7465798045602606,"And because the tree only have the depth D = 1, we have |δ⋆,n| = 0, so that the result holds at
716"
P,0.7472312703583062,"the depth D = 1. Let us assume that we have the result of the tree at the depth D. Now when the
717"
P,0.7478827361563518,"depth of the tree is D + 1, at the root node s0, the conditions of Assumption 1 hold as the results of
718"
P,0.7485342019543973,"Theorem 3 then we have
719"
P,0.749185667752443,"E[bVn(s0)] −V ⋆(s0)

(Lemma 7)
≤
|δ⋆,n| + O(log n"
P,0.7498371335504886,"n
) = |δ⋆,n| + O(n−1),"
P,0.7504885993485342,"where the bias
720"
P,0.7511400651465798,"|δ⋆,n| =
E[ bQn(s0, a⋆)] −Q⋆(s0, a⋆)

(contraction)
≤
γ ∥E[bV (1)
n
] −V ⋆∥∞"
P,0.7517915309446254,"(by induction)
≤
γO(n−1)."
P,0.752442996742671,"Therefore,
721"
P,0.7530944625407167,"E[bVn(s0)] −V ⋆(s0)
 ≤O(n−1),"
P,0.7537459283387622,"that concludes the proof.
722"
P,0.7543973941368078,"Next, we present the results of Theorem 6. The proof follows the same steps as Theorem 5.
723"
P,0.7550488599348534,"Theorem 6. (Convergence of Expected Payoff of PATS) We have at the root node s0,
724"
P,0.755700325732899,"E
hbVn (s0) −V ⋆(s0)

i
≤O(n−1)."
P,0.7563517915309447,"F
Limitations
725"
P,0.7570032573289902,"Computational Demands: The CATS distributional Monte Carlo Tree Search (MCTS) faces chal-
726"
P,0.7576547231270359,"lenges in managing computational demands while maintaining and updating probability distributions,
727"
P,0.7583061889250814,"leading to a slightly increased complexity.
728"
P,0.758957654723127,"Fixed precision: The PATS set of particles can increase in size if the observed value are different.
729"
P,0.7596091205211727,"We prevent this in the implementation by fixing the float precision.
730"
P,0.7602605863192182,"Number of atoms: Our approach’s performance is slightly influenced by hyperparameters, with the
731"
P,0.7609120521172639,"number of atoms being a critical factor. Suboptimal choices may affect performance.
732"
P,0.7615635179153094,"G
Experimental setup
733"
P,0.762214983713355,"All the experiments were done on 8 Intel Xeon Gold 6130 (Skylake), x86_64, 2.10GHz, 2 CPUs/node,
734"
P,0.7628664495114007,"16 cores/CPU. Whenever feasible, we opted for open-source implementations of algorithms and
735"
P,0.7635179153094462,"environments.
736 737"
P,0.7641693811074919,"Parameters selection We search the number of atoms from {10,20,...,100} and choose the
738"
P,0.7648208469055374,"results with best performances. We set the discount factor γ = .99 for MDPs, and γ = .95 for
739"
P,0.7654723127035831,"POMDPs. For UCT, we use the exploration constant C =
√"
P,0.7661237785016287,"2 × (Rmax −Rmin).
740"
P,0.7667752442996743,"Atari hyperparameters We run CATS in Atari with 10 random seeds, where each seed with 512
741"
P,0.7674267100977199,"samples and collect the average score. We found that only 512 simulations were necessary due to the
742"
P,0.7680781758957654,"utilization of a pretrained neural network. We run CATS with 100 atoms. The temperature parameter
743"
P,0.7687296416938111,"τ of MENTS and TENTS is tuned from {0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.2,
744"
P,0.7693811074918567,"0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}. The selected parameter τ are shown in Table 4. The exploration
745"
P,0.7700325732899023,"constant ϵ for MENTS and TENTS are set to 0.01. For Power-UCT, we select the power mean p = 2.
746"
P,0.7706840390879479,"Atari
747"
P,0.7713355048859935,Table 3: Average scores in Atari with 512 samples (10 seeds) ± 2 times std.
P,0.7719869706840391,"CATS
PATS
UCT
DQN
Power-UCT
TENTS
MENTS
Phoenix
3290.00 ± 1599.52
3619.00 ± 891.72
2450.00 ± 786.22
340.0 ± 0.00
560.00 ± 0.00
4423.00 ± 642.38
3098.30 ± 919.65
MsPacman
2058.00 ± 243.93
2232.00 ± 896.29
1792.00 ± 62.85
1930.00 ± 224.83
1982.00 ± 473.45
1922.00 ± 416.91
2018.30 ± 316.98
Alien
1765.0 ± 801.03
1724.00 ± 649.63
1900.00 ± 00.00
1094.00 ± 122.83
1748.00 ± 120.21
1613.00 ± 296.96
1508.60 ± 322.58
SpaceInvaders
826.0 ± 194.76
791.0 ± 332.52
525.00 ± 00.00
525.00 ± 0.00
672.00 ± 148.42
742.50 ± 193.53
832.55 ± 211.95
BeamRider
1952.00 ± 500.04
1848.0 ± 320.29
1889.60 ± 171.09
1952.00 ± 0.00
1577.60 ± 112.47
3013.00 ± 778.89
2822.18 ± 697.31
Asterix
6040.00 ± 1560.89
5495.00 ± 3106.64
5380.00 ± 1464.05
6220.00 ± 156.80
5540.00 ± 863.39
5180.00 ± 528.19
5576.00 ± 1397.91
Robotank
11.50 ± 2.11
11.9 ± 1.51
12.2 ± 1.04
10.20 ± 0.39
11.00 ± 1.55
12.10 ± 1.47
11.59 ± 1.36
Seaquest
3170.00 ± 787.61
3288.0 ± 889.41
3564.00 ± 86.83
2304.00 ± 531.31
2704.00 ± 318.93
2928.00 ± 801.11
3312.40 ± 390.77
Solaris
1062.0 ± 519.21
1196.00 ± 524.45
392.00 ± 198.61
1112.00 ± 521.53
452.00 ± 153.19
1168.00 ± 516.33
1118.20 ± 513.00
Asteroids
930.00 ± 100.12
953.00 ± 107.05
5380.00 ± 1464.05
860.00 ± 48.89
930.00 ± 54.66
1518.00 ± 121.48
1414.70 ± 261.59
Enduro
142.40 ± 31.21
131.10 ± 17.16
127.00 ± 10.07
133.60 ± 8.73
134.00 ± 6.69
115.40 ± 18.82
128.79 ± 16.26
Atlantis
35890.00 ± 1914.28
36180.0 ± 2592.70
34300.00 ± 00.00
34480.00 ± 119.76
35420.00 ± 1494.63
36280.00 ± 1476.24
36277.00 ± 1811.53
Hero
3006.50 ± 9.16
3020.50 ± 27.24
3011.50 ± 17.04
3005.00 ± 9.53
2998.00 ± 35.16
3008.00 ± 0.00
3044.55 ± 181.04
Frostbite
1582.00 ± 1041.37
1580.00 ± 1127.23
1900.00 ± 00.00
2407.00 ± 116.76
1754.00 ± 651.38
2357.00 ± 398.45
2388.20 ± 320.37
WizardOfWor
670.0 ± 192.09
590.00 ± 359.02
200.00 ± 00.00
530.00 ± 92.63
640.00 ± 134.53
1210.00 ± 183.52
1211.00 ± 314.30
Breakout
315.00 ± 85.80
302.10 ± 70.47
271.8 ± 54.63
288.10 ± 53.01
289.00 ± 44.46
337.00 ± 15.91
309.03 ± 35.13"
P,0.7726384364820847,"Atari environments (4) provide diverse video game-inspired scenarios commonly used in reinforce-
748"
P,0.7732899022801303,"ment learning research. These environments offer challenges based on classic Atari 2600 games
749"
P,0.7739413680781759,"(23; 38; 6). To explore enhanced exploration in deep reinforcement learning, we employ a Deep
750"
P,0.7745928338762215,"Q-Network pre-trained following the experimental setup outlined in (23). This pre-trained network
751"
P,0.7752442996742671,"initializes action-values for each node, combined with a Monte-Carlo Tree Search method similar to
752"
P,0.7758957654723126,"the AlphaGo one. Here, Pprior represents the Boltzmann distribution derived from the action-values
753"
P,0.7765472312703583,"Q(s, .) computed by the network. The results in Table 3 show that CATS and PATS outperform UCT,
754"
P,0.7771986970684039,"DQN, Power-UCT, TENTS and MENTS in most of the games. For example, CATS is significant
755"
P,0.7778501628664495,"better than other methods in Breakout, Enduro, while PATS is significant better than other methods
756"
P,0.7785016286644951,"in MsPacman, Solaris. Our intention in this experiment is not to assert exceptional superiority, but
757"
P,0.7791530944625407,"rather to emphasize that CATS and PATS actually work in complicated Atari benchmark.
758"
P,0.7798045602605863,Table 4: The hyperparameter τ (temperature) for MENTS and TENTS in Atari.
P,0.780456026058632,"MENTS
TENTS
Phoenix
0.07
0.6
MsPacman
0.09
0.03
Alien
0.1
0.03
SpaceInvaders
0.02
0.06
BeamRider
0.02
0.03
Asterix
0.02
0.1
Robotank
0.01
0.05
Seaquest
0.02
0.03
Solaris
0.03
0.06
Asteroids
0.08
0.2
Qbert
0.02
0.4
Enduro
0.02
0.1
Atlantis
0.08
0.03
Hero
0.4
0.03
Frostbite
0.01
0.02
WizardOfWor
0.1
0.01
Breakout
0.02
0.04"
P,0.7811074918566775,"NeurIPS Paper Checklist
759"
P,0.7817589576547231,"The checklist is designed to encourage best practices for responsible machine learning research,
760"
P,0.7824104234527687,"addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove
761"
P,0.7830618892508143,"the checklist: The papers not including the checklist will be desk rejected. The checklist should
762"
P,0.78371335504886,"follow the references and precede the (optional) supplemental material. The checklist does NOT
763"
P,0.7843648208469055,"count towards the page limit.
764"
P,0.7850162866449512,"Please read the checklist guidelines carefully for information on how to answer these questions. For
765"
P,0.7856677524429967,"each question in the checklist:
766"
P,0.7863192182410423,"• You should answer [Yes] , [No] , or [NA] .
767"
P,0.786970684039088,"• [NA] means either that the question is Not Applicable for that particular paper or the
768"
P,0.7876221498371335,"relevant information is Not Available.
769"
P,0.7882736156351792,"• Please provide a short (1–2 sentence) justification right after your answer (even for NA).
770"
P,0.7889250814332247,"The checklist answers are an integral part of your paper submission. They are visible to the
771"
P,0.7895765472312704,"reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it
772"
P,0.790228013029316,"(after eventual revisions) with the final version of your paper, and its final version will be published
773"
P,0.7908794788273615,"with the paper.
774"
P,0.7915309446254072,"The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.
775"
P,0.7921824104234527,"While ""[Yes] "" is generally preferable to ""[No] "", it is perfectly acceptable to answer ""[No] "" provided a
776"
P,0.7928338762214984,"proper justification is given (e.g., ""error bars are not reported because it would be too computationally
777"
P,0.793485342019544,"expensive"" or ""we were unable to find the license for the dataset we used""). In general, answering
778"
P,0.7941368078175896,"""[No] "" or ""[NA] "" is not grounds for rejection. While the questions are phrased in a binary way, we
779"
P,0.7947882736156352,"acknowledge that the true answer is often more nuanced, so please just use your best judgment and
780"
P,0.7954397394136807,"write a justification to elaborate. All supporting evidence can appear either in the main paper or the
781"
P,0.7960912052117264,"supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification
782"
P,0.796742671009772,"please point to the section(s) where related material for the question can be found.
783"
P,0.7973941368078176,"IMPORTANT, please:
784"
P,0.7980456026058632,"• Delete this instruction block, but keep the section heading “NeurIPS paper checklist"",
785"
P,0.7986970684039087,"• Keep the checklist subsection headings, questions/answers and guidelines below.
786"
P,0.7993485342019544,"• Do not modify the questions and only use the provided macros for your answers.
787"
P,0.8,"(i) Claims
788"
P,0.8006514657980456,"Question: Do the main claims made in the abstract and introduction accurately reflect the
789"
P,0.8013029315960912,"paper’s contributions and scope?
790"
P,0.8019543973941368,"Answer: [Yes] ,
791"
P,0.8026058631921824,"Justification: We discuss the problem of planning in stochastic environments and we present
792"
P,0.8032573289902281,"a method to tackle problem with clear contributions.
793"
P,0.8039087947882736,"Guidelines:
794"
P,0.8045602605863192,"• The answer NA means that the abstract and introduction do not include the claims
795"
P,0.8052117263843648,"made in the paper.
796"
P,0.8058631921824104,"• The abstract and/or introduction should clearly state the claims made, including the
797"
P,0.8065146579804561,"contributions made in the paper and important assumptions and limitations. A No or
798"
P,0.8071661237785016,"NA answer to this question will not be perceived well by the reviewers.
799"
P,0.8078175895765473,"• The claims made should match theoretical and experimental results, and reflect how
800"
P,0.8084690553745928,"much the results can be expected to generalize to other settings.
801"
P,0.8091205211726384,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
802"
P,0.8097719869706841,"are not attained by the paper.
803"
P,0.8104234527687296,"(ii) Limitations
804"
P,0.8110749185667753,"Question: Does the paper discuss the limitations of the work performed by the authors?
805"
P,0.8117263843648208,"Answer: [Yes]
806"
P,0.8123778501628665,"Justification: We discuss the limitation in Section 6
807"
P,0.8130293159609121,"Guidelines:
808"
P,0.8136807817589576,"• The answer NA means that the paper has no limitation while the answer No means that
809"
P,0.8143322475570033,"the paper has limitations, but those are not discussed in the paper.
810"
P,0.8149837133550488,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
811"
P,0.8156351791530945,"• The paper should point out any strong assumptions and how robust the results are to
812"
P,0.8162866449511401,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
813"
P,0.8169381107491857,"model well-specification, asymptotic approximations only holding locally). The authors
814"
P,0.8175895765472313,"should reflect on how these assumptions might be violated in practice and what the
815"
P,0.8182410423452768,"implications would be.
816"
P,0.8188925081433225,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
817"
P,0.8195439739413681,"only tested on a few datasets or with a few runs. In general, empirical results often
818"
P,0.8201954397394137,"depend on implicit assumptions, which should be articulated.
819"
P,0.8208469055374593,"• The authors should reflect on the factors that influence the performance of the approach.
820"
P,0.8214983713355049,"For example, a facial recognition algorithm may perform poorly when image resolution
821"
P,0.8221498371335505,"is low or images are taken in low lighting. Or a speech-to-text system might not be
822"
P,0.8228013029315961,"used reliably to provide closed captions for online lectures because it fails to handle
823"
P,0.8234527687296417,"technical jargon.
824"
P,0.8241042345276873,"• The authors should discuss the computational efficiency of the proposed algorithms
825"
P,0.8247557003257329,"and how they scale with dataset size.
826"
P,0.8254071661237785,"• If applicable, the authors should discuss possible limitations of their approach to
827"
P,0.826058631921824,"address problems of privacy and fairness.
828"
P,0.8267100977198697,"• While the authors might fear that complete honesty about limitations might be used by
829"
P,0.8273615635179153,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
830"
P,0.8280130293159609,"limitations that aren’t acknowledged in the paper. The authors should use their best
831"
P,0.8286644951140065,"judgment and recognize that individual actions in favor of transparency play an impor-
832"
P,0.8293159609120521,"tant role in developing norms that preserve the integrity of the community. Reviewers
833"
P,0.8299674267100977,"will be specifically instructed to not penalize honesty concerning limitations.
834"
P,0.8306188925081434,"(iii) Theory Assumptions and Proofs
835"
P,0.8312703583061889,"Question: For each theoretical result, does the paper provide the full set of assumptions and
836"
P,0.8319218241042345,"a complete (and correct) proof?
837"
P,0.8325732899022801,"Answer: [Yes]
838"
P,0.8332247557003257,"Justification: We provide the main theorems in the main paper and proofs in the appendix.
839"
P,0.8338762214983714,"Guidelines:
840"
P,0.8345276872964169,"• The answer NA means that the paper does not include theoretical results.
841"
P,0.8351791530944626,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
842"
P,0.8358306188925081,"referenced.
843"
P,0.8364820846905537,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
844"
P,0.8371335504885994,"• The proofs can either appear in the main paper or the supplemental material, but if
845"
P,0.8377850162866449,"they appear in the supplemental material, the authors are encouraged to provide a short
846"
P,0.8384364820846906,"proof sketch to provide intuition.
847"
P,0.8390879478827361,"• Inversely, any informal proof provided in the core of the paper should be complemented
848"
P,0.8397394136807818,"by formal proofs provided in appendix or supplemental material.
849"
P,0.8403908794788274,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
850"
P,0.8410423452768729,"(iv) Experimental Result Reproducibility
851"
P,0.8416938110749186,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
852"
P,0.8423452768729641,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
853"
P,0.8429967426710098,"of the paper (regardless of whether the code and data are provided or not)?
854"
P,0.8436482084690554,"Answer: [Yes]
855"
P,0.844299674267101,"Justification: Code and reproducibility steps are provided in supplementary material.
856"
P,0.8449511400651466,"Guidelines:
857"
P,0.8456026058631921,"• The answer NA means that the paper does not include experiments.
858"
P,0.8462540716612378,"• If the paper includes experiments, a No answer to this question will not be perceived
859"
P,0.8469055374592834,"well by the reviewers: Making the paper reproducible is important, regardless of
860"
P,0.847557003257329,"whether the code and data are provided or not.
861"
P,0.8482084690553746,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
862"
P,0.8488599348534202,"to make their results reproducible or verifiable.
863"
P,0.8495114006514658,"• Depending on the contribution, reproducibility can be accomplished in various ways.
864"
P,0.8501628664495114,"For example, if the contribution is a novel architecture, describing the architecture fully
865"
P,0.850814332247557,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
866"
P,0.8514657980456026,"be necessary to either make it possible for others to replicate the model with the same
867"
P,0.8521172638436482,"dataset, or provide access to the model. In general. releasing code and data is often
868"
P,0.8527687296416938,"one good way to accomplish this, but reproducibility can also be provided via detailed
869"
P,0.8534201954397395,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
870"
P,0.854071661237785,"of a large language model), releasing of a model checkpoint, or other means that are
871"
P,0.8547231270358306,"appropriate to the research performed.
872"
P,0.8553745928338762,"• While NeurIPS does not require releasing code, the conference does require all submis-
873"
P,0.8560260586319218,"sions to provide some reasonable avenue for reproducibility, which may depend on the
874"
P,0.8566775244299675,"nature of the contribution. For example
875"
P,0.857328990228013,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
876"
P,0.8579804560260587,"to reproduce that algorithm.
877"
P,0.8586319218241042,"(b) If the contribution is primarily a new model architecture, the paper should describe
878"
P,0.8592833876221498,"the architecture clearly and fully.
879"
P,0.8599348534201955,"(c) If the contribution is a new model (e.g., a large language model), then there should
880"
P,0.860586319218241,"either be a way to access this model for reproducing the results or a way to reproduce
881"
P,0.8612377850162867,"the model (e.g., with an open-source dataset or instructions for how to construct
882"
P,0.8618892508143322,"the dataset).
883"
P,0.8625407166123779,"(d) We recognize that reproducibility may be tricky in some cases, in which case
884"
P,0.8631921824104235,"authors are welcome to describe the particular way they provide for reproducibility.
885"
P,0.863843648208469,"In the case of closed-source models, it may be that access to the model is limited in
886"
P,0.8644951140065147,"some way (e.g., to registered users), but it should be possible for other researchers
887"
P,0.8651465798045602,"to have some path to reproducing or verifying the results.
888"
P,0.8657980456026059,"(v) Open access to data and code
889"
P,0.8664495114006515,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
890"
P,0.867100977198697,"tions to faithfully reproduce the main experimental results, as described in supplemental
891"
P,0.8677524429967427,"material?
892"
P,0.8684039087947882,"Answer: [Yes]
893"
P,0.8690553745928339,"Justification: Full code is available in supplementary material.
894"
P,0.8697068403908795,"Guidelines:
895"
P,0.8703583061889251,"• The answer NA means that paper does not include experiments requiring code.
896"
P,0.8710097719869707,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
897"
P,0.8716612377850163,"public/guides/CodeSubmissionPolicy) for more details.
898"
P,0.8723127035830619,"• While we encourage the release of code and data, we understand that this might not be
899"
P,0.8729641693811075,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
900"
P,0.8736156351791531,"including code, unless this is central to the contribution (e.g., for a new open-source
901"
P,0.8742671009771987,"benchmark).
902"
P,0.8749185667752443,"• The instructions should contain the exact command and environment needed to run to
903"
P,0.8755700325732899,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
904"
P,0.8762214983713354,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
905"
P,0.8768729641693811,"• The authors should provide instructions on data access and preparation, including how
906"
P,0.8775244299674267,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
907"
P,0.8781758957654723,"• The authors should provide scripts to reproduce all experimental results for the new
908"
P,0.8788273615635179,"proposed method and baselines. If only a subset of experiments are reproducible, they
909"
P,0.8794788273615635,"should state which ones are omitted from the script and why.
910"
P,0.8801302931596091,"• At submission time, to preserve anonymity, the authors should release anonymized
911"
P,0.8807817589576548,"versions (if applicable).
912"
P,0.8814332247557003,"• Providing as much information as possible in supplemental material (appended to the
913"
P,0.8820846905537459,"paper) is recommended, but including URLs to data and code is permitted.
914"
P,0.8827361563517915,"(vi) Experimental Setting/Details
915"
P,0.8833876221498371,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
916"
P,0.8840390879478828,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
917"
P,0.8846905537459283,"results?
918"
P,0.885342019543974,"Answer: [Yes]
919"
P,0.8859934853420195,"Justification: The experimental setting is detailed in the appendix.
920"
P,0.8866449511400651,"Guidelines:
921"
P,0.8872964169381108,"• The answer NA means that the paper does not include experiments.
922"
P,0.8879478827361563,"• The experimental setting should be presented in the core of the paper to a level of detail
923"
P,0.888599348534202,"that is necessary to appreciate the results and make sense of them.
924"
P,0.8892508143322475,"• The full details can be provided either with the code, in appendix, or as supplemental
925"
P,0.8899022801302932,"material.
926"
P,0.8905537459283388,"(vii) Experiment Statistical Significance
927"
P,0.8912052117263843,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
928"
P,0.89185667752443,"information about the statistical significance of the experiments?
929"
P,0.8925081433224755,"Answer: [Yes]
930"
P,0.8931596091205212,"Justification: We provide error bars for the plots. For Atari, we report the standard deviation.
931"
P,0.8938110749185668,"Guidelines:
932"
P,0.8944625407166124,"• The answer NA means that the paper does not include experiments.
933"
P,0.895114006514658,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
934"
P,0.8957654723127035,"dence intervals, or statistical significance tests, at least for the experiments that support
935"
P,0.8964169381107492,"the main claims of the paper.
936"
P,0.8970684039087948,"• The factors of variability that the error bars are capturing should be clearly stated (for
937"
P,0.8977198697068404,"example, train/test split, initialization, random drawing of some parameter, or overall
938"
P,0.898371335504886,"run with given experimental conditions).
939"
P,0.8990228013029316,"• The method for calculating the error bars should be explained (closed form formula,
940"
P,0.8996742671009772,"call to a library function, bootstrap, etc.)
941"
P,0.9003257328990228,"• The assumptions made should be given (e.g., Normally distributed errors).
942"
P,0.9009771986970684,"• It should be clear whether the error bar is the standard deviation or the standard error
943"
P,0.901628664495114,"of the mean.
944"
P,0.9022801302931596,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
945"
P,0.9029315960912052,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
946"
P,0.9035830618892509,"of Normality of errors is not verified.
947"
P,0.9042345276872964,"• For asymmetric distributions, the authors should be careful not to show in tables or
948"
P,0.904885993485342,"figures symmetric error bars that would yield results that are out of range (e.g. negative
949"
P,0.9055374592833876,"error rates).
950"
P,0.9061889250814332,"• If error bars are reported in tables or plots, The authors should explain in the text how
951"
P,0.9068403908794789,"they were calculated and reference the corresponding figures or tables in the text.
952"
P,0.9074918566775244,"(viii) Experiments Compute Resources
953"
P,0.9081433224755701,"Question: For each experiment, does the paper provide sufficient information on the com-
954"
P,0.9087947882736156,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
955"
P,0.9094462540716612,"the experiments?
956"
P,0.9100977198697069,"Answer: [Yes]
957"
P,0.9107491856677524,"Justification: We provide the details about the computer resources used (CPU and number
958"
P,0.9114006514657981,"of cores).
959"
P,0.9120521172638436,"Guidelines:
960"
P,0.9127035830618893,"• The answer NA means that the paper does not include experiments.
961"
P,0.9133550488599349,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
962"
P,0.9140065146579804,"or cloud provider, including relevant memory and storage.
963"
P,0.9146579804560261,"• The paper should provide the amount of compute required for each of the individual
964"
P,0.9153094462540716,"experimental runs as well as estimate the total compute.
965"
P,0.9159609120521173,"• The paper should disclose whether the full research project required more compute
966"
P,0.9166123778501629,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
967"
P,0.9172638436482085,"didn’t make it into the paper).
968"
P,0.9179153094462541,"(ix) Code Of Ethics
969"
P,0.9185667752442996,"Question: Does the research conducted in the paper conform, in every respect, with the
970"
P,0.9192182410423453,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
971"
P,0.9198697068403909,"Answer: [Yes]
972"
P,0.9205211726384365,"Justification: The research conducted in the paper conforms the Code of Ethics.
973"
P,0.9211726384364821,"Guidelines:
974"
P,0.9218241042345277,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
975"
P,0.9224755700325733,"• If the authors answer No, they should explain the special circumstances that require a
976"
P,0.923127035830619,"deviation from the Code of Ethics.
977"
P,0.9237785016286645,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
978"
P,0.9244299674267101,"eration due to laws or regulations in their jurisdiction).
979"
P,0.9250814332247557,"(x) Broader Impacts
980"
P,0.9257328990228013,"Question: Does the paper discuss both potential positive societal impacts and negative
981"
P,0.9263843648208469,"societal impacts of the work performed?
982"
P,0.9270358306188925,"Answer: [NA]
983"
P,0.9276872964169381,"Justification: The research conducted in the paper has no societal impact.
984"
P,0.9283387622149837,"Guidelines:
985"
P,0.9289902280130293,"• The answer NA means that there is no societal impact of the work performed.
986"
P,0.9296416938110749,"• If the authors answer NA or No, they should explain why their work has no societal
987"
P,0.9302931596091205,"impact or why the paper does not address societal impact.
988"
P,0.9309446254071662,"• Examples of negative societal impacts include potential malicious or unintended uses
989"
P,0.9315960912052117,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
990"
P,0.9322475570032573,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
991"
P,0.9328990228013029,"groups), privacy considerations, and security considerations.
992"
P,0.9335504885993485,"• The conference expects that many papers will be foundational research and not tied
993"
P,0.9342019543973942,"to particular applications, let alone deployments. However, if there is a direct path to
994"
P,0.9348534201954397,"any negative applications, the authors should point it out. For example, it is legitimate
995"
P,0.9355048859934854,"to point out that an improvement in the quality of generative models could be used to
996"
P,0.9361563517915309,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
997"
P,0.9368078175895765,"that a generic algorithm for optimizing neural networks could enable people to train
998"
P,0.9374592833876222,"models that generate Deepfakes faster.
999"
P,0.9381107491856677,"• The authors should consider possible harms that could arise when the technology is
1000"
P,0.9387622149837134,"being used as intended and functioning correctly, harms that could arise when the
1001"
P,0.9394136807817589,"technology is being used as intended but gives incorrect results, and harms following
1002"
P,0.9400651465798046,"from (intentional or unintentional) misuse of the technology.
1003"
P,0.9407166123778502,"• If there are negative societal impacts, the authors could also discuss possible mitigation
1004"
P,0.9413680781758957,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
1005"
P,0.9420195439739414,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
1006"
P,0.9426710097719869,"feedback over time, improving the efficiency and accessibility of ML).
1007"
P,0.9433224755700326,"(xi) Safeguards
1008"
P,0.9439739413680782,"Question: Does the paper describe safeguards that have been put in place for responsible
1009"
P,0.9446254071661238,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
1010"
P,0.9452768729641694,"image generators, or scraped datasets)?
1011"
P,0.9459283387622149,"Answer: [NA]
1012"
P,0.9465798045602606,"Justification: The research proposed in this paper poses no such risks.
1013"
P,0.9472312703583062,"Guidelines:
1014"
P,0.9478827361563518,"• The answer NA means that the paper poses no such risks.
1015"
P,0.9485342019543974,"• Released models that have a high risk for misuse or dual-use should be released with
1016"
P,0.949185667752443,"necessary safeguards to allow for controlled use of the model, for example by requiring
1017"
P,0.9498371335504886,"that users adhere to usage guidelines or restrictions to access the model or implementing
1018"
P,0.9504885993485342,"safety filters.
1019"
P,0.9511400651465798,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
1020"
P,0.9517915309446254,"should describe how they avoided releasing unsafe images.
1021"
P,0.952442996742671,"• We recognize that providing effective safeguards is challenging, and many papers do
1022"
P,0.9530944625407166,"not require this, but we encourage authors to take this into account and make a best
1023"
P,0.9537459283387623,"faith effort.
1024"
P,0.9543973941368078,"(xii) Licenses for existing assets
1025"
P,0.9550488599348534,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
1026"
P,0.955700325732899,"the paper, properly credited and are the license and terms of use explicitly mentioned and
1027"
P,0.9563517915309446,"properly respected?
1028"
P,0.9570032573289903,"Answer: [NA]
1029"
P,0.9576547231270358,"Justification: We do not use existing assets.
1030"
P,0.9583061889250815,"Guidelines:
1031"
P,0.958957654723127,"• The answer NA means that the paper does not use existing assets.
1032"
P,0.9596091205211726,"• The authors should cite the original paper that produced the code package or dataset.
1033"
P,0.9602605863192183,"• The authors should state which version of the asset is used and, if possible, include a
1034"
P,0.9609120521172638,"URL.
1035"
P,0.9615635179153095,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
1036"
P,0.962214983713355,"• For scraped data from a particular source (e.g., website), the copyright and terms of
1037"
P,0.9628664495114007,"service of that source should be provided.
1038"
P,0.9635179153094463,"• If assets are released, the license, copyright information, and terms of use in the
1039"
P,0.9641693811074918,"package should be provided. For popular datasets, paperswithcode.com/datasets
1040"
P,0.9648208469055375,"has curated licenses for some datasets. Their licensing guide can help determine the
1041"
P,0.965472312703583,"license of a dataset.
1042"
P,0.9661237785016287,"• For existing datasets that are re-packaged, both the original license and the license of
1043"
P,0.9667752442996743,"the derived asset (if it has changed) should be provided.
1044"
P,0.9674267100977199,"• If this information is not available online, the authors are encouraged to reach out to
1045"
P,0.9680781758957655,"the asset’s creators.
1046"
P,0.968729641693811,"(xiii) New Assets
1047"
P,0.9693811074918567,"Question: Are new assets introduced in the paper well documented and is the documentation
1048"
P,0.9700325732899023,"provided alongside the assets?
1049"
P,0.9706840390879479,"Answer: [Yes]
1050"
P,0.9713355048859935,"Justification: The provided code is well documented.
1051"
P,0.971986970684039,"Guidelines:
1052"
P,0.9726384364820847,"• The answer NA means that the paper does not release new assets.
1053"
P,0.9732899022801303,"• Researchers should communicate the details of the dataset/code/model as part of their
1054"
P,0.9739413680781759,"submissions via structured templates. This includes details about training, license,
1055"
P,0.9745928338762215,"limitations, etc.
1056"
P,0.9752442996742671,"• The paper should discuss whether and how consent was obtained from people whose
1057"
P,0.9758957654723127,"asset is used.
1058"
P,0.9765472312703583,"• At submission time, remember to anonymize your assets (if applicable). You can either
1059"
P,0.9771986970684039,"create an anonymized URL or include an anonymized zip file.
1060"
P,0.9778501628664495,"(xiv) Crowdsourcing and Research with Human Subjects
1061"
P,0.9785016286644951,"Question: For crowdsourcing experiments and research with human subjects, does the paper
1062"
P,0.9791530944625407,"include the full text of instructions given to participants and screenshots, if applicable, as
1063"
P,0.9798045602605863,"well as details about compensation (if any)?
1064"
P,0.9804560260586319,"Answer: [NA]
1065"
P,0.9811074918566776,"Justification: The paper does not involve crowdsourcing.
1066"
P,0.9817589576547231,"Guidelines:
1067"
P,0.9824104234527687,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1068"
P,0.9830618892508143,"human subjects.
1069"
P,0.9837133550488599,"• Including this information in the supplemental material is fine, but if the main contribu-
1070"
P,0.9843648208469056,"tion of the paper involves human subjects, then as much detail as possible should be
1071"
P,0.9850162866449511,"included in the main paper.
1072"
P,0.9856677524429968,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
1073"
P,0.9863192182410423,"or other labor should be paid at least the minimum wage in the country of the data
1074"
P,0.9869706840390879,"collector.
1075"
P,0.9876221498371336,"(xv) Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
1076"
P,0.9882736156351791,"Subjects
1077"
P,0.9889250814332248,"Question: Does the paper describe potential risks incurred by study participants, whether
1078"
P,0.9895765472312703,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
1079"
P,0.990228013029316,"approvals (or an equivalent approval/review based on the requirements of your country or
1080"
P,0.9908794788273616,"institution) were obtained?
1081"
P,0.9915309446254071,"Answer: [NA]
1082"
P,0.9921824104234528,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
1083"
P,0.9928338762214983,"Guidelines:
1084"
P,0.993485342019544,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1085"
P,0.9941368078175896,"human subjects.
1086"
P,0.9947882736156352,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
1087"
P,0.9954397394136808,"may be required for any human subjects research. If you obtained IRB approval, you
1088"
P,0.9960912052117263,"should clearly state this in the paper.
1089"
P,0.996742671009772,"• We recognize that the procedures for this may vary significantly between institutions
1090"
P,0.9973941368078176,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
1091"
P,0.9980456026058632,"guidelines for their institution.
1092"
P,0.9986970684039088,"• For initial submissions, do not include any information that would break anonymity (if
1093"
P,0.9993485342019544,"applicable), such as the institution conducting the review.
1094"
