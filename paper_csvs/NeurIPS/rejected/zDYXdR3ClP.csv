Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0011025358324145535,"Existing unified methods typically treat multi-degradation image restoration as a
1"
ABSTRACT,0.002205071664829107,"multi-task learning problem. Despite performing effectively compared to single
2"
ABSTRACT,0.0033076074972436605,"degradation restoration methods, they overlook the utilization of commonalities
3"
ABSTRACT,0.004410143329658214,"and specificities within multi-task restoration, thereby impeding the model‚Äôs per-
4"
ABSTRACT,0.005512679162072767,"formance. Inspired by the success of deep generative models and fine-tuning tech-
5"
ABSTRACT,0.006615214994487321,"niques, we proposed a universal image restoration framework based on multiple
6"
ABSTRACT,0.007717750826901874,"low-rank adapters (LoRA) from multi-domain transfer learning. Our framework
7"
ABSTRACT,0.008820286659316428,"leverages the pre-trained generative model as the shared component for multi-
8"
ABSTRACT,0.009922822491730982,"degradation restoration and transfers it to specific degradation image restoration
9"
ABSTRACT,0.011025358324145534,"tasks using low-rank adaptation. Additionally, we introduce a LoRA composing
10"
ABSTRACT,0.012127894156560088,"strategy based on the degradation similarity, which adaptively combines trained
11"
ABSTRACT,0.013230429988974642,"LoRAs and enables our model to be applicable for mixed degradation restoration.
12"
ABSTRACT,0.014332965821389196,"Extensive experiments on multiple and mixed degradations demonstrate that the
13"
ABSTRACT,0.015435501653803748,"proposed universal image restoration method not only achieves higher fidelity and
14"
ABSTRACT,0.016538037486218304,"perceptual image quality but also has better generalization ability than other unified
15"
ABSTRACT,0.017640573318632856,"image restoration models.
16"
INTRODUCTION,0.018743109151047408,"1
Introduction
17"
INTRODUCTION,0.019845644983461964,"In the wild, a range of distortions commonly appear in captured images, including noise[56], blur[14,
18"
INTRODUCTION,0.020948180815876516,"47, 6], low light[58, 22, 8], and various weather degradations[15, 51, 54, 45]. As a fundamental task
19"
INTRODUCTION,0.022050716648291068,"in low-level vision, image restoration aims to eliminate these distortions and recover sharp details and
20"
INTRODUCTION,0.023153252480705624,"original scene information from corrupted images. With the assistance of deep learning, an abundance
21"
INTRODUCTION,0.024255788313120176,"of restoration approaches [56, 3, 54, 2, 16, 14, 53] have made significant progress in eliminating
22"
INTRODUCTION,0.025358324145534728,"single degradation from images. However, these approaches typically require additional training from
23"
INTRODUCTION,0.026460859977949284,"scratch on specific image pairs in multi-degraded scenarios, which leads to inconvenience in usage
24"
INTRODUCTION,0.027563395810363836,"and limited generalization ability.
25"
INTRODUCTION,0.02866593164277839,"For simplicity and practicality, some existing works [15, 31, 55]consider training a unified model
26"
INTRODUCTION,0.029768467475192944,"(also called all-in-one model) to handle multiple degradations as multi-task learning. These studies
27"
INTRODUCTION,0.030871003307607496,"primarily explore how to discern degradation from the image and integrate it into the restoration
28"
INTRODUCTION,0.03197353914002205,"network. Nevertheless, these methods share all parameters across different degradations, resulting in
29"
INTRODUCTION,0.03307607497243661,"gradient conflicts [40, 52] that hinder further improvement of unified models‚Äô performance.
30"
INTRODUCTION,0.034178610804851156,"Digging deeper, the underlying issue lies in that the similarities among different image restoration
31"
INTRODUCTION,0.03528114663726571,"tasks and the inherent specificity of each degradation are not well considered and utilized in the
32"
INTRODUCTION,0.03638368246968027,"training. This limitation drives us to seek solutions for multi-degradation restoration by leveraging
33"
INTRODUCTION,0.037486218302094816,"both commonalities and specificities.
34 noisy hazy"
INTRODUCTION,0.03858875413450937,blurry
INTRODUCTION,0.03969128996692393,"low light
‚Ä¶"
INTRODUCTION,0.040793825799338476,Pre-trained
INTRODUCTION,0.04189636163175303,generative model
INTRODUCTION,0.04299889746416759,clean image
INTRODUCTION,0.044101433296582136,domain
INTRODUCTION,0.04520396912899669,specific
INTRODUCTION,0.04630650496141125,specific
INTRODUCTION,0.047409040793825796,specific
INTRODUCTION,0.04851157662624035,specific ùëùùë• ‚Ä¶
INTRODUCTION,0.04961411245865491,shared
INTRODUCTION,0.050716648291069456,transfer
INTRODUCTION,0.05181918412348401,"noise, text,"
INTRODUCTION,0.05292171995589857,"image
Image Generation"
INTRODUCTION,0.05402425578831312,Image Restoration
INTRODUCTION,0.05512679162072767,"Figure 1: Motivation of our work. A pre-trained generative model serves as the shared component
and minimal parameters are added to model the specificity of each degradation restoration task."
INTRODUCTION,0.05622932745314223,"Inspired by the successes of deep generative models[37, 36, 35] and fine-tuning techniques[11, 10, 4],
35"
INTRODUCTION,0.05733186328555678,"we propose addressing the aforementioned issue from the perspective of multi-domain transfer
36"
INTRODUCTION,0.05843439911797133,"learning, as presented in Figure 1. The pre-trained generative model exhibits powerful capabilities,
37"
INTRODUCTION,0.05953693495038589,"implying rich prior knowledge of clear image distribution p(x), which is exactly what is needed
38"
INTRODUCTION,0.06063947078280044,"for image restoration. Since image prior p(x) is degradation-agnostic and applicable to all types
39"
INTRODUCTION,0.06174200661521499,"of degraded images, the pre-trained generative model is an excellent candidate for serving as the
40"
INTRODUCTION,0.06284454244762955,"shared component for multiple degradation restoration. To model the transition from the clean image
41"
INTRODUCTION,0.0639470782800441,"domain to different degraded image domains, minimal specific parameters are required to fine-tune
42"
INTRODUCTION,0.06504961411245866,"the pre-trained model for each degradation restoration task. This approach not only isolates conflicts
43"
INTRODUCTION,0.06615214994487321,"between each degradation task but also ensures efficiency and performance during training.
44"
INTRODUCTION,0.06725468577728776,"Following the idea of multi-domain transfer learning, we proposed a universal image restoration
45"
INTRODUCTION,0.06835722160970231,"framework based on multiple low-rank adaptations, named UIR-LoRA. In our framework, the pre-
46"
INTRODUCTION,0.06945975744211687,"trained SD-turbo [39] serves as the shared fundamental model for multiple degradation restoration
47"
INTRODUCTION,0.07056229327453142,"tasks due to its powerful one-step generation capability and extensive image priors. Subsequently,
48"
INTRODUCTION,0.07166482910694598,"we incorporate the low-rank adaptation (LoRA) technique [11] to fine-tune the base model for each
49"
INTRODUCTION,0.07276736493936053,"specific image restoration task. This involves augmenting low-dimensional parameter matrices on
50"
INTRODUCTION,0.07386990077177508,"selected layers within the base model, ensuring efficient fine-tuning while maintaining independence
51"
INTRODUCTION,0.07497243660418963,"between LoRAs for each specific degradation. Additionally, we propose a LoRA composition strategy
52"
INTRODUCTION,0.07607497243660419,"based on degradation similarity. We calculate the similarity between degradation features extracted
53"
INTRODUCTION,0.07717750826901874,"from degraded images and existing degradation types, utilizing it as weights for combining different
54"
INTRODUCTION,0.0782800441014333,"LoRA experts. This strategy enables our method to be applicable for restoring mixed degradation
55"
INTRODUCTION,0.07938257993384785,"images. Moreover, we conducted extensive experiments and compared our approach with several
56"
INTRODUCTION,0.0804851157662624,"existing unified image restoration methods. The experimental results demonstrate that our method
57"
INTRODUCTION,0.08158765159867695,"achieves superior performance in the restoration of various degradations and mixed degradations. Not
58"
INTRODUCTION,0.08269018743109151,"only does our approach outperform existing methods in terms of distortion and perceptual metrics,
59"
INTRODUCTION,0.08379272326350606,"but it also exhibits significant improvements in visual quality.
60"
INTRODUCTION,0.08489525909592062,"Our contributions can be summarized as follows:
61"
INTRODUCTION,0.08599779492833518,"‚Ä¢ From the perspective of multi-domain transfer learning, we propose a novel universal image
62"
INTRODUCTION,0.08710033076074973,"restoration framework based on multiple low-rank adaptations. It leverages the pre-trained
63"
INTRODUCTION,0.08820286659316427,"generative model as the shared component for multi-degradation restoration and employs
64"
INTRODUCTION,0.08930540242557883,"distinct LoRAs for multiple degradations to efficiently transfer to specific degradation
65"
INTRODUCTION,0.09040793825799338,"restoration tasks.
66"
INTRODUCTION,0.09151047409040794,"‚Ä¢ We introduce a LoRAs composition strategy based on the degradation similarity, which
67"
INTRODUCTION,0.0926130099228225,"adaptively combines trained LoRAs and enables our model to be applicable for mixed
68"
INTRODUCTION,0.09371554575523705,"degradation restoration.
69"
INTRODUCTION,0.09481808158765159,"‚Ä¢ Through extensive experiments on multiple and mixed degradations, we demonstrate that the
70"
INTRODUCTION,0.09592061742006615,"proposed universal image restoration method not only achieves higher fidelity and perceptual
71"
INTRODUCTION,0.0970231532524807,"image quality but also has better generalization ability than other unified models.
72"
RELATED WORK,0.09812568908489526,"2
Related Work
73"
IMAGE RESTORATION,0.09922822491730982,"2.1
Image Restoration
74"
IMAGE RESTORATION,0.10033076074972437,"Specific Degradation Restoration. According to degradation type, image restoration tasks are
75"
IMAGE RESTORATION,0.10143329658213891,"categorized into different groups, including denoising, deblurring, inpainting, draining .etc. Most
76"
IMAGE RESTORATION,0.10253583241455347,"existing image restoration methods [2, 53, 16, 56, 5, 14] mainly address the issue with a single
77"
IMAGE RESTORATION,0.10363836824696802,"degradation. Traditional approaches [27, 28, 7] have proposed image priors. While these priors can
78"
IMAGE RESTORATION,0.10474090407938258,"be applied to different degraded images, their capability is limited. Due to the remarkable capability
79"
IMAGE RESTORATION,0.10584343991179714,"of the deep neural network (DNN), numerous DNN-based methods [2, 53, 16] have been proposed
80"
IMAGE RESTORATION,0.10694597574421169,"to tackle image restoration tasks. While DNN-based methods have made significant progress, they
81"
IMAGE RESTORATION,0.10804851157662625,"struggle with multiple degradations and mixed degradations, since they typically require retraining
82"
IMAGE RESTORATION,0.10915104740904079,"from scratch on data with the same degradation.
83"
IMAGE RESTORATION,0.11025358324145534,"Universal degradation restoration. Increasing attention is currently focused on developing a
84"
IMAGE RESTORATION,0.1113561190738699,"unified model to process multiple degradations. For example, AirNet[15] explores the degradation
85"
IMAGE RESTORATION,0.11245865490628446,"representation in latent space for separating them in the restoration network. PromptIR[31] utilizes a
86"
IMAGE RESTORATION,0.11356119073869901,"prompt block to extract the degradation-related features to improve the performance. Daclip-IR[20]
87"
IMAGE RESTORATION,0.11466372657111357,"introduces the clip-based encoder to distinguish the type of degradation and extract the semantics
88"
IMAGE RESTORATION,0.11576626240352811,"information from distorted images and embed them into a diffusion model to generate high-quality
89"
IMAGE RESTORATION,0.11686879823594266,"images. Despite the advancements, these unified models still have limitations. They also require
90"
IMAGE RESTORATION,0.11797133406835722,"retraining all parameters when unseen degradations arrive and have limited performance due to the
91"
IMAGE RESTORATION,0.11907386990077178,"gradient conflict.
92"
LOW-RANK ADAPTATION,0.12017640573318633,"2.2
Low-Rank Adaptation
93"
LOW-RANK ADAPTATION,0.12127894156560089,"LoRA [11] is proposed to fine-tune large models by freezing the pre-trained weights and introducing
94"
LOW-RANK ADAPTATION,0.12238147739801543,"trainable low-rank matrices. This fine-tuning method leverages the property of ""intrinsic dimension""
95"
LOW-RANK ADAPTATION,0.12348401323042998,"within neural networks, lowering the rank of additional matrices and making the re-training process
96"
LOW-RANK ADAPTATION,0.12458654906284454,"efficient. Concretely, given a weight matrices W ‚ààRn√óm in pre-trained model Œ∏p, two trainable
97"
LOW-RANK ADAPTATION,0.1256890848952591,"matrices B ‚ààRn√ór and A ‚ààRr√óm are inserted into the layer to represent the LoRA ‚àÜW = BA,
98"
LOW-RANK ADAPTATION,0.12679162072767364,"where r is the rank and satisfy r ‚â™mim(n, m), the updated weights W ‚Ä≤are calculated by
99"
LOW-RANK ADAPTATION,0.1278941565600882,"W ‚Ä≤ = W + ‚àÜW.
(1)"
LOW-RANK ADAPTATION,0.12899669239250275,"By applying LoRA in pre-trained models, numerous image generation methods [29, 13], show
100"
LOW-RANK ADAPTATION,0.13009922822491732,"superior performance in the field of image style and semantics concept transferring. Additionally,
101"
LOW-RANK ADAPTATION,0.13120176405733186,"fine-tuning methods like ControlNet [57], T2i-adapter [24] are also commonly employed in large-
102"
LOW-RANK ADAPTATION,0.13230429988974643,"scale pre-trained generative models such as Stable Diffusion [37], SDXL [30], and Imagen [38].
103"
MIXTURE OF EXPERTS,0.13340683572216097,"2.3
Mixture of Experts
104"
MIXTURE OF EXPERTS,0.1345093715545755,"Mixture of Experts (MoE) [41, 49, 48] is an effective approach to scale up neural network capacity to
105"
MIXTURE OF EXPERTS,0.13561190738699008,"improve performance. Specifically, MoE integrates multiple feed-forward networks into a transformer
106"
MIXTURE OF EXPERTS,0.13671444321940462,"block, where each feed-forward network is regarded as an expert. A gating function is introduced to
107"
MIXTURE OF EXPERTS,0.1378169790518192,"model the probability distribution across all experts in the MoE layer. The gating function is trainable
108"
MIXTURE OF EXPERTS,0.13891951488423374,"and determines the activation of specific experts within the MoE layer based on top-k values. Broadly
109"
MIXTURE OF EXPERTS,0.14002205071664828,"speaking, our framework aligns with the concept of MoE. However, unlike traditional MoE layers, we
110"
MIXTURE OF EXPERTS,0.14112458654906285,"employ the more efficient LoRA as experts in selected frozen layers and utilize a degradation-aware
111"
MIXTURE OF EXPERTS,0.1422271223814774,"router across all selected layers to uniformly activate experts, reducing learning complexity and
112"
MIXTURE OF EXPERTS,0.14332965821389196,"avoiding conflicts among different image restoration tasks on experts.
113"
METHODOLOGY,0.1444321940463065,"3
Methodology
114"
PROBLEM DEFINITION,0.14553472987872107,"3.1
Problem Definition
115"
PROBLEM DEFINITION,0.1466372657111356,"This paper seeks to develop a novel universal image restoration framework capable of handling
116"
PROBLEM DEFINITION,0.14773980154355015,"diverse forms of image degradation in the wild by fine-tuning the pre-trained generative model.
117"
PROBLEM DEFINITION,0.14884233737596472,"Consider a set of T image restoration tasks D = {Dk}T
k=1, where Dk = {(xi, yi)}nk
i=1 is the training
118"
PROBLEM DEFINITION,0.14994487320837926,"dataset containing nk images pairs of the k-th image degradation task. Within the set of tasks D,
119"
PROBLEM DEFINITION,0.15104740904079383,Pre-trained Weights
PROBLEM DEFINITION,0.15214994487320838,Frozen Layer ‚Ä¶
PROBLEM DEFINITION,0.15325248070562295,"üî•
Trainable LoRAs"
PROBLEM DEFINITION,0.1543550165380375,"hazy
blurry"
PROBLEM DEFINITION,0.15545755237045203,"‚Ä¶
noisy
‚àë ‚Ä¶"
PROBLEM DEFINITION,0.1565600882028666,"Image
encoder ‚àÜùëä! ‚àÜùëä"" ‚àÜùëä#"
PROBLEM DEFINITION,0.15766262403528114,"Degraded Image
Restored Image"
PROBLEM DEFINITION,0.1587651598676957,"weights ‚Ä¶ ùëì! ùë• ùëì"" ùë• ùëì# ùë•"
PROBLEM DEFINITION,0.15986769570011025,"ùëì$ ùë•
& %&# '"
PROBLEM DEFINITION,0.1609702315325248,"ùë†%ùëì% ùë•
ùëì$ ùë•+"
PROBLEM DEFINITION,0.16207276736493936,Degradation
PROBLEM DEFINITION,0.1631753031973539,similarity
PROBLEM DEFINITION,0.16427783902976847,ùë†! = ùëëùêµ√ó ùëë ùêµ ‚Ä¶
PROBLEM DEFINITION,0.16538037486218302,"ùë•!""
ùë•#$%"
PROBLEM DEFINITION,0.16648291069459759,"Text 
encoder"
PROBLEM DEFINITION,0.16758544652701213,Degradation-Aware
PROBLEM DEFINITION,0.16868798235942667,Router
PROBLEM DEFINITION,0.16979051819184124,"Universal
Image Restorer"
PROBLEM DEFINITION,0.17089305402425578,"hazy, blurry,
‚Ä¶, noisy ‚Ä¶"
PROBLEM DEFINITION,0.17199558985667035,"ùë†= ùëõùëúùëüùëö"" ùë†! ùë°ùëúùëùùêæ"
PROBLEM DEFINITION,0.1730981256890849,"Figure 2: Overview of UIR-LoRA. UIR-LoRA consists of two components: a degradation-aware
router and a universal image restorer. The router calculates degradation similarity in the latent space
of CLIP, while the restorer utilizes the similarity provided by the router to combine LoRAs and frozen
base model and restore images with multiple or mixed degadations."
PROBLEM DEFINITION,0.17420066152149946,"each task Dk only has a specific type of image degradation, with no intersection between any two
120"
PROBLEM DEFINITION,0.175303197353914,"tasks. Given a pre-trained generative model Œ∏p with frozen parameters, our objective is to learn a
121"
PROBLEM DEFINITION,0.17640573318632854,"set of composite {Œ∏k}T
k=1 to construct a unified model fŒ∏ that performs well on multi-degradation
122"
PROBLEM DEFINITION,0.17750826901874311,"restoration and mixed degradation restoration by transferring learning, where Œ∏ = Œ∏p + PT
k=1 skŒ∏k
123"
PROBLEM DEFINITION,0.17861080485115766,"and sk represents the composite weight for Œ∏k. The trainable {Œ∏k}T
k=1 can be optimized through
124"
PROBLEM DEFINITION,0.17971334068357223,"minimizing the overall image reconstruction loss:
125"
PROBLEM DEFINITION,0.18081587651598677,"L = E(x,y)‚ààDl(fŒ∏(x), y).
(2)"
PROBLEM DEFINITION,0.1819184123484013,"We will present how to design and optimize the trainable {Œ∏k}T
k=1 and construct the composite
126"
PROBLEM DEFINITION,0.18302094818081588,"weights s in the next sections.
127"
OVERVIEW OF UNIVERSAL FRAMEWORK,0.18412348401323042,"3.2
Overview of Universal Framework
128"
OVERVIEW OF UNIVERSAL FRAMEWORK,0.185226019845645,"Inspired by transferring learning, we introduce a novel universal image restoration framework based
129"
OVERVIEW OF UNIVERSAL FRAMEWORK,0.18632855567805953,"on multiple low-rank adaptations, named UIR-LoRA. Referring to Figure 2, our framework consists
130"
OVERVIEW OF UNIVERSAL FRAMEWORK,0.1874310915104741,"of two main components, namely degradation-aware router and universal image restorer, respectively.
131"
OVERVIEW OF UNIVERSAL FRAMEWORK,0.18853362734288864,"The degradation-aware router first extracts the degradation feature from input degraded images and
132"
OVERVIEW OF UNIVERSAL FRAMEWORK,0.18963616317530319,"then calculates the similarity probabilities s with existing degradations in the latent space of CLIP
133"
OVERVIEW OF UNIVERSAL FRAMEWORK,0.19073869900771775,"model [35, 20]. For the universal image restorer, it comprises a pre-trained generative model Œ∏p and
134"
OVERVIEW OF UNIVERSAL FRAMEWORK,0.1918412348401323,"T trainable LoRAs {Œ∏k}T
k=1. This design is primarily motivated by two considerations: firstly, the
135"
OVERVIEW OF UNIVERSAL FRAMEWORK,0.19294377067254687,"pre-trained generative model contains extensive image priors that are degradation-agnostic and can
136"
OVERVIEW OF UNIVERSAL FRAMEWORK,0.1940463065049614,"be shared across all types of degraded images. Secondly, each LoRA can independently capture
137"
OVERVIEW OF UNIVERSAL FRAMEWORK,0.19514884233737598,"specific characteristics of each degradation without gradient conflicts. In practice, the pre-trained
138"
OVERVIEW OF UNIVERSAL FRAMEWORK,0.19625137816979052,"SD-turbo [39] is employed as the frozen base model in our framework and each LoRA Œ∏k serves
139"
OVERVIEW OF UNIVERSAL FRAMEWORK,0.19735391400220506,"as an expert responsible for transferring the frozen base model to a specific degradation restoration
140"
OVERVIEW OF UNIVERSAL FRAMEWORK,0.19845644983461963,"task Dk. By adjusting the value of Top-K parameter within the degradation-aware router, different
141"
OVERVIEW OF UNIVERSAL FRAMEWORK,0.19955898566703417,"combinations of LoRAs in the universal image restorer can be activated, enabling the removal of a
142"
OVERVIEW OF UNIVERSAL FRAMEWORK,0.20066152149944874,"specific degradation and mixed degradation in multi-degraded scenarios.
143"
DEGRADATION-AWARE ROUTER,0.20176405733186328,"3.3
Degradation-Aware Router
144"
DEGRADATION-AWARE ROUTER,0.20286659316427783,"The Degradation-Aware Router is designed to provide the restorer with weights for LoRA combination
145"
DEGRADATION-AWARE ROUTER,0.2039691289966924,"based on degradation confidence. Following Daclip-ir [20], we utilized the pre-trained image encoder
146"
DEGRADATION-AWARE ROUTER,0.20507166482910694,"in CLIP [35] to obtain the degradation vector d ‚ààR1√óz from the input degraded image x, where z is
147"
DEGRADATION-AWARE ROUTER,0.2061742006615215,"degradation length in latent space. Differing from Daclip-ir [20], we use the degradation vector and
148"
DEGRADATION-AWARE ROUTER,0.20727673649393605,"existing degradations to calculate the similarity, instead of directly embedding the degradation vector
149"
DEGRADATION-AWARE ROUTER,0.20837927232635062,"into the restoration network in Daclip-ir [20]. The existing degradations refer to the vocabulary bank
150"
DEGRADATION-AWARE ROUTER,0.20948180815876516,"of diverse degradation types that we introduce in the router, such as ""noisy"", ""blurry"" and ""shadowed"".
151"
DEGRADATION-AWARE ROUTER,0.2105843439911797,"This vocabulary bank is highly compact and flexible when adding new degradation types. Similarly,
152"
DEGRADATION-AWARE ROUTER,0.21168687982359427,"by applying the text encoder of CLIP [35], the vocabulary bank can be encoded into the degradation
153"
DEGRADATION-AWARE ROUTER,0.2127894156560088,"bank B ‚ààRz√óT in the latent space. As presented in Figure 2, the original degradation similarity
154"
DEGRADATION-AWARE ROUTER,0.21389195148842338,"so ‚ààR1√óT is calculated by:
155"
DEGRADATION-AWARE ROUTER,0.21499448732083792,"so = dB.
(3)"
DEGRADATION-AWARE ROUTER,0.2160970231532525,"Building upon the original similarity, we adopt a more flexible and controllable Top-K strategy
156"
DEGRADATION-AWARE ROUTER,0.21719955898566704,"to modify so. Specifically, we select the Top-K largest values from the original similarity so, and
157"
DEGRADATION-AWARE ROUTER,0.21830209481808158,"normalize them to reallocate the weights for LoRAs. The reallocation process can be formulated as :
158"
DEGRADATION-AWARE ROUTER,0.21940463065049615,"s =
so ¬∑ MK
P so ¬∑ MK
,
(4)"
DEGRADATION-AWARE ROUTER,0.2205071664829107,"where MK represents a binary mask with the same length as so, where it is 1 when the corresponding
159"
DEGRADATION-AWARE ROUTER,0.22160970231532526,"value in so is among the Top-K, otherwise it is 0. With a smaller value of K, the restorer activates
160"
DEGRADATION-AWARE ROUTER,0.2227122381477398,"fewer LoRAs, reducing its computational load. For instance, with K = 1, only the most similar
161"
DEGRADATION-AWARE ROUTER,0.22381477398015434,"LoRA is activated and it yields effective results when s is accurate, but performance noticeably
162"
DEGRADATION-AWARE ROUTER,0.2249173098125689,"declines with inaccurate s. Conversely, as K increases, the restorer exhibits higher tolerance to s and
163"
DEGRADATION-AWARE ROUTER,0.22601984564498345,"the combination of LoRAs allows it to handle mixed degradation.
164"
UNIVERSAL IMAGE RESTORER,0.22712238147739802,"3.4
Universal Image Restorer
165"
UNIVERSAL IMAGE RESTORER,0.22822491730981256,"Our universal image restorer consists of a pre-trained generative model Œ∏p and a set of LoRAs
166"
UNIVERSAL IMAGE RESTORER,0.22932745314222713,"{Œ∏k}T
k=1. As illustrated in Figure 2, our universal image restorer takes the degraded image x and
167"
UNIVERSAL IMAGE RESTORER,0.23042998897464168,"similarity s predicted by the degradation-aware router as inputs. It then activates relevant LoRAs
168"
UNIVERSAL IMAGE RESTORER,0.23153252480705622,"based on s to recover the degraded image along with the frozen base model. Since one of our
169"
UNIVERSAL IMAGE RESTORER,0.2326350606394708,"objectives is to ensure that each LoRA serves as an expert in processing a specific degradation, the
170"
UNIVERSAL IMAGE RESTORER,0.23373759647188533,"number of LoRAs in the restorer aligns with the number of degradation types, T. In practice, we
171"
UNIVERSAL IMAGE RESTORER,0.2348401323042999,"select multiple layers from the base model, For a selected layer W of the pre-trained base model, a
172"
UNIVERSAL IMAGE RESTORER,0.23594266813671444,"sequence of trainable matrices {‚àÜWk}T
k=1 are added into this layer, and the parameters of all chosen
173"
UNIVERSAL IMAGE RESTORER,0.237045203969129,"layers L form a complete LoRA Œ∏k = {‚àÜW j
k}j‚ààL. As previously explained, each LoRA is a unique
174"
UNIVERSAL IMAGE RESTORER,0.23814773980154355,"expert responsible for a specific degradation. Drawing inspiration from Mixture of Expert (MoE), we
175"
UNIVERSAL IMAGE RESTORER,0.2392502756339581,"aggregate the outputs of each expert rather than directly merging parameters in [11]. Therefore, given
176"
UNIVERSAL IMAGE RESTORER,0.24035281146637266,"the input feature xin of the current layer and the similarity s, the total output xout of this modified
177"
UNIVERSAL IMAGE RESTORER,0.2414553472987872,"layer can be expressed as
178"
UNIVERSAL IMAGE RESTORER,0.24255788313120177,"xout = fo(xin) + K
X"
UNIVERSAL IMAGE RESTORER,0.24366041896361632,"i=1
sifi(xin),
(5)"
UNIVERSAL IMAGE RESTORER,0.24476295479603086,"where fi(xin) denotes the result of i-th trainable matrice Wi, particularly fo(xin) is output of the
179"
UNIVERSAL IMAGE RESTORER,0.24586549062844543,"frozen base layer. From the equation 5, it can be observed that the introduced LoRAs interact with the
180"
UNIVERSAL IMAGE RESTORER,0.24696802646085997,"frozen base model at intermediate feature layers in our restorer. This interaction forces the restorer
181"
UNIVERSAL IMAGE RESTORER,0.24807056229327454,"to leverage the image priors of the pre-trained generative model and eliminate degradation with the
182"
UNIVERSAL IMAGE RESTORER,0.24917309812568908,"assistance of LoRAs. In contrast to employing stable diffusion [37] directly as a post-processing
183"
UNIVERSAL IMAGE RESTORER,0.25027563395810365,"technique, our restorer yields results closer to the true scene without introducing inaccurate structural
184"
UNIVERSAL IMAGE RESTORER,0.2513781697905182,"details. Since each W is implemented using two low-rank matrices like the formula 1, the total
185"
UNIVERSAL IMAGE RESTORER,0.25248070562293273,"trainable parameters of our framework are much smaller than that of the pre-train generative model.
186"
TRAINING AND INFERENCE PROCEDURE,0.2535832414553473,"3.5
Training and Inference Procedure
187"
TRAINING AND INFERENCE PROCEDURE,0.25468577728776187,"During the training phase, for the efficient training of the universal image restorer, we ensure that
188"
TRAINING AND INFERENCE PROCEDURE,0.2557883131201764,"each batch is sampled from the same degradation type Dk, and activate the corresponding LoRA Œ∏k
189"
TRAINING AND INFERENCE PROCEDURE,0.25689084895259096,"Table 1: Comparison of the restoration results over ten different datasets. The best results are marked
in boldface."
TRAINING AND INFERENCE PROCEDURE,0.2579933847850055,"Model
Distortion
Perceptual
Complexity"
TRAINING AND INFERENCE PROCEDURE,0.25909592061742004,"PSNR‚Üë
SSIM ‚Üë
LPIPS ‚Üì
FID ‚Üì
Param /M
Runtime /s"
TRAINING AND INFERENCE PROCEDURE,0.26019845644983464,"SwinIR [16]
23.37
0.731
0.354
104.37
15.8
0.66
NAFNet [2]
26.34
0.847
0.159
55.68
67.9
0.54
Restormer [53]
26.43
0.850
0.157
54.03
26.1
0.14"
TRAINING AND INFERENCE PROCEDURE,0.2613009922822492,"AirNet [15]
25.62
0.844
0.182
64.86
7.6
1.50
PromptIR [31]
27.14
0.859
0.147
48.26
35.6
1.19
IR-SDE [21]
23.64
0.754
0.167
49.18
36.2
5.07
DiffBIR [17]
21.01
0.618
0.263
91.03
363.2
5.95
Daclip-IR [20]
27.01
0.794
0.127
34.89
295.2
4.09
UIR-LoRA (Ours)
28.08
0.864
0.104
30.58
95.2
0.44"
TRAINING AND INFERENCE PROCEDURE,0.2624035281146637,"for training. Since the dataset D is organized by degradation type without overlap and each LoRA
190"
TRAINING AND INFERENCE PROCEDURE,0.26350606394707826,"is assigned to handle each type of degradation correspondingly, the overall optimization process in
191"
TRAINING AND INFERENCE PROCEDURE,0.26460859977949286,"equation 2 can be decomposed into independent optimization processes for each degradation. This
192"
TRAINING AND INFERENCE PROCEDURE,0.2657111356119074,"design and training process circumvent task conflicts among multiple degradations and makes it
193"
TRAINING AND INFERENCE PROCEDURE,0.26681367144432194,"possible to use suitable loss functions for the specific degradation. Due to the availability of accurate
194"
TRAINING AND INFERENCE PROCEDURE,0.2679162072767365,"s during training and the use of pre-trained encoders from CLIP [35] and Daclip-ir [20] in our router,
195"
TRAINING AND INFERENCE PROCEDURE,0.269018743109151,"the router was not utilized during training.
196"
TRAINING AND INFERENCE PROCEDURE,0.2701212789415656,"In the inference phase, the similarity s is unknown and needs to be estimated from the degraded
197"
TRAINING AND INFERENCE PROCEDURE,0.27122381477398017,"image. The estimated similarity s serves as a reference in our framework and can also be manually
198"
TRAINING AND INFERENCE PROCEDURE,0.2723263506063947,"specified by users. Subsequently, our universal image restorer composite LoRAs and recovers the
199"
TRAINING AND INFERENCE PROCEDURE,0.27342888643880925,"input image with the guidance of s.
200"
EXPERIMENTS,0.2745314222712238,"4
Experiments
201"
EXPERIMENTAL SETTING,0.2756339581036384,"4.1
Experimental Setting
202"
EXPERIMENTAL SETTING,0.27673649393605293,"Datasets. We validate the effectiveness of our framework in multiple and mixed degradation scenarios.
203"
EXPERIMENTAL SETTING,0.27783902976846747,"In the case of multiple degradations, we follow Daclip-IR [20] and construct a dataset using 10
204"
EXPERIMENTAL SETTING,0.278941565600882,"different single degradation datasets. Briefly, the composite dataset comprises a total of 52800 image
205"
EXPERIMENTAL SETTING,0.28004410143329656,"pairs for training and 2490 image pairs for testing. The degradation types included are commonly
206"
EXPERIMENTAL SETTING,0.28114663726571115,"encountered in image restoration, such as blur, noise, shadow, JPEG compression, and weather
207"
EXPERIMENTAL SETTING,0.2822491730981257,"degradations. For mixed degradations, we utilize two degradation datasets, REDS [25] and LOLBlur
208"
EXPERIMENTAL SETTING,0.28335170893054024,"[58]. In REDS, the images are distorted by JPEG compression and blur, and those images in LOLBlur
209"
EXPERIMENTAL SETTING,0.2844542447629548,"have blur and low light. For more details about datasets in our experiments, please refer to Appendix.
210"
EXPERIMENTAL SETTING,0.2855567805953694,"Metrics. The objective of the image restoration task is to output images with enhanced visual quality
211"
EXPERIMENTAL SETTING,0.2866593164277839,"while maintaining high fidelity to the original scene information. This differs from image generation
212"
EXPERIMENTAL SETTING,0.28776185226019846,"tasks, which prioritize visual quality. Therefore, to thoroughly evaluate the effectiveness of our
213"
EXPERIMENTAL SETTING,0.288864388092613,"method, we utilize reference-based image quality assessment techniques from both distortion and
214"
EXPERIMENTAL SETTING,0.28996692392502754,"perceptual perspectives, including PSNR, SSIM, and LPIPS, as well as FID.
215"
EXPERIMENTAL SETTING,0.29106945975744214,"Comparison Methods. In the experiments, we primarily compare with several state-of-the-art
216"
EXPERIMENTAL SETTING,0.2921719955898567,"methods in image restoration, which fall into two categories: regression model and generative model.
217"
EXPERIMENTAL SETTING,0.2932745314222712,"Regression models include NAFNet [2], Restormer [53], as well as AirNet [15] and PromptIR [31]
218"
EXPERIMENTAL SETTING,0.29437706725468576,"proposed for multiple degradation restoration. DiffBIR [17], IR-SDE [21] and Daclip-IR [20] are
219"
EXPERIMENTAL SETTING,0.2954796030871003,"generative models built upon the diffusion model [9].
220"
IMPLEMENTATION DETAILS,0.2965821389195149,"4.2
Implementation Details
221"
IMPLEMENTATION DETAILS,0.29768467475192945,"During the training, we adapt an AdamW optimizer to update the weights of trainable parameters in
222"
IMPLEMENTATION DETAILS,0.298787210584344,"our model. Before training LoRA for specific degradation, we add skip-connections in the VAE of
223"
IMPLEMENTATION DETAILS,0.29988974641675853,"SD-turbo[39] like [29, 44] and train them with multiple degraded images. We set the initialization
224"
IMPLEMENTATION DETAILS,0.30099228224917307,"JPEG
GT
Restormer
PromptIR
Daclip-IR
Ours"
IMPLEMENTATION DETAILS,0.30209481808158767,"Noisy
GT
Restormer
PromptIR
Daclip-IR
Ours"
IMPLEMENTATION DETAILS,0.3031973539140022,"Raindrop
GT
Restormer
PromptIR
Daclip-IR
Ours"
IMPLEMENTATION DETAILS,0.30429988974641675,"Inpainting
GT
Restormer
PromptIR
Daclip-IR
Ours"
IMPLEMENTATION DETAILS,0.3054024255788313,Figure 3: Qualitative comparison on multiple degraded images.
IMPLEMENTATION DETAILS,0.3065049614112459,"learning rate to 2e-4 and decrease it with CosineAnnealingLR . We trained every LoRA for 80K
225"
IMPLEMENTATION DETAILS,0.30760749724366043,"iterations with batch size 8 and we keep the same hyper-parameters when training different LoRAs.
226"
IMPLEMENTATION DETAILS,0.308710033076075,"The default rank of LoRAs in VAE and Unet is 4 and 8, respectively.
227"
MULTIPLE IMAGE RESTORATION,0.3098125689084895,"4.3
Multiple Image Restoration
228"
MULTIPLE IMAGE RESTORATION,0.31091510474090406,"For fair comparisons, all methods are trained and tested on the multiple degradation dataset. The
229"
MULTIPLE IMAGE RESTORATION,0.31201764057331866,"results are presented in Table 1. We can find that our model, UIR-LoRA, considerably surpasses all
230"
MULTIPLE IMAGE RESTORATION,0.3131201764057332,"compared image restoration approaches across four metrics. This indicates that our approach can
231"
MULTIPLE IMAGE RESTORATION,0.31422271223814774,"balance generating clear structures and details while ensuring the restored images closely resemble the
232"
MULTIPLE IMAGE RESTORATION,0.3153252480705623,"original information of the scene. The visual comparison results depicted in Figure 7 also confirm this
233"
MULTIPLE IMAGE RESTORATION,0.3164277839029768,"assertion. Regression models such as NAFNet [2]and Restormer [53], lacking extensive image priors,
234"
MULTIPLE IMAGE RESTORATION,0.3175303197353914,"tend to produce blurred and over-smoothed images, leading to inferior visual outcomes. Conversely,
235"
MULTIPLE IMAGE RESTORATION,0.31863285556780596,"generative models Daclip-IR [20] excessively prioritize perceptual quality, yielding artifacts and
236"
MULTIPLE IMAGE RESTORATION,0.3197353914002205,"noise that diverge from the actual scene information. Our approach integrates the strengths of both
237"
MULTIPLE IMAGE RESTORATION,0.32083792723263505,"categories of methods, enabling strong performance in both distortion and perceptual aspects
238"
MIXED IMAGE RESTORATION,0.3219404630650496,"4.4
Mixed Image Restoration
239"
MIXED IMAGE RESTORATION,0.3230429988974642,"To evaluate the transferability of UIR-LoRA, we conduct some experiments on mixed degradation
240"
MIXED IMAGE RESTORATION,0.3241455347298787,"datasets from REDS[25] and LOLBlur [58]. Each image in these two datasets contains more than one
241"
MIXED IMAGE RESTORATION,0.32524807056229327,"type of degradation, like blur, jpeg compression, noise, and low light. We test the mixed degraded
242"
MIXED IMAGE RESTORATION,0.3263506063947078,"images using models trained on multiple degradations and set K to 2 in the router. As shown in
243"
MIXED IMAGE RESTORATION,0.3274531422271224,"Table 2, our method achieves superior results in both distortion and perceptual quality, particularly
244"
MIXED IMAGE RESTORATION,0.32855567805953695,"on the LOLBlur dataset. We also provide visual comparison results, as illustrated in Figure 4, our
245"
MIXED IMAGE RESTORATION,0.3296582138919515,"approach effectively enhances the low-light image compared to SOTA methods, highlighting its
246"
MIXED IMAGE RESTORATION,0.33076074972436603,"stronger transferability in the wild. More visual results can be found in Appendix.
247"
MIXED IMAGE RESTORATION,0.3318632855567806,"Table 2: Comparison of the restoration results on mixed degradation datasets. The best results are
marked in boldface."
MIXED IMAGE RESTORATION,0.33296582138919517,"Model
REDS
LOLBlur"
MIXED IMAGE RESTORATION,0.3340683572216097,"PSNR‚Üë
SSIM ‚Üë
LPIPS ‚Üì
FID ‚Üì
PSNR‚Üë
SSIM ‚Üë
LPIPS ‚Üì
FID ‚Üì"
MIXED IMAGE RESTORATION,0.33517089305402425,"SwinIR
21.53
0.676
0.449
116.80
10.06
0.320
0.619
124.52
NAFNet
25.06
0.721
0.412
122.12
10.57
0.397
0.477
85.77
Restormer
23.15
0.713
0.413
118.61
12.77
0.479
0.478
87.23
PromptIR
24.98
0.712
0.424
128.11
9.09
0.275
0.560
91.68
DiffBIR
20.70
0.598
0.377
122.76
9.86
0.288
0.611
125.41
Daclip-IR
24.30
0.699
0.337
95.29
14.52
0.599
0.358
68.10
UIR-LoRA
25.11
0.718
0.315
89.79
18.16
0.690
0.318
61.55"
MIXED IMAGE RESTORATION,0.3362734288864388,Degraded image GT
MIXED IMAGE RESTORATION,0.33737596471885334,"Restormer
PromptIR"
MIXED IMAGE RESTORATION,0.33847850055126794,"Daclip-IR
Ours"
MIXED IMAGE RESTORATION,0.3395810363836825,Figure 4: Qualitative comparison on multiple degraded images.
ABLATION STUDY,0.340683572216097,"4.5
Ablation Study
248"
ABLATION STUDY,0.34178610804851156,"Complexity Analysis. We compare model complexity with SOTA models. The comparison results
249"
ABLATION STUDY,0.3428886438809261,"are shown in Table 1, where we report the number of trainable parameters and the runtime for a
250"
ABLATION STUDY,0.3439911797133407,"256√ó256 image on an A100 GPU. The complexity of UIR-LoRA is comparable to regression models
251"
ABLATION STUDY,0.34509371554575524,"like NAFNet [2] and significantly more efficient than generative models like Daclip-IR [20].
252"
ABLATION STUDY,0.3461962513781698,"Effectiveness of Degradation-Aware Router. The degradation-aware router plays a crucial role in
253"
ABLATION STUDY,0.3472987872105843,"determining which LoRAs are activated in the inference. To comprehensively demonstrate the impact
254"
ABLATION STUDY,0.3484013230429989,"of the router, we conduct experiments with different selection strategies. As illustrated in Table 3,
255"
ABLATION STUDY,0.34950385887541346,"we have five strategies: ""random"" indicates activating a LoRA at random, ""average"" denotes using
256"
ABLATION STUDY,0.350606394707828,"average weights to activate all LoRAs, and ""Top-1"", ""Top-2"" and ""All"" correspond to setting K in the
257"
ABLATION STUDY,0.35170893054024255,"router to 1,2, and 10, respectively. From the comparison of these results, we can see that the random
258"
ABLATION STUDY,0.3528114663726571,"and average strategies result in poorer performance while using the strategy based on degradation
259"
ABLATION STUDY,0.3539140022050717,Table 3: Impact of strategies in router
ABLATION STUDY,0.35501653803748623,"Strategy
Multiple Degradation
Mixed Degradation"
ABLATION STUDY,0.35611907386990077,"PSNR‚Üë
SSIM ‚Üë
LPIPS ‚Üì
FID ‚Üì
PSNR‚Üë
SSIM ‚Üë
LPIPS ‚Üì
FID ‚Üì"
ABLATION STUDY,0.3572216097023153,"Random
17.52
0.617
0.388
126.48
10.35
0.323
0.577
104.84
Average
17.62
0.617
0.370
129.06
9.28
0.277
0.549
106.05
Top-1
28.06
0.864
0.105
30.62
18.04
0.683
0.321
61.65
Top-2
28.05
0.864
0.105
30.60
18.16
0.690
0.318
61.55
All
28.05
0.864
0.105
30.61
18.16
0.691
0.318
61.58"
ABLATION STUDY,0.35832414553472985,Figure 5: The impact of LoRA‚Äôs rank on deblurring and denoising tasks.
ABLATION STUDY,0.35942668136714445,"similarity achieves better outcomes. This suggests that the transferability between different types
260"
ABLATION STUDY,0.360529217199559,"of degradation is limited and that specific parameters are needed to address their particularities.
261"
ABLATION STUDY,0.36163175303197354,"Furthermore, the selection of the K value also affects the model‚Äôs performance. When an image has
262"
ABLATION STUDY,0.3627342888643881,"only one type of degradation, a smaller K value can result in comparable performance with lower
263"
ABLATION STUDY,0.3638368246968026,"inference costs. However, for mixed degradations, a larger K value is required to handle the more
264"
ABLATION STUDY,0.3649393605292172,"complex situation.
265"
ABLATION STUDY,0.36604189636163176,"Impact of LoRA‚Äôs Rank. Within our framework, LoRA is utilized to facilitate the transfer from
266"
ABLATION STUDY,0.3671444321940463,"the pre-trained generative model to the image restoration task. In order to investigate the impact of
267"
ABLATION STUDY,0.36824696802646084,"LoRA‚Äôs rank on the performance of image restoration, we conduct experiments using deblurring
268"
ABLATION STUDY,0.36934950385887544,"and denoising tasks chosen from ten distinct degradation categories. We set the initial rank to 2 and
269"
ABLATION STUDY,0.37045203969129,"incrementally increase the value by a factor of 2. The performance changes are depicted in Figure 5.
270"
ABLATION STUDY,0.3715545755237045,"It is evident that as the rank grows, the restoration results improve in distortion and perceptual quality,
271"
ABLATION STUDY,0.37265711135611906,"and at the same time, the number of trainable parameters also increases. Once the rank value exceeds
272"
ABLATION STUDY,0.3737596471885336,"4, the performance improvement becomes progressively marginal. Therefore, we set the default rank
273"
ABLATION STUDY,0.3748621830209482,"to 4 in our restorer to balance between performance and complexity.
274"
ABLATION STUDY,0.37596471885336274,Table 4: The accuracy of predicted degradation type
ABLATION STUDY,0.3770672546857773,"PSNR‚Üë
SSIM ‚Üë
LPIPS ‚Üì
FID ‚Üì
Accuracy ‚Üë"
ABLATION STUDY,0.37816979051819183,"Original
26.66
0.839
0.159
18.72
91.6
Modified
26.87
0.842
0.155
18.42
99.2"
ABLATION STUDY,0.37927232635060637,"Impact of Predicted Degradation.
275"
ABLATION STUDY,0.38037486218302097,"The resizing operation on input images in CLIP models [20, 35] may lead to inaccurate predictions
276"
ABLATION STUDY,0.3814773980154355,"of degradation types, especially for blurry images. To reduce its negative impact on performance, we
277"
ABLATION STUDY,0.38257993384785005,"introduce a simple way that uses the degradation vector of the image crop without resizing to correct
278"
ABLATION STUDY,0.3836824696802646,"the potential error in the resized image. Table 4 is the comparison conducted on blurry images from
279"
ABLATION STUDY,0.38478500551267913,"GoPro dataset. It can be observed that our model with modified operation has higher accuracy and
280"
ABLATION STUDY,0.38588754134509373,"better performance for deblurring.
281"
CONCLUSION,0.3869900771775083,"5
Conclusion
282"
CONCLUSION,0.3880926130099228,"In this paper, we propose a universal image restoration framework based on multiple low-rank
283"
CONCLUSION,0.38919514884233736,"adaptation, named UIR-LoRA, from the perspective of multi-domain transfer learning. UIR-LoRA
284"
CONCLUSION,0.39029768467475195,"utilizes a pre-trained generative model as the frozen base model and transfers its abundant image
285"
CONCLUSION,0.3914002205071665,"priors to different image restoration tasks using the LoRA technique. Moreover, we introduce a
286"
CONCLUSION,0.39250275633958104,"LoRAs‚Äô composition strategy based on the degradation similarity that allows UIR-LoRA applicable
287"
CONCLUSION,0.3936052921719956,"for multiple and mixed degradations in the wild. Extensive experiments on universal image restoration
288"
CONCLUSION,0.3947078280044101,"tasks demonstrate the effectiveness and better generalization capability of our proposed UIR-LoRA.
289"
LIMITATION AND DISCUSSION,0.3958103638368247,"6
Limitation and Discussion
290"
LIMITATION AND DISCUSSION,0.39691289966923926,"Although our UIR-LoRA has achieved remarkable performance in image restoration tasks under both
291"
LIMITATION AND DISCUSSION,0.3980154355016538,"multiple and mixed degradations, it still has limitations and problems for further exploration. For
292"
LIMITATION AND DISCUSSION,0.39911797133406834,"instance, adding new trainable parameters into the network for unseen degradations is unavoidable in
293"
LIMITATION AND DISCUSSION,0.4002205071664829,"image restoration tasks, although UIR-LoRA is already more efficient and flexible compared to other
294"
LIMITATION AND DISCUSSION,0.4013230429988975,"approaches.
295"
REFERENCES,0.402425578831312,"References
296"
REFERENCES,0.40352811466372657,"[1] Eirikur Agustsson and Radu Timofte. Ntire 2017 challenge on single image super-resolution:
297"
REFERENCES,0.4046306504961411,"Dataset and study. In Proceedings of the IEEE conference on computer vision and pattern
298"
REFERENCES,0.40573318632855565,"recognition workshops, pages 126‚Äì135, 2017.
299"
REFERENCES,0.40683572216097025,"[2] Liangyu Chen, Xiaojie Chu, Xiangyu Zhang, and Jian Sun. Simple baselines for image
300"
REFERENCES,0.4079382579933848,"restoration. In European conference on computer vision, pages 17‚Äì33. Springer, 2022.
301"
REFERENCES,0.40904079382579933,"[3] Liangyu Chen, Xin Lu, Jie Zhang, Xiaojie Chu, and Chengpeng Chen. Hinet: Half instance
302"
REFERENCES,0.4101433296582139,"normalization network for image restoration. In Proceedings of the IEEE/CVF Conference on
303"
REFERENCES,0.41124586549062847,"Computer Vision and Pattern Recognition, pages 182‚Äì192, 2021.
304"
REFERENCES,0.412348401323043,"[4] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping Luo.
305"
REFERENCES,0.41345093715545755,"Adaptformer: Adapting vision transformers for scalable visual recognition. Advances in Neural
306"
REFERENCES,0.4145534729878721,"Information Processing Systems, 35:16664‚Äì16678, 2022.
307"
REFERENCES,0.41565600882028664,"[5] Xueyang Fu, Jiabin Huang, Xinghao Ding, Yinghao Liao, and John Paisley. Clearing the
308"
REFERENCES,0.41675854465270123,"skies: A deep network architecture for single-image rain removal. IEEE Transactions on Image
309"
REFERENCES,0.4178610804851158,"Processing, 26(6):2944‚Äì2956, 2017.
310"
REFERENCES,0.4189636163175303,"[6] Dong Gong, Jie Yang, Lingqiao Liu, Yanning Zhang, Ian Reid, Chunhua Shen, Anton Van
311"
REFERENCES,0.42006615214994486,"Den Hengel, and Qinfeng Shi. From motion blur to motion flow: A deep learning solution
312"
REFERENCES,0.4211686879823594,"for removing heterogeneous motion blur. In Proceedings of the IEEE conference on computer
313"
REFERENCES,0.422271223814774,"vision and pattern recognition, pages 2319‚Äì2328, 2017.
314"
REFERENCES,0.42337375964718854,"[7] Shuhang Gu, Lei Zhang, Wangmeng Zuo, and Xiangchu Feng. Weighted nuclear norm min-
315"
REFERENCES,0.4244762954796031,"imization with application to image denoising. In Proceedings of the IEEE conference on
316"
REFERENCES,0.4255788313120176,"computer vision and pattern recognition, pages 2862‚Äì2869, 2014.
317"
REFERENCES,0.42668136714443217,"[8] Chunle Guo, Chongyi Li, Jichang Guo, Chen Change Loy, Junhui Hou, Sam Kwong, and
318"
REFERENCES,0.42778390297684676,"Runmin Cong. Zero-reference deep curve estimation for low-light image enhancement. In
319"
REFERENCES,0.4288864388092613,"Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages
320"
REFERENCES,0.42998897464167585,"1780‚Äì1789, 2020.
321"
REFERENCES,0.4310915104740904,"[9] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances
322"
REFERENCES,0.432194046306505,"in neural information processing systems, 33:6840‚Äì6851, 2020.
323"
REFERENCES,0.43329658213891953,"[10] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,
324"
REFERENCES,0.43439911797133407,"Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning
325"
REFERENCES,0.4355016538037486,"for nlp. In International conference on machine learning, pages 2790‚Äì2799. PMLR, 2019.
326"
REFERENCES,0.43660418963616315,"[11] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
327"
REFERENCES,0.43770672546857775,"Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In
328"
REFERENCES,0.4388092613009923,"International Conference on Learning Representations, 2022.
329"
REFERENCES,0.43991179713340683,"[12] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for
330"
REFERENCES,0.4410143329658214,"improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017.
331"
REFERENCES,0.4421168687982359,"[13] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-
332"
REFERENCES,0.4432194046306505,"concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference
333"
REFERENCES,0.44432194046306506,"on Computer Vision and Pattern Recognition, pages 1931‚Äì1941, 2023.
334"
REFERENCES,0.4454244762954796,"[14] Orest Kupyn, Volodymyr Budzan, Mykola Mykhailych, Dmytro Mishkin, and JiÀár√≠ Matas.
335"
REFERENCES,0.44652701212789414,"Deblurgan: Blind motion deblurring using conditional adversarial networks. In Proceedings of
336"
REFERENCES,0.4476295479603087,"the IEEE conference on computer vision and pattern recognition, pages 8183‚Äì8192, 2018.
337"
REFERENCES,0.4487320837927233,"[15] Boyun Li, Xiao Liu, Peng Hu, Zhongqin Wu, Jiancheng Lv, and Xi Peng. All-in-one image
338"
REFERENCES,0.4498346196251378,"restoration for unknown corruption. In Proceedings of the IEEE/CVF Conference on Computer
339"
REFERENCES,0.45093715545755236,"Vision and Pattern Recognition, pages 17452‚Äì17462, 2022.
340"
REFERENCES,0.4520396912899669,"[16] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. Swinir:
341"
REFERENCES,0.4531422271223815,"Image restoration using swin transformer. In Proceedings of the IEEE/CVF international
342"
REFERENCES,0.45424476295479604,"conference on computer vision, pages 1833‚Äì1844, 2021.
343"
REFERENCES,0.4553472987872106,"[17] Xinqi Lin, Jingwen He, Ziyan Chen, Zhaoyang Lyu, Ben Fei, Bo Dai, Wanli Ouyang, Yu Qiao,
344"
REFERENCES,0.4564498346196251,"and Chao Dong. Diffbir: Towards blind image restoration with generative diffusion prior. arXiv
345"
REFERENCES,0.45755237045203967,"preprint arXiv:2308.15070, 2023.
346"
REFERENCES,0.45865490628445427,"[18] Yun-Fu Liu, Da-Wei Jaw, Shih-Chia Huang, and Jenq-Neng Hwang. Desnownet: Context-aware
347"
REFERENCES,0.4597574421168688,"deep network for snow removal. IEEE Transactions on Image Processing, 27(6):3064‚Äì3073,
348"
REFERENCES,0.46085997794928335,"2018.
349"
REFERENCES,0.4619625137816979,"[19] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc
350"
REFERENCES,0.46306504961411243,"Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings
351"
REFERENCES,0.46416758544652703,"of the IEEE/CVF conference on computer vision and pattern recognition, pages 11461‚Äì11471,
352"
REFERENCES,0.4652701212789416,"2022.
353"
REFERENCES,0.4663726571113561,"[20] Ziwei Luo, Fredrik K Gustafsson, Zheng Zhao, Jens Sj√∂lund, and Thomas B Sch√∂n. Controlling
354"
REFERENCES,0.46747519294377066,"vision-language models for universal image restoration. arXiv preprint arXiv:2310.01018, 2023.
355"
REFERENCES,0.4685777287761852,"[21] Ziwei Luo, Fredrik K Gustafsson, Zheng Zhao, Jens Sj√∂lund, and Thomas B Sch√∂n. Image
356"
REFERENCES,0.4696802646085998,"restoration with mean-reverting stochastic differential equations. International Conference on
357"
REFERENCES,0.47078280044101434,"Machine Learning, 2023.
358"
REFERENCES,0.4718853362734289,"[22] Long Ma, Tengyu Ma, Risheng Liu, Xin Fan, and Zhongxuan Luo. Toward fast, flexible, and
359"
REFERENCES,0.4729878721058434,"robust low-light image enhancement. In Proceedings of the IEEE/CVF conference on computer
360"
REFERENCES,0.474090407938258,"vision and pattern recognition, pages 5637‚Äì5646, 2022.
361"
REFERENCES,0.47519294377067256,"[23] David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik. A database of human seg-
362"
REFERENCES,0.4762954796030871,"mented natural images and its application to evaluating segmentation algorithms and measuring
363"
REFERENCES,0.47739801543550164,"ecological statistics. In Proceedings of the IEEE/CVF International Conference on Computer
364"
REFERENCES,0.4785005512679162,"Vision, pages 416‚Äì423, 2001.
365"
REFERENCES,0.4796030871003308,"[24] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan.
366"
REFERENCES,0.4807056229327453,"T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion
367"
REFERENCES,0.48180815876515987,"models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages
368"
REFERENCES,0.4829106945975744,"4296‚Äì4304, 2024.
369"
REFERENCES,0.48401323042998895,"[25] Seungjun Nah, Sungyong Baik, Seokil Hong, Gyeongsik Moon, Sanghyun Son, Radu Timofte,
370"
REFERENCES,0.48511576626240355,"and Kyoung Mu Lee. Ntire 2019 challenge on video deblurring and super-resolution: Dataset
371"
REFERENCES,0.4862183020948181,"and study.
In Proceedings of the IEEE/CVF conference on computer vision and pattern
372"
REFERENCES,0.48732083792723263,"recognition workshops, pages 0‚Äì0, 2019.
373"
REFERENCES,0.4884233737596472,"[26] Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep multi-scale convolutional neural
374"
REFERENCES,0.4895259095920617,"network for dynamic scene deblurring. In Proceedings of the IEEE Conference on Computer
375"
REFERENCES,0.4906284454244763,"Vision and Pattern Recognition, pages 3883‚Äì3891, 2017.
376"
REFERENCES,0.49173098125689085,"[27] Jinshan Pan, Zhe Hu, Zhixun Su, and Ming-Hsuan Yang. Deblurring text images via l0-
377"
REFERENCES,0.4928335170893054,"regularized intensity and gradient prior. In Proceedings of the IEEE Conference on Computer
378"
REFERENCES,0.49393605292171994,"Vision and Pattern Recognition, pages 2901‚Äì2908, 2014.
379"
REFERENCES,0.49503858875413453,"[28] Jinshan Pan, Deqing Sun, Hanspeter Pfister, and Ming-Hsuan Yang. Blind image deblurring
380"
REFERENCES,0.4961411245865491,"using dark channel prior. In Proceedings of the IEEE conference on computer vision and pattern
381"
REFERENCES,0.4972436604189636,"recognition, pages 1628‚Äì1636, 2016.
382"
REFERENCES,0.49834619625137816,"[29] Gaurav Parmar, Taesung Park, Srinivasa Narasimhan, and Jun-Yan Zhu. One-step image
383"
REFERENCES,0.4994487320837927,"translation with text-to-image models. arXiv preprint arXiv:2403.12036, 2024.
384"
REFERENCES,0.5005512679162073,"[30] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M√ºller, Joe
385"
REFERENCES,0.5016538037486218,"Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image
386"
REFERENCES,0.5027563395810364,"synthesis. arXiv preprint arXiv:2307.01952, 2023.
387"
REFERENCES,0.503858875413451,"[31] Vaishnav Potlapalli, Syed Waqas Zamir, Salman Khan, and Fahad Khan. Promptir: Prompting
388"
REFERENCES,0.5049614112458655,"for all-in-one image restoration. In Thirty-seventh Conference on Neural Information Processing
389"
REFERENCES,0.5060639470782801,"Systems, 2023.
390"
REFERENCES,0.5071664829106945,"[32] Rui Qian, Robby T Tan, Wenhan Yang, Jiajun Su, and Jiaying Liu. Attentive generative
391"
REFERENCES,0.5082690187431091,"adversarial network for raindrop removal from a single image. In Proceedings of the IEEE
392"
REFERENCES,0.5093715545755237,"conference on computer vision and pattern recognition, pages 2482‚Äì2491, 2018.
393"
REFERENCES,0.5104740904079382,"[33] Xu Qin, Zhilin Wang, Yuanchao Bai, Xiaodong Xie, and Huizhu Jia. Ffa-net: Feature fusion
394"
REFERENCES,0.5115766262403528,"attention network for single image dehazing. In Proceedings of the AAAI conference on artificial
395"
REFERENCES,0.5126791620727673,"intelligence, pages 11908‚Äì11915, 2020.
396"
REFERENCES,0.5137816979051819,"[34] Liangqiong Qu, Jiandong Tian, Shengfeng He, Yandong Tang, and Rynson WH Lau. Deshad-
397"
REFERENCES,0.5148842337375965,"ownet: A multi-context embedding deep network for shadow removal. In Proceedings of the
398"
REFERENCES,0.515986769570011,"IEEE conference on computer vision and pattern recognition, pages 4067‚Äì4075, 2017.
399"
REFERENCES,0.5170893054024256,"[35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
400"
REFERENCES,0.5181918412348401,"Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
401"
REFERENCES,0.5192943770672547,"models from natural language supervision. In International conference on machine learning,
402"
REFERENCES,0.5203969128996693,"pages 8748‚Äì8763. PMLR, 2021.
403"
REFERENCES,0.5214994487320838,"[36] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark
404"
REFERENCES,0.5226019845644984,"Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on
405"
REFERENCES,0.523704520396913,"machine learning, pages 8821‚Äì8831. Pmlr, 2021.
406"
REFERENCES,0.5248070562293274,"[37] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer. High-
407"
REFERENCES,0.525909592061742,"resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF
408"
REFERENCES,0.5270121278941565,"conference on computer vision and pattern recognition, pages 10684‚Äì10695, 2022.
409"
REFERENCES,0.5281146637265711,"[38] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton,
410"
REFERENCES,0.5292171995589857,"Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.
411"
REFERENCES,0.5303197353914002,"Photorealistic text-to-image diffusion models with deep language understanding. Advances in
412"
REFERENCES,0.5314222712238148,"neural information processing systems, 35:36479‚Äì36494, 2022.
413"
REFERENCES,0.5325248070562293,"[39] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion
414"
REFERENCES,0.5336273428886439,"distillation. arXiv preprint arXiv:2311.17042, 2023.
415"
REFERENCES,0.5347298787210585,"[40] Ozan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. Advances
416"
REFERENCES,0.535832414553473,"in neural information processing systems, 31, 2018.
417"
REFERENCES,0.5369349503858876,"[41] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,
418"
REFERENCES,0.538037486218302,"and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts
419"
REFERENCES,0.5391400220507166,"layer. arXiv preprint arXiv:1701.06538, 2017.
420"
REFERENCES,0.5402425578831312,"[42] H Sheikh. Live image quality assessment database release 2. http://live.ece.utexas.
421"
REFERENCES,0.5413450937155457,"edu/research/quality, 2005.
422"
REFERENCES,0.5424476295479603,"[43] Radu Timofte, Eirikur Agustsson, Luc Van Gool, Ming-Hsuan Yang, and Lei Zhang. Ntire 2017
423"
REFERENCES,0.5435501653803748,"challenge on single image super-resolution: Methods and results. In Proceedings of the IEEE
424"
REFERENCES,0.5446527012127894,"conference on computer vision and pattern recognition workshops, pages 114‚Äì125, 2017.
425"
REFERENCES,0.545755237045204,"[44] Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin CK Chan, and Chen Change Loy. Ex-
426"
REFERENCES,0.5468577728776185,"ploiting diffusion prior for real-world image super-resolution. arXiv preprint arXiv:2305.07015,
427"
REFERENCES,0.5479603087100331,"2023.
428"
REFERENCES,0.5490628445424476,"[45] Yinglong Wang, Chao Ma, and Jianzhuang Liu. Smartassign: Learning a smart knowledge
429"
REFERENCES,0.5501653803748622,"assignment strategy for deraining and desnowing. In Proceedings of the IEEE/CVF Conference
430"
REFERENCES,0.5512679162072768,"on Computer Vision and Pattern Recognition, pages 3677‚Äì3686, 2023.
431"
REFERENCES,0.5523704520396913,"[46] Chen Wei, Wenjing Wang, Wenhan Yang, and Jiaying Liu. Deep retinex decomposition for
432"
REFERENCES,0.5534729878721059,"low-light enhancement. arXiv preprint arXiv:1808.04560, 2018.
433"
REFERENCES,0.5545755237045203,"[47] Jay Whang, Mauricio Delbracio, Hossein Talebi, Chitwan Saharia, Alexandros G Dimakis,
434"
REFERENCES,0.5556780595369349,"and Peyman Milanfar. Deblurring via stochastic refinement. In Proceedings of the IEEE/CVF
435"
REFERENCES,0.5567805953693495,"Conference on Computer Vision and Pattern Recognition, pages 16293‚Äì16303, 2022.
436"
REFERENCES,0.557883131201764,"[48] Xun Wu, Shaohan Huang, and Furu Wei. Mole: Mixture of lora experts. In The Twelfth
437"
REFERENCES,0.5589856670341786,"International Conference on Learning Representations, 2023.
438"
REFERENCES,0.5600882028665931,"[49] Yuan Xie, Shaohan Huang, Tianyu Chen, and Furu Wei. Moec: Mixture of expert clusters. In
439"
REFERENCES,0.5611907386990077,"Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 13807‚Äì13815,
440"
REFERENCES,0.5622932745314223,"2023.
441"
REFERENCES,0.5633958103638368,"[50] Wenhan Yang, Robby T Tan, Jiashi Feng, Jiaying Liu, Zongming Guo, and Shuicheng Yan. Deep
442"
REFERENCES,0.5644983461962514,"joint rain detection and removal from a single image. In Proceedings of the IEEE conference on
443"
REFERENCES,0.565600882028666,"computer vision and pattern recognition, pages 1357‚Äì1366, 2017.
444"
REFERENCES,0.5667034178610805,"[51] Wenhan Yang, Robby T Tan, Shiqi Wang, Yuming Fang, and Jiaying Liu. Single image deraining:
445"
REFERENCES,0.5678059536934951,"From model-based to data-driven and beyond. IEEE Transactions on pattern analysis and
446"
REFERENCES,0.5689084895259096,"machine intelligence, 43(11):4059‚Äì4077, 2020.
447"
REFERENCES,0.5700110253583242,"[52] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn.
448"
REFERENCES,0.5711135611907387,"Gradient surgery for multi-task learning. Advances in Neural Information Processing Systems,
449"
REFERENCES,0.5722160970231532,"33:5824‚Äì5836, 2020.
450"
REFERENCES,0.5733186328555678,"[53] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and
451"
REFERENCES,0.5744211686879823,"Ming-Hsuan Yang. Restormer: Efficient transformer for high-resolution image restoration. In
452"
REFERENCES,0.5755237045203969,"Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages
453"
REFERENCES,0.5766262403528115,"5728‚Äì5739, 2022.
454"
REFERENCES,0.577728776185226,"[54] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming-
455"
REFERENCES,0.5788313120176406,"Hsuan Yang, and Ling Shao. Multi-stage progressive image restoration. In Proceedings of the
456"
REFERENCES,0.5799338478500551,"IEEE/CVF conference on computer vision and pattern recognition, pages 14821‚Äì14831, 2021.
457"
REFERENCES,0.5810363836824697,"[55] Cheng Zhang, Yu Zhu, Qingsen Yan, Jinqiu Sun, and Yanning Zhang. All-in-one multi-
458"
REFERENCES,0.5821389195148843,"degradation image restoration network via hierarchical degradation representation. In Proceed-
459"
REFERENCES,0.5832414553472988,"ings of the 31st ACM International Conference on Multimedia, pages 2285‚Äì2293, 2023.
460"
REFERENCES,0.5843439911797134,"[56] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian
461"
REFERENCES,0.5854465270121278,"denoiser: Residual learning of deep cnn for image denoising. IEEE transactions on image
462"
REFERENCES,0.5865490628445424,"processing, 26(7):3142‚Äì3155, 2017.
463"
REFERENCES,0.587651598676957,"[57] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image
464"
REFERENCES,0.5887541345093715,"diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer
465"
REFERENCES,0.5898566703417861,"Vision, pages 3836‚Äì3847, 2023.
466"
REFERENCES,0.5909592061742006,"[58] Shangchen Zhou, Chongyi Li, and Chen Change Loy. Lednet: Joint low-light enhancement and
467"
REFERENCES,0.5920617420066152,"deblurring in the dark. In European conference on computer vision, pages 573‚Äì589. Springer,
468"
REFERENCES,0.5931642778390298,"2022.
469"
REFERENCES,0.5942668136714443,"A
Appendix
470"
REFERENCES,0.5953693495038589,"A.1
More Details about Datasets
471"
REFERENCES,0.5964718853362734,"For multiple degradations, we follow Daclip-IR [20] to construct the dataset, which includes a total
472"
REFERENCES,0.597574421168688,"of ten distinct degradation types: blurry, hazy, JPEG-compression, low-light, noisy, raindrop, rainy,
473"
REFERENCES,0.5986769570011026,"shadowed, snowy, and inpainting. The data sources and data splits for each degradation type are
474"
REFERENCES,0.5997794928335171,"illustrated in Table 5.
475"
REFERENCES,0.6008820286659317,Table 5: Details of the datasets with ten different image degradation types
REFERENCES,0.6019845644983461,"Dataset
Train
Test"
REFERENCES,0.6030871003307607,"Sources
Num
Sources
Num"
REFERENCES,0.6041896361631753,"Blurry
GoPro[26]
2 103
GoPro
1 111
Hazy
RESIDE-6k[33]
6 000
RESIDE-6k
1 000
JPEG
DIV2K[1] and Flickr2K[43]
3 550
LIVE1[42]
29
Low-light
LOL[46]
485
LOL
15
Noisy
DIV2K and Flickr2K
3 550
CBSD68[23]
68
Raindrop
RainDrop[32]
861
RainDrop
58
Rainy
Rain100H[50]
1 800
Rain100H
100
Shadowed
SRD[34]
2 680
SRD
408
Snowy
Snow100K-L[18]
1 872
Snow100K-L
601
Inpainting
CelebaHQ[12]
29 900
CelebaHQ and RePaint[19]
100"
REFERENCES,0.6052921719955898,"For mixed degradations, we utilize images from REDS[25] and LOLBlur[58]to evaluate the trans-
476"
REFERENCES,0.6063947078280044,"ferability of models. We sample 60 images from REDS and 200 images from LOLBlur dataset for
477"
REFERENCES,0.607497243660419,"testing. The degraded images from REDS dataset feature a variety of realistic scenes and objects,
478"
REFERENCES,0.6085997794928335,"which suffer from both motion blurs and compression. And the images from LOLBlur dataset cover
479"
REFERENCES,0.6097023153252481,"a range of real-world dynamic dark scenarios with mixed degradation of low light and blurs.
480"
REFERENCES,0.6108048511576626,"A.2
More Visual Results
481 Input GT"
REFERENCES,0.6119073869900772,"Restormer
PromptIR"
REFERENCES,0.6130099228224918,"Daclip-IR
Ours"
REFERENCES,0.6141124586549063,Figure 6: Qualitative comparison on mixed degraded images from LOLBlur dataset.
REFERENCES,0.6152149944873209,"A.3
Details about Metrics on Multiple Dagradation
482 Input GT"
REFERENCES,0.6163175303197354,"Restormer
PromptIR"
REFERENCES,0.61742006615215,"Daclip-IR
Ours Input GT"
REFERENCES,0.6185226019845645,"Restormer
PromptIR"
REFERENCES,0.619625137816979,"Daclip-IR
Ours"
REFERENCES,0.6207276736493936,Figure 7: Qualitative comparison on mixed degraded images from REDS dataset.
REFERENCES,0.6218302094818081,Table 6: Comparison of the restoration results over ten different datasets on PSNR
REFERENCES,0.6229327453142227,"Blurry
Hazy
JPEG
Low-light
Noisy
Raindrop
Rainy
Shadowed
Snowy
Inpainting
Average"
REFERENCES,0.6240352811466373,"SwinIR
24.49
23.49
24.44
19.59
25.13
24.64
22.07
23.97
21.86
24.05
23.37
NAFNet
26.12
24.05
26.81
22.16
27.16
30.67
27.32
24.16
25.94
29.03
26.34
Restormer
26.34
23.75
26.90
22.17
27.25
30.85
27.91
23.33
25.98
29.88
26.43
AirNet
26.25
23.56
26.98
14.24
27.51
30.68
28.45
23.48
24.87
30.15
25.62
PromptIR
26.50
25.19
26.95
23.14
27.56
31.35
29.24
24.06
27.23
30.22
27.14
IR-SDE
24.13
17.44
24.21
16.07
24.82
28.49
26.64
22.18
24.70
27.56
23.64
DiffBIR
22.79
20.52
22.39
16.96
21.60
23.22
21.04
22.27
20.63
18.77
21.01
Daclip-IR
27.03
29.53
23.70
22.09
24.36
30.81
29.41
27.27
26.83
28.94
27.01
Ours
26.66
30.28
27.15
22.45
27.74
30.51
28.26
28.63
28.09
30.88
28.06"
REFERENCES,0.6251378169790518,Table 7: Comparison of the restoration results over ten different datasets on SSIM
REFERENCES,0.6262403528114664,"Blurry
Hazy
JPEG
Low-light
Noisy
Raindrop
Rainy
Shadowed
Snowy
Inpainting
Average"
REFERENCES,0.6273428886438809,"SwinIR
0.758
0.848
0.734
0.735
0.690
0.758
0.623
0.757
0.665
0.743
0.731
NAFNet
0.804
0.926
0.780
0.809
0.768
0.924
0.848
0.839
0.869
0.901
0.847
Restormer
0.811
0.915
0.781
0.815
0.762
0.928
0.862
0.836
0.877
0.912
0.850
AirNet
0.805
0.916
0.783
0.781
0.769
0.926
0.867
0.832
0.846
0.911
0.844
PromptIR
0.815
0.933
0.784
0.829
0.774
0.931
0.876
0.842
0.887
0.918
0.859
IR-SDE
0.730
0.832
0.615
0.719
0.640
0.822
0.808
0.667
0.828
0.876
0.754
DiffBIR
0.695
0.761
0.607
0.665
0.395
0.682
0.573
0.568
0.566
0.678
0.618
Daclip-IR
0.810
0.931
0.532
0.796
0.579
0.882
0.854
0.811
0.854
0.894
0.794
Ours
0.839
0.962
0.782
0.826
0.789
0.908
0.857
0.862
0.893
0.916
0.864"
REFERENCES,0.6284454244762955,Table 8: Comparison of the restoration results over ten different datasets on LPIPS
REFERENCES,0.6295479603087101,"Blurry
Hazy
JPEG
Low-light
Noisy
Raindrop
Rainy
Shadowed
Snowy
Inpainting
Average"
REFERENCES,0.6306504961411246,"SwinIR
0.347
0.180
0.392
0.362
0.439
0.353
0.481
0.335
0.388
0.265
0.354
NAFNet
0.284
0.043
0.303
0.158
0.216
0.082
0.180
0.138
0.096
0.085
0.159
Restormer
0.282
0.054
0.300
0.156
0.215
0.083
0.170
0.145
0.095
0.072
0.157
AirNet
0.279
0.063
0.302
0.321
0.264
0.095
0.163
0.145
0.112
0.071
0.182
PromptIR
0.267
0.051
0.269
0.140
0.230
0.078
0.147
0.143
0.082
0.068
0.147
IR-SDE
0.198
0.168
0.246
0.185
0.232
0.113
0.142
0.223
0.107
0.065
0.167
DiffBIR
0.269
0.158
0.244
0.273
0.442
0.187
0.309
0.261
0.236
0.246
0.263
Daclip-IR
0.140
0.037
0.317
0.114
0.272
0.068
0.085
0.118
0.072
0.047
0.127
Ours
0.159
0.021
0.204
0.126
0.153
0.048
0.112
0.103
0.070
0.056
0.105"
REFERENCES,0.6317530319735392,Table 9: Comparison of the restoration results over ten different datasets on FID
REFERENCES,0.6328555678059536,"Blurry
Hazy
JPEG
Low-light
Noisy
Raindrop
Rainy
Shadowed
Snowy
Inpainting
Average"
REFERENCES,0.6339581036383682,"SwinIR
53.84
35.43
83.33
156.55
126.87
111.64
186.60
70.22
79.51
139.71
104.37
NAFNet
42.99
15.73
71.88
73.94
82.08
56.43
86.35
47.32
35.76
44.32
55.68
Restormer
39.08
15.34
72.68
78.22
87.14
50.97
78.16
48.33
33.45
36.96
54.03
AirNet
41.23
21.91
78.56
154.2
93.89
52.71
72.07
64.13
64.13
32.93
64.86
PromptIR
36.5
10.85
73.02
67.15
84.51
44.48
61.88
43.24
28.29
32.69
48.26
IR-SDE
29.79
23.16
61.85
66.42
79.38
50.22
63.07
50.71
34.63
32.61
49.18
DiffBIR
37.84
31.83
66.07
150.96
127.27
81.27
133.60
74.09
53.62
154.02
91.03
Daclip-IR
14.13
5.66
42.05
52.23
64.71
38.91
52.78
25.48
27.26
25.73
34.89
Ours
18.72
5.92
37.23
62.21
44.36
23.77
44.30
23.39
22.77
23.50
30.62"
REFERENCES,0.6350606394707828,Table 10: Impact of rank in LoRAs
REFERENCES,0.6361631753031973,"Rank
Deblurring
Denoising"
REFERENCES,0.6372657111356119,"PSNR‚Üë
SSIM ‚Üë
LPIPS ‚Üì
FID ‚Üì
PSNR‚Üë
SSIM ‚Üë
LPIPS ‚Üì
FID ‚Üì"
REFERENCES,0.6383682469680264,"2
26.35
0.831
0.170
21.35
27.57
0.783
0.163
48.98
4
26.64
0.841
0.157
18.79
27.74
0.789
0.153
44.32
8
26.79
0.845
0.151
18.01
27.81
0.791
0.150
43.29
16
26.80
0.846
0.151
17.90
27.83
0.792
0.147
42.82"
REFERENCES,0.639470782800441,"NeurIPS Paper Checklist
483"
CLAIMS,0.6405733186328556,"1. Claims
484"
CLAIMS,0.6416758544652701,"Question: Do the main claims made in the abstract and introduction accurately reflect the
485"
CLAIMS,0.6427783902976847,"paper‚Äôs contributions and scope?
486"
CLAIMS,0.6438809261300992,"Answer: [Yes]
487"
CLAIMS,0.6449834619625138,"Justification: The main claims made in the abstract and introduction1 accurately reflect the
488"
CLAIMS,0.6460859977949284,"paper‚Äôs contributions and scope.
489"
CLAIMS,0.6471885336273429,"Guidelines:
490"
CLAIMS,0.6482910694597575,"‚Ä¢ The answer NA means that the abstract and introduction do not include the claims
491"
CLAIMS,0.649393605292172,"made in the paper.
492"
CLAIMS,0.6504961411245865,"‚Ä¢ The abstract and/or introduction should clearly state the claims made, including the
493"
CLAIMS,0.6515986769570011,"contributions made in the paper and important assumptions and limitations. A No or
494"
CLAIMS,0.6527012127894156,"NA answer to this question will not be perceived well by the reviewers.
495"
CLAIMS,0.6538037486218302,"‚Ä¢ The claims made should match theoretical and experimental results, and reflect how
496"
CLAIMS,0.6549062844542448,"much the results can be expected to generalize to other settings.
497"
CLAIMS,0.6560088202866593,"‚Ä¢ It is fine to include aspirational goals as motivation as long as it is clear that these goals
498"
CLAIMS,0.6571113561190739,"are not attained by the paper.
499"
LIMITATIONS,0.6582138919514884,"2. Limitations
500"
LIMITATIONS,0.659316427783903,"Question: Does the paper discuss the limitations of the work performed by the authors?
501"
LIMITATIONS,0.6604189636163176,"Answer: [Yes]
502"
LIMITATIONS,0.6615214994487321,"Justification: The paper does discuss the limitations of the work in Sections 6.
503"
LIMITATIONS,0.6626240352811467,"Guidelines:
504"
LIMITATIONS,0.6637265711135611,"‚Ä¢ The answer NA means that the paper has no limitation while the answer No means that
505"
LIMITATIONS,0.6648291069459757,"the paper has limitations, but those are not discussed in the paper.
506"
LIMITATIONS,0.6659316427783903,"‚Ä¢ The authors are encouraged to create a separate ""Limitations"" section in their paper.
507"
LIMITATIONS,0.6670341786108048,"‚Ä¢ The paper should point out any strong assumptions and how robust the results are to
508"
LIMITATIONS,0.6681367144432194,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
509"
LIMITATIONS,0.6692392502756339,"model well-specification, asymptotic approximations only holding locally). The authors
510"
LIMITATIONS,0.6703417861080485,"should reflect on how these assumptions might be violated in practice and what the
511"
LIMITATIONS,0.6714443219404631,"implications would be.
512"
LIMITATIONS,0.6725468577728776,"‚Ä¢ The authors should reflect on the scope of the claims made, e.g., if the approach was
513"
LIMITATIONS,0.6736493936052922,"only tested on a few datasets or with a few runs. In general, empirical results often
514"
LIMITATIONS,0.6747519294377067,"depend on implicit assumptions, which should be articulated.
515"
LIMITATIONS,0.6758544652701213,"‚Ä¢ The authors should reflect on the factors that influence the performance of the approach.
516"
LIMITATIONS,0.6769570011025359,"For example, a facial recognition algorithm may perform poorly when image resolution
517"
LIMITATIONS,0.6780595369349504,"is low or images are taken in low lighting. Or a speech-to-text system might not be
518"
LIMITATIONS,0.679162072767365,"used reliably to provide closed captions for online lectures because it fails to handle
519"
LIMITATIONS,0.6802646085997794,"technical jargon.
520"
LIMITATIONS,0.681367144432194,"‚Ä¢ The authors should discuss the computational efficiency of the proposed algorithms
521"
LIMITATIONS,0.6824696802646086,"and how they scale with dataset size.
522"
LIMITATIONS,0.6835722160970231,"‚Ä¢ If applicable, the authors should discuss possible limitations of their approach to
523"
LIMITATIONS,0.6846747519294377,"address problems of privacy and fairness.
524"
LIMITATIONS,0.6857772877618522,"‚Ä¢ While the authors might fear that complete honesty about limitations might be used by
525"
LIMITATIONS,0.6868798235942668,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
526"
LIMITATIONS,0.6879823594266814,"limitations that aren‚Äôt acknowledged in the paper. The authors should use their best
527"
LIMITATIONS,0.6890848952590959,"judgment and recognize that individual actions in favor of transparency play an impor-
528"
LIMITATIONS,0.6901874310915105,"tant role in developing norms that preserve the integrity of the community. Reviewers
529"
LIMITATIONS,0.6912899669239251,"will be specifically instructed to not penalize honesty concerning limitations.
530"
THEORY ASSUMPTIONS AND PROOFS,0.6923925027563396,"3. Theory Assumptions and Proofs
531"
THEORY ASSUMPTIONS AND PROOFS,0.6934950385887542,"Question: For each theoretical result, does the paper provide the full set of assumptions and
532"
THEORY ASSUMPTIONS AND PROOFS,0.6945975744211687,"a complete (and correct) proof?
533"
THEORY ASSUMPTIONS AND PROOFS,0.6957001102535832,"Answer: [Yes]
534"
THEORY ASSUMPTIONS AND PROOFS,0.6968026460859978,"Justification: The paper provides the full set of assumptions and a complete (and correct)
535"
THEORY ASSUMPTIONS AND PROOFS,0.6979051819184123,"proof in Sections 3.
536"
THEORY ASSUMPTIONS AND PROOFS,0.6990077177508269,"Guidelines:
537"
THEORY ASSUMPTIONS AND PROOFS,0.7001102535832414,"‚Ä¢ The answer NA means that the paper does not include theoretical results.
538"
THEORY ASSUMPTIONS AND PROOFS,0.701212789415656,"‚Ä¢ All the theorems, formulas, and proofs in the paper should be numbered and cross-
539"
THEORY ASSUMPTIONS AND PROOFS,0.7023153252480706,"referenced.
540"
THEORY ASSUMPTIONS AND PROOFS,0.7034178610804851,"‚Ä¢ All assumptions should be clearly stated or referenced in the statement of any theorems.
541"
THEORY ASSUMPTIONS AND PROOFS,0.7045203969128997,"‚Ä¢ The proofs can either appear in the main paper or the supplemental material, but if
542"
THEORY ASSUMPTIONS AND PROOFS,0.7056229327453142,"they appear in the supplemental material, the authors are encouraged to provide a short
543"
THEORY ASSUMPTIONS AND PROOFS,0.7067254685777288,"proof sketch to provide intuition.
544"
THEORY ASSUMPTIONS AND PROOFS,0.7078280044101434,"‚Ä¢ Inversely, any informal proof provided in the core of the paper should be complemented
545"
THEORY ASSUMPTIONS AND PROOFS,0.7089305402425579,"by formal proofs provided in appendix or supplemental material.
546"
THEORY ASSUMPTIONS AND PROOFS,0.7100330760749725,"‚Ä¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
547"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7111356119073869,"4. Experimental Result Reproducibility
548"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7122381477398015,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
549"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7133406835722161,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
550"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7144432194046306,"of the paper (regardless of whether the code and data are provided or not)?
551"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7155457552370452,"Answer: [Yes]
552"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7166482910694597,"Justification: The paper provides a comprehensive description of the experimental setting in
553"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7177508269018743,"Sections 4.1 and implementation details in Sections 4.2, which are crucial for reproducing
554"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7188533627342889,"the main results.
555"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7199558985667034,"Guidelines:
556"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.721058434399118,"‚Ä¢ The answer NA means that the paper does not include experiments.
557"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7221609702315325,"‚Ä¢ If the paper includes experiments, a No answer to this question will not be perceived
558"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7232635060639471,"well by the reviewers: Making the paper reproducible is important, regardless of
559"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7243660418963617,"whether the code and data are provided or not.
560"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7254685777287762,"‚Ä¢ If the contribution is a dataset and/or model, the authors should describe the steps taken
561"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7265711135611908,"to make their results reproducible or verifiable.
562"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7276736493936052,"‚Ä¢ Depending on the contribution, reproducibility can be accomplished in various ways.
563"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7287761852260198,"For example, if the contribution is a novel architecture, describing the architecture fully
564"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7298787210584344,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
565"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7309812568908489,"be necessary to either make it possible for others to replicate the model with the same
566"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7320837927232635,"dataset, or provide access to the model. In general. releasing code and data is often
567"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7331863285556781,"one good way to accomplish this, but reproducibility can also be provided via detailed
568"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7342888643880926,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
569"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7353914002205072,"of a large language model), releasing of a model checkpoint, or other means that are
570"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7364939360529217,"appropriate to the research performed.
571"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7375964718853363,"‚Ä¢ While NeurIPS does not require releasing code, the conference does require all submis-
572"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7386990077177509,"sions to provide some reasonable avenue for reproducibility, which may depend on the
573"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7398015435501654,"nature of the contribution. For example
574"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.74090407938258,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
575"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7420066152149944,"to reproduce that algorithm.
576"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.743109151047409,"(b) If the contribution is primarily a new model architecture, the paper should describe
577"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7442116868798236,"the architecture clearly and fully.
578"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7453142227122381,"(c) If the contribution is a new model (e.g., a large language model), then there should
579"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7464167585446527,"either be a way to access this model for reproducing the results or a way to reproduce
580"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7475192943770672,"the model (e.g., with an open-source dataset or instructions for how to construct
581"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7486218302094818,"the dataset).
582"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7497243660418964,"(d) We recognize that reproducibility may be tricky in some cases, in which case
583"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7508269018743109,"authors are welcome to describe the particular way they provide for reproducibility.
584"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7519294377067255,"In the case of closed-source models, it may be that access to the model is limited in
585"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.75303197353914,"some way (e.g., to registered users), but it should be possible for other researchers
586"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7541345093715546,"to have some path to reproducing or verifying the results.
587"
OPEN ACCESS TO DATA AND CODE,0.7552370452039692,"5. Open access to data and code
588"
OPEN ACCESS TO DATA AND CODE,0.7563395810363837,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
589"
OPEN ACCESS TO DATA AND CODE,0.7574421168687983,"tions to faithfully reproduce the main experimental results, as described in supplemental
590"
OPEN ACCESS TO DATA AND CODE,0.7585446527012127,"material?
591"
OPEN ACCESS TO DATA AND CODE,0.7596471885336273,"Answer: [No]
592"
OPEN ACCESS TO DATA AND CODE,0.7607497243660419,"Justification: Upon acceptance of the paper, we will release the code under an open-source
593"
OPEN ACCESS TO DATA AND CODE,0.7618522601984564,"license, which will allow the community to access and verify the experimental results.
594"
OPEN ACCESS TO DATA AND CODE,0.762954796030871,"Guidelines:
595"
OPEN ACCESS TO DATA AND CODE,0.7640573318632855,"‚Ä¢ The answer NA means that paper does not include experiments requiring code.
596"
OPEN ACCESS TO DATA AND CODE,0.7651598676957001,"‚Ä¢ Please see the NeurIPS code and data submission guidelines (https://nips.cc/
597"
OPEN ACCESS TO DATA AND CODE,0.7662624035281147,"public/guides/CodeSubmissionPolicy) for more details.
598"
OPEN ACCESS TO DATA AND CODE,0.7673649393605292,"‚Ä¢ While we encourage the release of code and data, we understand that this might not be
599"
OPEN ACCESS TO DATA AND CODE,0.7684674751929438,"possible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not
600"
OPEN ACCESS TO DATA AND CODE,0.7695700110253583,"including code, unless this is central to the contribution (e.g., for a new open-source
601"
OPEN ACCESS TO DATA AND CODE,0.7706725468577729,"benchmark).
602"
OPEN ACCESS TO DATA AND CODE,0.7717750826901875,"‚Ä¢ The instructions should contain the exact command and environment needed to run to
603"
OPEN ACCESS TO DATA AND CODE,0.772877618522602,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
604"
OPEN ACCESS TO DATA AND CODE,0.7739801543550165,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
605"
OPEN ACCESS TO DATA AND CODE,0.7750826901874311,"‚Ä¢ The authors should provide instructions on data access and preparation, including how
606"
OPEN ACCESS TO DATA AND CODE,0.7761852260198456,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
607"
OPEN ACCESS TO DATA AND CODE,0.7772877618522602,"‚Ä¢ The authors should provide scripts to reproduce all experimental results for the new
608"
OPEN ACCESS TO DATA AND CODE,0.7783902976846747,"proposed method and baselines. If only a subset of experiments are reproducible, they
609"
OPEN ACCESS TO DATA AND CODE,0.7794928335170893,"should state which ones are omitted from the script and why.
610"
OPEN ACCESS TO DATA AND CODE,0.7805953693495039,"‚Ä¢ At submission time, to preserve anonymity, the authors should release anonymized
611"
OPEN ACCESS TO DATA AND CODE,0.7816979051819184,"versions (if applicable).
612"
OPEN ACCESS TO DATA AND CODE,0.782800441014333,"‚Ä¢ Providing as much information as possible in supplemental material (appended to the
613"
OPEN ACCESS TO DATA AND CODE,0.7839029768467475,"paper) is recommended, but including URLs to data and code is permitted.
614"
OPEN ACCESS TO DATA AND CODE,0.7850055126791621,"6. Experimental Setting/Details
615"
OPEN ACCESS TO DATA AND CODE,0.7861080485115767,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
616"
OPEN ACCESS TO DATA AND CODE,0.7872105843439912,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
617"
OPEN ACCESS TO DATA AND CODE,0.7883131201764058,"results?
618"
OPEN ACCESS TO DATA AND CODE,0.7894156560088202,"Answer: [Yes]
619"
OPEN ACCESS TO DATA AND CODE,0.7905181918412348,"Justification: The paper provides detailed information on all training and test aspects,
620"
OPEN ACCESS TO DATA AND CODE,0.7916207276736494,"including datasets, metrics, comparison methods, hyperparameters, the type of optimizer
621"
OPEN ACCESS TO DATA AND CODE,0.7927232635060639,"used, and other relevant details necessary to understand the results in Sections 4.1 and 4.2.
622"
OPEN ACCESS TO DATA AND CODE,0.7938257993384785,"Guidelines:
623"
OPEN ACCESS TO DATA AND CODE,0.794928335170893,"‚Ä¢ The answer NA means that the paper does not include experiments.
624"
OPEN ACCESS TO DATA AND CODE,0.7960308710033076,"‚Ä¢ The experimental setting should be presented in the core of the paper to a level of detail
625"
OPEN ACCESS TO DATA AND CODE,0.7971334068357222,"that is necessary to appreciate the results and make sense of them.
626"
OPEN ACCESS TO DATA AND CODE,0.7982359426681367,"‚Ä¢ The full details can be provided either with the code, in appendix, or as supplemental
627"
OPEN ACCESS TO DATA AND CODE,0.7993384785005513,"material.
628"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8004410143329658,"7. Experiment Statistical Significance
629"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8015435501653804,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
630"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.802646085997795,"information about the statistical significance of the experiments?
631"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8037486218302095,"Answer: [Yes]
632"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.804851157662624,"Justification: The paper reports error bars appropriately and includes correctly defined infor-
633"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8059536934950385,"mation regarding the statistical significance of the experiments, ensuring the transparency
634"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8070562293274531,"and reliability of the results.
635"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8081587651598677,"Guidelines:
636"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8092613009922822,"‚Ä¢ The answer NA means that the paper does not include experiments.
637"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8103638368246968,"‚Ä¢ The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
638"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8114663726571113,"dence intervals, or statistical significance tests, at least for the experiments that support
639"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8125689084895259,"the main claims of the paper.
640"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8136714443219405,"‚Ä¢ The factors of variability that the error bars are capturing should be clearly stated (for
641"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.814773980154355,"example, train/test split, initialization, random drawing of some parameter, or overall
642"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8158765159867696,"run with given experimental conditions).
643"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8169790518191842,"‚Ä¢ The method for calculating the error bars should be explained (closed form formula,
644"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8180815876515987,"call to a library function, bootstrap, etc.)
645"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8191841234840133,"‚Ä¢ The assumptions made should be given (e.g., Normally distributed errors).
646"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8202866593164277,"‚Ä¢ It should be clear whether the error bar is the standard deviation or the standard error
647"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8213891951488423,"of the mean.
648"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8224917309812569,"‚Ä¢ It is OK to report 1-sigma error bars, but one should state it. The authors should
649"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8235942668136714,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
650"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.824696802646086,"of Normality of errors is not verified.
651"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8257993384785005,"‚Ä¢ For asymmetric distributions, the authors should be careful not to show in tables or
652"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8269018743109151,"figures symmetric error bars that would yield results that are out of range (e.g. negative
653"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8280044101433297,"error rates).
654"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8291069459757442,"‚Ä¢ If error bars are reported in tables or plots, The authors should explain in the text how
655"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8302094818081588,"they were calculated and reference the corresponding figures or tables in the text.
656"
EXPERIMENTS COMPUTE RESOURCES,0.8313120176405733,"8. Experiments Compute Resources
657"
EXPERIMENTS COMPUTE RESOURCES,0.8324145534729879,"Question: For each experiment, does the paper provide sufficient information on the com-
658"
EXPERIMENTS COMPUTE RESOURCES,0.8335170893054025,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
659"
EXPERIMENTS COMPUTE RESOURCES,0.834619625137817,"the experiments?
660"
EXPERIMENTS COMPUTE RESOURCES,0.8357221609702316,"Answer: [Yes]
661"
EXPERIMENTS COMPUTE RESOURCES,0.836824696802646,"Justification: The paper provides detailed information regarding the computer resources
662"
EXPERIMENTS COMPUTE RESOURCES,0.8379272326350606,"used in Sections 4.1, and time of execution in Table 1.
663"
EXPERIMENTS COMPUTE RESOURCES,0.8390297684674752,"Guidelines:
664"
EXPERIMENTS COMPUTE RESOURCES,0.8401323042998897,"‚Ä¢ The answer NA means that the paper does not include experiments.
665"
EXPERIMENTS COMPUTE RESOURCES,0.8412348401323043,"‚Ä¢ The paper should indicate the type of compute workers CPU or GPU, internal cluster,
666"
EXPERIMENTS COMPUTE RESOURCES,0.8423373759647188,"or cloud provider, including relevant memory and storage.
667"
EXPERIMENTS COMPUTE RESOURCES,0.8434399117971334,"‚Ä¢ The paper should provide the amount of compute required for each of the individual
668"
EXPERIMENTS COMPUTE RESOURCES,0.844542447629548,"experimental runs as well as estimate the total compute.
669"
EXPERIMENTS COMPUTE RESOURCES,0.8456449834619625,"‚Ä¢ The paper should disclose whether the full research project required more compute
670"
EXPERIMENTS COMPUTE RESOURCES,0.8467475192943771,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
671"
EXPERIMENTS COMPUTE RESOURCES,0.8478500551267916,"didn‚Äôt make it into the paper).
672"
CODE OF ETHICS,0.8489525909592062,"9. Code Of Ethics
673"
CODE OF ETHICS,0.8500551267916208,"Question: Does the research conducted in the paper conform, in every respect, with the
674"
CODE OF ETHICS,0.8511576626240352,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
675"
CODE OF ETHICS,0.8522601984564498,"Answer: [Yes]
676"
CODE OF ETHICS,0.8533627342888643,"Justification: The research conducted in the paper is in full compliance with the NeurIPS
677"
CODE OF ETHICS,0.8544652701212789,"Code of Ethics.
678"
CODE OF ETHICS,0.8555678059536935,"Guidelines:
679"
CODE OF ETHICS,0.856670341786108,"‚Ä¢ The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
680"
CODE OF ETHICS,0.8577728776185226,"‚Ä¢ If the authors answer No, they should explain the special circumstances that require a
681"
CODE OF ETHICS,0.8588754134509372,"deviation from the Code of Ethics.
682"
CODE OF ETHICS,0.8599779492833517,"‚Ä¢ The authors should make sure to preserve anonymity (e.g., if there is a special consid-
683"
CODE OF ETHICS,0.8610804851157663,"eration due to laws or regulations in their jurisdiction).
684"
BROADER IMPACTS,0.8621830209481808,"10. Broader Impacts
685"
BROADER IMPACTS,0.8632855567805954,"Question: Does the paper discuss both potential positive societal impacts and negative
686"
BROADER IMPACTS,0.86438809261301,"societal impacts of the work performed?
687"
BROADER IMPACTS,0.8654906284454245,"Answer: [Yes]
688"
BROADER IMPACTS,0.8665931642778391,"Justification: The paper provides both the potential benefits and the risks associated with the
689"
BROADER IMPACTS,0.8676957001102535,"research, ensuring a comprehensive assessment of its societal implications.
690"
BROADER IMPACTS,0.8687982359426681,"Guidelines:
691"
BROADER IMPACTS,0.8699007717750827,"‚Ä¢ The answer NA means that there is no societal impact of the work performed.
692"
BROADER IMPACTS,0.8710033076074972,"‚Ä¢ If the authors answer NA or No, they should explain why their work has no societal
693"
BROADER IMPACTS,0.8721058434399118,"impact or why the paper does not address societal impact.
694"
BROADER IMPACTS,0.8732083792723263,"‚Ä¢ Examples of negative societal impacts include potential malicious or unintended uses
695"
BROADER IMPACTS,0.8743109151047409,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
696"
BROADER IMPACTS,0.8754134509371555,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
697"
BROADER IMPACTS,0.87651598676957,"groups), privacy considerations, and security considerations.
698"
BROADER IMPACTS,0.8776185226019846,"‚Ä¢ The conference expects that many papers will be foundational research and not tied
699"
BROADER IMPACTS,0.8787210584343991,"to particular applications, let alone deployments. However, if there is a direct path to
700"
BROADER IMPACTS,0.8798235942668137,"any negative applications, the authors should point it out. For example, it is legitimate
701"
BROADER IMPACTS,0.8809261300992283,"to point out that an improvement in the quality of generative models could be used to
702"
BROADER IMPACTS,0.8820286659316428,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
703"
BROADER IMPACTS,0.8831312017640573,"that a generic algorithm for optimizing neural networks could enable people to train
704"
BROADER IMPACTS,0.8842337375964718,"models that generate Deepfakes faster.
705"
BROADER IMPACTS,0.8853362734288864,"‚Ä¢ The authors should consider possible harms that could arise when the technology is
706"
BROADER IMPACTS,0.886438809261301,"being used as intended and functioning correctly, harms that could arise when the
707"
BROADER IMPACTS,0.8875413450937155,"technology is being used as intended but gives incorrect results, and harms following
708"
BROADER IMPACTS,0.8886438809261301,"from (intentional or unintentional) misuse of the technology.
709"
BROADER IMPACTS,0.8897464167585446,"‚Ä¢ If there are negative societal impacts, the authors could also discuss possible mitigation
710"
BROADER IMPACTS,0.8908489525909592,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
711"
BROADER IMPACTS,0.8919514884233738,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
712"
BROADER IMPACTS,0.8930540242557883,"feedback over time, improving the efficiency and accessibility of ML).
713"
SAFEGUARDS,0.8941565600882029,"11. Safeguards
714"
SAFEGUARDS,0.8952590959206174,"Question: Does the paper describe safeguards that have been put in place for responsible
715"
SAFEGUARDS,0.896361631753032,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
716"
SAFEGUARDS,0.8974641675854466,"image generators, or scraped datasets)?
717"
SAFEGUARDS,0.898566703417861,"Answer: [NA]
718"
SAFEGUARDS,0.8996692392502756,"Justification: The paper does not introduce assets that carry a high risk for misuse, therefore,
719"
SAFEGUARDS,0.9007717750826902,"no specific safeguards for data or model release are required.
720"
SAFEGUARDS,0.9018743109151047,"Guidelines:
721"
SAFEGUARDS,0.9029768467475193,"‚Ä¢ The answer NA means that the paper poses no such risks.
722"
SAFEGUARDS,0.9040793825799338,"‚Ä¢ Released models that have a high risk for misuse or dual-use should be released with
723"
SAFEGUARDS,0.9051819184123484,"necessary safeguards to allow for controlled use of the model, for example by requiring
724"
SAFEGUARDS,0.906284454244763,"that users adhere to usage guidelines or restrictions to access the model or implementing
725"
SAFEGUARDS,0.9073869900771775,"safety filters.
726"
SAFEGUARDS,0.9084895259095921,"‚Ä¢ Datasets that have been scraped from the Internet could pose safety risks. The authors
727"
SAFEGUARDS,0.9095920617420066,"should describe how they avoided releasing unsafe images.
728"
SAFEGUARDS,0.9106945975744212,"‚Ä¢ We recognize that providing effective safeguards is challenging, and many papers do
729"
SAFEGUARDS,0.9117971334068358,"not require this, but we encourage authors to take this into account and make a best
730"
SAFEGUARDS,0.9128996692392503,"faith effort.
731"
LICENSES FOR EXISTING ASSETS,0.9140022050716649,"12. Licenses for existing assets
732"
LICENSES FOR EXISTING ASSETS,0.9151047409040793,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
733"
LICENSES FOR EXISTING ASSETS,0.9162072767364939,"the paper, properly credited and are the license and terms of use explicitly mentioned and
734"
LICENSES FOR EXISTING ASSETS,0.9173098125689085,"properly respected?
735"
LICENSES FOR EXISTING ASSETS,0.918412348401323,"Answer: [Yes]
736"
LICENSES FOR EXISTING ASSETS,0.9195148842337376,"Justification: The paper meticulously cites all external assets in references, including code,
737"
LICENSES FOR EXISTING ASSETS,0.9206174200661521,"dataset, and models, acknowledging the contributions of their creators and respecting the
738"
LICENSES FOR EXISTING ASSETS,0.9217199558985667,"associated licenses and terms of use.
739"
LICENSES FOR EXISTING ASSETS,0.9228224917309813,"Guidelines:
740"
LICENSES FOR EXISTING ASSETS,0.9239250275633958,"‚Ä¢ The answer NA means that the paper does not use existing assets.
741"
LICENSES FOR EXISTING ASSETS,0.9250275633958104,"‚Ä¢ The authors should cite the original paper that produced the code package or dataset.
742"
LICENSES FOR EXISTING ASSETS,0.9261300992282249,"‚Ä¢ The authors should state which version of the asset is used and, if possible, include a
743"
LICENSES FOR EXISTING ASSETS,0.9272326350606395,"URL.
744"
LICENSES FOR EXISTING ASSETS,0.9283351708930541,"‚Ä¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
745"
LICENSES FOR EXISTING ASSETS,0.9294377067254685,"‚Ä¢ For scraped data from a particular source (e.g., website), the copyright and terms of
746"
LICENSES FOR EXISTING ASSETS,0.9305402425578831,"service of that source should be provided.
747"
LICENSES FOR EXISTING ASSETS,0.9316427783902976,"‚Ä¢ If assets are released, the license, copyright information, and terms of use in the
748"
LICENSES FOR EXISTING ASSETS,0.9327453142227122,"package should be provided. For popular datasets, paperswithcode.com/datasets
749"
LICENSES FOR EXISTING ASSETS,0.9338478500551268,"has curated licenses for some datasets. Their licensing guide can help determine the
750"
LICENSES FOR EXISTING ASSETS,0.9349503858875413,"license of a dataset.
751"
LICENSES FOR EXISTING ASSETS,0.9360529217199559,"‚Ä¢ For existing datasets that are re-packaged, both the original license and the license of
752"
LICENSES FOR EXISTING ASSETS,0.9371554575523704,"the derived asset (if it has changed) should be provided.
753"
LICENSES FOR EXISTING ASSETS,0.938257993384785,"‚Ä¢ If this information is not available online, the authors are encouraged to reach out to
754"
LICENSES FOR EXISTING ASSETS,0.9393605292171996,"the asset‚Äôs creators.
755"
NEW ASSETS,0.9404630650496141,"13. New Assets
756"
NEW ASSETS,0.9415656008820287,"Question: Are new assets introduced in the paper well documented and is the documentation
757"
NEW ASSETS,0.9426681367144433,"provided alongside the assets?
758"
NEW ASSETS,0.9437706725468578,"Answer: [Yes]
759"
NEW ASSETS,0.9448732083792724,"Justification: The novel universal image restoration framework introduced in the paper is
760"
NEW ASSETS,0.9459757442116868,"well documented, and the documentation is provided alongside the model in Sections 3,
761"
NEW ASSETS,0.9470782800441014,"offering comprehensive details for replication and application.
762"
NEW ASSETS,0.948180815876516,"Guidelines:
763"
NEW ASSETS,0.9492833517089305,"‚Ä¢ The answer NA means that the paper does not release new assets.
764"
NEW ASSETS,0.9503858875413451,"‚Ä¢ Researchers should communicate the details of the dataset/code/model as part of their
765"
NEW ASSETS,0.9514884233737596,"submissions via structured templates. This includes details about training, license,
766"
NEW ASSETS,0.9525909592061742,"limitations, etc.
767"
NEW ASSETS,0.9536934950385888,"‚Ä¢ The paper should discuss whether and how consent was obtained from people whose
768"
NEW ASSETS,0.9547960308710033,"asset is used.
769"
NEW ASSETS,0.9558985667034179,"‚Ä¢ At submission time, remember to anonymize your assets (if applicable). You can either
770"
NEW ASSETS,0.9570011025358324,"create an anonymized URL or include an anonymized zip file.
771"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.958103638368247,"14. Crowdsourcing and Research with Human Subjects
772"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9592061742006616,"Question: For crowdsourcing experiments and research with human subjects, does the paper
773"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.960308710033076,"include the full text of instructions given to participants and screenshots, if applicable, as
774"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9614112458654906,"well as details about compensation (if any)?
775"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9625137816979051,"Answer: [NA]
776"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9636163175303197,"Justification: The paper does not engage in crowdsourcing experiments or research with
777"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9647188533627343,"human subjects therefore, it does not include participant instructions, screenshots, or details
778"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9658213891951488,"about compensation.
779"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9669239250275634,"Guidelines:
780"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9680264608599779,"‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with
781"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9691289966923925,"human subjects.
782"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9702315325248071,"‚Ä¢ Including this information in the supplemental material is fine, but if the main contribu-
783"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9713340683572216,"tion of the paper involves human subjects, then as much detail as possible should be
784"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9724366041896362,"included in the main paper.
785"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9735391400220507,"‚Ä¢ According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
786"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9746416758544653,"or other labor should be paid at least the minimum wage in the country of the data
787"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9757442116868799,"collector.
788"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9768467475192943,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
789"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9779492833517089,"Subjects
790"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9790518191841234,"Question: Does the paper describe potential risks incurred by study participants, whether
791"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.980154355016538,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
792"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9812568908489526,"approvals (or an equivalent approval/review based on the requirements of your country or
793"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9823594266813671,"institution) were obtained?
794"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9834619625137817,"Answer: [NA]
795"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9845644983461963,"Justification: The paper does not involve research with human subjects, so there are no
796"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9856670341786108,"participant risks to disclose, and no Institutional Review Board (IRB) approvals or equivalent
797"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9867695700110254,"reviews were required.
798"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9878721058434399,"Guidelines:
799"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9889746416758545,"‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with
800"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9900771775082691,"human subjects.
801"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9911797133406836,"‚Ä¢ Depending on the country in which research is conducted, IRB approval (or equivalent)
802"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9922822491730982,"may be required for any human subjects research. If you obtained IRB approval, you
803"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9933847850055126,"should clearly state this in the paper.
804"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9944873208379272,"‚Ä¢ We recognize that the procedures for this may vary significantly between institutions
805"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9955898566703418,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
806"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9966923925027563,"guidelines for their institution.
807"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9977949283351709,"‚Ä¢ For initial submissions, do not include any information that would break anonymity (if
808"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9988974641675854,"applicable), such as the institution conducting the review.
809"
