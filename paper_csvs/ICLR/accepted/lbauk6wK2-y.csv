Section,Section Appearance Order,Paragraph
TSINGHUA UNIVERSITY,0.0,"1Tsinghua University
2Stanford University
pancy17@mails.tsinghua.edu.cn
{yanchaoy, kaichun, guibas}@cs.stanford.edu
duanyueqi@tsinghua.edu.cn"
ABSTRACT,0.0033333333333333335,ABSTRACT
ABSTRACT,0.006666666666666667,"We propose a framework to continuously learn object-centric representations for
visual learning and understanding. Existing object-centric representations either
rely on supervisions that individualize objects in the scene, or perform unsuper-
vised disentanglement that can hardly deal with complex scenes in the real world.
To mitigate the annotation burden and relax the constraints on the statistical com-
plexity of the data, our method leverages interactions to effectively sample di-
verse variations of an object and the corresponding training signals while learn-
ing the object-centric representations. Throughout learning, objects are streamed
one by one in random order with unknown identities, and are associated with la-
tent codes that can synthesize discriminative weights for each object through a
convolutional hypernetwork. Moreover, re-identiﬁcation of learned objects and
forgetting prevention are employed to make the learning process efﬁcient and ro-
bust. We perform an extensive study of the key features of the proposed frame-
work and analyze the characteristics of the learned representations. Furthermore,
we demonstrate the capability of the proposed framework in learning representa-
tions that can improve label efﬁciency in downstream tasks. Our code and trained
models are made publicly available at: https://github.com/pptrick/
Object-Pursuit."
INTRODUCTION,0.01,"1
INTRODUCTION"
INTRODUCTION,0.013333333333333334,"What are human infants and toddlers learning while they are manipulating a discovered object? And,
how do such continual interaction and learning experiences, i.e., objects are discovered and learned
one by one, help develop the capability to understand the scenes that consist of individual objects?
Inspired by these questions, we aim for training frameworks that enable an autonomous agent to con-
tinuously learn object-centric representations through self-supervised discovery and manipulation of
objects, so that the agent can later use the learned representations for visual scene understanding."
INTRODUCTION,0.016666666666666666,"A majority of object-centric representation learning methods focus on encoding images or video
clips into disentangled latent codes, each of which explains an entity in the scene, and together
they should reconstruct the input. However, without explicit supervision and more sophisticated in-
ductive biases beyond parsimony, the disentanglement usually has difﬁculties aligning with objects,
especially for complex scenes. We leverage the fact that an autonomous agent can actively explore
the scene, and propose that the data collected by manipulating a discovered object can serve as an
important source for building inductive biases for object-level disentanglement."
INTRODUCTION,0.02,"In our proposed framework, whenever an object is discovered by the agent, a dataset containing im-
ages and instance masks of this object can easily be sampled via interaction compared to annotating
all the objects. Theoretically speaking, any function of the images induced by the discovered object
could be a representation of the object. For example, let φ be an encoder implemented by a neural
network, and let x be the image of an object, we can say that φ(x) is a representation of the object.
Similarly, the encoder itself can also be a representation of this object since φ = arg minφ L(φ, x),
i.e., φ is the output of an optimization procedure that takes the object’s images as input."
INTRODUCTION,0.023333333333333334,*Equal Contribution.
INTRODUCTION,0.02666666666666667,Published as a conference paper at ICLR 2022
INTRODUCTION,0.03,"We employ network weights as the object-centric representations. Speciﬁcally, the proposed method
learns an object-centric representation from the data collected by manipulating a single object,
through learning a latent code that can be translated into a neural network. The neural network
is produced by a discriminative weight generation hypernetwork and is able to distinguish the rep-
resented object from anything else. In order to learn representations for objects that stream in one
by one, the proposed framework is augmented with an object re-identiﬁcation procedure to avoid
learning seen objects. Moreover, we hypothesize that object representations are embedded in a
low-dimensional manifold, so the proposed framework ﬁrst checks whether a new object can be
represented by learned objects; if not, the new object will be learned as a base object serving the
purpose of representing future objects, thus the name object pursuit. Furthermore, the proposed
framework deals with the catastrophic forgetting of learned object representations by enforcing the
hypernetwork to maintain the mapping between the learned representations and their corresponding
network weights."
INTRODUCTION,0.03333333333333333,"In summary, our work makes the following contributions: 1) we propose a novel framework named
object pursuit that can continuously learn object-centric representations using training data collected
from interactions with individual objects, 2) we perform an extensive study to understand the pursuit
dynamics and characterize its typical behaviors regarding the key design features, and 3) we analyze
the learned object space, in terms of its succinctness and effectiveness in representing objects, and
empirically demonstrate its potential for label efﬁcient visual learning."
RELATED WORK,0.03666666666666667,"2
RELATED WORK"
RELATED WORK,0.04,"Object-centric representation learning falls in the ﬁeld of disentangled representation learning
(Higgins et al., 2016; Kim & Mnih, 2018; Press et al., 2019; Chen et al., 2018b; Karras et al., 2019;
Li et al., 2020; Locatello et al., 2020a; Zhou et al., 2021). However, object-centric representations
require that the disentangled latents correspond to objects in the scene. For example, (Eslami et al.,
2016; Kosiorek et al., 2018) model image formation as a structured generative process so that each
component may represent an object in the generated image. One can also apply inverse graphics
(Yao et al., 2018; Wu et al., 2017) or spatial mixture models (Greff et al., 2017; 2019; Engelcke
et al., 2020b) to decompose images into interpretable latents. Monet (Burgess et al., 2019) jointly
predicts segmentation and representation with a recurrent variational auto-encoder. Capsule autoen-
coders (Kosiorek et al., 2019) are proposed to decompose images into parts and poses that can be
arranged into objects. To deal with complex images or scenes, (Yang et al., 2020; Bear et al., 2020)
employ motion to encourage deomposition into objects. Besides motion, (Klindt et al., 2021) shows
that the transition statistics can be informative about objects in natural videos. Similarly, (Kabra
et al., 2021) infers object latents and frame latents from videos. Slot-attention (Locatello et al.,
2020b; Jiang et al., 2020) employs the attention mechanism that aggregates features with similar ap-
pearance, while Giraffe (Niemeyer & Geiger, 2021) factorizes the scene using neural feature ﬁelds.
Even though better performance is achieved with more sophisticated network designs, scenes with
complex geometry and appearance still lag. As shown in (Engelcke et al., 2020a), the reconstruction
bottleneck has critical effects on the disentanglement quality. Instead of relying on reconstruction
as a learning signal, our work calls for interactions that stimulate and collect training data from
complex environments."
RELATED WORK,0.043333333333333335,"Rehearsal-based continual learning. In general, continual learning methods can be divided into
three streams: rehearsal-based, regularization-based, and expansion-based. The rehearsal-based
method manages buffers to replay past samples, in order to prevent from forgetting knowledge of
the preceding tasks. The regularization-based methods learn to regularize the changes in parameters
of the models. The expansion-based methods aim to expand model architectures in a dynamic
manner. Among these three types, rehearsal-based methods are widely-used due to their simplicity
and effectiveness (L¨uders et al., 2016; Kemker & Kanan, 2017; Rebufﬁet al., 2017; Cha et al.,
2021; von Oswald et al., 2019; Riemer et al., 2018; Lopez-Paz & Ranzato, 2017; Buzzega et al.,
2020; Aljundi et al., 2019; Chaudhry et al., 2020; Parisi et al., 2018; Lopez-Paz & Ranzato, 2017).
Samples from previous tasks can either be the data or corresponding network activations on the data.
For example, (Shin et al., 2017) proposes a dual-model architecture where training data from learned
tasks can be sampled from a generative model and (Draelos et al., 2017; Kamra et al., 2017) propose
sampling in the output space of an encoder for training tasks relying on an auto-encoder architecture.
ICaRL Rebufﬁet al. (2017) allows adding new classes progressively based on the training samples"
RELATED WORK,0.04666666666666667,Published as a conference paper at ICLR 2022
RELATED WORK,0.05,"Figure 1: Object space as discriminative weights. Objects live in a low-dimensional manifold of
a high-dimensional latent space. A latent code representing a speciﬁc object is translated into seg-
mentation weights that can distinguish the object from anything else at different viewing conditions.
The hypernetwork consists of blocks built of convolutional and upsampling layers."
RELATED WORK,0.05333333333333334,"with a small number of classes, while (Pellegrini et al., 2020; Li & Hoiem, 2017) store activations
volumes at some intermediate layer to alleviate the computation and storage requirement. Co2L (Cha
et al., 2021) proposes continual learning within the contrastive representation learning framework,
and (Balaji et al., 2020) studies continual learning in large scale where tasks in the input sequence are
not limited to classiﬁcation. Similar to the forgetting prevention component in our framework, von
Oswald et al. (2019) applies a task-conditioned hypernetwork to rehearse the task-speciﬁc weight
realizations. Please refer to (Parisi et al., 2019; Delange et al., 2021) for a more comprehensive
review on this subject."
RELATED WORK,0.056666666666666664,"Hypernetwork. The goal of hypernetworks is to generate the weights of a target network, which is
responsible for the main task (Ha et al., 2016; Krueger et al., 2017; Chung et al., 2016; Bertinetto
et al., 2016; Lorraine & Duvenaud, 2018; Sitzmann et al., 2020; Nirkin et al., 2021). For example,
(Krueger et al., 2017) proposes Bayesian hypernetworks to learn the variational inference in neural
networks and (Bertinetto et al., 2016) proposes to learn the network parameters in one shot. Hyper-
Seg (Nirkin et al., 2021) presents real-time semantic segmentation by employing a U-Net within a
U-Net architecture, and (Finn et al., 2019) applies hypernetwork to adapt to new tasks for continual
lifelong learning. Moreover, (Tay et al., 2020) proposes a new transformer architecture that lever-
ages task-conditioned hypernetworks for controlling its feed-forward layers, whereas (Ma et al.,
2021) proposes hyper-convolution, which implicitly represents the convolution kernel as a function
of kernel coordinates. Hypernetworks have shown great potential in different meta-learning settings
(Rusu et al., 2018; Munkhdalai & Yu, 2017; Wang et al., 2019), mainly due to that hypernetworks
are effective in compressing the primary networks’ weights as proved in (Galanti & Wolf, 2020)."
METHOD,0.06,"3
METHOD"
METHOD,0.06333333333333334,"We consider an agent that can explore the environment and manipulate objects which are discovered
in an unknown order. Suppose there are N objects in the scene, each of which randomly appears in
an image x ∈RH×W ×3, whose ground-truth instance segmentation mask is y ∈RH×W ×N. One
can train a deep neural network that maps an image x to its mask y with a dataset D = {(xi, yi)}
that consists of such paired training samples. However, sampling from the joint distribution p(x, y)
can be extremely time-consuming, e.g., someone may have to manually draw the instance masks for
every object in an image."
METHOD,0.06666666666666667,"On the other hand, sampling from the marginals can be much more accessible through interactions.
Let Dk be the dataset collected by observing an image xi and the corresponding binary mask of
the k-th object yk
i ∈RH×W , i.e., Dk = {(xi, yk
i )} ∼p(x, yk), which is the marginal distribution
obtained by integrating out other objects’ masks in y. The goal of the proposed object pursuit frame-
work is to learn object-centric representations from the data collected by continuously sampling the
marginals. Next, we detail the representations used for objects (as illustrated in Fig. 1), and how we
can learn them without catastrophic forgetting."
METHOD,0.07,Published as a conference paper at ICLR 2022
REPRESENTING OBJECTS VIA DISCRIMINATIVE WEIGHT GENERATION,0.07333333333333333,"3.1
REPRESENTING OBJECTS VIA DISCRIMINATIVE WEIGHT GENERATION"
REPRESENTING OBJECTS VIA DISCRIMINATIVE WEIGHT GENERATION,0.07666666666666666,"In order to represent an object, one can compute any functions of the data produced with this object.
For example, the encoding of an image containing a speciﬁc object that can be used to reconstruct
the input image. Here we take a conjugate perspective instead of asking the representation to store
information of an object that is good for reconstruction. We propose that the object-centric repre-
sentation of an object shall generate the mechanisms for performing certain downstream tasks on
this object, e.g., distinguishing this object from the others."
REPRESENTING OBJECTS VIA DISCRIMINATIVE WEIGHT GENERATION,0.08,"Let φ be a segmentation network with learnable weights θ that maps an image to a binary mask, i.e.,
φ : Θ × RH×W ×3 →RH×W . Moreover, let ψ : ζ →Θ be the mapping from the latent space ζ
to the weights of the segmentation backbone φ. We deﬁne the object-centric representation of an
object o as a latent zo ∈ζ, such that:"
REPRESENTING OBJECTS VIA DISCRIMINATIVE WEIGHT GENERATION,0.08333333333333333,"E(xi,yo"
REPRESENTING OBJECTS VIA DISCRIMINATIVE WEIGHT GENERATION,0.08666666666666667,"i )∼p(x,yo)∆(φ(ψ(zo), xi), yo"
REPRESENTING OBJECTS VIA DISCRIMINATIVE WEIGHT GENERATION,0.09,"i ) ≥τ,
(1)"
REPRESENTING OBJECTS VIA DISCRIMINATIVE WEIGHT GENERATION,0.09333333333333334,"where the expectation is computed according to p(x, yo), i.e., the marginal distribution of object o,
and ∆is a similarity measure between the prediction from φ and the sampled mask yo. In other
words, zo is a representation of object o, if the network weights generated from zo are capable
of predicting high-quality instance masks regarding the object under the corresponding marginal
distribution. The threshold τ is a scalar parameter that will be studied in the experiments. Now we
detail the proposed object pursuit framework, which uniﬁes object re-identiﬁcation, succinctness of
the representation space, and forgetting prevention, for continuously learning object representations."
OBJECT PURSUIT,0.09666666666666666,"3.2
OBJECT PURSUIT"
OBJECT PURSUIT,0.1,"Given the deﬁnition of object-centric representations in Eq. 1, our goal is to construct a low-
dimensional manifold to embed objects in the input space ζ of the weight generation hypernetwork
ψ. We conjecture that the low-dimensional manifold can be spanned by a set of base object repre-
sentations. More explicitly, we instantiate two lists z and µ, which store the representations of the
base objects and the embeddings of the learned objects, respectively. We denote zt−1 = {zi}m
i=1
and µt−1 = {µi}n
i=1 (n ≥m, with n the number of learned objects and m the number of base
objects, up to time t −1) as the constructed lists after encountering a (t −1)-th object. Note that µi
has the same dimension as the number of base object representations. Similarly, we denote ψt−1 as
the corresponding hypernetwork parameters."
OBJECT PURSUIT,0.10333333333333333,"As discussed, when the t-th object ot is discovered, a dataset Dt = {(xj, yt
j)} can be easily sampled
from the marginal distribution p(x, yt) through interactions. However, such object might already
be seen previously. Thus, it is necessary to apply re-identiﬁcation to avoid repetitively learning the
same object. According to the deﬁnition in Eq. 1, object ot will be claimed as a seen or learned
object if the following condition is true (| · | is the cardinality of a set):"
OBJECT PURSUIT,0.10666666666666667,"max
i≤|µt−1| E(xj,yt
j)∈Dt∆(φ(ψt−1(zi), xj), yt
j) ≥τ.
(2)"
OBJECT PURSUIT,0.11,"with zi = µi ·zt−1. In this case, object ot will be assigned the identity i∗that achieves the maximum
value. Otherwise, if Eq. 2 is not valid, ot is considered as an object that has not been learned."
OBJECT PURSUIT,0.11333333333333333,"Learning base object representations. An object ot that can not be identiﬁed with the list of
learned objects µt−1 can potentially serve as a base object whose representation should be added to
the list of base representations z. To ensure that object ot qualiﬁes as a base object, we propose the
following test which checks whether ot can be embedded in the current manifold spanned by zt−1:"
OBJECT PURSUIT,0.11666666666666667,"µ∗= arg max
µ∈R|zt−1|
E(xj,yt
j)∈Dt∆(φ(ψt−1(µT zt−1), xj), yt
j) + α∥µ∥1,
(3)"
OBJECT PURSUIT,0.12,"where µ∗is the optimal embedding for object ot regarding zt−1 under the ℓ1 regularizer to encourage
sparsity. If the ﬁrst term of Eq. 3 passes the threshold τ with the representation µ∗T zt−1, then we
consider ot as an object that should not be added to the list of bases since it can already be represented
by the existing base objects."
OBJECT PURSUIT,0.12333333333333334,"Next, if ot does not fall on the manifold spanned by zt−1, a joint learning of the representation of ot
and the hypernetwork ψ shall be performed so that a new base object representation can be added"
OBJECT PURSUIT,0.12666666666666668,Published as a conference paper at ICLR 2022
OBJECT PURSUIT,0.13,Figure 2: Data collected in iThor. Target objects are highlighted by their instance masks.
OBJECT PURSUIT,0.13333333333333333,"to the list. However, since updating the hypernetwork could result in catastrophic forgetting of the
previously learned object representations, it is also necessary to constrain the learning process, and
the training loss is:"
OBJECT PURSUIT,0.13666666666666666,"z∗, ψ∗= arg max
z,ψ
E(xj,yt
j)∈Dt∆(φ(ψ(z), xj), yt
j) + α∥z∥1 + β
X"
OBJECT PURSUIT,0.14,"i≤|µt−1|
∥ψ(µT
i zt−1) −ψt−1(µT
i zt−1)∥1,
(4)"
OBJECT PURSUIT,0.14333333333333334,"where the ﬁrst two terms help to ﬁnd a good representation for object ot under the sparsity con-
straint, and the third term enforces that the updated weight generation hypernetwork maintains the
previously learned object representations. The value of the negative scalar coefﬁcients α, β will be
detailed in the experiments."
OBJECT PURSUIT,0.14666666666666667,"Backward redundancy removal. The last but not the least component of the proposed object pur-
suit framework is to have a backward redundancy check. Since the weight generation hypernetwork
is updated to ψt = ψ∗with Eq. 4, there may now exist an embedding µ∗(computed using Eq. 3)
that re-certiﬁes object ot as an object falls on the manifold spanned by zt−1 under ψt. If this is true,
we set zt = zt−1, otherwise, z∗is added to the list of base object representations since object ot is
now conﬁrmed as a base object. In some rare cases, object ot might be hard to learn, e.g., z∗may not
satisfy the criterion described in Eq. 1 under the current hypernetwork ψt. In this case, we simply
toss away this object so that it can be better learned in the future as the pursuit process evolves. The
proposed object pursuit framework is also summarized in Algorithm. 1."
EXPERIMENTS,0.15,"4
EXPERIMENTS"
EXPERIMENTS,0.15333333333333332,"We target the learning scenario where a scene consists of multiple objects, each of them can be
discovered and manipulated through interactions. The objects are learned one by one in a continuous
manner but with unknown orders. There are two main aspects of the whole pipeline, i.e., data
collection by sampling the marginals of individual objects and construction of the object-centric
representations with Object Pursuit. We focus on continuous object-centric representation learning,
and thus orient our study on the behavior and characteristics of the proposed object pursuit algorithm.
We also perform experiments on one-shot and few-shot learning, and show the potential of the
learned object-centric representations in effectively reducing supervisions for object detection. Next,
we brief our data collection process."
SETUP,0.15666666666666668,"4.1
SETUP"
SETUP,0.16,"Data collection. To learn diverse objects from variant positions and viewing angles, we collect
synthetic data within the iThor environment ((Kolve et al., 2017)), which provides a set of interactive
objects and scenes, as well as accurate modeling of the physics. We collect data of 138 different
objects to generate their images and masks. The 138 objects are divided into 52 pretraining objects,
60 train objects for the pursuit process, and 25 test unseen objects. To focus on the representation
learning part, we abstract the interaction policy, and the data collection procedure of a single object
can be summarized as follows: 1) Randomly set the positions of all the objects in the scene. 2)
Calculate all available camera positions and viewing angles from which the target object (to be
learned) is visible so that the sampling is effective. The camera position, yaw angle, and pitch angle
change within the range of 0.4 (grid size), 4◦and 30◦respectively. 3) For each camera position and
viewing angle, we collect a 572 × 572 RGB image and a binary mask of the target object. 4) Repeat
(1-3) for all objects in the stream. Please check Fig. 2 for the sampled data."
SETUP,0.16333333333333333,Published as a conference paper at ICLR 2022
SETUP,0.16666666666666666,"Table 1:
Re-identiﬁcation:
recall
and precision on seen objects."
SETUP,0.17,"τ
0.5
0.6
0.7
0.8"
SETUP,0.17333333333333334,"recall
1.0
1.0
1.0
1.0
precision
1.0
1.0
1.0
1.0"
SETUP,0.17666666666666667,"Table 2: Re-identiﬁcation: rate of unseen objects been
identiﬁed along the course of the pursuit process."
SETUP,0.18,"τ
No. of trained objects"
SETUP,0.18333333333333332,"8
16
24
32
40
48
56"
SETUP,0.18666666666666668,"0.5
0.40
0.52
0.56
0.60
0.64
0.64
0.72
0.6
0.08
0.20
0.28
0.44
0.56
0.60
0.60
0.7
0.16
0.28
0.32
0.40
0.40
0.48
0.44
0.8
0.00
0.08
0.16
0.24
0.28
0.28
0.28"
SETUP,0.19,"Network implementation In our experiment, we use Deeplab v3+ (Chen et al., 2018a) as the seg-
mentation network φ, which consists of 3 parts: a backbone to encode features at different levels,
an aspp module, and a decoder to predict the segmentation probability per pixel. We use resnet18
as the backbone (encoder), whose weights are ﬁxed both in the pretraining and the pursuit process.
The weights of the aspp module and the decoder are generated by the convolutional hypernetwork
ψ. For each convolution layer in the aspp module and the decoder, ψ takes object representation
z as input, and predict weights of the convolution kernel using an upsampling convolution block.
The input representation z ﬁrst expanded to a 1024-dim vector by a linear mapping and resized to a
1 × 1 × 32 × 32 tensor. After going through several upsampling blocks, each of which consists of
an upsampling followed by a convolution and a leaky Relu, the 1 × 1 × 32 × 32 tensor turns into
the output kernel weight. For other network weights like ’running mean’ and ’running var’ in batch
normalization, the hypernetwork linearly maps representation z to generate them."
SETUP,0.19333333333333333,"Training details. For the similarity measure ∆, we use the dice score proposed in (Milletari et al.).
In addition to ∆, we ﬁnd that it will be beneﬁcial to add an extra binary cross-entropy term when
learning base object representations using Eq. 4. Note, to deal with imbalanced foreground and
background sizes, we also put a weighting on the entropy terms that correspond to the object so the
learning can be more efﬁcient. The sparsity constraint α is set to −0.2, −0.1 for Eq. 3 and Eq. 4
respectively, and β = −0.04 for all our experiments. To improve the convergence, we also warm
up the hypernetwork using the pretraining objects. During pretraining, each mini-batch contains
training data from one object, and we randomly choose which object to use in the next batch. In
backpropagation, we update the hypernetwork ψ and representation z for each object. When the
pretraining is done, we perform a redundancy check to get rid of the objects that can be represented
by others. For simplicity, this check is performed in sequential order, and we are left with a set of
base object representations to carry out the following studies."
ON THE REPRESENTATION QUALITY MEASURE,0.19666666666666666,"4.2
ON THE REPRESENTATION QUALITY MEASURE"
ON THE REPRESENTATION QUALITY MEASURE,0.2,"The learning dynamics and the output of Algorithm. 1, i.e., the lists of base object representations
z and the learned objects µ, together with the weight generation hypernetwork ψ, are primarily af-
fected by the representation quality measure τ introduced in Eq. 1. For example, τ controls whether
an object will be claimed as seen, and it also determines whether or not an object falls on the mani-
fold spanned by the current base object representations. We study each of them in the following."
RE-IDENTIFICATION,0.20333333333333334,"4.2.1
RE-IDENTIFICATION"
RE-IDENTIFICATION,0.20666666666666667,"As described in Eq. 2, when an object is discovered, it will be ﬁrst checked against the learned
objects and re-identiﬁed if the maximum expected similarity passes τ. To examine how the quality
measure τ inﬂuences the re-identiﬁcation process, we run multiple object pursuit processes with
different τ’s. All runs are performed with the same training object order so that the only variant is
the value of τ. For evaluation, we preserve a separate set of 25 objects (unseen test objects) that
never appear during training. Note, among these unseen test objects, there are also objects that are
similar to the training ones. And we use 27 objects (seen test objects) from the warp-up joint training
described above to check the re-identiﬁcation accuracy."
RE-IDENTIFICATION,0.21,"First, we check how τ affects the re-identiﬁcation for seen objects. As reported in Tab. 1, if an object
is learned and added to the object list µ, it will be claimed as seen by Eq. 2, and the re-identiﬁcation
accuracy is always one. This is true for τ varying between 0.5 and 0.8, which demonstrates the
robustness of the re-identiﬁcation process against τ for objects learned."
RE-IDENTIFICATION,0.21333333333333335,Published as a conference paper at ICLR 2022
RE-IDENTIFICATION,0.21666666666666667,"Figure 3: Unseen objects re-identiﬁed as learned. 1st row:
unseen objects, 2nd to 4th row: similar objects from the
learned object list. Bounding boxes highlight the objects
with embedded text indicating the instance identity."
RE-IDENTIFICATION,0.22,"Second, we check the behavior of the
re-identiﬁcation component for un-
seen objects under different τ’s along
the pursuit process. In Tab. 2, we can
observe that as more and more ob-
jects are learned during the pursuit,
the unseen objects that are claimed
as seen from the re-identiﬁcation pro-
cess also increase. This observation
is consistent across different τ’s. Fur-
thermore, the rate of unseen objects
identiﬁed as seen converges at the end
of the pursuit process, but at differ-
ent levels for different τ’s, i.e., the
converged rate is lower for larger τ.
It may seem incorrect if an unseen
object is claimed as seen by the re-
identiﬁcation component. However,
if we examine the unseen objects (see
Fig. 3), we can see that it is quite nat-
ural for these unseen objects to be la-
beled as seen, because they are sim-
ilar to one or multiple objects in the
object list µ.
This is indeed a de-
sired characteristic since representing
or learning an object that is similar to existing ones may not be informative. Moreover, one can adjust
τ to tune the similarity level. For example, if one insists on learning an object similar to previously
seen objects, increasing the value of τ should work as evidenced by the converged rates for τ’s in
Tab. 2."
RE-IDENTIFICATION,0.22333333333333333,"In a nutshell, the representation quality measure τ has little effect on the re-identiﬁcation recall
and accuracy for learned objects. Yet, it controls the granularity of the learned representations by
modulating the rate of unseen objects that would be identiﬁed as learned ones."
SUCCINCTNESS AND EXPRESSIVENESS,0.22666666666666666,"4.2.2
SUCCINCTNESS AND EXPRESSIVENESS"
SUCCINCTNESS AND EXPRESSIVENESS,0.23,"Table 3: Pursuit dynamics by varying
τ. Please see the enclosing descrip-
tion for the meaning of the metrics
and corresponding analysis."
SUCCINCTNESS AND EXPRESSIVENESS,0.23333333333333334,"τ
0.5
0.6
0.7
0.8"
SUCCINCTNESS AND EXPRESSIVENESS,0.23666666666666666,"|z|/N
0.34
0.42
0.42
0.40
|µ|/N
0.46
0.58
0.46
0.40
Re
0.19
0.21
0.00
0.00
Rf
0.08
0.18
0.42
0.54
Aµ
0.75
0.77
0.83
0.86"
SUCCINCTNESS AND EXPRESSIVENESS,0.24,"We want to study how the representation quality measure
τ affects the overall learning dynamics in terms of the suc-
cinctness and expressiveness of the learned base object rep-
resentations. By checking Eq. 2, Eq 3 and also the previous
experiment, we conjecture that if τ is small, objects similar
to the learned ones will be more easily identiﬁed as seen and
certiﬁed as on the manifold. If so, the number of objects that
will be used for learning the base representations may also
be small, thus increasing the succinctness of the ﬁnal repre-
sentations. Conversely, when τ increases, we would expect
that more objects will contribute to the base representations,
thus increasing the expressiveness. We like to check if the
observations align with our conjecture and how such behav-
ior affects the quality of the learned bases. To facilitate the analysis, we propose to check the
following quantities: 1) |z|/N, which is the portion of objects that contribute to base representa-
tions; 2) |µ|/N, which is the portion of learnable objects that are added to the object list µ; 3) Re,
rate of objects that are conﬁrmed unseen but can be expressed by the base object representations; 4)
Rf, rate of objects to be learned as base representations, which are later considered as redundant or
unqualiﬁed; 5) Aµ, segmentation accuracy on learned objects."
SUCCINCTNESS AND EXPRESSIVENESS,0.24333333333333335,"We report the above metrics across different τ’s in Tab. 3. As expected, a larger τ generally encour-
ages more objects to be learned as base representations. For example, the number of base objects
learned is much larger when τ increases from 0.5 to 0.6 (ﬁrst row). This is also evidenced by the
third row of Tab. 3, which shows that the probability of an unseen object to be expressed by base rep-"
SUCCINCTNESS AND EXPRESSIVENESS,0.24666666666666667,Published as a conference paper at ICLR 2022
SUCCINCTNESS AND EXPRESSIVENESS,0.25,"resentations will decrease as τ increases, creating more attempts to learn objects as bases. However,
the number of learnable objects, i.e., a base object or an object that falls on the manifold spanned by
the bases, attains the maximum at a medium value τ = 0.6 (second row). The underlying reason is
two-fold: First, a very small τ means that many objects will be identiﬁed as seen and thus discarded
to save computation; Second, a very large τ can make the qualiﬁcation of an object representation
extremely difﬁcult such that it will be put aside for future learning. The latter is also supported by
the metric Rf shown in the fourth row, i.e., the probability that an object will be considered redun-
dant or unqualiﬁed after learning as a base object will increase as τ becomes large. Lastly, when
checking the quality of the base representations in expressing a common set of learned objects, we
can see that the segmentation accuracy correlates with τ in a positive manner (ﬁfth row)."
SUCCINCTNESS AND EXPRESSIVENESS,0.25333333333333335,"In general, τ directly impacts the quality of the base representations for learned objects, but its effect
on the number of base representations produced by the pursuit procedure is not monotone. Within
a moderate range, we can increase τ to encourage learning more base representations, however, we
may not want τ to be too large that only a few objects are qualiﬁed as base representations."
LABEL EFFICIENCY,0.25666666666666665,"4.2.3
LABEL EFFICIENCY"
LABEL EFFICIENCY,0.26,"Table 4: N-shot learning the representation of a new object.
Training is performed by searching the optimal representation
either on the manifold spanned by the base objects, or over the
entire representation space. Segmentation accuracy on the test
set is reported for bases and hypernetworks learned at different
τ’s."
LABEL EFFICIENCY,0.2633333333333333,"n
over base object representations
full representation space"
LABEL EFFICIENCY,0.26666666666666666,"0.5
0.6
0.7
0.8
0.5
0.6
0.7
0.8"
LABEL EFFICIENCY,0.27,"1
0.377 0.416 0.454
0.446
0.225 0.264 0.288 0.289
5
0.595 0.606 0.634
0.614
0.461 0.475 0.468 0.453
10
0.622 0.647 0.677
0.649
0.542 0.526 0.524 0.520
2000 0.697 0.731 0.740
0.731
0.669 0.698 0.702 0.718"
LABEL EFFICIENCY,0.2733333333333333,"Besides the training dynamics,
we evaluate the usefulness of the
learned object base representa-
tions in terms of how it facilitates
learning the representation of a
new object with only a few anno-
tations. For comparison, we also
perform learning of the object
representations over the entire
representation space.
Training
is similar to Eq. 3. The quality
of the learned object representa-
tions is measured by their seg-
mentation accuracy on test data."
LABEL EFFICIENCY,0.27666666666666667,"As reported in Tab. 4, the quality
of the few-shot learned representations increases as τ gets large, which aligns with our observation
in the previous section that the expressiveness of the learned object base representations highly
correlates with τ. However, note that there is a slight drop in performance when τ increases from 0.7
to 0.8 (fourth and ﬁfth column). The reason is that as τ gets really large, it also becomes much easier
to omit objects that can not pass the quality test. As a result, the hypernetwork, which translates the
representation to network weights, also gets less trained. Thus, when tested on new objects, the
performance may not match that of the trained objects for the same set of base representations,
suggesting again that a moderate τ is needed to balance between the succinctness and generalization
of the learned base representations."
LABEL EFFICIENCY,0.28,"The above observation does not hold for the representations learned over the full space. Moreover,
when comparing the performance within the low data regime, we can see that those object repre-
sentations found on the manifold outperform those found in the entire space by a large margin. For
example, the new object representations found with the learned bases under τ = 0.7 outperform their
counterparts by 57.6%, 35.5%, 29.2% for the 1-shot, 5-shot, and 10-shot settings, respectively. This
demonstrates the potential of using the learned base representations to help reduce the supervision
needed to learn a new object. Also, it conﬁrms that the learned base representations are meaningful
since the manifold spanned by them provides a good regularity for learning unseen objects."
ORDER OF TRAINING OBJECTS,0.2833333333333333,"4.3
ORDER OF TRAINING OBJECTS
Table 5:
Pursuit dynamics under
random training object order."
ORDER OF TRAINING OBJECTS,0.2866666666666667,"|z|/N |µ|/N Re
Rf
Aµ"
ORDER OF TRAINING OBJECTS,0.29,"mean
0.43
0.50
0.10 0.15 0.76
std-dev
0.02
0.03
0.04 0.04 0.01"
ORDER OF TRAINING OBJECTS,0.29333333333333333,"The proposed object pursuit algorithm learns object repre-
sentations in a stream, so we also check how the learning
dynamics vary when the order of training objects changes.
We ﬁx τ to 0.6 and run ten pursuit processes with random
training object order. We reported the mean and standard de-
viation of the metrics proposed in Tab. 3. As observed in
Tab. 5, the pursuit process is robust to the training object order."
ORDER OF TRAINING OBJECTS,0.2966666666666667,Published as a conference paper at ICLR 2022
FORGETTING PREVENTION,0.3,"4.4
FORGETTING PREVENTION"
FORGETTING PREVENTION,0.30333333333333334,"Table 6:
Pursuit dynamics under
different forgetting prevention con-
straints."
FORGETTING PREVENTION,0.30666666666666664,"|β|
0.0
0.02
0.04
0.1"
FORGETTING PREVENTION,0.31,"|z|/N
0.61
0.46
0.42
0.39
|µ|/N
0.88
0.61
0.58
0.54
Re
0.13
0.14
0.21
0.21
Rf
0.19
0.14
0.18
0.21
Aµ
0.02
0.67
0.71
0.72
γf
0.97
0.04
0.02
0.02"
FORGETTING PREVENTION,0.31333333333333335,"In this section, we want to check if the forgetting preven-
tion term in Eq. 4 is effective and how it affects the pursuit
dynamics. We run pursuit processes with different values of
the coefﬁcient β, where the quality measure τ and training
object order is ﬁxed. Segmentation accuracy Aµ and forget-
ting rate γf (i.e., how many objects falls under the quality
measure after the process is ﬁnished) in Tab. 6 demonstrate
the effectiveness of the forgetting prevention term: when |β|
decreases, the segmentation accuracy drops, and the forget-
ting rate increases; when |β| vanishes, the forgetting rate
reaches 97%, which means that the hypernetwork almost
forgets all the object representations it previously learned.
We can also observe that both |z|/N and |µ|/N increase
when |β| decreases. This is due to the fact that when the hypernetwork forgets what are learned,
any incoming object will be unlikely to be considered as seen, nor to be expressed by current bases.
So the hypernetwork tends to learn them as new base objects, which causes |z|/N to increase. This
is also evidenced by the drop in Re, which is rate of new objects that are certiﬁed as on the object
manifold. Furthermore, without the constraint of the forgetting prevention term, it is more likely to
get higher accuracy in learning a new object, which decreases the number of unqualiﬁed objects.
Since the number of redundant objects and unqualiﬁed objects both drop when |β| decreases, |µ|/N
increases. Thus, in order to reduce computational cost and enforce learning meaningful representa-
tions, one would like to apply a relatively large |β| during the pursuit process."
FORGETTING PREVENTION,0.31666666666666665,"We can also observe that as |β| changes from 0.02 to 0.1, Rf increases monotonically, this is be-
cause the forgetting prevention constraint affects the quality of the learned representations, since
less freedom is available when |β| is extremely large. Consequently, fewer objects will be qualiﬁed
with a good representation measured by τ. On the other hand, Rf is also high when beta is set to
0. The reason is that when learning a new object without the constraint of the forgetting prevention
term, the hypernetwork tends to overﬁt, thus making it easier for this new object to be considered
as redundant, i.e., it can be expressed by the existing base representations, even though the learned
representation will be forgotten by the network right after the current learning episode."
MORE RESULTS,0.32,"4.5
MORE RESULTS"
MORE RESULTS,0.3233333333333333,"In the appendix, we also provide ablation studies on how the sparsity constraints in Eq. 3 and Eq. 4
affect the learning dynamics and the quality of the learned representations. By examining the most
relevant base objects for a novel object that can be expressed by the base representations (Fig. 4), we
can qualitatively see that high-level concepts are learned within the representation space as objects
that share similar geometry or appearance will be more correlated than others. For curiosity, we also
test the usefulness of the base object representations on real-world video objects. As demonstrated
in Fig. 7, the learned base representations can capture well the representations of real-world objects
with a single learning example even if they are trained on synthetic data."
CONCLUSION,0.32666666666666666,"5
CONCLUSION"
CONCLUSION,0.33,"We demonstrate that the proposed object pursuit framework can be used for continuously learning
object-centric representations from data collected by manipulating a single object. The key designs,
e.g., object re-identiﬁcation, forgetting prevention, and redundancy check, all contribute to the qual-
ity of the learned base object representations. We also show the potential of using the learned object-
centric representations for tasks at a low-annotation regime. Especially, the learned object manifold
provides a meaningful and effective prior on objects, which can facilitate downstream tasks that
require object-level reasoning. As inspired by an initial attempt on the real-world data (Fig. 7), we
would also like to check the proposed object pursuit algorithm in the real world. For example, we
can train an autonomous agent to collect data from the natural 3D environment with a more efﬁ-
cient interaction policy, and then test the learned object representations on real-world compositional
visual reasoning tasks. These are in our future research agenda."
CONCLUSION,0.3333333333333333,Published as a conference paper at ICLR 2022
CONCLUSION,0.33666666666666667,"Acknowledgement
Toyota Research Institute provided funds to support this work. It is also sup-
ported by a Vannevar Bush Faculty Fellowship and a grant from the Stanford HAI Institute."
ETHICS STATEMENT,0.34,"Ethics Statement.
The proposed object pursuit framework aims at learning object representations
for object-centric visual reasoning tasks. Currently, the experiments are performed in simulation,
which is publicly available and comes with a proper license. However, how to use the learned rep-
resentations could be an issue, and we will explicitly state the guideline on how to use our code and
trained models ethically. In our future research, when data collection in the real world is involved,
we will consult the university ethics review committee for advice. However, in the current form, we
do not observe any signiﬁcant concerns."
REPRODUCIBILITY STATEMENT,0.3433333333333333,"Reproducibility Statement.
Our code, training data, and learned models are made publicly avail-
able. We add detailed comments in the code so that the implementation can be easily understood.
For a preview of the implementation, please refer to the attached code in the supplementary materi-
als."
REFERENCES,0.3466666666666667,REFERENCES
REFERENCES,0.35,"Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample selection
for online continual learning. In Advances in neural information processing systems, pp. 11816–
11825, 2019."
REFERENCES,0.35333333333333333,"S Avinash Ramakanth and R Venkatesh Babu. Seamseg: Video object segmentation using patch
seams. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
376–383, 2014."
REFERENCES,0.3566666666666667,"Yogesh Balaji, Mehrdad Farajtabar, Dong Yin, Alex Mott, and Ang Li. The effectiveness of memory
replay in large scale continual learning. arXiv preprint arXiv:2010.02418, 2020."
REFERENCES,0.36,"Daniel Bear, Chaofei Fan, Damian Mrowca, Yunzhu Li, Seth Alter, Aran Nayebi, Jeremy Schwartz,
Li F Fei-Fei, Jiajun Wu, Josh Tenenbaum, et al. Learning physical graph representations from
visual scenes. Advances in Neural Information Processing Systems, 33, 2020."
REFERENCES,0.36333333333333334,"Luca Bertinetto, Jo˜ao F Henriques, Jack Valmadre, Philip Torr, and Andrea Vedaldi. Learning feed-
forward one-shot learners. In Advances in neural information processing systems, pp. 523–531,
2016."
REFERENCES,0.36666666666666664,"Christopher P Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matt
Botvinick, and Alexander Lerchner. Monet: Unsupervised scene decomposition and represen-
tation. arXiv preprint arXiv:1901.11390, 2019."
REFERENCES,0.37,"Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. Dark
experience for general continual learning: a strong, simple baseline. In Advances in neural infor-
mation processing systems, 2020."
REFERENCES,0.37333333333333335,"Sergi Caelles, Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Laura Leal-Taix´e, Daniel Cremers, and
Luc Van Gool. One-shot video object segmentation. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 221–230, 2017."
REFERENCES,0.37666666666666665,"Hyuntak Cha, Jaeho Lee, and Jinwoo Shin. Co2l: Contrastive continual learning. arXiv preprint
arXiv:2106.14413, 2021."
REFERENCES,0.38,"Hala Lamdouar Charig Yang, Erika Lu, Andrew Zisserman, and Weidi Xie. Self-supervised video
object segmentation by motion grouping. 2021."
REFERENCES,0.38333333333333336,"Arslan Chaudhry, Albert Gordo, Puneet Kumar Dokania, Philip Torr, and David Lopez-Paz. Using
hindsight to anchor past knowledge in continual learning. arXiv preprint arXiv:2002.08165, 2020."
REFERENCES,0.38666666666666666,"Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-
decoder with atrous separable convolution for semantic image segmentation. In Proceedings of
the European conference on computer vision (ECCV), pp. 801–818, 2018a."
REFERENCES,0.39,Published as a conference paper at ICLR 2022
REFERENCES,0.3933333333333333,"Ricky TQ Chen, Xuechen Li, Roger Grosse, and David Duvenaud. Isolating sources of disentangle-
ment in variational autoencoders. arXiv preprint arXiv:1802.04942, 2018b."
REFERENCES,0.39666666666666667,"Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural net-
works. arXiv preprint arXiv:1609.01704, 2016."
REFERENCES,0.4,"Matthias Delange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Greg
Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classiﬁcation
tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021."
REFERENCES,0.4033333333333333,"Timothy J Draelos, Nadine E Miner, Christopher C Lamb, Jonathan A Cox, Craig M Vineyard,
Kristofor D Carlson, William M Severa, Conrad D James, and James B Aimone. Neurogenesis
deep learning: Extending deep networks to accommodate new classes. In 2017 International
Joint Conference on Neural Networks (IJCNN), pp. 526–533. IEEE, 2017."
REFERENCES,0.4066666666666667,"Martin Engelcke, Oiwi Parker Jones, and Ingmar Posner. Reconstruction bottlenecks in object-
centric generative models. arXiv preprint arXiv:2007.06245, 2020a."
REFERENCES,0.41,"Martin Engelcke, Adam R. Kosiorek, Oiwi Parker Jones, and Ingmar Posner. Genesis: Generative
scene inference and sampling with object-centric latent representations. In International Confer-
ence on Learning Representations, 2020b. URL https://openreview.net/forum?id=
BkxfaTVFwH."
REFERENCES,0.41333333333333333,"SM Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, Geoffrey E Hinton,
et al. Attend, infer, repeat: Fast scene understanding with generative models. Advances in Neural
Information Processing Systems, 29:3225–3233, 2016."
REFERENCES,0.4166666666666667,"Qingnan Fan, Fan Zhong, Dani Lischinski, Daniel Cohen-Or, and Baoquan Chen. Jumpcut: non-
successive mask transfer and interpolation for video cutout. ACM Trans. Graph., 34(6):195–1,
2015."
REFERENCES,0.42,"Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine. Online meta-learning. In
International Conference on Machine Learning, pp. 1920–1930. PMLR, 2019."
REFERENCES,0.42333333333333334,"Tomer Galanti and Lior Wolf.
On the modularity of hypernetworks.
arXiv preprint
arXiv:2002.10006, 2020."
REFERENCES,0.4266666666666667,"Klaus Greff, Sjoerd Van Steenkiste, and J¨urgen Schmidhuber. Neural expectation maximization.
arXiv preprint arXiv:1708.03498, 2017."
REFERENCES,0.43,"Klaus Greff, Rapha¨el Lopez Kaufman, Rishabh Kabra, Nick Watters, Christopher Burgess, Daniel
Zoran, Loic Matthey, Matthew Botvinick, and Alexander Lerchner. Multi-object representation
learning with iterative variational inference. In International Conference on Machine Learning,
pp. 2424–2433. PMLR, 2019."
REFERENCES,0.43333333333333335,"Matthias Grundmann, Vivek Kwatra, Mei Han, and Irfan Essa. Efﬁcient hierarchical graph-based
video segmentation. In 2010 ieee computer society conference on computer vision and pattern
recognition, pp. 2141–2148. IEEE, 2010."
REFERENCES,0.43666666666666665,"David Ha, Andrew Dai, and Quoc V Le. Hypernetworks. arXiv preprint arXiv:1609.09106, 2016."
REFERENCES,0.44,"Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner.
beta-vae: Learning basic visual concepts with a
constrained variational framework. 2016."
REFERENCES,0.44333333333333336,"Jindong Jiang, Sepehr Janghorbani, Gerard De Melo, and Sungjin Ahn. Scalor: Generative world
models with scalable object representations. In International Conference on Learning Represen-
tations, 2020. URL https://openreview.net/forum?id=SJxrKgStDH."
REFERENCES,0.44666666666666666,"Rishabh Kabra, Daniel Zoran, Goker Erdogan, Loic Matthey, Antonia Creswell, Matthew Botvinick,
Alexander Lerchner, and Christopher P Burgess. Simone: View-invariant, temporally-abstracted
object representations via unsupervised video decomposition. arXiv preprint arXiv:2106.03849,
2021."
REFERENCES,0.45,Published as a conference paper at ICLR 2022
REFERENCES,0.4533333333333333,"Nitin Kamra, Umang Gupta, and Yan Liu. Deep generative dual memory network for continual
learning. arXiv preprint arXiv:1710.10368, 2017."
REFERENCES,0.45666666666666667,"Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks.
In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 4401–4410, 2019."
REFERENCES,0.46,"Ronald Kemker and Christopher Kanan. Fearnet: Brain-inspired model for incremental learning.
arXiv preprint arXiv:1711.10563, 2017."
REFERENCES,0.4633333333333333,"Hyunjik Kim and Andriy Mnih.
Disentangling by factorising.
In International Conference on
Machine Learning, pp. 2649–2658. PMLR, 2018."
REFERENCES,0.4666666666666667,"David A Klindt, Lukas Schott, Yash Sharma, Ivan Ustyuzhaninov, Wieland Brendel, Matthias
Bethge, and Dylan Paiton.
Towards nonlinear disentanglement in natural data with temporal
sparse coding. In International Conference on Learning Representations, 2021."
REFERENCES,0.47,"Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Daniel
Gordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. AI2-THOR: An Interactive 3D Environment
for Visual AI. arXiv, 2017."
REFERENCES,0.47333333333333333,"Adam R Kosiorek, Hyunjik Kim, Ingmar Posner, and Yee Whye Teh. Sequential attend, infer,
repeat: Generative modelling of moving objects. arXiv preprint arXiv:1806.01794, 2018."
REFERENCES,0.4766666666666667,"Adam R Kosiorek, Sara Sabour, Yee Whye Teh, and Geoffrey E Hinton. Stacked capsule autoen-
coders. arXiv preprint arXiv:1906.06818, 2019."
REFERENCES,0.48,"Philipp Kr¨ahenb¨uhl and Vladlen Koltun. Efﬁcient inference in fully connected crfs with gaussian
edge potentials. Advances in neural information processing systems, 24:109–117, 2011."
REFERENCES,0.48333333333333334,"David Krueger, Chin-Wei Huang, Riashat Islam, Ryan Turner, Alexandre Lacoste, and Aaron
Courville. Bayesian hypernetworks. arXiv preprint arXiv:1710.04759, 2017."
REFERENCES,0.4866666666666667,"Zhiyuan Li, Jaideep Vitthal Murkute, Prashnna Kumar Gyawali, and Linwei Wang.
Progres-
sive learning and disentanglement of hierarchical representations.
In International Confer-
ence on Learning Representations, 2020. URL https://openreview.net/forum?id=
SJxpsxrYPS."
REFERENCES,0.49,"Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis
and machine intelligence, 40(12):2935–2947, 2017."
REFERENCES,0.49333333333333335,"Francesco Locatello, Michael Tschannen, Stefan Bauer, Gunnar R¨atsch, Bernhard Sch¨olkopf, and
Olivier Bachem. Disentangling factors of variations using few labels. In International Confer-
ence on Learning Representations, 2020a. URL https://openreview.net/forum?id=
SygagpEKwB."
REFERENCES,0.49666666666666665,"Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold,
Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot atten-
tion. arXiv preprint arXiv:2006.15055, 2020b."
REFERENCES,0.5,"David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning.
Advances in neural information processing systems, 30:6467–6476, 2017."
REFERENCES,0.5033333333333333,"Jonathan Lorraine and David Duvenaud. Stochastic hyperparameter optimization through hypernet-
works. arXiv preprint arXiv:1802.09419, 2018."
REFERENCES,0.5066666666666667,"Benno L¨uders, Mikkel Schl¨ager, and Sebastian Risi. Continual learning through evolvable neural
turing machines. In Nips 2016 workshop on continual learning and deep networks (cldl 2016),
2016."
REFERENCES,0.51,"Tianyu Ma, Adrian V Dalca, and Mert R Sabuncu. Hyper-convolution networks for biomedical
image segmentation. arXiv preprint arXiv:2105.10559, 2021."
REFERENCES,0.5133333333333333,Published as a conference paper at ICLR 2022
REFERENCES,0.5166666666666667,"F Milletari, N Navab, SA Ahmadi, and V-net. Fully convolutional neural networks for volumetric
medical image segmentation. In Proceedings of the 2016 Fourth International Conference on 3D
Vision (3DV), pp. 565–571."
REFERENCES,0.52,"Tsendsuren Munkhdalai and Hong Yu. Meta networks. In International Conference on Machine
Learning, pp. 2554–2563. PMLR, 2017."
REFERENCES,0.5233333333333333,"Michael Niemeyer and Andreas Geiger. Giraffe: Representing scenes as compositional genera-
tive neural feature ﬁelds. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 11453–11464, 2021."
REFERENCES,0.5266666666666666,"Yuval Nirkin, Lior Wolf, and Tal Hassner. Hyperseg: Patch-wise hypernetwork for real-time seman-
tic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 4061–4070, 2021."
REFERENCES,0.53,"German I Parisi, Jun Tani, Cornelius Weber, and Stefan Wermter. Lifelong learning of spatiotempo-
ral representations with dual-memory recurrent self-organization. Frontiers in neurorobotics, 12:
78, 2018."
REFERENCES,0.5333333333333333,"German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual
lifelong learning with neural networks: A review. Neural Networks, 113:54–71, 2019."
REFERENCES,0.5366666666666666,"Lorenzo Pellegrini, Gabriele Grafﬁeti, Vincenzo Lomonaco, and Davide Maltoni. Latent replay for
real-time continual learning. In 2020 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS), pp. 10203–10209. IEEE, 2020."
REFERENCES,0.54,"F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M. Gross, and A. Sorkine-Hornung. A
benchmark dataset and evaluation methodology for video object segmentation. In Computer Vi-
sion and Pattern Recognition, 2016."
REFERENCES,0.5433333333333333,"Ori Press, Tomer Galanti, Sagie Benaim, and Lior Wolf. Emerging disentanglement in auto-encoder
based unsupervised image content transfer. In International Conference on Learning Representa-
tions, 2019. URL https://openreview.net/forum?id=BylE1205Fm."
REFERENCES,0.5466666666666666,"Sylvestre-Alvise Rebufﬁ, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl:
Incremental classiﬁer and representation learning. In Proceedings of the IEEE conference on
Computer Vision and Pattern Recognition, pp. 2001–2010, 2017."
REFERENCES,0.55,"Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and
David Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d cat-
egory reconstruction. In Proceedings of the IEEE/CVF International Conference on Computer
Vision, pp. 10901–10911, 2021."
REFERENCES,0.5533333333333333,"Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Gerald
Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interfer-
ence. arXiv preprint arXiv:1810.11910, 2018."
REFERENCES,0.5566666666666666,"Andrei A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osin-
dero, and Raia Hadsell.
Meta-learning with latent embedding optimization.
arXiv preprint
arXiv:1807.05960, 2018."
REFERENCES,0.56,"Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative
replay. arXiv preprint arXiv:1705.08690, 2017."
REFERENCES,0.5633333333333334,"Vincent Sitzmann, Eric R Chan, Richard Tucker, Noah Snavely, and Gordon Wetzstein. Metasdf:
Meta-learning signed distance functions. arXiv preprint arXiv:2006.09662, 2020."
REFERENCES,0.5666666666666667,"Yi Tay, Zhe Zhao, Dara Bahri, Donald Metzler, and Da-Cheng Juan. Hypergrid transformers: To-
wards a single model for multiple tasks. In International Conference on Learning Representa-
tions, 2020."
REFERENCES,0.57,"Johannes von Oswald, Christian Henning, Jo˜ao Sacramento, and Benjamin F Grewe. Continual
learning with hypernetworks. arXiv preprint arXiv:1906.00695, 2019."
REFERENCES,0.5733333333333334,Published as a conference paper at ICLR 2022
REFERENCES,0.5766666666666667,"Yu-Xiong Wang, Deva Ramanan, and Martial Hebert. Meta-learning to detect rare objects. In
Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9925–9934,
2019."
REFERENCES,0.58,"Jiajun Wu, Joshua B Tenenbaum, and Pushmeet Kohli. Neural scene de-rendering. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 699–707, 2017."
REFERENCES,0.5833333333333334,"Ning Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue, Yuchen Liang, Jianchao Yang, and Thomas
Huang.
Youtube-vos: A large-scale video object segmentation benchmark.
arXiv preprint
arXiv:1809.03327, 2018."
REFERENCES,0.5866666666666667,"Yanchao Yang, Antonio Loquercio, Davide Scaramuzza, and Stefano Soatto. Unsupervised moving
object detection via contextual information separation. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition, pp. 879–888, 2019."
REFERENCES,0.59,"Yanchao Yang, Yutong Chen, and Stefano Soatto. Learning to manipulate individual objects in an
image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 6558–6567, 2020."
REFERENCES,0.5933333333333334,"Shunyu Yao, Tzu Ming Harry Hsu, Jun-Yan Zhu, Jiajun Wu, Antonio Torralba, William T Freeman,
and Joshua B Tenenbaum. 3d-aware scene manipulation via inverse graphics. arXiv preprint
arXiv:1808.09351, 2018."
REFERENCES,0.5966666666666667,"Sharon Zhou, Eric Zelikman, Fred Lu, Andrew Y. Ng, Gunnar E. Carlsson, and Stefano Ermon.
Evaluating the disentanglement of deep generative models through manifold topology. In Inter-
national Conference on Learning Representations, 2021. URL https://openreview.net/
forum?id=djwS0m4Ft_A."
REFERENCES,0.6,Published as a conference paper at ICLR 2022
REFERENCES,0.6033333333333334,"A
APPENDIX"
REFERENCES,0.6066666666666667,"A.1
ALGORITHM"
REFERENCES,0.61,"Here is the Object Pursuit algorithm we describe in the method section. In this algorithm, we
ﬁrst check if the object is seen according to Eq. 2. If the object is seen, we directly move on to
the next object, otherwise, we start to check if the object can be represented by current bases ac-
cording to Eq. 3. If it can be expressed, we add this object to the object list µ and move on to
the next object, otherwise, we need to train for a new z and update the hypernetwork using loss
function Eq. 4. After training, we start a second redundancy check, with the same criterion Eq. 3
in the ﬁrst check. If the object can be represented by bases, we just add it to the object list µ,
otherwise, we consider it a new base and add it to both base list z and object list µ. The algo-
rithm runs in a loop to simulate that objects are continually showing up and learned by our system."
REFERENCES,0.6133333333333333,Algorithm 1: Object Pursuit
REFERENCES,0.6166666666666667,"Result: A set of object representations µT = {µi}N
i=1, zT = {zi}B
i=1, and the hypernet ψT ,
with T ≥N ≥B
initialization: z0 = µ0 = ∅, ψ0 is randomly initialized;
while t ≤T do"
REFERENCES,0.62,"Sample Dt = {(xj, yt
j)} ∼p(x, yt);
Check if object ot is in µt−1 (parameter τ);
if YES then"
REFERENCES,0.6233333333333333,"µt = µt−1;
zt = zt−1;
ψt = ψt−1;
else"
REFERENCES,0.6266666666666667,"Check if object ot can be represented using zt−1 (parameter τ);
if YES then"
REFERENCES,0.63,"µt = µt−1 S µot;
zt = zt−1;
ψt = ψt−1;
else"
REFERENCES,0.6333333333333333,"Training for zot and ψ′ under the constraint of all objects in µt−1;
ψt = ψ′;
Check if zot can now be approximated by zt−1;
if YES then"
REFERENCES,0.6366666666666667,"µt = µt−1 S µot;
zt = zt−1;
else"
REFERENCES,0.64,"µt = µt−1 S[0, 0, ..., 0, 1];
zt = zt−1 S zot;
end
end
end
end"
REFERENCES,0.6433333333333333,Published as a conference paper at ICLR 2022
REFERENCES,0.6466666666666666,"A.2
ABLATION ON THE SPARSITY CONSTRAINTS"
REFERENCES,0.65,"Table 7: Pursuit dynamics under dif-
ferent L-1 norm coefﬁcients on µ"
REFERENCES,0.6533333333333333,"|α|
0.0
0.1
0.2
0.5"
REFERENCES,0.6566666666666666,"|z|/N
0.45
0.42
0.42
0.40
|µ|/N
0.57
0.55
0.58
0.51
Re
0.11
0.19
0.21
0.08
Rf
0.21
0.15
0.17
0.25
Aµ
0.74
0.75
0.73
0.77"
REFERENCES,0.66,1. L-1 norm coefﬁcient on µ
REFERENCES,0.6633333333333333,"In this ablation study, we aim to understand whether L-1
norm on combination coefﬁcient µ in Eq. 3 affects an object
falls on the manifold or not. From Tab. 7 we can see that
when L-1 norm coefﬁcient |α| change from 0.0 to 0.2 (ab-
solute value), Re increases, which means more object can
be expressed by current bases, causing the number of base
objects (|z|/N) to decrease. It shows that if we properly in-
crease the constraint on µ and promote the sparsity of µ, it
may be easier to ﬁnd a µ that can express the object. How-
ever, if |α| exceeds a certain limit, e.g. 0.5, too much con-
straint added to µ (which makes µ harder to change), ﬁnding
a µ to express an object becomes difﬁcult, thus the number of objects that can be expressed (Re)
decreases, as shown in Tab. 7."
REFERENCES,0.6666666666666666,"We can also see that Rf ﬁrst drops then increases when |α| gets bigger. Our reasoning is that, even
though more objects can not be expressed by the bases at the ﬁrst redundancy check and have to be
learned as new representations when |α| increases, the chances that they are unqualiﬁed or redundant
after training also increase, making the base number continually decreases."
REFERENCES,0.67,2. L-1 norm coefﬁcient on z
REFERENCES,0.6733333333333333,"Table 8: Pursuit dynamics under dif-
ferent L-1 norm coefﬁcient on z"
REFERENCES,0.6766666666666666,"|α|
0.0
0.1
0.2
0.5"
REFERENCES,0.68,"|z|/N
0.46
0.42
0.42
0.39
|µ|/N
0.56
0.58
0.56
0.55
Re
0.12
0.21
0.21
0.24
Rf
0.14
0.18
0.12
0.19
Aµ
0.74
0.72
0.74
0.72"
REFERENCES,0.6833333333333333,"In this ablation study, we focus on how L-1 norm on z af-
fects object pursuit. We change the coefﬁcient α and eval-
uate the pursuit process. As Tab. 8 shows, when |α| in-
creases, the number of objects that can be expressed by
bases (Re) also increases, causing the base number (|z|/N)
to decrease. This is because regularization on z prevents
the object representations from getting too far from each
other, thus making z distribute more uniformly. Since z is
well distributed, it may be easier to ﬁnd a coefﬁcient µ that
can express an object. We also ﬁnd it in our experiment
that when |α| gets bigger, more objects will be considered
as unqualiﬁed due to their low training accuracy, especially when |α| = 0.5. It shows that more
constraints on z may cause it harder to ﬁnd a proper z to represent an object during training, thus
decrease the accuracy. It also explains why the number of learnable objects (|µ|/N) decreases when
|α| change from 0.1 to 0.5."
REFERENCES,0.6866666666666666,"A.3
UNDERSTAND THE BASE REPRESENTATIONS"
REFERENCES,0.69,"Fig. 4 shows some visualization results of unseen objects and the corresponding active base objects.
The results are from the pursuit process. When an unseen object is detected, and if it can be ex-
pressed by the current bases, we then ﬁnd the combination coefﬁcient µ through Eq. 3. In Fig. 4, we
show three examples that the unseen objects can be expressed by base objects, and the combination
coefﬁcient values are shown below the corresponding bases. We show the top 5 bases that have the
highest coefﬁcient value among all the bases."
REFERENCES,0.6933333333333334,"In the ﬁrst example (the 1st row), the base object ’DishSponge 1’ has the highest coefﬁcient value
in expressing a green cup (Cup 3). We conjecture that ’Cup 3’ and ’DishSponge 1’ share a similar
color, and green objects are rare in this set of bases, causing the DishSponge’s coefﬁcient to be the
highest one. ’Dumbell 1’ may share a similar shape with ’Cup 3’, since they are both thin in the
middle and thick in the end. The second example (the 2nd row) shows that if there is a base object
(Bowl 7) that is evidently more similar to the target object (Bowl 10) than other bases, its coefﬁcient
value may be the highest. Here Bowl 7 and Bowl 10 are similar in both color and shape, but they
are different in shape at the bottom of the bowl. In the third example (the 3rd row), the black cup
(Cup 2) and the black pot (Pot 1) share the same color, while the black cup (Cup 2) and the glass
cup (Cup 1) share a similar shape."
REFERENCES,0.6966666666666667,Published as a conference paper at ICLR 2022
REFERENCES,0.7,"Figure 4: Unseen objects expressed by base objects. Unseen objects in the 1st column (Cup 3,
Bowl 10, Cup 2) are represented by base objects in the 2nd to 6th columns with combination co-
efﬁcients shown below the object. Higher coefﬁcient means greater weight or importance in the
combination."
REFERENCES,0.7033333333333334,"These results show that the object representation space learned through object pursuit characterizes
some high-level concepts (e.g. color, shape) that help better understanding the objects. An unseen
object tends to be represented by a base object with similar color or shape. Although in some cases,
understanding the base representations is not as easy as the examples shown in Fig. 4, this regular
pattern is still evident in most cases."
REFERENCES,0.7066666666666667,"A.4
MORE VISUAL RESULTS ON SYNTHETIC DATA"
REFERENCES,0.71,"Fig. 5 shows more segmentation results from the experiments. In these examples (the 1st row and the
2nd row), ’bowl’ and ’kettle’ are two unseen objects which are not in the object list µ and the base list
z. To represent an unseen object, we ﬁx the hypernetwork and try to ﬁnd a combination coefﬁcient
µ∗to express the target object with the bases, according to Eq. 3. The unseen object, which resides
on the manifold (the object space) with latent code z can be segmented by the segmentation network
generated from its representation with the hypernetwork. Note, we test the segmentation network
on the validation dataset."
REFERENCES,0.7133333333333334,"Even if the unseen object is expressed by base objects which may share some similarity with it, the
unseen object can still be segmented accurately from various backgrounds in different positions and
angles, as Fig. 5 shows. It veriﬁes that the learned representations, together with the hypernetwork,
preserve discriminative object-centric knowledge."
REFERENCES,0.7166666666666667,"A.5
REAL-WORLD DATA COLLECTION AND LEARNING"
REFERENCES,0.72,"We propose that our framework object pursuit can be used with autonomous agents that explore
the environment and interact with objects. Through interactions, an agent could learn object-centric
representations, and this is an unsupervised setting since no human annotations are required when
the agent interacts with objects in the physical scene."
REFERENCES,0.7233333333333334,Published as a conference paper at ICLR 2022
REFERENCES,0.7266666666666667,"Figure 5: More segmentation results. The objects showed above are unseen objects that are ex-
pressed by base objects."
REFERENCES,0.73,"Figure 6: Demonstration of obtaining instance masks of manipulated objects through interaction.
1st row: images of a robotic arm manipulating a cube. 2nd row: instance masks obtained from
interaction by motion segmentation and edge reﬁnement."
REFERENCES,0.7333333333333333,"Currently, we perform the major experiments on synthetic data due to the lack of a robot to perform
the data collection in the real world. Moreover, we like to have a proof of concept before carrying
out real-world experiments, which could involve an enormous amount of research funds and effort,
which is out of the scope of our current work. However, we are conﬁdent that transitioning from the
synthetic environment to the real world is highly practical as all the technical components that need
to be used in the real world are ready."
REFERENCES,0.7366666666666667,"In order to evaluate how practical it is for a robot to perform the pursuit process in the real world,
we need to look into two aspects. First, how possible it is to obtain instance masks of the manipulated
object. Second, how efﬁcient the learning can be given the objects that need to be learned. Next, we
demonstrate how it is possible to get the instance masks of a real-world object through interaction.
And we discuss the second aspect in the following section."
REFERENCES,0.74,"A.5.1
COLLECT DATA IN THE REAL-WORLD VIA INTERACTION"
REFERENCES,0.7433333333333333,"In our setting, a robot manipulates only one object at a time and learns its representation. Two cues
can be used to obtain the instance mask of the object being manipulated. Robotic arm localization
and motion. It is easy to know where the robotic arm is within the view through calibration. Also,
motion segmentation is a well-studied topic on real-world objects in the literature (Charig Yang
et al., 2021; Yang et al., 2019). A possible pipeline is to ﬁrst apply motion segmentation on the
images, and this should give the masks of both the robotic arm and the object as they are both
moving. To remove the portion of the robotic arm, the agent can treat its arm as the ﬁrst object to
learn, so that it knows how to segment its arm in the future. By subtracting the arm, it now has the
mask of the manipulated object."
REFERENCES,0.7466666666666667,"Fig. 6 shows an example of getting instance masks from a video of object manipulation without
supervision. In this example, the background in the video is static (but the motion segmentation
algorithm we employed can also work when there is ego-motion), and we ﬁrst segment the cube
together with the robot hand using motion segmentation (Charig Yang et al., 2021). Then we remove
the robot hand from the segmentation mask, using “densecrf” (Kr¨ahenb¨uhl & Koltun, 2011) as
post-processing. As mentioned, there are other substitutes to exclude the hands, such as using a"
REFERENCES,0.75,Published as a conference paper at ICLR 2022
REFERENCES,0.7533333333333333,"speciﬁcally trained segmentation model of the robotic arm. The segmentation process can run at
5 frames per second. And the results show the practicality of obtaining instance masks during
manipulating objects in the real world."
REFERENCES,0.7566666666666667,"A.5.2
REAL-WORLD OBJECT PURSUIT"
REFERENCES,0.76,"To evaluate the efﬁciency and robustness of learning on real-world images or videos, we conduct
experiments on two large-scale real-world datasets, i.e., Youtube VOS (Xu et al., 2018) and CO3D
(Reizenstein et al., 2021)."
REFERENCES,0.7633333333333333,"YouTube-VOS We train and evaluate our framework on the Youtube-VOS dataset, which contains
65 categories. Each category may appear in multiple video sequences, and there may be multiple
instances of the same category in a video sequence. We traverse all the categories, and for each
category, we sample one video sequence from all sequences containing a single instance of this
category to serve as the object we pursuit (Note this is to stimulate the interaction that should happen
in the real world). The number of frames in a video sequence typically ranges from 20 - 36. For
each video sequence, we repeat the frames many times so that the effective dataset size is around
500 samples. For each frame, we apply horizontal ﬂip augmentation and random crop augmentation.
Speciﬁcally, we crop each frame with the same ratio along the x and y-axis, which is a random
number between 0.6 and 1. Finally, we resize the frame to 320x180 (The origin size is 1280x720)."
REFERENCES,0.7666666666666667,"CO3D We also test our framework on CO3D. Since processing the original dataset is too time-
consuming (18,619 objects of 50 MS-COCO categories), we randomly select 285 objects from 8
object classes: apple, banana, backpack, baseball bat, bench, bicycle, book, and bottle as training
objects. The data of each object contains a video sequence that shows different views of the object.
We randomly crop each frame to a square image, then apply horizontal ﬂip augmentation. After aug-
mentation, the effective dataset size is about 200 samples per object. Each frame is at the resolution
256x256."
REFERENCES,0.77,"Other training details. For both datasets, we initialize the hypernet with the model pretrained on
synthetic data, with no initial bases and objects. We set different quality measures τ (0.5, 0.6, 0.7,
and 0.8) to evaluate our framework on real-world data. Other settings are the same as the synthetic
data experiment."
REFERENCES,0.7733333333333333,"Table 9: Pursuit dynamics on YouTube-VOS
dataset by varying τ."
REFERENCES,0.7766666666666666,"τ
0.5
0.6
0.7
0.8"
REFERENCES,0.78,"|z|/N
0.35
0.38
0.38
0.40
|µ|/N
0.48
0.49
0.49
0.51
Re
0.20
0.16
0.14
0.10
Rf
0.30
0.32
0.52
0.58
Aµ
0.72
0.76
0.81
0.84"
REFERENCES,0.7833333333333333,"Table 10: Pursuit dynamics on CO3D dataset
by varying τ."
REFERENCES,0.7866666666666666,"τ
0.5
0.6
0.7
0.8"
REFERENCES,0.79,"|z|/N
0.15
0.21
0.22
0.28
|µ|/N
0.19
0.29
0.29
0.37
Re
0.14
0.24
0.16
0.11
Rf
0.37
0.38
0.55
0.58
Aµ
0.69
0.74
0.82
0.87"
REFERENCES,0.7933333333333333,"Pursuit dynamics. Tab. 9 and Tab. 10 show pursuit dynamics under different τ on YouTube-VOS
and CO3D datasets, respectively. The real-world data experiments share similar patterns with the
synthetic data experiments: when τ gets bigger, Aµ (average segmentation accuracy) and Rf (pro-
portion of unqualiﬁed and redundant objects) gets bigger, while Re (proportion of expressed objects)
ﬁrst increases then decreases. A higher τ excludes objects with poor training accuracy, thus improv-
ing the overall segmentation performance. On the other hand, a higher τ could make an object easier
to be considered as unqualiﬁed, which would be skipped and would not be learned by our system.
The number of bases and qualiﬁed objects both increase with τ, because fewer objects would be
claimed as seen when τ increases, increasing the number of learned objects. The similar patterns
between real-data and synthetic data show that the conclusions we made from Tab. 3 can generalize
to the real-world domain and transfer between different datasets."
REFERENCES,0.7966666666666666,"Learning efﬁciency. The running time of our framework depends on the threshold τ. Generally,
a smaller τ leads to a shorter running time since objects are easier to be expressed by bases or
recognized as seen objects, which will save the time of learning base object representations. In the
synthetic data experiments, τ = 0.5 could ﬁnish running 67 objects in about 10 hours, while τ = 0.8"
REFERENCES,0.8,Published as a conference paper at ICLR 2022
REFERENCES,0.8033333333333333,"needs 1.5 days. In the real-world experiments on youtubeVOS and CO3D datasets, our framework
can learn around 80 objects per day under τ = 0.5 and 40 objects per day under τ = 0.8."
REFERENCES,0.8066666666666666,"Conclusion on real-world practicality. With these experiments, we can conclude that the observa-
tions we make in the synthetic environment transfer to real-world data. Moreover, two key factors
that are directly related to how practical it is to run object pursuit in the real world are checked to
be positive. First, collecting data with object instance masks through interaction in the real world is
practical, as veriﬁed by the effectiveness of the proposed pipeline in the previous section. Second,
the pursuit framework is robust on the real-world data, and the efﬁciency is good enough to perform
learning in the real world. For example, in a house with hundreds of objects, the training can be
ﬁnished within three weeks and requires no human supervision, which is far more time-consuming
and complicated in practice."
REFERENCES,0.81,"A.6
ADDITIONAL RESULTS ON RE-IDENTIFICATION"
REFERENCES,0.8133333333333334,"In section 4.2.1, we demonstrate the impact of the quality measure τ on seen objects and unseen
objects separately during re-identiﬁcation. To further elaborate on the impact of τ, in this section,
we report precision and recall on both seen objects and unseen objects (collectively) on the scale of
all testing objects. For unseen objects, we deﬁne recall as the fraction of correctly identiﬁed unseen
objects among all the unseen objects, and precision is deﬁned as the fraction of correctly identiﬁed
unseen objects among all the objects we identify as unseen. Same for seen objects."
REFERENCES,0.8166666666666667,"Table 11: Re-identiﬁcation: recall and preci-
sion of unseen objects (on all testing objects)."
REFERENCES,0.82,"τ
0.5
0.6
0.7
0.8"
REFERENCES,0.8233333333333334,"recall
0.28
0.40
0.56
0.72
precision
1.0
1.0
1.0
1.0"
REFERENCES,0.8266666666666667,"Table 12: Re-identiﬁcation: recall and preci-
sion of seen objects (on all testing objects)."
REFERENCES,0.83,"τ
0.5
0.6
0.7
0.8"
REFERENCES,0.8333333333333334,"recall
1.0
1.0
1.0
1.0
precision
0.60
0.64
0.71
0.80"
REFERENCES,0.8366666666666667,"Tab. 11 and Tab. 12 report recall and precision on unseen objects and seen objects, taking all testing
objects into consideration. We collect the number of objects our model identiﬁes as seen or unseen
from the re-identiﬁcation experiment introduced in Section 4.2.1, then compute recall and precision.
From these two tables, the precision of unseen objects and the recall of seen objects are 1.0 under
all τ, which shows that a seen object can always be identiﬁed correctly. This is crucial to our
framework: if a seen object is not identiﬁed as seen, it would be problematic since we have to learn
that object repeatedly, unlimitedly enlarging the object list µ. On the other hand, the recall of unseen
objects and the precision of seen objects are lower than 1.0, showing that some unseen objects could
be identiﬁed as seen. By visualizing the data (Fig. 3), we ﬁnd this situation happens only when
the unseen object has substantial similarities., e.g., in terms of color or shape, to a seen object. It
has no signiﬁcant impact on the training process, since these “seen but actually unseen” objects
contribute very little information to the learned object representations. Therefore, our model learns
object representations in a way that prevents learning the same or highly similar objects, improving
efﬁciency."
REFERENCES,0.84,"We also ﬁnd that the recall of unseen objects and the precision of seen objects get larger when τ
gets bigger. It is because a larger τ increases the bar of determining an object as seen, according to
Eq. 2. Therefore, a difference would make the model consider two as different objects. As τ gets
bigger, the learned representation becomes more discriminative, at the cost of generalization, which
is a characteristic that allows us to tune the system depending on the actual need."
REFERENCES,0.8433333333333334,"A.7
EVALUATION ON BASE AND NON-BASE OBJECTS"
REFERENCES,0.8466666666666667,"Tab. 3 shows pursuit dynamics, including segmentation accuracy and re-identiﬁcation rate with dif-
ferent τ for base and non-base objects. We take all objects in the object list µ into consideration.
However, these objects are added to the object list in two different ways: some can be expressed by
current bases (non-base), while others are trained and accepted as a new base (base). For the former,
we simply ﬁnd combination coefﬁcients µ without updating the hypernet, optimizing the model in
much smaller parameter space than the latter. This may cause different pursuit dynamics in these"
REFERENCES,0.85,Published as a conference paper at ICLR 2022
REFERENCES,0.8533333333333334,"two situations. This section analyzes the segmentation accuracy and re-identiﬁcation rate to promote
the understanding on this aspect."
REFERENCES,0.8566666666666667,"Table
13:
Segmentation
accuracy
and
re-
identiﬁcation rate on base and non-base objects."
REFERENCES,0.86,"τ
0.5
0.6
0.7
0.8"
REFERENCES,0.8633333333333333,"non-base
Aµ
0.65
0.69
0.73
N/A
Rreid
1.0
1.0
1.0
N/A"
REFERENCES,0.8666666666666667,"base
Aµ
0.78
0.82
0.84
0.86
Rreid
1.0
1.0
1.0
1.0"
REFERENCES,0.87,"As reported in Tab. 13, for both base and
non-base objects, Aµ shows the average seg-
mentation accuracy, and Rreid shows the
proportion of the objects that are correctly
re-identiﬁed as seen objects. We report Aµ
and Rreid under different τ. For τ = 0.8,
we found that all objects that appeared in the
pursuit sequence could not be expressed by
bases at ﬁrst and would be learned as a new
base, since the threshold τ is too high for an
object to be considered as being expressed
by bases. In this case, we could not compute
Aµ and Rreid for non-base objects. For other τ, the re-identiﬁcation rate Rreid stays stably at 1.0,
showing that if a base or non-base object has been encountered, it could be re-identiﬁed correctly.
Tab. 13 shows that Aµ of both base and non-base objects increases when τ gets bigger, which shares
a similar pattern with Tab. 3."
REFERENCES,0.8733333333333333,"We also ﬁnd that Aµ on non-base objects could be lower than Aµ on base objects. It indicates
that updating the hypernet and training as a new base can perform better than simply combining
bases due to a larger degree of freedom. This difference can be reduced by increasing the capacity
of the hypernetwork. In conclusion, the re-identiﬁcation performance is stable and accurate on
both base and non-base objects, and the segmentation accuracy increases with τ. Furthermore, the
segmentation accuracy of base objects is generally higher than non-base objects."
REFERENCES,0.8766666666666667,"A.8
ONE-SHOT LEARNING ON REAL DATA"
REFERENCES,0.88,Table 14: Jaccard index on DAVIS evaluation set.
REFERENCES,0.8833333333333333,"Measure
ObjP. (Ours)
OSVOS
SEA
HVS
JMP"
REFERENCES,0.8866666666666667,"J Mean ↑
64.5
62.1
50.4
54.6
57.0
J Recall ↑
75.6
69.7
53.1
61.4
62.6
J Decay ↓
21.2
27.6
36.4
23.6
39.4"
REFERENCES,0.89,"We perform one-shot learning on
the DAVIS 2016 dataset (Per-
azzi et al., 2016), a video object
segmentation dataset in the real
scene. Under the one-shot learn-
ing scheme, we ﬁx the hypernet
and the bases, initialize the combi-
nation coefﬁcient µ with only one
training sample (ﬁrst frame in the sequence). From µ, we can get the representation z for the train-
ing object, generate the parameters of a segmentation network using the hypernet, then evaluate the
segmentation accuracy. We ﬁrst conduct pre-training on training dataset, acquiring an updated hy-
pernet and new bases. We then test one-shot learning on the DAVIS evaluation set, which contains
20 video sequences. We use the ﬁrst frame as initialization and evaluate the rest frames. Finally, we
compare our results with a set of semi-supervised video object segmentation works on the DAVIS
benchmark, using the Jaccard index (IoU) as the evaluation criterion."
REFERENCES,0.8933333333333333,"Table 15: The number of optimizable parameters
and average time consumed per object in one-
shot learning."
REFERENCES,0.8966666666666666,"Measure
ObjP. (Ours)
OSVOS"
REFERENCES,0.9,"# of Parameters
52
5,426,529
Learning Time (s)
10
90"
REFERENCES,0.9033333333333333,"Tab. 14 shows the quantitative evaluation on
DAVIS. We report Jaccard Mean (average Jac-
card score for all test objects), Jaccard Recall
(average Jaccard score for test objects whose
score is higher than 50.0), and Jaccard Decay
(evaluate the Jaccard score decay by time). We
compare our framework with video object seg-
mentation methods that use both appearance and
temporal information, such as SEA (Avinash Ra-
makanth & Venkatesh Babu, 2014), HVS (Grundmann et al., 2010) and JMP (Fan et al., 2015). We
also compare our work with OSVOS (Caelles et al., 2017), a recent work of one-shot learning on
video without temporal information. To make a fair comparison, we implement OSVOS using the
same structure as the primary network in our framework, and remove the post-processing. As shown
in Tab. 14, our method outperforms these four methods on both Jaccard mean and recall. Speciﬁc
experiment settings are described in A.9. Generally speaking, our method is comparable to the state-"
REFERENCES,0.9066666666666666,Published as a conference paper at ICLR 2022
REFERENCES,0.91,"Figure 7: Visualization results of one-shot learning on DAVIS dataset. 1st column: training data
that contains only one data sample. 2nd to 6th column: results on testing set (the rest frames in the
video clip)."
REFERENCES,0.9133333333333333,"of-the-art on one-shot object segmentation learning, though there is enough space for us to push the
performance."
REFERENCES,0.9166666666666666,"Furthermore, in terms of learning efﬁciency, our method performs much better, as shown in Tab. 15.
Since we only need to optimize the coefﬁcient µ with the size of |z|, the number of optimizable
parameters of our method is much smaller than that of OSVOS. So our framework is more efﬁcient
in storage if the trained networks need to be transmitted and used elsewhere. Another advantage of
our framework is that the one-shot learning is much faster, which shows the potential of using our
framework for real-time applications."
REFERENCES,0.92,"Fig. 7 shows some visualization results of one-shot learning on the DAVIS dataset. Although only
one sample is provided during one-shot training, our model can predict the masks on subsequent
frames. In some sequences, for example, the ’dog’ sequence showed in the 4th row of Fig. 7, the
viewing angle and the object’s shape vary signiﬁcantly between frames, making it challenging to
predict subsequent frames only based on the ﬁrst frame without any object prior. It could be inferred
from the results that our model contains useful object-centric priors that help segment objects in
subsequent frames."
REFERENCES,0.9233333333333333,"A.9
DETAILS OF REAL-WORLD ONE-SHOT LEARNING"
REFERENCES,0.9266666666666666,"To achieve better performance on one-shot learning tasks, we explore one-shot learning on real-
world datasets. We ﬁnd several critical factors that would signiﬁcantly affect the one-shot learning
performance from these experiments. Section A.8 shows the ﬁnal result we acquire on one-shot
learning. In this section, we’ll show how we reach the ﬁnal score step by step."
REFERENCES,0.93,"A.9.1
PRETRAINING: STREAMED LEARNING V.S. JOINT LEARNING"
REFERENCES,0.9333333333333333,"We propose two ways to learn objects’ representation: streamed learning and joint learning. In
streamed learning, objects are learned one by one in a certain order, as the algorithm algo.1 shows.
To prevent catastrophic forgetting problems in streamed learning, we add memory constraints with
a coefﬁcient β, as Eq. 4 shows. In joint learning, objects are learned together. Speciﬁcally, suppose"
REFERENCES,0.9366666666666666,Published as a conference paper at ICLR 2022
REFERENCES,0.94,"we have n objects in total, we jointly update the hypernet and n representations (z) during training.
Each mini-batch of training data contains data of a randomly selected object. Different objects
appear alternately, and the representation z corresponds to the current object is updated. Forgetting
problems would not occur in joint training."
REFERENCES,0.9433333333333334,"In this experiment, the hypernet predicts the parameters of the whole segmentation network. We
train it on the DAVIS training set, which contains 30 objects. The experiment found that ’joint
learning’ reaches higher average segmentation accuracy (52.96) than streamed learning (49.72).
There’re two possible reasons. First, we ﬁnd that the average validation accuracy of joint training
(85.6) is higher than streamed learning (81.2), suggesting that joint learning acquires better base
representations that would help represent a novel object. Second, although the memory constraint
of streamed learning helps the model remember previous knowledge, it also restricts the searching
range, which would cause negative effects on ﬁnding representation for a novel object. We’ll use
joint learning for pre-training in the following experiments to reach better performance."
REFERENCES,0.9466666666666667,"A.9.2
NETWORK ARCHITECTURE"
REFERENCES,0.95,"As the experiment section 4.1 introduces, we use an encoder-decoder network as the primary net-
work (segmentation network). In the last experiment (A.9.1), the hypernetwork predicts the parame-
ters of both the encoder and the decoder. However, there’s another option: the hypernetwork predicts
the parameters of the decoder only. In this setting, the encoder updates with the hypernetwork in
pre-training, and is ﬁxed in one-shot learning."
REFERENCES,0.9533333333333334,"In this experiment, we ﬁnd that such architecture differences would signiﬁcantly affect the one-shot
performance. For pre-training, we use ’joint learning’, as we mention in A.9.1. Other settings are the
same as A.9.1. When the hypernetwork predicts the decoder only, we ﬁnd that the test score (58.95)
is much higher than predicting the whole network (52.96). In our implementation, the encoder
has more network parameters than the corresponding hypernet that generates its parameters. The
hypernet could only generate a subset of all possible parameters of the encoder. The limitation of
the searching range in parameter space may cause the output result to fall into local extrema. Since
the encoders under these two settings are both ﬁxed during one-shot learning and share the same
structure, we assume that an independently trained encoder that is not predicted by the hypernet can
better extract useful high-level features for the downstream processing. We’ll use this setting in the
following experiments."
REFERENCES,0.9566666666666667,"A.9.3
PRETRAIN DATA"
REFERENCES,0.96,"In our previous experiments, we train the hypernet and object representations on the DAVIS training
set, which contains 30 objects only. In this section, we expand our training dataset with Youtube
VOS (Xu et al., 2018). In total, we train our network with 694 real-world objects; each object
contains a 60-90 frames video sequence. Compared to the previous test score (58.95), training on our
extended dataset reaches a much higher score (64.45). When our model is trained on more objects,
it learns more prior knowledge about objects, thus performing better on the one-shot learning task.
Training with more objects would help our model better discriminate the shared object properties
that could be learned by the hypernet and the independent properties that should be saved in the
object’s representation z."
REFERENCES,0.9633333333333334,"Table 16: One-shot learning accuracy and training efﬁciency.
One-shot training is performed by searching the optimal repre-
sentation either on the manifold spanned by the base objects, or
over the entire representation space."
REFERENCES,0.9666666666666667,"base representation
full representation space"
REFERENCES,0.97,"J Mean ↑
64.45
48.48
Learning Time (s) ↓
12
103"
REFERENCES,0.9733333333333334,"We also conduct experiments
to evaluate the effect of the
bases. In previous experiments,
we represent novel objects over
base object representations; in
this part, we also perform learn-
ing of the object representa-
tions over the entire representa-
tion space. Tab.16 shows that
searching over base objects representation space performs much better than searching over full rep-
resentation space on both testing accuracy and training efﬁciency. We show a similar pattern in
4.2.3. In this experiment, we construct the base representation space through joint training on 694
object data. In the ’full representation space’ setting, we use the same hypernet as the ’over base"
REFERENCES,0.9766666666666667,Published as a conference paper at ICLR 2022
REFERENCES,0.98,"representation’ setting. The result probably suggests that the base representation space, as a sub-
space of the full representation space, contains objects’ prior knowledge and shares some common
information about objects. When a novel object comes, it is more likely to accurately represent it
in this sub-space than in the full representation space. Searching over the full representation space
would make the searching process slower and possibly fall into local maxima."
REFERENCES,0.9833333333333333,"A.9.4
FINAL RESULT"
REFERENCES,0.9866666666666667,Table 17: Jaccard index on DAVIS evaluation set.
REFERENCES,0.99,"Measure
ObjP.(Origin)
ObjP.(Now)"
REFERENCES,0.9933333333333333,"J Mean ↑
49.7
64.5
J Recall ↑
62.3
75.6
J Decay ↓
29.4
21.2"
REFERENCES,0.9966666666666667,"The ﬁnal result is shown in Tab.17.
The best score is acquired when we
pre-train our model on 694 objects
(YoutubeVOS + DAVIS) using joint
training, and the hypernet predicts the
parameters of the decoder only. The
Jaccard score increases from 49.7 to
64.5 by modifying the critical factors."
