Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0017793594306049821,"Finding the values of model parameters from data is an essential task in science.
1"
ABSTRACT,0.0035587188612099642,"While iterative optimization algorithms like BFGS can find solutions to inverse
2"
ABSTRACT,0.005338078291814947,"problems with machine precision for simple problems, their reliance on local in-
3"
ABSTRACT,0.0071174377224199285,"formation limits their effectiveness for complex problems involving local minima,
4"
ABSTRACT,0.008896797153024912,"chaos, or zero-gradient regions. This study explores the potential for overcoming
5"
ABSTRACT,0.010676156583629894,"these limitations by jointly optimizing multiple examples. To achieve this, we
6"
ABSTRACT,0.012455516014234875,"employ neural networks to reparameterize the solution space and leverage the
7"
ABSTRACT,0.014234875444839857,"training procedure as an alternative to classical optimization. This approach is as
8"
ABSTRACT,0.01601423487544484,"versatile as traditional optimizers and does not require additional information about
9"
ABSTRACT,0.017793594306049824,"the inverse problems, meaning it can be added to existing general-purpose opti-
10"
ABSTRACT,0.019572953736654804,"mization libraries. We evaluate the effectiveness of this approach by comparing it
11"
ABSTRACT,0.021352313167259787,"to traditional optimization on various inverse problems involving complex physical
12"
ABSTRACT,0.023131672597864767,"systems, such as the incompressible Navier-Stokes equations. Our findings reveal
13"
ABSTRACT,0.02491103202846975,"significant improvements in the accuracy of the obtained solutions.
14"
INTRODUCTION,0.026690391459074734,"1
Introduction
15"
INTRODUCTION,0.028469750889679714,"Estimating model parameters by solving inverse problems [Tar05] is a central task in scientific
16"
INTRODUCTION,0.030249110320284697,"research, from detecting gravitational waves [GH18] to controlling plasma flows [MLA+19] to
17"
INTRODUCTION,0.03202846975088968,"searching for neutrinoless double-beta decay [AAA+13, AAA+18]. Iterative optimization algorithms,
18"
INTRODUCTION,0.033807829181494664,"such as limited-memory BFGS [LN89] or Gauss-Newton [GM78], are often employed for solving
19"
INTRODUCTION,0.03558718861209965,"unconstrained parameter estimation problems [PTVF07]. These algorithms offer advantages such
20"
INTRODUCTION,0.037366548042704624,"as ease of use, broad applicability, quick convergence, and high accuracy, typically limited only
21"
INTRODUCTION,0.03914590747330961,"by noise in the observations and floating point precision. However, they face several fundamental
22"
INTRODUCTION,0.04092526690391459,"problems that are rooted in the fact that these algorithms rely on local information, i.e., objective
23"
INTRODUCTION,0.042704626334519574,"values L(xk) and derivatives close to the current solution estimate xk, such as the gradient ∂L/∂x|xk
24"
INTRODUCTION,0.04448398576512456,"and the Hessian matrix ∂2L/∂x2|xk. Acquiring non-local information can be done in low-dimensional
25"
INTRODUCTION,0.046263345195729534,"solution spaces, but the curse of dimensionality prevents this approach for high-dimensional problems.
26"
INTRODUCTION,0.04804270462633452,"These limitations lead to poor performance or failure in various problem settings:
27"
INTRODUCTION,0.0498220640569395,"• Local optima attract the optimizer in the absence of a counter-acting force. Although using a
28"
INTRODUCTION,0.051601423487544484,"large step size or adding momentum to the optimizer can help to traverse small local minima,
29"
INTRODUCTION,0.05338078291814947,"local optimizers are fundamentally unable to avoid this issue.
30"
INTRODUCTION,0.05516014234875445,"• Flat regions can cause optimizers to become trapped along one or multiple directions.
31"
INTRODUCTION,0.05693950177935943,"Higher-order solvers can overcome this issue when the Hessian only vanishes proportionally
32"
INTRODUCTION,0.05871886120996441,"with the gradient, but all local optimizers struggle in zero-gradient regions.
33"
INTRODUCTION,0.060498220640569395,"• Chaotic regions, characterized by rapidly changing gradients, are extremely hard to optimize.
34"
INTRODUCTION,0.06227758007117438,"Iterative optimizers typically decrease their step size to compensate, which prevents the
35"
INTRODUCTION,0.06405693950177936,"optimization from progressing on larger scales.
36"
INTRODUCTION,0.06583629893238434,"In many practical cases, a set of observations is available, comprising many individual parameter
37"
INTRODUCTION,0.06761565836298933,"estimation problems, e.g., when repeating experiments multiple times or collecting data over a time
38"
INTRODUCTION,0.0693950177935943,"frame [CCC+19, DJO+18, GH18, AAA+13, MAL13] and, even in the absence of many recorded
39"
INTRODUCTION,0.0711743772241993,"samples, synthetic data can be generated to supplement the data set. Given such a set of inverse
40"
INTRODUCTION,0.07295373665480427,"problems, we pose the question: Can we find better solutions xi to general inverse problems by
41"
INTRODUCTION,0.07473309608540925,"optimizing them jointly instead of individually, without requiring additional information about the
42"
INTRODUCTION,0.07651245551601424,"problems?
43"
INTRODUCTION,0.07829181494661921,"To answer this question, we employ neural networks to formulate a joint optimization problem.
44"
INTRODUCTION,0.0800711743772242,"Neural networks as general function approximators are a natural and straightforward way to enable
45"
INTRODUCTION,0.08185053380782918,"joint optimization of multiple a priori independent examples. They have been extensively used in
46"
INTRODUCTION,0.08362989323843416,"the field of machine learning [GBCB16], and a large number of network architectures have been
47"
INTRODUCTION,0.08540925266903915,"developed, from multilayer perceptrons (MLPs) [Hay94] to convolutional networks (CNNs) [KSH12]
48"
INTRODUCTION,0.08718861209964412,"to transformers [VSP+17]. Overparameterized neural network architectures typically smoothly
49"
INTRODUCTION,0.08896797153024912,"interpolate the training data [BHM18, BPL21], allowing them to generalize, i.e., make predictions
50"
INTRODUCTION,0.09074733096085409,"about data the network was not trained on.
51"
INTRODUCTION,0.09252669039145907,"It has recently been shown that this generalization capability or inductive bias benefits the optimization
52"
INTRODUCTION,0.09430604982206406,"of individual problems with grid-like solution spaces by implicitly adding a prior to the optimization
53"
INTRODUCTION,0.09608540925266904,"based on the network architecture [UVL18, HSDG19]. However, these effects have yet to be
54"
INTRODUCTION,0.09786476868327403,"investigated for general inverse problems or in the context of joint optimization. We propose using
55"
INTRODUCTION,0.099644128113879,"the training process of a neural network as a drop-in component for traditional optimizers like BFGS
56"
INTRODUCTION,0.10142348754448399,"without requiring additional data, configuration, or tuning. Instead of making predictions about new
57"
INTRODUCTION,0.10320284697508897,"data after training, our objective is to solve only the problems that are part of the training set, i.e.,
58"
INTRODUCTION,0.10498220640569395,"the training itself produces the solutions to the inverse problems, and the network is never used for
59"
INTRODUCTION,0.10676156583629894,"inference. These solutions can also be combined with an iterative optimizer to improve accuracy.
60"
INTRODUCTION,0.10854092526690391,"Unlike related machine learning applications [KAT+19, SGGP+20, SFK+21, RT21, SHT22, HKT21,
61"
INTRODUCTION,0.1103202846975089,"SF18, RPM20, ALGS+22], where a significant goal is accelerating time-intensive computations, we
62"
INTRODUCTION,0.11209964412811388,"accept a higher computational demand if the resulting solutions are more accurate.
63"
INTRODUCTION,0.11387900355871886,"To quantify the gains in accuracy that can be obtained, we compare this approach to classical
64"
INTRODUCTION,0.11565836298932385,"optimization as well as related techniques on four experiments involving difficult inverse problems:
65"
INTRODUCTION,0.11743772241992882,"(i) a curve fit with many local minima, (ii) a billiards-inspired rigid body simulation featuring zero-
66"
INTRODUCTION,0.11921708185053381,"gradient areas, (iii) a chaotic system governed by the Kuramoto–Sivashinsky equation and (iv) an
67"
INTRODUCTION,0.12099644128113879,"incompressible fluid system that is only partially observable. We compare joint optimization to direct
68"
INTRODUCTION,0.12277580071174377,"iterative methods and related techniques in each experiment.
69"
RELATED WORK,0.12455516014234876,"2
Related work
70"
RELATED WORK,0.12633451957295375,"Neural networks have become popular tools to model physical processes, either completely replac-
71"
RELATED WORK,0.12811387900355872,"ing physics solvers [KAT+19, SGGP+20, SFK+21, RT21] or improving them [TSSP17, UBF+20,
72"
RELATED WORK,0.1298932384341637,"KSA+21]. This can improve performance since network evaluations and solvers may be run at lower
73"
RELATED WORK,0.13167259786476868,"resolution while maintaining stability and accuracy. Additionally, it automatically yields a differen-
74"
RELATED WORK,0.13345195729537365,"tiable forward process which can then be used to solve inverse problems [SF18, RPM20, ALGS+22],
75"
RELATED WORK,0.13523131672597866,"similar to how style transfer optimizes images [GEB16].
76"
RELATED WORK,0.13701067615658363,"Alternatively, neural networks can be used as regularizers to solve inverse problems on sparse tomog-
77"
RELATED WORK,0.1387900355871886,"raphy data [LSAH20] or employed recurrently for image denoising and super-resolution [PW17].
78"
RELATED WORK,0.14056939501779359,"Recent works have also explored them for predicting solutions to inverse problems [HKT21, SHT22].
79"
RELATED WORK,0.1423487544483986,"In these settings, neural networks are trained offline and then used to infer solutions to new inverse
80"
RELATED WORK,0.14412811387900357,"problems, eliminating the iterative optimization process at test time.
81"
RELATED WORK,0.14590747330960854,"Underlying many of these approaches are differentiable simulations required to obtain gradients of
82"
RELATED WORK,0.14768683274021352,"the inverse problem. These can be used in iterative optimization or to train neural networks. Many
83"
RELATED WORK,0.1494661921708185,"recent software packages have demonstrated this use of differentiable simulations, with general
84"
RELATED WORK,0.1512455516014235,"frameworks [HAL+20, SC19, HKT20] and specialized simulators [TLQL21, LLK19].
85"
RELATED WORK,0.15302491103202848,"Physics-informed neural networks [RPK19] encode solutions to optimization problems in the network
86"
RELATED WORK,0.15480427046263345,"weights themselves. They model a continuous solution to an ODE or PDE and are trained by
87"
RELATED WORK,0.15658362989323843,"formulating a loss function based on the differential equation, and have been explored for a variety
88"
RELATED WORK,0.1583629893238434,"of directions [YZWX19, LPY+21, KGZ+21]. However, as these approaches rely on loss terms
89"
RELATED WORK,0.1601423487544484,"formulated with neural network derivatives, they do not apply to general inverse problems.
90"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.1619217081850534,"3
Reparameterizing inverse problems with neural networks
91"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.16370106761565836,"We consider a set of n similar inverse problems where we take similar to mean we can express all of
92"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.16548042704626334,"them using a function F(ξi | xi) conditioned on a problem-specific vector xi with i = 1, ..., n. Each
93"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.16725978647686832,"inverse problem then consists of finding optimal parameters ξ∗
i such that a desired or observed output
94"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.16903914590747332,"yi is reproduced, i.e.
95"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.1708185053380783,"ξ∗
i = arg minξiL(F(ξi | xi), yi),
(1)"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.17259786476868327,"where L denotes an error measure, such as the squared L2 norm || · ||2
2. We assume that F is
96"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.17437722419928825,"differentiable and can be approximately simulated, i.e., the observed output yi may not be reproducible
97"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.17615658362989323,"exactly using F due to hidden information or stochasticity.
98"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.17793594306049823,"A common approach to finding ξ∗
i is performing a nonlinear optimization, minimizing L using
99"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.1797153024911032,the gradients ∂L
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.18149466192170818,"∂F
∂F
∂ξi . In strictly convex optimization, many optimizers guarantee convergence to
100"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.18327402135231316,"the global optimum in these circumstances. However, when considering more complex problems,
101"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.18505338078291814,"generic optimizers often fail to find the global optimum due to local optima, flat regions, or chaotic
102"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.18683274021352314,"regions. Trust region methods [Yua00] can be used on low-dimensional problems but scale poorly to
103"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.18861209964412812,"higher-dimensional problems. Without further domain-specific knowledge, these methods are limited
104"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.1903914590747331,"to individually optimizing all n inverse problems.
105"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.19217081850533807,"Instead of improving the optimizer itself, we want to investigate whether better solutions can be found
106"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.19395017793594305,"by jointly optimizing all problems. However, without domain-specific knowledge, it is unknown
107"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.19572953736654805,"which parameters of ξi are shared among multiple problems. We therefore first reparameterize the
108"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.19750889679715303,"full solution vectors ξi using a set of functions ˆξi, setting ξi ≡ˆξi(θ) where θ represents a set of
109"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.199288256227758,"shared parameters. With this change, the original parameters ξi become functions of θ, allowing θ
110"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.20106761565836298,"to be jointly optimized over all problems. Here, the different ˆξi can be considered transformation
111"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.20284697508896798,"functions mapping θ to the actual solutions ξi, similar to transforming Cartesian to polar coordinates.
112"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.20462633451957296,"Second, we sum the errors of all examples to define the overall objective function L = Pn
i=1 Li.
113"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.20640569395017794,"For generality, all ˆξi(θ) should be able to approximate arbitrary functions. We implement them as an
114"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.20818505338078291,"artificial neural network N with weights θ:
ˆξi(θ) ≡N(xi, yi | θ). Inserting these changes into Eq. 1
115"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.2099644128113879,"yields the reparameterized optimization problem
116"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.2117437722419929,"ξ∗
i = ˆξi(θ∗) ,
θ∗= argminθ n
X"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.21352313167259787,"i=1
L(F(N(xi, yi | θ) | xi), yi).
(2)"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.21530249110320285,"Net
𝑥𝑖, 𝑦𝑖 𝜃"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.21708185053380782,"Fwd.
ℒ𝑖
𝑦𝑖 + 𝐿 𝜉𝑖"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.2188612099644128,"Figure 1: Reparameterized optimiza-
tion"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.2206405693950178,"We see that the joint optimization with reparameterization
117"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.22241992882562278,"strongly resembles standard formulations of neural network
118"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.22419928825622776,"training where (xi, yi) is the input to the network and F ◦
119"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.22597864768683273,"L represents the effective loss function. However, from
120"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.2277580071174377,"the viewpoint of optimizing inverse problems, the network
121"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.22953736654804271,"is not primarily a function of (xi, yi) but rather a set of
122"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.2313167259786477,"transformation functions of θ, each corresponding to a fixed
123"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.23309608540925267,"and discrete (xi, yi). Figure 1 shows the computational graph
124"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.23487544483985764,"corresponding to Eq. 2.
125"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.23665480427046262,"While the tasks of optimizing inverse problems and learning
126"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.23843416370106763,"patterns from data may seem unrelated at first, there is a
127"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.2402135231316726,"strong connection between the two. The inductive bias of a chosen network architecture, which
128"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.24199288256227758,"enables generalization, also affects the update direction of classical optimizers under reparame-
129"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.24377224199288255,"terization. This can be seen most clearly if we consider gradient descent steps. There, individual
130"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.24555160142348753,optimization yields the updates ∆ξi = −η ∂Li
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.24733096085409254,"∂ξi with η denoting the step size. After reparameterization,
131"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.2491103202846975,"the updates are ∆θ = −η P i
∂Li"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.2508896797153025,"∂ξi
∂N"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.2526690391459075,"∂θ . As we can see, ∂N"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.25444839857651247,"∂θ , which is independent of the specific
132"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.25622775800711745,"example, now contributes a large part to the update direction, allowing for cross-talk between the
133"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.2580071174377224,"different optimization problems.
134"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.2597864768683274,"Despite the similarities to machine learning, the different use case of this setup leads to differences
135"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.2615658362989324,"in the training procedure. For example, while overfitting is usually seen as undesirable in machine
136"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.26334519572953735,"learning, we want the solutions to our inverse problems to be as accurate as possible, i.e. we want to
137"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.26512455516014233,"”overfit” to the data. Consequently, we do not have to worry about the curvature at θ∗and will not use
138"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.2669039145907473,"mini-batches for training the reparameterization network.
139"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.26868327402135234,"Supervised training.
Our main goal is obtaining an optimization scheme that works exactly like
140"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.2704626334519573,"classical optimizers, only requiring the forward process F, xi in the form of a numerical simulator,
141"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.2722419928825623,"and desired outputs yi. However, if we additionally have a prior on the solution space P(ξ), we can
142"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.27402135231316727,"generate synthetic training data {(xj, yi), ξj} with yj = F(xj, ξj) by sampling ξi ∼P(ξ). Using
143"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.27580071174377224,"this data set, we can alternatively train N with the supervised objective
144"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.2775800711743772,"˜L =
X"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.2793594306049822,"j
||N(xj, yj) −ξj||2
2.
(3)"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.28113879003558717,"Since N has the same inputs and outputs, we can use the same network architecture as above
145"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.28291814946619215,"and the solutions to the original inverse problems can be obtained as ξi = N(xi, yi). While this
146"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.2846975088967972,"method requires domain knowledge in the form of the distributions P(x) and P(ξ), it has the distinct
147"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.28647686832740216,"advantage of being independent of the characteristics of F. For example, if F is chaotic, directly
148"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.28825622775800713,"optimizing through F can yield very large and unstable gradients, while the loss landscape of ˜L can
149"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.2900355871886121,"still be smooth. However, we cannot expect the inferred solutions to be highly accurate as the network
150"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.2918149466192171,"is not trained on the inverse problems we want to solve and, thus, has to interpolate. Additionally, this
151"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.29359430604982206,"method is only suited to unimodal problems, i.e. inverse problems with a unique global minimum.
152"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.29537366548042704,"On multimodal problems, the network cannot be prevented from learning an interpolation of possible
153"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.297153024911032,"solutions, which may result in poor accuracy.
154"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.298932384341637,"Refinement
Obtaining a high accuracy on the inverse problems of interest is generally difficult
155"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.30071174377224197,"when the training set size is limited, which can result in suboptimal solutions. This is especially
156"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.302491103202847,"problematic when the global minima are narrow and no direct feedback from F is available, as in the
157"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.304270462633452,"case of supervised training. To ensure that all learned methods have the potential to compete with
158"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.30604982206405695,"gradient-based optimizers like BFGS, we pass the solution estimates for ξ to a secondary refinement
159"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.30782918149466193,"stage where they are used as an initial guess for BFGS. The refinement uses the true gradients of F
160"
REPARAMETERIZING INVERSE PROBLEMS WITH NEURAL NETWORKS,0.3096085409252669,"to find a nearby minimum of L.
161"
EXPERIMENTS,0.3113879003558719,"4
Experiments
162"
EXPERIMENTS,0.31316725978647686,"We perform a series of numerical experiments to test the convergence properties of the reparameterized
163"
EXPERIMENTS,0.31494661921708184,"joint optimization. An overview of the experiments is given in Tab. 1 and additional details of all
164"
EXPERIMENTS,0.3167259786476868,"performed experiments can be found in Appendix B. We run each experiment and method multiple
165"
EXPERIMENTS,0.3185053380782918,"times, varying the neural network initializations and data sets to obtain statistically significant results.
166"
EXPERIMENTS,0.3202846975088968,"To test the capabilities of the algorithms as a black-box extension of generic optimizers, all experi-
167"
EXPERIMENTS,0.3220640569395018,"ments use off-the-shelf neural network architectures and only require hyperparameter tuning in terms
168"
EXPERIMENTS,0.3238434163701068,"of decreasing the Adam [KB15] learning rate until stable convergence is reached. We then compare
169"
EXPERIMENTS,0.32562277580071175,"the reparameterized optimization to BFGS [LN89], a popular classical solver for unconstrained
170"
EXPERIMENTS,0.3274021352313167,"optimization problems, and to the neural adjoint method, which has been shown to outperform
171"
EXPERIMENTS,0.3291814946619217,"various other neural-network-based approaches for solving inverse problems [RPM20].
172"
EXPERIMENTS,0.3309608540925267,"Neural adjoint
The neural adjoint method relies on an approximation of the forward process
173"
EXPERIMENTS,0.33274021352313166,"by a surrogate neural network S(xi, ξi | θ). We first train the surrogate on an independent data set
174"
EXPERIMENTS,0.33451957295373663,Table 1: Overview of numerical experiments.
EXPERIMENTS,0.33629893238434166,"Experiment
∇= 0 areas
Chaotic
xi known
P(ξ) known"
EXPERIMENTS,0.33807829181494664,"Wave packet localization
No
No
No
Yes
Billiards
Yes
No
Yes
No
Kuramoto–Sivashinsky
No
Yes
Yes
Yes
Incompr. Navier-Stokes
No
Yes
No
Yes"
EXPERIMENTS,0.3398576512455516,"generated from the same distribution as the inverse problems and contains many examples. We use
175"
EXPERIMENTS,0.3416370106761566,"the same examples as for the supervised approach outlined above but switch the labels to match the
176"
EXPERIMENTS,0.34341637010676157,"network design, {(xi, ξi), yi}. After training, the weights θ are frozen and BFGS is used to optimize
177"
EXPERIMENTS,0.34519572953736655,"ξi on the proxy process ˜F(ξi | xi) = S(ξi, xi) + B(ξi) where B denotes a boundary loss term (see
178"
EXPERIMENTS,0.3469750889679715,"Appendix A). With the loss function L from Eq. 1, this yields the effective objective L(F(ξi | xi), yi)
179"
EXPERIMENTS,0.3487544483985765,"for solving the inverse problems. Like with the other methods, the result of the surrogate optimization
180"
EXPERIMENTS,0.3505338078291815,"is then used as a starting point for the refinement stage described above.
181"
WAVE PACKET LOCALIZATION,0.35231316725978645,"4.1
Wave packet localization
182"
WAVE PACKET LOCALIZATION,0.3540925266903915,"First, we consider a 1D curve fit. A noisy signal u(t) containing a wave packet centered at t0 is
183"
WAVE PACKET LOCALIZATION,0.35587188612099646,"measured, resulting in the observed data u(t) = A · sin(t −t0) · exp(−(t −t0)2/σ2) + ϵ(t) where
184"
WAVE PACKET LOCALIZATION,0.35765124555160144,"ϵ(t) denotes random noise and t = 1, ..., 256. An example waveform is shown in Fig. 2a. For fixed
185"
WAVE PACKET LOCALIZATION,0.3594306049822064,"A and σ, the task is to locate the wave packed, i.e. retrieve t0. This task is difficult for optimization
186"
WAVE PACKET LOCALIZATION,0.3612099644128114,"algorithms because the loss landscape (Fig. 2b) contains many local optima that must be traversed.
187"
WAVE PACKET LOCALIZATION,0.36298932384341637,"This results in alternating gradient directions when traversing the parameter space, with maximum
188"
WAVE PACKET LOCALIZATION,0.36476868327402134,"magnitude near the correct solution.
189"
WAVE PACKET LOCALIZATION,0.3665480427046263,"0
100
200
X 1.0 0.5 0.0 0.5 (a)"
WAVE PACKET LOCALIZATION,0.3683274021352313,Signal
WAVE PACKET LOCALIZATION,0.3701067615658363,"0.2
0.4
0.6
0.8
t0 20 0 20 (b) / 50"
WAVE PACKET LOCALIZATION,0.3718861209964413,"101
102
103 Steps 101"
WAVE PACKET LOCALIZATION,0.3736654804270463,8 × 100
WAVE PACKET LOCALIZATION,0.37544483985765126,9 × 100 Loss (c)
WAVE PACKET LOCALIZATION,0.37722419928825623,"BFGS
Reparameterized
Supervised
Neural Adjoint"
WAVE PACKET LOCALIZATION,0.3790035587188612,"2
8
32
128
Dataset Size 0 2 4 6 8 10 12 (d)"
WAVE PACKET LOCALIZATION,0.3807829181494662,"Figure 2: Wave packet localization. (a) Example waveform u(t), (b) corresponding loss and gradient
landscape for t0, (c) optimization curves without refinement, (d) refined loss L/n by the number of
examples n, mean and standard deviation over multiple network initializations and data sets."
WAVE PACKET LOCALIZATION,0.38256227758007116,"We generate the inverse problems by sampling random t0 and ϵ(t) from ground truth prior distributions
190"
WAVE PACKET LOCALIZATION,0.38434163701067614,"and simulating the corresponding outputs u(t) = Fϵ(t) | t0). Because the noise distribution ϵ(t) is
191"
WAVE PACKET LOCALIZATION,0.3861209964412811,"not available to any of the optimization methods, a perfect solution with L = 0 is impossible.
192"
WAVE PACKET LOCALIZATION,0.3879003558718861,"Fig. 2c shows the optimization process. Iterative optimizers like BFGS get stuck in local minima
193"
WAVE PACKET LOCALIZATION,0.3896797153024911,"quickly on this task. In most examples, BFGS moves a considerable distance in the first iteration and
194"
WAVE PACKET LOCALIZATION,0.3914590747330961,"then quickly halts. However, due to the oscillating gradient directions, this initial step is likely to
195"
WAVE PACKET LOCALIZATION,0.3932384341637011,"propel the estimate away from the global optimum, leading many solutions to lie further from the
196"
WAVE PACKET LOCALIZATION,0.39501779359430605,"actual optimum than the initial guess.
197"
WAVE PACKET LOCALIZATION,0.39679715302491103,"The neural adjoint method finds better solutions than BFGS for about a third of examples for n = 256
198"
WAVE PACKET LOCALIZATION,0.398576512455516,"(see Tab. 2). In many cases, the optimization progresses towards the boundary and gets stuck once
199"
WAVE PACKET LOCALIZATION,0.400355871886121,"the boundary loss B balances the gradients from the surrogate network.
200"
WAVE PACKET LOCALIZATION,0.40213523131672596,"To reparameterize the problem, we create a neural network N that maps the 256 values of the observed
201"
WAVE PACKET LOCALIZATION,0.40391459074733094,"signal u(t) to the unknown value t0. We chose a standard architecture inspired by image classification
202"
WAVE PACKET LOCALIZATION,0.40569395017793597,"networks [SZ14] and train it according to Eq. 2. The network consists of five convolutional layers
203"
WAVE PACKET LOCALIZATION,0.40747330960854095,"with ReLU activation functions, batch normalization, and max-pooling layers, followed by two
204"
WAVE PACKET LOCALIZATION,0.4092526690391459,"fully-connected layers. During the optimization, the estimate of t0 repeatedly moves from minimum
205"
WAVE PACKET LOCALIZATION,0.4110320284697509,"to minimum until settling after around 500 iterations. Like BFGS, most examples do not converge to
206"
WAVE PACKET LOCALIZATION,0.4128113879003559,"the global optimum and stop at a local minimum instead. However, the cross-talk between different
207"
WAVE PACKET LOCALIZATION,0.41459074733096085,"examples, induced by the shared parameters θ and the summation of the individual loss functions,
208"
WAVE PACKET LOCALIZATION,0.41637010676156583,"regularizes the movement in t0 space, preventing solutions from moving far away from the global
209"
WAVE PACKET LOCALIZATION,0.4181494661921708,"optimum. Meanwhile, the feedback from the analytic gradients of F ensures that each example finds
210"
WAVE PACKET LOCALIZATION,0.4199288256227758,"a locally optimal solution. Overall, this results in around 80% of examples finding a better solution
211"
WAVE PACKET LOCALIZATION,0.42170818505338076,"than BFGS.
212"
WAVE PACKET LOCALIZATION,0.4234875444839858,"For supervised training of N, we use the same training data set as for the neural adjoint method. This
213"
WAVE PACKET LOCALIZATION,0.42526690391459077,"approach’s much smoother loss landscape lets all solution estimates progress close to the ground
214"
WAVE PACKET LOCALIZATION,0.42704626334519574,"truth. However, lacking the gradient feedback from the forward process F, the inferred solutions
215"
WAVE PACKET LOCALIZATION,0.4288256227758007,"are slightly off from the actual solution and, since the highest loss values are close to the global
216"
WAVE PACKET LOCALIZATION,0.4306049822064057,"optimum, this raises the overall loss during training even though the solutions are approaching the
217"
WAVE PACKET LOCALIZATION,0.43238434163701067,"global optima. This phenomenon gets resolved with solution refinement using BFGS.
218"
WAVE PACKET LOCALIZATION,0.43416370106761565,"Fig. 2d shows the results for different numbers of inverse problems and training set sizes n. Since
219"
WAVE PACKET LOCALIZATION,0.4359430604982206,"BFGS optimizes each example independently, the data set size has no influence on its performance.
220"
WAVE PACKET LOCALIZATION,0.4377224199288256,"Variances in the mean final loss indicate that the specific selection of inverse problems may be slightly
221"
WAVE PACKET LOCALIZATION,0.4395017793594306,"easier or harder to solve than the average. The neural adjoint method and reparameterized optimization
222"
WAVE PACKET LOCALIZATION,0.4412811387900356,"both perform better than BFGS with the reparameterized optimization producing lower loss values.
223"
WAVE PACKET LOCALIZATION,0.4430604982206406,"However, both do not scale with n in this example. This feature can only be observed with supervised
224"
WAVE PACKET LOCALIZATION,0.44483985765124556,"training whose solution quality noticeably increases with n. This is due to the corresponding increase
225"
WAVE PACKET LOCALIZATION,0.44661921708185054,"in training set size, which allows the model to improve generalization and does not depend on
226"
WAVE PACKET LOCALIZATION,0.4483985765124555,"the number of tested inverse problems. For n ≥32, supervised training in combination with the
227"
WAVE PACKET LOCALIZATION,0.4501779359430605,"above-mentioned solution refinement consistently outperforms all other methods.
228"
WAVE PACKET LOCALIZATION,0.45195729537366547,"A detailed description of the network architecture along with additional learning curves, parameter
229"
WAVE PACKET LOCALIZATION,0.45373665480427045,"evolution plots as well as the performance on further data set sizes n can be found in Appendix B.1.
230"
BILLIARDS,0.4555160142348754,"4.2
Billiards
231"
BILLIARDS,0.45729537366548045,"Next, we consider a rigid-body setup inspired by differentiable billiards simulations of previous work
232"
BILLIARDS,0.45907473309608543,"[HAL+20]. The task consists of finding the optimal initial velocity ⃗v0 of a cue ball so it hits another
233"
BILLIARDS,0.4608540925266904,"ball, imparting momentum in a non-elastic collision to make the second ball come to rest at a fixed
234"
BILLIARDS,0.4626334519572954,"target location. This setup is portrayed in Fig. 3a and the corresponding loss landscape for a fixed x
235"
BILLIARDS,0.46441281138790036,"velocity in Fig. 3b. A collision only occurs if ⃗v0 is large enough and pointed towards the other ball.
236"
BILLIARDS,0.46619217081850534,"Otherwise, the second ball stays motionless, resulting in a constant loss value and ∂L"
BILLIARDS,0.4679715302491103,"∂⃗v0 = 0.
237"
BILLIARDS,0.4697508896797153,"0.0
0.5
1.0
1.5
2.0
X 0.0 0.5 Y Cue Ball"
BILLIARDS,0.47153024911032027,Target (a)
BILLIARDS,0.47330960854092524,"3
2
1
0
Cue Vy 4 2 0 2 4 6 (b)"
BILLIARDS,0.4750889679715303,"101
102
103 Steps 10
2 10
1 100 Loss (c)"
BILLIARDS,0.47686832740213525,"BFGS
Reparameterized
Supervised
Neural Adjoint"
BILLIARDS,0.4786476868327402,"4
16
64
256
Dataset Size 0.0 0.1 0.2 0.3 0.4 (d)"
BILLIARDS,0.4804270462633452,"Figure 3: Billiards experiment. (a) Task: the cue ball must hit the other ball so that it comes to rest
at the target, (b) corresponding loss and gradient landscape for vy, (c) optimization curves without
refinement, (d) refined loss L/n by number of examples n, mean and standard deviation over multiple
network initializations and data sets."
BILLIARDS,0.4822064056939502,"This property prevents classical optimizers from converging if they hit such a region in the solution
238"
BILLIARDS,0.48398576512455516,"space. The optimization curves are shown in Fig. 3c. BFGS only converges for those examples where
239"
BILLIARDS,0.48576512455516013,"the cue ball already hits the correct side of the other ball.
240"
BILLIARDS,0.4875444839857651,"For reparameterization, we employ a fully-connected neural network N with three hidden layers
241"
BILLIARDS,0.4893238434163701,"using Sigmoid activation functions and positional encoding. The joint optimization with N drastically
242"
BILLIARDS,0.49110320284697506,"improves the solutions. While for n ≤32 only small differences to BFGS can be observed, access to
243"
BILLIARDS,0.4928825622775801,"more inverse problems lets gradients from some problems steer the optimization of others that get
244"
BILLIARDS,0.49466192170818507,"no useful feedback. This results in almost all problems converging to the solution for n ≥64 (see
245"
BILLIARDS,0.49644128113879005,"Fig. 3d).
246"
BILLIARDS,0.498220640569395,"In this experiment, the distribution of the solutions P(⃗v0) is not available as hitting the target precisely
247"
BILLIARDS,0.5,"requires a specific velocity ⃗v0 that is unknown a-priori. We can, however, generate training data with
248"
BILLIARDS,0.501779359430605,"varying ⃗v0 and observe the final positions of the balls, then train a supervised N as well as a surrogate
249"
BILLIARDS,0.50355871886121,"network for the neural adjoint method on this data set. However, this is less efficient as most of the
250"
BILLIARDS,0.505338078291815,"examples in the data set do not result in an optimal collision.
251"
BILLIARDS,0.5071174377224199,"The neural adjoint method fails to approach the true solutions and instead gets stuck on the training
252"
BILLIARDS,0.5088967971530249,"data boundary in solution space. Likewise, the supervised model cannot accurately extrapolate the
253"
BILLIARDS,0.5106761565836299,"true solution distribution from the sub-par training set.
254"
BILLIARDS,0.5124555160142349,"4.3
Kuramoto–Sivashinsky equation
255"
BILLIARDS,0.5142348754448398,"The Kuramoto–Sivashinsky (KS) equation, originally developed to model the unstable behavior of
256"
BILLIARDS,0.5160142348754448,"flame fronts [Kur78], models a chaotic one-dimensional system, ˙u(t) = −∂2u"
BILLIARDS,0.5177935943060499,∂x2 −∂4u
BILLIARDS,0.5195729537366548,"∂x4 −u · ∇u. We
257"
BILLIARDS,0.5213523131672598,"consider a two-parameter inverse problem involving the forced KS equation with altered advection
258"
BILLIARDS,0.5231316725978647,"strength,
259"
BILLIARDS,0.5249110320284698,˙u(t) = α · G(x) −∂2u
BILLIARDS,0.5266903914590747,∂x2 −∂4u
BILLIARDS,0.5284697508896797,"∂x4 −β · u · ∇u,"
BILLIARDS,0.5302491103202847,"where G(x) is a fixed time-independent forcing term and α, β ∈R denote the unknown parameters
260"
BILLIARDS,0.5320284697508897,"governing the evolution. Each inverse problem starts from a randomly generated initial state u(t = 0)
261"
BILLIARDS,0.5338078291814946,"and is simulated until t = 25, by which point the system becomes chaotic but is still smooth enough
262"
BILLIARDS,0.5355871886120996,"to allow for gradient-based optimization. We constrain α ∈[−1, 1], β ∈[ 1 2, 3"
BILLIARDS,0.5373665480427047,"2] to keep the system
263"
BILLIARDS,0.5391459074733096,"numerically stable. Fig. 4a shows example trajectories of this setup and the corresponding gradient
264"
BILLIARDS,0.5409252669039146,landscape of ∂L
BILLIARDS,0.5427046263345195,"∂β ∥α=α∗for the true value of α is shown in Fig. 4b.
265"
BILLIARDS,0.5444839857651246,"0
20
40
Time 0 5 10 15 20 X (a)"
BILLIARDS,0.5462633451957295,"3
2
1
0 0 200 400 (b) / 5"
BILLIARDS,0.5480427046263345,"101
102
103 Steps 102 Loss (c)"
BILLIARDS,0.5498220640569395,"BFGS
Reparameterized
Supervised
Neural Adjoint"
BILLIARDS,0.5516014234875445,"4
8
32
128
Dataset Size 0 10 20 30 40 (d) 4 2 0 2 4"
BILLIARDS,0.5533807829181495,"Figure 4: Kuramoto–Sivashinsky experiment. (a) Example trajectory, (b) corresponding loss and
gradient landscape for β, (c) optimization curves without refinement, (d) refined loss L/n by number
of examples n, mean and standard deviation over multiple network initializations and data sets."
BILLIARDS,0.5551601423487544,"Fig. 4c shows the optimization curves for finding α, β. Despite the complex nature of the loss
266"
BILLIARDS,0.5569395017793595,"landscape, BFGS manages to find the correct solution in about 60% of cases. The reparameterized
267"
BILLIARDS,0.5587188612099644,"optimization, based on a similar network architecture as for the wavepacket experiment but utilizing
268"
BILLIARDS,0.5604982206405694,"2D convolutions, finds the correct solutions in over 80% of cases but, without refinement, the accuracy
269"
BILLIARDS,0.5622775800711743,"stagnates far from machine precision. Refining these solutions with BFGS, as described above, sees
270"
BILLIARDS,0.5640569395017794,"the accuracy of these cases decrease to machine precision in 4 to 17 iterations, less than the 12 to 22
271"
BILLIARDS,0.5658362989323843,"that BFGS requires when initialized from the distribution mean E[P(ξ)].
272"
BILLIARDS,0.5676156583629893,"Supervised training with refinement produces better solutions in 58% of examples, averaged over
273"
BILLIARDS,0.5693950177935944,"the shown n. The unrefined solutions benefit from larger n on this example because of the large
274"
BILLIARDS,0.5711743772241993,"number of possible observed outputs that the KS equation can produce for varying α, β. At n = 2,
275"
BILLIARDS,0.5729537366548043,"all unrefined solutions are worse than BFGS while for n ≥64 around 20% of problems find better
276"
BILLIARDS,0.5747330960854092,"solutions. With refinement, these number jump to 50% and 62%.
277"
BILLIARDS,0.5765124555160143,"This property also makes it hard for a surrogate network, required by the neural adjoint method,
278"
BILLIARDS,0.5782918149466192,"to accurately approximate the KS equation, causing the following adjoint optimization to yield
279"
BILLIARDS,0.5800711743772242,"inaccurate results that fail to match BFGS even after refinement.
280"
INCOMPRESSIBLE NAVIER-STOKES,0.5818505338078291,"4.4
Incompressible Navier-Stokes
281"
INCOMPRESSIBLE NAVIER-STOKES,0.5836298932384342,"Incompressible Newtonian fluids are described by the Navier-Stokes equations,
282"
INCOMPRESSIBLE NAVIER-STOKES,0.5854092526690391,"˙u(⃗x, t) = ν∇2u −u · ∇u −∇p
s.t.
∇2p = ∇· v"
INCOMPRESSIBLE NAVIER-STOKES,0.5871886120996441,"with ν ≥0. As they can result in highly complex dynamics [BB67], they represent a particularly
283"
INCOMPRESSIBLE NAVIER-STOKES,0.5889679715302492,"challenging test case, which is relevant for a variety of real-world problems [Pop00]. We consider
284"
INCOMPRESSIBLE NAVIER-STOKES,0.5907473309608541,"a setup similar to particle imaging velocimetry [Gra97] in which the velocity in the upper half of
285"
INCOMPRESSIBLE NAVIER-STOKES,0.5925266903914591,"a two-dimensional domain with obstacles can be observed. The velocity is randomly initialized in
286"
INCOMPRESSIBLE NAVIER-STOKES,0.594306049822064,"the whole domain and a localized force is applied near the bottom of the domain at t = 0. The task
287"
INCOMPRESSIBLE NAVIER-STOKES,0.5960854092526691,"is to reconstruct the position x0 and initial velocity ⃗v0 of this force region by observing the initial
288"
INCOMPRESSIBLE NAVIER-STOKES,0.597864768683274,"and final velocity field only in the top half of the domain. The initial velocity in the bottom half is
289"
INCOMPRESSIBLE NAVIER-STOKES,0.599644128113879,"unknown and cannot be recovered, making a perfect fit impossible. Fig. 5a,b show an example initial
290"
INCOMPRESSIBLE NAVIER-STOKES,0.6014234875444839,"and final state of the system. The final velocity field is measured at t = 56 by which time fast eddies
291"
INCOMPRESSIBLE NAVIER-STOKES,0.603202846975089,"have dissipated significantly.
292"
INCOMPRESSIBLE NAVIER-STOKES,0.604982206405694,"0
25
50
75
100
X 0 20 40 60 80 100 Y (a)"
INCOMPRESSIBLE NAVIER-STOKES,0.6067615658362989,"25
50
75
X 20 40 60 80 Y (b)"
INCOMPRESSIBLE NAVIER-STOKES,0.608540925266904,"101
102
103 Steps 102 Loss (c)"
INCOMPRESSIBLE NAVIER-STOKES,0.6103202846975089,"BFGS
Reparameterized
Supervised
Neural Adjoint"
INCOMPRESSIBLE NAVIER-STOKES,0.6120996441281139,"4
16
64
128
Dataset Size 0 20 40 60 80 100 120 (d) 3 2 1 0 1 2 0.4 0.2 0.0 0.2 0.4"
INCOMPRESSIBLE NAVIER-STOKES,0.6138790035587188,"Figure 5: Fluid experiment. (a,b) Example initial and final velocity fields, obstacles in gray. Only the
upper half, y ≥50, is observed. (c) Optimization curves without refinement, (d) refined loss L/n by
the number of examples n, mean and standard deviation over multiple network initializations and
data sets."
INCOMPRESSIBLE NAVIER-STOKES,0.6156583629893239,"Fig. 5c shows the optimization curves. On this problem, BFGS converges to some optimum in all
293"
INCOMPRESSIBLE NAVIER-STOKES,0.6174377224199288,"cases, usually within 10 iterations, sometimes requiring up to 40 iterations. However, many examples
294"
INCOMPRESSIBLE NAVIER-STOKES,0.6192170818505338,"get stuck in local optima.
295"
INCOMPRESSIBLE NAVIER-STOKES,0.6209964412811388,"For joint optimization, we reparameterize the solution space using a network architecture similar
296"
INCOMPRESSIBLE NAVIER-STOKES,0.6227758007117438,"to the previous experiment, featuring four 2D convolutional layers and two fully-connected layers.
297"
INCOMPRESSIBLE NAVIER-STOKES,0.6245551601423488,"For all tested n, the reparameterized optimization produces larger mean loss values than BFGS,
298"
INCOMPRESSIBLE NAVIER-STOKES,0.6263345195729537,"especially for small n. This results from about 10% of examples seeing higher than average loss
299"
INCOMPRESSIBLE NAVIER-STOKES,0.6281138790035588,"values. Nonetheless, 66.7% of the inverse problems are solved more accurately than BFGS on average
300"
INCOMPRESSIBLE NAVIER-STOKES,0.6298932384341637,"for n > 4.
301"
INCOMPRESSIBLE NAVIER-STOKES,0.6316725978647687,"The neural adjoint method nearly always converges to solutions within the training set parameter
302"
INCOMPRESSIBLE NAVIER-STOKES,0.6334519572953736,"space, not relying on the boundary loss. With solution refinement, this results in a mean loss that
303"
INCOMPRESSIBLE NAVIER-STOKES,0.6352313167259787,"seems largely independent of n and is slightly lower than the results from direct BFGS optimization.
304"
INCOMPRESSIBLE NAVIER-STOKES,0.6370106761565836,"However, most of this improvement comes from the secondary refinement stage which runs BFGS on
305"
INCOMPRESSIBLE NAVIER-STOKES,0.6387900355871886,"the true F. Without solutions refinement, the neural adjoint method yields inaccurate results, losing
306"
INCOMPRESSIBLE NAVIER-STOKES,0.6405693950177936,"to BFGS in 98.2% of cases.
307"
INCOMPRESSIBLE NAVIER-STOKES,0.6423487544483986,"Supervised training does not suffer from examples getting stuck in a local minimum early on. The
308"
INCOMPRESSIBLE NAVIER-STOKES,0.6441281138790036,"highest-loss solutions, which contribute the most to L, are about an order of magnitude better than
309"
INCOMPRESSIBLE NAVIER-STOKES,0.6459074733096085,"the worst BFGS solutions, leading to a much smaller total loss for n ≥16. With solution refinement,
310"
INCOMPRESSIBLE NAVIER-STOKES,0.6476868327402135,"64%, 73% and 72% of examples yield a better solution than BFGS for n = 16, 64, 128, respectively.
311"
DISCUSSION,0.6494661921708185,"5
Discussion
312"
DISCUSSION,0.6512455516014235,"In our experiments, we have focused on relatively small data sets of between 2 and 256 examples to
313"
DISCUSSION,0.6530249110320284,"quantify the worst-case for machine learning methods and observe trends. Using off-the-shelf neural
314"
DISCUSSION,0.6548042704626335,"network architectures and optimizers with no tuning to the specific problem, joint optimization finds
315"
DISCUSSION,0.6565836298932385,"better solutions than BFGS in an average of 69% of tested problems. However, to achieve the best
316"
DISCUSSION,0.6583629893238434,"accuracy, the solution estimates must be passed to a classical optimizer for refinement as training
317"
DISCUSSION,0.6601423487544484,"the network to this level of accuracy would take an inordinate amount of time and large data sets.
318"
DISCUSSION,0.6619217081850534,"Tuning the architectures to the specific examples could lead to further improvements in performance
319"
DISCUSSION,0.6637010676156584,"but would make the approach domain-dependent.
320"
DISCUSSION,0.6654804270462633,"When training data including ground truth solutions are available or can be generated, supervised
321"
DISCUSSION,0.6672597864768683,"learning can sidestep many difficulties that complex loss landscapes pose, such as local minima,
322"
DISCUSSION,0.6690391459074733,"Table 2: Fraction of inverse problems for which neural-network-based methods with refinement find
better or equal solutions than BFGS. Mean over multiple seeds and all n shown in subfigures (d)."
DISCUSSION,0.6708185053380783,"Experiment
Reparameterized
Supervised
Neural Adjoint
Better
Equal
Better
Equal
Better
Equal"
DISCUSSION,0.6725978647686833,"Wave packet fit
86.0%
1.8%
65.1%
14.4%
40.2%
47.4%
Billiards
61.7%
9.0%
27.0%
27.2%
1.6%
98.4%
Kuramoto–Sivashinsky
62.3%
0.0%
57.7%
0.0%
23.9%
62.2%
Incompr. Navier-Stokes
64.1%
0.0%
66.2%
0.1%
56.9%
0.1%"
DISCUSSION,0.6743772241992882,"alternating gradient directions, or zero-gradient areas. This makes supervised learning another
323"
DISCUSSION,0.6761565836298933,"promising alternative to direct optimization, albeit a more involved one.
324"
DISCUSSION,0.6779359430604982,"The neural adjoint method, on the other hand, yields only very minor improvements over BFGS
325"
DISCUSSION,0.6797153024911032,"optimization in our experiments, despite the surrogate network successfully learning to reproduce the
326"
DISCUSSION,0.6814946619217082,"training data. This is not surprising as the neural adjoint method tries to approximate the original
327"
DISCUSSION,0.6832740213523132,"loss landscape which is often difficult to optimize. Improvements over BFGS must therefore come
328"
DISCUSSION,0.6850533807829181,"from regularization effects and exposure to a larger part of the solution space. The fact that the neural
329"
DISCUSSION,0.6868327402135231,"adjoint method with solution refinement produces similar results almost independent of the number
330"
DISCUSSION,0.6886120996441281,"of data points n shows that the joint optimization has little benefit here. Instead, the refinement stage,
331"
DISCUSSION,0.6903914590747331,"which treats all examples independently, dominates the final solution quality. Note that the neural
332"
DISCUSSION,0.6921708185053381,"adjoint method is purely data-driven and does not require an explicit form for the forward process F,
333"
DISCUSSION,0.693950177935943,"making it more widely applicable than the setting considered here.
334"
DISCUSSION,0.6957295373665481,"Tab. 2 summarizes the improvements over classical optimizations for all methods. A corresponding
335"
DISCUSSION,0.697508896797153,"table without solution refinement can be found in Appendix B. Considering that reparameterized
336"
DISCUSSION,0.699288256227758,"optimization is the only network-based method that does not require domain-specific information
337"
DISCUSSION,0.701067615658363,"and nevertheless shows the biggest improvement overall, we believe it is the most attractive variant
338"
DISCUSSION,0.702846975088968,"among the three learned versions. Inverse problems for which reparameterized training does not
339"
DISCUSSION,0.7046263345195729,"find good solutions are easy to identify by their outlier loss values. In these cases, one could simply
340"
DISCUSSION,0.7064056939501779,"compare the solution to a reference solution obtained via direct optimization, and choose the best
341"
DISCUSSION,0.708185053380783,"result.
342"
DISCUSSION,0.7099644128113879,"Limitations
We have only considered unconstrained optimization problems in this work, enforcing
343"
DISCUSSION,0.7117437722419929,"hard constraints by running bounded parameters through a scaled tanh function which naturally
344"
DISCUSSION,0.7135231316725978,"clamps out-of-bounds values in a differentiable manner.
345"
DISCUSSION,0.7153024911032029,"The improved solutions found by joint optimization come with an increased computational cost
346"
DISCUSSION,0.7170818505338078,"compared to direct optimization. The time it took to train the reparameterization networks was 3x to
347"
DISCUSSION,0.7188612099644128,"6x longer for the first three experiments and 22x for the fluids experiment.
348"
CONCLUSIONS AND OUTLOOK,0.7206405693950177,"6
Conclusions and outlook
349"
CONCLUSIONS AND OUTLOOK,0.7224199288256228,"We have investigated the effects of joint optimization of multiple inverse problems by reparameterizing
350"
CONCLUSIONS AND OUTLOOK,0.7241992882562278,"the solution space using a neural network, showing that joint optimization can often find better
351"
CONCLUSIONS AND OUTLOOK,0.7259786476868327,"solutions than classical optimization techniques. Since our reparameterization approach does not
352"
CONCLUSIONS AND OUTLOOK,0.7277580071174378,"require any more information than classical optimizers, it can be used as a drop-in replacement. This
353"
CONCLUSIONS AND OUTLOOK,0.7295373665480427,"could be achieved by adding a function or option to existing optimization libraries that internally
354"
CONCLUSIONS AND OUTLOOK,0.7313167259786477,"sets up a standard neural network with the required number of inputs and outputs and runs the
355"
CONCLUSIONS AND OUTLOOK,0.7330960854092526,"optimization, hiding details of the training process, network architecture, and hyperparameters from
356"
CONCLUSIONS AND OUTLOOK,0.7348754448398577,"the user while making the gains in optimization accuracy conveniently accessible. To facilitate this,
357"
CONCLUSIONS AND OUTLOOK,0.7366548042704626,"we will make the full source code publicly available.
358"
CONCLUSIONS AND OUTLOOK,0.7384341637010676,"From accelerating matrix multiplications [FBH+22] to solving systems of linear equations [CHL+23,
359"
CONCLUSIONS AND OUTLOOK,0.7402135231316725,"SSHR19], it is becoming increasingly clear that machine learning methods can be applied to purely
360"
CONCLUSIONS AND OUTLOOK,0.7419928825622776,"numerical problems outside of typical big data settings, and our results show that this also extends to
361"
CONCLUSIONS AND OUTLOOK,0.7437722419928826,"solving nonlinear inverse problems.
362"
REFERENCES,0.7455516014234875,"References
363"
REFERENCES,0.7473309608540926,"[AAA+13] M Agostini, M Allardt, E Andreotti, AM Bakalyarov, M Balata, I Barabanov, M Barnabé
364"
REFERENCES,0.7491103202846975,"Heider, N Barros, L Baudis, C Bauer, et al. Pulse shape discrimination for gerda phase
365"
REFERENCES,0.7508896797153025,"i data. The European Physical Journal C, 73(10):2583, 2013.
366"
REFERENCES,0.7526690391459074,"[AAA+18] Craig E Aalseth, N Abgrall, Estanislao Aguayo, SI Alvis, M Amman, Isaac J Arnquist,
367"
REFERENCES,0.7544483985765125,"FT Avignone III, Henning O Back, Alexander S Barabash, PS Barbeau, et al. Search
368"
REFERENCES,0.7562277580071174,"for neutrinoless double-β decay in ge 76 with the majorana demonstrator. Physical
369"
REFERENCES,0.7580071174377224,"review letters, 120(13):132502, 2018.
370"
REFERENCES,0.7597864768683275,"[ALGS+22] Kelsey R Allen, Tatiana Lopez-Guevara, Kimberly Stachenfeld, Alvaro Sanchez-
371"
REFERENCES,0.7615658362989324,"Gonzalez, Peter Battaglia, Jessica Hamrick, and Tobias Pfaff. Physical design using
372"
REFERENCES,0.7633451957295374,"differentiable learned simulators. arXiv preprint arXiv:2202.00728, 2022.
373"
REFERENCES,0.7651245551601423,"[BB67] Cx K Batchelor and George Keith Batchelor. An introduction to fluid dynamics. Cam-
374"
REFERENCES,0.7669039145907474,"bridge university press, 1967.
375"
REFERENCES,0.7686832740213523,"[BHM18] Mikhail Belkin, Daniel J Hsu, and Partha Mitra. Overfitting or perfect fitting? risk
376"
REFERENCES,0.7704626334519573,"bounds for classification and regression rules that interpolate. Advances in neural
377"
REFERENCES,0.7722419928825622,"information processing systems, 31, 2018.
378"
REFERENCES,0.7740213523131673,"[BPL21] Randall Balestriero, Jerome Pesenti, and Yann LeCun. Learning in high dimension
379"
REFERENCES,0.7758007117437722,"always amounts to extrapolation. arXiv preprint arXiv:2110.09485, 2021.
380"
REFERENCES,0.7775800711743772,"[CCC+19] Giuseppe Carleo, Ignacio Cirac, Kyle Cranmer, Laurent Daudet, Maria Schuld, Naftali
381"
REFERENCES,0.7793594306049823,"Tishby, Leslie Vogt-Maranto, and Lenka Zdeborová. Machine learning and the physical
382"
REFERENCES,0.7811387900355872,"sciences. Reviews of Modern Physics, 91(4):045002, 2019.
383"
REFERENCES,0.7829181494661922,"[CHL+23] Salvatore Calì, Daniel C Hackett, Yin Lin, Phiala E Shanahan, and Brian Xiao. Neural-
384"
REFERENCES,0.7846975088967971,"network preconditioners for solving the dirac equation in lattice gauge theory. Physical
385"
REFERENCES,0.7864768683274022,"Review D, 107(3):034508, 2023.
386"
REFERENCES,0.7882562277580071,"[DJO+18] S Delaquis, MJ Jewell, I Ostrovskiy, M Weber, T Ziegler, J Dalmasson, LJ Kaufman,
387"
REFERENCES,0.7900355871886121,"T Richards, JB Albert, G Anton, et al. Deep neural networks for energy and position
388"
REFERENCES,0.791814946619217,"reconstruction in exo-200. Journal of Instrumentation, 13(08):P08023, 2018.
389"
REFERENCES,0.7935943060498221,"[FBH+22] Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-
390"
REFERENCES,0.7953736654804271,"Paredes, Mohammadamin Barekatain, Alexander Novikov, Francisco J R Ruiz, Julian
391"
REFERENCES,0.797153024911032,"Schrittwieser, Grzegorz Swirszcz, et al. Discovering faster matrix multiplication algo-
392"
REFERENCES,0.798932384341637,"rithms with reinforcement learning. Nature, 610(7930):47–53, 2022.
393"
REFERENCES,0.800711743772242,"[GBCB16] Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning.
394"
REFERENCES,0.802491103202847,"MIT press Cambridge, 2016.
395"
REFERENCES,0.8042704626334519,"[GEB16] Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Image style transfer using
396"
REFERENCES,0.806049822064057,"convolutional neural networks. In Proceedings of the IEEE conference on computer
397"
REFERENCES,0.8078291814946619,"vision and pattern recognition, pages 2414–2423, 2016.
398"
REFERENCES,0.8096085409252669,"[GH18] Daniel George and EA Huerta. Deep learning for real-time gravitational wave detection
399"
REFERENCES,0.8113879003558719,"and parameter estimation: Results with advanced ligo data. Physics Letters B, 778:64–
400"
REFERENCES,0.8131672597864769,"70, 2018.
401"
REFERENCES,0.8149466192170819,"[GM78] Philip E Gill and Walter Murray. Algorithms for the solution of the nonlinear least-
402"
REFERENCES,0.8167259786476868,"squares problem. SIAM Journal on Numerical Analysis, 15(5):977–992, 1978.
403"
REFERENCES,0.8185053380782918,"[Gra97] Ian Grant. Particle image velocimetry: a review. Proceedings of the Institution of
404"
REFERENCES,0.8202846975088968,"Mechanical Engineers, Part C: Journal of Mechanical Engineering Science, 211(1):55–
405"
REFERENCES,0.8220640569395018,"76, 1997.
406"
REFERENCES,0.8238434163701067,"[HAL+20] Yuanming Hu, Luke Anderson, Tzu-Mao Li, Qi Sun, Nathan Carr, Jonathan Ragan-
407"
REFERENCES,0.8256227758007118,"Kelley, and Frédo Durand. Difftaichi: Differentiable programming for physical simula-
408"
REFERENCES,0.8274021352313167,"tion. International Conference on Learning Representations (ICLR), 2020.
409"
REFERENCES,0.8291814946619217,"[Hay94] Simon Haykin. Neural networks: a comprehensive foundation. Prentice Hall PTR,
410"
REFERENCES,0.8309608540925267,"1994.
411"
REFERENCES,0.8327402135231317,"[HKT20] Philipp Holl, Vladlen Koltun, and Nils Thuerey. Learning to control pdes with differ-
412"
REFERENCES,0.8345195729537367,"entiable physics. In International Conference on Learning Representations (ICLR),
413"
REFERENCES,0.8362989323843416,"2020.
414"
REFERENCES,0.8380782918149466,"[HKT21] Philipp Holl, Vladlen Koltun, and Nils Thuerey. Scale-invariant learning by physics
415"
REFERENCES,0.8398576512455516,"inversion. arXiv preprint arXiv:2109.15048, 2021.
416"
REFERENCES,0.8416370106761566,"[HSDG19] Stephan Hoyer, Jascha Sohl-Dickstein, and Sam Greydanus. Neural reparameterization
417"
REFERENCES,0.8434163701067615,"improves structural optimization. arXiv preprint arXiv:1909.04240, 2019.
418"
REFERENCES,0.8451957295373665,"[KAT+19] Byungsoo Kim, Vinicius C. Azevedo, Nils Thuerey, Theodore Kim, Markus Gross,
419"
REFERENCES,0.8469750889679716,"and Barbara Solenthaler. Deep fluids: A generative network for parameterized fluid
420"
REFERENCES,0.8487544483985765,"simulations. Computer Graphics Forum, 2019.
421"
REFERENCES,0.8505338078291815,"[KB15] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In
422"
REFERENCES,0.8523131672597865,"International Conference on Learning Representations (ICLR), 2015.
423"
REFERENCES,0.8540925266903915,"[KGZ+21] Aditi Krishnapriyan, Amir Gholami, Shandian Zhe, Robert Kirby, and Michael W
424"
REFERENCES,0.8558718861209964,"Mahoney. Characterizing possible failure modes in physics-informed neural networks.
425"
REFERENCES,0.8576512455516014,"Advances in Neural Information Processing Systems, 34:26548–26560, 2021.
426"
REFERENCES,0.8594306049822064,"[KSA+21] Dmitrii Kochkov, Jamie A. Smith, Ayya Alieva, Qing Wang, Michael P. Brenner,
427"
REFERENCES,0.8612099644128114,"and Stephan Hoyer.
Machine learning accelerated computational fluid dynamics.
428"
REFERENCES,0.8629893238434164,"arXiv:2102.01010 [physics], 2021.
429"
REFERENCES,0.8647686832740213,"[KSH12] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with
430"
REFERENCES,0.8665480427046264,"deep convolutional neural networks. In Advances in Neural Information Processing
431"
REFERENCES,0.8683274021352313,"Systems, 2012.
432"
REFERENCES,0.8701067615658363,"[Kur78] Yoshiki Kuramoto. Diffusion-induced chaos in reaction systems. Progress of Theoretical
433"
REFERENCES,0.8718861209964412,"Physics Supplement, 64:346–367, 1978.
434"
REFERENCES,0.8736654804270463,"[LLK19] Junbang Liang, Ming Lin, and Vladlen Koltun. Differentiable cloth simulation for
435"
REFERENCES,0.8754448398576512,"inverse problems. In Advances in Neural Information Processing Systems, pages
436"
REFERENCES,0.8772241992882562,"771–780, 2019.
437"
REFERENCES,0.8790035587188612,"[LN89] Dong C Liu and Jorge Nocedal. On the limited memory bfgs method for large scale
438"
REFERENCES,0.8807829181494662,"optimization. Mathematical programming, 45(1-3):503–528, 1989.
439"
REFERENCES,0.8825622775800712,"[LPY+21] Lu Lu, Raphael Pestourie, Wenjie Yao, Zhicheng Wang, Francesc Verdugo, and
440"
REFERENCES,0.8843416370106761,"Steven G Johnson. Physics-informed neural networks with hard constraints for in-
441"
REFERENCES,0.8861209964412812,"verse design. SIAM Journal on Scientific Computing, 43(6):B1105–B1132, 2021.
442"
REFERENCES,0.8879003558718861,"[LSAH20] Housen Li, Johannes Schwab, Stephan Antholzer, and Markus Haltmeier. Nett: Solving
443"
REFERENCES,0.8896797153024911,"inverse problems with deep neural networks. Inverse Problems, 36(6):065005, 2020.
444"
REFERENCES,0.891459074733096,"[MAL13] Kohta Murase, Markus Ahlers, and Brian C Lacki. Testing the hadronuclear origin of
445"
REFERENCES,0.8932384341637011,"pev neutrinos observed with icecube. Physical Review D, 88(12):121301, 2013.
446"
REFERENCES,0.895017793594306,"[MLA+19] Rajesh Maingi, Arnold Lumsdaine, Jean Paul Allain, Luis Chacon, SA Gourlay,
447"
REFERENCES,0.896797153024911,"CM Greenfield, JW Hughes, D Humphreys, V Izzo, H McLean, et al. Summary
448"
REFERENCES,0.8985765124555161,"of the fesac transformative enabling capabilities panel report. Fusion Science and
449"
REFERENCES,0.900355871886121,"Technology, 75(3):167–177, 2019.
450"
REFERENCES,0.902135231316726,"[Pop00] Stephen Pope. Turbulent Flows. Cambridge University Press, 2000.
451"
REFERENCES,0.9039145907473309,"[PTVF07] William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery.
452"
REFERENCES,0.905693950177936,"Numerical Recipes. Cambridge University Press, 3 edition, 2007.
453"
REFERENCES,0.9074733096085409,"[PW17] Patrick Putzky and Max Welling. Recurrent inference machines for solving inverse
454"
REFERENCES,0.9092526690391459,"problems. arXiv preprint arXiv:1706.04008, 2017.
455"
REFERENCES,0.9110320284697508,"[RPK19] Maziar Raissi, Paris Perdikaris, and George Karniadakis. Physics-informed neural
456"
REFERENCES,0.9128113879003559,"networks: A deep learning framework for solving forward and inverse problems in-
457"
REFERENCES,0.9145907473309609,"volving nonlinear partial differential equations. Journal of Computational Physics,
458"
REFERENCES,0.9163701067615658,"378:686–707, 2019.
459"
REFERENCES,0.9181494661921709,"[RPM20] Simiao Ren, Willie Padilla, and Jordan Malof. Benchmarking deep inverse models
460"
REFERENCES,0.9199288256227758,"over time, and the neural-adjoint method. Advances in Neural Information Processing
461"
REFERENCES,0.9217081850533808,"Systems, 33:38–48, 2020.
462"
REFERENCES,0.9234875444839857,"[RT21] Stephan Rasp and Nils Thuerey. Data-driven medium-range weather prediction with a
463"
REFERENCES,0.9252669039145908,"resnet pretrained on climate simulations: A new model for weatherbench. Journal of
464"
REFERENCES,0.9270462633451957,"Advances in Modeling Earth Systems, 13(2):e2020MS002405, 2021.
465"
REFERENCES,0.9288256227758007,"[SC19] Samuel S Schoenholz and Ekin D Cubuk. Jax, md: End-to-end differentiable, hardware
466"
REFERENCES,0.9306049822064056,"accelerated, molecular dynamics in pure python. arXiv:1912.04232, 2019.
467"
REFERENCES,0.9323843416370107,"[SF18] Connor Schenck and Dieter Fox. Spnets: Differentiable fluid dynamics for deep neural
468"
REFERENCES,0.9341637010676157,"networks. In Conference on Robot Learning, pages 317–335, 2018.
469"
REFERENCES,0.9359430604982206,"[SFK+21] Kimberly Stachenfeld, Drummond B Fielding, Dmitrii Kochkov, Miles Cranmer, Tobias
470"
REFERENCES,0.9377224199288257,"Pfaff, Jonathan Godwin, Can Cui, Shirley Ho, Peter Battaglia, and Alvaro Sanchez-
471"
REFERENCES,0.9395017793594306,"Gonzalez. Learned coarse models for efficient turbulence simulation. arXiv preprint
472"
REFERENCES,0.9412811387900356,"arXiv:2112.15275, 2021.
473"
REFERENCES,0.9430604982206405,"[SGGP+20] Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec,
474"
REFERENCES,0.9448398576512456,"and Peter Battaglia. Learning to simulate complex physics with graph networks. In
475"
REFERENCES,0.9466192170818505,"International conference on machine learning, pages 8459–8468. PMLR, 2020.
476"
REFERENCES,0.9483985765124555,"[SHT22] Patrick Schnell, Philipp Holl, and Nils Thuerey. Half-inverse gradients for physical
477"
REFERENCES,0.9501779359430605,"deep learning. arXiv preprint arXiv:2203.10131, 2022.
478"
REFERENCES,0.9519572953736655,"[SSHR19] Johannes Sappl, Laurent Seiler, Matthias Harders, and Wolfgang Rauch. Deep learning
479"
REFERENCES,0.9537366548042705,"of preconditioners for conjugate gradient solvers in urban water related problems. arXiv
480"
REFERENCES,0.9555160142348754,"preprint arXiv:1906.06925, 2019.
481"
REFERENCES,0.9572953736654805,"[SZ14] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-
482"
REFERENCES,0.9590747330960854,"scale image recognition. arXiv preprint arXiv:1409.1556, 2014.
483"
REFERENCES,0.9608540925266904,"[Tar05] Albert Tarantola. Inverse problem theory and methods for model parameter estimation.
484"
REFERENCES,0.9626334519572953,"SIAM, 2005.
485"
REFERENCES,0.9644128113879004,"[TLQL21] Tetsuya Takahashi, Junbang Liang, Yi-Ling Qiao, and Ming C Lin. Differentiable fluids
486"
REFERENCES,0.9661921708185054,"with solid coupling for learning and control. In Proceedings of the AAAI Conference on
487"
REFERENCES,0.9679715302491103,"Artificial Intelligence, volume 35(7), pages 6138–6146, 2021.
488"
REFERENCES,0.9697508896797153,"[TSSP17] Jonathan Tompson, Kristofer Schlachter, Pablo Sprechmann, and Ken Perlin. Acceler-
489"
REFERENCES,0.9715302491103203,"ating eulerian fluid simulation with convolutional networks. In Proceedings of Machine
490"
REFERENCES,0.9733096085409253,"Learning Research, pages 3424–3433, 2017.
491"
REFERENCES,0.9750889679715302,"[UBF+20] Kiwon Um, Robert Brand, Yun Raymond Fei, Philipp Holl, and Nils Thuerey. Solver-
492"
REFERENCES,0.9768683274021353,"in-the-loop: Learning from differentiable physics to interact with iterative pde-solvers.
493"
REFERENCES,0.9786476868327402,"Advances in Neural Information Processing Systems, 2020.
494"
REFERENCES,0.9804270462633452,"[UVL18] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Deep image prior. In Pro-
495"
REFERENCES,0.9822064056939501,"ceedings of the IEEE conference on computer vision and pattern recognition, pages
496"
REFERENCES,0.9839857651245552,"9446–9454, 2018.
497"
REFERENCES,0.9857651245551602,"[VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
498"
REFERENCES,0.9875444839857651,"Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in
499"
REFERENCES,0.9893238434163701,"Neural Information Processing Systems, pages 5998–6008, 2017.
500"
REFERENCES,0.9911032028469751,"[Yua00] Ya-xiang Yuan. A review of trust region algorithms for optimization. In Iciam, volume
501"
REFERENCES,0.9928825622775801,"99(1), pages 271–282, 2000.
502"
REFERENCES,0.994661921708185,"[YZWX19] XIA Yang, Suhaib Zafar, J-X Wang, and Heng Xiao. Predictive large-eddy-simulation
503"
REFERENCES,0.99644128113879,"wall modeling via physics-informed neural networks.
Physical Review Fluids,
504"
REFERENCES,0.998220640569395,"4(3):034602, 2019.
505"
