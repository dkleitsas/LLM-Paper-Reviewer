Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0010121457489878543,"In the recent years of development of theoretical machine learning, over-
parametrization has been shown to be a powerful tool to resolve many fundamental
problems, such as the convergence analysis of deep neural network. While many
works have been focusing on designing various algorithms for over-parametrized
network with one-hidden layer, multiple-hidden layers framework has received
much less attention due to the complexity of the analysis, and even fewer al-
gorithms have been proposed. In this work, we initiate the study of the per-
formance of second-order algorithm on multiple-hidden layers over-parametrized
neural network. We propose a novel algorithm to train such network, in time sub-
quadratic in the width of the neural network. Our algorithm combines the Gram-
Gauss-Newton method, tensor-based sketching techniques and preconditioning."
INTRODUCTION,0.0020242914979757085,"1
INTRODUCTION"
INTRODUCTION,0.003036437246963563,"Deep neural networks have been playing a central role in both practical (such as computer vision
(LeCun et al., 1998; Krizhevsky et al., 2012; Szegedy et al., 2015; He et al., 2016), natural language
processing (Collobert et al., 2011; Devlin et al., 2018), automatic driving system, game playing
(Silver et al., 2016; 2017) ) and theoretical machine learning community (Li & Liang (2018); Jacot
et al. (2018); Du et al. (2019b); Allen-Zhu et al. (2019a;b); Du et al. (2019a); Song & Yang (2019);
Brand et al. (2021); Zou et al. (2018); Cao & Gu (2019); Lee et al. (2019a); Liu et al. (2020; 2021);
Chen et al. (2021)) . In order to analyze the dynamics of neural networks and obtain provable
guarantees, using over-parametrization has been a growing trend."
INTRODUCTION,0.004048582995951417,"In terms of understanding the convergence behavior of over-parametrized networks, most of the
attentions have been directed to the study of ﬁrst-order method such as gradient descent or stochastic
gradient descent. The widespread use of ﬁrst-order method is explained, to a large degree, by its
computational efﬁciency, since computing the gradient of the loss function at each iteration is usually
cheap and simple, let alone with its compatibility with random sampling-based method such as mini-
batch. One of the major drawbacks of ﬁrst-order methods is their convergence rate is typically slow
in many non-convex settings (poly(n, L, log(1/ϵ)) iterations, where n is the number of training
samples, L is the number of layers and ϵ is the precision of training), e.g., deep neural network
with ReLU activation, as shown in Allen-Zhu et al. (2019a)), which is often the case of a deep
over-parameterized neural network."
INTRODUCTION,0.005060728744939271,"Second-order method (which employs the information of the Hessian matrix) on the other hand
enjoys a much faster convergence rate (only log(1/ϵ) iterations (Zhang et al., 2019), but not
poly(n) log(1/ϵ) iterations) and exploits the local geometry of the loss function to overcome the
pathological curvature issues that are critical in ﬁrst-order method. Another clear advantage of
second-order method over ﬁrst-order method is it does not require the tuning of learning rate. The
expense of using second-order method is its prohibitive cost per iteration, as it is imperative to invert
a dynamically-changing Hessian matrix or equivalently, solving a regression task involving Hessian.
Given any weight matrix of size m × m (m is the width of network), its Hessian matrix has size
m2 × m2, which makes any naive implementation of second-order algorithm takes at least O(m4)
time since one needs to write down the Hessian. This explains the scarcity of deploying large-scale
second-order method in non-convex setting, such as training deep neural networks, in contrast to
their popular presence in convex optimization setting (Vaidya (1989); Daitch & Spielman (2008);
Lee et al. (2015); Cohen et al. (2019); Lee et al. (2019b); Jiang et al. (2020b;a); Song & Yu (2021))."
INTRODUCTION,0.006072874493927126,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.00708502024291498,"Recent works (Cai et al. (2019); Zhang et al. (2019)) improved the practicality of second-order
method on training deep networks and presented algorithms to train one-hidden layer over-
parametrized networks with smooth activation functions. Speciﬁcally, they achieve a running time
of O(mn2) per iteration. Their methods are essentially variants of Gauss-Newton method combined
with using neural tangent kernels (Jacot et al. (2018)) to prove the convergence. By using the idea of
randomized linear algebra (Clarkson & Woodruff (2013); Woodruff (2014)) and a clever sketching
matrix as a preconditioner, Brand et al. (2021) further improves the running time to eO(mn) per
iteration."
INTRODUCTION,0.008097165991902834,"However, all of these algorithms are for training a shallow network with one-hidden layer and fall
short on deep networks — First, it is not clear that their algorithms can be generalized to multi-layer
setting, due to the presence of gradient vanishing or exploding. In the seminal work of Allen-Zhu
et al. (2019a), they showed that as long as the networks are over-parametrized, ﬁrst-order methods
such as gradient descent and stochastic gradient descent won’t encounter such problem. But does
this still hold for second-order method? Can we provably show that second-order method has a
good performance when training deep over-parametrized networks? Second, even the fastest of
them (Brand et al. (2021)) would incur a running time of eO(m2nL) per iteration, which seems
unavoidable due to the size of intermediate weight matrices is m × m."
INTRODUCTION,0.009109311740890687,"In this work, we take the ﬁrst step to tame the beast — We propose a second-order method that
achieves subquadratic cost per iteration with respect to m, and show that it has linear convergence
rate in training deep over-parametrized neural networks. We emphasize the importance of obtaining
subquadratic algorithm, since in multi-layer settings, the network width is typically much larger than
in one-hidden layer setting (m ≥n8L12, (Zou & Gu, 2019))."
INTRODUCTION,0.010121457489878543,"Our work can be decomposed into two parts: algorithmically and analytically. From an algorith-
mic perspective, our method builds upon a variant of Gauss-Newton method (Björck (1996)) called
Gram-Gauss-Newton method (Cai et al. (2019); Brand et al. (2021)). In order to achieve a feasible
running time, we exploit two features of the gradient, which is the key ingredient to form the Jaco-
bian matrix: 1). The gradient is low rank (rank n). 2). The gradient can be formulated as the outer
product of two vectors. From an analytical perspective, our work is inspired by Allen-Zhu et al.
(2019a). In contrast to their proof which is a straightforward analysis of the gradient, we make use
of the multi-layer neural tangent kernels (Du et al. (2019a)) and establish a connection between a
Gram matrix we compute at each iteration and the NTK matrix."
INTRODUCTION,0.011133603238866396,Our Contributions. We summarize our technical contributions below.
INTRODUCTION,0.012145748987854251,"• We develop an analytical framework for the convergence behavior of second-order method
on training multi-layer over-parametrized neural network. To facilitate the analysis, we
exploit the equivalence between neural tangent kernels and our over-parametrized network.
• We design a second-order algorithm to train such networks, and achieve a cost per iteration
of eo(m2). Our algorithm makes use of Gram-Gauss-Newton method, tensor-based sketch-
ing techniques, and data structures that maintains a low rank representation efﬁciently.
• By combining fast tensor algebra techniques and sketching-based preconditioning, we de-
vise an algorithm to efﬁciently solve a regression problem where the involved matrix has
its rows being tensor product of vectors."
OUR RESULT,0.013157894736842105,"1.1
OUR RESULT"
OUR RESULT,0.01417004048582996,"Our main result can be summarized in the following three theorems, with one analyzing the conver-
gence behavior of a general Gram-based optimization framework, one designing an efﬁcient algo-
rithm to realize this second-order optimization scheme, and the other is a novel algorithm to solve
tensor-based regression in high precision and fast, which is a key step in our second-order method."
OUR RESULT,0.015182186234817813,"Throughout this paper, we will use n to denote the number of training data points, d to denote the
dimension of input data points, m to denote the width of the network and L to denote the number of
layers of the network. We use ft ∈Rn to denote the prediction of neural network at time t."
OUR RESULT,0.016194331983805668,"Our ﬁrst theorem demonstrates the fast convergence rate of our algorithm.
Theorem 1.1 (Convergence, informal version of Theorem F.19). Suppose the width of the neural
network satisﬁes m ≥poly(n, L), then there exists an algorithm (Algorithm 1) such that, over"
OUR RESULT,0.01720647773279352,Under review as a conference paper at ICLR 2022
OUR RESULT,0.018218623481781375,"the randomness of initialization of the network and the algorithm, with probability at least 1 −
e−Ω(log2 m), we have"
OUR RESULT,0.019230769230769232,∥ft+1 −y∥2 ≤1
OUR RESULT,0.020242914979757085,"2∥ft −y∥2,"
OUR RESULT,0.02125506072874494,where ft ∈Rn is the the prediction produced by neural network at time t.
OUR RESULT,0.022267206477732792,"The above theorem establishes the linear convergence behavior of our second-order method, which
is a standard convergence result for second-order method, as well as the same behavior as in one-
hidden layer over-parametrized networks (Brand et al. (2021)). However, compared to one-hidden
layer case, our analysis is much more sophisticated since we have to carefully control the probability
so that it does not blow up exponentially with respect to the number of layers."
OUR RESULT,0.02327935222672065,The next theorem concerns the cost per iteration of our second-order algorithm.
OUR RESULT,0.024291497975708502,"Theorem 1.2 (Runtime, informal version of Theorem B.1). There exists a randomized algorithm
(Algorithm 1) that trains a multi-layer neural network of width m with the cost per training iteration
being"
OUR RESULT,0.025303643724696356,O(m2−Ω(1)).
OUR RESULT,0.02631578947368421,"We improve the overall training time of multi-layer over-parametrized networks with second-order
method from Tinit +T ·O(m2) to Tinit +T ·o(m2), where Tinit is the initialization time of training,
typically takes O(m2). As we have argued before, multi-layer over-parametrized networks require
m to be in the order of n8, hence improving the cost per iteration from quadratic to subquadratic is
an important gain in speeding up training. Its advantage is even more evident when one seeks a high
precision solution, and hence the number of iterations T is large."
OUR RESULT,0.027327935222672066,"We highlight that it is non-trivial to obtain a subquadratic running time per iteration: If not handled
properly, computing the matrix-vector product with weight matrices will take O(m2) time! This
means that even for ﬁrst-order methods such as gradient descent, it is not clear how to achieve a
subquadratic running time, since one has to multiply the weight matrix with a vector in both forward
evaluation and backpropagation. In our case, we have also a Jacobian matrix of size n × m2, so
forming it naively will cost O(nm2) time, which is prohibitively large. Finally, note that the update
matrix is also an m × m matrix. In order to circumvent these problems, we exploit the fact that the
gradient is of low rank (rank n), hence one can compute a rank-n factorization and use it to support
fast matrix-vector product. We also observe that each row of the Jacobian matrix can be formulated
as a tensor product of two vectors, therefore we can make use of fast randomized linear algebra to
approximate the tensor product efﬁciently. As a byproduct, we have the following technical theorem:"
OUR RESULT,0.02834008097165992,"Theorem 1.3 (Fast Tensor Regression, informal version of Theorem D.14). Given two n×m matri-
ces U and V with m ≫n and a target vector c ∈Rn. Let J = [vec(u1v⊤
1 )⊤, . . . , vec(unv⊤
n )⊤] ∈
Rn×m2 where ui is the i-th row of matrix U ∀i ∈[n]. There is a randomized algorithm that takes
eO(nm + n2(log(κ/ϵ) + log(m/δ)) + nω) time and outputs a vector bx ∈Rn such that"
OUR RESULT,0.029352226720647773,∥JJ⊤bx −c∥2 ≤ϵ∥c∥2
OUR RESULT,0.030364372469635626,"holds with probability at least 1 −δ, and κ is the condition number of J."
OUR RESULT,0.03137651821862348,"From a high level, the algorithm proceeds as follows: given matrices U and V , it forms an approxi-
mation eJ ∈Rn×n log(m/δ), where each row is generated by applying fast tensor sketching technique
to ui and vi (Ahle et al. (2020)). Then, it uses another sketching matrix for eJ to obtain a good
preconditioner R for eJ. Subsequently, it runs a gradient descent to solve the regression."
OUR RESULT,0.032388663967611336,"To understand this runtime better, we note that nm term is the size of matrices U and V , hence
reading the entries from these matrices will take at least O(nm) time. The algorithm then uses
tensor-based sketching techniques (Ahle et al. (2020)) to squash length m2 tensors to length
O(n log(m/ϵδ)). All subsequent operations are performed on these much smaller vectors. Fi-
nally, computing the preconditioner takes eO(nω) time, and running the gradient descent takes
eO(n2 log(κ/ϵ)) time."
OUR RESULT,0.03340080971659919,Under review as a conference paper at ICLR 2022
RELATED WORK,0.03441295546558704,"1.2
RELATED WORK"
RELATED WORK,0.0354251012145749,"Second-order Method in Optimization. Though not as prevalent as ﬁrst-order method in deep
learning, second-order methods are one of the most popular in convex setting, such as linear pro-
gramming (Vaidya (1989); Daitch & Spielman (2008); Lee et al. (2015); Cohen et al. (2019)), em-
pirical risk minimization (Lee et al. (2019b)), cutting plane method (Jiang et al. (2020b)) and semi-
deﬁnite programming (Jiang et al. (2020a)). Due to the prohibitive high cost of implementing one
step of second-order method, most of these works focus on improving the cost per iteration."
RELATED WORK,0.03643724696356275,"In non-convex setting, there’s a vast body of ongoing works (Martens & Grosse (2015); Botev et al.
(2017); Pilanci & Wainwright (2017); Agarwal et al. (2017); Bernacchia et al. (2018); Cai et al.
(2019); Zhang et al. (2019); Brand et al. (2021); Yao et al. (2021)) that try to improve the practicality
of second-order method and adapt them to train deep neural networks. As shown in Cai et al. (2019),
it is possible to exploit the equivalence between over-parametrized networks and neural tangent
kernel to optimize an n×n matrix instead of an m2×m2 matrix, which is an important breakthrough
in gaining speedup for second-order method. Sketching and sampling-based methods can also be
used to accelerate the computation of inverses of the Hessian matrix (Pilanci & Wainwright (2017)).
In spirit, our work resembles most with Cai et al. (2019) and Brand et al. (2021), in the sense that
our optimization also works on an n × n Gram matrix. Our algorithm also makes use of sketching
and sampling, as in Pilanci & Wainwright (2017); Brand et al. (2021)."
RELATED WORK,0.03744939271255061,"Over-parameterized Neural Networks. In recent deep learning literature, understanding the ge-
ometry and convergence behavior of various optimization algorithms on over-parameterized neural
networks has received a lot of attention (Li & Liang (2018); Du et al. (2019b); Allen-Zhu et al.
(2019a;b); Du et al. (2019a); Song & Yang (2019); Ji & Telgarsky (2020); Zou et al. (2018); Cao &
Gu (2019); Liu et al. (2020; 2021)). The seminal work of Jacot et al. (2018) initiates the study of
neural tangent kernel (NTK), which is a powerful analytical tool in this area, since as long as the
neural network is wide enough (m ≥Ω(n4)), then the optimization dynamic on a neural network is
equivalent to that on a NTK."
RELATED WORK,0.038461538461538464,"Sketching. Using randomized linear algebra to reduce the dimension of the problem and speedup the
algorithms for various problems has been a growing trend in machine learning community (Sarlos
(2006); Clarkson & Woodruff (2013); Woodruff (2014)) due to its wide range of applications to
various tasks, especially the efﬁcient approximation of kernel matrices (Avron et al. (2014); Ahle
et al. (2020); Woodruff & Zandieh (2020)). The standard “Sketch-and-Solve” (Clarkson & Woodruff
(2013)) paradigm involves using sketching to reduce the dimension of the problem and then using
a blackbox for the original problem to gain an edge on computational efﬁciency. Another line of
work is to use sketching as a preconditioner (Woodruff (2014); Brand et al. (2021)) to obtain a high
precision solution."
RELATED WORK,0.039473684210526314,"Roadmap. In Section 2, we give a preliminary view of the training setup we consider in this paper.
In Section 2.1, we introduce the notations that will be used throughout this paper. In Section 2.2,
we consider the training setup. In Section 3, we overview the techniques employed in this paper.
In Section 3.2, we demonstrate various techniques to prove the convergence of our second-order
method. In Section 3.1, we examine the algorithmic tools utilized in this paper to achieve sub-
quadratic cost per iteration. In Section 4, we summarize the results in this paper and point out some
future directions."
PRELIMINARIES,0.04048582995951417,"2
PRELIMINARIES"
NOTATIONS,0.04149797570850203,"2.1
NOTATIONS"
NOTATIONS,0.04251012145748988,"For any positive integer n, we use [n] to denote the set {1, 2, · · · , n}. We use E[·] to denote expec-
tation and Pr[·] for probability. We use ∥x∥2 to denote the ℓ2 norm of a vector x. We use ∥A∥to
denote the spectral norm of matrix A. We use ∥A∥F to denote the Frobenius norm of A. We use
A⊤to denote the transpose of matrix A. We use Im to denote the identity matrix of size m × m.
For matrix A or vector x, we use ∥A∥0, ∥x∥0 to denote the number of nonzero entries of A and x
respectively. Note that ∥· ∥0 is a semi-norm since it satisﬁes triangle inequality. Given a real square
matrix A, we use λmax(A) and λmin(A) to denote its largest and smallest eigenvalues respectively.
Given a real matrix A, we use σmax(A) and σmin(A) to denote its largest and smallest singular"
NOTATIONS,0.043522267206477734,Under review as a conference paper at ICLR 2022
NOTATIONS,0.044534412955465584,"values respectively. We use N(µ, σ2) to denote the Gaussian distribution with mean µ and variance
σ2. We use eO(f(n)) to denote O(f(n) · poly log(f(n)). We use ⟨·, ·⟩to denote the inner product,
when applying to two vectors, this denotes the standard dot product between two vectors, and when
applying to two matrices, this means ⟨A, B⟩= tr[A⊤B] where tr[A] denote the trace of matrix A."
PROBLEM SETUP,0.04554655870445344,"2.2
PROBLEM SETUP"
PROBLEM SETUP,0.0465587044534413,"Let X ∈Rm0×n denote the data matrix with n data points and m0 features. Without loss of
generality, we assume ∥xi∥2 = 1, ∀i ∈[n]. Consider an L layer neural network with one vector
a ∈RmL and L matrices WL ∈RmL×mL−1, · · · , W2 ∈Rm2×m1 and W1 ∈Rm1×m0. We will use
Wℓ(t) to denote the weight matrix at layer ℓat time t, and ∇Wℓ(t) to denote its gradient. We also
use W(t) = {W1(t), . . . , WL(t)} to denote the collection of weight matrices at time t."
PROBLEM SETUP,0.04757085020242915,"Architecture. We ﬁrst describe our network architecture. The network consists of L hidden layers,
each represented by a weight matrix Wℓ∈Rmℓ×mℓ−1 for any ℓ∈[L]. The output layer consists of
a vector a ∈RmL. We deﬁne the neural network prediction function f : Rm0 →R as follows:"
PROBLEM SETUP,0.048582995951417005,"f(W, x) = a⊤φ(WL(φ(· · · φ(W1x)))),
where φ : R →R is the shifted ReLU activation function (σb(x) = max{x −b, 0}) applied
coordinate-wise to a vector."
PROBLEM SETUP,0.04959514170040486,We measure the loss via squared-loss function:
PROBLEM SETUP,0.05060728744939271,"L(W) = 1 2 n
X"
PROBLEM SETUP,0.05161943319838057,"i=1
(yi −f(W, xi))2."
PROBLEM SETUP,0.05263157894736842,This is also the objective function for our training.
PROBLEM SETUP,0.053643724696356275,We deﬁne the prediction function ft : Rm0×n →Rn as
PROBLEM SETUP,0.05465587044534413,"ft(X) = [f(W(t), x1)
f(W(t), x2)
· · ·
f(W(t), xn)]⊤."
PROBLEM SETUP,0.05566801619433198,Initialization. Our neural networks are initialized as follows:
PROBLEM SETUP,0.05668016194331984,"• For each ℓ∈[L], the initial weight matrix Wℓ(0) ∈Rmℓ×mℓ−1 is initialized such that each
entry is sampled from N(0,
2
mℓ)."
PROBLEM SETUP,0.057692307692307696,"• Each entry of a is an i.i.d. sample from {−
1
√mL , +
1
√mL } uniformly at random."
PROBLEM SETUP,0.058704453441295545,"Gradient. In order to write gradient in an elegant way, we deﬁne some artiﬁcial variables:
gi,1 = W1xi,
hi,1 = φ(W1xi),
∀i ∈[n]
gi,ℓ= Wℓhi,ℓ−1,
hi,ℓ= φ(Wℓhi,ℓ−1),
∀i ∈[n], ∀ℓ∈[L]\{1}
(1)"
PROBLEM SETUP,0.0597165991902834,"Di,1 = diag
 
φ′(W1xi)

,
∀i ∈[n]"
PROBLEM SETUP,0.06072874493927125,"Di,ℓ= diag
 
φ′(Wℓhi,ℓ−1)

,
∀i ∈[n], ∀ℓ∈[L]\{1}"
PROBLEM SETUP,0.06174089068825911,"Using the deﬁnitions of f and h, we have"
PROBLEM SETUP,0.06275303643724696,"f(W, xi) = a⊤hi,L,
∈R,
∀i ∈[n]"
PROBLEM SETUP,0.06376518218623482,"We can compute the gradient of L in terms of Wℓ∈Rmℓ×mℓ−1, for all ℓ≥2 ∂L(W) ∂Wℓ
= n
X"
PROBLEM SETUP,0.06477732793522267,"i=1
(f(W, xi) −yi) Di,ℓ
|{z}
mℓ×mℓ  
 L
Y"
PROBLEM SETUP,0.06578947368421052,"k=ℓ+1
W ⊤
k
|{z}
mk−1×mk"
PROBLEM SETUP,0.06680161943319839,"Di,k
|{z}
mk×mk "
PROBLEM SETUP,0.06781376518218624,"

a
|{z}
mL×1
h⊤
i,ℓ−1
| {z }
1×mℓ−1 (2)"
PROBLEM SETUP,0.06882591093117409,"Note that the gradient for W1 ∈Rm1×m0 (recall that m0 = d) is slightly different and can not be
written by general form. By the chain rule, we can compute the gradient with respect to W1, ∂L(W) ∂W1
= n
X"
PROBLEM SETUP,0.06983805668016195,"i=1
(f(W, xi) −yi) Di,1
|{z}
m1×m1  
 L
Y"
PROBLEM SETUP,0.0708502024291498,"k=2
W ⊤
k
|{z}
mk−1×mk"
PROBLEM SETUP,0.07186234817813765,"Di,k
|{z}
mk×mk "
PROBLEM SETUP,0.0728744939271255,"

a
|{z}
mL×1
x⊤
i
|{z}
1×m0 (3)"
PROBLEM SETUP,0.07388663967611336,Under review as a conference paper at ICLR 2022
PROBLEM SETUP,0.07489878542510121,"It is worth noting that the gradient matrix is of rank n, since it’s a sum of n rank-1 matrices."
PROBLEM SETUP,0.07591093117408906,"Jacobian. For each layer ℓ∈[L] and time t ∈[T], we deﬁne the Jacobian matrix Jℓ,t ∈Rn×mℓmℓ−1
via the following formulation:"
PROBLEM SETUP,0.07692307692307693,"Jℓ,t :=
h
vec( ∂f(W (t),x1)"
PROBLEM SETUP,0.07793522267206478,"∂Wℓ(t)
)
vec( ∂f(W (t),x2)"
PROBLEM SETUP,0.07894736842105263,"∂Wℓ(t)
)
· · ·
vec( ∂f(W (t),xn)"
PROBLEM SETUP,0.07995951417004049,"∂Wℓ(t)
)
i⊤
."
PROBLEM SETUP,0.08097165991902834,"The Gram matrix at layer ℓand time t is then deﬁned as Gℓ,t = Jℓ,tJ⊤
ℓ,t ∈Rn×n whose (i, j)-th"
PROBLEM SETUP,0.08198380566801619,"entry is ⟨∂f(W (t),xi)"
PROBLEM SETUP,0.08299595141700405,"∂Wℓ
, ∂f(W (t),xj)"
PROBLEM SETUP,0.0840080971659919,"∂Wℓ
⟩."
TECHNIQUE OVERVIEW,0.08502024291497975,"3
TECHNIQUE OVERVIEW"
TECHNIQUE OVERVIEW,0.0860323886639676,"In this section, we give an overview of the techniques employed in this paper. In Section 3.1, we
showcase our algorithm and explain various techniques being used to obtain a subquadratic cost per
iteration. In Section 3.2, we give an overview of the proof to show the convergence of our algorithm.
To give a simpler and cleaner presentation, we assume mℓ= m for all ℓ∈[L]."
SUBQUADRATIC TIME,0.08704453441295547,"3.1
SUBQUADRATIC TIME"
SUBQUADRATIC TIME,0.08805668016194332,"In this section, we overview the techniques deployed in our implementation of the second-order
method. Our main focus is to achieve subquadratic cost per iteration. Instead of using a Hessian
matrix of size m2 × m2, we use an n × n Gram matrix derived from the neural tangent kernel.
However this would still incur a cost of O(nm2) per iteration, since each gradient is an m × m
matrix and the Jacobian consists of n such gradients."
SUBQUADRATIC TIME,0.08906882591093117,We start by demonstrating our algorithm:
SUBQUADRATIC TIME,0.09008097165991903,Algorithm 1 Informal version of our algorithm.
SUBQUADRATIC TIME,0.09109311740890688,"1: procedure OURALGORITHM(f, {xi, yi}i∈[n])
▷Theorem 1.1,1.2
2:
/*Initialization*/
3:
Initialize Wℓ(0), ∀ℓ∈[L]
4:
Store W1(0)xi in memory, ∀i ∈[n]
▷Takes O(nm2) time
5:
Store hi,ℓ(0) ←φ(Wℓ(0)hi,ℓ−1(0)), ∀ℓ∈[L], i ∈[n] in memory
▷Takes O(nLm2) time
6:
for t = 0 →T do
7:
/*Forward computation*/
8:
for ℓ= 1 →L do
9:
vi,ℓ←hi,ℓ−1, ∀i ∈[n]
10:
hi,ℓ←φ((Wℓ(0) + ∆Wℓ)(hi,ℓ−1)), ∀i ∈[n]
▷Takes O(n2m) + o(nm2) time
11:
▷hi,ℓis sparse
12:
Di,ℓ←diag(φ′((Wℓ(0) + ∆Wℓ)hi,ℓ−1)), ∀i ∈[n]
▷Takes O(nm) time
13:
▷Di,ℓis sparse
14:
end for
15:
ft ←[a⊤h1,L, . . . , a⊤hn,L]⊤
▷Takes O(nm) time
16:
/*Backward computation*/
17:
for ℓ= L →1 do
18:
ui,ℓ←a⊤Di,LWL(t) . . . Di,ℓ+1Wℓ+1(t)Di,ℓ
▷Takes o(nLm2) time"
SUBQUADRATIC TIME,0.09210526315789473,"19:
Form eJℓ,t that approximates Jℓ,t using {ui,ℓ}n
i=1, {vi,ℓ}n
i=1
20:
▷Takes eO(mn) time, eJℓ,t ∈Rn×s where s = eO(n)"
SUBQUADRATIC TIME,0.0931174089068826,"21:
Compute gℓthat approximates ( eJℓ,t eJ⊤
ℓ,t)−1c
▷Takes eO(nm) time"
SUBQUADRATIC TIME,0.09412955465587045,"22:
Form J⊤
ℓ,tgℓvia low rank factorization Pn
i=1 gℓ,iui,ℓv⊤
i,ℓ
23:
Implicitly update ∆Wℓ←∆Wℓ+ Pn
i=1 gℓ,iui,ℓv⊤
i,ℓand store it in memory
24:
end for
25:
end for
26: end procedure"
SUBQUADRATIC TIME,0.0951417004048583,Under review as a conference paper at ICLR 2022
SUBQUADRATIC TIME,0.09615384615384616,"Step 1: Invert Gram by solving regression. Recall the update rule of generic algorithm is given
by"
SUBQUADRATIC TIME,0.09716599190283401,"Wℓ(t + 1) ←Wℓ(t) −J⊤
ℓ,t(Jℓ,tJ⊤
ℓ,t)c,"
SUBQUADRATIC TIME,0.09817813765182186,"where c is ft −y after proper scaling. Naively forming the Gram matrix Jℓ,tJ⊤
ℓ,t will take O(n2m2)
time and inverting it will take O(nω) time. To avoid the quadratic cost at this step, we instead solve
a regression, or a linear system since the Gram matrix has full rank: ﬁnd the vector gℓ,t ∈Rn such
that"
SUBQUADRATIC TIME,0.09919028340080972,"∥Jℓ,tJ⊤
ℓ,tgℓ,t −c∥2
2
is minimized. This enables us to utilize the power of sketching to solve the regression efﬁciently."
SUBQUADRATIC TIME,0.10020242914979757,"Step 2: Solve Gram regression via preconditioning. In order to solve the above regression, we
adapt the idea of obtaining a good preconditioner via sketching then apply iterative method to solve
it (Brand et al. (2021)). Roughly speaking, we ﬁrst use a random matrix S ∈Rs×m2 that has the
subspace embedding property (Sarlos (2006)) to reduce the number of rows of J⊤, then we run a QR
decomposition on matrix SJ⊤. This gives us a matrix R such that SJ⊤R has orthonormal columns.
We then use gradient descent to optimize the objective ∥JJ⊤Rzt −y∥2
2. Since S is a subspace
embedding for J⊤, we can make sure that the condition number of the matrix J⊤R is small (O(1)),
hence the gradient descent converges after log(κ/ϵ) iterations, where κ is the condition number of
J. However, in order to implement the gradient descent, we still need to multiply an m2 × n matrix
with a length n vector, in the worst case this will incur a time of O(nm2). In order to bypass this
barrier, we need to exploit extra structural properties of the Jocobian, which will be demonstrated in
the following steps."
SUBQUADRATIC TIME,0.10121457489878542,"Step 3: Low rank structure of the gradient. Instead of studying Jacobian directly, we ﬁrst try to
understand the low rank structure of the gradient. Consider ∂f(W,xi)"
SUBQUADRATIC TIME,0.10222672064777327,"∂Wℓ
∈Rm×m, it can be written as
(for simplicity, we use hi,0 to denote xi):"
SUBQUADRATIC TIME,0.10323886639676114,"∂f(W, xi)"
SUBQUADRATIC TIME,0.10425101214574899,"∂Wℓ
=
hi,ℓ−1
| {z }
vi∈Rm×1"
SUBQUADRATIC TIME,0.10526315789473684,"a⊤Di,LWL . . . Di,ℓ+1Wi,ℓ+1Di,ℓ
|
{z
}"
SUBQUADRATIC TIME,0.1062753036437247,"u⊤
i ∈R1×m ."
SUBQUADRATIC TIME,0.10728744939271255,"This means the gradient is essentially an outer product of two vectors, and hence has rank one. This
has several interesting consequences: for over-parametrized networks, the gradient is merely of rank
n instead of m. When using ﬁrst-order method such as gradient descent or stochastic gradient de-
scent, the weight is updated via a low rank matrix. To some extent, this explains why the weight does
not move too far from initialization in over-parametrized networks when using ﬁrst-order method to
train. Also, as we will illustrate below, this enables the efﬁcient approximation of Jacobian matrices
and maintenance of the change."
SUBQUADRATIC TIME,0.1082995951417004,"Step 4: Fast approximation of the Jacobian matrix. We now turn our attention to design a fast
approximation algorithm to the Jacobian matrix. Recall that Jacobian matrix Jℓ,t ∈Rn×m2 is an
n × m2 matrix, therefore writing down the matrix will take O(nm2) time. However, it is worth
noticing that each row of Jℓ,t is vec(uiv⊤
i )⊤, ∀i ∈[n], or equivalently, ui ◦vi where ◦denotes the
tensor product between two vectors. Suppose we are given the collection of {u1, . . . , un} ∈(Rm)n
and {v1, . . . , vn} ∈(Rm)n, then we can compute the tensor product ui ◦vi via tensor-based
sketching techniques, such as TensorSketch (Avron et al. (2014); Diao et al. (2017; 2019)) or
TensorSRHT (Ahle et al. (2020); Woodruff & Zandieh (2020)) in time nearly linear in m and
the targeted sketching dimension s, in contrast to the naive O(m2) time. Since it sufﬁces to pre-
serve the length of all vectors in the column space of J⊤
ℓ,t, the target dimension s can be chosen as
O(ϵ−2n · poly(log(m/ϵδ))). Use eJℓ,t ∈Rn×s to denote this approximation of Jℓ,t, we perform the
preconditioned gradient descent we described above on this smaller matrix. This enables to lower
the overall cost of the regression step to be subquadratic in m."
SUBQUADRATIC TIME,0.10931174089068826,"Step 5: Efﬁcient update via low rank factorization. The low rank structure of the gradient can fur-
ther be utilized to represent the change on weight matrices ∆W in a way such that any matrix-vector
product involving ∆W can be performed fast. Let gℓ∈Rn denote the solution to the regression
problem posed in Step 1. Note that by the update rule of our method, we shall use J⊤
ℓ,tgℓ∈Rm×m"
SUBQUADRATIC TIME,0.11032388663967611,"to update the weight matrix, but writing down the matrix will already take O(m2) time. Therefore,"
SUBQUADRATIC TIME,0.11133603238866396,Under review as a conference paper at ICLR 2022
SUBQUADRATIC TIME,0.11234817813765183,"it is instructive to ﬁnd a succinct representation for the update. The key observation is that each
column of J⊤
ℓ,t is a tensor product of two vectors: ui ◦vi or equivalently, uiv⊤
i . The update matrix
can be rewritten as Pn
i=1 gℓ,iuiv⊤
i , and we can use this representation for the update on the weight,
instead of adding it directly. Let Uℓ:="
SUBQUADRATIC TIME,0.11336032388663968,"""
|
|
. . .
|
gℓ,1u1
gℓ,2u2
. . .
gℓ,nun
|
|
. . .
| #"
SUBQUADRATIC TIME,0.11437246963562753,"∈Rm×n, Vℓ:="
SUBQUADRATIC TIME,0.11538461538461539,""" |
|
. . .
|
v1
v2
. . .
vn
|
|
. . .
| #"
SUBQUADRATIC TIME,0.11639676113360324,"∈Rm×n,"
SUBQUADRATIC TIME,0.11740890688259109,"then the update can be represented as UℓV ⊤
ℓ. Consider multiplying a vector y ∈Rm with this
representation, we ﬁrst multiply y with V ⊤
ℓ
∈Rn×m, which takes O(mn) time. Then we multi-
ply V ⊤
ℓy ∈Rn with Uℓ∈Rm×n which takes O(mn) time. This drastically reduces the cost of
multiplying the weight matrix with a vector from O(m2) to O(mn)."
SUBQUADRATIC TIME,0.11842105263157894,"Inspired by this idea, it is tempting to store all intermediate low rank representations across all
iterations and use them to facilitate matrix-vector product, which incurs a runtime of O(Tmn). This
is ﬁne when T is relatively small, however, if one looks for a high precision solution which requires
a large number of iterations, then T might be too large and O(Tmn) might be in the order of O(m2).
To circumvent this problem, we design the data structure so that it will exactly compute the m × m
change matrix and update the weight and clean up the cumulative changes. This can be viewed as a
“restart” of the data structure. To choose the correct number of updates before restarting, we utilize
the dual exponent of matrix multiplication, α (Gall & Urrutia, 2018), which means it takes O(m2)
time to multiply an m × m by an m × mα matrix. Hence, we restart the data structure after around
mα/n updates. Therefore, we achieve an amortized o(m2) time, which is invariant even though the
number of iterations T grows larger and larger."
CONVERGENCE ANALYSIS,0.1194331983805668,"3.2
CONVERGENCE ANALYSIS"
CONVERGENCE ANALYSIS,0.12044534412955465,"In this section, we demonstrate the strategy to prove that our second-order method achieves a linear
convergence rate on the training loss."
CONVERGENCE ANALYSIS,0.1214574898785425,"Step 1: Initialization. Let W(0) be the random initialization. We ﬁrst show that for any data point
xi, we have f(W(0), xi) = O(1). The analysis draws inspiration from Allen-Zhu et al. (2019a).
The general idea is, given a ﬁxed unit length vector x, multiplying it with a random Gaussian matrix
W will make sure that ∥Wx∥2
2 ≈2. Since W is a random Gaussian matrix, applying shifted ReLU
activation gives a random vector with a truncated Gaussian distribution conditioned on a binomial
random variable indicating which neurons are activated. We will end up with ∥φ(Wx)∥2 ≈1 as
well as φ(Wx) being sparse. Inductively applying this idea to each layer and carefully controlling
the error occurring at each layer, we can show that with good probability, f(W(0), xi) is a constant."
CONVERGENCE ANALYSIS,0.12246963562753037,"We also bound the spectral norm of ∂f(W (0),xi)"
CONVERGENCE ANALYSIS,0.12348178137651822,"∂Wℓ
by eO(
p"
CONVERGENCE ANALYSIS,0.12449392712550607,"L/m). One of the key part of this matrix is
the consecutive product DLWL . . . Dℓ+1Wℓ+1Dℓ. By studying the distribution of its product with a
ﬁxed vector, one can show that the spectral norm of this consecutive product is bounded by O(
√"
CONVERGENCE ANALYSIS,0.12550607287449392,"L).
Finally, we make use the fact that each entry of a is a Rademacher random variable scaled by 1/√m,
hence the norm of a⊤DLWL . . . Dℓ+1Wℓ+1Dℓis bounded by eO(
p"
CONVERGENCE ANALYSIS,0.12651821862348178,L/m) with good probability.
CONVERGENCE ANALYSIS,0.12753036437246965,"Furthermore, we show that the Gram matrix for the multiple-layer over-parametrized neural net-
work, which is deﬁned as Jℓ,0J⊤
ℓ,0, has a nontrivial minimum eigenvalue after the initialization. In
particular, we adapt the neural tangent kernel (NTK) for multiple-layer neural networks deﬁned by
Du et al. (2019a) into our setting by analyzing the corresponding Gaussian process with shifted
ReLU activation function. Then, we can prove that with high probability, the least eigenvalue of the
initial Gram matrix is lower bounded by the least eigenvalue of the neural tangent kernel matrix."
CONVERGENCE ANALYSIS,0.12854251012145748,"Step 2: Small perturbation. The next step is to show that if all weight matrices undergo a small
perturbation from initialization (in terms of spectral norm), then the corresponding Jacobian matrix
has not changed too much. As long as the perturbation is small enough, it is possible to show that
the change of the h vector (in terms of ℓ2 norm) and the consecutive product (in terms of spectral
norm) is also small. Finally, using the fact that a is a Rademacher vector with scaling 1/√m, we
can show that the change
∂f(W(0) + ∆W, xi)"
CONVERGENCE ANALYSIS,0.12955465587044535,"∂(Wℓ+ ∆Wℓ)
−∂f(W(0), xi) ∂Wℓ"
CONVERGENCE ANALYSIS,0.1305668016194332,Under review as a conference paper at ICLR 2022
CONVERGENCE ANALYSIS,0.13157894736842105,"has its spectral norm being bounded by eO(
p"
CONVERGENCE ANALYSIS,0.1325910931174089,"L/m) for any layer ℓ∈[L] and input data i ∈[n].
Consequently, the Frobenious norm of the Jacobian matrix is bounded by eO(
p"
CONVERGENCE ANALYSIS,0.13360323886639677,nL/m).
CONVERGENCE ANALYSIS,0.1346153846153846,"Step 3: Connect everything via a double induction. Put things together, we use a double induction
argument, where we assume the perturbation of weight matrix is small and the gap between ft and
y is at most 1/2 of the gap between ft−1 and y. By carefully bounding various terms and exploiting
the fact the Jacobian matrix always has a relative small spectral norm ( eO(
p"
CONVERGENCE ANALYSIS,0.13562753036437247,"nL/m)), we ﬁrst show
that the weights are not moving too far from the initialization, then use this fact to derive a ﬁnal
convergence bound for ∥ft −y∥2."
DISCUSSION AND FUTURE DIRECTIONS,0.13663967611336034,"4
DISCUSSION AND FUTURE DIRECTIONS"
DISCUSSION AND FUTURE DIRECTIONS,0.13765182186234817,"In this work, we propose and analyze a second-order method to train multi-layer over-parametrized
neural networks. Our algorithm achieves a linear convergence rate in terms of training loss, and
achieves a subquadratic (o(m2)) cost per training iteration. From an analytical perspective, we
greatly extend the analysis of (Allen-Zhu et al. (2019a)) to second-order method, coupled with
the usage of the equivalence between multi-layer over-parametrized networks and neural tangent
kernels (Du et al. (2019a)). From an algorithmic perspective, we achieve a subquadratic cost per
iteration, which is a signiﬁcant improvement from O(m2n) time per iteration due to the prohibitively
large network width m. Our algorithm combines various techniques, such as training with the Gram
matrix, solve the Gram regression via sketching-based preconditioning, fast tensor computation and
dimensionality reduction and low rank decomposition of weight updates. Our algorithm is especially
valuable when one requires a high precision solution on training loss, and hence the number of
iterations is large."
DISCUSSION AND FUTURE DIRECTIONS,0.13866396761133604,"One of the interesting questions from our work is: is it possible to obtain an algorithm that has a
nearly linear cost per iteration on m as in the case of training one-hidden layer over-parametrized
networks (Brand et al. (2021))? In particular, can this runtime be achieved under the current best
width of multi-layer over-parametrized networks (m ≥n8)? We note that the major limitation in
our method is the sparsity of the change of the diagonal matrices (∆D) is directly related to the
magnitude of the change of weights (∥∆W∥). In our analysis of convergence, we go through a
careful double induction argument, which in fact imposes on a lower bound on ∥∆W∥. It seems to
us that, in order to achieve a nearly linear runtime, one has to adapt a different analytical framework
or approach the problem from a different perspective."
DISCUSSION AND FUTURE DIRECTIONS,0.1396761133603239,"A related question is, how can we maintain the changes of weight more efﬁciently? In our work, we
achieve speedup in the neural network training process by observing that the change of the weights
are small in each iteration. Similar phenomenon also appears in some classical optimization problem
(e.g., solving linear program (Cohen et al., 2019; Jiang et al., 2021) and solving semideﬁnite pro-
gram (Jiang et al., 2020a)) and they achieve further speedup by using lazy update and amortization
techniques to compute the weight changes, or using more complicated data structure to maintain the
changes of the weight changes. Can we adapt their techniques to neural network training? An or-
thogonal direction to maintain the change is to design an initialization setup such that while we still
have enough randomness to obtain provable guarantees, the matrix-vector product with the initial
weight matrix can be performed faster than O(m2) by sparsifying the Gaussian matrix as in Derez-
i´nski et al. (2021) or imposing extra structural assumption such as using circulant Gaussian (Rauhut
et al., 2012; Nelson & NguyÅn, 2013; Krahmer et al., 2014)."
DISCUSSION AND FUTURE DIRECTIONS,0.14068825910931174,"Another question concerns activation functions. In this paper, we consider the shifted ReLU acti-
vation and design our algorithm and analysis around its properties. Is it possible to generalize our
algorithm and analysis to various other activation functions, such as sigmoid, tanh or leaky ReLU?
If one chooses a smooth activation, can we get a better result in terms of convergence rate? Can we
leverage this structure to design faster algorithms?"
DISCUSSION AND FUTURE DIRECTIONS,0.1417004048582996,"Finally, the network architecture considered in this paper is the standard feedforward network. Is
it possible to extend our analysis and algorithm to other architectures, such as recurrent neural
networks (RNN)? For RNN, the weight matrices for each layer are the same, hence it is trickier to
analyze the training dynamics on such networks. Though the convergence of ﬁrst-order method on
over-parametrized multi-layer RNN has been established, it is unclear whether such analysis can be
extended to second-order method."
DISCUSSION AND FUTURE DIRECTIONS,0.14271255060728744,Under review as a conference paper at ICLR 2022
ETHICS STATEMENT,0.1437246963562753,"Ethics Statement. This is a theory paper that proposed efﬁcient algorithm to train multi-layer over-
parametrized neural networks, hence it does not have direct ethics implications."
REPRODUCIBILITY STATEMENT,0.14473684210526316,"Reproducibility Statement. All results in this paper can be directly veriﬁed and reproduced via
reading into the proofs. For complete algorithm description and its runtime analysis, see Section B.
For speciﬁc data structure used in the algorithm, see Section C. For the result regarding the fast
tensor regression, see Section D. For convergence analysis of the algorithm, see Section E, F and G."
REFERENCES,0.145748987854251,REFERENCES
REFERENCES,0.14676113360323886,"Naman Agarwal, Brian Bullins, and Elad Hazan. Second-order stochastic optimization for machine
learning in linear time. 2017."
REFERENCES,0.14777327935222673,"Thomas D. Ahle, Michael Kapralov, Jakob Bæk Tejs Knudsen, Rasmus Pagh, Ameya Velingker,
David P. Woodruff, and Amir Zandieh. Oblivious sketching of high-degree polynomial kernels.
In Proceedings of the 2020 ACM-SIAM Symposium on Discrete Algorithms (SODA), pp. 141–160,
2020."
REFERENCES,0.14878542510121456,"Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In ICML, 2019a."
REFERENCES,0.14979757085020243,"Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of training recurrent neural
networks. In NeurIPS, 2019b."
REFERENCES,0.1508097165991903,"Haim Avron, Huy L. Nguyen, and David P. Woodruff. Subspace embeddings for the polynomial
kernel. In NeurIPS, 2014."
REFERENCES,0.15182186234817813,"Alberto Bernacchia, Mate Lengyel, and Guillaume Hennequin. Exact natural gradient in deep linear
networks and its application to the nonlinear case. In S. Bengio, H. Wallach, H. Larochelle,
K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing
Systems. Curran Associates, Inc., 2018."
REFERENCES,0.152834008097166,"Sergei Bernstein. On a modiﬁcation of chebyshev’s inequality and of the error formula of laplace.
Ann. Sci. Inst. Sav. Ukraine, Sect. Math, 1(4):38–49, 1924."
REFERENCES,0.15384615384615385,"Åke Björck. Numerical Methods for Least Squares Problems. Society for Industrial and Applied
Mathematics, 1996."
REFERENCES,0.1548582995951417,"Aleksandar Botev, Hippolyt Ritter, and David Barber. Practical Gauss-Newton optimisation for
deep learning. In Proceedings of the 34th International Conference on Machine Learning, pp.
557–565, 2017."
REFERENCES,0.15587044534412955,"Jan van den Brand, Binghui Peng, Zhao Song, and Omri Weinstein. Training (overparametrized)
neural networks in near-linear time. In ITCS, 2021."
REFERENCES,0.15688259109311742,"Tianle Cai, Ruiqi Gao, Jikai Hou, Siyu Chen, Dong Wang, Di He, Zhihua Zhang, and Liwei Wang.
Gram-gauss-newton method: Learning overparameterized neural networks for regression prob-
lems. arXiv preprint arXiv:1905.11675, 2019."
REFERENCES,0.15789473684210525,"Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and
deep neural networks. In NeurIPS, pp. 10835–10845, 2019."
REFERENCES,0.15890688259109312,"Zixiang Chen, Yuan Cao, Difan Zou, and Quanquan Gu. How much over-parameterization is suf-
ﬁcient to learn deep ReLU networks? In International Conference on Learning Representations
(ICLR), 2021."
REFERENCES,0.15991902834008098,"Herman Chernoff. A measure of asymptotic efﬁciency for tests of a hypothesis based on the sum of
observations. The Annals of Mathematical Statistics, pp. 493–507, 1952."
REFERENCES,0.16093117408906882,"Kenneth L. Clarkson and David P. Woodruff.
Low rank approximation and regression in input
sparsity time. In Symposium on Theory of Computing Conference (STOC), pp. 81–90, 2013."
REFERENCES,0.16194331983805668,"Michael B Cohen, Yin Tat Lee, and Zhao Song. Solving linear programs in the current matrix
multiplication time. In STOC, 2019."
REFERENCES,0.16295546558704455,Under review as a conference paper at ICLR 2022
REFERENCES,0.16396761133603238,"Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel
Kuksa. Natural language processing (almost) from scratch. Journal of machine learning research,
12:2493–2537, 2011."
REFERENCES,0.16497975708502025,"Samuel I Daitch and Daniel A Spielman. Faster approximate lossy generalized ﬂow via interior
point algorithms. In Proceedings of the fortieth annual ACM symposium on Theory of computing
(STOC), pp. 451–460, 2008."
REFERENCES,0.1659919028340081,"Michał Derezi´nski, Jonathan Lacotte, Mert Pilanci, and Michael W. Mahoney. Newton-less: Spar-
siﬁcation without trade-offs for the sketched newton update, 2021."
REFERENCES,0.16700404858299595,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018."
REFERENCES,0.1680161943319838,"Huaian Diao, Zhao Song, Wen Sun, and David Woodruff. Sketching for kronecker product regres-
sion and p-splines. In AISTATS, 2017."
REFERENCES,0.16902834008097167,"Huaian Diao, Rajesh Jayaram, Zhao Song, Wen Sun, and David Woodruff. Optimal sketching for
kronecker product regression and low rank approximation. In Advances in Neural Information
Processing Systems (NeurIPS), 2019."
REFERENCES,0.1700404858299595,"Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent ﬁnds global
minima of deep neural networks. In International Conference on Machine Learning (ICML),
2019a."
REFERENCES,0.17105263157894737,"Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In ICLR, 2019b."
REFERENCES,0.1720647773279352,"François Le Gall and Florent Urrutia. Improved rectangular matrix multiplication using powers
of the coppersmith-winograd tensor. In Proceedings of the Twenty-Ninth Annual ACM-SIAM
Symposium on Discrete Algorithms, SODA ’18, pp. 1029–1046, 2018."
REFERENCES,0.17307692307692307,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition
(CVPR), pp. 770–778, 2016."
REFERENCES,0.17408906882591094,"Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the
American Statistical Association, 58(301):13–30, 1963."
REFERENCES,0.17510121457489877,"Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: convergence and gen-
eralization in neural networks. In Proceedings of the 32nd International Conference on Neural
Information Processing Systems (NeurIPS), pp. 8580–8589, 2018."
REFERENCES,0.17611336032388664,"Ziwei Ji and Matus Telgarsky. Polylogarithmic width sufﬁces for gradient descent to achieve arbi-
trarily small test error with shallow relu networks. In ICLR, 2020."
REFERENCES,0.1771255060728745,"Haotian Jiang, Tarun Kathuria, Yin Tat Lee, Swati Padmanabhan, and Zhao Song. A faster interior
point method for semideﬁnite programming. In FOCS, 2020a."
REFERENCES,0.17813765182186234,"Haotian Jiang, Yin Tat Lee, Zhao Song, and Sam Chiu-wai Wong. An improved cutting plane
method for convex optimization, convex-concave games and its applications. In STOC, 2020b."
REFERENCES,0.1791497975708502,"Shunhua Jiang, Zhao Song, Omri Weinstein, and Hengjie Zhang. Faster dynamic matrix inverse for
faster lps. In STOC, 2021."
REFERENCES,0.18016194331983806,"Felix Krahmer, Shahar Mendelson, and Holger Rauhut. Suprema of chaos processes and the re-
stricted isometry property. Communications on Pure and Applied Mathematics, 67(11):1877–
1904, 2014."
REFERENCES,0.1811740890688259,"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep con-
volutional neural networks. Advances in neural information processing systems, 25:1097–1105,
2012."
REFERENCES,0.18218623481781376,"Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998."
REFERENCES,0.18319838056680163,Under review as a conference paper at ICLR 2022
REFERENCES,0.18421052631578946,"Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide Neural Networks of Any Depth Evolve as Linear Models
under Gradient Descent. 2019a."
REFERENCES,0.18522267206477733,"Yin Tat Lee, Aaron Sidford, and Sam Chiu-wai Wong. A faster cutting plane method and its impli-
cations for combinatorial and convex optimization. In Foundations of Computer Science (FOCS),
2015 IEEE 56th Annual Symposium on, pp. 1049–1065. IEEE, 2015."
REFERENCES,0.1862348178137652,"Yin Tat Lee, Zhao Song, and Qiuyi Zhang. Solving empirical risk minimization in the current matrix
multiplication time. In Conference on Learning Theory (COLT), pp. 2140–2157. PMLR, 2019b."
REFERENCES,0.18724696356275303,"Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In NeurIPS, 2018."
REFERENCES,0.1882591093117409,"Chaoyue Liu, Libin Zhu, and Misha Belkin. On the linearity of large non-linear models: when and
why the tangent kernel is constant. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and
H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 15954–15964.
Curran Associates, Inc., 2020."
REFERENCES,0.18927125506072875,"Chaoyue Liu, Libin Zhu, and Mikhail Belkin.
Loss landscapes and optimization in over-
parameterized non-linear systems and neural networks, 2021."
REFERENCES,0.1902834008097166,"Yichao Lu, Paramveer Dhillon, Dean P Foster, and Lyle Ungar. Faster ridge regression via the sub-
sampled randomized hadamard transform. In Advances in neural information processing systems
(NIPS), pp. 369–377, 2013."
REFERENCES,0.19129554655870445,"James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate
curvature. In Proceedings of the 32nd International Conference on International Conference on
Machine Learning - Volume 37, ICML’15, pp. 2408–2417. JMLR.org, 2015."
REFERENCES,0.19230769230769232,"Jelani Nelson and Huy L NguyÅn. Sparsity lower bounds for dimensionality reducing maps. In
Proceedings of the forty-ﬁfth annual ACM symposium on Theory of computing, pp. 101–110,
2013."
REFERENCES,0.19331983805668015,"Mert Pilanci and Martin J. Wainwright. Newton sketch: A near linear-time optimization algorithm
with linear-quadratic convergence. SIAM J. Optim., 27:205–245, 2017."
REFERENCES,0.19433198380566802,"Holger Rauhut, Justin Romberg, and Joel A Tropp. Restricted isometries for partial random circulant
matrices. Applied and Computational Harmonic Analysis, 32(2):242–254, 2012."
REFERENCES,0.19534412955465588,"Mark Rudelson and Roman Vershynin. Non-asymptotic theory of random matrices: extreme singu-
lar values. In Proceedings of the International Congress of Mathematicians 2010 (ICM 2010) (In
4 Volumes) Vol. I: Plenary Lectures and Ceremonies Vols. II–IV: Invited Lectures, pp. 1576–1602.
World Scientiﬁc, 2010."
REFERENCES,0.19635627530364372,"Tamas Sarlos. Improved approximation algorithms for large matrices via random projections. In
2006 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS), pp. 143–152.
IEEE, 2006."
REFERENCES,0.19736842105263158,"David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484–489, 2016."
REFERENCES,0.19838056680161945,"David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. nature, 550(7676):354–359, 2017."
REFERENCES,0.19939271255060728,"Zhao Song and Xin Yang. Quadratic sufﬁces for over-parametrization via matrix chernoff bound.
arXiv preprint arXiv:1906.03593, 2019."
REFERENCES,0.20040485829959515,"Zhao Song and Zheng Yu. Oblivious sketching-based central path method for linear programming.
In International Conference on Machine Learning (ICML), pp. 9835–9847. PMLR, 2021."
REFERENCES,0.201417004048583,Under review as a conference paper at ICLR 2022
REFERENCES,0.20242914979757085,"Zhao Song, David P. Woodruff, Zheng Yu, and Lichen Zhang. Fast sketching of polynomial kernels
of polynomial degree. In ICML, 2021."
REFERENCES,0.2034412955465587,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1–9, 2015."
REFERENCES,0.20445344129554655,"Pravin M Vaidya. Speeding-up linear programming using fast matrix multiplication. In 30th Annual
Symposium on Foundations of Computer Science, pp. 332–337. IEEE, 1989."
REFERENCES,0.2054655870445344,"Virginia Vassilevska Williams. Multiplying matrices faster than coppersmith-winograd. In Proceed-
ings of the forty-fourth annual ACM symposium on Theory of computing (STOC), pp. 887–898.
ACM, 2012."
REFERENCES,0.20647773279352227,"David P. Woodruff. Sketching as a tool for numerical linear algebra. Foundations and Trends in
Theoretical Computer Science, 10(1–2):1–157, 2014."
REFERENCES,0.2074898785425101,"David P Woodruff and Amir Zandieh. Near input sparsity time kernel embeddings via adaptive
sampling. In ICML, 2020."
REFERENCES,0.20850202429149797,"Zhewei Yao, Amir Gholami, Sheng Shen, Mustafa Mustafa, Kurt Keutzer, and Michael Mahoney.
Adahessian: An adaptive second order optimizer for machine learning. Proceedings of the AAAI
Conference on Artiﬁcial Intelligence, 35(12):10665–10673, May 2021."
REFERENCES,0.20951417004048584,"Guodong Zhang, James Martens, and Roger B Grosse. Fast convergence of natural gradient descent
for over-parameterized neural networks. In Advances in Neural Information Processing Systems
(NeurIPS), 2019."
REFERENCES,0.21052631578947367,"Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural
networks. In NeurIPS, pp. 2053–2062, 2019."
REFERENCES,0.21153846153846154,"Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep relu networks, 2018."
REFERENCES,0.2125506072874494,Under review as a conference paper at ICLR 2022
REFERENCES,0.21356275303643724,"Roadmap. In Section A, we remind readers with the notations and some probability tools. In
Section B, we illustrate the complete version of our algorithm and give a runtime analysis of it.
In Section C, we design a simple low rank maintenance data structure and show how to use it to
efﬁciently implement matrix-vector product. In Section D, we introduce an efﬁcient regression
solver handling our Jacobian and Gram regression. In Section E, we study the spectral property of
the Gram matrix at each layer and connects it with multi-layer neural tangent kernels. In Section F,
we analyze the convergence of our algorithm by using some heavy machinery such as structural
analysis of the gradient and a careful double induction. In Section G, we give a detailed proof of
one technical lemma."
REFERENCES,0.2145748987854251,"A
PRELIMINARIES AND PROBABILITY TOOLS"
REFERENCES,0.21558704453441296,"In this section, we introduce notations that will be used throughout the rest of the paper and several
useful probability tools that will be heavily exploited in the later proofs."
REFERENCES,0.2165991902834008,"Notations. For any positive integer n, we use [n] to denote the set {1, 2, · · · , n}. We use E[·] to
denote expectation and Pr[·] for probability. We use ∥x∥2 to denote the ℓ2 norm of a vector x. We
use ∥A∥to denote the spectral norm of matrix A. We use ∥A∥F to denote the Frobenius norm of
A. We use A⊤to denote the transpose of matrix A. We use Im to denote the identity matrix of size
m × m. For matrix A or vector x, we use ∥A∥0, ∥x∥0 to denote the number of nonzero entries of
A and x respectively. Note that ∥· ∥0 is a semi-norm since it satisﬁes triangle inequality. Given a
real square matrix A, we use λmax(A) and λmin(A) to denote its largest and smallest eigenvalues
respectively. Given a real matrix A, we use σmax(A) and σmin(A) to denote its largest and smallest
singular values respectively. We use N(µ, σ2) to denote the Gaussian distribution with mean µ and
variance σ2. We use eO(f(n)) to denote O(f(n) · poly log(f(n)). We use ⟨·, ·⟩to denote the inner
product, when applying to two vectors, this denotes the standard dot product between two vectors,
and when applying to two matrices, this means ⟨A, B⟩= tr[A⊤B] where tr[A] denote the trace of
matrix A.
Lemma A.1 (Chernoff bound Chernoff (1952)). Let X = Pn
i=1 Xi, where Xi = 1 with probability
pi and Xi = 0 with probability 1 −pi, and all Xi are independent. Let µ = E[X] = Pn
i=1 pi. Then
1. Pr[X ≥(1 + δ)µ] ≤exp(−δ2µ/3), ∀δ > 0 ;
2. Pr[X ≤(1 −δ)µ] ≤exp(−δ2µ/2), ∀0 < δ < 1.
Lemma A.2 (Hoeffding bound Hoeffding (1963)). Let X1, · · · , Xn denote n independent bounded
variables in [ai, bi]. Let X = Pn
i=1 Xi, then we have"
REFERENCES,0.21761133603238866,"Pr[|X −E[X]| ≥t] ≤2 exp

−
2t2
Pn
i=1(bi −ai)2 
."
REFERENCES,0.21862348178137653,"Lemma A.3 (Bernstein inequality Bernstein (1924)). Let X1, · · · , Xn be independent zero-mean
random variables. Suppose that |Xi| ≤M almost surely, for all i. Then, for all positive t, Pr "" n
X"
REFERENCES,0.21963562753036436,"i=1
Xi > t # ≤exp "
REFERENCES,0.22064777327935223,"−
t2/2
Pn
j=1 E[X2
j ] + Mt/3 ! ."
REFERENCES,0.2216599190283401,"Lemma A.4 (Anti-concentration of Gaussian distribution). Let X ∼N(0, σ2), then"
REFERENCES,0.22267206477732793,Pr[|X| ≤t] ∈(2
T,0.2236842105263158,"3
t
σ , 4"
T,0.22469635627530365,"5
t
σ )."
T,0.2257085020242915,"Lemma A.5 (Concentration of subgaussian random variables). Let a ∈Rn be a vector where each
entry of a is sampled from a subgaussian distribution of parameter σ2, then for any vector x ∈Rn,"
T,0.22672064777327935,"Pr[|⟨a, x⟩| ≥t · ∥x∥2] ≤2 exp(−t2"
T,0.22773279352226722,2σ2 ).
T,0.22874493927125505,"Lemma A.6 (Small ball probability). Let a ∈Rn be a vector such that |ai| ≥δ for all i ∈[n],
x1, . . . , xn are n i.i.d. Rademacher random variables. Then, there exist absolute constants C1, C2
such that for any t > 0,"
T,0.22975708502024292,"Pr[|⟨a, x⟩| ≤t] ≤min
 C1t"
T,0.23076923076923078,"∥a∥2
, C2t δ√n 
."
T,0.23178137651821862,Under review as a conference paper at ICLR 2022
T,0.23279352226720648,"B
COMPLETE ALGORITHM AND ITS RUNTIME ANALYSIS"
T,0.23380566801619435,"In this section, we ﬁrst present our complete algorithm, then give a runtime analysis of it."
T,0.23481781376518218,Algorithm 2 Complete version of our algorithm.
T,0.23582995951417005,"1: procedure COMPLETEALGORITHM(X ∈Rd×n, y ∈Rn)
▷Theorem B.1
2:
/*Initialization*/
3:
Initialize Wℓ(0), ∀ℓ∈[L]
4:
Store W1(0)xi in memory, ∀i ∈[n]
▷Takes O(nm2) time
5:
LOWRANKMAINTENANCE LMR
▷Algorithm 3
6:
LMR.INIT({W1(0) . . . , WL(0)})
7:
for t = 0 →T do
8:
/*Forward computation*/
9:
for ℓ= 1 →L do
10:
vi,ℓ←hi,ℓ−1, ∀i ∈[n]
11:
gi,ℓ←LMR.QUERY(ℓ, hi,ℓ−1)
▷Takes o(nm2) time
12:
hi,ℓ←φ(gi,ℓ), ∀i ∈[n]
13:
▷hi,ℓis sparse
14:
Di,ℓ←diag(φ′(gi,ℓ)), ∀i ∈[n]
▷Takes O(nm) time
15:
▷Di,ℓis sparse
16:
end for
17:
ft ←[a⊤h1,L, . . . , a⊤hn,L]⊤
▷Takes O(nm) time
18:
/*Backward computation*/
19:
for ℓ= L →1 do
20:
ui,ℓ←a⊤Di,LWL(t) . . . Di,ℓ+1Wℓ+1(t)Di,ℓ, ∀i ∈[n]
▷Takes o(nLm2) time
21:
gℓ←FASTTENSORREGRESSION({ui,ℓ}n
i=1, {vi,ℓ}n
i=1, c) with precision
p"
T,0.23684210526315788,"λ/n
22:
▷Algorithm 6
23:
LMR.UPDATE({gℓ,iui,ℓ}n
i=1, {vi,ℓ}n
i=1)
24:
end for
25:
end for
26: end procedure"
T,0.23785425101214575,"Theorem B.1 (Formal version of Theorem 1.2). Let X ∈Rd×n and y ∈Rn, and let k denote the
sparsity of Di,ℓand s denote the sparsity of ∆Di,ℓ, ∀ℓ∈[L], i ∈[n]. Let m denote the width of
neural network, L denote the number of layers and α denote the dual matrix multiplication exponent
(Def. C.1),then the running time of Algorithm 2 is
O(Tinit + T · Titer),
where
Tinit = O(m2(n + L)),"
T,0.2388663967611336,"Titer = eO((m1+α + m(s + k))L2 + m2−αnL).
Therefore, the cost per iteration of Algorithm 2 is
eO((m1+α + m(s + k))L2 + m2−αnL)."
T,0.23987854251012145,Proof. We analyze Tinit and Titer separately.
T,0.2408906882591093,"Initialization time. We will ﬁrst initialize (L −1) m × m matrices and one m × d matrix, which
takes O(m2L) time. Compute W1(0)xi for all i ∈[n] takes O(m2n) time. Finally, initialize the
data structure takes O(m2L) time. Hence, Tinit = O(m2(n + L))."
T,0.24190283400809717,"Cost per iteration. For each iteration, we perform one forward computation from layer 1 to L, then
backpropagate from layer L to 1."
T,0.242914979757085,"• Forward computation: In forward computation, we ﬁrst compute gi,ℓ∈Rm, which is
equivalent to form vi,ℓ+1, hence by Lemma C.5, it takes O(s + k + mα) time. Compute
hi,ℓand Di,ℓtakes O(m) time. These computations would be performed for all L layers,
hence the overall runtime of forward computation is O((s + k + mα)L) time."
T,0.24392712550607287,Under review as a conference paper at ICLR 2022
T,0.24493927125506074,"• Backpropagation:
In backpropagation, we ﬁrst compute ui,ℓ
∈Rm, which takes
O(mL(s + k + mα)) time.
Then, we call Algorithm 6 to solve the Gram regres-
sion problem, which due to Theorem D.14 takes eO(mn + nω) time.
Note that even
we want a high probability version of the solver with e−log2 nL failure probability so
that we can union bound over all layers and all iterations, we only pay extra log2 nL
term in running time, which is absorbed by the eO(·) notation. Finally, the update takes
O(m2−αn) time by Lemma C.2. Sum over all L layers, we get an overall running time of
eO((m1+α + m(s + k))L2 + m2−αnL) time."
T,0.24595141700404857,This concludes the proof of our Theorem.
T,0.24696356275303644,"Corollary B.2. Suppose the network width m is chosen as in F.25 and the shift parameter b is
chosen as in F.6, then the cost per iteration of Algorithm 2 is"
T,0.2479757085020243,eO(m1.8L2 + m1.69nL).
T,0.24898785425101214,"Remark B.3. As long as the neural network is wide enough, as in F.25 and we choose the shifted
threshold properly, as in F.6, then we can make sure that both sparsity parameters k and s to be
o(m), and we achieve subquadratic cost per iteration."
T,0.25,"C
LOW RANK MAINTENANCE AND EFFICIENT COMPUTATION OF THE
CHANGE"
T,0.25101214574898784,"In this seciton, we design a data structure to maintain the low rank representation of change of
weights, and then we show how to efﬁciently compute the low rank representation using this data
structure."
T,0.2520242914979757,"C.1
LOW RANK MAINTENANCE"
T,0.25303643724696356,"In this short section, we design a simple data structure to maintain the low rank representation of
change of weights, and show that the matrix-vector product can be implemented efﬁciently."
T,0.2540485829959514,"Before moving, we deﬁne some notions related to rectangular matrix multiplication."
T,0.2550607287449393,"Deﬁnition C.1 (Williams (2012); Gall & Urrutia (2018)). Let ω be the matrix multiplication expo-
nent such that it takes nω+o(1) time to multiply two n × n matrices."
T,0.2560728744939271,"Let α be the dual exponent of the matrix multiplication which is the supremum among all a ≥0 such
that it takes n2+o(1) time to multiply an n × n by n × na matrix."
T,0.25708502024291496,"Additionally, we deﬁne the function ω(·) where ω(b) denotes the exponent of multiplying an n × n
matrix by an n × nb matrix. Hence, we have ω(1) = ω and ω(α) = 2."
T,0.25809716599190285,"The overall idea of our low rank maintenance data structure is as follows: we keep accumulating the
low rank change, when the rank of the change reaches a certain threshold (mα), then we restart the
data structure and update the weight matrix."
T,0.2591093117408907,Under review as a conference paper at ICLR 2022
T,0.2601214574898785,Algorithm 3 Low rank maintenance data structure
T,0.2611336032388664,"1: data structure LOWRANKMAINTENANCE
▷Lemma C.2
2:
members
3:
rℓ, ∀ℓ∈[L]
▷rℓdenotes the accumulated rank of the change
4:
Wℓ, ∀ℓ∈[L]
▷{Wℓ}L
ℓ=1 ∈(Rm×m)L"
T,0.26214574898785425,"5:
∆Wℓ, ∀ℓ∈[L]
▷{∆Wℓ}L
ℓ=1 ∈(Rm×m)L"
T,0.2631578947368421,"6:
end members
7:
8:
procedures
9:
INIT({W1(0), . . . WL(0)})
▷Initialize the data structure
10:
UPDATE(Uℓ, Vℓ)
▷Update the low rank representation
11:
QUERY(ℓ, y)
▷Compute the matrix-vector product between ∆Wℓand y
12:
end procedures
13: end data structure"
T,0.26417004048583,Algorithm 4 Procedures of LRM data structure
T,0.2651821862348178,"1: procedure INIT({W1(0), . . . , WL(0)})
▷Lemma C.2
2:
Wℓ←Wℓ(0)
3:
∆Wℓ←0, ∀ℓ∈[L]
4:
rℓ←0, ∀ℓ∈[L]
5: end procedure
6:
7: procedure UPDATE(Uℓ∈Rm×n, Vℓ∈Rm×n)
▷Lemma C.2
8:
∆Wℓ←∆Wℓ+ UℓV ⊤
ℓ
without forming the product and sum the two matrices
9:
rℓ←rℓ+ n
10:
if rℓ= ma where a = ω(2) then
11:
Wℓ←Wℓ+ ∆Wℓ
▷Takes O(m2) time
12:
rℓ←0
13:
∆Wℓ←0
14:
end if
15: end procedure
16:
17: procedure QUERY(ℓ∈[L], y ∈Rm)
▷Lemma C.2
18:
z ←Wℓ· y + ∆Wℓ· y
▷Takes O(nnz(y) · m + mrℓ) time
19:
return z
20: end procedure"
T,0.26619433198380565,"Lemma C.2. There exists a deterministic data structure (Algorithm 3) such that maintains
∆W1, . . . , ∆WL such that"
T,0.26720647773279355,• The procedure INIT (Algorithm 4) takes O(m2L) time.
T,0.2682186234817814,"• The procedure UPDATE (Algorithm 4) takes O(nm2−α) amortized time, where α = ω(2)"
T,0.2692307692307692,"• The procedure QUERY (Algorithm 4) takes O(m · (nnz(y) + rℓ)) time, where rℓis the rank
of ∆Wℓwhen QUERY is called."
T,0.2702429149797571,"Proof. The runtime for INIT is obvious, for QUERY, notice that we are multiplying vector y with
a (possibly) dense matrix Wℓ∈Rm×m, which takes O(nnz(y) · m) time, and an accumulated
low rank matrix ∆Wℓwith rank rℓ. By using the low rank decomposition ∆Wℓ= UV ⊤with
U, V ∈Rm×rℓ, the time to multiply y with ∆W is O(mrℓ). Combine them together, we get a
running time of O(m · (nnz(y) + rℓ))."
T,0.27125506072874495,"It remains to analyze the amortized cost of UPDATE. Note that if rℓ< ma, then we just pay O(1)
time to update corresponding variables in the data structure. If rℓ= ma, then we will explicitly
form the m × m matrix ∆Wℓ. To form it, notice we have accumulated rℓ/n different sums of"
T,0.2722672064777328,Under review as a conference paper at ICLR 2022
T,0.2732793522267207,"rank-n decompositions, which can be represented as"
T,0.2742914979757085,"U = [Uℓ(1), Uℓ(2), . . . , Uℓ(rℓ/n)] ∈Rm×rℓ, V = [Vℓ(1), Vℓ(2), . . . , Vℓ(rℓ/n)] ∈Rm×rℓ,"
T,0.27530364372469635,"and ∆Wℓ= UV ⊤, which takes O(m2) time to compute since rℓ= ma and a = ω(2). Finally, note
that this update of Wℓonly happens once per rℓ/n number of calls to UPDATE, therefore we can
charge each step by O( m2"
T,0.27631578947368424,"rℓ/n) = O(m2−an) = O(m2−αn), arrives at our ﬁnal amortized running
time."
T,0.2773279352226721,"Remark C.3. Currently, the dual matrix multiplication exponent α ≈0.31 (Gall & Urrutia, 2018),
hence the amortized time for UPDATE is O(nm1.69). If m ≥n10/3, then we achieve an update time
of o(m2). Similarly, the time for QUERY is O(m · (nnz(y) + rℓ)) = O(m · nnz(y) + m1+α) =
O(m · nnz(y) + m1.31), as long as nnz(y) = o(m), then its running time is also o(m2). In our
application of training neural networks, we will make sure that the inputted vector y is sparse."
T,0.2783400809716599,"C.2
EFFICIENTLY COMPUTE ui,ℓ(t) AND vi,ℓ(t)"
T,0.2793522267206478,"In this section, we show that how to compute the vectors ui,ℓ, vi,ℓ∈Rm using the low rank structure
of the change of weights combined. Recall the deﬁnition of these vectors:"
T,0.28036437246963564,"ui,ℓ(t)⊤= a⊤Di,L(t)WL(t) . . . Di,ℓ+1(t)Wℓ+1(t)Di,ℓ(t) ∈R1×m,
vi,ℓ(t) = hi,ℓ−1(t) ∈Rm."
T,0.2813765182186235,"Before proceeding, we list the assumptions we will be using:"
T,0.2823886639676113,"• For any ℓ∈[L], Di,ℓ(t) is sD-sparse, where sD := k + s, k is the sparsity of Di,ℓ(0) and
s is the sparsity of Di,ℓ(t) −Di,ℓ(0)."
T,0.2834008097165992,"• For any ℓ∈[L], the change of the weight matrix Wℓ, ∆Wℓ(t) := Wℓ(t) −Wℓ(0), is of
low-rank. That is, ∆Wℓ(t) = Prt
j=1 yℓ,jz⊤
ℓ,j."
T,0.28441295546558704,"• For any i ∈[n], W1(0)xi is pre-computed."
T,0.2854251012145749,"We ﬁrst note that as a direct consequence of Di,ℓ(0) is k-sparse, hi,ℓ(0) is k-sparse as well. Simi-
larly, hi,ℓ(t) −hi,ℓ(0) has sparsity s. Hence hi,ℓ(t) has sparsity bounded by sD."
T,0.28643724696356276,"Compute ui,ℓ(t).
Compute ui,ℓ(t) is equivalent to compute the following vector:"
T,0.2874493927125506,"Di,ℓ(t)(Wℓ+1(0) + ∆Wℓ+1(t))⊤Di,ℓ+1(t) · · · (WL(0) + ∆WL(t))⊤Di,L(t)a."
T,0.28846153846153844,"First, we know that Di,L(t)a ∈Rm is an sD-sparse vector, and it takes O(sD) time. The next matrix
is (WL(0)+∆WL(t))⊤, which gives two terms: WL(0)⊤(Di,L(t)a) and ∆WL(t)⊤(Di,L(t)a). For
the ﬁrst term, since Di,L(t)a is sD-sparse, it takes O(msD)-time. For the second term, we have"
T,0.2894736842105263,"∆WL(t)⊤(Di,L(t)a) = rt
X"
T,0.29048582995951416,"j=1
zL,jy⊤
L,j(Di,L(t)a) = rt
X"
T,0.291497975708502,"j=1
zL,j · ⟨yL,j, Di,L(t)a⟩."
T,0.2925101214574899,"Each inner-product takes O(sD)-time and it takes O(mrt + sDrt) = O(mrt)-time in total. Hence,
in O(m(sD + rt))-time, we compute the vector WL(t)⊤Di,L(t)a. Note that we do not assume the
sparsity of a."
T,0.2935222672064777,"Thus, by repeating this process for the L −ℓintermediate matrices W ⊤
j (t)Di,j(t), we can obtain
the vector
 
L
Y"
T,0.29453441295546556,"j=ℓ+1
W ⊤
j (t)Di,j(t)  a"
T,0.29554655870445345,"in time O((L −ℓ)m(sD + rt)). Finally, by multiplying a sparse diagonal matrix Di,ℓ(t), we get the
desired vector ui,ℓ(t)."
T,0.2965587044534413,Under review as a conference paper at ICLR 2022
T,0.2975708502024291,"Compute vi,ℓ(t).
Note that vi,ℓ(t) is essentially hi,ℓ−1(t), so we consider how to compute hi,ℓ(t)
for general ℓ∈[L]. Recall that"
T,0.298582995951417,"hi,ℓ(t) = φ((Wℓ(0) + ∆Wℓ(t))hi,ℓ−1(t)),"
T,0.29959514170040485,"since hi,ℓ−1(t) is sD-sparse, the product Wℓ(0hi,ℓ−1(t))) can be computed in O(msD) time. For the
product ∆Wℓ(t)hi,ℓ−1(t) can be computed use the low rank decomposition, which takes O(mrt)
time. Apply the threshold ReLU takes O(m) time. Hence, the total time is O(m(rt + sD)) time."
T,0.3006072874493927,"The running time results are summarized in the following lemma:
Lemma C.4. For ℓ∈[L] and i ∈[n], suppose ∥Di,ℓ(0)∥0 ≤k. Let t > 0. Suppose the change of
Di,ℓis sparse, i.e., ∥Di,ℓ(t)−Di,ℓ(0)∥0 ≤s. For ℓ∈[L], i ∈[n], for any t > 0, suppose the change
of Wℓis of low-rank, i.e., ∆Wℓ(t) = Prt
j=1 yℓ,jz⊤
ℓ,j. We further assume that {yℓ,j, zℓ,j}ℓ∈[L],j∈[rt]
and {W1(0)xi}i∈[n] are pre-computed."
T,0.3016194331983806,"Then, for any ℓ∈[L] and i ∈[n], the vectors uℓ,i(t), vℓ,i(t) ∈Rm can be computed in O(mL(s +
k + rt))-time."
T,0.3026315789473684,"As a direct consequence, if we combine Lemma C.2 and Lemma C.4, then we get the following
corollary:
Corollary C.5. For ℓ∈[L] and i ∈[n], we can compute vi,ℓ(t), ui,ℓ(t) ∈Rm as in Algorithm 2
with the following time bound:"
T,0.30364372469635625,"• Compute ui,ℓ(t) in time O(mL(s + k + rℓ))."
T,0.30465587044534415,"• Compute vi,ℓ(t) in time O(m(s + k + rℓ))."
T,0.305668016194332,"D
FAST TENSOR PRODUCT REGRESSION"
T,0.3066801619433198,"In this section, we design a generic algorithm for solving the following type of regression task:"
T,0.3076923076923077,"Given two matrices U = [u⊤
1 , . . . , u⊤
n ]⊤, V = [v⊤
1 , . . . , v⊤
n ] ∈Rm×n with m ≫n, consider the
matrix J ∈Rn×m2 formed by J =  "
T,0.30870445344129555,"vec(u1v⊤
1 )⊤"
T,0.3097165991902834,"vec(u2v⊤
2 )⊤
...
vec(unv⊤
n )⊤  ."
T,0.3107287449392713,"We are also given a vector c ∈Rn, the goal is to solve the following regression task:"
T,0.3117408906882591,"min
x∈Rn ∥JJ⊤x −c∥2
2."
T,0.31275303643724695,"Our main theorem for this section is as follows:
Theorem D.1 (Restatement of Theorem D.14). Given two n × m matrices U and V , and a target
vector c ∈Rn. Let J = [vec(u1v⊤
1 )⊤, . . . , vec(unv⊤
n )⊤] ∈Rn×m2. There is an algorithm (Algo-
rithm 6) takes eO(nm + n2(log(κ/ϵ) + log(m/ϵδ)ϵ−2) + nω) time and outputs a vector bx ∈Rn
such that"
T,0.31376518218623484,"∥JJ⊤bx −c∥2 ≤ϵ∥c∥2
holds with probability at least 1 −δ, and κ is the condition number of J."
T,0.3147773279352227,"D.1
APPROXIMATE J VIA TensorSketch"
T,0.3157894736842105,"We introduce the notion of TensorSketch for two vectors:
Deﬁnition D.2. Let h1, h2 : [m] →[s] be 3-wise independent hash functions, also let σ : [m] →
{±1} be a 4-wise independent random sign function. The degree two TensorSketch transform,
S : Rm × Rm →Rs is deﬁned as follows: for any i, j ∈[m] and r ∈[s],"
T,0.3168016194331984,"Sr,(i,j) = σ1(i) · σ2(j) · 1[h1(i) + h2(j) = r mod s]."
T,0.31781376518218624,Under review as a conference paper at ICLR 2022
T,0.3188259109311741,"Remark D.3. Apply S to two vectors x, y ∈Rm can be implemented in time O(s log s + nnz(x) +
nnz(y))."
T,0.31983805668016196,We introduce one key technical lemma from Avron et al. (2014):
T,0.3208502024291498,"Lemma D.4 (Theorem 1 of Avron et al. (2014)). Let S ∈Rs×m2 be the TensorSketch matrix,
consider a ﬁxed n-dimensional subspace V . If s = Ω(n2/(ϵ2δ)), then with probability at least
1 −δ, ∥Sx∥2 = (1 ± ϵ)∥x∥2 simultaneously for all x ∈V ."
T,0.32186234817813764,Now we are ready to prove the main lemma of this section:
T,0.3228744939271255,"Lemma D.5. Let ϵ, δ ∈(0, 1) denote two parameters. Let J ∈Rn×m2 be a matrix such that i-th
row of J is deﬁned as vec(uiv⊤
i ) for some ui, vi ∈Rm. Then, we can compute a matrix eJ ∈Rn×s
such that for any vector x ∈Rn, with probability at least 1 −δ, we have"
T,0.32388663967611336,"∥eJ⊤x∥2 = (1 ± ϵ)∥J⊤x∥2,"
T,0.3248987854251012,where s = Ω(n2/(ϵ2δ)). The time to compute eJ is O(ns log s + nnz(U) + nnz(V )).
T,0.3259109311740891,"Proof. Notice that the row space of matrix J can be viewed as an n-dimensional subspace, hence,
by Lemma D.4, the TensorSketch matrix S with s = Ω(n2/(ϵ2δ)) can preserve the length of all
vectors in the subspace generated by J⊤with probability 1 −δ, to a multiplicative factor of 1 ± ϵ."
T,0.3269230769230769,"The running time part is to apply the FFT algorithm to each row of J with a total of n rows. For
each row, it takes O(s log s + m) time, hence the overall running time is O(n(s log s + m))."
T,0.32793522267206476,"D.2
APPROXIMATE J VIA TensorSRHT"
T,0.32894736842105265,"We note that the dependence on the target dimension of sketching is O(1/δ) for TensorSketch. We
introduce another kind of sketching technique for tensor, called TensorSRHT. The tradeoff is we
lose input sparsity runtime of matrices U and V ."
T,0.3299595141700405,"Deﬁnition D.6. We deﬁne the TensorSRHT S : Rd × Rd →Rm as S =
1
√mP · (HD1 × HD2),"
T,0.3309716599190283,"where each row of P ∈{0, 1}m×d2 contains only one 1 at a random coordinate, one can view P as
a sampling matrix. H is a d × d Hadamard matrix, and D1, D2 are two d × d independent diag-
onal matrices with diagonals that are each independently set to be a Rademacher random variable
(uniform in {−1, 1})."
T,0.3319838056680162,"Remark D.7. By using FFT algorithm, apply S to two vectors x, y ∈Rm takes time O(m log m+s)."
T,0.33299595141700405,We again introduce a technical lemma for TensorSRHT.
T,0.3340080971659919,"Lemma D.8 (Theorem 3 of Ahle et al. (2020)). Let S ∈Rs×m2 be the TensorSRHT matrix, con-
sider a ﬁxed n-dimensional subspace V . If s = Ω(n log3(nm/ϵδ)ϵ−2), then with probability at
least 1 −δ, ∥Sx∥2 = (1 ± ϵ)∥x∥2 simultaneously for all x ∈V ."
T,0.3350202429149798,"Lemma D.9.
Let ϵ, δ
∈
(0, 1) denote
two parameters.
Given a
list
of vectors
u1, · · · , um, v1, · · · , vm ∈Rm. Let J ∈Rn×m2 be a matrix with i-th row of J is deﬁned as
vec(uiv⊤
i ). Then, we can compute a matrix eJ ∈Rn×s such that for any vector x ∈Rn, with
probability at least 1 −δ, we have"
T,0.3360323886639676,"∥eJ⊤x∥2 = (1 ± ϵ)∥J⊤x∥2,"
T,0.33704453441295545,where s = Ω(n log3(nm/(ϵδ))ϵ−2). The time to compute eJ is O(n(m log m + s)).
T,0.33805668016194335,"Proof. The correctness follows directly from Lemma D.8. The running time follows from the FFT
algorithm to each row of J, each application takes O(m log m + s) time, and we need to apply it to
n rows."
T,0.3390688259109312,Under review as a conference paper at ICLR 2022
T,0.340080971659919,"D.3
TENSOR TRICK AS A PRECONDITIONER"
T,0.3410931174089069,"In this section, we use TensorSketch and TensorSRHT as a preconditioner to solve a regression task
involving JJ⊤. This is an important step to implement the Gram-Newton-Gauss iteration Cai et al.
(2019); Brand et al. (2021)."
T,0.34210526315789475,"Before proceeding, we introduce the notion of subspace embedding:
Deﬁnition D.10 (Subspace Embedding, Sarlos (2006)). Let A ∈RN×k, we say a matrix S ∈Rs×N
is a (1 ± ϵ) −ℓ2 subspace embedding for A if for any x ∈Rk, we have ∥SAx∥2
2 = (1 ± ϵ)∥Ax∥2
2.
Equivalently, ∥I −U ⊤S⊤SU∥≤ϵ where U is an orthonormal basis for the column space of A."
T,0.3431174089068826,"We will mainly utilize efﬁcient subspace embedding.
Deﬁnition D.11 (Lu et al. (2013); Woodruff (2014)). Given a matrix A ∈RN×k with N = poly(k),
then we can compute an S ∈Rkpoly(log(k/δ))/ϵ2×N such that with probability at least 1−δ, we have"
T,0.3441295546558704,∥SAx∥2 = (1 ± ϵ)∥Ax∥2
T,0.3451417004048583,"hols for all x ∈Rk. Moreover, SA can be computed in O(Nk log((k log N)/ϵ)) time."
T,0.34615384615384615,Algorithm 5 Fast Regression algorithm of Brand et al. (2021)]
T,0.347165991902834,"1: procedure FASTREGRESSION(A, y, ϵ)
▷Lemma D.12
2:
▷A ∈RN×k is full rank, ϵ ∈(0, 1/2)
3:
Compute a subspace embedding SA
▷S ∈Rkpoly(log k) × N
4:
Compute R such that SAR has orthonormal columns via QR decomposition
▷R ∈Rk×k"
T,0.3481781376518219,"5:
z0 = 0k ∈Rk"
T,0.3491902834008097,"6:
t ←0
7:
while ∥A⊤ARzt −y∥2 ≥ϵ do
8:
zt+1 ←zt −(R⊤A⊤AR)⊤(R⊤A⊤ARzt −R⊤y)
9:
t ←t + 1
10:
end while
11:
return Rzt
12: end procedure"
T,0.35020242914979755,"Lemma D.12 (Lemma 4.2 of Brand et al. (2021)). Let N = Ω(kpoly(log k)). Given a matrix
A ∈RN×k, let κ denote its condition number. Consider the following regression task:"
T,0.35121457489878544,"min
x∈Rk ∥A⊤Ax −y∥2."
T,0.3522267206477733,"Using the procedure FASTREGRESSION (Algorithm 5), with probability at least 1 −δ, we can
compute an ϵ-approximate solution bx satisfying"
T,0.3532388663967611,∥A⊤Abx −y∥2 ≤ϵ∥y∥2
T,0.354251012145749,in time eO(Nk log(κ/ϵ) + kω).
T,0.35526315789473684,"Our algorithm is similar to the ridge regression procedure in Song et al. (2021), where they ﬁrst
apply their sketching algorithm as a bootstrapping to reduce the dimension of the original matrix,
then use another subspace embedding to proceed and get stronger guarantee."
T,0.3562753036437247,"We shall ﬁrst prove a useful lemma.
Lemma D.13. Let A ∈RN×k, suppose SA is a subspace embedding for A (Def. D.11), then we
have for any x ∈Rk, with probability at least 1 −δ,"
T,0.35728744939271256,∥(SA)⊤SAx −b∥2 = (1 ± ϵ)∥A⊤Ax −b∥2.
T,0.3582995951417004,"Proof. Throughout the proof, we condition on the event that S preserves the length of all vectors in
the column space of A."
T,0.35931174089068824,Note that
T,0.3603238866396761,"∥(SA)⊤SAx −b∥2
2 = ∥(SA)⊤SAx∥2
2 + ∥b∥2
2 −2⟨(SA)⊤SAx, b⟩."
T,0.36133603238866396,Under review as a conference paper at ICLR 2022
T,0.3623481781376518,"We will ﬁrst bound the norm of (SA)⊤SAx, then the inner product term."
T,0.3633603238866397,"Bounding ∥(SA)⊤SAx∥2
2
Let U ∈RN×k be an orthonormal basis of A, then use the equivalent deﬁnition of subspace embed-
ding, we have ∥U ⊤S⊤SU −I∥≤ϵ, this means all the eigenvalues of U ⊤S⊤SU lie in the range
of of [(1 −ϵ)2, (1 + ϵ)2]. Let V denote the matrix U ⊤S⊤SU, then we know that all eigenvalues of
V ⊤V lie in range [(1 −ϵ)4, (1 + ϵ)4]. Setting ϵ as ϵ/4, we arrive at ∥V ⊤V −I∥≤ϵ. This shows
that for any x ∈Rk, we have ∥(SA)⊤SAx∥2 = (1 ± ϵ)∥A⊤Ax∥2."
T,0.3643724696356275,"Bounding ⟨(SA)⊤SAx, b⟩Note that"
T,0.36538461538461536,"⟨(SA)⊤SAx, b⟩= ⟨SAx, SAb⟩"
T,0.36639676113360325,"= 1/2 · (∥SAx∥2
2 + ∥SAb∥2
2 −∥SA(x −b)∥2
2)"
T,0.3674089068825911,"= 1/2 · (1 ± ϵ)(∥Ax∥2
2 + ∥Ab∥2
2 −∥A(x −b)∥2
2)"
T,0.3684210526315789,"= (1 ± ϵ)⟨A⊤Ax, b⟩."
T,0.3694331983805668,"Combining these two terms, we conclude that, with probability at least 1 −δ,"
T,0.37044534412955465,∥(SA)⊤SAx −b∥2 = (1 ± ϵ)∥A⊤Ax −b∥2.
T,0.3714574898785425,Algorithm 6 Fast Regression via tensor trick
T,0.3724696356275304,"1: procedure FASTTENSORREGRESSION({ui}n
i=1 ∈Rm×n, {vi}n
i=1 ∈Rm×n, c ∈Rn)
▷
Theorem D.14
2:
▷J = [vec(u1v⊤
1 )⊤, vec(u2v⊤
2 )⊤, . . . , vec(unv⊤
n )⊤]⊤∈Rn×m2"
T,0.3734817813765182,"3:
s1 ←Θ(n log3(nm/(ϵδ))ϵ−2)
4:
s2 ←Θ((n + log m) log n)
5:
Let S1 ∈Rs1×m2 be a sketching matrix
▷S1 can be TensorSketch or TensorSRHT
6:
Compute eJ = JS⊤
1 via FFT algorithm
▷eJ ∈Rn×s1"
T,0.37449392712550605,"7:
Let S2 ∈Rs2×s1 be a sketching matrix deﬁned in Deﬁnition D.11
8:
Compute a subspace embedding S2 eJ⊤"
T,0.37550607287449395,"9:
Compute R such that S2 eJ⊤R has orthonormal columns via QR decomposition
▷
R ∈Rn×n"
T,0.3765182186234818,"10:
z0 ←0k ∈Rk"
T,0.3775303643724696,"11:
t ←0
12:
while ∥eJ eJ⊤Rzt −c∥2 ≥ϵ do
13:
zt+1 ←zt −(R⊤eJ eJ⊤R)⊤(R⊤eJ eJ⊤Rzt −R⊤c)
14:
t ←t + 1
15:
end while
16:
return Rzt
17: end procedure"
T,0.3785425101214575,"Theorem D.14. Given two n × m matrices U and V , and a target vector c ∈Rn. Let J =
[vec(u1v⊤
1 )⊤, . . . , vec(unv⊤
n )⊤] ∈Rn×m2. There is an algorithm (Algorithm 6) takes eO(nm +
n2(log(κ/ϵ) + log(m/δ)) + nω) time and outputs a vector bx ∈Rn such that"
T,0.37955465587044535,∥JJ⊤bx −c∥2 ≤ϵ∥c∥2
T,0.3805668016194332,"holds with probability at least 1 −δ, and κ is the condition number of J."
T,0.3815789473684211,Proof. We can decompose Algorithm 6 into two parts:
T,0.3825910931174089,"• Applying S1 to efﬁciently form matrix eJ to approximate J and reduce its dimension, notice
here we only need ϵ for this part to be a small constant, pick ϵ = 0.1 sufﬁces."
T,0.38360323886639675,• Using S2 as a preconditioner and solve the regression problem iteratively.
T,0.38461538461538464,Under review as a conference paper at ICLR 2022
T,0.3856275303643725,Let bx denote the solution found by the iterative regime. We will prove this statement in two-folds:
T,0.3866396761133603,"• First, we will show that ∥eJ eJ⊤bx −c∥2 ≤ϵ∥c∥2 with probability at least 1 −δ;"
T,0.3876518218623482,"• Then, we will show that ∥JJ⊤bx −c∥2 = (1 ± 0.1)∥eJ eJ⊤bx −c∥2 with probability at least
1 −δ."
T,0.38866396761133604,"Combining these two statements, we can show that"
T,0.3896761133603239,"∥JJ⊤bx −c∥2 = (1 ± 0.1)∥eJ eJ⊤bx −c∥2
≤1.1ϵ∥c∥2"
T,0.39068825910931176,"Setting ϵ to ϵ/1.1 and δ to δ/2, we conclude our proof. It remains to prove these two parts."
T,0.3917004048582996,"Part 1. ∥eJ eJ⊤bx−c∥2 ≤ϵ∥c∥2 We observe the iterative procedure is essentially the same as running
FASTREGRESSION on input eJ⊤, y, ϵ, hence by Lemma D.12, we have with probability at least 1−δ,
∥eJ eJ⊤bx −c∥2 ≤ϵ∥c∥2."
T,0.39271255060728744,"Part 2. ∥JJ⊤bx −c∥2 = (1 ± 0.1)∥eJ eJ⊤bx −c∥2 To prove this part, note that by Lemma D.8, we
know that eJ⊤is a subspace embedding for J⊤. Hence, we can utilize Lemma D.13 and get that,
with probability at least 1 −δ, we have ∥JJ⊤bx −c∥2 = (1 ± 0.1)∥eJ eJ⊤bx −c∥2."
T,0.3937246963562753,"Combining these two parts, we have proven the correctness of the theorem. It remains to justify the
running time. Note that running time can be decomposed into two parts: 1). The time to generate eJ,
2). The time to compute bx via iterative scheme."
T,0.39473684210526316,"Part 1.
Generate eJ To generate eJ, we apply S1 ∈Rs1×m2 which is a TensorSRHT.
By
Lemma D.9, it takes O(n(m log m + s1)) time to compute eJ, plug in s1 = Θ(n log3(nm/δ)),
the time is eO(nm)."
T,0.395748987854251,"Part 2. Compute bx To compute bx, essentially we run FASTREGRESSION on eJ⊤, c, ϵ, hence by
Lemma D.12, it takes eO(s2n log(κ/ϵ) + nω) time, with s2 = Θ((n + log m) log n) and κ is the
condition number of eJ, which has the guarantee κ = (1 ± ϵ)κ(J). Hence, the overall running time
of this part is eO(n2 log(κ/ϵ) + nω)."
T,0.3967611336032389,"Put things together, the overall running time is eO(nm + n2 log(κ/ϵ) + nω)."
T,0.3977732793522267,"Remark D.15. Due to the probability requirement (union bounding over all layers and all data
points), here we only prove by using TensorSRHT. One can use similar strategy to obtain an input
sparsity time version using TensorSketch. We remark that this framework is similar to the ap-
proach Song et al. (2021) takes to solve kernel ridge regression, where one ﬁrst uses a shallow but
fast sketch to bootstrap, then use another sketching to proceed with the main task."
T,0.39878542510121456,"E
SPECTRAL PROPERTIES OF OVER-PARAMETRIZED DEEP NEURAL
NETWORK"
T,0.39979757085020245,"In this section, we study the spectral properties of our Gram matrix and connects it to multi-layer
NTK."
T,0.4008097165991903,"E.1
BOUNDS ON THE LEAST EIGENVALUE OF KERNEL AT INITIALIZATION"
T,0.4018218623481781,We ﬁrst deﬁne the Gram matrices for multiple layer neural network.
T,0.402834008097166,"Deﬁnition E.1 (Multiple layer Gram matrix). The Gram matrices Kℓ∈Rn×n for ℓ∈{0, . . . , L}
of an L-layer neural network are deﬁned as follows:"
T,0.40384615384615385,"• (K0)i,j := x⊤
i xj"
T,0.4048582995951417,Under review as a conference paper at ICLR 2022
T,0.4058704453441296,"• For ℓ> 0, let Σℓ,i,j :=

(Kℓ−1)i,i
(Kℓ−1)i,j
(Kℓ−1)j,i
(Kℓ−1)j,j"
T,0.4068825910931174,"
∈R2×2 for any (i, j) ∈[n] × [n]. Then,"
T,0.40789473684210525,"(Kℓ)i,j :=
E
(x1,x2)∼N(0,2Σℓ−1,i,j)[φ(x1)φ(x2)] ∀ℓ∈[L −1],"
T,0.4089068825910931,"(KL)i,j :=
E
(x1,x2)∼N(0,2ΣL−1,i,j)[φ′(x1)φ′(x2)]"
T,0.409919028340081,Let λL := λmin(KL) to be the minimum eigenvalue of the NTK kernel KL.
T,0.4109311740890688,"In the following lemma, we generalize Lemma C.3 in Brand et al. (2021) (also Lemma 3 in Cai et al.
(2019)) into multiple layer neural networks.
Lemma E.2 (Bounds on the least eigenvalue at initialization, multiple layer version of Lemma C.3
in Brand et al. (2021)). Let λℓdenote the minimum eigenvalue of NTK deﬁned for ℓ-th layer of
neural networks. Suppose mℓ= Ω(λ−2
ℓn2 log(n/δ)), then with probability 1 −δ, we have"
T,0.41194331983805665,λmin(Gℓ(0)) ≥3
T,0.41295546558704455,"4λℓ,
∀ℓ∈[L]"
T,0.4139676113360324,"Proof. For any ℓ∈[L], we have Gℓ= JℓJ⊤
ℓ∈Rn×n. Hence, for any i, j ∈[n],"
T,0.4149797570850202,"(Gℓ)i,j = vec(∂f(W, xi)"
T,0.4159919028340081,"∂Wℓ
)⊤vec(∂f(W, xj) ∂Wℓ
) = vec  Di,ℓ L
Y"
T,0.41700404858299595,"k=ℓ+1
W ⊤
k Di,kah⊤
i,ℓ−1 !⊤ vec  Dj,ℓ L
Y"
T,0.4180161943319838,"k=ℓ+1
W ⊤
k Dj,kah⊤
j,ℓ−1 ! = "
T,0.4190283400809717,"(hi,ℓ−1 ⊗Imℓ)  Di,ℓ L
Y"
T,0.4200404858299595,"k=ℓ+1
W ⊤
k Di,ka !!⊤"
T,0.42105263157894735,"(hj,ℓ−1 ⊗Imℓ)  Dj,ℓ L
Y"
T,0.42206477732793524,"k=ℓ+1
W ⊤
k Dj,ka ! =  Di,ℓ L
Y"
T,0.4230769230769231,"k=ℓ+1
W ⊤
k Di,ka !⊤"
T,0.4240890688259109,"(h⊤
i,ℓ−1 ⊗Imℓ)(hj,ℓ−1 ⊗Imℓ)  Dj,ℓ L
Y"
T,0.4251012145748988,"k=ℓ+1
W ⊤
k Dj,ka ! = a⊤ Di,ℓ L
Y"
T,0.42611336032388664,"k=ℓ+1
W ⊤
k Di,k !⊤ Dj,ℓ L
Y"
T,0.4271255060728745,"k=ℓ+1
W ⊤
k Dj,k !"
T,0.42813765182186236,"a · h⊤
i,ℓ−1hj,ℓ−1, where"
T,0.4291497975708502,"hi,ℓ−1 = ℓ−1
Y"
T,0.43016194331983804,"k=1
Di,kWkxi."
T,0.4311740890688259,"In particular,
(GL)i,j = a⊤Di,LDj,La · h⊤
i,L−1hj,L−1 = 1 m m
X"
T,0.43218623481781376,"r=1
φ′(⟨w(r)
L , hi,L−1⟩)φ′(⟨w(r)
L , hj,L−1⟩)h⊤
i,L−1hj,L−1
(4)"
T,0.4331983805668016,"We will prove that ∥GL −KL∥∞is small, which implies that λmin(GL) is close to λℓ. The proof
idea is similar to Du et al. (2019a) via induction on ℓ."
T,0.4342105263157895,"For ℓ= 1, recall (g1,i)k = P"
T,0.4352226720647773,"b∈[m](W1)k,b(xi)b for k ∈[m]. Hence, for any k ∈[m],"
T,0.43623481781376516,"E[(g1,i)k(g1,j)k] =
X"
T,0.43724696356275305,"b,b′∈[m]
E[(W1)k,b(W1)k,b′(xi)b(xj)b′] =
X"
T,0.4382591093117409,"b∈[m]
E[(W1)2
k,b] · (xi)b(xj)b
((W1)k,b ∼N(0, 2 m).) = 2 m X"
T,0.4392712550607287,"b∈[m]
(xi)b(xj)b = 2"
T,0.4402834008097166,"mx⊤
i xj."
T,0.44129554655870445,Under review as a conference paper at ICLR 2022
T,0.4423076923076923,"Then, we have"
T,0.4433198380566802,"E[h⊤
1,ih1,j] =
X"
T,0.444331983805668,"k∈[m]
E[(h1,i)k(h1,j)k] =
X"
T,0.44534412955465585,"k∈[m]
E[φ((g1,i)k)φ((g1,j)k)] =
X"
T,0.44635627530364375,"k∈[m]
E
(u,v)∼N(0, 2"
T,0.4473684210526316,"m Σ1,i,j)[φ(u)φ(v)]"
T,0.4483805668016194,"=
E
(u,v)∼N(0, 2"
T,0.4493927125506073,"m Σ1,i,j)[mφ(u)φ(v)]"
T,0.45040485829959515,"=
E
(u′,v′)∼N(0,2Σ1,i,j)[φ(u′)φ(v′)]"
T,0.451417004048583,"= (K1)i,j."
T,0.4524291497975709,"Next, we will show that h⊤
1,ih1,j concentrates around its expectation. First, for any k ∈[m],"
T,0.4534412955465587,"|(h1,i)k(h1,j)k| ≤|(g1,i)k(g1,j)k| ≤|⟨(W1)k,∗, xi⟩| · |⟨(W1)k,∗, xj⟩|."
T,0.45445344129554655,"Since ⟨(W1)k,∗, xi⟩∼N(0, 2∥xi∥2
2
m
), by the concentration of Gaussian distribution,"
T,0.45546558704453444,"|⟨(W1)k,∗, xi⟩| ≤√c ∀k ∈[m], i ∈[n]"
T,0.4564777327935223,"holds with probability at least 1−mne−cm/4. Conditioned on this event, we have |(h1,i)k(h1,j)k| ≤
c for all i, j ∈[n] and k ∈[m]. Then, by Hoeffding’s inequality, we have for any (i, j) ∈[n] × [n],"
T,0.4574898785425101,"Pr

|h⊤
1,ih1,j −(K1)i,j| ≥t

≤exp

−
t2"
T,0.458502024291498,2m · (2c)2
T,0.45951417004048584,"
= exp(−Ω(t2/(mc2)))."
T,0.4605263157894737,"Hence, by union bound, we get that"
T,0.46153846153846156,"max
(i,j)∈[n]×[n] |h⊤
1,ih1,j −(K1)i,j| ≤t"
T,0.4625506072874494,with probability at least
T,0.46356275303643724,1 −mn exp(−Ω(mc)) −n2 exp(−Ω(t2/(mc2))).
T,0.4645748987854251,If we choose c := log(mnL/δ)
T,0.46558704453441296,"m
and t := m−1/2 · polylog(nL/δ), we have with probability at least
1 −δ L,"
T,0.4665991902834008,"max
(i,j)∈[n]×[n] |h⊤
1,ih1,j −(K1)i,j| ≤eO(m−1/2)."
T,0.4676113360323887,"Suppose for ℓ= 1, . . . h (h < L), we have"
T,0.4686234817813765,"max
(i,j)∈[n]×[n] |h⊤
ℓ,ihℓ,j −(Kℓ)i,j| ≤eO(m−1/2)."
T,0.46963562753036436,"Consider ℓ= h + 1. By a similar computation, we have"
T,0.4706477732793522,"E
Wℓ[(gℓ,i)k(gℓ,j)k] = 2"
T,0.4716599190283401,"mh⊤
ℓ−1,ihℓ−1,j."
T,0.4726720647773279,Deﬁne a new covariance matrix
T,0.47368421052631576,"bΣℓ,i,j :=
h⊤
ℓ−1,ihℓ−1,i
h⊤
ℓ−1,ihℓ−1,j
h⊤
ℓ−1,jhℓ−1,i
h⊤
ℓ−1,jhℓ−1,j"
T,0.47469635627530365,"
∀(i, j) ∈[n] × [n]."
T,0.4757085020242915,We have
T,0.4767206477732793,"E
Wℓ[h⊤
i,ℓhj,ℓ] =
X"
T,0.4777327935222672,"k∈[m]
E
(u,v)∼N(0, 2"
T,0.47874493927125505,"m bΣℓ,i,j)
[φ(u)φ(v)]"
T,0.4797570850202429,"=
E
(u′,v′)∼N(0,2bΣℓ,i,j)
[φ(u′)φ(v′)]"
T,0.4807692307692308,":= ( bKℓ)i,j."
T,0.4817813765182186,Under review as a conference paper at ICLR 2022
T,0.48279352226720645,"Hence, we have with probability at least 1 −δ L,"
T,0.48380566801619435,"max
(i,j)∈[n]×[n]"
T,0.4848178137651822,"h⊤
ℓ,ihℓ,j −( bKℓ)i,j
 ≤eO(m−1/2).
(5)"
T,0.48582995951417,It remains to upper bound the difference ∥bKℓ−Kℓ∥∞.
T,0.4868421052631579,"bKℓ−Kℓ

∞=
max
(i,j)∈[n]×[n]"
T,0.48785425101214575,"E
(u,v)∼N(0,2bΣℓ,i,j)
[φ(u)φ(v)] −
E
(u,v)∼N(0,2Σℓ,i,j)[φ(u)φ(v)] ."
T,0.4888663967611336,Recall that
T,0.4898785425101215,"Σℓ,i,j :=

(Kℓ−1)i,i
(Kℓ−1)i,j
(Kℓ−1)j,i
(Kℓ−1)j,j"
T,0.4908906882591093,"
∀(i, j) ∈[n] × [n],"
T,0.49190283400809715,"and hence, by the induction hypothesis, we have"
T,0.49291497975708504,"∥bΣℓ,i,j −Σℓ,i,j∥∞≤
max
(i,j)∈[n]×[n]"
T,0.4939271255060729,"h⊤
ℓ−1,ihℓ−1,j −(Kℓ−1)i,j
 = eO(m−1/2)."
T,0.4949392712550607,"Notice that bΣℓ,i,j can be written as

∥hℓ−1,i∥2
2
cos(θℓ,i,j)∥hℓ−1,i∥2∥hℓ−1,j∥2
cos(θℓ,i,j)∥hℓ−1,i∥2∥hℓ−1,j∥2
∥hℓ−1,j∥2
2 
."
T,0.4959514170040486,"Moreover, when φ is the ReLU function, we have"
T,0.49696356275303644,"E
(u,v)∼N(0,2bΣℓ,i,j)
[φ(u)φ(v)] = 2∥hℓ−1,i∥2∥hℓ−1,j∥2 · F(θℓ,i,j), where"
T,0.4979757085020243,"F(θ) :=
E
(u,v)∼N(0,Σ(θ))[φ(u)φ(v)] with Σ(θ) :=

1
cos(θ)
cos(θ)
1 
."
T,0.49898785425101216,We note that F(θ) has the following analytic form:
T,0.5,F(θ) = 1
T,0.5010121457489879,"2π (sin(θ) + (π −θ) cos(θ)) ∈[0, 1/2].
(6)"
T,0.5020242914979757,"Similarly,"
T,0.5030364372469636,"E
(u,v)∼N(0,2Σℓ,i,j)[φ(u)φ(v)] = 2
q"
T,0.5040485829959515,"(Kℓ−1)i,i(Kℓ−1)j,j · F(τℓ,i,j),"
T,0.5050607287449392,"where τℓ,i,j := cos−1

(Kℓ−1)i,j
√"
T,0.5060728744939271,"(Kℓ−1)i,i(Kℓ−1)j,j"
T,0.507085020242915,"
. By the induction hypothesis, we have (Kℓ)i,j ∈"
T,0.5080971659919028,"h⊤
ℓ,ihℓ,j± eO(m−1/2) for all i, j ∈[n]. By Lemma F.7, we also have ∥hℓ,i∥2 ∈1±ϵ for all ℓ∈[L] and"
T,0.5091093117408907,"i ∈[n] with probability 1−O(nL)·e−Ω(mϵ2/L). They implies that cos(τℓ,i,j) ∈cos(θ)± eO(m−1/2).
Thus, by Taylor’s theorem, we have"
T,0.5101214574898786,"|F(θℓ,i,j) −F(τℓ,i,j)| ≤eO(m−1/2)."
T,0.5111336032388664,"Therefore, we have

E
(u,v)∼N(0,2bΣℓ,i,j)
[φ(u)φ(v)] −
E
(u,v)∼N(0,2Σℓ,i,j)[φ(u)φ(v)] "
T,0.5121457489878543,"= 2
∥hℓ−1,i∥2∥hℓ−1,j∥2F(θℓ,i,j) −
q"
T,0.5131578947368421,"(Kℓ−1)i,i(Kℓ−1)j,jF(τℓ,i,j)"
T,0.5141700404858299,≤eO(m−1/2).
T,0.5151821862348178,"That is,"
T,0.5161943319838057,"∥c
Kℓ−Kℓ∥∞≤eO(m−1/2).
(7)"
T,0.5172064777327935,Under review as a conference paper at ICLR 2022
T,0.5182186234817814,"Combining Eqs. (5) and (7) together, we get that"
T,0.5192307692307693,"max
(i,j)∈[n]×[n] |h⊤
ℓ,ihℓ,j −(Kℓ)i,j| ≤eO(m−1/2)"
T,0.520242914979757,holds with probability at least 1 −δ
T,0.521255060728745,L for ℓ= h + 1.
T,0.5222672064777328,"By induction, we have proved that for the ﬁrst L −1 layers, the intermediate correlation h⊤
ℓ,ihℓ,j is
close to the intermediate Gram matrix (Kℓ)i,j. Now, we consider the last layer. Recall GL is deﬁned
by Eq. (4), which has the same form as the correlation matrix of a two-layer over-parameterized
neural network with input data {hL−1,i}i∈[n]. Deﬁne"
T,0.5232793522267206,"( bKL)i,j := h⊤
L−1,ihL−1,j ·
E
w∼N(0,2Im)"
T,0.5242914979757085,"
φ′(w⊤hL−1,i)φ′(w⊤hL−1,j)

."
T,0.5253036437246964,"Then, by the analysis of the two-layer case (see for example Song & Yang (2019); Du et al. (2019b)),
we have"
T,0.5263157894736842,"∥GL −bKL∥≤λL 8 ,"
T,0.5273279352226721,"if m = Ω(λ−2
L n2 log(n/δ)), where λL := λmin(KL). It remains to bound ∥bKL −KL∥∞. Equiva-
lently, for any (i, j) ∈[n] × [n],"
T,0.52834008097166,"max
(i,j)∈[n]×[n]"
T,0.5293522267206477,"E
(u,v)∼N(0,2bΣL,i,j)
[φ′(u)φ′(v)] −
E
(u,v)∼N(0,2ΣL,i,j)[φ′(u)φ′(v)] ."
T,0.5303643724696356,The expectation has the following analytic form:
T,0.5313765182186235,"E
(z1,z2)∼N(0,Σ)[φ′(z1)φ′(z2)] = 1"
T,0.5323886639676113,4 + sin−1(ρ)
T,0.5334008097165992,"2π
with Σ =

p2
ρpq
ρpq
q2 
."
T,0.5344129554655871,"By the analysis of the (L −1)-layer, we know that |ρL,i,j −bρL,i,j| ≤eO(m−1/2), where ρL,i,j :=
cos(τL,i,j) and bρL,i,j := cos(θL,i,j). Also, notice that cos(τL,i,j) = F(τL−1,i,j) ∈[0, 1/2] by
Eq. (6). Hence, the derivative of the expectation is bounded, and by Taylor’s theorem, we have"
T,0.5354251012145749,∥bKL −KL∥∞≤eO(m−1/2).
T,0.5364372469635628,It implies that ∥bKL −KL∥≤λL
T,0.5374493927125507,"8 , which further implies that"
T,0.5384615384615384,∥GL −KL∥≤λL 4 .
T,0.5394736842105263,"Equivalently, we get that"
T,0.5404858299595142,λmin(GL) ≥3 4λL
T,0.541497975708502,with probability at least 1 −δ.
T,0.5425101214574899,The lemma is then proved.
T,0.5435222672064778,"E.2
BOUNDS ON THE LEAST EIGENVALUE DURING OPTIMIZATION"
T,0.5445344129554656,"In this section, we generalize the Lemma C.5 in Brand et al. (2021) into multiple layer neural net-
work
Lemma E.3 (Bounds on the least eigenvalue, multiple layer neural network version of Lemma C.5
of Brand et al. (2021)). Suppose m = Ω(λ−2
ℓn2 log(n/δ)), with probability least 1 −δ, for any set
of weights W1, · · · WL satisfying"
T,0.5455465587044535,∥Wℓ−Wℓ(0)∥≤R.
T,0.5465587044534413,then the following holds
T,0.5475708502024291,∥GL(W) −GL(W(0))∥F ≤λL/2.
T,0.548582995951417,Under review as a conference paper at ICLR 2022
T,0.5495951417004049,"Proof. Recall (GL)(W)i,j
=
1
m
Pm
r=1 φ′(⟨(WL)r, hi,L−1⟩)φ′(⟨(WL)r, hj,L−1⟩)h⊤
i,L−1hj,L−1.
For simplicity, let zi,r := (WL)⊤
r hi,L−1 and zi,r(0) := (WL(0))⊤
r hi,L−1(0)."
T,0.5506072874493927,"|(GL(W))i,j −(GL(W(0)))i,j| ="
T,0.5516194331983806,"h⊤
i,L−1hj,L−1
1
m m
X"
T,0.5526315789473685,"r=1
φ′(zi,r)φ′(zj,r) −hi,L−1(0)⊤hj,L−1(0) 1 m m
X"
T,0.5536437246963563,"r=1
φ′(zi,r(0))φ′(zj,r(0)) "
T,0.5546558704453441,"≤
h⊤
i,L−1hj,L−1 −hi,L−1(0)⊤hj,L−1(0)
 · 1 m m
X"
T,0.555668016194332,"r=1
φ′(zi,r)φ′(zj,r)"
T,0.5566801619433198,"+
hi,L−1(0)⊤hj,L−1(0)
 · 1 m  m
X"
T,0.5576923076923077,"r=1
φ′(zi,r)φ′(zj,r) −φ′(zi,r(0))φ′(zj,r(0)) "
T,0.5587044534412956,"≤
h⊤
i,L−1hj,L−1 −hi,L−1(0)⊤hj,L−1(0)
 + λL + eO(m−1/2) m  m
X"
T,0.5597165991902834,"r=1
φ′(zi,r)φ′(zj,r) −φ′(zi,r(0))φ′(zj,r(0)) ,"
T,0.5607287449392713,"where the last step follows from φ′(x) ∈{0, 1} and Lemma E.2."
T,0.5617408906882592,"To upper bound the above two terms, we ﬁrst need to bound the move of hi,L−1. For any ℓ∈[L−1],
we have"
T,0.562753036437247,"∥hi,ℓ−hi,ℓ(0)∥2 = ∥φ(Wℓhi,ℓ−1) −φ(Wℓ(0)hi,ℓ−1(0))∥2
≤∥φ(Wℓhi,ℓ−1) −φ(Wℓhi,ℓ−1(0))∥2
+ ∥φ(Wℓhi,ℓ−1(0)) −φ(Wℓ(0)hi,ℓ−1(0))∥2
≤(∥Wℓ−Wℓ(0)∥+ ∥Wℓ(0)∥) · ∥hi,ℓ−1 −hi,ℓ−1(0)∥2
+ ∥Wℓ−Wℓ(0)∥· ∥hi,ℓ−1(0)∥2
≤(R + cW )∥hi,ℓ−1 −hi,ℓ−1(0)∥2 + R(1 + ϵ),"
T,0.5637651821862348,"where cW := ∥Wℓ(0)∥≤3 by the well-known deviations bounds concerning the singular values
of Gaussian random matrices (Rudelson & Vershynin (2010)). Also, when ℓ= 0, we have ∥hi,0 −
hi,0(0)∥2 = 0. Hence, we get that for all ℓ∈[L −1], i ∈[n],"
T,0.5647773279352226,"∥hi,ℓ−hi,ℓ(0)∥2 ≤
√"
T,0.5657894736842105,"1 + ϵR(2cW )ℓ,"
T,0.5668016194331984,since R ≪1 by our choice of R.
T,0.5678137651821862,"Hence, it implies that
h⊤
i,L−1hj,L−1 −hi,L−1(0)⊤hj,L−1(0)"
T,0.5688259109311741,"≤
(hi,L−1 −hi,L−1(0))⊤hj,L−1
 +
hi,L−1(0)⊤(hj,L−1 −hj,L−1(0))"
T,0.569838056680162,≤2(1 + ϵ)R(2cW )L−1.
T,0.5708502024291497,"Similarly,"
T,0.5718623481781376,"λL + eO(m−1/2) m  m
X"
T,0.5728744939271255,"r=1
φ′(zi,r)φ′(zj,r) −φ′(zi,r(0))φ′(zj,r(0)) "
T,0.5738866396761133,= λL + eO(m−1/2) m
T,0.5748987854251012,"1(WL)⊤
r hi,L−1>0,(WL)⊤
r hj,L−1>0 −1(WL(0))⊤
r hi,L−1(0)>0,(WL(0))⊤
r hj,L−1(0)>0
 ."
T,0.5759109311740891,"By our assumption, we have ∥(WL)r −(WL(0))r∥2 ≤R.
We also know that ∥hi,L−1 −
hi,L−1(0)∥2 ≤√1 + ϵR(2cW )L−1. Then, we can follow the proof in Du et al. (2019b); Song
& Yang (2019) and deﬁne the event Ai,r := ∃w, h : ∥w −(WL(0))r∥2 ≤R, ∥h −hi,L−1(0)∥2 ≤
O(R) such that 1w⊤h>0 ̸= 1(WL(0))⊤
r hi,L−1(0)>0. We have Ai,r happens if and only if |((WL(0))r+
∆w)⊤hi,L−1(0)| < O(R(R + m−1/2)) for some ﬁxed vector ∆w of length at most R. It implies
that"
T,0.5769230769230769,"Pr[Ai,r] =
Pr
z∼N(0,1)[|z| < R + o(R)] ≤O(R)."
T,0.5779352226720648,Under review as a conference paper at ICLR 2022
T,0.5789473684210527,"Hence, using the same proof in the previous work, we get that"
T,0.5799595141700404,"λL + eO(m−1/2) m  m
X"
T,0.5809716599190283,"r=1
φ′(zi,r)φ′(zj,r) −φ′(zi,r(0))φ′(zj,r(0))"
T,0.5819838056680162,≤(λL + m−1/2)R.
T,0.582995951417004,"Putting them together, we have"
T,0.5840080971659919,"|(GL(W))i,j −(GL(W(0)))i,j| ≤(2(1 + ϵ)(2cW )L−1 + λL)R."
T,0.5850202429149798,"And by our choice of R, we get that"
T,0.5860323886639676,∥GL(W) −GL(W(0))∥F ≤(2(1 + ϵ)(2cW )L−1 + λL)nR ≤λL/2.
T,0.5870445344129555,The lemma is then proved.
T,0.5880566801619433,"F
CONVERGENCE ANALYSIS OF ALGORITHM 2"
T,0.5890688259109311,"In this section, we analyze the convergence behavior of Algorithm 2."
T,0.590080971659919,"F.1
PRELIMINARY"
T,0.5910931174089069,"We recall the initialization of our neural network.
Deﬁnition F.1 (Initialization). Let m = mℓfor all ℓ∈[L]. Let m0 = d. We assume weights are
initialized as"
T,0.5921052631578947,"• Each entry of weight vector a ∈RmL is i.i.d. sampled from {−
1
√mL , +
1
√mL } uniformly
at random."
T,0.5931174089068826,"• Each entry of weight matrices Wℓ∈Rmℓ×mℓ−1 sampled from N(0, 2/mℓ)."
T,0.5941295546558705,"We also restate the architecture of our neural network here.
Deﬁnition F.2 (Architecture). Our neural network is a standard L-layer feed-forward neural
network, with the activation functions deﬁned as a scaled version of shifted ReLU activation:
φ(x) = √cb1[x >
p"
T,0.5951417004048583,"2/mb]x, where cb := (2(1 −Φ(b) + bφ(b)))−1/2. Here b is a threshold
value we will pick later. At last layer, we use a scaled version of a vector with its entry being
Rademacher random variables. We deﬁne the neural network function f : Rm0 →R as"
T,0.5961538461538461,"f(W, xi) = a⊤φ(WLφ(WL−1φ(. . . φ(W1xi))))."
T,0.597165991902834,We measure the loss of the neural network via squared-loss function:
T,0.5981781376518218,"L(W) = 1 2 n
X"
T,0.5991902834008097,"i=1
(f(xi) −yi)2."
T,0.6002024291497976,We use ft : Rm0×n →Rn denote the prediction of our network:
T,0.6012145748987854,"ft(X) = [f(W(t), x1), . . . , f(W(t), xn)]⊤."
T,0.6022267206477733,"We state two assumptions here.
Assumption F.3 (Small Spectral Norm). Let t ∈{0, . . . , T} and let R ≤1 be a parameter. We
assume"
T,0.6032388663967612,"max
ℓ∈[L] ∥Wℓ(t) −Wℓ(0)∥≤R."
T,0.604251012145749,"Later, we will invoke this assumption by specifying the choice of R.
Assumption F.4 (Sparsity). Let t ∈{0, . . . , T} and let s ≥1 be an integer parameter. We assume"
T,0.6052631578947368,"∥∆Di,ℓ∥0 ≤s, ∀ℓ∈[L], i ∈[n]."
T,0.6062753036437247,"Later, we will invoke this assumption by specifying the choice of s."
T,0.6072874493927125,"Finally, throughout this entire section, we will assume mℓ= m for any ℓ∈[L]."
T,0.6082995951417004,Under review as a conference paper at ICLR 2022
T,0.6093117408906883,"F.2
TECHNICAL LEMMAS"
T,0.6103238866396761,"We ﬁrst show that during initialization, by using our threshold ReLU activation, the vector hi,ℓis
sparse, hence the diagonal matrix Di,ℓis sparse as well."
T,0.611336032388664,"Lemma F.5 (Sparse initialization). Let σb(x) = max{x −b, 0} be the threshold ReLU activation"
T,0.6123481781376519,"with threshold b > 0. After initialization, with probability 1 −nL · e−Ω(me−b2m/4), it holds for all
i ∈[n] and ℓ∈[L], ∥hi,ℓ∥0 ≤O(m · e−b2m/4)."
T,0.6133603238866396,"Proof. We ﬁx i ∈[n] and ℓ∈[L], since we will union bound over all i and ℓat last. Let ui ∈Rm
be a ﬁxed vector and Wℓ,r to denote the r-th row of Wℓ, then by the concentration of Gaussian, we
have"
T,0.6143724696356275,"Pr[σb(⟨Wℓ,r, ui⟩) > 0] =
Pr
z∼N(0, 2"
T,0.6153846153846154,m )[z > b] ≤exp(−b2m/4).
T,0.6163967611336032,"Let S be the following index set S := {r ∈[m] : ⟨Wℓ,r, ui⟩> b}, the above reasoning means that
for the indicator random variable 1[r ∈S], we have"
T,0.6174089068825911,E[1[r ∈S]] ≤exp(−b2m/4).
T,0.618421052631579,"Use Bernstein’s inequality (Lemma A.3) we have that for all t > 0,"
T,0.6194331983805668,Pr[|S| > k + t] ≤exp(−t2/2
T,0.6204453441295547,"k + t/3),"
T,0.6214574898785425,"where k := m · exp(−b2m/4). By picking t = k, we have"
T,0.6224696356275303,Pr[|S| > 2k] ≤exp(−3k 8 ).
T,0.6234817813765182,"Note that |S| is essentially the quantity ∥hi,ℓ∥0, hence we can union bound over all ℓand i and with
probability at least"
T,0.6244939271255061,"1 −nL · exp(−Ω(m · exp(b2m/4))),"
T,0.6255060728744939,"we have ∥hi,ℓ∥0 ≤2m · exp(−b2m/4)."
T,0.6265182186234818,"Remark F.6. The above lemma shows that by using the shifted ReLU activation, we make sure that
all hi,ℓare sparse after initialization. Speciﬁcally, we use k := m · exp(−b2m/4) as a sparsity
parameter. Later, we might re-scale b so that the probability becomes exp(−b2/2). We stress that
such re-scaling does not affect the sparsity of our initial vectors. If we re-scale b and choose it as
√0.4 log m, then k = m0.8 and hence with high probability, ∥hi,ℓ∥0 ≤O(m0.8)."
T,0.6275303643724697,"As a direct consequence, we note that all initial Di,ℓare k-sparse as well."
T,0.6285425101214575,"We state a lemma that handles the ℓ2 norm of hi,ℓwhen one uses truncated Gaussian distribution
instead. Due to the length and the delicacy of the proof, we defer it to Section G."
T,0.6295546558704453,"Lemma F.7 (Restatement of Lemma G.6). Let b > 0 be a ﬁxed scalar. Let the activation function
φ(x) := √cb1[x >
p"
T,0.6305668016194332,"2/mb]x, where cb := (2(1 −Φ(b) + bφ(b)))−1/2. Let ϵ ∈(0, 1), then over
the randomness of W(0), with probability at least 1 −O(nL) · exp(−Ω(m exp(−b2/2)ϵ2/L2)), we
have"
T,0.631578947368421,"∥hi,ℓ∥2 ∈[1 −ϵ, 1 + ϵ], ∀i ∈[n], ℓ∈[L]."
T,0.6325910931174089,We remark that the parameter e−b2/2m captures the sparsity during the initialization.
T,0.6336032388663968,"The second lemma handles the consecutive product that appears naturally in the gradient computa-
tion."
T,0.6346153846153846,"Lemma F.8 (Variant of Lemma 7.3 in Allen-Zhu et al. (2019a)). Suppose m ≥Ω(nL log(nL)).
With probability at least 1 −e−Ω(k/L2) over the randomness of initializations W1(0), . . . , WL(0) ∈
Rm×m, for all i ∈[n] and 1 ≤a ≤b ≤L,"
T,0.6356275303643725,Under review as a conference paper at ICLR 2022
T,0.6366396761133604,"(a) ∥WbDi,b−1Wb−1 . . . Di,aWa∥≤O(
√ L)."
T,0.6376518218623481,"(b) ∥WbDi,b−1Wb−1 . . . Di,aWav∥2 ≤2∥v∥2, for all vectors with ∥v∥0 ≤O(
m
L log m)."
T,0.638663967611336,"(c) ∥u⊤WbDi,b−1Wb−1 . . . Di,aWa∥2 ≤O(1)∥u∥2, for all vectors u with ∥u∥0 ≤O(
m
L log m)."
T,0.6396761133603239,"(d) For
any
intergers
1
≤
s
≤
O(
m
L log m),
with
probability
at
least
1 −
e−Ω(s log m) over the randomness of initializations W1(0), . . . , WL(0)
∈
Rm×m,
|u⊤WbDi,b−1Wb−1 . . . Di,aWav| ≤∥u∥2∥v∥2 · O(
m
L log m) for all vectors u, v with
∥u∥0, ∥v∥0 ≤s."
T,0.6406882591093117,"The proof is similarly to the original proof of the corresponding lemma in Allen-Zhu et al. (2019a),
however we replace the bound on hi,ℓwith our Lemma F.7. We highlight this does not change the
bound, merely in expense of a worse probability."
T,0.6417004048582996,"The next lemma concerns the bound on the product being used in backpropagation.
Lemma F.9 (Variant of Lemma 7.4 in Allen-Zhu et al. (2019a)). Suppose m ≥Ω(nL log(nL)).
Then with probability at least 1 −e−Ω(log2 m), for all i ∈[n], ℓ∈[L],"
T,0.6427125506072875,"∥a⊤Di,LWL . . . Di,ℓWℓ∥2 ≤O(log m
√m √ L)."
T,0.6437246963562753,"Proof. By Lemma F.8 part (a), we know that ∥WL . . . Di,ℓWℓ∥
≤
O(
√"
T,0.6447368421052632,"L), consequently,
∥Di,LWL . . . Di,ℓWℓ∥≤O(
√"
T,0.645748987854251,"L) since ∥Di,L∥≤1. This means for any vector u ∈Rm, with
probability at least 1 −e−Ω(k/L2), we have"
T,0.6467611336032388,"∥Di,LWL . . . Di,ℓWℓu∥2 ≤O(
√"
T,0.6477732793522267,L)∥u∥2.
T,0.6487854251012146,"Conditioning on this event and using the randomness of Rademacher vector a, we have"
T,0.6497975708502024,"Pr[|a⊤Di,LWL . . . Di,ℓWℓu| ≤t ·
√"
T,0.6508097165991903,"L∥u∥2] ≥1 −2 exp(−mt2 2 ),"
T,0.6518218623481782,"pick t = log m
√m , we know that"
T,0.652834008097166,"Pr[|a⊤Di,LWL . . . Di,ℓWℓu| ≤log m
√m ·
√"
T,0.6538461538461539,"L∥u∥2] ≥1 −2 exp(−log2 m 2
)."
T,0.6548582995951417,"This means with probability at least 1 −e−Ω(log2 m), we have"
T,0.6558704453441295,"∥a⊤Di,LWL . . . Di,ℓWℓ∥2 ≤O(log m
√m √ L)."
T,0.6568825910931174,"The next several lemmas bound norms after small perturbation.
Lemma F.10 (Lemma 8.2 in Allen-Zhu et al. (2019a)). Suppose Assumption F.3 is satisﬁed with
R ≤O(
1
L9/2 log3 m). With probability at least 1 −e−Ω(mR2/3L),"
T,0.6578947368421053,"(a) ∆gi,ℓcan be written as ∆gi,ℓ= ∆gi,ℓ,1 + ∆gi,ℓ,2 where ∥∆gi,ℓ,1∥2 ≤O(RL3/2) and"
T,0.6589068825910931,"∥∆gi,ℓ,2∥∞≤O( RL5/2√log m
√m
)."
T,0.659919028340081,"(b) ∥∆Di,ℓ∥0 ≤O(mR2/3L) and ∥(∆Di,ℓ)gi,ℓ∥2 ≤O(RL3/2)."
T,0.6609311740890689,"(c) ∥∆gi,ℓ∥2, ∥∆hi,ℓ∥2 ≤O(RL5/2√log m).
Remark F.11. Lemma F.10 establishes the connection between parameter R and s of Assump-
tion F.3 and F.4. As long as R is small, then we have s = O(mR2/3L). Such a relation enables us
to pick R to our advantage and ensure the sparsity of ∆Di,ℓis sublinear in m, and hence the update
time per iteration is subquadratic in m."
T,0.6619433198380567,Under review as a conference paper at ICLR 2022
T,0.6629554655870445,"Lemma F.12 (Lemma 8.6 in Allen-Zhu et al. (2019a)). Suppose Assumption F.4 is satisﬁed with
1 ≤s ≤O(
m
L3 log m) and m ≥Ω(nL log(nL)), with probability at least 1 −e−Ω(s log m) over the
randomness of W(0), for every i ∈[n], 1 ≤a ≤b ≤L, we have"
T,0.6639676113360324,"(a) ∥Wb(0)(Di,b−1(0) + ∆Di,b−1) . . . (Di,a(0) + ∆Di,a)Wa(0)∥≤O(
√ L)."
T,0.6649797570850202,"(b) ∥(Wb(0) + ∆Wb)(Di,b−1(0) + ∆Di,b−1) . . . (Di,a(0) + ∆Di,a)(Wa(0) + ∆Wa)∥≤
O(
√"
T,0.6659919028340081,"L) if Assumption F.3 is satisﬁed with R ≤O(
1
L1.5 )."
T,0.667004048582996,"Corollary F.13. Suppose Assumption F.3 is satisﬁed with R
≤
O(
1
L4.5 log3 m) and m
≥"
T,0.6680161943319838,"Ω(nL log(nL)), with probability at least 1 −e−Ω(log2 m) over the randomness of W(0) and a,
for any i ∈[n] and ℓ∈[L], we have"
T,0.6690283400809717,"∥a⊤(Di,L(0) + ∆Di,L(0))(WL(0) + ∆WL) . . . (Wℓ+1(0) + ∆Wℓ+1)(Di,ℓ(0) + ∆Di,ℓ)∥2"
T,0.6700404858299596,"≤eO(
p L/m)."
T,0.6710526315789473,"Proof. We ﬁrst note that by Lemma F.10 and the choice of R, we know that Assumption F.4 is
satisﬁed with 1 ≤s ≤O(
m
L2 log2 m)."
T,0.6720647773279352,"The proceeding proof is identical to Lemma F.9, use P to denote the product (Di,L(0) +
∆Di,L(0))(WL(0) + ∆WL) . . . (Wℓ+1(0) + ∆Wℓ+1)(Di,ℓ(0) + ∆Di,ℓ). Per Lemma F.12, we
know that with the conditions stated, we have"
T,0.6730769230769231,"∥P∥≤O(
√ L),"
T,0.6740890688259109,"conditioning on this even, for any vector u ∈Rm, with probability at least 1−e−Ω(s log m), we know
that"
T,0.6751012145748988,"∥Pu∥2 ≤O(
√"
T,0.6761133603238867,L)∥u∥2.
T,0.6771255060728745,"Use the randomness of vector a, we have"
T,0.6781376518218624,"Pr[|a⊤Pu| ≤t ·
√"
T,0.6791497975708503,L∥u∥2] ≥1 −2 exp(−mt2 2 ).
T,0.680161943319838,"Pick t = log m
√m , we are done."
T,0.6811740890688259,"Lemma F.14 (Variant of Lemma 8.7 in Allen-Zhu et al. (2019a)). Suppose Assumption F.3 is sat-
isﬁed with R ≤O(
1
L4.5 log3 m) and m ≥Ω(nL log(nL)), with probability at least 1 −e−Ω(log2 m)"
T,0.6821862348178138,"over the randomness of W(0) and a, for all i ∈[n], ℓ∈[L]. Deﬁne"
T,0.6831983805668016,"u := a⊤(Di,L(0) + ∆Di,L)(WL(0) + ∆WL(0)) . . . (Wℓ+1(0) + ∆Wℓ+1)(Di,ℓ(0) + ∆Di,ℓ),"
T,0.6842105263157895,"u0 := a⊤Di,L(0)WL(0) . . . Wℓ+1(0)Di,ℓ(0)."
T,0.6852226720647774,It satisﬁes that
T,0.6862348178137652,"∥u −u0∥2 ≤eO(
p L/m)."
T,0.687246963562753,"Proof. Note that we can invoke Corollary F.13 to give an upper bound on ∥u∥2 with probability
1 −e−Ω(log2 m),"
T,0.6882591093117408,"∥u∥2 ≤eO(
p L/m)."
T,0.6892712550607287,"Similarly, we can use Lemma F.9 to give an upper bound on ∥u0∥2 with probability 1−e−Ω(log2 m),"
T,0.6902834008097166,"∥u0∥2 ≤eO(
p L/m)."
T,0.6912955465587044,"Finally, by triangle inequality, we get"
T,0.6923076923076923,"∥u −u0∥2 ≤eO(
p L/m),"
T,0.6933198380566802,with the desired probability.
T,0.694331983805668,Under review as a conference paper at ICLR 2022
T,0.6953441295546559,"F.3
BOUNDS ON INITIALIZATION"
T,0.6963562753036437,"In the following lemma, we generalize the Lemma C.2 in Brand et al. (2021) into multiple layer
neural networks.
Lemma F.15 (Bounds on initialization, multiple layer version of Lemma C.2 in Brand et al. (2021)).
Suppose m = Ω(nL log(nL)), then with probability 1 −O(nL) · e−Ω(k/L2), we have the following"
T,0.6973684210526315,"• f(W, xi) = O(1), ∀i ∈[n]."
T,0.6983805668016194,"• With probability at least 1 −e−Ω(log2 m), we have ∥Jℓ,0,i∥= eO(
p"
T,0.6993927125506073,"L/m), ∀ℓ∈[L], ∀i ∈
[n].
Remark F.16. The bound of m in Brand et al. (2021) has a linear dependence on n because they
need to bound ∥W(0)∥2. If we do not need to bound ∥W(0)∥2, then m does not depend linearly on
n."
T,0.7004048582995951,Proof. We will prove the two parts of the statement separately.
T,0.701417004048583,"Part 1:
By deﬁnition, for any i ∈[n], we have"
T,0.7024291497975709,"f(W, xi) = a⊤φ(WL(φ(· · · φ(W1xi))))."
T,0.7034412955465587,"We shall make use Lemma F.7 here: with probability at least 1−O(nL)·exp(−Ω(k/L2)), we have
∥hi,L∥2 ∈[0.9, 1.1]. Recall that a ∈Rm has each of its entry being a Rademacher random variable
scaled by 1/m, hence it’s 1/m-subgaussian. Using the concentration of subgaussian (Lemma A.5),
we have that"
T,0.7044534412955465,"Pr[|f(W, xi)| ≥t] ≤2 exp(−
t2m
2∥hi,L∥2
),"
T,0.7054655870445344,"ﬁnally by noticing that t = Θ(1) and ∥hi,L∥2 ∈[0.9, 1.1], we conclude that"
T,0.7064777327935222,"Pr[|f(W, xi)| ≥O(1)] ≤2 exp(−Ω(m)),"
T,0.7074898785425101,"union bounding over all i, we conclude that"
T,0.708502024291498,"Pr[f(W, xi) ≥O(1)] ≤O(n) · exp(−Ω(m)), ∀i ∈[n]."
T,0.7095141700404858,"Part 2:
We will combine Lemma F.7 and F.9, with probability at least 1 −O(nL) · e−Ω(k/L2), we
have ∥hi,ℓ∥2 ∈[0.9, 1.1] and with probability at least 1−e−Ω(log2 m), we have ∥a⊤Di,L . . . Wℓ∥2 ≤
eO(
p"
T,0.7105263157894737,"L/m).
Combine them together, we know that with probability at least 1 −e−Ω(log2 m),
∥Jℓ,0,i∥≤eO(
p L/m)."
T,0.7115384615384616,"Remark F.17. By utilizing the structure of vector a, we can show that with a weaker probability
(1 −e−Ω(log2 m)), f(W, xi) has even smaller magnitude ( eO(
1
√m)). For our purpose, f(W, xi) =
O(1) sufﬁces."
T,0.7125506072874493,"F.4
BOUNDS ON SMALL PERTURBATION"
T,0.7135627530364372,"In the following, we generalize the Lemma C.4 in Brand et al. (2021) into multiple layer neural
network. For the simplicity of notation, we set all mℓto be m.
Lemma F.18 (multiple layer version of Lemma C.4 in Brand et al. (2021)). Suppose R ≤
O(
1
L4.5 log3 m) and m = Ω(nL log(nL)). With probability at least 1 −e−Ω(log2 m) over the random
initialization of W(0) = {W1(0), W2(0), · · · WL(0)}, the following holds for any set of weights
W1, · · · , WL satisfying"
T,0.7145748987854251,"∥Wℓ−Wℓ(0)∥≤R, ∀ℓ∈[L]"
T,0.7155870445344129,"• ∥JWℓ,xi −JWℓ(0),xi∥2 = eO(
p L/m)."
T,0.7165991902834008,Under review as a conference paper at ICLR 2022
T,0.7176113360323887,• ∥JWℓ−JWℓ(0)∥F = eO(n1/2p L/m).
T,0.7186234817813765,• ∥JWℓ∥F = eO(n1/2p L/m).
T,0.7196356275303644,"Proof. Part 1. To simplify the notation, we ignore the subscripts i below."
T,0.7206477732793523,Consider the following computation:
T,0.72165991902834,"∥JWℓ,xi −JWℓ(0),xi∥2 = ∥uh⊤
ℓ−1 −u0h⊤
ℓ−1(0)∥F"
T,0.7226720647773279,"= ∥u(hℓ−1(0) + ∆hℓ−1)⊤−u0h⊤
ℓ−1(0)∥F
= ∥(u −u0)h⊤
ℓ−1(0) + u(∆h⊤
ℓ−1)∥F
≤∥(u −u0)h⊤
ℓ−1(0)∥F + ∥u(∆h⊤
ℓ−1)∥F
≤∥u −u0∥2 · ∥hℓ−1(0)∥2 + ∥u∥2 · ∥∆hℓ−1∥2. where"
T,0.7236842105263158,"u := (Dℓ(0) + ∆Dℓ) L
Y"
T,0.7246963562753036,"k=ℓ+1
(Wk(0) + ∆Wk)⊤(Dk(0) + ∆Dk) ! a"
T,0.7257085020242915,"u0 := Dℓ(0) L
Y"
T,0.7267206477732794,"k=ℓ+1
Wk(0)⊤Dk(0) ! a"
T,0.7277327935222672,"By Lemma F.14, we know that with probability at least 1 −e−Ω(log2 m), we have"
T,0.728744939271255,"∥u −u0∥2 ≤eO(
p L/m)."
T,0.729757085020243,"By Lemma F.7, we know that with probability at least 1 −O(nL) · e−k/L2, we have"
T,0.7307692307692307,"∥hℓ−1(0)∥2 ∈[0.9, 1.1]."
T,0.7317813765182186,"By Corollary F.13, we know that with probability at least 1 −e−Ω(log2 m), we have"
T,0.7327935222672065,"∥u∥2 ≤eO(
p L/m)."
T,0.7338056680161943,"By Lemma F.10, we know that with probability at least 1 −e−Ω(mR2/3L), we have"
T,0.7348178137651822,∥∆hℓ−1∥2 ≤eO(RL2.5).
T,0.7358299595141701,"Note that due to the choice of R, we know that"
T,0.7368421052631579,∥∆hℓ−1∥2 ≤1.
T,0.7378542510121457,"Taking a union bound over all events, with probability at least 1 −e−Ω(log2 m), we achieve the
following bound:"
T,0.7388663967611336,"∥JWℓ,xi −JWℓ(0),xi∥2 ≤eO(
p L/m)."
T,0.7398785425101214,"Part 2. Note that the squared Frobenious norm is just the sum of all squared ℓ2 norm of rows, hence"
T,0.7408906882591093,∥JWℓ−JWℓ(0)∥F ≤eO(n1/2p L/m).
T,0.7419028340080972,Part 3. We will prove by triangle inequality:
T,0.742914979757085,∥JWℓ∥F ≤∥JWℓ(0)∥F + ∥JWℓ−JWℓ(0)∥F
T,0.7439271255060729,≤eO(n1/2p
T,0.7449392712550608,L/m) + eO(n1/2p L/m)
T,0.7459514170040485,= eO(n1/2p L/m).
T,0.7469635627530364,Under review as a conference paper at ICLR 2022
T,0.7479757085020243,"F.5
PUTTING IT ALL TOGETHER"
T,0.7489878542510121,"In this section, we will prove the following core theorem that analyzes the convergence behavior of
Algorithm 2:"
T,0.75,"Theorem F.19 (Formal version of Theorem 1.1). Suppose the width of the neural network satis-
ﬁes m = Ω(λ−2
L n2L2), then with probability at least 1 −e−Ω(log2 m) over the randomness of the
initialization of the neural network and the randomness of the algorithm, Algorithm 2 satisﬁes"
T,0.7510121457489879,∥ft+1 −y∥2 ≤1
T,0.7520242914979757,2∥ft −y∥2.
T,0.7530364372469636,"Before moving on, we introduce several deﬁnitions and prove some useful facts related to them."
T,0.7540485829959515,Deﬁnition F.20 (function J). We deﬁne
T,0.7550607287449392,"Jℓ(Z1, . . . , ZL)i := Di,ℓ(Zℓ) L
Y"
T,0.7560728744939271,"k=ℓ+1
Z⊤
k Di,k(Zk)a(hi(Z1, . . . , Zℓ−1))⊤
∈Rmℓ×mℓ−1 where"
T,0.757085020242915,"Di,ℓ(Zℓ) := diag(φ′(Zℓhi(Z1, . . . , Zℓ−1))),
∈Rmℓ×mℓ"
T,0.7580971659919028,"hi(Z1, . . . , Zℓ−1) := φ(Zℓ−1(φ(Zℓ−2 · · · (φ(Z1xi)))))
∈Rmℓ−1"
T,0.7591093117408907,"Fact F.21. Let Jℓdenote the function be deﬁned as Deﬁnition F.20. For any t ∈{0, . . . , T}, we
have"
T,0.7601214574898786,"ft+1 −ft = L
X ℓ=1 Z 1"
T,0.7611336032388664,"0
Jℓ((1 −s)W(t) + sW(t + 1))ds
⊤
· vec(Wℓ(t + 1) −Wℓ(t)),"
T,0.7621457489878543,"Proof. For i ∈[n], consider the i-th coordinate."
T,0.7631578947368421,"(ft+1 −ft)i =
Z 1"
T,0.7641700404858299,"0
f((1 −s)W(t) + sW(t + 1), xi)′ds =
Z 1 0 L
X ℓ=1  ∂f"
T,0.7651821862348178,"∂Wℓ
((1 −s)W(t) + sW(t + 1), xi)
⊤
· vec(Wℓ(t + 1) −Wℓ(t))ds = L
X ℓ=1 Z 1"
T,0.7661943319838057,"0
Jℓ((1 −s)W(t) + sW(t + 1))ids
⊤
· vec(Wℓ(t + 1) −Wℓ(t)),"
T,0.7672064777327935,"Thus, we complete the proof."
T,0.7682186234817814,"Fact F.22. For any t ∈{0, . . . , T}, we have Jℓ(W1(t), . . . , WL(t)) = Jℓ,t."
T,0.7692307692307693,"Proof. In order to simplify the notation, we drop the term t below."
T,0.770242914979757,"We note that for i ∈[n], the i-th row of matrix Jℓ,t is deﬁned as Di,ℓ( L
Y"
T,0.771255060728745,"k=ℓ+1
W ⊤
k Di,k)ah⊤
i,ℓ−1,"
T,0.7722672064777328,"where Di,ℓ= diag(φ′(Wℓhi,ℓ−1)) and hi,ℓ−1 = φ(Wℓ−1(φ(Wℓ−2 . . . (φ(W1xi))))), this is essen-
tially the same as hi(W1, . . . , Wℓ−1) and Di,ℓ(Wℓ). This completes the proof."
T,0.7732793522267206,We state the range we require for parameter R:
T,0.7742914979757085,"Deﬁnition F.23. We choose R so that
n
√"
T,0.7753036437246964,"mL
· 1"
T,0.7763157894736842,"λL
≤R ≤
1
L4.5 log3 m."
T,0.7773279352226721,Under review as a conference paper at ICLR 2022
T,0.77834008097166,"Remark F.24. Recall that the sparsity parameter s is directly related to R: s = O(mR2/3L), hence
to ensure the sparsity is small, we shall pick R as small as possible. Speciﬁcally, if we pick R to be
n
√"
T,0.7793522267206477,"mLλL , then s = O(λ−1
L n
√"
T,0.7803643724696356,"mL), as long as m ≫λ−2
L n2L then s ≈O(√m)."
T,0.7813765182186235,"Next, we pick the value of m:
Deﬁnition F.25. We choose m to be
m ≥Ω(λ−10/3
L
n10/3L5/3).
Remark F.26. The choice of m here makes sure that, as long as we pick R matching its lower
bound, then the sparsity s = O(m0.8), this matches the other sparsity parameter k, which is also in
the order of O(m0.8)."
T,0.7823886639676113,"We use induction to prove the following two claims recursively.
Deﬁnition F.27 (Induction hypothesis 1). Let t ∈[T] be a ﬁxed integer. We have
∥Wℓ(t) −Wℓ(0)∥≤R
holds for any ℓ∈[L].
Deﬁnition F.28 (Induction Hypothesis 2). Let t ∈[T] be a ﬁxed integer. We have"
T,0.7834008097165992,∥ft −y∥2 ≤1
T,0.7844129554655871,2∥ft−1 −y∥2.
T,0.7854251012145749,"Suppose the above two claims hold up to t, we prove they continue to hold for time t + 1. The
second claim is more delicate, we are going to prove it ﬁrst and we deﬁne"
T,0.7864372469635628,"Jℓ,t,t+1 :=
Z 1"
T,0.7874493927125507,"0
Jℓ

(1 −s)Wt + sWt+1

ds,"
T,0.7884615384615384,"where Jℓis deﬁned as Deﬁnition F.20.
Lemma F.29. Let g⋆
ℓ:= (Jℓ,tJ⊤
ℓ,t)−1 · 1"
T,0.7894736842105263,L(ft −y). We have
T,0.7904858299595142,"∥ft+1 −y∥2 ≤∥ft −y − L
X"
T,0.791497975708502,"ℓ=1
Jℓ,tJ⊤
ℓ,tgℓ,t∥2 + L
X"
T,0.7925101214574899,"ℓ=1
∥(Jℓ,t −Jℓ,t,t+1)J⊤
ℓ,tg⋆
ℓ∥2 + L
X"
T,0.7935222672064778,"ℓ=1
∥(Jℓ,t −Jℓ,t,t+1)J⊤
ℓ,t(gℓ,t −g⋆
ℓ)∥2.
(8)"
T,0.7945344129554656,"Proof. Consider the following computation:
∥ft+1 −y∥2
= ∥ft −y + (ft+1 −ft)∥2"
T,0.7955465587044535,"= ∥ft −y + L
X"
T,0.7965587044534413,"ℓ=1
Jℓ,t,t+1 · vec(Wℓ,t+1 −Wℓ,t)∥2"
T,0.7975708502024291,"= ∥ft −y − L
X"
T,0.798582995951417,"ℓ=1
Jℓ,t,t+1 · J⊤
ℓ,tgℓ,t∥2"
T,0.7995951417004049,"= ∥ft −y − L
X"
T,0.8006072874493927,"ℓ=1
Jℓ,tJ⊤
ℓ,tgℓ,t + L
X"
T,0.8016194331983806,"ℓ=1
Jℓ,tJ⊤
ℓ,tgℓ,t − L
X"
T,0.8026315789473685,"ℓ=1
Jℓ,t,t+1J⊤
ℓ,tgℓ,t∥2"
T,0.8036437246963563,"≤∥ft −y − L
X"
T,0.8046558704453441,"ℓ=1
Jℓ,tJ⊤
ℓ,tgℓ,t∥2 + L
X"
T,0.805668016194332,"ℓ=1
∥(Jℓ,t −Jℓ,t,t+1)J⊤
ℓ,tgℓ,t∥2"
T,0.8066801619433198,"≤∥ft −y − L
X"
T,0.8076923076923077,"ℓ=1
Jℓ,tJ⊤
ℓ,tgℓ,t∥2 + L
X"
T,0.8087044534412956,"ℓ=1
∥(Jℓ,t −Jℓ,t,t+1)J⊤
ℓ,tg⋆
ℓ∥2 + L
X"
T,0.8097165991902834,"ℓ=1
∥(Jℓ,t −Jℓ,t,t+1)J⊤
ℓ,t(gℓ,t −g⋆
ℓ)∥2,"
T,0.8107287449392713,"The second step follows from the deﬁnition of Jℓ,t,t+1 and simple calculus."
T,0.8117408906882592,Under review as a conference paper at ICLR 2022
T,0.812753036437247,Claim F.30 (1st term in Eq. (8)). We have
T,0.8137651821862348,"∥(ft −y) − L
X"
T,0.8147773279352226,"ℓ=1
Jℓ,tJ⊤
ℓ,tgℓ,t∥2 ≤1"
T,0.8157894736842105,6∥ft −y∥2.
T,0.8168016194331984,Proof. We have
T,0.8178137651821862,"∥(ft −y) − L
X"
T,0.8188259109311741,"ℓ=1
Jℓ,tJ⊤
ℓ,tgℓ,t∥2 = ∥ L
X"
T,0.819838056680162,"ℓ=1
( 1"
T,0.8208502024291497,"L(ft −y) −Jℓ,tJ⊤
ℓ,tgℓ,t)∥2 ≤ L
X"
T,0.8218623481781376,"ℓ=1
∥1"
T,0.8228744939271255,"L · (ft −y) −Jℓ,tJ⊤
ℓ,tgℓ,t∥2 ≤1"
T,0.8238866396761133,"6∥ft −y∥2,
(9)"
T,0.8248987854251012,"since gℓ,t is an ϵ0 (ϵ0 ≤1"
T,0.8259109311740891,6) approximate solution to regression problem
T,0.8269230769230769,"min
g
∥Jℓ,tJ⊤
ℓ,tg −1"
T,0.8279352226720648,L(ft −y)∥2.
T,0.8289473684210527,"Claim F.31 (2nd term in Eq. (8)). We have L
X"
T,0.8299595141700404,"ℓ=1
∥(Jℓ,t −Jℓ,t,t+1)J⊤
ℓ,tg⋆
ℓ∥2 ≤1"
T,0.8309716599190283,6∥ft −y∥2.
T,0.8319838056680162,"Proof. For the second term in Eq. (8), for any ℓ∈[L], we have"
T,0.832995951417004,"∥(Jℓ,t −Jℓ,t,t+1)J⊤
ℓ,tg⋆
ℓ∥2 ≤∥Jℓ,t −Jℓ,t,t+1∥· ∥J⊤
ℓ,tg⋆
ℓ∥2"
T,0.8340080971659919,"= ∥Jℓ,t −Jℓ,t,t+1∥· ∥J⊤
ℓ,t(Jℓ,tJ⊤
ℓ,t)−1 · 1"
T,0.8350202429149798,L(ft −y)∥2 ≤1
T,0.8360323886639676,"L · ∥Jℓ,t −Jℓ,t,t+1∥· ∥J⊤
ℓ,t(Jℓ,tJ⊤
ℓ,t)−1∥· ∥ft −y∥2.
(10)"
T,0.8370445344129555,"We bound these term separately. First,"
T,0.8380566801619433,"∥Jℓ,t −Jℓ,t,t+1∥=
Jℓ(Wt) −
Z 1"
T,0.8390688259109311,"0
Jℓ((1 −s)Wt + sWt+1)ds ≤
Z 1"
T,0.840080971659919,"0
∥Jℓ(Wt) −Jℓ((1 −s)Wt + sWt+1)∥ds ≤
Z 1"
T,0.8410931174089069,"0
∥Jℓ(Wt) −Jℓ(W0)∥+ ∥Jℓ(W0) −Jℓ((1 −s)Wt + sWt+1)∥ds"
T,0.8421052631578947,"≤∥Jℓ(Wt) −Jℓ(W0)∥+
Z 1"
T,0.8431174089068826,"0
∥Jℓ(W0) −Jℓ((1 −s)Wt + sWt+1)∥ds"
T,0.8441295546558705,≤eO(n1/2p
T,0.8451417004048583,"L/m),
(11)"
T,0.8461538461538461,"where by Fact F.22, we know ∥Jℓ(Wt)−Jℓ(W0)∥= ∥JWℓ(t) −JWℓ(0)∥≤eO(n1/2p"
T,0.847165991902834,"L/m), the last
inequality is by Lemma F.18. For the second term, we have"
T,0.8481781376518218,"∥(1 −s) · vec(Wℓ(t)) + s · vec(Wℓ(t + 1)) −vec(Wℓ(0))∥2
≤(1 −s) · ∥vec(Wℓ(t)) −vec(Wℓ(0))∥2 + s · ∥vec(Wℓ(t + 1)) −vec(Wℓ(0))∥2
= (1 −s) · ∥Wℓ(t) −Wℓ(0)∥F + s · ∥Wℓ(t + 1) −Wℓ(0)∥F
≤O(R)."
T,0.8491902834008097,Under review as a conference paper at ICLR 2022
T,0.8502024291497976,"This means the perturbation of (1 −s)Wℓ(t) + sWℓ(t + 1) with respect to Wℓ(0) is small, for any
ℓ∈[L], hence ∥Jℓ(W0) −Jℓ((1 −s)Wt + sWt+1)∥= eO(n1/2p L/m)."
T,0.8512145748987854,"Furthermore, we have"
T,0.8522267206477733,"∥J⊤
ℓ,t(Jℓ,tJ⊤
ℓ,t)−1∥=
1
σmin(J⊤
ℓ,t) ≤
p"
T,0.8532388663967612,"2/λL,
(12)"
T,0.854251012145749,"where the second inequality follows from σmin(Jℓ,t)
=
q"
T,0.8552631578947368,"λmin(Jℓ,tJ⊤
ℓ,t)
≥
p"
T,0.8562753036437247,"λL/2 (see
Lemma E.3)."
T,0.8572874493927125,"Combining Eq. (10), (11) and (12), we have L
X"
T,0.8582995951417004,"ℓ=1
∥(Jℓ,t −Jℓ,t,t+1)J⊤
ℓ,tg⋆∥2 ≤eO(
p"
T,0.8593117408906883,"nL/m) · λ−1/2
L
· ∥ft −y∥2 ≤1"
T,0.8603238866396761,"6∥ft −y∥2,
(13)"
T,0.861336032388664,where the last step follows from choice of m (Deﬁnition F.25).
T,0.8623481781376519,"Claim F.32 (3rd term in Eq. (8)). We have L
X"
T,0.8633603238866396,"ℓ=1
∥(Jℓ,t −Jℓ,t,t+1)J⊤
ℓ,t(gℓ,t −g⋆
ℓ)∥2 ≤1"
T,0.8643724696356275,6∥ft −y∥2
T,0.8653846153846154,Proof. We can show
T,0.8663967611336032,"∥(Jℓ,t −Jℓ,t,t+1)J⊤
ℓ,t(gℓ,t −g⋆
ℓ)∥2 ≤∥Jℓ,t −Jℓ,t,t+1∥· ∥J⊤
ℓ,t∥· ∥gℓ,t −g⋆
ℓ∥2.
(14)"
T,0.8674089068825911,"Moreover, one has
λL"
T,0.868421052631579,"2 ∥gℓ,t −g⋆
ℓ∥2 ≤λmin(Jℓ,tJ⊤
ℓ,t) · ∥gℓ,t −g⋆
ℓ∥2"
T,0.8694331983805668,"≤∥Jℓ,tJ⊤
ℓ,tgℓ,t −Jℓ,tJ⊤
ℓ,tg⋆
ℓ∥2"
T,0.8704453441295547,"= ∥Jℓ,tJ⊤
ℓ,tgℓ,t −1"
T,0.8714574898785425,L(ft −y)∥2 ≤ p λL/n
T,0.8724696356275303,"L
· ∥ft −y∥2.
(15)"
T,0.8734817813765182,"The ﬁrst step comes from λmin(Jℓ,tJ⊤
ℓ,t) = λmin(Gℓ,t) ≥λL/2 (see Lemma E.3) and the last step
comes from gt is an ϵ0 approximate solution. The fourth step follows from Eq. (15) and the fact that
∥(Jℓ,tJ⊤
ℓ,t)−1∥≤2/λL. The last step follows from gt is an ϵ0 (ϵ0 ≤
p"
T,0.8744939271255061,"λL/n) approximate solution
to the regression."
T,0.8755060728744939,"Consequently, we have"
T,0.8765182186234818,"∥(Jℓ,t −Jℓ,t,t+1)J⊤
ℓ,t(gℓ,t −g⋆
ℓ)∥2 ≤∥Jℓ,t −Jℓ,t,t+1∥· ∥J⊤
ℓ,t∥· ∥gℓ,t −g⋆
ℓ∥2"
T,0.8775303643724697,≤eO(n1/2p
T,0.8785425101214575,L/m) · eO(n1/2p
T,0.8795546558704453,"L/m) ·
2
L√nλL
· ∥ft −y∥2"
T,0.8805668016194332,"= eO(
√n"
T,0.881578947368421,"m )
2
√λL
· ∥ft −y∥2.
(16)"
T,0.8825910931174089,"The second step follows from Eq. (11) and (15) and the fact that ∥Jℓ,t∥≤eO(
p"
T,0.8836032388663968,"nL/m) (see
Lemma F.18)."
T,0.8846153846153846,"Finally, we have L
X"
T,0.8856275303643725,"ℓ=1
∥(Jℓ,t −Jℓ,t,t+1)J⊤
ℓ,t(gℓ,t −g⋆
ℓ)∥2 ≤eO(
√nL"
T,0.8866396761133604,"m )
2
√λL
· ∥ft −y∥2 ≤1"
T,0.8876518218623481,6∥ft −y∥2.
T,0.888663967611336,Under review as a conference paper at ICLR 2022
T,0.8896761133603239,The last step follows from choice of m (Deﬁnition F.25).
T,0.8906882591093117,Lemma F.33 (Putting it all together). We have
T,0.8917004048582996,∥ft+1 −y∥2 ≤1
T,0.8927125506072875,"2∥ft −y∥2.
(17)"
T,0.8937246963562753,"Proof. Combining Eq. (8), (9), (13), and (??), we have proved the second claim, i.e.,"
T,0.8947368421052632,∥ft+1 −y∥2 ≤1
T,0.895748987854251,2∥ft −y∥2.
T,0.8967611336032388,"F.6
WEIGHTS ARE NOT MOVING TOO FAR"
T,0.8977732793522267,"Lemma F.34. Let R be chosen as in Deﬁnition F.23, then the following holds:
∥Wℓ(t + 1) −Wℓ(0)∥≤R, ∀ℓ∈[L]."
T,0.8987854251012146,"Proof. It remains to show that Wt does not move far away from W0. First, we have
∥gℓ,t∥2 ≤∥g⋆
ℓ∥2 + ∥gℓ,t −g⋆
ℓ∥2 = 1"
T,0.8997975708502024,"L∥(Jℓ,tJ⊤
ℓ,t)−1(ft −y)∥2 + ∥gℓ,t −g⋆
ℓ∥2 ≤1"
T,0.9008097165991903,"L∥(Jℓ,tJ⊤
ℓ,t)−1∥· ∥(ft −y)∥2 + ∥gℓ,t −g⋆
ℓ∥2"
T,0.9018218623481782,"≤
2
LλL
· ∥ft −y∥2 +
2
L√nλL
· ∥ft −y∥2"
T,0.902834008097166,"≲
1
LλL
· ∥ft −y∥2
(18)"
T,0.9038461538461539,"where the third step follows from Eq. (15) and the last step follows from the obvious fact that
1/√nλL ≤1/λL."
T,0.9048582995951417,"Then
∥Wℓ(k + 1) −Wℓ(k)∥= ∥J⊤
ℓ,kgℓ,k∥"
T,0.9058704453441295,"≤∥Jℓ,k∥· ∥gℓ,k∥2"
T,0.9068825910931174,"≤eO(
p"
T,0.9078947368421053,"nL/m) ·
1
LλL
· ∥fk −y∥2"
T,0.9089068825910931,"≤eO(
p"
T,0.909919028340081,"n/Lm) ·
1
2kλL
· ∥f0 −y∥2"
T,0.9109311740890689,"≤eO(
n
√"
T,0.9119433198380567,"Lm
) ·
1
2kλL
."
T,0.9129554655870445,"The third step uses the fact that ∥Jℓ,k∥≤∥Jℓ,k∥F ≤O(p n"
T,0.9139676113360324,"Lm) by Lemma F.18, and the last step
uses the fact that both ∥f0∥2 and ∥y∥2 are in the order of O(1)."
T,0.9149797570850202,"Consequently, we have"
T,0.9159919028340081,"∥Wℓ(t + 1) −Wℓ(0)∥≤ t
X"
T,0.917004048582996,"k=0
∥Wℓ(k + 1) −Wℓ(k)∥ ≤ t
X"
T,0.9180161943319838,"k=0
eO(
n
√"
T,0.9190283400809717,"Lm
) · 1 λL 1
2k"
T,0.9200404858299596,"≤eO(
n
√"
T,0.9210526315789473,"Lm
) · 1 λL
."
T,0.9220647773279352,"By the choice of R (Deﬁnition F.23), we know this is upper bounded by R. This concludes our
proof."
T,0.9230769230769231,Under review as a conference paper at ICLR 2022
T,0.9240890688259109,"G
PROOF OF LEMMA F.7"
T,0.9251012145748988,"In this section, we prove a technical lemma (Lemma F.7) involving truncated gaussian distribution,
which correlates to the shifted ReLU activation we use.
Deﬁnition G.1 (Truncated Gaussian distribution). Suppose X ∼N(0, σ2). Let b ∈R. Then, we say
a random variable Y follows from a truncated Gaussian distribution Nb(0, σ2) if Y = X|X ≥b.
The probability density function for Nb(0, σ2) is as follows:"
T,0.9261133603238867,"f(y) =
1
σ(1 −Φ(b/σ)) ·
1
√"
T,0.9271255060728745,"2π e−y2/(2σ2)
y ∈[b, ∞),"
T,0.9281376518218624,"where Φ(·) is the standard Gaussian distribution’s CDF.
Fact G.2 (Properties of truncated Gaussian distribution). For b ∈R, suppose X ∼Nb(0, σ2). Let
β := b/σ. Then, we have"
T,0.9291497975708503,"• E[X] =
σφ(β)
1−Φ(β), where φ(x) :=
1
√"
T,0.930161943319838,2πe−x2/2.
T,0.9311740890688259,• Var[X] = σ2(1 + βφ(β)/(1 −Φ(β)) −(φ(β)/(1 −Φ(β)))2).
T,0.9321862348178138,"• X/σ ∼Nb/σ(0, 1)."
T,0.9331983805668016,"• When σ = 1, X is C(b + 1)-subgaussian, where C > 0 is an absolute constant.
Lemma G.3 (Concentration inequality for b-truncated chi-square distribution). For b ∈R, n > 0,
let X ∼χ2
b,n; that is, X = Pn
i=1 Y 2
i where Y1, . . . , Yn ∼Nb(0, 1) are independent b-truncated
Gaussian random variables. Then, there exist two constants C1, C2 such that for any t > 0,"
T,0.9342105263157895,"Pr
X −n(1 +
bφ(b)
1 −Φ(b))
 ≥nt

≤exp
 
−C1nt2/b4
+ exp
 
−C2nt/b2
."
T,0.9352226720647774,"In particular, we have"
T,0.9362348178137652,"Pr [|X −n(1 + b(b + 1))| ≥t] ≤exp
 
−C1t2/(nb4)

+ exp
 
−C2t/b2
."
T,0.937246963562753,"Proof. Since we know that Yi ∼Nb(0, 1) is C(b + 1)-subgaussian, it implies that Y 2
i is a sub-
exponential random variable with parameters (4
√"
T,0.9382591093117408,"2C2(b+1)2, 4C2(b+1)2). Hence, by the standard
concentration of sub-exponential random variables, we have Pr "" n
X"
T,0.9392712550607287,"i=1
Y 2
i −n E[Y 2
i ] ≥nt # ≤ 
 "
EXP,0.9402834008097166,"2 exp

−
nt2
2·32C4(b+1)4

if nt ≤8C2(b + 1)2"
EXP,0.9412955465587044,"2 exp

−
nt
2·4C2(b+1)2

otherwise"
EXP,0.9423076923076923,"≤2 exp
 
−C1nt2/b4
+ 2 exp
 
−C2nt/b2
."
EXP,0.9433198380566802,"Fact G.4. Let h ∈Rp be ﬁxed vectors and h ̸= 0, let b > 0 be a ﬁxed scalar, W ∈Rm×p be
random matrix with i.i.d. entries Wi,j ∼N(0, 2"
EXP,0.944331983805668,"m) and vector v ∈Rm deﬁned as vi = φ((Wh)i) =
1[(Wh)i ≥b](Wh)i. Then"
EXP,0.9453441295546559,"• |vi| follows i.i.d. from the following distribution: with probability 1−e−b2m/(4∥h∥2), |vi| =
0, and with probability e−b2m/(4∥h∥2), |vi| follows from truncated Gaussian distribution
Nb(0, 2"
EXP,0.9463562753036437,"m∥h∥2
2)."
EXP,0.9473684210526315,"•
m∥v∥2
2
2∥h∥2
2 is in distribution identical to χ2
b′,ω (b′-truncated chi-square distribution of order ω)"
EXP,0.9483805668016194,"where ω follows from binomial distribution B(m, e−b2m/(4∥h∥2)) and b′ =
√"
EXP,0.9493927125506073,"m/2
∥h∥2 b."
EXP,0.9504048582995951,"Proof. We assume each vector Wi is generated by ﬁrst generating a gaussian vector g ∼N(0, 2"
EXP,0.951417004048583,"mI)
and then setting Wi = ±g where the sign is chosen with half-half probability. Now, |⟨Wi, h⟩| =
|⟨g, h⟩| only depends on g, and is in distribution identical to Nb(0, 2"
EXP,0.9524291497975709,"m∥h∥2
2). Next, after the sign is"
EXP,0.9534412955465587,Under review as a conference paper at ICLR 2022
EXP,0.9544534412955465,"determined, the indicator 1[(Wih)i ≥b] is 1 with probability e−b2m/(4∥h∥2) and 0 with probability
1−e−b2m/(4∥h∥2). Therefore, |vi| satisﬁes the aforementioned distribution. As for ∥v∥2
2, letting ω ∈
{0, 1, . . . , m} be the variable indicates how many indicators are 1, then ω ∼B(m, e−b2m/(4∥h∥2))"
EXP,0.9554655870445344,"and m∥v∥2
2
2∥h∥2
2 ∼χ2
b′,ω, where b′ =
√"
EXP,0.9564777327935222,"m/2
∥h∥2 b."
EXP,0.9574898785425101,"Fact G.5 (Gaussian tail bound). For any b > 0, we have"
EXP,0.958502024291498,e−b2/2
EXP,0.9595141700404858,"C(b + 1) ≤1 −Φ(b) ≤e−b2/2,"
EXP,0.9605263157894737,where C is an absolute constant.
EXP,0.9615384615384616,"We prove a truncated Gaussian version of Lemma 7.1 of Allen-Zhu et al. (2019a).
Lemma G.6. Let b > 0 be a ﬁxed scalar. Let the activation function be deﬁned as φ(x) :=
√cb1[x >
p"
EXP,0.9625506072874493,"2/mb]x, where cb := (2(1 −Φ(b) + bφ(b)))−1/2. Let ϵ ∈(0, 1), then over the
randomness of W(0), with probability at least 1 −O(nL) · exp(−Ω(m exp(−b2/2)ϵ2/L2)), we
have"
EXP,0.9635627530364372,"∥hi,ℓ∥2 ∈[1 −ϵ, 1 + ϵ], ∀i ∈[n], ℓ∈[L]."
EXP,0.9645748987854251,"Proof. We only prove for a ﬁxed i ∈[n] and ℓ∈{0, 1, 2, . . . , L} because we can apply union bound
at the end. Below, we drop the subscript i for notational convenience, and write hi,ℓand xi as hℓ
and x respectively."
EXP,0.9655870445344129,"According to Fact G.4, ﬁxing any hℓ−1 ̸= 0 and letting Wℓbe the only source of randomness, we
have
m"
EXP,0.9665991902834008,"2 ∥hℓ∥2
2 ∼χ2
b/∥h∥2,ω,
with ω ∼B(m, 1 −Φ(b′)),"
EXP,0.9676113360323887,where b′ := b/∥hℓ−1∥2.
EXP,0.9686234817813765,"We ﬁrst consider the ℓ= 1 case. Then, we have ∥hℓ−1∥2 = 1, and b′ = b. Let Pb := 1 −Φ(b). By
Chernoff bound, for any δ ∈(0, 1), we have"
EXP,0.9696356275303644,Pr[ω ∈(1 ± δ)mPb] ≥1 −exp(−Ω(δ2Pbm)).
EXP,0.9706477732793523,"In the following proof, we condition on this event. By Fact G.5,"
EXP,0.97165991902834,"ω ∈(1 ± δ)Pbm ⇐⇒ω ∈ """
EXP,0.9726720647773279,(1 −δ) e−b2/2
EXP,0.9736842105263158,"C(b + 1)m, (1 + δ) exp(−b2/2)m # ."
EXP,0.9746963562753036,"By Lemma G.3, we have"
EXP,0.9757085020242915,"Pr

m"
EXP,0.9767206477732794,"2 ∥h1∥2
2 −ω

1 + bφ(b) Pb"
EXP,0.9777327935222672," > t

≤exp
 
−Ω(t2/(ωb4))

+ exp
 
−Ω(t/b2)
"
EXP,0.978744939271255,Note that
EXP,0.979757085020243,"ω

1 + bφ(b) Pb"
EXP,0.9807692307692307,"
∈(1 ± δ)mPb + (1 ± δ)mPb · bφ(b)"
EXP,0.9817813765182186,"Pb
= (1 ± δ)(Pb + bφ(b)) · m."
EXP,0.9827935222672065,"Let c−1
b
:= 2(Pb + bφ(b)) be the normalization constant. Then, we have"
EXP,0.9838056680161943,"Pr[|cb∥h1∥2
2 −(1 ± δ)| > 2tcb/m] ≤exp
 
−Ω(t2/(ωb4))

+ exp
 
−Ω(t/b2)

."
EXP,0.9848178137651822,"We want 2tcb/m = O(δ), i.e., t = O(δc−1
b m). Then, we have ωt = mΩ(1) > b2. Hence, by
Lemma G.3, we actually have"
EXP,0.9858299595141701,"Pr[|cb∥h1∥2
2 −(1 ± δ)| > O(δ)] ≤exp
 
−Ω(δm/(cbb2))

."
EXP,0.9868421052631579,"By taking δ = ϵ/L, we get that"
EXP,0.9878542510121457,"∥h1∥2
2 ∈[1 −ϵ/L, 1 + ϵ/L]"
EXP,0.9888663967611336,Under review as a conference paper at ICLR 2022
EXP,0.9898785425101214,with probability at least
EXP,0.9908906882591093,"1 −exp(−Ω(ϵ2Pbm/L2)) −exp
 
−Ω(ϵm/(cbb2L))

≥1 −exp(−Ω(ϵ2Pbm/L2)),"
EXP,0.9919028340080972,"where the last step follows from
1
cbb2 = Pb+bφ(b)"
EXP,0.992914979757085,"b2
= Θ(Pb)."
EXP,0.9939271255060729,"We can inductively prove the ℓ> 1 case. Since the blowup of the norm of h1 is 1 ± ϵ/L, the con-
centration bound is roughly the same for hℓfor ℓ≥2. Thus, by carefully choosing the parameters,
we can achieve ∥hℓ∥2
2 ∈[(1 −ϵ/L)ℓ, (1 + ϵ/L)ℓ] with high probability."
EXP,0.9949392712550608,"In this end, by a union bound over all the layers ℓ∈[L] and all the input data i ∈[n], we get that"
EXP,0.9959514170040485,"∥hi,ℓ∥2 ∈[1 −ϵ, 1 + ϵ]"
EXP,0.9969635627530364,with probability at least
EXP,0.9979757085020243,"1 −O(nL) exp(−Ω(ϵ2Pbm/L2)),"
EXP,0.9989878542510121,which completes the proof of the lemma.
