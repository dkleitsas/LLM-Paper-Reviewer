Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.001076426264800861,"We provide theoretical understandings of the implicit bias imposed by adversarial
training for homogeneous deep neural networks without any explicit regularization.
In particular, for deep linear networks adversarially trained by gradient descent on
a linearly separable dataset, we prove that the direction of the product of weight
matrices converges to the direction of the max-margin solution of the original
dataset. Furthermore, we generalize this result to the case of adversarial training
for non-linear homogeneous deep neural networks without the linear separability
of the dataset. We show that, when the neural network is adversarially trained
with ℓ2 or ℓ∞FGSM, FGM and PGD perturbations, the direction of the limit
point of normalized parameters of the network along the trajectory of the gradient
ﬂow converges to a KKT point of a constrained optimization problem that aims to
maximize the margin for adversarial examples. Our results theoretically justify the
longstanding conjecture that adversarial training modiﬁes the decision boundary
by utilizing adversarial examples to improve robustness, and potentially provides
insights for designing new robust training strategies."
INTRODUCTION,0.002152852529601722,"1
INTRODUCTION"
INTRODUCTION,0.0032292787944025836,"Deep neural networks (DNNs) have achieved great success in many ﬁelds such as computer vision
(Krizhevsky et al., 2012; He et al., 2015) and natural language processing (Collobert & Weston, 2008)
and other applications. These breakthroughs also lead to the importance of research on the robustness
and security issues of DNNs, among which those about adversarial examples are especially prevalent.
Adversarial examples are obtained by adding craftily designed imperceptible perturbations to the
original examples, which can sharply change the predictions of the DNNs with high conﬁdence
(Szegedy et al., 2014; Nguyen et al., 2015). Such vulnerability of DNNs renders security concerns
about deploying them in security-critical systems including vision for autonomous cars and face
recognition. It is therefore crucial to develop defense mechanisms against these adversarial examples
for deep learning."
INTRODUCTION,0.004305705059203444,"To this end, many defense strategies have been proposed to make DNNs resistant to adversarial
examples, such as adding a randomization layer before the input to the classiﬁer (Xie et al., 2018),
input transformations (Guo et al., 2018), adversarial training (Madry et al., 2018), etc. However,
Athalye et al. (2018) pointed out that most of the defense techniques are ineffective and give a false
sense of security due to obfuscated gradients except for adversarial training—a method which has
not been comprehensively attacked yet. Given a C-class training dataset {(xi, yi)}n
i=1 with natural
example xi ∈Rd and corresponding label yi ∈{1, ..., C}, adversarial training is formulated as
solving the following minimax optimization problem"
INTRODUCTION,0.005382131324004306,"min
W
˜L(W) = min
W
1
n n
X"
INTRODUCTION,0.006458557588805167,"i=1
max
δi∈B(0,ϵ) ℓ(xi + δi(W), yi; W) ,
(1)"
INTRODUCTION,0.007534983853606028,"where f is the DNN function, ℓis the loss function and δi(W) is the adversarial perturbation
generated by some adversary A(xi, yi; W) typically depending on model parameters W, within
the set B(0, ϵ) = {δ : ∥δ∥p ≤ϵ} around the natural example xi. Commonly, adversarial training
conducts a two-player game at each iteration: the inner maximization is to attack the model and"
INTRODUCTION,0.008611410118406888,Published as a conference paper at ICLR 2022
INTRODUCTION,0.00968783638320775,"ﬁnd the corresponding perturbation of the original example that maximizes the classiﬁcation loss ℓ;
the outer minimization, on the other hand, is to update the model parameters W with the gradient
descent method such that the loss ℓis minimized on adversarial examples generated by the inner
maximization (see Algorithm 1 for details)."
INTRODUCTION,0.010764262648008612,"To have a better theoretical understanding of the fact that adversarial training empirically improves
the robustness of DNNs against adversarial examples, Zhang et al. (2019) decomposed the prediction
error for adversarial examples and identiﬁed a trade-off between robustness and accuracy while Li
et al. (2020) studied the inductive bias of gradient descent based adversarial training for logistic
regression on linearly separable data. Faghri et al. (2021) studied adversarial robustness of linear
neural networks by exploring the optimization bias of different methods. Yu et al. (2021) studied
adversarial training through the bias-variance decomposition and showed that its generalization error
on clean examples mainly comes from the bias. However, even for the simplest DNN—the deep linear
network, we notice that there exists no work to theoretically understand such robustness achieved by
adversarial training through exploring its implicit bias."
INTRODUCTION,0.011840688912809472,"On the other hand, for the standard training, recent works extensively explored the implicit bias
imposed by gradient descent or its variants for DNNs in different settings. For a simple setting of
linear logistic regression on linearly separable data, Soudry et al. (2018); Ji & Telgarsky (2018);
Nacson et al. (2019b) derived that the direction of the model parameter converges to that of the
max-ℓ2-margin with divergent norm. Shamir (2021) concluded that gradient methods never overﬁt
when training linear predictors over separable dataset. Viewing the above model as a single-layer
network, a natural but more complicated extension is that for the deep linear network on linearly
separable data: Ji & Telgarsky (2019) proved that the gradient descent aligns the weight matrices
across layers and the product of weight matrices also converges to the direction of the max-ℓ2-margin
solution; Gunasekar et al. (2018) showed the implicit bias of gradient descent on linear convolution
networks. Nacson et al. (2019a); Lyu & Li (2020) further promoted the study of implicit bias of
gradient descent for standard training to the case of more general homogeneous non-linear neural
networks and proved that the limit point of the optimization path is along the direction of a KKT
point of the max-margin problem. Wei et al. (2019) studied the regularization path for homogeneous
DNNs and also proved the convergence to the max-margin direction in this setting. Banburski et al.
(2019) showed that gradient descent induces a dynamics of normalized weights converging to an
equilibrium which corresponds to a minimal norm solution."
INTRODUCTION,0.012917115177610334,"It is therefore our goal in this paper to theoretically understand the resistance to adversarial examples
for adversarially trained DNNs, linear and non-linear ones, through the lens of implicit bias imposed
by adversarial training. Due to the inner maximization, adversarial training differs a lot from standard
training and one should be careful to analyze the perturbed training dynamics. For the adversarial
training objective Eq. (1), various approaches have been proposed to solve the inner maximization,
such as fast gradient sign method (FGSM (Goodfellow et al., 2015)) and its stronger version projected
gradient descent (PGD (Madry et al., 2018)). The widely used adversarial training adopts PGD to
attack the model, while recent work (Wong et al., 2020) also suggested that a weaker adversary
can also surprisingly yield a model with satisfying robustness. Thus to conduct a comprehensive
study about the implicit bias of adversarial training for DNNs, we will use ℓ2-fast gradient method
(FGM (Miyato et al., 2016)), ℓ∞FGSM, ℓ2-PGD and ℓ∞-PGD to solve the inner maximization of
the adversarial training objective."
OUR CONTRIBUTION,0.013993541442411194,"1.1
OUR CONTRIBUTION"
OUR CONTRIBUTION,0.015069967707212056,"In this paper, we devote to answering two questions. First, is there any implicit bias imposed by
adversarial training for DNNs without explicit regularization? Second, if there exists such an implicit
bias, what are the convergence properties of the model parameters along the adversarial training
trajectory?"
OUR CONTRIBUTION,0.016146393972012917,"To this end, we ﬁrst investigate the adversarial training with ℓ2 adversarial perturbations for deep
linear networks on linear separable data where the allowed Euclidean distances from the adversarial
examples to their corresponding original examples ∥x′
i −xi∥2 are less than the max-ℓ2-margin of
the original dataset. Despite the simplicity of this setting, this problem is meaningful due to its
non-convexity and the introduction of adversarial examples, which heavily depend on the model
parameters, during the training process. We prove that gradient descent for adversarial training"
OUR CONTRIBUTION,0.017222820236813777,Published as a conference paper at ICLR 2022
OUR CONTRIBUTION,0.01829924650161464,"implicitly aligns weight matrices across layers and the direction of the product of weight matrices
also surprisingly converges to that of the max-ℓ2-margin solution of the original dataset—similar to
that of standard training for deep linear network (Ji & Telgarsky, 2019). Our results signiﬁcantly
generalize those in Li et al. (2020) for adversarial training of logistic regression. This simple yet
insightful case positively answers our ﬁrst question but partially answers the second one because it
still remains unclear why such convergence property can improve robustness considering its similarity
to that for the standard training. In fact, adversarial training differs from standard training in the
way how they impose such convergence property for parameters: the ﬁrst one is to maximize the
margin of adversarial examples while the latter is maximizing that of the original dataset, and these
two optimization problems happen to possess solutions along the same direction."
OUR CONTRIBUTION,0.0193756727664155,"We then move forward to explore a more general situation, adversarial training for homogeneous
non-linear DNNs without the linear separability of the dataset. We study the limit point of the
normalized model parameters along the adversarial training trajectory and show that"
OUR CONTRIBUTION,0.02045209903121636,"Theorem 1 (Informal). When the deep neural network is adversarially trained with one of the
ℓ2-FGM, FGSM, ℓ2-PGD and ℓ∞-PGD perturbations, the limit point of the normalized model
parameters is along the direction of a KKT point of a constrained optimization problem which aims
to maximize the margin of adversarial examples."
OUR CONTRIBUTION,0.021528525296017224,"This indicates that adversarial training is implicitly maximizing the margin of adversarial examples
rather than that of original dataset. Thus Theorem 1 provides another view for the high bias error
on clean examples of adversarial training in Yu et al. (2021) since distributions of adversarial and
clean examples are different. To the best of knowledge, these results are the ﬁrst attempt to analyze
the implicit bias of adversarial training for DNNs. We believe our results provide a theoretical
understanding on the effectiveness of adversarial training for improving robustness against adversarial
examples. It could potentially shed light on how to enhance the robustness of adversarially trained
models or even further inspire more effective defense mechanisms."
OUR CONTRIBUTION,0.022604951560818085,"Organization.
This paper is organized as follows. Section 2 is about notations and settings. Section
3 presents our main results on the implicit bias of adversarial training for DNNs. Section 4 provides
numerical experiments to support our claims. We conclude this work in Section 5 and discuss future
directions. Some technical proofs are deferred to supplementary materials."
OUR CONTRIBUTION,0.023681377825618945,Algorithm 1 Adversarial Training
OUR CONTRIBUTION,0.024757804090419805,"Input: Training set S = {(xi, yi)}n
i=1, Adversary A to solve the inner maximization, learning
rate η, initialization Wk for k ∈{1, . . . , L}
for t = 0 to T −1 do"
OUR CONTRIBUTION,0.02583423035522067,"S′(t) = ∅
for i = 1 to n do"
OUR CONTRIBUTION,0.02691065662002153,"x′
i(t) = A(xi, yi, W(t))
S′(t) = S′(t) S(x′
i(t), yi)
end for
for k = 1 to L do"
OUR CONTRIBUTION,0.02798708288482239,"Wk(t + 1) = Wk(t) −η(t) ∂˜
L(S′(t);W )"
OUR CONTRIBUTION,0.02906350914962325,"∂Wk
end for
end for"
PRELIMINARIES,0.030139935414424113,"2
PRELIMINARIES"
PRELIMINARIES,0.031216361679224973,"Notations.
For any matrix A ∈Rm×n, we denote its i-th row j-th column entry by Aij. Let AT
denote the transpose of A. ∥·∥F represents the Frobenius norm and ∥·∥p is the ℓp norm. The training
set is {(xi, yi)}n
i=1 where xi ∈Rd, ∥xi∥2 ≤1 and yi ∈{1, −1}. For a scalar function f : Rd 7→R,
we denote its gradient by ∇f. Furthermore, tr (A) = P"
PRELIMINARIES,0.03229278794402583,i Aii denotes the trace for the matrix A.
PRELIMINARIES,0.03336921420882669,We study the adversarial training for the L-layer positively homogeneous deep neural network
PRELIMINARIES,0.03444564047362755,"f(x; W) = WLϕL (WL−1 · · · ϕ3(W2ϕ2(W1x)) · · · )
(2)"
PRELIMINARIES,0.03552206673842842,Published as a conference paper at ICLR 2022
PRELIMINARIES,0.03659849300322928,"where Wk is the k-th layer weight matrix and ϕk is the activation function of the k-th layer 1. The
multi-c-homogeneity of the network is deﬁned by"
PRELIMINARIES,0.03767491926803014,"f(x; a1W1, · · · , aLWL) = L
Y"
PRELIMINARIES,0.038751345532831,"k=1
ac
kf(x; W)
(3)"
PRELIMINARIES,0.03982777179763186,"for any positive constants ak’s and c ≥1, where W = (W1, · · · , WL) is the collection of the
parameters of the network. For example, deep ReLU networks are multi-1-homogeneous. For
convenience, we also adopt the following notations for a multi-c-homogeneous DNN:"
PRELIMINARIES,0.04090419806243272,"ρk = ∥Wk∥F , ρ = ρc
1 · · · ρc
L
and
f(x; W) = ρf(x; c
W),
(4)"
PRELIMINARIES,0.04198062432723358,"where c
Wk = Wk/∥Wk∥F with ∥c
Wk∥F = 1 for k ∈{1, . . . , L}."
PRELIMINARIES,0.04305705059203445,"We use δi(W) to represent the adversarial perturbation of the original example xi within the perturba-
tion set B(0, ϵ) : {δ : ∥δ∥p ≤ϵ} around the original example xi for f(x; W). Furthermore, we use
the scale invariant adversarial perturbations deﬁned as follows for adversarial training in this paper.
Deﬁnition 1 (Scale invariant adversarial perturbation). An adversarial perturbation is said to be a
scale invariant adversarial perturbation for f(xi; W1, . . . , WL) and loss function ℓif it satisﬁes"
PRELIMINARIES,0.04413347685683531,"δi(a1W1, . . . , aLWL) = δi(W1, . . . , WL)
(5)"
PRELIMINARIES,0.04520990312163617,for any positive constants ak’s.
PRELIMINARIES,0.04628632938643703,"We will show in Section 3.2 that FGSM, ℓ2-FGM, ℓ2-PGD and ℓ∞-PGD perturbations for homoge-
neous DNNs are all scale invariant perturbations, which are important for analyzing different types of
perturbation in a uniﬁed manner. The empirical adversarial training loss with the perturbation δi(W)
is given by"
PRELIMINARIES,0.04736275565123789,"˜L(W) = 1 n n
X"
PRELIMINARIES,0.04843918191603875,"i=1
˜ℓ(yi, xi; W) = 1 n n
X"
PRELIMINARIES,0.04951560818083961,"i=1
ℓ(yif(xi + δi(W); W)) .
(6)"
PRELIMINARIES,0.05059203444564048,"For ease of notation, we denote ˜ℓi(W) = ˜ℓ(yi, xi; W). The loss function ℓis continuously differen-
tiable and satisﬁes
Assumption 1 (Loss function). ℓ> 0, ℓ′ < 0, limx→∞ℓ(x) = 0 and ℓx→−∞(x) = ∞."
PRELIMINARIES,0.05166846071044134,"Many widely used loss functions satisfy the above assumption such as ℓ(x) = e−x and the logistic
loss ℓ(x) = ln(1 + e−x). Furthermore, we make the following common assumptions about the
smoothness of f(x; W) and the adversarial perturbation δ(W):
Assumption 2 (Smoothness). With respect to W, yf(x; W) is locally Lipschitz for any ﬁxed x;
yif(xi; W) further have locally Lipschitz gradients and δi(W) are locally Lipschitz for all training
examples xi."
PRELIMINARIES,0.0527448869752422,"Remark.
Our results can also be generalized to non-smooth homogeneous neural networks straight-
forwardly (Appendix B.4). Assuming Lipschitzness about perturbations is because we focus on
popular perturbations such as ℓ2-FGM and PGD perturbations which have explicit forms and depend
on gradients of the network, whose Lipschitzness assumptions are quite common."
MAIN RESULTS,0.05382131324004306,"3
MAIN RESULTS"
MAIN RESULTS,0.05489773950484392,"We present our main theoretical results on the implicit bias of the gradient ﬂow/gradient descent for
adversarial training in this section. For the deep linear neural network, a special kind of homogeneous
models, in Section 3.1 we further restrict the dataset to be linearly separable and focus on ℓ2 adversarial
perturbations. We prove that the singular vectors corresponding to the largest singular values of
weight matrices get aligned across layers with the progress of adversarial training2. Based on this key
result, the product of weight matrices converges to the direction of the maximum margin solution"
MAIN RESULTS,0.05597416576964478,"1ϕk is a vector in our notations.
2This phenomenon for standard training has been studied by Ji & Telgarsky (2019) ﬁrst."
MAIN RESULTS,0.05705059203444564,Published as a conference paper at ICLR 2022
MAIN RESULTS,0.0581270182992465,"under the original data. Furthermore, we study a much more general scenario in Section 3.2: without
the assumption of linearly separability of data and norm constraints on adversarial perturbations,
we show that the gradient ﬂow of adversarial training with scale invariant adversarial perturbations
for the homogeneous nonlinear neural networks implicitly performs margin maximization under
the adversarial data. Due to the space limit, some technical proofs of Section 3.1 are deferred to
Appendix A, and we present proofs of Section 3.2 in Appendix B."
ADVERSARIAL TRAINING FOR DEEP LINEAR NETWORK,0.059203444564047365,"3.1
ADVERSARIAL TRAINING FOR DEEP LINEAR NETWORK"
ADVERSARIAL TRAINING FOR DEEP LINEAR NETWORK,0.060279870828848225,"In this section we restrict the dataset {(xi, yi)}n
i=1 to be linearly separable in the sense that there
exists a unit vector u such that ∀i ∈{1, . . . , n}, yi ⟨u, xi⟩> 0 and we explore the adversarial
training dynamics of gradient descent for deep linear networks, i.e., with identity activation function
ϕk(x) = x for all layers,
f(x, W) = WL · · · W1x.
(7)
Now let
¯u = arg max
∥u∥=1
min
i∈{1,...,n} yi ⟨xi, u⟩"
ADVERSARIAL TRAINING FOR DEEP LINEAR NETWORK,0.061356297093649086,"be the max-margin solution given by the hard-margin SVM and γm = max∥u∥=1 ym ⟨xm, u⟩be
the max-margin, where m = arg mini∈{1,...,n} yi ⟨xi, u⟩. We only consider the ℓ2-norm adversarial
perturbations in this section, i.e., B(0, ϵ) = {δ : ∥δ∥2 ≤ϵ}. The allowed Euclidean distance from an
adversarial example x′ to its original example x is assumed to satisfy"
ADVERSARIAL TRAINING FOR DEEP LINEAR NETWORK,0.062432723358449946,"∥x′ −x∥2 ≤ϵ ≤γm.
(8)"
ADVERSARIAL TRAINING FOR DEEP LINEAR NETWORK,0.06350914962325081,"We denote the product of all weight matrices by WΠ = WL · · · W1, which is simply a vector. Our
model is adversarially trained by gradient descent"
ADVERSARIAL TRAINING FOR DEEP LINEAR NETWORK,0.06458557588805167,Wk(t + 1) = Wk(t) −η(t) ∂˜L(t)
ADVERSARIAL TRAINING FOR DEEP LINEAR NETWORK,0.06566200215285253,"∂Wk(t).
(9)"
ADVERSARIAL TRAINING FOR DEEP LINEAR NETWORK,0.06673842841765339,"The inner maximization of the adversarial training objective Eq. (1) can be solved exactly in this case,
where the adversarial perturbations are taken as"
ADVERSARIAL TRAINING FOR DEEP LINEAR NETWORK,0.06781485468245425,"δi(W(t)) = −yiϵ
WΠ(t)
∥WΠ(t)∥2
(10)"
ADVERSARIAL TRAINING FOR DEEP LINEAR NETWORK,0.0688912809472551,"for the t-th iteration of adversarial training. For any layer k, the weight matrix can be decomposed as"
ADVERSARIAL TRAINING FOR DEEP LINEAR NETWORK,0.06996770721205597,"Wk = UkΣkV T
k
where Uk and Vk are the left and right singular matrices, respectively. Furthermore, we use uk and vk
to represent the left and right singular vectors corresponding to the largest singular value, respectively.
We emphasize that the training dynamics abruptly changes due to this perturbation and one should
be careful to analyze its effects. It can be easily seen that Eq. (10) is a scale invariant adversarial
perturbations by our deﬁnition. Note that with this kind of perturbation, the adversarial data during
the training are still linearly separable, since we have required that the perturbation is smaller than
the max-margin of the original dataset described in Eq. (8). We further assume that ℓsatisﬁes
Assumption 3. The loss function ℓis β-smooth and |ℓ′| ≤α."
ADVERSARIAL TRAINING FOR DEEP LINEAR NETWORK,0.07104413347685684,"such as logistic loss. We are now ready to explore the implicit bias of the gradient descent for
adversarial training. Inspired by the idea of studying the smoothness of loss function from Ji &
Telgarsky (2019) for standard training, here we ﬁrst examine whether our adversarial training loss
Eq. (1) possesses smoothness. Let the set S(r) denote the set of weights with bounded norm"
ADVERSARIAL TRAINING FOR DEEP LINEAR NETWORK,0.0721205597416577,"S(r) =

W
∥Wk∥F ≤r, ∥WΠ∥2 ≥¯r, k = 1, . . . , L
	
,
(11)"
ADVERSARIAL TRAINING FOR DEEP LINEAR NETWORK,0.07319698600645856,"where r ≥1 and ¯r is a small constant to avoid trivial solution. To simplify the notation, we let"
ADVERSARIAL TRAINING FOR DEEP LINEAR NETWORK,0.07427341227125941,"β(r, ¯r, ϵ) = r3LL2

α + β + ϵ ¯rL"
ADVERSARIAL TRAINING FOR DEEP LINEAR NETWORK,0.07534983853606028,"
2α + β + αβ ¯rL"
ADVERSARIAL TRAINING FOR DEEP LINEAR NETWORK,0.07642626480086114,"
.
(12)"
ADVERSARIAL TRAINING FOR DEEP LINEAR NETWORK,0.077502691065662,"The following lemma provides us a ﬁrst view of the overall trends of gradient descent for adversarial
training in deep linear network on linear separable dataset."
ADVERSARIAL TRAINING FOR DEEP LINEAR NETWORK,0.07857911733046287,Published as a conference paper at ICLR 2022
ADVERSARIAL TRAINING FOR DEEP LINEAR NETWORK,0.07965554359526372,"Lemma 1 (Overall trends of adversarial training for deep linear network). With adversarial pertur-
bation Eq. (10), under Assumption 1, Assumption 3 and the requirement Eq. (8), for the deep linear
network adversarially trained by gradient descent on linear separable data, the adversarial training
objective (1) is β(r, ¯r, ϵ)-smooth within the given set S(r) for a constant r. However, with constant
step size for gradient descent, the weight matrices will eventually leave the set S(r) if r remains
unchanged during the training"
ADVERSARIAL TRAINING FOR DEEP LINEAR NETWORK,0.08073196986006459,"max
k∈{1,...,L} ∥Wk(t1)∥F > r
for some t1 > 0.
(13)"
ADVERSARIAL TRAINING FOR DEEP LINEAR NETWORK,0.08180839612486544,"The ﬁrst part of this lemma implies the smoothness of the adversarial training loss Eq. (1) while also
leading the weight matrices to leave the set S(r). In fact, this is saying that maxk∈{1,...,L} ∥Wk∥F is
divergent along the adversarial training trajectory since maxk∈{1,...,L} ∥Wk∥F will be larger than r
for any given r. We can keep W(t + 1) inside S(r(t)) at every step t by delicately adjusting the step
size of gradient descent and treating r as a function of the step t."
ADVERSARIAL TRAINING FOR DEEP LINEAR NETWORK,0.08288482238966631,"Lemma 2 (Smoothness of the adversarial training loss). If the learning rate is taken as η(t) =
min{1, 1/β(r, ¯r, ϵ)}, then the adversarial training loss ˜L(W) is β(r, ¯r, ϵ)-smooth where r(t + 1) is
taken as"
ADVERSARIAL TRAINING FOR DEEP LINEAR NETWORK,0.08396124865446716,"r(t + 1) =
r(t)
if W(t + 1) ∈S(r(t) −µ(t)),
r(t) + µ(t)
otherwise"
ADVERSARIAL TRAINING FOR DEEP LINEAR NETWORK,0.08503767491926803,"during the training, where µ = rL−1(1 + ϵ)α/β(r, ¯r, ϵ)."
ADVERSARIAL TRAINING FOR DEEP LINEAR NETWORK,0.0861141011840689,"We show the divergence of maxk∈{1,...,L} ∥Wk∥F along the adversarial training trajectory in Lemma
1, and now we can further derive that the Frobenius norms of weight matrices for all layers are
divergent with the smoothness of the adversarial training loss in hand, considering that the differences
between any two layers during the adversarial training are always bounded (Lemma 6). Due to this
divergence of weight norms, only the direction of the product of weight matrices is important for the
model to make predictions."
ADVERSARIAL TRAINING FOR DEEP LINEAR NETWORK,0.08719052744886975,"Now we present our main theorem to provide insights on the theoretical understanding of the implicit
bias of gradient descent for adversarial training of deep linear network."
ADVERSARIAL TRAINING FOR DEEP LINEAR NETWORK,0.08826695371367062,"Theorem 2 (Convergence to the direction of the max-margin solution). With adversarial perturbation
Eq. (10), under Assumption 1, Assumption 3 and requirement (8), for gradient descent of the
adversarial training objective Eq. (6) with logistic loss, the singular vectors of the largest singular
values of adjacent layers get aligned if the learning rate is taken as that in Lemma 2:"
ADVERSARIAL TRAINING FOR DEEP LINEAR NETWORK,0.08934337997847147,"∀k ∈{1, . . . , L} :
|⟨uk, vk+1⟩| →1
(14)"
ADVERSARIAL TRAINING FOR DEEP LINEAR NETWORK,0.09041980624327234,"as t →∞. As a result, the direction of the product of weight matrices converges to that of the
max-margin solution
WΠ
∥WΠ∥2
→¯u.
(15)"
ADVERSARIAL TRAINING FOR DEEP LINEAR NETWORK,0.09149623250807319,"Remark.
The alignment phenomenon for standard training has been showed in the previous work
Ji & Telgarsky (2019), here we further extend this result to the case of adversarial training, where the
training objective is different from that of the standard training due to the introduction of adversarial
perturbation dependent on the parameters of the network. Our work is also a signiﬁcant generalization
of Li et al. (2020) which studied the convergence to max-margin solution of adversarial training for
logistic regression, a single-layer linear network."
ADVERSARIAL TRAINING FOR DEEP LINEAR NETWORK,0.09257265877287406,"Furthermore, although the direction of the solution Eq. (15) given by adversarial training does not
differ from that of standard training, i.e., the direction of ¯u, these two solutions are not same: the
ﬁrst is maximizing the margin of adversarial examples while the latter is maximizing that of original
dataset—which happens to be the same one in this setting. The similarity of these two solutions
comes from the fact that the adversarial data are also linearly separable under the requirement Eq. (8),
whose max-margin solution has the same direction as that of ¯u. The above reasoning will be more
clear in the next section where we show the solution of adversarial training as a max-margin solution
of adversarial examples more explicitly."
ADVERSARIAL TRAINING FOR DEEP LINEAR NETWORK,0.09364908503767493,Published as a conference paper at ICLR 2022
ADVERSARIAL TRAINING FOR HOMOGENEOUS DEEP NEURAL NETWORK,0.09472551130247578,"3.2
ADVERSARIAL TRAINING FOR HOMOGENEOUS DEEP NEURAL NETWORK"
ADVERSARIAL TRAINING FOR HOMOGENEOUS DEEP NEURAL NETWORK,0.09580193756727665,"Additional deﬁnitions
For simplicity, we will use exponential loss ℓ(x) = e−x in this section.
For an original example xi, the margin for its adversarial example xi + δi(W) is deﬁned as ˜γi =
yif(xi + δi(W); W) while for the whole dataset {(xi, yi)}n
i=1, the margin for their corresponding
adversarial examples is denoted by ˜γm where m = arg minm∈{1,...,n} yif(xi + δi(W); W). We
introduce the normalized margin as ˆγi = yif(xi + δi(c
W); c
W) to explore the convergence properties,
where c
Wk = Wk/ ∥Wk∥F such that ∥c
Wk∥F = 1 for any layer k . Furthermore, the margin for the
original example xi is deﬁned to be γi = yif(xi; W) ≥˜γi. To relieve the headaches of picking
suitable learning rates, we consider gradient ﬂow, gradient descent with inﬁnitesimal step, of the
training loss Eq. (6), which leads to dWk"
ADVERSARIAL TRAINING FOR HOMOGENEOUS DEEP NEURAL NETWORK,0.0968783638320775,"dt
= −"
ADVERSARIAL TRAINING FOR HOMOGENEOUS DEEP NEURAL NETWORK,0.09795479009687837,"∂˜L
∂Wk !T (16)"
ADVERSARIAL TRAINING FOR HOMOGENEOUS DEEP NEURAL NETWORK,0.09903121636167922,"for k ∈{1, . . . , L}. Note that the introduction of adversarial perturbations (relying on the network
parameters) abruptly perturbs the training dynamics. In this section, to conduct a comprehensive study
of adversarial training, we do not restrict the perturbation used for solving the inner maximization of
the adversarial training objective in advance while only require it to be a scale invariant adversarial
perturbation. By noting the following lemma, we can easily generalize our results to the commonly
used adversarial attacks:
Lemma 3. Under Assumption 1, ℓ2-FGM, FGSM, ℓ2-PGD and ℓ∞-PGD perturbations for homoge-
neous DNNs are all scale invariant adversarial perturbations."
ADVERSARIAL TRAINING FOR HOMOGENEOUS DEEP NEURAL NETWORK,0.10010764262648009,"The RHS of Eq. (16) is different from that of standard training due to the inner maximization. Our
key observation is the following property of adversarial training which can drastically simplify our
analysis despite of the complexity brought by the adversarial examples.
Lemma 4. For the exponential loss, along the gradient ﬂow trajectory of adversarial training with
scale invariant adversarial perturbations for homogeneous DNN f(x; W), we have tr"
ADVERSARIAL TRAINING FOR HOMOGENEOUS DEEP NEURAL NETWORK,0.10118406889128095,"∂˜L
∂Wk
Wk ! = −c n n
X"
ADVERSARIAL TRAINING FOR HOMOGENEOUS DEEP NEURAL NETWORK,0.10226049515608181,"i=1
e−˜γi˜γi.
(17)"
ADVERSARIAL TRAINING FOR HOMOGENEOUS DEEP NEURAL NETWORK,0.10333692142088267,"In the case of deep linear network, we adopt linearly separable dataset with the requirement Eq. (8),
yielding the fact that the adversarial data are also linearly separable. In this section we do not impose
such conditions but only apply a weaker assumption in a more general scenario.
Assumption 4. There exists a separability of adversarial examples of the dataset3: there exists a
time t0 such that yif(xi + δi(W(t0)); W(t0)) > 0 for all i ∈{1, . . . , n}."
ADVERSARIAL TRAINING FOR HOMOGENEOUS DEEP NEURAL NETWORK,0.10441334768568353,"Theoretically, Zhang et al. (2020); Gao et al. (2019) proved the convergence to low robust training
loss for heavily overparametrized nets. Moreover, adversarial training can typically achieve this
separability in practice, i.e., the model can ﬁt adversarial examples of the training dataset, which
makes the above assumption a reasonable one. In Section 3.1, we obtain divergent weight norms for
adversarial training. Thanks to Lemma 4, we can further generalize it to the case of homogeneous
network without linearly separability of dataset and the requirement Eq. (8) for adversarial examples.
This divergence of model parameters is necessary for the convergence of the adversarial training loss.
To begin with, recalling our deﬁnition of ρ = QL
k=1 ρc
k = QL
k=1 ∥Wk∥c
F , we state our ﬁrst main
theorem regarding the overall trend for gradient ﬂow of adversarial training as follows.
Theorem 3 (Divergence of weight norms). For the adversarial training objective (6) of the binary
classiﬁcation task with exponential loss and scale invariant adversarial perturbations, under Assump-
tion 2 and Assumption 4, the product of the Frobenius norms of weight matrices diverges to inﬁnity
along the trajectory of the gradient ﬂow"
ADVERSARIAL TRAINING FOR HOMOGENEOUS DEEP NEURAL NETWORK,0.1054897739504844,"ρ = Ω(ln t) →∞,"
ADVERSARIAL TRAINING FOR HOMOGENEOUS DEEP NEURAL NETWORK,0.10656620021528525,"and, as a result, the training loss ˜L converges to zero as t →∞."
ADVERSARIAL TRAINING FOR HOMOGENEOUS DEEP NEURAL NETWORK,0.10764262648008611,3Similar assumptions are previously used in Lyu & Li (2020); Ji & Telgarsky (2020)
ADVERSARIAL TRAINING FOR HOMOGENEOUS DEEP NEURAL NETWORK,0.10871905274488698,Published as a conference paper at ICLR 2022
ADVERSARIAL TRAINING FOR HOMOGENEOUS DEEP NEURAL NETWORK,0.10979547900968784,"Remark.
The divergence of weight norms for standard training of homogeneous DNNs has been
studied by, for example, Lyu & Li (2020); Wei et al. (2019). The above theorem considers adversarial
training, which implies that the margin for an adversarial example, ˜γi for any i ∈{1, . . . , n}, also
goes to inﬁnity along the adversarial training trajectory. This makes the normalized margin ˆγi
necessary to understand the convergence properties of the adversarial training solution as t →∞.
Built upon this result, we ﬁrst shed some lights on the implicit bias of the gradient ﬂow for the
adversarial training through the following theorem, which discusses the training dynamics of the
normalized margins for the adversarial examples.
Theorem 4 (Non-decreasing adversarial margin). For adversarial training with scale invariant
adversarial perturbations, when the separability of adversarial examples is achieved at t0, the
weighted sum of the changing rates of normalized margins for the adversarial examples are non-
negative for all t ≥t0:"
ADVERSARIAL TRAINING FOR HOMOGENEOUS DEEP NEURAL NETWORK,0.1108719052744887,"∀t ≥t0 : n
X"
ADVERSARIAL TRAINING FOR HOMOGENEOUS DEEP NEURAL NETWORK,0.11194833153928956,"i=1
e−ρˆγi dˆγi"
ADVERSARIAL TRAINING FOR HOMOGENEOUS DEEP NEURAL NETWORK,0.11302475780409042,"dt ≥0;
(18)"
ADVERSARIAL TRAINING FOR HOMOGENEOUS DEEP NEURAL NETWORK,0.11410118406889128,there exists a sufﬁciently large time t1 > t0 such that dˆγm
ADVERSARIAL TRAINING FOR HOMOGENEOUS DEEP NEURAL NETWORK,0.11517761033369214,dt ≥0 if ∀i : ˆγi −ˆγm = ω( 1
ADVERSARIAL TRAINING FOR HOMOGENEOUS DEEP NEURAL NETWORK,0.116254036598493,ρ) for all t ≥t1.
ADVERSARIAL TRAINING FOR HOMOGENEOUS DEEP NEURAL NETWORK,0.11733046286329386,"The above theorem reveals a relation for the normalized margins for adversarial examples. According
to the above lemma, since ρ →∞as t →∞, the term with weight e−ρˆγm, roughly speaking,
dominates the LHS of (18) after some time t1 ≥t0. Then dˆγm/dt4 will never be negative for ∀t ≥t1.
This directly implies that the gradient ﬂow is actually maximizing the margin for the adversarial
examples of the original dataset. Applying Lemma 4 and Theorem 3, in the following theorem, we
further strengthen the above observation for adversarial training for homogeneous DNNs.
Theorem 5 (Convergence to the direction of a KKT point). Under Assumption 2 and 4, for ex-
ponential loss and multi-c-homogeneous DNNs, the limit point of normalized weight parameters
{W/∥W∥2 : t ≥0} of the gradient ﬂow for the adversarial training objective Eq. (6) with scale
invariant adversarial perturbations is along the direction of a KKT point of the constrained norm-
minimization problem"
ADVERSARIAL TRAINING FOR HOMOGENEOUS DEEP NEURAL NETWORK,0.11840688912809473,"min
W1,...,WL
1
2 ∥W∥2
2
s.t. ˜γi ≥1
∀i ∈{1, . . . , n}.
(19)"
ADVERSARIAL TRAINING FOR HOMOGENEOUS DEEP NEURAL NETWORK,0.11948331539289558,"Remark 1.
According to Lemma 3, the above theorem can be directly applied to ℓ2-FGM, FGSM,
ℓ2 and ℓ∞-PGD perturbations. The requirement Eq. (8) in Section 3.1 is not required since the dataset
in this section is no longer linearly separable. It is well-known that the norm-minimization problem
is closely related to the margin-maximization problem in the sense that
Optimization problem(19) ⇐⇒
max
W1,...,WL ˜γm,
s.t. ∥W∥2 = 1."
ADVERSARIAL TRAINING FOR HOMOGENEOUS DEEP NEURAL NETWORK,0.12055974165769645,"Remark 2.
Theorem 5 is the key theorem of this section. It shows that, for adversarial training,
the gradient ﬂow is implicitly maximizing the margin of adversarial examples. Therefore, for
perturbations with norm constraints other than ℓ2-norm, (19) is no longer an ℓ2-norm optimization
problem but a mixed-norm one due to the perturbation δ(W) in ˜γm, which was ﬁrst observed by
Li et al. (2020) for adversarial training of one-layer neural network while our results are for more
general deep non-linear homogeneous neural networks. This is one of the major differences between
adversarial training and standard training—they are formulated from solving different optimization
problem. Theorem 5 provides theoretical understandings of the folklore that adversarial training can
modify the decision boundary against adversarial examples to improve robustness."
ADVERSARIAL TRAINING FOR HOMOGENEOUS DEEP NEURAL NETWORK,0.1216361679224973,"Remark 3.
In the context of standard training, Lyu & Li (2020) studied the properties of the nor-
malized margin through its approximation, the smooth-normalized margin. One can try to generalize
the analysis method in Lyu & Li (2020) to adversarial training with various kinds of adversarial
perturbations by further assuming the local smoothness regarding gradients when considering non-
smooth analysis. Generalizinig our results to non-smooth case can be found in Appendix B.4. To
easily adapt to adversarial training, our new strategy in this work is to directly analyze the normalized
margin itself for adversarial examples instead and utilizes the alignment phenomena observed by
Ji & Telgarsky (2020). Most importantly, what we seek to push forward in the current work are the
theoretical understandings of the adversarial training for DNNs through its implicit bias."
ADVERSARIAL TRAINING FOR HOMOGENEOUS DEEP NEURAL NETWORK,0.12271259418729817,"4Strictly speaking, this is not well-deﬁned since m may not be unique. However, we can use this deﬁnition
in an approximately correct way for t sufﬁciently large."
ADVERSARIAL TRAINING FOR HOMOGENEOUS DEEP NEURAL NETWORK,0.12378902045209902,Published as a conference paper at ICLR 2022
ADVERSARIAL TRAINING FOR HOMOGENEOUS DEEP NEURAL NETWORK,0.12486544671689989,(a) FGSM adversarial examples
ADVERSARIAL TRAINING FOR HOMOGENEOUS DEEP NEURAL NETWORK,0.12594187298170076,"0
50
100
150
200
250
epochs −0.05 −0.04 −0.03 −0.02 −0.01 0.00 0.01"
ADVERSARIAL TRAINING FOR HOMOGENEOUS DEEP NEURAL NETWORK,0.12701829924650163,adversarial normali ed margin
ADVERSARIAL TRAINING FOR HOMOGENEOUS DEEP NEURAL NETWORK,0.12809472551130247,"AT with PGD perturbation
standard training"
ADVERSARIAL TRAINING FOR HOMOGENEOUS DEEP NEURAL NETWORK,0.12917115177610333,(b) ℓ∞-PGD adversarial examples
ADVERSARIAL TRAINING FOR HOMOGENEOUS DEEP NEURAL NETWORK,0.1302475780409042,"10000
12500
15000
17500
20000
22500
25000
27500
30000
epochs −0.10 −0.08 −0.06 −0.04 −0.02 0.00 0.02 0.04"
ADVERSARIAL TRAINING FOR HOMOGENEOUS DEEP NEURAL NETWORK,0.13132400430570507,adversarial normalized margin
ADVERSARIAL TRAINING FOR HOMOGENEOUS DEEP NEURAL NETWORK,0.13240043057050593,AT wi h FGSM per urba iona
ADVERSARIAL TRAINING FOR HOMOGENEOUS DEEP NEURAL NETWORK,0.13347685683530677,"(c) FGSM adversarial examples
Figure 1: Adversarial training for the 3-layer neural network on MNIST. Adversarial normalized
margin for (a) FGSM adversarial examples; (b) ℓ∞-PGD adversarial examples; (c) FGSM adversarial
examples after 10000 epochs."
ADVERSARIAL TRAINING FOR HOMOGENEOUS DEEP NEURAL NETWORK,0.13455328310010764,"Trade-off between robustness and accuracy.
More signiﬁcantly, Theorem 5 also implies that
adversarial training leads to the trade-off between robustness and accuracy as in Zhang et al. (2019).
This can be seen as follows. Let f(·, W) denote the classiﬁer obtained from adversarial training.
Then for a clean training example (x, y) with y = 1 for simplicity, we have yf(x + δ(W); W) =
f(x + δ(W); W) > 0 since f(x; W) is a max-margin classiﬁer over adversarial data. This means
that, for any clean test example (x′, y′) around x in the sense that x′ = x + τ with ∥τ∥p ≤ϵ, we will
have
f(x′, W) = f(x + τ; W) ≥f(x + δ(W); W) > 0
(20)
due to the deﬁnition of δ(W) when y
= 1:
δ(W) = maxv∈B(x,ϵ) ℓ(yf(x + v; W)) =
minv∈B(x,ϵ) f(x + v; W) for ℓ′ < 0. Therefore, as a result of forcing our model to correctly
classify the adversarial example x + δ(W), it will classify any such x′ = x + τ as having labels 1
even there may be some of them having true labels -1, which leads to the drop of accuracy at the cost
of increasing robustness. Besides, since adversarial training maximizes the margin of the speciﬁc
adversarial perturbation used for training, the trained model performs worse on other perturbations."
NUMERICAL EXPERIMENTS,0.1356297093649085,"4
NUMERICAL EXPERIMENTS"
NUMERICAL EXPERIMENTS,0.13670613562970937,"In this section, we conduct numerical experiments on MNIST dataset to support our claims. We
adversarially trained a 3-layer neural network using SGD with constant learning rate and batch-size 80.
The model has the architecture of input layer-1024-ReLU-64-ReLU-output layer. We present results
for adversarial training with: (1) FGSM perturbations with ϵ = 16/255; (2) ℓ∞-PGD perturbations,
where the PGD is ran for 5 steps with step size 6/255 and ϵ = 16/255. As a comparison, we
also standardly trained a model with the same architecture to evaluate the normalized margin for
adversarial examples by attacking it with FGSM and PGD during its training process."
NUMERICAL EXPERIMENTS,0.1377825618945102,"Fig. 1(a) shows that the normalized margin for FGSM adversarial examples keeps increasing during
the process of adversarial training with FGSM perturbations. Meanwhile, the standardly trained
model has lower normalized margins for FGSM adversarial examples. Similar results exist for
ℓ∞-PGD adversarial examples as showed by Fig. 1(b). To see that adversarial normalized keeps
increasing more clearly, we also plot the adversarial training with FGSM perturbations after 10000
epochs in Fig. 1(c). More detailed experiments are in Appendix C. These empirical ﬁndings support
our claims that adversarial training implicitly maximizes the margin of adversarial examples."
CONCLUSION,0.13885898815931108,"5
CONCLUSION"
CONCLUSION,0.13993541442411195,"In this paper, we have studied the implicit bias of adversarial training for the deep linear networks and,
more generally, the homogeneous DNNs. We proved that adversarial training with scale invariant
adversarial perturbations implicitly performs margin-maximization for adversarial data during its
training process. Intuitively, such implicit bias strongly implies that adversarial training encourages
the neural networks to utilize adversarial examples more to improve its robustness. It will be an
interesting future direction to improve the effect of adversarial training by promoting it to enlarge
the margins of adversarial examples more effectively. It is also possible to design new defense
mechanisms by explicitly requiring the standard training to have the bias of maximizing the margin
of adversarial examples. We will leave these potential beneﬁts of our theoretical analysis as future
work."
CONCLUSION,0.14101184068891282,Published as a conference paper at ICLR 2022
CONCLUSION,0.14208826695371368,ACKNOWLEDGEMENT
CONCLUSION,0.14316469321851452,"This project is supported by Beijing Nova Program (No. 202072) from Beijing Municipal Science
Technology Commission."
REFERENCES,0.1442411194833154,REFERENCES
REFERENCES,0.14531754574811626,"Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. In arXiv:1802.06509, 2018."
REFERENCES,0.14639397201291712,"A. Athalye, N. Carlini, and D. Wagner. Obfuscated gradients give a false sense of security: Circum-
venting defenses to adversarial examples. In International Conference on Machine learning(ICML),
2018."
REFERENCES,0.147470398277718,"A. Banburski, Q. Liao, B. Miranda, T. Poggio, L. Rosasco, F. D. L. Torre, and J. Hidary. Theory iii:
Dynamics and generalization in deep networks. In arXiv:1903.04991, 2019."
REFERENCES,0.14854682454251883,Frank H Clarke. Generalized gradients and applications. 1975.
REFERENCES,0.1496232508073197,Frank H Clarke. Optimization and nonsmooth analysis. 1983.
REFERENCES,0.15069967707212056,"R. Collobert and J. Weston. A uniﬁed architecture for natural language processing: Deep neural
networks with multitask learning. In Proceedings of the 25th international conference on Machine
learning(ICML), 2008."
REFERENCES,0.15177610333692143,"Damek Davis, Dmitriy Drusvyatskiy, Sham Kakade, and Jason D. Lee. Stochastic subgradient
method converges on tame functions. In arXiv:1804.07795, 2018."
REFERENCES,0.15285252960172227,"S. S. Du, W. Hu, and J. D. Lee. Algorithmic regularization in learning deep homogeneous models:
Layers are automatically balanced. In arXiv: 1806.00900, 2018."
REFERENCES,0.15392895586652314,"J. Dutta, K. Deb, R. Tulshyan, and R. Arora. Approximate kkt points and a proximity measure for
termination. Journal of Global Optimization, pp. 1463–1499, 2013."
REFERENCES,0.155005382131324,"Fartash Faghri, Sven Gowal, Cristina Vasconcelos, David J. Fleet, Fabian Pedregosa, and Nico-
las Le Roux. Bridging the gap between adversarial robustness and optimization bias. In arXiv:
2102.08868, 2021."
REFERENCES,0.15608180839612487,"R. Gao, T. Cai, H. Li, L. Wang, C. Hsieh, and J. D. Lee. Convergence of adversarial training in
overparametrized neural networks. In arXiv:1906.07916, 2019."
REFERENCES,0.15715823466092574,"I. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. In
International Conference on Learning Representations (ICLR), 2015."
REFERENCES,0.15823466092572658,"Suriya Gunasekar, Jason D. Lee, Daniel Soudry, and Nathan Srebro. Implicit bias of gradient descent
on linear convolutional networks. In arXiv: 1806.00468, 2018."
REFERENCES,0.15931108719052745,"C. Guo, M. Rana, M. Cisse, and L. van der Maaten. Countering adversarial images using input
transformations. In International Conference on Learning Representations(ICLR), 2018."
REFERENCES,0.1603875134553283,"K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectiﬁers: Surpassing human-level per-
formance on imagenet classiﬁcation. In International conference on computer vision (ICCV),
2015."
REFERENCES,0.16146393972012918,"Z. Ji and M. Telgarsky. Risk and parameter convergence of logistic regression. In arXiv:1803.07300,
2018."
REFERENCES,0.16254036598493002,"Z. Ji and M. Telgarsky. Gradient descent aligns the layers of deep linear networks. In International
Conference on Learning Representations(ICLR), 2019."
REFERENCES,0.16361679224973089,"Z. Ji and M. Telgarsky. Directional convergence and alignment in deep learning. In arXiv:2006.06657,
2020."
REFERENCES,0.16469321851453175,Published as a conference paper at ICLR 2022
REFERENCES,0.16576964477933262,"A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenent classiﬁcation with deep convolutional neural
networks. In Advances in Neural Information Processing Systems(NeurIPS), 2012."
REFERENCES,0.1668460710441335,"Y. Li, E. X.Fang, H. Xu, and T. Zhao. Inductive bias of gradient descent based adversarial training on
separable data. In International Conference on Machine Learning(ICLR), 2020."
REFERENCES,0.16792249730893433,"K. Lyu and J. Li. Gradient descent maximizes the margin of homogeneous neural networks. In
International Conference on Learning Representations(ICLR), 2020."
REFERENCES,0.1689989235737352,"A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant
to adversarial attacks. In International Conference on Learning Representations(ICLR), 2018."
REFERENCES,0.17007534983853606,"T. Miyato, A. M. Dai, and I. Goodfellow. Adversarialtraining methods for semi-supervised text
classiﬁcation. In arXiv:1605.07725, 2016."
REFERENCES,0.17115177610333693,"M. S. Nacson, S. Gunasekar, J. Lee, N. Srebro, and D. Soudry. Lexicographic and depth-sensitive mar-
gins in homogeneous and non-homogeneous deep models. In Proceedings of the 36th International
Conference on Machine Learning, 2019a."
REFERENCES,0.1722282023681378,"M. S. Nacson, N. Srebro, and D. Soudry. Stochastic gradient descent on separable data: Exact
convergence with a ﬁxed learning rate. In Proceedings of Machine Learning Research, 2019b."
REFERENCES,0.17330462863293863,"A. Nguyen, J. Yosinski, and J. Clune. Deep neural networks are easily fooled: High conﬁdence
predictions for unrecognizable images. In Conference on computer vision and pattern recognition
(CVPR), 2015."
REFERENCES,0.1743810548977395,"Ohad Shamir. Gradient methods never overﬁt on separable data. In Journal of Machine Learning
Research, 2021."
REFERENCES,0.17545748116254037,"D. Soudry, E. Hoffer, M. S. Nacson, S. Gunasekar, and N. Srebro. The implicit bias of gradient
descent on separable data. Journal of Machine Learning Research, pp. 1–57, 2018."
REFERENCES,0.17653390742734124,"C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing
properties of neural networks. In International Conference on Learning Representations (ICLR),
2014."
REFERENCES,0.17761033369214208,"C. Wei, J. D Lee, Q. Liu, and T. Ma. Regularization matters: Generalization and optimization of neural
nets v.s. their induced kernel. In Advances in Neural Information Processing Systems(NeurIPS),
2019."
REFERENCES,0.17868675995694294,"E. Wong, L. Rice, and J. Z. Kolter. Fast is better than free: Revisiting adversarial training. In
International Conference on Learning Representations (ICLR), 2020."
REFERENCES,0.1797631862217438,"C. Xie, J. Wang, Z. Zhang, Z. Ren, and A. Yuille. Mitigating adversarial effects through randomization.
In International Conference on Learning Representations (ICLR), 2018."
REFERENCES,0.18083961248654468,"Yaodong Yu, Zitong Yang, Edgar Dobriban, Jacob Steinhardt, and Yi Ma. Understanding gen-
eralization in adversarial training via the bias-variance decomposition. In arXiv: 2103.09947,
2021."
REFERENCES,0.18191603875134554,"H. Zhang, Y. Yu, J. Jiao, E. P. Xing, L. E. Ghaoui, and M. I. Jordan. Theoretically principled trade-off
between robustness and accuracy. In International Conference on Machine Learning(ICLR), 2019."
REFERENCES,0.18299246501614638,"Y. Zhang, O. Plevrakis, S. S. Du, X. Li, Z. Song, and S. Arora. Over-parameterized adversarial
training: An analysis overcoming the curse of dimensionality. In arXiv: 2002.06668, 2020."
REFERENCES,0.18406889128094725,"A
PROOFS OF SECTION 3.1"
REFERENCES,0.18514531754574812,"In Section A.1, we discuss the smoothness of the adversarial training loss. In Section A.2, we prove
the alignment phenomenon of adversarial training for deep linear networks. We make the common
assumption that ˜L(W(0)) ≤˜L(0) = ℓ(0) and ∇˜L(W(0)) ̸= 0 to start adversarial training."
REFERENCES,0.18622174381054898,Published as a conference paper at ICLR 2022
REFERENCES,0.18729817007534985,"A.1
SMOOTHNESS OF THE ADVERSARIAL TRAINING LOSS"
REFERENCES,0.1883745963401507,"A.1.1
PROOF OF LEMMA 1"
REFERENCES,0.18945102260495156,"Proof. We ﬁrst prove that the adversarial training loss for the deep linear network is smooth within
S(r). For any W, V ∈S(r), let δW and δV denote the adversarial perturbations of the network
f(x; W) and f(x; V ), respectively. Then we have, by taking derivatives of L w.r.t W and using that
∥A∥F + ∥B∥F ≥∥A −B∥F ,"
REFERENCES,0.19052744886975242,"∂˜L
∂W1
−∂˜L ∂V1 F
≤1 n n
X i=1"
REFERENCES,0.1916038751345533,"W T
2 · · · W T
L xT
i yi˜ℓ′
i(WΠ) −V T
2 · · · V T
L xT
i yi˜ℓ′
i(VΠ)

F
|
{z
}
(a) + ϵ n n
X i=1 ∂∥WΠ∥"
REFERENCES,0.19268030139935413,"∂W1
˜ℓ′
i(WΠ) −∂∥VΠ∥"
REFERENCES,0.193756727664155,"∂V1
˜ℓ′
i(VΠ)

F
|
{z
}
(b)"
REFERENCES,0.19483315392895587,",
(21)"
REFERENCES,0.19590958019375673,"where the term (b) abruptly changes the training dynamics. For simplicity, we use x, y and ˜ℓ′ without
the subscript i because the analysis can be done similarly for any i ∈{1, . . . , n}. We begin with the
term (a) of Eq. (21):"
REFERENCES,0.1969860064585576,"(a) ≤
W T
2 · · · W T
L xT y˜ℓ′(WΠ) −V T
2 W T
3 · · · W T
L xT y˜ℓ′(WΠ)"
REFERENCES,0.19806243272335844,"+
V T
2 W T
3 · · · W T
L xT y˜ℓ′(WΠ) −V T
2 · · · V T
L xT y˜ℓ′(VΠ)"
REFERENCES,0.1991388589881593,"≤∥W2 −V2∥
W T
3 · · · W T
L xT y˜ℓ′(WΠ)"
REFERENCES,0.20021528525296017,"+ ∥V2∥
W T
3 · · · W T
L xT y˜ℓ′(WΠ) −V T
3 · · · V T
L xT y˜ℓ′(VΠ)"
REFERENCES,0.20129171151776104,"c
≤∥W2 −V2∥rL−2α + r
W T
3 · · · W T
L xT y˜ℓ′(WΠ) −V T
3 · · · V T
L xT y˜ℓ′(VΠ) d
≤ L
X"
REFERENCES,0.2023681377825619,"k=2
rL−2α ∥Wk −Vk∥+ rL−2∥VL∥
˜ℓ′(WΠ) −˜ℓ′(VΠ)"
REFERENCES,0.20344456404736275,"e
≤
 
rL−2α(L −1) + βLr2L−2
∥W −V ∥,
(22)"
REFERENCES,0.20452099031216361,"where c follows from ∥x∥≤1, ∥Wk∥≤r and ℓ′ ≤α; repeating the procedure of c until k = L
gives us d; e is because ∥Wk −Vk∥≤∥W −V ∥for any k and"
REFERENCES,0.20559741657696448,"˜ℓ′(WΠ) −˜ℓ′(VΠ)
 ≤β∥WΠ −VΠ∥≤βLrL−1∥W −V ∥
(23)"
REFERENCES,0.20667384284176535,"considering that ℓis a β-smooth function. It now remains us to bound the term (b) of Eq. (21). Note
that"
REFERENCES,0.2077502691065662,∂∥WΠ∥2
REFERENCES,0.20882669537136705,"∂W1
= W T
2 · · · W T
L WL · · · W1
∥WΠ∥2
,
(24)"
REFERENCES,0.20990312163616792,Published as a conference paper at ICLR 2022
REFERENCES,0.2109795479009688,then we have
REFERENCES,0.21205597416576966,"(b) ≤ϵ

W T
2 · · · W T
L WL · · · W1
∥WΠ∥
˜ℓ′(WΠ) −V T
2 W T
3 · · · W T
L WL · · · W1
∥WΠ∥
˜ℓ′(WΠ)"
REFERENCES,0.2131324004305705,"+ ϵ

V T
2 W T
3 · · · W T
L WL · · · W1
∥WΠ∥
˜ℓ′(WΠ) −V T
2 V T
3 · · · V T
L VL · · · V1
∥VΠ∥
˜ℓ′(VΠ)"
REFERENCES,0.21420882669537136,"c
≤ϵαr2L−2"
REFERENCES,0.21528525296017223,"¯rL
∥W2 −V2∥"
REFERENCES,0.2163616792249731,"+ ϵr

W T
3 · · · W T
L WL · · · W1
∥WΠ∥
˜ℓ′(WΠ) −V T
3 · · · V T
L VL · · · V1
∥VΠ∥
˜ℓ′(VΠ)"
REFERENCES,0.21743810548977396,"d
≤(2L −1)ϵαr2L−2"
REFERENCES,0.2185145317545748,"¯rL
∥W −V ∥+ ϵr2L−2"
REFERENCES,0.21959095801937567,˜ℓ′(WΠ)
REFERENCES,0.22066738428417654,"∥WΠ∥−
˜ℓ′(VΠ) ∥VΠ∥ "
REFERENCES,0.2217438105489774,"e
≤(2L −1)ϵαr2L−2"
REFERENCES,0.22282023681377824,"¯rL
∥W −V ∥+ ϵr2L−2 ¯rL"
REFERENCES,0.2238966630785791,"˜ℓ′(WΠ) −˜ℓ′(VΠ)
 + ϵαr2L−2"
REFERENCES,0.22497308934337998,"¯r2L
∥WΠ −VΠ∥"
REFERENCES,0.22604951560818085,"f
≤ϵr2L−2 ¯rL"
REFERENCES,0.2271259418729817,"
(2L −1)α + βLrL−1 + αβLrL−1 ¯rL"
REFERENCES,0.22820236813778255,"
∥W −V ∥,
(25)"
REFERENCES,0.22927879440258342,"where c follows from ∥Wk∥≤r and ˜ℓ′ ≤α; repeating c for 2L −1 times gives d; e follows from
∥∥WΠ∥−∥VΠ∥∥≤∥WΠ −VΠ∥; invoking Eq. (23) gives us f. Now we can bound Eq. (21) by
combing the above results

∂˜L
∂W1
−∂˜L ∂V1 F"
REFERENCES,0.2303552206673843,"≤

rL−2α(L −1) + βr2L−2L + ϵr2L−2 ¯rL"
REFERENCES,0.23143164693218515,"
(2L −1)α + βLrL−1 + αβLrL−1 ¯rL"
REFERENCES,0.232508073196986,"
∥W −V ∥"
REFERENCES,0.23358449946178686,"≤r3LL

α + β + ϵ ¯rL"
REFERENCES,0.23466092572658773,"
2α + β + αβ ¯rL"
REFERENCES,0.2357373519913886,"
∥W −V ∥,
(26)"
REFERENCES,0.23681377825618946,"where the last inequality is because we choose r ≥1. The above analysis is for the ﬁrst layer. Now
we can apply the same technique to other layers, which ﬁnally gives us"
REFERENCES,0.2378902045209903,"∥∇˜L(W) −∇˜L(V )∥≤r3LL2

α + β + ϵ ¯rL"
REFERENCES,0.23896663078579117,"
2α + β + αβ ¯rL"
REFERENCES,0.24004305705059203,"
∥W −V ∥,
(27)"
REFERENCES,0.2411194833153929,"thus ˜L is β(r, ¯r, ϵ)-smooth inside S(r) where"
REFERENCES,0.24219590958019377,"β(r, ¯r, ϵ) = r3LL2

α + β + ϵ ¯rL"
REFERENCES,0.2432723358449946,"
2α + β + αβ ¯rL"
REFERENCES,0.24434876210979548,"
.
(28)"
REFERENCES,0.24542518837459634,"We now prove the second statement of Lemma 1, which says that W will leave S(r) with constant r
and learning rate η. Let W(t), W(t + 1) ∈S(r) and η(t) = min{1, 1/β(r, ¯r, ϵ)}."
REFERENCES,0.2465016146393972,"Lemma 5 (Adapted from Lemma 6 in Ji & Telgarsky (2019)). Under Assumption 1 and Assumption
3, suppose gradient descent for adversarial training is run with a constant step size 1/β(r, ϵ, ¯r). Then
there exists a time t when maxk ∥Wk(t)∥F > r."
REFERENCES,0.24757804090419805,"Eq. (27) states that ˜L is β(r, ¯r, ϵ)-smooth, therefore"
REFERENCES,0.24865446716899892,"˜L(W(t + 1)) −˜L(W(t)) ≤
D
∇˜L(W(t)), −η(t) ˜L(W(t))
E
+ β(r, ¯r, ϵ)η(t)2"
REFERENCES,0.24973089343379978,"2
∥∇˜L(W(t))∥2"
REFERENCES,0.25080731969860065,= −η(t)
REFERENCES,0.2518837459634015,"2 ∥∇˜L(W(t))∥2,
(29)"
REFERENCES,0.2529601722282024,which also implies that ˜L(W(t)) never increases during the training
REFERENCES,0.25403659849300325,˜L(W(t + 1)) ≤˜L(W(t)) −η(t)
REFERENCES,0.25511302475780406,"2 ∥∇˜L(W(t))∥2.
(30)"
REFERENCES,0.25618945102260493,"By repeatedly applying Lemma 5 when the learning rate is constant, we know W will eventually
leave S(r), which also implies that maxk ∥Wk∥F is unbounded in this case."
REFERENCES,0.2572658772874058,Published as a conference paper at ICLR 2022
REFERENCES,0.25834230355220666,"A.1.2
PROOF OF LEMMA 2"
REFERENCES,0.25941872981700753,"Proof. For any step t with the learning rate in the Lemma 2, suppose that W(t) ∈S(r(t) −µ(t)),
we then have"
REFERENCES,0.2604951560818084,∥Wk(t + 1)∥F ≤∥Wk(t)∥F + η(t)
REFERENCES,0.26157158234660927,"∂˜L
∂Wk(t)"
REFERENCES,0.26264800861141013,"F
≤∥Wk(t)∥F + µ(t),"
REFERENCES,0.263724434876211,"where the last inequality follows from

∂˜L
∂Wk(t)"
REFERENCES,0.26480086114101187,"≤
Wk+1 · · · W T
L xT W T
1 · · · W T
k−1y˜ℓ′(WΠ)
 + ϵ

∂∥WΠ∥2"
REFERENCES,0.2658772874058127,"∂Wk
˜ℓ′(WΠ)"
REFERENCES,0.26695371367061355,≤(1 + ϵ
REFERENCES,0.2680301399354144,"¯rL )αr2L.
(31)"
REFERENCES,0.2691065662002153,"Therefore ∥Wk(t + 1)∥F will always stay in S(r(t)) and the smoothness is preserved.
If
∥Wk(t + 1)∥F is still less than r(t) −µ(t), then we keep r(t + 1) = r(t) while we increase
r(t + 1) to
r(t + 1) = r(t) + µ(t)"
REFERENCES,0.27018299246501615,"if r(t) −µ(t) ≤∥Wk(t + 1)∥F ≤r(t) such that ∥Wk(t + 1)∥F ≤r(t) ≤r(t + 1) −µ(t + 1),
which means that W(t + 2) will stay inside S(r(t + 1)). The overall smoothness can then be proved
by induction."
REFERENCES,0.271259418729817,"A.2
ALIGNMENT PHENOMENON"
REFERENCES,0.2723358449946179,We start with the following lemma for the adversarial training to prove the alignment phenomenon5:
REFERENCES,0.27341227125941875,"Lemma 6 (Bounded differences between weight norms). The difference of Frobenius norms for any
two layers is bounded during the adversarial training process
 ∥Wk(t)∥2
F −∥Wj(t)∥2
F
 −2 ˜L(W(0)) ≤
 ∥Wk(0)∥2
F −∥Wj(0)∥2
F
, ∀k, j ∈{1, . . . , L}."
REFERENCES,0.2744886975242196,"Proof. If there is not adversarial perturbation, then we can easily obtain"
REFERENCES,0.2755651237890204,"W T
k+1
∂L
∂Wk+1
= ∂L"
REFERENCES,0.2766415500538213,"∂Wk
W T
k
(32)"
REFERENCES,0.27771797631862216,"by conducting some algebra, where L stands for the empirical loss without adversarial perturbation.
Then invoking Eq. (24) for k by noting that tr (A) = tr
 
AT 
and tr (ABC) = tr (CAB), we have"
REFERENCES,0.27879440258342303,"yϵ˜ℓ(WΠ)W T
k+1
∂∥WΠ∥2"
REFERENCES,0.2798708288482239,"∂Wk+1
= yϵ˜ℓ(WΠ)"
REFERENCES,0.28094725511302476,"∥WΠ∥2
W T
k+1 · · · W T
L WL · · · W1W T
1 · · · W T
k"
REFERENCES,0.28202368137782563,= yϵ˜ℓ(WΠ)∂∥WΠ∥2
REFERENCES,0.2831001076426265,"∂Wk
W T
k .
(33)"
REFERENCES,0.28417653390742736,"Combining Eq. (32), for the adversarial empirical loss, we have"
REFERENCES,0.2852529601722282,"W T
k+1
∂˜L
∂Wk+1
= ∂˜L"
REFERENCES,0.28632938643702904,"∂Wk
W T
k ,
(34)"
REFERENCES,0.2874058127018299,which will give us the following relation
REFERENCES,0.2884822389666308,"W T
k+1(t + 1)Wk+1(t + 1) = W T
k+1(t)Wk+1(t) + η(t)2
 
∂˜L
∂Wk+1(t)"
REFERENCES,0.28955866523143164,"!T  
∂˜L
∂Wk+1(t) ! −η(t) "
REFERENCES,0.2906350914962325,"W T
k+1(t)"
REFERENCES,0.2917115177610334,"∂˜L
∂Wk+1(t) ! +"
REFERENCES,0.29278794402583425,"∂˜L
∂Wk+1(t) !T"
REFERENCES,0.2938643702906351,"Wk+1(t)  ,"
REFERENCES,0.294940796555436,5This property for standard training has previously been studied by Ji & Telgarsky (2019); Arora et al. (2018).
REFERENCES,0.2960172228202368,Published as a conference paper at ICLR 2022
REFERENCES,0.29709364908503766,"which can be easily seen by writing out the gradient descent update explicitly. One can derive a
similar relation for Wk(t + 1)W T
k (t + 1). Combing Eq. (32), we have"
REFERENCES,0.2981700753498385,"W T
k+1(t + 1)Wk+1(t + 1) −W T
k+1(t)Wk+1(t) −η(t)2
 
∂˜L
∂Wk+1(t)"
REFERENCES,0.2992465016146394,"!T  
∂˜L
∂Wk+1(t) !"
REFERENCES,0.30032292787944026,"= Wk(t + 1)W T
k (t + 1) −Wk(t)W T
k (t) −η(t)2
 
∂˜L
∂Wk(t)"
REFERENCES,0.3013993541442411,"!  
∂˜L
∂Wk(t) !T"
REFERENCES,0.302475780409042,".
(35)"
REFERENCES,0.30355220667384286,Summing up the above equation from 0 to t will give us
REFERENCES,0.30462863293864373,"W T
k+1(t)Wk+1(t) −W T
k+1(0)Wk+1(0) −Ak+1(t) = Wk(t)W T
k (t) −Wk(0)W T
k (0) −Bk(t)
(36)"
REFERENCES,0.30570505920344454,where symmetric matrices Ak and Bk are deﬁned by
REFERENCES,0.3067814854682454,"Ak(t) = t−1
X"
REFERENCES,0.3078579117330463,"τ=0
η(τ)2
 
∂˜L
∂Wk(τ)"
REFERENCES,0.30893433799784714,"!T  
∂˜L
∂Wk(τ) ! and"
REFERENCES,0.310010764262648,"Bk(t) = t−1
X"
REFERENCES,0.3110871905274489,"τ=0
η(τ)2
 
∂˜L
∂Wk(τ)"
REFERENCES,0.31216361679224974,"!  
∂˜L
∂Wk(τ) !T ."
REFERENCES,0.3132400430570506,Also notice that
REFERENCES,0.3143164693218515,"tr (Ak(t)) = tr (Bk(t))
(37) where"
REFERENCES,0.3153928955866523,"tr (Ak(t)) ≤ L
X"
REFERENCES,0.31646932185145316,"k′=1
tr "
REFERENCES,0.317545748116254,"
t−1
X"
REFERENCES,0.3186221743810549,"τ=0
η(τ)2
 
∂˜L
∂Wk′(τ)"
REFERENCES,0.31969860064585576,"!T  
∂˜L
∂Wk′(τ) !  = L
X k′=1 t−1
X"
REFERENCES,0.3207750269106566,"τ=0
η(τ)2

∂˜L
∂Wk′(τ)  2 F a
≤ t−1
X"
REFERENCES,0.3218514531754575,"τ=0
η(τ)
∇˜L(W(τ))

2 b
≤2 t−1
X"
REFERENCES,0.32292787944025836,"τ=0
η(τ)
h
˜L(W(τ)) −˜L(W(τ + 1))
i"
REFERENCES,0.3240043057050592,"c= 2 ˜L(W(0)) −2 ˜L(W(t)) ≤2 ˜L(W(0)),
(38)"
REFERENCES,0.32508073196986004,"where a is because η < 1; b is due to Eq. (29); summing up from τ = 0 to t −1 and noting that
˜L > 0 give us c. For any two layers k < j, taking trace of both sides of (36) and summing from k to
j, we have that the differences between the Frobenius norms for any two layers
∥Wk(t)∥2
F −∥Wj(t)∥2
F
 =
tr (Aj(t)) + ∥Wj(0)∥2
F −tr (Bk(t)) −∥Wk(0)∥2
F"
REFERENCES,0.3261571582346609,"≤2 ˜L(W(0)) +
∥Wj(0)∥2
F −∥Wk(0)∥2
F"
REFERENCES,0.32723358449946177,"are always bounded. Since max1≤k≤L ∥Wk∥F →∞, the above result implies that ∥Wk∥F →∞
for all k."
REFERENCES,0.32831001076426264,"A.2.1
PROOF OF THEOREM 2"
REFERENCES,0.3293864370290635,"Intuitively, Wk can be decomposed as"
REFERENCES,0.3304628632938644,"Wk(t) = Uk(t)Σk(t)V T
k (t)."
REFERENCES,0.33153928955866524,Published as a conference paper at ICLR 2022
REFERENCES,0.3326157158234661,"For Wk, denote the ﬁrst singular value, the corresponding left singular vector and right singular
vector by σk, uk and vk. Since σk →∞, the initialization becomes negligible as t →∞. According
to Eq. (36), this will lead to"
REFERENCES,0.333692142088267,"Uk(t)Σk(t)ΣT
k (t)U T
k (t) →Vk+1(t)ΣT
k+1(t)Σk+1(t)V T
k+1(t)"
REFERENCES,0.33476856835306784,"as t →∞. On the other hand, since WLW T
L has rank 1 for WL being a row vector, all layers have
rank 1 because they get aligned with WL and
Wk
∥Wk∥F →ukvT
k ."
REFERENCES,0.33584499461786865,"To further elaborate the above reasoning, we provide a more exact proof starting with the deﬁnition
of the alignment phenomenon."
REFERENCES,0.3369214208826695,"Deﬁnition 2 (Alignment phenomenon for deep linear networks.). For deep linear network f(x; W) =
WL · · · W1x, the alignment phenomenon is deﬁned as"
REFERENCES,0.3379978471474704,"∀k ∈{1, . . . , k} : | ⟨uk, vk+1⟩| →1
(39)"
REFERENCES,0.33907427341227125,"and Wk/∥Wk∥F →ukvT
k as t →∞along the training trajectory, where uk, vk are the left and
right singular vectors correspond to the largest singular value of Wk."
REFERENCES,0.3401506996770721,"The proof for the alignment phenomenon of adversarially trained deep linear networks in our setting
is as follows 6."
REFERENCES,0.341227125941873,Proof. Note that
REFERENCES,0.34230355220667386,"⟨uk, vk+1⟩2 σ2
k+1 = uT
k W T
k+1Wk+1uk + uT
k (vk+1σ2
k+1vT
k+1 −W T
k+1Wk+1)uk,
(40)"
REFERENCES,0.3433799784714747,"we have
uT
k W T
k+1Wk+1uk"
REFERENCES,0.3444564047362756,"σ2
k+1
+ ∥Wk+1∥2
2 −∥Wk+1∥2
F
σ2
k+1
≤⟨uk, vk+1⟩2 ≤1
(41)"
REFERENCES,0.3455328310010764,by utilizing the deﬁnition of matrix norm and
REFERENCES,0.34660925726587727,"∥Wk+1∥2
F ≥uT
k W T
k+1Wk+1uk.
(42)"
REFERENCES,0.34768568353067814,"According to Eq. (36), let"
REFERENCES,0.348762109795479,"Γk = W T
k+1(0)Wk+1(0) −Wk(0)Wk(0)T + Ak+1 −Bk,
(43)"
REFERENCES,0.34983853606027987,"and replace W T
k+1Wk+1 with Γk + WkW T
k in Eq. (41), we have"
REFERENCES,0.35091496232508074,"⟨uk, vk+1⟩2 ≥
σ2
k
σ2
k+1
+ uT
k Γkuk + ∥Wk+1∥2
2 −∥Wk+1∥2
F
σ2
k+1
.
(44)"
REFERENCES,0.3519913885898816,"To prove the alignment phenomenon, we now bound the RHS of Eq. (44) with 1 −α where α →0
when t →∞. We are going to bound 3 terms, namely uT
k Γkuk, ∥Wk∥2
2 −∥Wk∥2
F and σ2
k/σ2
k+1."
BOUND UT,0.35306781485468247,"1. Bound uT
k Γkuk."
BOUND UT,0.35414424111948334,"uT
k Γkuk ≥−uT
k Wk(0)Wk(0)T uk −uT
k Bkuk
≥−∥Wk(0)∥2
2 −tr (Bk)"
BOUND UT,0.35522066738428415,"≥−∥Wk(0)∥2
2 −2 ˜L(W(0)),
(45)"
BOUND UT,0.356297093649085,where we used Eq. (38) in the third inequality.
BOUND UT,0.3573735199138859,"2. Bound ∥Wk∥2
2 −∥Wk∥2
F . According to the deﬁnition of singular values, we have"
BOUND UT,0.35844994617868675,"∥Wk∥2
2 = σ2
k ≥vT
k+1WkW T
k vk+1
= vT
k+1(W T
k+1Wk+1 −Γk)vk+1
≥σ2
k+1 −vT
k+1∆Wk(0)vk+1 −tr (Bk)
(46)"
BOUND UT,0.3595263724434876,6Some parts of the proof is inspired by Ji & Telgarsky (2019)
BOUND UT,0.3606027987082885,Published as a conference paper at ICLR 2022
BOUND UT,0.36167922497308935,"where ∆Wk(0) = Wk(0)Wk(0)T −W T
k+1(0)Wk+1(0) and we used that tr (Bk) ≥∥Bk∥2
in the second inequality. On the other hand, by taking trace of both sides of Eq. (36), we
have"
BOUND UT,0.3627556512378902,"∥Wk∥2
F = ∥Wk+1∥2
F −∥Wk+1(0)∥2
F + ∥Wk(0)∥2
F −tr (Ak+1) + tr (Bk) .
(47)"
BOUND UT,0.3638320775026911,"Combined with Eq. (46), we have"
BOUND UT,0.36490850376749195,"∥Wk∥2
2 −∥Wk∥2
F ≥∥Wk+1∥2
2 −∥Wk+1∥2
F
+ (∥Wk+1(0)∥2
F −∥Wk(0)∥2
F ) −vT
k+1∆W(0)vk+1
+ (tr (Ak+1) −tr (Bk)) −tr (Bk)"
BOUND UT,0.36598493003229277,"a
≥∥Wk+2∥2
2 −∥Wk+2∥2
F
+ (∥Wk+2(0)∥2
F −∥Wk(0)∥2
F ) −(vT
k+1∆Wk(0)vk+1 + vT
k+2∆Wk+1(0)vk+2)"
BOUND UT,0.36706135629709363,"+ (tr (Ak+2) −tr (Bk)) −(tr (Bk) + tr (Bk+1))
. . ."
BOUND UT,0.3681377825618945,"b
≥∥WL(0)∥2
F −∥Wk(0)∥2
F − L−1
X"
BOUND UT,0.36921420882669537,"k′=k
vT
k′+1∆Wk′(0)vk′+1 − L−1
X"
BOUND UT,0.37029063509149623,"k′=k
tr (Bk′)"
BOUND UT,0.3713670613562971,+ tr (AL) −tr (Bk)
BOUND UT,0.37244348762109797,"c
≥M −2L ˜L(W(0)),
(48)"
BOUND UT,0.37351991388589884,"where a follows from Eq. (37); b follows from summing a from k to L −1 and that
∥WL∥2 = ∥WL∥F ; c is because Eq. (38) and that"
BOUND UT,0.3745963401506997,"M = min ∥WL(0)∥2
F −∥Wk(0)∥2
F − L−1
X"
BOUND UT,0.3756727664155005,"k′=k
vT
k′+1∆Wk′(0)vk′+1
(49)"
BOUND UT,0.3767491926803014,is ﬁnite.
BOUND UT,0.37782561894510225,"3. Bound σ2
k/σ2
k+1. Eq. (46) gives us that"
BOUND UT,0.3789020452099031,"σ2
k
σ2
k+1
≥1−vT
k+1∆Wk(0)vk+1 −tr (Bk)"
BOUND UT,0.379978471474704,"σ2
k+1
≥1−vT
k+1∆Wk(0)vk+1 + 2 ˜L(W(0))"
BOUND UT,0.38105489773950485,"σ2
k+1
, (50)"
BOUND UT,0.3821313240043057,"where vT
k+1∆Wk(0)vk+1 + 2 ˜L(W(0)) is ﬁnite."
BOUND UT,0.3832077502691066,"Putting the bounds obtained from the above 1, 2 and 3 back to Eq. (44) will give us"
BOUND UT,0.38428417653390745,"⟨uk, vk+1⟩2 ≥1 −∥Wk(0)∥2
2 + 2(L + 2) ˜L(W(0)) −M + vT
k+1∆Wk(0)vk+1
σ2
k+1
.
(51)"
BOUND UT,0.38536060279870826,"Considering that σk →∞and the numerator is ﬁnite, we thus conclude that the RHS of Eq. (51) will
converge to 1. Therefore, we have that"
BOUND UT,0.38643702906350913,"⟨uk, vk+1⟩2 →1
(52)"
BOUND UT,0.38751345532831,"as t →∞. Furthermore, since ∥Wk+1∥2
F −∥Wk+1∥2
2 is ﬁnite (see Eq. (48)) and σk →∞(i.e.,
∥Wk∥2
2/∥Wk∥2
F →1), we have
Wk
∥Wk∥F →ukvT
k because other singular values are negligible. We
now have the alignment phenomenon."
BOUND UT,0.38858988159311086,"Furthermore, we have
WΠ
∥WL∥F · · · ∥W1∥F
→v1uT
1 · · · vLuT
L →v1.
(53)"
BOUND UT,0.38966630785791173,"We now move forward to prove the alignment phenomenon for the ﬁrst layer. We assume that the
support vectors span the data space Rd and denote the orthogonal complement of span(¯u) by ¯u⊥.
Let P⊥denote the projection onto ¯u⊥. Under this assumption, Ji & Telgarsky (2019) showed the
following lemma7"
BOUND UT,0.3907427341227126,7This lemma is based on Lemma 12 in Soudry et al. (2018).
BOUND UT,0.39181916038751347,Published as a conference paper at ICLR 2022
BOUND UT,0.39289558665231433,"Lemma 7 (Lemma 3 in Ji & Telgarsky (2019)). Let S denote the set of indices of support vectors ,
then with probability 1,
κ :=
min
|ξ|=1,ξ⊥¯u max
i∈S ⟨ξ, xiyi⟩> 0
(54)"
BOUND UT,0.3939720129171152,if the data is sampled from absolutely continuous distribution.
BOUND UT,0.395048439181916,"To begin with, we ﬁrst introduce the following lemma.
Lemma 8. For the logistic loss function ℓ= ln(1+e−x), if ⟨WΠ, ¯u⟩≥0 and ∥P⊥WΠ∥≥2n/eκ =
ϕ, then
*"
BOUND UT,0.3961248654467169,"P⊥W1, ∂˜L ∂W1 +"
BOUND UT,0.39720129171151775,"≥0.
(55)"
BOUND UT,0.3982777179763186,"Proof. We ﬁrst decompose Eq. (55) as two parts:
*"
BOUND UT,0.3993541442411195,"P⊥W1, ∂˜L ∂W1 + = 1 n n
X"
BOUND UT,0.40043057050592035,"i=1
˜ℓ′
i 
yi"
BOUND UT,0.4015069967707212,"P⊥W1, ∂f(xi; W) ∂W1"
BOUND UT,0.4025834230355221,"−ϵ

P⊥W1, ∂∥WΠ∥ ∂W1"
BOUND UT,0.40365984930032295,"
,
(56)"
BOUND UT,0.4047362755651238,"where

P⊥W1, ∂∥WΠ∥ ∂W1"
BOUND UT,0.4058127018299246,"=
1
∥WΠ∥

P⊥W1, W T
1 · · · W T
L WL · · · W2"
BOUND UT,0.4068891280947255,"=
1
∥WΠ∥⟨P⊥WΠ, P⊥WΠ⟩≥0."
BOUND UT,0.40796555435952636,"Since ˜ℓ′ < 0, we have
*"
BOUND UT,0.40904198062432723,"P⊥W1, ∂˜L ∂W1 + ≥1 n n
X"
BOUND UT,0.4101184068891281,"i=1
˜ℓ′
iyi"
BOUND UT,0.41119483315392896,"P⊥W1, ∂f(xi; W) ∂W1 = 1 n n
X i=1"
BOUND UT,0.41227125941872983,"D
P⊥WΠ, xiyi˜ℓ′(WΠ)
E
.
(57)"
BOUND UT,0.4133476856835307,"Let xjyj ∈arg maxi∈S ⟨−P⊥WΠ, xiyi⟩which means that
⟨−P⊥WΠ, xjyj⟩≥κ∥P⊥WΠ∥.
Then we have
1
n n
X i=1"
BOUND UT,0.41442411194833156,"D
P⊥WΠ, xiyi˜ℓ′(WΠ)
E = 1 n n
X i=1"
BOUND UT,0.4155005382131324,"e−⟨WΠ,(xi+δi)yi⟩"
BOUND UT,0.41657696447793324,"1 + e−⟨WΠ,(xi+δi)yi⟩⟨−P⊥WΠ, xiyi⟩ = 1 n n
X i=1"
BOUND UT,0.4176533907427341,"e−⟨WΠ,(xi+δi)yi⟩"
BOUND UT,0.418729817007535,"1 + e−⟨WΠ,(xi+δi)yi⟩⟨−P⊥WΠ, P⊥xiyi⟩ ≥1 n n
X i=1"
BOUND UT,0.41980624327233584,"e−⟨WΠ,(xi+δi)yi⟩"
BOUND UT,0.4208826695371367,"1 + e−⟨WΠ,(xi+δi)yi⟩⟨−P⊥WΠ, P⊥xiyi⟩ ≥1"
BOUND UT,0.4219590958019376,"n
e−⟨WΠ,(xj+δj)yj⟩"
BOUND UT,0.42303552206673845,"1 + e−⟨WΠ,(xj+δj)yj⟩⟨−P⊥WΠ, P⊥xiyi⟩+ 1 n X k∈Ck"
BOUND UT,0.4241119483315393,"e−⟨WΠ,(xk+δk)yk⟩"
BOUND UT,0.4251883745963401,"1 + e−⟨WΠ,(xk+δk)yk⟩⟨−P⊥WΠ, P⊥xkyk⟩, (58)"
BOUND UT,0.426264800861141,"where Ck = {k : ⟨−P⊥WΠ, P⊥xkyk⟩≤0}. Considering that xjyj is a support vector, the ﬁrst term
of (58) can be bounded as follows:
1
n
e−⟨WΠ,(xj+δj)yj⟩"
BOUND UT,0.42734122712594186,"1 + e−⟨WΠ,(xj+δj)yj⟩⟨−P⊥WΠ, P⊥xiyi⟩ ≥κ"
BOUND UT,0.4284176533907427,"n∥P⊥WΠ∥e−⟨WΠ,γ¯u⟩eϵ∥WΠ∥
e−⟨WΠ,xjyj−γ¯u⟩"
BOUND UT,0.4294940796555436,"1 + e−⟨WΠ,(xj+δj)yj⟩ = κ"
BOUND UT,0.43057050592034446,"n∥P⊥WΠ∥e−⟨WΠ,γ¯u⟩eϵ∥WΠ∥
e−⟨P⊥WΠ,P⊥xjyj⟩"
BOUND UT,0.4316469321851453,"1 + eϵ∥WΠ∥e−⟨P⊥WΠ,P⊥xjyj⟩e−⟨WΠ,γ¯u⟩ ≥κ"
BOUND UT,0.4327233584499462,"2n∥P⊥WΠ∥e−⟨WΠ,γ¯u⟩eϵ∥WΠ∥,
(59)"
BOUND UT,0.43379978471474706,Published as a conference paper at ICLR 2022
BOUND UT,0.43487621097954793,"where we use that ϵ∥WΠ∥, ⟨WΠ, γ¯u⟩, ⟨P⊥WΠ, P⊥xjyj⟩> 0. Each term of the second part of (58)
can be bounded by"
BOUND UT,0.43595263724434874,"e−⟨WΠ,(xk+δk)yk⟩"
BOUND UT,0.4370290635091496,"1 + e−⟨WΠ,(xk+δk)yk⟩⟨−P⊥WΠ, P⊥xkyk⟩"
BOUND UT,0.4381054897739505,"≥eϵ∥WΠ∥e−⟨WΠ,xkyk⟩⟨−P⊥WΠ, P⊥xkyk⟩"
BOUND UT,0.43918191603875134,"= eϵ∥WΠ∥e−⟨WΠ,γ¯u⟩e−⟨WΠ,xkyk−γ¯u⟩⟨−P⊥WΠ, P⊥xkyk⟩"
BOUND UT,0.4402583423035522,"≥eϵ∥WΠ∥e−⟨WΠ,γ¯u⟩e−⟨−P⊥WΠ,P⊥xkyk⟩⟨−P⊥WΠ, P⊥xkyk⟩"
BOUND UT,0.4413347685683531,"≥eϵ∥WΠ∥e−⟨WΠ,γ¯u⟩

−1 e"
BOUND UT,0.44241119483315394,"
,
(60)"
BOUND UT,0.4434876210979548,where the second inequality follows from
BOUND UT,0.4445640473627557,"⟨WΠ, xkyk −γ¯u⟩= ⟨WΠ, P⊥xkyk⟩+ ⟨WΠ, xkyk −P⊥xkyk −γ¯u⟩
≥⟨WΠ, P⊥xkyk⟩= ⟨P⊥WΠ, P⊥xkyk⟩"
BOUND UT,0.4456404736275565,"while the last inequality comes from f(x) = −xe−x ≥−1/e for x ≥0. Finally, we have
D
P⊥WΠ, (xi + δi)yi˜ℓ′(WΠ)
E
≥e−⟨WΠ,γ¯u⟩eϵ∥WΠ∥
κ∥P⊥WΠ∥"
N,0.44671689989235736,"2n
−1 e"
N,0.4477933261571582,"
.
(61)"
N,0.4488697524219591,"Therefore,
D
P⊥WΠ, (xi + δi)yi˜ℓ′(WΠ)
E
≥0 if"
N,0.44994617868675996,∥P⊥WΠ∥≥2n
N,0.4510226049515608,"eκ = ϕ.
(62)"
N,0.4520990312163617,"We are now ready to prove the alignment phenomenon for the ﬁrst layer. Let P⊥W1 denote the
projection of rows of W1 onto ¯u⊥. We start from exploring the asymptotic behavior of ∥P⊥W1∥F"
N,0.45317545748116256,"∥W1∥F
when t →∞. The update of P⊥W1(t + 1) can be written explicitly as"
N,0.4542518837459634,"∥P⊥W1(t + 1)∥2
F"
N,0.45532831001076424,"= ∥P⊥W1(t)∥2
F −2η(t)
*"
N,0.4564047362755651,"P⊥W1(t), P⊥
∂˜L
∂W1(t) +"
N,0.45748116254036597,"+ η(t)2
P⊥
∂˜L
∂W1(t)  2 F"
N,0.45855758880516684,"≤∥P⊥W1(t)∥2
F −2η(t)
*"
N,0.4596340150699677,"P⊥W1(t), P⊥
∂˜L
∂W1(t) +"
N,0.4607104413347686,"+ η(t)2

∂˜L
∂W1(t)  2 F"
N,0.46178686759956944,"≤∥P⊥W1(t)∥2
F −2η(t)
*"
N,0.4628632938643703,"P⊥W1(t),
∂˜L
∂W1(t) +"
N,0.4639397201291712,"+ 2

˜L(W(t + 1)) −˜L(W(t))

(63)"
N,0.465016146393972,"where the last inequality follows from Eq. (30). We deﬁne a large enough step t0 as follows: for any
t ≥t0, we have"
N,0.46609257265877285,"∥W1(t + 1)∥2
F = ∥W1(t)∥2
F −2η(t) *"
N,0.4671689989235737,"W1(t),
∂˜L
∂W1(t) +"
N,0.4682454251883746,"+ η(t)2

∂˜L
∂W1(t)  2"
N,0.46932185145317545,"F
≥∥W1(t)∥2
F , (64)"
N,0.4703982777179763,where ”large enough t0” means that
N,0.4714747039827772,"ϕ2 + ˜L(t0)
∥W1(t0)∥F
→0
(65)"
N,0.47255113024757806,as ∥W1∥F →∞. Suppose that there exists a t1 ≥t0 such that
N,0.4736275565123789,"∥P⊥WΠ(t1 −1)∥F < ϕ and ∥P⊥WΠ(t1)∥F ≥ϕ,
(66)"
N,0.4747039827771798,Published as a conference paper at ICLR 2022
N,0.4757804090419806,"which is to say (recalling Lemma 8)
*"
N,0.47685683530678147,"P⊥W1(t),
∂˜L
∂W1(t) + ≥0"
N,0.47793326157158234,"=⇒∥P⊥W1(t + 1)∥2
F ≤∥P⊥W1(t)∥2
F + 2

˜L(W(t)) −˜L(W(t + 1))

,
(67)"
N,0.4790096878363832,"for t0 ≤t1 ≤t ≤t2 and t2 = ∞if we never have ∥P⊥WΠ(t)∥F < ϕ after t1. If there does not
exist such a t1, then we directly conclude that ∥P⊥W1∥F"
N,0.48008611410118407,"∥W1∥F
→0 since ∥W1∥F →∞. On the other
hand, for any t0 ≤t1 ≤t ≤t2"
N,0.48116254036598494,"∥P⊥W1(t)∥2
F
∥W1(t)∥2
F
≤
∥P⊥W1(t1)∥2
F + 2

˜L(W(t1)) −˜L(W(t))
"
N,0.4822389666307858,"∥W1(t1)∥2
F"
N,0.48331539289558667,"≤∥P⊥W1(t1)∥2
F + 2 ˜L(W(t1))"
N,0.48439181916038754,"∥W1(t1)∥2
F"
N,0.48546824542518835,≤ϕ2 + 2µ(t1)ϕ + µ2(t1) + 2 ˜L(W(t1))
N,0.4865446716899892,"∥W1(t1)∥2
F"
N,0.4876210979547901,"=
Φ(ϕ, t1)"
N,0.48869752421959095,"∥W1(t1)∥2
F
(68)"
N,0.4897739504843918,"where the last inequality follows from Eq. (31) and Φ(ϕ, t1) is deﬁned by"
N,0.4908503767491927,"Φ(ϕ, t1) = ϕ2 + 2µ(t1)ϕ + µ2(t1) + 2 ˜L(W(t1)) ≥Φ(ϕ, t′)
(69)"
N,0.49192680301399355,for any t′ ≥t1 because the loss and µ never increase. Therefore we can conclude that
N,0.4930032292787944,"∥P⊥W1(t)∥2
F
∥W1(t)∥2
F
≤
Φ(ϕ, t)"
N,0.4940796555435953,"∥W1(t)∥2
F
≤
Φ(ϕ, t1)"
N,0.4951560818083961,"∥W1(t1)∥2
F
→0
(70)"
N,0.49623250807319697,"for any t ≥t1. As a result,
| ⟨v1(t), ¯u⟩| →1.
(71)"
N,0.49730893433799783,"B
PROOFS OF SECTION 3.2"
N,0.4983853606027987,"We present some useful properties and proofs of Lemma 3 and Lemma 4 in Section B.1. Section
B.2 focuses on the divergences of weight norms. Section B.3 is about the convergence to KKT
points(Theorem 5)."
N,0.49946178686759957,"Additional notations
We use Wij;k to represent the i-th row j-th column entry of the k-th layer
weight martrix."
N,0.5005382131324004,"B.1
USEFUL PROPERTIES AND PROOFS OF LEMMA 3 AND LEMMA 4"
N,0.5016146393972013,"For any m × n matrix A, we denote v(A), an mn-dimensional vector, as the vectorized version of it:"
N,0.5026910656620022,v(A) = 
N,0.503767491926803,"




"
N,0.5048439181916039,"A11
A21
. . .
Am1
. . .
Amn "
N,0.5059203444564048,"





,
(72)"
N,0.5069967707212056,then the trace operator can be represented by
N,0.5080731969860065,"tr
 
AT B

= v(A)T v(B) = v(B)T v(A).
(73)"
N,0.5091496232508074,We ﬁrst introduce the Euler’s theorem on homogeneous functions.
N,0.5102260495156081,Published as a conference paper at ICLR 2022
N,0.511302475780409,"Lemma 9 (Euler’s theorem on homogeneous functions). If f(W1, . . . , WL) is a positive multi-ck-
homogeneous function"
N,0.5123789020452099,"f(ρ1W1, . . . , ρLWL) = L
Y"
N,0.5134553283100107,"k=1
ρck
k f(W1, . . . , WL)
(74)"
N,0.5145317545748116,"for positive constants ρk’s and ck ≥1, then we have:"
N,0.5156081808396125,"tr
∂f(W1, . . . , WL)"
N,0.5166846071044133,"∂Wk
Wk"
N,0.5177610333692142,"
= ckf(W1, . . . , WL).
(75)"
N,0.5188374596340151,"Proof. Taking derivatives of ρk on both sides of Eq. (74) for any given k ∈{1, . . . , L}, we have"
N,0.5199138858988159,"Wk, ∂f ∂Wk"
N,0.5209903121636168,"= tr

Wk
∂f
∂Wk 
= L
Y"
N,0.5220667384284177,"k′̸=k
ρck′
k′ f(x; W)ckρck−1
k
.
(76)"
N,0.5231431646932185,"Since ρk is arbitrary, we let ρL = · · · = ρk = · · · = ρ1 = 1, then the above equation becomes"
N,0.5242195909580194,"tr

Wk
∂f(W1, . . . , WL) ∂Wk"
N,0.5252960172228203,"
= ckf(W1, . . . , WL)
(77)"
N,0.5263724434876211,"for any k ∈{1, . . . , L}."
N,0.527448869752422,"Furthermore, we have
∂f(x; W)"
N,0.5285252960172229,"∂W
, W

= L
X"
N,0.5296017222820237,"k=1
tr
∂f(x; W)"
N,0.5306781485468245,"∂Wk
Wk "
N,0.5317545748116254,"= Kf(x, W)
(78)"
N,0.5328310010764262,"where K = PL
k=1 ck. For the deep neural networks deﬁned in Eq. (2) with homogeneous property,
we can then apply the above lemma to them."
N,0.5339074273412271,We examine whether Assumption 2 can still be applied to adversarial margin under Assumption 1.
N,0.534983853606028,"Lemma 10 (Assumption 2 for adversarial training). For any ﬁxed x,"
N,0.5360602798708288,"• if yf(x; W) is locally Lipschitz, then so does yf(x + δ(W); W) with perturbation δ(W)."
N,0.5371367061356297,"Proof. For ﬁxed x, suppose yf(W) is locally Lipschitz on Y , then for each W ⊂Y there
is a ZW containing W such that yf(W) is Lipschitz on ZW :"
N,0.5382131324004306,"∥yf(W) −yf(V )∥≤LW ∥W −V ∥for W, V ⊂ZW .
(79)"
N,0.5392895586652314,"For the adversarial perturbation, by deﬁnition δ(W) and δ(V ) are solutions of the inner
maximization for the adversarial training, in other words,"
N,0.5403659849300323,"yf(x + δ(W); W) ≤yf(x + δ(V ); W)
(80)
yf(x + δ(V ); V ) ≤yf(x + δ(W); V )
(81)"
N,0.5414424111948332,"because then we will have ℓ(x + δ(W); W) > ℓ(x + δ(V ); W) since ℓis non-increasing
under Assumption 1. As a result,"
N,0.542518837459634,"yf(x + δ(W); W) −yf(x + δ(V ); V ) ≤yf(x + δ(V ); W) −yf(x + δ(V ); V )
≤LW ∥W −V ∥
(82)"
N,0.5435952637244349,"for yf(x + δ(W); W) > yf(x + δ(V ); V ). The result for yf(x + δ(W); W) < yf(x +
δ(V ); V ) is similar."
N,0.5446716899892358,"• if ∀i : yif(xi; W) have locally Lipschitz gradients, then so do yif(xi + δi(W); W)."
N,0.5457481162540366,Published as a conference paper at ICLR 2022
N,0.5468245425188375,"Proof. For ﬁxed x, suppose y∇f(W) is locally Lipschitz on Y , then for each W ⊂Y there
is a ZW containing W such that y∇f(W), δ(W) are Lipschitz on ZW , for W, V ⊂ZW :"
N,0.5479009687836384,"∥y∇f(x; W) −y∇f(x; V )∥≤LW ∥W −V ∥
(83)
∥y∇f(x + δ(W); W) −y∇f(x + δ(V ); W)∥≤LW x∥δ(W) −δ(V )∥
≤LW xLW δ∥W −V ∥
(84) Then"
N,0.5489773950484392,"∥yi∇f(xi + δi(W); W) −yi∇f(xi + δi(V ); V )∥
≤∥yi∇f(xi + δi(W); W) −yi∇f(xi + δi(W); V )∥
+ ∥yi∇f(xi + δi(W); V ) −yi∇f(xi + δi(V ); V )∥
≤(LW + LW xLW δ) ∥W −V ∥.
(85)"
N,0.55005382131324,"B.1.1
PROOF OF LEMMA 3"
N,0.5511302475780409,"Proof. The proof can be done for multi-ck-homogeneous networks as deﬁned in Lemma 9. We ﬁrst
note that"
N,0.5522066738428417,"∇xf(x; ρ1W1, . . . , ρLWL) = L
Y"
N,0.5532831001076426,"k=1
ρck
k ∇xf(x; W1, . . . , WL)
(86)"
N,0.5543595263724435,"by taking derivatives with respect to x on both sides of Eq. (74). Therefore ∇xf(x; W1, . . . , WL) is
also positive homogeneous. Under Assumption 1, we prove Lemma 3 in the following cases:"
N,0.5554359526372443,1. ℓ2-FGM perturbation. This perturbations is taken as
N,0.5565123789020452,"δFGM(W) = ϵy˜ℓ′∇xf(x; W1, . . . , WL)
y˜ℓ′∇xf(x; W1, . . . , WL)

= −ϵy∇xf(x; W1, . . . , WL)"
N,0.5575888051668461,"∥∇xf(x; W1, . . . , WL)∥
(87)"
N,0.5586652314316469,"because ˜ℓ′ < 0. Invoking Eq. (86), we know that"
N,0.5597416576964478,"δFGM(ρ1W1, . . . , ρLWL) = −ϵy QL
k=1 ρck
k ∇xf(x; W1, . . . , WL)
QL
k=1 ρck
k ∥∇xf(x; W1, . . . , WL)∥"
N,0.5608180839612487,"= −ϵy∇xf(x; W1, . . . , WL)"
N,0.5618945102260495,"∥∇xf(x; W1, . . . , WL)∥
= δFGM(W1, . . . , WL).
(88)"
N,0.5629709364908504,2. FGSM perturbation.
N,0.5640473627556513,"δFGSM(W) = ϵsgn

y˜ℓ′∇xf(x; W1, . . . , WL)

,
(89)"
N,0.5651237890204521,where sgn() denotes the sign function. Then we have
N,0.566200215285253,"δFGSM(ρ1W1, . . . , ρLWL) = ϵsgn "
N,0.5672766415500539,"y˜ℓ′
L
Y"
N,0.5683530678148547,"k=1
ρck
k ∇xf(x; W1, . . . , WL) !"
N,0.5694294940796556,= δFGSM(W)
N,0.5705059203444564,"(90)
since ρk > 0 for any k thus will not affect the sign operation."
N,0.5715823466092572,"3. ℓ2 and ℓ∞-PGD perturbation. We only prove the case for ℓ2-PGD and the case for ℓ∞-PGD
is similar."
N,0.5726587728740581,"δj+1
PGD(W) = PB(0,ϵ)"
N,0.573735199138859,"
δj
PGD(W) −ξ y∇xf(x; W1, . . . , WL)"
N,0.5748116254036598,"∥∇xf(x; W1, . . . , WL)∥"
N,0.5758880516684607,"
,
(91)"
N,0.5769644779332616,Published as a conference paper at ICLR 2022
N,0.5780409041980624,"where PB(0,ϵ) denotes the projection operator, B(0, ϵ) is the perturbation set, j is the step
indices of PGD and ξ is the step size. We prove by induction. For j = 0, we have"
N,0.5791173304628633,"δ1
PGD(ρ1W1, . . . , ρLWL) = PB(0,ϵ)"
N,0.5801937567276642,"
−ξ y∇xf(x; W1, . . . , WL)"
N,0.581270182992465,"∥∇xf(x; W1, . . . , WL)∥ "
N,0.5823466092572659,"= δ1
PGD(W1, . . . , WL).
(92)"
N,0.5834230355220668,"If we have δj
PGD(ρ1W1, . . . , ρLWL) = δj
PGD(W1, . . . , WL), then for j + 1, we have"
N,0.5844994617868676,"δj+1
PGD(ρ1W1, . . . , ρLWL) = PB(0,ϵ) """
N,0.5855758880516685,"δj
PGD(ρW) −ξ y QL
k=1 ρk∇xf(x; W1, . . . , WL)
QL
k=1 ρk ∥∇xf(x; W1, . . . , WL)∥ #"
N,0.5866523143164694,"= PB(0,ϵ)"
N,0.5877287405812702,"
δj
PGD(W) −ξ y∇xf(x; W1, . . . , WL)"
N,0.5888051668460711,"∥∇xf(x; W1, . . . , WL)∥ "
N,0.589881593110872,"= δj+1
PGD(W1, . . . , WL).
(93)"
N,0.5909580193756727,"Therefore, these four adversarial perturbations are all scale invariant adversarial perturbations by our
deﬁnition."
N,0.5920344456404736,"B.1.2
PROOF OF LEMMA 4"
N,0.5931108719052745,"Proof. This can be easily proved by noting that, for any multi-ck-homogeneous functions,"
N,0.5941872981700753,"f(xi +δi(ρ1W1, . . . , ρLWL); ρ1W1, . . . , ρLWL) = L
Y"
N,0.5952637244348762,"k=1
ρck
k f(xi +δi(W1, . . . , WL); W1, . . . , WL)"
N,0.596340150699677,"(94)
because δ(W) is a scale invariant adversarial perturbations. Then f(x + δ(W); W) is still a homoge-
neous function and Lemma 9 can be applied to f(x + δ(W); W1, . . . , WL) and we have tr"
N,0.5974165769644779,"∂˜L
∂Wk
Wk ! = 1 n n
X"
N,0.5984930032292788,"i=1
˜ℓ′
iyitr
∂f(xi + δi(W); W1, . . . , WL)"
N,0.5995694294940797,"∂Wk
Wk  = −ck n n
X"
N,0.6006458557588805,"i=1
e−˜γiyif(xi + δi(W); W1, . . . , WL) = −ck n n
X"
N,0.6017222820236814,"i=1
e−˜γi˜γi.
(95)"
N,0.6027987082884823,Taking c1 = · · · = cL gives us Lemma 4.
N,0.6038751345532831,"B.2
DIVERGENCES OF FROBENIUS NORMS OF WEIGHT MATRICES"
N,0.604951560818084,"B.2.1
DIVERGENCES OF ALL LAYERS"
N,0.6060279870828849,"Lemma 11. The Frobenius norms of all layers grow at approximately the same rate along the
trajectory of gradient ﬂow for adversarial training with scale invariant adversarial perturbations for
multi-ck-homogeneous DNNs"
CL,0.6071044133476857,"1
cL"
CL,0.6081808396124866,"d∥WL∥2
F
dt
=
1
cL−1"
CL,0.6092572658772875,"d∥WL−1∥2
F
dt
= · · · = 1 c1"
CL,0.6103336921420882,"d∥W1∥2
F
dt
(96)"
CL,0.6114101184068891,"Remark.
Note that a similar conclusion as that of Lemma 11 exists for gradient ﬂow of the standard
training for multi-1-homogeneous networks Du et al. (2018). We generalize this property to the
adversarial training of multi-ck-homogeneous neural networks. When c1 = · · · = cL, we have that
all layer grow at the same rate."
CL,0.61248654467169,Published as a conference paper at ICLR 2022
CL,0.6135629709364908,"Proof. For any Wk,"
CL,0.6146393972012917,"d∥Wk∥2
F
dt
=
X i,j"
CL,0.6157158234660925,"d

Wij;kW T
ji;k
"
CL,0.6167922497308934,"dt
= −2
X i,j"
CL,0.6178686759956943,"∂˜L
∂Wij;k !T"
CL,0.6189451022604952,"W T
ji;k = 2ck n n
X"
CL,0.620021528525296,"i=1
e−˜γi˜γi,
(97)"
CL,0.6210979547900969,where we use dW
CL,0.6221743810548978,"dt = −

∂˜
L
∂Wk"
CL,0.6232508073196986,"T
and Lemma 4. One can then immediately notice that the above
equation does not depend on any speciﬁc k, thus we have"
CL,0.6243272335844995,"1
cL"
CL,0.6254036598493004,"d∥WL∥2
F
dt
=
1
cL−1"
CL,0.6264800861141012,"d∥WL−1∥2
F
dt
= · · · = 1 c1"
CL,0.6275565123789021,"d∥W1∥2
F
dt
.
(98)"
CL,0.628632938643703,"B.2.2
PROOF OF THEOREM 3"
CL,0.6297093649085038,We prove this theorem by exploring the time evolution of ρ.
CL,0.6307857911733046,"Proof. We start from a multi-ck-homogeneous function then take c1 = · · · = cL. Recalling
f(x; W) = ρf(x; c
W) where c
Wk = Wk/ ∥Wk∥F and ρ = L
Y"
CL,0.6318622174381054,"k=1
ρck
k = L
Y"
CL,0.6329386437029063,"k=1
∥Wk∥ck
F ,
(99)"
CL,0.6340150699677072,"the adversarially trained predictor with scale invariant adversarial perturbation is still homogeneous
(see Eq. (94)):
f (x + δ(W); W) = ρf

x + δ(c
W); c
W

.
(100)"
CL,0.635091496232508,"For ρk = ∥Wk∥F , we have dρk"
CL,0.6361679224973089,"dt = ck nρk n
X"
CL,0.6372443487621098,"i=1
e−˜γi˜γi
(101)"
CL,0.6383207750269106,"by invoking Eq. (97). The dynamical evolution of ρ will then be dρ dt = L
X"
CL,0.6393972012917115,"k=1
ρc1
1 · · · dρck
k
dt · · · ρcL
L = L
X k=1"
CL,0.6404736275565124,"c2
kρ2 ρ2
k"
N,0.6415500538213132,"1
n X"
N,0.6426264800861141,"i
e−˜γiˆγi !"
N,0.643702906350915,".
(102)"
N,0.6447793326157158,"Let t0 denote the time such that all worst case adversarial examples are correctly classiﬁed. We study
the trends for ρ after t0. On one hand, the empirical adversarial loss does not increase: −d ˜L"
N,0.6458557588805167,"dt = −∂˜L ∂W
dW"
N,0.6469321851453176,"dt =

dW dt  2"
N,0.6480086114101185,"2
≥0.
(103)"
N,0.6490850376749193,"On the other hand, recalling our deﬁnition of normalized adversarial margin, let"
N,0.6501614639397201,"m = arg
min
i∈{1,...,n} ˜γi = arg
min
i∈{1,...,n} ρˆγi
(104)"
N,0.6512378902045209,"and note the deﬁnition of t0, we have"
N,0.6523143164693218,e−˜γm ≤1 n X
N,0.6533907427341227,"i
e−˜γi = ˜L =⇒˜γm ≥ln
 1 ˜L(t)"
N,0.6544671689989235,"
≥ln

1
˜L(t0)"
N,0.6555435952637244,"
> 0
(105)"
N,0.6566200215285253,"because ˜L does not increase and ˜L(t) ≤˜L(t0) < 1. This also implies that ˜γm, thus ˜γi for all
i ∈{1, . . . , n}, can not be arbitrarily close to 0. Otherwise one would conclude that ˜L(t) may
be arbitrarily close to 1 which is a contradiction. Therefore we can immediately conclude that
dρ/dt > 0 in Eq. (102), which implies that ρ may diverge as t →∞. To have a clearer view"
N,0.6576964477933261,Published as a conference paper at ICLR 2022
N,0.658772874058127,"about the convergence property of ρ, we study the following relation regarding the time evolution of
eρyif(xi+δi(c
W )) after t0: X i"
N,0.6598493003229279,"deρyif(xi+δi(c
W );c
W )"
N,0.6609257265877287,"dt
=
X"
N,0.6620021528525296,"i
eρˆγiˆγi
dρ"
N,0.6630785791173305,"dt
|
{z
}
(a) +
X"
N,0.6641550053821313,"i
eρˆγiρyi
df(xi + δi(c
W); c
W)
dt
|
{z
}
(b)"
N,0.6652314316469322,".
(106)"
N,0.6663078579117331,"The term (a) can be computed by invoking Eq. (102) (a) = L
X k=1"
N,0.667384284176534,"c2
kρ2 nρ2
k X"
N,0.6684607104413348,"i
eρˆγiˆγi  X"
N,0.6695371367061357,"j
e−ρˆγj ˆγj   = L
X k=1"
N,0.6706135629709364,"c2
kρ2 nρ2
k  
n
X"
N,0.6716899892357373,"i=1
ˆγ2
i + 1 2 n
X"
N,0.6727664155005382,"i̸=j
ˆγiˆγj

eρ(ˆγi−ˆγj) + e−ρ(ˆγi−ˆγj)
  ≥ L
X k=1"
N,0.673842841765339,"c2
kρ2 nρ2
k n
X"
N,0.6749192680301399,"i=1
ˆγi !2 ≥ L
X k=1 ρ2ζ2"
N,0.6759956942949408,"nρ2
k
,
(107)"
N,0.6770721205597416,where the ﬁrst inequality follows from x + 1/x ≥2 for x ≥0 and we denote the minimum of P
N,0.6781485468245425,"i ˆγi
for t ≥t0 as ζ > 0. The existence of such ζ has been discussed below Eq. (105)."
N,0.6792249730893434,"The term (b) needs more analysis. For any ﬁxed adversarial example xi + δ(c
W), we have"
N,0.6803013993541442,"yi
df(xi + δi(c
W); c
W)
dt
= yi L
X"
N,0.6813778256189451,"k=1
tr"
N,0.682454251883746,"∂f(xi + δi(c
W); c
W) ∂c
Wk"
N,0.6835306781485468,"dc
Wk
dt !"
N,0.6846071044133477,",
(108)"
N,0.6856835306781486,"where dc
Wk/dt can be computed as follows"
N,0.6867599569429494,"dc
Wjl;k"
N,0.6878363832077503,"dt
=
X m,n"
N,0.6889128094725512,"∂c
Wjl;k
∂Wmn;k"
N,0.689989235737352,"dWmn;k dt =
X m,n"
N,0.6910656620021528,"∂
∂Wmn;k "
N,0.6921420882669537,"
Wjl;k
q"
N,0.6932185145317545,"tr
 
WkW T
k
 "
N,0.6942949407965554,dWmn;k dt = 1 ρk
N,0.6953713670613563,dWjl;k
N,0.6964477933261571,"dt
−
c
Wjl;k nρk n
X"
N,0.697524219590958,"i=1
yie−˜γi˜γi !"
N,0.6986006458557589,".
(109)"
N,0.6996770721205597,"On the other hand, we note that dWk"
N,0.7007534983853606,"dt
= −
X i"
N,0.7018299246501615,"∂˜L
∂f(xi + δi(W); W)"
N,0.7029063509149623,∂f(xi + δi(W); W) ∂Wk T
N,0.7039827771797632,"=
ρ
nρk X"
N,0.7050592034445641,"j
yje−˜γj
 
∂f(xj + δj(c
W); c
W) ∂c
Wk !T (110)"
N,0.7061356297093649,because ∂f(W )
N,0.7072120559741658,"∂Wk
=
ρ
ρk
∂f(c
W )
∂c
Wk . Combing Eq. (109) and Eq. (110), we have"
N,0.7082884822389667,"dc
Wk
dt
=
ρ
nρ2
k X"
N,0.7093649085037675,"j
yje−˜γj  "
N,0.7104413347685683,"∂f(xj + δj(c
W); c
W) ∂c
Wk !T"
N,0.7115177610333692,"−c
Wkf(xj + δj(c
W); c
W) "
N,0.71259418729817,".
(111)"
N,0.7136706135629709,Therefore v  
N,0.7147470398277718,"dc
Wk
dt !T "
N,0.7158234660925726,"=
ρ
nρ2
k X"
N,0.7168998923573735,"j
yje−˜γjAkv"
N,0.7179763186221744,"∂f(xj + δj(c
W); c
W) ∂c
Wk !"
N,0.7190527448869752,",
(112)"
N,0.7201291711517761,Published as a conference paper at ICLR 2022 where
N,0.721205597416577,"Ak = I −v
 
W T
k

v
 
W T
k
T"
N,0.7222820236813778,"∥Wk∥2
F
= I −v

c
W T
k

v

c
W T
k
T
(113)"
N,0.7233584499461787,can be seen as a projector operator. Putting Eq. (112) into Eq. (108) will give us the expression
N,0.7244348762109796,"yi
df(xi + δi(c
W); c
W)
dt
= L
X k=1"
N,0.7255113024757804,"ρ
nρ2
k X"
N,0.7265877287405813,"j
e−˜γjv"
N,0.7276641550053822,"∂ˆγj(c
W) ∂c
Wk !T"
N,0.728740581270183,"AT
k v"
N,0.7298170075349839,"∂ˆγi(c
W) ∂c
Wk !"
N,0.7308934337997847,".
(114)"
N,0.7319698600645855,"Now the term (b) in Eq. (106) is, by putting Eq. (114) back into it, (b) = L
X k=1 ρ2 nρ2
k X"
N,0.7330462863293864,"i
eρˆγi  X"
N,0.7341227125941873,"j
e−ρˆγjv
 ∂ˆγj ∂c
Wk"
N,0.7351991388589881,"T
AT
k v
 ∂ˆγi ∂c
Wk   = 1 2 L
X k=1 ρ2 nρ2
k n
X"
N,0.736275565123789,"i,j=1
v
 ∂ˆγi ∂c
Wk"
N,0.7373519913885899,"T
AT
k Akv
 ∂ˆγj ∂c
Wk"
N,0.7384284176533907," h
eρ(ˆγi−ˆγj) + e−ρ(ˆγi−ˆγj)i"
N,0.7395048439181916,"|
{z
}
(c)"
N,0.7405812701829925,".
(115)"
N,0.7416576964477933,"Rearranging eρ(ˆγi−ˆγj) + e−ρ(ˆγi−ˆγj) as follows
eρ(ˆγi−ˆγj) + e−ρ(ˆγi−ˆγj) = eρ(ˆγi+ˆγj) + e−ρ(ˆγi+ˆγj) −(eρˆγj −e−ρˆγj)(eρˆγi −e−ρˆγi)
will allow us to rewrite (c) as (c) =  X"
N,0.7427341227125942,"i
eρˆγiAkv
 ∂ˆγi ∂c
Wk  2 2
+  X"
N,0.7438105489773951,"i
e−ρˆγiAkv
 ∂ˆγi ∂c
Wk  2 2 −  X i"
N,0.7448869752421959," 
eρˆγi −e−ρˆγi
Akv
 ∂ˆγi ∂c
Wk  2 2 =  X"
N,0.7459634015069968,"i
eρˆγiAkv
 ∂ˆγi ∂c
Wk  2 2
+  X"
N,0.7470398277717977,"i
e−ρˆγiAkv
 ∂ˆγi ∂c
Wk  2 2 −  X"
N,0.7481162540365985,"i
eρˆγiAkv
 ∂ˆγi ∂c
Wk 
−
X"
N,0.7491926803013994,"i
e−ρˆγiAkv
 ∂ˆγi ∂c
Wk  2"
N,0.7502691065662002,"2
≥0.
(116)
Combing Eq. (107) and Eq. (116) with Eq. (106), we will have"
N,0.751345532831001,"1
n X i"
N,0.7524219590958019,"deρyif(xi+δi(c
W );c
W ) dt
≥ L
X k=1"
N,0.7534983853606028,ρ2(t)ζ2
N,0.7545748116254036,"n2ρ2
k(t).
(117)"
N,0.7556512378902045,"According to Eq. (101), we know that ∀k : dρk/dt > 0 after t0, therefore ∀k : ρ2/ρ2
k is lower
bounded by its value at t0 for t ∈[t0, ∞) because d
dt ρ2 ρ2
k 
= d dt "
N,0.7567276641550054,"ρ2ck−2
k L
Y"
N,0.7578040904198062,"k′̸=k
ρ2ck′
k′  "
N,0.7588805166846071,"= 2(ck −1)ρ2ck−3
k
˙ρk L
Y"
N,0.759956942949408,"k′̸=k
ρ2ck′
k′
+ ρ2ck−2
k L
X"
N,0.7610333692142088,"k′̸=k
2ck′ρ2ck′−1
k′
˙ρk′  
L
Y"
N,0.7621097954790097,"k′′̸=k′,k
ρ2ck′′
k′′  > 0,"
N,0.7631862217438106,"(118)
where ˙ρk denotes dρk/dt > 0 for any k due to Eq. (101). Integrating both sides of Eq. (117) from
t0 to t gives us"
N,0.7642626480086114,"1
n X i"
N,0.7653390742734123,"
eρ(t)yif(xi+δi(c
W );c
W (t)) −eρ(t0)yif(xi+δi(c
W );c
W (t0))
≥ L
X k=1 Z t t0"
N,0.7664155005382132,ρ2(τ)ζ2
N,0.767491926803014,"n2ρ2
k(τ)dτ ≥ L
X k=1"
N,0.7685683530678149,ρ2(t0)ζ2
N,0.7696447793326158,"n2ρ2
k(t0)(t −t0),
(119)"
N,0.7707212055974165,Published as a conference paper at ICLR 2022
N,0.7717976318622174,"where the last inequality follows from Eq. (118). Let t →∞in the above equation, we will have"
N,0.7728740581270183,"ρ = Ω(ln t) →∞
(120)"
N,0.7739504843918191,"since yif(xi + δi(c
W); c
W) is upper bounded, which can be easily deduced considering that all
weights have norm 1, and ζ is lower bounded as discussed before. Taking c1 = · · · = cL gives us the
conclusion."
N,0.77502691065662,"Note that ρ = ρc1
1 · · · ρcL
L and ρk for any k grows at the same rate for multi-c-homogeneous functions
(Lemma 11), we immediately have
Corollary 1. ∀k ∈{1, . . . , L} : ρk →∞as t →∞."
N,0.7761033369214209,"B.3
CONVERGENCE TO KKT POINT"
N,0.7771797631862217,"B.3.1
PROOF OF THEOREM 4"
N,0.7782561894510226,"Proof. Most calculations needed for the proof of this theorem have been done in Section B.2.2.
According to Eq. (114), we have X"
N,0.7793326157158235,"i
e−˜γiyi
df(x + δi(c
W); c
W)
dt
∝ L
X k=1"
N,0.7804090419806243,"1
ρ2
k X"
N,0.7814854682454252,"i
e−˜γi  X"
N,0.7825618945102261,"j
e−˜γjv"
N,0.7836383207750269,"∂ˆγj(c
W) ∂c
Wk !T"
N,0.7847147470398278,"AT
k v"
N,0.7857911733046287,"∂ˆγi(c
W) ∂c
Wk !  = L
X k=1"
N,0.7868675995694295,"1
ρ2
k  X"
N,0.7879440258342304,"i
e−˜γiAkv
 ∂ˆγi ∂c
Wk  2"
N,0.7890204520990313,"2
≥0.
(121)"
N,0.790096878363832,"B.3.2
PROOF OF THEOREM 5"
N,0.7911733046286329,The KKT conditions for the optimization problem Eq. (19) are
N,0.7922497308934338,"∀k :
1
2
∂ρ2
k
∂Wk
− n
X"
N,0.7933261571582346,"i=1
λi
∂˜γi
∂Wk
= 0
(122)"
N,0.7944025834230355,"∀i ∈{1, . . . , n} : λi(˜γi −1) = 0.
(123)"
N,0.7954790096878364,"Following Dutta et al. (2013); Lyu & Li (2020), we deﬁne the approximate KKT point in our settings
Deﬁnition 3 (Adapted from Deﬁnition C.4 in Lyu & Li (2020)). The approximate KKT points of the
optimization problem (19) are those feasible points which satisfy the following condtions: 1. 1"
N,0.7965554359526372,"2
∂∥W ∥2"
N,0.7976318622174381,"∂W
−Pn
i=1 λi
∂˜γi
∂W

2 ≤χ;"
N,0.798708288482239,"2. ∀i ∈{1, . . . , n} : λi(˜γi −1) ≤ξ,"
N,0.7997847147470398,"where χ, ξ > 0 and λi ≥0. These points are said to be (χ, ξ)-approximate KKT points."
N,0.8008611410118407,We now present the proof of Theorem 5.
N,0.8019375672766416,Proof. Let ∥W∥=
N,0.8030139935414424,"v
u
u
t L
X"
N,0.8040904198062433,"k=1
ρ2
k,
(124) V =
W"
N,0.8051668460710442,"˜γ1/K
m
,
(125)"
N,0.806243272335845,"where K = PL
k=1 c = cL. Since"
N,0.8073196986006459,"cf(x; W) = c˜γmf(x; V )
(126)"
N,0.8083961248654468,Published as a conference paper at ICLR 2022
N,0.8094725511302476,according to the deﬁnition of the multi-c-homogeneous functions and
N,0.8105489773950484,"cf(x; W) =

Wk, ∂f(x; W) ∂Wk"
N,0.8116254036598493,", cf(x; V ) =

Vk, ∂f(x; V ) ∂Vk"
N,0.8127018299246501,",
(127)"
N,0.813778256189451,"we have
∂f(x; V )"
N,0.8148546824542519,"∂V
=
1"
N,0.8159311087190527,"˜γ(K−1)/K
m"
N,0.8170075349838536,∂f(x; W)
N,0.8180839612486545,"∂W
.
(128)"
N,0.8191603875134553,"Speciﬁcally, in the following, we will show that V is a (χ, ξ)-approximate KKT points of the
optimization problem (19) with χ, ξ →0 along the training trajectory. Because the homogeneous
property of the network trained with scale invariant adversarial perturbations, it satisﬁes the MFCQ
condition by simply noting that"
N,0.8202368137782562,"∀i ∈{1, . . . , n} : tr
 ∂˜γi"
N,0.8213132400430571,"∂Wk
Wk"
N,0.8223896663078579,"
= c˜γi > 0.
(129)"
N,0.8234660925726588,This makes a point being a KKT point a necessary condition for it to be the optimal solution.
N,0.8245425188374597,"We now show that the limit point of V along the adversarial training trajectory is an (χ, ξ)-approximate
KKT point with χ, ξ →0 starting with the ﬁrst condition. Let λi ="
N,0.8256189451022605,"∂˜L
∂W  !−1"
N,0.8266953713670614,"∥W∥˜γ(K−2)/K
m
e−˜γi,
(130)"
N,0.8277717976318623,"then we have
V −
X"
N,0.8288482238966631,"i
λiyi
∂f(xi + δi(V ); V ) ∂V  2 2 a= W"
N,0.829924650161464,"˜γ1/K
m
−
X i"
N,0.8310010764262648,"λiyi
˜γ(K−1)/K
m"
N,0.8320775026910656,"∂f(xi + δi(W); W) ∂W  2 2 =
2(P"
N,0.8331539289558665,"k ρ2
k) (Q"
N,0.8342303552206674,"k ρ2c
k )1/Kˆγ2/K
m "
N,0.8353067814854682,"
1 −"
N,0.8363832077502691,"D
W, ∂˜
L
∂W
E"
N,0.83745963401507,"∥W∥
 ∂˜
L
∂W

2  
"
N,0.8385360602798708,"≤
2Lρ2
k′"
N,0.8396124865446717,"ρ2
k′′ˆγ2/K
m "
N,0.8406889128094726,"
1 −"
N,0.8417653390742734,"D
W, ∂˜
L
∂W
E"
N,0.8428417653390743,"∥W∥
 ∂˜
L
∂W

2 "
N,0.8439181916038752,"
= χ(t)
(131)"
N,0.844994617868676,"where a follows from (128); k′ = arg maxk ρk and k′′ = arg mink ρk. This is the ﬁrst condition
for the limit point of V being an approximate KKT point. We can further show that χ →0 as t →0
by noting the following alignment phenomenon which was originally observed by Ji & Telgarsky
(2020) and intended for standard training with ﬁxed training examples:"
N,0.8460710441334769,"Lemma 12 (Adapted from Theorem 4.1 in Ji & Telgarsky (2020)). Under Assumption 2 and
Assumption 4 for exponential loss and homogeneous deep neural networks, along the trajectory of
adversarial training with scale invariant adversarial perturbations, we have"
N,0.8471474703982778,"∀k ∈{1, . . . , k} : lim
t→∞"
N,0.8482238966630786,"D
W(t), ∂˜
L
∂W (t)
E"
N,0.8493003229278795,"∥W∥
 ∂˜
L
∂W (t)

= −1.
(132)"
N,0.8503767491926802,"The extension of Theorem 4.1 in Ji & Telgarsky (2020), which was intended for ﬁxed training
examples, to our settings is because, by our construction for adversarial training with scale invariant
adversarial perturbations, the adversarial training margin are locally Lipschitz with locally Lipschitz
gradients and the prediction function f(x+δ(W); W) is positively (multi)homogeneous with respect
to W(see Lemma 10 and Eq. (4)). Invoking Lemma 12 and considering that ˆγm can not be arbitrarily
close to 08 and ∀k : ρk →∞at the same rate according to Lemma 11 thus ρk′"
N,0.8514531754574811,"ρk′′ can not be inﬁnite,
we have,
lim
t→∞χ(t) →0.
(133)"
N,0.852529601722282,"8See Eq. (105). In fact, it keeps increasing after some time according to Theorem 4."
N,0.8536060279870828,Published as a conference paper at ICLR 2022
N,0.8546824542518837,"On the other hand, the second condition for the limit point of V being an approximate KKT point is n
X"
N,0.8557588805166846,"i=1
λi( ˜γi"
N,0.8568353067814855,"˜γm
−1) = n
X i=1 ∥W∥"
N,0.8579117330462863,"˜γ2/K
m"
N,0.8589881593110872,"∂ˆL
∂W F !−1"
N,0.860064585575888,"e−˜γi(˜γi −˜γm) a
≤
P"
N,0.8611410118406889,"k ρ2
k
ρ2/Kˆγ2/K
m n
X i=1"
N,0.8622174381054898,"e−˜γi(˜γi −˜γm)
D
W, ∂˜
L
∂W
E"
N,0.8632938643702907,"b
≤
P
k ρ2
k
ρ2/Kˆγ2/K
m n
X i=1"
N,0.8643702906350915,ne−˜γi(˜γi −˜γm)
N,0.8654467168998924,Kρe−˜γm P
N,0.8665231431646933,"j yjf(xj + δj(c
W); c
W)"
N,0.8675995694294941,"c
≤
n P"
N,0.868675995694295,"k ρ2
k
Kρ1+2/Kˆγ2/K
m n
X"
N,0.8697524219590959,"i=1
e−(˜γi−˜γm) ˜γi −˜γm ˆγm"
N,0.8708288482238966,"d
≤
n P"
N,0.8719052744886975,"k ρ2
k
Keρ1+2/Kˆγ1+2/K
m"
N,0.8729817007534983,"≤
nLρ2
k′"
N,0.8740581270182992,"Keρρ2
k′′ˆγ1+2/K
m
= ξ,
(134)"
N,0.8751345532831001,"where a follows from
D
W, ∂˜
L
∂W
E
≤∥W∥
 ∂˜
L
∂W
; b is due to Lemma 4 and ∀i : ˜γi ≥˜γm; c uses"
N,0.876210979547901,∀i : ˜γi ≥˜γm again; d is due to e−xx is upper bounded by its value at x = 1 for x ≥0. Since
N,0.8772874058127018,"∀k : limt→∞ρk(t) →∞at the same rate, we conclude that limt→∞
ρ2
k′
ρ2
k′′ can not be inﬁnite thus"
N,0.8783638320775027,"limt→∞ξ(t) →0 because ˆγm can not be arbitrarily close to 0 and limt→∞ρ →∞. Therefore, the
limit point of V is an (χ, ξ)-approximate KKT point of the mini-norm problem along the trajectory
of adversarial training with scale invariant adversarial perturbations where limt→∞χ(t), ξ(t) →0.
Restating the theorem in Dutta et al. (2013) regarding the relation between (χ, ξ)-approximate KKT
point and KKT point in our setting:"
N,0.8794402583423035,"Theorem 6 (Theorem 3.6 in Dutta et al. (2013) and Theorem C.4 in Lyu & Li (2020)). Let {xt : t ∈
N} be a sequence of feasible point of the optimization problem 19. xt is an (χt, ξt)-approximate
KKT poiint for all t with two sequences {χt > 0 : t ∈N} and {ξt > 0 : t ∈N} and χt, ξt →0. If
xt →x as t →∞and MFCQ holds at x, then x is a KKT point of this optimization problem."
N,0.8805166846071044,"We then immediately conclude that the limit point of {W(t)/∥W∥: t ≥0} of gradient ﬂow for
the adversarial training objective Eq. (6) is along the direction of a KKT point of the optimization
problem (19)."
N,0.8815931108719053,"B.4
GENERALIZATION TO NON-SMOOTH CASE"
N,0.8826695371367062,"It is not hard to generalize the current analysis to non-smooth case, which will include the deep ReLU
network, because, in our main steps, there are counterparts of our conclusions for non-smooth case
for that sub-differential (Clarke, 1975) is a generalization of gradient. The non-smooth analysis can
be done by ﬁrst replacing the gradient ﬂow equation Eq. (16) with its generalization, dW"
N,0.883745963401507,"dt ∈−∂˜L(W(t))
(135)"
N,0.8848223896663079,"where ∂˜L(W(t)) is the sub-differential. Then one can follow similar procedures as our approach to
make the generalization."
N,0.8858988159311088,"Proof. For simplicity, we only consider multi-1-homogeneous networks here, i.e., c1 = · · · = cL = 1,
which include the deep ReLU neural networks. Lemma 9 can be generalized to non-smooth case
according to Lemma C.1 in Ji & Telgarsky (2020) or Theorem B.2 in Lyu & Li (2020):"
N,0.8869752421959096,"Lemma 13 (Lemma C.1 in Ji & Telgarsky (2020)). Suppose f : Rn →R is locally Lipschitz and
L-positively homogeneous, then for any W ∈Rn and any W ∗∈∂f(W),"
N,0.8880516684607105,"⟨W, W ∗⟩= Lf(W).
(136)"
N,0.8891280947255114,Published as a conference paper at ICLR 2022
N,0.8902045209903121,"The starting point of the proof for Lemma 3 is the homogeneity of f(x; W) and can be easily
promoted to handle the non-smooth case."
N,0.891280947255113,"With generalizations of Lemma 3 and Euler’s theorem on homogeneous functions for non-smooth
case, the proof of Lemma 4 can also be generalized to non-smooth case accordingly because it
is based on the Euler’s theorem on homogeneous functions and Lemma 3 for adversarial training.
Speciﬁcally, it will become
dWk"
N,0.8923573735199138,"dt , Wk = 1 n n
X"
N,0.8934337997847147,"i=1
e−˜γi˜γi.
(137)"
N,0.8945102260495156,"The proof of Theorem 3 adopts Lemma 4 and can be generalized to non-smooth case by combining
the chain rule (Clarke, 1983) for gradient ﬂow to ensure Eq. (103) −d ˜L"
N,0.8955866523143164,"dt = −

∂˜L(W(t)), dW dt"
N,0.8966630785791173,"=

dW dt"
N,0.8977395048439182,"2
≥0
(138)"
N,0.898815931108719,"with similar approach as in Lemma B.9 in Ji & Telgarsky (2020) or Lemma 5.2 in Davis et al. (2018).
Then generalizing the rest of the proof of Theorem 3 is straightforward."
N,0.8998923573735199,"Finally, the generalization of Theorem 5 to non-smooth case can also be done because it adopts
Theorem 3, Lemma 4, the chain rule, and the gradient alignments Lemma 12 which holds for
non-smooth case."
N,0.9009687836383208,"B.5
DEEP LINEAR NETWORKS ADVERSARIALLY TRAINED WITH ℓ∞PERTURBATIONS"
N,0.9020452099031216,"Following the settings of Section 3.1, the ℓ∞-perturbation for deep linear networks f(x; W) =
WL · · · W1x can be given exactly as"
N,0.9031216361679225,"δ(W(t)) = −ϵyisgn(WΠ(t)).
(139)"
N,0.9041980624327234,"According to Theorem 5, the adversarial training solution is formulated from solving the following
optimization problem: max 1"
N,0.9052744886975242,"2∥W∥2
2
(140)"
N,0.9063509149623251,"s.t.
min
i∈{1,...,n} yiWΠxi −ϵ∥WΠ∥1 ≥1.
(141)"
N,0.907427341227126,"It can be seen that this optimization problem, a mixed-norm optimization problem, will have different
solutions when compared to the margin-maximization problem of the original data, which will lead
to a different decision boundary distinguishing adversarial training methods from the standard ones."
N,0.9085037674919269,"Besides, the constraint on ϵ (Eq. (8) for ℓ2 perturbation) should also be changed accordingly such
that the adversarial training examples are still linear separable:"
N,0.9095801937567277,"ϵ ≤γm,1
(142)"
N,0.9106566200215285,"where γm,1 is the max-ℓ1-norm margin:"
N,0.9117330462863293,"γm,1 = max
∥u∥1=1
min
i∈{1,...,n} yiuxi.
(143)"
N,0.9128094725511302,"C
SUPPLEMENTED EXPERIMENTS"
N,0.9138858988159311,"C.1
ADVERSARIAL TRAINING WITH DIFFERENT LOSS FUNCTIONS, ARCHITECTURES AND
VARYING PERTURBATION SIZES"
N,0.9149623250807319,"We present the results for adversarial training with different loss functions, architectures and varying
perturbation sizes in this section to further verify our theorems and assumptions. We use two different
models: one is a 3-layer neural network with the same architecture as that in the Section 4; the other
is a CNN with architecture
Input-Conv-ReLU-Pooling-Conv-ReLU-Pooling-FC-FC,
where Conv stands for convolution layer, FC stands for fully connected layer and Pooling is
max-pooling layer."
N,0.9160387513455328,Published as a conference paper at ICLR 2022
N,0.9171151776103337,"0
500
1000
1500
2000
2500
epochs −0.04 −0.03 −0.02 −0.01 0.00 0.01 0.02 0.03 0.04"
N,0.9181916038751345,adversarial normalized margin
N,0.9192680301399354,"FGSM, ε = 8/255
FGSM, ε = 12/255
FGSM, ε = 24/255
PGD, ε = 8/255
PGD, ε = 12/255
PGD, ε = 24/255"
N,0.9203444564047363,(a) Adversarial normalized margins for varying perturbation sizes
N,0.9214208826695371,"0
100
200
300
400
500
epochs 86 88 90 92 94 96 98 100"
N,0.922497308934338,adversarial training accuracy
N,0.9235737351991389,"FGSM, ε = 8/255
FGSM, ε = 12/255
FGSM, ε = 24/255
PGD, ε = 8/255
PGD, ε = 12/255
PGD, ε = 24/255"
N,0.9246501614639397,(b) Adversarial training accuracy for varying perturbation sizes
N,0.9257265877287406,"Figure 2: Adversarial training for the 3-layer neural network on MNIST to perform binary classi-
ﬁcation. The perturbations used for training are FGSM and ℓ∞-PGD perturbations with varying
perturbation sizes ϵ = 8/255, 12/255 and 24/255. The adversarial normalized margins and adversar-
ial training accuracies are evaluated with the corresponding perturbations used for training."
N,0.9268030139935415,"For the 3-layer network, we conduct adversarial training using both FGSM and ℓ∞-PGD perturbations
with perturbation sizes ϵ = 8/255, 12/255 and 24/255 to perform binary classiﬁcation where we
choose all examples with labels ”3” and ”8” from MNIST to be our dataset. The loss function is
logistic loss. Fig. 2(a) shows that for both FGSM and ℓ∞-PGD perturbations with varying perturbation
sizes, the adversarial normalized margins during training keep increasing after some step. Fig. 2(b)
reveals that the adversarial training accuracy can achieve 100% (after ∼200 epochs), which supports
Assumption 4."
N,0.9278794402583423,"For the multi-classiﬁcation task, let f(x; W)[j] denote the j-th output. Then the margin for an
adversarial example xi + δi corresponding to the original example xi is deﬁned by"
N,0.9289558665231432,"˜γi = f(xi + δi; W)[yi] −max
j̸=yi f(xi + δi; W)[j],"
N,0.930032292787944,Published as a conference paper at ICLR 2022
N,0.9311087190527448,"0
100
200
300
400
500
epochs −0.8 −0.6 −0.4 −0.2 0.0 0.2"
N,0.9321851453175457,adversarial normalized margin
N,0.9332615715823466,"FGSM, ε = 16/255
FGSM, ε = 32/255
PGD, ε = 16/255
PGD, ε = 32/255"
N,0.9343379978471474,(a) Adversarial normalized margins for varying perturbation sizes
N,0.9354144241119483,"0
100
200
300
400
500
epochs 90 92 94 96 98 100"
N,0.9364908503767492,adversarial training accuracy
N,0.93756727664155,"FGSM, ε = 16/255
FGSM, ε = 32/255
PGD, ε = 16/255
PGD, ε = 32/255"
N,0.9386437029063509,(b) Adversarial normalized margins for varying perturbation sizes
N,0.9397201291711518,"Figure 3: Adversarial training for the CNN on all training examples of MNIST to perform multi-
classiﬁcation. The perturbations used for training are FGSM and ℓ∞-PGD perturbations with varying
perturbation sizes ϵ = 16/255 and 32/255. The adversarial normalized margins and adversarial
training accuracies are evaluated with the corresponding perturbations used for training."
N,0.9407965554359526,Published as a conference paper at ICLR 2022
N,0.9418729817007535,and the margin for the adversarial data is deﬁned to be
N,0.9429494079655544,"˜γm =
min
i∈{1,...,n} ˜γi."
N,0.9440258342303552,"Then for the CNN, we conduct adversarial training using both FGSM and ℓ∞-PGD perturbations with
perturbation sizes ϵ = 16/255 and 32/255 to perform multi-classiﬁcation on all training examples of
MNIST to verify our claims and assumptions when the perturbation sizes are large. The loss function
is cross-entropy loss. It can be seen from Fig. 3(a) that the adversarial normalized margins keep
increasing after about 250 epochs, i.e., after the model ﬁts all adversarial training examples, for all
adversarial perturbations. Fig. 3(b) clearly shows that the adversarial training accuracy can reach
100% for all perturbation types and sizes, even the large size ϵ = 32/255."
N,0.9451022604951561,"These experiments show that adversarial normalized margins are different whenever perturbation
types or sizes are different. All models adversarially trained with all perturbations and loss functions
can achieve 100% adversarial training accuracy, which veriﬁes Assumption 4. Moreover, the trends
that adversarial normalized margins all keep increasing are clear, even long after the separation of
adversarial training examples. These results well support the claims of Theorem 5."
N,0.946178686759957,"C.2
EXPERIMENTS ABOUT ℓ2-FGM PERTURBATIONS FOR BINARY CLASSIFICATION"
N,0.9472551130247578,"0
100
200
300
400
epochs −3.0 −2.5 −2.0 −1.5 −1.0 −0.5 0.0 0.5 1.0"
N,0.9483315392895587,normalized margin for AEs
N,0.9494079655543596,"adversarial training
standard training"
N,0.9504843918191603,(a) ℓ2-FGM adversarial examples
N,0.9515608180839612,"0
100
200
300
400
epochs 50 60 70 80 90 100"
N,0.9526372443487621,adversarial training accuracy
N,0.9537136706135629,(b) Adversarial training accuracy
N,0.9547900968783638,"Figure 4: Adversarial training for the 3-layer neural network on MNIST. The attack method is
ℓ2-FGM with ϵ = 8. (a) Adversarial normalized margin for ℓ2-FGM adversarial examples during
adversarial training and standard training. (b) Adversarial training accuracy for ℓ2-FGM adversarial
training."
N,0.9558665231431647,"In the MNIST dataset, we adversarially trained a 3-layer neural network with the same architecture as
that in Section 4 using SGD with constant learning rate and batch-size 64. To have a clear view about
the difference between the implicit bias for standard training and adversarial training, the adversarial
perturbation used for training is ℓ2-FGM perturbation with ϵ = 8 since even the standard model can
ﬁt ℓ2-FGM adversarial examples easily in the MNIST dataset when the perturbation size ϵ is not
large. As a comparison, we also standardly trained a model with the same architecture to evaluate the
normalized margin for adversarial examples by attacking it with ℓ2-FGM during its training process."
N,0.9569429494079655,"As showed in Fig.4(b), the adversarilly training accuracy reaches 100% very quickly, which supports
Assumption 4. It can be seen in Fig.4(a) that the adversarial normalized margin keeps increasing dur-
ing the adversarial training process while the standardly trained model maintains a lower adversarial
normalized margin."
N,0.9580193756727664,"C.2.1
EXPERIMENTS ABOUT EXPONENTIAL LOSS"
N,0.9590958019375673,"For completeness, we also conduct experiment about the binary classiﬁcation where the model is
adversarially trained with exponential loss (Fig.5) for ℓ2-FGM perturbations. We select the examples
with label ”2” and ”8” from MNIST and adversarially trained a 2-layer neural network using SGD
with constant learning rate 10−5. The hidden layer is of size 10000 and the activation function is
ReLU. The normalized margin for adversarial data is deﬁned exactly as in Section 3.2. The adversarial"
N,0.9601722282023681,Published as a conference paper at ICLR 2022
N,0.961248654467169,"0
2000
4000
6000
8000
10000
epochs"
N,0.9623250807319699,−0.0030
N,0.9634015069967707,−0.0025
N,0.9644779332615716,−0.0020
N,0.9655543595263725,−0.0015
N,0.9666307857911733,−0.0010
N,0.9677072120559742,−0.0005
N,0.9687836383207751,0.0000
N,0.9698600645855759,0.0005 
N,0.9709364908503767,adversarial training
N,0.9720129171151776,"Figure 5: Adversarial normalized margin during adversarial training for binary classiﬁcation. The
loss function during training is taken as exponential loss."
N,0.9730893433799784,"0
5000
10000
15000
20000
25000
30000
epochs 0.6 0.8 1.0 1.2 1.4 1.6 1.8"
N,0.9741657696447793,"||W1||F
||W2||F
||W3||F
||W4||F"
N,0.9752421959095802,(a) 4-layer deep linear network
N,0.976318622174381,"0
5000
10000
15000
20000
25000
30000
epochs 0.6 0.8 1.0 1.2 1.4 1.6"
N,0.9773950484391819,"||W1||F
||W2||F
||W3||F"
N,0.9784714747039828,(b) 3-layer deep linear network
N,0.9795479009687836,"0
5000
10000
15000
20000
25000
30000
epochs 0.6 0.8 1.0 1.2 1.4 1.6 1.8"
N,0.9806243272335845,"||W1||F
||W2||F"
N,0.9817007534983854,(c) 2-layer deep linear network
N,0.9827771797631862,"Figure 6: Frobenius norms for weights of deep linear networks with different layers during adversarial
training. The perturbations are ℓ2 perturbations."
N,0.9838536060279871,"perturbations during the adversarial training are given by ℓ2-FGM with ϵ = 3. As showed in Fig.5,
although extremely slowly, the normalized margin for adversarial data gradually increases during the
adversarial training."
N,0.984930032292788,"C.3
DIVERGENCE OF WEIGHT NORMS FOR DEEP LINEAR NETWORKS DURING ADVERSARIAL
TRAINING"
N,0.9860064585575888,"We conduct adversarial training of deep linear networks with ℓ2 perturbation on a linearly separable
dataset. The loss function is logistic loss. Speciﬁcally, we use deep linear networks with layers 2, 3
and 4, respectively. Since the divergence is slow (∼ln t), it is hard to observe that ∥Wk∥F →∞."
N,0.9870828848223897,Published as a conference paper at ICLR 2022
N,0.9881593110871906,"However, as showed in Fig. 6(a), Fig. 6(b) and Fig. 6(c), the trends that ∥Wk∥F for all layers of deep
linear networks with different layers keep increasing are clear. Furthermore, we can see that the ratios
∥Wk∥2/∥Wk∥F for all layers of deep linear networks with different layers also keep increasing, as
showed by Fig. 7(a), Fig. 7(b) and Fig. 7(c)."
N,0.9892357373519914,"0
5000
10000
15000
20000
25000
30000
epochs 0.65 0.70 0.75 0.80 0.85 0.90 0.95"
N,0.9903121636167922,"1.00
||W1||2/||W1||F
||W2||2/||W2||F
||W3||2/||W3||F
||W4||2/||W4||F"
N,0.9913885898815931,(a) 4-layer deep linear network
N,0.9924650161463939,"0
5000
10000
15000
20000
25000
30000
epochs 0.6 0.7 0.8 0.9 1.0"
N,0.9935414424111948,"||W1||2/||W1||F
||W2||2/||W2||F
||W3||2/||W3||F"
N,0.9946178686759957,(b) 3-layer deep linear network
N,0.9956942949407965,"0
5000
10000
15000
20000
25000
30000
epochs 0.75 0.80 0.85 0.90 0.95 1.00"
N,0.9967707212055974,"||W1||2/||W1||F
||W2||2/||W2||F"
N,0.9978471474703983,(c) 2-layer deep linear network
N,0.9989235737351991,"Figure 7: Ratios of 2-norm and Frobenius norm for weights of deep linear networks with different
layers during adversarial training. The perturbations are ℓ2 perturbations."
