Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002717391304347826,"The Neural Tangent Kernel (NTK), deﬁned as the outer product of the neural
network (NN) Jacobians, ⇥✓(x1, x2) = ⇥"
ABSTRACT,0.005434782608695652,"@f(✓, x1) "" @✓ ⇤⇥"
ABSTRACT,0.008152173913043478,"@f(✓, x2) "" @✓"
ABSTRACT,0.010869565217391304,"⇤T , has
emerged as a central object of study in deep learning. In the inﬁnite width limit,
the NTK can sometimes be computed analytically and is useful for understanding
training and generalization of NN architectures. At ﬁnite widths, the NTK is also
used to better initialize NNs, compare the conditioning across models, perform
architecture search, and do meta-learning. Unfortunately, the ﬁnite width NTK is
notoriously expensive to compute, which severely limits its practical utility.
We perform the ﬁrst in-depth analysis of the compute and memory requirements
for NTK computation in ﬁnite width networks. Leveraging the structure of neu-
ral networks, we further propose two novel algorithms that change the exponent
of the compute and memory requirements of the ﬁnite width NTK, dramatically
improving efﬁciency.
We open-source [github.com/iclr2022anon/fast ﬁnite width ntk] our two algo-
rithms as general-purpose JAX function transformations that apply to any differ-
entiable computation (convolutions, attention, recurrence, etc.) and introduce no
new hyper-parameters."
INTRODUCTION,0.01358695652173913,"1
INTRODUCTION"
INTRODUCTION,0.016304347826086956,"The past few years have seen signiﬁcant progress towards a theoretical foundation for deep learning.
Much of this work has focused on understanding the properties of random functions in high dimen-
sions. One signiﬁcant line of work (Neal, 1994; Lee et al., 2018; Matthews et al., 2018; Novak et al.,
2019; Garriga-Alonso et al., 2019; Hron et al., 2020; Yang, 2019) established that in the limit of inﬁ-
nite width, randomly initialized Neural Networks (NNs) are Gaussian Processes (called the NNGP).
Building on this development, Jacot et al. (2018) showed that in function space the dynamics under
gradient descent could be computed analytically using the so-called Neural Tangent Kernel (NTK)
and Lee et al. (2019) showed that wide neural networks reduce to their linearization in weight space
throughout training. A related set of results (Belkin et al., 2019; Spigler et al., 2019) showed that
the ubiquitous bias-variance decomposition breaks down as high-dimensional models enter the so-
called interpolating regime. Together these results describe learning in the inﬁnite width limit and
help explain the impressive generalization capabilities of NNs."
INTRODUCTION,0.019021739130434784,"Insights from the wide network limit have had signiﬁcant practical impact. The conditioning of the
NTK has been shown to signiﬁcantly impact trainability and generalization in NNs (Schoenholz
et al., 2017; Xiao et al., 2018; 2020). This notion inspired initialization schemes like Fixup (Zhang
et al., 2019), MetaInit (Dauphin & Schoenholz, 2019), and Normalizer Free networks (Brock et al.,
2021a;b) and has enabled efﬁcient neural architecture search (Park et al., 2020; Chen et al., 2021b).
The NTK has additionally given insight into a wide range of phenomena such as: behavior of Gen-
erative Adversarial Networks (Franceschi et al., 2021), neural scaling laws (Bahri et al., 2021), and
neural irradiance ﬁelds (Tancik et al., 2020). Kernel regression using the NTK has further enabled
strong performance on small datasets (Arora et al., 2020), and applications such as dataset distilla-
tion (Nguyen et al., 2020; 2021) and uncertainty prediction (He et al., 2020; Adlam et al., 2020)."
INTRODUCTION,0.021739130434782608,"Despite the signiﬁcant promise of theory based on the NTK, computing the NTK in practice is chal-
lenging. In the inﬁnite width limit, the NTK can sometimes be computed analytically. However, it
remains intractable for many architectures, and ﬁnite width corrections can be important to describe
actual NNs used in practice. The NTK matrix can be computed for ﬁnite width networks as the outer"
INTRODUCTION,0.024456521739130436,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.02717391304347826,"product of Jacobians using forward or reverse mode automatic differentiation (AD),"
INTRODUCTION,0.029891304347826088,"⇥✓(x1, x2)
|
{z
}
O⇥O := ⇥"
INTRODUCTION,0.03260869565217391,"@f(✓, x1) "" @✓ ⇤"
INTRODUCTION,0.035326086956521736,"|
{z
}
O⇥P ⇥"
INTRODUCTION,0.03804347826086957,"@f(✓, x2) "" @✓"
INTRODUCTION,0.04076086956521739,"⇤T
|
{z
}
P⇥O ,
(1)"
INTRODUCTION,0.043478260869565216,"where f is the forward pass NN function producing outputs in RO, ✓2 RP are all trainable parame-
ters, and x1 and x2 are two inputs to the network. If inputs are batches of sizes N1 and N2, the NTK
is an N1O ⇥N2O matrix."
INTRODUCTION,0.04619565217391304,"Unfortunately, evaluating Eq. (1) is often infeasible due to time and memory requirements."
INTRODUCTION,0.04891304347826087,"In this paper, we perform the ﬁrst in-depth analysis of the compute and memory requirements for
the NTK as in Eq. (1). Noting that forward and reverse mode AD are two extremes of a wide
range of AD strategies (Naumann, 2004; 2008), we explore other methods for computing the NTK
leveraging the structure of NNs used in practice. We propose two novel methods for computing
the NTK that exploit different orderings of the computation. We describe the compute and memory
requirements of our techniques in fully-connected (FCN) and convolutional (CNN) settings, and
show that one is asymptotically more efﬁcient in both settings. We compute the NTK over a wide
range of NN architectures and demonstrate that these improvements are robust in practice. We
open-source implementations of both methods as JAX function transformations."
RELATED WORK,0.051630434782608696,"2
RELATED WORK"
RELATED WORK,0.05434782608695652,"The ﬁnite width NTK (denoted as simply NTK throughout this work) has been used extensively
in many recent works, but to our knowledge implementation details and compute costs were rarely
made public. Below we draw comparison to some of these works, but we stress that it only serves as
a sanity check to make sure our contribution is valuable relative to the scale of problems that have
been attempted (none of these works had efﬁcient NTK computation as their central goal)."
RELATED WORK,0.057065217391304345,"In order to compare performance of models based on the NTK and the inﬁnite width NTK, Arora
et al. (2019a, Table 2) compute the NTK of up to 20-layer, 128-channel CNN in a binary CIFAR-2
classiﬁcation setting. In an equivalent setting with the same hardware (NVIDIA V100), we are able
to compute the NTK of a 2048-channel CNN, i.e. a network with at least 256 times more parameters."
RELATED WORK,0.059782608695652176,"To demonstrate the stability of the NTK during training for wide networks, Lee et al. (2019, Figure
S6) compute the NTK of up to 3-layer 212-wide or 1-layer 214-wide FCNs. In the same setting with
the same hardware (NVIDIA V100), we can reach widths of at least 214 and 218 respectively, i.e.
handle networks with at least 16 times more parameters."
RELATED WORK,0.0625,"To investigate convergence of a WideResNet WRN-28-k (Zagoruyko & Komodakis, 2016) to its
inﬁnite width limit, Novak et al. (2020, Figure 2) evaluate the NTK of this model with widening
factor k up to 32. In matching setting and hardware, we are able to reach the widening factor of at
least 64, i.e. work with models at least 4 times larger."
RELATED WORK,0.06521739130434782,"To meta-learn NN parameters for transfer learning in a MAML-like (Finn et al., 2017) setting, Zhou
et al. (2021, Table 7) replace the inner training loop with NTK-based inference. They use up to
5-layer, 200-channel CNNs on MiniImageNet (Oreshkin et al., 2018) with scalar outputs and batch
size 25. In same setting we achieve at least 512 channels, i.e. support models at least 6 times larger."
RELATED WORK,0.06793478260869565,"Park et al. (2020, §4.1) use the NTK to predict the generalization performance of architectures in
the context of Neural Architecture Search (Zoph & Le, 2017, NAS); however, the authors comment
on its high computational burden and ultimately use a different proxy. In another NAS setting, Chen
et al. (2021a, §3.1.1) use the condition number of NTK to predict a model’s trainability. Remark-
ing its prohibitive cost, Chen et al. (2021b, Table 1) also use the NTK to evaluate the trainability
of several ImageNet (Deng et al., 2009) models such as ResNet 50/152 (He et al., 2016), Vision
Transformer (Dosovitskiy et al., 2021) and MLP-Mixer (Tolstikhin et al., 2021). However, in all of
the above cases the authors only evaluate a pseudo-NTK, i.e. an NTK of a scalar-valued function,1
which impacts the quality of the respective trainability/generalization proxy."
RELATED WORK,0.07065217391304347,"1Precisely, computing the Jacobian only for a single logit or the sum of all 1000 class logits. The result is
not the full NTK, but rather a single diagonal block or the sum of its 1000 diagonal blocks (ﬁnite width NTK is
a dense matrix, not block-diagonal)."
RELATED WORK,0.07336956521739131,Under review as a conference paper at ICLR 2022
RELATED WORK,0.07608695652173914,"Method
Time
Memory
Use when
Jacobian contraction
N2LO2W2
NOW2 + N2O2 + NLW + LW2
Don’t
NTK-vector products
N2 O2W + N2LOW2
NOW2 + N2O2 + NLW + LW2
O > W or N = 1
Structured derivatives
N2LO2W + N LOW2
NOW + N2O2 + NLW + LW2
O < W or L = 1"
RELATED WORK,0.07880434782608696,"Table 1: Asymptotic time and memory cost of computing the NTK for an FCN. Costs are for a
pair of batches of inputs of size N each, and for L-deep, W -wide FCN with O outputs. Resulting
NTK has shape NO ⇥NO. NTK-vector products allow a reduction of the time complexity, while
Structured derivatives reduce both time and memory complexity. Note: presented are asymptotic
cost estimates; in practice, all methods incur large constant multipliers (e.g. at least 3x for time; see
§3.1). However, this generally does not impact the relative performance of different methods. See
§3.6 for discussion, Table 7 for CNN, and Table 2 for more generic cost analysis."
RELATED WORK,0.08152173913043478,"Method
Time
Memory
Use when
Jacobian contraction
N O [FP] + N2O2P
N2O2 + NO ⇥"
RELATED WORK,0.08423913043478261,Yk + Pl⇤
RELATED WORK,0.08695652173913043,"+ NY + P
P ⌧Y, small O, exotic primitives
NTK-vector products
N2O [FP]
N2O2 + NO ⇥"
RELATED WORK,0.08967391304347826,Yk + Pl⇤
RELATED WORK,0.09239130434782608,"+ NY + P
FP < OP, large O, small N
Structured derivatives
N O [FP] + N O G + N [J −OP]
N2O2 + NOYk + NJk"
RELATED WORK,0.09510869565217392,"l + NY + P
FP > OP, large O, large N"
RELATED WORK,0.09782608695652174,"Table 2: Asymptotic time and memory cost estimates of computing the NTK for a generic
function. P stands for the number of all parameters in the network, Y stands for size of all pre-
activations in the network, FP stands for forward pass, and G and J depend on the structure of
FP (§B). For example, FCNs usually have a cheap FP OP, as it consists of a single matrix
multiply with the parameter matrix, and therefore NTK-vector products are recommended. CNNs,
notably when the number of output pixels D is large, have a costly FP ≥OP, since it amounts to D
matrix multiplies with the parameters, and therefore Structured derivatives are preferred. For precise
analysis, see Table 1 for FCN and Table 7 for CNN."
RELATED WORK,0.10054347826086957,"In contrast, in this work we can compute the full 1000 ⇥1000 NTK on the same models (1000
classes), i.e. perform a task 1000 times more costly."
RELATED WORK,0.10326086956521739,"Finally, we remark that in all of the above settings, scaling up by increasing width or by working
with the true NTK (vs the pseudo-NTK) should lead to improved downstream task performance
due to better inﬁnite width/linearization approximation or higher-quality trainability/generalization
proxy respectively, which makes our work especially relevant to modern research."
EFFICIENT FINITE WIDTH NTKS IN A SIMPLIFIED SETTING,0.10597826086956522,"3
EFFICIENT FINITE WIDTH NTKS IN A SIMPLIFIED SETTING"
EFFICIENT FINITE WIDTH NTKS IN A SIMPLIFIED SETTING,0.10869565217391304,"To gain intuition for the problem, we start by analyzing and improving the cost of computing the
NTK for a simple FCN. See §F for an equivalent analysis of CNNs. We summarize the resulting
complexities for FCN in Table 1, CNN in Table 7, and a general takeaway in Table 2."
EFFICIENT FINITE WIDTH NTKS IN A SIMPLIFIED SETTING,0.11141304347826086,"Setting. Consider an L-layer FCN f (✓, x) = ✓L φ ("
EFFICIENT FINITE WIDTH NTKS IN A SIMPLIFIED SETTING,0.11413043478260869,✓L−1 . . . ✓1 φ ( ✓0x ) . . . )
EFFICIENT FINITE WIDTH NTKS IN A SIMPLIFIED SETTING,0.11684782608695653,"2 RO, where O is the
number of logits. We denote individual weight matrices as ✓l with shapes W ⇥W (except for top-
layer ✓L of shape O ⇥W), where W is the width of the network, and write the set of all parameters
as ✓= vec ⇥"
EFFICIENT FINITE WIDTH NTKS IN A SIMPLIFIED SETTING,0.11956521739130435,"✓0, . . . , ✓L⇤"
EFFICIENT FINITE WIDTH NTKS IN A SIMPLIFIED SETTING,0.12228260869565218,2 RLW2+OW. We further deﬁne xl := φ ( yl−1)
EFFICIENT FINITE WIDTH NTKS IN A SIMPLIFIED SETTING,0.125,"as post-activations (with
x0 := x), and yl := ✓lxl as pre-activations with yL = f (✓, x). See Fig. 5 for a visual schematic
of these quantities. For simplicity, we assume that inputs x also have width W, and O = O (LW),
i.e. the number of logits is dominated by the product of width and depth. In §L we repeat the same
derivations without the latter assumption, and arrive at qualitatively identical conclusions."
EFFICIENT FINITE WIDTH NTKS IN A SIMPLIFIED SETTING,0.12771739130434784,The NTK of f evaluated at two inputs x1 and x2 is an O ⇥O matrix deﬁned as
EFFICIENT FINITE WIDTH NTKS IN A SIMPLIFIED SETTING,0.13043478260869565,"⇥✓:= @f(✓, x1) @✓"
EFFICIENT FINITE WIDTH NTKS IN A SIMPLIFIED SETTING,0.1331521739130435,"@f(✓, x2) @✓ T = L
X l=0"
EFFICIENT FINITE WIDTH NTKS IN A SIMPLIFIED SETTING,0.1358695652173913,"@f (✓, x1) @✓l"
EFFICIENT FINITE WIDTH NTKS IN A SIMPLIFIED SETTING,0.13858695652173914,"@f (✓, x2) @✓l T =: L
X l=0 ⇥l"
EFFICIENT FINITE WIDTH NTKS IN A SIMPLIFIED SETTING,0.14130434782608695,"✓2 RO⇥O,
(2)"
EFFICIENT FINITE WIDTH NTKS IN A SIMPLIFIED SETTING,0.14402173913043478,where we have deﬁned ⇥l
EFFICIENT FINITE WIDTH NTKS IN A SIMPLIFIED SETTING,0.14673913043478262,"✓to be the summands. We omit dependence on x1, x2, and f for brevity."
EFFICIENT FINITE WIDTH NTKS IN A SIMPLIFIED SETTING,0.14945652173913043,"In §3.1 and §3.2 we describe the cost of several fundamental AD operations that we will use as
building blocks throughout the text. We borrow the nomenclature introduced by Autograd (Maclau-"
EFFICIENT FINITE WIDTH NTKS IN A SIMPLIFIED SETTING,0.15217391304347827,Under review as a conference paper at ICLR 2022
EFFICIENT FINITE WIDTH NTKS IN A SIMPLIFIED SETTING,0.15489130434782608,"rin et al.) and describe Jacobian-vector products (JVP), vector-Jacobian products (VJP), as well as
the cost of computing the Jacobian @f(✓, x) "" @✓."
EFFICIENT FINITE WIDTH NTKS IN A SIMPLIFIED SETTING,0.15760869565217392,"In §3.3, we describe the baseline complexity of evaluating the NTK, by computing two Jacobians
and contracting them. This approach is used in most (likely all) prior works, and scales poorly with
the NN width W and output size O."
EFFICIENT FINITE WIDTH NTKS IN A SIMPLIFIED SETTING,0.16032608695652173,"In §3.4 we present our ﬁrst contribution, that consists in observing that many intermediate operations
on weights performed by NNs possess a certain structure, that can allow linear algebra simpliﬁca-
tions of the NTK expression, leading to a cheaper contraction and smaller memory footprint."
EFFICIENT FINITE WIDTH NTKS IN A SIMPLIFIED SETTING,0.16304347826086957,"In §3.5 we present our second contribution, where we rephrase the NTK computation as instantiating
itself row-by-row by applying the NTK-vector product function to columns of an identity matrix. As
we will show, this trades off Jacobian contraction for more forward passes, which proves beneﬁcial
in many (but not all) settings."
JACOBIAN-VECTOR PRODUCTS AND VECTOR-JACOBIAN PRODUCTS,0.16576086956521738,"3.1
JACOBIAN-VECTOR PRODUCTS AND VECTOR-JACOBIAN PRODUCTS"
JACOBIAN-VECTOR PRODUCTS AND VECTOR-JACOBIAN PRODUCTS,0.16847826086956522,We begin by deﬁning Jacobian-vector products and vector-Jacobian products:
JACOBIAN-VECTOR PRODUCTS AND VECTOR-JACOBIAN PRODUCTS,0.17119565217391305,"JVP(f,✓,x) : ✓t 2 RLW2+OW 7! @f (✓, x)"
JACOBIAN-VECTOR PRODUCTS AND VECTOR-JACOBIAN PRODUCTS,0.17391304347826086,"@✓
✓t 2 RO,
(3)"
JACOBIAN-VECTOR PRODUCTS AND VECTOR-JACOBIAN PRODUCTS,0.1766304347826087,"VJP(f,✓,x) : fc 2 RO 7! @f (✓, x) @✓ T"
JACOBIAN-VECTOR PRODUCTS AND VECTOR-JACOBIAN PRODUCTS,0.1793478260869565,"fc 2 RLW2+OW.
(4)"
JACOBIAN-VECTOR PRODUCTS AND VECTOR-JACOBIAN PRODUCTS,0.18206521739130435,"The JVP can be understood as pushing forward a tangent vector in weight space to a tangent vector
in the space of outputs; by contrast the VJP pulls back a cotangent vector in the space of outputs to
a cotangent vector in weight space. These elementary operations correspond to forward and reverse
mode AD respectively and serve as a basis for typical AD computations such as gradients, Jacobians,
Hessians, etc."
JACOBIAN-VECTOR PRODUCTS AND VECTOR-JACOBIAN PRODUCTS,0.18478260869565216,"Time and memory costs of JVP and VJP are asymptotically equivalent to the cost of the forward pass
(FP), except for VJP additionally requires storing all intermediate activations. (see §N and Fig. 6)."
JACOBIAN-VECTOR PRODUCTS AND VECTOR-JACOBIAN PRODUCTS,0.1875,"For the case of FCNs, the time cost2 of both operations is therefore"
JACOBIAN-VECTOR PRODUCTS AND VECTOR-JACOBIAN PRODUCTS,0.19021739130434784,[FP] = [cost of all intermediate layers] + [cost of the top layer] = ⇥ LW2⇤
JACOBIAN-VECTOR PRODUCTS AND VECTOR-JACOBIAN PRODUCTS,0.19293478260869565,"+ [OW] ⇠LW2.
For a single input, the memory cost of computing both the JVP and the VJP are respectively,"
JACOBIAN-VECTOR PRODUCTS AND VECTOR-JACOBIAN PRODUCTS,0.1956521739130435,[size of all weights] + [size of activations at a single layer] = ⇥
JACOBIAN-VECTOR PRODUCTS AND VECTOR-JACOBIAN PRODUCTS,0.1983695652173913,LW2 + OW ⇤
JACOBIAN-VECTOR PRODUCTS AND VECTOR-JACOBIAN PRODUCTS,0.20108695652173914,"+ [W + O] ⇠LW2,"
JACOBIAN-VECTOR PRODUCTS AND VECTOR-JACOBIAN PRODUCTS,0.20380434782608695,[size of all weights] + [size of activations in all layers] = ⇥
JACOBIAN-VECTOR PRODUCTS AND VECTOR-JACOBIAN PRODUCTS,0.20652173913043478,LW2 + OW ⇤
JACOBIAN-VECTOR PRODUCTS AND VECTOR-JACOBIAN PRODUCTS,0.20923913043478262,"+ [LW + O] ⇠LW2.
Despite the fact that the VJP requires more memory to store intermediate activations, we see that for
FCNs both computations are dominated by the cost of storing the weights."
JACOBIAN-VECTOR PRODUCTS AND VECTOR-JACOBIAN PRODUCTS,0.21195652173913043,"Batched inputs. If x is a batch of inputs of size N, the time cost of JVP and VJP increases linearly
to NLW2. The memory cost is slightly more nuanced. Since weights can be shared across inputs,
the memory cost of the JVP and VJP are respectively,"
JACOBIAN-VECTOR PRODUCTS AND VECTOR-JACOBIAN PRODUCTS,0.21467391304347827,[size of all weights] + N [size of activations at a single layer] = ⇥
JACOBIAN-VECTOR PRODUCTS AND VECTOR-JACOBIAN PRODUCTS,0.21739130434782608,LW2 + OW ⇤
JACOBIAN-VECTOR PRODUCTS AND VECTOR-JACOBIAN PRODUCTS,0.22010869565217392,"+ N [W + O] ⇠LW2 + NW + NO,"
JACOBIAN-VECTOR PRODUCTS AND VECTOR-JACOBIAN PRODUCTS,0.22282608695652173,[size of all weights] + N [size of activations in all layers] + N [size of all weight matrices] = ⇥
JACOBIAN-VECTOR PRODUCTS AND VECTOR-JACOBIAN PRODUCTS,0.22554347826086957,LW2 + OW ⇤
JACOBIAN-VECTOR PRODUCTS AND VECTOR-JACOBIAN PRODUCTS,0.22826086956521738,+ N [LW + O] + N ⇥
JACOBIAN-VECTOR PRODUCTS AND VECTOR-JACOBIAN PRODUCTS,0.23097826086956522,LW2 + OW ⇤
JACOBIAN-VECTOR PRODUCTS AND VECTOR-JACOBIAN PRODUCTS,0.23369565217391305,"⇠NLW2.
The cost of the VJP is dominated by the cost of storing the cotangents in weight space. However,
for the purposes of computing the NTK, we will be contracting Jacobians layerwise and so we will
only need to store one cotangent weight matrix, @f """
JACOBIAN-VECTOR PRODUCTS AND VECTOR-JACOBIAN PRODUCTS,0.23641304347826086,"@✓l, at a time. Thus, for the purposes of this
work we end up with the following costs:"
JACOBIAN-VECTOR PRODUCTS AND VECTOR-JACOBIAN PRODUCTS,0.2391304347826087,• JVP costs NLW2 time and LW2 + NW + NO memory.
JACOBIAN-VECTOR PRODUCTS AND VECTOR-JACOBIAN PRODUCTS,0.2418478260869565,• VJP costs NLW2 time and LW2 + NLW + NW2 + NOW memory.
JACOBIAN-VECTOR PRODUCTS AND VECTOR-JACOBIAN PRODUCTS,0.24456521739130435,"2To declutter notation, we omit the O symbol to indicate asymptotic complexity in this work."
JACOBIAN-VECTOR PRODUCTS AND VECTOR-JACOBIAN PRODUCTS,0.24728260869565216,Under review as a conference paper at ICLR 2022
JACOBIAN COMPUTATION,0.25,"3.2
JACOBIAN COMPUTATION"
JACOBIAN COMPUTATION,0.25271739130434784,"For neural networks, the Jacobian is most often computed by evaluating the VJP on rows of the
identity matrix IO, i.e. ⇥"
JACOBIAN COMPUTATION,0.2554347826086957,"@f (✓, x) "" @✓ ⇤T = ⇥"
JACOBIAN COMPUTATION,0.25815217391304346,"@f (✓, x) "" @✓"
JACOBIAN COMPUTATION,0.2608695652173913,"⇤T IO 2 R(LW2+OW)⇥O.
(5)"
JACOBIAN COMPUTATION,0.26358695652173914,"It follows that computing the Jacobian takes O evaluations of the VJP. However, as mentioned in
§3.1, we only need to store one @f """
JACOBIAN COMPUTATION,0.266304347826087,"@✓l at a time, while the weights and intermediate activations are
reused across evaluations. Thus, time and memory costs to compute the Jacobian are respectively,"
JACOBIAN COMPUTATION,0.26902173913043476,ON ([cost of all intermediate layers] + [cost of the top layer]) = ON (⇥ LW2⇤
JACOBIAN COMPUTATION,0.2717391304347826,+ [OW] )
JACOBIAN COMPUTATION,0.27445652173913043,"⇠NLOW2 + NO2W,"
JACOBIAN COMPUTATION,0.27717391304347827,[size of all weights] + N [size of activations in all layers] + ON [size of a single weight matrix] = ⇥
JACOBIAN COMPUTATION,0.2798913043478261,LW2 + OW ⇤
JACOBIAN COMPUTATION,0.2826086956521739,+ N [LW + O] + ON ⇥
JACOBIAN COMPUTATION,0.28532608695652173,W2 + OW ⇤
JACOBIAN COMPUTATION,0.28804347826086957,⇠LW2 + NLW + NOW2 + NO2W.
JACOBIAN COMPUTATION,0.2907608695652174,"Therefore, asymptotically,"
JACOBIAN COMPUTATION,0.29347826086956524,Jacobian costs NLOW2 + NO2W time and LW2 + NLW + NOW2 + NO2W memory.
JACOBIAN CONTRACTION,0.296195652173913,"3.3
JACOBIAN CONTRACTION"
JACOBIAN CONTRACTION,0.29891304347826086,"We now analyze the cost of computing the NTK, starting with the direct computation as the product
of two Jacobians. Consider a single summand from Eq. (2): ⇥l"
JACOBIAN CONTRACTION,0.3016304347826087,"✓
|{z}
O⇥O"
JACOBIAN CONTRACTION,0.30434782608695654,"= @f (✓, x1)"
JACOBIAN CONTRACTION,0.3070652173913043,"@✓l
|
{z
}
O⇥(W⇥W)"
JACOBIAN CONTRACTION,0.30978260869565216,"@f (✓, x2) @✓l T"
JACOBIAN CONTRACTION,0.3125,"|
{z
}
(W⇥W)⇥O .
(6)"
JACOBIAN CONTRACTION,0.31521739130434784,"The time cost of this contraction is O2W2, and the memory necessary to instantiate each factor and
the result is OW2 + O2. Repeating the above operation for each ✓l, we arrive at LO2W2 time cost
and unchanged memory, due to being able to process summands sequentially."
JACOBIAN CONTRACTION,0.3179347826086957,"Batched inputs. If we consider x1 and x2 to be input batches of size N, then the resulting NTK
is a matrix of shape NO ⇥NO, and the time cost becomes N2LO2W2, while memory grows to
[NTK matrix size] + [factors size] = N2O2 + NOW2."
JACOBIAN CONTRACTION,0.32065217391304346,"What remains is to account for the cost of computing and storing individual cotangents @f """
JACOBIAN CONTRACTION,0.3233695652173913,"@✓l,
which is exactly the cost of computing the Jacobian (§3.2). Adding the costs up we obtain"
JACOBIAN CONTRACTION,0.32608695652173914,"Jacobian contraction costs N2LO2W2 time and N2O2 + NOW2 + NO2W + LW2 + NLW
memory."
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.328804347826087,"3.4
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK"
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.33152173913043476,"We can rewrite ⇥l✓in Eq. (6) using the chain rule and our pre- and post-activation notation as: ⇥l ✓= """
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.3342391304347826,"@f (✓, x1) @ylx1 @yl"
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.33695652173913043,"x1
@✓l # """
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.33967391304347827,"@f (✓, x2) @ylx2 @yl"
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.3423913043478261,"x2
@✓l #T"
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.3451086956521739,"= @f (✓, x1)"
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.34782608695652173,"@ylx1
|
{z
}
O⇥W @yl"
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.35054347826086957,"x1
@✓l
| {z }
W⇥(W⇥W) @yl"
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.3532608695652174,"x2
@✓l T"
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.35597826086956524,"| {z }
(W⇥W)⇥W"
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.358695652173913,"@f (✓, x2) @ylx2 T"
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.36141304347826086,"|
{z
}
W⇥O . (7)"
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.3641304347826087,"At face value, rewriting Eq. (6) this way is unhelpful as it appears to have introduced additional
costly contractions. However, recall that yl = ✓lxl, and therefore @yl"
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.36684782608695654,"x1
@✓l = IW ⌦xl 1"
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.3695652173913043,"T ,
@yl"
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.37228260869565216,"x2
@✓l = IW ⌦xl 2"
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.375,"T ,
(8)"
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.37771739130434784,where ⌦is the Kronecker product. Plugging Eq. (8) into Eq. (7) we obtain (see §G)
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.3804347826086957,"Under review as a conference paper at ICLR 2022 ⇥l ✓= 0 B
@xl 1"
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.38315217391304346,"T
|{z}
1⇥W xl"
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.3858695652173913,"2
|{z}
W⇥1 1 C
A 2 6664"
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.38858695652173914,"@f (✓, x1)"
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.391304347826087,"@ylx1
|
{z
}
O⇥W"
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.39402173913043476,"@f (✓, x2) @ylx2 T"
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.3967391304347826,"|
{z
}
W⇥O 3"
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.39945652173913043,"7775 ,
(9)"
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.40217391304347827,"and observe that it takes only O2W time and OW + O2 memory. Accounting for depth, time cost
becomes LO2W, while memory does not change since the summands can be processed sequentially."
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.4048913043478261,"Batched inputs. The time cost grows quadratically with the bath size N up to N2LO2W, while the
memory cost increases to N2O2 + NOW to store the resulting NTK and @f (✓, x) "" @yl"
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.4076086956521739,x factors.
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.41032608695652173,"Finally, we need to account for the cost of computing the derivatives, @f """
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.41304347826086957,"@yl, and post-activations,
xl. Notice that both xl and @f """
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.4157608695652174,"@yl arises naturally when computing the Jacobian as the primals
and cotangents in layer l respectively. However, since we do not need to compute the weight space
cotangents explicitly (i.e. we cut the backpropagation algorithm short) the memory cost will be,"
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.41847826086956524,[size of all weights] + N [size of activations in all layers] = ⇥
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.421195652173913,LW2 + OW ⇤
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.42391304347826086,+ N [LW + O] ⇠LW2 + NLW.
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.4266304347826087,"The extra time cost is asymptotically the cost of O forward passes, NLOW2 which is the same as
the Jacobian. However, as we will see in experiments, in practice we’ll often compute the NTK
faster than the Jacobian due to not computing the weight space cotangents @f/@✓l. Altogether,"
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.42934782608695654,"By leveraging Structured derivatives in NN computations, we have reduced the cost of NTK
to N2LO2W + NLOW2 time and N2O2 + NOW + LW2 + NLW memory."
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.4320652173913043,"The key insight was to leverage the constant block-diagonal structure of the pre-activation derivatives
@yl"""
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.43478260869565216,"@✓l. This idea is quite general; as we discuss in §4 and detail in the appendix, similar structure
exists for many common operations such as convolutions, pooling, and arithmetic."
LEVERAGING STRUCTURED DERIVATIVES FOR COMPUTING THE NTK,0.4375,"We highlight that these computational improvements do not emerge automatically in AD. While
JAX and other libraries leverage structures analogous to Eq. (8) to efﬁciently compute single evalu-
ations of the VJP and JVP, this knowledge is lost once the (structureless) Jacobian @f(✓, x1)/@✓l is
instantiated, and cannot be taken advantage of in the following contraction with @f(✓, x2)/@✓l. We
discuss how we impose this structure to compute the NTK for general neural networks in §4."
NTK VIA NTK-VECTOR PRODUCTS,0.44021739130434784,"3.5
NTK VIA NTK-VECTOR PRODUCTS"
NTK VIA NTK-VECTOR PRODUCTS,0.4429347826086957,"Computing the Jacobian contraction using Jacobian ﬁrst instantiates the Jacobian using using VJPs
and then performs a contraction. Structured derivatives use a similar strategy, but speed-up the
contraction and avoid explicitly instantiating the weight space cotangents. Here we avoid performing
a contraction altogether at the cost of extra VJP/JVP calls; this ends up being beneﬁcial for FCNs."
NTK VIA NTK-VECTOR PRODUCTS,0.44565217391304346,"We introduce the linear function performing the NTK-vector product: ⇥VP : v 2 RO 7! ⇥✓v 2 RO.
Applying this function to O columns of the identity matrix IO allows us to compute the NTK, i.e.
⇥✓IO = ⇥✓. The cost of evaluating the NTK in this fashion is equal to O times the cost of a single
NTK-vector product evaluation ⇥VP(v). We now expand ⇥VP(v) = ⇥✓v as"
NTK VIA NTK-VECTOR PRODUCTS,0.4483695652173913,"@f (✓, x1) @✓"
NTK VIA NTK-VECTOR PRODUCTS,0.45108695652173914,"@f (✓, x2) @✓ T"
NTK VIA NTK-VECTOR PRODUCTS,0.453804347826087,"v = @f (✓, x1)"
NTK VIA NTK-VECTOR PRODUCTS,0.45652173913043476,"@✓
VJP(f,✓,x2) (v) = JVP(f,✓,x1) ⇥"
NTK VIA NTK-VECTOR PRODUCTS,0.4592391304347826,"VJP(f,✓,x2) (v) ⇤"
NTK VIA NTK-VECTOR PRODUCTS,0.46195652173913043,",
(10)"
NTK VIA NTK-VECTOR PRODUCTS,0.46467391304347827,"where we have observed that, if contracted from right to left, the NTK-vector product can be ex-
pressed as a composition of a JVP and VJP of the underlying function f. The cost of this operation
is asymptotically equivalent to the cost of the Jacobian, since it consists of O VJPs followed by O
(cheaper) JVPs. Therefore it costs LOW2 + O2W time and LW2 + OW2 + O2W memory."
NTK VIA NTK-VECTOR PRODUCTS,0.4673913043478261,"Batched inputs. In the batched setting Eq. (10) is repeated for each pair of inputs, and therefore time
increases by a factor of N2 to become N2LOW2 + N2O2W. However, the memory cost grows only
linearly in N (except for the cost of storing the NTK of size N2O2), since intermediate activations
and derivatives necessary to compute the JVP and VJP can be computed for each batch x1 and x2"
NTK VIA NTK-VECTOR PRODUCTS,0.4701086956521739,Under review as a conference paper at ICLR 2022
NTK VIA NTK-VECTOR PRODUCTS,0.47282608695652173,"separately; these quantities are then reused for every pairwise combination resulting in a memory
equivalent to the Jacobian, i.e. N2O2 + ("
NTK VIA NTK-VECTOR PRODUCTS,0.47554347826086957,LW2 + NOW2 + NO2W + NLW )
NTK VIA NTK-VECTOR PRODUCTS,0.4782608695652174,", resulting in"
NTK VIA NTK-VECTOR PRODUCTS,0.48097826086956524,"NTK computation as a sequence of NTK-vector products costs N2LOW2 + N2O2W time
and N2O2 + NOW2 + LW2 + NLW memory."
SUMMARY,0.483695652173913,"3.6
SUMMARY"
SUMMARY,0.48641304347826086,"Structured derivatives and NTK-vector products allow a reduction in the time cost of NTK compu-
tation in different ways, and Structured derivatives also reduce memory requirements. Structured
derivatives are beneﬁcial for wide networks, with large W, and NTK-vector products are beneﬁcial
for networks with large outputs O. We conﬁrm our predictions with FLOPs measurements in Fig. 1."
SUMMARY,0.4891304347826087,"We further conﬁrm our methods can provide orders of magnitude speed-ups and memory savings
on all major hardware platforms in Fig. 1 (right) and Fig. 3. However, we notice that our wall-clock
time measurements often deviate from predictions due to unaccounted constant overheads of various
methods, hardware speciﬁcs, padding, and the (largely black-box) behavior of the XLA compiler.
Notably, in practice, we ﬁnd Structured derivatives almost always outperform NTK-vector products."
SUMMARY,0.49184782608695654,"Finally, we evaluate our methods in the wild, and conﬁrm computational beneﬁts on full ImageNet
models in Fig. 2 (ResNets, He et al. (2016)) and Fig. 4 (WideResNets, Zagoruyko & Komodakis
(2016); Vision Transformers and Transformer-ResNet hybrids Dosovitskiy et al. (2021); Steiner
et al. (2021); and MLP-Mixers Tolstikhin et al. (2021)). Computing the full O ⇥O = 1000 ⇥1000
NTK for many of these models on modern accelerators is only possible with Structured derivatives."
STRUCTURED DERIVATIVES FOR GENERIC FUNCTIONS,0.4945652173913043,"4
STRUCTURED DERIVATIVES FOR GENERIC FUNCTIONS"
STRUCTURED DERIVATIVES FOR GENERIC FUNCTIONS,0.49728260869565216,"Here we generalize the idea of leveraging structure in subexpressions presented in §3.4. This section
(and our implementation) is not speciﬁc to NNs and applies to any differentiable function."
STRUCTURED DERIVATIVES FOR GENERIC FUNCTIONS,0.5,Consider two differentiable functions deﬁned on a common input domain: fi : (
STRUCTURED DERIVATIVES FOR GENERIC FUNCTIONS,0.5027173913043478,"✓0, . . . , ✓L)"
STRUCTURED DERIVATIVES FOR GENERIC FUNCTIONS,0.5054347826086957,2 RP0⇥···⇥PL 7! fi (
STRUCTURED DERIVATIVES FOR GENERIC FUNCTIONS,0.5081521739130435,"✓0, . . . , ✓L)"
ROI,0.5108695652173914,"2 ROi
(i 2 {1, 2})."
ROI,0.5135869565217391,"For NNs, typically ("
ROI,0.5163043478260869,"✓0, . . . , ✓L)"
ROI,0.5190217391304348,"correspond to trainable parameters in layers 0, . . . , L, and
fi ("
ROI,0.5217391304347826,"✓0, . . . , ✓L) = f ("
ROI,0.5244565217391305,"✓0, . . . , ✓L, xi )"
ROI,0.5271739130434783,", xi being network inputs, Oi = O being the number of outputs
(logits, classes). The NTK is deﬁned as"
ROI,0.529891304347826,"⇥✓(f1, f2) := L
X l=0 @f1 @✓l @f2 @✓l T"
ROI,0.532608695652174,"2 RO1⇥O2.
(11)"
ROI,0.5353260869565217,Assume the following decompositions of fi into computational graphs made of primitives yi: fi (
ROI,0.5380434782608695,"✓0, . . . , ✓L) = ˜fi ( y1"
ROI,0.5407608695652174,"i (✓0, . . . , ✓L), . . . , yKi"
ROI,0.5434782608695652,"i (✓0, . . . , ✓L) )"
ROI,0.5461956521739131,"(i 2 {1, 2}).
(12)"
ROI,0.5489130434782609,with yk i (
ROI,0.5516304347826086,"✓0, . . . , ✓L)"
RYK,0.5543478260869565,2 RYk
RYK,0.5570652173913043,"i . In common NNs, yki"
RYK,0.5597826086956522,"i
would correspond to pre-activations evaluated
on inputs xi in layer ki, and, without weight sharing, typically K1 = K2 = L. However, we do not
impose any relationship between the number of parameter variables L and number of primitives K1
and K2, allowing arbitrary weight sharing. We can then use the chain rule in Eq. (2) to obtain:"
RYK,0.5625,"⇥✓(f1, f2) ="
RYK,0.5652173913043478,"L,K1,K2
X"
RYK,0.5679347826086957,"l,k1,k2 "
RYK,0.5706521739130435,"@ ˜f1
@yk1 1 @yk1 1
@✓l !"
RYK,0.5733695652173914,"@ ˜f2
@yk2 2 @yk2 2
@✓l !T ="
RYK,0.5760869565217391,"L,K1,K2
X"
RYK,0.5788043478260869,"l,k1,k2"
RYK,0.5815217391304348,"@ ˜f1
@yk1 1 @yk1 1
@✓l @yk2 2
@✓l"
RYK,0.5842391304347826,T @ ˜f2 @yk2 2 T
RYK,0.5869565217391305,. (13)
RYK,0.5896739130434783,"All methods from §3 perform the sum of contractions in Eq. (13) one way or another. Jacobian con-
traction uses VJPs to implicitly contract each summand “outside-in”, i.e. it ﬁrst computes @fi "" @✓l"
RYK,0.592391304347826,"terms with VJPs followed by their contraction. As discussed in §3.3, this costs NO [FP] + N2O2P."
RYK,0.595108695652174,"NTK-vector products use both JVPs and VJPs to contract “Right-to-left”, i.e. ﬁrst compute @f2/@✓l"
RYK,0.5978260869565217,"as an implicit contraction of @f2 """
RYK,0.6005434782608695,"@y2 with @y2 """
RYK,0.6032608695652174,"@✓l via VJP, followed by an implicit contraction
of the result with @y1 """
RYK,0.6059782608695652,"@✓l via a JVP, followed by another implicit contraction with @f1 """
RYK,0.6086956521739131,"@y1 with
another JVP. Per §3.5 this costs N2O [FP]."
RYK,0.6114130434782609,Under review as a conference paper at ICLR 2022
RYK,0.6141304347826086,"FLOPs (per NTK entry)
Wall-clock time (TPUv3)"
RYK,0.6168478260869565,"Figure 1: FLOPs (left) and wall-clock time (right) of computing the NTK for a 10-layer ReLU
FCN. As predicted by Table 1, our methods almost always outperform Jacobian contraction, allow-
ing orders of magnitude speed-ups and memory improvements for realistic problem sizes. FLOPs
per NTK entry: We conﬁrm several speciﬁc predictions: (1) NTK-vector products are the best per-
forming method for N = 1, and have cost equivalent to Jacobian for any width W or output size O
(top row); (2) NTK-vector products offer an O-fold improvement over Jacobian contraction (left to
right columns); (3) NTK-vector products are equivalent to Jacobian contraction for O = 1 (leftmost
column); (4) Structured derivatives outperform NTK-vector products i↵O < W (O = W are plot-
ted as pale vertical lines, which is where Structured derivatives and NTK-vector products intersect);
(5) Structured derivatives approach the cost of Jacobian in the limit of large width W (left to right).
(6) All methods, as expected, scale quadratically with width W. Wall-clock runtime: In real appli-
cations, given hardware-speciﬁc constraints, padding, and delicate interplay with the XLA compiler,
we observe that: (1) NTK-vector products improve upon Jacobian contraction for O > 1, but the
effect is not perfectly robust (see bottom row for small W and Fig. 3, notably GPU platforms); (2)
Structured derivatives robustly outperform all other methods, including simply computing the Ja-
cobian, as discussed in §3.4; (3) Structured derivatives have lower memory footprint, and reach up
to 8x larger widths (bottom right; missing points indicate out-of-memory), i.e. can process models
up to 64x larger than other methods, as discussed in §3.4; (4) All methods have a smaller memory
footprint than Jacobian (see §3.1). More: Fig. 3 for other hardware platforms, §H for details."
RYK,0.6195652173913043,"Figure 2: Wall-clock time cost of computing an NTK for several ResNet sizes on a pair of
ImageNet inputs. Structured derivatives allow the NTK to be computed faster and for larger models
(see bottom row – missing points indicate out-of-memory). NTK-vector products, as predicted in
§3.6 and Table 2, are advantageous for large O (bottom row), but are suboptimal when the cost of
the forward pass is large relative to the number of parameters, e.g. when there is a lot of weight
sharing (see Table 7 and Table 2), which is the case for convolutions. See Fig. 4 for more ImageNet
models, §F for analysis of CNN NTK computational complexity, and §H for experimental details."
RYK,0.6222826086956522,Under review as a conference paper at ICLR 2022
RYK,0.625,"However, recall from §3.4, while JVPs and VJPs themselves are computationally optimal, higher-
order computations like their contraction (Jacobian contraction) or composition (NTK-vector prod-
ucts) are generally not. The idea of Structured derivatives is to design rules for efﬁcient computation
of such contractions, similarly to how JAX and has rules for efﬁcient JVPs and VJPs. From Eq. (13),
in the general case this requires hand-made rules for all pairwise combinations of primitives y1 and
y2. Due to quadratic scaling in the number of primitives, we restrict the current implementation to
rules that operate on individual primitives y. This still provides substantial computational beneﬁt."
RYK,0.6277173913043478,"Speciﬁcally, our rules identify a few simple types of structure (e.g. block diagonal, constant-block
diagonal, tiling) in @y """
RYK,0.6304347826086957,"@✓l, that allow us to simplify the contraction in Eq. (13). In practice this
amounts to replacing the inner terms @yk1 1 """
RYK,0.6331521739130435,"@✓l and @yk2 2 """
RYK,0.6358695652173914,"@✓l with their (much) smaller subarrays,
and modifying the instructions passed to np.einsum that contracts all 4 terms. In §C we provide
speciﬁc descriptions of our rules and their impact on the computational complexity of Eq. (13)."
RYK,0.6385869565217391,"In Table 1 and Table 7 we show that our rules are asymptotically better than Jacobian contraction for
matrix multiplications and convolutions, and verify that they are practically beneﬁcial in a much
wider set of operations used by contemporary ImageNet models in Fig. 2 and Fig. 4."
RYK,0.6413043478260869,"For both Structured derivatives and NTK-vector products a fully general and rigorous comparison
of complexities is not feasible since it will rely upon speciﬁcs of the model architecture and the pairs
of primitives, y1 and y2, present in the network. Nonetheless, we can offer heuristics that suggest
when each method will be beneﬁcial. The time complexity of Structured derivatives has the form
of NO [FP] + NOG + N [J −OP], where G is related to the cost of contraction, and J to the cost of
computing @y """
RYK,0.6440217391304348,"@✓l (exact values depend on the structure present in y1 and y2). This is guaranteed
to be no worse than Jacobian contraction for FCNs and CNNs. From Table 2, the performance of
NTK-vector products relative to Jacobian contraction ultimately depends on the cost of the forward
pass through the network, [FP], relative to OP. In practice this amounts to best performance on
models without weight sharing like FCNs."
RYK,0.6467391304347826,"Owing to the nuanced trade-offs between different computational methods in the general case, we
release all our implementations as a single function that allows the user to manually select the desired
implementation. For convenience, we include an automated setting which will perform FLOPs
analysis for each method at compilation time and automatically select the most efﬁcient one."
IMPLEMENTATION,0.6494565217391305,"5
IMPLEMENTATION"
IMPLEMENTATION,0.6521739130434783,"Both algorithms are implemented in JAX (Bradbury et al., 2018) as the following function trans-
formation ntk_fn : [f : (✓, x) 7! f(✓, x)] 7! [⇥: (x1, x2, ✓) 7! ⇥✓(x1, x2)] , i.e. our function
accepts any function f with the above signature and returns the efﬁcient NTK kernel function oper-
ating on inputs x1 and x2 and parameterized by ✓. Inputs x, parameters ✓, and outputs f(✓, x) can
be arbitrary PyTrees. We rely on many utilities from JAX and Neural Tangents (Novak et al., 2020)."
IMPLEMENTATION,0.654891304347826,"NTK-vector products algorithm is implemented by using JAX core operations such as vjp , jvp ,
and vmap to map the NTK-vp function to the IO matrix and to parallelize the computation over
pairwise combinations of N inputs in each batch x1 and x2."
IMPLEMENTATION,0.657608695652174,"Structured derivatives algorithm is implemented as a Jaxpr interpreter, built on top of the default
JAX reverse mode AD interpreter. On a high level, the algorithm performs the sum in Eq. (13). Each
summand is a contraction of 4 factors: @ ˜f1 """
IMPLEMENTATION,0.6603260869565217,"@y1, @y1/@✓, @y2/@✓, @ ˜f2 "" @y2."
IMPLEMENTATION,0.6630434782608695,"First, we linearize f to obtain a computational graph constructed out of a limited set (54,3 see
Table 5) of linear primitives y1, . . . , yK. Then, we can obtain two factors @ ˜f1 """
IMPLEMENTATION,0.6657608695652174,"@y1, @ ˜f2 """
IMPLEMENTATION,0.6684782608695652,"@y2 as part
of a backward pass almost identical to calling jax.jacobian (f)(✓, x). To contract these terms
with @y1/@✓and @y2/@✓, as described above, we query a dictionary of rules which map primitives
to a structural description (§C.8); for a given pair of primitives, these rules allow us to analytically
simplify the contraction and avoid explicitly instantiating the derivatives."
IMPLEMENTATION,0.6711956521739131,"3JAX leverages a similar approach to implement only 54 transpose rules for linear primitives for reverse
mode differentiation instead of 131 VJP rules (Frostig et al., 2021)."
IMPLEMENTATION,0.6739130434782609,Under review as a conference paper at ICLR 2022
REFERENCES,0.6766304347826086,REFERENCES
REFERENCES,0.6793478260869565,"Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu"
REFERENCES,0.6820652173913043,"Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorﬂow: A system for large-
scale machine learning. In 12th USENIX Symposium on Operating Systems Design and Imple-
mentation (OSDI 16), 2016."
REFERENCES,0.6847826086956522,"Ben Adlam, Jaehoon Lee, Lechao Xiao, Jeffrey Pennington, and Jasper Snoek. Exploring the un-"
REFERENCES,0.6875,"certainty properties of neural networks’ implicit priors in the inﬁnite-width limit. In International
Conference on Learning Representations, 2020."
REFERENCES,0.6902173913043478,"Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang."
REFERENCES,0.6929347826086957,"On exact computation with an inﬁnitely wide neural net. In Advances in Neural Information
Processing Systems, pp. 8141–8150. Curran Associates, Inc., 2019a."
REFERENCES,0.6956521739130435,"Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of"
REFERENCES,0.6983695652173914,"optimization and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584, 2019b."
REFERENCES,0.7010869565217391,"Sanjeev Arora, Simon S. Du, Zhiyuan Li, Ruslan Salakhutdinov, Ruosong Wang, and Dingli Yu."
REFERENCES,0.7038043478260869,"Harnessing the power of inﬁnitely wide deep nets on small-data tasks. In International Conference
on Learning Representations, 2020. URL https://openreview.net/forum?id=rkl8sJBYvH."
REFERENCES,0.7065217391304348,"Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. Explaining neural"
REFERENCES,0.7092391304347826,"scaling laws. arXiv preprint arXiv:2102.06701, 2021."
REFERENCES,0.7119565217391305,"Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal.
Reconciling modern machine-
learning practice and the classical bias–variance trade-off. Proceedings of the National Academy
of Sciences, 116(32):15849–15854, 2019."
REFERENCES,0.7146739130434783,"James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal"
REFERENCES,0.717391304347826,"Maclaurin, and Skye Wanderman-Milne. JAX: composable transformations of Python+NumPy
programs, 2018. URL http://github.com/google/jax."
REFERENCES,0.720108695652174,"Andrew Brock, Soham De, and Samuel L Smith. Characterizing signal propagation to close the"
REFERENCES,0.7228260869565217,"performance gap in unnormalized resnets. arXiv preprint arXiv:2101.08692, 2021a."
REFERENCES,0.7255434782608695,"Andrew Brock, Soham De, Samuel L Smith, and Karen Simonyan. High-performance large-scale"
REFERENCES,0.7282608695652174,"image recognition without normalization. arXiv preprint arXiv:2102.06171, 2021b."
REFERENCES,0.7309782608695652,"Wuyang Chen, Xinyu Gong, and Zhangyang Wang.
Neural architecture search on imagenet in
four gpu hours: A theoretically inspired perspective. In International Conference on Learning
Representations, 2021a."
REFERENCES,0.7336956521739131,"Xiangning Chen, Cho-Jui Hsieh, and Boqing Gong. When vision transformers outperform resnets"
REFERENCES,0.7364130434782609,"without pretraining or strong data augmentations, 2021b."
REFERENCES,0.7391304347826086,Yann Dauphin and Samuel S Schoenholz. Metainit: Initializing learning by learning to initialize. 2019.
REFERENCES,0.7418478260869565,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-"
REFERENCES,0.7445652173913043,"erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248–255. Ieee, 2009."
REFERENCES,0.7472826086956522,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas"
REFERENCES,0.75,"Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-
reit, and Neil Houlsby.
An image is worth 16x16 words: Transformers for image recogni-
tion at scale. In International Conference on Learning Representations, 2021. URL https:
//openreview.net/forum?id=YicbFdNTTy."
REFERENCES,0.7527173913043478,"Simon S Du, Kangcheng Hou, Russ R Salakhutdinov, Barnabas Poczos, Ruosong Wang, and Keyulu"
REFERENCES,0.7554347826086957,"Xu. Graph neural tangent kernel: Fusing graph neural networks with graph kernels. In Advances
in Neural Information Processing Systems. 2019."
REFERENCES,0.7581521739130435,Under review as a conference paper at ICLR 2022
REFERENCES,0.7608695652173914,"Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of"
REFERENCES,0.7635869565217391,"deep networks. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International
Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp.
1126–1135. PMLR, 06–11 Aug 2017. URL https://proceedings.mlr.press/v70/finn17a.
html."
REFERENCES,0.7663043478260869,"Jean-Yves Franceschi, Emmanuel de B´ezenac, Ibrahim Ayed, Micka¨el Chen, Sylvain Lamprier, and"
REFERENCES,0.7690217391304348,"Patrick Gallinari. A neural tangent kernel perspective of gans. arXiv preprint arXiv:2106.05566,
2021."
REFERENCES,0.7717391304347826,"Roy Frostig, Matthew J Johnson, Dougal Maclaurin, Adam Paszke, and Alexey Radul. Decompos-"
REFERENCES,0.7744565217391305,"ing reverse-mode automatic differentiation. arXiv preprint arXiv:2105.09469, 2021."
REFERENCES,0.7771739130434783,"Adri`a Garriga-Alonso, Laurence Aitchison, and Carl Edward Rasmussen. Deep convolutional net-"
REFERENCES,0.779891304347826,"works as shallow gaussian processes. In International Conference on Learning Representations,
2019."
REFERENCES,0.782608695652174,Andreas Griewank and Andrea Walther. Evaluating Derivatives. Society for Industrial and Applied
REFERENCES,0.7853260869565217,"Mathematics, second edition, 2008. doi: 10.1137/1.9780898717761. URL https://epubs.
siam.org/doi/abs/10.1137/1.9780898717761."
REFERENCES,0.7880434782608695,"Roger Grosse. Neural net training dynamics, January 2021. URL https://www.cs.toronto.edu/"
REFERENCES,0.7907608695652174,⇠rgrosse/courses/csc2541 2021/readings/L02 Taylor approximations.pdf.
REFERENCES,0.7934782608695652,Boris Hanin and Mihai Nica. Finite depth and width corrections to the neural tangent kernel. In
REFERENCES,0.7961956521739131,"International Conference on Learning Representations, 2020. URL https://openreview.net/
forum?id=SJgndT4KwB."
REFERENCES,0.7989130434782609,"Bobby He, Balaji Lakshminarayanan, and Yee Whye Teh. Bayesian deep ensembles via the neu-"
REFERENCES,0.8016304347826086,"ral tangent kernel.
In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina
Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33:
Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, Decem-
ber 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
0b1ec366924b26fc98fa7b71a9c249cf-Abstract.html."
REFERENCES,0.8043478260869565,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-"
REFERENCES,0.8070652173913043,"nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778, 2016."
REFERENCES,0.8097826086956522,"Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas"
REFERENCES,0.8125,"Steiner, and Marc van Zee. Flax: A neural network library and ecosystem for JAX, 2020. URL
http://github.com/google/flax."
REFERENCES,0.8152173913043478,"Tom Hennigan, Trevor Cai, Tamara Norman, and Igor Babuschkin. Haiku: Sonnet for JAX, 2020."
REFERENCES,0.8179347826086957,URL http://github.com/deepmind/dm-haiku.
REFERENCES,0.8206521739130435,"Jiri Hron, Yasaman Bahri, Jascha Sohl-Dickstein, and Roman Novak. Inﬁnite attention: NNGP and"
REFERENCES,0.8233695652173914,"NTK for deep attention networks. In International Conference on Machine Learning, 2020."
REFERENCES,0.8260869565217391,"Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-"
REFERENCES,0.8288043478260869,"eralization in neural networks. In Advances in Neural Information Processing Systems, 2018."
REFERENCES,0.8315217391304348,"Jaehoon Lee, Yasaman Bahri, Roman Novak, Sam Schoenholz, Jeffrey Pennington, and Jascha Sohl-"
REFERENCES,0.8342391304347826,"dickstein. Deep neural networks as gaussian processes. In International Conference on Learning
Representations, 2018."
REFERENCES,0.8369565217391305,"Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-"
REFERENCES,0.8396739130434783,"Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In Advances in Neural Information Processing Systems, 2019."
REFERENCES,0.842391304347826,"Jaehoon Lee, Samuel S Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak,"
REFERENCES,0.845108695652174,and Jascha Sohl-Dickstein. Finite versus inﬁnite neural networks: an empirical study. 2020.
REFERENCES,0.8478260869565217,"Dougal Maclaurin, David Duvenaud, and Ryan P Adams. Autograd: Effortless gradients in numpy."
REFERENCES,0.8505434782608695,Under review as a conference paper at ICLR 2022
REFERENCES,0.8532608695652174,James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate
REFERENCES,0.8559782608695652,"curvature. In International conference on machine learning, pp. 2408–2417. PMLR, 2015."
REFERENCES,0.8586956521739131,"Alexander G. de G. Matthews, Jiri Hron, Mark Rowland, Richard E. Turner, and Zoubin Ghahra-"
REFERENCES,0.8614130434782609,"mani. Gaussian process behaviour in wide deep neural networks. In International Conference on
Learning Representations, 2018."
REFERENCES,0.8641304347826086,Herman M¨untz. Solution directe de l’´equation s´eculaire et de quelques probl`emes analogues tran-
REFERENCES,0.8668478260869565,"scendants. C. R. Acad. Sci. Paris, 156:43–46, 1913."
REFERENCES,0.8695652173913043,Uwe Naumann. Optimal accumulation of jacobian matrices by elimination methods on the dual
REFERENCES,0.8722826086956522,"computational graph. Mathematical Programming, 99(3):399–421, 2004."
REFERENCES,0.875,"Uwe Naumann. Optimal jacobian accumulation is np-complete. Mathematical Programming, 112"
REFERENCES,0.8777173913043478,"(2):427–441, 2008."
REFERENCES,0.8804347826086957,"Radford M. Neal. Priors for inﬁnite networks (tech. rep. no. crg-tr-94-1). University of Toronto, 1994."
REFERENCES,0.8831521739130435,"Timothy Nguyen, Zhourong Chen, and Jaehoon Lee.
Dataset meta-learning from kernel ridge-
regression. arXiv preprint arXiv:2011.00050, 2020."
REFERENCES,0.8858695652173914,"Timothy Nguyen, Roman Novak, Lechao Xiao, and Jaehoon Lee. Dataset distillation with inﬁnitely"
REFERENCES,0.8885869565217391,"wide convolutional networks. arXiv preprint arXiv:2107.13034, 2021."
REFERENCES,0.8913043478260869,"Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron, Daniel A. Abo-"
REFERENCES,0.8940217391304348,"laﬁa, Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional networks with
many channels are gaussian processes. In International Conference on Learning Representations,
2019."
REFERENCES,0.8967391304347826,"Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander A. Alemi, Jascha Sohl-Dickstein,"
REFERENCES,0.8994565217391305,"and Samuel S. Schoenholz. Neural tangents: Fast and easy inﬁnite neural networks in python.
In International Conference on Learning Representations, 2020. URL https://github.com/
google/neural-tangents."
REFERENCES,0.9021739130434783,"Boris N. Oreshkin, Pau Rodr´ıguez L´opez, and Alexandre Lacoste. Tadam: Task dependent adaptive"
REFERENCES,0.904891304347826,"metric for improved few-shot learning. In NeurIPS, 2018."
REFERENCES,0.907608695652174,"Daniel S Park, Jaehoon Lee, Daiyi Peng, Yuan Cao, and Jascha Sohl-Dickstein. Towards nngp-"
REFERENCES,0.9103260869565217,"guided neural architecture search. arXiv preprint arXiv:2011.06006, 2020."
REFERENCES,0.9130434782608695,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor"
REFERENCES,0.9157608695652174,"Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Ed-
ward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit
Steiner, Lu Fang, Junjie Bai, and Soumith Chintala.
Pytorch: An imperative style, high-
performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-
Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems 32,
pp. 8024–8035. Curran Associates, Inc., 2019.
URL http://papers.neurips.cc/paper/
9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf."
REFERENCES,0.9184782608695652,"Jeffrey Pennington and Yasaman Bahri.
Geometry of neural network loss surfaces via random
matrix theory.
In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th Interna-
tional Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Re-
search, pp. 2798–2806. PMLR, 06–11 Aug 2017. URL https://proceedings.mlr.press/
v70/pennington17a.html."
REFERENCES,0.9211956521739131,"Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information"
REFERENCES,0.9239130434782609,"propagation. International Conference on Learning Representations, 2017."
REFERENCES,0.9266304347826086,"Stefano Spigler, Mario Geiger, St´ephane d’Ascoli, Levent Sagun, Giulio Biroli, and Matthieu Wyart."
REFERENCES,0.9293478260869565,"A jamming transition from under-to over-parametrization affects generalization in deep learning.
Journal of Physics A: Mathematical and Theoretical, 52(47):474001, 2019."
REFERENCES,0.9320652173913043,Under review as a conference paper at ICLR 2022
REFERENCES,0.9347826086956522,"Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas"
REFERENCES,0.9375,"Beyer. How to train your vit? data, augmentation, and regularization in vision transformers. arXiv
preprint arXiv:2106.10270, 2021."
REFERENCES,0.9402173913043478,"Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan,"
REFERENCES,0.9429347826086957,"Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, and Ren Ng. Fourier features let net-
works learn high frequency functions in low dimensional domains. NeurIPS, 2020."
REFERENCES,0.9456521739130435,"Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Un-"
REFERENCES,0.9483695652173914,"terthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, and
Alexey Dosovitskiy. Mlp-mixer: An all-mlp architecture for vision, 2021."
REFERENCES,0.9510869565217391,"Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel Schoenholz, and Jeffrey Pennington."
REFERENCES,0.9538043478260869,"Dynamical isometry and a mean ﬁeld theory of CNNs: How to train 10,000-layer vanilla convo-
lutional neural networks. In International Conference on Machine Learning, 2018."
REFERENCES,0.9565217391304348,"Lechao Xiao, Jeffrey Pennington, and Samuel S Schoenholz. Disentangling trainability and gener-"
REFERENCES,0.9592391304347826,"alization in deep learning. In International Conference on Machine Learning, 2020."
REFERENCES,0.9619565217391305,Sho Yaida. Non-Gaussian processes and neural networks at ﬁnite widths. In Mathematical and
REFERENCES,0.9646739130434783,"Scientiﬁc Machine Learning Conference, 2020."
REFERENCES,0.967391304347826,"Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior,"
REFERENCES,0.970108695652174,"gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760,
2019."
REFERENCES,0.9728260869565217,"Greg Yang.
Tensor programs ii: Neural tangent kernel for any architecture.
arXiv preprint
arXiv:2006.14548, 2020."
REFERENCES,0.9755434782608695,"Greg Yang, Jeffrey Pennington, Vinay Rao, Jascha Sohl-Dickstein, and Samuel S. Schoenholz. A"
REFERENCES,0.9782608695652174,"mean ﬁeld theory of batch normalization. In International Conference on Learning Representa-
tions, 2019."
REFERENCES,0.9809782608695652,"Sergey Zagoruyko and Nikos Komodakis.
Wide residual networks.
In British Machine Vision
Conference, 2016."
REFERENCES,0.9836956521739131,"Hongyi Zhang, Yann N Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without"
REFERENCES,0.9864130434782609,"normalization. arXiv preprint arXiv:1901.09321, 2019."
REFERENCES,0.9891304347826086,"Yufan Zhou, Zhenyi Wang, Jiayi Xian, Changyou Chen, and Jinhui Xu. Meta-learning with neural"
REFERENCES,0.9918478260869565,"tangent kernels. In International Conference on Learning Representations, 2021. URL https:
//openreview.net/forum?id=Ti87Pv5Oc8."
REFERENCES,0.9945652173913043,Barret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. 2017. URL
REFERENCES,0.9972826086956522,https://arxiv.org/abs/1611.01578.
