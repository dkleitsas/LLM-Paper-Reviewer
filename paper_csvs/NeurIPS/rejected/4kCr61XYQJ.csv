Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0009107468123861566,"Bayesian methodologies for handling count-valued time series have gained promi-
1"
ABSTRACT,0.0018214936247723133,"nence due to their ability to infer interpretable latent structures and to estimate
2"
ABSTRACT,0.00273224043715847,"uncertainties, and thus are especially suitable for dealing with noisy and incomplete
3"
ABSTRACT,0.0036429872495446266,"count data. Among these Bayesian models, Poisson-Gamma Dynamical Systems
4"
ABSTRACT,0.004553734061930784,"(PGDSs) are proven to be effective in capturing the evolving dynamics underlying
5"
ABSTRACT,0.00546448087431694,"observed count sequences. However, the state-of-the-art PGDS still falls short in
6"
ABSTRACT,0.006375227686703097,"capturing the time-varying transition dynamics that are commonly observed in
7"
ABSTRACT,0.007285974499089253,"real-world count time series. To mitigate this limitation, a non-stationary PGDS
8"
ABSTRACT,0.00819672131147541,"is proposed to allow the underlying transition matrices to evolve over time, and
9"
ABSTRACT,0.009107468123861567,"the evolving transition matrices are modeled by the specifically-designed Dirich-
10"
ABSTRACT,0.010018214936247723,"let Markov chains. Leveraging Dirichlet-Multinomial-Beta data augmentation
11"
ABSTRACT,0.01092896174863388,"techniques, a fully-conjugate and efficient Gibbs sampler is developed to perform
12"
ABSTRACT,0.011839708561020037,"posterior simulation. Experiments show that, in comparison with related models,
13"
ABSTRACT,0.012750455373406194,"the proposed non-stationary PGDS achieves improved predictive performance
14"
ABSTRACT,0.01366120218579235,"due to its capacity to learn non-stationary dependency structure captured by the
15"
ABSTRACT,0.014571948998178506,"time-evolving transition matrices.
16"
INTRODUCTION,0.015482695810564663,"1
Introduction
17"
INTRODUCTION,0.01639344262295082,"In recent years, there has been an increasing interest in modeling count time series. For instance,
18"
INTRODUCTION,0.017304189435336976,"some previous works [1, 2, 3] are concerned with how to learn the evolving topics behind text
19"
INTRODUCTION,0.018214936247723135,"corpus (frequencies of words) over time. Some works [4, 5, 6, 7] try to predict global immigrant
20"
INTRODUCTION,0.01912568306010929,"trends underlying international population movements. Count time series are often overdispersed,
21"
INTRODUCTION,0.020036429872495445,"sparse, high-dimensional, and thus can not be well modeled by widely used dynamic models such
22"
INTRODUCTION,0.020947176684881604,"as linear dynamical systems [8, 9]. Recently, many works [10, 11, 12, 13, 14, 15, 16] prefer to
23"
INTRODUCTION,0.02185792349726776,"choose distributions of the gamma-Poisson family to build their hierarchical Bayesian models. In
24"
INTRODUCTION,0.022768670309653915,"particular, these models enjoy strong explainability and can estimate uncertainty especially when the
25"
INTRODUCTION,0.023679417122040074,"observations are noisy and incomplete. Among these works, Poisson-Gamma Dynamical Systems
26"
INTRODUCTION,0.02459016393442623,"(PGDSs) [13] received a lot of attention because PGDS can learn how the latent dimensions excite
27"
INTRODUCTION,0.025500910746812388,"each other to capture complicated dynamics in observed count series. For instance, a very inspiring
28"
INTRODUCTION,0.026411657559198543,"research paper may motivate other researchers to publish papers on related topics [17]. The outbreak
29"
INTRODUCTION,0.0273224043715847,"of COVID-19 in one state, may lead to the rapid rising of COVID-19 cases in the nearby states and
30"
INTRODUCTION,0.028233151183970857,"vice versa [18]. In particular, PGDS can be efficiently learned with a tractable Gibbs sampling scheme
31"
INTRODUCTION,0.029143897996357013,"via Poisson-Logarithmic data augmentation and marginalization technique [11]. Due to its strong
32"
INTRODUCTION,0.030054644808743168,"flexibility, PGDS achieves better performance in predicting missing entities and future observations,
33"
INTRODUCTION,0.030965391621129327,"compared with related models [9, 15].
34"
INTRODUCTION,0.031876138433515486,"Despite these advantages, PGDS still can not capture the time-varying transition dynamics underlying
35"
INTRODUCTION,0.03278688524590164,"observed count sequences, which are commonly observed in real-world scenarios [19]. For instance,
36"
INTRODUCTION,0.033697632058287796,"during the initial stage of the COVID-19 pandemic, the worldwide counts of infectious patients were
37"
INTRODUCTION,0.03460837887067395,"significantly affected by various local policies, government interventions, and emergent events [20,
38"
INTRODUCTION,0.03551912568306011,"21, 22]. The cross transition dynamics among the different monitoring areas were also evolving as
39"
INTRODUCTION,0.03642987249544627,"the corresponding policies and interventions changed over time. Hence, PGDS unavoidably makes a
40"
INTRODUCTION,0.037340619307832425,"certain amount of approximation error in capturing the aforementioned non-stationary count time
41"
INTRODUCTION,0.03825136612021858,"series, using a time-invariant transition kernel.
42"
INTRODUCTION,0.039162112932604735,"To mitigate this limitation, Non-Stationary Poisson-Gamma Dynamical Systems (NS-PGDSs), a novel
43"
INTRODUCTION,0.04007285974499089,"kind of Poisson-gamma dynamical systems with non-stationary transition dynamics are developed.
44"
INTRODUCTION,0.040983606557377046,"More specifically, NS-PGDS captures the evolving transition dynamics by the specifically-designed
45"
INTRODUCTION,0.04189435336976321,"Dirichlet Markov chains. Via the Dirichlet-Multinomial-Beta data augmentation strategy, the Non-
46"
INTRODUCTION,0.042805100182149364,"Stationary Poisson-Gamma Dynamical Systems can be inferred with a conjugate-yet-efficient Gibbs
47"
INTRODUCTION,0.04371584699453552,"sampler. Our contributions are summarized as follows:
48"
INTRODUCTION,0.044626593806921674,"• We propose a Non-Stationary Poisson-Gamma Dynamical System (NS-PGDS), a novel
49"
INTRODUCTION,0.04553734061930783,"Poisson-gamma dynamical system with time-evolving transition matrices that can well
50"
INTRODUCTION,0.04644808743169399,"capture non-stationary transition dynamics underlying observed count series.
51"
INTRODUCTION,0.04735883424408015,"• Three Dirichlet Markov chains are dedicated to improving the flexibility and expressiveness
52"
INTRODUCTION,0.0482695810564663,"of NS-PGDSs, for capturing the complex transition dynamics behind sequential count data.
53"
INTRODUCTION,0.04918032786885246,"• Fully-conjugate-yet-efficient Gibbs samplers are developed via Dirichlet-Multinomial-Beta
54"
INTRODUCTION,0.05009107468123861,"augmentation techniques to perform posterior simulation for the proposed Dirichlet Markov
55"
INTRODUCTION,0.051001821493624776,"chains.
56"
INTRODUCTION,0.05191256830601093,"• Extensive experiments are conducted on four real-world datasets, to evaluate the performance
57"
INTRODUCTION,0.052823315118397086,"of the proposed NS-PGDS in predicting missing and future unseen observations. We also
58"
INTRODUCTION,0.05373406193078324,"provide exploratory analysis to demonstrate the explainable latent structure inferred by the
59"
INTRODUCTION,0.0546448087431694,"proposed NS-PGDS.
60"
PRELIMINARIES,0.05555555555555555,"2
Preliminaries
61"
PRELIMINARIES,0.056466302367941715,"Let y(t) =
h
y(t)
1 , · · · , y(t)
V
iT
∈NV be a vector of nonnegative count valued observations at time t.
62"
PRELIMINARIES,0.05737704918032787,"To capture the latent dynamics underlying count sequences, some previous works [23, 24] model the
63"
PRELIMINARIES,0.058287795992714025,"observations as
64"
PRELIMINARIES,0.05919854280510018,"y(t) = p

z(t)
, z(t) = f −1 
x(t)
,"
PRELIMINARIES,0.060109289617486336,"where p (·) is the observation likelihood function, and f (·) is an invertible link function that maps
65"
PRELIMINARIES,0.0610200364298725,"the parameters of observation component to continuous-valued latent variables x(t) ∈RK. The
66"
PRELIMINARIES,0.061930783242258654,"latent factor x(t) evolves over time according to a linear dynamical system (LDS) given by x(t) ∼
67"
PRELIMINARIES,0.06284153005464481,"N(Ax(t−1), Λ−1), where A is the state transition matrix of size K×K, and Λ = diag (λ1, · · · , λK)
68"
PRELIMINARIES,0.06375227686703097,"is the inverse covariance matrix with λ−1
k
determining the variance of k-th latent dimension. Han
69"
PRELIMINARIES,0.06466302367941712,"et al. [23] adopted the Extended Rank likelihood function to model count observations using LDS
70"
PRELIMINARIES,0.06557377049180328,"with time complexity O((K + V )3), which prevents it from practical applications for analyzing
71"
PRELIMINARIES,0.06648451730418943,"large-scale count data.
72"
PRELIMINARIES,0.06739526411657559,"Recently, Acharya et al. [15] and Schein et al. [13, 16] developed Poisson-gamma family models for
73"
PRELIMINARIES,0.06830601092896176,"sequential count observations. Gamma Process Dynamic Poisson Factor Analysis (GP-DPFA) [15]
74"
PRELIMINARIES,0.0692167577413479,"models count data as y(t)
v
∼Pois(PK
k=1 λkϕvkθ(t)
k ), where θ(t)
k
represents the strength of k-th latent
75"
PRELIMINARIES,0.07012750455373407,"factor at time t, and ϕvk captures the involvement degree of k-th factor to v-th observed dimension.
76"
PRELIMINARIES,0.07103825136612021,"To ensure the model identifiability, we can impose a restriction as P"
PRELIMINARIES,0.07194899817850638,"v ϕvk = 1, and thus place a
77"
PRELIMINARIES,0.07285974499089254,"Dirichlet prior over ϕk = [ϕ1k, · · · , ϕV k]T as ϕk ∼Dir (ϵ0, · · · , ϵ0).
78"
PRELIMINARIES,0.07377049180327869,"To capture the underlying dynamics, the latent factor θ(t)
k
evolves over time according to a gamma
79"
PRELIMINARIES,0.07468123861566485,"Markov chain as θ(t)
k
∼Gam(θ(t−1)
k
, ct), where ct is the rate parameter of the gamma distribution to
80"
PRELIMINARIES,0.075591985428051,"control the variance of the gamma Markov chains. Although GP-DPFA can well fit one-dimensional
81"
PRELIMINARIES,0.07650273224043716,"count sequences, it fails to learn how the latent dimensions interact with each other.
82"
PRELIMINARIES,0.07741347905282331,"To address this concern, Schein et al. [13] developed Poisson-gamma dynamical systems to
83"
PRELIMINARIES,0.07832422586520947,"capture the underlying transition dynamics.
In particular, θ(t)
k
evolves over time as θ(t)
k
∼
84"
PRELIMINARIES,0.07923497267759563,"Figure 1: The graphical representation of the NS-PGDS. The time interval is divided into equally-
spaced sub-intervals. Each sub-interval contains M time steps. The transition dynamics is stationary
within a sub-interval. In particular, the transition matrices evolve over sub-intervals via Dirichlet
Markov processes while latent factors evolve over time steps via Eq.(1)."
PRELIMINARIES,0.08014571948998178,"Gam(τ0
PK
k2=1 πkk2θ(t−1)
k2
, τ0), where πkk2 represents how k2-th latent factor excites the k-th latent
85"
PRELIMINARIES,0.08105646630236794,"factor at next time step, and PK
k=1 πkk2 = 1.
86"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.08196721311475409,"3
Non-Stationary Poisson-Gamma Dynamical Systems
87"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.08287795992714025,"Figure 2: An example illustrates the Poisson-gamma
dynamical systems with non-stationary transition
kernels. The three gamma dynamic processes in-
dependently evolve over time during the (i −1)-th
interval. During i-th interval, θ(t)
1
and θ(t)
2
gradually
starts to interact with each other while θ(t)
3
remains
independent to the other two dimensions. During
(i + 1)-th interval all the three latent components
start to interact with each other."
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.08378870673952642,"Real-world count time sequences are often non-
88"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.08469945355191257,"stationary because the external interventional
89"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.08561020036429873,"environments are always changing over time.
90"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.08652094717668488,"The stationary PGDS with a time-invariant tran-
91"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.08743169398907104,"sition kernel fails to capture such time-varying
92"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.0883424408014572,"transition dynamics. For instance, the tran-
93"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.08925318761384335,"sition dynamics behind COVID-19 infectious
94"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.09016393442622951,"processes are time-varying, and highly affected
95"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.09107468123861566,"by various interventional policies. Hence, to
96"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.09198542805100182,"mitigate this limitation, we model the count
97"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.09289617486338798,"sequences as
98"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.09380692167577413,"y(t)
v
∼Pois

δ(t) PK
k=1 ϕvkθ(t)
k

,"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.0947176684881603,"in which, the latent factors are specified by
99"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.09562841530054644,"θ(t)
k
∼Gam

τ0
PK
k2=1 π(t−1)
kk2
θ(t−1)
k2
, τ0

,
(1)
where
the
multiplicative
term
δ(t)
∼
100"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.0965391621129326,"Gam (ϵ0, ϵ0) and the transition matrices are
101"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.09744990892531877,"time-varying as Π(t) ≡
h
π(t)
kk2 iK"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.09836065573770492,"k,k2=1.
As
102"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.09927140255009108,"shown in Figure 2, to model the time-varying
103"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.10018214936247723,"transition dynamics, we assume the whole time
104"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.10109289617486339,"interval can be divided into I equally-spaced sub-intervals. The transition kernel behind complicated
105"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.10200364298724955,"dynamic counts is assumed to be static within each sub-interval, while evolving over sub-intervals,
106"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.1029143897996357,"to capture non-stationary behaviours. In another word, the proposed model allows the latent factors
107"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.10382513661202186,"to evolve over time steps while the transition matrices change over sub-intervals but assumed to be
108"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.10473588342440801,"stationary within each sub-interval, as shown in Figure 1. In particular, we let each sub-interval
109"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.10564663023679417,"contains M time steps, and the i-th interval contains time steps {t | t = (i −1) M + 1, · · · , iM}.
110"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.10655737704918032,"We define i (t) as the function that maps time step t to its corresponding sub-interval.
111"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.10746812386156648,"Dirichlet-Dirichlet Markov processes. To capture how the underlying transition kernel smoothly
112"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.10837887067395265,"evolves over sub-intervals, we first propose the Dirichlet-Dirichlet (Dir-Dir) Markov chain as
113"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.1092896174863388,"π(i)
k
| π(i−1)
k
∼Dir

ηKπ(i−1)
1k
, · · · , ηKπ(i−1)
Kk

,
(2)"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.11020036429872496,"where π(i)
k
represents the k-th column of Π(i), and the prior of the scaling parameter η is given by
114"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.1111111111111111,"η ∼Gam (e0, f0).
115"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.11202185792349727,"The initial states are defined as θ(1)
k
∼Gam (τ0νk, τ0).
The prior for the transition ker-
116"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.11293260473588343,"nel of the first sub-interval is given by π(1)
k
∼Dir (ν1νk, · · · , ξνk, · · · , νKνk), where νk ∼
117"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.11384335154826958,Gam( γ0
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.11475409836065574,"K , β) and ξ, β ∼Gam (ϵ0, ϵ0). Note that the expectation and variance of the transition
118"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.11566484517304189,"kernel at i-th sub-interval can be calculated as
119"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.11657559198542805,"E
h
π(i)
k
| π(i−1)
k
i
= π(i−1)
k
,
Var
h
π(i)
k1k | π(i−1)
k
i
=
π(i−1)
k1k

1 −π(i−1)
k1k
"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.11748633879781421,"ηK + 1
,"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.11839708561020036,"respectively. The transition dynamics of i-th sub-interval inherits the information of the previous
120"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.11930783242258652,"sub-interval, and also adapts to the data observed in the current sub-interval. The scaling parameter η
121"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.12021857923497267,"controls the variance of the transition matrices.
122"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.12112932604735883,"The prior specification defined in Eq.(2) by rescaling the transition matrix at the previous
123"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.122040072859745,"sub-interval allows the transition dynamics to change smoothly, and thus might be insuffi-
124"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.12295081967213115,"cient to capture the rapid changes observed in complicated dynamics.
To further improve
125"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.12386156648451731,"the flexibility of the transition structure, two modified Dirichlet Markov chains are studied to
126"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.12477231329690346,"capture the correlation structure between the dimensions of the transition matrices over time.
127"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.12568306010928962,"Figure 3:
Diagrams of the proposed
Dirichlet Markov constructions. (a) is the
Dir-Dir construction. (b) is the Dir-Gam-
Dir construction which takes mutation
into account. (c) illustrates the PR-Gam-
Dir construction which adopts Poisson
randomized gamma distribution and can
be equivalently represented as Eq.(5)."
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.12659380692167577,"Dirichlet-Gamma-Dirichlet Markov processes. We first
128"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.12750455373406194,"introduce the Dirichlet-Gamma-Dirichlet (Dir-Gam-Dir)
129"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.1284153005464481,"Markov chain to model the evolving transition matrices
130"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.12932604735883424,"as
131"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.1302367941712204,"π(i)
k
∼Dir

α(i)
1k, · · · , α(i)
Kk

,"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.13114754098360656,"α(i)
k1k ∼Gam

γ(i−1)
k
PK
k2=1 ψ(i−1)
kk1k2π(i−1)
k2k , c(i)
k

, (3)"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.1320582877959927,"where we use ψ(i−1)
kk1k2 to capture the mutation between two
132"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.13296903460837886,"consecutive sub-intervals, and its prior is given by
133"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.13387978142076504,"
ψ(i−1)
k1k2 , · · · , ψ(i−1)
kKk2"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.13479052823315119,"
∼Dir (ϵ0, · · · , ϵ0) ,"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.13570127504553733,"and γ(i)
k , c(i)
k
∼Gam (ϵ0, ϵ0). Compared with the con-
134"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.1366120218579235,"struction defined by Eq.(2), the expectation of Dirichlet-
135"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.13752276867030966,"Gamma-Dirichlet Markov chain is
136"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.1384335154826958,"E
h
π(i)
k
| π(i−1)
k
i
= Ψ(i−1)
k
π(i−1)
k
."
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.13934426229508196,"This construction takes interactions among components of columns into account. Hence it will
137"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.14025500910746813,"dramatically improve the flexibility of our model and thus better fit more complicated dynamics,
138"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.14116575591985428,"compared with Dir-Dir Markov chains that only yield smoothing transition dynamics.
139"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.14207650273224043,"Poisson-randomized-gamma-Dirichlet Markov processes. By leveraging the Poisson-randomized
140"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.1429872495446266,"gamma distribution [25], we introduce another type of time-varying transition kernels, which also
141"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.14389799635701275,"model the interactions among components like Dir-Gam-Dir construction but may induce different
142"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.1448087431693989,"properties such as sparsity. The Poisson-randomized-gamma-Dirichlet (PR-Gam-Dir) Markov chain
143"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.14571948998178508,"can be formulated as
144"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.14663023679417123,"π(i)
k
∼Dir

α(i)
1k, · · · , α(i)
Kk

, α(i)
k1k ∼RG1

ϵα, γ(i−1)
k
PK
k2=1 ψ(i−1)
kk1k2π(i−1)
k2k , c(i)
k

,
(4)"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.14754098360655737,"where RG1 (·) denotes the randomized gamma distribution of the first type. Similarly, for ψ(i−1)
kk1k2,
145"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.14845173041894352,"γ(i)
k , and c(i)
k , the priors are given by
146"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.1493624772313297,"
ψ(i−1)
k1k2 , · · · , ψ(i−1)
kKk2"
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.15027322404371585,"
∼Dir (ϵ0, · · · , ϵ0) , γ(i)
k , c(i)
k
∼Gam (ϵ0, ϵ0) , respectively."
NON-STATIONARY POISSON-GAMMA DYNAMICAL SYSTEMS,0.151183970856102,"The diagrams of three Dirichlet Markov constructions are shown in Figure 3.
147"
MARKOV CHAIN MONTE CARLO INFERENCE,0.15209471766848817,"4
Markov Chain Monte Carlo Inference
148"
MARKOV CHAIN MONTE CARLO INFERENCE,0.15300546448087432,"In this section, we present the Gibbs sampler for the proposed NS-PGDS. We only illustrate the key
149"
MARKOV CHAIN MONTE CARLO INFERENCE,0.15391621129326047,"points of the derivation and the details can be found in the appendix.
150"
MARKOV CHAIN MONTE CARLO INFERENCE,0.15482695810564662,"Lemma 1 If y ∼NB (a, g (ζ)) and l ∼CRT (y, a), where NB (·) refers to negative-binomial
151"
MARKOV CHAIN MONTE CARLO INFERENCE,0.1557377049180328,"distribution, CRT (·) represents Chinese restaurant table distribution [26], and g (z) = 1−exp (−z).
152"
MARKOV CHAIN MONTE CARLO INFERENCE,0.15664845173041894,"Then the joint distribution of y and l can be equivalently distributed as y ∼SumLog (l, g (ζ)) and
153"
MARKOV CHAIN MONTE CARLO INFERENCE,0.1575591985428051,"l ∼Pois (aζ) [11], i.e.
154"
MARKOV CHAIN MONTE CARLO INFERENCE,0.15846994535519127,"NB (y; a, g (ζ)) CRT (l; y, a) = SumLog (y; l, g (ζ)) Pois (l; aζ) ,"
MARKOV CHAIN MONTE CARLO INFERENCE,0.15938069216757741,"where SumLog (l, g (ζ)) = Pl
i=1 xi and xi ∼Log (g (ζ)) are independently and identically loga-
155"
MARKOV CHAIN MONTE CARLO INFERENCE,0.16029143897996356,"rithmic distributed random variables [27].
156"
MARKOV CHAIN MONTE CARLO INFERENCE,0.16120218579234974,"Lemma 2 Suppose n = (n1, · · · , nK) and"
MARKOV CHAIN MONTE CARLO INFERENCE,0.1621129326047359,"n | n ∼DirMult (n, r1, · · · , rK) ,"
MARKOV CHAIN MONTE CARLO INFERENCE,0.16302367941712204,"where DirMult (·) refers to Dirichlet-multimonial distribution. We sample the augmented variable
157"
MARKOV CHAIN MONTE CARLO INFERENCE,0.16393442622950818,"q | n ∼Beta (n, r·), where r· = PK
k=1 rk. According to [28], conditioning on q, we have
158"
MARKOV CHAIN MONTE CARLO INFERENCE,0.16484517304189436,"nk ∼NB (rk, q).
159"
MARKOV CHAIN MONTE CARLO INFERENCE,0.1657559198542805,"Sampling y(t)
vk : Use the relationship between Poisson and multinomial distributions, we sample
160"
MARKOV CHAIN MONTE CARLO INFERENCE,0.16666666666666666,"
y(t)
vk
K"
MARKOV CHAIN MONTE CARLO INFERENCE,0.16757741347905283,"k=1 | −

∼Mult "
MARKOV CHAIN MONTE CARLO INFERENCE,0.16848816029143898,"y(t)
v ,"
MARKOV CHAIN MONTE CARLO INFERENCE,0.16939890710382513,"ϕvkθ(t)
k
PK
k=1 ϕvkθ(t)
k !K k=1  ."
MARKOV CHAIN MONTE CARLO INFERENCE,0.1703096539162113,"Sampling ϕk : Via Dirichlet-multinomial conjugacy, the posterior of ϕk is
161"
MARKOV CHAIN MONTE CARLO INFERENCE,0.17122040072859745,"(ϕk | −) ∼Dir

ϵ0 + PT
t=1 y(t)
1k , · · · , ϵ0 + PT
t=1 y(t)
V k

."
MARKOV CHAIN MONTE CARLO INFERENCE,0.1721311475409836,"Sampling θ(t)
k
: To sample from the posterior of θ(t)
k , we first sample the auxiliary variables. Setting
162"
MARKOV CHAIN MONTE CARLO INFERENCE,0.17304189435336975,"l(T +1)
·k
= 0 and ζ(T +1) = 0, we sample the augmented variables backwards from t = T, · · · , 2,
163"
MARKOV CHAIN MONTE CARLO INFERENCE,0.17395264116575593,"
l(t)
k· | −

∼CRT

y(t)
·k + l(t+1)
·k
, τ0
PK
k2=1 πi(t−1)
kk2
θ(t−1)
k2 
,"
MARKOV CHAIN MONTE CARLO INFERENCE,0.17486338797814208,"
l(t)
k1 , · · · , l(t)
kK | −

∼Mult "
MARKOV CHAIN MONTE CARLO INFERENCE,0.17577413479052822,"l(t)
k· ,"
MARKOV CHAIN MONTE CARLO INFERENCE,0.1766848816029144,"πi(t−1)
k1
θ(t−1)
1
PK
k2=1 πi(t−1)
kk2
θ(t−1)
k2
, · · · ,
πi(t−1)
kK
θ(t−1)
K
PK
k2=1 πi(t−1)
kk2
θ(t−1)
k2 !! ."
MARKOV CHAIN MONTE CARLO INFERENCE,0.17759562841530055,"Let us define l(t)
·k = PK
k1=1 l(t)
k1k and ζ(t) = ln(1 + δ(t)"
MARKOV CHAIN MONTE CARLO INFERENCE,0.1785063752276867,"τ0 + ζ(t+1)). After sampling the auxiliary
164"
MARKOV CHAIN MONTE CARLO INFERENCE,0.17941712204007285,"variables, then for t = 1, · · · , T, by Poisson-gamma conjugacy, we obtain
165"
MARKOV CHAIN MONTE CARLO INFERENCE,0.18032786885245902,"
θ(t)
k
| −

∼Gam

y(t)
·k + l(t+1)
·k
+ τ0
PK
k2=1 πi(t−1)
kk2
θ(t−1)
k2
, τ0 + δ(t) + ζ(t+1)τ0

."
MARKOV CHAIN MONTE CARLO INFERENCE,0.18123861566484517,"Sampling Π(i) : We only illustrate Gibbs sampling algorithm for PR-Gam-Dir construction, sampling
166"
MARKOV CHAIN MONTE CARLO INFERENCE,0.18214936247723132,"algorithms for other constructions can be found in the appendix. We define M as the length of each
167"
MARKOV CHAIN MONTE CARLO INFERENCE,0.1830601092896175,"sub-interval, and I as the number of intervals. For i = I, · · · , 2, because (l(i)
1k , · · · , l(i)
Kk) and
168"
MARKOV CHAIN MONTE CARLO INFERENCE,0.18397085610200364,"(g(i+1)
·1k
, · · · , g(i+1)
·Kk ) are multinomially distributed, where l(i)
k1k = PiM
(i−1)M+1 l(t)
k1k refers to the
169"
MARKOV CHAIN MONTE CARLO INFERENCE,0.1848816029143898,"summation of l(t)
k1k over i-th sub-interval and same notation for other variables. By the definition
170"
MARKOV CHAIN MONTE CARLO INFERENCE,0.18579234972677597,"of Dirichlet-multinomial distribution and Lemma 2, defining g(I+1)
k1k
= 0, we sample the auxiliary
171"
MARKOV CHAIN MONTE CARLO INFERENCE,0.18670309653916212,"variables as (q(i)
k
| −) ∼Beta(l(i)
·k + g(i+1)
·k
, α(i)
·k ), then we have (l(i)
k1k + g(i+1)
·k1k ) ∼NB(α(i)
k1k, q(i)
k ).
172"
MARKOV CHAIN MONTE CARLO INFERENCE,0.18761384335154827,"Then we further sample (h(i)
k1k | −) ∼CRT(l(i)
k1k + g(i+1)
·k1k , α(i)
k1k). Via Lemma 1, we obtain h(i)
k1k ∼
173"
MARKOV CHAIN MONTE CARLO INFERENCE,0.1885245901639344,"Pois(−α(i)
k1kln(1 −q(i)
k )). For Dirichlet-Randomized-Gamma-Dirichlet Markov construction defined
174"
MARKOV CHAIN MONTE CARLO INFERENCE,0.1894353369763206,"by Eq.(4), we can equivalently represent it as
175"
MARKOV CHAIN MONTE CARLO INFERENCE,0.19034608378870674,"α(i)
k1k ∼Gam

g(i)
k1k + ϵα, c(i)
k

, g(i)
k1k = Pois

γ(i−1) PK
k2=1 ψ(i−1)
kk1k2π(i−1)
k2k

.
(5)"
MARKOV CHAIN MONTE CARLO INFERENCE,0.1912568306010929,"We define λ(i−1)
k1k
≜γ(i−1)
k
PK
k2=1 ψ(i−1)
kk1k2π(i−1)
k2k
for notation conciseness. By Poisson-gamma
176"
MARKOV CHAIN MONTE CARLO INFERENCE,0.19216757741347906,"conjugacy, we have (α(i)
k1k | −) ∼Gam(g(i)
k1k + ϵα + h(i)
k1k, c(i)
k −ln(1 −q(i)
k )). If ϵα > 0, we can
177"
MARKOV CHAIN MONTE CARLO INFERENCE,0.1930783242258652,"sample the posterior of g(i)
k1k via (g(i)
k1k | −) ∼Bessel(ϵα −1, 2
q"
MARKOV CHAIN MONTE CARLO INFERENCE,0.19398907103825136,"α(i)
k1kc(i)
k λ(i−1)
k1k
), where Bessel (·)
178"
MARKOV CHAIN MONTE CARLO INFERENCE,0.19489981785063754,"denotes Bessel distribution. If ϵα = 0, we sample g(i)
k1k via
179"
MARKOV CHAIN MONTE CARLO INFERENCE,0.19581056466302368,"
g(i)
k1k | −

∼"
MARKOV CHAIN MONTE CARLO INFERENCE,0.19672131147540983,"


 

"
MARKOV CHAIN MONTE CARLO INFERENCE,0.19763205828779598,"Pois

c(i)
k λ(i−1)
k1k"
MARKOV CHAIN MONTE CARLO INFERENCE,0.19854280510018216,"c(i)
k −ln

1−q(i)
k


if h(i)
k1k = 0"
MARKOV CHAIN MONTE CARLO INFERENCE,0.1994535519125683,"SCH

h(i)
k1k,
c(i)
k λ(i−1)
k1k"
MARKOV CHAIN MONTE CARLO INFERENCE,0.20036429872495445,"c(i)
k −ln

1−q(i)
k


otherwise,"
MARKOV CHAIN MONTE CARLO INFERENCE,0.20127504553734063,"where SCH (·) denotes the shifted confluent hypergeometric distribution [16]. Defining g(i)
k1k =
180"
MARKOV CHAIN MONTE CARLO INFERENCE,0.20218579234972678,"g(i)
k1·k = PK
k2=1 g(i)
k1k2k, we first augment
181"
MARKOV CHAIN MONTE CARLO INFERENCE,0.20309653916211293,"
g(i)
k11k, · · · , g(i)
k1Kk

∼Mult

g(i)
k1k,

ψ(i−1)
kk1k2π(i−1)
k2k
K k2=1 
,"
MARKOV CHAIN MONTE CARLO INFERENCE,0.2040072859744991,"then we obtain g(i)
k1k2k ∼Pois(γ(i−1)ψ(i−1)
kk1k2π(i−1)
k2k ). By Dirichlet-multinomial conjugacy, we have
182"
MARKOV CHAIN MONTE CARLO INFERENCE,0.20491803278688525,"
ψ(i−1)
k1k2 , · · · , ψ(i−1)
kKk2"
MARKOV CHAIN MONTE CARLO INFERENCE,0.2058287795992714,"
| −

∼Dir

ϵ0 + g(i)
1k2k, · · · , ϵ0 + g(i)
Kk2k

, and

π(i−1)
k
| −

∼Dir

α(i−1)
1k
+ l(i−1)
1k
+ g(i)
·1k, · · · , α(i−1)
Kk
+ l(i−1)
Kk
+ g(i)
·Kk

."
MARKOV CHAIN MONTE CARLO INFERENCE,0.20673952641165755,"Specifically, we have α(1)
k1k = νk1νk, if k1 ̸= k, and α(1)
k1k = ξνk, if k1 = k.
183"
RELATED WORK,0.20765027322404372,"5
Related Work
184"
RELATED WORK,0.20856102003642987,"Modeling count time sequences has been receiving increasing attentions in statistical and machine
185"
RELATED WORK,0.20947176684881602,"learning communities. Han et al. [23] adopted linear dynamical systems to capture the underlying
186"
RELATED WORK,0.2103825136612022,"dynamics of the data and leveraged Extended Rank likelihood function to model count observations.
187"
RELATED WORK,0.21129326047358835,"Some Poisson-gamma models assume that the count vector at each time step is modeled by Poisson
188"
RELATED WORK,0.2122040072859745,"factor analysis (PFA) [11] and leverage special stochastic processes to model the temporal dependen-
189"
RELATED WORK,0.21311475409836064,"cies of latent factors. For example, gamma process dynamic Poisson factor analysis (GP-DPFA) [15]
190"
RELATED WORK,0.21402550091074682,"adopts gamma Markov chains which assumes the latent factor of the next time step is drawn from
191"
RELATED WORK,0.21493624772313297,"a gamma distribution with the shape parameter be the latent factor of the current time step. Schein
192"
RELATED WORK,0.21584699453551912,"et al. [13] proposed Poisson-gamma dynamical systems (PGDSs), which take the interactions among
193"
RELATED WORK,0.2167577413479053,"latent dimensions into account and use a transition matrix to capture the interactions. Deep dynamic
194"
RELATED WORK,0.21766848816029144,"Poisson factor analysis (DDPFA) [29] adopts recurrent neural networks (RNNs) to capture the com-
195"
RELATED WORK,0.2185792349726776,"plex long-term dependencies of latent factors. Yang and Koeppl [30] applied Poisson-gamma count
196"
RELATED WORK,0.21948998178506376,"model to analyze relational data arising from longitudinal networks, which can capture the evolution
197"
RELATED WORK,0.2204007285974499,"of individual node-group memberships over time. Many modifications of PGDS have been proposed
198"
RELATED WORK,0.22131147540983606,"in recent years. Guo et al. [31] proposed deep Poisson-gamma dynamical systems which aim to
199"
RELATED WORK,0.2222222222222222,"capture the long-range temporal dependencies. Schein et al. [16] employed Poisson-randomized
200"
RELATED WORK,0.22313296903460839,"gamma distribution to build a new transition process of latent factors. Chen et al. [32] proposed
201"
RELATED WORK,0.22404371584699453,"Switching Poisson-gamma dynamical systems (SPGDS), allowing PGDS to select from several tran-
202"
RELATED WORK,0.22495446265938068,"sition matrices, and thus can better adapt to nonlinear dynamics. In contrast to SPGDS, the number
203"
RELATED WORK,0.22586520947176686,"of transition matrices of the proposed NS-PGDS is not limited and thus can be adopted to analyze
204"
RELATED WORK,0.226775956284153,"various complicated non-stationary count sequences. Filstroff et al. [33] extensively analyzed many
205"
RELATED WORK,0.22768670309653916,"gamma Markov chains for non-negative matrix factorization and introduced new gamma Markov
206"
RELATED WORK,0.22859744990892533,"chains with well-defined stationary distribution (BGAR).
207"
EXPERIMENTS,0.22950819672131148,"6
Experiments
208"
EXPERIMENTS,0.23041894353369763,"We conducted experiments for both predictive and exploratory analysis to demonstrate the ability of
209"
EXPERIMENTS,0.23132969034608378,"the proposed model in capturing non-stationary count time sequences. The baseline models included
210"
EXPERIMENTS,0.23224043715846995,"in the experiments are: 1) Gamma process dynamic Poisson factor analysis (GP-DPFA) [15].
211"
EXPERIMENTS,0.2331511839708561,"GP-DPFA models the evolution of latent components as θ(t)
k
∼Gam(θ(t−1)
k
, ct), in which each
212"
EXPERIMENTS,0.23406193078324225,"component evolves independently of the other components. 2) Gamma Markov chains on the
213"
EXPERIMENTS,0.23497267759562843,"rate parameter of gamma distribution (GMC-RATE) [33]. GMC-RATE adopts gamma Markov
214"
EXPERIMENTS,0.23588342440801457,"chains defined via the rate parameter of the gamma distribution to model the evolution of θ(t)
k
215"
EXPERIMENTS,0.23679417122040072,"GP-DPFA
GMC-RATE
GMC-HIER
BGAR
PGDS
NS-PGDS
(Dir-Dir)
NS-PGDS
(Dir-Gam-Dir)
NS-PGDS
(PR-Gam-Dir)"
EXPERIMENTS,0.23770491803278687,"ICEWS
MAE S
0.259 ±0.005
0.258 ±0.005
0.256 ±0.006
0.264 ±0.006
0.215 ±0.007
0.215 ±0.008
0.214 ±0.008
0.215 ±0.008
F
0.176 ±0.005
0.187 ±0.003
0.185 ±0.016
0.222 ±0.043
0.185 ±0.003
0.167 ±0.009
0.169 ±0.006
0.169 ±0.009
MRE S
0.125 ±0.003
0.124 ±0.002
0.122 ±0.003
0.130 ±0.004
0.102 ±0.005
0.101 ±0.005
0.101 ±0.005
0.102 ±0.005
F
0.099 ±0.006
0.114 ±0.003
0.111 ±0.018
0.142 ±0.036
0.108 ±0.001
0.094 ±0.005
0.097 ±0.004
0.097 ±0.008"
EXPERIMENTS,0.23861566484517305,"NIPS
MAE S 18.299 ±6.545 17.105 ±6.449
17.098 ±6.441
17.935 ±6.450 14.706 ±4.414 14.032 ±4.401 14.026 ±4.405 14.014 ±4.387
F 48.355 ±1.461 46.234 ±1.629 102.506 ±39.932 62.449 ±14.463 51.562 ±0.679 45.979 ±1.342 46.710 ±1.152 46.582 ±1.196
MRE S
0.729 ±0.412
0.684 ±0.316
0.664 ±0.315
0.769 ±0.366
0.590 ±0.097
0.581 ±0.090
0.581 ±0.090
0.580 ±0.090
F
0.415 ±0.016
0.387 ±0.023
0.580 ±0.148
0.465 ±0.049
0.459 ±0.006
0.399 ±0.003
0.395 ±0.006
0.397 ±0.003"
EXPERIMENTS,0.2395264116575592,"USEI
MAE S
4.681 ±0.564
4.931 ±0.872
4.748 ±0.829
5.244 ±0.939
4.703 ±0.538
4.600 ±0.542
4.608 ±0.541
4.596 ±0.562
F 11.665 ±0.367
9.454 ±0.809
12.423 ±1.060
21.948 ±0.133 11.118 ±0.220
7.973 ±1.222
7.168 ±1.221
7.296 ±1.127
MRE S
1.458 ±0.177
1.128 ±0.189
1.088 ±0.162
1.941 ±0.209
1.279 ±0.257
1.309 ±0.220
1.298 ±0.236
1.301 ±0.229
F
7.473 ±0.623
6.508 ±0.571
8.929 ±2.514
13.706 ±1.268
4.238 ±0.325
2.602 ±0.455
2.577 ±0.331
2.685 ±0.366"
EXPERIMENTS,0.24043715846994534,"COVID-19 MAE S
7.935 ±0.751
7.144 ±1.159
7.240 ±0.848
7.819 ±1.348
7.566 ±1.095
6.969 ±1.107
6.988 ±1.056
6.981 ±1.022
F
9.137 ±1.102
9.600 ±1.257
10.409 ±1.910
12.550 ±2.156
9.314 ±0.236
8.799 ±0.706
8.770 ±0.438
9.033 ±0.477
MRE S
0.564 ±0.126
0.493 ±0.136
0.504 ±0.109
0.769 ±0.169
0.558 ±0.130
0.523 ±0.125
0.525 ±0.124
0.526 ±0.123
F
0.627 ±0.106
0.556 ±0.052
0.585 ±0.067
0.759 ±0.150
0.585 ±0.007
0.523 ±0.028
0.519 ±0.017
0.513 ±0.014"
EXPERIMENTS,0.24134790528233152,"Table 1: Results of predictive analysis. ""S"" means data smoothing and ""F"" means data forecasting."
EXPERIMENTS,0.24225865209471767,"as θ(t)
k
∼Gam(α, β/θ(t−1)
k
). 3) Gamma Markov chains on the rate parameter with hierarchical
216"
EXPERIMENTS,0.24316939890710382,"auxiliary variable (GMC-HIER) [33]. GMC-HIER models the evolution of latent components with
217"
EXPERIMENTS,0.24408014571949,"an auxiliary variables as z(t)
k
∼Gam(αz, βzθ(t−1)
k
) and θ(t)
k
∼Gam(aθ, βθz(t)
k ). 4) Autogressive
218"
EXPERIMENTS,0.24499089253187614,"beta-gamma procecss (BGAR) [34, 33]. BGAR is also a gamma Markov model. In contrast to
219"
EXPERIMENTS,0.2459016393442623,"the above models, there is a well-defined stationary distribution for BGAR. 5) Poisson-gamma
220"
EXPERIMENTS,0.24681238615664844,"dynamical system (PGDS) [13] takes interactions among latent dimensions into account, and models
221"
EXPERIMENTS,0.24772313296903462,"the evolution of θ(t)
k
as θ(t)
k
∼Gam(τ0
PK
k2=1 πkk2θ(t−1)
k2
, τ0).
222"
EXPERIMENTS,0.24863387978142076,"The real-world datasets used in the experiments are: 1) Integrated Crisis Early Warning System
223"
EXPERIMENTS,0.2495446265938069,"(ICEWS): ICEWS is an international relations event dataset, comprising interaction events between
224"
EXPERIMENTS,0.25045537340619306,"countries extracted from news corpora. For ICEWS dataset, we have T = 365 time steps and
225"
EXPERIMENTS,0.25136612021857924,"V = 6197 dimensions, and we set M = 30. 2) NIPS: NIPS dataset contains the papers published in
226"
EXPERIMENTS,0.2522768670309654,"the NeurIPS conference from 1987 to 2015. We have T = 28 time steps and V = 2000 dimensions
227"
EXPERIMENTS,0.25318761384335153,"for NIPS dataset and we set M = 5. 3) U.S. Earthquake Intensity (USEI): USEI contains a
228"
EXPERIMENTS,0.2540983606557377,"collection of damage and felt reports for U.S. (and a few other countries) earthquakes. We use the
229"
EXPERIMENTS,0.2550091074681239,"monthly reports from 1957-1986 and have T = 348, V = 64 and set M = 34. 4) COVID-19: This
230"
EXPERIMENTS,0.25591985428051,"dataset contains daily death cases data for states in the United States, spanning from March 2020 to
231"
EXPERIMENTS,0.2568306010928962,"June 2020. For this dataset, we have V = 51 dimensions and T = 90 time steps and set M = 20.
232"
PREDICTIVE ANALYSIS,0.25774134790528236,"6.1
Predictive Analysis
233"
PREDICTIVE ANALYSIS,0.2586520947176685,"To compare the predictive performance of the proposed model with the baselines, we considered two
234"
PREDICTIVE ANALYSIS,0.25956284153005466,"standard tasks: data smoothing and forecasting. For data smoothing task, our objective is to predict
235"
PREDICTIVE ANALYSIS,0.2604735883424408,"y(t) given the remaining data observation Y \y(t). To this end, we randomly masked 10 percents of
236"
PREDICTIVE ANALYSIS,0.26138433515482695,"the observed data over non-adjacent time steps, and predicted the masked values. For forecasting task,
237"
PREDICTIVE ANALYSIS,0.26229508196721313,"we held out data of the last S time steps, and predicted y(T +1), · · · , y(T +S) given y(1), · · · , y(T ). In
238"
PREDICTIVE ANALYSIS,0.26320582877959925,"this experiment we set S = 2. We ran the baseline models including GP-DPFA, PGDS, GMC-RATE,
239"
PREDICTIVE ANALYSIS,0.2641165755919854,"GMC-HIER, BGAR, using their default settings as provided in [15, 13, 33]. For the NS-PGDS, we set
240"
PREDICTIVE ANALYSIS,0.2650273224043716,"K = 100 for ICEWS, K = 10 for other datasets, and set τ0 = 1, γ0 = 50, ϵ0 = 0.1. We performed
241"
PREDICTIVE ANALYSIS,0.2659380692167577,"4000 Gibbs sampling iterations. In the experiments, we found that the Gibbs sampler started to
242"
PREDICTIVE ANALYSIS,0.2668488160291439,"converge after 1000 iterations, and thus we set the burn-in time be 2000 iterations. We retained
243"
PREDICTIVE ANALYSIS,0.2677595628415301,"every hundredth sample, and averaged the predictions over the samples. Mean relative error (MRE)
244"
PREDICTIVE ANALYSIS,0.2686703096539162,"and mean absolute error (MAE) are adopted to evaluate the model’s predictive capability, which
245"
PREDICTIVE ANALYSIS,0.26958105646630237,"are defined as MRE =
1
T V
P t
P"
PREDICTIVE ANALYSIS,0.27049180327868855,"v
|y(t)
v −ˆy(t)
v |"
PREDICTIVE ANALYSIS,0.27140255009107467,"1+y(t)
v
and MAE =
1
T V
P t
P"
PREDICTIVE ANALYSIS,0.27231329690346084,"v | y(t)
v
−ˆy(t)
v
| respectively,
246"
PREDICTIVE ANALYSIS,0.273224043715847,"where y(t)
v
indicates the true count and ˆy(t)
v
is the prediction.
247"
PREDICTIVE ANALYSIS,0.27413479052823314,"As the experiment results shown in Table 1, the NS-PGDS exhibits improved performance in both
248"
PREDICTIVE ANALYSIS,0.2750455373406193,"data smoothing and forecasting tasks. We attribute this enhanced capability to the time-varying
249"
PREDICTIVE ANALYSIS,0.27595628415300544,"transition kernels, which effectively adapt to the non-stationary environment, and thus achieve
250"
PREDICTIVE ANALYSIS,0.2768670309653916,"improved predictive performance. For some datasets (e.g. ICEWS) and tasks, the effectiveness of the
251"
PREDICTIVE ANALYSIS,0.2777777777777778,"Dir-Gam-Dir and Pr-Gam-Dir constructions does not be exhibited in the numerical results. However,
252"
PREDICTIVE ANALYSIS,0.2786885245901639,"these two constructions indeed induce more informative patterns compared with Dir-Dir construction,
253"
PREDICTIVE ANALYSIS,0.2795992714025501,"as shown in the exploratory analysis.
254"
EXPLORATORY ANALYSIS,0.28051001821493626,"6.2
Exploratory Analysis
255"
EXPLORATORY ANALYSIS,0.2814207650273224,"Figure 4: The latent factors inferred by the NS-PGDS. (a) and (b) illustrate the top 2 latent factors
inferred from ICEWS dataset, (a) corresponds to Iraq war and (b) corresponds to the Six-Party Talks.
(c) illustrates the evolving trends of the top 5 latent factors inferred from NIPS dataset."
EXPLORATORY ANALYSIS,0.28233151183970856,"We used ICEWS and NIPS datasets for exploratory analysis, and chose the NS-PGDS with Dirichlet-
256"
EXPLORATORY ANALYSIS,0.28324225865209474,"Dirichlet Markov chains for illustration. Figure 4(a) and Figure 4(b) demonstrate the top 2 latent
257"
EXPLORATORY ANALYSIS,0.28415300546448086,"factors inferred by NS-PGDS from ICEWS dataset. From Figure 4(a) we can see that the main labels
258"
EXPLORATORY ANALYSIS,0.28506375227686703,"are “Iraq (IRQ)–United States (USA)"", “Iraq (IRQ)–United Kingdom (UK)"", “Russia (RUS)–United
259"
EXPLORATORY ANALYSIS,0.2859744990892532,"States (USA)"", and so on. This latent factor probably corresponds to the topic about Iraq war. Besides,
260"
EXPLORATORY ANALYSIS,0.28688524590163933,"in Figure 4(a), there is a peak around March, 2003, and we know that the Iraq war broke out exactly on
261"
EXPLORATORY ANALYSIS,0.2877959927140255,"20 March, 2003. In addition, the most dominant labels shown in Figure 4(b) are “Japan (JPN)–United
262"
EXPLORATORY ANALYSIS,0.2887067395264117,"States (USA)"", “China (CHN)–United States (USA)"", “North Korea (PRK)–United States (USA)"",
263"
EXPLORATORY ANALYSIS,0.2896174863387978,"“South Korea (KOR)–United States (USA)"", and so on. We can infer that this latent factor corresponds
264"
EXPLORATORY ANALYSIS,0.290528233151184,"to “Six-Party Talks"" and other accidents about it.
265"
EXPLORATORY ANALYSIS,0.29143897996357016,"Figure 4(c) demonstrates the evolving trends of the top 5 latent factors inferred by the NS-PGDS
266"
EXPLORATORY ANALYSIS,0.2923497267759563,"from NIPS dataset, and the legend indicates the representative words of the corresponding latent
267"
EXPLORATORY ANALYSIS,0.29326047358834245,"factors. Clearly, the green and blue lines correspond to the latent factors of neural network re-
268"
EXPLORATORY ANALYSIS,0.2941712204007286,"search which started to decline from the 1990s. From the 1990s we see that the latent factors
269"
EXPLORATORY ANALYSIS,0.29508196721311475,"about statistical and probabilistic methods began to dominate the NeurIPS conference. In addi-
270"
EXPLORATORY ANALYSIS,0.2959927140255009,"tion, the NS-PGDS also captured the revival of neural networks (blue line) from the 2010s. The
271"
EXPLORATORY ANALYSIS,0.29690346083788705,"above observations from the latent structure inferred by the NS-PGDS match our prior knowledge.
272"
EXPLORATORY ANALYSIS,0.2978142076502732,"Figure 5: Transition matrices inferred from NIPS dataset. (a)
illustrates the transition matrix inferred by the PGDS. (b)-(f)
illustrate the time-varying transition matrices inferred by the
NS-PGDS. 273"
EXPLORATORY ANALYSIS,0.2987249544626594,"Next, we explored the time-varying
274"
EXPLORATORY ANALYSIS,0.2996357012750455,"transition matrices inferred by the
275"
EXPLORATORY ANALYSIS,0.3005464480874317,"NS-PGDS. We chose NIPS dataset
276"
EXPLORATORY ANALYSIS,0.30145719489981787,"for illustratiuon, and set K = 10 and
277"
EXPLORATORY ANALYSIS,0.302367941712204,"the interval length M to be 5. The
278"
EXPLORATORY ANALYSIS,0.30327868852459017,"time-varying transition matrices are
279"
EXPLORATORY ANALYSIS,0.30418943533697634,"shown from Figure 5(b) to Figure
280"
EXPLORATORY ANALYSIS,0.30510018214936246,"5(f).
At the beginning, matrices
281"
EXPLORATORY ANALYSIS,0.30601092896174864,"shown in Figure 5(b) and Figure
282"
EXPLORATORY ANALYSIS,0.3069216757741348,"5(c) are close to identity matrices.
283"
EXPLORATORY ANALYSIS,0.30783242258652094,"Then the transition matrices tend
284"
EXPLORATORY ANALYSIS,0.3087431693989071,"to become block diagonal matrices
285"
EXPLORATORY ANALYSIS,0.30965391621129323,"with 2 blocks, as shown in Figure
286"
EXPLORATORY ANALYSIS,0.3105646630236794,"5(d)-5(f). The representative words
287"
EXPLORATORY ANALYSIS,0.3114754098360656,"for latent factors in the first block
288"
EXPLORATORY ANALYSIS,0.3123861566484517,"are
“state-linear-classification"",
289"
EXPLORATORY ANALYSIS,0.3132969034608379,"“network-neural-networks"", “kernel-
290"
EXPLORATORY ANALYSIS,0.31420765027322406,"image-space"",
“network-neural-
291"
EXPLORATORY ANALYSIS,0.3151183970856102,"networks"", “neural-networks-state"".
292"
EXPLORATORY ANALYSIS,0.31602914389799636,"The representative words for latent factors in the second block are “image-sparse-matrix"", “kernel-
293"
EXPLORATORY ANALYSIS,0.31693989071038253,"supervised-random"", “matrix-sample-random"", “inference-prior-latent"", “state-policy-gamma"". The
294"
EXPLORATORY ANALYSIS,0.31785063752276865,"first block primarily captured the correlations among the research topics about neural networks.
295"
EXPLORATORY ANALYSIS,0.31876138433515483,"The second block reflects that, from the 1990s, statistical learning and Bayesian methods began to
296"
EXPLORATORY ANALYSIS,0.319672131147541,"dominate, and these topics are highly correlated. Figure 5(a) illustrates the transition matrix inferred
297"
EXPLORATORY ANALYSIS,0.3205828779599271,"by the PGDS, which is averaged over all time steps. Compared with the NS-PGDS, the PGDS can
298"
EXPLORATORY ANALYSIS,0.3214936247723133,"not capture the informative time-varying transition dynamics. We also analyzed the features of the
299"
EXPLORATORY ANALYSIS,0.3224043715846995,"proposed Dirichlet Markov chains. The left column of Figure 6 demonstrates transition matrices
300"
EXPLORATORY ANALYSIS,0.3233151183970856,"of the first four sub-intervals of ICEWS dataset inferred by the NS-PGDS (Dir-Dir). Because of
301"
EXPLORATORY ANALYSIS,0.3242258652094718,"the Dir-Dir construction, the consecutive transition matrices smoothly change over time and thus
302"
EXPLORATORY ANALYSIS,0.3251366120218579,"the NS-PGDS may lack sufficient flexibility to capture rapid dynamics. The middle column of
303"
EXPLORATORY ANALYSIS,0.32604735883424407,"Figure 6 illustrates the transition matrices inferred by the NS-PGDS (Dir-Gam-Dir), which takes
304"
EXPLORATORY ANALYSIS,0.32695810564663025,"mutations among latent components into account and captured more complicated patterns. Transition
305"
EXPLORATORY ANALYSIS,0.32786885245901637,"matrices inferred by the PR-Gam-Dir construction are shown in the right column of Figure 6, these
306"
EXPLORATORY ANALYSIS,0.32877959927140255,"matrices not only exhibited sufficient flexibility but also captured sparser patterns compared with the
307"
EXPLORATORY ANALYSIS,0.3296903460837887,Dir-Gam-Dir construction.
EXPLORATORY ANALYSIS,0.33060109289617484,"Figure 6: From top to bottom are the first four transition matrices inferred by different Dirichlet
Markov chains from ICEWS dataset. Top row: Matrices inferred by the Dir-Dir construction. Middle
row: Matrices inferred by the Dir-Gam-Dir construction. Bottom row: Matrices inferred by the
PR-Gam-Dir construction.
308"
CONCLUSION,0.331511839708561,"7
Conclusion
309"
CONCLUSION,0.3324225865209472,"The Poisson-gamma dynamical systems with time-varying transition matrices, have been proposed to
310"
CONCLUSION,0.3333333333333333,"capture complicated dynamics observed in non-stationary count sequences. In particular, Dirichlet
311"
CONCLUSION,0.3342440801457195,"Markov chains are constructed to allow the underlying transition matrices to evolve over time.
312"
CONCLUSION,0.33515482695810567,"Although the Dirichlet Markov processes lack conjugacy, we have developed tractable-but-efficient
313"
CONCLUSION,0.3360655737704918,"Gibbs sampling algorithms to perform posterior simulation. The experiment results demonstrate the
314"
CONCLUSION,0.33697632058287796,"improved performance of the proposed NS-PGDS in data smoothing and forecasting tasks, compared
315"
CONCLUSION,0.33788706739526414,"with the PGDS with a stationary transition kernel. Moreover, the experimental results on several
316"
CONCLUSION,0.33879781420765026,"real-world data sets show the explainable structures inferred by the proposed NS-PGDS. For the
317"
CONCLUSION,0.33970856102003644,"future work, we plan to design a method that can find the point of change and thus the length of each
318"
CONCLUSION,0.3406193078324226,"sub-interval can be determined automatically instead of a constant. We also consider to generalize
319"
CONCLUSION,0.34153005464480873,"Dirichlet belief networks by incorporating the proposed Dirichlet Markov chain constructions, which
320"
CONCLUSION,0.3424408014571949,"allow the hierarchical topics to mutate across layers, and thus can generate more rich text information.
321"
CONCLUSION,0.34335154826958103,"And we also consider to capture non-stationary interaction dynamics among individuals over online
322"
CONCLUSION,0.3442622950819672,"social networks in the future research.
323"
REFERENCES,0.3451730418943534,"References
324"
REFERENCES,0.3460837887067395,"[1] David M Blei and John D Lafferty. Dynamic topic models. In Proceedings of the 23rd
325"
REFERENCES,0.3469945355191257,"international conference on Machine learning, pages 113–120, 2006.
326"
REFERENCES,0.34790528233151186,"[2] Xuerui Wang and Andrew McCallum. Topics over time: a non-markov continuous-time model
327"
REFERENCES,0.348816029143898,"of topical trends. In Proceedings of the 12th ACM SIGKDD international conference on
328"
REFERENCES,0.34972677595628415,"Knowledge discovery and data mining, pages 424–433, 2006.
329"
REFERENCES,0.35063752276867033,"[3] Patrick Jähnichen, Florian Wenzel, Marius Kloft, and Stephan Mandt. Scalable generalized
330"
REFERENCES,0.35154826958105645,"dynamic topic models. In International Conference on Artificial Intelligence and Statistics,
331"
REFERENCES,0.3524590163934426,"pages 1427–1435, 2018.
332"
REFERENCES,0.3533697632058288,"[4] Daniel Sheldon and Thomas G Dietterich. Collective graphical models. In Proceedings of the
333"
REFERENCES,0.3542805100182149,"24th International Conference on Neural Information Processing Systems, pages 1161–1169,
334"
REFERENCES,0.3551912568306011,"2011.
335"
REFERENCES,0.3561020036429873,"[5] James Raymer, Arkadiusz Wi´sniowski, Jonathan J Forster, Peter WF Smith, and Jakub Bijak.
336"
REFERENCES,0.3570127504553734,"Integrated modeling of european migration. Journal of the American Statistical Association,
337"
REFERENCES,0.35792349726775957,"108(503):801–819, 2013.
338"
REFERENCES,0.3588342440801457,"[6] Tom Wilson. Methods for estimating sub-state international migration: The case of australia.
339"
REFERENCES,0.35974499089253187,"Spatial Demography, 5(3):171–192, 2017.
340"
REFERENCES,0.36065573770491804,"[7] Philippe Wanner. How well can we estimate immigration trends using google data? Quality &
341"
REFERENCES,0.36156648451730417,"Quantity, 55(4):1181–1202, 2021.
342"
REFERENCES,0.36247723132969034,"[8] R. E. Kalman. A New Approach to Linear Filtering and Prediction Problems. Journal of Basic
343"
REFERENCES,0.3633879781420765,"Engineering, 82(1):35–45, 1960.
344"
REFERENCES,0.36429872495446264,"[9] Zoubin Ghahramani and Sam T Roweis. Learning nonlinear dynamical systems using an
345"
REFERENCES,0.3652094717668488,"em algorithm. In Proceedings of the 11th International Conference on Neural Information
346"
REFERENCES,0.366120218579235,"Processing Systems, pages 431–437, 1998.
347"
REFERENCES,0.3670309653916211,"[10] M Zhou and L Carin. Augment-and-conquer negative binomial processes. Advances in Neural
348"
REFERENCES,0.3679417122040073,"Information Processing Systems, 4:2546–2554, 2012.
349"
REFERENCES,0.36885245901639346,"[11] Mingyuan Zhou and Lawrence Carin. Negative binomial process count and mixture modeling.
350"
REFERENCES,0.3697632058287796,"IEEE Transactions on Pattern Analysis & Machine Intelligence, 37(02):307–320, 2015.
351"
REFERENCES,0.37067395264116576,"[12] Aaron Schein, John Paisley, David M Blei, and Hanna Wallach. Bayesian poisson tensor
352"
REFERENCES,0.37158469945355194,"factorization for inferring multilateral relations from sparse dyadic event counts. In Proceedings
353"
REFERENCES,0.37249544626593806,"of the 21th ACM SIGKDD International conference on knowledge discovery and data mining,
354"
REFERENCES,0.37340619307832423,"pages 1045–1054, 2015.
355"
REFERENCES,0.3743169398907104,"[13] Aaron Schein, Mingyuan Zhou, and Hanna Wallach. Poisson-gamma dynamical systems. In
356"
REFERENCES,0.37522768670309653,"Proceedings of the 30th International Conference on Neural Information Processing Systems,
357"
REFERENCES,0.3761384335154827,"pages 5012–5020, 2016.
358"
REFERENCES,0.3770491803278688,"[14] Aaron Schein, Mingyuan Zhou, David Blei, and Hanna Wallach. Bayesian poisson tucker
359"
REFERENCES,0.377959927140255,"decomposition for learning the structure of international relations. In International Conference
360"
REFERENCES,0.3788706739526412,"on Machine Learning, pages 2810–2819, 2016.
361"
REFERENCES,0.3797814207650273,"[15] Ayan Acharya, Joydeep Ghosh, and Mingyuan Zhou. Nonparametric bayesian factor analysis
362"
REFERENCES,0.3806921675774135,"for dynamic count matrices. In Artificial Intelligence and Statistics, pages 1–9, 2015.
363"
REFERENCES,0.38160291438979965,"[16] Aaron Schein, Scott W Linderman, Mingyuan Zhou, David M Blei, and Hanna Wallach.
364"
REFERENCES,0.3825136612021858,"Poisson-randomized gamma dynamical systems. In Proceedings of the 33rd International
365"
REFERENCES,0.38342440801457195,"Conference on Neural Information Processing Systems, pages 782–793, 2019.
366"
REFERENCES,0.3843351548269581,"[17] Jonathan Chang and David Blei. Relational topic models for document networks. In Proceedings
367"
REFERENCES,0.38524590163934425,"of the Twelth International Conference on Artificial Intelligence and Statistics, pages 81–88,
368"
REFERENCES,0.3861566484517304,"2009.
369"
REFERENCES,0.3870673952641166,"[18] H Juliette T Unwin, Swapnil Mishra, Valerie C Bradley, Axel Gandy, Thomas A Mellan, Helen
370"
REFERENCES,0.3879781420765027,"Coupland, Jonathan Ish-Horowicz, Michaela AC Vollmer, Charles Whittaker, Sarah L Filippi,
371"
REFERENCES,0.3888888888888889,"et al. State-level tracking of covid-19 in the united states. Nature Communications, 11(1):1–9,
372"
REFERENCES,0.38979963570127507,"2020.
373"
REFERENCES,0.3907103825136612,"[19] Rainer Winkelmann. Econometric Analysis of Count Data. Springer Publishing Company,
374"
REFERENCES,0.39162112932604737,"Incorporated, 5th edition, 2008.
375"
REFERENCES,0.3925318761384335,"[20] Guy Grossman, Soojong Kim, Jonah M Rexer, and Harsha Thirumurthy. Political partisanship
376"
REFERENCES,0.39344262295081966,"influences behavioral responses to governors’ recommendations for covid-19 prevention in the
377"
REFERENCES,0.39435336976320584,"united states. Proceedings of the National Academy of Sciences, 117(39):24144–24153, 2020.
378"
REFERENCES,0.39526411657559196,"[21] IHME COVID-19 Forecasting Team. Modeling covid-19 scenarios for the united states. Nature
379"
REFERENCES,0.39617486338797814,"medicine, 27(1):94–105, 2021.
380"
REFERENCES,0.3970856102003643,"[22] Luzhao Feng, Ting Zhang, Qing Wang, Yiran Xie, Zhibin Peng, Jiandong Zheng, Ying Qin,
381"
REFERENCES,0.39799635701275043,"Muli Zhang, Shengjie Lai, Dayan Wang, et al. Impact of covid-19 outbreaks and interventions
382"
REFERENCES,0.3989071038251366,"on influenza in china and the united states. Nature communications, 12(1):3249, 2021.
383"
REFERENCES,0.3998178506375228,"[23] Shaobo Han, Lin Du, Esther Salazar, and Lawrence Carin. Dynamic rank factor model for text
384"
REFERENCES,0.4007285974499089,"streams. In Proceedings of the 27th International Conference on Neural Information Processing
385"
REFERENCES,0.4016393442622951,"Systems-Volume 2, pages 2663–2671, 2014.
386"
REFERENCES,0.40255009107468126,"[24] Rahi Kalantari and Mingyuan Zhou. Graph gamma process generalized linear dynamical
387"
REFERENCES,0.4034608378870674,"systems. arXiv preprint arXiv:2007.12852, 2020.
388"
REFERENCES,0.40437158469945356,"[25] Lin Yuan and John D Kalbfleisch. On the bessel distribution and related problems. Annals of
389"
REFERENCES,0.40528233151183973,"the Institute of Statistical Mathematics, 52:438–447, 2000.
390"
REFERENCES,0.40619307832422585,"[26] Yee Whye Teh, Michael I Jordan, Matthew J Beal, and David M Blei. Hierarchical dirichlet
391"
REFERENCES,0.40710382513661203,"processes. Journal of the American Statistical Association, 101(476):1566–1581, 2006.
392"
REFERENCES,0.4080145719489982,"[27] Norman L Johnson, Adrienne W Kemp, and Samuel Kotz. Univariate discrete distributions,
393"
REFERENCES,0.4089253187613843,"volume 444. John Wiley & Sons, 2005.
394"
REFERENCES,0.4098360655737705,"[28] Mingyuan Zhou. Nonparametric bayesian negative binomial factor analysis. Bayesian Analysis,
395"
REFERENCES,0.4107468123861566,"13(4):1065–1093, 2018.
396"
REFERENCES,0.4116575591985428,"[29] Chengyue Gong and Win-bin Huang. Deep dynamic poisson factorization model. In Proceedings
397"
REFERENCES,0.412568306010929,"of the 31st International Conference on Neural Information Processing Systems, pages 1665–
398"
REFERENCES,0.4134790528233151,"1673, 2017.
399"
REFERENCES,0.4143897996357013,"[30] Sikun Yang and Heinz Koeppl. Dependent relational gamma process models for longitudinal
400"
REFERENCES,0.41530054644808745,"networks. In International Conference on Machine Learning, pages 5551–5560, 2018.
401"
REFERENCES,0.41621129326047357,"[31] Dandan Guo, Bo Chen, Hao Zhang, and Mingyuan Zhou. Deep poisson gamma dynamical
402"
REFERENCES,0.41712204007285975,"systems. In Proceedings of the 32nd International Conference on Neural Information Processing
403"
REFERENCES,0.4180327868852459,"Systems, pages 8451–8461, 2018.
404"
REFERENCES,0.41894353369763204,"[32] Wenchao Chen, Bo Chen, Yicheng Liu, Qianru Zhao, and Mingyuan Zhou. Switching poisson
405"
REFERENCES,0.4198542805100182,"gamma dynamical systems. In Proceedings of the Twenty-Ninth International Conference on
406"
REFERENCES,0.4207650273224044,"International Joint Conferences on Artificial Intelligence, pages 2029–2036, 2021.
407"
REFERENCES,0.4216757741347905,"[33] Louis Filstroff, Olivier Gouvert, Cédric Févotte, and Olivier Cappé. A comparative study of
408"
REFERENCES,0.4225865209471767,"gamma markov chains for temporal non-negative matrix factorization. IEEE Transactions on
409"
REFERENCES,0.42349726775956287,"Signal Processing, 69:1614–1626, 2021.
410"
REFERENCES,0.424408014571949,"[34] Peter AW Lewis, Edward McKenzie, and David Kennedy Hugus. Gamma processes. Stochastic
411"
REFERENCES,0.42531876138433516,"Models, 5(1):1–30, 1989.
412"
REFERENCES,0.4262295081967213,"[35] John Frank Charles Kingman. Poisson processes, volume 3. Clarendon Press, 1992.
413"
REFERENCES,0.42714025500910746,"A
MCMC Inference
414"
REFERENCES,0.42805100182149364,"Notation. When expressing the full conditionals for Gibbs sampling, we use the shorthand “–” to
415"
REFERENCES,0.42896174863387976,"denote all other variables. We use “·” as an index summation shorthand, e.g., x·j = P"
REFERENCES,0.42987249544626593,"i xij.
416"
REFERENCES,0.4307832422586521,"In this section, we present a fully-conjugate and efficient Gibbs sampler for the proposed NS-PGDS.
417"
REFERENCES,0.43169398907103823,"The sampling algorithms depend on several key technical results, which we will repeatedly exploit,
418"
REFERENCES,0.4326047358834244,"thus we list them below.
419"
REFERENCES,0.4335154826958106,"Negative-binomial Distribution. Let y ∼Pois (cλ), and λ ∼Gam(a, b). If we marginalize
420"
REFERENCES,0.4344262295081967,"over λ, then y ∼NB

a,
c
b+c

is a negative-binomial distributed random variable. We can further
421"
REFERENCES,0.4353369763205829,"parameterize it as y ∼NB (a, g (ζ)), where g (z) = 1 −exp (−z) and ζ = ln
 
1 + c"
REFERENCES,0.43624772313296906,"b

.
422"
REFERENCES,0.4371584699453552,"Lemma 1. If y ∼NB (a, g (ζ)) and l ∼CRT (y, a), where CRT (·) represents Chinese restaurant
table distribution [26], then the joint distribution of y and l can be equivalently distributed as
y ∼SumLog (l, g (ζ)) and l ∼Pois (aζ) [11], i.e.
NB (y; a, g (ζ)) CRT (l; y, a) = SumLog (y; l, g (ζ)) Pois (l; aζ) ,"
REFERENCES,0.43806921675774135,"where SumLog (l, g (ζ)) = Pl
i=1 xi and xi ∼Log (g (ζ)) are independently and identically loga-
423"
REFERENCES,0.43897996357012753,"rithmic distributed random variables [27].
424"
REFERENCES,0.43989071038251365,"Lemma 2. Suppose n = (n1, · · · , nK) and n | n ∼DirMult (n, r1, · · · , rK) , where DirMult (·)
425"
REFERENCES,0.4408014571948998,"refers to Dirichlet-multimonial distribution. We sample the augmented variable q | n ∼Beta (n, r·),
426"
REFERENCES,0.44171220400728595,"where r· = PK
k=1 rk. According to [28], conditioning on q, we have nk ∼NB (rk, q).
427"
REFERENCES,0.4426229508196721,"Lemma 3. If y· = PS
s=1 ys, and ys
i.i.d
∼Pois(λs), s = 1, · · · , S. Then y· ∼Pois(PS
s=1 λs) and
428"
REFERENCES,0.4435336976320583,"(y1, · · · , yS) ∼Mult(y·, (
λ1
PS
s=1 λs , · · · ,
λS
PS
s=1 λs )), where Mult (·) represents multinomial distribu-
429"
REFERENCES,0.4444444444444444,"tion [35].
430"
REFERENCES,0.4453551912568306,"Sampling y(t)
vk : Use the relationship between Poisson and multinomial distributions as described by
431"
REFERENCES,0.44626593806921677,"Lemma 3, given observed counts and latent parameters, we sample
432"
REFERENCES,0.4471766848816029,"
y(t)
vk
K"
REFERENCES,0.44808743169398907,"k=1 | −

∼Mult "
REFERENCES,0.44899817850637525,"y(t)
v ,"
REFERENCES,0.44990892531876137,"ϕvkθ(t)
k
PK
k=1 ϕvkθ(t)
k !K k=1 "
REFERENCES,0.45081967213114754,".
(6)"
REFERENCES,0.4517304189435337,"Then the distribution of y(t)
vk is y(t)
vk ∼Pois(δ(t)ϕvkθ(t)
k ).
433"
REFERENCES,0.45264116575591984,"Sampling ϕk: Via Dirichlet-multinomial conjugacy, the posterior of ϕk is
434"
REFERENCES,0.453551912568306,"(ϕk | −) ∼Dir  ϵ0 + T
X"
REFERENCES,0.4544626593806922,"t=1
y(t)
1k , · · · , ϵ0 + T
X"
REFERENCES,0.4553734061930783,"t=1
y(t)
V k ! .
(7)"
REFERENCES,0.4562841530054645,"Marginalizing over θ(t)
k : Note that y(t)
v
= y(t)
v· = PK
k=1 y(t)
vk and y(t)
vk ∼Pois(δ(t)ϕvkθ(t)
k ). Then
435"
REFERENCES,0.45719489981785066,"we define y(t)
·k = PV
v=1 y(t)
vk . Because PV
v=1 ϕvk = 1, we obtain y(t)
·k ∼Pois(δ(t)θ(t)
k ).
436"
REFERENCES,0.4581056466302368,"We start by marginalizing over θ(T )
k
, using the definition of negative-binomial distribution, we obtain
437"
REFERENCES,0.45901639344262296,"y(T )
·k
∼NB  τ0 K
X"
REFERENCES,0.4599271402550091,"k2=1
πi(T −1)
kk2
θ(T −1)
k2
, g

ζ(T )! ,"
REFERENCES,0.46083788706739526,where ζ(T ) = ln(1 + δ(T )
REFERENCES,0.46174863387978143,"τ0 ). Next, we further marginalize over θ(T −1)
k
. To this end, we first sample
438"
REFERENCES,0.46265938069216755,"auxiliary variables
439"
REFERENCES,0.46357012750455373,"l(T )
k
∼CRT "
REFERENCES,0.4644808743169399,"y(T )
·k , τ0 K
X"
REFERENCES,0.465391621129326,"k2=1
πi(T −1)
kk2
θ(T −1)
k2 ! ."
REFERENCES,0.4663023679417122,"By Lemma 1, the joint distribution of y(T )
·k
and l(T )
k
can be expressed as
440"
REFERENCES,0.4672131147540984,"y(T )
·k
∼SumLog

l(T )
k
, g

ζ(T )
and l(T )
k
∼Pois "
REFERENCES,0.4681238615664845,"ζ(T )τ0 K
X"
REFERENCES,0.4690346083788707,"k2=1
πi(T −1)
kk2
θ(T −1)
k2 ! ."
REFERENCES,0.46994535519125685,"Via Lemma 3, we re-express the auxiliary variables as
441"
REFERENCES,0.470856102003643,"l(T )
k
= l(T )
k·
= K
X"
REFERENCES,0.47176684881602915,"k2=1
l(T )
kk2, and obtain l(T )
kk2 ∼Pois

ζ(T )τ0πi(T −1)
kk2
θ(T −1)
k2 
."
REFERENCES,0.4726775956284153,"Then we define l(T )
·k
= PK
k1=1 l(T )
k1k. Leveraging Lemma 3 and PK
k1=1 πi(T −1)
k1k
= 1, we obtain
442"
REFERENCES,0.47358834244080145,"l(T )
·k
∼Pois

ζ(T )τ0θ(T −1)
k

and

l(T )
1k , · · · , l(T )
Kk

∼Mult

l(T )
·k ,

πi(T −1)
1k
, · · · , πi(T −1)
Kk

."
REFERENCES,0.4744990892531876,"Next, note that y(T −1)
·k
∼Pois(δ(T −1)θ(T −1)
k
), if we introduce m(T −1)
k
= y(T −1)
·k
+ l(T )
·k , then we
443"
REFERENCES,0.47540983606557374,"have
444"
REFERENCES,0.4763205828779599,"m(T −1)
k
∼Pois

θ(T −1)
k

δ(T −1) + ζ(T )τ0

."
REFERENCES,0.4772313296903461,"Because the prior of θ(T −1)
k
is gamma distributed, by the definition of negative-binomial distribution,
445"
REFERENCES,0.4781420765027322,"we can again marginalize over θ(T −1)
k
to obtain
446"
REFERENCES,0.4790528233151184,"m(T −1)
k
∼NB  τ0 K
X"
REFERENCES,0.47996357012750457,"k2=1
πi(T −2)
kk2
θ(T −2)
k2
, g

ζ(T −1)! ,"
REFERENCES,0.4808743169398907,where ζ(T −1) = ln(1 + δ(T −1)
REFERENCES,0.48178506375227687,"τ0
+ ζ(T )). Then we introduce auxiliary variables
447"
REFERENCES,0.48269581056466304,"l(T −1)
k
∼CRT "
REFERENCES,0.48360655737704916,"m(T −1)
k
, τ0 K
X"
REFERENCES,0.48451730418943534,"k2=1
πi(T −2)
kk2
θ(T −2)
k2 ! ."
REFERENCES,0.4854280510018215,"And similar to the case for t = T, we can obtain
448"
REFERENCES,0.48633879781420764,"l(T −1)
·k
∼Pois

ζ(T −1)τ0θ(T −2)
k

and m(T −2)
k
∼NB  τ0 K
X"
REFERENCES,0.4872495446265938,"k2=1
πi(T −3)
kk2
θ(T −3)
k2
, g

ζ(T −2)! ."
REFERENCES,0.48816029143898,"Thus we have marginalized over θ(T −2)
k
. Note that we can repeat this marginalization process
449"
REFERENCES,0.4890710382513661,recursively until t = 1 with ζ(t) = ln(1 + δ(t)
REFERENCES,0.4899817850637523,"τ0 + ζ(t+1)) and m(T )
k
= y(T )
·k
to maginalize over all the
450"
REFERENCES,0.49089253187613846,"θ(t)
k .
451"
REFERENCES,0.4918032786885246,"Sampling θ(t)
k
: Via the above marginalization process, to sample from the posterior of θ(t)
k , we
452"
REFERENCES,0.49271402550091076,"first sample the auxiliary variables. Setting l(T +1)
·k
= 0 and ζ(T +1) = 0, we sample the augmented
453"
REFERENCES,0.4936247723132969,"variables backwards from t = T, · · · , 2,
454"
REFERENCES,0.49453551912568305,"
l(t)
k· | −

∼CRT "
REFERENCES,0.49544626593806923,"y(t)
·k + l(t+1)
·k
, τ0 K
X"
REFERENCES,0.49635701275045535,"k2=1
πi(t−1)
kk2
θ(t−1)
k2 ! ,
(8)"
REFERENCES,0.4972677595628415,"
l(t)
k1 , · · · , l(t)
kK | −

∼Mult "
REFERENCES,0.4981785063752277,"l(t)
k· ,"
REFERENCES,0.4990892531876138,"πi(t−1)
k1
θ(t−1)
1
PK
k2=1 πi(t−1)
kk2
θ(t−1)
k2
, · · · ,
πi(t−1)
kK
θ(t−1)
K
PK
k2=1 πi(t−1)
kk2
θ(t−1)
k2 !! .
(9)"
REFERENCES,0.5,"And via Lemma 3, we obtain
455

l(t)
1k , · · · , l(t)
Kk

∼Mult

l(t)
·k , πi(t−1)
1k
, · · · , πi(t−1)
Kk

(10)"
REFERENCES,0.5009107468123861,"We compute ζ(t) recursively via
456"
REFERENCES,0.5018214936247724,"ζ(t) = ln

1 + δ(t)"
REFERENCES,0.5027322404371585,"τ0
+ ζ(t+1)

.
(11)"
REFERENCES,0.5036429872495446,"After sampling the auxiliary variables, then for t = 1, · · · , T, by Poisson-gamma conjugacy, we
457"
REFERENCES,0.5045537340619308,"obtain
458

θ(1)
k
| −

∼Gam

y(1)
·k + l(2)
·k + τ0νk, τ0 + δ(1) + ζ(2)τ0

,
(12)"
REFERENCES,0.505464480874317,"
θ(t)
k
| −

∼Gam "
REFERENCES,0.5063752276867031,"y(t)
·k + l(t+1)
·k
+ τ0 K
X"
REFERENCES,0.5072859744990893,"k2=1
πi(t−1)
kk2
θ(t−1)
k2
, τ0 + δ(t) + ζ(t+1)τ0 !"
REFERENCES,0.5081967213114754,".
(13) 459"
REFERENCES,0.5091074681238615,"Sampling Π(i) : We define M as the length of each sub-interval, and I as the number of intervals.
460"
REFERENCES,0.5100182149362478,"For i = I, by Eq.(10), (l(I)
1k , · · · , l(I)
Kk) is multinomial distributed. Thus by multinomial-Dirichlet
461"
REFERENCES,0.5109289617486339,"conjugacy, we obtain
462"
REFERENCES,0.51183970856102,"
π(I)
k
| −

∼Dir

α(I)
1k + l(I)
1k , · · · , α(I)
Kk + l(I)
Kk

,
(14)"
REFERENCES,0.5127504553734062,"where l(I)
k1k indicates the summation of l(t)
k1k over I-th sub-interval, i.e. l(I)
k1k = PT
t=(I−1)M+1 l(t)
k1k.
463"
REFERENCES,0.5136612021857924,"Inference for Dirichlet-Dirichlet Markov chains. For Dirichlet-Dirichlet Markov chains, α(i)
k1k =
464"
REFERENCES,0.5145719489981785,"ηKπ(i−1)
k1k . By Eq.(10), (l(i)
1k , · · · , l(i)
Kk) is multinomial distributed. If we marginalize (π(i)
1k , · · · , π(i)
Kk),
465"
REFERENCES,0.5154826958105647,"(l(i)
1k , · · · , l(i)
Kk) will be Dirichlet-multinomial distributed. Thus by Lemma 2, for i = I, we first sample
466"
REFERENCES,0.5163934426229508,"the auxiliary variables as
467"
REFERENCES,0.517304189435337,"
q(I)
k
| −

∼Beta

l(I)
·k , ηK

and

h(I)
k1k | −

∼CRT

l(I)
k1k, ηKπ(I−1)
k1k

.
(15)"
REFERENCES,0.5182149362477231,"Similarly, by Eq.(18), (h(i)
1k, · · · , h(i)
Kk) is also Dirichlet-multinomial distributed. Thus for i =
468"
REFERENCES,0.5191256830601093,"I −1, · · · , 2, we sample the auxiliary variables as
469"
REFERENCES,0.5200364298724954,"
q(i)
k
| −

∼Beta

l(i)
·k + h(i+1)
·k
, ηK

and

h(i)
k1k | −

∼CRT

l(i)
k1k + h(i+1)
k1k , ηKπ(i−1)
k1k

, (16)"
REFERENCES,0.5209471766848816,"where l(i)
k1k = PiM
(i−1)M+1 l(t)
k1k refers to the summation of l(t)
k1k over i-th interval. Via Lemma 2,
470"
REFERENCES,0.5218579234972678,"conditioning on q(i)
k , we have
471"
REFERENCES,0.5227686703096539,"
l(i)
k1k + h(i+1)
k1k

∼NB

ηKπ(i−1)
k1k , q(i)
k

."
REFERENCES,0.52367941712204,"Then via Lemma 1, we obtain
472"
REFERENCES,0.5245901639344263,"h(i)
k1k ∼Pois

−ηKπ(i−1)
k1k ln

1 −q(i)
k

.
(17)"
REFERENCES,0.5255009107468124,"Note that by Eq.(17), h(i)
k1k is Poisson distributed and by Lemma 3, we obtain
473"
REFERENCES,0.5264116575591985,"
h(i)
1k, · · · , h(i)
Kk

∼Mult

h(i)
·k ,

π(i−1)
1k
, · · · , π(i−1)
Kk

.
(18)"
REFERENCES,0.5273224043715847,"In addition, note that
474"
REFERENCES,0.5282331511839709,"
l(i−1)
1k
, · · · , l(i−1)
Kk

∼Mult

l(i−1)
·k
,

π(i−1)
1k
, · · · , π(i−1)
Kk

,"
REFERENCES,0.529143897996357,"via Dirichlet-multinomial conjugacy, for i = I −1, · · · , 2, we obtain
475"
REFERENCES,0.5300546448087432,"
π(i)
k
| −

∼Dir

ηKπ(i−1)
1k
+ l(i)
1k + h(i+1)
1k
, · · · , ηKπ(i−1)
Kk
+ l(i)
Kk + h(i+1)
Kk

.
(19)"
REFERENCES,0.5309653916211293,"Specifically, for i = 1, we have
476"
REFERENCES,0.5318761384335154,"
π(1)
k
| −

∼Dir

ν1νk + l(1)
1k + h(2)
1k , · · · , ξνk + l(1)
kk + h(2)
kk , · · · , νKνk + l(1)
Kk + h(2)
Kk

.
(20)"
REFERENCES,0.5327868852459017,"For sampling η, note that (h(i)
k1k | −) ∼Pois(−ηKπ(i−1)
k1k ln

1 −q(i)
k

), i = I, · · · , 2. Given the
477"
REFERENCES,0.5336976320582878,"prior η ∼Gam (e0, f0), via Poisson-gamma conjugacy, we obtain
478"
REFERENCES,0.5346083788706739,"(η | −) ∼Gam  e0 + I
X i=2 K
X k1=1 K
X"
REFERENCES,0.5355191256830601,"k2=1
h(i)
k1k2, f0 −K I
X i=2 K
X"
REFERENCES,0.5364298724954463,"k=1
ln

1 −q(i)
k
!"
REFERENCES,0.5373406193078324,".
(21)"
REFERENCES,0.5382513661202186,"Inference for Dirichlet-Gamma-Dirichlet Markov chains. For Dirichlet-Gamma-Dirichlet Markov
479"
REFERENCES,0.5391621129326047,"chains
480"
REFERENCES,0.5400728597449909,"α(i)
k1k ∼Gam "
REFERENCES,0.5409836065573771,"γ(i−1)
k K
X"
REFERENCES,0.5418943533697632,"k2=1
ψ(i−1)
kk1k2π(i−1)
k2k , c(i)
k ! ."
REFERENCES,0.5428051001821493,"By Eq.(10), (l(i)
1k , · · · , l(i)
Kk) is multinomial distributed.
If we marginalize (π(i)
1k , · · · , π(i)
Kk),
481"
REFERENCES,0.5437158469945356,"(l(i)
1k , · · · , l(i)
Kk) will be Dirichlet-multinomial distributed. Thus by Lemma 2, for i = I, we first
482"
REFERENCES,0.5446265938069217,"sample the auxiliary variables as
483"
REFERENCES,0.5455373406193078,"
q(I)
k
| −

∼Beta

l(I)
·k , α(I)
·k

and

h(I)
k1k | −

∼CRT

l(I)
k1k, α(I)
k1k

.
(22)"
REFERENCES,0.546448087431694,"Similarly, by Eq.(27), (g(i)
·1k, · · · , g(i)
·Kk) is also Dirichlet-multinomial distributed. Thus for i =
484"
REFERENCES,0.5473588342440802,"I −1, · · · , 2, we sample the auxiliary variables as
485"
REFERENCES,0.5482695810564663,"
q(i)
k
| −

∼Beta

l(i)
·k + g(i+1)
·k
, α(i)
·k

and

h(i)
k1k | −

∼CRT

l(i)
k1k + g(i+1)
·k1k , α(i)
k1k

.
(23)"
REFERENCES,0.5491803278688525,"Via Lemma 2, conditioning on q(i)
k , we have
486"
REFERENCES,0.5500910746812386,"
l(i)
k1k + g(i+1)
·k1k

∼NB

α(i)
k1k, q(i)
k

."
REFERENCES,0.5510018214936248,"Then via Lemma 1, we obtain
487"
REFERENCES,0.5519125683060109,"h(i)
k1k ∼Pois

−α(i)
k1kln

1 −q(i)
k

."
REFERENCES,0.5528233151183971,"Thus via Poisson-gamma conjugacy, we obtain
488"
REFERENCES,0.5537340619307832,"
α(i)
k1k | −

∼Gam "
REFERENCES,0.5546448087431693,"γ(i−1)
k K
X"
REFERENCES,0.5555555555555556,"k2=1
ψ(i−1)
kk1k2π(i−1)
k2k
+ h(i)
k1k, c(i)
k −ln

1 −q(i)
k
!"
REFERENCES,0.5564663023679417,".
(24)"
REFERENCES,0.5573770491803278,"Marginalizing over α(i)
k1k, and via the definition of negative-binomial distribution, we have
489"
REFERENCES,0.558287795992714,"h(i)
k1k ∼NB "
REFERENCES,0.5591985428051002,"γ(i−1)
k K
X"
REFERENCES,0.5601092896174863,"k2=1
ψ(i−1)
kk1k2π(i−1)
k2k ,
−ln

1 −q(i)
k
"
REFERENCES,0.5610200364298725,"c(i)
k −ln

1 −q(i)
k
  ."
REFERENCES,0.5619307832422586,"Then using Lemma 1, we sample
490"
REFERENCES,0.5628415300546448,"
g(i)
k1k | −

∼CRT "
REFERENCES,0.563752276867031,"h(i)
k1k, γ(i−1)
k K
X"
REFERENCES,0.5646630236794171,"k2=1
ψ(i−1)
kk1k2π(i−1)
k2k !"
REFERENCES,0.5655737704918032,",
(25)"
REFERENCES,0.5664845173041895,"and obtain
491"
REFERENCES,0.5673952641165756,"g(i)
k1k ∼Pois "
REFERENCES,0.5683060109289617,"γ(i−1)
k K
X"
REFERENCES,0.569216757741348,"k2=1
ψ(i−1)
kk1k2π(i−1)
k2k ln

1 −ln

1 −q(i)
k
 
c(i)
k
! ."
REFERENCES,0.5701275045537341,"If we define g(i)
k1k = g(i)
k1·k = PK
k2=1 g(i)
k1k2k, and augment
492"
REFERENCES,0.5710382513661202,"
g(i)
k11k, · · · , g(i)
k1Kk

∼Mult

g(i)
k1k,

ψ(i−1)
kk1k2π(i−1)
k2k
K k2=1"
REFERENCES,0.5719489981785064,"
.
(26)"
REFERENCES,0.5728597449908925,"By Lemma 3, we have
493"
REFERENCES,0.5737704918032787,"g(i)
k1k2k ∼Pois

γ(i−1)ψ(i−1)
kk1k2π(i−1)
k2k ln

1 −ln

1 −q(i)
k
 
c(i)
k

."
REFERENCES,0.5746812386156649,"Using Lemma 3 and PK
k1 ψ(i−1)
kk1k2 = 1, we have,
494"
REFERENCES,0.575591985428051,"
g(i)
·1k, · · · , g(i)
·Kk

∼Mult

g(i)
·k ,

π(i−1)
k1k
K k1=1"
REFERENCES,0.5765027322404371,"
,
(27)"
REFERENCES,0.5774134790528234,"495

g(i)
1k2k, · · · , g(i)
Kk2k

∼Mult

g(i)
·k2k,

ψ(i−1)
kk1k2 K k1=1 
."
REFERENCES,0.5783242258652095,"Thus by Dirichlet-multinomial conjugacy, for i = I, · · · , 2, we can obtain
496"
REFERENCES,0.5792349726775956,"
ψ(i−1)
k1k2 , · · · , ψ(i−1)
kKk2"
REFERENCES,0.5801457194899818,"
| −

∼Dir

ϵ0 + g(i)
1k2k, · · · , ϵ0 + g(i)
Kk2k

,
(28)

π(i−1)
k
| −

∼Dir

α(i−1)
1k
+ l(i−1)
1k
+ g(i)
·1k, · · · , α(i−1)
Kk
+ l(i−1)
Kk
+ g(i)
·Kk

. (29)"
REFERENCES,0.581056466302368,"For sampling γ(i−1)
k
, note that by Eq.(26) and PK
k1 ψ(i−1)
kk1k2 = 1, we have
497"
REFERENCES,0.5819672131147541,"g(i)
·k = K
X"
REFERENCES,0.5828779599271403,"k1=1
g(i)
k1k and g(i)
·k ∼Pois

γ(i−1)
k
ln

1 −ln

1 −q(i)
k
 
c(i)
k

.
(30)"
REFERENCES,0.5837887067395264,"Thus via Poisson-gamma conjugacy, we obtain
498"
REFERENCES,0.5846994535519126,"
γ(i−1)
k
| −

∼Gam

ϵ0 + g(i)
·k , ϵ0 + ln

1 −ln

1 −q(i)
k

.
(31)"
REFERENCES,0.5856102003642987,"By gamma-gamma conjugacy, we have
499"
REFERENCES,0.5865209471766849,"
c(i)
k
| −

∼Gam "
REFERENCES,0.587431693989071,"ϵ0 + γ(i−1)
k
, ϵ0 + K
X"
REFERENCES,0.5883424408014571,"k1=1
α(i)
k1k !"
REFERENCES,0.5892531876138434,".
(32)"
REFERENCES,0.5901639344262295,"Inference for Dirichlet-Randomized-Gamma-Dirichlet Markov chains.
For Dirichlet-
500"
REFERENCES,0.5910746812386156,"Randomized-Gamma-Dirichlet Markov chains,
501"
REFERENCES,0.5919854280510018,"α(i)
k1k ∼RG1 "
REFERENCES,0.592896174863388,"ϵα, γ(i−1)
K
X"
REFERENCES,0.5938069216757741,"k2=1
ψ(i−1)
kk1k2π(i−1)
k2k , c(i)
k ! ,"
REFERENCES,0.5947176684881603,"which can be equivalently represented as
502"
REFERENCES,0.5956284153005464,"α(i)
k1k ∼Gam

g(i)
k1k + ϵα, c(i)
k

, and g(i)
k1k = Pois "
REFERENCES,0.5965391621129326,"γ(i−1)
K
X"
REFERENCES,0.5974499089253188,"k2=1
ψ(i−1)
kk1k2π(i−1)
k2k ! ."
REFERENCES,0.5983606557377049,"By Eq.(10), (l(i)
1k , · · · , l(i)
Kk) is multinomial distributed.
If we marginalize (π(i)
1k , · · · , π(i)
Kk),
503"
REFERENCES,0.599271402550091,"(l(i)
1k , · · · , l(i)
Kk) will be Dirichlet-multinomial distributed. Thus by Lemma 2, for i = I, we first
504"
REFERENCES,0.6001821493624773,"sample the auxiliary variables as
505"
REFERENCES,0.6010928961748634,"
q(I)
k
| −

∼Beta

l(I)
·k , α(I)
·k

and

h(I)
k1k | −

∼CRT

l(I)
k1k, α(I)
k1k

.
(33)"
REFERENCES,0.6020036429872495,"Similarly, by Eq.(39), (g(i)
·1k, · · · , g(i)
·Kk) is also Dirichlet-multinomial distributed. Thus for i =
506"
REFERENCES,0.6029143897996357,"I −1, · · · , 2, we sample the auxiliary variables as
507"
REFERENCES,0.6038251366120219,"
q(i)
k
| −

∼Beta

l(i)
·k + g(i+1)
·k
, α(i)
·k

and

h(i)
k1k | −

∼CRT

l(i)
k1k + +g(i+1)
·k1k , α(i)
k1k

. (34)"
REFERENCES,0.604735883424408,"Via Lemma 2, conditioning on q(i)
k , we have
508"
REFERENCES,0.6056466302367942,"
l(i)
k1k + g(i+1)
·k1k

∼NB

α(i)
k1k, q(i)
k

."
REFERENCES,0.6065573770491803,"Then via Lemma 1, we obtain
509"
REFERENCES,0.6074681238615665,"h(i)
k1k ∼Pois

−α(i)
k1kln

1 −q(i)
k

."
REFERENCES,0.6083788706739527,"Via Poisson-gamma conjugacy, we first sample
510"
REFERENCES,0.6092896174863388,"
α(i)
k1k | −

∼Gam

g(i)
k1k + ϵα + h(i)
k1k, c(i)
k −ln

1 −q(i)
k

.
(35)"
REFERENCES,0.6102003642987249,"If ϵα > 0, we can sample the posterior of g(i)
k1k via
511"
REFERENCES,0.6111111111111112,"
g(i)
k1k | −

∼Bessel "
REFERENCES,0.6120218579234973,"ϵα −1, 2"
REFERENCES,0.6129326047358834,"v
u
u
tα(i)
k1kc(i)
k γ(i−1)
k K
X"
REFERENCES,0.6138433515482696,"k2=1
ψ(i−1)
kk1k2π(i−1)
k2k "
REFERENCES,0.6147540983606558,",
(36)"
REFERENCES,0.6156648451730419,"where Bessel (·) denotes Bessel distribution. If ϵα = 0, we sample g(i)
k1k via
512"
REFERENCES,0.6165755919854281,"
g(i)
k1k | −

∼"
REFERENCES,0.6174863387978142,"


 

"
REFERENCES,0.6183970856102003,"Pois

c(i)
k γ(i−1)
k
PK
k2=1 ψ(i−1)
kk1k2π(i−1)
k2k"
REFERENCES,0.6193078324225865,"c(i)
k −ln

1−q(i)
k


if h(i)
k1k = 0"
REFERENCES,0.6202185792349727,"SCH

h(i)
k1k,
c(i)
k γ(i−1)
k
PK
k2=1 ψ(i−1)
kk1k2π(i−1)
k2k"
REFERENCES,0.6211293260473588,"c(i)
k −ln

1−q(i)
k


otherwise,
(37)"
REFERENCES,0.6220400728597449,"where SCH (·) denotes the shifted confluent hypergeometric distribution [16].
513"
REFERENCES,0.6229508196721312,"Defining g(i)
k1k = g(i)
k1·k = PK
k2=1 g(i)
k1k2k, we first augment
514"
REFERENCES,0.6238615664845173,"
g(i)
k11k, · · · , g(i)
k1Kk

∼Mult

g(i)
k1k,

ψ(i−1)
kk1k2π(i−1)
k2k
K k2=1"
REFERENCES,0.6247723132969034,"
.
(38)"
REFERENCES,0.6256830601092896,"By Lemma 3, we have
515"
REFERENCES,0.6265938069216758,"g(i)
k1k2k ∼Pois

γ(i−1)ψ(i−1)
kk1k2π(i−1)
k2k

,"
REFERENCES,0.6275045537340619,"and because PK
k1 ψ(i−1)
kk1k2 = 1, we have
516"
REFERENCES,0.6284153005464481,"
g(i)
·1k, · · · , g(i)
·Kk

∼Mult

g(i)
·k ,

π(i−1)
k1k
K k1=1"
REFERENCES,0.6293260473588342,"
, and
(39)"
REFERENCES,0.6302367941712204,"
g(i)
1k2k, · · · , g(i)
Kk2k

∼Mult

g(i)
·k2k,

ψ(i−1)
kk1k2 K k1=1 
."
REFERENCES,0.6311475409836066,"Thus by Dirichlet-multinomial conjugacy, for i = I, · · · , 2, we have
517"
REFERENCES,0.6320582877959927,"
ψ(i−1)
k1k2 , · · · , ψ(i−1)
kKk2"
REFERENCES,0.6329690346083788,"
| −

∼Dir

ϵ0 + g(i)
1k2k, · · · , ϵ0 + g(i)
Kk2k

,
(40)"
REFERENCES,0.6338797814207651,"518

π(i−1)
k
| −

∼Dir

α(i−1)
1k
+ l(i−1)
1k
+ g(i)
·1k, · · · , α(i−1)
Kk
+ l(i−1)
Kk
+ g(i)
·Kk

.
(41)"
REFERENCES,0.6347905282331512,"Via Poisson-gamma conjugacy, we obtain
519"
REFERENCES,0.6357012750455373,"
γ(i−1)
k
| −

∼Gam

ϵ0 + g(i)
·k , ϵ0 + 1

.
(42)"
REFERENCES,0.6366120218579235,"By gamma-gamma conjugacy, we have
520"
REFERENCES,0.6375227686703097,"
c(i)
k
| −

∼Gam "
REFERENCES,0.6384335154826958,"ϵ0 + γ(i−1)
k
, ϵ0 + K
X"
REFERENCES,0.639344262295082,"k1=1
α(i)
k1k !"
REFERENCES,0.6402550091074681,".
(43)"
REFERENCES,0.6411657559198543,"Specifically, for i = 1, we have α(1)
k1k = νk1νk, if k1 ̸= k. And α(1)
k1k = ξνk, if k1 = k.
521"
REFERENCES,0.6420765027322405,"Sampling νk and ξ : As we sample Π(i), by the definition of Dirichlet-multinomial distribution, we
522"
REFERENCES,0.6429872495446266,"obtain
523
 
l(1)
1k + g(2)
·1k, · · · , l(1)
Kk + g(2)
·Kk

∼DirMult (ν1νK, · · · , ξνk, · · · , νKνk) ,"
REFERENCES,0.6438979963570127,"where l(1)
k1k = PM
t=1 l(t)
k1k. In particular, with a little abuse of notation here, for Dir-Dir construction,
524"
REFERENCES,0.644808743169399,"we take g(2)
·k1k = h(2)
k1k. We first sample
525"
REFERENCES,0.6457194899817851,"
h(1)
k1k | −

∼ 
 "
REFERENCES,0.6466302367941712,"CRT

l(1)
k1k + g(2)
·k1k, νk1νk

k1 ̸= k"
REFERENCES,0.6475409836065574,"CRT

l(1)
k1k + g(2)
·k1k, ξνk

k1 = k.
(44)"
REFERENCES,0.6484517304189436,"Then we sample
526"
REFERENCES,0.6493624772313297,"q(1)
k
∼Beta "
REFERENCES,0.6502732240437158,"l(1)
·k + g(2)
·k , νk  X"
REFERENCES,0.651183970856102,"k1̸=k
νk1 + ξ   "
REFERENCES,0.6520947176684881,".
(45)"
REFERENCES,0.6530054644808743,"We further introduce
527"
REFERENCES,0.6539162112932605,"nk =h(1)
kk +
X"
REFERENCES,0.6548269581056466,"k1̸=k
h(1)
k1k +
X"
REFERENCES,0.6557377049180327,"k2̸=k
h(1)
kk2 + l(1)
k· , and"
REFERENCES,0.656648451730419,"ρk =τ0ζ(1) −ln

1 −q(1)
k

"
REFERENCES,0.6575591985428051,"ξ +
X"
REFERENCES,0.6584699453551912,"k1̸=k
νk1  −
X"
REFERENCES,0.6593806921675774,"k2̸=k
ln

1 −q(1)
k2"
REFERENCES,0.6602914389799636,"
νk2."
REFERENCES,0.6612021857923497,"Via Poisson-gamma conjugacy, we have
528"
REFERENCES,0.6621129326047359,(ξ | −) ∼Gam
REFERENCES,0.663023679417122,"γ0
K +
X"
REFERENCES,0.6639344262295082,"k
h(1)
kk , β −
X"
REFERENCES,0.6648451730418944,"k
νkln

1 −q(1)
k
!"
REFERENCES,0.6657559198542805,",
(46)"
REFERENCES,0.6666666666666666,"(νk | −) ∼Gam
γ0"
REFERENCES,0.6675774134790529,"K + nk, β + ρk

.
(47) 529"
REFERENCES,0.668488160291439,"Sampling δ(t) and β : Via Poisson-gamma conjugacy
530"
REFERENCES,0.6693989071038251,"
δ(t) | −

∼Gam  ϵ0 + V
X"
REFERENCES,0.6703096539162113,"v=1
y(t)
v , ϵ0 + K
X"
REFERENCES,0.6712204007285975,"k=1
θ(t)
k !"
REFERENCES,0.6721311475409836,".
(48)"
REFERENCES,0.6730418943533698,"And by gamma-gamma conjugacy, we obtain
531"
REFERENCES,0.6739526411657559,(β | −) ∼Gam 
REFERENCES,0.674863387978142,"ϵ0 + γ0, ϵ0 + K
X"
REFERENCES,0.6757741347905283,"k=1
νk !"
REFERENCES,0.6766848816029144,".
(49)"
REFERENCES,0.6775956284153005,"The full procedure of our Gibbs sampling algorithms are summarized in Algorithm 1, Algorithm 2
532"
REFERENCES,0.6785063752276868,"and Algorithm 3.
533"
REFERENCES,0.6794171220400729,Algorithm 1 Gibbs sampling algorithm for NS-PGDS (Dir-Dir Markov construction)
REFERENCES,0.680327868852459,"Input: observed count sequence {y(t)}T
t=1, iterations J .
Initialize the model’s rank K, hyperparameters γ0, ϵ0, e0, f0.
for iter = 1 to J do"
REFERENCES,0.6812386156648452,"Sample {y(t)
vk }v,k via Eq.(6).
Sample {ϕk}k via Eq.(7).
Sample {δ(t)}t via Eq.(48). Update ζ(t) as"
REFERENCES,0.6821493624772313,"ζ(T +1) = 0,
ζ(t) = ln

1 + δ(t)"
REFERENCES,0.6830601092896175,"τ0 + ζ(t+1)
, t = T, · · · , 1."
REFERENCES,0.6839708561020036,"Set l(T +1)
·k
= 0.
for t = T to 2 do"
REFERENCES,0.6848816029143898,"Sample {l(t)
k· }k and {l(t)
kk2}k,k2 via Eq.(8) and Eq.(9) respectively.
end for
for t = 1 to T do"
REFERENCES,0.6857923497267759,"Sample {θ(t)
k }k via Eq.(12) and Eq.(13).
end for
for i = 1 to I do"
REFERENCES,0.6867030965391621,"Sample {q(i)
k }k and {h(i)
k1k}k1,k via Eq.(16), Eq.(44) and Eq.(45)."
REFERENCES,0.6876138433515483,"Sample {π(i)
k }k via Eq.(14) and Eq.(19).
Sample η via Eq.(21).
end for
Sample ξ, {νk}k, β via Eq.(46), Eq.(47) and Eq.(49) respectively.
end for
Output posterior means: {θ(1:T )
k
}k, {ϕk}k, {π(i)
k }k, δ(1:T ), ξ, {νk}k, β."
REFERENCES,0.6885245901639344,Algorithm 2 Gibbs sampling algorithm for NS-PGDS (Dir-Gam-Dir Markov construction)
REFERENCES,0.6894353369763205,"Input: observed count sequence {y(t)}T
t=1, iterations J .
Initialize the model’s rank K, hyperparameters γ0, ϵ0, e0, f0.
for iter = 1 to J do"
REFERENCES,0.6903460837887068,"Sample {y(t)
vk }v,k via Eq.(6).
Sample {ϕk}k via Eq.(7).
Sample {δ(t)}t via Eq.(48). Update ζ(t) as"
REFERENCES,0.6912568306010929,"ζ(T +1) = 0,
ζ(t) = ln

1 + δ(t)"
REFERENCES,0.692167577413479,"τ0 + ζ(t+1)
, t = T, · · · , 1."
REFERENCES,0.6930783242258652,"Set l(T +1)
·k
= 0.
for t = T to 2 do"
REFERENCES,0.6939890710382514,"Sample {l(t)
k· }k and {l(t)
kk2}k,k2 via Eq.(8) and Eq.(9) respectively.
end for
for t = 1 to T do"
REFERENCES,0.6948998178506375,"Sample {θ(t)
k }k via Eq.(12) and Eq.(13).
end for
for i = 1 to I do"
REFERENCES,0.6958105646630237,"Sample {α(i)
k1k}k1,k and {c(i)
k }k via Eq.(24) and Eq.(32)."
REFERENCES,0.6967213114754098,"Sample {q(i)
k }k and {h(i)
k1k}k1,k via Eq.(22), Eq.(23), Eq.(44) and Eq.(45).
Sample {gk1k}k1,k and {gk1k2k}k1,k2,k via Eq.(25) and Eq.(26) respectively.
Sample {ψkk1k2}k,k1,k2 via Eq.(28).
Sample {γ(i)
k }k via Eq.(31).
Sample {π(i)
k }k via Eq.(14) and Eq.(29).
end for
Sample ξ, {νk}k, β via Eq.(46), Eq.(47) and Eq.(49) respectively.
end for
Output posterior means: {θ(1:T )
k
}k, {ϕk}k, {π(i)
k }k, δ(1:T ), ξ, {νk}k, β."
REFERENCES,0.697632058287796,Algorithm 3 Gibbs sampling algorithm for NS-PGDS (PR-Gam-Dir Markov construction)
REFERENCES,0.6985428051001822,"Input: observed count sequence {y(t)}T
t=1, iterations J .
Initialize the model’s rank K, hyperparameters γ0, ϵ0, e0, f0.
for iter = 1 to J do"
REFERENCES,0.6994535519125683,"Sample {y(t)
vk }v,k via Eq.(6).
Sample {ϕk}k via Eq.(7).
Sample {δ(t)}t via Eq.(48). Update ζ(t) as"
REFERENCES,0.7003642987249544,"ζ(T +1) = 0,
ζ(t) = ln

1 + δ(t)"
REFERENCES,0.7012750455373407,"τ0 + ζ(t+1)
, t = T, · · · , 1."
REFERENCES,0.7021857923497268,"Set l(T +1)
·k
= 0.
for t = T to 2 do"
REFERENCES,0.7030965391621129,"Sample {l(t)
k· }k and {l(t)
kk2}k,k2 via Eq.(8) and Eq.(9) respectively.
end for
for t = 1 to T do"
REFERENCES,0.7040072859744991,"Sample {θ(t)
k }k via Eq.(12) and Eq.(13).
end for
for i = 1 to I do"
REFERENCES,0.7049180327868853,"Sample {α(i)
k1k}k1,k and {c(i)
k }k via Eq.(33) and Eq.(43)."
REFERENCES,0.7058287795992714,"Sample {q(i)
k }k and {h(i)
k1k}k1,k via Eq.(33), Eq.(34), Eq.(44) and Eq.(45).
Sample {gk1k}k1,k via Eq.(36) and Eq.(37).
Sample {gk1k2k}k1,k2,k via Eq.(38).
Sample {γ(i)
k }k via Eq.(42).
Sample {ψkk1k2}k,k1,k2 via Eq.(40).
Sample {π(i)
k }k via Eq.(14), and Eq.(41).
end for
Sample ξ, {νk}k, β via Eq.(46), Eq.(47) and Eq.(49) respectively.
end for
Output posterior means: {θ(1:T )
k
}k, {ϕk}k, {π(i)
k }k, δ(1:T ), ξ, {νk}k, β."
REFERENCES,0.7067395264116576,"NeurIPS Paper Checklist
534"
CLAIMS,0.7076502732240437,"1. Claims
535"
CLAIMS,0.7085610200364298,"Question: Do the main claims made in the abstract and introduction accurately reflect the
536"
CLAIMS,0.7094717668488161,"paper’s contributions and scope?
537"
CLAIMS,0.7103825136612022,"Answer: [Yes]
538"
CLAIMS,0.7112932604735883,"Justification: The main contributions of this paper are the constructions of Poisson-Gamma
539"
CLAIMS,0.7122040072859745,"dynamical systems with non-stationary transition dynamics and the corresponding Gibbs
540"
CLAIMS,0.7131147540983607,"sampler. The constructions can be found in sec.2 and sec.3 and the derivation of Gibbs
541"
CLAIMS,0.7140255009107468,"sampler can be found in sec.4 and the appendix. The experiments have demonstrated the
542"
CLAIMS,0.714936247723133,"effectiveness and features of the proposed model.
543"
CLAIMS,0.7158469945355191,"Guidelines:
544"
CLAIMS,0.7167577413479053,"• The answer NA means that the abstract and introduction do not include the claims
545"
CLAIMS,0.7176684881602914,"made in the paper.
546"
CLAIMS,0.7185792349726776,"• The abstract and/or introduction should clearly state the claims made, including the
547"
CLAIMS,0.7194899817850637,"contributions made in the paper and important assumptions and limitations. A No or
548"
CLAIMS,0.7204007285974499,"NA answer to this question will not be perceived well by the reviewers.
549"
CLAIMS,0.7213114754098361,"• The claims made should match theoretical and experimental results, and reflect how
550"
CLAIMS,0.7222222222222222,"much the results can be expected to generalize to other settings.
551"
CLAIMS,0.7231329690346083,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
552"
CLAIMS,0.7240437158469946,"are not attained by the paper.
553"
LIMITATIONS,0.7249544626593807,"2. Limitations
554"
LIMITATIONS,0.7258652094717668,"Question: Does the paper discuss the limitations of the work performed by the authors?
555"
LIMITATIONS,0.726775956284153,"Answer: [Yes]
556"
LIMITATIONS,0.7276867030965392,"Justification: As we discussed in the conclusion part, the length of each sub-interval is a
557"
LIMITATIONS,0.7285974499089253,"constant and is treated as a hyper-parameter of the model. In the future work, we plan to
558"
LIMITATIONS,0.7295081967213115,"design a method that can find the point of change and thus the length of each sub-interval
559"
LIMITATIONS,0.7304189435336976,"can be determined automatically.
560"
LIMITATIONS,0.7313296903460837,"Guidelines:
561"
LIMITATIONS,0.73224043715847,"• The answer NA means that the paper has no limitation while the answer No means that
562"
LIMITATIONS,0.7331511839708561,"the paper has limitations, but those are not discussed in the paper.
563"
LIMITATIONS,0.7340619307832422,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
564"
LIMITATIONS,0.7349726775956285,"• The paper should point out any strong assumptions and how robust the results are to
565"
LIMITATIONS,0.7358834244080146,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
566"
LIMITATIONS,0.7367941712204007,"model well-specification, asymptotic approximations only holding locally). The authors
567"
LIMITATIONS,0.7377049180327869,"should reflect on how these assumptions might be violated in practice and what the
568"
LIMITATIONS,0.738615664845173,"implications would be.
569"
LIMITATIONS,0.7395264116575592,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
570"
LIMITATIONS,0.7404371584699454,"only tested on a few datasets or with a few runs. In general, empirical results often
571"
LIMITATIONS,0.7413479052823315,"depend on implicit assumptions, which should be articulated.
572"
LIMITATIONS,0.7422586520947176,"• The authors should reflect on the factors that influence the performance of the approach.
573"
LIMITATIONS,0.7431693989071039,"For example, a facial recognition algorithm may perform poorly when image resolution
574"
LIMITATIONS,0.74408014571949,"is low or images are taken in low lighting. Or a speech-to-text system might not be
575"
LIMITATIONS,0.7449908925318761,"used reliably to provide closed captions for online lectures because it fails to handle
576"
LIMITATIONS,0.7459016393442623,"technical jargon.
577"
LIMITATIONS,0.7468123861566485,"• The authors should discuss the computational efficiency of the proposed algorithms
578"
LIMITATIONS,0.7477231329690346,"and how they scale with dataset size.
579"
LIMITATIONS,0.7486338797814208,"• If applicable, the authors should discuss possible limitations of their approach to
580"
LIMITATIONS,0.7495446265938069,"address problems of privacy and fairness.
581"
LIMITATIONS,0.7504553734061931,"• While the authors might fear that complete honesty about limitations might be used by
582"
LIMITATIONS,0.7513661202185792,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
583"
LIMITATIONS,0.7522768670309654,"limitations that aren’t acknowledged in the paper. The authors should use their best
584"
LIMITATIONS,0.7531876138433515,"judgment and recognize that individual actions in favor of transparency play an impor-
585"
LIMITATIONS,0.7540983606557377,"tant role in developing norms that preserve the integrity of the community. Reviewers
586"
LIMITATIONS,0.7550091074681239,"will be specifically instructed to not penalize honesty concerning limitations.
587"
THEORY ASSUMPTIONS AND PROOFS,0.75591985428051,"3. Theory Assumptions and Proofs
588"
THEORY ASSUMPTIONS AND PROOFS,0.7568306010928961,"Question: For each theoretical result, does the paper provide the full set of assumptions and
589"
THEORY ASSUMPTIONS AND PROOFS,0.7577413479052824,"a complete (and correct) proof?
590"
THEORY ASSUMPTIONS AND PROOFS,0.7586520947176685,"Answer: [Yes]
591"
THEORY ASSUMPTIONS AND PROOFS,0.7595628415300546,"Justification: The derivation of the Gibbs sampler is the main theoretical part of this paper
592"
THEORY ASSUMPTIONS AND PROOFS,0.7604735883424408,"which can be found in sec.4 and the appendix.
593"
THEORY ASSUMPTIONS AND PROOFS,0.761384335154827,"Guidelines:
594"
THEORY ASSUMPTIONS AND PROOFS,0.7622950819672131,"• The answer NA means that the paper does not include theoretical results.
595"
THEORY ASSUMPTIONS AND PROOFS,0.7632058287795993,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
596"
THEORY ASSUMPTIONS AND PROOFS,0.7641165755919854,"referenced.
597"
THEORY ASSUMPTIONS AND PROOFS,0.7650273224043715,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
598"
THEORY ASSUMPTIONS AND PROOFS,0.7659380692167578,"• The proofs can either appear in the main paper or the supplemental material, but if
599"
THEORY ASSUMPTIONS AND PROOFS,0.7668488160291439,"they appear in the supplemental material, the authors are encouraged to provide a short
600"
THEORY ASSUMPTIONS AND PROOFS,0.76775956284153,"proof sketch to provide intuition.
601"
THEORY ASSUMPTIONS AND PROOFS,0.7686703096539163,"• Inversely, any informal proof provided in the core of the paper should be complemented
602"
THEORY ASSUMPTIONS AND PROOFS,0.7695810564663024,"by formal proofs provided in appendix or supplemental material.
603"
THEORY ASSUMPTIONS AND PROOFS,0.7704918032786885,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
604"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7714025500910747,"4. Experimental Result Reproducibility
605"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7723132969034608,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
606"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.773224043715847,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
607"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7741347905282332,"of the paper (regardless of whether the code and data are provided or not)?
608"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7750455373406193,"Answer: [Yes]
609"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7759562841530054,"Justification: We carefully described the proposed model in sec.2 and sec.3 and the experi-
610"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7768670309653917,"ment details can be found in sec.6.1.
611"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7777777777777778,"Guidelines:
612"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7786885245901639,"• The answer NA means that the paper does not include experiments.
613"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7795992714025501,"• If the paper includes experiments, a No answer to this question will not be perceived
614"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7805100182149363,"well by the reviewers: Making the paper reproducible is important, regardless of
615"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7814207650273224,"whether the code and data are provided or not.
616"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7823315118397086,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
617"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7832422586520947,"to make their results reproducible or verifiable.
618"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7841530054644809,"• Depending on the contribution, reproducibility can be accomplished in various ways.
619"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.785063752276867,"For example, if the contribution is a novel architecture, describing the architecture fully
620"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7859744990892532,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
621"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7868852459016393,"be necessary to either make it possible for others to replicate the model with the same
622"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7877959927140255,"dataset, or provide access to the model. In general. releasing code and data is often
623"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7887067395264117,"one good way to accomplish this, but reproducibility can also be provided via detailed
624"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7896174863387978,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
625"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7905282331511839,"of a large language model), releasing of a model checkpoint, or other means that are
626"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7914389799635702,"appropriate to the research performed.
627"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7923497267759563,"• While NeurIPS does not require releasing code, the conference does require all submis-
628"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7932604735883424,"sions to provide some reasonable avenue for reproducibility, which may depend on the
629"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7941712204007286,"nature of the contribution. For example
630"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7950819672131147,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
631"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7959927140255009,"to reproduce that algorithm.
632"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7969034608378871,"(b) If the contribution is primarily a new model architecture, the paper should describe
633"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7978142076502732,"the architecture clearly and fully.
634"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7987249544626593,"(c) If the contribution is a new model (e.g., a large language model), then there should
635"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7996357012750456,"either be a way to access this model for reproducing the results or a way to reproduce
636"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8005464480874317,"the model (e.g., with an open-source dataset or instructions for how to construct
637"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8014571948998178,"the dataset).
638"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.802367941712204,"(d) We recognize that reproducibility may be tricky in some cases, in which case
639"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8032786885245902,"authors are welcome to describe the particular way they provide for reproducibility.
640"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8041894353369763,"In the case of closed-source models, it may be that access to the model is limited in
641"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8051001821493625,"some way (e.g., to registered users), but it should be possible for other researchers
642"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8060109289617486,"to have some path to reproducing or verifying the results.
643"
OPEN ACCESS TO DATA AND CODE,0.8069216757741348,"5. Open access to data and code
644"
OPEN ACCESS TO DATA AND CODE,0.807832422586521,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
645"
OPEN ACCESS TO DATA AND CODE,0.8087431693989071,"tions to faithfully reproduce the main experimental results, as described in supplemental
646"
OPEN ACCESS TO DATA AND CODE,0.8096539162112932,"material?
647"
OPEN ACCESS TO DATA AND CODE,0.8105646630236795,"Answer: [No]
648"
OPEN ACCESS TO DATA AND CODE,0.8114754098360656,"Justification: The authors will release the data and code as soon as possible if this paper
649"
OPEN ACCESS TO DATA AND CODE,0.8123861566484517,"could be accepted.
650"
OPEN ACCESS TO DATA AND CODE,0.8132969034608379,"Guidelines:
651"
OPEN ACCESS TO DATA AND CODE,0.8142076502732241,"• The answer NA means that paper does not include experiments requiring code.
652"
OPEN ACCESS TO DATA AND CODE,0.8151183970856102,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
653"
OPEN ACCESS TO DATA AND CODE,0.8160291438979964,"public/guides/CodeSubmissionPolicy) for more details.
654"
OPEN ACCESS TO DATA AND CODE,0.8169398907103825,"• While we encourage the release of code and data, we understand that this might not be
655"
OPEN ACCESS TO DATA AND CODE,0.8178506375227687,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
656"
OPEN ACCESS TO DATA AND CODE,0.8187613843351548,"including code, unless this is central to the contribution (e.g., for a new open-source
657"
OPEN ACCESS TO DATA AND CODE,0.819672131147541,"benchmark).
658"
OPEN ACCESS TO DATA AND CODE,0.8205828779599271,"• The instructions should contain the exact command and environment needed to run to
659"
OPEN ACCESS TO DATA AND CODE,0.8214936247723132,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
660"
OPEN ACCESS TO DATA AND CODE,0.8224043715846995,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
661"
OPEN ACCESS TO DATA AND CODE,0.8233151183970856,"• The authors should provide instructions on data access and preparation, including how
662"
OPEN ACCESS TO DATA AND CODE,0.8242258652094717,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
663"
OPEN ACCESS TO DATA AND CODE,0.825136612021858,"• The authors should provide scripts to reproduce all experimental results for the new
664"
OPEN ACCESS TO DATA AND CODE,0.8260473588342441,"proposed method and baselines. If only a subset of experiments are reproducible, they
665"
OPEN ACCESS TO DATA AND CODE,0.8269581056466302,"should state which ones are omitted from the script and why.
666"
OPEN ACCESS TO DATA AND CODE,0.8278688524590164,"• At submission time, to preserve anonymity, the authors should release anonymized
667"
OPEN ACCESS TO DATA AND CODE,0.8287795992714025,"versions (if applicable).
668"
OPEN ACCESS TO DATA AND CODE,0.8296903460837887,"• Providing as much information as possible in supplemental material (appended to the
669"
OPEN ACCESS TO DATA AND CODE,0.8306010928961749,"paper) is recommended, but including URLs to data and code is permitted.
670"
OPEN ACCESS TO DATA AND CODE,0.831511839708561,"6. Experimental Setting/Details
671"
OPEN ACCESS TO DATA AND CODE,0.8324225865209471,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
672"
OPEN ACCESS TO DATA AND CODE,0.8333333333333334,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
673"
OPEN ACCESS TO DATA AND CODE,0.8342440801457195,"results?
674"
OPEN ACCESS TO DATA AND CODE,0.8351548269581056,"Answer: [Yes]
675"
OPEN ACCESS TO DATA AND CODE,0.8360655737704918,"Justification: The experiment details can be found in sec.6.1.
676"
OPEN ACCESS TO DATA AND CODE,0.836976320582878,"Guidelines:
677"
OPEN ACCESS TO DATA AND CODE,0.8378870673952641,"• The answer NA means that the paper does not include experiments.
678"
OPEN ACCESS TO DATA AND CODE,0.8387978142076503,"• The experimental setting should be presented in the core of the paper to a level of detail
679"
OPEN ACCESS TO DATA AND CODE,0.8397085610200364,"that is necessary to appreciate the results and make sense of them.
680"
OPEN ACCESS TO DATA AND CODE,0.8406193078324226,"• The full details can be provided either with the code, in appendix, or as supplemental
681"
OPEN ACCESS TO DATA AND CODE,0.8415300546448088,"material.
682"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8424408014571949,"7. Experiment Statistical Significance
683"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.843351548269581,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
684"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8442622950819673,"information about the statistical significance of the experiments?
685"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8451730418943534,"Answer: [Yes]
686"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8460837887067395,"Justification: Table 1 reports the predictive performance of the proposed model and the
687"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8469945355191257,"corresponding standard deviation. The results are computed by running the Gibbs sampling
688"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8479052823315119,"several times from different initialization.
689"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.848816029143898,"Guidelines:
690"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8497267759562842,"• The answer NA means that the paper does not include experiments.
691"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8506375227686703,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
692"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8515482695810564,"dence intervals, or statistical significance tests, at least for the experiments that support
693"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8524590163934426,"the main claims of the paper.
694"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8533697632058288,"• The factors of variability that the error bars are capturing should be clearly stated (for
695"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8542805100182149,"example, train/test split, initialization, random drawing of some parameter, or overall
696"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.855191256830601,"run with given experimental conditions).
697"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8561020036429873,"• The method for calculating the error bars should be explained (closed form formula,
698"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8570127504553734,"call to a library function, bootstrap, etc.)
699"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8579234972677595,"• The assumptions made should be given (e.g., Normally distributed errors).
700"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8588342440801457,"• It should be clear whether the error bar is the standard deviation or the standard error
701"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8597449908925319,"of the mean.
702"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.860655737704918,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
703"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8615664845173042,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
704"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8624772313296903,"of Normality of errors is not verified.
705"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8633879781420765,"• For asymmetric distributions, the authors should be careful not to show in tables or
706"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8642987249544627,"figures symmetric error bars that would yield results that are out of range (e.g. negative
707"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8652094717668488,"error rates).
708"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8661202185792349,"• If error bars are reported in tables or plots, The authors should explain in the text how
709"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8670309653916212,"they were calculated and reference the corresponding figures or tables in the text.
710"
EXPERIMENTS COMPUTE RESOURCES,0.8679417122040073,"8. Experiments Compute Resources
711"
EXPERIMENTS COMPUTE RESOURCES,0.8688524590163934,"Question: For each experiment, does the paper provide sufficient information on the com-
712"
EXPERIMENTS COMPUTE RESOURCES,0.8697632058287796,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
713"
EXPERIMENTS COMPUTE RESOURCES,0.8706739526411658,"the experiments?
714"
EXPERIMENTS COMPUTE RESOURCES,0.8715846994535519,"Answer: [Yes]
715"
EXPERIMENTS COMPUTE RESOURCES,0.8724954462659381,"Justification: The experiments are conducted on a server with an Intel(R) Xeon(R) CPU
716"
EXPERIMENTS COMPUTE RESOURCES,0.8734061930783242,"E5-2699Cv4 @ 2.20GHz and 64G RAM.
717"
EXPERIMENTS COMPUTE RESOURCES,0.8743169398907104,"Guidelines:
718"
EXPERIMENTS COMPUTE RESOURCES,0.8752276867030966,"• The answer NA means that the paper does not include experiments.
719"
EXPERIMENTS COMPUTE RESOURCES,0.8761384335154827,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
720"
EXPERIMENTS COMPUTE RESOURCES,0.8770491803278688,"or cloud provider, including relevant memory and storage.
721"
EXPERIMENTS COMPUTE RESOURCES,0.8779599271402551,"• The paper should provide the amount of compute required for each of the individual
722"
EXPERIMENTS COMPUTE RESOURCES,0.8788706739526412,"experimental runs as well as estimate the total compute.
723"
EXPERIMENTS COMPUTE RESOURCES,0.8797814207650273,"• The paper should disclose whether the full research project required more compute
724"
EXPERIMENTS COMPUTE RESOURCES,0.8806921675774135,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
725"
EXPERIMENTS COMPUTE RESOURCES,0.8816029143897997,"didn’t make it into the paper).
726"
CODE OF ETHICS,0.8825136612021858,"9. Code Of Ethics
727"
CODE OF ETHICS,0.8834244080145719,"Question: Does the research conducted in the paper conform, in every respect, with the
728"
CODE OF ETHICS,0.8843351548269581,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
729"
CODE OF ETHICS,0.8852459016393442,"Answer: [Yes]
730"
CODE OF ETHICS,0.8861566484517304,"Justification: We have checked the NeurIPS Code of Ethics and make sure this work is with
731"
CODE OF ETHICS,0.8870673952641166,"the NeurIPS Code of Ethics.
732"
CODE OF ETHICS,0.8879781420765027,"Guidelines:
733"
CODE OF ETHICS,0.8888888888888888,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
734"
CODE OF ETHICS,0.8897996357012751,"• If the authors answer No, they should explain the special circumstances that require a
735"
CODE OF ETHICS,0.8907103825136612,"deviation from the Code of Ethics.
736"
CODE OF ETHICS,0.8916211293260473,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
737"
CODE OF ETHICS,0.8925318761384335,"eration due to laws or regulations in their jurisdiction).
738"
BROADER IMPACTS,0.8934426229508197,"10. Broader Impacts
739"
BROADER IMPACTS,0.8943533697632058,"Question: Does the paper discuss both potential positive societal impacts and negative
740"
BROADER IMPACTS,0.895264116575592,"societal impacts of the work performed?
741"
BROADER IMPACTS,0.8961748633879781,"Answer: [Yes]
742"
BROADER IMPACTS,0.8970856102003643,"Justification: For positive societal impacts, we have discussed in conclusion section for the
743"
BROADER IMPACTS,0.8979963570127505,"potential application for textual analysis and social networks. And the authors think this
744"
BROADER IMPACTS,0.8989071038251366,"work does not have potential negative societal impacts.
745"
BROADER IMPACTS,0.8998178506375227,"Guidelines:
746"
BROADER IMPACTS,0.900728597449909,"• The answer NA means that there is no societal impact of the work performed.
747"
BROADER IMPACTS,0.9016393442622951,"• If the authors answer NA or No, they should explain why their work has no societal
748"
BROADER IMPACTS,0.9025500910746812,"impact or why the paper does not address societal impact.
749"
BROADER IMPACTS,0.9034608378870674,"• Examples of negative societal impacts include potential malicious or unintended uses
750"
BROADER IMPACTS,0.9043715846994536,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
751"
BROADER IMPACTS,0.9052823315118397,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
752"
BROADER IMPACTS,0.9061930783242259,"groups), privacy considerations, and security considerations.
753"
BROADER IMPACTS,0.907103825136612,"• The conference expects that many papers will be foundational research and not tied
754"
BROADER IMPACTS,0.9080145719489982,"to particular applications, let alone deployments. However, if there is a direct path to
755"
BROADER IMPACTS,0.9089253187613844,"any negative applications, the authors should point it out. For example, it is legitimate
756"
BROADER IMPACTS,0.9098360655737705,"to point out that an improvement in the quality of generative models could be used to
757"
BROADER IMPACTS,0.9107468123861566,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
758"
BROADER IMPACTS,0.9116575591985429,"that a generic algorithm for optimizing neural networks could enable people to train
759"
BROADER IMPACTS,0.912568306010929,"models that generate Deepfakes faster.
760"
BROADER IMPACTS,0.9134790528233151,"• The authors should consider possible harms that could arise when the technology is
761"
BROADER IMPACTS,0.9143897996357013,"being used as intended and functioning correctly, harms that could arise when the
762"
BROADER IMPACTS,0.9153005464480874,"technology is being used as intended but gives incorrect results, and harms following
763"
BROADER IMPACTS,0.9162112932604736,"from (intentional or unintentional) misuse of the technology.
764"
BROADER IMPACTS,0.9171220400728597,"• If there are negative societal impacts, the authors could also discuss possible mitigation
765"
BROADER IMPACTS,0.9180327868852459,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
766"
BROADER IMPACTS,0.918943533697632,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
767"
BROADER IMPACTS,0.9198542805100182,"feedback over time, improving the efficiency and accessibility of ML).
768"
SAFEGUARDS,0.9207650273224044,"11. Safeguards
769"
SAFEGUARDS,0.9216757741347905,"Question: Does the paper describe safeguards that have been put in place for responsible
770"
SAFEGUARDS,0.9225865209471766,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
771"
SAFEGUARDS,0.9234972677595629,"image generators, or scraped datasets)?
772"
SAFEGUARDS,0.924408014571949,"Answer: [NA]
773"
SAFEGUARDS,0.9253187613843351,"Justification: This work poses no such risks.
774"
SAFEGUARDS,0.9262295081967213,"Guidelines:
775"
SAFEGUARDS,0.9271402550091075,"• The answer NA means that the paper poses no such risks.
776"
SAFEGUARDS,0.9280510018214936,"• Released models that have a high risk for misuse or dual-use should be released with
777"
SAFEGUARDS,0.9289617486338798,"necessary safeguards to allow for controlled use of the model, for example by requiring
778"
SAFEGUARDS,0.9298724954462659,"that users adhere to usage guidelines or restrictions to access the model or implementing
779"
SAFEGUARDS,0.930783242258652,"safety filters.
780"
SAFEGUARDS,0.9316939890710383,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
781"
SAFEGUARDS,0.9326047358834244,"should describe how they avoided releasing unsafe images.
782"
SAFEGUARDS,0.9335154826958105,"• We recognize that providing effective safeguards is challenging, and many papers do
783"
SAFEGUARDS,0.9344262295081968,"not require this, but we encourage authors to take this into account and make a best
784"
SAFEGUARDS,0.9353369763205829,"faith effort.
785"
LICENSES FOR EXISTING ASSETS,0.936247723132969,"12. Licenses for existing assets
786"
LICENSES FOR EXISTING ASSETS,0.9371584699453552,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
787"
LICENSES FOR EXISTING ASSETS,0.9380692167577414,"the paper, properly credited and are the license and terms of use explicitly mentioned and
788"
LICENSES FOR EXISTING ASSETS,0.9389799635701275,"properly respected?
789"
LICENSES FOR EXISTING ASSETS,0.9398907103825137,"Answer: [NA]
790"
LICENSES FOR EXISTING ASSETS,0.9408014571948998,"Justification: This paper does not use existing assets.
791"
LICENSES FOR EXISTING ASSETS,0.941712204007286,"Guidelines:
792"
LICENSES FOR EXISTING ASSETS,0.9426229508196722,"• The answer NA means that the paper does not use existing assets.
793"
LICENSES FOR EXISTING ASSETS,0.9435336976320583,"• The authors should cite the original paper that produced the code package or dataset.
794"
LICENSES FOR EXISTING ASSETS,0.9444444444444444,"• The authors should state which version of the asset is used and, if possible, include a
795"
LICENSES FOR EXISTING ASSETS,0.9453551912568307,"URL.
796"
LICENSES FOR EXISTING ASSETS,0.9462659380692168,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
797"
LICENSES FOR EXISTING ASSETS,0.9471766848816029,"• For scraped data from a particular source (e.g., website), the copyright and terms of
798"
LICENSES FOR EXISTING ASSETS,0.9480874316939891,"service of that source should be provided.
799"
LICENSES FOR EXISTING ASSETS,0.9489981785063752,"• If assets are released, the license, copyright information, and terms of use in the
800"
LICENSES FOR EXISTING ASSETS,0.9499089253187614,"package should be provided. For popular datasets, paperswithcode.com/datasets
801"
LICENSES FOR EXISTING ASSETS,0.9508196721311475,"has curated licenses for some datasets. Their licensing guide can help determine the
802"
LICENSES FOR EXISTING ASSETS,0.9517304189435337,"license of a dataset.
803"
LICENSES FOR EXISTING ASSETS,0.9526411657559198,"• For existing datasets that are re-packaged, both the original license and the license of
804"
LICENSES FOR EXISTING ASSETS,0.953551912568306,"the derived asset (if it has changed) should be provided.
805"
LICENSES FOR EXISTING ASSETS,0.9544626593806922,"• If this information is not available online, the authors are encouraged to reach out to
806"
LICENSES FOR EXISTING ASSETS,0.9553734061930783,"the asset’s creators.
807"
NEW ASSETS,0.9562841530054644,"13. New Assets
808"
NEW ASSETS,0.9571948998178507,"Question: Are new assets introduced in the paper well documented and is the documentation
809"
NEW ASSETS,0.9581056466302368,"provided alongside the assets?
810"
NEW ASSETS,0.9590163934426229,"Answer: [NA]
811"
NEW ASSETS,0.9599271402550091,"Justification: This paper does not release new assets.
812"
NEW ASSETS,0.9608378870673953,"Guidelines:
813"
NEW ASSETS,0.9617486338797814,"• The answer NA means that the paper does not release new assets.
814"
NEW ASSETS,0.9626593806921676,"• Researchers should communicate the details of the dataset/code/model as part of their
815"
NEW ASSETS,0.9635701275045537,"submissions via structured templates. This includes details about training, license,
816"
NEW ASSETS,0.9644808743169399,"limitations, etc.
817"
NEW ASSETS,0.9653916211293261,"• The paper should discuss whether and how consent was obtained from people whose
818"
NEW ASSETS,0.9663023679417122,"asset is used.
819"
NEW ASSETS,0.9672131147540983,"• At submission time, remember to anonymize your assets (if applicable). You can either
820"
NEW ASSETS,0.9681238615664846,"create an anonymized URL or include an anonymized zip file.
821"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9690346083788707,"14. Crowdsourcing and Research with Human Subjects
822"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9699453551912568,"Question: For crowdsourcing experiments and research with human subjects, does the paper
823"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.970856102003643,"include the full text of instructions given to participants and screenshots, if applicable, as
824"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9717668488160291,"well as details about compensation (if any)?
825"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9726775956284153,"Answer: [NA]
826"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9735883424408015,"Justification: This paper does not involve crowdsourcing nor research with human subjects.
827"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9744990892531876,"Guidelines:
828"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9754098360655737,"• The answer NA means that the paper does not involve crowdsourcing nor research with
829"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.97632058287796,"human subjects.
830"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9772313296903461,"• Including this information in the supplemental material is fine, but if the main contribu-
831"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9781420765027322,"tion of the paper involves human subjects, then as much detail as possible should be
832"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9790528233151184,"included in the main paper.
833"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9799635701275046,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
834"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9808743169398907,"or other labor should be paid at least the minimum wage in the country of the data
835"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9817850637522769,"collector.
836"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.982695810564663,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
837"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9836065573770492,"Subjects
838"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9845173041894353,"Question: Does the paper describe potential risks incurred by study participants, whether
839"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9854280510018215,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
840"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9863387978142076,"approvals (or an equivalent approval/review based on the requirements of your country or
841"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9872495446265938,"institution) were obtained?
842"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.98816029143898,"Answer: [NA]
843"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9890710382513661,"Justification: This paper does not involve crowdsourcing nor research with human subjects.
844"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9899817850637522,"Guidelines:
845"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9908925318761385,"• The answer NA means that the paper does not involve crowdsourcing nor research with
846"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9918032786885246,"human subjects.
847"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9927140255009107,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
848"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9936247723132969,"may be required for any human subjects research. If you obtained IRB approval, you
849"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.994535519125683,"should clearly state this in the paper.
850"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9954462659380692,"• We recognize that the procedures for this may vary significantly between institutions
851"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9963570127504554,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
852"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9972677595628415,"guidelines for their institution.
853"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9981785063752276,"• For initial submissions, do not include any information that would break anonymity (if
854"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9990892531876139,"applicable), such as the institution conducting the review.
855"
