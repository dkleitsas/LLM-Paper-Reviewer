Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0014577259475218659,"Evaluating the worst-case performance of a reinforcement learning (RL) agent un-
der the strongest/optimal adversarial perturbations on state observations (within
some constraints) is crucial for understanding the robustness of RL agents. How-
ever, ﬁnding the optimal adversary is challenging, in terms of both whether we
can ﬁnd the optimal attack and how efﬁciently we can ﬁnd it. Existing works on
adversarial RL either use heuristics-based methods that may not ﬁnd the strongest
adversary, or directly train an RL-based adversary by treating the agent as a part of
the environment, which can ﬁnd the optimal adversary but may become intractable
in a large state space. This paper introduces a novel attacking method to ﬁnd the
optimal attacks through collaboration between a designed function named “actor”
and an RL-based learner named “director”. The actor crafts state perturbations for
a given policy perturbation direction, and the director learns to propose the best
policy perturbation directions. Our proposed algorithm, PA-AD, is theoretically
optimal and signiﬁcantly more efﬁcient than prior RL-based works in environ-
ments with large state spaces. Empirical results show that our proposed PA-AD
universally outperforms state-of-the-art attacking methods in various Atari and
MuJoCo environments. By applying PA-AD to adversarial training, we achieve
state-of-the-art empirical robustness in multiple tasks under strong adversaries."
INTRODUCTION,0.0029154518950437317,"1
INTRODUCTION"
INTRODUCTION,0.004373177842565598,"Deep Reinforcement Learning (DRL) has achieved incredible success in many applications. How-
ever, recent works (Huang et al., 2017; Pattanaik et al., 2018) reveal that a well-trained RL agent
may be vulnerable to test-time evasion attacks, making it risky to deploy RL models in high-stakes
applications. As in most related works, we consider a state adversary which adds imperceptible
noise to the observations of an agent such that its cumulative reward is reduced during test time."
INTRODUCTION,0.0058309037900874635,"In order to understand the vulnerability of an RL agent and to improve its certiﬁed robustness, it
is important to evaluate the worst-case performance of the agent under any adversarial attacks with
certain constraints. In other words, it is crucial to ﬁnd the strongest/optimal adversary that can
minimize the cumulative reward gained by the agent with ﬁxed constraints, as motivated in a recent
paper by Zhang et al. (2021). Therefore, we focus on the following question:"
INTRODUCTION,0.007288629737609329,"Given an arbitrary attack radius (budget) ϵ for each step of the deployment, what is the worst-
case performance of an agent under the strongest adversary?"
INTRODUCTION,0.008746355685131196,myopic
INTRODUCTION,0.01020408163265306,myopic
INTRODUCTION,0.011661807580174927,Victim
INTRODUCTION,0.013119533527696793,strongest
INTRODUCTION,0.014577259475218658,"Figure 1: An example that a myopic
adversary is not the strongest."
INTRODUCTION,0.016034985422740525,"Finding the strongest adversary in RL is challenging. Many
existing attacks (Huang et al., 2017; Pattanaik et al., 2018) are
based on heuristics, crafting adversarial states at every step in-
dependently, although steps are interrelated in contrast to im-
age classiﬁcation tasks. These heuristic methods can often ef-
fectively reduce the agent’s reward, but are not guaranteed to
achieve the strongest attack under a given budget. This type of
attack is “myopic” since it does not plan for the future. Fig-
ure 1 shows an intuitive example, where myopic adversaries
only prevent the agent from selecting the best action in the current step, but the strongest adversary
can strategically “lead” the agent to a trap, which is the worst event for the agent."
INTRODUCTION,0.01749271137026239,Published as a conference paper at ICLR 2022
INTRODUCTION,0.018950437317784258,"Achieving computational efﬁciency arises as another challenge in practice, even if the strongest
adversary can be found in theory. A recent work (Zhang et al., 2020a) points out that learning the
optimal state adversary is equivalent to learning an optimal policy in a new Markov Decision Process
(MDP). A follow-up work (Zhang et al., 2021) shows that the learned adversary signiﬁcantly outper-
forms prior adversaries in MuJoCo games. However, the state space and the action space of the new
MDP are both as large as the state space in the original environment, which can be high-dimensional
in practice. For example, video games and autonomous driving systems use images as observations.
In these tasks, learning the state adversary directly becomes computationally intractable."
INTRODUCTION,0.02040816326530612,"To overcome the above two challenges, we propose a novel attack method called Policy Adver-
sarial Actor Director (PA-AD), where we design a “director” and an “actor” that collaboratively
ﬁnds the optimal state perturbations. In PA-AD, a director learns an MDP named Policy Adversary
MDP (PAMDP), and an actor is embedded in the dynamics of PAMDP. At each step, the director
proposes a perturbing direction in the policy space, and the actor crafts a perturbation in the state
space to lead the victim policy towards the proposed direction. Through a trail-and-error process, the
director can ﬁnd the optimal way to cooperate with the actor and attack the victim policy. Theoret-
ical analysis shows that the optimal policy in PAMDP induces an optimal state adversary. The size
of PAMDP is generally smaller than the adversarial MDP deﬁned by Zhang et al. (2021) and thus is
easier to be learned efﬁciently using off-the-shelf RL algorithms. With our proposed director-actor
collaborative mechanism, PA-AD outperforms state-of-the-art attacking methods on various types
of environments, and improves the robustness of many DRL agents by adversarial training."
INTRODUCTION,0.021865889212827987,"Summary of Contributions
(1) We establish a theoretical understanding of the optimality of evasion attacks from the perspective
of policy perturbations, allowing a more efﬁcient implementation of optimal attacks.
(2) We introduce a Policy Adversary MDP (PAMDP) model, whose optimal policy induces the op-
timal state adversary under any attacking budget ϵ.
(3) We propose a novel attack method, PA-AD, which efﬁciently searches for the optimal adversary
in the PAMDP. PA-AD is a general method that works on stochastic and deterministic victim poli-
cies, vectorized and pixel state spaces, as well as discrete and continuous action spaces.
(4) Empirical study shows that PA-AD universally outperforms previous attacking methods in vari-
ous environments, including Atari games and MuJoCo tasks. PA-AD achieves impressive attacking
performance in many environments using very small attack budgets,
(5) Combining our strong attack PA-AD with adversarial training, we signiﬁcantly improve the ro-
bustness of RL agents, and achieve the state-of-the-art robustness in many tasks."
PRELIMINARIES AND NOTATIONS,0.023323615160349854,"2
PRELIMINARIES AND NOTATIONS"
PRELIMINARIES AND NOTATIONS,0.02478134110787172,"The Victim RL Agent
In RL, an agent interacts with an environment modeled by a Markov
Decision Process (MDP) denoted as a tuple M = ⟨S, A, P, R, γ⟩, where S is a state space with
cardinality |S|, A is an action space with cardinality |A|, P : S × A →∆(S) is the transition
function 1, R : S × A →R is the reward function, and γ ∈(0, 1) is the discount factor. In this
paper, we consider a setting where the state space is much larger than the action space, which arises
in a wide variety of environments. For notation simplicity, our theoretical analysis focuses on a ﬁnite
MDP, but our algorithm applies to continuous state spaces and continuous action spaces, as veriﬁed
in experiments. The agent takes actions according to its policy, π : S →∆(A). We suppose the
victim uses a ﬁxed policy π with a function approximator (e.g. a neural network) during test time.
We denote the space of all policies as Π, which is a Cartesian product of |S| simplices. The value
of a policy π ∈Π for state s ∈S is deﬁned as V π(s) = Eπ,P [P∞
t=0 γtR(st, at)|s0 = s]."
PRELIMINARIES AND NOTATIONS,0.026239067055393587,"Evasion Attacker
Evasion attacks are test-time attacks that aim to reduce the expected total
reward gained by the agent/victim. As in most literature (Huang et al., 2017; Pattanaik et al., 2018;
Zhang et al., 2020a), we assume the attacker knows the victim policy π (white-box attack). However,
the attacker does not know the environment dynamics, nor does it have the ability to change the
environment directly. The attacker can observe the interactions between the victim agent and the
environment, including states, actions and rewards. We focus on a typical state adversary (Huang
et al., 2017; Zhang et al., 2020a), which perturbs the state observations returned by the environment
before the agent observes them. Note that the underlying states in the environment are not changed."
PRELIMINARIES AND NOTATIONS,0.027696793002915453,1∆(X) denotes the the space of probability distributions over X.
PRELIMINARIES AND NOTATIONS,0.029154518950437316,Published as a conference paper at ICLR 2022
PRELIMINARIES AND NOTATIONS,0.030612244897959183,"Formally, we model a state adversary by a function h which perturbs state s ∈S into ˜s := h(s), so
that the input to the agent’s policy is ˜s instead of s. In practice, the adversarial perturbation is usually
under certain constraints. In this paper, we consider the common ℓp threat model (Goodfellow et al.,
2015): ˜s should be in Bϵ(s), where Bϵ(s) denotes an ℓp norm ball centered at s with radius ϵ ≥0, a
constant called the budget of the adversary for every step. With the budget constraint, we deﬁne the
admissible state adversary and the admissible adversary set as below."
PRELIMINARIES AND NOTATIONS,0.03206997084548105,"Deﬁnition 1 (Set of Admissible State Adversaries Hϵ). A state adversary h is said to be admissible
if ∀s ∈S, we have h(s) ∈Bϵ(s). The set of all admissible state adversaries is denoted by Hϵ."
PRELIMINARIES AND NOTATIONS,0.033527696793002916,"Then the goal of the attacker is to ﬁnd an adversary h∗in Hϵ that maximally reduces the cumulative
reward of the agent. In this work, we propose a novel method to learn the optimal state adversary
through the identiﬁcation of an optimal policy perturbation deﬁned and motivated in the next section."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.03498542274052478,"3
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.03644314868804665,"In this section, we ﬁrst motivate our idea of interpreting evasion attacks as perturbations of policies,
then discuss how to efﬁciently ﬁnd the optimal state adversary via the optimal policy perturbation."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.037900874635568516,"Figure 2: Equivalence between eva-
sion attacks and policy perturbations."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.03935860058309038,"Evasion Attacks Are Perturbations of Policies
Although
existing literature usually considers state-attacks and action-
attacks separately, we point out that evasion attacks, either ap-
plied to states or actions, are essentially equivalent to perturb-
ing the agent’s policy π into another policy πh in the policy
space Π. For instance, as shown in Figure 2, if the adversary
h alters state s into state ˜s, the victim selects an action ˜a based
on π(·|˜s). This is equivalent to directly perturbing π(·|s) to πh(·|s) := π(·|˜s). (See Appendix A for
more detailed analysis including action adversaries.)"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.04081632653061224,"In this paper, we aim to ﬁnd the optimal state adversary through the identiﬁcation of the “optimal
policy perturbation”, which has the following merits. (1) πh(·|s) usually lies in a lower dimensional
space than h(s) for an arbitrary state s ∈S. For example, in Atari games, the action space is discrete
and small (e.g. |A| = 18), while a state is a high-dimensional image. Then the state perturbation
h(s) is an image, while πh(·|s) is a vector of size |A|. (2) It is easier to characterize the optimality of
a policy perturbation than a state perturbation. How a state perturbation changes the value of a victim
policy depends on both the victim policy network and the environment dynamics. In contrast, how
a policy perturbation changes the victim value only depends on the environment. Our Theorem 4 in
Section 3 and Theorem 12 in Appendix B both provide insights about how V π changes as π changes
continuously. (3) Policy perturbation captures the essence of evasion attacks, and uniﬁes state and
action attacks. Although this paper focuses on state-space adversaries, the learned “optimal policy
perturbation” can also be used to conduct action-space attacks against the same victim."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.04227405247813411,"Characterizing the Optimal Policy Adversary
As depicted in Figure 3, the policy perturbation
serves as a bridge connecting the perturbations in the state space and the value space. Our goal is
to ﬁnd the optimal state adversary by identifying the optimal “policy adversary”. We ﬁrst deﬁne
an Admissible Adversarial Policy Set (Adv-policy-set) BH
ϵ (π) ⊂Π as the set of policies perturbed
from π by all admissible state adversaries h ∈Hϵ. In other words, when a state adversary perturbs
states within an ℓp norm ball Bϵ(·), the victim policy is perturbed within BH
ϵ (π)."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.043731778425655975,"Deﬁnition 2 (Admissible Adversarial Policy Set (Adv-policy-set) BH
ϵ (π)). For an MDP M, a
ﬁxed victim policy π, we deﬁne the admissible adversarial policy set (Adv-policy-set) w.r.t. π, de-
noted by BH
ϵ (π), as the set of policies that are perturbed from π by all admissible adversaries, i.e.,"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.04518950437317784,"BH
ϵ (π) := {πh ∈Π : ∃h ∈Hϵ s.t ∀s, πh(·|s) = π(·|h(s))}.
(1)"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.04664723032069971,"Remarks
(1) BH
ϵ (π) is a subset of the policy space Π and it surrounds the victim π, as shown
in Figure 3(middle). In the same MDP, BH
ϵ (π) varies for different victim π or different attack
budget ϵ. (2) In Appendix B, we characterize the topological properties of BH
ϵ (π). We show that
for a continuous function π (e.g., neural network), BH
ϵ (π) is connected and compact, and the value
functions generated by all policies in the Adv-policy-set BH
ϵ (π) form a polytope (Figure 3(right)),
following the polytope theorem by Dadashi et al. (2019)."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.048104956268221574,Published as a conference paper at ICLR 2022 𝑠 𝑆
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.04956268221574344,"ℎ(𝑠)
𝜋 𝜋! Π
𝑉 𝑉"" 𝑉""!"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.05102040816326531,"End-to-end
SA-RL:"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.052478134110787174,An RL problem in a large MDP
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.05393586005830904,"PA-AD
(ours):"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.05539358600583091,"An RL problem in a small MDP
An optimization problem"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.056851311953352766,"The Director
The Actor"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.05830903790087463,(Policy network)
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.0597667638483965,Environment
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.061224489795918366,"𝑩𝝐𝑯(𝝅) 𝑉!∗ 𝑉!""∗ 𝑩𝝐(𝒔)"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.06268221574344024,Victim 𝝅
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.0641399416909621,"Figure 3: A state adversary h perturbs s into h(s) ∈Bϵ(s) in the state space; hence, the victim’s policy π is
perturbed into πh within the Adv-policy-set BH
ϵ (π); as a result, the expected total reward the victim can gain
becomes V πh instead of V π. A prior work SA-RL (Zhang et al., 2021) directly uses an RL agent to learn
the best state adversary h∗, which works for MDPs with small state spaces, but suffers from high complexity
in larger MDPs. In contrast, we ﬁnd the optimal state adversary h∗efﬁciently through identifying the optimal
policy adversary πh∗. Our proposed attack method called PA-AD contains an RL-based “director” which learns
to propose policy perturbation πh in the policy space, and a non-RL “actor”, which targets at the proposed πh
and computes adversarial states in the state space. Through this collaboration, the director can learn the optimal
policy adversary πh∗using RL methods, such that the actor executes h∗as justiﬁed in Theorem 7."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.06559766763848396,"Given that the Adv-policy-set BH
ϵ (π) contains all the possible policies the victim may execute under
admissible state perturbations, we can characterize the optimality of a state adversary through the
lens of policy perturbations. Recall that the attacker’s goal is to ﬁnd a state adversary h∗∈Hϵ
that minimizes the victim’s expected total reward. From the perspective of policy perturbation, the
attacker’s goal is to perturb the victim’s policy to another policy πh∗∈BH
ϵ (π) with the lowest value.
Therefore, we can deﬁne the optimal state adversary and the optimal policy adversary as below."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.06705539358600583,"Deﬁnition 3 (Optimal State Adversary h∗and Optimal Policy Adversary πh∗). For an MDP
M, a ﬁxed policy π, and an admissible adversary set Hϵ with attacking budget ϵ,
(1) an optimal state adversary h∗satisﬁes h∗∈argminh∈HϵV πh(s), ∀s ∈S, which leads to
(2) an optimal policy adversary πh∗satisﬁes πh∗∈argminπh∈BH
ϵ (π)V πh(s), ∀s ∈S.
Recall that πh is the perturbed policy caused by adversary h, i.e., πh(·|s) = π(·|h(s)), ∀s ∈S."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.06851311953352769,"Deﬁnition 3 implies an equivalent relationship between the optimal state adversary and the optimal
policy adversary: an optimal state adversary leads to an optimal policy adversary, and any state
adversary that leads to an optimal policy adversary is optimal. Theorem 19 in Appendix D.1 shows
that there always exists an optimal policy adversary for a ﬁxed victim π, and learning the optimal
policy adversary is an RL problem. (A similar result have been shown by Zhang et al. (2020a) for
the optimal state adversary, while we focus on the policy perturbation.)"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.06997084548104957,"Due to the equivalence, if one ﬁnds an optimal policy adversary πh∗, then the optimal state adversary
can be found by executing targeted attacks with target policy πh∗. However, directly ﬁnding the
optimal policy adversary in the Adv-policy-set BH
ϵ (π) is challenging since BH
ϵ (π) is generated by
all admissible state adversaries in Hϵ and is hard to compute. To address this challenge, we ﬁrst get
insights from theoretical characterizations of the Adv-policy-set BH
ϵ (π). Theorem 4 below shows
that the “outermost boundary” of BH
ϵ (π) always contains an optimal policy adversary. Intuitively, a
policy π′ is in the outermost boundary of BH
ϵ (π) if and only if no policy in BH
ϵ (π) is farer away from
π than π′ in the direction π′ −π. Therefore, if an adversary can perturb a policy along a direction,
it should push the policy as far away as possible in this direction under the budget constraints.
Then, the adversary is guaranteed to ﬁnd an optimal policy adversary after trying all the perturbing
directions. In contrast, such a guarantee does not exist for state adversaries, justifying the beneﬁts
of considering policy adversaries. Our proposed algorithm in Section 4 applies this idea to ﬁnd the
optimal attack: an RL-based director searches for the optimal perturbing direction, and an actor is
responsible for pushing the policy to the outermost boundary of BH
ϵ (π) with a given direction."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.07142857142857142,"Theorem 4. For an MDP M, a ﬁxed policy π, and an admissible adversary set Hϵ, deﬁne the
outermost boundary of the admissible adversarial policy set BH
ϵ (π) w.r.t π as
∂πBH
ϵ (π) := {π′ ∈BH
ϵ (π) : ∀s ∈S, θ > 0, ∄ˆπ ∈BH
ϵ (π) s.t. ˆπ(·|s) = π′(·|s) + θ(π′(·|s) −π(·|s))}.
(2)
Then there exists a policy ˜π ∈∂πBH
ϵ (π), such that ˜π is the optimal policy adversary w.r.t. π."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.0728862973760933,"Theorem 4 is proven in Appendix B.3, and we visualize the outermost boundary in Appendix B.5."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.07434402332361516,Published as a conference paper at ICLR 2022
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.07580174927113703,"4
PA-AD: OPTIMAL AND EFFICIENT EVASION ATTACK"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.07725947521865889,"In this section, we ﬁrst formally deﬁne the optimality of an attack algorithm and discuss some exist-
ing attack methods. Then, based on the theoretical insights in Section 3, we introduce our algorithm,
Policy Adversarial Actor Director (PA-AD) that has an optimal formulation and is efﬁcient to use."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.07871720116618076,"Although many attack methods for RL agents have been proposed (Huang et al., 2017; Pattanaik
et al., 2018; Zhang et al., 2020a), it is not yet well-understood how to characterize the strength and
the optimality of an attack method. Therefore, we propose to formulate the optimality of an attack
algorithm, which answers the question “whether the attack objective ﬁnds the strongest adversary”."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.08017492711370262,"Deﬁnition 5 (Optimal Formulation of Attacking Algorithm). An attacking algorithm Algo is said to
have an optimal formulation iff for any MDP M, policy π and admissible adversary set Hϵ under
attacking budget ϵ, the set of optimal solutions to its objective, HAlgo
ϵ
, is a subset of the optimal
adversaries against π, i.e., HAlgo
ϵ
⊆H∗
ϵ := {h∗|h∗∈argminh∈HϵV πh(s), ∀s ∈S}."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.08163265306122448,"Many heuristic-based attacks, although are empirically effective and efﬁcient, do not meet the re-
quirements of optimal formulation. In Appendix D.3, we categorize existing heuristic attack meth-
ods into four types, and theoretically prove that there exist scenarios where these heuristic methods
may not ﬁnd the strongest adversary. A recent paper (Zhang et al., 2021) proposes to learn the opti-
mal state adversary using RL methods, which we will refer to as SA-RL in our paper for simplicity.
SA-RL can be viewed as an “end-to-end” RL attacker, as it directly learns the optimal state adversary
such that the value of the victim policy is minimized. The formulation of SA-RL satisﬁes Deﬁni-
tion 5 and thus is optimal. However, SA-RL learns an MDP whose state space and action space
are both the same as the original state space. If the original state space is high-dimensional (e.g.
images), learning a good policy in the adversary’s MDP may become computationally intractable,
as empirically shown in Section 6."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.08309037900874636,"Can we address the optimal attacking problem in an efﬁcient manner? SA-RL treats the victim and
the environment together as a black box and directly learns a state adversary. But if the victim policy
is known to the attacker (e.g. in adversarial training), we can exploit the victim model and simplify
the attacking problem while maintaining the optimality. Therefore, we propose a novel algorithm,
Policy Adversarial Actor Director (PA-AD), that has optimal formulation and is generally more ef-
ﬁcient than SA-RL. PA-AD decouples the whole attacking process into two simpler components:
policy perturbation and state perturbation, solved by a “director” and an “actor” through collabora-
tion. The director learns the optimal policy perturbing direction with RL methods, while the actor
crafts adversarial states at every step such that the victim policy is perturbed towards the given di-
rection. Compared to the black-box SA-RL, PA-AD is a white-box attack, but works for a broader
range of environments more efﬁciently. Note that PA-AD can be used to conduct black-box attack
based on the transferability of adversarial attacks (Huang et al., 2017), although it is out of the scope
of this paper. Appendix F.2 provides a comprehensive comparison between PA-AD and SA-RL in
terms of complexity, optimality, assumptions and applicable scenarios."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.08454810495626822,Environment Actor
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.08600583090379009,Reward
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.08746355685131195,"Action 
State"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.08892128279883382,Policy Perturbing
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.09037900874635568,Direction
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.09183673469387756,Victim Policy
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.09329446064139942,"The actor's task: similar to a (targeted) evasion attack in supervised learning.
Can be solved by optimization methods (FGSM, PGD, etc)."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.09475218658892129,"The director's task: minimize the total reward gained from the environment.
Can be solved by RL methods (PPO, DQN, etc)."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.09620991253644315,Director
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.09766763848396501,Our Method: Policy Adversarial Actor Director:
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.09912536443148688,Optimal And Efficient
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.10058309037900874,Environment State
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.10204081632653061,"Reward 
Action"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.10349854227405247,Victim Policy
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.10495626822157435,"State
Adversary"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.10641399416909621,An End-to-end RL Attacker (SA-RL):
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.10787172011661808,Optimal But Inefficient
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.10932944606413994,"Environment
State"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.11078717201166181,Action
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.11224489795918367,Victim Policy
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.11370262390670553,"A Heuristic Attacker:
Efficient But Non-optimal "
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.1151603498542274,"State
Adversary"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.11661807580174927,"Figure 4: An overview of PA-AD compared with a heuristic attacker and an end-to-end RL attacker. Heuristic
attacks are efﬁcient, but may not ﬁnd the optimal adversary as they do not learn from the environment dynamics.
An end-to-end RL attacker directly learns a policy to generate state perturbations, but is inefﬁcient in large-state-
space environments. In contrast, our PA-AD solves the attack problem with a combination of an RL-based
director and a non-RL actor, so that PA-AD achieves both optimality and efﬁciency."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.11807580174927114,Published as a conference paper at ICLR 2022
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.119533527696793,"Formally, for a given victim policy π, our proposed PA-AD algorithm solves a Policy Adversary
MDP (PAMDP) deﬁned in Deﬁnition 6. An actor denoted by g is embedded in the dynamics of the
PAMDP, and a director searches for an optimal policy ν∗in the PAMDP."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.12099125364431487,"Deﬁnition 6 (Policy Adversary MDP (PAMDP) c
M). Given an MDP M = ⟨S, A, P, R, γ⟩, a
ﬁxed stochastic victim policy π, an attack budget ϵ ≥0, we deﬁne a Policy Adversarial MDP
c
M = ⟨S, b
A, bP, bR, γ⟩, where the action space is b
A := {d ∈[−1, 1]|A|, P|A|
i=1 di = 0}, and ∀s, s′ ∈
S, ∀ba ∈b
A,
bP(s′|s, ba) =
X"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.12244897959183673,"a∈A π(a|g(ba, s))P(s′|s, a),
bR(s, ba) = −
X"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.1239067055393586,"a∈A π(a|g(ba, s))R(s, a),"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.12536443148688048,where g is the actor function deﬁned as
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.12682215743440234,"g(ba, s) = argmax˜s∈Bϵ(s)∥π(˜s) −π(s)∥subject to
 
π(˜s) −π(s)
Tba = ∥π(˜s) −π(s)∥∥ba∥.
(G)"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.1282798833819242,"If the victim policy is deterministic, i.e., πD := argmaxaπ(a|s), (subscript D stands for determin-
istic), the action space of PAMDP is b
AD :=A, and the actor function gD is"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.12973760932944606,"gD(ba, s) = argmax˜s∈Bϵ(s)
 
π(ba|˜s) −maxa∈A,a̸=baπ(a|˜s)

.
(GD)"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.13119533527696792,Detailed deﬁnition of the deterministic-victim version of PAMDP is in Appendix C.1.
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.1326530612244898,"A key to PA-AD is the director-actor collaboration mechanism. The input to director policy ν is the
current state s in the original environment, while its output ba is a signal to the actor denoting “which
direction to perturb the victim policy into”. b
A is designed to contain all “perturbing directions” in
the policy space. That is, ∀ba ∈bA, there exists a constant θ0 ≥0 such that ∀θ ≤θ0, π(·|s) + θ ba"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.13411078717201166,"∥ba∥
belongs to the simplex ∆(A). The actor g takes in the state s and director’s direction ba and then
computes a state perturbation within the attack budget. Therefore, the director and the actor together
induce a state adversary: h(s) := g(ν(s), s), ∀s ∈S. The deﬁnition of PAMDP is slightly different
for a stochastic victim policy and a deterministic victim policy, as described below.
For a stochastic victim π, the director’s action ba ∈b
A is designed to be a unit vector lying in the
policy simplex, denoting the perturbing direction in the policy space. The actor, once receiving the
perturbing direction ba, will “push” the policy as far as possible by perturbing s to g(ba, s) ∈Bϵ(s),
as characterized by the optimization problem (G). In this way, the policy perturbation resulted by
the director and the actor is always in the outermost boundary of BH
ϵ (π) w.r.t. the victim π, where
the optimal policy perturbation can be found according to Theorem 4.
For a deterministic victim πD, the director’s action ba ∈b
AD can be viewed as a target action in the
original action space, and the actor conducts targeted attacks to let the victim execute ba, by forcing
the logit corresponding to the target action to be larger than the logits of other actions."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.13556851311953352,"In both the stochastic-victim and deterministic-victim case, PA-AD has an optimal formulation as
stated in Theorem 7 (proven in Appendix D.2).
Theorem 7 (Optimality of PA-AD). For any MDP M, any ﬁxed victim policy π, and any attack
budget ϵ ≥0, an optimal policy ν∗in c
M induces an optimal state adversary against π in M. That
is, the formulation of PA-AD is optimal, i.e., HPA-AD ⊆H∗
ϵ ."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.13702623906705538,Algorithm 1: Policy Adversarial Actor Director (PA-AD)
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.13848396501457727,1 Input: Initialization of director’s policy ν; victim policy π; budget ϵ; start state s0
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.13994169096209913,"2 for t = 0, 1, 2, ... do"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.141399416909621,"3
Director samples a policy perturbing direction bat ∼ν(·|st)"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.14285714285714285,"4
Actor perturbs st to ˜st = gD(bat, st) if Victim is deterministic, otherwise to ˜st = g(bat, st)"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.14431486880466474,"5
Victim takes action at ∼π(·|˜st), proceeds to st+1, receives rt"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.1457725947521866,"6
Director saves (st, bat, −rt, st+1) to its buffer"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.14723032069970846,"7
Director updates its policy ν using any RL algorithm"
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.14868804664723032,"Efﬁciency of PA-AD
As commonly known, the sample complexity and computational cost of
learning an MDP usually grow with the cardinalities of its state space and action space. Both SA-
RL and PA-AD have state space S, the state space of the original MDP. But the action space of
SA-RL is also S, while our PA-AD has action space R|A| for stochastic victim policies, or A for
deterministic victim policies. In most DRL applications, the state space (e.g., images) is much larger
than the action space, then PA-AD is generally more efﬁcient than SA-RL as it learns a smaller MDP."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.15014577259475217,Published as a conference paper at ICLR 2022
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.15160349854227406,"The attacking procedure is illustrated in Algorithm 1. At step t, the director observes a state st, and
proposes a policy perturbation bat, then the actor searches for a state perturbation to meet the policy
perturbation. Afterwards, the victim acts with the perturbed state ˜st, then the director updates its
policy based on the opposite value of the victim’s reward. Note that the actor solves a constrained
optimization problem, (GD) or (G). Problem (GD) is similar to a targeted attack in supervised
learning, while the stochastic version (G) can be approximately solved with a Lagrangian relaxation.
In Appendix C.2, we provide our implementation details for solving the actor’s optimization, which
empirically achieves state-of-the-art attack performance as veriﬁed in Section 6."
UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS,0.15306122448979592,"Extending to Continuous Action Space
Our PA-AD can be extended to environments with con-
tinuous action spaces, where the actor minimizes the distance between the policy action and the
target action, i.e., argmins′∈Bϵ(s)∥π(s′) −ba∥. More details and formal deﬁnitions of the variant of
PA-AD in continuous action space are provided in Appendix C.3. In Section 6, we show experimen-
tal results in MuJoCo tasks, which have continuous action spaces."
RELATED WORK,0.15451895043731778,"5
RELATED WORK"
RELATED WORK,0.15597667638483964,"Heuristic-based Evasion Attacks on States
There are many works considering evasion attacks
on the state observations in RL. Huang et al. (2017) ﬁrst propose to use FGSM (Goodfellow et al.,
2015) to craft adversarial states such that the probability that the agent selects the “best” action is
minimized. The same objective is also used in a recent work by Korkmaz (2020), which adopts a
Nesterov momentum-based optimization method to further improve the attack performance. Pat-
tanaik et al. (2018) propose to lead the agent to select the “worst” action based on the victim’s Q
function and use gradient descent to craft state perturbations. Zhang et al. (2020a) deﬁne the con-
cept of a state-adversarial MDP (SAMDP) and propose two attack methods: Robust SARSA and
Maximal Action Difference. The above heuristic-based methods are shown to be effective in many
environments, although might not ﬁnd the optimal adversaries, as proven in Appendix D.3."
RELATED WORK,0.15743440233236153,"RL-based Evasion Attacks on States
As discussed in Section 4, SA-RL (Zhang et al., 2021)
uses an end-to-end RL formulation to learn the optimal state adversary, which achieves state-of-
the-art attacking performance in MuJoCo tasks. For a pixel state space, an end-to-end RL attacker
may not work as shown by our experiment in Atari games (Section 6). Russo & Proutiere (2021)
propose to use feature extraction to convert the pixel state space to a small state space and then learn
an end-to-end RL attacker. But such feature extractions require expert knowledge and can be hard
to obtain in many real-world applications. In contrast, our PA-AD works for both pixel and vector
state spaces and does not require expert knowledge."
RELATED WORK,0.1588921282798834,"Other Works Related to Adversarial RL
There are many other papers studying adversarial RL
from different perspectives, including limited-steps attacking (Lin et al., 2017; Kos & Song, 2017),
multi-agent scenarios (Gleave et al., 2020), limited access to data (Inkawhich et al., 2020), and
etc. Adversarial action attacks (Xiao et al., 2019; Tan et al., 2020; Tessler et al., 2019; Lee et al.,
2021) are developed separately from state attacks; although we mainly consider state adversaries,
our PA-AD can be extended to action attacks as formulated in Appendix A. Poisoning (Behzadan
& Munir, 2017; Huang & Zhu, 2019; Sun et al., 2021; Zhang et al., 2020b; Rakhsha et al., 2020) is
another type of adversarial attacks that manipulates the training data, different from evasion attacks
that deprave a well-trained policy. Training a robust agent is the focus of many recent works (Pinto
et al., 2017; Fischer et al., 2019; L¨utjens et al., 2020; Oikarinen et al., 2020; Zhang et al., 2020a;
2021). Although our main goal is to ﬁnd a strong attacker, we also show by experiments that our
proposed attack method signiﬁcantly improves the robustness of RL agents by adversarial training."
EXPERIMENTS,0.16034985422740525,"6
EXPERIMENTS"
EXPERIMENTS,0.1618075801749271,"In this section, we show that PA-AD produces stronger evasion attacks than state-of-the-art attack
algorithms on various OpenAI Gym environments, including Atari and MuJoCo tasks. Also, our
experiment justiﬁes that PA-AD can evaluate and improve the robustness of RL agents."
EXPERIMENTS,0.16326530612244897,"Baselines and Performance Metric
We compare our proposed attack algorithm with existing
evasion attack methods, including MinBest (Huang et al., 2017) which minimizes the probability that
the agent chooses the “best” action, MinBest +Momentum (Korkmaz, 2020) which uses Nesterov
momentum to improve the performance of MinBest, MinQ (Pattanaik et al., 2018) which leads"
EXPERIMENTS,0.16472303206997085,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.1661807580174927,"Environment
Natural
Reward
ϵ
Random
MinBest
MinBest +
Momentum
MinQ
MaxDiff
SA-RL
PA-AD
(ours) DQN"
EXPERIMENTS,0.16763848396501457,"Boxing
96 ± 4
0.001
95 ± 4
53 ± 16
52 ± 18
88 ± 7
95 ± 5
94 ± 6
19 ± 11"
EXPERIMENTS,0.16909620991253643,"Pong
21 ± 0
0.0002
21 ± 0
−10 ± 4
−14 ± 2
14 ± 3
15 ± 4
20 ± 1
−21 ± 0"
EXPERIMENTS,0.17055393586005832,"RoadRunner 46278 ± 4447 0.0005 44725 ± 6614 17012 ± 6243 15823 ± 5252 5765 ± 12331 36074 ± 6544 43615 ± 7183
0 ± 0"
EXPERIMENTS,0.17201166180758018,"Freeway
34 ± 1
0.0003
34 ± 1
12 ± 1
12 ± 1
15 ± 2
22 ± 3
34 ± 1
9 ± 1"
EXPERIMENTS,0.17346938775510204,"Seaquest
10650 ± 2716 0.0005 8177 ± 2962 3820 ± 1947
2337 ± 862
6468 ± 2493 5718 ± 1884 8152 ± 3113 2304 ± 838"
EXPERIMENTS,0.1749271137026239,"Alien
1623 ± 252 0.00075 1650 ± 381
819 ± 486
775 ± 648
938 ± 446
869 ± 279
1693 ± 439 256 ± 210"
EXPERIMENTS,0.17638483965014579,"Tutankham
227 ± 29
0.00075
221 ± 65
30 ± 13
26 ± 16
88 ± 74
130 ± 48
202 ± 65
0 ± 0 A2C"
EXPERIMENTS,0.17784256559766765,"Breakout
356 ± 79
0.0005
355 ± 79
86 ± 104
74 ± 95
N/A
304 ± 111
353 ± 79
44 ± 62"
EXPERIMENTS,0.1793002915451895,"Seaquest
1752 ± 70
0.005
1752 ± 73
356 ± 153
179 ± 83
N/A
46 ± 52
1752 ± 71
4 ± 13"
EXPERIMENTS,0.18075801749271136,"Pong
20 ± 1
0.0005
20 ± 1
−4 ± 8
−11 ± 7
N/A
18 ± 3
20 ± 1
−13 ± 6"
EXPERIMENTS,0.18221574344023322,"Alien
1615 ± 601
0.001
1629 ± 592 1062 ± 610
940 ± 565
N/A
1482 ± 633 1661 ± 625 507 ± 278"
EXPERIMENTS,0.1836734693877551,"Tutankham
258 ± 53
0.001
260 ± 54
139 ± 26
134 ± 28
N/A
196 ± 34
260 ± 54
71 ± 47"
EXPERIMENTS,0.18513119533527697,"RoadRunner 34367 ± 6355 0.002 35851 ± 6675 9198 ± 3814 5410 ± 3058
N/A
31856 ± 7125 36550 ± 6848 2773 ± 3468"
EXPERIMENTS,0.18658892128279883,"Table 1: Average episode rewards ± standard deviation of vanilla DQN and A2C agents under different evasion
attack methods in Atari environments. Results are averaged over 1000 episodes. Note that RS works for
continuous action spaces, thus is not included. MinQ is not applicable to A2C which does not have a Q
network. In each row, we bold the strongest (best) attack performance over all attacking methods."
EXPERIMENTS,0.1880466472303207,"the agent to select actions with the lowest action values based on the agent’s Q network, Robust
SARSA (RS) (Zhang et al., 2020a) which performs the MinQ attack with a learned stable Q network,
MaxDiff (Zhang et al., 2020a) which maximizes the KL-divergence between the original victim
policy and the perturbed policy, as well as SA-RL (Zhang et al., 2021) which directly learns the state
adversary with RL methods. We consider state attacks with ℓ∞norm as in most literature (Zhang
et al., 2020a; 2021). Appendix E.1 provides hyperparameter settings and implementation details."
EXPERIMENTS,0.18950437317784258,"PA-AD Finds the Strongest Adversaries in Atari Games
We ﬁrst evaluate the performance of
PA-AD against well-trained DQN (Mnih et al., 2015) and A2C (Mnih et al., 2016) victim agents
on Atari games with pixel state spaces. The observed pixel values are normalized to the range of
[0, 1]. SA-RL and PA-AD adversaries are learned using the ACKTR algorithm (Wu et al., 2017)
with the same number of steps. (Appendix E.1 shows hyperparameter settings.) Table 1 presents
the experiment results, where PA-AD signiﬁcantly outperforms all baselines against both DQN and
A2C victims. In contrast, SA-RL does not converge to a good adversary in the tested Atari games
with the same number of training steps as PA-AD, implying the importance of sample efﬁciency.
Surprisingly, using a relatively small attack budget ϵ, PA-AD leads the agent to the lowest possible
reward in many environments such as Pong, RoadRunner and Tutankham, whereas other attackers
may require larger attack budget to achieve the same attack strength. Therefore, we point out that
vanilla RL agents are extremely vulnerable to carefully learned adversarial attacks. Even if an RL
agent works well under naive attacks, a carefully learned adversary can let an agent totally fail with
the same attack budget, which stresses the importance of evaluating and improving the robustness
of RL agents using the strongest adversaries. Our further investigation in Appendix F.3 shows that
RL models can be generally more vulnerable than supervised classiﬁers, due to the different loss
and architecture designs. In Appendix E.2.1, we show more experiments with various selections of
the budget ϵ, where one can see PA-AD reduces the average reward more than all baselines over
varying ϵ’s in various environments."
EXPERIMENTS,0.19096209912536444,"PA-AD Finds the Strongest Adversaries MuJoCo Tasks
We further evaluate PA-AD on Mu-
JoCo games, where both state spaces and action spaces are continuous. We use the same setting with
Zhang et al. (2021), where both the victim and the adversary are trained with PPO (Schulman et al.,
2017). During test time, the victim executes a deterministic policy, and we use the deterministic"
EXPERIMENTS,0.1924198250728863,"Environment
State
Dimension
Natural
Reward
ϵ
Random
MaxDiff
RS
SA-RL
PA-AD
(ours)"
EXPERIMENTS,0.19387755102040816,"Hopper
11
3167 ± 542
0.075
2101 ± 793
1410 ± 655
794 ± 238
636 ± 9
160 ± 136"
EXPERIMENTS,0.19533527696793002,"Walker
17
4472 ± 635
0.05
3007 ± 1200
2869 ± 1271
1336 ± 654
1086 ± 516
804 ± 130"
EXPERIMENTS,0.1967930029154519,"HalfCheetah
17
7117 ± 98
0.15
5486 ± 1378
1836 ± 866
489 ± 758
−660 ± 218
−356 ± 307"
EXPERIMENTS,0.19825072886297376,"Ant
111
5687 ± 758
0.15
5261 ± 1005
1759 ± 828
268 ± 227
−872 ± 436
−2580 ± 872"
EXPERIMENTS,0.19970845481049562,"Table 2: Average episode rewards ± standard deviation of vanilla PPO agent under different evasion attack
methods in MuJoCo environments. Results are averaged over 50 episodes. Note that MinBest and MinQ do
not ﬁt this setting, since MinBest works for discrete action spaces, and MinQ requires the agent’s Q network."
EXPERIMENTS,0.20116618075801748,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.20262390670553937,"version of PA-AD with a continuous action space, as discussed in Section 4 and Appendix C.3. We
use the same attack budget ϵ as in Zhang et al. (2021) for all MuJoCo environments. Results in Ta-
ble 2 show that PA-AD reduces the reward much more than heuristic methods, and also outperforms
SA-RL in most cases. In Ant, our PA-AD achieves much stronger attacks than SA-RL, since PA-AD
is more efﬁcient than SA-RL when the state space is large. Admittedly, PA-AD requires additional
knowledge of the victim model, while SA-RL works in a black-box setting. Therefore, SA-RL is
more applicable to black-box scenarios with a relatively small state space, whereas PA-AD is more
applicable when the attacker has access to the victim (e.g. in adversarial training as shown in Ta-
ble 3). Appendix E.2.3 provides more empirical comparison between SA-RL and PA-AD, which
shows that PA-AD converges faster, takes less running time, and is less sensitive to hyperparameters
than SA-RL by a proper exploitation of the victim model."
EXPERIMENTS,0.20408163265306123,"Environment
Model
Natural
Reward
Random
MaxDiff
RS
SA-RL
PA-AD
(ours)
Average reward
across attacks"
EXPERIMENTS,0.2055393586005831,"Hopper
(state-dim: 11)
ϵ: 0.075"
EXPERIMENTS,0.20699708454810495,"SA-PPO
3705 ± 2
2710 ± 801
2652 ± 835
1130 ± 42
1076 ± 791
856 ± 21
1684.8"
EXPERIMENTS,0.20845481049562684,"ATLA-PPO
3291 ± 600 3165 ± 576
2814 ± 725
2244 ± 618
1772 ± 802
1232 ± 350
2245.4"
EXPERIMENTS,0.2099125364431487,"PA-ATLA-PPO (ours) 3449 ± 237 3325 ± 239
3145 ± 546
3002 ± 129
1529 ± 284
2521 ± 325
2704.4"
EXPERIMENTS,0.21137026239067055,"Walker
(state-dim: 17)
ϵ: 0.05"
EXPERIMENTS,0.21282798833819241,"SA-PPO
4487 ± 61
4867 ± 39
3668 ± 1789
3808 ± 138
2908 ± 1136
1042 ± 153
3258.6"
EXPERIMENTS,0.21428571428571427,"ATLA-PPO
3842 ± 475 3927 ± 368
3836 ± 492
3239 ± 894
3663 ± 707
1224 ± 770
3177.8"
EXPERIMENTS,0.21574344023323616,"PA-ATLA-PPO (ours) 4178 ± 529
4129 ± 78
4024 ± 572
3966 ± 307
3450 ± 478
2248 ± 131
3563.4"
EXPERIMENTS,0.21720116618075802,"Halfcheetah
(state-dim: 17)
ϵ: 0.15"
EXPERIMENTS,0.21865889212827988,"SA-PPO
3632 ± 20
3619 ± 18
3624 ± 23
3283 ± 20
3028 ± 23
2512 ± 16
3213.2"
EXPERIMENTS,0.22011661807580174,"ATLA-PPO
6157 ± 852 6164 ± 603
5790 ± 174
4806 ± 603
5058 ± 718
2576 ± 1548
4878.8"
EXPERIMENTS,0.22157434402332363,"PA-ATLA-PPO (ours) 6289 ± 342 6215 ± 346
5961 ± 53
5226 ± 114
4872 ± 79
3840 ± 673
5222.8"
EXPERIMENTS,0.2230320699708455,"Ant
(state-dim: 111)
ϵ: 0.15"
EXPERIMENTS,0.22448979591836735,"SA-PPO
4292 ± 384 4986 ± 452
4662 ± 522
3412 ± 1755 2511 ± 1117 −1296 ± 923
2855.0"
EXPERIMENTS,0.2259475218658892,"ATLA-PPO
5359 ± 153 5366 ± 104
5240 ± 170
4136 ± 149
3765 ± 101
220 ± 338
3745.4"
EXPERIMENTS,0.22740524781341107,"PA-ATLA-PPO (ours) 5469 ± 106 5496 ± 158
5328 ± 196
4124 ± 291
3694 ± 188
2986 ± 864
4325.6"
EXPERIMENTS,0.22886297376093295,"Table 3: Average episode rewards ± standard deviation of robustly trained PPO agents under different attack
methods. Results are averaged over 50 episodes. In each row corresponding to a robust agent, we bold the
strongest attack. The gray cells are the most robust agents with the highest average rewards across attacks.
Our PA-AD achieves the strongest attack against robust models, and our PA-ATLA-PPO achieves the most
robust performance under multiple attacks. The attack budget ϵ’s are the same as in Zhang et al. (2021)."
EXPERIMENTS,0.2303206997084548,"Training and Evaluating Robust Agents
A natural application of PA-AD is to evaluate the ro-
bustness of a known model, or to improve the robustness of an agent via adversarial training, where
the attacker has white-box access to the victim. Inspired by ATLA (Zhang et al., 2021) which al-
ternately trains an agent and an SA-RL attacker, we propose PA-ATLA, which alternately trains an
agent and a PA-AD attacker. In Table 3, we evaluate the performance of PA-ATLA for a PPO agent
(namely PA-ATLA-PPO) in MuJoCo tasks, compared with state-of-the-art robust training methods,
SA-PPO (Zhang et al., 2020a) and ATLA-PPO (Zhang et al., 2021) 2. From the table, we make
the following observations. (1) Our PA-AD attacker can signiﬁcantly reduce the reward of previ-
ous “robust” agents. Take the Ant environment as an example, although SA-PPO and ATLA-PPO
agents gain 2k+ and 3k+ rewards respectively under SA-RL, the previously strongest attack, our PA-
AD still reduces their rewards to about -1.3k and 200+ with the same attack budget. Therefore, we
emphasize the importance of understanding the worst-case performance of RL agents, even robustly-
trained agents. (2) Our PA-ATLA-PPO robust agents gain noticeably higher average rewards across
attacks than other robust agents, especially under the strongest PA-AD attack. Under the SA-RL
attack, PA-ATLA-PPO achieves comparable performance with ATLA-PPO, although ATLA-PPO
agents are trained to be robust against SA-RL. Due to the efﬁciency of PA-AD, PA-ATLA-PPO
requires fewer training steps than ATLA-PPO, as justiﬁed in Appendix E.2.4. The results of attack-
ing and training robust models in Atari games are in Appendix E.2.5 and E.2.6, where PA-ATLA
improves the robustness of Atari agents against strong attacks with ϵ as large as 3/255."
CONCLUSION,0.23177842565597667,"7
CONCLUSION"
CONCLUSION,0.23323615160349853,"In this paper, we propose an attack algorithm called PA-AD for RL problems, which achieves opti-
mal attacks in theory and signiﬁcantly outperforms prior attack methods in experiments. PA-AD can
be used to evaluate and improve the robustness of RL agents before deployment. A potential future
direction is to use our formulation for robustifying agents under both state and action attacks."
CONCLUSION,0.23469387755102042,"2We use ATLA-PPO(LSTM)+SA Reg, the most robust method reported by Zhang et al. (2021)."
CONCLUSION,0.23615160349854228,Published as a conference paper at ICLR 2022
CONCLUSION,0.23760932944606414,ACKNOWLEDGMENTS
CONCLUSION,0.239067055393586,"This work is supported by National Science Foundation IIS-1850220 CRII Award 030742-00001
and DOD-DARPA-Defense Advanced Research Projects Agency Guaranteeing AI Robustness
against Deception (GARD), and Adobe, Capital One and JP Morgan faculty fellowships."
ETHICS STATEMENT,0.24052478134110788,ETHICS STATEMENT
ETHICS STATEMENT,0.24198250728862974,"Despite the rapid advancement of interactive AI and ML systems using RL agents, the learning agent
could fail catastrophically in the presence of adversarial attacks, exposing a serious vulnerability
in current RL systems such as autonomous driving systems, market-making systems, and security
monitoring systems. Therefore, there is an urgent need to understand the vulnerability of an RL
model, otherwise, it may be risky to deploy a trained agent in real-life applications, where the
observations of a sensor usually contain unavoidable noise."
ETHICS STATEMENT,0.2434402332361516,"Although the study of a strong attack method may be maliciously exploited to attack some RL
systems, it is more important for the owners and users of RL systems to get aware of the vulnerability
of their RL agents under the strongest possible adversary. As the old saying goes, “if you know
yourself and your enemy, you’ll never lose a battle”. In this work, we propose an optimal and
efﬁcient algorithm for evasion attacks in Deep RL (DRL), which can signiﬁcantly inﬂuence the
performance of a well-trained DRL agent, by adding small perturbations to the state observations of
the agent. Our proposed method can automatically measure the vulnerability of an RL agent, and
discover the “ﬂaw” in a model that might be maliciously attacked. We also show in experiments
that our attack method can be applied to improve the robustness of an RL agent via robust training.
Since our proposed attack method achieves state-of-the-art performance, the RL agent trained under
our proposed attacker could be able to “defend” against any other adversarial attacks with the same
constraints. Therefore, our work has the potential to help combat the threat to high-stakes systems."
ETHICS STATEMENT,0.24489795918367346,"A limitation of PA-AD is that it requires the “attacker” to know the victim’s policy, i.e., PA-AD
is a white-box attack. If the attacker does not have full access to the victim, PA-AD can still be
used based on the transferability of adversarial attacks (Huang et al., 2017), although the optimal-
ity guarantee does not hold in this case. However, this limitation only restricts the ability of the
malicious attackers. In contrast, PA-AD should be used when one wants to evaluate the worst-case
performance of one’s own RL agent, or to improve the robustness of an agent under any attacks,
since PA-AD produces strong attacks efﬁciently. In these cases, PA-AD does have white-box access
to the agent. Therefore, PA-AD is more beneﬁcial to defenders than attackers."
REPRODUCIBILITY STATEMENT,0.24635568513119532,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.2478134110787172,"For theoretical results, we provide all detailed technical proofs and lemmas in Appendix. In Ap-
pendix A, we analyze the equivalence between evasion attacks and policy perturbations. In Ap-
pendix B, we theoretically prove some topological properties of the proposed Adv-policy-set, and
derive Theorem 4 that the outermost boundary of BH
ϵ (π) always contains an optimal policy per-
turbation. In Appendix D, we systematically characterize the optimality of many existing attack
methods. We theoretically show (1) the existence of an optimal adversary, (2) the optimality of our
proposed PA-AD, and (3) the optimality of many heuristic attacks, following our Deﬁnition 5 in
Section 4.
For experimental results, the detailed algorithm description in various types of environments is
provided in Appendix C. In Appendix E, we illustrate the implementation details, environment set-
tings, hyperparameter settings of our experiments. Additional experimental results show the per-
formance of our algorithm from multiple aspects, including hyperparameter sensitivity, learning
efﬁciency, etc. In addition, in Appendix F, we provide some detailed discussion on the algorithm
design, as well as a comprehensive comparison between our method and prior works.
The source code and running instructions for both Atari and MuJoCo experiments are in our sup-
plementary materials. We also provide trained victim and attacker models so that one can directly
test their performance using a test script we provide."
REPRODUCIBILITY STATEMENT,0.24927113702623907,Published as a conference paper at ICLR 2022
REFERENCES,0.25072886297376096,REFERENCES
REFERENCES,0.2521865889212828,"Vahid Behzadan and Arslan Munir. Vulnerability of deep reinforcement learning to policy induction
attacks. In International Conference on Machine Learning and Data Mining in Pattern Recogni-
tion, pp. 262–275. Springer, 2017."
REFERENCES,0.2536443148688047,"Robert Dadashi, Adrien Ali Taiga, Nicolas Le Roux, Dale Schuurmans, and Marc G. Bellemare. The
value function polytope in reinforcement learning. In Kamalika Chaudhuri and Ruslan Salakhut-
dinov (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97
of Proceedings of Machine Learning Research, pp. 1486–1495, Long Beach, California, USA,
09–15 Jun 2019. PMLR."
REFERENCES,0.25510204081632654,"Marc Fischer, Matthew Mirman, Steven Stalder, and Martin Vechev. Online robustness training for
deep reinforcement learning. arXiv preprint arXiv:1911.00887, 2019."
REFERENCES,0.2565597667638484,"Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, and Stuart Russell. Adver-
sarial policies: Attacking deep reinforcement learning. In International Conference on Learning
Representations, 2020."
REFERENCES,0.25801749271137026,"Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In International Conference on Learning Representations, 2015."
REFERENCES,0.2594752186588921,"David Silver Hado Van Hasselt, Arthur Guez. Deep reinforcement learning with double q-learning.
In Thirtieth AAAI Conference on Artiﬁcial Intelligence, 2016."
REFERENCES,0.260932944606414,"Ashley Hill, Antonin Rafﬁn, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, Rene Traore,
Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Rad-
ford, John Schulman, Szymon Sidor, and Yuhuai Wu. Stable baselines. https://github.
com/hill-a/stable-baselines, 2018."
REFERENCES,0.26239067055393583,"Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks
on neural network policies. arXiv preprint arXiv:1702.02284, 2017."
REFERENCES,0.26384839650145775,"Yunhan Huang and Quanyan Zhu. Deceptive reinforcement learning under adversarial manipula-
tions on cost signals. In International Conference on Decision and Game Theory for Security, pp.
217–237. Springer, 2019."
REFERENCES,0.2653061224489796,"Matthew Inkawhich, Yiran Chen, and Hai Li. Snooping attacks on deep reinforcement learning.
In Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Sys-
tems, AAMAS ’20, pp. 557–565, Richland, SC, 2020. International Foundation for Autonomous
Agents and Multiagent Systems. ISBN 9781450375184."
REFERENCES,0.26676384839650147,"Ezgi Korkmaz.
Nesterov momentum adversarial perturbations in the deep reinforcement learn-
ing domain. In ICML 2020 Inductive Biases, Invariances and Generalization in Reinforcement
Learning Workshop, 2020."
REFERENCES,0.26822157434402333,"Jernej Kos and Dawn Song.
Delving into adversarial attacks on deep policies.
arXiv preprint
arXiv:1705.06452, 2017."
REFERENCES,0.2696793002915452,"Ilya Kostrikov.
Pytorch implementations of reinforcement learning algorithms.
https://
github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail, 2018."
REFERENCES,0.27113702623906705,"Xian Yeow Lee, Yasaman Esfandiari, Kai Liang Tan, and Soumik Sarkar. Query-based targeted
action-space adversarial policies on deep reinforcement learning agents. In Proceedings of the
ACM/IEEE 12th International Conference on Cyber-Physical Systems, ICCPS ’21, pp. 87–97,
New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383530."
REFERENCES,0.2725947521865889,"Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, Ming-Yu Liu, and Min Sun.
Tactics of adversarial attack on deep reinforcement learning agents. In Proceedings of the 26th
International Joint Conference on Artiﬁcial Intelligence, IJCAI’17, pp. 3756–3762. AAAI Press,
2017. ISBN 9780999241103."
REFERENCES,0.27405247813411077,"Bj¨orn L¨utjens, Michael Everett, and Jonathan P How. Certiﬁed adversarial robustness for deep
reinforcement learning. In Conference on Robot Learning, pp. 1328–1337. PMLR, 2020."
REFERENCES,0.2755102040816326,Published as a conference paper at ICLR 2022
REFERENCES,0.27696793002915454,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations, 2018."
REFERENCES,0.2784256559766764,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529, 2015."
REFERENCES,0.27988338192419826,"Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International conference on machine learning, pp. 1928–1937, 2016."
REFERENCES,0.2813411078717201,"Tuomas Oikarinen, Tsui-Wei Weng, and Luca Daniel. Robust deep reinforcement learning through
adversarial loss. arXiv preprint arXiv:2008.01976, 2020."
REFERENCES,0.282798833819242,"Anay Pattanaik, Zhenyi Tang, Shuijing Liu, Gautham Bommannan, and Girish Chowdhary. Robust
deep reinforcement learning with adversarial attacks. In Proceedings of the 17th International
Conference on Autonomous Agents and MultiAgent Systems, AAMAS ’18, pp. 2040–2042, Rich-
land, SC, 2018. International Foundation for Autonomous Agents and Multiagent Systems."
REFERENCES,0.28425655976676384,"Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. Robust adversarial reinforce-
ment learning. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pp. 2817–2826. JMLR. org, 2017."
REFERENCES,0.2857142857142857,"Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John
Wiley & Sons, Inc., USA, 1st edition, 1994. ISBN 0471619779."
REFERENCES,0.28717201166180756,"Amin Rakhsha, Goran Radanovic, Rati Devidze, Xiaojin Zhu, and Adish Singla. Policy teaching
via environment poisoning: Training-time adversarial attacks against reinforcement learning. In
International Conference on Machine Learning, pp. 7974–7984, 2020."
REFERENCES,0.2886297376093295,"Alessio Russo and Alexandre Proutiere. Optimal attacks on reinforcement learning policies. In
American Control Conference (ACC)., 2021."
REFERENCES,0.29008746355685133,"John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017."
REFERENCES,0.2915451895043732,"Yanchao Sun, Da Huo, and Furong Huang. Vulnerability-aware poisoning mechanism for online rl
with unknown dynamics. In International Conference on Learning Representations, 2021."
REFERENCES,0.29300291545189505,"Kai Liang Tan, Yasaman Esfandiari, Xian Yeow Lee, Soumik Sarkar, et al. Robustifying reinforce-
ment learning agents via action space adversarial training. In 2020 American control conference
(ACC), pp. 3959–3964. IEEE, 2020."
REFERENCES,0.2944606413994169,"Chen Tessler, Yonathan Efroni, and Shie Mannor. Action robust reinforcement learning and appli-
cations in continuous control. In International Conference on Machine Learning, pp. 6215–6224.
PMLR, 2019."
REFERENCES,0.29591836734693877,"Ioannis Antonoglou Tom Schaul, John Quan and David Silver. Prioritized experience replay. In
International Conference on Learning Representations, 2016."
REFERENCES,0.29737609329446063,"Yuhuai Wu, Elman Mansimov, Roger B Grosse, Shun Liao, and Jimmy Ba. Scalable trust-region
method for deep reinforcement learning using kronecker-factored approximation. In Advances in
neural information processing systems, pp. 5279–5288, 2017."
REFERENCES,0.2988338192419825,"Chaowei Xiao, Xinlei Pan, Warren He, Jian Peng, Mingjie Sun, Jinfeng Yi, Mingyan Liu, Bo Li,
and Dawn Song.
Characterizing attacks on deep reinforcement learning.
arXiv preprint
arXiv:1907.09470, 2019."
REFERENCES,0.30029154518950435,"Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Mingyan Liu, Duane Boning, and Cho-Jui Hsieh.
Robust deep reinforcement learning against adversarial perturbations on state observations. In
H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural
Information Processing Systems, volume 33, pp. 21024–21037. Curran Associates, Inc., 2020a."
REFERENCES,0.30174927113702626,Published as a conference paper at ICLR 2022
REFERENCES,0.3032069970845481,"Huan Zhang, Hongge Chen, Duane S Boning, and Cho-Jui Hsieh. Robust reinforcement learning
on state observations with learned optimal adversary. In International Conference on Learning
Representations, 2021."
REFERENCES,0.30466472303207,"Xuezhou Zhang, Yuzhe Ma, Adish Singla, and Xiaojin Zhu. Adaptive reward-poisoning attacks
against reinforcement learning. In International Conference on Machine Learning, 2020b."
REFERENCES,0.30612244897959184,Published as a conference paper at ICLR 2022
REFERENCES,0.3075801749271137,"Appendix: Who Is the Strongest Enemy? Towards Optimal and Efﬁ-
cient Evasion Attacks in Deep RL"
REFERENCES,0.30903790087463556,"A
RELATIONSHIP BETWEEN EVASION ATTACKS AND POLICY
PERTURBATIONS."
REFERENCES,0.3104956268221574,"As mentioned in Section 2, all evasion attacks can be regarded as perturbations in the policy space.
To be more speciﬁc, we consider the following 3 cases, where we assume the victim uses policy π."
REFERENCES,0.3119533527696793,Case 1 (attack on states): deﬁne the state adversary as function h such that ∀s ∈S
REFERENCES,0.31341107871720114,h(s) = ˜s ∈Bϵ(s) := {s′ ∈S : ∥s′ −s∥≤ϵ}.
REFERENCES,0.31486880466472306,"(For simplicity, we consider the attacks within a ϵ-radius norm ball.)
In this case, for all s ∈S, the victim samples action from πh(·|s) = π(·|h(s)) = π(˜s), which is
equivalent to the victim executing a perturbed policy πh ∈Π."
REFERENCES,0.3163265306122449,"Case 2 (attack on actions for a deterministic π): deﬁne the action adversary as function h(A) :
S × A →A, and ∀s ∈S, a ∈A"
REFERENCES,0.3177842565597668,h(A)(a|s) = ˜a ∈Bϵ(a) := {a′ ∈A : ∥a′ −a∥≤ϵ}.
REFERENCES,0.31924198250728864,"In this case, there exists a policy πh(A) such that πh(A)(s) = h(A)(a|s) = ˜a, which is equivalent to
the victim executing policy πh(A) ∈Π."
REFERENCES,0.3206997084548105,"Case 3 (attack on actions for a stochastic π): deﬁne the action adversary as function h(A) : S×A →
A, and ∀s ∈S, a ∈A"
REFERENCES,0.32215743440233235,"h(A)(a|s) = ˜a such that {∥π(·|s) −Pr(·|s)∥≤ϵ},"
REFERENCES,0.3236151603498542,"where Pr(˜a|s) denotes the probability that the action is perturbed into ˜a.
In this case, there exists a policy πh(A) such that πh(A)(s) = Pr(·|s), which is equivalent to the
victim executing policy πh(A) ∈Π."
REFERENCES,0.3250728862973761,"Most existing evasion RL works (Huang et al., 2017; Pattanaik et al., 2018; Zhang et al., 2020a;
2021) focus on state attacks, while there are also some works (Tessler et al., 2019; Tan et al., 2020)
studying action attacks. For example, Tessler et al. (Tessler et al., 2019) consider Case 2 and Case 3
above and train an agent that is robust to action perturbations."
REFERENCES,0.32653061224489793,"These prior works study either state attacks or action attacks, considering them in two different
scenarios. However, the ultimate goal of robust RL is to train an RL agent that is robust to any threat
models. Otherwise, an agent that is robust against state attacks may still be ruined by an action
attacker. We take a step further to this ultimate goal by proposing a framework, policy attack, that
uniﬁes observation attacks and action attacks."
REFERENCES,0.32798833819241985,"Although the focus of this paper is on state attacks, we would like to point out that our proposed
method can also deal with action attacks (the director proposes a policy perturbation direction, and
an actor perturbs the action accordingly). It is also an exciting direction to explore hybrid attacks
(multiple actors conducting states perturbations and action perturbations altogether, directed by a
single director.) Our policy perturbation framework can also be easily incorporated in robust training
procedures, as an agent that is robust to policy perturbations is simultaneously robust to both state
attacks and action attacks."
REFERENCES,0.3294460641399417,"B
TOPOLOGICAL PROPERTIES OF THE ADMISSIBLE ADVERSARIAL POLICY
SET"
REFERENCES,0.33090379008746357,"As discussed in Section 3, ﬁnding the optimal state adversary in the admissible adversary set Hϵ
can be converted to a problem of ﬁnding the optimal policy adversary in the Adv-policy-set BH
ϵ (π).
In this section, we characterize the topological properties of BH
ϵ (π), and identify how the value
function changes as the policy changes within BH
ϵ (π)."
REFERENCES,0.3323615160349854,"In Section B.1, we show that under the settings we consider, BH
ϵ (π) is a connected and compact
subset of Π. Then, Section B.2, we deﬁne some additional concepts and re-formulate the notations."
REFERENCES,0.3338192419825073,Published as a conference paper at ICLR 2022
REFERENCES,0.33527696793002915,"In Section B.3, we prove Theorem 4 in Section 3 that the outermost boundary of BH
ϵ (π) always
contains an optimal policy perturbation. In Section B.4, we prove that the value functions of policies
in BH
ϵ (π) (or more generally, any connected and compact subset of Π) form a polytope. Section B.6
shows an example of the polytope result with a 2-state MDP, and Section B.5 shows examples of the
outermost boundary deﬁned in Theorem 4."
REFERENCES,0.336734693877551,"B.1
THE SHAPE OF ADV-POLICY-SET BH
ϵ (π)"
REFERENCES,0.33819241982507287,"It is important to note that BH
ϵ (π) is generally connected and compact as stated in the following
lemma."
REFERENCES,0.3396501457725947,"Lemma 8 (BH
ϵ (π) is connected and compact). Given an MDP M, a policy π that is a continuous
mapping, and admissible adversary set Hϵ := {h : h(s) ∈Bϵ(s), ∀s ∈S} (where ϵ > 0 is a
constant), the admissible adversarial policy set BH
ϵ (π) is a connected and compact subset of Π."
REFERENCES,0.34110787172011664,"Proof of Lemma 8. For an arbitrary state s ∈S, an admissible adversary h ∈Hϵ perturbs it within
an ℓp norm ball Bϵ(s), which is connected and compact. Since π is a continuous mapping, we know
π(s) is compact and connected."
REFERENCES,0.3425655976676385,"Therefore, BH
ϵ (π) as a Cartesian product of a ﬁnite number of compact and connected sets, is com-
pact and connected."
REFERENCES,0.34402332361516036,"B.2
ADDITIONAL NOTATIONS AND DEFINITIONS FOR PROOFS"
REFERENCES,0.3454810495626822,We ﬁrst formally deﬁne some concepts and notations.
REFERENCES,0.3469387755102041,"For a stationary and stochastic policy π : S →∆(A), we can deﬁne the state-to-state transition
function as
P π(s′|s) :=
X"
REFERENCES,0.34839650145772594,"a∈A
π(a|s)P(s′|s, a), ∀s, s′ ∈S,"
REFERENCES,0.3498542274052478,and the state reward function as
REFERENCES,0.35131195335276966,"Rπ(s) :=
X"
REFERENCES,0.35276967930029157,"a∈A
π(a|s)R(s, a), ∀s ∈S."
REFERENCES,0.35422740524781343,"Then the value of π, denoted as V π, can be computed via the Bellman equation"
REFERENCES,0.3556851311953353,V π = Rπ + γP πV π = (I −γP π)−1Rπ.
REFERENCES,0.35714285714285715,"We further use Πsi to denote the projection of Π into the simplex of the i-th state, i.e., the space of
action distributions at state si."
REFERENCES,0.358600583090379,"Let fv : Π →R|S| be a mapping that maps policies to their corresponding value functions. Let
V = fv(Π) be the space of all value functions."
REFERENCES,0.36005830903790087,"Dadashi et al. (Dadashi et al., 2019) show that the image of fv applied to the space of policies, i.e.,
fv(Π), form a (possibly non-convex) polytope as deﬁned below."
REFERENCES,0.36151603498542273,"Deﬁnition 9 ((Possibly non-convex) polytope). A is called a convex polytope iff there are k ∈N
points x1, x2, · · · , xk ∈Rn such that A = Conv(x1, · · · , xk). Furthermore, a (possibly non-
convex) polytope is deﬁned as a ﬁnite union of convex polytopes."
REFERENCES,0.3629737609329446,"And a more general concept is (possibly non-convex) polyhedron, which might not be bounded."
REFERENCES,0.36443148688046645,"Deﬁnition 10 ((Possibly non-convex) polyhedron). A is called a convex polyhedron iff it is the
intersection of k ∈N half-spaces ˆB1, ˆB2, · · · , ˆBk, i.e., A = ∩k
i=1 ˆBi. Furthermore, a (possibly
non-convex) polyhedron is deﬁned as a ﬁnite union of convex polyhedra."
REFERENCES,0.36588921282798836,"In addition, let Y π
s1,··· ,sk be the set of policies that agree with π on states s1, · · · , sk. Dadashi et
al. (Dadashi et al., 2019) also prove that the values of policies that agree on all but one state s, i.e.,
fv(Y π
S\{s}), form a line segment, which can be bracketed by two policies that are deterministic on
s. Our Lemma 14 extends this line segment result to our setting where policies are restricted in a
subset of policies."
REFERENCES,0.3673469387755102,Published as a conference paper at ICLR 2022
REFERENCES,0.3688046647230321,"B.3
PROOF OF THEOREM 4: BOUNDARY CONTAINS OPTIMAL POLICY PERTURBATIONS"
REFERENCES,0.37026239067055394,"Lemma 4 in Dadashi et al. (2019) shows that policies agreeing on all but one state have certain
monotone relations. We restate this result in Lemma 11 below.
Lemma 11 (Monotone Policy Interpolation). For any π0, π1 ∈Y π
S\{s} that agree with π on all
states except for s ∈S, deﬁne a function l : [0, 1] →V as"
REFERENCES,0.3717201166180758,l(α) = fv(απ1 + (1 −α)π0).
REFERENCES,0.37317784256559766,"Then we have
(1) l(0) ≽l(1) or l(1) ≽l(0) (≽stands for element-wise greater than or equal to);
(2) If l(0) = l(1), then l(α) = l(0), ∀α ∈[0, 1];
(3) If l(0) ̸= l(1), then there is a strictly monotonic rational function ρ : [0, 1] →R, such that
l(α) = ρ(α)l(1) + (1 −ρ(α))l(0)."
REFERENCES,0.3746355685131195,"More intuitively, Lemma 11 suggests that the value of πα := απ1 + (1 −α)π0 changes (strictly)
monotonically with α, unless the values of π0, π1 and πα are all equal. With this result, we can
proceed to prove Theorem 4."
REFERENCES,0.3760932944606414,Proof of Theorem 4. We will prove the theorem by contradiction.
REFERENCES,0.37755102040816324,"Suppose there is a policy ˆπ ∈BH
ϵ (π) such that ˆπ /∈∂πBH
ϵ (π) and fv(ˆπ) = V ˆπ < V ˜π, ∀˜π ∈BH
ϵ (π),
i.e., there is no optimal policy adversary on the outermost boundary of BH
ϵ (π)."
REFERENCES,0.37900874635568516,"Then according to the deﬁnition of ∂πBH
ϵ (π), there exists at least one state s ∈S such that we can
ﬁnd another policy π′ ∈BH
ϵ (π) agreeing with ˆπ on all states except for s, where π′(s) satisﬁes"
REFERENCES,0.380466472303207,ˆπ(·|s) = απ(·|s) + (1 −α)π′(·|s)
REFERENCES,0.3819241982507289,"for some scalar α ∈(0, 1)."
REFERENCES,0.38338192419825073,"Then by Lemma 11, either of the following happens:"
REFERENCES,0.3848396501457726,"(1) fv(π) ≻fv(ˆπ) ≻fv(π′).
(2) fv(π) = fv(ˆπ) = fv(π′);"
REFERENCES,0.38629737609329445,"Note that fv(ˆπ) ≻fv(π) is impossible because we have assumed ˆπ has the lowest value over all
policies in BH
ϵ (π) including π."
REFERENCES,0.3877551020408163,"If (1) is true, then π′ is a better policy adversary than ˆπ in BH
ϵ (π), which contradicts with the
assumption."
REFERENCES,0.3892128279883382,"If (2) is true, then π′ is another optimal policy adversary. By recursively applying the above process
to π′, we can ﬁnally ﬁnd an optimal policy adversary on the outermost boundary of BH
ϵ (π), which
also contradicts with our assumption."
REFERENCES,0.39067055393586003,"In summary, there is always an optimal policy adversary lying on the outermost boundary of BH
ϵ (π)."
REFERENCES,0.39212827988338195,"B.4
PROOF OF THEOREM 12: VALUES OF POLICIES IN ADMISSIBLE ADVERSARIAL POLICY
SET FORM A POLYTOPE"
REFERENCES,0.3935860058309038,"We ﬁrst present a theorem that describes the “shape” of the value functions generated by all admis-
sible adversaries (admissible adversarial policies).
Theorem 12 (Policy Perturbation Polytope). For a ﬁnite MDP M, consider a policy π and an
Adv-policy-set BH
ϵ (π). The space of values (a subspace of R|S|) of all policies in BH
ϵ (π), denoted
by VBH
ϵ (π), is a (possibly non-convex) polytope."
REFERENCES,0.39504373177842567,"In the remaining of this section, we prove a more general version of Theorem 12 as below.
Theorem 13 (Policy Subset Polytope). For a ﬁnite MDP M, consider a connected and compact
subset of Π, denoted as T . The space of values (a subspace of R|S|) of all policies in T , denoted by
VT , is a (possibly non-convex) polytope."
REFERENCES,0.3965014577259475,Published as a conference paper at ICLR 2022
REFERENCES,0.3979591836734694,"According to Lemma 8, BH
ϵ (π) is a connected and compact subset of Π, thus Theorem 12 is a
special case of Theorem 13."
REFERENCES,0.39941690962099125,"Additional Notations
To prove Theorem 13, we further deﬁne a variant of Y π
s1,··· ,sk as T π
s1,··· ,sk,
which is the set of policies that are in T and agree with π on states s1, · · · , sk, i.e.,"
REFERENCES,0.4008746355685131,"T π
s1,··· ,sk := {π′ ∈T : π′(si) = π(si), ∀i = 1, · · · , k}."
REFERENCES,0.40233236151603496,"Note that different from BH
ϵ (π), T is no longer restricted under an admissible adversary set and can
be any connected and compact subset of Π."
REFERENCES,0.4037900874635568,"The following lemma shows that the values of policies in T that agree on all but one state form a
line segment."
REFERENCES,0.40524781341107874,"Lemma 14. For a policy π ∈T and an arbitrary state s ∈S, there are two policies in ∂πT π
S\{s},
namely π−
s , π+
s , such that ∀π′ ∈T π
S\{s},"
REFERENCES,0.4067055393586006,"fv(π−
s ) ≼fv(π′) ≼fv(π+
s ),
(3)"
REFERENCES,0.40816326530612246,"where ≼denotes element-wise less than or equal to (if a ≼b, then ai ≤bi for all index i). Moreover,
the image of fv restricted to T π
S\{s} is a line segment."
REFERENCES,0.4096209912536443,"Proof of Lemma 14. Lemma 5 in Dadashi et al. (2019) has shown that fv is inﬁnitely differentiable
on Π, hence we know fv(T π
S\{s}) is compact and connected. According to Lemma 4 in Dadashi
et al. (2019), for any two policies π1, π2 ∈Y π
S\{s}, either fv(π1) ≼fv(π2), or fv(π2) ≼fv(π1)
(there exists a total order). The same property applies to T π
S\{s} since T π
S\{s} is a subset of Y π
S\{s}."
REFERENCES,0.4110787172011662,"Therefore, there exists π−
s and π+
s that achieve the minimum and maximum over all policies in
T π
S\{s}. Next we show π−
s and π+
s can be found on the outermost boundary of T π
S\{s}."
REFERENCES,0.41253644314868804,"Assume π+
s /∈∂πT π
S\{s}, and for all ˜π ∈T π
S\{s}, fv(˜π) ≺fv(π+
s ). Then we can ﬁnd another policy
π′ ∈∂πT π
S\{s} such that π+
s = απ + (1 −α)π′ for some scalar α ∈(0, 1). Then according to
Lemma 11, fv(π′) ≽fv(π+
s ), contradicting with the assumption. Therefore, one should be able to
ﬁnd a policy on the outermost boundary of T π
S\{s} whose value dominates all other policies. And
similarly, we can also ﬁnd π−
s on ∂πT π
S\{s}."
REFERENCES,0.4139941690962099,"Furthermore, fv(T π
S\{s}) is a subset of fv(Y π
S\{s}) since T π
S\{s} is a subset of Y π
S\{s}. Given that
fv(Y π
S\{s}) is a line segment, and fv(T π
S\{s}) is connected, we can conclude that fv(T π
S\{s}) is also
a line segment."
REFERENCES,0.41545189504373176,"Next, the following lemma shows that π+
s and π−
s and their linear combinations can generate values
that cover the set fv(T π
S\{s})."
REFERENCES,0.41690962099125367,"Lemma 15. For a policy π ∈T , an arbitrary state s ∈S, and π+
s , π−
s deﬁned in Lemma 14, the
following three sets are equivalent:
(1) fv(T π
S\{s});
(2) fv
 
closure(T π
S\{s})

, where closure(·) is the convex closure of a set;
(3) {fv(απ+
s + (1 −α)π−
s )|α ∈[0, 1]};
(4) {αfv(π+
s ) + (1 −α)fv(π−
s )|α ∈[0, 1]};"
REFERENCES,0.41836734693877553,Proof of Lemma 15. We show the equivalence by showing (1) ⊆(4) ⊆(3) ⊆(2) ⊆(1) as below.
REFERENCES,0.4198250728862974,"(2) ⊆(1): For any π1, π2 ∈T π
S\{s}, without loss of generality, suppose fv(π1) ≼fv(π2). According
to Lemma 11, for any α ∈[0, 1], fv(π1) ≼απ1 + (1 −α)π2 ≼fv(π2). Therefore, any convex
combinations of policies in T π
S\{s} has value that is in the range of fv(T π
S\{s}). So the values of
policies in the convex closure of T π
S\{s} do not exceed fv(T π
S\{s}), i.e., (2) ⊆(1)."
REFERENCES,0.42128279883381925,Published as a conference paper at ICLR 2022
REFERENCES,0.4227405247813411,"(3) ⊆(2): Based on the deﬁnition, απ+
s + (1 −α)π−
s ∈closure(T π
S\{s}), so (3) ⊆(2)."
REFERENCES,0.42419825072886297,"(4) ⊆(3): According to Lemma 11, there exists a strictly monotonic rational function ρ : [0, 1] →R,
such that
l(α) = fv(απ+
s + (1 −α)π−
s ) = ρ(α)fv(π+
s ) + (1 −ρ(α))fv(π−
s )."
REFERENCES,0.42565597667638483,"Therefore, due to intermediate value theorem, for α ∈[0, 1], ρ(α) takes all values from 0 to 1. So
(4) = (3)."
REFERENCES,0.4271137026239067,"(1) ⊆(4): Lemma 14 shows that fv(T π
S\{s}) is a line segment bracketed by fv(π+
s ) and fv(π−
s ).
Therefore, for any π′ ∈T π
S\{s}, its value is a convex combination of fv(π+
s ) and fv(π−
s )."
REFERENCES,0.42857142857142855,"Next, we show that the relative boundary of the value space constrained to T π
s1,··· ,sk is covered by
policies that dominate or are dominated in at least one state. The relative interior of set A in B is
deﬁned as the set of points in A that have a relative neighborhood in A ∩B, denoted as relintBA.
The relative boundary of set A in B, denoted as ∂BA, is deﬁned as the set of points in A that are
not in the relative interior of A, i.e., ∂BA = A\relintBA. When there is no ambiguity, we omit the
subscript of ∂to simplify notations."
REFERENCES,0.43002915451895046,"In addition, we introduce another notation F π
s1,··· ,sk := V π + span(Cπ
k+1, · · · , Cπ
|S|), where Cπ
i
stands for the i-th column of the matrix (I −γP π)−1. Note that F π
s1,··· ,sk is the same with Hπ
s1,··· ,sk
in Dadashi et al. Dadashi et al. (2019), and we change H to F in order to distinguish from the
admissible adversary set Hϵ deﬁned in our paper."
REFERENCES,0.4314868804664723,"Lemma 16. For a policy π ∈T , k ≤|S|, and a set of policies T π
s1,··· ,sk that agree with π on
s1, · · · , sk (perturb π only at sk+1, · · · , s|S|), deﬁne Vt := fv(T π
s1,··· ,sk). Deﬁne two sets of policies
X+
s := {π′ ∈T π
s1,··· ,sk : π′(·|s) = π+
s (·|s)}, and X−
s := {π′ ∈T π
s1,··· ,sk : π′(·|s) = π−
s (·|s)}.
We have that the relative boundary of Vt in F π
s1,··· ,sk is included in the value functions spanned by
policies in T π
s1,··· ,sk ∩(X+
sj ∪X−
sj) for at least one s /∈{s1, · · · , sk}, i.e., ∂Vt ⊂ |S|
["
REFERENCES,0.4329446064139942,"j=k+1
fv(T π
s1,··· ,sk ∩(X+
sj ∪X−
sj))"
REFERENCES,0.43440233236151604,Proof of Lemma 16. We ﬁrst prove the following claim:
REFERENCES,0.4358600583090379,"Claim 1: For a policy π0 ∈T π
s1,··· ,sk, if ∀j ∈{k + 1, · · · , |S|}, ∄π′ ∈closure(T π
s1,··· ,sk) ∩(X+
sj ∪
X−
sj) such that fv(π′) = fv(π0), then fv(π0) has a relative neighborhood in Vt ∩F π
s1,··· ,sk."
REFERENCES,0.43731778425655976,"First, based on Lemma 14 and Lemma 15, we can construct a policy ˆπ ∈closure(T π
s1,··· ,sk) such
that fv(ˆπ) = fv(π0) through the following steps:"
REFERENCES,0.4387755102040816,Algorithm 2: Constructing ˆπ
REFERENCES,0.4402332361516035,1 Set πk = π0
REFERENCES,0.44169096209912534,"2 for j = k + 1, · · · , |S| do"
REFERENCES,0.44314868804664725,"3
Find π+
sj, π−
sj ∈T πj−1
S\{sj}"
REFERENCES,0.4446064139941691,"4
Find πj = ˆαjπ+
sj + (1 −ˆαj)π−
sj such that fv(πj) = fv(πj−1)"
REFERENCES,0.446064139941691,5 Return ˆπ = π|S|
REFERENCES,0.44752186588921283,"Denote the concatenation of αj’s as a vector ˆα := [ˆαk+1, · · · , ˆα|S|]."
REFERENCES,0.4489795918367347,"According to the assumption that ∀j ∈{k + 1, · · · , |S|}, ∄π′ ∈closure(T π
s1,··· ,sk) ∩(X+
sj ∪X−
sj)
such that fv(π′) = fv(π0), we have ˆαj /∈{0, 1}, ∀j = k + 1, · · · , |S|. Then, deﬁne a function
φ : (0, 1)|S|−k →Vt such that"
REFERENCES,0.45043731778425655,"φ(α) = fv(πα), where"
REFERENCES,0.4518950437317784,"(
πα(·|sj) = απ+
sj + (1 −α)π−
sj
if j ∈{k + 1, · · · , |S|}
πα(·|sj) = ˆπ(·|sj)
otherwise"
REFERENCES,0.45335276967930027,Published as a conference paper at ICLR 2022
REFERENCES,0.45481049562682213,Then we have that
REFERENCES,0.45626822157434405,1. φ is continuously differentiable.
REFERENCES,0.4577259475218659,2. φ(ˆα) = fv(ˆπ).
REFERENCES,0.45918367346938777,"3.
∂φ
∂αj is non-zero at ˆα (because of Lemma 11 (3))."
REFERENCES,0.4606413994169096,"4.
∂φ
∂αj is along the i-the column of (I −γP ˆπ)−1 (see Lemma 3 in Dadashi et al. Dadashi
et al. (2019))."
REFERENCES,0.4620991253644315,"Therefore, by the inverse theorem function, there is a neighborhood of φ(α) = fv(ˆπ) in the image
space."
REFERENCES,0.46355685131195334,"Now we have proved Claim 1. As a result, for any policy π0 ∈T π
s1,··· ,sk, if fv(π0) is in the relative
boundary of Vt in F π
s1,··· ,sk, then ∃j ∈{k + 1, · · · , |S|}, π′ ∈closure(T π
s1,··· ,sk) ∩(X+
sj ∪X−
sj)
such that fv(π′) = fv(π0). Based on Lemma 15, we can also ﬁnd π′′ ∈T π
s1,··· ,sk ∩(X+
sj ∪X−
sj)
such that fv(π′′) = fv(π0). So Lemma 16 holds."
REFERENCES,0.4650145772594752,"Now, we are ﬁnally ready to prove Theorem 13."
REFERENCES,0.46647230320699706,"Proof of Theorem 13. We will show that ∀{s1, · · · , sk} ⊆S, the value Vt = fv(T π
s1,··· ,sk) is a
polytope."
REFERENCES,0.4679300291545189,"We prove the above claim by induction on the cardinality of the number of states k. In the base case
where k = |S|, Vt = {fv(π)} is a polytope."
REFERENCES,0.46938775510204084,"Suppose the claim holds for k + 1, then we show it also holds for k, i.e., for a policy π ∈Π, the
value of T π
s1,··· ,sk ⊆Y π
s1,··· ,sk ⊆Π for a polytope."
REFERENCES,0.4708454810495627,"According to Lemma 16, we have ∂Vt ⊂ |S|
["
REFERENCES,0.47230320699708456,"j=k+1
fv(T π
s1,··· ,sk ∩(X+
sj ∪X−
sj)) = |S|
["
REFERENCES,0.4737609329446064,"j=k+1
Vt ∩(F +
sj ∪F −
sj))"
REFERENCES,0.4752186588921283,"where ∂Vt denotes the relative boundary of Vt in F π
s1,··· ,sk; F +
sj and F −
sj are two afﬁne hyperplanes
of F π
s1,··· ,sk, standing for the value space of policies that agree with π+
sj and π−
sj in state sj respec-
tively."
REFERENCES,0.47667638483965014,Then we can get
REFERENCES,0.478134110787172,"1. Vt = fv(T π
s1,··· ,sk) is closed as T π
s1,··· ,sk is compact and fv is continuous."
REFERENCES,0.47959183673469385,"2. ∂Vt ⊂S|S|
j=k+1(F +
sj ∪F −
sj)), a ﬁnite number of afﬁne hyperplanes in F π
s1,··· ,sk."
REFERENCES,0.48104956268221577,"3. Vt ∩F +
sj (or Vt ∩F −
sj) is a polyhedron by induction assumption."
REFERENCES,0.48250728862973763,"Hence, based on Proposition 1 by Dadashi et al. Dadashi et al. (2019), we get Vt is a polyhedron.
Since Vt ⊆V is bounded, we can further conclude that Vt is a polytope."
REFERENCES,0.4839650145772595,"Therefore, for an arbitrary connected and compact set of policies T ⊆Π, let π ∈T be an arbitrary
policy in T , then fv(T ) = fv(T π
∅) is a polytope."
REFERENCES,0.48542274052478135,"B.5
EXAMPLES OF THE OUTERMOST BOUNDARY"
REFERENCES,0.4868804664723032,"See Figure 5 for examples of the outermost boundary for different BH
ϵ (π)’s."
REFERENCES,0.48833819241982507,Published as a conference paper at ICLR 2022
REFERENCES,0.4897959183673469,"Figure 5: Two examples of the outermost boundary with |A| = 3 actions at one single state s. The large
triangle denotes the distributions over the action space at state s, i.e., Πs; π1, π2 and π3 are three policies
that deterministically choose a1, a2 and a3 respectively. π is the victim policy, the dark green area is the
BH
ϵ (π)s : BH
ϵ (π) ∩Πs. The red solid curve depicts the outermost boundary of BH
ϵ (π)s. Note that a policy is
in the outermost boundary of BH
ϵ (π) iff it is in the outermost boundary of BH
ϵ (π)s for all s ∈S. !""∗ !""$ ∗"
REFERENCES,0.4912536443148688,"Figure 6: Value space of an example MDP. The values of the whole policy space Π form a polytope (blue) as
suggested by Dadashi et al. (2019). The values of all perturbed policies with Hϵ also form a polytope (green)
as suggested by Theorem 12."
REFERENCES,0.49271137026239065,"B.6
AN EXAMPLE OF THE POLICY PERTURBATION POLYTOPE"
REFERENCES,0.49416909620991256,"An example is given by Figure 6, where we deﬁne an MDP with 2 states and 3 actions. We train
an DQN agent with one-hot encodings of the states, and then randomly perturb the states within
an ℓ∞ball with ϵ = 0.8. By sampling 5M random policies, and 100K random perturbations, we
visualize the value space of approximately the whole policy space Π and the admissible adversarial
policy set BH
ϵ (π), both of which are polytopes (boundaries are ﬂat). A learning agent searches for
the optimal policy π∗whose value is the upper right vertex of the larger blue polytope, while the
attacker attempts to ﬁnd an optimal adversary h∗, which perturbs a given clean policy π to the worst
perturbed policy πh∗whose value is the lower left vertex of the smaller green polytope. This also
justiﬁes the fact that learning an optimal adversary is as difﬁcult as learning an optimal policy in an
RL problem."
REFERENCES,0.4956268221574344,The example MDP Mex:
REFERENCES,0.4970845481049563,"|A| = 3, γ = 0.8
ˆr = [−0.1, −1., 0.1, 0.4, 1.5, 0.1]
ˆP = [[0.9, 0.1], [0.2, 0.8], [0.7, 0.3], [0.05, 0.95], [0.25, 0.75], [0.3, 0.7]]"
REFERENCES,0.49854227405247814,The base/clean policy π:
REFERENCES,0.5,"π(a1|s1) = 0.215, π(a2|s1) = 0.429, π(a3|s1) = 0.356
π(a1|s2) = 0.271, π(a2|s2) = 0.592, π(a3|s2) = 0.137"
REFERENCES,0.5014577259475219,Published as a conference paper at ICLR 2022
REFERENCES,0.5029154518950437,"C
EXTENTIONS AND ADDITIONAL DETAILS OF OUR ALGORITHM"
REFERENCES,0.5043731778425656,"C.1
ATTACKING A DETERMINISTIC VICTIM POLICY"
REFERENCES,0.5058309037900874,"For a deterministic victim πD = argmaxaπ(a|s), we deﬁne Deterministic Policy Adversary MDP
(D-PAMDP) as below, where a subscript D is added to all components to distinguish them from their
stochastic counterparts. In D-PAMDP, the director proposes a target action baD ∈A(=: b
AD), and
the actor tries its best to let the victim output this target action.
Deﬁnition 17 (Deterministic Policy Adversary MDP (D-PAMDP)). Given an MDP M =
⟨S, A, P, R, γ⟩, a ﬁxed and deterministic victim policy πD, we deﬁne a Deterministic Policy Ad-
versarial MDP c
MD = ⟨S, b
AD, bPD, bRD, γ⟩, where the action space is b
AD = b
AD, and ∀s, s′ ∈
S, ∀ba ∈A,
bPD(s′|s, ba) = P(s′|s, πD(g(ba, s))),
bRD(s, ba) = −R(s, πD(g(ba, s))).
The actor function g is deﬁned as
gD(ba, s) = argmax˜s∈Bϵ(s)
 
π(ba|˜s) −maxa∈A,a̸=baπ(a|˜s)

(GD)"
REFERENCES,0.5072886297376094,The optimal policy of D-PAMDP is an optimal adversary against πD as proved in Appendix D.2.2
REFERENCES,0.5087463556851312,"C.2
IMPLEMENTATION DETAILS OF PA-AD"
REFERENCES,0.5102040816326531,"To address the actor function g (or gD) deﬁned in (G) and (GD), we let the actor maximize objectives
JD and J within the Bϵ(·) ball around the original state, for a deterministic victim and a stochastic
victim, respectively. Below we explicitly deﬁne JD and J."
REFERENCES,0.5116618075801749,"Actor Objective for Deterministic Victim
For the deterministic variant of PA-AD, the actor func-
tion (GD) is simple and can be directly solved to identify the optimal adversary. Concretely, we
deﬁne the following objective"
REFERENCES,0.5131195335276968,"JD(˜s; ba, s) := π(ba|˜s) −maxa∈A,a̸=baπ(a|˜s),
(JD)"
REFERENCES,0.5145772594752187,"which can be realized with the multi-class classiﬁcation hinge loss. In practice, a relaxed cross-
entropy objective can also be used to maximize π(ba|˜s)."
REFERENCES,0.5160349854227405,"Actor Objective for Stochastic Victim
Different from the deterministic-victim case, the actor
function for a stochastic victim deﬁned in (G) requires solving a more complex optimization prob-
lem with a non-convex constraint set, which in practice can be relaxed to (J) (a Lagrangian relax-
ation) to efﬁciently get an approximation of the optimal adversary.
argmax˜s∈Bϵ(s)J(˜s; ba, s) := ∥π(·|˜s) −π(·|s)∥+ λ × CosineSim
 
π(·|˜s) −π(·|s), ba

(J)
where CosineSim in the second refers to the cosine similarity function; the ﬁrst term measures how
far away the policy is perturbed from the victim policy; λ is a hyper-parameter controlling the trade-
off between the two terms. Experimental results show that our PA-AD is not sensitive to the value
of λ. In our reported results in Section 6, we set λ as 1. Appendix E.2.2 shows the evaluation of our
algorithm using varying λ’s."
REFERENCES,0.5174927113702624,"The procedure of learning the optimal adversary is depicted in Algorithm 3, where we simply use
the Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2015) to approximately solve the actor’s
objective, although more advanced solvers such as Projected Gradient Decent (PGD) can be applied
to further improve the performance. Experiment results in Section 6 verify that the above FGSM-
based implementation achieves state-of-the-art attack performance."
REFERENCES,0.5189504373177842,"What is the Inﬂuence of the Relaxation in (J)? First, it is important that the relaxation is only
needed for a stochastic victim. For a deterministic victim, which is often the case in practice, the
actor solves the original unrelaxed objective.
Second, as we will discuss in the next paragraph, the optimality of both SA-RL and PA-AD is re-
garding the formulation. That is, SA-RL and PA-AD formulate the optimal attack problem as an
MDP whose optimal policy is the optimal adversary. However, in a large-scale task, deep RL algo-
rithms themselves usually do not converge to the globally optimal policy and exploration becomes
the main challenge. Thus, when the adversary’s MDP is large, the suboptimality caused by the RL
solver due to exploration difﬁculties could be much more severe than the suboptimality caused by
the relaxation of the formulation. The comparison between SA-RL and PA-AD in our experiments"
REFERENCES,0.5204081632653061,Published as a conference paper at ICLR 2022
REFERENCES,0.521865889212828,Algorithm 3: Policy Adversarial Actor Director (PA-AD) with FGSM
REFERENCES,0.5233236151603499,1 Input: Initialization of director’s policy ν; victim policy π; budget ϵ; start state s0
REFERENCES,0.5247813411078717,"2 for t = 0, 1, 2, ... do"
REFERENCES,0.5262390670553936,"3
Director samples a policy perturbing direction bat ∼ν(·|st)"
IF VICTIM IS DETERMINISTIC THEN,0.5276967930029155,"4
if Victim is deterministic then"
IF VICTIM IS DETERMINISTIC THEN,0.5291545189504373,"5
# for a deterministic victim, JD is deﬁned in Equation (JD)"
IF VICTIM IS DETERMINISTIC THEN,0.5306122448979592,"6
Actor computes the gradient of its objective ∇δJD(st + δ; bat, st)"
ELSE,0.532069970845481,"7
else"
ELSE,0.5335276967930029,"8
# for a stochastic victim, J is deﬁned in Equation (J)"
ELSE,0.5349854227405247,"9
Actor computes the gradient of its objective ∇δJ(st + δ; bat, st)"
ELSE,0.5364431486880467,"10
Actor sets ˜st = st + ϵ · sign(δ)"
ELSE,0.5379008746355685,"11
Victim takes action at ∼π(·|˜st), proceeds to st+1, receives rt"
ELSE,0.5393586005830904,"12
Director saves (st, bat, −rt, st+1) to its buffer"
ELSE,0.5408163265306123,"13
Director updates its policy ν using any RL algorithm"
ELSE,0.5422740524781341,"can justify that the size of the adversary MDP has a larger impact than the relaxation of the problem
on the ﬁnal solution found by the attackers.
Third, in Appendix F.1, we empirically show that with the relaxed objective, PA-AD can still ﬁnd
the optimal attacker in 3 example environments."
ELSE,0.543731778425656,"Optimality in Formulation v.s. Approximated Optimality in Practice
PA-AD has an optimal
formulation, as the optimal solution to its objective (the optimal policy in PAMDP) is always an op-
timal adversary (Theorem 7). Similarly, the previous attack method SA-RL has an optimal solution
since the optimal policy in the adversary’s MDP is also an optimal adversary. However, in practice
where the environments are in a large scale and the number of samples is ﬁnite, the optimal policy
is not guaranteed to be found by either PA-AD and SA-RL with deep RL algorithms. Therefore, for
practical consideration, our goal is to search for a good solution or approximate the optimal solution
using optimization techniques (e.g. actor-critic learning, one-step FGSM attack, Lagrangian relax-
ation for the stochastic-victim attack). In experiments (Section 6), we show that our implementation
universally ﬁnds stronger attackers than prior methods, which veriﬁes the effectiveness of both our
theoretical framework and our practical implementation."
ELSE,0.5451895043731778,"C.3
VARIANTS FOR ENVIRONMENTS WITH CONTINUOUS ACTION SPACES"
ELSE,0.5466472303206997,"Although the analysis in the main paper focuses on an MDP whose action space is discrete, our
algorithm also extends to a continuous action space as justiﬁed in our experiments."
ELSE,0.5481049562682215,"C.3.1
FOR A DETERMINISTIC VICTIM"
ELSE,0.5495626822157434,"In this case, we can still use the formulation D-PAMDP, but a slightly different actor function
gD(ba, s) = argmin˜s∈Bϵ(s)∥πD(˜s) −ba∥.
(GCD)"
ELSE,0.5510204081632653,"C.3.2
FOR A STOCHASTIC VICTIM"
ELSE,0.5524781341107872,"Different from a stochastic victim in a discrete action space whose actions are sampled from a cate-
gorical distribution, a stochastic victim in a continuous action space usually follows a parametrized
probability distribution with a certain family of distributions, usually Gaussian distributions. In
this case, the formulation of PAMDP in Deﬁnition 6 is impractical. However, since the mean of
a Gaussian distribution has the largest probability to be selected, one can still use the formulation
in (GCD), while replacing πD(˜s) with the mean of the output distribution. Then, the director and
the actor can collaboratively let the victim output a Gaussian distribution whose mean is the target
action. If higher accuracy is needed, we can use another variant of PAMDP, named Continuous
Policy Adversary MDP (C-PAMDP) that can also control the variance of the Gaussian distribution."
ELSE,0.5539358600583091,"Deﬁnition 18 (Continuous Policy Adversary MDP (C-PAMDP)). Given an MDP M
=
⟨S, A, P, R, γ⟩where A is continuous, a ﬁxed and stochastic victim policy π, we deﬁne a Con-
tinuous Policy Adversarial MDP c
MC = ⟨S, b
AC, bPC, bRC, γ⟩, where the action space is b
AD = A,"
ELSE,0.5553935860058309,Published as a conference paper at ICLR 2022
ELSE,0.5568513119533528,"and ∀s, s′ ∈S, ∀ba ∈A,"
ELSE,0.5583090379008746,"bP(s′|s, ba) =
Z"
ELSE,0.5597667638483965,"A
π(a|g(ba, s))P(s′|s, a) da,
bR(s, ba) = −
Z"
ELSE,0.5612244897959183,"A
π(a|g(ba, s))R(s, a)da."
ELSE,0.5626822157434402,"The actor function g is deﬁned as
g(ba, s) = argmin˜s∈Bϵ(s)KL(π(·|˜s)||N(ba, σ2I|A|)).
(GC)
where σ is a hyper-parameter, and N denotes a multivariate Gaussian distribution."
ELSE,0.5641399416909622,"In short, Equation (GC) encourages the victim to output a distribution that is similar to the target
distribution. The hyperparameter σ controls the standard deviation of the target distribution. One
can set σ to be small in order to let the victim execute the target action ba with higher probabilities."
ELSE,0.565597667638484,"D
CHARACTERIZE OPTIMALITY OF EVASION ATTACKS"
ELSE,0.5670553935860059,"In this section, we provide a detailed characterization for the optimality of evasion attacks from the
perspective of policy perturbation, following Deﬁnition 5 in Section 4. Section D.1 establishes the
existence of the optimal policy adversary which is deﬁned in Section 3. Section D.2 then provides
a proof for Theorem 7 that the formulation of PA-AD is optimal. We also analyze the optimality of
heuristic attacks in Section D.3."
ELSE,0.5685131195335277,"D.1
EXISTENCE OF AN OPTIMAL POLICY ADVERSARY"
ELSE,0.5699708454810496,"Theorem 19 (Existence of An Optimal Policy Adversary). Given an MDP M = ⟨S, A, P, R, γ⟩,
and a ﬁxed stationary policy π on M, let Hϵ be a non-empty set of admissible state adversaries and
BH
ϵ (π) be the corresponding Adv-policy-set, then there exists an optimal policy adversary πh∗∈
BH
ϵ (π) such that πh∗∈argminπh∈BH
ϵ (π)V πh
M (s), ∀s ∈S."
ELSE,0.5714285714285714,"Proof. We prove Theorem 19 by constructing a new MDP corresponding to the original MDP M
and the victim π."
ELSE,0.5728862973760933,"Deﬁnition 20 (Policy Perturbation MDP). For a given MDP M, a ﬁxed stochastic victim pol-
icy π, and an admissible state adversary set Hϵ, deﬁne a policy perturbation MDP as MP =
⟨S, AP , PP , RP , γ⟩, where AP = ∆(A), and ∀s ∈S, aP ∈AP ,"
ELSE,0.5743440233236151,"RP (s, aP ) := { −P"
ELSE,0.575801749271137,"a∈A aP (a|s)R(s, a)
if ∃h ∈Hϵ s.t. aP (·|s) = π(·|h(s))
−∞
otherwise
(4)"
ELSE,0.577259475218659,"PP (s′|s, aP ) :=
X"
ELSE,0.5787172011661808,"a∈A
aP (a|s)P(s′|s, a)
(5)"
ELSE,0.5801749271137027,Then we can prove Theorem 19 by proving the following lemma.
ELSE,0.5816326530612245,Lemma 21. The optimal policy in MP is an optimal policy adversary for π in M.
ELSE,0.5830903790087464,"Let NP denote the set of deterministic policies in MP . According to the traditional MDP the-
ory (Puterman, 1994), there exists a deterministic policy that is optimal in MP . Note that Hϵ is
non-empty, so there exists at least one policy in MP with value ≥−∞, and then the optimal policy
should have value ≥−∞. Denote this optimal and deterministic policy as ν∗
P ∈NP . Then we write
the Bellman equation of ν∗
P , i.e.,"
ELSE,0.5845481049562682,"V ν∗
P
P (s) = max
νP ∈NP RP (s, νP (s)) + γ
X"
ELSE,0.5860058309037901,"s′∈S
PP (s′|s, νP (s))V νP
P (s′)"
ELSE,0.5874635568513119,"= max
νP ∈NP "" −
X"
ELSE,0.5889212827988338,"a∈A
νP (a|s)R(s, a) + γ
X s′∈S X"
ELSE,0.5903790087463557,"a∈A
νP (a|s)P(s′|s, a)V νP
P (s′) #"
ELSE,0.5918367346938775,"= max
νP ∈NP X"
ELSE,0.5932944606413995,"a∈A
νP (a|s) """
ELSE,0.5947521865889213,"−R(s, a) + γ
X"
ELSE,0.5962099125364432,"s′∈S
P(s′|s, a)V νP
P (s′) #
(6)"
ELSE,0.597667638483965,Published as a conference paper at ICLR 2022
ELSE,0.5991253644314869,"Note that ν∗
P (s) is a distribution on action space, ν∗
P (a|s) is the probability of a given by distribution
ν∗(s)."
ELSE,0.6005830903790087,"Multiply both sides of Equation (6) by −1, and we obtain"
ELSE,0.6020408163265306,"−V ν∗
P
P (s) = min
ν∈NP X"
ELSE,0.6034985422740525,"a∈A
νP (s)(a|s) """
ELSE,0.6049562682215743,"R(s, a) + γ
X"
ELSE,0.6064139941690962,"s′∈S
P(s′|s, a)
 
−V νP
P (s′)

# (7)"
ELSE,0.607871720116618,"In the original MDP M, an optimal policy adversary (if exists) πh∗for π should satisfy"
ELSE,0.60932944606414,"V πh∗(s) =
min
πh∈BH
ϵ (π) X"
ELSE,0.6107871720116618,"a∈A
πh(a|s) """
ELSE,0.6122448979591837,"R(s, a) + γ
X"
ELSE,0.6137026239067055,"s′∈S
P(s′|s, a)V πh(s′) # (8)"
ELSE,0.6151603498542274,"By comparing Equation (7) and Equation (8) we get the conclusion that ν∗
P is an optimal policy
adversary for π in M."
ELSE,0.6166180758017493,"D.2
PROOF OF THEOREM 7: OPTIMALITY OF OUR PA-AD"
ELSE,0.6180758017492711,"In this section, we provide theoretical proof of the optimality of our proposed evasion RL algorithm
PA-AD."
ELSE,0.619533527696793,"D.2.1
OPTIMALITY OF PA-AD FOR A STOCHASTIC VICTIM"
ELSE,0.6209912536443148,"We ﬁrst build a connection between the PAMDP c
M deﬁned in Deﬁnition 6 (Section 4) and the
policy perturbation MDP deﬁned in Deﬁnition 20 (Appendix D.1)."
ELSE,0.6224489795918368,"A deterministic policy ν in the PAMDP c
M can induce a policy ˆνP in MP in the following way:
bνP (s) = π(·|g(ν(s), s)), ∀s ∈S. More importantly, the values of ν and bνP in c
M and MP are
equal because of the formulations of the two MDPs, i.e., bV ν = V bνP
P , where bV and VP denote the
value functions in c
M and VP respectively."
ELSE,0.6239067055393586,"Proposition 22 below builds the connection of the optimality between the policies in these two
MDPs.
Proposition 22. An optimal policy in c
M induces an optimal policy in MP ."
ELSE,0.6253644314868805,"Proof of Proposition 22. Let ν∗be an deterministic optimal policy in c
M, and it induces a policy in
MP , namely bνP ."
ELSE,0.6268221574344023,"Let us assume bνP is not an optimal policy in MP , hence there exists a policy ν∗
P in MP s.t.
V ν∗
P
P (s) > V bνP
P (s) for at least one s ∈S. And based on Theorem 4, we are able to ﬁnd such a ν∗
P
whose corresponding policy perturbation is on the outermost boundary of B(π), i.e., ν∗∈∂πBH
ϵ (π)."
ELSE,0.6282798833819242,"Then we can construct a policy ν′ in c
M such that ν′(s) = ν∗
P (s) −π(s), ∀s ∈S. And based on
Equation (G), π(·|g(ν′(s), s)) is in ∂πB(π(s)) for all s ∈S. According to the deﬁnition of ∂π, if
two policy perturbations perturb π in the same direction and are both on the outermost boundary,
then they are equal. Thus, we can conclude that π(g(ν′(s), s)) = ν∗
P (s), ∀s ∈S. Then we obtain
bV ν′(s) = V ν∗
P
P (s), ∀s ∈S."
ELSE,0.6297376093294461,"Now we have conditions:
(1) bV ν∗(s) = V bνP
P (s), ∀s ∈S;
(2) V ν∗
P
P (s) > V bνP
P (s) for at least one s ∈S;
(3) ∃ν′ such that bV ν′(s) = V ν∗
P
P (s), ∀s ∈S."
ELSE,0.6311953352769679,"From (1), (2) and (3), we can conclude that bV ν′(s) > bV ν∗(s) for at least one s ∈S, which conﬂicts
with the assumption that ν∗is optimal in c
M. Therefore, Proposition 22 is proven."
ELSE,0.6326530612244898,Published as a conference paper at ICLR 2022
ELSE,0.6341107871720116,"Proposition 22 and Lemma 21 together justiﬁes that the optimal policy of c
M, namely ν∗, induces
an optimal policy adversary for π in the original M. Then, if the director learns the optimal policy
in c
M, then it collaborates with the actor and generates the optimal state adversary h∗by h∗(s) =
g(ν∗(s), s), ∀s ∈S."
ELSE,0.6355685131195336,"D.2.2
OPTIMALITY OF OUR PA-AD FOR A DETERMINISTIC VICTIM"
ELSE,0.6370262390670554,"In this section, we show that the optimal policy in D-PAMDP (the deterministic variant of PAMDP
deﬁned in Appendix C.1) also induces an optimal policy adversary in the original environment."
ELSE,0.6384839650145773,"Let πD be a deterministic policy reduced from a stochastic policy π, i.e.,
πD(s) := argmaxa∈Aπ(a|s), ∀s ∈S.
Note that in this case, the Adv-policy-set BH
ϵ (π) is not connected as it contains only deterministic
policies. Therefore, we re-formulate the policy perturbation MDP introduced in Appendix D.1 with
a deterministic victim as below:"
ELSE,0.6399416909620991,"Deﬁnition 23 (Deterministic Policy Perturbation MDP). For a given MDP M, a ﬁxed determin-
istic victim policy π, and an admissible adversary set Hϵ, deﬁne a deterministic policy perturbation
MDP as MDP = ⟨S, ADP , PDP , RDP , γ⟩, where ADP = A, and ∀s ∈S, aDP ∈ADP ,"
ELSE,0.641399416909621,"RDP (s, aDP ) := { −R(s, aDP )
if ∃h ∈Hϵ s.t. aDP (s) = πD(h(s))
−∞
otherwise
(9)"
ELSE,0.6428571428571429,"PDP (s′|s, aDP ) := P(s, aDP )
(10)"
ELSE,0.6443148688046647,"MDP can be viewed as a special case of MP where only deterministic policies have ≥−∞values.
Therefore Theorem 19 and Lemma 21 also hold for deterministic victims."
ELSE,0.6457725947521866,"Next we will show that an optimal policy in c
MD induces an optimal policy in MDP ."
ELSE,0.6472303206997084,"Proposition 24. An optimal policy in c
MD induces an optimal policy in MDP ."
ELSE,0.6486880466472303,"Proof of Proposition 24. We will prove Proposition 24 by contradiction. Let ν∗be an optimal policy
in c
MD, and it induces a policy in MDP , namely bνDP ."
ELSE,0.6501457725947521,"Let us assume bνDP is not an optimal policy in MDP , hence there exists a deterministic policy ν∗
DP
in MDP s.t. V ν∗
DP
DP (s) > V bνDP
DP (s) for at least one s ∈S. Without loss of generality, suppose
V ν∗
DP
DP (s0) > V bνDP
DP (s0)."
ELSE,0.6516034985422741,"Next we construct another policy ν′ in c
MD by setting ν′(s) = ν∗
DP (s), ∀s ∈S. Given that ν∗
DP
is deterministic, ν′ is also a deterministic policy. So we use ν∗
DP (s) and ν′(s) to denote the action
selected by ν∗
DP and ν′ respectively at state s."
ELSE,0.6530612244897959,"For an arbitrary state si, let ai := ν∗
DP (si). Since ν∗
DP is the optimal policy in MDP , we get
that there exists a state adversary h ∈Hϵ such that πD(h(si)) = ai, or equivalently, there exists a
state ˜si ∈Bϵ(si) such that argmaxa∈Aπ(˜si) = ai. Then, the solution to the actor’s optimization
problem (GD) given direction ai and state si, denoted as ˜s∗, satisﬁes
˜s∗= argmaxs′∈Bϵ(s)
 
π(ba|s′) −argmaxa∈A,a̸=baπ(a|s′)

(11)
and we can get
π(ba|˜s∗) −argmaxa∈A,a̸=baπ(a|˜s∗) ≥π(ba|˜si) −argmaxa∈A,a̸=baπ(a|˜si) > 0
(12)
Given that argmaxa∈Aπ(ai|˜si)
=
ai, we obtain argmaxa∈Aπ(ai|˜s∗)
=
ai, and hence
πD(gD(ai, si)) = ai. Since this relation holds for an arbitrary state s, we can get
πD(gD(ν′(s), s)) = πD(gD(ν′(s), s)) = ν′(s), ∀s ∈S
(13)"
ELSE,0.6545189504373178,Published as a conference paper at ICLR 2022
ELSE,0.6559766763848397,"Also, we have ∀s ∈S
bV ν′
D (s) = bRD(s, ν′(s)) +
X"
ELSE,0.6574344023323615,"s′∈S
bPD(s′|s, ν′(s))bV ν′
D (s′)
(14)"
ELSE,0.6588921282798834,"V ν∗
DP
DP (s) = RDP (s, ν∗
DP (s)) +
X"
ELSE,0.6603498542274052,"s′∈S
PDP (s′|s, ν∗
DP (s))V ν∗
DP
DP ((s′)
(15)"
ELSE,0.6618075801749271,"Therefore, bV ν′
D (s) = V ν∗
DP
DP (s), ∀s ∈S."
ELSE,0.6632653061224489,"Then we have
bV ν′
D (s0) ≤bV ν∗(s0) = V bνDP
DP (s0) < V ν∗
DP
DP (s0) = bV ν′
D (s0)
(16)"
ELSE,0.6647230320699709,"which gives bV ν′
D (s0) < bV ν′
D (s0), so there is a contradiction."
ELSE,0.6661807580174927,"Combining the results of Proposition 24 and Lemma 21 , for a deterministic victim, the optimal
policy in D-PAMDP gives an optimal adversary for the victim."
ELSE,0.6676384839650146,"D.3
OPTIMALITY OF HEURISTIC-BASED ATTACKS"
ELSE,0.6690962099125365,"There are many existing methods of ﬁnding adversarial state perturbations for a ﬁxed RL policy,
most of which are solving some optimization problems deﬁned by heuristics. Although these meth-
ods are empirically shown to be effective in many environments, it is not clear how strong these
adversaries are in general. In this section, we carefully summarize and categorize existing heuristic
attack methods into 4 types, and then characterize their optimality in theory."
ELSE,0.6705539358600583,"D.3.1
TYPE I - MINIMIZE THE BEST (MINBEST)"
ELSE,0.6720116618075802,"A common idea of evasion attacks in supervised learning is to reduce the probability that the learner
selects the “correct answer” Goodfellow et al. (2015). Prior works Huang et al. (2017); Kos & Song
(2017); Korkmaz (2020) apply a similar idea to craft adversarial attacks in RL, where the objective
is to minimize the probability of selecting the “best” action, i.e.,
hMinBest ∈argminh∈Hϵπh(a+|s), ∀s ∈S
(I)
where a+ is the “best” action to select at state s. Huang et al.Huang et al. (2017) deﬁne a+ as
argmaxa∈AQπ(s, a) for DQN, or argmaxa∈Aπ(a|s) for TRPO and A3C with a stochastic π. Since
the agent’s policy π is usually well-trained in the original MDP, a+ can be viewed as (approximately)
the action taken by an optimal deterministic policy π∗(s)."
ELSE,0.673469387755102,"Lemma 25 (Optimality of MinBest). Denote the set of optimal solutions to objective (I) as
HMinBest. There exist an MDP M and an agent policy π, such that HMinBest does not contain an
optimal adversary h∗, i.e., HMinBest ∩H∗
ϵ = ∅."
ELSE,0.6749271137026239,"Proof of Lemma 25. We prove this lemma by constructing the following MDP such that for any
victim policy, there exists a reward conﬁguration in which MinBest attacker is not optimal."
ELSE,0.6763848396501457,Figure 7: A simple MDP where MinBest Attacker cannot ﬁnd the optimal adversary for a given victim policy.
ELSE,0.6778425655976676,"Here, let r1 = r(s4|s2, a1), r2 = r(s5|s2, a2), r3 = r(s3|s1, a2). Assuming all the other rewards
are zero, transition dynamics are deterministic, and states s3, s4, s5 are the terminal states. For the"
ELSE,0.6793002915451894,Published as a conference paper at ICLR 2022
ELSE,0.6807580174927114,"sake of simplicity, we also assume that the discount factor here γ = 1.
Now given a policy π such that π(a1|s1) = β1 and π(a1|s2) = β2 (β1, β2 ∈[0, 1]), we could ﬁnd
r1, r2, r3 such that the following constraints hold:
r1 > r2 ⇐⇒Qπ(s1, a1) > Qπ(s1, a2)
(17)
β2r1 + (1 −β2)r2 > r3 ⇐⇒Qπ(s2, a1) > Qπ(s2, a2)
(18)
r3 > (β2 −ϵ2)r2 + (1 −β2 + ϵ2)r2 ⇐⇒r3 > Qπ(s1, a1) −ϵ2(r1 −r2)
(19)
Now we consider the Adv-policy-set"
ELSE,0.6822157434402333,"BH
ϵ (π) =
n
π′ ∈Π
 ∥π′(·|s1) −π(·|s1)∥< ϵ1, ∥π′(·|s2) −π(·|s2)∥< ϵ2
o
."
ELSE,0.6836734693877551,"Under these three linear constraints, the policy given by MinBest attacker satisﬁes that
πhMinBest(a1|s1) = β1 −ϵ1, and πhMinBest(a1|s2) = β2 −ϵ2. On the other hand, we can ﬁnd an-
other admissible policy adversary πh∗(a1|s1) = β1 + ϵ1, and πh∗(a1|s2) = β2 −ϵ2. Now we show
that V πh∗(s1) < V πhMinBest (s1), and thus MinBest attacker is not optimal."
ELSE,0.685131195335277,"V πhMinBest(s1) = (β1 −ϵ1)
h
(β2 −ϵ2)r1 + (1 −β2 + ϵ2)r2
i
+ (1 −β1 + ϵ1)r3
(20)"
ELSE,0.6865889212827988,"= (β1 −ϵ1)(β2 −ϵ2)r1 + (β1 −ϵ1)(1 −β2 + ϵ2)r2 + (1 −β1 + ϵ1)r3
(21)"
ELSE,0.6880466472303207,"V πh∗(s1) = (β1 + ϵ1)
h
(β2 −ϵ2)r1 + (1 −β2 + ϵ2)r2
i
+ (1 −β1 −ϵ1)r3
(22)"
ELSE,0.6895043731778425,"= (β1 + ϵ1)(β2 −ϵ2)r1 + (β1 + ϵ1)(1 −β2 + ϵ2)r2 + (1 −β1 −ϵ1)r3
(23)
Therefore,
V πh∗(s1) −V πhMinBest (s1) = 2ϵ1(β2 −ϵ2)r2 + 2ϵ1(1 −β2 + ϵ2)r2 −2ϵ1r3
(24)"
ELSE,0.6909620991253644,"= 2ϵ1
h
(β2 −ϵ2)r2 + (1 −β2 + ϵ2)r2 −r3
i
(25)"
ELSE,0.6924198250728864,"< 0 Because of the constraint (19)
(26)"
ELSE,0.6938775510204082,"D.3.2
TYPE II - MAXIMIZE THE WORST (MAXWORST)"
ELSE,0.6953352769679301,"Pattanaik et al. (Pattanaik et al., 2018) point out that only preventing the agent from selecting the
best action does not necessarily result in a low total reward. Instead, Pattanaik et al. (Pattanaik et al.,
2018) propose another objective function which maximizes the probability of selecting the worst
action, i.e.,
hMaxWorst ∈argmaxh∈Hϵπh(a−|s), ∀s ∈S
(II)"
ELSE,0.6967930029154519,"where a−refers to the “worst” action at state s. Pattanaik et al.(Pattanaik et al., 2018) deﬁne the
“worst” action as the actions with the lowest Q value, which could be ambiguous, since the Q
function is policy-dependent. If a worst policy π−∈argminπV π(s), ∀s ∈S is available, one
can use a−= argminQπ−(s, a). However, in practice, the attacker usually only has access to the
agent’s current policy π, so it can also choose a−= argminQπ(s, a). Note that these two selections
are different, as the agent’s policy π is usually far away from the worst policy."
ELSE,0.6982507288629738,"Lemma 26 (Optimality of MaxWorst). Denote the set of optimal solutions to objective (II) as
HMaxWorst, which include both versions of MaxWorst attacker formulations as we discussed above.
Then there exist an MDP M and an agent policy π, such that HMaxWorst contains a non-optimal
adversary h∗, i.e., HMaxWorst ̸⊂H∗
ϵ ."
ELSE,0.6997084548104956,"Proof of Lemma 26.
Case I: Using current policy to compute the target action
We prove this lemma by constructing the MDP in Figure 8 such that for any victim policy, there
exists a reward conﬁguration in which MaxWorst attacker is not optimal.
Here, let r1 = r(s11|s1, a1), r2 = r(s12|s1, a2), r3 = r(s21|s2, a1), r4 = r(s22|s2, a2). Assuming
all the other rewards are zero, transition dynamics are deterministic, and states s11, s12, s21, s22 are
the terminal states. For the sake of simplicity, we also assume that the discount factor here γ = 1.
Now given a policy π such that π(a1|s0) = β0, π(a1|s1) = β1, and π(a2|s2) = β2 (β0, β1, β2 ∈
[0, 1]), consider the Adv-policy-set"
ELSE,0.7011661807580175,"BH
ϵ (π) =
n
π′ ∈Π
 ∥π′(·|s1)−π(·|s1)∥< ϵ0, ∥π′(·|s1)−π(·|s1)∥< ϵ1, ∥π′(·|s2)−π(·|s2)∥< ϵ2,
o
."
ELSE,0.7026239067055393,Published as a conference paper at ICLR 2022
ELSE,0.7040816326530612,"Figure 8: A simple MDP where the ﬁrst version of MaxWorst Attacker cannot ﬁnd the optimal adversary for a
given victim policy."
ELSE,0.7055393586005831,"We could ﬁnd r1, r2, r3, r4 such that the following linear constraints hold:
β1r1 + (1 −β1)r2 >β2r3 + (1 −β2)r4 ⇐⇒Qπ(s0, a1) > Qπ(s0, a2)
(27)
r1 >r2 ⇐⇒Qπ(s1, a1) > Qπ(s1, a2)
(28)
r3 >r4 ⇐⇒Qπ(s2, a1) > Qπ(s2, a2)
(29)
(β1 −ϵ1)r1 + (1 −β1 + ϵ1)r2 <(β2 −ϵ2)r3 + (1 −β2 + ϵ2)r4
(30)
Now,
given these constraints,
the perturbed policy given by MaxWorst attaker satisﬁes
πhMaxWorst(a1|s0) = β0 −ϵ0, πhMaxWorst(a1|s1) = β1 −ϵ1, and πhMaxWorst(a1|s2) = β2 −ϵ2. How-
ever, consider another perturbed policy πh∗in Adv-policy-set such that πh∗(a1|s0) = β0 + ϵ0,
πh∗(a1|s1) = β1 −ϵ1, and πh∗(a1|s2) = β2 −ϵ2. We will prove that V πh∗(s1) < V πhMaxWorst(s1),
and thus MaxWorst attacker is not optimal.
On the one hand,"
ELSE,0.706997084548105,"V πhMaxWorst(s1) =(β0 −ϵ0)
h
(β1 −ϵ1)r1 + (1 −β1 + ϵ1)r2
i
+ (1 −β0 + ϵ0)
h
(β2 −ϵ2)r3 + (1 −β2 + ϵ2)r4
i (31)"
ELSE,0.7084548104956269,"=(β0 −ϵ0)(β1 −ϵ1)r1 + (β0 −ϵ0)(1 −β1 + ϵ1)r2
+ (1 −β0 + ϵ0)(β2 −ϵ2)r3 + (1 −β0 + ϵ0)(1 −β2 + ϵ2)r4
(32)
On the other hand,"
ELSE,0.7099125364431487,"V πh∗(s1) =(β0 + ϵ0)
h
(β1 −ϵ1)r1 + (1 −β1 + ϵ1)r2
i
+ (1 −β0 −ϵ0)
h
(β2 −ϵ2)r3 + (1 −β2 + ϵ2)r4
i (33)"
ELSE,0.7113702623906706,"=(β0 + ϵ0)(β1 −ϵ1)r1 + (β0 + ϵ0)(1 −β1 + ϵ1)r2
+ (1 −β0 −ϵ0)(β2 −ϵ2)r3 + (1 −β0 −ϵ0)(1 −β2 + ϵ2)r4
(34)
Therefore,
V πh∗(s1) −V πhMaxWorst (s1) =2ϵ0(β1 −ϵ1)r1 + 2ϵ0(1 −β1 + ϵ1)r2
−2ϵ0(β2 −ϵ2)r3 −2ϵ0(1 −β2 + ϵ2)r4
(35)
< 0 Because of the constraint (30)
(36)
Case II: Using worst policy to compute the target action"
ELSE,0.7128279883381924,"Figure 9: A simple MDP where the second version of MaxWorst Attacker cannot ﬁnd the optimal adversary
for a given victim policy."
ELSE,0.7142857142857143,Published as a conference paper at ICLR 2022
ELSE,0.7157434402332361,"Same as before, we construct a MDP where HMaxWorst contains a non-optimal adversary.
Let
r1 = r(s1|s0, a1), r2 = r(s2|s0, a2), r3 = r(s3|s0, a3). Assuming all the other rewards are zero,
transition dynamics are deterministic, and states s1, s2, s3 are the terminal states. For the sake of
simplicity, we also assume that the discount factor here γ = 1.
Let pi be the given policy such that π(a1|s0) = β1 and π(a2|s0) = β2. Now without loss of gener-
ality, we assume r1 > r2 > r3 (∗). Then the worst policy π′ satisﬁes that π′(a3|s0) = 1. Consider"
ELSE,0.717201166180758,"the Adv-policy-set BH
ϵ (π) =
n
π′ ∈Π
 ∥π′(·|s0) −π(·|s0)∥1 < ϵ
o
. Then HMaxWorst =
n
π′ ∈"
ELSE,0.7186588921282799,"Π
 π′(a3|s0) = (1 −β1 −β2) + ϵ
o
."
ELSE,0.7201166180758017,"Now consider two policies πh1, πh2 ∈HMaxWorst, where πh1(a1|s0) = β1, πh1(a2|s0) = β2 −ϵ,
πh2(a1|s0) = β1 −ϵ, πh2(a2|s0) = β2. Then V πh1(s0) −V πh2(s0) = ϵ(r1 −r2) > 0. Therefore,
πh1 ∈HMaxWorst but it’s not optimal."
ELSE,0.7215743440233237,"D.3.3
TYPE III - MINIMIZE Q VALUE (MINQ)."
ELSE,0.7230320699708455,"Another idea of attacking Pattanaik et al. (2018); Zhang et al. (2020a) is to craft perturbations such
that the agent selects actions with minimized Q values at every step, i.e.,"
ELSE,0.7244897959183674,"hMinQ ∈argminh∈Hϵ
X"
ELSE,0.7259475218658892,"a∈A πh(a|s) ˆQπ(s, a), ∀s ∈S
(III)"
ELSE,0.7274052478134111,"where ˆQ is the approximated Q function of the agent’s original policy. For example, Pattanaik et
al.Pattanaik et al. (2018) directly use the agent’s Q network (of policy π), while the Robust SARSA
(RS) attack proposed by Zhang et al.Zhang et al. (2020a) learns a more stable Q network for the
agent’s policy π. Note that in practice, this type of attack is usually applied to deterministic agents
(e.g., DQN, DDPG, etc), then the objective becomes argminh∈Hϵ ˆQπ(s, πh(s)), ∀s ∈S Pattanaik
et al. (2018); Zhang et al. (2020a); Oikarinen et al. (2020). In this case, the MinQ attack is equivalent
to the MaxWorst attack with the current policy as the target."
ELSE,0.7288629737609329,"Lemma 27 (Optimality of MinQ). Denote the set of optimal solutions to objective (III) as HMinQ,
which include both versions of MinQ attacker formulations as we discussed above. Then there exist
an MDP M and an agent policy π, such that HMinQ contains a non-optimal adversary h∗, i.e.,
HMinQ ̸⊂H∗
ϵ ."
ELSE,0.7303206997084548,"Proof of Lemma 26.
Case I: For a deterministic victim
In the deterministic case
hMinQ ∈argminh∈Hϵ ˆQπ(s, πh(s)) = argmaxh∈Hϵπh(argmina ˆQπ(s, a)|s), ∀s ∈S
(IIID)
In this case, the objective is equivalent to objective (II), thus Lemma 27 holds."
ELSE,0.7317784256559767,"Case II: For a stochastic victim
In this case, we consider the MDP in Figure 8 and condition (27) to (30). Then the MinQ objective
gives πhMinQ(a1|s0) = β0 −ϵ0, πhMinQ(a1|s1) = β1 −ϵ1, and πhMinQ(a1|s2) = β2 −ϵ2."
ELSE,0.7332361516034985,"According to the proof of the ﬁrst case of Lemma 26, πhMinQ = πhMaxWorst is not an optimal adversary.
Thus Lemma 27 holds."
ELSE,0.7346938775510204,"D.3.4
TYPE IV - MAXIMIZE DIFFERENCE (MAXDIFF)."
ELSE,0.7361516034985423,"The MAD attack proposed by Zhang et al. (Zhang et al., 2020a) is to maximize the distance between
the perturbed policy πh and the clean policy π, i.e.,
hMaxDiff ∈argmaxh∈HϵDTV[πh(·|s)||π(·|s)], ∀s ∈S
(IV)
where TV denotes the total variance distance between two distributions.
In practical imple-
mentations, the TV distance can be replaced by the KL-divergence, as DTV[πh(·|s)||π(·|s)] ≤
(DKL[πh(·|s)||π(·|s)])2. This type of attack is inspired by the fact that if two policies select ac-
tions with similar action distributions on all the states, then the value of the two policies is also
small (see Theorem 5 in Zhang et al. (2020a))."
ELSE,0.7376093294460642,Published as a conference paper at ICLR 2022
ELSE,0.739067055393586,"Lemma 28 (Optimality of MaxDiff). Denote the set of optimal solutions to objective (IV) as
HMaxDiff. There exist an MDP M and an agent policy π, such that HMaxDiff contains a non-optimal
adversary h∗, i.e., HMaxDiff ̸⊂H∗
ϵ ."
ELSE,0.7405247813411079,"Proof of Lemma 28. The proof follows from the proof of lemma 25. In the MDP we constructed,
π′ = β1 −ϵ1, πhMinBest(a1|s2) = β2 −ϵ2 is one of the policies that has the maximum KL divergence
from the victim policy within Adv-policy-set. However, as we proved in 25, this is not the optimally
perturbed policy. Therefore, MaxDiff attacker may not be optimal."
ELSE,0.7419825072886297,"E
ADDITIONAL EXPERIMENT DETAILS AND RESULTS"
ELSE,0.7434402332361516,"In this section, we provide details of our experimental settings and present additional experimental
results. Section E.1 describes our implementation details and hyperparameter settings for Atari and
MuJoCo experiments. Section E.2 provide additional experimental results, including experiments
with varying budgets (ϵ) in Section E.2.1, more comparison between SA-RL and PA-AD in terms
of convergence rate and sensitivity to hyperparameter settings as in Section E.2.3, robust training in
MuJoCo games with fewer training steps in Section E.2.4, attacking performance on robust models
in Atari games in Section E.2.5, as well as robust training results in Atari games in Section E.2.6."
ELSE,0.7448979591836735,"E.1
IMPLEMENTATION DETAILS"
ELSE,0.7463556851311953,"E.1.1
ATARI EXPERIMENTS"
ELSE,0.7478134110787172,"In this section we report the conﬁgurations and hyperparameters we use for DQN, A2C and ACKTR
in Atari environments. We use GeForce RTX 2080 Ti GPUs for all the experiments."
ELSE,0.749271137026239,"DQN Victim
We compare PA-AD algorithm with other attacking algorithms on 7 Atari games.
For DQN, we take the softmax of the Q values Q(s, ·) as the victim policy π(·|s) as in prior
works (Huang et al., 2017). For these environments, we use the wrappers provided by stable-
baselines (Hill et al., 2018), where we clip the environment rewards to be −1 and 1 during training
and stack the last 4 frames as the input observation to the DQN agent. For the victim agent, we im-
plement Double Q learning (Hado Van Hasselt, 2016) and prioritized experience replay (Tom Schaul
& Silver, 2016). The clean DQN agents are trained for 6 million frames, with a learning rate 0.00001
and the same network architecture and hyperparameters as the ones used in Mnih et al. (2015). In
addition, we use a replay buffer of size 5 × 105. Prioritized replay buffer sampling is used with
α = 0.6 and β increases from 0.4 to 1 linearly during training. During evaluation, we execute the
agent’s policy without epsilon greedy exploration for 1000 episodes."
ELSE,0.750728862973761,"A2C Victim
For the A2C victim agent, we also use the same preprocessing techniques and con-
volutional layers as the one used in Mnih et al. (2015). Besides, values and policy network share
the same CNN layers and a fully-connected layer with 512 hidden units. The output layer is a cat-
egorical distribution over the discrete action space. We use 0.0007 as the initial learning rate and
apply linear learning rate decay, and we train the victim A2C agent for 10 million frames. During
evaluation, the A2C victim executes a stochastic policy (for every state, the action is sampled from
the categorical distribution generated by the policy network). Our implementation of A2C is mostly
based on an open-source implementation by Kostrikov Kostrikov (2018)."
ELSE,0.7521865889212828,"ACKTR Adversary
To train the director of PA-AD and the adversary in SA-RL, we use
ACKTR (Wu et al., 2017) with the same network architecture as A2C. We train the adversaries
of PA-AD and SA-RL for the same number of steps for a fair comparison. For the DQN victim, we
use a learning rate 0.0001 and train the adversaries for 5 million frames. For the A2C victim, we
use a learning rate 0.0007 and train the adversaries for 10 million frames. Our implementation of
ACKTR is mostly based on an open-source implementation by Kostrikov Kostrikov (2018)."
ELSE,0.7536443148688047,"Heuristic Attackers
For the MinBest attacker, we following the algorithm proposed by Huang
et al. (2017) which uses FGSM to compute adversarial state perturbations. The MinBest + Mo-"
ELSE,0.7551020408163265,Published as a conference paper at ICLR 2022
ELSE,0.7565597667638484,"mentum attacker is implemented according to the algorithm proposed by Korkmaz (2020), and we
set the number of iterations to be 10, the decaying factor µ to be 0.5 (we tested 0.01, 0.1, 0.5, 0.9
and found 0.5 is relatively better while the difference is minor). Our implementation of the MinQ
attacker follows the gradient-based attack by Pattanaik et al. (2018), and we also set the number of
iterations to be 10. For the MaxDiff attacker, we refer to Algorithm 3 in Zhang et al. (2020a) with
the number of iterations equal to 10. In addition, we implement a random attacker which perturbs
state s to ˜s = s + ϵsign(µ), where µ is sampled from a standard multivariate Gaussian distribution
with the same dimension as s."
ELSE,0.7580174927113703,"E.1.2
MUJOCO EXPERIMENTS"
ELSE,0.7594752186588921,"For four OpenAI Gym MuJoCo continuous control environments, we use PPO with the original
fully connected (MLP) structure as the policy network to train the victim policy. For robustness
evaluations, the victim and adversary are both trained using PPO with independent value and policy
optimizers. We complete all the experiments on MuJoCo using 32GB Tesla V100."
ELSE,0.760932944606414,"PPO Victim
We directly use the well-trained victim model provided by Zhang et al. (2020a)."
ELSE,0.7623906705539358,"PPO Adversary
Our PA-AD adversary is trained by PPO and we use a grid search of a part
of adversary hyperparameters (including learning rates of the adversary policy network and policy
network, the entropy regularization parameter and the ratio clip ϵ for PPO) to train the adversary as
powerful as possible. The reported optimal attack result is from the strongest adversary among all
50 trained adversaries."
ELSE,0.7638483965014577,"Other Attackers
For Robust Sarsa (RS) attack, we use the implementation and the optimal RS
hyperparameters from Zhang et al. (2020a) to train the robust value function to attack the victim.
The reported RS attack performance is the best one over the 30 trained robust value functions."
ELSE,0.7653061224489796,"For MaxDiff attack, the maximal action difference attacker is implemented referring to Zhang et al.
(2020a)."
ELSE,0.7667638483965015,"For SA-RL attacker, following Zhang et al. (2021), the hyperparameters is the same as the optimal
hyperparameters of vanilla PPO from a grid search. And the training steps are set for different
environments. For the strength of SA-PPO regularization κ, we choose from 1 × 10−6 to 1 and
report the worst-case reward."
ELSE,0.7682215743440233,"Robust Training
For ATLA Zhang et al. (2021), the hyperparameters for both victim policy and
adversary remain the same as those in vanilla PPO training. To ensure sufﬁcient exploration, we run
a small-scale grid search for the entropy bonus coefﬁcient for agent and adversary. The experiment
results show that a larger entropy bonus coefﬁcient allows the agent to learn a better policy for the
continual-improving adversary. In robust training experiments, we use larger training steps in all
the MuJoCo environments to guarantee policy convergence. We train 5 million steps in Hopper,
Walker, and HalfCheetah environments and 10 million steps for Ant. For reproducibility, the ﬁnal
results we reported are the experimental performance of the agent with medium robustness from 21
agents training with the same hyperparameter set."
ELSE,0.7696793002915452,"E.2
ADDITIONAL EXPERIMENT RESULTS"
ELSE,0.7711370262390671,"E.2.1
ATTACKING PERFORMANCE WITH VARIOUS BUDGETS"
ELSE,0.7725947521865889,"In Table 1, we report the performance of our PA-AD attacker under a chosen epsilon across different
environments. To see how PA-AD algorithm performs across different values of ϵ’s, here we select
three Atari environments each for DQN and A2C victim agents and plot the performance of PA-AD
under various ϵ’s compared with the baseline attackers in Figure 10. We can see from the ﬁgures
that our PA-AD universally outperforms baseline attackers concerning various ϵ’s."
ELSE,0.7740524781341108,"In Table 2, we provide the evaluation results of PA-AD under a commonly unused epsilon in four
MuJoCo experiments (Zhang et al. (2020a; 2021)) to show that PA-AD attacker also has the best
attacking performance compared with other attackers under different ϵ’s in Figure 11."
ELSE,0.7755102040816326,Published as a conference paper at ICLR 2022
ELSE,0.7769679300291545,"0
0.2
0.4
0.6
0.8
1
1.2 ·10−3 20 40 60 80 100 ϵ"
ELSE,0.7784256559766763,Average Return
ELSE,0.7798833819241983,"MinBest
MinBest Momentum
MinQ
MaxDiff
PA-AD"
ELSE,0.7813411078717201,(a) DQN Boxing
ELSE,0.782798833819242,"0
0.2
0.4
0.6
0.8
1 ·10−3 −20 −10 0 10 20 ϵ"
ELSE,0.7842565597667639,Average Return
ELSE,0.7857142857142857,"MinBest
MinBest Momentum
MinQ
MaxDiff
PA-AD"
ELSE,0.7871720116618076,(b) DQN Pong
ELSE,0.7886297376093294,"0
0.2
0.4
0.6
0.8
1 ·10−3 0 1 2 3 4 5 ·104 ϵ"
ELSE,0.7900874635568513,Average Return
ELSE,0.7915451895043731,"MinBest
MinBest Momentum
MinQ
MaxDiff
PA-AD"
ELSE,0.793002915451895,(c) DQN RoadRunner
ELSE,0.7944606413994169,"0
0.2
0.4
0.6
0.8
1 ·10−3 −20 −10 0 10 20 ϵ"
ELSE,0.7959183673469388,Average Return
ELSE,0.7973760932944607,"MinBest
MinBest Momentum
MaxDiff
PA-AD"
ELSE,0.7988338192419825,(d) A2C Pong
ELSE,0.8002915451895044,"0
0.2
0.4
0.6
0.8
1 ·10−3 0 100 200 300 400 ϵ"
ELSE,0.8017492711370262,Average Return
ELSE,0.8032069970845481,"MinBest
MinBest Momentum
MaxDiff
PA-AD"
ELSE,0.8046647230320699,(e) A2C Breakout
ELSE,0.8061224489795918,"0
0.2
0.4
0.6
0.8
1 ·10−2 0 500 1,000 1,500 ϵ"
ELSE,0.8075801749271136,Average Return
ELSE,0.8090379008746356,"MinBest
MinBest Momentum
MaxDiff
PA-AD"
ELSE,0.8104956268221575,(f) A2C Seaquest
ELSE,0.8119533527696793,"Figure 10: Comparison of different attack methods against DQN and A2C victims in Atari w.r.t. different
budget ϵ’s."
ELSE,0.8134110787172012,"0.02 0.04 0.06 0.08 0.10 0.12 0.14 0 1,000 2,000 3,000 ϵ"
ELSE,0.814868804664723,Average Return
ELSE,0.8163265306122449,"MaxDiff
Robust Sarsa
SA-RL
PA-AD"
ELSE,0.8177842565597667,(a) PPO Hopper
ELSE,0.8192419825072886,"0.02 0.04 0.06 0.08 0.10 0.12 0.14 1,000 2,000 3,000 4,000 5,000 ϵ"
ELSE,0.8206997084548106,Average Return
ELSE,0.8221574344023324,"MaxDiff
Robust Sarsa
SA-RL
PA-AD"
ELSE,0.8236151603498543,(b) PPO Walker2d
ELSE,0.8250728862973761,"0.0500.0750.1000.1250.1500.1750.2000.2250.250
−4,000"
ELSE,0.826530612244898,"−2,000 0 2,000 4,000 6,000 ϵ"
ELSE,0.8279883381924198,Average Return
ELSE,0.8294460641399417,"MaxDiff
Robust Sarsa
SA-RL
PA-AD"
ELSE,0.8309037900874635,(c) PPO Ant
ELSE,0.8323615160349854,Figure 11: Comparison of different attack methods against PPO victims in MuJoCo w.r.t. different budget ϵ’s.
ELSE,0.8338192419825073,Published as a conference paper at ICLR 2022
ELSE,0.8352769679300291,"E.2.2
HYPERPARAMETER TEST"
ELSE,0.8367346938775511,"In our Actor-Director Framework, solving an optimal actor is a constraint optimization problem.
Thus, in our algorithm, we instead use Lagrangian relaxation for the actor’s constraint optimization.
In this section, we report the effects of different choices of the relaxation hyperparameter λ on
the ﬁnal performance of our algorithm. Although we set λ by default to be 1 and keep it ﬁxed
throughout all of the other experiments, here we ﬁnd that in fact, difference choice of λ has
only minor impact on the performance of the attacker. This result demonstrates that our PA-AD
algorithm is robust to different choices of relaxation hyperparameters."
ELSE,0.8381924198250729,Table 4: Performance of PA-AD across difference choices of the relaxation hyperparameter λ
ELSE,0.8396501457725948,"Pong
Boxing
Nature Reward
21 ± 0
96 ± 4
λ = 0.2
−19 ± 2
16 ± 12
λ = 0.4
−18 ± 2
17 ± 12
λ = 0.6
−20 ± 2
19 ± 15
λ = 0.8
−19 ± 2
14 ± 12
λ = 1.0
−19 ± 2
15 ± 12
λ = 2.0
−20 ± 1
21 ± 15
λ = 5.0
−20 ± 1
19 ± 14"
ELSE,0.8411078717201166,(a) Atari
ELSE,0.8425655976676385,"Ant
Walker
Nature Reward
5687 ± 758
4472 ± 635
λ = 0.2
−2274 ± 632
897 ± 157
λ = 0.4
−2239 ± 716
923 ± 132
λ = 0.6
−2456 ± 853
954 ± 105
λ = 0.8
−2597 ± 662
872 ± 162
λ = 1.0
−2580 ± 872
804 ± 130
λ = 2.0
−2378 ± 794
795 ± 124
λ = 5.0
−2425 ± 765
814 ± 140"
ELSE,0.8440233236151603,(b) Mujoco
ELSE,0.8454810495626822,"E.2.3
EMPIRICAL COMPARISON BETWEEN PA-AD AND SA-RL"
ELSE,0.8469387755102041,"In this section, we provide more empirical comparison between PA-AD and SA-RL. Note that PA-
AD and SA-RL are different in terms of their applicable scenarios: SA-RL is a black-box attack
methods, while PA-AD is a white-box attack method. When the victim model is known, we can see
that by a proper exploitation of the victim model, PA-AD demonstrates better attack performance,
higher sample and computational efﬁciency, as well as higher scalability. Appendix F.2 shows de-
tailed theoretical comparison between SA-RL and PA-AD."
ELSE,0.8483965014577259,"PA-AD has better convergence property than SA-RL. In Figure 12, we plot the learning curves
of SA-RL and PA-AD in the CartPole environment and the Ant environment. Compared with SA-
RL attacker, PA-AD has a higher attacking strength in the beginning and converges much faster. In
Figure 12b, we can see that PA-AD has a “warm-start” (the initial reward of the victim is already
signiﬁcantly reduced) compared with SA-RL attacker which starts from scratch. This is because
PA-AD always tries to maximize the distance between the perturbed policy and the original victim
policy in every step according to the actor function (G). So in the beginning of learning, PA-AD
works similarly to the MaxDiff attacker, while SA-RL works similarly to a random attacker. We also
note that although PA-AD algorithm is proposed particularly for environments that have state spaces
much larger than action spaces, in CartPole where the state dimensions is fewer than the number of
actions, PA-AD still works better than SA-RL because of the distance maximization."
ELSE,0.8498542274052479,"(a) Learning curve of SA-RL and PA-AD attacker
against an A2C victim in CartPole."
ELSE,0.8513119533527697,"(b) Learning curve of SA-RL and PA-AD attacker
against a PPO victim in Ant."
ELSE,0.8527696793002916,"Figure 12: Comparison of convergence rate between SA-RL and PA-AD in Ant and Cartpole. Results are
averaged over 10 random seeds."
ELSE,0.8542274052478134,Published as a conference paper at ICLR 2022
ELSE,0.8556851311953353,"PA-AD is more computationally efﬁcient than SA-RL. Our experiments in Section 6 show that
PA-AD converges to a better adversary than SA-RL given the same number of training steps, which
veriﬁes the sample efﬁciency of PA-AD. Another aspect of efﬁciency is based on the computational
resources, including running time and required memory. For RL algorithms, the computation cost
comes from the interaction with the environment (the same for SA-RL and PA-AD) and the pol-
icy/value update. If the state space S is higher-dimensional than the action space A, then SA-RL
requires a larger policy network than PA-AD since SA-RL has a higher-dimensional output, and
thus SA-RL has more network parameters than PA-AD, which require more memory cost and more
computation operations. On the other hand, PA-AD requires to solve an additional optimization
problem deﬁned by the actor objective (G) or (GD). In our implementation, we use FGSM which
only requires one-step gradient computation and is thus efﬁcient. But if more advanced optimiza-
tion algorithms (e.g. PGD) are used, more computations may be needed. In summary, if S is much
larger than A, PA-AD is more computational efﬁcient than SA-RL; if A is much larger than S, SA-
RL is more efﬁcient than PA-AD; if the sizes of S and A are similar, PA-AD may be slightly more
expensive than SA-RL, depending on the optimization methods selected for the actor."
ELSE,0.8571428571428571,"To verify the above analysis, we compare computational training time for training SA-RL and PA-
AD attackers, which shows that PA-AD is more computationally efﬁcient. Especially on the envi-
ronment with high-dimensional states like Ant, PA-AD takes signiﬁcantly less training time than
SA-RL (and ﬁnds a better adversary than SA-RL), which quantiﬁes the efﬁciency of our algorithm
in empirical experiments."
ELSE,0.858600583090379,"Method
Hopper
Walker2d
HalfCheetah
Ant
SA-RL
1.80
1.92
1.76
4.88
PA-AD
1.43
1.46
1.40
3.76"
ELSE,0.8600583090379009,"Table 5: Average training time (in hours) of SA-RL and PA-AD in MuJoCo environments, using GeForce
RTX 2080 Ti GPUs. For Hopper, Walker2d and HalfCheetah, SA-RL and PA-AD are both trained for 2 million
steps; for Ant, SA-RL and PA-AD are both trained for 5 million steps"
ELSE,0.8615160349854227,"PA-AD is less sensitive to hyperparameters settings than SA-RL. In addition to better ﬁnal
attacking results and convergence property, we also observe that PA-AD is much less sensitive to
hyerparameter settings compared to SA-RL. On the Walker environment, we run a grid search over
216 different conﬁgurations of hyperparameters, including actor learning rate, critic learning rate,
entropy regularization coefﬁcient, and clipping threshold in PPO. Here for comparison we plot two
histograms of the agent’s ﬁnal attacked results across different hyperparameter conﬁgurations."
ELSE,0.8629737609329446,"(a) SA-RL Attacker
(b) PA-AD Attacker"
ELSE,0.8644314868804664,"Figure 13: Histograms of victim rewards under different hyperparameter settings of SA-RL and PA-AD on
Walker."
ELSE,0.8658892128279884,"The perturbation radius is set to be 0.05, for which the mean reward reported by Zhang et al. (2020a)
is 1086. However, as we can see from this histogram, only one out of the 216 conﬁgurations of SA-
RL achieves an attacking reward within the range 1000-2000, while in most hyperparameter settings,
the mean attacked return lies in the range 4000-4500. In contrast, about 10% hyperparameter settings"
ELSE,0.8673469387755102,Published as a conference paper at ICLR 2022
ELSE,0.8688046647230321,"of PA-AD algorithm are able to reduce the reward to 500-1000, and another 10% settings could
reduce the reward to 1000-2000. Therefore, the performance of PA-AD attacker is generally better
and more robust across different hyperparameter conﬁgurations than SA-RL."
ELSE,0.8702623906705539,"E.2.4
ROBUST TRAINING EFFICIENCY ON MUJOCO BY PA-ATLA"
ELSE,0.8717201166180758,"In the ATLA process proposed by Zhang et al. (2021), one alternately trains an agent and an adver-
sary. As a result, the agent policy may learn to adapt to the speciﬁc type of attacker it encounters
during training. In Table 3, we present the performance of our robust training method PA-ATLA-
PPO compared with ATLA-PPO under different types of attacks during testing. ATLA-PPO uses
SA-RL to train the adversary, while PA-ATLA-PPO uses PA-AD to train the adversary during al-
ternating training. As a result, we can see that ATLA-PPO models perform better under the SA-RL
attack, and PA-ATLA-PPO performs better under the PA-AD attack. However, the advantage of
ATLA-PPO over PA-ATLA-PPO against SA-RL attack is much smaller than the advantage of PA-
ATLA-PPO over ATLA-PPO against PA-AD attack. In addition, our PA-ATLA-PPO models signif-
icantly outperform ATLA-PPO models against other heuristic attack methods, and achieve higher
average rewards across all attack methods. Therefore, PA-ATLA-PPO is generally more robust than
ATLA-PPO."
ELSE,0.8731778425655977,"Furthermore, the efﬁciency of training an adversary could be the bottleneck in the ATLA Zhang
et al. (2021) process for practical usage. Appendix E.2.3 suggests that our PA-AD generally con-
verges faster than SA-RL. Therefore, when the computation resources are limited, PA-ATLA-PPO
can train robust agents faster than ATLA-PPO. We conduct experiments on continuous control envi-
ronments to empirically show the efﬁciency comparison between PA-ATLA-PPO and ATLA-PPO.
In Table 6, we show the robustness performance of two ATLA methods with 2 million training steps
for Hopper, Walker and Halfcheetah and 5 million steps for Ant (Compared with results in Table 3,
we have reduced training steps by half or more). It can be seen that our PA-ATLA-PPO models still
signiﬁcantly outperform the original ATLA-PPO models under different types of attacks. More im-
portantly, our PA-ATLA-PPO achieves higher robustness under SA-RL attacks in Walker and Ant,
suggesting the efﬁciency and effectiveness of our method."
ELSE,0.8746355685131195,"Environment
ϵ
step(million)
Model
Natural
Reward
RS
Zhang et al. (2020a)
SA-RL
Zhang et al. (2021)
PA-AD
(ours)
Average reward
across attacks"
ELSE,0.8760932944606414,"Hopper
0.075
2
ATLA-PPO
1763 ± 818
1349 ± 174
1172 ± 344
477 ± 30
999.3"
ELSE,0.8775510204081632,"PA-ATLA-PPO
2164 ± 121
1720 ± 490
1119 ± 123
1024 ± 188
1287.7"
ELSE,0.8790087463556852,"Walker
0.05
2
ATLA-PPO
3183 ± 842
2405 ± 529
2170 ± 1032
516 ± 47
1697.0"
ELSE,0.880466472303207,"PA-ATLA-PPO
3206 ± 445
2749 ± 106
2332 ± 198
1072 ± 247
2051.0"
ELSE,0.8819241982507289,"Halfcheetah
0.15
2
ATLA-PPO
4871 ± 112
3781 ± 645
3493 ± 372
856 ± 118
2710.0"
ELSE,0.8833819241982507,"PA-ATLA-PPO
5257 ± 94
4012 ± 290
3329 ± 183
1670 ± 149
3003.7"
ELSE,0.8848396501457726,"Ant
0.15
5
ATLA-PPO
3267 ± 51
3062 ± 149
2208 ± 56
−18 ± 100
1750.7"
ELSE,0.8862973760932945,"PA-ATLA-PPO
3991 ± 71
3364 ± 254
2685 ± 41
2403 ± 82
2817.3"
ELSE,0.8877551020408163,"Table 6: Average episode rewards ± standard deviation of robust models with fewer training steps under
different evasion attack methods. Results are averaged over 50 episodes. We bold the strongest attack in each
row. The gray cells are the most robust agents with the highest average rewards across all attacks."
ELSE,0.8892128279883382,"E.2.5
ATTACKING ROBUSTLY TRAINED AGENTS ON ATARI"
ELSE,0.89067055393586,"In this section, we show the attack performance of our proposed algorithm PA-AD against DRL
agents that are trained to be robust by prior works (Zhang et al., 2020a; Oikarinen et al., 2020) in
Atari games."
ELSE,0.892128279883382,"Zhang et al. (2020a) propose SA-DQN, which minimizes the action change under possible state
perturbations within ℓp norm ball, i.e., to minimize the extra loss"
ELSE,0.8935860058309038,"RDQN(θ) :=
X"
ELSE,0.8950437317784257,"s
max

max
ˆs∈B(s) max
a̸=a∗Qθ(ˆs, a) −Qθ (ˆs, a∗(s)) , −c

(37)"
ELSE,0.8965014577259475,"where θ refers to the Q network parameters, a∗(s) = argmaxaQθ(a|s), and c is a small constant.
Zhang et al. (2020a) solve the above optimization problem by a convex relaxation of the Q network,
which achieves 100% action certiﬁcation (i.e. the rate that action changes with a constrained state
perturbation) in Pong and Freeway, over 98% certiﬁcation in BankHeist and over 47% certiﬁcation
in RoadRunner under attack budget ϵ = 1/255."
ELSE,0.8979591836734694,Published as a conference paper at ICLR 2022
ELSE,0.8994169096209913,"Environment
Natural
Reward
ϵ
Random
MinBest
Huang et al. (2017)"
ELSE,0.9008746355685131,"MinBest +
Momentum
Korkmaz (2020)"
ELSE,0.902332361516035,"MinQ
Pattanaik et al. (2018)
MaxDiff
Zhang et al. (2020a)
PA-AD
(ours)"
ELSE,0.9037900874635568,"SA-DQN
RoadRunner 46440 ± 5797
1
255 45032 ± 7125
40422 ± 8301
43856 ± 5445
42790 ± 8456
45946 ± 8499
38652 ± 6550"
ELSE,0.9052478134110787,"BankHeist
1237 ± 11
1
255
1236 ± 12
1235 ± 15
1233 ± 17
1237 ± 14
1236 ± 13
1237 ± 14"
ELSE,0.9067055393586005,"RADIAL
-DQN"
ELSE,0.9081632653061225,"RoadRunner 39102 ± 13727
1
255 41584 ± 8351
41824 ± 7858
42330 ± 8925
40572 ± 9988
42014 ± 8337
38214 ± 9119"
ELSE,0.9096209912536443,"3
255 23766 ± 6129
9808 ± 4345
35598 ± 8191
39866 ± 6001
18994 ± 6451
1366 ± 3354"
ELSE,0.9110787172011662,"BankHeist
1060 ± 95
1
255
1037 ± 103
991 ± 105
988 ± 102
1021 ± 96
1042 ± 112
999 ± 100"
ELSE,0.9125364431486881,"3
255
1011 ± 130
801 ± 114
460 ± 310
842 ± 33
1023 ± 110
397 ± 172"
ELSE,0.9139941690962099,"RADIAL
-A3C"
ELSE,0.9154518950437318,"RoadRunner 30854 ± 7281
1
255 30828 ± 7297
31296 ± 7095
31132 ± 6861
30838 ± 5743
32038 ± 6898
30550 ± 7182"
ELSE,0.9169096209912536,"3
255 30690 ± 7006
30198 ± 6075
29936 ± 5388
29988 ± 6340
31170 ± 7453
29768 ± 5892"
ELSE,0.9183673469387755,"BankHeist
847 ± 31
1
255
847 ± 31
847 ± 33
848 ± 31
848 ± 31
848 ± 31
848 ± 31"
ELSE,0.9198250728862973,"3
255
848 ± 31
644 ± 158
822 ± 11
842 ± 33
834 ± 30
620 ± 168"
ELSE,0.9212827988338192,"Table 7: Average episode rewards ± standard deviation of SA-DQN, RADIAL-DQN, RADIAL-A3C robust
agents under different evasion attack methods in Atari environments RoadRunner and BankHeist. All attack
methods use 30-step PGD to compute adversarial state perturbations. Results are averaged over 50 episodes.
In each row, we bold the strongest attack, except for the rows where none of the attacker reduces the reward
signiﬁcantly (which suggests that the corresponding agent is relatively robust).)
Oikarinen et al. (2020) propose another robust training method named RADIAL-RL. By adding a
adversarial loss to the classical loss of the RL agents, and solving the adversarial loss with interval
bound propagation, the proposed RADIAL-DQN and RADIAL-A3C achieve high rewards in Pong,
Freeway, BankHeist and RoadRunner under attack budget ϵ = 1/255 and ϵ = 3/255."
ELSE,0.922740524781341,"Implementation of the Robust Agents and Environments.
We directly use the trained SA-DQN
agents provided by Zhang et al. (2020a), as well as RADIAL-DQN and RADIAL-A3C agents pro-
vided by Oikarinen et al. (2020). During test time, the agents take actions deterministically. In order
to reproduce the results in these papers, we use the same environment conﬁgurations as in Zhang
et al. (2020a) and Oikarinen et al. (2020), respectively. But note that the environment conﬁgura-
tions of SA-DQN and RADIAL-RL are simpler versions of the traditional Atari conﬁgurations we
use (described in Appendix E.1.1). Both SA-DQN and RADIAL-RL use a single frame instead
of the stacking as 4 frames. Moreover, SA-DQN restricts the number of actions as 6 (4 for Pong)
in each environment, although the original environments have 18 actions (6 for Pong). The above
simpliﬁcations in environments can make robust training easier since the dimensionality of the input
space is much smaller, and the number of possible outputs is restricted."
ELSE,0.924198250728863,"Attack Methods
In experiments, we ﬁnd that the robust agents are much harder to attack than
vanilla agents in Atari games, as claimed by the robust training papers (Zhang et al., 2020a; Oikari-
nen et al., 2020). A reason is that Atari games have discrete action spaces, and leading an agent
to make a different decision at a state with a limited perturbation could be difﬁcult. Therefore, we
use a 30-step Projected Gradient Descent for all attack methods (with step size ϵ/10), including
MinBest (Huang et al., 2017) and our PA-AD which use FGSM for attacking vanilla models. Note
that the PGD attacks used by Zhang et al. (2020a) and Oikarinen et al. (2020) in their experiments
are the same as the MinBest-PGD attack we use. For our PA-AD, we use PPO to train the adversary
since PPO is relatively stable. The learning rate is set to be 5e −4, and the clip threshold is 0.1.
Note that SA-DQN, RADIAL-DQN and RADIAL-A3C agents all take deterministic actions, so we
use the deterministic formulation of PA-AD as described in Appendix C.1. In our implementation,
we simply use a CrossEntropy loss for the actor as in Equation (38).
gD(ba, s) = argmins′∈Bϵ(s)CrossEntropy(π(s′), ba).
(38)"
ELSE,0.9256559766763849,"Experiment Results
In Table 7, we reproduce the results reported by Zhang et al. (2020a)
and Oikarinen et al. (2020), and demonstrate the average rewards gained by these robust agents
under different attacks in RoadRunner and BankHeist. Note that SA-DQN is claimed to be robust to
attacks with budget ϵ = 1/255, and RADIAL-DQN and RADIAL-A3C are claimed to be relatively
robust against up to ϵ = 3/255 attacks. (ℓ∞is used in both papers.) So we use the same ϵ’s for these
agents in our experiments."
ELSE,0.9271137026239067,"It can be seen that compared with vanilla agents in Table 1, SA-DQN, RADIAL-DQN and RADIAL-
A3C are more robust due to the robust training processes. However, in some environments, PA-AD
can still decrease the rewards of the agent signiﬁcantly. For example, in RoadRunner with ϵ ="
ELSE,0.9285714285714286,Published as a conference paper at ICLR 2022
ELSE,0.9300291545189504,"3/255, RADIAL-DQN gets 1k+ reward against our PA-AD attack, although RADIAL-DQN under
other attacks can get 10k+ reward as reported by Oikarinen et al. (2020). In contrast, we ﬁnd that
RADIAL-A3C is relatively robust, although the natural rewards gained by RADIAL-A3C are not as
high as RADIAL-DQN and SA-DQN. Also, as SA-DQN achieves over 98% action certiﬁcation in
BankHeist, none of the attackers is able to noticeably reduce its reward with ϵ = 1/255."
ELSE,0.9314868804664723,"Therefore, our PA-AD can approximately evaluate the worst-case performance of an RL agent under
attacks with ﬁxed constraints, i.e., PA-AD can serve as a “detector” for the robustness of RL agents.
For agents that perform well under other attacks, PA-AD may still ﬁnd ﬂaws in the models and
decrease their rewards; for agents that achieve high performance under PA-AD attack, they are very
likely to be robust against other attack methods."
ELSE,0.9329446064139941,"E.2.6
IMPROVING ROBUSTNESS ON ATARI BY PA-ATLA"
ELSE,0.934402332361516,"Note that different from SA-DQN (Zhang et al., 2020a) and RADIAL-RL (Oikarinen et al., 2020)
discussed in Appendix E.2.5, we use the traditional Atari conﬁgurations (Mnih et al., 2015) without
any simpliﬁcation (e.g. disabling frame stacking, or restricting action numbers). We aim to im-
prove the robustness of the agents in original Atari environments, as in real-world applications, the
environments could be complex and unchangeable."
ELSE,0.9358600583090378,"Baselines
We propose PA-ATLA-A2C by combining our PA-AD and the ATLA framework pro-
posed by Zhang et al. (2021). We implement baselines including vanilla A2C, adversarially trained
A2C (with MinBest (Huang et al., 2017) and MaxDiff (Zhang et al., 2020a) adversaries attacking
50 frames). SA-A2C (Zhang et al., 2020a) is implemented using SGLD and convex relaxations in
Atari environments."
ELSE,0.9373177842565598,"In Table 6, naive adversarial training methods have unreliable performance under most strong attacks
and SA-A2C is ineffective under PA-AD strongest attack. To provide evaluation using different ϵ,
we provide the attack rewards of all robust models with different attack budgets ϵ. Under all at-
tacks with different ϵ value, PA-ATLA-A2C models outperform all other robust models and achieve
consistently better average rewards across attacks. We can observe that our PA-ATLA-A2C training
method can considerably enhance the robustness in Atari environments."
ELSE,0.9387755102040817,"Model
Natural
Reward
ϵ
Random
MinBest
Huang et al. (2017)
MaxDiff
Zhang et al. (2020a)
SA-RL
Zhang et al. (2021)
PA-AD
(ours)
Average reward
across attacks"
ELSE,0.9402332361516035,"A2C
vanilla
1228 ± 93
1/255
1223 ± 77
972 ± 99
1095 ± 107
1132 ± 30
436 ± 74
971.6"
ELSE,0.9416909620991254,"3/255
1064 ± 129
697 ± 153
913 ± 164
928 ± 124
284 ± 116
777.2"
ELSE,0.9431486880466472,"A2C
(adv: MinBest Huang et al. (2017))
948 ± 94
1/255
932 ± 69
927 ± 30
936 ± 11
940 ± 103
704 ± 19
887.8"
ELSE,0.9446064139941691,"3/255
874 ± 51
813 ± 32
829 ± 27
843 ± 126
521 ± 72
774.2"
ELSE,0.9460641399416909,"A2C
(adv: MaxDiff Zhang et al. (2020a))
743 ± 29
1/255
756 ± 42
702 ± 89
752 ± 79
749 ± 85
529 ± 45
697.6"
ELSE,0.9475218658892128,"3/255
712 ± 109
638 ± 133
694 ± 115
686 ± 110
403 ± 101
626.6"
ELSE,0.9489795918367347,"SA-A2CZhang et al. (2021)
1029 ± 152
1/255
1054 ± 31
902 ± 89
1070 ± 42
1067 ± 18
836 ± 70
985.8"
ELSE,0.9504373177842566,"3/255
985 ± 47
786 ± 52
923 ± 52
972 ± 126
644 ± 153
862.0"
ELSE,0.9518950437317785,"PA-ATLA-A2C
(ours)
1076 ± 56
1/255
1055 ± 204
957 ± 78
1069 ± 94
1045 ± 143
862 ± 106
997.6"
ELSE,0.9533527696793003,"3/255
1026 ± 78
842 ± 154
967 ± 82
976 ± 159
757 ± 132
913.6"
ELSE,0.9548104956268222,"Table 8: Average episode rewards ± standard deviation over 50 episodes of A2C, A2C with adv. training,
SA-A2C and our PA-ATLA-A2C robust models under different evasion attack methods in Atari environment
BankHeist. In each row, we bold the strongest attack. The gray cells are the most robust agents with the
highest average rewards across all attacks."
ELSE,0.956268221574344,"F
ADDITIONAL DISCUSSION OF OUR ALGORITHM"
ELSE,0.9577259475218659,"F.1
OPTIMALITY OF OUR RELAXED OBJECTIVE FOR STOCHASTIC VICTIMS"
ELSE,0.9591836734693877,Proof of Concept: Optimality Evaluation in A Small MDP
ELSE,0.9606413994169096,"We implemented and tested heuristic attacks and our PA-AD in the 2-state MDP example used in
Appendix B.6, and visualize the results in Figure 14. For simplicity, assume the adversaries can
perturb only perturb π at s1 within a ℓ2 norm ball of radius 0.2. And we let all adversaries perturb
the policy directly based on their objective functions. As shown in Figure 14a, all possible ˜π(s1)’s
form a disk in the policy simplex, and executing above methods, as well as our PA-AD, leads to 4
different policies on this disk. All these computed policy perturbations are on the boundary of the
policy perturbation ball, justifying our Theorem 4."
ELSE,0.9620991253644315,Published as a conference paper at ICLR 2022
ELSE,0.9635568513119533,"(a)
(b)
(c)"
ELSE,0.9650145772594753,"Figure 14: Comparison of the optimality of different adversaries. (a) The policy perturbation generated for s1
by all attack methods. (b) The values of corresponding policy perturbations. (c) A zoomed in version of (b),
where the values of all possible policy perturbations are rendered. Our method ﬁnds the policy perturbation
that achieves the lowest reward among all perturbations."
ELSE,0.9664723032069971,"As our theoretical results suggest, the resulted value vectors lie on a line segment shown in Fig-
ure 14b and a zoomed in version Figure 14c, where one can see that MinBest, MaxWorst and MAD
all fail to ﬁnd the optimal adversary (the policy with lowest value). On the contrary, our PA-AD
ﬁnds the optimal adversary that achieves the lowest reward over all policy perturbations."
ELSE,0.967930029154519,For Continuous MDP: Optimality Evaluation in CartPole and MountainCar
ELSE,0.9693877551020408,"We provided a comparison between SA-RL and PA-AD in the CartPole environment in Figure 15,
where we can see the SA-RL and PA-AD converge to the same result (the learned SA-RL adversary
and PA-AD adversary have the same attacking performance)."
ELSE,0.9708454810495627,Figure 15: Learning curve of SA-RL and PA-AD attacker against an A2C victim in CartPole.
ELSE,0.9723032069970845,"CartPole has a 4-dimensional state space, and contains 2 discrete actions. Therefore since SA-RL
has an optimal formulation, we expect SA-RL to converge to the optimal adversary in a small MDP
like CartPole. Then the result in Figure 15 suggests that our PA-AD algorithm, although with a
relaxation in the actor optimization, also converges to the optimal adversary with even a faster rate
than SA-RL (the reason is explained in Appendix E.2.3)."
ELSE,0.9737609329446064,"In addition to CartPole, we also run experiments in MountainCar with a 2-dimensional state space
against a DQN victim. The SA-RL attacker reduces the victim reward to -128, and our PA-AD
attacker reduces the victim reward to -199.45 within the same number of training steps. Note that the
lowest reward in MountainCar is -200, so our PA-AD indeed converges to a near-optimal adversary,
while SA-RL fails to converge to a near-optimal adversary. This is because MountainCar is an
environment with relatively spare rewards. The actor in PA-AD utilizes our Theorem 4 and only
focuses on perturbations in the outermost boundary, which greatly reduces the exploration
burden in solving an RL problem. In contrast, SA-RL directly uses RL algorithms to learn the
perturbation, and thus it has difﬁculties in converging to the optimal solution."
ELSE,0.9752186588921283,"F.2
MORE COMPARISON BETWEEN SA-RL AND PA-AD"
ELSE,0.9766763848396501,"We provide a more detailed comparison between SA-RL and PA-AD from the following multiple
aspects to claim our contribution."
ELSE,0.978134110787172,Published as a conference paper at ICLR 2022
SIZE OF THE ADVERSARY MDP,0.9795918367346939,"1. Size of the Adversary MDP
Suppose the original MDP has size |S|, |A| for its state space and action space, respectively. Both
PA-AD and SA-RL construct an adversary’s MDP and search for the optimal policy in it. But the
adversary’s MDPs for PA-AD and SA-RL have different sizes."
SIZE OF THE ADVERSARY MDP,0.9810495626822158,"PA-AD: state space is of size |S|, action space is of size R|A|−1 for a stochastic victim, or |A| for a
deterministic victim.
SA-RL: state space is of size |S|, action space is of size |S|."
SIZE OF THE ADVERSARY MDP,0.9825072886297376,"2. Learning Complexity and Efﬁciency
When the state space is larger than the action space, which is very common in RL environments, PA-
AD solves a smaller MDP than SA-RL and thus more efﬁcient. In environments with pixel-based
states, SA-RL becomes computationally intractable, while PA-AD still works. It is also important
to note that the actor’s argmax problem in PA-AD further accelerates the convergence, as it rules out
the perturbations that do not push the victim policy to its outermost boundary. Our experiment and
analysis in Appendix E.2.3 verify the efﬁciency advantage of our PA-AD compared with SA-RL,
even in environments with small state spaces."
OPTIMALITY,0.9839650145772595,"3. Optimality
PA-AD: (1) the formulation is optimal for a deterministic victim policy; (2) for a stochastic victim
policy, the original formulation is optimal, but in practical implementations, a relaxation is used
which may not have optimality guarantees.
SA-RL: the formulation is optimal.
Note that both SA-RL and PA-AD require training an RL attacker, but the RL optimization process
may not converge to the optimal solution, especially in deep RL domains. Therefore, SA-RL and
PA-AD are both approximating the optimal adversary in practical implementations."
KNOWLEDGE OF THE VICTIM,0.9854227405247813,"4. Knowledge of the Victim
PA-AD: needs to know the victim policy (white-box). Note that in a black-box setting, PA-AD can
still be used based on the transferability of adversarial attacks in RL agents, as veriﬁed by Huang
et al. (2017). But the optimality guarantee of PA-AD does not hold in the black-box setting.
SA-RL: does not need to know the victim policy (black-box).
It should be noted that the white-box setting is realistic and helps in robust training:
(1) The white-box assumption is common in existing heuristic methods.
(2) It is always a white-box process to evaluate and improve the robustness of a given agent, for
which PA-AD is the SOTA method. As discussed in our Ethics Statement, the ultimate goal of ﬁnd-
ing the strongest attacker is to better understand and improve the robustness of RL agents. During
the robust training process, the victim is the main actor one wants to train, so it is a white-box set-
ting. The prior robust training art ATLA (Zhang et al., 2021) uses the black-box attacker SA-RL,
despite the fact that it has white-box access to the victim actor. Since SA-RL does not utilize the
knowledge of the victim policy, it usually has to deal with a more complex MDP and face converg-
ing difﬁculties. In contrast, if one replaces SA-RL with our PA-AD, PA-AD can make good use of
the victim policy and ﬁnd a stronger attacker with the same training steps as SA-RL, as veriﬁed in
our Section 6 and Appendix E.2.6."
APPLICABLE SCENARIOS,0.9868804664723032,"5. Applicable Scenarios
SA-RL is a good choice if (1) the action space is much larger than the state space in the original
MDP, or the state space is small and discrete; (2) the attacker wants to conduct black-box attacks.
PA-AD is a good choice if (1) the state space is much larger than the state space in the original
MDP; (2) the victim policy is known to the attacker; (3) the goal is to improve the robustness of
one’s own agent via adversarial training."
APPLICABLE SCENARIOS,0.9883381924198251,"In summary, as we discussed in Section 4, there is a trade-off between efﬁciency and optimality
in evasion attacks in RL. SA-RL has an optimal RL formulation, but empirical results show that
SA-RL usually do not converge to the optimal adversary in a continuous state space, even in a low-
dimensional state space (e.g. see Appendix F.1 for an experiment in MountainCar). Therefore, the
difﬁculty of solving an adversary’s MDP is the bottleneck for ﬁnding the optimal adversary. Our
PA-AD, although may sacriﬁce the theoretical optimality in some cases, greatly reduces the size and
the exploration burden of the attacker’s RL problem (can also be regarded as trading some estimation
bias off for lower variance). Empirical evaluation shows our PA-AD signiﬁcantly outperforms SA-
RL in a wide range of environments."
APPLICABLE SCENARIOS,0.9897959183673469,Published as a conference paper at ICLR 2022
APPLICABLE SCENARIOS,0.9912536443148688,"Though PA-AD requires to have access to the victim policy, PA-AD solves a smaller-sized RL
problem than SA-RL by utilizing the victim’s policy and can be applied on evaluating/improving
the robustness of RL policy. It is possible to let PA-AD work in a black-box setting based on the
transferability of adversarial attacks. For example, in a black-box setting, the attacker can train a
proxy agent in the same environment, and use PA-AD to compute a state perturbation for the proxy
agent, then apply the state perturbation to attack the real victim agent. This is out of the scope of
this paper, and will be a part of our future work."
APPLICABLE SCENARIOS,0.9927113702623906,"F.3
VULNERABILITY OF RL AGENTS"
APPLICABLE SCENARIOS,0.9941690962099126,"It is commonly known that neural networks are vulnerable to adversarial attacks (Goodfellow et al.,
2015). Therefore, it is natural that deep RL policies, which are modeled by neural networks, are
also vulnerable to adversarial attacks (Huang et al., 2017). However, there are few works discussing
the difference between deep supervised classiﬁers and DRL policies in terms of their vulnerabilities.
In this section, we take a step further and investigate the vulnerability of DRL agents, through a
comparison with standard adversarial attacks on supervised classiﬁers. Our main conclusion is
that commonly used deep RL policies can be instrinsically much more vulnerable to small-radius
adversarial attacks. The reasons are explained below."
OPTIMIZATION PROCESS,0.9956268221574344,"1. Optimization process
Due to the different loss functions that RL and supervised learning agents are trained on, the size of
robustness radius of an RL policy is much smaller than that of a vision-based classiﬁer.
On the one hand, computer vision-based image classiﬁers are trained with cross-entropy loss. There-
fore, the classiﬁer is encouraged to make the output logit of the correct label to be larger than the
logits of other labels to maximize the log probability of choosing the correct label. On the other
hand, RL agents, in particular DQN agents, are trained to minimize the Bellman Error instead. Thus
the agent is not encouraged to maximize the absolute difference between the values of different ac-
tions. Therefore, if we assume the two networks are lipschitz continuous and their lipschitz constants
do not differ too much, it is clear that a supervised learning agent has a much larger perturbation
radius than an RL agent."
OPTIMIZATION PROCESS,0.9970845481049563,"To prove our claim empirically, we carried out a simple experiment, we compare the success rate
of target attacks of a well-trained DQN agent on Pong with an image classiﬁer trained on the
CIFAR-10 dataset with similar network architecture. For a fair comparison, we use the same image
preprocessing technique, which is to divide the pixel values by 255 and no further normalization
is applied. On both the image-classiﬁer and DQN model, we randomly sample a target label other
than the model predicted label and run the same 100-step projected gradient descent (PGD) attack
to minimize the cross-entropy loss between the model output and the predicted label. We observe
that for a perturbation radius of 0.005 (l∞norm), the success rate of a targeted attack for the
image classiﬁer is only 15%, whereas the success rate of a targeted attack for the DQN model is
100%. This veriﬁes our claim that a common RL policy is much more vulnerable to small-radius
adversarial attacks than image classiﬁers."
NETWORK COMPLEXITY,0.9985422740524781,"2. Network Complexity
In addition, we also want to point out that the restricted network complexity of those commonly
used deep RL policies could play an important role here. Based on the claim by Madry et al. (2018),
a neural network with greater capacity could have much better robustness, even when trained with
only clean examples. But for the neural network architectures commonly used in RL applications,
the capacity of the networks is very limited compared to SOTA computer vision applications. For
example, the commonly used DQN architecture proposed in Mnih et al. (2015) only has 3 convolu-
tional layers and 2 fully connected layers. But in vision tasks, a more advanced and deeper structure
(e.g. ResNet has 100 layers) is used. Therefore, it is natural that the perturbation radius need for
attacking an RL agent is much smaller than the common radius studied in the supervised evasion
attack and adversarial learning literature."
