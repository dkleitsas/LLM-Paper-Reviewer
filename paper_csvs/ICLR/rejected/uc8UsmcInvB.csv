Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0019193857965451055,"A common lens to theoretically study neural net architectures is to analyze the functions
they can approximate. However, constructions from approximation theory may be
unrealistic and therefore less meaningful. For example, a common unrealistic trick is to
encode target function values using infinite precision. To address these issues, this work
proposes a formal definition of statistically meaningful (SM) approximation which
requires the approximating network to exhibit good statistical learnability. We study
SM approximation for two function classes: boolean circuits and Turing machines.
We show that overparameterized feedforward neural nets can SM approximate boolean
circuits with sample complexity depending only polynomially on the circuit size, not the
size of the network. In addition, we show that transformers can SM approximate Turing
machines with computation time bounded by T with sample complexity polynomial
in the alphabet size, state space size, and logpTq. We also introduce new tools for
analyzing generalization which provide much tighter sample complexities than the
typical VC-dimension or norm-based bounds, which may be of independent interest."
INTRODUCTION,0.003838771593090211,"1
INTRODUCTION"
INTRODUCTION,0.005758157389635317,"Dating back to the seminal works on universal approximation (Cybenko, 1989; Hornik et al., 1989; Park
& Sandberg, 1991; Leshno et al., 1993), a common way to theoretically study neural nets has been through
their expressivity, which measures the ability of neural nets to approximate well-behaved functions. This
perspective has shaped how researchers perceive different types of deep learning architectures: a basic
way to theoretically justify new architectures is to study their approximation capabilities. This has led to
a number of analyses studying universal approximation capabilities for various widely-used architectures,
such as recurrent neural nets (RNNs) (Schäfer & Zimmermann, 2007), graph neural nets (Scarselli et al.,
2008), convolutional networks (Bao et al., 2014; Zhou, 2020; Yarotsky, 2021), residual networks (Lin &
Jegelka, 2018), transformers (Yun et al., 2019), and neural ODEs (Teshima et al., 2020; Zhang et al., 2020)."
INTRODUCTION,0.007677543186180422,"However, approximation theoretic results often misalign with more meaningful end-to-end guarantees,
because models constructed in the literature often exhibit unrealistic properties. For example, a common
technique in the universal approximation literature is to rely strongly on infinite-precision weights and
activations, or exponentially many parameters to encode the desired function values (Hornik et al., 1989;
Cybenko, 1989; Leshno et al., 1993; Lin & Jegelka, 2018; Yun et al., 2019; Sannai et al., 2019). This issue
even arises outside of universal approximation, e.g., various papers demonstrate the ability of RNNs and
transformers to simulate various computational models such as Turing machines and automata, but require
strong reliance on arbitrary precision (Siegelmann & Sontag, 1995; Pérez et al., 2019; Korsky & Berwick,
2019; Bhattamishra et al., 2020). Infinite precision can inflate the expressivity of an architecture in a
unrealistic and misleading way: for example, finite width RNNs with infinite precision can simulate Turing
machines, but finite-precision, finite-width RNNs cannot, as implied by streaming lower bounds (Alon
et al., 1999). As another example, Park et al. (2020) exploit infinite precision in the parameters to show
that a neural net with parameter count sublinear in n can memorize n arbitrary input-label pairs. However,
a simple counting argument reveals that this result cannot be proven using finite precision networks – there
are 2n input-labeling pairs, but only 2opnq finite precision networks with opnq parameters."
INTRODUCTION,0.009596928982725527,"More broadly, the ideal theoretical perspective should consider not only whether target functions can be
expressed, but also whether the constructed networks are plausibly learnable. Learnability is important
because empirical settings do not operate in the infinite data, unbounded computation regime – they
require fitting the target function with access to limited number of samples from an empirical distribution.
The question of studying learnability can be decomposed into studying optimization and generalization."
INTRODUCTION,0.011516314779270634,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.013435700575815739,"Unfortunately, a rigorous analysis of optimization is unresolved even for simple two-layer nets (Mei et al.,
2018). Generalization is more tractable, so we propose to study expressivity and generalization together."
INTRODUCTION,0.015355086372360844,"Towards the goal of studying more meaningful notions of approximation, this work proposes the notion
of statistically meaningful (SM) approximation. This definition requires not only the existence of an
approximating network, but also that it has good statistical learnability. Consider a setting where the aim
is to fit the target function G using the approximating family F and a finite sample of training data. SM
approximation requires existence of a loss whose empirical risk minimizer in F leads to a model with low
approximation error in fitting G. We define the sample complexity of the approximation as the number of
training samples needed to guarantee at most ϵ approximation error and study SM approximation with low
sample complexity bounds. SM approximation essentially eliminates all statistical concerns for learnability
(optimization-related concerns can remain)."
INTRODUCTION,0.01727447216890595,"We present two case studies on SM approximation. First, we demonstrate that overparameterized feed-
forward neural nets can SM approximate boolean circuits with a low sample complexity that depends
only on the intrinsic circuit size. Though it is simple to construct neural nets to approximate boolean
circuits, bounding the sample complexity of the approximation is challenging. For example, standard
norm-based generalization bounds for the naive construction scale exponentially in depth (Bartlett et al.,
2017). Furthermore, VC dimension-based bounds would scale polynomially in the number of parameters
in the network (Harvey et al., 2017), which is problematic because for practical optimization concerns,
neural nets are typically overparameterized in terms of width (Zhang et al., 2016). In contrast, our sample
complexity bound for SM approximation depends only on the intrinsic circuit size, up to logarithmic factors."
INTRODUCTION,0.019193857965451054,"Our second case study is on SM approximating Turing machines with transformers. We consider
a class of Turing machines with bounded computation time T and construct encoder-decoder-based
transformers (Vaswani et al., 2017) which SM approximate these Turing machines.
The sample
complexity of the approximation depends on a polynomial in logT and the sizes of the state space and
the alphabet of the Turing machine. Though constructions for approximating Turing machines from
prior work (Siegelmann & Sontag, 1995; Pérez et al., 2019; Bhattamishra et al., 2020) have not been
formally studied from a sample complexity perspective, existing bounds would depend at least linearly on
T. Furthermore, our construction only uses loglogT precision, compared to at least logT in prior works,
allowing us to achieve the exponential improvement in the sample complexity."
INTRODUCTION,0.02111324376199616,"Proving sample complexity guarantees for our statistically meaningful approximation results is nontrivial
and requires additional insights, for both the constructions and the generalization analyses. To obtain
our sample complexity bounds, we leverage a recent approach to bound generalization in terms of
data-dependent notions of Lipschitzness (Wei & Ma, 2019b). We develop theoretical tools to convert a
broad class of neural nets, with possibly large Lipschitzness, into ones with small Lipschitzness on the
training data, by introducing a number of new layers that is linear in depth. Our result applies to neural
nets where each entry in the hidden representations on the training data takes values from a finite set (e.g.,
binary entries), and may be of independent interest."
INTRODUCTION,0.023032629558541268,"In summary, our contributions are: 1) we propose a new notion of statistically meaningful approximation,
intended to provide more meaningful approximation guarantees by requiring that the approximating family
have good statistical learnability; 2) we prove that feedforward neural nets can meaningfully approximate
boolean circuits with sample complexity that depends polynomially on the width and depth of the circuit;
and 3) we show that transformers can meaningfully approximate Turing machines with sample complexity
logarithmic in the computation time."
RELATED WORKS,0.02495201535508637,"1.1
RELATED WORKS"
RELATED WORKS,0.026871401151631478,"Classifical approximation theory for neural networks has a long history. Hornik et al. (1989); Cybenko
(1989), and Leshno et al. (1993) show that neural nets with one hidden layer are universal approximators
but require the hidden layer size to grow exponentially in input dimension. Barron (1993) uses the Fourier
transform to write target functions as infinite-width networks and subsamples neurons to obtain widths
which depend only on target function properties. Lee et al. (2017); Ji et al. (2020) prove recent related
developments in this direction of universal approximation."
RELATED WORKS,0.028790786948176585,"Many works study benefits of deep networks over shallow ones (Bengio & Delalleau, 2011; Arora et al.,
2016; Telgarsky, 2016; Eldan & Shamir, 2016; Daniely, 2017; Chatziafratis et al., 2020; 2019). Bengio &
Delalleau (2011) show separation for exact representation, whereas Telgarsky (2016) shows separation for
approximate representations with univariate inputs. Eldan & Shamir (2016) demonstrate high-dimensional
functions that can be approximated by two-layer polynomial-sized neural networks, but cannot be approx-
imated by one-layer neural nets with subexponential hidden units. Via reduction to certain complexity theo-"
RELATED WORKS,0.030710172744721688,Under review as a conference paper at ICLR 2022
RELATED WORKS,0.03262955854126679,"retic questions, Vardi & Shamir (2020) show that proving constant depth separations may be hard. Malach
et al. (2021) analyze the relationship between optimization and approximability, showing in various settings
that deeper networks cannot be optimized if shallow networks cannot approximate them. This demonstrates
that depth separation results (Telgarsky, 2016) from approximation theory can be misleading in the sense
that gradient descent anyways cannot optimize the deep networks used to construct the approximation."
RELATED WORKS,0.0345489443378119,"Another area of study is on the ability of deep networks to memorize training data (Zhang et al., 2016;
Yun et al., 2018; Park et al., 2020; Vershynin, 2020). Yun et al. (2018) show that Θpnq parameters
are sufficient to memorize Θpnq training points for ReLU nets with at least 3 layers, and Park et al.
(2020) reduce the parameter requirement to sublinear in n. Similar results have been proven for residual
architectures (Hardt & Ma, 2016) and convolutional nets (Nguyen & Hein, 2018). Bartlett et al. (2019)
analyze the VC-dimension of neural nets, leading to upper and lower bounds on the parameter count
needed to fit training data. Other works study expressivity via connections to tensor approximation and
sum-product networks (Cohen & Shashua, 2016; Cohen et al., 2016)."
RELATED WORKS,0.036468330134357005,"There is a long line of work on studying the ability of neural nets to recognize and represent formal languages.
The seminal work of Siegelmann & Sontag (1995) shows that RNNs are Turing complete but leverages in-
finite precision in the hidden activations. Chen et al. (2018) extend this result to ReLU activations and study
implications in language modeling. Many variants of transformers are shown to be Turing-complete, but
these constructions also rely on arbitrary precision (Pérez et al., 2019; Bhattamishra et al., 2020). A number
of recent works have also proven results for generating or recognizing formal languages with finite-precision
neural nets (Weiss et al., 2018; Korsky & Berwick, 2019; Hewitt et al., 2020), but these results do not con-
sider Turing machines or analyze statistical properties of their constructions. Bounding the sample complex-
ity of SM approximation requires additional complications in both the construction and statistical analysis."
NOTATION,0.03838771593090211,"1.2
NOTATION"
NOTATION,0.04030710172744722,"Let f˝g denote the composition of functions f and g. For a family of functions G, let f˝Gﬁtf˝g:gPGu
denote the family of compositions between f and functions in G. For a set S and function f :S ÑY, let
fpSq denote the set tfpsq:sPSuĎY. We use 1d to denote the all-one’s vector in d dimensions, with
the subscripted omitted if clear. For iPrds, we let 1dpiq denote the one-hot embedding in d-dimensions,
which is 1 at index i and 0 everywhere else. We use the notation rOp¨q to hide poly-logarithmic factors in
the argument. The notation À,Á indicates the existence of a constant factor such that the inequality holds.
— denotes that the Á and À relations simultaneously hold. We use polyp¨q to indicate the existence of a
polynomial in the argument which makes the equation true. For a set A (e.g., the set of alphabet symbols for
a Turing machine) let A˚ denote the set of all sequences of elements of A, where sequence length can vary."
NOTATION,0.04222648752399232,"Let P denote a distribution over a space of inputs X. Let ξ1,...,ξn be n i.i.d. Rademacher variables sampled
from t´1,`1u. The expected n-sample Rademacher complexity of F on P is as follows: Radn,PpFqﬁ
Epxiqn
i“1
i.i.d
„ P
“
Eξ1,...,ξn
“
supFPF
1
n
řn
i“1ξiFpxiq
‰‰
, where pxiqn
i“1 denotes n i.i.d. samples from P."
STATISTICALLY MEANINGFUL APPROXIMATION,0.044145873320537425,"2
STATISTICALLY MEANINGFUL APPROXIMATION"
STATISTICALLY MEANINGFUL APPROXIMATION,0.046065259117082535,"We consider settings where we wish to approximate every member G in a real-valued function class G
with some function F in function class F. In this work, F is some family of neural networks. Fix a loss
ℓ:RˆRÑr0,1s. The classical definition of ϵ-approximation states that F ϵ-approximates G with respect
to ℓ,P if for all GPG, there exists F PF such that Ex„PrℓpFpxq,Gpxqqsďϵ."
STATISTICALLY MEANINGFUL APPROXIMATION,0.04798464491362764,"The issue with this classical notion of approximation is that in machine learning settings, we only have
access to G, the function we wish to learn, through its values on a finite training set: pxi,Gpxiqqn
i“1. If
we disregard this fact, we could end up constructing functions F which approximate G, but could have
a number of unrealistic characteristics such as infinite precision. These drawbacks would mean that F
cannnot be realistically learned from the training sample."
STATISTICALLY MEANINGFUL APPROXIMATION,0.04990403071017274,"This work studies a stronger notion of approximation, statistically meaningful (SM) approximation, to elim-
inate statistical concerns related to fitting G on a finite sample. SM approximation, defined below, requires
that G is learnable via empirical risk minimization using models from F, when data is generated from P."
STATISTICALLY MEANINGFUL APPROXIMATION,0.05182341650671785,"Definition 2.1 (SM approximation). F ϵ-SM approximates G with respect to ℓ,P with sample complexity
n if there exists a losssℓ:FˆX ˆRÑr0,1s such that the following holds for all GPG:"
STATISTICALLY MEANINGFUL APPROXIMATION,0.053742802303262956,Under review as a conference paper at ICLR 2022
STATISTICALLY MEANINGFUL APPROXIMATION,0.05566218809980806,"Define pF PF to be the empirical minimizer of the loss sℓfor fitting G on an i.i.d. sample pxi,Gpxiqqn
i“1
of n examples labeled by G: pF ﬁargminFPF
1
n
řn
i“1sℓpF,xi,Gpxiqq. Then with probability 0.99 over the
draw of pxiqn
i“1, pF approximates G in the classical sense: Ex„PrℓppFpxq,Gpxqqsďϵ."
STATISTICALLY MEANINGFUL APPROXIMATION,0.05758157389635317,"Definition 2.1 eases the statistical concerns associated with classical approximation theory: given a finite
sample pxi,Gpxiqqn
i“1, the empirical risk minimizer ofsℓover F is guaranteed to ϵ-approximate G on the
population distribution. It is important that the lossessℓ(which can be interpreted as a training surrogate
loss) and ℓcan be different, as this allows the empirical risk to include regularization."
STATISTICALLY MEANINGFUL APPROXIMATION,0.05950095969289827,"Though Definition 2.1 may be reminiscent of PAC-learnability, there is a major conceptual difference:
SM approximation unifies expressivity and generalization, whereas PAC-learnability is only concerned
with generalization. The main focus of PAC-learnability is achieving a low loss relative to the best function
in the hypothesis class, which is assumed to have 0 loss in the realizable case. SM approximation also
requires proving that the best function in F achieves near-zero loss."
BACKGROUND AND TOOLS,0.061420345489443376,"2.1
BACKGROUND AND TOOLS"
BACKGROUND AND TOOLS,0.06333973128598848,"To prove SM-approximation guarantees, Definition 2.1 requires a loss surrogatesℓsuch that the empirical
risk minimizer of sℓon the training data can approximate functions in G. The following proposition
provides several conditions onsℓwhich lead to SM approximation guarantees.
Proposition 2.2. For loss function ℓ: R ˆ R Ñ r0,1s and distribution P, suppose there exists a loss
sℓ:FˆX ˆRÑr0,1s, intended as a surrogate loss for ℓ, satisfying the following properties:"
BACKGROUND AND TOOLS,0.06525911708253358,"1) For all F PF, xPX, yPR,sℓpF,x,yqěℓpFpxq,yq."
BACKGROUND AND TOOLS,0.0671785028790787,"2) For all G P G, consider the function class LG ﬁtx ÞÑ sℓpF,x,Gpxqq : F P Fu. Then the n-sample
Rademacher complexity of LG is bounded: Radn,PpLGqďϵ."
BACKGROUND AND TOOLS,0.0690978886756238,"3) For all GPG, there exists F PF with small surrogate loss: Ex„PrsℓpF,x,Gpxqqsďϵ."
BACKGROUND AND TOOLS,0.0710172744721689,"Then F O
´
ϵ` 1
?n
¯
-SM approximates G with respect to ℓ,P with sample complexity n."
BACKGROUND AND TOOLS,0.07293666026871401,"By Proposition 2.2, it suffices that sℓupper bounds the target loss ℓand has low complexity, and F
approximates G with respect to sℓ, P in the classical sense. The proof follows from standard techniques
for bounding generalization based on Rademacher complexity and is provided in Section A."
BACKGROUND AND TOOLS,0.07485604606525911,"All-layer margin loss. We introduce one particular construction for sℓused in subsequent sections, which
is motivated by the all-layer margin generalization bound proposed by (Wei & Ma, 2019b). This bound
is based on data-dependent Lipschitzness measures (Nagarajan & Kolter, 2019; Wei & Ma, 2019a), and
can provide stronger guarantees than classical norm-based bounds (Neyshabur et al., 2015; Bartlett et al.,
2017; Neyshabur et al., 2017; Golowich et al., 2018)."
BACKGROUND AND TOOLS,0.07677543186180422,"We focus on the binary classification setting, where GpxqPt0,1u, and study approximation with respect
to the 0-1 loss ℓ0-1pz,yqﬁ1ppy´0.5qzď0q where yPt0,1u is assumed to be a binary label. We consider a
family of functions F parameterized by p-dimensional parameters θPΘĎRp, with a general architecture
function F :X ˆRpÑR. Thus, F “txÞÑFpx,θq:θPΘu, and we sometimes use θ to identify an element
of F. Throughout the paper, we define Θ as a }¨}1-norm bounded set: }θ}1 ďα, @θPΘ. We define the
parameter-based all-layer margin ρF :RpˆX ˆt0,1uÑR as follows:"
BACKGROUND AND TOOLS,0.07869481765834933,"ρFpθ,x,yqﬁmin }δ}2
subject to py´0.5q¨Fpx,θ`δqď0
(2.1)"
BACKGROUND AND TOOLS,0.08061420345489444,"We omit the architecture from the subscript when it is clear from context. This quantity measures the
stability of the model around an input x in parameter space. As is the case for the standard output margin,
a larger all-layer margin, or better stability, implies better generalization."
BACKGROUND AND TOOLS,0.08253358925143954,"We modified the definition in (Wei & Ma, 2019b) to consider perturbations δ in parameter space,
whereas Wei & Ma (2019b) consider perturbations to the hidden layers. The parameter-space formulation is
simpler and subsumes the results in (Wei & Ma, 2019b). Our formulation also accounts for weight sharing,
which is important for our Turing machine results, whereas the formulation of (Wei & Ma, 2019b) could not."
BACKGROUND AND TOOLS,0.08445297504798464,"A key and immediate property of the all-layer margin is that it is strictly positive if and only if Fpx,θq
predicts the correct label. We can leverage this property to construct a surrogate loss. For some parameter"
BACKGROUND AND TOOLS,0.08637236084452975,Under review as a conference paper at ICLR 2022
BACKGROUND AND TOOLS,0.08829174664107485,"γ intended to lower bound the all-layer margins, we define the losssℓγ as follows:"
BACKGROUND AND TOOLS,0.09021113243761997,"sℓγpθ,x,yq“ $
’
& ’
%"
BACKGROUND AND TOOLS,0.09213051823416507,"1 if ρpθ,x,yqď0
1´ ρpθ,x,yq"
BACKGROUND AND TOOLS,0.09404990403071017,"γ
if 0ăρpθ,x,yqďγ
0 if ρpθ,x,yqěγ
(2.2)"
BACKGROUND AND TOOLS,0.09596928982725528,"Note that sℓγ composes the classical ramp loss, which is used to prove margin-based generalization
complexity bounds, with the value of the all-layer margin. By our construction, it immediately follows
thatsℓγpθ,x,Gpxqqěℓ0-1pFpx,θq,Gpxqq, as is required of a surrogate loss."
BACKGROUND AND TOOLS,0.09788867562380038,"We show that to obtain sample complexity bounds for SM approximation of G in a classification setting,
it suffices to prove that functions in F can fit labels of GPG with large all-layer margin.
Lemma 2.3. Fix any neural net architecture F : X ˆRp Ñ R, and define Fα ﬁtx ÞÑ Fpx,θq : θ P Θu,
where we assume ΘĎRp is such that }θ}1ďα for all θPΘ. Fix ϵě0. Suppose that for all GPG, there
exists θPΘ such that the following holds:
Ex„Pr1pρpθ,x,Gpxqqăγqsďϵ
(2.3)"
BACKGROUND AND TOOLS,0.09980806142034548,"Then Fα ϵ-SM approximates G with respect to ℓ0-1,P with sample complexity rO
´
1
ϵ2
´
α2logppq"
BACKGROUND AND TOOLS,0.1017274472168906,"γ2
`1
¯¯
."
BACKGROUND AND TOOLS,0.1036468330134357,"We note that rO hides poly-logarithmic factors in the arguments, in this case, polylogpα2logppq"
BACKGROUND AND TOOLS,0.10556621880998081,"γ2ϵ2
q factors.
The proof closely follows (Wei & Ma, 2019b), is deferred to Section A. In Section A, we also state a
generalization bound for 0-1 loss based on (2.1), which may be of independent interest. We use (2.2) and
Lemma 2.3 to prove that neural nets can SM approximate Boolean circuits and Turing machines."
SM APPROXIMATION OF BOOLEAN CIRCUITS WITH FEEDFORWARD NETS,0.10748560460652591,"3
SM APPROXIMATION OF BOOLEAN CIRCUITS WITH FEEDFORWARD NETS"
SM APPROXIMATION OF BOOLEAN CIRCUITS WITH FEEDFORWARD NETS,0.10940499040307101,"This section shows that feedforward neural nets can SM approximate Boolean circuits with sample
complexity that depends polynomially on the size of the circuit. A boolean circuit G:t0,1um Ñt0,1u
on m inputs bits is described by a directed acyclic graph, with vertices of this graph referred to as “gates”.
The graph contains m input gates of indegree 0, which are identified with the input bits. The remaining
gates each compute a boolean function taking values at their parents as arguments, and a designated output
gate produces the output of the entire circuit. We consider boolean circuits consisting of AND, OR, and
NOT gates, which compute the corresponding boolean functions on 2, 2, and 1 inputs, respectively and
are sufficient to compute any boolean function (Savage, 1998). We also allow identity (ID) gates, which
take 1 input and output the same value."
SM APPROXIMATION OF BOOLEAN CIRCUITS WITH FEEDFORWARD NETS,0.11132437619961612,"We consider layered circuits, where we can partition the gates into layers such that the only edges in the
graph occur from gates in layer i to gates in layer i`1 for some i. Note that we can transform any boolean
circuit into a layered one by adding ID gates. Letting q denote the number of layers and r the maximum
number of gates in any layer, we say that the circuit has depth q and width r. We say that a circuit with
s total gates has size s. Our convention will be that the set of input gates is considered a layer, so rěm.
We consider the following class of boolean circuits:
Gq,r,s“tG:t0,1umÑt0,1u:G computed by circuit with depth q, size s, and width ru
We will approximate Gq,r,s using a family of width w, depth d feedforward ReLU nets pa-
rameterized by linear weights and biases θ
“
pW0, b0, ... , Wd, bdq computed as follows:
Fw,dpx, θq “ WdφpWd´1φp¨¨¨ φpW0x ` b0q ¨¨¨ q ` bd´1q ` bd, where all intermediate layers have
width w for simplicity and φ denotes the coordinate-wise ReLU activation. The weight parameters are
set so that for 1 ď i ď d´1, Wi P Rwˆw, W0 P Rwˆm, and Wd P R1ˆw. The bias parameters are such
that bi PRw for 0ďiďd´1, and bd PR. To control the sample complexity, we restrict our attention to
the set of parameters with total }¨}1-norm bounded by α, giving the following function class:
Fw,d,α“txÞÑFw,dpx,θq:}θ}1ďαu"
SM APPROXIMATION OF BOOLEAN CIRCUITS WITH FEEDFORWARD NETS,0.11324376199616124,"The following theorem states that feedforward neural nets can statistically meaningfully approximate
boolean circuits with sample complexity polynomial in the circuit size.
Theorem 3.1. Consider the class Gq,r,s of size-s,width-r, and depth-q layered boolean circuits, and the
class Fw,d,α of neural nets above. Suppose wÁr, α—s, and d—q."
SM APPROXIMATION OF BOOLEAN CIRCUITS WITH FEEDFORWARD NETS,0.11516314779270634,"Then for all ϵą0 and any input distribution P over t0,1um, Fw,d,α ϵ-SM approximates G with respect"
SM APPROXIMATION OF BOOLEAN CIRCUITS WITH FEEDFORWARD NETS,0.11708253358925144,"to ℓ0-1,P with sample complexity polypsqrO
´
logpwdq"
SM APPROXIMATION OF BOOLEAN CIRCUITS WITH FEEDFORWARD NETS,0.11900191938579655,"ϵ2
¯
."
SM APPROXIMATION OF BOOLEAN CIRCUITS WITH FEEDFORWARD NETS,0.12092130518234165,Under review as a conference paper at ICLR 2022
SM APPROXIMATION OF BOOLEAN CIRCUITS WITH FEEDFORWARD NETS,0.12284069097888675,"We note that the bound in Theorem 3.1 only scales logarithmically in the width w of the network, even
if w is arbitrarily greater than the circuit width r. This ensures that even heavily overparameterized nets
will have low sample complexity of the approximation."
SM APPROXIMATION OF BOOLEAN CIRCUITS WITH FEEDFORWARD NETS,0.12476007677543186,"For this setting, the all-layer margin loss in (2.2) is essential for proving tight sample complexity bounds, as
other surrogate lossessℓwould give weaker results. For example, if we choose ℓ0-1 as the surrogate loss, VC-
dimension bounds (Harvey et al., 2017) imply that Fw,d,α statistically meaningfully approximates Gq,r,s
with sample complexity scaling in polypwqq under the conditions of Theorem 3.1. This suffers a polynomial
dependence on the overparameterized width w, which is not ideal for realistic settings, where neural nets
are often wider than necessary to facilitate optimization. In contrast, our dependence on w is logarithmic.
Another possible surrogate loss is the output margin-based ramp loss, which can be used to prove norm-
based sample complexities (Bartlett et al., 2017). However, these bounds depend on śd
i“1}Wi}op (or
related quantities), which would be exponentially large in d for the naive construction in Section 3.1."
SM APPROXIMATION OF BOOLEAN CIRCUITS WITH FEEDFORWARD NETS,0.12667946257197696,"3.1
PROOF SKETCH FOR THEOREM 3.1"
SM APPROXIMATION OF BOOLEAN CIRCUITS WITH FEEDFORWARD NETS,0.12859884836852206,"There are two key steps in the proof. First, given any layered circuit G P G, we construct a neural net
that directly simulates G by computing the layers of G one-by-one, which is simple to do by directly
constructing ReLU and linear layers to simulate the AND, OR, NOT, and ID gates.
Lemma 3.2. In the setting of Theorem 3.1, let G denote the layered boolean circuit, which we aim to
compute using a neural net. Let gi :t0,1uri´1 Ñt0,1uri denote function computed between the i´1-th
and i-th layers of G, which we assume have ri´1 and ri gates, respectively, so G“gq´1˝¨¨¨˝g1. Then
there exist functions f1,...,fq´1, where each fi is computed by a feedforward ReLU net with two linear
and activation layers, such that for all iPrq´1s and xPt0,1um"
SM APPROXIMATION OF BOOLEAN CIRCUITS WITH FEEDFORWARD NETS,0.13051823416506717,fi˝¨¨¨˝f1pxq“gi˝¨¨¨˝g1pxq
SM APPROXIMATION OF BOOLEAN CIRCUITS WITH FEEDFORWARD NETS,0.1324376199616123,"Thus, the composition Fp¨,θq ﬁfq´1˝¨¨¨˝f1 satisfies Fpx,θq “ Gpxq for all x P t0,1um. Note that we
omitted the dependency of fq´1,...,f1 on parameters θ for simplicity."
SM APPROXIMATION OF BOOLEAN CIRCUITS WITH FEEDFORWARD NETS,0.1343570057581574,"Lower bounding all-layer margin. The next step for proving SM approximation is to construct a loss
sℓso that the empirical risk minimizer of sℓon the training data has good sample complexity. This crucially
requires the all-layer margin tool developed in Section 2.1, as other complexity measures (e.g. norm-based)
would not give good sample complexity bounds."
SM APPROXIMATION OF BOOLEAN CIRCUITS WITH FEEDFORWARD NETS,0.1362763915547025,"Recall that the all-layer margin ρFpθ,x,Gpxqq measures the stability of the output Fpx,θq to perturbations
in to θ, and, by Lemma 2.3, it suffices to show that F has large all-layer margin on x P t0,1um.
Unfortunately, we cannot guarantee that the naive construction from Lemma 3.2 has large all-layer margin
without further modifications. To remedy this issue, Theorem D.6 introduces a generic way to convert the
model Fp¨,θq, with possibly small all-layer margin on xPt0,1um, into a new architecture and parameter
set F 1p¨,θ1q, with provably large all-layer margin on xPt0,1um, such that F 1px,θ1q“Fpx,θq on all inputs
xPt0,1um. The construction relies on introducing new layers to F to obtain F 1 and increases the total
number of layers by only a constant factor. This step of the proof is formally stated in the following lemma.
Lemma 3.3. In the setting of Lemma 3.2, let Fp¨,θq“fq´1˝¨¨¨˝f1 be the neural net with parameters θ
constructed to compute the circuit G. There exist “correction functions” ζ1,...,ζq´2, where ζi is computed
by a neural net with two activation and linear layers, such that the composition"
SM APPROXIMATION OF BOOLEAN CIRCUITS WITH FEEDFORWARD NETS,0.1381957773512476,"F 1p¨,θ1qﬁfq´1˝ζq´2˝fq´2˝¨¨¨˝ζ1˝f1"
SM APPROXIMATION OF BOOLEAN CIRCUITS WITH FEEDFORWARD NETS,0.1401151631477927,"has large all-layer margin. Here θ1 denotes the collection of all parameters. Concretely, ρF 1pθ1,x,Gpxqqě
1
polypsq for all xPt0,1um. Note that we omitted the dependency of fi,ζi on parameters θ1 for simplicity."
SM APPROXIMATION OF BOOLEAN CIRCUITS WITH FEEDFORWARD NETS,0.1420345489443378,"We convey the core intuitions for Lemma 3.3 in a simplified toy setting as follows. Consider the case
where we start with an initial architecture f computing fpx,pW1,...,Wdqq“
´śd
i“1Wi
¯
x´0.5, where
Wi PR. In this simplified setting, we consider Wi “1 @i. For input x“1 and target y“1, the all-layer
margin is small: ρfpp1,...,1q,1,1qÀ 1
?"
SM APPROXIMATION OF BOOLEAN CIRCUITS WITH FEEDFORWARD NETS,0.14395393474088292,"d, where the architecture is in the subscript. Indeed, choosing δi“ 3"
SM APPROXIMATION OF BOOLEAN CIRCUITS WITH FEEDFORWARD NETS,0.14587332053742802,"d,
we have fp1,p1´ 3"
SM APPROXIMATION OF BOOLEAN CIRCUITS WITH FEEDFORWARD NETS,0.14779270633397312,"d,...,1´ 3"
SM APPROXIMATION OF BOOLEAN CIRCUITS WITH FEEDFORWARD NETS,0.14971209213051823,dqq“p1´ 3
SM APPROXIMATION OF BOOLEAN CIRCUITS WITH FEEDFORWARD NETS,0.15163147792706333,"dqd´0.5«expp´3q´0.5ă0. Thus, by the definition of all-layer
margin, ρfpp1,...,1q,1,1qď
ař"
SM APPROXIMATION OF BOOLEAN CIRCUITS WITH FEEDFORWARD NETS,0.15355086372360843,"iδ2
i À 1
? d."
SM APPROXIMATION OF BOOLEAN CIRCUITS WITH FEEDFORWARD NETS,0.15547024952015356,"Now we will insert ReLU layers in f to increase the all-layer margin to Ωp1q. We use ReLU layers to
implement the round function, which has the key property that roundpzq“1 @zě2{3."
SM APPROXIMATION OF BOOLEAN CIRCUITS WITH FEEDFORWARD NETS,0.15738963531669867,Under review as a conference paper at ICLR 2022
SM APPROXIMATION OF BOOLEAN CIRCUITS WITH FEEDFORWARD NETS,0.15930902111324377,"Proposition 3.4. For any z PR, we can implement the function roundpzq“ $
& %"
SM APPROXIMATION OF BOOLEAN CIRCUITS WITH FEEDFORWARD NETS,0.16122840690978887,"0
if ză1{3
3x´1
if 1{3ďză2{3
1
if zě2{3
via a feedforward ReLU net, as follows: roundpzq“3φpz´1{3q´3φpz´2{3q."
SM APPROXIMATION OF BOOLEAN CIRCUITS WITH FEEDFORWARD NETS,0.16314779270633398,"We consider the following function rf, which inserts round between every layer in f:"
SM APPROXIMATION OF BOOLEAN CIRCUITS WITH FEEDFORWARD NETS,0.16506717850287908,"rfpx,pW1,...,Wdqq“roundpWdroundpWd´1¨¨¨roundpW1xq¨¨¨qq´0.5
(3.1)"
SM APPROXIMATION OF BOOLEAN CIRCUITS WITH FEEDFORWARD NETS,0.16698656429942418,"For this demonstration, we ignore the parameters of round, though the actual proof considers these param-
eters. The following claim shows that (3.1) preserves the output of f while increasing the all-layer margin:"
SM APPROXIMATION OF BOOLEAN CIRCUITS WITH FEEDFORWARD NETS,0.1689059500959693,"Claim 3.5. In the setting above, it holds that rfp1,p1,...,1qq“fp1,p1,...,1qq and ρrfpp1,...,1q,1,1qě 1 3."
SM APPROXIMATION OF BOOLEAN CIRCUITS WITH FEEDFORWARD NETS,0.1708253358925144,"This reflects a significant increase in the all-layer margin, while only increasing depth by a constant
factor. The proof is simple: we observe that if δiď 1"
SM APPROXIMATION OF BOOLEAN CIRCUITS WITH FEEDFORWARD NETS,0.1727447216890595,"3 for all i, the function output will not change because
roundpzq“1 @zě 2"
SM APPROXIMATION OF BOOLEAN CIRCUITS WITH FEEDFORWARD NETS,0.1746641074856046,3. This immediately gives the all-layer margin lower bound 1 3.
SM APPROXIMATION OF BOOLEAN CIRCUITS WITH FEEDFORWARD NETS,0.1765834932821497,"To apply this construction more generally, we note that round corrects errors in previous layers. In
the more general setting, we insert “correction functions” ζ between each layer satisfying the key
property that ζph1q “ h if h is the intended output of the layer and h1 is any perturbed value satisfying
}h1´h}2ď 1"
SM APPROXIMATION OF BOOLEAN CIRCUITS WITH FEEDFORWARD NETS,0.1785028790786948,"3. Since intended outputs of layers in the function constructed by Lemma 3.2 are binary-valued
in t0,1uw because F simulates a boolean circuit, we can simply apply the function round constructed
in Proposition 3.4 elementwise as the correction function. By the construction, this can be implemented
by adding two additional feedforward ReLU layers per correction function. Following the intuition
for Claim 3.5, we prove that inserting these correction functions guarantees a large all-layer margin
(Theorem D.6) on all xPt0,1um. This leads to the proof of Lemma 3.3. We can complete the proof of
Theorem 3.1 by invoking Lemma 2.3, as shown in Section B."
SM APPROXIMATION OF TURING MACHINES WITH TRANSFORMERS,0.18042226487523993,"4
SM APPROXIMATION OF TURING MACHINES WITH TRANSFORMERS"
SM APPROXIMATION OF TURING MACHINES WITH TRANSFORMERS,0.18234165067178504,"In this section, we show that transformers SM approximate Turing machines with computation time
bounded by T, using sample complexity polynomial in logT and the state space and alphabet sizes of the
Turing machine. Constructions from prior work would require the sample complexity of the approximation
to be linear in T (Siegelmann & Sontag, 1995; Chen et al., 2018; Pérez et al., 2019; Bhattamishra et al.,
2020). Thus, we obtain an exponential improvement in the dependency on T."
SM APPROXIMATION OF TURING MACHINES WITH TRANSFORMERS,0.18426103646833014,"We briefly describe a Turing machine; see (Sipser, 2013) for a more thorough survey. A Turing machine
is a model for computation specified by a tuple pZ,A,S,Ztermq containing a set of states Z, a tape alphabet
A, a transition function S:ZˆAÑZˆAˆt´1,`1u, and set of terminal states Zterm indicating accept or
reject. For simplicity, we assume the Turing machine has a single tape, as any single-tape Turing machine
can simulate a multi-tape one with only quadratic increase in runtime (Sipser, 2013). Given an input
xPA˚ recorded on the left-most part of the tape, the Turing machine performs computation in a sequence
of timesteps. In each timestep, the machine determines the next state, symbol to write, and direction to
move the head via the transition function."
SM APPROXIMATION OF TURING MACHINES WITH TRANSFORMERS,0.18618042226487524,"We let TMpZ,A,S,Ztermq denote the function computed by the Turing machine, which produces an output
in t0,1u (if the machine halts). Fixing the alphabet A, we consider the class of binary functions computed
by Turing machines with at most k states terminating in T steps:"
SM APPROXIMATION OF TURING MACHINES WITH TRANSFORMERS,0.18809980806142035,"Gk,T ﬁtxÞÑTMpZ,A,S,Ztermqpxq:|Z|ďk, and @xPX,TMpZ,A,S,Ztermq terminates in T steps u
(4.1)"
TRANSFORMER ARCHITECTURE FOR SM-APPROXIMATING TURING MACHINES,0.19001919385796545,"4.1
TRANSFORMER ARCHITECTURE FOR SM-APPROXIMATING TURING MACHINES"
TRANSFORMER ARCHITECTURE FOR SM-APPROXIMATING TURING MACHINES,0.19193857965451055,"We study approximation of G with a family of architectures consisting of both an encoder and decoder com-
ponent (Vaswani et al., 2017), described as follows. The encoder architecture is simple and only performs
an embedding of the input symbols, using learnable symbol embeddings EPRwˆ|A| and fixed positional
encodings βp1q,βp2q,...PRw. Given input xPA˚ with m symbols, the encoder produces m output vectors
in Rw via Encipx,Eq“E:,xi`βpiq, where Enci denotes the output of the encoder at the i-th position."
TRANSFORMER ARCHITECTURE FOR SM-APPROXIMATING TURING MACHINES,0.19385796545105566,"The decoder iteratively computes an output, running for T steps. We define a transformer layer of the
decoder as a sequence of modules consisting of decoder self-attention, followed by encoder-decoder
attention, followed by three feedforward ReLU layers."
TRANSFORMER ARCHITECTURE FOR SM-APPROXIMATING TURING MACHINES,0.19577735124760076,Under review as a conference paper at ICLR 2022
TRANSFORMER ARCHITECTURE FOR SM-APPROXIMATING TURING MACHINES,0.19769673704414586,"Attention layers. Attention layers consist of key, value, and query functions K,V,Q, each of which com-
putes a linear transformation. We omit parameters here for simplicity. Restricted to a single decoder timestep,
the attention layer takes two types of inputs: a sequence of previously-computed representations h1,...,hi,
and a current input representation h1. The layer first applies the key, value, and query functions as follows:"
TRANSFORMER ARCHITECTURE FOR SM-APPROXIMATING TURING MACHINES,0.19961612284069097,"τ0,τ1,...,τi“Qph1qJK0,Qph1qJKph1q,...,Qph1qJKphiq
v0,v1,...,vi“V0,V ph1q,...,V phiq"
TRANSFORMER ARCHITECTURE FOR SM-APPROXIMATING TURING MACHINES,0.20153550863723607,"where K0 and V0 are fixed “null” key and value vectors which are learned parameters of the layer. Letting
J denote the set of indices tj:τj “maxtτ0,...,τiuu, the attention layer performs hard-max attention (Pérez
et al., 2019) to compute the output, as follows:"
TRANSFORMER ARCHITECTURE FOR SM-APPROXIMATING TURING MACHINES,0.2034548944337812,"Attnph1,ph1,...,hiqq“h1` 1 |J | ÿ"
TRANSFORMER ARCHITECTURE FOR SM-APPROXIMATING TURING MACHINES,0.2053742802303263,"jPJ
vj"
TRANSFORMER ARCHITECTURE FOR SM-APPROXIMATING TURING MACHINES,0.2072936660268714,"Our theory also applies to the standard softmax attention used in practice, but we focus on the hard-max
case for a simpler proof. Let hpjq
t
denote the representation computed by the j-th layer of the decoder at
timestep t. At timestep i, decoder self-attention at the pj`1q-th layer computes Attnphpjq
i ,phpjq
1 ,...,hpjq
i qq.
Letting e1,...,em denote the encoder outputs, encoder-decoder self-attention at the pj`1q-th layer and
i-th step would compute Attnphpjq
i ,pe1,...,emqq."
TRANSFORMER ARCHITECTURE FOR SM-APPROXIMATING TURING MACHINES,0.2092130518234165,"Transformer layers. We use feedforward layers which apply 3 standard ReLU layers, as follows:
FFphq“φpW3φpW2φpW1h`b1q`b2q`b3q. Our theory also allows for residual feedforward layers, and
the architecture here is chosen mainly to simplify the construction."
TRANSFORMER ARCHITECTURE FOR SM-APPROXIMATING TURING MACHINES,0.21113243761996162,"A transformer layer applies these constructions in sequence. Letting Hpjq
i
“phpjq
1 ,...,hpjq
i q denote the output
after the j-th transformer layer for timesteps 1ďtďi, and θpjq the parameters of the layer, we compute"
TRANSFORMER ARCHITECTURE FOR SM-APPROXIMATING TURING MACHINES,0.21305182341650672,"hpj`1,decq
i
“Attnphpjq
i ,Hpjq
i ,θ(j + 1, dec-attn)q"
TRANSFORMER ARCHITECTURE FOR SM-APPROXIMATING TURING MACHINES,0.21497120921305182,"hpj`1,encq
i
“Attnphpj`1,decq
i
,pe1,...,emq,θ(j + 1, enc-attn)q"
TRANSFORMER ARCHITECTURE FOR SM-APPROXIMATING TURING MACHINES,0.21689059500959693,"Trphpjq
i ,Hpjq
i ,pe1,...,emq,θpj`1qq“FFphpj`1,encq,θpj + 1, ffqq"
TRANSFORMER ARCHITECTURE FOR SM-APPROXIMATING TURING MACHINES,0.21880998080614203,"Note that we included the explicit dependence of the attention layers on the parameters for completeness.
We now set hpj`1q
i
“Trphpjq
i ,Hpjq
i ,pe1,...,emq,θpj`1qq."
TRANSFORMER ARCHITECTURE FOR SM-APPROXIMATING TURING MACHINES,0.22072936660268713,"Decoder outputs. We consider d-layer decoders, so oi ﬁhpdq
i
denotes the output of the decoder at time
i, which is also inputted to the decoder at time i`1 as follows: hp0q
i`1“hpdq
i
`βpi`1q. The initial decoder"
TRANSFORMER ARCHITECTURE FOR SM-APPROXIMATING TURING MACHINES,0.22264875239923224,"input hp0q
0
is a trainable parameter. The decoder runs for a fixed number of timesteps T 1 and produces the
prediction θJ
clshpdq
T 1 . For simplicity, we assume T 1“T, the computation time of the Turing machine family."
TRANSFORMER ARCHITECTURE FOR SM-APPROXIMATING TURING MACHINES,0.22456813819577734,"Note that our architecture allows long (length T) decoding sequences, whereas typical architectures in
practice use decoding sequences with roughly the same length as the input (Vaswani et al., 2017). The
architecture we study is similar to ones studied by (Pérez et al., 2019; Bhattamishra et al., 2020)."
TRANSFORMER ARCHITECTURE FOR SM-APPROXIMATING TURING MACHINES,0.22648752399232247,"We use xÞÑFw,d,Tpx,θq to denote the described transformer architecture with parameters θ, w-dimensional
hidden layers, d transformer layers in the decoder, and T decoder steps. This leads to the following class
of transformer functions: Fw,d,α,T “ tx ÞÑ Fw,d,Tpx,θq : }θ}1 ď αu. The following theorem states that
this class of transformers SM approximates the Turing machine family G defined in (4.1) with sample
complexity polynomial in logT, k and |A|.
Theorem 4.1. In the setting above, consider the class G of functions computed by Turing machines with
at most k states, alphabet A, and computation time bounded by T steps for inputs xPX. Suppose that
wÁk|A|`logT, d—logT, and α“polypk,|A|,logTq."
TRANSFORMER ARCHITECTURE FOR SM-APPROXIMATING TURING MACHINES,0.22840690978886757,"Then for all ϵą0 and any input distribution P over X, Fw,d,α,T ϵ-SM approximates G with respect to"
TRANSFORMER ARCHITECTURE FOR SM-APPROXIMATING TURING MACHINES,0.23032629558541268,"ℓ0-1,P with sample complexity polypk,|A|,logTqrO
´
logpwdq"
TRANSFORMER ARCHITECTURE FOR SM-APPROXIMATING TURING MACHINES,0.23224568138195778,"ϵ2
¯
."
TRANSFORMER ARCHITECTURE FOR SM-APPROXIMATING TURING MACHINES,0.23416506717850288,"As with Section 3, we set the surrogate losssℓin Definition 2.1 to be the all-layer margin loss defined in
Section 2.1. Commonly-used alternatives for the surrogate loss would not suffice for either our construction
or ones in prior work (Siegelmann & Sontag, 1995; Chen et al., 2018; Pérez et al., 2019; Bhattamishra et al.,
2020). First, the VC dimension of Fw,d,α,T is at least ΩpwTq. This is because transformer architectures"
TRANSFORMER ARCHITECTURE FOR SM-APPROXIMATING TURING MACHINES,0.236084452975048,Under review as a conference paper at ICLR 2022
TRANSFORMER ARCHITECTURE FOR SM-APPROXIMATING TURING MACHINES,0.2380038387715931,"which contain a decoder component can express RNNs, which by lower bounds have VC dimension at least
wT (Koiran & Sontag, 1998). This indicates that using ℓ0-1 as the surrogate loss would lead to sample com-
plexities that are suboptimal in both the overparameterized width w and the computation T. Second, the cor-
rect norm-based Rademacher complexity bound to use for transformers is unclear; however, the RNN-based
equivalent would scale with the T-th power of some parameter norm, or exponentially in T. Thus, as in
Section 3, the all-layer margin surrogate loss (2.2) is essential for obtaining our sample complexity bounds."
TRANSFORMER ARCHITECTURE FOR SM-APPROXIMATING TURING MACHINES,0.2399232245681382,"4.2
PROOF SKETCH FOR THEOREM 4.1"
TRANSFORMER ARCHITECTURE FOR SM-APPROXIMATING TURING MACHINES,0.2418426103646833,"Following Lemma 2.3, our goal is to construct a transformer which can simulate Turing machines with
large all-layer margin, namely, Ω
´
1
polypk,|A|,logTq
¯
. The fundamental limitation of prior work (Pérez et al.,
2019) towards attaining this is that the positional embeddings are required to store values as small as
1
polypTq. Our construction cannot afford to rely on values this small – informally, if the construction relies
on the exact values of these small entries, then the all layer margin would be at most
1
polypTq because
perturbing the layer by the small entries could change the prediction. Instead, we propose using Binpiq,
the binary encoding of i in rlogTs bits, as the positional encoding for timestep i. This allows us to use
unique positional encodings for each timestep which do not rely on arbitrary precision."
TRANSFORMER ARCHITECTURE FOR SM-APPROXIMATING TURING MACHINES,0.2437619961612284,"We describe the construction of the transformers. Fix a Turing machine GPG. We first require notation to
describe the computation of G. For input xPX, we define zipxq, aipxq to be the Turing machine state and
symbol under the tape head at the conclusion of step i. We let lipxq denote the location of the Turing machine
head at the conclusion of step i. During the timestep, the Turing machine computes Spzi´1pxq,ai´1pxqq,
writes a new symbol under the head at location li´1pxq, and moves the head either left or right. Let uipxq
denote the symbol written during timestep i, and qipxqPtleft,rightu the movement direction of the head."
TRANSFORMER ARCHITECTURE FOR SM-APPROXIMATING TURING MACHINES,0.2456813819577735,"Following (Pérez et al., 2019) with several key modifications, we simulate the Turing machine using
the transformer as follows. Each timestep will maintain the invariance that oi contains an encoding of
zipxq,aipxq, and lipxq. Given that this invariance holds until timestep i, the transformer simulates timestep
i`1 of the Turing machine with the following steps:"
TRANSFORMER ARCHITECTURE FOR SM-APPROXIMATING TURING MACHINES,0.2476007677543186,"1) Use feedforward layers to apply transition S on zipxq and aipxq, which can be read from oi,
to obtain zi`1pxq, ui`1pxq, and movement direction qi`1pxqPtleft, rightu.
2) Using feedforward layers, compute li`1pxq from qi`1pxq and the encoding of lipxq in oi.
3) Compute ai`1pxq. We use decoder self-attention to search over past timesteps which wrote to
li`1pxq. Our aim is to find ui1pxq, where i1“maxtjďi`1:lj´1pxq“li`1pxqu. We implement
a binary search over past timesteps j, which is needed to find the largest j ď i ` 1 where
lj´1pxq“li`1pxq. The binary search can be implemented with OprlogTsq decoder self-attention
layers, and the construction ensures large all-layer margin.
4) If no such i1 from the previous timestep existed, we check whether li`1pxq contained an input
symbol using encoder-decoder attention and copy this input symbol if so.
5) If no symbols were found in 3) or 4), li`1pxq must contain the blank symbol (meaning it wasn’t
visited yet by the head). Thus, we have computed ai`1pxq, so we have all the information needed
to compute the new embedding oi`1."
TRANSFORMER ARCHITECTURE FOR SM-APPROXIMATING TURING MACHINES,0.2495201535508637,"To lower bound the all-layer margin of the constructed transformer, we use Theorem D.6, which requires
existence of a “correction function” which can correct outputs in previous layers. Since we construct a
network with intermediate layer entries in t0,1u, we can use the same correction function as Section 3.1,
which rounds to the nearest bit. The full proof is provided in Section C."
CONCLUSION,0.2514395393474088,"5
CONCLUSION"
CONCLUSION,0.2533589251439539,"This work proposes a new definition of approximation, statistically meaningful approximation, which
ensures that the approximating family not only has sufficient expressivity, but also exhibits good statistical
learnability. Towards a first analysis with this definition, we show approximability of two function classes:
boolean circuits and Turing machines, with strong sample complexity guarantees depending only on the
intrinsic properties of these function classes. There are several interesting directions to extend our study
of statistically meaningful approximation. Examples include proving more upper and lower bounds for
statistically meaningful approximation for different target functions and neural net architectures, and using
our definition as a lens to compare architectures."
CONCLUSION,0.255278310940499,Under review as a conference paper at ICLR 2022
ETHICS AND REPRODUCIBILITY STATEMENTS,0.2571976967370441,"6
ETHICS AND REPRODUCIBILITY STATEMENTS"
ETHICS AND REPRODUCIBILITY STATEMENTS,0.2591170825335892,"An ethics statement is not applicable for this work – this work is mainly theoretical and is several layers
removed from empirical applications."
ETHICS AND REPRODUCIBILITY STATEMENTS,0.26103646833013433,"Section A contains the proofs for Section 2. Section B contains the formal construction and proof for
boolean circuits. Section C contains the formal construction and proof for Turing machines. Section D
rigorously introduces the correction function machinery and lower bounds the all-layer margin in terms
of properties of the correction functions."
REFERENCES,0.2629558541266795,REFERENCES
REFERENCES,0.2648752399232246,"Noga Alon, Yossi Matias, and Mario Szegedy. The space complexity of approximating the frequency
moments. Journal of Computer and system sciences, 58(1):137–147, 1999."
REFERENCES,0.2667946257197697,"Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee. Understanding deep neural
networks with rectified linear units. arXiv preprint arXiv:1611.01491, 2016."
REFERENCES,0.2687140115163148,"Chenglong Bao, Qianxiao Li, Zuowei Shen, Cheng Tai, Lei Wu, and Xueshuang Xiang. Approximation
analysis of convolutional neural networks. work, 65, 2014."
REFERENCES,0.2706333973128599,"Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE
Transactions on Information theory, 39(3):930–945, 1993."
REFERENCES,0.272552783109405,"Peter Bartlett, Dylan J Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural
networks. arXiv preprint arXiv:1706.08498, 2017."
REFERENCES,0.2744721689059501,"Peter L Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight vc-dimension and
pseudodimension bounds for piecewise linear neural networks. The Journal of Machine Learning
Research, 20(1):2285–2301, 2019."
REFERENCES,0.2763915547024952,"Yoshua Bengio and Olivier Delalleau. On the expressive power of deep architectures. In International
conference on algorithmic learning theory, pp. 18–36. Springer, 2011."
REFERENCES,0.2783109404990403,"Satwik Bhattamishra, Arkil Patel, and Navin Goyal.
On the computational power of trans-
formers and its implications in sequence modeling.
In Proceedings of the 24th Con-
ference on Computational Natural Language Learning,
pp. 455–475,
Online,
November
2020. Association for Computational Linguistics.
doi: 10.18653/v1/2020.conll-1.37.
URL
https://www.aclweb.org/anthology/2020.conll-1.37."
REFERENCES,0.2802303262955854,"Vaggos Chatziafratis, Sai Ganesh Nagarajan, Ioannis Panageas, and Xiao Wang. Depth-width trade-offs
for relu networks via sharkovsky’s theorem. arXiv preprint arXiv:1912.04378, 2019."
REFERENCES,0.2821497120921305,"Vaggos Chatziafratis, Sai Ganesh Nagarajan, and Ioannis Panageas. Better depth-width trade-offs for
neural networks through the lens of dynamical systems. In International Conference on Machine
Learning, pp. 1469–1478. PMLR, 2020."
REFERENCES,0.2840690978886756,"Yining Chen, Sorcha Gilroy, A. Maletti, Jonathan May, and Kevin Knight. Recurrent neural networks
as weighted language recognizers. In NAACL-HLT, 2018."
REFERENCES,0.28598848368522073,"Nadav Cohen and Amnon Shashua. Convolutional rectifier networks as generalized tensor decompositions.
In International Conference on Machine Learning, pp. 955–963. PMLR, 2016."
REFERENCES,0.28790786948176583,"Nadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A tensor
analysis. In Conference on learning theory, pp. 698–728. PMLR, 2016."
REFERENCES,0.28982725527831094,"George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control,
signals and systems, 2(4):303–314, 1989."
REFERENCES,0.29174664107485604,"Amit Daniely.
Depth separation for neural networks.
In Satyen Kale and Ohad Shamir
(eds.), Proceedings of the 2017 Conference on Learning Theory,
volume 65 of Pro-
ceedings of Machine Learning Research, pp. 690–696. PMLR, 07–10 Jul 2017.
URL
http://proceedings.mlr.press/v65/daniely17a.html."
REFERENCES,0.29366602687140114,"Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In Conference
on learning theory, pp. 907–940. PMLR, 2016."
REFERENCES,0.29558541266794625,Under review as a conference paper at ICLR 2022
REFERENCES,0.29750479846449135,"Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of neural
networks. In Conference On Learning Theory, pp. 297–299. PMLR, 2018."
REFERENCES,0.29942418426103645,"Moritz Hardt and Tengyu Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231, 2016."
REFERENCES,0.30134357005758156,"Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight vc-dimension bounds for piecewise
linear neural networks. In Conference on Learning Theory, pp. 1064–1068. PMLR, 2017."
REFERENCES,0.30326295585412666,"John Hewitt, Michael Hahn, Surya Ganguli, Percy Liang, and Christopher D Manning. Rnns can generate
bounded hierarchical languages with optimal memory. arXiv preprint arXiv:2010.07515, 2020."
REFERENCES,0.30518234165067176,"Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal
approximators. Neural networks, 2(5):359–366, 1989."
REFERENCES,0.30710172744721687,"Ziwei Ji, Matus Telgarsky, and Ruicheng Xian. Neural tangent kernels, transportation mappings, and
universal approximation. In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=HklQYxBKwS."
REFERENCES,0.30902111324376197,"Pascal Koiran and Eduardo D Sontag. Vapnik-chervonenkis dimension of recurrent neural networks.
Discrete Applied Mathematics, 86(1):63–79, 1998."
REFERENCES,0.31094049904030713,"Samuel A Korsky and Robert C Berwick.
On the computational power of rnns.
arXiv preprint
arXiv:1906.06349, 2019."
REFERENCES,0.31285988483685223,"Holden Lee, Rong Ge, Tengyu Ma, Andrej Risteski, and Sanjeev Arora. On the ability of neural nets
to express distributions. In Satyen Kale and Ohad Shamir (eds.), Proceedings of the 2017 Conference
on Learning Theory, volume 65 of Proceedings of Machine Learning Research, pp. 1271–1296. PMLR,
07–10 Jul 2017. URL http://proceedings.mlr.press/v65/lee17a.html."
REFERENCES,0.31477927063339733,"Moshe Leshno, Vladimir Ya Lin, Allan Pinkus, and Shimon Schocken. Multilayer feedforward networks
with a nonpolynomial activation function can approximate any function. Neural networks, 6(6):
861–867, 1993."
REFERENCES,0.31669865642994244,"Hongzhou Lin and Stefanie Jegelka. Resnet with one-neuron hidden layers is a universal approximator.
arXiv preprint arXiv:1806.10909, 2018."
REFERENCES,0.31861804222648754,"Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir. The connection between approx-
imation, depth separation and learnability in neural networks. arXiv preprint arXiv:2102.00434, 2021."
REFERENCES,0.32053742802303264,"Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-layers
neural networks. Proceedings of the National Academy of Sciences, pp. E7665–E7671, 2018."
REFERENCES,0.32245681381957775,"Vaishnavh Nagarajan and J Zico Kolter. Deterministic pac-bayesian generalization bounds for deep
networks via generalizing noise-resilience. arXiv preprint arXiv:1905.13344, 2019."
REFERENCES,0.32437619961612285,"Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural networks.
In Conference on Learning Theory, pp. 1376–1401. PMLR, 2015."
REFERENCES,0.32629558541266795,"Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro.
A pac-bayesian approach to
spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564, 2017."
REFERENCES,0.32821497120921306,"Quynh Nguyen and Matthias Hein. Optimization landscape and expressivity of deep cnns. In International
conference on machine learning, pp. 3730–3739. PMLR, 2018."
REFERENCES,0.33013435700575816,"Jooyoung Park and Irwin W Sandberg. Universal approximation using radial-basis-function networks.
Neural computation, 3(2):246–257, 1991."
REFERENCES,0.33205374280230326,"Sejun Park, Jaeho Lee, Chulhee Yun, and Jinwoo Shin. Provable memorization via deep neural networks
using sub-linear parameters. arXiv preprint arXiv:2010.13363, 2020."
REFERENCES,0.33397312859884837,"Jorge Pérez, Javier Marinkovi´c, and Pablo Barceló. On the turing completeness of modern neural network
architectures. arXiv preprint arXiv:1901.03429, 2019."
REFERENCES,0.33589251439539347,"Akiyoshi Sannai, Yuuki Takai, and Matthieu Cordonnier. Universal approximations of permutation
invariant/equivariant functions by deep neural networks. arXiv preprint arXiv:1903.01939, 2019."
REFERENCES,0.3378119001919386,J. Savage. Models of computation - exploring the power of computing. 1998.
REFERENCES,0.3397312859884837,Under review as a conference paper at ICLR 2022
REFERENCES,0.3416506717850288,"Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.
Computational capabilities of graph neural networks. IEEE Transactions on Neural Networks, 20(1):
81–102, 2008."
REFERENCES,0.3435700575815739,"Anton Maximilian Schäfer and Hans-Georg Zimmermann. Recurrent neural networks are universal
approximators. International journal of neural systems, 17(04):253–263, 2007."
REFERENCES,0.345489443378119,"Hava T Siegelmann and Eduardo D Sontag. On the computational power of neural nets. Journal of
computer and system sciences, 50(1):132–150, 1995."
REFERENCES,0.3474088291746641,"Michael Sipser. Introduction to the Theory of Computation. Course Technology, Boston, MA, third
edition, 2013. ISBN 113318779X."
REFERENCES,0.3493282149712092,"Matus Telgarsky. benefits of depth in neural networks. In Vitaly Feldman, Alexander Rakhlin, and Ohad
Shamir (eds.), 29th Annual Conference on Learning Theory, volume 49 of Proceedings of Machine
Learning Research, pp. 1517–1539, Columbia University, New York, New York, USA, 23–26 Jun 2016.
PMLR. URL http://proceedings.mlr.press/v49/telgarsky16.html."
REFERENCES,0.3512476007677543,"Takeshi Teshima, Koichi Tojo, Masahiro Ikeda, Isao Ishikawa, and Kenta Oono. Universal approximation
property of neural ordinary differential equations. arXiv preprint arXiv:2012.02414, 2020."
REFERENCES,0.3531669865642994,"Gal Vardi and Ohad Shamir.
Neural networks with small weights and depth-separation barri-
ers.
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances
in Neural Information Processing Systems,
volume 33,
pp. 19433–19442. Curran Asso-
ciates, Inc., 2020.
URL https://proceedings.neurips.cc/paper/2020/file/
e1fe6165cad3f7f3f57d409f78e4415f-Paper.pdf."
REFERENCES,0.3550863723608445,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017."
REFERENCES,0.3570057581573896,"Roman Vershynin. Memory capacity of neural networks with threshold and relu activations. arXiv preprint
arXiv:2001.06938, 2020."
REFERENCES,0.35892514395393477,"Colin Wei and Tengyu Ma. Data-dependent sample complexity of deep neural networks via lipschitz
augmentation. arXiv preprint arXiv:1905.03684, 2019a."
REFERENCES,0.36084452975047987,"Colin Wei and Tengyu Ma. Improved sample complexities for deep networks and robust classification
via an all-layer margin. arXiv preprint arXiv:1910.04284, 2019b."
REFERENCES,0.362763915547025,"Gail Weiss, Yoav Goldberg, and Eran Yahav. On the practical computational power of finite precision
rnns for language recognition. arXiv preprint arXiv:1805.04908, 2018."
REFERENCES,0.3646833013435701,"Dmitry Yarotsky.
Universal approximations of invariant maps by neural networks.
Constructive
Approximation, pp. 1–68, 2021."
REFERENCES,0.3666026871401152,"Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Small relu networks are powerful memorizers: a tight analysis
of memorization capacity. arXiv preprint arXiv:1810.07770, 2018."
REFERENCES,0.3685220729366603,"Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar. Are transform-
ers universal approximators of sequence-to-sequence functions? arXiv preprint arXiv:1912.10077, 2019."
REFERENCES,0.3704414587332054,"Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep
learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016."
REFERENCES,0.3723608445297505,"Han Zhang, Xi Gao, Jacob Unterman, and Tom Arodz. Approximation capabilities of neural odes and
invertible residual networks. In International Conference on Machine Learning, pp. 11086–11095.
PMLR, 2020."
REFERENCES,0.3742802303262956,"Ding-Xuan Zhou. Universality of deep convolutional neural networks. Applied and computational
harmonic analysis, 48(2):787–794, 2020."
REFERENCES,0.3761996161228407,Under review as a conference paper at ICLR 2022
REFERENCES,0.3781190019193858,"A
PROOFS FOR SECTION 2"
REFERENCES,0.3800383877159309,We prove Proposition 2.2 and Lemma 2.3.
REFERENCES,0.381957773512476,"Proof of Proposition 2.2. Let pxiqn
i“1 denote a n i.i.d. training examples drawn from P and fix GPG. De-
fine LpFqﬁEx„PrsℓpF,x,Gpxqqs and pLpFqﬁ1"
REFERENCES,0.3838771593090211,"n
řn
i“1sℓpF,xi,Gpxiqq. Let pF PF denote argminFPF pLpFq,
the empirical risk minimizer of pL, which we aim to show has population loss for fitting G bounded by
Opϵ` 1
?nq. By standard arguments using Rademacher complexity, we have with probability 1´δ,"
REFERENCES,0.3857965451055662,"sup
FPF
|LpFq´pLpFq|ď2Radn,PpLGq` c"
REFERENCES,0.3877159309021113,logp2{δq n ď2ϵ` c
REFERENCES,0.3896353166986564,logp2{δq
REFERENCES,0.3915547024952015,"n
(A.1)"
REFERENCES,0.3934740882917466,"Now note that by the condition 3) onsℓ, there exists F ‹ with LpF ‹qďϵ. Now we have"
REFERENCES,0.39539347408829173,"LppFq´LpF ‹qďpLppFq´pLppFqq`ppLppFq´pLpF ‹qq`ppLpF ‹q´LpF ‹qq
We bound the first and last term in parenthesis by applying (A.1), and the middle term is bounded by
0, by definition of pF. It follows that"
REFERENCES,0.39731285988483683,LppFq´LpF ‹qď4ϵ`2 c
REFERENCES,0.39923224568138194,logp2{δq n
REFERENCES,0.40115163147792704,ùñ LppFqď5ϵ`2 c
REFERENCES,0.40307101727447214,logp2{δq
REFERENCES,0.4049904030710173,"n
where we used LpF ‹qďϵ. Finally, we use the fact thatsℓupper bounds ℓ, so Ex„PrℓppFpxq,GpxqqsďLppFq.
Plugging in δ“0.01 gives the desired result."
REFERENCES,0.4069097888675624,"Proof of Lemma 2.3. We first observe that sℓγpθ,x,yqď1pρpθ,x,yqăγq by definition, so by (2.3), for all
GPG we have
inf
θPΘEx„Prsℓpθ,x,Gpxqqsďϵ"
REFERENCES,0.4088291746641075,"Thus, it remains to check the Rademacher complexity condition for applying Proposition 2.2. Fixing any
GPG, define the function class LG as in Definition 2.1."
REFERENCES,0.4107485604606526,"We first observe that following the same argument as Claim A.4 of (Wei & Ma, 2019b) (except we apply
the perturbations to the parameters, rather than the hidden layers), |ρpθ,x,yq´ρpθ1,x,yq|ď}θ´θ1}2 for
any θ,θ1PRp. Let N}¨}2pε,Θq denote the ε-covering number of Θ in }¨}2-norm, and N}¨}8pε,LGq the ε-
covering number of LG in the norm defined by }H´H1}8“maxxPX|Hpxq´H1pxq| for any H,H1PLG.
The arguments of (Wei & Ma, 2019b) imply that logN}¨}8pε,LGqďlogN}¨}2pγε,ΘqďO
´Y
α2logppq"
REFERENCES,0.4126679462571977,"γ2ε2
]¯
,
where the last inequality is from standard covering number bounds for }¨}1 balls. Now we can apply
this covering number bound in the Dudley entropy integral, another standard step to bound Rademacher"
REFERENCES,0.4145873320537428,"complexity, to obtain that for all n, Radn,PpLGqÀ
αlogn?"
REFERENCES,0.4165067178502879,"logppq
γ?n
(see arguments in (Wei & Ma, 2019b) for
more detail). Solving for n such that the r.h.s. of this equation is bounded by ϵ gives the desired result."
REFERENCES,0.418426103646833,"Note that from the proof of Lemma 2.3, we would also obtain the following parameter-space all-layer
margin generalization bound as a corollary, which may be of independent interest:
Corollary A.1. In the setting of Lemma 2.3, let Q denote a distribution over px,yq pairs, with pxi,yiqn
i“1
denoting a set of n i.i.d. training samples from Q. With probability 1´δ over the draw of the training
samples, all classifiers Fp¨,θqPF which achieve zero 0-1 training loss satisfy"
REFERENCES,0.42034548944337813,"Ex„Qrℓ0-1pFpx,θq,yqsďO ¨ ˝α
a"
REFERENCES,0.42226487523992323,"logppq
?n"
REFERENCES,0.42418426103646834,"g
f
f
e1 n n
ÿ i“1"
REFERENCES,0.42610364683301344,"1
ρpθ,xi,yiq2 ˛"
REFERENCES,0.42802303262955854,"‚`ξ
(A.2)"
REFERENCES,0.42994241842610365,"where ξÀO
´
logp1{δq`logpnq
?n
¯
is a low-order term."
REFERENCES,0.43186180422264875,"The proof of Corollary A.1 simply follows by plugging in the coverning number bound on ρ derived in
the proof of Lemma 2.3 into Lemma 2.2 of (Wei & Ma, 2019b)."
REFERENCES,0.43378119001919385,Under review as a conference paper at ICLR 2022
REFERENCES,0.43570057581573896,"B
PROOFS FOR SECTION 3"
REFERENCES,0.43761996161228406,"This section completes the proof of Section 3. The following lemma formally states that we can construct
the neural net to simulate the circuit layerwise.
Lemma B.1. In the setting of Theorem 3.1, let G denote the layered boolean circuit, which we aim to
compute using a neural net. Let Gi :t0,1uri´1 Ñt0,1uri denote function computed between the i´1-th
and i-th layers of G, which we assume have ri´1 and ri gates, respectively. Let f denote the following
2-layer neural net architecture, parameterized by θ“pW1,b1,W2,b2q:
fph,θq“φpW2φpW1h`b1q`b2q"
REFERENCES,0.43953934740882916,"Then there exist θ with }θ}1“Opriq such that for any hPt0,1uri´1,"
REFERENCES,0.44145873320537427,"fprh,θq“Č
Giphq"
REFERENCES,0.44337811900191937,"where rh takes h and appends w´ri´1 zeros, and likewise for Ă
Giphq."
REFERENCES,0.44529750479846447,"We note that the proof of Lemma 3.2 follows by applying Lemma B.1 q´1 times. Using Lemma B.1,
we can complete the proof of Theorem 3.1."
REFERENCES,0.4472168905950096,"Proof of Theorem 3.1. Our proof will construct a neural network to compute any boolean circuit with
all-layer margin lower bound
1
polypr,qq. By Lemma 2.3, this will be sufficient to guarantee meaningful
approximation."
REFERENCES,0.4491362763915547,"There are two steps in our construction: first, given any layered circuit GPGq,r,s, we construct a neural net
that directly simulates G by computing the layers of G one-by-one. Our construction shows that we can
compute every layer in G using two feedforward ReLU layers, and results in a neural net pF computing G,
but with possibly small all-layer margin. The next step is to convert pF into a neural net with large all-layer
margin, i.e., implement Lemma 3.3. To do this, we insert “correction functions” (Definition D.1) between
every group of layers in pF. These correction layers leverage the knowledge that unperturbed outputs of
these layers should be contained in t0,1uw and perform elementwise rounding to map perturbed values
back to t0,1uw. Theorem D.6 formally shows that by introducing these correction layers can guarantee a
lower bound on the all-layer margin roughly depending on the Lipschitz constants of each individual layer.
Furthermore, each correction layer can be computed via two feedforward ReLU layers, so introducing
the correction layers only increases depth by a constant factor."
REFERENCES,0.4510556621880998,"We implement the proof plan by first applying Lemma B.1 q times in order to obtain the function pF
computing G (with padding) mentioned above. The total }¨}1-norm of the parameters so far is at most
s. Now we use the correction function described in Proposition 3.4, which we apply coordinate-wise on
non-padding coordinates. We apply the correction functions after each layer constructed in Lemma B.1.
Note that each correction function requires at most double the width of the corresponding layer in the
circuit, and the parameters for all correction functions add total }¨}1-norm at most Opsq."
REFERENCES,0.45297504798464494,"Note that at this point, minor modifications are still required in order to apply Theorem D.6. The neural
net output is in t0,1uw, not t´1,1u; we can remedy this by setting the last layer to compute the linear
transformation z ÞÑ 2z´1 on the single non-padding coordinate corresponding to the output. Second,
to make the depth of the architecture consistently d, we can add sequences of identity functions before this
last linear layer just constructed, followed by correction layers, until each of the constructed approximating
functions reaches the desired fixed depth d. This finally gives us parameters θ with }¨}1-norm bound
Ops ` dq, so that the set of constructed functions is contained in Fw,d,α. Thus, we showed that for
GPGq,r,s, there exists θ such that Fpx,θq“2Gpxq´1 for all xPt0,1um."
REFERENCES,0.45489443378119004,"Finally, it is straightforward to check that Condition D.3 for Theorem D.6 is satisfied for Lipschitzness
parameters which are polynomial in the circuit width r. Thus, we apply Theorem D.6 to obtain a lower
bound pγ“
1
polypr,qq ě
1
polypsq on the all-layer margin for every input xPt0,1um. Finally, we directly apply
Lemma 2.3 using γ“pγ to obtain the desired result."
REFERENCES,0.45681381957773515,"The following proposition will be used to construct basic gates in the circuit with a simple feedforward
ReLU network."
REFERENCES,0.45873320537428025,"Proposition B.2. Let x “
„
x1
x2"
REFERENCES,0.46065259117082535,"
P t0,1u2 be binary inputs to AND and OR gates. The following"
REFERENCES,0.46257197696737046,"feedforward ReLU networks compute the AND and OR functions: FANDpxq “ φpx1 ` x2 ´ 1q, and
FORpxq“1´φp1´x1´x2q."
REFERENCES,0.46449136276391556,Under review as a conference paper at ICLR 2022
REFERENCES,0.46641074856046066,"Proof of Lemma B.1. Each row of W1 and value in b1 will correspond to a single entry in the output
of Ă
Gi. The same applies for W2,b2. W2 will be set to a diagonal matrix with entries in t´1,0,1u. For
the 0 entries which only serve to pad the dimension, we set corresponding values in W1,b1,W2,b2 to be
0. For the remainder of the entries of Ă
Gi corresponding to actual gates in the circuit, in the case that the
gates compute AND or OR, we fill in the values of corresponding rows in W1,b1,W2,b2 to implement
the constructions for AND and OR in Proposition B.2. The construction for ID and NOT are even simpler.
For example, to implement NOTpzq“1´z for zPt0,1u on coordinate j, we can set the j-th row of W1
to have -1 on the diagonal and 0 everywhere else, pb1qj “1, pb2qj “0, and pW2qj,j “1. It is easy to check
that }θ}1“Opriq with this construction."
REFERENCES,0.46833013435700577,"C
PROOF OF THEOREM 4.1"
REFERENCES,0.47024952015355087,"C.1
ADDITIONAL SETUP AND NOTATION"
REFERENCES,0.472168905950096,"We fix any Turing machine GPG and construct a transformer which can simulate G. Throughout this
section, a superscript will be used to index layer indices, and a subscript to index timesteps."
REFERENCES,0.4740882917466411,"We assume that the initial state of the tape has the input written at the left-most positions. The Turing
machine always starts at a fixed initial state zinit. We let r∅sPA denote the blank symbol, which initially
fills all positions on the tape which aren’t part of the input. We construct a transformer that simulates
the Turing machine up until it reaches a terminal state in Zterm, at which the transformer will loop in that
state until it hits a computation time T."
REFERENCES,0.4760076775431862,"We introduce some notation which will appear throughout the construction. Define wpos ﬁrlog2Ts.
We use wpos to denote the effective dimension of the position embedding, as only wpos coordinates will
be non-zero. For 0 ď i ď T, define Binpiq P Rwpos to be the vector containing the binary encoding of i:
Binpiqj “1 if the binary representation of i contains 1 in the j-th bit and 0 otherwise."
REFERENCES,0.4779270633397313,"For simplicity, the proof will focus on the setting without overparameterization, where we choose the di-
mension w“wTMﬁ|Z|`2|A|`3wpos`wscr for storing all the hidden representations of the model, where
wscr“Opwpos`|A|`|Z|q. We can extend our analysis to allow for arbitrary over-parameterization using
wąwTM by designating a certain subset of the coordinates to always equal 0, and performing calculations
using only a subset of wTM coordinates. We group the wTM coordinates using the following symbols: st for
encoding the state, sym1, sym2 for encoding symbols, pos1 and pos2, pos3 for encoding position, and scr,
which is used as scratch space. Thus, for hPRw, we can index its coordinates via the groups as follows: h“ »"
REFERENCES,0.4798464491362764,———————–
REFERENCES,0.4817658349328215,"hst
PR|Z|"
REFERENCES,0.4836852207293666,"hsym1
PR|A|"
REFERENCES,0.4856046065259117,"hsym2
PR|A|
hpos1
PRwpos
hpos2
PRwpos
hpos3
PRwpos
hscr
PRwscr ﬁ"
REFERENCES,0.4875239923224568,ﬃﬃﬃﬃﬃﬃﬃﬂ
REFERENCES,0.4894433781190019,"When the meaning is clear from context, we use the superscript to index coordinate groups as described."
REFERENCES,0.491362763915547,"The position embedding βpiq is defined formally so that βpiqpos1 “ Binpiq, and βpiq is 0 in all other
coordinates. The encoder embedding matrix E is such that"
REFERENCES,0.4932821497120921,Encipxqsym1 “1|A|pxq
REFERENCES,0.4952015355086372,"Encipxqpos1 “Binpiq
(C.1)"
REFERENCES,0.4971209213051823,"where Encipxq has 0’s at all other coordinates. embedding function e:AÑRd for the encoder is defined
such that epxqsym1 “1|A|pxq, the one-hot encoding for xPA, and 0 everywhere else. We use o1,...,oT
to refer to the output embeddings of the decoder. Our construction maintains the invariant that the output
embedding oi encodes zipxq, aipxq, lipxq for each i. To achieve this, we maintain"
REFERENCES,0.4990403071017274,"ost
i “1|Z|pzipxqq"
REFERENCES,0.5009596928982726,"osym1
i
“1|A|paipxqq"
REFERENCES,0.5028790786948176,"opos2
i
“Binplipxqq (C.2)"
REFERENCES,0.5047984644913628,Under review as a conference paper at ICLR 2022
REFERENCES,0.5067178502879078,"and oi has 0 at all other coordinates. Thus, the input oi`βpi`1q to the decoder at step i`1 is of the form"
REFERENCES,0.508637236084453,poi`βpi`1qqst“1|Z|pzipxqq
REFERENCES,0.510556621880998,poi`βpi`1qqsym1 “1|A|paipxqq
REFERENCES,0.5124760076775432,"poi`βpi`1qqpos1 “Binpiq
poi`βpi`1qqpos2 “Binplipxqq (C.3)"
REFERENCES,0.5143953934740882,"C.2
COMPLETING THE PROOF"
REFERENCES,0.5163147792706334,"We implement the first step 1) in Section 4.2 using the following lemma. Note that the lemma uses two
consecutive feedforward ReLU layers, but in our actual proof we will simulate this using two transformer
layers where the attention parameters are all 0, and only the feedforward layers are instantiated.
Lemma C.1. Let O denote the set of decoder inputs in the form (C.3) encoding zi´1pxq, ai´1pxq, li´1pxq
for some timestep i. For parameters θ “ pW1,b1,W2,b2q, consider the following function computing a
sequence of two feedforward ReLU layers: fph,θq “ φpW2φpW1h`b1q`b2q. There exist parameters
θ such that for decoder inputs hPO,
fph,θqst“1|Z|pzipxqq"
REFERENCES,0.5182341650671785,"fph,θqsym2 “1|A|puipxqq"
REFERENCES,0.5201535508637236,"fph,θqpos1 “Binpiq
fph,θqpos2 “Binpli´1pxqq (C.4)"
REFERENCES,0.5220729366602687,"Furthermore, fph,θqscr will contain a one-hot encoding for qipxq, and besides this, fph,θq is 0 at all other
coordinates. The parameters satisfy }θ}1“Op|Z||A|`wposq."
REFERENCES,0.5239923224568138,"Proof. We follow the construction used in Lemma B.2 of (Pérez et al., 2019). The first layer computes
a one-hot encoding of the state, symbol input pair. We choose W1:RwTM ÑR|Z||A|`wTM so that the first
|Z|A| rows are described by:"
REFERENCES,0.525911708253359,"pW1qst
pz,aq,:“1|Z|pzq"
REFERENCES,0.527831094049904,"pW1qsym1
pz,aq,:“1|A|paq"
REFERENCES,0.5297504798464492,"and 0 everywhere else. The remaining rows of wTM rows of W1 simply implement the identity mapping.
We choose b1 so that its first |Z||A| entries are -1, and all other entries are 0. We observe that from this
construction, for all hPO where h encodes zi´1pxq,ai´1pxq,"
REFERENCES,0.5316698656429942,"φpW1h`b1q“
„
1|Z||A|ppzi´1pxq,ai´1pxqqq
h "
REFERENCES,0.5335892514395394,"This is because before the ReLU, the first |Z||A| entries of W1h will have 2 on the pzi´1pxq,ai´1pxqq-th
entry and be bounded by 1 everywhere else, so adding α1 and applying the activation will zero out all
but one entry."
REFERENCES,0.5355086372360844,"Now it is simple to pick W2 so that fph,θq is as desired because we can construct it to exactly encode
the output of Spz,aq for each of its first pz,aq columns and copy over the other necessary entries of h
as needed by (C.4)."
REFERENCES,0.5374280230326296,"The next lemma demonstrates that we can use an additional sequence of feedforward ReLU layers to
produce Binplipxqq, given Binpli´1pxqq and qipxq.
Lemma C.2. In the setting of Theorem 4.1 and Lemma C.1 above, there is a function f parameterized
by θ composed of Opwposq feedforward ReLU layers such that for any h computed by the function in
Lemma C.1 in the form (C.4) at timestep i,
fph,θqst“1|Z|pzipxqq"
REFERENCES,0.5393474088291746,"fph,θqsym2 “1|A|puipxqq"
REFERENCES,0.5412667946257198,"fph,θqpos1 “Binpiq
fph,θqpos2 “Binpli´1pxqq
fph,θqpos3 “Binplipxqq (C.5)"
REFERENCES,0.5431861804222649,"At all other coordinates,
Fph, θq takes value 0.
Furthermore,
the parameters satisfy
}θ}1“Opwposp|Z|`|A|`wposqq."
REFERENCES,0.54510556621881,Under review as a conference paper at ICLR 2022
REFERENCES,0.5470249520153551,"Proof. As the construction of Lemma C.1 encoded qipxq, the movement direction of the head, we can use
feedforward ReLU layers to implement binary addition to either add or subtract 1 from li´1pxq. Let v1,v2
denote the bits in the scratch dimensions indicating the head movement, where v1“1,v2“0 indicates left
and v1“0,v2“1 indicates right. Then more specifically, we first use Opwposq feedforward ReLU layers to
compute li´1pxq´v1, and then Opwposq additional feedforward ReLU layers to compute li´1pxq´v1`v2.
Note that the output would always be lipxq by the definition of v1,v2."
REFERENCES,0.5489443378119002,"It remains to implement a module which computes Binpj´v1q given v1,Binpjq, and Binpj`v2q given
v2,Binpjq for any jPrTs. We can express the binary addition by a depth-Opwposq binary circuit, which can
in turn be expressed by a neural net with Opwposq layers where each weight matrix has }¨}1-norm p|Z|`
|A|`wposq (which is required to implement the identity mapping to copy forward the other dimensions
of h which aren’t involved in the binary addition). This gives the desired total }¨}1-norm bound."
REFERENCES,0.5508637236084453,"The next lemmas implement steps 3), 4), 5) in Section 4.2. For the following lemmas, it will be helpful
to further index the scratch dimensions as follows: for a vector hPwscr, hscr“ » —–"
REFERENCES,0.5527831094049904,hscr1 PR|A|
REFERENCES,0.5547024952015355,"hscr2 PR|A|
hscr3 PRwpos
hscr4 PR3 ﬁ ﬃﬂ"
REFERENCES,0.5566218809980806,"Lemma C.3. In the setting of Theorem 4.1 and Lemma C.2 above, fix any timestep i and define
i1 “ maxt1 ď t ď i : lt´1pxq “ lipxqu. If j such that lt´1pxq “ lipxq exists, we define i1 “ 0 otherwise.
Consider any Hi “ph1,...,hiq, where ht is computed by the layer in Lemma C.2 for timestep t, and in
the form (C.5). There is a function f parameterized by θ consisting of Opwposq total self-attention and
linear layers such that for all such Hi, the following holds:"
REFERENCES,0.5585412667946257,"fphi,Hi,θqst“1|Z|pzipxqq"
REFERENCES,0.5604606525911708,"fphi,Hi,θqsym2 “1|A|puipxqq"
REFERENCES,0.5623800383877159,"fphi,Hi,θqpos1 “Binpiq
fphi,Hi,θqpos2 “Binpli´1pxqq
fphi,Hi,θqpos3 “Binplipxqq"
REFERENCES,0.564299424184261,"fphi,Hi,θqscr1 “
""1|A|pui1pxqq
if i1ą0
0
otherwise"
REFERENCES,0.5662188099808061,"Fphi,Hi,θqscr4
1
“1pi1ą0q (C.6)"
REFERENCES,0.5681381957773513,"At all other coordinates,
FpH, θq takes value 0.
Furthermore,
the parameters satisfy
}θ}1“Opwposp|Z|`|A|`wposqq."
REFERENCES,0.5700575815738963,"The proof plan will roughly implement a binary search to find i1, leveraging the attention layers. The
first step in the binary search is to verify whether i1ą0, described below.
Claim C.4. In the setting of Lemma C.3, let Hi“h1,...,hi be the input representations for timesteps 1,...,i.
Suppose that each ht for 1ďtďi satisfies the following:"
REFERENCES,0.5719769673704415,"hpos1
t
“Binptq"
REFERENCES,0.5738963531669866,"hpos2
t
“Binplt´1pxqq
(C.7)"
REFERENCES,0.5758157389635317,"Additionally, suppose that hi is of the form in (C.5). Then there is a function fp0q parameterized by θ
such that"
REFERENCES,0.5777351247600768,"fp0qphi,Hi,θqscr1 “0"
REFERENCES,0.5796545105566219,"fp0qphi,Hi,θqscr3 “0"
REFERENCES,0.581573896353167,"fp0qphi,Hi,θqscr4
1
“1pi1ą0q (C.8)"
REFERENCES,0.5834932821497121,The function fp0q can be computed by a single decoder self-attention layer with }θ}1“Opwposq.
REFERENCES,0.5854126679462572,"Next, we implement the binary search itself, using wpos self-attention layers. Each step of the binary
search reveals a single bit of i1, so the j-th attention layer will compute a representation storing the j most"
REFERENCES,0.5873320537428023,Under review as a conference paper at ICLR 2022
REFERENCES,0.5892514395393474,"significant bits of i1. We let BinjplqPt0,1uwpos to denote the binary encoding of the j most significant bits
of l: pBinjplqqj1 “pBinplqqj1 for 1ďj1ďj, and pBinjplqqj1 “0 for j1ąj. We also set Bin0plq“0. We use
the superscript pjq to indicate the j-th set of layers in the binary search. The following claim implements
each step of the binary search rigorously."
REFERENCES,0.5911708253358925,"Claim C.5. In the setting above and of Lemma C.3, let Hpjq
i
“ hpjq
1 ,...,hpjq
i
be the representations
computed after the j-th group of layers for timesteps 1 through i, for 0ďjďwpos´1. Suppose that each
hpjq
t
for 1ďtďi satisfies the following:"
REFERENCES,0.5930902111324377,"hpjq,pos1
t
“Binptq"
REFERENCES,0.5950095969289827,"hpjq,pos2
t
“Binplt´1pxqq
(C.9)"
REFERENCES,0.5969289827255279,"In addition, suppose that hpjq
i
satisfies:"
REFERENCES,0.5988483685220729,"hpjq,scr1
i
“0"
REFERENCES,0.6007677543186181,"hpjq,scr3
i
“
"" Binjpi1q
if i1ą0
0
otherwise"
REFERENCES,0.6026871401151631,"phpjq,scr4
i
q1“1pi1ą0q"
REFERENCES,0.6046065259117083,(C.10)
REFERENCES,0.6065259117082533,"with all other coordinates matching the quantities prescribed in (C.5). Then there is a function fpj`1q
parameterized by θ such that"
REFERENCES,0.6084452975047985,"fpj`1qphpjq
i ,Hpjq
i ,θqscr1 “0"
REFERENCES,0.6103646833013435,"fpj`1qphpjq
i ,Hpjq
i ,θqscr3 “
"" Binj`1pi1q
if i1ą0
0
otherwise"
REFERENCES,0.6122840690978887,"fpj`1qphpjq
i ,Hpjq
i ,θqscr4
1
“1pi1ą0q"
REFERENCES,0.6142034548944337,(C.11)
REFERENCES,0.6161228406909789,"with all other coordinates matching those prescribed in (C.5). We note that fpj`1q consists of a single
decoder self-attention layer followed by single feedforward ReLU layer, with }θ}1“Op|Z|`|A|`wposq."
REFERENCES,0.6180422264875239,"At the end of the wpos-th application of the binary search, we would have found Binpi1q exactly. It remains
to apply another attention layer which attends directly to timestep i1 and copies ui1pxq.
Claim C.6. In the setting above and of Lemma C.3, let Hi “h1,...,hi be the representations computed
after the wpos-th group of layers constructed in Claim C.5 for timesteps 1 through i. Suppose that each
ht for 1ďtďi satisfies the following:"
REFERENCES,0.6199616122840691,"hsym2
t
“1|A|putpxqq"
REFERENCES,0.6218809980806143,"hpos1
t
“Binptq"
REFERENCES,0.6238003838771593,"hpos2
t
“Binplt´1pxqq"
REFERENCES,0.6257197696737045,(C.12)
REFERENCES,0.6276391554702495,"In addition, suppose that hi satisfies:"
REFERENCES,0.6295585412667947,"hscr1
i
“0"
REFERENCES,0.6314779270633397,"hscr3
i
“
"" Binpi1q
if i1ą0
0
otherwise"
REFERENCES,0.6333973128598849,"phscr4
i
q1“1pi1ą0q"
REFERENCES,0.6353166986564299,(C.13)
REFERENCES,0.6372360844529751,with all other coordinates matching the quantities prescribed in (C.5). Then there is a function fpwpos`1q
REFERENCES,0.6391554702495201,"parameterized by θ such that fpwpos`1qphi,Hi,θq computes the desired output in (C.6). Furthermore,
fpwpos`1q consists of a single decoder self-attention layer followed by a single feedforward ReLU layer,
and }θ}1“Op|Z|`|A|`wposq."
REFERENCES,0.6410748560460653,"Putting these together, we complete the proof of Lemma C.3."
REFERENCES,0.6429942418426103,"Proof of Lemma C.3. For the purposes of this proof, we index the layers by a superscript to avoid
confusion with indexing timesteps. We set fp0q to be the function defined in Claim C.4. We note that"
REFERENCES,0.6449136276391555,Under review as a conference paper at ICLR 2022
REFERENCES,0.6468330134357005,"layers output by fp0q satisfy the condition of Claim C.5, so we can apply Claim C.5 inductively to obtain
layers fp1q,...,fpwposq where their applying their composition results in representations satisfying (C.12)
and (C.13). Now we set fpwpos`1q to be the function constructed in Claim C.5, which gives the desired
output. Finally, we note that by summing the }¨}1 bounds for the parameters constructed in each layer,
we can finally obtain }θ}1“Opwposp|Z|`|A|`wposqq."
REFERENCES,0.6487523992322457,"We fill in the proofs of Claims C.4, C.5, and C.6 below."
REFERENCES,0.6506717850287908,"Proof of Claim C.4. To construct the decoder self-attention, the query function will be of the form
Qphq “ WQh`bQ and Kphq “ WKh`bK, where WQ,WK P Rpwpos`1qˆw and bQ,bK P Rwpos`1. We
choose the parameters such that the following equations hold:"
REFERENCES,0.6525911708253359,Qphq1:wpos “2hpos3´1
REFERENCES,0.654510556621881,Qphqwpos`1“1 and
REFERENCES,0.6564299424184261,Kphq1:wpos “2hpos2´1
REFERENCES,0.6583493282149712,Kphqwpos`1“0
REFERENCES,0.6602687140115163,"The value function V phq is such that V phqscr4
1
“ 1, and V phqℓ“ 0 on all other coordinates ℓ, which
can be implemented by a linear transformer. Finally, we set the null key K0 and value V0 such that
pK0qwpos`1“wpos´1, with 0 everywhere else, and V0“0. Letting θattn denote the attention parameters,
the layer is of the form"
REFERENCES,0.6621880998080614,"fp0qphi,Hi,θq“Attnphi,Hi,θq"
REFERENCES,0.6641074856046065,"To see that fp0q satisfies (C.8), observe that if i1 ą 0, QphiqJKphi1q “ wpos by (C.7) and construction
of Q,K. On the other hand, QphiqJK0 “ wpos´1. Thus, argmaxtQphiqJKphtq P ris, which implies
that fp0qphi,Hi,θqscr4
1
“ 1 by the construction of V . In the other case where i1 “ 0, we note that
QphiqJKphtqďwpos´2 for all 1ďtďi, so the null position is attended to. By construction of V0, this
implies fp0qphi,Hi,θqscr4
1
“0. As V,V0 are 0 on all other coordinates, it follows that (C.8) holds. It’s also
easy to observe that the }θ}1 is as desired."
REFERENCES,0.6660268714011516,"Proof of Claim C.5. The first layer in fpj`1q computes decoder self-attention. The query function is of
the form Qphq“WQh`bQ, and the key function is of the form Kphq“WKh`bh, where WQ,WK P
Rpwpos`j`2qˆw and bQ,bK PRpwpos`j`2q. We choose the parameters so that the following equations hold:"
REFERENCES,0.6679462571976967,Qphq1:wpos “2hpos3´1
REFERENCES,0.6698656429942419,"Qphqwpos`1:wpos`j “2hscr3
1:j ´1"
REFERENCES,0.6717850287907869,Qphqwpos`j`1“1
REFERENCES,0.6737044145873321,Qphqwpos`j`2“1 and
REFERENCES,0.6756238003838771,Kphq1:wpos “2hpos2´1
REFERENCES,0.6775431861804223,"Kphqwpos`1:wpos`j`1“2hpos1
1:j`1´1"
REFERENCES,0.6794625719769674,Kphqwpos`j`2“0
REFERENCES,0.6813819577735125,"Both of these functions can be constructed via linear transformations of h, with }WQ}1`}WK}1`}bQ}1`
}bK}1 “Opwposq. Now we construct the value function V phq“WV h`bV such that V phqscr4
3
“1 and
V phqℓ“0 on all other coordinates, which is also easily implemented by a linear layer. For the attention, the
last quantities to construct are the null key K0 and value V0. K0 will satisfy pK0qwpos`j`2“wpos`j, with
0 everywhere else. V0 will simply be 0 on all coordinates. Letting θattn“pWQ,bQ,WK,bK,WV ,bV ,K0,V0q
denote the attention parameters, the first layer will now be in the form"
REFERENCES,0.6833013435700576,"fpj`1q,1phpjq
i ,Hpjq
i ,θattnq“Attnphpjq
i ,Hpjq
i ,θattnq"
REFERENCES,0.6852207293666027,Under review as a conference paper at ICLR 2022
REFERENCES,0.6871401151631478,"where Attn uses the constructed key, value, and query functions. We claim that fpj`1q,1phpjq
i ,Hpjq
i ,θattnq
satisfies the following:"
REFERENCES,0.6890595009596929,"fpj`1q,1phpjq
i ,Hpjq
i ,θattnqscr4
3
“
"" 1 if i1ą0 and has pj`1q-th bit 1
0 otherwise
(C.14)"
REFERENCES,0.690978886756238,"For all other coordinates ℓ, fpj`1q,1phpjq
i ,Hpjq
i ,θattnqℓ“ phpjq
i qℓ. To see this, we first observe that
Qphpjq
i qJK0 “ wpos ` j. Next, we observe that Qphpjq
i q1:wpos produces the encoding of lipxq using
binary t´1, `1u bits, and Kphpjq
t q1:wpos produces the encoding of lt´1pxq using binary t´1, `1u
bits by (C.9). In addition, Qphpjq
i qwpos`1:wpos`j “ 2Binjpi1q ´ 1 if i1 ą 0 and all 0’s otherwise, and"
REFERENCES,0.6928982725527831,"Kphpjq
t qwpos`1:wpos`j`1“2Binj`1ptq´1. Note that by our construction, the maximum possible value of"
REFERENCES,0.6948176583493282,"Qphpjq
i qJKphpjq
t q is wpos`j`1, and the next largest possible value is wpos`j´1. Now there are 3 cases:"
REFERENCES,0.6967370441458733,"Case 1: i1“0. In this case, we note that lipxq never matches lt´1pxq for 1ďtďi. Thus, by construction
of the first wpos coordinates of Q and K, the largest possible value of Qphpjq
i qJKphpjq
t q is wpos`j´1,
so the attention will always only attend to the null position, so the layer adds V0 “0 to hpjq
i , preserving
its value. Note that phpjq,scr4
i
q3“0 in this case, which matches the desired behavior."
REFERENCES,0.6986564299424184,"Case 2: i1ą0, and has pj`1q-th bit 0. In this case, we note that for all tąi1, Qphpjq
i qJKphpjq
t qďwpos`j´
1, because by definition such t must satisfy lt´1pxq‰lipxq, so the first wpos coordinates contribute at most
wpos´2 to the dot product. On the other hand, if tďi1, t must have pj`1q-th bit 0, so Kphpjq
t qwpos`j`1“"
REFERENCES,0.7005758157389635,"´1. This doesn’t match the pwpos`j`1q-th bit of the query, so Qphpjq
i qJKphpjq
t qďwpos`j´1 again.
Thus, in this case, the null position is attended to again. The same reasoning as Case 1 then applies."
REFERENCES,0.7024952015355086,"Case 3: i1ą0 and has pj`1q-th bit 1. In this case, maxtQphpjq
i qJKphpjq
t q“wpos`j`1: for example, t“
i1 achieves this maximum by our construction. As a result, the null position is not attended to. All the values
in the positions attended to satisfy V phpjq
t qscr4
3
“1, which matches the pj`1q-th bit of i1. Thus, (C.14) holds."
REFERENCES,0.7044145873320538,"Finally, to complete the proof we simply append an additional feedforward ReLU layer which copies the
value fpj`1q,1phpjq
i ,Hpjq
i ,θattnqscr4
3
to the output bit corresponding to the position indexed by ¨scr3
j`1. This layer
will also set the output bit corresponding to ¨scr4
3
to 0. Note that these operations can be implemented with a
linear layer, and applying a ReLU activation after won’t change the output, which is in t0,1uw. By (C.10),
the constructed function will thus satisfy (C.11). It’s also easy to observe that }θ}1 is as desired."
REFERENCES,0.7063339731285988,"Proof of Claim C.6. The attention layer uses key and query functions which each compute linear
transformations from Rw to R2wpos`1. The value function is also linear. We choose parameters such that"
REFERENCES,0.708253358925144,Qphq1:wpos “2hpos3´1
REFERENCES,0.710172744721689,Qphqwpos`1:2wpos “2hscr3´1
REFERENCES,0.7120921305182342,Qphq2wpos`1“1 and
REFERENCES,0.7140115163147792,Kphq1:wpos “2hpos2´1
REFERENCES,0.7159309021113244,Kphqwpos`1:2wpos “2hpos1´1
REFERENCES,0.7178502879078695,Kphq2wpos`1“0 and
REFERENCES,0.7197696737044146,V phqscr1 “hsym2
REFERENCES,0.7216890595009597,"Furthermore, we choose null keys and positions such that pK0q2wpos`1“2wpos´1, and V0“0. To follow
the attention layer, we construct a linear layer which simply zeros out coordinates indexed by ¨scr3 and pre-
serves all other coordinates. Note that because all outputs are either 0 or 1, applying a ReLU activation won’t
change the result. To see that this construction computes (C.6), we observe that if i1ą0, QphiqJKphi1q“
2wpos. Otherwise, if i1“0, QphiqJKphtqď2wpos´2 for all 1ďtďi. On the other hand, it always hold
that QphiqJK0“2wpos´1. Thus, if i1ą0, the attention attends exactly to i1, so the value function satisfies"
REFERENCES,0.7236084452975048,Under review as a conference paper at ICLR 2022
REFERENCES,0.72552783109405,"V phi1q“1|A|pui1pxqq, which would produce the output in (C.6), as desired. On the other hand, if i1“0, the
attention attends to the null position, so the attention layer sets fpwpos`1qphi,Hi,θqscr1 “0. Thus, fpwpos`1q
also produces the desired output in this case. It’s also easy to observe that the }θ}1 is as desired."
REFERENCES,0.727447216890595,"The next step is to complete step 4) in Section 4.2 using encoder-decoder attention. The following lemma
provides this construction.
Lemma C.7. In the setting of Theorem 4.1 and Lemma C.3, consider any timestep i and let h denote an
output of the function constructed in Lemma C.3, in the form (C.6). Let e1,...,em denote the outputs of the
encoder, in the form (C.1). There is a function f with parameter θ consisting of a single encoder-decoder
attention layer such that for all such h in the form (C.6), the following holds:"
REFERENCES,0.7293666026871402,"fph,pe1,...,emq,θqst“1|Z|pzipxqq"
REFERENCES,0.7312859884836852,"fph,pe1,...,emq,θqsym2 “1|A|puipxqq"
REFERENCES,0.7332053742802304,"fph,pe1,...,emq,θqpos1 “Binpiq
fph,pe1,...,emq,θqpos2 “Binpli´1pxqq
fph,pe1,...,emq,θqpos3 “Binplipxqq"
REFERENCES,0.7351247600767754,"fph,pe1,...,emq,θqscr1 “
""1|A|pui1pxqq
if i1ą0
0
otherwise"
REFERENCES,0.7370441458733206,"fph,pe1,...,emq,θqscr2 “
""1|A|pxlipxqq
if lipxqďm
0
otherwise"
REFERENCES,0.7389635316698656,"fph,pe1,...,emq,θqscr4
1
“1pi1ą0q
fph,pe1,...,emq,θqscr4
2
“1plipxqďmq"
REFERENCES,0.7408829174664108,(C.15)
REFERENCES,0.7428023032629558,"At all other coordinates, fph, pe1, ... , emq, θq takes value 0.
Furthermore, the parameters satisfy
}θ}1“Op|A|`wposq."
REFERENCES,0.744721689059501,"Proof. We choose the encoder-decoder attention layer so that the key, value, and query functions are linear
transformations. The key and query functions map Rw to Rwpos`1 and compute the following:"
REFERENCES,0.746641074856046,Qphq1:wpos “2hpos3´1
REFERENCES,0.7485604606525912,Qphqwpos`1“1 and
REFERENCES,0.7504798464491362,Kphq1:wpos “2hpos1´1
REFERENCES,0.7523992322456814,Kphqwpos`1“0
REFERENCES,0.7543186180422264,The value function computes
REFERENCES,0.7562380038387716,V phqscr2 “hsym1
REFERENCES,0.7581573896353166,"V phqscr4
2
“1"
REFERENCES,0.7600767754318618,"with 0’s in all other coordinates. The null key K0 satisfies pK0qwpos`1 “ wpos´1, with 0’s in all other
coordinates. The null value V0 satisfies V0“0. We set"
REFERENCES,0.761996161228407,"fph,pe1,...,emq,θq“Attnph,pe1,...,emq,θq"
REFERENCES,0.763915547024952,"where Attn is the decoder-encoder attention using the key, value, and query described above. Now we
observe that from this construction, if h is in the form provided in (C.6), then Qphq1:wpos “ Binplipxqq.
In addition, we have Kpejq1:wpos “ epos1
j
“ Binpjq for 1 ď j ď m. Thus, by construction of V,K0,V0,
if lipxq ď m, the attention attends to position lipxq in the embedding. The value function for this
position satisfies V pelipxqqscr2 “ esym1
lipxq “ 1|A|pxlipxqq. Thus, in this case Fph,θq computes the desired
output in (C.15). On the other hand, if lipxq ą m, then the attention will attend to the null position, as
QphqJK0“wpos´1, and the largest possible score for all other positions is wpos´2. In this case, (C.15)
holds again. It is also easy to check that the desired bound on }θ}1 would hold."
REFERENCES,0.7658349328214972,"Finally, we implement step 5) of the outline in Section 4.2 in the following lemma."
REFERENCES,0.7677543186180422,Under review as a conference paper at ICLR 2022
REFERENCES,0.7696737044145874,"Lemma C.8. In the setting of Theorem 4.1 and Lemma C.7, consider any timestep i and any h output
by the function in Lemma C.7 taking the form in (C.15). Then there is a function f with parameters θ
consisting of a constant number of feedforward ReLU layers satisfying the following:"
REFERENCES,0.7715930902111324,"fph,θqst“1|Z|pzipxqq"
REFERENCES,0.7735124760076776,"fph,θqsym1 “1|A|paipxqq"
REFERENCES,0.7754318618042226,"fph,θqpos2 “Binplipxqq"
REFERENCES,0.7773512476007678,(C.16)
REFERENCES,0.7792706333973128,"At all other coordinates,
Fph, θq takes values 0.
Furthermore,
the parameters satisfy
}θ}1“Op|Z|`|A|`wposq."
REFERENCES,0.781190019193858,Proof. It suffices to construct a sequence of layers which performs the following operations:
REFERENCES,0.783109404990403,1) Compute the following vector vPR3: v“
REFERENCES,0.7850287907869482,"$
’
’
’
’
’
’
&"
REFERENCES,0.7869481765834933,"’
’
’
’
’
’
% »"
REFERENCES,0.7888675623800384,"–
1
0
0 ﬁ"
REFERENCES,0.7907869481765835,"ﬂ
if hscr4
1
“1 »"
REFERENCES,0.7927063339731286,"–
0
hscr4
2
1´hscr4
2 ﬁ"
REFERENCES,0.7946257197696737,"ﬂ
if hscr4
1
“0"
REFERENCES,0.7965451055662188,"Note that v encodes the location of the symbol aipxq, as aipxq“ui1pxq if i1ą0, aipxq“xlipxq
if i1 “0 and lipxqďm, and aipxq“r∅s otherwise. The vector v is a one-hot vector indicating
which of these three cases holds."
REFERENCES,0.7984644913627639,"2) We
can
take
v1
and
compute
AND
with
all
bits
of
hscr1,
which
computes
1|A|pui1pxqq“1|A|paipxqq if i1ą0, and 0 otherwise."
REFERENCES,0.800383877159309,"3) We take v2 and compute AND with all bits of hscr2, which computes 1|A|pxlipxqq if v2“1, and
0 otherwise."
REFERENCES,0.8023032629558541,"4) We take v3 and compute AND with all bits of 1|A|pr∅sq, which computes 1|A|paipxqq if v3“1."
REFERENCES,0.8042226487523992,"5) We add the outputs of 2), 3), and 4) together, which gives 1|A|paipxqq. We copy this quantity
into the output coordinates indexed by ¨sym1. Then we set coordinates not listed in (C.16) to 0,
producing the desired output."
REFERENCES,0.8061420345489443,"Each of these operations can be computed by a constant number of feedforward ReLU layers, with total
parameter norm satisfying }θ}1“Op|Z|`|A|`wposq."
REFERENCES,0.8080614203454894,"Proof of Theorem 4.1. We construct a neural net to compute any Turing machine with all-layer margin
lower bound
1
polypk,|A|,logTq and apply Lemma 2.3 to turn this into a statement about statistically meaningful
approximation."
REFERENCES,0.8099808061420346,"For our Turing machine construction, we follow the outline laid out in Section 4.2. Fix any GPG. As
mentioned, we first consider the case where w“wTM exactly, as overparameterization is easy to deal with
by always designating some subset of extra coordinates to be 0. We construct a transformer pF to compute G.
First, we note that Lemma C.1 constructs a layer to compute the functionality described in 1). Next, the layer
in Lemma C.2 performs the functionality in 2). Likewise, Lemmas C.3, C.7, C.8 construct layers which
perform 3), 4), and 5). Thus, by applying the layers constructed from these lemmas in sequence, we obtain
a transformer such that the output oT contains an onehot encoding for zTpxq: 1|Z|pzTpxqq. We can now
apply a linear weight vector θcls on the output to obtain θJ
clsoT, where pθclsqz “1 for accept states zPZterm
and pθclsqz “´1 for reject states. For inputs xPX, by our construction this computes the desired TMpxq.
Next, following Theorem 3.1, we insert correction functions (Definition D.1) between every group of
constructed layers, which can be implemented via two feedforward ReLU layers following Proposition 3.4.
The parameters for all correction functions add total }¨}1-norm at most polypk,|A|,logTq. Let pFpx,pθq
denote the transformer constructed this way, with parameters pθ. Note that for all xPX, pFpx,pθq“2Gpxq´1."
REFERENCES,0.8119001919385797,Under review as a conference paper at ICLR 2022
REFERENCES,0.8138195777351248,"Next, there are several steps remaining to convert pF into the fixed architecture F tr
w,d,T. First, we need
to convert the layers in pF into transformer layers. This is achievable because every single decoder
self-attention or encoder-decoder attention layer or feedforward ReLU module can be converted into a
transformer layer by setting the two unused modules in the transformer layer to implement the identity
function. This only increases the }¨}1-norm by polypk,|A|,logTq. Note that in particular, we can perform
this conversion such that the correction functions form the last 2 feedforward ReLU layers in every
transformer layer. The first 3 layers in the transformer layer correspond to ones constructed in the lemmas.
Second, we need to expand the dimension to a consistent width w. This is achievable by padding each
layer with coordinates designated to be 0, without affecting any of the }¨}1-norm bounds on the parameters.
Third, we need to expand the depth to a fixed depth d. We can achieve this by appending transformer
layers which compute the identity function (and also include correction functions) as needed."
REFERENCES,0.8157389635316699,"Now we aim to apply Theorem D.6 by viewing the transformer as a very deep network with depth
d “ OpT logTq, by applying each of the steps in the transformer computation in sequence. Note that
our construction for the transformer layers is such that we can view the self-attention, encoder-decoder
attention, and single feedforward ReLU layer as a single function in the setting of Theorem D.6. The
correction function corresponds to the last 2 feedforward ReLU layers in the transformer layer. (We observe
that there are actually m layers which depend on the input x, not a single layer f0 as in the setting of
Theorem D.6, but this is a minor difference where the same argument of Theorem D.6 still easily applies.)
Note that this network uses layer-based weight sharing, which is handled by Theorem D.6. Furthermore,
the depth of this network doesn’t affect the all-layer margin because Theorem D.6 doesn’t depend on
the number of layers. We also observe that Condition D.4 holds for λ“polyp|Z|,|A|,logTq, because all
of the intermediate layers are sparse binary vectors with at most |Z|`|A|`logT nonzero entries."
REFERENCES,0.817658349328215,"Finally, it remains to check that Condition D.3 can hold for all of the defined layers for parameters that
are polynomial in |Z|,|A|,logT. This is straightforward to check for transformer layers where the attention
layers have parameters 0, as standard results on the Lipschitzness of a single ReLU network would apply.
For layers where the functionality comes from the attention mechanism, we observe that for valid inputs
xPX, the largest attention score is always greater than the second largest by a margin of 1. Furthermore,
ties only occur when all of the value vectors for the attended positions are already the same. As a result,
the positions attended to by the layer will not change unless we perturb the parameters and inputs by
Ωppoly´1p|Z|,|A|,logTqq. This reasoning can be used to conclude that Condition D.3 with Lipschitz
constants polyp|Z|,|A|,logTq, and distance parameters Ωppoly´1p|Z|,|A|,logTqq holds. As a result, the
all-layer margin bound from applying Theorem D.6 will also be Ωppoly´1p|Z|,|A|,logTqq, as desired.
Finally, applying Lemma 2.3 with γ “ Ωppoly´1p|Z|,|A|,logTqq and using the fact that the parameter
}¨}1-norms are bounded by α gives the desired result."
REFERENCES,0.8195777351247601,"D
ALL-LAYER MARGIN LOWER BOUNDS VIA CORRECTION FUNCTIONS"
REFERENCES,0.8214971209213052,"We consider a generalized architecture for a d-layer network as follows. Let f0:X ˆΘ0ÑRw map space
of inputs xPX and parameters θPΘ0 to w-dimensional space. For simplicity we assume all intermediate
layers have dimension w, and let fi :RwˆΘi ÑRw be the i-th function in the neural net for dąiě1.
We define fd to output values in R. Let θ“pθ0,...,θdqPΘ denote the full vector of parameters. The i-th
hidden layer hi computes the following value, defined recursively:
h0px,θq“f0px,θ0q
hipx,θq“fiph0px,θq,...,hi´1px,θq,θiq
The model computes output hdpx,θq. We will assume the existence of “correction” functions ζ parame-
terized by ξ“pξ0,...,ξd´1qPΞ0ˆ¨ˆΞd´1 which correct errors in the model output for inputs X:
Definition D.1 (Correction functions). Let F 1 : X Ñ R be a model defined by layer functions f0,...,fd.
Then ζ0,...,ζd´1 :Rw ÑRw, ξ is a set of correction functions and parameters for F 1, θ with radius σζ
if for all iPrd´1s,xPX and phPRX satisfying }ph´hipx,θq}2ďσζ,"
REFERENCES,0.8234165067178503,"ζipph,ξiq“hipx,θq
We now define the function output F with correction layers recursively by
g0px,θ,ξq“f0px,θ0q
rhipx,θ,ξq“ζipgi´1px,θ,ξq,ξiq @0ďiďd´1"
REFERENCES,0.8253358925143954,"gipx,θ,ξq“fiprh0px,θ,ξq,...,rhi´1px,θ,ξq,θi,ξiq @1ďiďd
Fpx,θ,ξq“gdpx,θ,ξq (D.1)"
REFERENCES,0.8272552783109405,Under review as a conference paper at ICLR 2022
REFERENCES,0.8291746641074856,"We note that for all xPX, Fpx,θ,ξq“hdpx,θq."
REFERENCES,0.8310940499040307,"The key observation is that by adding correction layers to the model, we can transform a model with
possibly small all-layer margin on the input data to one with large all-layer margin. We first need to
characterize the Lipschitzness of the individual layers.
Definition D.2. We say that a function fp¨,θq : D Ñ Dout is pκθ,µ,σh,σθq-nice on H Ď D with respect
to |||¨||| if the following hold:"
REFERENCES,0.8330134357005758,"}fph,θq´fph,pθq}2ďκθ}θ´pθ}2maxt|||h|||,1u
@}θ´pθ}ďσθ,hPH"
REFERENCES,0.8349328214971209,"}fph,pθq´fpph,pθq}2ďµ
ˇˇˇ
ˇˇˇ
ˇˇˇh´ph
ˇˇˇ
ˇˇˇ
ˇˇˇ
@
ˇˇˇ
ˇˇˇ
ˇˇˇh´ph
ˇˇˇ
ˇˇˇ
ˇˇˇďσh,}θ´pθ}ďσθ,hPH"
REFERENCES,0.836852207293666,"We will focus on the following norm on tuples of inputs pv1,...,viq, where hj PRw for all jPris:"
REFERENCES,0.8387715930902111,"|||pv1,...,viq|||“max
j }vj}2
(D.2)"
REFERENCES,0.8406909788867563,"We analyze the function F output by a model with correction layers satisfying the following assumptions:
Condition D.3. There are constants κθ,κξ,µ,σh,σθ,σζ such that the following hold."
REFERENCES,0.8426103646833013,"For iě1, suppose that fi is pκθ,µ,σh,σθq-nice at θi on ph0,...,hi´1qpXq with respect to |||¨|||."
REFERENCES,0.8445297504798465,"In addition, suppose that f0 satisfies }f0px,θq´f0px,pθq}2ďµ0}θ´pθ}2 for all xPX,θPΘ0."
REFERENCES,0.8464491362763915,"Furthermore, suppose that for all i, ζi satisfies }ζiph,ξiq´ζiph,pξq}2 ď κξmaxt}h}2,1u}ξi´pξ}2 for all
pξ with }ξi´pξ}2ďσξ and hPRw."
REFERENCES,0.8483685220729367,"These conditions are all standard Lipschitzness-based conditions on the individual layer functions. Our
lower bound for the all-layer margin will be expressed in terms of the constants here."
REFERENCES,0.8502879078694817,"We will also need to assume a bound λ on the norms of each of the layers computed by hi.
Condition D.4. The norms of the true layer values are bounded, that is, Dλ such that for all 0 ď i ď d
and xPX,"
REFERENCES,0.8522072936660269,"maxt}hipx,θq}2,1uďλ
(D.3)"
REFERENCES,0.8541266794625719,"We will also consider models with weight sharing, which allows our analysis to apply to architectures
such as the transformer in Section 4.
Definition D.5 (Layer-based weight sharing). Let Θ1 ĎRw1, Θ0ĎRw0,...,ΘdĎRwd be some spaces of
real-valued parameters. Suppose we wish to perform copying on parameters θ1PΘ1 to produce parameters
θ“pθ0,...θdqPΘ“Θ0ˆ¨¨¨Θd, where θi is the set of parameters given to layer function fi. We say that
a tuple of functions τ “pτ0,...,τdq:Θ1ÑΘ is a layer-based weight sharing scheme if each τi is of the form"
REFERENCES,0.8560460652591171,"τipθ1q“pθ1
π1,...,θ1
πbiq
(D.4)"
REFERENCES,0.8579654510556622,"where π1,...,πbi is a set of distinct indices taking values in rw1s. Note that this ensures that parameters
are not duplicated within a layer."
REFERENCES,0.8598848368522073,"We will now prove our main lower bound for the all-layer margin based on inserting correction functions
at every layer.
Theorem D.6. In the above setting, suppose that Conditions D.3 and D.4 hold for a function F in the
form given by (D.1) parametrized by θ with correction layers ζ0,...ζd´1 parameterized by ξ with correction
radius σζ ă 1. Suppose that Fpxq P t´1,`1u @x P X. Then for all x P X, we can bound the all-layer
margin of F (defined in (2.1))as follows:"
REFERENCES,0.8618042226487524,"ρFppθ,ξq,x,1pFpx,θ,ξqě0qqěmint λ"
REFERENCES,0.8637236084452975,"µ0
,σζ"
REFERENCES,0.8656429942418427,"µ0
,σθ,σξ, 1"
REFERENCES,0.8675623800383877,"2κθ
, σζ"
REFERENCES,0.8694817658349329,"2κθλ, σh"
REFERENCES,0.8714011516314779,"2κξλ,
σζ
4λµκξ
,
1
4µκξ
u
(D.5)"
REFERENCES,0.8733205374280231,"Here the subscript F makes it explicit that the all-layer margin is for the architecture F. Furthermore, if
we consider any layer-based weight-shared model F 1px,θ1qﬁFpx,τp1qpθ1q,τp2qpθ1qq for valid weight-tying
mappings τp1q, τp2q (Definition D.5), the same bound holds for ρF 1pθ1,x,1pF 1px,θ1qě0qq."
REFERENCES,0.8752399232245681,Under review as a conference paper at ICLR 2022
REFERENCES,0.8771593090211133,"Our proof will first consider the case without weight sharing. We use pθ“ppθ0,...,pθdq and pξ“ppξ0,...,pξd´1q
to denote a perturbed set of parameter vectors. Furthermore, define the partially perturbed parameter sets
pθiﬁppθ0,...,pθi,θi`1,...,θdq and pξiﬁppξ0,...,pξi,ξi`1,...,ξdq. We also use pθ´1ﬁθ and pξ´1ﬁξ when convenient."
REFERENCES,0.8790786948176583,We consider perturbations such that the following norm bounds hold:
REFERENCES,0.8809980806142035,}pθ0´θ0}2ďmint λ
REFERENCES,0.8829174664107485,"µ0
,σζ"
REFERENCES,0.8848368522072937,"µ0
u
(D.6)"
REFERENCES,0.8867562380038387,"}pθi´θi}2ďmintσθ, 1"
REFERENCES,0.8886756238003839,"2κθ
, σζ"
REFERENCES,0.8905950095969289,"2κθλu
(D.7)"
REFERENCES,0.8925143953934741,"}pξi´pξi}2ďmintσξ, σh"
REFERENCES,0.8944337811900192,"2κξλ,
σζ
4λµκξ
,
1
4µκξ
u
(D.8)"
REFERENCES,0.8963531669865643,"We show that such perturbations won’t change the label predicted by the model, and so therefore the
minimum of these quantities immediately gives a lower bound on the all-layer margin. Our proof will
be by induction, with the following lemma providing the base case.
Lemma D.7. In the setting of Theorem D.6, suppose that (D.6) holds. Then the following hold:"
REFERENCES,0.8982725527831094,"rh0px,pθ,ξq“h0px,θq"
REFERENCES,0.9001919385796545,"}g0px,pθ,pξq´h0px,θq}2ďmintλ,σζu"
REFERENCES,0.9021113243761996,"The next lemma provides the inductive step. Starting with the base case, we show that because of the
presence of the correction functions, the perturbations with our given bounds won’t change the next
layer output by too much. This allows the correction function to fix the output of the next layer, and this
argument can extend inductively.
Lemma D.8. In the setting of Theorem D.6, fix some 1ďiďd. Suppose that for all 0ďj ăi, it holds
that for all xPX,"
REFERENCES,0.9040307101727447,"rhjpx,pθ,pξj´1q“hjpx,θq
(D.9) and"
REFERENCES,0.9059500959692899,"}gjpx,pθ,pξq´hjpx,θq}2ďmintλ,σζu"
REFERENCES,0.9078694817658349,"In addition, suppose that pθ,θ,pξ,ξ satisfy (D.7) and (D.8). Then it follows that for all xPX,"
REFERENCES,0.9097888675623801,"}gipx,pθ,pξq´hipx,θq}2ďmintλ,σζu"
REFERENCES,0.9117082533589251,"Furthermore, for 1ďiďd´1, we additionally have"
REFERENCES,0.9136276391554703,"rhipx,pθ,pξi´1q“hipx,θq"
REFERENCES,0.9155470249520153,"Combined, the two lemmas above allow us to inductively show that the prediction of the model is not
changed whenever the perturbations are bounded by (D.6), (D.7), and (D.8). Next, we show that this
translates directly to an all-layer margin lower bound.
Lemma D.9. In the setting of Theorem D.6, suppose there exist norm bounds a0,...,ad, b0,...,bd´1 such
that whenever }pθi´θi}2ďai and }pξi´ξi}2ďbi, |Fpx,θ,ξq´Fpx,pθ,pξq|ă1 for all xPX. Then we obtain
the following lower bound on the all-layer margin, for all xPX:"
REFERENCES,0.9174664107485605,"ρFppθ,ξq,x,1pFpx,θ,ξqě0qqěminta0,...,ad,b0,...,bd´1u"
REFERENCES,0.9193857965451055,"The same lower bound applies if we consider models that use layer-based weight sharing, defined by
F 1px,θ1qﬁFpx,τp1qpθ1q,τp2qpθ1qq for valid weight-tying mappings τp1q, τp2q (Definition D.5)."
REFERENCES,0.9213051823416507,We can combine these steps to formally complete the proof of Theorem D.6.
REFERENCES,0.9232245681381958,"Proof of Theorem D.6. Assuming the perturbation bounds (D.6) (D.7), and (D.8) hold, we can apply
induction with Lemma D.7 as the base case and Lemma D.8 as the inductive step to conclude that
|Fpx,pθ,pξq´Fpx,θ,ξq|ďσζ ă1 for all xPX. We can now apply Lemma D.9 to obtain the desired bound
on the all-layer margin."
REFERENCES,0.9251439539347409,We fill in the proofs of the supporting lemmas below.
REFERENCES,0.927063339731286,Under review as a conference paper at ICLR 2022
REFERENCES,0.9289827255278311,"Proof of Lemma D.7. By our definitions and Condition D.3, we have"
REFERENCES,0.9309021113243762,"}g0px,pθ,pξq´h0px,θq}2“}f0px,pθ0q´f0px,θ0q}2ďµ0}θ0´pθ0}2ďmintλ,σζu"
REFERENCES,0.9328214971209213,Now we can apply the Definition D.1 of the correction function to get
REFERENCES,0.9347408829174664,"rh0px,pθ,ξq“ζ0pg0px,pθ,pξq,ξ0q“h0px,θq"
REFERENCES,0.9366602687140115,"Proof of Lemma D.8. By expanding the expression for hi, we observe that"
REFERENCES,0.9385796545105566,"hipx,θq“fiph0px,θq,...,hi´1px,θq,θiq"
REFERENCES,0.9404990403071017,"“fiprh0px,pθ,ξq,rh1px,pθ,pξ0q...,rhi´1px,pθ,pξi´2q,θiq
(D.10)"
REFERENCES,0.9424184261036468,We obtained the equality via (D.9). Now we write
REFERENCES,0.944337811900192,"gipx,pθ,pξq“fiprh0px,pθ,pξq,...,rhi´1px,pθ,pξq,pθiq
(D.11)"
REFERENCES,0.946257197696737,"We subtract the two expressions and add and subtract fiprh0px,pθ,ξq,rh1px,pθ,ξ0q...,rhi´1px,pθ,ξi´1q,pθiq to
obtain"
REFERENCES,0.9481765834932822,"gipx,pθ,pξq´hipx,θq“E1`E2
where"
REFERENCES,0.9500959692898272,"E1ﬁfiprh0px,pθ,pξq,...,rhi´1px,pθ,pξq,pθiq"
REFERENCES,0.9520153550863724,"´fiprh0px,pθ,ξq,rh1px,pθ,pξ0q...,rhi´1px,pθ,pξi´2q,pθiq"
REFERENCES,0.9539347408829175,"E2ﬁfiprh0px,pθ,ξq,rh1px,pθ,pξ0q...,rhi´1px,pθ,pξi´2q,pθiq"
REFERENCES,0.9558541266794626,"´fiprh0px,pθ,ξq,rh1px,pθ,pξ0q...,rhi´1px,pθ,pξi´2q,θiq"
REFERENCES,0.9577735124760077,We first bound E1. We note that for all 0ďjďi´1
REFERENCES,0.9596928982725528,"}rhjpx,pθ,pξq´rhjpx,pθ,pξj´1q}2“}ζjpgjpx,pθ,pξq,pξjq´ζjpgjpx,pθ,pξq,ξjq}2"
REFERENCES,0.9616122840690979,"ďκξmaxt}gjpx,pθ,pξq}2,1u}pξj´ξj}2"
REFERENCES,0.963531669865643,"The last inequality used Condition D.3 and }pξj´ξj}2ďσξ. Now defining H1ﬁprh0px,pθ,pξq,...,rhi´1px,pθ,pξqq
and H ﬁprh0px,pθ,ξq,rh1px,pθ,pξ0q...,rhi´1px,pθ,pξi´2qq, it follows that
ˇˇˇˇˇˇH´H1ˇˇˇˇˇˇ“ max
0ďjďi´1κξmaxt}gjpx,pθ,pξq}2,1u}pξj´ξj}2"
REFERENCES,0.9654510556621881,"Plugging in }gjpx,pθ,pξq}2 ď}hjpx,θq}2`}gjpx,pθ,pξq´hjpx,θq}2 ď2λ, λě1, and }pξj ´ξj}2 ď
σh
2κξλ, we
obtain |||H´H1|||ďσh. Furthermore, we note that H Pph0,...,hi´1qpXq, so we can apply Condition D.3
and Definition D.2 to obtain"
REFERENCES,0.9673704414587332,"}E1}2“}fipH1,pθiq´fipH,pθiq}2"
REFERENCES,0.9692898272552783,"ďµ
ˇˇˇˇˇˇH´H1ˇˇˇˇˇˇ
(since }pθi´θi}2ďσθ and |||H´H1|||ďσh)"
REFERENCES,0.9712092130518234,"ď2λµκξmax
j }pξj´ξj}2"
REFERENCES,0.9731285988483686,"Next, we bound E2 by applying Condition D.3 and Definition D.2 again, using }pθi´θi}2ďσθ:"
REFERENCES,0.9750479846449136,"}E2}2“}fipH,pθiq´fipH,θiq}2"
REFERENCES,0.9769673704414588,"ďκθ}pθi´θi}2maxt|||H|||,1u"
REFERENCES,0.9788867562380038,"“κθ}pθi´θi}2maxt}hjpx,θq}2ujYt1u"
REFERENCES,0.980806142034549,ďκθ}pθi´θi}2λ
REFERENCES,0.982725527831094,"where we applied Condition D.4. By triangle inequality, follows that"
REFERENCES,0.9846449136276392,"}gipx,pθ,pξq´hipx,θq}2ď}E1}2`}E2}2"
REFERENCES,0.9865642994241842,"ďκθ}pθi´θi}2λ`2λµκξmax
j }pξj´ξj}2"
REFERENCES,0.9884836852207294,Under review as a conference paper at ICLR 2022
REFERENCES,0.9904030710172744,"Now by the assumptions on }pθi´θi}2 and }pξj´ξj}2, we can check that the r.h.s. is bounded by mintλ,σζu."
REFERENCES,0.9923224568138196,"Finally, we note that by Definition D.1 of the correction function, we have"
REFERENCES,0.9942418426103646,"rhipx,pθ,pξi´1q“ζipgipx,pθ,pξq,ξiq“hipx,θq"
REFERENCES,0.9961612284069098,"where we used the fact that }gipx,pθ,pξq´hipx,θq}2ďσζ."
REFERENCES,0.9980806142034548,"Proof of Lemma D.9. Note that if }pθ,ξq´ppθ,pξq}2ă¯aﬁminta0,...,ad,b0,...,bd´1u, then by the conditions
of the lemma, |Fpx,θ,ξq´Fpx,pθ,pξq|ă1. However, because Fpx,θ,ξqPt´1,`1u for all xPX, the sign
of the output is unchanged, which means Fpx,θ,ξqFpx,pθ,pξqą0. This means that we must perturb pθ,ξq
by }¨}2-norm at least ¯a to satisfy the constraint in the all-layer margin definition, giving us the lower bound.
We note that a similar argument applies to layer-based weight sharing because there are no parameters
shared within a layer, so if the perturbation to θ1 has ℓ2 norm less than ¯a, the parameters in τp1qpθ1q, τp2qpθ1q
will also have a perturbation of at most ¯a in each layer. The same reasoning as before then applies."
