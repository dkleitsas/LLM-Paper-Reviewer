Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0024509803921568627,"3D snapshot microscopy enables fast volumetric imaging by capturing a 3D volume
in a single 2D camera image and performing computational reconstruction. Fast
volumetric imaging has a variety of biological applications such as whole brain
imaging of rapid neural activity in larval zebraﬁsh. The optimal microscope design
for this optical 3D-to-2D encoding is both sample- and task-dependent, with no
general solution known. Deep learning based decoders can be combined with a
differentiable simulation of an optical encoder for end-to-end optimization of both
the deep learning decoder and optical encoder. This technique has been used to
engineer local optical encoders for other problems such as depth estimation, 3D
particle localization, and lensless photography. However, 3D snapshot microscopy
is known to require a highly non-local optical encoder which existing UNet-based
decoders are not able to engineer. We show that a neural network architecture
based on global kernel Fourier convolutional neural networks can efﬁciently decode
information from multiple depths in a volume, globally encoded across a 3D
snapshot image. We show in simulation that our proposed networks succeed
in engineering and reconstructing optical encoders for 3D snapshot microscopy
where the existing state-of-the-art UNet architecture fails. We also show that our
networks outperform the state-of-the-art learned reconstruction algorithms for a
computational photography dataset collected on a prototype lensless camera which
also uses a highly non-local optical encoding."
INTRODUCTION,0.004901960784313725,"1
INTRODUCTION"
INTRODUCTION,0.007352941176470588,"Volumetric imaging has been a valuable technique for measuring neural activity at single neuron
resolution across the brain [2, 22, 36]. We are particularly motivated by the problem of imaging neu-
ronal activity across the whole brain of transparent model organisms such as larval zebraﬁsh (Danio
rerio) at higher volume rates, to enable richer studies of neural dynamics and signal propagation.
Existing approaches to whole-brain imaging involve sequential acquisition of stacks of 2D images
[2], typically resulting in 0.5Hz - 2Hz volume capture rates, limited by the number of 2D depth plane
acquisitions. However, since neural activity occurs at orders of magnitude faster timescales, faster
volume imaging rates can provide understanding of neural computations with signiﬁcantly more
detail."
INTRODUCTION,0.00980392156862745,"3D snapshot microscopy techniques can signiﬁcantly speed up the imaging of a 3D sample volume
using non-local optical encodings to combine information corresponding to different depth planes in
the volume into a single 2D image, allowing for fast volume acquisition at the frame rate of the camera
sensor followed by computational decoding to reconstruct the 3D volume [5, 3, 4, 7, 13, 16, 18, 17,
20, 27, 26, 30, 38, 37, 33]. The optimal optical design for a 3D snapshot microscope is unknown in
general [5], and likely depends on the speciﬁc sample structure. Newly developed programmable
microscopes with up to 106 free parameters, e.g. pixels on a spatial light modulator (SLM), enable
the implementation of a rich space of microscope encodings, and present the possibility of direct
optimization of snapshot microscope parameters speciﬁcally for particular classes of samples and
imaging tasks."
INTRODUCTION,0.012254901960784314,"Deep learning-based decoders combined with differentiable simulations of optical encoders have
enabled end-to-end optimization of both the deep learning decoder and the optical encoder. This end-
to-end optical engineering approach has recently been applied to depth from defocus [6, 10, 14, 35],
lensless photography [13], or particle localization microscopy [23]. However, all such applications"
INTRODUCTION,0.014705882352941176,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.01715686274509804,"either use far fewer optical parameters or operate on relatively compact or local encodings due to
small ﬁlters in their deep networks [6, 23, 35, 10, 14, 34, 13]. There are two barriers to optimizing
more parameters and producing global versus local encodings: (1) the typical convolutional networks
with small ﬁlters used by previous methods for computational reconstruction have a prior to decode
locally, and therefore limit optical designs to local encodings, unlike the global PSFs used in snapshot
microscopy and (2) the optimization of optical parameters involves backpropagation through both
the deep learning reconstruction and a differentiable optical simulation, which is more expensive
computationally for non-local optical encodings. We focus on two computational optics problems
that demonstrate a superior deep network architecture that can successfully reconstruct from images
encoded by non-local optical encoders and with orders of magnitude more optical parameters than
previously attempted. To address the computational burden of simulating and optimizing non-local
encoders, we also developed a parallel, multi-GPU optical simulation framework."
INTRODUCTION,0.0196078431372549,"Problem statement We deﬁne an optical encoder as Mφ parameterized by φ and a computational
decoder as Rθ parameterized by θ. We wish to develop a reconstruction network architecture for
decoding non-local encodings in 2D images c produced by Mφ and also to enable the end-to-end
optimization of φ to produce non-local encodings. We’ll consider two concrete applications: (1)
3D snapshot microscopy using an SLM-based programmable microscope (Figure 1A), where we
wish to optimize both the reconstruction network Rθ and the optical encoding Mφ based on the
SLM parameters in order to image 3D samples v and reconstruct 3D volumes ˆv, and (2) lensless
photography (Figure 4), where we wish to optimize only the reconstruction network Rθ to produce
images of natural scenes imaged by a lensless camera M implementing a ﬁxed optical encoding."
OUR CONTRIBUTIONS,0.022058823529411766,"1.1
OUR CONTRIBUTIONS"
OUR CONTRIBUTIONS,0.024509803921568627,"1. We developed a large-scale, parallel, multi-GPU differentiable wave optics simulation of a
programmable microscope, based on a 4f optical model with a phase mask (φ) implemented
using a spatial light modulator (SLM), described further in Appendix A.1. SLMs φ can
have over 106 optimizable parameters.
2. We collected a large dataset of high resolution 3D confocal volumes of zebraﬁsh larvae
for the purpose of end-to-end sample-speciﬁc optimization of the parameters of the pro-
grammable 3D snapshot microscope.
3. We introduce an efﬁcient new reconstruction network architecture for decoding from non-
local optical encoders using very large global convolutions implemented via Fourier convo-
lutions.
4. We show that our networks outperform the state-of-the-art deep networks for both volume
reconstruction and microscope parameter optimization, and for image reconstruction from
lensless photography [20].
5. Our method enables, for the ﬁrst time, direct end-to-end optimization of highly non-local
optical encoders in the space of SLM pixels with over 106 parameters. In simulation, we
demonstrate the potential for signiﬁcant improvements in imaging resulting from sample-
speciﬁc engineering of optical encoders."
PRIOR WORK,0.02696078431372549,"1.2
PRIOR WORK"
PRIOR WORK,0.029411764705882353,"Neural network architectures for computational imaging have all used convolution layers with small
ﬁlters. End-to-end optimization of optical encoders have largely been performed with UNet-based
architectures [6, 10, 14, 35, 13], with one method using ResNet-based architectures [23]. Such
optimization has always led to local optical encodings. End-to-end optimization has never been
attempted for large-ﬁeld of view 3D snapshot microscopy due to the difﬁculty of simulating and
reconstructing from non-local encoders. However, small ﬁlter convolutional deep networks have been
used in a non-end-to-end manner to reconstruct volumes from 3D snapshot microscopes designed
using microlens arrays [33, 34]. For photography and MRI, using deep learning combined with
unrolling iterations of traditional deconvolution algorithms provides the beneﬁts of fast amortized
inference and higher quality reconstructions due to learning of structural priors [8, 9, 20, 34]."
PRIOR WORK,0.031862745098039214,"In our work, we demonstrate that convolution layers with large ﬁlters implemented efﬁciently in the
Fourier domain enable the end-to-end learning of highly non-local optical encoders for 3D snapshot
microscopy. Large convolution ﬁlters have been shown to be helpful for other computer vision"
PRIOR WORK,0.03431372549019608,Under review as a conference paper at ICLR 2022
PRIOR WORK,0.03676470588235294,"applications such as semantic segmentation and salient object detection [25, 21], and the Fourier
domain parameterization of small ﬁlters has been described previously [28]."
PRIOR WORK,0.0392156862745098,"A pioneering strategy in 3D snapshot microscopy has been light ﬁeld microscopy [16], which employs
a microlens array at the microscope’s image plane to create subimages encoding both amplitude and
phase of light [16, 1]. A variety of microlens-array-based light ﬁeld microscopes have been used
to perform whole-brain imaging [16, 27, 37, 36, 30, 26, 7, 12, 38]. [38] optimizes the placement
of microlenses, but not in an end-to-end manner. Despite variation in design, microlens-based
microscopes have, to various degrees, three main limitations that can be improved: 1) blocking or
scattering of light between microlenses, causing light inefﬁciency, 2) not making use of all pixels on
the camera to encode a 3D sample, leading to inefﬁcient compression and suboptimal reconstructions
ˆv, and 3) a ﬁxed optical encoding scheme."
PRIOR WORK,0.041666666666666664,"An alternative to using microlenses is to implement a coded detection strategy using a phase mask or
diffuser to spread light broadly across the camera sensor [3, 4, 5, 13, 17, 18, 38]. The designed phase
masks can be implemented either by manufacturing a custom optical element or using a programmable
SLM [10, 14, 35, 13, 38, 23]. Using a programmable element allows different microscope parameters
to be used for different samples."
METHODS,0.04411764705882353,"2
METHODS"
D SNAPSHOT MICROSCOPY,0.04656862745098039,3D snapshot microscopy
D SNAPSHOT MICROSCOPY,0.049019607843137254,FourierNet
D SNAPSHOT MICROSCOPY,0.051470588235294115,FourierUNet
D RECONSTRUCTION,0.05392156862745098,"3D reconstruction
3D sample"
D IMAGE,0.056372549019607844,2D image
D IMAGE,0.058823529411764705,"upsample 2×
and concatenate conv"
D IMAGE,0.061274509803921566,crop ½×
D IMAGE,0.06372549019607843,"reconstruction network
(computational decoder)
programmable microscope
(optical encoder)"
D IMAGE,0.0661764705882353,phase mask convs convs convs
D IMAGE,0.06862745098039216,"conv
convs"
D IMAGE,0.07107843137254902,Fourier convolution
D IMAGE,0.07352941176470588,multiscale Fourier convolution
D IMAGE,0.07598039215686274,"upsample 2×
and concatenate
crop ½×"
D IMAGE,0.0784313725490196,"Figure 1: Overview of our problem setup and our proposed network architectures. Top row (A)
shows the problem of 3D snapshot microscopy, where we computationally reconstruct a 3D sample
from a 2D image. Middle row (B) shows our proposed FourierNet architecture, which includes a
Fourier convolution layer that enables efﬁcient computation of global features. Bottom row (C)
shows an extension of our proposed architecture, the FourierUNet, which mimics the multiscale
feature extraction of a standard UNet efﬁciently and with global features using a multiscale Fourier
convolution."
D IMAGE,0.08088235294117647,"We show our network architecture and an overview of autoencoder training both the microscope
parameters φ and reconstruction network parameters θ in Figure 1A. The programmable microscope
is simulated by a differentiable implementation of a wave-optics model of light propagation. We
have selected a programmable microscope design based on pupil-plane phase modulation with a
programmable spatial light modulator, for which imaging is well-approximated by a computationally-
efﬁcient convolution [11]. A detailed description of our simulation is provided in Appendix A.1. For
lensless photography, there is no optical simulation because the images have been collected on a real
camera."
D IMAGE,0.08333333333333333,Under review as a conference paper at ICLR 2022
"FOURIERNET AND FOURIERUNET ARCHITECTURES FOR RECONSTRUCTION FROM
OPTICAL ENCODERS",0.0857843137254902,"2.1
FOURIERNET AND FOURIERUNET ARCHITECTURES FOR RECONSTRUCTION FROM
OPTICAL ENCODERS"
"FOURIERNET AND FOURIERUNET ARCHITECTURES FOR RECONSTRUCTION FROM
OPTICAL ENCODERS",0.08823529411764706,"Because images created by optical encoders can potentially encode signals from the incoming light
ﬁeld to any location in the camera image in a non-local manner, it is essential that a reconstruction
network have global context. Existing multi-scale architectures such as the UNet [29] can achieve
global context, but at the expense of many computation layers with small convolution kernels which
we believe have a local information bias which is inappropriate for computational optics. In this paper,
we introduce relatively shallow architectures for computational imaging, which rely on convolution
layers with very large ﬁlters, implemented efﬁciently in the Fourier domain [28]."
"FOURIERNET AND FOURIERUNET ARCHITECTURES FOR RECONSTRUCTION FROM
OPTICAL ENCODERS",0.09068627450980392,"FourierNet We propose a simple three layer convolutional network architecture with very large
global convolutions at the very ﬁrst layer, followed by two standard local convolutional layers (Figure
1B). We deﬁne a global convolution as a convolution with kernel size equal to the input image. Such
a convolution achieves global context in a single step but is computationally expensive. Global
convolutions are implemented more efﬁciently in the Fourier domain, yielding a speed up of two
orders of magnitude. Due to the use of Fourier convolutions to enable global context, we call our
architecture FourierNet. In contrast to a typical UNet which can contain many tens of convolution
layers, the FourierNet is only three layers deep, which requires backpropagation through fewer layers
compared to a typical UNet with the same receptive ﬁeld."
"FOURIERNET AND FOURIERUNET ARCHITECTURES FOR RECONSTRUCTION FROM
OPTICAL ENCODERS",0.09313725490196079,"FourierUNet We also propose a multi-scale variant of the FourierNet by bringing together elements
of the multi-scale UNet and the single-scale FourierNet. Here, we take advantage of the fact that
down-sampling in image space corresponds to a simple cropping operation in the Fourier domain,
resulting in a band-limited computation of a feature map. We efﬁciently implement multi-scale global
Fourier convolutions (Figure 1C) to replace the encoding/“analysis” pathway of a UNet. We then
use the standard decoding/“synthesis” pathway of the UNet to combine the multi-scale features into
a single 3D volume reconstruction (Appendix A.3, A.4). Thus we can study whether multi-scale
features or global context is more important for decoding non-local optical encoders."
"FOURIERNET AND FOURIERUNET ARCHITECTURES FOR RECONSTRUCTION FROM
OPTICAL ENCODERS",0.09558823529411764,"Fourier domain convolutions It is well-known that large kernel convolutions can be implemented
more efﬁciently in the Fourier domain [24, 19, 32]. A naive implementation of global convolution
requires O(N 2) operations, where N is number of pixels in both the image and the kernel. An
alternative global convolution implementation is to Fourier transform the input x and convolution
kernel w, perform element-wise multiplication in Fourier space, and ﬁnally inverse Fourier transform,
requiring only O(N log N) operations [19, 32]. Following [28], we store and optimize the weights in
Fourier space W. This over-parameterization costs 8× the memory of an equivalent real valued large
ﬁlter but saves the computational cost of Fourier transforming the real-valued weights (Appendix
A.3). Thus a Fourier convolution is deﬁned:
Re{F−1 {W ⊙F {x}}}
(1)
For image and kernel sizes of 256 × 256 pixels, our implementation leads to nearly 500× speedup:
standard PyTorch convolution takes 2860ms, while Fourier convolution takes 5.92ms on a TITAN X."
"FOURIERNET AND FOURIERUNET ARCHITECTURES FOR RECONSTRUCTION FROM
OPTICAL ENCODERS",0.09803921568627451,"Multi-scale Fourier domain convolutions It is well-known [24] that downsampling corresponds
to cropping in the Fourier domain. Thus the Fourier convolution can be extended to efﬁciently
produce multi-scale feature representations in one step (Figure 1C). We deﬁne our multi-scale Fourier
convolution as 
Re{F−1 {W1 ⊙crop1 [c]}}, . . . , Re{F−1 {Wn ⊙cropn [c]}}
	
(2)
where subscript denotes scale level (higher subscript indicates lower spatial scale/more cropping in
Fourier space) and we precompute c := F {c} once."
"PHYSICS-BASED AUTOENCODER FOR SIMULTANEOUS ENGINEERING OF MICROSCOPE
ENCODER AND RECONSTRUCTION NETWORK DECODER",0.10049019607843138,"2.2
PHYSICS-BASED AUTOENCODER FOR SIMULTANEOUS ENGINEERING OF MICROSCOPE
ENCODER AND RECONSTRUCTION NETWORK DECODER"
"PHYSICS-BASED AUTOENCODER FOR SIMULTANEOUS ENGINEERING OF MICROSCOPE
ENCODER AND RECONSTRUCTION NETWORK DECODER",0.10294117647058823,"We describe the imaging process as the following transformation from the 3D light intensity volume
of the sample v to the 2D image formed on the camera c:
µc = Mφ(v)
(3)
c = max ([µc + √µcϵ] , 0), ϵ ∼N(0, 1)
(4)
where Mφ denotes the microscope parameterized by a 2D phase mask, φ. This phase mask φ
describes the 3D-to-2D encoding of this microscope model completely. A Poisson distribution"
"PHYSICS-BASED AUTOENCODER FOR SIMULTANEOUS ENGINEERING OF MICROSCOPE
ENCODER AND RECONSTRUCTION NETWORK DECODER",0.1053921568627451,Under review as a conference paper at ICLR 2022
"PHYSICS-BASED AUTOENCODER FOR SIMULTANEOUS ENGINEERING OF MICROSCOPE
ENCODER AND RECONSTRUCTION NETWORK DECODER",0.10784313725490197,"with mean rate µc describes the physics of photon detection at the camera, but sampling from this
distribution is not differentiable. We approximate the noise distribution with a rectiﬁed Gaussian.
We include details on Mφ in Appendix A.1 [11]. Jointly training reconstruction networks and
microscope parameters involves image simulation, reconstruction, then gradient backpropagation to
update the reconstruction network and microscope parameters. Details on parallelization, planewise
reconstruction networks, and planewise sparse gradients are provided in Appendix A.2."
"PHYSICS-BASED AUTOENCODER FOR SIMULTANEOUS ENGINEERING OF MICROSCOPE
ENCODER AND RECONSTRUCTION NETWORK DECODER",0.11029411764705882,"Our loss function computes the normalized mean squared error (MSE) LHNMSE between the high
pass ﬁltered sample and reconstruction weighted by the normalized MSE LNMSE between the original
sample and reconstruction (to include low frequency content in the objective). Formally, our loss
function L(v, ˆv) is deﬁned:"
"PHYSICS-BASED AUTOENCODER FOR SIMULTANEOUS ENGINEERING OF MICROSCOPE
ENCODER AND RECONSTRUCTION NETWORK DECODER",0.11274509803921569,"L(v, ˆv) = LHNMSE(v, ˆv) + βLNMSE(v, ˆv)
(5)"
"PHYSICS-BASED AUTOENCODER FOR SIMULTANEOUS ENGINEERING OF MICROSCOPE
ENCODER AND RECONSTRUCTION NETWORK DECODER",0.11519607843137254,"LHNMSE(v, ˆv) = E

(H(v) −H(ˆv))2"
"PHYSICS-BASED AUTOENCODER FOR SIMULTANEOUS ENGINEERING OF MICROSCOPE
ENCODER AND RECONSTRUCTION NETWORK DECODER",0.11764705882352941,"E(H(v)2)
, LNMSE(v, ˆv) = E

(v −ˆv)2"
"PHYSICS-BASED AUTOENCODER FOR SIMULTANEOUS ENGINEERING OF MICROSCOPE
ENCODER AND RECONSTRUCTION NETWORK DECODER",0.12009803921568628,"E(v2)
(6)"
"PHYSICS-BASED AUTOENCODER FOR SIMULTANEOUS ENGINEERING OF MICROSCOPE
ENCODER AND RECONSTRUCTION NETWORK DECODER",0.12254901960784313,"where H(·) denotes high pass ﬁltering and E(·) denotes computing the mean. Both loss terms are
normalized as shown to reduce variance in L, which can otherwise take on large magnitude values
and cause training instability. For our experiments, we set the weight β for the LNMSE term to 0.1."
RESULTS,0.125,"3
RESULTS"
RESULTS,0.12745098039215685,"Larval Zebraﬁsh Dataset High resolution volumes of transgenic larval zebraﬁsh whole brains
expressing nuclear-restricted GCaMP6 calcium indicator in all neurons were imaged using a confocal
microscope. These images are representative of brain-wide activity imaging. We train on 58 different
zebraﬁsh volumes (which we augment heavily) and test on 10 held-out volumes. For all experiments,
we downsample the high resolution confocal data to (1.0 µm z, 1.625 µm y, 1.625 µm x). We created
4 datasets from these data called Type A, B, C, and D corresponding to imaging different ﬁelds of
view. Full speciﬁcations for these datasets are in Table 4 (Appendix A.2). For Figure 2, we restrict
the ﬁeld of view to (200 µm z, 416 µm y, 416 µm x) with a tall cylinder cutout of diameter 193 µm
and height 200 µm and image with 256 × 256 pixels on the simulated camera sensor. We call this
setting Type D. Figure 3 and Table 2 show our larger experiments with 512 × 512 simulated camera
pixels, with a ﬁeld of view of (250 µm z, 832 µm y, 832 µm x) and sample types Type A, Type B,
and Type C."
RESULTS,0.12990196078431374,"DiffuserCam Lensless Mirﬂickr Dataset We also test reconstruction performance on experimental
computational photography data1 from [20] (Figure 4). This is a dataset constructed by displaying
RGB color natural images from the MIRFlickr dataset on a monitor and then capturing diffused
images by the DiffuserCam lensless camera. The dataset contains 24,000 pairs of DiffuserCam and
ground truth images. The goal of the dataset is to learn to reconstruct the ground truth images from
the diffused images. As in [20], we train on 23,000 paired diffused and ground truth images, and test
on 999 held-out pairs of images."
FOURIERNETS OUTPERFORM UNETS FOR ENGINEERING NON-LOCAL OPTICAL ENCODERS,0.1323529411764706,"3.1
FOURIERNETS OUTPERFORM UNETS FOR ENGINEERING NON-LOCAL OPTICAL ENCODERS"
FOURIERNETS OUTPERFORM UNETS FOR ENGINEERING NON-LOCAL OPTICAL ENCODERS,0.13480392156862744,"Table 1: Quality of reconstructed volumes after optimizing microscope parameters to image sample
Type D on 256 × 256 pixel camera (mean ± s.e.m., n = 10)"
FOURIERNETS OUTPERFORM UNETS FOR ENGINEERING NON-LOCAL OPTICAL ENCODERS,0.13725490196078433,"Microscope
Reconstruction
LHNMSE ↓
MS-SSIM ↑
PSNR ↑
Time ↓(s)"
FOURIERNETS OUTPERFORM UNETS FOR ENGINEERING NON-LOCAL OPTICAL ENCODERS,0.13970588235294118,"FourierNet2D
FourierNet3D
0.6409 ± 0.0213
0.955 ± 0.004
34.78 ± 0.88
0.71
FourierNet2D
FourierUNet3D
0.6325 ± 0.0222
0.956 ± 0.003
34.74 ± 0.83
1.37
FourierNet2D
UNet3D
0.7659 ± 0.0130
0.922 ± 0.008
30.06 ± 0.93
3.68
UNet2D
UNet3D
0.7120 ± 0.0160
0.913 ± 0.009
29.17 ± 1.13
3.68"
FOURIERNETS OUTPERFORM UNETS FOR ENGINEERING NON-LOCAL OPTICAL ENCODERS,0.14215686274509803,"We compare optimizing microscope parameters φ with two neural networks: 1) using our FourierNet
with 2D convolutions (FourierNet2D) and 2) using a vanilla UNet with 2D convolutions (UNet2D)."
FOURIERNETS OUTPERFORM UNETS FOR ENGINEERING NON-LOCAL OPTICAL ENCODERS,0.14460784313725492,1Publicly available: https://waller-lab.github.io/LenslessLearning/dataset.html
FOURIERNETS OUTPERFORM UNETS FOR ENGINEERING NON-LOCAL OPTICAL ENCODERS,0.14705882352941177,"Under review as a conference paper at ICLR 2022 x
y z
x x
y 50 μm"
FOURIERNETS OUTPERFORM UNETS FOR ENGINEERING NON-LOCAL OPTICAL ENCODERS,0.14950980392156862,100 μm
FOURIERNETS OUTPERFORM UNETS FOR ENGINEERING NON-LOCAL OPTICAL ENCODERS,0.15196078431372548,FourierNet
FOURIERNETS OUTPERFORM UNETS FOR ENGINEERING NON-LOCAL OPTICAL ENCODERS,0.15441176470588236,ground truth UNet z y x
FOURIERNETS OUTPERFORM UNETS FOR ENGINEERING NON-LOCAL OPTICAL ENCODERS,0.1568627450980392,xy max projection
FOURIERNETS OUTPERFORM UNETS FOR ENGINEERING NON-LOCAL OPTICAL ENCODERS,0.15931372549019607,xz max projection
FOURIERNETS OUTPERFORM UNETS FOR ENGINEERING NON-LOCAL OPTICAL ENCODERS,0.16176470588235295,xy max projection
S,0.1642156862745098,3.00 s
S,0.16666666666666666,"0.01 s
0.01 s"
S,0.16911764705882354,light propagation
S,0.1715686274509804,"simulated camera image
reconstructed volume"
S,0.17401960784313725,"Figure 2: Comparing simulated camera images and reconstructions of a Type D volume captured
using our FourierNet (middle) versus UNet (right) microscopes. Top row shows simulated 256 × 256
pixel camera images; bottom right of camera image shows approximate acquisition time (given a
reasonable number of simulated photons per pixel, i.e. SNR).Ground truth has no corresponding
camera image, because the 3D volume is imaged directly via confocal microscopy (acquisition time
in xy view). Colored arrows in left column show projection axis for each row. White arrows show
individual neurons clearly visible for FourierNet, but not for UNet."
S,0.17647058823529413,"Training is a two step process in which a planewise reconstruction network with fewer parameters and
2D convolutions is used during optimization of φ, then once φ is ﬁxed a more powerful reconstruction
network with 3D convolutions is used. We ﬁnd this scheme achieves better reconstruction quality
because the reconstruction network does not need to constantly adapt to a changing optical encoding
(Appendix A.3). We train these microscopes and reconstruction networks on Type D, where samples
are tall cylindrical cutouts of zebraﬁsh with diameter 193 µm and height 200 µm. Samples are imaged
on a camera with 256 × 256 pixels (Figure 2). FourierNet2D has 2 convolution layers (with 99.8%
of its kernel parameters in the initial Fourier convolution layer), while UNet2D has 32 convolution
layers (kernel parameters approximately uniformly distributed per layer). FourierNet2D and UNet2D
both have ∼4 × 107 parameters; FourierNet3D has ∼6 × 107 parameters vs. ∼108 for UNet3D.
Architecture details are in Appendix A.3, A.4."
S,0.17892156862745098,"Simulated camera images (Figure 2) show that the UNet microscope does not make sufﬁcient use
of camera pixels, producing only a single view of the sample. We believe this is due to a local
information prior in the small kernels of UNets. The FourierNet microscope uses more camera pixels
and performs better than the UNet microscope for reconstruction (Figure 2, quantiﬁed in Table 1)."
S,0.18137254901960784,Under review as a conference paper at ICLR 2022
S,0.18382352941176472,"Table 2: Sample speciﬁc microscope parameter optimization across 3 different zebraﬁsh sample types
imaged with 512 × 512 pixel camera (mean PSNR (top), MS-SSIM (bottom) ± s.e.m., n = 10).
Green shaded regions show the regions of interest for each sample type, cropped from the full volume."
S,0.18627450980392157,"microscope parameters optimized for
Type A
Type B
Type C"
S,0.18872549019607843,tested on
S,0.19117647058823528,"Type A
49.75 ± 1.35
0.998 ± 0.000
46.01 ± 1.33
0.996 ± 0.001
42.63 ± 1.14
0.992 ± 0.002
Type B
35.53 ± 1.41
0.965 ± 0.004
37.28 ± 0.96
0.972 ± 0.003
35.34 ± 1.16
0.967 ± 0.003
Type C
30.87 ± 1.15
0.912 ± 0.007
31.48 ± 0.93
0.920 ± 0.006
33.79 ± 0.90
0.935 ± 0.006"
S,0.19362745098039216,"Times in Table 1 are for only the forward pass on a single GPU; one training iteration on 8 GPUs takes
∼0.4 seconds for FourierNet3D and ∼0.8 seconds for UNet3D (Appendix A.3). Both reconstruction
networks must reconstruct from images that have a compressed encoding of 3D information, but the
FourierNet2D is clearly more effective than the UNet2D at optimizing this encoding."
S,0.19607843137254902,"3.2
FOURIERNETS OUTPERFORM UNETS FOR 3D SNAPSHOT MICROSCOPY VOLUME
RECONSTRUCTION"
S,0.19852941176470587,"We can determine which architecture is better for volume reconstruction by choosing ﬁxed microscope
parameters and varying the architecture. In Table 1, we compare results using a FourierNet with 3D
convolutions (FourierNet3D), a FourierUNet with 3D convolutions (FourierUNet3D), and a vanilla
UNet with 3D convolutions (UNet3D). Architecture details are in Appendix A.3, A.4."
S,0.20098039215686275,"Reconstruction results in Table 1 compare normalized MSE LHNMSE between the high pass ﬁltered
sample and high pass ﬁltered reconstruction, the multiscale structural similarity MS −SSIM between
the sample and reconstruction, and ﬁnally the peak signal-to-noise ratio PSNR. We also visualize
reconstruction results for a volume in the head of a zebraﬁsh in Figure 2. The best reconstruction
networks are (equally) FourierNet3D and FourierUNet3D. The UNet3D reconstruction networks
(using either microscope) fall signiﬁcantly behind the FourierNet3D/FourierUNet3D reconstruction
networks in all metrics.
3.3
ENGINEERED OPTICAL ENCODING DEPENDS ON SAMPLE STRUCTURE"
S,0.2034313725490196,"To explore the effect of sample structure and size on optimized φ and the resulting reconstruction
performance, we optimized φs for three different sample types: 1) Type A, samples with a short
cylinder cutout of 386 µm diameter and 25 µm height, 2) Type B, samples with a tall cylinder cutout
of 386 µm diameter and 250 µm height, and 3) Type C, samples without any cutout of dimension
(250 µm z × 832 µm y × 832 µm x) (Table 2). All sample types were imaged with 512 × 512
pixels on the simulated camera. We then tested reconstruction performance on all combinations of
optimized microscopes and samples, as shown in Table 2 and visualized in Figure 3 for Type B. We
include architecture details in Appendix A.3, A.5."
S,0.20588235294117646,"We see in Table 2 that for all sample types, highest performance is achieved using the microscope
optimized for that particular sample. These results show that our optimization process produces
optical encoders that are more optimal for a speciﬁc type of sample over others."
"FOURIERNETS OUTPERFORM STATE-OF-THE-ART FOR RECONSTRUCTING NATURAL
IMAGES CAPTURED BY DIFFUSERCAM LENSLESS CAMERA",0.20833333333333334,"3.4
FOURIERNETS OUTPERFORM STATE-OF-THE-ART FOR RECONSTRUCTING NATURAL
IMAGES CAPTURED BY DIFFUSERCAM LENSLESS CAMERA"
"FOURIERNETS OUTPERFORM STATE-OF-THE-ART FOR RECONSTRUCTING NATURAL
IMAGES CAPTURED BY DIFFUSERCAM LENSLESS CAMERA",0.2107843137254902,"We compare our FourierNet architecture to the best learned method from [20] using an unrolled
ADMM and a denoising UNet, as well as to a vanilla UNet from [20]. Architecture details are in
Appendix A.3, A.6. We can see that both of our methods visually outperform the methods from [20]
in Figure 4. Table 3 shows that our methods outperform the others on all metrics. In [20], a combined
loss using MSE and learned perceptual loss (LPIPS) is suggested to improve visual reconstruction
quality. Table 3 shows that training our FourierNet on both MSE and LPIPS results in the lowest"
"FOURIERNETS OUTPERFORM STATE-OF-THE-ART FOR RECONSTRUCTING NATURAL
IMAGES CAPTURED BY DIFFUSERCAM LENSLESS CAMERA",0.21323529411764705,"Under review as a conference paper at ICLR 2022 x
y z x 50 μm"
"FOURIERNETS OUTPERFORM STATE-OF-THE-ART FOR RECONSTRUCTING NATURAL
IMAGES CAPTURED BY DIFFUSERCAM LENSLESS CAMERA",0.21568627450980393,100 μm
"FOURIERNETS OUTPERFORM STATE-OF-THE-ART FOR RECONSTRUCTING NATURAL
IMAGES CAPTURED BY DIFFUSERCAM LENSLESS CAMERA",0.2181372549019608,"ground truth (Type B)
optimized for Type A
optimized for Type B
optimized for Type C"
"FOURIERNETS OUTPERFORM STATE-OF-THE-ART FOR RECONSTRUCTING NATURAL
IMAGES CAPTURED BY DIFFUSERCAM LENSLESS CAMERA",0.22058823529411764,simulated camera image
"FOURIERNETS OUTPERFORM STATE-OF-THE-ART FOR RECONSTRUCTING NATURAL
IMAGES CAPTURED BY DIFFUSERCAM LENSLESS CAMERA",0.22303921568627452,"Figure 3: Reconstructed volumes resulting from imaging Type B samples by microscopes optimized
for Type A, B, C. Imaging Type B samples with microscope parameters optimized for Type B samples
yields the best reconstructions. Top to bottom: xy max projection, xz max projection, simulated
512 × 512 camera image."
"FOURIERNETS OUTPERFORM STATE-OF-THE-ART FOR RECONSTRUCTING NATURAL
IMAGES CAPTURED BY DIFFUSERCAM LENSLESS CAMERA",0.22549019607843138,"Table 3: Quality of natural image reconstruction on the DiffuserCam Lensless Mirﬂickr Dataset
(mean ± s.e.m., n = 999). Superscripts denote loss function: 1 MSE, 2 MSE+LPIPS."
"FOURIERNETS OUTPERFORM STATE-OF-THE-ART FOR RECONSTRUCTING NATURAL
IMAGES CAPTURED BY DIFFUSERCAM LENSLESS CAMERA",0.22794117647058823,"Method
MSE ↓(×10−2) LPIPS ↓
MS-SSIM ↑
PSNR ↑
Time ↓(ms)"
"FOURIERNETS OUTPERFORM STATE-OF-THE-ART FOR RECONSTRUCTING NATURAL
IMAGES CAPTURED BY DIFFUSERCAM LENSLESS CAMERA",0.23039215686274508,"FourierNet1
0.39 ± 0.007
0.20 ± 0.00
0.882 ± 0.001
24.8 ± 0.09
37.54
FourierNet2
0.54 ± 0.010
0.16 ± 0.00
0.868 ± 0.001
23.4 ± 0.09
37.54
FourierUNet1
0.43 ± 0.009
0.22 ± 0.00
0.875 ± 0.001
24.5 ± 0.09
204.69"
"FOURIERNETS OUTPERFORM STATE-OF-THE-ART FOR RECONSTRUCTING NATURAL
IMAGES CAPTURED BY DIFFUSERCAM LENSLESS CAMERA",0.23284313725490197,"Le-ADMM-U2 [20] 0.75 ± 0.021
0.19 ± 0.00
0.865 ± 0.002
22.1 ± 0.09
59.01
UNet2 [20]
1.68 ± 0.060
0.24 ± 0.00
0.818 ± 0.002
19.2 ± 0.11
06.97"
"FOURIERNETS OUTPERFORM STATE-OF-THE-ART FOR RECONSTRUCTING NATURAL
IMAGES CAPTURED BY DIFFUSERCAM LENSLESS CAMERA",0.23529411764705882,"LPIPS loss across all methods, but does not offer better visual performance than our MSE-only model
as seen in Figure 4. FourierUNet does not show improved results over FourierNet."
GLOBAL RECEPTIVE FIELD IS MORE IMPORTANT THAN MULTISCALE FEATURES,0.23774509803921567,"3.5
GLOBAL RECEPTIVE FIELD IS MORE IMPORTANT THAN MULTISCALE FEATURES"
GLOBAL RECEPTIVE FIELD IS MORE IMPORTANT THAN MULTISCALE FEATURES,0.24019607843137256,"UNets are effective because: (1) features are computed at multiple scales and (2) large receptive ﬁelds
are achieved in few layers. FourierUNets allowed us to decouple these two explanations because the
receptive ﬁeld is global in a single layer. We see on both our microscopy dataset (which does not have
multiscale structure) and the lensless photography dataset (which does have multiscale structure) that
the FourierUNet does not improve upon the FourierNet. Thus we see that it is more important for
decoding from non-local optical encoders to have a global receptive ﬁeld than multi-scale features."
DISCUSSION,0.2426470588235294,"4
DISCUSSION
Summary We have presented FourierNets for decoding from highly non-local optical encodings. We
have demonstrated that FourierNets enable end-to-end optimization of non-local optical encodings"
DISCUSSION,0.24509803921568626,Under review as a conference paper at ICLR 2022
DISCUSSION,0.24754901960784315,"ground truth
DiﬀuserCam"
DISCUSSION,0.25,"image
FourierNet ()
(MSE) 
Le-ADMM-U
(MSE + LPIPS)
UNet
(MSE + LPIPS)
FourierNet () 
(MSE + LPIPS) 1 1 1 1 1 1 2 2 1 1 2 2"
DISCUSSION,0.25245098039215685,"Figure 4: Comparisons of our method (second and third rows) to state-of-the-art learned reconstruction
methods on lensless diffused images of natural scenes. Regions labeled 1⃝indicate missing details,
either resolution or textures in backgrounds. Regions labeled 2⃝indicate hallucinated textures.
Note that the previous state-of-the-art solutions [20] exhibit both issues more often compared to our
models."
DISCUSSION,0.2549019607843137,"where UNets fail for simulated 3D snapshot microscopy, and also that FourierNets beat state-of-the-art
decoders for lensless photography. FourierNets provide fast amortized inference and reconstruction
times many orders of magnitude faster than traditional iterative reconstruction algorithms [20] and
higher quality reconstructions due to effective learning of sample structural priors. Generally, our
global kernel architecture using Fourier convolutions could be applied to other problems where
global integration of features is necessary (though we have focused on computational microscopy
and photography where such global mixing of information dominates)."
DISCUSSION,0.25735294117647056,"Limitations Our engineered optical encoders have not yet been experimentally tested on a pro-
grammable microscope. This will require measuring and accounting for system aberrations and
calibration to implement the engineered optical encodings. However, the parameters of even an
imperfectly implemented optical encoder can be perfectly measured and used to train reconstruc-
tion networks to near optimal performance. Our claim that sample type-speciﬁc optical encoders
perform better on their respective samples has only been evaluated in simulation. Further, imaging a
different sample class (i.e. with different spatiotemporal statistics) will require retraining at least the
reconstruction network, and ideally also the optical encoder. Our autoencoder training style requires
simulation of imaging with gradients, which can be memory expensive and limits the total ﬁeld of
view. Our training approach requires multi-GPU parallelization (described in Appendix A.2), and in
practice we optimized microscopes using 8 Quadro RTX 8000 GPUs or 4 RTX 2080 Ti GPUs for the
larger and smaller experiments, respectively. Training a microscope and decoder requires at least two
weeks of total training. Our Fourier convolution layers require a ﬁxed input size to truly be global,
though this could be addressed by resampling."
DISCUSSION,0.25980392156862747,"Reproducibility We train on 8 Quadro RTX 8000 GPUs for the largest experiments, and have
described our pre-processing, training, and testing procedures in Appendix A.2, A.3, A.4. We will
also release our simulation software, training scripts, and our datasets upon publication."
DISCUSSION,0.2622549019607843,Under review as a conference paper at ICLR 2022
REFERENCES,0.2647058823529412,REFERENCES
REFERENCES,0.26715686274509803,"[1] E.H. Adelson and J.Y.A. Wang. Single lens stereo with a plenoptic camera. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 14(2):99–106, Feb 1992. ISSN 1939-3539. doi:
10.1109/34.121783."
REFERENCES,0.2696078431372549,"[2] Misha B. Ahrens, Michael B. Orger, Drew N. Robson, Jennifer M. Li, and Philipp J. Keller.
Whole-brain functional imaging at cellular resolution using light-sheet microscopy. Nature
Methods, 10(55):413–420, May 2013. ISSN 1548-7105. doi: 10.1038/nmeth.2434."
REFERENCES,0.27205882352941174,"[3] Nick Antipa, Grace Kuo, Reinhard Heckel, Ben Mildenhall, Emrah Bostan, Ren Ng, and Laura
Waller. Diffusercam: lensless single-exposure 3d imaging. Optica, 5(1):1–9, Jan 2018. ISSN
2334-2536. doi: 10.1364/OPTICA.5.000001."
REFERENCES,0.27450980392156865,"[4] M. S. Asif, A. Ayremlou, A. Veeraraghavan, R. Baraniuk, and A. Sankaranarayanan. Flatcam:
Replacing lenses with masks and computation. In 2015 IEEE International Conference on
Computer Vision Workshop (ICCVW), pp. 663–666, Dec 2015. doi: 10.1109/ICCVW.2015.89."
REFERENCES,0.2769607843137255,"[5] Michael Broxton. Volume Reconstruction and Resolution Limits for Three Dimensional Snapshot
Microscopy. PhD thesis, Stanford University, Aug 2017."
REFERENCES,0.27941176470588236,"[6] Julie Chang and Gordon Wetzstein. Deep optics for monocular depth estimation and 3d object
detection. arXiv:1904.08601 [cs, eess], Apr 2019. URL http://arxiv.org/abs/1904.
08601. arXiv: 1904.08601."
REFERENCES,0.2818627450980392,"[7] Lin Cong, Zeguan Wang, Yuming Chai, Wei Hang, Chunfeng Shang, Wenbin Yang, Lu Bai,
Jiulin Du, Kai Wang, and Quan Wen. Rapid whole brain imaging of neural activity in freely
behaving larval zebraﬁsh (danio rerio). eLife, 6:e28158, Sep 2017. ISSN 2050-084X. doi:
10.7554/eLife.28158."
REFERENCES,0.28431372549019607,"[8] Steven Diamond, Vincent Sitzmann, Felix Heide, and Gordon Wetzstein. Unrolled optimization
with deep priors. arXiv e-prints, 1705:arXiv:1705.08041, May 2017."
REFERENCES,0.2867647058823529,"[9] Jiangxin Dong, Stefan Roth, and Bernt Schiele. Deep wiener deconvolution: Wiener meets
deep learning for image deblurring. Advances in Neural Information Processing Systems, 33:
1048–1059, 2020."
REFERENCES,0.28921568627450983,"[10] Xiong Dun, Xiong Dun, Hayato Ikoma, Gordon Wetzstein, Zhanshan Wang, Zhanshan Wang,
Xinbin Cheng, Xinbin Cheng, Xinbin Cheng, Yifan Peng, and et al. Learned rotationally
symmetric diffractive achromat for full-spectrum computational imaging. Optica, 7(8):913–922,
Aug 2020. ISSN 2334-2536. doi: 10.1364/OPTICA.394413."
REFERENCES,0.2916666666666667,"[11] J. Goodman. Introduction to Fourier Optics. Macmillan Learning, 4 edition, 2017. ISBN
978-1-319-11916-4."
REFERENCES,0.29411764705882354,"[12] Logan Grosenick, Michael Broxton, Christina K. Kim, Conor Liston, Ben Poole, Samuel
Yang, Aaron Andalman, Edward Scharff, Noy Cohen, Ofer Yizhar, and et al. Identiﬁcation of
cellular-activity dynamics across large tissue volumes in the mammalian brain. bioRxiv, pp.
132688, May 2017. doi: 10.1101/132688."
REFERENCES,0.2965686274509804,"[13] Y. Hua, S. Nakamura, M. S. Asif, and A. C. Sankaranarayanan. Sweepcam — depth-aware
lensless imaging using programmable masks. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 42(7):1606–1617, Jul 2020. ISSN 1939-3539. doi: 10.1109/TPAMI.2020.
2986784."
REFERENCES,0.29901960784313725,"[14] Hayato Ikoma, Cindy M. Nguyen, Christopher A. Metzler, Yifan Peng, and Gordon Wetzstein.
Depth from defocus with learned optics for imaging and occlusion-aware depth estimation. In
2021 IEEE International Conference on Computational Photography (ICCP), pp. 1–12, May
2021. doi: 10.1109/ICCP51581.2021.9466261."
REFERENCES,0.3014705882352941,"[15] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. arXiv:1502.03167 [cs], Mar 2015. URL http://arxiv.
org/abs/1502.03167. arXiv: 1502.03167."
REFERENCES,0.30392156862745096,Under review as a conference paper at ICLR 2022
REFERENCES,0.30637254901960786,"[16] Marc Levoy, Ren Ng, Andrew Adams, Matthew Footer, and Mark Horowitz. Light ﬁeld
microscopy. In ACM SIGGRAPH 2006 Papers, SIGGRAPH ’06, pp. 924–934. Association for
Computing Machinery, Jul 2006. ISBN 978-1-59593-364-5. doi: 10.1145/1179352.1141976.
URL https://doi.org/10.1145/1179352.1141976."
REFERENCES,0.3088235294117647,"[17] Fanglin Linda Liu, Grace Kuo, Nick Antipa, Kyrollos Yanny, and Laura Waller. Fourier
diffuserscope: single-shot 3d fourier light ﬁeld microscopy with a diffuser. Optics Express, 28
(20):28969, Sep 2020. ISSN 1094-4087. doi: 10.1364/OE.400876."
REFERENCES,0.3112745098039216,"[18] Fanglin Linda Liu, Vaishnavi Madhavan, Nick Antipa, Grace Kuo, Saul Kato, and Laura
Waller. Single-shot 3d ﬂuorescence microscopy with fourier diffusercam. In Biophoton-
ics Congress: Optics in the Life Sciences Congress 2019 (BODA,BRAIN,NTM,OMA,OMP)
(2019), paper NS2B.3, pp. NS2B.3. Optical Society of America, Apr 2019. doi: 10.1364/
NTM.2019.NS2B.3.
URL https://www.osapublishing.org/abstract.cfm?
uri=NTM-2019-NS2B.3."
REFERENCES,0.3137254901960784,"[19] Michael Mathieu, Mikael Henaff, and Yann LeCun. Fast training of convolutional networks
through ffts. arXiv:1312.5851 [cs], Mar 2014. URL http://arxiv.org/abs/1312.
5851. arXiv: 1312.5851."
REFERENCES,0.3161764705882353,"[20] Kristina Monakhova, Joshua Yurtsever, Grace Kuo, Nick Antipa, Kyrollos Yanny, and Laura
Waller. Learned reconstructions for practical mask-based lensless imaging. Optics Express, 27
(20):28075–28090, Sep 2019. ISSN 1094-4087. doi: 10.1364/OE.27.028075."
REFERENCES,0.31862745098039214,"[21] Nan Mu, Xin Xu, and Xiaolong Zhang. Salient object detection in low contrast images via
global convolution and boundary reﬁnement. In 2019 IEEE/CVF Conference on Computer
Vision and Pattern Recognition Workshops (CVPRW), pp. 743–751. IEEE, Jun 2019. ISBN
978-1-72812-506-0. doi: 10.1109/CVPRW.2019.00102. URL https://ieeexplore.
ieee.org/document/9025357/."
REFERENCES,0.32107843137254904,"[22] Yu Mu, Davis V. Bennett, Mikail Rubinov, Sujatha Narayan, Chao-Tsung Yang, Masashi
Tanimoto, Brett D. Mensh, Loren L. Looger, and Misha B. Ahrens. Glia accumulate evidence
that actions are futile and suppress unsuccessful behavior. Cell, 178(1):27–43.e19, Jun 2019.
ISSN 0092-8674. doi: 10.1016/j.cell.2019.05.050."
REFERENCES,0.3235294117647059,"[23] Elias Nehme, Daniel Freedman, Racheli Gordon, Boris Ferdman, Lucien E. Weiss, Onit Alalouf,
Tal Naor, Reut Orange, Tomer Michaeli, and Yoav Shechtman. Deepstorm3d: dense 3d
localization microscopy and psf design by deep learning. Nature Methods, 17(77):734–740, Jul
2020. ISSN 1548-7105. doi: 10.1038/s41592-020-0853-5."
REFERENCES,0.32598039215686275,"[24] Alan V. Oppenheim and Alan S. Willsky. Signals and Systems: Pearson New International
Edition. Pearson Education Limited, Jul 2013. ISBN 978-1-292-02590-2. Google-Books-ID:
ut9oAQAACAAJ."
REFERENCES,0.3284313725490196,"[25] Chao Peng, Xiangyu Zhang, Gang Yu, Guiming Luo, and Jian Sun. Large kernel matters —
improve semantic segmentation by global convolutional network. In 2017 IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pp. 1743–1751. IEEE, Jul 2017. ISBN
978-1-5386-0457-1. doi: 10.1109/CVPR.2017.189. URL http://ieeexplore.ieee.
org/document/8099672/."
REFERENCES,0.33088235294117646,"[26] Robert Prevedel, Young-Gyu Yoon, Maximilian Hoffmann, Nikita Pak, Gordon Wetzstein,
Saul Kato, Tina Schrödel, Ramesh Raskar, Manuel Zimmer, Edward S. Boyden, and et al.
Simultaneous whole-animal 3d imaging of neuronal activity using light-ﬁeld microscopy. Nature
Methods, 11(77):727–730, Jul 2014. ISSN 1548-7105. doi: 10.1038/nmeth.2964."
REFERENCES,0.3333333333333333,"[27] Nicolas C. Pégard, Hsiou-Yuan Liu, Nick Antipa, Maximillian Gerlock, Hillel Adesnik, and
Laura Waller. Compressive light-ﬁeld microscopy for 3d neural activity recording. Optica, 3(5):
517–524, May 2016. ISSN 2334-2536. doi: 10.1364/OPTICA.3.000517."
REFERENCES,0.33578431372549017,"[28] Oren Rippel, Jasper Snoek, and Ryan P Adams.
Spectral representations for convolu-
tional neural networks.
In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems, volume 28. Curran Asso-
ciates, Inc., 2015. URL https://proceedings.neurips.cc/paper/2015/file/
536a76f94cf7535158f66cfbd4b113b6-Paper.pdf."
REFERENCES,0.3382352941176471,Under review as a conference paper at ICLR 2022
REFERENCES,0.34068627450980393,"[29] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for
biomedical image segmentation. In Nassir Navab, Joachim Hornegger, William M. Wells, and
Alejandro F. Frangi (eds.), Medical Image Computing and Computer-Assisted Intervention
– MICCAI 2015, Lecture Notes in Computer Science, pp. 234–241. Springer International
Publishing, 2015. ISBN 978-3-319-24574-4. doi: 10.1007/978-3-319-24574-4_28."
REFERENCES,0.3431372549019608,"[30] Oliver Skocek, Tobias Nöbauer, Lukas Weilguny, Francisca Martínez Traub, Chuying Naomi
Xia, Maxim I. Molodtsov, Abhinav Grama, Masahito Yamagata, Daniel Aharoni, David D. Cox,
and et al. High-speed volumetric imaging of neuronal activity in freely moving rodents. Nature
Methods, 15(66):429–432, Jun 2018. ISSN 1548-7105. doi: 10.1038/s41592-018-0008-0."
REFERENCES,0.34558823529411764,"[31] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing
ingredient for fast stylization. arXiv:1607.08022 [cs], Nov 2017. URL http://arxiv.
org/abs/1607.08022. arXiv: 1607.08022."
REFERENCES,0.3480392156862745,"[32] Nicolas Vasilache, Jeff Johnson, Michael Mathieu, Soumith Chintala, Serkan Piantino, and Yann
LeCun. Fast convolutional nets with fbfft: A gpu performance evaluation. arXiv:1412.7580
[cs], Apr 2015. URL http://arxiv.org/abs/1412.7580. arXiv: 1412.7580."
REFERENCES,0.35049019607843135,"[33] Nils Wagner, Fynn Beuttenmueller, Nils Norlin, Jakob Gierten, Juan Carlos Bofﬁ, Joachim
Wittbrodt, Martin Weigert, Lars Hufnagel, Robert Prevedel, and Anna Kreshuk. Deep learning-
enhanced light-ﬁeld imaging with continuous validation. Nature Methods, 18(5):557–563, May
2021. ISSN 1548-7105. doi: 10.1038/s41592-021-01136-0."
REFERENCES,0.35294117647058826,"[34] Zhaoqiang Wang, Lanxin Zhu, Hao Zhang, Guo Li, Chengqiang Yi, Yi Li, Yicong Yang, Yichen
Ding, Mei Zhen, Shangbang Gao, and et al. Real-time volumetric reconstruction of biological
dynamics with light-ﬁeld microscopy and deep learning. Nature Methods, pp. 1–6, Feb 2021.
ISSN 1548-7105. doi: 10.1038/s41592-021-01058-x."
REFERENCES,0.3553921568627451,"[35] Yicheng Wu, Vivek Boominathan, Huaijin Chen, Aswin Sankaranarayanan, and Ashok Veer-
araghavan. Phasecam3d — learning phase masks for passive single view depth estimation. In
2019 IEEE International Conference on Computational Photography (ICCP), pp. 1–12, May
2019. doi: 10.1109/ICCPHOT.2019.8747330."
REFERENCES,0.35784313725490197,"[36] Weijian Yang and Rafael Yuste. In vivo imaging of neural activity. Nature Methods, 14(44):
349–359, Apr 2017. ISSN 1548-7105. doi: 10.1038/nmeth.4230."
REFERENCES,0.3602941176470588,"[37] Kyrollos Yanny, Nick Antipa, Ren Ng, Laura Waller, and Laura Waller. Miniature 3d ﬂuores-
cence microscope using random microlenses. In Biophotonics Congress: Optics in the Life
Sciences Congress 2019 (BODA,BRAIN,NTM,OMA,OMP) (2019), paper BT3A.4, pp. BT3A.4.
Optical Society of America, Apr 2019. doi: 10.1364/BRAIN.2019.BT3A.4. URL https:
//www.osapublishing.org/abstract.cfm?uri=BRAIN-2019-BT3A.4."
REFERENCES,0.3627450980392157,"[38] Kyrollos Yanny, Nick Antipa, William Liberti, Sam Dehaeck, Kristina Monakhova, Fan-
glin Linda Liu, Konlin Shen, Ren Ng, and Laura Waller. Miniscope3d: optimized single-shot
miniature 3d ﬂuorescence microscopy. Light: Science & Applications, 9(11):171, Oct 2020.
ISSN 2047-7538. doi: 10.1038/s41377-020-00403-7."
REFERENCES,0.36519607843137253,Under review as a conference paper at ICLR 2022
REFERENCES,0.36764705882352944,"A
APPENDIX"
REFERENCES,0.3700980392156863,"A.1
FORWARD SIMULATION OF PROGRAMMABLE 3D SNAPSHOT MICROSCOPE"
D SAMPLE,0.37254901960784315,3D sample
D SAMPLE,0.375,(pupil plane)
D CAMERA IMAGE,0.37745098039215685,2D camera image
D CAMERA IMAGE,0.3799019607843137,"lens 1
lens 2"
D CAMERA IMAGE,0.38235294117647056,phase mask
D CAMERA IMAGE,0.38480392156862747,"f
f
f
f"
D CAMERA IMAGE,0.3872549019607843,"Figure 5: Diagram of a 4f optical model that is the basis for our simulated microscope Mφ, showing
the Fourier plane in which we have the programmable and trainable 2D phase mask φ."
D CAMERA IMAGE,0.3897058823529412,"Here we describe our wave optics simulation of the microscope Mφ, which we model as a 4f system
[11]. The 4f optical system consists of two lenses, the ﬁrst spaced one focal length from the object
plane and the second spaced one focal length away from one focal length beyond the ﬁrst lens (Figure
5). In between these two lenses, we can place a phase mask to manipulate the light ﬁeld before
passing through the second lens and forming an image on the camera sensor."
D CAMERA IMAGE,0.39215686274509803,"We are concerned here with ﬂuorescence microscopy, meaning that the sources of light that we image
are individual ﬂuorescent molecules, which we can model as point emitters. Because these molecules
emit incoherent light, the camera sensor in effect sums the contributions of each point emitter. In
order to model such an imaging system, we ﬁrst need to address modeling a single point emitter’s
image on the camera."
D CAMERA IMAGE,0.3946078431372549,"We can analytically calculate the complex-valued light ﬁeld one focal length after the ﬁrst lens (which
we call the pupil plane) due to a point source centered at some plane z (where z is a distance from the
object plane z = 0). If the point source were centered (x = 0, y = 0) in the object focal plane z = 0,
we would have a plane wave at the pupil plane, but for the more general case of a point source at an
arbitrary plane z relative to the object plane z = 0, we can analytically calculate the complex-valued
light ﬁeld entering the pupil plane:"
D CAMERA IMAGE,0.39705882352941174,"upoint(k; z) = exp "" i2πz rn λ"
D CAMERA IMAGE,0.39950980392156865,"2
−||k||2
2 # (7)"
D CAMERA IMAGE,0.4019607843137255,"where upoint is the incoming light ﬁeld entering the pupil due to a point source centered in the plane at
z, k ∈R2 denotes frequency space coordinates of the light ﬁeld in the pupil plane, n is the refractive
index, and λ is the wavelength of light [11]."
D CAMERA IMAGE,0.40441176470588236,"In this pupil plane, we can then apply a phase mask φ to the light ﬁeld, which is modeled as a
multiplication of upoint(k; z) and eiφ(k), the complex phase of the pupil function. The light ﬁeld
exiting the pupil is therefore described by"
D CAMERA IMAGE,0.4068627450980392,"upupil(k; z) = upoint(k; z)p(k)
(8)"
D CAMERA IMAGE,0.40931372549019607,"where p(k) is the pupil function, composed of an amplitude a(k) and phase φ(k):"
D CAMERA IMAGE,0.4117647058823529,"p(k) = a(k)eiφ(k)
(9)"
D CAMERA IMAGE,0.41421568627450983,"a(k) =
1
||k||2 ≤NA"
D CAMERA IMAGE,0.4166666666666667,"λ
0
||k||2 > NA"
D CAMERA IMAGE,0.41911764705882354,"λ
(10)"
D CAMERA IMAGE,0.4215686274509804,where NA is the numerical aperture of the lens [11].
D CAMERA IMAGE,0.42401960784313725,The light ﬁeld at the camera plane can then be described by a Fourier transform [11]:
D CAMERA IMAGE,0.4264705882352941,"ucamera = F {upupil}
(11)"
D CAMERA IMAGE,0.42892156862745096,Under review as a conference paper at ICLR 2022
D CAMERA IMAGE,0.43137254901960786,The camera measures the intensity of this complex ﬁeld:
D CAMERA IMAGE,0.4338235294117647,"s(x; z) = |ucamera(x; z)|2
(12)
where x ∈R2 denotes spatial coordinates in the camera plane [11]."
D CAMERA IMAGE,0.4362745098039216,"We can call this intensity s the point response function (PRF). If the shape of the PRF is translationally
equivariant in x, meaning that moving a point source in-plane creates the same ﬁeld at the camera,
just shifted by the corresponding amount, then we call this PRF a point spread function (PSF). Note
that moving the point source in z will not give the same shape, which allows our system to encode
depth information through the PSF [5]."
D CAMERA IMAGE,0.4387254901960784,"In order to avoid edge effects during imaging, we simulate the PSF at a larger ﬁeld of view, then crop
and taper the edges of the PSF:
staper = crop[s] ⊙t
(13)
where t is a taper function created by taking the sigmoid of a distance transform divided by a
width factor controlling how quickly the taper goes to 0 at the edges and ⊙denotes elementwise
multiplication. We intentionally simulate a larger ﬁeld of view than the sample in order to avoid edge
artifacts. The purpose of the crop[·] is to cut the PSF to the correct ﬁeld of view. The purpose of the
tapering is to remove artifacts at the edges of the cropped PSF. After we compute this cropped and
tapered PSF, we also downsample staper to the size of the data v in order to save memory."
D CAMERA IMAGE,0.4411764705882353,"Imaging is equivalent to the convolution of the incoming light ﬁeld volume intensity v and the
cropped and tapered PSF staper for a given plane. At the camera plane, the light ﬁeld intensity is
measured by the camera sensor. Therefore, we can describe the forward model as the following
convolution and integral over planes:"
D CAMERA IMAGE,0.44362745098039214,"µc(x) =
ZZ
v(τx; z)staper(x −τx; z) dτx dz
(14)"
D CAMERA IMAGE,0.44607843137254904,"We then model shot noise of the camera sensor to produce the ﬁnal image c, for which the appropriate
model is sampling from a Poisson distribution with a mean of µc [11]:
c ∼Poisson (µc)
(15)
However, because we cannot use the reparameterization trick to take pathwise derivatives through
the discrete Poisson distribution, we instead approximate the noise model with a rectiﬁed Gaussian
distribution:
ϵ ∼N(0, 1)
(16)
c ≈max ([µc + √µcϵ] , 0)
(17)"
D CAMERA IMAGE,0.4485294117647059,"We now turn our attention to selecting the number of pixels used in the phase mask, i.e. the number
of parameters for Mφ. We ﬁrst need to determine the pixel size for Nyquist sampling the image
plane with an objective of a given NA (numerical aperture). For a given pixel size ∆x, we know that
in frequency space coordinates we will have a bandwidth of
1
∆x, spanning −
1
2∆x to
1
2∆x. Because
we must have"
D CAMERA IMAGE,0.45098039215686275,||k||2 ≤NA
D CAMERA IMAGE,0.4534313725490196,"λ
(18)"
D CAMERA IMAGE,0.45588235294117646,we know that the Nyquist sampling pixel size is given by
D CAMERA IMAGE,0.4583333333333333,"∆x∗=
λ
2NA.
(19)"
D CAMERA IMAGE,0.46078431372549017,"Therefore, in the image plane, for a desired ﬁeld of view L we must have at least"
D CAMERA IMAGE,0.4632352941176471,"N ∗=
L
∆x∗
(20)"
D CAMERA IMAGE,0.46568627450980393,"pixels. The discretization in the pupil plane will be the same, which means we will need to have at
least N ∗pixels in the pupil plane to achieve the appropriate light ﬁeld in the image plane. For our
settings of NA = 0.8, λ = 0.532µm, and L = 823µm, we have ∆x∗= 0.3325µm and N ∗= 2476
pixels. Thus, a reasonable choice is ∆x = 0.325µm and N = 2560 pixels. Note that these simulation
parameters are independent of the camera pixels; we have determined only how many pixels must
be used in the phase mask in order to ensure our PSF can occupy the full ﬁeld of view. The camera
sensor can sample the ﬁeld at the image plane at an independent pixel size."
D CAMERA IMAGE,0.4681372549019608,Under review as a conference paper at ICLR 2022
D CAMERA IMAGE,0.47058823529411764,"A.2
TRAINING PSFS AND VOLUME RECONSTRUCTION NETWORKS"
D CAMERA IMAGE,0.4730392156862745,100 μm
D CAMERA IMAGE,0.47549019607843135,"256 × 256 pixels, Type D
256 × 256 pixels, Type D
FourierNet2D microscope
UNet2D microscope"
D CAMERA IMAGE,0.47794117647058826,"Figure 6: FourierNet successfully optimizes a PSF to image and reconstruct Type D where UNet fails.
The FourierNet learned to produce multiple pencils in its PSF, which create multiple views of the
sample in the camera image. UNet learned only a single pencil and fails to utilize the majority of
pixels in the camera image to encode views of the sample. Top row shows simulated camera image
of a Type D example, middle row shows xy max projection of the PSF, and bottom row shows xz
max projection of the PSF."
D CAMERA IMAGE,0.4803921568627451,"Given a simulation of imaging, we can deﬁne two modes of autoencoder training: (1) jointly training
the phase mask parameters φ and weak reconstruction networks in order to learn a good PSF for a
particular class of samples (i.e. samples with the same spatiotemporal statistics), and (2) training a
stronger reconstruction network only with a ﬁxed, pre-trained φ."
D CAMERA IMAGE,0.48284313725490197,"Deﬁnition of terms For both cases of training, the general framework is to simulate imaging using
confocal volumes of pan-neuronal labeled larval zebraﬁsh, reconstruct from the simulated image,
then update the reconstruction network and, if desired, the microscope parameters. We will deﬁne the
microscope parameters as φ and the reconstruction network parameters as θ for any reconstruction
network Rθ(c) where Rθ maps 2D images to 3D volume reconstructions. For our training algorithms
listed below, we also deﬁne: D our dataset, v a ground truth volume, ˆv a reconstructed volume,
L a computed loss, zs a list of z plane indices that will be imaged/reconstructed, αφ the learning rate
for the microscope parameters, αθ the learning rate for the reconstruction network parameters, and
β the weight of the non-high pass ﬁltered component of the loss. When selecting a random ground
truth volume, we also perform random shift, rotation, ﬂip, and brightness augmentations."
D CAMERA IMAGE,0.4852941176470588,"Microscope simulation parameters When simulating the zebraﬁsh imaging, we use a wavelength
of 0.532 µm for all simulations. The NA of our microscope is 0.8. The refractive index n is 1.33. We
downsample all volumes to (1.0 µm z, 1.625 µm y, 1.625 µm x). We use a taper width of 5 for all"
D CAMERA IMAGE,0.4877450980392157,Under review as a conference paper at ICLR 2022
D CAMERA IMAGE,0.49019607843137253,200 μm
D CAMERA IMAGE,0.49264705882352944,"512 × 512 pixels, Type A
512 × 512 pixels, Type B
512 × 512 pixels, Type C
optimized and imaged on:
optimized and imaged on:
optimized and imaged on:"
D CAMERA IMAGE,0.4950980392156863,"Figure 7: Optimizing PSFs for different samples result in PSFs tailored to each sample. Note that
PSF optimized for Type A (left) has pencils with a span in z that matches Type A. PSF optimized
for Type B (middle) has pencils that span the entire z depth. PSF optimized for Type C (right) has
pencils spread farther apart to account for the larger sample. Top row shows simulated camera image
of a Type A, B, or C example respectively, middle row shows xy max projection of the PSF, and
bottom row shows xz max projection of the PSF."
D CAMERA IMAGE,0.49754901960784315,"simulations, and simulate the PSF at 50% larger dimensions in x and y. The resolution of the camera
(for all zebraﬁsh datasets) is also (1.625 µm y, 1.625 µm x)."
D CAMERA IMAGE,0.5,"Initialization of φ For Type A, B, and D, we initialize φ to produce a PSF consisting of 6 pencil
beams at different locations throughout the depth of the volume, with the centers of these beams
arranged in a hexagonal pattern in x and y. Because our optimizations generally ﬁnd PSFs with many
pencils, we ﬁnd that initializing with such a pattern helps to converge to a more optimal PSF (data
not shown)."
D CAMERA IMAGE,0.5024509803921569,"For Type C, we instead initialize with a single helix spanning the depth of the volume (the “Potato
Chip” from [5]), which seems to ﬁnd a local minimum for φ that produces a PSF with more pencils
(and therefore views in the camera image)."
D CAMERA IMAGE,0.5049019607843137,"Data settings and augmentation for Type A, B, C, D Using our total 58 training zebraﬁsh volumes
and 10 testing zebraﬁsh volumes (imaged through confocal microscopy), we crop in four different
ways to create four different datasets. For training volumes, we crop from random locations from
each volume as a form of augmentation. For testing, we crop from the same location. Physically,
these crops correspond to either placing a circular aperture before light hits the 4f system or changing
the illumination thickness in z, because samples would be illuminated from the side in a real
implementation of this microscope. We model these by cropping cylinders (or cubes if there is no
aperture) of different diameters and heights. We show details for all types Type A, B, C, D in Table 4,"
D CAMERA IMAGE,0.5073529411764706,Under review as a conference paper at ICLR 2022
D CAMERA IMAGE,0.5098039215686274,"where the diameter of the cylinder is labeled “Aperture Diameter” and the illumination thickness is
labeled “Height”."
D CAMERA IMAGE,0.5122549019607843,"We augment our volumes during training by taking random locations from these volumes, randomly
ﬂipping the volumes in both z and y, and also randomly rotating in pitch, yaw, and roll. Most
importantly, we also randomly scale the brightness of our samples and add random background
levels which serve to adjust the signal-to-noise ratio (SNR) of the resulting simulated images. The
only exception to these augmentations is Type C, where we set all the volumes to the same in-plane
vertical orientation (while still applying rotation augmentations in pitch and roll)."
D CAMERA IMAGE,0.5147058823529411,"Table 4: Speciﬁcations of all zebraﬁsh datasets Type A, B, C, D for reconstruction
Dataset
Camera (px)
Height (planes)
Span (z, y, x) (µm)
Aperture Diameter (µm)"
D CAMERA IMAGE,0.5171568627450981,"Type A
512 × 512
12
(25, 832, 832)
386"
D CAMERA IMAGE,0.5196078431372549,"Type B
512 × 512
128
(250, 832, 832)
386"
D CAMERA IMAGE,0.5220588235294118,"Type C
512 × 512
128
(250, 832, 832)
-"
D CAMERA IMAGE,0.5245098039215687,"Type D
256 × 256
96
(200, 416, 416)
193"
D CAMERA IMAGE,0.5269607843137255,"Parallelizing imaging and reconstruction Furthermore, because this simulation can become too
expensive in memory to ﬁt on a single device, we generally perform the simulation, reconstruction,
and loss calculation in parallel for both training modes. Therefore, any variable that has a s subscript
refers to a list of chunks of that variable that will be run on each device. A j superscript indicates
a particular chunk for GPU j. For example zs is a list of plane indices to be imaged/reconstructed,
and zj
s is the jth chunk of plane indices that will be imaged/reconstructed on GPU j. We denote
parallel for any operations that are performed in parallel and scatter for splitting data into
chunks and spreading across multiple GPUs. Imaging can be cleanly parallelized: chunks of a
PSF and sample can be partially imaged on multiple GPUs independently because the convolution
occurs per plane, then ﬁnally all partial images can be summed together onto a single GPU. The
reconstructions can similarly take the ﬁnal image and reconstruct partial chunks (as well as calculate
losses on partial chunks) of the volume independently per device. We implicitly gather data to
the same GPU when computing sums (P) or means (E). The functions parallel image and
compute PSF follow the deﬁnitions above in equations 14 and 12. In the algorithms shown here,
parallel image applies the same convolution described above in equation 14."
D CAMERA IMAGE,0.5294117647058824,"Sparse gradients and downsampling We additionally support training and reconstructing only some
of the planes for imaging and potentially a different partial group of planes during reconstruction, as
a way to sparsely compute gradients for optimization of θm and save memory. The planes not imaged
with gradients can still contribute to the image (without their gradients being tracked) in order to
make the problem more difﬁcult for the reconstruction network. Over multiple iterations, this can
become equivalent to the more expensive densely computed gradient method, essentially trading
training time for memory. An additional memory saving measure not written in the algorithms is to
compute the PSF at a high resolution, then downsample the PSF using a 2D sum pool to preserve
total energy in order to reduce memory usage when performing the imaging and reconstruction. We
denote with no gradient tracking to show an operation without gradients."
D CAMERA IMAGE,0.5318627450980392,"A.3
IMPLEMENTATION DETAILS"
D CAMERA IMAGE,0.5343137254901961,"Fourier convolution details Our Fourier convolution uses complex number weights, implemented
as two channels of real numbers. Furthermore, in order to prevent the convolution from wrapping
around the edges, we have to pad the input to double the size. The size of the weight must match
the size of this padded input. This means that the number of parameters for our Fourier convolution
implementation is 8× the number of parameters required for a global kernel in a spatial convolution
(though the Fourier convolution is signiﬁcantly faster). We do this to save an extra padding and
Fourier operation, trading memory for speed. Because the simulation of imaging requires more
memory than the reconstruction network, we found this to be an acceptable tradeoff."
D CAMERA IMAGE,0.5367647058823529,"Common network details All convolutions (including Fourier convolutions) use “same” padding.
For FourierUNets and vanilla UNets, downsampling and upsampling is performed only in the x and y"
D CAMERA IMAGE,0.5392156862745098,Under review as a conference paper at ICLR 2022
D CAMERA IMAGE,0.5416666666666666,"Algorithm 1: Parallel PSF engineering by joint training of reconstruction network and phase
mask. Microscope Mφ parameters are φ, reconstruction network Rθ parameters are θ, dataset is
D, learning rates for φ and θ are αφ and αθ respectively, plane indices to image and reconstruct
from zs, and weight for LNMSE is β.
Input :Mφ, φ, αφ, Rθ, θ, αθ, D, zs, β"
D CAMERA IMAGE,0.5441176470588235,1 for v ∈D do
D CAMERA IMAGE,0.5465686274509803,"// select plane indices to be imaged with and without
gradients"
D CAMERA IMAGE,0.5490196078431373,"2
zs,gradient, zs,no gradient ←select planes(zs)
// move sample planes to be imaged with gradients to multiple
GPUs"
D CAMERA IMAGE,0.5514705882352942,"3
vs,gradient ←scatter(v, zs,gradient)
// move sample planes to be imaged without gradients to
multiple GPUs"
D CAMERA IMAGE,0.553921568627451,"4
vs,no gradient ←scatter(v, zs,no gradient)
// compute PSF with gradients on multiple GPUs"
D CAMERA IMAGE,0.5563725490196079,"5
ss,gradient ←parallel(compute PSF(Mφ, zj
s) for zj
s in zs,gradient)
// compute partial image with gradients on multiple GPUs"
D CAMERA IMAGE,0.5588235294117647,"6
cgradient ←parallel image(ss,gradient, vs,gradient)
// compute PSF without gradients on multiple GPUs"
WITH NO GRADIENT TRACKING,0.5612745098039216,"7
with no gradient tracking"
WITH NO GRADIENT TRACKING,0.5637254901960784,"8
ss,no gradient ←parallel(compute PSF(Mφ, zj
s) for zj
s in zs,no gradient)"
END,0.5661764705882353,"9
end
// compute partial image without gradients on multiple GPUs"
WITH NO GRADIENT TRACKING,0.5686274509803921,"10
with no gradient tracking"
WITH NO GRADIENT TRACKING,0.571078431372549,"11
cno gradient ←parallel image(ss,no gradient, vs,no gradient)"
END,0.5735294117647058,"12
end
// compute full image by summing partial images onto one GPU"
END,0.5759803921568627,"13
c ←P[cgradient, cno gradient]
// select plane indices to be reconstructed"
END,0.5784313725490197,"14
zs,reconstruct ←select planes(zs)
// move sample planes that will be reconstructed to multiple
GPUs"
END,0.5808823529411765,"15
vs,reconstruct ←scatter(v, zs,reconstruct)
// compute mean of high passed sample for loss normalization"
END,0.5833333333333334,"16
µH(v) ←E(H(vs,reconstruct)2)
// compute mean of sample for loss normalization"
END,0.5857843137254902,"17
µv ←E(v2
s,reconstruct)
// move reconstruction networks to multiple GPUs"
END,0.5882352941176471,"18
Rθ,s ←scatter(Rθ)
// compute reconstruction and loss on multiple GPUs"
END,0.5906862745098039,"19
L ←parallel reconstruct/loss(c, vs,reconstruct, Rθ,s, µH(v), µv, β)
// compute gradients for all parameters"
END,0.5931372549019608,"20
gθ ←∇θL"
END,0.5955882352941176,"21
gφ ←∇φL
// update all parameters"
END,0.5980392156862745,"22
θ ←ADAM(αθ, θ, gθ)"
END,0.6004901960784313,"23
φ ←ADAM(αφ, φ, gφ)"
END,0.6029411764705882,24 end
END,0.6053921568627451,"dimensions (we do not downsample or upsample in z because there could potentially not be enough
planes to do so). We train all networks using the ADAM optimizer with all default PyTorch parameters
except the learning rate, which we always set to 10−4 for the reconstruction network parameters θ
and 10−2 for the phase mask parameters φ."
END,0.6078431372549019,Under review as a conference paper at ICLR 2022
END,0.6102941176470589,"Algorithm 2: Parallel training a reconstruction network given a pre-trained phase mask. Mi-
croscope Mφ parameters are φ (phase mask), reconstruction network Rθ parameters are θ,
dataset is D, learning rates for φ and θ are αφ and αθ respectively, plane indices to image and
reconstruct from are zs, and weight for LNMSE is β.
Input :Mφ, φ, αφ, Rθ, θ, αθ, D, zs, β
// compute PSF without gradients on multiple GPUs"
WITH NO GRADIENT TRACKING,0.6127450980392157,1 with no gradient tracking
WITH NO GRADIENT TRACKING,0.6151960784313726,"2
sno gradient ←parallel(compute PSF(Mφ, zj
s) for zj
s in zs)"
END,0.6176470588235294,3 end
END,0.6200980392156863,4 for v ∈D do
END,0.6225490196078431,// select plane indices to be imaged without gradients
END,0.625,"5
zs,no gradient ←select planes(zs)
// move sample planes to be imaged without gradients to
multiple GPUs"
END,0.6274509803921569,"6
vs,no gradient ←scatter(v, zs,no gradient)
// move necessary PSF planes to multiple GPUs"
END,0.6299019607843137,"7
ss,no gradient ←scatter(sno gradient, zs,no gradient)
// compute image without gradients on multiple GPUs"
WITH NO GRADIENT TRACKING,0.6323529411764706,"8
with no gradient tracking"
WITH NO GRADIENT TRACKING,0.6348039215686274,"9
c ←parallel image(ss,no gradient, vs,no gradient)"
END,0.6372549019607843,"10
end
// select plane indices to be reconstructed"
END,0.6397058823529411,"11
zs,reconstruct ←select planes(zs)
// move sample planes that will be reconstructed to multiple
GPUs"
END,0.6421568627450981,"12
vs,reconstruct ←scatter(v, zs,reconstruct)
// compute mean of high passed sample for loss normalization"
END,0.6446078431372549,"13
µH(v) ←E[H(vs,reconstruct)2]
// compute mean of sample for loss normalization"
END,0.6470588235294118,"14
µv ←E[v2
s,reconstruct]
// move reconstruction networks to multiple GPUs"
END,0.6495098039215687,"15
Rθ,s ←scatter(Rθ)
// compute reconstruction and loss on multiple GPUs"
END,0.6519607843137255,"16
L ←parallel reconstruct/loss(c, vs,reconstruct, Rθ,s, µH(v), µv, β)
// compute gradients for reconstruction networks only"
END,0.6544117647058824,"17
gθ ←∇θL
// update reconstruction network parameters only"
END,0.6568627450980392,"18
θ ←ADAM(αθ, θ, gθ)"
END,0.6593137254901961,19 end
END,0.6617647058823529,"Algorithm 3: Parallel imaging. PSF planes on multiple GPUs are ss, sample planes on multiple
GPUs to be imaged are vs.
Input
:ss, vs
Output :c
// compute images in parallel on multiple GPUs, then sum to
single GPU"
END,0.6642156862745098,"1 c ←P[parallel(convolve(sj
s, vj
s) for (sj
s, vj
s) in (ss, vs))]"
RETURN C,0.6666666666666666,2 return c
RETURN C,0.6691176470588235,"Normalization We use input scaling during both training and inference in order to normalize out
differences in the brightness of the image and prevent instabilities in our gradients. This means
we divide out the median value of the input (scaled by some factor in order to bring the loss to
a reasonable range) and then undo this scaling after the output of the network. This effectively
linearizes our reconstruction networks, meaning a scaling of the image sent to the network will
exactly scale the output by that value. We also ﬁnd this is a more effective and simpler alternative to"
RETURN C,0.6715686274509803,Under review as a conference paper at ICLR 2022
RETURN C,0.6740196078431373,"Algorithm 4: Parallel reconstruction/loss calculation. Camera image is c, sample planes on
multiple GPUs are vs, reconstruction networks on multiple GPUs are Rθ,s, mean for LHNMSE
normalization is µH(v), mean for LNMSE normalization is µv, and weight for LNMSE is β."
RETURN C,0.6764705882352942,"Input
:c, vs, Rθ,s, µH(v), µv, β
Output :L
// compute reconstruction and loss in parallel on multiple GPUs"
RETURN C,0.678921568627451,"1 ˆvs ←concatenate(parallel(Rj
s(c) for Rj
s in Rθ,s))"
RETURN C,0.6813725490196079,"2 Ls ←parallel( E[(H(vj
s)−H(ˆvj
s))2]
µH(v)
+ β E[(vj
s−ˆvj
s)2]
µv
for (ˆvj
s, vj
s) in (ˆvs, vs))
// compute mean of scattered losses on single GPU"
RETURN C,0.6838235294117647,3 L ←E[Ls]
RETURN L,0.6862745098039216,4 return L
RETURN L,0.6887254901960784,"using a BatchNorm on our inputs. We continue to use BatchNorm between our convolution layers
within the reconstruction network [15], which is effectively InstanceNorm in our case where batch
size is 1 [31]."
RETURN L,0.6911764705882353,"Planewise network training logic When we train PSFs by optimizing φ, we train separate recon-
struction networks per plane. This allows us to ﬂexibly compute sparse gradients across different
planes from iteration to iteration, as described in Appendix A.2. In order to do this, we create
placeholder networks on any number of GPUs, then copy the parameters stored on CPU for each
plane’s reconstruction network to a network on the GPU as needed during a forward pass. After
calculating an update with the optimizer, we copy the parameter values back to the corresponding
parameter on CPU."
RETURN L,0.6936274509803921,"Training times We optimize our Type D microscopy experiments on 8 RTX 2080 Ti GPUs. For
these, we can compare training times for the different network architectures. One training iteration
(including microscope simulation, reconstruction, backpropagation, and parameter update) takes
∼0.6 seconds for FourierNet2D and ∼1.3 seconds for UNet2D when optimizing both φ and θ. One
training iteration takes ∼0.4 seconds for FourierNet3D, ∼0.7 seconds for FourierUNet3D, and ∼0.8
seconds for UNet3D when only optimizing θ. More details are found in Tables 5 and 6."
RETURN L,0.696078431372549,"Table 5: Type D experiment training times
Network
Optimizing
# parameters
# train steps
Train step time (s)
Total time (h)"
RETURN L,0.6985294117647058,"FourierNet2D
θ, φ
∼4.2 × 107
106
∼0.8
∼222
FourierNet3D
θ
∼6.3 × 107
106
∼0.4
∼111
FourierUNet3D
θ
∼8.4 × 107
106
∼0.7
∼194"
RETURN L,0.7009803921568627,"UNet2D
θ, φ
∼4.0 × 107
106
∼1.3
∼361
UNet3D
θ
∼1.0 × 108
106
∼0.8
∼222"
RETURN L,0.7034313725490197,"A.4
DETAILS FOR FOURIERNETS OUTPERFORM UNETS FOR ENGINEERING NON-LOCAL
OPTICAL ENCODERS AND 3D SNAPSHOT MICROSCOPY VOLUME RECONSTRUCTION"
RETURN L,0.7058823529411765,"For our experiments in Sections 3.1 and 3.2, we use 40 planes at 5µm resolution in z and therefore 40
reconstruction networks to train PSFs. When training reconstruction networks only to produce the
higher quality reconstructions, we use 96 planes at 1µm resolution in z (chosen so that the planes
actually span 200 µm in z). We train in both settings without any sparse planewise gradients, meaning
we image and reconstruct all 40 or all 96 planes, respectively. We show details of all datasets used for
training reconstructions in Table 4."
RETURN L,0.7083333333333334,"We show the details of our FourierNet2D architecture for training PSFs in Table 7 and our Fourier-
Net3D architecture for training reconstruction networks in Table 8. We also show details for training
times for both training PSFs and for training more powerful reconstruction networks in Table 5. We"
RETURN L,0.7107843137254902,Under review as a conference paper at ICLR 2022
RETURN L,0.7132352941176471,"Table 6: Type A, B, C experiment training times
Network
Optimizing
# parameters Type # train steps Train step time (s) Total time (h)"
RETURN L,0.7156862745098039,"FourierNet2D θ, φ
∼1.7 × 108
A
5.8 × 105
∼1.1
∼177
FourierNet3D θ (ﬁxed φ for A) ∼3.4 × 108
A
∼2.6 × 105
∼1.6
∼116
FourierNet3D θ (ﬁxed φ for A) ∼3.4 × 108
B
∼1.3 × 105
∼1.6
∼58
FourierNet3D θ (ﬁxed φ for A) ∼3.4 × 108
C
∼1.3 × 105
∼1.6
∼58"
RETURN L,0.7181372549019608,"FourierNet2D θ, φ
∼1.7 × 108
B
5.8 × 105
∼1.1
∼177
FourierNet3D θ (ﬁxed φ for B) ∼3.4 × 108
A
∼1.2 × 105
∼1.6
∼53
FourierNet3D θ (ﬁxed φ for B) ∼3.4 × 108
B
106
∼1.6
∼444
FourierNet3D θ (ﬁxed φ for B) ∼3.4 × 108
C
∼5.0 × 105
∼1.6
∼222"
RETURN L,0.7205882352941176,"FourierNet2D θ, φ
∼1.7 × 108
C
5.8 × 105
∼1.1
∼177
FourierNet3D θ (ﬁxed φ for C) ∼3.4 × 108
A
∼3.4 × 105
∼1.6
∼151
FourierNet3D θ (ﬁxed φ for C) ∼3.4 × 108
B
∼3.4 × 105
∼1.6
∼151
FourierNet3D θ (ﬁxed φ for C) ∼3.4 × 108
C
∼3.7 × 105
∼1.6
∼164"
RETURN L,0.7230392156862745,"Table 7: FourierNet2D detailed architecture (1 per plane)
Layer type
Kernel size
Stride
Notes
Shape (C, D, H, W)"
RETURN L,0.7254901960784313,"InputScaling
-
-
scale: 0.01
(1, 1, 256, 256)
FourierConv2D
(256, 256)
(2, 2)
-
(8, 1, 256, 256)
LeakyReLU
-
-
slope: -0.01
(8, 1, 256, 256)
BatchNorm2D
-
-
-
(8, 1, 256, 256)
Conv2D
(11, 11)
(1, 1)
-
(1, 1, 256, 256)
ReLU
-
-
-
(1, 1, 256, 256)
InputRescaling
-
-
scale: 0.01
(1, 1, 256, 256)"
RETURN L,0.7279411764705882,"Table 8: FourierNet3D detailed architecture (8 GPUs)
Layer type
Kernel size
Stride
Notes
Shape (C, D, H, W)"
RETURN L,0.7303921568627451,"InputScaling
-
-
scale: 0.01
(1, 1, 256, 256)
FourierConv2D
(256, 256)
(2, 2)
-
(60, 1, 256, 256)
LeakyReLU
-
-
slope: -0.01
(60, 1, 256, 256)
BatchNorm2D
-
-
-
(60, 1, 256, 256)
Reshape2D3D
-
-
-
(5, 12, 256, 256)
Conv3D
(11, 7, 7)
(1, 1, 1)
-
(5, 12, 256, 256)
LeakyReLU
-
-
slope: -0.01
(5, 12, 256, 256)
BatchNorm3D
-
-
-
(5, 12, 256, 256)
Conv3D
(11, 7, 7)
(1, 1, 1)
-
(1, 12, 256, 256)
ReLU
-
-
-
(1, 12, 256, 256)
InputRescaling
-
-
scale: 0.01
(1, 12, 256, 256)"
RETURN L,0.7328431372549019,"trained all networks for Type D for the same number of iterations (more than necessary for PSFs to
meaningfully converge)2."
RETURN L,0.7352941176470589,"The architecture of FourierUNet3D is 4 scales, with a cropping factor of 2 per scale in the encoding
path and an upsampling factor of 2 in the decoding path. For each scale, we perform a Fourier
convolution in the encoding path producing 480 feature maps, which are concatenated with the
incoming feature maps of the decoding convolutions at the corresponding scale (just as in a normal
UNet). In the decoding path, we use 3D convolutions with kernel size (3, 5, 5), producing 12 3D
feature maps each. There are two such convolutions per scale. Note that this requires we reshape
the 2D feature maps from the Fourier convolutions to 3D. This is followed by a 1x1 convolution"
"TRAINING
TIMES
ARE",0.7377450980392157,"2Training
times
are
approximate,
and
actual
total
time
was
longer
due
to
checkpoint-
ing/snapshotting/validation of data and/or differences in load on the clusters being used."
"TRAINING
TIMES
ARE",0.7401960784313726,Under review as a conference paper at ICLR 2022
"TRAINING
TIMES
ARE",0.7426470588235294,"Table 9: FourierUNet3D detailed architecture (8 GPUs)
Scale
Repeat
Layer type
Kernel size
Stride
Notes
Shape (C, D, H, W)"
"TRAINING
TIMES
ARE",0.7450980392156863,"1
1
InputScaling
-
-
scale: 0.01
(1, 1, 256, 256)
1
1
Multiscale
FourierConv2D
+ ReLU
+ BatchNorm2D"
"TRAINING
TIMES
ARE",0.7475490196078431,"(256, 256)
(2, 2)
-
(60, 1, 256, 256)"
"TRAINING
TIMES
ARE",0.75,"2
(128, 128)
(2, 2)
(60, 1, 128, 128)
3
(64, 64)
(2, 2)
(60, 1, 64, 64)
4
(32, 32)
(2, 2)
(60, 1, 32, 32)"
"TRAINING
TIMES
ARE",0.7524509803921569,"4
1
Reshape2D3D
-
-
-
(5, 12, 32, 32)
3
1
Upsample2D
-
-
-
(5, 12, 64, 64)
3
2
Conv3D
+ ReLU
+ BatchNorm3D"
"TRAINING
TIMES
ARE",0.7549019607843137,"(11, 7, 7)
(1, 1, 1)
-
(5, 12, 64, 64)"
"TRAINING
TIMES
ARE",0.7573529411764706,"2
1
Upsample2D
-
-
-
(5, 12, 128, 128)
2
2
Conv3D
+ ReLU
+ BatchNorm3D"
"TRAINING
TIMES
ARE",0.7598039215686274,"(11, 7, 7)
(1, 1, 1)
-
(5, 12, 128, 128)"
"TRAINING
TIMES
ARE",0.7622549019607843,"1
1
Upsample2D
-
-
-
(5, 12, 256, 256)
1
2
Conv3D
+ ReLU
+ BatchNorm3D"
"TRAINING
TIMES
ARE",0.7647058823529411,"(11, 7, 7)
(1, 1, 1)
-
(5, 12, 256, 256)"
"TRAINING
TIMES
ARE",0.7671568627450981,"1
1
Conv3D
+ ReLU
(1, 1, 1)
(1, 1, 1)
-
(1, 12, 256, 256)"
"TRAINING
TIMES
ARE",0.7696078431372549,"1
1
InputRescaling
-
-
scale: 0.01
(1, 12, 256, 256)"
"TRAINING
TIMES
ARE",0.7720588235294118,"producing the 3D reconstruction output. We show a diagram of this architecture in Figure 1C, and
details of this architecture in Table 9."
"TRAINING
TIMES
ARE",0.7745098039215687,"For our UNet2D, each encoding convolution produced 24 feature maps (except the ﬁrst scale, for
which the ﬁrst convolution produced 12 feature maps and the second convolution produced 24 feature
maps). Each decoding convolution produced 24 feature maps, but took an input of 48 feature maps
where 24 feature maps were concatenated from the corresponding encoding convolution at that scale.
At the end of the UNet2D, a (1, 1) convolution reduced the 24 ﬁnal feature maps to 1 feature map.
This single feature map is interpreted as the ﬁnal output of the network, i.e. the reconstructed plane.
UNet2D requires many more feature maps per plane and more layers than FourierNet, because these
are necessary in order for the network to be able to integrate information from a larger ﬁeld of view.
We show the details of our UNet2D architecture in Table 10."
"TRAINING
TIMES
ARE",0.7769607843137255,"The architecture of the vanilla UNet3D is also 4 scales, with a max pooling factor of 2 per scale in
the encoding path and an upsampling factor of 2 in the decoding path. Each scale of the encoding
path produces 480 2D feature maps. These are concatenated to the incoming feature maps of the
decoding convolutions at the corresponding scale, again with a reshape from 2D to 3D. Each scale
of the decoding path produces 48 3D feature maps. Again, this is followed by a 1x1 convolution
producing the 3D reconstruction output. All convolutions are in 3D with a kernel size of (5, 7, 7),
with the z dimension being ignored for the encoding path because the input is 2D. We show the
details of our UNet3D architecture in Table 11."
"TRAINING
TIMES
ARE",0.7794117647058824,"A.5
DETAILS FOR ENGINEERED OPTICAL ENCODING DEPENDS ON SAMPLE STRUCTURE"
"TRAINING
TIMES
ARE",0.7818627450980392,"For our experiments in Section 3.3, we use 64 planes at 1µm resolution in z and therefore 64
reconstruction networks to train PSFs. When training reconstruction networks only to produce the
higher quality reconstructions, we use 128 planes at 1µm resolution in z (chosen so that the planes
actually span 250 µm in z). We train in the reconstruction only setting without any sparse planewise
gradients, meaning we image and reconstruct all 128 planes. However, when training a PSF we image
and reconstruct 40 planes at a time with gradient per iteration (spread across 8 GPUs). These 40"
"TRAINING
TIMES
ARE",0.7843137254901961,Under review as a conference paper at ICLR 2022
"TRAINING
TIMES
ARE",0.7867647058823529,"Table 10: UNet2D detailed architecture (1 per plane)
Scale
Repeat
Layer type
Kernel size
Stride
Notes
Shape (C, D, H, W)"
"TRAINING
TIMES
ARE",0.7892156862745098,"1
1
InputScaling
-
-
scale: 0.01
(1, 1, 256, 256)
1
1
Conv2D
+ ReLU
+ BatchNorm2D"
"TRAINING
TIMES
ARE",0.7916666666666666,"(7, 7)
(1, 1)
-
(12, 1, 256, 256)"
"TRAINING
TIMES
ARE",0.7941176470588235,"1
1
Conv2D
+ ReLU
+ BatchNorm2D"
"TRAINING
TIMES
ARE",0.7965686274509803,"(7, 7)
(1, 1)
-
(24, 1, 256, 256)"
"TRAINING
TIMES
ARE",0.7990196078431373,"2
1
MaxPool2D
(2, 2)
(2, 2)
-
(24, 1, 128, 128)
2
2
Conv2D
+ ReLU
+ BatchNorm2D"
"TRAINING
TIMES
ARE",0.8014705882352942,"(7, 7)
(1, 1)
-
(24, 1, 128, 128)"
"TRAINING
TIMES
ARE",0.803921568627451,"n
1
MaxPool2D
(2, 2)
(2, 2)
-
(24, 1,
256
2n−1 ,
256
2n−1 )
n
2
Conv2D
+ ReLU
+ BatchNorm2D"
"TRAINING
TIMES
ARE",0.8063725490196079,"(7, 7)
(1, 1)
-
(24, 1,
256
2n−1 ,
256
2n−1 )"
"TRAINING
TIMES
ARE",0.8088235294117647,"8
1
MaxPool2D
(2, 2)
(2, 2)
-
(24, 1, 2, 2)
8
2
Conv2D
+ ReLU
+ BatchNorm2D"
"TRAINING
TIMES
ARE",0.8112745098039216,"(7, 7)
(1, 1)
-
(24, 1, 2, 2)"
"TRAINING
TIMES
ARE",0.8137254901960784,"7
1
Upsample2D
-
-
-
(24, 1, 4, 4)
7
2
Conv2D
+ ReLU
+ BatchNorm2D"
"TRAINING
TIMES
ARE",0.8161764705882353,"(7, 7)
(1, 1)
-
(24, 1, 4, 4)"
"TRAINING
TIMES
ARE",0.8186274509803921,"n
1
Upsample2D
-
-
-
(24, 1,
256
2n−1 ,
256
2n−1 )
n
2
Conv2D
+ ReLU
+ BatchNorm2D"
"TRAINING
TIMES
ARE",0.821078431372549,"(7, 7)
(1, 1)
-
(24, 1,
256
2n−1 ,
256
2n−1 )"
"TRAINING
TIMES
ARE",0.8235294117647058,"1
1
Upsample2D
-
-
-
(24, 1, 256, 256)
1
2
Conv2D
+ ReLU
+ BatchNorm2D"
"TRAINING
TIMES
ARE",0.8259803921568627,"(7, 7)
(1, 1)
-
(24, 1, 256, 256)"
"TRAINING
TIMES
ARE",0.8284313725490197,"1
1
Conv2D
+ ReLU
(1, 1)
(1, 1)
-
(1, 1, 256, 256)"
"TRAINING
TIMES
ARE",0.8308823529411765,"1
1
InputRescaling
-
-
scale: 0.01
(1, 1, 256, 256)"
"TRAINING
TIMES
ARE",0.8333333333333334,"planes are chosen randomly at every iteration from the 64 total possible planes, making potentially
separate draws of planes for imaging and reconstruction. We show details of all datasets used for
training reconstructions in Table 4."
"TRAINING
TIMES
ARE",0.8357843137254902,"We show the details of our FourierNet2D architecture for training PSFs at the larger ﬁeld of view in
Type A, B, C in Table 12 and our FourierNet3D architecture for training reconstruction networks at
the larger ﬁeld of view in Type A, B, C in Table 13. There are no other networks used for these larger
ﬁeld of view experiments. We also show details for training times for both training PSFs and for
training more powerful reconstruction networks in Table 6. All PSFs in these networks were trained
for the same number of iterations. However, reconstruction networks for some of these experiments
were only trained for as long as necessary to converge (with some exceptions where we attempted
longer training to check for performance gains with long training periods). Generally, we observed
that performance for such reconstruction networks does not meaningfully change with many more
iterations of training3."
"TRAINING
TIMES
ARE",0.8382352941176471,"3Training
times
are
approximate,
and
actual
total
time
was
longer
due
to
checkpoint-
ing/snapshotting/validation of data and/or differences in load on the clusters being used."
"TRAINING
TIMES
ARE",0.8406862745098039,Under review as a conference paper at ICLR 2022
"TRAINING
TIMES
ARE",0.8431372549019608,"Table 11: UNet3D detailed architecture (8 GPUs)
Scale
Repeat
Layer type
Kernel size
Stride
Notes
Shape (C, D, H, W)"
"TRAINING
TIMES
ARE",0.8455882352941176,"1
1
InputScaling
-
-
scale: 0.01
(1, 1, 256, 256)
1
1
Conv2D
+ ReLU
+ BatchNorm2D"
"TRAINING
TIMES
ARE",0.8480392156862745,"(7, 7)
(1, 1)
-
(30, 1, 256, 256)"
"TRAINING
TIMES
ARE",0.8504901960784313,"1
1
Conv2D
+ ReLU
+ BatchNorm2D"
"TRAINING
TIMES
ARE",0.8529411764705882,"(7, 7)
(1, 1)
-
(60, 1, 256, 256)"
"TRAINING
TIMES
ARE",0.8553921568627451,"2
1
MaxPool2D
(2, 2)
(2, 2)
-
(60, 1, 128, 128)
2
2
Conv2D
+ ReLU
+ BatchNorm2D"
"TRAINING
TIMES
ARE",0.8578431372549019,"(7, 7)
(1, 1)
-
(60, 1, 128, 128)"
"TRAINING
TIMES
ARE",0.8602941176470589,"3
1
MaxPool2D
(2, 2)
(2, 2)
-
(60, 1, 64, 64)
3
2
Conv2D
+ ReLU
+ BatchNorm2D"
"TRAINING
TIMES
ARE",0.8627450980392157,"(7, 7)
(1, 1)
-
(60, 1, 64, 64)"
"TRAINING
TIMES
ARE",0.8651960784313726,"4
1
MaxPool2D
(2, 2)
(2, 2)
-
(60, 1, 32, 32)
4
2
Conv2D
+ ReLU
+ BatchNorm2D"
"TRAINING
TIMES
ARE",0.8676470588235294,"(7, 7)
(1, 1)
-
(60, 1, 32, 32)"
"TRAINING
TIMES
ARE",0.8700980392156863,"4
1
Reshape2D3D
-
-
-
(5, 12, 32, 32)
3
1
Upsample2D
-
-
-
(5, 12, 64, 64)
3
2
Conv3D
+ ReLU
+ BatchNorm3D"
"TRAINING
TIMES
ARE",0.8725490196078431,"(11, 7, 7)
(1, 1, 1)
-
(5, 12, 64, 64)"
"TRAINING
TIMES
ARE",0.875,"2
1
Upsample2D
-
-
-
(5, 12, 128, 128)
2
2
Conv3D
+ ReLU
+ BatchNorm3D"
"TRAINING
TIMES
ARE",0.8774509803921569,"(11, 7, 7)
(1, 1, 1)
-
(5, 12, 128, 128)"
"TRAINING
TIMES
ARE",0.8799019607843137,"1
1
Upsample2D
-
-
-
(5, 12, 256, 256)
1
2
Conv3D
+ ReLU
+ BatchNorm3D"
"TRAINING
TIMES
ARE",0.8823529411764706,"(11, 7, 7)
(1, 1, 1)
-
(5, 12, 256, 256)"
"TRAINING
TIMES
ARE",0.8848039215686274,"1
1
Conv3D
+ ReLU
(1, 1, 1)
(1, 1, 1)
-
(1, 12, 256, 256)"
"TRAINING
TIMES
ARE",0.8872549019607843,"1
1
InputRescaling
-
-
scale: 0.01
(1, 12, 256, 256)"
"TRAINING
TIMES
ARE",0.8897058823529411,"Table 12: FourierNet2D detailed architecture (1 per plane)
Layer type
Kernel size
Stride
Notes
Shape (C, D, H, W)"
"TRAINING
TIMES
ARE",0.8921568627450981,"InputScaling
-
-
scale: 0.01
(1, 1, 512, 512)
FourierConv2D
(512, 512)
(2, 2)
-
(5, 1, 512, 512)
LeakyReLU
-
-
slope: -0.01
(5, 1, 512, 512)
BatchNorm2D
-
-
-
(5, 1, 512, 512)
Conv2D
(11, 11)
(1, 1)
-
(1, 1, 512, 512)
ReLU
-
-
-
(1, 1, 512, 512)
InputRescaling
-
-
scale: 0.01
(1, 1, 512, 512)"
"TRAINING
TIMES
ARE",0.8946078431372549,"A.6
DETAILS FOR FOURIERNETS OUTPERFORM STATE-OF-THE-ART FOR RECONSTRUCTING
NATURAL IMAGES CAPTURED BY DIFFUSERCAM LENSLESS CAMERA"
"TRAINING
TIMES
ARE",0.8970588235294118,"We performed no augmentations for this set of trainings reconstructing RGB color images of natural
scenes from RGB diffused images taken through a DiffuserCam [20]. We modiﬁed our FourierNet2D
architecture to create the FourierNetRGB architecture and our FourierUNet2D architecture to create"
"TRAINING
TIMES
ARE",0.8995098039215687,Under review as a conference paper at ICLR 2022
"TRAINING
TIMES
ARE",0.9019607843137255,"Table 13: FourierNet3D detailed architecture (8 GPUs)
Layer type
Kernel size
Stride
Notes
Shape (C, D, H, W)"
"TRAINING
TIMES
ARE",0.9044117647058824,"InputScaling
-
-
scale: 0.01
(1, 1, 512, 512)
FourierConv2D
(512, 512)
(2, 2)
-
(80, 1, 512, 512)
LeakyReLU
-
-
slope: -0.01
(80, 1, 512, 512)
BatchNorm2D
-
-
-
(80, 1, 512, 512)
Reshape2D3D
-
-
-
(5, 16, 512, 512)
Conv3D
(11, 7, 7)
(1, 1, 1)
-
(5, 16, 512, 512)
LeakyReLU
-
-
slope: -0.01
(5, 16, 512, 512)
BatchNorm3D
-
-
-
(5, 16, 512, 512)
Conv3D
(11, 7, 7)
(1, 1, 1)
-
(1, 16, 512, 512)
ReLU
-
-
-
(1, 16, 512, 512)
InputRescaling
-
-
scale: 0.01
(1, 16, 512, 512)"
"TRAINING
TIMES
ARE",0.9068627450980392,"Table 14: DLMD experiment training times.
Superscripts denote loss function:
1 MSE, 2
MSE+LPIPS.
Network
Optimizing
# parameters
# train steps
Train step time (s)
Total time (h)"
"TRAINING
TIMES
ARE",0.9093137254901961,"FourierNetRGB1
θ
∼1.6 × 107
2.2 × 105
∼0.43
∼26
FourierNetRGB2
θ
∼1.6 × 107
1.1 × 105
∼0.47
∼14
FourierUNetRGB1
θ
∼7.1 × 107
2.5 × 105
∼3.3
∼229"
"TRAINING
TIMES
ARE",0.9117647058823529,"Le-ADMM-U2 [20]
θ
∼4.0 × 107
-
-
-
UNet2 [20]
θ
∼1.0 × 108
-
-
-"
"TRAINING
TIMES
ARE",0.9142156862745098,"Table 15: FourierNetRGB detailed architecture
Layer type
Kernel size
Stride
Notes
Shape (N, C, H, W)"
"TRAINING
TIMES
ARE",0.9166666666666666,"FourierConv2D
(270, 480)
(2, 2)
-
(4, 3, 270, 480)
LeakyReLU
-
-
slope: -0.01
(4, 20, 270, 480)
BatchNorm2D
-
-
-
(4, 20, 270, 480)
Conv2D
(11, 11)
(1, 1)
-
(4, 64, 270, 480)
BatchNorm2D
-
-
-
(4, 64, 270, 480)
LeakyReLU
-
-
slope: -0.01
(4, 64, 270, 480)
Conv2D
(11, 11)
(1, 1)
-
(4, 64, 270, 480)
BatchNorm2D
-
-
-
(4, 64, 270, 480)
LeakyReLU
-
-
slope: -0.01
(4, 64, 270, 480)
Conv2D
(11, 11)
(1, 1)
-
(4, 3, 270, 480)
ReLU
-
-
-
(4, 3, 270, 480)"
"TRAINING
TIMES
ARE",0.9191176470588235,"the FourierUNetRGB architecture, outlined in Table 15 and Table 16 respectively. Training details are
shown in Table 14. Because these reconstructions are of 2D images only and required no microscope
simulation, we were able to use a batch size of 4 images per iteration."
"TRAINING
TIMES
ARE",0.9215686274509803,Under review as a conference paper at ICLR 2022
"TRAINING
TIMES
ARE",0.9240196078431373,"Table 16: FourierUNetRGB detailed architecture
Scale
Repeat
Layer type
Kernel size
Stride
Notes
Shape (N, C, H, W)"
"TRAINING
TIMES
ARE",0.9264705882352942,"1
1
Multiscale
FourierConv2D
+ ReLU
+ BatchNorm2D"
"TRAINING
TIMES
ARE",0.928921568627451,"(270, 480)
(2, 2)
-
(4, 64, 270, 480)"
"TRAINING
TIMES
ARE",0.9313725490196079,"2
(135, 240)
(2, 2)
(4, 64, 135, 240)
3
(67, 120)
(2, 2)
(4, 64, 67, 120)
4
(33, 60)
(2, 2)
(4, 64, 33, 60)"
"TRAINING
TIMES
ARE",0.9338235294117647,"3
1
Upsample2D
-
-
-
(4, 64, 67, 120)
3
2
Conv2D
+ ReLU
+ BatchNorm2D"
"TRAINING
TIMES
ARE",0.9362745098039216,"(11, 11)
(1, 1)
-
(4, 64, 67, 120)"
"TRAINING
TIMES
ARE",0.9387254901960784,"2
1
Upsample2D
-
-
-
(4, 64, 135, 240)
2
2
Conv2D
+ ReLU
+ BatchNorm2D"
"TRAINING
TIMES
ARE",0.9411764705882353,"(11, 11)
(1, 1)
-
(4, 64, 135, 240)"
"TRAINING
TIMES
ARE",0.9436274509803921,"1
1
Upsample2D
-
-
-
(4, 64, 270, 480)
1
2
Conv2D
+ ReLU
+ BatchNorm2D"
"TRAINING
TIMES
ARE",0.946078431372549,"(11, 11)
(1, 1)
-
(4, 64, 270, 480)"
"TRAINING
TIMES
ARE",0.9485294117647058,"1
1
Conv2D
+ ReLU
(1, 1)
(1, 1)
-
(4, 3, 270, 480)"
"TRAINING
TIMES
ARE",0.9509803921568627,Under review as a conference paper at ICLR 2022
"TRAINING
TIMES
ARE",0.9534313725490197,ground truth 50 μm
"TRAINING
TIMES
ARE",0.9558823529411765,"FourierNet2D
FourierNet3D"
"TRAINING
TIMES
ARE",0.9583333333333334,"FourierNet2D
FourierUNet3D"
"TRAINING
TIMES
ARE",0.9607843137254902,"FourierNet2D
UNet3D"
"TRAINING
TIMES
ARE",0.9632352941176471,"UNet2D
UNet3D"
"TRAINING
TIMES
ARE",0.9656862745098039,256 × 256 px camera
"TRAINING
TIMES
ARE",0.9681372549019608,"Figure 8: Slab views of a Type D example volume reconstruction, showing our methods (Fouri-
erNet/FourierUNet) do the best job of reconstructing throughout the volume. Note that the UNet
reconstructions are blurry across all slabs, with few exceptions. Colored boxes show which sample
planes a particular slab comes from, corresponding to boxes in xz projection view at top. Annotation
Mφ shows which network architecture was used for microscope optimization; annotation Rθ shows
which architecture was used for reconstruction."
"TRAINING
TIMES
ARE",0.9705882352941176,Under review as a conference paper at ICLR 2022
"TRAINING
TIMES
ARE",0.9730392156862745,"ground truth (Type A)
optimized for Type A
optimized for Type B
optimized for Type C"
"TRAINING
TIMES
ARE",0.9754901960784313,"50 μm
512 × 512 px camera"
"TRAINING
TIMES
ARE",0.9779411764705882,"Figure 9: Slab views of an example Type A volume show that the microscope optimized for Type A
results in the best reconstructions. Note that the reconstruction with a microscope optimized for Type
A is almost identical to the ground truth, while the other microscopes create blurrier reconstructions.
Slabs are xy max projections in thinner chunks as opposed to projecting through the entire volume.
Colored boxes show which sample planes a particular slab comes from, corresponding to boxes in xz
projection view at top."
"TRAINING
TIMES
ARE",0.9803921568627451,Under review as a conference paper at ICLR 2022
"TRAINING
TIMES
ARE",0.9828431372549019,"ground truth (Type B)
optimized for Type A
optimized for Type B
optimized for Type C 50 μm"
"TRAINING
TIMES
ARE",0.9852941176470589,512 × 512 px camera
"TRAINING
TIMES
ARE",0.9877450980392157,"Figure 10: Slab views of an example Type B volume show that the microscope optimized for Type B
results in the best reconstructions; other microscopes result in blurrier reconstructions. Colored boxes
show which sample planes a particular slab comes from, corresponding to boxes in xz projection
view at top."
"TRAINING
TIMES
ARE",0.9901960784313726,Under review as a conference paper at ICLR 2022
"TRAINING
TIMES
ARE",0.9926470588235294,"ground truth (Type C)
optimized for Type A
optimized for Type B
optimized for Type C 50 μm"
"TRAINING
TIMES
ARE",0.9950980392156863,512 × 512 px camera
"TRAINING
TIMES
ARE",0.9975490196078431,"Figure 11: Slab views of an example Type C volume show that microscope optimized for Type C
provides most consistent reconstruction. Colored boxes have same meaning as Figures 9, 10."
