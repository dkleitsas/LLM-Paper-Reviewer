Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0036101083032490976,"We introduce Resilient Multiple Choice Learning (rMCL), an extension of the
MCL approach for conditional distribution estimation in regression settings where
multiple targets may be sampled for each training input. Multiple Choice Learning
is a simple framework to tackle multimodal density estimation, using the Winner-
Takes-All (WTA) loss for a set of hypotheses. In regression settings, the existing
MCL variants focus on merging the hypotheses, thereby eventually sacriﬁcing
the diversity of the predictions. In contrast, our method relies on a novel learned
scoring scheme underpinned by a mathematical framework based on Voronoi tessel-
lations of the output space, from which we can derive a probabilistic interpretation.
After empirically validating rMCL with experiments on synthetic data, we further
assess its merits on the sound source localization task, demonstrating its practical
usefulness and the relevance of its interpretation."
INTRODUCTION,0.007220216606498195,"1
Introduction"
INTRODUCTION,0.010830324909747292,"Machine learning models are commonly trained to produce, for any given input, a single prediction.
In most cases, for instance, when minimizing the empirical risk with quadratic loss, this prediction
can be interpreted as the conditional output expectation given the input. However, there are many
tasks for which the conditional output distribution can be multimodal, either by the nature of the task
or due to various sources of uncertainty. Temporal tracking and forecasting, for instance, are problems
of this type [5, 32, 44]. In such cases, the mean might fall in a low-density region of the conditional
probability function, and it would be beneﬁcial to predict multiple hypotheses instead [34, 20]. In
this context, Multiple Choice Learning (MCL) has emerged as a simple pragmatic solution [17, 29].
Thanks to a network architecture with multiple heads, MCL models can produce multiple hypotheses,
one per head. During supervised training, the gradients are only computed for the head that provides
the best prediction given the current input sample. This Winner-Takes-All (WTA) training scheme
allows each head to specialize in a region of the output space. Accordingly, Rupprecht et al. [34]
have proposed a probabilistic interpretation of MCL based on Voronoi tessellations."
INTRODUCTION,0.01444043321299639,"On that account, MCL suffers from two signiﬁcant issues: hypotheses collapse and overconﬁdence.
Hypotheses collapse occurs during training when a head takes the lead, the others being almost never
selected under WTA and thus not updated. This leads to a situation where most heads are not trained
effectively and produce meaningless outputs during inference. Overconﬁdence can also be observed
at inference time when looking at MCL under the lens of its probabilistic interpretation. Hypotheses
that correspond to rare events tend to be over-represented, thus not reﬂecting the true distribution of"
INTRODUCTION,0.018050541516245487,"outputs well. Multiple variants of MCL have been developed to alleviate these issues, in particular
for classiﬁcation or segmentation tasks [29, 27, 42, 13]."
INTRODUCTION,0.021660649819494584,"In this paper, we are interested in MCL in the context of regression settings, which has been mostly
overlooked. More speciﬁcally, we propose rMCL, for resilient Multiple Choice Learning, a MCL
variant with a learned scoring scheme to mitigate overconﬁdence. Unlike prior work, rMCL is
designed to handle more general settings where multiple targets may be sampled for each input
during training. Moreover, we propose a probabilistic interpretation of rMCL, and show that rMCL’s
scoring ﬁxes overconﬁdence by casting multi-choice regression as conditional distribution estimation.
While introduced in the context of the original WTA loss, the proposed scoring approach is also
compatible with other variants of WTA. We validate our claims with experiments on synthetic data,
and we further evaluate the performance of rMCL on the sound source localization problem. The
accompanying code with a rMCL implementation is made available.1"
RELATED WORK,0.02527075812274368,"2
Related Work"
RELATED WORK,0.02888086642599278,"Uncertainty estimation. The estimation of machine learning model’s uncertainty can be studied
from different perspectives depending on the type of ambiguity being considered. The uncertainty can
concern either the model (epistemic) or the data (aleatoric) [7, 12]. Kendall and Gal [22] showed that
those types of uncertainty can be combined in deep learning settings, placing both a distribution on the
weights and the outputs of the model. Epistemic uncertainty estimation may, for instance, be achieved
by variational approximation of the weights posterior [31, 14]. Independent ensembles (IE) of neural
networks [18, 11] is another widespread technique for epistemic uncertainty approximation. Several
neural networks, trained from different random initializations, provide samples for approximating the
weights posterior distribution [26]."
RELATED WORK,0.032490974729241874,"Multiple Choice learning. Originally introduced by Guzman-Rivera et al. [17] and adapted to deep
learning settings by Lee et al. [28, 29], MCL is a framework in which multiple output heads propose
different possible predictions. Since many datasets contain only a single output realization for each
input, the heads are trained with a Winner-Takes-All scheme where only the head that made the best
prediction is updated. It can be understood as a speciﬁc case of dependent ensembles [8, 46, 4, 47]
where the members interact with each other during training. Alternatively, [34] have shown that MCL
can be adapted successfully for multi-label classiﬁcation [23]."
RELATED WORK,0.036101083032490974,"The Winner-Takes-All training scheme, however, can cause two main issues known as hypotheses
collapse and overconﬁdence [34, 6, 10, 19, 10, 27, 42]. Most related previous works tackle either or
both of those questions. Hypotheses collapse occurs during training when some heads receive little or
no updates due to bad initial values under the WTA loss and other hypotheses taking the lead. To ﬁx
this issue, Rupprecht et al. [34] proposed a relaxed Winner-Takes-All (RWTA, or ""-WTA) loss that
allows the update of non-winner heads, albeit with gradients scaled by a small constant "". While this
approach ensures that every head receives gradients updates, it also tends to level the different heads,
which counters the initial goal of WTA and thus needs careful tuning. Finally, one can leverage the
evolving Winner-Takes-All loss [32], with the top-n heads getting updated (top-n-WTA) instead of
only the best one. The authors validate that their approach, with a scheduled decreasing value for n,
achieves improved conditioning of the outputs by reducing inconsistent hypotheses in the context of
future frames prediction."
RELATED WORK,0.039711191335740074,"On the other hand, overconﬁdence is an issue that can be observed in inference when evaluating a
model as a density estimator. In this case, one would want the different outputs to be distributed
across the different modes of the true conditional probability of the outputs. Empirical observations
[27, 42] show that this is not usually the case, and rare events tend to be overly represented, rendering
such a model inadequate for integration within real-world decision-making systems. In this context,
Lee et al. [27] proposed Conﬁdent Multiple Choice Learning (CMCL) for solving overconﬁdence in
a classiﬁcation setup. Additionally to the WTA loss, CMCL is trained by maximizing the entropy of
the class distributions predicted by the non-selected hypotheses. The classiﬁer’s ﬁnal prediction is
based on a simple average of the discrete class distributions predicted by each of the heads. Although
designed for tackling the overconﬁdence problem, CMCL reduces the diversity of the hypotheses.
On that account, Versatile Multiple Choice Learning [42] (vMCL) proposed to address the issue by
leveraging a choice network aiming at predicting a score for each hypothesis head. This may be seen"
RELATED WORK,0.04332129963898917,1https://github.com/Victorletzelter/code-rMCL
RELATED WORK,0.04693140794223827,"as a variant of Mixture-of-Experts approaches [33], in which the choice network is supervised with
explicit targets. The ﬁnal prediction in vMCL is derived by weighting the class distributions from
each hypothesis by their respective scores. These works focus on classiﬁcation tasks and aggregate the
hypotheses, thereby losing individual information from each head. In contrast, we address regression
tasks and demonstrate how to beneﬁt from the diversity of predictions without aggregation."
RELATED WORK,0.05054151624548736,"We propose to revisit the vMCL approach for regression, single-target or multi-target, and to extend
accordingly the mathematical interpretation of the WTA proposed by Rupprecht et al. [34]."
RMCL REGRESSION AND ITS PROBABILISTIC INTERPRETATION,0.05415162454873646,"3
rMCL regression and its probabilistic interpretation"
RMCL REGRESSION AND ITS PROBABILISTIC INTERPRETATION,0.05776173285198556,"After reviewing MCL, this section introduces resilient Multiple Choice Learning, dubbed rMCL.
This proposed variant handles multi-target settings thanks to its scoring scheme. We then provide a
distribution learning interpretation of rMCL that is relevant to regression tasks, thus completing the
MCL probabilistic interpretations of prior works [9, 34]."
RMCL REGRESSION AND ITS PROBABILISTIC INTERPRETATION,0.061371841155234655,"3.1
Fixing the overconﬁdence issue in Multiple Choice Learning"
RMCL REGRESSION AND ITS PROBABILISTIC INTERPRETATION,0.06498194945848375,"Let X ✓Rd and Y ✓Rq be the input and output spaces in a supervised learning setting, such that
the training set D is composed of samples (xs, ys) from an underlying joint distribution p(x, y) over
X ⇥Y."
RMCL REGRESSION AND ITS PROBABILISTIC INTERPRETATION,0.06859205776173286,"Multiple choice learning was proposed to address tasks with ambiguous outputs, i.e., for which
the ground-truth distribution p(y | x) is multimodal [17, 29, 10]. Adapted by Lee et al. [29] for
deep-learning settings in Stochastic Multiple Choice Learning (sMCL), it leverages several models
f✓, (f 1"
RMCL REGRESSION AND ITS PROBABILISTIC INTERPRETATION,0.07220216606498195,"✓, . . . , f K"
RMCL REGRESSION AND ITS PROBABILISTIC INTERPRETATION,0.07581227436823104,"✓) 2 F(X, YK), referred to as K hypotheses, trained using the Winner-takes-all
(or Oracle) loss. It consists, given an underlying loss function ` and for each sample (xs, ys) in the
current batch, of ﬁrst the computation of"
RMCL REGRESSION AND ITS PROBABILISTIC INTERPRETATION,0.07942238267148015,"L(f✓(xs), ys) ,
min
k2[[1,K]] ` ! f k"
RMCL REGRESSION AND ITS PROBABILISTIC INTERPRETATION,0.08303249097472924,"✓(xs) , ys "" (1)"
RMCL REGRESSION AND ITS PROBABILISTIC INTERPRETATION,0.08664259927797834,"after each forward pass, followed by backpropagation on the winner hypothesis, that is, the minimizing
one. Assuming now that a set Ys of targets is available in the training set for input xs, Firman et
al. [10] have shown that (1) can be generalized by updating the best hypothesis per target using the
meta-loss"
RMCL REGRESSION AND ITS PROBABILISTIC INTERPRETATION,0.09025270758122744,"L (f✓(xs) , Ys) = X y2Ys K
X k=1 1 !"
RMCL REGRESSION AND ITS PROBABILISTIC INTERPRETATION,0.09386281588447654,"y 2 Yk (xs) "" ` ! f k"
RMCL REGRESSION AND ITS PROBABILISTIC INTERPRETATION,0.09747292418772563,"✓(xs) , y "" ,
(2) where"
RMCL REGRESSION AND ITS PROBABILISTIC INTERPRETATION,0.10108303249097472,"Yk(x) , $"
RMCL REGRESSION AND ITS PROBABILISTIC INTERPRETATION,0.10469314079422383,"y 2 Y, ` ! f k"
RMCL REGRESSION AND ITS PROBABILISTIC INTERPRETATION,0.10830324909747292,"✓(x), y """
RMCL REGRESSION AND ITS PROBABILISTIC INTERPRETATION,0.11191335740072202,< ` (f r
RMCL REGRESSION AND ITS PROBABILISTIC INTERPRETATION,0.11552346570397112,"✓(x), y) , 8r 6= k  .
(3)"
RMCL REGRESSION AND ITS PROBABILISTIC INTERPRETATION,0.11913357400722022,"This loss can, however, lead to poor conditioning of the output set of predictions (hypotheses collapse),
with only one or a few hypotheses being exclusively selected for backpropagation. Furthermore,
sMCL is subject to the overconﬁdence problem. Following Theorem 1 of [34], a necessary condition
for minimizing the risk Z X K
X k=1 Z Yk(x) ` ! f k"
RMCL REGRESSION AND ITS PROBABILISTIC INTERPRETATION,0.12274368231046931,"✓(x), y """
RMCL REGRESSION AND ITS PROBABILISTIC INTERPRETATION,0.1263537906137184,"p(x, y)dydx,
(4)"
RMCL REGRESSION AND ITS PROBABILISTIC INTERPRETATION,0.1299638989169675,is that each prediction f k
RMCL REGRESSION AND ITS PROBABILISTIC INTERPRETATION,0.13357400722021662,"✓(x) coincides with the centroid of Yk(x) with respect to p(y | x). We say
that the components {Yk(x)}k form a centroidal Voronoi tessellation [9]. If ` is for instance the `2-
loss, this condition means that for each non-zero probability cell k, f k"
RMCL REGRESSION AND ITS PROBABILISTIC INTERPRETATION,0.1371841155234657,"✓(x) amounts to the conditional
mean of p(y | x) within the Voronoi cell Yk(x). However, this theorem tells us nothing about the
predictions in very low probability zones; in such regions, the inference-time predictions f k"
RMCL REGRESSION AND ITS PROBABILISTIC INTERPRETATION,0.1407942238267148,"✓(x) will
be meaningless. During inference, the sMCL indeed faces a limitation: it cannot solely rely on the
predicted hypotheses to identify Voronoi cells in the output space with low probability mass (see
Figure 1). This observation leads us to propose hypothesis-scoring heads γ1"
RMCL REGRESSION AND ITS PROBABILISTIC INTERPRETATION,0.1444043321299639,"✓, . . . , γK"
RMCL REGRESSION AND ITS PROBABILISTIC INTERPRETATION,0.148014440433213,"✓2 F(X, [0, 1]).
Those aim to predict, for unseen input x, the probability P(Yx 2 Yk(x)) where Yx ⇠p(y | x) with
the aim of mitigating this overconﬁdence issue."
RMCL REGRESSION AND ITS PROBABILISTIC INTERPRETATION,0.15162454873646208,"While MCL variants have often been proposed for classiﬁcation tasks, we are, to the best of our
knowledge, the ﬁrst to propose to solve this issue for multi-target regression, interpreting the problem
as a multimodal conditional distribution estimation."
RESILIENT MULTIPLE CHOICE LEARNING,0.1552346570397112,"3.2
Resilient Multiple Choice Learning"
RESILIENT MULTIPLE CHOICE LEARNING,0.1588447653429603,"We consider hereafter a problem of estimation of a multimodal conditional distribution denoted
p(y | x) for each input x 2 X. In a real-world setting, only one or several samples (the targets)
drawn from p(y | x) are usually accessible. Sound source localization is a concrete instance of such
multimodal prediction for whose p(y | xs) represents the sound source position for an input audio
clip xs 2 X at a given time t."
RESILIENT MULTIPLE CHOICE LEARNING,0.1624548736462094,"In this last example, each target sample in Ys may represent the location of a mode of the ground-truth
multimodal distribution. For such multi-output regression tasks, the MCL training of a randomly
initialized multi-hypotheses model with scoring functions, (f 1"
RESILIENT MULTIPLE CHOICE LEARNING,0.16606498194945848,"✓, . . . , f K ✓, γ1"
RESILIENT MULTIPLE CHOICE LEARNING,0.16967509025270758,"✓, . . . , γK"
RESILIENT MULTIPLE CHOICE LEARNING,0.17328519855595667,"✓) can be adapted
as follows."
RESILIENT MULTIPLE CHOICE LEARNING,0.17689530685920576,"For each training sample (xs, Ys), let"
RESILIENT MULTIPLE CHOICE LEARNING,0.18050541516245489,"K+(xs) , ⇢"
RESILIENT MULTIPLE CHOICE LEARNING,0.18411552346570398,"k+ 2 [[1, K]] : 9y 2 Ys, k+ 2 argmin k `(f k"
RESILIENT MULTIPLE CHOICE LEARNING,0.18772563176895307,"✓(xs), y) ( (5)"
RESILIENT MULTIPLE CHOICE LEARNING,0.19133574007220217,"and K−(xs) , [[1, K]] −K+(xs) be the set of positive (or winner) and negative hypotheses respec-
tively. It is then possible to combine the multi-target WTA loss L in (2) with a hypothesis scoring
loss"
RESILIENT MULTIPLE CHOICE LEARNING,0.19494584837545126,"Lscoring(✓) , − ⇣
X"
RESILIENT MULTIPLE CHOICE LEARNING,0.19855595667870035,k+2K+(xs)
RESILIENT MULTIPLE CHOICE LEARNING,0.20216606498194944,log γk+
RESILIENT MULTIPLE CHOICE LEARNING,0.20577617328519857,"✓
(xs) + X"
RESILIENT MULTIPLE CHOICE LEARNING,0.20938628158844766,k−2K−(xs) log ⇣
RESILIENT MULTIPLE CHOICE LEARNING,0.21299638989169675,1 −γk−
RESILIENT MULTIPLE CHOICE LEARNING,0.21660649819494585,"✓
(xs) ⌘⌘ ,
(6)"
RESILIENT MULTIPLE CHOICE LEARNING,0.22021660649819494,"in a compound loss L + βLscoring. This novel approach differs from previous MCL variants [42] in its
ability to predict multimodal distributions in regression settings and by the introduction of separated
scoring branches that are updated based on loss (6), such that the target for the scoring branch k is
the probability that hypothesis k is among the winners for that sample."
RESILIENT MULTIPLE CHOICE LEARNING,0.22382671480144403,"A resource-efﬁcient implementation of rMCL is achieved by deriving the hypotheses and score heads
from a shared representation, with distinct parameters at the ﬁnal stages of the architecture (e.g.,
through independent fully connected layers). Designed as such, it is also possible to reduce memory
cost by updating only a fraction of score heads associated with the negative hypotheses at each
training step, typically with distinct samples k−⇠U (K−(xs)). This trick could also alleviate the
imbalanced binary classiﬁcation task that the scoring heads face with a large number of hypotheses
|K−(xs)| ≫|K+(xs)| (typically, the number of target samples at disposal is small relative to the
number of hypotheses K). The variants of the WTA (e.g., top-n-WTA, ""-WTA, see Sec. 2) are also
compatible with rMCL. Inference with the proposed rMCL model is outlined in Algorithm 1."
RESILIENT MULTIPLE CHOICE LEARNING,0.22743682310469315,"As highlighted above, the hypothesis output can be interpreted, in the context of risk minimization,
as the conditional mean of the Voronoi cell Yk(x) it deﬁnes, providing that the cell has non-zero
probability. Furthermore, given (6), the output of the score head γk"
RESILIENT MULTIPLE CHOICE LEARNING,0.23104693140794225,"✓(x) can be interpreted as an
approximation of the probability of a sample from p(y | x) to belong to this cell."
RESILIENT MULTIPLE CHOICE LEARNING,0.23465703971119134,Algorithm 1 Inference in the rMCL model
RESILIENT MULTIPLE CHOICE LEARNING,0.23826714801444043,"Input:
Unlabelled
input
x
2
X.
Trained
hypotheses
and
score
heads
(f 1"
RESILIENT MULTIPLE CHOICE LEARNING,0.24187725631768953,"✓, . . . , f K ✓, γ1"
RESILIENT MULTIPLE CHOICE LEARNING,0.24548736462093862,"✓, . . . , γK"
RESILIENT MULTIPLE CHOICE LEARNING,0.2490974729241877,"✓) 2 F(X, Y)K ⇥F(X, [0, 1])K.
Output:
Prediction of the output conditional distribution p(y | x).
1: Perform a forward pass by computing f 1"
RESILIENT MULTIPLE CHOICE LEARNING,0.2527075812274368,"✓(x), . . . , f K"
RESILIENT MULTIPLE CHOICE LEARNING,0.2563176895306859,"✓(x), γ1"
RESILIENT MULTIPLE CHOICE LEARNING,0.259927797833935,"✓(x), . . . , γK ✓(x)."
RESILIENT MULTIPLE CHOICE LEARNING,0.26353790613718414,2: Construct the associated Voronoi components Yk(x) (3) with Y = [K
RESILIENT MULTIPLE CHOICE LEARNING,0.26714801444043323,"k=1Yk(x).
3: Normalize the predicted scores γk"
RESILIENT MULTIPLE CHOICE LEARNING,0.27075812274368233,✓(x)  γk
RESILIENT MULTIPLE CHOICE LEARNING,0.2743682310469314,✓(x)/PK
RESILIENT MULTIPLE CHOICE LEARNING,0.2779783393501805,k=1 γk
RESILIENT MULTIPLE CHOICE LEARNING,0.2815884476534296,"✓(x).
4: If Yx ⇠p(y | x), then for k such that γk"
RESILIENT MULTIPLE CHOICE LEARNING,0.2851985559566787,"✓(x) > 0, interpret the predictions as estimations of
⇢ γk"
RESILIENT MULTIPLE CHOICE LEARNING,0.2888086642599278,"✓(x) = P(Yx 2 Yk(x))
f k"
RESILIENT MULTIPLE CHOICE LEARNING,0.2924187725631769,"✓(x) = E[Yx | Yx 2 Yk(x)]
(7)"
PROBABILISTIC INTERPRETATION AT INFERENCE TIME,0.296028880866426,"3.3
Probabilistic interpretation at inference time"
PROBABILISTIC INTERPRETATION AT INFERENCE TIME,0.2996389891696751,"Let us consider a trained rMCL model, such as the one described in the previous section. Following
the theoretical interpretation of [34] and the motivation of solving the overconﬁdence issue, as
explained in Section 3.1, the output objectives of the model can be summarized in (7). Although it is
possible, assuming that those properties are veriﬁed, to deduce the following law of total expectation"
PROBABILISTIC INTERPRETATION AT INFERENCE TIME,0.30324909747292417,"E[Yx] = K
X k=1 γk"
PROBABILISTIC INTERPRETATION AT INFERENCE TIME,0.30685920577617326,✓(x)f k
PROBABILISTIC INTERPRETATION AT INFERENCE TIME,0.3104693140794224,"✓(x),
(8)"
PROBABILISTIC INTERPRETATION AT INFERENCE TIME,0.3140794223826715,"the above quantity may not be informative enough, especially if the law of Yx ⇠p(y | x) is
multimodal. Indeed, to be able to derive a complete probabilistic interpretation, we still need to
characterize how rMCL approximates the full conditional density of Yx. For that purpose, it is
sufﬁcient to ﬁx a law ⇡k for Yx within each Voronoi cell k to accurately represent the predicted
distribution in each region of the output space as per the rMCL model, as shown in Proposition 1.
Proposition 1 (Probabilistic interpretation of rMCL). With the above notations, let us consider a
multi-hypothesis model with properties of (7). Let us furthermore assume that Y k"
PROBABILISTIC INTERPRETATION AT INFERENCE TIME,0.3176895306859206,"x ⇠⇡k(· | x)
(9)"
PROBABILISTIC INTERPRETATION AT INFERENCE TIME,0.3212996389891697,"i.e., the law Y k"
PROBABILISTIC INTERPRETATION AT INFERENCE TIME,0.3249097472924188,"x of Yx conditioned by the event {Yx 2 Yk(x)} is described by ⇡k for each input
x 2 X with constraint E[Y k"
PROBABILISTIC INTERPRETATION AT INFERENCE TIME,0.3285198555956679,x ] = f k
PROBABILISTIC INTERPRETATION AT INFERENCE TIME,0.33212996389891697,"✓(x). Then, for each measurable set A ✓Y,"
PROBABILISTIC INTERPRETATION AT INFERENCE TIME,0.33574007220216606,"P(Yx 2 A) = Z y2A K
X k=1 γk"
PROBABILISTIC INTERPRETATION AT INFERENCE TIME,0.33935018050541516,"✓(x)⇡k(y | x)dy.
(10)"
PROBABILISTIC INTERPRETATION AT INFERENCE TIME,0.34296028880866425,Proof. We have
PROBABILISTIC INTERPRETATION AT INFERENCE TIME,0.34657039711191334,"P(Yx 2 A)
=
P"
PROBABILISTIC INTERPRETATION AT INFERENCE TIME,0.35018050541516244,"k P(Yx 2 A \ Yk(x))
(by sigma-additivity)
=
P"
PROBABILISTIC INTERPRETATION AT INFERENCE TIME,0.35379061371841153,"k P(Yx 2 Yk(x))P(Yx 2 A | Yx 2 Yk(x))
(by Bayes’ theorem)
=
P k γk ✓(x) R"
PROBABILISTIC INTERPRETATION AT INFERENCE TIME,0.3574007220216607,"y2A ⇡k(y | x)dy
(by (9)).
⇤"
PROBABILISTIC INTERPRETATION AT INFERENCE TIME,0.36101083032490977,"Following the principle of maximum entropy [38], the uniform distribution inside each cell Y k"
PROBABILISTIC INTERPRETATION AT INFERENCE TIME,0.36462093862815886,"x ⇠
U(Yk(x)) is the least-informative distribution, assuming that the output space Y is with ﬁnite volume
and the hypotheses lie in the geometric center of their associated cell. With this prior, the predicted
distribution can be interpreted as"
PROBABILISTIC INTERPRETATION AT INFERENCE TIME,0.36823104693140796,"ˆp(y | x) = K
X k=1 γk ✓(x)1 !"
PROBABILISTIC INTERPRETATION AT INFERENCE TIME,0.37184115523465705,"y 2 Yk(x) """
PROBABILISTIC INTERPRETATION AT INFERENCE TIME,0.37545126353790614,"V(Yk(x))
,
(11)"
PROBABILISTIC INTERPRETATION AT INFERENCE TIME,0.37906137184115524,"where V(Yk(x)) , R"
PROBABILISTIC INTERPRETATION AT INFERENCE TIME,0.38267148014440433,"y2Yk(x) dy is the volume of Yk(x). Similarly, if we model the output distribu-
tion as a mixture of Dirac deltas such that in each cell k, γk"
PROBABILISTIC INTERPRETATION AT INFERENCE TIME,0.3862815884476534,✓(x) > 0 ) Y k
PROBABILISTIC INTERPRETATION AT INFERENCE TIME,0.3898916967509025,x ⇠δf k
PROBABILISTIC INTERPRETATION AT INFERENCE TIME,0.3935018050541516,"✓(x), then"
PROBABILISTIC INTERPRETATION AT INFERENCE TIME,0.3971119133574007,"ˆp(y | x) = K
X k=1 γk"
PROBABILISTIC INTERPRETATION AT INFERENCE TIME,0.4007220216606498,✓(x)δf k
PROBABILISTIC INTERPRETATION AT INFERENCE TIME,0.4043321299638989,"✓(x)(y).
(12)"
TOY EXAMPLE,0.40794223826714804,"3.4
Toy example"
TOY EXAMPLE,0.41155234657039713,"We build upon the toy example formalized in [34, §4.1] to validate the proposed algorithm and its
interpretation. In particular, we seek to illustrate how rMCL handles the overconﬁdence problem in a
regression task. The toy problem involves the prediction of a 2D distribution p(y | x) based on scalar
input x 2 X, where X = [0, 1]. The training dataset is constructed by selecting for each random
input x ⇠U ([0, 1]) one of the four sections S1 = [−1, 0) ⇥[−1, 0), S2 = [−1, 0) ⇥[0, 1], S3 =
[0, 1]⇥[−1, 0), S4 = [0, 1]⇥[0, 1], with probabilities p (S1) = p (S4) = 1−x"
TOY EXAMPLE,0.4151624548736462,"2 , p (S2) = p (S3) = x"
TOY EXAMPLE,0.4187725631768953,"2.
Whenever a region is selected, a point is then sampled uniformly in this region. To slightly increase
the complexity of the dataset, it has been chosen to enable the sampling of two (instead of one) target
training samples with probability q(x) for each input x. Here, q is a piece-wise afﬁne function with
q(0) = 1, q( 1"
TOY EXAMPLE,0.4223826714801444,"2) = 0, and q(1) = 1."
TOY EXAMPLE,0.4259927797833935,"We compared the behavior of our proposed approach rMCL against sMCL and independent ensembles
(IE) using a 3-layer perceptron backbone with 256 hidden units and 20 output hypotheses, and `2 as
the underlying distance. For rMCL, we used scoring weight β = 1. In addition to the full-ﬂedged
rMCL, we assess a variant denoted ‘rMCL⇤’ where a single negative hypothesis is uniformly selected
during training. For independent ensembles, we trained with independent random initialization
20 single-hypothesis models, the predictions of which being combined to be comparable to multi-
hypothesis models. The training processes were executed until the convergence of the training loss.
We report the checkpoints for which the validation loss was the lowest. All networks were optimized
using Adam [24]. We show in Figure 1 (top right) test predictions of the classical sMCL compared to
those of the rMCL model. In visualizing the predictions where the ground-truth distribution is known,
we observe empirically that the centroidal tessellation property, E[Y k"
TOY EXAMPLE,0.4296028880866426,x ] = f k
TOY EXAMPLE,0.4332129963898917,"✓(x), is veriﬁed with
good approximation (see red points Figure 1, bottom right). We otherwise notice the overconﬁdence
problem of the sMCL model in low-density zones when the output distribution is multimodal by
looking at the output prediction, e.g., for x = 0.1 and x = 0.9 in Figure 1 where the sMCL predictions
fail in low-density zones. In contrast, rMCL solves this issue, assigning to each Voronoi cell a score
that approximates the probability mass of the ground-truth distribution in this zone (see Figure 1,
bottom). Furthermore, the independent ensembles (triangles) suffer from a collapse issue around the
conditional mean of the ground-truth distribution. This behavior is expected as the minimizer of the
continuous formulation of the risk (4) on the whole output space is the conditional expectation of the
output given the input."
TOY EXAMPLE,0.4368231046931408,"Figure 1: Comparisons on a 2D regression toy problem. Comparing sMCL with standard WTA
(‘sMCL’, pink diamonds), proposed rMCL (blue circles), a variant of rMCL with a single negative
hypothesis chosen uniformly at training (‘rMCL⇤’, purple crosses), and Independent Ensemble (‘IE’,
yellow triangles). (Left) Test EMD for different inputs x; (Right) Inference at x = 0.1, 0.6, 0.9 and
Voronoi tessellations generated by the hypotheses. Green points are samples from the ground-truth
distribution at the corresponding input. Cells conditional mean (red points) matches with the predicted
hypotheses. The score predicted in each cell, displayed as color saturation of the blue circles, as per
scale on the right, approximates the corresponding proportion of points. At x = 0.1 and 0.9, we can
observe how rMCL tackles well the overconﬁdence issue in the low-density zones."
TOY EXAMPLE,0.4404332129963899,"From a quantitative perspective, as demonstrated in Fig. 1, we can evaluate the effectiveness of
rMCL in addressing the overconﬁdence issue compared to the classical sMCL. To ensure a fair
comparison with sMCL settings, we have opted for using the Earth Mover’s Distance (EMD) metric,
which measures the discrepancy between the ground truth and predicted distributions. Predicted
distributions are considered mixtures of Dirac deltas as in (12) for the metric computation, with sMCL
predictions assigning uniform mode weights. As depicted in Fig. 1, rMCL outperforms sMCL when
the target distribution is multimodal, i.e., for extreme values of x. However, in the unimodal scenario,
the performances of rMCL and sMCL are fairly similar. Furthermore, the ‘rMCL⇤’ variant (depicted
by crosses) that is lighter regarding memory consumption exhibits performance comparable to the
standard rMCL. Additional assessments, which are detailed in Appendix B.2, have demonstrated the
robustness of the rMCL when corrupting the training dataset with outliers in the output space."
EXPERIMENTS WITH AUDIO DATA,0.44404332129963897,"4
Experiments with audio data"
SOUND SOURCE LOCALIZATION AS A CONDITIONAL DISTRIBUTION ESTIMATION PROBLEM,0.44765342960288806,"4.1
Sound source localization as a conditional distribution estimation problem"
SOUND SOURCE LOCALIZATION AS A CONDITIONAL DISTRIBUTION ESTIMATION PROBLEM,0.45126353790613716,"Sound source localization (SSL) is the task of predicting the successive angular positions of audio
sources from a multichannel input audio clip x 2 X [15]. This problem can be cast either as a
regression problem (estimating the continuous positions of the sources, e.g., [37]) or a classiﬁcation
one (segmenting the output space into zones in which we wish to detect a source, e.g., [2]). While
not requiring the number of sources to be known in advance, the classiﬁcation approach suffers
from the lack of balanced data and limited spatial resolution. On the other hand, the regression
approach enables off-grid localization but typically suffers from the source permutation problem [40].
Assuming that the maximum number of overlapping sources M is known, a solution for handling
this multi-target regression problem is to predict, at each time frame t depending on the chosen
output resolution, a vector accounting for the sources’ activity at 2 {0, 1}M, as well as azimuth
and elevation angles φt 2 RM and #t 2 RM. A model can be trained with a permutation invariant
training (PIT) approach [1, 37, 45] using an optimization criterion of the form"
SOUND SOURCE LOCALIZATION AS A CONDITIONAL DISTRIBUTION ESTIMATION PROBLEM,0.4548736462093863,L(✓) = X t
SOUND SOURCE LOCALIZATION AS A CONDITIONAL DISTRIBUTION ESTIMATION PROBLEM,0.4584837545126354,"min
σ2SM `CE (σ(ˆat), at) + `g ⇣"
SOUND SOURCE LOCALIZATION AS A CONDITIONAL DISTRIBUTION ESTIMATION PROBLEM,0.4620938628158845,"(σ(ˆφt), σ(ˆ#t)), (φt, #t) ⌘"
SOUND SOURCE LOCALIZATION AS A CONDITIONAL DISTRIBUTION ESTIMATION PROBLEM,0.4657039711191336,",
(13)"
SOUND SOURCE LOCALIZATION AS A CONDITIONAL DISTRIBUTION ESTIMATION PROBLEM,0.4693140794223827,"where `CE and `g correspond respectively to a cross-entropy term and a geometrical loss, the latter
being computed only for active sources indexes. SM is the set of permutations of M elements and
the notation σ(z) stands for the M-dim vector z with its components permuted according to σ. In
the following, we will denote Yt the set of source positions at time t."
SOUND SOURCE LOCALIZATION AS A CONDITIONAL DISTRIBUTION ESTIMATION PROBLEM,0.4729241877256318,"With the distribution learning mindset, this task can be seen as an attempt to estimate, at each
time step, the sound source position distribution p(y | x), which can be viewed as a Dirac mixture,
p(y | x) / P"
SOUND SOURCE LOCALIZATION AS A CONDITIONAL DISTRIBUTION ESTIMATION PROBLEM,0.47653429602888087,"yi2Yt δyi(y), if we suppose that the targets are point-wise, with another Dirac mixture
representing the predicted active modes at predicted positions. Therefore, a natural way to evaluate
such SSL regression models is to solve the linear assignment problem, e.g., using Hungarian matching
with spherical distance as an underlying metric. To handle more general distributions, we propose to
generalize the metric used to the Earth Mover’s Distance (see Sec. 4.2)."
SOUND SOURCE LOCALIZATION AS A CONDITIONAL DISTRIBUTION ESTIMATION PROBLEM,0.48014440433212996,"The rMCL framework is well suited to SSL as it allows one to beneﬁt from both the advantages
of the regression and classiﬁcation viewpoints in the same spirit as [41]. There is no need for
prior knowledge of the number of sources, and it avoids challenges related to imbalanced spatial
positions and the source permutation problem of (13). This method comes at a low computational
cost regarding added parameters when opting for a low-level representation shared by the hypotheses
and scores heads (see Sec. 3.2). Furthermore, it allows for producing a heat map of the sound sources’
positions with a probabilistic prediction, which could otherwise account for their spatial dispersion
depending on the chosen law ⇡k selected in the Voronoi cells. Modeling the sound sources as point
sources, a delta law will be selected following the formulation of Proposition 1."
EXPERIMENTAL SETUP,0.48375451263537905,"4.2
Experimental setup"
EXPERIMENTAL SETUP,0.48736462093862815,"Datasets. We conducted our experiments on increasingly complex SSL datasets originally introduced
by Adavanne et al. [1]: i) ANSYN, derived from DCASE 2016 Sound Event Detection challenge with
spatially localized events in anechoic conditions and ii) RESYN, a variant of ANSYN in reverberant
conditions. Each dataset is divided into three sub-datasets depending on the maximum number of
overlapping events (1, 2, or 3, denoted as D1, D2, and D3). These sub-datasets are further divided
into three splits (including one for testing). Each training split contains 300 recordings (30 s), of
which 60 are reserved for validation. Preprocessing is detailed in Appendix A.2. For each experiment,
we used all training splits from D1, D2, and D3. Evaluation was then conducted for each test set
based on the overlapping levels. Moreover, we present results from additional SSL datasets, including
REAL [1] and DCASE19 [3], in Appendix B.3."
EXPERIMENTAL SETUP,0.49097472924187724,"Metrics. To assess the performance of the different methods, we employed the following metrics:"
EXPERIMENTAL SETUP,0.49458483754512633,"• The ‘Oracle’ error (#): O(xn, Yn) =
1
|Yn| P"
EXPERIMENTAL SETUP,0.4981949458483754,"ym2Yn mink2[[1,K]] d ! f k"
EXPERIMENTAL SETUP,0.5018050541516246,"✓(xn) , ym "" ."
EXPERIMENTAL SETUP,0.5054151624548736,"• The Earth Mover’s Distance (EMD #): also known as the 1st Wasserstein distance,"
EXPERIMENTAL SETUP,0.5090252707581228,"it is a distance measure between two probability distributions. In this context, these"
EXPERIMENTAL SETUP,0.5126353790613718,"are the predicted distribution, ˆp(y | xn) = PK"
EXPERIMENTAL SETUP,0.516245487364621,k=1 γk
EXPERIMENTAL SETUP,0.51985559566787,✓(xn)δf k
EXPERIMENTAL SETUP,0.5234657039711191,"✓(xn)(y), and the ground-
truth one, p(y | xn) =
1
|Yn| P"
EXPERIMENTAL SETUP,0.5270758122743683,ym2Yn δym(y). The EMD solves the optimization prob-
EXPERIMENTAL SETUP,0.5306859205776173,"lem W1 (p(·|xn), ˆp(·|xn)) = min 2 PK k=1 P"
EXPERIMENTAL SETUP,0.5342960288808665,"ym2Yn  k,md(f k"
EXPERIMENTAL SETUP,0.5379061371841155,"✓(xn), ym), where  =
! (f k"
EXPERIMENTAL SETUP,0.5415162454873647,"✓(xn), ym) """
EXPERIMENTAL SETUP,0.5451263537906137,"k,m is a transport plan and  is the set of all valid transport plans [21]."
EXPERIMENTAL SETUP,0.5487364620938628,"While the oracle error assesses the quality of the best hypothesis predicted for each target, the
EMD metric looks at all hypotheses. It provides insight into the overall consistency of the pre-
dicted distribution. The EMD also penalizes the overconﬁdence issue of the WTA with sMCL
as described in Sec. 3.4.
In order to ﬁt the directional nature of the sound source localiza-
tion task, these metrics are computed in a spherical space equipped with distance d(ˆy, y) =
arccos[sin(ˆ#) sin(#) + cos(ˆ#) cos(#) cos(φ −ˆφ)], where (ˆφ, ˆ#) and (φ, #) are source positions"
EXPERIMENTAL SETUP,0.5523465703971119,"ˆy and y, respectively, expressed as azimuth and elevation angles."
EXPERIMENTAL SETUP,0.555956678700361,"Neural network backbone. We employed the CRNN architecture of SeldNet [1] as a backbone,
which we modiﬁed to enable multi-hypothesis predictions. This adaptation involves adjusting the
output format and duplicating the last fully-connected layers for hypothesis and scoring heads.
As only the prediction heads are adjusted, the number of parameters added to the architecture is
negligible. Refer to Appendix A.2 for additional architecture and training details."
EXPERIMENTAL SETUP,0.5595667870036101,"Baselines. We compared the proposed rMCL with several baselines, each utilizing the same feature-
extractor backbone as previously described. The baselines include a Permutation Invariant Training
variant (‘PIT variant’) proposed in [37] for SSL, the conventional WTA setup (‘WTA, 5 hyp.’) and
its single hypothesis variant (‘WTA, 1 hyp.’), its ""-relaxed version with "" = 0.5 (‘""-WTA, 5 hyp.’)
and its top-n variant with n = 3 (‘top-n-WTA, 5 hyp.’), as well as independent ensembles (‘IE’). To
ensure a fair comparison, we used the same number of 5 hypotheses for the WTAs, sufﬁcient to cover
the maximum of 3 sound sources in the dataset (refer to Sec. 4.5 and the sensitivity study in Table 4).
For the single hypothesis WTA, the head is updated by only considering the best target, as this fares
better than using one update per target. IE was constructed from ﬁve such single hypothesis WTA
models trained independently with random initialization."
COMPARATIVE EVALUATION OF RMCL IN SSL,0.5631768953068592,"4.3
Comparative evaluation of rMCL in SSL"
COMPARATIVE EVALUATION OF RMCL IN SSL,0.5667870036101083,"Table 1: Source localization in anechoic conditions. Average scores (± standard deviation) on
ANSYN dataset for PIT, IE, various WTA-based methods, and proposed rMCL. The ‘EMD’ evaluates
all the hypotheses jointly, while the ‘Oracle’ error only looks at the best hypotheses. D1 corresponds
to the single-target setting, while D2 and D3 consider up to 2 and 3 targets, respectively."
COMPARATIVE EVALUATION OF RMCL IN SSL,0.5703971119133574,"Dataset: ANSYN
EMD D1
EMD D2
EMD D3
Oracle D1
Oracle D2
Oracle D3"
COMPARATIVE EVALUATION OF RMCL IN SSL,0.5740072202166066,"PIT variant
6.22 ± 0.80
14.65 ± 1.22
23.41 ± 1.39
3.58 ± 0.46
10.58 ± 0.91
18.10 ± 1.04
IE (5 members)
4.05 ± 0.48
21.64 ± 2.29
34.34 ± 2.37
1.24 ± 0.24
16.91 ± 2.05
28.82 ± 2.25
WTA, 1 hyp.
3.97 ± 0.55
24.69 ± 2.72
39.66 ± 2.67
3.97 ± 0.55
24.69 ± 2.72
39.66 ± 2.67
WTA, 5 hyp.
48.22 ± 1.78
44.41 ± 1.25
41.83 ± 0.96
3.56 ± 0.39
6.53 ± 0.44
10.44 ± 0.58
top-n-WTA, 5 hyp.
5.14 ± 0.80
18.09 ± 1.32
25.12 ± 1.30
3.33 ± 0.46
7.48 ± 0.79
13.54 ± 0.96
""-WTA, 5 hyp.
5.51 ± 0.66
19.20 ± 1.78
28.39 ± 1.68
3.62 ± 0.57
10.86 ± 1.26
17.44 ± 1.22
rMCL, 5 hyp.
7.04 ± 0.58
13.87 ± 0.99
20.76 ± 1.04
3.85 ± 0.46
7.16 ± 0.67
11.29 ± 0.78"
COMPARATIVE EVALUATION OF RMCL IN SSL,0.5776173285198556,"Table 2: Source localization in reverberant conditions. Results on RESYN dataset, with same
table layout as in Table 1."
COMPARATIVE EVALUATION OF RMCL IN SSL,0.5812274368231047,"Dataset: RESYN
EMD D1
EMD D2
EMD D3
Oracle D1
Oracle D2
Oracle D3"
COMPARATIVE EVALUATION OF RMCL IN SSL,0.5848375451263538,"PIT variant
10.53 ± 0.90
25.09 ± 2.14
35.05 ± 1.98
4.89 ± 0.55
15.92 ± 1.36
24.95 ± 1.62
IE (5 members)
8.24 ± 1.02
26.65 ± 2.49
38.7 ± 2.62
3.77 ± 0.61
20.95 ± 2.27
32.85 ± 2.48
WTA, 1 hyp.
8.32 ± 1.28
29.26 ± 2.85
43.25 ± 2.94
8.32 ± 1.28
29.26 ± 2.85
43.25 ± 2.94
WTA, 5 hyp.
57.88 ± 1.71
51.74 ± 1.36
47.38 ± 1.26
5.81 ± 0.58
9.46 ± 0.71
13.33 ± 0.69
top-n-WTA, 5 hyp.
42.74 ± 2.86
37.25 ± 1.88
36.48 ± 1.40
6.21 ± 0.79
11.02 ± 1.00
17.32 ± 1.11
""-WTA, 5 hyp.
8.84 ± 1.09
27.3 ± 2.52
38.43 ± 2.42
6.48 ± 0.95
20.54 ± 2.33
30.18 ± 2.15
rMCL, 5 hyp.
12.14 ± 1.12
24.45 ± 1.91
32.28 ± 1.85
5.74 ± 0.66
10.5 ± 0.87
14.6 ± 0.87"
COMPARATIVE EVALUATION OF RMCL IN SSL,0.5884476534296029,"We present in Tables 1 and 2 the results of rMCL and baselines in the anechoic (ANSYN) and rever-
berant (RESYN) conditions, respectively. The consistency of the results should be noted. For each
model, the metrics tend to improve as the number of sources decreases and, under similar conditions,
results are marginally lower for more challenging datasets. Unsurprisingly, the single hypothesis
approach exhibits strong performances in the unimodal cases (D1), but performs signiﬁcantly worse
in the multimodal cases (D2 and D3). Ensembling (IE) improves its performance but still displays the
lack of diversity already exposed in Section 3.4. As for the 5-hypothesis WTA, in its original form, it
is ill-suited for multi-target regression due to the overconﬁdence issue: despite a strong oracle metric,
the EMD results are very poor. On the other hand, rMCL surpasses its competitors and shows the
best EMD in every multimodal setting. It also consistently obtains the second-best oracle error, only
slightly above WTA, while not suffering from overconﬁdence."
COMPARATIVE EVALUATION OF RMCL IN SSL,0.592057761732852,"We also present results for REAL and DCASE19 in Appendix B.3, that display similar trends. We
did not observe the collapse issue (see Section 3.1) in our settings, neither with WTA nor with the
proposed rMCL model. We believe it is solved in practice by the variability of the data samples in the
stochastic optimization during training [19]; refer to Appendix B.1 for further discussions."
COMBINING RMCL WITH OTHER WTA VARIANTS,0.5956678700361011,"4.4
Combining rMCL with other WTA variants"
COMBINING RMCL WITH OTHER WTA VARIANTS,0.5992779783393501,"Table 3: Combining proposed rMCL with other WTA variants. Average scores (± standard
deviation) for source localization in anechoic (top) and reverberant (bottom) conditions."
COMBINING RMCL WITH OTHER WTA VARIANTS,0.6028880866425993,"Dataset: ANSYN
EMD D1
EMD D2
EMD D3
Oracle D1
Oracle D2
Oracle D3"
COMBINING RMCL WITH OTHER WTA VARIANTS,0.6064981949458483,"rMCL, 5 hyp.
7.04 ± 0.58
13.87 ± 0.99
20.76 ± 1.04
3.85 ± 0.46
7.16 ± 0.67
11.29 ± 0.78
top-n-rMCL, 5 hyp.
5.46 ± 0.62
13.88 ± 1.06
21.45 ± 1.10
4.2 ± 0.55
8.04 ± 0.74
13.72 ± 0.89
""-rMCL, 5 hyp.
5.89 ± 0.69
12.13 ± 0.98
19.95 ± 1.16
3.6 ± 0.54
8.76 ± 0.89
14.47 ± 1.02"
COMBINING RMCL WITH OTHER WTA VARIANTS,0.6101083032490975,"Dataset: RESYN
EMD D1
EMD D2
EMD D3
Oracle D1
Oracle D2
Oracle D3"
COMBINING RMCL WITH OTHER WTA VARIANTS,0.6137184115523465,"rMCL, 5 hyp.
12.14 ± 1.12
24.45 ± 1.91
32.28 ± 1.85
5.74 ± 0.66
10.5 ± 0.87
14.6 ± 0.87
top-n-rMCL, 5 hyp.
9.3 ± 1.15
23.81 ± 2.22
33.33 ± 2.06
6.99 ± 0.85
13.43 ± 1.40
19.72 ± 1.29
""-rMCL, 5 hyp.
8.64 ± 1.03
22.82 ± 2.12
32.47 ± 1.97
6.08 ± 0.91
18.39 ± 2.07
26.92 ± 1.94"
COMBINING RMCL WITH OTHER WTA VARIANTS,0.6173285198555957,"As noted in Section 3.2, rMCL’s improvements are, in theory, orthogonal to that of other WTA variants.
In this section, we combine the rMCL with top-n-WTA and ""-WTA approaches into respectively
top-n-rMCL and ""-rMCL. The results are shown in Tables 3. We note that while top-n rMCL does
not provide signiﬁcant improvement to rMCL, ""-rMCL, on the other hand improves EMD scores
at the expense of increased oracle error. We observe similar effects when this method is applied to
WTA, indicating that there are indeed some additive effects of ""-WTA and rMCL. Also note that with
regards to Tables 1 and 2, all the proposed variants still get better EMD than all competitors under
multimodal conditions, showing the robustness of our method."
EFFECT OF THE NUMBER OF HYPOTHESES,0.6209386281588448,"4.5
Effect of the number of hypotheses"
EFFECT OF THE NUMBER OF HYPOTHESES,0.6245487364620939,"The impact of varying the number of hypotheses on the performance of the rMCL model is presented
in Table 4 and Figure B.4. First and foremost, we notice the anticipated trend of the oracle metric
improving as the number of hypotheses increases. Concerning the EMD metric, the single hypoth-
esis model, which avoids errors from negative hypotheses, is most effective in handling unimodal"
EFFECT OF THE NUMBER OF HYPOTHESES,0.628158844765343,Table 4: Sensitivity analysis. Effect of the number of hypotheses on the performance of rMCL.
EFFECT OF THE NUMBER OF HYPOTHESES,0.631768953068592,"Dataset: ANSYN
EMD D1
EMD D2
EMD D3
Oracle D1
Oracle D2
Oracle D3"
EFFECT OF THE NUMBER OF HYPOTHESES,0.6353790613718412,"PIT variant
6.22 ± 0.80
14.65 ± 1.22
23.41 ± 1.39
3.58 ± 0.46
10.58 ± 0.91
18.10 ± 1.04
WTA, 1 hyp.
3.97 ± 0.55
24.69 ± 2.72
39.66 ± 2.67
3.97 ± 0.55
24.69 ± 2.72
39.66 ± 2.67
rMCL, 3 hyp.
9.89 ± 0.95
14.37 ± 0.91
20.96 ± 1.03
5.65 ± 0.73
8.6 ± 0.63
13.42 ± 0.86
rMCL, 5 hyp.
7.04 ± 0.58
13.87 ± 0.99
20.76 ± 1.04
3.85 ± 0.46
7.16 ± 0.67
11.29 ± 0.78
rMCL, 10 hyp.
9.14 ± 0.76
15.2 ± 0.84
21.28 ± 0.96
2.94 ± 0.35
4.76 ± 0.39
7.54 ± 0.50
rMCL, 20 hyp.
9.13 ± 0.71
16.04 ± 0.84
22.55 ± 0.87
2.06 ± 0.22
3.61 ± 0.30
5.83 ± 0.37"
EFFECT OF THE NUMBER OF HYPOTHESES,0.6389891696750902,"distributions. In the case of the EMD metric applied to multimodal distributions, we observe that
multiple hypothesis models improve the results, but excessively increasing the number of hypotheses
may marginally degrade performances. A further study could examine this phenomenon in detail,
which may be related to the expressive power of the hypothesis heads or the accumulation of errors
in score heads predictions."
DISCUSSION AND LIMITATIONS,0.6425992779783394,"5
Discussion and limitations"
DISCUSSION AND LIMITATIONS,0.6462093862815884,"In the audio experiments, the performance of rMCL was found to be affected by the number of
hypotheses to tune depending on the complexity of the dataset. Moreover, as with the other variants
of WTA, ﬁxing the overconﬁdence issue with rMCL slightly degrades the performance of the best
hypothesis (oracle error) for a reason that is yet to be determined. Otherwise, while ""-WTA behaves
as expected with rMCL, trading off the quality of the best hypotheses for overall performance,
top-n-WTA does not exhibit the same behavior. This discrepancy warrants further investigation."
DISCUSSION AND LIMITATIONS,0.6498194945848376,"The probabilistic interpretation of rMCL, as presented in Algorithm 1 and in Sec. 3.3, states that
the different hypotheses would be organized in an optimal partition of the output space, forming
a Voronoi tessellation. Ideally, each hypothesis would capture a region of the distribution, and the
scores representing how likely this zone would activate in a given context. This interpretation remains
theoretically valid whenever train and test examples are sampled from the same joint distribution
p(x, y). However, in realistic settings, the hypotheses might not adhere to meaningful regions, but
this could be controlled and evaluated, provided that an external validation dataset is available. In
future work, we intend to use calibration techniques [16, 39] to identify and alleviate these issues."
DISCUSSION AND LIMITATIONS,0.6534296028880866,"To sum up, this paper proposes a new Multiple Choice Learning variant, suitable for tackling the MCL
overconﬁdence problem in regression settings. Our method is based on a learned scoring scheme
that handles situations where a set of targets is available for each input. Furthermore, we propose
a probabilistic interpretation of the model, and we illustrate its relevance with an evaluation on
synthetic data. Its practical usefulness is also demonstrated in the context of point-wise sound source
localization. Further work could include a speciﬁc study about the specialization of the predictors,
and the validation of the proposed algorithm in increasingly complex real-world datasets."
DISCUSSION AND LIMITATIONS,0.6570397111913358,Acknowledgments
DISCUSSION AND LIMITATIONS,0.6606498194945848,"This work was funded by the French Association for Technological Research (ANRT CIFRE contract
2022-1854). We would like to thank the reviewers for their valuable feedback."
REFERENCES,0.6642599277978339,References
REFERENCES,0.6678700361010831,"[1] Sharath Adavanne, Archontis Politis, Joonas Nikunen, and Tuomas Virtanen. Sound event"
REFERENCES,0.6714801444043321,"localization and detection of overlapping sources using convolutional recurrent neural networks.
IEEE Journal of Selected Topics in Signal Processing, 13(1):34–48, 2018. 7, 8, 14, 15, 16, 17"
REFERENCES,0.6750902527075813,"[2] Sharath Adavanne, Archontis Politis, and Tuomas Virtanen. Direction of arrival estimation for"
REFERENCES,0.6787003610108303,"multiple sound sources using convolutional recurrent neural network. In 2018 26th European
Signal Processing Conference (EUSIPCO), pages 1462–1466. IEEE, 2018. 7"
REFERENCES,0.6823104693140795,"[3] Sharath Adavanne, Archontis Politis, and Tuomas Virtanen. A multi-room reverberant dataset"
REFERENCES,0.6859205776173285,"for sound event localization and detection. arXiv preprint arXiv:1905.08546, 2019. 7, 17"
REFERENCES,0.6895306859205776,"[4] Kazi Md Rokibul Alam, Nazmul Siddique, and Hojjat Adeli. A dynamic ensemble learning"
REFERENCES,0.6931407942238267,"algorithm for neural networks. Neural Computing and Applications, 32:8675–8690, 2020. 2"
REFERENCES,0.6967509025270758,[5] Samuel S Blackman. Multiple hypothesis tracking for multiple target tracking. IEEE Aerospace
REFERENCES,0.7003610108303249,"and Electronic Systems Magazine, 19(1):5–18, 2004. 1"
REFERENCES,0.703971119133574,"[6] Mike Brodie, Chris Tensmeyer, Wes Ackerman, and Tony Martinez. Alpha model domination"
REFERENCES,0.7075812274368231,"in multiple choice learning. In 2018 17th IEEE International Conference on Machine Learning
and Applications (ICMLA), pages 879–884. IEEE, 2018. 2"
REFERENCES,0.7111913357400722,[7] Armen Der Kiureghian and Ove Ditlevsen. Aleatory or epistemic? does it matter? Structural
REFERENCES,0.7148014440433214,"safety, 31(2):105–112, 2009. 2"
REFERENCES,0.7184115523465704,[8] Thomas G Dietterich. Ensemble methods in machine learning. In International workshop on
REFERENCES,0.7220216606498195,"multiple classiﬁer systems, pages 1–15. Springer, 2000. 2"
REFERENCES,0.7256317689530686,"[9] Qiang Du, Vance Faber, and Max Gunzburger. Centroidal voronoi tessellations: Applications"
REFERENCES,0.7292418772563177,"and algorithms. SIAM review, 41(4):637–676, 1999. 3"
REFERENCES,0.7328519855595668,"[10] Michael Firman, Neill DF Campbell, Lourdes Agapito, and Gabriel J Brostow. Diversenet:"
REFERENCES,0.7364620938628159,"When one right answer is not enough. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 5598–5607, 2018. 2, 3"
REFERENCES,0.740072202166065,"[11] Stanislav Fort, Huiyi Hu, and Balaji Lakshminarayanan. Deep ensembles: A loss landscape"
REFERENCES,0.7436823104693141,"perspective. arXiv preprint arXiv:1912.02757, 2019. 2"
REFERENCES,0.7472924187725631,[12] Yarin Gal et al. Uncertainty in deep learning. 2016. 2
REFERENCES,0.7509025270758123,"[13] Nuno Cruz Garcia, Sarah Adel Bargal, Vitaly Ablavsky, Pietro Morerio, Vittorio Murino, and"
REFERENCES,0.7545126353790613,"Stan Sclaroff. Distillation multiple choice learning for multimodal action recognition. In
Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages
2755–2764, 2021. 2"
REFERENCES,0.7581227436823105,[14] Alex Graves. Practical variational inference for neural networks. Advances in neural information
REFERENCES,0.7617328519855595,"processing systems, 24, 2011. 2"
REFERENCES,0.7653429602888087,"[15] Pierre-Amaury Grumiaux, Sr ¯dan Kiti´c, Laurent Girin, and Alexandre Guérin. A survey of"
REFERENCES,0.7689530685920578,"sound source localization with deep learning methods. The Journal of the Acoustical Society of
America, 152(1):107–151, 2022. 7"
REFERENCES,0.7725631768953068,"[16] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural"
REFERENCES,0.776173285198556,"networks. In International conference on machine learning, pages 1321–1330. PMLR, 2017.
10"
REFERENCES,0.779783393501805,"[17] Abner Guzman-Rivera, Dhruv Batra, and Pushmeet Kohli. Multiple choice learning: Learning"
REFERENCES,0.7833935018050542,"to produce multiple structured outputs. Advances in neural information processing systems, 25,
2012. 1, 2, 3"
REFERENCES,0.7870036101083032,[18] Lars Kai Hansen and Peter Salamon. Neural network ensembles. IEEE transactions on pattern
REFERENCES,0.7906137184115524,"analysis and machine intelligence, 12(10):993–1001, 1990. 2"
REFERENCES,0.7942238267148014,"[19] Eddy Ilg, Ozgun Cicek, Silvio Galesso, Aaron Klein, Osama Makansi, Frank Hutter, and"
REFERENCES,0.7978339350180506,"Thomas Brox. Uncertainty estimates and multi-hypotheses networks for optical ﬂow. In
Proceedings of the European Conference on Computer Vision (ECCV), pages 652–667, 2018. 2,
9, 16"
REFERENCES,0.8014440433212996,[20] Ehsan Imani and Martha White. Improving regression performance with distributional losses.
REFERENCES,0.8050541516245487,"In International conference on machine learning, pages 2157–2166. PMLR, 2018. 1"
REFERENCES,0.8086642599277978,"[21] Leonid V Kantorovich. On the translocation of masses. In Dokl. Akad. Nauk. USSR (NS),"
REFERENCES,0.8122743682310469,"volume 37, pages 199–201, 1942. 8"
REFERENCES,0.8158844765342961,[22] Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for
REFERENCES,0.8194945848375451,"computer vision? Advances in neural information processing systems, 30, 2017. 2"
REFERENCES,0.8231046931407943,"[23] Youngwook Kim, Jae Myung Kim, Zeynep Akata, and Jungwoo Lee. Large loss matters in"
REFERENCES,0.8267148014440433,"weakly supervised multi-label classiﬁcation. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 14156–14165, 2022. 2"
REFERENCES,0.8303249097472925,[24] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
REFERENCES,0.8339350180505415,"arXiv:1412.6980, 2014. 6, 14"
REFERENCES,0.8375451263537906,[25] Harold W Kuhn. The hungarian method for the assignment problem. Naval research logistics
REFERENCES,0.8411552346570397,"quarterly, 2(1-2):83–97, 1955. 15"
REFERENCES,0.8447653429602888,"[26] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable"
REFERENCES,0.8483754512635379,"predictive uncertainty estimation using deep ensembles. Advances in neural information
processing systems, 30, 2017. 2"
REFERENCES,0.851985559566787,"[27] Kimin Lee, Changho Hwang, KyoungSoo Park, and Jinwoo Shin. Conﬁdent multiple choice"
REFERENCES,0.855595667870036,"learning. In International Conference on Machine Learning, pages 2014–2023. PMLR, 2017. 2"
REFERENCES,0.8592057761732852,"[28] Stefan Lee, Senthil Purushwalkam, Michael Cogswell, David Crandall, and Dhruv Batra. Why"
REFERENCES,0.8628158844765343,"m heads are better than one: Training a diverse ensemble of deep networks. arXiv preprint
arXiv:1511.06314, 2015. 2"
REFERENCES,0.8664259927797834,"[29] Stefan Lee, Senthil Purushwalkam Shiva Prakash, Michael Cogswell, Viresh Ranjan, David"
REFERENCES,0.8700361010830325,"Crandall, and Dhruv Batra. Stochastic multiple choice learning for training diverse deep
ensembles. Advances in Neural Information Processing Systems, 29, 2016. 1, 2, 3"
REFERENCES,0.8736462093862816,[30] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International
REFERENCES,0.8772563176895307,"Conference on Learning Representations, 2018. 15"
REFERENCES,0.8808664259927798,[31] David JC MacKay. A practical bayesian framework for backpropagation networks. Neural
REFERENCES,0.8844765342960289,"computation, 4(3):448–472, 1992. 2"
REFERENCES,0.8880866425992779,"[32] Osama Makansi, Eddy Ilg, Ozgun Cicek, and Thomas Brox. Overcoming limitations of mixture"
REFERENCES,0.8916967509025271,"density networks: A sampling and ﬁtting framework for multimodal future prediction. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
7144–7153, 2019. 1, 2, 16"
REFERENCES,0.8953068592057761,[33] Saeed Masoudnia and Reza Ebrahimpour. Mixture of experts: a literature survey. The Artiﬁcial
REFERENCES,0.8989169675090253,"Intelligence Review, 42(2):275, 2014. 3"
REFERENCES,0.9025270758122743,"[34] Christian Rupprecht, Iro Laina, Robert DiPietro, Maximilian Baust, Federico Tombari, Nassir"
REFERENCES,0.9061371841155235,"Navab, and Gregory D Hager. Learning in an uncertain world: Representing ambiguity through
multiple hypotheses. In Proceedings of the IEEE international conference on computer vision,
pages 3591–3600, 2017. 1, 2, 3, 5, 14"
REFERENCES,0.9097472924187726,"[35] Justin Salamon, Christopher Jacoby, and Juan Pablo Bello. A dataset and taxonomy for urban"
REFERENCES,0.9133574007220217,"sound research. In Proceedings of the 22nd ACM international conference on Multimedia, pages
1041–1044, 2014. 17"
REFERENCES,0.9169675090252708,"[36] Christopher Schymura, Benedikt Bönninghoff, Tsubasa Ochiai, Marc Delcroix, Keisuke Ki-"
REFERENCES,0.9205776173285198,"noshita, Tomohiro Nakatani, Shoko Araki, and Dorothea Kolossa. PILOT: Introducing trans-
formers for probabilistic sound event localization. In Interspeech 2021, pages 2117–2121.
ISCA, 2021. 14, 15, 16"
REFERENCES,0.924187725631769,"[37] Christopher Schymura, Tsubasa Ochiai, Marc Delcroix, Keisuke Kinoshita, Tomohiro Nakatani,"
REFERENCES,0.927797833935018,"Shoko Araki, and Dorothea Kolossa. Exploiting attention-based sequence-to-sequence archi-
tectures for sound event localization. In 2020 28th European Signal Processing Conference
(EUSIPCO), pages 231–235. IEEE, 2021. 7, 8, 14, 15, 16"
REFERENCES,0.9314079422382672,[38] John Shore and Rodney Johnson. Axiomatic derivation of the principle of maximum entropy and
REFERENCES,0.9350180505415162,"the principle of minimum cross-entropy. IEEE Transactions on information theory, 26(1):26–37,
1980. 5"
REFERENCES,0.9386281588447654,"[39] Hao Song, Tom Diethe, Meelis Kull, and Peter Flach. Distribution calibration for regression. In"
REFERENCES,0.9422382671480144,"International Conference on Machine Learning, pages 5897–5906. PMLR, 2019. 10"
REFERENCES,0.9458483754512635,"[40] Aswin Shanmugam Subramanian, Chao Weng, Shinji Watanabe, Meng Yu, and Dong Yu. Deep"
REFERENCES,0.9494584837545126,"learning based multi-source localization with source splitting and its effectiveness in multi-talker
speech recognition. Computer Speech & Language, 75:101360, 2022. 7"
REFERENCES,0.9530685920577617,"[41] Harshavardhan Sundar, Weiran Wang, Ming Sun, and Chao Wang. Raw waveform based"
REFERENCES,0.9566787003610109,"end-to-end deep convolutional network for spatial localization of multiple acoustic sources. In
ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), pages 4642–4646. IEEE, 2020. 7"
REFERENCES,0.9602888086642599,"[42] Kai Tian, Yi Xu, Shuigeng Zhou, and Jihong Guan. Versatile multiple choice learning and its"
REFERENCES,0.9638989169675091,"application to vision computing. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 6349–6357, 2019. 2, 4"
REFERENCES,0.9675090252707581,"[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,"
REFERENCES,0.9711191335740073,"Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
processing systems, 30, 2017. 15"
REFERENCES,0.9747292418772563,"[44] Zehan Wang, Sihong Zhou, Yuyao Huang, and Wei Tian.
Dsmcl: Dual-level stochastic
multiple choice learning for multi-modal trajectory prediction. In 2020 IEEE 23rd International
Conference on Intelligent Transportation Systems (ITSC), pages 1–6. IEEE, 2020. 1"
REFERENCES,0.9783393501805054,"[45] Dong Yu, Morten Kolbæk, Zheng-Hua Tan, and Jesper Jensen. Permutation invariant training of"
REFERENCES,0.9819494584837545,"deep models for speaker-independent multi-talker speech separation. In 2017 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 241–245. IEEE, 2017.
7"
REFERENCES,0.9855595667870036,"[46] Zhi-Hua Zhou, Jianxin Wu, and Wei Tang. Ensembling neural networks: many could be better"
REFERENCES,0.9891696750902527,"than all. Artiﬁcial intelligence, 137(1-2):239–263, 2002. 2"
REFERENCES,0.9927797833935018,"[47] Xiaoyan Zhu, Jiaxuan Li, Jingtao Ren, Jiayin Wang, and Guangtao Wang. Dynamic ensemble"
REFERENCES,0.9963898916967509,"learning for multi-label classiﬁcation. Information Sciences, 623:94–111, 2023. 2"
