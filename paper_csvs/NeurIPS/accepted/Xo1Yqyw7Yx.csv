Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0022271714922048997,"The wider application of end-to-end learning methods to embodied decision-
making domains remains bottlenecked by their reliance on a superabundance
of training data representative of the target domain. Meta-reinforcement learn-
ing (meta-RL) approaches abandon the aim of zero-shot generalization—the goal
of standard reinforcement learning (RL)—in favor of few-shot adaptation, and
thus hold promise for bridging larger generalization gaps. While learning this
meta-level adaptive behavior still requires substantial data, efﬁcient environment
simulators approaching real-world complexity are growing in prevalence. Even
so, hand-designing sufﬁciently diverse and numerous simulated training tasks for
these complex domains is prohibitively labor-intensive. Domain randomization
(DR) and procedural generation (PG), offered as solutions to this problem, re-
quire simulators to possess carefully-deﬁned parameters which directly translate
to meaningful task diversity—a similarly prohibitive assumption. In this work,
we present DIVA, an evolutionary approach for generating diverse training tasks
in such complex, open-ended simulators. Like unsupervised environment design
(UED) methods, DIVA can be applied to arbitrary parameterizations, but can ad-
ditionally incorporate realistically-available domain knowledge—thus inheriting
the ﬂexibility and generality of UED, and the supervised structure embedded in
well-designed simulators exploited by DR and PG. Our empirical results showcase
DIVA’s unique ability to overcome complex parameterizations and successfully
train adaptive agent behavior, far outperforming competitive baselines from prior
literature. These ﬁndings highlight the potential of such semi-supervised environ-
ment design (SSED) approaches, of which DIVA is the ﬁrst humble constituent, to
enable training in realistic simulated domains, and produce more robust and capable
adaptive agents. Our code is available at https://github.com/robbycostales/diva."
INTRODUCTION,0.004454342984409799,"1
Introduction"
INTRODUCTION,0.0066815144766146995,"Despite the broadening application of reinforcement learning (RL) methods to real-world problems
[1, 2], generalization to new scenarios—ones not explicitly supported by the training set—remains a
fundamental challenge [3]. Meta-reinforcement learning (meta-RL), an extension of the RL frame-
work, is formulated speciﬁcally for training adaptive agents, and is thus well-suited for overcoming
these generalization gaps [4]. One recent work has demonstrated that meta-RL agents can be trained
at scale to achieve adaptation capabilities on par with human subjects [5]. However, learning this
human-like adaptive behavior naturally requires a large amount of data representative of the down-
stream (or target) distribution. For task distributions approaching real-world complexity—precisely
the ones of interest—designing each scenario by hand is prohibitively expensive."
INTRODUCTION,0.008908685968819599,∗Correspondence to rscostal@usc.edu.
INTRODUCTION,0.011135857461024499,"target diversity f2
f1"
INTRODUCTION,0.013363028953229399,"structured 
vs.
unstructured"
INTRODUCTION,0.015590200445434299,environment
INTRODUCTION,0.017817371937639197,"generators 
f3"
INTRODUCTION,0.0200445434298441,"f2
axis of 
diversity"
INTRODUCTION,0.022271714922048998,"axis of 
diversity
f3"
INTRODUCTION,0.024498886414253896,"(idealistic)
(realistic)"
INTRODUCTION,0.026726057906458798,"treat as
“black box”"
INTRODUCTION,0.028953229398663696,"archive of levels
random parameters"
INTRODUCTION,0.031180400890868598,"low diversity
discovered diversity DIVA"
INTRODUCTION,0.0334075723830735,QD updates
INTRODUCTION,0.035634743875278395,"Figure 1: Highly structured environment simulators assume access to parameterizations ES(θ)
for which random seeds θi directly produce meaningfully diverse features (e.g. RACING tracks
with challenging turns). Open-ended environments with ﬂexible, unstructured parameterizations
EU(θ)—though enabling more complex emergent features—lack direct control over high-level
features of interest. We introduce DIVA, an approach that effectively creates a more workable
parameterization EQD(θ) by evolving levels beyond the minimally diverse population from EU(θ).
By training on these discovered levels, DIVA enables superior performance on downstream tasks."
INTRODUCTION,0.0378619153674833,"Prior works have explored the use of domain randomization (DR) and procedural generation (PG)
techniques to produce diverse training data for learning agents [6]. Despite eliminating the need for
hand-designing each task individually, human labor is still required to carefully design an environment
generator that can produce diverse, high-quality tasks. As environments become more complex and
open-ended, the ability to hand-design such a robust generator becomes increasingly infeasible.
Some methods, like PLR [7], attempt to ameliorate this limitation by learning a curriculum over
the generated levels, but these works still operate under the assumption that the generator produces
meaningfully diverse levels with a high probability."
INTRODUCTION,0.0400890868596882,"Unsupervised environment design (UED) [8] are a broad class of appproaches which use performance-
based metrics to adaptively form a curriculum of training levels. ACCEL [9], a state-of-the-art
UED method, uses an evolutionary process to discover more interesting regions of the simulator’s
parameter space (i.e. appropriately challenging tasks) than can be found by random sampling. While
UED approaches are designed to be generally applicable and require little domain knowledge, they
implicitly require a very constrained environment generator—one in which all axes of difﬁculty
correspond to meaningful learning potential for the downstream distribution. Moreover, when faced
with complex open-ended environments with arbitrary parameterizations, even ACCEL is not able to
efﬁciently explore the solution space, as it is still bottlenecked by the speed of agent evaluations."
INTRODUCTION,0.042316258351893093,"In this work, we introduce DIVA, an approach for generating diverse training tasks in open-ended
simulators to train adaptive agents. By using quality diversity (QD) optimization to efﬁciently explore
the solution space, DIVA bypasses the problem of needing to evaluate agents on all generated levels.
QD also enables ﬁne-grained control over the axes of diversity to be captured in the training tasks,
allowing the ﬂexible integration of task-related prior knowledge from both domain experts and
learning approaches. We demonstrate that DIVA, with limited supervision in the form of feature
samples from the target distribution, signiﬁcantly outperforms state of the art UED approaches—
despite the UED approaches being provided with signiﬁcantly more interactions. We further show
that UED techniques can be integrated into DIVA. Preliminary results with this combination (which
we call DIVA+) are promising, and suggest an exciting avenue for future work."
PRELIMINARIES,0.044543429844097995,"2
Preliminaries"
PRELIMINARIES,0.0467706013363029,"Meta-reinforcement learning.
We use the meta-reinforcement learning (meta-RL) framework to
train adaptive agents, which involves learning an adaptive policy πφ over a distribution of tasks T ."
PRELIMINARIES,0.04899777282850779,"begin stage 1
guide solutions to target
begin stage 2"
PRELIMINARIES,0.051224944320712694,"target 
region"
PRELIMINARIES,0.053452115812917596,sample mask
PRELIMINARIES,0.0556792873051225,"populate target
sample solutions"
PRELIMINARIES,0.05790645879732739,"(a)
(b)
(c)
(d)
(e)"
PRELIMINARIES,0.060133630289532294,Density
PRELIMINARIES,0.062360801781737196,"Figure 2: DIVA archive updates on ALCHEMY. The ﬁrst stage (a) begins with bounds that
encapsulate initial solutions, and the target region. As the ﬁrst stage progresses (b), and QD discovers
more of the solution space, the sampling region for the emitters gradually shrinks towards the target
region. The second stage begins by redeﬁning the archive bounds to be the target region and including
some extra feature dimensions (c). QD ﬁlls out just the target region now (d), using sample weights
from the target-derived prior (e), the same distribution used to sample levels during meta-training."
PRELIMINARIES,0.0645879732739421,"Each Mi ∈T is a Markov decision process (MDP) deﬁned by a tuple ⟨S, A, P, R, γ, T⟩, where
S is the set of states, A is the set of actions, P(st+1|st, at) is the transition distribution between
states given the current state and action, R(st, at) is the reward function, γ ∈[0, 1] is the discount
factor, and T is the horizon. Meta-training involves sampling tasks Mi ∼T , collecting trajectories
D = {τ h}H
h=0—where H is the number of episodes in each trial τ pertaining to the Mi—and
optimizing policy parameters φ to maximize the expected discounted returns across all episodes."
PRELIMINARIES,0.066815144766147,"VariBAD [10] is a context variable-based meta-RL approach which belongs to the wider class of RNN-
based methods [11, 12]. While prior methods [13, 14] also use context variables to assist in task adap-
tation, VariBAD uniquely learns within a belief-augmented MDP (BAMDP) ⟨S, A, Z, P, R, γ, T⟩
where the context variables z ∈Z encodes the agent’s uncertainty about the task, promoting Bayesian
exploration. VariBAD utilizes an RNN-based variational autoencoder (VAE) to model a posterior
belief over possible tasks given the full agent trajectory, permitting efﬁcient updates to prior beliefs."
PRELIMINARIES,0.06904231625835189,"Quality diversity.
For a given problem, quality diversity (QD) optimization framework aims to
generate a set of diverse, high-quality solutions. Formally, a problem instance of QD [15] speciﬁes
an objective function J : Rn →R and k features fi : Rn →R. Let S = f(Rn) be the feature
space formed by the range of f, where f : Rn →Rk is the joint feature vector. For each s ∈S,
the QD objective is to ﬁnd a solution θ ∈Rn where f(θ) = s and J(θ) is maximized. Since
Rk is continuous, an algorithm solving the QD problem deﬁnition above would require unbounded
memory to store all solutions. QD algorithms in the MAP-Elites [16] family therefore discretize
S via a tessellation method, where G is a tessellation of the continuous feature space S into NG
cells. In employing a MAP-Elites algorithm, we relax the QD objective to ﬁnd a set of solutions
θi, i ∈{1, . . . , NG}, such that each θi occupies one unique cell in G. We call the occupants θi of all
M cells, each with its own position f(θi) and objective value J(θi), the archive of solutions."
PROBLEM SETTING,0.07126948775055679,"3
Problem setting"
PROBLEM SETTING,0.07349665924276169,"One assumption underlying UED methods is that random parameters—or parameter perturbations
for ACCEL —produce meaningfully different levels to justify the expense of computing objectives on
each newly generated level. However, when the genotype is not well-behaved—when meaningful di-
versity is rarely generated through random sampling or mutations—these algorithms waste signiﬁcant
time evaluating redundant levels. In our work, we discard the assumption of well-behaved genotypes
in favor of making fewer, more realistic assumptions about complex environment generators. There
are several assumptions we make about the simulated environments DIVA has access to."
PROBLEM SETTING,0.0757238307349666,"Genotypes.
We assume access to an unstructured environment parameterization function EU(θ),
where each θ is a genotype (corresponding to the QD solutions θi) describing parameters to be fed
into the environment generator. QD algorithms can support both continuous and discrete genotype
spaces, and in this work we evaluate on domains with both kinds. Crucially, we make no assumption
of the quality of the training tasks produced by this random generator. We only assume that (1) there
is some nonzero (and for practical purposes, nontrivial) probability that this generator will produce
a valid level for training—one in which success is possible and positive rewards are in reach; and
(2) that it is computationally feasible to discover meaningful feature diversity through an intelligent
search over the parameter space—an assumption implicit in all QD applications."
PROBLEM SETTING,0.0779510022271715,"Features.
We assume access to a pre-deﬁned set of features, S = f(Rn), that capture axes of
diversity which accurately characterize the diversity to be expected within the downstream task
distribution. It is also possible to learn or select good environment features from a sample of tasks
from the downstream distribution, which we discuss in Section 7. For the sake of simplicity, we use
a grid archive as our tessellation G, where the k dimensions of the discrete archive correspond to
the deﬁned features. The number of bins for each feature is a hyperparameter, and can be learned
or adapted over the course of training. We generally ﬁnd it to be helpful to use moderately high
resolutions to ease the search, since smaller leaps in feature-level diversity are required to uncover
new cells. By default, we use 100 sample feature values across all domains, but demonstrate in
ablation studies that that signiﬁcantly fewer may be used (see Appendix C)."
DIVA,0.0801781737193764,"4
DIVA"
DIVA,0.08240534521158129,"DIVA assumes access to a small set of feature samples representative of the target domain. It does
not, however, require access to the underlying levels themselves. This is a key distinction, as the
former is a signiﬁcantly weaker assumption. Consider the problem of training in-home assistive
robots in simulation with the objective of adapting to real-world houses. It is more likely we have
access to publicly available data describing typical houses—dimensions, stylistic features, etc.—than
we have access to corresponding simulator parameters which produce those exact feature values."
DIVA,0.08463251670378619,"Feature density estimation.
DIVA begins by constructing a QD archive with appropriate bounds
and resolution. Given a set of speciﬁed features {fi}k and a handful of downstream feature samples,
we ﬁrst infer each feature’s underlying distribution. These can be approximated with kernel density
estimation (KDE), or we can work with certain families of parameterized distributions. For our
experiments, we assume each feature is either (independently) normally or uniformly distributed. We
use a statistical test2 to evaluate the ﬁt of each distribution family, and select the best-ﬁtting. Setting
the resolution for discrete feature dimensions is straightforward, and depends only on the range. For
continuous features, the resolution should enable enough signal for discovering new cells, while
avoiding practical issues that arise with too many cells3. See Section 5 for domain-speciﬁc details."
DIVA,0.08685968819599109,"Two-stage QD updates.
Once the feature-speciﬁc target distributions are determined, we can use
these to set bounds for each archive dimension. A naïve approach would be to set the archive ranges
for each feature based on the conﬁdence bounds of the target distribution. However, random samples
from EU may not produce feature values that fall within the target range. We found this to be a major
issue in the ALCHEMY domain (see Figure 2), and for some features in RACING. We solve this
problem by setting the initial archive bounds to include both randomly generated samples from EU, as
well as the full target region. As the updates progress, we gradually update the sample mask—which
is used to inform the sampling of new solutions—towards the target region. We observe empirically
that updating and applying this mask provides an enormous speed-up in guiding solutions towards the
target region (see Figure 15). After this ﬁrst stage, solutions are inserted into a new archive deﬁned
by the proper target bounds. See Appendix A for more speciﬁcs on these two QD update stages."
DIVA,0.08908685968819599,Algorithm 1 DIVA
DIVA,0.09131403118040089,"# Stage 1: discover target region
1: G ←initialize_archive()
2: for i in range(NS1) do
3:
G ←qd_update(G, J, M, BQD)
4:
M ←update_sample_mask(M, G)
# Stage 2: populate target region
5: G ←update_archive_bounds(G)
6: for i in range(NS2) do
7:
G ←qd_update(G, J, ∅, BQD)
# Meta-learn over QD archive
8: for i in range(NVB) do
9:
M ←EU(θ′ ∼PG(θ))
10:
τ ←perform_rollout(M, πφ)
11:
Dπ ←store_rollout(Dπ, τ)
12:
meta_update(πφ, Dπ)"
DIVA,0.0935412026726058,"Overview.
DIVA consists of three stages. Stage 1
(S1) begins by initializing the archive with bounds that
include both the downstream feature samples (the target
region), as well as the initial population generated from
EU(θ). S1 then proceeds with alternating QD updates,
to discover new solutions, and sample mask updates,
to guide the population towards the target region. In
Stage 2 (S2), the archive is reinitialized with existing
solutions, but is now bounded by the target region. QD
updates continue to further diversify the population,
now targeting the downstream feature values speciﬁ-
cally. The last stage is standard meta-training, where
training task parameters are now drawn from PG(θ), a
distribution over the feature space approximated using
the downstream feature samples, discretized over the
archive cells. See Appendix A for detailed pseudocode."
DIVA,0.0957683741648107,"2We use a Kolmogorov–Smirnov test for features with continuous values and Chi-squared for discrete.
3Memory is one concern; another is that optimizing objectives across all cells is slower with more cells."
EMPIRICAL RESULTS,0.09799554565701558,"5
Empirical results"
EMPIRICAL RESULTS,0.10022271714922049,"Baselines.
We implement the following baselines to evaluate their relative performance to DIVA.
ODS is the “oracle"" agent trained over the downstream environment distribution ES(θ), used for
evaluation. With this baseline, we are benchmarking the upper bound in performance from the
perspective of a learning algorithm that has access to the underlying data distribution.4 DR is the
meta-learner trained over a task distribution deﬁned by performing domain randomization over the
space of valid genotypes, θ, under the training parameterization, EU(θ). Robust PLR (PLR⊥) [17]
is the improved and theoretically grounded version of PLR [7], where agents’ performance-based
PLR objectives are evaluated on each level before using them for training. ACCEL [9] is the same as
PLR⊥but instead of randomly sampling over the genotype space to generate levels for evaluation,
levels are mutated from existing solutions. All baselines use VariBAD [10] as their base meta-learner."
EMPIRICAL RESULTS,0.10244988864142539,"Experimental setup.
The oracle agent (ODS) is ﬁrst trained over the each environment’s down-
stream distribution to tune VariBAD’s hyperparameters. These environment-speciﬁc VariBAD settings
are then ﬁxed while hyperparameters for DIVA and the other baselines are tuned. For fairness of
comparison—since DIVA is allowed NQD QD update steps to ﬁll its archive before meta-training—
we allow each UED approach (PLR⊥and ACCEL) to use signiﬁcantly more environment steps for
agent evaluations (details discussed below per environment). All empirical results were run with 5
seeds unless otherwise speciﬁed, and error bars indicate a 95% conﬁdence region for the metric in
question. The QD archive parameters were set per environment, and for ALCHEMY and RACING,
relied on some hand-tuning to ﬁnd the right combinations of features and objectives. We leave it to
future work to perform a deeper analysis on what constitutes good archive design, and how to better
automate this process."
GRIDNAV,0.10467706013363029,"5.1
GRIDNAV"
GRIDNAV,0.10690423162583519,"Figure 3: Left: A GRIDNAV agent attempt-
ing to locate the goal across two episodic
rollouts. Right: The marginal probability
of sampled goals inhabiting each y for dif-
ferent complexities k of Ek(θ)."
GRIDNAV,0.1091314031180401,"Our ﬁrst evaluation domain is a modiﬁed version of
GRIDNAV (Figure 3), originally introduced to motivate
and benchmark VariBAD [10]. The agent spawns at
the center of the grid at the start of each episode, and
receives a slight negative reward (r = −0.1) each step
until it discovers (inhabits) the goal cell, at which point
it also receives a larger positive reward (r = 1.0)."
GRIDNAV,0.111358574610245,"Parameterization.
We parameterize the task space
(i.e. the goal location) to reduce the likelihood of gen-
erating meaningfully diverse goals. Speciﬁcally, each
EUk (or Ek) introduces k genes to the solution genotype
which together deﬁne the ﬁnal y location. Each gene
j can assume the values θj ∈{−1, 0, 1}, and the ﬁnal
y location is determined by summing these values, and
performing a ﬂoor division to map the bounds back to
the original range of the grid. As k increases, y values are increasingly biased towards 0, as shown
on the right side of Figure 3. For more details on the GRIDNAV domain, see Appendix B.1."
GRIDNAV,0.11358574610244988,"QD updates.
We deﬁne the archive features to be the x and y coordinates of the goal location.
The objective is set to the current iteration, so that newer solutions are prioritized (additional
details in Appendix B.1). DIVA is provided NS2 = 8.0 × 104 (NS1 = 0) QD update iterations
for ﬁlling the archive. To compensate, PLR⊥and ACCEL are each provided with an additional
9.6 × 106 environment steps for evaluating PLR scores, which amounts to three times as many total
interactions—since all methods are provided NE = 4.8×106 interactions for training. If each “reset""
call counts as one environment step5, the UED baselines are effectively granted 2.4× more additional
step data than what DIVA additionally receives through its QD updates (details in Appendix E.1)."
GRIDNAV,0.11581291759465479,"Results.
From Figure 4a, we see that increasing genotype complexity (i.e. larger k) reduces goal
diversity for DR—which is expected given the parameterization deﬁned for EU. We can also see
that DIVA, as a result of its QD updates, can effectively capture goal diversity, even as complexity"
GRIDNAV,0.11804008908685969,"4Technically, reweighting this distribution (e.g. via PLR) may produce a stronger oracle, but for the purposes
of this work, we assume the unaltered downstream distribution can be efﬁciently trained over, sans curriculum.
5In general, rendering the environment (via “reset"") is required to compute level features for DIVA."
GRIDNAV,0.12026726057906459,"20
40
60
80
100
Genotype Complexity k 0.0 0.2 0.4 0.6 0.8 1.0"
GRIDNAV,0.12249443207126949,Percent Coverage
GRIDNAV,0.12472160356347439,Goal Diversity by θ Complexity
GRIDNAV,0.12694877505567928,"DIVA
DR∗ DR (a)"
GRIDNAV,0.1291759465478842,"0
1
2
3
Updates
×102 0.0 0.2 0.4 0.6 0.8 1.0"
GRIDNAV,0.13140311804008908,Percent Coverage
GRIDNAV,0.133630289532294,∗values from Figure 3a (k = 24) ∗ ∗ ∗
GRIDNAV,0.1358574610244989,UED Goal Diversity (k = 24) PLR⊥ ACCEL
GRIDNAV,0.13808463251670378,"Valid
Unique (b)"
GRIDNAV,0.1403118040089087,"0.0
0.2
0.4
0.6
0.8
1.0
Updates
×104 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75"
GRIDNAV,0.14253897550111358,Returns
GRIDNAV,0.1447661469933185,×101 Final Episode Returns
GRIDNAV,0.14699331848552338,"∗ODS
DIVA
ACCEL
PLR⊥ DR (c)"
GRIDNAV,0.1492204899777283,"DIVA
ACCEL
PLR⊥
DR
Method 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9"
GRIDNAV,0.1514476614699332,Success Rate
GRIDNAV,0.15367483296213807,"Success Rate by Episode τ1
τ2 (d)"
GRIDNAV,0.155902004454343,"Figure 4: GRIDNAV analysis and results. (a) Target region coverage produced by DIVA and DR
over different genotype complexities k. DR represents the average coverage of batches corresponding
to the size of the QD archive. DR∗represents the total number of unique levels discovered over
the course of parameter randomization steps which equal in number to the additional environments
PLR⊥is provided for evaluation. DR∗is thus an upper bound on the diversity that PLR⊥can capture.
500k iterations (QD or otherwise) are used across all results. (b) The diversity produced by PLR⊥
and ACCEL over the course of training (later updates omitted due to no change in trend). (c) Final
episode return curves for DIVA and baselines. (d) Final method success rates across each episode."
GRIDNAV,0.15812917594654788,"increases. When we ﬁx the complexity (k = 24) and train over the EU distribution, we see that the
UED approaches are unable to incidentally discover and capture diversity over the course of training
(Figure 4b). DIVA’s explicit focus on capturing meaningful level diversity enables it to signiﬁcantly
outperform these baselines in terms of episodic return (Figure 4c) and success rate (Figure 4d)."
ALCHEMY,0.1603563474387528,"5.2
ALCHEMY"
ALCHEMY,0.16258351893095768,"ALCHEMY [18] is an artiﬁcial chemistry environment with a combinatorially complex task distri-
bution. Each task is deﬁned by some latent chemistry, which inﬂuences the underlying dynamics,
as well as agent observations. To successfully maximize returns over the course of a trial, the agent
must infer and exploit this latent chemistry. At the start of each episode, the agent is provided a new
set of (1-12) potions and (1-3) stones, where each stone has a latent state deﬁned by a speciﬁc vertex
of a cube, i.e. ({0, 1}, {0, 1}, {0, 1}), and each potion has a latent effect, or speciﬁc manner in which
it transforms stone latent states (see Figure 5a). The agent observes only salient artifacts of this latent
information, and must use interactions to identify the ground-truth mechanics. At each step, the agent
can apply any remaining potion to any remaining stone. Each stone’s value is maximized the closer
its latent state is to (1, 1, 1), and rewards are produced when stones are cast into the cauldron."
ALCHEMY,0.16481069042316257,"To make training feasible on academic resources, we perform evaluations on the symbolic version
of ALCHEMY, as opposed to the full Unity-based version. Symbolic ALCHEMY contains the same
mechanistic complexity, minus the visuomotor challenges which are irrelevant to this project’s aims."
ALCHEMY,0.16703786191536749,"Parameterization.
ES(θ) is the downstream distribution containing maximal stone diversity. For
training, implement EUk where k controls the level of difﬁculty in generating diverse stones. Speciﬁ-
cally, we introduce a larger set of coordinating genes θj ∈{0, 1} that together specify the initial stone
latent states, similar to the mechanism we used in GRIDNAV to limit goal diversity. Each stone latent
coordinate is speciﬁed with k genes, and only when all k genes are set to 1 is the latent coordinate is
set to 1. When any of the genes are 0, the latent coordinate is 0. For our experiments we set k = 8,
and henceforth use EU to signify EU8."
ALCHEMY,0.16926503340757237,"QD updates.
We use features LATENTSTATEDIVERSITY and MANHATTANTOOPTIMAL —both
of which target stone latent state diversity from different angles. See Appendix B.2 for more speciﬁcs
on these features and other details surrounding ALCHEMY’s archive construction. Like GRIDNAV,
the objective is set to bias new solutions. DIVA is provided with NS1 = 8.0 × 104 and NS2
= 3.0 × 104 QD update iterations. PLR⊥and ACCEL are compensated such that they receive 3.5×
more additional step data than what DIVA receives via QD updates (see Appendix E.1 for details)."
ALCHEMY,0.1714922048997773,"Results.
Our empirical results demonstrate that DIVA is able to generate latent stone states with
diversity representative of the target distribution. We see this both quantitatively in Figure 5b, and
qualitatively in Figure 6. In Figure 5c, we see this diversity translates to signiﬁcantly better results"
ALCHEMY,0.17371937639198218,"(0, 0, 0)"
ALCHEMY,0.1759465478841871,"(1, 1, 1)
y z x P1 P2 (a)"
ALCHEMY,0.17817371937639198,ManhattanToOptimal
ALCHEMY,0.18040089086859687,LatentStateDiversity
ALCHEMY,0.18262806236080179,Feature Values
ALCHEMY,0.18485523385300667,Density
ALCHEMY,0.1870824053452116,"ES
DIVA
EU (b)"
ALCHEMY,0.18930957683741648,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
Updates
×104 0 1 2 3 4 5 6 7"
ALCHEMY,0.1915367483296214,Returns
ALCHEMY,0.19376391982182628,Final Episode Returns (c)
ALCHEMY,0.19599109131403117,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
Updates
×104 0.2 0.4 0.6 0.8 1.0 1.2 1.4"
ALCHEMY,0.19821826280623608,Quantity
ALCHEMY,0.20044543429844097,"×104
Unique Genotypes"
ALCHEMY,0.2026726057906459,"∗ODS
DIVA
ACCEL
PLR⊥ DR (d)"
ALCHEMY,0.20489977728285078,"Figure 5: ALCHEMY environment and results. (a) A visual representation of ALCHEMY’s structured
stone latent space. P1 and P2 represent potions acting on stones. Only P1 results in a latent state
change, because P2 would push the stone outside of the valid latent lattice. (b) Marginal feature
distributions for ES (the structured target distribution), DIVA, and EU (the unstructured distribution
used directly for DR, and to initialize DIVA’s archive). (c) Final episode return curves for DIVA and
baselines. (d) Number of unique genotypes used by each method over the course of meta-training. 3 1 2
3 2
1
4 5
6 4 5 6"
ALCHEMY,0.2071269487750557,Recency
ALCHEMY,0.20935412026726058,"Early samples
Later samples"
ALCHEMY,0.21158129175946547,LatentStateDiversity
ALCHEMY,0.21380846325167038,ManhattanToOptimal
ALCHEMY,0.21603563474387527,Archive after S1
ALCHEMY,0.2182628062360802,"Figure 6: ALCHEMY level diversity. Early on in DIVA’s QD updates (left), the levels in the archive
do not posses much latent stone diversity—all are close to (1, 1, 1). As samples begin populating the
target region in later QD updates (right), we see stone diversity is signiﬁcantly increased."
ALCHEMY,0.22048997772828507,"on ES over baselines. Despite generating roughly as many unique genotypes as DIVA (Figure 5d),
PLR⊥and ACCEL are unable to generate training stone sets of signiﬁcant phenotypical diversity to
enable success on the downstream distribution."
RACING,0.22271714922049,"5.3
RACING"
RACING,0.22494432071269488,"Lastly, we evaluate DIVA on the RACING domain introduced by [17]. In this environment, the agent
controls a race car via simulated steering and gas pedal mechanisms, and is rewarded for efﬁciently
completing the track, Mi ∈T . We adapt this RL environment to the meta-RL setting by lowering
the resolution of the observation space signiﬁcantly. By increasing the challenge of perception, even
competent agents beneﬁt from multiple episodes to better understand the underlying track. For all of
our experiments, we use H = 2 episodes per trial, and a ﬂattened 15 × 15 pixel observation space."
RACING,0.22717149220489977,"Setup.
We use three different parameterizations in our experiments: (1) ES(θ) is the downstream
distribution we use for evaluating all methods, training ODS, and setting archive bounds for DIVA.
Parameters θ are used to seed the random generation of control points which in turn parameterize
a sequence of Bézier curves designed to smoothly transition between the control locations. Track
diversity is further enforced by rejecting levels with control points that possess a standard deviation
below a certain threshold. (2) EUk(θ) is a reparameterization of ES(θ) that makes track diversity
harder to generate, with the difﬁculty proportional to the value of k ∈N. For our experiments, we use
k = 32 (which we will denote simply as EU(θ)), which roughly means that meaningful diversity is
32× less likely to randomly occur than when k = 1 (which is equivalent to ES(θ)). This is achieved
by deﬁning a small region in the center, 32 (or k, in general) times smaller than the track boundaries,
where all points outside the region are projected onto the unit square, and scaled to the track size. (3)
EF1(θ) uses θ as an RNG seed to select between a set of 20 hand-crafted levels ofﬁcial Formula-1
tracks [17], and is used to benchmark DIVA’s zero-shot generalization to a new target distribution."
RACING,0.22939866369710468,"QD updates.
We deﬁne features TOTALANGLECHANGES (TAC) and CENTEROFMASSX (CX) for
the archive dimensions. Levels from EU lack curvature (see Figure 8) so TAC, which is deﬁned as the
sum of angle changes between track segments, is useful for directly targeting this desired curvature."
RACING,0.23162583518930957,"AreaToLengthRatio
CenterOfMassX"
RACING,0.23385300668151449,"VarianceX
TotalAngleChanges"
RACING,0.23608017817371937,Feature Values
RACING,0.2383073496659243,Density
RACING,0.24053452115812918,"ES
EF1
DIVA
EU (a)"
RACING,0.24276169265033407,"0.0
0.2
0.4
0.6
0.8
1.0
Updates
×104 0.0 0.2 0.4 0.6 0.8 1.0"
RACING,0.24498886414253898,Returns
RACING,0.24721603563474387,"×103
Final Episode Returns"
RACING,0.24944320712694878,"∗ODS
DIVA
ACCEL
PLR⊥ DR (b)"
RACING,0.2516703786191537,"80%
90%
100%
Completion Threshold 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7"
RACING,0.25389755011135856,Success Rate
RACING,0.2561247216035635,Track Completion
RACING,0.2583518930957684,"∗ODS
DIVA
ACCEL
PLR⊥ DR (c)"
RACING,0.26057906458797325,"Figure 7: RACING features and main results. Left: Marginal feature distributions for ES (target
distribution), EF1 (human-designed F1 tracks), DIVA, and EU (the unstructured distribution used for
DR, the original levels that DIVA evolves)—cropped for readability. Center: Final episode return
curves for DIVA and baselines on ES. Right: Track completion rates by method, evaluated on ES."
RACING,0.26280623608017817,CenterOfMassX
RACING,0.2650334075723831,TotalAngleChanges
RACING,0.267260579064588,Archive at t = 0
RACING,0.26948775055679286,"J
CenterOfMassX"
RACING,0.2717149220489978,TotalAngleChanges
RACING,0.2739420935412027,Final archive J
RACING,0.27616926503340755,"Figure 8: RACING level diversity. We see that random EU levels, used by DR, and which form
the initial population of DIVA, are unable to produce qualitatively diverse tracks (left). After the
two-stage QD-updates, DIVA is able to produce tracks of high qualitative diversity (right)."
RACING,0.27839643652561247,"CX, or the average location of the segments, targets diversity in the location of these high-density
(high-curvature) regions. We compute an alignment objective over features CENTEROFMASSY and
VARIANCEY to further target downstream diversity. See Appendix B.3 for more details relevant to
the archive construction process for RACING. DIVA is provided with 2.5 × 105 initial QD updates
on RACING. PLR⊥and ACCEL are compensated with 4.0× more additional step data than what
DIVA receives through QD updates (see Appendix E.1 for more details)."
RACING,0.2806236080178174,"Main results.
Results are shown in Figure 7. DIVA outperforms all baselines, including the UED
approaches, which have access to three times as many environment interactions. From Figure 8, we
see that ﬁnal DIVA levels contain signiﬁcantly more diversity than randomization over EU."
RACING,0.2828507795100223,F1 Levels
RACING,0.28507795100222716,"80%
90%
100%
Completion Threshold 0.00 0.05 0.10 0.15"
RACING,0.2873051224944321,Success Rate
RACING,0.289532293986637,Track Completion on EF1(θ)
RACING,0.29175946547884185,"∗ODS
DIVA
DR"
RACING,0.29398663697104677,"Figure 9: Sample F1 levels (top), and
track completion rates by methods tar-
geting ES, evaluated on EF1 (bottom)."
RACING,0.2962138084632517,"Transfer to F1 tracks.
Next, we evaluate the ability
of these trained policies to zero-shot transfer to human-
designed F1 levels [17], EF1. Though qualitative differ-
ences are apparent (see Figure 9), from Figure 7a we can
additionally see how these levels differ quantitatively. Even
though DIVA uses feature samples from ES to deﬁne its
archive, we see from the results in Figure 9 that DIVA is
not only able to complete many of these tracks, but is also
able to signiﬁcantly outperform ODS. This result may seem
unlikely, given that DIVA bases its axes of diversity on
ES. One possible explanation is that while DIVA success-
fully matches its TOTALANGLECHANGES distribution to
ES (see Figure 7), because it is less likely for all 12 control
points to be mutated to the diversity-enabling region than
just a few control points with sharp angles, DIVA “opts"" for
the latter, and thus produces fewer, sharper angles, which is
evidently useful for transferring to (these) human-designed
tracks. This hypothesis matches what we see qualitatively
from the DIVA-produced levels in Figure 8."
RACING,0.2984409799554566,"Combining DIVA and UED.
While PLR⊥and ACCEL struggle on our evaluation domains,
they still have utility of their own, which we hypothesize may be compatible with DIVA’s. As a
preliminary experiment to evaluate the potential of such a combination, we introduce DIVA+, which
still uses DIVA to generate diverse training samples via QD, but additionally uses PLR⊥to deﬁne"
RACING,0.30066815144766146,"80%
90%
100%
Completion Threshold 0.0 0.1 0.2 0.3 0.4 0.5"
RACING,0.3028953229398664,Success Rate
RACING,0.3051224944320713,Track Completion
RACING,0.30734966592427615,"DIVA1
DIVA1+
DIVA2
DIVA2+"
RACING,0.30957683741648107,"Figure 10: DIVA+ results compared to
DIVA, for (1) misspeciﬁed, and (2) well-
speciﬁed archives, evaluated on ES."
RACING,0.311804008908686,"a new distribution over these levels based on approxi-
mate learning potential. Instead of randomly sampling
levels from EU, the PLR⊥evaluation mechanism sam-
ples levels from the DIVA-induced distribution over
the archive. We perform experiments on two differ-
ent archives generated by DIVA: (1) an archive that
is slightly misspeciﬁed (see Appendix B.3 for details),
and (2) the archive used in our main results. From Fig-
ure 10, we see that while performance does not sig-
niﬁcantly improve for (2), the combination of DIVA
and PLR⊥is able to signiﬁcantly improve performance
on (1), and even statistically match the original DIVA
results. These results highlight the potential of such
hybrid (QD+UED) semi-supervised environment design
(SSED) approaches, a promising area for future work."
RELATED WORK,0.31403118040089084,"6
Related work"
RELATED WORK,0.31625835189309576,"Meta-reinforcement learning.
Meta-reinforcement learning methods range from gradient-based
approaches (e.g. MAML) [19], RNN context-based approaches [12, 11] (e.g. RL2), and the slew
of emerging works utilizing transformers [20, 5, 21]. We use VariBAD [10], a state-of-the-art
context variable-based approach that extends RL2 by using variational inference to incorporate task
uncertainty into its beliefs. HyperX [22], an extension that uses reward-bonuses, was not found to
improve performance on our domains. In each of these works, the training distribution is given; none
address the problem of generating diverse training scenarios in absence of such a distribution."
RELATED WORK,0.3184855233853007,"Procedural environment generation.
Procedural (content) generation (PCG / PG) [6] is a vast
ﬁeld. Many RL and meta-RL domains themselves have PG baked-in (e.g. ProcGen [23], Meta-World,
[24], Alchemy [18], and XLand [5]). Each of these works rely on human engineering to produce
levels with meaningfully diverse features. A related stream of works apply scenario generation to
robotics—some works essentially perform PCG [25, 26], while others integrate more involved search
mechanics [27, 28, 29, 30]. One prior work [31] deﬁnes a formal but generic parameterization for
applying PG to generate meta-RL tasks. It is yet to be shown, however, if such an approach can scale
to domains with vastly different dynamics, and greater complexity."
RELATED WORK,0.3207126948775056,"Unsupervised environment design.
UED approaches—which use behavioral metrics to automati-
cally deﬁne and adapt a curriculum of suitable tasks for agent training—form the frontier of research
on open-endedness. The recent stream of open-ended agent/environment co-evolution works (e.g.
[32, 33, 34]) was kickstarted by the POET [35, 36] algorithm. The “UED” term itself originated in
PAIRED [8], which uses the performance of an “antagonist” agent to deﬁne the curriculum for the
main (protagonist) agent. PLR [7] introduces an approach for weighting training levels based on
learning potential, using various proxy metrics to capture this high-level concept. [17] introduces
PLR⊥, which only trains on levels that have been previously evaluated, and thus enabling certain
theoretical robustness guarantees. AdA [5] uses PLR as a cornerstone of their approach for generating
diverse training levels for adaptive agents in a complex, open-ended task space. ACCEL [9] borrows
PLR⊥’s scoring procedure, but the best-performing solutions are instead mutated, so the buffer not
only collects and prioritizes levels of higher learning potential, but evolves them. We use ACCEL as
our main baseline because it has demonstrated state-of-the art results on relevant domains, and like
DIVA, evolves a population of levels. The main algorithmic differences between ACCEL and DIVA
are that ACCEL (1) performs additional evaluation rollouts to produce scores during training and (2)
uses a 1-d buffer instead of DIVA’s multi-dimensional archive. PLR⊥serves as a secondary baseline
in this work; its non-evolutionary nature makes it a useful comparison to DR."
RELATED WORK,0.32293986636971045,"Scenario generation via QD.
A number of recent works apply QD to simulated environments in
order to generate diverse scenarios, with distinct aims. Some works, like DSAGE [37], uses QD
to develop diverse levels for the purpose of probing a pretrained agent for interesting behaviors.
In another line of work applies QD to human-robot interaction (HRI), and ranges from generating"
RELATED WORK,0.32516703786191536,"diverse scenarios [38], to ﬁnding failure modes in shared autonomy systems [39] and human-aware
planners [40]. DIVA’s application of QD inspired by these approaches, as they produce meaningfully
diverse environment scenarios, but no prior work exists which applies QD to deﬁne a task distribution
for agent training, much less adaptive agent training, or overcoming difﬁcult parameterizations in
open-ended environments."
DISCUSSION,0.3273942093541203,"7
Discussion"
DISCUSSION,0.32962138084632514,"The present work enables adaptive agent training on open-ended environment simulators by in-
tegrating the unconstrained nature of unsupervised environment design (UED) approaches, with
the implicit supervision baked into procedural generation (PG) and domain randomization (DR)
methods. Unlike PG and DR, which requires domain knowledge to be carefully incorporated into
the environment generation process, DIVA is able to ﬂexibly incorporate domain knowledge, and
can discover new levels representative of the downstream distribution. And instead of relying on
behavioral metrics to infer a general, ungrounded form of “learning potential”, like UED—which
becomes increasingly unconstrained and therefore less useful a signal as environments become more
complex and open-ended—DIVA is able to directly incorporate downstream feature samples to target
speciﬁc, meaningful axes of diversity. With only a handful of downstream feature samples to set the
parameters of the QD archive, our experiments (Section 5) demonstrate DIVA’s ability to outperform
competitive baselines compensated with three times as many environment steps during training."
DISCUSSION,0.33184855233853006,"In its current form, the most obvious limitation of DIVA is that, in addition to assuming access
to downstream feature samples, the axes of diversity themselves must be speciﬁed. However, we
imagine these axes of diversity could be learned automatically from a set of sample levels, or selected
from a larger set of candidate features; it may be possible to adapt existing QD works to automate
this process in related settings [41]. The present work also lacks a more thorough analysis of what
constitutes good archive design. While some amount of heuristic decision-making is unavoidable
when applying learning algorithms to speciﬁc domains, a promising future direction would be to
study how to approach DIVA’s archive design from a more algorithmic perspective."
DISCUSSION,0.33407572383073497,"DIVA currently performs QD iterations over the environment parameter space deﬁned by EU(θ),
where each component of the genotype θ represents some salient input parameter to the simulator.
Prior works in other domains (e.g. [42]) have demonstrated QD’s ability to explore the latent space of
generative models. One natural direction for future work would therefore be to apply DIVA to neural
environment generators (rather than algorithmic generators), where θ would instead correspond to
the latent input space of the generative model. If the latent space of these models is more convenient
to work with than the raw environment parameters—e.g. due to greater smoothness with respect
to meaningful axes of diversity—this may help QD more efﬁceintly discover samples within the
target region. Conversely, DIVA’s ability to discover useful regions of the parameter space means
these neural environment generators do not need to be “well-behaved"", or match a speciﬁc target
distribution. Since these generative models are also likely to be differentiable, DIVA can additionally
incorporate gradient-based QD works (e.g. DQD [15]) to accelerate its search."
DISCUSSION,0.3363028953229399,"Preliminary results with DIVA+ demonstrate the additional potential of combining UED and DIVA
approaches. The F1 transfer results (i.e. DIVA outperforming ODS trained directly on ES) further
suggest that agents beneﬁt from ﬂexible incorporation of downstream knowledge. In future work, we
hope to study more principle integrations of UED and DIVA-like approaches, and to more generally
explore this exciting new area of semi-supervised environment design (SSED)."
DISCUSSION,0.33853006681514475,"More broadly, now equipped with DIVA, researchers can develop more general-purpose, open-ended
simulators, without concerning themselves with constructing convenient, well-behaved parameteri-
zations. Evaluations in this work required constructing our own contrived paramterizations, since
domains are rarely released without carefully designed parameterizations. It is no longer necessary to
accomodate the assumption made my DR, PG, and UED approaches—that either randomization over
the parameter space should produce meaningful diversity, or that all forms of level difﬁculty ought to
correspond to meaningful learning potential. So long as diverse tasks are possible to generate, even if
sparsely distributed within the paramter space, QD may be used to discover these regions, and exploit
them for agent training. Based on the promising empirical results presented in this work, we are
hopeful that DIVA will enable future works to tackle even more complicated domains, and assist
researchers in designing more capable and behaviorally interesting adaptive agents."
REPRODUCIBILITY STATEMENT,0.34075723830734966,"8
Reproducibility statement"
REPRODUCIBILITY STATEMENT,0.3429844097995546,"The source code, along with thorough documentation for reproducing each result in this paper, is
publicly available on Github6. Even without this code, researchers should be able to fully reproduce
the algorithm from the details in the main body, the pseudocode provided in Appendix A, and training
details (hyperparameters and hardware information) provided in Appendix E."
ETHICS STATEMENT,0.34521158129175944,"9
Ethics statement"
ETHICS STATEMENT,0.34743875278396436,"Like all fundamental technologies, this work has the potential to be misapplied for malicious purposes.
The authors do not believe, however, that the methods introduced in this work present a signiﬁcant or
unique risk for misuse or abuse. The authors intend for DIVA to be applied to use-cases that have the
best interests of humanity (including concern for the earth and other sentient creatures) at heart."
ACKNOWLEDGEMENTS,0.34966592427616927,"10
Acknowledgements"
ACKNOWLEDGEMENTS,0.3518930957683742,"This work was partially supported by NSF CAREER (#2145077) and the DARPA EMHAT project.
We thank Tjanaka et al., the developers of pyribs [43], whose library served as the basis for our QD
implementations. We thank Zintgraf et al., the authors of VariBAD [10], whose codebase served as
the basis for our meta-RL agent. We thank Jiang et al. and Parker-Holder et al., the authors of PLR
[7] and ACCEL [9], respectively, for their implementations which served as the basis for our UED
baselines. We speciﬁcally thank Minqi Jiang for answering questions related to the PLR codebase in
the early stages of development, and Varun Bhatt for helpful discussion at various stages of this work."
REFERENCES,0.35412026726057905,References
REFERENCES,0.35634743875278396,"[1] X. Wang, S. Wang, X. Liang, D. Zhao, J. Huang, X. Xu, B. Dai, and Q. Miao, “Deep reinforcement
learning: A survey,” IEEE Trans. Neural Netw. Learn. Syst., vol. 35, pp. 5064–5078, Apr. 2024."
REFERENCES,0.3585746102449889,"[2] K. Sivamayil, E. Rajasekar, B. Aljafari, S. Nikolovski, S. Vairavasundaram, and I. Vairavasundaram, “A
systematic study on reinforcement learning based applications,” Energies, vol. 16, p. 1512, Feb. 2023."
REFERENCES,0.36080178173719374,"[3] R. Kirk, A. Zhang, E. Grefenstette, and T. Rocktäschel, “A survey of zero-shot generalisation in deep
reinforcement learning,” jair, vol. 76, pp. 201–264, Jan. 2023."
REFERENCES,0.36302895322939865,"[4] J. Beck, R. Vuorio, E. Z. Liu, Z. Xiong, L. Zintgraf, C. Finn, and S. Whiteson, “A survey of meta-
reinforcement learning,” arXiv [cs.LG], Jan. 2023."
REFERENCES,0.36525612472160357,"[5] J. Bauer, K. Baumli, F. Behbahani, A. Bhoopchand, N. Bradley-Schmieg, M. Chang, N. Clay, A. Collister,
V. Dasagi, L. Gonzalez, K. Gregor, E. Hughes, S. Kashem, M. Loks-Thompson, H. Openshaw, J. Parker-
Holder, S. Pathak, N. Perez-Nieves, N. Rakicevic, T. Rocktäschel, Y. Schroecker, S. Singh, J. Sygnowski,
K. Tuyls, S. York, A. Zacherl, and L. M. Zhang, “Human-timescale adaptation in an open-ended task
space,” in Proceedings of the 40th International Conference on Machine Learning (A. Krause, E. Brunskill,
K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, eds.), vol. 202 of Proceedings of Machine Learning
Research, pp. 1887–1935, PMLR, 23–29 Jul 2023."
REFERENCES,0.3674832962138085,"[6] N. Shaker, J. Togelius, and M. J. Nelson, Procedural Content Generation in Games. Computational
Synthesis and Creative Systems, Springer, 2016."
REFERENCES,0.36971046770601335,"[7] M. Jiang, E. Grefenstette, and T. Rocktäschel, “Prioritized Level Replay,” in Proceedings of the 38th
International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event (M. Meila and
T. Zhang, eds.), vol. 139 of Proceedings of Machine Learning Research, pp. 4940–4950, PMLR, 2021."
REFERENCES,0.37193763919821826,"[8] M. Dennis, N. Jaques, E. Vinitsky, A. Bayen, S. Russell, A. Critch, and S. Levine, “Emergent complexity
and zero-shot transfer via unsupervised environment design,” Advances in neural information processing
systems, vol. 33, pp. 13049–13061, 2020."
REFERENCES,0.3741648106904232,"[9] J. Parker-Holder, M. Jiang, M. Dennis, and others, “Evolving curricula with regret-based environment
design,” International Conference on Machine Learning, 2022."
REFERENCES,0.37639198218262804,"[10] L. M. Zintgraf, K. Shiarlis, M. Igl, S. Schulze, Y. Gal, K. Hofmann, and S. Whiteson, “VariBAD: A
Very Good Method for Bayes-adaptive Deep RL via Meta-learning,” in 8th International Conference on
Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, OpenReview.net, 2020."
REFERENCES,0.37861915367483295,6Codebase: https://github.com/robbycostales/diva
REFERENCES,0.38084632516703787,"[11] Y. Duan, J. Schulman, X. Chen, P. L. Bartlett, I. Sutskever, and P. Abbeel, “RL2: Fast Reinforcement
Learning via Slow Reinforcement Learning,” arXiv:1611.02779 [cs, stat], Nov. 2016."
REFERENCES,0.3830734966592428,"[12] J. Wang, Z. Kurth-Nelson, H. Soyer, J. Z. Leibo, D. Tirumala, R. Munos, C. Blundell, D. Kumaran,
and M. M. Botvinick, “Learning to reinforcement learn,” in Proceedings of the 39th Annual Meeting of
the Cognitive Science Society, CogSci 2017, London, UK, 16-29 July 2017 (G. Gunzelmann, A. Howes,
T. Tenbrink, and E. J. Davelaar, eds.), cognitivesciencesociety.org, 2017."
REFERENCES,0.38530066815144765,"[13] L. Zintgraf, K. Shiarli, V. Kurin, K. Hofmann, and S. Whiteson, “Fast context adaptation via meta-learning,”
in International Conference on Machine Learning, pp. 7693–7702, PMLR, 2019."
REFERENCES,0.38752783964365256,"[14] K. Rakelly, A. Zhou, C. Finn, S. Levine, and D. Quillen, “Efﬁcient Off-policy Meta-reinforcement Learning
via Probabilistic Context Variables,” in Proceedings of the 36th International Conference on Machine
Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA (K. Chaudhuri and R. Salakhutdinov,
eds.), vol. 97 of Proceedings of Machine Learning Research, pp. 5331–5340, PMLR, 2019."
REFERENCES,0.3897550111358575,"[15] M. C. Fontaine and S. Nikolaidis, “Differentiable Quality Diversity,” in Advances in Neural Information
Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS
2021, December 6-14, 2021, virtual (M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang, and J. W.
Vaughan, eds.), pp. 10040–10052, 2021."
REFERENCES,0.39198218262806234,"[16] J. Mouret and J. Clune, “Illuminating search spaces by mapping elites,” CoRR, vol. abs/1504.04909, 2015."
REFERENCES,0.39420935412026725,"[17] M. Jiang, M. Dennis, J. Parker-Holder, J. N. Foerster, E. Grefenstette, and T. Rocktäschel, “Replay-guided
Adversarial Environment Design,” in Advances in Neural Information Processing Systems 34: Annual
Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual
(M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang, and J. W. Vaughan, eds.), pp. 1884–1897, 2021."
REFERENCES,0.39643652561247217,"[18] J. X. Wang, M. King, N. P. M. Porcel, Z. Kurth-Nelson, T. Zhu, C. Deck, P. Choy, M. Cassin, M. Reynolds,
H. F. Song, et al., “Alchemy: A benchmark and analysis toolkit for meta-reinforcement learning agents,” in
Thirty-ﬁfth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round
2), 2021."
REFERENCES,0.3986636971046771,"[19] C. Finn, P. Abbeel, and S. Levine, “Model-agnostic Meta-learning for Fast Adaptation of Deep Networks,”
in Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW,
Australia, 6-11 August 2017 (D. Precup and Y. W. Teh, eds.), vol. 70 of Proceedings of Machine Learning
Research, pp. 1126–1135, PMLR, 2017."
REFERENCES,0.40089086859688194,"[20] L. C. Melo, “Transformers are Meta-Reinforcement Learners,” in Proceedings of the 39th International
Conference on Machine Learning (K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato,
eds.), vol. 162 of Proceedings of Machine Learning Research, pp. 15340–15359, PMLR, 2022."
REFERENCES,0.40311804008908686,"[21] J. Grigsby, L. Fan, and Y. Zhu, “AMAGO: Scalable in-context reinforcement learning for adaptive agents,”
in The Twelfth International Conference on Learning Representations, 2024."
REFERENCES,0.4053452115812918,"[22] L. M. Zintgraf, L. Feng, C. Lu, M. Igl, K. Hartikainen, K. Hofmann, and S. Whiteson, “Exploration in
Approximate Hyper-state Space for Meta Reinforcement Learning,” in Proceedings of the 38th International
Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event (M. Meila and T. Zhang,
eds.), vol. 139 of Proceedings of Machine Learning Research, pp. 12991–13001, PMLR, 2021."
REFERENCES,0.40757238307349664,"[23] K. Cobbe, C. Hesse, J. Hilton, and J. Schulman, “Leveraging Procedural Generation to Benchmark
Reinforcement Learning,” in Proceedings of the 37th International Conference on Machine Learning,
ICML 2020, 13-18 July 2020, Virtual Event, vol. 119 of Proceedings of Machine Learning Research,
pp. 2048–2056, PMLR, 2020."
REFERENCES,0.40979955456570155,"[24] T. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and S. Levine, “Meta-World: A Benchmark
and Evaluation for Multi-task and Meta Reinforcement Learning,” in 3rd Annual Conference on Robot
Learning, CoRL 2019, Osaka, Japan, October 30 - November 1, 2019, Proceedings (L. P. Kaelbling,
D. Kragic, and K. Sugiura, eds.), vol. 100 of Proceedings of Machine Learning Research, pp. 1094–1100,
PMLR, 2019."
REFERENCES,0.41202672605790647,"[25] J. Arnold and R. Alexander, “Testing autonomous robot control software using procedural content gen-
eration,” in Lecture Notes in Computer Science, Lecture notes in computer science, pp. 33–44, Berlin,
Heidelberg: Springer Berlin Heidelberg, 2013."
REFERENCES,0.4142538975501114,"[26] D. J. Fremont, T. Dreossi, S. Ghosh, X. Yue, A. L. Sangiovanni-Vincentelli, and S. A. Seshia, “Scenic:
a language for scenario speciﬁcation and scene generation,” in Proceedings of the 40th ACM SIGPLAN
Conference on Programming Language Design and Implementation, PLDI 2019, (New York, NY, USA),
p. 63–78, Association for Computing Machinery, 2019."
REFERENCES,0.41648106904231624,"[27] G. E. Mullins, P. G. Stankiewicz, R. C. Hawthorne, and S. K. Gupta, “Adaptive generation of challenging
scenarios for testing and evaluation of autonomous vehicles,” J. Syst. Softw., vol. 137, pp. 197–215, Mar.
2018."
REFERENCES,0.41870824053452116,"[28] Y. Abeysirigoonawardena, F. Shkurti, and G. Dudek, “Generating adversarial driving scenarios in high-
ﬁdelity simulators,” in 2019 International Conference on Robotics and Automation (ICRA), pp. 8271–8277,
IEEE, May 2019."
REFERENCES,0.4209354120267261,"[29] A. Gambi, M. Mueller, and G. Fraser, “Automatically testing self-driving cars with search-based procedural
content generation,” in Proceedings of the 28th ACM SIGSOFT International Symposium on Software
Testing and Analysis, (New York, NY, USA), ACM, July 2019."
REFERENCES,0.42316258351893093,"[30] Y. Zhou, S. Booth, N. Figueroa, and J. Shah, “Rocus: Robot controller understanding via sampling,” in
Proceedings of the 5th Conference on Robot Learning (A. Faust, D. Hsu, and G. Neumann, eds.), vol. 164
of Proceedings of Machine Learning Research, pp. 850–860, PMLR, 08–11 Nov 2022."
REFERENCES,0.42538975501113585,"[31] T. Miconi, “Procedural generation of meta-reinforcement learning tasks,” Feb. 2023."
REFERENCES,0.42761692650334077,"[32] T. Gabor, A. Sedlmeier, M. Kiermeier, T. Phan, M. Henrich, M. Pichlmair, B. Kempter, C. Klein, H. Sauer,
R. S. Ag, and J. Wieghardt, “Scenario co-evolution for reinforcement learning on a grid world smart factory
domain,” in Proceedings of the Genetic and Evolutionary Computation Conference, (New York, NY, USA),
ACM, July 2019."
REFERENCES,0.4298440979955457,"[33] D. M. Bossens and D. Tarapore, “QED: Using quality-environment-diversity to evolve resilient robot
swarms,” IEEE Trans. Evol. Comput., vol. 25, pp. 346–357, Apr. 2021."
REFERENCES,0.43207126948775054,"[34] A. Dharna, J. Togelius, and L. B. Soros, “Co-generation of game levels and game-playing agents,” in Pro-
ceedings of the Sixteenth AAAI Conference on Artiﬁcial Intelligence and Interactive Digital Entertainment,
AIIDE’20, AAAI Press, 2020."
REFERENCES,0.43429844097995546,"[35] R. Wang, J. Lehman, J. Clune, and K. O. Stanley, “Paired open-ended trailblazer (POET): Endlessly
generating increasingly complex and diverse learning environments and their solutions,” arXiv [cs.NE],
Jan. 2019."
REFERENCES,0.4365256124721604,"[36] R. Wang, J. Lehman, A. Rawal, J. Zhi, Y. Li, J. Clune, and K. Stanley, “Enhanced POET: Open-ended
reinforcement learning through unbounded invention of learning challenges and their solutions,” in
Proceedings of the 37th International Conference on Machine Learning (H. D. III and A. Singh, eds.),
vol. 119 of Proceedings of Machine Learning Research, pp. 9940–9951, PMLR, 13–18 Jul 2020."
REFERENCES,0.43875278396436523,"[37] V. Bhatt, B. Tjanaka, M. Fontaine, and S. Nikolaidis, “Deep surrogate assisted generation of environments,”
Advances in Neural Information Processing Systems, vol. 35, pp. 37762–37777, 2022."
REFERENCES,0.44097995545657015,"[38] D. Gravina, A. Khalifa, A. Liapis, J. Togelius, and G. N. Yannakakis, “Procedural content generation
through quality diversity,” in 2019 IEEE Conference on Games (CoG), pp. 1–8, ieeexplore.ieee.org, Aug.
2019."
REFERENCES,0.44320712694877507,"[39] M. C. Fontaine and S. Nikolaidis, “A Quality Diversity Approach to Automatically Generating Human-
robot Interaction Scenarios in Shared Autonomy,” in Robotics: Science and Systems XVII, Virtual Event,
July 12-16, 2021 (D. A. Shell, M. Toussaint, and M. A. Hsieh, eds.), 2021."
REFERENCES,0.44543429844098,"[40] M. C. Fontaine, Y. Hsu, Y. Zhang, B. Tjanaka, and S. Nikolaidis, “On the Importance of Environments
in Human-robot Coordination,” in Robotics: Science and Systems XVII, Virtual Event, July 12-16, 2021
(D. A. Shell, M. Toussaint, and M. A. Hsieh, eds.), 2021."
REFERENCES,0.44766146993318484,"[41] L. Grillotti and A. Cully, “Unsupervised behavior discovery with quality-diversity optimization,” IEEE
Trans. Evol. Comput., 2022."
REFERENCES,0.44988864142538976,"[42] A. Khalifa, P. Bontrager, S. Earle, and J. Togelius, “PCGRL: Procedural Content Generation via Reinforce-
ment Learning,” in Proceedings of the Sixteenth AAAI Conference on Artiﬁcial Intelligence and Interactive
Digital Entertainment, AIIDE 2020, virtual, October 19-23, 2020 (L. Lelis and D. Thue, eds.), pp. 95–101,
AAAI Press, 2020."
REFERENCES,0.4521158129175947,"[43] B. Tjanaka, M. C. Fontaine, Y. Zhang, S. Sommerer, and others, “pyribs: A bare-bones python library for
quality diversity optimization,” 2021."
REFERENCES,0.45434298440979953,Appendix
REFERENCES,0.45657015590200445,"A
Algorithmic details"
REFERENCES,0.45879732739420936,"Algorithm.
The pseudocode below walks through the entire training process for DIVA in abstract.
All of the new components that DIVA introduces is written in green, and all DIVA+ modiﬁcations
are in blue. Original VariBAD training steps are in black, and all inline comments are in orange."
REFERENCES,0.4610244988864143,Algorithm 2 DIVA (detailed)
REFERENCES,0.46325167037861914,"1: # Initialize VariBAD and QD components
2: πφ ←init_policy();
fenc, fdec ←init_vae()
▷Initialize VariBAD components
3: Dπ, DVAE ←init_storage_buffers()
▷Initialize VariBAD buffers
4: Θ0 ←{θi}n0 ∼P(θ)
▷Sample initial solutions from space of valid genotypes
5: F0 = [f(θ1), . . . , f(θn0)] ←compute_features(Θ)
▷Compute env. features
6: J0 = [J(θ1), . . . , J(θn0)] ←compute_objectives(Θ)
▷Compute env. objectives
7: G ←initialize_archive(F0, FS)
▷Init. archive to contain both F0 and target features FS
8: G ←insert_solutions(G, (Θ0, F0, J0))
▷Add random solutions to archive"
REFERENCES,0.46547884187082406,"9: # QD stage 1: discover target region
10: for i in range(NS1) do
11:
G ←qd_update(G, J, M, BQD)
▷Perform QD update (batch size BQD) to populate archive
12:
M ←update_sample_mask(M, G)
▷Move mask gradually towards target region"
REFERENCES,0.46770601336302897,"13: # QD stage 2: populate target region
14: G ←update_archive_bounds(G)
▷Create ﬁnal archive (target region) before S2 updates
15: for i in range(NS2) do
16:
G ←qd_update(G, J, ∅, BQD)
▷Perform QD update to populate target region"
REFERENCES,0.46993318485523383,"17: # Meta-learning over QD archive
18: for i in range(NVB) do
19:
θ′ ∼PG(Θ)
▷Sample solution θ′ from QD archive using approximated target density from FS
20:
M ←generate_environment(θ′)
▷Generate new training environment from solution θ′"
REFERENCES,0.47216035634743875,"21:
# Produce meta-RL rollouts
22:
τ ←perform_policy_rollout(M, πφ)
23:
Dπ, DVAE ←add_to_buffers(Dπ, DVAE, τ)"
REFERENCES,0.47438752783964366,"24:
# Update VAE and policy
25:
fenc, fdec ←varibad_vae_update(fenc, fdec, DVAE)
26:
if after_vae_pretraining() then
27:
varibad_policy_update(πφ, Dπ)"
REFERENCES,0.4766146993318486,"28:
# Perform DIVA+ QD updates
29:
if DIVA+ and (i % qd_update_interval =
0) then
30:
for qd_updates_per_iter do
31:
G ←qd_update(G, JPLR⊥, ∅, BQD)
▷Perform QD update with PLR objective"
REFERENCES,0.47884187082405344,Algorithm 3 QD update
REFERENCES,0.48106904231625836,"1: # Perform a single QD update on archive G with batch size B.
2: function qd_update(G, J, M, B)
3:
˜ΘB×n = [˜θ1, . . . , ˜θB] ←sample_from_emitters(G, M, B)
▷Get mutated batch of solutions
4:
F B×k = [f(˜θ1), . . . , f(˜θB)] ←compute_features( ˜Θ)
▷Compute env. features
5:
J B×1 = [J(˜θ1), . . . , J(˜θB)] ←compute_objectives( ˜Θ)
▷Compute env. objectives
6:
G′ ←add_solutions(G, ( ˜Θ, F , J))
▷Add new solutions to archive if they are elites
7:
return G′"
REFERENCES,0.48329621380846327,"Details on the two-stage QD updates
Here we provide more details on the process described in
Section Section 4. Hyperparameters NS1 and NS2 are set to deﬁne the number of QD updates to
perform in each stage (see Appendix D). In proportion to how many updates in S1 have elapsed, if
the sample mask is enabled, the mask is moved at a linear pace from encapsulating the full S1 archive,
to covering only the target region. We also set a hyperparameter, NSM (see Appendix D), which"
REFERENCES,0.48552338530066813,"speciﬁes the minimum number of solutions which must exist within the mask’s new bounds for it to
be updated. This is to ensure the mask never outpaces the search process. The mask was only found
to be necessary in the ALCHEMY environment. In S1 we sample solutions uniformly from within
the mask. In S2, we begin sampling from the discretized target density distribution approximated
from the downstream feature samples. Two stages are used for RACING as well, since many initial
samples fall outside of the target region, but masking was not found to be necessary. The sample
mask has a relatively straightforward implementation for MAP-Elites, which we use for ALCHEMY’s
discrete genotype (and GRIDNAV, where no mask is required). Since MAP-Elite updates entail
performing mutations on solutions directly sampled from the archive, the mask is implemented to
only consider solutions that fall within the mask bounds. However, since the CMA-ES-based emitter
we use for RACING operates by sampling from a parameterized distribution, instead of sampling from
the archive directly, the mask would need to be applied to these parameters instead of the archive."
REFERENCES,0.48775055679287305,"B
Domain details"
REFERENCES,0.48997772828507796,"B.1
GRIDNAV"
REFERENCES,0.4922048997772829,"GRIDNAV features.
The following features are deﬁned for the GRIDNAV environment:"
REFERENCES,0.49443207126948774,Table 1: GRIDNAV features.
REFERENCES,0.49665924276169265,"Name
Abbr.
Description"
REFERENCES,0.49888641425389757,"XPOSITION
XP
x position of the goal.
YPOSITION
YP
y position of the goal."
REFERENCES,0.5011135857461024,"B.2
ALCHEMY"
REFERENCES,0.5033407572383074,"ALCHEMY features.
We deﬁned the following features for the ALCHEMY environment:"
REFERENCES,0.5055679287305123,Table 2: ALCHEMY features.
REFERENCES,0.5077951002227171,"Name
Abbr.
Description"
REFERENCES,0.5100222717149221,"MANHATTANTOOPTIMAL
MTO
Average Manhattan distance between all stones (across
all trials) to the optimal state.
STONETOSTONEDISTANCE
STSD
Average Euclidean distance between all pairs of stones
(across all trials).
GRAPHNUMBOTTLENECKS
GNB
The number of bottlenecks in the graph topology.
LATENTSTATEDIVERSITY
LSD
The ’diversity’ of the latent stone states (across all trials).
Diversity is calculated as the standard deviation of each
latent state coordinate across all stones.
PARITYFIRSTPOTION
PFP
First potion location (ﬁrst trial, ﬁrst potion), as a parity
measure.
PARITYFIRSTSTONE
PFS
First stone location (ﬁrst trial, ﬁrst stone), as a parity
measure.
POTIONEFFECTDIVERSITY
PED
The ’diversity’ of the potion effects (across all trials).
Diversity is calculated as the standard deviation of each
potion effect coordinate across all potions.
POTIONPERMUTATION
PP
Potion permutation.
POTIONREFLECTION
PR
Potion reﬂection.
STONEREFLECTION
SRE
Stone reﬂection.
STONEROTATION
SRO
Stone rotation.
STONETOSTONEDISTANCEVARIANCE
STSDV
Variance of the distances between stones (across all
trials)."
REFERENCES,0.512249443207127,"Figure 11 contains the feature distributions for the structured and unstructured environment param-
eterizations on ALCHEMY, computed over 100 feature samples. Figure 14 shows the covariance
between feature values for RACING, computed over 100 feature samples."
REFERENCES,0.5144766146993318,"amto
astsd
lsd
pfp
pfs"
REFERENCES,0.5167037861915368,"ped
sre
sro
stsdv"
REFERENCES,0.5189309576837416,Feature Values
REFERENCES,0.5211581291759465,"Density ES
EU"
REFERENCES,0.5233853006681515,Figure 11: ALCHEMY all feature distributions. amto astsd lsd pfp pfs ped sre sro stsdv amto astsd lsd pfp pfs ped sre sro stsdv
REFERENCES,0.5256124721603563,Covariance: ES amto astsd lsd pfp pfs ped sre sro stsdv amto astsd lsd pfp pfs ped sre sro stsdv
REFERENCES,0.5278396436525612,Covariance: EU −1.00 −0.75 −0.50 −0.25 0.00 0.25 0.50 0.75 1.00 −1.00 −0.75 −0.50 −0.25 0.00 0.25 0.50 0.75 1.00
REFERENCES,0.5300668151447662,Figure 12: ALCHEMY measure covariances.
REFERENCES,0.532293986636971,"Archive hyperparameters for ALCHEMY were determined based on some knowledge about the domain,
as well as the feature distributions (Figure 13). We noticed a major deviation between EU and ES in
the feature LATENTSTATEDIVERSITY (LSD), and an even greater on in MANHATTANTOOPTIMAL
(MTO). These two constitute the initial dimensions of the archive, and we found the sample mask
updates to be crucial to reach and ﬁll the target region (see Figure 15). We use PARITYFIRSTSTONE
(PFS) in the second stage to encourage more diversity once the target is reached; it is excluded from
the ﬁrst stage, which is focused on simply reaching the target. The archive for the ﬁrst stage is of
shape [100, 300, 1], corresponding to LSD, MTO, and PFS. The second stage shape is [150, 150, 5].
We found this archive to produce diverse enough solutions, evidenced by the number and spread in the
target region, so we used this setting to train our DIVA agents. The only objective we found useful
for ALCHEMY was a slight bias for newly generated solutions, which we also used for GRIDNAV.
We hypothesize this prevents the archive from getting “stuck” with a suboptimal set of solutions, in
absense of other objectives."
REFERENCES,0.534521158129176,"B.3
RACING"
REFERENCES,0.5367483296213809,"RACING features.
See Table 13 for all features deﬁned on RACING. Figure 13 contains the
feature distributions for the structured and unstructured environment parameterizations on RACING,
computed over 100 feature samples. Figure 14 shows the covariance between feature values for
RACING, computed over 100 samples."
REFERENCES,0.5389755011135857,"Archive hyperparameters for RACING were determined through trial and error, by viewing the samples
produced by the archives at the end of the QD updates, as well as the target coverage metrics. After a
few iterations, it became clear that Total Angle Change TOTALANGLECHANGES (TAC) was the most
useful feature, and so we tried pairing it with a number of others, prioritizing other features with low
absolute covariance (see Figure 14)."
REFERENCES,0.5412026726057907,"The best performing archive used by DIVA on RACING uses TAC and CX as its features, and used a
measure alignment objective over CY and VY. The measure alignment objective rewards solutions for
having measure values over the speciﬁc measures that are similar to the target distribution. We also"
REFERENCES,0.5434298440979956,Table 3: RACING features.
REFERENCES,0.5456570155902004,"Name
Abbr.
Description"
REFERENCES,0.5478841870824054,"AREATOLENGTHRATIO
ATLR
The ratio of enclosed area to curve length.
AVERAGECURVATURE
AC
The average curvature at midpoints of Beziér segments.
CENTEROFMASSX
CX
The center of mass x position over the curve.
CENTEROFMASSY
CY
The center of mass y position over the curve.
CURVEDISTANCESVARIANCE
CDV
The variability in distances between successive points.
CURVELENGTH
CL
The total length of the Beziér curve.
ENCLOSEDAREA
EA
The area enclosed by the Beziér curve.
MEDIANX
MX
The median x position over the curve.
MEDIANY
MY
The median y position over the curve.
SIGNIFICANTANGLECHANGES
SAC
The sum of signiﬁcant angle changes across the curve.
TOTALANGLECHANGES
TAC
The total change in angle across the curve.
TOTALCURVATURE
TC
The total curvature over each segment and sum them up.
VARIANCEX
VX
The variance of the x positions over the curve.
VARIANCEY
VY
The variance of the y positions over the curve."
REFERENCES,0.5501113585746102,"atlr
ac
cx
cy
cdv
cl
ea"
REFERENCES,0.5523385300668151,"mx
my
sac
tac
tc
vx
vy"
REFERENCES,0.5545657015590201,Feature Values
REFERENCES,0.5567928730512249,"Density ES
EU"
REFERENCES,0.5590200445434298,Figure 13: RACING all feature distributions.
REFERENCES,0.5612472160356348,"found that randomly sampling these objective values according to the target distribution provided
some additional support in covering the target region efﬁciently."
REFERENCES,0.5634743875278396,"The slightly “misspeciﬁed” archive was chosen because its solutions generated some diversity, but
not as much as the aforementioned one. This archive uses TAC and ATLR as its features, and uses a
measure diversity objective over just CY. Instead of prioritizing alignment to the target distribution,
the diversity objective samples a handful of solutions from the archive, and uses the current solutions
deviation from these as its objective."
REFERENCES,0.5657015590200446,We use ﬁnal archive dimensions of 500 × 500 for both S1 and S2.
REFERENCES,0.5679287305122495,"C
Ablation analysis"
REFERENCES,0.5701559020044543,"Sample mask ablation.
Figure 15 shows the beneﬁt of updating the sample mask bounds during
the ﬁrst archive ﬁlling stage on ALCHEMY. Not only does this approach produce signiﬁcantly more
total archive solutions, but more importantly, progress towards ﬁlling the target region speciﬁcally is
accelerated."
REFERENCES,0.5723830734966593,"D
Hyperparameter sensitivity analysis"
REFERENCES,0.5746102449888641,"Varying QD mutation rate.
We perform an ablation on the QD mutation rate, which is the
probability that a given gene will be mutated (for MAP-Elites). We perform this ablation on
ALCHEMY because the its search is the most challenging of the three environments we consider
(it is the sole environment that required a longer S1 and the sample mask trick for accelerating to
accelerate the search). We see from Figure 16 that ALCHEMY results are not very sensitive to the
setting of the mutation rate. atlr ac cx cy cdv cl ea mx my sac tac tc vx vy atlr ac cx cy cdv cl ea mx my sac tac tc vx vy"
REFERENCES,0.576837416481069,Covariance: ES atlr ac cx cy cdv cl ea mx my sac tac tc vx vy atlr ac cx cy cdv cl ea mx my sac tac tc vx vy
REFERENCES,0.579064587973274,Covariance: EU −1.00 −0.75 −0.50 −0.25 0.00 0.25 0.50 0.75 1.00 −1.00 −0.75 −0.50 −0.25 0.00 0.25 0.50 0.75 1.00
REFERENCES,0.5812917594654788,Figure 14: RACING measure covariances.
REFERENCES,0.5835189309576837,"0
1
2
3
# QD Updates (S1) ×104 0.0 0.5 1.0 1.5 2.0"
REFERENCES,0.5857461024498887,# of Solutions
REFERENCES,0.5879732739420935,×103 # Target Solutions
REFERENCES,0.5902004454342984,"0
1
2
3
# QD Updates (S1) ×104 0 1 2 3 4"
REFERENCES,0.5924276169265034,# of Solutions
REFERENCES,0.5946547884187082,×103 # Archive Solutions
REFERENCES,0.5968819599109132,"Masking
No masking"
REFERENCES,0.5991091314031181,"Figure 15: ALCHEMY sample mask ablation curves. This speciﬁc result is the result of two seeds
instead of ﬁve, as we found the variance to be very low for this ablation (validated across other
parameter settings)."
REFERENCES,0.6013363028953229,"Varying number of QD updates.
We perform a similar ablation to test how robust DIVA is to the
number of QD updates performed. We see from Figure 17 that ALCHEMY results suffer somewhat
from fewer updates (e.g. for only 10k in each stage), still signiﬁcantly outperform baselines in
each case. The trend is clear, however: more QD updates produces more solutions, which generally
translates to better performance, even if slightly."
REFERENCES,0.6035634743875279,"Varying number of downstream samples.
Next we test how robust DIVA is to the number of
downstream samples used to compute the target distribution. In Figure 18 we see that, despite the
errors increasing with fewer samples, DIVA still signiﬁcantly outperforms baselines with as few as
ﬁve samples."
REFERENCES,0.6057906458797327,"E
Training details"
REFERENCES,0.6080178173719376,"E.1
DIVA hyperparameters"
REFERENCES,0.6102449888641426,Table 4 displays the hyperparameters used for DIVA across all domains.
REFERENCES,0.6124721603563474,"A note on NTRS computation
The initial QD population (n0) is implemented such that the ﬁrst set
of QD updates simply generates n0 random levels from EU, before performing the actual mutations
(for ME) or intelligent sampling (for ES). Thus, the formula we use for computing NTRS, the total
reset steps provided to DIVA (see Table 4), which we use to compare the extra steps we provide
PLR⊥and ACCEL (discussed in Section 5), does not include n0; it is simply the product of the batch
size and the total number of QD iterations."
REFERENCES,0.6146993318485523,"0.005
0.01
0.04
0.08
Mutation Rate 0 2 4 6"
REFERENCES,0.6169265033407573,Final Episode Return
REFERENCES,0.6191536748329621,Final Return by Mutation Rate
REFERENCES,0.621380846325167,Method
REFERENCES,0.623608017817372,"DIVA
∗ODS
PLR⊥"
REFERENCES,0.6258351893095768,"0.005
0.01
0.04
0.08
Mutation Rate 0 2000 4000 6000 8000"
REFERENCES,0.6280623608017817,# Target Solutions
REFERENCES,0.6302895322939867,# Target Solutions by Mutation Rate
REFERENCES,0.6325167037861915,"Figure 16: Effect of varying QD mutation rate in ALCHEMY. Left: The returns for the ﬁnal
episode by mutation rate, after training on archives produced with each mutation rate. Right: The
ﬁnal number of solutions in the archive after performing QD updates with each mutation rate. This
result was produced by running three different seeds for each mutation rate."
REFERENCES,0.6347438752783965,"10000
20000
40000
120000
# QD Updates 0 2 4 6"
REFERENCES,0.6369710467706013,Final Episode Returns
REFERENCES,0.6391982182628062,(log scale)
REFERENCES,0.6414253897550112,Returns by # QD Updates
REFERENCES,0.643652561247216,Method
REFERENCES,0.6458797327394209,"DIVA
∗ODS
PLR⊥"
REFERENCES,0.6481069042316259,"10000
20000
40000
120000
# QD Updates 0.00 0.25 0.50 0.75 1.00 1.25"
REFERENCES,0.6503340757238307,# Target Solutions ×104
REFERENCES,0.6525612472160356,(log scale)
REFERENCES,0.6547884187082406,# Target Solutions by # QD Updates
REFERENCES,0.6570155902004454,"Figure 17: Effect of varying the number of QD updates in ALCHEMY. Left: The returns for
the ﬁnal episode by number of QD updates in each stage (NS1 = NS2). Right: The ﬁnal number of
solutions in the archive after performing each number of QD updates. This result was produced by
running three different seeds for each setting."
REFERENCES,0.6592427616926503,"E.2
VariBAD hyperparameters"
REFERENCES,0.6614699331848553,Table 5 displays the hyperparameters used for VariBAD across all domains.
REFERENCES,0.6636971046770601,"E.3
Baseline hyperparameters"
REFERENCES,0.6659242761692651,"E.3.1
PLR⊥"
REFERENCES,0.6681514476614699,Table 6 displays the hyperparameters used for PLR⊥across all domains.
REFERENCES,0.6703786191536748,"E.3.2
ACCEL"
REFERENCES,0.6726057906458798,"ACCEL uses the same hyperparameters as PLR⊥(see Table 6), combined with the same evolutionary
hyperparameters used for DIVA’s QD archive (see Table 4)."
REFERENCES,0.6748329621380846,"E.4
Computational details"
REFERENCES,0.6770601336302895,"All results were produced on a handful of Titan X or Xp GPUs. Environments were parallelized
across multiple CPU cores to accelerate training. While the experiment time varies by method and
environment, most experiments take less than a day to run to completion. PLR⊥and ACCEL take the
longest, as they required twice as many environment steps as the other methods—on the two latter
domains, these methods take well over a day to run to completion."
REFERENCES,0.6792873051224945,"5
10
25
50
100
500
# Feature Samples 0 2 4 6"
REFERENCES,0.6815144766146993,Final Episode Returns
REFERENCES,0.6837416481069042,(log scale)
REFERENCES,0.6859688195991092,Returns by # Feature Samples
REFERENCES,0.688195991091314,Method
REFERENCES,0.6904231625835189,"DIVA
∗ODS
PLR⊥"
REFERENCES,0.6926503340757239,"5
10
25
50
100
500
# Feature Samples 2 4 6"
REFERENCES,0.6948775055679287,Sample Means MAE
REFERENCES,0.6971046770601337,"×10−2
AverageManhattanToOptimal"
REFERENCES,0.6993318485523385,"Sample Means MAE
Sample Variances MAE 0.5 1.0 1.5 2.0 2.5 3.0"
REFERENCES,0.7015590200445434,Sample Variances MAE ×10−2
REFERENCES,0.7037861915367484,(log scale)
REFERENCES,0.7060133630289532,"5
10
25
50
100
500
# Feature Samples 0.5 1.0 1.5 2.0"
REFERENCES,0.7082405345211581,Sample Means MAE
REFERENCES,0.7104677060133631,"×10−2
LatentStateDiversity 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7126948775055679,Sample Variances MAE ×10−2
REFERENCES,0.7149220489977728,(log scale)
REFERENCES,0.7171492204899778,"Figure 18: Effect of varying number of samples in ALCHEMY. Left: DIVA evaluation returns
for the ﬁnal episode by number of downstream samples, after training on archives produced with by
using each number of samples to produce archive bounds and prior. Center/Right: Errors for mean
and variance parameters of the normal distribution based on number of samples used for computation;
for MANHATTANTOOPTIMAL and LATENTSTATEDIVERSITY. For all plots, ﬁve seeds were used
for each hyperparameter setting."
REFERENCES,0.7193763919821826,Table 4: DIVA hyperparameter settings.
REFERENCES,0.7216035634743875,"Name
Description
Value"
REFERENCES,0.7238307349665924,"NS1
Number of QD updates in stage S1
0† / 80,000⋆/ 50,000⋄"
REFERENCES,0.7260579064587973,"NS2
Number of QD updates in stage S2
100,000† / 30,000⋆/ 200,000⋄"
REFERENCES,0.7282850779510023,"NQD
(Effective) total QD updates: NS1 + NS2
100,000† / 110,000⋆/ 250,000⋄"
REFERENCES,0.7305122494432071,"n0
Initial population size
1,000†⋆/ 2,000⋄"
REFERENCES,0.732739420935412,"ne
Number of QD solution emitters
5
Be
Sampling batch size of each QD emitter
8†⋄/ 5⋆"
REFERENCES,0.734966592427617,"BQD
(Effective) QD batch size per update (ne × Be)
40†⋄/ 25⋆"
REFERENCES,0.7371937639198218,"NTRS
(Effective) total reset steps (NQD × BQD)
4.0 × 106 † / 2.75 × 106 ⋆/ 1.0 × 107 ⋄"
REFERENCES,0.7394209354120267,"qd_emitter
Type of QD emitter
MAP-Elites (ME)†⋆/ CMA-ES (CMA) ⋄"
REFERENCES,0.7416481069042317,"pME
Mutation percentage for ME emitter
0.1† / 0.02⋆"
REFERENCES,0.7438752783964365,"σES
Initial sigma for ES emitter
0.1⋄"
REFERENCES,0.7461024498886414,"ABSM
Anneal sample mask bounds during S1
False† / True⋆⋄"
REFERENCES,0.7483296213808464,"NSM
Minimum solutions in sample mask
40⋆/ 1000⋄"
REFERENCES,0.7505567928730512,"Jnew
Enable objective for slightly biasing new solutions
True †⋆/ False ⋄"
REFERENCES,0.7527839643652561,"JMD
Enable measure diversity objective
False
JMA
Enable measure alignment objective
False †⋆/ True ⋄"
REFERENCES,0.755011135857461,"JRnd
Enable randomize objective
False †⋆/ True ⋄"
REFERENCES,0.7572383073496659,"NE
Total meta-training env. steps (ref. from Table 5)
4.8 × 106 †⋆/ 2.0 × 107 ⋄"
REFERENCES,0.7594654788418709,"NE+
Additional env. steps provided to UED baselines
9.6 × 106 †⋆/ 4.0 × 107 ⋄"
REFERENCES,0.7616926503340757,"NE′
(Effective) total UED env. steps
14.4 × 106 †⋆/ 6.0 × 107 ⋄"
REFERENCES,0.7639198218262806,"NE+/NTRS
(Effective) Ratio of add. steps for UED vs DIVA
2.4† / 3.5⋆/ 4.0⋄"
REFERENCES,0.7661469933184856,"†GRIDNAV, ⋆ALCHEMY, ⋄RACING"
REFERENCES,0.7683741648106904,Table 5: VariBAD hyperparameter settings.
REFERENCES,0.7706013363028953,"Name
Description
Value"
REFERENCES,0.7728285077951003,"ϵπ
Optimizer epsilon for policy
1e-8
γ
Discount factor for rewards
0.99
policy_state_emb_dim
State embedding dimension for policy
64
policy_latent_emb_dim
Latent embedding dimension for policy
64
policy_norm_state
Normalize state input
True
policy_norm_latent
Normalize latent input
True
policy_norm_belief
Normalize belief input
True
policy_norm_rew
Normalize rewards for policy
True† / False⋆⋄"
REFERENCES,0.7750556792873051,"policy_layers
Hidden layers for policy network
[128, 128]†⋆/ [128]⋄"
REFERENCES,0.77728285077951,"policy_activation
Activation function for policy
tanh†⋆"
REFERENCES,0.779510022271715,"policy_init_method
Initialization method for policy
normc†⋆"
REFERENCES,0.7817371937639198,"policy_optimizer
Optimizer for policy
adam
policy_lr
Learning rate for policy
0.0007
policy_init_sd
Initial standard deviation for policy
1.0
policy_val_loss_coef
Value loss coefﬁcient for policy
0.5
policy_entropy_coef
Entropy coefﬁcient for policy
0.01
policy_use_gae
Use generalized advantage estimation
True
policy_tau
GAE parameter for policy
0.95
policy_max_grad_norm
Maximum gradient norm for policy
0.5"
REFERENCES,0.7839643652561247,"NVB
Total number of VariBAD learning updates
1.0 × 104 †⋄/ 1.5 × 104 ⋆"
REFERENCES,0.7861915367483296,"policy_num_steps
Number of environment steps per update
60† / 40⋆/ 500⋄"
REFERENCES,0.7884187082405345,"num_processes
Number of parallel environments
8†⋆/ 4⋄"
REFERENCES,0.7906458797327395,"NE
Total env. steps (product of prior three)
4.8 × 106 †⋆/ 2.0 × 107 ⋄"
REFERENCES,0.7928730512249443,"ppo_num_epochs
Number of epochs per PPO update
2†⋆/ 8⋄"
REFERENCES,0.7951002227171492,"ppo_num_minibatch
Number of minibatches for PPO
4
ppo_huber_loss
Use Huber loss for PPO
True
ppo_clip_val_loss
Use clipped value loss for PPO
True
ppo_clip_param
Clipping parameter for PPO
0.05†⋆/ 0.01⋄"
REFERENCES,0.7973273942093542,"αVAE
Learning rate for VAE
0.001
NVAE
Size of VAE buffer
5,000† / 100,000⋆/ 1,000⋄"
REFERENCES,0.799554565701559,"BVAE
Number of trajectories per VAE update
25†⋆/ 10⋄"
REFERENCES,0.8017817371937639,"precollect_len
Frames to pre-collect before training
5,000
num_vae_updates
Number of VAE update steps per iteration
3
pretrain_len
Number of VAE pre-training updates
0
kl_weight
Weight for KL term
0.01
action_emb_size
Action embedding size for VAE
8
state_emb_size
State embedding size for VAE
16
rew_emb_size
Reward embedding size for VAE
16
enc_gru_hidden_size
GRU hidden size in encoder
128
latent_dim
Latent dimension for VAE
10†⋆/ 5⋄"
REFERENCES,0.8040089086859689,"rew_loss_coeff
Reward loss coefﬁcient
1.0
rew_dec_layers
Layers for reward decoder
[64, 32]
rew_multihead
Use multihead for reward prediction
False
rew_pred_type
Reward prediction type
bernoulli
kl_to_gaus_prior
KL term to Gaussian prior
False
rl_loss_thru_enc
Backprop RL loss through encoder
False
vae_loss_coef
VAE loss coefﬁcient
1.0"
REFERENCES,0.8062360801781737,"†GRIDNAV, ⋆ALCHEMY, ⋄RACING"
REFERENCES,0.8084632516703786,Table 6: PLR hyperparameter settings.
REFERENCES,0.8106904231625836,"Name
Description
Value"
REFERENCES,0.8129175946547884,"NPLR
Number of levels to store in our buffer
45† / 112,500⋆/ 8,000⋄"
REFERENCES,0.8151447661469933,"fPLR
Level replay score transform function
power
βS
Level replay temperature start
1.0
βE
Level replay temperature end
1.0
score(τ, π)
Level replay scoring function
Positive value loss
ϵPLR
Level replay epsilon for eps-greedy sampling
0.05
preplay
Probability of sampling replay vs. new level
0.5
αPLR
Level score EWA smoothing factor
0.0"
REFERENCES,0.8173719376391982,"ρC
Staleness coefﬁcient
0.7
fC
Staleness normalization transform
power
βC
Staleness normalization temperature
1.0"
REFERENCES,0.8195991091314031,"†GRIDNAV, ⋆ALCHEMY, ⋄RACING"
REFERENCES,0.821826280623608,NeurIPS Paper Checklist
CLAIMS,0.8240534521158129,1. Claims
CLAIMS,0.8262806236080178,"Question: Do the main claims made in the abstract and introduction accurately reﬂect the
paper’s contributions and scope?
Answer: [Yes]
Justiﬁcation: The two main claims in the abstract are as follows: “Our empirical results
demonstrate DIVA’s [1] unique ability to leverage ill-parameterized simulators to train
adaptive behavior in meta-RL agents, [2] far outperforming competitive baselines."" (1):
This ability is indeed “unique”, as far as the authors are aware, and is supported by the
discussion of related works in Section 6. (2): The empirical results in this paper presented in
Section 5 support the claim that DIVA far outperforms other baselines for training meta-RL
agents. Speciﬁc pieces of evidence include the GRIDNAV results in Figure 4, the ALCHEMY
results in Figure 5, and the results in RACING, contained in Figure 7, 9, and 10. The ﬁnal
sentence in the abstract makes a more general, weaker claim about the presented method’s
potential: “These ﬁndings highlight the potential of approaches like DIVA to enable training
in complex open-ended domains, and to produce more robust and adaptable agents."" The
authors believe the same set of evidence supports this claim as well, but is ultimately up to
the reader—and more importantly, future work—to decide.
Guidelines:"
CLAIMS,0.8285077951002228,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reﬂect how
much the results can be expected to generalize to other settings.
• It is ﬁne to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2. Limitations"
CLAIMS,0.8307349665924276,"Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justiﬁcation: The authors discuss limitations of the present work in Section 7. This Dis-
cussion section covers (1) key takeaways from the paper, (2) limitations, and (3) promising
avenues for future work. The authors combine these sections because the points are all
interrelated, and having them in the same place preserves cohesion and ﬂow of the writing.
Guidelines:"
CLAIMS,0.8329621380846325,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-speciﬁcation, asymptotic approximations only holding locally). The authors
should reﬂect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reﬂect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reﬂect on the factors that inﬂuence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efﬁciency of the proposed algorithms
and how they scale with dataset size."
CLAIMS,0.8351893095768375,"• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be speciﬁcally instructed to not penalize honesty concerning limitations."
THEORY ASSUMPTIONS AND PROOFS,0.8374164810690423,3. Theory Assumptions and Proofs
THEORY ASSUMPTIONS AND PROOFS,0.8396436525612472,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?"
THEORY ASSUMPTIONS AND PROOFS,0.8418708240534521,Answer: [NA]
THEORY ASSUMPTIONS AND PROOFS,0.844097995545657,"Justiﬁcation: The paper does not include any formalized theoretical results. The authors did
not ﬁnd reason to provide any theorems, formulas, or proofs to support the claims made."
THEORY ASSUMPTIONS AND PROOFS,0.8463251670378619,Guidelines:
THEORY ASSUMPTIONS AND PROOFS,0.8485523385300668,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8507795100222717,4. Experimental Result Reproducibility
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8530066815144766,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8552338530066815,"Answer: [Yes]
Justiﬁcation: In addition to the provided code (see Section 8), which is self-contained
and includes documentation for reproducing all empirical results in this paper, the authors
additionally include all training details, including hyperparameter settings for all methods,
in Appendix E."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8574610244988864,Guidelines:
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8596881959910914,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or veriﬁable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might sufﬁce, or if the contribution is a speciﬁc model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8619153674832962,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results."
OPEN ACCESS TO DATA AND CODE,0.8641425389755011,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.8663697104677061,"Question: Does the paper provide open access to the data and code, with sufﬁcient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
OPEN ACCESS TO DATA AND CODE,0.8685968819599109,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.8708240534521158,"Justiﬁcation: The code is available and well-documented (see Section 8), and contains
sufﬁcient instructions to faithfully reproduce the main experimental results. Additionally,
all necessary details required to independently reproduce the results are self contained in the
paper, with hyperparameter settings and other details fully described in Appendix E."
OPEN ACCESS TO DATA AND CODE,0.8730512249443207,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.8752783964365256,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted."
OPEN ACCESS TO DATA AND CODE,0.8775055679287305,6. Experimental Setting/Details
OPEN ACCESS TO DATA AND CODE,0.8797327394209354,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
OPEN ACCESS TO DATA AND CODE,0.8819599109131403,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.8841870824053452,"Justiﬁcation: See Appendix E for all relevant training and test details not covered in the
main body. Additionally, details about each evaluation domain, if likewise not covered in
the main body, are available in Appendix B. Lastly, all of these details are self-contained
and documented in the code (see Section 8)."
OPEN ACCESS TO DATA AND CODE,0.8864142538975501,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.888641425389755,• The answer NA means that the paper does not include experiments.
OPEN ACCESS TO DATA AND CODE,0.89086859688196,"• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material."
OPEN ACCESS TO DATA AND CODE,0.8930957683741648,7. Experiment Statistical Signiﬁcance
OPEN ACCESS TO DATA AND CODE,0.8953229398663697,"Question: Does the paper report error bars suitably and correctly deﬁned or other appropriate
information about the statistical signiﬁcance of the experiments?"
OPEN ACCESS TO DATA AND CODE,0.8975501113585747,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.8997772828507795,"Justiﬁcation: All results (excepting visualizations, and the trend curve in Figure 4 (a), for
which they are not needed) are accompanied with error bars that capture the factors of
variability, mostly due to random seeding (affecting many different random settings of the
algorithm, e.g. initial model weights, sampling, etc.). The signiﬁcance of all error bars, and
number of seeds used for each experiment, are detailed in Section 5. Other training details
are available in Appendix E."
OPEN ACCESS TO DATA AND CODE,0.9020044543429844,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.9042316258351893,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, conﬁ-
dence intervals, or statistical signiﬁcance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not veriﬁed.
• For asymmetric distributions, the authors should be careful not to show in tables or
ﬁgures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding ﬁgures or tables in the text."
EXPERIMENTS COMPUTE RESOURCES,0.9064587973273942,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.9086859688195991,"Question: For each experiment, does the paper provide sufﬁcient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.910913140311804,Answer: [Yes]
EXPERIMENTS COMPUTE RESOURCES,0.9131403118040089,"Justiﬁcation: All compute resources (workers, memory, time of execution) are detailed in
Appendix E.4."
EXPERIMENTS COMPUTE RESOURCES,0.9153674832962138,Guidelines:
EXPERIMENTS COMPUTE RESOURCES,0.9175946547884187,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper)."
CODE OF ETHICS,0.9198218262806236,9. Code Of Ethics
CODE OF ETHICS,0.9220489977728286,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?"
CODE OF ETHICS,0.9242761692650334,Answer: [Yes]
CODE OF ETHICS,0.9265033407572383,"Justiﬁcation: The authors have read and understood the NeurIPS Code of Ethics, and the
paper conforms to the code in every respect."
CODE OF ETHICS,0.9287305122494433,Guidelines:
CODE OF ETHICS,0.9309576837416481,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction)."
BROADER IMPACTS,0.933184855233853,10. Broader Impacts
BROADER IMPACTS,0.9354120267260579,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?"
BROADER IMPACTS,0.9376391982182628,Answer: [Yes]
BROADER IMPACTS,0.9398663697104677,"Justiﬁcation: The authors speciﬁcally discuss the broader impacts of the present work in
Section 9."
BROADER IMPACTS,0.9420935412026726,Guidelines:
BROADER IMPACTS,0.9443207126948775,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake proﬁles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact speciﬁc
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efﬁciency and accessibility of ML)."
SAFEGUARDS,0.9465478841870824,11. Safeguards
SAFEGUARDS,0.9487750556792873,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?"
SAFEGUARDS,0.9510022271714922,Answer: [NA]
SAFEGUARDS,0.9532293986636972,"Justiﬁcation: The authors feel that such safeguards are unnecessary for the present work,
as the risk for misuse is estimated by the authors to be very low—it is unclear how this
work and its artifacts can be directly used for malicious purposes. The authors are willing to
adjust this position if members of the community feel otherwise."
SAFEGUARDS,0.955456570155902,Guidelines:
SAFEGUARDS,0.9576837416481069,• The answer NA means that the paper poses no such risks.
SAFEGUARDS,0.9599109131403119,"• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety ﬁlters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort."
LICENSES FOR EXISTING ASSETS,0.9621380846325167,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.9643652561247216,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
LICENSES FOR EXISTING ASSETS,0.9665924276169265,Answer: [Yes]
LICENSES FOR EXISTING ASSETS,0.9688195991091314,"Justiﬁcation: All used resources are either cited properly in the paper and/or are documented
and credited in the codebase (see Section 8)."
LICENSES FOR EXISTING ASSETS,0.9710467706013363,Guidelines:
LICENSES FOR EXISTING ASSETS,0.9732739420935412,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators."
NEW ASSETS,0.9755011135857461,13. New Assets
NEW ASSETS,0.977728285077951,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?"
NEW ASSETS,0.9799554565701559,Answer: [Yes]
NEW ASSETS,0.9821826280623608,"Justiﬁcation: The new asset introduced in this paper is the codebase (see Section 8), which
is well-documented for the purpose of reproducibility, and contains details about training,
the license, and limitations, etc."
NEW ASSETS,0.9844097995545658,Guidelines:
NEW ASSETS,0.9866369710467706,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip ﬁle."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9888641425389755,14. Crowdsourcing and Research with Human Subjects
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9910913140311804,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9933184855233853,"Answer: [NA]
Justiﬁcation: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9955456570155902,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is ﬁne, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justiﬁcation: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9977728285077951,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary signiﬁcantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
