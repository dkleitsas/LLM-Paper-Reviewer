Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003003003003003003,"Deep neural networks (DNNs) often rely on easy–to–learn discriminatory fea-
tures, or cues, that are not necessarily essential to the problem at hand. For ex-
ample, ducks in an image may be recognized based on their typical background
scenery, such as lakes or streams. This phenomenon, also known as shortcut
learning, is emerging as a key limitation of the current generation of machine
learning models. In this work, we introduce a set of experiments to deepen our
understanding of shortcut learning and its implications. We design a training setup
with several shortcut cues, named WCST-ML, where each cue is equally conducive
to the visual recognition problem at hand. Even under equal opportunities, we
observe that (1) certain cues are preferred to others, (2) solutions biased to the
easy–to–learn cues tend to converge to relatively ﬂat minima on the loss surface,
and (3) the solutions focusing on those preferred cues are far more abundant in the
parameter space. We explain the abundance of certain cues via their Kolmogorov
(descriptional) complexity: solutions corresponding to Kolmogorov-simple cues
are abundant in the parameter space and are thus preferred by DNNs. Our stud-
ies are based on the synthetic dataset DSprites and the face dataset UTKFace. In
our WCST-ML, we observe that the bias of models leans toward simple cues, such
as color and ethnicity. Our ﬁndings emphasize the importance of active human
intervention to remove the model biases that may cause negative societal impacts."
INTRODUCTION,0.006006006006006006,"1
INTRODUCTION"
INTRODUCTION,0.009009009009009009,"Emerging studies on the inner mechanisms of deep neural networks (DNNs) have revealed that
many models have shortcut biases (Cadene et al., 2019; Weinzaepfel & Rogez, 2021; Bahng et al.,
2020; Geirhos et al., 2020). DNNs often pick up simple, non-essential cues, which are nonetheless
effective within a particular dataset. For example, a DNN trained for the task of animal recognition
may recognize ducks while attending on water backgrounds, given the strong correlations between
such background cues and the target label (Choe et al., 2020). These shortcut biases often result
in a striking qualitative difference between human and machine recognition systems; for example,
convolutional neural networks (CNNs) trained on ImageNet extensively rely on texture features,
while humans would preferentially look at the global shape of objects (Geirhos et al., 2019). In
other cases, the shortcut bias arises in models that suppress certain streams of inputs: visual question
answering (VQA) models often neglect the entire image cues, for one does not require images to
answer questions like “what color is the banana in the image?” (Cadene et al., 2019)"
INTRODUCTION,0.012012012012012012,"Since DNNs have successfully outperformed humans on many tasks (Silver et al., 2016; Rajpurkar
et al., 2017), such a phenomenon may look benign at face value. However, the shortcut biases
become problematic when it comes to the generalization to more challenging test-time conditions,
where the shortcuts are no longer valid (Cadene et al., 2019; Weinzaepfel & Rogez, 2021; Bahng
et al., 2020; Geirhos et al., 2020). These biases also cause ethical concerns when the shortcut features
adopted by a model are sensitive like gender or skin color (Wang et al., 2019; Xu et al., 2020)."
INTRODUCTION,0.015015015015015015,"Instead of proposing a method or solution, this work focuses on deepening our understanding of the
shortcut bias phenomena. In particular, we design a dataset where multiple cues are equally valid"
INTRODUCTION,0.018018018018018018,∗First two authors contributed equally.
INTRODUCTION,0.021021021021021023,Predictions on oﬀ-diagonal cells Scale Shape Color Scale Shape Color
INTRODUCTION,0.024024024024024024,"Diagonal Training Data
Unbiased Testing Data X"
INTRODUCTION,0.02702702702702703,"Y
1
2
3 f
1 2 3 scale shape"
INTRODUCTION,0.03003003003003003,"color
X"
INTRODUCTION,0.03303303303303303,lets us infer the cue adopted by f .
INTRODUCTION,0.036036036036036036,Train f on the diagonal dataset.
INTRODUCTION,0.03903903903903904,"pred
adopted cue ( ( ( ( ( ( ( ( = ("
INTRODUCTION,0.042042042042042045,"Figure 1: Wisconsin Card Sorting Test for Machine Learners (WCST-ML). The training dataset (left) poses
equally plausible cues to the DNN: color, shape, and scale. Which cue will a model f choose to use, when
given such training data? By examining f’s output on unbiased off-diagonal samples at test time (middle), one
may discover the cue f has adopted. In the example: 1-scale, 2-shape, and 3-color."
INTRODUCTION,0.04504504504504504,"for solving a particular task and observe which cues tend to be preferentially adopted by DNNs.
The experimental setup is inspired by the Wisconsin Card Sorting Test (WCST, Banno et al. (2012))
in Cognitive Neuroscience1. See Figure 1 for an illustration of the setup. It consists of a training
set with multiple highly correlated cues (e.g. color, shape, and scale) that offer equally plausible
pathways to the successful target prediction (Y ∈{1, 2, 3}). We call this a diagonal training set,
highlighting the spatial arrangement of such samples in the product space of all combinations. A
model f trained on such a dataset will adopt or neglect certain cues. We analyse the cues adopted by
a model by observing its predictions on off-diagonal samples. With regards to Figure 1, for example,
consider the f’s prediction for a small, blue triangle as an off-diagonal sample. The prediction values
f(
) ∈{1, 2, 3} tell us which cue the model is biased towards to, e.g. if f(
) = 1, then f is biased
towards scale; if f(
) = 2, towards shape; and if f(
) = 3 towards color. All three scenarios are
plausible and only testing on off-diagonal samples will reveal model bias."
INTRODUCTION,0.04804804804804805,"We make important observations on the nature of shortcut bias under WCST-ML. We discover that,
despite the equal amounts of correlations with the target label, there tends to be a preferential order-
ing of cues. The preference is largely shared across different DNN architectures, such as feedforward
networks, ResNets (He et al., 2015), Vision Transformers (Dosovitskiy et al., 2021), and multiple
initial parameters. From the parameter-space perspective, we further observe that the set of solutions
Θp biased to the preferred cues takes a far greater volume than those corresponding to the averted
cues Θa. The loss landscape also tends to be ﬂatter around Θp than around Θa."
INTRODUCTION,0.05105105105105105,"Why are certain cues preferred to others by general DNNs? We provide an explanation based on the
Kolmogorov complexity of cues, which measures the minimal description length for representing
cues (Kolmogorov, 1963). Prior studies have shown that in the parameter space of generic DNNs,
there are exponentially more Kolmogorov-simple functions than Kolmogorov-complex ones (Valle-
Perez et al., 2019; De Palma et al., 2019). Based on these theoretical results, we argue that DNNs
are naturally drawn to Kolmogorov-simple cues. We empirically verify that the preferences for cues
correlate well with their Kolmogorov complexity estimates."
INTRODUCTION,0.05405405405405406,"What are the consequences of the inborn preference for simple cues? Firstly, this may hinder the
generalization of DNNs to challenging test scenarios where the simple shortcut cues are no longer
valid (Geirhos et al., 2020). Secondly, we expose the possibility that certain protected attributes cor-
respond to the simple shortcut cue for the task at hand, endangering the fairness of DNNs (Barocas
et al., 2017). In such a case, human intervention on the learning procedure may be necessary to
enforce fairness, for the dataset and DNNs can be naturally drawn to exploit protected attributes."
INTRODUCTION,0.057057057057057055,"The primary goal of this manuscript is to shed light on the nature of shortcut biases and the underly-
ing mechanisms behind the scenes. Our contributions are summarized as follows: an experimental
setup for studying the shortcut bias in-depth (WCST-ML) (§2); novel observations on the nature of
shortcut biases, such as the existence of preferential ordering of cues and its connections to the ge-
ometry of the loss landscape in the parameter space (§3); an explanation based on the descriptional"
INTRODUCTION,0.06006006006006006,"1Original WCST gauges the subjects’ cognitive ability to ﬂexibly shift their underlying rules (adopted cues)
for categorizing samples. Inability to do so may indicate dysfunctional frontal lobe activities."
INTRODUCTION,0.06306306306306306,"complexity of cues (§4); and a discussion on the implications on generalization and fairness (§5),
such as the preferential use of ethnical features for face recognition on the UTKFace dataset."
SETUP,0.06606606606606606,"2
SETUP"
SETUP,0.06906906906906907,"We introduce the setup that will provide the basis for the analysis in this paper. We describe the pro-
cedure for building a dataset with multiple equally valid cues for recognition (§2.1). The procedure
is applied to DSprites and UTKFace datasets in §2.2. In §2.3, we introduce terminologies for the
analysis of the parameter space and make theoretical connections with our data framework."
SETUP,0.07207207207207207,"2.1
DATA FRAMEWORK: WCST-ML"
SETUP,0.07507507507507508,"Many factors affect the preference of models to certain cues. The existence of dominant classes is
an example; it encourages models to favor cues conducive to a good performance on the dominant
classes (Barocas et al., 2017; Hashimoto et al., 2018). In other cases, some cues have higher degrees
of correlation with the target label (Geirhos et al., 2020). In this work, we test whether bias is still
present under fair conditions, i.e.: when a training dataset contains a set of valid cues, each of which
equally correlates with the targets, will DNNs still have a preference for certain cues? If so, why?"
SETUP,0.07807807807807808,"To study this, we introduce a data construction framework called Wisconsin Card Sorting Test for
Machine Learners (WCST-ML), named after a clinical test in cognitive neuroscience (Banno et al.,
2012). See Figure 1 for an overview. As a running example, we assume a dataset where each
image can be described by varying two latent variables, object shape and object color. Let X and
Y denote image and label, respectively. We write Xij for the image with color i and shape j, where
i, j ∈{1, · · · , L}. When we want to consider K > 2 varying factors, we may write Xi1,··· ,iK for the
image random variable with kth factor chosen to be ik ∈{1, · · · , L}. Importantly, we ﬁx the number
of categories for each factor to L to enforce similar conditions for all cues. Similar learning setups
have appeared in prior papers: “Cross-bias generalisation” (Bahng et al., 2020), “What if multiple
features are predictive?” (Hermann & Lampinen, 2020), and “Zero generalization opportunities”
(Eulig et al., 2021). While we fully acknowledge the conceptual similarities, we stress that our work
presents the ﬁrst dedicated study into the cue selection problem and the underlying mechanisms."
SETUP,0.08108108108108109,"The same set of images {Xij | 1 ≤i, j ≤L} admits two possible tasks: color and shape classiﬁ-
cation. The task is determined by the labels Y . Denoting Yij as the label for image Xij, setting
Yij = i leads to the color classiﬁcation, and setting Yij = j leads to the shape classiﬁcation tasks.
We may then build the data distribution for the task at hand via"
SETUP,0.08408408408408409,"Dcolor :=
["
SETUP,0.08708708708708708,"1≤i,j≤L
(Xij, Yij = i)
Dshape :=
["
SETUP,0.09009009009009009,"1≤i,j≤L
(Xij, Yij = j)
(1)"
SETUP,0.09309309309309309,"for color and shape recognition tasks, respectively. More generally, we may write"
SETUP,0.0960960960960961,"Dk :=
["
SETUP,0.0990990990990991,"1≤i1,··· ,iK≤L
(Xi1,··· ,iK, Yi1,··· ,iK = ik)
(2)"
SETUP,0.1021021021021021,"for the data distribution where the task is to recognize the kth cue. We deﬁne the union of random
variables as the balanced mixture: SL
i=1 Zi := ZI where I ∼Unif {1, · · · , L}."
SETUP,0.10510510510510511,"We now introduce the notion of a diagonal dataset, where every cue (e.g. color and shape) contains
all the needed information to predict the true label Y . That is, a perfect prediction for either color or
shape attribute leads to a 100% accuracy for the task at hand. This can be done by letting the factors
always vary together i = j in the dataset (and thus the name). We write"
SETUP,0.10810810810810811,"Ddiag :=
["
SETUP,0.1111111111111111,"1≤i≤L
(Xii, Yii = i).
(3)"
SETUP,0.11411411411411411,"Such a dataset completely leaves it to the model to choose the cue for recognition. Given a model f
trained on Ddiag, we analyse the recognition cue adopted by f by measuring its unbiased accuracy
on all the cells (See Figure 1). There are K different unbiased accuracies for each task, depending
on how the off-diagonal cells are labelled: e.g. Dcolor and Dshape in equation 1. For a general setting
with K cues, the unbiased accuracy for kth cue is deﬁned as Y=1 Y=2 Y=3"
SETUP,0.11711711711711711,"Y=1
Y=2
Y=3"
SETUP,0.12012012012012012,"Y=Indian
Y=White
Y=Asian
Y=Black"
SETUP,0.12312312312312312,"Y=infant
Y=children Y=young adult
Y=senior"
SETUP,0.12612612612612611,"Figure 2: Dataset samples. The same set of images for each dataset accommodates two tasks, depending on
the labeling scheme (row-wise or column-wise). The diagonal dataset Ddiag (indicated with borders of color ■)
is identical regardless of the row- or column-wise labeling. Left: DSprites. Right: UTKFace."
SETUP,0.12912912912912913,"acck(f) := 1 Lk
X"
SETUP,0.13213213213213212,"i1,··· ,iK
Pr [f(Xi1,··· ,iK) = ik] .
(4)"
SETUP,0.13513513513513514,"Proposition 1. For k ∈{1, · · · , K}, acck(f) = 1 if and only if f(Xi1,··· ,iK) = ik almost surely
for all 1 ≤i1, · · · , iK ≤L. Moreover, if the condition above holds (i.e. f is perfectly biased to cue
k), then accm(f) = 1"
SETUP,0.13813813813813813,L for all m ̸= k.
SETUP,0.14114114114114115,"The proposition implies that the unbiased accuracy is capable of detecting the bias in a model f:
acck(f) = 1 implies that f’s prediction is solely based on the cue k. It also emphasizes that it is
impossible for a model to be perfectly biased to multiple cues. Finally, we remark that the WCST-ML
analysis does not require the cues to be orthogonal or interpretable to humans. The only requirement
is the availability of the labelled samples (Xi1,··· ,iK, Yi1,··· ,iK) for the cue of interest."
DATASETS FOR ANALYSIS,0.14414414414414414,"2.2
DATASETS FOR ANALYSIS"
DATASETS FOR ANALYSIS,0.14714714714714713,"DSprites (Matthey et al., 2017) is an image dataset of symbolic objects like triangles, squares, and
ellipses on black background. It consists of images with all possible combinations of ﬁve factors:
shape, scale, orientation, and X-Y position. The total number of combinations is 3 × 6 × 40 × 32 ×
32 = 737, 280. We augment the dataset with another axis of variation: 4 colors (white, red, green,
and blue). This results in 737, 280×4 = 2, 949, 120 images in the augmented dataset. Each image is
of resolution 64 × 64. We have identiﬁed degenerate orientation labels due to rotational symmetries
for certain shapes. We process the data further by collapsing the symmetries to a uniﬁed orientation
label. For example, we assign the same orientation label for squares at rotations 0, π"
DATASETS FOR ANALYSIS,0.15015015015015015,"2 , π, and 3π 2 ."
DATASETS FOR ANALYSIS,0.15315315315315314,"UTKFace (Zhang et al., 2017) is a face dataset with 33, 488 images with exhaustive annotations of
age, gender, and ethnicity per image. We utilize the Aligned and Cropped Face version to focus the
variation in the facial features only. Ages range from 0 to 116; gender is either male or female; and
ethnicity is one of White, Black, Asian, Indian, and Others (e.g. Hispanic, Latino, Middle Eastern)
following Zhang et al. (2017). Images are resized from 224 × 224 to 64 × 64."
DATASETS FOR ANALYSIS,0.15615615615615616,"Applying the data framework.
For each analysis, we select a subset of features S and build the
training set Ddiag and test sets Dk (§2.1) anew based on S. Figure 2 shows examples from DSprites
with S = {shape, color} and UTKFace with S = {age, ethnicity}, respectively. To enforce the
same number of classes L per feature, we set l as the minimal number of classes among the selected
features S. For features with #classes > L, we sub-sample the classes to match #classes = L. For
continuous features like age and rotation, we build categories based on intervals, deﬁned such that
the L classes are balanced. We randomly vary cues /∈S in the constructed datasets Ddiag and Dk.
Figure 2 shows the example datasets of Ddiag and Dk for each dataset."
PARAMETER SPACE AND SOLUTIONS,0.15915915915915915,"2.3
PARAMETER SPACE AND SOLUTIONS"
PARAMETER SPACE AND SOLUTIONS,0.16216216216216217,"We deﬁne the hypothesis class for DNNs through their weights and biases, collectively written as
θ ∈Θ (e.g. all possible weight and bias values of the ResNet50 architecture (He et al., 2016)). We"
PARAMETER SPACE AND SOLUTIONS,0.16516516516516516,"will use a non-negative loss L ≥0 to characterize the ﬁt of the model fθ on data D. 0-1 loss (and
cross-entropy loss) is such an example:"
PARAMETER SPACE AND SOLUTIONS,0.16816816816816818,"L(θ; D) = Pr(X,Y )∼D[fθ(X) ̸= Y ].
(5)"
PARAMETER SPACE AND SOLUTIONS,0.17117117117117117,"We deﬁne the solution set as Θ⋆:= {θ ∈Θ : L(θ; D) = 0}; in practice we deﬁne it with a small
threshold L(θ; D) ≤ϵ."
PARAMETER SPACE AND SOLUTIONS,0.17417417417417416,"We now make a connection between the datasets and solution sets. Assume that we have a dataset of
form (Xij, Yij) where Yij = i (the color task; see §2.1). The corresponding solution set is deﬁned
as ΘYij
ij
:= {θ : L(θ; (Xij, Yij = i)) = 0}. Deﬁne it similarly for the shape task (Yij = j)."
PARAMETER SPACE AND SOLUTIONS,0.17717717717717718,Proposition 2. If D = S
PARAMETER SPACE AND SOLUTIONS,0.18018018018018017,"(i,j)∈I(Xij, Yij) for some index set I, then the corresponding solution set"
PARAMETER SPACE AND SOLUTIONS,0.1831831831831832,is the intersection of the solutions sets T
PARAMETER SPACE AND SOLUTIONS,0.18618618618618618,"(i,j)∈I Θ⋆Yij
ij
."
PARAMETER SPACE AND SOLUTIONS,0.1891891891891892,"Proof. If L never attains 0, the solution set is always empty and the statement is vacuously true.
Otherwise, we may decompose L(θ) into the average
1
|I|
P"
PARAMETER SPACE AND SOLUTIONS,0.1921921921921922,"(i,j)∈I Lij(θ). Due to the non-negativity
of the loss function, L(θ) = 0 if and only if Lij(θ) = 0 for all (i, j) ∈I.
■"
PARAMETER SPACE AND SOLUTIONS,0.19519519519519518,"The proposition leads to a neat summary of the relations among solution sets like Θcolor := {θ :
L(θ; (Xij, i)) = 0} and Θshape deﬁned similarly."
PARAMETER SPACE AND SOLUTIONS,0.1981981981981982,"Corollary 3. Θcolor ⊂Θdiag and Θshape ⊂Θdiag. Moreover, Θcolor
T Θshape = ∅."
PARAMETER SPACE AND SOLUTIONS,0.2012012012012012,"Proof. The ﬁrst statement immediately follows from the proposition above and the fact that Ddiag =
Dcolor
T Dshape. For the second, observe that there is no function that satisﬁes fθ(Xij) = i and
fθ(Xij) = j at the same time for i ̸= j.
■ Θ1 Θ2"
PARAMETER SPACE AND SOLUTIONS,0.2042042042042042,"Θ3
Θdiag
ΘK−1"
PARAMETER SPACE AND SOLUTIONS,0.2072072072072072,"ΘK
We may easily extend the result to datasets with K cues. Given the solutions
sets Θk for each cue 1 ≤k ≤K, we have Θk ⊂Θdiag for all k and {Θk}k are
pairwise disjoint (diagram on the right). This follows from Proposition 1 that a
single model cannot achieve low error for multiple cues (Dk) at the same time."
OBSERVATIONS,0.21021021021021022,"3
OBSERVATIONS"
OBSERVATIONS,0.2132132132132132,"Based on the proposed WCST-ML, we study the natural tendencies of deep neural networks (DNNs)
to favor certain cues over the others. We also present a parameter-space view that hints to the
underlying mechanisms for the cue preference."
PREFERENTIAL ORDERING OF CUES,0.21621621621621623,"3.1
PREFERENTIAL ORDERING OF CUES"
PREFERENTIAL ORDERING OF CUES,0.21921921921921922,"In the ﬁrst set of experiments, we observe which cues are preferentially used by the network when
training on a dataset composed of an equally represented set of cues: the diagonal dataset Ddiag in
§2.1. We train three qualitatively different types of DNNs: (1) Feed Forward neural network (FFnet),
(2) ResNet with depth 20 (ResNet20), and (3) Vision Transformer (ViT). Details of the architectures
and training setups can be found in Appendix §A. The results are summarized in Figure 3."
PREFERENTIAL ORDERING OF CUES,0.2222222222222222,"Models adopt cues with uneven likelihood.
Figure 3 shows the unbiased accuracies of the models
when trained on the diagonal sets for each dataset (DSprites and UTKFace) with the involved cues
denoted as S. We report the diagonal accuracy and the K unbiased accuracies for each cue in S on
the held-out test set. The number of labels L is 3 for DSprites and 2 for UTKFace. On DSprites,
we observe that for all architectures, color is by far the most preferred cue (unbiased accuracy
near 100%). For FFnet and ViT, the extreme color bias forces the accuracy on other cues to be
1
L = 33.3% as predicted by Proposition 1. ResNet20 behaves less extremely and picks up scale,
shape, and orientation, in the order of preference. On UTKFace, there is no extreme dominant cue,
but there is a general preference to use ethnicity cues for making predictions, followed by gender
and age cues. The trend is clear for all models, and while the FFnet model shows a slightly more
variable performance across runs, the ranking of cues is preserved throughout. The preference of"
PREFERENTIAL ORDERING OF CUES,0.22522522522522523,DSprites
PREFERENTIAL ORDERING OF CUES,0.22822822822822822,"S =
n
shape, scale
orientation, color
o"
PREFERENTIAL ORDERING OF CUES,0.23123123123123124,"0
20
40
60
80
100 120
Epoch 0 20 40 60 80 100"
PREFERENTIAL ORDERING OF CUES,0.23423423423423423,Accuracy
PREFERENTIAL ORDERING OF CUES,0.23723723723723725,diagonal color
PREFERENTIAL ORDERING OF CUES,0.24024024024024024,"scale
shape
orientation
diagonal avg. acc ±"
PREFERENTIAL ORDERING OF CUES,0.24324324324324326,color avg. acc ±
PREFERENTIAL ORDERING OF CUES,0.24624624624624625,orientation avg. acc ±
PREFERENTIAL ORDERING OF CUES,0.24924924924924924,scale avg. acc ±
PREFERENTIAL ORDERING OF CUES,0.25225225225225223,shape avg. acc ±
PREFERENTIAL ORDERING OF CUES,0.2552552552552553,"0
20
40
60
80
100 120
Epoch 0 20 40 60 80 100"
PREFERENTIAL ORDERING OF CUES,0.25825825825825827,Accuracy
PREFERENTIAL ORDERING OF CUES,0.26126126126126126,diagonal color
PREFERENTIAL ORDERING OF CUES,0.26426426426426425,"scale
shape"
PREFERENTIAL ORDERING OF CUES,0.2672672672672673,orientation
PREFERENTIAL ORDERING OF CUES,0.2702702702702703,"0
20
40
60
80
100 120
Epoch 0 20 40 60 80 100"
PREFERENTIAL ORDERING OF CUES,0.2732732732732733,Accuracy
PREFERENTIAL ORDERING OF CUES,0.27627627627627627,diagonal color shape
PREFERENTIAL ORDERING OF CUES,0.27927927927927926,"scale
orientation"
PREFERENTIAL ORDERING OF CUES,0.2822822822822823,UTKFace
PREFERENTIAL ORDERING OF CUES,0.2852852852852853,"S =
n ethnicity
gender, age
o"
PREFERENTIAL ORDERING OF CUES,0.2882882882882883,"0
20
40
60
80
100 120
Epoch 0 20 40 60 80 100"
PREFERENTIAL ORDERING OF CUES,0.2912912912912913,Accuracy
PREFERENTIAL ORDERING OF CUES,0.29429429429429427,diagonal
PREFERENTIAL ORDERING OF CUES,0.2972972972972973,ethnicity
PREFERENTIAL ORDERING OF CUES,0.3003003003003003,gender age
PREFERENTIAL ORDERING OF CUES,0.3033033033033033,diagonal avg. acc ±
PREFERENTIAL ORDERING OF CUES,0.3063063063063063,age avg. acc ±
PREFERENTIAL ORDERING OF CUES,0.30930930930930933,ethnicity avg. acc ±
PREFERENTIAL ORDERING OF CUES,0.3123123123123123,gender avg. acc ±
PREFERENTIAL ORDERING OF CUES,0.3153153153153153,"0
20
40
60
80
100 120
Epoch 0 20 40 60 80 100"
PREFERENTIAL ORDERING OF CUES,0.3183183183183183,Accuracy
PREFERENTIAL ORDERING OF CUES,0.3213213213213213,diagonal
PREFERENTIAL ORDERING OF CUES,0.32432432432432434,ethnicity
PREFERENTIAL ORDERING OF CUES,0.32732732732732733,gender age
PREFERENTIAL ORDERING OF CUES,0.3303303303303303,"0
20
40
60
80
100 120
Epoch 0 20 40 60 80 100"
PREFERENTIAL ORDERING OF CUES,0.3333333333333333,Accuracy
PREFERENTIAL ORDERING OF CUES,0.33633633633633636,diagonal
PREFERENTIAL ORDERING OF CUES,0.33933933933933935,ethnicity
PREFERENTIAL ORDERING OF CUES,0.34234234234234234,gender age
PREFERENTIAL ORDERING OF CUES,0.34534534534534533,diagonal avg. acc ±
PREFERENTIAL ORDERING OF CUES,0.3483483483483483,age avg. acc ±
PREFERENTIAL ORDERING OF CUES,0.35135135135135137,ethnicity avg. acc ±
PREFERENTIAL ORDERING OF CUES,0.35435435435435436,gender avg. acc ±
PREFERENTIAL ORDERING OF CUES,0.35735735735735735,"FFnet
ResNet20
ViT"
PREFERENTIAL ORDERING OF CUES,0.36036036036036034,"Figure 3: Preferential ordering of cues. Diagonal and unbiased accuracies of models trained on the diagonal
training set Ddiag composed of the cues deﬁned by S and tested on the off-diagonal sets Dk where k ∈S."
PREFERENTIAL ORDERING OF CUES,0.3633633633633634,"Figure 4: 3-D visualisation of loss surface around different solution types Θ⋆.
For each cue, we show 3-D plots of the same loss surface (side and top views). X and Y axes are
randomly chosen directions in the parameter space; Z axis shows the loss value at each (X, Y )
measured against the diagonal data Ddiag. All the surfaces are shown at the same scale."
PREFERENTIAL ORDERING OF CUES,0.3663663663663664,"ethnicity echoes the DNN’s preference of color as a strong cue in DSprites; DNNs are drawn to
utilizing skin color. Another curious phenomenon is that none of the three cues sufﬁciently explains
the 100% diagonal accuracy, suggesting the existence of an unlabeled shortcut cue other than the
three considered. Finally, when the most dominant cue is left out, other cues activate in its place:
scale for DSprites and gender for UTKFace (Appendix §B)."
PREFERENTIAL ORDERING OF CUES,0.36936936936936937,"Consistency of preferences.
We have observed a consistent preferential ordering of cues shown
by qualitatively different types of architectures. The results suggest the existence of a common
denominator that depends solely on the nature of the cues, rather than the architectures. We ﬁnally
remark that the experiments were run 10 times with different random initialization; error bars in
Figure 3 show ±σ standard deviation. The small variability observed across most training shows
the consistency of the preferential ranking with respect to initialization. For a pair of cues, we refer
to the one that is more likely to be chosen by a model trained on Ddiag as the preferred cue and the
other as the averted cue."
THE ABUNDANCE OF PREFERRED-CUE SOLUTIONS,0.37237237237237236,"3.2
THE ABUNDANCE OF PREFERRED-CUE SOLUTIONS"
THE ABUNDANCE OF PREFERRED-CUE SOLUTIONS,0.37537537537537535,"Why are certain cues preferred to others? We look for the answers in the parameter space. More
speciﬁcally, we study the geometric properties of the loss function and the corresponding solution
sets in the parameter space. The section is based on §2.3. We write the solution set corresponding
to the preferred cues as preferred solutions Θp and averted cues as averted solutions Θa."
THE ABUNDANCE OF PREFERRED-CUE SOLUTIONS,0.3783783783783784,"How to ﬁnd the averted solutions θ ∈Θa?
Naively training a DNN on Ddiag will most likely
result in a preferred solution Θp. We thus use the full dataset Dk corresponding to the averted cue
k. (§2.1) as the training set to ﬁnd an averted solution θ ∈Θa. To ensure a fair comparison in terms
of the amount of training data, we also use the full dataset Dk for the preferred cues."
THE ABUNDANCE OF PREFERRED-CUE SOLUTIONS,0.3813813813813814,"Qualitative view on the loss surface.
We examine the geometric properties of the loss surface for
ResNet20 around different solutions types (preferred and averted). We use the 2D visualization tool
for loss landscape by Li et al. (2018). The X-Y grid is spanned by two random vectors in RD where
D is the number of parameters. In Figure 4, we observe that the preferred solutions (e.g. Θcolor)
are characterized with a ﬂatter and wider surface in the vicinity. In contrast, local surface around
averted (e.g. Θorientation) solutions exhibit relatively sharper and narrower valleys. A similar trend is
observed for the UTKFace cues at a milder degree: Θethnicity is a slightly ﬂatter solution than Θage. Θp
Θa"
THE ABUNDANCE OF PREFERRED-CUE SOLUTIONS,0.3843843843843844,"basin for
basin for
Θp"
THE ABUNDANCE OF PREFERRED-CUE SOLUTIONS,0.38738738738738737,"Θa
Size of the basin of attraction.
We compare the size of the preferred Θp and
averted Θa solution sets using the notion of the basin of attraction (BA) for a
cue k: a subset N in the parameter space such that initializing the training with
a point θ0 ∈N leads to a k-biased solution θ⋆∈Θk. To measure the size of
the BA for Θk, we start by obtaining a solution θk ∈Θk biased to the cue k.
We then check when the ﬁnal models trained from perturbed points θk + v (where ∥v∥2 is small)
stops being biased to the cue k. We treat the norm of the bias-shifting critical perturbation ∥v∥2 as
the size of the BA for Θk."
THE ABUNDANCE OF PREFERRED-CUE SOLUTIONS,0.39039039039039036,"Preferred solutions have wider BA.
Figure 5 shows the size of BA measured according to the
method above. The upper row shows initialization around solutions biased to cues in DSprites. For
the color cue, we observe that initializing far away from the color-biased solution still results in
a color-biased solution. For shape, scale, and orientation cues, we observe that initializing away
from the respective solutions gradually results in color-biased solutions with a reduction in the bias
towards the respective biases in the unperturbed initial parameters. The sizes of basins of attrac-
tion are ordered according to color>shape>scale>orientation, largely following the preferential
ordering observed in §3.1. The lower row of Figure 5 shows the results for the three cues in UTK-
Face. As opposed to DSprites, the solutions for ethnicity, age, and gender do not re-converge to
a clear cue after retraining for only 50 epochs. The sizes of basins of attraction are ordered as
ethnicity>gender>age, following the preferential ordering of cues observed in §3.1. Θdiag Θa
Θp path"
THE ABUNDANCE OF PREFERRED-CUE SOLUTIONS,0.3933933933933934,"Path connectivity.
Recent works have proposed methods to ﬁnd near zero-
loss paths between two solutions (Garipov et al., 2018; Draxler et al., 2018;
Benton et al., 2021). We extend the technique to ﬁnd the zero-loss (w.r.t. Ddiag)
path between preferred and averted solutions. We locate the ﬂipping moment
for the bias along the path; there must be a bias-shift boundary where the bias
to the preferred cue shifts to the averted cue along the path (by an intermediate-
value-theorem-like argument). By comparing the length of the segment up to this boundary, we may
estimate the relative abundance of the preferred and averted solutions in the parameter space."
THE ABUNDANCE OF PREFERRED-CUE SOLUTIONS,0.3963963963963964,"Paths connecting preferred and averted solutions and the bias-shift boundary.
Figure 6 visu-
alizes the paths between the preferred and averted solutions as well as the corresponding shifts in
biases along the path. We observe that the paths generally show a single shift in bias. The shift is
also usually quick; for the most part of the path, the solutions tend to be heavily biased towards ei-
ther of the cues. For the path from Θcolor to Θorientation, we observe a tendency for Θcolor to dominate
the path until near Θorientation, suggesting the relative abundance of color-biased solutions Θcolor in
the set of diagonal solutions Θdiag compared to the orientation Θorientation. The path from Θethnicity to
Θgender is composed of a longer segment for the Θethnicity part. The relative abundance of cues in the
diagonal solutions Θdiag resonates the preferential ordering of cues seen in §3.1."
EXPLANATIONS,0.3993993993993994,"4
EXPLANATIONS"
EXPLANATIONS,0.4024024024024024,"We have observed intriguing properties of cues and shortcut biases. Even under the equal oppor-
tunities for the cues to be adopted, models have a preference for a few cues over the others. The"
EXPLANATIONS,0.40540540540540543,"0
25
50
75 100 125 150 175 200
Size of perturbation ||v||2 0 20 40 60 80 100"
EXPLANATIONS,0.4084084084084084,Accuracy color
EXPLANATIONS,0.4114114114114114,"orientation
scale
shape"
EXPLANATIONS,0.4144144144144144,(a) Color
EXPLANATIONS,0.4174174174174174,"0
25
50
75 100 125 150 175 200
Size of perturbation ||v||2 0 20 40 60 80 100"
EXPLANATIONS,0.42042042042042044,Accuracy color shape
EXPLANATIONS,0.42342342342342343,"scale
orientation"
EXPLANATIONS,0.4264264264264264,(b) Shape
EXPLANATIONS,0.4294294294294294,"0
25
50
75 100 125 150 175 200
Size of perturbation ||v||2 0 20 40 60 80 100"
EXPLANATIONS,0.43243243243243246,Accuracy color scale
EXPLANATIONS,0.43543543543543545,"orientation
shape"
EXPLANATIONS,0.43843843843843844,(c) Scale
EXPLANATIONS,0.44144144144144143,"0
25
50
75 100 125 150 175 200
Size of perturbation ||v||2 0 20 40 60 80 100"
EXPLANATIONS,0.4444444444444444,Accuracy color
EXPLANATIONS,0.44744744744744747,"scale
orientation
shape"
EXPLANATIONS,0.45045045045045046,(d) Orientation
EXPLANATIONS,0.45345345345345345,"0
25
50
75 100 125 150 175 200
Size of perturbation ||v||2 0 20 40 60 80 100"
EXPLANATIONS,0.45645645645645644,Accuracy
EXPLANATIONS,0.4594594594594595,"age
gender"
EXPLANATIONS,0.4624624624624625,ethnicity
EXPLANATIONS,0.46546546546546547,(e) Ethnicity
EXPLANATIONS,0.46846846846846846,"0
25
50
75 100 125 150 175 200
Size of perturbation ||v||2 0 20 40 60 80 100"
EXPLANATIONS,0.47147147147147145,Accuracy
EXPLANATIONS,0.4744744744744745,"gender
age"
EXPLANATIONS,0.4774774774774775,ethnicity
EXPLANATIONS,0.4804804804804805,(f) Gender
EXPLANATIONS,0.48348348348348347,"0
25
50
75 100 125 150 175 200
Size of perturbation ||v||2 0 20 40 60 80 100"
EXPLANATIONS,0.4864864864864865,Accuracy
EXPLANATIONS,0.4894894894894895,ethnicity
EXPLANATIONS,0.4924924924924925,"gender
age"
EXPLANATIONS,0.4954954954954955,(g) Age
EXPLANATIONS,0.4984984984984985,"Figure 5: Basin of attraction. We initialize ResNet20 around solutions biased to each of the four cues in
DSprites (upper row) and three cues in UTKFace (lower row). We report the averaged unbiased accuracies
for models initialized with perturbed solutions θ⋆+ v. The x-axes are the perturbation sizes ∥v∥2. From the
perturbed initial parameters, each model is trained for 50 epochs."
EXPLANATIONS,0.5015015015015015,"Accuracies on path
Diagonal loss
Preferred-cue loss
Averted-cue loss"
EXPLANATIONS,0.5045045045045045,"Color to
orientation"
EXPLANATIONS,0.5075075075075075,"0
10
20
30
40
50
60
Path Step 65 70 75 80 85 90 95 100"
EXPLANATIONS,0.5105105105105106,Accuracy (%)
EXPLANATIONS,0.5135135135135135,"Diagonal
Color
Orientation"
EXPLANATIONS,0.5165165165165165,"0
50
100
150
200
0.2
0.1
0.0
0.1
0.2
0.3
0.4
0.5
0.6"
EXPLANATIONS,0.5195195195195195,"color
orientation"
EXPLANATIONS,0.5225225225225225,"0.0022
0.014
0.025
0.043
0.076
0.14
0.25
0.45"
EXPLANATIONS,0.5255255255255256,> 0.45
EXPLANATIONS,0.5285285285285285,"0
50
100
150
200
0.2
0.1
0.0
0.1
0.2
0.3
0.4
0.5
0.6"
EXPLANATIONS,0.5315315315315315,"color
orientation"
EXPLANATIONS,0.5345345345345346,"0.005
0.02
0.041
0.086
0.19
0.43
0.99
2.3 > 2.3"
EXPLANATIONS,0.5375375375375375,"0
50
100
150
200
0.2
0.1
0.0
0.1
0.2
0.3
0.4
0.5
0.6"
EXPLANATIONS,0.5405405405405406,"color
orientation"
EXPLANATIONS,0.5435435435435435,"0.0091
0.025
0.045
0.093
0.2
0.46
1.1
2.4 > 2.4"
EXPLANATIONS,0.5465465465465466,"Ethnicity to
gender"
EXPLANATIONS,0.5495495495495496,"0
10
20
30
40
50
60
Path Step 65 70 75 80 85 90 95 100"
EXPLANATIONS,0.5525525525525525,Accuracy (%)
EXPLANATIONS,0.5555555555555556,"Diagonal
Ethnicity
Gender"
EXPLANATIONS,0.5585585585585585,"10 0
10 20 30 40 50 60 70
0.1 0.0 0.1 0.2 0.3"
EXPLANATIONS,0.5615615615615616,"ethnicity
gender"
EXPLANATIONS,0.5645645645645646,"0.0016
0.013
0.021
0.034
0.057
0.095
0.16
0.27"
EXPLANATIONS,0.5675675675675675,> 0.27
EXPLANATIONS,0.5705705705705706,"10 0
10 20 30 40 50 60 70
0.1 0.0 0.1 0.2 0.3"
EXPLANATIONS,0.5735735735735735,"ethnicity
gender"
EXPLANATIONS,0.5765765765765766,"0.0057
0.021
0.041
0.088
0.19
0.44
1
2.3 > 2.3"
EXPLANATIONS,0.5795795795795796,"10 0
10 20 30 40 50 60 70
0.1 0.0 0.1 0.2 0.3"
EXPLANATIONS,0.5825825825825826,"ethnicity
gender"
EXPLANATIONS,0.5855855855855856,"0.006
0.021
0.04
0.083
0.18
0.4
0.88
2 > 2"
EXPLANATIONS,0.5885885885885885,"Figure 6: Paths connecting Θp and Θa. We compute paths between preferred and averted solutions along
which the diagonal-dataset (Ddiag) loss is zero. On the leftmost column, we show the unbiased accuracies for the
preferred and averted cues along the path; we observe a bias shift in the middle in each case. The three rightmost
columns visualize the loss values around the zero-loss paths (solid curves), with respect to the losses computed
on diagonal Ddiag, preferred cue Dp, and averted cue Da datasets. The zero-loss paths are quadratic Bezier
curves, parametrised by three dots shown in the plots: the two end points (preferred and averted solutions) and
a third point that regulate the curve shape by “pulling” it. Inspired by Garipov et al. (2018)."
EXPLANATIONS,0.5915915915915916,"preference is stable across architectures and weight initialization. We have also found a strong cor-
relation between the preference for a cue and the abundance of solutions biased to that cue. In this
section, we explain the observations based on the complexity of cues."
EXPLANATIONS,0.5945945945945946,"Kolmogorov complexity of cues (KCC).
We formally deﬁne cue as the condition distribution
pY |X for a given input distribution pX; in a sense, it concurs with the deﬁnition of task. Kolmogorov
complexity (KC, Kolmogorov (1963)) measures complexity of binary strings based on the minimal
length among programs that generate the string. Achille et al. (2018) have deﬁned the Kolmogorov
complexity of a task (or a cue) pY |X as:"
EXPLANATIONS,0.5975975975975976,"K(pY |X) =
min
L(f;X,Y )<δ K(f)
(6)"
EXPLANATIONS,0.6006006006006006,"for some sufﬁciently small scalar δ > 0. f denotes a discriminative model mapping X to Y and L
is a suitable loss function. K(f) is the KC for model f. Intuitively, K(pY |X) measures the minimal
complexity of the function f required to memorize the labeling pY |X on the training set (i.e. L < δ)."
EXPLANATIONS,0.6036036036036037,"Measuring KCC.
We estimate the KC of four cues (color, scale, shape, and orientation) consid-
ered in DSprites based on equation 6. Since it is impractical to optimise K(f) over an open-world
of all possible models f, we constrain the search to a homogeneous family of models f ∈F. We
use ResNet20 with variable channel sizes (Zagoruyko & Komodakis, 2016) for F. Given that KC
is uncomputable, we use the number of parameters in f as an approximation to K(f) in practice"
EXPLANATIONS,0.6066066066066066,"(Valle-Perez et al., 2019). We set the memorization criterion L(f; X, Y ) < δ as “training-set error
for f is < 1%”. Under this setup, we have veriﬁed that the complexities of cues are 1.2K parameters
for color, 4.6K for scale, 17.6K for shape, and > 273K for orientation. We were not able to fully
memorize the orientation labels under the network scales we considered. The complexity ranking
strongly concurs with the preferential ordering found in §3.1."
EXPLANATIONS,0.6096096096096096,"The simplicity bias in DNN parameter space.
How are the cue complexity and shortcut bias
connected? We explain the underlying mechanism based on a related theory in a different context:
explaining the generalizability of DNNs with the simplicity bias (Valle-Perez et al., 2019; Shah et al.,
2020). It has been argued that, despite a large number of parameters, DNNs still generalize due to
the inborn tendency to encode simple functions. This phenomenon has ﬁlled the theoretical gap left
by the classical learning theory. The argument is that a (uniform) random selection of parameters
for a DNN is likely to result in a function f that is simple in the sense of KC. More precisely, the
likelihood of a function f being encoded by a DNN is governed by the inequality p(f) ≲2−a e
K(f)+b,
where K(f) is the KC of f and a > 0, b are constants independent of f (Valle-Perez et al., 2019).
This inequality stems from the Algorithmic Information Theory and has been validated to hold for
a large variety of systems that arise naturally, such as RNA sequences and solutions to a large class
of differential equations (Dingle et al., 2018). The argument concludes that the natural abundance
of simple functions in the parameter space makes it more likely for the solutions to be simpler (Wu
et al., 2017), leading to the simplicity bias and thus helping DNNs generalize well."
EXPLANATIONS,0.6126126126126126,"The simplicity bias leads to shortcuts.
We have seen that cues have different KCs, necessitating
DNNs with different complexities to fully represent them. The simplicity bias in the parameter space
explains the abundance of solutions biased to simple cues like color Θcolor rather than complex cues
like orientation Θorientation (§3.2). The difference in the abundance in turn makes it more likely for a
model trained on the diagonal dataset Ddiag to be biased towards simple cues (§3.1)."
DISCUSSION AND CONCLUSION,0.6156156156156156,"5
DISCUSSION AND CONCLUSION"
DISCUSSION AND CONCLUSION,0.6186186186186187,"We have delved into the shortcut learning phenomenon in deep neural networks (DNNs). We have
devised a training setup WCST-ML where multiple cues are equally valid for solving the task at hand
and have veriﬁed that, despite the equal validity, DNNs tend to choose cues in a certain order (e.g.
color is preferred to orientation). We have shown that the simple cues (in the sense of Kolmogorov)
are far more frequently represented in the parameter space and thus are far more likely to be adopted
by DNNs. We conclude the paper with two ﬁnal remarks on the implications of our studies."
DISCUSSION AND CONCLUSION,0.6216216216216216,"Simplicity bias and generalization.
The simplicity bias theory (Valle-Perez et al., 2019) has been
developed to explain the unreasonably good generalizability of DNNs. Our paper shows that it may
interfere with the generalization. This apparent paradox stems from the difference in the setup. The
simplicity bias is often safe to be exploited in the standard iid setup, albeit with caveats argued by
Shah et al. (2020). On the other hand, it may lead to less effective generalization across distribution
shifts as in our learning setup (Geirhos et al., 2020), if the simple cues are no longer generaliz-
able. Thus, for the generalization across distribution shifts, machine learners may need to be given
additional guidance to overcome the inborn tendencies to pick up simple cues for recognition."
DISCUSSION AND CONCLUSION,0.6246246246246246,"Societal implications.
An issue with the simplicity bias and shortcut learning is that relying on
simple cues is sometimes unethical. We have seen in the UTKFace experiments that ethnicity tends
to be a more convenient shortcut than age, for example, if they equally correlate with the target
label at hand. This is an alarming phenomenon. It emphasizes the importance and inevitability of
active human intervention to undo certain naturally-arising biases to socially harmful cues in learned
models. It also conﬁrms that we are on the right track to actively regularize the models to behave
according to the social norms (Barocas et al., 2017; Mehrabi et al., 2021)."
DISCUSSION AND CONCLUSION,0.6276276276276276,"Toolbox contribution.
Along the way, we contribute interesting novel tools that future research
can utilise, such as the computation of Kolmogorov complexity (KC) for generic cues (equation 6)
and the method for ﬁnding a heterogeneous zero-loss curve connecting two solutions biased to dif-
ferent cues (§3.2; Garipov et al. (2018))."
DISCUSSION AND CONCLUSION,0.6306306306306306,REPRODUCIBILITY
DISCUSSION AND CONCLUSION,0.6336336336336337,"We understand and resonate with the importance of reproducible research practices in machine learn-
ing. The reproducibility of this paper hinges on our use of open-source data, models, training tech-
niques, and deep learning frameworks. We have speciﬁed a detailed experimental setup in §2.2 and
§A. For the consistency of experimental observations, we have run the experiments multiple times
with different random seeds, whenever applicable. For those cases, we have reported the mean and
standard deviation for each data point."
ETHICS STATEMENT,0.6366366366366366,ETHICS STATEMENT
ETHICS STATEMENT,0.6396396396396397,"We delve into the potential ethical issues with DNNs. Our work excavates the potential ethical harms
caused by deep neural networks’ natural tendencies to prefer simple cues, where using the simple
cues may not conform well to the social norm (e.g. ethnicity cue). We have thus advocated an active
human involvement to regularize such DNNs to adopt more socially acceptable cues. As for the
experiments, part of our studies is based on a face dataset (UTKFace). We conﬁrm that the UTK-
Face dataset has been released with an open-source license with the non-commercial clause2. The
copyright for the images still belongs to their respective owners. Inheriting the policy of UTKFace,
we will erase the face samples in Figure 2 upon request by the owners of the respective images."
REFERENCES,0.6426426426426426,REFERENCES
REFERENCES,0.6456456456456456,"Alessandro Achille, Glen Mbeng, and Stefano Soatto. Dynamics and reachability of learning tasks.
arXiv preprint arXiv:1810.02440, 2018."
REFERENCES,0.6486486486486487,"Hyojin Bahng, Sanghyuk Chun, Sangdoo Yun, Jaegul Choo, and Seong Joon Oh. Learning de-biased
representations with biased representations. In International Conference on Machine Learning
(ICML), 2020."
REFERENCES,0.6516516516516516,"Masahiro Banno, Takayoshi Koide, Branko Aleksic, Takashi Okada, Tsutomu Kikuchi, Kunihiro
Kohmura, Yasunori Adachi, Naoko Kawano, Tetsuya Iidaka, and Norio Ozaki.
Wisconsin
card sorting test scores and clinical and sociodemographic correlates in schizophrenia: mul-
tiple logistic regression analysis.
BMJ Open, 2(6), 2012.
ISSN 2044-6055.
doi: 10.1136/
bmjopen-2012-001340. URL https://bmjopen.bmj.com/content/2/6/e001340."
REFERENCES,0.6546546546546547,"Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness in machine learning. Conference on
Neural Information Processing Systems (NeurIPS) Tutorial, 1:2017, 2017."
REFERENCES,0.6576576576576577,"Gregory W Benton, Wesley J Maddox, Sanae Lotﬁ, and Andrew Gordon Wilson. Loss surface
simplexes for mode connecting volumes and fast ensembling. arXiv preprint arXiv:2102.13042,
2021."
REFERENCES,0.6606606606606606,"Remi Cadene, Corentin Dancette, Matthieu Cord, Devi Parikh, et al. Rubi: Reducing unimodal
biases for visual question answering. In Advances in Neural Information Processing Systems, pp.
839–850, 2019."
REFERENCES,0.6636636636636637,"Junsuk Choe, Seong Joon Oh, Seungho Lee, Sanghyuk Chun, Zeynep Akata, and Hyunjung Shim.
Evaluating weakly supervised object localization methods right. In Conference on Computer
Vision and Pattern Recognition (CVPR), 2020."
REFERENCES,0.6666666666666666,"Giacomo De Palma, Bobak Kiani, and Seth Lloyd. Random deep neural networks are biased towards
simple functions. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran As-
sociates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
feab05aa91085b7a8012516bc3533958-Paper.pdf."
REFERENCES,0.6696696696696697,"Kamaludin Dingle, Chico Q Camargo, and Ard A Louis. Input–output maps are strongly biased
towards simple outputs. Nature communications, 9(1):1–7, 2018."
REFERENCES,0.6726726726726727,2https://susanqq.github.io/UTKFace/
REFERENCES,0.6756756756756757,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An im-
age is worth 16x16 words: Transformers for image recognition at scale. In International Confer-
ence on Learning Representations (ICLR), 2021."
REFERENCES,0.6786786786786787,"Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred Hamprecht. Essentially no barriers
in neural network energy landscape. In International conference on machine learning, pp. 1309–
1318. PMLR, 2018."
REFERENCES,0.6816816816816816,"Elias Eulig, Piyapat Saranrittichai, Chaithanya Kumar Mummadi, Kilian Rambach, William Beluch,
Xiahan Shi, and Volker Fischer. Diagvib-6: A diagnostic benchmark suite for vision models
in the presence of shortcut and generalization opportunities. In Proceedings of the IEEE/CVF
International Conference on Computer Vision (ICCV), October 2021."
REFERENCES,0.6846846846846847,"Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry Vetrov, and Andrew Gordon Wilson.
Loss surfaces, mode connectivity, and fast ensembling of dnns. In Proceedings of the 32nd Inter-
national Conference on Neural Information Processing Systems, pp. 8803–8812, 2018."
REFERENCES,0.6876876876876877,"Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and
Wieland Brendel.
Imagenet-trained cnns are biased towards texture; increasing shape bias
improves accuracy and robustness. In International Conference on Learning Representations
(ICLR), 2019."
REFERENCES,0.6906906906906907,"Robert Geirhos, J¨orn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel,
Matthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature
Machine Intelligence, 2(11):665–673, 2020."
REFERENCES,0.6936936936936937,"Tatsunori Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness with-
out demographics in repeated loss minimization.
In Jennifer Dy and Andreas Krause (eds.),
Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceed-
ings of Machine Learning Research, pp. 1929–1938. PMLR, 10–15 Jul 2018. URL https:
//proceedings.mlr.press/v80/hashimoto18a.html."
REFERENCES,0.6966966966966966,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing
human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE international
conference on computer vision, pp. 1026–1034, 2015."
REFERENCES,0.6996996996996997,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778, 2016."
REFERENCES,0.7027027027027027,"Katherine Hermann and Andrew Lampinen.
What shapes feature representations?
exploring
datasets, architectures, and training.
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Bal-
can, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
9995–10006. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/
paper/2020/file/71e9c6620d381d60196ebe694840aaaa-Paper.pdf."
REFERENCES,0.7057057057057057,"Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wil-
son.
Averaging weights leads to wider optima and better generalization.
arXiv preprint
arXiv:1803.05407, 2018."
REFERENCES,0.7087087087087087,"Andrei N Kolmogorov. On tables of random numbers. Sankhy¯a: The Indian Journal of Statistics,
Series A, pp. 369–376, 1963."
REFERENCES,0.7117117117117117,"Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss land-
scape of neural nets. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran As-
sociates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf."
REFERENCES,0.7147147147147147,"Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dsprites: Disentanglement
testing sprites dataset. https://github.com/deepmind/dsprites-dataset/, 2017."
REFERENCES,0.7177177177177178,"Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A survey
on bias and fairness in machine learning. ACM Computing Surveys (CSUR), 54(6):1–35, 2021."
REFERENCES,0.7207207207207207,"Pranav Rajpurkar, Jeremy Irvin, Kaylie Zhu, Brandon Yang, Hershel Mehta, Tony Duan, Daisy
Ding, Aarti Bagul, Curtis Langlotz, Katie Shpanskaya, et al. Chexnet: Radiologist-level pneumo-
nia detection on chest x-rays with deep learning. arXiv preprint arXiv:1711.05225, 2017."
REFERENCES,0.7237237237237237,"Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, and Praneeth Netrapalli. The
pitfalls of simplicity bias in neural networks. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
9573–9585. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/
paper/2020/file/6cfe0e6127fa25df2a0ef2ae1067d915-Paper.pdf."
REFERENCES,0.7267267267267268,"David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484–489, 2016."
REFERENCES,0.7297297297297297,"Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and
Herv´e J´egou.
Training data-efﬁcient image transformers & distillation through attention.
In
International Conference on Machine Learning, pp. 10347–10357. PMLR, 2021."
REFERENCES,0.7327327327327328,"Guillermo Valle-Perez, Chico Q. Camargo, and Ard A. Louis.
Deep learning generalizes be-
cause the parameter-function map is biased towards simple functions. In International Confer-
ence on Learning Representations, 2019. URL https://openreview.net/forum?id=
rye4g3AqFm."
REFERENCES,0.7357357357357357,"Tianlu Wang, Jieyu Zhao, Mark Yatskar, Kai-Wei Chang, and Vicente Ordonez. Balanced datasets
are not enough: Estimating and mitigating gender bias in deep image representations. In Pro-
ceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5310–5319, 2019."
REFERENCES,0.7387387387387387,"Philippe Weinzaepfel and Gr´egory Rogez. Mimetics: Towards understanding human actions out of
context. International Journal of Computer Vision, 129(5):1675–1690, 2021."
REFERENCES,0.7417417417417418,"Lei Wu, Zhanxing Zhu, et al. Towards understanding generalization of deep learning: Perspective
of loss landscapes. arXiv preprint arXiv:1706.10239, 2017."
REFERENCES,0.7447447447447447,"Tian Xu, Jennifer White, Sinan Kalkan, and Hatice Gunes. Investigating bias and fairness in facial
expression recognition. In European Conference on Computer Vision, pp. 506–523. Springer,
2020."
REFERENCES,0.7477477477477478,"Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In The British Machine Vision
Conference (BMVC), 2016."
REFERENCES,0.7507507507507507,"Zhifei Zhang, Yang Song, and Hairong Qi. Age progression/regression by conditional adversarial
autoencoder. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 5810–5818, 2017."
REFERENCES,0.7537537537537538,"A
MODEL ARCHITECTURES"
REFERENCES,0.7567567567567568,"FFnet.
The FFnet architecture tested is a fully connected feedforward neural network with 5 hid-
den layers with respectively 4096, 2048, 600, 300 and 100 hidden units. ReLU activations are used
throughout. We train it using the Adam optimizer with the default parameters given by PyTorch
1.8.0."
REFERENCES,0.7597597597597597,"ResNet20.
The ResNet20 architecture is adapted from He et al. (2016), with a depth of 20. We
train it using the Adam optimizer with the default parameters given by PyTorch 1.8.0."
REFERENCES,0.7627627627627628,"ViT.
The Visual Transformer architecture is adapted from (Touvron et al., 2021). Given the di-
mensions of the images in ColorDSprites and consequently UTKFace, we implement a ViT model
with patch size 8, embedding dimension 192, depth 12, and 3 attention heads. We train the model
with the SGD optimizer with learning rate 5 × 10−3, momentum 0.9, and weight decay 1 × 10−4."
REFERENCES,0.7657657657657657,"B
PREFERENTIAL ORDERING AFTER REMOVING THE DOMINANT CUE"
REFERENCES,0.7687687687687688,"For both DSprites and UTKFace, removing the dominant cue (color and ethnicity respectively) from
S does not change the ranking of the remaining cues; e.g. scale and gender then become the dominant
features in respective datasets. See Figure B.1 that extends the main paper Figure 3."
REFERENCES,0.7717717717717718,DSprites
REFERENCES,0.7747747747747747,"S =

shape, scale
orientation "
REFERENCES,0.7777777777777778,"0
20
40
60
80
100 120
Epoch 0 20 40 60 80 100"
REFERENCES,0.7807807807807807,Accuracy
REFERENCES,0.7837837837837838,diagonal scale shape
REFERENCES,0.7867867867867868,orientation
REFERENCES,0.7897897897897898,diagonal avg. acc ±
REFERENCES,0.7927927927927928,orientation avg. acc ±
REFERENCES,0.7957957957957958,scale avg. acc ±
REFERENCES,0.7987987987987988,shape avg. acc ±
REFERENCES,0.8018018018018018,"0
20
40
60
80
100 120
Epoch 0 20 40 60 80 100"
REFERENCES,0.8048048048048048,Accuracy
REFERENCES,0.8078078078078078,diagonal scale shape
REFERENCES,0.8108108108108109,orientation
REFERENCES,0.8138138138138138,"0
20
40
60
80
100 120
Epoch 0 20 40 60 80 100"
REFERENCES,0.8168168168168168,Accuracy
REFERENCES,0.8198198198198198,diagonal scale shape
REFERENCES,0.8228228228228228,orientation
REFERENCES,0.8258258258258259,UTKFace
REFERENCES,0.8288288288288288,"S =

gender
age "
REFERENCES,0.8318318318318318,"0
20
40
60
80
100 120
Epoch 0 20 40 60 80 100"
REFERENCES,0.8348348348348348,Accuracy
REFERENCES,0.8378378378378378,diagonal
REFERENCES,0.8408408408408409,gender age
REFERENCES,0.8438438438438438,diagonal avg. acc ±
REFERENCES,0.8468468468468469,age avg. acc ±
REFERENCES,0.8498498498498499,gender avg. acc ±
REFERENCES,0.8528528528528528,"0
20
40
60
80
100 120
Epoch 0 20 40 60 80 100"
REFERENCES,0.8558558558558559,Accuracy
REFERENCES,0.8588588588588588,diagonal
REFERENCES,0.8618618618618619,gender age
REFERENCES,0.8648648648648649,"0
20
40
60
80
100 120
Epoch 0 20 40 60 80 100"
REFERENCES,0.8678678678678678,Accuracy
REFERENCES,0.8708708708708709,diagonal
REFERENCES,0.8738738738738738,gender age
REFERENCES,0.8768768768768769,diagonal avg. acc ±
REFERENCES,0.8798798798798799,age avg. acc ±
REFERENCES,0.8828828828828829,gender avg. acc ±
REFERENCES,0.8858858858858859,"FFnet
ResNet20
ViT"
REFERENCES,0.8888888888888888,"Figure B.1: Preferential ordering of cues. Diagonal and unbiased accuracies of models trained on the
diagonal training set Ddiag composed of the cues deﬁned by S and tested on the off-diagonal sets Dk where
k ∈S."
REFERENCES,0.8918918918918919,"C
LOSS CURVATURE AROUND SOLUTIONS"
REFERENCES,0.8948948948948949,"We supplement the visualization of loss landscape around solutions in Figure 4 of main paper with
quantitative estimation of the respective curvatures. The mean curvature of a surface deﬁned by
function f is deﬁned as:"
REFERENCES,0.8978978978978979,κ(f) := 1
REFERENCES,0.9009009009009009,D∆f := 1 D ∂2f
REFERENCES,0.9039039039039038,"∂x2
1
+ · · · + ∂2f ∂x2
D"
REFERENCES,0.9069069069069069,"
(C.1)"
REFERENCES,0.9099099099099099,"To make it computationally tractable for large D (e.g. DNN parameters), we estimate the curvature
with a Monte-Carlo estimation as done in Izmailov et al. (2018). For a point x ∈RD, a sufﬁciently
small scalar ϵ > 0, and a unit sphere SD−1 ⊂RD, the curvature is computed by:"
REFERENCES,0.9129129129129129,"κ(f, x, ϵ) ≈1"
REFERENCES,0.9159159159159159,"ϵ2 [Ef(x + ϵr) −f(x)]
where
r ∼Unif(SD−1).
(C.2)"
REFERENCES,0.918918918918919,How does this approximate the mean curvature? We provide the following proposition.
REFERENCES,0.9219219219219219,"Proposition 4. For a point x ∈RD, a scalar ϵ > 0, a twice-differentiable function f, and a unit
sphere SD−1 ⊂RD, the following quantity"
REFERENCES,0.924924924924925,"1
ϵ2 [Ef(x + ϵr) −f(x)]
where
r ∼Unif(SD)
(C.3)"
REFERENCES,0.9279279279279279,approximates the Laplacian ∆f up to a constant multiplier.
REFERENCES,0.9309309309309309,Proof. We begin with a Taylor expansion on f(x + ϵr):
REFERENCES,0.933933933933934,f(x + ϵr) = f(x) + ϵrT ∇f(x) + 1
REFERENCES,0.9369369369369369,"2ϵ2rT ∇2f(x)r + h(ϵ, r)
(C.4)"
REFERENCES,0.93993993993994,"where h(ϵ,r)"
REFERENCES,0.9429429429429429,"ϵ2
→0 as ϵ ↓0 for any r. Thus,"
REFERENCES,0.9459459459459459,"1
ϵ2 [Ef(x + ϵr) −f(x)] = 1"
E,0.948948948948949,"2E

rT ∇2f(x)r + h(ϵ, r) ϵ2"
E,0.9519519519519519,"
(C.5) = 1"
E,0.954954954954955,"2E[rT ∇2f(x)r] + o(ϵ)
(C.6)"
E,0.9579579579579579,"by Er = 0 and the Dominated Convergence Theorem for Lebesgue integrals (also noting that r is
deﬁned on a compact set SD−1)."
E,0.960960960960961,"We now apply a change of basis such that the Hessian ∇2f(x) is diagonalised: ∇2f(x) = QT ΛQ.
This is possible because the Hessian is a real, symmetric matrix. Since the change of basis do not
alter the distribution for r, we compute"
E,0.963963963963964,"E[rT ∇2f(x)r] = E[rT Λr] = Var(r1)λ1 + · · · + Var(rD)λD
(C.7)"
E,0.9669669669669669,"= Var(r1)(λ1 + · · · + λD) = Var(r1)tr(∇2f(x)) = Var(r1)∆f(x)
(C.8)"
E,0.96996996996997,"using the isotropicity of r and the preservation of the trace against changes of basis. We thus con-
clude
1
ϵ2 [Ef(x + ϵr) −f(x)] = C∆f(x) + o(ϵ)
(C.9)"
E,0.972972972972973,"where C is a constant independent of f and x.
■"
E,0.975975975975976,"Results.
Figure C.1 shows the approximated curvature at varying radius values ϵ around different
solutions for ResNet20. We have used 100 Monte-Carlo samples. We observe that for DSprites,
the curvature of the orientation loss surface tends to overshoot quickly as one walks away from the
solution point. Surfaces for other cues tend to be smoother. On UTKFace, gender tends to provide a
steeper surface than the other cues like age and ethnicity."
E,0.978978978978979,"0
20
40
60
80
100
120
140
160
Weight offset radius ( ) 0.002 0.000 0.002 0.004 0.006 0.008"
E,0.9819819819819819,Mean curvature ( )
E,0.984984984984985,"Color
Orientation
Scale
Shape"
E,0.987987987987988,"10
20
30
40
50
60
Weight offset radius ( ) 0.000 0.002 0.004 0.006 0.008 0.010"
E,0.990990990990991,Mean curvature ( )
E,0.993993993993994,"Age
Ethnicity
Gender"
E,0.996996996996997,Figure C.1: Mean curvature around different solutions. Left: DSprites. Right: UTKFace.
