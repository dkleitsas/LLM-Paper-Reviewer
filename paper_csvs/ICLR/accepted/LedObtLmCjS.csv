Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003663003663003663,"The dominant framework for off-policy multi-goal reinforcement learning in-
volves estimating goal conditioned Q-value function. When learning to achieve
multiple goals, data efﬁciency is intimately connected with generalization of the
Q-function to new goals. The de-facto paradigm is to approximate Q(s, a, g) us-
ing monolithic neural networks. To improve generalization of the Q-function, we
propose a bilinear decomposition that represents the Q-value via a low-rank ap-
proximation in the form of a dot product between two vector ﬁelds. The ﬁrst vector
ﬁeld, f(s, a), captures the environment’s local dynamics at the state s; whereas
the second component, φ(s, g), captures the global relationship between the cur-
rent state and the goal. We show that our bilinear decomposition scheme sub-
stantially improves data efﬁciency, and has superior transfer to out-of-distribution
goals compared to prior methods. Empirical evidence is provided on the simulated
Fetch robot task-suite, and dexterous manipulation with a Shadow hand."
INTRODUCTION,0.007326007326007326,"1
INTRODUCTION"
INTRODUCTION,0.01098901098901099,"Learning a Q-value function (Watkins & Dayan, 1992) is among the core problems in reinforcement
learning (RL) (Sutton & Barto, 2018). It is often desired to learn a goal-conditioned Q-value func-
tion (Kaelbling, 1993) that estimates rewards for multiple goals. In multi-goal RL, the Q-function
takes the form of Q(s, a, g) which indicates the utility of taking action a at state s for approaching
the goal g. To enable the agent to work across a wide range of goals, it is usual to train Q(s, a, g) for
a large set of goals. Such training requires substantially more data than learning a Q-value function
for a single goal (Schaul et al., 2015)."
INTRODUCTION,0.014652014652014652,"The data efﬁciency of learning Q(s, a, g) has a close relationship with the generalization of the
Q-function to new goals. To understand why, consider the training process where goals are sam-
pled from a pre-deﬁned training distribution. The target of Q-learning is to reduce the error on the
Q-function for all goals in the training set. At an intermediate point in the training process, the
agent would have encountered a subset of training goals. Suppose on these goals the error in ap-
proximating the Q-function is low. If the learned Q-function generalizes to new goals sampled in
subsequent training, less data would be required to correct the small errors in Q-function for these
goals. Conversely, poor generalization would necessitate collecting a large amount of data."
INTRODUCTION,0.018315018315018316,"Currently popular methods (Schaul et al., 2015; Andrychowicz et al., 2017) represent the Q(s, a, g)
with a monolithic neural network, shown in Figure 1a, whose ability to generalize to new goals
is solely derived from the neural network architecture. Because such models jointly process the
tuple (s, a, g), the Q-function fails to generalize on new combinations of previously seen individual
components. E.g, given training tuples, (sA, aA, gA) and (sA, aB, gB), the monolithic network
might fail to generalize to a new data (sA, aA, gB) despite that sA, aA, and gB have been seen at
least once during training."
INTRODUCTION,0.02197802197802198,"We proposed bilinear value networks (BVN) to improve such generalization by introducing an
inductive bias in the neural network architecture that disentangles the local dynamics from the ques-
tion of how to reach the goal. Concretely, we propose to represent the goal-conditioned Q-value"
INTRODUCTION,0.02564102564102564,"∗Equal contribution, order determined randomly. Authors are also with Computer Science and Artiﬁcial
Laboratory (CSAIL), and afﬁliated with the Laboratory for Information and Decision Systems (LIDS). Corre-
spondence to {zwhong,geyang}@csail.mit.edu"
INTRODUCTION,0.029304029304029304,Published as a conference paper at ICLR 2022 s a g
INTRODUCTION,0.03296703296703297,"Q
Q(s, a, g)"
INTRODUCTION,0.03663003663003663,(a) A Monolithic value network s a g
INTRODUCTION,0.040293040293040296,"Q(s, a, g)"
INTRODUCTION,0.04395604395604396,"ƒ(s, a)"
INTRODUCTION,0.047619047619047616,"φ(s, g)
s"
INTRODUCTION,0.05128205128205128,"(b) Bilinear value network
(c) Bilinear decomposition"
INTRODUCTION,0.054945054945054944,"Figure 1: Illustration of why Bilinear Value Networks (BVNs) improve generalization to new goals.
(a) The traditional monolithic goal-conditioned value function jointly models the inputs (s, a, g).
(b) Bilinear Value Networks (BVN) introduce an inductive bias in the Q-function computation by
disentangling the effect of actions at the current state from how to reach the goal. This is realized
by representing Q(s, a, g) = φ(s, g)T f(s, a), where the two vector ﬁelds represent “what happens
next”f(s, a) and “where to go next” φ(s, g). (c) A 2D navigation toy example, where state s and
goal g are both (x, y) positions. The task of this toy example is to reach the goal position g. Gray
arrows indicate the vector ﬁeld φ(s, g) evaluated at all states. The black arrow indicates the best
alignment with f(s, a)."
INTRODUCTION,0.05860805860805861,"function as interaction between two concerns, φ(s, g) (“where to go”) and f(s, a) (“what happens
next”), where each concern can be represented as a neural network. The architecture of BVN is il-
lustrated in Figure 1b. φ(s, g) models how to change the state to get closer to the goal, while f(s, a)
captures where the agent can move from the current state. The Q-function is computed as the align-
ment between these concerns: φ(s, g)T f(s, a). As the φ-network only requires state-goal (s, g) and
f-network simply needs state-action (s, a), the learned Q-value function can generalize to the new
data (sA, aA, gB) since both (sA, aA) and (sA, gB) are the seen data for f and φ, respectively. Such
decomposition is possible because the knowledge of local dynamics (f) is independent of where to
go (φ) and thus f(s, a) be re-used across goals."
INTRODUCTION,0.06227106227106227,"Another intuition about the proposed BVN decomposition is: BVN would predict a high Q-value
when the vector representation of the desired future state, φ(s, g), aligns with the vector representa-
tion of the future state, f(s, a), after executing action a. We illustrate this intuition with help of a
toy example shown in Figure 1c. Here the agent’s state s and the goal g are 2D positions, and the
task is to get close to g shown as a pink cross. As an analogy to “what happens next”, the black
arrow originating at state s (shown as the blue circle) denotes the succeeding state after taking an
action at state s, which is goal-agnostic. Independent of action, the grey arrows can be viewed as
the direction to the goal from the current state s (i.e., φ(s, g) or “where to go”). While this illustra-
tion was in 2D, we ﬁnd that the proposed bilinear value decomposition substantially improves data
efﬁciency (Section 5.1) and generalization to new goals (Section 5.2) in complex and standard simu-
lated benchmark tasks: (i) dexterous manipulation with a twenty degrees-of-freedom (DoF) shadow
hand and (ii) object pick-place tasks using the Fetch manipulator."
PRELIMINARIES,0.06593406593406594,"2
PRELIMINARIES"
PRELIMINARIES,0.0695970695970696,"We consider the multi-goal reinforcement learning problem (Kaelbling, 1993; Plappert et al., 2018),
formulated as a goal-conditioned Markov Decision Process (MDP). We assume inﬁnite horizon with
the discount factor γ, and use S, A, G for the state, action, and goal spaces. At the beginning of each
episode, an initial state s0 and a goal g is sampled according to the distribution ρS and ρG. The
goal g is ﬁxed throughout the episode. At every time step t, the agent takes an action according to
at ∼π(st, g), and receives the reward rt,g = R(st, at, g) while transitioning to the next state st+1.
The objective of multi-goal RL is to produce a policy π∗that maximizes the expected return J"
PRELIMINARIES,0.07326007326007326,"π∗= arg max
π
J(π) where J(π) = E
 ∞
X"
PRELIMINARIES,0.07692307692307693,"τ=t
γτ−trτ,g|st = s

,
(1)"
PRELIMINARIES,0.08058608058608059,Published as a conference paper at ICLR 2022
PRELIMINARIES,0.08424908424908426,"where the reward rτ,g is often structured as the follows:"
PRELIMINARIES,0.08791208791208792,"R(sτ, aτ, g) :=
0,
goal reached
−1,
otherwise.
∀τ ∈N
(2)"
PRELIMINARIES,0.09157509157509157,"Note that our proposed parameterization scheme, BVN, does not rely on this reward structure in
general. All of the baselines use a discount factor γ = 0.98."
OFF-POLICY RL WITH ACTOR-CRITIC METHODS,0.09523809523809523,"2.1
OFF-POLICY RL WITH ACTOR-CRITIC METHODS"
OFF-POLICY RL WITH ACTOR-CRITIC METHODS,0.0989010989010989,"It is common to use actor-critic algorithms such as deep deterministic policy gradient (DDPG) (Sil-
ver et al., 2014), twin-delayed DDPG (TD3) (Fujimoto et al., 2018) and soft actor-critic
(SAC) (Haarnoja et al., 2018), to solve multi-goal continuous state-action space tasks considered
this paper. In DDPG, the critic Q is learned by minimizing the temporal-difference (TD) loss:"
OFF-POLICY RL WITH ACTOR-CRITIC METHODS,0.10256410256410256,"L(Q) = E(st,at)∼Uniform(D)(rt,g + γQ(st+1, π(st+1, g), g) −Q(st, at, g))2,
(3)"
OFF-POLICY RL WITH ACTOR-CRITIC METHODS,0.10622710622710622,"where D is the the replay buffer (Mnih et al., 2015) containing state transition tuples
(st, at, rt, st+1, g). The actor π is trained to maximize the score from the critic, using the deter-
ministic policy gradient:"
OFF-POLICY RL WITH ACTOR-CRITIC METHODS,0.10989010989010989,"∇πJ(π) = E[∇aQ(s, a, g)|a=π(s,g)].
(4)"
OFF-POLICY RL WITH ACTOR-CRITIC METHODS,0.11355311355311355,"In multi-goal RL settings, we refer to the goal-conditioned critic as the universal value function
approximator (UVFA) (Schaul et al., 2015) because Q(s, a, g) approximates the expected utility for
all goals ∈G. Because the actor relies on the critic for supervision, the ability for the critic to
quickly learn and adapt to unseen goals is a key contributing factor to the overall sample efﬁciency
of the learning procedure. A critic that provides better value estimates using less sample and opti-
mization gradient steps, will lead to faster policy improvement via the gradient in Equation 4. For
data efﬁciency it is common to use off-policy actor-critic methods with hindsight experience replay
(HER) (Andrychowicz et al., 2017). It is known that TD3 and SAC outperform DDPG on single-
task continuous control problems. However, in the multi-goal setting when used along with HER,
we found DDPG to outperform both TD3 and SAC on the benchmark tasks we considered (see Sec-
tion B). We therefore use DDPG for majority of our experiments and report SAC/TD3 results in the
appendix."
BILINEAR VALUE NETWORKS,0.11721611721611722,"3
BILINEAR VALUE NETWORKS"
BILINEAR VALUE NETWORKS,0.12087912087912088,"(a) f(s, a)⊤φ(g)
(b) f(s, a)⊤φ(s, g)"
BILINEAR VALUE NETWORKS,0.12454212454212454,"Figure 2: Comparison between two value decomposi-
tion schemes. The task, states, and goals are the same
as Figure 1c. The black square indicates obstacles that
the agent cannot pass through. Gray arrows indicate
the vector ﬁeld φ evaluated at a particular goal g, for all
states s ∈S. (a) As in prior work Schaul et al. (2015),
the vector ﬁeld φ(g) is a constant and does not depend
on s. (b) BVN parameterizes φ as a function of both s
and g making it more expressive, but still maintaining
the beneﬁt of disentanglement from f(s, a)."
BILINEAR VALUE NETWORKS,0.1282051282051282,"Bilinear value network (BVN) factorizes
the value function approximator into a bi-
linear form from two vector representa-
tions into a scalar:"
BILINEAR VALUE NETWORKS,0.13186813186813187,"Q(s, a, g) = f(s, a)⊤φ(s, g),
(5)"
BILINEAR VALUE NETWORKS,0.13553113553113552,"where f : S × A 7→Rd and φ : S × G 7→
Rd. The ﬁrst component f(s, a) is a lo-
cal ﬁeld that only concerns the immediate
action at the current state s. The second
component is more global in the sense that
φ(s, g) captures long-horizon relationship
between the current state and the goal."
BILINEAR VALUE NETWORKS,0.1391941391941392,"Closely related to our work is the low-
rank bilinear decomposition, Q(s, a, g) =
f(s, a)⊤φ(g), proposed in Schaul et al.
(2015).
Though Schaul et al. (2015)
showed this decomposition attains bet-
ter data efﬁciency in simple environments
(e.g., gridworlds), this decomposition is
not expressive enough to model value functions in complex environments, which we will show"
BILINEAR VALUE NETWORKS,0.14285714285714285,Published as a conference paper at ICLR 2022
D MAZE,0.14652014652014653,2D Maze y
D MAZE,0.15018315018315018,"x
(a)
(b)
(c)"
D MAZE,0.15384615384615385,"Figure 3: (a) The U-maze environment where the states s and goals g are (x, y) on the plane. The
task is to reach the goal position (red circle). (b) The learned Q-value predictions Q(s, a∗, g) with
the best action a∗over states s in the 2-dimensional state space for a goal g indicated by the red
circle. Overall, values decrease as the agent gets further and further away from the goal. (c) The
latent vectors produced by the agent, projected to a 2D vector space. There is an overall trend for φ
to decrease in magnitude as one gets closer to the goal. (c Inset) Comparing the alignment of vector
representation of a random (suboptimal) action produced by the f component, f(s, arand) (pink)
against the vector corresponding to the optimal action from the policy π, f(s, a∗) (blue) from the
phi vector (black). The optimal alignment corresponds to an angle of 90◦because the maximum
Q-value in this environment is 0 and not 1. We ﬁnd that the optimal action consistently produces a
better alignment in comparison to a sub-optimal action."
D MAZE,0.1575091575091575,"in Section 5.2. Consequently, the majority of multi-goal RL algorithms use a monolithic network
shown in Figure 1a to represent the value function."
D MAZE,0.16117216117216118,"To motivate the design of our factorization, we present a vector analysis perspective on previous
value decomposition Schaul et al. (2015) and our method in Figure 2, where each point on the 2D
plane is a state s. In Figure 2a, we illustrate the vector ﬁeld of low-rank bilinear UVFA (Schaul
et al., 2015), where f : S × A 7→Rd and φ : G 7→Rd. It can be seen that φ(g) is a constant across
states since φ(g) does not depend on the state s in Figure 2a. This makes learning more challenging
due to restricted expressivity. The expressivity of low-rank bilinear UVFA is restricted since the
such a decomposition can only model linear relationships between states and goals. Consequently,
the burden of modeling different Q-values for different states falls completely on f. In contrast, in
Figure 2b, the ﬁeld φ(s, g) produced by BVN is also a function of the state s. This makes φ in BVN
strictly more expressive than low-rank bilinear UVFA that do not depend on s."
D MAZE,0.16483516483516483,"Bilinear value networks are a drop-in replacement for the monolithic neural networks for approxi-
mating universal value function approximator. Our implementation is not speciﬁc to a method and be
used to learn value function using a variety of methods such as DDPG, SAC, TD3 or even methods
that we do not make use of temporal differenceing."
D MAZE,0.1684981684981685,"3.1
A TOY EXAMPLE: VISUALIZING f(s, a) AND φ(s, g)"
D MAZE,0.17216117216117216,"To examine whether the learned bilinear value decomposition reﬂect our intuition mentioned in
Section 1, we trained a deep deterministic policy gradient (DDPG) (Lillicrap et al., 2016) with
hindsight experience replay (HER) (Andrychowicz et al., 2017) agent using BVN in a 2D maze
shown in Figure 3 (a), where s ∈R2, a ∈R2, g ∈R2, and the reward is the negative euclidean
distance between states and goals. The output dimensions of f and φ are both 16."
D MAZE,0.17582417582417584,"We computed the vectors z∗
f = f(s, a∗) and zφ = φ(s, g) densely on a grid of states (x, y) with
respect to the ﬁxed goal g, where a∗= π(s, g) is the optimal action predicted by the learned policy.
For visualization, we used principal component analysis (PCA) to project zφ to a 2-dimensional
vector and is plotted as black arrows in Figure 3(c). We cannot use PCA to plot z∗
f on the 2D plane
along with zphi because f(s, a∗) and φ(s, g) live in two different vector spaces. Instead, we plot
f(s, a∗) as a rotation from zφ denoted as ∠(f(s, a∗), φ(s, g)). Speciﬁcally, we ﬁrst plot the 2D
projection of zphi = φ(s, g) as a 2D vector and then plot z∗
f = f(s, a∗) by rotating the plotted zφ
by angle of ∠(f(s, a∗), φ(s, g))."
D MAZE,0.1794871794871795,Published as a conference paper at ICLR 2022
D MAZE,0.18315018315018314,"Further, Figure 3(c) shows that the length of zphi correlates to the distance between the state (s) and
the goal (g), providing further evidence in support of our hypothesis that φ(s, g) captures proximity
to the goal. If f(s, a) indeed captures the dynamics, then z∗
f (i.e., vector representation of future state
after executing the optimal action a∗) should be closely aligned to zphi to produce the maximal Q-
value. To test if this is the case, we also plot in pink zrand
f
denoting the vector representation of future
state after executing a random action arand. We ﬁnd that in comparison to f(s, arand), f(s, a∗)
better aligns with φ(s, g) validating our hypothesis. Note that the optimal alignment corresponds to
an angle of 90◦because the maximum Q-value in this environment is 0 and not 1. As a result, f for
the optimal action will be orthogonal to φ."
EXPERIMENTAL SETUP,0.18681318681318682,"4
EXPERIMENTAL SETUP"
EXPERIMENTAL SETUP,0.19047619047619047,"Tasks
We evaluate our method in all the multi-goal RL benchmarks in Plappert et al. (2018),
Fetch robot and Shadow hand manipulation tasks, as shown in Figure 10. For Fetch robot domain,
the robot is required to move the object to a target position. In Shadow hand domain, the agent
controls the joints of the hand’s ﬁngers and manipulates the objects in hand to a target pose. For all
tasks, we set discount factor γ = 0.98 based on Plappert et al. (2018). The details, including the
reward functions, of these tasks can be found in the Section A.2."
EXPERIMENTAL SETUP,0.19413919413919414,"Baselines
We compare with two baselines, UVFA (Monlithic) (Andrychowicz et al., 2017; Schaul
et al., 2015) and UVFA (Bilinear) proposed in Schaul et al. (2015). UVFA (Monlithic) is the most
common multi-goal value function architecture that parametrizes the Q-function by a monolithic
neural network. UVFA (Bilinear) is a bi-linear value function that decomposes the Q-function as
Q(s, a, g) = f(s, a)⊤φ(g), where f and φ denote two neural networks respectively. Our method
uses a different bilinear decomposition and we refer to it as BVN (Ours)."
EXPERIMENTAL SETUP,0.1978021978021978,"Implementation
The baselines and our method are trained by deep deterministic policy gradi-
ent (DDPG) (Lillicrap et al., 2016) with hindsight experience replay (HER) (Andrychowicz et al.,
2017). We use DDPG with HER as the base algorithm for our method and the baselines because it
outperforms other recent Q-learning algorithms (Haarnoja et al., 2018; Fujimoto et al., 2018) on the
task of learning a goal-conditioned Q-function. Comparison of DDPG against SAC (Haarnoja et al.,
2018) and TD3 (Fujimoto et al., 2018) is provided in Section B. For all baselines, the Q-function is
implemented as a three layer neural network with 256 neurons per layer. In our method, f and φ are
both implemented as a 3-layer neural network with 176 neurons to match the total number of pa-
rameters of the baselines’ Q-function. For BVN (Ours) and UVFA (Bilinear), the output dimensions
f, φ(s, g), and φ(g) are set as 16. The rest of hyperparameters for training are left in Section A.3."
EXPERIMENTAL SETUP,0.20146520146520147,"Evaluation metric
We ran each experiment with 5 different random seeds and report the mean
(solid or dashed line) and 95%-conﬁdence interval (shaded region) using bootstrapping method (Di-
Ciccio & Efron, 1996). Each training plot reports the average success rate over 15 testing rollouts
as a function of training epochs, where the goal is sampled in the beginning of each testing rollout.
Each epoch consists of a ﬁxed amount of data (see Section A.3 for details)."
RESULTS,0.20512820512820512,"5
RESULTS"
RESULTS,0.2087912087912088,"Our experiments answer whether BVN improves sample efﬁciency and generalizes better to unseen
goals. In Section 5.3, we provide ablation studies probing the importance of several design decisions."
SAMPLE EFFICIENCY AND ASYMPTOTIC PERFORMANCE,0.21245421245421245,"5.1
SAMPLE EFFICIENCY AND ASYMPTOTIC PERFORMANCE"
SAMPLE EFFICIENCY AND ASYMPTOTIC PERFORMANCE,0.21611721611721613,"Figure 4 compares the sample efﬁciency of our method, BVN (Ours), against baselines on the Fetch
tasks. The results show that our method has higher data efﬁciency in four out of eight tasks and
matches the baselines in other tasks. Our method also attains higher asymptotic performance in
Bin-place. Figure 5 shows that on the Shadow hand task suite, BVN improves the sample efﬁciency
in six out of the eight tasks and is on-par with the baselines for the other two tasks. These results
suggest that BVN consistently improves sample efﬁciency of learning goal-conditioned policies."
SAMPLE EFFICIENCY AND ASYMPTOTIC PERFORMANCE,0.21978021978021978,Published as a conference paper at ICLR 2022
SAMPLE EFFICIENCY AND ASYMPTOTIC PERFORMANCE,0.22344322344322345,"0
10
20
30
40
50
0% 25% 50% 75% 100%"
SAMPLE EFFICIENCY AND ASYMPTOTIC PERFORMANCE,0.2271062271062271,Success rate Reach
SAMPLE EFFICIENCY AND ASYMPTOTIC PERFORMANCE,0.23076923076923078,"0
25
50
75
100
125
150 Push"
SAMPLE EFFICIENCY AND ASYMPTOTIC PERFORMANCE,0.23443223443223443,"0
50
100
150
200 Slide"
SAMPLE EFFICIENCY AND ASYMPTOTIC PERFORMANCE,0.23809523809523808,"0
50
100
150
200"
SAMPLE EFFICIENCY AND ASYMPTOTIC PERFORMANCE,0.24175824175824176,Pick & Place
SAMPLE EFFICIENCY AND ASYMPTOTIC PERFORMANCE,0.2454212454212454,"0
25
50
75
100
Epoch 0% 25% 50% 75% 100%"
SAMPLE EFFICIENCY AND ASYMPTOTIC PERFORMANCE,0.2490842490842491,Success rate
SAMPLE EFFICIENCY AND ASYMPTOTIC PERFORMANCE,0.25274725274725274,Drawer-Open
SAMPLE EFFICIENCY AND ASYMPTOTIC PERFORMANCE,0.2564102564102564,"0
25
50
75
100
Epoch"
SAMPLE EFFICIENCY AND ASYMPTOTIC PERFORMANCE,0.2600732600732601,Drawer-Close
SAMPLE EFFICIENCY AND ASYMPTOTIC PERFORMANCE,0.26373626373626374,"0
25
50
75
100
Epoch"
SAMPLE EFFICIENCY AND ASYMPTOTIC PERFORMANCE,0.2673992673992674,Bin-place
SAMPLE EFFICIENCY AND ASYMPTOTIC PERFORMANCE,0.27106227106227104,"0
25
50
75
100
Epoch"
SAMPLE EFFICIENCY AND ASYMPTOTIC PERFORMANCE,0.27472527472527475,Bin-pick
SAMPLE EFFICIENCY AND ASYMPTOTIC PERFORMANCE,0.2783882783882784,"UVFA (Monolithic)
UVFA (Bilinear)
BVN (Ours)"
SAMPLE EFFICIENCY AND ASYMPTOTIC PERFORMANCE,0.28205128205128205,"Figure 4: Learning curves on the Fetch gym environments, and the Fetch-extension (see Section A.4)
task suit. BVN attained higher success rates using less data than the baselines in 5 out 8 domains.
Also, BVN achieved higher asymptotic performance in Bin-place."
SAMPLE EFFICIENCY AND ASYMPTOTIC PERFORMANCE,0.2857142857142857,"0
10
20
30
40
50 0% 25% 50% 75% 100%"
SAMPLE EFFICIENCY AND ASYMPTOTIC PERFORMANCE,0.2893772893772894,Success rate
SAMPLE EFFICIENCY AND ASYMPTOTIC PERFORMANCE,0.29304029304029305,Block Rotate Z
SAMPLE EFFICIENCY AND ASYMPTOTIC PERFORMANCE,0.2967032967032967,"0
25
50
75
100"
SAMPLE EFFICIENCY AND ASYMPTOTIC PERFORMANCE,0.30036630036630035,Block Parallel
SAMPLE EFFICIENCY AND ASYMPTOTIC PERFORMANCE,0.304029304029304,"0
25
50
75
100"
SAMPLE EFFICIENCY AND ASYMPTOTIC PERFORMANCE,0.3076923076923077,Block Rotate XYZ
SAMPLE EFFICIENCY AND ASYMPTOTIC PERFORMANCE,0.31135531135531136,"0
25
50
75
100 Block"
SAMPLE EFFICIENCY AND ASYMPTOTIC PERFORMANCE,0.315018315018315,"0
10
20
30
40
50
Epoch 0% 25% 50% 75% 100%"
SAMPLE EFFICIENCY AND ASYMPTOTIC PERFORMANCE,0.31868131868131866,Success rate
SAMPLE EFFICIENCY AND ASYMPTOTIC PERFORMANCE,0.32234432234432236,Egg Rotate
SAMPLE EFFICIENCY AND ASYMPTOTIC PERFORMANCE,0.326007326007326,"0
25
50
75
100
Epoch Egg"
SAMPLE EFFICIENCY AND ASYMPTOTIC PERFORMANCE,0.32967032967032966,"0
10
20
30
40
50
Epoch"
SAMPLE EFFICIENCY AND ASYMPTOTIC PERFORMANCE,0.3333333333333333,Pen Rotate
SAMPLE EFFICIENCY AND ASYMPTOTIC PERFORMANCE,0.336996336996337,"0
25
50
75
100
Epoch Pen"
SAMPLE EFFICIENCY AND ASYMPTOTIC PERFORMANCE,0.34065934065934067,"UVFA (Monolithic)
UVFA (Bilinear)
BVN (Ours)"
SAMPLE EFFICIENCY AND ASYMPTOTIC PERFORMANCE,0.3443223443223443,"Figure 5: Learning curves on the Shadow hand dexterous manipulation suite. BVN learns faster
than the baselines on six out of eight domains."
GENERALIZATION TO UNSEEN GOALS,0.34798534798534797,"5.2
GENERALIZATION TO UNSEEN GOALS"
GENERALIZATION TO UNSEEN GOALS,0.3516483516483517,"Our central hypothesis is that BVN increases data efﬁciency by generalizing better to new goals.
To test this, we split the training and evaluation goals in two ways in the Fetch robot domain: (1)
left-to-right and (2) near-to-far. In near-to-far scenario, all goals within a pre-specifed euclidean
distance are used for training (near) and goals sampled outside the threshold distance are used for
testing (i.e., far). The train-test split of goals is illustrated in Figure 6b by colors green and pink
respectively. For left-to-right scenario, we split the goal space into the left and right half-spaces
(see Figure 6a). During training, we disable HER to prevent the Q-function from receiving goals in
the testing split due to relabeling. We train the baselines and our method until they achieve similar
success rates (see Section A.3 for details). For testing, we ﬁnetune the learned Q-function and the
policy using the weights from the best model over 3 random seeds from training time. We ﬁnetune
the Q-function and the policy using HER."
GENERALIZATION TO UNSEEN GOALS,0.3553113553113553,"The ﬁne-tuning performance of the baselines and BVN is reported in Figure 6c. We ﬁnd that BVN
(shown in blue) quickly adapts to new goals and outperforms the monolithic UVFA (shown in grey).
This result suggests that BVN generalizes better than UVFA (Monlithic) to new goals."
GENERALIZATION TO UNSEEN GOALS,0.358974358974359,"Recall that in the BVN formulation, f(s, a) does not encode any goal speciﬁc information. We
therefore hypothesized that only ﬁnetuning φ(s, g) should be sufﬁcient for generalizing to new goals.
We call this version of ﬁnetuning as BVN (freeze f). Results in Figure 6c support our hypothesis"
GENERALIZATION TO UNSEEN GOALS,0.3626373626373626,Published as a conference paper at ICLR 2022
GENERALIZATION TO UNSEEN GOALS,0.3663003663003663,(a) Left & Right Train Test
GENERALIZATION TO UNSEEN GOALS,0.36996336996337,(b) Near & Far Test Train
GENERALIZATION TO UNSEEN GOALS,0.37362637362637363,(c) Learning curves of ﬁne-tuning in new goal distributions
GENERALIZATION TO UNSEEN GOALS,0.3772893772893773,"0
25 50 75 100 125 150 0% 25% 50% 75% 100%"
GENERALIZATION TO UNSEEN GOALS,0.38095238095238093,Success rate
GENERALIZATION TO UNSEEN GOALS,0.38461538461538464,(left-to-right) Push
GENERALIZATION TO UNSEEN GOALS,0.3882783882783883,"0
50
100
150
200 Slide"
GENERALIZATION TO UNSEEN GOALS,0.39194139194139194,"0
50
100
150
200"
GENERALIZATION TO UNSEEN GOALS,0.3956043956043956,Pick & Place
GENERALIZATION TO UNSEEN GOALS,0.3992673992673993,"0
25 50 75 100 125 150 Epoch 0% 25% 50% 75% 100%"
GENERALIZATION TO UNSEEN GOALS,0.40293040293040294,Success rate
GENERALIZATION TO UNSEEN GOALS,0.4065934065934066,(near-to-far)
GENERALIZATION TO UNSEEN GOALS,0.41025641025641024,"0
50
100
150
200
Epoch"
GENERALIZATION TO UNSEEN GOALS,0.4139194139194139,"0
50
100
150
200
Epoch"
GENERALIZATION TO UNSEEN GOALS,0.4175824175824176,"UVFA (Monolithic)
BVN (finetune f)
BVN (freeze f)"
GENERALIZATION TO UNSEEN GOALS,0.42124542124542125,"Figure 6: (a & b) Examples showing (a) how we split the goal distribution into left and right and (b)
according to the radius to the center (near versus far). We pretrain on the green regions, then ﬁnetune
on the red regions. The state distribution remain identical to the vanilla Fetch environment. (c) Fine-
tuning curves on unseen goals. BVN achieves better performance than the UVFA (Monlithic) on all
tasks in both left-to-right and near-to-far adaptation scenarios. Freezing f causes BVN performance
to plateau, indicating that both φ and f are needed for approximating the multi-goal value function."
GENERALIZATION TO UNSEEN GOALS,0.4249084249084249,"as BVN (freeze f) (orange) indeed outperforms UVFA (Monlithic). However, there is a signiﬁcant
performance gap between BVN (ﬁnetune f) and BVN (freeze f). This result indicates that while
BVN provides an inductive bias to disentangle the local dynamics, f(s, a), from the goal-speciﬁc
component, φ(s, g), such disentanglement is not fully learned. This could be due to several reasons
such as learning on limited data or because the disentangelment between f and φ is under-speciﬁed
upto a scalar factor (i.e., λf and 1"
GENERALIZATION TO UNSEEN GOALS,0.42857142857142855,"λφ yield the same result for a scalar λ), etc. Nonetheless, the
results convincingly demonstrate that the proposed bilinear decomposition increased data efﬁciency
and improves generalization. The topic of achieving perfect disentanglement is outside the scope of
current work and a worthwhile direction for future research."
ABLATION STUDIES,0.43223443223443225,"5.3
ABLATION STUDIES"
ABLATION STUDIES,0.4358974358974359,"How does latent dimension affect our method?
Though the dimensions of the latent space (i.e.,
output dimension of f and φ) are set as 16 across all the experiments above, we investigate the ef-
fect of the dimensionality of the latent space on the agent performance. Figure 7 shows the learning
curve for BVN with 3, 4, 8 and 16 dimensional latent spaces. The performance consistently im-
proves as latent dimensions increase, and even a BVN with just a 3-dimensional latent space still
outperforms the monolithic UVFA baseline. Hence bilinear value decomposition produces consis-
tent gain with respect to the monolithic UVFA baseline regardless of the latent dimensions, and
beneﬁts performance-wise from a larger latent space."
ABLATION STUDIES,0.43956043956043955,"Dot product “·” vs ℓ2 Metric ∥· ∥2
As it is unclear if the dot-product is the best choice to model
the interaction between two vector ﬁelds f and φ, we compare two alternative ways to decompose
the value function using the same variable grouping as the BVN. In the ﬁrst variant, we replace the
dot-product with an ℓ2 metric ∥· ∥2, and parameterize the Q function via Q(s, a, g) = −∥f(s, a) −
⃗φ(s, g)∥2. The negative sign is added because multi-goal RL tasks typically assume the reward is
within the range [−1, 0]. We set the latent dimension to be three as it is the best one found through
hyperparameter search (see Section A.4). In the second variant we linearly combine the two latent
vectors: Q(s, a, g) = wifi(s, a)+viφi(s, g), where xiyi implies a sum over the i indices implicitly.
This is the Einstein notation, or “einsum”. The results shown in Figure 8 shows that both the bilinear"
ABLATION STUDIES,0.4432234432234432,Published as a conference paper at ICLR 2022
ABLATION STUDIES,0.4468864468864469,"0
25
50
75
100 125 150
Epoch 0% 25% 50% 75% 100%"
ABLATION STUDIES,0.45054945054945056,Success rate Push
ABLATION STUDIES,0.4542124542124542,"0
50
100
150
200
Epoch"
ABLATION STUDIES,0.45787545787545786,Pick & Place
ABLATION STUDIES,0.46153846153846156,"MLP(s, a, g)
BVN(d=16)"
ABLATION STUDIES,0.4652014652014652,"BVN(d=8)
BVN(d=4)"
ABLATION STUDIES,0.46886446886446886,BVN(d=3)
ABLATION STUDIES,0.4725274725274725,"Figure 7: Increasing the latent dimension of f
and φ improves our method’s performance. De-
spite that, the result shows that a low dimensional
latent space (d = 3) is sufﬁcient to enable our
method to outperform the baseline UVFA (Mon-
lithic)."
ABLATION STUDIES,0.47619047619047616,"0
20
40
60
80 100 120 140
Epoch 0% 25% 50% 75% 100%"
ABLATION STUDIES,0.47985347985347987,Success rate Push
ABLATION STUDIES,0.4835164835164835,"0
50
100
150
200
Epoch"
ABLATION STUDIES,0.48717948717948717,Pick & Place
ABLATION STUDIES,0.4908424908424908,"⃗f
⊤·⃗g
⃗f
⃗g 2
w
ifi +v
igi"
ABLATION STUDIES,0.4945054945054945,"Figure 8: Replacing the dot-product with an ℓ2
norm does not affect the ﬁnal performance or the
sample complexity. The initial performance is af-
fected marginally. Whereas the linear combina-
tion is too restrictive."
ABLATION STUDIES,0.4981684981684982,"and the metric decomposition attain good performance. The ℓ2 metric decomposition is marginally
slower during the initial learning phase. The linear combination works poorly, likely due to the
restricted expressivity."
ABLATION STUDIES,0.5018315018315018,"Alternate Bi-Linear Decomposition
Lastly, we investigate if there are other viable ways to de-
compose the Q-function based on grouping the inputs s, a, g. An alternative hypothesis is that
any grouping of the inputs arguments (i.e., s, a, g) could improve the sample efﬁciency. To test
if this is the case, we compare our bilinear value decomposition scheme against two alternatives,
f(s, a)⊤φ(a, g) and f(s, a, g)⊤φ(g), as well as the scheme f(s, a)⊤φ(g) shown in Section 5.1. The
results in Figure 9 show that neither matches the performance of the bilinear value network, which
suggests correct grouping does play a crucial role. It is important to process s and g together and
duplicate the state input s on both branches."
ABLATION STUDIES,0.5054945054945055,"0
25
50
75
100
125
150
Epoch 0% 25% 50% 75% 100%"
ABLATION STUDIES,0.5091575091575091,Success rate Push
ABLATION STUDIES,0.5128205128205128,"0
100
200
300
400
500
Epoch Slide"
ABLATION STUDIES,0.5164835164835165,"0
50
100
150
200
Epoch"
ABLATION STUDIES,0.5201465201465202,Pick & Place
ABLATION STUDIES,0.5238095238095238,"f(s,a)
T (g)
f(s,a,g)
T (g)
f(s,a)
T (a,g)
f(s,a)
T (s,g)"
ABLATION STUDIES,0.5274725274725275,"Figure 9: Comparison between alternative factorizations show that our bilinear value decomposition
scheme, f(s, a)⊤φ(s, g), outperforms these alternatives. This suggests that this particular input
grouping is crucial."
RELATED WORKS,0.5311355311355311,"6
RELATED WORKS"
RELATED WORKS,0.5347985347985348,"A large number of prior works have explored ways to decompose value function to improve sample
efﬁciency of value function approximation. Classic works (Sutton & Barto, 2018; Littman et al.,
2001; Singh et al., 2003) approximate the value function in a set of linear basis. Successor fea-
tures (Dayan, 1993; Kulkarni et al., 2016; Borsa et al., 2019) factorize the value function as the dot
product between expectation of state-action feature occupancy and a vector of reward coefﬁcients
to the features.r Laplacian reinforcement learning (Mahadevan & Maggioni, 2006; 2007) represents
the value function using a set of Fourier basis. The above previous works investigate value function
decomposition in single task learning. Instead, bilinear value networks is focused on decomposing a
multi-goal value function by disentangling the representations for goals and actions. Also, bilinear"
RELATED WORKS,0.5384615384615384,Published as a conference paper at ICLR 2022
RELATED WORKS,0.5421245421245421,"value networks use end-to-end training to learn the decomposition instead of separating the basis
learning and the value approximation. To summarize prior works and their choice of parameteriza-
tion, we compile key publications in Table 1, ordered by the year of publication, in the appendix."
CONCLUSION AND DISCUSSION,0.5457875457875457,"7
CONCLUSION AND DISCUSSION"
CONCLUSION AND DISCUSSION,0.5494505494505495,"Our empirical evaluation shows signiﬁcant sample efﬁciency gain when bilinear value networks is
used with multi-goal RL tasks. Moreover, bilinear value networks does not introduce extra hyper-
paramters for training, except for the output dimensions of f and φ. Our ablations demonstrate that
the beneﬁts of BVN result from faster adaptation of the value function to new goals. An exciting
direction for future research is to facilitate transfer across different robots’ morphology and goal
spaces. One could keep the state and goal spaces ﬁxed and train a single φ(s, g) that is shared be-
tween the different morphologies. The difference between morphologies could be accommodated
by different f (e.g., different manipulators for the same task). Also, one can pre-train a shared f for
the same robot and train φ for different tasks (i.e., goal spaces). In addition, we further discuss the
additional implications of our experimental results in Section C of appendix."
CONCLUSION AND DISCUSSION,0.5531135531135531,ACKNOWLEDGMENTS
CONCLUSION AND DISCUSSION,0.5567765567765568,"We thank members of Improbable AI Lab for the helpful discussion and feedback. We are grate-
ful to MIT Supercloud and the Lincoln Laboratory Supercomputing Center for providing HPC re-
sources. The research in this paper was supported by the MIT-IBM Watson AI Lab, in part by
the Army Research Ofﬁce and was accomplished under Grant Number W911NF-21-1-0328 and by
the National Science Foundation AI Institute for Artiﬁcial Intelligence and Fundamental Interac-
tions ( https://iaiﬁ.org/) under Cooperative Agreement PHY-2019786. The views and conclusions
contained in this document are those of the authors and should not be interpreted as representing
the ofﬁcial policies, either expressed or implied, of the Army Research Ofﬁce or the United States
Air Force or the U.S. Government. The U.S. Government is authorized to reproduce and distribute
reprints for Government purposes notwithstanding any copyright notation herein."
REPRODUCIBILITY STATEMENT,0.5604395604395604,"8
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.5641025641025641,"We provide detailed instructions for reproducing the results in this paper in the Appendix. Please
refer to Section Section A.4."
REFERENCES,0.5677655677655677,REFERENCES
REFERENCES,0.5714285714285714,"Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob
McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience re-
play. In Advances in Neural Information Processing Systems, 2017. 1, 3, 4, 5, 11"
REFERENCES,0.575091575091575,"Diana Borsa, Andr´e Barreto, John Quan, Daniel Mankowitz, R´emi Munos, Hado van Hasselt, David
Silver, and Tom Schaul.
Universal successor features approximators.
In Proceedings of the
International Conference on Learning Representations, 2019. 8, 11"
REFERENCES,0.5787545787545788,"Peter Dayan. Improving generalization for temporal difference learning: The successor representa-
tion. Neural Comput., 5(4):613–624, 1993. ISSN 0899-7667. 8"
REFERENCES,0.5824175824175825,"Thomas J DiCiccio and Bradley Efron. Bootstrap conﬁdence intervals. Statistical science, 11(3):
189–228, 1996. 5"
REFERENCES,0.5860805860805861,"Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in
actor-critic methods. Proceedings of the International Conference on Machine Learning, 2018.
3, 5, 14"
REFERENCES,0.5897435897435898,"Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In Proceedings of the
International Conference on Machine Learning, 2018. 3, 5, 14"
REFERENCES,0.5934065934065934,Published as a conference paper at ICLR 2022
REFERENCES,0.5970695970695971,"Leslie Pack Kaelbling. Learning to achieve goals. In Proceedings of the International Joint Confer-
ence on Artiﬁcial Intelligence, 1993. 1, 2"
REFERENCES,0.6007326007326007,"Tejas D. Kulkarni, Ardavan Saeedi, Simanta Gautam, and Samuel J. Gershman. Deep successor
reinforcement learning. ArXiv, abs/1606.02396, 2016. 8, 11"
REFERENCES,0.6043956043956044,"Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In Pro-
ceedings of the International Conference on Learning Representations, 2016. 4, 5"
REFERENCES,0.608058608058608,"M Littman, R Sutton, and Satinder Singh. Predictive representations of state. 2001. 8, 11"
REFERENCES,0.6117216117216118,"Sridhar Mahadevan and Mauro Maggioni. Value function approximation with diffusion wavelets
and laplacian eigenfunctions. Advances in Neural Information Processing Systems, 2006. 8, 11"
REFERENCES,0.6153846153846154,"Sridhar Mahadevan and Mauro Maggioni. Proto-value functions: A laplacian framework for learn-
ing representation and control in markov decision processes. Journal of Machine Learning Re-
search, 2007. 8, 11"
REFERENCES,0.6190476190476191,"Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andy Ballard, Andrea Banino, Misha
Denil, Ross Goroshin, Laurent Sifre, Koray Kavukcuoglu, et al. Learning to navigate in complex
environments. Proceedings of the International Conference on Representation Learning, 2017.
11"
REFERENCES,0.6227106227106227,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature, 2015. 3"
REFERENCES,0.6263736263736264,"Silviu Pitis, Harris Chan, Stephen Zhao, Bradly Stadie, and Jimmy Ba. Maximum entropy gain ex-
ploration for long horizon multi-goal reinforcement learning. In Proceedings of the International
Conference on Machine Learning, 2020. 14"
REFERENCES,0.63003663003663,"Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Pow-
ell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, et al. Multi-goal reinforce-
ment learning: Challenging robotics environments and request for research.
arXiv preprint
arXiv:1802.09464, 2018. 2, 5, 11, 12, 14, 15"
REFERENCES,0.6336996336996337,"Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approxima-
tors. In Proceedings of the International Conference on Machine Learning, 2015. 1, 3, 4, 5, 11,
12"
REFERENCES,0.6373626373626373,"David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms.
In International Conference on Machine Learning,
2014. 3"
REFERENCES,0.6410256410256411,"Satinder P Singh, Michael L Littman, Nicholas K Jong, David Pardoe, and Peter Stone. Learning
predictive state representations. Proceedings in the International Conference on Machine Learn-
ing, 2003. 8, 11"
REFERENCES,0.6446886446886447,"Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. MIT Press, Oc-
tober 2018. ISBN 9780262352703. URL https://play.google.com/store/books/
details?id=uWV0DwAAQBAJ. 1, 8, 11"
REFERENCES,0.6483516483516484,"Richard S Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick M Pilarski, Adam White,
and Doina Precup. Horde: A scalable real-time architecture for learning knowledge from unsuper-
vised sensorimotor interaction. In Proceedings of the International Conference on Autonomous
Agents and Multiagent Systems, 2011. 11"
REFERENCES,0.652014652014652,"Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3):279–292, 1992. 1"
REFERENCES,0.6556776556776557,Published as a conference paper at ICLR 2022
REFERENCES,0.6593406593406593,Table 1: Comprehensive list of linear value decomposition schemes from the literature.
REFERENCES,0.663003663003663,"Reference
Parameterization"
REFERENCES,0.6666666666666666,"Linear basis (Sutton & Barto; Littman et al.; Singh et al.)
Q(s, a) = P ciQi(s, a)
Proto-Value Function (Mahadevan & Maggioni, 2006; 2007)
V (s) = P λiV i(s)
General Value Approximator (GVF/Horde, Sutton et al. 2011)
Q(s, a, θ) = θ⊤φ(s, a)
Successor Representation (SR, Kulkarni et al. 2016)
Q(s, a) = M(s, a, s′)R(s′)"
REFERENCES,0.6703296703296703,"Monolithic MLP UVFA (Schaul et al.; Andrychowicz et al.)
MLP(s, a, g)
Recurrent UVFA (Mirowski et al., 2017)
RNN(zs, a, zg), zs, zg are embeddings
Low-rank Bilinear UVFA (Schaul et al., 2015)
Q(s, a, g) = f(s, a)⊤w(g)
USFA (Borsa et al., 2019)
Q(s, a, ⃗z) = f(s, a, ⃗z)⊤⃗w"
REFERENCES,0.673992673992674,"Bilinear Value Network (ours)
Q(s, a, g) = f(s, a)⊤φ(s, g)"
REFERENCES,0.6776556776556777,"A
APPENDIX"
REFERENCES,0.6813186813186813,"A.1
DETAILS ON PREVIOUS LINEAR DECOMPOSITION SCHEMES"
REFERENCES,0.684981684981685,We present the comparison of parameterizations in Table 1.
REFERENCES,0.6886446886446886,"A.2
IMPLEMENTATION DETAILS OF TASKS"
REFERENCES,0.6923076923076923,"All experiments in Section 5.1 happen on the standard object and dexterous manipulation tasks from
the gym robotics suite (Plappert et al., 2018)."
REFERENCES,0.6959706959706959,"We brieﬂy summarize the speciﬁcations of the tasks in the list below and illustrate these tasks in
Figure 10:"
REFERENCES,0.6996336996336996,Table 2: Environment Speciﬁcations.
REFERENCES,0.7032967032967034,"Fetch Robot
Shadow Hand
Task
S
A
G
Task
S
A
G"
REFERENCES,0.706959706959707,"Reach
R10
R3
R3
BlockRotateZ
R61
R20
R7"
REFERENCES,0.7106227106227107,"Push
R25
R3
R3
BlockRotateParallel
R61
R20
R7"
REFERENCES,0.7142857142857143,"PickAndPlace
R25
R4
R3
BlockRotateXYZ
R61
R20
R7"
REFERENCES,0.717948717948718,"Slide
R25
R3
R3
BlockRotate
R61
R20
R7"
REFERENCES,0.7216117216117216,"DrawerOpen
R25
R3
R3
Egg
R61
R20
R7"
REFERENCES,0.7252747252747253,"DrawerClose
R25
R3
R3
EggRotate
R61
R20
R7"
REFERENCES,0.7289377289377289,"Pen
R61
R20
R7"
REFERENCES,0.7326007326007326,"PenRotate
R61
R20
R7"
REFERENCES,0.7362637362637363,"In addition to (Plappert et al., 2018), we also use the tasks from Fetch extension1. For left/right
environments used in Section 5.2, the goal (x, y, z) with x < 0 are left goals, and x ≥0 right goals.
In near/far environments, we set the distance thresholds as 0.2 for Slide, 0.2 for PickAndPlace, 0.1
for Reach, and 0.15 for Push. The goals with distance below the threshold are considered as near
goals, otherwise far goals."
REFERENCES,0.73992673992674,"A.3
TRAINING HYPERPARAMETERS"
REFERENCES,0.7435897435897436,"Fetch
We adapted the hyperparameters in (Plappert et al., 2018) for training in a single desktop.
In Plappert et al. (2018), the agent is trained by multiple workers collecting data in parallel, where
each worker collects rollouts using the same policy with different random seeds. The performances
of the agent trained by our hyperparameters match that in (Plappert et al., 2018)."
REFERENCES,0.7472527472527473,"• num workers: 2 for Reach, 8 for Push, 16 for Pick & Place, and 20 for Slide."
REFERENCES,0.7509157509157509,• Batch size: 1024
REFERENCES,0.7545787545787546,1https://github.com/geyang/gym-fetch
REFERENCES,0.7582417582417582,Published as a conference paper at ICLR 2022
REFERENCES,0.7619047619047619,"Figure 10: Renderings of the fetch robot and shadow hand domains considered in this work. Left-
to-right: (top) Fetch push, pick-and-place, slide, bin-pick, box-open; (bottom) Shadow hand block,
egg, pen. For Fetch, the goal is to move the gray object to the red dot by controlling the arm. For
Shadow hand, the objective is to reorient the object (e.g., block, egg, and pen) to the desired poses
rendered by the transparent object."
REFERENCES,0.7655677655677655,Table 3: The success rates of each method in the pretrained tasks used in Section 5.2.
REFERENCES,0.7692307692307693,"Left
Near"
REFERENCES,0.7728937728937729,"Push
Pick & Place
Slide
Push
Pick & Place
Slide"
REFERENCES,0.7765567765567766,"UVFA (Monolithic)
96%
50%
81%
93%
91%
96%
BVN
96%
59%
48%
95%
96%
90%"
REFERENCES,0.7802197802197802,• Warm up rollouts: We collected 100 initial rollouts for preﬁlling the replay buffer.
REFERENCES,0.7838827838827839,"• Training frequency: We train the agent per 2 environment steps. Each iteration of
training uses the num workers batches for training."
REFERENCES,0.7875457875457875,"The rest of hyperparameters remain the same as that used in (Plappert et al., 2018)."
REFERENCES,0.7912087912087912,"Shadow hand
We use the hyperparameters used in (Plappert et al., 2018) and reuse the open
source code2."
REFERENCES,0.7948717948717948,"Adaptation experiments (Section 5.2)
We present the success rates after pretraining in Table 3.
Note that in the pretraining stage of each method and task, we trained the agents with 5 different
random seeds and took the best-performing models as the initialization at the ﬁne-tuning time."
REFERENCES,0.7985347985347986,"A.4
IMPLEMENTATION DETAILS"
REFERENCES,0.8021978021978022,"Q-function architecture
The monolithic network baselines in this paper represents the Q-function
as a multi-layer perceptron (MLP) network that takes in s, a, and g concatenated together as a single
vector. The bilinear UVFA (Schaul et al., 2015) uses an MLP f that takes the concatenated s and a as
inputs, and an MLP φ using g as inputs. Our biliniear value network on the other hand, concatenate
[s, a] together as the input to f, and [s, g] as the inputs of φ."
REFERENCES,0.8058608058608059,"Policy architecture
We use an MLP as the policy network for all the methods."
REFERENCES,0.8095238095238095,"For our method and its variants presented in Section 5.3 , we use the following nomenclature to
denote them:"
REFERENCES,0.8131868131868132,1. Dot product This is our proposed method. We set the outputs’ dimension of f and φ as 16.
REFERENCES,0.8168498168498168,2https://github.com/TianhongDai/hindsight-experience-replay
REFERENCES,0.8205128205128205,Published as a conference paper at ICLR 2022
REFERENCES,0.8241758241758241,"2. 2-norm We parametrize Q-function as Q(s, a, g) = ∥f(s, a) −φ(s, g)∥2. We use set the dimen-
sion of the outputs from f and φ as 3. Figure 11 shows that d = 3 works the best for 2-norm."
REFERENCES,0.8278388278388278,"3. concat We concatenate f(s, a) and φ(s, g), feed the concatenated vectors to an MLP layer, and
take the outputs of this layer as the Q-value Q(s, a, g). The dimensions of the outputs of f and φ
are 16."
REFERENCES,0.8315018315018315,"0
25
50
75 100 125 150
Epoch 0% 25% 50% 75% 100%"
REFERENCES,0.8351648351648352,Success rate Push
REFERENCES,0.8388278388278388,"0
50
100
150
200
Epoch"
REFERENCES,0.8424908424908425,Pick & Place
REFERENCES,0.8461538461538461,"UVFA
d=16
d=8
d=4
d=3"
REFERENCES,0.8498168498168498,Figure 11: 3-dimensional latent space works the best for 2-norm variant (see Section A.4).
REFERENCES,0.8534798534798534,"A.5
TRANSFERRING ACROSS TASKS"
REFERENCES,0.8571428571428571,"In multi-goal RL tasks, we observe that most of tasks are identical in terms of high-level goals, but
they have different environment dynamics. For instance, the objective of ”Push” and ”Slide” are
all to move the object to the destination. They only differ in the friction coefﬁcient of the table
where the agent cannot slide objects in ”Push” because of high friction. Because we decouple
the state-action and goals into two different functions, f(s, a) and φ(s, g), one potential beneﬁt
on multi-goal RL is reusing φ for transferring to tasks with the same (or similar) task objectives
(i.e., reward speciﬁcation). Reusing φ(s, g) exploits the learned reward-speciﬁc state-goal relations.
Then we simply need to learn a new f to recover the reward-agnostic relations in the new task. To
examine this hypothesis, we select three tasks with the same state and action spaces3, and measure
the performance of transferring amongst these tasks. In transferring an agent from a source task to
a target task, we initialize UVFA (Monlithic) and BVN (no reset) agent’s policy and Q-function by
the neural networks’ weights learned in the source task and ﬁne-tune them in the target task. For
BVN (reset f), we reinitialize f and ﬁne-tune φ when training them in the target task. Figure 12
shows the success rates of transferring from a source task to a target task. For example, ”Push to
Slide” indicates transferring from ”Push” to ”Slide”. It can be seen that BVN (reset f) attains higher
success rates at convergence than the baselines and BVN (no reset) in 4 out 6 tasks (i.e., ”Push to
Slide”, ”Pick & Place to Push”, ”Push to Pick & Place”, and ”Pick & Place to Slide”)."
REFERENCES,0.8608058608058609,"The limited efﬁciency gain could result from that φ is dependent on states s and thus φ cannot be
effectively reused in new tasks. Even though the target task’s reward function is the same as that
in the source task, the relation of states and goals can be affected by the dynamics. For instance,
suppose we have a state where the object is near the goal. In ”Pick & Place”, because the friction
coefﬁcients of the table are high, the φ(s, g) should direct the agent to lift the object. On the other
hand, in ”Slide”, the φ(s, g) is expected to guide the agent to push the object to slide."
REFERENCES,0.8644688644688645,"3If the two tasks have different state/action spaces, we will not be able to use the weights from a task and
resume training in another task."
REFERENCES,0.8681318681318682,Published as a conference paper at ICLR 2022
REFERENCES,0.8717948717948718,"0
25
50
75
100
125
150
Epoch 0% 25% 50% 75% 100%"
REFERENCES,0.8754578754578755,Success Rate
REFERENCES,0.8791208791208791,Pick&Place => Push
REFERENCES,0.8827838827838828,"0
50
100
150
200
Epoch 0% 25% 50% 75% 100%"
REFERENCES,0.8864468864468864,Success Rate
REFERENCES,0.8901098901098901,Push => Pick&Place
REFERENCES,0.8937728937728938,"0
50
100
150
200
Epoch 0% 25% 50% 75% 100%"
REFERENCES,0.8974358974358975,Success Rate
REFERENCES,0.9010989010989011,Pick&Place => Slide
REFERENCES,0.9047619047619048,"0
50
100
150
200
Epoch 0% 25% 50% 75% 100%"
REFERENCES,0.9084249084249084,Success Rate
REFERENCES,0.9120879120879121,Push => Slide
REFERENCES,0.9157509157509157,"0
25
50
75
100
125
150
Epoch 0% 25% 50% 75% 100%"
REFERENCES,0.9194139194139194,Success Rate
REFERENCES,0.9230769230769231,Slide => Push
REFERENCES,0.9267399267399268,"0
50
100
150
200
Epoch 0% 25% 50% 75% 100%"
REFERENCES,0.9304029304029304,Success Rate
REFERENCES,0.9340659340659341,Slide => Pick&Place
REFERENCES,0.9377289377289377,"MLP (no reset)
BVN (no reset)
BVN (reset f)
Figure 12: The two-stream architecture of the BVN allows us to improve transfer by resetting f. Re-
setting f and ﬁne-tuning φ achieve higher success rate than the baselines when transferring between
push and pick & place, and to slide."
REFERENCES,0.9413919413919414,"0
10k
20k
30k
40k
50k
Steps 0% 25% 50% 75% 100%"
REFERENCES,0.945054945054945,Success rate Reach
REFERENCES,0.9487179487179487,"0
100k
200k
300k
400k
500k
Steps Push"
REFERENCES,0.9523809523809523,"0
500k
1.0M
1.5M
2.0M
Steps Slide"
REFERENCES,0.9560439560439561,"0
500k
1.0M
1.5M
Steps"
REFERENCES,0.9597069597069597,Pick & Place
REFERENCES,0.9633699633699634,"DDPG + BVN
DDPG + MLP
TD3 + MLP
SAC + MLP
Figure 13: We show that our BVN improves the sample efﬁciency over the best baseline, DDPG +
MLP, in 4 commonly used environments in multi-goal RL."
REFERENCES,0.967032967032967,"B
COMPARISON AGAINST SOTA ALGORITHMS"
REFERENCES,0.9706959706959707,"Bilinear value networks achieves State-Of-The-Art (SOTA) performance in common multi-goal re-
inforcement learning tasks (Plappert et al., 2018). In ﬁgure 13, we compare bilinear value networks
against two strong baselines: soft actor critic (SAC, see Haarnoja et al. 2018) and twin-delayed
deep deterministic policy gradient (TD3, see Fujimoto et al. 2018). We reuse the hyperparameters
reported in Fujimoto et al. 2018) and implement TD3 in our codebase. As for SAC, we use the
implementation in this codebase4. Figure 13 shows the performance of each baseline algorithm and
our method (DDPG + BVN). We show that our DDPG + BVN outperforms the best baseline, and
ﬁnd that DDPG works the best among all the MLP variants. Our ﬁndings align with (Pitis et al.,
2020) and the documentation in its released code5. Pitis et al. (2020) also found that DDPG works
better than TD3 and SAC in multi-goal RL domains."
REFERENCES,0.9743589743589743,"C
ADDITIONAL DISCUSSION"
REFERENCES,0.978021978021978,"Why does other parameter grouping schemes fail?
In Section 5.3,
we showed that
f(s, a, g)T φ(g) inputs grouping fails to improve sample efﬁciency. One explanation to this result
is that f(s, a, g) cannot prevent negative interference from new goals encountered during training"
REFERENCES,0.9816849816849816,4https://github.com/spitis/mrl
REFERENCES,0.9853479853479854,Published as a conference paper at ICLR 2022
REFERENCES,0.989010989010989,"since goals g couple with states-actions s, a. As a result, the merit of bi-linear value function decom-
position vanishes and even causes extra burden on learning because the number of model parameters
is larger than UVFA (Monlithic). This preliminary ﬁnding shows that to beneﬁt from decomposition,
the guideline is to only expose necessary inputs to each component. Redundant inputs in either
component could break the decomposition."
REFERENCES,0.9926739926739927,"How is 2-norm compared with dot product?
Section 5.3 demonstrated that using 2-norm to
reduce f(s, a) and φ(s, g) into Q-value matches the performance of our BVN with even lower
dimensions of Zf and Zφ. However, 2-norm relies on an assumption that the reward function must
be non-positive or non-negative since norm is deﬁned as a non-negative value. The ∥f(s, a) −
φ(s, g)∥can only ﬁt to either non-positive or non-negative Q-value by manipulating the sign of norm.
On the other hand, dot product is more expressive and free from the reward function assumption."
REFERENCES,0.9963369963369964,"Can our method facilitate policy transfer?
Since our method decomposes the value function into
a local (i.e., goal-agnostic) and a global (i.e., goal-speciﬁc) component, our BVN might facilitate
policy transfer by resetting either one of components and ﬁne-tuning another one. As multi-goal
tasks considered in (Plappert et al., 2018) have the same objective in nature but different dynamics,
one potential advantage of our method is to reuse φ(s, g) in the new task and re-train f(s, a). The
results in Section A.5 showed that the agent transferred by BVN converges to the the performance of
training from scratch in the target task slightly faster than the baselines. One reason for the limited
policy transferring performance is that φ(s, g) conditions on states s. Consequently, environment
dynamics shifts could affect the representations of φ. We, nevertheless, showed that states variables
are crucial in φ. One exciting avenue is to disentangle the representations for keeping the rich
expressivity as well as desired modularity for transferring across tasks."
