Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0035335689045936395,"Moving beyond testing on in-distribution data, works on Out-of-Distribution
(OOD) detection have recently increased in popularity. A recent attempt to catego-
rize OOD data introduces the concept of near and far OOD detection. Speciﬁcally,
prior works deﬁne characteristics of OOD data in terms of detection difﬁculty. We
propose to characterize the spectrum of OOD data using two types of distribu-
tion shifts: covariate shift and concept shift, where covariate shift corresponds to
change in style, e.g., noise, and concept shift indicates change in semantics. This
characterization reveals that sensitivity to each type of shift is important to the
detection and conﬁdence calibration of OOD data. Consequently, we investigate
score functions that capture sensitivity to each type of dataset shift and methods
that improve them. To this end, we theoretically derive two score functions for
OOD detection, the covariate shift score and concept shift score, based on the
decomposition of KL-divergence for both scores, and propose a geometrically-
inspired method (Geometric ODIN) to improve OOD detection under both shifts
with only in-distribution data. Additionally, the proposed method naturally leads
to an expressive post-hoc calibration function which yields state-of-the-art cali-
bration performance on both in-distribution and out-of-distribution data. We are
the ﬁrst to propose a method that works well across both OOD detection and cali-
bration, and under different types of shifts."
INTRODUCTION,0.007067137809187279,"1
INTRODUCTION"
INTRODUCTION,0.01060070671378092,"Out-of-distribution (OOD) detection is a fundamental research area important for downstream tasks
with open-world assumptions such as continual learning (Smith et al., 2021), open-set learning (Yu
et al., 2020) and safety-critical applications such as self-driving (Bojarski et al., 2016). However,
common OOD detection benchmarks1 are not well categorized. This prevents us from more sys-
tematically studying of OOD detection. A recent attempt (Winkens et al., 2020; Fort et al., 2021) to
provide a more granular characterization of OOD data introduces the concept of near OOD (Winkens
et al., 2020; Fort et al., 2021). Near OOD detection is posed as a more challenging problem than
far OOD detection because near OOD datasets usually share similar semantics, and style, e.g., all
natural images of similar environment, as the training dataset. Existing works characterize OOD
in terms of difﬁculty of OOD detection such as Confusion Log Probability (CLP) (Winkens et al.,
2020). However, this characterization only conveys difﬁculty of OOD detection across a single
dimension and does not expose intrinsic characteristics of OOD data."
INTRODUCTION,0.014134275618374558,"In this paper we provide a more systematic categorization of OOD data, which motivates a more
robust OOD detection method and helps us reﬂect on some existing approaches. Speciﬁcally, there
are at least two dimensions along which we can characterize the spectrum of OOD data. Intuitively,
out-of-distribution data refers to data that are sampled from distributions different from the train-
ing distribution. This is well known in machine learning as distribution shift (Moreno-Torres et al.,
2012). There are two dominant shift types: covariate shift2 and concept shift. The former usually"
INTRODUCTION,0.0176678445229682,"1Texturess (Cimpoi et al., 2014), SVHN (Netzer et al., 2011), Place365 (Zhou et al., 2017), LSUN-
Crop/Resize (Yu et al., 2015), and iSUN (Xu et al., 2015)
2Covariate shift is often associated with model calibration (Ovadia et al., 2019; Chan et al., 2020)"
INTRODUCTION,0.02120141342756184,"refers to change in style, e.g., clean to noised and natural images to cartoon, and the latter refers to
change in semantics, e.g., dog to cat. Even though existing works Hsu et al. (2020) have acknowl-
edged the different types of OOD datasets and the difﬁculty of detecting them, their methods do
not distinguish them and do not explicitly disentangle them. To study these two distribution shifts,
we propose to create a benchmark with multiple magnitude of distribution shift in each dimension.
Speciﬁcally, we use corrupted CIFAR10C/CIFAR100C (Hendrycks & Dietterich, 2019), originally
developed for testing robustness, with varying degrees of severity to represent increasing covariate
shift; we also introduce a new benchmark, CIFAR100 Splits, which divides the CIFAR100 dataset
into 10 splits with increasing conceptual shift from CIFAR10 classes, measured by semantic simi-
larity in a word embedding space (Pennington et al., 2014). We show that the increasing conceptual
difference measured by semantic similarity in language translates to a spectrum of OOD datasets
under gradual concept shift measured by the difﬁculty of OOD detection in the image space."
INTRODUCTION,0.024734982332155476,"To address these two distribution shifts, we need to approach it from two aspects: representation and
modeling, analogous to the role of entropy and Bayesian inference in uncertainty quantiﬁcation (De-
peweg et al., 2018) where entropy is a score function that represents uncertainty and Bayesian in-
ference is a method that models uncertainty (H¨ullermeier & Waegeman, 2021). To represent these
two shifts, we can derive score functions which output scalars indicating the severity of distribution
shift. Speciﬁcally we derive two score functions by expanding and decomposing the KL divergence
between a predictive distribution from a classiﬁer and a discrete uniform distribution. Arising from
the decomposition is a covariate shift score which is a function of feature norms and a concept shift
score which is a function of feature angles. To model these two shifts, and motivated by the covariate
score (a function of norms) and the concept score (a function of angles), we propose to directly im-
prove the sensitivity of norms and angles to distribution shift through a geometric perspective (Tian
et al., 2021). Speciﬁcally, we adopt the Geometric Sensitivity Decomposition proposed in Tian et al.
(2021), originally developed for model calibration, which decomposes norms and angles into a vari-
ance component and a scalar offset, and improve it by parametrizng the scalar offsets as standalone
networks. Our approach improves detection of OOD data under both covariate and concept shifts."
INTRODUCTION,0.028268551236749116,"While OOD detection and calibration3 has been studied separately in the literature, they share the
same underlying motivation: conﬁdence/uncertainty should be low/high when distribution shift oc-
curs. Our method to improve OOD detection naturally yields a powerful calibration function in the
family of intra class-preserving functions (Rahimi et al., 2020). This class of functions provides
enough expressive power to calibrate complex decision boundaries in neural networks and proves to
be effective in calibration on in-distribution data. Therefore, we apply a post-calibration (Guo et al.,
2017) method on a validation set as described in those works. Additionally, thanks to the improved
sensitivity to distribution shift, our model also is on par with state-of-the-art calibration performance
on distribution shifted data as well. To summarize our contributions:"
INTRODUCTION,0.03180212014134275,"• We propose to characterize the spectrum of OOD data in terms of covariate and concept
shift and unify the notion of near OOD in different literature (Sec. 3.1). Additionally, we
construct a new benchmark (e.g., CIFAR100 Splits) to represent gradual distribution shift
in each direction (Sec. 4.1.2 and 4.1.3).
• To represent distribution shifts, we analytically derive two OOD score functions based on
KL-divergence to capture both shifts more effectively (Sec. 3.2).
• To model distribution shifts, we propose Geometric ODIN to improve the sensitivity of
neural networks during training (Sec. 3.3) and calibration during inference (Sec. 3.4). This
method achieves state-of-the-art performance on both detection (Sec. 4.1) and calibration
(Sec. 4.2) of OOD data."
RELATED WORK,0.0353356890459364,"2
RELATED WORK"
RELATED WORK,0.038869257950530034,"Out-of-Distribution (OOD) detection methods can be largely divided into two camps depending on
whether they require OOD data during training. Hendrycks et al. (2018); Thulasidasan et al. (2020);
Roy et al. (2021) leverages anomalous data in training. Our method belongs to the class of methods
that do not assume the availability of OOD data during training. Hendrycks & Gimpel (2016) uses
the maximum softmax probability (MSP) to detect incorrect predictions and OOD data. Lee et al."
RELATED WORK,0.04240282685512368,3OOD detection focuses on concept-shifted data while calibration focuses on covariate-shifted data.
RELATED WORK,0.045936395759717315,"Figure 1: Illustration of near and far Out-of-Distribution data. Top left: CIFAR10 dataset (training).
Top right: CIFAR100 dataset. Lower left: Corrupted CIFAR10. Lower right: SVHN dataset."
RELATED WORK,0.04946996466431095,"(2018) proposes to use Mahalanobis distance by ﬁtting a Gaussian mixture model (GMM) in the
feature space. Mukhoti et al. (2021) uses log density of the GMM model instead. Liu et al. (2020b)
uses an energy score as the uncertainty metric. ODIN (Liang et al., 2017) uses a combination of input
processing and post-training tuning to improve OOD detection performance. Generalized ODIN
Hsu et al. (2020) (also Techapanurak et al. (2019)) includes an additional network in the last layer
to improve OOD detection during training. There are other interesting OOD detection approaches
without OOD data such as using contrastive learning with various transformations (Winkens et al.,
2020; Tack et al., 2020), training a deep ensemble of multiple models (Lakshminarayanan et al.,
2016) and leveraging large pretrained models (Fort et al., 2021). They require extended training
time, hyperparameter tuning and careful selections of transformations, whereas our method does not
introduce any hyperparameters and has negligible inﬂuence on standard cross-entropy training time."
RELATED WORK,0.053003533568904596,"Model Calibration methods can also be largely divided in two categories: 1) training time calibra-
tion using augmentations (Thulasidasan et al., 2019; Jang et al., 2021), using modiﬁed losses (Kumar
et al., 2018); 2) post-hoc calibration (Guo et al., 2017; Kull et al., 2019; Kumar et al., 2019; Rahimi
et al., 2020). Recently, Rahimi et al. (2020) formally generalizes a family of expressive functions
for calibration, the intra order-preserving functions. This class of functions has more representation
power to calibrate more complex decision boundaries in neural networks. Our proposed method be-
longs to this class of functions. However, all these works focus on calibration of in-distribution data.
To obtain better calibration on OOD data, on which the conﬁdence of a model needs to decrease ac-
cordingly, sensitivity and deterministic uncertainty modeling is explored by SNGP (Van Amersfoort
et al., 2020) and DUQ (Liu et al., 2020a). Our proposed model not only belongs to the family of intra
order-preserving functions, which ensures good in-distribution calibration, but also improves sen-
sitivity to distribution shift, which improves out-of-distribution calibration simultaneously. Please
refer to Appendix 6.8 for a more detailed discussion on related works."
METHOD,0.05653710247349823,"3
METHOD"
METHOD,0.06007067137809187,"3.1
MOTIVATION: COVARIATE AND CONCEPT SHIFT"
METHOD,0.0636042402826855,"We follow Moreno-Torres et al. (2012) for the formal deﬁnition of distribution shift, covariate shift
and concept shift. Let X ∈RD denote the covariate which is the input and Y ∈R denote the output
label. Distribution shift happens when the training joint distribution is not equal to the testing joint
distribution Ptr(X, Y ) ̸= Ptst(X, Y ). Covariate shift appears when Ptr(Y |X) = Ptst(Y |X) and
Ptr(X) ̸= Ptst(X). Concept shift appears when Ptr(Y |X) ̸= Ptst(Y |X) and Ptr(X) = Ptst(X).
However, in the image domain, it is rare that such concept shift occurs without changes in P(X).
We therefore modify the equality in concept shﬁt to Ptr(X) ≈Ptst(X) where superﬁcial, low-level
statistics may be retained."
METHOD,0.06713780918727916,"Speciﬁcally, covariate shift happens when the testing data is non-semantically different from the
training data and concept shift happens when the testing data is semantically different from the
training data. We illustrate a spectrum of OOD dataset in Fig. 1. In this example, CIFAR10 is the
training dataset. CIFAR100 represents concept shift because CIFAR100 has a non-overlapping label
space, i.e., Ptr(Y |X) ̸= Ptst(Y |X), but similar style with CIFAR10. The corrupted CIFAR10C
dataset (Hendrycks & Dietterich, 2019) represents covariate shift because it has the same labels but"
METHOD,0.0706713780918728,"Figure 2: Diagram of Score functions, Geometric ODIN training and calibration The paper
proposes two score functions and an OOD detection and calibration method: Geometric ODIN.
Loss is backprobagated to all components during training and only to α and β during calibration."
METHOD,0.07420494699646643,"different style, i.e., Ptr(X) ̸= Ptst(X), compared to CIFAR10. SVHN represents both covariate
and concept shift due to its non-overlapping label space and very different style."
METHOD,0.07773851590106007,"As shown in Fig. 2, the ﬁrst goal of this paper is to derive score functions to represent the shift in
either P(X) or P(Y |X). We denote the score function that reﬂects change from Ptr(X) to Ptst(X)
as the covariate shift score function: g(x) : RD →R and the score function that reﬂects change
from Ptr(Y |X) to Ptst(Y |X) as the concept shift score function: h(y, x) : Y × RD →R. The
second goal is to improve the sensitivity of these scores to their corresponding distribution shift.
Speciﬁcally, we propose a geometrically inspired out-of-distribution detection method with only in-
distribution data (Geometric ODIN). The third goal is to show that, unlike prior works which treat
OOD detection (Mukhoti et al., 2021) and calibration (Van Amersfoort et al., 2020; Liu et al., 2020a)
separately, our proposed Geomeric ODIN naturally leads to an expressive calibration function in the
family of intra order-preserving functions (Rahimi et al., 2020), which ensures good calibration."
COVARIATE AND CONCEPT SCORE FUNCTIONS,0.0812720848056537,"3.2
COVARIATE AND CONCEPT SCORE FUNCTIONS"
COVARIATE AND CONCEPT SCORE FUNCTIONS,0.08480565371024736,"In this section, we theoretically derive two score functions, g(x) and h(y, x), based on the KL-
divergence between a uniform distribution U and a predicted distribution P ∈RM, where M is the
number of classes. By starting from KL-divergence, we hinge the subsequent derivation of score
functions on a physical meaningful uncertainty measure, i.e., how far the predicted distribution is
from a uniform distribution. This relationship ensures a natural interpretation of score functions
because predictions on distribution shifted data should have larger uncertainty, i.e. smaller dis-
tance from uniform. We are speciﬁcally interested in softmax-linear models for classiﬁcation. They
typically consist of a feature extractor and a linear layer followed by a softmax activation. Let
f ∈RD denote a feature vector from the feature extractor4. The output of the linear layer, i.e.,
logits, l ∈RM =< f, W > is deﬁned as the inner product between the feature vector and a weight
matrix in the linear layer. Let li = ∥f∥2∥wi∥2 cos φi denote the ith logit, Pi =
exp li
PM
j=1 exp lj denote"
COVARIATE AND CONCEPT SCORE FUNCTIONS,0.08833922261484099,the predicted probability of the ith class. The KL-divergence KL(U||P) can be written as following:
COVARIATE AND CONCEPT SCORE FUNCTIONS,0.09187279151943463,"KL(U||P) = − M
X i=1"
COVARIATE AND CONCEPT SCORE FUNCTIONS,0.09540636042402827,"1
M ln MPi = ln M
X"
COVARIATE AND CONCEPT SCORE FUNCTIONS,0.0989399293286219,"j=1
exp lj"
COVARIATE AND CONCEPT SCORE FUNCTIONS,0.10247349823321555,"|
{z
}
Log−Sum−Exp −1 M M
X"
COVARIATE AND CONCEPT SCORE FUNCTIONS,0.10600706713780919,"i=1
li −ln M
(1)"
COVARIATE AND CONCEPT SCORE FUNCTIONS,0.10954063604240283,Now we can use the inequality property of Log-Sum-Exp (LSE)5 functions in Eq. 2 to bound Eq. 1.
COVARIATE AND CONCEPT SCORE FUNCTIONS,0.11307420494699646,"max
j
lj ≤ln M
X"
COVARIATE AND CONCEPT SCORE FUNCTIONS,0.1166077738515901,"j=1
exp lj ≤max
j
lj + ln M
(2)"
COVARIATE AND CONCEPT SCORE FUNCTIONS,0.12014134275618374,Therefore the KL-divergence (Eq. 1) can be bounded as follows:
COVARIATE AND CONCEPT SCORE FUNCTIONS,0.12367491166077739,"U −ln M ≤KL(U||P) ≤U
(3)"
BOLD LETTER INDICATES VECTORS,0.127208480565371,"4Bold letter indicates vectors
5Note that the negative LSE function is also deﬁned as free energy in Liu et al. (2020b)."
BOLD LETTER INDICATES VECTORS,0.13074204946996468,where U = maxj lj −1
BOLD LETTER INDICATES VECTORS,0.13427561837455831,"M
PM
i=1 li. U can be further decomposed into two multiplicative components
by plugging in the deﬁnition of logits li:"
BOLD LETTER INDICATES VECTORS,0.13780918727915195,"U = max
j
lj −1 M M
X"
BOLD LETTER INDICATES VECTORS,0.1413427561837456,"i=1
li ="
BOLD LETTER INDICATES VECTORS,0.14487632508833923,"g(x)
z}|{
∥f∥2 "
BOLD LETTER INDICATES VECTORS,0.14840989399293286,"max
j
∥wj∥2 cos φj −1 M M
X"
BOLD LETTER INDICATES VECTORS,0.1519434628975265,"i=1
∥wi∥2 cos φi !"
BOLD LETTER INDICATES VECTORS,0.15547703180212014,"|
{z
}
h(y,x) (4)"
BOLD LETTER INDICATES VECTORS,0.15901060070671377,"We deﬁne the covariate shift score function as g(x) ≜∥f∥2 because the norm of a feature vector
is the sum of squared activation values and only depends on the input. Intuitively, activation of a
neural network on covariate-shifted data should be weaker than in-distribution data. Therefore, g(x)
assigns a higher value to in-distribution data than to OOD data. We deﬁne the concept shift score
function as h(y, x) ≜maxj ∥wj∥2 cos φj −
1
M
PM
i=1 ∥wi∥2 cos φi because it is the difference
between the cosine distance of the predicted class and the average cosine distance of all classes and
depends on both the input and ﬁnal class membership, assigned by the max operator. Intuitively,
class assignment should be less obvious under concept shift and the difference should be small.
Consequently, h(y, x) assigns a higher value to in-distribution data than to OOD data.In retrospect,
our deﬁnition of covariate shift and concept shift scores supports existing ﬁndings that the feature
norms correspond to intra-class variance and angles reﬂect inter-class variation (Liu et al., 2018).
Intuitively, covariate shift represents non-semantic change within a speciﬁc class, i.e., intra-class
variance; concept shift represents semantic changes, i.e, inter-class variation. Here we formalize
the intuition and observations in Liu et al. (2018) as score functions derived analytically from a
KL-divergence viewpoint. We provide an extended discussion of these scores in Appendix 7."
BOLD LETTER INDICATES VECTORS,0.1625441696113074,"More importantly, the combined score function U (Eq. 4) carries a physical meaning: it bounds the
KL-divergence between a uniform distribution and the predictive distribution. Intuitively, a small U
indicates large uncertainty because U upper bounds KL(U||P), and a large U indicates small un-
certainty because it also appears in the lower bound. We can derive U also by Taylor-expanding the
softmax equation as in Liang et al. (2017), which claims that U is responsible for good OOD detec-
tion. Our ﬁndings conﬁrm this and push it further by decomposing it into different components. A
similar scoring function, S(x) = ∥f∥2 maxj ∥wj∥2 cos φj, is used in Tack et al. (2020), but is only
empirically motivated based on observations with limited analytical insights such as its relationship
to uncertainty and the functionality of each of components. In contrast, our derivation clearly shows
the relationship between these score functions and the KL-divergence, which is as an uncertainty
measure, and disentangles their roles. We will compare the g(x), h(y, x) and U as score functions
in subsequent experiments and analyze their respective sensitivity to different shifts (Sec. 4.1)."
GEOMETRIC OUT-OF-DISTRIBUTION DETECTION WITH IN-DISTRIBUTION DATA,0.16607773851590105,"3.3
GEOMETRIC OUT-OF-DISTRIBUTION DETECTION WITH IN-DISTRIBUTION DATA"
GEOMETRIC OUT-OF-DISTRIBUTION DETECTION WITH IN-DISTRIBUTION DATA,0.1696113074204947,"As derived in Eq. 4, the covariate score is a function of feature norms and the concept score is a
function of feature angles. Consequently, improving the sensitivity of feature norms and feature
angles to data shifts seems to be the natural next step to improve OOD detection. Therefore, we
adopt Geometric Sensitivity Decomposition (GSD) (Tian et al., 2021) (reviewed in appendix 6.3)
to improve sensitivity to covariate and concept shifts. Speciﬁcally, GSD improves sensitivity by
extracting sensitive components from norms ∥f ∗∥26 and angles |φ∗
i | through a decomposition of
them into: a scalar offset and a variance component as shown in Eq. 5. Scalar offsets Cf and Cφ
minimize the loss on the training set and the variance components f and φi account sensitively for
variances in samples."
GEOMETRIC OUT-OF-DISTRIBUTION DETECTION WITH IN-DISTRIBUTION DATA,0.17314487632508835,"∥f ∗∥2 = ∥f∥2 + Cf,
|φ∗
i | = |φi| −|Cφ|
(5)"
GEOMETRIC OUT-OF-DISTRIBUTION DETECTION WITH IN-DISTRIBUTION DATA,0.17667844522968199,"With the decomposed components, the original logit l∗
i = ∥f ∗∥2∥wi∥2 cos φ∗
i , can be written as:"
GEOMETRIC OUT-OF-DISTRIBUTION DETECTION WITH IN-DISTRIBUTION DATA,0.18021201413427562,"l∗
i = ∥f ∗∥2∥wi∥2 cos φ∗
i ≈li = "
GEOMETRIC OUT-OF-DISTRIBUTION DETECTION WITH IN-DISTRIBUTION DATA,0.18374558303886926,"


1
cos Cφ
| {z }
α"
GEOMETRIC OUT-OF-DISTRIBUTION DETECTION WITH IN-DISTRIBUTION DATA,0.1872791519434629,"∥f∥2 +
1
cos Cφ
| {z }
α"
GEOMETRIC OUT-OF-DISTRIBUTION DETECTION WITH IN-DISTRIBUTION DATA,0.19081272084805653,"β
z}|{
Cf "
GEOMETRIC OUT-OF-DISTRIBUTION DETECTION WITH IN-DISTRIBUTION DATA,0.19434628975265017,"

∥wi∥2 cos φi
(6)"
GEOMETRIC OUT-OF-DISTRIBUTION DETECTION WITH IN-DISTRIBUTION DATA,0.1978798586572438,6The superscript ∗denotes the original component before decomposition.
GEOMETRIC OUT-OF-DISTRIBUTION DETECTION WITH IN-DISTRIBUTION DATA,0.20141342756183744,"where li denote the new ith logit. In Eq. 6, the new7 feature f is a direct output of a feature extractor,
and is modiﬁed by α and β as also illustrated Fig. 2. Note that the calculation of score functions in
Sec. 3.2 only uses the feature and is independent of α and β."
GEOMETRIC OUT-OF-DISTRIBUTION DETECTION WITH IN-DISTRIBUTION DATA,0.2049469964664311,"Because cos Cφ and Cf are scalar offsets, we can parametrize them separately from the main network.
Unlike GSD which parametrizes them as instance-independent scalars, inspired by Hsu et al. (2020);
Techapanurak et al. (2019), we make α(f) and β(f) instance-dependent scalars and use a single
linear layer to learn them. To enforce numerical constraints, i.e., 0 < α < 1 and β > 0, α(f) uses a
sigmoid activation and β(f) uses a softplus activation. Finally, the relaxed output is:"
GEOMETRIC OUT-OF-DISTRIBUTION DETECTION WITH IN-DISTRIBUTION DATA,0.20848056537102475,"P(Y = i|x) =
exp li
PM
j=1 exp lj
=
exp

1
α(f)∥f∥2 + β(f)"
GEOMETRIC OUT-OF-DISTRIBUTION DETECTION WITH IN-DISTRIBUTION DATA,0.21201413427561838,"α(f)

∥wi∥2 cos φi
"
GEOMETRIC OUT-OF-DISTRIBUTION DETECTION WITH IN-DISTRIBUTION DATA,0.21554770318021202,"PM
j=1 exp

1
α(f)∥f∥2 + β(f)"
GEOMETRIC OUT-OF-DISTRIBUTION DETECTION WITH IN-DISTRIBUTION DATA,0.21908127208480566,"α(f)

∥wj∥2 cos φj

(7)"
GEOMETRIC OUT-OF-DISTRIBUTION DETECTION WITH IN-DISTRIBUTION DATA,0.2226148409893993,"Now the new predicted norm ∥f∥2 and angle φi are more sensitive to input changes because they
encode variances in samples as shown in Eq. 5. Therefore, including β (related to norms) improves
sensitivity to covariate shift and including α (related to angles) improves sensitivity to concept shift.
Note that, under this construction, Generalized ODIN (Hsu et al., 2020) is a special case of our
proposed method. Generalized ODIN only includes the α(f) which only improves angle sensitivity
but not norm sensitivity. Unlike Hsu et al. (2020)’s probabilistic perspective8, our model builds on a
geometric perspective and captures both covariate and concept shifts by improving norm and angle
sensitivity. The new model can be trained identically as the vanilla network without additional hy-
perparameter tuning and extended training time. Combined with score functions derived in Sec. 3.2,
Geometric ODIN achieves state-of-the-art OOD detection performance (Sec. 4.1)"
CALIBRATION OF GEOMETRIC ODIN,0.22614840989399293,"3.4
CALIBRATION OF GEOMETRIC ODIN"
CALIBRATION OF GEOMETRIC ODIN,0.22968197879858657,"Like most softmax-linear classiﬁcation models, Geometric ODIN also suffers from miscalibration
right after training. Speciﬁcally, predictions tend to be overconﬁdent (Guo et al., 2017). In the case
of Geometric ODIN, the cause of overconﬁdence is obvious by construction. The offset scalars
β(f) and α(f) are designed to minimize the training loss. Practically, they are optimized to push
the predicted probability of the ground truth class to be close to 1 during training, and this intended
behavior unsolicitedly continues to inference time when accuracy is not as high as during training.
Nevertheless, our construction of α(f) and β(f) belongs to a family of intra order-preserving func-
tions, which has potentially strong representation to calibrate complex functions in deep networks
according to Theorem 1 in Rahimi et al. (2020).
To prove that, it’s sufﬁce to show that that α(l)
and β(l) 9 forms a strictly positive function, m(l) as shown in Eq. 8"
CALIBRATION OF GEOMETRIC ODIN,0.2332155477031802,"¯li =
 1"
CALIBRATION OF GEOMETRIC ODIN,0.23674911660777384,α(l)∥f∥2 + β(l) α(l)
CALIBRATION OF GEOMETRIC ODIN,0.24028268551236748,"
∥wi∥2 cos φi =
 1"
CALIBRATION OF GEOMETRIC ODIN,0.24381625441696114,"α(l) +
β(l)
α(l)∥f∥2 "
CALIBRATION OF GEOMETRIC ODIN,0.24734982332155478,"|
{z
}
m(l)"
CALIBRATION OF GEOMETRIC ODIN,0.2508833922261484,"li
z
}|
{
∥f∥2∥wi∥2 cos φi
(8)"
CALIBRATION OF GEOMETRIC ODIN,0.254416961130742,"Because 0 < α(l) < 1 and β(l) > 0, the function m(l) is strictly positive and thus satisﬁes
Theorem 1 in Rahimi et al. (2020) (See appendix 6.4 for more details). We follow a simple procedure
proposed in Guo et al. (2017); Rahimi et al. (2020) to calibrate Geometric ODIN by minimizing the
negative log likelihood (NLL) on a evaluation dataset for a few epochs. During calibration the
gradients to other parts of the model are stopped and only these two linear layers, α(l) and β(l), are
optimized (Fig. 2). Naturally with the improved sensitivity and calibrated functions, the calibration
of Geometric ODIN is on par with state-of-the-art models on both in-distribution and OOD data.
Note that, as mentioned in Sec. 3.3, because the calculation of score functions for OOD detection
only depends on the feature f and is independent of α and β, calibration does not affect the OOD
detection performance and does not change prediction results due to the order-preserving property."
CALIBRATION OF GEOMETRIC ODIN,0.2579505300353357,"7Even though both f ∗and f are outputs directly from the feature extractor, we use original and new to
indicate whether GSD is applied.
8α(f) is interpreted as P(din|x), the probability of x being in-distribution.
9Following Rahimi et al. (2020), which uses logits l as input instead of the feature f. Our derivation in
previous sections is still valid because l solely depends on f given a model."
CALIBRATION OF GEOMETRIC ODIN,0.26148409893992935,"AUROC↑
Score Functions
ID:CIFAR100
ID:CIFAR10"
CALIBRATION OF GEOMETRIC ODIN,0.26501766784452296,"Near
Far
Far
Near
Near
Far
Far
CIFAR10
SVHN
TIN(R)
CIFAR100
CIFAR10C
SVHN
TIN(R)"
CALIBRATION OF GEOMETRIC ODIN,0.26855123674911663,Vanilla Wide-ResNet-28-10
CALIBRATION OF GEOMETRIC ODIN,0.27208480565371024,"MSP (Hendrycks & Gimpel, 2016)
80.68±0.34
77.37±2.25
81.65±0.14
88.93±0.37
70.58±0.59
93.66±1.79
87.98±0.35
Energy (Liu et al., 2020b)
80.74±0.45
79.48±2.91
82.04±0.14
88.84±0.44
70.60±0.52
94.39±2.30
88.16±0.39
(Zagoruyko & Komodakis, 2016)
Mahanobis (Lee et al., 2018)
66.72±0.77
93.55±1.18
76.54±0.31
87.26±1.21
68.38±0.50
99.19±0.22
87.33±1.39
GMM Density (Mukhoti et al., 2021)
66.75±0.74
93.91±0.71
76.58±0.29
87.26±1.20
75.71±0.80
99.19±0.22
87.33±1.39"
CALIBRATION OF GEOMETRIC ODIN,0.2756183745583039,"DUQ (Van Amersfoort et al., 2020)
Kernel Distance
−
−
−
85.92±0.35*
−
93.71±0.61*
−
SNGP (Liu et al., 2020a)
SoftMax Entropy
−
85.71±0.81*
−
91.13±0.15*
−
94.0±1.30*
−
DDU (Mukhoti et al., 2021)
GMM Density
67.65±0.20
92.59±1.47
77.72±0.15
90.69±0.42
76.00±0.00
97.12±1.21
84.89±1.01
Hyper-Free/Generalized ODIN
Cosine Similarity
76.90±0.30
95.39±1.31
82.93±0.18
92.28±0.16
75.84±0.70
99.56±0.12
92.03±0.15
5-Ensemble (Lakshminarayanan et al., 2016)
SoftMax Entropy
−
79.54±0.91*
−
92.13±0.02*
−
97.73±0.31*
−"
CALIBRATION OF GEOMETRIC ODIN,0.2791519434628975,"Ours: α(x)-only
h(y, x)
79.24±0.37
83.75±3.30
82.67±0.22
92.28±0.15
77.00±0.00
97.56±0.90
91.83±0.09"
CALIBRATION OF GEOMETRIC ODIN,0.2826855123674912,Ours: α-β
CALIBRATION OF GEOMETRIC ODIN,0.2862190812720848,"h(y, x)
79.72±0.41
88.72±2.99
82.89±0.24
91.29±0.07
73.80±0.45
98.42±0.21
90.87±0.10
g(x)
60.70±0.56
93.15±2.06
72.34±0.42
89.08±0.24
76.80±1.10
99.40±0.14
90.79±0.39
U
71.42±0.27
95.77±0.33
80.86±0.53
92.31±0.21
78.00±0.71
99.54±0.08
92.85±0.19"
CALIBRATION OF GEOMETRIC ODIN,0.28975265017667845,"Table 1: AUROC ↑for Near and Far OOD detection. Results are averaged over 5 runs. Our
α(x)-only model is the same as Generalized ODIN (Hsu et al., 2020) (hI(x) variant) without its
input processing. Hyper-free (Techapanurak et al., 2019) and Generalized ODIN (hC(x) variant)
are the same. * denotes results from Mukhoti et al. (2021)."
EXPERIMENTS,0.29328621908127206,"4
EXPERIMENTS"
EXPERIMENTS,0.2968197879858657,"In this section, we present results for Out-of-Distribution detection in Sec. 4.1 and calibration in
Sec. 4.2. We are the ﬁrst to present a method that works well on both OOD detection and cal-
ibration, disentangling shift types. Implementation: Following prior works (Liu et al., 2020a;
Van Amersfoort et al., 2020; Mukhoti et al., 2021), we use Wide-ResNet-28-10 (Mukhoti et al.,
2021) for all experiments. We train the model using SGD with an initial learning rate of 0.1 for 200
epochs. The learning rate is annealed with a cosine scheduler (Loshchilov & Hutter, 2016) (more
details in appendix 6.5). Metrics: For OOD detection in Sec. 4.1, we use the common AUROC and
TNR@TPR95 as benchmark metrics. For calibration in Sec. 4.2, we use the Expected Calibration
Error (ECE) (Guo et al., 2017) with 15 bins and Negative Log Likelihood (NLL) which is a strictly
proper scoring rule (Gneiting & Raftery, 2007). Datasets: CIFAR10 (Krizhevsky et al., a) and CI-
FAR100 (Krizhevsky et al., b) are considered near OOD datasets (Winkens et al., 2020; Fort et al.,
2021) to each other. SVHN (Netzer et al., 2011) is considered a far OOD dataset to both CIFAR10
and CIFAR100 due to its shift in both concept and style. The CIFAR10C/CIFAR100C (Hendrycks
& Dietterich, 2019) dataset is a variant of CIFAR10/CIFAR100 corrupted by 15 types of noises with
5 degrees of severity. We also introduce CIRAF100 Splits benchmark which consists of 10 datasets
with increasing concept shift from CIFAR10 classes (Sec. 4.1.3)."
OUT-OF-DISTRIBUTION DETECTION RESULTS,0.3003533568904594,"4.1
OUT-OF-DISTRIBUTION DETECTION RESULTS"
NEAR AND FAR OOD DETECTION,0.303886925795053,"4.1.1
NEAR AND FAR OOD DETECTION"
NEAR AND FAR OOD DETECTION,0.30742049469964666,"The α-β model is the best. In Tab. 1, we present OOD detection results against state-of-the-art
methods on existing near and far OOD categorization. For near OOD detection under strong con-
cept shift, CIFAR10 (ID) vs. CIFAR100 (OOD), both our α-only and α-β variants achieve the the
best performance. This demonstrates that α(x) improves the sensitivity of angles and hence the
sensitivity to concept-shifted data. For near OOD detection under strong covariate shift, CIFAR10
(ID) vs. CIFAR10C (OOD), the α-β variant achieves the the best performance. This suggests that
β(x) improves the sensitivity of norms and hence the sensitivity to covariate-shifted data. In CI-
FAR100 (ID) vs. CIFAR10 (OOD) experiments, the performance of the α-only and α-β variants are
within variance and is close to some other compared methods, we can not make clear observations
from those experiments10. For far OOD detection, CIFAR10/CIFAR100 (ID) vs. SVHN (OOD), the
α-β model achieves state-of-the-art performance. This reconﬁrms that β(x) improves sensitivity to
covariate-shifted data, because SVHN has both covariate and concept shifts compared to the CIFAR
datasets, and the α-β model outperforms the α-only variant, which only improves on concept shift,
by a noticeable margin. In terms of score functions, the best performing one for the α-β model
is U, which is a product of g(x) and h(y, x) (Sec. 3.2), while that of the α-only model is h(y, x).
This shows that depending on which component is more sensitive, different scoring functions are"
NEAR AND FAR OOD DETECTION,0.31095406360424027,"10Other confounding factors could contribute to the close performance. Prior works either omit comparisons
under these settings (Mukhoti et al., 2021) or report only marginal improvement (Winkens et al., 2020)"
NEAR AND FAR OOD DETECTION,0.31448763250883394,"a α-β, α-only, β-only and vanilla models using score functions g(x) and h(y, x)
b All models using U"
NEAR AND FAR OOD DETECTION,0.31802120141342755,"Figure 3: Capturing Covariate Shift (Motion Blur) All results are averaged over 5 runs. Modeling
covairate shift (α-β model) yields the best performance as shown in Fig. 3b. g(x) is more responsive
to covariate shift than h(y, x) as shown in Fig. 3a with the α-β model."
NEAR AND FAR OOD DETECTION,0.3215547703180212,"a α-β, α-only, β-only and vanilla models using score functions g(x) and h(y, x)
b All models using U"
NEAR AND FAR OOD DETECTION,0.3250883392226148,"Figure 4: Capturing Concept Shift (CIFAR100 Splits) All results are averaged over 5 runs. Mod-
eling concept shift (both α-only and α-β models) yields the best performance as shown in Fig. 4b.
h(y, x) is more responsive to concept shift than g(x) as shown in Fig. 4a with the α-β model."
NEAR AND FAR OOD DETECTION,0.3286219081272085,"preferred. When the sensitivity of both norms and angles are improved, as in the α-β variant, the
combined score function U performs well under different distribution shifts."
OOD DETECTION UNDER COVARIATE SHIFT WITH IMAGE CORRUPTION,0.3321554770318021,"4.1.2
OOD DETECTION UNDER COVARIATE SHIFT WITH IMAGE CORRUPTION"
OOD DETECTION UNDER COVARIATE SHIFT WITH IMAGE CORRUPTION,0.33568904593639576,"The g(x) score captures covariate shift. We compare the α-β, α-only (β = 0), β-only (α = 1)
variants and the vanilla model on CIFAR10C (Hendrycks & Dietterich, 2019) corrupted by mo-
tion blur in Fig. 3 with increasing degrees of noise (see appendix 6.7 for additional experiments).
From 3a, we observe that 1) as covariate shift severity increases, OOD detection becomes easier
because AUROC increases with increasing severity. 2) the vanilla model is more sensitive to the
concept shift component because h(y, x) > g(x) in the vanilla model plot even though covariate
shift is the dominant distribution shift in this example. 3) when sensitivity to both covariate and
concept shift is improved , the α-β model becomes more sensitive to the covariate shift component.
This suggests that the dominant shift in this example is indeed covariate shift. From Fig. 3b, we ob-
serve that the α-β model outperforms the β-only model using the combined score function U. This
suggests that improving sensitivity to both shifts and using U yield the best OOD detection perfor-
mance. In retrospect, the performance of OOD detection has always relied on two components: the
model and the score function. Some works propose more responsive score functions, e.g., energy
in Liu et al. (2020b), and some works propose more sensitive models, e.g., DDU in Mukhoti et al.
(2021) to improve the performance of existing score functions."
OOD DETECTION UNDER COVARIATE SHIFT WITH IMAGE CORRUPTION,0.3392226148409894,"4.1.3
OOD DETECTION UNDER CONCEPT SHIFT WITH CIFAR100 SPECIAL SPLITS"
OOD DETECTION UNDER COVARIATE SHIFT WITH IMAGE CORRUPTION,0.34275618374558303,"Finding a dataset to benchmark gradual concept shift is not straightforward because concept shift is
traditionally thought as binary: overlapping or non-overlapping. However, not all non-overlapping
labels are the same. For example, pickup truck (CIFAR100) is semantically much closer to truck
(CIFAR10) than sunﬂowers (CIFAR100) is. To create this gradual concept/semantic shift, we pro-
pose to divide the CIFAR100 dataset into 10 sub-datasets with increasing conceptual difference from
CIFAR10 classes. Specially, we use 300 dimensional Glove word embeddings11 (Pennington et al.,
2014) trained on the entire wikipedia2014 and Gigaword5 (Napoles et al., 2012) to measure seman-
tic closeness (inner product) between CIFAR100 and CIFAR10 classes. The result is 10 subdatasets
split from CIFAR100. Please refer to appendix 6.6 for the full splits."
OOD DETECTION UNDER COVARIATE SHIFT WITH IMAGE CORRUPTION,0.3462897526501767,"The h(y, x) score captures concept shift. Following the previous section, we benchmark the α-β,
α-only, β-only variants and the vanilla model on the newly created CIFAR100 Splits. From Fig. 4a,
we observe that 1) as concept shift severity increases, OOD detection becomes easier because AU-"
OOD DETECTION UNDER COVARIATE SHIFT WITH IMAGE CORRUPTION,0.3498233215547703,11https://nlp.stanford.edu/projects/glove/
OOD DETECTION UNDER COVARIATE SHIFT WITH IMAGE CORRUPTION,0.35335689045936397,"ROC increases with increasing severity. 2) both concept shift and covariate shift are present because
AUROC using either h(y, x) or g(x) increases. 3) the vanilla model is dominantly more sensitive to
the concept shift component because concept shift is the dominant distribution shift in CIFAR100
Splits by construction and vanilla ResNet is more sensitive to concept shift (the same behavior is
also observed on covariate-shift-heavy data in Sec. 4.1.2). 4) when sensitivity to both shifts is im-
proved, the α-β model is still more sensitive to concept shift (h(y, x) > g(x)). This reconﬁrms
that the dominant shift type is indeed concept shift. From Fig. 4b, interestingly, we observe that
all three variants perform similarly and all outperform the vanilla model. Combined with previous
observations that the dataset has strong concept shift and the vanilla model is already very sen-
sitive to concept shift, improving sensitivity to the covariate shift component yields equally good
performance as improving sensitivity to both shifts."
OUT-OF-DISTRIBUTION CALIBRATION RESULTS,0.3568904593639576,"4.2
OUT-OF-DISTRIBUTION CALIBRATION RESULTS"
OUT-OF-DISTRIBUTION CALIBRATION RESULTS,0.36042402826855124,"Accuracy ↑
ECE ↓
NLL ↓
Clean
Corrupted
Clean
Corrupted
Clean
Corrupted"
OUT-OF-DISTRIBUTION CALIBRATION RESULTS,0.36395759717314485,"Vanilla
96.23±0.13
69.78±1.22
0.015±0.001
0.148±0.008
0.148±0.005
1.107±0.042
Temp Scaling (Guo et al., 2017)
96.23±0.13
69.78±1.22
0.003±0.001
0.107±0.009
0.131±0.003
0.906±0.029
Matrix scaling (Guo et al., 2017)
95.98±0.10
69.81±1.11
0.005±0.000
0.107±0.009
0.145±0.008
0.966±0.041
Dirichlet (Kull et al., 2019)
96.10±0.10
69.75±1.09
0.004±0.002
0.114±0.007
0.130±0.004
0.977±0.032
DUQ (Van Amersfoort et al., 2020)†
94.7±0.02
71.6±0.02
0.034±0.002
0.183±0.011
0.239±0.02
1.348±0.01
SNGP (Liu et al., 2020a)†
95.9±0.01
74.6±0.01
0.018±0.001
0.090±0.012
0.138±0.01
0.935±0.01
GSD (Tian et al., 2021)
95.9±0.01
74.9±0.05
0.008±0.002
0.085±0.012
0.140±0.004
0.853±0.039"
OUT-OF-DISTRIBUTION CALIBRATION RESULTS,0.3674911660777385,"Ours: α-β
95.99±0.12
70.41±0.55
0.001±0.000
0.071±0.0112
0.130±0.003
0.854±0.029"
OUT-OF-DISTRIBUTION CALIBRATION RESULTS,0.3710247349823322,Table 2: Calibration on CIFAR10 averaged over 5 seed. † denotes results from Liu et al. (2020a).
OUT-OF-DISTRIBUTION CALIBRATION RESULTS,0.3745583038869258,"Accuracy ↑
ECE ↓
NLL ↓
Clean
Corrupted
Clean
Corrupted
Clean
Corrupted"
OUT-OF-DISTRIBUTION CALIBRATION RESULTS,0.37809187279151946,"Vanilla
80.66± 0.20
50.25±0.48
0.035±0.002
0.171±0.017
0.774±0.007
2.384±0.039
Temp Scaling (Guo et al., 2017)
80.99±0.20
50.25±0.48
0.033±0.002
0.163±0.017
0.776±0.006
2.368±0.039
Matrix scaling (Guo et al., 2017)
79.31 ±0.25
48.74±0.55
0.03±0.003
0.160±0.016
0.791±0.013
2.532±0.051
Dirichlet (Kull et al., 2019)
80.70±0.36
50.06±0.46
0.015±0.002
0.144±0.017
0.743±0.025
2.346±0.017
DUQ (Van Amersfoort et al., 2020)†
78.5±0.02
50.4±0.02
0.119±0.001
0.281±0.012
0.980±0.02
2.841±0.01
SNGP (Liu et al., 2020a)†
79.9±0.03
49.0±0.02
0.025±0.012
0.117±0.014
0.847±0.01
2.626±0.01
GSD (Tian et al., 2021)
79.8±0.03
49.8±0.03
0.027±0.003
0.088±0.007
0.784±0.011
2.236±0.021"
OUT-OF-DISTRIBUTION CALIBRATION RESULTS,0.38162544169611307,"Ours: α-β
79.21±0.19
49.21±0.61
0.010±0.001
0.084±0.009
0.754±0.005
2.323±0.057"
OUT-OF-DISTRIBUTION CALIBRATION RESULTS,0.38515901060070673,Table 3: Calibration on CIFAR100 averaged over 5 seeds. † denotes results from Liu et al. (2020a).
OUT-OF-DISTRIBUTION CALIBRATION RESULTS,0.38869257950530034,"As proved in Sec. 3.4, our model naturally leads to a calibration function in the family of in-
tra order-preserving functions (Rahimi et al., 2020), which ensures good in-distribution calibra-
tion performance. Moreover, thanks to the improved sensitivity, the model can potentially achieve
state-of-the-art out-of-distribution calibration as well. We benchmark our models on both clean CI-
FAR10/CIFAR100 as well as corrupted CIFAR10C/CIFAR100C (Hendrycks & Dietterich, 2019).
Following prior works (Rahimi et al., 2020), we use 5-fold cross validation for in-distribution cal-
ibration experiments. As shown in Tab. 2 and Tab. 3, our model is comparable to state-of-the-art
models on both in-distribution and out-of-distribution calibration. In the literature, OOD detection
and calibration have been studied separately, but our method and experimentation tackles both tasks."
CONCLUSION,0.392226148409894,"5
CONCLUSION"
CONCLUSION,0.3957597173144876,"In this work, we propose to characterize the spectrum of out-of-distribution (OOD) data using co-
variate shift and concept shift. Unlike difﬁculty of OOD detection, distribution shift exposes intrinsic
characteristics of OOD data. Consequently, to achieve good OOD detection performance, a model
needs to consider both distribution shifts. At representation level, we derive two score functions that
represent and capture each shift separately. At modeling level, inspired by these score functions,
we propose a geometrically-inspired method, Geometric ODIN, to improve a model’s sensitivity to
both shift. Furthermore, Geometric ODIN is the ﬁrst method that considers both OOD detection
and calibration, targeting both concept and covariate shifts, and yields state-of-the-art performance
in both tasks. Finally, we hope that the distribution shift perspective can lead to a new direction to
study OOD data and unify the the ﬁelds of detection and calibration of OOD data."
REFERENCES,0.3992932862190813,REFERENCES
REFERENCES,0.4028268551236749,"Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon
Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al. End to end learning
for self-driving cars. arXiv preprint arXiv:1604.07316, 2016."
REFERENCES,0.40636042402826855,"Alex Chan, Ahmed Alaa, Zhaozhi Qian, and Mihaela Van Der Schaar. Unlabelled data improves
bayesian uncertainty calibration under covariate shift. In International Conference on Machine
Learning, pp. 1392–1402. PMLR, 2020."
REFERENCES,0.4098939929328622,"Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. De-
scribing textures in the wild. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 3606–3613, 2014."
REFERENCES,0.4134275618374558,"Stefan Depeweg, Jose-Miguel Hernandez-Lobato, Finale Doshi-Velez, and Steffen Udluft. Decom-
position of uncertainty in bayesian deep learning for efﬁcient and risk-sensitive learning.
In
International Conference on Machine Learning, pp. 1184–1193. PMLR, 2018."
REFERENCES,0.4169611307420495,"Stanislav Fort, Jie Ren, and Balaji Lakshminarayanan. Exploring the limits of out-of-distribution
detection. arXiv preprint arXiv:2106.03004, 2021."
REFERENCES,0.4204946996466431,"Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and estimation.
Journal of the American statistical Association, 102(477):359–378, 2007."
REFERENCES,0.42402826855123676,"Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved training of wasserstein gans. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fer-
gus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems,
volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/
paper/2017/file/892c3b1c6dccd52936e27cbd0ff683d6-Paper.pdf."
REFERENCES,0.4275618374558304,"Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger.
On calibration of modern neural
networks. In International Conference on Machine Learning, pp. 1321–1330. PMLR, 2017."
REFERENCES,0.43109540636042404,"Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common cor-
ruptions and perturbations. Proceedings of the International Conference on Learning Represen-
tations, 2019."
REFERENCES,0.43462897526501765,"Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassiﬁed and out-of-distribution
examples in neural networks. arXiv preprint arXiv:1610.02136, 2016."
REFERENCES,0.4381625441696113,"Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier
exposure. arXiv preprint arXiv:1812.04606, 2018."
REFERENCES,0.4416961130742049,"Yen-Chang Hsu, Yilin Shen, Hongxia Jin, and Zsolt Kira.
Generalized odin: Detecting out-
of-distribution image without learning from out-of-distribution data.
In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10951–10960, 2020."
REFERENCES,0.4452296819787986,"Eyke H¨ullermeier and Willem Waegeman. Aleatoric and epistemic uncertainty in machine learning:
An introduction to concepts and methods. Machine Learning, 110(3):457–506, 2021."
REFERENCES,0.44876325088339225,"Sooyong Jang, Insup Lee, and James Weimer. Improving classiﬁer conﬁdence using lossy label-
invariant transformations. In International Conference on Artiﬁcial Intelligence and Statistics,
pp. 4051–4059. PMLR, 2021."
REFERENCES,0.45229681978798586,"Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced re-
search). a. URL http://www.cs.toronto.edu/˜kriz/cifar.html."
REFERENCES,0.4558303886925795,"Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-100 (canadian institute for advanced
research). b. URL http://www.cs.toronto.edu/˜kriz/cifar.html."
REFERENCES,0.45936395759717313,"Meelis Kull, Miquel Perello-Nieto, Markus K¨angsepp, Hao Song, Peter Flach, et al. Beyond temper-
ature scaling: Obtaining well-calibrated multiclass probabilities with dirichlet calibration. arXiv
preprint arXiv:1910.12656, 2019."
REFERENCES,0.4628975265017668,"Ananya Kumar, Percy Liang, and Tengyu Ma.
Veriﬁed uncertainty calibration.
arXiv preprint
arXiv:1909.10155, 2019."
REFERENCES,0.4664310954063604,"Aviral Kumar, Sunita Sarawagi, and Ujjwal Jain. Trainable calibration measures for neural networks
from kernel mean embeddings. In International Conference on Machine Learning, pp. 2805–
2814. PMLR, 2018."
REFERENCES,0.46996466431095407,"Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles. arXiv preprint arXiv:1612.01474, 2016."
REFERENCES,0.4734982332155477,"Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple uniﬁed framework for detecting
out-of-distribution samples and adversarial attacks. Advances in neural information processing
systems, 31, 2018."
REFERENCES,0.47703180212014135,"Shiyu Liang, Yixuan Li, and Rayadurgam Srikant. Enhancing the reliability of out-of-distribution
image detection in neural networks. arXiv preprint arXiv:1706.02690, 2017."
REFERENCES,0.48056537102473496,"Jeremiah Zhe Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Tania Bedrax-Weiss, and Balaji Lakshmi-
narayanan. Simple and principled uncertainty estimation with deterministic deep learning via
distance awareness. arXiv preprint arXiv:2006.10108, 2020a."
REFERENCES,0.4840989399293286,"Weitang Liu, Xiaoyun Wang, John D Owens, and Yixuan Li.
Energy-based out-of-distribution
detection. arXiv preprint arXiv:2010.03759, 2020b."
REFERENCES,0.4876325088339223,"Weiyang Liu, Zhen Liu, Zhiding Yu, Bo Dai, Rongmei Lin, Yisen Wang, James M Rehg, and
Le Song. Decoupled networks. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 2771–2779, 2018."
REFERENCES,0.4911660777385159,"Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv
preprint arXiv:1608.03983, 2016."
REFERENCES,0.49469964664310956,"Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018."
REFERENCES,0.49823321554770317,"Jose G Moreno-Torres, Troy Raeder, Roc´ıo Alaiz-Rodr´ıguez, Nitesh V Chawla, and Francisco Her-
rera. A unifying view on dataset shift in classiﬁcation. Pattern recognition, 45(1):521–530, 2012."
REFERENCES,0.5017667844522968,"Jishnu Mukhoti, Andreas Kirsch, Joost van Amersfoort, Philip HS Torr, and Yarin Gal. Deterministic
neural networks with appropriate inductive biases capture epistemic and aleatoric uncertainty.
arXiv preprint arXiv:2102.11582, 2021."
REFERENCES,0.5053003533568905,"Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining well calibrated proba-
bilities using bayesian binning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
volume 29, 2015."
REFERENCES,0.508833922261484,"Courtney Napoles, Matthew R Gormley, and Benjamin Van Durme.
Annotated gigaword.
In
Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale
Knowledge Extraction (AKBC-WEKEX), pp. 95–100, 2012."
REFERENCES,0.5123674911660777,"Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading
digits in natural images with unsupervised feature learning.
2011.
URL http://ufldl.
stanford.edu/housenumbers/nips2011_housenumbers.pdf."
REFERENCES,0.5159010600706714,"Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua V
Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty?
evaluating predictive uncertainty under dataset shift. arXiv preprint arXiv:1906.02530, 2019."
REFERENCES,0.519434628975265,"Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word
representation. In Proceedings of the 2014 conference on empirical methods in natural language
processing (EMNLP), pp. 1532–1543, 2014."
REFERENCES,0.5229681978798587,"Amir Rahimi, Amirreza Shaban, Ching-An Cheng, Richard Hartley, and Byron Boots. Intra order-
preserving functions for calibration of multi-class neural networks. Advances in Neural Informa-
tion Processing Systems, 33:13456–13467, 2020."
REFERENCES,0.5265017667844523,"Abhijit Guha Roy, Jie Ren, Shekoofeh Azizi, Aaron Loh, Vivek Natarajan, Basil Mustafa, Nick
Pawlowski, Jan Freyberg, Yuan Liu, Zach Beaver, et al.
Does your dermatology classiﬁer
know what it doesn’t know?
detecting the long-tail of unseen conditions.
arXiv preprint
arXiv:2104.03829, 2021."
REFERENCES,0.5300353356890459,"James Smith, Jonathan Balloch, Yen-Chang Hsu, and Zsolt Kira. Memory-efﬁcient semi-supervised
continual learning: The world is its own replay buffer. arXiv preprint arXiv:2101.09536, 2021."
REFERENCES,0.5335689045936396,"Jihoon Tack, Sangwoo Mo, Jongheon Jeong, and Jinwoo Shin. Csi: Novelty detection via contrastive
learning on distributionally shifted instances. arXiv preprint arXiv:2007.08176, 2020."
REFERENCES,0.5371024734982333,"Engkarat Techapanurak, Masanori Suganuma, and Takayuki Okatani. Hyperparameter-free out-of-
distribution detection using softmax of scaled cosine similarity. arXiv preprint arXiv:1905.10628,
2019."
REFERENCES,0.5406360424028268,"Sunil Thulasidasan, Gopinath Chennupati, Jeff Bilmes, Tanmoy Bhattacharya, and Sarah Michalak.
On mixup training: Improved calibration and predictive uncertainty for deep neural networks.
arXiv preprint arXiv:1905.11001, 2019."
REFERENCES,0.5441696113074205,"Sunil Thulasidasan, Sushil Thapa, Sayera Dhaubhadel, Gopinath Chennupati, Tanmoy Bhat-
tacharya, and Jeff Bilmes. A simple and effective baseline for out-of-distribution detection using
abstention. 2020."
REFERENCES,0.5477031802120141,"Junjiao Tian, Dylan Yung, Yen-Chang Hsu, and Zsolt Kira. A geometric perspective towards neural
calibration via sensitivity decomposition. In NeurIPS, 2021."
REFERENCES,0.5512367491166078,"Joost Van Amersfoort, Lewis Smith, Yee Whye Teh, and Yarin Gal. Uncertainty estimation using a
single deep deterministic neural network. In International Conference on Machine Learning, pp.
9690–9700. PMLR, 2020."
REFERENCES,0.5547703180212014,"Martin Wattenberg, Fernanda Vi´egas, and Ian Johnson. How to use t-sne effectively. Distill, 1(10):
e2, 2016."
REFERENCES,0.558303886925795,"Jim Winkens, Rudy Bunel, Abhijit Guha Roy, Robert Stanforth, Vivek Natarajan, Joseph R Ledsam,
Patricia MacWilliams, Pushmeet Kohli, Alan Karthikesalingam, Simon Kohl, et al. Contrastive
training for improved out-of-distribution detection. arXiv preprint arXiv:2007.05566, 2020."
REFERENCES,0.5618374558303887,"Pingmei Xu, Krista A Ehinger, Yinda Zhang, Adam Finkelstein, Sanjeev R Kulkarni, and Jianxiong
Xiao. Turkergaze: Crowdsourcing saliency with webcam based eye tracking. arXiv preprint
arXiv:1504.06755, 2015."
REFERENCES,0.5653710247349824,"Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun:
Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv
preprint arXiv:1506.03365, 2015."
REFERENCES,0.568904593639576,"Qing Yu, Daiki Ikami, Go Irie, and Kiyoharu Aizawa. Multi-task curriculum framework for open-set
semi-supervised learning. In European Conference on Computer Vision, pp. 438–454. Springer,
2020."
REFERENCES,0.5724381625441696,"Sergey Zagoruyko and Nikos Komodakis.
Wide residual networks.
arXiv preprint
arXiv:1605.07146, 2016."
REFERENCES,0.5759717314487632,"Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10
million image database for scene recognition. IEEE transactions on pattern analysis and machine
intelligence, 40(6):1452–1464, 2017."
APPENDIX,0.5795053003533569,"6
APPENDIX"
INTRODUCTION TO OUT-OF-DISTRIBUTION DETECTION,0.5830388692579506,"6.1
INTRODUCTION TO OUT-OF-DISTRIBUTION DETECTION"
INTRODUCTION TO OUT-OF-DISTRIBUTION DETECTION,0.5865724381625441,"Out-of-Distribution detection is the task of separating in-distribution data from out-of-distribution
(OOD) data. For example, a classiﬁer, trained on a training distribution Ptr(X, Y ), is tested on a dif-
ferent test joint distribution Ptst(X, Y ). When the test joint distribution Ptst(X, Y ) ̸= Ptr(X, Y ),
data sampled from Ptst(X, Y ) are considered OOD data. Ptst(X, Y ) can be different in two ways,
i.e., covariate shift and concept shift (Sec. 3.2). Data sampled from the same distribution as train-
ing, Ptr(X, Y ), such as a conventional validation/test split, are in-distribution (ID) data. To detect
OOD data, a score function is used such as maximum softmax probability (Hendrycks & Gimpel,
2016) and energy (Liu et al., 2020b). The score function assigns higher value to OOD data than ID
data. Consequently, OOD data can be detected by setting a suitable threshold. The commonly used
benchmark, Area Under the Receiver Operating Characteristic curve (AUROC), plots true positive
rate of ID data against false positive rate of OOD data by sweeping through a range of thresholds,
and is a threshold-independent metric for measuring OOD detection performance."
INTRODUCTION TO CONFIDENCE CALIBRATION,0.5901060070671378,"6.2
INTRODUCTION TO CONFIDENCE CALIBRATION"
INTRODUCTION TO CONFIDENCE CALIBRATION,0.5936395759717314,"Colloquially, conﬁdence calibration refers to aligning the maximum predicted probability (conﬁ-
dence) from a multi-class classiﬁer to the empirical accuracy of the predicted class to be the ground
truth. For example, if a classiﬁer classiﬁes 100 images to dog with a conﬁdence of 0.8 for each
prediction, than 80 out of 100 those images should contain a dog. Formally, we adopt the deﬁnition
from Kull et al. (2019) to deﬁne conﬁdence calibration. Let ˆP : X →∆x be a probabilistic multi-
class classiﬁer for M classes. For any input x ∈X, ˆP(x) = ( ˆP1(x), ..., ˆPM(x)) is a probability
vector."
INTRODUCTION TO CONFIDENCE CALIBRATION,0.5971731448763251,"Deﬁnition 1 A probabilistic classiﬁer ˆP : X →∆x is conﬁdence-calibrated, if for any p ∈[0, 1]"
INTRODUCTION TO CONFIDENCE CALIBRATION,0.6007067137809188,P(Y = arg max ˆP(x)| max ˆP(x) = p) = p
INTRODUCTION TO CONFIDENCE CALIBRATION,0.6042402826855123,One notion of conﬁdence calibration is the expected difference between the conﬁdence and accuracy.
INTRODUCTION TO CONFIDENCE CALIBRATION,0.607773851590106,"E ˆ
P
hP(Y = arg max ˆP(x)| max ˆP(x) = p) −p

i"
INTRODUCTION TO CONFIDENCE CALIBRATION,0.6113074204946997,"Expected Calibration Error (ECE) (Naeini et al., 2015) (Guo et al., 2017) is a metric that approxi-
mates this expectation. ECE partitions predictions into several M equally-sized bins according to
the predicted conﬁdence, and then calculates average accuracy and conﬁdence for each bin. Let ˆy
and ˆp be the predicted class and conﬁdence of an input x with corresponding label y. Bn denotes
the n-th bin. The accuracy and conﬁdence are calculated for each bin as follows:"
INTRODUCTION TO CONFIDENCE CALIBRATION,0.6148409893992933,"acc(Bn) =
1
|Bn| X"
INTRODUCTION TO CONFIDENCE CALIBRATION,0.6183745583038869,"i∈Bn
1(ˆyi = yi)
(9)"
INTRODUCTION TO CONFIDENCE CALIBRATION,0.6219081272084805,"conf(Bn) =
1
|Bn| X"
INTRODUCTION TO CONFIDENCE CALIBRATION,0.6254416961130742,"i∈Bn
ˆpi
(10)"
INTRODUCTION TO CONFIDENCE CALIBRATION,0.6289752650176679,"Than ECE is deﬁned as a weighted sum of the difference between conﬁdence and accuracy in each
bin. Therefore, lower ECE indicates better calibration. ECE = N
X n=1 |Bn|"
INTRODUCTION TO CONFIDENCE CALIBRATION,0.6325088339222615,"N
|acc(Bn) −conf(Bn)|"
GEOMETRIC SENSITIVITY DECOMPOSITION REVIEW,0.6360424028268551,"6.3
GEOMETRIC SENSITIVITY DECOMPOSITION REVIEW"
GEOMETRIC SENSITIVITY DECOMPOSITION REVIEW,0.6395759717314488,"Geometric sensitivity decomposition (GSD) is proposed in Tian et al. (2021) to improve the sen-
sitivity of neural networks to input changes. Speciﬁcally, GSD views the output of the last linear
layer, i.e., logits, in a softmax-linear model as inner products, between the feature and weights in"
GEOMETRIC SENSITIVITY DECOMPOSITION REVIEW,0.6431095406360424,"the linear layer, , l ∈RM =< f, W >. The inner products can be written as products of norms
and cosine similarity, e.g., the ith logit is l∗
i = ∥f ∗∥2∥wi∥2 cos φ∗
i . GSD proposes to decompose
norms, ∥f ∗∥2, and angles, φ∗
i , into two components: an instance-dependent residual component and
an instance-independent scalar as shown in Eq. 11. The instance-independent scalar acts as freely
optimizable parameters during training to minimize training loss.
∥f ∗∥2 = ∥f∥2 + Cf
|φ∗
i | = |φi| −|Cφ|
(11)"
GEOMETRIC SENSITIVITY DECOMPOSITION REVIEW,0.6466431095406361,Decomposed components in Eq. 11 can be plugged into equation for logits.
GEOMETRIC SENSITIVITY DECOMPOSITION REVIEW,0.6501766784452296,"∥f ∗∥2 cos φ∗
i = ∥f ∗∥2 cos |φ∗
i | = (∥f∥2 + Cf) cos (|φi| −|Cφ|)
(12)"
GEOMETRIC SENSITIVITY DECOMPOSITION REVIEW,0.6537102473498233,"= (∥f∥2 + Cf)
1
cos |Cφ| cos |φi| "
GEOMETRIC SENSITIVITY DECOMPOSITION REVIEW,0.657243816254417,"


1 −sin |Cφ|2 "
GEOMETRIC SENSITIVITY DECOMPOSITION REVIEW,0.6607773851590106,"


1 −cos |Cφ| sin |φi|"
GEOMETRIC SENSITIVITY DECOMPOSITION REVIEW,0.6643109540636042,"sin |Cφ| cos |φi|
|
{z
}
E.q. 13 "
GEOMETRIC SENSITIVITY DECOMPOSITION REVIEW,0.6678445229681979,"


 "
GEOMETRIC SENSITIVITY DECOMPOSITION REVIEW,0.6713780918727915,"


"
GEOMETRIC SENSITIVITY DECOMPOSITION REVIEW,0.6749116607773852,"Eq. 12 can be simpliﬁed by assuming that the angle |φ∗
i | is small. This equivalent assumes that
training data are closely clustered to linear classiﬁers speciﬁed by the linear weight W."
GEOMETRIC SENSITIVITY DECOMPOSITION REVIEW,0.6784452296819788,"cos |Cφ| sin |φi|
sin |Cφ| cos |φi| = sin (|φi| + |Cφ|) + sin |φi|"
GEOMETRIC SENSITIVITY DECOMPOSITION REVIEW,0.6819787985865724,"sin (|φi| + |Cφ|) −sin |φy| ≈1
(13)"
GEOMETRIC SENSITIVITY DECOMPOSITION REVIEW,0.6855123674911661,"Therefore, logits can be written as the following."
GEOMETRIC SENSITIVITY DECOMPOSITION REVIEW,0.6890459363957597,"∥f ∗∥2∥wi∥2 cos φ∗
i ≈ "
GEOMETRIC SENSITIVITY DECOMPOSITION REVIEW,0.6925795053003534,"


1
cos Cφ
| {z }
α"
GEOMETRIC SENSITIVITY DECOMPOSITION REVIEW,0.696113074204947,"∥f∥2 +
1
cos Cφ
| {z }
α"
GEOMETRIC SENSITIVITY DECOMPOSITION REVIEW,0.6996466431095406,"β
z}|{
Cf "
GEOMETRIC SENSITIVITY DECOMPOSITION REVIEW,0.7031802120141343,"

∥wi∥2 cos φi
(14)"
GEOMETRIC SENSITIVITY DECOMPOSITION REVIEW,0.7067137809187279,"Because cos Cφ and Cf are instance-independent, we can parametrize them separately from the main
network as α and β respectively."
INTRA ORDER PRESERVING FUNCTIONS REVIEW,0.7102473498233216,"6.4
INTRA ORDER PRESERVING FUNCTIONS REVIEW"
INTRA ORDER PRESERVING FUNCTIONS REVIEW,0.7137809187279152,"Rahimi et al. (2020) formalizes a family of powerful calibration function: intra order preserving
functions. Many familiar functions are in this family such as the softmax function and temperature
scaling (Guo et al., 2017). Formally, a function f : Rn →Rn is intra order-preserving if for
any x ∈Rn, both x and f(x) share share the same ranking. The following theorem from Rahimi
et al. (2020) provides sufﬁcient and necessary conditions for constructing an intra order preserving
function."
INTRA ORDER PRESERVING FUNCTIONS REVIEW,0.7173144876325088,"Theorem 1 A continuous function f : Rn →Rn is intra order-preserving, if and only if f(x) =
S(x)−1Uw(x) with U being an upper-triangular matrix of ones and w : Rn →Rn being a
continuous function such that"
INTRA ORDER PRESERVING FUNCTIONS REVIEW,0.7208480565371025,"• wi(x) = 0 if yi = yi−1 and i < n,"
INTRA ORDER PRESERVING FUNCTIONS REVIEW,0.7243816254416962,"• wi(x) > 0 if yi > yi1 and i < n,"
INTRA ORDER PRESERVING FUNCTIONS REVIEW,0.7279151943462897,• wn(x) is arbitrary
INTRA ORDER PRESERVING FUNCTIONS REVIEW,0.7314487632508834,"where y = S(x)x is the sorted version (descending, i.e., y1 ≥y2) of x and S(x) : Rn →Rn is a
sorting matrix."
INTRA ORDER PRESERVING FUNCTIONS REVIEW,0.734982332155477,"The task of constructing an intra order preserving function comes down to designing the function
w(x). A speciﬁc form of w(x) is proposed in Rahimi et al. (2020): wi(x) = σ(yi −yi−1)mi(x)."
INTRA ORDER PRESERVING FUNCTIONS REVIEW,0.7385159010600707,"As long as σ is a positive function, i.e., σ(x) = 0 when x = 0 and σ(x) > 0 otherwise, and
m(x) > 0 is a strictly positive function, the contructed function will satisfy Theorem 1."
INTRA ORDER PRESERVING FUNCTIONS REVIEW,0.7420494699646644,"Our construction of α and β satisﬁes the necessary conditions. For calibration, the function
f acts on the logits l so we use l instead of x in subsequent proof. Let li = ∥f∥2∥wi∥2 cos φi
denote the sorted12 ith logit and ¯li =

1
α(l)∥f∥2 + β(l)"
INTRA ORDER PRESERVING FUNCTIONS REVIEW,0.7455830388692579,"α(l)

∥wi∥2 cos φi detnote the actual output ith"
INTRA ORDER PRESERVING FUNCTIONS REVIEW,0.7491166077738516,"logit (sorted) with α and β. The goal is to show that ¯l and l share the same ranking by satisfying
Theorem 1. We start by extracting the m(x) function in Eq. 15."
INTRA ORDER PRESERVING FUNCTIONS REVIEW,0.7526501766784452,"¯li =
 1"
INTRA ORDER PRESERVING FUNCTIONS REVIEW,0.7561837455830389,α(l)∥f∥2 + β(l) α(l)
INTRA ORDER PRESERVING FUNCTIONS REVIEW,0.7597173144876325,"
∥wi∥2 cos φi =
 1"
INTRA ORDER PRESERVING FUNCTIONS REVIEW,0.7632508833922261,"α(l) +
β(l)
α(l)∥f∥2 "
INTRA ORDER PRESERVING FUNCTIONS REVIEW,0.7667844522968198,"|
{z
}
m(l)"
INTRA ORDER PRESERVING FUNCTIONS REVIEW,0.7703180212014135,"li
z
}|
{
∥f∥2∥wi∥2 cos φi
(15)"
INTRA ORDER PRESERVING FUNCTIONS REVIEW,0.773851590106007,"Therefore, we can construct the w(x) function by taking the difference between two adjacent logits
for i < n.





"
INTRA ORDER PRESERVING FUNCTIONS REVIEW,0.7773851590106007,"



"
INTRA ORDER PRESERVING FUNCTIONS REVIEW,0.7809187279151943,"wi(l) = ¯li −¯li−1 =
 1"
INTRA ORDER PRESERVING FUNCTIONS REVIEW,0.784452296819788,"α(l) +
β(l)
α(l)∥f∥2 "
INTRA ORDER PRESERVING FUNCTIONS REVIEW,0.7879858657243817,"|
{z
}
m(l)>0"
INTRA ORDER PRESERVING FUNCTIONS REVIEW,0.7915194346289752,"σ≥0
z
}|
{
(li −li−1),
for
i < n"
INTRA ORDER PRESERVING FUNCTIONS REVIEW,0.7950530035335689,"wi(l) = li
for
i = n (16)"
INTRA ORDER PRESERVING FUNCTIONS REVIEW,0.7985865724381626,where li ≥li−1.
INTRA ORDER PRESERVING FUNCTIONS REVIEW,0.8021201413427562,"Because 0 < α(l) < 1 and β(l) > 0, the function m(l) is strictly positive and thus satisﬁes Theorem
1 in Rahimi et al. (2020). Speciﬁcally, m(l) corresponds to the strictly positive function m(x) and
li −li−1 corresponds to the positive function σ(yi −yi−1), due to the sorting in descending order."
IMPLEMENTATION DETAILS,0.8056537102473498,"6.5
IMPLEMENTATION DETAILS"
IMPLEMENTATION DETAILS,0.8091872791519434,"Out-of-Distribution Detection Following prior works (Liu et al., 2020a; Van Amersfoort et al.,
2020; Mukhoti et al., 2021), we use Wide-ResNet-28-10 (Mukhoti et al., 2021) for all OOD detection
experiments. We train the model using SGD and cross entropy loss with an initial learning rate of
0.1 for 200 epochs. The SGD optimizer is conﬁged with 5.0e −4 weight decay and 0.9 momentum.
Batch size is 128. The learning rate is annealed with a cosine scheduler (Loshchilov & Hutter, 2016).
We adopt the ofﬁcial code13 for the implementation of DDU (Mukhoti et al., 2021). DDU (Mukhoti
et al., 2021) and Mahanobis (Lee et al., 2018) distance use density estimation by ﬁtting a Gaussian
mixture model (GMM) to the learned feature space. We use the ofﬁcial implementation of DDU to
ﬁt a GMM to the feature space (feature from the CNN). For Manhanobis distance, we ﬁt a GMM to
the low dimensional logit space instead for computational stability."
IMPLEMENTATION DETAILS,0.8127208480565371,"Calibration The backbone training follows the same procedure as the previous section. During the
calibration/tuning stage, we train all models for 20 epochs with cosine learning rate decay using
SGD and cross entropy loss. In this stage, we freeze the backbone models and only tune the cal-
ibration parameters/functions following (Guo et al., 2017). In our case, only α and β are tuned.
For calibration on out-of-distribution data, the models are tuned on the entire validation set. For
calibration on in-distribution data, we use 5-fold cross validation using the validation set following
prior work (Rahimi et al., 2020). For matrix scaling Guo et al. (2017) and Dirichlet scaling (Kull
et al., 2019), we use the Off-Diagonal and Intercept Regularisation (ODIR) (Kull et al., 2019) with a
default scaling of 1 × 10−7. Note that we use the same initial learning rate, 0.1, for all methods ex-
cept for Dirichlet scaling. Dirichlet scaling requires smaller learning rate and we tuned the learning
rate on different datasets. For CIFAR10, we use 0.01 and for CIFAR100 we use 0.005."
IMPLEMENTATION DETAILS,0.8162544169611308,"6.6
CIFAR100 CONCEPT SHIFT SPLITS"
IMPLEMENTATION DETAILS,0.8197879858657244,"While it is natural to associate covariate shift with increasing degrees of image corruption, ﬁnding a
dataset to benchmark gradual concept shift is not straightforward because concept shift is tradition-
ally thought as binary: overlapping or non-overlapping. However, not all non-overlapping labels"
IMPLEMENTATION DETAILS,0.823321554770318,"12The original order can be restored by applying S(l)−1 to l.
13https://github.com/omegafragger/DDU"
IMPLEMENTATION DETAILS,0.8268551236749117,"1
2
3
4
5
6
7
8
9
10
AVE.
STD."
IMPLEMENTATION DETAILS,0.8303886925795053,"CIFAR10
airplane
automobile
bird
cat
deer
dog
frog
horse
ship
truck"
IMPLEMENTATION DETAILS,0.833922261484099,"Group 9
cattle
shrew
motorcycle
squirrel
snake
trout
sea
tractor
bus
pickup
24.96
2.39
Group 8
bear
elephant
leopard
camel
lizard
rabbit
beaver
spider
raccoon
orchid
21.99
0.44
Group 7
lion
mountain
crab
bicycle
turtle
beetle
train
mouse
snail
otter
20.18
1.14
Group 6
possum
shark
forest
pine
dinosaur
boy
porcupine
wolf
road
butterﬂy
17.79
0.32
Group 5
girl
rocket
man
tiger
bee
tank
whale
baby
kangaroo
dolphin
16.26
0.44
Group 4
willow
worm
chimpanzee
skunk
cup
mushroom
oak
cockroach
crocodile
hamster
14.64
0.55
Group 3
castle
can
bridge
lobster
house
bed
fox
maple
pear
woman
12.65
0.63
Group 2
palm
streetcar
pepper
keyboard
bottle
seal
rose
couch
caterpillar
goldﬁsh
10.18
0.51
Group 1
ﬂatﬁsh
apple
orange
plate
table
tulip
bowl
television
skyscraper
ray
8.95
0.25
Group 0
wardrobe
lamp
plain
lawnmower
chair
poppy
clock
cloud
sunﬂower
telephone
7.5
0.97"
IMPLEMENTATION DETAILS,0.8374558303886925,"Table 4: CIFAR100 Concept Shift Splits Small group numbers indicate less conceptual similarity
to CIFAR10 classes. The similarity is calculated using inner product between the Glove embeddings
of a CIFAR100 class and a CIFAR10 class. For each CIFAR100 class, the largest similarity to each
CIFAR10 class is taken as the overall similarity to CIFAR10. The average shows average similarity
to CIFAR10 and the standard deviation shows in-group variance."
IMPLEMENTATION DETAILS,0.8409893992932862,"AUROC↑
TNR@TPR95↑
1
2
3
4
5
1
2
3
4
5 g(x)"
IMPLEMENTATION DETAILS,0.8445229681978799,"vanilla
58.66±0.93
64.59±1.42
69.17±1.90
69.18±2.03
72.66±2.44
10.75±0.50
17.31±0.50
24.05±1.12
24.13±1.24
29.88±2.03
α(x)-only
62.01±1.03
69.60±1.55
75.29±1.90
75.46±1.91
79.50±2.32
9.21±1.86
14.04±3.97
19.39±5.64
19.56±5.66
24.38±6.88
α(x)-β(x)
67.00±0.55
77.87±0.51
85.77±0.53
85.86±0.53
91.25±0.56
14.07±1.50
26.32±2.55
41.77±3.03
42.33±3.11
57.99±2.83"
IMPLEMENTATION DETAILS,0.8480565371024735,"h(y, x)"
IMPLEMENTATION DETAILS,0.8515901060070671,"vanilla
61.02±0.44
69.80±0.74
76.43±0.79
76.47±0.81
81.39±0.90
9.98±0.70
16.84±1.15
24.70±1.53
24.61±1.78
32.13±2.08
α(x)-only
63.98±0.88
74.39±0.97
81.58±0.91
81.66±0.93
86.67±0.85
10.52±1.07
18.18±2.20
27.69±3.68
27.65±3.96
36.79±5.43
α(x)-β(x)
59.11±0.27
68.43±0.78
76.19±0.85
76.16±0.85
82.20±0.73
10.22±0.23
17.72±0.85
27.28±1.32
27.19±1.26
36.91±1.55 U"
IMPLEMENTATION DETAILS,0.8551236749116607,"vanilla
60.66±0.51
68.42±0.65
74.32±0.90
74.34±1.00
78.72±1.21
10.36±0.37
17.60±0.67
26.08±0.72
25.97±1.10
33.69±1.24
α(x)-only
64.20±0.86
74.05±1.02
81.16±0.98
81.29±0.99
86.18±0.97
10.25±0.50
17.63±1.08
26.65±1.93
26.62±2.15
35.57±3.52
α(x)-β(x)
66.86±0.47
77.92±0.45
85.82±0.52
85.83±0.47
91.10±0.47
13.32±1.13
25.21±2.12
40.67±3.13
40.74±2.65
56.48±2.62"
IMPLEMENTATION DETAILS,0.8586572438162544,Table 5: Capturing Covariate Shift (Motion Blur) All results are averaged over 5 runs.
IMPLEMENTATION DETAILS,0.8621908127208481,"are the same. For example, pickup truck (CIFAR100) is much closer to truck (CIFAR10) than sun-
ﬂowers (CIFAR100) is semantically. To create this gradual concept/semantic shift, we propose to
divide the CIFAR100 dataset into 10 sub-datasets with increasing conceptual difference from CI-
FAR10 classes. Specially, we use Glove word embeddings (Pennington et al., 2014) wtih 6 billion
tokens trained on the entire wikipedia2014 and Gigaword5 (Napoles et al., 2012) to measure seman-
tic closeness (inner product) between CIFAR100 and CIFAR10 classes. The result is 10 subdatasets
split from CIFAR100 as shown in Tab. 4. Our experiments demonstrate that the conceptual differ-
ence measured by semantic similarity in language translates to a spectrum of near OOD datasets
measured by the difﬁculty of OOD detection in the image space."
ADDITIONAL RESULTS,0.8657243816254417,"6.7
ADDITIONAL RESULTS"
ADDITIONAL RESULTS,0.8692579505300353,"We provide additional results and tabulated results of ﬁgures in the main paper. Speciﬁcally, Tab. 5
and Tab. 8 are tabulated versions of Fig .3 and Fig. 4 in the main paper respectively. In addition to
motion blur, we also provide out-of-distribution detection results on shot noise in Tab. 7 and zoom
blur in Tab. 6."
EXTENDED RELATED WORK,0.872791519434629,"6.8
EXTENDED RELATED WORK"
EXTENDED RELATED WORK,0.8763250883392226,"Out-of-Distribution (OOD) detection methods can be largely divided into two camps depending
on whether they require OOD data during training. Hendrycks et al. (2018) leverages anomalous"
EXTENDED RELATED WORK,0.8798586572438163,"AUROC↑
TNR@TPR95↑
1
2
3
4
5
1
2
3
4
5 g(x)"
EXTENDED RELATED WORK,0.8833922261484098,"vanilla
61.28±1.13
67.27±1.71
72.82±2.27
77.22±2.69
81.84±3.16
13.45±0.40
19.58±1.02
27.45±2.32
34.91±4.21
44.26±6.32
α(x)-only
66.64±1.59
68.72±2.82
72.16±3.87
73.84±5.01
75.84±6.29
10.18±1.70
10.70±3.02
13.38±4.92
14.84±6.35
17.02±8.37
α(x)-β(x)
71.21±5.89
77.72±2.77
81.78±6.68
85.53±7.40
89.74±7.36
20.96±5.50
26.17±8.08
35.67±11.38
43.35±14.53
54.86±18.97"
EXTENDED RELATED WORK,0.8869257950530035,"h(y, x)"
EXTENDED RELATED WORK,0.8904593639575972,"vanilla
66.19±0.78
72.12±1.44
78.67±1.67
83.14±1.81
87.83±1.65
13.43±0.88
18.11±1.57
26.31±2.84
34.37±4.00
45.88±4.64
α(x)-only
69.34±1.63
75.49±2.40
81.93±2.61
86.37±2.55
90.65±2.28
15.69±1.34
22.16±2.80
32.81±4.52
42.91±6.14
55.99±7.39
α(x)-β(x)
63.73±1.26
69.92±2.29
74.80±2.31
79.42±3.61
85.11±4.50
13.73±1.07
18.40±1.67
27.09±2.53
35.60±2.81
48.24±2.39 U"
EXTENDED RELATED WORK,0.8939929328621908,"vanilla
64.63±0.50
70.94±0.88
77.26±1.12
81.87±1.45
86.64±1.54
13.95±0.50
19.35±1.18
28.27±2.01
36.98±3.00
48.82±3.69
α(x)-only
69.65±1.11
74.51±1.67
80.42±1.88
84.46±2.00
88.68±1.86
14.65±0.80
19.19±1.50
27.64±2.55
35.07±3.81
45.66±5.12
α(x)-β(x)
71.25±5.14
78.00±1.74
82.23±5.71
86.14±6.44
90.50±6.41
19.93±3.05
25.84±4.08
36.81±5.72
46.54±7.28
60.70±8.87"
EXTENDED RELATED WORK,0.8975265017667845,Table 6: Capturing Covariate Shift (Zoom Blur) All results are averaged over 5 runs.
EXTENDED RELATED WORK,0.901060070671378,"AUROC↑
TNR@TPR95↑
1
2
3
4
5
1
2
3
4
5 g(x)"
EXTENDED RELATED WORK,0.9045936395759717,"vanilla
59.84±1.81
65.04±2.43
73.34±3.47
75.85±3.97
79.53±4.80
13.08±2.86
18.59±5.03
28.40±11.06
32.46±14.24
38.75±20.04
α(x)-only
61.88±2.75
67.05±3.65
75.05±4.64
77.35±4.66
80.49±4.49
8.48±1.75
10.27±2.85
12.98±5.77
13.82±7.16
15.42±9.38
α(x)-β(x)
64.78±0.99
73.21±1.84
87.01±3.05
90.63±2.99
94.55±2.42
11.34±1.60
18.58±4.55
44.31±12.91
55.61±14.72
71.07±14.96"
EXTENDED RELATED WORK,0.9081272084805654,"h(y, x)"
EXTENDED RELATED WORK,0.911660777385159,"vanilla
68.14±1.72
75.80±2.32
85.92±3.13
88.20±3.24
90.97±3.44
16.93±1.80
24.75±3.08
40.55±6.21
45.58±7.59
52.06±10.06
α(x)-only
69.88±0.79
78.30±0.93
89.39±1.04
91.79±1.14
94.49±1.23
18.64±1.12
28.91±1.36
52.12±3.39
59.56±4.80
69.62±7.44
α(x)-β(x)
68.62±0.86
76.47±1.00
87.26±1.01
90.00±1.40
93.42±1.96
17.41±0.52
26.48±1.73
46.80±4.93
54.26±7.29
65.65±11.14 U"
EXTENDED RELATED WORK,0.9151943462897526,"vanilla
65.07±2.17
72.59±2.29
83.05±2.27
85.62±2.34
88.95±2.84
16.32±1.11
23.96±1.95
39.06±3.77
44.28±5.13
51.05±7.28
α(x)-only
67.81±1.22
75.91±1.38
87.19±1.61
89.81±1.71
92.96±1.74
15.07±0.82
22.39±1.84
40.17±5.87
47.07±7.81
57.90±11.05
α(x)-β(x)
67.23±0.55
76.13±0.98
89.12±2.02
92.19±2.12
95.49±1.89
14.94±0.97
24.27±3.25
51.27±9.38
61.50±11.07
75.45±11.63"
EXTENDED RELATED WORK,0.9187279151943463,Table 7: Capturing Covariate Shift (Shot Noise) All results are averaged over 5 runs.
EXTENDED RELATED WORK,0.9222614840989399,"AUROC↑
TNR@TPR95↑
1
2
3
4
5
6
7
8
9
10
1
2
3
4
5
6
7
8
9
10 g(x)"
EXTENDED RELATED WORK,0.9257950530035336,"vanilla
75.88
79.64
80.35
81.43
80.23
81.48
79.83
80.03
78.99
83.37
33.12
38.52
41.94
41.38
40.42
44.20
39.34
42.18
38.72
47.06
α(x)
80.28
87.23
86.77
89.10
87.37
89.59
86.90
86.27
85.66
87.87
29.34
38.42
37.14
44.28
36.90
44.40
37.32
38.80
32.22
43.00
α(x)-β(x)
79.05
89.50
88.51
90.49
88.97
90.98
89.29
90.98
91.78
91.15
31.20
42.18
41.02
49.14
40.16
50.76
42.94
52.00
49.16
51.32"
EXTENDED RELATED WORK,0.9293286219081273,"h(y, x)"
EXTENDED RELATED WORK,0.9328621908127208,"vanilla
86.78
90.74
91.00
90.20
89.85
91.10
91.84
91.85
93.90
94.09
46.16
52.50
52.58
51.66
48.58
52.64
53.38
58.10
61.60
63.28
α(x)
87.83
91.36
91.68
91.36
91.43
92.43
92.75
93.58
94.87
95.52
47.46
53.78
55.46
55.90
53.44
61.04
58.38
65.60
68.92
72.54
α(x)-β(x)
88.16
89.93
90.18
89.68
89.53
91.31
91.81
92.59
94.41
95.22
48.74
51.66
53.74
53.88
52.86
59.32
57.70
64.56
68.10
72.54 U"
EXTENDED RELATED WORK,0.9363957597173145,"vanilla
84.02
88.52
88.45
88.59
87.52
89.09
89.18
88.96
91.13
92.26
46.78
52.96
54.38
53.38
50.28
54.02
53.88
58.04
60.02
63.40
α(x)
86.54
91.84
91.99
92.56
92.06
93.26
92.69
93.13
94.12
95.10
46.68
54.48
56.30
57.82
53.92
62.20
57.88
62.86
64.56
70.08
α(x)-β(x)
84.58
91.96
91.85
92.65
91.94
93.31
92.55
93.54
95.06
95.07
44.72
54.62
55.10
58.94
52.76
63.54
57.72
66.42
69.20
70.02"
EXTENDED RELATED WORK,0.9399293286219081,Table 8: Capturing Concept Shift (CIFAR100 Splits) All results are averaged over 5 runs.
EXTENDED RELATED WORK,0.9434628975265018,"data in training which enables classiﬁers to generalize and detect unseen OOD data. Thulasidasan
et al. (2020) extends existing classiﬁer to include an OOD class. Roy et al. (2021) assigns multiple
classes for outliers instead of a single class. Our method belongs to the class of methods that do
not assume the availability of OOD data during training. Hendrycks & Gimpel (2016) discovers
that correctly classiﬁed examples have larger maximum softmax probability (MSP) and propose to
use it to detect incorrect predictions and OOD data. Lee et al. (2018) proposes to use Mahalanobis
distance by ﬁtting a Gaussian mixture model (GMM) in the feature space. Mukhoti et al. (2021)
uses log density of the GMM model instead. Liu et al. (2020b) uses an energy score as the uncer-
tainty metric to distinguish between in-distribution and OOD data. ODIN (Liang et al., 2017) uses
a combination of input processing and post-training tuning to improve OOD detection performance.
Generalized ODIN Hsu et al. (2020) (also Techapanurak et al. (2019)) includes an additional net-
work in the last layer to improve OOD detection during training. There are many other interesting
OOD detection approaches that have achieved state-of-the-art performance without OOD data such
as using contrastive learning with various transformations (Winkens et al., 2020; Tack et al., 2020),
training a deep ensemble of multiple models (Lakshminarayanan et al., 2016) and leveraging large
pretrained models (Fort et al., 2021). They require extended training time, hyperparameter tuning
and careful selections of transformations, whereas our method does not introduce any hyperparam-
eters and has negligible inﬂuence on standard cross-entropy training time. For example, Winkens
et al. (2020) trains a constrastive model for 1200 epochs on CIFAR100 whereas our model requires
only 200 epochs (same as standard classiﬁer training) on the same dataset."
EXTENDED RELATED WORK,0.9469964664310954,"Model Calibration methods can also be largely divided in two categories: 1) training time calibra-
tion using augmentations (Thulasidasan et al., 2019; Jang et al., 2021), using modiﬁed losses (Kumar
et al., 2018); 2) post-hoc calibration (Guo et al., 2017; Kull et al., 2019; Kumar et al., 2019; Rahimi
et al., 2020). Guo et al. (2017) proposes to calibrate the conﬁdence of a trained classiﬁer using tem-
perature scaling, a single scalar that softens overconﬁdent softmax predictions. Kull et al. (2019)
extends single-class conﬁdence calibration to multiclass calibration using the Dirichlet distribution.
Kumar et al. (2019) proposes a scaling-binning calibrator that is more sample efﬁcient. Recently,
Rahimi et al. (2020) formally generalizes a family of expressive functions for calibration, the in-
tra order-preserving functions. This class of functions has more representation power to calibrate
more complex decision boundaries in neural networks. Our proposed method belongs to this class
of functions. However, all these works focus on calibration of in-distribution data. To obtain better
calibration on OOD data, on which the conﬁdence of a model needs to decrease accordingly, sensi-
tivity and deterministic uncertainty modeling is explored by SNGP (Van Amersfoort et al., 2020)
and DUQ (Liu et al., 2020a). SNGP uses a bounded spectral normalization regularization (Miyato
et al., 2018) during training and DUQ adopts a two-sided gradient penalty (Gulrajani et al., 2017) to
improve model sensitivity to distribution shift. Our proposed model not only belongs to the family of
intra order-preserving functions, which ensures good in-distribution calibration, but also improves
sensitivity to distribution shift, which improves out-of-distribution calibration simultaneously."
EXTENDED RELATED WORK,0.950530035335689,"a Feature Space 1
b Feature Space 2
c Histogram of Pixel Values"
EXTENDED RELATED WORK,0.9540636042402827,"Figure 5: Fig. 5a and Fig. 5b show visualization of a 3-dimensional feature space of a model trained
on CIFAR10. The black cluster represents Gaussian-noise corrupted CIFAR10 data. Fig. 5c shows
histogram of pixels of CIFAR10, CIFAR100 and SVHN."
DISCUSSION ON COVARIATE AND CONCEPT SCORE DECOMPOSITION,0.9575971731448764,"7
DISCUSSION ON COVARIATE AND CONCEPT SCORE DECOMPOSITION"
DISCUSSION ON COVARIATE AND CONCEPT SCORE DECOMPOSITION,0.9611307420494699,"In Sec. 3.2 in the main paper, two score functions are proposed to capture changes due to covariate
and concept shift. As shown in Eq. 17, g(x) denotes the covariate shift score function and h(y, x)
denotes the concept shift score function."
DISCUSSION ON COVARIATE AND CONCEPT SCORE DECOMPOSITION,0.9646643109540636,"U = max
j
lj −1 M M
X"
DISCUSSION ON COVARIATE AND CONCEPT SCORE DECOMPOSITION,0.9681978798586572,"i=1
li ="
DISCUSSION ON COVARIATE AND CONCEPT SCORE DECOMPOSITION,0.9717314487632509,"g(x)
z}|{
∥f∥2 "
DISCUSSION ON COVARIATE AND CONCEPT SCORE DECOMPOSITION,0.9752650176678446,"max
j
∥wj∥2 cos φj −1 M M
X"
DISCUSSION ON COVARIATE AND CONCEPT SCORE DECOMPOSITION,0.9787985865724381,"i=1
∥wi∥2 cos φi !"
DISCUSSION ON COVARIATE AND CONCEPT SCORE DECOMPOSITION,0.9823321554770318,"|
{z
}
h(y,x) (17)"
DISCUSSION ON COVARIATE AND CONCEPT SCORE DECOMPOSITION,0.9858657243816255,"Conceptually, these two scores disentangle covariate shift and concept shift. However, in practice,
both shifts almost always happen at the same time in the image domain, i.e., covariate shifted data,
e.g., noised data, often result in concept shift, i.e., increasing ambiguity in class assignment. There-
fore, both scores can increase and decrease simultaneously as observed in Sec. 4.1.2 and Sec. 4.1.3.
The importance of separating them conceptually is to provide a clean perspective to study robust
out-of-distribution detection methods that will work well on different situations under either a single
distribution shift or a mixed of shifts. It very likely that better score functions can be derived to
better disentangle the effects of distribution shifts. In turn, methods that improve sensitivity of those
new score functions can be motivated."
ADDITIONAL FIGURES,0.9893992932862191,"8
ADDITIONAL FIGURES"
ADDITIONAL FIGURES,0.9929328621908127,"While it seems impossible that concept shift could happen without change in P(x), this is possible
if we deﬁne the support,P(x), as the distribution of pixels, ie., the lowest level characteristics,
completely stripped of any semantic information. A simple example would be reshufﬂing the pixels
of a CIFAR10 image where the semantics, P(y|x) , is completely changed, while P(x) remains
the same. Using CIFAR100 to represent concept shift is based on this reasoning. We provide
a visualization of pixel distributions of CIFAR10, CIFAR100 and SVHN in Fig. 5c. CIFAR10
and CIFAR100 share very similar pixel distributions while the pixel distributions of SVHN are
very different from those of CIFAR10. The similarity in pixel distributions between CIFAR10 and
CIFAR100 could be attributed to the data generation/collection process."
ADDITIONAL FIGURES,0.9964664310954063,"To further support the statement that norms are sensitive to covariate shift and justify the choice of
deﬁning the covariate shift score g(x) = ||x||2, we show a visualization of CIFAR10 classes in the
feature space of a model trained on CIFAR10 in Fig. 5a and 5b. Similar to Liu et al. (2018), we
change the last feature dimension of a ResNet to 3. In this way, we can directly visualize the learned
feature space without resorting to dimension reduction techniques such as T-SNE (Wattenberg et al.,
2016). In addition to the clean validation data from CIFAR10, we also visualize CIFAR10 data
corrupted by Gaussian noise. The noised data represents severe covariate shift. We can observe that
the noised data are clustered around the origin indicating very small norms."
