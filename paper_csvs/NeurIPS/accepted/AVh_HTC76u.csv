Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.003676470588235294,"It has been observed that the generalization performance of neural networks corre-
lates with the sharpness of their loss landscape. Dinh et al. (2017) [8] have observed
that existing formulations of sharpness measures fail to be invariant with respect to
scaling and reparametrization. While some scale-invariant measures have recently
been proposed, reparametrization-invariant measures are still lacking. Moreover,
they often do not provide any theoretical insights into generalization performance
nor lead to practical use to improve the performance. Based on an information
geometric analysis of the neural network parameter space, in this paper we propose
a reparametrization-invariant sharpness measure that captures the change in loss
with respect to changes in the probability distribution modeled by neural networks,
rather than with respect to changes in the parameter values. We reveal some the-
oretical connections of our measure to generalization performance. In particular,
experiments confirm that using our measure as a regularizer in neural network
training significantly improves performance."
INTRODUCTION,0.007352941176470588,"1
Introduction"
INTRODUCTION,0.011029411764705883,"From recent discoveries in deep learning, it has been conjectured that the generalization performance
of neural networks correlates with the sharpness of their loss landscape. In particular, lower general-
ization performance of large-batch training of stochastic gradient descent (SGD) has been attributed
to its convergence to sharper minima [22, 49], and several optimization methods have shown better
generalization performance by actively finding solutions with lower sharpness [7, 19, 4, 43, 12, 24].
Minimum description length (MDL)-based arguments [16, 17] and the generalization upper bound of
the PAC-Bayes theory for neural networks [10] also suggest that the flatter the loss landscape, the
better the generalization performance. Furthermore, in [21, 32], measures based on the sharpness
concept show better performance in predicting the generalization performance of models among
various generalization measures considered in the past including the margin [5] and norm [34, 33]."
INTRODUCTION,0.014705882352941176,"Various measures for sharpness and flatness have been proposed in the literature. Hochreiter &
Schmidhuber (1997) [17] propose the concept of ‘flat minima,’ in which the flatness of a minimum
is interpreted as the volume of the connected region in the parameter space over which the loss has
approximately similar values. Similarly, the sharpness has often been measured by the maximum loss
value (inside a Euclidean ball) near the minimum [22] or by the spectral norm of the Hessian at the
minimum [49]."
INTRODUCTION,0.01838235294117647,"The above definitions of sharpness have some notable critical flaws. Dinh et al. (2017) [8] point out
that certain parameter scalings (e.g., for neural networks with ReLU activation functions [31]) and
reparametrizations may have no effect on the model output or overall generalization performance,
and yet lead to wildly different values of sharpness.1 Other sharpness measures have been proposed
to address the lack of scale-invariance [26, 46, 40, 39], but these measures fail to be invariant with
respect to reparametrizations; Section 5 of [8] offers several nonlinear reparametrization examples.
The connections between sharpness and generalization performance also remain elusive."
INTRODUCTION,0.022058823529411766,"In this paper we address the lack of scale- and reparametrization-invariance of existing sharpness
measures, as well as the problem that they often do not explain the generalization performance of deep
learning networks nor lead to practical use to improve the performance. Our approach rests on the
observation and insight that sharpness should be measured taking into proper account the geometry
of the underlying parameter space. For this purpose we draw upon tools from information geometry,
paying attention to the fact that in most classification problems, probabilistic classification models
are parametrized in terms of neural networks. In information geometry, the Fisher information matrix
(FIM) serves as a natural Riemannian metric for the parameter space [3] and allows for measuring
the change in probability density with respect to changes in the parameter values."
INTRODUCTION,0.025735294117647058,"The eigenspectra of the FIM of a neural network are observed to have a small number of positive
outliers (the number is usually equated with the number of classes) and a bulk consisting of small
eigenvalues [41, 37, 36, 48, 14]. This observation indicates that the number of principal varying
components of the probability densities modeled by neural networks is significantly lower compared
to the number of parameters. Therefore, identically weighting every possible parameter change
direction in the parameter space – that is, applying Euclidean geometry – can be problematic, since it
places too much weight on parameter change directions that are meaningless with respect to changes
in the probabilistic model, or equivalently, not enough weight is placed along meaningful directions."
INTRODUCTION,0.029411764705882353,"We argue that one should consider these implications and use the change in probabilistic models as a
‘ruler’ when defining a sharpness measure of neural network loss landscapes that does not depend on
how the model is parametrized."
INTRODUCTION,0.03308823529411765,"Building on the above geometric analysis of the neural network parameter space, in this paper we
propose a new sharpness measure based on information geometry. This measure is by definition
reparametrization-invariant. Additionally, we show that this sharpness measure simultaneously satis-
fies the scale-invariance for neural networks with ReLU activation functions, since it is invariant for
parameter transforms that do not change the model output. Hence our measure is free from all the
reparametrization- and scale-variance issues of previous sharpness measures raised by [8]."
INTRODUCTION,0.03676470588235294,"As a second contribution of this paper, we also show that our geometric sharpness measure sheds
an important insight on generalization performance. As detailed in Section 3, when the FIM is
replaced with the Hessian in our measure, the measure is in a form similar to Takeuchi’s information
criterion (TIC), which quantifies the asymptotic bias of the log-likelihood value (usually the negative
loss) evaluated at the maximum likelihood estimates hence corresponding to the expectation of the
generalization gap [44]. TIC for neural networks has empirically shown a strong correlation with
the generalization gap [45]. One can apply the same argument to our measure since the Hessian can
be approximated by the FIM under mild conditions for deep neural networks [28]. In addition, we
show that our measure can be linked to generalization in another geometrical way. The measure
is associated with a margin defined in an information geometric sense, enabling the interpretation
that a smaller sharpness measure indicates larger margins. We also demonstrate experimentally that
using our measure as a regularizer to train neural networks can significantly improve generalization
performance."
INTRODUCTION,0.04044117647058824,Our contributions can be summarized as follows:
INTRODUCTION,0.04411764705882353,"• We provide an information geometric analysis of the neural network parameter space by
investigating the eigensubspace of the Fisher information matrix (FIM) of neural networks.
• We propose a reparametrization- and scale-invariant sharpness measure based on information
geometry that is free from the problems of existing sharpness measures posed by [8], and
discuss the relation between the measure and generalization performance."
INTRODUCTION,0.04779411764705882,"1In this paper, unlike other works where parameter scalings are often referred to as (linear) reparametrizations,
following [8], it is called a reparametrization to represent a model on a different parameter space obtained by
applying a (nonlinear) bijection to the original parameters."
INTRODUCTION,0.051470588235294115,"• We use the proposed measure as a regularizer when training neural networks, and demon-
strate improved generalization performance."
INTRODUCTION,0.05514705882352941,"We provide an information geometric analysis of the neural network parameter space in Section 2.
Building on this analysis, Section 3 presents our reparametrization-invariant sharpness measure
based on information geometry and connects the proposed measure to generalization. In Section 4,
we perform numerical experiments using our measure as a regularizer to improve generalization
performance in neural network training."
AN INFORMATION GEOMETRIC ANALYSIS OF THE NEURAL NETWORK PARAMETER SPACE,0.058823529411764705,"2
An information geometric analysis of the neural network parameter space"
AN INFORMATION GEOMETRIC ANALYSIS OF THE NEURAL NETWORK PARAMETER SPACE,0.0625,"In this paper, we focus on the probabilistic classification models parametrized by neural networks. Let
x ∈RD denote data, and y ∈{1, . . . , C} denote class labels with C as the number of classes. Let
p(x, y) and p(y|x; θ) respectively denote the data generating distribution and the parametric model of
the probability density of classes given data. Here the parameter θ ∈Rm is the set of all the weight
and bias parameters of neural networks. We consider a neural network as a function f(·; θ) : RD →
RC, x 7→f(x; θ) = (f1(x; θ), . . . , fC(x; θ))⊤, where fj(·; θ) : RD →R is a function that returns
the j-th logit for j = 1, . . . , C. Our parametric model is then represented as p(y|x; θ) =
exp(fy(x))
P"
AN INFORMATION GEOMETRIC ANALYSIS OF THE NEURAL NETWORK PARAMETER SPACE,0.0661764705882353,"j exp(fj(x)),"
AN INFORMATION GEOMETRIC ANALYSIS OF THE NEURAL NETWORK PARAMETER SPACE,0.06985294117647059,"and we denote the negative log-likelihood by l(y, f(x; θ)) = −log p(y|x; θ)."
AN INFORMATION GEOMETRIC ANALYSIS OF THE NEURAL NETWORK PARAMETER SPACE,0.07352941176470588,"Suppose data {(x1, y1), . . . , (xN, yN)} are drawn i.i.d. from the data generating distribution p(x, y).
The cross-entropy loss, our training objective function throughout the paper, can be written as
follows:2"
AN INFORMATION GEOMETRIC ANALYSIS OF THE NEURAL NETWORK PARAMETER SPACE,0.07720588235294118,"L(θ) = 1 N N
X"
AN INFORMATION GEOMETRIC ANALYSIS OF THE NEURAL NETWORK PARAMETER SPACE,0.08088235294117647,"i=1
li(θ),
(1)"
AN INFORMATION GEOMETRIC ANALYSIS OF THE NEURAL NETWORK PARAMETER SPACE,0.08455882352941177,"where li(θ) = l(yi, f(xi; θ)) = −log p(yi|xi; θ)."
AN INFORMATION GEOMETRIC ANALYSIS OF THE NEURAL NETWORK PARAMETER SPACE,0.08823529411764706,"2.1
The Fisher information matrix (FIM)"
AN INFORMATION GEOMETRIC ANALYSIS OF THE NEURAL NETWORK PARAMETER SPACE,0.09191176470588236,"The Fisher information matrix (FIM) for the family of probability density functions p(x, y; θ) =
p(x)p(y|x; θ) parametrized by the neural network parameters θ is defined as"
AN INFORMATION GEOMETRIC ANALYSIS OF THE NEURAL NETWORK PARAMETER SPACE,0.09558823529411764,"F(θ) = Ep(x) """
AN INFORMATION GEOMETRIC ANALYSIS OF THE NEURAL NETWORK PARAMETER SPACE,0.09926470588235294,"Ep(y|x;θ) "" ∂l ∂θ"
AN INFORMATION GEOMETRIC ANALYSIS OF THE NEURAL NETWORK PARAMETER SPACE,0.10294117647058823,⊤ ∂l ∂θ ##
AN INFORMATION GEOMETRIC ANALYSIS OF THE NEURAL NETWORK PARAMETER SPACE,0.10661764705882353,"= Ep(x) """
AN INFORMATION GEOMETRIC ANALYSIS OF THE NEURAL NETWORK PARAMETER SPACE,0.11029411764705882,"J(x)⊤Ep(y|x;θ) "" ∂l ∂f"
AN INFORMATION GEOMETRIC ANALYSIS OF THE NEURAL NETWORK PARAMETER SPACE,0.11397058823529412,"⊤ ∂l ∂f # J(x) # = 1 N N
X"
AN INFORMATION GEOMETRIC ANALYSIS OF THE NEURAL NETWORK PARAMETER SPACE,0.11764705882352941,"i=1
J⊤
i
 
diag(pi) −pip⊤
i

Ji,
(2)"
AN INFORMATION GEOMETRIC ANALYSIS OF THE NEURAL NETWORK PARAMETER SPACE,0.1213235294117647,"where the dependence of l(y, f(x; θ)) to y, f(x; θ) is omitted for simplicity and J(x) = ∂f"
AN INFORMATION GEOMETRIC ANALYSIS OF THE NEURAL NETWORK PARAMETER SPACE,0.125,"∂θ (x) ∈
RC×m is the Jacobian of the logits with respect to the parameters. In deriving (2), the expectation
with respect to p(x) is approximated by the finite sum over data points x1, . . . , xN drawn i.i.d. from"
AN INFORMATION GEOMETRIC ANALYSIS OF THE NEURAL NETWORK PARAMETER SPACE,0.12867647058823528,"p(x), Ji = J(xi), and we use the fact that Ep(y|xi;θ)"
AN INFORMATION GEOMETRIC ANALYSIS OF THE NEURAL NETWORK PARAMETER SPACE,0.1323529411764706,"
∂l
∂f
⊤
∂l
∂f

= diag(pi) −pip⊤
i , where"
AN INFORMATION GEOMETRIC ANALYSIS OF THE NEURAL NETWORK PARAMETER SPACE,0.13602941176470587,"pi = (p(1|xi; θ), . . . , p(C|xi; θ))⊤∈RC is the model’s prediction on the probability that xi belongs
to each class and diag(pi) ∈RC×C is a diagonal matrix whose (j, j) entry is (pi)j."
AN INFORMATION GEOMETRIC ANALYSIS OF THE NEURAL NETWORK PARAMETER SPACE,0.13970588235294118,"In this paper, among many of the important properties and applications of the FIM (e.g., being the
Cramer-Rao lower bound of estimator variances in estimation theory and statistics), we focus on the
fact that the FIM can serve as a metric to measure the distance between the parametric probability
density models in information geometry. Differential geometrically speaking, the FIM acts as a natural
Riemannian metric on the statistical manifold, a space where the family of probability densities
modeled with smoothly varying parameters θ ∈Rm is gathered.3 The FIM makes it possible to"
AN INFORMATION GEOMETRIC ANALYSIS OF THE NEURAL NETWORK PARAMETER SPACE,0.14338235294117646,"2We discuss the applicability of our measure to the square loss in Appendix F.
3Note that the family of probabilistic models from neural networks is usually not a manifold in a mathemati-
cally rigorous sense due to the inherent singularities in the neural network parameter space (see Section 12.2 of
[3]), but this fact is not crucial for our discussion."
AN INFORMATION GEOMETRIC ANALYSIS OF THE NEURAL NETWORK PARAMETER SPACE,0.14705882352941177,"5
10
15
20
0 10 20 30 40"
AN INFORMATION GEOMETRIC ANALYSIS OF THE NEURAL NETWORK PARAMETER SPACE,0.15073529411764705,Eigenvalues
AN INFORMATION GEOMETRIC ANALYSIS OF THE NEURAL NETWORK PARAMETER SPACE,0.15441176470588236,"(a) Top 20 eigenvalues of
F(θ) 2
0
2 1 0 1"
AN INFORMATION GEOMETRIC ANALYSIS OF THE NEURAL NETWORK PARAMETER SPACE,0.15808823529411764,"(b)
Perturbed
decision
boundaries from ∆θ1 2
0
2 1 0 1"
AN INFORMATION GEOMETRIC ANALYSIS OF THE NEURAL NETWORK PARAMETER SPACE,0.16176470588235295,"(c)
Perturbed
decision
boundaries from ∆θ2 2
0
2 1 0 1"
AN INFORMATION GEOMETRIC ANALYSIS OF THE NEURAL NETWORK PARAMETER SPACE,0.16544117647058823,"(d)
Perturbed
decision
boundaries from ∆θ10"
AN INFORMATION GEOMETRIC ANALYSIS OF THE NEURAL NETWORK PARAMETER SPACE,0.16911764705882354,"Figure 1: A synthetic three-class classification example. For (b)-(d), the black, red, and blue lines
correspond to decision boundaries of the neural network with the trained parameter values, and pa-
rameter values perturbed along the k-th eigenvector ∆θk ∈Rm (associated with the k-th eigenvalue)
of F(θ) with steps of 0.5 and -0.5, respectively."
AN INFORMATION GEOMETRIC ANALYSIS OF THE NEURAL NETWORK PARAMETER SPACE,0.17279411764705882,"measure geometric quantities such as length, angle, and volume on the manifold. (We refer the reader
to [6, 11] for the backgrounds on the Riemannian manifolds and differential geometry, and [3, 35, 28]
for those on the information geometry and the Fisher information matrix.) Note that when the FIM is
not full-rank, e.g., in the case of overparameterized neural networks with m ≫N, it can be used as a
pseudo metric."
AN INFORMATION GEOMETRIC ANALYSIS OF THE NEURAL NETWORK PARAMETER SPACE,0.17647058823529413,"The FIM is closely related to the KL-divergence, which is frequently used as an information theoretic
measure of discrepancy between probability density functions. The KL-divergence between two
probability density functions with infinitesimal parameter difference dθ ∈Rm can be approximated
as"
AN INFORMATION GEOMETRIC ANALYSIS OF THE NEURAL NETWORK PARAMETER SPACE,0.1801470588235294,"KL(p(x, y; θ + dθ)||p(x, y; θ)) ≈1"
AN INFORMATION GEOMETRIC ANALYSIS OF THE NEURAL NETWORK PARAMETER SPACE,0.18382352941176472,"2dθ⊤F(θ)dθ.
(3)"
AN ANALYSIS OF THE EIGENSUBSPACE OF THE FIM,0.1875,"2.2
An analysis of the eigensubspace of the FIM"
AN ANALYSIS OF THE EIGENSUBSPACE OF THE FIM,0.19117647058823528,"This section discusses the characteristics of the eigensubspace of FIM of neural networks. In the
analysis of the eigenspectra of deep neural network Hessians of [37], the FIM in (2) has been
decomposed as follows:"
AN ANALYSIS OF THE EIGENSUBSPACE OF THE FIM,0.1948529411764706,"F(θ) = 1 N N
X"
AN ANALYSIS OF THE EIGENSUBSPACE OF THE FIM,0.19852941176470587,"i=1
J⊤
i
 
diag(pi) −pip⊤
i

Ji = 1 N N
X"
AN ANALYSIS OF THE EIGENSUBSPACE OF THE FIM,0.20220588235294118,"i=1
J⊤
i M ⊤
i MiJi,
(4)"
AN ANALYSIS OF THE EIGENSUBSPACE OF THE FIM,0.20588235294117646,"where Mi = diag(√pi)
 
I −1p⊤
i

∈RC×C, √pi = (
p"
AN ANALYSIS OF THE EIGENSUBSPACE OF THE FIM,0.20955882352941177,"(pi)1, . . . ,
p"
AN ANALYSIS OF THE EIGENSUBSPACE OF THE FIM,0.21323529411764705,"(pi)C)⊤∈RC, and 1 =
(1, . . . , 1)⊤∈RC is a vector whose elements are all one."
AN ANALYSIS OF THE EIGENSUBSPACE OF THE FIM,0.21691176470588236,"According to the decomposition, the FIM can be thought of as a non-centralized second moment of
the (modified) logit gradients, i.e., each row of MiJi ∈RC×m whose j-th row vector is represented
as"
AN ANALYSIS OF THE EIGENSUBSPACE OF THE FIM,0.22058823529411764,"(MiJi)⊤
j: =
q"
AN ANALYSIS OF THE EIGENSUBSPACE OF THE FIM,0.22426470588235295,(pi)j · 
AN ANALYSIS OF THE EIGENSUBSPACE OF THE FIM,0.22794117647058823,"(Ji,j)⊤− C
X"
AN ANALYSIS OF THE EIGENSUBSPACE OF THE FIM,0.23161764705882354,"j′=1
(pi)j′(Ji,j′)⊤ "
AN ANALYSIS OF THE EIGENSUBSPACE OF THE FIM,0.23529411764705882,"∈Rm,
(5)"
AN ANALYSIS OF THE EIGENSUBSPACE OF THE FIM,0.23897058823529413,"where Ji,j ∈R1×m is the j-th row of Ji. It has been observed that these logit gradients form a kind
of hierarchical structure after sufficient training. In the observation from [36], the gradients of the
c′ ̸= c-th logit calculated from the data in class c are gathered to form a cluster. A matrix obtained
from averaging the outer products of the averaged logit gradients belonging to each cluster is then
attributed to the outliers of the eigenspectra, and the outliers of the eigenvalues appear as much as the
number of classes. That is, the principal eigenvalues of the FIM (and possibly the eigenvectors) can
be closely connected with distinguishing each class c from the rest of the c′ ̸= c classes (as can be
implied from averaging (5) for j ̸= c and by assuming (pi)c ≈1). This becomes more evident in
Appendix A, which assumes the prediction is balanced as (pi)c′ = ϵ for all c′ ̸= c with ϵ ≪1."
AN ANALYSIS OF THE EIGENSUBSPACE OF THE FIM,0.2426470588235294,"In order to more intuitively understand the characteristics of the eigensubspace of FIM implied from
the above analysis, we provide a classification example for two-dimensional synthetic data generated
from mixtures of three Gaussians in Figure 1. We trained a three-layer fully connected neural network
with 70-dimensional hidden units (the number of parameters = 5,393)."
AN ANALYSIS OF THE EIGENSUBSPACE OF THE FIM,0.24632352941176472,"Figure 1 (a) shows the top twenty eigenvalues of the FIM of the trained neural network. Note that
there are few outliers among the eigenvalues in the entire 5,393-dimensional parameter space, with
the remainder being almost 0. The eigenvalues indicate the rate of change of the probability density
(modeled by the neural network) measured by the KL-divergence for a unit change of parameter
values along the corresponding eigenvectors. The existence of a few outliers means that the density
change mainly occurs in only a few specific directions and that the amount of change in probability
density can vary significantly according to the direction of change in parameter values."
AN ANALYSIS OF THE EIGENSUBSPACE OF THE FIM,0.25,"This fact can also be observed in decision boundaries obtained from neural networks with the
parameter values perturbed along some eigenvectors shown in Figure 1 (b-d). When perturbing
the values along the principal eigenvectors, a significant change appears in the decision boundary,
increasing or decreasing the margin of certain classes. On the other hand, for eigenvectors with small
eigenvalues, there is almost no change in the decision boundary according to the same level of change
in parameter values (in the Euclidean sense)."
AN ANALYSIS OF THE EIGENSUBSPACE OF THE FIM,0.2536764705882353,"These examples imply that it would be more meaningful to reflect the different influences on the
probabilistic model according to the change direction when we measure the quantities concerning
changes in the parameter values, e.g., the sharpness of the loss landscape. By reflecting on these
implications and using the unit change in probability distribution modeled by neural networks as our
‘ruler’ to measure the sharpness, that is, applying the information geometry, the following section
presents a sharpness measure that does not rely on how the model is parametrized."
AN INFORMATION GEOMETRIC SHARPNESS MEASURE,0.25735294117647056,"3
An information geometric sharpness measure"
AN INFORMATION GEOMETRIC SHARPNESS MEASURE,0.2610294117647059,"Reflecting the above analysis, we define an information geometric sharpness (IGS) measure as
follows:"
AN INFORMATION GEOMETRIC SHARPNESS MEASURE,0.2647058823529412,"IGS(θ) = 1 N N
X i=1 ∂li ∂θ"
AN INFORMATION GEOMETRIC SHARPNESS MEASURE,0.26838235294117646,"
F(θ)†
∂li ∂θ"
AN INFORMATION GEOMETRIC SHARPNESS MEASURE,0.27205882352941174,"⊤
,
(6)"
AN INFORMATION GEOMETRIC SHARPNESS MEASURE,0.2757352941176471,"where F(θ)† ∈Rm×m is the pseudo-inverse of F(θ) and li = l(yi, f(xi; θ)) is the cross-entropy
loss evaluated at the i-th data (xi, yi)."
AN INFORMATION GEOMETRIC SHARPNESS MEASURE,0.27941176470588236,"Our measure evaluates the squared norm of the loss gradients calculated at each data point using the
(pseudo-inverse of) FIM and averages them for all data points.4 Since
  ∂li"
AN INFORMATION GEOMETRIC SHARPNESS MEASURE,0.28308823529411764,"∂θ
⊤∈Rm belongs to the
span of the eigenvectors of F(θ) with non-zero eigenvalues, our sharpness measure is well-defined in
the sense that there are not any components of
  ∂li"
AN INFORMATION GEOMETRIC SHARPNESS MEASURE,0.2867647058823529,"∂θ

neglected in measuring its information geometric
norm (we elaborate on this in Appendix B)."
AN INFORMATION GEOMETRIC SHARPNESS MEASURE,0.29044117647058826,"Our sharpness measure in (6) is an intrinsic quantity, i.e., coordinate-invariant (in differential geomet-
ric terms). To see why, observe that under a local coordinate transformation (or a reparametrization)
ϕ : Rm →Rm, θ 7→θ′ = ϕ(θ), i.e., the model is now represented with respect to a different
parameter θ′ as ˜f(·; θ′) = f(·; ϕ−1(θ′)) = f(·; θ), F and
  ∂li"
AN INFORMATION GEOMETRIC SHARPNESS MEASURE,0.29411764705882354,"∂θ

transform according to the following
rules: (i) F 7→F ′ = Φ−⊤FΦ−1, where Φ = ∂ϕ"
AN INFORMATION GEOMETRIC SHARPNESS MEASURE,0.2977941176470588,"∂θ ∈Rm×m; (ii)
  ∂li"
AN INFORMATION GEOMETRIC SHARPNESS MEASURE,0.3014705882352941,"∂θ

7→
  ∂li"
AN INFORMATION GEOMETRIC SHARPNESS MEASURE,0.30514705882352944,"∂θ′

=
  ∂li"
AN INFORMATION GEOMETRIC SHARPNESS MEASURE,0.3088235294117647,"∂θ

Φ−1, where"
AN INFORMATION GEOMETRIC SHARPNESS MEASURE,0.3125,"it can be verified that the
  ∂li"
AN INFORMATION GEOMETRIC SHARPNESS MEASURE,0.3161764705882353,"∂θ

F(θ)†   ∂li"
AN INFORMATION GEOMETRIC SHARPNESS MEASURE,0.31985294117647056,"∂θ
⊤remains the same. Since the above measure gives the
same value regardless of which parametrization (e.g., θ or θ′) the statistical model is parametrized, it
is free from the reparametrization-variance issues raised in [8]."
AN INFORMATION GEOMETRIC SHARPNESS MEASURE,0.3235294117647059,"We can also define other reparametrization-invariant measures by measuring the information geomet-
ric norm of the gradients of the entire loss or the mini-batch losses. For mini-batches of size b, the
corresponding measure can be defined as follows:"
AN INFORMATION GEOMETRIC SHARPNESS MEASURE,0.3272058823529412,IGSb(θ) = Ep(B)  
B,0.33088235294117646,"1
b X xi∈B ∂li ∂θ !"
B,0.33455882352941174,"F(θ)†
 
1
b X xi∈B ∂li ∂θ !⊤"
B,0.3382352941176471,",
(7)"
B,0.34191176470588236,"where Ep(B) [·] denotes the expectation with respect to some distribution p(B) of mini-batches
B ⊂{x1, . . . , xN} of size |B| = b. We discuss the relation between (6) and (7) in Appendix C. Note"
B,0.34558823529411764,"4The reason for defining the sharpness measure as in (6) will be evident in Section 3.3 where we develop the
relationship between our measure and the generalization."
B,0.3492647058823529,"that this definition can be linked to the concept of m-sharpness, which measures the sharpness of the
loss landscape by averaging the sharpness of mini-batch losses and shows a better correlation to the
generalization as the batch size reduces [12]."
TRANSFORMATION INVARIANCE,0.35294117647058826,"3.1
Transformation invariance"
TRANSFORMATION INVARIANCE,0.35661764705882354,"Neural networks composed of activation functions such as ReLU and leaky ReLU possess scale-
invariant properties. In the case of layer-wise scaling, for consecutive layers of linear transforms and
ReLU, e.g., f(x; {W1, . . . , WL}) = WL · ReLU(WL−1 · · · ReLU(W1x)), where Wl ∈Rdl×dl−1
for l = 1, . . . , L with d0 = D and dL = C, if each weight Wl is multiplied by a constant cl > 0 and
the constants satisfy QL
l=1 cl = 1, there is no change in the outputs of the neural network under the
same input values. A similar concept includes the node-wise scaling, multiplying weights entering a
node (or a hidden variable in neural networks) by a positive constant and dividing weights out of the
node by the same constant."
TRANSFORMATION INVARIANCE,0.3602941176470588,"These kinds of weight scaling can be viewed as a transformation (or a mapping between identical
parameter spaces) that ensures that the neural network model maintains the same probability density
function, i.e., remains equivalent. If a transformation is differentiable and locally invertible without
changing the probability density function that the neural network parameter models for all parameters
on a given neighborhood U ⊆Rm of θ, our measures defined in (6) and (7) become invariant with
respect to such a transformation. This fact can be expressed as the following proposition:"
TRANSFORMATION INVARIANCE,0.3639705882352941,"Proposition 3.1. Suppose there exist open subsets U, V ⊆Rm and an invertible and locally
differentiable transformation g : U →V that satisfies f(x; θ) = f(x; g(θ)) for all x ∈RD and
θ ∈U with g(U) ⊆V . The information geometric sharpness measures in (6) and (7) are invariant
to such a transformation, i.e., IGS(θ) = IGS(g(θ)) (or IGSb(θ) = IGSb(g(θ))) for all θ ∈U."
TRANSFORMATION INVARIANCE,0.36764705882352944,The proof of Proposition 3.1 is provided in Appendix D.1.
TRANSFORMATION INVARIANCE,0.3713235294117647,"Since the layer-wise and node-wise scalings considered for neural networks with ReLU satisfy
the assumptions of Proposition 3.1, our measures in (6) and (7) are invariant to such scalings.
Proposition 3.1 is more general than the scale-invariance, and Appendix D.2 provides examples of
transformations other than parameter scalings that satisfy the assumptions in the proposition."
A COMPARISON TO SOME PREVIOUS SHARPNESS MEASURES,0.375,"3.2
A comparison to some previous sharpness measures"
A COMPARISON TO SOME PREVIOUS SHARPNESS MEASURES,0.3786764705882353,"IGS
Tr(H)
Fisher -Rao"
A COMPARISON TO SOME PREVIOUS SHARPNESS MEASURES,0.38235294117647056,Rangamani
A COMPARISON TO SOME PREVIOUS SHARPNESS MEASURES,0.3860294117647059,et al.
A COMPARISON TO SOME PREVIOUS SHARPNESS MEASURES,0.3897058823529412,Petzka
A COMPARISON TO SOME PREVIOUS SHARPNESS MEASURES,0.39338235294117646,"et al. 10
4 10
2 100 102 104"
A COMPARISON TO SOME PREVIOUS SHARPNESS MEASURES,0.39705882352941174,"Sharpness(re. )
Sharpness(ori. )"
A COMPARISON TO SOME PREVIOUS SHARPNESS MEASURES,0.4007352941176471,"Figure 2: The ratio of sharpness measures evalu-
ated for the reparametrized models to those for the
original models ( Sharpness (re.)"
A COMPARISON TO SOME PREVIOUS SHARPNESS MEASURES,0.40441176470588236,"Sharpness (ori.)). Note that the y-axis
is in the log scale. For the nonlinear reparametriza-
tion, we use η = g(θ) = (|θ−ˆθ|2 +b)a(θ−ˆθ)+ ˆη
considered in [8] with several choices of (a, b),
which are represented by different colors."
A COMPARISON TO SOME PREVIOUS SHARPNESS MEASURES,0.40808823529411764,"After [8] pointed out the critical scale- and
reparametrization-variance issues in the sharp-
ness measures considered in [17, 22, 49], var-
ious scale-invariant sharpness measures have
been proposed that suit to neural networks with
activation functions satisfying the non-negative
homogeneity conditions (i.e., σ(a·x) = a·σ(x)
for a > 0) such as ReLU [40, 39, 46, 26]. The
measure presented in [40] is based on the ge-
ometry of the quotient manifold obtained from
an equivalence class of neural networks, estab-
lishing the scale-invariance depending on the
considered equivalence types. The work in [39]
considers a layer-wise flatness measure for neu-
ral networks and additionally explores the con-
ditions for the measure to explain the generaliza-
tion well. Also, a scale-invariant sharpness measure based on the PAC-Bayes theory is developed in
[46]. The Fisher-Rao norm is proposed as a capacity measure for neural networks in [26] and defined
as θ⊤F(θ)θ, which is scale-invariant."
A COMPARISON TO SOME PREVIOUS SHARPNESS MEASURES,0.4117647058823529,"However, the above measures do not satisfy the invariance to general nonlinear reparametrizations,
hence solving the problem raised by [8] only partially. That is, for a nonlinear reparametrization
θ 7→η = g(θ), the measures evaluated on the reparametrized model ˜f(·; η) = f(·; g−1(η)) = f(·; θ)
(with respect to η) can arbitrarily vary from those evaluated on the original model f(·; θ) (with respect
to θ). In Figure 2, we empirically demonstrate the existence of this problem in conventional sharpness"
A COMPARISON TO SOME PREVIOUS SHARPNESS MEASURES,0.41544117647058826,"measures such as the trace of the Hessian, the measures of [26], [40], and [39] (respectively denoted
as Tr(H), Fisher-Rao, Rangamani et al., and Petzka et al. on the x-axis of the figure; see Appendix G.1
for details). We can observe that the magnitudes of these measures can vary significantly."
A COMPARISON TO SOME PREVIOUS SHARPNESS MEASURES,0.41911764705882354,"Compared with the previous measures, only our measure (denoted as IGS on the x-axis of Figure 2)
is invariant to the considered reparametrizations. These experiments empirically demonstrate that our
measure satisfies the reparametrization-invariance, hence providing a solution that properly resolves
the issues posed by [8]."
CONNECTIONS TO THE GENERALIZATION,0.4227941176470588,"3.3
Connections to the generalization"
CONNECTIONS TO THE GENERALIZATION,0.4264705882352941,"3.3.1
Connections to Takeuchi’s information criterion (TIC)"
CONNECTIONS TO THE GENERALIZATION,0.43014705882352944,"Our measures can be linked to generalization in different aspects. When we train parametric models
via the maximum likelihood estimation (this is equivalent to minimizing the negative log-likelihood of
(1)), the obtained maximum likelihood estimate is asymptotically unbiased, guaranteed theoretically
by the asymptotic normality. However, the log-likelihood value (or the negative loss) evaluated at the
obtained estimate does not enjoy such a property."
CONNECTIONS TO THE GENERALIZATION,0.4338235294117647,"The well-known Akaike’s information criterion (AIC) in the model selection literature calculates
this bias under the assumption that the parametric model contains the actual data distribution [1].
Takeuchi’s information criterion (TIC) is a generalized form of the AIC by considering the case of
misspecified models, i.e., the parametric models do not contain the actual data distribution [44, 9, 45]."
CONNECTIONS TO THE GENERALIZATION,0.4375,"The bias b(ˆθ) of the log-likelihood value evaluated at the maximum likelihood estimate is defined as
follows:"
CONNECTIONS TO THE GENERALIZATION,0.4411764705882353,b(ˆθ) = ED  1 |D| X
CONNECTIONS TO THE GENERALIZATION,0.44485294117647056,"(xi,yi)∈D
log p(xi, yi; ˆθ(D)) −Ep(x,y)
h
log p(x, y; ˆθ(D))
i
"
CONNECTIONS TO THE GENERALIZATION,0.4485294117647059,",
(8)"
CONNECTIONS TO THE GENERALIZATION,0.4522058823529412,"where D = {(x1, y1), . . . , (xN, yN)} is the set of N training data with (xi, yi) ∼p(x, y) and ˆθ(D)
denotes the maximum likelihood estimate obtained from using D [9]. Since our loss is the negative
log-likelihood, the first and second terms inside ED[·] correspond to the negative training and test
losses, respectively. Consequently, the bias in (8) becomes the expectation of the parametric model’s
generalization gap (i.e., test loss – training loss)."
CONNECTIONS TO THE GENERALIZATION,0.45588235294117646,"TIC calculates this bias under asymptotic assumptions, i.e., in the limit N →∞, as follows [9]:"
CONNECTIONS TO THE GENERALIZATION,0.45955882352941174,"TIC = lim
N→∞b(ˆθ) = 1"
CONNECTIONS TO THE GENERALIZATION,0.4632352941176471,"N Tr(H(θ0)−1C(θ0)),
(9)"
CONNECTIONS TO THE GENERALIZATION,0.46691176470588236,"where θ0 ∈Rm is a local maximum of the expected log-likelihood, H(θ0) ∈Rm×m is the Hessian"
CONNECTIONS TO THE GENERALIZATION,0.47058823529411764,"of the (expected) loss, and C(θ0) = Ep(x,y)"
CONNECTIONS TO THE GENERALIZATION,0.4742647058823529,"
∂l(y,f(x;θ))"
CONNECTIONS TO THE GENERALIZATION,0.47794117647058826,"∂θ
⊤
∂l(y,f(x;θ))"
CONNECTIONS TO THE GENERALIZATION,0.48161764705882354,"∂θ

θ=θ0
∈Rm×m is the"
CONNECTIONS TO THE GENERALIZATION,0.4852941176470588,"non-centered covariance of the loss gradients. Here both H(θ0) and C(θ0) are evaluated using the
data generating distribution p(x, y)."
CONNECTIONS TO THE GENERALIZATION,0.4889705882352941,"The formulation in (9) is very similar to our information geometric sharpness measure when evaluated
at a local minimum ˆθ ∈Rm of the loss in (1). Compared to TIC, our measure in (6) contains the FIM
instead of the Hessian, and the expectation for C(θ) is taken with respect to the empirical distribution
rather than the true distribution. After a model is sufficiently trained, the Hessian can be approximated
to the FIM [36, 28], indicating that our sharpness measure is closely related to TIC."
CONNECTIONS TO THE GENERALIZATION,0.49264705882352944,"It has been observed that TIC predicts the generalization gap of neural network models well when the
expectation with respect to p(x, y) is approximated by a finite sum of the integrands over the test data
[45]. These findings are also confirmed by experiments using our measures. In Figure 3, compared
to the other sharpness measures (labeled the same as in Figure 2), we can observe that our measure
correlates better with the generalization gap. The experimental details are provided in Appendix G.2."
CONNECTIONS TO THE GENERALIZATION,0.4963235294117647,0.0275
CONNECTIONS TO THE GENERALIZATION,0.5,0.0350
CONNECTIONS TO THE GENERALIZATION,0.5036764705882353,0.0425 MNIST
CONNECTIONS TO THE GENERALIZATION,0.5073529411764706,"IGS
Tr(H)
Fisher-Rao
Rangamani et al.
Petzka et al."
CONNECTIONS TO THE GENERALIZATION,0.5110294117647058,"0
1
2.5 2.7 2.9 3.1"
CONNECTIONS TO THE GENERALIZATION,0.5147058823529411,CIFAR-10
CONNECTIONS TO THE GENERALIZATION,0.5183823529411765,"0
1
0
1
0
1
0
1"
CONNECTIONS TO THE GENERALIZATION,0.5220588235294118,"R=0.660
R=0.608
R=0.366
R=0.565
R=0.570"
CONNECTIONS TO THE GENERALIZATION,0.5257352941176471,"R=0.610
R=0.544
R=0.233
R=0.538
R=0.555"
CONNECTIONS TO THE GENERALIZATION,0.5294117647058824,"0.0
0.2
0.4
0.6
0.8
1.0"
CONNECTIONS TO THE GENERALIZATION,0.5330882352941176,Normalized sharpness measures 0.0 0.2 0.4 0.6 0.8 1.0
CONNECTIONS TO THE GENERALIZATION,0.5367647058823529,Generalization gap
CONNECTIONS TO THE GENERALIZATION,0.5404411764705882,"Figure 3: Plots for various sharpness measures (normalized to
be in [0,1]) vs. the generalization gap. The top and bottom rows
are obtained using MNIST and CIFAR-10 data sets, respectively.
In each subplot, we provide the correlation coefficient between
the corresponding measure and the generalization gap."
CONNECTIONS TO THE GENERALIZATION,0.5441176470588235,"lBCE(x, z; θ) = log 2"
CONNECTIONS TO THE GENERALIZATION,0.5477941176470589,"l
ϵ
BCE(x, z; θ +
i,k∆θk) = log 2 x"
CONNECTIONS TO THE GENERALIZATION,0.5514705882352942,"ϵ
θ →θ +
i,k∆θk xi"
CONNECTIONS TO THE GENERALIZATION,0.5551470588235294,"Figure 4: Change in the decision
boundary according to the pertur-
bation in parameter values θ →
θ + ϵi,k∆θk."
CONNECTIONS TO THE MARGIN,0.5588235294117647,"3.3.2
Connections to the margin"
CONNECTIONS TO THE MARGIN,0.5625,"Our measure can also be linked to generalization in terms of margin. Unlike the usual definition of
the margin as the distance from the decision boundary to its closest data, we devise a similar concept
developed in the parametric model space rather than the input space."
CONNECTIONS TO THE MARGIN,0.5661764705882353,"Given a model parameter value, if we perturb the value so that the model’s decision boundary passes
through the location of a nearby data as shown in Figure 4, the difference between the initial and the
perturbed parameter values would correspond to a sort of margin defined in the model parameter
space. Since the parameter space dimension is high, we can perturb the model parameter value in
various directions. Considering the perturbations of parameter values along the eigenvectors of the
FIM results in an interesting connection between this margin and our sharpness measure. We now
formalize this relationship for the binary classification case, while that for the multi-class case is
derived in Appendix E."
CONNECTIONS TO THE MARGIN,0.5698529411764706,"We define the binary cross-entropy loss as li(θ) = lBCE(xi, zi; θ) = −zi log p(xi; θ) −(1 −
zi) log(1 −p(xi; θ)), where zi ∈{0, 1} denotes the binary class label for an input xi ∈RD and
p(xi; θ) denotes the model’s prediction on the probability that xi belongs to class one. The condition
for xi to lie on the decision boundary is p(xi) = 1"
CONNECTIONS TO THE MARGIN,0.5735294117647058,"2, which is equal to li(θ) = log 2 regardless of the
zi value."
CONNECTIONS TO THE MARGIN,0.5772058823529411,"Suppose we perturb the neural network parameter value from θ to θ + ϵi,k∆θk, where ∆θk ∈Rm
is the k-th eigenvector of the FIM associated with the k-th eigenvalue λk and ϵi,k is a scalar. For
the decision boundary of the perturbed model to lie on xi, the scalar ϵi,k should satisfy log 2 =
li(θ + ϵi,k∆θk) ≈li(θ) + ϵi,k
  ∂li"
CONNECTIONS TO THE MARGIN,0.5808823529411765,"∂θ

∆θk, where we apply the first-order Taylor expansion by
assuming small ϵi,k. The scalar ϵi,k can then be approximated as ϵi,k ≈log 2−li(θ)
 ∂li"
CONNECTIONS TO THE MARGIN,0.5845588235294118,"∂θ

∆θk ."
CONNECTIONS TO THE MARGIN,0.5882352941176471,"A sort of margin can be derived in an information geometric sense by computing the squared norm of
the perturbation vector ϵi,k∆θk using the inner product based on the FIM as follows:"
CONNECTIONS TO THE MARGIN,0.5919117647058824,"ϵ2
i,k∆θ⊤
k F(θ)∆θk = ϵ2
i,kλk ≈λk"
CONNECTIONS TO THE MARGIN,0.5955882352941176,"log 2 −li(θ)
  ∂li"
CONNECTIONS TO THE MARGIN,0.5992647058823529,"∂θ

∆θk !2"
CONNECTIONS TO THE MARGIN,0.6029411764705882,".
(10)"
CONNECTIONS TO THE MARGIN,0.6066176470588235,"The summation over k = 1, . . . , m′ (with m′ as the largest k with non-zero λk) of the reciprocal of
these squared norms (hence weighing more on small ϵ2
i,k values which would reduce the error from
approximations) can then be related to our sharpness measure as follows: m′
X k=1"
CONNECTIONS TO THE MARGIN,0.6102941176470589,"1
ϵ2
i,kλk
≈ m′
X k=1 1
λk   ∂li"
CONNECTIONS TO THE MARGIN,0.6139705882352942,"∂θ

∆θk
log 2 −li(θ) !2 =   ∂li"
CONNECTIONS TO THE MARGIN,0.6176470588235294,"∂θ

F(θ)†   ∂li ∂θ
⊤"
CONNECTIONS TO THE MARGIN,0.6213235294117647,"(log 2 −li(θ))2
,
(11)"
CONNECTIONS TO THE MARGIN,0.625,where we have used F(θ)† = Pm′
CONNECTIONS TO THE MARGIN,0.6286764705882353,"k=1
1
λk ∆θk∆θ⊤
k in deriving the last identity. Note that our sharpness
measure at each data (xi, yi) appears in the numerator of (11). This indicates that the smaller our
sharpness measure, the larger the squared norms (ϵ2
i,kλk) in the LHS, which means that the margin
becomes larger in an information geometric sense."
CONNECTIONS TO THE MARGIN,0.6323529411764706,"(a) Decision boundary
from SGD"
CONNECTIONS TO THE MARGIN,0.6360294117647058,"(b) Decision boundary
from SGD with regular-
ization"
CONNECTIONS TO THE MARGIN,0.6397058823529411,"0
4k
8k
12k
16k
20k
Iterations 0 1 2 3 Loss"
CONNECTIONS TO THE MARGIN,0.6433823529411765,"Train
Test
Poison"
CONNECTIONS TO THE MARGIN,0.6470588235294118,"(c) Loss values from
SGD"
CONNECTIONS TO THE MARGIN,0.6507352941176471,"0
4k
8k
12k
16k
20k
Iterations 0 1 2 3 Loss"
CONNECTIONS TO THE MARGIN,0.6544117647058824,"Train
Test
Poison"
CONNECTIONS TO THE MARGIN,0.6580882352941176,"(d) Loss values from
SGD with regularization"
CONNECTIONS TO THE MARGIN,0.6617647058823529,"Figure 5: A synthetic three-class classification example trained using SGD with and without regular-
ization under the effect of poison data. The loss values are smoothed for better visualization."
USING THE SHARPNESS MEASURE AS A REGULARIZER TO TRAIN NEURAL NETWORKS,0.6654411764705882,"4
Using the sharpness measure as a regularizer to train neural networks"
USING THE SHARPNESS MEASURE AS A REGULARIZER TO TRAIN NEURAL NETWORKS,0.6691176470588235,"We now apply our sharpness measure as a regularizer to train neural networks to see if regularizing
the measure can improve the generalization performance. The corresponding loss function is written
as"
USING THE SHARPNESS MEASURE AS A REGULARIZER TO TRAIN NEURAL NETWORKS,0.6727941176470589,"L(θ) = 1 N N
X"
USING THE SHARPNESS MEASURE AS A REGULARIZER TO TRAIN NEURAL NETWORKS,0.6764705882352942,"i=1
li(θ) + ρ · IGS(θ),
(12)"
USING THE SHARPNESS MEASURE AS A REGULARIZER TO TRAIN NEURAL NETWORKS,0.6801470588235294,"where ρ is a coefficient for the regularization term. We consider a toy example and then consider
image classification tasks involving MNIST and CIFAR-10/100 data sets. We also provide some
tractable and possibly efficient ways to regularize our sharpness measure. All the experiments have
been performed using the PyTorch library [38]."
A TOY EXAMPLE,0.6838235294117647,"4.1
A toy example"
A TOY EXAMPLE,0.6875,"We first apply our regularizer to a toy example. In [18], they have trained neural networks on
poison data with the wrong labels to hamper the generalization performance and make the loss
landscape extremely sharp while fitting all the training data. We follow this experimental setting and
check whether the obtained solution can avoid the undesirable generalization performance and loss
landscapes when applying our regularizer (calculated on the training data). The experimental details,
as well as how the IGS is approximated for this example, are provided in Appendix G.3."
A TOY EXAMPLE,0.6911764705882353,"The experimental results are shown in Figure 5. The results from SGD without regularization show
very irregular decision boundaries with tiny margins, whereas relatively soft boundaries with larger
margins appear when our regularizer is used, especially around the training data. Applying our
regularizer also shows a lower generalization gap, i.e., the difference between the training and test
losses, than the SGD without regularization."
A TOY EXAMPLE,0.6948529411764706,"4.2
MNIST and CIFAR-10/100"
A TOY EXAMPLE,0.6985294117647058,"To check the effect of the regularizer in more realistic settings, we apply our regularizer to the classifi-
cation of MNIST and CIFAR-10/100 data sets. For this experiment, we regularize the mini-batch IGS
defined in (7). For an efficient implementation of our regularizer, the following approximation on
perturbed loss functions with a perturbation δ = ρF(θ)†   ∂l"
A TOY EXAMPLE,0.7022058823529411,"∂θ
⊤∈Rm (with a small ρ > 0) is useful:"
A TOY EXAMPLE,0.7058823529411765,"l(θ + δ) ≈l(θ) +
 ∂l ∂θ"
A TOY EXAMPLE,0.7095588235294118,"
δ ≈l(θ) + ρ ·
 ∂l ∂θ"
A TOY EXAMPLE,0.7132352941176471,"
F(θ)†
 ∂l ∂θ"
A TOY EXAMPLE,0.7169117647058824,"⊤
.
(13)"
A TOY EXAMPLE,0.7205882352941176,"Note that for l =
1
b
P
xi∈B li with a mini-batch B of size |B| = b, minimizing (13) becomes
minimizing a stochastic version of (12) (with IGSb(θ)), where the stochasticity comes from sampling
a mini-batch B. Therefore we minimize (13) in our experiments with ρ as a tunable hyperparameter.
To obtain δ, we resort to an EKFAC-based approximation for the natural gradients [13] as detailed in
Appendix G.4.1."
A TOY EXAMPLE,0.7242647058823529,"A three-layer fully connected neural network (3FCN) is used for the experiments using MNIST data,
and various convolutional neural networks such as VGG [42], ResNet [15], and WideResNet [50]
are used for CIFAR-10/100 experiments, with data augmentation, cosine annealing [27], and label"
A TOY EXAMPLE,0.7279411764705882,"Table 1: Averages and standard errors of the test classification accuracies for SGD, GR, SAM, ASAM,
and SGD with our regularization method on MNIST, CIFAR-10, and CIFAR-100 data sets."
A TOY EXAMPLE,0.7316176470588235,"DATA SET
MODEL
SGD
GR
SAM
ASAM
OURS"
A TOY EXAMPLE,0.7352941176470589,"MNIST
3FCN
98.13± 0.05
98.20 ± 0.01
98.62 ± 0.01
98.65 ± 0.07
98.65 ± 0.03"
A TOY EXAMPLE,0.7389705882352942,"CIFAR-10
VGG11-BN
92.75 ± 0.11
93.13 ± 0.14
93.61 ± 0.08
93.78 ± 0.13
93.81 ± 0.06
RESNET-56
94.08 ± 0.17
94.40 ± 0.23
95.22 ± 0.11
95.36 ± 0.06
94.96 ± 0.01
WRN-16-8
95.98 ± 0.03
96.15 ± 0.10
96.70 ± 0.14
97.03 ± 0.06
96.57 ± 0.10"
A TOY EXAMPLE,0.7426470588235294,"CIFAR-100
VGG11-BN
72.25 ± 0.17
73.05 ± 0.32
73.69 ± 0.20
73.61 ± 0.18
74.24 ± 0.22
RESNET-56
73.14 ± 0.10
74.67 ± 0.25
75.90 ± 0.25
75.87 ± 0.27
76.07 ± 0.11
WRN-16-8
80.55 ± 0.33
81.13 ± 0.06
82.20 ± 0.11
82.38 ± 0.09
82.44 ± 0.29"
A TOY EXAMPLE,0.7463235294117647,"smoothing [30] methods additionally applied. For comparison, we consider the gradient regularization
(GR) method [4, 43], sharpness-aware minimization (SAM) method [12], and the adaptive SAM
(ASAM) method [24], an extension of SAM to involve the scale-invariance. The detailed experimental
settings are explained in Appendix G.4.2."
A TOY EXAMPLE,0.75,"We provide the averages and standard errors of the test classification accuracies obtained from three
runs of each method in Table 1. As can be seen from the table, one can confirm that the generalization
performance of SGD is significantly improved with our regularizer. Furthermore, our method shows
better performance than the GR method and a comparable performance improvement with the SAM
and ASAM methods."
CONCLUSION,0.7536764705882353,"5
Conclusion"
CONCLUSION,0.7573529411764706,"Various empirical results observed from running deep learning algorithms such as SGD have suggested
that minima with a flatter loss landscape tend to show better generalization performance. However,
the previous definitions of sharpness/flatness have faced a severe dependence on reparametrizations
or parameter scalings that do not change the model output and the generalization performance [8]."
CONCLUSION,0.7610294117647058,"In this paper, based on an information geometric analysis of the neural network parameter space, we
have proposed an information geometric sharpness measure which is reparametrization- and scale-
invariant, making it free from the issues posed by [8]. We have also discussed the connection of the
measure to generalization. In addition, a regularizer that reduces the suggested sharpness measure is
proposed, showing a significant improvement in generalization performance under practical settings."
CONCLUSION,0.7647058823529411,"Unlike many other sharpness measures, the Fisher information matrix (FIM) plays a crucial role in
our measure. Since it can be computationally demanding to calculate the FIM and our sharpness
measure for large-scale neural networks, more efficient evaluation methods should be developed to
apply our measure to such networks. Concerning the regularization of our measure during training,
exploring better ways to reduce the time complexity and increase generalization performance is left
for future work. Furthermore, analyzing the generalization properties of models obtained from deep
learning algorithms such as SGD or natural gradient descent [2, 28] using our measure would be
another intriguing future work."
CONCLUSION,0.7683823529411765,Acknowledgments and Disclosure of Funding
CONCLUSION,0.7720588235294118,"C. Jang and Y.-K. Noh were supported by IITP Artificial Intelligence Graduate School Program
for Hanyang University funded by MSIT (Grant No. 2020-0-01373). S. Lee was supported by a
KIAS Individual Grant (AP083601) via the Center for AI and Natural Sciences at Korea Institute
for Advanced Study. F. C. Park was supported in part by SRRC NRF grant 2016R1A5A1938472,
IITP-MSIT grant 2022-0-00480 (Training and Inference Methods for Goal-Oriented AI Agents), SNU-
AIIS, SNU-IAMD, and the SNU Institute for Engineering Research. Y.-K. Noh was partly supported
by NRF/MSIT (Grant No. 2018R1A5A7059549, 2021M3E5D2A01019545) and IITP/MSIT (Grant
No. IITP-2021-0-02068)."
REFERENCES,0.7757352941176471,References
REFERENCES,0.7794117647058824,"[1] Akaike, H.: A new look at the statistical model identification. IEEE transactions on automatic control
19(6), 716–723 (1974)"
REFERENCES,0.7830882352941176,"[2] Amari, S.I.: Natural gradient works efficiently in learning. Neural computation 10(2), 251–276 (1998)"
REFERENCES,0.7867647058823529,"[3] Amari, S.i.: Information geometry and its applications, vol. 194. Springer (2016)"
REFERENCES,0.7904411764705882,"[4] Barrett, D.G., Dherin, B.: Implicit gradient regularization. arXiv preprint arXiv:2009.11162 (2020)"
REFERENCES,0.7941176470588235,"[5] Bartlett, P.L., Foster, D.J., Telgarsky, M.J.: Spectrally-normalized margin bounds for neural networks.
Advances in Neural Information Processing Systems 30, 6240–6249 (2017)"
REFERENCES,0.7977941176470589,"[6] Boothby, W.M.: An introduction to differentiable manifolds and Riemannian geometry, Revised, vol. 120.
Gulf Professional Publishing (2003)"
REFERENCES,0.8014705882352942,"[7] Chaudhari, P., Choromanska, A., Soatto, S., LeCun, Y., Baldassi, C., Borgs, C., Chayes, J., Sagun, L.,
Zecchina, R.: Entropy-sgd: Biasing gradient descent into wide valleys. Journal of Statistical Mechanics:
Theory and Experiment 2019(12), 124018 (2019)"
REFERENCES,0.8051470588235294,"[8] Dinh, L., Pascanu, R., Bengio, S., Bengio, Y.: Sharp minima can generalize for deep nets. In: International
Conference on Machine Learning, pp. 1019–1028. PMLR (2017)"
REFERENCES,0.8088235294117647,"[9] Dixon, M., Ward, T.: Takeuchi’s information criteria as a form of regularization.
arXiv preprint
arXiv:1803.04947 (2018)"
REFERENCES,0.8125,"[10] Dziugaite, G.K., Roy, D.M.: Computing nonvacuous generalization bounds for deep (stochastic) neural
networks with many more parameters than training data. arXiv preprint arXiv:1703.11008 (2017)"
REFERENCES,0.8161764705882353,"[11] Fomenko, A., Novikov, S., Dubrovin, B.: Modern Geometry-Methods and Applications, Part I: The
Geometry of Surfaces, Transformation Groups, and Fields. Springer, Berlin (1992)"
REFERENCES,0.8198529411764706,"[12] Foret, P., Kleiner, A., Mobahi, H., Neyshabur, B.: Sharpness-aware minimization for efficiently improving
generalization. arXiv preprint arXiv:2010.01412 (2020)"
REFERENCES,0.8235294117647058,"[13] George, T., Laurent, C., Bouthillier, X., Ballas, N., Vincent, P.: Fast approximate natural gradient descent
in a kronecker factored eigenbasis. Advances in Neural Information Processing Systems 31 (2018)"
REFERENCES,0.8272058823529411,"[14] Ghorbani, B., Krishnan, S., Xiao, Y.: An investigation into neural net optimization via hessian eigenvalue
density. In: International Conference on Machine Learning, pp. 2232–2241. PMLR (2019)"
REFERENCES,0.8308823529411765,"[15] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the
IEEE conference on computer vision and pattern recognition, pp. 770–778 (2016)"
REFERENCES,0.8345588235294118,"[16] Hinton, G.E., Van Camp, D.: Keeping the neural networks simple by minimizing the description length of
the weights. In: Proceedings of the sixth annual conference on Computational learning theory, pp. 5–13
(1993)"
REFERENCES,0.8382352941176471,"[17] Hochreiter, S., Schmidhuber, J.: Flat minima. Neural computation 9(1), 1–42 (1997)"
REFERENCES,0.8419117647058824,"[18] Huang, W.R., Emam, Z., Goldblum, M., Fowl, L., Terry, J.K., Huang, F., Goldstein, T.: Understanding
generalization through visualizations (2020)"
REFERENCES,0.8455882352941176,"[19] Izmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D., Wilson, A.G.: Averaging weights leads to wider
optima and better generalization. In: 34th Conference on Uncertainty in Artificial Intelligence 2018, UAI
2018, pp. 876–885. Association For Uncertainty in Artificial Intelligence (AUAI) (2018)"
REFERENCES,0.8492647058823529,"[20] Jastrzebski, S., Arpit, D., Astrand, O., Kerg, G.B., Wang, H., Xiong, C., Socher, R., Cho, K., Geras,
K.J.: Catastrophic fisher explosion: Early phase fisher matrix impacts generalization. In: International
Conference on Machine Learning, pp. 4772–4784. PMLR (2021)"
REFERENCES,0.8529411764705882,"[21] Jiang, Y., Neyshabur, B., Mobahi, H., Krishnan, D., Bengio, S.: Fantastic generalization measures and
where to find them. In: International Conference on Learning Representations (2019)"
REFERENCES,0.8566176470588235,"[22] Keskar, N.S., Nocedal, J., Tang, P.T.P., Mudigere, D., Smelyanskiy, M.: On large-batch training for
deep learning: Generalization gap and sharp minima. In: 5th International Conference on Learning
Representations, ICLR 2017 (2017)"
REFERENCES,0.8602941176470589,"[23] Kunstner, F., Hennig, P., Balles, L.: Limitations of the empirical fisher approximation for natural gradient
descent. Advances in neural information processing systems 32 (2019)"
REFERENCES,0.8639705882352942,"[24] Kwon, J., Kim, J., Park, H., Choi, I.K.: Asam: Adaptive sharpness-aware minimization for scale-invariant
learning of deep neural networks. arXiv preprint arXiv:2102.11600 (2021)"
REFERENCES,0.8676470588235294,"[25] LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document recognition.
Proceedings of the IEEE 86(11), 2278–2324 (1998)"
REFERENCES,0.8713235294117647,"[26] Liang, T., Poggio, T., Rakhlin, A., Stokes, J.: Fisher-rao metric, geometry, and complexity of neural
networks. In: The 22nd International Conference on Artificial Intelligence and Statistics, pp. 888–896.
PMLR (2019)"
REFERENCES,0.875,"[27] Loshchilov, I., Hutter, F.: Sgdr: Stochastic gradient descent with warm restarts.
arXiv preprint
arXiv:1608.03983 (2016)"
REFERENCES,0.8786764705882353,"[28] Martens, J.: New insights and perspectives on the natural gradient method. Journal of Machine Learning
Research 21, 1–76 (2020)"
REFERENCES,0.8823529411764706,"[29] Martens, J., Grosse, R.: Optimizing neural networks with kronecker-factored approximate curvature. arXiv
preprint arXiv:1503.05671 (2015)"
REFERENCES,0.8860294117647058,"[30] Müller, R., Kornblith, S., Hinton, G.: When does label smoothing help? arXiv preprint arXiv:1906.02629
(2019)"
REFERENCES,0.8897058823529411,"[31] Nair, V., Hinton, G.E.: Rectified linear units improve restricted boltzmann machines. In: Icml (2010)"
REFERENCES,0.8933823529411765,"[32] Neyshabur, B., Bhojanapalli, S., Mcallester, D., Srebro, N.: Exploring generalization in deep learning.
Advances in Neural Information Processing Systems 30, 5947–5956 (2017)"
REFERENCES,0.8970588235294118,"[33] Neyshabur, B., Salakhutdinov, R., Srebro, N.: Path-sgd: path-normalized optimization in deep neural
networks. In: Proceedings of the 28th International Conference on Neural Information Processing Systems-
Volume 2, pp. 2422–2430 (2015)"
REFERENCES,0.9007352941176471,"[34] Neyshabur, B., Tomioka, R., Srebro, N.: Norm-based capacity control in neural networks. In: Conference
on Learning Theory, pp. 1376–1401. PMLR (2015)"
REFERENCES,0.9044117647058824,"[35] Nielsen, F.: An elementary introduction to information geometry. Entropy 22(10), 1100 (2020)"
REFERENCES,0.9080882352941176,"[36] Papyan, V.: The full spectrum of deepnet hessians at scale: Dynamics with sgd training and sample size.
arXiv preprint arXiv:1811.07062 (2018)"
REFERENCES,0.9117647058823529,"[37] Papyan, V.: Measurements of three-level hierarchical structure in the outliers in the spectrum of deepnet
hessians. In: International Conference on Machine Learning, pp. 5012–5021. PMLR (2019)"
REFERENCES,0.9154411764705882,"[38] Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L.,
Lerer, A.: Automatic differentiation in pytorch (2017)"
REFERENCES,0.9191176470588235,"[39] Petzka, H., Kamp, M., Adilova, L., Sminchisescu, C., Boley, M.: Relative flatness and generalization.
Advances in Neural Information Processing Systems 34 (2021)"
REFERENCES,0.9227941176470589,"[40] Rangamani, A., Nguyen, N.H., Kumar, A., Phan, D., Chin, S.P., Tran, T.D.: A scale invariant measure of
flatness for deep network minima. In: ICASSP 2021-2021 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), pp. 1680–1684. IEEE (2021)"
REFERENCES,0.9264705882352942,"[41] Sagun, L., Evci, U., Guney, V.U., Dauphin, Y., Bottou, L.: Empirical analysis of the hessian of over-
parametrized neural networks. arXiv preprint arXiv:1706.04454 (2017)"
REFERENCES,0.9301470588235294,"[42] Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556 (2014)"
REFERENCES,0.9338235294117647,"[43] Smith, S.L., Dherin, B., Barrett, D.G., De, S.: On the origin of implicit regularization in stochastic gradient
descent. arXiv preprint arXiv:2101.12176 (2021)"
REFERENCES,0.9375,"[44] Takeuchi, K.: The distribution of information statistics and the criterion of goodness of fit of models.
Mathematical Science 153, 12–18 (1976)"
REFERENCES,0.9411764705882353,"[45] Thomas, V., Pedregosa, F., Merriënboer, B., Manzagol, P.A., Bengio, Y., Le Roux, N.: On the interplay
between noise and curvature and its effect on optimization and generalization. In: International Conference
on Artificial Intelligence and Statistics, pp. 3503–3513. PMLR (2020)"
REFERENCES,0.9448529411764706,"[46] Tsuzuku, Y., Sato, I., Sugiyama, M.: Normalized flat minima: Exploring scale invariant definition of
flat minima for neural networks using pac-bayesian analysis. In: International Conference on Machine
Learning, pp. 9636–9647. PMLR (2020)"
REFERENCES,0.9485294117647058,"[47] Wu, J., Hu, W., Xiong, H., Huan, J., Braverman, V., Zhu, Z.: On the noisy gradient descent that generalizes
as sgd. In: International Conference on Machine Learning, pp. 10367–10376. PMLR (2020)"
REFERENCES,0.9522058823529411,"[48] Yao, Z., Gholami, A., Keutzer, K., Mahoney, M.W.: Pyhessian: Neural networks through the lens of the
hessian. In: 2020 IEEE International Conference on Big Data (Big Data), pp. 581–590. IEEE (2020)"
REFERENCES,0.9558823529411765,"[49] Yao, Z., Gholami, A., Lei, Q., Keutzer, K., Mahoney, M.W.: Hessian-based analysis of large batch training
and robustness to adversaries. Advances in Neural Information Processing Systems 31 (2018)"
REFERENCES,0.9595588235294118,"[50] Zagoruyko, S., Komodakis, N.: Wide residual networks. arXiv preprint arXiv:1605.07146 (2016)"
REFERENCES,0.9632352941176471,Checklist
REFERENCES,0.9669117647058824,1. For all authors...
REFERENCES,0.9705882352941176,"(a) Do the main claims made in the abstract and introduction accurately reflect the paper’s
contributions and scope? [Yes] The need to consider information geometry in mea-
suring sharpness is discussed in Section 2; the definition of our sharpness measure,
its reparametrization-invariance, and its connection to generalization are explained in
Section 3; and the experiments to use our measure as a regularizer in neural network
training are performed in Section 4.
(b) Did you describe the limitations of your work? [Yes] See Section 5."
REFERENCES,0.9742647058823529,"(c) Did you discuss any potential negative societal impacts of your work? [N/A]
(d) Have you read the ethics review guidelines and ensured that your paper conforms to
them? [Yes] We have read the ethics review guidelines and checked that our paper
conforms to them.
2. If you are including theoretical results..."
REFERENCES,0.9779411764705882,"(a) Did you state the full set of assumptions of all theoretical results? [Yes] See Section 3.1.
(b) Did you include complete proofs of all theoretical results? [Yes] See Appendix D.1.
3. If you ran experiments..."
REFERENCES,0.9816176470588235,"(a) Did you include the code, data, and instructions needed to reproduce the main experi-
mental results (either in the supplemental material or as a URL)? [Yes] See supplemen-
tal materials.
(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
were chosen)? [Yes] See Appendix G.
(c) Did you report error bars (e.g., with respect to the random seed after running experi-
ments multiple times)? [Yes] See Tables 1 and 2.
(d) Did you include the total amount of compute and the type of resources used (e.g., type
of GPUs, internal cluster, or cloud provider)? [Yes] See Appendix G.4.2.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets..."
REFERENCES,0.9852941176470589,"(a) If your work uses existing assets, did you cite the creators? [Yes] See Appendix G.
(b) Did you mention the license of the assets? [Yes] See Appendix G."
REFERENCES,0.9889705882352942,(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]
REFERENCES,0.9926470588235294,"See supplemental materials.
(d) Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? [N/A]
(e) Did you discuss whether the data you are using/curating contains personally identifiable
information or offensive content? [N/A]
5. If you used crowdsourcing or conducted research with human subjects..."
REFERENCES,0.9963235294117647,"(a) Did you include the full text of instructions given to participants and screenshots, if
applicable? [N/A]
(b) Did you describe any potential participant risks, with links to Institutional Review
Board (IRB) approvals, if applicable? [N/A]
(c) Did you include the estimated hourly wage paid to participants and the total amount
spent on participant compensation? [N/A]"
