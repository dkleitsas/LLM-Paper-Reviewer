Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001658374792703151,"Learning Rate Warmup is a popular heuristic for training neural networks, espe-
cially at larger batch sizes, despite limited understanding of its benefits. Warmup
decreases the update size ∆wt = ηtut early in training by using lower values for
the learning rate ηt. In this work we argue that warmup benefits training by keeping
the overall size of ∆wt limited, counteracting large initial values of ut. Focusing
on small-scale GPT training with AdamW/Lion, we explore the following question:
Why and by which criteria are early updates ut too large? We analyze different
metrics for the update size including the ℓ2-norm, resulting directional change,
and impact on the representations of the network, providing a new perspective on
warmup. In particular, we find that warmup helps counteract large angular updates
as well as a limited critical batch size early in training. Finally, we show that
the need for warmup can be significantly reduced or eliminated by modifying the
optimizer to explicitly normalize ut based on the aforementioned metrics."
INTRODUCTION,0.003316749585406302,"1
Introduction"
INTRODUCTION,0.004975124378109453,"Neural networks are typically trained using variations of stochastic gradient descent. The weight
updates ∆w have the form ∆w = ηu, where η denotes the learning rate and u an unscaled update
vector derived from the history of weight gradients. Throughout training, the learning rate is often
adjusted over time t according to a learning rate schedule, η = ηt. This schedule frequently includes
an initial phase known as a learning rate warmup, where the learning rate starts low and is increased
to a target value before being reduced according to a decay schedule. Both the choice of warmup and
decay strategy can significantly affect the final model performance. In this work, we focus on the
linear warmup introduced by Goyal et al. [10] for large batch size ResNet [11] training, which is also
commonly used for transformers [37]."
INTRODUCTION,0.006633499170812604,"The length of the warmup is a hyperparameter that requires tuning, which is complicated by the
fact that the reasons for its effectiveness are somewhat unclear. Warmup empirically helps stabilize
training and allows the use of larger learning rates throughout the rest of training, which can speed up
the process and provide beneficial regularization [10]. Since the learning rate simply scales the size
of the updates ∆w = ηu, warmup must achieve these effects by decreasing the size of early updates.
However, it is not fully clear why this helps. Are the initial updates too large for some reason? For
example, we might need small ηt values to counteract large u values early in training. How should
we quantify what makes an update ∆w large? Why do large updates adversely affect training?"
INTRODUCTION,0.008291873963515755,"In this work, we explore warmup from this perspective, focusing on GPT2 [29] training with adaptive
optimizers like AdamW [24] and Lion [3]. We identify three key issues that necessitate warmup:"
INTRODUCTION,0.009950248756218905,"1. The way Adam handles momentum can lead to artificially large initial updates ∆w.
2. Early updates ∆w are large compared to the initial weight magnitude of w for matrices.
3. The gradients of early samples are highly correlated, limiting effective mini-batch sizes."
INTRODUCTION,0.011608623548922056,"We demonstrate that simple modifications to the optimizer can mitigate the first two issues: eliminating
the momentum bias correction in AdamW and scaling matrix updates to match their magnitude, akin
to the Rotational Optimizers by Kosson et al. [19]. We analyze the third issue in terms of the rate
at which the internal neural representations of the network are changing (sometimes called feature
learning). When the gradients of different samples are highly correlated these internal representations
change too fast, which we conjecture can lead to issues with the non-linearities (e.g. activation
functions) of the network. This can also be seen as the critical batch size [28] being too low early
in training to enable the use of the peak learning rate. We derive a scaling factor based on the
signal-to-noise ratio of the gradient that can help mitigate this, functioning like an automatic learning
rate warmup. Alternatively, we show that using high momentum values in conjunction to the first two
methods may suffice to enable efficient training without warmup."
RELATED WORK,0.013266998341625208,"2
Related Work"
RELATED WORK,0.014925373134328358,"Learning rate warmups have been used since at least ResNet [11], where a lower constant learning
rate was applied at the start of training. Earlier works may have employed similar concepts; for
example, Sutskever et al. [35] utilized a momentum schedule that could induce a similar effect in the
“effective learning rate” as defined by Fu et al. [6]. The practice of linear warmup in its current form
was popularized by Goyal et al. [10] and Vaswani et al. [37]."
RELATED WORK,0.01658374792703151,"Warmup has been studied indirectly in various neural network optimizer works. A notable example is
RAdam [23], a modification of Adam [18] aimed at reducing the need for warmup. However, Ma
and Yarats [26] demonstrated that RAdam essentially incorporates a fixed warmup schedule within
the optimizer. In appx. C.1 we show that this warmup effect is insufficient in our setup. Relative
optimizers like LARS [47] and LAMB [48] are also believed to reduce the necessity for warmup [21].
Bernstein et al. [2] propose a relative optimizer called Fromage and analyze how relative weight
changes relate to representation changes, but differ from our approach in that they do not describe
the effects of the gradient signal-to-noise ratio on this relationship. We build upon Kosson et al.
[19] which showed that weight decay can make standard optimizers function as approximate relative
optimizers and proposed variants that reduce the benefit of warmup without fully eliminating it."
RELATED WORK,0.01824212271973466,"The effect of warmup in transformers was empirically studied by Wortsman et al. [41]. Xiong et al.
[43] proposed the pre-LN normalization placement for transformers, showing it reduces the need for
warmup. Huang et al. [13] studied initialization in transformers showing a link to warmup."
RELATED WORK,0.01990049751243781,"Finally, warmup has been studied directly on its own. Gotmare et al. [9] studied the effect of warmup,
finding it helps avoid overly large updates to the weights of later layers which could be frozen to
achieve a similar benefit. Gilmer et al. [7] study the need for warmup from a curvature perspective,
showing it may help “push” the optimization trajectory towards flatter regions where higher learning
rates are stable. Smith et al. [32] arrive at a similar conclusion, there is a stable learning rate that
varies throughout training based on the curvature which limits the learning rate early on, necessitating
warmup. These works focus on SGD with momentum, but it is less clear how curvature affects
Adam-like or relative optimizers as we discuss later."
RELATED WORK,0.02155887230514096,"The relation between stochastic gradient noise and learning rate has been studied in several works
[46, 28, 49, 34, 22, 27]. They find that the update size can be increased roughly linearly with the batch
size up to a certain critical batch size that depends on ratio of the mean and variance of the mini-batch
gradient. We show how the signal-to-noise ratio (SNR) of the mini-batch gradient amplifies changes
to the neural representations of a network given a normalized update in weight space. We observe
that the SNR starts out high but decreases over time, which translates to large early changes in the
internal representations without warmup."
BASELINE EXPERIMENTAL SETUP & RESULTS,0.02321724709784411,"3
Baseline Experimental Setup & Results"
BASELINE EXPERIMENTAL SETUP & RESULTS,0.024875621890547265,"Our main experiments focus on the training of a 124M parameter GPT2 [29] model on the Open-
WebText corpus [8]. The model has 12 transformer blocks with an embedding dimension of 768.
Our base training is performed at batch size 480 with a sequence length of 1024. We train for 5000
iterations which translates into roughly 20 tokens per parameter, as suggested by Chinchila [12].
The baselines use AdamW [24] (see algo. 1) with weight decay λ = 0.1, momentum coefficient
β1 = 0.9, smoothing coefficient β2 = 0.95, and ε = 10−8. The learning rate schedule consists of a"
BASELINE EXPERIMENTAL SETUP & RESULTS,0.026533996683250415,"0
2500
5000
Iteration 0 1"
BASELINE EXPERIMENTAL SETUP & RESULTS,0.028192371475953566,Learning Rate Schedule
BASELINE EXPERIMENTAL SETUP & RESULTS,0.029850746268656716,"0%
2%
5%
10%
20%"
BASELINE EXPERIMENTAL SETUP & RESULTS,0.03150912106135987,"10
3
10
2"
BASELINE EXPERIMENTAL SETUP & RESULTS,0.03316749585406302,Learning Rate 3.1 3.2 3.3 3.4 3.5
BASELINE EXPERIMENTAL SETUP & RESULTS,0.03482587064676617,Validation Loss
BASELINE EXPERIMENTAL SETUP & RESULTS,0.03648424543946932,"0
0.05
0.50
1
Iteration (  scale) 4 6 8 10"
BASELINE EXPERIMENTAL SETUP & RESULTS,0.03814262023217247,Training Loss
BASELINE EXPERIMENTAL SETUP & RESULTS,0.03980099502487562,"0
0.05
0.50
1
Iteration (  scale) 0 1"
BASELINE EXPERIMENTAL SETUP & RESULTS,0.04145936981757877,2-update size / max
BASELINE EXPERIMENTAL SETUP & RESULTS,0.04311774461028192,"Figure 1: Warmup significantly benefits GPT2 training with AdamW. Panel 1: Trapezoidal learning
rate schedules with different warmup lengths and 50% linear cooldown. Panel 2: Final validation loss
for various learning rate and warmup configurations. Note the performance gap between no-warmup
(black) and other configurations. Panel 3: Training curves comparing the best no-warmup run to
a 5% warmup with the same learning rate. The warmup run quickly surpasses the no-warmup run.
Panel 4: Comparison of ℓ2 update norms for these runs shows large initial updates without warmup."
BASELINE EXPERIMENTAL SETUP & RESULTS,0.04477611940298507,"linear warmup followed by a constant phase and eventually linear cooldown spanning half of training
(see examples in fig. 1). This schedule keeps the peak learning rate and decay phase identical for
different warmup lengths. This differs from other schedules, e.g. cosine, where the warmup length
typically affects the whole shape. The learning rate value and the warmup length are swept for
various configurations. Our code is based on NanoGPT [16] with additional utilities by Kosson et al.
[19]. The hyperparameter values and base training configuration are adopted from NanoGPT. See
appx. C.2 for experiments on additional architectures and datasets."
BASELINE EXPERIMENTAL SETUP & RESULTS,0.04643449419568822,"Figure 1 shows the baseline performance for our setup. We observe that even short warmup can
significantly improve performance. Not using warmup results in faster initial progress for a given
learning rate, but eventually falls behind leaving a permanent gap. Warmup not only stabilizes higher
learning rates, but also prevents a lasting degradation of the model that can not be mitigated by
simply training for slightly longer. We notice that although Adam normalizes the update size, its
ℓ2-magnitude varies significantly throughout training with a large spike at the start of training."
BASELINE EXPERIMENTAL SETUP & RESULTS,0.04809286898839138,"4
The Interaction of Momentum and the ℓ2-Update Norm in AdamW"
BASELINE EXPERIMENTAL SETUP & RESULTS,0.04975124378109453,"In this section we analyze the reason behind the large ℓ2-norm of early updates in our AdamW
baseline seen in panel 4 of fig. 1. We find that this primarily stems from the β1 bias correction. We
then explore to what extent these large initial updates contribute to the need for warmup by modifying
the optimizer to directly control the ℓ2-norm of the update. Although we find this is to be insufficient
to replace warmup on its own, these changes are an important component of our later methods."
BASELINE EXPERIMENTAL SETUP & RESULTS,0.05140961857379768,"Adam-like optimizers such as AdamW (algo. 1) differ from simpler methods like SGD with momen-
tum in that they normalize the update size with the gradient magnitude. This makes them invariant to
a rescaling of the loss function and helps counteract potential differences in the gradient magnitude
between layers. An important consequence of this is that the unscaled updates u are not large simply
due to large initial gradients, unlike in plain SGD and other optimizers that don’t normalize their"
BASELINE EXPERIMENTAL SETUP & RESULTS,0.05306799336650083,"Algorithm 1 AdamW (PyTorch variant, differing from the original by Loshchilov and Hutter [24])"
BASELINE EXPERIMENTAL SETUP & RESULTS,0.05472636815920398,"Require: Learning rate ηt, weight decay λ, momentum β1, magnitude smoothing β2, ε for numerical stability"
BASELINE EXPERIMENTAL SETUP & RESULTS,0.05638474295190713,"1: Initialize: Time step t ←0, parameter vector θ0, momentum vector m0 ←0, magnitude vector v0 ←0
2: while stopping criteria not met :
3:
t ←t + 1
4:
gt ←Mini-batch gradient w.r.t. θt−1
5:
mt ←β1mt−1 + (1 −β1)gt
6:
vt ←β2vt−1 + (1 −β2)g2
t
7:
ˆmt ←mt/(1 −βt
1)
8:
ˆvt ←vt/(1 −βt
2)
9:
θt ←(1 −ηtλ)θt−1 −ηt ˆmt/(√ˆvt + ε)"
BASELINE EXPERIMENTAL SETUP & RESULTS,0.05804311774461028,"10
3
10
2"
BASELINE EXPERIMENTAL SETUP & RESULTS,0.05970149253731343,Learning Rate 3.1 3.2 3.3 3.4 3.5
BASELINE EXPERIMENTAL SETUP & RESULTS,0.06135986733001658,Validation Loss
BASELINE EXPERIMENTAL SETUP & RESULTS,0.06301824212271974,"0%
2%
5%
10%
20%"
BASELINE EXPERIMENTAL SETUP & RESULTS,0.06467661691542288,"0
0.05
0.50
1
Iteration (  scale) 4 6 8 10"
BASELINE EXPERIMENTAL SETUP & RESULTS,0.06633499170812604,Training Loss
BASELINE EXPERIMENTAL SETUP & RESULTS,0.06799336650082918,"0
0.05
0.50
1
Iteration (  scale) 0 1"
BASELINE EXPERIMENTAL SETUP & RESULTS,0.06965174129353234,2-update size / max
BASELINE EXPERIMENTAL SETUP & RESULTS,0.07131011608623548,"0
0.05
0.50
1
Iteration (  scale) 0 1"
BASELINE EXPERIMENTAL SETUP & RESULTS,0.07296849087893864,"Avg 
-update size / max"
BASELINE EXPERIMENTAL SETUP & RESULTS,0.07462686567164178,"Figure 2: LionA (algo. 2) fails to significantly reduce the warmup advantage. Panel 1: Final
validation loss across various learning rates and warmup percentages shows a reduced but still
significant no-warmup penalty compared to AdamW (fig. 1). Panel 2: Training curves for 0% vs. 5%
warmup at the highest stable learning rate for 0%, with warmup quickly overtaking no-warmup as
before. Panel 3: LionA successfully controls the ℓ2-update norm. Panel 4: Early angular updates
(see §5) are large without warmup and do not follow the learning rate schedule throughout training."
BASELINE EXPERIMENTAL SETUP & RESULTS,0.07628524046434494,"updates. Such un-normalized optimizers might diverge to infinity if a high learning rate is combined
with large initial gradients or large curvature, as the update size is unbounded. Preventing this could
be an additional benefit of warmup for SGD on top of the effects discussed in this work."
BASELINE EXPERIMENTAL SETUP & RESULTS,0.0779436152570481,"Although AdamW normalizes the update size based on the gradient, its magnitude can still vary
throughout training as seen in fig. 1. This can be caused by changes in the gradient magnitude
over time, especially when using different values of β1 and β2. However, it can also be caused by
momentum and especially the bias correction (algo. 1, line 7). The magnitude of mt depends on
the alignment of subsequent gradients g1, . . . , gt whereas the normalization factor vt does not. For
example, when each gt is an independent zero-mean random vector with a fixed second moment
E[g2
t ] = σ2, we have (see appx. B.1 for details):"
BASELINE EXPERIMENTAL SETUP & RESULTS,0.07960199004975124,"E[m2
t] = (1 −β2t
1 )1 −β1"
BASELINE EXPERIMENTAL SETUP & RESULTS,0.0812603648424544,"1 + β1
σ2,
E[vt] = (1 −βt
2)σ2
(1)"
BASELINE EXPERIMENTAL SETUP & RESULTS,0.08291873963515754,"In this case the bias correction for β1 is incorrect since it is derived for a constant gradient. With the
bias correction the size becomes E[∥ˆm∥2] = 1+βt
1
1−βt
1
1−β1
1+β1 σ2, amplifying the norm of early updates by
p"
BASELINE EXPERIMENTAL SETUP & RESULTS,0.0845771144278607,"(1 + βt
1)/(1 −βt
1). This factor is larger if the gradients between successive steps are negatively
correlated, which we empirically observe happening in our setup (see §6.2)."
BASELINE EXPERIMENTAL SETUP & RESULTS,0.08623548922056384,"The ℓ2-norm of AdamW updates can therefore vary significantly due to the initial bias correction,
changes in the alignment of the gradients throughout training, and potential variations in the gradient
norm over time. Lion [3] is a closely related optimizer that uses an element-wise sign operation
to normalize the update, giving +1 for positive values, −1 for negative values and 0 for zeros.
Ignoring the possibility of zeros, this gives a constant update norm. Lion is closely related to Adam,
and can be obtained by tracking the size of mt instead of gt in line 6 while setting β2 = 0. It
also uses a form of scaled Nesterov momentum instead of the traditional heavy-ball variant and
a hyperparameter specification that differs significantly from AdamW. We propose a Lion variant,
LionA (algo. 2), that keeps the hyperparameters more compatible with those of AdamW. The learning
rate is kept comparable by scaling the ℓ2 update size to match that of AdamW in the random-gradient
scenario, see appx. B.1 for the derivation of the scaling factors. Due to its ability to perfectly control
the size of each update, we use Lion based methods for the remaining of this paper. This avoids
confounding effects in Adam-like optimizers, such as v being inaccurate from rapidly decreasing
gradient magnitudes early in training, which can induce an additional warmup-like effect."
BASELINE EXPERIMENTAL SETUP & RESULTS,0.087893864013267,"In fig. 2 we repeat the baseline sweep using LionA. Despite perfect control of the ℓ2 update norm
(as seen in panel 3), the benefit of warmup remains. This leads us to conclude that the ℓ2 update
size is not sufficient to quantify the “effectively” large updates that we conjecture warmup
mitigates. The final panel shows that the angular update size (see definition in the following section),
proposed to be a better measure of an effective step size by Wan et al. [38], still varies throughout
training with a spike at the start of training. In the next section we explore the reasons for the large
initial angular updates and how they significantly contribute to the need for warmup."
BASELINE EXPERIMENTAL SETUP & RESULTS,0.08955223880597014,"Algorithm 2 LionA: A modified version of the Lion [3] optimizer for greater compatibility with
AdamW (algo. 1). The sign operation replaces the magnitude smoothing, explicitly controlling the
ℓ2-norm of each update. Additional scaling keeps the hyperparameters η, λ comparable to AdamW."
BASELINE EXPERIMENTAL SETUP & RESULTS,0.0912106135986733,"Require: Learning rate ηt, weight decay λ, momentum β, Nesterov flag ν"
BASELINE EXPERIMENTAL SETUP & RESULTS,0.09286898839137644,"1: Initialize: Time step t ←0, parameter vector θ0, momentum vector m0 ←0
2: while stopping criteria not met :
3:
t ←t + 1
4:
gt ←Mini-batch gradient w.r.t. θt−1
5:
mt ←βmt−1 + (1 −β)gt
6:
if Nesterov flag ν is set :"
BASELINE EXPERIMENTAL SETUP & RESULTS,0.0945273631840796,"7:
θt ←(1 −ηtλ)θt−1 −ηt · q"
BASELINE EXPERIMENTAL SETUP & RESULTS,0.09618573797678276,(1 −β2)2 + β4 1−β
BASELINE EXPERIMENTAL SETUP & RESULTS,0.0978441127694859,"1+β · sign(βmt + (1 −β)gt)
8:
else:
9:
θt ←(1 −ηtλ)θt−1 −ηt · q"
BASELINE EXPERIMENTAL SETUP & RESULTS,0.09950248756218906,"1−β
1+β · sign(mt)"
THE IMPORTANCE AND IRREGULARITY OF THE ANGULAR UPDATE SIZE,0.1011608623548922,"5
The Importance and Irregularity of the Angular Update Size"
THE IMPORTANCE AND IRREGULARITY OF THE ANGULAR UPDATE SIZE,0.10281923714759536,"The effect of a weight vector wt ∈RC used in a dot product with some vector x (e.g., in a neuron):"
THE IMPORTANCE AND IRREGULARITY OF THE ANGULAR UPDATE SIZE,0.1044776119402985,"⟨wt, x⟩= ∥wt∥∥x∥cos (∠(wt, x))
(2)"
THE IMPORTANCE AND IRREGULARITY OF THE ANGULAR UPDATE SIZE,0.10613598673300166,"can be understood in terms of its magnitude ∥wt∥and direction wt/∥wt∥. The magnitude acts like a
gain, scaling the outputs, whereas the direction determines which input representations x the system
responds to. The angular update size [38] of an update wt 7→wt+1 is defined as:"
THE IMPORTANCE AND IRREGULARITY OF THE ANGULAR UPDATE SIZE,0.1077943615257048,"∠(wt+1, wt) = arccos
⟨wt−1, wt+1⟩"
THE IMPORTANCE AND IRREGULARITY OF THE ANGULAR UPDATE SIZE,0.10945273631840796,"∥wt∥∥wt∥ 
(3)"
THE IMPORTANCE AND IRREGULARITY OF THE ANGULAR UPDATE SIZE,0.1111111111111111,"and measures how fast the direction of wt changes during training, and thus its “preference” for x."
THE IMPORTANCE AND IRREGULARITY OF THE ANGULAR UPDATE SIZE,0.11276948590381426,"With BatchNorm [15] and similar operations [1, 14, 42, 4], a network can become invariant to the
magnitude of weight vectors like wt, such that only the direction matters and the vector is said to be
scale-invariant. Weight Normalization [30] provides a good example of this, changing the system to:"
THE IMPORTANCE AND IRREGULARITY OF THE ANGULAR UPDATE SIZE,0.11442786069651742,"⟨wt/∥wt∥, x⟩= ∥x∥cos (∠(wt, x))
(4)"
THE IMPORTANCE AND IRREGULARITY OF THE ANGULAR UPDATE SIZE,0.11608623548922056,"Note that although the system output is invariant to the magnitude ∥wt∥, traditional optimizers are
not. Scaling the value of a scale-invariant weight vector by a factor of c > 0, results in a gradient that
is scaled by c−1 and curvature that is scaled by c−2 (see appx. B.2). For SGD this scales the angular
update by c−2 and for Adam-like optimizers it is scaled by c−1. With weight decay the magnitude of
scale-invariant vectors trends towards a certain stable equilibrium value over time which also results
in a specific expected angular update size as described by Wan et al. [38], Kosson et al. [19]."
THE IMPORTANCE AND IRREGULARITY OF THE ANGULAR UPDATE SIZE,0.11774461028192372,"This has several important implications. Changing the initialization magnitude of scale-invariant
weights will scale the angular updates over time for standard optimizers, resulting in effects similar
to modifying the learning rate schedule. For initial weight magnitudes that are small compared to the
equilibrium magnitude, the early angular updates will be large and these optimizers may benefit from
learning rate warmup to counteract this. These effects also make the notion of “curvature” somewhat
arbitrary as it can be scaled without changing the encoded function. Optimizers that specifically
account for the weight magnitude would be invariant to these effects which may reduce the need
for warmup from the traditional curvature perspective. Although standard transformers are not
fully scale-invariant, many of the angular update insights still approximately hold for un-normalized
weights [19]."
THE IMPORTANCE AND IRREGULARITY OF THE ANGULAR UPDATE SIZE,0.11940298507462686,"In light of this, we modify LionA to better control the angular update size by making the updates
to weight matrices proportional to their weight magnitude, resulting in algo. 3. We normalize the
angular update size to match the equilibrium value, replacing weight decay with projections similar
to Kosson et al. [19]. However, unlike their RVs, we make the angular updates proportional to the
learning rate schedule which we found was necessary for good performance in our case. We also
do not rely on additional exponential moving averages to control the angular update size, instead
utilizing the fixed update size from the LionA optimizer. This is similar to the Adam scheme used by
Karras et al. [17] with good results for diffusion models. No additional normalization operations or
scaling factors are introduced, which we still find to result in decent performance."
THE IMPORTANCE AND IRREGULARITY OF THE ANGULAR UPDATE SIZE,0.12106135986733002,"Algorithm 3 LionAR: A rotational version of algo. 2 inspired by Kosson et al. [19]. The parameter
vector is divided into sub-vectors θ = [θ(1), . . . , θ(P )], each corresponding to either the weight
vector of a neuron (e.g. a matrix row / a convolutional filter), or other parameters such as gains and
biases. The updates of neuronal weight vectors are scaled to be proportional to their magnitude
which is kept constant through projections that replace weight decay. Additional hyperparameter
adjustments are made for compatibility with AdamW. The weight decay hyperparameter remains,
fulfilling its primary role as a scaling factor for the relative updates of neurons [19]."
THE IMPORTANCE AND IRREGULARITY OF THE ANGULAR UPDATE SIZE,0.12271973466003316,"Require: Learning rate ηt, weight decay λ, momentum β, Nesterov flag ν"
THE IMPORTANCE AND IRREGULARITY OF THE ANGULAR UPDATE SIZE,0.12437810945273632,"1: Initialize: Time step t ←0, parameter vector θ0, momentum vector m0 ←0
2: while stopping criteria not met :
3:
t ←t + 1
4:
[g(1)
t
, . . . , g(P )
t
] ←Mini-batch gradient w.r.t. θt−1, divided into sub-vectors like θ
5:
for p ∈{1, . . . , P} :
6:
m(p)
t
←βm(p)
t−1 + (1 −β)g(p)
t
7:
if Nesterov flag ν is set :
8:
u(p)
t
←βm(p)
t
+ (1 −β)g(p)
t
9:
γ ← q"
THE IMPORTANCE AND IRREGULARITY OF THE ANGULAR UPDATE SIZE,0.12603648424543948,(1 −β2)2 + β4 1−β
THE IMPORTANCE AND IRREGULARITY OF THE ANGULAR UPDATE SIZE,0.12769485903814262,"1+β
# Nesterov momentum scaling factor
10:
else:
11:
u(p)
t
←m(p)
t
12:
γ ← q"
THE IMPORTANCE AND IRREGULARITY OF THE ANGULAR UPDATE SIZE,0.12935323383084577,"1−β
1+β
# Heavy-ball momentum scaling factor"
THE IMPORTANCE AND IRREGULARITY OF THE ANGULAR UPDATE SIZE,0.1310116086235489,"13:
if θ(p) ∈RC is a neuronal weight vector :
14:
ˆθ(p)
t
←θ(p)
t−1 −
ηt
maxτ (ητ ) · p"
THE IMPORTANCE AND IRREGULARITY OF THE ANGULAR UPDATE SIZE,0.13266998341625208,"2 maxτ(ητ)λ · γ · (∥θ(p)
0 ∥/
√"
THE IMPORTANCE AND IRREGULARITY OF THE ANGULAR UPDATE SIZE,0.13432835820895522,"C) · sign(u(p)
t
)"
THE IMPORTANCE AND IRREGULARITY OF THE ANGULAR UPDATE SIZE,0.13598673300165837,"15:
θ(p)
t
←ˆθ(p)
t
· ∥θ(p)
0 ∥/∥ˆθ(p)
t
∥
# Reset the magnitude to the initial value
16:
else:
17:
θ(p)
t
←θ(p)
t−1 −ηt · γ · sign(u(p)
t
)"
THE IMPORTANCE AND IRREGULARITY OF THE ANGULAR UPDATE SIZE,0.13764510779436154,"10
3
10
2"
THE IMPORTANCE AND IRREGULARITY OF THE ANGULAR UPDATE SIZE,0.13930348258706468,Learning Rate 3.1 3.2 3.3 3.4 3.5
THE IMPORTANCE AND IRREGULARITY OF THE ANGULAR UPDATE SIZE,0.14096185737976782,Validation Loss
THE IMPORTANCE AND IRREGULARITY OF THE ANGULAR UPDATE SIZE,0.14262023217247097,"0%
2%
5%
10%
20%"
THE IMPORTANCE AND IRREGULARITY OF THE ANGULAR UPDATE SIZE,0.14427860696517414,"0
0.05
0.50
1
Iteration (  scale) 4 6 8 10"
THE IMPORTANCE AND IRREGULARITY OF THE ANGULAR UPDATE SIZE,0.14593698175787728,Training Loss
THE IMPORTANCE AND IRREGULARITY OF THE ANGULAR UPDATE SIZE,0.14759535655058043,"0
0.05
0.50
1
Iteration (  scale) 0 1"
THE IMPORTANCE AND IRREGULARITY OF THE ANGULAR UPDATE SIZE,0.14925373134328357,"Avg 
-update size / max"
THE IMPORTANCE AND IRREGULARITY OF THE ANGULAR UPDATE SIZE,0.15091210613598674,"0
0.05
0.50
1
Iteration (  scale) 0 1"
THE IMPORTANCE AND IRREGULARITY OF THE ANGULAR UPDATE SIZE,0.15257048092868988,Avg Local RRC / max
THE IMPORTANCE AND IRREGULARITY OF THE ANGULAR UPDATE SIZE,0.15422885572139303,"Figure 3: LionAR (algo. 3) reduces but does not fully eliminate the benefit of warmup. Panel 1:
LionAR is more stable across learning rates and shows a reduced but still significant performance
gap without warmup. Panel 2: Comparing the 0% and 5% warmup for learning rate ≈10−2 shows
the warmup run overtaking early in training. Panel 3: LionAR precisely controls the angular update
size throughout training. Panel 4: Despite fixed angular (and thus relative) updates in weight space,
the relative change of the internal representations (see §6) is large initially without warmup."
THE IMPORTANCE AND IRREGULARITY OF THE ANGULAR UPDATE SIZE,0.1558872305140962,"Figure 3 repeats the GPT2 training sweep with LionAR. Consistent with the findings of Kosson et al.
[19] we find that controlling the angular updates stabilizes training and decreases the benefit
from warmup, but does not entirely eliminate it in this setting. Both the angular change and the
ℓ2-norm are simple measures of the update magnitude in parameter space that do not account for
the direction or other aspects of the update. In the next section we show how a fixed update size
in parameter space can result in large changes to the internal representations of the network (a.k.a.
features, activations etc), as shown in the final panel of fig. 3."
EARLY GRADIENT ALIGNMENT RESULTS IN LARGE REPRESENTATION CHANGES,0.15754560530679934,"6
Early Gradient Alignment Results in Large Representation Changes"
EARLY GRADIENT ALIGNMENT RESULTS IN LARGE REPRESENTATION CHANGES,0.15920398009950248,"Our two approaches to measuring and controlling the update size in weight space failed to fully
explain the need for warmup. As an alternative to the parameters, we can analyze changes in"
EARLY GRADIENT ALIGNMENT RESULTS IN LARGE REPRESENTATION CHANGES,0.16086235489220563,"the internal representations or activations of the neural network (feature learning). Although this
is harder to analyze and control, it may ultimately be a better measure of the true impact of an
update. A parameter update can only affect the network output, and hence the loss, by changing the
representation of the network inputs at some layer. Large, sudden, changes in the representations
could significantly affect the non-linearities, potentially causing lasting issues such as dead ReLUs
or vanishing gradients from saturated sigmoids. This could in turn explain the lasting performance
degradation observed without warmup."
EARLY GRADIENT ALIGNMENT RESULTS IN LARGE REPRESENTATION CHANGES,0.1625207296849088,"A given parameter update will affect the representations of each distinct input sample differently. The
gradients computed on these samples also generally differ, but can align to some extent. For a higher
gradient alignment, the impact of a parameter update of a given magnitude on the representations
will be larger than otherwise. We will analyze this for a dot product making up a single neuron:"
EARLY GRADIENT ALIGNMENT RESULTS IN LARGE REPRESENTATION CHANGES,0.16417910447761194,"y = w⊤X = [y1, . . . , yB]⊤= [⟨w, x1⟩, . . . , ⟨w, xB⟩]⊤
(5)"
EARLY GRADIENT ALIGNMENT RESULTS IN LARGE REPRESENTATION CHANGES,0.16583747927031509,"where X = [x1, . . . , xB] ∈RC×B are the C-dimensional representations of a random mini-batch of
B inputs that is fed into the neuron, w ∈RC is the weight vector, and y ∈RB is a batch of outputs.
For a weight update w 7→w + ∆w, we aim to quantify the size of the output change ∆y = ∆w⊤X
computed on the same inputs. We focus on the Relative Representation Change (RRC): ∥∆y∥"
EARLY GRADIENT ALIGNMENT RESULTS IN LARGE REPRESENTATION CHANGES,0.16749585406301823,"∥y∥
= ∥∆w⊤X∥"
EARLY GRADIENT ALIGNMENT RESULTS IN LARGE REPRESENTATION CHANGES,0.1691542288557214,"∥w⊤X∥
(6)"
EARLY GRADIENT ALIGNMENT RESULTS IN LARGE REPRESENTATION CHANGES,0.17081260364842454,"similar to the angular weight updates, as the sensitivity to the absolute change ∥∆y∥can be unclear
due to normalization or other scaling operations. Note that this is a measure of a local change, not
accounting for changes in the inputs X from updates to preceding layers (global change)."
NORMALIZED GRADIENT DESCENT,0.1724709784411277,"6.1
Normalized Gradient Descent"
NORMALIZED GRADIENT DESCENT,0.17412935323383086,We will focus our analysis on the relatively tractable case of normalized gradient descent with updates:
NORMALIZED GRADIENT DESCENT,0.175787728026534,"∆w = −η
g
p"
NORMALIZED GRADIENT DESCENT,0.17744610281923714,"E[∥g∥2]
,
g = 1 B B
X"
NORMALIZED GRADIENT DESCENT,0.1791044776119403,"b=1
gb
(7)"
NORMALIZED GRADIENT DESCENT,0.18076285240464346,"where gb is the gradient of some loss w.r.t. w for the b-th element of the mini-batch. We will use the
following definitions, properties, lemmas and assumptions for this system (see appx. B.4 for details):"
NORMALIZED GRADIENT DESCENT,0.1824212271973466,"• D1: We define gb =: ¯g + ˜gb where ¯g = E[g] and ˜gb is the difference with E[˜gb] = 0.
• D2: We define φ := E[∥¯g∥2]/E[∥˜gb∥2] as the Signal-to-Noise Ratio (SNR) of the gradient.
• P1: For a neuron, gb ∥xb, and hence xb = sign(⟨xb, gb⟩) · (∥xb∥/∥gb∥) · (¯g + ˜gb).
• L1: Consider two independent random vectors a ∈RC and b ∈RC, whose elements are
independent and identically distributed (IID). If at least one of the vectors has a zero-mean
distribution, then the expected value of the squared inner product of a and b is given by
E[⟨a, b⟩2] = E[∥a∥2]E[∥b∥2]/C.
• A1: We assume the following vector pairs satisfy L1: (xi, ˜gb) when i ̸= b, (¯g, ˜gb) and (w, xb)."
NORMALIZED GRADIENT DESCENT,0.18407960199004975,This allows us to compute the expected square relative representation change (detailed in appx. B.4):
NORMALIZED GRADIENT DESCENT,0.1857379767827529,E[(∆yb)2]
NORMALIZED GRADIENT DESCENT,0.18739635157545606,"E[y2
b]
=
η2C
B2∥w∥2
1
E[∥g∥2] "
NORMALIZED GRADIENT DESCENT,0.1890547263681592,E[∥gb∥2] + B −1
NORMALIZED GRADIENT DESCENT,0.19071310116086235,"C
E[∥˜gi∥2]"
NORMALIZED GRADIENT DESCENT,0.19237147595356552,+ (B−1)2
NORMALIZED GRADIENT DESCENT,0.19402985074626866,E[∥gb∥2]
NORMALIZED GRADIENT DESCENT,0.1956882255389718,"
∥¯g∥4 + ∥¯g∥2E[∥˜gb∥2] C"
NORMALIZED GRADIENT DESCENT,0.19734660033167495,"
+ 2(B−1)∥¯g∥2 ! (8)"
NORMALIZED GRADIENT DESCENT,0.19900497512437812,"=
η2C
B2∥w∥2
1
φ + 1 B "
NORMALIZED GRADIENT DESCENT,0.20066334991708126,(φ+1) + B−1
NORMALIZED GRADIENT DESCENT,0.2023217247097844,"C
+
(B−1)2φ φ + 1"
NORMALIZED GRADIENT DESCENT,0.20398009950248755,"
φ + 1 C"
NORMALIZED GRADIENT DESCENT,0.20563847429519072,"
+ 2(B−1)φ ! (9)"
NORMALIZED GRADIENT DESCENT,0.20729684908789386,"The expected relative change in the output for a given sample can be broken down into three sources,
the contribution of the sample itself (first term), random interference from the “noise” ˜gi of other
samples (second term), and finally amplification of the common mean component ¯g (third term)."
NORMALIZED GRADIENT DESCENT,0.208955223880597,"10
4
100 SNR 10
2 10
1 100"
NORMALIZED GRADIENT DESCENT,0.21061359867330018,LR scaling for const RRC B=100 B=102 B=104
NORMALIZED GRADIENT DESCENT,0.21227197346600332,"0 0.05
0.50
1
Iteration (  scale) 10
4 10
3 10
2 10
1 100"
NORMALIZED GRADIENT DESCENT,0.21393034825870647,"Signal-to-Noise Ratio 0%
5%"
NORMALIZED GRADIENT DESCENT,0.2155887230514096,"0 0.05
0.50
1
Iteration (  scale) 1.0 0.5 0.0 0.5 1.0"
NORMALIZED GRADIENT DESCENT,0.21724709784411278,"Alignment cos
(gt, mt
1)"
NORMALIZED GRADIENT DESCENT,0.21890547263681592,"0 0.05
0.50
1
Iteration (  scale) 1.0 0.5 0.0 0.5 1.0"
NORMALIZED GRADIENT DESCENT,0.22056384742951907,Velocity-Grad Cancellation
NORMALIZED GRADIENT DESCENT,0.2222222222222222,"Figure 4: Equation (9) predicts that the learning rate needs to be downscaled for higher signal to
noise ratios (φ) to keep the relative representation change constant. Larger batch sizes are affected
more, with scaling becoming significant when φ > B−1. Panel 2: Measurements of the SNR for the
two highlighted runs in fig. 3. Note the SNR starts very high but is also remains large in comparison
to our B = 480 for almost all of training. Panel 3: The gradient is strongly oppositely aligned with
the momentum vector for most of training (shown for an example layer). Panel 4: Projecting the
momentum component of the updates onto the gradient component shows that this results in the
momentum vector “cancelling” roughly half the gradient on average."
NORMALIZED GRADIENT DESCENT,0.22388059701492538,"The RRC expression provides many interesting insights. In the case of large input dimension C →∞
and small SNR φ ≈0, keeping the RRC constant for different batch sizes involves scaling the
learning rate η ∝
√"
NORMALIZED GRADIENT DESCENT,0.22553897180762852,"B, as suggested by Malladi et al. [27] for Adam. When the SNR φ is some
finite and value and C is still large, this scaling rule instead starts to break down around B = 1/φ,
matching the predicted critical batch size of e.g. McCandlish et al. [28]. The role of the dimension
C in the expression is curious, suggesting that narrower layers experience larger changes due to
random inference from other samples in a given batch. The C in the leading factor also suggests
that the angular updates can be smaller for a larger input dimension, similar to what is proposed in
µ-parameterization [44, 45]. Most importantly, this expression shows that if the SNR changes
throughout training the learning rate needs to be adjusted to keep the RRC constant. In
particular, with large batch sizes, a high initial SNR results in large representation changes
which warmup can help prevent. The first panel of fig. 4 shows how eq. (9) predicts we should
downscale the learning rate for different batch sizes and SNRs, assuming we originally scaled the
learning rate η ∝
√"
NORMALIZED GRADIENT DESCENT,0.22719734660033167,"B and that C is large. The second panel confirms that the SNR indeed starts out
large, suggesting lower initial learning rates are needed, i.e. warmup."
NORMALIZED GRADIENT DESCENT,0.22885572139303484,"In the first panel of fig. 5, we show the results of adding a term that scales the update size as predicted
by eq. (9). This acts similar to an automatic warmup based on online measurements of the SNR
which we obtain from the gradient accumulation of micro-batches. Although this helps close the
gap between warmup and no-warmup, the overall performance is slightly worse. One potential issue
is that our batch size of 480 is quite large compared to the measured SNR, exceeding the critical
batch size estimation throughout most of training. This results in a scaling of the step size throughout
training, which distorts the decay phase. It also requires large learning rate values to counteract
the scaling, which may destabilize the training of non-matrix weights like gains. We increase the
weight decay by a factor of 32× to try to increase the angular updates relative to gains in order to
compensate, but this value was not tuned and is unlikely to be optimal. We believe approaches that
aim to directly control the RRC are a promising direction but require further work to be practical."
THE ROLE OF MOMENTUM,0.23051409618573798,"6.2
The Role of Momentum"
THE ROLE OF MOMENTUM,0.23217247097844113,"Momentum is believed to be a key enabler of optimization with larger batch sizes [32, 39, 31]. How-
ever, it is unclear how it should change predictions for the critical batch size or relative representation
change. Momentum spreads the application of a gradient out over multiple steps which tends to make
each update smaller, especially for a random walk, which is reflected in our scaling coefficients in
algo. 2 and 3. The smaller updates are counteracted by an increased correlation in their direction,
which can result in similar “long term” changes from each gradient sample, especially for simpler
methods like SGD that don’t normalize the step size. In the last two panels of fig. 4 we observe that
in our setup the gradient and momentum are negatively correlated, counteracting each other. We find
momentum crucial for performant training, panel 2 of fig. 5 shows significant degradation without it."
THE ROLE OF MOMENTUM,0.23383084577114427,"10
3
10
2"
THE ROLE OF MOMENTUM,0.23548922056384744,Learning Rate 3.1 3.2 3.3 3.4 3.5
THE ROLE OF MOMENTUM,0.23714759535655058,Validation Loss
THE ROLE OF MOMENTUM,0.23880597014925373,LionAR =0.9 +RRC comp
THE ROLE OF MOMENTUM,0.24046434494195687,"0%
5%
10%"
THE ROLE OF MOMENTUM,0.24212271973466004,"10
3
10
2"
THE ROLE OF MOMENTUM,0.24378109452736318,Learning Rate 4.0 4.2 4.4
THE ROLE OF MOMENTUM,0.24543946932006633,Validation Loss
THE ROLE OF MOMENTUM,0.2470978441127695,LionAR =0
THE ROLE OF MOMENTUM,0.24875621890547264,"0%
5%
20%"
THE ROLE OF MOMENTUM,0.2504145936981758,"10
3
10
2"
THE ROLE OF MOMENTUM,0.25207296849087896,Learning Rate 3.1 3.2 3.3 3.4 3.5
THE ROLE OF MOMENTUM,0.2537313432835821,Validation Loss
THE ROLE OF MOMENTUM,0.25538971807628524,LionAR =0.98 +N +MC
THE ROLE OF MOMENTUM,0.2570480928689884,"0%
2%
5%
10%
20%"
THE ROLE OF MOMENTUM,0.25870646766169153,"10
3
10
2"
THE ROLE OF MOMENTUM,0.2603648424543947,Learning Rate 3.1 3.2 3.3 3.4 3.5
THE ROLE OF MOMENTUM,0.2620232172470978,Validation Loss
THE ROLE OF MOMENTUM,0.263681592039801,LionA =0.98 +N +MC
THE ROLE OF MOMENTUM,0.26533996683250416,"0%
2%
5%
10%
20%"
THE ROLE OF MOMENTUM,0.2669983416252073,"Figure 5: Panel 1: LionAR with a correction factor for the RRC based on eq. (9) does not benefit from
a warmup. Panel 2: LionAR training without momentum results in drastically lower performance.
Panel 3: In LionAR with increased momentum β = 0.98, Nesterov momentum and an inverse bias
correction for early momentum, no warmup performs best. Panel 4: The same does not apply to
LionA, suggesting that these changes are not sufficient without controlling the angular updates."
THE ROLE OF MOMENTUM,0.26865671641791045,"We believe the smaller update sizes for momentum combined with the potential for later gradients
to counteract earlier gradients during their application over time, can help stabilize training. An
otherwise large relative representation change is spread out over multiple steps and counteracted
by later gradients. Higher values of momentum should amplify these effects. Looking at the total
contribution of each gradient also implies that with momentum early updates should be smaller
when measured in parameter space, otherwise the relative representation change for those
samples is too large. This is equivalent to entirely removing the β1 bias correction in AdamW
(algo. 1, line 7), or introducing an inverse bias correction in Lion like algorithms (see appx. B.1 for
details). Higher β values should help amplify the stabilization effects of momentum. In fig. 5 we
find that at higher momentum values LionAR no longer benefits from warmup unlike LionA
which still needs it. These experiments use Nesterov momentum and the additional inverse bias
correction, though these adjustments offer only minor improvements compared to higher momentum."
THE DETRIMENTAL EFFECTS OF LARGE UPDATES,0.2703150912106136,"7
The Detrimental Effects of Large Updates"
THE DETRIMENTAL EFFECTS OF LARGE UPDATES,0.27197346600331673,"In appx. A we empirically investigate potential causes for the lasting performance degradation from
large initial updates for a small ResNet model. We find that the performance impact best correlates
with the number of dead ReLUs and is improved by the use of leaky-ReLUs, which fits well with our
perspective of large changes in the internal representations. We also investigated whether overfitting
to initial training samples or the correlation between weight vectors of different neurons could explain
the necessity for warmup, but these factors did not show a significant impact."
THE ROLE OF LARGER BATCH SIZES,0.2736318407960199,"8
The Role of Larger Batch Sizes"
THE ROLE OF LARGER BATCH SIZES,0.2752902155887231,"Warmup is often used with larger batch sizes in particular, for example in the setting where Goyal et al.
[10] first proposed using linear warmup. Although this was for SGD, we expect the need for warmup
to be amplified at larger batch sizes for two of the reasons we identified. The first is that larger batch
sizes are more likely to exceed the critical batch size early in training. The second is the size of early
angular updates. As shown by Kosson et al. [19], the equilibrium weight magnitude depends on the
learning rate and weight decay value. Common hyperparameter scaling rules for a modified batch
size only change the learning rate but not the weight decay, which shifts the equilibrium magnitude.
The smaller the initialization magnitude is compared to the equilibrium magnitude, the larger the
early angular updates will be relative to their steady state value, potentially necessitating warmup."
LIMITATIONS,0.2769485903814262,"9
Limitations"
LIMITATIONS,0.27860696517412936,"Our main experiments focus on a single network which may not be broad enough to generalize to a
wide range of networks. In appx. C.2 we experiments with an additional dataset and architecture but
the scale of the experiments is still limited and they cover a limited range of hyperparameters. We
believe we identify real factors that contribute to the need for warmup, but these may not be the only"
LIMITATIONS,0.2802653399668325,"ones across a broader range of settings. Similarly, the promising results for reducing or eliminating
the warmup with higher momentum values or the relative representation correction would benefit
from broader validation."
CONCLUSION,0.28192371475953565,"10
Conclusion"
CONCLUSION,0.2835820895522388,"In this work we explored how the size of the updates ∆w = ηu impacts the need for learning rate
warmup. We showed that u can be large initially when measured in terms of its ℓ2-norm (§4), the
resulting directional change in w (angular update, §5), as well as the resulting change in the internal
representations of the network (relative representation change, §6). We argued that small initial
values of the learning rate η are beneficial to counteract large values of u, i.e. that a learning rate
warmup simply keeps some notion of the overall “effective” update size reasonable. We showed this
experimentally rather than theoretically by modifying the optimizers to normalize the size of u based
on each metric and measuring how these changes affected the benefit of using learning rate warmup."
CONCLUSION,0.28524046434494194,"The two weight-based measures of the update size, the ℓ2-norm and angular update did not fully
account for the need for warmup. However, quantifying the update size in terms of the relative change
in neural representations shows potential. This measure is closely linked to the angular update size
but accounts for changes in the signal characteristics of the gradient, which can vary significantly
throughout training. Effectively controlling neural representation changes is a challenging task we
leave for future work, but our initial attempts show encouraging results in reducing the need for a
manually configured warmup. We also highlighted the importance of high momentum for warmup;
when combined with angular update control and an inverse bias correction, it may enable efficient
warmup-free training. Overall, our work provides new insights into the benefit of learning rate
warmup with modern optimizers beyond SGD and suggests potential directions for eliminating it."
CONCLUSION,0.28689883913764513,"Although we present new methods we consider promising, we still recommend the use of a short
warmup in practice. Fully eliminating it seems to require significant modifications that also need
further validation across additional settings. However, we hope to have provided the reader with
a new perspective and simple intuition for why warmup is beneficial for training. We also hope
our work inspires further exploration of how learning should be controlled and scheduled in neural
network training. In particular, it seems that the learning rate in current optimizers does not really
control the “rate of learning”, making learning rate schedules and the use of warmup highly arbitrary."
REFERENCES,0.2885572139303483,References
REFERENCES,0.2902155887230514,"[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016. 5"
REFERENCES,0.29187396351575456,"[2] Jeremy Bernstein, Arash Vahdat, Yisong Yue, and Ming-Yu Liu. On the distance between
two neural networks and the stability of learning. Advances in Neural Information Processing
Systems, 33:21370–21381, 2020. arXiv:2002.03432. 2"
REFERENCES,0.2935323383084577,"[3] Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Hieu Pham, Xuanyi
Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, and Quoc V Le. Symbolic discovery of opti-
mization algorithms. In Thirty-seventh Conference on Neural Information Processing Systems,
2023. URL https://openreview.net/forum?id=ne6zeqLFCZ. arXiv:2302.06675. 1, 4, 5"
REFERENCES,0.29519071310116085,"[4] Vitaliy Chiley, Ilya Sharapov, Atli Kosson, Urs Koster, Ryan Reece, Sofia Samaniego de la
Fuente, Vishal Subbiah, and Michael James. Online normalization for training neural networks.
Advances in Neural Information Processing Systems, 32, 2019. arXiv:1905.05894. 5"
REFERENCES,0.296849087893864,"[5] Katie E Everett, Lechao Xiao, Mitchell Wortsman, Alexander A Alemi, Roman Novak, Peter J
Liu, Izzeddin Gur, Jascha Sohl-Dickstein, Leslie Pack Kaelbling, Jaehoon Lee, and Jeffrey
Pennington. Scaling exponents across parameterizations and optimizers. In Forty-first Inter-
national Conference on Machine Learning, 2024. URL https://openreview.net/forum?
id=0ksNeD1SJT. arXiv:2407.05872. 18"
REFERENCES,0.29850746268656714,"[6] Jingwen Fu, Bohan Wang, Huishuai Zhang, Zhizheng Zhang, Wei Chen, and Nanning
Zheng. When and why momentum accelerates sgd: An empirical study. arXiv preprint
arXiv:2306.09000, 2023. 2"
REFERENCES,0.30016583747927034,"[7] Justin Gilmer, Behrooz Ghorbani, Ankush Garg, Sneha Kudugunta, Behnam Neyshabur, David
Cardoze, George Edward Dahl, Zachary Nado, and Orhan Firat. A loss curvature perspective
on training instabilities of deep learning models.
In International Conference on Learn-
ing Representations, 2022.
URL https://openreview.net/forum?id=OcKMT-36vUs.
arXiv:2110.04369. 2"
REFERENCES,0.3018242122719735,"[8] Aaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/
OpenWebTextCorpus, 2019. 2, 22"
REFERENCES,0.3034825870646766,"[9] Akhilesh Gotmare, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. A closer look
at deep learning heuristics: Learning rate restarts, warmup and distillation. In International
Conference on Learning Representations, 2019. URL https://openreview.net/forum?
id=r14EOsCqKX. arXiv:1810.13243. 2"
REFERENCES,0.30514096185737977,"[10] Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola,
Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training
imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. 1, 2, 9"
REFERENCES,0.3067993366500829,"[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 770–778, 2016. arXiv:1512.03385. 1, 2, 14"
REFERENCES,0.30845771144278605,"[12] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.
Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. URL
https://arxiv.org/abs/2203.15556. 2"
REFERENCES,0.3101160862354892,"[13] Xiao Shi Huang, Felipe Perez, Jimmy Ba, and Maksims Volkovs. Improving transformer
optimization through better initialization. In International Conference on Machine Learn-
ing, pages 4475–4483. PMLR, 2020.
URL https://proceedings.mlr.press/v119/
huang20f.html. 2"
REFERENCES,0.3117744610281924,"[14] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance
normalization. In Proceedings of the IEEE international conference on computer vision, pages
1501–1510, 2017. arXiv:1703.06868. 5"
REFERENCES,0.31343283582089554,"[15] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. In International conference on machine learning, pages
448–456. pmlr, 2015. arXiv:1502.03167. 5"
REFERENCES,0.3150912106135987,"[16] Andrej Karpathy. nanogpt. https://github.com/karpathy/nanoGPT/, 2023. 3"
REFERENCES,0.3167495854063018,"[17] Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine.
Analyzing and improving the training dynamics of diffusion models. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 24174–24184, 2024.
arXiv:2312.02696. 5"
REFERENCES,0.31840796019900497,"[18] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations (ICLR), San Diega, CA, USA, 2015. arXiv:1412.6980.
2"
REFERENCES,0.3200663349917081,"[19] Atli Kosson, Bettina Messmer, and Martin Jaggi. Rotational equilibrium: How weight decay bal-
ances learning across neural networks. In Forty-first International Conference on Machine Learn-
ing, 2024. URL https://openreview.net/forum?id=MQirNNU2pC. arXiv:2305.17212. 2,
3, 5, 6, 9, 14, 16"
REFERENCES,0.32172470978441126,"[20] Alex Krizhevsky. Learning multiple layers of features from tiny images. self-published, 2009.
URL https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf. 14"
REFERENCES,0.32338308457711445,"[21] Zhiyuan Li, Kaifeng Lyu, and Sanjeev Arora. Reconciling modern deep learning with traditional
optimization analyses: The intrinsic learning rate. Advances in Neural Information Processing
Systems, 33:14544–14555, 2020. arXiv:2010.02916. 2"
REFERENCES,0.3250414593698176,"[22] Zhiyuan Li, Sadhika Malladi, and Sanjeev Arora. On the validity of modeling SGD with
stochastic differential equations (SDEs). In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wort-
man Vaughan, editors, Advances in Neural Information Processing Systems, 2021. URL
https://openreview.net/forum?id=goEdyJ_nVQI. arXiv:2102.12470. 2"
REFERENCES,0.32669983416252074,"[23] Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and
Jiawei Han. On the variance of the adaptive learning rate and beyond. In International
Conference on Learning Representations, 2020. URL https://openreview.net/forum?
id=rkgz2aEKDr. arXiv:1908.03265. 2, 21"
REFERENCES,0.3283582089552239,"[24] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International
Conference on Learning Representations, 2019. URL https://openreview.net/forum?
id=Bkg6RiCqY7. arXiv:1711.05101. 1, 2, 3, 21"
REFERENCES,0.33001658374792703,"[25] Kaifeng Lyu, Zhiyuan Li, and Sanjeev Arora. Understanding the generalization benefit of
normalization layers: Sharpness reduction. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave,
and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL
https://openreview.net/forum?id=xp5VOBxTxZ. arXiv:2206.07085. 16"
REFERENCES,0.33167495854063017,"[26] Jerry Ma and Denis Yarats. On the adequacy of untuned warmup for adaptive optimization. In
Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 8828–8836,
2021. arXiv:1910.04209. 2, 21"
REFERENCES,0.3333333333333333,"[27] Sadhika Malladi, Kaifeng Lyu, Abhishek Panigrahi, and Sanjeev Arora. On the SDEs and
scaling rules for adaptive gradient algorithms. In Alice H. Oh, Alekh Agarwal, Danielle
Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems,
2022. URL https://openreview.net/forum?id=F2mhzjHkQP. arXiv:2205.10287. 2, 8"
REFERENCES,0.33499170812603646,"[28] Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model
of large-batch training. arXiv preprint arXiv:1812.06162, 2018. 2, 8"
REFERENCES,0.33665008291873966,"[29] Alec
Radford,
Jeff
Wu,
Rewon
Child,
David
Luan,
Dario
Amodei,
and
Ilya
Sutskever.
Language models are unsupervised multitask learners.
self-published,
2019.
URL https://d4mucfpksywv.cloudfront.net/better-language-models/
language_models_are_unsupervised_multitask_learners.pdf. 1, 2"
REFERENCES,0.3383084577114428,"[30] Tim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to
accelerate training of deep neural networks. Advances in neural information processing systems,
29, 2016. arXiv:1602.07868. 5"
REFERENCES,0.33996683250414594,"[31] Christopher J Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein, Roy Frostig, and
George E Dahl. Measuring the effects of data parallelism on neural network training. Journal
of Machine Learning Research, 20(112):1–49, 2019. arXiv:1811.03600. 8"
REFERENCES,0.3416252072968491,"[32] Samuel Smith, Erich Elsen, and Soham De. On the generalization benefit of noise in stochastic
gradient descent. In International Conference on Machine Learning, pages 9058–9067. PMLR,
2020. arXiv:2006.15081. 2, 8"
REFERENCES,0.34328358208955223,"[33] Daria
Soboleva,
Faisal
Al-Khateeb,
Robert
Myers,
Jacob
R
Steeves,
Joel
Hestness,
and
Nolan
Dey.
Slimpajama:
A
627b
token
cleaned
and
deduplicated
version
of
redpajama.
https://cerebras.ai/blog/
slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama,
2023. 21, 22"
REFERENCES,0.3449419568822554,"[34] Sebastian Stich, Amirkeivan Mohtashami, and Martin Jaggi. Critical parameters for scalable
distributed learning with large batches and asynchronous updates. In International Conference
on Artificial Intelligence and Statistics, pages 4042–4050. PMLR, 2021. arXiv:2103.02351. 2"
REFERENCES,0.3466003316749585,"[35] Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initial-
ization and momentum in deep learning. In International conference on machine learning, pages
1139–1147. PMLR, 2013. URL https://proceedings.mlr.press/v28/sutskever13.
html. 2"
REFERENCES,0.3482587064676617,"[36] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,
Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open
foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 22"
REFERENCES,0.34991708126036486,"[37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
processing systems, 30, 2017. arXiv:1706.03762. 1, 2"
REFERENCES,0.351575456053068,"[38] Ruosi Wan, Zhanxing Zhu, Xiangyu Zhang, and Jian Sun.
Spherical motion dynam-
ics: Learning dynamics of normalized neural network using sgd and weight decay.
In
M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors,
Advances in Neural Information Processing Systems, volume 34, pages 6380–6391. Cur-
ran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/
326a8c055c0d04f5b06544665d8bb3ea-Paper.pdf. arXiv:2006.08419. 4, 5"
REFERENCES,0.35323383084577115,"[39] Runzhe Wang, Sadhika Malladi, Tianhao Wang, Kaifeng Lyu, and Zhiyuan Li. The marginal
value of momentum for small learning rate SGD. In The Twelfth International Conference on
Learning Representations, 2024. URL https://openreview.net/forum?id=3JjJezzVkT.
arXiv:2307.15196. 8"
REFERENCES,0.3548922056384743,"[40] Ross
Wightman.
Pytorch
image
models.
https://github.com/rwightman/
pytorch-image-models, 2019. 14"
REFERENCES,0.35655058043117743,"[41] Mitchell Wortsman, Peter J Liu, Lechao Xiao, Katie Everett, Alex Alemi, Ben Adlam, John D
Co-Reyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, et al. Small-scale proxies for
large-scale transformer training instabilities. arXiv preprint arXiv:2309.14322, 2023. URL
https://arxiv.org/abs/2309.14322. 2"
REFERENCES,0.3582089552238806,"[42] Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference
on computer vision (ECCV), pages 3–19, 2018. arXiv:1803.08494. 5"
REFERENCES,0.3598673300165838,"[43] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai
Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer
architecture. In International Conference on Machine Learning, pages 10524–10533. PMLR,
2020. arXiv:2002.04745. 2"
REFERENCES,0.3615257048092869,"[44] Greg Yang, Edward J Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick
Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tuning large neural networks via
zero-shot hyperparameter transfer. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman
Vaughan, editors, Advances in Neural Information Processing Systems, 2021. URL https:
//openreview.net/forum?id=Bx6qKuBM2AD. arXiv:2203.03466. 8"
REFERENCES,0.36318407960199006,"[45] Greg Yang, James B Simon, and Jeremy Bernstein. A spectral condition for feature learning.
arXiv preprint arXiv:2310.17813, 2023. 8"
REFERENCES,0.3648424543946932,"[46] Dong Yin, Ashwin Pananjady, Max Lam, Dimitris Papailiopoulos, Kannan Ramchandran,
and Peter Bartlett. Gradient diversity: a key ingredient for scalable distributed learning. In
International Conference on Artificial Intelligence and Statistics, pages 1998–2007. PMLR,
2018. arXiv:1706.05699. 2"
REFERENCES,0.36650082918739635,"[47] Yang You, Igor Gitman, and Boris Ginsburg. Large batch training of convolutional networks.
arXiv preprint arXiv:1708.03888, 2017. 2"
REFERENCES,0.3681592039800995,"[48] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan
Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep
learning: Training bert in 76 minutes. In International Conference on Learning Representations,
2020. URL https://openreview.net/forum?id=Syx4wnEtvH. arXiv:1904.00962. 2"
REFERENCES,0.36981757877280264,"[49] Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George Dahl, Chris
Shallue, and Roger B Grosse. Which algorithmic choices matter at which batch sizes? Insights
from a noisy quadratic model. Advances in Neural Information Processing Systems, 32, 2019.
arXiv:1907.04164. 2"
REFERENCES,0.3714759535655058,"100
200
300
400
500
600
700
800"
REFERENCES,0.373134328358209,Epochs 86 88 90 92
REFERENCES,0.3747927031509121,Test Accuracy
REFERENCES,0.37645107794361526,Epoch Sweep
STANDARD,0.3781094527363184,"1 Standard
8 Standard
128 Standard
1 Leaky ReLU (0.1)
8 Leaky ReLU (0.1)
128 Leaky ReLU (0.1)"
STANDARD,0.37976782752902155,"Figure 6: The performance gap caused by large initial updates persists despite extended training
(800 epochs) in a standard ResNet-20. We investigate the influence of network non-linearities by
comparing two training methods while scaling update sizes during the 5 epoch initial phase by factors
of 1, 8, and 128: Standard (S), which employs traditional ReLU activations, and Leaky ReLU, which
replaces ReLUs with Leaky ReLUs using a scaling factor of α = 0.1. We observe that training with
Leaky ReLU results in smaller performance degradation from large initial updates, suggesting that
the non-linearities in the network might substantially impact the observed performance degradation."
STANDARD,0.3814262023217247,"A
The Detrimental Effects of Large Updates"
STANDARD,0.38308457711442784,"To investigate the effects of large updates at the beginning of training, we conducted controlled
experiments on a ResNet-20 model on CIFAR-10 [20] due to resource constraints. We controlled the
average angular update throughout training using the rotational optimizer variant of AdamW proposed
by Kosson et al. [19]. For the initial phase of training, 5 epochs, we either use a standard learning rate
of 0.05 or amplify it by a factor of 8 or 128. This results in very large updates, scaling the rotation
by either
√"
OR,0.38474295190713104,"8 or
√"
OR,0.3864013266998342,"128. For all experiments, we used a weight decay of 0.01, β1 = 0.9, β2 = 0.999,
5 epoch initial phase, and trained for 205 epochs in total with a cosine schedule unless otherwise
specified. The data was pre-processed by normalizing it with a mean of (0.4914, 0.4822, 0.4465) and
a standard deviation of (0.2023, 0.1994, 0.2010) and applying simple data augmentation techniques
as described by He et al. [11]. To run the experiment, we used the codebase from Wightman [40] and
extended the utilities from Kosson et al. [19]."
OR,0.3880597014925373,"As shown in fig. 6, the performance of standard training does not recover when large updates are used
at the beginning of training, even when the training time is extended to four times the normal duration
for ReLU networks. This suggest that large, controlled, initial updates can result in a permanent
performance degradation, similar to what we observe without warmup in advanced settings. The
impact is much smaller when replacing ReLUs with leaky ReLUs, suggesting that the non-linearities
in the network might substantially contribute to the performance degradation."
OR,0.38971807628524047,"In fig. 7 we measure the fraction of dead ReLUs directly across different settings and scaling factors.
We find that large initial updates do indeed lead to a large number of permanently dead units and
that the final accuracy suffers when this is the case. This effect can be mitigated by freezing the
biases at the beginning of training, as shown in the table in fig. 7. We also observe that replacing the
actual gradients with random gradients has a much smaller impact, suggesting that the direction of
the updates also matters for the degradation, not only their size."
OR,0.3913764510779436,"Interestingly, we did not find a connection to overfitting to a small number of samples at the beginning
of training. The performance of 92.1 can be recovered in this case. Additionally, we explored stable
rank measurements as a potential factor but did not find a significant relation, as detailed in fig. 8."
OR,0.39303482587064675,"B
Additional Mathematical and Technical Details"
OR,0.3946932006633499,"B.1
The magnitude of the Momentum Vector"
OR,0.3963515754560531,"Let’s assume a scalar gradient gt (e.g. for some coordinate) that is a random variable that is indepen-
dent across time and has a zero mean distribution that does not change across time, i.e. E[gt] = 0"
OR,0.39800995024875624,"Method
Scale 1
Scale 8
Scale 128"
OR,0.3996683250414594,"Standard
92.13±0.2 89.48±0.2 88.22±1.6"
OR,0.4013266998341625,"Standard frozen bias
92.30±0.3 92.08±0.3 92.30±0.2"
OR,0.40298507462686567,"Random
92.05±0.3 91.74±0.2 89.54±0.3"
OR,0.4046434494195688,"Random frozen bias
92.27±0.2 92.12±0.4 92.20±0.1"
OR,0.40630182421227196,"Leaky ReLU
92.16±0.3 91.48±0.3 91.82±0.4"
OR,0.4079601990049751,Leaky ReLU frozen bias 92.46±0.2 92.49±0.1 92.35±0.2
OR,0.4096185737976783,"0
200
400
600
800
1000 0.0 0.2 0.4 0.6"
OR,0.41127694859038144,inactive activations steps
OR,0.4129353233830846,"S-fb 1
S-fb 8
S-fb 128"
OR,0.41459369817578773,"R-fb 1
R-fb 8
R-fb 128"
OR,0.41625207296849087,"R 1
R 8
R 128"
OR,0.417910447761194,"S 1
S 8
S 128"
OR,0.41956882255389716,"Figure 7: Comparison of the performance (final test accuracy) and fraction of dead ReLUs (inactive
activations) across different settings. The learning rate in the initial phase of 5 epochs is scaled by a
factor of either 1, 8, or 128. Standard (S) denotes normal training, while Frozen Biases (fb) involves
freezing the biases at the onset of training. The Random (R) approach employs random gradient
directions at the start of training, and Leaky ReLU replaces the ReLUs in standard models with Leaky
ReLUs using a scaling factor of α = 0.1. We observe a notable correspondence between large initial
updates, higher ratios of dead ReLUs in ResNet-20, and performance degradation."
OR,0.42122719734660036,"0
200
400
600
800
1000 0.02 0.04 0.06"
OR,0.4228855721393035,mean stable rank steps
OR,0.42454394693200664,"1 S (fb)
8 S (fb)"
OR,0.4262023217247098,"128 S (fb)
1 R (fb)"
OR,0.42786069651741293,"8 R (fb)
128 R (fb)"
R,0.4295190713101161,"1 R
8 R"
R,0.4311774461028192,"128 R
1 S"
S,0.43283582089552236,"8 S
128 S"
S,0.43449419568822556,"Figure 8: Impact of varying update sizes during the warmup phase on the stable rank of a standard
ResNet-20. The learning rate in the initial phase of 5 epochs is scaled by a factor of either 1, 8, or
128. We evaluate the effects of across different training configurations: Standard (S) denotes normal
training; Frozen Biases (fb) involves freezing the biases at the onset of training; and Random (R)
employs random gradient directions at the start of training. The stable rank remains largely consistent
across these methods, except when using extremely large updates—specifically, scaling the learning
rate by a factor of 128 without freezing biases—which leads to noticeable variations in the rank."
S,0.4361525704809287,"and E[g2
t ] = σ2. For standard heavyball-momentum mt, with m0 = 0 and coefficient β (equivalent
to β1 for Adam) we have:"
S,0.43781094527363185,"E[m2
t] = E[(βmt−1 + (1 −β)gt)2]
(10) = E   "
S,0.439469320066335,"(1 −β) t−1
X"
S,0.44112769485903813,"i=0
βigt−i !2"
S,0.4427860696517413,"
(11) = E "
S,0.4444444444444444,"(1 −β)2
t−1
X"
S,0.4461028192371476,"i=0
β2ig2
t−i + (1 −β)2
t−1
X j=0 t−1
X"
S,0.44776119402985076,"k=0
k̸=j"
S,0.4494195688225539,β2t−j−kgt−jgg−k 
S,0.45107794361525705,"
(12)"
S,0.4527363184079602,"= (1 −β)2
t−1
X"
S,0.45439469320066334,"i=0
β2iE[g2
t−i] + (1 −β)2
t−1
X j=0 t−1
X"
S,0.4560530679933665,"k=0
k̸=j"
S,0.4577114427860697,"β2t−j−kE[gt−j]E[gg−k]
(13)"
S,0.4593698175787728,"= (1 −β)2
t−1
X"
S,0.46102819237147596,"i=0
β2iσ2 + 0
(14)"
S,0.4626865671641791,= (1 −β)2 1 −β2t
S,0.46434494195688225,"1 −β2 σ2
(15)"
S,0.4660033167495854,"= (1 −β)2
1 −β2t"
S,0.46766169154228854,"(1 −β)(1 + β)σ2
(16)"
S,0.4693200663349917,= (1 −β2t)1 −β
S,0.4709784411276949,"1 + β σ2
(17)"
S,0.472636815920398,"In the limit t →∞we have (1 −β2t) →1. We can derive the size of the second-moment vt in
AdamW in an analogous way, obtaining E[vt] = (1 −βt
2)σ2. For a random walk, the update size of
Adam is scaled in a similar way. Since the update size of Lion is fixed and does not depend on β, we"
S,0.47429519071310117,"scale the update size to match that of AdamW for a random walk in a steady state, i.e. by γ =
q"
S,0.4759535655058043,"1−β
1+β
as seen in algo. 2."
S,0.47761194029850745,"For Nesterov momentum, the update is modified to use:
ut = βmt + (1 −β)gt
(18)
= β (βmt−1 + (1 −β)gt) + (1 −β)gt
(19)"
S,0.4792703150912106,"= β2mt−1 + (1 −β)(1 + β)gt
(20)
Note that mt−1 and gt are independent and zero-mean, allowing us to use the previous result for:"
S,0.48092868988391374,"E[u2
t] = E
h 
β2mt−1 + (1 −β)(1 + β)gt
2i
(21)"
S,0.48258706467661694,"= β4E[m2
t−1] + (1 −β2)2E[g2
t ]
(22)"
S,0.4842454394693201,= β4(1 −β2t−2)1 −β
S,0.4859038142620232,"1 + β σ2 + (1 −β2)2σ2
(23)"
S,0.48756218905472637,"In the limit t →∞this gives the Nesterov scaling factor used in LionA (algo. 2) to ensure that the
update size corresponds to that of AdamW using an analogous Nesterov update."
S,0.4892205638474295,"Inverse bias correction for momentum. Adam uses a bias correction to attempt to fix the update
size over time. This scales early updates resulting in the contributions of the corresponding gradients
being amplified. The relative representation change for those samples is increased as a result, similar
to applying the same update multiple times. Removing the β1 bias correction from AdamW removes
this effect. LionA and LionAR similarly scale the update size, making it constant. We can counteract
this by changing our scaling factors to use the time varying expressions based on the derivations above.
Note however, that this assumed the gradients were uncorrelated so it only approximately undoes
the scaling effect for real values with arbitrary alignment of successive gradients. To summarize, the
inverse bias correction for momentum changes the momentum scaling factors (γ in algo. 3) to vary
over time:"
S,0.49087893864013266,"Nesterov:
γt = s"
S,0.4925373134328358,(1 −β2)2 + (1 −β2t−2)β4 1 −β
S,0.494195688225539,"1 + β
(24)"
S,0.49585406301824214,"Heavy-ball:
γt = s"
S,0.4975124378109453,(1 −β2t)1 −β
S,0.49917081260364843,"1 + β
(25)"
S,0.5008291873963516,"B.2
Properties of Scale Invariance"
S,0.5024875621890548,"Derivations for the gradient magnitude and curvature can be found in existing works, for example
Lyu et al. [25]. When a scale invariant weight is scaled by a factor c > 0, the gradient is scaled by
c−1 which scales the ratio of the gradient norm and weight norm, and therefore the angular updates,
by c−2. For normalized optimizers like Adam and Lion, where the update norm is not affected by the
gradient magnitude, this factor is decreased to c−1."
S,0.5041459369817579,"B.3
The Angular Update Size in LionAR"
S,0.5058043117744611,"The scaling factor for the angular update size in algo. 3 is adopted directly from the AdamW value
derived by Kosson et al. [19]. Since the Nesterov momentum does not change the total contribution
of each gradient it does not affect the equilibrium magnitude. The expected angular updates are
therefore scaled in the same way as the RMS update norm we derived in appx. B.1."
S,0.5074626865671642,"B.4
Relative Representation Change for Normalized Gradient Descent"
S,0.5091210613598673,"Property (P1):
For a dot product y = ⟨w, x⟩and loss L (xb) that depends on y, we have:"
S,0.5107794361525705,∂L (xb)
S,0.5124378109452736,"∂w
= ∂L (xb)"
S,0.5140961857379768,"∂y
∂y
∂w = ∂L (xb)"
S,0.5157545605306799,"∂y
xb
(26)"
S,0.5174129353233831,where ∂L (xb)
S,0.5190713101160862,"∂y
is a scalar, ensuring that gb := ∂L (xb)"
S,0.5207296849087893,"∂w
∥xb, assuming the vectors are not zero."
S,0.5223880597014925,"Lemma (L1):
Consider two independent random vectors a ∈RC and b ∈RC, whose elements
are independent and identically distributed (IID). If at least one of the vectors has a zero-mean
distribution, then the expected value of the squared inner product of a and b is given by:"
S,0.5240464344941956,"E[⟨a, b⟩2] = E[∥a∥2]E[∥b∥2]"
S,0.5257048092868989,"C
(27)"
S,0.527363184079602,"Proof: Let a = (a1, a2, . . . , aC) and b = (b1, b2, . . . , bC). The inner product ⟨a, b⟩is given by:"
S,0.5290215588723052,"⟨a, b⟩= C
X"
S,0.5306799336650083,"i=1
aibi."
S,0.5323383084577115,"We need to find E[⟨a, b⟩2]. Expanding the square of the inner product:"
S,0.5339966832504146,"⟨a, b⟩2 = C
X"
S,0.5356550580431177,"i=1
aibi !2 = C
X i=1 C
X"
S,0.5373134328358209,"j=1
aibiajbj."
S,0.538971807628524,"Taking the expectation, we get:"
S,0.5406301824212272,"E[⟨a, b⟩2] = E  
C
X i=1 C
X"
S,0.5422885572139303,"j=1
aibiajbj  = C
X i=1 C
X"
S,0.5439469320066335,"j=1
E[aibiajbj]."
S,0.5456053067993366,"Since a and b are independent and their elements are IID, we have:"
S,0.5472636815920398,E[aibiajbj] = E[aiaj]E[bibj].
S,0.548922056384743,Consider two cases:
S,0.5505804311774462,"1. When i = j:
E[aibiaibi] = E[a2
i ]E[b2
i ]."
S,0.5522388059701493,"2. When i ̸= j:
E[aibiajbj] = E[ai]E[bi]E[aj]E[bj].
Given that at least one of a or b has a zero-mean distribution, say a without loss of generality, we
have E[ai] = 0. Thus:
E[aibiajbj] = 0."
S,0.5538971807628524,"So, the expectation simplifies to:"
S,0.5555555555555556,"E[⟨a, b⟩2] = C
X"
S,0.5572139303482587,"i=1
E[a2
i ]E[b2
i ]."
S,0.5588723051409619,"Since ai and bi are IID, we have:"
S,0.560530679933665,"E[a2
i ] = E[a2
1]
and
E[b2
i ] = E[b2
1]."
S,0.5621890547263682,"Therefore:
E[⟨a, b⟩2] = CE[a2
1]E[b2
1]."
S,0.5638474295190713,Recognizing that:
S,0.5655058043117744,"E[∥a∥2] = E "" C
X"
S,0.5671641791044776,"i=1
a2
i #"
S,0.5688225538971807,"= CE[a2
1],"
S,0.5704809286898839,"E[∥b∥2] = E "" C
X"
S,0.572139303482587,"i=1
b2
i #"
S,0.5737976782752903,"= CE[b2
1],"
S,0.5754560530679934,we have:
S,0.5771144278606966,"E[a2
1] = E[∥a∥2]"
S,0.5787728026533997,"C
and
E[b2
1] = E[∥b∥2] C
. Thus:"
S,0.5804311774461028,"E[⟨a, b⟩2] = C
E[∥a∥2] C"
S,0.582089552238806, E[∥b∥2] C
S,0.5837479270315091,"
= E[∥a∥2]E[∥b∥2] C
."
S,0.5854063018242123,This completes the proof.
S,0.5870646766169154,"Assumption (A1):
We assume the following vector pairs satisfy L1: (xi, ˜gb) when i ̸= b, (¯g, ˜gb)
and (w, xb)."
S,0.5887230514096186,"Vector pairs of the type (xi, ˜gb) and (¯g, ˜gb) should be independent and ˜gb has a zero mean dis-
tribution. However, the elements of each vector are not necessarily IID. For (w, xb), this is an
even stronger assumption. Generally, neither w nor xb is guaranteed to be IID or zero mean, and
their independence later in training does not necessarily hold. Applying weight standardization to
w or batch normalization to x would suffice to make their mean zero. Overall, this assumption
can be viewed as a simplifying approximation to obtain reasonable predictions without additional
information about these vectors. Everett et al. [5] explore the behavior of ⟨w, xb⟩throughout training
and find that it is more complicated than assumed here. This will lead to additional factors that may
affect the RRC but we do not attempt to analyze."
S,0.5903814262023217,"Deriving the Relative Representation Change:
Applying L1 directly gives us the original expected
square output :"
S,0.5920398009950248,"E[y2
b] = E[⟨w, xb⟩2] = ∥w∥2E[∥xb∥2]"
S,0.593698175787728,"C
(28)"
S,0.5953565505804311,For the expected square representation change we get:
S,0.5970149253731343,"E[(∆yb)2]
(29)"
S,0.5986733001658375,"= E[⟨−ηg/
p"
S,0.6003316749585407,"E[∥g∥2], xb⟩2]
(30) = η2"
S,0.6019900497512438,"B2
1
E[∥g∥2]E   B
X"
S,0.603648424543947,"i=1
⟨gi, xb⟩ !2"
S,0.6053067993366501,"
(31) = η2"
S,0.6069651741293532,"B2
1
E[∥g∥2]E   "
S,0.6086235489220564,"sign(⟨xb, gb⟩)∥gb∥∥xb∥+
X"
S,0.6102819237147595,"i̸=B
⟨gi, xb⟩   2"
S,0.6119402985074627,"
(32) = η2"
S,0.6135986733001658,"B2
1
E[∥g∥2]E   "
S,0.615257048092869,"sign(⟨xb, gb⟩)∥gb∥∥xb∥+ (B −1)⟨¯g, xb⟩+
X"
S,0.6169154228855721,"i̸=b
⟨˜gi, xb⟩   2"
S,0.6185737976782753,"
(33) (34)"
S,0.6202321724709784,"where we have used the definitions from eq. (7) and D1. Using property P1, we can write:"
S,0.6218905472636815,"⟨¯g, xb⟩= *"
S,0.6235489220563848,"¯g,
sign(⟨xb, gb⟩)∥xb∥"
S,0.6252072968490879,∥gb∥· (¯g + ˜gb) + (35)
S,0.6268656716417911,"= sign(⟨xb, gb⟩)∥xb∥"
S,0.6285240464344942,"∥gb∥(∥¯g∥2 + ⟨¯g, ˜gb⟩)
(36)"
S,0.6301824212271974,Plugging this into the previous expression yields E[(∆yb)2] = η2
S,0.6318407960199005,"B2
1
E[∥g∥2]E   "
S,0.6334991708126037,"sign(⟨xb, gb⟩)

∥gb∥∥xb∥+ (B −1)∥xb∥"
S,0.6351575456053068,"∥gb∥(∥¯g∥2 + ⟨¯g, ˜gb⟩)

+
X"
S,0.6368159203980099,"i̸=b
⟨˜gi, xb⟩   2  (37)"
S,0.6384742951907131,"Squaring the expression results in various cross but all remaining dot products except the sign one are
zero in expectation (due to the noise ˜g) and independent from each other. The cross terms involving
these thus all disappear under the expectation. We apply Lemma L1 to their squares and approximate
the expected norms of xb and gb as being independent. This gives E[(∆yb)2] = η2"
S,0.6401326699834162,"B2
E[∥xb∥2]"
S,0.6417910447761194,E[∥g∥2]
S,0.6434494195688225,"
E[∥gb∥2] + (B −1)2∥¯g∥2"
S,0.6451077943615257,E[∥g∥2]
S,0.6467661691542289,"
∥¯g∥2 + E[∥˜gb∥2] C"
S,0.648424543946932,"
(38)"
S,0.6500829187396352,+2(B −1)∥¯g∥2 + B −1
S,0.6517412935323383,"C
E[∥˜gi∥2]

(39)"
S,0.6533996683250415,We can compute the expected magnitude of the batch gradient as:
S,0.6550580431177446,"E[∥g∥2] = E[∥1 B B
X"
S,0.6567164179104478,"i=1
(¯g + ˜gi)∥2] = E[∥(¯g + 1 B B
X"
S,0.6583747927031509,"i=1
˜gi)∥2] = ∥¯g∥2 + 1"
S,0.6600331674958541,"B E[∥gi∥2]
(40)"
S,0.6616915422885572,and similarly E[∥gb∥2] = ∥¯g∥2 + E[∥˜gb∥2]. Using these facts we can further write E[(∆yb)2] = η2
S,0.6633499170812603,"B2
E[∥xb∥2]
E[∥¯g∥2] + 1"
S,0.6650082918739635,B E[∥gi∥2]
S,0.6666666666666666,"
∥¯g∥2 + E[∥˜gb∥2] +
(B −1)2∥¯g∥2"
S,0.6683250414593698,∥¯g∥2 + E[∥˜gb∥2]
S,0.6699834162520729,"
∥¯g∥2 + E[∥˜gb∥2] C "
S,0.6716417910447762,+2(B −1)∥¯g∥2 + B −1
S,0.6733001658374793,"C
E[∥˜gi∥2]

(41)"
S,0.6749585406301825,"Combining this with the previous expression for E[y2
b] and the definition (D2) of the signal-to-noise
ratio φ := E[∥¯g∥2]/E[∥˜gb∥2] we obtain the expression in the main body:"
S,0.6766169154228856,E[(∆yb)2]
S,0.6782752902155887,"E[y2
b]
=
η2C
B2∥w∥2
1
E[∥g∥2] "
S,0.6799336650082919,E[∥gb∥2] + B −1
S,0.681592039800995,"C
E[∥˜gi∥2]"
S,0.6832504145936982,+ (B−1)2
S,0.6849087893864013,E[∥gb∥2]
S,0.6865671641791045,"
∥¯g∥4 + ∥¯g∥2E[∥˜gb∥2] C"
S,0.6882255389718076,"
+ 2(B−1)∥¯g∥2 ! (42)"
S,0.6898839137645107,"=
η2C
B2∥w∥2
1
φ + 1 B "
S,0.6915422885572139,(φ+1) + B−1
S,0.693200663349917,"C
+
(B−1)2φ φ + 1"
S,0.6948590381426202,"
φ + 1 C"
S,0.6965174129353234,"
+ 2(B−1)φ ! (43)"
S,0.6981757877280266,"B.5
Estimating the Signal-to-Noise Ratio"
S,0.6998341625207297,"We use accumulation over the microbatches to estimate the SNR at a given time. Let’s assume we
have A microbatches of size M each, with the average gradient of a microbatch denoted gm and the
average gradient of the whole batch denoted g = 1 A
P m gm."
S,0.7014925373134329,"We estimate the variance of the norm of a single gradient example, i.e. the noise power as:"
S,0.703150912106136,"PN =
A
A −1 · M · 1⊤
 
1
A X"
S,0.7048092868988391,"m
g2
m −g2
! (44)"
S,0.7064676616915423,The signal power is estimated as:
S,0.7081260364842454,"PS = 1⊤g2 −
1
AM PN
(45)"
S,0.7097844112769486,"Our SNR estimate is then:
φ = PS/PN
(46)"
S,0.7114427860696517,"B.6
RRC Correction Factor"
S,0.7131011608623549,"The RRC correction is done based on eq. (9) and the SNR estimation eq. (46). We assume the learning
rate was originally scaled with the square root of the batch size, which is derived for an SNR of zero,
and downscale the step size to compensate for the measured SNR and batch size. We define:"
S,0.714759535655058,"ρ =
1
B(1 + φ) "
S,0.7164179104477612,(φ+1) + B−1
S,0.7180762852404643,"C
+
(B−1)2φ φ + 1"
S,0.7197346600331676,"
φ + 1 C"
S,0.7213930348258707,"
+ 2(B−1)φ
! (47)"
S,0.7230514096185738,"For numerical purposes, we clamp 1 ≤ρ ≤B which corresponds to φ = 0 and φ = ∞for a large
C →∞. The update scaling factor is the square root of an EMA of the inverse of this quantity.
We use the same coefficient as for the momentum and compute this for the matrix of each linear
layer independently. This form for the scaling factor is somewhat arbitrary, complicated by the fact
that Lion-like algorithms fix the step size exactly, so scaling the gradient at each step size can not
change the magnitude of the update. For Adam or SGD like algorithms we could scale the gradient
contributions directly instead of scaling the update size."
S,0.724709784411277,"B.7
Run-to-run Variance / Uncertainty Estimation"
S,0.7263681592039801,"We do not quantify the uncertainty for every GPT2 configuration in our sweeps. This would require
significantly more compute and our estimates of the uncertainty for select points indicate that this
would not qualitatively change our results. For the baseline AdamW run the run-to-run differences in
the validation loss over different seeds are around 0.05. However, the relative ranking of different
runs remained the same."
S,0.7280265339966833,"B.8
Computational Requirements"
S,0.7296849087893864,"Our experiments are performed on A100 GPUs with either 40GB or 80GB of RAM. One training run
for our GPT2 setup takes around 4h, running on a single GPU. Reproducing the GPT2 experiments
reported in the main body should take on the order of 1000 GPU hours. Including our preliminary
experiments brings this up to around 3x this amount."
S,0.7313432835820896,"C
Additional Experiments"
S,0.7330016583747927,"C.1
Comparison with RAdam"
S,0.7346600331674958,"RAdamW combines the variance reduction technique of Liu et al. [23] with the decoupled weight
decay of Loshchilov and Hutter [24]. Since it is a well known technique for reducing the need for
warmup it serves as a good comparison and contextualization of our work. Figure 9 shows that
while RAdamW outperforms the AdamW baseline without warmup, it is unable to match longer
warmups. Ma and Yarats [26] suggests that RAdamW is approximately equivalent to 4 steps of
SGDM followed by AdamW with a special type of built-in warmup with an effective length of around
2/(1 −β2) = 40, which is likely too short in our setting. For comparison the 2% warmup shown
corresponds to 100 steps but is too short for optimal performance."
S,0.736318407960199,"The analysis of Liu et al. [23] is based on the idea that early in training the second-moment estimates
are not accurate (noisy) and can therefore not be trusted to scale the update properly. This could in
turn contribute to the need for warmup, although Ma and Yarats [26] question this interpretation. We
first note that without momentum, perfect estimates of the second moment at the current time step
would control the expected ℓ2-norm of the update. This relates our approach of looking at the update
size to the adaptive learning rate view of Liu et al. [23]. Secondly, we note that counteracting noisy
estimates of the second moment can not be the sole reason warmup is beneficial. This is supported
by the fact that both SGD and Lion empirically need warmup in various settings but do not use the
second moment at all, indicating there are additional factors that contribute to the need for warmup."
S,0.7379767827529021,"0
2500
5000
Iteration 0 1"
S,0.7396351575456053,Learning Rate Schedule
S,0.7412935323383084,"0%
2%
5%
10%
20%"
S,0.7429519071310116,"10
3
10
2"
S,0.7446102819237148,Learning Rate 3.1 3.2 3.3 3.4 3.5 3.6
S,0.746268656716418,Final Validation Loss
S,0.7479270315091211,"AdamW
RAdamW"
S,0.7495854063018242,"0
2500
5000
Iteration 0 1"
S,0.7512437810945274,Learning Rate Schedule
S,0.7529021558872305,"0%
2%
5%
10%
20%"
S,0.7545605306799337,"10
3
10
2"
S,0.7562189054726368,Learning Rate 3.1 3.2 3.3 3.4 3.5
S,0.75787728026534,Final Validation Loss
S,0.7595356550580431,"AdamW
RAdamW"
S,0.7611940298507462,"Figure 9: Comparison of the AdamW baseline with a cosine schedule and RAdamW. Panel 1: Cosine
schedules with different warmup lengths. Note how the warmup shifts the curve, affecting the whole
schedule including the decay portion. Panel 2: The final validation loss for GPT2-124M training
on OpenWebText using the cosine schedules. Note that RAdamW helps but does not eliminate the
need for warmup. Panel 3: The original trapezoidal schedules used in our experiments. Panel 4:
Trapezoidal GPT2-124M OpenWebText results. The RAdamW results are similar to those in panel 2."
S,0.7628524046434494,"C.2
Model & Dataset Ablations"
S,0.7645107794361525,"In this section we repeat some of our main experiments, varying the dataset and model architecture."
S,0.7661691542288557,"10
3
10
2"
S,0.7678275290215588,Learning Rate 3.0 3.1 3.2 3.3
S,0.7694859038142621,Final Validation Loss LionA
S,0.7711442786069652,"0%
5%
10%"
S,0.7728026533996684,"10
3
10
2"
S,0.7744610281923715,Learning Rate 3.0 3.1 3.2 3.3
S,0.7761194029850746,Final Validation Loss
S,0.7777777777777778,LionAR
S,0.7794361525704809,"0%
5%
10%"
S,0.7810945273631841,"10
3
10
2"
S,0.7827529021558872,Learning Rate 3.0 3.1 3.2 3.3
S,0.7844112769485904,Final Validation Loss
S,0.7860696517412935,LionAR =0.98 +N +MC
S,0.7877280265339967,"0%
5%
10%"
S,0.7893864013266998,"10
3
10
2"
S,0.7910447761194029,Learning Rate 3.0 3.1 3.2 3.3
S,0.7927031509121062,Final Validation Loss
S,0.7943615257048093,"LionAR + RRC comp 0%
5%"
S,0.7960199004975125,"Figure 10: Dataset ablation study, GPT2-124M on SlimPajama [33]. Four update control approaches
are shown (see titles). The overall results are similar to those for OpenWebText in the manuscript."
S,0.7976782752902156,"Figure 10 shows the effects of changing the dataset used in our experiments from OpenWebText [8]
to SlimPajama [33]. Overall the results are similar as before, when controlling the ℓ2-norm via LionA
warmup is still beneficial, controlling the angular updates via LionAR decreases the gap significantly.
The higher momentum LionAR with Nesterov momentum and our momentum correction eliminates
the gap fully. The RRC also seems to eliminate the benefit of warmup but still has the same practical
limitations as we describe in §6."
S,0.7993366500829188,"10
3
10
2"
S,0.8009950248756219,Learning Rate 3.1 3.2 3.3 3.4 3.5
S,0.802653399668325,Final Validation Loss LionA
S,0.8043117744610282,"0%
5%
10%"
S,0.8059701492537313,"10
3
10
2"
S,0.8076285240464345,Learning Rate 3.1 3.2 3.3 3.4 3.5
S,0.8092868988391376,Final Validation Loss
S,0.8109452736318408,LionAR
S,0.8126036484245439,"0%
5%
10%"
S,0.814262023217247,"10
3
10
2"
S,0.8159203980099502,Learning Rate 3.1 3.2 3.3 3.4 3.5
S,0.8175787728026535,Final Validation Loss
S,0.8192371475953566,LionAR =0.98 +N +MC
S,0.8208955223880597,"0%
5%
10%"
S,0.8225538971807629,"Figure 11: Architecture ablation, Llama2-124M on OpenWebText. Here LionAR is already sufficient
to eliminate the need for warmup, no additional RRC compensation is needed. This could be due to
the critical batch size being larger (for unclear reasons)."
S,0.824212271973466,"Figure 11 shows the effects of changing the architecture from GPT2 to a Llama style [36] while
keeping the dataset and parameter count (124m) the same. This change consists of using SwiGLU
activations, RoPE embeddings and RMSNorm. In this case LionAR is able to fully eliminate the
need for warmup without any additional tricks like the RRC compensation or momentum corrections.
Based on our analysis these additional tricks are likely only needed when the critical batch size is
very small initially. We expect that using larger batch sizes could necessitate these additional tricks
for Llama, but do not explore this further here. 10
3"
S,0.8258706467661692,Learning Rate 2.8 2.9 3.0 3.1
S,0.8275290215588723,"Final Validation Loss LionA 0%
5%"
S,0.8291873963515755,"10
3
10
2
10
1"
S,0.8308457711442786,Learning Rate 2.8 2.9 3.0 3.1
S,0.8325041459369817,Final Validation Loss
S,0.8341625207296849,"LionAR 0%
5%"
S,0.835820895522388,"10
3
10
2
10
1"
S,0.8374792703150912,Learning Rate 2.8 2.9 3.0 3.1
S,0.8391376451077943,Final Validation Loss
S,0.8407960199004975,"LionAR =0.98 +N +MC 0%
5%"
S,0.8424543946932007,"Figure 12: Larger llama2-209M training on SlimPajama. This experiment changes the architecture,
dataset, model size, and training length (proportional to the model size). In this case LionAR suffices
on its own again, no additional RRC correction needed as in the smaller llama experiments."
S,0.8441127694859039,"Figure 12 uses a larger Llama model with twice the depth. It also trains for twice as many iterations
to keep the ratio of tokens to parameters similar. The overall results resemble those of the smaller
llama experiments in fig. 11."
S,0.845771144278607,NeurIPS Paper Checklist
CLAIMS,0.8474295190713101,1. Claims
CLAIMS,0.8490878938640133,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?"
CLAIMS,0.8507462686567164,Answer: [Yes]
CLAIMS,0.8524046434494196,"Justification: We have aimed to write our abstract and introduction to accurately reflect the
paper’s contributions and scope."
CLAIMS,0.8540630182421227,Guidelines:
CLAIMS,0.8557213930348259,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper."
LIMITATIONS,0.857379767827529,2. Limitations
LIMITATIONS,0.8590381426202321,Question: Does the paper discuss the limitations of the work performed by the authors?
LIMITATIONS,0.8606965174129353,"Answer: [Yes]
Justification: We include a limitation section §9 which discusses the main overarching
limitations from our perspective."
LIMITATIONS,0.8623548922056384,Guidelines:
LIMITATIONS,0.8640132669983416,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations."
THEORY ASSUMPTIONS AND PROOFS,0.8656716417910447,3. Theory Assumptions and Proofs
THEORY ASSUMPTIONS AND PROOFS,0.867330016583748,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?"
THEORY ASSUMPTIONS AND PROOFS,0.8689883913764511,Answer: [Yes]
THEORY ASSUMPTIONS AND PROOFS,0.8706467661691543,"Justification: Our work is mostly empirical, aside from the analysis of the relative represen-
tation change where we state the assumptions upfront, with additional details deligated to
the appendix."
THEORY ASSUMPTIONS AND PROOFS,0.8723051409618574,Guidelines:
THEORY ASSUMPTIONS AND PROOFS,0.8739635157545605,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8756218905472637,4. Experimental Result Reproducibility
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8772802653399668,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.87893864013267,"Answer: [Yes]
Justification: We provide the main hyperparameters and other aspects of our training. We
are not aware of any missing details that would significantly hinder reproducibility."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8805970149253731,Guidelines:
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8822553897180763,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results."
OPEN ACCESS TO DATA AND CODE,0.8839137645107794,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.8855721393034826,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
OPEN ACCESS TO DATA AND CODE,0.8872305140961857,Answer: [No]
OPEN ACCESS TO DATA AND CODE,0.8888888888888888,"Justification: The datasets we use are freely available online but we have not released our
code."
OPEN ACCESS TO DATA AND CODE,0.8905472636815921,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.8922056384742952,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted."
OPEN ACCESS TO DATA AND CODE,0.8938640132669984,6. Experimental Setting/Details
OPEN ACCESS TO DATA AND CODE,0.8955223880597015,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
OPEN ACCESS TO DATA AND CODE,0.8971807628524047,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.8988391376451078,Justification: We have attempted include all relevant information.
OPEN ACCESS TO DATA AND CODE,0.900497512437811,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.9021558872305141,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9038142620232172,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9054726368159204,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9071310116086235,Answer: [No]
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9087893864013267,"Justification: Our main results are qualitative and computationally expensive for us, so we
do not estimate the variance for each point in our sweeps. We give an estimate based on
select points in."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9104477611940298,Guidelines:
EXPERIMENT STATISTICAL SIGNIFICANCE,0.912106135986733,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9137645107794361,"• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text."
EXPERIMENTS COMPUTE RESOURCES,0.9154228855721394,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.9170812603648425,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.9187396351575456,Answer: [Yes]
EXPERIMENTS COMPUTE RESOURCES,0.9203980099502488,Justification: We do this for our main experiments in appx. B.8
EXPERIMENTS COMPUTE RESOURCES,0.9220563847429519,Guidelines:
EXPERIMENTS COMPUTE RESOURCES,0.9237147595356551,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper)."
CODE OF ETHICS,0.9253731343283582,9. Code Of Ethics
CODE OF ETHICS,0.9270315091210614,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?"
CODE OF ETHICS,0.9286898839137645,Answer: [Yes]
CODE OF ETHICS,0.9303482587064676,Justification: We read it and believe we fully conform to it.
CODE OF ETHICS,0.9320066334991708,Guidelines:
CODE OF ETHICS,0.9336650082918739,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction)."
BROADER IMPACTS,0.9353233830845771,10. Broader Impacts
BROADER IMPACTS,0.9369817578772802,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?"
BROADER IMPACTS,0.9386401326699834,Answer: [NA]
BROADER IMPACTS,0.9402985074626866,"Justification: This paper is focused on understanding deep learning in general, and does not
have an specific societal impacts beyond those of the field overall."
BROADER IMPACTS,0.9419568822553898,Guidelines:
BROADER IMPACTS,0.9436152570480929,• The answer NA means that there is no societal impact of the work performed.
BROADER IMPACTS,0.945273631840796,"• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML)."
SAFEGUARDS,0.9469320066334992,11. Safeguards
SAFEGUARDS,0.9485903814262023,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?"
SAFEGUARDS,0.9502487562189055,Answer: [NA]
SAFEGUARDS,0.9519071310116086,Justification: We don’t realease anything that falls under this category.
SAFEGUARDS,0.9535655058043118,Guidelines:
SAFEGUARDS,0.9552238805970149,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort."
LICENSES FOR EXISTING ASSETS,0.956882255389718,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.9585406301824212,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
LICENSES FOR EXISTING ASSETS,0.9601990049751243,Answer: [Yes]
LICENSES FOR EXISTING ASSETS,0.9618573797678275,Justification: We do this to the best of our ability.
LICENSES FOR EXISTING ASSETS,0.9635157545605307,Guidelines:
LICENSES FOR EXISTING ASSETS,0.9651741293532339,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided."
LICENSES FOR EXISTING ASSETS,0.966832504145937,"• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators."
NEW ASSETS,0.9684908789386402,13. New Assets
NEW ASSETS,0.9701492537313433,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?"
NEW ASSETS,0.9718076285240465,Answer: [NA]
NEW ASSETS,0.9734660033167496,Justification: No new assets introduced
NEW ASSETS,0.9751243781094527,Guidelines:
NEW ASSETS,0.9767827529021559,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.978441127694859,14. Crowdsourcing and Research with Human Subjects
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9800995024875622,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9817578772802653,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9834162520729685,Justification: No crowdsourcing / human subjects used
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9850746268656716,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9867330016583747,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.988391376451078,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9900497512437811,"Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9917081260364843,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9933665008291874,Justification: No crowdsourcing / human subjects used
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9950248756218906,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9966832504145937,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9983416252072969,"• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
