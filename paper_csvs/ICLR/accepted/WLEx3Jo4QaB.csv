Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003125,"Given the prevalence of large-scale graphs in real-world applications, the stor-
age and time for training neural models have raised increasing concerns.
To
alleviate the concerns, we propose and study the problem of graph condensa-
tion for graph neural networks (GNNs). SpeciÔ¨Åcally, we aim to condense the
large, original graph into a small, synthetic and highly-informative graph, such
that GNNs trained on the small graph and large graph have comparable perfor-
mance. We approach the condensation problem by imitating the GNN training
trajectory on the original graph through the optimization of a gradient match-
ing loss and design a strategy to condense node features and structural infor-
mation simultaneously. Extensive experiments have demonstrated the effective-
ness of the proposed framework in condensing different graph datasets into in-
formative smaller graphs. In particular, we are able to approximate the orig-
inal test accuracy by 95.3% on Reddit, 99.8% on Flickr and 99.0% on Cite-
seer, while reducing their graph size by more than 99.9%, and the condensed
graphs can be used to train various GNN architectures.
Code is released at
https://github.com/ChandlerBang/GCond."
INTRODUCTION,0.00625,"1
INTRODUCTION"
INTRODUCTION,0.009375,"Many real-world data can be naturally represented as graphs such as social networks, chemical
molecules, transportation networks, and recommender systems (Battaglia et al., 2018; Wu et al.,
2019b; Zhou et al., 2018). As a generalization of deep neural networks for graph-structured data,
graph neural networks (GNNs) have achieved great success in capturing the abundant information
residing in graphs and tackle various graph-related applications (Wu et al., 2019b; Zhou et al., 2018)."
INTRODUCTION,0.0125,"However, the prevalence of large-scale graphs in real-world scenarios, often on the scale of millions
of nodes and edges, poses signiÔ¨Åcant computational challenges for training GNNs. More dramat-
ically, the computational cost continues to increase when we need to retrain the models multiple
times, e.g., under incremental learning settings, hyperparameter and neural architecture search. To
address this challenge, a natural idea is to properly simplify, or reduce the graph so that we can
not only speed up graph algorithms (including GNNs) but also facilitate storage, visualization and
retrieval for associated graph data analysis tasks."
INTRODUCTION,0.015625,"There are two main strategies to simplify graphs: graph sparsiÔ¨Åcation (Peleg & Sch¬®affer, 1989;
Spielman & Teng, 2011) and graph coarsening (Loukas & Vandergheynst, 2018; Loukas, 2019) .
Graph sparsiÔ¨Åcation approximates a graph with a sparse graph by reducing the number of edges,
while graph coarsening directly reduces the number of nodes by replacing the original node set with
its subset. However, these methods have some shortcomings: (1) sparsiÔ¨Åcation becomes much less
promising in simplifying graphs when nodes are also associated with attributes as sparsiÔ¨Åcation does
not reduce the node attributes; (2) the goal of sparsiÔ¨Åcation and coarsening is to preserve some graph"
INTRODUCTION,0.01875,‚àóWork done while author was on internship at Snap Inc.
INTRODUCTION,0.021875,Published as a conference paper at ICLR 2022
INTRODUCTION,0.025,"Test accuracies
GCN: 89.4%
SGC: 89.6%
APPNP: 87.8%
GraphSAGE: 89.1%"
INTRODUCTION,0.028125,"(ùë®!,ùëø!, ùíÄ‚Ä≤)"
INTRODUCTION,0.03125,Condense
INTRODUCTION,0.034375,"(ùë®,ùëø,ùíÄ)"
INTRODUCTION,0.0375,"Test accuracies
GCN: 93.9%
SGC: 93.5%
APPNP: 94.3%
GraphSAGE: 93.0%"
INTRODUCTION,0.040625,"153,932 training nodes
154 training nodes
Figure 1: We study the graph condensation problem, which seeks to learn a small, synthetic graph,
features and labels {A‚Ä≤, X‚Ä≤, Y‚Ä≤} from a large, original dataset {A, X, Y}, which can be used to train
GNN models that generalize comparably to the original. Shown: An illustration of our proposed
GCOND graph condensation approach‚Äôs empirical performance, which exhibits 95.3% of original
graph test performance with 99.9% data reduction."
INTRODUCTION,0.04375,"properties such as principle eigenvalues (Loukas & Vandergheynst, 2018) that could be not optimal
for the downstream performance of GNNs. In this work, we ask if it is possible to signiÔ¨Åcantly
reduce the graph size while providing sufÔ¨Åcient information to well train GNN models."
INTRODUCTION,0.046875,"Motivated by dataset distillation (Wang et al., 2018) and dataset condensation (Zhao et al., 2021)
which generate a small set of images to train deep neural networks on the downstream task, we
aim to condense a given graph through learning a synthetic graph structure and node attributes.
Correspondingly, we propose the task of graph condensation1. It aims to minimize the performance
gap between GNN models trained on a synthetic, simpliÔ¨Åed graph and the original training graph. In
this work, we focus on attributed graphs and the node classiÔ¨Åcation task. We show that we are able
to reduce the number of graph nodes to as low as 0.1% while training various GNN architectures to
reach surprisingly good performance on the synthetic graph. For example, in Figure 1, we condense
the graph of the Reddit dataset with 153,932 training nodes into only 154 synthetic nodes together
with their connections. In essence, we face two challenges for graph condensation: (1) how to
formulate the objective for graph condensation tractable for learning; and (2) how to parameterize
the to-be-learned node features and graph structure. To address the above challenges, we adapt
the gradient matching scheme in (Zhao et al., 2021) and match the gradients of GNN parameters
w.r.t. the condensed graph and original graph. In this way, the GNN trained on condensed graph
can mimic the training trajectory of that on real data. Further, we carefully design the strategy for
parametrizations for the condensed graph. In particular, we introduce the strategy of parameterizing
the condensed features as free parameters and model the synthetic graph structure as a function of
features, which takes advantage of the implicit relationship between structure and node features,
consumes less number of parameters and offers better performance."
INTRODUCTION,0.05,Our contributions can be summarized as follows:
INTRODUCTION,0.053125,"1. We make the Ô¨Årst attempt to condense a large-real graph into a small-synthetic graph, such that
the GNN models trained on the large graph and small graph have comparable performance. We
introduce a proposed framework for graph condensation (GCOND) which parameterizes the con-
densed graph structure as a function of condensed node features, and leverages a gradient match-
ing loss as the condensation objective.
2. Through extensive experimentation, we show that GCOND is able to condense different graph
datasets and achieve comparable performance to their larger counterparts. For instance, GCOND
approximates the original test accuracy by 95.3% on Reddit, 99.8% on Flickr and 99.0% on Cite-
seer, while reducing their graph size by more than 99.9%. Our approach consistently outperforms
coarsening, coreset and dataset condensation baselines.
3. We show that the condensed graphs can generalize well to different GNN test models. Addition-
ally, we observed reliable correlation of performances between condensed dataset training and
whole-dataset training in the neural architecture search (NAS) experiments."
RELATED WORK,0.05625,"2
RELATED WORK"
RELATED WORK,0.059375,"Dataset Distillation & Condensation. Dataset distillation (DD) (Wang et al., 2018; Bohdal et al.,
2020; Nguyen et al., 2021) aims to distill knowledge of a large training dataset into a small synthetic"
RELATED WORK,0.0625,1We aim to condense both graph structure and node attributes. A formal deÔ¨Ånition is given in Section 3.
RELATED WORK,0.065625,Published as a conference paper at ICLR 2022
RELATED WORK,0.06875,"dataset, such that a model trained on the synthetic set is able to obtain the comparable performance
to that of a model trained on the original dataset. To improve the efÔ¨Åciency of DD, dataset conden-
sation (DC) (Zhao et al., 2021; Zhao & Bilen, 2021) is proposed to learn the small synthetic dataset
by matching the gradients of the network parameters w.r.t. large-real and small-synthetic training
data. However, these methods are designed exclusively for image data and are not applicable to
non-Euclidean graph-structured data where samples (nodes) are interdependent. In this work, we
generalize the problem of dataset condensation to graph domain and we seek to jointly learn the
synthetic node features as well as graph structure. Additionally, our work relates to coreset meth-
ods (Welling, 2009; Sener & Savarese, 2018; RebufÔ¨Ået al., 2017), which seek to Ô¨Ånd informative
samples from the original datasets. However, they rely on the presence of representative samples,
and tend to give suboptimal performance."
RELATED WORK,0.071875,"Graph SparsiÔ¨Åcation & Coarsening. Graph sparsiÔ¨Åcation and coarsening are two means of reduc-
ing the size of a graph. SparsiÔ¨Åcation reduces the number of edges while approximating pairwise
distances (Peleg & Sch¬®affer, 1989), cuts (Karger, 1999) or eigenvalues (Spielman & Teng, 2011)
while coarsening reduces the number of nodes with similar constraints (Loukas & Vandergheynst,
2018; Loukas, 2019; Deng et al., 2020), typically by grouping original nodes into super-nodes, and
deÔ¨Åning their connections. Cai et al. (2021) proposes a GNN-based framework to learn these con-
nections to improve coarsening quality. Huang et al. (2021b) adopts coarsening as a preprocessing
method to help scale up GNNs. Graph condensation also aims to reduce the number of nodes, but
aims to learn synthetic nodes and connections in a supervised way, rather than unsupervised group-
ing as in these prior works. Graph pooling is also related to our work, but it targets at improving
graph-level representation learning (see Appendix D)."
RELATED WORK,0.075,"Graph Neural Networks. Graph neural networks (GNNs) are a modern way to capture the intuition
that inferences for individual samples (nodes) can be enhanced by utilizing graph-based information
from neighboring nodes (Kipf & Welling, 2017; Hamilton et al., 2017; Klicpera et al., 2019; Velick-
ovic et al., 2018; Wu et al., 2019b;a; Liu et al., 2020; 2021; You et al., 2021; Zhou et al., 2021; Zhao
et al., 2022). Due to their prevalence, various real-world applications have been tremendously fa-
cilitated including recommender systems (Ying et al., 2018a; Fan et al., 2019), computer vision (Li
et al., 2019) and drug discovery (Duvenaud et al., 2015)."
RELATED WORK,0.078125,"Graph Structure Learning. Our work is also related to graph structure learning, which explores
methods to learn graphs from data. One line of work (Dong et al., 2016; Egilmez et al., 2017) learns
graphs under certain structural constraints (e.g. sparsity) based on graph signal processing. Recent
efforts aim to learn graphs by leveraging GNNs (Franceschi et al., 2019; Jin et al., 2020; Chen et al.,
2020). However, these methods are incapable of learning graphs with smaller size, and are thus not
applicable for graph condensation."
METHODOLOGY,0.08125,"3
METHODOLOGY"
METHODOLOGY,0.084375,"In this section, we present our proposed graph condensation framework, GCOND. Consider that we
have a graph dataset T = {A, X, Y}, where A ‚ààRN√óN is the adjacency matrix, N is the number
of nodes, X ‚ààRN√ód is the d-dimensional node feature matrix and Y ‚àà{0, . . . , C ‚àí1}N denotes
the node labels over C classes. Graph condensation aims to learn a small, synthetic graph dataset
S = {A‚Ä≤, X‚Ä≤, Y‚Ä≤} with A‚Ä≤ ‚ààRN ‚Ä≤√óN‚Ä≤, X‚Ä≤ ‚ààRN‚Ä≤√óD, Y‚Ä≤ ‚àà{0, . . . , C ‚àí1}N‚Ä≤ and N ‚Ä≤ ‚â™N, such
that a GNN trained on S can achieve comparable performance to one trained on the much larger T .
Thus, the objective can be formulated as the following bi-level problem,
min
S
L (GNNŒ∏S(A, X), Y)
s.t
Œ∏S = arg min
Œ∏
L(GNNŒ∏(A‚Ä≤, X‚Ä≤), Y‚Ä≤),
(1)"
METHODOLOGY,0.0875,"where GNNŒ∏ denotes the GNN model parameterized with Œ∏, Œ∏S denotes the parameters of the
model trained on S, and L denotes the loss function used to measure the difference between model
predictions and ground truth, i.e. cross-entropy loss. However, optimizing the above objective can
lead to overÔ¨Åtting on a speciÔ¨Åc model initialization. To generate condensed data that generalizes to
a distribution of random initializations PŒ∏0, we rewrite the objective as follows:
min
S
EŒ∏0‚àºPŒ∏0 [L (GNNŒ∏S(A, X), Y)]
s.t.
Œ∏S = arg min
Œ∏
L(GNNŒ∏(Œ∏0)(A‚Ä≤, X‚Ä≤), Y‚Ä≤).
(2)"
METHODOLOGY,0.090625,"where Œ∏(Œ∏0) indicates that Œ∏ is a function acting on Œ∏0. Note that the setting discussed above is for
inductive learning where all the nodes are labeled and test nodes are unseen during training. We can
easily generalize graph condensation to transductive setting by assuming Y is partially labeled."
METHODOLOGY,0.09375,Published as a conference paper at ICLR 2022
GRAPH CONDENSATION VIA GRADIENT MATCHING,0.096875,"3.1
GRAPH CONDENSATION VIA GRADIENT MATCHING"
GRAPH CONDENSATION VIA GRADIENT MATCHING,0.1,"To tackle the optimization problem in Eq. (2), one strategy is to compute the gradient of L w.r.t S and
optimize S via gradient descent, as in dataset distillation (Wang et al., 2018). However, this requires
solving a nested loop optimization and unrolling the whole training trajectory of the inner problem,
which can be prohibitively expensive. To bypass the bi-level optimization, we follow the gradient
matching method proposed in (Zhao et al., 2021) which aims to match the network parameters w.r.t.
large-real and small-synthetic training data by matching their gradients at each training step. In this
way, the training trajectory on small-synthetic data S can mimic that on the large-real data T , i.e.,
the models trained on these two datasets converge to similar solutions (parameters). Concretely, the
parameter matching process for GNNs can be modeled as follows:"
GRAPH CONDENSATION VIA GRADIENT MATCHING,0.103125,minS EŒ∏0‚àºPŒ∏0
GRAPH CONDENSATION VIA GRADIENT MATCHING,0.10625,"hPT ‚àí1
t=0 D

Œ∏S
t , Œ∏T
t
i
with"
GRAPH CONDENSATION VIA GRADIENT MATCHING,0.109375,"Œ∏S
t+1 = optŒ∏

L

GNNŒ∏S
t (A‚Ä≤, X‚Ä≤), Y‚Ä≤
and Œ∏T
t+1 = optŒ∏

L

GNNŒ∏T
t (A, X), Y

(3)"
GRAPH CONDENSATION VIA GRADIENT MATCHING,0.1125,"where D(¬∑, ¬∑) is a distance function, T is the number of steps of the whole training trajectory, optŒ∏ is
the update rule for model parameters, and Œ∏S
t , Œ∏T
t denote the model parameters trained on S and T
at time step t, respectively. Since our goal is to match the parameters step by step, we then consider
one-step gradient descent for the update rule optŒ∏:"
GRAPH CONDENSATION VIA GRADIENT MATCHING,0.115625,"Œ∏S
t+1 ‚ÜêŒ∏S
t ‚àíŒ∑‚àáŒ∏L

GNNŒ∏S
t (A‚Ä≤, X‚Ä≤), Y‚Ä≤
and
Œ∏T
t+1 ‚ÜêŒ∏T
t ‚àíŒ∑‚àáŒ∏L

GNNŒ∏T
t (A, X), Y
"
GRAPH CONDENSATION VIA GRADIENT MATCHING,0.11875,"(4)
where Œ∑ is the learning rate for the gradient descent. Based on the observation made in Zhao et al.
(2021) that the distance between Œ∏S
t and Œ∏T
t is typically small, we can simplify the objective as a
gradient matching process as follows,"
GRAPH CONDENSATION VIA GRADIENT MATCHING,0.121875,"min
S EŒ∏0‚àºPŒ∏0"
GRAPH CONDENSATION VIA GRADIENT MATCHING,0.125,"""T ‚àí1
X"
GRAPH CONDENSATION VIA GRADIENT MATCHING,0.128125,"t=0
D (‚àáŒ∏L (GNNŒ∏t(A‚Ä≤, X‚Ä≤), Y‚Ä≤) , ‚àáŒ∏L (GNNŒ∏t(A, X), Y)) # (5)"
GRAPH CONDENSATION VIA GRADIENT MATCHING,0.13125,"where Œ∏S
t and Œ∏T
t are replaced by Œ∏t, which is trained on the small-synthetic graph. The distance D
is further deÔ¨Åned as the sum of the distance dis at each layer. Given two gradients GS ‚ààRd1√ód2 and
GT ‚ààRd1√ód2 at a speciÔ¨Åc layer, the distance dis(¬∑, ¬∑) used for condensation is deÔ¨Åned as follows,"
GRAPH CONDENSATION VIA GRADIENT MATCHING,0.134375,"dis(GS, GT ) = d2
X i=1 "
GRAPH CONDENSATION VIA GRADIENT MATCHING,0.1375,"1 ‚àí
GS
i ¬∑ GT
i
GS
i
 GT
i ! (6)"
GRAPH CONDENSATION VIA GRADIENT MATCHING,0.140625,"where GS
i , GT
i are the i-th column vectors of the gradient matrices. With the above formulations,
we are able to achieve parameter matching through an efÔ¨Åcient strategy of gradient matching."
GRAPH CONDENSATION VIA GRADIENT MATCHING,0.14375,"We note that jointly learning the three variables A‚Ä≤, X‚Ä≤ and Y‚Ä≤ is highly challenging, as they are
interdependent. Hence, to simplify the problem, we Ô¨Åx the node labels Y‚Ä≤ while keeping the class
distribution the same as the original labels Y."
GRAPH CONDENSATION VIA GRADIENT MATCHING,0.146875,"Graph Sampling. GNNs are often trained in a full-batch manner (Kipf & Welling, 2017; Wu
et al., 2019b). However, as suggested by previous works that reconstruct data from gradients (Zhu
et al., 2019), large batch size tends to make reconstruction more difÔ¨Åcult because more variables are
involved during optimization. To make things worse, the computation cost of GNNs gets expensive
on large graphs as the forward pass of GNNs involves the aggregation of enormous neighboring
nodes. To address the above issues, we sample a Ô¨Åxed-size set of neighbors on the original graph in
each aggregation layer of GNNs and adopt a mini-batch training strategy. To further reduce memory
usage and ease optimization, we calculate the gradient matching loss for nodes from different classes
separately, as matching the gradients w.r.t. the data from a single class is easier than that from all
classes. SpeciÔ¨Åcally, for a given class c, we sample a batch of nodes of class c together with a
portion of their neighbors from large-real data T . We denote the process as (Ac, Xc, Yc) ‚àºT .
For the condensed graph A‚Ä≤, we sample a batch of synthetic nodes of class c but do not sample
their neighbors. In other words, we use all of their neighbors, i.e., all other nodes, during the
aggregation process, since we need to learn the connections with other nodes. We denote the process
as (A‚Ä≤
c, X‚Ä≤
c, Y‚Ä≤
c) ‚àºS."
"MODELING CONDENSED GRAPH DATA
ONE ESSENTIAL CHALLENGE IN THE GRAPH CONDENSATION PROBLEM IS HOW TO MODEL THE CONDENSED GRAPH",0.15,"3.2
MODELING CONDENSED GRAPH DATA
One essential challenge in the graph condensation problem is how to model the condensed graph
data and resolve dependency among nodes. The most straightforward way is to treat both A‚Ä≤ and X‚Ä≤"
"MODELING CONDENSED GRAPH DATA
ONE ESSENTIAL CHALLENGE IN THE GRAPH CONDENSATION PROBLEM IS HOW TO MODEL THE CONDENSED GRAPH",0.153125,Published as a conference paper at ICLR 2022
"MODELING CONDENSED GRAPH DATA
ONE ESSENTIAL CHALLENGE IN THE GRAPH CONDENSATION PROBLEM IS HOW TO MODEL THE CONDENSED GRAPH",0.15625,"as free parameters. However, the number of parameters in A‚Ä≤ grows quadratically as N ‚Ä≤ increases.
The increased model complexity can pose challenges in optimizing the framework and increase the
risk of overÔ¨Åtting. Therefore, it is desired to parametrize the condensed adjacency matrix in a way
where the number of parameters does not grow too fast. On the other hand, treating A‚Ä≤ and X‚Ä≤
as independent parameters overlooks the implicit correlations between graph structure and features,
which have been widely acknowledged in the literature (III et al., 2014; Shalizi & Thomas, 2011);
e.g., in social networks, users interact with others based on their interests, while in e-commerce,
users purchase products due to certain product attributes. Hence, we propose to model the condensed
graph structure as a function of the condensed node features:"
"MODELING CONDENSED GRAPH DATA
ONE ESSENTIAL CHALLENGE IN THE GRAPH CONDENSATION PROBLEM IS HOW TO MODEL THE CONDENSED GRAPH",0.159375,"A‚Ä≤ = gŒ¶(X‚Ä≤),
with A‚Ä≤
ij = Sigmoid
MLPŒ¶([x‚Ä≤
i; x‚Ä≤
j]) + MLPŒ¶([x‚Ä≤
j; x‚Ä≤
i])
2 
(7)"
"MODELING CONDENSED GRAPH DATA
ONE ESSENTIAL CHALLENGE IN THE GRAPH CONDENSATION PROBLEM IS HOW TO MODEL THE CONDENSED GRAPH",0.1625,"where MLPŒ¶ is a multi-layer neural network parameterized with Œ¶ and [¬∑; ¬∑] denotes concatenation.
In Eq. (7), we intentionally control A‚Ä≤
ij = A‚Ä≤
ji to make the condensed graph structure symmetric
since we are mostly dealing with symmetric graphs. It can also adjust to asymmetric graphs by
setting A‚Ä≤
ij = Sigmoid(MLPŒ¶([xi; x‚Ä≤
j]). Then we rewrite our objective as"
"MODELING CONDENSED GRAPH DATA
ONE ESSENTIAL CHALLENGE IN THE GRAPH CONDENSATION PROBLEM IS HOW TO MODEL THE CONDENSED GRAPH",0.165625,"min
X‚Ä≤,Œ¶ EŒ∏0‚àºPŒ∏0"
"MODELING CONDENSED GRAPH DATA
ONE ESSENTIAL CHALLENGE IN THE GRAPH CONDENSATION PROBLEM IS HOW TO MODEL THE CONDENSED GRAPH",0.16875,"""T ‚àí1
X"
"MODELING CONDENSED GRAPH DATA
ONE ESSENTIAL CHALLENGE IN THE GRAPH CONDENSATION PROBLEM IS HOW TO MODEL THE CONDENSED GRAPH",0.171875,"t=0
D (‚àáŒ∏L (GNNŒ∏t(gŒ¶(X‚Ä≤), X‚Ä≤), Y‚Ä≤) , ‚àáŒ∏L (GNNŒ∏t(A, X), Y)) # (8)"
"MODELING CONDENSED GRAPH DATA
ONE ESSENTIAL CHALLENGE IN THE GRAPH CONDENSATION PROBLEM IS HOW TO MODEL THE CONDENSED GRAPH",0.175,"Note that there are two clear beneÔ¨Åts of the above formulation over the na¬®ƒ±ve one (free parameters).
Firstly, the number of parameters for modeling graph structure no longer depends on the number
of nodes, hence avoiding jointly learning O(N ‚Ä≤2) parameters; as a result, when N ‚Ä≤ gets larger,
GCOND suffers less risk of overÔ¨Åtting. Secondly, if we want to grow the synthetic graph by adding
more synthetic nodes condensed from real graph, the trained MLPŒ¶ can be employed to infer the
connections of new synthetic nodes, and hence we only need to learn their features."
"MODELING CONDENSED GRAPH DATA
ONE ESSENTIAL CHALLENGE IN THE GRAPH CONDENSATION PROBLEM IS HOW TO MODEL THE CONDENSED GRAPH",0.178125,"Alternating Optimization Schema. Jointly optimizing X‚Ä≤ and Œ¶ is often challenging as they are
directly affecting each other. Instead, we propose to alternatively optimize X‚Ä≤ and Œ¶: we update Œ¶
for the Ô¨Årst œÑ1 epochs and then update X‚Ä≤ for œÑ2 epochs; the process is repeated until the stopping
condition is met ‚Äì we Ô¨Ånd empirically that this does better as shown in Appendix C."
"MODELING CONDENSED GRAPH DATA
ONE ESSENTIAL CHALLENGE IN THE GRAPH CONDENSATION PROBLEM IS HOW TO MODEL THE CONDENSED GRAPH",0.18125,"SparsiÔ¨Åcation. In the learned condensed adjacency matrix A‚Ä≤, there can exist some small values
which have little effect on the aggregation process in GNNs but still take up a certain amount of
storage (e.g. 4 bytes per Ô¨Çoat). Thus, we remove the entries whose values are smaller than a given
threshold Œ¥ to promote sparsity of the learned A‚Ä≤. We further justify that suitable choices of Œ¥ for
sparsiÔ¨Åcation do not degrade performance a lot in Appendix C."
"MODELING CONDENSED GRAPH DATA
ONE ESSENTIAL CHALLENGE IN THE GRAPH CONDENSATION PROBLEM IS HOW TO MODEL THE CONDENSED GRAPH",0.184375,"The detailed algorithm can be found in Algorithm 1 in Appendix B. In detail, we Ô¨Årst set the con-
densed label set Y‚Ä≤ to Ô¨Åxed values and initialize X‚Ä≤ as node features randomly selected from each
class. In each outer loop, we sample a GNN model initialization Œ∏ from a distribution PŒ∏. Then,
for each class we sample the corresponding node batches from T and S, and calculate the gradient
matching loss within each class. The sum of losses from different classes are used to update X‚Ä≤ or Œ¶.
After that we update the GNN parameters for œÑŒ∏ epochs. When Ô¨Ånishing the updating of condensed
graph parameters, we use A‚Ä≤ = ReLU(gŒ¶(X‚Ä≤) ‚àíŒ¥) to obtain the Ô¨Ånal sparsiÔ¨Åed graph structure."
"MODELING CONDENSED GRAPH DATA
ONE ESSENTIAL CHALLENGE IN THE GRAPH CONDENSATION PROBLEM IS HOW TO MODEL THE CONDENSED GRAPH",0.1875,"A ‚ÄúGraphless‚Äù Model Variant. We now explore another parameterization for the condensed graph
data. We provide a model variant named GCOND-X that only learns the condensed node features
X‚Ä≤ without learning the condensed structure A‚Ä≤. In other words, we use a Ô¨Åxed identity matrix I as
the condensed graph structure. SpeciÔ¨Åcally, this model variant aims to match the gradients of GNN
parameters on the large-real data (A, X) and small-synthetic data (I, X‚Ä≤). Although GCOND-X
is unable to learn the condensed graph structure which can be highly useful for downstream data
analysis, it still shows competitive performance in Table 2 in the experiments because the features
are learned to incorporate relevant information from the graph via the matching loss."
EXPERIMENTS,0.190625,"4
EXPERIMENTS"
EXPERIMENTS,0.19375,"In this section, we design experiments to validate the effectiveness of the proposed framework
GCOND. We Ô¨Årst introduce experimental settings, then compare GCOND against representative
baselines with discussions and Ô¨Ånally show some advantages of GCOND."
EXPERIMENTS,0.196875,Published as a conference paper at ICLR 2022
EXPERIMENTAL SETUP,0.2,"4.1
EXPERIMENTAL SETUP"
EXPERIMENTAL SETUP,0.203125,"Datasets. We evaluate the condensation performance of the proposed framework on three transduc-
tive datasets, i.e., Cora, Citeseer (Kipf & Welling, 2017) and Ogbn-arxiv (Hu et al., 2020), and two
inductive datasets, i.e., Flickr (Zeng et al., 2020) and Reddit (Hamilton et al., 2017). We use the pub-
lic splits for all the datasets. For the inductive setting, we follow the setup in (Hamilton et al., 2017)
where the test graph is not available during training. Dataset statistics are shown in Appendix A."
EXPERIMENTAL SETUP,0.20625,"Baselines.
We compare our proposed methods to Ô¨Åve baselines:
(i) one graph coarsening
method (Loukas, 2019; Huang et al., 2021b), (ii-iv) three coreset methods (Random, Herd-
ing (Welling, 2009) and K-Center (Farahani & Hekmatfar, 2009; Sener & Savarese, 2018)), and
(v) dataset condensation (DC). For the graph coarsening method, we adopt the variation neighbor-
hoods method implemented by Huang et al. (2021b). For coreset methods, we Ô¨Årst use them to
select nodes from the original dataset and induce a subgraph from the selected nodes to serve as the
reduced graph. In Random, the nodes are randomly selected. The Herding method, which is often
used in continual learning (RebufÔ¨Ået al., 2017; Castro et al., 2018), picks samples that are closest
to the cluster center. K-Center selects the center samples to minimize the largest distance between
a sample and its nearest center. We use the implementations provided by Zhao et al. (2021) for
Herding, K-Center and DC. As vanilla DC cannot leverage any structure information, we develop a
variant named DC-Graph, which additionally leverages graph structure during test stage, to replace
DC for the following experiments. A comparison between DC, DC-Graph, GCOND and GCOND-X
is shown in Table 1 and their training details can be found in Appendix A.3."
EXPERIMENTAL SETUP,0.209375,"Evaluation. We Ô¨Årst use the aforementioned baselines to obtain condensed graphs and then eval-
uate them on GNNs for both transductive and inductive node classiÔ¨Åcation tasks. For transductive
datasets, we condense the full graph with N nodes into a synthetic graph with rN (0 < r < 1)
nodes, where r is the ratio of synthetic nodes to original nodes. For inductive datasets, we only
condense the training graph since the rest of the full graph is not available during training. The
choices of r2 are listed in Table 2. For each r, we generate 5 condensed graphs with different seeds.
To evaluate the effectiveness of condensed graphs, we have two stages: (1) a training stage, where
we train a GNN model on the condensed graph, and (2) a test stage, where the trained GNN uses the
test graph (or full graph in transductive setting) to infer the labels for test nodes. The resulting test
performance is compared with that obtained when training on original datasets. All experiments are
repeated 10 times, and we report average performance and variance."
EXPERIMENTAL SETUP,0.2125,"Hyperparameter settings. As our goal is to generate highly informative synthetic graphs which can
beneÔ¨Åt GNNs, we choose one representative model, GCN (Kipf & Welling, 2017), for performance
evaluation. For the GNN used in condensation, i.e., the GNNŒ∏(¬∑) in Eq. (8), we adopt SGC (Wu
et al., 2019a) which decouples the propagation and transformation process but still shares similar
graph Ô¨Åltering behavior as GCN. Unless otherwise stated, we use 2-layer models with 256 hidden
units. The weight decay and dropout for the models are set to 0 in condensation process. More
details for hyper-parameter tuning can be found in Appendix A."
COMPARISON WITH BASELINES,0.215625,"4.2
COMPARISON WITH BASELINES"
COMPARISON WITH BASELINES,0.21875,"In this subsection, we test the performance of a 2-layer GCN on the condensed graphs, and compare
the proposed GCOND and GCOND-X with baselines. Notably, all methods produce both structure
and node features, i.e. A‚Ä≤ and X‚Ä≤, except DC-Graph and GCOND-X. Since DC-Graph and GCOND-
X do not produce any structure, we simply use an identity matrix as the adjacency matrix when
training GNNs solely on condensed features. However, during inference, we use the full graph
(transductive setting) or test graph (inductive setting) to propagate information based on the trained
GNNs. This training paradigm is similar to the C&S model (Huang et al., 2021a) which trains
an MLP without the graph information and performs label propagation based on MLP predictions.
Table 2 reports node classiÔ¨Åcation performance; we make the following observations:"
COMPARISON WITH BASELINES,0.221875,"Obs 1. Condensation methods achieve promising performance even with extremely large re-
duction rates. Condensation methods, i.e., GCOND, GCOND-X and DC-Graph, outperform coreset
methods and graph coarsening signiÔ¨Åcantly at the lowest ratio r for each dataset. This shows the
importance of learning synthetic data using the guidance from downstream tasks. Notably, GCOND"
COMPARISON WITH BASELINES,0.225,2We determine r based on original graph size and labeling rate ‚Äì see Appendix A for details.
COMPARISON WITH BASELINES,0.228125,Published as a conference paper at ICLR 2022
COMPARISON WITH BASELINES,0.23125,"Table 1: Information comparison used during condensation, training and test for reduction methods.
A‚Ä≤, X‚Ä≤ and A, X are condensed (original) graph and features, respectively."
COMPARISON WITH BASELINES,0.234375,"DC
DC-Graph
GCOND-X
GCOND"
COMPARISON WITH BASELINES,0.2375,"Condensation
Xtrain
Xtrain
Atrain, Xtrain
Atrain, Xtrain"
COMPARISON WITH BASELINES,0.240625,"Training
X‚Ä≤
X‚Ä≤
X‚Ä≤
A‚Ä≤, X‚Ä≤"
COMPARISON WITH BASELINES,0.24375,"Test
Xtest
Atest, Xtest
Atest, Xtest
Atest, Xtest"
COMPARISON WITH BASELINES,0.246875,"Table 2: GCOND and GCOND-X achieves promising performance in comparison to baselines even
with extremely large reduction rates. We report transductive performance on Citeseer, Cora, Ogbn-
arxiv; inductive performance on Flickr, Reddit. Performance is reported as test accuracy (%)."
COMPARISON WITH BASELINES,0.25,"Baselines
Proposed"
COMPARISON WITH BASELINES,0.253125,"Dataset
Ratio (r)
Random
(A‚Ä≤, X‚Ä≤)
Herding
(A‚Ä≤, X‚Ä≤)
K-Center
(A‚Ä≤, X‚Ä≤)
Coarsening
(A‚Ä≤, X‚Ä≤)
DC-Graph
(X‚Ä≤)
GCOND-X
(X‚Ä≤)
GCOND
(A‚Ä≤, X‚Ä≤)
Whole
Dataset"
COMPARISON WITH BASELINES,0.25625,"Citeseer
0.9%
54.4¬±4.4
57.1¬±1.5
52.4¬±2.8
52.2¬±0.4
66.8¬±1.5
71.4¬±0.8
70.5¬±1.2
71.7¬±0.1
1.8%
64.2¬±1.7
66.7¬±1.0
64.3¬±1.0
59.0¬±0.5
66.9¬±0.9
69.8¬±1.1
70.6¬±0.9
3.6%
69.1¬±0.1
69.0¬±0.1
69.1¬±0.1
65.3¬±0.5
66.3¬±1.5
69.4¬±1.4
69.8¬±1.4"
COMPARISON WITH BASELINES,0.259375,"Cora
1.3%
63.6¬±3.7
67.0¬±1.3
64.0¬±2.3
31.2¬±0.2
67.3¬±1.9
75.9¬±1.2
79.8¬±1.3
81.2¬±0.2
2.6%
72.8¬±1.1
73.4¬±1.0
73.2¬±1.2
65.2¬±0.6
67.6¬±3.5
75.7¬±0.9
80.1¬±0.6
5.2%
76.8¬±0.1
76.8¬±0.1
76.7¬±0.1
70.6¬±0.1
67.7¬±2.2
76.0¬±0.9
79.3¬±0.3"
COMPARISON WITH BASELINES,0.2625,"Ogbn-arxiv
0.05%
47.1¬±3.9
52.4¬±1.8
47.2¬±3.0
35.4¬±0.3
58.6¬±0.4
61.3¬±0.5
59.2¬±1.1
71.4¬±0.1
0.25%
57.3¬±1.1
58.6¬±1.2
56.8¬±0.8
43.5¬±0.2
59.9¬±0.3
64.2¬±0.4
63.2¬±0.3
0.5%
60.0¬±0.9
60.4¬±0.8
60.3¬±0.4
50.4¬±0.1
59.5¬±0.3
63.1¬±0.5
64.0¬±0.4"
COMPARISON WITH BASELINES,0.265625,"Flickr
0.1%
41.8¬±2.0
42.5¬±1.8
42.0¬±0.7
41.9¬±0.2
46.3¬±0.2
45.9¬±0.1
46.5¬±0.4
47.2¬±0.1
0.5%
44.0¬±0.4
43.9¬±0.9
43.2¬±0.1
44.5¬±0.1
45.9¬±0.1
45.0¬±0.2
47.1¬±0.1
1%
44.6¬±0.2
44.4¬±0.6
44.1¬±0.4
44.6¬±0.1
45.8¬±0.1
45.0¬±0.1
47.1¬±0.1"
COMPARISON WITH BASELINES,0.26875,"Reddit
0.05%
46.1¬±4.4
53.1¬±2.5
46.6¬±2.3
40.9¬±0.5
88.2¬±0.2
88.4¬±0.4
88.0¬±1.8
93.9¬±0.0
0.1%
58.0¬±2.2
62.7¬±1.0
53.0¬±3.3
42.8¬±0.8
89.5¬±0.1
89.3¬±0.1
89.6¬±0.7
0.2%
66.3¬±1.9
71.0¬±1.6
58.5¬±2.1
47.4¬±0.9
90.5¬±1.2
88.8¬±0.4
90.1¬±0.5"
COMPARISON WITH BASELINES,0.271875,"achieves 79.8%, 80.1% and 79.3% at 1.3%, 2.6% and 5.2% condensation ratios at Cora, while the
whole dataset performance is 81.2%. The GCOND variants also show promising performance on
Cora, Flickr and Reddit at all coarsening ratios. Although the gap between whole-dataset Ogbn-arxiv
and our methods is larger, they still outperform baselines by a large margin."
COMPARISON WITH BASELINES,0.275,"Obs 2. Learning X‚Ä≤ instead of (A‚Ä≤, X‚Ä≤) as the condensed graph can also lead to good results.
GCOND-X achieves close performance to GCOND on 11 of 15 cases. Since our objective in graph
condensation is to achieve parameter matching through gradient matching, training a GNN on the
learned features X‚Ä≤ with identity adjacency matrix is also able to mimic the training trajectory of
GNN parameters. One reason could be that X‚Ä≤ has already encoded node features and structural in-
formation of the original graph during the condensation process. However, there are many scenarios
where the graph structure is essential such as the generalization to other GNN architectures (e.g.,
GAT) and visualizing the patterns in the data. More details are given in the following subsections."
COMPARISON WITH BASELINES,0.278125,"Obs 3. Condensing node features and structural information simultaneously can lead to better
performance. In most cases, GCOND and GCOND-X obtain much better performance than DC-
Graph. One key reason is that GCOND and GCOND-X can take advantage of both node features
and structural information in the condensation process. We notice that DC-Graph achieves a highly
comparable result (90.5%) on Reddit at 0.2% condensation ratio to the whole dataset performance
(93.9%). This may indicate that the original training graph structure might not be useful. To verify
this assumption, we train a GCN on the original Reddit dataset without using graph structure (i.e.,
setting Atrain = I), but allow using the test graph structure for inference using the trained model. The
obtained performance is 92.5%, which is very close to the original performance 93.9%, indicating
that training without graph structure can still achieve comparable performance. We also note that
learning X‚Ä≤, A‚Ä≤ simultaneously creates opportunities to absorb information from graph structure
directly into learned features, lessening reliance on distilling graph properties reliably while still
achieving good generalization performance from features."
COMPARISON WITH BASELINES,0.28125,"Obs 4. Larger condensed graph size does not strictly indicate better performance. Although
larger condensed graph sizes allow for more parameters which can potentially encapsulate more
information from original graph, it simultaneously becomes harder to optimize due to the increased"
COMPARISON WITH BASELINES,0.284375,Published as a conference paper at ICLR 2022
COMPARISON WITH BASELINES,0.2875,"Table 3: Graph condensation can work well with different architectures. Avg. stands for the average
test accuracy of APPNP, Cheby, GCN, GraphSAGE and SGC. SAGE stands for GraphSAGE."
COMPARISON WITH BASELINES,0.290625,"Methods
Data
MLP
GAT
APPNP
Cheby
GCN
SAGE
SGC
Avg."
COMPARISON WITH BASELINES,0.29375,"Citeseer
r = 1.8%"
COMPARISON WITH BASELINES,0.296875,"DC-Graph
X‚Ä≤
66.2
-
66.4
64.9
66.2
65.9
69.6
66.6
GCOND-X
X‚Ä≤
69.6
-
69.7
70.6
69.7
69.2
71.6
70.2
GCOND
A‚Ä≤, X‚Ä≤
63.9
55.4
69.6
68.3
70.5
66.2
70.3
69.0"
COMPARISON WITH BASELINES,0.3,"Cora
r = 2.6%"
COMPARISON WITH BASELINES,0.303125,"DC-Graph
X‚Ä≤
67.2
-
67.1
67.7
67.9
66.2
72.8
68.3
GCOND-X
X‚Ä≤
76.0
-
77.0
74.1
75.3
76.0
76.1
75.7
GCOND
A‚Ä≤, X‚Ä≤
73.1
66.2
78.5
76.0
80.1
78.2
79.3
78.4"
COMPARISON WITH BASELINES,0.30625,"Ogbn-arxiv
r = 0.25%"
COMPARISON WITH BASELINES,0.309375,"DC-Graph
X‚Ä≤
59.9
-
60.0
55.7
59.8
60.0
60.4
59.2
GCOND-X
X‚Ä≤
64.1
-
61.5
59.5
64.2
64.4
64.7
62.9
GCOND
A‚Ä≤, X‚Ä≤
62.2
60.0
63.4
54.9
63.2
62.6
63.7
61.6"
COMPARISON WITH BASELINES,0.3125,"Flickr
r = 0.5%"
COMPARISON WITH BASELINES,0.315625,"DC-Graph
X‚Ä≤
43.1
-
45.7
43.8
45.9
45.8
45.6
45.4
GCOND-X
X‚Ä≤
42.1
-
44.6
42.3
45.0
44.7
44.4
44.2
GCOND
A‚Ä≤, X‚Ä≤
44.8
40.1
45.9
42.8
47.1
46.2
46.1
45.6"
COMPARISON WITH BASELINES,0.31875,"Reddit
r = 0.1%"
COMPARISON WITH BASELINES,0.321875,"DC-Graph
X‚Ä≤
50.3
-
81.2
77.5
89.5
89.7
90.5
85.7
GCOND-X
X‚Ä≤
40.1
-
78.7
74.0
89.3
89.3
91.0
84.5
GCOND
A‚Ä≤, X‚Ä≤
42.5
60.2
87.8
75.5
89.4
89.1
89.6
86.3"
COMPARISON WITH BASELINES,0.325,"model complexity. We observe that once the condensation ratio reaches a certain threshold, the
performance becomes stable. However, the performance of coreset methods and graph coarsening is
much more sensitive to the reduction ratio. Coreset methods only select existing samples while graph
coarsening groups existing nodes into super nodes. When the reduction ratio is too low, it becomes
extremely difÔ¨Åcult to select informative nodes or form representative super nodes by grouping."
GENERALIZABILITY OF CONDENSED GRAPHS,0.328125,"4.3
GENERALIZABILITY OF CONDENSED GRAPHS"
GENERALIZABILITY OF CONDENSED GRAPHS,0.33125,"Next, we illustrate the generalizability of condensed graphs from the following three perspectives."
GENERALIZABILITY OF CONDENSED GRAPHS,0.334375,"Different Architectures. Next, we show the generalizability of the graph condensation proce-
dure. SpeciÔ¨Åcally, we show test performance when using a graph condensed by one GNN model
to train different GNN architectures. SpeciÔ¨Åcally, we choose APPNP (Klicpera et al., 2019), GCN,
SGC (Wu et al., 2019a), GraphSAGE (Hamilton et al., 2017), Cheby (Defferrard et al., 2016) and
GAT (Velickovic et al., 2018). We also include MLP and report the results in Table 3. From the ta-
ble, we Ô¨Ånd that the condensed graphs generated by GCOND show good generalization on different
architectures. We may attribute such transferability across different architectures to similar Ô¨Åltering
behaviors of those GNN models, which have been studied in Ma et al. (2020); Zhu et al. (2021)."
GENERALIZABILITY OF CONDENSED GRAPHS,0.3375,"Versatility of GCOND. The proposed GCOND is highly composable in that we can adopt various
GNNs inside the condensation network. We investigate the performances of various GNNs when us-
ing different GNN models in the condensation process, i.e., GNNŒ∏(¬∑) in Eq. (8). We choose APPNP,
Cheby, GCN, GraphSAGE and SGC to serve as the models used in condensation and evaluation.
Note that we omit GAT due to its deterioration under large neighborhood sizes (Ma et al., 2021).
We choose Cora and Ogbn-arxiv to report the performance in Table 4 where C and T denote con-
densation and test models, respectively. The graphs condensed by different GNNs all show strong
transfer performance on other architectures."
GENERALIZABILITY OF CONDENSED GRAPHS,0.340625,"Neural Architecture Search. We also perform experiments on neural architecture search, detailed
in Appendix C.2. We search 480 architectures of APPNP and perform the search process on Cora,
Citeseer and Ogbn-arxiv. SpeciÔ¨Åcally, we train each architecture on the reduced graph for epochs on
as the model converges faster on the smaller graph. We observe reliable correlation of performances
between condensed dataset training and whole-dataset training as shown in Table 9: 0.76/0.79/0.64
for Cora/Citeseer/Ogbn-arxiv."
ANALYSIS ON CONDENSED DATA,0.34375,"4.4
ANALYSIS ON CONDENSED DATA"
ANALYSIS ON CONDENSED DATA,0.346875,"Statistics of Condensed Graphs. In Table 5, we compare several properties between condensed
graphs and original graphs. Note that a widely used homophily measure is deÔ¨Åned in (Zhu et al.,
2020) but it does not apply to weighted graphs. Hence, when computing homophily, we binarize the
graphs by removing edges whose weights are smaller than 0.5. We make the following observations.
First, while achieving similar performance for downstream tasks, the condensed graphs contain
fewer nodes and take much less storage. Second, the condensed graphs are less sparse than their"
ANALYSIS ON CONDENSED DATA,0.35,Published as a conference paper at ICLR 2022
ANALYSIS ON CONDENSED DATA,0.353125,"Table 4: Cross-architecture performance is shown in test accuracy (%). SAGE: GraphSAGE. Graphs
condensed by different GNNs all show strong transfer performance on other architectures."
ANALYSIS ON CONDENSED DATA,0.35625,"(a) Cora, r=2.6%"
ANALYSIS ON CONDENSED DATA,0.359375,"C\T
APPNP
Cheby
GCN
SAGE
SGC"
ANALYSIS ON CONDENSED DATA,0.3625,"APPNP 72.1¬±2.6 60.8¬±6.4 73.5¬±2.4 72.3¬±3.5 73.1¬±3.1
Cheby
75.3¬±2.9 71.8¬±1.1 76.8¬±2.1 76.4¬±2.0 75.5¬±3.5
GCN
69.8¬±4.0 53.2¬±3.4 70.6¬±3.7 60.2¬±1.9 68.7¬±5.4
SAGE
77.1¬±1.1 69.3¬±1.7 77.0¬±0.7 76.1¬±0.7 77.7¬±1.8
SGC
78.5¬±1.0 76.0¬±1.1 80.1¬±0.6 78.2¬±0.9 79.3¬±0.7"
ANALYSIS ON CONDENSED DATA,0.365625,"(b) Ogbn-arxiv, r=0.05%"
ANALYSIS ON CONDENSED DATA,0.36875,"C\T
APPNP
Cheby
GCN
SAGE
SGC"
ANALYSIS ON CONDENSED DATA,0.371875,"APPNP 60.3¬±0.2 51.8¬±0.5 59.9¬±0.4 59.0¬±1.1 61.2¬±0.4
Cheby
57.4¬±0.4 53.5¬±0.5 57.4¬±0.8 57.1¬±0.8 58.2¬±0.6
GCN
59.3¬±0.4 51.8¬±0.7 60.3¬±0.3 60.2¬±0.4 59.2¬±0.7
SAGE
57.6¬±0.8 53.9¬±0.6 58.1¬±0.6 57.8¬±0.7 59.0¬±1.1
SGC
59.7¬±0.5 49.5¬±0.8 59.2¬±1.1 58.9¬±1.6 60.5¬±0.6"
ANALYSIS ON CONDENSED DATA,0.375,"Table 5: Comparison between condensed graphs and original graphs. The condensed graphs have
fewer nodes and are more dense."
ANALYSIS ON CONDENSED DATA,0.378125,"Citeseer, r=0.9% Cora, r=1.3% Ogbn-arxiv, r=0.25% Flickr, r=0.5%
Reddit, r=0.1%"
ANALYSIS ON CONDENSED DATA,0.38125,"Whole
GCOND
Whole
GCOND
Whole
GCOND
Whole
GCOND
Whole
GCOND"
ANALYSIS ON CONDENSED DATA,0.384375,"Accuracy
70.7
70.5
81.5
79.8
71.4
63.2
47.1
47.1
94.1
89.4
#Nodes
3,327
60
2,708
70
169,343
454
44,625
223
153,932
153
#Edges
4,732
1,454
5,429
2,128
1,166,243
3,354
218,140
3,788
10,753,238
301
Sparsity
0.09%
80.78%
0.15%
86.86%
0.01%
3.25%
0.02%
15.23%
0.09%
2.57%
Homophily
0.74
0.65
0.81
0.79
0.65
0.07
0.33
0.28
0.78
0.04
Storage
47.1 MB
0.9 MB
14.9 MB 0.4 MB 100.4 MB
0.3 MB
86.8 MB 0.5 MB
435.5 MB
0.4 MB"
ANALYSIS ON CONDENSED DATA,0.3875,"(a) Cora, r=2.5%
(b) Citeseer, r=1.8%
(c) Arxiv, r=0.05%
(d) Flickr, r=0.1%
(e) Reddit, r=0.1%"
ANALYSIS ON CONDENSED DATA,0.390625,"Figure 2: Condensed graphs sometimes exhibit structure mimicking the original (a, b, d). Other
times (c, e), learned features absorb graph properties and create less explicit graph reliance."
ANALYSIS ON CONDENSED DATA,0.39375,"larger counterparts. Since the condensed graph is on extremely small scale, there would be almost
no connections between nodes if the condensed graph maintains the original sparsity. Third, for
Citeseer, Cora and Flickr, the homophily information are well preserved in the condensed graphs."
ANALYSIS ON CONDENSED DATA,0.396875,"Visualization. We present the visualization results for all datasets in Figure 4, where nodes with
the same color are from the same class. Notably, as the learned condensed graphs are weighted
graphs, we use black lines to denote the edges with weights larger than 0.5 and gray lines to denote
the edges with weights smaller than 0.5. From Figure 4, we can observe some patterns in the con-
densed graphs, e.g., the homophily patterns on Cora and Citeseer are well preserved. Interestingly,
the learned graph for Reddit is very close to a star graph where almost all the nodes only have con-
nections with very few center nodes. Such a structure can be meaningless for GNNs because almost
all the nodes receive the information from their neighbors. In this case, the learned features X‚Ä≤ play
a major role in training GNN parameters, indicating that the original training graph of Reddit is not
very informative, aligning with our observations in Section 4.2."
CONCLUSION,0.4,"5
CONCLUSION"
CONCLUSION,0.403125,"The prevalence of large-scale graphs poses great challenges in training graph neural networks. Thus,
we study a novel problem of graph condensation which targets at condensing a large-real graph
into a small-synthetic one while maintaining the performances of GNNs. Through our proposed
framework, we are able to signiÔ¨Åcantly reduce the graph size while approximating the original per-
formance. The condensed graphs take much less space of storage and can be used to efÔ¨Åciently
train various GNN architectures. Future work can be done on (1) improving the transferability of
condensed graphs for different GNNs, (2) studying graph condensation for other tasks such as graph
classiÔ¨Åcation and (3) designing condensation framework for multi-label datasets."
CONCLUSION,0.40625,Published as a conference paper at ICLR 2022
CONCLUSION,0.409375,ACKNOLWEDGEMENT
CONCLUSION,0.4125,"Wei Jin and Jiliang Tang are supported by the National Science Foundation (NSF) under grant num-
bers IIS1714741, CNS1815636, IIS1845081, IIS1907704, IIS1928278, IIS1955285, IOS2107215,
and IOS2035472, the Army Research OfÔ¨Åce (ARO) under grant number W911NF-21-1-0198, the
Home Depot, Cisco Systems Inc. and SNAP Inc."
ETHICS STATEMENT,0.415625,ETHICS STATEMENT
ETHICS STATEMENT,0.41875,"To the best of our knowledge, there are no ethical issues with this paper."
REPRODUCIBILITY STATEMENT,0.421875,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.425,"To ensure reproducibility of our experiments, we provide our source code at https://github.
com/ChandlerBang/GCond. The hyper-parameters are described in details in the appendix.
We also provide a pseudo-code implementation of our framework in the appendix."
REFERENCES,0.428125,REFERENCES
REFERENCES,0.43125,"Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi,
Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al.
Relational inductive biases, deep learning, and graph networks. ArXiv preprint, 2018."
REFERENCES,0.434375,"Ondrej Bohdal, Yongxin Yang, and Timothy Hospedales. Flexible dataset distillation: Learn labels
instead of images. ArXiv preprint, 2020."
REFERENCES,0.4375,"Chen Cai, Dingkang Wang, and Yusu Wang. Graph coarsening with neural networks. In 9th Inter-
national Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7,
2021, 2021."
REFERENCES,0.440625,"Francisco M Castro, Manuel J Mar¬¥ƒ±n-Jim¬¥enez, Nicol¬¥as Guil, Cordelia Schmid, and Karteek Alahari.
End-to-end incremental learning. In Proceedings of the European conference on computer vision
(ECCV), 2018."
REFERENCES,0.44375,"Yu Chen, Lingfei Wu, and Mohammed J. Zaki.
Iterative deep graph learning for graph neural
networks: Better and robust node embeddings. In Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual, 2020."
REFERENCES,0.446875,"Micha¬®el Defferrard, Xavier Bresson, and Pierre Vandergheynst.
Convolutional neural networks
on graphs with fast localized spectral Ô¨Åltering. In Advances in Neural Information Processing
Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-
10, 2016, Barcelona, Spain, 2016."
REFERENCES,0.45,"Chenhui Deng, Zhiqiang Zhao, Yongyu Wang, Zhiru Zhang, and Zhuo Feng. Graphzoom: A multi-
level spectral approach for accurate and scalable graph embedding. In 8th International Confer-
ence on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020."
REFERENCES,0.453125,"Xiaowen Dong, Dorina Thanou, Pascal Frossard, and Pierre Vandergheynst. Learning laplacian
matrix in smooth graph signal representations. IEEE Transactions on Signal Processing, (23),
2016."
REFERENCES,0.45625,"David Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, Rafael G¬¥omez-Bombarelli, Timo-
thy Hirzel, Al¬¥an Aspuru-Guzik, and Ryan P. Adams. Convolutional networks on graphs for learn-
ing molecular Ô¨Ångerprints. In Advances in Neural Information Processing Systems 28: Annual
Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal,
Quebec, Canada, 2015."
REFERENCES,0.459375,"Hilmi E Egilmez, Eduardo Pavez, and Antonio Ortega. Graph learning from data under laplacian
and structural constraints. IEEE Journal of Selected Topics in Signal Processing, (6), 2017."
REFERENCES,0.4625,Published as a conference paper at ICLR 2022
REFERENCES,0.465625,"Wenqi Fan, Yao Ma, Qing Li, Yuan He, Yihong Eric Zhao, Jiliang Tang, and Dawei Yin. Graph
neural networks for social recommendation. In The World Wide Web Conference, WWW 2019,
San Francisco, CA, USA, May 13-17, 2019, 2019."
REFERENCES,0.46875,"Reza Zanjirani Farahani and Masoud Hekmatfar. Facility location: concepts, models, algorithms
and case studies. 2009."
REFERENCES,0.471875,"Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric.
ArXiv preprint, 2019."
REFERENCES,0.475,"Luca Franceschi, Mathias Niepert, Massimiliano Pontil, and Xiao He. Learning discrete structures
for graph neural networks. In Proceedings of the 36th International Conference on Machine
Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, Proceedings of Machine
Learning Research, 2019."
REFERENCES,0.478125,"Hongyang Gao and Shuiwang Ji. Graph u-nets. In Proceedings of the 36th International Conference
on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, Proceedings
of Machine Learning Research, 2019."
REFERENCES,0.48125,"William L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large
graphs. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, 2017."
REFERENCES,0.484375,"Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta,
and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. In Ad-
vances in Neural Information Processing Systems 33: Annual Conference on Neural Information
Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020."
REFERENCES,0.4875,"Qian Huang, Horace He, Abhay Singh, Ser-Nam Lim, and Austin R. Benson. Combining label prop-
agation and simple models out-performs graph neural networks. In 9th International Conference
on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021a."
REFERENCES,0.490625,"Zengfeng Huang, Shengzhong Zhang, Chong Xi, Tang Liu, and Min Zhou. Scaling up graph neural
networks via graph coarsening. In In Proceedings of the 27th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining (KDD ‚Äô21), 2021b."
REFERENCES,0.49375,"Joseph J. Pfeiffer III, Sebasti¬¥an Moreno, Timothy La Fond, Jennifer Neville, and Brian Gallagher.
Attributed graph models: modeling network structure with correlated attributes. In 23rd Inter-
national World Wide Web Conference, WWW ‚Äô14, Seoul, Republic of Korea, April 7-11, 2014,
2014."
REFERENCES,0.496875,"Wei Jin, Yao Ma, Xiaorui Liu, Xianfeng Tang, Suhang Wang, and Jiliang Tang. Graph structure
learning for robust graph neural networks. In KDD ‚Äô20: The 26th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, 2020."
REFERENCES,0.5,"David R Karger. Random sampling in cut, Ô¨Çow, and network design problems. Mathematics of
Operations Research, (2), 1999."
REFERENCES,0.503125,"Thomas N. Kipf and Max Welling. Semi-supervised classiÔ¨Åcation with graph convolutional net-
works. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France,
April 24-26, 2017, Conference Track Proceedings, 2017."
REFERENCES,0.50625,"Johannes Klicpera, Aleksandar Bojchevski, and Stephan G¬®unnemann.
Predict then propagate:
Graph neural networks meet personalized pagerank. In 7th International Conference on Learning
Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2019."
REFERENCES,0.509375,"Guohao Li, Matthias M¬®uller, Ali K. Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep
as cnns? In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul,
Korea (South), October 27 - November 2, 2019, 2019."
REFERENCES,0.5125,"Meng Liu, Hongyang Gao, and Shuiwang Ji. Towards deeper graph neural networks. In Proceedings
of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining.
ACM, 2020."
REFERENCES,0.515625,Published as a conference paper at ICLR 2022
REFERENCES,0.51875,"Xiaorui Liu, Wei Jin, Yao Ma, Yaxin Li, Hua Liu, Yiqi Wang, Ming Yan, and Jiliang Tang. Elastic
graph neural networks. In Proceedings of the 38th International Conference on Machine Learn-
ing, ICML 2021, 18-24 July 2021, Virtual Event, Proceedings of Machine Learning Research,
2021."
REFERENCES,0.521875,"Andreas Loukas. Graph reduction with spectral and cut guarantees. J. Mach. Learn. Res., (116),
2019."
REFERENCES,0.525,"Andreas Loukas and Pierre Vandergheynst. Spectrally approximating large graphs with smaller
graphs. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018,
Stockholmsm¬®assan, Stockholm, Sweden, July 10-15, 2018, Proceedings of Machine Learning Re-
search, 2018."
REFERENCES,0.528125,"Xiaojun Ma, Ziyao Li, Lingjun Xu, Guojie Song, Yi Li, and Chuan Shi. Learning discrete adaptive
receptive Ô¨Åelds for graph convolutional networks, 2021."
REFERENCES,0.53125,"Yao Ma, Xiaorui Liu, Tong Zhao, Yozen Liu, Jiliang Tang, and Neil Shah. A uniÔ¨Åed view on graph
neural networks as graph signal denoising. ArXiv preprint, 2020."
REFERENCES,0.534375,"Timothy Nguyen, Zhourong Chen, and Jaehoon Lee.
Dataset meta-learning from kernel ridge-
regression. In 9th International Conference on Learning Representations, ICLR 2021, Virtual
Event, Austria, May 3-7, 2021, 2021."
REFERENCES,0.5375,"David Peleg and Alejandro A Sch¬®affer. Graph spanners. Journal of graph theory, (1), 1989."
REFERENCES,0.540625,"Sylvestre-Alvise RebufÔ¨Å, Alexander Kolesnikov, Georg Sperl, and Christoph H. Lampert. icarl:
Incremental classiÔ¨Åer and representation learning. In 2017 IEEE Conference on Computer Vision
and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, 2017."
REFERENCES,0.54375,"Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set
approach. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver,
BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018."
REFERENCES,0.546875,"Cosma Rohilla Shalizi and Andrew C Thomas.
Homophily and contagion are generically con-
founded in observational social network studies. Sociological methods & research, (2), 2011."
REFERENCES,0.55,"Daniel A Spielman and Shang-Hua Teng.
Spectral sparsiÔ¨Åcation of graphs.
SIAM Journal on
Computing, (4), 2011."
REFERENCES,0.553125,"Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine
learning research, (11), 2008."
REFERENCES,0.55625,"Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua
Bengio. Graph attention networks. In 6th International Conference on Learning Representations,
ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings,
2018."
REFERENCES,0.559375,"Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei A Efros. Dataset distillation. ArXiv
preprint, 2018."
REFERENCES,0.5625,"Max Welling. Herding dynamical weights to learn. In Proceedings of the 26th Annual International
Conference on Machine Learning, ICML 2009, Montreal, Quebec, Canada, June 14-18, 2009,
ACM International Conference Proceeding Series, 2009."
REFERENCES,0.565625,"Felix Wu, Amauri H. Souza Jr., Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Q. Weinberger.
Simplifying graph convolutional networks. In Proceedings of the 36th International Conference
on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, Proceedings
of Machine Learning Research, 2019a."
REFERENCES,0.56875,"Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S Yu. A
comprehensive survey on graph neural networks. ArXiv preprint, 2019b."
REFERENCES,0.571875,Published as a conference paper at ICLR 2022
REFERENCES,0.575,"Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and Jure
Leskovec. Graph convolutional neural networks for web-scale recommender systems. In Pro-
ceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data
Mining, KDD 2018, London, UK, August 19-23, 2018, 2018a."
REFERENCES,0.578125,"Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, William L. Hamilton, and Jure Leskovec.
Hierarchical graph representation learning with differentiable pooling. In Advances in Neural In-
formation Processing Systems 31: Annual Conference on Neural Information Processing Systems
2018, NeurIPS 2018, December 3-8, 2018, Montr¬¥eal, Canada, 2018b."
REFERENCES,0.58125,"Yuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. Graph contrastive learning auto-
mated. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021,
18-24 July 2021, Virtual Event, Proceedings of Machine Learning Research, 2021."
REFERENCES,0.584375,"Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor K. Prasanna.
Graphsaint: Graph sampling based inductive learning method. In 8th International Conference
on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020."
REFERENCES,0.5875,"Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. An end-to-end deep learning
architecture for graph classiÔ¨Åcation. In Proceedings of the Thirty-Second AAAI Conference on
ArtiÔ¨Åcial Intelligence, (AAAI-18), the 30th innovative Applications of ArtiÔ¨Åcial Intelligence (IAAI-
18), and the 8th AAAI Symposium on Educational Advances in ArtiÔ¨Åcial Intelligence (EAAI-18),
New Orleans, Louisiana, USA, February 2-7, 2018, 2018."
REFERENCES,0.590625,"Bo Zhao and Hakan Bilen. Dataset condensation with differentiable siamese augmentation. In
Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July
2021, Virtual Event, Proceedings of Machine Learning Research, 2021."
REFERENCES,0.59375,"Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. Dataset condensation with gradient matching.
In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria,
May 3-7, 2021, 2021."
REFERENCES,0.596875,"Lingxiao Zhao, Wei Jin, Leman Akoglu, and Neil Shah. From stars to subgraphs: Uplifting any
GNN with local structure awareness. In International Conference on Learning Representations,
2022. URL https://openreview.net/forum?id=Mspk_WYKoEH."
REFERENCES,0.6,"Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li,
and Maosong Sun. Graph neural networks: A review of methods and applications. ArXiv preprint,
2018."
REFERENCES,0.603125,"Kaixiong Zhou, Ninghao Liu, Fan Yang, Zirui Liu, Rui Chen, Li Li, Soo-Hyun Choi, and
Xia Hu.
Adaptive label smoothing to regularize large-scale graph training.
arXiv preprint
arXiv:2108.13555, 2021."
REFERENCES,0.60625,"Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai Koutra. Beyond
homophily in graph neural networks: Current limitations and effective designs. In Advances in
Neural Information Processing Systems 33: Annual Conference on Neural Information Process-
ing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020."
REFERENCES,0.609375,"Ligeng Zhu, Zhijian Liu, and Song Han. Deep leakage from gradients. In Advances in Neural In-
formation Processing Systems 32: Annual Conference on Neural Information Processing Systems
2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, 2019."
REFERENCES,0.6125,"Meiqi Zhu, Xiao Wang, Chuan Shi, Houye Ji, and Peng Cui. Interpreting and unifying graph neural
networks with an optimization framework. In Proceedings of the Web Conference 2021, 2021."
REFERENCES,0.615625,Published as a conference paper at ICLR 2022
REFERENCES,0.61875,"A
DATASETS AND HYPER-PARAMETERS"
REFERENCES,0.621875,"A.1
DATASETS"
REFERENCES,0.625,"We evaluate the proposed framework on three transductive datasets, i.e., Cora, Citeseer (Kipf &
Welling, 2017) and Ogbn-arxiv (Hu et al., 2020), and two inductive datasets, i.e., Flickr (Zeng et al.,
2020) and Reddit (Hamilton et al., 2017). Since all the datasets have public splits, we download them
from PyTorch Geometric (Fey & Lenssen, 2019) and use those splits throughout the experiments.
Dataset statistics are shown in Table 6."
REFERENCES,0.628125,"Table 6: Dataset statistics. The Ô¨Årst three are transductive datasets and the last two are inductive
datasets."
REFERENCES,0.63125,"Dataset
#Nodes
#Edges
#Classes
#Features
Training/Validation/Test"
REFERENCES,0.634375,"Cora
2,708
5,429
7
1,433
140/500/1000
Citeseer
3,327
4,732
6
3,703
120/500/1000
Ogbn-arxiv
169,343
1,166,243
40
128
90,941/29,799/48,603"
REFERENCES,0.6375,"Flickr
89,250
899,756
7
500
44,625/22312/22313
Reddit
232,965
57,307,946
210
602
15,3932/23,699/55,334"
REFERENCES,0.640625,"A.2
HYPER-PARAMETER SETTING"
REFERENCES,0.64375,"Condensation Process. For DC, we tune the number of hidden layers in a range of {1, 2, 3} and
Ô¨Åx the number of hidden units to 256. We further tune the number of epochs for training DC in
a range of {100, 200, 400}. For GCOND, without speciÔ¨Åc mention, we adopt a 2-layer SGC (Wu
et al., 2019a) with 256 hidden units as the GNN used for gradient matching. The function gŒ¶ that
models the relationship between A‚Ä≤ and X‚Ä≤ is implemented as a multi-layer perceptron (MLP).
SpeciÔ¨Åcally, we adopt a 3-layer MLP with 128 hidden units for small graphs (Cora and Citeseer)
and 256 hidden units for large graphs (Flickr, Reddit and Ogbn-arxiv). We tune the training epoch for
GCOND in a range of {400, 600, 1000}. For GCOND-X, we tune the number of hidden layers in a
range of {1, 2, 3} and Ô¨Åx the number of hidden units to 256. We further tune the number of epochs
for training GCOND-X in a range of {100, 200, 400}. We tune the learning rate for all the methods
in a range of {0.1, 0.01, 0.001, 0.0001}. Furthermore, we set Œ¥ to be 0.05, 0.05, 0.01, 0.01, 0.01 for
Citeseer, Cora, Ogbn-arxiv, Flickr and Reddit, respectively."
REFERENCES,0.646875,"For the choices of condensation ratio r, we divide the discussion into two parts. The Ô¨Årst part is
about transductive datasets. For Cora and Citeseer, since their labeling rates are very small (5.2%
and 3.6%, respectively), we choose r to be {25%, 50%, 100%} of the labeling rate. Thus, we Ô¨Ånally
choose {1.3%, 2.6%, 5.2%} for Cora and {0.9%, 1.8%, 3.6%} for Citeseer. For Ogbn-arxiv, we
choose r to be {0.1%, 0.5%, 1%} of its labeling rate (53%), thus being {0.05%, 0.25%, 0.5%}. The
second part is about inductive datasets. As the nodes in the training graphs are all labeled in inductive
datasets, we simply choose {0.1%, 0.5%, 0.1%} for Flickr and 0.05%, 0.1%, 0.2% for Reddit."
REFERENCES,0.65,"Evaluation Process. During the evaluation process, we set dropout rate to be 0 and weight decay to
be 0.0005 when training various GNNs. The number of epochs is set to 3000 for GAT while it is set
to 600 for other models. The initial learning rate is set to 0.01."
REFERENCES,0.653125,"Settings for Table 3 and Table 4. In both condensation stage and evaluation stage, we set the depth
of GNNs to 2. During condensation stage, we set weight decay to 0, dropout to 0 and training epochs
to 1000. During evaluation stage, we set weight decay to 0.0005, dropout to 0 and training epochs
to 600."
REFERENCES,0.65625,"A.3
TRAINING DETAILS OF DC-GRAPH, GCOND-X AND GCOND"
REFERENCES,0.659375,"DC-Graph: During the condensation stage, DC-Graph only leverages the node features to produce
condensed node features X‚Ä≤. During the training stage of evaluation, DC-Graph takes the condensed
features X‚Ä≤ together with an identity matrix as the graph structure to train a GNN. In the later test
stage of evaluation, the GNN takes both test node features and test graph structure as input to make
predictions for test nodes."
REFERENCES,0.6625,Published as a conference paper at ICLR 2022
REFERENCES,0.665625,"GCOND-X: During the condensation stage, GCOND-X leverages both the graph structure and node
features to produce condensed node features X‚Ä≤. During the training stage of evaluation, GCOND-X
takes the condensed features X‚Ä≤ together with an identity matrix as the graph structure to train a
GNN. In the later test stage of evaluation, the GNN takes both test node features and test graph
structure as input to make predictions for test nodes."
REFERENCES,0.66875,"GCOND: During the condensation stage, GCOND leverages both the graph structure and node
features to produce condensed graph data (A‚Ä≤, X‚Ä≤). During the training stage of evaluation, GCOND
takes the condensed data (A‚Ä≤, X‚Ä≤) to train a GNN. In the later test stage of evaluation, the GNN takes
both test node features and test graph structure as input to make predictions for test nodes."
REFERENCES,0.671875,"B
ALGORITHM"
REFERENCES,0.675,"We show the detailed algorithm of GCOND in Algorithm 1. In detail, we Ô¨Årst set the condensed
label set Y‚Ä≤ to Ô¨Åxed values and initialize X‚Ä≤ as node features randomly selected from each class. In
each outer loop, we sample a GNN model initialization Œ∏ from a distribution PŒ∏. Then, for each
class we sample the corresponding node batches from T and S, and calculate the gradient matching
loss within each class. The sum of losses from different classes are used to update X‚Ä≤ or Œ¶. After
that we update the GNN parameters for œÑŒ∏ epochs. When Ô¨Ånishing the updating of condensed graph
parameters, we use A‚Ä≤ = ReLU(gŒ¶(X‚Ä≤) ‚àíŒ¥) to obtain the Ô¨Ånal sparsiÔ¨Åed graph structure."
REFERENCES,0.678125,Algorithm 1: GCOND for Graph Condensation
REFERENCES,0.68125,"1 Input: Training data T = (A, X, Y), pre-deÔ¨Åned condensed labels Y‚Ä≤"
REFERENCES,0.684375,2 Initialize X‚Ä≤ as node features randomly selected from each class
REFERENCES,0.6875,"3 for k = 0, . . . , K ‚àí1 do"
REFERENCES,0.690625,"4
Initialize Œ∏0 ‚àºPŒ∏0
5
for t = 0, . . . , T ‚àí1 do"
REFERENCES,0.69375,"6
D‚Ä≤ = 0"
REFERENCES,0.696875,"7
for c = 0, . . . , C ‚àí1 do"
REFERENCES,0.7,"8
Compute A‚Ä≤ = gŒ¶(X‚Ä≤); then S = {A‚Ä≤, X‚Ä≤, Y‚Ä≤}"
REFERENCES,0.703125,"9
Sample (Ac, Xc, Yc) ‚àºT and (A‚Ä≤
c, X‚Ä≤
c, Y‚Ä≤
c) ‚àºS
‚ñ∑detailed in Section 3.1"
REFERENCES,0.70625,"10
Compute LT = L (GNNŒ∏t(Ac, Xc), Yc) and LS = L (GNNŒ∏t(A‚Ä≤
c, X‚Ä≤
c), Y‚Ä≤
c)"
REFERENCES,0.709375,"11
D‚Ä≤ ‚ÜêD‚Ä≤ + D(‚àáŒ∏tLT , ‚àáŒ∏tLS)"
REFERENCES,0.7125,"12
if t%(œÑ1 + œÑ2) < œÑ1 then"
REFERENCES,0.715625,"13
Update X‚Ä≤ ‚ÜêX‚Ä≤ ‚àíŒ∑1‚àáX‚Ä≤D‚Ä≤"
ELSE,0.71875,"14
else"
ELSE,0.721875,"15
Update Œ¶ ‚ÜêŒ¶ ‚àíŒ∑2‚àáŒ¶D‚Ä≤"
ELSE,0.725,"16
Update Œ∏t+1 ‚ÜêoptŒ∏(Œ∏t, S, œÑŒ∏)
‚ñ∑œÑŒ∏ is the number of steps for updating Œ∏"
ELSE,0.728125,17 A‚Ä≤ = ReLU(gŒ¶(X‚Ä≤) ‚àíŒ¥)
ELSE,0.73125,"18 Return: (A‚Ä≤, X‚Ä≤, Y‚Ä≤)"
ELSE,0.734375,"C
MORE EXPERIMENTS"
ELSE,0.7375,"C.1
ABLATION STUDY"
ELSE,0.740625,"Different Parameterization. We study the effect of different parameterizations for modeling A‚Ä≤
and compare GCOND with modeling A‚Ä≤ as free parameters in Table 7. From the table, we observe a
signiÔ¨Åcant improvement by taking into account the relationship between A‚Ä≤ and X‚Ä≤. This suggests
that directly modeling the structure as a function of features can ease the optimization and lead to
better condensed graph data."
ELSE,0.74375,"Joint optimization versus alternate optimization. We perform the ablation study on joint opti-
mization and alternate optimization when updating Œ¶ and X‚Ä≤. The results are shown in Table 8.
From the table, we can observe that joint optimization always gives worse performance and the
standard deviation is much higher than alternate optimization."
ELSE,0.746875,Published as a conference paper at ICLR 2022
ELSE,0.75,Table 7: Ablation study on different parametrizations.
ELSE,0.753125,"Parameters
Citeseer, r=1.8%
Cora, r=2.6%
Ogbn-arxiv, r=0.25%"
ELSE,0.75625,"A‚Ä≤, X‚Ä≤
62.2¬±4.8
75.5¬±0.6
63.0¬±0.5
Œ¶, X‚Ä≤
70.6¬±0.9
80.1¬±0.6
63.2¬±0.3"
ELSE,0.759375,Table 8: Ablation study on different optimization strategies.
ELSE,0.7625,"Citeseer, r=1.8%
Cora, r=2.6%
Ogbn-arxiv, r=0.25%
Flickr, r=0.5%
Reddit, r=0.1%"
ELSE,0.765625,"Joint
68.2¬±3.0
79.9¬±1.6
62.8¬±1.1
45.4¬±0.4
87.5¬±1.8
Alternate
70.6¬±0.9
80.1¬±0.6
63.2¬±0.3
47.1¬±0.1
89.5¬±0.8"
ELSE,0.76875,"C.2
NEURAL ARCHITECTURE SEARCH"
ELSE,0.771875,"We focus on APPNP instead of GCN since the architecture of APPNP involves more hyper-
parameters regarding its architecture setup. The detailed search space is shown as follows:"
ELSE,0.775,"(a) Number of propagation K:
we search the number of propagation K in the range of
{2, 4, 6, 8, 10}.
(b) Residual coefÔ¨Åcient Œ±: for the residual coefÔ¨Åcient in APPNP, we search it in the range of
{0.1, 0.2}.
(c) Hidden dimension: We collect the set of dimensions that are widely adopted by existing work
as the candidates, i.e., {16,32,64,128,256,512}.
(d) Activation function: The set of available activation functions is listed as follows: {Sigmoid,
Tanh, ReLU, Linear, Softplus, LeakyReLU, ReLU6, ELU}"
ELSE,0.778125,"In total, for each dataset we search 480 architectures of APPNP and we perform the search process
on Cora, Citeseer and Ogbn-arxiv. SpeciÔ¨Åcally, we train each architecture on the reduced graph for
epochs on as the model converges faster on the smaller graph. We use the best validation accuracy
to choose the Ô¨Ånal architecture. We report (1) the Pearson correlation between validation accuracies
obtained by architectures trained on condensed graphs and those trained on original graphs, and (2)
the average test accuracy of the searched architecture over 20 runs."
ELSE,0.78125,"Table 9: Neural Architecture Search. Methods are compared in validation accuracy correlation and
test accuracy obtained by searched architecture. Whole means the architecture is searched using
whole dataset."
ELSE,0.784375,"Pearson Correlation
Test Accuracy"
ELSE,0.7875,"Random
Herding
GCOND
Random
Herding
GCOND
Whole"
ELSE,0.790625,"Cora
0.40
0.21
0.76
82.9
82.9
83.1
82.6
Citeseer
0.56
0.29
0.79
71.4
71.3
71.3
71.6
Ogbn-arxiv
0.63
0.60
0.64
71.1
71.2
71.2
71.9"
ELSE,0.79375,"C.3
TIME COMPLEXITY AND RUNNING TIME"
ELSE,0.796875,"Time Complexity. We start from analyzing the time complexity of calculating gradient matching
loss, i.e., line 8 to line 11 in Algorithm 1. Let the number of MLP layers in gŒ¶ be L and r be the
number of sampled neighbors per node. For simplicity, we assume all the hidden units are d for all
layers and we use L-layer GCN for the analysis. The forward process of gŒ¶ has a complexity of
O(N ‚Ä≤2d2). The forward process of GCN on the original graph has a complexity of O(rLNd2) and
that on condensed graph has a complexity of O(LN ‚Ä≤2d + LN ‚Ä≤d). The complexity of calculating the
second-order derivatives in backward propagation has an additional factor of O(|Œ∏t||A‚Ä≤|+|Œ∏t||X‚Ä≤|),
which can be reduced to O(|Œ∏t| + |A‚Ä≤| + |X‚Ä≤|) with Ô¨Ånite difference approximation. Although there
are C iterations in line 7-11, we note that the process is easily parallelizable. Furthermore, the
process of updating Œ∏t in line 16 has a complexity of œÑŒ∏(LN ‚Ä≤2d + LN ‚Ä≤d). Considering there are
T iterations and K different initializations, we multiply the aforementioned complexity by KT. To"
ELSE,0.8,Published as a conference paper at ICLR 2022
ELSE,0.803125,"sum up, we can see that the time complexity linearly increases with number of nodes in the original
graph."
ELSE,0.80625,"Running Time. We now report the running time of the proposed GCOND for different condensation
rates. SpeciÔ¨Åcally, we vary the condensation rates in the range of {0.1%, 0.5%, 1%} on Ogbn-arxiv
and {1%, 5%, 10%} on Cora. The running time of 50 epochs on one single A100-SXM4 GPU is
reported in Table 10.The whole condensation process (1000 epochs) for generating 0.5% condensed
graph of Ogbn-arxiv takes around 2.4 hours, which is an acceptable cost given the huge beneÔ¨Åts of
the condensed graph."
ELSE,0.809375,Table 10: Running time of GCOND for 50 epochs.
ELSE,0.8125,"r
0.1%
0.5%
1%
r
1%
5%
10%"
ELSE,0.815625,"Ogbn-arxiv
348.6s
428.2s
609.8s
Cora
37.4s
43.9s
64.8s"
ELSE,0.81875,"C.4
SPARSIFICATION"
ELSE,0.821875,"In this subsection, we investigate the effect of threshold Œ¥ on the test accuracy and sparsity. In
detail, we vary the values of the threshold Œ¥ used for truncating adjacency matrix in a range of
{0.01, 0.05, 0.1, 0.2, 0.4, 0.6, 0.8}, and report the corresponding test accuracy and sparsity in Fig-
ure 3. From the Ô¨Ågure, we can see that increasing Œ¥ can effectively increase the sparsity of the
obtained adjacency matrix without affecting the performance too much."
ELSE,0.825,"0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Threshold 20 40 60 80 100"
ELSE,0.828125,Accuracy/Sparisity (%)
ELSE,0.83125,"acc
sparsity"
ELSE,0.834375,"(a) Cora, r=2.5%"
ELSE,0.8375,"0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Threshold 20 40 60 80 100"
ELSE,0.840625,Accuracy/Sparisity (%)
ELSE,0.84375,"acc
sparsity"
ELSE,0.846875,"(b) Citeseer, r=1.8%"
ELSE,0.85,"0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Threshold 0 10 20 30 40 50 60 70 80"
ELSE,0.853125,Accuracy/Sparisity (%)
ELSE,0.85625,"acc
sparsity"
ELSE,0.859375,"(c) Ogbn-arxiv, r=0.05%"
ELSE,0.8625,"0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Threshold 10 20 30 40 50 60 70 80 90"
ELSE,0.865625,Accuracy/Sparisity (%)
ELSE,0.86875,"acc
sparsity"
ELSE,0.871875,"(d) Flickr, r=0.1%"
ELSE,0.875,"0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Threshold 0 20 40 60 80"
ELSE,0.878125,Accuracy/Sparisity (%)
ELSE,0.88125,"acc
sparsity"
ELSE,0.884375,"(e) Reddit, r=0.1%"
ELSE,0.8875,Figure 3: Test accuracy and sparsity under different values of Œ¥.
ELSE,0.890625,"C.5
DIFFERENT DEPTH AND HIDDEN UNITS."
ELSE,0.89375,"Depth Versus Hidden Units. We vary the number of model layers (GCN) in a range of {1, 2, 3, 4}
and the number of hidden units in a range of {16, 32, 64, 128, 256}, and test them on the condensed
graphs of Cora and Citeseer. The results are reported in Table 11. From the table, we can observe
that changing the number of layers impacts the model performance a lot while changing the number
of units does not."
ELSE,0.896875,Published as a conference paper at ICLR 2022
ELSE,0.9,"Table 11: Test accuracy on different numbers of hidden units (H) and layers (L). When L=1, there
is no hidden layer so the number of hidden units is meaningless."
ELSE,0.903125,"(a) Cora, r=2.6%"
ELSE,0.90625,"H\L
1
2
3
4"
ELSE,0.909375,"16
74.8¬±0.5
76.8¬±1.0
68.0¬±3.0
50.9¬±9.5
32
-
79.2¬±0.7
70.4¬±3.2
61.1¬±7.2
64
-
79.2¬±1.0
72.0¬±3.3
64.5¬±2.2
128
-
79.9¬±0.3
76.6¬±1.8
61.8¬±3.8
256
-
80.1¬±0.6
75.9¬±1.6
65.6¬±2.9"
ELSE,0.9125,"(b) Citeseer, r=1.8%"
ELSE,0.915625,"H\L
1
2
3
4"
ELSE,0.91875,"16
58.6¬±12.1
69.2¬±1.3
56.9¬±8.4
40.4¬±1.2
32
-
69.4¬±1.3
59.9¬±10.2
42.6¬±3.6
64
-
69.7¬±1.5
62.3¬±10.3
43.6¬±3.7
128
-
70.2¬±1.4
63.3¬±9.7
51.6¬±1.8
256
-
70.6¬±0.9
63.5¬±10.0
52.9¬±5.5"
ELSE,0.921875,"Table 12: Cross-depth accuracy on Cora, r=2.6%"
ELSE,0.925,"C\T
2
3
4
5
6"
ELSE,0.928125,"2
80.30
80.70
79.46
76.06
71.23
3
40.62
72.37
40.14
67.19
35.02
4
74.24
72.56
76.26
71.70
65.12
5
71.31
75.73
70.95
73.13
67.12
6
75.20
75.18
75.67
76.16
75.00"
ELSE,0.93125,"Propagation Versus Transformation. We further study the effect of propagation and transforma-
tion on the condensed graph. We use Cora as an example and use SGC as the test model due to its
decoupled architecture. SpeciÔ¨Åcally, we vary both the propagation layers and transformation layers
of SGC in the range of {1, 2, 3, 4, 5}, and report the performance in Table 13. As can be seen, the
condensed graph still achieves good performance with 3 and 4 layers of propagation. Although the
condensed graph is generated under 2-layer SGC, it is able to generalize to 3-layer and 4-layer SGC.
When increasing the propagation to 5, the performance degrades a lot which could be the cause of
the oversmoothing issue. On the other hand, stacking more transformation layers can Ô¨Årst help boost
the performance but then hurt. Given the small scale of the graph, SGC suffers the overÔ¨Åtting issue
in this case."
ELSE,0.934375,"Cross-depth Performance. We show the cross-depth performance in Table 12. SpeciÔ¨Åcally, we use
SGC of different depth in the condensation to generate condensed graphs and then use them to test
on SGC of different depth. Note that in this table, we set weight decay to 0 and dropout to 0.5. We
can observe that usgin a deeper GNN is not always helpful. Stacking more layers do not necessarily
mean we can learn better condensed graphs since more nodes are involved during the optimization,
and this makes optimization more difÔ¨Åcult."
ELSE,0.9375,"Table 13: Test accuracy of SGC on different transformations and propagations for Cora, r=2.6%"
ELSE,0.940625,"Trans\Prop
1
2
3
4
5"
ELSE,0.94375,"1
77.09¬±0.43
79.02¬±1.17
78.12¬±2.13
74.04¬±3.60
61.19¬±7.73
2
76.94¬±0.50
79.01¬±0.57
79.11¬±1.15
77.57¬±1.03
72.37¬±4.25
3
75.28¬±0.58
77.95¬±0.67
74.16¬±1.50
70.58¬±3.71
58.28¬±8.90
4
66.87¬±0.73
66.54¬±0.82
59.24¬±1.60
43.94¬±6.33
30.45¬±9.67
5
46.44¬±0.91
37.29¬±3.23
16.05¬±2.74
15.33¬±2.79
15.33¬±2.79"
ELSE,0.946875,"C.6
VISUALIZATION OF NODE FEATURES."
ELSE,0.95,"In addition, we provide the t-SNE (Van der Maaten & Hinton, 2008) plots of condensed node fea-
tures to further understand the condensed graphs. In Cora and Citeseer, the condensed node features
are well clustered. For Ogbn-arxiv and Reddit, we also observe some clustered pattern for the nodes
within the same class. In contrast, the condensed features are less discriminative in Flickr, which
indicates that the condensed structure information can be essential in training GNN."
ELSE,0.953125,Published as a conference paper at ICLR 2022
ELSE,0.95625,"(a) Cora, r=2.5%
(b) Citeseer, r=1.8%
(c) Arxiv, r=0.05%
(d) Flickr, r=0.1%
(e) Reddit, r=0.1%"
ELSE,0.959375,Figure 4: The t-SNE plots of node features in condensed graphs.
ELSE,0.9625,"C.7
PERFORMANCES ON ORIGINAL GRAPHS"
ELSE,0.965625,We show the performances of various GNNs on original graphs in Table 14 to serve as references.
ELSE,0.96875,Table 14: Performances of various GNNs on original graphs. SAGE: GraphSAGE.
ELSE,0.971875,"GAT
Cheby
SAGE
SGC
APPNP
GCN"
ELSE,0.975,"Cora
83.1
81.4
81.2
81.4
83.1
81.2
Citeseer
70.8
70.2
70.1
71.3
71.8
71.7
Ogbn-arxiv
71.5
71.4
71.5
71.4
71.2
71.7
Flickr
44.3
47.0
46.1
46.2
47.3
47.1
Reddit
91.0
93.1
93.0
93.5
94.3
93.9"
ELSE,0.978125,"C.8
EXPERIMENTS ON PUBMED."
ELSE,0.98125,"We also show the experiments for Pubmed with condensation ratio of 0.3% in Table 15. From the
table, we can observe that GCOND approximates the original performance very well (77.92% vs.
79.32% on GCN). It also generalizes well to different architectures and outperforms GCOND-X and
DC-Graph, indicating that it is important to leverage the graph structure information and learn a
condensed structure."
ELSE,0.984375,Table 15: Performance of different GNNs on Pubmed (r=0.3%).
ELSE,0.9875,"APPNP
Cheby
GCN
GraphSage
SGC"
ELSE,0.990625,"DC-Graph
72.76¬±1.39
72.66¬±0.59
72.44¬±2.90
71.96¬±2.50
75.43¬±0.65
GCOND-X
73.91¬±0.41
74.57¬±1.00
71.81¬±0.94
73.10¬±2.08
76.72¬±0.65
GCOND
76.77¬±1.17
75.48¬±0.82
77.92¬±0.42
71.12¬±3.10
75.91¬±1.38"
ELSE,0.99375,"D
MORE RELATED WORK"
ELSE,0.996875,"Graph pooling. Graph pooling (Zhang et al., 2018; Ying et al., 2018b; Gao & Ji, 2019) also gener-
ates a coarsened graph with smaller size. Zhang et al. (2018) is one the Ô¨Årst to propose an end-to-end
architecture for graph classiÔ¨Åcation by incorporating graph pooling. Later, DiffPool (Ying et al.,
2018b) proposes to use GNNs to parameterize the process of node grouping. However, those meth-
ods are majorly tailored for the graph classiÔ¨Åcation task and the coarsened graphs are a byproduct
graph during the representation learning process."
