Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0029585798816568047,"Predicting molecular conformations from molecular graphs is a fundamental prob-
lem in cheminformatics and drug discovery. Recently, signiﬁcant progress has been
achieved with machine learning approaches, especially with deep generative mod-
els. Inspired by the diffusion process in classical non-equilibrium thermodynamics
where heated particles will diffuse from original states to a noise distribution, in
this paper, we propose a novel generative model named GEODIFF for molecular
conformation prediction. GEODIFF treats each atom as a particle and learns to
directly reverse the diffusion process (i.e., transforming from a noise distribution
to stable conformations) as a Markov chain. Modeling such a generation process
is however very challenging as the likelihood of conformations should be roto-
translational invariant. We theoretically show that Markov chains evolving with
equivariant Markov kernels can induce an invariant distribution by design, and
further propose building blocks for the Markov kernels to preserve the desirable
equivariance property. The whole framework can be efﬁciently trained in an end-to-
end fashion by optimizing a weighted variational lower bound to the (conditional)
likelihood. Experiments on multiple benchmarks show that GEODIFF is superior or
comparable to existing state-of-the-art approaches, especially on large molecules.1"
INTRODUCTION,0.005917159763313609,"1
INTRODUCTION"
INTRODUCTION,0.008875739644970414,"Graph representation learning has achieved huge success for molecule modeling in various tasks rang-
ing from property prediction (Gilmer et al., 2017; Duvenaud et al., 2015) to molecule generation (Jin
et al., 2018; Shi et al., 2020), where typically a molecule is represented as an atom-bond graph.
Despite its effectiveness in various applications, a more intrinsic and informative representation
for molecules is the 3D geometry, also known as conformation, where atoms are represented as
their Cartesian coordinates. The 3D structures determine the biological and physical properties of
molecules and hence play a key role in many applications such as computational drug and material de-
sign (Thomas et al., 2018; Gebauer et al., 2021; Jing et al., 2021; Batzner et al., 2021). Unfortunately,
how to predict stable molecular conformation remains a challenging problem. Traditional methods
based on molecular dynamics (MD) or Markov chain Monte Carlo (MCMC) are very computationally
expensive, especially for large molecules (Hawkins, 2017)."
INTRODUCTION,0.011834319526627219,"Recently, signiﬁcant progress has been made with machine learning approaches, especially with
deep generative models. For example, Simm & Hernandez-Lobato (2020); Xu et al. (2021b) studied
predicting atomic distances with variational autoencoders (VAEs) (Kingma & Welling, 2013) and
ﬂow-based models (Dinh et al., 2017) respectively. Shi et al. (2021) proposed to use denoising score
matching (Song & Ermon, 2019; 2020) to estimate the gradient ﬁelds over atomic distances, through
which the gradient ﬁelds over atomic coordinates can be calculated. Ganea et al. (2021) studied
generating conformations by predicting both bond lengths and angles. As molecular conformations
are roto-translational invariant, these approaches circumvent directly modeling atomic coordinates by
leveraging intermediate geometric variables such as atomic distances, bond and torsion angles, which"
INTRODUCTION,0.014792899408284023,1Code is available at https://github.com/MinkaiXu/GeoDiff.
INTRODUCTION,0.01775147928994083,Published as a conference paper at ICLR 2022
INTRODUCTION,0.020710059171597635,"are roto-translational invariant. As a result, they are able to achieve very compelling performance.
However, as all these approaches seek to indirectly model the intermediate geometric variables, they
have inherent limitations in either training or inference process (see Sec. 2 for a detailed description).
Therefore, an ideal solution would still be directly modeling the atomic coordinates and at the same
time taking the roto-translational invariance property into account."
INTRODUCTION,0.023668639053254437,"In this paper, we propose such a solution called GEODIFF, a principled probabilistic framework
based on denoising diffusion models (Sohl-Dickstein et al., 2015). Our approach is inspired by the
diffusion process in nonequilibrium thermodynamics (De Groot & Mazur, 2013). We view atoms
as particles in a thermodynamic system, which gradually diffuse from the original states to a noisy
distribution in contact with a heat bath. At each time step, stochastic noises are added to the atomic
positions. Our high-level idea is learning to reverse the diffusion process, which recovers the target
geometric distribution from the noisy distribution. In particular, inspired by recent progress of
denoising diffusion models on image generation (Ho et al., 2020; Song et al., 2020), we view the
noisy geometries at different timesteps as latent variables, and formulate both the forward diffusion
and reverse denoising process as Markov chains. Our goal is to learn the transition kernels such
that the reverse process can recover realistic conformations from the chaotic positions sampled
from a noise distribution. However, extending existing methods to geometric generation is highly
non-trivial: a direct application of diffusion models on the conformation generation task lead to poor
generation quality. As mentioned above, molecular conformations are roto-translational invariant,
i.e., the estimated (conditional) likelihood should be unaffected by translational and rotational
transformations (Köhler et al., 2020). To this end, we ﬁrst theoretically show that a Markov process
starting from an roto-translational invariant prior distribution and evolving with roto-translational
equivariant Markov kernels can induce an roto-translational invariant density function. We further
provide practical parameterization to deﬁne a roto-translational invariant prior distribution and a
Markov kernel imposing the equivariance constraints. In addition, we derive a weighted variational
lower bound of the conditional likelihood of molecular conformations, which also enjoys the roto-
translational invariance and can be efﬁciently optimized."
INTRODUCTION,0.026627218934911243,"A unique strength of GEODIFF is that it directly acts on the atomic coordinates and entirely bypasses
the usage of intermediate elements for both training and inference. This general formulation enjoys
several crucial advantages. First, the model can be naturally trained end-to-end without involving
any sophisticated techniques like bilevel programming (Xu et al., 2021b), which beneﬁts from small
optimization variances. Besides, instead of solving geometries from bond lengths or angles, the
one-stage sampling fashion avoids accumulating any intermediate error, and therefore leads to more
accurate predicted structures. Moreover, GEODIFF enjoys a high model capacity to approximate the
complex distribution of conformations. Thus, the model can better estimate the highly multi-modal
distribution and generate structures with high quality and diversity."
INTRODUCTION,0.029585798816568046,"We conduct comprehensive experiments on multiple benchmarks, including conformation generation
and property prediction tasks. Numerical results show that GEODIFF consistently outperforms
existing state-of-the-art machine learning approaches, and by a large margin on the more challenging
large molecules. The signiﬁcantly superior performance demonstrate the high capacity to model the
complex distribution of molecular conformations and generate both diverse and accurate molecules."
RELATED WORK,0.03254437869822485,"2
RELATED WORK"
RELATED WORK,0.03550295857988166,"Recently, various deep generative models have been proposed for conformation generation. Among
them, CVGAE (Mansimov et al., 2019) ﬁrst proposed a VAE model to directly generate 3D atomic
coordinates, which fails to preserve the roto-translation equivariance property of conformations and
suffers from poor performance. To address this problem, the majority of subsequent models are
based on intermediate geometric elements such as atomic distances and torsion angles. A favorable
property of these elements is the roto-translational invariance, (e.g. atomic distances does not
change when rotating the molecule), which has been shown to be an important inductive bias for
molecular geometry modeling (Köhler et al., 2020). However, such a decomposition suffers from
several drawbacks for either training or sampling. For example, GRAPHDG (Simm & Hernandez-
Lobato, 2020) and CGCF (Xu et al., 2021a) proposed to predict the interatomic distance matrix
by VAE and Flow respectively, and then solve the geometry through the Distance Geometry (DG)
technique (Liberti et al., 2014), which searches reasonable coordinates that matches with the predicted"
RELATED WORK,0.038461538461538464,Published as a conference paper at ICLR 2022
RELATED WORK,0.04142011834319527,"distances. CONFVAE further improves this pipeline by designing an end-to-end framework via bilevel
optimization (Xu et al., 2021b). However, all these approaches suffer from the accumulated error
problem, meaning that the noise in the predicted distances will misguide the coordinate searching
process and lead to inaccurate or even erroneous structures. To overcome this problem, CONFGF (Shi
et al., 2021; Luo et al., 2021) proposed to learn the gradient of the log-likelihood w.r.t coordinates.
However, in practice the model is still aided by intermediate geometric elements, in that it ﬁrst
estimates the gradient w.r.t interatomic distances via denoising score matching (DSM) (Song &
Ermon, 2019; 2020), and then derives the gradient of coordinates using the chain rule. The problem
is, by learning the distance gradient via DSM, the model is fed with perturbed distance matrices,
which may violate the triangular inequality or even contain negative values. As a consequence, the
model is actually learned over invalid distance matrices but tested with valid ones calculated from
coordinates, making it suffer from serious out-of-distribution (Hendrycks & Gimpel, 2016) problem.
Most recently, another concurrent work (Ganea et al., 2021) proposed a highly systematic (rule-based)
pipeline named GEOMOL, which learns to predict a minimal set of geometric quantities (i.e. length
and angles) and then reconstruct the local and global structures of the conformation in a sophisticated
procedure. Besides, there has also been efforts to use reinforcement learning for conformation
search Gogineni et al. (2020). Nevertheless, this method relies on rigid rotor approximation and can
only model the torsion angles, and thus fundamentally differs from other approaches."
PRELIMINARIES,0.04437869822485207,"3
PRELIMINARIES"
NOTATIONS AND PROBLEM DEFINITION,0.047337278106508875,"3.1
NOTATIONS AND PROBLEM DEFINITION"
NOTATIONS AND PROBLEM DEFINITION,0.05029585798816568,"Notations. In this paper each molecule with n atoms is represented as an undirected graph G = ⟨V, E⟩,
where V = {vi}n
i=1 is the set of vertices representing atoms and E = {eij | (i, j) ⊆|V| × |V|} is
the set of edges representing inter-atomic bonds. Each node vi ∈V describes the atomic attributes,
e.g., the element type. Each edge eij ∈E describes the corresponding connection between vi and
vj, and is labeled with its chemical type. In addition, we also assign the unconnected edges with a
virtual type. For the geometry, each atom in V is embedded by a coordinate vector c ∈R3 into the
3-dimensional space, and the full set of positions (i.e., the conformation) can be represented as a
matrix C = [c1, c2, · · · , cn] ∈Rn×3."
NOTATIONS AND PROBLEM DEFINITION,0.05325443786982249,"Problem Deﬁnition. The task of molecular conformation generation is a conditional generative
problem, where we are interested in generating stable conformations for a provided graph G. Given
multiple graphs G, and for each G given its conformations C as i.i.d samples from an underlying
Boltzmann distribution (Noé et al., 2019), our goal is learning a generative model pθ(C|G), which is
easy to draw samples from, to approximate the Boltzmann function."
EQUIVARIANCE,0.05621301775147929,"3.2
EQUIVARIANCE"
EQUIVARIANCE,0.05917159763313609,"Equivariance is ubiquitous in machine learning for atomic systems, e.g., the vectors of atomic dipoles
or forces should rotate accordingly w.r.t. the conformation coordinates (Thomas et al., 2018; Weiler
et al., 2018; Fuchs et al., 2020; Miller et al., 2020; Simm et al., 2021; Batzner et al., 2021). It has
shown effectiveness to integrate such inductive bias into model parameterization for modeling 3D
geometry, which is critical for the generalization capacity (Köhler et al., 2020; Satorras et al., 2021a).
Formally, a function F : X →Y is equivariant w.r.t a group G if:"
EQUIVARIANCE,0.0621301775147929,"F ◦Tg(x) = Sg ◦F(x),
(1)"
EQUIVARIANCE,0.0650887573964497,"where Tg and Sg are transformations for an element g ∈G, acting on the vector spaces X and
Y, respectively. In this work, we consider the SE(3) group, i.e., the group of rotation, translation
in 3D space. This requires the estimated likelihood unaffected with translational and rotational
transformations, and we will elaborate on how our method satisfy this property in Sec. 4."
GEODIFF METHOD,0.06804733727810651,"4
GEODIFF METHOD"
GEODIFF METHOD,0.07100591715976332,"In this section, we elaborate on the proposed equivariant diffusion framework. We ﬁrst present
a high level description of our 3D diffusion formulation in Sec. 4.1, based on recent progress of
denoising diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020). Then we emphasize several"
GEODIFF METHOD,0.07396449704142012,Published as a conference paper at ICLR 2022
GEODIFF METHOD,0.07692307692307693,"𝑪!
𝑪""
𝑪!""#
⋯"
GEODIFF METHOD,0.07988165680473373,"𝑝! 𝐶""#$ 𝒢, 𝐶"")"
GEODIFF METHOD,0.08284023668639054,"𝑞𝐶"" 𝐶""#$) 𝑪$
⋯"
GEODIFF METHOD,0.08579881656804733,"Figure 1: Illustration of the diffusion and reverse process of GEODIFF. For diffusion process,
noise from ﬁxed posterior distributions q(Ct|Ct−1) is gradually added until the conformation is de-
stroyed. Symmetrically, for generative process, an initial state CT is sampled from standard Gaussian
distribution, and the conformation is progressively reﬁned via the Markov kernels pθ(Ct−1|G, Ct)."
GEODIFF METHOD,0.08875739644970414,"non-trivial challenges of building diffusion models for geometry generation scenario, and show how
we technically tackle these issues. Speciﬁcally, in Sec. 4.2, we present how we parameterize pθ(C|G)
so that the conditional likelihood is roto-translational invariant, and in Sec. 4.3, we introduce our
surgery of the training objective to make the optimization also invariant of translation and rotation.
Finally, we brieﬂy show how to draw samples from our model in Sec. 4.4."
FORMULATION,0.09171597633136094,"4.1
FORMULATION"
FORMULATION,0.09467455621301775,"Let C0 denotes the ground truth conformations and let Ct for t = 1, · · · , T be a sequence of latent
variables with the same dimension, where t is the index for diffusion steps. Then a diffusion
probabilistic model (Sohl-Dickstein et al., 2015) can be described as a latent variable model with two
processes: the forward diffusion process, and the reverse generative process. Intuitively, the diffusion
process progressively injects small noises to the data C0, while the generative process learns to revert
the diffusion process by gradually eliminating the noise to recover the ground truth. We provide a
high-level schematic of the processes in Fig. 1."
FORMULATION,0.09763313609467456,"Diffusion process. Following the physical insight, we model the particles C as an evolving thermo-
dynamic system. With time going by, the equilibrium conformation C0 will gradually diffuse to the
next chaotic states Ct, and ﬁnally converge into a white noise distribution after T iterations. Different
from typical latent variable models, in diffusion model this forward process is deﬁned as a ﬁxed
(rather than trainable) posterior distribution q(C1:T |C0). Speciﬁcally, we deﬁne it as a Markov chain
according to a ﬁxed variance schedule β1, . . . , βT :"
FORMULATION,0.10059171597633136,"q(C1:T |C0) = T
Y"
FORMULATION,0.10355029585798817,"t=1
q(Ct|Ct−1),
q(Ct|Ct−1) = N(Ct;
p"
FORMULATION,0.10650887573964497,"1 −βtCt−1, βtI).
(2)"
FORMULATION,0.10946745562130178,"Note that, in this work we do not impose speciﬁc (invariance) requirement upon the diffusion process,
as long as it can efﬁciently draw noisy samples for training the generative process pθ(C0)."
FORMULATION,0.11242603550295859,"Let αt = 1 −βt and ¯αt = Qt
s=1 αs, a special property of the forward process is that q(Ct|C0) of
arbitrary timestep t can be calculated in closed form q(Ct|C0) = N(Ct; √¯αtC0, (1 −¯αt)I)2. This
indicates with sufﬁciently large T, the whole forward process will convert C0 to whitened isotropic
Gaussian, and thus it is natural to set p(CT ) as a standard Gaussian distribution."
FORMULATION,0.11538461538461539,"Reverse Process. Our goal is learning to recover conformations C0 from the white noise CT , given
speciﬁed molecular graphs G. We consider this generative procedure as a reverse dynamics of the
above diffusion process, starting from the noisy particles CT ∼p(CT ). We formulate this reverse
dynamics as a conditional Markov chain with learnable transitions:"
FORMULATION,0.11834319526627218,"pθ(C0:T −1|G, CT ) = T
Y"
FORMULATION,0.12130177514792899,"t=1
pθ(Ct−1|G, Ct),
pθ(Ct−1|G, Ct) = N(Ct−1; µθ(G, Ct, t), σ2
t I).
(3)"
FORMULATION,0.1242603550295858,"Herein µθ are parameterized neural networks to estimate the means, and σt can be any user-deﬁned
variance. The initial distribution p(CT ) is set as a standard Gaussian. Given a graph G, its 3D structure
is generated by ﬁrst drawing chaotic particles CT from p(CT ), and then iteratively reﬁned through
the reverse Markov kernels pθ(Ct−1|G, Ct)."
FORMULATION,0.12721893491124261,2Detailed derivations are provided in the Appendix A.
FORMULATION,0.1301775147928994,Published as a conference paper at ICLR 2022
FORMULATION,0.13313609467455623,"Having formulated the reverse dynamics, the marginal likelihood can be calculated by pθ(C0|G) =
R
p(CT )pθ(C0:T −1|G, CT )dC1:T . Herein a non-trivial problem is that the likelihood should be
invariant w.r.t translation and rotation, which has proved to be a critical inductive bias for 3D object
generation (Köhler et al., 2020; Satorras et al., 2021a). In the following subsections, we will elaborate
on how we parameterize the Markov kernels pθ(Ct−1|G, Ct) to achieve this desired property, and also
how to maximize this likelihood by taking the invariance into account."
EQUIVARIANT REVERSE GENERATIVE PROCESS,0.13609467455621302,"4.2
EQUIVARIANT REVERSE GENERATIVE PROCESS"
EQUIVARIANT REVERSE GENERATIVE PROCESS,0.1390532544378698,"Instead of directly leveraging existing methods, we consider building the density pθ(C0) that is
invariant to rotation and translation transformations. Intuitively, this requires the likelihood to be
unaffected by translations and rotations. Formally, let Tg be some roto-translational transformations
of a group element g ∈SE(3), then we have the following statement:
Proposition 1. Let p(xT ) be an SE(3)-invariant density function, i.e., p(xT ) = p(Tg(xT )). If
Markov transitions p(xt−1|xt) are SE(3)-equivariant, i.e., p(xt−1|xt) = p(Tg(xt−1)|Tg(xt)), then
we have that the density pθ(x0) =
R
p(xT )pθ(x0:T −1|xT )dx1:T is also SE(3)-invariant."
EQUIVARIANT REVERSE GENERATIVE PROCESS,0.14201183431952663,"This proposition indicates that the dynamics starting from an invariant standard density along an
equivariant Gaussian Markov kernel can result in an invariant density. Now we provide a practical
implementation of GEODIFF based on the recent denoising diffusion framework (Ho et al., 2020)."
EQUIVARIANT REVERSE GENERATIVE PROCESS,0.14497041420118342,"Invariant Initial Density p(CT ). We ﬁrst introduce the invariant distribution p(CT ), which will
also be employed in the equivariant Markov chain. We borrow the idea from Köhler et al. (2020)
to consider systems with zero center of mass (CoM), termed CoM-free systems. We deﬁne p(CT )
as a “CoM-free standard density” ˆρ(C), built upon an isotropic normal density ρ(C): for evaluating
the likelihood ˆρ(C) we can ﬁrstly translate C to zero CoM and then calculate ρ(C), and for sampling
from ˆρ(C) we can ﬁrst sample from ρ(C) and then move the CoM to zero."
EQUIVARIANT REVERSE GENERATIVE PROCESS,0.14792899408284024,"We provide a formal theoretical analysis of ˆρ(C) in Appendix A. Intuitively, the isotropic Gaussian
is manifestly invariant to rotations around the zero CoM. And by considering CoM-free system,
moving the particles to zero CoM can always ensure the translational invariance. Consequently, ˆρ(C)
is constructed as a roto-transitional invariant density."
EQUIVARIANT REVERSE GENERATIVE PROCESS,0.15088757396449703,"Equivariant Markov Kernels p(Ct−1|G, Ct). Similar to the prior density, we also consider equip-
ping all intermediate structures Ct as CoM-free systems. Speciﬁcally, given mean µθ(G, Ct, t) and
variance σt, the likelihood of Ct−1 will be calculated by ˆρ( Ct−1−µθ(G,Ct,t)"
EQUIVARIANT REVERSE GENERATIVE PROCESS,0.15384615384615385,"σt
). The CoM-free Gaussian
ensures the translation invariance in the Markov kernels. Consequently, to achieve the equivariant
property deﬁned in Proposition 1, we focus on the rotation equivariance."
EQUIVARIANT REVERSE GENERATIVE PROCESS,0.15680473372781065,"Then in general, the key requirement is to ensure the means µθ(G, Ct, t) to be roto-translation
equivariant w.r.t Ct. Following Ho et al. (2020), we consider the following parameterization of µθ:"
EQUIVARIANT REVERSE GENERATIVE PROCESS,0.15976331360946747,"µθ(Ct, t) =
1
√αt"
EQUIVARIANT REVERSE GENERATIVE PROCESS,0.16272189349112426,"
Ct −
βt
√1 −¯αt
ϵθ(G, Ct, t)

,
(4)"
EQUIVARIANT REVERSE GENERATIVE PROCESS,0.16568047337278108,"where ϵθ are neural networks with trainable parameters θ. Intuitively, the model ϵθ learns to predict the
noise necessary to decorrupt the conformations. This is analogous to the physical force ﬁelds (Schütt
et al., 2017; Zhang et al., 2018; Hu et al., 2021; Shuaibi et al., 2021), which also gradually push
particles towards convergence around the equilibrium states."
EQUIVARIANT REVERSE GENERATIVE PROCESS,0.16863905325443787,"Now the problem is transformed to constructing ϵθ to be roto-translational equivariant. We draw
inspirations from recent equivariant networks (Thomas et al., 2018; Satorras et al., 2021b) to design an
equivariant convolutional layer, named graph ﬁeld network (GFN). In the l-th layer, GFN takes node
embeddings hl ∈Rn×b (b denotes the feature dimension) and corresponding coordinate embeddings
xl ∈Rn×3 as inputs, and outputs hl+1 and xl+1 as follows:"
EQUIVARIANT REVERSE GENERATIVE PROCESS,0.17159763313609466,"mij = Φm
 
hl
i, hl
j, ∥xl
i −xl
j∥2, eij; θm

(5)"
EQUIVARIANT REVERSE GENERATIVE PROCESS,0.17455621301775148,"hl+1
i
= Φh

hl
i,
X"
EQUIVARIANT REVERSE GENERATIVE PROCESS,0.17751479289940827,"j∈N(i)
mij; θh

(6)"
EQUIVARIANT REVERSE GENERATIVE PROCESS,0.1804733727810651,"xl+1
i
=
X"
EQUIVARIANT REVERSE GENERATIVE PROCESS,0.1834319526627219,j∈N(i)
DIJ,0.1863905325443787,"1
dij
(ci −cj) Φx (mij; θx)
(7)"
DIJ,0.1893491124260355,Published as a conference paper at ICLR 2022
DIJ,0.19230769230769232,"where Φ are feed-forward networks and dij denotes interatomic distances. N(i) denotes the neighbor-
hood of ith node, including both connected atoms and other ones within a radius threshold τ, which
enables the model to explicitly capture long-range interactions and support molecular graphs with
disconnected components. Initial embeddings h0 are combinations of atom and timestep embeddings,
and x0 are atomic coordinates. The main difference between proposed GFN and other GNNs lies
in equation 7, where x is updated as a combination of radial directions weighted by Φx : Rb →R.
Such vector ﬁeld xL enjoys the roto-translation equivariance property. Formally, we have:"
DIJ,0.1952662721893491,"Proposition 2. Parameterizing ϵθ(G, C, t) as a composition of L GFN layers, and take the xL after
L updates as the output. Then the noise vector ﬁeld ϵθ is SE(3) equivariant w.r.t the 3D system C."
DIJ,0.19822485207100593,"Intuitively, given hl already invariant and xl equivariant, the message embedding m will also be
invariant since it only depends on invariant features. Since x is updated with the relative differences
ci −cj weighted by invariant features, it will be translation-invariant and rotation-equivariant. Then
inductively, composing ϵθ with L GFN layers enables equivariance with Ct. We provide the formal
proof of equivariance properties in Appendix A."
IMPROVED TRAINING OBJECTIVE,0.20118343195266272,"4.3
IMPROVED TRAINING OBJECTIVE"
IMPROVED TRAINING OBJECTIVE,0.20414201183431951,"Having formulated the generative process and the model parameterization, now we consider the
practical training objective for the reverse dynamics. Since directly optimizing the exact log-likelihood
is intractable, we instead maximize the usual variational lower bound (ELBO)3:"
IMPROVED TRAINING OBJECTIVE,0.20710059171597633,"E

log pθ(C0|G)

= E
h
log Eq(C1:T |C0)
pθ(C0:T |G)
q(C1:T |C0) i"
IMPROVED TRAINING OBJECTIVE,0.21005917159763313,"≥−Eq
h
T
X"
IMPROVED TRAINING OBJECTIVE,0.21301775147928995,"t=1
DKL(q(Ct−1|Ct, C0)∥pθ(Ct−1|Ct, G))
i
:= −LELBO
(8)"
IMPROVED TRAINING OBJECTIVE,0.21597633136094674,"where q(Ct−1|Ct, C0) is analytically tractable as N(
√¯αt−1βt"
IMPROVED TRAINING OBJECTIVE,0.21893491124260356,"1−¯αt
C0 +
√αt(1−¯αt−1)"
IMPROVED TRAINING OBJECTIVE,0.22189349112426035,"1−¯αt
Ct, 1−¯αt−1"
IMPROVED TRAINING OBJECTIVE,0.22485207100591717,"1−¯αt βt)3. Most
recently, Ho et al. (2020) showed that under the parameterization in equation 4, the ELBO of the
diffusion model can be further simpliﬁed by calculating the KL divergences between Gaussians as
weighted L2 distances between the means ϵθ and ϵ3. Formally, we have:"
IMPROVED TRAINING OBJECTIVE,0.22781065088757396,"Proposition 3. (Ho et al., 2020) Under the parameterization in equation 4, we have:"
IMPROVED TRAINING OBJECTIVE,0.23076923076923078,"LELBO = T
X"
IMPROVED TRAINING OBJECTIVE,0.23372781065088757,"t=1
γtE{C0,G}∼q(C0,G),ϵ∼N(0,I)
h
∥ϵ −ϵθ(G, Ct, t)∥2
2
i
(9)"
IMPROVED TRAINING OBJECTIVE,0.23668639053254437,"where Ct = √¯αtC0 + √1 −¯αtϵ. The weights γt =
βt
2αt(1−¯αt−1) for t > 1, and γ1 =
1
2α1 ."
IMPROVED TRAINING OBJECTIVE,0.23964497041420119,"The intuition of this objective is to independently sample chaotic conformations of different timesteps
from q(Ct−1|Ct, C0), and use ϵθ to model the noise vector ϵ. To yield a better empirical performance,
Ho et al. (2020) suggests to set all weights γt as 1, which is in line with the the objectives of recent
noise conditional score networks (Song & Ermon, 2019; 2020)."
IMPROVED TRAINING OBJECTIVE,0.24260355029585798,"As ϵθ is designed to be equivariant, it is natural to require its supervision signal ϵ to be equivariant
with Ct. Note that once this is achieved, the ELBO will also become invariant. However, the ϵ in
the forward diffusion process is not imposed with such equivariance, violating the above properties.
Here we propose two approaches to obtain the modiﬁed noise vector ˆϵ, which, after replacing ϵ in the
L2 distance calculation in equation 9, achieves the desired equivariance:"
IMPROVED TRAINING OBJECTIVE,0.2455621301775148,Alignment approach. Considering the fact that ϵ can be calculated by Ct−√¯αtC0
IMPROVED TRAINING OBJECTIVE,0.2485207100591716,"√1−¯αt
, we can ﬁrst rotate"
IMPROVED TRAINING OBJECTIVE,0.2514792899408284,"and translate C0 to ˆC0 by aligning w.r.t Ct, and then compute ˆϵ as Ct−√¯αt ˆC0"
IMPROVED TRAINING OBJECTIVE,0.25443786982248523,√1−¯αt . Since the aligned
IMPROVED TRAINING OBJECTIVE,0.257396449704142,"conformation ˆC0 is equivariant with Ct, the processed ˆϵ will also enjoy the equivariance. Speciﬁcally,
the alignment is implemented by ﬁrst translating C0 to the same CoM of Ct and then solve the optimal
rotation matrix by Kabsch alignment algorithm (Kabsch, 1976)."
IMPROVED TRAINING OBJECTIVE,0.2603550295857988,3The detailed derivations and full proofs are provided in Appendix A.
IMPROVED TRAINING OBJECTIVE,0.26331360946745563,Published as a conference paper at ICLR 2022
IMPROVED TRAINING OBJECTIVE,0.26627218934911245,"Chain-rule approach. Another meaningful observation is that by reparameterizing the Gaussian
distribution q(Ct|C0) as Ct = √¯αtC0 + √1 −¯αtϵ, ϵ can be viewed as a weighted score function
√1 −¯αt∇Ct q(Ct|C0). Shi et al. (2021) recently shows that generally this score function ∇Ct q(Ct|·)
can be designed to be equivariant by decomposing it into ∂Ctdt ∇dtq(Ct|·) with the chain rule, where
dt can be any invariant features of the structures Ct such as the inter-atomic distances. We refer
readers to Shi et al. (2021) for more details. The insight is that as gradient of invariant variables w.r.t
equivariant variables, the partial derivative ∂Ctdt will always be equivalent with Ct. In this work,
under the common assumption that d also follows a Gaussian distribution (Kingma & Welling, 2013),
our practical implementation is to ﬁrst approximately calculate ∇dtq(Ct|C0) as dt−√¯αtd0"
IMPROVED TRAINING OBJECTIVE,0.2692307692307692,"1−¯αt
, and then"
IMPROVED TRAINING OBJECTIVE,0.27218934911242604,compute the modiﬁed noise vector ˆϵ as √1 −¯αt ∂Ctdt( dt−√¯αtd0
IMPROVED TRAINING OBJECTIVE,0.27514792899408286,"1−¯αt
) = ∂Ctdt·(dt−√¯αtd0)
√1−¯αt
."
SAMPLING,0.2781065088757396,"4.4
SAMPLING"
SAMPLING,0.28106508875739644,"Algorithm 1 Sampling Algorithm of GEODIFF.
Input: the molecular graph G, the learned reverse model ϵθ.
Output: the molecular conformation C."
SAMPLING,0.28402366863905326,"1: Sample CT ∼p(CT ) = N(0, I)
2: for s = T, T −1, · · · , 1 do
3:
Shift Cs to zero CoM
4:
Compute µθ(Cs, G, s) from ϵθ(Cs, G, s) using equation 4
5:
Sample Cs−1 ∼N(Cs−1; µθ(Cs, G, s), σ2
t I)
6: end for
7: return C0 as C"
SAMPLING,0.2869822485207101,"With a learned reverse dynamics
ϵθ(G, Ct, t), the transition means
µθ(G, Ct, t) can be calculated by
equation 4. Thus, given a graph
G, its geometry C0 is generated
by ﬁrst sampling chaotic parti-
cles CT
∼
p(CT ), and then
progressively sample Ct−1
∼
pθ(Ct−1|G, Ct) for t = T, T −
1, · · · , 1. This process is Marko-
vian, which gradually shifts the
previous noisy positions towards
equilibrium states. We provide the pseudo code of the whole sampling process in Algorithm 1."
EXPERIMENT,0.28994082840236685,"5
EXPERIMENT"
EXPERIMENT,0.29289940828402367,"In this section, we empirically evaluate GEODIFF on the task of equilibrium conformation generation
for both small and drug-like molecules. Following existing work (Shi et al., 2021; Ganea et al.,
2021), we test the proposed method as well as the competitive baselines on two standard benchmarks:
Conformation Generation (Sec. 5.2) and Property Prediction (Sec. 5.3). We ﬁrst present the
general experiment setups, and then describe task-speciﬁc evaluation protocols and discuss the results
in each section. The implementation details are provided in Appendix C."
EXPERIMENT SETUP,0.2958579881656805,"5.1
EXPERIMENT SETUP"
EXPERIMENT SETUP,0.2988165680473373,"Datasets. Following prior works (Xu et al., 2021a;b), we also use the recent GEOM-QM9 (Ramakr-
ishnan et al., 2014) and GEOM-Drugs (Axelrod & Gomez-Bombarelli, 2020) datasets. The former
one contains small molecules while the latter one are medium-sized organic compounds. We borrow
the data split produced by Shi et al. (2021). For both datasets, the training split consists of 40, 000
molecules with 5 conformations for each, resulting in 200, 000 conformations in total. The valid
split share the same size as training split. The test split contains 200 distinct molecules, with 22, 408
conformations for QM9 and 14, 324 ones for Drugs."
EXPERIMENT SETUP,0.30177514792899407,"Baselines. We compare GEODIFF with 6 recent or established state-of-the-art baselines. For the ML
approaches, we test the following models with highest reported performance: CVGAE (Mansimov
et al., 2019), GRAPHDG (Simm & Hernandez-Lobato, 2020), CGCF (Xu et al., 2021a), CONF-
VAE (Xu et al., 2021b) and CONFGF (Shi et al., 2021). We also test the classic RDKIT (Riniker &
Landrum, 2015) method, which is arguably the most popular open-source software for conformation
generation. We refer readers to Sec. 2 for a detailed discussion of these models."
CONFORMATION GENERATION,0.3047337278106509,"5.2
CONFORMATION GENERATION"
CONFORMATION GENERATION,0.3076923076923077,"Evaluation metrics. The task aims to measure both quality and diversity of generated conformations
by different models. We follow Ganea et al. (2021) to evaluate 4 metrics built upon root-mean-square"
CONFORMATION GENERATION,0.3106508875739645,Published as a conference paper at ICLR 2022
CONFORMATION GENERATION,0.3136094674556213,"Table 1: Results on the GEOM-Drugs dataset, without FF optimization."
CONFORMATION GENERATION,0.3165680473372781,"COV-R (%) ↑
MAT-R (Å) ↓
COV-P (%) ↑
MAT-P (Å) ↓
Models
Mean
Median
Mean
Median
Mean
Median
Mean
Median"
CONFORMATION GENERATION,0.31952662721893493,"CVGAE
0.00
0.00
3.0702
2.9937
-
-
-
-
GRAPHDG
8.27
0.00
1.9722
1.9845
2.08
0.00
2.4340
2.4100
CGCF
53.96
57.06
1.2487
1.2247
21.68
13.72
1.8571
1.8066
CONFVAE
55.20
59.43
1.2380
1.1417
22.96
14.05
1.8287
1.8159
GEOMOL
67.16
71.71
1.0875
1.0586
-
-
-
-
CONFGF
62.15
70.93
1.1629
1.1596
23.42
15.52
1.7219
1.6863"
CONFORMATION GENERATION,0.3224852071005917,"GEODIFF-A
88.36
96.09
0.8704
0.8628
60.14
61.25
1.1864
1.1391
GEODIFF-C
89.13
97.88
0.8629
0.8529
61.47
64.55
1.1712
1.1232"
CONFORMATION GENERATION,0.3254437869822485,"* The COV-R and MAT-R results of CVGAE, GRAPHDG, CGCF, and CONFGF are borrowed from Shi
et al. (2021). The results of GEOMOL are borrowed from a most recent study Zhu et al. (2022). Other
results are obtained by our own experiments. The results of all models for the GEOM-QM9 dataset
(summarized in Tab. 5) are collected in the same way."
CONFORMATION GENERATION,0.32840236686390534,"deviation (RMSD), which is deﬁned as the normalized Frobenius norm of two atomic coordinates
matrices, after alignment by Kabsch algorithm (Kabsch, 1976). Formally, let Sg and Sr denote the
sets of generated and reference conformers respectively, then the Coverage and Matching metrics (Xu
et al., 2021a) following the conventional Recall measurement can be deﬁned as:"
CONFORMATION GENERATION,0.33136094674556216,"COV-R(Sg, Sr) =
1
|Sr|"
CONFORMATION GENERATION,0.3343195266272189,"n
C ∈Sr| RMSD(C, ˆC) ≤δ, ˆC ∈Sg
o,
(10)"
CONFORMATION GENERATION,0.33727810650887574,"MAT-R(Sg, Sr) =
1
|Sr| X"
CONFORMATION GENERATION,0.34023668639053256,"C∈Sr
min
ˆC∈Sg
RMSD(C, ˆC),
(11)"
CONFORMATION GENERATION,0.3431952662721893,"where δ is a pre-deﬁned threshold. The other two metrics COV-P and MAT-P inspired by Precision
can be deﬁned similarly but with the generated and reference sets exchanged. In practice, Sg is set
as twice of the size of Sr for each molecule. Intuitively, the COV scores measure the percentage
of structures in one set covered by another set, where covering means the RMSD between two
conformations is within a certain threshold δ. By contrast, the MAT scores measure the average
RMSD of conformers in one set with its closest neighbor in another set. In general, higher COV rates
or lower MAT score suggest that more realistic conformations are generated. Besides, the Precision
metrics depend more on the quality, while the Recall metrics concentrate more on the diversity. Either
metrics can be more appealing considering the speciﬁc scenario. Following previous works (Xu et al.,
2021a; Ganea et al., 2021), δ is set as 0.5Å and 1.25Å for QM9 and Drugs datasets respectively."
CONFORMATION GENERATION,0.34615384615384615,"Results & discussion. The results are summarized in Tab. 1 and Tab. 5 (left in Appendix. D). As
noted in Sec. 4.3, GEODIFF can be trained with two types of modiﬁed ELBO, named alignment
and chain-rule approaches. We denote models learned by these two objectives as GEODIFF-A and
GEODIFF-C respectively. As shown in the tables, GEODIFF consistently outperform the state-of-the-
art ML models on all datasets and metrics, especially by a signiﬁcant margin for more challenging
large molecules (Drugs dataset). The results demonstrate the superior capacity of GEODIFF to model
the multi modal distribution, and generative both accurate and diverse conformations. We also notice
that in general GEODIFF-C performs slightly better than GEODIFF-A, which suggests that chain-rule
approach leads to a better optimization procedure. We thus take GEODIFF-C as the representative in
the following comparisons. We visualize samples generated by different models in Fig. 2 to provide a
qualitative comparison, where GEODIFF is shown to capture better both local and global structures."
CONFORMATION GENERATION,0.34911242603550297,"On the more challenging Drugs dataset, we further test RDKIT. As shown in Tab. 2, our observation
is in line with previous studies (Shi et al., 2021) that the state-of-the-art ML models (shown in Tab. 1)
perform better on COV-R and MAT-R. However, for the new Precision-based metrics we found that
ML models are still not comparable. This indicates that ML models tend to explore more possible
representatives while RDKIT concentrates on a few most common ones, prioritizes quality over
diversity. Previous works (Mansimov et al., 2019; Xu et al., 2021b) suggest that this is because
RDKIT involves an additional empirical force ﬁeld (FF) (Halgren, 1996) to optimize the structure,
and we follow them to also combine GEODIFF with FF to yield a more fair comparison. Results in"
CONFORMATION GENERATION,0.3520710059171598,Published as a conference paper at ICLR 2022
CONFORMATION GENERATION,0.35502958579881655,Reference
CONFORMATION GENERATION,0.35798816568047337,ConfGF
CONFORMATION GENERATION,0.3609467455621302,GeoDiff
CONFORMATION GENERATION,0.363905325443787,GraphDG Graph
CONFORMATION GENERATION,0.3668639053254438,"Figure 2: Examples of generated structures from Drugs dataset. For every model, we show the
conformation best-aligned with the ground truth. More examples are provided in Appendix E."
CONFORMATION GENERATION,0.3698224852071006,"Table 2: Results on the GEOM-Drugs dataset, with FF optimization."
CONFORMATION GENERATION,0.3727810650887574,"COV-R (%) ↑
MAT-R (Å) ↓
COV-P (%) ↑
MAT-P (Å) ↓
Models
Mean
Median
Mean
Median
Mean
Median
Mean
Median"
CONFORMATION GENERATION,0.3757396449704142,"RDKIT
60.91
65.70
1.2026
1.1252
72.22
88.72
1.0976
0.9539
GEODIFF + FF
92.27
100.00
0.7618
0.7340
84.51
95.86
0.9834
0.9221"
CONFORMATION GENERATION,0.378698224852071,"Tab. 2 demonstrate that GEODIFF +FF can keep the superior diversity (Recall metrics) while also
enjoy signiﬁcantly improved accuracy ((Precision metrics))."
PROPERTY PREDICTION,0.3816568047337278,"5.3
PROPERTY PREDICTION"
PROPERTY PREDICTION,0.38461538461538464,Table 3: MAE of predicted ensemble properties in eV.
PROPERTY PREDICTION,0.3875739644970414,"Method
E
Emin
∆ϵ
∆ϵmin
∆ϵmax
RDKIT
0.9233
0.6585
0.3698
0.8021
0.2359
GRAPHDG
9.1027
0.8882
1.7973
4.1743
0.4776
CGCF
28.9661
2.8410
2.8356
10.6361
0.5954
CONFVAE
8.2080
0.6100
1.6080
3.9111
0.2429
CONFGF
2.7886
0.1765
0.4688
2.1843
0.1433"
PROPERTY PREDICTION,0.3905325443786982,"GEODIFF
0.25974
0.1551
0.3091
0.7033
0.1909"
PROPERTY PREDICTION,0.39349112426035504,"Evaluation metrics. This task estimates
the molecular ensemble properties (Axel-
rod & Gomez-Bombarelli, 2020) over a set
of generated conformations. This can pro-
vide an direct assessment on the quality
of generated samples. In speciﬁc, we fol-
low Shi et al. (2021) to extract a split from
GEOM-QM9 covering 30 molecules, and
generate 50 samples for each. Then we use
the chemical toolkit PSI4 (Smith et al., 2020) to calculate each conformer’s energy E and HOMO-
LUMO gap ϵ, and compare the average energy E, lowest energy Emin, average gap ∆ϵ, minimum
gap ∆ϵmin, and maximum gap ∆ϵmax with the ground truth."
PROPERTY PREDICTION,0.39644970414201186,"Results & discussions. The mean absolute errors (MAE) between calculated properties and the
ground truth are reported in Tab. 3. CVGAE is excluded due to the poor performance, which is also
reported in Simm & Hernandez-Lobato (2020); Shi et al. (2021). The properties are highly sensitive
to geometric structure, and thus the superior performance demonstrate that GEODIFF can consistently
predict more accurate conformations across different molecules."
CONCLUSION,0.3994082840236686,"6
CONCLUSION"
CONCLUSION,0.40236686390532544,"We propose GEODIFF, a novel probabilistic model for generating molecular conformations. GEODIFF
marries denoising diffusion models with geometric representations, where we parameterize the reverse
generative dynamics as a Markov chain, and novelly impose roto-translational invariance into the
density with equivariant Markov kernels. We derive a tractable invariant objective from the variational
lower bound to optimize the likelihood. Comprehensive experiments over multiple tasks demonstrate
that GEODIFF is competitive with the existing state-of-the-art models. Future work includes further
improving or accelerating the model with other recent progress of diffusion models, and extending
our method to other challenging structures such as proteins."
CONCLUSION,0.40532544378698226,Published as a conference paper at ICLR 2022
CONCLUSION,0.40828402366863903,ACKNOWLEDGEMENT
CONCLUSION,0.41124260355029585,"Minkai thanks Huiyu Cai, David Wipf, Zuobai Zhang, and Zhaocheng Zhu for their helpful discus-
sions and comments. This project is supported by the Natural Sciences and Engineering Research
Council (NSERC) Discovery Grant, the Canada CIFAR AI Chair Program, collaboration grants be-
tween Microsoft Research and Mila, Samsung Electronics Co., Ltd., Amazon Faculty Research Award,
Tencent AI Lab Rhino-Bird Gift Fund and a NRC Collaborative R&D Project (AI4D-CORE-06).
This project was also partially funded by IVADO Fundamental Research Project grant PRF-2019-
3583139727. The Stanford team is supported by NSF(#1651565, #1522054, #1733686), ONR
(N000141912145), AFOSR (FA95501910024), ARO (W911NF-21-1-0125) and Sloan Fellowship."
REFERENCES,0.41420118343195267,REFERENCES
REFERENCES,0.4171597633136095,"Mohammed AlQuraishi. End-to-end differentiable learning of protein structure. Cell systems, 8(4):
292–301, 2019."
REFERENCES,0.42011834319526625,"Simon Axelrod and Rafael Gomez-Bombarelli. Geom: Energy-annotated molecular conformations
for property prediction and molecular generation. arXiv preprint arXiv:2006.05531, 2020."
REFERENCES,0.4230769230769231,"Simon Batzner, Tess E Smidt, Lixin Sun, Jonathan P Mailoa, Mordechai Kornbluth, Nicola Molinari,
and Boris Kozinsky. Se (3)-equivariant graph neural networks for data-efﬁcient and accurate
interatomic potentials. arXiv preprint arXiv:2101.03164, 2021."
REFERENCES,0.4260355029585799,"Julian Chibane, Thiemo Alldieck, and Gerard Pons-Moll. Implicit functions in feature space for 3d
shape reconstruction and completion. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 6970–6981, 2020."
REFERENCES,0.4289940828402367,"Sybren Ruurds De Groot and Peter Mazur. Non-equilibrium thermodynamics. Courier Corporation,
2013."
REFERENCES,0.4319526627218935,"Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP. In
ICLR, 2017."
REFERENCES,0.4349112426035503,"David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alán
Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular
ﬁngerprints. In Advances in neural information processing systems, pp. 2224–2232, 2015."
REFERENCES,0.4378698224852071,"Fabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling. Se(3)-transformers: 3d roto-
translation equivariant attention networks. NeurIPS, 2020."
REFERENCES,0.4408284023668639,"Octavian-Eugen Ganea, Lagnajit Pattanaik, Connor W Coley, Regina Barzilay, Klavs F Jensen,
William H Green, and Tommi S Jaakkola. Geomol: Torsional geometric generation of molecular
3d conformer ensembles. arXiv preprint arXiv:2106.07802, 2021."
REFERENCES,0.4437869822485207,"Niklas WA Gebauer, Michael Gastegger, Stefaan SP Hessmann, Klaus-Robert Müller, and Kristof T
Schütt. Inverse design of 3d molecular structures with conditional generative neural networks.
arXiv preprint arXiv:2109.04824, 2021."
REFERENCES,0.4467455621301775,"Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70, pp. 1263–1272. JMLR. org, 2017."
REFERENCES,0.44970414201183434,"T. Gogineni, Ziping Xu, Exequiel Punzalan, Runxuan Jiang, Joshua A Kammeraad, Ambuj Tewari,
and P. Zimmerman. Torsionnet: A reinforcement learning approach to sequential conformer search.
ArXiv, abs/2006.07078, 2020."
REFERENCES,0.4526627218934911,"Thomas A Halgren. Merck molecular force ﬁeld. v. extension of mmff94 using experimental data,
additional computational data, and empirical rules. Journal of Computational Chemistry, 17(5-6):
616–641, 1996."
REFERENCES,0.4556213017751479,"Paul CD Hawkins. Conformation generation: the state of the art. Journal of Chemical Information
and Modeling, 57(8):1747–1756, 2017."
REFERENCES,0.45857988165680474,Published as a conference paper at ICLR 2022
REFERENCES,0.46153846153846156,"Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassiﬁed and out-of-distribution
examples in neural networks. arXiv preprint arXiv:1610.02136, 2016."
REFERENCES,0.46449704142011833,"Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. arXiv preprint
arXiv:2006.11239, 2020."
REFERENCES,0.46745562130177515,"Weihua Hu, Muhammed Shuaibi, Abhishek Das, Siddharth Goyal, Anuroop Sriram, Jure Leskovec,
Devi Parikh, and Larry Zitnick. Forcenet: A graph neural network for large-scale quantum
chemistry simulation. 2021."
REFERENCES,0.47041420118343197,"John Ingraham, Adam J Riesselman, Chris Sander, and Debora S Marks. Learning protein structure
with a differentiable simulator. In International Conference on Learning Representations, 2019."
REFERENCES,0.47337278106508873,"Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for
molecular graph generation. arXiv preprint arXiv:1802.04364, 2018."
REFERENCES,0.47633136094674555,"Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael John Lamarre Townshend, and Ron Dror.
Learning from protein structure with geometric vector perceptrons. In International Conference on
Learning Representations, 2021."
REFERENCES,0.47928994082840237,"John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger,
Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al. Highly accurate
protein structure prediction with alphafold. Nature, 596(7873):583–589, 2021."
REFERENCES,0.4822485207100592,"Wolfgang Kabsch. A solution for the best rotation to relate two sets of vectors. Acta Crystallographica
Section A: Crystal Physics, Diffraction, Theoretical and General Crystallography, 32(5):922–923,
1976."
REFERENCES,0.48520710059171596,"Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In 2nd International
Conference on Learning Representations, 2013."
REFERENCES,0.4881656804733728,"Jonas Köhler, Leon Klein, and Frank Noe. Equivariant ﬂows: Exact likelihood generative learning for
symmetric densities. In Proceedings of the 37th International Conference on Machine Learning,
2020."
REFERENCES,0.4911242603550296,"Leo Liberti, Carlile Lavor, Nelson Maculan, and Antonio Mucherino. Euclidean distance geometry
and applications. SIAM review, 56(1):3–69, 2014."
REFERENCES,0.4940828402366864,"Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. ArXiv,
abs/2103.01458, 2021."
REFERENCES,0.4970414201183432,"Shitong Luo, Chence Shi, Minkai Xu, and Jian Tang. Predicting molecular conformation via dynamic
graph score matching. Advances in Neural Information Processing Systems, 34, 2021."
REFERENCES,0.5,"Elman Mansimov, Omar Mahmood, Seokho Kang, and Kyunghyun Cho. Molecular geometry
prediction using a deep generative graph neural network. arXiv preprint arXiv:1904.00314, 2019."
REFERENCES,0.5029585798816568,"B. Miller, M. Geiger, T. Smidt, and F. Noé. Relevance of rotationally equivariant convolutions for
predicting molecular properties. ArXiv, abs/2008.08461, 2020."
REFERENCES,0.5059171597633136,"Frank Noé, Simon Olsson, Jonas Köhler, and Hao Wu. Boltzmann generators: Sampling equilibrium
states of many-body systems with deep learning. Science, 365(6457), 2019."
REFERENCES,0.5088757396449705,"Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole Von Lilienfeld. Quantum
chemistry structures and properties of 134 kilo molecules. Scientiﬁc data, 1(1):1–7, 2014."
REFERENCES,0.5118343195266272,"Sereina Riniker and Gregory A. Landrum. Better informed distance geometry: Using what we know
to improve conformation generation. Journal of Chemical Information and Modeling, 55(12):
2562–2574, 2015."
REFERENCES,0.514792899408284,"Victor Garcia Satorras, Emiel Hoogeboom, Fabian B Fuchs, Ingmar Posner, and Max Welling. E (n)
equivariant normalizing ﬂows for molecule generation in 3d. arXiv preprint arXiv:2105.09016,
2021a."
REFERENCES,0.5177514792899408,Published as a conference paper at ICLR 2022
REFERENCES,0.5207100591715976,"Victor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E(n) equivariant graph neural networks,
2021b."
REFERENCES,0.5236686390532544,"Kristof Schütt, Pieter-Jan Kindermans, Huziel Enoc Sauceda Felix, Stefan Chmiela, Alexandre
Tkatchenko, and Klaus-Robert Müller. Schnet: A continuous-ﬁlter convolutional neural network
for modeling quantum interactions. In Advances in Neural Information Processing Systems, pp.
991–1001. Curran Associates, Inc., 2017."
REFERENCES,0.5266272189349113,"Andrew W Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green,
Chongli Qin, Augustin Žídek, Alexander WR Nelson, Alex Bridgland, et al. Improved protein
structure prediction using potentials from deep learning. Nature, 577(7792):706–710, 2020."
REFERENCES,0.5295857988165681,"Chence Shi, Minkai Xu, Zhaocheng Zhu, Weinan Zhang, Ming Zhang, and Jian Tang. Graphaf: a
ﬂow-based autoregressive model for molecular graph generation. arXiv preprint arXiv:2001.09382,
2020."
REFERENCES,0.5325443786982249,"Chence Shi, Shitong Luo, Minkai Xu, and Jian Tang.
Learning gradient ﬁelds for molecular
conformation generation. ArXiv, 2021."
REFERENCES,0.5355029585798816,"Muhammed Shuaibi, Adeesh Kolluru, Abhishek Das, Aditya Grover, Anuroop Sriram, Zachary Ulissi,
and C Lawrence Zitnick. Rotation invariant graph neural networks using spin convolutions. arXiv
preprint arXiv:2106.09575, 2021."
REFERENCES,0.5384615384615384,"Gregor Simm and Jose Miguel Hernandez-Lobato. A generative model for molecular distance
geometry.
In Hal Daumé III and Aarti Singh (eds.), Proceedings of the 37th International
Conference on Machine Learning, volume 119, pp. 8949–8958. PMLR, 2020."
REFERENCES,0.5414201183431953,"Gregor N. C. Simm, Robert Pinsler, Gábor Csányi, and José Miguel Hernández-Lobato. Symmetry-
aware actor-critic for 3d molecular design. In International Conference on Learning Representa-
tions, 2021."
REFERENCES,0.5443786982248521,"Daniel G. A. Smith, L. Burns, A. Simmonett, R. Parrish, M. C. Schieber, Raimondas Galvelis,
P. Kraus, H. Kruse, Roberto Di Remigio, Asem Alenaizan, A. M. James, S. Lehtola, Jonathon P
Misiewicz, et al. Psi4 1.4: Open-source software for high-throughput quantum chemistry. The
Journal of chemical physics, 2020."
REFERENCES,0.5473372781065089,"Jascha Sohl-Dickstein, Eric A Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised
learning using nonequilibrium thermodynamics. arXiv preprint arXiv:1503.03585, 2015."
REFERENCES,0.5502958579881657,"Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv
preprint arXiv:2010.02502, 2020."
REFERENCES,0.5532544378698225,"Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.
In Advances in Neural Information Processing Systems, pp. 11918–11930, 2019."
REFERENCES,0.5562130177514792,"Yang Song and Stefano Ermon. Improved techniques for training score-based generative models.
NeurIPS, 2020."
REFERENCES,0.5591715976331361,"N. Thomas, T. Smidt, Steven M. Kearnes, Lusann Yang, L. Li, Kai Kohlhoff, and P. Riley. Tensor
ﬁeld networks: Rotation- and translation-equivariant neural networks for 3d point clouds. ArXiv,
2018."
REFERENCES,0.5621301775147929,"M. Weiler, M. Geiger, M. Welling, W. Boomsma, and T. Cohen. 3d steerable cnns: Learning
rotationally equivariant features in volumetric data. In NeurIPS, 2018."
REFERENCES,0.5650887573964497,"Minkai Xu, Shitong Luo, Yoshua Bengio, Jian Peng, and Jian Tang. Learning neural generative
dynamics for molecular conformation generation. In International Conference on Learning
Representations, 2021a."
REFERENCES,0.5680473372781065,"Minkai Xu, Wujie Wang, Shitong Luo, Chence Shi, Yoshua Bengio, Rafael Gomez-Bombarelli,
and Jian Tang. An end-to-end framework for molecular conformation generation via bilevel
programming. arXiv preprint arXiv:2105.07246, 2021b."
REFERENCES,0.5710059171597633,Published as a conference paper at ICLR 2022
REFERENCES,0.5739644970414202,"Linfeng Zhang, Jiequn Han, Han Wang, Roberto Car, and Weinan E. Deep Potential Molecular
Dynamics: A Scalable Model with the Accuracy of Quantum Mechanics. Physical Review Letters,
120(14):143001, 2018."
REFERENCES,0.5769230769230769,"Jinhua Zhu, Yingce Xia, Chang Liu, Lijun Wu, Shufang Xie, Tong Wang, Yusong Wang, Wengang
Zhou, Tao Qin, Houqiang Li, et al. Direct molecular conformation generation. arXiv preprint
arXiv:2202.01356, 2022."
REFERENCES,0.5798816568047337,Published as a conference paper at ICLR 2022
REFERENCES,0.5828402366863905,"A
PROOFS"
REFERENCES,0.5857988165680473,"A.1
PROPERTIES OF THE DIFFUSION MODEL"
REFERENCES,0.5887573964497042,"We include proofs for several key properties of the probabilistic diffusion model here to be self-
contained. For more detailed discussions, please refer to Ho et al. (2020). Let {β0, ..., βT } be a
sequence of variances, and αt = 1 −βt and ¯αt = Qt
s=1 αs. The two following properties are crucial
for deriving the ﬁnal tractable objective in equation 9."
REFERENCES,0.591715976331361,Property 1. Tractable marginal of the forward process:
REFERENCES,0.5946745562130178,"q(Ct|C0) =
Z
q(C1:t|C0) dC1:(t−1) = N(Ct; √¯αtC0, (1 −¯αt)I)."
REFERENCES,0.5976331360946746,"Proof. Let ϵi’s be independent standard Gaussian random variables. Then, by deﬁnition of the
Markov kernels q(Ct|Ct−1) in equation 2, we have"
REFERENCES,0.6005917159763313,"Ct
= √αtCt−1 + √βtϵt
= √αtαt−1Ct−2 +
p"
REFERENCES,0.6035502958579881,"αtβt−1ϵt−1 + √βtϵt
= √αtαt−1αt−1Ct−3 +
p"
REFERENCES,0.606508875739645,"αtαt−1βt−2ϵt−2 +
p"
REFERENCES,0.6094674556213018,"αtβt−1ϵt−1 + √βtϵt
= · · ·
= √¯αtC0 +
p"
REFERENCES,0.6124260355029586,"αtαt−1 · · · α2β1ϵ1 + · · · +
p"
REFERENCES,0.6153846153846154,αtβt−1ϵt−1 + √βtϵt (12)
REFERENCES,0.6183431952662722,"Therefore q(Ct|C0) is still Gaussian, and the mean of Ct is √¯αtC0, and the variance matrix is
(αtαt−1 · · · α2β1 + · · · + αtβt−1 + βt)I = (1 −¯αt)I. Then we have:"
REFERENCES,0.621301775147929,"q(Ct|C0) = N(Ct; √¯αtC0, (1 −¯αt)I)."
REFERENCES,0.6242603550295858,This property provides convenient closed-form evaluation of Ct knowing C0:
REFERENCES,0.6272189349112426,"Ct = √¯αtC0 +
√"
REFERENCES,0.6301775147928994,"1 −¯αtϵ,"
REFERENCES,0.6331360946745562,"where ϵ ∼N(0, I)."
REFERENCES,0.636094674556213,"Besides, it is worth noting that,"
REFERENCES,0.6390532544378699,"q(CT |C0) = N(CT ; √¯αT C0, (1 −¯αT )I),"
REFERENCES,0.6420118343195266,"where ¯αT = QT
t=1(1 −βt) approaches zero with large T, which indicates the diffusion process can
ﬁnally converge into a whitened noisy distribution."
REFERENCES,0.6449704142011834,Property 2. Tractable posterior of the forward process:
REFERENCES,0.6479289940828402,"q(Ct−1|Ct, C0) = N(Ct−1;
√¯αt−1βt"
REFERENCES,0.650887573964497,"1 −¯αt
C0 +
√αt(1 −¯αt−1)"
REFERENCES,0.6538461538461539,"1 −¯αt
Ct, (1 −¯αt−1)"
REFERENCES,0.6568047337278107,"1 −¯αt
βtI)."
REFERENCES,0.6597633136094675,Proof. Let ˜βt = 1−¯αt−1
REFERENCES,0.6627218934911243,"1−¯αt βt, then we can derive the posterior by Bayes rule:"
REFERENCES,0.665680473372781,"q(Ct−1|Ct, C0)
= q(Ct|Ct−1) q(Ct−1|C0)"
REFERENCES,0.6686390532544378,q(Ct|C0)
REFERENCES,0.6715976331360947,"= N(Ct; √αtCt−1, βtI) N(Ct−1; √¯αt−1C0, (1 −¯αt−1)I)"
REFERENCES,0.6745562130177515,"N(Ct; √¯αtC0, (1 −¯αt)I)
= (2πβt)−d"
REFERENCES,0.6775147928994083,2 (2π(1 −¯αt−1))−d
REFERENCES,0.6804733727810651,"2 (2π(1 −¯αt))
d
2 ×"
REFERENCES,0.6834319526627219,"exp

−∥Ct −√αtCt−1∥2"
REFERENCES,0.6863905325443787,"2βt
−∥Ct−1 −√¯αt−1C0∥2"
REFERENCES,0.6893491124260355,"2(1 −¯αt−1)
+ ∥Ct −√¯αtC0∥2"
REFERENCES,0.6923076923076923,2(1 −¯αt) 
REFERENCES,0.6952662721893491,= (2π ˜βt)−d
EXP,0.6982248520710059,2 exp  −1 2˜βt
EXP,0.7011834319526628,"Ct−1 −
√¯αt−1βt"
EXP,0.7041420118343196,"1 −¯αt
C0 −
√αt(1 −¯αt−1)"
EXP,0.7071005917159763,"1 −¯αt
Ct 2! (13)"
EXP,0.7100591715976331,"Then we have the posterior q(Ct−1|Ct, C0) as the given form."
EXP,0.7130177514792899,Published as a conference paper at ICLR 2022
EXP,0.7159763313609467,"A.2
PROOF OF PROPOSITION 1"
EXP,0.7189349112426036,"Let Tg be some roto-translational transformations of a group element g ∈SE(3), and let p(xT ) be a
density which is SE(3)-invariant, i.e., p(xT ) = p(Tg(xT )). If the Markov transitions p(xt−1|xt) are
SE(3)-equivariant, i.e., p(xt−1|xt) = p(Tg(xt−1)|Tg(xt)), then we have that the density pθ(x0) =
R
p(xT )pθ(x0:T −1|xT )dx1:T is also SE(3)-invariant."
EXP,0.7218934911242604,Proof.
EXP,0.7248520710059172,"pθ(Tg(x0)) =
Z
p(Tg(xT ))pθ(Tg(x0:T −1)|Tg(xT ))dx1:T"
EXP,0.727810650887574,"=
Z
p(Tg(xT ))ΠT
t=1pθ(Tg(xt−1)|Tg(xt))dx1:T"
EXP,0.7307692307692307,"=
Z
p(xT )ΠT
t=1pθ(Tg(xt−1)|Tg(xt))dx1:T
(invariant prior p(xT ))"
EXP,0.7337278106508875,"=
Z
p(xT )ΠT
t=1pθ(xt−1|xt)dx1:T
(equivariant kernels p(xt−1|xt))"
EXP,0.7366863905325444,"=
Z
p(xT )pθ(x0:T −1|xT )dx1:T"
EXP,0.7396449704142012,= pθ(x0) (14)
EXP,0.742603550295858,"A.3
PROOF OF PROPOSITION 2"
EXP,0.7455621301775148,"In this section we prove that the output x of GFN deﬁned in equation 5, 6 and 7 is translationally
invariant and rotationally equivariant with the input C. Let g ∈R3 denote any translation transforma-
tions and orthogonal matrices R ∈R3×3 denote any rotation transformations. let Rx be shorthand
for (Rx1, · · · , RxN). Formally, we aim to prove that the model satisﬁes:"
EXP,0.7485207100591716,"Rxl+1, hl+1 = GFN(Rxl, RC + g, hl).
(15)"
EXP,0.7514792899408284,"This equation indicates that, given xl already rotationally equivalent with C, and hl already invariant,
then such property can propagate through a single GFN layer to xl+1 and hl+1."
EXP,0.7544378698224852,"Proof. Firstly, given that hl already invariant to SE(3) transformations, we have that the messages
mij calculated from equation 5 will also be invariant. This is because it sorely relies on the
distance between two atoms, which are manifestly invariant to rotations ∥Rxl
i −Rxl
j∥2 = (xl
i −
xl
j)⊤R⊤R(xl
i −xl
j) = (xl
i −xl
j)⊤I(xl
i −xl
j) = ∥xl
i −xl
j∥2. Formally, the invariance of messages
in equation 5 can be written as:"
EXP,0.757396449704142,"mi,j = Φm

hl
i, hl
j,
Rxl
i −Rxl
j
2 , eij

= Φm

hl
i, hl
j,
xl
i −xl
j
2 , eij

.
(16)"
EXP,0.7603550295857988,"And similarly, the ht+1 updated from equation 6 will also be invariant."
EXP,0.7633136094674556,"Next, we prove that the vector x updated from equation 7 preserves rotational equivariance and
translational invariance. Given mij already invariant as proven above, we have that:
X"
EXP,0.7662721893491125,j∈N(i)
DIJ,0.7692307692307693,"1
dij
(Rci + g −Rcj −g) Φx (mi,j) = R
X"
DIJ,0.772189349112426,j∈N(i)
DIJ,0.7751479289940828,"1
dij
(ci −cj) Φx (mi,j) = Rxl+1
i
.
(17)"
DIJ,0.7781065088757396,"Therefore, we have that rotating and translating c results in the same rotation and no translation on
xl+1 by updating through equation 7."
DIJ,0.7810650887573964,Thus we can conclude that the property deﬁned in equation 15 is satisﬁed.
DIJ,0.7840236686390533,"Having proved the equivariance property of a single GFN layer, then inductively, we can draw
conclusion that a composition of L GFN layers will also preserve the same equivariance."
DIJ,0.7869822485207101,Published as a conference paper at ICLR 2022
DIJ,0.7899408284023669,"A.4
PROOF OF PROPOSITION 3"
DIJ,0.7928994082840237,"We ﬁrst derive the variational lower bound (ELBO) objective in equation 8. The ELBO can be
calculated as follows:"
DIJ,0.7958579881656804,"E log pθ(C0|G) = E log Eq(C1:T |C0)
hpθ(C0:T −1|G, CT ) × p(CT )"
DIJ,0.7988165680473372,q(C1:T |C0) i
DIJ,0.8017751479289941,"≥Eq log pθ(C0:T −1|G, CT ) × p(CT )"
DIJ,0.8047337278106509,q(C1:T |C0)
DIJ,0.8076923076923077,"= Eq
h
log p(CT ) − T
X"
DIJ,0.8106508875739645,"t=1
log pθ(Ct−1|G, Ct)"
DIJ,0.8136094674556213,q(Ct|Ct−1) i
DIJ,0.8165680473372781,"= Eq
h
log p(CT ) −log pθ(C0|G, C1)"
DIJ,0.8195266272189349,"q(C1|C0)
− T
X t=2"
DIJ,0.8224852071005917,"
log pθ(Ct−1|G, Ct)"
DIJ,0.8254437869822485,"q(Ct−1|Ct, C0) + log q(Ct−1|C0)"
DIJ,0.8284023668639053,q(Ct|C0) i
DIJ,0.8313609467455622,"= Eq
h
log
p(CT )
q(CT |C0) −log pθ(C0|G, C1) − T
X"
DIJ,0.834319526627219,"t=2
log pθ(Ct−1|G, Ct)"
DIJ,0.8372781065088757,"q(Ct−1|Ct, C0) i"
DIJ,0.8402366863905325,"= −Eq
h
KL
 
q(CT |C0)∥p(CT )

+ T
X"
DIJ,0.8431952662721893,"t=2
KL
 
q(Ct−1|Ct, C0)∥pθ(Ct−1|G, Ct)

−log pθ(C0|G, C1)
i
. (18)"
DIJ,0.8461538461538461,"It can be noted that the ﬁrst term KL
 
q(CT |C0)∥p(CT )

is a constant, which can be omit-
ted in the objective.
Furthermore, for brevity, we also merge the ﬁnal term log pθ(C0|G, C1)
into the second term (sum over KL divergences),
and ﬁnally derive that LELBO
=
PT
t=1 DKL(q(Ct−1|Ct, C0)∥pθ(Ct−1|G, Ct)) as in equation 8."
DIJ,0.849112426035503,"Now we consider how to compute the KL divergences as the proposition 3. Since both q(Ct−1|Ct, C0)
and pθ(Ct−1|G, Ct) are Gaussian share the same covariance matrix ˜βtI, the KL divergence between
them can be calculated by the squared ℓ2 distance between their means weighed by a certain weights
1
2 ˜βt . By the expression of q(Ct|C0), we have the reparameterization that Ct = √¯αtC0 + √1 −¯αtϵ.
Then we can derive:"
DIJ,0.8520710059171598,"Eq KL
 
q(Ct−1|Ct, C0)∥pθ(G, Ct−1|Ct)
"
DIJ,0.8550295857988166,"=
1
2˜βt
EC0"
DIJ,0.8579881656804734,√¯αt−1βt
DIJ,0.8609467455621301,"1 −¯αt
C0 +
√αt(1 −¯αt−1)"
DIJ,0.863905325443787,"1 −¯αt
Ct −
1
√αt"
DIJ,0.8668639053254438,"
Ct −
βt
√1 −¯αt
ϵθ(Ct, G, t)
 2"
DIJ,0.8698224852071006,"=
1
2˜βt
EC0,ϵ "
DIJ,0.8727810650887574,√¯αt−1βt
DIJ,0.8757396449704142,"1 −¯αt
· Ct −√1 −¯αtϵ
√¯αt
+
√αt(1 −¯αt−1)"
DIJ,0.878698224852071,"1 −¯αt
Ct −
1
√αt"
DIJ,0.8816568047337278,"
Ct −
βt
√1 −¯αt
ϵθ(Ct, G, t)
 2"
DIJ,0.8846153846153846,"=
1
2˜βt
·
β2
t
αt(1 −¯αt)EC0,ϵ
0 · Ct + ϵ −ϵθ(Ct, G, t)
2"
DIJ,0.8875739644970414,"=
β2
t
2 1−¯αt−1"
DIJ,0.8905325443786982,"1−¯αt βtαt(1 −¯αt)
EC0,ϵ
ϵ −ϵθ(Ct, G, t)
2"
DIJ,0.893491124260355,"= γtEC0,ϵ
ϵ −ϵθ(Ct, t)
2 ,
(19)"
DIJ,0.8964497041420119,"where γt represent the wights
βt
2αt(1−¯αt−1). And we ﬁnish the proof."
DIJ,0.8994082840236687,"A.5
ANALYSIS OF THE INVARIANT DENSITY IN SEC. 4.2"
DIJ,0.9023668639053254,"Given a geometric system x ∈RN·3, we obtain the CoM-free ˆx by subtracting its CoM. This can be
considered as a linear transformation:"
DIJ,0.9053254437869822,"ˆx = Qx, where Q = I3 ⊗

IN −1"
DIJ,0.908284023668639,"N 1N1T
N

(20)"
DIJ,0.9112426035502958,"where Ik denotes the k × k identity matrix and 1k denotes the k-dimensional vector ﬁlled with ones.
It can be noted that Q is a symmetric projection operator, i.e., Q2 = Q and QT = Q. And we also"
DIJ,0.9142011834319527,Published as a conference paper at ICLR 2022
DIJ,0.9171597633136095,"have that rank[Q] = (N −1) · 3. Furthermore, let U represent the space of CoM-free systems, we
can easily have that Qy = y for any y ∈U since the CoM of y is already zero."
DIJ,0.9201183431952663,"Formally, let n = N · 3 and set Rn with an isotropic normal distribution ρ = N(0, In), then the
CoM-free density can be formally written as ˆρ = N(0, QInQT ) = N(0, QQT ). Thus, sampling
from ˆρ can be trivially achieved by sampling from ρ and then projecting with Q. And ˆρ(y) can be
calculated by ρ(y) since for any y ∈U we have ∥y∥2
2 = ∥Qy∥2
2, and thus ρ(y) = ˆρ(y)."
DIJ,0.9230769230769231,"And in this paper, with the SE(3)-equivariant Markov kernels of the reverse process, any CoM-free
system will transit to another CoM-free system. And thus we can induce a well-deﬁned Markov chain
on the subspace spanned by Q."
DIJ,0.9260355029585798,"B
OTHER RELATED WORK"
DIJ,0.9289940828402367,"Protein structure generation. There has also been many recent works working on protein structure
folding. For example, Boltzmann generators Noé et al. (2019) use ﬂow-based models to generate
the structure of protein main chains. AlQuraishi (2019) uses recurrent networks to model the amino
acid sequences. Ingraham et al. (2019) proposed neural networks to learn an energy simulator to
infer the protein structures. Most recently, AlphaFold Senior et al. (2020); Jumper et al. (2021) has
signiﬁcantly improved the performance of protein structure generation. Nevertheless, proteins are
mainly linear backbone structures while general molecules are highly branched with various rings,
making protein folding approaches unsuitable for our setting."
DIJ,0.9319526627218935,"Point cloud generation. Recently, some other works (Luo & Hu, 2021; Chibane et al., 2020) has
also been proposed for 3D structure generation with diffusion-based models, but focus on the point
cloud problem. Unfortunately, in general, point clouds are not considered as graphs with various
atom and bond information, and equivariance is also not widely considered, making these methods
fundamentally different from our model."
DIJ,0.9349112426035503,"C
EXPERIMENT DETAILS"
DIJ,0.9378698224852071,"In this section, we introduce the details of our experiments. In practice, the means ϵθ are parameterized
as compositions of both typical invariant MPNNs (Schütt et al., 2017) and the proposed equivariant
GFNs in Sec. 4.2. As a default setup, the MPNNs for parameterizing the means ϵθ are all implemented
with 4 layers, and the hidden embedding dimension is set as 128. After the MPNNs, we can obtain
the informative invariant atom embeddings, which we denote as h0. Then the embeddings h0 are
fed into equivariant layers and updated with equation 5, equation 6, and equation 7 to obtain the
equivariant output. For the training of GEODIFF, we train the model on a single Tesla V100 GPU
with a learning rate of 0.001 until convergence and Adam (Kingma & Welling, 2013) as the optimizer.
The practical training time is ~48 hours. The other hyper-parameters of GEODIFF are summarized in
Tab. 4, including highest variance level βT , lowest variance level βT , the variance schedule, number
of diffusion timesteps T, radius threshold for determining the neighbor of atoms τ, batch size, and
number of training iterations."
DIJ,0.9408284023668639,"Table 4: Additional hyperparameters of our GEODIFF.
Task
β1
βT
β scheduler
T
τ
Batch Size
Train Iter."
DIJ,0.9437869822485208,"QM9
1e-7
2e-3
sigmoid
5000
10Å
64
1M
Drugs
1e-7
2e-3
sigmoid
5000
10Å
32
1M"
DIJ,0.9467455621301775,"D
ADDITIONAL EXPERIMENTS"
DIJ,0.9497041420118343,"D.1
RESULTS FOR GEOM-QM9"
DIJ,0.9526627218934911,The results on the GEOM-QM9 dataset are reported in Tab. 5.
DIJ,0.9556213017751479,Published as a conference paper at ICLR 2022
DIJ,0.9585798816568047,"Table 5: Results on the GEOM-QM9 dataset, without FF optimization."
DIJ,0.9615384615384616,"COV-R (%) ↑
MAT-R (Å) ↓
COV-P (%) ↑
MAT-P (Å) ↓
Models
Mean
Median
Mean
Median
Mean
Median
Mean
Median"
DIJ,0.9644970414201184,"CVGAE
0.09
0.00
1.6713
1.6088
-
-
-
-
GRAPHDG
73.33
84.21
0.4245
0.3973
43.90
35.33
0.5809
0.5823
CGCF
78.05
82.48
0.4219
0.3900
36.49
33.57
0.6615
0.6427
CONFVAE
77.84
88.20
0.4154
0.3739
38.02
34.67
0.6215
0.6091
GEOMOL
71.26
72.00
0.3731
0.3731
-
-
-
-
CONFGF
88.49
94.31
0.2673
0.2685
46.43
43.41
0.5224
0.5124"
DIJ,0.9674556213017751,"GEODIFF-A
90.54
94.61
0.2104
0.2021
52.35
50.10
0.4539
0.4399
GEODIFF-C
90.07
93.39
0.2090
0.1988
52.79
50.29
0.4448
0.4267"
DIJ,0.9704142011834319,"Table 6: Additional results on the GEOM-Drugs dataset, without FF optimization."
DIJ,0.9733727810650887,"COV-R (%) ↑
MAT-R (Å) ↓
COV-P (%) ↑
MAT-P (Å) ↓
Models
Mean
Median
Mean
Median
Mean
Median
Mean
Median"
DIJ,0.9763313609467456,"GEODIFF (T=1000)
82.96
96.29
0.9525
0.9334
48.27
46.03
1.3205
1.2724"
DIJ,0.9792899408284024,"D.2
ABLATION STUDY WITH FEWER DIFFUSION STEPS"
DIJ,0.9822485207100592,"We also test our method with fewer diffusion steps. Speciﬁcally, we test the setting with T = 1000,
β1 =1e-7 and βT =9e-3. The results on the more challenging Drugs dataset are shown in Tab. 6.
Compared with the results in Tab. 1, we can observe that when setting the diffusion steps as 1000,
though slightly weaker than the performance with 5000 decoding steps, the model can already
outperforms all existing baselines. Note that, the most competitive baseline CONFGF (Shi et al., 2021)
also requires 5000 sampling steps, which indicates that our model can achieve better performance
with fewer computational costs compared with the state-of-the-art method."
DIJ,0.985207100591716,"E
MORE VISUALIZATIONS"
DIJ,0.9881656804733728,"We provide more visualization of generated structures in Fig. 3. The molecules are chosen from the
test split of GEOM-Drugs dataset."
DIJ,0.9911242603550295,Published as a conference paper at ICLR 2022
DIJ,0.9940828402366864,"Graph
Conformations"
DIJ,0.9970414201183432,Figure 3: Visualization of drug-like conformations generated by GEODIFF.
