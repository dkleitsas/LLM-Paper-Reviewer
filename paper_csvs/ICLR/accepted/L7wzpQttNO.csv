Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002004008016032064,"Diffusion probabilistic models (DPMs) and their extensions have emerged as com-
petitive generative models yet confront challenges of efÔ¨Åcient sampling.
We
propose a new bilateral denoising diffusion model (BDDM) that parameterizes
both the forward and reverse processes with a schedule network and a score
network, which can train with a novel bilateral modeling objective. We show
that the new surrogate objective can achieve a lower bound of the log marginal
likelihood tighter than a conventional surrogate. We also Ô¨Ånd that BDDM al-
lows inheriting pre-trained score network parameters from any DPMs and con-
sequently enables speedy and stable learning of the schedule network and op-
timization of a noise schedule for sampling. Our experiments demonstrate that
BDDMs can generate high-Ô¨Ådelity audio samples with as few as three sam-
pling steps. Moreover, compared to other state-of-the-art diffusion-based neu-
ral vocoders, BDDMs produce comparable or higher quality samples indistin-
guishable from human speech, notably with only seven sampling steps (143x
faster than WaveGrad and 28.6x faster than DiffWave). We release our code at
https://github.com/tencent-ailab/bddm."
INTRODUCTION,0.004008016032064128,"1
INTRODUCTION"
INTRODUCTION,0.006012024048096192,"Deep generative models have shown a tremendous advancement in speech synthesis (van den Oord
et al., 2016; Kalchbrenner et al., 2018; Prenger et al., 2019; Kumar et al., 2019; Kong et al., 2020b;
Chen et al., 2020; Kong et al., 2021). Successful generative models can be mainly divided into two
categories: generative adversarial network (GAN) (Goodfellow et al., 2014) based and likelihood-
based. The former is based on adversarial learning, where the objective is to generate data indistin-
guishable from the training data. Yet, the training GANs can be very unstable, and the relevant train-
ing objectives are not suitable to compare against different GANs. The latter uses log-likelihood or
surrogate objectives for training, but they also have intrinsic limitations regarding generation speed
or quality. For example, the autoregressive models (van den Oord et al., 2016; Kalchbrenner et al.,
2018), while being capable of generating high-Ô¨Ådelity data, are limited by their inherently slow
sampling process and the poor scaling properties on high-dimensional data. Likewise, the Ô¨Çow-
based models (Dinh et al., 2016; Kingma & Dhariwal, 2018; Chen et al., 2018; Papamakarios et al.,
2021) rely on specialized architectures to build a normalized probability model, whose training is
less parameter-efÔ¨Åcient. Other prior works use surrogate objectives, such as the evidence lower
bound in variational auto-encoders (Kingma & Welling, 2014; Rezende et al., 2014; Maal√∏e et al.,
2019) and the contrastive divergence in energy-based models (Hinton, 2002; Carreira-Perpinan &
Hinton, 2005). These models, despite showing improved speed, typically only work well for low-
dimensional data, and, in general, the sample qualities are not competitive to the GAN-based and
the autoregressive models (Bond-Taylor et al., 2021)."
INTRODUCTION,0.008016032064128256,"An up-and-coming class of likelihood-based models is the diffusion probabilistic models (DPMs)
(Sohl-Dickstein et al., 2015), which introduces the idea of using a forward diffusion process to
sequentially corrupt a given distribution and learning the reversal of such diffusion process to restore
the data distribution for sampling. From a similar perspective, Song & Ermon (2019) proposed
the score-based generative models by applying the score matching technique (Hyvarinen & Dayan,"
INTRODUCTION,0.01002004008016032,Published as a conference paper at ICLR 2022
INTRODUCTION,0.012024048096192385,"2005) to train a neural network such that samples can be generated via Langevin dynamics. Along
these two lines of research, Ho et al. (2020) proposed the denoising diffusion probabilistic models
(DDPMs) for high-quality image syntheses. Dhariwal & Nichol (2021) demonstrated that improved
DDPMs Nichol & Dhariwal (2021) are capable of generating high-quality images of comparable
or even superior quality to the state-of-the-art (SOTA) GAN-based models. For speech syntheses,
DDPMs were also applied in Wavegrad (Chen et al., 2020) and DiffWave (Kong et al., 2021) to
produce higher-Ô¨Ådelity audio samples than the conventional non-autoregressive models (Yamamoto
et al., 2020; Kumar et al., 2019; Yang et al., 2021; Bi¬¥nkowski et al., 2020) and matched the quality
of the SOTA autoregressive methods (Chen et al., 2020)."
INTRODUCTION,0.014028056112224449,"Despite the compelling results, the diffusion generative models are two to three orders of magnitude
slower than other generative models such as GANs and VAEs. Their primary limitation is that they
require up to thousands of diffusion steps during training to learn the target distribution. Therefore a
large number of reverse steps are often required at sampling time. Recently, extensive investigations
have been conducted to reduce the sampling steps for efÔ¨Åciently generating high-quality samples,
which we will discuss in the related work in Section 2. Distinctively, we conceived that we might
train a neural network to efÔ¨Åciently and adaptively estimate a much shorter noise schedule for sam-
pling while achieving generation performances comparable or superior to the conventional DPMs.
With such an incentive, after introducing the conventional DPMs as our background in Section 3, we
propose in Section 4 bilateral denoising diffusion models (BDDMs), named after a bilateral mod-
eling perspective ‚Äì parameterizing the forward and reverse processes with a schedule network and
a score network, respectively. We theoretically derive that the schedule network should be trained
after the score network is optimized. For training the schedule network, we propose a novel objec-
tive to minimize the gap between a newly derived lower bound and the log marginal likelihood. We
describe the training algorithm as well as the fast and high-quality sampling algorithm in Section 5.
The training of the schedule network converges very fast using our newly derived objective, and its
training only adds negligible overhead to DDPM‚Äôs. In Section 6, our neural vocoding experiments
demonstrated that BDDMs could generate high-Ô¨Ådelity samples with as few as three sampling steps.
Moreover, our method can produce speech samples indistinguishable from human speech with only
seven sampling steps (143x faster than WaveGrad and 28.6x faster than DiffWave)."
RELATED WORK,0.01603206412825651,"2
RELATED WORK"
RELATED WORK,0.018036072144288578,"Prior works showed that noise scheduling is crucial for efÔ¨Åcient and high-Ô¨Ådelity data generation in
DPMs. DDPMs (Ho et al., 2020) used a shared linear noise schedule for both training and sampling,
which, however, requires thousands of sampling iterations to obtain competitive results. To speed
up the sampling process, one class of related work, including (Chen et al., 2020; Kong et al., 2021;
Nichol & Dhariwal, 2021), attempts to use a different, shorter noise schedule for sampling. For
clarity, we thereafter denote the training noise schedule as Œ≤ ‚ààRT and the sampling noise schedule
as ÀÜŒ≤ ‚ààRN with N < T. In particular, Chen et al. (2020) applied a grid search (GS) algorithm to
select ÀÜŒ≤. Unfortunately, GS becomes prohibitively slow when N grows large, e.g., N = 6 took more
than a day on a single NVIDIA Tesla P40 GPU. This is because the time costs of GS algorithm grow
exponentially with N, i.e., O(9N) with 9 bins as the default setting in Chen et al. (2020). Instead
of searching, Kong et al. (2021) devised a fast sampling (FS) algorithm based on an expert-deÔ¨Åned
6-step noise schedule for their score network. However, this speciÔ¨Åcally tuned noise schedule is
hard to generalize to other score networks, tasks, or datasets."
RELATED WORK,0.02004008016032064,"Another class of noise scheduling methods searches for a subsequence of time indices of the training
noise schedule, which we call the time schedule. DDIMs (Song et al., 2021) introduced an accel-
erated reverse process that relies on a pre-speciÔ¨Åed time schedule. A linear and a quadratic time
schedule were used in DDIMs and showed superior generation quality over DDPMs within 10 to
100 sampling steps. Nichol & Dhariwal (2021) proposed a re-scaled noise schedule for fast sam-
pling, but this also requires pre-specifying the time schedule and the training noise schedule. Nichol
& Dhariwal (2021) also proposed learning variances for the reverse processes, whereas the variances
of the forward processes, i.e., the noise schedule, which affected both the means and variances of
the reverse processes, were not learnable. According to the results of (Song et al., 2021; Nichol &
Dhariwal, 2021), using a linear or quadratic time schedule resulted in quite different performances
in different datasets, implying that the optimal choice of schedule varies with the datasets. So, there
remains a challenge in Ô¨Ånding a short and effective schedule for fast sampling on different datasets."
RELATED WORK,0.022044088176352707,Published as a conference paper at ICLR 2022
RELATED WORK,0.02404809619238477,"Notably, Kong & Ping (2021) proposed a method to map a noise schedule to a time schedule for
fast sampling. In this sense, searching for a time schedule becomes a sub-set of the noise scheduling
problem, which resembles the above category of methods."
RELATED WORK,0.026052104208416832,"Although DPMs (Sohl-Dickstein et al., 2015) and DDPMs (Ho et al., 2019) mentioned that the noise
schedule could be learned by re-parameterization, the approach was not investigated in their works.
Closely related works that learn a noise schedule emerged until very recently. San-Roman et al.
(2021) proposed a noise estimation (NE) method, which trained a neural net with a regression loss
to estimate the noise scale from the noisy sample at each time point, and then predicted the next
noise scale. However, NE requires a prior assumption of the noise schedule following a linear or
Fibonacci rule. Most recently, a concurrent work to ours by Kingma et al. (2021) jointly trained a
neural net to predict the signal-to-noise ratio (SNR) by maximizing the variational lower bound. The
SNR was then used for noise scheduling. Different from ours, this scheduling neural net only took
t as input and is independent of the noisy sample generated during the loop of sampling process.
Intrinsically, with limited information about the sampled data, the predicted SNR could deviate from
the actual SNR of the noisy data during sampling."
BACKGROUND,0.028056112224448898,"3
BACKGROUND"
BACKGROUND,0.03006012024048096,"3.1
DIFFUSION PROBABILISTIC MODELS (DPMS)"
BACKGROUND,0.03206412825651302,"Given i.i.d. samples {x0 ‚ààRD} from an unknown data distribution pdata(x0), diffusion prob-
abilistic models (DPMs) (Sohl-Dickstein et al., 2015) deÔ¨Åne a forward process q(x1:T |x0) =
QT
t=1 q(xt|xt‚àí1) that converts any complex data distribution into a simple, tractable distribution af-
ter T steps of diffusion. A reverse process pŒ∏(xt‚àí1|xt) parameterized by Œ∏ is used to model the data
distribution: pŒ∏(x0) =
R
œÄ(xT ) QT
t=1 pŒ∏(xt‚àí1|xt)dx1:T , where œÄ(xT ) is the prior distribution for
starting the reverse process. Then, the variational parameters Œ∏ can be learned by maximizing the
standard log evidence lower bound (ELBO):"
BACKGROUND,0.03406813627254509,"Felbo := Eq """
BACKGROUND,0.036072144288577156,"log pŒ∏(x0|x1) ‚àí T
X"
BACKGROUND,0.03807615230460922,"t=2
DKL (q(xt‚àí1|xt, x0)||pŒ∏(xt‚àí1|xt)) ‚àíDKL (q(xT |x0)||œÄ(xT )) # . (1)"
BACKGROUND,0.04008016032064128,"3.2
DENOISING DIFFUSION PROBABILISTIC MODELS (DDPMS)"
BACKGROUND,0.04208416833667335,"As an extension to DPMs, denoising diffusion probabilistic models (DDPMs) (Ho et al., 2020)
applied the score matching technique (Hyvarinen & Dayan, 2005; Song & Ermon, 2019) to deÔ¨Åne
the reverse process. In particular, DDPMs considered a Gaussian diffusion process parameterized
by a noise schedule Œ≤ ‚ààRT with 0 < Œ≤1, . . . , Œ≤T < 1:"
BACKGROUND,0.04408817635270541,"qŒ≤(x1:T |x0) := T
Y"
BACKGROUND,0.04609218436873747,"t=1
qŒ≤t(xt|xt‚àí1),
where
qŒ≤t(xt|xt‚àí1) := N(
p"
BACKGROUND,0.04809619238476954,"1 ‚àíŒ≤txt‚àí1, Œ≤tI).
(2)"
BACKGROUND,0.050100200400801605,"Based on the nice property of isotropic Gaussians, one can express xt directly conditioned on x0:"
BACKGROUND,0.052104208416833664,"qŒ≤(xt|x0) = N(Œ±tx0, (1 ‚àíŒ±2
t)I),
where
Œ±t = tY i=1 p"
BACKGROUND,0.05410821643286573,"1 ‚àíŒ≤i.
(3)"
BACKGROUND,0.056112224448897796,"To revert this forward process, DDPMs employ a score network1 œµŒ∏(xt, Œ±t) to deÔ¨Åne"
BACKGROUND,0.05811623246492986,pŒ∏(xt‚àí1|xt) := N
BACKGROUND,0.06012024048096192,"1
‚àö1 ‚àíŒ≤t "
BACKGROUND,0.06212424849699399,"xt ‚àí
Œ≤t
p"
BACKGROUND,0.06412825651302605,"1 ‚àíŒ±2
t
œµŒ∏ (xt, Œ±t) ! , Œ£t ! ,
(4)"
BACKGROUND,0.06613226452905811,"1Here, œµŒ∏(xt, Œ±t) is conditioned on the continuous noise scale Œ±t, as in (Song et al., 2020b; Chen et al.,
2020). Alternatively, the score network can also be conditioned on a discrete time index œµŒ∏(xt, t), as in (Song
et al., 2021; Ho et al., 2020). An approximate mapping of a noise schedule to a time schedule (Kong & Ping,
2021) exists, therefore we consider conditioning on noise scales as the general case."
BACKGROUND,0.06813627254509018,Published as a conference paper at ICLR 2022
BACKGROUND,0.07014028056112225,"Figure 1: A bilateral denoising diffusion model (BDDM) introduces a junctional variable xt and a
schedule network œÜ. The schedule network can optimize the shortened noise schedule ÀÜŒ≤n(œÜ) if we
know the score of the distribution at the junctional step, using the KL divergence to directly compare
pŒ∏‚àó(ÀÜxn‚àí1|ÀÜxn = xt) against the re-parameterized forward process posteriors."
BACKGROUND,0.07214428857715431,where Œ£t is the co-variance matrix deÔ¨Åned for the reverse process. Ho et al. (2020) showed that
BACKGROUND,0.07414829659318638,"setting Œ£t = ÀúŒ≤tI =
1‚àíŒ±2
t‚àí1
1‚àíŒ±2
t Œ≤tI is optimal for a deterministic x0, while setting Œ£t = Œ≤tI is optimal
for a white noise x0 ‚àºN(0, I). Alternatively, Nichol & Dhariwal (2021) proposed learnable
variances by interpolating the two optimals with a jointly trained neural network, i.e., Œ£t,Œ∏(x) :=
diag(exp(vŒ∏(x) log Œ≤t + (1 ‚àívŒ∏(x)) log ÀúŒ≤t)), where vŒ∏(x) ‚ààRD is a trainable network."
BACKGROUND,0.07615230460921844,"Note that the calculation of the complete ELBO in Eq. (1) requires T forward passes of the score
network, which would make the training computationally prohibitive for a large T. To feasibly
train the score network, instead of computing the complete ELBO, Ho et al. (2020) proposed an
efÔ¨Åcient training mechanism by sampling from a discrete uniform distribution: t ‚àºU{1, ..., T},
x0 ‚àºpdata(x0), œµt ‚àºN(0, I) at each training iteration to compute the training loss:"
BACKGROUND,0.0781563126252505,"L(t)
ddpm(Œ∏) :=
œµt ‚àíœµŒ∏"
BACKGROUND,0.08016032064128256,"
Œ±tx0 +
q"
BACKGROUND,0.08216432865731463,"1 ‚àíŒ±2
tœµt, Œ±t  2"
BACKGROUND,0.0841683366733467,"2
,
(5)"
BACKGROUND,0.08617234468937876,"which is a re-weighted form of DKL (qŒ≤(xt‚àí1|xt, x0)||pŒ∏(xt‚àí1|xt)). Ho et al. (2020) reported that
the re-weighting worked effectively for learning Œ∏. Yet, we demonstrate it is deÔ¨Åcient for learning
the noise schedule Œ≤ in our ablation experiment in Section 6.2."
BACKGROUND,0.08817635270541083,"4
BILATERAL DENOISING DIFFUSION MODELS (BDDMS)"
PROBLEM FORMULATION,0.09018036072144289,"4.1
PROBLEM FORMULATION"
PROBLEM FORMULATION,0.09218436873747494,"For fast sampling with DPMs, we strive for a noise schedule ÀÜŒ≤ for sampling that is much shorter
than the noise schedule Œ≤ for training. As shown in Fig. 1, we deÔ¨Åne two separate diffusion
processes corresponding to the noise schedules, Œ≤ and ÀÜŒ≤, respectively. The upper diffusion pro-
cess parameterized by Œ≤ is the same as in Eq.
(2), whereas the lower process is deÔ¨Åned as
q ÀÜŒ≤(ÀÜx1:N|ÀÜx0) = QN
n=1 q ÀÜŒ≤n(ÀÜxn|ÀÜxn‚àí1) with much fewer diffusion steps (N ‚â™T). In our prob-"
PROBLEM FORMULATION,0.09418837675350701,"lem formulation, Œ≤ is given, but ÀÜŒ≤ is unknown. The goal is to Ô¨Ånd a ÀÜŒ≤ for the reverse process
pŒ∏(ÀÜxn‚àí1|ÀÜxn; ÀÜŒ≤n) such that ÀÜx0 can be effectively recovered from ÀÜxN with N reverse steps."
MODEL DESCRIPTION,0.09619238476953908,"4.2
MODEL DESCRIPTION"
MODEL DESCRIPTION,0.09819639278557114,"Although many prior arts (Ho et al., 2020; Chen et al., 2020; Song et al., 2021; San-Roman et al.,
2021) directly applied a shortened linear or Fibonacci noise schedule to the reverse process, we
argue that these are sub-optimal solutions. Theoretically, the diffusion process speciÔ¨Åed by a new
shortened noise schedule is essentially different from the one used to train the score network Œ∏.
Therefore, Œ∏ is not guaranteed suitable for reverting the shortened diffusion process. This issue
motivated a novel modeling perspective to establish a link between the shortened schedule ÀÜŒ≤ and
the score network Œ∏, i.e., to have ÀÜŒ≤ optimized according to Œ∏."
MODEL DESCRIPTION,0.10020040080160321,Published as a conference paper at ICLR 2022
MODEL DESCRIPTION,0.10220440881763528,"As a starting point, we consider an N = ‚åäT/œÑ‚åã, where 1 ‚â§œÑ < T is a hyperparameter controlling
the step size such that each diffusion step between two consecutive variables in the shorter diffusion
process corresponds to œÑ diffusion steps in the longer one. Based on Eq. (2), we deÔ¨Åne the following:"
MODEL DESCRIPTION,0.10420841683366733,q ÀÜŒ≤n+1(ÀÜxn+1|ÀÜxn = xt) := qŒ≤(xt+œÑ|xt) = N Ô£´ Ô£≠ s
MODEL DESCRIPTION,0.1062124248496994,"Œ±2
t+œÑ
Œ±2
t
xt,

1 ‚àíŒ±2
t+œÑ
Œ±2
t 
I Ô£∂"
MODEL DESCRIPTION,0.10821643286573146,"Ô£∏,
(6)"
MODEL DESCRIPTION,0.11022044088176353,"where xt is an intermediate diffused variable we introduced to link the two differently indexed
diffusion sequences. We call it a junctional variable, which can be easily generated given x0 and Œ≤
during training: xt = Œ±tx0 +
p"
MODEL DESCRIPTION,0.11222444889779559,"1 ‚àíŒ±2
tœµn."
MODEL DESCRIPTION,0.11422845691382766,"Unfortunately, for the reverse process when x0 is not given, the junctional variable is intractable.
However, our key observation is that while using the score by a score network Œ∏‚àótrained for the
long Œ≤-parameterized diffusion process, a short noise schedule ÀÜŒ≤(œÜ) can be optimized accordingly
by introducing a schedule network œÜ. We provide its mathematical derivations in Appendix A.3.
Next, we present a formal deÔ¨Ånition of BDDM and derive its training objectives, L(n)
score(Œ∏) and
L(n)
step(œÜ; Œ∏‚àó), for the score network and the schedule network, respectively, in more detail."
SCORE NETWORK,0.11623246492985972,"4.3
SCORE NETWORK"
SCORE NETWORK,0.11823647294589178,"Recall that a DDPM starts the reverse process with a white noise xT ‚àºN(0, I) and takes T steps
to recover the data distribution:"
SCORE NETWORK,0.12024048096192384,"pŒ∏(x0)
DDPM
:= EN(0,I)

EpŒ∏(x1:T ‚àí1|xT ) [pŒ∏(x0|x1:T )]

.
(7)"
SCORE NETWORK,0.12224448897795591,"A BDDM, in contrast, starts from the junctional variable xt, and reverts a shorter sequence of
diffusion random variables with only n steps:"
SCORE NETWORK,0.12424849699398798,"pŒ∏(ÀÜx0)
BDDM
:= Eq ÀÜ
Œ≤(ÀÜxn‚àí1;xt,œµn)

EpŒ∏(ÀÜx1:n‚àí2|ÀÜxn‚àí1) [pŒ∏(ÀÜx0|ÀÜx1:n‚àí1)]

,
2 ‚â§n ‚â§N,
(8)"
SCORE NETWORK,0.12625250501002003,"where q ÀÜŒ≤(ÀÜxn‚àí1; xt, œµn) is deÔ¨Åned as a re-parameterization on the posterior:"
SCORE NETWORK,0.1282565130260521,"q ÀÜŒ≤(ÀÜxn‚àí1; xt, œµn) :=q ÀÜŒ≤  ÀÜxn‚àí1"
SCORE NETWORK,0.13026052104208416,"ÀÜxn = xt, ÀÜx0 = xt ‚àí
p"
SCORE NETWORK,0.13226452905811623,"1 ‚àíÀÜŒ±2nœµn
ÀÜŒ±n ! (9) =N Ô£´ Ô£≠
1
q"
SCORE NETWORK,0.1342685370741483,"1 ‚àíÀÜŒ≤n
xt ‚àí
ÀÜŒ≤n
q"
SCORE NETWORK,0.13627254509018036,"(1 ‚àíÀÜŒ≤n)(1 ‚àíÀÜŒ±2n)
œµn, 1 ‚àíÀÜŒ±2
n‚àí1
1 ‚àíÀÜŒ±2n
ÀÜŒ≤nI Ô£∂"
SCORE NETWORK,0.13827655310621242,"Ô£∏,
(10)"
SCORE NETWORK,0.1402805611222445,"where ÀÜŒ±n = Qn
i=1 q"
SCORE NETWORK,0.14228456913827656,"1 ‚àíÀÜŒ≤i, xt = Œ±tx0 +
p"
SCORE NETWORK,0.14428857715430862,"1 ‚àíŒ±2
tœµn is the junctional variable that maps xt
to ÀÜxn given an approximate index t ‚àºU{(n ‚àí1)œÑ, ..., nœÑ ‚àí1, nœÑ} and a sampled white noise
œµn ‚àºN(0, I). Detailed derivation from Eq. (9) to (10) is provided in Appendix A.2."
TRAINING OBJECTIVE FOR SCORE NETWORK,0.1462925851703407,"4.3.1
TRAINING OBJECTIVE FOR SCORE NETWORK"
TRAINING OBJECTIVE FOR SCORE NETWORK,0.14829659318637275,"With the above deÔ¨Ånition, a new form of lower bound to the log marginal likelihood can be derived
such that log pŒ∏(ÀÜx0) ‚â•F(n)
score(Œ∏) := ‚àíL(n)
score(Œ∏) ‚àíRŒ∏(ÀÜx0, xt), where"
TRAINING OBJECTIVE FOR SCORE NETWORK,0.15030060120240482,"L(n)
score(Œ∏) :=DKL

pŒ∏(ÀÜxn‚àí1|ÀÜxn = xt)||q ÀÜŒ≤(ÀÜxn‚àí1; xt, œµn)

,
(11)"
TRAINING OBJECTIVE FOR SCORE NETWORK,0.1523046092184369,"RŒ∏(ÀÜx0, xt) := ‚àíEpŒ∏(ÀÜx1|ÀÜxn=xt) [log pŒ∏(ÀÜx0|ÀÜx1)] .
(12)"
TRAINING OBJECTIVE FOR SCORE NETWORK,0.15430861723446893,"See detailed derivation in Proposition 1 in Appendix A.2. In the following Proposition 2, we prove
that via the junctional variable xt, the solution Œ∏‚àófor optimizing the objective L(t)
ddpm(Œ∏), ‚àÄt ‚àà"
TRAINING OBJECTIVE FOR SCORE NETWORK,0.156312625250501,"{1, ..., T} is also the solution for optimizing L(n)
score(Œ∏), ‚àÄn ‚àà{2, ..., N}. Thereby, we show that the
score network Œ∏ can be trained with L(t)
ddpm(Œ∏) and re-used for reverting the short diffusion process
over ÀÜxN:0. Although the newly derived lower bound result in the same objective as the conventional
score network, it for the Ô¨Årst time establishes a link between the score network Œ∏ and ÀÜxN:0. The
connection is essential for learning ÀÜŒ≤, which we will describe next."
TRAINING OBJECTIVE FOR SCORE NETWORK,0.15831663326653306,Published as a conference paper at ICLR 2022
SCHEDULE NETWORK,0.16032064128256512,"4.4
SCHEDULE NETWORK"
SCHEDULE NETWORK,0.1623246492985972,"In BDDMs, a schedule network is introduced to the forward process by re-parameterizing ÀÜŒ≤n as
ÀÜŒ≤n(œÜ) = fœÜ

xt; ÀÜŒ≤n+1

, and recall that during training, we can use xt = Œ±tx0 +
p"
SCHEDULE NETWORK,0.16432865731462926,"1 ‚àíŒ±2
tœµn and"
SCHEDULE NETWORK,0.16633266533066132,"ÀÜŒ≤n+1 = 1 ‚àí
Œ±2
t+œÑ
Œ±2
t . Through the re-parameterization, the task of noise scheduling, i.e., searching"
SCHEDULE NETWORK,0.1683366733466934,"for ÀÜŒ≤, can now be reformulated as training a schedule network fœÜ that ancestrally estimates data-
dependent variances. The schedule network learns to predict ÀÜŒ≤n based on the current noisy sample
xt ‚Äì this makes our method fundamentally different from existing and concurrent work, includ-
ing Kingma et al. (2021) ‚Äì as we reveal that, aside from ÀÜŒ≤n+1, t, or n that reÔ¨Çects diffusion step
information, xt is also essential for noise scheduling from a reverse direction at inference time."
SCHEDULE NETWORK,0.17034068136272545,"SpeciÔ¨Åcally, we adopt the ancestral step information (ÀÜŒ≤n+1) to derive an upper bound for the current
step while leaving the schedule network only to take the current noisy sample xt as input to predict
a relative change of noise scales against the ancestral step. First, we derive an upper bound of ÀÜŒ≤n
by proving 0 < ÀÜŒ≤n < min
n
1 ‚àí
ÀÜŒ±2
n+1
1‚àíÀÜŒ≤n+1 , ÀÜŒ≤n+1
o
in Appendix A.1. Then, by multiplying the upper"
SCHEDULE NETWORK,0.17234468937875752,"bound by a ratio estimated by a neural network œÉœÜ : RD 7‚Üí(0, 1), we deÔ¨Åne"
SCHEDULE NETWORK,0.1743486973947896,"fœÜ(xt; ÀÜŒ≤n+1) := min

1 ‚àí
ÀÜŒ±2
n+1
1 ‚àíÀÜŒ≤n+1
, ÀÜŒ≤n+1"
SCHEDULE NETWORK,0.17635270541082165,"
œÉœÜ(xt),
(13)"
SCHEDULE NETWORK,0.17835671342685372,"where the network parameter set œÜ is learned to estimate the ratio between two consecutive noise
scales (ÀÜŒ≤n and ÀÜŒ≤n+1) from the current noisy input xt."
SCHEDULE NETWORK,0.18036072144288579,"Finally, at inference time for noise scheduling, starting from a maximum reverse steps (N) and two
hyperparameters (ÀÜŒ±N, ÀÜŒ≤N), we ancestrally predict the noise scale ÀÜŒ≤n(œÜ) = fœÜ

ÀÜxn; ÀÜŒ≤n+1

, for n"
SCHEDULE NETWORK,0.18236472945891782,"from N to 1, and cumulatively update the product ÀÜŒ±n =
ÀÜŒ±n+1
‚àö"
SCHEDULE NETWORK,0.1843687374749499,1‚àíÀÜŒ≤n+1 .
TRAINING OBJECTIVE FOR SCHEDULE NETWORK,0.18637274549098196,"4.4.1
TRAINING OBJECTIVE FOR SCHEDULE NETWORK"
TRAINING OBJECTIVE FOR SCHEDULE NETWORK,0.18837675350701402,"Here we describe how to learn the network parameters œÜ effectively. First, we demonstrated that
œÜ should be trained after Œ∏ is well-optimized, referring to Proposition 3 in Appendix A.3. The
Proposition also shows that we are minimizing the gap between the lower bound F(n)
score(Œ∏‚àó) and
log pŒ∏‚àó(ÀÜx0), i.e., log pŒ∏‚àó(ÀÜx0) ‚àíF(n)
score(Œ∏‚àó), by minimizing the following objective"
TRAINING OBJECTIVE FOR SCHEDULE NETWORK,0.1903807615230461,"L(n)
step(œÜ; Œ∏‚àó) :=DKL

pŒ∏‚àó(ÀÜxn‚àí1|ÀÜxn = xt)||q ÀÜŒ≤n(œÜ)(ÀÜxn‚àí1; x0, Œ±t)

,
(14)"
TRAINING OBJECTIVE FOR SCHEDULE NETWORK,0.19238476953907815,"which is deÔ¨Åned as a KL divergence to directly compare pŒ∏‚àó(ÀÜxn‚àí1|ÀÜxn = xt) against the re-
parameterized forward process posteriors, which are tractable when conditioned on the junctional
noise scale Œ±t and x0."
TRAINING OBJECTIVE FOR SCHEDULE NETWORK,0.19438877755511022,"The detailed derivation of Eq. (14) is also provided in the proof of Proposition 3 to get its concrete
formulas as shown in Step (8-10) in Alg. 2."
TRAINING OBJECTIVE FOR SCHEDULE NETWORK,0.1963927855711423,"5
ALGORITHMS: TRAINING, NOISE SCHEDULING, AND SAMPLING"
TRAINING SCORE AND SCHEDULE NETWORKS,0.19839679358717435,"5.1
TRAINING SCORE AND SCHEDULE NETWORKS"
TRAINING SCORE AND SCHEDULE NETWORKS,0.20040080160320642,"Following the theoretical result in Appendix A.3, Œ∏ should be optimized before learning œÜ. Thereby
Ô¨Årst, to train the score network œµŒ∏, we refer to the settings in (Ho et al., 2020; Chen et al., 2020; Song
et al., 2021) to deÔ¨Åne Œ≤ as a linear noise schedule: Œ≤t = Œ≤start + t"
TRAINING SCORE AND SCHEDULE NETWORKS,0.20240480961923848,"T (Œ≤end ‚àíŒ≤start),
for
1 ‚â§t ‚â§T,
where Œ≤start and Œ≤end are two hyperparameter that speciÔ¨Åes the start value and the end value. This
results in Algorithm 1, which resembles the training algorithm in (Ho et al., 2020)."
TRAINING SCORE AND SCHEDULE NETWORKS,0.20440881763527055,"Next, based on the converged score network Œ∏‚àó, we train the schedule network œÜ. We draw an
n ‚àºU{2, . . . , N} at each training step, and then draw a t ‚àºU{(n ‚àí1)œÑ, ..., nœÑ}. These together
can be re-formulated as directly drawing t ‚àºU{œÑ, ..., T ‚àíœÑ} for a Ô¨Åner-scale time step. Then, we"
TRAINING SCORE AND SCHEDULE NETWORKS,0.20641282565130262,Published as a conference paper at ICLR 2022
TRAINING SCORE AND SCHEDULE NETWORKS,0.20841683366733466,Algorithm 1 Training Score Network (Œ∏)
TRAINING SCORE AND SCHEDULE NETWORKS,0.21042084168336672,"1: Given T, {Œ≤t}T
t=1
2: {Œ±t}T
t=1 = {Qt
i=1
‚àö1 ‚àíŒ≤t}T
t=1
3: repeat
4:
x0 ‚àºpdata(x0)
5:
t ‚àºU{1, . . . , T}
6:
œµt ‚àºN(0, I)
7:
xt = Œ±tx0 +
p"
TRAINING SCORE AND SCHEDULE NETWORKS,0.2124248496993988,"1 ‚àíŒ±2
tœµt
8:
L(t)
ddpm = ‚à•œµt ‚àíœµŒ∏(xt, Œ±t)‚à•2
2
9:
Take a gradient descent step on ‚àáŒ∏L(t)
ddpm
10: until converged"
TRAINING SCORE AND SCHEDULE NETWORKS,0.21442885771543085,Algorithm 3 Noise Scheduling
TRAINING SCORE AND SCHEDULE NETWORKS,0.21643286573146292,"1: Given Œ∏‚àó, ÀÜŒ±N, ÀÜŒ≤N, xN ‚àºN(0, I)
2: for n = N to 2 do
3:
ÀÜxn‚àí1 ‚àºpŒ∏‚àó(ÀÜxn‚àí1|ÀÜxn; ÀÜŒ±n, ÀÜŒ≤n)
4:
ÀÜŒ±n‚àí1 =
ÀÜŒ±n
‚àö"
TRAINING SCORE AND SCHEDULE NETWORKS,0.218436873747495,"1‚àíÀÜŒ≤n
5:
ÀÜŒ≤n‚àí1 = min{1 ‚àíÀÜŒ±2
n‚àí1, ÀÜŒ≤n}œÉœÜ(ÀÜxn‚àí1)
6:
if ÀÜŒ≤n‚àí1 < Œ≤1 then
7:
return ÀÜŒ≤n, . . . , ÀÜŒ≤N
8:
end if
9: end for
10: return ÀÜŒ≤1, . . . , ÀÜŒ≤N"
TRAINING SCORE AND SCHEDULE NETWORKS,0.22044088176352705,Algorithm 2 Training Schedule Network (œÜ)
TRAINING SCORE AND SCHEDULE NETWORKS,0.22244488977955912,"1: Given Œ∏‚àó, œÑ, T, {Œ±t, Œ≤t}T
t=1
2: repeat
3:
x0 ‚àºpdata(x0)
4:
t ‚àºU{œÑ, . . . , T ‚àíœÑ}
5:
Œ¥t = 1 ‚àíŒ±2
t
6:
œµn ‚àºN(0, I)
7:
xt = Œ±tx0 + ‚àöŒ¥tœµn"
TRAINING SCORE AND SCHEDULE NETWORKS,0.22444889779559118,"8:
ÀÜŒ≤n = min

Œ¥t, 1 ‚àí
Œ±2
t+œÑ
Œ±2
t"
TRAINING SCORE AND SCHEDULE NETWORKS,0.22645290581162325,"
œÉœÜ(xt)"
TRAINING SCORE AND SCHEDULE NETWORKS,0.22845691382765532,"9:
C = 4‚àí1 log(Œ¥t/ÀÜŒ≤n) + 2‚àí1D

ÀÜŒ≤n/Œ¥t ‚àí1
"
TRAINING SCORE AND SCHEDULE NETWORKS,0.23046092184368738,"10:
L(n)
step =
Œ¥t
2(Œ¥t‚àíÀÜŒ≤n)"
TRAINING SCORE AND SCHEDULE NETWORKS,0.23246492985971945,"œµn ‚àí
ÀÜŒ≤n
Œ¥t œµŒ∏‚àó(xt, Œ±t)

2 2 + C"
TRAINING SCORE AND SCHEDULE NETWORKS,0.23446893787575152,"11:
Take a gradient descent step on ‚àáœÜL(n)
step
12: until converged
Algorithm 4 Sampling"
TRAINING SCORE AND SCHEDULE NETWORKS,0.23647294589178355,"1: Given Œ∏‚àó, {ÀÜŒ≤n}Ns
n=1, ÀÜxNs ‚àºN(0, I)"
TRAINING SCORE AND SCHEDULE NETWORKS,0.23847695390781562,"2: {ÀÜŒ±n}Ns
n=1 =
Qn
i=1 q"
TRAINING SCORE AND SCHEDULE NETWORKS,0.24048096192384769,1 ‚àíÀÜŒ≤n Ns
TRAINING SCORE AND SCHEDULE NETWORKS,0.24248496993987975,"n=1
3: for n = Ns to 1 do
4:
ÀÜxn‚àí1 ‚àºpŒ∏‚àó(ÀÜxn‚àí1|ÀÜxn; ÀÜŒ±n, ÀÜŒ≤n)
5: end for
6: return ÀÜx0"
TRAINING SCORE AND SCHEDULE NETWORKS,0.24448897795591182,"sequentially compute the variables needed for calculating L(n)
step(œÜ; Œ∏‚àó), as presented in Algorithm 2.
We observed that, although a linear schedule is used to deÔ¨Åne Œ≤, the noise schedule of ÀÜŒ≤ predicted
by fœÜ is not limited to but rather different from a linear one."
NOISE SCHEDULING FOR FAST AND HIGH-QUALITY SAMPLING,0.24649298597194388,"5.2
NOISE SCHEDULING FOR FAST AND HIGH-QUALITY SAMPLING"
NOISE SCHEDULING FOR FAST AND HIGH-QUALITY SAMPLING,0.24849699398797595,"After the score network and the schedule network are trained, the inference procedure can divide
into two phases: (1) the noise scheduling phase and (2) the sampling phase."
NOISE SCHEDULING FOR FAST AND HIGH-QUALITY SAMPLING,0.250501002004008,"First, we run the noise scheduling process similarly to a sampling process with N iterations maxi-
mum. Different from training, where Œ±t is forward-computed, ÀÜŒ±n is instead a backward-computed
variable (from N to 1) that may deviate from the forward one because {ÀÜŒ≤i}n‚àí1
i
are unknown in the
noise scheduling phase during inference. To start noise scheduling, we Ô¨Årst set two hyperparame-
ters: ÀÜŒ±N and ÀÜŒ≤N. We use Œ≤1, the smallest noise scale seen in training, as a threshold to early stop
the noise scheduling process so that we can ignore small noise scales (< Œ≤1) that were never seen
by the score network. Overall, the noise scheduling process presents in Algorithm 3."
NOISE SCHEDULING FOR FAST AND HIGH-QUALITY SAMPLING,0.25250501002004005,"In practice, we apply a grid search algorithm of M bins to Algorithm 3, which takes O(M 2) time, to
Ô¨Ånd proper values for (ÀÜŒ±N, ÀÜŒ≤N). We used M = 9 as in (Chen et al., 2020). The grid search for our
noise scheduling algorithm can be evaluated on a small subset of the training samples. Empirically,
even as few as 1 sample for evaluation works well in our algorithm. Finally, given the predicted
noise schedule ÀÜŒ≤ ‚ààRNs, we generate samples with Ns sampling steps, as shown in Algorithm 4."
EXPERIMENTS,0.2545090180360721,"6
EXPERIMENTS"
EXPERIMENTS,0.2565130260521042,"We conducted a series of experiments on neural vocoding tasks to evaluate the proposed BDDMs.
First, we compared BDDMs against several strongest models that have been published: the mixture
of logistics (MoL) WaveNet (Oord et al., 2018) implemented in (Yamamoto, 2020), the WaveGlow
(Prenger et al., 2019) implemented in (Valle, 2020), the MelGAN (Kumar et al., 2019) implemented
in (Kumar, 2019), the HiFi-GAN (Kong et al., 2020b) implemented in (Kong et al., 2020a) and
the two most recently proposed diffusion-based vocoders, i.e., WaveGrad (Chen et al., 2020) and
DiffWave (Kong et al., 2021), both re-implemented in our code. The hyperparameter settings of
BDDMs and all these models are detailed in Appendix B."
EXPERIMENTS,0.25851703406813625,"In addition, we also compared BDDMs to a variety of scheduling and acceleration techniques ap-
plicable to DDPMs, including the grid search (GS) approach in WaveGrad, the fast sampling (FS)"
EXPERIMENTS,0.2605210420841683,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.2625250501002004,"Table 1: Comparison of neural vocoders in terms of MOS with 95% conÔ¨Ådence intervals, real-time
factor (RTF) and model size in megabytes (MB) for inference. The highest score and the scores that
are not signiÔ¨Åcantly different from the highest score (p-values ‚â•0.05) are bold-faced."
EXPERIMENTS,0.26452905811623245,"Neural Vocoder
MOS
RTF
Size"
EXPERIMENTS,0.2665330661322645,"Ground-truth
4.64 ¬± 0.08
‚Äî
‚Äî"
EXPERIMENTS,0.2685370741482966,"WaveNet (MoL) (Oord et al., 2018)
3.52 ¬± 0.16
318.6
282MB
WaveGlow (Prenger et al., 2019)
3.03 ¬± 0.15
0.0198
645MB
MelGAN (Kumar et al., 2019)
3.48 ¬± 0.14
0.00396
17MB
HiFi-GAN (Kong et al., 2020b)
4.33 ¬± 0.12
0.0134
54MB
WaveGrad - 1000 steps (Chen et al., 2020)
4.36 ¬± 0.13
38.2
183MB
DiffWave - 200 steps (Kong et al., 2021)
4.49 ¬± 0.13
7.30
27MB"
EXPERIMENTS,0.27054108216432865,"BDDM - 3 steps (ÀÜŒ±N = 0.68, ÀÜŒ≤N = 0.53)
3.64 ¬± 0.13
0.110
27MB
BDDM - 7 steps (ÀÜŒ±N = 0.62, ÀÜŒ≤N = 0.42)
4.43 ¬± 0.11
0.256
27MB
BDDM - 12 steps (ÀÜŒ±N = 0.67, ÀÜŒ≤N = 0.12)
4.48 ¬± 0.12
0.438
27MB"
EXPERIMENTS,0.2725450901803607,"Table 2: Comparison of sampling acceleration methods with the same score network and the same
number of steps. The highest score and the scores that are not signiÔ¨Åcantly different from the highest
score (p-values ‚â•0.05) are bold-faced."
EXPERIMENTS,0.2745490981963928,"Steps
Acceleration Method
STOI
PESQ
MOS 3"
EXPERIMENTS,0.27655310621242485,"GS (Chen et al., 2020)
0.965 ¬± 0.009
3.66 ¬± 0.20
3.61 ¬± 0.12
FS (Kong et al., 2021)
0.939 ¬± 0.023
3.09 ¬± 0.23
3.10 ¬± 0.12
DDIM (Song et al., 2021)
0.943 ¬± 0.015
3.42 ¬± 0.27
3.25 ¬± 0.13
NE (San-Roman et al., 2021)
0.966 ¬± 0.010
3.62 ¬± 0.18
3.55 ¬± 0.12
BDDM
0.966 ¬± 0.011
3.63 ¬± 0.24
3.64 ¬± 0.13 7"
EXPERIMENTS,0.2785571142284569,"FS (Kong et al., 2021)
0.981 ¬± 0.006
3.68 ¬± 0.24
3.70 ¬± 0.14
DDIM (Song et al., 2021)
0.974 ¬± 0.008
3.85 ¬± 0.12
3.94 ¬± 0.12
NE (San-Roman et al., 2021)
0.978 ¬± 0.007
3.75 ¬± 0.18
4.02 ¬± 0.11
BDDM
0.983 ¬± 0.006
3.96 ¬± 0.09
4.43 ¬± 0.11"
EXPERIMENTS,0.280561122244489,"12
DDIM (Song et al., 2021)
0.979 ¬± 0.006
3.90 ¬± 0.10
4.16 ¬± 0.12
NE (San-Roman et al., 2021)
0.981 ¬± 0.007
3.82 ¬± 0.13
3.98 ¬± 0.14
BDDM
0.987 ¬± 0.006
3.98 ¬± 0.12
4.48 ¬± 0.12"
EXPERIMENTS,0.28256513026052105,"approach based on a user-deÔ¨Åned 6-step schedule in DiffWave, the DDIMs (Song et al., 2021) and
a noise estimation (NE) approach (San-Roman et al., 2021). For fair and reproducible comparison
with other models and approaches, we used the LJSpeech dataset (Ito & Johnson, 2017), which
consists of 13,100 22kHz audio clips of a female speaker. All diffusion models were trained on the
same training split as in (Chen et al., 2020). We also replicated the comparative experiment of neural
vocoding using a multi-speaker VCTK dataset (Yamagishi et al., 2019) as presented in Appendix C
and obtained a result consistent with that obtained from the LJSpeech dataset."
SAMPLING QUALITY IN OBJECTIVE AND SUBJECTIVE METRICS,0.2845691382765531,"6.1
SAMPLING QUALITY IN OBJECTIVE AND SUBJECTIVE METRICS"
SAMPLING QUALITY IN OBJECTIVE AND SUBJECTIVE METRICS,0.2865731462925852,"To assess the quality of each generated audio sample, we used both objective and subjective mea-
sures for comparing different neural vocoders given the same ground-truth spectrogram s as the
condition, i.e., œµŒ∏(x, s, Œ±t). SpeciÔ¨Åcally, we used two scale-invariant metrics: the perceptual evalu-
ation of speech quality (PESQ) (Rix et al., 2001) and the short-time objective intelligibility (STOI)
(Taal et al., 2010) to measure the noisiness and the distortion of the generated speech relative to the
reference speech. Mean opinion score (MOS) was also used as a subjective metric for evaluating the
naturalness of the generated speech. The assessment scheme of MOS is included in Appendix B."
SAMPLING QUALITY IN OBJECTIVE AND SUBJECTIVE METRICS,0.28857715430861725,"In Table 1, we compared BDDMs against the state-of-the-art (SOTA) vocoders. To predict noise
schedules with different sampling steps (3, 7, and 12), we set three pairs of {ÀÜŒ±N, ÀÜŒ≤N} for BDDMs
by running on Algorithm 3 a quick hyperparameter grid search, which is detailed in Appendix B.
Among the 9 evaluated vocoders, only our proposed BDDMs with 7 and 12 steps and DiffWave with
200 steps showed no statistic-signiÔ¨Åcant difference from the ground-truth in terms of MOS. More-
over, BDDMs signiÔ¨Åcantly outspeeded DiffWave in terms of RTFs. Notably, previous diffusion-"
SAMPLING QUALITY IN OBJECTIVE AND SUBJECTIVE METRICS,0.2905811623246493,Published as a conference paper at ICLR 2022
SAMPLING QUALITY IN OBJECTIVE AND SUBJECTIVE METRICS,0.2925851703406814,"Figure 2: Different training losses for œÉœÜ
Figure 3: Different lower bounds to log pŒ∏(x0)"
SAMPLING QUALITY IN OBJECTIVE AND SUBJECTIVE METRICS,0.29458917835671344,"based vocoders achieved high MOS scores at the cost of an unacceptable RTF for industrial deploy-
ment. In contrast, BDDMs managed to achieve a high standard of generation quality with only 7
sampling steps (143x faster than WaveGrad and 28.6x faster than DiffWave)."
SAMPLING QUALITY IN OBJECTIVE AND SUBJECTIVE METRICS,0.2965931863727455,"In Table 2, we evaluated BDDMs and alternative accelerated sampling methods, which used the
same score network for a pair-to-pair comparison. The GS method performed stably when the
step number was small (i.e., N ‚â§6) but not scalable to more step numbers, which were therefore
bypassed in the comparisons of 7 and 12 steps. The FS method by Song et al. (2021) was linearly
interpolated to 3 and 7 steps for a fair comparison. Comparing its 7-step and 3-step results, we
observed that the FS performance degraded drastically. Both the DDIM and the NE methods were
stable across all the steps but were not performing competitively enough. In comparison, BDDMs
consistently attained the leading scores across all the steps. This evaluation conÔ¨Årmed that BDDM
was superior to other acceleration methods for DPMs in terms of both stability and quality."
ABLATION STUDY AND ANALYSIS,0.2985971943887776,"6.2
ABLATION STUDY AND ANALYSIS"
ABLATION STUDY AND ANALYSIS,0.30060120240480964,"We attribute the primary advantage of BDDMs to the newly derived objective L(n)
step for learning œÜ.
To better reason about this, we performed an ablation study, where we substituted the proposed loss
with the standard negative ELBO for learning œÜ as mentioned by Sohl-Dickstein et al. (2015). We
plotted the network outputs with different training losses in Fig. 2. It turned out that, when using
L(n)
elbo to learn œÜ, the network output rapidly collapsed to zero within several training steps; whereas,
the network trained with L(n)
step produced Ô¨Çuctuating outputs. The Ô¨Çuctuation is a desirable property
showing the network properly predicts t-dependent noise scales, as t is a random time step drawn
from a uniform distribution in training."
ABLATION STUDY AND ANALYSIS,0.3026052104208417,"By setting ÀÜŒ≤ = Œ≤, we empirically validated that F(t)
bddm := F(t)
score+L(t)
step ‚â•F(t)
elbo with their respective
values at t ‚àà[20, 180] using the same optimized Œ∏‚àó. Each value is provided with 95% conÔ¨Ådence
intervals, as shown in Fig. 3. In this experiment, we used the LJ speech dataset and set T = 200 and
œÑ = 20. Notably, we dropped their common entropy term RŒ∏(ÀÜx0, xt) < 0 to mainly compare their
KL divergences. This explains those positive lower bound values in the plot. The graph shows that
our proposed bound F(t)
bddm is always a tighter lower bound than the standard one across all examined
t. Moreover, we found that F(t)
bddm attained low values with a relatively much lower variance for
t ‚â§50, where F(t)
elbo was highly volatile. This implies that F(t)
bddm better tackles the difÔ¨Åcult training
part, i.e., when the score becomes more challenging to estimate as t ‚Üí0."
CONCLUSIONS,0.3046092184368738,"7
CONCLUSIONS"
CONCLUSIONS,0.3066132264529058,"BDDMs parameterize the forward and reverse processes with a schedule network and a score net-
work, of which the former‚Äôs optimization is tied with the latter by introducing a junctional variable.
We derived a new lower bound that leads to the same training loss for the score network as in DDPMs
(Ho et al., 2020), which thus enables inheriting any pre-trained score networks in DDPMs. We also
showed that training the schedule network after a well-optimized score network can be viewed as
tightening the lower bound. Followed from the theoretical results, an efÔ¨Åcient training algorithm and
a noise scheduling algorithm were respectively designed for BDDMs. Finally, in our experiments,
BDDMs showed a clear edge over the previous diffusion-based vocoders."
CONCLUSIONS,0.30861723446893785,Published as a conference paper at ICLR 2022
REFERENCES,0.3106212424849699,REFERENCES
REFERENCES,0.312625250501002,"Miko≈Çaj Bi¬¥nkowski, Jeff Donahue, Sander Dieleman, Aidan Clark, Erich Elsen, Norman
Casagrande, Luis C. Cobo, and Karen Simonyan. High Ô¨Ådelity speech synthesis with adversarial
networks. International conference on learning representations, 2020."
REFERENCES,0.31462925851703405,"S Bond-Taylor, A Leach, Y Long, and CG Willcocks. Deep generative modelling: A comparative
review of vaes, gans, normalizing Ô¨Çows, energy-based and autoregressive models. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, 2021."
REFERENCES,0.3166332665330661,"Miguel A Carreira-Perpinan and Geoffrey E Hinton. On contrastive divergence learning. AISTATS,
pp. 33‚Äì40, 2005."
REFERENCES,0.3186372745490982,"Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan. Wave-
grad: Estimating gradients for waveform generation. In International conference on learning
representations, 2020."
REFERENCES,0.32064128256513025,"Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differ-
ential equations. Advances in neural information processing systems, pp. 6571‚Äì6583, 2018."
REFERENCES,0.3226452905811623,"Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances
in neural information processing systems, 34, 2021."
REFERENCES,0.3246492985971944,"Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv
preprint arXiv:1605.08803, 2016."
REFERENCES,0.32665330661322645,"Brendan J Frey. Local probability propagation for factor analysis. Advances in neural information
processing systems, 12:442‚Äì448, 1999."
REFERENCES,0.3286573146292585,"Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information
processing systems, 27, 2014."
REFERENCES,0.3306613226452906,"G. E. Hinton. Training products of experts by minimizing contrastive divergence. neural computa-
tion. Neural computation, pp. 14(8):1771‚Äì1800, 2002."
REFERENCES,0.33266533066132264,"Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. Flow++: Improving Ô¨Çow-
based generative models with variational dequantization and architecture design. ICML, 2019."
REFERENCES,0.3346693386773547,"Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in
neural information processing systems, 33:6840‚Äì6851, 2020."
REFERENCES,0.3366733466933868,"Aapo Hyvarinen and Peter Dayan. Estimation of non-normalized statistical models by score match-
ing. Journal of Machine Learning Research, pp. 6(4), 2005."
REFERENCES,0.33867735470941884,"Keith Ito and Linda Johnson. The lj speech dataset. https://keithito.com/LJ-Speech-Dataset/, 2017."
REFERENCES,0.3406813627254509,"Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lock-
hart, Florian Stimberg, Aaron Oord, Sander Dieleman, and Koray Kavukcuoglu. EfÔ¨Åcient neural
audio synthesis. In International Conference on Machine Learning, pp. 2410‚Äì2419. PMLR, 2018."
REFERENCES,0.342685370741483,"Diederik P Kingma and Prafulla Dhariwal. Glow: Generative Ô¨Çow with invertible 1x1 convolutions.
Advances in neural information processing systems, pp. 10215‚Äì10224, 2018."
REFERENCES,0.34468937875751504,"Diederik P Kingma and Max Welling. Auto-encoding variational bayes. stat, 1050:1, 2014."
REFERENCES,0.3466933867735471,"Diederik P Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. In
Advances in neural information processing systems, 2021."
REFERENCES,0.3486973947895792,"Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. HiÔ¨Å-gan: Generative adversarial networks for
efÔ¨Åcient and high Ô¨Ådelity speech synthesis. https://github.com/jik876/hifi-gan,
2020a."
REFERENCES,0.35070140280561124,"Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. HiÔ¨Å-gan: Generative adversarial networks for
efÔ¨Åcient and high Ô¨Ådelity speech synthesis. Advances in neural information processing systems,
33, 2020b."
REFERENCES,0.3527054108216433,Published as a conference paper at ICLR 2022
REFERENCES,0.35470941883767537,"Zhifeng Kong and Wei Ping. On fast sampling of diffusion probabilistic models. In ICML Workshop
on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models, 2021."
REFERENCES,0.35671342685370744,"Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile
diffusion model for audio synthesis. International conference on learning representations, 2021."
REFERENCES,0.3587174348697395,"Kundan Kumar, Rithesh Kumar, Thibault de Boissiere, Lucas Gestin, Wei Zhen Teoh, Jose Sotelo,
Alexandre de Br¬¥ebisson, Yoshua Bengio, and Aaron C Courville. Melgan: Generative adversarial
networks for conditional waveform synthesis. Advances in neural information processing systems,
32, 2019."
REFERENCES,0.36072144288577157,"Rithesh Kumar. OfÔ¨Åcial repository for the paper melgan: Generative adversarial networks for condi-
tional waveform synthesis. https://github.com/descriptinc/melgan-neurips,
2019."
REFERENCES,0.3627254509018036,"Max WY Lam, Jun Wang, Dan Su, and Dong Yu. Effective low-cost time-domain audio separation
using globally attentive locally recurrent networks. In 2021 IEEE Spoken Language Technology
Workshop (SLT), pp. 801‚Äì808. IEEE, 2021."
REFERENCES,0.36472945891783565,"Lars Maal√∏e, Marco Fraccaro, Valentin Li¬¥evin, and Ole Winther. Biva: A very deep hierarchy of
latent variables for generative modeling. Advances in neural information processing systems, pp.
6548‚Äì6558, 2019."
REFERENCES,0.3667334669338677,"Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models.
In International Conference on Machine Learning, pp. 8162‚Äì8171. PMLR, 2021."
REFERENCES,0.3687374749498998,"Aaron Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, Koray Kavukcuoglu,
George Driessche, Edward Lockhart, Luis Cobo, Florian Stimberg, et al. Parallel wavenet: Fast
high-Ô¨Ådelity speech synthesis. In International conference on machine learning, pp. 3918‚Äì3926.
PMLR, 2018."
REFERENCES,0.37074148296593185,"George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lak-
shminarayanan. Normalizing Ô¨Çows for probabilistic modeling and inference. JMLR, pp. 22(57):1‚Äì
64, 2021."
REFERENCES,0.3727454909819639,"Ryan Prenger, Rafael Valle, and Bryan Catanzaro. Waveglow: A Ô¨Çow-based generative network
for speech synthesis. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), pp. 3617‚Äì3621. IEEE, 2019."
REFERENCES,0.374749498997996,"Flavio Protasio Ribeiro, Dinei Florencio, Cha Zhang, and Mike Seltzer. CROWDMOS: An approach
for crowdsourcing mean opinion score studies. In ICASSP. IEEE, 2011. Edition: ICASSP."
REFERENCES,0.37675350701402804,"Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. ICML, pp. 1278‚Äì1286, 2014."
REFERENCES,0.3787575150300601,"Antony W Rix, John G Beerends, Michael P Hollier, and Andries P Hekstra. Perceptual evaluation
of speech quality (pesq)-a new method for speech quality assessment of telephone networks and
codecs. In 2001 IEEE International Conference on Acoustics, Speech, and Signal Processing.
Proceedings (Cat. No. 01CH37221), volume 2, pp. 749‚Äì752. IEEE, 2001."
REFERENCES,0.3807615230460922,"Robin San-Roman, Eliya Nachmani, and Lior Wolf. Noise estimation for generative diffusion mod-
els. arXiv preprint arXiv:2104.02600, 2021."
REFERENCES,0.38276553106212424,"Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014."
REFERENCES,0.3847695390781563,"Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised
learning using nonequilibrium thermodynamics. In International Conference on Machine Learn-
ing, pp. 2256‚Äì2265, 2015."
REFERENCES,0.3867735470941884,"Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. Interna-
tional conference on learning representations, 2021."
REFERENCES,0.38877755511022044,Published as a conference paper at ICLR 2022
REFERENCES,0.3907815631262525,"Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.
Advances in neural information processing systems, 32, 2019."
REFERENCES,0.3927855711422846,"Yang Song and Stefano Ermon. Improved techniques for training score-based generative models.
Advances in neural information processing systems, 33:12438‚Äì12448, 2020."
REFERENCES,0.39478957915831664,"Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable approach
to density and score estimation. UAI, pp. 574‚Äì584, 2020a."
REFERENCES,0.3967935871743487,"Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben
Poole. Score-based generative modeling through stochastic differential equations. In Interna-
tional conference on learning representations, 2020b."
REFERENCES,0.39879759519038077,"Cees H Taal, Richard C Hendriks, Richard Heusdens, and Jesper Jensen. A short-time objective
intelligibility measure for time-frequency weighted noisy speech. In 2010 IEEE international
conference on acoustics, speech and signal processing, pp. 4214‚Äì4217. IEEE, 2010."
REFERENCES,0.40080160320641284,"Rafael Valle.
Waveglow: a Ô¨Çow-based generative network for speech synthesis.
https://
github.com/NVIDIA/waveglow, 2020."
REFERENCES,0.4028056112224449,"Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,
Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for
raw audio. Proc. 9th ISCA Speech Synthesis Workshop, pp. 125‚Äì125, 2016."
REFERENCES,0.40480961923847697,"Daniel Watson, Jonathan Ho, Mohammad Norouzi, and William Chan. Learning to efÔ¨Åciently sam-
ple from diffusion probabilistic models. arXiv preprint arXiv:2106.03802, 2021."
REFERENCES,0.40681362725450904,"Junichi Yamagishi, Christophe Veaux, and Kirsten MacDonald. CSTR VCTK Corpus: English
multi-speaker corpus for CSTR voice cloning toolkit (version 0.92), 2019."
REFERENCES,0.4088176352705411,"Ryuichi Yamamoto. Wavenet vocoder. https://github.com/r9y9/wavenet_vocoder,
2020."
REFERENCES,0.41082164328657317,"Ryuichi Yamamoto, Eunwoo Song, and Jae-Min Kim. Parallel. Wavegan: A fast waveform genera-
tion model based on generative adversarial networks with multi-resolution spectrogram. ICASSP,
2020."
REFERENCES,0.41282565130260523,"Geng Yang, Shan Yang, Kai Liu, Peng Fang, Wei Chen, and Lei Xie. Multi-band melgan: Faster
waveform generation for high-quality text-to-speech. In 2021 IEEE Spoken Language Technology
Workshop (SLT), pp. 492‚Äì498. IEEE, 2021."
REFERENCES,0.4148296593186373,Published as a conference paper at ICLR 2022
REFERENCES,0.4168336673346693,"A
THEORETICAL DERIVATIONS FOR BDDMS"
REFERENCES,0.4188376753507014,"In this section, we provide the theoretical supports for the following:"
REFERENCES,0.42084168336673344,‚Ä¢ The derivation for upper bounding ÀÜŒ≤n (see Appendix A.1).
REFERENCES,0.4228456913827655,"‚Ä¢ The score network Œ∏ trained with L(t)
ddpm(Œ∏) for the reverse process pŒ∏(xt‚àí1|xt) can be
re-used for the reverse process pŒ∏(ÀÜxn‚àí1|ÀÜxn) (see Appendix A.2)."
REFERENCES,0.4248496993987976,"‚Ä¢ The schedule network œÜ can be trained with L(n)
step(œÜ; Œ∏‚àó) after the score network Œ∏ is opti-
mized. (see Appendix A.3)."
REFERENCES,0.42685370741482964,"A.1
DERIVING AN UPPER BOUND FOR NOISE SCALE"
REFERENCES,0.4288577154308617,"Since monotonic noise schedules have been successfully applied to in many prior arts including
DPMs (Ho et al., 2020; Kingma et al., 2021) and score-based methods (Song et al., 2020a; Song
& Ermon, 2020), we also follow the monotonic assumption and derive an upper bound for ÀÜŒ≤n as
below:
Remark 1. Suppose the noise schedule for sampling is monotonic, i.e., 0 < ÀÜŒ≤1 < . . . < ÀÜŒ≤N < 1,
then, for 1 ‚â§n < N, ÀÜŒ≤n satisÔ¨Åes the following inequality:"
REFERENCES,0.4308617234468938,"0 < ÀÜŒ≤n < min

1 ‚àí
ÀÜŒ±2
n+1
1 ‚àíÀÜŒ≤n+1
, ÀÜŒ≤n+1"
REFERENCES,0.43286573146292584,"
.
(15)"
REFERENCES,0.4348697394789579,"Proof. By the general deÔ¨Ånition of noise schedule, we know that 0 < ÀÜŒ≤1, . . . , ÀÜŒ≤N < 1 (Note: no"
REFERENCES,0.43687374749499,"inequality sign in between). Given that ÀÜŒ±n = Qn
i=1 q"
REFERENCES,0.43887775551102204,"1 ‚àíÀÜŒ≤i, we also have 0 < ÀÜŒ±1, . . . , ÀÜŒ±t < 1."
REFERENCES,0.4408817635270541,"First, we show that ÀÜŒ≤n < 1 ‚àí
ÀÜŒ±2
n+1
1‚àíÀÜŒ≤n+1 :"
REFERENCES,0.44288577154308617,"ÀÜŒ±n‚àí1 =
ÀÜŒ±n
q"
REFERENCES,0.44488977955911824,"1 ‚àíÀÜŒ≤n
< 1 ‚áê‚áíÀÜŒ≤n < 1 ‚àíÀÜŒ±2
n = 1 ‚àí
ÀÜŒ±2
n+1
1 ‚àíÀÜŒ≤n+1
.
(16)"
REFERENCES,0.4468937875751503,"Next, we show that ÀÜŒ≤n < 1 ‚àíÀÜŒ±n+1: ÀÜŒ±n
q"
REFERENCES,0.44889779559118237,"1 ‚àíÀÜŒ≤n
=
ÀÜŒ±n
q"
REFERENCES,0.45090180360721444,1 ‚àíÀÜŒ≤n
REFERENCES,0.4529058116232465,"1 ‚àíÀÜŒ≤n
=
ÀÜŒ±n+1
1 ‚àíÀÜŒ≤n
< 1 ‚áê‚áíÀÜŒ≤n < 1 ‚àíÀÜŒ±n+1.
(17)"
REFERENCES,0.45490981963927857,"Now, we have ÀÜŒ≤n < min
n
1 ‚àí
ÀÜŒ±2
n+1
1‚àíÀÜŒ≤n+1 , 1 ‚àíÀÜŒ±n+1
o
. When 1 ‚àíÀÜŒ±n+1 < 1 ‚àí
ÀÜŒ±2
n+1
1‚àíÀÜŒ≤n+1 , we can show"
REFERENCES,0.45691382765531063,that ÀÜŒ≤n+1 < 1 ‚àíÀÜŒ±n+1:
REFERENCES,0.4589178356713427,"1 ‚àíÀÜŒ±n+1 < 1 ‚àí
ÀÜŒ±2
n+1
1 ‚àíÀÜŒ≤n+1
= 1 ‚àíÀÜŒ±2
n ‚áê‚áíÀÜŒ±n+1 > ÀÜŒ±2
n ‚áê‚áíÀÜŒ±2
n+1
ÀÜŒ±2n
> ÀÜŒ±n+1
(18)"
REFERENCES,0.46092184368737477,"‚áê‚áí1 ‚àíÀÜŒ±2
n+1
ÀÜŒ±2n
< 1 ‚àíÀÜŒ±n+1 ‚áê‚áíÀÜŒ≤n+1 < 1 ‚àíÀÜŒ±n+1.
(19)"
REFERENCES,0.46292585170340683,"By the assumption of monotonic sequence, we also have ÀÜŒ≤n < ÀÜŒ≤n+1. Knowing that ÀÜŒ≤n+1 <"
REFERENCES,0.4649298597194389,"1‚àíÀÜŒ±n+1 is always true, we obtain a tighter bound for ÀÜŒ≤n: 0 < ÀÜŒ≤n < min
n
1 ‚àí
ÀÜŒ±2
n+1
1‚àíÀÜŒ≤n+1 , ÀÜŒ≤n+1
o
."
REFERENCES,0.46693386773547096,"A.2
DERIVING THE TRAINING OBJECTIVE FOR SCORE NETWORK"
REFERENCES,0.46893787575150303,"First, followed from the data distribution modeling of BDDMs as proposed in Eq. (8):"
REFERENCES,0.4709418837675351,"pŒ∏(ÀÜx0) := EÀÜxn‚àí1‚àºq ÀÜ
Œ≤(ÀÜxn‚àí1;xt,œµn)

EÀÜx1:n‚àí2‚àºpŒ∏(ÀÜx1:n‚àí2|ÀÜxn‚àí1) [pŒ∏(ÀÜx0|ÀÜx1:n‚àí1)]

,
(20)"
REFERENCES,0.4729458917835671,we can derive a new lower bound to the log marginal likelihood as follows:
REFERENCES,0.4749498997995992,Published as a conference paper at ICLR 2022
REFERENCES,0.47695390781563124,"Proposition 1. Given xt ‚àºqŒ≤(xt|x0), the following lower bound holds for n ‚àà{2, . . . , N}:"
REFERENCES,0.4789579158316633,"log pŒ∏(ÀÜx0) ‚â•F(n)
score(Œ∏) := ‚àíL(n)
score(Œ∏) ‚àíRŒ∏(ÀÜx0, xt),
(21) where"
REFERENCES,0.48096192384769537,"L(n)
score(Œ∏) := DKL

pŒ∏(ÀÜxn‚àí1|ÀÜxn = xt)||q ÀÜŒ≤(ÀÜxn‚àí1; xt, œµn)

,
(22)"
REFERENCES,0.48296593186372744,"RŒ∏(ÀÜx0, xt) := ‚àíEpŒ∏(ÀÜx1|ÀÜxn=xt) [log pŒ∏(ÀÜx0|ÀÜx1)] .
(23)"
REFERENCES,0.4849699398797595,Proof.
REFERENCES,0.48697394789579157,"log pŒ∏(ÀÜx0) = log
Z
pŒ∏(ÀÜx0:n‚àí2|ÀÜxn‚àí1)q ÀÜŒ≤(ÀÜxn‚àí1; xt, œµn)dÀÜx1:n‚àí1
(24)"
REFERENCES,0.48897795591182364,"= log
Z
pŒ∏(ÀÜx0:n‚àí2|ÀÜxn‚àí1)q ÀÜŒ≤(ÀÜxn‚àí1; xt, œµn)pŒ∏(ÀÜx1:n‚àí1|ÀÜxn = xt)"
REFERENCES,0.4909819639278557,pŒ∏(ÀÜx1:n‚àí1|ÀÜxn = xt)dÀÜx1:n‚àí1 (25)
REFERENCES,0.49298597194388777,"= log EpŒ∏(ÀÜx1,n‚àí1|ÀÜxn=xt)"
REFERENCES,0.49498997995991983,"""
pŒ∏(ÀÜx0|ÀÜx1)q ÀÜŒ≤(ÀÜxn‚àí1; xt, œµn)"
REFERENCES,0.4969939879759519,pŒ∏(ÀÜxn‚àí1|ÀÜxn = xt) # (26)
REFERENCES,0.49899799599198397,"[Jensen‚Äôs Inequality] ‚â•EpŒ∏(ÀÜx1,ÀÜxn‚àí1|ÀÜxn=xt) """
REFERENCES,0.501002004008016,"log
pŒ∏(ÀÜx0|ÀÜx1)q ÀÜŒ≤(ÀÜxn‚àí1; xt, œµn)"
REFERENCES,0.503006012024048,pŒ∏(ÀÜxn‚àí1|ÀÜxn = xt) # (27)
REFERENCES,0.5050100200400801,"=EpŒ∏(ÀÜx1|ÀÜxn=xt) [log pŒ∏(ÀÜx0|ÀÜx1)] ‚àíDKL

pŒ∏(ÀÜxn‚àí1|ÀÜxn = xt)||q ÀÜŒ≤(ÀÜxn‚àí1; xt, œµn)
 (28)"
REFERENCES,0.5070140280561122,"= ‚àíL(n)
score(Œ∏) ‚àíRŒ∏(ÀÜx0, xt)
(29)"
REFERENCES,0.5090180360721442,"Next, we show that the score network Œ∏ trained with L(t)
ddpm(Œ∏) can be re-used in BDDMs. We Ô¨Årst
provide the derivation for Eq. (9- 10). We have"
REFERENCES,0.5110220440881763,"q ÀÜŒ≤(ÀÜxn‚àí1; xt, œµn) := q ÀÜŒ≤ "
REFERENCES,0.5130260521042084,"ÀÜxn‚àí1|ÀÜxn = xt, ÀÜx0 = xt ‚àí
p"
REFERENCES,0.5150300601202404,"1 ‚àíÀÜŒ±2nœµn
ÀÜŒ±n ! (30) = N Ô£´"
REFERENCES,0.5170340681362725,Ô£≠ÀÜŒ±n‚àí1 ÀÜŒ≤n
REFERENCES,0.5190380761523046,1 ‚àíÀÜŒ±2n
REFERENCES,0.5210420841683366,"xt ‚àí
p"
REFERENCES,0.5230460921843687,"1 ‚àíÀÜŒ±2nœµn
ÀÜŒ±n
+ q"
REFERENCES,0.5250501002004008,"1 ‚àíÀÜŒ≤n(1 ‚àíÀÜŒ±2
n‚àí1)"
REFERENCES,0.5270541082164328,"1 ‚àíÀÜŒ±2n
xt, 1 ‚àíÀÜŒ±2
n‚àí1
1 ‚àíÀÜŒ±2n
ÀÜŒ≤nI Ô£∂"
REFERENCES,0.5290581162324649,"Ô£∏
(31) = N Ô£´ Ô£≠ Ô£´"
REFERENCES,0.531062124248497,"Ô£≠
ÀÜŒ±n‚àí1 ÀÜŒ≤n
ÀÜŒ±n(1 ‚àíÀÜŒ±2n) + q"
REFERENCES,0.533066132264529,"1 ‚àíÀÜŒ≤n(1 ‚àíÀÜŒ±2
n‚àí1)"
REFERENCES,0.5350701402805611,1 ‚àíÀÜŒ±2n Ô£∂
REFERENCES,0.5370741482965932,"Ô£∏xt ‚àí
ÀÜŒ±n‚àí1 ÀÜŒ≤n
ÀÜŒ±n
p"
REFERENCES,0.5390781563126252,"1 ‚àíÀÜŒ±2n
œµn, 1 ‚àíÀÜŒ±2
n‚àí1
1 ‚àíÀÜŒ±2n
ÀÜŒ≤nI Ô£∂"
REFERENCES,0.5410821643286573,"Ô£∏
(32) = N Ô£´ Ô£≠
1
q"
REFERENCES,0.5430861723446894,"1 ‚àíÀÜŒ≤n
xt ‚àí
ÀÜŒ≤n
q"
REFERENCES,0.5450901803607214,"(1 ‚àíÀÜŒ≤n)(1 ‚àíÀÜŒ±2n)
œµn, 1 ‚àíÀÜŒ±2
n‚àí1
1 ‚àíÀÜŒ±2n
ÀÜŒ≤nI Ô£∂"
REFERENCES,0.5470941883767535,"Ô£∏.
(33)"
REFERENCES,0.5490981963927856,"Proposition 2. Suppose xt ‚àºqŒ≤(xt|x0), then any solution satisfying Œ∏‚àó= argminŒ∏L(t)
ddpm(Œ∏), ‚àÄt ‚àà"
REFERENCES,0.5511022044088176,"{1, ..., T}, also satisÔ¨Åes Œ∏‚àó= argminŒ∏L(n)
score(Œ∏), ‚àÄn ‚àà{2, ..., N}."
REFERENCES,0.5531062124248497,"Proof. By the deÔ¨Ånition in Eq. (4), we have"
REFERENCES,0.5551102204408818,"pŒ∏(ÀÜxn‚àí1|ÀÜxn = xt) =N Ô£´ Ô£≠
1
q"
REFERENCES,0.5571142284569138,1 ‚àíÀÜŒ≤n 
REFERENCES,0.5591182364729459,"xt ‚àí
ÀÜŒ≤n
p"
REFERENCES,0.561122244488978,"1 ‚àíÀÜŒ±2n
œµŒ∏ (xt, ÀÜŒ±n) !"
REFERENCES,0.56312625250501,", 1 ‚àíÀÜŒ±2
n‚àí1
1 ‚àíÀÜŒ±2n
ÀÜŒ≤nI Ô£∂"
REFERENCES,0.5651302605210421,"Ô£∏.
(34)"
REFERENCES,0.5671342685370742,"Here, from the training objective in Eq. (5), since xt = Œ±tx0+
p"
REFERENCES,0.5691382765531062,"1 ‚àíŒ±2
tœµn, the noise scale argument
for the score network is known to be Œ±t. Therefore, we can use œµŒ∏ (xt, Œ±t) instead of œµŒ∏ (xt, ÀÜŒ±n) for"
REFERENCES,0.5711422845691383,Published as a conference paper at ICLR 2022
REFERENCES,0.5731462925851704,"expanding L(n)
score(Œ∏). Since pŒ∏(ÀÜxn‚àí1|ÀÜxn = xt) and q ÀÜŒ≤(ÀÜxn‚àí1; xt, œµn) are two isotropic Gaussians
with the same variance, the KL divergence is a scaled ‚Ñì2-norm of their means‚Äô difference:"
REFERENCES,0.5751503006012024,"L(n)
score(Œ∏) :=DKL

pŒ∏(ÀÜxn‚àí1|ÀÜxn = xt)||q ÀÜŒ≤(ÀÜxn‚àí1; xt, œµn)

(35)"
REFERENCES,0.5771543086172345,"=
1 ‚àíÀÜŒ±2
n
2(1 ‚àíÀÜŒ±2
n‚àí1)ÀÜŒ≤n "
Q,0.5791583166332666,"1
q"
Q,0.5811623246492986,1 ‚àíÀÜŒ≤n 
Q,0.5831663326653307,"xt ‚àí
ÀÜŒ≤n
p"
Q,0.5851703406813628,"1 ‚àíÀÜŒ±2n
œµŒ∏ (xt, Œ±t) ! (36) ‚àí Ô£´ Ô£≠
1
q"
Q,0.5871743486973948,"1 ‚àíÀÜŒ≤n
xt ‚àí
ÀÜŒ≤n
q"
Q,0.5891783567134269,"(1 ‚àíÀÜŒ≤n)(1 ‚àíÀÜŒ±2n)
œµn Ô£∂ Ô£∏  2 2 (37)"
Q,0.591182364729459,"= (1 ‚àíÀÜŒ≤n)(1 ‚àíÀÜŒ±2
n)"
Q,0.593186372745491,"2(1 ‚àíÀÜŒ≤n ‚àíÀÜŒ±2n)ÀÜŒ≤n  ÀÜŒ≤n
q"
Q,0.5951903807615231,"(1 ‚àíÀÜŒ≤n)(1 ‚àíÀÜŒ±2n)
(œµn ‚àíœµŒ∏ (xt, Œ±t))  2 2 (38)"
Q,0.5971943887775552,"= (1 ‚àíÀÜŒ≤n)(1 ‚àíÀÜŒ±2
n)"
Q,0.5991983967935872,2(1 ‚àíÀÜŒ≤n ‚àíÀÜŒ±2n)ÀÜŒ≤n
Q,0.6012024048096193,"ÀÜŒ≤2
n
(1 ‚àíÀÜŒ±2n)(1 ‚àíÀÜŒ≤n)
‚à•œµn ‚àíœµŒ∏ (xt, Œ±t)‚à•2
2
(39)"
Q,0.6032064128256514,"=
ÀÜŒ≤n
2(1 ‚àíÀÜŒ≤n ‚àíÀÜŒ±2n)"
Q,0.6052104208416834,œµn ‚àíœµŒ∏
Q,0.6072144288577155,"
Œ±tx0 +
q"
Q,0.6092184368737475,"1 ‚àíŒ±2
tœµn, Œ±t  2"
Q,0.6112224448897795,"2
,
(40)"
Q,0.6132264529058116,"which is proportional to L(t)
ddpm :=
œµn ‚àíœµŒ∏

Œ±tx0 +
p"
Q,0.6152304609218436,"1 ‚àíŒ±2
tœµn, Œ±t

2"
Q,0.6172344689378757,"2 as deÔ¨Åned in Eq. (5).
Thus,"
Q,0.6192384769539078,"argminŒ∏L(t)
ddpm(Œ∏) ‚â°argminŒ∏L(n)
score(Œ∏).
(41)"
Q,0.6212424849699398,"Next, we can simplify RŒ∏(ÀÜx0, xt) to a reconstruction loss for ÀÜx0:
RŒ∏(ÀÜx0, xt) := ‚àíEpŒ∏(ÀÜx1|ÀÜxn=xt) [log pŒ∏(ÀÜx0|ÀÜx1)]
(42)"
Q,0.6232464929859719,=EpŒ∏(ÀÜx1|ÀÜxn=xt) Ô£Æ
Q,0.625250501002004,"Ô£∞log N Ô£´ Ô£≠
1
q"
Q,0.627254509018036,1 ‚àíÀÜŒ≤1 
Q,0.6292585170340681,"ÀÜx1 ‚àí
ÀÜŒ≤1
p"
Q,0.6312625250501002,"1 ‚àíÀÜŒ±2
1
œµŒ∏(ÀÜx1, ÀÜŒ±1), ÀÜŒ≤1I !Ô£∂ Ô£∏ Ô£π"
Q,0.6332665330661322,"Ô£ª
(43)"
Q,0.6352705410821643,=EpŒ∏(ÀÜx1|ÀÜxn=xt) Ô£Æ Ô£ØÔ£∞D
Q,0.6372745490981964,"2 log 2œÄ ÀÜŒ≤1 +
1 2ÀÜŒ≤1"
Q,0.6392785571142284,"ÀÜx0 ‚àí
1
q"
Q,0.6412825651302605,1 ‚àíÀÜŒ≤1 Ô£´
Q,0.6432865731462926,"Ô£≠ÀÜx1 ‚àí
ÀÜŒ≤1
q"
Q,0.6452905811623246,"ÀÜŒ≤1
œµŒ∏(ÀÜx1, ÀÜŒ±1) Ô£∂ Ô£∏  2 2 Ô£π Ô£∫Ô£ª (44) =D"
Q,0.6472945891783567,"2 log 2œÄ ÀÜŒ≤1 +
1"
Q,0.6492985971943888,"2ÀÜŒ≤1
EpŒ∏(ÀÜx1|ÀÜxn=xt) Ô£Æ Ô£ØÔ£∞"
Q,0.6513026052104208,"ÀÜx0 ‚àí
1
q"
Q,0.6533066132264529,1 ‚àíÀÜŒ≤1
Q,0.655310621242485,"
ÀÜx1 ‚àí
q"
Q,0.657314629258517,"ÀÜŒ≤1œµŒ∏(ÀÜx1, ÀÜŒ±1)
 2 2 Ô£π Ô£∫Ô£ª,"
Q,0.6593186372745491,"(45)
where pŒ∏(ÀÜx1|ÀÜxn = xt) can be efÔ¨Åciently sampled using the reverse process in (Song et al., 2021).
Yet, in practice, similar to the training in (Song et al., 2021; Chen et al., 2020; Kong et al., 2021),
we dropped RŒ∏(ÀÜx0, xt) when training Œ∏. In theory, we know that RŒ∏(ÀÜx0, xt) achieves its optimal
value at Œ∏‚àó= argminŒ∏‚à•œµŒ∏(ÀÜx1, ÀÜŒ±1)‚àíœµ1‚à•2
2, which shares a similar objective as L(t)
ddpm. By minimizing"
Q,0.6613226452905812,"L(t)
ddpm, we train a score network Œ∏‚àóthat best minimizes ‚àÜœµt := ‚à•œµt ‚àíœµŒ∏‚àó(Œ±tx0 +
p"
Q,0.6633266533066132,"1 ‚àíŒ±2
tœµt, Œ±t)‚à•2
2
for all 1 ‚â§t ‚â§T. Since the Ô¨Årst diffusion step has the smallest effect on corrupting ÀÜx0 (i.e.,
Œ≤1 ‚âà0), it sufÔ¨Åces to consider a ÀÜŒ±1 = ‚àö1 ‚àíŒ≤1 = Œ±1, in which case we can jointly minimize
RŒ∏(ÀÜx0, xt) by minimizing L(1)
ddpm."
Q,0.6653306613226453,"In this sense, during training, given xt ‚àºqŒ≤(xt|x0), we can train the score network with the same
training objective as in DDPMs and DDIMs. Practically, it is beneÔ¨Åcial for BDDMs as we can re-use
the score network Œ∏ of any well-trained DDPM or DDIM."
Q,0.6673346693386774,Published as a conference paper at ICLR 2022
Q,0.6693386773547094,"A.3
DERIVING THE TRAINING OBJECTIVE FOR SCHEDULE NETWORK"
Q,0.6713426853707415,"Given that Œ∏ can be trained to maximize the log evidence with the pre-speciÔ¨Åed noise schedule Œ≤ for
training, the consequent question of interest in BDDMs is how to Ô¨Ånd a fast and good enough noise
schedule ÀÜŒ≤ ‚ààRN for sampling given an optimized Œ∏‚àó. In BDDMs, this problem is reduced to how
to effectively learn the network parameters œÜ ."
Q,0.6733466933867736,"Proposition 3. Suppose Œ∏ has been optimized and hypothetically converged to the optimal Œ∏‚àó,
where by optimal it means that with Œ∏‚àówe have pŒ∏‚àó(ÀÜxn‚àí1|ÀÜxn = xt) = q ÀÜŒ≤(ÀÜxn‚àí1; xt, œµn) given"
Q,0.6753507014028056,"xt ‚àºqŒ≤(xt|x0). When ÀÜŒ≤ is unknown but we have x0 = ÀÜx0 and ÀÜŒ±n = Œ±t, we can minimize the
gap between the optimal lower bound F(n)
score(Œ∏‚àó) and log pŒ∏‚àó(ÀÜx0), i.e, log pŒ∏‚àó(ÀÜx0) ‚àíF(n)
score(Œ∏‚àó), by
minimizing the following objective with respect to ÀÜŒ≤n:"
Q,0.6773547094188377,"L(n)
step(ÀÜŒ≤n; Œ∏‚àó) :=DKL

pŒ∏‚àó(ÀÜxn‚àí1|ÀÜxn = xt)||q ÀÜŒ≤n(ÀÜxn‚àí1|x0; Œ±t)

(46)"
Q,0.6793587174348698,"=
Œ¥t
2(Œ¥t ‚àíÀÜŒ≤n)"
Q,0.6813627254509018,"œµn ‚àí
ÀÜŒ≤n
Œ¥t
œµŒ∏‚àó

Œ±tx0 +
p"
Q,0.6833667334669339,"Œ¥tœµn, Œ±t
 2"
Q,0.685370741482966,"2
+ C,
(47) where"
Q,0.687374749498998,"Œ¥t = 1 ‚àíŒ±2
t,
C = 1"
Q,0.6893787575150301,4 log Œ¥t
Q,0.6913827655310621,"ÀÜŒ≤n
+ D 2 ÀÜŒ≤n Œ¥t
‚àí1 !"
Q,0.6933867735470942,".
(48)"
Q,0.6953907815631263,"Proof. Note that ÀÜx0 = x0, ÀÜŒ±n = Œ±t, xt = Œ±tx0 +
p"
Q,0.6973947895791583,"1 ‚àíŒ±2
tœµn and pŒ∏‚àó(ÀÜxn‚àí1|ÀÜxn = xt) =
q ÀÜŒ≤(ÀÜxn‚àí1; xt, œµn). When x0 is given to pŒ∏‚àó, we can express the probability as follows:"
Q,0.6993987975951904,"pŒ∏‚àó(ÀÜxn‚àí1|ÀÜxn = xt(x0), ÀÜx0 = x0)
(49)"
Q,0.7014028056112225,"=
Z
N(z; 0, I)pŒ∏‚àó(ÀÜxn‚àí1|ÀÜxn = Œ±tx0 +
q"
Q,0.7034068136272545,"1 ‚àíŒ±2
tz)dz
(50)"
Q,0.7054108216432866,"=
Z
N(z; 0, I)q ÀÜŒ≤(ÀÜxn‚àí1; xt = Œ±tx0 +
q"
Q,0.7074148296593187,"1 ‚àíŒ±2
tz, œµn = z)dz
(51)"
Q,0.7094188376753507,"=
Z
N(z; 0, I)N Ô£´"
Q,0.7114228456913828,"Ô£≠ÀÜxn‚àí1; Œ±tx0 +
p"
Q,0.7134268537074149,"1 ‚àíŒ±2
tz
q"
Q,0.7154308617234469,"1 ‚àíÀÜŒ≤n
‚àí
ÀÜŒ≤n
q"
Q,0.717434869739479,"(1 ‚àíÀÜŒ≤n)(1 ‚àíÀÜŒ±2n)
z, 1 ‚àíÀÜŒ±2
n‚àí1
1 ‚àíÀÜŒ±2n
ÀÜŒ≤nI Ô£∂ Ô£∏dz (52)"
Q,0.7194388777555111,"[See Eq. (2) in (Frey, 1999)] =N Ô£´"
Q,0.7214428857715431,"Ô£¨
Ô£≠ÀÜxn‚àí1;
Œ±tx0
q"
Q,0.7234468937875751,"1 ‚àíÀÜŒ≤n
, Ô£´ Ô£¨
Ô£≠ Ô£´ Ô£≠
p"
Q,0.7254509018036072,"1 ‚àíŒ±2
t
q"
Q,0.7274549098196392,"1 ‚àíÀÜŒ≤n
‚àí
ÀÜŒ≤n
q"
Q,0.7294589178356713,"(1 ‚àíÀÜŒ≤n)(1 ‚àíŒ±2
t) Ô£∂ Ô£∏ 2"
Q,0.7314629258517034,"+ 1 ‚àíŒ±2
t/(1 ‚àíÀÜŒ≤n)
1 ‚àíŒ±2
t
ÀÜŒ≤n Ô£∂ Ô£∑
Ô£∏I Ô£∂ Ô£∑
Ô£∏ (53) =N Ô£´"
Q,0.7334669338677354,"Ô£≠ÀÜxn‚àí1;
Œ±tx0
q"
Q,0.7354709418837675,"1 ‚àíÀÜŒ≤n
, 1 ‚àíŒ±2
t ‚àíÀÜŒ≤n
1 ‚àíÀÜŒ≤n
I Ô£∂"
Q,0.7374749498997996,"Ô£∏=: q ÀÜŒ≤n(ÀÜxn‚àí1; x0, Œ±t),
(54)"
Q,0.7394789579158316,"where, different from pŒ∏‚àó(ÀÜxn‚àí1|ÀÜxn = xt), from Eq. (49) to Eq. (50), instead of conditioning on a
speciÔ¨Åc xt, when x0 is given xt can be generated using any z ‚àºN(0, I)."
Q,0.7414829659318637,Published as a conference paper at ICLR 2022
Q,0.7434869739478958,"From this, we can express the gap between log pŒ∏‚àó(ÀÜx0) and F(n)
score(Œ∏‚àó) in the following form:"
Q,0.7454909819639278,"log pŒ∏‚àó(ÀÜx0 = x0) ‚àíF(n)
score(Œ∏‚àó)
(55)"
Q,0.7474949899799599,"= log pŒ∏‚àó(ÀÜx0 = x0) ‚àíEpŒ∏‚àó(ÀÜx1,n‚àí1|ÀÜxn=xt) """
Q,0.749498997995992,"log
pŒ∏(ÀÜx0|ÀÜx1)q ÀÜŒ≤(ÀÜxn‚àí1; xt, œµn)"
Q,0.751503006012024,pŒ∏(ÀÜxn‚àí1|ÀÜxn = xt) # (56)
Q,0.7535070140280561,= log pŒ∏‚àó(ÀÜx0 = x0) ‚àíEpŒ∏‚àó(ÀÜx1:n‚àí1|ÀÜxn=xt)
Q,0.7555110220440882,"
log pŒ∏‚àó(ÀÜx0:n‚àí1|ÀÜxn = xt)"
Q,0.7575150300601202,pŒ∏‚àó(ÀÜx1:n‚àí1|ÀÜxn = xt)
Q,0.7595190380761523,"
(57)"
Q,0.7615230460921844,=EpŒ∏‚àó(ÀÜx1:n‚àí1|ÀÜxn=xt)
Q,0.7635270541082164,"
log
pŒ∏‚àó(ÀÜx1:n‚àí1|ÀÜxn = xt)
pŒ∏‚àó(ÀÜx1:n‚àí1|ÀÜxn = xt, ÀÜx0 = x0)"
Q,0.7655310621242485,"
(58)"
Q,0.7675350701402806,"=EpŒ∏‚àó(ÀÜxn‚àí1|ÀÜxn=xt) """
Q,0.7695390781563126,log pŒ∏‚àó(ÀÜxn‚àí1|ÀÜxn = xt)
Q,0.7715430861723447,"q ÀÜŒ≤n(ÀÜxn‚àí1; x0, Œ±t) # (59)"
Q,0.7735470941883767,"=DKL

pŒ∏‚àó(ÀÜxn‚àí1|ÀÜxn = xt)||q ÀÜŒ≤n(ÀÜxn‚àí1; x0, Œ±t)

(60)"
Q,0.7755511022044088,"Next, we evaluate the above KL divergence term. By deÔ¨Ånition, we have"
Q,0.7775551102204409,"pŒ∏‚àó(ÀÜxn‚àí1|ÀÜxn = xt) = N Ô£´ Ô£≠
1
q"
Q,0.779559118236473,1 ‚àíÀÜŒ≤n 
Q,0.781563126252505,"xt ‚àí
ÀÜŒ≤n
p"
Q,0.7835671342685371,"1 ‚àíÀÜŒ±2n
œµŒ∏‚àó(xt, ÀÜŒ±n) !"
Q,0.7855711422845691,", 1 ‚àíÀÜŒ±2
n‚àí1
1 ‚àíÀÜŒ±2n
ÀÜŒ≤nI Ô£∂"
Q,0.7875751503006012,"Ô£∏
(61)"
Q,0.7895791583166333,"Together with Eq. (54), we have"
Q,0.7915831663326653,"L(n)
step(ÀÜŒ≤n; Œ∏‚àó) := DKL

pŒ∏‚àó(ÀÜxn‚àí1|ÀÜxn = xt)||q ÀÜŒ≤n(ÀÜxn‚àí1; x0, Œ±t)

(62)"
Q,0.7935871743486974,"=
1 ‚àíÀÜŒ≤n
2(1 ‚àíÀÜŒ≤n ‚àíŒ±2
t)  Œ±t
q"
Q,0.7955911823647295,"1 ‚àíÀÜŒ≤n
x0 ‚àí
1
q"
Q,0.7975951903807615,1 ‚àíÀÜŒ≤n 
Q,0.7995991983967936,"xt ‚àí
ÀÜŒ≤n
p"
Q,0.8016032064128257,"1 ‚àíŒ±2
t
œµŒ∏‚àó(xt, Œ±t) ! 2 2"
Q,0.8036072144288577,"+ C
(63)"
Q,0.8056112224448898,"=
1 ‚àíÀÜŒ≤n
2(1 ‚àíÀÜŒ≤n ‚àíŒ±2
t)  Œ±t
q"
Q,0.8076152304609219,"1 ‚àíÀÜŒ≤n
x0 ‚àí
1
q"
Q,0.8096192384769539,1 ‚àíÀÜŒ≤n 
Q,0.811623246492986,"Œ±tx0 +
q"
Q,0.8136272545090181,"1 ‚àíŒ±2
tœµn ‚àí
ÀÜŒ≤n
p"
Q,0.8156312625250501,"1 ‚àíŒ±2
t
œµŒ∏‚àó(xt, Œ±t) ! 2 2 + C (64)"
Q,0.8176352705410822,"=
1 ‚àíÀÜŒ≤n
2(1 ‚àíÀÜŒ≤n ‚àíŒ±2
t)  s"
Q,0.8196392785571143,"1 ‚àíŒ±2
t
1 ‚àíŒ≤n
œµn ‚àí
ÀÜŒ≤n
p"
Q,0.8216432865731463,"(1 ‚àíŒ≤n)(1 ‚àíŒ±2
t)
œµŒ∏‚àó(xt, Œ±t)  2 2"
Q,0.8236472945891784,"+ C
(65)"
Q,0.8256513026052105,"=
1 ‚àíŒ±2
t
2(1 ‚àíÀÜŒ≤n ‚àíŒ±2
t)"
Q,0.8276553106212425,"œµn ‚àí
ÀÜŒ≤n
1 ‚àíŒ±2
t
œµŒ∏‚àó(xt, Œ±t)  2"
Q,0.8296593186372746,"2
+ C
(66)"
Q,0.8316633266533067,"=
Œ¥t
2(Œ¥t ‚àíÀÜŒ≤n)"
Q,0.8336673346693386,"œµn ‚àí
ÀÜŒ≤n
Œ¥t
œµŒ∏‚àó

Œ±tx0 +
p"
Q,0.8356713426853707,"Œ¥tœµn, Œ±t
 2"
Q,0.8376753507014028,"2
+ C,
(67) where"
Q,0.8396793587174348,"Œ¥t = 1 ‚àíŒ±2
t,
C = 1"
Q,0.8416833667334669,4 log Œ¥t
Q,0.843687374749499,"ÀÜŒ≤n
+ D 2 ÀÜŒ≤n Œ¥t
‚àí1 !"
Q,0.845691382765531,".
(68)"
Q,0.8476953907815631,"As we use a schedule network œÜ to estimate ÀÜŒ≤n from (ÀÜŒ±n+1, ÀÜŒ≤n+1) as deÔ¨Åned in Eq. (13), we obtain
the Ô¨Ånal step loss for learning œÜ:"
Q,0.8496993987975952,"L(n)
step(œÜ; Œ∏‚àó) =
Œ¥t
2(Œ¥t ‚àíÀÜŒ≤n(œÜ))"
Q,0.8517034068136272,"œµn ‚àí
ÀÜŒ≤n(œÜ)"
Q,0.8537074148296593,"Œ¥t
œµŒ∏‚àó(xt, Œ±t)  2 2
+ 1"
LOG,0.8557114228456913,"4 log
Œ¥t
ÀÜŒ≤n(œÜ)
+ D 2"
LOG,0.8577154308617234,"ÀÜŒ≤n(œÜ) Œ¥t
‚àí1 ! . (69)"
LOG,0.8597194388777555,Published as a conference paper at ICLR 2022
LOG,0.8617234468937875,"This proposed objective for training the schedule network can be interpreted as to better model
the data distribution (i.e., maximizing log pŒ∏(ÀÜx0)) by correcting the gradient scale ÀÜŒ≤n for the next
reverse step (from ÀÜxn to ÀÜxn‚àí1) given the gradient vector œµŒ∏‚àóestimated by the score network Œ∏‚àó."
LOG,0.8637274549098196,"B
EXPERIMENTAL DETAILS"
LOG,0.8657314629258517,"B.1
CONVENTIONAL GRID SEARCH ALGORITHM FOR DDPMS"
LOG,0.8677354709418837,"We reproduced the grid search algorithm in (Chen et al., 2020), in which a 6-step noise schedule was
searched. In our paper, we generalized the grid search algorithm by similarly sweeping the N-step
noise schedule over the following possibilities with a bin width M = 9:"
LOG,0.8697394789579158,"{1, 2, 3, 4, 5, 6, 7, 8, 9} ‚äó{10‚àí6¬∑N/N, 10‚àí6¬∑(N‚àí1)/N, ..., 10‚àí6¬∑1/N},
(70)"
LOG,0.8717434869739479,"where ‚äódenotes the cartesian product applied on two sets. LS-MSE was used as a metric to select
the solution during the search. When N = 6, we resemble the GS algorithm in (Chen et al., 2020).
Note that above searching method normally does not scale up to N > 6 steps for its exponential
computational cost O(9N)."
LOG,0.87374749498998,"B.2
HYPERPARAMETER SETTING IN BDDMS"
LOG,0.875751503006012,"Algorithm 2 took a skip factor œÑ to control the stride for training the schedule network. The value
of œÑ would affect the coverage of step sizes when training the schedule network, hence affecting
the predicted number of steps N for inference ‚Äì the higher œÑ is, the shorter the predicted inference
schedule tends to be. We set œÑ = 66 for training the BDDM vocoders in this paper."
LOG,0.8777555110220441,"For initializing Algorithm 3 for noise scheduling, we could take as few as 1 training sample for
validation, perform a grid search on the hyperparameters {(ÀÜŒ±N = 0.1Œ±T i, ÀÜŒ≤N = 0.1j)} for i, j =
1, ..., 9, i.e., 81 possibilities in total, and use the PESQ measure as the selection metric. Then, the
predicted noise schedule corresponding to the maximum PESQ was stored and applied to the online
inference afterward, as shown in Algorithm 4. Note that this searching has a complexity of only
O(M 2) (e.g., M = 9 in this case), which is much more efÔ¨Åcient than O(M N) in the conventional
grid search algorithm in (Chen et al., 2020), as discussed in Section B.1."
LOG,0.8797595190380761,"B.3
IMPLEMENTATION DETAILS"
LOG,0.8817635270541082,"Our proposed BDDMs and the baseline methods were all implemented with the Pytorch library. The
score networks for the LJ and VCTK speech datasets were trained from scratch on a single NVIDIA
Tesla P40 GPU with batch size 32 for about 1M steps, which took about 3 days."
LOG,0.8837675350701403,"For the model architecture, we used the same architecture as in DiffWave (Kong et al., 2021) for
the score network with 128 residual channels; we adopted a lightweight GALR network (Lam et al.,
2021) for the schedule network. GALR was originally proposed for speech enhancement, so we con-
sidered it well suited for predicting the noise scales. For the conÔ¨Åguration of the GALR network, we
used a window length of 8 samples for encoding, a segment size of 64 for segmentation and only two
GALR blocks of 128 hidden dimensions, and other settings were inherited from (Lam et al., 2021).
To make the schedule network output with a proper range and dimension, we applied a sigmoid func-
tion to the last block‚Äôs output of the GALR network. Then the result was averaged over the segments
and the feature dimensions to obtain the predicted ratio: œÉœÜ(x) = AvgPool2D(œÉ(GALR(x))),
where GALR(¬∑) denotes the GALR network, AvgPool2D(¬∑) denotes the average pooling operation
applied to the segments and the feature dimensions, and œÉ(x) := 1/(1 + e‚àíx). The same net-
work architecture was used for the NE approach for estimating Œ±2
t and was shown better than the
ConvTASNet used in the original paper (San-Roman et al., 2021). It is also notable that the com-
putational cost of a schedule network is indeed fractional compared to the cost of a score network,
as predicting a noise scalar variable is intrinsically a relatively much easier task. Our GALR-based
schedule network, while being able to produce stable and reliable results, was about 3.6 times faster
than the score network. The training of schedule networks for BDDMs took only 10k steps to
converge, which consumed no more than an hour on a single GPU."
LOG,0.8857715430861723,Published as a conference paper at ICLR 2022
LOG,0.8877755511022044,Table 3: Ratings that have been used in evaluation of speech naturalness of synthetic samples.
LOG,0.8897795591182365,"Rating
Naturalness
DeÔ¨Ånition"
UNSATISFACTORY,0.8917835671342685,"1
Unsatisfactory
Very annoying, distortion is objectionable.
2
Poor
Annoying distortion, but not objectionable.
3
Fair
Perceptible distortion, slightly annoying.
4
Good
Slight perceptible level of distortion, but not annoying.
5
Excellent
Imperceptible level of distortion."
UNSATISFACTORY,0.8937875751503006,"Table 4: Performances of different noise schedules on the multi-speaker VCTK speech dataset, each
of which used the same score network (Chen et al., 2020) œµŒ∏(¬∑) that was trained on VCTK for about
1M iterations."
UNSATISFACTORY,0.8957915831663327,"Noise schedule
LS-MSE (‚Üì)
MCD (‚Üì)
STOI (‚Üë)
PESQ (‚Üë)
MOS (‚Üë)"
UNSATISFACTORY,0.8977955911823647,"DDPM (Ho et al., 2020; Chen et al., 2020)
8 steps (Grid Search)
101
2.09
0.787
3.31
4.22 ¬± 0.04
1,000 steps (Linear)
85.0
2.02
0.798
3.39
4.40 ¬± 0.05"
UNSATISFACTORY,0.8997995991983968,"DDIM (Song et al., 2021)
8 steps (Linear)
553
3.20
0.701
2.81
3.83 ¬± 0.04
16 steps (Linear)
412
2.90
0.724
3.04
3.88 ¬± 0.05
21 steps (Linear)
355
2.79
0.739
3.12
4.12 ¬± 0.05
100 steps (Linear)
259
2.58
0.759
3.30
4.27 ¬± 0.04"
UNSATISFACTORY,0.9018036072144289,"NE (San-Roman et al., 2021)
8 steps (Linear)
208
2.54
0.740
3.10
4.18 ¬± 0.04
16 steps (Linear)
183
2.53
0.742
3.20
4.26 ¬± 0.04
21 steps (Linear)
852
3.57
0.699
2.66
3.70 ¬± 0.03"
UNSATISFACTORY,0.9038076152304609,"BDDM (ÀÜŒ±N, ÀÜŒ≤N)
8 steps (0.2, 0.9)
98.4
2.11
0.774
3.18
4.20 ¬± 0.04
16 steps (0.5, 0.5)
73.6
1.93
0.813
3.39
4.35 ¬± 0.05
21 steps (0.5, 0.1)
76.5
1.83
0.827
3.43
4.48 ¬± 0.06"
UNSATISFACTORY,0.905811623246493,"Regarding the image generation task, to demonstrate the generalizability of our method, we directly
adopted a score network pre-trained on the CIFAR-10 dataset implemented by a third-party open-
source repository. Regarding the schedule network, to demonstrate that it does not have to use
specialized architecture, we replaced GALR by the VGG11 (Simonyan & Zisserman, 2014), which
was also used by as a noise estimator in (San-Roman et al., 2021). The output dimension (number
of classes) of VGG11 was set to 1. Similar to the setting for GALR in speech synthesis, we added
a sigmoid activation to the last layer to ensure a [0, 1] output. Similar to the training in speech
domain, we trained the VGG11-based schedule networks while freezing the score networks for 10k
steps, which normally can be Ô¨Ånished in about two hours."
UNSATISFACTORY,0.9078156312625251,"Our code for the speech vocoding and the image generation experiments will be uploaded to Github
after the Ô¨Ånal decision of ICLR is released."
UNSATISFACTORY,0.9098196392785571,"B.4
CROWD-SOURCED SUBJECTIVE EVALUATION"
UNSATISFACTORY,0.9118236472945892,"All our Mean Opinion Score (MOS) tests were crowd-sourced. We refer to the MOS scores in (Pro-
tasio Ribeiro et al., 2011), and the scoring criteria have been included in Table 3 for completeness.
The samples were presented and rated one at a time by the testers."
UNSATISFACTORY,0.9138276553106213,"C
ADDITIONAL EXPERIMENTS"
UNSATISFACTORY,0.9158316633266533,"A
demonstration
page
at
https://bilateral-denoising-diffusion-model.
github.io shows some samples generated by BDDMs trained on LJ speech and VCTK datasets."
UNSATISFACTORY,0.9178356713426854,Published as a conference paper at ICLR 2022
UNSATISFACTORY,0.9198396793587175,"C.1
MULTI-SPEAKER SPEECH SYNTHESIS"
UNSATISFACTORY,0.9218436873747495,"In addition to the single-speaker speech synthesis, we evaluated BDDMs on the multi-speaker
speech synthesis benchmark VCTK (Yamagishi et al., 2019). VCTK consists of utterances sam-
pled at 48 KHz by 108 native English speakers with various accents. We split the VCTK dataset for
training and testing: 100 speakers were used for training the multi-speaker model and 8 speakers
for testing. We trained on a 44257-utterance subset (40 hours) and evaluated on a held-out 100-
utterance subset. For the score network, we used the Wavegrad architecture (Chen et al., 2020) so
as to examine whether the superiority of BDDMs remains in a different dataset and with a different
score network architecture."
UNSATISFACTORY,0.9238476953907816,"Results are presented in Table 4. For this multi-speaker VCTK dataset, we obtained consistent
observations with that for the single-speaker LJ dataset presented in the main paper. Again, the
proposed BDDM with only 16 or 21 steps outperformed the DDPM with 1,000 steps. To the best
of our knowledge, ours was the Ô¨Årst work that reported this degree of superior. When reducing
to 8 steps, BDDM obtained performance on par with (except for a worse PESQ) the costly grid-
searched 8 steps (which were unscalable to more steps) in DDPM. For NE, we could again observe
a degradation from its 16 steps to 21 steps, indicating the instability of NE for the VCTK dataset
likewise. In contrast, BDDM gave continuously improved performance while increasing the step
number."
UNSATISFACTORY,0.9258517034068137,"C.2
COMPARING DIFFERENT REVERSE PROCESSES FOR BDDMS"
UNSATISFACTORY,0.9278557114228457,"This section demonstrates that BDDMs do not restrict the sampling procedure to a specialized re-
verse process in Algorithm 4. In particular, we evaluated different reverse processes, including that
of DDPMs as shown in Eq. (4) and DDIMs (Song et al., 2021), for BDDMs and compared the ob-
jective scores on the generated samples. DDIMs (Song et al., 2021) formulate a non-Markovian gen-
erative process that accelerates the inference while keeping the same training procedure as DDPMs.
The original generative process in Eq. (4) in DDPMs is modiÔ¨Åed into"
UNSATISFACTORY,0.9298597194388778,"p(œÑ)
Œ∏ (x0:T ) := œÄ(xT ) S
Y"
UNSATISFACTORY,0.9318637274549099,"i=1
p(Œ≥i)
Œ∏
(xŒ≥i‚àí1|xŒ≥i) √ó
Y"
UNSATISFACTORY,0.9338677354709419,"t‚àà¬ØŒ≥
p(t)
Œ∏ (x0|xt),
(71)"
UNSATISFACTORY,0.935871743486974,"where Œ≥ is a sub-sequence of length N of [1, ..., T] with Œ≥N = T, and ¬ØŒ≥ := {1, ..., T} \ Œ≥ is deÔ¨Åned
as its complement; Therefore, only part of the models are used in the sampling process."
UNSATISFACTORY,0.9378757515030061,"To achieve the above, DDIMs deÔ¨Åned a prediction function f (t)
Œ∏ (xt) that depends on œµŒ∏ to predict
the observation x0 given xt directly:"
UNSATISFACTORY,0.9398797595190381,"f (t)
Œ∏ (xt) := 1 Œ±t"
UNSATISFACTORY,0.9418837675350702,"
xt ‚àí
q"
UNSATISFACTORY,0.9438877755511023,"1 ‚àíŒ±2
tœµŒ∏(xt, Œ±t)

.
(72)"
UNSATISFACTORY,0.9458917835671342,"By leveraging this prediction function, the conditionals in Eq. (71) are formulated as"
UNSATISFACTORY,0.9478957915831663,"p(Œ≥i)
Œ∏
(xŒ≥i‚àí1|xŒ≥i) = N
Œ±Œ≥i‚àí1"
UNSATISFACTORY,0.9498997995991983,"Œ±Œ≥i
(xŒ≥i ‚àíœÇœµŒ∏(xŒ≥i, Œ±Œ≥i)) , œÉ2
Œ≥iI

ifi ‚àà[N], i > 1
(73)"
UNSATISFACTORY,0.9519038076152304,"p(t)
Œ∏ (x0|xt) = N(f (t)
Œ∏ (xt), œÉ2
t I)
otherwise,
(74)"
UNSATISFACTORY,0.9539078156312625,"where the detailed derivation of œÉt and œÇ can be referred to (Song et al., 2021). In the original
DDIMs, the accelerated reverse process produces samples over the subsequence of Œ≤ indexed by Œ≥:
ÀÜŒ≤ = {Œ≤n|n ‚ààŒ≥}. In BDDMs, to apply the DDIM reverse process, we use the ÀÜŒ≤ predicted by the
schedule network in place of a subsequence of the training schedule Œ≤."
UNSATISFACTORY,0.9559118236472945,"Finally. the objective scores are given in Table 5. Note that the subjective evaluation (MOS) is
omitted here since the other assessments above have shown that the MOS scores are highly corre-
lated with the objective measures, including STOI and PESQ. They indicate that applying BDDMs
to either DDPM or DDIM reverse process leads to comparable and competitive results. Meanwhile,
the results show some subtle differences: BDDMs over a DDPM reverse process gave slightly better
samples in terms of signal error and consistency metrics (i.e., LS-MSE and MCD), while BDDM
over a DDIM reverse process tended to generate better samples in terms of intelligibility and per-
ceptual metrics (i.e., STOI and PESQ)."
UNSATISFACTORY,0.9579158316633266,Published as a conference paper at ICLR 2022
UNSATISFACTORY,0.9599198396793587,"Table 5: Performances of different reverse processes for BDDMs on the VCTK speech dataset, each
of which used the same score network (Chen et al., 2020) œµŒ∏(¬∑) and the same noise schedule."
UNSATISFACTORY,0.9619238476953907,"Noise schedule
LS-MSE (‚Üì)
MCD (‚Üì)
STOI (‚Üë)
PESQ (‚Üë)"
UNSATISFACTORY,0.9639278557114228,"BDDM (DDPM reverse process)
8 steps (0.3, 0.9, 1e‚àí5)
91.3
2.19
0.936
3.22
16 steps (0.7, 0.1, 1e‚àí6)
73.3
1.88
0.949
3.32
21 steps (0.5, 0.1, 1e‚àí6)
72.2
1.91
0.950
3.33"
UNSATISFACTORY,0.9659318637274549,"BDDM (DDIM reverse process)
8 steps (0.3, 0.9, 1e‚àí5)
91.8
2.19
0.938
3.26
16 steps (0.7, 0.1, 1e‚àí6)
77.7
1.96
0.953
3.37
21 steps (0.5, 0.1, 1e‚àí6)
77.6
1.96
0.954
3.39"
UNSATISFACTORY,0.9679358717434869,"Table 6: Comparing sampling methods for DDPM with different number of sampling steps in terms
of FIDs in CIFAR10."
UNSATISFACTORY,0.969939879759519,"Sampling method
Sampling steps
FID"
UNSATISFACTORY,0.9719438877755511,"DDPM (baseline) (Ho et al., 2020)
1000
3.17"
UNSATISFACTORY,0.9739478957915831,"DDPM (sub-VP) (Song et al., 2020b)
‚àº100
3.69"
UNSATISFACTORY,0.9759519038076152,"DDPM (DP + reweighting) (Watson et al., 2021)
128
5.24
64
6.74"
UNSATISFACTORY,0.9779559118236473,"DDIM (quadratic) (Song et al., 2021)
100
4.16
50
4.67"
UNSATISFACTORY,0.9799599198396793,"FastDPM (approx. STEP) (Kong & Ping, 2021)
100
2.86
50
3.20"
UNSATISFACTORY,0.9819639278557114,"2Improved DDPM (hybrid) (Nichol & Dhariwal, 2021)
100
4.63
50
5.09"
UNSATISFACTORY,0.9839679358717435,"VDM (augmented) (Kingma et al., 2021)
1000
7.413"
UNSATISFACTORY,0.9859719438877755,"Ours BDDM
100
2.38
50
2.93"
UNSATISFACTORY,0.9879759519038076,"C.3
UNCONDITIONAL IMAGE GENERATION"
UNSATISFACTORY,0.9899799599198397,"For the unconditional image generation task, we evaluated the proposed BDDMs on the benchmark
CIFAR-10 (32 √ó 32) dataset. The score functions, including those initially proposed in DDPMs
(Ho et al., 2020) or DDIMs (Song et al., 2021) and those pre-trained in the above third-party im-
plementations, are all conditioned on a discrete step-index. We estimated the noise schedule ÀÜŒ≤ in
continuous space using the VGG11 schedule network and then mapped it to discrete time schedule
using the approximation method in (Kong & Ping, 2021)."
UNSATISFACTORY,0.9919839679358717,"Table 6 shows the performances of different sampling methods for DDPMs in CIFAR-10. By set-
ting the maximum number of sampling steps (N) for noise scheduling, we can fairly compare the
improvements achieved by BDDMs against related methods in the literature in terms of FID. Re-
markably, BDDMs with 100 sampling steps not only surpassed the 1000-step DDPM baseline, but
also produced the SOTA FID performance amongst all generative models using less than or equal to
100 sampling steps."
UNSATISFACTORY,0.9939879759519038,"2Our implementation was based on https://github.com/openai/improved-diffusion
3The authors of VDM claimed that they tuned the hyperparameters only for minimizing the likelihood and
did not pursue further tuning of the model to improve FID."
UNSATISFACTORY,0.9959919839679359,Published as a conference paper at ICLR 2022 ùëõ= 3 ùëõ= 2 ùëõ= 1 ùëõ= 0
UNSATISFACTORY,0.9979959919839679,"Figure 4: Spectrum plots of the speech samples produced by BDDM within 3 sampling steps. The
Ô¨Årst row shows the spectrum of a random signal for starting the reverse process. Then, from the top
to the bottom, we show the spectrum of the resultant signal after each step of the reverse process
performed by the BDDM. We also provide the corresponding WAV Ô¨Åles on our demo page."
