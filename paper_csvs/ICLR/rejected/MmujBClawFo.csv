Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.005291005291005291,"Transformer models have achieved signiﬁcant improvements in performance for
various learning tasks in natural language processing and computer vision. Much
of their success is attributed to the use of attention layers that capture long-range
interactions among data tokens (such as words and image patches) via attention
coefﬁcients that are global and adapted to the input data at test time. In this pa-
per we study the principles behind attention and its connections with prior art.
Speciﬁcally, we show that attention builds upon a long history of prior work on
manifold learning and image processing, including methods such as kernel-based
regression, non-local means, locally linear embedding, subspace clustering and
sparse coding. Notably, we show that self-attention is closely related to the notion
of self-expressiveness in subspace clustering, wherein data points to be clustered
are expressed as linear combinations of all other points with coefﬁcients designed
to attend to other points in the same group, thus capturing long-range interactions.
We also show that heuristics in sparse self-attention can be studied in a more prin-
cipled manner using prior literature on sparse coding and sparse subspace cluster-
ing. We thus conclude that the key innovations of attention mechanisms relative to
prior art are the use of many learnable parameters, and multiple heads and layers."
INTRODUCTION,0.010582010582010581,"1
INTRODUCTION"
INTRODUCTION,0.015873015873015872,"Attention, i.e., the ability to selectively focus on a subset of sensory observations, while ignoring
other irrelevant information, is a central component of human perception. For example, only a few
words in a sentence may be useful for predicting the next word, or only a small portion of an image
may be relevant for recognizing an object. This property of biological systems has inspired the recent
development of attention-based neural architectures (Bahdanau et al., 2014), such as Transformers
(Vaswani et al., 2017), BERT (Devlin et al., 2018), GPT (Radford et al., 2018; 2019), RoBERTa (Liu
et al., 2019), and T5 (Raffel et al., 2019), which have achieved impressive performance in multiple
natural language processing tasks, including text classiﬁcation (Chaudhari et al., 2019; Galassi et al.,
2020), machine translation (Ott et al., 2018), and question answering (Garg et al., 2020). Attention-
based architectures have also led to state-of-the-art results in various computer vision tasks (Khan
et al., 2021), including image classiﬁcation (Dosovitskiy et al., 2020), object detection (Carion et al.,
2020; Zhu et al., 2020), and visual question answering (Tan & Bansal, 2019; Su et al., 2019)."
INTRODUCTION,0.021164021164021163,"Much of the success behind attention-based architectures is attributed to their ability to capture long-
range interactions among data tokens (such as words and image patches) via attention coefﬁcients
that are global, learnable and adapted to the input at test time. For example, while recurrent neu-
ral network architectures in natural language processing predict the next word in a sentence using
information about a few previous words, self-attention mechanisms make a prediction based on in-
teractions among all words. Similarly, while convolutional architectures in computer vision compute
local interactions among image patches using weights that do not depend on the input image at test
time, vision transformers compute global interactions that are adapted to the input at test time."
INTRODUCTION,0.026455026455026454,"In this paper, we show that many of the key ideas behind attention, which we brieﬂy summarize
in Section 2, build upon a long history of prior work on manifold learning and image processing.
In Section 3 we show that the scaled dot product attention mechanism is equivalent to kernel-
based regression with the Gaussian kernel (Nadaraya, 1964; Watson, 1964), as recently pointed out
in (Chaudhari et al., 2019; Zhang et al., 2021a), and that more general attention mechanisms can
be obtained by choosing other kernels. We also show in Section 3 that the non-local means image
denoising algorithm (Buades et al., 2005), which can also be understood as a form of kernel-based
regression, is the main building block behind the vision transformer (ViT) (Dosovitskiy et al., 2020)."
INTRODUCTION,0.031746031746031744,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.037037037037037035,"As a consequence, we argue that the key innovation of attention relative to kernel-based regression
is not on its ability to capture global long-range interactions that are adapted to the input data (some-
thing that non-local means already does), but rather on the use of many learnable parameters for
deﬁning attention. In contrast, classical kernel methods typically tune only the kernel bandwidth."
INTRODUCTION,0.042328042328042326,"In Section 4 we establish several connections between masked attention and Locally Linear Embed-
ding (LLE) (Roweis & Saul, 2000; 2003). Speciﬁcally, we show that LLE learns a low-dimensional
representation of a dataset using a masked self-attention mechanism where the masks are deﬁned by
the nearest neighbors of a data point. The resulting coefﬁcients are not constrained to be nonneg-
ative, thus allowing for both positive and negative attention. Moreover, they depend explicitly on
multiple data tokens, unlike attention coefﬁcients which depend only on a pair of tokens. We also
show that LLE’s training objective can be interpreted as a ﬁll in the blanks self-supervised learning
objective. However, a key limitation of LLE is that its local neighborhood is pre-speciﬁed, so a data
point cannot attend to any other point. This issue is resolved by self-expressiveness, which connects
every point to every other point and uses sparse regularization to reveal which points to attend to."
INTRODUCTION,0.047619047619047616,"In Section 5 we show that self-attention is closely related the notion of self-expressiveness of El-
hamifar & Vidal (2009; 2013); Vidal et al. (2016), wherein the data points to be clustered are ex-
pressed as linear combinations of other points with global coefﬁcients that are adapted to the data
and capture long-range interactions among data points. Such self-expressive coefﬁcients are then
used to deﬁne a data afﬁnity matrix which is used to cluster the data. A ﬁrst difference between
self-attention and self-expressive coefﬁcients is that the latter are not restricted to be non-negative,
thus allowing for both positive and negative attention. A second difference is that self-expressive co-
efﬁcients are not deﬁned as a function of the tokens parametrized by learnable weights. Instead, the
coefﬁcients are learned directly using an unsupervised loss. A third difference is that self-expressive
coefﬁcients are typically regularized to be sparse or low-rank. As a consequence, we argue that
the key innovation of self-attention relative to self-expressiveness is neither in its ability to capture
global long-range interactions that are adapted to the data nor in the ability to learn such interactions
(something that self-expressiveness already does), but rather on the fact that attention mechanisms
use multiple attention-heads in parallel and are stacked into deep architectures."
INTRODUCTION,0.05291005291005291,"We conclude with future directions on how to improve self-expressiveness using self-attention and
vice versa. For example, we argue that the use of sparse regularization in (Elhamifar & Vidal, 2009;
2013) to automatically select the most relevant coefﬁcients is a more principled way of handling a
large number of tokens than restricting attention to arbitrary local neighborhoods, as done e.g., in
criss-cross attention (Huang et al., 2019). To achieve this, we suggest unrolling the sparse encod-
ing mechanism in order to induce sparse attention maps through multiple layers of attention. We
conjecture this may not only improve self-attention-based architectures through the use of sparse
regularizers on the attention coefﬁcients, but also improve subspace clustering methods by using
self-attention, as recently proposed in (Zhang et al., 2021b). This could also allow one to extend sub-
space clustering methods to nonlinear manifolds by stacking multiple layers of self-expressiveness."
INTRODUCTION,0.0582010582010582,"2
TRANSFORMERS, ATTENTION AND SELF-ATTENTION"
TRANSFORMER,0.06349206349206349,"2.1
TRANSFORMER"
TRANSFORMER,0.06878306878306878,"Figure 1: Transformer
encoder
architecture
Vaswani et al. (2017)."
TRANSFORMER,0.07407407407407407,"The transformer architecture was originally designed for processing data
sequences, e.g., a sequence of words in a sentence. As shown in Figure 1,
each element of the sequence is ﬁrst mapped to a vector space through
a suitable embedding, e.g., a Word2vec embedding of a word. Since the
architecture does not depend on the position and order of the input se-
quence, a positional encoding is added to the each input embedding. The
resulting input tokens are then processed by a multi-head attention layer.
This layer computes output tokens as linear combinations of input tokens
weighted by attention coefﬁcients designed to capture long-range interac-
tions among input tokens, such as word associations. The output tokens are
then processed by a residual connection followed by layer normalization,
a feed-forward network such as an MLP, and another residual connection
and normalization layer. Therefore, the main component of the transformer
architecture is the (multi-head) attention layer, which we describe next."
TRANSFORMER,0.07936507936507936,Under review as a conference paper at ICLR 2022
ATTENTION,0.08465608465608465,"2.2
ATTENTION"
ATTENTION,0.08994708994708994,"Figure
2:
Scaled
dot product attention
Vaswani et al. (2017)."
ATTENTION,0.09523809523809523,"As illustrated in Figure 2, the attention layer is designed to capture long-
range interactions among three types of input tokens: queries, keys and
values. It does so by comparing queries to keys to produce a set of attention
coefﬁcients, which are then used to generate linear combinations of the val-
ues. Speciﬁcally, let us denote the queries by matrix Q = [q1, . . . , qNq] ∈
Rd×Nq, the keys by matrix K = [k1, . . . , kNk] ∈Rd×Nk, and the corre-
sponding values by matrix V = [v1, . . . , vNk] ∈Rdv×Nk. The attention
layer computes an attention coefﬁcient cij = attn(ki, qj) ∈[0, 1] for each
key-query pair and returns a linear combination of the values as follows: zj = Nk
X"
ATTENTION,0.10052910052910052,"i=1
vicij or Z = V C, where C = attn(K, Q) ∈[0, 1]Nk×Nq.
(1)"
ATTENTION,0.10582010582010581,"Intuitively, the attention coefﬁcient cij measures the importance of key ki for representing query
qj and the representation zj combines the values vi that are most important for qj. The are many
possible choices for the attention mechanism, including additive attention, multiplicative attention
and dot product attention. A common choice is scaled dot product attention, which applies a softmax
operator to the dot product of keys and queries scaled by the square root of their dimension, i.e.:"
ATTENTION,0.1111111111111111,"C = softmax
 K⊤Q
√ d"
ATTENTION,0.1164021164021164,"
=
exp(k⊤
i qj/
√ d)
P"
ATTENTION,0.12169312169312169,"i exp(k⊤
i qj/
√"
ATTENTION,0.12698412698412698,"d)
.
(2)"
ATTENTION,0.13227513227513227,"Since the coefﬁcients are non-negative and add up to one, zj is a convex combination of the values."
ATTENTION,0.13756613756613756,Let us illustrate the intuition behind attention using the following (overly simpliﬁed) examples:
ATTENTION,0.14285714285714285,"1. Suppose we would like to translate sentences from French to English. Let qj be a feature
embedding for the jth word of a sentence in French, and let ki = vi be an embedding for
the ith word of the corresponding sentence in English. Ideally, the attention mechanism
should be designed such that the coefﬁcient cij is large (cij ≈1) only for key-query pairs
(i, j) that correspond to the translation of French word i into English word j, in which case
the output to French query qj will be its translation into English zj = vi.
2. Suppose we are given an image-caption pair and we would like to ﬁnd which regions in
the image corresponds to each word in the caption. Assume we also have a collection of
bounding boxes extracted from the image, e.g., using an object detector. Let the queries be
feature embeddings for the words in the caption and the keys and values be CNN features
extracted from the bounding boxes. Ideally, the attention mechanism should be designed
such that cij is large when the box i corresponds to word j. That is, the attention mechanism
is designed to tell us which regions to pay attention to for each word."
ATTENTION,0.14814814814814814,"Of course, in order for multilingual word embeddings to align with each other, or for word embed-
dings to match image features, both features need to be mapped to a common latent space through
a learnable transformation. We discuss such mappings in the next subsection in the context of self-
attention, but such mapping also apply here."
SELF-ATTENTION,0.15343915343915343,"2.3
SELF-ATTENTION"
SELF-ATTENTION,0.15873015873015872,"Let X = [x1, . . . , xN] ∈RD×N denote a set of data tokens, such as words or image patches. The
goal of self-attention is to capture long-range interactions among such tokens. Such interactions are
captured by ﬁrst transforming these tokens into keys, queries and values via learnable coefﬁcient
matrices WK ∈Rd×D, WQ ∈Rd×D, and WV ∈Rdv×d, respectively, as follows:"
SELF-ATTENTION,0.164021164021164,"K = WKX ∈Rd×N,
Q = WQX ∈Rd×N,
and
V = WV X ∈Rdv×n.
(3)"
SELF-ATTENTION,0.1693121693121693,"Then, we can deﬁne a set of transformed tokens using attention, e.g.:"
SELF-ATTENTION,0.1746031746031746,"Z = V softmax(K⊤Q/
√"
SELF-ATTENTION,0.17989417989417988,"d).
(4)"
SELF-ATTENTION,0.18518518518518517,"Let us illustrate the intuition behind self-attention using the vision transformer (ViT) proposed in
(Dosovitskiy et al., 2020). As shown in Figure 2.3, the ViT divides an input image into a collection"
SELF-ATTENTION,0.19047619047619047,Under review as a conference paper at ICLR 2022
SELF-ATTENTION,0.19576719576719576,"Figure 3: ViT architecture (Dosovitskiy et al., 2020)."
SELF-ATTENTION,0.20105820105820105,"of patches and maps those patches to a set
of vectors via a learnable linear projection.
Each projected patch is augmented with a
positional encoding for the location of the
patch in the image. Since ViT is designed
for image classiﬁcation, an additional (zero)
token is added to the input of the trans-
former. This token is expected to capture
class information and is learned during train-
ing. The transformer encoder processes all
these tokens using self-attention.
Speciﬁ-
cally, new tokens are formed as linear com-
binations of patches weighted by attention
coefﬁcients that capture relationships among
image patches. Moreover, attention coefﬁ-
cients relating the class token to patch tokens are expected to capture which patches to pay attention
to in order to classify the image. The output class token is then passed through an MLP head to pro-
duce class probabilities. The network parameters (input class token, patch projection, self-attention
weights, encoder MLP, MLP head) are learned using a cross-entropy loss for classiﬁcation."
MASKED ATTENTION AND SPARSE ATTENTION,0.20634920634920634,"2.4
MASKED ATTENTION AND SPARSE ATTENTION"
MASKED ATTENTION AND SPARSE ATTENTION,0.21164021164021163,"As discussed in the introduction, much of the success of attention-based architectures is attributed
to the fact that attention layers capture long-range interactions among data tokens via attention co-
efﬁcients that are global and adapted to the input data at test time. However, the use of the softmax
operator often leads to dense attention maps, whose computation can be both memory and computa-
tionally intensive. Moreover, in applications such as document summarization, question answering
or visual grounding, the attention maps are expected to be sparse. One approach to addressing this
issue is to restrict non-zero attention coefﬁcients to certain patterns, such as the criss-cross pattern
proposed in (Huang et al., 2019). In the architecture shown in Figure 2, this is implemented via
masks, hence the name masked attention. However, pre-deﬁning local attention maps might miss
important long-range interactions. As an alternative, Martins & Astudillo (2016) propose to sub-
stitute the softmax operator by a sparsemax operator, which directly induces sparse attention maps.
However, it is not clear why doing so would automatically lead to selecting tokens that are more
informative. This motivated He et al. (2021) to propose heuristics for combining attention maps
in order to select informative tokens for ﬁne-grained recognition. Overall, a rigorous method for
inducing sparsity while maintaining the most informative long-range interactions remains elusive."
MASKED ATTENTION AND SPARSE ATTENTION,0.21693121693121692,"3
KERNEL REGRESSION, NON-LOCAL MEANS DENOISING AND ATTENTION"
MASKED ATTENTION AND SPARSE ATTENTION,0.2222222222222222,"We begin with what arguably is one of the earliest incarnations of the idea of self-attention, namely
kernel regression (Nadaraya, 1964; Watson, 1964). Interestingly, kernel regression is also at the root
of a well-known image denoising algorithm, namely non-local means (Buades et al., 2005), which
we show is strongly connected to the vision transformer (ViT) (Dosovitskiy et al., 2020)."
KERNEL REGRESSION,0.2275132275132275,"3.1
KERNEL REGRESSION"
KERNEL REGRESSION,0.2328042328042328,"The connection between attention and kernel regression was recently pointed out in Chaudhari et al.
(2019); Zhang et al. (2021a). Kernel regression (Nadaraya, 1964; Watson, 1964) is a non-parametric
method for ﬁtting a function f : X →Y to samples {(xj, yj)}N
j=1 drawn from X ×Y, which uses a
kernel density estimator to approximate the minimum-mean-squared-error predictor ˆf(x)=E(y|x).
Speciﬁcally, given a kernel κ : X ×X →R, Nadaraya (1964) and Watson (1964) show that one can
estimate ˆf(x) as a weighted combination of the values of yj, i.e.,"
KERNEL REGRESSION,0.23809523809523808,"f(x) = N
X"
KERNEL REGRESSION,0.24338624338624337,"j=1
α(x, xj)yj = N
X j=1"
KERNEL REGRESSION,0.24867724867724866,"κ(x, xj)
PN
i=1 κ(x, xi)
yj.
(5)"
KERNEL REGRESSION,0.25396825396825395,"Intuitively, the weighting function α(x, xj) encodes the relevance of xj for predicting f(x)."
KERNEL REGRESSION,0.25925925925925924,Under review as a conference paper at ICLR 2022
KERNEL REGRESSION,0.26455026455026454,"When κ is a Gaussian kernel, κ(x, xj) = exp(−∥x−xj∥2
2
2σ2
), the expression in equation 5 in reduces to"
KERNEL REGRESSION,0.2698412698412698,"f(x) = N
X j=1"
KERNEL REGRESSION,0.2751322751322751,"exp(−∥x−xj∥2
2
2σ2
)
PN
i=1 exp(−∥x−xi∥2
2
2σ2
)
yj = N
X"
KERNEL REGRESSION,0.2804232804232804,"j=1
softmax(−∥x −xj∥2
2
2σ2
)yj.
(6)"
KERNEL REGRESSION,0.2857142857142857,"Therefore, Nadayara-Watson regression is an attention mechanism where the query is q = x,
the keys are kj = xj, the values are vj = yj, and the attention function is softmax applied to minus
the normalized squared distance between query and key. Further assuming that keys and queries are
normalized as ∥x∥2 = ∥xj∥2 = 1 so that ∥x −xj∥2
2 = 2(1 −x⊤xj) yields scaled dot product
attention:"
KERNEL REGRESSION,0.291005291005291,"f(x) = N
X j=1"
KERNEL REGRESSION,0.2962962962962963,exp( x⊤xj
KERNEL REGRESSION,0.30158730158730157,"σ2 )
PN
i=1 exp( x⊤xi"
KERNEL REGRESSION,0.30687830687830686,"σ2 )
yj = N
X"
KERNEL REGRESSION,0.31216931216931215,"j=1
softmax(x⊤xj"
KERNEL REGRESSION,0.31746031746031744,"2σ2 )yj.
(7)"
KERNEL REGRESSION,0.32275132275132273,"Despite this obvious connection, we note that kernel regression with the Gaussian kernel is a local
attention mechanism that is unable to capture general long-range interactions. This is because the at-
tention weights depend upon the distance between the key and the query, which is adapted using only
one tunable parameter: σ. When σ is very small, although all pairwise interactions are computed,
large interactions occur only in a local neighborhood, which results in a local attention mechanism.
On the other hand, when σ is very large all weights are similar and we get f(x) ≈1"
KERNEL REGRESSION,0.328042328042328,"N
P yj, which is
clearly not an effective attention mechanism. Therefore, the key advantage of attention with respect
to kernel regression is that it incorporates learnable linear transformations for both key and queries.
Speciﬁcally, if we let q = Wx and kj = Wxj, we obtain q⊤kj = x⊤W ⊤Wxj. In order for
kernel regression to achieve such a learnable dot product, it would need to use a Gaussian kernel
with a full covariance matrix Σ, and learn the resulting dot product which is given by x⊤Σ−1xj."
KERNEL REGRESSION,0.3333333333333333,"More generally, observe that the expression in equation 5 can be used to deﬁne new attention mecha-
nisms by choosing different kernel function κ. For example, the Gaussian, Laplace and Wasserstein
kernels are all members of the exponential family, as they are deﬁned as the exponential of minus a
squared distance, i.e., κ(q, k) = exp(−dist(q, k)2). In this case, the resulting attention mechanism
attn(q, k) = softmax(−dist(q, k)2) is deﬁned based on a notion of similarity (Graves et al., 2014).
On the other hand, it is not clear if all existing attention mechanisms (e.g., additive attention) can be
written in terms of a kernel."
NON-LOCAL MEANS DENOISING,0.3386243386243386,"3.2
NON-LOCAL MEANS DENOISING"
NON-LOCAL MEANS DENOISING,0.3439153439153439,"As the name suggests, image denoising methods aim to remove noise in an mage. The most basic
image denoising method is based on computing the average intensity of a set of neighboring pixels.
Typically, a local Gaussian weighted average is used. Speciﬁcally, if xj denotes the 2D coordinates
of pixel j and yj denote its intensity or RGB values, the denoised image at pixel x takes the form
in equation 5. Since σ is typically chosen to be small (say 3-11 pixels) and Gaussian weights decay
very quickly with the distance ∥x −xj∥, it is customary to restrict the sum in equation 5 to a
neighborhood of x of size ≈3σ. In this case, the sum becomes a convolution with a Gaussian ﬁlter.
Therefore, classical denoising is a local attention mechanism with queries and keys denoting pixel
locations (i.e., qj = kj = xj) and values denoting image intensities (i.e., vj = yj)."
NON-LOCAL MEANS DENOISING,0.3492063492063492,"Non-local means introduces two key modiﬁcations to classical image denoising. First, it computes
the weighted average of the intensities of all pixels, not just of a local neighborhood of x, as in
equation 5. Second, it uses a Gaussian kernel based on the intensity value yj rather than the pixel
location xj. This allows the algorithm to be non-local in that it ﬁnds other (possibly far away) pixels
with similar intensities. Speciﬁcally, in its simplest form, non-local means denoises the image as"
NON-LOCAL MEANS DENOISING,0.3544973544973545,"f(x) = N
X j=1"
NON-LOCAL MEANS DENOISING,0.35978835978835977,exp(−∥y−yj∥2
NON-LOCAL MEANS DENOISING,0.36507936507936506,"2σ2
)
PN
i=1 exp(−∥y−yi∥2"
NON-LOCAL MEANS DENOISING,0.37037037037037035,"2σ2
)
yj.
(8)"
NON-LOCAL MEANS DENOISING,0.37566137566137564,"Therefore, this simpliﬁed form of non-local means denoising is a self-attention mechanism with
queries, keys and values denoting image brightness (i.e., qj = kj = vj = yj)."
NON-LOCAL MEANS DENOISING,0.38095238095238093,"A slightly more general form of the non-local means algorithm computes a Gaussian kernel not on
the intensities y and yj of a single pixel, but on the intensities of patches centered at pixels x and
xj, respectively. This allows the algorithm to attend to distant patches that are similar and hence"
NON-LOCAL MEANS DENOISING,0.3862433862433862,Under review as a conference paper at ICLR 2022
NON-LOCAL MEANS DENOISING,0.3915343915343915,"useful for denoising. Therefore, non-local means denoising is an attention mechanism where the
queries and keys are the intensities of image patches and the values are the intensities of the central
pixel. This connection had been noted in Wang et al. (2018), but surprisingly it is not mentioned in
(Dosovitskiy et al., 2020). Indeed, notice that the steps of non-local means are equivalent to:"
NON-LOCAL MEANS DENOISING,0.3968253968253968,1. Extract a set of overlapping patches from the image.
NON-LOCAL MEANS DENOISING,0.4021164021164021,2. Flatten these patches.
NON-LOCAL MEANS DENOISING,0.4074074074074074,"3. Apply an attention mechanism with the keys and queries being the ﬂattened patches and
the values being the intensity of their central pixel."
NON-LOCAL MEANS DENOISING,0.4126984126984127,"Therefore, non-local means denoising is closely related to the vision transformer, except that (a)
there is no additional classiﬁcation token, (b) the projection of patches is ﬁxed as the identity rather
than learned, (c) no positional encoding is added to the embedded patches, and (d) a single-head and
single-layer self-attention mechanism is used without normalization or fully connected layers."
NON-LOCAL MEANS DENOISING,0.41798941798941797,"4
LOCALLY LINEAR EMBEDDING (LLE) AND LOCAL SELF-ATTENTION"
NON-LOCAL MEANS DENOISING,0.42328042328042326,"In this section, we show that LLE learns a low-dimensional representation of a dataset by using a
masked local self-attention mechanism. Speciﬁcally, we show that LLE coefﬁcients can be inter-
preted as local attention weights with masks deﬁned by the nearest neighbors. We note that LLE
coefﬁcients are not constrained to be nonnegative, thus allowing for both positive and negative atten-
tion, and LLE coefﬁcients depend explicitly on multiple data tokens, unlike additive attention and
scaled dot product which depend only on a pair of tokens (except for softmax). Finally, we show that
LLE’s training objective can be interpreted as a ﬁll in the blanks self-supervised learning objective."
LOCALLY LINEAR EMBEDDING,0.42857142857142855,"4.1
LOCALLY LINEAR EMBEDDING"
LOCALLY LINEAR EMBEDDING,0.43386243386243384,"Let us ﬁrst recall that LLE aims to learn a locally-linear low-dimensional embedding {yj}N
j=1 ⊂Rd"
LOCALLY LINEAR EMBEDDING,0.43915343915343913,"of a given data set {xj}N
j=1 ⊂RD, where D is the data dimension and d ≪D is the embedding
dimension. LLE computes this low-dimensional embedding by ﬁrst expressing each data point xj
as an afﬁne combination of its K-nearest neighbors, i.e., by ﬁnding coefﬁcients cij ∈R such that
xj ≈P"
LOCALLY LINEAR EMBEDDING,0.4444444444444444,i∈Nj xicij and P
LOCALLY LINEAR EMBEDDING,0.4497354497354497,"i∈Nj cij = 1, where Nj ⊂{1, . . . , N} is the set of K-nearest neighbors
of xj. More speciﬁcally, LLE ﬁnds the coefﬁcients by minimizing the reconstruction error"
LOCALLY LINEAR EMBEDDING,0.455026455026455,"min
{cij} N
X j=1"
LOCALLY LINEAR EMBEDDING,0.4603174603174603,"xj −
X"
LOCALLY LINEAR EMBEDDING,0.4656084656084656,"i∈Nj
xicij
2
2
s.t.
X"
LOCALLY LINEAR EMBEDDING,0.4708994708994709,"i∈Nj
cij = 1
∀j = 1, . . . , N.
(9)"
LOCALLY LINEAR EMBEDDING,0.47619047619047616,"Once these coefﬁcients have been found, LLE ﬁnds a low-dimensional representation that is centered
at the origin, has unit covariance, and minimizes the same reconstruction error, i.e."
LOCALLY LINEAR EMBEDDING,0.48148148148148145,"min
{yj} N
X j=1"
LOCALLY LINEAR EMBEDDING,0.48677248677248675,"yj −
X"
LOCALLY LINEAR EMBEDDING,0.49206349206349204,"i∈Nj
yicij
2
2
s.t. N
X"
LOCALLY LINEAR EMBEDDING,0.4973544973544973,"j=1
yj = 0
and N
X"
LOCALLY LINEAR EMBEDDING,0.5026455026455027,"j=1
yjy⊤
j = I.
(10)"
LLE VERSUS LOCAL-ATTENTION,0.5079365079365079,"4.2
LLE VERSUS LOCAL-ATTENTION"
LLE VERSUS LOCAL-ATTENTION,0.5132275132275133,"In order to show that LLE uses a masked local self-attention mechanism, observe that the coefﬁcient
cij in the expression xj ≈P"
LLE VERSUS LOCAL-ATTENTION,0.5185185185185185,"i∈Nj xicij can be interpreted as an attention weight that measures the
contribution of point xi to point xj. Speciﬁcally, note that the optimization problem in equation 9
can be decoupled as N optimization problems, one for each xj, and that the optimal coefﬁcients
for xj are a function of the query xj and the keys {xi}i∈Nj, i.e., {c∗
ij}i∈Nj = f(xj, {xi}i∈Nj)1.
All other coefﬁcients {c∗
ij}i̸∈Nj are set to zero, thus the K nearest neighbors deﬁne a local attention"
LLE VERSUS LOCAL-ATTENTION,0.5238095238095238,"1If j1, j2, . . . , jK are the indices of the K-NN of xj, cj = [cj1,j, cj2,j, . . . , cjK,j]⊤∈RK is its vector of
afﬁne coefﬁcients and Gj = [gj
il] ∈RK×K is its local Gram matrix deﬁned as gj
il = (xi −xj)⊤(xl −xj) if"
LLE VERSUS LOCAL-ATTENTION,0.5291005291005291,"xi and xj that are K-NN of xj, then the optimal solution is cj =
G−1
j
1"
LLE VERSUS LOCAL-ATTENTION,0.5343915343915344,"1⊤G−1
j
1."
LLE VERSUS LOCAL-ATTENTION,0.5396825396825397,Under review as a conference paper at ICLR 2022
LLE VERSUS LOCAL-ATTENTION,0.544973544973545,"mask. Note also that the constraint in equation 9 ensures the weights add up to 1 without requiring a
softmax post-processing. Finally, notice that the training objective in equation 9 is a ﬁll in the blanks
objective, where the nearest neighbors of xj, {xi}i∈Nj, are used to predict the missing token xj."
LLE VERSUS LOCAL-ATTENTION,0.5502645502645502,"Despite these similarities between LLE and existing attention mechanisms, there are some important
differences. First, most existing attention mechanisms compute a score function applied to a single
query-key pair and then apply the softmax function so that attention weights are between 0 and 1.
In contrast, LLE coefﬁcients are not constrained to be nonnegative, thus allowing for both positive
and negative attention. Moreover, LLE coefﬁcients depend on both the query and multiple keys.
Another difference, perhaps the most important one, is that in most existing attention mechanisms
cij is a parametrized function of the query-key pair whose weights are learned during training. In
sharp contrast, LLE learns the values of cij directly, which makes it more difﬁcult to evaluate the
coefﬁcients for new data, as the optimization problem in equation 9 needs to be solved anew."
LLE VERSUS LOCAL-ATTENTION,0.5555555555555556,"Despite these differences, we note many of the key ingredients of attention (key, query, value, mask)
were already present in the original LLE formulation, albeit for different purposes. In particular,
LLE is based on the idea that each query attends its K nearest neighbors by writing itself as an
afﬁne combination of such neighbors. The attention weights thus capture the local geometry of the
data manifold and are hence used to ﬁnd the low-dimensional embedding as per equation 10."
LLE VERSUS LOCAL-ATTENTION,0.5608465608465608,"5
SUBSPACE CLUSTERING, SELF-EXPRESSIVENESS AND SELF-ATTENTION"
LLE VERSUS LOCAL-ATTENTION,0.5661375661375662,"A key limitation of LLE is that its local neighborhood is pre-speciﬁed, so a data point cannot attend
to any other point. In this section we show that this issue is resolved by self-expressiveness (El-
hamifar & Vidal, 2009; 2013; Vidal et al., 2016), which connects every point to every other point
and uses sparse regularization to reveal which points to attend to. Speciﬁcally, we show that self-
expressiveness based subspace clustering methods such as sparse subspace clustering (Elhamifar
& Vidal, 2009; 2013; Wang & Xu, 2013), low-rank subspace clustering (Liu et al., 2010; Vidal &
Favaro, 2014), least squares regression (Lu et al., 2012) and extensions (Wang et al., 2013) compute
a data afﬁnity using a global masked self-attention mechanism where the queries, keys and values
are the data points to be clustered, and the self-expressive coefﬁcients of a data point are designed to
attend to other points in the same subspace. We note, however, that self-expressive coefﬁcients are
not constrained to be nonnegative, thus allowing for both positive and negative attention. We also
show that self-expressive coefﬁcients are global in that they truly depend on multiple data points,
unlike most attention mechanisms that depend only on a pair of tokens (except for softmax). Finally,
we show that the subspace clustering training objective can be interpreted as a ﬁll in the blanks self-
supervised learning objective where each data point is regressed with respect to all other data points."
SUBSPACE CLUSTERING AND SELF-EXPRESSIVENESS,0.5714285714285714,"5.1
SUBSPACE CLUSTERING AND SELF-EXPRESSIVENESS"
SUBSPACE CLUSTERING AND SELF-EXPRESSIVENESS,0.5767195767195767,"Subspace clustering refers to the problem of clustering data drawn from a union of subspaces. Self-
expressiveness based methods solve this problem by expressing each data point as a linear combi-
nation of all other data points. The resulting self-expressive coefﬁcients reveal information about
which points belong to the same subspace, hence they can be used to deﬁne a suitable data afﬁnity
matrix. The clustering of the data is then obtained by applying spectral clustering to such an afﬁnity."
SUBSPACE CLUSTERING AND SELF-EXPRESSIVENESS,0.582010582010582,"More formally, let X = [x1, . . . , xN] be a set of points drawn from a union of n subspaces of RD of
dimension d ≪D which we wish to cluster. Assume that the data from each subspace is sufﬁciently
rich so that any d points from one group span the subspace associated to that group. Then, each data
point xj can be expressed as a linear combination of d other points in its own subspace. That is, for
all j = 1, . . . , N, there exist at most d non-zero coefﬁcients cij ∈R such that:"
SUBSPACE CLUSTERING AND SELF-EXPRESSIVENESS,0.5873015873015873,"xj =
X"
SUBSPACE CLUSTERING AND SELF-EXPRESSIVENESS,0.5925925925925926,"i̸=j
xicij,
or
X = XC and diag(C) = 0,
(11)"
SUBSPACE CLUSTERING AND SELF-EXPRESSIVENESS,0.5978835978835979,"where C ∈RN×N is the matrix of coefﬁcients. Notice that in the above constraint data points are
expressed as linear combinations of each other, hence the name self-expressiveness."
SUBSPACE CLUSTERING AND SELF-EXPRESSIVENESS,0.6031746031746031,"Since our goal is to use the self-expressive coefﬁcients to deﬁne an afﬁnity matrix for clustering the
data, ideally the coefﬁcients should have the property that cij ̸= 0 only if points xi and xj are in"
SUBSPACE CLUSTERING AND SELF-EXPRESSIVENESS,0.6084656084656085,Under review as a conference paper at ICLR 2022
SUBSPACE CLUSTERING AND SELF-EXPRESSIVENESS,0.6137566137566137,"the same subspace. Coefﬁcients that satisfy such a property are guaranteed to exist since a point
can always be expressed in terms of d points in its own subspace. Moreover, if d ≪N, i.e., if the
subspaces are low-dimensional and the number of data points is sufﬁciently large, such coefﬁcients
are sparse. This motivates the sparse subspace clustering objective (Elhamifar & Vidal, 2009)"
SUBSPACE CLUSTERING AND SELF-EXPRESSIVENESS,0.6190476190476191,"min
{cij} ∥xj −
X"
SUBSPACE CLUSTERING AND SELF-EXPRESSIVENESS,0.6243386243386243,"i̸=j
xicij∥2
2 + λ
X"
SUBSPACE CLUSTERING AND SELF-EXPRESSIVENESS,0.6296296296296297,"i̸=j
|cij|,
or
min
C:diag(C)=0 ∥X −XC∥2
F + λ∥C∥1,
(12)"
SUBSPACE CLUSTERING AND SELF-EXPRESSIVENESS,0.6349206349206349,"where the ﬁrst term measures how well a data point is reconstructed in terms of other data points, the
second term uses ℓ1 regularization to encourage sparsity, and λ > 0 is a regularization parameter.
More generally, one can use other regularizers Θ and write the objective in terms of the matrix C
min
C ∥X −XC∥2
F + λΘ(C).
(13)"
SUBSPACE CLUSTERING AND SELF-EXPRESSIVENESS,0.6402116402116402,"Once the coefﬁcients have been computed (see next subsection), it is common to select the largest
nonzero coefﬁcients to induce additional sparsity and to normalize the columns of C so that they add
up to one (Elhamifar & Vidal, 2013). Alternatively, once can add an ℓ1-normalization constraints
to the optimization problem in equation 13, as is commonly done in afﬁne subspace clustering
(Elhamifar & Vidal, 2013; Li et al., 2018; You et al., 2019). Interestingly, it appears that sofmax
normalization of the coefﬁcients has never used in the subspace clustering literature. Finally, given
C, the data is clustered by applying spectral clustering to an afﬁnity matrix that is often constructed
by symmetrizing the absolute values of the self-expressive coefﬁcients, i.e., A = |C| + |C⊤|."
SELF-EXPRESSIVE COEFFICIENTS FOR DIFFERENT REGULARIZERS,0.6455026455026455,"5.2
SELF-EXPRESSIVE COEFFICIENTS FOR DIFFERENT REGULARIZERS"
SELF-EXPRESSIVE COEFFICIENTS FOR DIFFERENT REGULARIZERS,0.6507936507936508,"The least squares regression approach (Lu et al., 2012) uses Θ(C)=∥C∥2
F and gives a closed form
solution for C = (X⊤X + λI)−1X⊤X = V (Σ2 + λI)−1Σ2V ⊤, where X = UΣV ⊤is the
SVD of the data. Therefore, the self-expressive coefﬁcient cij = v⊤
i (Σ2+λI)−1Σ2vj is a weighted
dot product of rows of V . When λ is large enough we get a scaled dot product of the data points C ≈1"
SELF-EXPRESSIVE COEFFICIENTS FOR DIFFERENT REGULARIZERS,0.656084656084656,"λX⊤X.
(14)"
SELF-EXPRESSIVE COEFFICIENTS FOR DIFFERENT REGULARIZERS,0.6613756613756614,"The low-rank subspace clustering approach (Liu et al., 2010; Vidal & Favaro, 2014) uses a nuclear
norm regularizer Θ(C) = ∥C∥∗to induce low-rank coefﬁcients. The solution can be computed in
closed form from the SVD of the data as C = V ReLUλ(Σ)V ⊤, where ReLUλ(x) = max(x−λ, 0).
As before, this can be interpreted as a weighted dot product of rows of V , except that some weights
can be zero to induce low-rank."
SELF-EXPRESSIVE COEFFICIENTS FOR DIFFERENT REGULARIZERS,0.6666666666666666,"The sparse subspace clustering approach (Elhamifar & Vidal, 2009; 2013; Wang & Xu, 2013) uses
the ℓ1 norm Θ(C) = ∥C∥1 to induce sparse coefﬁcients. In this case, the coefﬁcients cannot be
computed in closed form. However, a common approach is to use the Iterative Shrinkage Thresh-
olding Algorithm (ISTA) proposed by (Beck & Teboulle, 2009), which can be written as:2"
SELF-EXPRESSIVE COEFFICIENTS FOR DIFFERENT REGULARIZERS,0.671957671957672,"Ck+1 = ReLUλ
 
(I −ϵX⊤X)Ck + ϵX⊤X

= ReLUλ
 
Ck + ϵX⊤(X −XCk)),
(15)"
SELF-EXPRESSIVE COEFFICIENTS FOR DIFFERENT REGULARIZERS,0.6772486772486772,"Figure 4: Towards a
sparse transformer?"
SELF-EXPRESSIVE COEFFICIENTS FOR DIFFERENT REGULARIZERS,0.6825396825396826,"where ϵ > 0 is a step size. We note that equation 15 is the point of the
departure for the unrolling approach proposed in (Gregor & LeCun, 2010),
which connects sparse coding with neural networks. In that approach, the
iterates are interpreted as activation functions of a neural network and the
linear transformations (I −ϵX⊤X) and X⊤X as learnable weights."
SELF-EXPRESSIVE COEFFICIENTS FOR DIFFERENT REGULARIZERS,0.6878306878306878,"As a future research direction, we suggest further exploring the connections
between sparse subspace clustering and transformers via unrolling, which we
conjecture will allow us to extend subspace clustering to nonlinear manifolds
through the use of (deep) multi-layer attention models. More speciﬁcally,
notice that we can partially re-interpret equation 15 as the update of one
attention layer. This is because in equation 15, the term X −XCk can
be interpreted as applying attention Ck to input data X and then adding a
(negative) residual connection with the input X. Then, the multiplication by
X⊤in in equation 15 and the ReLU nonlinearity can be interpreted as the
feedforward layer of the transformer. Of course, the analogy is not perfect
because the addition of Ck is not quite a residual connection."
SELF-EXPRESSIVE COEFFICIENTS FOR DIFFERENT REGULARIZERS,0.6931216931216931,2We have neglected the constraint diag(C) = 0 for ease of exposition
SELF-EXPRESSIVE COEFFICIENTS FOR DIFFERENT REGULARIZERS,0.6984126984126984,Under review as a conference paper at ICLR 2022
SELF-EXPRESSIVENESS VERSUS SELF-ATTENTION,0.7037037037037037,"5.3
SELF-EXPRESSIVENESS VERSUS SELF-ATTENTION"
SELF-EXPRESSIVENESS VERSUS SELF-ATTENTION,0.708994708994709,"Notice from equation 11 that self-expressiveness can be interpreted as a self-attention mechanism
where the query qj = xj is expressed as a linear combination of all values vi = xi, i = 1, . . . , N,
with attention coefﬁcients cij determined by the queries qj = xj and the keys ki = xi. However,
we note that self-expressive coefﬁcients (SEC) are more general than self-attention coefﬁcients."
SELF-EXPRESSIVENESS VERSUS SELF-ATTENTION,0.7142857142857143,"1. SEC are not restricted to be nonnegative, allowing for both positive and negative attention.
2. SEC are not restricted to be an explicit function of a single key-query pair. For example, the
closed form solution to least squares regression has a term of the form (λI + X⊤X)−1,
which makes cij a function of all key-query pairs. The only case where self-expressive
coefﬁcients yield an explicit function of a single key-query pair is when λ is large that as
per equation 14, which resembles a scaled dot product attention.
3. SEC are not deﬁned as a function of the tokens parametrized by learnable weights. Instead,
the coefﬁcients are learned directly using an unsupervised loss. This is, however, a potential
disadvantage of self-expressiveness, as it makes it difﬁcult to compute coefﬁcients at test
time. This issue is addressed in (Zhang et al., 2021b) by using learnable coefﬁcients.
4. SEC are typically regularized to be sparse or low-rank. We argue that the use of sparse
regularization in (Elhamifar & Vidal, 2009; 2013) to automatically select the most relevant
coefﬁcients is a more principled way of handling a large number of tokens than restricting
attention to arbitrary local neighborhoods, e.g., in criss-cross attention (Huang et al., 2019)."
SELF-EXPRESSIVENESS VERSUS SELF-ATTENTION,0.7195767195767195,"As a consequence, we argue that the key innovation of self-attention relative to self-expressiveness
is neither in its ability to capture global long-range interactions that are adapted to the data nor in the
ability to learn such interactions (something that self-expressiveness already does), but rather on the
fact that attention mechanisms have been stacked into deep architectures and with multiple attention-
heads in parallel. As suggested in the previous section, further exploring the connections between
sparse subspace clustering and transformers via unrolling might lead to (deep) multi-layer subspace
clustering models. Alternatively, one may use attention mechanisms to parametrize self-expressive
coefﬁcients, as recently suggested in (Zhang et al., 2021b)."
SPARSE CODING AND SPARSE ATTENTION,0.7248677248677249,"5.4
SPARSE CODING AND SPARSE ATTENTION"
SPARSE CODING AND SPARSE ATTENTION,0.7301587301587301,"The connections made between self-expressiveness and self-attention also suggest new directions
towards improving transformers via sparse encoding. Speciﬁcally, recall that in standard sparse cod-
ing, a data point y is expressed as a sparse linear combination of dictionary atoms A = [a1, . . . , aN]
with coefﬁcients c by solving the optimization problem
min
c ∥y −Ac∥2
2 + λ∥c∥1.
(16)"
SPARSE CODING AND SPARSE ATTENTION,0.7354497354497355,"Reinterpreting the data point y as the query q and the dictionary A as the set of keys K, and solving
the problem for multiple queries Q leads to an attention mechanism of the form
Z = V C∗
where
C∗= arg min
C ∥Q −KC∥2
2 + λ∥C∥1.
(17)"
SPARSE CODING AND SPARSE ATTENTION,0.7407407407407407,"Since solving a sparse coding problem can be costly, we unroll sparse coding iterates and obtain:
Z = V CK
where
Ck+1 = ReLUλ((I −ϵK⊤K)Ck + ϵK⊤Q),
k = 1, . . . , K.
(18)
Observe that the update has a rather interesting structure. The term K⊤K is dot product self-
attention, while the term K⊤Q is dot product attention. Therefore, the update equation combines
standard attention and self-attention to produce a new sparse attention map."
CONCLUSIONS,0.746031746031746,"6
CONCLUSIONS"
CONCLUSIONS,0.7513227513227513,"We have shown that attention builds upon a long history of prior work on manifold learning and im-
age processing, including methods such as kernel-based regression, non-local means, locally linear
embedding, subspace clustering and sparse coding. In particular, we showed that many of the key
ideas behind attention, such as its ability to capture global long-range interactions that are learned
and adapted to the input, had already appeared in the literature. Therefore, the key innovations of
attention mechanisms relative to prior art are the use of many learnable parameters, and multiple
heads and layers."
CONCLUSIONS,0.7566137566137566,Under review as a conference paper at ICLR 2022
ETHICS STATEMENT,0.7619047619047619,"Ethics Statement
This work focuses on understanding the principles behind transformers and
connecting it to well established topics in machine learning research. The research conducted in the
framework of this work raises no ethical issues or any violations vis-a-vis the ICLR Code of Ethics."
REFERENCES,0.7671957671957672,REFERENCES
REFERENCES,0.7724867724867724,"Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473, 2014."
REFERENCES,0.7777777777777778,"A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse prob-
lems. SIAM Journal on Imaging Sciences, 2(1):183–202, 2009."
REFERENCES,0.783068783068783,"Antoni Buades, Bartomeu Coll, and Jean-Michel Morel. A non-local algorithm for image denoising.
In IEEE Conference on Computer Vision and Pattern Recognition, 2005."
REFERENCES,0.7883597883597884,"Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and
Sergey Zagoruyko.
End-to-end object detection with transformers.
In European Conference
on Computer Vision, pp. 213–229. Springer, 2020."
REFERENCES,0.7936507936507936,"Sneha Chaudhari, Varun Mithal, Gungor Polatkan, and Rohan Ramanath. An attentive survey of
attention models. arXiv preprint arXiv:1904.02874, 2019."
REFERENCES,0.798941798941799,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018."
REFERENCES,0.8042328042328042,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An
image is worth 16x16 words: Transformers for image recognition at scale.
arXiv preprint
arXiv:2010.11929, 2020."
REFERENCES,0.8095238095238095,"Ehsan Elhamifar and Ren´e Vidal. Sparse subspace clustering. In IEEE Conference on Computer
Vision and Pattern Recognition, pp. 2790–2797, 2009."
REFERENCES,0.8148148148148148,"Ehsan Elhamifar and Ren´e Vidal. Sparse subspace clustering: Algorithm, theory, and applications.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(11):2765–2781, 2013."
REFERENCES,0.8201058201058201,"Andrea Galassi, Marco Lippi, and Paolo Torroni. Attention in natural language processing. IEEE
Transactions on Neural Networks and Learning Systems, 2020."
REFERENCES,0.8253968253968254,"Siddhant Garg, Thuy Vu, and Alessandro Moschitti. Tanda: Transfer and adapt pre-trained trans-
former models for answer sentence selection. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, volume 34, pp. 7780–7788, 2020."
REFERENCES,0.8306878306878307,"Alex Graves, Greg Wayne, and Ivo Danihelka.
Neural turing machines.
arXiv preprint
arXiv:1410.5401, 2014."
REFERENCES,0.8359788359788359,"K. Gregor and Y. LeCun. Learning fast approximations of sparse coding. In International Confer-
ence on Machine Learning, pp. 399–406. Omnipress, 2010."
REFERENCES,0.8412698412698413,"Ju He, Jie-Neng Chen, Shuai Liu, Adam Kortylewski, Cheng Yang, Yutong Bai, Changhu Wang,
and Alan Yuille. Transfg: A transformer architecture for ﬁne-grained recognition. arXiv preprint
arXiv:2103.07976, 2021."
REFERENCES,0.8465608465608465,"Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu. Ccnet:
Criss-cross attention for semantic segmentation. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pp. 603–612, 2019."
REFERENCES,0.8518518518518519,"Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and
Mubarak Shah. Transformers in vision: A survey. arXiv preprint arXiv:2101.01169, 2021."
REFERENCES,0.8571428571428571,"Chun-Guang Li, Chong You, and Ren´e Vidal. On geometric analysis of afﬁne sparse subspace
clustering. IEEE Journal on Selected Topics in Signal Processing, 12(6):1520–1533, 2018."
REFERENCES,0.8624338624338624,Under review as a conference paper at ICLR 2022
REFERENCES,0.8677248677248677,"Guangcan Liu, Zhouchen Lin, and Yingrui Yu. Robust subspace segmentation by low-rank repre-
sentation. In International Conference on Machine Learning, pp. 663–670, 2010."
REFERENCES,0.873015873015873,"Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692, 2019."
REFERENCES,0.8783068783068783,"Can-Yi Lu, Hai Min, Zhong-Qiu Zhao, Lin Zhu, De-Shuang Huang, and Shuicheng Yan. Robust
and efﬁcient subspace segmentation via least squares regression. In European Conference on
Computer Vision, pp. 347–360, 2012."
REFERENCES,0.8835978835978836,"Andre Martins and Ramon Astudillo. From softmax to sparsemax: A sparse model of attention
and multi-label classiﬁcation. In International conference on machine learning, pp. 1614–1623.
PMLR, 2016."
REFERENCES,0.8888888888888888,"Elizbar A Nadaraya. On estimating regression. Theory of Probability & Its Applications, 9(1):
141–142, 1964."
REFERENCES,0.8941798941798942,"Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation.
arXiv preprint arXiv:1806.00187, 2018."
REFERENCES,0.8994708994708994,"Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-
standing by generative pre-training (2018), 2018."
REFERENCES,0.9047619047619048,"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language
models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019."
REFERENCES,0.91005291005291,"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text
transformer. arXiv preprint arXiv:1910.10683, 2019."
REFERENCES,0.9153439153439153,"S. Roweis and L. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science,
290(5500):2323–2326, 2000."
REFERENCES,0.9206349206349206,"S. Roweis and L. Saul. Think globally, ﬁt locally: Unsupervised learning of low dimensional mani-
folds. Journal of Machine Learning Research, 4:119–155, 2003."
REFERENCES,0.9259259259259259,"Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. Vl-bert: Pre-training
of generic visual-linguistic representations. arXiv preprint arXiv:1908.08530, 2019."
REFERENCES,0.9312169312169312,"Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from trans-
formers. arXiv preprint arXiv:1908.07490, 2019."
REFERENCES,0.9365079365079365,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Neural Information Processing
Systems, pp. 5998–6008, 2017."
REFERENCES,0.9417989417989417,"Ren´e Vidal and Paolo Favaro. Low rank subspace clustering (LRSC). Pattern Recognition Letters,
43:47–61, 2014."
REFERENCES,0.9470899470899471,"Ren´e Vidal, Yi Ma, and Shankar Sastry. Generalized Principal Component Analysis. Springer
Verlag, 2016."
REFERENCES,0.9523809523809523,"Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2018."
REFERENCES,0.9576719576719577,"Yu-Xiang Wang and Huan Xu. Noisy sparse subspace clustering. In International Conference on
Machine Learning, pp. 89–97, 2013."
REFERENCES,0.9629629629629629,"Yu-Xiang Wang, Huan Xu, and Chenlei Leng. Provable subspace clustering: When LRR meets
SSC. In Neural Information Processing Systems, 2013."
REFERENCES,0.9682539682539683,"Geoffrey S Watson. Smooth regression analysis. Sankhy¯a: The Indian Journal of Statistics, Series
A, pp. 359–372, 1964."
REFERENCES,0.9735449735449735,Under review as a conference paper at ICLR 2022
REFERENCES,0.9788359788359788,"Chong You, Chun-Guang Li, Daniel P. Robinson, and Ren´e Vidal. Is an afﬁne constraint needed for
afﬁne subspace clustering? In IEEE International Conference on Computer Vision, 2019."
REFERENCES,0.9841269841269841,"Aston Zhang, Zachary C Lipton, Mu Li, and Alexander J Smola. Dive into deep learning. arXiv
preprint arXiv:2106.11342, 2021a."
REFERENCES,0.9894179894179894,"Shangzhi Zhang, Chong You, Ren´e Vidal, and Chun-Guang Li. Learning a self-expressive network
for subspace clustering. In IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2021b."
REFERENCES,0.9947089947089947,"Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr:
Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020."
