Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.005154639175257732,"Traditional semi-supervised learning (SSL) assumes that the feature distributions of
labeled and unlabeled data are consistent which rarely holds in realistic scenarios.
In this paper, we propose a novel SSL setting, where unlabeled samples are drawn
from a mixed distribution that deviates from the feature distribution of labeled
samples. Under this setting, previous SSL methods tend to predict wrong pseudo-
labels with the model fitted on labeled data, resulting in noise accumulation. To
tackle this issue, we propose Self-Supervised Feature Adaptation (SSFA), a generic
framework for improving SSL performance when labeled and unlabeled data come
from different distributions. SSFA decouples the prediction of pseudo-labels from
the current model to improve the quality of pseudo-labels. Particularly, SSFA
incorporates a self-supervised task into the SSL framework and uses it to adapt
the feature extractor of the model to the unlabeled data. In this way, the extracted
features better fit the distribution of unlabeled data, thereby generating high-quality
pseudo-labels. Extensive experiments show that our proposed SSFA is applicable
to various pseudo-label-based SSL learners and significantly improves performance
in labeled, unlabeled, and even unseen distributions."
INTRODUCTION,0.010309278350515464,"1
Introduction"
INTRODUCTION,0.015463917525773196,"Semi-Supervised Learning (SSL) uses a small amount of labeled data and a large amount of unlabeled
data to alleviate the pressure of data labeling and improve the generalization ability of the model.
Traditional SSL methods [21, 43, 32, 28, 22, 30, 5, 6] usually assume that the feature distribution of
unlabeled data is consistent with that of labeled one. However, in many real scenarios, this assumption
may not hold due to different data sources. When unlabeled data is sampled from distributions differ-
ent from labeled data, traditional SSL algorithms will suffer from severe performance degradation,
which greatly limits their practical application."
INTRODUCTION,0.020618556701030927,"In real-world scenarios, it is quite common to observe feature distribution mismatch between labeled
and unlabeled samples. On the one hand, unlabeled samples could contain various corruptions. For
example, in automatic driving, the annotated images used for training can hardly cover all driving
scenes, and a plethora of images under varying weather and camera conditions are captured during
real driving. Similarly, in medical diagnosis, individual differences and shooting conditions among
patients could incur various disturbances in unlabeled data. On the other hand, unlabeled samples
could contain unseen styles. For example, in many tasks, the labeled samples are typically real-world
photos, while unlabeled data collected from the Internet usually contain more styles that are not
present in the labeled data, such as cartoons or sketches. Notably, although the distributions of these
unlabeled data may differ from the labeled data, there is implicit common knowledge between them
that can compensate for the diversity and quantity of training data. Therefore, it is crucial to enlarge
the SSL scope to effectively utilize unlabeled data from different distributions."
INTRODUCTION,0.02577319587628866,"In this study, we focus on a more realistic scenario of Feature Distribution Mismatch SSL (FDM-
SSL), i.e., the feature distributions of labeled and unlabeled data could be different and the feature
distributions of test data could contain multiple distributions. It is generally observed that the
performance of classical SSL algorithms [22, 30, 6, 9] degrades substantially under feature distribution
mismatch. Specifically, in the early stages of training, classical SSL algorithms typically utilize the
current model fitted on labeled data to produce pseudo-labels for unlabeled data. However, when
the distribution of unlabeled data deviates, the current model are not applicable to unlabeled data,
resulting in massive incorrect pseudo-labels and aggravating confirmation bias [3]. Recently, some
works [7, 19] attempt to address FDM-SSL. These approaches assume that the unlabeled feature
distribution comes from a single source and they only focus on the labeled distribution, which do not
always hold in real tasks. However, due to the unknown test scenarios, it is desirable to develop a
method to perform well on labeled, unlabeled and even unseen distributions simultaneously."
INTRODUCTION,0.030927835051546393,"In this work, we propose a generalized Self-Supervised Feature Adaptation (SSFA) framework for
FDM-SSL which does not need to know the distribution of unlabeled data ahead of time. The core
idea of SSFA is to decouple pseudo-label predictions from the current model to address distribution
mismatch. SSFA consists of two modules, including the semi-supervised learning module and
the feature adaptation module. Inspired by [31] that the main classification task can be indirectly
optimized through the auxiliary task, SSFA incorporates an auxiliary self-supervised task into the
SSL module to train with the main task. In the feature adaptation module, given the current model
primarily fitted on labeled data, SSFA utilizes the self-supervised task to update the feature extractor
before making predictions on the unlabeled data. After the feature extractor adapts to the unlabeled
distribution, refined features can be used to generate more accurate pseudo-labels to assist SSL."
INTRODUCTION,0.03608247422680412,"Furthermore, the standard evaluation protocol of SSL normally assumes that test samples follow the
same feature distribution as labeled training data. However, this is too restricted to reflect the diversity
of real-world applications, where different tasks may focus on different test distributions. It is strongly
desired that the SSL model can perform well across a wide range of test distributions. Therefore, in
this work, we propose new evaluation protocols that involve test data from labeled, unlabeled and
unseen distributions, allowing for a more comprehensive assessment of SSL performance. Extensive
experiments are conducted on two types of feature distribution mismatch SSL tasks, i.e., corruption
and style mismatch. The experimental results demonstrate that various pseudo-label-based SSL
methods can be directly incorporated into SSFA, yielding consistent performance gains across a wide
range of test distributions."
RELATED WORK,0.041237113402061855,"2
Related work"
RELATED WORK,0.04639175257731959,"Semi-Supervised Learning (SSL). Existing SSL methods [44, 7, 30, 41, 50, 9, 6] typically combine
various commonly used techniques for semi-supervised tasks, such as consistency regularization[21,
43, 32, 28] and pseudo-label [22], to achieve state-of-the-art performance. For example, [6] expands
the dataset by interpolating between labeled and unlabeled data. [5, 30] use the confident weak
augmented view prediction results to generate pseudo-labels for the corresponding strong augmented
view. [41, 50, 9, 44] improve the performance by applying adaptive thresholds instead of fixed
thresholds. However, these works focus on traditional semi-supervised learning, which assumes
that the labeled data and unlabeled data are sampled from the same distribution. Recently, some
works [2, 53] address the issue of different feature distributions for labeled and unlabeled data
under certain prerequisites. [2] assumes the test and unlabeled samples are drawn from the same
single distribution. [53] studies Semi-Supervised Domain Generalization (SSDG), which requires
multi-source partially-labeled training data. To further expand SSL to a more realistic scenario,
we introduce a new Feature Distribution Mismatch SSL (FDM-SSL) setting, where the unlabeled
data comes from multiple distributions and may differ from the labeled distribution. To solve the
FDM-SSL, we propose Self-Supervised Feature Adaptation (SSFA), a generic framework to improve
the performance in labeled, unlabeled, and even unseen test distributions."
RELATED WORK,0.05154639175257732,"Unsupervised Domain Adaptation (UDA). UDA aims to transfer knowledge from a source domain
with sufficient labeled data to an unlabeled target domain through adaptive learning. The UDA
methods can be roughly categorized into two categories, namely metric-based methods [23, 35, 25,
34, 49] and adversarial methods [1, 16, 8, 24, 46, 29, 37, 10, 42]. Metric-based methods focus on
measuring domain discrepancy to align the feature distributions between source and unlabeled target
domains. Adversarial methods [16, 47, 24] use a domain classifier as a discriminator to enforce the"
RELATED WORK,0.05670103092783505,"Table 1: Comparison between different problem settings.
Task
Labeled
Unlabeled
Train setting
Test distribution"
RELATED WORK,0.061855670103092786,"traditional SSL
scarce
abundant
pl(x) = pu(x)
pl(x)
UDA
abundant
abundant
pl(x) Ì¸= pu(x)
pu(x)
TTA
abundant
-
pl(x)
pu(x)
FDM-SSL
scarce
abundant
pl(x) Ì¸= pu(x)
pl(x), pu(x), punseen(x)"
RELATED WORK,0.06701030927835051,"feature extractor to learn domain-invariant features through adversarial training. Another task similar
to FDM-SSL is Unsupervised Domain Expansion (UDE) [39, 51, 33, 45], which aims to maintain
the modelâ€™s performance on the labeled domain after adapting to the unlabeled domain. The main
differences between UDA, UDE and FDM-SSL are the number of labeled data during training and
the distribution of the unlabeled data. In FDM-SSL, the labeled data is scarce, and the distribution of
unlabeled data is not limited to a specific domain but rather a mixture of multiple domains. These
challenges make UDA and UDE methods unable to be directly applied to FDM-SSL."
RELATED WORK,0.07216494845360824,"Test-Time Adaptation (TTA). If the distribution of test data differs from that of training data, the
modelâ€™s performance may suffer performance degradation. TTA methods focus on improving the
modelâ€™s performance with a two-stage process: a model should first adapt to the test samples and then
make predictions of them. For example, [31] and [15] introduce an auxiliary self-learning task to adapt
the model to test distribution. [38] modifies the scaling and bias parameters of BatchNorm layers
based on entropy minimization. [52, 13, 12, 26] try to improve the robustness of test-time adaptation.
Unlike most TTA methods which suppose the test samples come from a single distribution, [40]
assumes a mixed test distribution that changes continuously and stably. However, these assumptions
are still difficult to satisfy in real-life scenarios. In addition, similar to UDA, TTA methods generally
require a model trained well on the source domain. In contrast, FDM-SSL setting only requires scarce
labeled data and has no restrictions on the distribution of unlabeled data."
PROBLEM SETTING,0.07731958762886598,"3
Problem Setting"
PROBLEM SETTING,0.08247422680412371,"For Feature Distribution Mismatch SSL (FDM-SSL) problem, a model observes a labeled set
Dl = {(xi, yi)}Nl
i=1 and an unlabeled set Du = {(uj)}Nu
j=1 with Nu â‰«Nl, where xi and ui are input
images and yi is the label of xi. The labeled image xi and unlabeled image uj are drawn from two
different feature distributions pl(x) and pu(x), respectively. Note that different from previous works,
in FDM-SSL setting the unlabeled samples may come from a mixture of multiple distributions rather
than just one distribution: i.e. pu(x) = w0pl(x) + PK
k=1 wkpk
u(x), where w0 and wk represent the
weights of the labeled distribution pl(x) and the kâˆ’th unlabeled distribution pk
u(x) respectively. The
goal of FDM-SSL is to train a model that generalizes well over a large range of varying test data
distributions, including labeled, unlabeled and even distributions unseen during training (punseen(x)).
The differences between traditional SSL, UDA, TTA and FDM-SSL, are summarized in Table 1."
PROBLEM SETTING,0.08762886597938144,"The core of FDM-SSL is to enhance the utilization of unlabeled samples in the presence of feature
distribution mismatch. On one hand, the shared information, such as patterns and structures, in
the unlabeled samples can provide effective cues for model adaptation. On the other hand, these
mismatched unlabeled samples can facilitate the learning of a more robust model by exposing it to a
wide range of data distributions."
METHOD,0.09278350515463918,"4
Method"
OVERVIEW,0.0979381443298969,"4.1
Overview"
OVERVIEW,0.10309278350515463,"In the FDM-SSL setting, due to the feature distribution shift from pl(x) to pu(x), directly applying
the SSL model fitted on labeled data to unlabeled data may lead to massive inaccurate predictions on
the unlabeled data, thereby aggravating confirmation bias and impairing SSL learning."
OVERVIEW,0.10824742268041238,"This motivates us to propose Self-Supervised Feature Adaptation (SSFA), a unified framework
for FDM-SSL, which decouples the pseudo-label predictions from the current model to address
distribution mismatch. As illustrated in Figure 1, SSFA consists of two modules: a semi-supervised"
OVERVIEW,0.1134020618556701,"learning module and a feature adaptation module. The semi-supervised learning module incorporates
a pseudo-label-based SSL learner with a self-supervised auxiliary task that shares a portion of feature
extractor parameters with the main classification task. [15] has pointed out that the main task can be
indirectly optimized by optimizing the auxiliary task. To this end, the feature adaptation module is
designed to update the current model through self-supervised learning on unlabeled data, so as to
better match the unlabeled distribution and predict higher-quality pseudo-labels for semi-supervised
learning. Therefore, by optimizing the auxiliary self-supervised task individually, the updated model
can make more accurate classification decisions in the main task. ð‘¥
ð‘¢!"
OVERVIEW,0.11855670103092783,Initialized Weights
OVERVIEW,0.12371134020618557,"Semi-Supervised Learning Module ð“›ð’™ ð‘¢! ""ð’’â€² ð“›ð’‚ð’–ð’™"
OVERVIEW,0.12886597938144329,"Feature Adaptation Module ð“›ð’‚ð’‘ð’• ðœ½ð’„ ðœ½ð’” ðœ½ð’ˆ "" ðœ½ð’„ ðœ½ð’” X ð“›ð’–"
OVERVIEW,0.13402061855670103,stop gradient
OVERVIEW,0.13917525773195877,"ð‘¥, ð‘¢!, ð‘¢"" ð‘¥ ð‘¢"" ðœ½ð’ˆ
ð‘¢$"
OVERVIEW,0.14432989690721648,"Figure 1: The pipeline of SSFA. Let x, uw and us denote a batch of the labeled data, the weak
augmentation and the strong augmentation of unlabeled data respectively, {Â·} represent the data
stream."
SEMI-SUPERVISED LEARNING MODULE,0.14948453608247422,"4.2
Semi-Supervised Learning Module"
SEMI-SUPERVISED LEARNING MODULE,0.15463917525773196,"In the Semi-Supervised Learning Module, we introduce a self-supervised task as an auxiliary task,
which is optimized together with the main task. As shown in Figure 1, the network parameter Î¸
comprises three parts: Î¸g for the shared encoder, Î¸c for the main task head, and Î¸s for the auxiliary
task head. During training, SSL module optimizes a supervised loss Lx, an unsupervised loss Lu
and a self-supervised auxiliary loss Laux, simultaneously. Typically, given a batch of labeled data
{(xb, yb)}B
b=1 with size B and a batch of unlabeled data {(ub)}ÂµB
b=1 with size ÂµB, where Âµ is the ratio
of unlabeled data to labeled data, Lx applies standard cross-entropy loss on labeled examples:"
SEMI-SUPERVISED LEARNING MODULE,0.15979381443298968,"Lx = 1 B B
X"
SEMI-SUPERVISED LEARNING MODULE,0.16494845360824742,"b=1
H(xb, yb; Î¸g, Î¸c),
(1)"
SEMI-SUPERVISED LEARNING MODULE,0.17010309278350516,"where H (Â·, Â·) is the cross-entropy function. Different SSL learners [30, 5] may design different Lu.
One of the widely used is the pseudo-label loss.In particular, given the pseudo-label qb for each
unlabeled input ub, Lu in traditional SSL can be formulated as:"
SEMI-SUPERVISED LEARNING MODULE,0.17525773195876287,"Lu =
1
ÂµB ÂµB
X"
SEMI-SUPERVISED LEARNING MODULE,0.18041237113402062,"b=1
â„¦(ub, qb; Î¸g, Î¸c),
(2)"
SEMI-SUPERVISED LEARNING MODULE,0.18556701030927836,"where â„¦is the per-sample supervised loss, e.g., mean-square error [6] and cross-entropy loss [30]."
SEMI-SUPERVISED LEARNING MODULE,0.19072164948453607,"Furthermore, we introduce a self-supervised learning task to the SSL module for joint training to
optimize the parameter of the auxiliary task head Î¸s. To learn from labeled and unlabeled data
distributions simultaneously, it is necessary to use all samples from both distributions for training.
Therefore, Laux can be formulated as:"
SEMI-SUPERVISED LEARNING MODULE,0.1958762886597938,"Laux =
1
(2Âµ + 1)B B
X"
SEMI-SUPERVISED LEARNING MODULE,0.20103092783505155,"b=1
â„“s(xb; Î¸g, Î¸s) + ÂµB
X"
SEMI-SUPERVISED LEARNING MODULE,0.20618556701030927,"b=1
â„“s(Aw(ub); Î¸g, Î¸s) + ÂµB
X"
SEMI-SUPERVISED LEARNING MODULE,0.211340206185567,"b=1
â„“s(As(ub); Î¸g, Î¸s) ! ,"
SEMI-SUPERVISED LEARNING MODULE,0.21649484536082475,"(3)
where â„“s denotes the self-supervised loss function, Aw and As denote the weak and strong augmen-
tation functions respectively. In order to maintain consistent optimization between the main task"
SEMI-SUPERVISED LEARNING MODULE,0.22164948453608246,"and auxiliary task, it is necessary to optimize both tasks jointly. So we add Laux to semi-supervised
learning module and the final object is:"
SEMI-SUPERVISED LEARNING MODULE,0.2268041237113402,"LSSFA = Lx + Î»uLu + Î»aLaux,
(4)"
SEMI-SUPERVISED LEARNING MODULE,0.23195876288659795,where Î»u and Î»a are hyper-parameters denoting the relative weights of Lu and Laux respectively.
FEATURE ADAPTATION MODULE,0.23711340206185566,"4.3
Feature Adaptation Module"
FEATURE ADAPTATION MODULE,0.2422680412371134,"Some classic SSL approaches [30, 6, 5] directly use the outputs of the current classifier as pseudo-
labels. In particular, [30] applies weak and strong augmentations to unlabeled samples and generates
pseudo-labels using the modelâ€™s predictions on weakly augmented unlabeled samples, i.e., qb =
pm (y|Aw (ub) ; Î¸g, Î¸c), where pm (y|u; Î¸) denotes the predicted class distribution of the model Î¸
given unlabeled image u. However, as the classification model (Î¸g, Î¸c) is mainly fitted to the labeled
distribution, the prediction on Aw(ub) is usually inaccurate under distribution mismatch between the
labeled and unlabeled samples."
FEATURE ADAPTATION MODULE,0.24742268041237114,"To alleviate this problem, we design a Feature Adaptation Module to adapt the model to the unlabeled
data distribution before making predictions, thereby producing more reliable pseudo-labels for
optimizing the unsupervised loss Lu. More specifically, before making pseudo-label predictions, we
firstly fine-tune the shared feature extractor Î¸g by minimizing the self-supervised auxiliary task loss
on unlabeled samples:"
FEATURE ADAPTATION MODULE,0.25257731958762886,"Lapt =
1
ÂµB ÂµB
X"
FEATURE ADAPTATION MODULE,0.25773195876288657,"b=1
â„“s(Aw(ub); Î¸g, Î¸s).
(5)"
FEATURE ADAPTATION MODULE,0.26288659793814434,"Here Î¸g is updated to Î¸â€²
g = arg min Lapt. Notably, since excessive adaptation may lead to deviation
in the optimization direction and largely increase calculation costs, we only perform one-step
optimization in the adaptation stage."
FEATURE ADAPTATION MODULE,0.26804123711340205,"After self-supervised adaptation, we use
 
Î¸â€²
g, Î¸c

to generate the updated prediction, which can be
denoted as:
qâ€²
b = pm(y|Aw(ub); Î¸â€²
g, Î¸c).
(6)"
FEATURE ADAPTATION MODULE,0.27319587628865977,"We convert qâ€²
b to the hard â€œone-hotâ€ label Ë†qâ€²
b and denote Ë†qâ€²
b as the pseudo-label for the corresponding
strongly augmented unlabeled sample Aw(ub). After that, Î¸â€²
g will be discarded without influencing
other model parts during training. In the end, Lu in the SSL module (Equation 4) can then be
computed with cross-entropy loss as:"
FEATURE ADAPTATION MODULE,0.27835051546391754,"Lu =
1
ÂµB ÂµB
X"
FEATURE ADAPTATION MODULE,0.28350515463917525,"b=1
I(max(qâ€²
b) > Ï„)H(As(ub), Ë†qâ€²
b; Î¸g, Î¸c).
(7)"
THEORETICAL INSIGHTS,0.28865979381443296,"4.4
Theoretical Insights"
THEORETICAL INSIGHTS,0.29381443298969073,"In our framework, we jointly train the model with a self-supervised task with loss function â„“s , and a
main task with loss function â„“m. Let h âˆˆH be a feasible hypothesis and Du = {(ui, yu
i )}N
i=1 be the
unlabeled dataset. Note that the ground-truth label yu
i of ui is actually unavailable, but just used for
analysis."
THEORETICAL INSIGHTS,0.29896907216494845,"Lemma 1 ([31]) Assume that for all x, y, â„“m(x, y; h) is differentiable, convex and Î²-smooth in h,
and both âˆ¥âˆ‡â„“m(x, y; h)âˆ¥, âˆ¥âˆ‡â„“s(x; h)âˆ¥â‰¤G for all h âˆˆH. With a fixed learning rate Î· =
Ïµ
Î²G2 , for
every x, y such that âŸ¨âˆ‡â„“m(x, y; h), âˆ‡â„“s(x; h)âŸ©> Ïµ, we have"
THEORETICAL INSIGHTS,0.30412371134020616,"â„“m(x, y; hâ€²) < â„“m(x, y; h),
(8)"
THEORETICAL INSIGHTS,0.30927835051546393,"where hâ€² is the updated hypothesis, namely hâ€² = h âˆ’Î·âˆ‡â„“s(x, y; h)."
THEORETICAL INSIGHTS,0.31443298969072164,"Let Ë†Em(h, Du) =
1
K
PK
i=1 â„“m(xi
u, yi
u; h) denote the empirical risk of the main task on Du, and
Ë†Es(h, Du) = 1"
THEORETICAL INSIGHTS,0.31958762886597936,"K
PK
i=1 â„“s(xi
u; h) denote the empirical risk of the self-supervised task on Du. Under
the premise of Lemma 1 and the following sufficient condition, with a fixed learning rate Î·:"
THEORETICAL INSIGHTS,0.3247422680412371,"âŸ¨âˆ‡Ë†Em(h, Du), âˆ‡Ë†Es(h, Du)âŸ©> Ïµ,
(9)"
THEORETICAL INSIGHTS,0.32989690721649484,"we can get that:
Ë†Em(hâ€², Du) < Ë†Em(h, Du),
(10)"
THEORETICAL INSIGHTS,0.33505154639175255,"where hâ€² is the updated hypothesis, namely hâ€² = h âˆ’Î·âˆ‡Ë†Es(h, Du)."
THEORETICAL INSIGHTS,0.3402061855670103,"0
1
2
3
4
5
6
Gradient inner product 10 0 10 20 30 40 50 60"
THEORETICAL INSIGHTS,0.34536082474226804,"Figure 2: Scatter plot of the gradient inner
product between the two tasks, and the im-
provement from SSFA. We transform the
x-axis with log(x) + 1 for clarity."
THEORETICAL INSIGHTS,0.35051546391752575,"Therefore, in the smooth and convex case, the empir-
ical risk of the main task Ë†Em can theoretically tend to
0 by optimizing the empirical risk of self-supervised
task Ë†Es. Thus, in our feature adaptation module, by
optimizing the self-supervised loss for unlabeled sam-
ples, we can indirectly optimize the main loss, thereby
mitigating confirmation bias and making full use of
the unlabeled samples. As above analysis, the gradient
correlation plays a determining factor in the success of
optimizing Ë†Em through Ë†Es in the smooth and convex
case. For non-convex deep loss functions, we provide
empirical evidence to show that our theoretical insights
also hold. Figure 2 plots the correlation between the
gradient inner product (of the main and auxiliary tasks)
and the performance improvement of the model on the
test set, where each point in the figure represents the average result of a set of test samples. In
Figure 2, we observe that there is a positive correlation between the gradient inner product and
model performance improvement for non-convex loss functions on the deep learning model. This
phenomenon is consistent with the theoretical conclusion, that is, a stronger gradient correlation
indicates a higher performance improvement."
EXPERIMENTS,0.3556701030927835,"5
Experiments"
EXPERIMENTS,0.36082474226804123,"In this section, we evaluate our proposed SSFA framework on various datasets, where labeled and
unlabeled data are sampled from different distributions. To this end, we consider two different
distribution mismatch scenarios: image corruption and style change, in the following experiments.
More experimental details and results are provided in the Appendix."
EXPERIMENTAL SETTING,0.36597938144329895,"5.1
Experimental Setting"
EXPERIMENTAL SETTING,0.3711340206185567,"Datasets. For the image corruption experiments, we create the labeled domain by sampling images
from CIFAR100 [20], and the unlabeled domain by sampling images from a mixed dataset consisting
of CIFAR100 and CIFAR100 with Corruptions (CIFAR100-C). The CIFAR100-C is constructed
by applying various corruptions to the original images in CIFAR100, following the methodology
used for ImageNet-C [18]. We use ten types of corruption as training corruptions while reserving the
other five corruptions as unseen corruptions for testing. The proportion of unlabeled samples from
CIFAR100-C was controlled by the hyper-parameter ratio. For the style change experiments, we use
OFFICE-31 [27] and OFFICE-HOME [36] benchmarks. Specifically, we designate one domain in
each dataset as the labeled domain, while the unlabeled domain comprises either another domain or a
mixture of multiple domains."
EXPERIMENTAL SETTING,0.37628865979381443,"Implementation Details. In corruption experiments, we use WRN-28-8 [48] as the backbone except
for [6] where WRN-28-2 is used to prevent training collapse. In style experiments, we use ResNet-50
[17] pre-trained on ImageNet [14] as the backbone for OFFICE-31 and OFFICE-HOME. To ensure
fairness, we use the same hyperparameters for different methods employed in our experiments."
EXPERIMENTAL SETTING,0.38144329896907214,"Comparison Methods. We compare our method with three groups of methods: (1) Supervised
method as a baseline to show the effectiveness of training with unlabeled data.(2) Classical UDA
methods including DANN [1] and CDAN [24]; and (3) Popular SSL methods including MixMatch
[6], ReMixMatch[5], FixMatch [30], FM-Rot (joint training with rotation prediction task), FM-Pre
(pre-trained by rotation prediction tasks), FreeMatch [41], SoftMatch [9], and Adamatch [7]."
EXPERIMENTAL SETTING,0.3865979381443299,"Evaluation Protocols. We evaluate the performance across labeled, unlabeled and unseen distribu-
tions to verify the general applicability of our framework. Therefore, we consider three evaluation
protocols: (1) Label-Domain Evaluation (L): test samples are drawn from labeled distribution, (2)"
EXPERIMENTAL SETTING,0.3917525773195876,Table 2: Comparison of accuracy (%) for Feature Distribution Mismatch SSL on CIFAR100.
EXPERIMENTAL SETTING,0.39690721649484534,"Method
400 labeled
4000 labeled
10000 labeled"
EXPERIMENTAL SETTING,0.4020618556701031,"ratio 0.5
ratio 1.0
ratio 1.0
ratio 1.0"
EXPERIMENTAL SETTING,0.4072164948453608,"L
UL
US
L
UL
US
L
UL
US
L
UL
US"
EXPERIMENTAL SETTING,0.41237113402061853,"Supervised
10.6
8.9
6.9
10.6
8.9
6.9
48.0 17.0 24.0 61.6 20.8 30.8
DANN [1]
11.3
9.3
7.0
11.4
9.3
7.0
46.7 30.8 27.6 61.4 42.7 37.8
CDAN [24]
11.6
9.5
7.3
11.6 10.0
7.5
47.1 31.3 28.8 62.6 41.1 39.1
MixMatch [6]
10.7
3.6
5.4
15.4
2.5
6.1
46.4 10.9 21.1 60.3 31.4 37.6
AdaMatch [7]
6.8
4.6
1.3
6.0
4.5
2.1
19.1
6.4
1.8
26.7
9.0
1.6"
EXPERIMENTAL SETTING,0.4175257731958763,"ReMixMatch [5]
39.6 20.6 32.7 39.2 19.1
3.1
68.4 35.8 58.1 75.5 40.2 63.2
RM-SSFA (ours)
43.1 22.9 35.1 43.0 23.1 35.1 71.0 37.9 60.8 76.4 41.6 63.5"
EXPERIMENTAL SETTING,0.422680412371134,"FixMatch [30]
25.8
4.7
16.5 15.7
3.5
8.5
53.0 16.0 39.0 65.3 33.0 52.8
FM-SSFA (ours)
37.0 23.2 25.8 25.7 22.2 22.5 60.2 52.5 51.2 69.1 57.8 58.0"
EXPERIMENTAL SETTING,0.42783505154639173,"FreeMatch [41]
35.0
1.4
21.0 17.4
2.2
7.2
55.2 16.3 41.4 67.0 23.9 53.4
FreeM-SSFA (ours) 37.6 25.1 31.4 27.5 18.7 21.9 62.0 54.2 53.0 70.2 61.8 60.0"
EXPERIMENTAL SETTING,0.4329896907216495,"SoftMatch [9]
35.5
2.9
24.1 19.4
4.6
12.8 58.3 29.7 48.9 68.3 34.7 56.3
SM-SSFA (ours)
40.8 29.5 34.8 31.1 22.4 25.2 62.6 54.4 53.8 70.3 61.8 59.9"
EXPERIMENTAL SETTING,0.4381443298969072,"UnLabel-Domain Evaluation (UL): test samples are drawn from unlabeled distribution, and (3)
UnSeen-Domain Evaluation (US): test samples are drawn from unseen distribution."
MAIN RESULTS,0.44329896907216493,"5.2
Main Results"
MAIN RESULTS,0.4484536082474227,"Image Corruption. Our SSFA framework is a generic framework that can be easily integrated with
existing pseudo-label-based SSL methods. In this work, we combine SSFA with four SSL methods:
FixMatch, ReMixMatch, FreeMatch and SoftMatch, denoted by FM-SSFA, RM-SSFA, FreeM-
SSFA and SM-SSFA, respectively. Table 2 shows the compared results on the image corruption
experiment with different numbers of labeled samples and ratio. We can observe that (1) the UDA
methods, DANN and CDAN, exhibit very poor performance on labeled-domain evaluation (L).
This is because UDA methods aim to adapt the model to the unlabeled domain, which sacrifices
performance on the labeled domain. And the UDA methods are primarily designed to adapt to a
single unlabeled distribution, making it unsuitable for FDM-SSL scenarios. In contrast, our methods
largely outperform the two UDA methods on all evaluations. (2) The traditional SSL methods suffer
from significant performance degradation in the FDM-SSL setting, particularly in unlabeled-domain
evaluation (UL). And this degradation becomes more severe when the number of labeled data is small
and the proportion of unlabeled data with corruption is high, due to the increased feature mismatch
degree. Our methods largely outperform these SSL methods, validating the effectiveness of our
self-supervised feature adaptation strategy for FDM-SSL. (3) SSFA largely improves the performance
of original SSL methods on all three evaluations. The gains on unlabeled-domain evaluation (UL)
indicate that SSFA can generate more accurate pseudo-labels for unlabeled samples and reduce
confirmation bias. Moreover, the gains on unseen-domain evaluation (US) validate that SSFA can
enhance the modelâ€™s robustness and generalization."
MAIN RESULTS,0.4536082474226804,"Style Change. We conduct experiments on OFFICE-31 and OFFICE-HOME benchmarks to evaluate
the impact of style change on SSL methods. Specifically, we evaluate SSL methods in two scenarios
where unlabeled data is sampled from a single distribution and a mixed distribution. The results are
summarized in Table 3. Compared to the supervised method, most SSL methods present significant
performance degradation. One possible explanation is that style change causes an even greater feature
distribution mismatch compared to image corruption, resulting in more erroneous predictions for
unlabeled data. As shown in Table 3(a), our method shows consistent performance improvement
over existing UDA and SSL methods on OFFICE-31. In some cases, such as ""W/A"", the task itself
may be relatively simple, so using only label data can already achieve high accuracy. The results on
OFFICE-HOME are summarized in Table 3(b). To demonstrate that our method can indeed improve
the accuracy of pseudo-labels, we add an evaluation protocol UU, which indicates the pseudo-labels
accuracy of selected unlabeled samples. As shown in Table 3(b), compared to existing UDA and SSL
methods, our approach achieves the best performance on all setups and significantly improves the
accuracy of pseudo-labels on unlabeled data. The results provide experimental evidence that supports
the theoretical analysis in Section 4.4."
MAIN RESULTS,0.4587628865979381,"Table 3: Comparison of accuracy (%) for Feature Distribution Mismatch SSL on OFFICE-31 and
OFFICE-HOME."
MAIN RESULTS,0.4639175257731959,(a) Results on OFFICE-31
MAIN RESULTS,0.4690721649484536,"Method
Single Domain
Multiple Domains"
MAIN RESULTS,0.4742268041237113,"A/D
A/W
D/A
W/A
A/DW
D/AW"
MAIN RESULTS,0.4793814432989691,"L
UL
US
L
UL
US
L
UL
US
L
UL
US
L
UL
L
UL"
MAIN RESULTS,0.4845360824742268,"supervised
65.9
57.1
52.0
65.9
47.8
61.0
93.0
56.8
84.9
93.6
56.2
92.4
68.0
51.9
93.3
60.7
DANN [1]
53.9
17.9
20.1
15.3
7.2
4.6
74.7
4.8
16.7
2.5
2.9
4.2
28.1
7.6
4.7
2.8
CDAN [24]
2.9
5.6
2.6
2.9
2.3
4.2
4.7
2.9
2.6
41.3
7.7
18.5
2.9
3.2
45.5
10.7
FixMatch [30]
58.4
46.9
53.3
56.3
9.5
19.3
82.8
4.7
50.6
78.3
9.4
62.6
55.0
48.8
80.2
40.3
FM-Rot
60.0
53.1
57.1
54.6
27.4
19.3
70.8
29.5
50.6
75.9
17.9
66.9
54.0
40.1
63.0
26.9"
MAIN RESULTS,0.4896907216494845,"FM-SSFA
67.5
61.7
63.0
66.4
56.5
62.5
93.0
59.3
85.9
93.0
56.4
92.8
69.1
60.8
92.4
61.8"
MAIN RESULTS,0.4948453608247423,(b) Results on OFFICE-HOME
MAIN RESULTS,0.5,"Method
A/CPR
C/APR
P/ACR
A/ACPR
C/ACPR
R/ACPR"
MAIN RESULTS,0.5051546391752577,"L
UL
UU
L
UL
UU
L
UL
UU
L
UL
UU
L
UL
UU
L
UL
UU"
MAIN RESULTS,0.5103092783505154,"supervised
48.4 39.9
-
40.9 31.2
-
68.4 36.5
-
49.3 43.0
-
41.0 36.3
-
61.6 46.8
-
DANN [1]
45.0 37.8
-
31.8
7.7
-
53.9 25.6
-
49.3 42.0
-
30.5 20.2
-
44.5 32.1
-
CDAN [24]
30.4 22.8
-
1.2
1.8
-
63.0 34.3
-
4.4
4.2
-
10.5
6.9
-
13.5 10.2
-
FixMatch [30] 27.2 17.7 19.3 28.1 19.6 19.4 55.6 24.9 30.6 32.4 23.0 31.0 36.9 30.6 36.0 42.2 31.5 35.4
FM-Pre
21.2 15.0 15.4 28.6 22.9 25.4 51.9 22.0 26.3 30.6 20.1 24.8 37.9 33.8 41.7 33.4 23.8 24.9
FM-Rot
1.7
2.1
0.0
33.4 20.1
0.7
43.4 14.6 17.2 25.6 17.7 21.3 37.3 31.0 33.2 45.6 34.7 42.3"
MAIN RESULTS,0.5154639175257731,"FM-SSFA
50.7 44.0 69.0 45.1 37.6 66.2 70.6 41.9 70.5 55.0 45.5 73.2 44.7 41.7 66.0 64.8 52.7 80.3"
FEATURE VISUALIZATION,0.520618556701031,"5.3
Feature Visualization"
FEATURE VISUALIZATION,0.5257731958762887,"Figure 3 visualizes the domain-level features generated by SSL models with/without SSFA respec-
tively. In Figure 3 (a), the vanilla FixMatch model maps labeled and unlabeled samples to different
clusters in the feature spaces without fusing samples from different domains. Conversely, Figure
3 (b) shows that our FM-SSFA model can effectively fuse these samples. We further compute the
A-distance metric [4] to measure the distributional divergence between the labeled and unlabeled
distributions. The A-distance of FM-SSFA (0.91) is lower than that of FixMatch (1.10), verifying the
implicit distribution alignment of SSFA."
FEATURE VISUALIZATION,0.5309278350515464,"Additionally, Figure 4 visualizes the class-level features generated by SSL models with/without SSFA
respectively. In Figure 4 (a), FixMatch fails to distinguish samples from different classes. In contrast,
FM-SSFA can separate samples from different categories well, as shown in Figure 4 (b). This result
indicates that SSFA can help to extract more discriminative features."
FEATURE VISUALIZATION,0.5360824742268041,"(a) FixMatch
(b) FM-SSFA"
FEATURE VISUALIZATION,0.5412371134020618,"label
unlabel0
unlabel1
unlabel2
unlabel3
unlabel4
unlabel5
unlabel6
unlabel7
unlabel8
unlabel9"
FEATURE VISUALIZATION,0.5463917525773195,"Figure 3: Visualization of domain-level features using different
methods, where ""label"" represents the labeled data drawn from
the labeled domain, and ""unlabel0"" to ""unlabel9"" represent the
unlabeled data drawn from ten unlabeled domains respectively."
FEATURE VISUALIZATION,0.5515463917525774,"(a) FixMatch
(b) FM-SSFA"
FEATURE VISUALIZATION,0.5567010309278351,"class0
class1
class2
class3"
FEATURE VISUALIZATION,0.5618556701030928,"Figure 4: Visualization of class-
level features using different meth-
ods."
ABLATION STUDY,0.5670103092783505,"5.4
Ablation Study"
ABLATION STUDY,0.5721649484536082,"Combined with different self-supervised tasks. Table 4 compares different self-supervised tasks
combined with our SSFA framework on the image corruption experiment. We employ different
self-supervised losses corresponding to the rotation prediction task, contrastive learning task, and"
ABLATION STUDY,0.5773195876288659,"Table 4: Combined with different self-supervised tasks. â€œRotâ€, â€œSimClrâ€, and â€œEMâ€ represent the
rotation prediction task, contrastive learning task [11] and entropy minimization task [38] respectively."
ABLATION STUDY,0.5824742268041238,"Method
400 labeled
4000 labeled 10000 labeled"
ABLATION STUDY,0.5876288659793815,"ratio 0.5
ratio 1.0
ratio 1.0
ratio 1.0"
ABLATION STUDY,0.5927835051546392,"L
UL US
L
UL US
L
UL US
L
UL
US"
ABLATION STUDY,0.5979381443298969,"FixMatch[30]
25.8 4.7 16.5 15.7 3.5 8.5 53.0 16.0 39.0 65.3 33.0 52.8
FM-SSFA (Rot)
37.0 23.2 25.8 25.7 22.2 22.5 60.2 52.5 51.2 69.1 57.8 58.0
FM-SSFA (SimClr) 22.8 13.7 17.4 19.2 13.6 14.1 55.3 43.8 45.1 66.2 53.6 52.9
FM-SSFA (EM)
22.9 14.5 17.2 18.0 15.9 14.6 55.2 44.2 44.7 66.7 52.5 52.6"
ABLATION STUDY,0.6030927835051546,"entropy minimization task, which are the cross entropy loss, the contrastive loss from SimCLR
and the entropy loss, respectively. As shown, different self-supervised tasks can bring significant
performance gains. And the rotation prediction task leads to the largest improvements compared
to the other two self-supervised tasks. This can be attributed to the more optimal parameters and
the direct supervision signals in rotation prediction. In the experiments, we employ the rotation
prediction task as the default self-supervised task."
ABLATION STUDY,0.6082474226804123,"The effectiveness of Feature Adaptation Module. To evaluate the effectiveness of the Feature
Adaptation Module in the SSFA framework, we compare FM-SSFA with a baseline method FM-Rot,
where we remove the Feature Adaptation Module and only add the self-supervised rotation prediction
task. As shown in Table 3(a) and 3(b), FM-SSFA largely outperforms FM-Rot on OFFICE-31 and
OFFICE-HOME, especially in the UL evaluation metric. The superiority of FM-SSFA highlights
that the Feature Adaptation Module helps the model to adapt to unlabeled samples from different
distributions, resulting in better generalization on unlabeled domain. Moreover, we can observe the
FM-Rot is only marginally better than FixMatch and, in some cases, may even perform worse. These
results suggest that simply integrating the self-supervised task into SSL methods only brings limited
performance gains and may even be detrimental in some scenarios."
ABLATION STUDY,0.6134020618556701,"Table 5: The performance of SSFA when the
distribution shift between labeled and unlabeled
data is small on CIFAR100."
ABLATION STUDY,0.6185567010309279,"Method
ratio 0.0
ratio 0.1"
ABLATION STUDY,0.6237113402061856,"L/UL US
L
UL US"
ABLATION STUDY,0.6288659793814433,"FixMatch [30] 33.3 25.8 31.7 10.5 24.8
FM-SSFA
41.3 33.0 41.2 15.8 32.9"
ABLATION STUDY,0.634020618556701,"Small distribution shift between labeled and un-
labeled data. To demonstrate the robustness of our
proposed method, we evaluate SSFA on scenarios
where there is a small distribution shift between
labeled and unlabeled data. Specially, we conduct
experiments on two setups: ratio=0.0 (degrade
into traditional SSL) and ratio=0.1. Table 5 shows
our method can still bring significant improvements
over the baseline. This indicates that our proposed
SSFA framework is robust to various SSL scenarios
even under a low noise ratio."
ABLATION STUDY,0.6391752577319587,"0.5
0.6
0.7
0.8
0.9
Threshold 20 40 60 80 100"
ABLATION STUDY,0.6443298969072165,Accuracy
ABLATION STUDY,0.6494845360824743,"FM(L)
FM(UL)
FM(UU)
FM-SSFA(L)
FM-SSFA(UL)
FM-SSFA(UU)"
ABLATION STUDY,0.654639175257732,"Figure 5: The impact of Ï„ for different SSL
models on OFFICE-31 (â€œA/Wâ€ task)."
ABLATION STUDY,0.6597938144329897,"Robustness to confident threshold Ï„. Figure 5
illustrates the impact of confident threshold Ï„ for
different SSL models on OFFICE-31. As shown
in Figure 5, the vanilla FixMatch model is highly
sensitive to the values of Ï„. For example, when Ï„
is set to 0.95, FixMatch achieves relatively high
performance with confident enough pseudo-labels.
When Ï„ is lowered to 0.85, the performance of
FixMatch on UL and UU degrades significantly,
indicating that the model has deteriorated by too
many wrong pseudo-labels. In contrast, FM-SSFA
is more robust to different values of Ï„."
ABLATION STUDY,0.6649484536082474,"The number of shared layers between auxiliary and main task. Table 6 analyzes the impact of
different numbers of shared layers in the shared feature extractor of the auxiliary and main task. As
shown, the performance difference is negligible between sharing 2 and 3 layers in the feature extractor,
while a significant performance degradation happens if we share all layers (4 layers) of the feature
extractor. We argue that this is because too many shared parameters of the feature extractor may lead
to over-adaptation of the feature extractor and compromise of the main task, thus the predictions of"
ABLATION STUDY,0.6701030927835051,"Table 6: The impact of shared layers between auxiliary and main task on OFFICE-HOME. â€œX layersâ€
denotes the number of shared layers for the feature extractor."
ABLATION STUDY,0.6752577319587629,"Method
A/CPR
C/APR
P/ACR
R/ACP"
ABLATION STUDY,0.6804123711340206,"L
UL
UU
L
UL
UU
L
UL
UU
L
UL
UU"
ABLATION STUDY,0.6855670103092784,"FM-SSFA (2 layers) 48.9 39.9 70.8 46.0 39.5 69.0 70.3 42.0 67.3 61.0 40.8 67.0
FM-SSFA (3 layers) 47.5 39.8 71.0 45.4 39.0 77.6 72.3 45.4 74.4 63.4 43.9 76.8
FM-SSFA (4 layers) 44.4 39.1 57.1
1.3
2.4
0.0
57.9 33.4 59.5 40.3 28.7 54.3"
ABLATION STUDY,0.6907216494845361,"pseudo-labels may be more erroneous. In the experiments, we set the number of shared layers to 2 by
default."
CONCLUSION,0.6958762886597938,"6
Conclusion"
CONCLUSION,0.7010309278350515,"In this paper, we focus on a realistic SSL setting, FDM-SSL, involving a mismatch between the
labeled and unlabeled distributions, complex mixed unlabeled distributions and widely unknown test
distributions. The primary challenge lies in the scarcity of labeled data and the potential presence of
mixed distributions within the unlabeled data. To address this challenge, we propose a generalized
framework SSFA, which introduces a self-supervised task to adapt the feature extractor to the
unlabeled distribution. By incorporating this self-supervised adaptation, the model can improve the
accuracy of pseudo-labels to alleviate confirmation bias, thereby enhancing the generalization and
robustness of the SSL model under distribution mismatch."
CONCLUSION,0.7061855670103093,"Broader Impacts and Limitations. The new problem setting takes into account the modelâ€™s
generalization across different distributions, which is essential for expanding the application of SSL
methods in real-world scenarios. Additionally, SSFA serves as a simple yet effective framework
that can be seamlessly integrated with any pseudo-label-based SSL methods, enhancing the overall
performance and adaptability of these models. However, the performance of SSFA is affected by the
shared parameters between the main task and the auxiliary task. We hope that SSFA can attract more
future attention to explore the effectiveness of feature adaptation with the self-supervised task."
CONCLUSION,0.711340206185567,Acknowledgments
CONCLUSION,0.7164948453608248,"This work is partially supported by National Key R&D Program of China no. 2021ZD0111901,
National Natural Science Foundation of China (NSFC): 61976203, 62276246 and 62306301, and
National Postdoctoral Program for Innovative Talents under Grant BX20220310."
REFERENCES,0.7216494845360825,References
REFERENCES,0.7268041237113402,"[1] Hana Ajakan, Pascal Germain, Hugo Larochelle, FranÃ§ois Laviolette, and Mario Marchand.
Domain-adversarial neural networks. arXiv, 2014."
REFERENCES,0.7319587628865979,"[2] Gholamali Aminian, Mahed Abroshan, Mohammad Mahdi Khalili, Laura Toni, and Miguel
Rodrigues. An information-theoretical approach to semi-supervised learning under covariate-
shift. In AISTATS, 2022."
REFERENCES,0.7371134020618557,"[3] Eric Arazo, Diego Ortego, Paul Albert, Noel E. Oâ€™Connor, and Kevin McGuinness. Pseudo-
labeling and confirmation bias in deep semi-supervised learning. In IJCNN, 2020."
REFERENCES,0.7422680412371134,"[4] Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations
for domain adaptation. In NeurIPS, 2006."
REFERENCES,0.7474226804123711,"[5] David Berthelot, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang,
and Colin Raffel. Remixmatch: Semi-supervised learning with distribution matching and
augmentation anchoring. In ICLR, 2020."
REFERENCES,0.7525773195876289,"[6] David Berthelot, Nicholas Carlini, Ian J. Goodfellow, Nicolas Papernot, Avital Oliver, and Colin
Raffel. Mixmatch: A holistic approach to semi-supervised learning. In NeurIPS, 2019."
REFERENCES,0.7577319587628866,"[7] David Berthelot, Rebecca Roelofs, Kihyuk Sohn, Nicholas Carlini, and Alexey Kurakin.
Adamatch: A unified approach to semi-supervised learning and domain adaptation. In ICLR,
2022."
REFERENCES,0.7628865979381443,"[8] Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, and Dilip Krishnan.
Unsupervised pixel-level domain adaptation with generative adversarial networks. In CVPR,
2017."
REFERENCES,0.7680412371134021,"[9] Hao Chen, Ran Tao, Yue Fan, Yidong Wang, Jindong Wang, Bernt Schiele, Xing Xie, Bhiksha
Raj, and Marios Savvides.
Softmatch: Addressing the quantity-quality tradeoff in semi-
supervised learning. In ICLR, 2023."
REFERENCES,0.7731958762886598,"[10] Minghao Chen, Shuai Zhao, Haifeng Liu, and Deng Cai. Adversarial-learned loss for domain
adaptation. In AAAI, 2020."
REFERENCES,0.7783505154639175,"[11] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework
for contrastive learning of visual representations. In ICML, 2020."
REFERENCES,0.7835051546391752,"[12] Francesco Croce, Sven Gowal, Thomas Brunner, Evan Shelhamer, Matthias Hein, and Taylan
Cemgil. Evaluating the adversarial robustness of adaptive test-time defenses. In ICML, 2022."
REFERENCES,0.788659793814433,"[13] Mohammad Zalbagi Darestani, Jiayu Liu, and Reinhard Heckel. Test-time training can close
the natural distribution shift performance gap in deep learning based compressed sensing. In
ICML, 2022."
REFERENCES,0.7938144329896907,"[14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In CVPR, 2009."
REFERENCES,0.7989690721649485,"[15] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei A. Efros. Test-time training with masked
autoencoders. In NeurIPS, 2022."
REFERENCES,0.8041237113402062,"[16] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, FranÃ§ois
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural
networks. The journal of machine learning research, 17:2096â€“2030, 2016."
REFERENCES,0.8092783505154639,"[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In CVPR, 2016."
REFERENCES,0.8144329896907216,"[18] Dan Hendrycks and Thomas G. Dietterich. Benchmarking neural network robustness to common
corruptions and perturbations. In ICLR, 2019."
REFERENCES,0.8195876288659794,"[19] Zhuo Huang, Chao Xue, Bo Han, Jian Yang, and Chen Gong. Universal semi-supervised
learning. In NeurIPS, 2021."
REFERENCES,0.8247422680412371,"[20] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Technical
report, University of Totonto, 2009."
REFERENCES,0.8298969072164949,"[21] Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. In ICLR,
2017."
REFERENCES,0.8350515463917526,"[22] D.-H. Lee. Pseudo-label : The simple and efficient semi-supervised learning method for deep
neural networks. In ICML, 2013."
REFERENCES,0.8402061855670103,"[23] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I. Jordan. Learning transferable features
with deep adaptation networks. In ICML, 2015."
REFERENCES,0.845360824742268,"[24] Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I. Jordan. Conditional adversarial
domain adaptation. In NeurIPS, 2018."
REFERENCES,0.8505154639175257,"[25] Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, and Philip S. Yu. Transfer
feature learning with joint distribution adaptation. In ICCV, 2013."
REFERENCES,0.8556701030927835,"[26] Prabhu Teja S and FranÃ§ois Fleuret. Test time adaptation through perturbation robustness. In
NeurIPS, 2021."
REFERENCES,0.8608247422680413,"[27] Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models to
new domains. In ECCV, 2010."
REFERENCES,0.865979381443299,"[28] Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic trans-
formations and perturbations for deep semi-supervised learning. In NeurIPS, 2016."
REFERENCES,0.8711340206185567,"[29] Swami Sankaranarayanan, Yogesh Balaji, Carlos Domingo Castillo, and Rama Chellappa.
Generate to adapt: Aligning domains using generative adversarial networks. In CVPR, 2018."
REFERENCES,0.8762886597938144,"[30] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin Raffel,
Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li.
Fixmatch: Simplifying semi-
supervised learning with consistency and confidence. In NeurIPS, 2020."
REFERENCES,0.8814432989690721,"[31] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A. Efros, and Moritz Hardt. Test-time
training with self-supervision for generalization under distribution shifts. In ICML, 2020."
REFERENCES,0.8865979381443299,"[32] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged
consistency targets improve semi-supervised deep learning results. In ICLR (Workshop), 2017."
REFERENCES,0.8917525773195877,"[33] Kaibin Tian, Qijie Wei, and Xirong Li. Co-teaching for unsupervised domain adaptation and
expansion. arXiv, 2022."
REFERENCES,0.8969072164948454,"[34] Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across
domains and tasks. In ICCV, 2015."
REFERENCES,0.9020618556701031,"[35] Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain
confusion: Maximizing for domain invariance. arXiv, 2014."
REFERENCES,0.9072164948453608,"[36] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan.
Deep hashing network for unsupervised domain adaptation. In CVPR, 2017."
REFERENCES,0.9123711340206185,"[37] Riccardo Volpi, Pietro Morerio, Silvio Savarese, and Vittorio Murino. Adversarial feature
augmentation for unsupervised domain adaptation. In CVPR, 2018."
REFERENCES,0.9175257731958762,"[38] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent:
Fully test-time adaptation by entropy minimization. In ICLR, 2021."
REFERENCES,0.9226804123711341,"[39] Jie Wang, Kaibin Tian, Dayong Ding, Gang Yang, and Xirong Li. Unsupervised domain expan-
sion for visual categorization. ACM Transactions on Multimedia Computing, Communications,
and Applications, 17:1â€“24, 2021."
REFERENCES,0.9278350515463918,"[40] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation.
In CVPR, 2022."
REFERENCES,0.9329896907216495,"[41] Yidong Wang, Hao Chen, Qiang Heng, Wenxin Hou, Yue Fan, Zhen Wu, Jindong Wang,
Marios Savvides, Takahiro Shinozaki, Bhiksha Raj, Bernt Schiele, and Xing Xie. Freematch:
Self-adaptive thresholding for semi-supervised learning. In ICLR, 2023."
REFERENCES,0.9381443298969072,"[42] Guoqiang Wei, Cuiling Lan, Wenjun Zeng, and Zhibo Chen. Metaalign: Coordinating domain
alignment and classification for unsupervised domain adaptation. In CVPR, 2021."
REFERENCES,0.9432989690721649,"[43] Qizhe Xie, Zihang Dai, Eduard H. Hovy, Thang Luong, and Quoc Le. Unsupervised data
augmentation for consistency training. In NeurIPS, 2020."
REFERENCES,0.9484536082474226,"[44] Yi Xu, Lei Shang, Jinxing Ye, Qi Qian, Yu-Feng Li, Baigui Sun, Hao Li, and Rong Jin. Dash:
Semi-supervised learning with dynamic thresholding. In ICML, 2021."
REFERENCES,0.9536082474226805,"[45] Shiqi Yang, Yaxing Wang, Joost Van De Weijer, Luis Herranz, and Shangling Jui. Generalized
source-free domain adaptation. In ICCV, 2021."
REFERENCES,0.9587628865979382,"[46] C. Yu, J. Wang, Y. Chen, and M. Huang. Transfer learning with dynamic adversarial adaptation
network. In ICDM, 2020."
REFERENCES,0.9639175257731959,"[47] Chaohui Yu, Jindong Wang, Yiqiang Chen, and Meiyu Huang. Transfer learning with dynamic
adversarial adaptation network. In ICDM, 2019."
REFERENCES,0.9690721649484536,"[48] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In BMVC, 2016."
REFERENCES,0.9742268041237113,"[49] Werner Zellinger, Thomas Grubinger, Edwin Lughofer, Thomas NatschlÃ¤ger, and Susanne
Saminger-Platz. Central moment discrepancy (CMD) for domain-invariant representation
learning. In ICLR, 2017."
REFERENCES,0.979381443298969,"[50] Bowen Zhang, Yidong Wang, Wenxin Hou, Hao Wu, Jindong Wang, Manabu Okumura, and
Takahiro Shinozaki. Flexmatch: Boosting semi-supervised learning with curriculum pseudo
labeling. In NeurIPS, 2021."
REFERENCES,0.9845360824742269,"[51] Jing Zhang, Wanqing Li, Chang Tang, Philip Ogunbona, et al. Unsupervised domain expansion
from multiple sources. arXiv, 2020."
REFERENCES,0.9896907216494846,"[52] Marvin Zhang, Sergey Levine, and Chelsea Finn. MEMO: test time robustness via adaptation
and augmentation. In NeurIPS, 2022."
REFERENCES,0.9948453608247423,"[53] Kaiyang Zhou, Chen Change Loy, and Ziwei Liu. Semi-supervised domain generalization with
stochastic stylematch. International Journal of Computer Vision, 131:2377â€“2387, 2023."
