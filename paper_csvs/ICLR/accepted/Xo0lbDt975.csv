Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0030211480362537764,"Federated Learning (FL) has emerged as the tool of choice for training deep mod-
els over heterogeneous and decentralized datasets. As a reﬂection of the experi-
ences from different clients, severe class imbalance issues are observed in real-
world FL problems. Moreover, there exists a drastic mismatch between the im-
balances from the local and global perspectives, i.e. a local majority class can be
the minority of the population. Additionally, the privacy requirement of FL poses
an extra challenge, as one should handle class imbalance without identifying the
minority class. In this paper we propose a novel agnostic constrained learning
formulation to tackle the class imbalance problem in FL without requiring fur-
ther information beyond the standard FL objective. A meta algorithm, CLIMB, is
designed to solve the target optimization problem, with its convergence property
analyzed under certain oracle assumptions. Through an extensive empirical study
over various data heterogeneity and class imbalance conﬁgurations, we showcase
that CLIMB considerably improves the performance in the minority class without
compromising the overall accuracy of the classiﬁer, which signiﬁcantly outper-
forms previous arts. In fact, we observe the greatest performance boost in the
most difﬁcult scenario where every client only holds data from one class. The
code can be found here."
INTRODUCTION,0.006042296072507553,"1
INTRODUCTION"
INTRODUCTION,0.00906344410876133,"Class imbalance is ubiquitous in real world supervised learning problems. Examples span from
medical applications (Lee & Shin, 2020; Roy et al., 2019; Choudhury et al., 2019), fraud detection
(Yang et al., 2019; Chan et al., 1999), to consumer based applications (Wang et al., 2021b; Wu et al.,
2020; Long et al., 2020). In these scenarios, data belonging to a subset of classes constitute a great
proportion of the population while data from the minority classes, generated by uncommon events,
are scarce (He & Garcia, 2009). Having a non-uniform number of samples per class deteriorates the
performance of the classiﬁer in the minority class (Huang et al., 2016), resulting in low training and
testing accuracy. More importantly, unintended consequences of mistreating the minority class can
be catastrophic (Van Hulse et al., 2007) if the problem is not handled appropriately."
INTRODUCTION,0.012084592145015106,"From the perspectives of data heterogeneity and privacy, the issue of class imbalance is even more
signiﬁcant in the setting of Federated Learning (FL). Due to the heterogeneity of the local data
distributions, there can be a signiﬁcant mismatch between the local and global imbalance, i.e. the
class that is a minority locally can actually be a majority class globally. Moreover, for the purpose
of privacy protection in FL, one should tackle the class-imbalance problem in an agnostic manner,
i.e. the proposed algorithms should not require the minority class to be identiﬁed."
INTRODUCTION,0.015105740181268883,"In the centralized training setting, techniques like balanced sampling, loss re-weighting, and gra-
dient tuning have achieved many successes (Johnson & Khoshgoftaar, 2019). However, due to the
extra difﬁculty of handling the class imbalance problem in FL, previous arts that rely on the ex-
plicit identiﬁcation of the minority class do not directly apply. While research along this line is
quite limited, a commonly used heuristic is to estimate the portion of a class using the norm of the
gradient per class. When used as proxies, these quantities are then utilized to reweight the losses"
INTRODUCTION,0.01812688821752266,Published as a conference paper at ICLR 2022
INTRODUCTION,0.021148036253776436,"corresponding to different classes (Wang et al., 2021a; Yang et al., 2020). A key drawback of (Wang
et al., 2021a) is that a subset of the client’s local dataset needs to be shared, which is not ideal for
privacy preserving purposes. Moreover, the effectiveness of such approaches degrades when there
is a notable mismatch between the local imbalance and the global imbalance (Wang et al., 2021a)."
INTRODUCTION,0.02416918429003021,"In this work, we target the most difﬁcult yet possibly most interesting setting in FL, where there is
a signiﬁcant mismatch between the local and global imbalance. Concretely, on certain clients, a mi-
nority class from a global perspective is in the majority locally. Practitioners often encounters such
a mismatch due to the highly heterogeneous data conﬁguration in FL and the principle of locality
(Wang et al., 2021a). For example, when the local data are produced by a minority user, the corre-
sponding minority data could occupy a large portion of the local distribution. In this scenario, due
to the scarceness of the minority data from a global perspective, the corresponding underrepresented
client will usually experience poor service quality from the trained model, giving rise to potentially
severe fairness issue."
INTRODUCTION,0.027190332326283987,"To overcome the aforementioned challenges, we propose a constrained learning formulation to han-
dle the class imbalance issue in FL while accounting for both heterogeneity and privacy. In brief, we
impose constraints on the standard FL formulation so that the empirical loss on every client should
not overly exceed the average empirical loss. Such constraints are shown to force the classiﬁer to
account for all classes equally and hence mitigate the detrimental effect of class imbalance, under
a type of heterogeneous data conﬁguration that captures the mismatch between the local and global
imbalance. The advantages of our formulation are threefold: First, in contrast to previous arts which
usually rely on some heuristics to reweight the loss functions for different classses, our approach
is principled, yielding a simple optimization interpretation. Second, unlike existing methods, our
formulation requires no additional information compared to the original FL formulation and it treats
data from different classes agnostically, and hence is less likely to leak the private information of
the clients. Third, from our extensive empirical study, our formulation can signiﬁcantly improve the
testing accuracy on the minority class, without compromising the overall performance."
INTRODUCTION,0.030211480362537766,"Contribution. We summarize our contributions as follows.
1. Problem Formulation: To address the challenge of class imbalance in FL with data hetero-
geneity, we propose a novel constrained FL formulation with explicit enforcement on the similarity
between the local empirical losses. Using only information from the standard FL objective, our
approach is completely agnostic to class distribution of the client data and its proxies, as opposed to
the existing literature (which mostly violate privacy requirements of FL).
2. Meta-Algorithm: We solve our constrained optimization problem via a primal-dual approach.
For a ﬁxed dual variable, the corresponding Lagrangian function enjoys a similar structure as the
standard FL loss, but with non-uniform weights on the local objectives. Accordingly, we propose
an efﬁcient (meta-) algorithm called CLIMB that can use any FL optimization method as a subrou-
tine, with nearly negligible communication overhead. Furthermore, we analyze the convergence
properties of CLIMB under certain oracle assumptions.
3. Accuracy Improvement in the minority class: On benchmark datasets, CLIMB with Fed-Avg
as base solver achieves signiﬁcant enhancement of the accuracy in the minority class without com-
promising the overall accuracy under various data heterogeneity and class imbalance scenarios."
RELATED WORKS,0.03323262839879154,"2
RELATED WORKS"
RELATED WORKS,0.03625377643504532,"Class Imbalance in Centralized Setting. In the centralized learning setting, the number of samples
per class is known. We categorize existing solutions that exploit such information as follows.
Balanced sampling. In order to balance the data used for gradient calculation, the most common
approaches are either to under sample from the majority classes (Liu et al., 2008), or to augment
the minority class (Chawla et al., 2002; Guo & Viktor, 2004). Both methods seek to generate an
artiﬁcial uniform distribution of data (Buda et al., 2018; Pouyanfar et al., 2018).
Loss reweighting. Methods in this category focus on re-weighting the loss functions for different
classes, with extra emphases on the mistakes made in the minority class (Cui et al., 2019; Ling &
Sheng, 2008; Sun et al., 2007; Wang et al., 2016). There also exist works like (Lin et al., 2017) that
adjust the scale of the loss sample-wise, without exploiting the global data distribution.
Gradient tuning. In the context of neural networks, some other works focus on tuning the gradient
per class (Anand et al., 1993) showing faster rates of convergence in the class imbalance setting."
RELATED WORKS,0.03927492447129909,Published as a conference paper at ICLR 2022
RELATED WORKS,0.04229607250755287,"To do this, the gradient is reshaped in order to have an equal magnitude when projected to all the
gradients per class, while keeping the same norm as the original gradient.
Class Imbalance in Federated Learning. Combating with the issue of class imbalance is more
challenging in FL since the data composition in such a setting is generally unknown and the minor-
ity classes are often difﬁcult to identify Li et al. (2020); Wang et al. (2020b). While research in this
direction are quite limited, to the best of our knowledge, we classify them into the following two
classes based on whether a proxy of the data composition is explicitly built.
(i) A common strategy in existing methods is to explicitly build proxies of the data composition
based on some heuristics. These proxies are dynamically adjusted during the learning procedure
and allow the aforementioned three techniques, balanced sampling, loss re-weighting and gradient
tuning, to be utilized in the FL setting: Wang et al. (2021a) suggests that there exists a propor-
tional relation between the magnitude of the gradients the corresponds to the last layer of the neural
network and the sample quantity. Based on such observation, the proposed the Ratio-Loss,
a class-wise re-weighted version of the standard cross entropy loss. We note that to deﬁne the
Ratio-Loss, one needs to collect a subset of client data as “auxiliary data”, which may not be
ideal in the setting of FL. On the same vein, Yang et al. (2020) argues that the gradient per class can
be used as a proxy to infer the imbalances in the distributions of the clients, and thus clients should
be selected according to how uniform the magnitude of gradient per class is. We also notice the
work Astraea which introduces extra virtual components called mediators between the FL server
and clients (Duan et al., 2019). The mediator is assumed to have access to the local data distribu-
tions of the clients so that client rescheduling and data augmentation can be carried out accordingly.
Through the prepossessing on the mediator, the gradients communicated to the server are made bal-
anced class-wise. However, when privacy is taken into consideration, methods like Astraea may
not be appropriate as in essence it is directly built on the global data distribution.
(ii) Without explicitly constructing an estimation of the data composition, there are works that resort
to techniques like active learning and reinforcement learning to implicitly learn the data composi-
tion as the optimization progresses. Works along this research line usually rely on client selection
to mitigate the effects of class imbalances. In (Goetz et al., 2019), to address the class imbalance
problem, there will a higher probability to sampling clients that posses the global minority class.
Other works, leverage client selection as a multi-armed bandit problem, and design a client selec-
tion policy to balance the gradients (Xia et al., 2020). Other arts leverage Q-learning techniques to
select clients in order to minimize the overall loss Wang et al. (2020a). However, we believe in the
most practical setting of FL, clients that are available per communication round is incoercible and
hence these strategies have limited applicability.
Comparison with Previous Works. Our method signiﬁcantly differs from the above strategies in
the following ways: (1) Our method requires no knowledge of the data composition, nor any proxy
of such information. Consequently, our approach is agnostic to the minority class and better pre-
serves clients’ privacy. Such a property draws clear distinction between our work and the works
listed in class (i) above. (2) As will be more clear in the following section, we only perform client-
wise re-weighting as opposed to the class-wise re-weighting schemes used in previous works, which
further emphasize the agnostic nature of our approach. (3) In contrast to the methods in class (ii)
above, our approach does not require active client selection, enjoying a broader applicability in the
most practical and interesting settings."
FEDERATED LEARNING WITH EMPIRICAL LOSS CONSTRAINTS,0.045317220543806644,"3
FEDERATED LEARNING WITH EMPIRICAL LOSS CONSTRAINTS"
FEDERATED LEARNING WITH EMPIRICAL LOSS CONSTRAINTS,0.04833836858006042,"We consider the problem of multi-class classiﬁcation. Let x ∈X ⊆Rd be the input and use
y ∈Y = {1, . . . , C} to denote the target label, where C is the total number of classes. In the
context of FL, we assume to have N clients, each of which posses its own private data distribution
pi(x, y). Here, pi(x, y) is a joint probability distribution of the input x and the output label y. One
can decompose pi(x, y) as pi(x, y) = p(x|y)pi(y), where p(x|y) is the conditional distribution of
the input x given class y and pi(y) is the marginal distribution of class y on client i. We assume that
the conditional distribution p(x|y) is identical on all devices, but the marginal distribution pi(y) can
vary signiﬁcantly due to the heterogeneity of the data conﬁguration."
FEDERATED LEARNING WITH EMPIRICAL LOSS CONSTRAINTS,0.0513595166163142,"Let H = {φ : X × Θ →RC} be a family of parameterized predictors with parameter θ ∈Θ ⊆RQ.
Let ℓ: RC × Y →R+ be the loss function, then the local objective function of client i is deﬁned as"
FEDERATED LEARNING WITH EMPIRICAL LOSS CONSTRAINTS,0.054380664652567974,"fi(θ) = E(x,y)∼pi[ℓ(φ(x, θ), y)].
(1)"
FEDERATED LEARNING WITH EMPIRICAL LOSS CONSTRAINTS,0.05740181268882175,Published as a conference paper at ICLR 2022
FEDERATED LEARNING WITH EMPIRICAL LOSS CONSTRAINTS,0.06042296072507553,"For multi-class classiﬁcation, ℓis often chosen to be the cross entropy loss. In the standard formu-
lation of FL, the goal is to minimize the global average of local objectives"
FEDERATED LEARNING WITH EMPIRICAL LOSS CONSTRAINTS,0.0634441087613293,"min
θ∈Θ
¯f(θ) := 1 N N
X"
FEDERATED LEARNING WITH EMPIRICAL LOSS CONSTRAINTS,0.06646525679758308,"i=1
fi(θ).
(2)"
FEDERATED LEARNING WITH EMPIRICAL LOSS CONSTRAINTS,0.06948640483383686,"While it is well known that, in the presence of class imbalance, the above vanilla formulation will
produce models that perform poorly on the minority data, we consider the following conﬁguration
of heterogeneous local class distributions in order to make a quantitative analysis of such a phe-
nomenon for a concrete class imbalance setting. More importantly, such a setting will also motivate
our constrained FL formulation. We emphasize that the following setting is just used as a motivating
example to showcase our claims, and our results apply generally to any FL setting."
FEDERATED LEARNING WITH EMPIRICAL LOSS CONSTRAINTS,0.07250755287009064,"Motivating Example. Let u be the uniform distribution over the classes, i.e. for y ∼u, Pr(y =
c) = 1"
FEDERATED LEARNING WITH EMPIRICAL LOSS CONSTRAINTS,0.0755287009063444,"C , ∀c ∈Y and let δc be the Dirac distribution of class c. We assume for some small but ﬁxed
α ∈[0, 1], the local class distribution of the client i is a mixture of the uniform distribution over all
classes and the Dirac distribution of a ﬁxed class ci, i.e. pi = αu + (1 −α)δci. We use Nc to denote
the number clients with ci = c. Note that in the limit setting when α = 0, the aforementioned
conﬁguration captures the most heterogeneous setting: clients only have data from a single class.
We consider an extreme case of class imbalance under the above conﬁguration. Without loss of
generality, we consider the binary classiﬁcation problem, i.e. C = 2, and we assume class 1 to be
the minority class with N1 = 1. We deﬁne gi(θ) := Ex∼p(x|y=i)[ℓ(φ(x, θ), i)] as the loss of the
predictor φ(·, θ) on the data with y = i. We can calculate that"
FEDERATED LEARNING WITH EMPIRICAL LOSS CONSTRAINTS,0.07854984894259819,"¯f(θ) =
α"
FEDERATED LEARNING WITH EMPIRICAL LOSS CONSTRAINTS,0.08157099697885196,2 + 1 −α N
FEDERATED LEARNING WITH EMPIRICAL LOSS CONSTRAINTS,0.08459214501510574,"
g1(θ) +
α"
FEDERATED LEARNING WITH EMPIRICAL LOSS CONSTRAINTS,0.08761329305135952,2 + (1 −α)(N −1) N
FEDERATED LEARNING WITH EMPIRICAL LOSS CONSTRAINTS,0.09063444108761329,"
g2(θ).
(3)"
FEDERATED LEARNING WITH EMPIRICAL LOSS CONSTRAINTS,0.09365558912386707,"Clearly, when α, the portion of data with uniform label distribution, is small, e.g. α = 0, and N, the
total number of clients, is large, the loss of the predictor on class 1 has negligible weight, which often
leads to the poor performance on the minority class in the trained classiﬁer as the majority classes
dominate the gradient. Moreover, observe that when α is small, there is a signiﬁcant mismatch
between the local and global class imbalance: the global minority class 1 is in the majority on
the corresponding client locally. Such phenomenon is pertinent to the FL setting due to the data
heterogeneity and poses a great challenge for tackling the issue of class imbalance in FL."
CONSTRAINED FL FORMULATION,0.09667673716012085,"3.1
CONSTRAINED FL FORMULATION"
CONSTRAINED FL FORMULATION,0.09969788519637462,"In our work, to address the class imbalance challenge, we propose to minimize the following con-
strained FL (CFL) formulation"
CONSTRAINED FL FORMULATION,0.1027190332326284,"P ∗
ϵ = min
θ∈Θ
¯f(θ) := 1 N N
X"
CONSTRAINED FL FORMULATION,0.10574018126888217,"i=1
fi(θ)
(CFL)"
CONSTRAINED FL FORMULATION,0.10876132930513595,"s.t. fi(θ) −¯f(θ) ≤ϵ, ∀i ∈[1, . . . , N].
Here, ϵ is a tolerance constant that controls the enforced closeness in the training loss among clients,
and we emphasize the dependence of the optimal value P ∗
ϵ on ϵ by encoding it in the subscript. As a
motivation, we show that the constraint in our formulation (CFL) can be translated to a constraint on
the performance of the minority class, under the above class imbalance setting: Consider the setting
where the tolerance constant is very small, i.e. ϵ is close to zero. WLOG, we assume that the ﬁrst
device is the one that has ci = 1. Recall the deﬁnition of gi above Eq.(3). One can compute that"
CONSTRAINED FL FORMULATION,0.11178247734138973,f1(θ) −¯f(θ) = (1 −α)(N −1)
CONSTRAINED FL FORMULATION,0.1148036253776435,"N
(g1(θ) −g2(θ)) ≤ϵ ⇐⇒g1(θ) −g2(θ) ≤
Nϵ
(1 −α)(N −1)."
CONSTRAINED FL FORMULATION,0.11782477341389729,"Therefore, our formulation (CFL) enjoys a clear class-balancing interpretation in the highly hetero-
geneous setting when α is small and N is large. Note that this is achieved without the identiﬁcation
of the minority class and server collects no additional information compared to the vanilla FL."
CONSTRAINED FL FORMULATION,0.12084592145015106,"While the above nice interpretation of the proposed constraint may not hold exactly for general class
imbalance settings, in spirit, we want the resulting classiﬁer from our algorithm to perform similarly
on every class, or in other words to account for the minority class and majority class equally. In the
following section, we discuss how to solve the proposed formulation by alternating the primal and
dual updates of an equivalent Lagrangian formulation."
CONSTRAINED FL FORMULATION,0.12386706948640483,Published as a conference paper at ICLR 2022
CONSTRAINED FL FORMULATION,0.1268882175226586,Algorithm 1 CLIMB: CLass IMBalance Federated Learning
CONSTRAINED FL FORMULATION,0.1299093655589124,"1: Input: initial model θ0, a subroutine ClientUpdate, dual step size ηD, maximum round T;
2: Initialize the dual variables λ = [0, . . . , 0];
3: for t = 1, 2, . . . , T do
4:
Compute weights: ∀i ∈[N], wi = 1 + λi −¯λ, with ¯λ = 1"
CONSTRAINED FL FORMULATION,0.13293051359516617,"N
PN
i=1 λi;
5:
Primal Update: θt+1 ←ClientUpdate({wi}N
i=1, θt);
6:
Dual Update: ∀i ∈[N], λi ←[λi + ηD(fi(θt+1) −¯f(θt+1) −ϵ)]+, with ¯f = 1"
CONSTRAINED FL FORMULATION,0.13595166163141995,"N
PN
i=1 fi;
7: end for
8: Output: model θT +1"
ALGORITHM CONSTRUCTION,0.13897280966767372,"3.2
ALGORITHM CONSTRUCTION"
ALGORITHM CONSTRUCTION,0.1419939577039275,"In order to solve problem (CFL), we resort to the method of Lagrange multipliers. By introducing
the dual variables λ = [λ1, . . . , λN] ∈RN
+, we deﬁne the Lagrangian function as,"
ALGORITHM CONSTRUCTION,0.14501510574018128,"L(θ, λ) = 1 N N
X"
ALGORITHM CONSTRUCTION,0.14803625377643503,"i=1
fi(θ) + λi "
ALGORITHM CONSTRUCTION,0.1510574018126888,"fi(θ) −1 N N
X"
ALGORITHM CONSTRUCTION,0.1540785498489426,"j=1
fj(θ) −ϵ  
(4) = 1 N N
X"
ALGORITHM CONSTRUCTION,0.15709969788519637,"i=1
(1 + λi −¯λ)fi(θ) −λiϵ,
with ¯λ = 1 N N
X"
ALGORITHM CONSTRUCTION,0.16012084592145015,"i=1
λi.
(5)"
ALGORITHM CONSTRUCTION,0.16314199395770393,"With this in hand, we can construct a lower bound of the above constrained optimization problem
using the Lagrangian L(θ, λ) as follows:"
ALGORITHM CONSTRUCTION,0.1661631419939577,"D∗
ϵ = max
λ∈RN
+
min
θ∈Θ L(θ, λ) ≤min
θ∈Θ max
λ∈RN
+
L(θ, λ) = P ∗
ϵ ,"
ALGORITHM CONSTRUCTION,0.1691842900302115,"where we often refer to D∗
ϵ as the dual problem. To solve the dual problem, we propose CLIMB, a
method described in Algorithm 1, which is discussed in detail as follows.
CLIMB proceeds by alternatingly optimizing over the primal variable θ and the dual variable λ.
Primal Update. For a ﬁxed λ, the minimization of L with respect to θ is equivalent to a re-weighted
version of the standard unconstrained FL objective (2):"
ALGORITHM CONSTRUCTION,0.17220543806646527,"min
θ∈Θ L(θ, λ) ⇐⇒min
θ∈Θ
1
N N
X"
ALGORITHM CONSTRUCTION,0.17522658610271905,"i=1
(1 + λi −¯λ)fi(θ).
(6)"
ALGORITHM CONSTRUCTION,0.1782477341389728,"Importantly, this simple and canonical form allows us to perform the update on θ using any FL
solver as the base optimizer. For ﬂexibility, we do not explicitly choose the base optimizer in our
algorithm description, but refer to the update on θ as the subroutine ClientUpdate({wi}N
i=1, θt) →
θt+1. Such a subroutine takes the non-uniform weights {wi}N
i=1 on the local objectives and the
current consensus model θt as inputs, and returns an updated model θt+1. Note that every call to
ClientUpdate may consists of multiple communication rounds.
Dual Update. Once we have obtained the new consensus model θt+1, we perform dual update on λ
by taking a single dual ascent step of the following equivalent objective (given some ﬁxed θ),"
ALGORITHM CONSTRUCTION,0.18126888217522658,"max
λ∈RN
+
L(θ, λ) ⇐⇒max
λ∈RN
+"
N,0.18429003021148035,"1
N N
X"
N,0.18731117824773413,"i=1
λi "
N,0.1903323262839879,"fi(θ) −1 N N
X"
N,0.1933534743202417,"j=1
fj(θ) −ϵ "
N,0.19637462235649547,".
(7)"
N,0.19939577039274925,"To evaluate the gradient of L with respect to λ, we need to ﬁrst broadcast the consensus model
θt+1 and then aggregate the function values f(θt+1). Since the broadcast model can be used for
the next round of primal update, the only overhead of CLIMB compared to the standard FL solver
ClientUpdate, is to transmit the functional value, which is negligible."
N,0.20241691842900303,"Remark 3.1 We emphasize that CLIMB can be implemented in a privacy-preserving manner: A
client can carry out its update locally given the access to the global average of the dual variables ¯λ
and the global average of the loss functions ¯f(θ). These quantities can be computed via the standard
FL technique of Homomorphic Encryption without revealing the exact value of the dual variable λi
and local loss fi(θ) to the server, as elaborated in Appendix D."
N,0.2054380664652568,Published as a conference paper at ICLR 2022
THEORETICAL GUARANTEES,0.2084592145015106,"3.3
THEORETICAL GUARANTEES"
THEORETICAL GUARANTEES,0.21148036253776434,"Based on the recent progress in constrained learning (Chamon et al., 2021), we show that under mild
regularity conditions, the duality gap between the dual problem D∗
ϵ and the primal problem P ∗
ϵ can
be controlled by some quantity that describes the capability of the parametric function class H."
THEORETICAL GUARANTEES,0.21450151057401812,"Assumption 3.1 The loss function ℓin the deﬁnition of the local objective (1) is L-Lipschitz,
i.e.∥ℓ(x, ·) −ℓ(z, ·)∥≤L∥x −z∥, and bounded by B."
THEORETICAL GUARANTEES,0.2175226586102719,Assumption 3.2 The conditional distribution p(x|y) is non-atomic for all y ∈RC.
THEORETICAL GUARANTEES,0.22054380664652568,"While usually the local data distribution is discrete, we can always augment it by randomly per-
turbing the data points with white noise, this is often used as data augmentation in vision tasks."
THEORETICAL GUARANTEES,0.22356495468277945,"Assumption 3.3 There exists a convex hypothesis class ˆH such that H ⊆ˆH, and there exists a
constant ξ > 0 such that ∀ˆφ ∈ˆH, there exists θ ∈Θ such that supx∈X ∥ˆφ(x) −φ(x, θ)∥≤ξ."
THEORETICAL GUARANTEES,0.22658610271903323,"A simple strategy to construct ˆH is to take the convex hull of H. When H is sufﬁciently rich, ξ can
be expected to be small. Notice that this bound can be decreased by increasing the richness of the
function class H. All the proofs can be found in the Appendix."
THEORETICAL GUARANTEES,0.229607250755287,"Theorem 3.1 (Near Zero Duality Gap) Under Assumptions 3.1, 3.2,3.3, and the Contrained Fed-
erated Learning problem is feasible in ˆH with constraint ϵ−2Lξ, the Constrained Federated Learn-
ing problem has near zero-duality gap,"
THEORETICAL GUARANTEES,0.2326283987915408,"P ∗
ϵ −D∗
ϵ ≤(2|λ∗
ϵ−2Lξ|1 + 1)Lξ,
(8)"
THEORETICAL GUARANTEES,0.23564954682779457,"where λ∗
ϵ−2Lξ is the optimal dual variable associated with the Constrained Federated Learning
problem (CFL) with constraints ϵ −2Lξ over the space of functions ˆH."
THEORETICAL GUARANTEES,0.23867069486404835,"Theorem 3.1 establishes an upper bound on the duality gap of the Constrained Federated Learning
Problem CFL. The gap depends on the Lipschitz constant of the loss function L, the richness of the
function class ξ, and the optimal dual variable of a more restrictive problem. Note that we required
the Constrained Federated Learning problem to be feasible for constraint ϵ −2Lξ. In the case of the
cross-entropy loss, as long as ϵ −2Lξ > 0, this can be satisﬁed by a classiﬁer that assigns a uniform
label for every sample, as each individual loss will be equal to each other.
Since the minimization of the Lagrangian function L is non-convex, to show the convergence of
CLIMB, we need an additional oracle assumption as follows."
THEORETICAL GUARANTEES,0.24169184290030213,"Assumption 3.4 (Approximate Solution) For every dual variable λ ∈RN
+, and precision δ > 0
there exists an oracle approximate solution θλ such that L(θλ, λ) ≤minθ∈RQ L(θ, λ) + δ."
THEORETICAL GUARANTEES,0.24471299093655588,"Theorem 3.2 (Convergence) Deﬁne the dual function d(λ) = minθ∈Θ L(θ, λ). Under Assump-
tions 3.1 to 3.4, for a ﬁxed tolerance r > 0, the iterates generated by Algorithm 1 converge to a
neighborhood of the dual problem D∗
ϵ in at most Tr = O(1/r) steps, i.e.,"
THEORETICAL GUARANTEES,0.24773413897280966,"d(λTr) ≥D∗
ϵ −δ −ηD"
THEORETICAL GUARANTEES,0.25075528700906347,"2 B2 −r,
(9)"
EXPERIMENTS,0.2537764350453172,"4
EXPERIMENTS"
EXPERIMENTS,0.256797583081571,"In this section, we evaluate our formulation (CFL) against the competitors on various FL scenarios
at the presence of class imbalance. Our results highlight the beneﬁts of our approach especially
when the local data distributions are severely heterogeneous and there is a signiﬁcant mismatch
between local and global imbalance. To ensure a fair comparison, we use Fed-Avg as the base
optimizer in the current experiment for all formulations: the standard FL objective in Eq.(2), the
proposed formulation in Eq.(CFL), Ratio-Loss (Wang et al., 2021a), and Focal-Loss (Lin
et al., 2017). We emphasize that the novelty of our work lies in the new constrained FL formulation
(CFL) and is orthogonal to how the formulation is solved. We now describe the datasets and models
used in our experiments with more details provided in Appendix A.
Datasets Three benchmark datasets are used in our experiments with the default train/test splits,"
EXPERIMENTS,0.2598187311178248,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.2628398791540785,"Imbalance
ratio
Dataset
Level of
heterogeneity"
EXPERIMENTS,0.26586102719033233,"Baseline
(Eq.(2))"
EXPERIMENTS,0.2688821752265861,"CLIMB
(this work)
Ratio-Loss
Focal-Loss"
EXPERIMENTS,0.2719033232628399,ρ = 20
EXPERIMENTS,0.27492447129909364,CIFAR10
EXPERIMENTS,0.27794561933534745,1 minority class out of 10 total classes
EXPERIMENTS,0.2809667673716012,"α = 0.1
0.0532
(0.6754)"
EXPERIMENTS,0.283987915407855,"0.2080
(0.6829)"
EXPERIMENTS,0.28700906344410876,"0.1140
(0.6727)"
EXPERIMENTS,0.29003021148036257,"0.0450
(0.6445)"
EXPERIMENTS,0.2930513595166163,"α = 0.2
0.137
(0.7121)"
EXPERIMENTS,0.29607250755287007,"0.3230
(0.7121)"
EXPERIMENTS,0.2990936555891239,"0.1790
(0.7037)"
EXPERIMENTS,0.3021148036253776,"0.0430
(0.6914)
3 minority classes out of 10 total classes"
EXPERIMENTS,0.30513595166163143,"α = 0.1
0
(0.5669)"
EXPERIMENTS,0.3081570996978852,"0.2810
(0.6031)"
EXPERIMENTS,0.311178247734139,"0
(0.5746)"
EXPERIMENTS,0.31419939577039274,"0
(0.6565)"
EXPERIMENTS,0.31722054380664655,"α = 0.2
0.1279
(0.7098)"
EXPERIMENTS,0.3202416918429003,"0.3240
(0.7167)"
EXPERIMENTS,0.32326283987915405,"0.1790
(0.7054)"
EXPERIMENTS,0.32628398791540786,"0.0552
(0.6905) MNIST"
EXPERIMENTS,0.3293051359516616,1 minority class out of 10 total classes
EXPERIMENTS,0.3323262839879154,"α = 0.1
0.6889
(0.9375)"
EXPERIMENTS,0.33534743202416917,"0.8552
(0.9556)"
EXPERIMENTS,0.338368580060423,"0.8472
(0.9544)"
EXPERIMENTS,0.3413897280966767,"0.6186
(0.9278)"
EXPERIMENTS,0.34441087613293053,"α = 0.2
0.7925
(0.9540)"
EXPERIMENTS,0.3474320241691843,"0.8748
(0.9616)"
EXPERIMENTS,0.3504531722054381,"0.8052
(0.9555)"
EXPERIMENTS,0.35347432024169184,"0.7784
(0.9479)
3 minority classes out of 10 total classes"
EXPERIMENTS,0.3564954682779456,"α = 0.1
0.3425
(0.8260)"
EXPERIMENTS,0.3595166163141994,"0.6987
(0.9158)"
EXPERIMENTS,0.36253776435045315,"0.4134
(0.8484)"
EXPERIMENTS,0.36555891238670696,"0.1944
(0.7938)"
EXPERIMENTS,0.3685800604229607,"α = 0.2
0.4720
(0.8596)"
EXPERIMENTS,0.3716012084592145,"0.7290
(0.9153)"
EXPERIMENTS,0.37462235649546827,"0.6717
(0.9063)"
EXPERIMENTS,0.3776435045317221,"0.4602
(0.8654) ρ =10"
EXPERIMENTS,0.3806646525679758,CIFAR10
EXPERIMENTS,0.38368580060422963,1 minority class out of 10 total classes
EXPERIMENTS,0.3867069486404834,"α = 0.1
0.2058
(0.6841)"
EXPERIMENTS,0.38972809667673713,"0.3629
(0.7041)"
EXPERIMENTS,0.39274924471299094,"0.2164
(0.6839)"
EXPERIMENTS,0.3957703927492447,"0.0414
(0.6543)"
EXPERIMENTS,0.3987915407854985,"α = 0.2
0.1813
(0.7113)"
EXPERIMENTS,0.40181268882175225,"0.3743
(0.7312)"
EXPERIMENTS,0.40483383685800606,"0.2657
(0.7083)"
EXPERIMENTS,0.4078549848942598,"0.1347
(0.6911)
3 minority classes out of 10 total classes"
EXPERIMENTS,0.4108761329305136,"α = 0.1
0.0492
(0.5933)"
EXPERIMENTS,0.41389728096676737,"0.2280
(0.6358)"
EXPERIMENTS,0.4169184290030212,"0.0315
(0.5825)"
EXPERIMENTS,0.4199395770392749,"0
(0.5548)"
EXPERIMENTS,0.4229607250755287,"α = 0.2
0.1064
(0.6380)"
EXPERIMENTS,0.4259818731117825,"0.2734
(0.6696)"
EXPERIMENTS,0.42900302114803623,"0.0993
(0.6211)"
EXPERIMENTS,0.43202416918429004,"0.0298
(0.5982) MNIST"
EXPERIMENTS,0.4350453172205438,1 minority class out of 10 total classes
EXPERIMENTS,0.4380664652567976,"α = 0.1
0.8473
(0.9534)"
EXPERIMENTS,0.44108761329305135,"0.9305
(0.9584)"
EXPERIMENTS,0.44410876132930516,"0.8851
(0.9558)"
EXPERIMENTS,0.4471299093655589,"0.8469
(0.9470)"
EXPERIMENTS,0.4501510574018127,"α = 0.2
0.8962
(0.9634)"
EXPERIMENTS,0.45317220543806647,"0.9239
(0.9657)"
EXPERIMENTS,0.4561933534743202,"0.8798
(0.9615)"
EXPERIMENTS,0.459214501510574,"0.8953
(0.9586)
3 minority classes out of 10 total classes"
EXPERIMENTS,0.4622356495468278,"α = 0.1
0.6045
(0.8906)"
EXPERIMENTS,0.4652567975830816,"0.8084
(0.9356)"
EXPERIMENTS,0.46827794561933533,"0.6981
(0.9119)"
EXPERIMENTS,0.47129909365558914,"0.4690
(0.8670)"
EXPERIMENTS,0.4743202416918429,"α = 0.2
0.7272
(0.9190)"
EXPERIMENTS,0.4773413897280967,"0.8195
(0.9392)"
EXPERIMENTS,0.48036253776435045,"0.7663
(0.9320)"
EXPERIMENTS,0.48338368580060426,"0.6961
(0.9099)"
EXPERIMENTS,0.486404833836858,"Table 1: The minority class testing accuracy and the overall testing accuracy (the quantity in the
parentheses) after 5000 communication rounds. If there are multiple minority classes, we report
the worst of them. Here N, the number of devices, is 500. The base FL solver is Fed-Avg with
partial-participation: 100 devices participate in every communication round."
EXPERIMENTS,0.48942598187311176,"which are MNIST (LeCun et al., 1998), CIFAR10 (Krizhevsky et al., 2009) and Fashion-MNIST
(Xiao et al., 2017). The results on the last dataset are deferred to the appendix due to space limitation.
Heterogeneity. We generate heterogeneity in the local data distributions according to the strategy
from (Karimireddy et al., 2020; Hsu et al., 2019): Let α ∈[0, 1] be some constant that determines
the level of heterogeneity. For a ﬁxed α, we divide the dataset among N = 100 (moderate) or
N = 500 (massive) clients as follows: for we allocate to each client a portion of α i.i.d. data and
the remaining portion of (1 −α) by sorting according to label. In our appendix, we also consider
the Dirichlet type heterogeneous data allocation scheme which is wildly used in the literature of
Federated Learning, for example (Hsu et al., 2019; Acar et al., 2020).
Data Imbalance. We simulate the phenomenon of class imbalance by removing data belong to
the minority classes: Observe that the datasets included in our experiments both have 10 perfectly
balanced classes. For the minority class(es), we retain only 1/ρ portion of the corresponding data.
Here, ρ ≥1 the ratio between the numbers of data in the majority class and in the minority class"
EXPERIMENTS,0.49244712990936557,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.4954682779456193,"and is termed the imbalance ratio. For example, when there are 3 minority classes with ρ = 10,
90% of the data belong to classes 0, 1, 2 (without loss of generality) are manually removed. In our
experiments, we consider the setting of 1 or 3 minority classes and we take ρ = 5, 10, 20.
Models We follow the choice of model architectures in (Acar et al., 2020; McMahan et al., 2017).
Speciﬁcally, we use a 2 hidden layer fully-connected neural network for MNIST, where the numbers
of neurons are (128, 128). For CIFAR10, we use a CNN model consisting of 2 convolutional layers
with 64 5 × 5 ﬁlters followed by 2 fully connected layers with 394 and 192 neurons. We note that
higher testing accuracy on the included datasets can be obtain by using models with high capacity,
but is orthogonal to our research."
RESULTS SUMMARY,0.4984894259818731,"4.1
RESULTS SUMMARY"
RESULTS SUMMARY,0.5015105740181269,"To evaluate the effectiveness of an approach against the challenge of class imbalance, one needs to
take into consideration both the performance on the minority class(es) and the average performance
on all the classes. We report both quantities after sufﬁcient communication rounds (5000 rounds
for CIFAR10 and 1000 rounds for MNIST) under various experiment settings in Tables 1 and 2.
In every cell of these tables, the quantity above denote the minority class testing accuracy and the
quantity in the parentheses is the average performance on all the classes. We also considered the
case where there are multiple minority classes and we report the worst accuracy among the minority
classes. Our approach outperforms previous arts in all cases, often by a large margin.
Imbalance Ratio and Number of Minority Classes. The imbalance ratio ρ and the number of
minority classes are two important quantities to measure the difﬁculty of a class imbalance problem.
Under all the included choices, CLIMB consistently beats previous arts in both minority class testing
accuracy and average performance on all the classes. Therefore, we conclude that CLIMB is able
to boost the performance on the minority class without compromising the performance on the other
classes. This is a rare merit when addressing the class imbalance problem. In fact, methods like
Ratio-Lossis able to improve the testing accuracy on the minority class, but it also sacriﬁces the
performance on the rest classes, leading to an inferior average testing accuracy.
Level of Heterogeneity. We test the performance of CLIMB under different levels of heterogene-
ity and observe that CLIMB outperforms the included methods considerably in all settings. It has
the biggest advantage over the competitors in the most heterogeneous setting, α = 0. There are
situations that existing methods completely fail in the minority class with less than 10% accuracy,
but CLIMB is still able to correctly classify most of the minority data, e.g. see Table 2→ρ = 5 →
MNIST→3 minority classes→α = 0.
Moderate vs. Large Number of Devices An important goal of FL is to exploit the computational
resources of the IoT devices. Hence, the scalability to a large number of devices is a critical prop-
erty of an FL method. We hence test CLIMB on both moderate (N = 100, see Table 2) and massive
devices (N = 500, see Table 1) settings. To make things more practical, we instantiate the sub-
routine ClientUpdate using Fed-Avg with the partial-participation scheme in the massive device
setting. Speciﬁcally, 100 devices participate model training after every Fed-Avg global communica-
tion round. We clearly observe the advantage of CLIMB in all of these setups."
RESULTS SUMMARY,0.5045317220543807,"CONCLUSION
In this paper we proposed a novel agnostic constrained learning formulation to tackle the problem
of class imbalance in the Federated Learning setting. By introducing constraints in the learning
procedure we enforced the performance to be similar in all clients, thus accounting for the class
imbalances. In terms of privacy protection, our formulation requires no further information than
the standard FL objective to be collected in the server and it never estimates the data composition
as opposed to all previous approaches. Moreover, compared with previous arts which are usually
heuristic based, our approach is principled as it is purely optimization based and can be efﬁciently
solved via the proposed meta-algorithm CLIMB, yielding major practical beneﬁts. Our extensive
empirical study showcases the superiority of proposed constrained formulation over previous arts."
RESULTS SUMMARY,0.5075528700906344,ACKNOWLEDGMENTS
RESULTS SUMMARY,0.5105740181268882,"This research is supported by AFOSR Award 19RT0726, NSF HDR TRIPODS award 1934876,
NSF award CPS-1837253, NSF award CIF-1910056, NSF CAREER award CIF-1943064, and NSF
award CCF-2112665."
RESULTS SUMMARY,0.513595166163142,Published as a conference paper at ICLR 2022
RESULTS SUMMARY,0.5166163141993958,"Imbalance
ratio
Dataset
Level of
heterogeneity"
RESULTS SUMMARY,0.5196374622356495,"Baseline
(Eq.(2))"
RESULTS SUMMARY,0.5226586102719033,"CLIMB
(this work)
Ratio-Loss
Focal-Loss"
RESULTS SUMMARY,0.525679758308157,ρ = 10
RESULTS SUMMARY,0.5287009063444109,CIFAR10
RESULTS SUMMARY,0.5317220543806647,1 minority class out of 10 total classes
RESULTS SUMMARY,0.5347432024169184,"α = 0.0
0.0229
(0.5734)"
RESULTS SUMMARY,0.5377643504531722,"0.5575
(0.6076)"
RESULTS SUMMARY,0.540785498489426,"0
(0.4836)"
RESULTS SUMMARY,0.5438066465256798,"0
(0.4205)"
RESULTS SUMMARY,0.5468277945619335,"α = 0.1
0.2753
(0.7143)"
RESULTS SUMMARY,0.5498489425981873,"0.5054
(0.7246)"
RESULTS SUMMARY,0.552870090634441,"0.2929
(0.6951)"
RESULTS SUMMARY,0.5558912386706949,"0.2284
(0.6860)"
RESULTS SUMMARY,0.5589123867069486,"α = 0.2
0.2988
0.7348"
RESULTS SUMMARY,0.5619335347432024,"0.4689
(0.7511)"
RESULTS SUMMARY,0.5649546827794562,"0.3825
(0.7329)"
RESULTS SUMMARY,0.56797583081571,"0.2618
0.7249
3 minority classes out of 10 total classes"
RESULTS SUMMARY,0.5709969788519638,"α = 0.0
0.0402
(0.5534)"
RESULTS SUMMARY,0.5740181268882175,"0.2756
(0.5598)"
RESULTS SUMMARY,0.5770392749244713,"0
(0.4678)"
RESULTS SUMMARY,0.5800604229607251,"0
(0.4527)"
RESULTS SUMMARY,0.5830815709969789,"α = 0.1
0.1316
(0.6189)"
RESULTS SUMMARY,0.5861027190332326,"0.3399
(0.6637)"
RESULTS SUMMARY,0.5891238670694864,"0.0690
(0.615)"
RESULTS SUMMARY,0.5921450151057401,"0.0408
(0.5976)"
RESULTS SUMMARY,0.595166163141994,"α = 0.2
0.2566
(0.6659)"
RESULTS SUMMARY,0.5981873111782477,"0.3292
(0.6983)"
RESULTS SUMMARY,0.6012084592145015,"0.1916
(0.6504)"
RESULTS SUMMARY,0.6042296072507553,"0.1213
(0.6346) MNIST"
RESULTS SUMMARY,0.6072507552870091,1 minority class out of 10 total classes
RESULTS SUMMARY,0.6102719033232629,"α = 0.0
0.3092
(0.8630)"
RESULTS SUMMARY,0.6132930513595166,"0.9175
(0.9341)"
RESULTS SUMMARY,0.6163141993957704,"0.4650
(0.8630)"
RESULTS SUMMARY,0.6193353474320241,"0.3078
(0.8556)"
RESULTS SUMMARY,0.622356495468278,"α = 0.1
0.8597
(0.9586)"
RESULTS SUMMARY,0.6253776435045317,"0.9428
(0.9675)"
RESULTS SUMMARY,0.6283987915407855,"0.9189
(0.9648)"
RESULTS SUMMARY,0.6314199395770392,"0.8348
(0.9529)"
RESULTS SUMMARY,0.6344410876132931,"α = 0.2
0.8750
(0.9640)"
RESULTS SUMMARY,0.6374622356495468,"0.9377
(0.9715)"
RESULTS SUMMARY,0.6404833836858006,"0.9283
(0.9706)"
RESULTS SUMMARY,0.6435045317220544,"0.8882
(0.9631)
3 minority classes out of 10 total classes"
RESULTS SUMMARY,0.6465256797583081,"α = 0.0
0.0153
(0.7133)"
RESULTS SUMMARY,0.649546827794562,"0.8029
(0.9115)"
RESULTS SUMMARY,0.6525679758308157,"0.0631
0.7222"
RESULTS SUMMARY,0.6555891238670695,"0.0717
0.7401"
RESULTS SUMMARY,0.6586102719033232,"α = 0.1
0.7263
(0.9189)"
RESULTS SUMMARY,0.6616314199395771,"0.8828
(0.9522)"
RESULTS SUMMARY,0.6646525679758308,"0.7870
(0.9341)"
RESULTS SUMMARY,0.6676737160120846,"0.6674
(0.9069)"
RESULTS SUMMARY,0.6706948640483383,"α = 0.2
0.7950
(0.9352)"
RESULTS SUMMARY,0.6737160120845922,"0.8004
(0.9416)"
RESULTS SUMMARY,0.676737160120846,"0.7801
(0.9364)"
RESULTS SUMMARY,0.6797583081570997,"0.7785
(0.9274) ρ =5"
RESULTS SUMMARY,0.6827794561933535,CIFAR10
RESULTS SUMMARY,0.6858006042296072,1 minority class out of 10 total classes
RESULTS SUMMARY,0.6888217522658611,"α = 0.0
0.2892
(0.6382)"
RESULTS SUMMARY,0.6918429003021148,"0.5987
(0.6468)"
RESULTS SUMMARY,0.6948640483383686,"0.0942
(0.5506)"
RESULTS SUMMARY,0.6978851963746223,"0.1491
(0.5631)"
RESULTS SUMMARY,0.7009063444108762,"α = 0.1
0.4101
(0.7186)"
RESULTS SUMMARY,0.7039274924471299,"0.6075
(0.7351)"
RESULTS SUMMARY,0.7069486404833837,"0.4011
(0.7008)"
RESULTS SUMMARY,0.7099697885196374,"0.3558
(0.6994)"
RESULTS SUMMARY,0.7129909365558912,"α = 0.2
0.5335
(0.7536)"
RESULTS SUMMARY,0.716012084592145,"0.6063
(0.7556)"
RESULTS SUMMARY,0.7190332326283988,"0.5054
(0.7427)"
RESULTS SUMMARY,0.7220543806646526,"0.4359
(0.7380)
3 minority classes out of 10 total classes"
RESULTS SUMMARY,0.7250755287009063,"α = 0.0
0.0313
(0.4742)"
RESULTS SUMMARY,0.7280966767371602,"0.3813
(0.5639)"
RESULTS SUMMARY,0.7311178247734139,"0.0512
(0.5108)"
RESULTS SUMMARY,0.7341389728096677,"0
(0.4514)"
RESULTS SUMMARY,0.7371601208459214,"α = 0.1
0.5135
(0.6636)"
RESULTS SUMMARY,0.7401812688821753,"0.6420
(0.6977)"
RESULTS SUMMARY,0.743202416918429,"0.4600
(0.6632)"
RESULTS SUMMARY,0.7462235649546828,"0.4213
(0.6384)"
RESULTS SUMMARY,0.7492447129909365,"α = 0.2
0.5251
(0.6960)"
RESULTS SUMMARY,0.7522658610271903,"0.6328
(0.7202)"
RESULTS SUMMARY,0.7552870090634441,"0.5751
(0.6883)"
RESULTS SUMMARY,0.7583081570996979,"0.5311
(0.6749) MNIST"
RESULTS SUMMARY,0.7613293051359517,1 minority class out of 10 total classes
RESULTS SUMMARY,0.7643504531722054,"α = 0.0
0.7245
(0.8953)"
RESULTS SUMMARY,0.7673716012084593,"0.9154
(0.9224)"
RESULTS SUMMARY,0.770392749244713,"0.8020
(0.9016)"
RESULTS SUMMARY,0.7734138972809668,"0.7429
(0.8992)"
RESULTS SUMMARY,0.7764350453172205,"α = 0.1
0.9378
(0.9666)"
RESULTS SUMMARY,0.7794561933534743,"0.9582
(0.9693)"
RESULTS SUMMARY,0.7824773413897281,"0.9286
(0.9651)"
RESULTS SUMMARY,0.7854984894259819,"0.9408
(0.9624)"
RESULTS SUMMARY,0.7885196374622356,"α = 0.2
0.9448
(0.9711)"
RESULTS SUMMARY,0.7915407854984894,"0.9670
(0.9734)"
RESULTS SUMMARY,0.7945619335347432,"0.9561
(0.9733)"
RESULTS SUMMARY,0.797583081570997,"0.9481
(0.9686)
3 minority classes out of 10 total classes"
RESULTS SUMMARY,0.8006042296072508,"α = 0.0
0.0160
(0.7722)"
RESULTS SUMMARY,0.8036253776435045,"0.8744
(0.9137)"
RESULTS SUMMARY,0.8066465256797583,"0
(0.7370)"
RESULTS SUMMARY,0.8096676737160121,"0.0544
(0.7826)"
RESULTS SUMMARY,0.8126888217522659,"α = 0.1
0.8294
(0.9422)"
RESULTS SUMMARY,0.8157099697885196,"0.9217
(0.9606)"
RESULTS SUMMARY,0.8187311178247734,"0.8520
(0.9501)"
RESULTS SUMMARY,0.8217522658610272,"0.7987
(0.9361)"
RESULTS SUMMARY,0.824773413897281,"α = 0.2
0.8557
(0.9536)"
RESULTS SUMMARY,0.8277945619335347,"0.8818
(0.9595)"
RESULTS SUMMARY,0.8308157099697885,"0.8730
(0.9536)"
RESULTS SUMMARY,0.8338368580060423,"0.8321
(0.9442)"
RESULTS SUMMARY,0.8368580060422961,"Table 2: The minority class testing accuracy and the overall testing accuracy (the quantity in the
parentheses) after sufﬁciently many communication rounds. If there are multiple minority classes,
we report the worst of them. Here N, the number of devices, is 100. The base FL solver is Fed-Avg
with full-participation: all devices participate in every communication round."
RESULTS SUMMARY,0.8398791540785498,Published as a conference paper at ICLR 2022
REFERENCES,0.8429003021148036,REFERENCES
REFERENCES,0.8459214501510574,"Durmus Alp Emre Acar, Yue Zhao, Ramon Matas, Matthew Mattina, Paul Whatmough, and
Venkatesh Saligrama. Federated learning based on dynamic regularization. In International Con-
ference on Learning Representations, 2020."
REFERENCES,0.8489425981873112,"Rangachari Anand, Kishan G Mehrotra, Chilukuri K Mohan, and Sanjay Ranka. An improved
algorithm for neural network classiﬁcation of imbalanced training sets. IEEE Transactions on
Neural Networks, 4(6):962–969, 1993."
REFERENCES,0.851963746223565,"Dimitri Bertsekas. Convex optimization theory. Athena Scientiﬁc, 2009."
REFERENCES,0.8549848942598187,"Dimitri Bertsekas. Convex optimization algorithms. Athena Scientiﬁc, 2015."
REFERENCES,0.8580060422960725,Stephen Boyd and Almir Mutapcic. Subgradient methods.
REFERENCES,0.8610271903323263,"Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004."
REFERENCES,0.8640483383685801,"Mateusz Buda, Atsuto Maki, and Maciej A Mazurowski. A systematic study of the class imbalance
problem in convolutional neural networks. Neural Networks, 106:249–259, 2018."
REFERENCES,0.8670694864048338,"Luiz Chamon and Alejandro Ribeiro. Probably approximately correct constrained learning. Ad-
vances in Neural Information Processing Systems, 33, 2020."
REFERENCES,0.8700906344410876,"Luiz FO Chamon, Santiago Paternain, Miguel Calvo-Fullana, and Alejandro Ribeiro. Constrained
learning with non-convex losses. arXiv preprint arXiv:2103.05134, 2021."
REFERENCES,0.8731117824773413,"Philip K Chan, Wei Fan, Andreas L Prodromidis, and Salvatore J Stolfo. Distributed data mining in
credit card fraud detection. IEEE Intelligent Systems and Their Applications, 14(6):67–74, 1999."
REFERENCES,0.8761329305135952,"Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic
minority over-sampling technique. Journal of artiﬁcial intelligence research, 16:321–357, 2002."
REFERENCES,0.879154078549849,"Olivia Choudhury, Yoonyoung Park, Theodoros Salonidis, Aris Gkoulalas-Divanis, Issa Sylla, et al.
Predicting adverse drug reactions on distributed health data using federated learning. In AMIA An-
nual symposium proceedings, volume 2019, pp. 313. American Medical Informatics Association,
2019."
REFERENCES,0.8821752265861027,"Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced loss based
on effective number of samples. In Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition, pp. 9268–9277, 2019."
REFERENCES,0.8851963746223565,"Moming Duan, Duo Liu, Xianzhang Chen, Yujuan Tan, Jinting Ren, Lei Qiao, and Liang Liang.
Astraea: Self-balancing federated learning for improving classiﬁcation accuracy of mobile deep
learning applications. In 2019 IEEE 37th international conference on computer design (ICCD),
pp. 246–254. IEEE, 2019."
REFERENCES,0.8882175226586103,"Jack Goetz, Kshitiz Malik, Duc Bui, Seungwhan Moon, Honglei Liu, and Anuj Kumar. Active
federated learning. arXiv preprint arXiv:1909.12641, 2019."
REFERENCES,0.8912386706948641,"Hongyu Guo and Herna L Viktor.
Learning from imbalanced data sets with boosting and data
generation: the databoost-im approach. ACM Sigkdd Explorations Newsletter, 6(1):30–39, 2004."
REFERENCES,0.8942598187311178,"Haibo He and Edwardo A Garcia. Learning from imbalanced data. IEEE Transactions on knowledge
and data engineering, 21(9):1263–1284, 2009."
REFERENCES,0.8972809667673716,"Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data
distribution for federated visual classiﬁcation. arXiv preprint arXiv:1909.06335, 2019."
REFERENCES,0.9003021148036254,"Chen Huang, Yining Li, Chen Change Loy, and Xiaoou Tang. Learning deep representation for
imbalanced classiﬁcation. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 5375–5384, 2016."
REFERENCES,0.9033232628398792,"Justin M Johnson and Taghi M Khoshgoftaar. Survey on deep learning with class imbalance. Journal
of Big Data, 6(1):1–54, 2019."
REFERENCES,0.9063444108761329,Published as a conference paper at ICLR 2022
REFERENCES,0.9093655589123867,"Peter Kairouz, H Brendan McMahan, Brendan Avent, Aur´elien Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Ad-
vances and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019."
REFERENCES,0.9123867069486404,"Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In
International Conference on Machine Learning, pp. 5132–5143. PMLR, 2020."
REFERENCES,0.9154078549848943,Alex Krizhevsky et al. Learning multiple layers of features from tiny images. 2009.
REFERENCES,0.918429003021148,"Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998."
REFERENCES,0.9214501510574018,"Geun Hyeong Lee and Soo-Yong Shin. Federated learning on clinical benchmark data: Performance
assessment. J Med Internet Res, 22(10):e20891, Oct 2020. ISSN 1438-8871. doi: 10.2196/20891.
URL http://www.jmir.org/2020/10/e20891/."
REFERENCES,0.9244712990936556,"Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of
fedavg on non-iid data, 2020."
REFERENCES,0.9274924471299094,"Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll´ar. Focal loss for dense
object detection. In Proceedings of the IEEE international conference on computer vision, pp.
2980–2988, 2017."
REFERENCES,0.9305135951661632,"Charles X Ling and Victor S Sheng.
Cost-sensitive learning and the class imbalance problem.
Encyclopedia of machine learning, 2011:231–235, 2008."
REFERENCES,0.9335347432024169,"Xu-Ying Liu, Jianxin Wu, and Zhi-Hua Zhou. Exploratory undersampling for class-imbalance learn-
ing. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 39(2):539–550,
2008."
REFERENCES,0.9365558912386707,"Guodong Long, Yue Tan, Jing Jiang, and Chengqi Zhang.
Federated Learning for Open
Banking, pp. 240–254.
Springer International Publishing, Cham, 2020.
ISBN 978-3-030-
63076-8.
doi:
10.1007/978-3-030-63076-8 17.
URL https://doi.org/10.1007/
978-3-030-63076-8_17."
REFERENCES,0.9395770392749244,"Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Ar-
cas.
Communication-Efﬁcient Learning of Deep Networks from Decentralized Data.
In
Aarti Singh and Jerry Zhu (eds.), Proceedings of the 20th International Conference on Artiﬁ-
cial Intelligence and Statistics, volume 54 of Proceedings of Machine Learning Research, pp.
1273–1282. PMLR, 20–22 Apr 2017. URL https://proceedings.mlr.press/v54/
mcmahan17a.html."
REFERENCES,0.9425981873111783,"Samira Pouyanfar, Yudong Tao, Anup Mohan, Haiman Tian, Ahmed S Kaseb, Kent Gauen, Ryan
Dailey, Sarah Aghajanzadeh, Yung-Hsiang Lu, Shu-Ching Chen, et al. Dynamic sampling in
convolutional neural networks for imbalanced data classiﬁcation. In 2018 IEEE conference on
multimedia information processing and retrieval (MIPR), pp. 112–117. IEEE, 2018."
REFERENCES,0.945619335347432,"Alejandro Ribeiro.
Optimal resource allocation in wireless communication and networking.
EURASIP Journal on Wireless Communications and Networking, 2012(1):1–19, 2012."
REFERENCES,0.9486404833836858,"Abhijit Guha Roy, Shayan Siddiqui, Sebastian P¨olsterl, Nassir Navab, and Christian Wachinger.
Braintorrent: A peer-to-peer environment for decentralized federated learning. arXiv preprint
arXiv:1905.06731, 2019."
REFERENCES,0.9516616314199395,"Yanmin Sun, Mohamed S Kamel, Andrew KC Wong, and Yang Wang. Cost-sensitive boosting for
classiﬁcation of imbalanced data. Pattern recognition, 40(12):3358–3378, 2007."
REFERENCES,0.9546827794561934,"Fabio Tardella. A new proof of the lyapunov convexity theorem. SIAM journal on control and
optimization, 28(2):478–481, 1990."
REFERENCES,0.9577039274924471,"Jason Van Hulse, Taghi M Khoshgoftaar, and Amri Napolitano. Experimental perspectives on learn-
ing from imbalanced data. In Proceedings of the 24th international conference on Machine learn-
ing, pp. 935–942, 2007."
REFERENCES,0.9607250755287009,Published as a conference paper at ICLR 2022
REFERENCES,0.9637462235649547,"Hao Wang, Zakhary Kaplan, Di Niu, and Baochun Li. Optimizing federated learning on non-iid data
with reinforcement learning. In IEEE INFOCOM 2020-IEEE Conference on Computer Commu-
nications, pp. 1698–1707. IEEE, 2020a."
REFERENCES,0.9667673716012085,"Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H. Vincent Poor. Tackling the objective
inconsistency problem in heterogeneous federated optimization, 2020b."
REFERENCES,0.9697885196374623,"Lixu Wang, Shichao Xu, Xiao Wang, and Qi Zhu. Addressing class imbalance in federated learning.
In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 35, pp. 10165–10173,
2021a."
REFERENCES,0.972809667673716,"Shoujin Wang, Wei Liu, Jia Wu, Longbing Cao, Qinxue Meng, and Paul J Kennedy.
Training
deep neural networks on imbalanced data sets. In 2016 international joint conference on neural
networks (IJCNN), pp. 4368–4374. IEEE, 2016."
REFERENCES,0.9758308157099698,"Yi Wang, Imane Lahmam Bennani, Xiufeng Liu, Mingyang Sun, and Yao Zhou. Electricity con-
sumer characteristics identiﬁcation: A federated learning approach. IEEE Transactions on Smart
Grid, 12(4):3637–3647, 2021b. doi: 10.1109/TSG.2021.3066577."
REFERENCES,0.9788519637462235,"Qiong Wu, Xu Chen, Zhi Zhou, and Junshan Zhang. Fedhome: Cloud-edge based personalized
federated learning for in-home health monitoring. IEEE Transactions on Mobile Computing, pp.
1–1, 2020. doi: 10.1109/TMC.2020.3045266."
REFERENCES,0.9818731117824774,"Wenchao Xia, Tony QS Quek, Kun Guo, Wanli Wen, Howard H Yang, and Hongbo Zhu. Multi-
armed bandit-based client scheduling for federated learning. IEEE Transactions on Wireless Com-
munications, 19(11):7108–7123, 2020."
REFERENCES,0.9848942598187311,"Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms, 2017."
REFERENCES,0.9879154078549849,"Miao Yang, Akitanoshou Wong, Hongbin Zhu, Haifeng Wang, and Hua Qian. Federated learning
with class imbalance reduction, 2020."
REFERENCES,0.9909365558912386,"Wensi Yang, Yuhang Zhang, Kejiang Ye, Li Li, and Cheng-Zhong Xu. Ffd: A federated learning
based method for credit card fraud detection. In Keke Chen, Sangeetha Seshadri, and Liang-Jie
Zhang (eds.), Big Data – BigData 2019, pp. 18–32, Cham, 2019. Springer International Publish-
ing. ISBN 978-3-030-23551-2."
REFERENCES,0.9939577039274925,"Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, and
Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. In Interna-
tional Conference on Machine Learning, pp. 7252–7261. PMLR, 2019."
REFERENCES,0.9969788519637462,"Xinwei Zhang, Mingyi Hong, Sairaj Dhople, Wotao Yin, and Yang Liu. Fedpd: A federated learning
framework with optimal rates and adaptivity to non-iid data. arXiv preprint arXiv:2005.11418,
2020."
