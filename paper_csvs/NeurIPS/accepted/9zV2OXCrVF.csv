Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0023094688221709007,"1In clustering, the choice of initial centers is crucial for the convergence speed of
the algorithms. We propose a new initialization scheme for the k-median problem
in the general metric space (e.g., discrete space induced by graphs), based on
the construction of metric embedding tree structure of the data. We propose a
novel and efficient search algorithm which finds initial centers that can be used
subsequently for the local search algorithm. The so-called HST initialization
method can produce initial centers achieving lower error than those from another
popular method k-median++, also with higher efficiency when k is not too small.
Our HST initialization are then extended to the setting of differential privacy
(DP) to generate private initial centers. We show that the error of applying DP
local search followed by our private HST initialization improves prior results on
the approximation error, and approaches the lower bound within a small factor.
Experiments demonstrate the effectiveness of our proposed methods."
INTRODUCTION,0.004618937644341801,"1
Introduction"
INTRODUCTION,0.006928406466512702,"Clustering is an important classic problem in unsupervised learning that has been widely studied in
statistics, data mining, machine learning, network analysis, etc. (Punj and Stewart, 1983; Dhillon
and Modha, 2001; Banerjee et al., 2005; Berkhin, 2006; Abbasi and Younis, 2007). The objective of
clustering is to divide a set of data points into clusters, such that items within the same cluster exhibit
similarities, while those in different clusters distinctly differ. This is concretely measured by the sum
of distances (or squared distances) between each point to its nearest cluster center. One conventional
notion to evaluate a clustering algorithms is: with high probability, cost(C, D) ≤γOPTk(D) + ξ,
where C is the centers output by the algorithm and cost(C, D) is a cost function defined for C on
dataset D. OPTk(D) is the cost of optimal clustering solution on D. When everything is clear from
context, we will use OPT for short. Here, γ is called multiplicative error and ξ is called additive
error. Alternatively, we may also use the notion of expected cost."
INTRODUCTION,0.009237875288683603,"Two popularly studied clustering problems are 1) the k-median problem, and 2) the k-means problem.
The origin of k-median dates back to the 1970’s (e.g., Kaufman et al. (1977)), where one tries to find
the best location of facilities that minimizes the cost measured by the distance between clients and
facilities. Formally, given a set of points D and a distance measure, the goal is to find k center points
minimizing the sum of absolute distances of each sample point to its nearest center. In k-means, the
objective is to minimize the sum of squared distances instead. There are two general frameworks
for clustering. One heuristic is the Lloyd’s algorithm (Lloyd, 1982), which is built upon an iterative
distortion minimization approach. In most cases, this method can only be applied to numerical data,"
INTRODUCTION,0.011547344110854504,1This work was initially submitted in 2020 and was made public in 2021.
INTRODUCTION,0.013856812933025405,"typically in the (continuous) Euclidean space. Clustering in general metric spaces (discrete spaces) is
also important and useful when dealing with, for example, the graph data, where Lloyd’s method
is no longer applicable. A more generally applicable approach, the local search method (Kanungo
et al., 2002; Arya et al., 2004), has also been widely studied. It iteratively finds the optimal swap
between the center set and non-center data points to keep lowering the cost. Local search can achieve
a constant approximation (i.e., γ = O(1)) to the optimal solution (Arya et al., 2004). For general
metric spaces, the state of the art approximation ratio is 2.675 for k-median Byrka et al. (2015) and
6.357 for k-means Ahmadian et al. (2017)."
INTRODUCTION,0.016166281755196306,"Initialization of cluster centers. It is well-known that the performance of clustering can be highly
sensitive to initialization. If clustering starts with good initial centers with small approximation error,
the algorithm may use fewer iterations to find a better solution. The k-median++ algorithm (Arthur
and Vassilvitskii, 2007) iteratively selects k data points as initial centers, favoring distant points in a
probabilistic way, such that the initial centers tend to be well spread over the data points (i.e., over
different clusters). The produced initial centers are proved to have O(log k) multiplicative error.
Follow-up works further improved its efficiency and scalability, e.g., Bahmani et al. (2012); Bachem
et al. (2016); Lattanzi and Sohler (2019); Choo et al. (2020); Cohen-Addad et al. (2021); Grunau
et al. (2023); Fan et al. (2023). In this work, we propose a new initialization framework, called HST
initialization, which is built upon a novel search algorithm on metric embedding trees constructed
from the data. Our method achieves improved approximation error compared with k-median++.
Moreover, importantly, our initialization scheme can be conveniently combined with the notion of
differential privacy (DP) to protect the data privacy."
INTRODUCTION,0.018475750577367205,"Clustering with Differential Privacy. The concept of differential privacy (Dwork, 2006; McSherry
and Talwar, 2007) has been popular to rigorously define and resolve the problem of keeping useful
information for machine learning models, while protecting privacy for each individual. DP has been
adopted to a variety of algorithms and tasks, such as regression, classification, principle component
analysis, graph distance release, matrix completion, optimization, and deep learning (Chaudhuri and
Monteleoni, 2008; Chaudhuri et al., 2011; Abadi et al., 2016; Ge et al., 2018; Wei et al., 2020; Dong
et al., 2022; Fan and Li, 2022; Fan et al., 2022; Fang et al., 2023; Li and Li, 2023a,b). Private k-means
clustering has also been widely studied, e.g., Feldman et al. (2009); Nock et al. (2016); Feldman
et al. (2017), mostly in the continuous Euclidean space. Balcan et al. (2017) considered identifying a
good candidate set (in a private manner) of centers before applying private local search, which yields
O(log3 n) multiplicative error and O((k2 + d) log5 n) additive error. Later on, the private Euclidean
k-means error ais further improved by Stemmer and Kaplan (2018), with more advanced candidate
set selection. Huang and Liu (2018) gave an optimal algorithm in terms of minimizing Wasserstein
distance under some data separability condition."
INTRODUCTION,0.020785219399538105,"For private k-median clustering, Feldman et al. (2009); Ghazi et al. (2020) considered the problem
in high dimensional Euclidean space. However, it is rather difficult to extend their analysis to more
general metrics in discrete spaces (e.g., on graphs). The strategy of Balcan et al. (2017) to form a
candidate center set could as well be adopted to k-median, which leads to O(log3/2 n) multiplicative
error and O((k2 + d) log3 n) additive error in the Euclidean space where n is the sample size. In
discrete space, Gupta et al. (2010) proposed a private method for the classical local search heuristic,
which applies to both k-medians and k-means. To cast privacy on each swapping step, the authors
applied the exponential mechanism of McSherry and Talwar (2007). Their method produced an
ϵ-differentially private solution with cost 6OPT + O(△k2 log2 n/ϵ), where △is the diameter of
the point set. In this work, we will show that our proposed HST initialization can improve the DP
local search for k-median of Gupta et al. (2010) in terms of both approximation error and efficiency.
Stemmer and Kaplan (2018); Jones et al. (2021) proposed (ϵ, δ)-differentially private solution also
with constant multiplicative error but smaller additive error."
INTRODUCTION,0.023094688221709007,The main contributions of this work include the following:
INTRODUCTION,0.025404157043879907,"• We introduce the Hierarchically Well-Separated Tree (HST) as an initialization tool for the
k-median clustering problem. We design an efficient sampling strategy to select the initial
center set from the tree, with an approximation factor O(log min{k, △}) in the non-private
setting, which is O(log min{k, d}) when log △= O(log d). This improves the O(log k)
error of k-median++. Moreover, the complexity of our HST based method can be smaller
than that of k-median++ when the number of clusters k is not too small (k ≥log n), which
is a common scenario in practical applications."
INTRODUCTION,0.02771362586605081,"• We propose a differentially private version of HST initialization under the setting of Gupta
et al. (2010) in discrete metric space. The so-called DP-HST algorithm finds initial centers
with O(log n) multiplicative error and O(ϵ−1△k2 log2 n) additive error. Moreover, run-
ning DP-local search starting from this initialization gives O(1) multiplicative error and
O(ϵ−1△k2(log log n) log n) additive error, which improves previous results towards the
well-known lower bound O(ϵ−1△k log(n/k)) on the additive error of DP k-median (Gupta
et al., 2010) within a small O(k log log n) factor. This is the first clustering initialization
method with ϵ-differential privacy guarantee and improved error rate in general metric space.
• We conduct experiments on simulated and real-world datasets to demonstrate the effective-
ness of our methods. In both non-private and private settings, our proposed HST-based
approach achieves smaller cost at initialization than k-median++, which may also lead to
improvements in the final clustering quality."
BACKGROUND AND SETUP,0.03002309468822171,"2
Background and Setup"
BACKGROUND AND SETUP,0.03233256351039261,"The definition of differential privacy (DP) is as follows.
Definition 2.1 (Differential Privacy (DP) (Dwork, 2006)). If for any two adjacent datasets D and D′
with symmetric difference of size one and any O ⊂Range(A), an algorithm A with map f satisfies"
BACKGROUND AND SETUP,0.03464203233256351,"Pr[A(D) ∈O] ≤eϵPr[A(D′) ∈O],"
BACKGROUND AND SETUP,0.03695150115473441,then algorithm A is said to be ϵ-differentially private (ϵ-DP).
BACKGROUND AND SETUP,0.03926096997690531,"Intuitively, DP requires that after removing any data point from D (e.g., a node in a graph), the output
of D′ should not be too different from that of the original dataset D. The Laplace mechanism adds
Laplace(η(f)/ϵ) noise to the output where η(f) = sup|D−D′|=1 |f(D) −f(D′)| is the sensitivity
of f, which is known to achieve ϵ-DP. The exponential mechanism is also a tool for many DP
algorithms with discrete outputs. Let O be the output domain. The utility function q : D × O →R is
what we aim to maximize. The exponential mechanism outputs an element o ∈O with probability
P[A(D) = o] ∝exp( ϵq(D,o)"
BACKGROUND AND SETUP,0.04157043879907621,2η(q) ). Both mechanisms will be used in our paper.
K-MEDIAN CLUSTERING AND LOCAL SEARCH,0.04387990762124711,"2.1
k-Median Clustering and Local Search"
K-MEDIAN CLUSTERING AND LOCAL SEARCH,0.046189376443418015,"In this paper, we follow the classic problem setting in the metric clustering literature, e.g. Arya et al.
(2004); Gupta et al. (2010). Specifically, the definitions of metric k-median clustering problem (DP
and non-DP) are stated as follow.
Definition 2.2 (k-median). Given a universe point set U and a metric ρ : U × U →R, the goal of
k-median to pick F ⊆U with |F| = k to minimize"
K-MEDIAN CLUSTERING AND LOCAL SEARCH,0.04849884526558892,"k-median:
costk(F, U) =
X"
K-MEDIAN CLUSTERING AND LOCAL SEARCH,0.050808314087759814,"v∈U
min
f∈F ρ(v, f).
(1)"
K-MEDIAN CLUSTERING AND LOCAL SEARCH,0.053117782909930716,Let D ⊆U be a set of “demand points”. The goal of DP k-median is to minimize
K-MEDIAN CLUSTERING AND LOCAL SEARCH,0.05542725173210162,"DP k-median:
costk(F, D) =
X"
K-MEDIAN CLUSTERING AND LOCAL SEARCH,0.057736720554272515,"v∈D
min
f∈F ρ(v, f),
(2)"
K-MEDIAN CLUSTERING AND LOCAL SEARCH,0.06004618937644342,"and the output F is required to be ϵ-differentially private with respect to D. We may drop “F” and
use “costk(U)” or “costk(D)” if there is no risk of ambiguity."
K-MEDIAN CLUSTERING AND LOCAL SEARCH,0.06235565819861432,"Note that in Definition 2.2, our aim is to protect the privacy of a subset D ⊂U. To better understand
the motivation and application scenario, we provide a real-world example as below.
Example 2.3. Consider U to be the universe of all users in a social network (e.g., Facebook, LinkedIn,
etc.). Each user (account) has some public information (e.g., name, gender, interests, etc.), but also
has some private personal data that can only be seen by the data server. Let D be a set of users
grouped by some feature that might be set as private. Suppose a third party plans to collaborate with
the most influential users in D for e.g., commercial purposes, thus requesting the cluster centers of D.
In this case, we need a differentially private algorithm to safely release the centers, while protecting
the individuals in D from being identified (since the membership of D is private)."
K-MEDIAN CLUSTERING AND LOCAL SEARCH,0.06466512702078522,"Algorithm 1: Local search for k-median clustering (Arya et al., 2004)"
K-MEDIAN CLUSTERING AND LOCAL SEARCH,0.06697459584295612,"Input: Data points U, parameter k, constant α
Initialization: Randomly select k points from U as initial center set F
while ∃x ∈F, y ∈U s.t. cost(F −{x} + {y}) ≤(1 −α/k)cost(F) do"
K-MEDIAN CLUSTERING AND LOCAL SEARCH,0.06928406466512702,"Select (x, y) ∈Fi × (D \ Fi) with arg minx,y{cost(F −{x} + {y})}
Swap operation: F ←F −{x} + {y}
Output: Center set F"
K-MEDIAN CLUSTERING AND LOCAL SEARCH,0.07159353348729793,"The (non-private) local search procedure for k-median proposed by Arya et al. (2004) is summarized
in Algorithm 1. First, we randomly pick k points in U as the initial centers. In each iteration, we
search over all x ∈F and y ∈U, and do the swap F ←F −{x} + {y} such that the new centers
improve the cost the most, and if the improvement is more than (1 −α/k) for some α > 0 2. We
repeat the procedure until no such swap exists. Arya et al. (2004) showed that the output center set F
achieves 5 approximation error to the optimal solution, i.e., cost(F) ≤5OPT."
K-MEDIAN CLUSTERING AND LOCAL SEARCH,0.07390300230946882,"2.2
k-median++ Initialization"
K-MEDIAN CLUSTERING AND LOCAL SEARCH,0.07621247113163972,"Although local search is able to find a solution with constant error, it takes O(n2) per iteration (Re-
sende and Werneck, 2007) in expected O(k log n) steps (which gives total complexity O(kn2 log n))
when started from a random center set, which would be slow for large datasets. Indeed, we do not
need such complicated/meticulous algorithm to reduce the cost at the beginning, i.e., when the cost is
large. To accelerate the process, efficient initialization methods find a “roughly” good center set as
the starting point for local search. In the paper, we compare our new initialization scheme mainly
with a popular (and perhaps the most well-known) initialization method, the k-median++ (Arthur
and Vassilvitskii, 2007) 3 as presented in Algorithm 2. The output centers C by k-median++ achieve
O(log k) approximation error with time complexity O(nk). Starting from the initialization, we only
need to run O(k log log k) steps of the computationally heavy local search to reach a constant error
solution. Thus, initialization may greatly improve the clustering efficiency."
K-MEDIAN CLUSTERING AND LOCAL SEARCH,0.07852193995381063,"Algorithm 2: k-median++ initialization (Arthur and Vassilvitskii, 2007)"
K-MEDIAN CLUSTERING AND LOCAL SEARCH,0.08083140877598152,"Input: Data points U, number of centers k
Randomly pick a point c1 ∈U and set F = {c1}
for i = 2, ..., k do"
K-MEDIAN CLUSTERING AND LOCAL SEARCH,0.08314087759815242,"Select ci = u ∈U with probability
ρ(u,F )
P"
K-MEDIAN CLUSTERING AND LOCAL SEARCH,0.08545034642032333,"u′∈U ρ(u′,F )
F = F ∪{ci}
Output: k-median++ initial center set F"
K-MEDIAN CLUSTERING AND LOCAL SEARCH,0.08775981524249422,"3
Initialization via Hierarchical Well-Separated Tree (HST)"
K-MEDIAN CLUSTERING AND LOCAL SEARCH,0.09006928406466513,"In this section, we propose our new initialization scheme for k-median clustering, and provide our
analysis in the non-private case solving (1). The idea is based on the metric embedding theory. We
will start with an introduction to the main tool used in our approach."
K-MEDIAN CLUSTERING AND LOCAL SEARCH,0.09237875288683603,"3.1
Hierarchically Well-Separated Tree (HST)"
K-MEDIAN CLUSTERING AND LOCAL SEARCH,0.09468822170900693,"In this paper, for an L-level tree, we will count levels in a descending order down the tree. We
use hv to denote the level of v, and let ni be the number of nodes at level i. The Hierarchically
Well-Separated Tree (HST) is based on the padded decompositions of a general metric space in a
hierarchical manner (Fakcharoenphol et al., 2004). Let (U, ρ) be a metric space with |U| = n, and
we will refer to this metric space without specific clarification. A β–padded decomposition of U"
K-MEDIAN CLUSTERING AND LOCAL SEARCH,0.09699769053117784,"2Arya et al. (2004) chooses the first swap that improves the cost by (1−α/k), instead of picking the smallest
cost among all such swaps as in Algorithm 1. Both methods are valid and yield the same approximation error.
3In the original paper of Arthur and Vassilvitskii (2007), the k-median++ algorithm was called D1-sampling. 1 2 3 4 5
6 7
8 9 10"
K-MEDIAN CLUSTERING AND LOCAL SEARCH,0.09930715935334873,"4,2,…,10 10 10"
K-MEDIAN CLUSTERING AND LOCAL SEARCH,0.10161662817551963,"[1,2,3]
[6,4,5,7,8,9]"
K-MEDIAN CLUSTERING AND LOCAL SEARCH,0.10392609699769054,"1
[3,2]
[5,4,6]
[8,7]
9"
K-MEDIAN CLUSTERING AND LOCAL SEARCH,0.10623556581986143,Level 3
K-MEDIAN CLUSTERING AND LOCAL SEARCH,0.10854503464203233,Level 2
K-MEDIAN CLUSTERING AND LOCAL SEARCH,0.11085450346420324,Level 1
K-MEDIAN CLUSTERING AND LOCAL SEARCH,0.11316397228637413,"Figure 1: An example of a 3-level padded decomposition and the corresponding 2-HST. Left: The
thickness of the ball represents the level. The colors correspond to different levels in the HST in the
right panel. “△”’s are the center nodes of partitions (balls), and “×”’s are the non-center data points.
Right: The 2-HST generated from the padded decomposition. Bold indices represent the centers."
K-MEDIAN CLUSTERING AND LOCAL SEARCH,0.11547344110854503,"is a probabilistic partition of U such that the diameter of each cluster Ui ∈U is at most β, i.e.,
ρ(u, v) ≤β, ∀u, v ∈Ui, i = 1, ..., k. The formal definition of HST is given as below.
Definition 3.1. Assume minu,v∈U ρ(u, v) = 1 and denote the diameter △= maxu,v∈U ρ(u, v). An
α-Hierarchically Well-Separated Tree (α-HST) with depth L is an edge-weighted rooted tree T, such
that an edge between any pair of two nodes of level i −1 and level i has length at most △/αL−i."
K-MEDIAN CLUSTERING AND LOCAL SEARCH,0.11778290993071594,"Our analysis will consider α = 2-HST for conciseness, since α only affects the constants in our
theoretical analysis. Figure 1 is an example 2-HST (right panel) with L = 3 levels, along with its
underlying padded decompositions (left panel). Using Algorithm 3, a 2-HST can be built as follows:
we first find a padded decomposition PL = {PL,1, ..., PL,nL} of U with parameter β = △/2.
The center of each partition in PL,j serves as a root node in level L. Then, we re-do a padded
decomposition for each partition PL,j, to find sub-partitions with diameter β = △/4, and set the
corresponding centers as the nodes in level L −1, and so on. Each partition at level i is obtained
with β = △/2L−i. This process proceeds until a node has a single point (leaf), or a pre-specified
tree depth is reached. It is worth mentioning that, Blelloch et al. (2017) proposed an efficient HST
construction in O(m log n) time, where n and m are the number of nodes and edges in a graph,
respectively. Therefore, the construction of HST can be very efficient in practice."
K-MEDIAN CLUSTERING AND LOCAL SEARCH,0.12009237875288684,"The first step of our method is to embed the data points into an HST (see Algorithm 4). Next, we
will describe our new strategy to search for the initial centers on the tree (w.r.t. the tree metric).
Before moving on, it is worth mentioning that, there are polynomial time algorithms for computing
an exact k-median solution in the tree metric (Tamir (1996); Shah (2003)). However, the dynamic
programming algorithms have high complexity (e.g., O(kn2)), making them unsuitable for the
purpose of fast initialization. Moreover, it is unknown how to apply them effectively to the private
case. The three key merits of our new algorithm are: (1) It is more efficient than k-median++ when k
is not too small, which is a very common scenario in practice; (2) It achieves O(1) approximation
error in the tree metric; (3) It can be easily extended to incorporating differential privacy (DP)."
K-MEDIAN CLUSTERING AND LOCAL SEARCH,0.12240184757505773,"Algorithm 3: Build 2-HST(U, L)"
K-MEDIAN CLUSTERING AND LOCAL SEARCH,0.12471131639722864,"Input: Data points U with diameter △, L
Randomly pick a point in U as the root node of T
Let r = △/2
Apply a permutation π on U
// so points will be chosen in a random sequence
for each v ∈U do"
K-MEDIAN CLUSTERING AND LOCAL SEARCH,0.12702078521939955,"Set Cv = [v]
for each u ∈U do"
K-MEDIAN CLUSTERING AND LOCAL SEARCH,0.12933025404157045,"Add u ∈U to Cv if d(v, u) ≤r and u /∈S"
K-MEDIAN CLUSTERING AND LOCAL SEARCH,0.13163972286374134,"v′̸=v Cv′
Set the non-empty clusters Cv as the children nodes of T
for each non-empty cluster Cv do"
K-MEDIAN CLUSTERING AND LOCAL SEARCH,0.13394919168591224,"Run 2-HST(Cv, L −1) to extend the tree T; stop until L levels or reaching a leaf node
Output: 2-HST T"
HST INITIALIZATION ALGORITHM,0.13625866050808313,"3.2
HST Initialization Algorithm"
HST INITIALIZATION ALGORITHM,0.13856812933025403,"Let L = log ∆and suppose T is a level-L 2-HST in (U, ρ), where we assume L is an integer. For
a node v at level i, we use T(v) to denote the subtree rooted at v. Let Nv = |T(v)| be the number
of data points in T(v). The search strategy for the initial centers, NDP-HST initialization (“NDP”
stands for “Non-Differentially Private”), is presented in Algorithm 4 with two phases."
HST INITIALIZATION ALGORITHM,0.14087759815242495,"Subtree search. The first step is to identify the subtrees that contain the k centers. To begin with, k
initial centers C1 are picked from T who have the largest score(v) = N(v) · 2hv. This is intuitive,
since to get a good clustering, we typically want the ball surrounding each center to include more
data points. Next, we do a screening over C1: if there is any ancestor-descendant pair of nodes, we
remove the ancestor from C1. If the current size of C1 is smaller than k, we repeat the process until k
centers are chosen (we do not re-select nodes in C1 and their ancestors). This way, C1 contains k
root nodes of k disjoint subtrees."
HST INITIALIZATION ALGORITHM,0.14318706697459585,Algorithm 4: NDP-HST initialization
HST INITIALIZATION ALGORITHM,0.14549653579676675,"Input: U, △, k
Initialization: L = log △, C0 = ∅, C1 = ∅
Call Algorithm 3 to build a level-L 2-HST T using U
for each node v in T do"
HST INITIALIZATION ALGORITHM,0.14780600461893764,"Nv ←|U ∩T(v)|, score(v) ←Nv · 2hv
while |C1| < k do"
HST INITIALIZATION ALGORITHM,0.15011547344110854,"Add top (k −|C1|) nodes with highest score to C1
for each v ∈C1 do"
HST INITIALIZATION ALGORITHM,0.15242494226327943,"C1 = C1 \ {v}, if ∃v′ ∈C1 such that v′ is a descendant of v
C0 = FIND-LEAF(T, C1)
Output: Initial center set C0 ⊆U"
HST INITIALIZATION ALGORITHM,0.15473441108545036,"Leaf search. After we find C1 the set of k subtrees, the next step is to find the center in each subtree
using Algorithm 5 (“FIND-LEAF”). We employ a greedy search strategy, by finding the child node
with largest score level by level, until a leaf is found. This approach is intuitive since the diameter of
the partition ball exponentially decays with the level. Therefore, we are in a sense focusing more and
more on the region with higher density (i.e., with more data points)."
HST INITIALIZATION ALGORITHM,0.15704387990762125,The complexity of our search algorithm is given as follows. All proofs are placed in Appendix B.
HST INITIALIZATION ALGORITHM,0.15935334872979215,Proposition 3.2 (Complexity). Algorithm 4 takes O(dn log n) time in the Euclidean space.
HST INITIALIZATION ALGORITHM,0.16166281755196305,"Remark 3.3 (Comparison with k-median++). The complexity of of k-median++ is O(dnk) in the
Euclidean space (Arthur and Vassilvitskii, 2007). Our algorithm would be faster when k > log n,
which is a common scenario. Similar comparison also holds for general metrics."
APPROXIMATION ERROR OF HST INITIALIZATION,0.16397228637413394,"3.3
Approximation Error of HST Initialization"
APPROXIMATION ERROR OF HST INITIALIZATION,0.16628175519630484,"We provide the error analysis of our algorithm. Firstly, we show that the initial center set produced by
NDP-HST is already a good approximation to the optimal k-median solution. Let ρT (x, y) = dT (x, y)
denote the “2-HST metric” between x and y in the 2-HST T, where dT (x, y) is the tree distance
between nodes x and y in T. By Definition 3.1 and since △= 2L, in the analysis we assume"
APPROXIMATION ERROR OF HST INITIALIZATION,0.16859122401847576,"Algorithm 5: FIND-LEAF (T, C1)"
APPROXIMATION ERROR OF HST INITIALIZATION,0.17090069284064666,"Input: T, C1
Initialization: C0 = ∅
for each node v in C1 do"
APPROXIMATION ERROR OF HST INITIALIZATION,0.17321016166281755,while v is not a leaf node do
APPROXIMATION ERROR OF HST INITIALIZATION,0.17551963048498845,"v ←argw max{Nw, w ∈ch(v)}, where ch(v) denotes the children nodes of v
Add v to C0
Output: Initial center set C0 ⊆U"
APPROXIMATION ERROR OF HST INITIALIZATION,0.17782909930715934,"equivalently that the edge weight of the i-th level is 2i−1. The crucial step of our analysis is to
examine the approximation error in terms of the 2-HST metric, after which the error can be adapted
to the general metrics by the following qwll-known result.
Lemma 3.4 (Bartal (1996)). In a metric space (U, ρ) with |U| = n and diameter △, it holds that
∀x, y ∈U, E[ρT (x, y)] = O(min{log n, log △})ρ(x, y). In the Euclidean space Rd, E[ρT (x, y)] =
O(d)ρ(x, y), ∀x, y ∈U."
APPROXIMATION ERROR OF HST INITIALIZATION,0.18013856812933027,"Recall C0, C1 from Algorithm 4. We define"
APPROXIMATION ERROR OF HST INITIALIZATION,0.18244803695150116,"costT
k (U) =
X"
APPROXIMATION ERROR OF HST INITIALIZATION,0.18475750577367206,"y∈U
min
x∈C0 ρT (x, y),
(3)"
APPROXIMATION ERROR OF HST INITIALIZATION,0.18706697459584296,"costT
k
′(U, C1) =
min
|F ∩T (v)|=1,
∀v∈C1 X"
APPROXIMATION ERROR OF HST INITIALIZATION,0.18937644341801385,"y∈U
min
x∈F ρT (x, y),
(4)"
APPROXIMATION ERROR OF HST INITIALIZATION,0.19168591224018475,"OPT T
k (U) =
min
F ⊂U,|F |=k X"
APPROXIMATION ERROR OF HST INITIALIZATION,0.19399538106235567,"y∈U
min
x∈F ρT (x, y) ≡min
C′
1
costT
k
′(U, C′
1).
(5)"
APPROXIMATION ERROR OF HST INITIALIZATION,0.19630484988452657,"For simplicity, we will use costT
k
′(U) to denote costT
k
′(U, C1). Here, OPT T
k (5) is the cost of the
global optimal solution with the 2-HST metric. The last equivalence in (5) holds because the optimal
centers can always be located in k disjoint subtrees, as each leaf only contains one point. (3) is the
k-median cost with 2-HST metric of the output C0 of Algorithm 4. (4) is the optimal cost after the
subtrees are chosen. That is, it represents the minimal cost to pick one center from each subtree in
C1. We first bound the error of the subtree search step and the leaf search step, respectively.
Lemma 3.5 (Subtree search). costT
k
′(U) ≤5OPT T
k (U)."
APPROXIMATION ERROR OF HST INITIALIZATION,0.19861431870669746,"Lemma 3.6 (Leaf search). costT
k (U) ≤2costT
k
′(U)."
APPROXIMATION ERROR OF HST INITIALIZATION,0.20092378752886836,"Combining Lemma 3.5 and Lemma 3.6, we obtain:
Theorem 3.7 (2-HST error). Running Algorithm 4, we have costT
k (U) ≤10OPT T
k (U)."
APPROXIMATION ERROR OF HST INITIALIZATION,0.20323325635103925,"Thus, HST-initialization produces an O(1) approximation to the optimal cost in the 2-HST metric.
Define costk(U) as (1) for our HST centers, and the optimal cost w.r.t. ρ as"
APPROXIMATION ERROR OF HST INITIALIZATION,0.20554272517321015,"OPTk(U) = min
|F |=k X"
APPROXIMATION ERROR OF HST INITIALIZATION,0.20785219399538107,"y∈U
min
x∈F ρ(x, y).
(6)"
APPROXIMATION ERROR OF HST INITIALIZATION,0.21016166281755197,"We have the following result based on Lemma 3.4.
Theorem 3.8. In the general metric space, the expected k-median cost of NDP-HST (Algorithm 4) is
E[costk(U)] = O(min{log n, log △})OPTk(U).
Remark 3.9. In the Euclidean space, Makarychev et al. (2019) showed that using O(log k) random
projections suffices for k-median to achieve O(1) error. Thus, if log △= O(log d), by Lemma 3.4,
HST initialization is able to achieve O(log(min{d, k})) error, which is better than O(log k) of
k-median++ (Arthur and Vassilvitskii, 2007) when d is small."
APPROXIMATION ERROR OF HST INITIALIZATION,0.21247113163972287,"NDP-HST Local Search. We are interested in the approximation quality of standard local search
(Algorithm 1), when the initial centers are produced by our NDP-HST.
Theorem 3.10. When initialized by NDP-HST, local search achieves O(1) approximation error in
expected O(k log log min{n, △}) number of iterations for input in general metric space."
APPROXIMATION ERROR OF HST INITIALIZATION,0.21478060046189376,"We remark that the initial centers found by NDP-HST can be used for k-means clustering analogously.
For general metrics, E[costkm(U)] = O(min{log n, log △})2OPTkm(U) where costkm(U) is the
optimal k-means cost. See Appendix C for more detaileds."
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.21709006928406466,"4
HST Initialization with Differential Privacy"
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.21939953810623555,"In this section, we consider initialization and clustering with differential privacy (DP). Recall (2)
that in this problem, U is the universe of data points, and D ⊂U is a demand set that needs to
be clustered with privacy. Since U is public, simply running initialization algorithms on U would"
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.22170900692840648,Algorithm 6: DP-HST initialization
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.22401847575057737,"Input: U, D, △, k, ϵ
Build a level-L 2-HST T based on input U
for each node v in T do"
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.22632794457274827,"Nv ←|D ∩T(v)|, ˆ
Nv ←Nv + Lap(2(L−hv)/ϵ), score(v) ←ˆN(v) · 2hv"
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.22863741339491916,"Based on ˆNv, apply the same strategy as Algorithm 4: find C1; C0 = FIND-LEAF(C1)
Output: Private initial center set C0 ⊆U"
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.23094688221709006,Algorithm 7: DP-HST local search
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.23325635103926096,"Input: U, demand points D ⊆U, parameter k, ϵ, T
Initialization: F1 the private initial centers generated by Algorithm 6 with privacy ϵ/2
Set parameter ϵ′ =
ϵ
4△(T +1)
for i = 1 to T do"
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.23556581986143188,"Select (x, y) ∈Fi × (V \ Fi) with prob. proportional to exp(−ϵ′ × (cost(Fi −{x} + {y}))
Let Fi+1 ←Fi −{x} + {y}
Select j from {1, 2, ..., T + 1} with probability proportional to exp(−ϵ′ × cost(Fj))
Output: F = Fj the private center set"
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.23787528868360278,"preserve the privacy of D. However, 1) this might be too expensive; 2) in many cases one would
probably want to incorporate some information about D in the initialization, since D could be a very
imbalanced subset of U. For example, D may only contain data points from one cluster, out of tens
of clusters in U. In this case, initialization on U is likely to pick initial centers in multiple clusters,
which would not be helpful for clustering on D."
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.24018475750577367,"Next, we show how our proposed HST initialization can be easily combined with differential privacy
and at the same time contains useful information about the demand set D, leading to improved
approximation error (Theorem 4.3). Again, suppose T is an L = log △-level 2-HST of universe U in
a general metric space. Denote Nv = |T(v) ∩D| for a node point v. Our private HST initialization
(DP-HST) is similar to the non-private Algorithm 4. To gain privacy, we perturb Nv by adding i.i.d.
Laplace noise: ˆ
Nv = Nv + Lap(2(L−hv)/ϵ), where Lap(2(L−hv)/ϵ) is a Laplace random number
with rate 2(L−hv)/ϵ. We will use the perturbed ˆNv for node sampling instead of the true value Nv, as
described in Algorithm 6. The DP guarantee of this initialization scheme is straightforward by the
composition theory (Dwork, 2006).
Theorem 4.1. Algorithm 6 is ϵ-differentially private."
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.24249422632794457,"Proof. For each level i, the subtrees T(v, i) are disjoint to each other. The privacy budget used in
i-th level is ϵ/2(L−i), so by composition the total privacy budget is P"
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.24480369515011546,i ϵ/2(L−i) < ϵ.
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.2471131639722864,"Theorem 4.2. Algorithm 6 finds initial centers such that
E[costk(D)] = O(log n)(OPTk(D) + kϵ−1△log n)."
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.24942263279445728,"DP-HST Local Search. Similarly, we can use private HST initialization to improve the performance
of private k-median local search, which is presented in Algorithm 7. After DP-HST initialization, the
DP local search procedure follows Gupta et al. (2010) using the exponential mechanism.
Theorem 4.3. Algorithm 7 achieves ϵ-differential privacy. The output centers achieve costk(D) ≤
6OPTk(D)+O(ϵ−1k2△(log log n) log n) in O(k log log n) iterations with probability (1−
1
poly(n))."
HST INITIALIZATION WITH DIFFERENTIAL PRIVACY,0.2517321016166282,"In prior literature, the DP local search with random initialization (Gupta et al., 2010) has 6 multiplica-
tive error and O(ϵ−1△k2 log2 n) additive error. Our result improves the log n term to log log n in
the additive error. Meanwhile, the number of iterations needed is improved from T = O(k log n) to
O(k log log n) (see Appendix A for an empirical justification). Notably, it has been shown in Gupta
et al. (2010) that for k-median problem, the lower bounds on the multiplicative and additive error of
any ϵ-DP algorithm are O(1) and O(ϵ−1△k log(n/k)), respectively. Our result matches the lower
bound on the multiplicative error, and the additive error is only worse than the bound by a factor of
O(k log log n). To our knowledge, Theorem 4.3 is the first result in the literature to improve the error
of DP local search in general metric space."
NUMERICAL RESULTS,0.2540415704387991,"5
Numerical Results"
DATASETS AND ALGORITHMS,0.25635103926096997,"5.1
Datasets and Algorithms"
DATASETS AND ALGORITHMS,0.2586605080831409,"Discrete Euclidean space. Following previous work, we test k-median clustering on the MNIST
hand-written digit dataset (LeCun et al., 1998) with 10 natural clusters (digit 0 to 9). We set U as
10000 randomly chosen data points. We choose the demand set D using two strategies: 1) “balance”,
where we randomly choose 500 samples from U; 2) “imbalance”, where D contains 500 random
samples from U only from digit “0” and “8” (two clusters). We note that, the imbalanced D is a very
practical setting in real-world scenarios, where data are typically not uniformly distributed. On this
dataset, we test clustering with both l1 and l2 distance as the underlying metric."
DATASETS AND ALGORITHMS,0.26096997690531176,"Metric space induced by graph. Random graphs have been widely considered in testing k-median
methods (Balcan et al., 2013; Todo et al., 2019). Our construction of graphs follows a similar
approach as the synthetic pmedinfo graphs provided by the popular OR-Library (Beasley, 1990). The
metric ρ for this experiment is the (weighted) shortest path distance. To generate a size-n graph, we
first randomly split the nodes into 10 clusters. Within each cluster, each pair of nodes is connected
with probability 0.2, and with weight drawn from uniform [0, 1]. For every pair of clusters, we
randomly connect some nodes from each cluster, with weights following uniform [0.5, r]. A larger
r makes the graph more separable, i.e., clusters are farther from each other (see Appendix A for
example graphs). For this task, U has 3000 nodes, and the private set D (500 nodes) is chosen using
the similar “balanced” and “imbalanced” approachs as described above. In the imbalanced case, we
choose the demand set D randomly from only two clusters."
DATASETS AND ALGORITHMS,0.2632794457274827,"Algorithms.
We compare the following clustering algorithms in both non-DP and DP setting:
(1) NDP-rand: Local search with random initialization; (2) NDP-kmedian++: Local search with
k-median++ initialization (Algorithm 2); (3) NDP-HST: Local search with NDP-HST initialization
(Algorithm 4), as described in Section 3; (4) DP-rand: Standard DP local search algorithm (Gupta
et al., 2010), which is Algorithm 7 with initial centers randomly chosen from U; (5) DP-kmedian++:
DP local search with k-median++ initialization run on U; (6) DP-HST: DP local search with
HST-initialization (Algorithm 7). For non-DP tasks, we set L = 6. For DP clustering, we use L = 8."
DATASETS AND ALGORITHMS,0.26558891454965355,"For non-DP methods, we set α = 10−3 in Algorithm 1 and the maximum number of iterations as
20. To examine the quality of initialization as well as the final centers, We report both the cost at
initialization and the cost of the final output. For DP methods, we run the algorithms for T = 20
steps and report the results with ϵ = 1 (comparisons/results with other T and ϵ are similar). We test
k ∈{2, 5, 10, 15, 20}. The average cost over T iterations is reported for robustness. All the results
are averaged over 10 independent repetitions."
RESULTS,0.2678983833718245,"5.2
Results"
RESULTS,0.2702078521939954,"The results on MNIST and graph data are given in Figure 2. Here we present the l2-clustering on
MNIST, and the simulated graph with r = 1 (clusters are less separable). The comparisons are
similar for both l1 metric on MNIST and r = 100 graph (see Figure 4 in Appendix A):"
RESULTS,0.27251732101616627,"• From the left column, the initial centers found by HST has lower cost than k-median++ and
random initialization, for both non-DP and DP setting, and for both balanced and imbalanced
demand set D. This confirms that the proposed HST initialization is more powerful than
k-median++ in finding good initial centers.
• From the right column, we also observe lower final cost of HST followed by local search in
DP clustering. In the non-DP case, the final cost curves overlap, which means that despite
HST offers better initial centers, local search can always find a good solution eventually.
• The advantage of DP-HST, in terms of both the initial and the final cost, is more significant
when D is an imbalanced subset of U. As mentioned before, this is because our DP-HST
initialization approach also privately incorporates the information of D."
RESULTS,0.2748267898383372,"To sum up, the proposed HST initialization scheme could perform better with various metrics and
data patterns, in both non-private and private setting—in all cases, HST finds better initial centers
with smaller cost than k-median++. HST considerably outperforms k-median++ in the private and
imbalanced D setting, for MNIST with both l2 and l1 metric, and for graph with both r = 100 (highly
separable) and r = 1 (less separable)."
RESULTS,0.27713625866050806,"2 
5 
10
15
20
k 3500 4000 4500 5000"
RESULTS,0.279445727482679,initial cost
RESULTS,0.2817551963048499,Balanced D
RESULTS,0.2840646651270208,MNIST - l2
RESULTS,0.2863741339491917,"NDP-HST
NDP-kmedian++
NDP-rand
DP-HST
DP-kmedian++
DP-rand"
RESULTS,0.28868360277136257,"2 
5 
10
15
20
k 3500 4000 4500"
RESULTS,0.2909930715935335,k-median cost
RESULTS,0.29330254041570436,Balanced D
RESULTS,0.2956120092378753,MNIST - l2
RESULTS,0.2979214780600462,"NDP-HST
NDP-kmedian++
NDP-rand
DP-HST
DP-kmedian++
DP-rand"
RESULTS,0.3002309468822171,"2 
5 
10
15
20
k 3500 4000 4500 5000 5500"
RESULTS,0.302540415704388,initial cost
RESULTS,0.30484988452655887,Imbalanced D
RESULTS,0.3071593533487298,MNIST - l2
RESULTS,0.3094688221709007,"NDP-HST
NDP-kmedian++
NDP-rand
DP-HST
DP-kmedian++
DP-rand"
RESULTS,0.3117782909930716,"2 
5 
10
15
20
k 3500 4000 4500"
RESULTS,0.3140877598152425,k-median cost
RESULTS,0.3163972286374134,Imbalanced D
RESULTS,0.3187066974595843,MNIST - l2
RESULTS,0.3210161662817552,"NDP-HST
NDP-kmedian++
NDP-rand
DP-HST
DP-kmedian++
DP-rand"
RESULTS,0.3233256351039261,"2 
5 
10
15
20
k 40 60 80 100"
RESULTS,0.325635103926097,initial cost
RESULTS,0.3279445727482679,Balanced D
RESULTS,0.3302540415704388,GRAPH r = 1
RESULTS,0.3325635103926097,"NDP-HST
NDP-kmedian++
NDP-rand
DP-HST
DP-kmedian++
DP-rand"
RESULTS,0.3348729792147806,"2 
5 
10
15
20
k 20 40 60 80 100"
RESULTS,0.3371824480369515,k-median cost
RESULTS,0.3394919168591224,Balanced D
RESULTS,0.3418013856812933,GRAPH r = 1
RESULTS,0.3441108545034642,"NDP-HST
NDP-kmedian++
NDP-rand
DP-HST
DP-kmedian++
DP-rand"
RESULTS,0.3464203233256351,"2 
5 
10
15
20
k 20 40 60 80 100"
RESULTS,0.34872979214780603,initial cost
RESULTS,0.3510392609699769,Imbalanced D
RESULTS,0.3533487297921478,GRAPH r = 1
RESULTS,0.3556581986143187,"NDP-HST
NDP-kmedian++
NDP-rand
DP-HST
DP-kmedian++
DP-rand"
RESULTS,0.3579676674364896,"2 
5 
10
15
20
k 20 40 60 80"
RESULTS,0.36027713625866054,k-median cost
RESULTS,0.3625866050808314,Imbalanced D
RESULTS,0.3648960739030023,GRAPH r = 1
RESULTS,0.3672055427251732,"NDP-HST
NDP-kmedian++
NDP-rand
DP-HST
DP-kmedian++
DP-rand"
RESULTS,0.3695150115473441,"Figure 2: k-median cost on the MNIST (l2-metric) and graph dataset (r = 1). 1st column: initial
cost. 2nd column: final output cost."
CONCLUSION,0.371824480369515,"6
Conclusion"
CONCLUSION,0.3741339491916859,"We develop a new initialization framework for the k-median problem in the general metric space. Our
approach, called HST initialization, is built upon the HST structure from metric embedding theory.
We propose a novel and efficient tree search approach which provably improves the approximation
error of the k-median++ method, and has lower complexity (higher efficiency) than k-median++
when k is not too small, which is a common practice. Moreover, we propose differentially private
(DP) HST initialization algorithm, which adapts to the private demand point set, leading to better
clustering performance. When combined with subsequent DP local search heuristic, our algorithm is
able to improve the additive error of DP local search, which is close to the theoretical lower bound
within a small factor. Experiments with Euclidean metrics and graph metrics verify the effectiveness
of our methods, which improve the cost of both the initial centers and the final k-median output."
REFERENCES,0.37644341801385683,References
REFERENCES,0.3787528868360277,"Martín Abadi, Andy Chu, Ian J. Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar,
and Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC
Conference on Computer and Communications Security (CCS), pages 308–318, Vienna, Austria,
2016."
REFERENCES,0.3810623556581986,"Ameer Ahmed Abbasi and Mohamed F. Younis. A survey on clustering algorithms for wireless
sensor networks. Comput. Commun., 30(14-15):2826–2841, 2007."
REFERENCES,0.3833718244803695,"Sara Ahmadian, Ashkan Norouzi-Fard, Ola Svensson, and Justin Ward. Better guarantees for k-
means and euclidean k-median by primal-dual algorithms. In Proceedings of the 58th IEEE Annual
Symposium on Foundations of Computer Science (FOCS), pages 61–72, Berkeley, CA, 2017."
REFERENCES,0.3856812933025404,"David Arthur and Sergei Vassilvitskii. k-means++: the advantages of careful seeding. In Proceedings
of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1027–
1035, New Orleans, LA, 2007."
REFERENCES,0.38799076212471134,"Vijay Arya, Naveen Garg, Rohit Khandekar, Adam Meyerson, Kamesh Munagala, and Vinayaka
Pandit. Local search heuristics for k-median and facility location problems. SIAM J. Comput., 33
(3):544–562, 2004."
REFERENCES,0.3903002309468822,"Olivier Bachem, Mario Lucic, S. Hamed Hassani, and Andreas Krause. Approximate k-means++ in
sublinear time. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI),
pages 1459–1467, Phoenix, AZ, 2016."
REFERENCES,0.39260969976905313,"Bahman Bahmani, Benjamin Moseley, Andrea Vattani, Ravi Kumar, and Sergei Vassilvitskii. Scalable
k-means++. Proc. VLDB Endow., 5(7):622–633, 2012."
REFERENCES,0.394919168591224,"Maria-Florina Balcan, Steven Ehrlich, and Yingyu Liang.
Distributed k-means and k-median
clustering on general communication topologies. In Advances in Neural Information Processing
Systems (NIPS), pages 1995–2003, Lake Tahoe, NV, 2013."
REFERENCES,0.3972286374133949,"Maria-Florina Balcan, Travis Dick, Yingyu Liang, Wenlong Mou, and Hongyang Zhang. Differ-
entially private clustering in high-dimensional euclidean spaces. In Proceedings of the 34th
International Conference on Machine Learning (ICML), pages 322–331, Sydney, Australia, 2017."
REFERENCES,0.3995381062355658,"Arindam Banerjee, Srujana Merugu, Inderjit S. Dhillon, and Joydeep Ghosh. Clustering with bregman
divergences. J. Mach. Learn. Res., 6:1705–1749, 2005."
REFERENCES,0.4018475750577367,"Yair Bartal. Probabilistic approximations of metric spaces and its algorithmic applications. In
Proceedings of the 37th Annual Symposium on Foundations of Computer Science (FOCS), pages
184–193, Burlington, VT, 1996."
REFERENCES,0.40415704387990764,"John E Beasley. OR-Library: distributing test problems by electronic mail. Journal of the Operational
Research Society, 41(11):1069–1072, 1990."
REFERENCES,0.4064665127020785,"Pavel Berkhin. A survey of clustering data mining techniques. In Grouping Multidimensional Data,
pages 25–71. Springer, 2006."
REFERENCES,0.40877598152424943,"Guy E. Blelloch, Yan Gu, and Yihan Sun. Efficient construction of probabilistic tree embeddings. In
Proceedings of the 44th International Colloquium on Automata, Languages, and Programming
(ICALP), pages 26:1–26:14, Warsaw, Poland, 2017."
REFERENCES,0.4110854503464203,"Jaroslaw Byrka, Thomas W. Pensyl, Bartosz Rybicki, Aravind Srinivasan, and Khoa Trinh. An
improved approximation for k-median, and positive correlation in budgeted optimization. In
Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),
pages 737–756, San Diego, CA, 2015."
REFERENCES,0.4133949191685912,"Kamalika Chaudhuri and Claire Monteleoni. Privacy-preserving logistic regression. In Advances in
Neural Information Processing Systems (NIPS), pages 289–296, Vancouver, Canada, 2008."
REFERENCES,0.41570438799076215,"Kamalika Chaudhuri, Claire Monteleoni, and Anand D. Sarwate. Differentially private empirical risk
minimization. J. Mach. Learn. Res., 12:1069–1109, 2011."
REFERENCES,0.418013856812933,"Davin Choo, Christoph Grunau, Julian Portmann, and Václav Rozhon. k-means++: few more
steps yield constant approximation. In International Conference on Machine Learning, pages
1909–1917, 2020."
REFERENCES,0.42032332563510394,"Vincent Cohen-Addad, Silvio Lattanzi, Ashkan Norouzi-Fard, Christian Sohler, and Ola Svens-
son. Parallel and efficient hierarchical k-median clustering. In Advances in Neural Information
Processing Systems (NeurIPS), pages 20333–20345, virtual, 2021."
REFERENCES,0.4226327944572748,"Inderjit S. Dhillon and Dharmendra S. Modha. Concept decompositions for large sparse text data
using clustering. Mach. Learn., 42(1/2):143–175, 2001."
REFERENCES,0.42494226327944573,"Jinshuo Dong, Aaron Roth, and Weijie J Su. Gaussian differential privacy. Journal of the Royal
Statistical Society Series B: Statistical Methodology, 84(1):3–37, 2022."
REFERENCES,0.42725173210161665,"Cynthia Dwork. Differential privacy. In Proceedings of the 33rd International Colloquium on
Automata, Languages and Programming (ICALP), Part II, pages 1–12, Venice, Italy, 2006."
REFERENCES,0.4295612009237875,"Jittat Fakcharoenphol, Satish Rao, and Kunal Talwar. A tight bound on approximating arbitrary
metrics by tree metrics. J. Comput. Syst. Sci., 69(3):485–497, 2004."
REFERENCES,0.43187066974595845,"Chenglin Fan and Ping Li. Distances release with differential privacy in tree and grid graph. In IEEE
International Symposium on Information Theory (ISIT), pages 2190–2195, 2022."
REFERENCES,0.4341801385681293,"Chenglin Fan, Ping Li, and Xiaoyun Li. Private graph all-pairwise-shortest-path distance release
with improved error rate. In Advances in Neural Information Processing Systems (NeurIPS), New
Orleans, LA, 2022."
REFERENCES,0.43648960739030024,"Chenglin Fan, Ping Li, and Xiaoyun Li. LSDS++ : Dual sampling for accelerated k-means++. In
Proceedings of the International Conference on Machine Learning (ICML), pages 9640–9649,
Honolulu, HI, 2023."
REFERENCES,0.4387990762124711,"Huang Fang, Xiaoyun Li, Chenglin Fan, and Ping Li. Improved convergence of differential private
sgd with gradient clipping. In Proceedings of the Eleventh International Conference on Learning
Representations (ICLR), Kigali, Rwanda, 2023."
REFERENCES,0.44110854503464203,"Dan Feldman, Amos Fiat, Haim Kaplan, and Kobbi Nissim. Private coresets. In Proceedings of the
41st Annual ACM Symposium on Theory of Computing (STOC), pages 361–370, Bethesda, MD,
2009."
REFERENCES,0.44341801385681295,"Dan Feldman, Chongyuan Xiang, Ruihao Zhu, and Daniela Rus. Coresets for differentially private
k-means clustering and applications to privacy in mobile sensor networks. In Proceedings of the
16th ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN),
pages 3–15, Pittsburgh, PA, 2017."
REFERENCES,0.4457274826789838,"Jason Ge, Zhaoran Wang, Mengdi Wang, and Han Liu. Minimax-optimal privacy-preserving sparse
PCA in distributed systems. In Proceedings of the International Conference on Artificial Intelli-
gence and Statistics (AISTATS), pages 1589–1598, Playa Blanca, Lanzarote, Canary Islands, Spain,
2018."
REFERENCES,0.44803695150115475,"Badih Ghazi, Ravi Kumar, and Pasin Manurangsi. Differentially private clustering: Tight approxima-
tion ratios. In Advances in Neural Information Processing Systems (NeurIPS), virtual, 2020."
REFERENCES,0.4503464203233256,"Christoph Grunau, Ahmet Alper Özüdo˘gru, Václav Rozhoˇn, and Jakub Tˇetek. A nearly tight analysis
of greedy k-means++. In Proceedings of the 2023 Annual ACM-SIAM Symposium on Discrete
Algorithms (SODA), pages 1012–1070, Florence, Italy, 2023."
REFERENCES,0.45265588914549654,"Anupam Gupta, Katrina Ligett, Frank McSherry, Aaron Roth, and Kunal Talwar. Differentially private
combinatorial optimization. In Proceedings of the Twenty-First Annual ACM-SIAM Symposium on
Discrete Algorithms (SODA), pages 1106–1125, Austin, TX, 2010."
REFERENCES,0.45496535796766746,"Zhiyi Huang and Jinyan Liu. Optimal differentially private algorithms for k-means clustering. In
Proceedings of the 37th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database
Systems (PODS), pages 395–408, Houston, TX, 2018."
REFERENCES,0.45727482678983833,"Matthew Jones, Huy L. Nguyen, and Thy D. Nguyen. Differentially private clustering via maximum
coverage. In Proeedings of the Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI),
pages 11555–11563, Virtual Event, 2021."
REFERENCES,0.45958429561200925,"Tapas Kanungo, David M. Mount, Nathan S. Netanyahu, Christine D. Piatko, Ruth Silverman, and
Angela Y. Wu. A local search approximation algorithm for k-means clustering. In Proceedings of
the 18th Annual Symposium on Computational Geometry (CG), pages 10–18, Barcelona, Spain,
2002."
REFERENCES,0.4618937644341801,"Leon Kaufman, Marc Vanden Eede, and Pierre Hansen. A plant and warehouse location problem.
Journal of the Operational Research Society, 28(3):547–554, 1977."
REFERENCES,0.46420323325635104,"Silvio Lattanzi and Christian Sohler. A better k-means++ algorithm via local search. In Proceedings
of the 36th International Conference on Machine Learning (ICML), pages 3662–3671, Long Beach,
CA, 2019."
REFERENCES,0.4665127020785219,"Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proc. IEEE, 86(11):2278–2324, 1998."
REFERENCES,0.46882217090069284,"Ping Li and Xiaoyun Li. Differential privacy with random projections and sign random projections.
In Advances in Neural Information Processing Systems (NeurIPS), New Orleans, LA, 2023a."
REFERENCES,0.47113163972286376,"Xiaoyun Li and Ping Li. Differentially private one permutation hashing and bin-wise consistent
weighted sampling. arXiv preprint arXiv:2306.07674, 2023b."
REFERENCES,0.47344110854503463,"Stuart P. Lloyd. Least squares quantization in PCM. IEEE Trans. Inf. Theory, 28(2):129–136, 1982."
REFERENCES,0.47575057736720555,"Konstantin Makarychev, Yury Makarychev, and Ilya P. Razenshteyn. Performance of johnson-
lindenstrauss transform for k-means and k-medians clustering. In Proceedings of the 51st Annual
ACM SIGACT Symposium on Theory of Computing (STOC), pages 1027–1038, Phoenix, AZ, 2019."
REFERENCES,0.4780600461893764,"Frank McSherry and Kunal Talwar. Mechanism design via differential privacy. In Proceedings of
the 48th Annual IEEE Symposium on Foundations of Computer Science (FOCS), pages 94–103,
Providence, RI, 2007."
REFERENCES,0.48036951501154734,"Richard Nock, Raphaël Canyasse, Roksana Boreli, and Frank Nielsen. k-variates++: more pluses
in the k-means++. In Proceedings of the 33nd International Conference on Machine Learning
(ICML), pages 145–154, New York City, NY, 2016."
REFERENCES,0.48267898383371827,"Girish Punj and David W Stewart. Cluster analysis in marketing research: Review and suggestions
for application. Journal of Marketing Research, 20(2):134–148, 1983."
REFERENCES,0.48498845265588914,"Mauricio G. C. Resende and Renato Fonseca F. Werneck. A fast swap-based local search procedure
for location problems. Ann. Oper. Res., 150(1):205–230, 2007."
REFERENCES,0.48729792147806006,"Rahul Shah. Faster algorithms for k-median problem on trees with smaller heights. Technical Report,
2003."
REFERENCES,0.4896073903002309,"Uri Stemmer and Haim Kaplan. Differentially private k-means with constant multiplicative error.
In Advances in Neural Information Processing Systems (NeurIPS), pages 5436–5446, Montréal,
Canada, 2018."
REFERENCES,0.49191685912240185,"Arie Tamir. An o(pn2) algorithm for the p-median and related problems on tree graphs. Oper. Res.
Lett., 19(2):59–64, 1996."
REFERENCES,0.4942263279445728,"Keisuke Todo, Atsuyoshi Nakamura, and Mineichi Kudo. A fast approximate algorithm for k-median
problem on a graph. In Proceedings of the 15th International Workshop on Mining and Learning
with Graphs (MLG), Anchorage, AK, 2019."
REFERENCES,0.49653579676674364,"Kang Wei, Jun Li, Ming Ding, Chuan Ma, Howard H. Yang, Farhad Farokhi, Shi Jin, Tony Q. S. Quek,
and H. Vincent Poor. Federated learning with differential privacy: Algorithms and performance
analysis. IEEE Trans. Inf. Forensics Secur., 15:3454–3469, 2020."
REFERENCES,0.49884526558891457,"k-Median Clustering via Metric Embedding: Towards Better
Initialization with Differential Privacy
(Supplementary Material)"
REFERENCES,0.5011547344110855,"A
More Details on Experiments"
REFERENCES,0.5034642032332564,"A.1
Examples of Graph Data"
REFERENCES,0.5057736720554272,"In Figure 3, we plot two example graphs (subgraphs of 50 nodes) with r = 100 and r = 1. When
r = 100, the graph is highly separable (i.e., clusters are far from each other). When r = 1, the
clusters are harder to be distinguished from each other. 1 2
3 4 5 6 7 8 9 10 11 12 13
14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49
50"
REFERENCES,0.5080831408775982,"Figure 3: Example of synthetic graphs: subgraph of 50 nodes. Upper r = 1. Bottom: r = 100.
Darker and thicker edged have smaller distance. When r = 100, the graph is more separable."
REFERENCES,0.5103926096997691,"2 
5 
10
15
20
k 4 5 6"
REFERENCES,0.5127020785219399,initial cost
BALANCED D,0.5150115473441108,"104
Balanced D"
BALANCED D,0.5173210161662818,MNIST - l1
BALANCED D,0.5196304849884527,"NDP-HST
NDP-kmedian++
NDP-rand
DP-HST
DP-kmedian++
DP-rand"
BALANCED D,0.5219399538106235,"2 
5 
10
15
20
k 3.5 4 4.5 5 5.5"
BALANCED D,0.5242494226327945,k-median cost
BALANCED D,0.5265588914549654,"104
Balanced D"
BALANCED D,0.5288683602771362,MNIST - l1
BALANCED D,0.5311778290993071,"NDP-HST
NDP-kmedian++
NDP-rand
DP-HST
DP-kmedian++
DP-rand"
BALANCED D,0.5334872979214781,"2 
5 
10
15
20
k 4 5 6 7"
BALANCED D,0.535796766743649,initial cost
IMBALANCED D,0.5381062355658198,"104
Imbalanced D"
IMBALANCED D,0.5404157043879908,MNIST - l1
IMBALANCED D,0.5427251732101617,"NDP-HST
NDP-kmedian++
NDP-rand
DP-HST
DP-kmedian++
DP-rand"
IMBALANCED D,0.5450346420323325,"2 
5 
10
15
20
k 3.5 4 4.5 5 5.5 6"
IMBALANCED D,0.5473441108545035,k-median cost
IMBALANCED D,0.5496535796766744,"104
Imbalanced D"
IMBALANCED D,0.5519630484988453,MNIST - l1
IMBALANCED D,0.5542725173210161,"NDP-HST
NDP-kmedian++
NDP-rand
DP-HST
DP-kmedian++
DP-rand"
IMBALANCED D,0.5565819861431871,"2 
5 
10
15
20
k 0 500 1000"
IMBALANCED D,0.558891454965358,initial cost
IMBALANCED D,0.5612009237875288,Balanced D
IMBALANCED D,0.5635103926096998,GRAPH r = 100
IMBALANCED D,0.5658198614318707,"NDP-HST
NDP-kmedian++
NDP-rand
DP-HST
DP-kmedian++
DP-rand"
IMBALANCED D,0.5681293302540416,"2 
5 
10
15
20
k 0 500 1000"
IMBALANCED D,0.5704387990762124,k-median cost
IMBALANCED D,0.5727482678983834,Balanced D
IMBALANCED D,0.5750577367205543,GRAPH r = 100
IMBALANCED D,0.5773672055427251,"NDP-HST
NDP-kmedian++
NDP-rand
DP-HST
DP-kmedian++
DP-rand"
IMBALANCED D,0.5796766743648961,"2 
5 
10
15
20
k 0 200 400 600 800"
IMBALANCED D,0.581986143187067,initial cost
IMBALANCED D,0.5842956120092379,Imbalanced D
IMBALANCED D,0.5866050808314087,"GRAPH r = 100
NDP-HST
NDP-kmedian++
NDP-rand
DP-HST
DP-kmedian++
DP-rand"
IMBALANCED D,0.5889145496535797,"2 
5 
10
15
20
k 0 100 200 300"
IMBALANCED D,0.5912240184757506,k-median cost
IMBALANCED D,0.5935334872979214,Imbalanced D
IMBALANCED D,0.5958429561200924,GRAPH r = 100
IMBALANCED D,0.5981524249422633,"NDP-HST
NDP-kmedian++
NDP-rand
DP-HST
DP-kmedian++
DP-rand"
IMBALANCED D,0.6004618937644342,"Figure 4: k-median cost on the MNIST (l1-metric) and graph dataset (r = 100) 1st column: initial
cost. 2nd column: final output cost."
IMBALANCED D,0.6027713625866051,"A.2
More Experiments"
IMBALANCED D,0.605080831408776,"A.3
Improved Iteration Cost of DP-HST"
IMBALANCED D,0.6073903002309469,"In Theorem 4.3, we show that under differential privacy constraints, the proposed DP-HST (Algo-
rithm 7) improves both the approximation error and the number of iterations required to find a good
solution of classical DP local search (Gupta et al., 2010). In this section, we provide some numerical
results to justify the theory."
IMBALANCED D,0.6096997690531177,"First, we need to properly measure the iteration cost of DP local search. This is because, unlike the
non-private clustering, the k-median cost after each iteration in DP local search is not decreasing
monotonically, due to the probabilistic exponential mechanism. To this end, for the cost sequence
with length T = 20, we compute its moving average sequence with window size 5. Attaining the"
IMBALANCED D,0.6120092378752887,"minimal value of the moving average indicates that the algorithm has found a “local optimum”, i.e.,
it has reached a “neighborhood” of solutions with small clustering cost. Thus, we use the number of
iterations to reach such local optimum as the measure of iteration cost. The results are provided in
Figure 5. We see that on all the tasks (MNIST with l1 and l2 distance, and graph dataset with r = 1
and r = 100), DP-HST has significantly smaller iterations cost. In Figure 6, we further report the
k-median cost of the best solution in T iterations found by each DP algorithm. We see that DP-HST
again provide the smallest cost. This additional set of experiments again validates the claims of
Theorem 4.3, that DP-HST is able to found better initial centers in fewer iterations."
IMBALANCED D,0.6143187066974596,"2 
5 
10
15
20
k 6 8 10 12 14 16"
IMBALANCED D,0.6166281755196305,iterations to min cost
IMBALANCED D,0.6189376443418014,MNIST - l1
IMBALANCED D,0.6212471131639723,"DP-HST
DP-kmedian++
DP-rand"
IMBALANCED D,0.6235565819861432,"2 
5 
10
15
20
k 4 6 8 10 12 14"
IMBALANCED D,0.625866050808314,iterations to min cost
IMBALANCED D,0.628175519630485,MNIST - l2
IMBALANCED D,0.6304849884526559,"DP-HST
DP-kmedian++
DP-rand"
IMBALANCED D,0.6327944572748267,"2 
5 
10
15
20
k 6 8 10 12 14 16"
IMBALANCED D,0.6351039260969977,iterations to min cost
IMBALANCED D,0.6374133949191686,GRAPH r = 100
IMBALANCED D,0.6397228637413395,"DP-HST
DP-kmedian++
DP-rand"
IMBALANCED D,0.6420323325635104,"2 
5 
10
15
20
k 6 8 10 12 14 16"
IMBALANCED D,0.6443418013856813,iterations to min cost
IMBALANCED D,0.6466512702078522,GRAPH r = 1
IMBALANCED D,0.648960739030023,"DP-HST
DP-kmedian++
DP-rand"
IMBALANCED D,0.651270207852194,"Figure 5: Iteration cost to reach a locally optimal solution, on MNIST and graph datasets with
different k. The demand set is an imbalanced subset of the universe."
IMBALANCED D,0.6535796766743649,"2 
5 
10
15
20
k 4 4.5 5"
IMBALANCED D,0.6558891454965358,min k-median cost
IMBALANCED D,0.6581986143187067,"104
Imbalanced D"
IMBALANCED D,0.6605080831408776,MNIST - l1
IMBALANCED D,0.6628175519630485,"DP-HST
DP-kmedian++
DP-rand"
IMBALANCED D,0.6651270207852193,"2 
5 
10
15
20
k 3000 3500 4000"
IMBALANCED D,0.6674364896073903,min k-median cost
IMBALANCED D,0.6697459584295612,Imbalanced D
IMBALANCED D,0.6720554272517321,MNIST - l2
IMBALANCED D,0.674364896073903,"DP-HST
DP-kmedian++
DP-rand"
IMBALANCED D,0.6766743648960739,"Figure 6: The k-median cost of the best solution found by each differentially private algorithm. The
demand set is an imbalanced subset of the universe. Same comparison holds on graph data."
IMBALANCED D,0.6789838337182448,"B
Technical Proofs"
IMBALANCED D,0.6812933025404158,The following composition result of differential privacy will be used in our proof.
IMBALANCED D,0.6836027713625866,"Theorem B.1 (Composition Theorem (Dwork, 2006)). If Algorithms A1, A2, ..., Am are
ϵ1, ϵ2, ..., ϵm differentially private respectively, then the union (A1(D), A2(D), ..., Am(D)) is
Pm
i=1 ϵi-DP."
IMBALANCED D,0.6859122401847575,"B.1
Proof of Lemma 3.5"
IMBALANCED D,0.6882217090069284,"Proof. Consider the intermediate output of Algorithm 4, C1 = {v1, v2, ..., vk}, which is the set of
roots of the minimal subtrees each containing exactly one output center C0. Suppose one of the
optimal “root set” that minimizes (4) is C∗
1 = {v′
1, v′
2, ..., v′
k}. If C1 = C∗
1, the proof is done. Thus,
we prove the case for C1 ̸= C∗
1. Note that T(v), v ∈C1 are disjoint subtrees. We have the following
reasoning."
IMBALANCED D,0.6905311778290993,"• Case 1: for some i, j′, vi is a descendant node of v′
j. Since the optimal center point f ∗is a
leaf node by the definition of (4), we know that there must exist one child node of v′
j that
expands a subtree which contains f ∗. Therefore, we can always replace v′
j by one of its
child nodes. Hence, we can assume that vi is not a descendant of v′
j."
IMBALANCED D,0.6928406466512702,"Note that, we have score(v′
j) ≤score(vi) if v′
j /∈C∗
1 ∩C1. Algorithm 4 sorts all the nodes
based on cost value, and it would have more priority to pick v′
j than vi if score(v′
j) >
score(vi) and vi is not a child node of v′
j."
IMBALANCED D,0.6951501154734411,"• Case 2: for some i, j′, v′
j is a descendant of vi. In this case, optimal center point f ∗, which
is a leaf of T(vi), must also be a leaf node of T(v′
j). We can simply replace C1 with the
swap C1 \ {vi} + {v′
j} which does not change costT
k
′(U). Hence, we can assume that v′
j is
not a descendant of vi."
IMBALANCED D,0.6974595842956121,"• Case 3:
Otherwise.
By the construction of C1, we know that score(v′
j)
≤
min{score(vi), i = 1, ..., k} when v′
j ∈C∗
1 \ C1. Consider the swap between C1 and
C∗
1. By the definition of tree distance, we have OPT T
k (U) ≥P"
IMBALANCED D,0.6997690531177829,"vi∈C1\C∗
1 Nvi2hvi, since
{T(vi), vi ∈C1 \ C∗
1} does not contain any center of the optimal solution determined by
C∗
1 (which is also the optimal “root set” for OPT T
k (U))."
IMBALANCED D,0.7020785219399538,"Thus, we only need to consider Case 3. Let us consider the optimal clustering with center set be
C∗= {c∗
1, c∗
2, ..., c∗
k} (each center c∗
j is a leaf of subtree whose root be c′
j), and S′
j be the leaves
assigned to c∗
j. Let Sj denote the set of leaves in S′
j whose distance to c∗
j is strictly smaller than its
distance to any centers in C1. Let Pj denote the union of paths between leaves of Sj to its closest
center in C1. Let v′′
j be the nodes in Pj with highest level satisfying T(v′′
j ) ∩C1 = ∅. The score of"
IMBALANCED D,0.7043879907621247,"v′′
j is 2
hv′′
j N(v′′
j ). That means the swap with a center v′
j into C1 can only reduce 4 · 2
hv′′
j N(v′′
j ) to"
IMBALANCED D,0.7066974595842956,"costT
k
′(U) (the tree distance between any leaf in Sj and its closest center in C1 is at most 4 · 2
hv′′
j ).
We just use v′
j to represent v′′
j for later part of this proof for simplicity. By our reasoning, summing
all the swaps over C∗
1 \ C1 gives"
IMBALANCED D,0.7090069284064665,"costT
k
′(U) −OPT T
k (U) ≤4
X"
IMBALANCED D,0.7113163972286374,"v′
j∈C∗
1 \C1
Nv′
j2
hv′
j ,"
IMBALANCED D,0.7136258660508084,"OPT T
k (U) ≥
X"
IMBALANCED D,0.7159353348729792,"vi∈C1\C∗
1
Nvi2hvi."
IMBALANCED D,0.7182448036951501,"Also, based on our discussion on Case 1, it holds that"
IMBALANCED D,0.7205542725173211,"Nv′
j2
hv′
j −Nvi2hvi ≤0."
IMBALANCED D,0.7228637413394919,"Summing them together, we have costT
k
′(U) ≤5OPT T
k (U)."
IMBALANCED D,0.7251732101616628,"B.2
Proof of Lemma 3.6"
IMBALANCED D,0.7274826789838337,"Proof. Since the subtrees in C1 are disjoint, it suffices to consider one subtree with root v. With a
little abuse of notation, let costT
1
′(v, U) denote the optimal k-median cost within the point set T(v)
with one center in 2-HST:"
IMBALANCED D,0.7297921478060047,"costT
1
′(v, U) = min
x∈T (v) X"
IMBALANCED D,0.7321016166281755,"y∈T (v)
ρT (x, y),
(7)"
IMBALANCED D,0.7344110854503464,"which is the optimal cost within the subtree. Suppose v has more than one children u, w, ..., otherwise
the optimal center is clear. Suppose the optimal solution of costT
1
′(v, U) chooses a leaf node in
T(u), and our HST initialization algorithm picks a leaf of T(w). If u = w, then HST chooses the
optimal one where the argument holds trivially. Thus, we consider u ̸= w. We have the following
two observations:"
IMBALANCED D,0.7367205542725174,"• Since one needs to pick a leaf of T(u) to minimize costT
1
′(v, U), we have costT
1
′(v, U) ≥
P"
IMBALANCED D,0.7390300230946882,"x∈ch(v),x̸=u Nx · 2hx where ch(u) denotes the children nodes of u."
IMBALANCED D,0.7413394919168591,"• By our greedy strategy, costT
1 (v, U) ≤P"
IMBALANCED D,0.74364896073903,"x∈ch(u) Nx · 2hx ≤costT
1
′(v, U) + Nu · 2hu."
IMBALANCED D,0.745958429561201,"Since hu = hw, we have
2hu · (Nu −Nw) ≤0,"
IMBALANCED D,0.7482678983833718,"since our algorithm picks subtree roots with highest scores.
Then we have costT
1 (v, U) ≤
costT
1
′(v, U) + Nw · 2hw ≤2costT
1
′(v, U). Since the subtrees in C1 are disjoint, the union of
centers for OPT T
1 (v, U), v ∈C1 forms the optimal centers with size k. Note that, for any data point
p ∈U \ C1, the tree distance ρT (p, f) for ∀f that is a leaf node of T(v), v ∈C1 is the same. That
is, the choice of leaf in T(v) as the center does not affect the k-median cost under 2-HST metric.
Therefore, union bound over k subtree costs completes the proof."
IMBALANCED D,0.7505773672055427,"B.3
Proof of Proposition 3.2"
IMBALANCED D,0.7528868360277137,"Proof. It is known that the 2-HST can be constructed in O(dn log n) (Bartal, 1996). The subtree
search in Algorithm 4 involves at most sorting all the nodes in the HST based on the score, which
takes O(nlogn). We use a priority queue to store the nodes in C1. When we insert a new node v into
queue, its parent node (if existing in the queue) would be removed from the queue. The number of
nodes is O(n) and each operation (insertion, deletion) in a priority queue based on score has O(log n)
complexity. Lastly, the total time to obtain C0 is O(n), as the FIND-LEAF only requires a top down
scan in k disjoint subtrees of T. Summing parts together proves the claim."
IMBALANCED D,0.7551963048498845,"B.4
Proof of Theorem 4.2"
IMBALANCED D,0.7575057736720554,"Similarly, we prove the error in general metric by first analyzing the error in 2-HST metric. Then the
result follows from Lemma 3.4. Let costT
k (D), costT
k
′(D) and OPT T
k (D) be defined analogously
to (3), (4) and (5), where “y ∈U” in the summation is changed into “y ∈D” since D is the demand
set. That is,"
IMBALANCED D,0.7598152424942263,"costT
k (D) =
X"
IMBALANCED D,0.7621247113163973,"y∈D
min
x∈C0 ρT (x, y),
(8)"
IMBALANCED D,0.7644341801385681,"costT
k
′(D, C1) =
min
|F ∩T (v)|=1,∀v∈C1 X"
IMBALANCED D,0.766743648960739,"y∈D
min
x∈F ρT (x, y),
(9)"
IMBALANCED D,0.76905311778291,"OPT T
k (D) =
min
F ⊂D,|F |=k X"
IMBALANCED D,0.7713625866050808,"y∈D
min
x∈F ρT (x, y) ≡min
C′
1
costT
k
′(D, C′
1).
(10)"
IMBALANCED D,0.7736720554272517,We have the following.
IMBALANCED D,0.7759815242494227,"Lemma B.2. costT
k (D) ≤10OPT T
k (D) + 10ckϵ−1△log n with probability 1 −4k/nc."
IMBALANCED D,0.7782909930715936,"Proof. The result follows by combining the following Lemma B.4, Lemma B.5, and applying union
bound."
IMBALANCED D,0.7806004618937644,"Lemma B.3. For any node v in T, with probability 1 −1/nc, | ˆNv · 2hv −Nv · 2hv| ≤cϵ−1△log n."
IMBALANCED D,0.7829099307159353,"Proof. Since ˆ
Nv = Nv + Lap(2(L−hv)/2/ϵ), we have"
IMBALANCED D,0.7852193995381063,Pr[| ˆNv −Nv| ≥x/ϵ] = exp(−x/2(L−hv)).
IMBALANCED D,0.7875288683602771,"As L = log △, we have"
IMBALANCED D,0.789838337182448,Pr[| ˆNv −Nv| ≥x△/(2hvϵ)] ≤exp(−x).
IMBALANCED D,0.792147806004619,"Hence, for some constant c > 0,"
IMBALANCED D,0.7944572748267898,Pr[| ˆNv · 2hv −Nv · 2hv| ≤cϵ−1△log n] ≥1 −exp(−c log n) = 1 −1/nc.
IMBALANCED D,0.7967667436489607,"Lemma B.4 (DP Subtree Search). With probability 1 −2k/nc, costT
k
′(D) ≤5OPT T
k (D) +
4ckϵ−1△log n."
IMBALANCED D,0.7990762124711316,"Proof. The proof is similar to that of Lemma 3.5. Consider the intermediate output of Algorithm 4,
C1 = {v1, v2, ..., vk}, which is the set of roots of the minimal disjoint subtrees each containing
exactly one output center C0. Suppose one of the optimal “root set” that minimizes (4) is C∗
1 =
{v′
1, v′
2, ..., v′
k}. Assume C1 ̸= C∗
1. By the same argument as the proof of Lemma 3.5, we consider
for some i, j such that vi ̸= v′
j, where vi is not a descendent of v′
j and v′
j is either a descendent
of vi. By the construction of C1, we know that score(v′
j) ≤min{score(vi), i = 1, ..., k} when
v′
j ∈C∗
1 \ C1. Consider the swap between C1 and C∗
1. By the definition of tree distance, we have
OPT T
k (U) ≥P"
IMBALANCED D,0.8013856812933026,"vi∈C1\C∗
1 Nvi2hvi, since {T(vi), vi ∈C1 \ C∗
1} does not contain any center of the
optimal solution determined by C∗
1 (which is also the optimal “root set” for OPT T
k )."
IMBALANCED D,0.8036951501154734,"Let us consider the optimal clustering with center set be C∗= {c∗
1, c∗
2, ..., c∗
k} (each center c∗
j is a
leaf of subtree whose root be c′
j), and S′
j be the leaves assigned to c∗
j. Let Sj denote the set of leaves
in S′
j whose distance to c∗
j is strictly smaller than its distance to any centers in C1. Let Pj denote
the union of paths between leaves of Sj to its closest center in C1. Let v′′
j be the nodes in Pj with"
IMBALANCED D,0.8060046189376443,"highest level satisfying T(v′′
j ) ∩C1 = ∅. The score of v′′
j is 2
hv′′
j N(v′′
j ). That means the swap with a"
IMBALANCED D,0.8083140877598153,"center v′
j into C1 can only reduce 4 · 2
hv′′
j N(v′′
j ) to costT
k
′(U) (the tree distance between any leaf in"
IMBALANCED D,0.8106235565819861,"Sj and its closest center in C1 is at most 4 · 2
hv′′
j ). We just use v′
j to represent v′′
j for later part of this
proof for simplicity. Summing all the swaps over C∗
1 \ C1, we obtain"
IMBALANCED D,0.812933025404157,"costT
k
′(U) −OPT T
k (U) ≤4
X"
IMBALANCED D,0.815242494226328,"v′
j∈C∗
1 \C1
Nv′
j2
hv′
j ,"
IMBALANCED D,0.8175519630484989,"OPT T
k (U) ≥
X"
IMBALANCED D,0.8198614318706697,"vi∈C1\C∗
1
Nvi2hvi."
IMBALANCED D,0.8221709006928406,"Applying union bound with Lemma B.3, with probability 1 −2/nc, we have"
IMBALANCED D,0.8244803695150116,"Nv′
j2
hv′
j −Nvi2hvi ≤2cϵ−1△log n."
IMBALANCED D,0.8267898383371824,"Consequently, we have with probability, 1 −2k/nc,"
IMBALANCED D,0.8290993071593533,"costT
k
′(D) ≤5OPT T
k (D) + 4c|C1 \ C∗
1|ϵ−1△log n"
IMBALANCED D,0.8314087759815243,"≤5OPT T
k (D) + 4ckϵ−1△log n,"
IMBALANCED D,0.8337182448036952,which proves the claim.
IMBALANCED D,0.836027713625866,"Lemma B.5 (DP Leaf Search). With probability 1 −2k/nc, Algorithm 6 produces initial centers
with costT
k (D) ≤2costT
k
′(D) + 2ckϵ−1△log n."
IMBALANCED D,0.8383371824480369,"Proof. The proof strategy follows Lemma 3.6. We first consider one subtree with root v. Let
costT
1
′(v, U) denote the optimal k-median cost within the point set T(v) with one center in 2-HST:"
IMBALANCED D,0.8406466512702079,"costT
1
′(v, D) = min
x∈T (v) X"
IMBALANCED D,0.8429561200923787,"y∈T (v)∩D
ρT (x, y).
(11)"
IMBALANCED D,0.8452655889145496,"Suppose v has more than one children u, w, ..., and the optimal solution of costT
1
′(v, U) chooses a
leaf node in T(u), and our HST initialization algorithm picks a leaf of T(w). If u = w, then HST
chooses the optimal one where the argument holds trivially. Thus, we consider u ̸= w. We have the
following two observations:"
IMBALANCED D,0.8475750577367206,"• Since one needs to pick a leaf of T(u) to minimize costT
1
′(v, U), we have costT
1
′(v, U) ≥
P"
IMBALANCED D,0.8498845265588915,"x∈ch(v),x̸=u Nx · 2hx where ch(u) denotes the children nodes of u."
IMBALANCED D,0.8521939953810623,"• By our greedy strategy, costT
1 (v, U) ≤P"
IMBALANCED D,0.8545034642032333,"x∈ch(u) Nx · 2hx ≤costT
1
′(v, U) + Nu · 2hu."
IMBALANCED D,0.8568129330254042,"As hu = hw, leveraging Lemma B.3, with probability 1 −2/nc,"
IMBALANCED D,0.859122401847575,2hu · (Nu −Nw) ≤2hu( ˆNu −ˆNw) + 2cϵ−1△log n
IMBALANCED D,0.8614318706697459,≤2cϵ−1△log n.
IMBALANCED D,0.8637413394919169,"since our algorithm picks subtree roots with highest scores.
Then we have costT
1 (v, D) ≤
costT
k
′(v, D) + Nw · 2hu + 2cϵ−1△log n ≤2costT
k
′(v, D) + 2cϵ−1△log n with high probabil-
ity. Lastly, applying union bound over the disjoint k subtrees gives the desired result."
IMBALANCED D,0.8660508083140878,"B.5
Proof of Theorem 4.3"
IMBALANCED D,0.8683602771362586,"Proof. The privacy analysis is straightforward, by using the composition theorem (Theorem B.1).
Since the sensitivity of cost(·) is △, in each swap iteration the privacy budget is ϵ/2(T + 1). Also,
we spend another ϵ/2(T + 1) privacy for picking a output. Hence, the total privacy is ϵ/2 for local
search. Algorithm 6 takes ϵ/2 DP budget for initialization, so the total privacy is ϵ."
IMBALANCED D,0.8706697459584296,"The analysis of the approximation error follows from Gupta et al. (2010), where the initial cost is
reduced by our private HST method. We need the following two lemmas."
IMBALANCED D,0.8729792147806005,"Lemma B.6 (Gupta et al. (2010)). Assume the solution to the optimal utility is unique. For any
output o ∈O of 2△ϵ-DP exponential mechanism on dataset D, it holds for ∀t > 0 that"
IMBALANCED D,0.8752886836027713,"Pr[q(D, o) ≤max
o∈O q(D, o) −(ln |O| + t)/ϵ] ≤e−t,"
IMBALANCED D,0.8775981524249422,where |O| is the size of the output set.
IMBALANCED D,0.8799076212471132,"Lemma B.7 (Arya et al. (2004)). For any set F ⊆D with |F| = k, there exists some swap (x, y)
such that the local search method admits"
IMBALANCED D,0.8822170900692841,"costk(F, D) −costk(F −{x} + {y}, D) ≥costk(F, D) −5OPT(D) k
."
IMBALANCED D,0.8845265588914549,"From Lemma B.7, we know that when costk(Fi, D) > 6OPT(D), there exists a swap (x, y) s.t."
IMBALANCED D,0.8868360277136259,"costk(Fi −{x} + {y}, D) ≤(1 −1"
IMBALANCED D,0.8891454965357968,"6k )costk(Fi, D)."
IMBALANCED D,0.8914549653579676,"At each iteration, there are at most n2 possible outputs (i.e., possible swaps), i.e., |O| = n2. Using
Lemma B.6 with t = 2 log n, for ∀i,"
IMBALANCED D,0.8937644341801386,"Pr[costk(Fi+1, D) ≥costk(F ∗
i+1, D) + 4log n"
IMBALANCED D,0.8960739030023095,"ϵ′
] ≥1 −1/n2,"
IMBALANCED D,0.8983833718244804,"where costk(F ∗
i+1, D) is the minimum cost among iteration 1, 2, ..., t + 1. Hence, we have that
as long as cost(Fi, D) > 6OPT(D) + 24k log n"
IMBALANCED D,0.9006928406466512,"ϵ′
, the improvement in cost is at least by a factor of
(1 −1"
IMBALANCED D,0.9030023094688222,"6k). By Theorem 4.2, we have costk(F1, D) ≤C(log n)(6OPT(D) + 6k△log n/ϵ) for some
constant C > 0. Let T = 6Ck log log n. We have that"
IMBALANCED D,0.9053117782909931,"E[cost(Fi, D)] ≤(6OPT(D) + 6kϵ−1△log n)C(log n)(1 −1/6k)6Ck log log n"
IMBALANCED D,0.9076212471131639,"≤6OPT(D) + 6kϵ−1△log n ≤6OPT(D) + 24k log n ϵ′
."
IMBALANCED D,0.9099307159353349,"Therefore, with probability at least (1−T/n2), there exists an i ≤T s.t. cost(Fi, D) ≤6OPT(D)+
24k log n"
IMBALANCED D,0.9122401847575058,"ϵ′
. Then by using the Lemma B.7, one will pick an Fj with additional additive error 4 ln n/ϵ′"
IMBALANCED D,0.9145496535796767,"to the min{cost(Fj, D), j = 1, 2, ..., T} with probability 1 −1/n2. Consequently, we know that the
expected additive error is"
IMBALANCED D,0.9168591224018475,"24k△log n/ϵ′ + 4 log n/ϵ′ = O(ϵ−1k2△(log log n) log n),"
IMBALANCED D,0.9191685912240185,with probability 1 −1/poly(n).
IMBALANCED D,0.9214780600461894,"C
Extend HST Initialization to k-Means"
IMBALANCED D,0.9237875288683602,"Naturally, our HST method can also be applied to k-means clustering problem. In this section, we
extend the HST to k-means and provide some brief analysis similar to k-median. We present the
analysis in the non-private case, which can then be easily adapted to the private case. Define the
following costs for k-means."
IMBALANCED D,0.9260969976905312,"costT
km(U) =
X"
IMBALANCED D,0.9284064665127021,"y∈U
min
x∈C0 ρT (x, y)2,
(12)"
IMBALANCED D,0.930715935334873,"costT
km
′(U, C1) =
min
|F ∩T (v)|=1,∀v∈C1 X"
IMBALANCED D,0.9330254041570438,"y∈U
min
x∈F ρT (x, y)2,
(13)"
IMBALANCED D,0.9353348729792148,"OPT T
km(U) =
min
F ⊂U,|F |=k X"
IMBALANCED D,0.9376443418013857,"y∈U
min
x∈F ρT (x, y)2 ≡min
C′
1
costT
km
′(U, C′
1).
(14)"
IMBALANCED D,0.9399538106235565,"For simplicity, we will use costT
km
′(U) to denote costT
km
′(U, C1) if everything is clear from context.
Here, OPT T
km (14) is the cost of the global optimal solution with 2-HST metric."
IMBALANCED D,0.9422632794457275,"Lemma C.1 (Subtree search). costT
km
′(U) ≤17OPT T
km(U)."
IMBALANCED D,0.9445727482678984,"Proof. The analysis is similar with the proof of Lemma 3.5. Thus, we mainly highlight the difference.
Let us just use some notations the same as in Lemma 3.5 here. Let us consider the clustering with
center set be C∗= {c∗
1, c∗
2, ..., c∗
k} (each center c∗
j is a leaf of subtree whose root be c′
j), and S′
j be
the leaves assigned to c∗
j in optimal k-means clustering in tree metric. Let Sj denote the set of leaves
in S′
j whose distance to c∗
j is strictly smaller than its distance to any centers in C1. Let Pj denote the
union of paths between leaves of Sj to its closest center in C1. Let v′′
j be the nodes in Pj with highest"
IMBALANCED D,0.9468822170900693,"level satisfying T(v′′
j ) ∩C1 = ∅. The score of v′′
j is 2
hv′′
j N(v′′
j ). That means the swap with a center"
IMBALANCED D,0.9491916859122402,"v′
j into C1 can only reduce (4 · 2
hv′′
j )2N(v′′
j ) to costT
km
′(U). We just use v′
j to represent v′′
j for later
part of this proof for simplicity. By our reasoning, summing all the swaps over C∗
1 \ C1 gives"
IMBALANCED D,0.9515011547344111,"costT
km
′(U) −OPT T
km(U) ≤
X"
IMBALANCED D,0.953810623556582,"v′
j∈C∗
1 \C1
Nv′
j · (4 · 2
hv′
j )2,"
IMBALANCED D,0.9561200923787528,"OPT T
km(U) ≥
X"
IMBALANCED D,0.9584295612009238,"vi∈C1\C∗
1
Nvi(2hvi)2."
IMBALANCED D,0.9607390300230947,"Also, based on our discussion on Case 1, it holds that"
IMBALANCED D,0.9630484988452656,"Nv′
j2
hv′
j −Nvi2hvi ≤0."
IMBALANCED D,0.9653579676674365,"Summing them together, we have costT
km
′(U) ≤17OPT T
km(U)."
IMBALANCED D,0.9676674364896074,"Next, we show that the greedy leaf search strategy (Algorithm 5) only leads to an extra multiplicative
error of 2."
IMBALANCED D,0.9699769053117783,"Lemma C.2 (Leaf search). costT
km(U) ≤2costT
km
′(U)."
IMBALANCED D,0.9722863741339491,"Proof. Since the subtrees in C1 are disjoint, it suffices to consider one subtree with root v. With a
little abuse of notation, let costT
1
′(v, U) denote the optimal k-means cost within the point set T(v)
with one center in 2-HST:"
IMBALANCED D,0.9745958429561201,"costT
1
′(v, U) = min
x∈T (v) X"
IMBALANCED D,0.976905311778291,"y∈T (v)
ρT (x, y)2,
(15)"
IMBALANCED D,0.9792147806004619,"which is the optimal cost within the subtree. Suppose v has more than one children u, w, ..., otherwise
the optimal center is clear. Suppose the optimal solution of costT
1
′(v, U) chooses a leaf node in
T(u), and our HST initialization algorithm picks a leaf of T(w). If u = w, then HST chooses the
optimal one where the argument holds trivially. Thus, we consider u ̸= w. We have the following
two observations:"
IMBALANCED D,0.9815242494226328,"• Since one needs to pick a leaf of T(u) to minimize costT
1
′(v, U), we have costT
1
′(v, U) ≥
P"
IMBALANCED D,0.9838337182448037,"x∈ch(v),x̸=u Nx · (2hx)2 where ch(u) denotes the children nodes of u."
IMBALANCED D,0.9861431870669746,"• By our greedy strategy, costT
1 (v, U) ≤P"
IMBALANCED D,0.9884526558891455,"x∈ch(u) Nx·(2hx)2 ≤costT
1
′(v, U)+Nu·(2hu)2."
IMBALANCED D,0.9907621247113164,"Since hu = hw, we have
2hu · (Nu −Nw) ≤0,
since our algorithm picks subtree roots with highest scores.
Then we have costT
1 (v, U) ≤
costT
1
′(v, U) + Nw · (2hw)2 ≤2costT
1
′(v, U). Since the subtrees in C1 are disjoint, the union
of centers for OPT T
1 (v, U), v ∈C1 forms the optimal centers with size k. Note that, for any data
point p ∈U \ C1, the tree distance ρT (p, f) for ∀f that is a leaf node of T(v), v ∈C1 is the same.
That is, the choice of leaf in T(v) as the center does not affect the k-median cost under 2-HST metric.
Therefore, union bound over k subtree costs completes the proof."
IMBALANCED D,0.9930715935334873,"We are ready to state the error bound for our proposed HST initialization (Algorithm 4), which is a
natural combination of Lemma C.1 and Lemma C.2.
Theorem C.3 (HST initialization). costT
km(U) ≤34OPT T
km(U)."
IMBALANCED D,0.9953810623556582,"We have the following result based on Lemma 3.4.
Theorem C.4. In a general metric space,"
IMBALANCED D,0.9976905311778291,"E[costkm(U)] = O(min{log n, log △})2OPTkm(U)."
