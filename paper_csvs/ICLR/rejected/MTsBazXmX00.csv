Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0036496350364963502,"Target Propagation (TP) algorithms compute targets instead of gradients along
neural networks and propagate them backward in a way that is similar yet differ-
ent than gradient back-propagation (BP). The idea was ﬁrst presented as a pertur-
bative alternative to back-propagation that may improve gradient evaluation accu-
racy when training multi-layer neural networks (Le Cun et al., 1989). However,
TP may have remained more of a template algorithm with many variations than
a well-identiﬁed algorithm. Revisiting insights of Le Cun et al. (1989) and more
recently of Lee et al. (2015), we present a simple version of target propagation
based on a regularized inversion of network layers, easily implementable in a dif-
ferentiable programming framework. We compare its computational complexity
to the one of BP and delineate the regimes in which TP can be attractive compared
to BP. We show how our TP can be used to train recurrent neural networks with
long sequences on various sequence modeling problems. The experimental results
underscore the importance of regularization in TP in practice."
INTRODUCTION,0.0072992700729927005,"1
INTRODUCTION"
INTRODUCTION,0.010948905109489052,"Target propagation algorithms can be seen as perturbative learning alternatives to the gradient back-
propagation algorithm, where virtual targets are propagated backward instead of gradients (Le Cun,
1986; Le Cun et al., 1989; Rohwer, 1990; Mirowski & LeCun, 2009; Bengio, 2014; Goodfellow
et al., 2016). A high-level summary is presented in Fig. 1: while gradient back-propagation con-
siders storing intermediate gradients in a forward pass, target propagation algorithms proceed by
computing and storing approximate inverses. The approximate inverses are then passed on back-
ward along the graph of computations to ﬁnally yield a weight update for stochastic learning."
INTRODUCTION,0.014598540145985401,"Target propagation aims to take advantage of the availability of approximate inverses to compute
better descent directions for the objective at hand. Bengio et al. (2013); Bengio (2020) argued that
the approach could be relevant for problems involving multiple compositions such as the training of
Recurrent Neural Networks (RNNs), which generally suffer from the phenomenon of exploding or
vanishing gradients (Hochreiter, 1998; Bengio et al., 1994; ?). Recently, empirical results indeed
showed the potential advantages of target propagation over classical gradient back-propagation for
training RNNs on several tasks (Manchev & Spratling, 2020). However, these recent investigations
remain built on multiple approximations, which hinder the analysis of the core idea of TP, i.e., using
layer inverses."
INTRODUCTION,0.01824817518248175,"On the theoretical side, difference target propagation, a modern variant of target propagation, was
related to an approximate Gauss-Newton method, suggesting interesting venues to explain the ben-
eﬁts of target propagation (Bengio, 2020; Meulemans et al., 2020). Previous works have considered
approximating inverses by adding multiple reverse layers (Manchev & Spratling, 2020; Meulemans
et al., 2020; Bengio, 2020). However, it is unclear whether such reverse layers actually learn layer
inverses during the training process. Even if they were, the additional cost of computational com-
plexity of learning approximate inverses should be carefully accounted for."
INTRODUCTION,0.021897810218978103,"In this work, we propose a simple target propagation approach, revisiting the original insights
of Le Cun et al. (1989) on the critical importance of the good conditioning of layer inverses. We
deﬁne regularized inverses through a variational formulation and we obtain approximate inverses
via these regularized inverses. In this spirit, we can also interpret the difference target propagation
formula (Lee et al., 2015) as a ﬁnite difference approximation of a linearized regularized inverse.
We propose a smoother formula that can directly be integrated into a differentiable programming
framework."
INTRODUCTION,0.025547445255474453,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.029197080291970802,"GUadienW Back-PUoSagaWion
TaUgeW PUoSagaWion"
INTRODUCTION,0.032846715328467155,FoUZaUd PaVV
INTRODUCTION,0.0364963503649635,BackZaUd PaVV
INTRODUCTION,0.040145985401459854,"Fig. 1: Our implementation of target propagation uses linearization of gradient inverses instead of gradients in
a backward pass akin to gradient back-propagation."
INTRODUCTION,0.043795620437956206,"We detail the computational complexity of the proposed target propagation and compare it to the
one of gradient back-propagation, showing that the additional cost of computing inverses can be
effectively amortized for very long sequences. Following the benchmark of Manchev & Spratling
(2020), we observe that the proposed target propagation can perform better than classical gradient-
based methods on several tasks involving RNNs."
INTRODUCTION,0.04744525547445255,"Related work.
Many variations of back-propagation algorithms have been explored; see Werbos
(1994); Goodfellow et al. (2016) for an extensive bibliography. Closer to target propagation, pe-
nalized formulations of the training problem have been considered to decouple the optimization of
the weights in a distributed way or using an ADMM approach (Carreira-Perpinan & Wang, 2014;
Taylor et al., 2016; Gotmare et al., 2018). Rather than modifying the backward operations in the
layers, one can also modify the weight updates for deep forward networks by using a regularized
inverse (Frerix et al., 2018). Wiseman et al. (2017) recast target propagation as an ADMM-like algo-
rithm for language modeling and reported disappointing experimental results. Recently, in a careful
experimental benchmark evaluation, Manchev & Spratling (2020) explored further target propaga-
tion to train RNNs, mapping a sequence to a single ﬁnal output, in an attempt to understand the ben-
eﬁts of target propagation to capture long-range dependencies, and obtained promising experimental
results. Another line of research has considered synthetic gradients that approximate gradients us-
ing an additional layer instead of using back-propagated gradients (Jaderberg et al., 2017; Czarnecki
et al., 2017) to speed up the training of deep neural networks. Recently, Ahmad et al. (2020); Dalm
et al. (2021) considered using analytical inverses to implement target propagation and blend it with
what they called a gradient-adjusted incremental formula. Yet, an additional orthogonality penalty
is critical for their approach to work. Recently, Meulemans et al. (2020) considered using as many
reverse layers as forwarding operations. We focus here on the optimization gains of using target
propagation that cannot be obtained by adding a prohibitive number of reverse layers. Finally, we
do not discuss the biological plausibility of TP since we are unable to comment on this. We refer
the interested reader to, e.g., (Bengio, 2020)."
INTRODUCTION,0.051094890510948905,"Notations.
For f : Rp ⇥Rq ! Rd, we denote @xf(x, y) = !"
INTRODUCTION,0.05474452554744526,"@f j(x, y)/@xi """
INTRODUCTION,0.058394160583941604,"i,j 2 Rd⇥p."
TARGET PROPAGATION WITH LINEARIZED REGULARIZED INVERSES,0.06204379562043796,"2
TARGET PROPAGATION WITH LINEARIZED REGULARIZED INVERSES"
TARGET PROPAGATION WITH LINEARIZED REGULARIZED INVERSES,0.06569343065693431,"While target propagation was initially developed for multi-layer neural networks, we focus on its
implementation for recurrent neural networks, as we shall follow the benchmark of Manchev &
Spratling (2020) in the experiments. Recurrent Neural Networks (RNNs) are also a canonical family
of neural networks in which interesting phenomena arise in back-propagation algorithms."
TARGET PROPAGATION WITH LINEARIZED REGULARIZED INVERSES,0.06934306569343066,"Problem setting.
A simple RNN parameterized by ✓= (Whh, Wxh, bh, Why, by) maps a se-
quence of inputs x1:⌧= (x1, . . . , x⌧) to an output ˆy = g✓(x1:⌧) by computing hidden states ht 2 Rp
corresponding to the inputs xt."
TARGET PROPAGATION WITH LINEARIZED REGULARIZED INVERSES,0.072992700729927,"Formally, the output ˆy and the hidden states ht are computed as an output operation following
transition operations deﬁned as"
TARGET PROPAGATION WITH LINEARIZED REGULARIZED INVERSES,0.07664233576642336,"ˆy = c✓(h⌧) := s(Whyh⌧+ by),
ht = f✓,t(ht−1) := a(Wxhxt + Whhht−1 + bh)
for t 2 {1, . . . , ⌧},"
TARGET PROPAGATION WITH LINEARIZED REGULARIZED INVERSES,0.08029197080291971,Under review as a conference paper at ICLR 2022
TARGET PROPAGATION WITH LINEARIZED REGULARIZED INVERSES,0.08394160583941605,"where s is, e.g., the soft-max function for classiﬁcation tasks, a is a non-linear operation such as
the hyperbolic tangent function, and the initial hidden state is generally ﬁxed as h0 = 0. Given
samples of sequence-output pairs (x1:⌧, y), the RNN is trained to minimize the error `(y, g✓(x1:⌧))
of predicting ˆy = g✓(x1:⌧) instead of y."
TARGET PROPAGATION WITH LINEARIZED REGULARIZED INVERSES,0.08759124087591241,"As one considers longer sequences, RNNs face the challenge of exploding/vanishing gradients
@g✓(x1:⌧)/@ht (Bengio & Frasconi, 1995); see Appendix A for more discussion. We acknowl-
edge that speciﬁc parameterization-based strategies have been proposed to address this issue of ex-
ploding/vanishing gradients, such as orthonormal parameterizations of the weights (Arjovsky et al.,
2016; Helfrich et al., 2018; Lezcano-Casado & Martınez-Rubio, 2019). The focus here is to simplify
and understand target propagation as a backpropagation-type algorithm using RNNs as a workbench.
Indeed, training RNNs is an optimization problem involving multiple compositions for which ap-
proximate inverses can easily be available. The framework could also be potentially applied to, e.g.,
time-series or control models (Roulet et al., 2019)."
TARGET PROPAGATION WITH LINEARIZED REGULARIZED INVERSES,0.09124087591240876,"Given the parameters Whh, Wxh, bh of the transition operations, we can get approximate inverses
of f✓,t(ht−1) for all t 2 {1, . . . , ⌧}, that yield optimization surrogates that can be better perform-
ing than the ones corresponding to regular gradients. We present below a simple version of target
propagation based on regularized inverses and inverse linearizations."
TARGET PROPAGATION WITH LINEARIZED REGULARIZED INVERSES,0.0948905109489051,"Back-propagating targets.
The idea of target propagation is to compute virtual targets vt for each
layer t = ⌧, . . . , 1 such that if the layers were able to match their corresponding target at time t, i.e.,
f✓,t(ht−1) ⇡vt, the objective would decrease. The ﬁnal target v⌧is computed as a gradient step on
the loss w.r.t. h⌧. The targets are then back-propagated using an approximate inverse1 f −1"
TARGET PROPAGATION WITH LINEARIZED REGULARIZED INVERSES,0.09854014598540146,"✓,t of f✓,t
at each time step."
TARGET PROPAGATION WITH LINEARIZED REGULARIZED INVERSES,0.10218978102189781,"Formally, consider an RNN that computed ⌧states h1, . . . , h⌧from a sequence x1, . . . , x⌧with
associated output y. For a given stepsize γh > 0, we propose to back-propagate targets by computing"
TARGET PROPAGATION WITH LINEARIZED REGULARIZED INVERSES,0.10583941605839416,"v⌧= h⌧−γh@h`(y, c✓(h⌧)),
(1)"
TARGET PROPAGATION WITH LINEARIZED REGULARIZED INVERSES,0.10948905109489052,vt−1 = ht−1 + @hf −1
TARGET PROPAGATION WITH LINEARIZED REGULARIZED INVERSES,0.11313868613138686,"✓,t (ht)"
TARGET PROPAGATION WITH LINEARIZED REGULARIZED INVERSES,0.11678832116788321,">(vt −ht),
for t 2 {⌧, . . . , 1}.
(2)"
TARGET PROPAGATION WITH LINEARIZED REGULARIZED INVERSES,0.12043795620437957,"The update rule (2) blends two ideas: i) regularized inversion; ii) linear approximation. We shall
describe below that our update (2) allows us to interpret the “magic formula” of difference target
propagation in Eq. 15 of Lee et al. (2015) as 0th-order ﬁnite difference approximation, while ours is
a 1st-order linear approximation. We shall also show that (2) puts in practice an insight from Bengio
(2020) suggesting to use the inverse of the gradients in the spirit of a Gauss-Newton method."
TARGET PROPAGATION WITH LINEARIZED REGULARIZED INVERSES,0.12408759124087591,"Once all targets are computed, the parameters of the transition operations are updated such that the
outputs of f✓,t at each time step move closer to the given target. Formally, the update consists of a
gradient step with stepsize γ✓on the squared error between the targets and the current outputs, i.e.,
for ✓h 2 {Whh, Wxh, bh}, ✓next"
TARGET PROPAGATION WITH LINEARIZED REGULARIZED INVERSES,0.12773722627737227,"h
= ✓h −γ✓ ⌧
X t=1"
TARGET PROPAGATION WITH LINEARIZED REGULARIZED INVERSES,0.13138686131386862,"@✓hkf✓,t(ht−1) −vtk2"
TARGET PROPAGATION WITH LINEARIZED REGULARIZED INVERSES,0.13503649635036497,"2/2.
(3)"
TARGET PROPAGATION WITH LINEARIZED REGULARIZED INVERSES,0.1386861313868613,"As for the parameters ✓y = (Why, by) of the output operation, they are updated by a simple gradient
step on the loss with a stepsize γ✓."
REGULARIZED INVERSION,0.14233576642335766,"2.1
REGULARIZED INVERSION"
REGULARIZED INVERSION,0.145985401459854,"To explore further the original idea of Le Cun et al. (1989), we consider using the variational deﬁni-
tion of the inverse, f −1"
REGULARIZED INVERSION,0.14963503649635038,"✓,t (vt) = argmin"
REGULARIZED INVERSION,0.15328467153284672,"vt−12Rp kf✓,t(vt−1) −vtk2"
REGULARIZED INVERSION,0.15693430656934307,2 = argmin
REGULARIZED INVERSION,0.16058394160583941,vt−12Rp ka(Wxhxt + Whhvt−1 + bh) −vtk2
REGULARIZED INVERSION,0.16423357664233576,"2.
(4)"
REGULARIZED INVERSION,0.1678832116788321,"As long as vt belongs to the image f✓,t(Rp) of f✓,t, this deﬁnition recovers exactly the inverse
of vt by f✓,t. More generally, if vt 62 f✓,t(Rp), Eq. (4) computes the best approximation of the"
REGULARIZED INVERSION,0.17153284671532848,"1In the following, to ease the presentation, we abuse notations and denote approximate inverses by f −1 ✓,t ."
REGULARIZED INVERSION,0.17518248175182483,Under review as a conference paper at ICLR 2022
REGULARIZED INVERSION,0.17883211678832117,"inverse in the sense of the Euclidean projection. When one considers an activation function a and
✓h = (Whh, Wxh, bh), the solution of (4) can easily be computed."
REGULARIZED INVERSION,0.18248175182481752,"Formally, for the sigmoid, the hyperbolic tangent or the ReLU, their inverse can be obtained analyt-
ically for any vt 2 a(Rp). So for vt 2 a(Rp) and Whh full rank, we get f −1"
REGULARIZED INVERSION,0.18613138686131386,"✓,t (vt) = (W >"
REGULARIZED INVERSION,0.1897810218978102,hhWhh)−1W >
REGULARIZED INVERSION,0.19343065693430658,hh(a−1(vt) −Wxhxt −bh).
REGULARIZED INVERSION,0.19708029197080293,"If vt 62 a(Rp), the minimizer of (4) is obtained by ﬁrst projecting vt onto a(Rp), before inverting
the linear operation. To account for non-invertible matrices Whh, we also add a regularization in
the computation of the inverse. Overall we consider approximating the inverse of the layer by a
regularized inverse of the form f −1"
REGULARIZED INVERSION,0.20072992700729927,"✓,t (vt) = (W >"
REGULARIZED INVERSION,0.20437956204379562,hhWhh + r I)−1W >
REGULARIZED INVERSION,0.20802919708029197,"hh(a−1(⇡(vt)) −Wxhxt −bh),"
REGULARIZED INVERSION,0.2116788321167883,with r > 0 and ⇡a projection onto a(Rp).
REGULARIZED INVERSION,0.21532846715328466,"Regularized inversion vs.
parameterized inversion.
Bengio (2014); Manchev & Spratling
(2020) parameterize the inverse as a reverse layer such that f −1"
REGULARIZED INVERSION,0.21897810218978103,"✓,t (vt) =  ✓0,t(vt) := a(Wxhxt + V vt + c),"
REGULARIZED INVERSION,0.22262773722627738,"and learn the parameters ✓0 = (V, c) for this reverse layer to approximate the inverse of the forward
computations. The parameterized layer needs to be learned to get a good approximation which in-
volves numerically solving an optimization problem for each layer. These optimization problems
come with a computational cost that can be better controlled by using regularized inversions pre-
sented earlier."
REGULARIZED INVERSION,0.22627737226277372,"However, the approach based on parameterized inverses may lack theoretical grounding, as pointed
out by Bengio (2020), as we do not know how close the learned inverse is to the actual inverse
throughout the training process. In contrast, the regularized inversion (4) is less ad hoc and clearly
deﬁned and, as we shall show in the experiments, leads to competitive performance on real datasets."
REGULARIZED INVERSION,0.22992700729927007,"In any case, the analytic formulation of the inverse gives simple insights on an approach with pa-
rameterized inverses. Namely, the analytical formula suggests parameterizing the reverse layer s.t.
(i) the reverse activation is deﬁned as the inverse of the activation and not any activation, (ii) the
layer uses a non-linear operation followed by a linear one instead of the usual scheme, i.e., a linear
operation followed by a non-linear one."
LINEARIZED INVERSION,0.23357664233576642,"2.2
LINEARIZED INVERSION"
LINEARIZED INVERSION,0.23722627737226276,"Earlier instances of target propagation used direct inverses of the network layers such that the target
propagation update formula would read vt−1 = f −1"
LINEARIZED INVERSION,0.24087591240875914,"✓,t (vt) in (2). Yet, we are unaware of a success-
ful implementation of TP using directly the inverses. To circumvent this issue, Lee et al. (2015)
proposed the difference target propagation formula that back-propagates the targets as"
LINEARIZED INVERSION,0.24452554744525548,vt−1 = ht−1 + f −1
LINEARIZED INVERSION,0.24817518248175183,"✓,t (vt) −f −1"
LINEARIZED INVERSION,0.2518248175182482,"✓,t (ht)."
LINEARIZED INVERSION,0.25547445255474455,"If the inverses were exact, the difference target propagation formula would reduce to vt−1 =
f −1"
LINEARIZED INVERSION,0.2591240875912409,"✓,t (vt). Lee et al. (2015) introduced the difference target propagation formula to mitigate the
approximation error of the inverses by parameterized layers. In practice, difference-type target prop-
agation appears to be the only known successful implementation of target propagation we are aware
of. The difference target propagation formula can naturally be interpreted as an approximation of
the linearization used in (2), as f −1"
LINEARIZED INVERSION,0.26277372262773724,"✓,t (vt) −f −1"
LINEARIZED INVERSION,0.2664233576642336,"✓,t (ht) = @hf −1"
LINEARIZED INVERSION,0.27007299270072993,"✓,t (ht)"
LINEARIZED INVERSION,0.2737226277372263,">(vt −ht) + O(kvt −htk2 2),"
LINEARIZED INVERSION,0.2773722627737226,where @hf −1
LINEARIZED INVERSION,0.28102189781021897,"✓,t (ht)"
LINEARIZED INVERSION,0.2846715328467153,> denotes the Jacobian of the inverse of the layer at ht.
LINEARIZED INVERSION,0.28832116788321166,"We show in Appendix D that the ﬁrst-order approximation we propose (2) leads to slightly better
training curves than the ﬁnite-difference approximation. Moreover, our interpretation illuminates
the “mystery” of this formula, which appeared to be critical to the success of target propagation."
LINEARIZED INVERSION,0.291970802919708,Under review as a conference paper at ICLR 2022
LINEARIZED INVERSION,0.2956204379562044,"...
...  ... ... ... ..."
LINEARIZED INVERSION,0.29927007299270075,"Fig. 2: The graph of computations of target propagation is the same as the one of gradient back-propagation
except that f −1 needs to be computed and Jacobian of the inverses, @hf −1 > are used instead of gradients @hf
in the transition operations."
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.3029197080291971,"3
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.30656934306569344,"Graph of computations.
Gradient back-propagation and target propagation both compute a de-
scent direction for the objective at hand. The difference lies in the oracles computed and stored
in the forward pass, while the graph of computations remains the same. To clarify this view, we
reformulate target propagation in terms of displacements λt := vt −ht such that Eq. (1), (2) and (3)
read"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.3102189781021898,"λ⌧= −γh@h`(y, c✓(h⌧)),
λt−1 = @hf −1"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.31386861313868614,"✓,t (ht)"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.3175182481751825,">λt,
for t 2 {⌧, . . . , 1}, d✓h = ⌧
X t=1"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.32116788321167883,"@✓hf✓,t(ht−1)λt,
✓next"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.3248175182481752,"h
= ✓h + γhd✓h."
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.3284671532846715,"Target propagation amounts then to computing a descent direction d✓h for the parameters ✓h with
a graph of computations, illustrated in Fig. 2, analogous to that of gradient-back-propagation illus-
trated in Appendix A. The difference lies in the use of the Jacobian of the inverse"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.33211678832116787,@hf −1
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.3357664233576642,"✓,t (ht)"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.33941605839416056,">
instead of
@hf✓,t(ht−1)."
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.34306569343065696,"The implementation of TP with the formula (2) can be done in a differentiable programming frame-
work, where, rather than computing the gradient of the layer, one evaluates the inverse and keep the
Jacobian of the inverse. With the precise graph of computation of TP and BP, we can compare their
computational complexity explicitly and bound the difference of the directions they output."
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.3467153284671533,"Arithmetic complexity.
Clearly, the space complexities of gradient back-propagation (BP) and
our implementation of target propagation (TP) are the same since the Jacobians of the inverse, and
the original gradients have the same size. In terms of time complexity, TP appears at ﬁrst glance
to introduce an important overhead since it requires the computation of some inverses. However, a
close inspection of the formula of the regularized inverse reveals that a matrix inversion needs to be
computed only once for all time steps. Therefore the cost of the inversion may be amortized if the
length of the sequence is particularly long."
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.35036496350364965,"Formally, the time complexity of the forward-backward pass of gradient back-propagation is essen-
tially driven by matrix-vector products, i.e., TBP = ⌧
X t=1 2"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.354014598540146,"4T (f✓,t) + T (@hf✓,t) + T (@✓hf✓,t)
|
{z
}
Forward"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.35766423357664234,"+ T (@hf✓,t(ht−1)) + T (@✓f✓,t(ht−1))
|
{z
}
Backward 3 5"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.3613138686131387,"⇡⌧(dp + p2 + pq) + ⌧(p2 + pq),"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.36496350364963503,"where d is the dimension of the input xt, q is the dimension of the parameters ✓h, for a function
f we denote by T (f) the time complexity to evaluate f and we consider e.g. @✓f✓,t(ht−1)) as the
linear function λ ! @✓f✓,t(ht−1))λ."
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.3686131386861314,Under review as a conference paper at ICLR 2022
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.3722627737226277,"On the other hand, the time complexity of target propagation is TTP = ⌧
X t=1 2"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.3759124087591241,"64T (f✓,t)+T (f −1"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.3795620437956204,"✓,t )+T (@✓hf✓,t) +T (@hf −1"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.38321167883211676,"✓,t )
|
{z
}
Forward"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.38686131386861317,+ T (@hf −1
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.3905109489051095,"✓,t )(ht)>)+T (@✓f✓,t(ht−1))
|
{z
}
Backward 3 75"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.39416058394160586,+ P(f −1
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.3978102189781022,"✓,t ),"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.40145985401459855,where P(f −1
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.4051094890510949,"✓,t ) is the cost of encoding the inverse, which, in our case, amounts to the cost of encoding
g✓: z ! (W >"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.40875912408759124,hhWhh + r I)−1W >
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.4124087591240876,"hh, such that our regularized inverse can be computed as f −1"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.41605839416058393,"✓,t (vt) =
g✓(a−1(vt)−Wxhxt+bh). Encoding g comes at the cost of inverting one matrix of size p. Therefore,
the time-complexity of target propagation can be estimated as"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.4197080291970803,"TTP ⇡p3 + ⌧(dp + p2 + pq) + ⌧(p2 + pq) ⇡TBP
if ⌧≥p,"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.4233576642335766,"i.e., for long sequences whose length is larger than the dimension of the hidden states, the cost of TP
with regularized inverses is approximately the same as the cost of BP. If a parameterized inverse was
used rather than a regularized inverse, the cost of encoding the inverse would correspond to the cost
of updating the reverse layers by, e.g., a stochastic gradient descent. This update has a cost similar
to BP. However, it is unclear whether these updates get us close to the actual inverses."
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.42700729927007297,"Bounding the difference between target propagation and gradient back-propagation.
As the
computational graphs of BP and TP are the same, we can bound the difference between the oracles
returned by both methods. First, note that the updates of the parameters of the output functions are
the same since, in TP, gradients steps of the loss are used to update these parameters. The difference
between TP and BP lies in the updates with respect to the parameters of the transition operations.
For BP, the updates are computed by chain rule as"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.4306569343065693,"@✓h` (y, g✓(x1:⌧)) = ⌧
X t=1"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.4343065693430657,"@✓hf✓,t(ht−1)@h⌧ @ht"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.43795620437956206,"@h`(y, c✓(h⌧)),"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.4416058394160584,where the term @h⌧/@ht decomposes along the time steps as @h⌧/@ht = Q⌧
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.44525547445255476,"s=t+1 @hf✓,s(hs−1).
The direction computed by TP has the same structure, namely it can be decomposed for γh = 1 as d✓= ⌧
X t=1"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.4489051094890511,"@✓hf✓,t(ht−1) ˆ@h⌧ ˆ@ht"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.45255474452554745,"@h`(y, c✓(h⌧)),"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.4562043795620438,where ˆ@h⌧/ˆ@ht = Q⌧
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.45985401459854014,s=t+1 @hf −1
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.4635036496350365,"✓,s (hs)"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.46715328467153283,">. We can then bound the difference between the directions
given by BP or TP as, for any matrix norm k · k as formally stated in the following lemma."
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.4708029197080292,"Lemma 3.1.
The difference between the oracle returned by gradient back-propagation
@✓h` (y, g✓(x1:⌧)) and the oracle returned by target propagation can be bounded as"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.4744525547445255,"k@✓h` (y, g✓(x1:⌧)) −d✓k c
sup
t=1,...,⌧"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.4781021897810219,"k@hf✓,t(ht−1) −@hf −1"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.48175182481751827,"✓,t (ht) >k,"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.4854014598540146,"where
c
=
P⌧ t=1 Pt−1"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.48905109489051096,"s=0 asbt−1−s
with
a
=
supt=1,...⌧k@hf✓,t(ht−1)k, b
="
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.4927007299270073,"supt=1,...⌧k@hf −1"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.49635036496350365,"✓,t (ht) >k."
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.5,"For regularized inverses, we have, denoting ut = Wxhxt + Whhht−1 + bh,"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.5036496350364964,"k@hf✓,t(ht−1) −@hf −1"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.5072992700729927,"✓,t (ht)"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.5109489051094891,>k kW > hhk ⇣
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.5145985401459854,kra(ut) −ra(ut)−1k + k I −(W >
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.5182481751824818,hhWhh + r I)−1kkra(ut)−1k ⌘ .
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.5218978102189781,"For the two oracles to be close, we then need the preactivation ut = Wxhxt + Whhht−1 + bh to
lie in the region of the activation function that is close to being linear s.t. ra(ut) ⇡I. We also
need (W >"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.5255474452554745,"hhWhh + r I)−1 to be close to the identity which can be the case if, e.g., r = 0 and the
weight matrices Wh were orthonormal. By initializing the weight matrices as orthonormal matrices,
the differences between the two oracles can be closer. However, in the long term, target propagation
appears to give better oracles, as shown in the experiments below."
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.5291970802919708,Under review as a conference paper at ICLR 2022
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.5328467153284672,"0
2
4
Iterations
⇥104 0 2000 4000"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.5364963503649635,Train Loss
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.5401459854014599,"0
2
4
Iterations
⇥104
0 2000 4000"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.5437956204379562,"0
1
2
Iterations
⇥104
0 50 100"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.5474452554744526,"0
2
4
Iterations
⇥104
0 25 50 75 100"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.551094890510949,Accuracy
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.5547445255474452,"0
2
4
Iterations
⇥104
0 25 50 75 100"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.5583941605839416,"0
1
2
Iterations
⇥104
0 25 50 75 100 TP
BP"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.5620437956204379,"Fig. 3: Temporal order problem T = 60, Temporal Problem T = 120, Adding problem T = 30."
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.5656934306569343,"Target propagation as a Gauss-Newton method?
Recently target propagation has been inter-
preted as an approximate Gauss-Newton method, by considering that the difference target propa-
gation formula approximates the linearization of the inverse, which itself is a priori equal to the
inverse of the gradients (Bengio, 2020; Meulemans et al., 2020; 2021). Namely, provided that
f −1"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.5693430656934306,"✓,t (f✓,t(ht−1)) ⇡ht−1 such that @hf✓,t(ht−1)@hf −1"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.572992700729927,"✓,t (ht) ⇡I, we have"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.5766423357664233,@hf −1
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.5802919708029197,"✓,t (ht) ⇡(@hf✓,t(ht−1))−1 ."
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.583941605839416,"By composing the inverses of the gradients, we get an update similar to the one of Gauss-Newton
(GN) method. Namely, recall that if n invertible functions f1, . . . , fn were composed to solve a
least square problem of the form kfn ◦. . . ◦f1(x) −yk2"
GRADIENT BACK-PROPAGATION VERSUS TARGET PROPAGATION,0.5875912408759124,"2, a Gauss-Newton update would take the
form x(k+1) = x(k) −@x0f1(x0)−>@x1f2(x1)−> . . . @xn−1f(xn−1)−>(xn −y) where xt is deﬁned
iteratively as x0 = x(k), xt+1 = ft(xt). In other words, GN and TP share the idea of composing the
inverse of gradients. However, numerous differences remain: (i) in e.g. RNNs, the gradients w.r.t.
to the weights are deﬁned as a sum of the composition of the gradients and the inverse of this sum
is a priori not the sum of the inverses, (ii) if the real rationale of GN was used, the gradients w.r.t.
the loss should also be inverted, and the gradients w.r.t. to the weights should also be inverted which
is not the case in the usual implementation of TP Lee et al. (2015); Bengio (2020). Even if TP was
approximating GN, it is unclear whether GN updates are adapted to stochastic problems."
EXPERIMENTS,0.5912408759124088,"4
EXPERIMENTS"
EXPERIMENTS,0.5948905109489051,"In the following, we compare our simple target propagation approach, which we shall refer to as TP,
to gradient Back-Propagation referred to as BP. We follow the experimental benchmark of Manchev
& Spratling (2020) to which we add results on RNNs on CIFAR and GRUs on FashionMNIST.
Additional experiments, details on the initialization and the hyper-parameter selection can be found
in Appendix D."
EXPERIMENTS,0.5985401459854015,"Data.
We consider two synthetic datasets generated to present training difﬁculties for RNNs and
several real datasets consisting of scanning images pixel by pixel to classify them (Hochreiter &
Schmidhuber, 1997; Le et al., 2015; Manchev & Spratling, 2020)."
EXPERIMENTS,0.6021897810218978,"Temporal order problem. A sequence of length T is generated using a set of randomly chosen
symbols {a, b, c, d}. Two additional symbols X and Y are added at positions t1 2 [T/10, 2T/10]
and t2 2 [4T/10, 5T/10]. The network must predict the correct order of appearance of X and Y
out of four possible choices {XX, XY, Y X, Y Y }."
EXPERIMENTS,0.6058394160583942,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.6094890510948905,"0
2
4
Iterations
⇥104 2000 4000 6000 8000"
EXPERIMENTS,0.6131386861313869,Train Loss
EXPERIMENTS,0.6167883211678832,"0
2
4
Iterations
⇥104 2000 4000 6000 8000"
EXPERIMENTS,0.6204379562043796,"0.0
2.5
5.0
7.5
Iterations
⇥103
7000 7100 7200"
EXPERIMENTS,0.6240875912408759,"0.0
2.5
5.0
7.5
Iterations
⇥103 6000 7000 8000"
EXPERIMENTS,0.6277372262773723,"0
2
4
Iterations
⇥104
0 25 50 75 100"
EXPERIMENTS,0.6313868613138686,Accuracy
EXPERIMENTS,0.635036496350365,"0
2
4
Iterations
⇥104
0 25 50 75 100"
EXPERIMENTS,0.6386861313868614,"0.0
2.5
5.0
7.5
Iterations
⇥103
0 10 20"
EXPERIMENTS,0.6423357664233577,"0.0
2.5
5.0
7.5
Iterations
⇥103
0 10 20 30 40 TP
BP"
EXPERIMENTS,0.6459854014598541,"Fig. 4: Image classiﬁcation pixel by pixel. From left to right: MNIST, MNIST with permuted images, CI-
FAR10, FashionMNIST with GRU."
EXPERIMENTS,0.6496350364963503,"Adding problem. The input consists of two sequences: one is made of randomly chosen numbers
from [0, 1], and the other one is a binary sequence full of zeros except at positions t1 2 [1, T/10]
and t2 2 [T/10, T/2]. The second position acts as a marker for the time steps t1 and t2. The goal of
the network is to output the mean of the two random numbers of the ﬁrst sequence (Xt1 + Xt2)/2."
EXPERIMENTS,0.6532846715328468,"Image classiﬁcation pixel by pixel.
The inputs are images of (i) grayscale handwritten digits
given in the database MNIST (LeCun & Cortes, 1998), (ii) colored objects from the database
CIFAR10 (Krizhevsky, 2009) or (iii) grayscale images of clothes from the database FashionM-
NIST (Xiao et al., 2017). The images are scanned pixel by pixel and channel by channel for CI-
FAR10, and fed to a sequential network such as a simple RNN or a GRU network (Cho et al.,
2014). The inputs are then sequences of 28 ⇥28 = 784 pixels for MNIST or FashionMNIST and
32 ⇥32 ⇥3 = 3072 pixels for CIFAR with a very long-range dependency problem. We also con-
sider permuting the images of MNIST by a ﬁxed permutation before feeding them into the network,
which gives potentially longer dependencies in the sequential data."
EXPERIMENTS,0.656934306569343,"Model.
In both synthetic settings, we consider randomly generated mini-batches of size 20, a sim-
ple RNN with hidden states of dimension 100, and hyperbolic tangent activation. For the temporal
order problem, the last layer uses a soft-max function on top of a linear operation, and the loss is
the cross-entropy. For the adding problem, the last layer is linear, the loss is the mean-squared error,
and a sample is considered to be accurately predicted if the mean squared error is less than 0.04 as
done by (Manchev & Spratling, 2020)."
EXPERIMENTS,0.6605839416058394,"For the classiﬁcation of images with sequential networks, we consider mini-batches of size 16 and a
cross-entropy loss. For MNIST and CIFAR, we consider a simple RNN with hidden states of dimen-
sion 100, hyperbolic tangent activation, and a softmax output. For FashionMNIST, we consider a
GRU network and adapted our implementation of target propagation to that case while using hidden
states of dimension 100 and a softmax output."
EXPERIMENTS,0.6642335766423357,"Target propagation can tackle long sequences better than gradient back-propagation.
In
Fig. 3, we observe that TP performs better than BP on the temporal ordering problem: it is able
to reach 100% accuracy in fewer iterations than BP for sequences of length 60 and, for sequences
of length 120, it is still able to reach 100% accuracy in fewer than 40 000 iterations while BP is
not. On the other hand, for the adding problem, TP performs less well than BP. The contrast in
performance between the two synthetic tasks was also observed by (Manchev & Spratling, 2020)
using difference target propagation with parameterized inverses.
The main difference between
these tasks is the different nature of the outputs, which are binary for the temporal problem and
continuous for the adding problem."
EXPERIMENTS,0.6678832116788321,"In Fig. 4, we observe that TP generally performs better than BP for image classiﬁcation tasks. For
the MNIST dataset, it reaches around 74% accuracy after 4·104 iterations. This phenomenon is also"
EXPERIMENTS,0.6715328467153284,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.6751824817518248,Region of parameters
EXPERIMENTS,0.6788321167883211,Zith conYergence
EXPERIMENTS,0.6824817518248175,(6a) Conv. w.r.t. stepsize & regularization
EXPERIMENTS,0.6861313868613139,"4
14
49
196
392
784
Length 16 32 64 128 256 512 Width BP TP"
EXPERIMENTS,0.6897810218978102,(6b) Perf. vs width & length.
EXPERIMENTS,0.6934306569343066,"observed with permuted images, where the optimization appears smoother, and TP obtains around
86% accuracy after 4 · 104 iterations and is still faster than BP. On the CIFAR dataset, no algorithms
appear to reach a signiﬁcant accuracy, though TP is still faster. On the FashionMNIST dataset,
where a GRU network is used, our implementation of TP performs on par with BP, which shows
that our approach can be generalized to more complex networks than a simple RNN."
EXPERIMENTS,0.6970802919708029,"Target propagation requires a non-zero regularization term.
As mentioned in Sec. 3, by using
an analytical formula to compute the inverse of the layers, we can question the interpretation of TP
as a Gauss-Newton method, which would amount to TP without regularization. To understand the
effect of the regularization term, we computed the area under the training loss curve of TP for 400
iterations on a log10 grid of varying step-sizes γ✓and regularizations r for a ﬁxed γh = 10−3. The
results are presented in Fig. 6a, where the smaller the area, the brighter the point and the absence
of dots in the grid mean that the algorithm diverged. Fig. 6a shows that without regularization we
were not able to obtain convergence of the algorithm. Simply using the gradients of the inverse as in
a Gauss-Newton method may not directly work for RNNs. Additional modiﬁcations of the method
could be added to make target propagation closer to Gauss-Newton, such as inverting the layers with
respect to their parameters as proposed by Bengio (2020). For now, the regularization appears to
successfully handle the rationale of target propagation."
EXPERIMENTS,0.7007299270072993,"Target propagation is adapted for long sequences.
In Fig. 6b, we compare the performance of
BP and TP in terms of accuracy after 400 iterations on the MNIST problem for various widths deter-
mined by the size of the hidden states and various lengths determined by the size of the inputs (i.e.,
we feed the RNN with k pixels at a time, which gives a length 784/k). Fig 6b shows that TP is gen-
erally appropriate for long sequences, while BP remains more efﬁcient for short sequences. TP can
then be seen as an interesting alternative for dynamical problems which involve many discretization
steps as in RNNs and related architectures."
EXPERIMENTS,0.7043795620437956,CONCLUSION
EXPERIMENTS,0.708029197080292,"We proposed a simple target propagation approach grounded in two important computational com-
ponents, regularized inversion, and linearized propagation. The proposed approach also sheds light
on previous insights and successful rules for target propagation. We will publicly release our code
to facilitate the reproduction of the results. We have used target propagation within a stochastic gra-
dient outer loop to train neural networks for a fair comparison to stochastic gradient using gradient
backpropagation. Developing adaptive stochastic gradient algorithms in the spirit of Adam that lead
to boosts in performance when using target propagation instead of gradient backpropagation is an
interesting avenue for future work. Continuous counterparts of target propagation in a neural ODE
spirit is also an interesting avenue for future work."
REFERENCES,0.7116788321167883,REFERENCES
REFERENCES,0.7153284671532847,"Nasir Ahmad, Marcel A van Gerven, and Luca Ambrogioni. Gait-prop: A biologically plausible"
REFERENCES,0.718978102189781,"learning rule derived from backpropagation of error. Advances in Neural Information Processing
Systems, 33, 2020."
REFERENCES,0.7226277372262774,Under review as a conference paper at ICLR 2022
REFERENCES,0.7262773722627737,"Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In"
REFERENCES,0.7299270072992701,"Proceedings of the 33rd International Conference on Machine Learning, 2016."
REFERENCES,0.7335766423357665,Yoshua Bengio. How auto-encoders could provide credit assignment in deep networks via target
REFERENCES,0.7372262773722628,"propagation. arXiv preprint arXiv:1407.7906, 2014."
REFERENCES,0.7408759124087592,Yoshua Bengio. Deriving differential target propagation from iterating approximate inverses. arXiv
REFERENCES,0.7445255474452555,"preprint arXiv:2007.15139, 2020."
REFERENCES,0.7481751824817519,Yoshua Bengio and Paolo Frasconi. Diffusion of context and credit information in markovian mod-
REFERENCES,0.7518248175182481,"els. Journal of Artiﬁcial Intelligence Research, 3:249–270, 1995."
REFERENCES,0.7554744525547445,"Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient"
REFERENCES,0.7591240875912408,"descent is difﬁcult. IEEE transactions on neural networks, 5(2):157–166, 1994."
REFERENCES,0.7627737226277372,"Yoshua Bengio, Nicholas Léonard, and Aaron Courville.
Estimating or propagating gradients
through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013."
REFERENCES,0.7664233576642335,Miguel Carreira-Perpinan and Weiran Wang. Distributed optimization of deeply nested systems. In
REFERENCES,0.7700729927007299,"Proceedings of the 17th International Conference on Artiﬁcial Intelligence and Statistics, 2014."
REFERENCES,0.7737226277372263,"Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-"
REFERENCES,0.7773722627737226,"ger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014."
REFERENCES,0.781021897810219,"Wojciech Marian Czarnecki, Grzegorz ´Swirszcz, Max Jaderberg, Simon Osindero, Oriol Vinyals,"
REFERENCES,0.7846715328467153,"and Koray Kavukcuoglu. Understanding synthetic gradients and decoupled neural interfaces. In
Proceedings of the 34th International Conference on Machine Learning, 2017."
REFERENCES,0.7883211678832117,"Sander Dalm, Nasir Ahmad, Luca Ambrogioni, and Marcel van Gerven. Scaling up learning with"
REFERENCES,0.791970802919708,"gait-prop. arXiv preprint arXiv:2102.11598, 2021."
REFERENCES,0.7956204379562044,"Olivier Devolder, François Glineur, and Yurii Nesterov.
First-order methods of smooth convex
optimization with inexact oracle. Mathematical Programming, 146(1-2):37–75, 2014."
REFERENCES,0.7992700729927007,"Thomas Frerix, Thomas Möllenhoff, Michael Moeller, and Daniel Cremers. Proximal backpropa-"
REFERENCES,0.8029197080291971,"gation. In Proceedings of the 6th International Conference on Learning Representations, 2018."
REFERENCES,0.8065693430656934,"Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. The MIT Press, 2016."
REFERENCES,0.8102189781021898,"Akhilesh Gotmare, Valentin Thomas, Johanni Brea, and Martin Jaggi. Decoupling backpropagation"
REFERENCES,0.8138686131386861,"using constrained optimization methods. In Credit Assignment in Deep Learning and Reinforce-
ment Learning Workshop (ICML 2018 ECA), 2018."
REFERENCES,0.8175182481751825,"Kyle Helfrich, Devin Willmott, and Qiang Ye. Orthogonal recurrent neural networks with scaled"
REFERENCES,0.8211678832116789,"cayley transform. In Proceedings of the 35th International Conference on Machine Learning,
2018."
REFERENCES,0.8248175182481752,Sepp Hochreiter. The vanishing gradient problem during learning recurrent neural nets and problem
REFERENCES,0.8284671532846716,"solutions. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 6(02):
107–116, 1998."
REFERENCES,0.8321167883211679,"Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):"
REFERENCES,0.8357664233576643,"1735–1780, 1997."
REFERENCES,0.8394160583941606,"Max Jaderberg, Wojciech Marian Czarnecki, Simon Osindero, Oriol Vinyals, Alex Graves, David"
REFERENCES,0.843065693430657,"Silver, and Koray Kavukcuoglu. Decoupled neural interfaces using synthetic gradients. In Pro-
ceedings of the 34th International Conference on Machine Learning, 2017."
REFERENCES,0.8467153284671532,"Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University"
REFERENCES,0.8503649635036497,"of Toronto, 2009."
REFERENCES,0.8540145985401459,"Quoc V Le, Navdeep Jaitly, and Geoffrey E Hinton. A simple way to initialize recurrent networks"
REFERENCES,0.8576642335766423,"of rectiﬁed linear units. arXiv preprint arXiv:1504.00941, 2015."
REFERENCES,0.8613138686131386,Under review as a conference paper at ICLR 2022
REFERENCES,0.864963503649635,Yann Le Cun. Learning process in an asymmetric threshold network. In Disordered systems and
REFERENCES,0.8686131386861314,"biological organization. Springer, 1986."
REFERENCES,0.8722627737226277,"Yann Le Cun, Conrad C Galland, and Geoffrey E Hinton. GEMINI: gradient estimation through"
REFERENCES,0.8759124087591241,"matrix inversion after noise injection. In Advances in neural information processing systems, pp.
141–148, 1989."
REFERENCES,0.8795620437956204,"Yann
LeCun
and
Corinna
Cortes.
MNIST
handwritten
digit
database.
http://yann.lecun.com/exdb/mnist/, 1998."
REFERENCES,0.8832116788321168,"Dong-Hyun Lee, Saizheng Zhang, Asja Fischer, and Yoshua Bengio. Difference target propagation."
REFERENCES,0.8868613138686131,"In Machine Learning and Knowledge Discovery in Databases. Springer, 2015."
REFERENCES,0.8905109489051095,Mario Lezcano-Casado and David Martınez-Rubio. Cheap orthogonal constraints in neural net-
REFERENCES,0.8941605839416058,"works: A simple parametrization of the orthogonal and unitary group. In Proceedings of the 36th
International Conference on Machine Learning, 2019."
REFERENCES,0.8978102189781022,Nikolay Manchev and Michael Spratling. Target propagation in recurrent neural networks. Journal
REFERENCES,0.9014598540145985,"of Machine Learning Research, 21(7):1–33, 2020."
REFERENCES,0.9051094890510949,"Alexander Meulemans, Francesco Carzaniga, Johan Suykens, João Sacramento, and Benjamin F."
REFERENCES,0.9087591240875912,"Grewe.
A theoretical framework for target propagation.
In Advances in Neural Information
Processing Systems 33, 2020."
REFERENCES,0.9124087591240876,"Alexander Meulemans, Matilde Tristany Farinha, Javier García Ordóñez, Pau Vilimelis Aceituno,"
REFERENCES,0.916058394160584,"João Sacramento, and Benjamin F Grewe. Credit assignment in neural networks through deep
feedback control. arXiv preprint arXiv:2106.07887, 2021."
REFERENCES,0.9197080291970803,Piotr Mirowski and Yann LeCun. Dynamic factor graphs for time series modeling. In Machine
REFERENCES,0.9233576642335767,"Learning and Knowledge Discovery in Databases. Springer, 2009."
REFERENCES,0.927007299270073,"Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. Understanding the exploding gradient prob-"
REFERENCES,0.9306569343065694,"lem. arXiv preprint arXiv:1211.5063, 2012."
REFERENCES,0.9343065693430657,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor"
REFERENCES,0.9379562043795621,"Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep
learning library. In Advances in Neural Information Processing Systems 32. 2019."
REFERENCES,0.9416058394160584,"Richard Rohwer.
The “moving targets"" training algorithm.
In Advances in neural information
processing systems 3, 1990."
REFERENCES,0.9452554744525548,"Vincent Roulet, Siddhartha Srinivasa, Dmitriy Drusvyatskiy, and Zaid Harchaoui.
Iterative lin-
earized control: stable algorithms and complexity guarantees. In International Conference on
Machine Learning, 2019."
REFERENCES,0.948905109489051,"D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning Internal Representations by Error"
REFERENCES,0.9525547445255474,"Propagation. MIT Press, Cambridge, MA, USA, 1986."
REFERENCES,0.9562043795620438,"Ilya Sutskever, James Martens, and Geoffrey E Hinton. Generating text with recurrent neural net-"
REFERENCES,0.9598540145985401,"works. In Proceedings of the 28th International Conference on Machine Learning, 2011."
REFERENCES,0.9635036496350365,"Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initial-"
REFERENCES,0.9671532846715328,"ization and momentum in deep learning. In Proceedings of the 30th International conference on
machine learning, 2013."
REFERENCES,0.9708029197080292,"Gavin Taylor, Ryan Burmeister, Zheng Xu, Bharat Singh, Ankit Patel, and Tom Goldstein. Training"
REFERENCES,0.9744525547445255,"neural networks without gradients: A scalable ADMM approach. In Proceedings of the 33rd
International Conference on Machine Learning, 2016."
REFERENCES,0.9781021897810219,Paul Werbos. The Roots of Backpropagation: From Ordered Derivatives to Neural Networks and
REFERENCES,0.9817518248175182,"Political Forecasting. Wiley-Interscience, 1994."
REFERENCES,0.9854014598540146,Under review as a conference paper at ICLR 2022
REFERENCES,0.9890510948905109,"Sam Wiseman, Sumit Chopra, Marc-Aurelio Ranzato, Arthur Szlam, Ruoyu Sun, Soumith Chin-"
REFERENCES,0.9927007299270073,"tala, and Nicolas Vasilache. Training language models using target-propagation. arXiv preprint
arXiv:1702.04770, 2017."
REFERENCES,0.9963503649635036,"Han Xiao, Kashif Rasul, and Roland Vollgraf.
Fashion-mnist:
a novel image dataset
for benchmarking machine learning algorithms, 2017.
URL https://github.com/
zalandoresearch/fashion-mnist."
