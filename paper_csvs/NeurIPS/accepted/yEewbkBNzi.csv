Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0011876484560570072,"In this paper, we provide a rigorous proof of convergence of the Adaptive Moment
Estimate (Adam) algorithm for a wide class of optimization objectives. Despite the
popularity and efficiency of the Adam algorithm in training deep neural networks,
its theoretical properties are not yet fully understood, and existing convergence
proofs require unrealistically strong assumptions, such as globally bounded gra-
dients, to show the convergence to stationary points. In this paper, we show that
Adam provably converges to ϵ-stationary points with O(ϵ−4) gradient complexity
under far more realistic conditions. The key to our analysis is a new proof of bound-
edness of gradients along the optimization trajectory of Adam, under a generalized
smoothness assumption according to which the local smoothness (i.e., Hessian
norm when it exists) is bounded by a sub-quadratic function of the gradient norm.
Moreover, we propose a variance-reduced version of Adam with an accelerated
gradient complexity of O(ϵ−3)."
INTRODUCTION,0.0023752969121140144,"1
Introduction"
INTRODUCTION,0.0035629453681710215,"In this paper, we study the non-convex unconstrained stochastic optimization problem"
INTRODUCTION,0.004750593824228029,"min
x {f(x) = Eξ [f(x, ξ)]} .
(1)"
INTRODUCTION,0.0059382422802850355,"The Adaptive Moment Estimation (Adam) algorithm [23] has become one of the most popular
optimizers for solving (1) when f is the loss for training deep neural networks. Owing to its efficiency
and robustness to hyper-parameters, it is widely applied or even sometimes the default choice in many
machine learning application domains such as natural language processing [44; 4; 13], generative
adversarial networks [35; 21; 55], computer vision [14], and reinforcement learning [28; 33; 40]. It
is also well known that Adam significantly outperforms stochastic gradient descent (SGD) for certain
models like transformer [50; 24; 1]."
INTRODUCTION,0.007125890736342043,"Despite its success in practice, theoretical analyses of Adam are still limited. The original proof
of convergence in [23] was later shown by [37] to contain gaps. The authors in [37] also showed
that for a range of momentum parameters chosen independently with the problem instance, Adam
does not necessarily converge even for convex objectives. However, in deep learning practice, the
hyper-parameters are in fact problem-dependent as they are usually tuned after given the problem and
weight initialization. Recently, there have been many works proving the convergence of Adam for
non-convex functions with various assumptions and problem-dependent hyper-parameter choices.
However, these results leave significant room for improvement. For example, [12; 19] prove the
convergence to stationary points assuming the gradients are bounded by a constant, either explicitly
or implicitly. On the other hand, [51; 45] consider weak assumptions, but their convergence results
are still limited. See Section 2 for more detailed discussions of related works."
INTRODUCTION,0.00831353919239905,"To address the above-mentioned gap between theory and practice, we provide a new convergence
analysis of Adam without assuming bounded gradients, or equivalently, Lipschitzness of the objective
function. In addition, we also relax the standard global smoothness assumption, i.e., the Lipschitzness
of the gradient function, as it is far from being satisfied in deep neural network training. Instead, we"
INTRODUCTION,0.009501187648456057,"consider a more general, relaxed, and non-uniform smoothness condition according to which the
local smoothness (i.e., Hessian norm when it exists) around x is bounded by a sub-quadratic function
of the gradient norm ∥∇f(x)∥(see Definition 3.2 and Assumption 2 for the details). This generalizes
the (L0, L1) smoothness condition proposed by [49] based on language model experiments. Even
though our assumptions are much weaker and more realistic, we can still obtain the same O(ϵ−4)
gradient complexity for convergence to an ϵ-stationary point."
INTRODUCTION,0.010688836104513063,"The key to our analysis is a new technique to obtain a high probability, constant upper bound on the
gradients along the optimization trajectory of Adam, without assuming Lipschitzness of the objective
function. In other words, it essentially turns the bounded gradient assumption into a result that can be
directly proven. Bounded gradients imply bounded stepsize at each step, with which the analysis of
Adam essentially reduces to the simpler analysis of AdaBound [31]. Furthermore, once the gradient
boundedness is achieved, the analysis under the generalized non-uniform smoothness assumption is
not much harder than that under the standard smoothness condition. We will introduce the technique
in more details in Section 5. We note that the idea of bounding gradient norm along the trajectory of
the optimization algorithm can be use in other problems as well. For more details, we refer the reader
to our concurrent work [26] in which we present a set of new techiniques and methods for bounding
gradient norm for other optimization algorithms under a generalized smoothness condition."
INTRODUCTION,0.011876484560570071,"Another contribution of this paper is to show that the gradient complexity of Adam can be further
improved with variance reduction methods. To this end, we propose a variance-reduced version of
Adam by modifying its momentum update rule, inspired by the idea of the STORM algorithm [9].
Under additional generalized smoothness assumption of the component function f(·, ξ) for each ξ,
we show that this provably accelerates the convergence with a gradient complexity of O(ϵ−3). This
rate improves upon the existing result of [47] where the authors obtain an asymptotic convergence of
their approach to variance reduction for Adam in the non-convex setting, under the bounded gradient
assumption."
CONTRIBUTIONS,0.013064133016627079,"1.1
Contributions"
CONTRIBUTIONS,0.014251781472684086,"In light of the above background, we summarize our main contributions as follows."
CONTRIBUTIONS,0.015439429928741092,"• We develop a new analysis to show that Adam converges to stationary points under relaxed
assumptions. In particular, we do not assume bounded gradients or Lipschitzness of the
objective function. Furthermore, we also consider a generalized non-uniform smoothness
condition where the local smoothness or Hessian norm is bounded by a sub-quadratic
function of the gradient norm. Under these more realistic assumptions, we obtain a dimension
free gradient complexity of O(ϵ−4) if the gradient noise is centered and bounded.
• We generalize our analysis to the setting where the gradient noise is centered and has
sub-Gaussian norm, and show the convergence of Adam with a gradient complexity of
O(ϵ−4 log3.25(1/ϵ)).
• We propose a variance-reduced version of Adam (VRAdam) with provable convergence
guarantees. In particular, we obtain the accelerated O(ϵ−3) gradient complexity."
RELATED WORK,0.0166270783847981,"2
Related work"
RELATED WORK,0.017814726840855107,"In this section, we discuss the relevant literature related to convergence of Adam and the generalized
smoothness condition, and defer additional related work on variants of Adam and variance reduction
methods to Appendix A."
RELATED WORK,0.019002375296912115,"Convergence of Adam. Adam was first proposed by Kingma and Ba [23] with a theoretical
convergence guarantee for convex functions. However, Reddi et al. [37] found a gap in the proof of
this convergence analysis, and also constructed counter-examples for a range of hyper-parameters
on which Adam does not converge. That being said, the counter-examples depend on the hyper-
parameters of Adam, i.e., they are constructed after picking the hyper-parameters. Therefore, it
does not rule out the possibility of obtaining convergence guarantees for problem-dependent hyper-
parameters, as also pointed out by [42; 51]."
RELATED WORK,0.020190023752969122,"Many recent works have developed convergence analyses of Adam with various assumptions and
hyper-parameter choices. Zhou et al. [54] show Adam with certain hyper-parameters can work on
the counter-examples of [37]. De et al. [10] prove convergence for general non-convex functions"
RELATED WORK,0.021377672209026127,"assuming gradients are bounded and the signs of stochastic gradients are the same along the trajectory.
The analysis in [12] also relies on the bounded gradient assumption. Guo et al. [19] assume the
adaptive stepsize is upper and lower bounded by two constants, which is not necessarily satisfied
unless assuming bounded gradients or considering the AdaBound variant [31]. [51; 45] consider
very weak assumptions. However, they show either 1) “convergence” only to some neighborhood
of stationary points with a constant radius, unless assuming the strong growth condition; or 2)
convergence to stationary points but with a slower rate."
RELATED WORK,0.022565320665083134,"Generalized smoothness condition. Generalizing the standard smoothness condition in a variety of
settings has been a focus of many recent papers. Recently, [49] proposed a generalized smoothness
condition called (L0, L1) smoothness, which assumes the local smoothness or Hessian norm is
bounded by an affine function of the gradient norm. The assumption was well-validated by extensive
experiments conducted on language models. Various analyses of different algorithms under this
condition were later developed [48; 34; 52; 17; 38; 8]. One recent closely-related work is [45] which
studies converges of Adam under the (L0, L1) smoothness condition. However, their results are still
limited, as we have mentioned above. In this paper, we consider an even more general smoothness
condition where the local smoothness is bounded by a sub-quadratic function of the gradient norm,
and prove the convergence of Adam under this condition. In our concurrent work [26], we further
analyze various other algorithms in both convex and non-convex settings under similar generalized
smoothness conditions following the same key idea of bounding gradients along the trajectory."
PRELIMINARIES,0.023752969121140142,"3
Preliminaries"
PRELIMINARIES,0.02494061757719715,"Notation. Let ∥·∥denote the Euclidean norm of a vector or spectral norm of a matrix. For any given
vector x, we use (x)i to denote its i-th coordinate and x2, √x, |x| to denote its coordinate-wise
square, square root, and absolute value respectively. For any two vectors x and y, we use x ⊙y
and x/y to denote their coordinate-wise product and quotient respectively. We also write x ⪯y
or x ⪰y to denote the coordinate-wise inequality between x and y, which means (x)i ≤(y)i or
(x)i ≥(y)i for each coordinate index i. For two symmetric real matrices A and B, we say A ⪯B or
A ⪰B if B −A or A −B is positive semi-definite (PSD). Given two real numbers a, b ∈R, we
denote a ∧b := min{a, b} for simplicity. Finally, we use O(·), Θ(·), and Ω(·) for the standard big-O,
big-Theta, and big-Omega notation."
DESCRIPTION OF THE ADAM ALGORITHM,0.026128266033254157,"3.1
Description of the Adam algorithm"
DESCRIPTION OF THE ADAM ALGORITHM,0.027315914489311165,Algorithm 1 ADAM
DESCRIPTION OF THE ADAM ALGORITHM,0.028503562945368172,"1: Input: β, βsq, η, λ, T, xinit
2: Initialize m0 = v0 = 0 and x1 = xinit
3: for t = 1, · · · , T do
4:
Draw a new sample ξt and perform the following updates
5:
mt = (1 −β)mt−1 + β∇f(xt, ξt)
6:
vt = (1 −βsq)vt−1 + βsq(∇f(xt, ξt))2"
DESCRIPTION OF THE ADAM ALGORITHM,0.029691211401425176,"7:
ˆmt =
mt
1−(1−β)t
8:
ˆvt =
vt
1−(1−βsq)t
9:
xt+1 = xt −
η
√ˆvt+λ ⊙ˆmt
10: end for"
DESCRIPTION OF THE ADAM ALGORITHM,0.030878859857482184,"The formal definition of Adam proposed in [23] is shown in Algorithm 1, where Lines 5–9 describe
the update rule of iterates {xt}1≤t≤T . Lines 5–6 are the updates for the first and second order
momentum, mt and vt, respectively. In Lines 7–8, they are re-scaled to ˆmt and ˆvt in order to correct
the initialization bias due to setting m0 = v0 = 0. Then the iterate is updated by xt+1 = xt−ht⊙ˆmt
where ht = η/(√ˆvt + λ) is the adaptive stepsize vector for some parameters η and λ."
ASSUMPTIONS,0.032066508313539195,"3.2
Assumptions"
ASSUMPTIONS,0.0332541567695962,"In what follows below, we will state our main assumptions for analysis of Adam."
FUNCTION CLASS,0.0344418052256532,"3.2.1
Function class"
FUNCTION CLASS,0.035629453681710214,"We start with a standard assumption in optimization on the objective function f whose domain lies in
a Euclidean space with dimension d."
FUNCTION CLASS,0.03681710213776722,"Assumption 1. The objective function f is differentiable and closed within its open domain
dom(f) ⊆Rd and is bounded from below, i.e., f ∗:= infx f(x) > −∞.
Remark 3.1. A function f is said to be closed if its sub-level set {x ∈dom(f) | f(x) ≤a} is closed
for each a ∈R. In addition, a continuous function f over an open domain is closed if and only f(x)
tends to infinity whenever x approaches to the boundary of dom(f), which is an important condition
to ensure the iterates of Adam with a small enough stepsize η stay within the domain with high
probability. Note that this condition is mild since any continuous function defined over the entire
space Rd is closed."
FUNCTION CLASS,0.03800475059382423,"Besides Assumption 1, the only additional assumption we make regarding f is that its local smooth-
ness is bounded by a sub-quadratic function of the gradient norm. More formally, we consider the
following (ρ, L0, Lρ) smoothness condition with 0 ≤ρ < 2.
Definition 3.2. A differentiable real-valued function f is (ρ, L0, Lρ) smooth for some constants
ρ, L0, Lρ ≥0 if the following inequality holds almost everywhere in dom(f)
∇2f(x)
 ≤L0 + Lρ ∥∇f(x)∥ρ ."
FUNCTION CLASS,0.039192399049881234,"Remark 3.3. When ρ = 1 , Definition 3.2 reduces to the (L0, L1) smoothness condition in [49].
When ρ = 0 or Lρ = 0, it reduces to the standard smoothness condition.
Assumption 2. The objective function f is (ρ, L0, Lρ) smooth with 0 ≤ρ < 2."
FUNCTION CLASS,0.040380047505938245,"The standard smooth function class is very restrictive as it only contains functions that are upper
and lower bounded by quadratic functions. The (L0, L1) smooth function class is more general
since it also contains, e.g., univariate polynomials and exponential functions. Assumption 2 is even
more general and contains univariate rational functions, double exponential functions, etc. See
Appendix D.1 for the formal propositions and proofs. We also refer the reader to our concurrent
work [26] for more detailed discussions of examples of (ρ, L0, Lρ) smooth functions for different ρs."
FUNCTION CLASS,0.04156769596199525,"It turns out that bounded Hessian norm at a point x implies local Lipschitzness of the gradient in the
neighborhood around x. In particular, we have the following lemma.
Lemma 3.4. Under Assumptions 1 and 2, for any a > 0 and two points x ∈dom(f), y ∈Rd such
that ∥y −x∥≤
a
L0+Lρ(∥∇f(x)∥+a)ρ , we have y ∈dom(f) and"
FUNCTION CLASS,0.04275534441805225,∥∇f(y) −∇f(x)∥≤(L0 + Lρ(∥∇f(x)∥+ a)ρ) · ∥y −x∥.
FUNCTION CLASS,0.043942992874109264,"Remark 3.5. Lemma 3.4 can be actually used as the definition of (ρ, L0, Lρ) smooth functions in
place of Assumption 2. Besides the local gradient Lipschitz condition, it also suggests that, as long
as the update at each step is small enough, the iterates will not go outside of the domain."
FUNCTION CLASS,0.04513064133016627,"For the special case of ρ = 1, choosing a = max{∥∇f(x)∥, L0/L1}, one can verify that the required
locality size in Lemma 3.4 satisfies
a
L0+L1(∥∇f(x)∥+a) ≥
1
3L1 . In this case, Lemma 3.4 states that
∥x −y∥≤1/(3L1) implies ∥∇f(y) −∇f(x)∥≤2(L0 + L1 ∥∇f(x)∥) ∥y −x∥. Therefore, it
reduces to the local gradient Lipschitz condition for (L0, L1) smooth functions in [49; 48] up to
numerical constant factors. For ρ ̸= 1, the proof is more involved because Grönwall’s inequality used
in [49; 48] no longer applies. Therefore we defer the detailed proof of Lemma 3.4 to Appendix D.2."
STOCHASTIC GRADIENT,0.04631828978622328,"3.2.2
Stochastic gradient"
STOCHASTIC GRADIENT,0.047505938242280284,"We consider one of the following two assumptions on the stochastic gradient ∇f(xt, ξt) in our
analysis of Adam.
Assumption 3. The gradient noise is centered and almost surely bounded. In particular, for some
σ ≥0 and all t ≥1,"
STOCHASTIC GRADIENT,0.048693586698337295,"Et−1[∇f(xt, ξt)] = ∇f(xt),
∥∇f(xt, ξt) −∇f(xt)∥≤σ, a.s.,"
STOCHASTIC GRADIENT,0.0498812351543943,"where Et−1[ · ] := E[ · |ξ1, . . . , ξt−1] is the conditional expectation given ξ1, . . . , ξt−1.
Assumption 4. The gradient noise is centered with sub-Gaussian norm. In particular, for some
R ≥0 and all t ≥1,"
STOCHASTIC GRADIENT,0.0510688836104513,"Et−1[∇f(xt, ξt)] = ∇f(xt),
Pt−1 (∥∇f(xt, ξt) −∇f(xt)∥≥s) ≤2e−s2"
STOCHASTIC GRADIENT,0.052256532066508314,"2R2 , ∀s ∈R,"
STOCHASTIC GRADIENT,0.05344418052256532,"where Et−1[ · ] := E[ · |ξ1, . . . , ξt−1] and Pt−1[ · ] := P[ · |ξ1, . . . , ξt−1] are the conditional expecta-
tion and probability given ξ1, . . . , ξt−1."
STOCHASTIC GRADIENT,0.05463182897862233,"Assumption 4 is strictly weaker than Assumption 3 since an almost surely bounded random variable
clearly has sub-Gaussian norm, but it results in a slightly worse convergece rate up to poly-log factors
(see Theorems 4.1 and 4.2). Both of them are stronger than the most standard bounded variance
assumption E[∥∇f(xt, ξt) −∇f(xt)∥2] ≤σ2 for some σ ≥0, although Assumption 3 is actually a
common assumption in existing analyses under the (L0, L1) smoothness condition (see e.g. [49; 48]).
The extension to the bounded variance assumption is challenging and a very interesting future work
as it is also the assumption considered in the lower bound [3]. We suspect that such an extension
would be straightforward if we consider a mini-batch version of Algorithm 1 with a batch size of
S = Ω(ϵ−2), since this results in a very small variance of O(ϵ2) and thus essentially reduces the
analysis to the deterministic setting. However, for practical Adam with an O(1) batch size, the
extension is challenging and we leave it as a future work."
RESULTS,0.055819477434679333,"4
Results"
RESULTS,0.057007125890736345,"In the section, we provide our convergence results for Adam under Assumptions 1, 2, and 3 or 4. To
keep the statements of the theorems concise, we first define several problem-dependent constants.
First, we let ∆1 := f(x1) −f ∗< ∞be the initial sub-optimality gap. Next, given a large enough
constant G > 0, we define"
RESULTS,0.05819477434679335,"r := min
n
1
5LρGρ−1 ,
1
5(Lρ−1
0
Lρ)1/ρ
o
,
L := 3L0 + 4LρGρ,
(2)"
RESULTS,0.05938242280285035,"where L can be viewed as the effective smoothness constant along the trajectory if one can show
∥∇f(xt)∥≤G and ∥xt+1 −xt∥≤r at each step (see Section 5 for more detailed discussions). We
will also use c1, c2 to denote some small enough numerical constants and C1, C2 to denote some
large enough ones. The formal convergence results under Assumptions 1, 2, and 3 are presented in
the following theorem, whose proof is deferred in Appendix E."
RESULTS,0.060570071258907364,"Theorem 4.1. Suppose Assumptions 1, 2, and 3 hold. Denote ι := log(1/δ) for any 0 < δ < 1. Let"
RESULTS,0.06175771971496437,"G be a constant satisfying G ≥max
n
2λ, 2σ, √C1∆1L0, (C1∆1Lρ)
1
2−ρ
o
. Choose"
RESULTS,0.06294536817102138,"0 ≤βsq ≤1,
β ≤min

1, c1λϵ2 σ2G√ι"
RESULTS,0.06413301662707839,"
,
η ≤c2 min
rλ"
RESULTS,0.06532066508313539,"G ,
σλβ
LG√ι,
λ3/2β L
√ G 
."
RESULTS,0.0665083135391924,"Let T = max
n
1
β2 ,
C2∆1G"
RESULTS,0.06769596199524941,"ηϵ2
o
. Then with probability at least 1−δ, we have ∥∇f(xt)∥≤G for every"
RESULTS,0.0688836104513064,"1 ≤t ≤T, and 1"
RESULTS,0.07007125890736342,"T
PT
t=1 ∥∇f(xt)∥2 ≤ϵ2."
RESULTS,0.07125890736342043,"Note that G, the upper bound of gradients along the trajectory, is a constant that depends on
λ, σ, L0, Lρ, and the initial sub-optimality gap ∆1, but not on ϵ. There is no requirement on the
second order momentum parameter βsq, although many existing works like [12; 51; 45] need certain
restrictions on it. We choose very small β and η, both of which are O(ϵ2). Therefore, from the choice
of T, it is clear that we obtain a gradient complexity of O(ϵ−4), where we only consider the leading
term. We are not clear whether the dependence on ϵ is optimal or not, as the Ω(ϵ−4) lower bound in
[3] assumes the weaker bounded variance assumption than our Assumpion 3. However, it matches
the state-of-the-art complexity among existing analyses of Adam."
RESULTS,0.07244655581947744,"One limitation of the dependence of our complexity on λ is O(λ−2), which might be large since
λ is usually small in practice, e.g., the default choice is λ = 10−8 in the PyTorch implementation.
There are some existing analyses on Adam [12; 51; 45] whose rates do not depend explicitly on λ or
only depend on log(1/λ). However, all of them depend on poly(d), whereas our rate is dimension
free. The dimension d is also very large, especially when training transformers, for which Adam is
widely used. We believe that independence on d is better than that on λ, because d is fixed given the
architecture of the neural network but λ is a hyper-parameter which we have the freedom to tune. In
fact, based on our preliminary experimental results on CIFAR-10 shown in Figure 1, the performance
of Adam is not very sensitive to the choice of λ. Although the default choice of λ is 10−8, increasing
it up to 0.01 only makes minor differences."
RESULTS,0.07363420427553444,"As discussed in Section 3.2.2, we can generalize the bounded gradient noise condition in Assumption 3
to the weaker sub-Gaussian noise condition in Assumption 4. The following theorem formally shows
the convergence result under Assumptions 1, 2, and 4, whose proof is deferred in Appendix E.6."
RESULTS,0.07482185273159145,"0
100
200
300
400 Epoch 0 20 40 60 80"
RESULTS,0.07600950118764846,Test Error (%)
RESULTS,0.07719714964370546,λ=1e-8
RESULTS,0.07838479809976247,λ=1e-4
RESULTS,0.07957244655581948,λ=1e-3
RESULTS,0.08076009501187649,λ=0.01 λ=0.1 λ=1
RESULTS,0.08194774346793349,(a) CNN
RESULTS,0.0831353919239905,"0
100
200
300
400 Epoch 0 20 40 60 80"
RESULTS,0.08432304038004751,Test Error (%)
RESULTS,0.0855106888361045,λ=1e-8
RESULTS,0.08669833729216152,λ=1e-4
RESULTS,0.08788598574821853,λ=1e-3
RESULTS,0.08907363420427554,λ=0.01 λ=0.1 λ=1
RESULTS,0.09026128266033254,(b) ResNet-Small
RESULTS,0.09144893111638955,"0
100
200
300
400 Epoch 0 20 40 60 80"
RESULTS,0.09263657957244656,Test Error (%)
RESULTS,0.09382422802850356,λ=1e-8
RESULTS,0.09501187648456057,λ=1e-4
RESULTS,0.09619952494061758,λ=1e-3
RESULTS,0.09738717339667459,λ=0.01 λ=0.1 λ=1
RESULTS,0.09857482185273159,(c) ResNet110
RESULTS,0.0997624703087886,"Figure 1: Test errors of different models trained on CIFAR-10 using the Adam optimizer with
β = 0.9, βsq = 0.999, η = 0.001 and different λs. From left to right: (a) a shallow CNN with 6
layers; (b) ResNet-Small with 20 layers; and (c) ResNet110 with 110 layers."
RESULTS,0.10095011876484561,"Theorem 4.2. Suppose Assumptions 1, 2, and 4 hold.
Denote ι
:=
log(2/δ) and
σ
:=
R
p"
RESULTS,0.1021377672209026,"2 log(4T/δ) for any 0
<
δ
<
1.
Let G be a constant satisfying G
≥"
RESULTS,0.10332541567695962,"max
n
2λ, 2σ, √C1∆1L0, (C1∆1Lρ)
1
2−ρ
o
. Choose"
RESULTS,0.10451306413301663,"0 ≤βsq ≤1,
β ≤min

1, c1λϵ2 σ2G√ι"
RESULTS,0.10570071258907364,"
,
η ≤c2 min
rλ"
RESULTS,0.10688836104513064,"G ,
σλβ
LG√ι,
λ3/2β L
√ G 
."
RESULTS,0.10807600950118765,"Let T = max
n
1
β2 ,
C2∆1G"
RESULTS,0.10926365795724466,"ηϵ2
o
. Then with probability at least 1−δ, we have ∥∇f(xt)∥≤G for every"
RESULTS,0.11045130641330166,"1 ≤t ≤T, and 1"
RESULTS,0.11163895486935867,"T
PT
t=1 ∥∇f(xt)∥2 ≤ϵ2."
RESULTS,0.11282660332541568,"Note that the main difference of Theorem 4.2 from Theorem 4.1 is that σ is now O(√log T) instead
of a constant. With some standard calculations, one can show that the gradient complexity in
Theorem 4.2 is bounded by O(ϵ−4 logp(1/ϵ)), where p = max

3, 9+2ρ"
RESULTS,0.11401425178147269,"4
	
< 3.25."
ANALYSIS,0.11520190023752969,"5
Analysis"
BOUNDING THE GRADIENTS ALONG THE OPTIMIZATION TRAJECTORY,0.1163895486935867,"5.1
Bounding the gradients along the optimization trajectory"
BOUNDING THE GRADIENTS ALONG THE OPTIMIZATION TRAJECTORY,0.11757719714964371,"We want to bound the gradients along the optimization trajectory mainly for two reasons. First, as
discussed in Section 2, many existing analyses of Adam rely on the assumption of bounded gradients,
because unbounded gradient norm leads to unbounded second order momentum ˆvt which implies
very small stepsize, and slow convergence. On the other hand, once the gradients are bounded, it is
straightforward to control ˆvt as well as the stepsize, and therefore the analysis essentially reduces to
the easier one for AdaBound. Second, informally speaking1, under Assumption 2, bounded gradients
also imply bounded Hessians, which essentially reduces the (ρ, L0, Lρ) smoothness to the standard
smoothness. See Section 5.2 for more formal discussions."
BOUNDING THE GRADIENTS ALONG THE OPTIMIZATION TRAJECTORY,0.1187648456057007,"In this paper, instead of imposing the strong assumption of globally bounded gradients, we develop a
new analysis to show that with high probability, the gradients are always bounded along the trajectory
of Adam until convergence. The essential idea can be informally illustrated by the following “circular""
reasoning that we will make precise later. On the one hand, if ∥∇f(xt)∥≤G for every t ≥1, it is
not hard to show the gradient converges to zero based on our discussions above. On the other hand,
we know that a converging sequence must be upper bounded. Therefore there exists some G′ such
that ∥∇f(xt)∥≤G′ for every t ≥1. In other words, the bounded gradient condition implies the
convergence result and the convergence result also implies the boundedness condition, forming a
circular argument."
BOUNDING THE GRADIENTS ALONG THE OPTIMIZATION TRAJECTORY,0.11995249406175772,"This circular argument is of course flawed. However, we can break the circularity of reasoning and
rigorously prove both the bounded gradient condition and the convergence result using a contradiction"
THE STATEMENT IS INFORMAL BECAUSE HERE WE CAN ONLY SHOW BOUNDED GRADIENTS AND HESSIANS AT THE ITERATE,0.12114014251781473,"1The statement is informal because here we can only show bounded gradients and Hessians at the iterate
points, which only implies local smoothness near the neighborhood of each iterate point (see Section 5.2).
However, the standard smoothness condition is a stronger global condition which assumes bounded Hessian at
every point within a convex set."
THE STATEMENT IS INFORMAL BECAUSE HERE WE CAN ONLY SHOW BOUNDED GRADIENTS AND HESSIANS AT THE ITERATE,0.12232779097387174,"argument. Before introducing the contradiction argument, we first need to provide the following
useful lemma, which is the reverse direction of a generalized Polyak-Lojasiewicz (PL) inequality,
whose proof is deferred in Appendix D.3."
THE STATEMENT IS INFORMAL BECAUSE HERE WE CAN ONLY SHOW BOUNDED GRADIENTS AND HESSIANS AT THE ITERATE,0.12351543942992874,"Lemma 5.1. Under Assumptions 1 and 2, we have ∥∇f(x)∥2 ≤3(3L0+4Lρ ∥∇f(x)∥ρ)(f(x)−f ∗)."
THE STATEMENT IS INFORMAL BECAUSE HERE WE CAN ONLY SHOW BOUNDED GRADIENTS AND HESSIANS AT THE ITERATE,0.12470308788598575,"Define the function ζ(u) :=
u2
3(3L0+4Lρuρ) over u ≥0. It is easy to verify that if ρ < 2, ζ is increasing
and its range is [0, ∞). Therefore, ζ is invertible and ζ−1 is also increasing. Then, for any constant
G > 0, denoting F = ζ(G), Lemma 5.1 suggests that if f(x) −f ∗≤F, we have"
THE STATEMENT IS INFORMAL BECAUSE HERE WE CAN ONLY SHOW BOUNDED GRADIENTS AND HESSIANS AT THE ITERATE,0.12589073634204276,∥∇f(x)∥≤ζ−1(f(x) −f ∗) ≤ζ−1(F) = G.
THE STATEMENT IS INFORMAL BECAUSE HERE WE CAN ONLY SHOW BOUNDED GRADIENTS AND HESSIANS AT THE ITERATE,0.12707838479809977,"In other words, if ρ < 2, the gradient is bounded within any sub-level set, even though the sub-level
set could be unbounded. Then, let τ be the first time the sub-optimality gap is strictly greater than F,
truncated at T + 1, or formally,"
THE STATEMENT IS INFORMAL BECAUSE HERE WE CAN ONLY SHOW BOUNDED GRADIENTS AND HESSIANS AT THE ITERATE,0.12826603325415678,"τ := min{t | f(xt) −f ∗> F} ∧(T + 1).
(3)"
THE STATEMENT IS INFORMAL BECAUSE HERE WE CAN ONLY SHOW BOUNDED GRADIENTS AND HESSIANS AT THE ITERATE,0.12945368171021376,"Then at least when t < τ, we have f(xt) −f ∗≤F and thus ∥∇f(xt)∥≤G. Based on our
discussions above, it is not hard to analyze the updates before time τ, and one can contruct some
Lyapunov function to obtain an upper bound on f(xτ) −f ∗. On the other hand, if τ ≤T, we
immediately obtain a lower bound on f(xτ), that is f(xτ) −f ∗> F, by the definition of τ in (3). If
the lower bound is greater than the upper bound, it leads to a contradiction, which shows τ = T + 1,
i.e., the sub-optimality gap and the gradient norm are always bounded by F and G respectively before
the algorithm terminates. We will illustrate the technique in more details in the simple deterministic
setting in Section 5.3, but first, in Section 5.2, we introduce several prerequisite lemmas on the
(ρ, L0, Lρ) smoothness."
LOCAL SMOOTHNESS,0.13064133016627077,"5.2
Local smoothness"
LOCAL SMOOTHNESS,0.13182897862232779,"In Section 5.1, we informally mentioned that (ρ, L0, Lρ) smoothness essentially reduces to the
standard smoothness if the gradient is bounded. In this section, we will make the statement more
precise. First, note that Lemma 3.4 implies the following useful corollary.
Corollary 5.2. Under Assumptions 1 and 2, for any G > 0 and two points x ∈dom(f), y ∈Rd"
LOCAL SMOOTHNESS,0.1330166270783848,"such that ∥∇f(x)∥≤G and ∥y −x∥≤r := min
n
1
5LρGρ−1 ,
1
5(Lρ−1
0
Lρ)1/ρ
o
, denoting L :="
LOCAL SMOOTHNESS,0.1342042755344418,"3L0 + 4LρGρ, we have y ∈dom(f) and"
LOCAL SMOOTHNESS,0.13539192399049882,"∥∇f(y) −∇f(x)∥≤L ∥y −x∥,
f(y) ≤f(x) +

∇f(x), y −x

+ L"
LOCAL SMOOTHNESS,0.13657957244655583,2 ∥y −x∥2 .
LOCAL SMOOTHNESS,0.1377672209026128,"The proof of Corollary 5.2 is deferred in Appendix D.4. Although the inequalities in Corollary 5.2
look very similar to the standard global smoothness condition with constant L, it is still a local
condition as it requires ∥x −y∥≤r. Fortunately, at least before τ, such a requirement is easy to
satisfy for small enough η, according to the following lemma whose proof is deferred in Appendix E.5.
Lemma 5.3. Under Assumption 3, if t < τ and choosing G ≥σ, we have ∥xt+1 −xt∥≤ηD where
D := 2G/λ."
LOCAL SMOOTHNESS,0.13895486935866982,"Then as long as η ≤r/D, we have ∥xt+1 −xt∥≤r which satisfies the requirement in Corollary 5.2.
Then we can apply the inequalities in it in the same way as the standard smoothness condition. In other
words, most classical inequalities derived for standard smooth functions also apply to (ρ, L0, Lρ)
smooth functions."
LOCAL SMOOTHNESS,0.14014251781472684,"5.3
Warm-up: analysis in the deterministic setting"
LOCAL SMOOTHNESS,0.14133016627078385,"In this section, we consider the simpler deterministic setting where the stochastic gradient ∇f(xt, ξt)
in Algorithm 1 or (18) is replaced with the exact gradient ∇f(xt). As discussed in Section 5.1, the
key in our contradiction argument is to obtain both upper and lower bounds on f(xτ) −f ∗. In the
following derivations, we focus on illustrating the main idea of our analysis technique and ignore
minor proof details. In addition, all of them are under Assumptions 1, 2, and 3."
LOCAL SMOOTHNESS,0.14251781472684086,"In order to obtain the upper bound, we need the following two lemmas. First, denoting ϵt :=
ˆmt −∇f(xt), we can obtain the following informal descent lemma for deterministic Adam."
LOCAL SMOOTHNESS,0.14370546318289787,"Lemma 5.4 (Descent lemma, informal). For any t < τ, choosing G ≥λ and a small enough η,"
LOCAL SMOOTHNESS,0.14489311163895488,f(xt+1) −f(xt) ⪅−η
LOCAL SMOOTHNESS,0.14608076009501186,4G ∥∇f(xt)∥2 + η
LOCAL SMOOTHNESS,0.14726840855106887,"2λ ∥ϵt∥2 ,
(4)"
LOCAL SMOOTHNESS,0.14845605700712589,where “⪅” omits less important terms.
LOCAL SMOOTHNESS,0.1496437054631829,"Compared with the standard descent lemma for gradient descent, there is an additional term of ∥ϵt∥2"
LOCAL SMOOTHNESS,0.1508313539192399,"in Lemma 5.4. In the next lemma, we bound this term recursively."
LOCAL SMOOTHNESS,0.15201900237529692,"Lemma 5.5 (Informal). Choosing β = Θ(ηGρ+1/2), if t < τ, we have"
LOCAL SMOOTHNESS,0.15320665083135393,∥ϵt+1∥2 ≤(1 −β/4) ∥ϵt∥2 + λβ
LOCAL SMOOTHNESS,0.1543942992874109,"16G ∥∇f(xt)∥2 .
(5)"
LOCAL SMOOTHNESS,0.15558194774346792,"The proof sketches of the above two lemmas are deferred in Appendix B. Now we combine them to
get the upper bound on f(xτ) −f ∗. Define the function Φt := f(xt) −f ∗+ 2η"
LOCAL SMOOTHNESS,0.15676959619952494,"λβ ∥ϵt∥2. Note that for
any t < τ, (4)+ 2η"
LOCAL SMOOTHNESS,0.15795724465558195,λβ ×(5) gives
LOCAL SMOOTHNESS,0.15914489311163896,Φt+1 −Φt ≤−η
LOCAL SMOOTHNESS,0.16033254156769597,"8G ∥∇f(xt)∥2 .
(6)"
LOCAL SMOOTHNESS,0.16152019002375298,"The above inequality shows Φt is non-increasing and thus a Lyapunov function. Therefore, we have"
LOCAL SMOOTHNESS,0.16270783847980996,"f(xτ) −f ∗≤Φτ ≤Φ1 = ∆1,"
LOCAL SMOOTHNESS,0.16389548693586697,"where in the last inequality we use Φ1 = f(x1) −f ∗= ∆1 since ϵ1 = ˆm1 −∇f(x1) = 0 in the
deterministic setting."
LOCAL SMOOTHNESS,0.16508313539192399,"As discussed in Section 5.1, if τ ≤T, we have F < f(xτ) −f ∗≤∆1. Note that we are able to
choose a large enough constant G so that F =
G2
3(3L0+4LρGρ) is greater than ∆1, which leads to a
contradiction and shows τ = T + 1. Therefore, (6) holds for all 1 ≤t ≤T. Taking a summation
over 1 ≤t ≤T and re-arranging terms, we get"
T,0.166270783847981,"1
T T
X"
T,0.167458432304038,"t=1
∥∇f(xt)∥2 ≤8G(Φ1 −ΦT +1)"
T,0.16864608076009502,"ηT
≤8G∆1"
T,0.16983372921615203,"ηT
≤ϵ2,"
T,0.171021377672209,if choosing T ≥8G∆1
T,0.17220902612826602,"ηϵ2 , i.e., it shows convergence with a gradient complexity of O(ϵ−2) since both
G and η are constants independent of ϵ in the deterministic setting."
EXTENSION TO THE STOCHASTIC SETTING,0.17339667458432304,"5.4
Extension to the stochastic setting"
EXTENSION TO THE STOCHASTIC SETTING,0.17458432304038005,"In this part, we briefly introduce how to extend the analysis to the more challenging stochastic setting.
It becomes harder to obtain an upper bound on f(xτ) −f ∗because Φt is no longer non-increasing
due to the existence of noise. In addition, τ defined in (3) is now a random variable. Note that all the
derivations, such as Lemmas 5.4 and 5.5, are conditioned on the random event t < τ. Therefore, one
can not simply take a total expectation of them to show E[Φt] is non-increasing."
EXTENSION TO THE STOCHASTIC SETTING,0.17577197149643706,"Fortunately, τ is in fact a stopping time with nice properties. If the noise is almost surely bounded
as in Assumption 3, by a more careful analysis, we can obtain a high probability upper bound on
f(xτ)−f ∗using concentration inequalities. Then we can still obtain a contradiction and convergence
under this high probability event. If the noise has sub-Gaussian norm as in Assumption 4, one can
change the definition of τ to"
EXTENSION TO THE STOCHASTIC SETTING,0.17695961995249407,"τ := min{t | f(xt) −f ∗> F} ∧min{t | ∥∇f(xt) −∇f(xt, ξt)∥> σ} ∧(T + 1)"
EXTENSION TO THE STOCHASTIC SETTING,0.17814726840855108,"for appropriately chosen F and σ. Then at least when t < τ, the noise is bounded by σ. Hence we
can get the same upper bound on f(xτ) −f ∗as if Assumption 3 still holds. However, when t ≤T,
the lower bound f(xτ) −f ∗> F does not necessarily holds, which requires some more careful
analyses. The details of the proofs are involved and we defer them in Appendix E."
VARIANCE-REDUCED ADAM,0.17933491686460806,"6
Variance-reduced Adam"
VARIANCE-REDUCED ADAM,0.18052256532066507,"In this section, we propose a variance-reduced version of Adam (VRAdam). This new algorithm is
depicted in Algorithm 2. Its main difference from the original Adam is that in the momentum update"
VARIANCE-REDUCED ADAM,0.18171021377672208,"rule (Line 6), an additional term of (1 −β) (∇f(xt, ξt) −∇f(xt−1, ξt)) is added, inspired by the
STORM algorithm [9]. This term corrects the bias of mt so that it is an unbiased estimate of ∇f(xt)
in the sense of total expectation, i.e., E[mt] = ∇f(xt). We will also show that it reduces the variance
and accelerates the convergence."
VARIANCE-REDUCED ADAM,0.1828978622327791,"Aside from the adaptive stepsize, one major difference between Algorithm 2 and STORM is that
our hyper-parameters η and β are fixed constants whereas theirs are decreasing as a function of t.
Choosing constant hyper-parameters requires a more accurate estimate at the initialization. That is
why we use a mega-batch S1 to evaluate the gradient at the initial point to initialize m1 and v1 (Lines
2–3). In practice, one can also do a full-batch gradient evaluation at initialization. Note that there is
no initialization bias for the momentum, so we do not re-scale mt and only re-scale vt. We also want
to point out that although the initial mega-batch gradient evaluation makes the algorithm a bit harder
to implement, constant hyper-parameters are usually easier to tune and more common in training
deep neural networks. It should be not hard to extend our analysis to time-decreasing η and β and we
leave it as an interesting future work."
VARIANCE-REDUCED ADAM,0.1840855106888361,Algorithm 2 VARIANCE-REDUCED ADAM (VRADAM)
VARIANCE-REDUCED ADAM,0.18527315914489312,"1: Input: β, βsq, η, λ, T, S1, xinit
2: Draw a batch of samples S1 with size S1 and use them to evaluate the gradient ∇f(xinit, S1).
3: Initialize m1 = ∇f(xinit, S1), v1 = βsqm2
1, and x2 = xinit −
ηm1
|m1|+λ.
4: for t = 2, · · · , T do
5:
Draw a new sample ξt and perform the following updates:
6:
mt = (1 −β)mt−1 + β∇f(xt, ξt)+(1 −β) (∇f(xt, ξt) −∇f(xt−1, ξt))
7:
vt = (1 −βsq)vt−1 + βsq(∇f(xt, ξt))2"
VARIANCE-REDUCED ADAM,0.18646080760095013,"8:
ˆvt =
vt
1−(1−βsq)t
9:
xt+1 = xt −
η
√ˆvt+λ ⊙mt
10: end for"
VARIANCE-REDUCED ADAM,0.1876484560570071,"In addition to Assumption 1, we need to impose the following assumptions which can be viewed as
stronger versions of Assumptions 2 and 3, respectively."
VARIANCE-REDUCED ADAM,0.18883610451306412,"Assumption 5. The objective function f and the component function f(·, ξ) for each fixed ξ are
(ρ, L0, Lρ) smooth with 0 ≤ρ < 2."
VARIANCE-REDUCED ADAM,0.19002375296912113,"Assumption 6. The random variables {ξt}1≤t≤T are sampled i.i.d. from some distribution P such
that for any x ∈dom(f),"
VARIANCE-REDUCED ADAM,0.19121140142517815,"Eξ∼P[∇f(x, ξ)] = ∇f(x),
∥∇f(x, ξ) −∇f(x)∥≤σ, a.s."
VARIANCE-REDUCED ADAM,0.19239904988123516,"Remark 6.1. Assumption 6 is stronger than Assumption 3. Assumption 3 applies only to the iterates
generated by the algorithm, while Assumption 6 is a pointwise assumption over all x ∈dom(f)
and further assumes an i.i.d. nature of the random variables {ξt}1≤t≤T . Also note that, similar
to Adam, it is straightforward to generalize the assumption to noise with sub-Gaussian norm as in
Assumption 4."
ANALYSIS,0.19358669833729217,"6.1
Analysis"
ANALYSIS,0.19477434679334918,"In this part, we briefly discuss challenges in the analysis of VRAdam. The detailed analysis is
deferred in Appendix F. Note that Corollary 5.2 requires bounded update ∥xt+1 −xt∥≤r at each
step. For Adam, it is easy to satisfy for a small enough η according to Lemma 5.3. However, for
VRAdam, obtaining a good enough almost sure bound on the update is challenging even though the
gradient noise is bounded. To bypass this difficulty, we directly impose a bound on ∥∇f(xt) −mt∥
by changing the definition of the stopping time τ, similar to how we deal with the sub-Gaussian noise
condition for Adam. In particular, we define"
ANALYSIS,0.19596199524940616,τ := min{t | ∥∇f(xt)∥> G} ∧min{t | ∥∇f(xt) −mt∥> G} ∧(T + 1).
ANALYSIS,0.19714964370546317,"Then by definition, both ∥∇f(xt)∥and ∥∇f(xt) −mt∥are bounded by G before time τ, which
directly implies bounded update ∥xt+1 −xt∥. Of course, the new definition brings new challenges to
lower bounding f(xτ) −f ∗, which requires more careful analyses specific to the VRAdam algorithm.
Please see Appendix F for the details."
CONVERGENCE GUARANTEES FOR VRADAM,0.19833729216152018,"6.2
Convergence guarantees for VRAdam"
CONVERGENCE GUARANTEES FOR VRADAM,0.1995249406175772,"In the section, we provide our main results for convergence of VRAdam under Assumptions 1, 5, and 6.
We consider the same definitions of problem-dependent constants ∆1, r, L as those in Section 4 to
make the statements of theorems concise. Let c be a small enough numerical constant and C be a
large enough numerical constant. The formal convergence result is shown in the following theorem."
CONVERGENCE GUARANTEES FOR VRADAM,0.2007125890736342,"Theorem 6.2. Suppose Assumptions 1, 5, and 6 hold. For any 0 < δ < 1, let G > 0 be a constant
satisfying G ≥max
n
2λ, 2σ,
p"
CONVERGENCE GUARANTEES FOR VRADAM,0.20190023752969122,"C∆1L0/δ, (C∆1Lρ/δ)
1
2−ρ
o
. Choose 0 ≤βsq ≤1 and β = a2η2,"
CONVERGENCE GUARANTEES FOR VRADAM,0.20308788598574823,"where a = 40L
√"
CONVERGENCE GUARANTEES FOR VRADAM,0.2042755344418052,Gλ−3/2. Choose
CONVERGENCE GUARANTEES FOR VRADAM,0.20546318289786222,"η ≤c · min (
rλ"
CONVERGENCE GUARANTEES FOR VRADAM,0.20665083135391923,"G ,
λ
L,
λ2δ
∆1L2 ,
λ2√"
CONVERGENCE GUARANTEES FOR VRADAM,0.20783847980997625,"δϵ
σGL )"
CONVERGENCE GUARANTEES FOR VRADAM,0.20902612826603326,",
T = 64G∆1"
CONVERGENCE GUARANTEES FOR VRADAM,0.21021377672209027,"ηδϵ2 ,
S1 ≥
1
2β2T ."
CONVERGENCE GUARANTEES FOR VRADAM,0.21140142517814728,"Then with probability at least 1 −δ, we have ∥∇f(xt)∥≤G for every 1 ≤t ≤T, and
1
T
PT
t=1 ∥∇f(xt)∥2 ≤ϵ2."
CONVERGENCE GUARANTEES FOR VRADAM,0.21258907363420426,"Note that the choice of G, the upper bound of gradients along the trajectory of VRAdam, is very
similar to that in Theorem 4.1 for Adam. The only difference is that now it also depends on the failure
probability δ. Similar to Theorem 4.1, there is no requirement on βsq and we choose a very small
β = O(ϵ2). However, the variance reduction technique allows us to take a larger stepsize η = O(ϵ)
(compared with O(ϵ2) for Adam) and obtain an accelerated gradient complexity of O(ϵ−3), where
we only consider the leading term. We are not sure whether it is optimal as the Ω(ϵ−3) lower bound
in [3] assumes the weaker bounded variance condition. However, our result significantly improves
upon [47], which considers a variance-reduced version of Adam by combining Adam and SVRG [22]
and only obtains asymptotic convergence in the non-convex setting. Similar to Adam, our gradient
complexity for VRAdam is dimension free but its dependence on λ is O(λ−2). Another limitation is
that, the dependence on the failure probability δ is polynomial, worse than the poly-log dependence
in Theorem 4.1 for Adam."
CONCLUSION AND FUTURE WORKS,0.21377672209026127,"7
Conclusion and future works"
CONCLUSION AND FUTURE WORKS,0.21496437054631828,"In this paper, we proved the convergence of Adam and its variance-reduced version under less
restrictive assumptions compared to those in the existing literature. We considered a generalized non-
uniform smoothness condition, according to which the Hessian norm is bounded by a sub-quadratic
function of the gradient norm almost everywhere. Instead of assuming the Lipschitzness of the
objective function as in existing analyses of Adam, we use a new contradiction argument to prove that
gradients are bounded by a constant along the optimization trajectory. There are several interesting
future directions that one could pursue following this work."
CONCLUSION AND FUTURE WORKS,0.2161520190023753,"Relaxation of the bounded noise assumption. Our analysis relies on the assumption of bounded
noise or noise with sub-Gaussian norm. However, the existing lower bounds in [3] consider the
weaker bounded variance assumption. Hence, it is not clear whether the O(ϵ−4) complexity we obtain
for Adam is tight in this setting. It will be interesting to see whether one can relax the assumption
to the bounded variance setting. One may gain some insights from recent papers such as [16; 46]
that analyze AdaGrad under weak noise conditions. An alternative way to show the tightness of the
O(ϵ−4) complexity is to prove a lower bound under the bounded noise assumption."
CONCLUSION AND FUTURE WORKS,0.2173396674584323,"Potential applications of our technique. Another interesting future direction is to see if the
techniques developed in this work for bounding gradients (including those in the the concurrent
work [26]) can be generalized to improve the convergence results for other optimization problems and
algorithms. We believe it is possible so long as the function class is well behaved and the algorithm
is efficient enough so that f(xτ) −f ∗can be well bounded for some appropriately defined stopping
time τ."
CONCLUSION AND FUTURE WORKS,0.21852731591448932,"Understanding why Adam is better than SGD. We want to note that our results can not explain
why Adam is better than SGD for training transformers, because [26] shows that non-adaptive SGD
converges with the same O(ϵ−4) gradient complexity under even weaker conditions. It would be
interesting and impactful if one can find a reasonable setting (function class, gradient oracle, etc)
under which Adam or other adaptive methods provably outperform SGD."
CONCLUSION AND FUTURE WORKS,0.21971496437054633,Acknowledgments
CONCLUSION AND FUTURE WORKS,0.2209026128266033,"This work was supported, in part, by the MIT-IBM Watson AI Lab and ONR Grants N00014-20-1-
2394 and N00014-23-1-2299. We also acknowledge support from DOE under grant DE-SC0022199,
and NSF through awards DMS-2031883 and DMS-1953181."
REFERENCES,0.22209026128266032,References
REFERENCES,0.22327790973871733,"[1] Kwangjun Ahn, Xiang Cheng, Minhak Song, Chulhee Yun, Ali Jadbabaie, and Suvrit Sra.
Linear attention is (maybe) all you need (to understand transformer optimization). arXiv
preprint arXiv:2310.01082, 2023."
REFERENCES,0.22446555819477435,"[2] Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization. In
International conference on machine learning, pages 699–707. PMLR, 2016."
REFERENCES,0.22565320665083136,"[3] Yossi Arjevani, Yair Carmon, John C Duchi, Dylan J Foster, Nathan Srebro, and Blake Wood-
worth. Lower bounds for non-convex stochastic optimization. Mathematical Programming, 199
(1-2):165–214, 2023."
REFERENCES,0.22684085510688837,"[4] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,
Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M.
Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,
Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Rad-
ford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. ArXiv,
abs/2005.14165, 2020."
REFERENCES,0.22802850356294538,"[5] Congliang Chen, Li Shen, Fangyu Zou, and Wei Liu. Towards practical adam: Non-convexity,
convergence theory, and mini-batch acceleration. The Journal of Machine Learning Research,
23(1):10411–10457, 2022."
REFERENCES,0.22921615201900236,"[6] Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence of a class of
adam-type algorithms for non-convex optimization. arXiv preprint arXiv:1808.02941, 2018."
REFERENCES,0.23040380047505937,"[7] Ziyi Chen, Yi Zhou, Yingbin Liang, and Zhaosong Lu. Generalized-smooth nonconvex opti-
mization is as efficient as smooth nonconvex optimization. arXiv preprint arXiv:2303.02854,
2023."
REFERENCES,0.23159144893111638,"[8] Michael Crawshaw, Mingrui Liu, Francesco Orabona, Wei Zhang, and Zhenxun Zhuang.
Robustness to unbounded smoothness of generalized signsgd. Advances in Neural Information
Processing Systems, 35:9955–9968, 2022."
REFERENCES,0.2327790973871734,"[9] Ashok Cutkosky and Francesco Orabona. Momentum-based variance reduction in non-convex
sgd. ArXiv, abs/1905.10018, 2019."
REFERENCES,0.2339667458432304,"[10] Soham De, Anirbit Mukherjee, and Enayat Ullah. Convergence guarantees for rmsprop and
adam in non-convex optimization and an empirical comparison to nesterov acceleration. arXiv:
Learning, 2018."
REFERENCES,0.23515439429928742,"[11] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradi-
ent method with support for non-strongly convex composite objectives. Advances in neural
information processing systems, 27, 2014."
REFERENCES,0.23634204275534443,"[12] Alexandre D’efossez, Léon Bottou, Francis R. Bach, and Nicolas Usunier. A simple convergence
proof of adam and adagrad. arXiv: Machine Learning, 2020."
REFERENCES,0.2375296912114014,"[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of
deep bidirectional transformers for language understanding. ArXiv, abs/1810.04805, 2019."
REFERENCES,0.23871733966745842,"[14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image
recognition at scale. ArXiv, abs/2010.11929, 2020."
REFERENCES,0.23990498812351543,"[15] Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal non-
convex optimization via stochastic path-integrated differential estimator. Advances in neural
information processing systems, 31, 2018."
REFERENCES,0.24109263657957244,"[16] Matthew Faw, Isidoros Tziotis, Constantine Caramanis, Aryan Mokhtari, Sanjay Shakkottai,
and Rachel Ward. The power of adaptivity in sgd: Self-tuning step sizes with unbounded
gradients and affine variance. In Conference on Learning Theory, pages 313–355. PMLR, 2022."
REFERENCES,0.24228028503562946,"[17] Matthew Faw, Litu Rout, Constantine Caramanis, and Sanjay Shakkottai. Beyond uniform
smoothness: A stopped analysis of adaptive sgd. arXiv preprint arXiv:2302.06570, 2023."
REFERENCES,0.24346793349168647,"[18] Sébastien Gadat and Ioana Gavra. Asymptotic study of stochastic adaptive algorithms in
non-convex landscape. The Journal of Machine Learning Research, 23(1):10357–10410, 2022."
REFERENCES,0.24465558194774348,"[19] Zhishuai Guo, Yi Xu, Wotao Yin, Rong Jin, and Tianbao Yang. A novel convergence analysis
for algorithms of the adam family. ArXiv, abs/2112.03459, 2021."
REFERENCES,0.24584323040380046,"[20] Hideaki Iiduka. Theoretical analysis of adam using hyperparameters close to one without
lipschitz smoothness. Numerical Algorithms, pages 1–39, 2023."
REFERENCES,0.24703087885985747,"[21] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation
with conditional adversarial networks. 2017 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 5967–5976, 2016."
REFERENCES,0.24821852731591448,"[22] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In NIPS, 2013."
REFERENCES,0.2494061757719715,"[23] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR,
abs/1412.6980, 2014."
REFERENCES,0.2505938242280285,"[24] Frederik Kunstner, Jacques Chen, Jonathan Wilder Lavington, and Mark Schmidt. Noise is not
the main factor behind the gap between sgd and adam on transformers, but sign descent might
be. arXiv preprint arXiv:2304.13960, 2023."
REFERENCES,0.2517814726840855,"[25] Lihua Lei, Cheng Ju, Jianbo Chen, and Michael I Jordan. Non-convex finite-sum optimization
via scsg methods. Advances in Neural Information Processing Systems, 30, 2017."
REFERENCES,0.2529691211401425,"[26] Haochuan Li, Jian Qian, Yi Tian, Alexander Rakhlin, and Ali Jadbabaie. Convex and non-convex
optimization under generalized smoothness. arXiv preprint arXiv:2306.01264, 2023."
REFERENCES,0.25415676959619954,"[27] Zhize Li, Hongyan Bao, Xiangliang Zhang, and Peter Richtárik. Page: A simple and optimal
probabilistic gradient estimator for nonconvex optimization. In International conference on
machine learning, pages 6286–6295. PMLR, 2021."
REFERENCES,0.25534441805225655,"[28] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Manfred Otto Heess, Tom
Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement
learning. CoRR, abs/1509.02971, 2015."
REFERENCES,0.25653206650831356,"[29] Deyi Liu, Lam M Nguyen, and Quoc Tran-Dinh. An optimal hybrid variance-reduced algorithm
for stochastic composite nonconvex optimization. arXiv preprint arXiv:2008.09055, 2020."
REFERENCES,0.25771971496437057,"[30] Zijian Liu, Perry Dong, Srikanth Jagabathula, and Zhengyuan Zhou. Near-optimal high-
probability convergence for non-convex stochastic optimization with variance reduction. arXiv
preprint arXiv:2302.06032, 2023."
REFERENCES,0.2589073634204275,"[31] Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic
bound of learning rate. ArXiv, abs/1902.09843, 2019."
REFERENCES,0.26009501187648454,"[32] Julien Mairal. Optimization with first-order surrogate functions. In International Conference
on Machine Learning, pages 783–791. PMLR, 2013."
REFERENCES,0.26128266033254155,"[33] Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lill-
icrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep
reinforcement learning. ArXiv, abs/1602.01783, 2016."
REFERENCES,0.26247030878859856,"[34] Jiang Qian, Yuren Wu, Bojin Zhuang, Shaojun Wang, and Jing Xiao. Understanding gradient
clipping in incremental gradient methods. In International Conference on Artificial Intelligence
and Statistics, 2021."
REFERENCES,0.26365795724465557,"[35] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with
deep convolutional generative adversarial networks. CoRR, abs/1511.06434, 2015."
REFERENCES,0.2648456057007126,"[36] Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola. Stochastic vari-
ance reduction for nonconvex optimization. In Maria Florina Balcan and Kilian Q. Weinberger,
editors, Proceedings of The 33rd International Conference on Machine Learning, volume 48
of Proceedings of Machine Learning Research, pages 314–323, New York, New York, USA,
20–22 Jun 2016. PMLR. URL https://proceedings.mlr.press/v48/reddi16.html."
REFERENCES,0.2660332541567696,"[37] Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond.
ArXiv, abs/1904.09237, 2018."
REFERENCES,0.2672209026128266,"[38] Amirhossein Reisizadeh, Haochuan Li, Subhro Das, and Ali Jadbabaie. Variance-reduced
clipping for non-convex optimization. arXiv preprint arXiv:2303.00883, 2023."
REFERENCES,0.2684085510688836,"[39] Nicolas Roux, Mark Schmidt, and Francis Bach. A stochastic gradient method with an expo-
nential convergence _rate for finite training sets. In F. Pereira, C.J. Burges, L. Bottou, and K.Q.
Weinberger, editors, Advances in Neural Information Processing Systems, volume 25. Curran
Associates, Inc., 2012. URL https://proceedings.neurips.cc/paper_files/paper/
2012/file/905056c1ac1dad141560467e0a99e1cf-Paper.pdf."
REFERENCES,0.2695961995249406,"[40] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal
policy optimization algorithms. ArXiv, abs/1707.06347, 2017."
REFERENCES,0.27078384798099764,"[41] Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized
loss minimization. Journal of Machine Learning Research, 14(1), 2013."
REFERENCES,0.27197149643705465,"[42] Naichen Shi, Dawei Li, Mingyi Hong, and Ruoyu Sun. Rmsprop converges with proper
hyper-parameter. In International Conference on Learning Representations, 2021."
REFERENCES,0.27315914489311166,"[43] Quoc Tran-Dinh, Nhan H Pham, Dzung T Phan, and Lam M Nguyen. Hybrid stochastic gradient
descent algorithms for stochastic nonconvex optimization. arXiv preprint arXiv:1905.05920,
2019."
REFERENCES,0.27434679334916867,"[44] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.
Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. ArXiv, abs/1706.03762,
2017."
REFERENCES,0.2755344418052256,"[45] Bohan Wang, Yushun Zhang, Huishuai Zhang, Qi Meng, Zhirui Ma, Tie-Yan Liu, and Wei
Chen. Provable adaptivity in adam. ArXiv, abs/2208.09900, 2022."
REFERENCES,0.27672209026128264,"[46] Bohan Wang, Huishuai Zhang, Zhiming Ma, and Wei Chen. Convergence of adagrad for
non-convex objectives: Simple proofs and relaxed assumptions. In The Thirty Sixth Annual
Conference on Learning Theory, pages 161–190. PMLR, 2023."
REFERENCES,0.27790973871733965,"[47] Ruiqi Wang and Diego Klabjan. Divergence results and convergence of a variance reduced
version of adam. ArXiv, abs/2210.05607, 2022."
REFERENCES,0.27909738717339666,"[48] Bohang Zhang, Jikai Jin, Cong Fang, and Liwei Wang. Improved analysis of clipping algorithms
for non-convex optimization. Advances in Neural Information Processing Systems, 33:15511–
15521, 2020."
REFERENCES,0.28028503562945367,"[49] J. Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates
training: A theoretical justification for adaptivity. arXiv: Optimization and Control, 2019."
REFERENCES,0.2814726840855107,"[50] Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank Reddi,
Sanjiv Kumar, and Suvrit Sra. Why are adaptive methods good for attention models? Advances
in Neural Information Processing Systems, 33:15383–15393, 2020."
REFERENCES,0.2826603325415677,"[51] Yushun Zhang, Congliang Chen, Naichen Shi, Ruoyu Sun, and Zhimin Luo. Adam can converge
without any modification on update rules. ArXiv, abs/2208.09632, 2022."
REFERENCES,0.2838479809976247,"[52] Shen-Yi Zhao, Yin-Peng Xie, and Wu-Jun Li. On the convergence and improvement of stochastic
normalized gradient descent. Science China Information Sciences, 64, 2021."
REFERENCES,0.2850356294536817,"[53] Dongruo Zhou, Jinghui Chen, Yuan Cao, Yiqi Tang, Ziyan Yang, and Quanquan Gu. On
the convergence of adaptive gradient methods for nonconvex optimization. arXiv preprint
arXiv:1808.05671, 2018."
REFERENCES,0.2862232779097387,"[54] Zhiming Zhou, Qingru Zhang, Guansong Lu, Hongwei Wang, Weinan Zhang, and Yong
Yu. Adashift: Decorrelation and convergence of adaptive learning rate methods.
ArXiv,
abs/1810.00143, 2018."
REFERENCES,0.28741092636579574,"[55] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image
translation using cycle-consistent adversarial networks. 2017 IEEE International Conference
on Computer Vision (ICCV), pages 2242–2251, 2017."
REFERENCES,0.28859857482185275,"[56] Fangyu Zou, Li Shen, Zequn Jie, Weizhong Zhang, and Wei Liu. A sufficient condition for
convergences of adam and rmsprop. 2019 IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), pages 11119–11127, 2018."
REFERENCES,0.28978622327790976,"A
Additional related work"
REFERENCES,0.29097387173396677,"In this section, we discuss additional related work on variants of Adam and variance reduction
methods."
REFERENCES,0.2921615201900237,"Variants of Adam.
After Reddi et al. [37] pointed out the non-convergence issue with Adam,
various variants of Adam that can be proved to converge were proposed [56; 18; 6; 5; 31; 54].
For example, AMSGrad [37] and AdaFom [6] modify the second order momentum so that it is
non-decreasing. AdaBound [31] explicitly imposes upper and lower bounds on the second order
momentum so that the stepsize is also bounded. AdaShift [54] uses a new estimate of the second
order momentum to correct the bias. There are also some works [53; 18; 20] that provide convergence
guarantees of these variants. One closely related work to ours is [47], which considers a variance-
reduced version of Adam by combining Adam and SVRG [22]. However, they assume bounded
gradients and can only get an asymptotic convergence in the non-convex setting."
REFERENCES,0.29334916864608074,"Variance reduction methods.
The technique of variance reduction was introduced to accelerate
convex optimization in the finite-sum setting [39; 22; 41; 32; 11]. Later, many works studied
variance-reduced methods in the non-convex setting and obtained improved convergence rates
for standard smooth functions. For example, SVRG and SCSG improve the O(ϵ−4) gradient
complexity of stochastic gradient descent (SGD) to O(ϵ−10/3) [2; 36; 25]. Many new variance
reduction methods [15; 43; 29; 27; 9; 30] were later proposed to further improve the complexity
to O(ϵ−3), which is optimal and matches the lower bound in [3]. Recently, [38; 7] obtained the
O(ϵ−3) complexity for the more general (L0, L1) smooth functions. Our variance-reduced Adam
is motivated by the STORM algorithm proposed by [9], where an additional term is added in the
momentum update to correct the bias and reduce the variance."
REFERENCES,0.29453681710213775,"B
Proof sketches of informal lemmas in Section 5.3"
REFERENCES,0.29572446555819476,"In this section, we provide the proof sketches of the informal lemmas in Section 5.3. We focus on
illustrating the ideas rather than rigorous proof details. Please see Appendix E for more rigorous and
detailed proofs of Adam in the stochastic setting."
REFERENCES,0.29691211401425177,"Proof Sketch of Lemma 5.4. By the definition of τ, for all t < τ, we have f(xt) −f ∗≤F which
implies ∥∇f(xt)∥≤G. Then from the update rule (18) in Proposition E.1 provided later in
Appendix E, it is easy to verify ˆvt ⪯G2 since ˆvt is a convex combination of {(∇f(xs))2}s≤t. Let
ht := η/(√ˆvt + λ) be the stepsize vector and denote Ht := diag(ht). We know"
REFERENCES,0.2980997624703088,"η
2GI ⪯
η
G + λI ⪯Ht ⪯η"
REFERENCES,0.2992874109263658,"λI.
(7)"
REFERENCES,0.3004750593824228,"As discussed in Section 5.2, when η is small enough, we can apply Corollary 5.2 to obtain"
REFERENCES,0.3016627078384798,"f(xt+1) −f(xt) ⪅

∇f(xt), xt+1 −xt"
REFERENCES,0.3028503562945368,"= −∥∇f(xt)∥2
Ht −∇f(xt)⊤Htϵt ≤−1"
REFERENCES,0.30403800475059384,"2 ∥∇f(xt)∥2
Ht + 1"
REFERENCES,0.30522565320665085,"2 ∥ϵt∥2
Ht ≤−η"
REFERENCES,0.30641330166270786,4G ∥∇f(xt)∥2 + η
REFERENCES,0.30760095011876487,"2λ ∥ϵt∥2 ,"
REFERENCES,0.3087885985748218,where in the first (approximate) inequality we ignore the second order term 1
REFERENCES,0.30997624703087884,"2L ∥xt+1 −xt∥2 ∝η2
in Corollary 5.2 for small enough η; the equality applies the update rule xt+1 −xt = −Ht ˆmt =
−Ht(∇f(xt) + ϵt); in the second inequality we use 2a⊤Ab ≤∥a∥2
A + ∥b∥2
A for any PSD matrix A
and vectors a and b; and the last inequality is due to (7)."
REFERENCES,0.31116389548693585,"Proof Sketch of Lemma 5.5. By the update rule (18) in Proposition E.1, we have"
REFERENCES,0.31235154394299286,"ϵt+1 = (1 −αt+1) (ϵt + ∇f(xt) −∇f(xt+1)) .
(8)"
REFERENCES,0.31353919239904987,"For small enough η, we can apply Corollary 5.2 to get"
REFERENCES,0.3147268408551069,"∥∇f(xt+1)−∇f(xt)∥2 ≤L2 ∥xt+1−xt∥2 ≤O(η2G2ρ) ∥ˆmt∥2 ≤O(η2G2ρ)(∥∇f(xt)∥2+∥ϵt∥2),
(9)"
REFERENCES,0.3159144893111639,"where the second inequality is due to L = O(Gρ) and ∥xt+1 −xt∥= O(η) ∥ˆmt∥; and the last
inequality uses ˆmt = ∇f(xt) + ϵt and Young’s inequality ∥a + b∥2 ≤2 ∥a∥2 + 2 ∥b∥2. Therefore,"
REFERENCES,0.3171021377672209,∥ϵt+1∥2 ≤(1 −αt+1)(1 + αt+1/2) ∥ϵt∥2 + (1 + 2/αt+1) ∥∇f(xt+1) −∇f(xt)∥2
REFERENCES,0.3182897862232779,"≤(1 −αt+1/2) ∥ϵt∥2 + O(η2G2ρ/αt+1)

∥∇f(xt)∥2 + ∥ϵt∥2"
REFERENCES,0.3194774346793349,≤(1 −β/4) ∥ϵt∥2 + λβ
REFERENCES,0.32066508313539194,"16G ∥∇f(xt)∥2 ,"
REFERENCES,0.32185273159144895,where the first inequality uses (8) and Young’s inequality ∥a + b∥2 ≤(1 + u) ∥a∥2 + (1 + 1/u) ∥b∥2
REFERENCES,0.32304038004750596,"for any u > 0; the second inequality uses (1 −αt+1)(1 + αt+1/2) ≤1 −αt+1/2 and (9); and in the
last inequality we use β ≤αt+1 and choose β = Θ(ηGρ+1/2) which implies O(η2G2ρ/αt+1) ≤
λβ
16G ≤β/4."
REFERENCES,0.32422802850356297,"C
Probabilistic lemmas"
REFERENCES,0.3254156769596199,"In this section, we state several well-known and useful probabilistic lemmas without proof.
Lemma C.1 (Azuma-Hoeffding inequality). Let {Zt}t≥1 be a martingale with respect to a filtration
{Ft}t≥0. Assume that |Zt −Zt−1| ≤ct almost surely for all t ≥0. Then for any fixed T, with
probability at least 1 −δ,"
REFERENCES,0.32660332541567694,ZT −Z0 ≤
REFERENCES,0.32779097387173395,"v
u
u
t2 T
X"
REFERENCES,0.32897862232779096,"t=1
c2
t log(1/δ)."
REFERENCES,0.33016627078384797,"Lemma C.2 (Optional Stopping Theorem). Let {Zt}t≥1 be a martingale with respect to a filtration
{Ft}t≥0. Let τ be a bounded stopping time with respect to the same filtration. Then we have
E[Zτ] = E[Z0]."
REFERENCES,0.331353919239905,"D
Proofs related to (ρ, L0, Lρ) smoothness"
REFERENCES,0.332541567695962,"In this section, we provide proofs related to (ρ, L0, Lρ) smoothness. In what follows, we first
provide a formal proposition in Appendix D.1 showing that univariate rational functions and double
exponential functions are (ρ, L0, Lρ) smooth with ρ < 2, as we claimed in Section 3.2.1, and then
provide the proofs of Lemma 3.4, Lemma 5.1, and Corollary 5.2 in Appendix D.2, D.3 and D.4
respectively."
REFERENCES,0.333729216152019,"D.1
Examples"
REFERENCES,0.334916864608076,"Proposition D.1. Any univariate rational function P(x)/Q(x), where P, Q are two polynomials,
and any double exponential function a(bx), where a, b > 1, are (ρ, L0, Lρ) smooth with 1 < ρ < 2.
However, they are not necessarily (L0, L1) smooth."
REFERENCES,0.336104513064133,Proof of Proposition D.1. We prove the proposition in the following four parts:
REFERENCES,0.33729216152019004,"1. Univariate rational functions are (ρ, L0, Lρ) smooth with 1 < ρ < 2. Let f(x) = P(x)/Q(x)
where P and Q are two polynomials. Then the partial fractional decomposition of f(x) is given by"
REFERENCES,0.33847980997624705,"f(x) = w(x) + m
X i=1 ji
X r=1"
REFERENCES,0.33966745843230406,"Air
(x −ai)r + n
X i=1 ki
X r=1"
REFERENCES,0.34085510688836107,"Birx + Cir
(x2 + bix + ci)r ,"
REFERENCES,0.342042755344418,"where w(x) is a polynomial, Air, Bir, Cir, ai, bi, ci are all real constants satisfying b2
i −4ci < 0 for
each 1 ≤i ≤n which implies x2 + bix + ci > 0 for all x ∈R. Assume Aiji ̸= 0 without loss of"
REFERENCES,0.34323040380047504,"generality. Then we know f has only finite singular points {ai}1≤i≤m and has continuous first and
second order derivatives at all other points. To simplify notation, denote"
REFERENCES,0.34441805225653205,"pir(x) :=
Air
(x −ai)r ,
qir(x) :=
Birx + Cir
(x2 + bix + ci)r ."
REFERENCES,0.34560570071258906,"Then we have f(x) = w(x) + Pm
i=1
Pji
r=1 pir(x) + Pn
i=1
Pki
r=1 qir(x). For any 3/2 < ρ < 2, we
know that ρ > r+2"
REFERENCES,0.34679334916864607,r+1 for any r ≥1. Then we can show that
REFERENCES,0.3479809976247031,"lim
x→ai
|f ′(x)|ρ"
REFERENCES,0.3491686460807601,"|f ′′(x)| = lim
x→ai"
REFERENCES,0.3503562945368171,"p′
iji(x)
ρ
p′′
iji(x)
 = ∞,
(10)"
REFERENCES,0.3515439429928741,"where the first equality is because one can easily verify that the first and second order derivatives
of piji dominate those of all other terms when x goes to ai, and the second equality is because
p′
iji(x)
ρ = O
 
(x −ai)−ρ(ji+1)
,
p′′
iji(x)
 = O
 
(x −ai)−(ji+2)
, and ρ(ji + 1) > ji + 2 (here
we assume ji ≥1 since otherwise there is no need to prove (10) for i). Note that (10) implies that,
for any Lρ > 0, there exists δi > 0 such that"
REFERENCES,0.3527315914489311,"|f ′′(x)| ≤Lρ |f ′(x)|ρ ,
if |x −ai| < δi.
(11)"
REFERENCES,0.35391923990498814,"Similarly, one can show limx→∞|f ′(x)|
ρ"
REFERENCES,0.35510688836104515,"|f ′′(x)| = ∞, which implies there exists M > 0 such that"
REFERENCES,0.35629453681710216,"|f ′′(x)| ≤Lρ |f ′(x)|ρ ,
if |x| > M.
(12)
Define
B := {x ∈R | |x| ≤M and |x −ai| ≥δi, ∀i} ."
REFERENCES,0.35748218527315917,"We know B is a compact set and therefore the continuous function f ′′ is bounded within B, i.e., there
exists some constant L0 > 0 such that
|f ′′(x)| ≤L0,
if x ∈B.
(13)
Combining (11), (12), and (13), we have shown
|f ′′(x)| ≤L0 + Lρ |f ′(x)|ρ ,
∀x ∈dom(f),
which completes the proof of the first part."
REFERENCES,0.3586698337292161,"2. Rational functions are not necessarily (L0, L1) smooth. Consider the ration function f(x) =
1/x. Then we know that f ′(x) = −1/x2 and f ′′(x) = 2/x3. Note that for any 0 < x ≤
min{(L0 + 1)−1/3, (L1 + 1)−1}, we have"
REFERENCES,0.35985748218527314,|f ′′(x)| = 1
REFERENCES,0.36104513064133015,x3 + 1
REFERENCES,0.36223277909738716,"x · |f ′(x)| > L0 + L1 |f ′(x)| ,"
REFERENCES,0.36342042755344417,"which shows f is not (L0, L1) smooth for any L0, L1 ≥0."
REFERENCES,0.3646080760095012,"3. Double exponential functions are (ρ, L0, Lρ) smooth with 1 < ρ < 2. Let f(x) = a(bx), where
a, b > 1, be a double exponential function. Then we know that"
REFERENCES,0.3657957244655582,"f ′(x) = log(a) log(b) bxa(bx),
f ′′(x) = log(b)(log(a)bx + 1) · f ′(x).
For any ρ > 1, we have"
REFERENCES,0.3669833729216152,"lim
x→+∞
|f ′(x)|ρ"
REFERENCES,0.3681710213776722,"|f ′′(x)| =
lim
x→+∞
|f ′(x)|ρ−1"
REFERENCES,0.3693586698337292,"log(b)(log(a)bx + 1) =
lim
y→+∞
(log(a) log(b)y)ρ−1 a(ρ−1)y"
REFERENCES,0.37054631828978624,"log(b)(log(a)y + 1)
= ∞,"
REFERENCES,0.37173396674584325,"where the first equality is a direct calculation; the second equality uses change of variable y = bx; and
the last equality is because exponential function grows faster than linear function. Then we complete
the proof following a similar argument to that in Part 1."
REFERENCES,0.37292161520190026,"4. Double exponential functions are not necessarily (L0, L1) smooth. Consider the double
exponential function f(x) = e(ex). Then we have"
REFERENCES,0.37410926365795727,"f ′(x) = exe(ex),
f ′′(x) = (ex + 1) · f ′(x).
For any x ≥max {log(L0 + 1), log(L1 + 1)}, we can show that
|f ′′(x)| > (L1 + 1)f ′(x) > L0 + L1 |f ′(x)| ,
which shows f is not (L0, L1) smooth for any L0, L1 ≥0."
REFERENCES,0.3752969121140142,"D.2
Proof of Lemma 3.4"
REFERENCES,0.37648456057007124,"Before proving Lemma 3.4, we need the following lemma that generalizes (a special case of)
Grönwall’s inequality.
Lemma D.2. Let u : [a, b] →[0, ∞) and ℓ: [0, ∞) →(0, ∞) be two continuous functions. Suppose
u′(t) ≤ℓ(u(t)) for all t ∈(a, b). Denote function ϕ(w) :=
R
1
ℓ(w) dw. We have for all t ∈[a, b],"
REFERENCES,0.37767220902612825,ϕ(u(t)) ≤ϕ(u(a)) −a + t.
REFERENCES,0.37885985748218526,"Proof of Lemma D.2. First, by definition, we know that ϕ is increasing since ϕ′ =
1
ℓ> 0. Let
function v be the solution of the following differential equation"
REFERENCES,0.38004750593824227,"v′(t) = ℓ(v(t)) ∀t ∈(a, b),
v(a) = u(a).
(14)"
REFERENCES,0.3812351543942993,It is straightforward to verify that the solution to (14) satisfies
REFERENCES,0.3824228028503563,ϕ(v(t)) −t = ϕ(u(a)) −a.
REFERENCES,0.3836104513064133,"Then it suffices to show ϕ(u(t)) ≤ϕ(v(t)) ∀t ∈[a, b]. Note that"
REFERENCES,0.3847980997624703,"(ϕ(u(t)) −ϕ(v(t)))′ = ϕ′(u(t))u′(t) −ϕ′(v(t))v′(t) =
u′(t)
ℓ(u(t)) −v′(t)"
REFERENCES,0.3859857482185273,"ℓ(v(t)) ≤0,"
REFERENCES,0.38717339667458434,"where the inequality is because u′(t) ≤ℓ(u(t)) by the assumption of this lemma and v′(t) = ℓ(v(t))
by (14). Since ϕ(u(a)) −ϕ(v(a)) = 0, we know for all t ∈[a, b], ϕ(u(t)) ≤ϕ(v(t))."
REFERENCES,0.38836104513064135,"With Lemma D.2, one can bound the gradient norm within a small enough neighborhood of a given
point as in the following lemma.
Lemma D.3. Suppose f is (ρ, L0, Lρ) smooth for some ρ, ρ, L0, Lρ ≥0. For any a > 0 and points
x, y ∈dom(f) satisfying ∥y −x∥≤
a
L0+Lρ(∥∇f(x)∥+a)ρ , we have"
REFERENCES,0.38954869358669836,∥∇f(y)∥≤∥∇f(x)∥+ a.
REFERENCES,0.39073634204275537,"Proof of Lemma D.3. Denote functions z(t) := (1−t)x+ty and u(t) := ∥∇f(z(t))∥for 0 ≤t ≤1.
Note that for any 0 ≤t ≤s ≤1, by triangle inequality,"
REFERENCES,0.3919239904988123,u(s) −u(t) ≤∥∇f(z(s)) −∇f(z(t))∥.
REFERENCES,0.39311163895486934,"We know that u(t) = ∥∇f(z(t))∥is differentiable since f is second order differentiable2. Then we
have"
REFERENCES,0.39429928741092635,"u′(t) = lim
s↓t
u(s) −u(t)"
REFERENCES,0.39548693586698336,"s −t
≤lim
s↓t
∥∇f(z(s)) −∇f(z(t))∥"
REFERENCES,0.39667458432304037,"s −t
=
lim
s↓t
∇f(z(s)) −∇f(z(t)) s −t "
REFERENCES,0.3978622327790974,"≤
∇2f(z(t))
 ∥y −x∥≤(L0 + Lρu(t)ρ) ∥y −x∥."
REFERENCES,0.3990498812351544,"Let ϕ(w) :=
R w
0
1
(L0+Lρvρ)∥y−x∥dv. By Lemma D.2, we know that"
REFERENCES,0.4002375296912114,ϕ (∥∇f(y)∥) = ϕ(u(1)) ≤ϕ(u(0)) + 1 = ϕ (∥∇f(x)∥) + 1.
REFERENCES,0.4014251781472684,"Denote ψ(w) :=
R w
0
1
(L0+Lρvρ)dv = ϕ(w) · ∥y −x∥. We have"
REFERENCES,0.4026128266033254,ψ (∥∇f(y)∥) ≤ψ (∥∇f(x)∥) + ∥y −x∥
REFERENCES,0.40380047505938244,"≤ψ (∥∇f(x)∥) +
a
L0 + Lρ(∥∇f(x)∥+ a)ρ"
REFERENCES,0.40498812351543945,"≤
Z ∥∇f(x)∥ 0"
REFERENCES,0.40617577197149646,"1
(L0 + Lρvρ)dv +
Z ∥∇f(x)∥+a"
REFERENCES,0.40736342042755347,∥∇f(x)∥
REFERENCES,0.4085510688836104,"1
(L0 + Lρvρ)dv"
REFERENCES,0.40973871733966744,=ψ(∥∇f(x)∥+ a)
REFERENCES,0.41092636579572445,"Since ψ is increasing, we have ∥∇f(y)∥≤∥∇f(x)∥+ a."
REFERENCES,0.41211401425178146,"2Here we assume u(t) > 0 for 0 < t < 1. Otherwise, we can define tm = sup{0 < t < 1 | u(t) = 0} and
consider the interval [tm, 1] instead."
REFERENCES,0.41330166270783847,"With Lemma D.3, we are ready to prove Lemma 3.4."
REFERENCES,0.4144893111638955,"Proof of Lemma 3.4. Denote z(t) := (1 −t)x + ty for some y ∈Rd satisfying ∥y −x∥≤
a
L0+Lρ(∥∇f(x)∥+a)ρ . We first show y ∈dom(f) by contradiction. Suppose y /∈dom(f), let
us define tb := inf{0 ≤t ≤1 | z(t) /∈X} and zb := z(tb). Then we know zb is a boundary point of
X. Since f is a closed function with an open domain, we have"
REFERENCES,0.4156769596199525,"lim
t↑tb f(z(t)) = ∞.
(15)"
REFERENCES,0.4168646080760095,"On the other hand, by the definition of tb, we know z(t) ∈X for every 0 ≤t < tb. Then by
Lemma D.3, for all 0 ≤t < tb, we have ∥∇f(z(t))∥≤∥∇f(x)∥+ a. Therefore for all 0 ≤t < tb"
REFERENCES,0.4180522565320665,"f(z(t)) ≤f(x) +
Z t 0"
REFERENCES,0.4192399049881235,"∇f(z(s)), y −x

ds"
REFERENCES,0.42042755344418054,"≤f(x) + (∥∇f(x)∥+ a) · ∥y −x∥
<∞,"
REFERENCES,0.42161520190023755,which contradicts with (15). Therefore we have shown y ∈dom(f). We have
REFERENCES,0.42280285035629456,∥∇f(y) −∇f(x)∥= Z 1
REFERENCES,0.42399049881235157,"0
∇2f(z(t)) · (y −x) dt"
REFERENCES,0.4251781472684085,"≤∥y −x∥·
Z 1"
REFERENCES,0.42636579572446553,"0
(L0 + Lρ ∥∇f(z(t))∥ρ) dt"
REFERENCES,0.42755344418052255,≤∥y −x∥· (L0 + Lρ · (∥∇f(x)∥+ a)ρ)
REFERENCES,0.42874109263657956,where the last inequality is due to Lemma D.3.
REFERENCES,0.42992874109263657,"D.3
Proof of Lemma 5.1"
REFERENCES,0.4311163895486936,"Proof of Lemma 5.1. Denote G := ∥∇f(x)∥and L := 3L0 + 4LρGρ. Let y := x −
1
2L∇f(x).
Then we have"
REFERENCES,0.4323040380047506,∥y −x∥= G
REFERENCES,0.4334916864608076,"2L =
G
6L0 + 8LρGρ ≤min"
REFERENCES,0.4346793349168646,"(
1
5LρGρ−1 ,
1
5(Lρ−1
0
Lρ)1/ρ ) =: r,"
REFERENCES,0.4358669833729216,"where the inequality can be easily verified considering both cases of G ≤(L0/Lρ)1/ρ and G ≥
(L0/Lρ)1/ρ. Then based on Corollary 5.2, we have y ∈dom(f) and"
REFERENCES,0.43705463182897863,"f ∗−f(x) ≤f(y) −f(x) ≤

∇f(x), y −x

+ L"
REFERENCES,0.43824228028503565,2 ∥y −x∥2 = 3LG2
REFERENCES,0.43942992874109266,"8
≤LG2 3
,"
REFERENCES,0.44061757719714967,which completes the proof.
REFERENCES,0.4418052256532066,"D.4
Proof of Corollary 5.2"
REFERENCES,0.44299287410926363,"Proof of Corollary 5.2. First, Lemma 3.4 states that for any a > 0,"
REFERENCES,0.44418052256532065,"∥y −x∥≤
a
L0+Lρ·(∥∇f(x)∥+a)ρ =⇒∥∇f(y)−∇f(x)∥≤(L0+Lρ · (∥∇f(x)∥+a)ρ) ∥y −x∥."
REFERENCES,0.44536817102137766,"If ∥∇f(x)∥≤G, we choose a = max{G, (L0/Lρ)1/ρ}. Then it is straightforward to verify that"
REFERENCES,0.44655581947743467,"a
L0 + Lρ · (∥∇f(x)∥+ a)ρ ≥min"
REFERENCES,0.4477434679334917,"(
1
5LρGρ−1 ,
1
5(Lρ−1
0
Lρ)1/ρ ) =: r,"
REFERENCES,0.4489311163895487,L0 + Lρ · (∥∇f(x)∥+ a)ρ ≤3L0 + 4LρGρ =: L.
REFERENCES,0.4501187648456057,"Therefore we have shown for any x, y satisfying ∥y −x∥≤r,"
REFERENCES,0.4513064133016627,"∥∇f(y) −∇f(x)∥≤L ∥y −x∥.
(16)"
REFERENCES,0.4524940617577197,"Next, let z(t) := (1 −t)x + ty for 0 ≤t ≤1. We know"
REFERENCES,0.45368171021377673,"f(y) −f(x) =
Z 1 0"
REFERENCES,0.45486935866983375,"∇f(z(t), y −x

dt =
Z 1 0"
REFERENCES,0.45605700712589076,"∇f(x), y −x

+

∇f(z(t)) −∇f(x), y −x

dt"
REFERENCES,0.45724465558194777,"≤

∇f(x), y −x

+
Z 1"
REFERENCES,0.4584323040380047,"0
L ∥z(t) −x∥∥y −x∥dt"
REFERENCES,0.45961995249406173,"=

∇f(x), y −x

+ L ∥y −x∥2
Z 1"
T DT,0.46080760095011875,"0
t dt"
T DT,0.46199524940617576,"=

∇f(x), y −x

+ 1"
T DT,0.46318289786223277,"2L ∥y −x∥2 ,"
T DT,0.4643705463182898,where the inequality is due to (16).
T DT,0.4655581947743468,"E
Convergence analysis of Adam"
T DT,0.4667458432304038,"In this section, we provide detailed convergence analysis of Adam. We will focus on proving
Theorem 4.1 under the bounded noise assumption (Assumption 3) in most parts of this section except
Appendix E.6 where we will show how to generalize the results to noise with sub-Gaussian norm
(Assumption 4) and provide the proof of Theorem 4.2."
T DT,0.4679334916864608,"For completeness, we repeat some important technical definitions here. First, we define"
T DT,0.4691211401425178,"ϵt := ˆmt −∇f(xt)
(17)"
T DT,0.47030878859857483,"as the deviation of the re-scaled momentum from the actual gradient. Given a large enough constant
G defined in Theorem 4.1, denoting F =
G2
3(3L0+4LρGρ), we formally define the stopping time τ as"
T DT,0.47149643705463185,"τ := min{t | f(xt) −f ∗> F} ∧(T + 1),"
T DT,0.47268408551068886,"i.e., τ is the first time when the sub-optimality gap is strictly greater than F, truncated at T + 1 to
make sure it is bounded in order to apply Lemma C.2. Based on Lemma 5.1 and the discussions
below it, we know that if t < τ, we have both f(xt) −f ∗≤F and ∥∇f(xt)∥≤G. It is clear to see
that τ is a stopping time3 with respect to {ξt}t≥1 because the event {τ ≥t} is a function of {ξs}s<t
and independent of {ξs}s≥t. Next, let"
T DT,0.47387173396674587,"ht :=
η
√ˆvt + λ"
T DT,0.4750593824228028,"be the stepsize vector and Ht := diag(ht) be the diagonal stepsize matrix. Then the update rule can
be written as"
T DT,0.47624703087885983,xt+1 = xt −ht ⊙ˆmt = xt −Ht ˆmt.
T DT,0.47743467933491684,"Finally, as in Corollary 5.2 and Lemma 5.3, we define the following constants."
T DT,0.47862232779097386,r := min
T DT,0.47980997624703087,"(
1
5LρGρ−1 ,
1
5(Lρ−1
0
Lρ)1/ρ ) ,"
T DT,0.4809976247030879,"L := 3L0 + 4LρGρ,
D := 2G/λ."
T DT,0.4821852731591449,"E.1
Equivalent update rule of Adam"
T DT,0.4833729216152019,"The bias correction steps in Lines 7–8 make Algorithm 1 a bit complicated. In the following
proposition, we provide an equivalent yet simpler update rule of Adam."
T DT,0.4845605700712589,"3Indeed, τ −1 is also a stopping time because ∇f(xt) only depends on {ξs}s<t, but that is unnecessary for
our analysis."
T DT,0.4857482185273159,"Proposition E.1. Denote αt =
β
1−(1−β)t and αsq
t =
βsq
1−(1−βsq)t . Then the update rule in Algorithm 1
is equivalent to"
T DT,0.48693586698337293,"ˆmt = (1 −αt) ˆmt−1 + αt∇f(xt, ξt),"
T DT,0.48812351543942994,"ˆvt = (1 −αsq
t )ˆvt−1 + αsq
t (∇f(xt, ξt))2,
(18)"
T DT,0.48931116389548696,"xt+1 = xt −
η
√ˆvt + λ ⊙ˆmt,"
T DT,0.49049881235154397,"where initially we set ˆm1 = ∇f(x1, ξ1) and ˆv1 = (∇f(x1, ξ1))2. Note that since 1−α1 = 1−αsq
1 =
0, there is no need to define ˆm0 and ˆv0."
T DT,0.4916864608076009,"Proof of Proposition E.1. Denote Zt = 1 −(1 −β)t. Then we know αt = β/Zt and mt = Zt ˆmt.
By the momentum update rule in Algorithm 1, we have"
T DT,0.49287410926365793,"Zt ˆmt = (1 −β)Zt−1 ˆmt−1 + β∇f(xt, ξt)."
T DT,0.49406175771971494,Note that Zt satisfies the following property
T DT,0.49524940617577196,(1 −β)Zt−1 = 1 −β −(1 −β)t = Zt −β.
T DT,0.49643705463182897,Then we have
T DT,0.497624703087886,ˆmt =Zt −β
T DT,0.498812351543943,"Zt
· ˆmt−1 + β"
T DT,0.5,"Zt
· ∇f(xt, ξt)"
T DT,0.501187648456057,"=(1 −αt) ˆmt−1 + αt∇f(xt, ξt)."
T DT,0.502375296912114,"Next, we verify the initial condition. By Algorithm 1, since we set m0 = 0, we have m1 =
β∇f(x1, ξ1). Therefore we have ˆm1 = m1/Z1 = ∇f(x1, ξ1) since Z1 = β. Then the proof is
completed by applying the same analysis on vt and ˆvt."
T DT,0.503562945368171,"E.2
Useful lemmas for Adam"
T DT,0.504750593824228,"In this section, we list several useful lemmas for the convergence analysis. Their proofs are all
deferred in Appendix E.5."
T DT,0.505938242280285,"First note that when t < τ, all the quantities in the algorithm are well bounded. In particular, we have
the following lemma.
Lemma E.2. If t < τ, we have"
T DT,0.5071258907363421,"∥∇f(xt)∥≤G,
∥∇f(xt, ξt)∥≤G + σ,
∥ˆmt∥≤G + σ,"
T DT,0.5083135391923991,"ˆvt ⪯(G + σ)2,
η
G + σ + λ ⪯ht ⪯η λ."
T DT,0.5095011876484561,"Next, we provide a useful lemma regarding the time-dependent re-scaled momentum parameters in
(18)."
T DT,0.5106888361045131,"Lemma E.3. Let αt =
β
1−(1−β)t , then for all T ≥2, we have PT
t=2 α2
t ≤3(1 + β2T)."
T DT,0.5118764845605701,"In the next lemma, we provide an almost sure bound on ϵt in order to apply Azuma-Hoeffding
inequality (Lemma C.1)."
T DT,0.5130641330166271,"Lemma E.4. Denote γt−1 = (1 −αt)(ϵt−1 + ∇f(xt−1) −∇f(xt)). Choosing η ≤min
n
r
D, σβ"
T DT,0.5142517814726841,"DL
o
,"
T DT,0.5154394299287411,"if t ≤τ, we have ∥ϵt∥≤2σ and ∥γt−1∥≤2σ."
T DT,0.5166270783847982,"Finally, the following lemma hides messy calculations and will be useful in the contradiction
argument.
Lemma E.5. Denote"
T DT,0.517814726840855,I1 :=8G ηλ
T DT,0.5190023752969121,"
∆1λ + 8σ2
 η"
T DT,0.5201900237529691,"β + ηβT

+ 20σ2η
p"
T DT,0.5213776722090261,"(1/β2 + T)ι

,"
T DT,0.5225653206650831,I2 :=8GF
T DT,0.5237529691211401,"η
= 8G3 3ηL."
T DT,0.5249406175771971,"Under the parameter choices in either Theorem 4.1 or Theorem 4.2, we have I1 ≤I2 and I1/T ≤ϵ2."
T DT,0.5261282660332541,"E.3
Proof of Theorem 4.1"
T DT,0.5273159144893111,"Before proving the main theorems, several important lemmas are needed. First, we provide a descent
lemma for Adam.
Lemma E.6. If t < τ, choosing G ≥σ + λ and η ≤min
 r D, λ"
L,0.5285035629453682,"6L
	
, we have"
L,0.5296912114014252,f(xt+1) −f(xt) ≤−η
L,0.5308788598574822,4G ∥∇f(xt)∥2 + η
L,0.5320665083135392,λ ∥ϵt∥2 .
L,0.5332541567695962,"Proof of Lemma E.6. By Lemma E.2, we have if t < τ,"
L,0.5344418052256532,"ηI
2G ≤
ηI
G + σ + λ ⪯Ht ⪯ηI"
L,0.5356294536817102,"λ .
(19)"
L,0.5368171021377672,"Since we choose η ≤
r
D, by Lemma 5.3, we have ∥xt+1 −xt∥≤r if t < τ. Then we can apply
Corollary 5.2 to show that for any t < τ,"
L,0.5380047505938242,"f(xt+1) −f(xt) ≤

∇f(xt), xt+1 −xt

+ L"
L,0.5391923990498813,2 ∥xt+1 −xt∥2
L,0.5403800475059383,= −(∇f(xt))⊤Ht ˆmt + L
L,0.5415676959619953,"2 ˆm⊤
t H2
t ˆmt"
L,0.5427553444180523,"≤−∥∇f(xt)∥2
Ht −(∇f(xt))⊤Htϵt + ηL"
L,0.5439429928741093,"2λ ∥ˆmt∥2
Ht ≤−2"
L,0.5451306413301663,"3 ∥∇f(xt)∥2
Ht + 3"
L,0.5463182897862233,"4 ∥ϵt∥2
Ht + ηL λ"
L,0.5475059382422803,"
∥∇f(xt)∥2
Ht + ∥ϵt∥2
Ht  ≤−1"
L,0.5486935866983373,"2 ∥∇f(xt)∥2
Ht + ∥ϵt∥2
Ht ≤−η"
L,0.5498812351543944,4G ∥∇f(xt)∥2 + η
L,0.5510688836104513,"λ ∥ϵt∥2 ,"
L,0.5522565320665083,"where the second inequality uses (17) and (19); the third inequality is due to Young’s inequality
a⊤Ab ≤1"
L,0.5534441805225653,"3 ∥a∥2
A + 3"
L,0.5546318289786223,"4 ∥b∥2
A and ∥a + b∥2
A ≤2 ∥a∥2
A + 2 ∥b∥A for any PSD matrix A; the second last
inequality uses η ≤
λ
6L; and the last inequality is due to (19)."
L,0.5558194774346793,"The following lemma bounds the sum of the error term ∥ϵt∥2 before the stopping time τ. Since its
proof is complicated, we defer it in Appendix E.4."
L,0.5570071258907363,"Lemma E.7. If G ≥2σ and η ≤min
n
r
D, λ3/2β"
L,0.5581947743467933,"6L
√ G, σβ"
L,0.5593824228028503,"DL
o
, with probability 1 −δ, τ−1
X"
L,0.5605700712589073,"t=1
∥ϵt∥2 −λ"
L,0.5617577197149644,8G ∥∇f(xt)∥2 ≤8σ2 (1/β + βT) + 20σ2p
L,0.5629453681710214,(1/β2 + T) log(1/δ).
L,0.5641330166270784,"Combining Lemma E.6 and Lemma E.7, we obtain the following useful lemma, which simultaneously
bounds f(xt) −f ∗and Pτ−1
t=1 ∥∇f(xt)∥2."
L,0.5653206650831354,"Lemma E.8. If G ≥2 max{λ, σ} and η ≤min
n
r
D, λ3/2β"
L,0.5665083135391924,"6L
√ G, σβ"
L,0.5676959619952494,"DL
o
, then with probability at least
1 −δ, τ−1
X"
L,0.5688836104513064,"t=1
∥∇f(xt)∥2 + 8G"
L,0.5700712589073634,η (f(xτ) −f ∗) ≤8G ηλ
L,0.5712589073634204,"
∆1λ + 8σ2
 η"
L,0.5724465558194775,"β + ηβT

+ 20σ2η
p"
L,0.5736342042755345,"(1/β2 + T) log(1/δ)

."
L,0.5748218527315915,"Proof of Lemma E.8. By telescoping, Lemma E.6 implies τ−1
X"
L,0.5760095011876485,"t=1
2 ∥∇f(xt)∥2 −8G"
L,0.5771971496437055,λ ∥ϵt∥2 ≤8G
L,0.5783847980997625,η (f(x1) −f(xτ)) ≤8∆1G
L,0.5795724465558195,"η
.
(20)"
L,0.5807600950118765,"Lemma E.7 could be written as τ−1
X t=1"
G,0.5819477434679335,8G
G,0.5831353919239906,λ ∥ϵt∥2 −∥∇f(xt)∥2 ≤8G λ
G,0.5843230403800475,"
8σ2 (1/β + βT) + 20σ2p"
G,0.5855106888361045,"(1/β2 + T) log(1/δ)

.
(21)"
G,0.5866983372921615,(20) + (21) gives the desired result.
G,0.5878859857482185,"With Lemma E.8, we are ready to complete the contradiction argument and the convergence analysis.
Below we provide the proof of Theorem 4.1."
G,0.5890736342042755,"Proof of Theorem 4.1. According to Lemma E.8, there exists some event E with P(E) ≥1 −δ, such
that conditioned on E, we have"
G,0.5902612826603325,8G
G,0.5914489311163895,η (f(xτ) −f ∗) ≤8G ηλ
G,0.5926365795724465,"
∆1λ + 8σ2
 η"
G,0.5938242280285035,"β + ηβT

+ 20σ2η
p"
G,0.5950118764845606,"(1/β2 + T) log(1/δ)

=: I1. (22)"
G,0.5961995249406176,"By the definition of τ, if τ ≤T, we have"
G,0.5973871733966746,8G
G,0.5985748218527316,η (f(xτ) −f ∗) > 8GF
G,0.5997624703087886,"η
= 8G3"
G,0.6009501187648456,3ηL =: I2.
G,0.6021377672209026,"Based on Lemma E.5, we have I1 ≤I2, which leads to a contradiction. Therefore, we must have
τ = T + 1 conditioned on E. Then, Lemma E.8 also implies that under E,"
T,0.6033254156769596,"1
T"
T,0.6045130641330166,"T −1
X"
T,0.6057007125890737,"t=1
∥∇f(xt)∥2 ≤I1"
T,0.6068883610451307,"T ≤ϵ2,"
T,0.6080760095011877,where the last inequality is due to Lemma E.5.
T,0.6092636579572447,"E.4
Proof of Lemma E.7"
T,0.6104513064133017,"In order to prove Lemma E.7, we need the following several lemmas.
Lemma E.9. Denote γt−1 = (1 −αt)(ϵt−1 + ∇f(xt−1) −∇f(xt)). If G ≥2σ and η ≤"
T,0.6116389548693587,"min
n
r
D, λ3/2β"
L,0.6128266033254157,"6L
√ G"
L,0.6140142517814727,"o
, we have for every 2 ≤t ≤τ,"
L,0.6152019002375297,"∥ϵt∥2 ≤

1 −αt 2"
L,0.6163895486935868,"
∥ϵt−1∥2 + λβ"
L,0.6175771971496437,"16G ∥∇f(xt−1)∥2 + α2
tσ2 + 2αt"
L,0.6187648456057007,"γt−1, ∇f(xt, ξt) −∇f(xt)

."
L,0.6199524940617577,"Proof of Lemma E.9. According to the update rule (18), we have"
L,0.6211401425178147,"ϵt =(1 −αt)(ϵt−1 + ∇f(xt−1) −∇f(xt)) + αt(∇f(xt, ξt) −∇f(xt))
=γt−1 + αt(∇f(xt, ξt) −∇f(xt)).
(23)"
L,0.6223277909738717,"Since we choose η ≤
r
D, by Lemma 5.3, we have ∥xt −xt−1∥≤r if t ≤τ. Therefore by
Corollary 5.2, for any 2 ≤t ≤τ,"
L,0.6235154394299287,∥∇f(xt−1) −∇f(xt)∥≤L ∥xt −xt−1∥≤ηL
L,0.6247030878859857,λ ∥ˆmt−1∥≤ηL
L,0.6258907363420427,"λ (∥∇f(xt−1)∥+ ∥ϵt−1∥) ,
(24)"
L,0.6270783847980997,Therefore
L,0.6282660332541568,∥γt−1∥2 = ∥(1 −αt)ϵt−1 + (1 −αt)(∇f(xt−1) −∇f(xt))∥2
L,0.6294536817102138,"≤(1 −αt)2 (1 + αt) ∥ϵt−1∥2 + (1 −αt)2

1 + 1 αt"
L,0.6306413301662708,"
∥∇f(xt−1) −∇f(xt)∥2"
L,0.6318289786223278,≤(1 −αt) ∥ϵt−1∥2 + 1
L,0.6330166270783848,"αt
∥∇f(xt−1) −∇f(xt)∥2"
L,0.6342042755344418,≤(1 −αt) ∥ϵt−1∥2 + 2η2L2 λ2β
L,0.6353919239904988,"
∥∇f(xt−1)∥2 + ∥ϵt−1∥2"
L,0.6365795724465558,"≤

1 −αt 2"
L,0.6377672209026128,"
∥ϵt−1∥2 + λβ"
L,0.6389548693586699,"16G ∥∇f(xt−1)∥2 ,"
L,0.6401425178147269,"where the first inequality uses Young’s inequality ∥a + b∥2 ≤(1 + u) ∥a∥2 + (1 + 1/u) ∥b∥2 for any
u > 0; the second inequality is due to
(1 −αt)2 (1 + αt) = (1 −αt)(1 −α2
t) ≤(1 −αt),"
L,0.6413301662707839,"(1 −αt)2

1 + 1 αt 
= 1"
L,0.6425178147268409,"αt
(1 −αt)2 (1 + αt) ≤1"
L,0.6437054631828979,"αt
(1 −αt) ≤1 αt
;"
L,0.6448931116389549,the third inequality uses (24) and Young’s inequality; and in the last inequality we choose η ≤λ3/2β
L,0.6460807600950119,"6L
√ G,"
L,0.6472684085510689,which implies 2η2L2
L,0.6484560570071259,"λ2β
≤
λβ
16G ≤β 2 ≤αt"
L,0.649643705463183,"2 . Then by (23), we have"
L,0.6508313539192399,∥ϵt∥2 = ∥γt−1∥2 + 2αt
L,0.6520190023752969,"γt−1, ∇f(xt, ξt) −∇f(xt)

+ α2
t ∥∇f(xt, ξt) −∇f(xt)∥2"
L,0.6532066508313539,"≤

1 −αt 2"
L,0.6543942992874109,"
∥ϵt−1∥2 + λβ"
L,0.6555819477434679,"16G ∥∇f(xt−1)∥2 + α2
tσ2 + 2αt"
L,0.6567695961995249,"γt−1, ∇f(xt, ξt) −∇f(xt)

."
L,0.6579572446555819,Lemma E.10. Denote γt−1 = (1 −αt)(ϵt−1 + ∇f(xt−1) −∇f(xt)). If G ≥2σ and η ≤
L,0.6591448931116389,"min
n
r
D, σβ"
L,0.6603325415676959,"DL
o
, with probability 1 −δ, τ
X"
L,0.661520190023753,"t=2
αt"
L,0.66270783847981,"γt−1, ∇f(xt, ξt) −∇f(xt)

≤5σ2p"
L,0.663895486935867,(1 + β2T) log(1/δ).
L,0.665083135391924,"Proof of Lemma E.10. First note that τ
X"
L,0.666270783847981,"t=2
αt"
L,0.667458432304038,"γt−1, ∇f(xt, ξt) −∇f(xt)

= T
X"
L,0.668646080760095,"t=2
αt"
L,0.669833729216152,"γt−11τ≥t, ∇f(xt, ξt) −∇f(xt)

."
L,0.671021377672209,"Since τ is a stopping time, we know that 1τ≥t is a function of {ξs}s<t. Also, by definition, we know
γt−1 is a function of {ξs}s<t. Then, denoting"
L,0.672209026128266,Xt = αt
L,0.6733966745843231,"γt−11τ≥t, ∇f(xt, ξt) −∇f(xt)

,"
L,0.6745843230403801,"we know that Et−1[Xt] = 0, which implies {Xt}t≤T is a martingale difference sequence. Also, by
Assumption 3 and Lemma E.4, we can show that for all 2 ≤t ≤T,
|Xt| ≤αtσ ∥γt−11τ≥t∥≤2αtσ2.
Then by the Azuma-Hoeffding inequality (Lemma C.1), we have with probability at least 1 −δ, T
X"
L,0.6757719714964371,"t=2
Xt ≤2σ2"
L,0.6769596199524941,"v
u
u
t2 T
X"
L,0.6781472684085511,"t=2
α2
t log(1/δ) ≤5σ2p"
L,0.6793349168646081,"(1 + β2T) log(1/δ),"
L,0.6805225653206651,where in the last inequality we use Lemma E.3.
L,0.6817102137767221,Then we are ready to prove Lemma E.7.
L,0.6828978622327792,"Proof of Lemma E.7. By Lemma E.9, we have for every 2 ≤t ≤τ,
β"
L,0.684085510688836,2 ∥ϵt−1∥2 ≤αt
L,0.6852731591448931,2 ∥ϵt−1∥2 ≤∥ϵt−1∥2 −∥ϵt∥2 + λβ
L,0.6864608076009501,"16G ∥∇f(xt−1)∥2 + α2
tσ2 + 2αt"
L,0.6876484560570071,"γt−1, ∇f(xt, ξt) −∇f(xt)

."
L,0.6888361045130641,"Taking a summation over t from 2 to τ, we have
τ
X t=2 β"
L,0.6900237529691211,2 ∥ϵt−1∥2 −λβ
L,0.6912114014251781,"16G ∥∇f(xt−1)∥2 ≤∥ϵ1∥2 −∥ϵτ∥2 + σ2
τ
X"
L,0.6923990498812351,"t=2
α2
t + 10σ2p"
L,0.6935866983372921,(1 + β2T) log(1/δ)
L,0.6947743467933492,≤4σ2(1 + β2T) + 10σ2p
L,0.6959619952494062,"(1 + β2T) log(1/δ),"
L,0.6971496437054632,"where the first inequality uses Lemma E.10; and the second inequality uses Lemma E.3 and ∥ϵ1∥2 =
∥∇f(x1, ξ1) −∇f(x1)∥2 ≤σ2. Then we complete the proof by multiplying both sides by 2/β."
L,0.6983372921615202,"E.5
Omitted proofs for Adam"
L,0.6995249406175772,"In this section, we provide all the omitted proofs for Adam including those of Lemma 5.3 and all the
lemmas in Appendix E.2."
L,0.7007125890736342,"Proof of Lemma 5.3. According to Lemma E.2, if t < τ,"
L,0.7019002375296912,∥xt+1 −xt∥≤η
L,0.7030878859857482,λ ∥ˆmt∥≤η(G + σ)
L,0.7042755344418052,"λ
≤2ηG λ ."
L,0.7054631828978623,"Proof of Lemma E.2. By definition of τ, we have ∥∇f(xt)∥≤G if t < τ. Then Assumption 3
directly implies ∥∇f(xt, ξt)∥≤G + σ. ∥ˆmt∥can be bounded by a standard induction argument as
follows. First note that ∥ˆm1∥= ∥∇f(x1, ξ1)∥≤G + σ. Supposing ∥ˆmk−1∥≤G + σ for some
k < τ, then we have"
L,0.7066508313539193,"∥ˆmk∥≤(1 −αk) ∥ˆmk−1∥+ αk ∥∇f(xk, ξk)∥≤G + σ."
L,0.7078384798099763,"Then we can show ˆvt ⪯(G + σ)2 in a similar way noting that (∇f(xt, ξt))2 ⪯∥∇f(xt, ξt)∥2 ≤
(G + σ)2. Given the bound on ˆvt, it is straight forward to bound the stepsize ht."
L,0.7090261282660333,"Proof of Lemma E.3. First, when t ≥1/β, we have (1 −β)t ≤1/e. Therefore,
X"
L,0.7102137767220903,"1/β≤t≤T
(1 −(1 −β)t)−2 ≤(1 −1/e)−2T ≤3T."
L,0.7114014251781473,"Next, note that when t < 1/β, we have (1 −β)t ≤1 −1"
L,0.7125890736342043,"2βt. Then we have
X"
L,0.7137767220902613,"2≤t<1/β
(1 −(1 −β)t)−2 ≤4 β2
X"
L,0.7149643705463183,"t≥2
t−m ≤3 β2 ."
L,0.7161520190023754,"Therefore we have PT
t=2 α2
t ≤3(1 + β2T)."
L,0.7173396674584323,"Proof of Lemma E.4. We prove ∥ϵt∥≤2σ for all t ≤τ by induction. First, note that for t = 1, we
have
∥ϵ1∥= ∥∇f(x1, ξ1) −∇f(x1)∥≤σ ≤2σ.
Now suppose ∥ϵt−1∥≤2σ for some 2 ≤t ≤τ. According to the update rule (18), we have"
L,0.7185273159144893,"ϵt =(1 −αt)(ϵt−1 + ∇f(xt−1) −∇f(xt)) + αt(∇f(xt, ξt) −∇f(xt)),"
L,0.7197149643705463,which implies
L,0.7209026128266033,∥ϵt∥≤(2 −αt)σ + ∥∇f(xt−1) −∇f(xt)∥.
L,0.7220902612826603,"Since we choose η ≤
r
D, by Lemma 5.3, we have ∥xt −xt−1∥≤ηD ≤r if t ≤τ. Therefore by
Corollary 5.2, we have for any 2 ≤t ≤τ,"
L,0.7232779097387173,"∥∇f(xt) −∇f(xt−1)∥≤L ∥xt −xt−1∥≤ηDL ≤σαt,"
L,0.7244655581947743,"where the last inequality uses the choice of η and β ≤αt. Therefore we have ∥ϵt∥≤2σ which
completes the induction. Then it is straight forward to show"
L,0.7256532066508313,∥γt−1∥≤(1 −αt) (2σ + αtσ) ≤2σ.
L,0.7268408551068883,Proof of Lemma E.5. We first list all the related parameter choices below for convenience.
L,0.7280285035629454,"G ≥max
n
2λ, 2σ,
p"
L,0.7292161520190024,"C1∆1L0, (C1∆1Lρ)
1
2−ρ
o
,
β ≤min

1, c1λϵ2 σ2G√ι 
,"
L,0.7304038004750594,"η ≤c2 min
rλ"
L,0.7315914489311164,"G ,
σλβ
LG√ι, λ3/2β L
√ G"
L,0.7327790973871734,"
,
T = max
 1"
L,0.7339667458432304,"β2 , C2∆1G ηϵ2 
."
L,0.7351543942992874,"We will show I1/I2 ≤1 first. Note that if denoting W =
3L
λG2 , we have"
L,0.7363420427553444,"I1/I2 = W∆1λ + 8Wσ2
 η"
L,0.7375296912114014,"β + ηβT

+ 20Wσ2p"
L,0.7387173396674585,"(η2/β2 + η2T)ι,"
L,0.7399049881235155,Below are some facts that can be easily verified given the parameter choices.
L,0.7410926365795725,"(a) By the choice of G, we have G2 ≥6∆1(3L0 + 4LρGρ) = 6∆1L for large enough C1,
which implies W ≤
1
2∆1λ."
L,0.7422802850356295,"(b) By the choice of T, we have ηβT ≤η"
L,0.7434679334916865,"β + C2∆1Gβ ϵ2
."
L,0.7446555819477435,"(c) By the choice of T, we have η2T = max

η
β
2
, C2η∆1G ϵ2"
L,0.7458432304038005,"
≤

η
β
2
+ C2∆1σβ"
L,0.7470308788598575,"ϵ2
· η β ≤"
L,0.7482185273159145,"3
2

η
β
2
+ 1"
L,0.7494061757719715,"2

C2∆1σβ"
L,0.7505938242280285,"ϵ2
2
."
L,0.7517814726840855,"(d) By the choice of η, we have η/β ≤
c2σλ
LG√ι, which implies Wσ2√ι · η"
L,0.7529691211401425,β ≤3c2σ3
L,0.7541567695961995,"G3
≤
1
200 for
small enough c2."
L,0.7553444180522565,"(e) By the choice of β and (a), we have W σ2∆1G√ιβ"
L,0.7565320665083135,"ϵ2
≤σ2G√ιβ"
L,0.7577197149643705,"2λϵ2
≤
1
100C2 for small enough c1."
L,0.7589073634204275,"Therefore,"
L,0.7600950118764845,I1/I2 ≤1
L,0.7612826603325415,"2 + 8Wσ2
2η"
L,0.7624703087885986,β + C2∆1Gβ ϵ2
L,0.7636579572446556,"
+ 20Wσ2√ι   s 5η2"
L,0.7648456057007126,2β2 + 1 2
L,0.7660332541567696,"C2∆1σβ ϵ2 2
  ≤1"
L,0.7672209026128266,2 + 48Wσ2√ι · η
L,0.7684085510688836,"β + 24C2Wσ2∆1G√ιβ ϵ2 ≤1,"
L,0.7695961995249406,"where the first inequality is due to Facts (a-c); the second inequality uses σ ≤G, ι ≥1, and
√"
L,0.7707838479809976,"a + b ≤√a +
√"
L,0.7719714964370546,"b for a, b ≥0; and the last inequality is due to Facts (d-e)."
L,0.7731591448931117,"Next, we will show I1/T ≤ϵ2. We have"
L,0.7743467933491687,I1/T =8G∆1
L,0.7755344418052257,"ηT
+ 64σ2G"
L,0.7767220902612827,"λβT
+ 64σ2Gβ"
L,0.7779097387173397,"λ
+ 160σ2G√ι λ"
L,0.7790973871733967,"r
1
β2T 2 + 1 T ≤8ϵ2"
L,0.7802850356294537,"C2
+ 224σ2G√ι"
L,0.7814726840855107,"λβT
+ 64σ2Gβ"
L,0.7826603325415677,"λ
+ 160σ2G√ι λ
√ T ≤8ϵ2"
L,0.7838479809976246,"C2
+ 450σ2G√ιβ λ =
 8"
L,0.7850356294536817,"C2
+ 450c1 
ϵ2 ≤ϵ2,"
L,0.7862232779097387,where in the first inequality we use T ≥C2∆1G
L,0.7874109263657957,"ηϵ2
and
√"
L,0.7885985748218527,"a + b ≤√a +
√"
L,0.7897862232779097,"b for a, b ≥0; the second
inequality uses T ≥
1
β2 ; the second equality uses the parameter choice of β; and in the last inequality
we choose a large enough C2 and small enough c1."
L,0.7909738717339667,"E.6
Proof of Theorem 4.2"
L,0.7921615201900237,Proof of Theorem 4.2. We define stopping time τ as follows
L,0.7933491686460807,"τ1 := min{t | f(xt) −f ∗> F} ∧(T + 1),
τ2 := min{t | ∥∇f(xt) −∇f(xt, ξt)∥> σ} ∧(T + 1),
τ := min{τ1, τ2}."
L,0.7945368171021377,"Then it is straightforward to verify that τ1, τ2, τ are all stopping times."
L,0.7957244655581948,"Since we want to show P(τ ≤T) is small, noting that {τ ≤T} = {τ = τ1 ≤T} ∪{τ = τ2 ≤T},
it suffices to bound both P(τ = τ1 ≤T) and P(τ = τ2 ≤T)."
L,0.7969121140142518,"First, we know that"
L,0.7980997624703088,P(τ = τ2 ≤T) ≤P(τ2 ≤T) =P  [
L,0.7992874109263658,"1≤t≤T
∥∇f(xt) −∇f(xt, ξt)∥> σ   ≤
X"
L,0.8004750593824228,"1≤t≤T
P (∥∇f(xt) −∇f(xt, ξt)∥> σ) ≤
X"
L,0.8016627078384798,"1≤t≤T
E [Pt−1 (∥∇f(xt) −∇f(xt, ξt)∥> σ)] ≤
X"
L,0.8028503562945368,"1≤t≤T
E
h
2e−σ2 2R2
i"
L,0.8040380047505938,"=2Te−σ2 2R2 ≤δ/2,"
L,0.8052256532066508,"where the fourth inequality uses Assumption 4; and the last inequality uses σ = R
p"
L,0.8064133016627079,2 log(4T/δ).
L,0.8076009501187649,"Next, if τ = τ1 ≤T, by definition, we have f(xτ) −f ∗> F, or equivalently,"
G,0.8087885985748219,8G
G,0.8099762470308789,η (f(xτ) −f ∗) > 8GF
G,0.8111638954869359,"η
= 8G3"
G,0.8123515439429929,3ηL =: I2.
G,0.8135391923990499,"On the other hand, since for any t < τ, under the new definition of τ, we still have"
G,0.8147268408551069,"f(xt) −f ∗≤F,
∥f(xt)∥≤G,
∥∇f(xt) −∇f(xt, ξt)∥≤σ."
G,0.815914489311164,"Then we know that Lemma E.8 still holds because all of its requirements are still satisfied, i.e., there
exists some event E with P(E) ≤δ/2, such that under its complement Ec, τ−1
X"
G,0.8171021377672208,"t=1
∥∇f(xt)∥2 + 8G"
G,0.8182897862232779,η (f(xτ) −f ∗) ≤8G ηλ
G,0.8194774346793349,"
∆1λ + 8σ2
 η"
G,0.8206650831353919,"β + ηβT

+ 20σ2η
p"
G,0.8218527315914489,"(1/β2 + T)ι
"
G,0.8230403800475059,=: I1.
G,0.8242280285035629,"By Lemma E.5, we know I1 ≤I2, which suggests that Ec ∩{τ = τ1 ≤T} = ∅, i.e., {τ = τ1 ≤
T} ⊂E. Then we can show"
G,0.8254156769596199,P(E ∪{τ ≤T}) ≤P(E) + P(τ = τ2 ≤T) ≤δ.
G,0.8266033254156769,"Therefore,"
G,0.827790973871734,"P(Ec ∩{τ = T + 1}) ≥1 −P(E ∪{τ ≤T}) ≥1 −δ,"
G,0.828978622327791,"and under the event Ec ∩{τ = T + 1}, we have τ = T + 1 and"
T,0.830166270783848,"1
T t
X"
T,0.831353919239905,"t=1
∥∇f(xt)∥2 ≤I1/T ≤ϵ2,"
T,0.832541567695962,where the last inequality is due to Lemma E.5.
T,0.833729216152019,"F
Convergence analysis of VRAdam"
T,0.834916864608076,"In this section, we provide detailed convergence analysis of VRAdam and prove Theorem 6.2. To do
that, we first provide some technical definitions4. Denote"
T,0.836104513064133,ϵt :=mt −∇f(xt)
T,0.83729216152019,4Note that the same symbol for Adam and VRAdam may have different meanings.
T,0.838479809976247,"as the deviation of the momentum from the actual gradient. From the update rule in Algorithm 2, we
can write
ϵt = (1 −β)ϵt−1 + Wt,
(25)
where we define
Wt :=∇f(xt, ξt) −∇f(xt) −(1 −β) (∇f(xt−1, ξt) −∇f(xt−1)) ."
T,0.8396674584323041,"Let G be the constant defined in Theorem 6.2 and denote F :=
G2
3(3L0+4LρGρ). We define the
following stopping times as discussed in Section 6.1.
τ1 := min{t | f(xt) −f ∗> F} ∧(T + 1),
τ2 := min{t | ∥ϵt∥> G} ∧(T + 1),
(26)
τ := min{τ1, τ2}.
It is straight forward to verify that τ1, τ2, τ are all stopping times. Then if t < τ, we have
f(xt) −f ∗≤F,
∥∇f(xt)∥≤G,
∥ϵt∥≤G.
Then we can also bound the update ∥xt+1 −xt∥≤ηD where D = 2G/λ if t < τ (see Lemma F.3
for the details). Finally, we consider the same definition of r and L as those for Adam. Specifically,"
T,0.8408551068883611,r := min
T,0.8420427553444181,"(
1
5LρGρ−1 ,
1
5(Lρ−1
0
Lρ)1/ρ )"
T,0.8432304038004751,",
L := 3L0 + 4LρGρ.
(27)"
T,0.8444180522565321,"F.1
Useful lemmas"
T,0.8456057007125891,"We first list several useful lemmas in this section without proofs. Their proofs are deferred later in
Appendix F.3."
T,0.8467933491686461,"To start with, we provide a lemma on the local smoothness of each component function f(·, ξ) when
the gradient of the objective function f is bounded.
Lemma F.1. For any constant G ≥σ and two points x ∈dom(f), y ∈Rd such that ∥∇f(x)∥≤G
and ∥y −x∥≤r/2, we have y ∈dom(f) and
∥∇f(y) −∇f(x)∥≤L ∥y −x∥,
∥∇f(y, ξ) −∇f(x, ξ)∥≤4L ∥y −x∥, ∀ξ,"
T,0.8479809976247031,"f(y) ≤f(x) +

∇f(x), y −x

+ 1"
T,0.8491686460807601,"2L ∥y −x∥2 ,"
T,0.850356294536817,where r and L are defined in (27).
T,0.8515439429928741,"With the new definition of stopping time τ in (26), all the quantities in Algorithm 2 are well bounded
before τ. In particular, the following lemma holds.
Lemma F.2. If t < τ, we have
∥∇f(xt)∥≤G,
∥∇f(xt, ξt)∥≤G + σ,
∥mt∥≤2G,"
T,0.8527315914489311,"ˆvt ⪯(G + σ)2,
η
G + σ + λ ⪯ht ⪯η λ."
T,0.8539192399049881,"Next, we provide the following lemma which bounds the update at each step before τ.
Lemma F.3. if t < τ, ∥xt+1 −xt∥≤ηD where D = 2G/λ."
T,0.8551068883610451,"The following lemma bounds ∥Wt∥when t ≤τ.
Lemma F.4. If t ≤τ, G ≥2σ, and η ≤
r
2D,"
T,0.8562945368171021,∥Wt∥≤βσ + 5ηL
T,0.8574821852731591,"λ
(∥∇f(xt−1)∥+ ∥ϵt−1∥) ."
T,0.8586698337292161,"Finally, we present some inequalities regarding the parameter choices, which will simplify the
calculations later.
Lemma F.5. Under the parameter choices in Theorem 6.2, we have 2∆1 F
≤δ"
T,0.8598574821852731,"4,
λ∆1β"
T,0.8610451306413301,"ηG2
≤δ"
T,0.8622327790973872,"4,
ηβT ≤λ∆1"
T,0.8634204275534442,"8σ2 ,
η ≤λ3/2"
L,0.8646080760095012,"40L r β
G."
L,0.8657957244655582,"F.2
Proof of Theorem 6.2"
L,0.8669833729216152,"Before proving the theorem, we will need to present several important lemmas. First, note that the
descent lemma still holds for VRAdam.
Lemma F.6. If t < τ, choosing G ≥σ + λ and η ≤min
 r 2D, λ"
L,0.8681710213776722,"6L
	
, we have"
L,0.8693586698337292,f(xt+1) −f(xt) ≤−η
L,0.8705463182897862,4G ∥∇f(xt)∥2 + η
L,0.8717339667458432,λ ∥ϵt∥2 .
L,0.8729216152019003,Proof of Lemma F.6. The proof is essentially the same as that of Lemma E.6.
L,0.8741092636579573,"Lemma F.7. Choose G ≥max {2σ, 2λ}, S1 ≥
1
2β2T , and η ≤min

r
2D, λ3/2"
L,0.8752969121140143,"40L q β
G"
L,0.8764845605700713,"
. We have E"
L,0.8776722090261283,"""τ−1
X t=1 β"
L,0.8788598574821853,2 ∥ϵt∥2 −λβ
L,0.8800475059382423,"16G ∥∇f(xt)∥2
#"
L,0.8812351543942993,≤4σ2β2T −E[∥ϵτ∥2].
L,0.8824228028503563,"Proof of Lemma F.7. By Lemma F.4, we have"
L,0.8836104513064132,∥Wt∥2 ≤2σ2β2 + 100η2L2 λ2
L,0.8847980997624703,"
∥∇f(xt−1)∥2 + ∥ϵt−1∥2"
L,0.8859857482185273,≤2σ2β2 + λβ
G,0.8871733966745843,16G
G,0.8883610451306413,"
∥∇f(xt−1)∥2 + ∥ϵt−1∥2
,"
G,0.8895486935866983,where in the second inequality we choose η ≤λ3/2
L,0.8907363420427553,40L q
L,0.8919239904988123,"β
G. Therefore, noting that
λβ
16G ≤β/2, by (25),
we have"
L,0.8931116389548693,"∥ϵt∥2 =(1 −β)2 ∥ϵt−1∥2 + ∥Wt∥2 + (1 −β)

ϵt−1, Wt "
L,0.8942992874109263,≤(1 −β/2) ∥ϵt−1∥2 + λβ
L,0.8954869358669834,"16G ∥∇f(xt−1)∥2 + 2σ2β2 + (1 −β)

ϵt−1, Wt ."
L,0.8966745843230404,"Taking a summation over 2 ≤t ≤τ and re-arranging the terms, we get τ−1
X t=1 β"
L,0.8978622327790974,2 ∥ϵt∥2 −λβ
L,0.8990498812351544,"16G ∥∇f(xt)∥2 ≤∥ϵ1∥2 −∥ϵτ∥2 + 2σ2β2(τ −1) + (1 −β) τ
X t=2"
L,0.9002375296912114,"ϵt−1, Wt ."
L,0.9014251781472684,"Taking expectations on both sides, noting that E "" τ
X t=2"
L,0.9026128266033254,"ϵt−1, Wt # = 0"
L,0.9038004750593824,"by the Optional Stopping Theorem (Lemma C.2), we have E"
L,0.9049881235154394,"""τ−1
X t=1 β"
L,0.9061757719714965,2 ∥ϵt∥2 −λβ
L,0.9073634204275535,"16G ∥∇f(xt)∥2
#"
L,0.9085510688836105,"≤2σ2β2T + E[∥ϵ1∥2] −E[∥ϵτ∥2] ≤4σ2β2T −E[∥ϵτ∥2],"
L,0.9097387173396675,"where in the second inequality we choose S1 ≥
1
2β2T which implies E[∥ϵ1∥2] ≤σ2/S1 ≤2σ2β2T."
L,0.9109263657957245,"Lemma F.8. Under the parameter choices in Theorem 6.2, we have E"
L,0.9121140142517815,"""τ−1
X"
L,0.9133016627078385,"t=1
∥∇f(xt)∥2
#"
L,0.9144893111638955,≤16G∆1
L,0.9156769596199525,"η
,
E[f(xτ) −f ∗] ≤2∆1,
E[∥ϵτ∥2] ≤λ∆1β η
."
L,0.9168646080760094,"Proof of Lemma F.8. First note that according to Lemma F.5, it is straight forward to verify that the
parameter choices in Theorem 6.2 satisfy the requirements in Lemma F.6 and Lemma F.7. Then by
Lemma F.6, if t < τ,"
L,0.9180522565320665,f(xt+1) −f(xt) ≤−η
L,0.9192399049881235,4G ∥∇f(xt)∥2 + η
L,0.9204275534441805,λ ∥ϵt∥2 .
L,0.9216152019002375,"Taking a summation over 1 ≤t < τ, re-arranging terms, multiplying both sides by 8G"
L,0.9228028503562945,"η , and taking an
expection, we get E"
L,0.9239904988123515,"""τ−1
X"
L,0.9251781472684085,"t=1
2 ∥∇f(xt)∥2 −8G"
L,0.9263657957244655,"λ ∥ϵt∥2
# ≤8G"
L,0.9275534441805225,η E[f(x1) −f(xτ)] ≤8G
L,0.9287410926365796,η (∆1 −E[f(xτ) −f ∗]) . (28)
L,0.9299287410926366,"By Lemma F.7, we have E"
L,0.9311163895486936,"""τ−1
X t=1"
G,0.9323040380047506,8G
G,0.9334916864608076,"λ ∥ϵt∥2 −∥∇f(xt)∥2
#"
G,0.9346793349168646,≤64Gσ2βT
G,0.9358669833729216,"λ
−16G"
G,0.9370546318289786,λβ E[∥ϵτ∥2] ≤8G∆1
G,0.9382422802850356,"η
−16G"
G,0.9394299287410927,"λβ E[∥ϵτ∥2], (29)"
G,0.9406175771971497,where the last inequality is due to Lemma F.5. Then (28) + (29) gives E
G,0.9418052256532067,"""τ−1
X"
G,0.9429928741092637,"t=1
∥∇f(xt)∥2
# + 8G"
G,0.9441805225653207,η E[f(xτ) −f ∗] + 16G
G,0.9453681710213777,"λβ E[∥ϵτ∥2] ≤16G∆1 η
,"
G,0.9465558194774347,which completes the proof.
G,0.9477434679334917,"With all the above lemmas, we are ready to prove the theorem."
G,0.9489311163895487,"Proof of Theorem 6.2. First note that according to Lemma F.5, it is straight forward to verify that the
parameter choices in Theorem 6.2 satisfy the requirements in all the lemmas for VRAdam."
G,0.9501187648456056,"Then, first note that if τ = τ1 ≤T, we know f(xτ) −f ∗> F by the definition of τ. Therefore,"
G,0.9513064133016627,P(τ = τ1 ≤T) ≤P(f(xτ) −f ∗> F) ≤E[f(xτ) −f ∗]
G,0.9524940617577197,"F
≤2∆1 F
≤δ 4,"
G,0.9536817102137767,"where the second inequality uses Markov’s inequality; the third inequality is by Lemma F.8; and the
last inequality is due to Lemma F.5."
G,0.9548693586698337,"Similarly, if τ2 = τ ≤T, we know ∥ϵτ∥> G. We have"
G,0.9560570071258907,P(τ2 = τ ≤T) ≤P(∥ϵτ∥> G) = P(∥ϵτ∥2 > G2) ≤E[∥ϵτ∥2]
G,0.9572446555819477,"G2
≤λ∆1β"
G,0.9584323040380047,"ηG2
≤δ 4,"
G,0.9596199524940617,"where the second inequality uses Markov’s inequliaty; the third inequality is by Lemma F.8; and the
last inequality is due to Lemma F.5. where the last inequality is due to Lemma F.5. Therefore,"
G,0.9608076009501187,P(τ ≤T) ≤P(τ1 = τ ≤T) + P(τ2 = τ ≤T) ≤δ 2.
G,0.9619952494061758,"Also, note that by Lemma F.8 16G∆1 η
≥E"
G,0.9631828978622328,"""τ−1
X"
G,0.9643705463182898,"t=1
∥∇f(xt)∥2
#"
G,0.9655581947743468,"≥P(τ = T + 1)E "" T
X"
G,0.9667458432304038,"t=1
∥∇f(xt)∥2
 τ = T + 1 # ≥1"
E,0.9679334916864608,"2E "" T
X"
E,0.9691211401425178,"t=1
∥∇f(xt)∥2
 τ = T + 1 # ,"
E,0.9703087885985748,"where the last inequality is due to P(τ = T + 1) = 1 −P(τ ≤T) ≥1 −δ/2 ≥1/2. Then we can
get E ""
1
T T
X"
E,0.9714964370546318,"t=1
∥∇f(xt)∥2
 τ = T + 1 #"
E,0.9726840855106889,≤32G∆1
E,0.9738717339667459,"ηT
≤δϵ2 2 ."
E,0.9750593824228029,"Let F :=
n
1
T
PT
t=1 ∥∇f(xt)∥2 > ϵ2o
be the event of not converging to stationary points. By
Markov’s inequality, we have"
E,0.9762470308788599,P(F|τ = T + 1) ≤δ 2.
E,0.9774346793349169,"Therefore,
P(F ∪{τ ≤T}) ≤P(τ ≤T) + P(F|τ = T + 1) ≤δ,"
E,0.9786223277909739,"i.e., with probability at least 1 −δ, we have both τ = T + 1 and 1"
E,0.9798099762470309,"T
PT
t=1 ∥∇f(xt)∥2 ≤ϵ2."
E,0.9809976247030879,"F.3
Proofs of lemmas in Appendix F.1"
E,0.982185273159145,"Proof of Lemma F.1. This lemma is a direct corollary of Corollary 5.2. Note that by Assumption 6,
we have ∥∇f(x, ξ)∥≤G + σ ≤2G. Hence, when computing the locality size and smoothness
constant for the component function f(·, ξ), we need to replace the constant G in Corollary 5.2 with
2G, that is why we get a smaller locality size of r/2 and a larger smoothness constant of 4L."
E,0.9833729216152018,"Proof of Lemma F.2. The bound on ∥mt∥is by the definition of τ in (26). All other quantities for
VRAdam are defined in the same way as those in Adam (Algorithm 1), so they have the same upper
bounds as in Lemma E.2."
E,0.9845605700712589,Proof of Lemma F.3.
E,0.9857482185273159,"∥xt+1 −xt∥≤η ∥mt∥/λ ≤2ηG/λ = ηD,"
E,0.9869358669833729,"where the first inequality uses the update rule in Algorithm 2 and ht ⪯η/λ by Lemma F.2; the
second inequality is again due to Lemma F.2."
E,0.9881235154394299,"Proof of Lemma F.4. By the definition of Wt, it is easy to verify that"
E,0.9893111638954869,"Wt = β(∇f(xt, ξt) −∇f(xt)) + (1 −β)δt, where"
E,0.9904988123515439,"δt = ∇f(xt, ξt) −∇f(xt−1, ξt) −∇f(xt) + ∇f(xt−1)."
E,0.9916864608076009,Then we can bound
E,0.9928741092636579,"∥δt∥≤∥∇f(xt, ξt) −∇f(xt−1, ξt)∥+ ∥∇f(xt) −∇f(xt−1)∥
≤5L ∥xt −xt−1∥ ≤5ηL"
E,0.994061757719715,"λ
(∥∇f(xt−1)∥+ ∥ϵt−1∥) ,"
E,0.995249406175772,"where the second inequality uses Lemma F.1; and the last inequality is due to ∥xt −xt−1∥≤
η ∥mt−1∥/λ ≤η (∥∇f(xt−1)∥+ ∥ϵt−1∥) /λ. Then, we have"
E,0.996437054631829,∥Wt∥≤βσ + 5ηL
E,0.997624703087886,"λ
(∥∇f(xt−1)∥+ ∥ϵt−1∥) ."
E,0.998812351543943,Proof of Lemma F.5. These inequalities can be obtained by direct calculations.
