Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0012547051442910915,"We study the controllability of large-scale networked dynamical systems when
complete knowledge of network structure is unavailable. In particular, we establish
the power of learning community-based representations to understand the ability
of a group of control nodes to steer the network to a target state. We are moti-
vated by abundant real-world examples, ranging from power and water systems to
brain networks, in which practitioners do not have access to ﬁne-scale knowledge
of the network. Rather, knowledge is limited to coarse summaries of network
structure. Existing work on ""model order reduction"" starts with full knowledge
of ﬁne-scale structure and derives a coarse-scale (lower-dimensional) model that
well-approximates the ﬁne-scale system. In contrast, in this paper the controlla-
bility aspects of the coarse system are derived from coarse summaries without
knowledge of the ﬁne-scale structure. We study under what conditions measures of
controllability for the (unobserved) ﬁne-scale system can be well approximated by
measures of controllability derived from the (observed) coarse-scale system. To
accomplish this, we require knowledge of some inherent parametric structure of the
ﬁne-scale system that makes this type of inverse problem feasible. To this end, we
assume that the underlying ﬁne-scale network is generated by the stochastic block
model (SBM) often studied in community detection. We quantify controllability
using the “average controllability” metric and bound the difference between the
controllability of the ﬁne-scale system and that of the coarse-scale system. Our
analysis indicates the necessity of underlying structure to make possible the learn-
ing of community-based representations, and to be able to quantify accurately the
controllability of coarsely characterized networked dynamical systems."
INTRODUCTION,0.002509410288582183,"1
INTRODUCTION"
INTRODUCTION,0.0037641154328732747,"In this paper we study controllability for networked dynamical systems when our knowledge of
system structure is limited to coarse summaries. We are motivated by myriad real-world settings
where system identiﬁcation must be performed based upon measurements taken by low-resolution
instruments unable to probe ﬁne-scale structure. Our motivating example is the human brain. While
efforts are under way to produce a canonical human brain map, our knowledge of the brain as an
interconnected, network system is not yet to the level of the whole-brain individual neuron (Betzel
and Bassett, 2017). And yet, motivated by emerging medical technologies, there are important control
tasks we would like to tackle. For example, novel brain implants designed for epilepsy patients aim
to “steer” the brain away from states that correspond to seizures (Heck et al., 2014; Muldoon et al.,
2016). Our goal is to quantify the controllability of a ﬁne-scale networked dynamical system given
access only to coarse knowledge of network structure. Generally, without parametric structure, this
is impossible. But real networks do have structure and so in our model we assume the ﬁne-scale
network has a connectivity induced by an underlying stochastic block model (SBM)."
INTRODUCTION,0.005018820577164366,"Approximation of high-dimensional (ﬁne-scale) dynamical systems by lower-dimensional (coarse-
scale) ones is known as “model order reduction” (MOR) in the controls literature. There is a key
difference in assumptions that differentiate our setting from that literature. In MOR the starting point
is a complete description of the high-dimensional system. The task is to formulate a lower-dimension
system, the dynamics of which well-approximate those of the full system. In contrast, we start from
coarse summaries of the ﬁne-scale system. We do not have access to the ﬁne-scale dynamics and
must exploit parametric knowledge (via the assumption of a generative SBM). One might think of the"
INTRODUCTION,0.006273525721455458,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.0075282308657465494,"distinction as akin to “active” versus “passive” MOR. Traditional MOR is active in that it actively
decides how to coarsen the system to yield the best reduction. But for us, our knowledge is limited by
the precision of our instrumental observations, so passively collected data is our starting point."
INTRODUCTION,0.00878293601003764,"Controllability is a function both of system dynamics and how we actuate the system (Pasqualetti
et al., 2014; Yuan et al., 2013). Herein we assume that we both measure and actuate a system only
coarsely. A control question we study is which coarse-level actuations are most “inﬂuential” in
controlling the underlying ﬁne-scale system. Such knowledge can assist with actuation selection;
e.g., in our motivating epilepsy application, where best to position devices to be able to collapse the
unstable brain-state oscillations that lead to seizures. (O’Leary et al., 2018; Pazhouhandeh et al.,
2019; Kassiri et al., 2017; Shulyzki et al., 2015) To accomplish our goal we characterize the average
controllability of a vector of systems, each corresponding to a different coarse-scale actuation input.
By comparing these vectors, and because these vectors well approximate the corresponding vectors
for the ﬁne-scale system, we aim (in the long term) to produce clinically-usable information for the
neurologist."
INTRODUCTION,0.010037641154328732,"Contribution: Our work is the ﬁrst of its kind that proposes a learning-based framework for inferring
the controllability of ﬁne networks from coarse measurements, and characterizes the mismatch
between the controllability of the coarse and ﬁne-scale networks. We study two approaches."
INTRODUCTION,0.011292346298619825,"1. In Section 5, we build from MOR. We deﬁne an auxiliary, ﬁctitious, reduced-order system
based on the coarse data, and use the average controllability vector of this system to approx-
imate that of the ﬁne-scale system. We derive a tight upper bound on the “approximation-
error” which is the sum of two terms. One term goes to zero as the coarse network size
increases and the network becomes dense. The second term is a function of the synchro-
nization between the coarse summary data and the underlying community structure. If
synchronization is not sufﬁciently high, this term may not approach zero even as the network
size increases.
2. In Section 6 we learn the ﬁne-scale system’s average controllability vector directly from the
coarse data. This learning-based algorithm builds on the mixed-membership algorithm of
Mao et al. (2017) for unsupervised learning of the parameters and the community structure of
a SBM. We derive a tight upper bound on estimation error and characterize its convergence.
Although the error bound implicitly depends on synchronization, unlike in the MOR-based
approach, the error of this approach converges to zero as the coarse network size and its
density increases."
INTRODUCTION,0.012547051442910916,"2
BACKGROUND / RELATED WORK"
INTRODUCTION,0.013801756587202008,"Coarsened SBM as a generative process: The study of extracting community structure from coarse
summaries is recent. The authors in (Ghoroghchian et al., 2021) used the stochastic block model
(SBM), developed in the community detection literature (Abbe, 2017), to lay out a framework for a
coarsened and weighted variant of the SBM. We build off those results in this paper. The structure of
many real-world networks, including brain networks is, at least empirically, known to have community
structure across various spatial scales (Sporns and Betzel, 2016; Pavlovi´c et al., 2020). The SBM and
its variants provide a powerful modeling framework to facilitate fundamental understanding of graph
community organization and have found applications in many domains, including social and power
networks. (Dulac et al., 2020; Funke and Becker, 2019; Abbe, 2017)."
INTRODUCTION,0.015056461731493099,"Complex networks controllability: The development of control methods for complex networks is a
major effort in network science (Scheid et al., 2020). Coupling traditional notions of controllability
with graph theory reveals several insights into the role of network structure (e.g., presence of
communities, diameter, and sparsity), size, and edge weight strength in controlling large-scale
networks (Wu-Yan et al., 2018; Kim et al., 2018; Constantino et al., 2019; Sun, 2015). Further one
may want to understand which group of nodes, when actuated as inputs, can be used to steer the
network to an arbitrary target state, and at what cost (Cortesi et al., 2014; Gu et al., 2015). Recent
works in network neuroscience Gu et al. (2015) have popularized the notion of average controllability.
This scalar metric associate a measure the relative control inﬂuence of a group of nodes. In this paper
we consider a vector of such scalar measures to study the comparative inﬂuence of different sets of
nodes. To the best of our knowledge ours is the ﬁrst work that characterizes this type of error bounds
for the controllability of coarse graphs."
INTRODUCTION,0.01631116687578419,Under review as a conference paper at ICLR 2022
PRELIMINARY NOTIONS,0.01756587202007528,"3
PRELIMINARY NOTIONS"
PRELIMINARY NOTIONS,0.018820577164366373,"Notation: We denote vectors and matrices using bold faced small and upper case letters. The n
dimensional all-ones and -zero vectors are denoted by 1n and 0n. For M = [Muv] ∈Rn×m, deﬁne
∥M∥∞= max1≤u≤n
Pm
v=1 |Muv|; ∥M∥max = maxu,v |Muv|; and ∥M∥2 =
p"
PRELIMINARY NOTIONS,0.020075282308657464,"λmax(MTM). Let
m = n, then deﬁne the spectral radius by ρ(M) = maxi{|λi|}; diag(M) = [M11, . . . , Mnn]T ∈
Rn; and Diag(M) sets the off-diagonal entries of M to zero. For matrices M′
is with arbitrary
dimensions, BlkDiag(M1, . . . , Md) denotes the block diagonal matrix. The inequality M1 ≤M2
implies element wise inequality. We write f(n) = O(h(n)) iff there exist positive reals c0 and n0
such that |f(n)| ≤c0h(n) for all n ≥n0. The support of a vector, supp(m), is the set of indices i
such that mi ̸= 0. The cardinality of a set V is denoted by |V|. For a positive integer m, we denote
[m] ≜{1, . . . , m}. 1(m) returns a vector of same size with non-zero replaced by 1."
PRELIMINARY NOTIONS,0.02132998745294856,"Networks: A network is deﬁned by an un-directed graph G ≜(V, E), where the node set V ≜
{1, . . . , n} and edge set E ⊆V × V. For an edge (u, v) ∈E, assign the weight Auv = Avu ∈R,
and deﬁne the weighted symmetric adjacency matrix of G as A ≜[Auv], where Auv = Auv = 0
whenever Auv /∈E. A random network is an un-directed graph with a random adjacency matrix."
LINEAR DYNAMICAL SYSTEM ON RANDOM NETWORK,0.02258469259723965,"3.1
LINEAR DYNAMICAL SYSTEM ON RANDOM NETWORK"
LINEAR DYNAMICAL SYSTEM ON RANDOM NETWORK,0.02383939774153074,"For a network G with n nodes and the symmetric adjacency matrix A, associate a state xi[k] ∈R to
the i-th node, and let the nodes evolve with the linear and time-invariant (LTI) dynamics 1:"
LINEAR DYNAMICAL SYSTEM ON RANDOM NETWORK,0.025094102885821833,"x[t + 1] =
1
c · tr(A)Ax[t] + Bu[t],
∀t = 0, 1, . . . .
(1)"
LINEAR DYNAMICAL SYSTEM ON RANDOM NETWORK,0.026348808030112924,"The state x[t] = [x1[t], . . . , xn[t]]T is steered to an arbitrary value by an input u[t] ∈Rn. Here, the
input matrix B = Diag(b) ∈Rn×n, where b ∈{0, 1}n determines which components of u[t] enters
the network2. For e.g., for B = Diag(1n1, 0n−n1), the input enters the network through control
nodes set K = {1, . . . , n1}. The normalization c · tr(A) factor, with appropriately chosen constant
c > 0, ensures that system in Eq. 1 is asymptotically stable. Finally, we deﬁne Anom ≜
1
c·tr(A)A for
the normalized matrix, and use this convention throughout the paper."
LINEAR DYNAMICAL SYSTEM ON RANDOM NETWORK,0.027603513174404015,"For ﬁxed system matrix Anom, a necessary and sufﬁcient condition for the asymptotic stability3 of
Eq. 1 is that ρ(Anom) ≤1. For random Anom, we consider the probabilistic stability: P[ρ(Anom) ≤
1]—the greater the value, the greater the chance that Anom is stable. For SBM generated random
symmetric matrices, we provide sharp non-asymptotic lower bounds on P[ρ(Anom) ≤1]."
LINEAR DYNAMICAL SYSTEM ON RANDOM NETWORK,0.028858218318695106,"The networked LTI system in Eq. 1 is T-step controllable if x[0] = 0 can be steered to any target
state x ∈Rn for some inputs: u[0], . . . , u[T −1]. The T-step controllability Gramian of Eq. 1 given
below, among other things, allows us to study if Eq. 1 is controllable or not."
LINEAR DYNAMICAL SYSTEM ON RANDOM NETWORK,0.030112923462986198,"CT (Anom, B) = PT −1
t=0 (Anom)tBBT(Anom)t.
(2)"
LINEAR DYNAMICAL SYSTEM ON RANDOM NETWORK,0.03136762860727729,"By deﬁnition CT (Anom, B) ⪰0, and it is well known that G with n nodes is T-step controllable if
n-step controllable; or equivalently, CT (Anom, B) ≻0. For other interesting properties of Eq. 2 we
refer to (Chen, 1999). For the simplicity of exposition, we let T →∞and consider the inﬁnite time
horizon Gramian: C(Anom, B) = limT →∞CT (Anom, B), which exists with 1 −P[ρ(Anom) ≥1];
see also Pasqualetti et al. (2014). We drop the notation (Anom, B) in C when the context is clear."
LINEAR DYNAMICAL SYSTEM ON RANDOM NETWORK,0.03262233375156838,"Average energy: A widely used metric to measure how hard or easy it is to control the network is
average energy:
R"
LINEAR DYNAMICAL SYSTEM ON RANDOM NETWORK,0.033877038895859475,"∥x∥2=1 xTC†x dx/
R"
LINEAR DYNAMICAL SYSTEM ON RANDOM NETWORK,0.03513174404015056,"∥x∥2=1 dx, which evaluates to n−1tr(C†) (Cortesi et al., 2014)."
LINEAR DYNAMICAL SYSTEM ON RANDOM NETWORK,0.03638644918444166,"Here, where C† is the pseudo inverse, and xTC†x is the minimum control energy needed to steer
x[0] = 0 to an arbitrary target state x ∈Rn. Thus, average energy measures the minimum control
energy required to steer x[0] = 0 to an arbitrary state uniformly distributed over the unit sphere."
LINEAR DYNAMICAL SYSTEM ON RANDOM NETWORK,0.037641154328732745,"1 One may think our LTI model as the linearized system of an underlying non-linear system. Controllability
of non-linear systems require a case by case analysis and we leave this topic for future research.
2Alternatively, Bu[t] = BKuK[t], where BK is the sub-matrix of B whose columns are indexed by K ⊂[n].
However, we stick with notation in Eq. 1 to make our analysis less cumbersome.
3The LTI system Eq. 1 is asymptotically stable if ∥x[t]∥2 →0 as t →∞, for u[t] = 0 and x[0] ̸= 0."
LINEAR DYNAMICAL SYSTEM ON RANDOM NETWORK,0.03889585947302384,Under review as a conference paper at ICLR 2022
LINEAR DYNAMICAL SYSTEM ON RANDOM NETWORK,0.04015056461731493,"Average controllability: Numerical computation of C† for large-scale networks is demanding. Owing
to the fact that tr(C†) ≥1/tr(C), one uses tr(C)—called the average controllability—as a proxy for
average energy Gu et al. (2015). The higher the average controllability is for a given set of control
nodes deﬁned by B, the smaller their average energy, thus higher their inﬂuence on the network."
STOCHASTIC BLOCK MODELS,0.04140526976160602,"3.2
STOCHASTIC BLOCK MODELS"
STOCHASTIC BLOCK MODELS,0.04265997490589712,"Stochastic block models (SBMs) are probabilistic models that produce random graphs with planted
communities. Formally, let Gﬁne ≜(V, E) be the un-directed graph (also referred as ﬁne graph) with
n nodes and random edge weights generated according to the general SBM(n, Q, p):
Deﬁnition 1. (General SBM) In the general SBM(n, Q, p), the graph Gﬁne is partitioned to K
disjoint sub-graphs (or communities) of relative sizes p = [p1, . . . , pK] such that V = ∪K
k=1Vk. Two
nodes u ∈Vk and v ∈Vk′ are joined by an edge with the weight Auv ∈{0, 1}, which is drawn with
probability Qkk′ independently from other edges, for all k, k′ ∈[K]."
STOCHASTIC BLOCK MODELS,0.043914680050188205,"In General SBM, the probability distribution of weights Auv is common for all u ∈Vk and u ∈Vk′.
The general SBM(n, Q, p) thus generates a weighted symmetric graph with K communities with
non-identical in- and cross-edge connection probabilities given by Q ∈[0, 1]K×K. Alternatively,"
STOCHASTIC BLOCK MODELS,0.0451693851944793,"Auv ∼Bernoulli(Qk,k′)
if k, k′ ∈[K] : Pku > 0, Pk′v > 0,
(3)"
STOCHASTIC BLOCK MODELS,0.04642409033877039,where the community membership matrix P = [Pkv] ∈RK×n is given by
STOCHASTIC BLOCK MODELS,0.04767879548306148,"PPT = Diag (|V1|, . . . , |VK|) with Pkv =

1
if v ∈Vk,
0
otherwise.
(4)"
STOCHASTIC BLOCK MODELS,0.04893350062735257,We deﬁne D ≜1
STOCHASTIC BLOCK MODELS,0.050188205771643665,"nPPT which is a diagonal matrix of relative community sizes.
Deﬁnition 2. (Coarse SBM Ghoroghchian et al. (2021)) Deﬁne a coarse-scale summary to A as"
STOCHASTIC BLOCK MODELS,0.05144291091593475,"eA ≜WAWT
∈Rm×m,
(5)"
STOCHASTIC BLOCK MODELS,0.05269761606022585,"where the coarsening matrix W ∈Rm,n is (a) r-homogeneous, for all i ∈[m]; that is, each i-th row of
W (say wi) has r non-zero terms and all rows have constant row sum and (b) (WWT = 1 rIm)."
STOCHASTIC BLOCK MODELS,0.053952321204516936,"Here, eA can be interpreted as the symmetric adjacency matrix of an un-directed graph Gcoarse with
m nodes— referred to as coarse graph. This interpretation is helpful when we discuss LTI system
associated with eA in Section 5. We refer the nodes in Gcoarse to as c-nodes4 as opposed to the ﬁne
nodes in Gﬁne. Note that r ≤n"
STOCHASTIC BLOCK MODELS,0.05520702634880803,"m, and r ≪n indicating that c-nodes can cover the ﬁne graph only
sparsely. In other words, there may exist (several) ﬁne nodes that do not contribute to A (see Fig. 1).
The main goal of our paper is to quantify controllability of Gﬁne, with community structure, using the
coarsely inferred network Gcoarse. Importantly, we do not have access to the way the coarse graph is
acquired at the time of decision making though the results depend on them."
STOCHASTIC BLOCK MODELS,0.056461731493099125,"From Eq. 8 and Eq. 3, the expected quantities of ¯A ≜E[A] and ¯eA ≜E[ eA] can be computed as"
STOCHASTIC BLOCK MODELS,0.05771643663739021,"¯A = PTQP
and
¯eA = (WPT)
| {z }
Φ"
STOCHASTIC BLOCK MODELS,0.05897114178168131,"Q(WPT)T,
(6)"
STOCHASTIC BLOCK MODELS,0.060225846925972396,"where Φ ∈Rm×K is the coarse community membership matrix, and Φik captures the extent to which
the i-th c-node overlaps with the k-th community. Let us also deﬁne the resolution parameter."
STOCHASTIC BLOCK MODELS,0.06148055207026349,"ν ≜
min
i∈[m],k∈[K]:Φik>0 Φik.
(7)"
STOCHASTIC BLOCK MODELS,0.06273525721455459,"By deﬁnition 1/r ≤ν ≤1. In what follows, we assume that a c-node has a constant minimum
overlap (i.e. ν) with each community that independent of other system parameters."
STOCHASTIC BLOCK MODELS,0.06398996235884567,"The example below will highlight the structural differences among matrices P, Φ, and W.
Example 1. For ﬁne network shown in Fig. 1, the following hold"
STOCHASTIC BLOCK MODELS,0.06524466750313676,4“c-” stands for compound or coarse.
STOCHASTIC BLOCK MODELS,0.06649937264742785,Under review as a conference paper at ICLR 2022
STOCHASTIC BLOCK MODELS,0.06775407779171895,"1. P = BlkDiag(1T
12, 1T
18, 1T
6 ). Here, 1d is the d-dimensional all-ones column vector."
STOCHASTIC BLOCK MODELS,0.06900878293601004,"2. Φ = [ΦT
1 , ΦT
2 , · · · , ΦT
6 ]T, where Φ1 = [1, 0, 0]; Φ2 = [ 2 3, 1"
STOCHASTIC BLOCK MODELS,0.07026348808030113,"3, 0]; Φ3 = Φ4 = [0, 1, 0];
Φ5 = [0, 1 3, 2"
STOCHASTIC BLOCK MODELS,0.07151819322459223,"3]; and Φ6 = [0, 0, 1]."
STOCHASTIC BLOCK MODELS,0.07277289836888332,"3. Finally, each row of the coarsening matrix W has three non-zero entries, all equal to 1/3."
STOCHASTIC BLOCK MODELS,0.0740276035131744,"Each c-node in {1, 3, 4, 6} covers one community; each c-nodes in {2, 5} overlap with two."
STOCHASTIC BLOCK MODELS,0.07528230865746549,"Assumption 1. (SBM scaling Abbe (2017)) For all k ∈[K], we have 0 < cmin ≤|Vk|"
STOCHASTIC BLOCK MODELS,0.07653701380175659,"n
≤cmax < 1,
where cmin, cmax are constants. There exists a ρn ∈(0, 1) and a non-negative matrix Q(c), such that"
STOCHASTIC BLOCK MODELS,0.07779171894604768,"Q = ρnQ(c),
(8)
Assumption 2. (Fully-Synchronized c-node): The coarse graph has at least one pure node per
community k ∈[K]. Formally, for all k ∈[K], there exist a coarse node i ∈[m] such that Φik = 1.
Assumption 3. (Uniform Coarsening): There exist ˜cmin and ˜cmax independent of m such that
˜cmin1K ≤1T
mΦ/m ≤˜cmax1K."
STOCHASTIC BLOCK MODELS,0.07904642409033877,"We also assume that communities have self-connections, that is, tr(Q(c)) > 0, and Qc ≤1K×K.
Assumption 1 helps us uniformly control the sparsity of connection in and cross communities. For
several real-world networks, ρn typically decreases with n Abbe (2017). Assumption 2 states that
for each community there exist at least one c-node that is fully inside one community. We call such
c-node fully synchronized (see Remark 1). Finally, Assumption 3 ensures that the relative coverage
of each community measured by coarse nodes scales linearly with respect to the graph size."
STOCHASTIC BLOCK MODELS,0.08030112923462986,"Figure 1: Schematic of MOR and learning-based approaches for estimating θgroup,A. (a) For Gﬁne consisting
of n = 36 nodes, we have K = 3 communities (V1, V2, V3) with m = 6 coarse (c)-nodes (K1,...,K6), each
having a coverage size r = 3. The control node set is K3. (b) In MOR approach, we infer θgroup,A via reduced
order dynamical system Scoarse. (c) Learning based approach capitalizes on mixed membership (MM) Algorithm
2 to estimate θgroup,A directly from eA, thereby avoiding the need to do consider Scoarse."
STOCHASTIC BLOCK MODELS,0.08155583437892096,"Remark 1. (Synchronization of community and coarsening): The coarsening operation is oblivi-
ous to the community structure in Gﬁne. Thus, the i-th c-node contains information about multiple
communities if supp(Wi,:) ∩supp(Pi,:) ̸= ∅(the subscript denotes the i-th row), for i ̸= j. Perfect
synchronization: A special case where the intersection is non-empty only when i = j. This happens
when all the communities have the same size and each c-node covers only one whole community."
PROBLEM STATEMENT,0.08281053952321205,"4
PROBLEM STATEMENT"
PROBLEM STATEMENT,0.08406524466750313,"Consider the LTI system on the network Gﬁne, deﬁned by the symmetric adjacency matrixin A Eq. 3:
Sﬁne : x[t + 1] = Anomx[t] + BKu[t],
(9)"
PROBLEM STATEMENT,0.08531994981179424,"where Anom ≜
1
c·tr(A)A, A ∈Rn×n, and BK ∈Rn×n selects a set of control nodes K ⊂Gﬁne.
Depending on the controllability properties of Gﬁne (see, Section 3.1), the inputs at these |K| control
nodes may or may not effect all the states in x[k]. Let Ki = supp(wi), be the set (hereafter, group)
of r nodes coarsened by wi. The following assumption states that Gﬁne is controllable from all
Ki ⊂Gﬁne; see Remark 2."
PROBLEM STATEMENT,0.08657465495608532,Under review as a conference paper at ICLR 2022
PROBLEM STATEMENT,0.08782936010037641,"Assumption 4. Let BKi = Diag(wT
i ). For any i ∈[m], the Gramian C(Anom, BKi) is full rank."
PROBLEM STATEMENT,0.0890840652446675,"As ρ(Anom) < 1 holds with high probability (see Lemma 1), it follows that C(Anom, BKi) exits, and
hence, the inﬁnite time Gramian C(Anom, BKi) exists. Assumption 4 ensures that average energy
(see Section 3.1) is ﬁnite; however, average controllability need not be ﬁnite."
PROBLEM STATEMENT,0.0903387703889586,"For Ki-th control node set with input matrix BKi = Diag(wT
i ), associate the average controllabity
measure: θ(i)
group,A ≜tr[C(Anom, BKi)], for all i ∈[m]. Since C(·) is a p.s.d matrix, it follows that"
PROBLEM STATEMENT,0.09159347553324969,"θ(i)
group,A ≥0. Accordingly, deﬁne the group average controllability vector for Sﬁne:"
PROBLEM STATEMENT,0.09284818067754078,"θgroup,A ≜

θ(1)
group,A, . . . , θ(m)
group,A

∈Rm,
(10)"
PROBLEM STATEMENT,0.09410288582183186,"which summarizes the average controllability measure (see Sec 3.1) for all control nodes sets
{K1, . . . , Km}. Thus, θgroup,A helps infer (i) Kis that drive the network to the desired target state
with least control effort, and (ii) if such control node sets should have a community structure."
PROBLEM STATEMENT,0.09535759096612297,"In this paper, using only the knowledge of eA in Eq. 5, we want to estimate the random vector
θgroup,A in Eq. 10. We consider two contrasting approaches: (i) the traditional model order reduction
(MOR), where we rely upon the reduced order auxiliary system Scoarse (see Eq. 11) governed by
eA to infer θgroup,A; and (ii) the learning based approach, where we directly estimate θgroup,A
using a clustering based mixed membership community learning algorithm; see Section 6. Fig. 1
provides a nice graphical illustration of our approaches. Broadly, our analysis highlights the role of
community structure, coarsening process, and graph sparsity conditions on the performance of these
both approaches. Our numerical simulations show that both approaches can outperform each other;
however, learning based approach outperforms its counterpart in several parametric regions Finally,
our main results in Sections 5 and 6 are probabilistic in nature because A is a random matrix.
Remark 2. (Group nodes controllability) Assumption 4 demands that a group of nodes should be
able to control Gﬁne, which holds true for brain networks (Pasqualetti et al., 2019). This assumption is
a very weaker condition than asking Gﬁne to be controllable from every single node. Moreover, if Gﬁne
is not controllable from the control nodes, we can decompose the state-space of Gﬁne into controllable
and uncontrollable sub-spaces (Chen, 1999), and adapt our analysis to the controllable sub-space."
MOR APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.09661229611041405,"5
MOR APPROACH FOR GROUP AVERAGE CONTROLLABILITY"
MOR APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.09786700125470514,"We provide a tight upper bound on the element wise error between θgroup,A in Eq. 10 and θcoarse, e
A
in Eq. 13. The latter quantity is the group average controllability vector of the reduced order system:"
MOR APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.09912170639899624,"Scoarse : ex[t + 1] = eAnomex[t] + eBu[t] ,
(11)"
MOR APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.10037641154328733,"where eAnom ≜
1
˜c·tr( e
A) eA , eA = WAWT is given by Eq. 5 and the normalization factor ˜c · tr( eA) is"
MOR APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.10163111668757842,"used for stability purposes, where ˜c > 0. Importantly, the state ex[t] ∈Rm is not a compression
of the true state x[t] in Sﬁne and m ≪n (hence the name MOR). Rather, ex[t] is a ﬁctitious state
that describes the dynamics of the (scaled) matrix eA. This ﬁctitious state is controlled by the input
eBu[t] ∈Rm."
MOR APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.1028858218318695,"The following lemma states that Sﬁne and Scoarse are asymptotically stable with very high probability.
Thus, θgroup,A in 10 and θcoarse, e
A in 13 are well deﬁned. Let c, ˜c, β ≥0, and deﬁne the stability"
MOR APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.10414052697616061,"indices: κnom := (cβ tr( ¯A)−∥¯A∥∞)/(n(1−cβ)2) and eκnom := (˜cβ tr( ¯eA)−∥¯eA∥∞)/(m(1−r˜cβ)2),
where ¯A ∈Rn×n and ¯eA ∈Rm×m are given in Eq. 6 and r is the coverage size.
Lemma 1. (Probabilistic stability of Sﬁne and Scoarse): Suppose that the stability indices κnom and
eκnom are strictly positive for some constants c, ˜c > 0, and 0 ≤β < 1. Then,"
MOR APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.1053952321204517,"P [ρ(Anom) ≥β] ≤n exp (−2κnom)
and
P[ρ( eAnom) ≥β] ≤m exp (−2eκnom) .
(12)"
MOR APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.10664993726474278,"For m = n, we have r = 1, and hence, the probabilistic inequalities in Eq. 12 coincide with each
other. Further, the higher κnom and eκnom, the higher the chance that Sﬁne and Scoarse are stable.
Interestingly, κnom and eκnom do not explicitly scale with n and m. In fact, κnom ≥κnom, lb, where"
MOR APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.10790464240903387,Under review as a conference paper at ICLR 2022
MOR APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.10915934755332497,"κnom, lb := (cβ cmin tr(Q) −cmax ∥Q∥∞)/(1 −cβ)2 is independent of n. Thus, stability is not
guaranteed for larger networks modeled using Sﬁne with c · trA normalization."
MOR APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.11041405269761606,"Instead, κnom, lb →∞as cβ →1, thereby P [ρ(Anom) ≥β] ≤n exp(−2κnom, lb) →0. So, under
what conditions κnom > 0 for cβ = 1? One such condition is cβ ≥cmax ∥Q∥∞/(cmin tr(Q)).
Let cmax = cmin . For diagonally dominant probability matrix Q (i.e., the community structure is
assortative—more in-community edges than across-community edgess), tr(Q) ≥∥Q∥∞, and hence,
we can choose c and β such that cβ = 1. However, non diagonally dominant matrices can satisfy
tr(Q) ≥∥Q∥∞. For example, a symmetric matrix Q ∈R3×3, with Qii = 0.2, Q12 = 0.25, and
Q13 = 0.01 is not diagonally dominant because Qii ≤P"
MOR APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.11166875784190715,"j̸=i Qij, for all i, but tr(Q) ≥∥Q∥∞."
MOR APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.11292346298619825,"We now bound the difference between θgroup,A in 10 and θcoarse, e
A in 13. Let eAnom ≜eA/(˜c · tr( eA))"
MOR APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.11417816813048934,"and eBi = rWdiag(wT
i )—the coarsened input matrix. Let the average controllability for the i-th
c-node be θ(i)
group, e
A ≜tr[C( eAnom, eBi)]≥0. Deﬁne the average controllability vector for Scoarse:"
MOR APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.11543287327478043,"θcoarse, e
A ≜

θ(1)
coarse, e
A, . . . , θ(m)
coarse, e
A"
MOR APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.11668757841907151,"
∈Rm.
(13)"
MOR APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.11794228356336262,"In what follows, when the synchronization holds, we show that θcoarse, e
A associated with Scoarse can
well approximate θgroup,A associated with Sﬁne. Deﬁne the error metric:"
MOR APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.1191969887076537,"∆i(A, eA) ≜ "
MOR APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.12045169385194479,"rθ(i)
group,A −1
Pm
i=1[rθ(i)
group,A −1]
−
θ(i)
coarse, e
A −1
Pm
i=1[θ(i)
coarse, e
A −1]"
MOR APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.12170639899623588,",
for all i ∈[m].
(14)"
MOR APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.12296110414052698,"The proposed error metric ∆i(A, eA) allows us to do a fair shifting- and scaling-free comparison
between the vectors θgroup,A and θcoarse, e
A. The shift factor ""-1"" accounts for the inherent ""+1"" shift
in the average controllability deﬁnition and the scale factor r discounts the 1/r factor in θgroup,A."
MOR APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.12421580928481807,"Akin to ∆i(A, eA) in Eq. 14, deﬁne ∆i( ¯A, e¯A) associated with the expected quantities ¯A and e¯A."
MOR APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.12547051442910917,"Theorem 1. (Element wise bound on θgroup,A−θcoarse, e
A): Let ∆i(A, eA) and ∆i( ¯A, e¯A) be deﬁned"
MOR APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.12672521957340024,"as above. Then, under the assumptions in Lemma 1, ∆i(A, eA) ≤∆i( ¯A, e¯A) + O

1
ρnm +
r2
√mρ3n "
MOR APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.12797992471769135,"with probability at least 1 −6 exp(−2κ(Q(c), D, ν, m, n, ρn)), where, for constants 0 < δ, ζ < 1,"
MOR APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.12923462986198245,"κ(Q(c), D, ν, m, n, ρn) = min
n
nρ2
n(tr(DQ(c))ζ)2, m(ρnν2tr(Q(c))δ)2"
MOR APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.13048933500627352,"ρ2
nmn(˜cmincmin||Q(c)||1,1ζ)2, ρ2
nm2(˜c2
min||Q(c)||1,1δ)2o
,"
MOR APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.13174404015056462,"with ν, D, and Q(c) given by Eq. 7, Eq. 4, and Eq. 8, and ˜cmin is deﬁned in Assumption 3."
MOR APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.1329987452948557,"Theorem 1 says that ∆i(A, eA) ≈O

1
ρnm +
r2
√mρ3n"
MOR APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.1342534504391468,"
provided ∆i( ¯A, e¯A) is small. Thus, for ﬁxed n,
MOR based estimate θcoarse, e
A approximates θgroup,A if the graph density ρn or number of c-nodes
is large, which we validate using numerical simulations as well. It can be shown that the bias term
∆i( ¯A, e¯A) is exactly zero if the communities are synchronized with the coarsening process (see
Remark 1) and that Qii = p > 0 and Qij = q > 0, for i ̸= j."
LEARNING APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.1355081555834379,"6
LEARNING APPROACH FOR GROUP AVERAGE CONTROLLABILITY"
LEARNING APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.13676286072772897,"We present our learning based approach to estimate θgroup,A in Eq. 10. Unlike the MOR based
approach that relies on Sﬁne, we directly estimate elements in θgroup,A based on the popular mixed
membership (MM) community learning algorithms (Mao et al., 2020; Huang et al., 2019; Mao et al.,
2018; 2017; Aicher et al., 2015). Speciﬁcally, we work with the MM algorithm by (Mao et al., 2020)
which is not only numerically efﬁcient but also has strong theoretical guarantees.
Lemma 2. Let θgroup, ¯A be obtained by replacing A with the expected matrix ¯A in θgroup,A, given
by Eq. 10. Let P and Φ be as in Eq. 4 and Eq. 6. Suppose that Assumption 1 hold. Then,"
LEARNING APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.13801756587202008,"θgroup, ¯A = (1m + dΦdiag(Υ)) /r,
(15)"
LEARNING APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.13927227101631118,Under review as a conference paper at ICLR 2022
LEARNING APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.14052697616060225,"where d = 1/(ntr[DQ(c)]), D = (1/n)PPT, and Q(c) is given by Eq. 8, and"
LEARNING APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.14178168130489335,"Υ ≜(nd)Q(c)DQ(c)[I −((nd)DQ(c))2]−1.
(16)"
LEARNING APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.14303638644918445,"Lemma 2 gives us a formula to compute the group average controllability vector associated with the
expected matrix ¯A (see Supplemental material for complementary explanation and interpretation).
Theorem 3 (see Appendix) shows that ∆i(A, eA) ≤ϵ, for arbitrary ϵ > 0, hold with high probability.
In view of this fact, we propose our candidate estimator as
ˆθgroup ≜1m + ˆΦdiag(ˆΥ),
(17)"
LEARNING APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.14429109159347553,"where ˆΥ ≜
ˆQ ˆD ˆQ
tr( ˆD ˆQ)(I −(
ˆD ˆQ
tr( ˆD ˆQ))2)−1. The hatted quantities ˆΦ and ˆQ are obtained from Algorithm5"
LEARNING APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.14554579673776663,"2, which takes as input eA and number of communities K. Instead, we obtain ˆD from Algorithm 1.
Importantly, ˆθgroup in Eq. 17 is obtained from coarsened matrix eA but not the ﬁne scale matrix A."
LEARNING APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.1468005018820577,"Theorem 2. (Component wise error bound between ˆθgroup and θgroup,A): Suppose that there exist
constants ˜cmin and ˜cmax that satisfy Assumption 3. Then, under the hypotheses stated in Lemma 1,

rθ(i)
group,A −1
Pm
i=1(rθ(i)
group,A −1)
−
ˆθ(i)
group −1
Pm
i=1(ˆθ(i)
group −1)"
LEARNING APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.1480552070263488,"|
{z
}"
LEARNING APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.1493099121706399,"≜b∆i( e
A)"
LEARNING APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.15056461731493098,"=O
 1 m[ 1"
LEARNING APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.15181932245922208,"ρn
+ ||EΦ||max + ||EQ||max + ||ED||max]
"
LEARNING APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.15307402760351319,"holds with probability at least 1−3 exp(−2ˆκ(Q(c), D, m, n, ρn)), where EΦ ≜ˆΦ−Φ, EQ ≜ˆQ−Q,
and ED ≜ˆD −D are the error matrices. Further, for a constant 0 < ζ < 1, the exponent
ˆκ(Q(c), D, m, n, ρn) = min
n
nρ2
n(tr(DQ(c))ζ)2, mnρ2
n(˜cmincmin||Q(c)||1,1ζ)2o
."
LEARNING APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.15432873274780426,"Theorem 2 suggests that for sufﬁciently large m, the estimate ˆθgroup approximates θgroup,A to an
arbitrary precision if: (a) the graph is dense enough (larger ρn), (b) the coarse community membership
matrix is well estimated (smaller6 ||EΦ||max), (c) the cross-community probability estimation do
not suffer from high error (smaller ||EQ||max), and the relative community sizes estimated from
the coarse graph are close the ones in the ﬁne graph (smaller ||ED||). The result of Theorem 2 is
important because one can directly infer the most inﬂuential control node set Ki (one with high
average controllability) via the most inﬂuential i-the c-nodes, and vice versa."
LEARNING APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.15558343789209536,Algorithm 1: Direct Inference of the Group Average Controllability
LEARNING APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.15683814303638646,"Require: estimates ˆΦ and ˆQ from Algorithm 2, and the number of communities K"
LEARNING APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.15809284818067754,"1: compute ˆD = diag(
ˆΦT1m
1K ˆΦT1m )"
LEARNING APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.15934755332496864,"2: return ˆθgroup = 1m + ˆΦdiag
 ˆQ ˆD ˆQ"
LEARNING APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.1606022584692597,"tr( ˆD ˆQ)(I −(
ˆD ˆQ
tr( ˆD ˆQ))2)−1)
"
LEARNING APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.1618569636135508,"Algorithm 2: Mixed Membership Community Estimation Algorithm (Mao et al., 2020)"
LEARNING APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.16311166875784192,"Require: Coarse adjacency matrix eA, number of communities K"
LEARNING APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.164366373902133,"1: compute the highest K eigen-decomposition of eA as ˆV ˆΛ ˆV T and set Spruned = Prune( ˆV )
2: set X = ˆV ([m]\Sp.runed, :) and compute Spure = Successive Projection Algorithm(XT)"
LEARNING APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.1656210790464241,"3: set Xpure = X(Spure, :) and compute un-normalized ˆΦun-nom = ˆV X−1
pure
4: ˆΦun-nom
ik
←0
if ˆΦun-nom
ik
< e−12, ∀i ∈[m], k ∈[K]
5: return ˆΦ = Diag−1( ˆΦun-nom1K) ˆΦun-nom and ˆQ = XpureˆΛXT
pure"
LEARNING APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.1668757841907152,"5This MM algorithm adapted from Mao et al. (2020) is a type of spectral clustering method that ﬁrst performs
eigen decomposition of eA to ﬁnd the overlapping membership (Φik) of the ﬁne nodes. A pruning step is also
included (see steps 4 and 5 in Algorithm 2) to speed up the algorithm performance.
6 (Mao et al., 2020) showed that ||EΦ||max and ||EQ||max stated in Theorem 2 approach zero under some
conditions, as m →∞. But these conditions might not be applicable to our setup because of our coarsening
operation. However, our simulations show that b∆i( eA) decreases with m. The behavior of ||EΦ||max and
||EQ||max with respect to graph scaling is left for future work."
LEARNING APPROACH FOR GROUP AVERAGE CONTROLLABILITY,0.16813048933500627,Under review as a conference paper at ICLR 2022
SIMULATIONS,0.16938519447929737,"7
SIMULATIONS"
SIMULATIONS,0.17063989962358847,We validate our theoretical results by plotting7 the errors 1
SIMULATIONS,0.17189460476787954,"m
Pm
i=1 ∆i(A, eA) and 1"
SIMULATIONS,0.17314930991217065,"m
Pm
i=1 b∆i( eA) and
show that these errors are comparable to the bounds we obtained in Theorems 1 and 2. We generate
A ∼SBM(n, Q, p) and then determine eA = WAWT (see Supplemental material for generating
W). We set number of ﬁne nodes n = 5000, the overlap parameter η = 0.1, and the number of
communities K = 5. Finally, for Q ∈RK×K, we set Qkk = p = 0.05 and Qkk′ = q = 0.01 (for
k ̸= k′). If not speciﬁed, the number of c-nodes m = 100 and coverage size per c-node r = 4.
Fig. 2(a)-2(d) illustrate the qualitative behavior of the errors with respect to changes in m, ρn, and
the degree of (non-)synchronization in coarse nodes (i.e., η), and r. To fairly compare these errors,"
SIMULATIONS,0.17440401505646172,"we also consider a base line error: Pm
i=1 |
µi−1
Pm
i=1(µi−1) −
rθ(i)
group,A−1
Pm
i=1(rθ(i)
group,A−1)|/m, where µ ∈[1, 2]m"
SIMULATIONS,0.17565872020075282,contains i.i.d. uniform random variable drawn independently of eA.
SIMULATIONS,0.17691342534504392,"(a) w.r.t. no. coarse nodes m.
(b) w.r.t. ρn for p = 5q = 0.005."
SIMULATIONS,0.178168130489335,"(c) w.r.t. overlap extent η.
(d) w.r.t. coverage size r."
SIMULATIONS,0.1794228356336261,"Figure 2: Estimation error based on MOR and Learning approaches. MOR Error=Pm
i=1 ∆i(A, eA)/m and
Learning Error=Pm
i=1 b∆i( eA)/m. The shaded region in the ﬁgures represent one standard deviation computed
for 20 independent realizations. We make the following observations. First, both the learning and MOR based
errors are consistently better than the random baseline. Second, the learning based approach has consistently
smaller error than that of the MOR approach for large parametric regimes. Third, (a) shows that all errors
monotonically decrease as m increases. This is consistent with our bounds in Theorems 1 and 2. Fourth, (b)
shows that errors decrease as ρn increases. This is expected because larger values of ρn result in more distant
in- and cross-community edge densities. This makes community representation extraction and controllability
estimation easier. Fifth, (c) demonstrates the higher tolerance of the Learning approach to situations whenin
coarse measurements are less synchronized, in comparison to the MOR method. Finally, (d) shows that error
decreases with r. This should be the case as larger r means more ﬁne nodes are sampled during coarsening.
8
CONCLUSION AND FUTURE WORK"
SIMULATIONS,0.1806775407779172,"We introduced a learning-based framework that exploits the power of community-based representation
learning to infer average controllability of ﬁne graphs from coarse summary data. We compared the
performance of this approach with that of MOR approach. For both these methods, we derived high
probability error bounds on the deviation between the error estimate and ground truth, and validated
the theory with numerical simulations. Our results highlight the role of ﬁne- and coarse-network
sizes, graph density, and community synchronization bias (see Remark 1 in modulating the estimation
errors. Interestingly, for the latter approach, we show that the estimation error decreases with network
size albeit the synchronization bias, which is not the case with the MOR-based approach. For
future, we plan to implement our theory to study the role of coarsening, community structures , and
synchronization aspects on the controllability of brain networks."
SIMULATIONS,0.18193224592220827,7The Python code to reproduce the results is attached to the submitted ﬁle.
SIMULATIONS,0.18318695106649938,Under review as a conference paper at ICLR 2022
ETHICS STATEMENT,0.18444165621079048,"9
ETHICS STATEMENT"
ETHICS STATEMENT,0.18569636135508155,"Although our work mainly takes a theoretical perspective to the controllability of coarse graphs
motivated by therapeutic neuroscience applications, our results can potentially involve negative
impacts if employed in other applications. For instance, identifying the most inﬂuential groups of
nodes (or equivalently individuals) in a social network as a result of estimating the network group
average controllability, may motivate manipulative actions; i.e. the most inﬂuential node group
may be selected for control, in order to steer the whole network towards an unethical goal (like
manipulating individuals in a social network to vote in favour of a particular election candidate). The
negative impact may also result in bias against less-inﬂuential nodes, as they will be ignored when it
comes to the selection of node groups for control actuation."
REPRODUCIBILITY STATEMENT,0.18695106649937265,"10
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.18820577164366373,"All the results presented in this paper are reproducible. The theoretical ﬁndings are annotated and
step-by-step elaborated in the appendix. The data generation process and the parameter values used
for numerical simulations are fully explained. In addition, the Python code from which the simulation
ﬁgures are generated is attached to the submission ﬁles. The code is also on Github and the repository
will go public upon submission acceptance."
REPRODUCIBILITY STATEMENT,0.18946047678795483,Under review as a conference paper at ICLR 2022
REFERENCES,0.19071518193224593,REFERENCES
REFERENCES,0.191969887076537,"Emmanuel Abbe. Community detection and stochastic block models: recent developments. The Journal of
Machine Learning Research, 18(1):6446–6531, 2017."
REFERENCES,0.1932245922208281,"Christopher Aicher, Abigail Z Jacobs, and Aaron Clauset. Learning latent block structure in weighted networks.
Journal of Complex Networks, 3(2):221–248, 2015."
REFERENCES,0.1944792973651192,"Richard F Betzel and Danielle S Bassett. Multi-scale brain networks. Neuroimage, 160:73–83, 2017."
REFERENCES,0.19573400250941028,Chi-Tsong Chen. Linear system theory and design. 1999.
REFERENCES,0.19698870765370138,"Pedro H Constantino, Wentao Tang, and Prodromos Daoutidis. Topology effects on sparse control of complex
networks with laplacian dynamics. Scientiﬁc reports, 9(1):1–9, 2019."
REFERENCES,0.19824341279799249,"Fabrizio L. Cortesi, Tyler H. Summers, and John Lygeros. Submodularity of energy related controllability
metrics. In 53rd IEEE Conference on Decision and Control, pages 2883–2888, 2014. doi: 10.1109/CDC.
2014.7039832."
REFERENCES,0.19949811794228356,"Adrien Dulac, Eric Gaussier, and Christine Largeron. Mixed-membership stochastic block models for weighted
networks. In Conference on Uncertainty in Artiﬁcial Intelligence, pages 679–688. PMLR, 2020."
REFERENCES,0.20075282308657466,"Thorben Funke and Till Becker. Stochastic block models: A comparison of variants and inference methods.
PloS one, 14(4):e0215296, 2019."
REFERENCES,0.20200752823086573,"Naﬁseh Ghoroghchian, Gautam Dasarathy, and Stark Draper. Graph community detection from coarse measure-
ments: Recovery conditions for the coarsened weighted stochastic block model. In International Conference
on Artiﬁcial Intelligence and Statistics, pages 3619–3627. PMLR, 2021."
REFERENCES,0.20326223337515684,"Shi Gu, Fabio Pasqualetti, Matthew Cieslak, Qawi K Telesford, B Yu Alfred, Ari E Kahn, John D Medaglia,
Jean M Vettel, Michael B Miller, Scott T Grafton, et al. Controllability of structural brain networks. Nature
communications, 6(1):1–10, 2015."
REFERENCES,0.20451693851944794,"Christianne N Heck, David King-Stephens, Andrew D Massey, Dileep R Nair, Barbara C Jobst, Gregory L
Barkley, Vicenta Salanova, Andrew J Cole, Michael C Smith, Ryder P Gwinn, et al. Two-year seizure
reduction in adults with medically intractable partial onset epilepsy treated with responsive neurostimulation:
ﬁnal results of the rns system pivotal trial. Epilepsia, 55(3):432–441, 2014."
REFERENCES,0.205771643663739,"Ling Huang, Chang-Dong Wang, and Hongyang Chao. ocomm: Overlapping community detection in multi-view
brain network. IEEE/ACM Transactions on Computational Biology and Bioinformatics, 2019."
REFERENCES,0.20702634880803011,"Hossein Kassiri, Sana Tonekaboni, M Tariqus Salam, Nima Soltani, Karim Abdelhalim, Jose Luis Perez
Velazquez, and Roman Genov. Closed-loop neurostimulators: A survey and a seizure-predicting design
example for intractable epilepsy treatment. IEEE trans. on biomedical circuits and systems, 11(5):1026–1040,
2017."
REFERENCES,0.20828105395232122,"Fritz Keinert. Course notes: Applied linear algebra. https://orion.math.iastate.edu/keinert/
math507/notes/chapter5.pdf."
REFERENCES,0.2095357590966123,"Jason Z Kim, Jonathan M Soffer, Ari E Kahn, Jean M Vettel, Fabio Pasqualetti, and Danielle S Bassett. Role of
graph architecture in controlling dynamical networks with applications to neural systems. Nature physics, 14
(1):91–98, 2018."
REFERENCES,0.2107904642409034,"Xueyu Mao, Purnamrita Sarkar, and Deepayan Chakrabarti. On mixed memberships and symmetric nonnegative
matrix factorizations. In International Conference on Machine Learning, pages 2324–2333. PMLR, 2017."
REFERENCES,0.2120451693851945,"Xueyu Mao, Purnamrita Sarkar, and Deepayan Chakrabarti. Overlapping clustering models, and one (class) svm
to bind them all. In Advances in Neural Information Processing Systems, pages 2126–2136, 2018."
REFERENCES,0.21329987452948557,"Xueyu Mao, Purnamrita Sarkar, and Deepayan Chakrabarti.
Estimating mixed memberships with sharp
eigenvector deviations. Journal of the American Statistical Association, (just-accepted):1–24, 2020."
REFERENCES,0.21455457967377667,"Sarah Feldt Muldoon, Fabio Pasqualetti, Shi Gu, Matthew Cieslak, Scott T Grafton, Jean M Vettel, and Danielle S
Bassett. Stimulation-based control of dynamic brain networks. PLoS computational biology, 12(9):e1005076,
2016."
REFERENCES,0.21580928481806774,"Gerard O’Leary, David M Groppe, Tauﬁk A Valiante, Naveen Verma, and Roman Genov. NURIP: Neural
interface processor for brain-state classiﬁcation and programmable-waveform neurostimulation. IEEE Journal
of Solid-State Circuits, 53(11):3150–3162, 2018."
REFERENCES,0.21706398996235884,Under review as a conference paper at ICLR 2022
REFERENCES,0.21831869510664995,"Fabio Pasqualetti, Sandro Zampieri, and Francesco Bullo. Controllability metrics, limitations and algorithms for
complex networks. IEEE Transactions on Control of Network Systems, 1(1):40–52, 2014."
REFERENCES,0.21957340025094102,"Fabio Pasqualetti, Shi Gu, and Danielle S Bassett. Re: Warnings and caveats in brain controllability. NeuroImage,
197:586–588, 2019."
REFERENCES,0.22082810539523212,"Dragana M Pavlovi´c, Bryan RL Guillaume, Emma K Towlson, Nicole MY Kuek, Soroosh Afyouni, Petra E
Vértes, BT Thomas Yeo, Edward T Bullmore, and Thomas E Nichols. Multi-subject stochastic blockmodels
for adaptive analysis of individual differences in human brain network cluster structure. NeuroImage, 220:
116611, 2020."
REFERENCES,0.22208281053952322,"M Reza Pazhouhandeh, Gerard O’Leary, Iliya Weisspapir, David Groppe, Xuan-Thuan Nguyen, Karim Ab-
delhalim, Hamed Mazhab Jafari, Tauﬁk A Valiante, Peter Carlen, Naveen Verma, et al. 22.8 adaptively
clock-boosted auto-ranging responsive neurostimulator for emerging neuromodulation applications. In 2019
IEEE International Solid-State Circuits Conference-(ISSCC), pages 374–376. IEEE, 2019."
REFERENCES,0.2233375156838143,"Brittany H Scheid, Arian Ashourvan, Jennifer Stiso, Kathryn A Davis, Fadi Mikhail, Fabio Pasqualetti, Brian
Litt, and Danielle S Bassett. Time-evolving controllability of effective connectivity networks during seizure
progression. arXiv preprint arXiv:2004.03059, 2020."
REFERENCES,0.2245922208281054,"Ruslana Shulyzki, Karim Abdelhalim, Arezu Bagheri, M Tariqus Salam, Carlos M Florez, Jose Luis Perez
Velazquez, Peter L Carlen, and Roman Genov. 320-channel active probe for high-resolution neuromonitoring
and responsive neurostimulation. IEEE trans. on biomedical circuits and systems, 9(1):34–49, 2015."
REFERENCES,0.2258469259723965,"Olaf Sporns and Richard F Betzel. Modular brain networks. Annual review of psychology, 67:613–640, 2016."
REFERENCES,0.22710163111668757,"Peng Gang Sun. Controllability and modularity of complex networks. Information Sciences, 325:20–32, 2015."
REFERENCES,0.22835633626097868,"Elena Wu-Yan, Richard F Betzel, Evelyn Tang, Shi Gu, Fabio Pasqualetti, and Danielle S Bassett. Benchmarking
measures of network controllability on canonical graph models. Journal of Nonlinear Science, pages 1–39,
2018."
REFERENCES,0.22961104140526975,"Zhengzhong Yuan, Chen Zhao, Zengru Di, Wen-Xu Wang, and Ying-Cheng Lai. Exact controllability of complex
networks. Nature communications, 4(1):1–9, 2013."
REFERENCES,0.23086574654956085,Under review as a conference paper at ICLR 2022
REFERENCES,0.23212045169385195,"A
APPENDIX"
REFERENCES,0.23337515683814303,"In this appendix, we ﬁrst provide further detailed explanation for the numerical simulations that was
missing due to space constraints. Next we have a remark that adds complementary interpretation of
the group controllability formula estimated from the learning approach. Finally, we provide proofs
for all results stated in our paper."
REFERENCES,0.23462986198243413,"A.1
MORE ON NUMERICAL SIMULATIONS: HOW TO GENERATE W"
REFERENCES,0.23588456712672523,"For a realization of eA, we obtain the support of each row of W independently by ﬁrst generating a
random vector π ∈{η}K using a Dirichlet distribution; larger positive η result in greater overlap and
more communities. For each community k where πk > 0, ⌊r · πk⌋ﬁne nodes are randomly chosen
from Vnon−sel
k
(i.e., non-selected ﬁne indices in community k) and set as the support of wi. The
chosen indices are removed from Vnon−sel
k
.This process continues until the support of all wis are
selected."
REFERENCES,0.2371392722710163,"A.2
MORE ON THE ESTIMATED GROUP CONTROLLABILITY USING THE LEARNING APPROACH"
REFERENCES,0.2383939774153074,"Remark 3. (Synchronization versus controllability) Recall that Φi,k = |{v : v ∈Vk ∩
supp(wi)}|/r (for all k ∈[K], i ∈[m]) is the fraction of c-node i’s overlap with community
k. Thus, from Eq. 15, it follows that θ(i)
group, ¯A ∝P"
REFERENCES,0.2396486825595985,"k∈[K] Φi,kΥkk. In view of this observation and
Lemma 2, we observe that c-nodes that have the largest overlap with communities of strongest Υkk
are the most controllable."
REFERENCES,0.24090338770388958,"Additional notation: For n × m dimensional real matrix M, denote ∥M∥F =
pP u
P"
REFERENCES,0.24215809284818068,"v M2uv =
p"
REFERENCES,0.24341279799247176,"tr(MTM).
For a symmetric matrix M, denote λmax(M) to be the maximum eigenvalue.
diag(M) = [M11, . . . , Mnn]T ∈Rn, and Diag(M) sets the off-diagonal entries of M to zero."
REFERENCES,0.24466750313676286,Useful matrix norm bounds:
REFERENCES,0.24592220828105396,"1. Let Z = XY. Then, ∥Z∥F ≤∥X∥2∥Y∥F ≤∥X∥F ∥Y∥F .
2. Cauchy-Schwartz inequality: tr(XTY) ≤∥X∥F ∥Y∥F .
3. For any norm: If ||M|| < 1 ⇒||(I −M)−1|| <
1
1−||M|| (Keinert)."
REFERENCES,0.24717691342534504,"4. ∥M∥∞≤√n∥M∥2
5. ∥M∥1 ≤√n∥M∥2
6. For any m × n matrix M, we have ∥M∥2 ≤√m∥M∥∞.
7. ∥MXY∥max ≤∥M∥∞∥X∥max∥Y∥1
8. ||M2 −M′2|| ≤||M −M′||(||M|| + ||M′||)"
REFERENCES,0.24843161856963614,"Proof. ||M2−M′2|| = ||M2−MM′+MM′−M′2|| = ||M(M−M′)+(M−M′)M′|| ≤
||M −M′||(||M|| + ||M′||)"
REFERENCES,0.24968632371392724,9. M−1 −M′−1 = M−1(M′ −M)M′−1
REFERENCES,0.25094102885821834,"Lemma 3. (Lower-Bound Probability of Joint Events:) For the intersection of two events ℧1 and
℧2 we have: P {℧1 ∩℧2} ≥P {℧1} + P {℧2} −1."
REFERENCES,0.2521957340025094,"A.3
PROOFS FOR SECTION 5: MOR APPROACH FOR GROUP AVERAGE CONTROLLABILITY"
REFERENCES,0.2534504391468005,"A.3.1
PROOF OF LEMMA 1"
REFERENCES,0.2547051442910916,"Proof. We prove only the left inequality in Eq. 12. The right inequality in Eq. 12 can be proved using
similar steps, and the details are omitted. Because Anom = A/(c · tr(A)) is symmetric, it follows
that ρ(Anom) = ∥Anom∥2. From this observation and the fact that ∥Anom∥2 ≤∥Anom∥∞, we have
P [ρ(Anom) ≥β] = P [∥Anom∥2 ≥β] ≤P [∥Anom∥∞≥β]
= P [∥A∥∞−cβtr(A) ≥0] .
(18)"
REFERENCES,0.2559598494353827,Under review as a conference paper at ICLR 2022
REFERENCES,0.2572145545796738,"Let F = ∥A∥∞−cβtr(A) and note that E[F] = E∥A∥∞−cβtr( ¯A) ≥∥¯A∥∞−cβtr( ¯A).
The inequality follows because ∥A∥∞= maxi∈[n]
Pn
j=1 |aij| and that E[max{X1, . . . , Xt}] ≥
max{E[X1], . . . , E[Xt]}. From these observations, inequality in Eq. 18 can be further bounded as"
REFERENCES,0.2584692597239649,P [F ≥0] = P [F −E[F] ≥−E[F]]
REFERENCES,0.25972396486825594,"≤P

F −E[F] ≥cβtr( ¯A) −∥¯A∥∞

.
(19)"
REFERENCES,0.26097867001254704,"We show that F is a sub-Gaussian random variable and then bound the right term in Eq. 19 using the
well-known concentration inequality results. Rewrite F as follows"
REFERENCES,0.26223337515683814,"F = max 
  n
X"
REFERENCES,0.26348808030112925,"j=1
|a1j|, n
X"
REFERENCES,0.26474278544542035,"j=1
|a2j|, . . . , n
X"
REFERENCES,0.2659974905897114,"j=1
|anj| 
"
REFERENCES,0.2672521957340025,"−cβtr(A) = max 
  n
X"
REFERENCES,0.2685069008782936,"j=1
|a1j| −cβtr(A), n
X"
REFERENCES,0.2697616060225847,"j=1
|a2j| −cβtr(A), . . . , n
X"
REFERENCES,0.2710163111668758,"j=1
|anj| −cβtr(A) 
  = max 
  n
X"
REFERENCES,0.2722710163111669,"j=1
(a1j −cβajj), n
X"
REFERENCES,0.27352572145545795,"j=1
(a2j −cβajj), . . . , n
X"
REFERENCES,0.27478042659974905,"j=1
(anj −cβajj) 
"
REFERENCES,0.27603513174404015,".
(20)"
REFERENCES,0.27728983688833125,"In the last equality, we drop the absolute values because aij ∈[0, 1]. From the latter fact, we
also note that (akl −akk) ∈[−cβ, 1]. Thus, for all k ̸= l, (akl −akk) is bounded, and hence,
sub-Gaussian with parameter at most
p"
REFERENCES,0.27854454203262236,"1 + (cβ)2/2. Instead, for k = l, (akl −akk) is sub-Gaussian
with parameter at most |1 −cβ|/2. Finally, from Deﬁnition 1, notice that each term in the summation
Pn
j=1(akj −cβajj) is independent. From these facts and the linearity of sub-Gaussians, we note
that, for all k ∈[n], summand Pn
j=1(akj −cβajj) is sub-Gaussian with parameter at most: σ = 1 2 p"
REFERENCES,0.2797992471769134,(n −1)(1 + (cβ)2) + (1 −cβ)2.
REFERENCES,0.2810539523212045,"Putting all these pieces together in conjunction with the facts that maxima of sub-Gaussians concen-
trates near its expectation and cβtr( ¯A) −∥¯A∥∞≥0 (by assumption), from Eq. 19, we have"
REFERENCES,0.2823086574654956,"P

F −E[F] ≥cβtr( ¯A) −∥¯A∥∞

≤n exp
 
−(cβtr( ¯A) −∥¯A∥∞)2/2σ2"
REFERENCES,0.2835633626097867,"≤n exp
 
−2(cβtr( ¯A) −∥¯A∥∞)2/(n(1 −cβ)2)

.
(21)"
REFERENCES,0.2848180677540778,"The last inequality follows because 2σ2 ≥n/2(1 −cβ)2. The left inequality in Eq. 12 follows by
combining inequalities in Eq. 18 and Eq. 21. The proof is now complete."
REFERENCES,0.2860727728983689,"A.3.2
LEMMA 4:UPPER AND LOWER BOUNDS OF THE TRACE OF THE FINE- AND
COARSE-SCALE MATRICES"
REFERENCES,0.28732747804265996,"Lemma 4. (Upper and Lower bounds of the trace of the ﬁne- and coarse-scale matrices). For
constants 0 < δ, ζ < 1:"
REFERENCES,0.28858218318695106,"P

nρnζ(l) ≤tr(A) ≤nρnζ(u)

≥1 −2 exp
 
−2nρ2
n(tr(DQ(c))ζ)2
,"
REFERENCES,0.28983688833124216,"P
h
mρnν2δ(l) ≤tr( eA) ≤mρnδ(u)
i
≥1 −2 exp (−2m(ρnν2tr(Q(c))δ)2),"
REFERENCES,0.29109159347553326,"P

1TWAWT1 ≥˜c2
minρnm2||Q(c)||1,1(δ + 1)

≥1 −exp
 
−2ρ2
nm2(˜c2
min||Q(c)||1,1δ)2
,
P

1T
mWA1n ≥˜cmincminρnmn||Q(c)||1,1(ζ + 1)

≥1 −exp
 
−2ρ2
nmn(˜cmincmin||Q(c)||1,1ζ)2 (22)"
REFERENCES,0.29234629861982436,"where ν is the coarsening resolution parameter (c.f. Eq. 7), and"
REFERENCES,0.2936010037641154,"ζ(l) ≜tr(DQ(c))(1 −ζ), ζ(u) ≜tr(DQ(c))(1 + ζ)
δ(l) ≜tr(Q(c))(1 −δ), δ(u) ≜tr(Q(c))(1 + δ)
(23)"
REFERENCES,0.2948557089084065,Under review as a conference paper at ICLR 2022
REFERENCES,0.2961104140526976,"A.3.3
PROOF OF LEMMA 4"
REFERENCES,0.2973651191969887,"Proof. From deﬁnition, E[tr(A)] = tr(PTQP) = nρntr(DQ(c)) and E[tr( eA)] = ρntr(ΦQ(c)ΦT).
Using the Hoeffding’s inequality for tail bounding independent random variables, for a constant
0 < ζ < 1 we have:"
REFERENCES,0.2986198243412798,"P

|tr(A) −nρntr(DQ(c))| ≥nρntr(DQ(c))ζ

≤2 exp ( −2(nρntr(DQ(c))ζ)2"
REFERENCES,0.2998745294855709,"n
) = 2 exp "
REFERENCES,0.30112923462986196,"

−2nρ2
n(tr(DQ(c))ζ
|
{z
}"
REFERENCES,0.30238393977415307,"=
ζ(u)−ζ(l) 2 )2  

 (24) Hence"
REFERENCES,0.30363864491844417,"nρn tr(DQ(c))(1 −ζ)
|
{z
}
ζ(l)"
REFERENCES,0.30489335006273527,"≤tr(A) ≤nρn tr(DQ(c))(1 + ζ)
|
{z
}
ζ(u)
(25)"
REFERENCES,0.30614805520702637,"We follow similar steps for eA. For all i ∈[m]:
X"
REFERENCES,0.3074027603513174,"k,k′∈[K]
ΦikQ(c)
kk′Φik′
≥
X"
REFERENCES,0.3086574654956085,"k∈[K]:Φik>0
[Φ2
ikQ(c)
kk + 2Φik
X"
REFERENCES,0.3099121706398996,"k′>k
Q(c)
kk′Φik′]"
REFERENCES,0.3111668757841907,"≥ν2tr(Q(c)) + 2ν2||Q(c)||min
≥ν2tr(Q(c)) (26)"
REFERENCES,0.3124215809284818,based on which the following upper and lower bounds on tr( e¯A) can be found.
REFERENCES,0.3136762860727729,"mρnν2tr(Q(c)) ≤tr( e¯A) = ρntr(ΦQ(c)ΦT) = ρn m
X i=1 X"
REFERENCES,0.31493099121706397,"k,k′∈[K]
ΦikQ(c)
kk′Φik′ ≤mρntr(Q(c)) (27)"
REFERENCES,0.3161856963613551,We can now use Hoeffding’s inequality to obtain the tail bound for a constant 0 < δ < 1:
REFERENCES,0.3174404015056462,"P
h
|tr( eA) −ρntr(ΦQ(c)ΦT)| ≥ρntr(ΦQ(c)ΦT)δ
i
≤2 exp ( −2(ρntr(ΦQ(c)ΦT))2 m
)"
REFERENCES,0.3186951066499373,"≤2 exp (−2m(ρnν2tr(Q(c))δ)2).
(28)"
REFERENCES,0.3199498117942284,The event in Eq. 28 leads to the following upper and lower bound on tr( eA):
REFERENCES,0.3212045169385194,"mρnν2 tr(Q(c))(1 −δ)
|
{z
}
δ(l)"
REFERENCES,0.3224592220828105,"≤ρntr(ΦQ(c)ΦT)(1 −δ) ≤tr( eA) ≤ρntr(ΦQ(c)ΦT)(1 + δ) ≤mρn tr(Q(c))(1 + δ)
|
{z
}
δ(u) . (29)"
REFERENCES,0.3237139272271016,"We then replacing δ(l), δ(u) deﬁned in Eq. 29 into Eq. 28."
REFERENCES,0.32496863237139273,"Similarly, event"
REFERENCES,0.32622333751568383,"1TWAWT1 ≥
1TΦ
|{z}
≥˜cminm1K
QΦT1(1 + δ) ≥˜c2
minρnm2||Q(c)||1,1(1 + δ)
(30)"
REFERENCES,0.32747804265997493,happens with probability at least
REFERENCES,0.328732747804266,"1 −P

1TWAWT1 −1TΦQΦT1 < ˜c2
minρnm2||Q(c)||1,1δ

≥1 −exp ( −2(˜c2
minρnm2||Q(c)||1,1δ)2"
REFERENCES,0.3299874529485571,"m2
)
≥1 −exp
 
−2ρ2
nm2(˜c2
min||Q(c)||1,1δ)2 (31)"
REFERENCES,0.3312421580928482,using Assumption 3 and following using one-sided Hoeffding’s concentration inequality.
REFERENCES,0.3324968632371393,"Finally, similar arguments can be applied to the following event"
T,0.3337515683814304,"1T
mWA1n ≥
1mΦ
| {z }
≥˜cminm1K"
T,0.33500627352572143,"Q
P1n
|{z}
=ndiag(D)(1+ζ)≥ncmin1K"
T,0.33626097867001253,"≥˜cmincminρnmn||Q(c)||1,1(1 + ζ)
(32)"
T,0.33751568381430364,Under review as a conference paper at ICLR 2022
T,0.33877038895859474,that occurs with probability at least
T,0.34002509410288584,"1 −P

1T
mWA1n −1mΦQP1n < ˜cmincminρnmn||Q(c)||1,1ζ

≥1 −exp

−2 (˜cmincminρnmn||Q(c)||1,1ζ)2 mn
"
T,0.34127979924717694,"≥1 −exp
 
−2ρ2
nmn(˜cmincmin||Q(c)||1,1ζ)2 (33)"
T,0.342534504391468,using one-sided Hoeffding’s inequality. This concludes the proof.
T,0.3437892095357591,"A.3.4
LEMMA: ERROR BETWEEN THE GRAMIANS OF RANDOM AND EXPECTED LTI SYSTEMS"
T,0.3450439146800502,"Let Sﬁne and Scoarse denote the expected dynamics of LTI Eq. 9 when A and eA are replaced with the
expected quantities ¯A and e¯A. The following result provides an error bound between the difference of
Gramians of Sﬁne and Sﬁne and that of Scoarse and Scoarse."
T,0.3462986198243413,"Lemma 5. (Error between the Gramians of random and expected LTI systems):
Under
the assumptions stated in Lemma 1, the following holds with probability at least 1 −
2 exp
 
−2nρ2
n(tr(DQ(c))ζ)2
:"
T,0.3475533249686324,"αn ≜∥C(Anom, In×n) −C( ¯Anom, In×n)∥max ≤
[1 + ∥Q(c)∥max"
T,0.34880803011292344,tr(DQ(c))][ 1
T,0.35006273525721454,ζ(l) + ρn∥Q(c)∥max
T,0.35131744040150564,tr(DQ(c)) ]
T,0.35257214554579674,"c2(1 −β2)ζ(l)
h
1 −
1
c2·tr2(DQ(c))
i
1
ρ2nn = O( 1"
T,0.35382685069008785,"ρ2nn), (34)"
T,0.35508155583437895,and with probability at least 1 −2 exp (−2m(ρnν2tr(Q(c))δ)2):
T,0.35633626097867,"eαn ≜∥C( eAnom, Im×m) −C( ¯eAnom, Im×m)∥max ≤
[1 + ∥Q(c)∥max"
T,0.3575909661229611,ν2tr(Q(c))][ 1
T,0.3588456712672522,δ(l) + ρn∥Q(c)∥max
T,0.3601003764115433,"tr(Q(c))
]"
T,0.3613550815558344,"˜c2(1 −β2)δ(l)(1 −(
1
˜cν2tr(Q(c)))2)
1
mρ2nν4 = O

1
mρ2nν4 
, (35)"
T,0.36260978670012545,where r = |supp(wi)| is the homogeneous coarsening parameter.
T,0.36386449184441655,"Note that C(D, I) = (I −D2)−1, where D can take Anom, ¯Anom, eAnom, or ¯eAnom. Thus, Theorem 5
is effectively bounding the difference of resolvents (zI −D2)−1 evaluated at z = 1. For n = m, we
have r = 1, ν = 1 and both αn and eαn coincide. Lemma 5 is basically a concentration result for the
Gramians of Sﬁne (or Scoarse) and Sﬁne (or Scoarse); however the rate at which the difference goes to
zero is different for ﬁne- and coarse systems. Further, smaller the stability margin 1 −β, looser are
the bounds in Eq. 34 and Eq. 35."
T,0.36511919698870765,"A.3.5
PROOF OF LEMMA 5"
T,0.36637390213299875,"We begin by proving the inequality in Eq. 34. From Eq. 2 and Lemma 1, the limit below exists with
the probability stated in the statement of lemma."
T,0.36762860727728985,"C(Anom, In×n)
≜lim
T →∞CT (Anom, In×n) = limT →∞
PT −1
t=0 (Anom)t(AT
nom)t"
T,0.36888331242158096,"= lim
T →∞
PT −1
t=0 (Anom)2t = (I −A2
nom)−1.
(36)"
T,0.370138017565872,"The last but one equality follows because A is a symmetric matrix, ∥Anom∥2 = ∥
A
c·tr(A)∥2 ≤β < 1
(follows from the lemma’s hypothesis), the last one from the Neumann series formula. Also, we have"
T,0.3713927227101631,"∥¯Anom∥2 = ∥
¯A
c·tr( ¯A)∥2
= ∥
PTQ(c)P
c·tr(PTQ(c)P)∥2 ≤
√"
T,0.3726474278544542,∥PTQ(c)P∥∞∥PTQ(c)P∥1
T,0.3739021329987453,"c·ntr(DQ(c))
≤
1
c·tr(DQ(c)) < 1,
(37)"
T,0.3751568381430364,"since [PTQ(c)P]ℓv ≤1 for all ℓ, v ∈[n]. Similarly"
T,0.37641154328732745,"C( ¯Anom, In×n)
≜lim
T →∞CT ( ¯Anom, In×n) = (I −¯A2
nom)−1."
T,0.37766624843161856,Under review as a conference paper at ICLR 2022
T,0.37892095357590966,"From these observations, we establish the following identity (explanations for each step succeeds the
equations):"
T,0.38017565872020076,"αn
= ∥C(Anom, In×n) −C( ¯Anom, In×n)∥max
= ∥(I −(Anom)2)−1 −(I −( ¯Anom)2)−1∥max
(a)
= ∥(I −Anom)−1(A2
nom −¯A2
nom)(I −¯A2
nom)−1∥max
(b)
≤∥(I −A2
nom)−1∥∞∥A2
nom −( ¯A2
nom∥max∥(I −¯A2
nom)−1∥1
(c)
≤√n∥(I −A2
nom)−1∥2∥A2
nom −¯A2
nom∥max
√n∥(I −¯A2
nom)−1∥2
(d)
≤n
1
1−∥A2nom∥2 ∥A2
nom −¯A2
nom∥max
1
1−∥¯A2nom∥2
(e)
≤n
1
1−β2 ∥A2
nom −¯A2
nom∥max
1
1−
1
c2·tr2(DQ(c))
(f)
≤n
1
1−β2 ∥Anom −¯Anom∥max[∥Anom∥max + ∥¯Anom∥max]
1
1−
1
c2·tr2(DQ(c))
≤n
1
1−β2 ∥
A
c·tr(A) −
¯A
c·tr( ¯A)∥max[
1
cnρnζ(l) + cρn∥Q(c)∥max"
T,0.38143036386449186,"nρntr(DQ(c))]
1
1−
1
c2·tr2(DQ(c))
≤n
1
1−β2
1
c·tr(A)∥A −¯A∥max[1 + n∥¯A∥max"
T,0.38268506900878296,"tr( ¯A) ]
1
cnρn [ 1"
T,0.383939774153074,ζ(l) + ρn∥Q(c)∥max
T,0.3851944792973651,"tr(DQ(c)) ]
1
1−
1
c2·tr2(DQ(c))
(g)
≤n
1
1−β2
1
cnρnζ(l) [1 + nρn∥Q(c)∥max"
T,0.3864491844416562,"ρnntr(DQ(c))]
1
cnρn [ 1"
T,0.3877038895859473,ζ(l) + ρn∥Q(c)∥max
T,0.3889585947302384,"tr(DQ(c)) ]
1
1−
1
c2·tr2(DQ(c))"
T,0.39021329987452946,"≤
[1 + ∥Q(c)∥max"
T,0.39146800501882056,tr(DQ(c))][ 1
T,0.39272271016311167,ζ(l) + ρn∥Q(c)∥max
T,0.39397741530740277,tr(DQ(c)) ]
T,0.39523212045169387,"c2(1 −β2)ζ(l)
h
1 −
1
c2·tr2(DQ(c))
i"
T,0.39648682559598497,"|
{z
}
O(1)"
T,0.397741530740276,"1
ρ2nn"
T,0.3989962358845671,"= O(
1
ρ2nn). (38)"
T,0.4002509410288582,"where (a)-(d),(f) follow the inequalities itemized at the beginning of the appendix; (e) is because of
the assumption in Lemma 1, Eq. 37, and that:"
T,0.4015056461731493,"∥(I −A2
nom)−1∥2 ≤1/(1 −∥A2
nom∥2) ≤1/(1 −∥Anom∥2
2) ≤1/(1 −β2);
(39)"
T,0.4027603513174404,"and the rest of inequalities follow from deﬁnitions of Anom, ¯Anom in the paragraphs processing Eq. 1
and Eq. 11."
T,0.40401505646173147,"The proof for the inequality in Eq. 35 follows similar lines as above, and hence, we provide a sketch,
but not the full details. From Eq. 2, note that the following"
T,0.40526976160602257,"C( eAnom, Im×m)
≜lim
T →∞CT ( eAnom, Im×m) = limT →∞
PT −1
t=0 eAt
nom

eAT
nom
t"
T,0.4065244667503137,"= lim
T →∞
PT −1
t=0 eA2t
nom = (I −eA2
nom)−1,
(40)"
T,0.4077791718946048,"where ∥eAnom∥2 = ∥
e
A
˜c·tr( e
A)∥2 ≤β < 1 (from the lemma’s hypothesis). Moreover, we have"
T,0.4090338770388959,"C( ¯eAnom, Im×m) = (I −¯eA
2"
T,0.410288582183187,"nom)−1, where"
T,0.411543287327478,"∥¯eAnom∥2 = ∥
e¯A
˜ctr( e¯A)∥2 = ∥
√"
T,0.4127979924717691,∥ΦQ(c)ΦT∥∞∥ΦQ(c)ΦT∥1
T,0.41405269761606023,"˜ctr(ΦQ(c)ΦT)
∥2 ≤
√mm
˜cmν2tr(Q(c)) ≤
1
˜cν2tr(Q(c))
(41)"
T,0.41530740276035133,following the fact that [ΦQ(c)ΦT]ij ≤1.
T,0.41656210790464243,Under review as a conference paper at ICLR 2022
T,0.4178168130489335,"Using these observations, we obtain the following inequality by taking similar steps as those for αn
in Eq. 38 (inner steps are removed due to redundancy) :"
T,0.4190715181932246,"˜αn
≜∥C( eAnom, Im×n) −C( ¯eAnom, Im×m)∥max
≤m
1
1−∥( e
A2nom∥2
1
˜ctr( e
A)∥eA −e¯A∥max[1 + m∥e¯A∥max"
T,0.4203262233375157,"tr( e¯A)
][∥eAnom∥max + ∥¯eAnom∥max]
1"
T,0.4215809284818068,"1−∥¯e
A
2
nom∥2
≤m
1
1−β2
1
mρnν2δ(l) [1 + mρn∥Q(c)∥max"
T,0.4228356336260979,"mρnν2tr(Q(c))][
1
˜cmρnν2δ(l) +
ρn∥Q(c)∥max
˜cmρnν2tr(Q(c))]
1"
T,0.424090338770389,"1−∥(
ee¯
A
tr( e¯
A) )2∥2"
T,0.42534504391468003,"≤m
1
1−β2
1
mρnν2δ(l) [1 + ∥Q(c)∥max"
T,0.42659974905897113,"ν2tr(Q(c))][
1
˜cmρnν2δ(l) +
ρn∥Q(c)∥max
˜cmρnν2tr(Q(c))]
1
1−(
1
˜cν2tr(Q(c)) )2"
T,0.42785445420326224,"≤
[1 + ∥Q(c)∥max"
T,0.42910915934755334,ν2tr(Q(c))][ 1
T,0.43036386449184444,δ(l) + ρn∥Q(c)∥max
T,0.4316185696361355,"tr(Q(c))
]"
T,0.4328732747804266,"˜c2(1 −β2)δ(l)(1 −(
1
˜cν2tr(Q(c)))2)
|
{z
}
O(1)"
T,0.4341279799247177,"1
mρ2nν4"
T,0.4353826850690088,"= O

1
mρ2nν4

. (42)"
T,0.4366373902132999,The proof is now complete.
T,0.437892095357591,"A.3.6
PROOF OF THEOREM 1"
T,0.43914680050188204,"The proof of the theorem makes use of Lemma 5. Let i ∈[m], and recall that Bi = diag(wT
i ) and
eBi = rWdiag(wT
i ). From Eq. 10 and Eq. 13, consider the following bound"
T,0.44040150564617314,"∆i(A, eA) ≜
|
rθ(i)
group,A−1
m
X"
T,0.44165621079046424,"i=1
(rθ(i)
group,A −1)
−
θ(i)
coarse, e
A−1
m
X"
T,0.44291091593475534,"i=1
(θ(i)
coarse, e
A −1)
|"
T,0.44416562107904645,"≤|
rθ(i)
group,A −1 m
X"
T,0.4454203262233375,"i=1
(rθ(i)
group,A −1)"
T,0.4466750313676286,"−
rθ(i)
group, ¯A −1 m
X"
T,0.4479297365119197,"i=1
(rθ(i)
group, ¯A −1) |"
T,0.4491844416562108,"|
{z
}
=∆i(A, ¯A)"
T,0.4504391468005019,"+ |
θ(i)
coarse, e
A −1 m
X"
T,0.451693851944793,"i=1
(θ(i)
coarse, e
A −1)"
T,0.45294855708908405,"−
θ(i)"
T,0.45420326223337515,"coarse, e¯A −1 m
X"
T,0.45545796737766625,"i=1
(θ(i)"
T,0.45671267252195735,"coarse, e¯A −1) |"
T,0.45796737766624845,"|
{z
}"
T,0.4592220828105395,"=∆i( e
A, e¯A)"
T,0.4604767879548306,"+ |
rθ(i)
group, ¯A −1 m
X"
T,0.4617314930991217,"i=1
(rθ(i)
group, ¯A −1)"
T,0.4629861982434128,"−
θ(i)"
T,0.4642409033877039,"coarse, ¯e
A −1 m
X"
T,0.465495608531995,"i=1
(θ(i)"
T,0.46675031367628605,"coarse, ¯e
A −1) |"
T,0.46800501882057716,"|
{z
}"
T,0.46925972396486826,"bias=∆i( ¯A), e¯A) |"
T,0.47051442910915936,"≤∆i(A, ¯A) + ∆i( eA, e¯A) + bias. (43)"
T,0.47176913425345046,"The complete proof of the bound on ∆i(A, ¯A) will be elaborated in Thm. 3 and it follows:"
T,0.4730238393977415,"∆i(A, ¯A)"
T,0.4742785445420326,"≤
ζ2
(u)
˜cmincmin||Q(c)||1,1(1 + ζ)"
T,0.4755332496863237,[1 + ∥Q(c)∥max
T,0.4767879548306148,"tr(DQ(c)) ][
1
ζ(l) + ρn∥Q(c)∥max"
T,0.4780426599749059,tr(DQ(c)) ]
T,0.479297365119197,"(1 −β2)ζ(l)
h
1 −
1
c2·tr2(DQ(c)) i "
T,0.48055207026348806,"1 +
K||Q(c)DQ(c)||max"
T,0.48180677540777916,||Q(c)DQ(c)||min(1 −( ||DQ(c)||F
T,0.48306148055207027,tr(DQ(c)) )2)  
T,0.48431618569636137,"|
{z
}
O(1) 1
ρnm"
T,0.48557089084065247,"= O(
1
ρnm)
(44)"
T,0.4868255959849435,Under review as a conference paper at ICLR 2022
T,0.4880803011292346,"We now derive an upper bound on ∆i( eA, e¯A)."
T,0.4893350062735257,"∆i( eA, e¯A) = |
θ(i)
coarse, e
A−1
m
X"
T,0.4905897114178168,"i=1
(θ(i)
coarse, e
A −1)
−
θ(i)"
T,0.4918444165621079,"coarse, e¯
A−1 m
X"
T,0.493099121706399,"i=1
(θ(i)"
T,0.49435382685069007,"coarse, e¯A −1)
|"
T,0.49560853199498117,"≤
1
m
X"
T,0.4968632371392723,"i=1
(θ(i)
coarse, e
A −1)
|θ(i)
coarse, e
A −θ(i)"
T,0.4981179422835634,"coarse, e¯A + m
X"
T,0.4993726474278545,"i=1
(θ(i)
coarse, e
A −θ(i)"
T,0.5006273525721455,"coarse, e¯A) m
X"
T,0.5018820577164367,"i=1
(θ(i)"
T,0.5031367628607277,"coarse, e¯A −1)
[θ(i)"
T,0.5043914680050188,"coarse, e¯A −1]|"
T,0.5056461731493099,"≤
1
m
X"
T,0.506900878293601,"i=1
(θ(i)
coarse, e
A −1)
||θcoarse, e
A −θcoarse, e¯A||max "
T,0.5081555834378921,"
1 + |
m
m
X"
T,0.5094102885821832,"i=1
(θ(i)"
T,0.5106649937264742,"coarse, e¯A −1)
[θ(i)"
T,0.5119196988707654,"coarse, e¯A −1]|  "
T,0.5131744040150564,"≤
1
m
X"
T,0.5144291091593476,"i=1
(θ(i)
coarse, e
A −1)
˜αn "
T,0.5156838143036386,"
1 +
m||θcoarse, e¯
A−1||max
m
X"
T,0.5169385194479298,"i=1
(θ(i)"
T,0.5181932245922208,"coarse, e¯A −1) "
T,0.5194479297365119,"
, (45)"
T,0.520702634880803,"From the equality after Eq. 40, we have θ(i)"
T,0.5219573400250941,"coarse, e¯A = diagi
 
(I −¯A2
nom)−1
(46)"
T,0.5232120451693852,"Furthermore, since ΦQ(c)ΦT ≤11T then (ΦQ(c)ΦT)2 ≤11T11T = m11T"
T,0.5244667503136763,"||θcoarse, e¯A −1||max
= ∥(I −¯A2
nom)−1 ¯A2
nom∥max
≤
1
tr2(ΦQ(c)ΦT)∥(I −¯A2
nom)−1m11T∥max
≤
m
tr2(ΦQ(c)ΦT)∥(I −¯A2
nom)−1∥∞
≤
m√m
tr2(ΦQ(c)ΦT)∥(I −¯A2
nom)−1∥2
≤
m√m
(mρnν2tr(Q(c))2 ∥(I −¯A2
nom)−1∥2
≤
1
√mρ2nν4tr2(Q(c))(1−(
1
˜cν2tr(Q(c)) )2). (47)"
T,0.5257214554579673,"Similarly m
X"
T,0.5269761606022585,"i=1
(θ(i)"
T,0.5282308657465495,"coarse, e¯A −1)
= tr[(I −¯eA
2"
T,0.5294855708908407,nom)−1 −I]
T,0.5307402760351317,"= tr[ ¯eA
2"
T,0.5319949811794228,"nom(I −¯eA
2"
T,0.533249686323714,nom)−1]
T,0.534504391468005,"≥tr[ ¯eA
2 nom]"
T,0.5357590966122961,"≥
tr[(ΦQ(c)ΦT)
2]
tr2[ΦQ(c)ΦT]
≥m2ν3˜cmintr[(Q(c))2]"
T,0.5370138017565872,(˜cmtr[Q(c)])2
T,0.5382685069008782,= ν3˜cmintr[(Q(c))2]
T,0.5395232120451694,˜c2tr2[Q(c)] (48)
T,0.5407779171894604,"where all inequalities follow well-known matrix norm axiom. The last line in Eq. 48 is based on
Assumption 2, and"
T,0.5420326223337516,"tr[
 
ΦQ(c)ΦT2]
≥ν˜cminmtr[Φ(Q(c))2ΦT]
≥ν˜cminmmν2tr[(Q(c))2]
= m2ν3˜cmintr[(Q(c))2]
(49)"
T,0.5432873274780426,Under review as a conference paper at ICLR 2022
T,0.5445420326223338,similar to the derivation of Eq. 27 since for all k ∈[K]:
T,0.5457967377666249,"[ΦTΦ]kk =
X"
T,0.5470514429109159,"i∈[m]
Φ2
ik ≥ν
X"
T,0.548306148055207,"i∈[m]
Φik ≥ν˜cminm
(50)"
T,0.5495608531994981,"We use the deﬁnition of θcoarse, e
A in Eq. 13: | m
X"
T,0.5508155583437893,"i=1
(θ(i)
coarse, e
A −1)|
= m
X"
T,0.5520702634880803,"i=1
diagi

(I −eA2
nom)−1 −I
"
T,0.5533249686323714,"= tr

(I −eA2
nom)−1 −I
"
T,0.5545796737766625,"≥tr( eA2
nom)
≥tr( e
A2)
tr2( e
A)
(a)
≥m2ρn||Q(c)||1,1"
T,0.5558343789209536,"r2
˜c2
min(1+δ)
(mρnδ(u))2"
T,0.5570890840652447,"=
1
ρnr2
˜c2
min(1+δ)||Q(c)||1,1"
T,0.5583437892095358,"δ(u))2
, (51)"
T,0.5595984943538268,where the inequality (a) in Eq. 51 comes from
T,0.560853199498118,"tr( eA2)
=
X"
T,0.562107904642409,"i,j
eA2
ij =
X i,j  1 r2
X"
T,0.5633626097867002,"v∈Ki,ℓ∈Kj
Avℓ   2"
T,0.5646173149309912,"=
1
r4
X i,j X"
T,0.5658720200752823,"v,v′∈Ki,ℓ,ℓ′∈Kj
AvℓAv′ℓ′"
T,0.5671267252195734,"=
1
r4  
X"
T,0.5683814303638645,"v∈∪iKi, ℓ∈∪jKj"
T,0.5696361355081556,"Avℓ+
X"
T,0.5708908406524467,v̸=v′∈∪iKi or ℓ̸= ℓ′ ∈∪jKj
T,0.5721455457967378,AvℓAv′ℓ′  
T,0.5734002509410289,"≥
1
r2
X"
T,0.5746549560853199,"i,j
wT
i Awj"
T,0.5759096612296111,"=
1
r2 1TWAWT1
≥
1
r2
1TΦ
|{z}
≥˜cminm1K
Q
ΦT1
|{z}
≥˜cminm1T
K"
T,0.5771643663739021,(1 + δ)
T,0.5784190715181933,"≥m2ρn||Q(c)||1,1"
T,0.5796737766624843,"r2
˜c2
min(1 + δ), (52)"
T,0.5809284818067754,where the event deﬁned in Eq. 30 is used whose corresponding probability is Eq. 31.
T,0.5821831869510665,"Replacing Eq. 47, Eq. 48, Eq. 51, and Eq. 42 into Eq. 45 yields:"
T,0.5834378920953576,"∆i( eA, e¯A)
≤ρnr2
δ(u))2"
T,0.5846925972396487,"˜c2
min(1+δ)||Q(c)||1,1 ˜αn  1 +"
T,0.5859473023839398,"m
1
√mρ2nν4tr2(Q(c))(1−(
1
˜cν2tr(Q(c))
)2)"
T,0.5872020075282308,ν3 ˜cmintr[(Q(c))2]
T,0.588456712672522,˜c2tr2[Q(c)]  
T,0.589711417816813,"= O

ρnr2√m"
T,0.5909661229611042,"mρ2
nρ2
n "
T,0.5922208281053952,"= O

r2
√mρ3
n  (53)"
T,0.5934755332496863,The statement of the theorem follows by invoking Eq. 44 and Eq. 53 into Eq. 43:
T,0.5947302383939774,"∆i(
1
tr(A)A,
1
tr( e
A) eA)
= bias + O

1
ρnm +
r2
√mρ3n"
T,0.5959849435382685,"
.
(54)"
T,0.5972396486825596,with a joint probability of at least
T,0.5984943538268507,"1 −2 exp

−2nρ2
n(tr(DQ(c))ζ)2
−exp

−2ρ2
nmn(˜cmincmin||Q(c)||1,1ζ)2"
T,0.5997490589711418,"−2 exp (−2m(ρnν2tr(Q(c))δ)2) −exp

−2ρ2
nm2(˜c2
min||Q(c)||1,1δ)2
(55)"
T,0.6010037641154329,per Lemma 3. We then get the minimum of the four exponents in Eq. 70. The proof is now complete.
T,0.6022584692597239,Under review as a conference paper at ICLR 2022
T,0.6035131744040151,"A.4
PROOFS FOR SECTION 6: LEARNING APPROACH FOR GROUP AVERAGE
CONTROLLABILITY"
T,0.6047678795483061,"A.4.1
PROOF OF LEMMA 2"
T,0.6060225846925973,"We start by deﬁning θﬁne,M similar to Eq. 10 and Eq. 13 (M will be later substituted with A and ¯A),"
T,0.6072772898368883,"θT
ﬁne,M ≜[tr[C(Mnom, e1)]
. . .
tr[C(Mnom, en)]] ∈Rn×1.
(56)
Using the Gramian deﬁnition in Eq. 2, we have:"
T,0.6085319949811794,"θ(i)
ﬁne,M
= tr [C(Mnom, ei)] = tr "" ∞
X"
T,0.6097867001254705,"τ=0
Mτ
nomeieT
i Mτ
nom # = ∞
X"
T,0.6110414052697616,"τ=0
tr(Mτ
nomeieT
i Mτ
nom) = ∞
X"
T,0.6122961104140527,"τ=0
tr(eT
i M2τ
nomei) = ∞
X"
T,0.6135508155583438,"τ=0
diagi(M2τ
nom)"
T,0.6148055207026348,"= diagi ∞
X"
T,0.616060225846926,"τ=0
M2τ
nom !"
T,0.617314930991217,"= diagi
 
(I −M2
nom)−1
. (57)"
T,0.6185696361355082,"We ﬁrst simplify the term θﬁne, ¯A in Eq. 60, by substituting ¯A with PTQP introduced prior to Eq. 6:"
T,0.6198243412797992,"θﬁne, ¯A
= "
T,0.6210790464240903,"
tr[C( ¯Anom, e1)]
. . .
tr[C( ¯Anom, en)] "
T,0.6223337515683814,"= diag ∞
X"
T,0.6235884567126725,"τ=0
¯A2τ
nom !"
T,0.6248431618569636,"= diag( ∞
X"
T,0.6260978670012547,"τ=0
(
1
tr(PTQP)PTQP)2τ) = diag( ∞
X"
T,0.6273525721455459,"τ=0
(
1
ntr(DQ(c))PTQ(c)P)2τ)"
T,0.6286072772898369,"= diag(I + (
1
ntr(DQ(c)))2PTQ(c)PPTQ(c)P + (
1
ntr(DQ(c)))4PTQ(c)PPTQ(c)P + · · · )"
T,0.6298619824341279,= diag(I + 1
T,0.6311166875784191,"n(
1
tr(DQ(c)))2PTQ(c) 1 nPPT"
T,0.6323713927227101,| {z } ≜D Q(c)P + 1
T,0.6336260978670013,"n(
1
tr(DQ(c)))4PTQ(c) 1 nPPT"
T,0.6348808030112923,"| {z }
Q(c) 1 nPPT"
T,0.6361355081555834,"| {z }
Q(c) 1 nPPT"
T,0.6373902132998746,"| {z }
Q(c)P + · · · )"
T,0.6386449184441656,"= 1n +
1
ntr(DQ(c))diag(PTQ(c)
DQ(c)"
T,0.6398996235884568,"tr(DQ(c))P +
1
tr(A)PTQ(c)
DQ(c)"
T,0.6411543287327478,"tr(DQ(c))
DQ(c)"
T,0.6424090338770388,"tr(DQ(c))
DQ(c)"
T,0.64366373902133,tr(DQ(c))P + · · · )
T,0.644918444165621,"= 1n +
1
ntr(DQ(c))diag(PT Q(c)DQ(c)"
T,0.6461731493099122,"tr(DQ(c)) [I + (
DQ(c)"
T,0.6474278544542033,"tr(DQ(c)))2 + · · · ]
|
{z
}"
T,0.6486825595984943,"≜Υ(Q(c),D) P)"
T,0.6499372647427855,"= 1n +
1
ntr(DQ(c))diag(PTΥ(Q(c), D)P)"
T,0.6511919698870765,"(a)= 1n +
1
ntr(DQ(c))(P ◦P)Tdiag(Υ(Q(c), D))"
T,0.6524466750313677,"(b)= 1n +
1
ntr(DQ(c))PTdiag(Υ),
(58)
where (a) is due to the special structure of P Eq. 4 since for an arbitrary matrix M of appropriate
size:
diagi(PTMP)
=
X"
T,0.6537013801756587,"k,k′∈[K]
PkiMk,k′Pk′i =
X"
T,0.6549560853199499,"k,k′∈[K]
PkiMk,k′Pk′i =
X"
T,0.6562107904642409,"k∈[K]
PkiMk,kPki =
X"
T,0.657465495608532,"k∈[K]
P2
kiMk,k"
T,0.6587202007528231,"= (Pi ◦Pi)diag(M), (59)"
T,0.6599749058971142,Under review as a conference paper at ICLR 2022
T,0.6612296110414053,and (b) is true because P is binary. Replacing Eq. 58 into Eq. 60 yields:
T,0.6624843161856964,"θgroup, ¯A
= 1"
T,0.6637390213299874,"rW

1n +
1
ntr(DQ(c))PTdiag(Υ))

= 1"
T,0.6649937264742786,"r

1m +
1
ntr(DQ(c))Φdiag(Υ))

(60)"
T,0.6662484316185696,The proof is now complete.
T,0.6675031367628608,"A.4.2
THEOREM 3 AND PROOF"
T,0.6687578419071518,Deﬁne the error metric similar to Eq. 14:
T,0.6700125470514429,"∆i(A, ¯A) ≜ "
T,0.671267252195734,"rθ(i)
group,A −1
Pm
i=1[rθ(i)
group,A −1]
−
rθ(i)
group, e
A −1
Pm
i=1[rθ(i)
group, ¯A −1]"
T,0.6725219573400251,",
for all i ∈[m].
(61)"
T,0.6737766624843162,"Theorem 3. (Component wise error bound between θgroup,A and θgroup, ¯A): Let ∆i(A, ¯A) be
deﬁned as above and ν be the resolution parameter given by Eq. 7. Under the assumptions stated in
Section 3 and Lemma 1, the following holds:"
T,0.6750313676286073,"∆i(A, ¯A) ≤
ζ2
(u)
˜cmincmin||Q(c)||1,1(1 + ζ)"
T,0.6762860727728983,[1 + ∥Q(c)∥max
T,0.6775407779171895,"tr(DQ(c)) ][
1
ζ(l) + ρn∥Q(c)∥max"
T,0.6787954830614805,tr(DQ(c)) ]
T,0.6800501882057717,"(1 −β2)ζ(l)
h
1 −
1
c2·tr2(DQ(c)) i "
T,0.6813048933500627,"1 +
K||Q(c)DQ(c)||max"
T,0.6825595984943539,||Q(c)DQ(c)||min(1 −( ||DQ(c)||F
T,0.6838143036386449,tr(DQ(c)) )2)  
T,0.685069008782936,"|
{z
}
O(1) 1
ρnm (62)"
T,0.6863237139272271,"with probability at least 1 −3 exp
 
−2ˆκ(Q(c), D, m, n, ρn)

, where Further, for a constant 0 <
ζ < 1, the exponent is ˆκ(Q(c), D, m, n, ρn) = min
n
nρ2
n(tr(DQ(c))ζ)2, mnρ2
n(˜cmincmin||Q(c)||1,1ζ)2o
."
T,0.6875784190715182,"A.4.3
PROPOSITION 4 AND PROOF"
T,0.6888331242158093,"Proposition 4. (Group Average Controllability for ¯A) The group average controllability vector for
¯A is"
T,0.6900878293601004,"θ(i)
group,M = 1"
T,0.6913425345043914,"r wiθﬁne,M.
(63)"
T,0.6925972396486826,"Proof. We begin by simplifying the group average controllability using its deﬁnition in Eq. 10 and
the deﬁnition of Gramian in Eq. 2, for a general matrix notation M which can be replaced by either
A or ¯A:"
T,0.6938519447929736,"θ(i)
group,M
= tr

C(Mnom, diag(wT
i ))
 = tr "" ∞
X"
T,0.6951066499372648,"τ=0
Mτ
nomdiag(wT
i )diag(wT
i )TMτ
nom # = tr "" ∞
X"
T,0.6963613550815558,"τ=0
Mτ
nomdiag((wi ◦wi)T)Mτ
nom #"
T,0.6976160602258469,"(a)=
1
r2 tr  
∞
X"
T,0.698870765370138,"τ=0
Mτ
nom(
X"
T,0.7001254705144291,"v∈supp(wi)
eveT
v )Mτ
nom  "
T,0.7013801756587202,"=
1
r2
X"
T,0.7026348808030113,"v∈supp(wi)
tr "" ∞
X"
T,0.7038895859473023,"τ=0
Mτ
nomeveT
v Mτ
nom #"
T,0.7051442910915935,"=
1
r2
X"
T,0.7063989962358845,"v∈supp(wi)
tr "" ∞
X"
T,0.7076537013801757,"τ=0
Mτ
nomeveT
v Mτ
nom #"
T,0.7089084065244667,"|
{z
}"
T,0.7101631116687579,"θ(v)
ﬁne,M , (64)"
T,0.7114178168130489,"where (a) is due to the assumption of r-homogeneous W, ◦denotes the Hadamard product, and we
have already deﬁned θﬁne,M in Eq. 56. Putting Eq. 64 in vector form concludes the proof."
T,0.71267252195734,Under review as a conference paper at ICLR 2022
T,0.7139272271016311,"A.4.4
PROOF OF THEOREM 3"
T,0.7151819322459222,"Proof. We start by substituting M into Eq. 63, from Proposition 4, with A and ¯A, yields :"
T,0.7164366373902133,"∆i(A, ¯A) = |
rθ(i)
group,A−1
m
X"
T,0.7176913425345044,"i=1
(rθ(i)
group,A −1)
−
rθ(i)
group, ¯
A−1
m
X"
T,0.7189460476787954,"i=1
(rθ(i)
group, ¯A −1)
|"
T,0.7202007528230866,"= |
wiθﬁne,A−1
m
X"
T,0.7214554579673776,"i=1
(wiθﬁne,A −1)
−
wiθﬁne, ¯
A−1
m
X"
T,0.7227101631116688,"i=1
(wiθﬁne, ¯A −1)
|"
T,0.7239648682559598,"=
1
m
X"
T,0.7252195734002509,"i=1
(wiθﬁne,A −1)
|wiθﬁne,A −1 − m
X"
T,0.726474278544542,"i=1
(wiθﬁne,A −1) m
X"
T,0.7277289836888331,"i=1
(wiθﬁne, ¯A −1)
[wiθﬁne, ¯A −1]|"
T,0.7289836888331243,"=
1
m
X"
T,0.7302383939774153,"i=1
(wiθﬁne,A −1)
|wi(θﬁne,A −θﬁne, ¯A) −   m
X"
T,0.7314930991217063,"i=1
(wiθﬁne,A −1) m
X"
T,0.7327478042659975,"i=1
(wiθﬁne, ¯A −1)
−1 "
T,0.7340025094102886,"
[wiθﬁne, ¯A −1]|"
T,0.7352572145545797,"≤
1
m
X"
T,0.7365119196988708,"i=1
(wiθﬁne,A −1) "
T,0.7377666248431619,"
|wi(θﬁne,A −θﬁne, ¯A)| + | m
X"
T,0.739021329987453,"i=1
wi(θﬁne,A −θﬁne, ¯A) m
X"
T,0.740276035131744,"i=1
(wiθﬁne, ¯A −1)
[wiθﬁne, ¯A −1]|  "
T,0.7415307402760352,"≤
1
m
X"
T,0.7427854454203262,"i=1
(wiθﬁne,A −1) "
T,0.7440401505646174,"
|wi(θﬁne,A −θﬁne, ¯A)| + | m
X"
T,0.7452948557089084,"i=1
wi(θﬁne,A −θﬁne, ¯A) m
X i=1"
T,0.7465495608531995,"1
ntr(DQ(c))Φ(i)diag(Υ)"
T,0.7478042659974906,"1
ntr(DQ(c))Φ(i)diag(Υ)|  "
T,0.7490589711417817,"≤
1
m
X"
T,0.7503136762860728,"i=1
(wiθﬁne,A −1) "
T,0.7515683814303639,"
|wi(θﬁne,A −θﬁne, ¯A)| + | m
X"
T,0.7528230865746549,"i=1
wi(θﬁne,A −θﬁne, ¯A) m
X"
T,0.7540777917189461,"i=1
Φ(i)diag(Υ)
Φ(i)diag(Υ)|  "
T,0.7553324968632371,"≤
1
m
X"
T,0.7565872020075283,"i=1
(wiθﬁne,A −1)"
T,0.7578419071518193,"h
||θﬁne,A −θﬁne, ¯A||max +
m||θﬁne,A−θﬁne, ¯
A||max
m||Υ||min
||Υ||max
i"
T,0.7590966122961104,"≤
1
m
X"
T,0.7603513174404015,"i=1
(wiθﬁne,A −1)
||θﬁne,A −θﬁne, ¯A||max
h
1 + ||Υ||max"
T,0.7616060225846926,||Υ||min i
T,0.7628607277289837,"≤
1
m
X"
T,0.7641154328732748,"i=1
(wiθﬁne,A −1)
αn
h
1 + ||Υ||max"
T,0.7653701380175659,"||Υ||min i
, (65)"
T,0.766624843161857,"where (a) is due to Cauchy-Schwartz inequality, αn is deﬁned in Eq. 34, θﬁne, ¯A is substituted from
Eq. 58, and (b) is the result of the properties of the coarsening matrix in Deﬁnition 2. We use the"
T,0.767879548306148,Under review as a conference paper at ICLR 2022
T,0.7691342534504392,"deﬁnition of θﬁne,A in Eq. 57: m
X"
T,0.7703889585947302,"i=1
(wiθﬁne,A −1)
= m
X"
T,0.7716436637390214,"i=1
widiag
 
(I −A2
nom)−1 −I
"
T,0.7728983688833124,"= 1mWdiag
 
(I −A2
nom)−1 −I
"
T,0.7741530740276035,"= 1mWdiag
 
A2
nom
"
T,0.7754077791718946,"= 1mWdiag
 
A2
nom
"
T,0.7766624843161857,"=
1
c2·tr2(A)1mWdiag
 
A2"
T,0.7779171894604768,"=
1
c2·tr2(A)1mWA1n
(a)
≥
˜cmincminρnmn||Q(c)||1,1(1+ζ)"
T,0.7791718946047679,c2(ζ(u)ρnn)2
T,0.7804265997490589,"= ˜cmincmin||Q(c)||1,1(1+ζ)"
T,0.7816813048933501,"c2ζ2
(u)
m
ρnn. (66)"
T,0.7829360100376411,"inequality (a) uses the event deﬁned in Eq. 32 is used whose corresponding probability is Eq. 33.
Using the deﬁnition of Υ in Eq. 16"
T,0.7841907151819323,"||Υ||max
≤|| Q(c)DQ(c)"
T,0.7854454203262233,"tr(DQ(c)) (I −(
DQ(c)"
T,0.7867001254705144,tr(DQ(c)))2)−1||max
T,0.7879548306148055,"≤
K||Q(c)DQ(c)||max"
T,0.7892095357590966,tr(DQ(c))(1−( ||DQ(c)||F
T,0.7904642409033877,"tr(DQ(c)) )2)
= O(1) (67)"
T,0.7917189460476788,"||Υ||min
≥|| Q(c)DQ(c)"
T,0.7929736511919699,"tr(DQ(c)) (I −(
DQ(c)"
T,0.794228356336261,tr(DQ(c)))2)−1||min
T,0.795483061480552,≥||Q(c)DQ(c)||min
T,0.7967377666248432,"tr(DQ(c))
= Ω(1), (68)"
T,0.7979924717691342,"where Ω(.) is the opposite scaling of O(.); we write f(m) = Ω(h(m)) iff there exist positive reals
c0 and m0 such that |f(n)| ≥c0h(m) for all m ≥m0."
T,0.7992471769134254,We substitute Eq. 66 and Eq. 34 (with probability Eq. 24) into Eq. 65
T,0.8005018820577164,"∆i(A, ¯A)
≤
ζ2
(u)
˜cmincmin||Q(c)||1,1(1+ζ)"
T,0.8017565872020075,[1+ ∥Q(c)∥max
T,0.8030112923462986,"tr(DQ(c)) ][
1
ζ(l) + ρn∥Q(c)∥max"
T,0.8042659974905897,"tr(DQ(c))
]"
T,0.8055207026348808,(1−β2)ζ(l)
T,0.8067754077791719,"
1−
1
c2·tr2(DQ(c))  """
T,0.8080301129234629,"1 +
K||Q(c)DQ(c)||max"
T,0.8092848180677541,||Q(c)DQ(c)||min(1−( ||DQ(c)||F
T,0.8105395232120451,"tr(DQ(c)) )2) # 1
ρnm (69)"
T,0.8117942283563363,with a joint probability of at least
T,0.8130489335006273,"1 −2 exp

−2nρ2
n(tr(DQ(c))ζ)2
−exp

−2ρ2
nmn(˜cmincmin||Q(c)||1,1ζ)2
(70)"
T,0.8143036386449184,per Lemma 3. We then get the minimum of the two exponents in Eq. 70 which concludes the proof.
T,0.8155583437892095,"A.4.5
PROOF OF THEOREM 2"
T,0.8168130489335006,We start by using the triangle inequality for absolute values:
T,0.8180677540777918,"b∆i( eA) = |
ˆθ(i)
group−1
m
X"
T,0.8193224592220828,"i=1
(ˆθ(i)
group −1)
−
rθ(i)
group,A−1
m
X"
T,0.820577164366374,"i=1
(rθ(i)
group,A −1)
|"
T,0.821831869510665,"≤|
rθ(i)
group, ¯A −1 m
X"
T,0.823086574654956,"i=1
(rθ(i)
group, ¯A −1)"
T,0.8243412797992472,"−
rθ(i)
group,A −1 m
X"
T,0.8255959849435383,"i=1
(rθ(i)
group,A −1) |"
T,0.8268506900878294,"|
{z
}
∆i(A, ¯A)"
T,0.8281053952321205,"+ |
ˆθ(i)
group −1
m
X"
T,0.8293601003764115,"i=1
(ˆθ(i)
group −1)"
T,0.8306148055207027,"−
rθ(i)
group, ¯A −1 m
X"
T,0.8318695106649937,"i=1
(rθ(i)
group, ¯A −1) |"
T,0.8331242158092849,"|
{z
}"
T,0.8343789209535759,"≜b∆i( ˜A, ¯A)
(71)"
T,0.835633626097867,Under review as a conference paper at ICLR 2022
T,0.8368883312421581,"The ﬁrst term on the RHS of Eq. 71 has already been bounded in Thm.3. Next, we bound the second
term in Eq. 71 and combine the two bounds at the end."
T,0.8381430363864492,"We substitute ˆθ(i)
group from the output of Algorithm 1, and θ(i)
group, ¯A from Eq. 60, for Υ deﬁned in"
T,0.8393977415307403,"Eq. 16. For notation simplicity we write Υ(Q(c), D) as Υ and Υ( ˆQ(c), ˆD) as ˆΥ yields"
T,0.8406524466750314,"b∆i( ˜A, ¯A) = |
ˆθ(i)
group−1
m
X"
T,0.8419071518193224,"i=1
(ˆθ(i)
group −1)
−
rθ(i)
group, ¯
A−1
m
X"
T,0.8431618569636136,"i=1
(rθ(i)
group, ¯A −1)
| = "
T,0.8444165621079046,"ˆΦ(i)diag( ˆΥ)
m
X"
T,0.8456712672521958,"i=1
ˆΦ(i)diag(ˆΥ)
−
Φ(i)diag(Υ)
m
X"
T,0.8469259723964868,"i=1
Φ(i)diag(Υ) "
T,0.848180677540778,"=
1
m
X"
T,0.849435382685069,"i=1
ˆΦ(i)diag(ˆΥ) "
T,0.8506900878293601,"ˆΦ(i)diag(ˆΥ) − m
X"
T,0.8519447929736512,"i=1
ˆΦ(i)diag(ˆΥ) m
X"
T,0.8531994981179423,"i=1
Φ(i)diag(Υ)
Φ(i)diag(Υ)  . (72)"
T,0.8544542032622334,"We set ˆΦ(i) = Φ(i) + EΦ, ˆQ(c) = Q(c) + EQ, ˆD = D + ED, and ˆΥ = Υ + ¯E where EΦ, EQ and
ED are error matrices of appropriate sizes. Substitution of theses error matrices into Eq. 72, as well
as multiple applications of the triangle inequality, gives:
b∆i( ˜A, ¯A)"
T,0.8557089084065245,"=
1
m
X"
T,0.8569636135508155,"i=1
ˆΦ(i)diag(ˆΥ) "
T,0.8582183186951067,"(Φ(i) + EΦ)diag(Υ + ¯E) − m
X"
T,0.8594730238393977,"i=1
(Φ(i) + EΦ)diag(Υ + ¯E) m
X"
T,0.8607277289836889,"i=1
Φ(i)diag(Υ)
Φ(i)diag(Υ) "
T,0.8619824341279799,"=
1
m
X"
T,0.863237139272271,"i=1
ˆΦ(i)diag(ˆΥ) "
T,0.8644918444165621,"(Φ(i) + EΦ)[diag(Υ) + diag( ¯E)] − m
X"
T,0.8657465495608532,"i=1
(Φ(i) + EΦ)[diag(Υ) + diag( ¯E)] m
X"
T,0.8670012547051443,"i=1
Φ(i)diag(Υ)
Φ(i)diag(Υ) "
T,0.8682559598494354,"=
1
m
X"
T,0.8695106649937264,"i=1
ˆΦ(i)diag(ˆΥ) "
T,0.8707653701380176,"Φ(i)diag( ¯E) + EΦ[diag(Υ) + diag( ¯E)] − m
X"
T,0.8720200752823086,"i=1
Φ(i)diag( ¯E) + mEΦ[diag(Υ) + diag( ¯E)] m
X"
T,0.8732747804265998,"i=1
Φ(i)diag(Υ)
Φ(i)diag(Υ) "
T,0.8745294855708908,"=
1
m
X"
T,0.875784190715182,"i=1
ˆΦ(i)diag(ˆΥ) "
T,0.877038895859473,"[Φ(i) + EΦ]diag( ¯E) + [EΦ − m
X"
T,0.8782936010037641,"i=1
Φ(i)diag( ¯E) + mEΦ[diag(Υ) + diag( ¯E)] m
X"
T,0.8795483061480552,"i=1
Φ(i)diag(Υ)
Φ(i)]diag(Υ) "
T,0.8808030112923463,"≤
1
m
X"
T,0.8820577164366374,"i=1
ˆΦ(i)diag(ˆΥ) "
T,0.8833124215809285,"
(1 + ||EΦ||1)|| ¯E||max + [||EΦ||1 + m
X"
T,0.8845671267252195,"i=1
Φ(i)diag( ¯E) + mEΦ[diag(Υ) + diag( ¯E)] m
X"
T,0.8858218318695107,"i=1
Φ(i)diag(Υ)
]||Υ||max "
T,0.8870765370138017,"
."
T,0.8883312421580929,"(73)
To further simplify Eq. 73, we ﬁnd upper bounds on the terms inside. The two terms ||Υ||max and
||Υ||min have already been bounded in Eq. 67 and Eq. 68 and we have:"
"M
X",0.8895859473023839,"1
m
X"
"M
X",0.890840652446675,"i=1
ˆΦ(i)diag(ˆΥ)
≤
1
m||diag( ˆΥ)||min"
"M
X",0.8920953575909661,"≤
1
m|| ˆΥ||min
= O( 1 m) (74)"
"M
X",0.8933500627352572,"The following inequality holds from the deﬁnition of norms:
||EΦ||1 ≤K||EΦ||max
(75)"
"M
X",0.8946047678795483,Under review as a conference paper at ICLR 2022
"M
X",0.8958594730238394,"Using Eq. 74 and Eq. 75, the inner term in the last line of Eq. 73 is simpliﬁed as: m
X"
"M
X",0.8971141781681304,"i=1
Φ(i)diag( ¯E) + mEΦ[diag(Υ) + diag( ¯E)] m
X"
"M
X",0.8983688833124216,"i=1
Φ(i)diag(Υ)
= m
X"
"M
X",0.8996235884567126,"i=1
Φ(i)diag( ¯E) m
X"
"M
X",0.9008782936010038,"i=1
Φ(i)diag(Υ)
+ m EΦ[diag(Υ)+diag( ¯
E)]
m
X"
"M
X",0.9021329987452948,"i=1
Φ(i)diag(Υ)"
"M
X",0.903387703889586,"≤m||diag( ¯
E)||max
m||diag(Υ)||min + mK ||EΦ||max[||diag(Υ)||max+||diag( ¯
E)||max]
m||diag(Υ)||min
≤|| ¯
E||max
||Υ||min + K ||EΦ||max[||Υ||max+|| ¯
E||max]
||Υ||min
= O(||EΦ||max + || ¯E||max)
(76)"
"M
X",0.904642409033877,"Replacing Eq. 67, Eq. 68, Eq. 74, Eq. 75, and Eq. 76 into Eq. 73 simpliﬁes it as:"
"M
X",0.9058971141781681,"|
ˆθ(i)
group−1
m
X"
"M
X",0.9071518193224593,"i=1
(ˆθ(i)
group −1)
−
rθ(i)
group, ¯
A−1
m
X"
"M
X",0.9084065244667503,"i=1
(rθ(i)
group, ¯A −1)
| = O( 1"
"M
X",0.9096612296110415,"m[||EΦ||max + || ¯E||max]).
(77)"
"M
X",0.9109159347553325,We now simplify the term || ¯E||max in Eq. 79:
"M
X",0.9121706398996235,"|| ¯E||max
= ||ˆΥ −Υ||max
= ||
ˆQ(c) ˆD ˆQ(c)"
"M
X",0.9134253450439147,"tr( ˆD ˆQ(c)) (I −(
ˆD ˆQ(c)"
"M
X",0.9146800501882058,tr( ˆD ˆQ(c)))2)−1 −Q(c)DQ(c)
"M
X",0.9159347553324969,"tr(DQ(c)) (I −(
DQ(c)"
"M
X",0.917189460476788,tr(DQ(c)))2)−1||max
"M
X",0.918444165621079,"=
1
tr( ˆD ˆQ(c))|| ˆQ(c) ˆD ˆQ(c)(I −(
ˆD ˆQ(c)"
"M
X",0.9196988707653702,tr( ˆD ˆQ(c)))2)−1 −tr( ˆD ˆQ(c))
"M
X",0.9209535759096612,"tr(DQ(c))Q(c)DQ(c)(I −(
DQ(c)"
"M
X",0.9222082810539524,tr(DQ(c)))2)−1||max. (78)
"M
X",0.9234629861982434,"To continue the simpliﬁcation of || ¯E||max, we bring the two error matrices EQ and ED introduced in
the statement of the theorem into play:"
"M
X",0.9247176913425345,"(Q(c) + EQ)
(D + ED)(Q(c) + EQ) = (Q(c) + EQ)(DQ(c) + DEQ + EDQ(c) + EDEQ)
= Q(c)DQ(c) + Γ,
(79)"
"M
X",0.9259723964868256,where we deﬁne
"M
X",0.9272271016311167,"Γ ≜Q(c)DEQ + Q(c)EDQ(c) + Q(c)EDEQ + EQDQ(c) + EQDEQ + EQEDQ(c) + EQEDEQ
(80) and"
"M
X",0.9284818067754078,"(D + ED)
(Q(c) + EQ) = DQ(c) + DEQ + EDQ(c) + EDEQ
|
{z
}
E"
"M
X",0.9297365119196989,".
(81)"
"M
X",0.93099121706399,Replacing Eq. 79 and Eq. 81 into Eq. 78 yields:
"M
X",0.9322459222082811,"|| ¯E||max
=
1
tr( ˆD ˆQ(c))||(Q(c)DQ(c) + Γ)(I −(
ˆD ˆQ(c)"
"M
X",0.9335006273525721,tr( ˆD ˆQ(c)))2)−1 −tr(DQ(c)+E)
"M
X",0.9347553324968633,"tr(DQ(c)) Q(c)DQ(c)(I −(
DQ(c)"
"M
X",0.9360100376411543,tr(DQ(c)))2)−1||max
"M
X",0.9372647427854455,"=
1
tr( ˆD ˆQ(c))||Q(c)DQ(c)[(I −(
ˆD ˆQ(c)"
"M
X",0.9385194479297365,"tr( ˆD ˆQ(c)))2)−1 −(I −(
DQ(c)"
"M
X",0.9397741530740276,tr(DQ(c)))2)−1]
"M
X",0.9410288582183187,"+Γ(I −(
ˆD ˆQ(c)"
"M
X",0.9422835633626098,"tr( ˆD ˆQ(c)))2)−1 −
tr(E)
tr(DQ(c))Q(c)DQ(c)(I −(
DQ(c)"
"M
X",0.9435382685069009,tr(DQ(c)))2)−1||max
"M
X",0.944792973651192,"=
1
tr( ˆD ˆQ(c))[||Q(c)DQ(c)[ ∞
X"
"M
X",0.946047678795483,"ℓ=1
(
ˆD ˆQ(c)"
"M
X",0.9473023839397742,"tr( ˆD ˆQ(c))
)2ℓ−(
DQ(c)"
"M
X",0.9485570890840652,tr(DQ(c)))2ℓ
"M
X",0.9498117942283564,"|
{z
} ≜ϵ1 ]"
"M
X",0.9510664993726474,"+Γ(I −(
ˆD ˆQ(c)"
"M
X",0.9523212045169385,"tr( ˆD ˆQ(c)))2)−1 −
tr(E)
tr(DQ(c))Q(c)DQ(c)(I −(
DQ(c)"
"M
X",0.9535759096612296,tr(DQ(c)))2)−1||max]
"M
X",0.9548306148055207,"≤
K
tr( ˆD ˆQ(c))[||Q(c)DQ(c)||max||ϵ1||max + ||Γ||max||(I −(
ˆD ˆQ(c)"
"M
X",0.9560853199498118,tr( ˆD ˆQ(c)))2)−1||max
"M
X",0.9573400250941029,"+
tr(E)
tr(DQ(c))||Q(c)DQ(c)||max||(I −(
DQ(c)"
"M
X",0.958594730238394,"tr(DQ(c)))2)−1||max]
= O(||ϵ1||max + ||Γ||max + tr(E))
= O(||ϵ1||max + ||Γ||max + ||E||max)
(82)"
"M
X",0.9598494353826851,Under review as a conference paper at ICLR 2022
"M
X",0.9611041405269761,We deﬁne another error term:
"M
X",0.9623588456712673," 
DQ(c) + E
2ℓ
= (DQ(c))2ℓ+ 2ℓ
X"
"M
X",0.9636135508155583,"ν=1
· · ·"
"M
X",0.9648682559598495,| {z }
"M
X",0.9661229611041405,≜R=O(E)
"M
X",0.9673776662484316,".
(83)"
"M
X",0.9686323713927227,We can then simplify ϵ1 for ℓ≥1 as:
"M
X",0.9698870765370138,"||ϵ1||max
= || ∞
X ℓ=1 1"
"M
X",0.9711417816813049,"(tr( ˆD ˆQ(c)))2ℓ """
"M
X",0.972396486825596,( ˆD ˆQ(c))2ℓ−(tr( ˆD ˆQ(c))
"M
X",0.973651191969887,"tr(DQ(c)))2ℓ(DQ(c))2ℓ
# ||max = || ∞
X ℓ=1 1"
"M
X",0.9749058971141782,(tr( ˆD ˆQ(c)))2ℓ
"M
X",0.9761606022584692,"
(DQ(c))2ℓ+ R −(1 +
tr(R)
tr(DQ(c)))2ℓ(DQ(c))2ℓ

||max = || ∞
X ℓ=1 1"
"M
X",0.9774153074027604,(tr( ˆD ˆQ(c)))2ℓ
"M
X",0.9786700125470514,"
R −(
tr(R)
tr(DQ(c)))2ℓ(DQ(c))2ℓ

||max"
"M
X",0.9799247176913425,"= ||[(1 −(
1
tr( ˆD ˆQ(c)))2)−1 −1]R − ∞
X ℓ=1 """
"M
X",0.9811794228356336,"(
tr(R)"
"M
X",0.9824341279799247,"tr(DQ(c))tr( ˆD ˆQ(c))
)2ℓ(DQ(c))2ℓ
# ||max"
"M
X",0.9836888331242158,"= ||[(1 −(
1
tr( ˆD ˆQ(c)))2)−1 −1]R −[(I −(
tr(R)
tr(DQ(c))tr( ˆD ˆQ(c))DQ(c))2)−1 −I]||max"
"M
X",0.9849435382685069,"≤[(1 −(
1
tr( ˆD ˆQ(c)))2)−1 −1]||R||max + ||[(I −(
tr(R)
tr(DQ(c))tr( ˆD ˆQ(c))DQ(c))2)−1 −I]||max
= O(||R||max)
= O(||E||max)
(84)"
"M
X",0.986198243412798,"Similarly, we can rewrite an upper bound on ∥Γ∥max as:"
"M
X",0.9874529485570891,"∥Γ∥max
= K2(∥Q(c)D∥max∥EQ∥max + ∥Q(c)∥max∥ED∥max∥Q∥max + ∥Q(c)∥max∥ED∥max∥EQ∥max
+∥EQ∥max∥DQ(c)∥max + ∥EQ∥max∥D∥max∥EQ∥max
+∥EQ∥max∥ED∥max∥Q(c)∥max + ∥EQ∥max∥ED∥max∥EQ∥max)
= K2(∥EQ∥max + ∥ED∥max + ∥ED∥max∥EQ∥max + ∥EQ∥max + ∥EQ∥2
max
+∥EQ∥max∥ED∥max + ∥EQ∥2
max∥ED∥max)
= O(||EQ||max + ||ED||max),
(85) and"
"M
X",0.9887076537013801,"∥E∥max
= ∥DEQ + EDQ(c) + EDEQ∥max
= K(∥D∥max∥EQ∥max + ∥ED∥max∥Q(c)∥max + ∥ED∥max∥EQ∥max)
= K(∥EQ∥max + ∥ED∥max + ∥ED∥max∥EQ∥max)
= O(||EQ||max + ||ED||max). (86)"
"M
X",0.9899623588456713,"By substituting Eq. 84, Eq. 86, and Eq. 85 into Eq. 82, we get:"
"M
X",0.9912170639899623,"|| ¯E||max
= O(||EQ||max + ||ED||max).
(87)"
"M
X",0.9924717691342535,Replacing Eq. 87 into the original error in Eq. 79 yields:
"M
X",0.9937264742785445,"|
ˆθ(i)
group−1
m
X"
"M
X",0.9949811794228356,"i=1
(ˆθ(i)
group −1)
−
rθ(i)
group, ¯
A−1
m
X"
"M
X",0.9962358845671268,"i=1
(rθ(i)
group, ¯A −1)
| = O
  1"
"M
X",0.9974905897114178,"m[||EΦ||max + ||EQ||max + ||ED||max]

.
(88)"
"M
X",0.998745294855709,which happens with the same lower bound probability as in Eq. 70. The proof is now complete.
