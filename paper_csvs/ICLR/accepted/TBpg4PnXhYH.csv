Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0045662100456621,"We introduce a new approach for speech pre-training named SPIRAL which works
by learning denoising representation of perturbed data in a teacher-student frame-
work.
SpeciÔ¨Åcally, given a speech utterance, we Ô¨Årst feed the utterance to a
teacher network to obtain corresponding representation. Then the same utter-
ance is perturbed and fed to a student network. The student network is trained to
output representation resembling that of the teacher. At the same time, the teacher
network is updated as moving average of student‚Äôs weights over training steps. In
order to prevent representation collapse, we apply an in-utterance contrastive loss
as pre-training objective and impose position randomization on the input to the
teacher. SPIRAL achieves competitive or better results compared to state-of-the-
art speech pre-training method wav2vec 2.0, with signiÔ¨Åcant reduction of training
cost (80% for BASE model, 65% for LARGE model). Furthermore, we address
the problem of noise-robustness that is critical to real-world speech applications.
We propose multi-condition pre-training by perturbing the student‚Äôs input with
various types of additive noise. We demonstrate that multi-condition pre-trained
SPIRAL models are more robust to noisy speech (9.0% - 13.3% relative word er-
ror rate reduction on real noisy test data), compared to applying multi-condition
training solely in the Ô¨Åne-tuning stage. Source code is available 1."
INTRODUCTION,0.0091324200913242,"1
INTRODUCTION"
INTRODUCTION,0.0136986301369863,"Industrial-scale automatic speech recognition (ASR) systems are usually trained with ten-thousands
of hours of hand-transcribed speech data (Galvez et al., 2021). However, labeling speech data is ex-
pensive and time-consuming, especially for languages with small speaker populations, or for speciÔ¨Åc
domains (e.g., legal, Ô¨Ånancial, scientiÔ¨Åc)."
INTRODUCTION,0.0182648401826484,"Recently, methods of utilizing unlabeled speech data to improve speech recognition system have
achieved remarkable progress. Amongst them, self-training (Manohar et al., 2015; Kahn et al.,
2020a; Synnaeve et al., 2020a; Chen et al., 2020b; Xu et al., 2020; Park et al., 2020b; Xiao et al.,
2021), also known as pseudo-labeling, starts by training an ASR model with labeled speech data,
which is referred to as teacher model. Then the teacher model, usually combined with a language
model (LM), is used to produce pseudo-labels for unlabeled speech data. Finally, the labeled data
and the pseudo-labeled data are combined to train a new model, which is referred to as student
model. The process is repeated by taking the student model as the teacher in next iteration. Another
line of work is speech pre-training (van den Oord et al., 2019; Chung & Glass, 2020; Wang et al.,
2020; Baevski et al., 2020b; Liu et al., 2020). Pre-training learns speech representation from unla-
beled data in a self-supervised way. The pre-trained model is then Ô¨Åne-tuned on the labeled data.
Self-training and pre-training are complementary as shown in recent work (Xu et al., 2021; Zhang
et al., 2020)."
INTRODUCTION,0.0228310502283105,"In this paper, we introduce a new speech pre-training method which works by learning denois-
ing representation of perturbed data with the teacher-student framework, named as Self-supervised"
INTRODUCTION,0.0273972602739726,1https://github.com/huawei-noah/Speech-Backbones/tree/main/SPIRAL
INTRODUCTION,0.0319634703196347,Published as a conference paper at ICLR 2022
INTRODUCTION,0.0365296803652968,"Perturbation-Invariant Representation Learning (SPIRAL). Compared to state-of-the-art speech pre-
training methods such as wav2vec 2.0 (Baevski et al., 2020b) and HuBERT (Hsu et al., 2021), our
method allows end-to-end training with a single contrastive loss, and without relying on discrete unit
discovery techniques such as vector quantization (J¬¥egou et al., 2011; Baevski et al., 2020a;b) or iter-
ative clustering process (Hsu et al., 2021). We apply multi-condition training with SPIRAL (Seltzer
et al., 2013; Ko et al., 2015) to improve noise-robustness for the downstream speech tasks."
INTRODUCTION,0.0410958904109589,"SPIRAL is motivated by the observation that human tolerates speech perturbations or distortions
fairly well. For example, people can communicate effectively in a noisy environment, or over a
distorted telephone channel. Therefore, we hypothesize that by learning representation invariant to
perturbation, the model will learn high-level representation which can enhance speech applications."
INTRODUCTION,0.045662100456621,"To learn perturbation-invariant representation in a self-supervised way, we employ a teacher-student
framework similar to Tarvainen & Valpola (2017). During pre-training, given a speech utterance, we
guide the student network which consumes the perturbed utterance to learn from the teacher network
which consumes the clean utterance. The student is trained to produce denoised representation of
the perturbed utterance similar to teacher‚Äôs representation of the clean utterance. Meanwhile, the
teacher, which shares the same model architecture with student, is updated as moving average of the
student‚Äôs weights over past training steps."
INTRODUCTION,0.0502283105022831,"We apply the in-utterance contrastive loss to avoid model collapse to trivial constant representa-
tion (Chopra et al., 2005). As speech utterance are sequential data, there is another possible trivial
solution which we call positional collapse. Positional collapse occurs when the student ‚Äúcheats‚Äù by
exploiting position correlation in teacher‚Äôs representation to minimize the loss, while ignoring the
content of the input utterance. To prevent positional collapse, we propose position randomization
by adding random number of paddings on both sides of input utterance to the teacher."
INTRODUCTION,0.0547945205479452,"Large-scale speech pre-training is computationally demanding. To reduce computation cost, we
adopt a gradual down-sampling strategy in SPIRAL model, which has been veriÔ¨Åed effective in
speech recognition literatures with negligible performance degradation (Peddinti et al., 2018; Han
et al., 2020b; Huang et al., 2020). We also speculate that aggressive down-sampling helps to remove
redundancy in speech."
INTRODUCTION,0.0593607305936073,"To evaluate the effectiveness of SPIRAL, we conduct experiments on LibriSpeech and Libri-Light
datasets. By training a small convolutional classiÔ¨Åer on the representation of a frozen SPIRAL
model, we can achieve WER of 3.5% and 6.4% on Librispeech test-clean and test-other respectively.
SPIRAL achieves competitive or better results compared to state-of-the-art speech pre-training
methods, while being much more training-efÔ¨Åcient. We also demonstrate that multi-condition pre-
trained SPIRAL are more robust to noisy speech with 9.0% - 13.3% relative word error rate (WER)
reduction on real noisy test data from ChiME-3 (Barker et al., 2015), compared to the model apply-
ing multi-condition training solely in Ô¨Åne-tuning stage."
RELATED WORK,0.0639269406392694,"2
RELATED WORK"
RELATED WORK,0.0684931506849315,"Mean Teacher (MT) (Tarvainen & Valpola, 2017) proposes using a student network to learn from a
teacher network which is the moving average version of the student in the semi-supervised learning
setting. The authors apply a supervised loss for labeled data and a consistency loss between teacher
and student predictions for unlabeled data. However, direct application of MT to self-supervised
learning leads to representation collapse (Grill et al., 2020)."
RELATED WORK,0.0730593607305936,"Noisy student training (NST) (Xie et al., 2020; Park et al., 2020b) is a self-training method. NST
demonstrates the importance of the aggressive injection of noise into the student. Although not
emphasized, no noise is injected into pseudo-labeling process of the teacher. We consider our work
as an extension of self-training approach to the self-supervised learning regime. Instead of using the
teacher to provide pseudo-labels, we utilize the teacher for pseudo-reference representation."
RELATED WORK,0.0776255707762557,"Denoising autoencoders (Vincent et al., 2008) learn to recover a clean input from a corrupted ver-
sion. However, speech data contain redundancy which is irrelevant to some speech applications
such as speech recognition. Previous work (Baevski et al., 2019) shows that speech pre-training
by recovering masked input speech features is not effective. In SPIRAL, we instead enforce latent
representation of a corrupted input to resemble that of the corresponding clean input."
RELATED WORK,0.0821917808219178,Published as a conference paper at ICLR 2022
RELATED WORK,0.0867579908675799,"Bootstrap Your Own Latent (BYOL) (Grill et al., 2020) is a self-supervised image representation
learning method. The method is based on a teacher-student framework similar to MT. The authors
refer to student network as online network and teacher network as target network. They observe
that naive application of MT to self-supervised learning leads to trivial constant representation.
They prevent the representation collapse by appending a predictor to the student network. The
theory behind is under investigation (Chen & He, 2021; Tian et al., 2021). Our method draws
inspirations from BYOL and shares the similar architecture, but there are crucial differences. Instead
of learning a single global representation for an image as in BYOL, SPIRAL learns a sequence of
representation for an utterance. We aim for sequence applications such as speech recognition. In our
preliminary experiments, we observe that appending a predictor to student network is not sufÔ¨Åcient
to prevent trivial constant representation for sequential representation learning. We use in-utterance
contrastive loss (Baevski et al., 2020b) combined with input position randomization to successfully
avoid representation collapse. We still keep the predictor in SPIRAL, but only for the sake of
performance improvement from our observation. Another difference is that BYOL does not perform
representation denoising. BYOL applies perturbation, which they call augmentation, to both the
inputs of the teacher and the student. We demonstrate that representation denoising is crucial for
speech pre-training. When perturbation is applied to the teacher‚Äôs input, the effectiveness of speech
pre-training degrades drastically."
RELATED WORK,0.091324200913242,"Wav2vec 2.0 (Baevski et al., 2020b) is a self-supervised speech representation learning method
which belongs to the masked prediction family. Masked prediction methods are effective for text
pre-training (Devlin et al., 2019), but not for speech pre-training when naively applied (Baevski
et al., 2019). The reason is that speech data contains redundancy such as speaker information, pro-
nunciation variations, which are irrelevant to the semantic meaning of the utterance. To overcome
this problem, wav2vec 2.0 perform masking in intermediate latent space and performs target dis-
cretization with a differentiable quantization scheme. However, quantization leads to a more com-
plex model by introducing additional hyper-parameters and an additional diversity loss. SPIRAL
does not utilize quantization, and still achieves competitive performance compared to wav2vec 2.0.
We hypothesize that aggressive down-sampling and learning by matching output representation may
help to remove redundancy from the learned representation. We leave the investigation of whether
target discretization could further improve SPIRAL for future work."
RELATED WORK,0.0958904109589041,"Liang et al. (2018) demonstrates that under the supervised learning setting, enforcing noise-invariant
representation by penalizing difference between clean and noisy data improves ASR model accuracy."
METHOD,0.1004566210045662,"3
METHOD"
SELF-SUPERVISED PERTURBATION-INVARIANT REPRESENTATION LEARNING,0.1050228310502283,"3.1
SELF-SUPERVISED PERTURBATION-INVARIANT REPRESENTATION LEARNING
(SPIRAL)"
SELF-SUPERVISED PERTURBATION-INVARIANT REPRESENTATION LEARNING,0.1095890410958904,"Figure 1 shows the diagram of SPIRAL in the pre-training stage, where we use two neural networks,
a student FŒ∏ and a teacher FŒ∏‚Ä≤. The weights of the teacher Œ∏‚Ä≤ is the moving average of the weights
of the student Œ∏. At step t, the weights of the teacher Œ∏‚Ä≤
t are updated as"
SELF-SUPERVISED PERTURBATION-INVARIANT REPRESENTATION LEARNING,0.1141552511415525,"Œ∏‚Ä≤
t ‚ÜêŒ±tŒ∏‚Ä≤
t‚àí1 + (1 ‚àíŒ±t)Œ∏t,
(1)"
SELF-SUPERVISED PERTURBATION-INVARIANT REPRESENTATION LEARNING,0.1187214611872146,"where Œ±t determines the rate of weight updates. Given a speech utterance X = (x1, . . . , xT ) of
length T, the student takes a perturbed version Àú
X = s(X) = (Àúx1, . . . , ÀúxT ) as input where s(¬∑) is
a perturbation function. The output of the student is a representation sequence Z = F( Àú
X; Œ∏) =
(z1, . . . , zT ). The teacher takes the same utterance without perturbation as input and output an-
other representation sequence Z‚Ä≤ = F(X; Œ∏‚Ä≤) = (z‚Ä≤
1, . . . , z‚Ä≤
T ). For each representation zi ‚ààZ,
the student is trained to match the teacher‚Äôs representation z‚Ä≤
i at the same position amongst k dis-
tracting samples. The distracting samples are randomly drawn from other positions of the same
utterance in Z‚Ä≤, which is found to be more effective than samples drawn from an entire batch of ut-
terances (Baevski et al., 2020b). The in-utterance contrastive loss is deÔ¨Åned following Sohn (2016);
Wu et al. (2018) as, L = ‚àí T
X"
SELF-SUPERVISED PERTURBATION-INVARIANT REPRESENTATION LEARNING,0.1232876712328767,"i=1
log
exp(œÜ(zi, z‚Ä≤
i)/Œ∫)
P"
SELF-SUPERVISED PERTURBATION-INVARIANT REPRESENTATION LEARNING,0.1278538812785388,"j‚ààDi exp(œÜ(zi, z‚Ä≤
j)/Œ∫),
(2)"
SELF-SUPERVISED PERTURBATION-INVARIANT REPRESENTATION LEARNING,0.1324200913242009,Published as a conference paper at ICLR 2022
SELF-SUPERVISED PERTURBATION-INVARIANT REPRESENTATION LEARNING,0.136986301369863,"Student
Teacher"
SELF-SUPERVISED PERTURBATION-INVARIANT REPRESENTATION LEARNING,0.1415525114155251,"In-utterance
contrastive loss"
SELF-SUPERVISED PERTURBATION-INVARIANT REPRESENTATION LEARNING,0.1461187214611872,Perturbation
SELF-SUPERVISED PERTURBATION-INVARIANT REPRESENTATION LEARNING,0.1506849315068493,Moving average
SELF-SUPERVISED PERTURBATION-INVARIANT REPRESENTATION LEARNING,0.1552511415525114,"Random positional 
padding"
SELF-SUPERVISED PERTURBATION-INVARIANT REPRESENTATION LEARNING,0.1598173515981735,"ùëß1, ‚Ä¶ , ùëßùëá
ùëß1"
SELF-SUPERVISED PERTURBATION-INVARIANT REPRESENTATION LEARNING,0.1643835616438356,"‚Ä≤, ‚Ä¶ , ùëßùëá ‚Ä≤"
SELF-SUPERVISED PERTURBATION-INVARIANT REPRESENTATION LEARNING,0.1689497716894977,Gradient
SELF-SUPERVISED PERTURBATION-INVARIANT REPRESENTATION LEARNING,0.1735159817351598,"ùë•1, ‚Ä¶ , ùë•ùëá"
SELF-SUPERVISED PERTURBATION-INVARIANT REPRESENTATION LEARNING,0.1780821917808219,"ùë•1, ‚Ä¶ ,  ùë•ùëá ùúÉ
ùúÉ‚Ä≤ + ‚àí"
SELF-SUPERVISED PERTURBATION-INVARIANT REPRESENTATION LEARNING,0.182648401826484,Figure 1: Illustration of SPIRAL architecture for speech pre-training.
SELF-SUPERVISED PERTURBATION-INVARIANT REPRESENTATION LEARNING,0.1872146118721461,"where œÜ(a, b) = aT b/‚à•a‚à•‚à•b‚à•is cosine similarity, Di is the set of indices of distractors for the i-th
position, and Œ∫ is the temperature parameter."
SELF-SUPERVISED PERTURBATION-INVARIANT REPRESENTATION LEARNING,0.1917808219178082,"However, applying in-utterance contrastive loss could cause a kind of representation collapse which
we refer to as positional collapse. Contrastive candidates are sampled based on their positions in
utterances. When a teacher‚Äôs representation z‚Ä≤
i is correlated with its position i (e.g., correlation
introduced by positional encoding in Transformer), the student could exploit this correlation to gen-
erate its representation zi solely based on the position index i, while ignoring content of the input.
In this case, the model does not learn meaningful representation of the input content. Therefore,
we prevent positional collapse by randomizing positions of teacher‚Äôs representation. In particular,
we add random number of padding data at both ends of the input to the teacher to randomly shift
the position information for each output representation z‚Ä≤
i. The student thereby is unable to exploit
the spurious position information to minimize the contrastive loss. Note that when calculating the
contrastive loss, we exclude the corresponding representation of the padded data."
MODEL ARCHITECTURE,0.1963470319634703,"3.2
MODEL ARCHITECTURE"
MODEL ARCHITECTURE,0.2009132420091324,Transformer
MODEL ARCHITECTURE,0.2054794520547945,Layers
MODEL ARCHITECTURE,0.2100456621004566,Transformer
MODEL ARCHITECTURE,0.2146118721461187,Layers
MODEL ARCHITECTURE,0.2191780821917808,Projection
MODEL ARCHITECTURE,0.2237442922374429,"Head
Predictor
Subsampling"
MODEL ARCHITECTURE,0.228310502283105,Convolution
MODEL ARCHITECTURE,0.2328767123287671,Subsampling
MODEL ARCHITECTURE,0.2374429223744292,Convolution
MODEL ARCHITECTURE,0.2420091324200913,Encoder
"MS
RATE",0.2465753424657534,"40ms
rate"
"MS
RATE",0.2511415525114155,"80ms
rate
10ms
rate"
"MS
RATE",0.2557077625570776,"ùëì(‚àô)
ùëî(‚àô)
ùëû(‚àô)"
"MS
RATE",0.2602739726027397,"Figure 2: The architecture of the student model in SPIRAL. The frame rate of input is denoted as
‚Äò10/40/80 ms‚Äô. The dashed line indicates the optional predictor which can be removed with small
performance degradation. The structure of the teacher model is the same but without the predictor."
"MS
RATE",0.2648401826484018,"As illustrated in Figure 2, student FŒ∏ is composed of an encoder f(¬∑), a projection head g(¬∑) (Chen
et al., 2020a) and an optional predictor q(¬∑) (Grill et al., 2020), i.e., FŒ∏ = (f ‚ó¶g‚ó¶q)(¬∑; Œ∏). The teacher
FŒ∏‚Ä≤ has the same structure expect that it has no predictor, FŒ∏‚Ä≤ = (f‚ó¶g)(¬∑; Œ∏‚Ä≤). The encoder consists of
two blocks. In each block, we Ô¨Årst apply temporal convolutions to perform down-sampling, followed
by Transformer (Vaswani et al., 2017) with convolutional relative position encoding (Baevski et al.,"
"MS
RATE",0.2694063926940639,Published as a conference paper at ICLR 2022
"MS
RATE",0.273972602739726,"2020b). Each convolution is followed by layer normalization (LN) (Ba et al., 2016) and ReLU. For
the projection head, we apply a simple linear layer. The predictor consists of two layers of temporal
convolution and a linear layer. The convolutions are followed by batch normalization (BN) (Ioffe &
Szegedy, 2015) and ReLU."
"MS
RATE",0.2785388127853881,"During pre-training, we add computation noise to both the student and the teacher by applying
dropout (Srivastava et al., 2014) and LayerDrop (Fan et al., 2020) in Transformer. We use the same
dropout and LayerDrop rates for the student and the teacher."
ADAPTIVE SPECAUGMENT,0.2831050228310502,"3.3
ADAPTIVE SPECAUGMENT"
ADAPTIVE SPECAUGMENT,0.2876712328767123,"We apply adaptive SpecAugment similar to Park et al. (2020a) as the primary perturbation method.
Along either time or frequency dimension, we sample uniformly a certain proportion p of all time-
steps to be start indices and mask the subsequent consecutive L time-steps. The masked time-steps
is Ô¨Ålled with zeros along frequency dimension. Along time dimension, we use Gaussian noise as
masking values to avoid numerical problems for LN (Park et al., 2020a)."
MULTI-CONDITION PRE-TRAINING,0.2922374429223744,"3.4
MULTI-CONDITION PRE-TRAINING"
MULTI-CONDITION PRE-TRAINING,0.2968036529680365,"For noise-robust pre-training with SPIRAL, we perturb input of the student with various types
of additive noise. We consider this technique as an implementation of multi-condition training
(MCT) (Seltzer et al., 2013) in self-supervised setting. SpeciÔ¨Åcally, for each input utterance to the
student, we sample noise clips from a noise dataset, and mix the noise clips with the whole utterance
by addition in time-domain. We Ô¨Årst uniformly sample a signal-to-noise ratio (SNR) from a pre-
deÔ¨Åned range for each utterance. Then we scale the noise volume according to the required SNR. In
our preliminary experiments, we found that applying additive noise alone as perturbation degrades
performance. Therefore, we apply additive noise perturbation together with adaptive SpecAugment."
MODEL FINE-TUNING,0.3013698630136986,"3.5
MODEL FINE-TUNING"
MODEL FINE-TUNING,0.3059360730593607,"After pre-training, we take the encoder from the student model in SPIRAL and add a randomly
initialized convolutional classiÔ¨Åer on top of it. The convolutional classiÔ¨Åer is composed of two
layers of convolution, followed by LN and ReLU, and a linear output projection. The convolution
Ô¨Ålters consist of 512 channels with kernel width of 5."
MODEL FINE-TUNING,0.3105022831050228,"We Ô¨Åne-tune the model with connectionist temporal classiÔ¨Åcation (CTC) (Graves et al., 2006) ob-
jective for speech recognition. We use 1024 subwords as output units. The sub-words are generated
from training transcripts of LibriSpeech with SentencePiece (Kudo & Richardson, 2018)."
MODEL FINE-TUNING,0.3150684931506849,"We further investigate SPIRAL‚Äôs ability to learn high-level representation of speech during pre-
training. In addition to whole-model Ô¨Åne-tuning, we apply frozen Ô¨Åne-tuning. We freeze the pre-
trained parameters and only Ô¨Åne-tune the convolutional classiÔ¨Åer which can only perform local clas-
siÔ¨Åcation due to limited receptive Ô¨Åeld."
EXPERIMENTAL SETUP,0.319634703196347,"4
EXPERIMENTAL SETUP"
DATA,0.3242009132420091,"4.1
DATA"
DATA,0.3287671232876712,"For pre-training, we use the 960-hour training data (ignoring the labels) from LibriSpeech (Panay-
otov et al., 2015)(LS-960), or 60k-hour unlabeled audio data from Libri-Light (Kahn et al., 2020b)
(LL-60K). For Libri-Light, we segment the data using ofÔ¨Åcial tools with a threshold of 16s, resulting
in 46.9k hours of data. The two datasets are both derived from English audiobooks from LibriVox
project2. For ASR Ô¨Åne-tuning, we apply 100-hour subset (train-clean-100) as low-resource labeled
data and entire LS-960 with labels as high-resource labeled data, both from LibriSpeech."
DATA,0.3333333333333333,"For multi-condition training, we use the noise dataset from Reddy et al. (2021). The dataset consists
of 181 hours of noise data with about 150 noise types and 70,000 clips. We shufÔ¨Çe and split the noise
data with a ratio of 8:1:1, which are used for training, synthesizing noisy dev-sets and synthetic noisy"
DATA,0.3378995433789954,2https://librivox.org/
DATA,0.3424657534246575,Published as a conference paper at ICLR 2022
DATA,0.3470319634703196,"Table 1: Detailed conÔ¨Ågurations of the SPIRAL BASE and LARGE models.
Modules
Conv.1
Transf.1
Conv.2
Transf.2
Proj. H.
Predictor
#Params"
DATA,0.3515981735159817,"Hyper
-params"
DATA,0.3561643835616438,"layer
layer
kernel size
emb. dim.
kernel size
emb. dim.
kernel size
channel
ffn dim.
channel
ffn dim.
dim.
channel
stride
layerdrop
stride
layerdrop
attn. heads
attn. heads"
DATA,0.3607305936073059,"BASE
model"
DATA,0.365296803652968,"2
10
5,5,1
512
5,1
768
5,5,1
384,512,512
2048
1536,768
3072
256
256,256,256
91.5M
2,2,1
0
2,1
0.05
8
12"
DATA,0.3698630136986301,"LARGE
model"
DATA,0.3744292237442922,"4
20
5,5,1
512
5,1
1024
5,5,1
384,512,512
2048
2048,1024
4096
512
512,512,512
287M
2,2,1
0.05
2,1
0.05
8
16"
DATA,0.3789954337899543,Table 2: Comparison of pre-training cost between wav2vec 2.0 and SPIRAL.
DATA,0.3835616438356164,"Model
Unlabeled data
Training steps
GPU days
Mixed precision"
DATA,0.3881278538812785,"Wav2vec 2.0 BASE (Baevski et al., 2020b)
LS-960
500k
102.4
‚úì
SPIRAL BASE
LS-960
200k
20.8
-"
DATA,0.3926940639269406,"Wav2vec 2.0 LARGE (Baevski et al., 2020b)
LL-60k
1000k
665.6
‚úì
SPIRAL LARGE
LL-60k
500k
232.0
-"
DATA,0.3972602739726027,"test-sets (results in Appendix A.2) respectively. SNRs of speech mixtures are set from 0 to 30 dB.
We evaluate on real noisy data test set from CHiME-3 (Barker et al., 2015), which is comprised
of speech data recorded in real noisy environments (bus, cafe, pedestrian area, and street junction).
The data are recorded with a microphone array composed of multiple microphone channels located
at different positions of a tablet, and a close-talking microphone."
TRAINING SETUPS,0.4018264840182648,"4.2
TRAINING SETUPS"
TRAINING SETUPS,0.4063926940639269,"We apply 128-dimensional log-mel Ô¨Ålterbank extracted with 20 ms window and 10 ms stride as the
input acoustic feature. We experiment with BASE model and LARGE model conÔ¨Ågurations as shown
in Table 1. The numbers of parameters are comparable to wav2vec 2.0 BASE and LARGE models
correspondingly. For SpecAugment, we set p = 0.025 and L = 20 for time-dimension mask, and
p = 0.02 and L = 20 for frequency-dimension mask."
TRAINING SETUPS,0.410958904109589,"In pre-training, we optimize with Adam (Kingma & Ba, 2015) optimizer, warming up the learn-
ing rate for the Ô¨Årst 8% of updates to a peak of 3e-3. Then the learning rate decays to 0 with
a cosine schedule. The moving average update rate Œ±t of teacher‚Äôs weight also follows a cosine
schedule (Grill et al., 2020). We increase Œ±t from 0.995 to 1.0 and from 0.990 to 0.999 for BASE
and LARGE models respectively. We train the BASE model with batch size of 24 per GPU for 200k
steps on 16 V100 GPUs, which takes about 1.3 days. For the LARGE model, we train with batch size
of 20 per GPU for 500k steps on 32 V100 GPUs, which takes about 7.25 days. As shown in Table 2,
there is a signiÔ¨Åcant reduction of training cost (GPU days) compared to wav2vec 2.0 (Baevski et al.,
2020b). SPIRAL requires 80% and 65% less training cost for BASE and LARGE respectively. Note
that mix-precision training is not applied for SPIRAL yet."
TRAINING SETUPS,0.4155251141552511,"For Ô¨Åne-tuning, we optimize with Adam and a tri-state rate schedule where the learning rate is
warmed up for the Ô¨Årst 10% of updates to 3e-5, held constant for the next 40% and then linearly
decayed to zero following Baevski et al. (2020b). We Ô¨Åne-tune BASE and LARGE with batch size of
14 and 18 per GPU respectively on 8 GPUs for 80k steps on train-clean-100. We Ô¨Åne-tune LARGE
with batch size of 10 per GPU on 16 GPUs for 320k steps on LS-960. We apply SpecAugment
for whole-model Ô¨Åne-tuning but not for frozen Ô¨Åne-tuning. For multi-condition pre-training and"
TRAINING SETUPS,0.4200913242009132,Published as a conference paper at ICLR 2022
TRAINING SETUPS,0.4246575342465753,"Table 3: ASR results Ô¨Åne-tuned from low-resource train-clean-100. Language models used in de-
coding are listed in LM. We compare SPIRAL BASE pre-trained on LS-960 and SPIRAL LARGE
pre-trained on LL-60k with previous methods. We report WER (%) on Librispeech dev/test sets."
TRAINING SETUPS,0.4292237442922374,"Model
Unlabeled
LM
dev
test
data
clean
other
clean
other"
TRAINING SETUPS,0.4337899543378995,"Supervised/Semi-Supervised
Hybrid DNN/HMM (L¬®uscher et al., 2019)
-
4-gram
5.0
19.5
5.8
18.6
Iter. pseudo-labeling (Xu et al., 2020)
LL-60k
4-gram+Transf.
3.19
6.14
3.72
7.11
Noisy student (Park et al., 2020b)
LS-860
LSTM
3.9
8.8
4.2
8.6"
TRAINING SETUPS,0.4383561643835616,"Self-supervised
wav2vec 2.0 BASE (Baevski et al., 2020b)
LS-960
-
6.1
13.5
6.1
13.3
SPIRAL BASE frozen (ours)
LS-960
-
7.9
12.7
7.6
13.0
SPIRAL BASE (ours)
LS-960
-
5.5
11.1
5.4
11.2"
TRAINING SETUPS,0.4429223744292237,"wav2vec 2.0 BASE (Baevski et al., 2020b)
LS-960
4-gram
2.7
7.9
3.4
8.0
SPIRAL BASE (ours)
LS-960
4-gram
2.7
7.0
3.3
7.5"
TRAINING SETUPS,0.4474885844748858,"wav2vec 2.0 BASE (Baevski et al., 2020b)
LS-960
Transf.
2.2
6.3
2.6
6.3
SPIRAL BASE (ours)
LS-960
Transf.
2.3
5.8
2.7
6.1"
TRAINING SETUPS,0.4520547945205479,"wav2vec 2.0 LARGE (Baevski et al., 2020b)
LL-60k
-
3.3
6.5
3.1
6.3
SPIRAL LARGE frozen (ours)
LL-60k
-
7.1
9.2
6.6
9.7
SPIRAL LARGE (ours)
LL-60k
-
3.3
5.9
3.3
6.3"
TRAINING SETUPS,0.45662100456621,"wav2vec 2.0 LARGE (Baevski et al., 2020b)
LL-60k
Transf.
1.9
4.0
2.0
4.0
SPIRAL LARGE (ours)
LL-60k
Transf.
1.9
3.9
2.2
4.3"
TRAINING SETUPS,0.4611872146118721,"Ô¨Åne-tuning, we randomly perturb each utterance with additive noise with 50% probability before
applying SpecAugment. SNR is uniformly sampled from 0-30 dB."
LANGUAGE MODEL AND DECODING,0.4657534246575342,"4.3
LANGUAGE MODEL AND DECODING"
LANGUAGE MODEL AND DECODING,0.4703196347031963,"We use a word-level Transformer LM (Baevski & Auli, 2019) trained on Librispeech LM corpus
which is identical to Synnaeve et al. (2020b). For low-resource ASR setting, we also evaluate
SPIRAL BASE with the ofÔ¨Åcial LibriSpeech 4-gram LM. We observe that models Ô¨Åne-tuned with
subword units performs worse than models Ô¨Åne-tuned with character units when decoding with
word-level LM. Therefore, we apply character-based models for LM decoding, which is the same
setting as wav2vec 2.0. The results of LM decoding with subword-based models are available in
Appendix A.1."
LANGUAGE MODEL AND DECODING,0.4748858447488584,"As output frame rate of pre-trained SPIRAL encoder is low (80ms), the output sequence may be too
short for character units. To reuse the pre-trained encoder, we devise an upsampling strategy for
the SPIRAL encoder output in Ô¨Åne-tuning stage. We apply a 1-D convolution layer to project the
original encoder output of dimension d into a vector of dimension 4d. At each time-step, we reshape
the projected output vector from (1, 4d) to (4, d). The frame rate now becomes 20ms. Then we feed
the upsampled outputs to convolutional classiÔ¨Åer."
LANGUAGE MODEL AND DECODING,0.4794520547945205,"We perform random search for decoding parameters and choose the best parameters according to
performance on dev-other with beam 50. The Ô¨Ånal test performance is measured with beam 500.
We use the beam search decoder of Pratap et al. (2019)."
RESULTS,0.4840182648401826,"5
RESULTS"
"EVALUATION UNDER LOW-RESOURCE AND HIGH-RESOURCE LABELED DATA
SETTINGS",0.4885844748858447,"5.1
EVALUATION UNDER LOW-RESOURCE AND HIGH-RESOURCE LABELED DATA
SETTINGS"
"EVALUATION UNDER LOW-RESOURCE AND HIGH-RESOURCE LABELED DATA
SETTINGS",0.4931506849315068,"We Ô¨Årst evaluate our method under a low-resource ASR setting in which we Ô¨Åne-tune the models
with 100-hour LibriSpeech data (train-clean-100). The results are shown in Table 3. We evaluate
a BASE model pre-trained with 960-hour LibriSpeech (LS-960) and a LARGE model pre-trained
with Libri-Light (LL-60K). The frozen BASE model performs well, achieving a WER of 13.0%"
"EVALUATION UNDER LOW-RESOURCE AND HIGH-RESOURCE LABELED DATA
SETTINGS",0.4977168949771689,Published as a conference paper at ICLR 2022
"EVALUATION UNDER LOW-RESOURCE AND HIGH-RESOURCE LABELED DATA
SETTINGS",0.502283105022831,"Table 4: ASR results Ô¨Åne-tuned from high-resource LS-960. Language models used in decoding
are listed in LM. We compare SPIRAL LARGE pre-trained on Libri-Light (LL-60k) with previous
methods. We report WER (%) on Librispeech dev/test sets."
"EVALUATION UNDER LOW-RESOURCE AND HIGH-RESOURCE LABELED DATA
SETTINGS",0.5068493150684932,"Model
Unlabeled
LM
dev
test
data
clean
other
clean
other"
"EVALUATION UNDER LOW-RESOURCE AND HIGH-RESOURCE LABELED DATA
SETTINGS",0.5114155251141552,"Supervised
ContextNet (Han et al., 2020a)
-
LSTM
1.9
3.9
1.9
4.1
Conformer (Gulati et al., 2020)
-
LSTM
2.1
4.3
1.9
3.9"
"EVALUATION UNDER LOW-RESOURCE AND HIGH-RESOURCE LABELED DATA
SETTINGS",0.5159817351598174,"Semi-supervised
CTC Transf. + PL (Synnaeve et al., 2020a)
LL-60k
CLM+Transf.
2.10
4.79
2.33
4.54
S2S Transf. + PL (Synnaeve et al., 2020a)
LL-60k
CLM+Transf.
2.00
3.65
2.09
4.11
Iter. pseudo-labeling Xu et al. (2020)
LL-60k
4-gram+Transf.
1.85
3.26
2.10
4.01
Noisy student (Park et al., 2020b)
LL-60k
LSTM
1.6
3.4
1.7
3.4"
"EVALUATION UNDER LOW-RESOURCE AND HIGH-RESOURCE LABELED DATA
SETTINGS",0.5205479452054794,"Self-supervised
wav2vec 2.0 LARGE (Baevski et al., 2020b)
LL-60k
-
2.1
4.5
2.2
4.5
SPIRAL LARGE frozen (ours)
LL-60k
-
4.0
6.2
3.5
6.4
SPIRAL LARGE (ours)
LL-60k
-
2.1
4.3
2.2
4.6"
"EVALUATION UNDER LOW-RESOURCE AND HIGH-RESOURCE LABELED DATA
SETTINGS",0.5251141552511416,"wav2vec 2.0 LARGE (Baevski et al., 2020b)
LL-60k
Transf.
1.6
3.0
1.8
3.3
SPIRAL LARGE (ours)
LL-60k
Transf.
1.5
3.1
1.8
3.5"
"EVALUATION UNDER LOW-RESOURCE AND HIGH-RESOURCE LABELED DATA
SETTINGS",0.5296803652968036,"on test-other, which is on par with wav2vec 2.0 BASE. This suggests that SPIRAL indeed learns
meaningful high-level representations in a self-supervised way. When we Ô¨Åne-tune the whole BASE
model, the model achieves WER of 5.4% and 11.2% on test-clean and test-other respectively, outper-
forming wav2vec 2.0 BASE with 11.5% and 15.8% relative WER reduction. When decoding with
Transformer LM, the BASE model achieves WER of 2.7% and 6.1% on test-clean and test-other
respectively. The results are on par with wav2vec 2.0 BASE."
"EVALUATION UNDER LOW-RESOURCE AND HIGH-RESOURCE LABELED DATA
SETTINGS",0.5342465753424658,"The SPIRAL LARGE model consists of more parameters and is pre-trained with more data. The
model achieves WER of 2.2% and 4.3% on test-clean and test-other respectively. The signiÔ¨Åcant
improvement of LARGE over BASE demonstrates the scalability of SPIRAL. The results of SPIRAL
LARGE are competitive to wav2vec 2.0 LARGE. This is encouraging, as SPIRAL LARGE only takes
35% of training cost of wav2vec 2.0 LARGE."
"EVALUATION UNDER LOW-RESOURCE AND HIGH-RESOURCE LABELED DATA
SETTINGS",0.5388127853881278,"We further evaluate SPIRAL LARGE pre-trained with Libri-Light (LL-60K) under a high-resource
ASR setting with 960-hour LS-960 as Ô¨Åne-tuning data. As shown in Table 4, the LARGE model
achieves WER of 1.8% and 3.5% on test-clean and test-other respectively, which are on par with the
wav2vec 2.0 LARGE model. We note that the supervised models and the noisy student model (Park
et al., 2020b) in Table 4 are autoregressive models. Our models are Ô¨Åne-tuned with CTC objective
which is non-autoregressive and generally inferior to autoregressive models. We use CTC objective
for its simplicity and comparability to previous speech pre-training methods."
"EVALUATION UNDER LOW-RESOURCE AND HIGH-RESOURCE LABELED DATA
SETTINGS",0.54337899543379,"We consider SPIRAL as a preferred alternative to wav2vec 2.0 given that SPIRAL only requires
20%‚àí35% computation cost of wav2vec 2.0. We expect further efÔ¨Åciency improvement when we
implement mix-precision training for SPIRAL."
NOISE-ROBUST PRE-TRAINING,0.547945205479452,"5.2
NOISE-ROBUST PRE-TRAINING"
NOISE-ROBUST PRE-TRAINING,0.5525114155251142,"To evaluate noise-robustness of the pre-trained models, we compare the effects of applying multi-
condition training (MCT) in pre-training or Ô¨Åne-tuning stages of SPIRAL. The results are shown in
Table 5. The vanilla SPIRAL BASE model and wav2vec 2.0 BASE model deteriorate with signiÔ¨Å-
cantly higher WER on noisy test data."
NOISE-ROBUST PRE-TRAINING,0.5570776255707762,"On real noisy test speech data in CHiME-3 for different microphone channels (ch), SPIRAL with
multi-condition pre-training signiÔ¨Åcantly improves speech recognition performance. Compared to
the model applying MCT solely in Ô¨Åne-tuning, applying MCT both in pre-training and Ô¨Åne-tuning
achieves 12.4%, 13.3% and 9.0% relative WER reduction for ch 1, 5 and 2 respectively. There
is smaller performance improvement of 3.8% relative WER reduction for ch 0, which is a close-
talking microphone with the highest SNR. We note that ch 2 faces backwards to the speaker. SNR"
NOISE-ROBUST PRE-TRAINING,0.5616438356164384,Published as a conference paper at ICLR 2022
NOISE-ROBUST PRE-TRAINING,0.5662100456621004,"Table 5: Evaluation on noise-robustness of the models. We use wav2vec 2.0 BASE released by the
authors as the baseline. The SPIRAL BASE models are pre-trained with LS-960 and Ô¨Åne-tuned with
train-clean-100. We report WER (%) on Librispeech and CHiME-3 real data test sets."
NOISE-ROBUST PRE-TRAINING,0.5707762557077626,"BASE model
Pre-train
Fine-tune
Librispeech
CHiME-3
w/ MCT
w/ MCT
clean
other
ch0
ch5
ch1
ch2"
NOISE-ROBUST PRE-TRAINING,0.5753424657534246,"wav2vec 2.0
-
-
6.1
13.3
23.2
56.1
68.3
98.1
SPIRAL
-
-
5.4
11.2
24.1
52.1
58.9
92.6
SPIRAL
-
‚úì
5.7
11.4
20.8
35.5
41.1
76.4
SPIRAL
‚úì
-
5.7
11.5
20.8
33.6
38.5
74.0
SPIRAL
‚úì
‚úì
5.9
11.4
20.0
31.1
35.6
69.5"
NOISE-ROBUST PRE-TRAINING,0.5799086757990868,"of the recordings from ch 2 is the lowest, leading to high WER. We note that other pre-training
methods including wav2vec 2.0 may beneÔ¨Åt from multi-condition training, which are worth for
further investigation."
ABLATIONS,0.5844748858447488,"5.3
ABLATIONS"
INPUT PERTURBATION AND COMPUTATION NOISE OF TEACHER,0.589041095890411,"5.3.1
INPUT PERTURBATION AND COMPUTATION NOISE OF TEACHER"
INPUT PERTURBATION AND COMPUTATION NOISE OF TEACHER,0.593607305936073,"SPIRAL learns denoised representation of perturbed data. By default, we only apply perturbation to
the input of the student. An alternative method is to perturb both inputs of the teacher and the student,
and optimize consistency between their representations (Grill et al., 2020; Chen & He, 2021). We
conduct experiments to evaluate the effects of perturbing the input and adding computation noise
(dropout and LayerDrop) to the teacher. The results are shown in Table 8 in Appendix A.3. The
results suggest that applying SpecAugment to teacher‚Äôs input degrades performance signiÔ¨Åcantly.
Performance degradation decreases but is still signiÔ¨Åcant with lower ratio and width of the masks.
This supports the necessity of representation denoising, and our view of SPIRAL as an extension of
self-training in which teacher network are fed with clean input. The results also support applying
computation noise to teacher during pre-training. There is a 15.9% relative WER reduction with
computation noise. This may be linked to Gal & Ghahramani (2016)."
EFFECTS OF PREDICTOR AND PROJECTION HEAD,0.5981735159817352,"5.3.2
EFFECTS OF PREDICTOR AND PROJECTION HEAD"
EFFECTS OF PREDICTOR AND PROJECTION HEAD,0.6027397260273972,"We do ablation studies to understand the role of predictor and projection head in SPIRAL. The re-
sults are shown in Table 9 in Appendix A.3. When removing the predictor from the student, we
observe performance degradation, but representation collapse does not happen. In the architectures
relying on predictor to prevent collapse (Grill et al., 2020; Chen & He, 2021), applying batch nor-
malization (BN) in the predictor is essential. While in SPIRAL, we observe that BN in the predictor
can be replaced by layer normalization (LN) with a small performance degradation. When the pre-
dictor is removed, we observe performance improvement by applying a convolutional projection
head. The convolutional projection head is composed of a temporal convolution layer with LN and
ReLU, and a linear layer. But applying convolutional projection head to the model with a predictor,
there is no further performance improvement. This suggests that convolutional projection head and
predictor play a similar role in SPIRAL, and they are not complementary."
CONCLUSION,0.6073059360730594,"6
CONCLUSION"
CONCLUSION,0.6118721461187214,"We presented SPIRAL, a new approach to speech pre-training by learning denoising representation
of perturbed data with a teacher-student framework. SPIRAL can learn high-level speech repre-
sentation in self-supervised way. Training a small convolutional classiÔ¨Åer on frozen representation
of SPIRAL achieves WER of 3.5% and 6.4% on Librispeech test-clean and test-other respectively.
We show that SPIRAL achieves competitive or better results compared to state-of-the-art speech
pre-training methods, with signiÔ¨Åcant reduction of training cost. We investigate multi-condition
pre-training and demonstrates that multi-condition pre-training is more effective than solely apply-
ing multi-condition training in the Ô¨Åne-tuning stage. We presume SPIRAL as a general pre-training
method, which can apply to other modalities such as images and text. We leave it for future work."
CONCLUSION,0.6164383561643836,Published as a conference paper at ICLR 2022
REFERENCES,0.6210045662100456,REFERENCES
REFERENCES,0.6255707762557078,"Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton.
Layer normalization, 2016.
arXiv:1607.06450."
REFERENCES,0.6301369863013698,"Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In
7th International Conference on Learning Representations, ICLR 2019, 2019."
REFERENCES,0.634703196347032,"Alexei Baevski, Michael Auli, and Abdelrahman Mohamed. Effectiveness of self-supervised pre-
training for speech recognition, 2019. arXiv:1911.03912."
REFERENCES,0.639269406392694,"Alexei Baevski, Steffen Schneider, and Michael Auli. vq-wav2vec: Self-supervised learning of
discrete speech representations. In 8th International Conference on Learning Representations
ICLR 2020, 2020a."
REFERENCES,0.6438356164383562,"Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A frame-
work for self-supervised learning of speech representations. In Advances in Neural Information
Processing Systems, volume 33, 2020b."
REFERENCES,0.6484018264840182,"Jon Barker, Ricard Marxer, Emmanuel Vincent, and Shinji Watanabe. The third ‚ÄòCHiME‚Äô speech
separation and recognition challenge: Dataset, task and baselines. In 2015 IEEE Workshop on
Automatic Speech Recognition and Understanding (ASRU), pp. 504‚Äì511, 2015."
REFERENCES,0.6529680365296804,"Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In Proceedings of the 37th International Conference
on Machine Learning (ICML), pp. 1597‚Äì1607, 2020a."
REFERENCES,0.6575342465753424,"Xinlei Chen and Kaiming He. Exploring simple Siamese representation learning. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15750‚Äì15758, 2021."
REFERENCES,0.6621004566210046,"Yang Chen, Weiran Wang, and Chao Wang. Semi-supervised ASR by end-to-end self-training. In
Proc. Interspeech 2020, pp. 2787‚Äì2791, 2020b."
REFERENCES,0.6666666666666666,"S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity metric discriminatively, with application
to face veriÔ¨Åcation. In 2005 IEEE Computer Society Conference on Computer Vision and Pattern
Recognition (CVPR‚Äô05), pp. 539‚Äì546, 2005."
REFERENCES,0.6712328767123288,"Yu-An Chung and James Glass. Generative pre-training for speech with autoregressive predictive
coding. In 2020 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), pp. 3497‚Äì3501, 2020."
REFERENCES,0.6757990867579908,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, NAACL-HLT 2019, pp. 4171‚Äì4186, 2019."
REFERENCES,0.680365296803653,"Angela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth on demand with
structured dropout. In 8th International Conference on Learning Representations ICLR 2020,
2020."
REFERENCES,0.684931506849315,"Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model
uncertainty in deep learning. In Proceedings of The 33rd International Conference on Machine
Learning (ICML), pp. 1050‚Äì1059, 2016."
REFERENCES,0.6894977168949772,"Daniel Galvez, Greg Diamos, Juan Manuel Ciro Torres, Keith Achorn, Anjali Gopi, David Kanter,
Max Lam, Mark Mazumder, and Vijay Janapa Reddi. The People‚Äôs Speech: A large-scale diverse
english speech recognition dataset for commercial usage. In Thirty-Ô¨Åfth Conference on Neural
Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021."
REFERENCES,0.6940639269406392,"Alex Graves, Santiago Fern¬¥andez, Faustino Gomez, and J¬®urgen Schmidhuber. Connectionist tem-
poral classiÔ¨Åcation: Labelling unsegmented sequence data with recurrent neural networks. In
Proceedings of the 23rd International Conference on Machine Learning (ICML), pp. 369‚Äì376,
2006."
REFERENCES,0.6986301369863014,Published as a conference paper at ICLR 2022
REFERENCES,0.7031963470319634,"Jean-Bastien Grill, Florian Strub, Florent Altch¬¥e, Corentin Tallec, Pierre Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,
Bilal Piot, koray kavukcuoglu, Remi Munos, and Michal Valko. Bootstrap your own latent - a
new approach to self-supervised learning. In Advances in Neural Information Processing Systems,
volume 33, pp. 21271‚Äì21284, 2020."
REFERENCES,0.7077625570776256,"Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo
Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang. Conformer: Convolution-augmented
transformer for speech recognition. In Proc. Interspeech 2020, pp. 5036‚Äì5040, 2020."
REFERENCES,0.7123287671232876,"Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati,
Ruoming Pang, and Yonghui Wu. ContextNet: Improving convolutional neural networks for
automatic speech recognition with global context. In Proc. Interspeech 2020, pp. 3610‚Äì3614,
2020a."
REFERENCES,0.7168949771689498,"Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati,
Ruoming Pang, and Yonghui Wu. ContextNet: Improving convolutional neural networks for
automatic speech recognition with global context. In Proc. Interspeech 2020, pp. 3610‚Äì3614,
2020b."
REFERENCES,0.7214611872146118,"Wei-Ning Hsu, Yao-Hung Hubert Tsai, Benjamin Bolte, Ruslan Salakhutdinov, and Abdelrahman
Mohamed. Hubert: How much can a bad teacher beneÔ¨Åt ASR pre-training?
In 2021 IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 6533‚Äì6537,
2021."
REFERENCES,0.726027397260274,"Wenyong Huang, Wenchao Hu, Yu Ting Yeung, and Xiao Chen. Conv-Transformer Transducer:
Low Latency, Low Frame Rate, Streamable End-to-End Speech Recognition. In Proc. Interspeech
2020, pp. 5001‚Äì5005, 2020."
REFERENCES,0.730593607305936,"Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In Proceedings of the 32nd International Conference on Machine
Learning (ICML), pp. 448‚Äì456, 2015."
REFERENCES,0.7351598173515982,"Herve J¬¥egou, Matthijs Douze, and Cordelia Schmid.
Product quantization for nearest neighbor
search. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(1):117‚Äì128, 2011."
REFERENCES,0.7397260273972602,"Jacob Kahn, Ann Lee, and Awni Hannun. Self-training for end-to-end speech recognition. In 2020
IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 7084‚Äì
7088, 2020a."
REFERENCES,0.7442922374429224,"Jacob Kahn, Morgane Rivi`ere, Weiyi Zheng, Evgeny Kharitonov, Qiantong Xu, Pierre-Emmanuel
Mazar¬¥e, Julien Karadayi, Vitaliy Liptchinsky, Ronan Collobert, Christian Fuegen, et al. Libri-
light: A benchmark for ASR with limited or no supervision. In 2020 IEEE International Confer-
ence on Acoustics, Speech and Signal Processing (ICASSP), pp. 7669‚Äì7673, 2020b."
REFERENCES,0.7488584474885844,"Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd Interna-
tional Conference on Learning Representations, ICLR 2015, 2015."
REFERENCES,0.7534246575342466,"Tom Ko, Vijayaditya Peddinti, Daniel Povey, and Sanjeev Khudanpur. Audio augmentation for
speech recognition. In Proc. Interspeech 2015, pp. 3586‚Äì3589, 2015."
REFERENCES,0.7579908675799086,"Taku Kudo and John Richardson. SentencePiece: A simple and language independent subword
tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing: System Demonstrations, 2018."
REFERENCES,0.7625570776255708,"Davis Liang, Zhiheng Huang, and Zachary C Lipton. Learning noise-invariant representations for
robust speech recognition. In 2018 IEEE Spoken Language Technology Workshop (SLT), pp.
56‚Äì63, 2018."
REFERENCES,0.7671232876712328,"Andy T. Liu, Shu-wen Yang, Po-Han Chi, Po-chun Hsu, and Hung-yi Lee. Mockingjay: Unsuper-
vised speech representation learning with deep bidirectional transformer encoders. In 2020 IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 6419‚Äì6423,
2020."
REFERENCES,0.771689497716895,Published as a conference paper at ICLR 2022
REFERENCES,0.776255707762557,"Christoph L¬®uscher, Eugen Beck, Kazuki Irie, Markus Kitza, Wilfried Michel, Albert Zeyer, Ralf
Schl¬®uter, and Hermann Ney. RWTH ASR Systems for LibriSpeech: Hybrid vs Attention. In
Proc. Interspeech 2019, pp. 231‚Äì235, 2019."
REFERENCES,0.7808219178082192,"Vimal Manohar, Daniel Povey, and Sanjeev Khudanpur. Semi-supervised maximum mutual infor-
mation training of deep neural network acoustic models. In Proc. Interspeech 2015, pp. 2630‚Äì
2634, 2015."
REFERENCES,0.7853881278538812,"Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: An ASR
corpus based on public domain audio books. In 2015 IEEE international conference on acoustics,
speech and signal processing (ICASSP), pp. 5206‚Äì5210. IEEE, 2015."
REFERENCES,0.7899543378995434,"Daniel S. Park, Yu Zhang, Chung-Cheng Chiu, Youzheng Chen, Bo Li, William Chan, Quoc V. Le,
and Yonghui Wu. Specaugment on large scale datasets. In 2020 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP), pp. 6879‚Äì6883, 2020a."
REFERENCES,0.7945205479452054,"Daniel S. Park, Yu Zhang, Ye Jia, Wei Han, Chung-Cheng Chiu, Bo Li, Yonghui Wu, and Quoc V.
Le. Improved noisy student training for automatic speech recognition. In Proc. Interspeech 2020,
pp. 2817‚Äì2821, 2020b."
REFERENCES,0.7990867579908676,"Vijayaditya Peddinti, Yiming Wang, Daniel Povey, and Sanjeev Khudanpur. Low latency acoustic
modeling using temporal convolution and LSTMs. IEEE Signal Processing Letters, 25(3):373‚Äì
377, 2018."
REFERENCES,0.8036529680365296,"Vineel Pratap, Awni Hannun, Qiantong Xu, Jeff Cai, Jacob Kahn, Gabriel Synnaeve, Vitaliy
Liptchinsky, and Ronan Collobert. Wav2Letter++: A fast open-source speech recognition system.
In 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
pp. 6460‚Äì6464, 2019."
REFERENCES,0.8082191780821918,"Chandan KA Reddy, Harishchandra Dubey, Vishak Gopal, Ross Cutler, Sebastian Braun, Hannes
Gamper, Robert Aichner, and Sriram Srinivasan. ICASSP 2021 deep noise suppression challenge.
In 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
pp. 6623‚Äì6627. IEEE, 2021."
REFERENCES,0.8127853881278538,"Michael L. Seltzer, Dong Yu, and Yongqiang Wang. An investigation of deep neural networks for
noise robust speech recognition. In 2013 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), pp. 7398‚Äì7402, 2013."
REFERENCES,0.817351598173516,"Kihyuk Sohn. Improved deep metric learning with multi-class N-pair loss objective. In Advances in
Neural Information Processing Systems, volume 29, 2016."
REFERENCES,0.821917808219178,"Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overÔ¨Åtting. The journal of machine
learning research, 15(1):1929‚Äì1958, 2014."
REFERENCES,0.8264840182648402,"Gabriel Synnaeve, Qiantong Xu, Jacob Kahn, Tatiana Likhomanenko, Edouard Grave, Vineel
Pratap, Anuroop Sriram, Vitaliy Liptchinsky, and Ronan Collobert. End-to-end ASR: from su-
pervised to semi-supervised learning with modern architectures. In workshop on Self-supervision
in Audio and Speech (SAS), 37th International Conference on Machine Learning (ICML 2020),
2020a."
REFERENCES,0.8310502283105022,"Gabriel Synnaeve, Qiantong Xu, Jacob Kahn, Tatiana Likhomanenko, Edouard Grave, Vineel
Pratap, Anuroop Sriram, Vitaliy Liptchinsky, and Ronan Collobert. End-to-end ASR: from su-
pervised to semi-supervised learning with modern architectures. In workshop on Self-supervision
in Audio and Speech (SAS), 37th International Conference on Machine Learning (ICML 2020),
2020b."
REFERENCES,0.8356164383561644,"Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consis-
tency targets improve semi-supervised deep learning results. In Advances in Neural Information
Processing Systems, volume 30, 2017."
REFERENCES,0.8401826484018264,"Yuandong Tian, Xinlei Chen, and Surya Ganguli. Understanding self-supervised learning dynam-
ics without contrastive pairs. In Proceedings of the 38th International Conference on Machine
Learning (ICML), pp. 10268‚Äì10278, 2021."
REFERENCES,0.8447488584474886,Published as a conference paper at ICLR 2022
REFERENCES,0.8493150684931506,"Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding, 2019. arXiv:1807.03748v2."
REFERENCES,0.8538812785388128,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
≈Å ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-
mation Processing Systems, volume 30, 2017."
REFERENCES,0.8584474885844748,"Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and
composing robust features with denoising autoencoders. In Proceedings of the 25th International
Conference on Machine Learning (ICML), pp. 1096‚Äì1103, 2008."
REFERENCES,0.863013698630137,"Weiran Wang, Qingming Tang, and Karen Livescu.
Unsupervised pre-training of bidirectional
speech encoders via masked reconstruction. In 2020 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), pp. 6889‚Äì6893, 2020."
REFERENCES,0.867579908675799,"Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-
parametric instance discrimination. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pp. 3733‚Äì3742, 2018."
REFERENCES,0.8721461187214612,"Alex Xiao, Christian Fuegen, and Abdelrahman Mohamed. Contrastive semi-supervised learning
for asr. In 2021 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), pp. 3870‚Äì3874, 2021."
REFERENCES,0.8767123287671232,"Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V. Le. Self-training with noisy student
improves imagenet classiÔ¨Åcation. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), June 2020."
REFERENCES,0.8812785388127854,"Qiantong Xu, Tatiana Likhomanenko, Jacob Kahn, Awni Hannun, Gabriel Synnaeve, and Ronan
Collobert. Iterative pseudo-labeling for speech recognition. In Proc. Interspeech 2020, pp. 1006‚Äì
1010, 2020."
REFERENCES,0.8858447488584474,"Qiantong Xu, Alexei Baevski, Tatiana Likhomanenko, Paden Tomasello, Alexis Conneau, Ronan
Collobert, Gabriel Synnaeve, and Michael Auli. Self-training and pre-training are complementary
for speech recognition. In 2021 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), pp. 3030‚Äì3034, 2021."
REFERENCES,0.8904109589041096,"Yu Zhang, James Qin, Daniel S. Park, Wei Han, Chung-Cheng Chiu, Ruoming Pang, Quoc V. Le,
and Yonghui Wu. Pushing the limits of semi-supervised learning for automatic speech recogni-
tion. In NeurIPS 2020 workshop: Self-Supervised Learning for Speech and Audio Processing,
2020."
REFERENCES,0.8949771689497716,Published as a conference paper at ICLR 2022
REFERENCES,0.8995433789954338,"A
APPENDIX"
REFERENCES,0.9041095890410958,"A.1
OUTPUT UNITS COMPARISON FOR LANGUAGE MODEL DECODING"
REFERENCES,0.908675799086758,"We evaluate decoding performance of different combinations of SPIRAL output units and Trans-
former LM. The subword-level LM is trained on the same Librispeech LM corpus and shares the
same 1024 subwords as the SPIRAL model Ô¨Åne-tuned with subword units."
REFERENCES,0.91324200913242,"Table 6: ASR results Ô¨Åne-tuned from low-resource train-clean-100 and high-resource train-960. The
model units and language models for decoding are listed in Ô¨Åne-tuning units and LM respectively.
We compare SPIRAL BASE pre-trained on LS-960 and SPIRAL LARGE pre-trained on LL-60k with
previous methods. We report WER (%) on Librispeech dev/test sets."
REFERENCES,0.9178082191780822,"Model
Unlabeled
Fine-tuning
LM
dev
test
data
units
clean
other
clean
other"
REFERENCES,0.9223744292237442,"Low-resource
SPIRAL BASE
LS-960
subword
-
5.5
11.1
5.4
11.2
SPIRAL BASE
LS-960
char
-
5.3
11.0
5.4
11.1
SPIRAL BASE
LS-960
subword
word
2.9
6.8
3.2
7.2
SPIRAL BASE
LS-960
subword
subword
2.7
6.3
2.9
6.7
SPIRAL BASE
LS-960
char
word
2.3
5.8
2.7
6.1"
REFERENCES,0.9269406392694064,"SPIRAL LARGE
LL-60k
subword
-
3.3
5.9
3.3
6.3
SPIRAL LARGE
LL-60k
char
-
3.5
6.3
3.5
6.7
SPIRAL LARGE
LL-60k
subword
word
2.4
4.5
2.5
4.8
SPIRAL LARGE
LL-60k
subword
subword
2.3
4.5
2.4
4.8
SPIRAL LARGE
LL-60k
char
word
1.9
3.9
2.2
4.3"
REFERENCES,0.9315068493150684,"High-resource
SPIRAL LARGE
LL-60k
subword
-
2.1
4.3
2.2
4.6
SPIRAL LARGE
LL-60k
char
-
2.2
4.5
2.3
4.7
SPIRAL LARGE
LL-60k
subword
word
1.7
3.5
1.9
3.7
SPIRAL LARGE
LL-60k
subword
subword
1.6
3.3
1.7
3.5
SPIRAL LARGE
LL-60k
char
word
1.5
3.1
1.8
3.5"
REFERENCES,0.9360730593607306,"A.2
PERFORMANCE ON SYNTHETIC NOISY DATASET"
REFERENCES,0.9406392694063926,"On the synthetic noisy dataset (NS-Librispeech) with matched SNR range (0-30 dB) of the training
data, SPIRAL pre-trained and Ô¨Åne-tuned with MCT is more effective than applying MCT solely in
Ô¨Åne-tuning. We observe 5.3% and 9.1% relative WER reduction on synthetic noisy test-clean and
test-other sets respectively."
REFERENCES,0.9452054794520548,"Table 7: Evaluation on noise-robustness of the models. We use wav2vec 2.0 BASE released by the
authors as the baseline. The SPIRAL BASE models are pre-trained with LS-960 and Ô¨Åne-tuned with
train-clean-100. We report WER (%) on Librispeech test sets and synthetic noisy Librispeech test
sets at 0 - 30 dB (NS-Librispeech)."
REFERENCES,0.9497716894977168,"BASE model
Pre-train
Fine-tune
Librispeech
NS-Librispeech
w/ MCT
w/ MCT
clean
other
clean
other"
REFERENCES,0.954337899543379,"wav2vec 2.0
-
-
6.1
13.3
14.4
27.4
SPIRAL
-
-
5.4
11.2
12.2
23.3
SPIRAL
-
‚úì
5.7
11.4
7.6
16.5
SPIRAL
‚úì
-
5.7
11.5
7.4
15.8
SPIRAL
‚úì
‚úì
5.9
11.4
7.2
15.0"
REFERENCES,0.958904109589041,Published as a conference paper at ICLR 2022
REFERENCES,0.9634703196347032,"A.3
RESULTS OF ABLATION STUDIES"
REFERENCES,0.9680365296803652,Here are the results of ablation studies discussed in Section 5.3.
REFERENCES,0.9726027397260274,"Table 8: Ablation studies of input perturbation with SpecAugment and computation noise on teacher.
We list the mask ratio and mask length as p, L for time and frequency masks. The Ô¨Årst row is the
default setting of SPIRAL. We apply SPIRAL BASE Ô¨Åne-tuned with train-clean-100, and report
WER (%) on the Librispeech dev-other set."
REFERENCES,0.9771689497716894,"Time mask
Frequency mask
Computation noise
dev other"
REFERENCES,0.9817351598173516,"-
-
‚úì
11.1
-
-
-
13.2
0.025, 20
0.02, 20
‚úì
47.9
0.0125, 20
0.01, 20
‚úì
42.8
0.025, 10
0.02, 10
‚úì
39.4"
REFERENCES,0.9863013698630136,"Table 9: Ablation studies of predictor and projection head in SPIRAL. We apply SPIRAL BASE
Ô¨Åne-tuned with train-clean-100, and report WER (%) on the Librispeech dev-other set."
REFERENCES,0.9908675799086758,"Architecture
dev other"
REFERENCES,0.9954337899543378,"SPIRAL BASE
11.1
+ predictor use LN
11.6
+ conv proj. head
11.5
‚Äì predictor
13.7
+ conv proj. head
12.1"
