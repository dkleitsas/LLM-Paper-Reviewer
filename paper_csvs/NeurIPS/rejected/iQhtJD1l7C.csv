Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002680965147453083,"Improving the decision-making capabilities of agents is a key challenge on the
1"
ABSTRACT,0.005361930294906166,"road to artificial intelligence [25]. To improve the planning skills needed to make
2"
ABSTRACT,0.00804289544235925,"good decisions, MuZero’s agent [17, 10, 1, 18, 13, 7] combines prediction by a
3"
ABSTRACT,0.010723860589812333,"network model and planning by a tree search using the predictions. MuZero’s
4"
ABSTRACT,0.013404825737265416,"learning process can fail when predictions are poor but planning requires them
5"
ABSTRACT,0.0160857908847185,"[28]. We use this as an impetus to get the agent to explore parts of the decision
6"
ABSTRACT,0.01876675603217158,"tree in the environment that it otherwise would not explore. The agent achieves
7"
ABSTRACT,0.021447721179624665,"this, first by normal planning to come up with an improved policy [7]. Second, it
8"
ABSTRACT,0.024128686327077747,"randomly deviates from this policy at the beginning of each training episode. And
9"
ABSTRACT,0.02680965147453083,"third, it switches back to the improved policy at a random time step to experience
10"
ABSTRACT,0.029490616621983913,"the rewards from the environment associated with the improved policy, which is the
11"
ABSTRACT,0.032171581769437,"basis for learning the correct value expectation. The simple board game Tic-Tac-
12"
ABSTRACT,0.03485254691689008,"Toe is used to illustrate how this approach can improve the agent’s decision-making
13"
ABSTRACT,0.03753351206434316,"ability. The source code, written entirely in Java, is available at «a github url».
14"
INTRODUCTION,0.040214477211796246,"1
Introduction
15"
INTRODUCTION,0.04289544235924933,"A reinforcement learning agent has a simple interface to its environment [26, 25]: It partially observes
16"
INTRODUCTION,0.045576407506702415,"the environment, acts, and receives rewards (Figure 1).
17"
INTRODUCTION,0.04825737265415549,"Despite this simplicity, it is hypothesised [23] that intelligence, and its associated abilities, can be
18"
INTRODUCTION,0.05093833780160858,"understood as subserving the maximisation of reward.
19"
INTRODUCTION,0.05361930294906166,"Following this idea, MuZero [17] achieved a new state-of-the-art, outperforming all previous algo-
20"
INTRODUCTION,0.05630026809651475,"rithms on the Atari suite and matching the superhuman performance of its predecessor AlphaZero at
21"
INTRODUCTION,0.058981233243967826,"Go, Chess and Shogi. The MuZero agent learned acting through self-play, without even knowing the
22"
INTRODUCTION,0.06166219839142091,"rules of the game - strictly following the agent-environment interface.
23"
INTRODUCTION,0.064343163538874,"The agent’s mind combines fast predictions from a neural network model with slow algorithmic
24"
INTRODUCTION,0.06702412868632708,"planning. This is similar to the way humans use fast intuitive and slow rational thinking [11].
25"
INTRODUCTION,0.06970509383378017,"Despite MuZero’s successes, its learning procedure can fail if the value prediction is poor where
26"
INTRODUCTION,0.07238605898123325,"planning needs it. It has recently been shown how an amateur-level agent can beat KataGo [29, 30],
27"
INTRODUCTION,0.07506702412868632,"a state-of-the-art open-source Go implementation based on MuZero’s predecessor AlphaZero, by
28"
INTRODUCTION,0.0777479892761394,"leading KataGo to parts of the decision tree that it would never visit in any self-play training
29"
INTRODUCTION,0.08042895442359249,"games [28].
30"
INTRODUCTION,0.08310991957104558,"We use this as an impetus to make the agent curious about parts of the decision tree for which it
31"
INTRODUCTION,0.08579088471849866,"otherwise gains little or no experience in the environment. We do not claim to provide a solution that
32"
INTRODUCTION,0.08847184986595175,"solves all related problems, especially not the motivation example.
33"
INTRODUCTION,0.09115281501340483,"planning - ""rationale""
finding an improved
policy for the next action"
INTRODUCTION,0.0938337801608579,"action
decision making
deciding the next action"
INTRODUCTION,0.09651474530831099,"observation
reward
legal actions"
INTRODUCTION,0.09919571045576407,"environment
agent"
INTRODUCTION,0.10187667560321716,"experience
memory of episodes
such as games"
INTRODUCTION,0.10455764075067024,"model - ""intuition""
state representation,
next state generation,
reward prediction,
value prediction,
policy prediction"
INTRODUCTION,0.10723860589812333,"black
box"
INTRODUCTION,0.10991957104557641,"decision making
uses the
improved policy"
INTRODUCTION,0.1126005361930295,"planning
is based
on the model"
INTRODUCTION,0.11528150134048257,"model learns
from
experience
Reanalyse
is an additional
learning option"
INTRODUCTION,0.11796246648793565,"Figure 1: The interaction between the agent and the environment: The agent makes observations
about the environment, is informed about the legal actions it can choose from, and potentially receives
a reward after taking an action. This is the experienced information about the otherwise black-box
environment. Together with internal information, such as actions taken, they form a memory of
episodes. The agent uses this experience to train a model. The model’s predictions include an in-mind
state representation for observations, the value and policy for a state representation, the reward and
the next state representation for an action. Based on the model’s predictions, the agent plans an
improved policy by partially unrolling the decision tree internally. Based on the improved policy
resulting from the planning, the agent decides which action to take, taking into account its desire
to explore the environment. The agent can also revisit states from its memory and re-analyse them
[17, 18, 31]. With Reanalyse, there are two model optimisation loops: one via the environment and
one entirely in the agent’s mind."
INTRODUCTION,0.12064343163538874,"Since the agent in this approach is actively seeking new experiences to feed into its model, we call
34"
INTRODUCTION,0.12332439678284182,"it curiosity. We distinguish two domains of this curiosity - one for known unknowns and one for
35"
INTRODUCTION,0.1260053619302949,"unknown unknowns [12, 4]. Our approach falls into the category of curious about unknown unknowns,
36"
INTRODUCTION,0.128686327077748,"as the agent seeks new experiences regardless of confidence in existing knowledge.
37"
INTRODUCTION,0.13136729222520108,"This active search consists of three parts: First, the agent performs normal planning at each time step,
38"
INTRODUCTION,0.13404825737265416,"resulting in an improved policy. Second, in each training episode, the agent starts to act according
39"
INTRODUCTION,0.13672922252010725,"to a policy that is steered by a temperature parameter T > 1 from the optimised policy received
40"
INTRODUCTION,0.13941018766756033,"from planning (T = 1) towards a random action selection (T →∞). Third, to still learn the value
41"
INTRODUCTION,0.14209115281501342,"of following the optimised policy from the associated environmental rewards, the agent randomly
42"
INTRODUCTION,0.1447721179624665,"switches back to following the improved policy from planning. Thus, the action policy for all actions
43"
INTRODUCTION,0.14745308310991956,"is a hybrid policy.
44"
INTRODUCTION,0.15013404825737264,"This makes the decision process a higher-level process that uses the tree search results from the
45"
INTRODUCTION,0.15281501340482573,"planning process, but not necessarily on a one-to-one basis. So when we structure the agent, we
46"
INTRODUCTION,0.1554959785522788,"add a decision making component that is responsible for deciding the next action, Figure 1. This
47"
INTRODUCTION,0.1581769436997319,"responsibility includes, in particular, adding curiosity. With this structuring, we hope to contribute to
48"
INTRODUCTION,0.16085790884718498,"the cross-disciplinary quest for a common model of the intelligent decision maker [25].
49"
INTRODUCTION,0.16353887399463807,"We also investigate two other cases with small contributions from us, arriving at three cases where
50"
INTRODUCTION,0.16621983914209115,"we contribute - all three about the role of randomness:
51"
INTRODUCTION,0.16890080428954424,"Additional randomness after planning Use of the hybrid policy introduced here in the training
52"
INTRODUCTION,0.17158176943699732,"context.
53"
INTRODUCTION,0.1742627345844504,"Additional randomness before planning In AlphaZero and MuZero, a Dirichlet noise was added
54"
INTRODUCTION,0.1769436997319035,"to the prior probabilities in the root node when entering the tree search to aid exploration.
55"
INTRODUCTION,0.17962466487935658,"It was removed in Gumbel MuZero since it was not necessary to improve the policy with
56"
INTRODUCTION,0.18230563002680966,"the model fixed. However, we use Dirichlet noise for the following heuristic reason: it adds
57"
INTRODUCTION,0.18498659517426275,"a force toward choosing actions without unfair preference if the actions would not differ
58"
INTRODUCTION,0.1876675603217158,"in value under a perfect strategy. If no force is added, one such action may be favoured
59"
INTRODUCTION,0.1903485254691689,"by the agent, potentially preventing the agent from gaining experience from the parts of
60"
INTRODUCTION,0.19302949061662197,"the decision tree after the unfavourable actions. This can lead to a worse model, a worse
61"
INTRODUCTION,0.19571045576407506,"planning result, and therefore worse decisions. Another argument for avoiding unwarranted
62"
INTRODUCTION,0.19839142091152814,"bias is to be stable against potential future changes in the environment that would favour an
63"
INTRODUCTION,0.20107238605898123,"action other than the one the agent has learned to choose. We are aware that changing the
64"
INTRODUCTION,0.2037533512064343,"policy with Dirichlet noise may cost some inference steps from the planning budget.
65"
INTRODUCTION,0.2064343163538874,"Less randomness during planning in eager playout Gumbel MuZero enters the planning for train-
66"
INTRODUCTION,0.20911528150134048,"ing playouts with the model’s policy and draws from this policy - technically introducing a
67"
INTRODUCTION,0.21179624664879357,"Gumbel value to achieve drawing without replacement. For the training context, this ensures
68"
INTRODUCTION,0.21447721179624665,"that all root actions are considered exactly according to the existing knowledge of the agent.
69"
INTRODUCTION,0.21715817694369974,"For an eager playout context, the situation is different. When making a decision only once, it
70"
INTRODUCTION,0.21983914209115282,"can be beneficial for the agent to decide eagerly - like changing the temperature from 1 to 0.
71"
INTRODUCTION,0.2225201072386059,"With this in mind, we also examine the playout case T=0 by setting the Gumbel value to 0.
72"
INTRODUCTION,0.225201072386059,"We show for the simple board game Tic-Tac-Toe that these three contributions improve the decisions
73"
INTRODUCTION,0.22788203753351208,"made by the agent. We use confidence intervals at the 99% confidence level. In addition, we provide
74"
INTRODUCTION,0.23056300268096513,"experimental examples to support our interpretation of how the improvements through using the
75"
INTRODUCTION,0.23324396782841822,"hybrid policy and through using the Dirichlet noise occur - in these cases without proving statistical
76"
INTRODUCTION,0.2359249329758713,"significance.
77"
INTRODUCTION,0.2386058981233244,"A limitation of this work is that we do not prove that we can reproduce all the results obtained by
78"
INTRODUCTION,0.24128686327077747,"applying MuZero, in particular to the board games Go, Chess, Shogi and the Atari game suite.
79"
INTRODUCTION,0.24396782841823056,"Another limitation is that we do not show the application to the Reanalyse [17, 18, 31] loop here.
80"
RECENT HISTORICAL BACKGROUND,0.24664879356568364,"2
Recent Historical Background
81"
RECENT HISTORICAL BACKGROUND,0.24932975871313673,"AlphaGo [20] was the first AI engine to beat the best human player in a full-sized game of Go in
82"
RECENT HISTORICAL BACKGROUND,0.2520107238605898,"March 2016. It used value networks to evaluate board positions and policy networks to select moves.
83"
RECENT HISTORICAL BACKGROUND,0.2546916890080429,"The networks were trained using a combination of supervised learning from human expert games,
84"
RECENT HISTORICAL BACKGROUND,0.257372654155496,"and reinforcement learning from self-play games. The reinforcement learning used a tree search,
85"
RECENT HISTORICAL BACKGROUND,0.26005361930294907,"which combines Monte Carlo simulation with value and policy networks.
86"
RECENT HISTORICAL BACKGROUND,0.26273458445040215,"AlphaGo Zero [21] eliminated the need to train with external input games. Thus, AlphaZero [22]
87"
RECENT HISTORICAL BACKGROUND,0.26541554959785524,"generalised the AlphaGo algorithm and applied it to the games of Chess and Shogi. A major
88"
RECENT HISTORICAL BACKGROUND,0.2680965147453083,"improvement to the algorithm was the continuous updating of the network.
89"
RECENT HISTORICAL BACKGROUND,0.2707774798927614,"In 2020, MuZero [17] has eliminated the need for a resettable simulator. Instead, MuZero learns a
90"
RECENT HISTORICAL BACKGROUND,0.2734584450402145,"model of the environment to the extent necessary for its in-mind planning. It extends AlphaZero’s
91"
RECENT HISTORICAL BACKGROUND,0.2761394101876676,"successful application of the classic board games Go, Chess and Shogi to 57 Atari games. MuZero
92"
RECENT HISTORICAL BACKGROUND,0.27882037533512066,"Unplugged [18] allows the agent to learn by re-analysing previously experienced episodes in mind.
93"
RECENT HISTORICAL BACKGROUND,0.28150134048257375,"Sampled Muzero [10] extends MuZero to domains with arbitrarily complex action spaces by planning
94"
RECENT HISTORICAL BACKGROUND,0.28418230563002683,"over sampled actions. Stochastic MuZero [1] extends MuZero’s deterministic model to a stochastic
95"
RECENT HISTORICAL BACKGROUND,0.2868632707774799,"model that incorporates after states. It is demonstrated in the games 2048 and Backgammon.
96"
RECENT HISTORICAL BACKGROUND,0.289544235924933,"EfficientZero [31], based on MuZero Unplugged [18] and SPR [19] achieved above-average human
97"
RECENT HISTORICAL BACKGROUND,0.29222520107238603,"performance on Atari games with only two hours of real-time gaming experience. This experience
98"
RECENT HISTORICAL BACKGROUND,0.2949061662198391,"efficiency was a milestone. Main contributions are (1) Self-Supervised Consistency Loss, (2) End-To-
99"
RECENT HISTORICAL BACKGROUND,0.2975871313672922,"End Prediction of the Value Prefix, (3) Model-Based Off-Policy Correction. EfficientZero’s source
100"
RECENT HISTORICAL BACKGROUND,0.3002680965147453,"code is available on GitHub.
101"
RECENT HISTORICAL BACKGROUND,0.30294906166219837,"While MuZero’s planning step produces an asymptotic policy improvement when many steps are
102"
RECENT HISTORICAL BACKGROUND,0.30563002680965146,"used to unfold the decision tree, Gumbel MuZero [7, 6] introduced a planning algorithm that could
103"
RECENT HISTORICAL BACKGROUND,0.30831099195710454,"improve the policy for any budget of unfolding steps - using a given model. The source code for the
104"
RECENT HISTORICAL BACKGROUND,0.3109919571045576,"tree search is available on GitHub.
105"
RECENT HISTORICAL BACKGROUND,0.3136729222520107,"As a commercially relevant use case, MuZero has been applied to video stream compression [13].
106"
RECENT HISTORICAL BACKGROUND,0.3163538873994638,"And as the first extension of AlphaZero to mathematics, AlphaTensor [8] demonstrates the ability to
107"
RECENT HISTORICAL BACKGROUND,0.3190348525469169,"accelerate the process of algorithmic discovery by finding faster matrix multiplication algorithms.
108"
RECENT HISTORICAL BACKGROUND,0.32171581769436997,"The open-source community has applied the AlphaZero and MuZero algorithms to various projects.
109"
RECENT HISTORICAL BACKGROUND,0.32439678284182305,"Notable examples in the field of board games are Leela Chess Zero [14] and KataGo [29, 30].
110"
RECENT HISTORICAL BACKGROUND,0.32707774798927614,"The existence of open-source implementations encouraged the search for weaknesses in the agents. It
111"
RECENT HISTORICAL BACKGROUND,0.3297587131367292,"was shown how adversarial policies could beat professional-level KataGo agents [28] using a strategy
112"
RECENT HISTORICAL BACKGROUND,0.3324396782841823,"that an amateur player could follow. The main idea of the strategy is to lead the KataGo agent into
113"
RECENT HISTORICAL BACKGROUND,0.3351206434316354,"areas of the decision tree where it has a poor value premise and therefore makes weak decisions.
114"
RELATED WORK,0.3378016085790885,"3
Related Work
115"
RELATED WORK,0.34048257372654156,"Finding an appropriate trade-off between exploration and exploitation is a core challenge in reinforce-
116"
RELATED WORK,0.34316353887399464,"ment learning [26]. By building a model that includes dynamics, as in MuZero [17], the magnitude
117"
RELATED WORK,0.34584450402144773,"of this challenge has increased because there are two worlds on stage: The environment as the real
118"
RELATED WORK,0.3485254691689008,"world and the model as an in-mind world. Gumbel MuZero [7] brought a planning algorithm that
119"
RELATED WORK,0.3512064343163539,"monotonically improves the policy with any budget of recurrent inference steps within MuZero’s
120"
RELATED WORK,0.353887399463807,"given in-mind world.
121"
RELATED WORK,0.35656836461126007,"AlphaZero [22] and MuZero [17] add a Dirichlet noise to the model’s policy predictions before
122"
RELATED WORK,0.35924932975871315,"starting the tree search in their planning step to ensure exploration.
123"
RELATED WORK,0.36193029490616624,"The off-policy maximum entropy deep reinforcement learning algorithm SAC [9] uses the entropy
124"
RELATED WORK,0.3646112600536193,"of the policy times a temperature factor as an additional reward. Adding such an additional reward
125"
RELATED WORK,0.3672922252010724,"falls into the category of curious about known unknowns as this intrinsic reward is derived from the
126"
RELATED WORK,0.3699731903485255,"agent’s policy.
127"
RELATED WORK,0.3726541554959786,"After planning, MuZero [17] uses a temperature parameter T to vary between T = 1 for exploration
128"
RELATED WORK,0.3753351206434316,"and T = 0 for exploitation following AlphaZero’s [22] approach for board games. For Atari games,
129"
RELATED WORK,0.3780160857908847,"this is done for all moves, not just the first few moves as in board games. The temperature is lowered
130"
RELATED WORK,0.3806970509383378,"as a function of the number of training steps of the network, thereby shifting the planning policy from
131"
RELATED WORK,0.38337801608579086,"exploration to exploitation.
132"
RELATED WORK,0.38605898123324395,"Go-Exploit [27] based on AlphaZero samples the starting state of its self-play trajectories from an
133"
RELATED WORK,0.38873994638069703,"archive of states of interest. This approach can only be used if the environment interaction allows
134"
RELATED WORK,0.3914209115281501,"episodes to start from any state.
135"
WHAT THIS WORK BUILDS UPON,0.3941018766756032,"4
What This Work Builds Upon
136"
WHAT THIS WORK BUILDS UPON,0.3967828418230563,"This work builds on MuZero [17]. For our examples, we use the case of non-intermediate rewards
137"
WHAT THIS WORK BUILDS UPON,0.39946380697050937,"from the environment. For the planning component and the model base we use Gumbel MuZero [7].
138"
WHAT THIS WORK BUILDS UPON,0.40214477211796246,"The model is extended for Self-Supervised Consistency Loss from EfficientZero [31]. The resulting
139"
WHAT THIS WORK BUILDS UPON,0.40482573726541554,"model is presented in Appendix A.
140"
ILLUSTRATING THE NEED FOR IMPROVEMENT OF THE AGENT,0.4075067024128686,"5
Illustrating the Need for Improvement of the Agent
141"
ILLUSTRATING THE NEED FOR IMPROVEMENT OF THE AGENT,0.4101876675603217,"The agent’s need to explore the decision tree beyond good actions can be illustrated by the simple
142"
ILLUSTRATING THE NEED FOR IMPROVEMENT OF THE AGENT,0.4128686327077748,"game of Tic-Tac-Toe [3].
143"
ILLUSTRATING THE NEED FOR IMPROVEMENT OF THE AGENT,0.4155495978552279,"In Tic-Tac-Toe, the optimal outcome for both players is a draw. Figure 2 shows such a game.
144 time"
ILLUSTRATING THE NEED FOR IMPROVEMENT OF THE AGENT,0.41823056300268097,Figure 2: An example of an optimally played game of Tic-Tac-Toe.
ILLUSTRATING THE NEED FOR IMPROVEMENT OF THE AGENT,0.42091152815013405,"Suppose an agent takes the role of both players - self-play - and only makes perfect moves in the
145"
ILLUSTRATING THE NEED FOR IMPROVEMENT OF THE AGENT,0.42359249329758714,"environment. Then he would never observe from the environment what could happen after a bad
146"
ILLUSTRATING THE NEED FOR IMPROVEMENT OF THE AGENT,0.4262734584450402,"move, e.g. after the first bad move shown in Figure 3.
147 time"
ILLUSTRATING THE NEED FOR IMPROVEMENT OF THE AGENT,0.4289544235924933,after a bad action
ILLUSTRATING THE NEED FOR IMPROVEMENT OF THE AGENT,0.4316353887399464,"Figure 3: A Tic-Tac-Toe game with two bad actions (red), one by player o and one by player x."
ILLUSTRATING THE NEED FOR IMPROVEMENT OF THE AGENT,0.4343163538873995,"Suppose such an agent takes the role of player x and plays against another player o. If a player o
148"
ILLUSTRATING THE NEED FOR IMPROVEMENT OF THE AGENT,0.43699731903485256,"makes a bad move, the agent may not be able to take advantage of it and win. Instead, the agent
149"
ILLUSTRATING THE NEED FOR IMPROVEMENT OF THE AGENT,0.43967828418230565,"might make a bad move as in Figure 3 and lose.
150"
ILLUSTRATING THE NEED FOR IMPROVEMENT OF THE AGENT,0.44235924932975873,"To observe more than the world of perfect actions, the agent must deviate from the perfect game
151"
ILLUSTRATING THE NEED FOR IMPROVEMENT OF THE AGENT,0.4450402144772118,"when it acts in the environment during training. This could be achieved by separating the search for
152"
ILLUSTRATING THE NEED FOR IMPROVEMENT OF THE AGENT,0.4477211796246649,"an optimised policy for the next action from the decision of what to do next. During training the
153"
ILLUSTRATING THE NEED FOR IMPROVEMENT OF THE AGENT,0.450402144772118,"decision component shown in Figure 1 could deviate from its optimised policy to get to novel parts
154"
ILLUSTRATING THE NEED FOR IMPROVEMENT OF THE AGENT,0.45308310991957107,"of the decision tree and finish from there according to its optimised policy.
155"
AGENT IMPROVEMENTS,0.45576407506702415,"6
Agent Improvements
156"
AGENT IMPROVEMENTS,0.4584450402144772,"Two of the three contributions of the paper mentioned in the introduction are described here in more
157"
AGENT IMPROVEMENTS,0.46112600536193027,"detail - supported by small proofs in the Appendix C.
158"
EXPLORING USING A HYBRID POLICY,0.46380697050938335,"6.1
Exploring Using a Hybrid Policy
159"
EXPLORING USING A HYBRID POLICY,0.46648793565683644,"Suppose we have a normal policy Pnormal and an exploring policy Pexplore. Also, suppose the
160"
EXPLORING USING A HYBRID POLICY,0.4691689008042895,"model is to be trained using Pnormal. In a playout with Pexplore ̸= Pnormal there would be an
161"
EXPLORING USING A HYBRID POLICY,0.4718498659517426,"off-policy-issue for the value target [18]. To avoid this problem, the playout could be done with a
162"
EXPLORING USING A HYBRID POLICY,0.4745308310991957,"hybrid policy Phybrid, starting with Pexplore and switching to Pnormal at a random time tstartNormal
163"
EXPLORING USING A HYBRID POLICY,0.4772117962466488,"before the expected end of the episode tend.
164"
EXPLORING USING A HYBRID POLICY,0.47989276139410186,"Phybrid =
Pexploring
if t < tstartNormal
Pnormal
if t ≥tstartNormal
(1)"
EXPLORING USING A HYBRID POLICY,0.48257372654155495,"tstartNormal = random(0, tend)
(2)"
EXPLORING USING A HYBRID POLICY,0.48525469168900803,"...
..."
EXPLORING USING A HYBRID POLICY,0.4879356568364611,"Figure 4: Before tstartNormal the actions are chosen according to the exploring policy Pexploring.
From tstartNormal the actions are chosen according to the normal policy Pnormal."
EXPLORING USING A HYBRID POLICY,0.4906166219839142,"The value target after tstartNormal could then be set up just as without exploration as the value
165"
EXPLORING USING A HYBRID POLICY,0.4932975871313673,"information propagates backwards in time during training and is therefore not influenced by what
166"
EXPLORING USING A HYBRID POLICY,0.4959785522788204,"happened before tstartNormal. But the value target before tstartNormal would be set to keep its
167"
EXPLORING USING A HYBRID POLICY,0.49865951742627346,"existing value. Therefore, the value function would only learn from the normal policy.
168"
EXPLORING USING A HYBRID POLICY,0.5013404825737265,"We concretise Phybrid in two steps. In the first step, we specify Pexploring as a drawing from a
169"
EXPLORING USING A HYBRID POLICY,0.5040214477211796,"probability distribution
170"
EXPLORING USING A HYBRID POLICY,0.5067024128686327,"pexploring = softmax (ln(pnormal)/T)
(3)"
EXPLORING USING A HYBRID POLICY,0.5093833780160858,"with a temperature of T > 1.1 pnormal is used for the models policy training target.
171"
EXPLORING USING A HYBRID POLICY,0.5120643431635389,"In the second step, we concretise pnormal to be the improved policy of Gumbel MuZero derived from
172"
EXPLORING USING A HYBRID POLICY,0.514745308310992,"the completed Q-values in the notation of Gumbel MuZero [7]. Using equation 21 in Appendix C.1
173"
EXPLORING USING A HYBRID POLICY,0.517426273458445,"we get
174"
EXPLORING USING A HYBRID POLICY,0.5201072386058981,"pexploring = softmax
logits + σ(Qcompleted) T 
(4)"
EXPLORING USING A HYBRID POLICY,0.5227882037533512,"The value target for the non-intermediate reward case is then given by
175"
EXPLORING USING A HYBRID POLICY,0.5254691689008043,"vtarget
t
=
vinitialInference,t
if t < tstartNormal
rmeasured
tend
if t ≥tstartNormal
(5)"
EXPLORING USING A HYBRID POLICY,0.5281501340482574,"where rmeasured
tend
is the reward returned by the environment and vinitialInference is the value v0
t
176"
EXPLORING USING A HYBRID POLICY,0.5308310991957105,"produced by the model version that is used when acting in the environment. This ensures that the
177"
EXPLORING USING A HYBRID POLICY,0.5335120643431636,"value for this model version is not forced but later model versions taken from a buffer are forced
178"
EXPLORING USING A HYBRID POLICY,0.5361930294906166,"towards vinitialInference. See Appendix D.5 for why we did not use the improved value from
179"
EXPLORING USING A HYBRID POLICY,0.5388739946380697,"planning as the target value.
180"
EAGER PLAYOUT WITHOUT GUMBEL NOISE,0.5415549597855228,"6.2
Eager Playout without Gumbel noise
181"
EAGER PLAYOUT WITHOUT GUMBEL NOISE,0.5442359249329759,"When planning according to Gumbel MuZero [7] in an eager playout, we can enter planning with a
182"
EAGER PLAYOUT WITHOUT GUMBEL NOISE,0.546916890080429,"temperature 0 ≤T ≤1 and still improve the decision by planning as shown in Appendix C.4. This is
183"
EAGER PLAYOUT WITHOUT GUMBEL NOISE,0.5495978552278821,"especially true for T →0 which we achieve by setting the Gumbel value to 0 (see Appendix C.3).
184"
EXPERIMENTS - GAME TIC-TAC-TOE,0.5522788203753352,"7
Experiments - Game Tic-Tac-Toe
185"
EXPERIMENTS - GAME TIC-TAC-TOE,0.5549597855227882,"The paper’s three contributions are tested on the game Tic-Tac-Toe. Appendix D informs about
186"
EXPERIMENTS - GAME TIC-TAC-TOE,0.5576407506702413,"experimental details, Appendix E about the open source implementation used to run the experiments.
187"
TRAINING WITH AND WITHOUT EXPLORING - ALL GAMES,0.5603217158176944,"7.1
Training With and Without Exploring - All Games
188 0 500 1000 1500 2000 2500 3000 3500 4000"
TRAINING WITH AND WITHOUT EXPLORING - ALL GAMES,0.5630026809651475,"0
100
200
300
400
500
600
700
800
900
1000
1100
1200 0 2 4 6 8 10 12 14 16 18 20"
TRAINING WITH AND WITHOUT EXPLORING - ALL GAMES,0.5656836461126006,"0
100
200
300
400
500
600
700
800
900
1000 1100 1200"
TRAINING WITH AND WITHOUT EXPLORING - ALL GAMES,0.5683646112600537,"Figure 5: Number of bad decisions as a function of the training epoch - mean value over 10 samples
with 99% confidence intervals. For details on the counting of the bad decisions see Appendix D."
TRAINING WITH AND WITHOUT EXPLORING - ALL GAMES,0.5710455764075067,"In Tic-Tac-Toe, the agent shows a large difference in the quality of decisions depending on whether
189"
TRAINING WITH AND WITHOUT EXPLORING - ALL GAMES,0.5737265415549598,"the exploration introduced in the previous section is turned on or off. While without this exploration
190"
TRAINING WITH AND WITHOUT EXPLORING - ALL GAMES,0.5764075067024129,"1Note that the temperature parameter T in MuZero [17] varies between T = 1 for exploration and T = 0 for
exploitation, whereas here we explore with a temperature of T > 1."
TRAINING WITH AND WITHOUT EXPLORING - ALL GAMES,0.579088471849866,"the average of bad decisions for a trained model applied to all possible game decisions is 340 ± 80
191"
TRAINING WITH AND WITHOUT EXPLORING - ALL GAMES,0.5817694369973191,"after 1000 epochs, with exploration, it is 0, 8 ± 0, 3 (see Figure 5). This is an improvement by a
192"
TRAINING WITH AND WITHOUT EXPLORING - ALL GAMES,0.5844504021447721,"factor 435 ± 190.
193"
TRAINING WITH AND WITHOUT EXPLORING - ONE GAME,0.5871313672922251,"7.2
Training With and Without Exploring - One Game
194"
TRAINING WITH AND WITHOUT EXPLORING - ONE GAME,0.5898123324396782,"To gain insight into the cause of the effect seen in Figure 5, we now restrict our investigation to the
195"
TRAINING WITH AND WITHOUT EXPLORING - ONE GAME,0.5924932975871313,"particular game shown in Figure 3. Note that the second move in this particular game is already a bad
196"
TRAINING WITH AND WITHOUT EXPLORING - ONE GAME,0.5951742627345844,"move, so all the states behind that move would not occur in perfect play.
197"
TRAINING WITH AND WITHOUT EXPLORING - ONE GAME,0.5978552278820375,"From the model versions trained without exploration, we look for a model version with which the
198"
TRAINING WITH AND WITHOUT EXPLORING - ONE GAME,0.6005361930294906,"agent would make the second bad decision in the situation of Figure 3. To find out why it does so, and
199"
TRAINING WITH AND WITHOUT EXPLORING - ONE GAME,0.6032171581769437,"why the agent using a model trained with exploration would not, we look at the value expectation of
200"
TRAINING WITH AND WITHOUT EXPLORING - ONE GAME,0.6058981233243967,"the model vτ
t , since planning relies heavily on the quality of the value expectation. t denotes the time
201"
TRAINING WITH AND WITHOUT EXPLORING - ONE GAME,0.6085790884718498,"at which the initial inference starts and the in-mind time τ denotes the number of recurrent inference
202"
TRAINING WITH AND WITHOUT EXPLORING - ONE GAME,0.6112600536193029,"steps from that point. For a detailed definition of vτ
t , see Appendix A.1.
203"
TRAINING WITH AND WITHOUT EXPLORING - ONE GAME,0.613941018766756,"When examining the value expectation of the model, it is not sufficient to consider the value expecta-
204"
TRAINING WITH AND WITHOUT EXPLORING - ONE GAME,0.6166219839142091,"tion v0
t immediately after the initial inference. Since the unfolding of the decision tree happens at the
205"
TRAINING WITH AND WITHOUT EXPLORING - ONE GAME,0.6193029490616622,"in-mind time τ, we need to look for all relevant in-mind times τ.
206"
TRAINING WITH AND WITHOUT EXPLORING - ONE GAME,0.6219839142091153,"Therefore we examine
207"
TRAINING WITH AND WITHOUT EXPLORING - ONE GAME,0.6246648793565683,"vτ
t (t′, tstart) ="
TRAINING WITH AND WITHOUT EXPLORING - ONE GAME,0.6273458445040214,"(
v0
t′
if t′ < tstart
vt′−tstart
tstart
if t′ ≥tstart
(6)"
TRAINING WITH AND WITHOUT EXPLORING - ONE GAME,0.6300268096514745,"with 0 ≤t′ ≤18 and 0 ≤tstart ≤8.
208"
TRAINING WITH AND WITHOUT EXPLORING - ONE GAME,0.6327077747989276,"In Figure 6 we examine what the expectations of a particular model vτ
t look like.
209 -1 -0,5 0 0,5 1"
TRAINING WITH AND WITHOUT EXPLORING - ONE GAME,0.6353887399463807,"0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18 -1 -0,5 0 0,5 1"
TRAINING WITH AND WITHOUT EXPLORING - ONE GAME,0.6380697050938338,"0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18"
TRAINING WITH AND WITHOUT EXPLORING - ONE GAME,0.6407506702412868,exploration-on
TRAINING WITH AND WITHOUT EXPLORING - ONE GAME,0.6434316353887399,exploration-off
TRAINING WITH AND WITHOUT EXPLORING - ONE GAME,0.646112600536193,"Figure 6: model version epoch = 1028, tstart = 0 in light grey to tstart = 8 in dark grey - for the
exploration off case tstart = 6 in red. The red value expectation falsely pretends that player x’s bad
move would be good."
TRAINING WITH AND WITHOUT EXPLORING - ONE GAME,0.6487935656836461,"This is an example of a plausible cause - no statistical significance is claimed - that prevents agents
210"
TRAINING WITH AND WITHOUT EXPLORING - ONE GAME,0.6514745308310992,"trained without the additional exploration from making correct decisions: The value expectation of
211"
TRAINING WITH AND WITHOUT EXPLORING - ONE GAME,0.6541554959785523,"the model provides wrong values. The planning that uses them has no chance of leading to a correct
212"
TRAINING WITH AND WITHOUT EXPLORING - ONE GAME,0.6568364611260054,"decision.
213"
PLAYOUT WITH AND WITHOUT GUMBEL NOISE - ALL GAMES,0.6595174262734584,"7.3
Playout With and Without Gumbel Noise - All Games
214"
PLAYOUT WITH AND WITHOUT GUMBEL NOISE - ALL GAMES,0.6621983914209115,"The playouts during the test in Figure 5 were done with the same Gumbel value as during training.
215"
PLAYOUT WITH AND WITHOUT GUMBEL NOISE - ALL GAMES,0.6648793565683646,"Playing out eagerly by setting the Gumbel value to 0 during planning reduces the number of bad
216"
PLAYOUT WITH AND WITHOUT GUMBEL NOISE - ALL GAMES,0.6675603217158177,"decisions. Figure 7 plots the difference number of bad decisions with Gumbel minus the number of bad
217"
PLAYOUT WITH AND WITHOUT GUMBEL NOISE - ALL GAMES,0.6702412868632708,"decisions without Gumbel. In the exploration-on case, we see an improvement by a factor 2.1 ± 0.3.
218 0 20 40 60 80 100 120 140"
PLAYOUT WITH AND WITHOUT GUMBEL NOISE - ALL GAMES,0.6729222520107239,"0
100
200
300
400
500
600
700
800
900
1000
1100
1200"
PLAYOUT WITH AND WITHOUT GUMBEL NOISE - ALL GAMES,0.675603217158177,"Figure 7: Number of bad decisions in playouts with Gumbel minus the number of bad decisions in
playouts without Gumbel - a rolling average of the last 50 epochs, mean value over 10 samples, 99%
confidence intervals."
TRAINING WITH AND WITHOUT DIRICHLET NOISE - ALL GAMES,0.67828418230563,"7.4
Training With and Without Dirichlet Noise - All Games
219"
TRAINING WITH AND WITHOUT DIRICHLET NOISE - ALL GAMES,0.6809651474530831,"Figure 5 is based on models trained with Dirichlet noise added to the policy entering the tree search.
220"
TRAINING WITH AND WITHOUT DIRICHLET NOISE - ALL GAMES,0.6836461126005362,"We compare the decision performance of these models with models trained without Dirichlet noise,
221"
TRAINING WITH AND WITHOUT DIRICHLET NOISE - ALL GAMES,0.6863270777479893,"leaving all other hyperparameters unchanged. In the exploration-on case, we see an improvement by
222"
TRAINING WITH AND WITHOUT DIRICHLET NOISE - ALL GAMES,0.6890080428954424,"a factor 3.6 ± 1.2 using Dirichlet noise, see Figure 8 compared to Figure 5. Appendix D.7 speculates
223"
TRAINING WITH AND WITHOUT DIRICHLET NOISE - ALL GAMES,0.6916890080428955,"why this is the case.
224 0 50 100 150 200 250"
TRAINING WITH AND WITHOUT DIRICHLET NOISE - ALL GAMES,0.6943699731903485,"0
100
200
300
400
500
600
700
800
900
1000
1100
1200"
TRAINING WITH AND WITHOUT DIRICHLET NOISE - ALL GAMES,0.6970509383378016,"0
2
4
6
8
10
12
14
16
18
20"
TRAINING WITH AND WITHOUT DIRICHLET NOISE - ALL GAMES,0.6997319034852547,"0
100
200
300
400
500
600
700
800
900
1000 1100 1200"
TRAINING WITH AND WITHOUT DIRICHLET NOISE - ALL GAMES,0.7024128686327078,"Figure 8: Number of bad decisions for models trained without Dirichlet noise minus the number of
bad decisions for models trained with Dirichlet noise - rolling average of the last 100 epochs, 10
samples, 99% confidence intervals."
DISCUSSION,0.7050938337801609,"8
Discussion
225"
DISCUSSION,0.707774798927614,"We have introduced a new exploration approach to learn more from the environment. The new
226"
DISCUSSION,0.710455764075067,"idea is to use two separate policies in a combined hybrid policy Phybrid, starting episodes with one
227"
DISCUSSION,0.7131367292225201,"for exploration Pexploring - to take the agent to situations it would otherwise not experience - and
228"
DISCUSSION,0.7158176943699732,"randomly switching to the other policy Pnormal for finishing the episode with normal training. We
229"
DISCUSSION,0.7184986595174263,"derived Pexploring from Pnormal by using a softmax temperature to introduce noise, set Pnormal to
230"
DISCUSSION,0.7211796246648794,"be the improved policy from Gumbel MuZero [7] and applied it to the game Tic-Tac-Toe. In these
231"
DISCUSSION,0.7238605898123325,"experiments, at a statistical confidence level of 99%, we observe a reduction in bad decisions by
232"
DISCUSSION,0.7265415549597856,"a factor of 435 ± 190. A selective check suggests that the reason for the wrong decisions before
233"
DISCUSSION,0.7292225201072386,"introducing the new exploration approach lies in an incorrect value expectation of the agent’s model.
234"
DISCUSSION,0.7319034852546917,"In further experiments on the Tic-Tac-Toe game at a statistical confidence level of 99%, we observed
235"
DISCUSSION,0.7345844504021448,"that training with Dirichlet noise resulted in a network with better decision ability than training
236"
DISCUSSION,0.7372654155495979,"without Dirichlet noise and that playout of the trained network without Gumbel noise showed better
237"
DISCUSSION,0.739946380697051,"decision ability than playout with Gumbel noise.
238"
DISCUSSION,0.7426273458445041,"Having found large improvement factors for Tic-Tac-Toe, we should ask ourselves: have we reached
239"
DISCUSSION,0.7453083109919572,"state-of-the-art? For all situations where we could fully unfold the decision tree in a classical manner,
240"
DISCUSSION,0.7479892761394102,"we should consider perfect decisions as state-of-the-art. Therefore we have not fully reached the
241"
DISCUSSION,0.7506702412868632,"state-of-the-art for Tic-Tac-Toe.
242"
DISCUSSION,0.7533512064343163,"It would be interesting to see how the approaches tested here for Tic-Tac-Toe pay off for Go, Chess,
243"
DISCUSSION,0.7560321715817694,"Shogi, and the Atari games on which MuZero was tested.
244"
DISCUSSION,0.7587131367292225,"What could be done to improve the approach presented here?
245"
DISCUSSION,0.7613941018766756,"Exploration Level When using the hybrid policy in the experiments, we used a fixed exploration
246"
DISCUSSION,0.7640750670241286,"temperature. In general, a higher temperature will distribute the agent’s starting points for
247"
DISCUSSION,0.7667560321715817,"normal policy actions more randomly, whereas a lower temperature would keep the starting
248"
DISCUSSION,0.7694369973190348,"points closer to the best action path the agent could take. A strategy needs to be found on
249"
DISCUSSION,0.7721179624664879,"how to best set the exploration level to improve decisions.
250"
DISCUSSION,0.774798927613941,"Entropy reward Entropy as an intrinsic reward[9] could be added as a curiosity mechanism for
251"
DISCUSSION,0.7774798927613941,"known unknowns. We speculate that this - as one aspect - would remove the need for
252"
DISCUSSION,0.7801608579088471,"Dirichlet noise. The measurements from Appendix D.7 seem to point in this direction.
253"
DISCUSSION,0.7828418230563002,"Reanalyse learning cycle The use of the Reanalyse learning cycle is a key feature in reducing the
254"
DISCUSSION,0.7855227882037533,"need for interaction with the environment. Extending the use of the techniques presented
255"
DISCUSSION,0.7882037533512064,"here to the Reanalyse learning cycle would therefore be useful. It would be of particular
256"
DISCUSSION,0.7908847184986595,"interest to Reanalyse the states that lead to rewarded actions, as the reward is a direct input
257"
DISCUSSION,0.7935656836461126,"from the environment and the source of the derived value. We speculate that this could
258"
DISCUSSION,0.7962466487935657,"improve the model’s value predictions and thus the quality of decisions. A theoretically
259"
DISCUSSION,0.7989276139410187,"sound solution to the off-policy problem would be helpful in this regard.
260"
DISCUSSION,0.8016085790884718,"Adversarial Exploration If the agent randomly deviates from the optimised strategy during explo-
261"
DISCUSSION,0.8042895442359249,"ration, it is unlikely to get into the situations the adversarial player put it into in Go [28].
262"
DISCUSSION,0.806970509383378,"Therefore, it may be necessary to devise an exploration strategy using an adversary as a
263"
DISCUSSION,0.8096514745308311,"counterpart in such games.
264"
DISCUSSION,0.8123324396782842,"We hope to provide a useful technique for better learning the value function of the model as a basis
265"
DISCUSSION,0.8150134048257373,"for better planning-based decisions by the agent. It could serve as a starting point to help the agent
266"
DISCUSSION,0.8176943699731903,"become more curious.
267"
REFERENCES,0.8203753351206434,"References
268"
REFERENCES,0.8230563002680965,"[1] Ioannis Antonoglou, Julian Schrittwieser, Sherjil Ozair, Thomas K Hubert, and David Silver. Planning in
269"
REFERENCES,0.8257372654155496,"stochastic environments with a learned model. 2021.
270"
REFERENCES,0.8284182305630027,"[2] Junjie Bai, Fang Lu, Ke Zhang, et al. Onnx: Open neural network exchange. https://github.com/
271"
REFERENCES,0.8310991957104558,"onnx/onnx, 2019.
272"
REFERENCES,0.8337801608579088,"[3] József Beck. Combinatorial games: tic-tac-toe theory, volume 114. Cambridge University Press Cam-
273"
REFERENCES,0.8364611260053619,"bridge, 2008.
274"
REFERENCES,0.839142091152815,"[4] Tyson R Browning and Ranga V Ramasesh. Reducing unwelcome surprises in project management. MIT
275"
REFERENCES,0.8418230563002681,"Sloan Management Review, 56(3):53–62, 2015.
276"
REFERENCES,0.8445040214477212,"[5] Johannes Czech, Patrick Korus, and Kristian Kersting. Improving alphazero using monte-carlo graph
277"
REFERENCES,0.8471849865951743,"search. In Proceedings of the International Conference on Automated Planning and Scheduling, volume 31,
278"
REFERENCES,0.8498659517426274,"pages 103–111, 2021.
279"
REFERENCES,0.8525469168900804,"[6] Ivo Danihelka. Planning and Policy Improvement. PhD thesis, UCL (University College London), 2023.
280"
REFERENCES,0.8552278820375335,"[7] Ivo Danihelka, Arthur Guez, Julian Schrittwieser, and David Silver. Policy improvement by planning with
281"
REFERENCES,0.8579088471849866,"gumbel. In International Conference on Learning Representations, 2021.
282"
REFERENCES,0.8605898123324397,"[8] Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes, Mohammadamin
283"
REFERENCES,0.8632707774798928,"Barekatain, Alexander Novikov, Francisco J R Ruiz, Julian Schrittwieser, Grzegorz Swirszcz, et al.
284"
REFERENCES,0.8659517426273459,"Discovering faster matrix multiplication algorithms with reinforcement learning. Nature, 610(7930):47–53,
285"
REFERENCES,0.868632707774799,"2022.
286"
REFERENCES,0.871313672922252,"[9] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum
287"
REFERENCES,0.8739946380697051,"entropy deep reinforcement learning with a stochastic actor. In International conference on machine
288"
REFERENCES,0.8766756032171582,"learning, pages 1861–1870. PMLR, 2018.
289"
REFERENCES,0.8793565683646113,"[10] Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Mohammadamin Barekatain, Simon Schmitt,
290"
REFERENCES,0.8820375335120644,"and David Silver. Learning and planning in complex action spaces. pages 4476–4486, 2021.
291"
REFERENCES,0.8847184986595175,"[11] Daniel Kahneman. Thinking, fast and slow. Macmillan, 2011.
292"
REFERENCES,0.8873994638069705,"[12] Joseph Luft and Harry Ingham. The johari window. Human relations training news, 5(1):6–7, 1961.
293"
REFERENCES,0.8900804289544236,"[13] Amol Mandhane, Anton Zhernov, Maribeth Rauh, Chenjie Gu, Miaosen Wang, Flora Xue, Wendy Shang,
294"
REFERENCES,0.8927613941018767,"Derek Pang, Rene Claus, Ching-Han Chiang, et al. Muzero with self-competition for rate control in vp9
295"
REFERENCES,0.8954423592493298,"video compression. arXiv preprint arXiv:2202.06626, 2022.
296"
REFERENCES,0.8981233243967829,"[14] Pascutto, Gian-Carlo and Linscott, Gary. Leela chess zero. URL http://lczero.org/.
297"
REFERENCES,0.900804289544236,"[15] Mary Phuong and Marcus Hutter. Formal algorithms for transformers. arXiv preprint arXiv:2207.09238,
298"
REFERENCES,0.903485254691689,"2022.
299"
REFERENCES,0.9061662198391421,"[16] Lutz Roeder. Netron, Visualizer for neural network, deep learning, and machine learning models, 12 2017.
300"
REFERENCES,0.9088471849865952,"URL https://github.com/lutzroeder/netron.
301"
REFERENCES,0.9115281501340483,"[17] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt,
302"
REFERENCES,0.9142091152815014,"Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi
303"
REFERENCES,0.9168900804289544,"by planning with a learned model. Nature, 588(7839):604–609, 2020.
304"
REFERENCES,0.9195710455764075,"[18] Julian Schrittwieser, Thomas Hubert, Amol Mandhane, Mohammadamin Barekatain, Ioannis Antonoglou,
305"
REFERENCES,0.9222520107238605,"and David Silver. Online and offline reinforcement learning by planning with a learned model. Advances
306"
REFERENCES,0.9249329758713136,"in Neural Information Processing Systems, 34, 2021.
307"
REFERENCES,0.9276139410187667,"[19] Max Schwarzer, Ankesh Anand, Rishab Goel, R Devon Hjelm, Aaron Courville, and Philip Bachman.
308"
REFERENCES,0.9302949061662198,"Data-efficient reinforcement learning with self-predictive representations. arXiv preprint arXiv:2007.05929,
309"
REFERENCES,0.9329758713136729,"2020.
310"
REFERENCES,0.935656836461126,"[20] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian
311"
REFERENCES,0.938337801608579,"Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go
312"
REFERENCES,0.9410187667560321,"with deep neural networks and tree search. nature, 529(7587):484–489, 2016.
313"
REFERENCES,0.9436997319034852,"[21] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
314"
REFERENCES,0.9463806970509383,"Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human
315"
REFERENCES,0.9490616621983914,"knowledge. nature, 550(7676):354–359, 2017.
316"
REFERENCES,0.9517426273458445,"[22] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,
317"
REFERENCES,0.9544235924932976,"Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement learning
318"
REFERENCES,0.9571045576407506,"algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):1140–1144, 2018.
319"
REFERENCES,0.9597855227882037,"[23] David Silver, Satinder Singh, Doina Precup, and Richard S Sutton. Reward is enough. Artificial Intelligence,
320"
REFERENCES,0.9624664879356568,"299:103535, 2021.
321"
REFERENCES,0.9651474530831099,"[24] Student. The probable error of a mean. Biometrika, 6(1):1–25, 1908.
322"
REFERENCES,0.967828418230563,"[25] Richard S Sutton. The quest for a common model of the intelligent decision maker. arXiv preprint
323"
REFERENCES,0.9705093833780161,"arXiv:2202.13252, 2022.
324"
REFERENCES,0.9731903485254692,"[26] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
325"
REFERENCES,0.9758713136729222,"[27] Alexandre Trudeau and Michael Bowling. Targeted search control in alphazero for effective policy
326"
REFERENCES,0.9785522788203753,"improvement. arXiv preprint arXiv:2302.12359, 2023.
327"
REFERENCES,0.9812332439678284,"[28] Tony Tong Wang, Adam Gleave, Nora Belrose, Tom Tseng, Joseph Miller, Michael D Dennis, Yawen
328"
REFERENCES,0.9839142091152815,"Duan, Viktor Pogrebniak, Sergey Levine, and Stuart Russell. Adversarial policies beat professional-level
329"
REFERENCES,0.9865951742627346,"go ais. arXiv preprint arXiv:2211.00241, 2022.
330"
REFERENCES,0.9892761394101877,"[29] David J Wu. Katago. URL https://github.com/lightvector/KataGo/.
331"
REFERENCES,0.9919571045576407,"[30] David J Wu. Accelerating self-play learning in go. arXiv preprint arXiv:1902.10565, 2019.
332"
REFERENCES,0.9946380697050938,"[31] Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, and Yang Gao. Mastering atari games with
333"
REFERENCES,0.9973190348525469,"limited data. Advances in Neural Information Processing Systems, 34:25476–25488, 2021.
334"
