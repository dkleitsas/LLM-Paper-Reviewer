Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0029850746268656717,"Deep learning has seen a movement away from representing examples with a
monolithic hidden state towards a richly structured state. For example, Trans-
formers segment by position, and object-centric architectures decompose images
into entities. In all these architectures, interactions between different elements
are modeled via pairwise interactions: Transformers make use of self-attention
to incorporate information from other positions and object-centric architectures
make use of graph neural networks to model interactions among entities. We
consider how to improve on pairwise interactions in terms of global coordina-
tion and a coherent, integrated representation that can be used for downstream
tasks. In cognitive science, a global workspace architecture has been proposed in
which functionally specialized components share information through a common,
bandwidth-limited communication channel. We explore the use of such a communi-
cation channel in the context of deep learning for modeling the structure of complex
environments. The proposed method includes a shared workspace through which
communication among different specialist modules takes place but due to limits
on the communication bandwidth, specialist modules must compete for access.
We show that capacity limitations have a rational basis in that (1) they encourage
specialization and compositionality and (2) they facilitate the synchronization of
otherwise independent specialists."
INTRODUCTION,0.005970149253731343,"1
INTRODUCTION"
INTRODUCTION,0.008955223880597015,"1. Parallel, competing specialists
2. Write to shared workspace
3. Broadcast workspace contents"
INTRODUCTION,0.011940298507462687,"Figure 1: Step 1: an ensemble of specialist mod-
ules doing their own default processing; at a par-
ticular computational stage, depending upon the
input, a subset of the specialists becomes active.
Step 2: the active specialists get to write informa-
tion in a shared global workspace. Step 3: the
contents of the workspace are broadcast to all spe-
cialists."
INTRODUCTION,0.014925373134328358,"Deep Learning has seen a movement towards
more structured models with cleaner separation
between different pieces of information often
handled by different components. The induced
structure, and separation of knowledge has im-
proved generalization, model-size scaling, and
long-range dependencies (Berner et al., 2019;
Vinyals et al., 2019; Brown et al., 2020). This
opens up questions about how to achieve coher-
ence and coordination between different com-
ponents in such architectures. Looking back to
the 1980s, the focus in AI was much less on
learning and more on constructing articulated,
multi-component architectures and examining
how intelligence might emerge from interactions
among this collection of simple, functionally
specialized components (Fodor, 1983; Braiten-
berg, 1986; Minsky, 1988; Brooks, 1991). Each
of these specialist modules is on the scale of a typical component of a computer program, like a
subroutine that implements a narrow, prespeciﬁed function from certain input contents to certain"
INTRODUCTION,0.01791044776119403,"1 Mila, University of Montreal, 2 Google Deepmind, 3 Max Planck Institute Germany, 4 Google Research,
Brain Team, 5 Microsoft Research, New York, NY, 6 Indian Institute of Technology, Delhi, Corresponding
authors: anirudhgoyal9119@gmail.com"
INTRODUCTION,0.020895522388059702,Published as a conference paper at ICLR 2022.
INTRODUCTION,0.023880597014925373,"output contents. Through appropriate communication and coordination, a set of specialists can
achieve complex, dynamic, and ﬂexible behavior patterns."
INTRODUCTION,0.026865671641791045,"As a concrete illustration, consider the task of driving a car in terms of specialists. One specialist
might monitor the position of the car with respect to lines on the road, and another specialist might
adjust the steering direction based on the perceptual data. In addition, there might be specialists
which provide alerts when certain events occur, such as loud sounds, reaching a critical intersection
on a route, or coming into close proximity to the car in front. To execute the task of driving the car
properly, all these specialists need to interact coherently and broadcast their individual information to
each other."
INTRODUCTION,0.029850746268656716,"Arguably, modern ML and AI has yet to develop broad architectural frameworks for learning both
the specialist modules and how they should interact, while the classical view lacks an articulate
story about how learning could take place successfully in such frameworks. In this article, we
revisit this classical view with modern machine learning tools based on end-to-end learning and
differentiable memory and attention mechanisms. Inspired by the Global Workspace Theory (Baars,
1993; Dehaene et al., 1998; Shanahan and Baars, 2005; Shanahan, 2006; 2010; 2012; Dehaene et al.,
2017) from cognitive neuroscience, we argue that more ﬂexibility and generalization emerge through
an architecture of specialists if their training encourages them to communicate effectively with one
another via the bottleneck of a shared workspace (Figure. 1)."
INTRODUCTION,0.03283582089552239,"Distributed specialist modules. From a computational perspective, articulated multi-component
architectures composed of sparsely interacting specialist modules show desirable scaling properties
(e.g., more specialists can seamlessly be added), increased robustness (the system can tolerate the
removal of or changes in individual specialists), and efﬁciency (information is processed predomi-
nantly locally, reducing the cost of communication between specialists). However, modularization
also requires mechanisms to establish sharing of compatible representations across specialists, a form
of shared internal language. While portions of a task might be solved by independent specialists,
synchronization is critical particularly when there are statistical, functional, or causal dependencies
among the specialists."
INTRODUCTION,0.03582089552238806,"Coherence through a shared workspace. In cognitive neuroscience, the Global Workspace Theory
(GWT) (Baars, 1993; Dehaene et al., 2017) suggests an architecture allowing specialist modules to
interact. The key claim of GWT is the existence of a shared representation—sometimes called a
blackboard, sometimes a workspace—that can be modiﬁed by any specialist and that is broadcast
to all specialists, along with the notion that write access is limited to maintain coherence. Our
interpretation of this restriction on write access is that it stems from an assumption on the form of
the joint distribution between high-level concepts. In this paper, we explore a communication and
coordination scheme similar to the one proposed by GWT for modern neural network architectures
like Transformers (Vaswani et al., 2017; Dehghani et al., 2018; Parmar et al., 2018; Radford et al.,
2019; Brown et al., 2020) and attention-based modular architectures (Goyal et al., 2019; Rahaman
et al., 2020; Mittal et al., 2020a; Goyal et al., 2020; Madan et al., 2021)."
INTRODUCTION,0.03880597014925373,"In terms of our driving example, the workspace could be used to override default behaviors by giving
high priority to specialist modules which provide alerts of various sorts (loud sounds, presence of a
child on the street), allowing specialists which respond to such alerts to take control of behavior over
default driving routines. This scenario implies that prioritization of signals in a shared workspace is
critical."
INTRODUCTION,0.041791044776119404,"A shared communication channel necessitates common representations. For a multitude of
specialist modules to cooperate, a common language is necessary (Baars, 1997). For example, in
the driving scenario, alerts may come from auditory or visual processing specialists, but regardless
of the source, a signal for danger must be placed in the workspace to override default behavior,
whether that behavior is controlled by a radio-tuning specialist or a steering specialist. Although
specialist modules can be pre-wired to have compatible communication interfaces, we will model
an architecture in which an ensemble of specialist modules is trained in coordination, which should
lead to a shared language (Colagrosso and Mozer, 2005). Internally, individual specialists can use
whatever form of representations that serves them, but their inputs and outputs require alignment
with other specialists in order to synchronize. For example, an unusual event such as a rough thud
under the wheels might not have been previously experienced, but the mere signalling of novelty"
INTRODUCTION,0.04477611940298507,"In the literature, specialists are sometimes referred to as processes or agents."
INTRODUCTION,0.04776119402985075,Published as a conference paper at ICLR 2022.
INTRODUCTION,0.050746268656716415,Computation
INTRODUCTION,0.05373134328358209,a) RIMs
INTRODUCTION,0.056716417910447764,Feed Forward Layer
INTRODUCTION,0.05970149253731343,Computation
INTRODUCTION,0.0626865671641791,b) Transformer
INTRODUCTION,0.06567164179104477,Computation
INTRODUCTION,0.06865671641791045,c) TIMs
INTRODUCTION,0.07164179104477612,Computation
INTRODUCTION,0.07462686567164178,d) Universal Transformer
INTRODUCTION,0.07761194029850746,Computation
INTRODUCTION,0.08059701492537313,"Shared Workspace } } 1
2"
INTRODUCTION,0.08358208955223881,a) RIMs + SW
INTRODUCTION,0.08656716417910448,Feed Forward Layer
INTRODUCTION,0.08955223880597014,Computation
INTRODUCTION,0.09253731343283582,"Shared Workspace }
} 1 2"
INTRODUCTION,0.0955223880597015,b) Transformer + SW
INTRODUCTION,0.09850746268656717,Computation
INTRODUCTION,0.10149253731343283,"Shared Workspace }
} 1 2"
INTRODUCTION,0.1044776119402985,c) TIMs + SW
INTRODUCTION,0.10746268656716418,Computation
INTRODUCTION,0.11044776119402985,"Shared Workspace }
} 1 2"
INTRODUCTION,0.11343283582089553,d) Universal Transformer + SW
INTRODUCTION,0.11641791044776119,Position-wise FFN
INTRODUCTION,0.11940298507462686,Feed forward layer
INTRODUCTION,0.12238805970149254,"Shared workspace
Updated state of 
specialist (t+1)
Initial state of 
specialist (t)
Activated specialist"
INTRODUCTION,0.1253731343283582,TIMs mechanism
INTRODUCTION,0.12835820895522387,"Inter-module 
communication
Pairwise 
communication } 1 } 2"
INTRODUCTION,0.13134328358208955,Write step
INTRODUCTION,0.13432835820895522,Broadcast step
INTRODUCTION,0.1373134328358209,"Figure 2: Using a Shared Workspace for creating global coherence in RIMs, Transformers,
TIMs and Universal Transformers (UT). (Top Half) All four of these architectures use pairwise
communication (using key-value attention) to establish coherence between individual specialist
modules. In the case of RIMs (Goyal et al., 2019) and TIMs (Lamb et al., 2021), these specialists are
independent modules that compete with each other in order to take control over the state update based
on a given input. In the case of Transformers (Vaswani et al., 2017) and Universal Transformers
(Dehghani et al., 2018), each specialist is associated with a different position. Activated specialists
are denoted by a blue shade and the intensity depends on the degree of activation. In the case of
Universal Transformers, the state update dynamics for each position is shared across all layers and all
positions (denoted by a yellow triangle). (Bottom Half) We replace pairwise communication with a
shared workspace to create global coherence between different specialists. Communication using
the shared workspace is a two-step process (as denoted by 1 and 2 in the ﬁgures). In the ﬁrst step
(1), specialists compete for write access to the shared workspace, resulting in a subset of them being
activated (in blue), and only the activated specialists perform the write operation on the workspace.
In the second step (2), the contents of the shared workspace are broadcast to all the specialists."
INTRODUCTION,0.14029850746268657,"could override default specialists. Without a global communication channel, specialists would have
to learn to communicate through pairwise interactions, which might limit coordination of behavior in
novel situations: global communication ensures exchangeability of knowledge to achieve systematic
generalization."
SYNCHRONIZING NEURAL MODULES THROUGH A SHARED WORKSPACE,0.14328358208955225,"2
SYNCHRONIZING NEURAL MODULES THROUGH A SHARED WORKSPACE"
SYNCHRONIZING NEURAL MODULES THROUGH A SHARED WORKSPACE,0.14626865671641792,"We investigate a neural architecture reminiscent of the GW model, where a number of sparsely
communicating specialist modules interact via a shared working memory. In particular, we extend
the Transformer (Vaswani et al., 2017), attention and slot-based modular architectures (Goyal et al.,
2019) by adding a shared workspace and allowing modules (each representing an entity) to compete
for write access in each computational stage."
SYNCHRONIZING NEURAL MODULES THROUGH A SHARED WORKSPACE,0.14925373134328357,"Key-value attention. Key-value attention deﬁnes the backbone of updates to the hidden states in the
proposed model. This form of attention is widely used in self-attention models and performs well on
a wide array of tasks (Bahdanau et al., 2014; Vaswani et al., 2017; Santoro et al., 2018). Key-value
attention selects an input value based on the match of a query vector to a key vector associated with
each value. To allow differentiability and thus easier learnability, selection is soft and computes
a convex combination of all the values. Such a mechanism makes it possible to change on-the-ﬂy
both the source of input and how the shared workspace is updated. It also makes the outputs of the
specialists and the elements of the memory permutation invariant: they should be considered as an
unordered set of elements to be selected by an attention mechanism from the contents of specialists.
More precisely, soft attention uses the product of a query (represented as a matrix Q of dimensionality
Nr × d, with Nr queries, and d the dimension of each query) with a set of No objects each associated
with a key as a row in matrix KT (No × d). After normalization with a softmax the resulting convex
weights are used to combine the values Vi (row i of matrix V ): where the softmax is applied to each
row of its argument matrix, yielding a set of convex weights. For our experiments, we use multihead
dot product attention."
SYNCHRONIZING NEURAL MODULES THROUGH A SHARED WORKSPACE,0.15223880597014924,"Neural modules with pairwise interactions.
Our approach to synchronizing neural modules is
highly general and mostly agnostic to the task, domain, or speciﬁc choice of architecture, with the"
SYNCHRONIZING NEURAL MODULES THROUGH A SHARED WORKSPACE,0.15522388059701492,Published as a conference paper at ICLR 2022.
SYNCHRONIZING NEURAL MODULES THROUGH A SHARED WORKSPACE,0.1582089552238806,"only requirement being that the model consists of multiple specialist modules which either operate
independently or have sparse interactions requiring to only match pairs of modules at a time. Our
goal is to explore how introducing a shared workspace can help these modules to become better
synchronized and coordinated. We show the utility of the shared workspace for synchronization in (a)
Transformers (Vaswani et al., 2017), in which all interactions between positions are performed via
attention, and (b) slot-based architectures like Recurrent Independent Mechanisms or RIMs (Goyal
et al., 2019) in which all pairwise interactions between modules are performed via attention. In the
context of slot-based architectures, each slot’s content is associated with a specialist module, whereas
in Transformers different entities each associated with a different position acts as a specialist module
(Figure 2)."
SYNCHRONIZING NEURAL MODULES THROUGH A SHARED WORKSPACE,0.16119402985074627,"Both Transformers and RIMs utilize a self-attention mechanism for sharing information between
modules, typically implemented in a pairwise manner, i.e., each specialist attends to every other
specialist. Instead, we facilitate information sharing among specialist modules through a limited
capacity shared workspace. In this framework at each computational stage, different specialists
compete for write access to the common workspace. The contents of the workspace, in turn, are
broadcast to all specialist modules simultaneously.
Notation.
The input is processed through a sequence of computational stages indexed by t, and
at each stage, ns entities are operated on (i.e., ns different modules in slot-based architectures like
RIMs or ns different positions in the case of Transformers). Each of these ns specialist modules has
a distinct internal nh-dimensional state hk
t , for k ∈{1, ..., ns}. The specialist modules communicate
with each other via a shared workspace divided into nm memory slots, each consisting of a vector
of nl elements, denoted M = [m1; . . . mj; . . . mnm]. The shared workspace is updated across
different computational stages i.e., different time-steps in recurrent architecture and different layers
in the case of Transformers. At each computational stage t, different specialists compete for writing
in the shared workspace, but all specialists can read from the current state of the workspace. In the
case of an autoregressive task, we can restrict the information sharing to previous positions and keep
a separate version of the workspace for each position."
SYNCHRONIZING NEURAL MODULES THROUGH A SHARED WORKSPACE,0.16417910447761194,"2.1
SPECIFICS OF THE SHARED WORKSPACE."
SYNCHRONIZING NEURAL MODULES THROUGH A SHARED WORKSPACE,0.16716417910447762,"Step 1: Process Input to obtain an entity representation for each specialist. The ﬁrst step is external
to the proposed method, and involves processing the input to form the initial representation vector for
each of the different specialists. Different common deep learning architectures can be used to form the
representation of different specialists. For example, Transformers start with a matrix ns × nh whose
rows are initialized as the nh-dimensional embeddings of the input at each position of the sequence.
Slot-Based Recurrent architectures like RIMs consist of a single-layer recurrent structure where the
hidden state ht at computational stage t is decomposed into the substates of the ns specialists, hk
t for
k = 1, ...ns."
SYNCHRONIZING NEURAL MODULES THROUGH A SHARED WORKSPACE,0.1701492537313433,"In the proposed scheme, within each computational stage, the updates of the hidden state of different
specialists follow a two-step process. First, specialists compete and write to a shared workspace.
Second, information from the workspace gets broadcast to all the specialists, as detailed next."
SYNCHRONIZING NEURAL MODULES THROUGH A SHARED WORKSPACE,0.17313432835820897,"Step 2: Writing Information in the shared workspace. The specialists compete to write into the
shared workspace, whose contents need to be updated in the context of new information received
from different specialists. This step ensures that only the critically important signals make it to the
shared workspace, therefore preventing the workspace from being cluttered. Let matrix R represent
the combined state of all the specialists (i.e. hk
t
∀k ∈{1, . . . , ns} as the rows of R). In order to
implement the competition between specialists to write into the workspace, we use a key-query-value
attention mechanism. In this case, the query is a function of the state of the current workspace memory
content, represented by matrix M (with one row per slot of the memory), i.e e
Q = M f
W q. Keys
and values are a function of the information from the specialists i.e., a function of R. We apply dot
product attention to get the updated memory matrix: M ←softmax
 e
Q(Rf
W e)T
√de"
SYNCHRONIZING NEURAL MODULES THROUGH A SHARED WORKSPACE,0.1761194029850746,"
Rf
W v. The use
of a regular softmax to write into M leads to a standard soft competition among different specialists
to write in the shared workspace. One can also use a top-k softmax (Ke et al., 2018) to select a ﬁxed
number of specialists allowed to write in the shared workspace: based on the pre-softmax values, a
ﬁxed number of k specialists which have the highest values are selected, and get access to write in the
shared workspace. Selection with a top-k softmax is a hybrid between hard and soft selection. We"
SYNCHRONIZING NEURAL MODULES THROUGH A SHARED WORKSPACE,0.1791044776119403,Published as a conference paper at ICLR 2022.
SYNCHRONIZING NEURAL MODULES THROUGH A SHARED WORKSPACE,0.18208955223880596,"denote the set of thus selected specialists as Ft. We note that we can apply the attention mechanism
multiple times to distill information from different specialists into the shared workspace. Here, the
contents of the shared workspace are updated in the gated way as proposed in RMC (Santoro et al.,
2018). We ask the reader to refer to appendix section C for more details."
SYNCHRONIZING NEURAL MODULES THROUGH A SHARED WORKSPACE,0.18507462686567164,"Step 3: Broadcast of information from the shared workspace. Each specialist then updates its state
using the information broadcast from the shared workspace. We again utilize an attention mechanism
to perform this consolidation. All the specialists create queries bqk = hk
t c
W q, which are matched
with the keys bκj = (mj c
W e)T
∀k ∈{1, . . . , ns}, j ∈{1, . . . , nm} from the updated memory"
SYNCHRONIZING NEURAL MODULES THROUGH A SHARED WORKSPACE,0.1880597014925373,"slots, forming attention weights sk,j = softmax

bqkbκj
√de"
SYNCHRONIZING NEURAL MODULES THROUGH A SHARED WORKSPACE,0.191044776119403,"
. The memory slot values generated by
each slot of the shared workspace and the attention weights are then used to update the state of all
the specialists: hk
t ←hk
t + P"
SYNCHRONIZING NEURAL MODULES THROUGH A SHARED WORKSPACE,0.19402985074626866,"j sk,jbvj where bvj = mj c
W v
∀k ∈{1, . . . , ns}. After receiving
the broadcast information from the workspace, each specialist update their state by applying some
dynamics function i.e., one step update of LSTM or GRU units in the case of recurrent architectures,
and a feedforward layer in the case of Transformers. This yields the new value hk
t+1 for the k-th
specialist, from which we start the next stage (t + 1)."
SYNCHRONIZING NEURAL MODULES THROUGH A SHARED WORKSPACE,0.19701492537313434,"Replacing pairwise interactions among neural modules with interaction facilitated by the shared
workspace allows for the following:"
SYNCHRONIZING NEURAL MODULES THROUGH A SHARED WORKSPACE,0.2,"1. Higher-order (HO) interaction among neural modules. The two-step write-read process ﬁrst
allows each memory slot to store a ‘ﬁltered summary’ of the current input where the ‘ﬁlter’ is
determined by the previous state of that slot (‘Query’ for the write step). Neural modules then
summarize the information contained in these slots and update their state. Hence unlike pairwise
interaction, messages passed among neural modules in the shared workspace setting also include
HO interaction terms; those consisting of more than 2 modules at a time. Naturally, HO interaction
require that messages passed among neural modules lie in the same representation space, which is
precisely what we aim to achieve by allowing message passing only via a singular global channel."
SYNCHRONIZING NEURAL MODULES THROUGH A SHARED WORKSPACE,0.20298507462686566,"2. Dynamic ﬁltering due to persistence of memory. With a shared workspace (SW), contents of the
memory slot play a key role in ﬁltering and summarizing the information contained in the input at a
given time step. Persistence of memory throughout an episode 1) would allow the memory layer to
summarize and ﬁlter information based on what it has seen thus far 2) should ideally lead to better
generalization as the model is able to dynamically modify its ﬁltering machinery for a particular input.
In contrast, “inducing points” in Set Transformers (Lee et al., 2019) are ﬁxed after training and hence
the bottleneck cannot adjust itself on the ﬂy for any new input. We present comparisons on several
tasks in section 4. They show the importance of these two properties by comparing performance
of SW with a) 2×Self-Attention (to simulate HO interaction without global communication) b) a
version without memory persistence, in Appendix D."
SYNCHRONIZING NEURAL MODULES THROUGH A SHARED WORKSPACE,0.20597014925373133,"Computational Complexity of using shared workspace for synchronizing different specialists. To
encourage a coherent global coordination, Transformers and slot-based recurrent architectures rely on
pairwise interactions captured via an attention mechanism. Unfortunately, such attention mechanisms
scale quadratically with the number of specialists. Here, we propose a method which uses a shared
workspace to create global coherence between different specialists and in the process, replaces the
pairwise interactions of conventional dot-product attention. The computational complexity of the
proposed method is thus linear in the number of specialists. In our experimentation, the number of
memory slots is practically constant, which suggests a very favourable scaling behavior, and certainly
much less than quadratic. As a point of reference, what would correspond to the number of slots in
human working memory (Baars, 1993) is indeed very small (less than 10 slots)."
RELATED WORK,0.208955223880597,"3
RELATED WORK"
RELATED WORK,0.21194029850746268,"This work taps into a line of reasoning put forward by historical works, such as Minsky (1988);
Braitenberg (1986); Fodor (1983), wherein it is argued that in order to be able to deal with a wide
spectrum of conditions and tasks, an intelligent system should be comprised of many interacting
specialized modules or programs, rather than a single “one-size-ﬁts-all” entity. While modular
architectures have been the subject of a number of research directions, (Jacobs et al., 1991; Bottou
and Gallinari, 1991; Ronco et al., 1997; Reed and De Freitas, 2015; Andreas et al., 2016; Rosenbaum
et al., 2017; Fernando et al., 2017; Shazeer et al., 2017; Rosenbaum et al., 2019; Goyal and Bengio,"
RELATED WORK,0.21492537313432836,Published as a conference paper at ICLR 2022.
RELATED WORK,0.21791044776119403,"2020), we focus here on a mechanism for achieving coherence and synchronization between specialist
modules via a global workspace shared between all specialists."
RELATED WORK,0.2208955223880597,"Prior works have explored incorporating slot-based memory in the context of recurrent neural
networks (Graves et al., 2014; 2016; Santoro et al., 2018). In the context of transformers, Burtsev and
Sapunov (2020) introduce memory tokens that are processed in addition to sequence tokens, whereas
Dai et al. (2019) (Transformer-XL) propose to partition a long sequence to smaller segments and use
the activations of the previous segment in memory while processing the current segment. Building on
the latter, Rae et al. (2019) propose to store activations from prior segments in a compressed memory.
However, these methods do not restrict memory writes to be sparse and competitive. Recent advances
in this direction include the global neuronal workspace (GNW) model (Dehaene and Changeux,
2011), which identiﬁes the global workspace with a large network of excitatory pyramidal neurons
with long-range axonal processes connecting prefrontal and parietal cortices. Further, deploying a
shared workspace to establish coherence between different specialists as opposed to using all-pair
communication has an added beneﬁt, in that it allows us to tackle the O(n2) complexity of self-
attention. This makes our work related to previous work on reducing the computational complexity
of dot product attention in Transformers. Lee et al. (2019) introduce the ISAB module, which maps
between sets and comprises two dot-product attention layers. In the ﬁrst layer, a set of trainable
parameters are used as queries and the elements of the input set as keys; in the second layer, the
output of the ﬁrst layer is used as keys and the input set as queries. However, unlike in this work, the
intermediate states (corresponding to the output of the ﬁrst layer) are not maintained across layers.
Concurrent to our work, (Jaegle et al., 2021) also introduced the idea of using a latent bottleneck
for addressing quadratic complexity by learning a bottleneck but there are important differences.
For example. in Perceiver the latent bottleneck iteratively queries the information about different
positions, and does not maintain the representation of the different specialists. More precisely, in our
proposed method different specialists write information in the workspace and then information gets
read from the shared workspace. In Perceiver, the latent bottleneck iteratively reads information from
the set of positions. We also show the applicability of the proposed idea both for slot based models
and Transformers."
RELATED WORK,0.22388059701492538,"The proposed model can also be seen as integrating out different ideas popular in modular architectures
(Andreas et al., 2016; Goyal et al., 2019), memory networks (Graves et al., 2014; Santoro et al., 2018)
and mixture of experts (Jacobs et al., 1991), and hence combining some of their beneﬁts in a uniﬁed
architecture. The proposed model is factored as a set of specialists (incorporating modularity). The
proposed model achieves coordination among different specialists via the use of a shared workspace
(in the Neural Turing machines, there is only a single specialist i.e., without any modularity). Multiple
experts can be active at the same time (generally not the case with a mixture of experts)."
EXPERIMENTS,0.22686567164179106,"4
EXPERIMENTS"
EXPERIMENTS,0.2298507462686567,"Here we brieﬂy outline the tasks on which we applied the idea of the shared workspace and direct
the reader to the appendix for some more experiments (Appendix G), full details on each task and
details on hyperparameter settings for the model. The experiments have the following goals: (a)
Demonstrate that the use of the shared workspace can improve results on a wide array of challenging
benchmark tasks, with the goal of demonstrating the practical utility and breadth of the technique.
(b) Show that the shared workspace addresses coherence between different specialists by achieving
improved performance without requiring all pairwise interactions. Finally, to show wide applicability
of our model, we integrate SW in TIMs (Lamb et al., 2021), SCOFF (Goyal et al., 2020) and BRIMs
(Mittal et al., 2020b) and show improvements over the default communication method used in each."
EXPERIMENTS,0.23283582089552238,"Making sense of the visual input.
Using a shared workspace introduces a bottleneck in sharing
of information between specialists. Since the size of the workspace is limited and generally much
lower than the number of specialists, there is a limit to the amount of information that can be ex-
changed among specialists. We hypothesize that mediating communication through a limited capacity
workspace should encourage the model to look at relevant information that is important for the
downstream objective. We test this hypothesis on a set of visually challenging benchmarks. For our
experiments, we use either Transformers or RIMs as a backbone. We consider variants of Trans-
formers based on different subsets of important properties. Transformers [TR]: Self-attention based
multi-layer architecture (Vaswani et al., 2017) with shared parameters across layers. Set transformer
[ISAB]: Transformers where self attention is replaced by ISAB module (Lee et al., 2019). Sparse"
EXPERIMENTS,0.23582089552238805,Published as a conference paper at ICLR 2022.
EXPERIMENTS,0.23880597014925373,"Figure 3:
Detecting Equilateral Triangles.
Here, we compare the performance of the Trans-
formers with shared workspace to other Trans-
former baselines. Here, we plot the test accuracy
for each model."
EXPERIMENTS,0.2417910447761194,"Model
Top-1 %
Top-5 %
ISAB
65.3±0.025
83.6±0.011
STR
70.6±0.08
87.33±0.06
TR
70.83±0.44
87.8±0.08
TR + HC
70.17±0.31
88.33±0.2
TR + HSW (OURS)
71.07±0.04
88.6±0.49
TR + SSW (OURS)
71.33±0.34
88.3±0.05"
EXPERIMENTS,0.24477611940298508,"Table 1:
Comparison on CATER Object
Tracking. Here, we compare the Top-1 and
Top-5 accuracy of Transformers with shared
workspace and Transformers with self-attention.
We can see that Transformers with a shared
workspace outperform those with pairwise self-
attention.
Transformers [STR]: Transformers with sparse factorizations of the attention matrix (Child et al.,
2019). High Capacity Transformers [TR+HC]: Same as TR but with different parameters across
layers. Transformers with Shared Workspace with soft-competition [TR+SSW]: Transformers with
different positions competing with each other to write in shared workspace using soft-competition.
Transformers with Shared Workspace with top-k competition [TR+HSW]: Transformers with differ-
ent positions competing with each other to write in shared workspace using top-k competition. For a
more detailed description of all the tasks described below, we ask the reader to appendix section E."
EXPERIMENTS,0.24776119402985075,"Figure 4: Comparison on Sort-of-CLEVR rela-
tional reasoning. Speed of convergence for rela-
tional and non-relational questions in the sort-of-
clevr dataset. We can see that the proposed model
converges much faster than the baselines in both
cases."
EXPERIMENTS,0.2507462686567164,"Detecting Equilateral Triangles. We ﬁrst use
a simple toy task to test our hypothesis where
the model should detect equilateral triangles in
images (Ahmad and Omohundro, 2009). Each
image is of size 64 × 64 and contains 3 ran-
domly placed clusters of points. For equilat-
eral triangles, the midpoints of these clusters are
equidistant from each other. This is a binary
classiﬁcation task where the model has to pre-
dict whether the three given clusters form an
equilateral triangle or not. To feed an image into
a Transformer, we follow the same methodol-
ogy as used in vision Transformers (Dosovitskiy
et al., 2020). We ﬁrst divide an image into equal
sized 4 × 4 patches and treat each patch as a
different input position of the Transformer."
EXPERIMENTS,0.2537313432835821,"To solve this task correctly, the model only needs
to attend to relevant information i.e., to patches
that contain the cluster of points. Therefore, us-
ing a limited capacity shared workspace should
be useful here. Our results (presented in Figure
3) conﬁrm this hypothesis. We can see that Transformers with shared workspace attention converge
much faster and reach higher accuracy as compared to the baseline Transformer. Our method also
outperforms Set Transformer by a signiﬁcant margin."
EXPERIMENTS,0.25671641791044775,"Multi MNIST Generation. In this task, we train an Image Transformer (Parmar et al., 2018) (pixel-
by-pixel, raster-order generative model) for next-pixel prediction on the “MultiMNIST dataset” where
each image consists of 4 independently sampled MNIST digits stacked horizontally to form one
image (see Figure 10 for demonstration). The main aim of this task is to observe the inductive
biases that allow for specialization of mechanisms in TIMs (Lamb et al., 2021). Each image in
the MultiMNIST dataset can be broken down into different sets of independent spatial components.
Since the digits which make up the image are independently selected, the joint distribution of pixel
intensities in any one of the four sections of the image is statistically independent of the pixel
intensities in any other section of the image. Moreover each section of the image can be further
broken down into independent spatial components: one that pertains to the background and one that
pertains to the foreground. One can expect that architectures that are made up of sparsely interacting"
EXPERIMENTS,0.25970149253731345,Published as a conference paper at ICLR 2022.
EXPERIMENTS,0.2626865671641791,"different mechanisms to naturally capture this statistical independence by dividing labour among
different mechanisms. While, for monolithic architectures, a major portion of their training time
will be spent in learning these statistical independencies from scratch. We ﬁnd that replacing the
pairwise communication in TIMs with a shared workspace (TIMs + SW) leads to better and more
interpretable division of labor among specialists as shown in Figure 5. From the ﬁgure, It is clear that
the TIMs model is unable to divide labour among specialists with mechanism 2 being activation for
all the pixels in the image. On the other hand, we can see that TIMs + SW is able to divide labor
among specialists with each mechanism focusing on a different aspect of the image. We can see
that mechanism 2 gets activated for the digits which are present towards the centre of each of the 4
columns while mechanisms 3 and 4 cover the background of the digits, with mechanism 3 covering
the area between adjacent digits and mechanism 4 covering the area above and below the digits. Thus,
we can see that using a shared workspace aids the division of labor among different specialists. We
also ﬁnd that TIMs + SW results in the least cross-entropy loss in the test set when compared to TIMs
and Image Transformers (Parmar et al., 2018). Results shown in appendix Table 5."
EXPERIMENTS,0.2656716417910448,"(a) Mechanism Activa-
tion Maps for TIMs"
EXPERIMENTS,0.26865671641791045,"(b)
Mechanism
Ac-
tivation
Maps
for
TIMs+SW"
EXPERIMENTS,0.2716417910447761,"Figure 5: This ﬁgure shows the mechanism acti-
vation map for all 4 mechanims used in the multi-
mnist generation task for both TIMs and TIMs +
SW. Both the images in the ﬁgure correspond to the
activation maps from 4 different examples. Each
activation map contains 4 mechanisms shown from
left to right in a single row. Each mechanism is
shown using a 32 x 32 image, a particular pixel in
a mechanism activation map is shown in white if
that mechanism was used during the generation of
that pixel while generating the image."
EXPERIMENTS,0.2746268656716418,"CATER: Object Tracking. Cater is a spatio-
temporal reasoning video dataset introduced in
Girdhar and Ramanan (2019). Each video con-
tains 3D objects organized in a 6 × 6 grid. Each
object affords certain actions that can be per-
formed on them. These actions result in move-
ment of the concerned objects and change in
their positions. Some of these actions include:
rotate, pick-place, slide, contain. Throughout
the duration of the video, a number of these ac-
tions are performed to get the ﬁnal state of the
grid. Note that only a single object undergoes
an action, at any instant. The task that we fo-
cus on here is called localization. In this task,
the goal is to predict the location of the target
object in the ﬁnal frame. In this case the target
object is called a snitch. The snitch as well as
the other objects move across the 6 × 6 grid. In
some scenarios, the snitch may be covered by
other objects hence hiding it from the view. In
such cases, tracking the movement of the snitch
across frames becomes essential. Therefore, cap-
turing long-range temporal dependencies is es-
sential to solve this task."
EXPERIMENTS,0.27761194029850744,"The information exchange limit enforced by the
limited capacity of the shared workspace should
be useful here as well. For CATER, in some frames the snitch is not visible as it is covered by other
objects. Therefore, ideally the model only needs to attend to frames in which the snitch is visible.
Additionally, if the snitch is visible throughout the video in all frames, then to accurately predict
the ﬁnal position of the snitch, the model only needs to attend to the ﬁnal frame of the video and
can completely ignore the initial frames. The results for this task are presented in Table 1. We also
experimented with both soft competition TR+SSW and hard competition TR+HSW, with only
k = 5 specialists writing into the shared workspace. We can see that models with a shared workspace
outperform those with pairwise multihead attention thus conﬁrming our hypothesis about the beneﬁts
of a shared workspace for this task. As shown in Table 1 proposed method convincingly outperforms
the Set Transformer."
EXPERIMENTS,0.28059701492537314,"Relational Reasoning : Sort-of-CLEVR. In relational reasoning, the model is tasked with answer-
ing questions about certain properties of various objects and their relations with other objects. The
model is presented with an image and a question for that image. This task has a clear sparse structure
as in order to answer the questions correctly, it needs to only reason about a speciﬁc subset of objects
that the question mentions. For this task, we use the Sort-of-CLEVR dataset (Santoro et al., 2017)."
EXPERIMENTS,0.2835820895522388,"Each image in Sort-of-CLEVR is of size 75 × 75 and contains 6 randomly placed geometrical shapes
of 6 possible colors and 2 possible shapes. Each image comes with 10 relational questions and 10 non-"
EXPERIMENTS,0.2865671641791045,Published as a conference paper at ICLR 2022.
EXPERIMENTS,0.28955223880597014,"relational questions. Non-relational questions only consider properties of individual objects. On the
other hand, relational questions consider relations among multiple objects. For more details about the
question see appendix Figure 8. The input to the model consists of the image and the corresponding
question. We ﬁrst obtain a sequence of equal-sized patches for the image as in vision Transformers
(Dosovitskiy et al., 2020). We concatenate the resulting patch sequence with the representation of
the question and pass the combined sequence through the Transformer. Sort-of-CLEVR has a ﬁnite
number of possible answers, hence this task is setup as a classiﬁcation task."
EXPERIMENTS,0.29253731343283584,"Model
Num. Slots
ARI ↑
MSE ↓"
EXPERIMENTS,0.2955223880597015,"SCOFF
-
0.276±0.001
0.083±0.0
SCOFF + SW
2
0.154±0.007 0.135±0.002
SCOFF + SW
4
0.487±0.085
0.059±0.0
SCOFF + SW
5
0.915±0.0
0.035±0.0"
EXPERIMENTS,0.29850746268656714,"SCOFF + SW
8
0.891±0.001
0.039±0.0
SCOFF + SW
10
0.351±0.001
0.08±0.0"
EXPERIMENTS,0.30149253731343284,"Table 2: Here we show the performance
of SCOFF augmented with shared
workspace attention on the bouncing
balls task.
We also analyse the ef-
fect of varying number of slots in the
shared workspace.
This also shows
that by increasing the number of slots
performance decreases hence validat-
ing claims regarding bandwidth lim-
ited communication channel via shared
workspace."
EXPERIMENTS,0.3044776119402985,"We present the results for this task in Figure 4. We observe
that the Transformers with the shared workspace converge
faster and outperform the baselines for relational as well as
non-relational questions. The superior performance with
shared memory can be attributed to the inherent sparsity
of this task. For instance, in non-relational questions, the
model only needs to attend to a single object referenced in
the question to answer it correctly, while relational ques-
tions only consider a small subset of objects in the image,
thus sparsity is helpful for both these types of questions.
Therefore, the limited capacity of the shared workspace
forces the model to attend to only relevant information."
EXPERIMENTS,0.3074626865671642,"Shared Workspace for Physical Reasoning.
In this
task, we consider a set of bouncing balls and the model is
tasked with predicting the trajectory of the balls at each
step. In order to solve this task, a coherent picture of
where and which objects will collide needs to be estab-
lished by the learner. We use the bouncing-ball dataset
from Van Steenkiste et al. (2018). We train the model for
next-step prediction. We compare the proposed approach against SCOFF (Goyal et al., 2020). The
results of our comparison are shown in Table 2. We use the ARI and MSE metric for comparison.
ARI measures how well the different balls are segregated into different slots, higher ARI means better
segregation. We can see that using a shared workspace results in higher ARI as compared to pairwise
communication in SCOFF. Thus, using a shared workspace results in better division of labor among
specialists. We also compare the proposed method against other baselines in appendix section F.1."
EXPERIMENTS,0.31044776119402984,"Shared Workspace for Atari Video Games.
We start by training RIMs, RIMs + shared workspace
(SW) on three ""source"" games (Pong, River Raid, and Seaquest) and test if the learned features
transfer to a different subset of randomly selected ""target"" games (Alien, Asterix, Boxing, Centipede,
Gopher, Hero, James Bond, Krull, Robotank, Road Runner, Star Gunner, and Wizard of Wor). We
take a sufﬁcient number of specialists in RIMs (10). We train on source games for 10M steps, and
then ﬁne-tune on transfer games for 10M more steps. We choose these games as they were also used
in the original RIMs paper (Goyal et al., 2019). Using a suite of 36 game pairs, we ﬁnd that RIMs
+ SW outperforms RIMs on both game A (a median performance ratio of 1.13; mean of 1.16) and
game B (a median performance ratio of 1.11; mean of 1.15). The improved performance with RIMs +
SW is due to better forward transfer (knowledge acquired for game A facilitates the learning of game
B) and reduced backward interference (knowledge acquired for game B does not disrupt knowledge
acquired for game A), presumably thanks to a more appropriate modularization of knowledge."
CONCLUSION,0.31343283582089554,"5
CONCLUSION"
CONCLUSION,0.3164179104477612,"Inspired by cognitive neuroscience global workspace theories, we have proposed a shared workspace
model for establishing coherence among modular neural specialists while exchanging information
in a systematic way. We show that using a limited capacity shared workspace as a bottleneck for
mediating communication among specialists results in better performance across a wide range of
visual reasoning benchmarks as compared to the pairwise interactions typically used in self-attention
schemes. The proposed approach combines several key properties: knowledge and expertise is
divided among specialists, they compete to post new contents to the workspace, and after being
updated, the shared workspace is accessible to all specialists for their own updates."
CONCLUSION,0.3194029850746269,Published as a conference paper at ICLR 2022.
ETHICS STATEMENT,0.32238805970149254,ETHICS STATEMENT
ETHICS STATEMENT,0.3253731343283582,"The authors do not foresee any negative social impacts of this work, but of course the accumulation
of improvements in ML could be misused as it may give more power to nefarious agents."
REPRODUCIBILITY STATEMENT,0.3283582089552239,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.33134328358208953,"We use Algorithms 1 and 2 for our experiments, we will be releasing the code after the review process.
We also provide our code in the supplementary material."
REFERENCES,0.33432835820895523,REFERENCES
REFERENCES,0.3373134328358209,S. Ahmad and S. Omohundro. Equilateral triangles: A challenge for connectionist vision. 2009.
REFERENCES,0.3402985074626866,"Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 39–48,
2016."
REFERENCES,0.34328358208955223,"Bernard J Baars. A cognitive theory of consciousness. Cambridge University Press, 1993."
REFERENCES,0.34626865671641793,"Bernard J Baars. In the theatre of consciousness. global workspace theory, a rigorous scientiﬁc theory
of consciousness. Journal of Consciousness Studies, 4(4):292–309, 1997."
REFERENCES,0.3492537313432836,"Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473, 2014."
REFERENCES,0.3522388059701492,"Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław D˛ebiak, Christy
Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale
deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019."
REFERENCES,0.35522388059701493,"Léon Bottou and Patrick Gallinari. A framework for the cooperation of learning algorithms. In
Advances in neural information processing systems, pages 781–788, 1991."
REFERENCES,0.3582089552238806,"Valentino Braitenberg. Vehicles: Experiments in synthetic psychology. MIT press, 1986."
REFERENCES,0.3611940298507463,"Rodney A Brooks. Intelligence without representation. Artiﬁcial intelligence, 47(1-3):139–159,
1991."
REFERENCES,0.3641791044776119,"Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv preprint arXiv:2005.14165, 2020."
REFERENCES,0.36716417910447763,"Mikhail S Burtsev and Grigory V Sapunov. Memory transformer. arXiv preprint arXiv:2006.11527,
2020."
REFERENCES,0.3701492537313433,"Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse
transformers. arXiv preprint arXiv:1904.10509, 2019."
REFERENCES,0.373134328358209,"Michael D. Colagrosso and Michael C Mozer.
Theories of access consciousness.
In L. K.
Saul, Y. Weiss, and L. Bottou, editors, Advances in Neural Information Processing Sys-
tems 17, pages 289–296. MIT Press, 2005.
URL http://papers.nips.cc/paper/
2715-theories-of-access-consciousness.pdf."
REFERENCES,0.3761194029850746,"Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdi-
nov. Transformer-xl: Attentive language models beyond a ﬁxed-length context. arXiv preprint
arXiv:1901.02860, 2019."
REFERENCES,0.37910447761194027,"S. Dehaene, H. Lau, and S. Kouider. What is consciousness, and could machines have it? Science,
358(6362):486–492, 2017."
REFERENCES,0.382089552238806,"Stanislas Dehaene and Jean-Pierre Changeux. Experimental and theoretical approaches to conscious
processing. Neuron, 70(2):200–227, 2011."
REFERENCES,0.3850746268656716,Published as a conference paper at ICLR 2022.
REFERENCES,0.3880597014925373,"Stanislas Dehaene, Michel Kerszberg, and Jean-Pierre Changeux. A neuronal model of a global
workspace in effortful cognitive tasks. Proceedings of the national Academy of Sciences, 95(24):
14529–14534, 1998."
REFERENCES,0.39104477611940297,"Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal
transformers. arXiv preprint arXiv:1807.03819, 2018."
REFERENCES,0.3940298507462687,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An
image is worth 16x16 words: Transformers for image recognition at scale.
arXiv preprint
arXiv:2010.11929, 2020."
REFERENCES,0.3970149253731343,"Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A Rusu,
Alexander Pritzel, and Daan Wierstra. Pathnet: Evolution channels gradient descent in super neural
networks. arXiv preprint arXiv:1701.08734, 2017."
REFERENCES,0.4,"Jerry A Fodor. The modularity of mind. MIT press, 1983."
REFERENCES,0.40298507462686567,"Rohit Girdhar and Deva Ramanan. CATER: A diagnostic dataset for compositional actions and
temporal reasoning. CoRR, abs/1910.04744, 2019. URL http://arxiv.org/abs/1910.
04744."
REFERENCES,0.4059701492537313,"Anirudh Goyal and Yoshua Bengio. Inductive biases for deep learning of higher-level cognition.
arXiv preprint arXiv:2011.15091, 2020."
REFERENCES,0.408955223880597,"Anirudh Goyal, Alex Lamb, Jordan Hoffmann, Shagun Sodhani, Sergey Levine, Yoshua Bengio, and
Bernhard Schölkopf. Recurrent independent mechanisms. arXiv preprint arXiv:1909.10893, 2019."
REFERENCES,0.41194029850746267,"Anirudh Goyal, Alex Lamb, Phanideep Gampa, Philippe Beaudoin, Sergey Levine, Charles Blundell,
Yoshua Bengio, and Michael Mozer. Object ﬁles and schemata: Factorizing declarative and
procedural knowledge in dynamical systems. arXiv preprint arXiv:2006.16225, 2020."
REFERENCES,0.41492537313432837,"Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. CoRR, abs/1410.5401, 2014.
URL http://arxiv.org/abs/1410.5401."
REFERENCES,0.417910447761194,"Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-
Barwi´nska, Sergio Gómez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al.
Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626):
471–476, 2016."
REFERENCES,0.4208955223880597,"Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of
local experts. Neural computation, 3(1):79–87, 1991."
REFERENCES,0.42388059701492536,"Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Joao Carreira.
Perceiver: General perception with iterative attention. arXiv preprint arXiv:2103.03206, 2021."
REFERENCES,0.42686567164179107,"Andrej Karpathy. karpathy/mingpt, Aug 2020. URL https://github.com/karpathy/
minGPT."
REFERENCES,0.4298507462686567,"Nan Rosemary Ke, Anirudh Goyal ALIAS PARTH GOYAL, Olexa Bilaniuk, Jonathan Binas,
Michael C Mozer, Chris Pal, and Yoshua Bengio. Sparse attentive backtracking: Temporal
credit assignment through reminding. In Advances in neural information processing systems, pages
7640–7651, 2018."
REFERENCES,0.43283582089552236,"Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.43582089552238806,"Alex Lamb, Di He, Anirudh Goyal, Guolin Ke, Chien-Feng Liao, Mirco Ravanelli, and Yoshua
Bengio. Transformers with competitive ensembles of independent mechanisms, 2021. URL
https://openreview.net/forum?id=1TIrbngpW0x."
REFERENCES,0.4388059701492537,"Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set trans-
former: A framework for attention-based permutation-invariant neural networks. In International
Conference on Machine Learning, pages 3744–3753, 2019."
REFERENCES,0.4417910447761194,Published as a conference paper at ICLR 2022.
REFERENCES,0.44477611940298506,"Kanika Madan, Nan Rosemary Ke, Anirudh Goyal, Bernhard Schölkopf, and Yoshua Bengio. Meta
attention networks: Meta-learning attention to modulate information between recurrent independent
mechanisms. In International Conference on Learning Representations, 2021. URL https:
//openreview.net/forum?id=Lc28QAB4ypz."
REFERENCES,0.44776119402985076,"Marvin Minsky. Society of mind. Simon and Schuster, 1988."
REFERENCES,0.4507462686567164,"Sarthak Mittal, Alex Lamb, Anirudh Goyal, Vikram Voleti, Murray Shanahan, Guillaume Lajoie,
Michael Mozer, and Yoshua Bengio. Learning to combine top-down and bottom-up signals in
recurrent neural networks with attention over modules. In International Conference on Machine
Learning, pages 6972–6986. PMLR, 2020a."
REFERENCES,0.4537313432835821,"Sarthak Mittal, Alex Lamb, Anirudh Goyal, Vikram Voleti, Murray Shanahan, Guillaume Lajoie,
Michael Mozer, and Yoshua Bengio. Learning to combine top-down and bottom-up signals in
recurrent neural networks with attention over modules. In International Conference on Machine
Learning, pages 6972–6986. PMLR, 2020b."
REFERENCES,0.45671641791044776,"Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,
and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of
NAACL-HLT 2019: Demonstrations, 2019."
REFERENCES,0.4597014925373134,"Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and
Dustin Tran. Image transformer. In International Conference on Machine Learning, pages
4055–4064. PMLR, 2018."
REFERENCES,0.4626865671641791,"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019."
REFERENCES,0.46567164179104475,"Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive
transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019."
REFERENCES,0.46865671641791046,"Nasim Rahaman, Anirudh Goyal, Muhammad Waleed Gondal, Manuel Wuthrich, Stefan Bauer, Yash
Sharma, Yoshua Bengio, and Bernhard Schölkopf. S2rms: Spatially structured recurrent modules.
arXiv preprint arXiv:2007.06533, 2020."
REFERENCES,0.4716417910447761,"Scott Reed and Nando De Freitas. Neural programmer-interpreters. arXiv preprint arXiv:1511.06279,
2015."
REFERENCES,0.4746268656716418,"Eric Ronco, Henrik Gollee, and Peter J Gawthrop. Modular neural networks and self-decomposition.
Technical Report CSC-96012, 1997."
REFERENCES,0.47761194029850745,"Clemens Rosenbaum, Tim Klinger, and Matthew Riemer. Routing networks: Adaptive selection of
non-linear functions for multi-task learning. arXiv preprint arXiv:1711.01239, 2017."
REFERENCES,0.48059701492537316,"Clemens Rosenbaum, Ignacio Cases, Matthew Riemer, and Tim Klinger. Routing networks and the
challenges of modular and compositional computation. arXiv preprint arXiv:1904.12774, 2019."
REFERENCES,0.4835820895522388,"Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter
Battaglia, and Timothy Lillicrap. A simple neural network module for relational reasoning. In
Advances in neural information processing systems, pages 4967–4976, 2017."
REFERENCES,0.48656716417910445,"Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Theophane Weber,
Daan Wierstra, Oriol Vinyals, Razvan Pascanu, and Timothy Lillicrap. Relational recurrent neural
networks. In Advances in Neural Information Processing Systems, pages 7299–7310, 2018."
REFERENCES,0.48955223880597015,"Murray Shanahan. A cognitive architecture that combines internal simulation with a global workspace.
Consciousness and cognition, 15(2):433–449, 2006."
REFERENCES,0.4925373134328358,"Murray Shanahan. Embodiment and the inner life: Cognition and Consciousness in the Space of
Possible Minds. Oxford University Press, USA, 2010."
REFERENCES,0.4955223880597015,"Murray Shanahan. The brain’s connective core and its role in animal cognition. Philosophical
Transactions of the Royal Society B: Biological Sciences, 367(1603):2704–2714, 2012."
REFERENCES,0.49850746268656715,Published as a conference paper at ICLR 2022.
REFERENCES,0.5014925373134328,"Murray Shanahan and Bernard Baars. Applying global workspace theory to the frame problem.
Cognition, 98(2):157–176, 2005."
REFERENCES,0.5044776119402985,"Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and
Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv
preprint arXiv:1701.06538, 2017."
REFERENCES,0.5074626865671642,"Sjoerd Van Steenkiste, Michael Chang, Klaus Greff, and Jürgen Schmidhuber. Relational neural
expectation maximization: Unsupervised discovery of objects and their interactions. arXiv preprint
arXiv:1802.10353, 2018."
REFERENCES,0.5104477611940299,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pages 5998–6008, 2017."
REFERENCES,0.5134328358208955,"Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung
Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in
starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019."
REFERENCES,0.5164179104477612,Published as a conference paper at ICLR 2022.
REFERENCES,0.5194029850746269,"Part I
Appendix"
REFERENCES,0.5223880597014925,"A
PSEUDO CODES"
REFERENCES,0.5253731343283582,"Alg. 1 shows the integration of shared workspace with RIMs (Goyal et al., 2019). We replace
the direct module to module interaction via attention in RIMs, with shared workspace. Specialists
compete to write in the shared workspace, and the contents of the workspace are broadcasted to all
the specialists."
REFERENCES,0.5283582089552239,"Alg. 2 shows the integration of the shared workspace with TIMs (Lamb et al., 2021). Again we
replace the direct module to module communication in TIMs, with a shared workspace."
REFERENCES,0.5313432835820896,Algorithm 1: Shared Workspace integration with RIMs
REFERENCES,0.5343283582089552,"Input: Current sequence element, xt and previous state of the specialist, {ht−1,k}, for k ∈{1, . . . , ns}
and structure of memory as a matrix M with row wise compartmentalized memories, where mi refers to the
state of slot i (total number of slots is nm)."
REFERENCES,0.5373134328358209,Step 1: Process image by position p with fully convolutional net
REFERENCES,0.5402985074626866,"• cp = [CNN(xt)]p
• zt = [cp ep]
(concatenate encoding of position to CNN output)"
REFERENCES,0.5432835820895522,Step 2: Specialists compete to be selected to update the workspace based on current input
REFERENCES,0.5462686567164179,"• qk = ht−1,kW q"
REFERENCES,0.5492537313432836,"• sk = softmax
  qkκ
√de

, where κ = (ztW e)T"
REFERENCES,0.5522388059701493,• Construct a set Ft which contains the indices of the nsel specialists that have the largest sk
REFERENCES,0.5552238805970149,"• ¯ht,k ="
REFERENCES,0.5582089552238806,"(
gk (skztW v, ht−1,k)
k ∈Ft ,
ht−1,k
k /∈Ft ,"
REFERENCES,0.5611940298507463,• ak = skztW v ∀k ∈Ft (Scaled Dot Product Attention)
REFERENCES,0.564179104477612,Step 3: Activated specialists write in a shared workspace
REFERENCES,0.5671641791044776,"• e
Q = M f
W q"
REFERENCES,0.5701492537313433,• R = [M; A] where A is the matrix whose rows are the ak∀k ∈Ft
REFERENCES,0.573134328358209,"• M ←softmax
 e
Q(R f
W e)T
√de"
REFERENCES,0.5761194029850746,"
Rf
W v"
REFERENCES,0.5791044776119403,Step 4: Broadcast of information from the shared workspace
REFERENCES,0.582089552238806,"• bqk = ¯ht,k c
W q
∀k ∈{1, . . . , ns}"
REFERENCES,0.5850746268656717,"• sk,j = softmax

bqk bκj
√de"
REFERENCES,0.5880597014925373,"
where bκj = (mj c
W e)T
∀k ∈{1, . . . , ns}, j ∈{1, . . . , nm}"
REFERENCES,0.591044776119403,"• ht,k = ¯ht,k + P"
REFERENCES,0.5940298507462687,"j sk,j bvj where bvj = mj c
W v
∀k ∈{1, . . . , ns}"
REFERENCES,0.5970149253731343,"B
HYPERPARAMETERS"
REFERENCES,0.6,Table 3 lists the different hyper-parameters.
REFERENCES,0.6029850746268657,Parameters in RIMs+SW:
REFERENCES,0.6059701492537314,RIMs with shared workspace has three set of parameters:
REFERENCES,0.608955223880597,"• Parameters corresponding to Input attention Parameters for the attention for the k-th
specialist θk = (W q
k , W e, W v) corresponding to query, keys, and values respectively. Each
specialist has different query parameters but share the same keys and values (which are
function of the input). In the table it corresponds to the inp keys, inp values, inp heads
respectively.
• Writing in a shared workspace: Parameters corresponding to the writing in the memory.
Here, we follow the similar mechanisms as in RMC(Santoro et al., 2018), where shared"
REFERENCES,0.6119402985074627,Published as a conference paper at ICLR 2022.
REFERENCES,0.6149253731343284,Algorithm 2: Shared Workspace integration with TIMs
REFERENCES,0.6179104477611941,"Notation: Consider hl as the output of the lth transformer layer. Let sequence length of original input be T
and embedding dimension of transformer be D. Let the transformer be composed of nb mechanisms and
memory be denoted as a matrix M with row wise compartmentalized memories, where mi refers to the state
of slot i (total number of slots is nm). Consider hk
l = hl[:, (k −1)D/nb : kD/nb] to be the hidden state of
mechanism indexed k at layer l."
REFERENCES,0.6208955223880597,"Initialization: Convert the raw input X ∈RT ×vocab_size to
h0 = positional_encoding + Embedding(X) where h0 ∈RT ×D. Initialize memory matrix M which
remains common for all layers in the transformer."
REFERENCES,0.6238805970149254,Input to the layer l: hl−1 having shape RT ×D
REFERENCES,0.6268656716417911,"Step 1: Mechanisms compete to be selected to update the workspace based on the input they receive from
the previous layer"
REFERENCES,0.6298507462686567,• W c ∈RD/nb×1
REFERENCES,0.6328358208955224,"• ck = hk
l−1W c
k
∀k ∈{1, . . . , nb}"
REFERENCES,0.6358208955223881,"• c = softmax(concat(c1, .., cnb)), c ∈RT ×nb"
REFERENCES,0.6388059701492538,"• For each time step t in the original sequence of length T, we use the soft score c to select the top nsel
mechanisms which would self-attend and write to the memory. Hence generating set Ft which stores the
indices of nsel mechanisms for position t ∈{1, 2, ..., T}. Also construct c∗
k ∈RT ×D/nb where"
REFERENCES,0.6417910447761194,"c∗
k[t, :] ="
REFERENCES,0.6447761194029851,"(
c[t][k]
k ∈Ft ,
0
k /∈Ft ,"
REFERENCES,0.6477611940298508,Step 2: Selected mechanisms self-attend and update their hidden state
REFERENCES,0.6507462686567164,"• residualk = hk
l−1"
REFERENCES,0.6537313432835821,"• ¯hk
l = c∗
k ⊙SelfAttention(hk
l−1) + residualk
∀k ∈{1, . . . , nb}"
REFERENCES,0.6567164179104478,Step 3: Selected mechanisms write on the shared workspace
REFERENCES,0.6597014925373135,• Memory matrix M was last modiﬁed by mechanisms of layer l −1
REFERENCES,0.6626865671641791,"• Let ak = c∗
k ⊙¯hk
l and a = concat(a1, .., anb). Absorb the ﬁrst dimension (corresponding to position in
the sequence) in the batch dimension by reshaping a. Perform the same steps as in algorithm 1."
REFERENCES,0.6656716417910448,"• e
Q = M f
W q"
REFERENCES,0.6686567164179105,• R = [M; A] where A = aW v
REFERENCES,0.6716417910447762,"• M ←softmax
 e
Q(R f
W e)T
√de"
REFERENCES,0.6746268656716418,"
Rf
W v"
REFERENCES,0.6776119402985075,Step 4: Broadcast of information from the shared workspace
REFERENCES,0.6805970149253732,• Reshape the new memory to bring back the sequence dimension. Perform the same steps as in algorithm 1.
REFERENCES,0.6835820895522388,"• bqk = ¯hk
l c
W q
∀k ∈{1, . . . , nb}"
REFERENCES,0.6865671641791045,"• sk,j = softmax

bqk bκj
√de"
REFERENCES,0.6895522388059702,"
where bκj = (mj c
W e)T
∀k ∈{1, . . . , nb}, j ∈{1, . . . , nm}"
REFERENCES,0.6925373134328359,"• hk
l = ¯hk
l + P"
REFERENCES,0.6955223880597015,"j sk,j bvj where bvj = mj c
W v
∀k ∈{1, . . . , nb}"
REFERENCES,0.6985074626865672,Published as a conference paper at ICLR 2022.
REFERENCES,0.7014925373134329,Equilateral Triangles
REFERENCES,0.7044776119402985,Non Equilateral Triangles
REFERENCES,0.7074626865671642,Figure 6: A demonstration of the detecting equilateral triangles task.
REFERENCES,0.7104477611940299,"Parameter
Value"
REFERENCES,0.7134328358208956,"Number of specialists (ns)
6
Size of each specialist
85
Number of memory slots (nm)
Optimizer
Adam(Kingma and Ba, 2014)
learning rate
1 · 10−4
batch size
64
Inp keys
64
Inp Values
85
Inp Heads
4
Inp Dropout
0.1
Number of memory slots
4
Number of memory heads
1
Size of attention head
32
Key size
32
Number of MLP layers in Attention
3
Gate Style
’unit’
Memory Attention Heads
4
Memory Attention keys
32
Memory Attention Values
32"
REFERENCES,0.7164179104477612,Table 3: Generic Hyperparameters for the proposed model (for RIMs)
REFERENCES,0.7194029850746269,"workspace is seen as a Matrix with row wise compartmentalized memories (i.e slots) i.e
f
W q, f
W e, f
W v. In the table it corresponds to number of memory slots, number of memory
heads, size of attention head, key size and number of mlp layers in attention. These are
the same hyper-paramter as in RMC (Santoro et al., 2018). We tried two different set of
hyper-parameters (a) where we only have a single slot and (b) where we have 4 slots."
REFERENCES,0.7223880597014926,"• Broadcast of Information from the shared workspace: In this process, the information
in the workspace gets broadcasted to all the specialists such that each specialist produces
a query, and the keys and values are a function of the memory state. Each specialist gets
information from the memory according to its query, and this information is used to update
the state of each specialist in a residual fashion. This corresponds to the parameters of c
W v,
c
W q, c
W e in the table i.e memory attention heads, memory attention keys, and memory
attention values. We did not do any hyper-parameter search for these hyper-parameters."
REFERENCES,0.7253731343283583,Published as a conference paper at ICLR 2022.
REFERENCES,0.7283582089552239,Resources Used:
REFERENCES,0.7313432835820896,"• For vision tasks like Sort-of-clever, Equilateral triangle, CIFAR classiﬁcation, it takes about
6 hours to run 200 epochs on V100 (32G) GPU."
REFERENCES,0.7343283582089553,"• It takes about 2 days to train the proposed model on bouncing ball task for 100 epochs on
V100 (32G) GPU. We did not do any hyper-parameter search speciﬁc to a particular dataset
(i.e 4Balls or 678Balls or Curtain Task). We ran the proposed model for different number of
memory slots (i.e 2/4/8) for all the different datasets."
REFERENCES,0.7373134328358208,"• For Starcraft task, it takes about 5 days to train on V100 (16G) GPU with batch size of 4."
REFERENCES,0.7402985074626866,"C
IMPLEMENTATION DETAILS"
REFERENCES,0.7432835820895523,"Writing Information in the shared workspace. While writing information to the shared workspace,
we update the workspace using a gating mechanism as proposed in Santoro et al. (2018). The gating
mechanism consists of input and forget gates. Let M t−1 and M t be the previous and updated
memory matrix respectively. Let M be the result of the attention mechanism as described in step 2
of section 2.1. Let X1...ns be the input to ns specialists. The gating mechanism can be formulated as
follows."
REFERENCES,0.746268656716418,"¯
X = 1 ns ns
X"
REFERENCES,0.7492537313432835,"i=1
relu(Xi × W 1)"
REFERENCES,0.7522388059701492,"K = ¯
X + tanh(M t−1)"
REFERENCES,0.755223880597015,I = sigmoid(KW I)
REFERENCES,0.7582089552238805,F = sigmoid(KW F )
REFERENCES,0.7611940298507462,M t = I × tanh(M) + F × M t−1
REFERENCES,0.764179104477612,"Here, I and F indicate the input and forget gates respectively. Note that W 1 is shared across all ns
specialists."
REFERENCES,0.7671641791044777,"D
PROPERTIES OF SHARED WORKSPACE"
REFERENCES,0.7701492537313432,"In section 2, we claim that higher-order interaction terms and effects due to persistence of memory
are key contributors to Shared Workspace performance. We support those claims here:"
REFERENCES,0.7731343283582089,"Shared Workspace vs repeated self attention Higher-order interaction can be simulated by repeat-
ing the self-attention step multiple times at the same layer/time-step. However, due to the absence of
a global communication channel, there is no constraint that the messages passed among the neural
modules should lie in the same representation space. We modify a standard transformer where we
repeat the self-attention step two times in every layer. We expect that 2×Self Attention will perform
worse than SW. We also run a model where both self-attention as well as shared workspace is used by
the transformer to update its state."
REFERENCES,0.7761194029850746,"Persistence of Memory To check whether persistence is crucial for our model to perform well,
we run a model where we re-initialize the shared workspace at every layer. Again we expect that
removing memory persistence should result in a drop in performance and speed of convergence."
REFERENCES,0.7791044776119403,We run these models on sort-of-clevr dataset and present the results in ﬁgure 7
REFERENCES,0.7820895522388059,"We note that removing persistence of memory results in signiﬁcantly slower convergence. Replacing
SW with 2×SA results in a signiﬁcant drop in performance."
REFERENCES,0.7850746268656716,Published as a conference paper at ICLR 2022.
REFERENCES,0.7880597014925373,"Figure 7: Comparison on Sort-of-CLEVR relational reasoning. Speed of convergence for rela-
tional and non-relational questions in the sort-of-clevr dataset. We can see that the Shared Workspace
model converges faster and generalizes better as compared to all the other models. Here SW refers to
shared workspace, 2×SA refers to applying self-attention twice in the same layer, SW+SA refers
using both Shared Workspace and Self Attention in each transformer layer."
REFERENCES,0.7910447761194029,Figure 8: A sample from the sort-of-clevr dataset.
REFERENCES,0.7940298507462686,"E
TRANSFORMER TASKS"
REFERENCES,0.7970149253731343,"E.1
DETECTING EQUILATERAL TRIANGLES"
REFERENCES,0.8,"A demonstration of this task can be found in ﬁgure 6. We use images of size 64 × 64 for this task.
Our training dataset consists of 50000 examples and we evaluate on 10000 examples. We follow the
same setup as vision transformers Dosovitskiy et al. (2020) for this task. We divide the image into
patches of size 4 × 4, this sequence of patches is fed as input to a 4-layered transformer along with
the CLS token which is used for classiﬁcation. We set hidden dim to 256 and ffn dim to 512. For
the proposed model (TR+SSW, TR+HSW), We use a query and key size of 32, and value size of
64. We use 4 heads during reading from and writing into the shared workspace which consist of 8
memory slots. For the baseline models (TR, TR + HC, STR), we use query, key and value size of 64
and 4 heads. For training, we use a batch size of 64. We train the model for 200 epochs using Adam
optimizer with a learning rate of 0.0001. We anneal the learning rate using cosine annealing."
REFERENCES,0.8029850746268656,"E.2
SORT-OF-CLEVR"
REFERENCES,0.8059701492537313,"Figure 8 shows a sample from this dataset. The images in this dataset are of size 75 × 75. Each
question is encoded into 11 bits. The ﬁrst 6 bits indicate color, the next 2 bits indicate question
type (relational or non-relational), and the remaining 3 bits indicate question subtype (according
to ﬁgure 8). We use a 4-layered transformer for this task with hidden dim set to 256 and ffn dim
set to 512. For the proposed model (TR+SSW, TR+HSW), We use a query and key size of 32,
and value size of 64. We use 4 heads during reading from and writing into the shared workspace
which consists of 8 memory slots. For the baseline models (TR, TR + HC, STR), we use query,
key and value size of 64 and 4 heads. We encode the 11 bit question into a 256 dimensional vector
representation and concatenate it with the sequence of 15×15 sized patched obtained from the image."
REFERENCES,0.808955223880597,Published as a conference paper at ICLR 2022.
REFERENCES,0.8119402985074626,"We use the representation corresponding to the CLS token for classiﬁcation. We train the model using
cross-entropy loss. We use a batch size of 64 and train the model for 100 epochs. We use Adam
optimizer with a learning rate of 0.0001 for training."
REFERENCES,0.8149253731343283,"E.3
CATER: OBJECT TRACKING"
REFERENCES,0.817910447761194,"Each CATER video consists of about 300 frames of size 224 × 224. We ﬁrst sample frames at a
sampling rate of 6 which results in 50 frames. From these 50 frames, we stack 5 consecutive frames
together and pass each stack through a 18 layered resnet. The corresponding sequence of 10 frames is
passed as input to the transformer. This task is setup as a classiﬁcation task where we have to predict
which cell in the 6 × 6 grid contains the snitch in the ﬁnal frame. We use a 6-layered transformer
with hidden dim set to 512 and ffn dim set to 2048. For the proposed model (TR+SSW, TR+HSW),
We use a query and key size of 32, and value size of 64. We use 8 heads during reading from and
writing into the shared workspace which consists of 8 memory slots. For the baseline models (TR,
TR + HC, STR), we use query, key and value size of 64 and 8 heads."
REFERENCES,0.8208955223880597,"F
RIMS TASKS"
REFERENCES,0.8238805970149253,"F.1
BOUNCING BALL"
REFERENCES,0.826865671641791,"30
45
Steps 0 1 2 3 4 5"
REFERENCES,0.8298507462686567,Binary Cross Entropy
REFERENCES,0.8328358208955224,(a) 4 Balls
REFERENCES,0.835820895522388,"30
45
Steps"
REFERENCES,0.8388059701492537,(b) 678 Balls
REFERENCES,0.8417910447761194,"30
45
Steps"
REFERENCES,0.844776119402985,(c) Curtain LSTM RMC RIMs
REFERENCES,0.8477611940298507,RIMs + SW
REFERENCES,0.8507462686567164,"Figure 9: Bouncing ball motion: Prediction er-
ror comparison of the proposed method, LSTM,
RIMs and RMC baseline. Given 10 frames of
ground truth, the model predicts the rollout over
the next 35 steps. Here, we present the BCE
for the 30th frame and 45th frame. The pro-
posed SW extension performs better than other
baselines in accurately predicting the dynamics,
with an increasing advantage as the number of
unrolled steps (30 vs 45) and balls ((a) vs (b))
increases. Results are an average over 5 random
seeds."
REFERENCES,0.8537313432835821,"The dataset consists of 50,000 training examples
and 10,000 test examples showing ∼50 frames of
either 4 solid balls bouncing in a conﬁned square
geometry (4Balls), 6-8 balls bouncing in a con-
ﬁned geometry (678Balls), 3 balls bouncing in a
conﬁned geometry with an occluded region (Cur-
tain), or balls of different colors (Colored 4Balls)
and (Colored 678Balls). We train baselines as well
as the proposed shared workspace extension (e.g.,
RIMs + SW). As shown in Fig. 9, we study the
performance of the proposed model compared with
LSTM, RIMs and RMC. The ﬁrst 10 frames of
ground truth are fed in and then the system is rolled
out for the next 35 time steps. During the rollout
phase, the proposed method performs better than
the baselines in accurately predicting the dynamics
of the balls as reﬂected by cross entropy (CE)."
REFERENCES,0.8567164179104477,"We trained baselines as well as proposed model for
about 100 epochs. We use the same architecture for
encoder as well as decoder as in (Van Steenkiste
et al., 2018). Hyper-parameters speciﬁc to the pro-
posed architecture are listed in Tab. 3."
REFERENCES,0.8597014925373134,"G
INTEGRATING
SW WITH MORE ARCHITECTURES"
REFERENCES,0.8626865671641791,"G.1
TIMS"
REFERENCES,0.8656716417910447,"TIMs was proposed by Lamb et al. (2021). A transformer network is divided into ‘independent mech-
anisms’ which update their state via sharing information between positions and sharing information
between mechanisms. The information sharing step between mechanisms can be replaced by SW to
create TIMs+SW."
REFERENCES,0.8686567164179104,"G.1.1
MULTIMNIST GENERATION"
REFERENCES,0.8716417910447761,"In this task, we train an Image Transformer Parmar et al. (2018) (pixel-by-pixel, raster-order generative
model) for next pixel prediction task on the “MultiMNIST dataset”"
REFERENCES,0.8746268656716418,Published as a conference paper at ICLR 2022.
REFERENCES,0.8776119402985074,"Parameter
Value"
REFERENCES,0.8805970149253731,Common Parameters
REFERENCES,0.8835820895522388,"Optimizer
Adam(Kingma and Ba, 2014)
Learning rate
1 · 10−3
Batch size
12
Number of attention heads
8 TR"
REFERENCES,0.8865671641791045,"Size of transformer layer
256 TIMs"
REFERENCES,0.8895522388059701,"Number of mechanisms
4
Size of mechanism
48"
REFERENCES,0.8925373134328358,TIMs+SW
REFERENCES,0.8955223880597015,"Number of mechanisms
4
Size of mechanism
40
Number of memory slots
2
Size of memory slots
160
Memory Attention Heads
8
Gate Style
’unit’
Number of MLP layers in Attention
2"
REFERENCES,0.8985074626865671,Table 4: Hyperparameters for MultiMNIST Task
REFERENCES,0.9014925373134328,"Each 32 × 32 image in this dataset is made up of four randomly selected (and augmented) MNIST
digits (resized to 32×8) placed side-by-side as shown in ﬁgure 10. The digits themselves are selected
independently of one-another."
REFERENCES,0.9044776119402985,"The main aim of creating such a task is to observe the working of independent mechanisms in
architectures such as TIMs (Lamb et al., 2021). Each image in the MultiMNIST dataset can be
broken down into different sets of independent spatial components. Since the digits which make up
the image are independently selected, the joint distribution of pixel intensities in any one of the four
sections of the image is statistically independent of the pixel intensities in any other section of the
image. Moreover each section of the image can be further broken down into independent spatial
components: one that pertains to the background and one that pertains to the foreground."
REFERENCES,0.9074626865671642,"It is expected that a monolithic architecture (having a single computational unit) would have to devote
a signiﬁcant portion of its training to learn the statistical independence between the different con-
stituents of the image. On the other hand, architectures made up of sparsely interacting independent
mechanisms have a natural way of capturing such statistical independence. A division of labour
where each mechanism is focused on the generation of a distinct independent constituent of the
image should allow for better generalization on the test set. Once the generation of a constituent is
completed, the task can be handed over to some other mechanism based on current position in the
image."
REFERENCES,0.9104477611940298,"For this experiment we train a standard transformer with shared parameters across all layers (denoted
by TR), TIMs (Lamb et al., 2021) with 4 mechanisms, and a modiﬁed version of TIMs with 4 mecha-
nisms where the pair-wise communication between the mechanisms is replaced by communication
via a shared workspace (denoted by TIMs+SW)."
REFERENCES,0.9134328358208955,"Training. We follow the minGPT Image Transformer setup Karpathy (2020) for our experiments.
All three of the conﬁgurations have 8 layers, 8 heads for multi-headed attention and use the exact
same parameter initialization and base architecture. We train all three of the models for 20 epochs."
REFERENCES,0.9164179104477612,"In the TR model, all of the 8 monolithic layers share the same set of parameters. In TIMs and
TIMs+SW, the ﬁrst two layers are the standard monolithic layers having shared parameters. The
middle four layers in both of these architectures are modular layers with four mechanisms. These four"
REFERENCES,0.9194029850746268,Published as a conference paper at ICLR 2022.
REFERENCES,0.9223880597014925,"Model
Loss"
REFERENCES,0.9253731343283582,"TR
0.000058
TIMs (4 mechanisms)
0.000050
TIMs+SW (4 mechanisms)
0.000042"
REFERENCES,0.9283582089552239,"Table 5: MultiMNIST Generation Task: We report cross-entropy loss between the generated pixel
values and the true pixel values on the test set of MultiMNIST Generation Task (smaller numbers are
better)"
REFERENCES,0.9313432835820895,"Figure 10: A randomly selected batch of 16 images from the MultiMNIST generation dataset (4 rows
and 4 columns)"
REFERENCES,0.9343283582089552,"layers share the same set of parameters. In the case of TIMs+SW, the four mechanisms in these layers
communicate via a shared workspace (having 2 memory slots). This shared workspace is common
for all four middles layers and is absent in TIMs where the mechanisms communicate via pair-wise
competition as proposed in the original paper. TIMs and TIMs+SW architectures are concluded by
two more monolithic layers which again share the same parameters."
REFERENCES,0.9373134328358209,"For all three models to have comparable number of parameters, we chose the transformer embedding
dimension to be 256 for TR model, 192 for TIMs model and 160 for TIMs+SW model. In TIMs and
TIMs+SW, the embedding dimension is divided equally among the four specialists. Each memory
slot in the shared workspace of the TIMs+SW model has a 160 dimensional embedding and the
model uses four heads to perform read and write operations on the shared workspace. Total number
of parameters for all three architectures lie between 1M and 1.8M."
REFERENCES,0.9402985074626866,"Results. We observe the best cross-entropy loss in 20 epochs on the test set of the MultiMNIST
dataset for the next pixel prediction task in the table 5. We further plot the sixth layer “mechanism
activation score” of TIMs and TIMs+SW while generating the ﬁrst four images of the test set in the
best epoch (shown in ﬁgure 5)."
REFERENCES,0.9432835820895522,"G.1.2
USING WORKSPACE FOR LANGUAGE MODELLING"
REFERENCES,0.9462686567164179,"We train our models on the WikiText-103 dataset by posing a language modeling problem. The
dataset is divided into train, test and validation sets which are composed out of 28,475, 60 and 60
articles respectively. The total number of tokens in the train set is more than 103 million, hence the
name of the dataset. This dataset retains numbers, punctuation and case."
REFERENCES,0.9492537313432836,"Training. We train our models for 15 epochs for the next word prediction task on the WikiText-103
dataset and report the perplexity on the validation set. We show the results using TIMs (Lamb
et al., 2021) with 4 mechanisms and TIMs+SW with 4 mechanisms (where we replace the pairwise
communication in TIMs with communication via a shared workspace like in the MultiMNIST
experiment). We modify the FAIRSEQ Ott et al. (2019) transformer language model class for all of
our experiments."
REFERENCES,0.9522388059701492,"For TIMs+SW, we train and test two different variants: TIMs+SSW uses soft attention to generate the
activation scores of competing independent mechanisms whereas TIMs+HSW uses top-k attention
with k=2."
REFERENCES,0.9552238805970149,Published as a conference paper at ICLR 2022.
REFERENCES,0.9582089552238806,"Since in this test, our aim is to compare the performance of the two models for the language modeling
task, the architectures are only made up of a transformer decoder. In both of the models, there are 8
transformer decoder layers divided into 3 sets. The ﬁrst 2 layers are standard monolithic decoder
layers which share the same parameters. The next 4 layers are modular layers (TIMs layers or
TIMs+SW layers depending on the model choice). These layers also share the same parameters
among themselves. The last 2 layers are again standard monolothic decoder layers, both sharing the
same parameters."
REFERENCES,0.9611940298507463,"The inputs to the network are 1024 dimensional word embeddings, input to a transformer layer of
dimension 1024 and feed forward dimension of 2048."
REFERENCES,0.9641791044776119,"Both of the networks have 8 attention heads with head dimension of 128. The total transformer
layer size of 8 × 128 = 1024 is equally divided among the four mechanisms. In the case of TIMs,
these mechanisms (in layers 3,4,5) interact via pair-wise communication, whereas in TIMs+SSW
and TIMs+HSW, these mechanisms interact via a shared workspace. The shared workspace has 2
memory slots, each 1024 dimensional, having 4 attention heads for reading and writing."
REFERENCES,0.9671641791044776,"Parameter
Value"
REFERENCES,0.9701492537313433,Common Parameters
REFERENCES,0.9731343283582089,"Optimizer
Adam(Kingma and Ba, 2014)
Learning rate
5 · 10−4
Adam betas
0.99, 0.98
Weight decay
0.01
lr scheduler
‘inverse square root’
Max tokens per gpu
3078
Batch size multiple
8
Number of attention heads
8
Transformer layer size
1024
Number of Mechanisms
4
Update frequency
4
Number of warmup updates
4000
Starting Warmup lr
1 · 10−7"
REFERENCES,0.9761194029850746,TIMs+SSW
REFERENCES,0.9791044776119403,"Number of memory slots
2
Size of memory slots
1024
Memory Attention Heads
4
Gate Style
’unit’
Number of MLP layers in Attention
3
top-k competition
False"
REFERENCES,0.982089552238806,TIMs+HSW
REFERENCES,0.9850746268656716,"Number of memory slots
2
Size of memory slots
1024
Memory Attention Heads
4
Gate Style
’unit’
Number of MLP layers in Attention
3
top-k competition
True, k=2"
REFERENCES,0.9880597014925373,Table 6: Hyperparameters for WikiText-103 Language Modeling Task
REFERENCES,0.991044776119403,"Results. We plot the perplexity (per epoch) on the validation set. All models have comparable
number of parameters (within a 10% difference). We note that TIMs performs poorly on this dataset
but adding shared workspace improves the performance consistently. We also note that sparsity
indeed helps as TIMs+HSW performed the best."
REFERENCES,0.9940298507462687,Published as a conference paper at ICLR 2022.
REFERENCES,0.9970149253731343,"Figure 11: Per epoch validation perplexity for TIMs, TIMs+SSW, TIMs+HSW for wikitext-103
language modeling task"
