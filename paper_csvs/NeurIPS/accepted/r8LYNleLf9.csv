Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.004405286343612335,"Quantization is an effective way to compress neural networks. By reducing the
bit width of the parameters, the processing efﬁciency of neural network models at
edge devices can be notably improved. Most conventional quantization methods
utilize real datasets to optimize quantization parameters and ﬁne-tune. Due to
the inevitable privacy and security issues of real samples, the existing real-data-
driven methods are no longer applicable. Thus, a natural method is to introduce
synthetic samples for zero-shot quantization (ZSQ). However, the conventional
synthetic samples fail to retain the detailed texture feature distributions, which
severely limits the knowledge transfer and performance of the quantized model.
In this paper, a novel ZSQ method, TexQ is proposed to address this issue. We
ﬁrst synthesize a calibration image and extract its calibration center for each class
with a texture feature energy distribution calibration method. Then, the calibration
centers are used to guide the generator to synthesize samples. Finally, the mixup
knowledge distillation module is introduced to diversify the synthetic samples for
ﬁne-tuning. Extensive experiments on CIFAR10/100 and ImageNet show that
TexQ is observed to perform state-of-the-art in low bit width quantization. For
example, when ResNet-18 is quantized to 3-bit, TexQ achieves a 12.18% top-1
accuracy increase on ImageNet compared to state-of-the-art methods."
INTRODUCTION,0.00881057268722467,"1
Introduction"
INTRODUCTION,0.013215859030837005,"Limited by the computing capability of edge devices such as mobile phones, deep neural network
models inevitably require compression for terminal scenarios [1, 2]. Compared to popular com-
pressing methods such as pruning and distillation, quantization is easier to achieve lightweight and
hardware compatibility [3, 4, 5, 6]. By compressing ﬂoating-point parameters to low-bit ﬁxed-point
integer, quantization reduces the resource footprint and improves computing efﬁciency. For exam-
ple, when moving the parameters tensor (weights and activation) from 32 to 4 bits, the memory
consumption is reduced by a factor of 8, while the computational overhead of matrix multiplication
goes down by a factor of 64 at a squared rate. Most research on quantization focuses on quantization-
aware training (QAT) [7, 8, 9, 10] and post-training quantization (PTQ) [11, 12, 13, 14, 15, 16]. QAT
introduces fake quantization nodes when training and achieves performance close to full precision.
However, it relies on the full training set and lacks a uniform speciﬁcation, making it unfriendly
to deploy. Thus, PTQ has gained attention, which only requires a small part of the training set to
statistically optimize the quantization parameters [17, 18, 19, 20]."
INTRODUCTION,0.01762114537444934,"Due to privacy and security constraints, access to certain real data might be prohibited [21], such
as patient medical images, conﬁdential business information, etc. Therefore, zero-shot quantization"
INTRODUCTION,0.022026431718061675,∗Corresponding author
INTRODUCTION,0.02643171806167401,"(ZSQ) [22, 23, 21, 24, 25] is proposed to circumvent this limitations. Among previous studies, syn-
thetic sample-based methods [21, 24, 25] have attracted attention by their excellent performance.
Most studies synthesize samples that resemble the distribution of the real samples by extracting
statistics in the full precision model, such as batch-normalization statistics [21], categorical labels
[24], intermediate features [25], etc. However, despite a good ﬁt to the model statistics and high
classiﬁcation conﬁdence, there is still a huge gap between the texture feature distribution they con-
tain and that of real samples. Such a phenomenon is likely to result in bad performance, as studies
show that CNNs rely on texture feature to make decisions, for texture feature is ""easy to learn"" for
CNNs [26, 27, 28]. In addition, the conventional multiple-constraint paradigm limits the diversity
of synthetic samples [29, 30]."
INTRODUCTION,0.030837004405286344,"IntraQ
Real sample
Ours"
INTRODUCTION,0.03524229074889868,"Figure 1: LBP feature cluster-
ing of samples from CIFAR10,
IntraQ and our method. Visual-
ization with t-SNE [31]"
INTRODUCTION,0.039647577092511016,"To tackle the above problems, the following exploratory experi-
ments were carried out. Firstly, local binary pattern (LBP) [32]
was introduced to characterize the texture feature. A batch of real
samples and conventional synthetic samples with class conﬁdence
above 99.9% were extracted their LBP feature for clustering. As
presented in Figure 1, the domain gap (point distribution) and di-
versity gap (point density) can be observed. We further explored
the texture bias of the quantized model to prove the importance
of texture feature. As shown in Figure 2, same model was re-
spectively quantized with real samples and conventional synthetic
samples, and tested in four image patterns: original, greyscale
(removes color feature), binary (removes color and texture fea-
ture), and edge (retains shape feature and restores a few texture
feature). It illustrates that removing texture feature causes a sharp
performance decrease. The model quantized with synthetic sam-
ples drops off a lot on performance while restoring some of the
texture feature in Figure 2d can recover its performance. These
experiments imply that the ZSQ model is strongly biased towards
texture feature and tends to make decisions with it. Inspired by the experiments, an intuitive idea
is to calibrate the synthetic samples to retain the texture feature distribution, which facilitates the
model to learn and improve its performance. 35.43 33.01"
INTRODUCTION,0.04405286343612335,"77.03
79.62 67.03 69.08 34.65 28.19"
INTRODUCTION,0.048458149779735685,"(a) Original 
(b) Greyscale          
(c) Binary            
(d) Edge"
INTRODUCTION,0.05286343612334802,"Top-1 Accuracy (%)
Real samples               Synthetic samples"
INTRODUCTION,0.05726872246696035,Figure 2: Test results of real/synthetic samples quantized model on CIFAR10 in 4 image patterns.
INTRODUCTION,0.06167400881057269,"In this work, we propose TexQ, a zero-shot quantization method using synthetic calibration centers
to calibrate samples and retain texture feature distributions. We ﬁrst generate a calibration images
and its center for each class. Further, the centers guide the generator to synthesize samples. The
two-stage synthetic sample paradigm alleviates the homogeneity caused by multiple constraints.
Finally, mixup knowledge distillation is introduced to enhance sample diversity and avoid overﬁtting.
With the above methods, the proposed TexQ performs state-of-the-art on various datasets and model
settings."
RELATED WORK,0.06607929515418502,"2
Related work"
TEXTURE FEATURE EXTRACTION,0.07048458149779736,"2.1
Texture feature extraction"
TEXTURE FEATURE EXTRACTION,0.07488986784140969,"Texture feature is powerful visual cue that provides useful information in identifying objects or
regions of interest in images [33]. To make use of these cues, texture feature extraction methods"
TEXTURE FEATURE EXTRACTION,0.07929515418502203,"has been widely used in natural image processing, among which ﬁltering approaches are proven to
be successful, including LAWS ﬁlters, dyadic Gabor ﬁlter banks, wavelet transforms and so on [34].
Studies [35, 36, 37] have shown that image texture feature are conductive to image classiﬁcation
and are class separable. For instance, [38] introduce GLCM and Gabor texture feature from regions
of interest for better mammogram classiﬁcation. Recently, many studies [39, 40, 41] identify the
importance of textures feature for CNNs. [39] found that texture representations could capture
the statistical characteristics of images for CNNs. [40] showed that classic CNNs were unable to
recognize sketches where textures are missing and shapes are left. Similarly, [41, 42] validated that
CNNs were biased towards textures than shapes, for example, ResNet-50 biased 77.9% texture."
ZERO-SHOT QUANTIZATION,0.08370044052863436,"2.2
Zero-shot quantization"
ZERO-SHOT QUANTIZATION,0.0881057268722467,"Most zero-shot quantization (also called data-free quantization) studies try to recover the quantiza-
tion error from three perspectives. The ﬁrst perspective designs quantization parameters by using
model properties [22, 23, 43, 44] without acquiring any data. For instance, Nagel et al. [22] pro-
posed a scale-equivariance property of activation functions to equalize the weight ranges of the
network. Meller et al. [44] highlighted an inversely proportional factorization of convolutional
neural networks to decrease the degradation caused by quantization. The above methods avoid the
access to data but suffer from severe performance degradation in low bit widths. For example, DFQ
achieved a 0.11% top-1 accuracy in the 4-bit MobileNetV2 case [22, 24]. Therefore, more ZSQ
methods resort to synthetic samples. The second perspective adopts optimization-based methods to
synthesize samples by aligning the statistics in the full-precision network [21, 30, 29]. ZeroQ [21]
adopted batch normalization statistics alignment to optimize the standard Gaussian noise. DSG [30]
proposed a slack distribution alignment to diversify samples. IntraQ [29] highlighted the intra-class
heterogeneity and retained this property in the synthetic samples for better performance. The third
perspective adopts a generator to synthesize samples [24, 45, 46, 47, 48]. For example, GDFQ [24]
proposed a knowledge-matching generator to produce synthetic data with labels by introducing the
cross-entropy loss. ClusterQ [45] utilized the feature distribution alignment to imitate the distribu-
tion of real data. AdaDFQ [47] proposed a zero-sum game to adaptively regulate the adaptability of
synthetic samples. The existing studies provide us effective method to ﬁt statistics in full-precision
models, however, none of these considered texture feature distribution in synthetic samples, though
it is fundamental for CNNs to learn and make decisions. To the extent of our knowledge, this work
is the ﬁrst to consider texture feature in ZSQ, introducing a synthetic calibration center to calibrate
synthetic samples."
METHODOLOGY,0.09251101321585903,"3
Methodology"
PRELIMINARIES,0.09691629955947137,"3.1
Preliminaries"
QUANTIZER,0.1013215859030837,"3.1.1
Quantizer"
QUANTIZER,0.10572687224669604,"Following ZeroQ [21], asymmetric quantization is adopted. Given a ﬂoating-point value xf (weights
or activations) and quantization bit width BW, the quantized integer xq can be obtained as:"
QUANTIZER,0.11013215859030837,"xq = Clip(⌊xf · S −ZP⌉, 0, 2BW −1)
(1)"
QUANTIZER,0.1145374449339207,"where S =
2BW −1
xf max−xf min is the scaling factor mapping the ﬂoating-point number to a ﬁxed-point
integer, ZP = xf min · S is the zero point mapping the ﬂoating-point minimum to zero, ⌊input⌉
rounds its input to the nearest integer. Clip(tensor, rmin, rmax) clamps the tensor elements to be
between rmin and rmax. The dequantized value xd can be obtained as:"
QUANTIZER,0.11894273127753303,xd = xq + ZP
QUANTIZER,0.12334801762114538,"S
.
(2)"
DATA SYNTHESIS,0.1277533039647577,"3.1.2
Data synthesis"
DATA SYNTHESIS,0.13215859030837004,"PTQ requires a small real dataset D = {(x, y)} containing samples x and labels y. Similarly, most
ZSQ introduce synthetic sample sets D = {(x, y)}. To ﬁt the batch normalization statistics (BNS)
in the full-precision model (F), a basic principle [21, 45, 29] is using BNS alignment loss in Eq. 3."
DATA SYNTHESIS,0.13656387665198239,"LBNS = L
X l=1"
DATA SYNTHESIS,0.14096916299559473,"µl(x) −µl
F 
2 +
σl(x) −σl
F 
2,
(3)"
DATA SYNTHESIS,0.14537444933920704,"where µlF and σlF are the running mean and variance stored in the l-th BN layer of F. The mean
and variance of the synthetic sample batch x in the l-th layer of F are given by µl(x) and σl(x)."
DATA SYNTHESIS,0.14977973568281938,"Given generator G, standard Gaussian noise z, and a target label y, the synthetic sample can be
obtained as x = G (z|y). The cross-entropy loss in Eq. 4 is used to generate label-oriented samples
[24, 19]."
DATA SYNTHESIS,0.15418502202643172,"LCE = E(x,y)∼{(¯x,¯y)} [Cross-entropy (F(¯x), ¯y)] .
(4)"
TEXTURE FEATURE DISTRIBUTION CALIBRATION,0.15859030837004406,"3.2
Texture feature distribution calibration"
TEXTURE FEATURE DISTRIBUTION CALIBRATION,0.16299559471365638,"In this section, the calibration method and mixup knowledge distillation module are proposed. The
framework of our TexQ is illustrated in Figure 3."
TEXTURE FEATURE DISTRIBUTION CALIBRATION,0.16740088105726872,"MKD
(Only for knowledge"
TEXTURE FEATURE DISTRIBUTION CALIBRATION,0.17180616740088106,distillation)
TEXTURE FEATURE DISTRIBUTION CALIBRATION,0.1762114537444934,"F
ො𝑥
ො𝑦 𝓛 C"
TEXTURE FEATURE DISTRIBUTION CALIBRATION,0.18061674008810572,"LAWS
𝓛 C L-BNS 𝓛 C CE G
ത𝑦 𝑧"
TEXTURE FEATURE DISTRIBUTION CALIBRATION,0.18502202643171806,"ҧ𝑥
𝓛 Q KD 𝓛 G CE"
TEXTURE FEATURE DISTRIBUTION CALIBRATION,0.1894273127753304,Calibration Center Q F 𝓛 G BNS ҧ𝑥mix 𝓛 G
TEXTURE FEATURE DISTRIBUTION CALIBRATION,0.19383259911894274,"L-BNS , 𝓛 G D-BNS"
TEXTURE FEATURE DISTRIBUTION CALIBRATION,0.19823788546255505,Calibration sample
TEXTURE FEATURE DISTRIBUTION CALIBRATION,0.2026431718061674,Synthetic sample
TEXTURE FEATURE DISTRIBUTION CALIBRATION,0.20704845814977973,Gaussian noise
TEXTURE FEATURE DISTRIBUTION CALIBRATION,0.21145374449339208,"G
Generator
F
Full-precision network
Q
Quantized network
𝓛 G / 𝓛 Q Loss for G / Q"
TEXTURE FEATURE DISTRIBUTION CALIBRATION,0.21585903083700442,"Forward propagation
Backward gradient 
Loss calculation"
TEXTURE FEATURE DISTRIBUTION CALIBRATION,0.22026431718061673,"𝓛 CE
𝓛 BNS
𝓛 L-BNS
𝓛 D-BNS
𝓛 LAW
MKD"
TEXTURE FEATURE DISTRIBUTION CALIBRATION,0.22466960352422907,"Cross-entropy loss
BNS alignment loss
Layered BNS alignment loss
BNS-distorted loss
Texture feature energy distribution calibration loss
Mixup knowledge distillation module 𝓛 Q CE"
TEXTURE FEATURE DISTRIBUTION CALIBRATION,0.2290748898678414,"Figure 3: An overview of the proposed framework TexQ. Calibration centers are produced to guide
G to synthesize samples. Q is ﬁne-tuned with both calibration samples and synthetic samples."
TEXTURE FEATURE ENERGY DISTRIBUTION CALIBRATION LOSS,0.23348017621145375,"3.2.1
Texture feature energy distribution calibration loss"
TEXTURE FEATURE ENERGY DISTRIBUTION CALIBRATION LOSS,0.23788546255506607,"To quantitatively measure the texture feature distribution, LAWS texture feature energy [49, 50] is
introduced. We select four 1-dimensional ﬁlters provided by LAWS: E5=[-1, -2, 0, 2, 1], S5=[-1, 0,
2, 0, -1], W5=[-1, 2, 0, -2, 1], R5=[1, -4, 6, -4, 1], which stand for edge, spot, wave, and ripple. By
convolving two 1-dimensional LAWS ﬁlters of size 1×5 with each other, a total of 16 2-dimensional
ﬁlters of size 5×5 can be obtained. Each ﬁlter extracts a basic element of the texture, for example,
R5R5 denotes a high-frequency point ﬁlter, and E5S5 denotes a V shape ﬁlter. To be speciﬁc, the
calculation procedure of R5R5 is given as an example in Eq. 5."
TEXTURE FEATURE ENERGY DISTRIBUTION CALIBRATION LOSS,0.2422907488986784,"R5R5 = R5
T ∗R5 =  "
TEXTURE FEATURE ENERGY DISTRIBUTION CALIBRATION LOSS,0.24669603524229075,"1
-4
6
-4
1 "
TEXTURE FEATURE ENERGY DISTRIBUTION CALIBRATION LOSS,0.2511013215859031,"∗[ 1
-4
6
-4
1 ] =  "
TEXTURE FEATURE ENERGY DISTRIBUTION CALIBRATION LOSS,0.2555066079295154,"1
-4
6
-4
1
-4
16
-24
16
-4
6
-24
36
-24
6
-4
16
-24
16
-4
1
-4
6
-4
1 "
TEXTURE FEATURE ENERGY DISTRIBUTION CALIBRATION LOSS,0.2599118942731278,"
(5)"
TEXTURE FEATURE ENERGY DISTRIBUTION CALIBRATION LOSS,0.2643171806167401,"Original    
E5E5 (Edge) R5R5 (High frequency) E5S5 (V shape) √
√ √
√
√ √
Cat Tree Cloth"
TEXTURE FEATURE ENERGY DISTRIBUTION CALIBRATION LOSS,0.2687224669603524,Dominant √
TEXTURE FEATURE ENERGY DISTRIBUTION CALIBRATION LOSS,0.27312775330396477,Figure 4: Features extracted with LAWS ﬁlters.
TEXTURE FEATURE ENERGY DISTRIBUTION CALIBRATION LOSS,0.2775330396475771,Proportion (%)
TEXTURE FEATURE ENERGY DISTRIBUTION CALIBRATION LOSS,0.28193832599118945,Texture elements
TEXTURE FEATURE ENERGY DISTRIBUTION CALIBRATION LOSS,0.28634361233480177,"Figure 5: Visualization of distribu-
tion T from different samples."
TEXTURE FEATURE ENERGY DISTRIBUTION CALIBRATION LOSS,0.2907488986784141,"(a) ImageNet              (b) Synthetic sample (IntraQ)   (c) Calibration sample (Ours)  (d) Synthetic sample (Ours) 
Number of dominant texture elements"
TEXTURE FEATURE ENERGY DISTRIBUTION CALIBRATION LOSS,0.29515418502202645,Proportion (%)
TEXTURE FEATURE ENERGY DISTRIBUTION CALIBRATION LOSS,0.29955947136563876,"Figure 6: Dominant texture elements number distribution of samples from the ImageNet training set,
IntraQ synthetic samples, calibration images as well as synthetic samples generated by our method.
Synthetic samples were obtained with pre-trained ResNet-18."
TEXTURE FEATURE ENERGY DISTRIBUTION CALIBRATION LOSS,0.3039647577092511,"Straightforward visualization of the texture feature extracted by LAWS texture feature ﬁlters is pro-
vided in Figure 4. It can be observed that different types of images contain different dominant
texture feature. For example, the trees with lots of leaves including V-shape, high-frequency points
and edge features, and the cat lying on the chair including mainly V-shape and edge features."
TEXTURE FEATURE ENERGY DISTRIBUTION CALIBRATION LOSS,0.30837004405286345,"It is reasonable to evaluate texture distributions by analyzing basic elements, for complex texture
feature are composed of simple elements. We apply 16 ﬁlters to obtain the texture feature energy
distributions T of a sample as shown in Figure 5. Assuming that the top-k elements in T account for
more than 80% are the dominant texture elements, the set of these k elements is recorded as K. As
displayed in Figure 6, the real samples (Figure 6a) have at most 9 dominant texture elements, i.e., k
≤9, while there is almost only 1 in the conventional synthetic samples (Figure 6b). To be detailed,
in Figure 5, the distribution curve of the conventional synthetic sample presents a clear peak, indi-
cating a single dominant element, which reduces the intra-class heterogeneity and the information
contained. Unlike this, the curve of the real sample presents a detailed texture distribution. To retain
this property, texture feature energy distribution calibration loss of Eq. 6 is proposed to dynamically
adjust the detailed texture feature distribution of calibration images for each class."
TEXTURE FEATURE ENERGY DISTRIBUTION CALIBRATION LOSS,0.31277533039647576,"LC
LAW S = max (∥Kmax −θU∥2 −ε, 0) + max (∥(1 −Kmax) −θL∥2 −ε, 0) ,
(6)"
TEXTURE FEATURE ENERGY DISTRIBUTION CALIBRATION LOSS,0.31718061674008813,"where Kmax is the largest element in K. The hyperparameters θU, θL, and ε control the upper and
lower bounds of the elements in K. Speciﬁcally, θU is the upper bound of Kmax, and θL is the lower
bound of the sum of all elements in K except Kmax. ε is the softness factor to alleviate convergence
problem. Both upper and lower bounds are used together to extract detailed distribution."
TEXTURE FEATURE ENERGY DISTRIBUTION CALIBRATION LOSS,0.32158590308370044,"To introduce the calibration loss, a possible scheme is directly applying constraint to the generator.
However, multiple constraints to generator results in slow iterations and homogeneous samples. We
adopt a two-stage method, which includes an optimization method to synthesize accurate calibra-
tion centers for each class and a generation method to synthesize samples with the constraint on
calibration centers."
LAYERED BNS ALIGNMENT CONSTRAINT,0.32599118942731276,"3.2.2
Layered BNS alignment constraint"
LAYERED BNS ALIGNMENT CONSTRAINT,0.3303964757709251,"Considering that neural networks extract class-independent low-level features (e.g., texture and con-
tours) in shallow layers and class-related high-level features in deep layers, the layered BNS align-
ment loss (LL-BNS) is proposed. We adopt loose constraints in shallow layers to facilitate the
expression of texture feature and tightened constraints in deep layers to ﬁt class-related information."
LAYERED BNS ALIGNMENT CONSTRAINT,0.33480176211453744,"In calibration set generation stage, given the calibration set C = {(bx, by)}, the LCL-BNS of Eq. 7
constraints C to align the BNS in F."
LAYERED BNS ALIGNMENT CONSTRAINT,0.3392070484581498,"LCL-BNS =
L
P l=1"
LAYERED BNS ALIGNMENT CONSTRAINT,0.3436123348017621,"wl · (µl( bxc) −µlF )

2 +
wl ·
 
σl( bxc) −σlF 
2,"
LAYERED BNS ALIGNMENT CONSTRAINT,0.34801762114537443,"wl =

0.2, 1 ≤l <
 L"
LAYERED BNS ALIGNMENT CONSTRAINT,0.3524229074889868,"2

−2
1.1,
 L"
LAYERED BNS ALIGNMENT CONSTRAINT,0.3568281938325991,"2

−2 ≤l ≤L ,
(7)"
LAYERED BNS ALIGNMENT CONSTRAINT,0.36123348017621143,"where L is the number of BN layers, µl( bxc) and σl( bxc) denote the mean and variance of C in the
l-th layer of F. wl is the loss weight for the l-th layer. ⌈input⌉returns the smallest integer greater
than its input."
LAYERED BNS ALIGNMENT CONSTRAINT,0.3656387665198238,"In synthetic sample generation stage, LGL-BNS in Eq. 8 constrains the generator to synthesize
samples that ﬁt the calibration centers of corresponding class label k."
LAYERED BNS ALIGNMENT CONSTRAINT,0.3700440528634361,"LGL-BNS =
L
P l=1"
LAYERED BNS ALIGNMENT CONSTRAINT,0.3744493392070485,"wl ·

µl (x|y = k) −µlC ( bxc| byc = k)

2"
LAYERED BNS ALIGNMENT CONSTRAINT,0.3788546255506608,"+
wl ·

σl (x|y = k) −σlC ( bxc| byc = k)

2, wl =

0, 1 ≤l <
 L"
LAYERED BNS ALIGNMENT CONSTRAINT,0.3832599118942731,"2

−2
1,
 L"
LAYERED BNS ALIGNMENT CONSTRAINT,0.3876651982378855,"2

−2 ≤l ≤L ,
(8)"
LAYERED BNS ALIGNMENT CONSTRAINT,0.3920704845814978,"where µl (x|y = k) and σl (x|y = k) denote the mean and variance of the synthetic samples.
µlC ( bxc| byc = k) and σlC ( bxc| byc = k) are the corresponding mean and variance calibration center."
MIXUP KNOWLEDGE DISTILLATION MODULE,0.3964757709251101,"3.2.3
Mixup knowledge distillation module"
MIXUP KNOWLEDGE DISTILLATION MODULE,0.4008810572687225,"Knowledge distillation is commonly used to transfer the output distribution from full-precision
model F to the quantized model Q in the ﬁne-tuning stage, which requires diversiﬁed samples.
Some studies introduce Mixup [51, 48] augmentation to generate samples with mixed labels to ﬁne-
tune the quantized model with cross-entropy loss. However, the labels produced by the such methods
are not accurate enough, and taking such mixed labels to ﬁne-tune the quantized model tends to be
disastrously misleading. Therefore, we advocate discarding mixed labels and taking only mixed
samples to knowledge distillate the quantized model, and name this method mixup knowledge dis-
tillation. Speciﬁcally, as presented in Eq. 9, new sample xmix is generated by weighted fusion of
2 samples xi and xj randomly selected in a batch for distillation. Mixup proportion λ is sampled
from a standard uniform distribution. The probability p = 20% is observed to perform best."
MIXUP KNOWLEDGE DISTILLATION MODULE,0.4052863436123348,"xmix =
λxi + (1 −λ)xj, with probability p"
MIXUP KNOWLEDGE DISTILLATION MODULE,0.40969162995594716,"xi, with probability 1 −p
, λ ∼U (0, 1)
(9)"
QUANTIZATION PROCESS,0.41409691629955947,"3.3
Quantization process"
QUANTIZATION PROCESS,0.4185022026431718,"3.3.1
Step1: Calibration set generation"
QUANTIZATION PROCESS,0.42290748898678415,"In this stage, the calibration set with Gaussian noise initialization is optimized with the loss in Eq.
10, capturing the preferred texture of each class from F. We ﬁx F and back-propagate the loss to
the calibration images. For each class, the calibration image is trained for at most 1500 iterations
individually, with a warm-up of 150 iterations at half learning rate without LCLAW S. Their BNS
are extracted as the calibration centers to guide the generator in the next step."
QUANTIZATION PROCESS,0.42731277533039647,"LC = LC
LAW S + α1 · LC
L-BNS + α2 · LC
CE
(10)"
QUANTIZATION PROCESS,0.43171806167400884,"3.3.2
Step2: Synthetic samples generation"
QUANTIZATION PROCESS,0.43612334801762115,"In this stage, generator G synthesize samples of different classes that ﬁt corresponding calibration
centers via LGL-BNS. Following FDDA [19], BNS-distorted loss in Eq. 11 is introduced to avoid
overﬁtting by Gaussian noise interfering with the calibration center. νµ = 0.5 and νσ = 1.0 control
the distortion degree of mean and variance."
QUANTIZATION PROCESS,0.44052863436123346,"LGD-BNS =
L
P l=1"
QUANTIZATION PROCESS,0.44493392070484583,"wl ·

µl (x|y = k) −N
 
µlC ( bxc| byc = k) , νµ

2"
QUANTIZATION PROCESS,0.44933920704845814,"+
wl ·

σl (x|y = k) −N
 
σlC ( bxc| byc = k) , νσ

2, wl =

0, 1 ≤l <
 L"
QUANTIZATION PROCESS,0.45374449339207046,"2

−2
1,
 L"
QUANTIZATION PROCESS,0.4581497797356828,"2

−2 ≤l ≤L .
(11)"
QUANTIZATION PROCESS,0.46255506607929514,"In addition, LGCE and LGBNS are used to align the label and BNS of F. Up to this point, the total
loss employed by generator G can be summarized in Eq. 12."
QUANTIZATION PROCESS,0.4669603524229075,"LG = LG
CE + α3 · LG
BNS + α4 · LG
L-BNS + α5 · LG
D-BNS.
(12)"
QUANTIZATION PROCESS,0.4713656387665198,"3.3.3
Step3: Quantized model ﬁne-tuning"
QUANTIZATION PROCESS,0.47577092511013214,"To make full use of the data, we take both samples in D and C as the input of Q and apply
cross-entropy loss LQCE to ﬁne-tune. Subsequently, both samples in D and C are input into the
mixup knowledge distillation module to produce mixup samples. With the input of mixup samples,
Kullback-Leibler loss in Eq. 13 is applied to transfer the output of F to Q."
QUANTIZATION PROCESS,0.4801762114537445,"LQ
KD = E(x,y)∼C∪D [Kullback-Leibler (Q(xmix), F(xmix))] .
(13)"
QUANTIZATION PROCESS,0.4845814977973568,"At this point, the overall loss for ﬁne-tuning Q can be summarized as:"
QUANTIZATION PROCESS,0.4889867841409692,"LQ = LQ
CE + α6 · LQ
KD
(14)"
EXPERIMENT,0.4933920704845815,"4
Experiment"
EXPERIMENTAL SETTINGS AND DETAILS,0.4977973568281938,"4.1
Experimental settings and details"
EXPERIMENTAL SETTINGS AND DETAILS,0.5022026431718062,"We report top-1 accuracy on validation sets of CIFAR-10/100 [52] and ImageNet [53]. Networks
selected include ResNet-20 [54] for CIFAR-10/100, ResNet-18, MobileNetV2 [55] and ResNet-50
for ImageNet. All experiments are implemented with Pytorch [56] via the code of FDDA [19] and
IntraQ [29], and run on an NVIDIA GeForce RTX 3090 GPU. Calibration images are iterated with
a constant learning rate of 0.05. Generator is imported from GDFQ [24] with a initial learning rate
of 1e-3 multiplied by 0.1 every 100 epochs. In ﬁne-tuning, batchsize is 128 for CIFAR-10/100 and
16 for ImageNet, adjusting by cosine annealing [57]. We warm up G for 50 epochs, then update G
and Q for 450 epochs. The optimal conﬁgurations on trade-off parameters from α1 to α6 obtained
by grid search are 2, 10, 0.4, 0.02, 1.8, and 20."
PERFORMANCE COMPARISON,0.5066079295154186,"4.2
Performance comparison"
PERFORMANCE COMPARISON,0.5110132158590308,"To demonstrate the efﬁcacy of our TexQ, we conduct experiment in 3/4-bit case, since high accuracy
can be easily achieved with a larger bit width. For instance, the advanced AdaDFQ [47] trails full
precision with a 0.08% top-1 accuracy in 5-bit ResNet-20 case. In this section, WBAB indicates the
weights and activations are quantized to B-bit. Best results in boldface."
PERFORMANCE COMPARISON,0.5154185022026432,"4.2.1
CIFAR-10/100"
PERFORMANCE COMPARISON,0.5198237885462555,"We compare the performance against the advanced ZSQ methods on CIFAR-10/100. As presented in
Table 1, our TexQ is observed to achieve state-of-the-art among the competitors, improving the top-1
accuracy by 3.13%/1.58% in 3-bit CIFAR-10/100 case compared to the advanced AdaDFQ. Similar"
PERFORMANCE COMPARISON,0.5242290748898678,"results can be observed in the 4-bit case. In particular, the top-1 accuracy of our TexQ exceeds that
of the same framework with real data in 4-bit case, demonstrating that the proposed TexQ can fully
extract the feature distribution in simple datasets."
PERFORMANCE COMPARISON,0.5286343612334802,Table 1: Results of ResNet-20 on CIFAR-10/100.
PERFORMANCE COMPARISON,0.5330396475770925,"Bit width
Method
Top-1 Accuracy(%)"
PERFORMANCE COMPARISON,0.5374449339207048,"CIFAR-10
CIFAR-100"
PERFORMANCE COMPARISON,0.5418502202643172,"Full-precision
93.89
70.33"
PERFORMANCE COMPARISON,0.5462555066079295,"Real data
91.52
66.80"
PERFORMANCE COMPARISON,0.5506607929515418,"GDFQ [24] (ECCV 2020)
90.11
63.75
ARC [58] (IJCAI 2021)
88.55
62.76
Qimera [48] (NeurIPS 2021)
91.26
65.10
W4A4
IntraQ [29] (CVPR 2022)
91.49
64.98
ARC+AIT [59] (CVPR 2022)
90.49
61.05
AdaSG [46] (AAAI 2023)
92.10
66.42
AdaDFQ [47] (CVPR 2023)
92.31
66.81
TexQ (Ours)
92.68
67.18"
PERFORMANCE COMPARISON,0.5550660792951542,"GDFQ [24, 47] (ECCV 2020)
75.11
47.61
ARC [58] (IJCAI 2021)
-
40.15
Qimera [48, 47] (NeurIPS 2021)
74.43
46.13"
PERFORMANCE COMPARISON,0.5594713656387665,"W3A3
IntraQ [29] (CVPR 2022)
77.07
48.25
ARC+AIT [59] (CVPR 2022)
-
41.34
AdaSG [46] (AAAI 2023)
84.14
52.76
AdaDFQ [47] (CVPR 2023)
84.89
52.74
TexQ (Ours)
86.47
55.87"
IMAGENET,0.5638766519823789,"4.2.2
ImageNet"
IMAGENET,0.5682819383259912,"We further compare with competitors on challenging ImageNet, the results are presented in Table 2."
IMAGENET,0.5726872246696035,"Table 2: Results of ResNet-18, MobileNetV2 and ResNet-50 on ImageNet."
IMAGENET,0.5770925110132159,"Bit width
Method
Top-1 Accuracy(%)"
IMAGENET,0.5814977973568282,"ResNet-18
MobileNetV2
ResNet-50"
IMAGENET,0.5859030837004405,"Full-precision
71.47
72.49
77.73"
IMAGENET,0.5903083700440529,"GDFQ [24] (ECCV 2020)
60.60
59.43
54.16
ZAQ [25] (CVPR 2021)
52.64
0.10
53.02
ARC [58] (IJCAI 2021)
61.32
60.13
64.37
Qimera [48] (NeurIPS 2021)
63.84
61.62
66.25
W4A4
IntraQ [29] (CVPR 2022)
66.47
65.10
-
ARC+AIT [59] (CVPR 2022)
65.73
66.47
68.27
AdaSG [46] (AAAI 2023)
66.50
65.15
68.58
AdaDFQ [47] (CVPR 2023)
66.53
65.41
68.38
TexQ (Ours)
67.73
67.07
70.72"
IMAGENET,0.5947136563876652,"GDFQ [24, 47] (ECCV 2020)
20.23
1.46
0.31
ARC [58] (IJCAI 2021)
23.37
14.30
1.63"
IMAGENET,0.5991189427312775,"W3A3
Qimera [48, 47] (NeurIPS 2021)
1.17
-
-
AdaSG [46] (AAAI 2023)
37.04
26.90
16.98
AdaDFQ [47] (CVPR 2023)
38.10
28.99
17.63
TexQ (Ours)
50.28
32.80
25.27"
IMAGENET,0.6035242290748899,"Bit width
For low bit width case, conventional ZSQ methods suffer from a huge accuracy loss,
while our TexQ performs good generalization capability. Speciﬁcally, for 4-bit ResNet-18 case,"
IMAGENET,0.6079295154185022,"TexQ achieving 67.73% accuracy, outperforming AdaDFQ by 1.2%. In 3-bit case, TexQ reaches
50.28% outstanding accuracy, outperforming AdaDFQ by 12.18%, with a standard deviation of
0.34 in 5 repeated experiments. Similar results were obtained in MobileNetV2."
IMAGENET,0.6123348017621145,"Network Size
Larger models are likely lead to poor performance with existing ZSQ methods,
especially in low bit width case. For example, in 3-bit case, Qimera [48, 47] performs well on
the small ResNet-20, while achieves only 1.17% accuracy on the larger ResNet-18. In contrast,
our TexQ achieved 55.87% ultra-low when quantizing ResNet-18 to 3-bit, and leads AdaDFQ with
7.64% on 3-bit ResNet50 case. These results again demonstrate the effectiveness of our method."
ABLATION STUDY,0.6167400881057269,"4.3
Ablation study"
ABLATION STUDY,0.6211453744493393,"Hyperparameters
After empirical initialization, the hyperparameters k, θU, θL, and ε are
searched for their optimal value with grid search by quantizing ResNet-18 to 3-bit on ImageNet,
as displayed in Figure 7. The optimal conﬁgurations are k=9, θU=0.3, θL=0.5, and ε=0.015. The
hyperparameter k represents the number of dominant texture elements, whose optimal value of k=9
is consistent with what we observe in the real samples. The sum of the optimal values of θU and
θL is 0.8, which ﬁts our assumptions about the proportion of dominant texture elements. To avoid
complex searches, these parameters were used for all experiments. While this may not be optimal
for all networks, it is sufﬁcient to exhibit the advanced performance of TexQ. 49.53 48.23 50.07 50.94 50.15 48.42 48.77 49.80 48.35 50.94"
ABLATION STUDY,0.6255506607929515,Top-1 accuracy (%) 48.78 47.66 50.02 50.94 50.94 47.81 50.40 50.94 48.87 49.39
ABLATION STUDY,0.6299559471365639,"(a)  k 
(b) θU
(c)  θL
(b)  ε"
ABLATION STUDY,0.6343612334801763,Figure 7: Inﬂuence of the hyperparameters.
ABLATION STUDY,0.6387665198237885,"Modules
Ablation on key modules including LCLAW S (Eq. 6), LCL-BNS (Eq. 7) and mixup
knowledge distillation (MKD, Eq. 9) is conducted. As presented in Table 3, dropping one or two of
them results in an accuracy loss. The largest accuracy loss (8.37%) is observed when both LCLAW S
and LCL-BNS are removed, which indicates their cooperative relationship: LCL-BNS loosens the
shallow layers constraint, facilitating LCLAW S to calibrate."
ABLATION STUDY,0.6431718061674009,Table 3: Ablations on modules. We report the top-1 accuracy of 3-bit ResNet-18 on ImageNet.
ABLATION STUDY,0.6475770925110133,"LCLAW S
LCL-BNS
MKD
Top-1 Accuracy(%)"
ABLATION STUDY,0.6519823788546255,"✓
✓
✓
50.94
✓
✓
49.03
✓
✓
43.31
✓
✓
49.63
✓
42.65
✓
49.00
✓
42.57"
DISCUSSION,0.6563876651982379,"5
Discussion"
DISCUSSION,0.6607929515418502,"5.1
Why does TexQ work?"
DISCUSSION,0.6651982378854625,"Mitigating the domain gap
Our synthetic samples with calibration match the real distribution
better, as presented in Figure 1 and Figure 6. Further, Figure 5 shows our synthetic sample has a
more detailed texture distribution similar to the real one."
DISCUSSION,0.6696035242290749,(a) ImageNet (IntraQ)   (b) ImageNet (Ours) (c) CIFAR10 (IntraQ)  (d) CIFAR10 (Ours) (e) CIFAR100 (IntraQ)  (f) CIFAR100 (Ours)
DISCUSSION,0.6740088105726872,"Figure 8: Visualization of synthetic samples. For ImageNet and CIFAR10/100, samples are obtained
with ResNet-18 and ResNet-20, respectively. The advanced IntraQ was selected as a competitor."
DISCUSSION,0.6784140969162996,(a) Without Calibration  (b) Introduce Calibration
DISCUSSION,0.6828193832599119,"Figure 9: Visualization of the
running mean of BNS in ResNet-20."
DISCUSSION,0.6872246696035242,"(a) ImageNet         
(b) Ours             
(c) IntraQ 1.0 0.8 0.6 0.4 0.2"
"COSINE
SIMILARITY",0.6916299559471366,"0.0
Cosine
similarity"
"COSINE
SIMILARITY",0.6960352422907489,"Figure 10: Cosine similarity confusion matrix of samples.
Synthetic samples are obtained with ResNet-18."
"COSINE
SIMILARITY",0.7004405286343612,"Increasing inter-class distance
As displayed in Figure 9a, the class centers of samples from con-
ventional methods is unclear, limiting the inter-class distance. The situation is improved by intro-
ducing calibration centers, as shown in Figure 9b."
"COSINE
SIMILARITY",0.7048458149779736,"Enhancing sample diversity
Figure 1 shows our synthetic samples scatter a lot compared with
IntraQ, implying the enhancement of sample diversity. To put it bluntly, synthetic images from
different methods are visualized in Figure 8, showing the improvement in color and texture diversity
of our samples. Further, the intra-class diversity is observed through cosine similarity confusion
matrix of samples in Figure 10, showing that our samples reach a similar diversity to the real ones."
LIMITATIONS AND FUTURE WORK,0.7092511013215859,"5.2
Limitations and future work"
LIMITATIONS AND FUTURE WORK,0.7136563876651982,"This work is built upon the concept of ""texture"", and the texture ﬁlters adopted are designed man-
ually. Thus, the application to other tasks with different modality would be limited. We envision
adding training process to the ﬁlters, which would generalize the calibration method proposed to
other modality. Low-bit quantization remains a challenge. For instance, TexQ achieve 70.72% ac-
curacy by quantizing ResNet-50 to 4-bit, while it drops to 25.27% in 3-bit case. The introduction of
advanced distillation methods holds promise for more better results."
CONCLUSION,0.7180616740088106,"6
Conclusion"
CONCLUSION,0.7224669603524229,"In this paper, we observe a non-negligible detailed texture distribution in the real samples. To retain
this property in synthetic samples, we introduce synthetic calibration images and centers to calibrate
the generator. To diversify the samples, mixup knowledge distillation module is introduce to create
diversiﬁed samples for ﬁne-tuning. Extensive experiments show our state-of-the-art performance on
mainstream networks and datasets, especially for low bit width quantization."
CONCLUSION,0.7268722466960352,Acknowledgment
CONCLUSION,0.7312775330396476,"This work was supported by the Science and Technology Research Program of Shenzhen City (No.
KCXFZ20201221173207022, No.WDZC2020200821141349001) and the Jilin Fuyuan Guan Food
Group Co., Ltd."
REFERENCES,0.73568281938326,References
REFERENCES,0.7400881057268722,"[1] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig
Adam, and Dmitry Kalenichenko.
Quantization and training of neural networks for efﬁcient integer-
arithmetic-only inference. In Proceedings of the IEEE conference on computer vision and pattern recog-
nition, pages 2704–2713, 2018."
REFERENCES,0.7444933920704846,"[2] Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A Horowitz, and William J Dally. Eie:
Efﬁcient inference engine on compressed deep neural network. ACM SIGARCH Computer Architecture
News, 44(3):243–254, 2016."
REFERENCES,0.748898678414097,"[3] Yu-Hsin Chen, Tien-Ju Yang, Joel Emer, and Vivienne Sze. Eyeriss v2: A ﬂexible accelerator for emerg-
ing deep neural networks on mobile devices. IEEE Journal on Emerging and Selected Topics in Circuits
and Systems, 9(2):292–308, 2019."
REFERENCES,0.7533039647577092,"[4] Lei Deng, Guoqi Li, Song Han, Luping Shi, and Yuan Xie. Model compression and hardware acceleration
for neural networks: A comprehensive survey. Proceedings of the IEEE, 108(4):485–532, 2020."
REFERENCES,0.7577092511013216,"[5] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with
pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015."
REFERENCES,0.762114537444934,"[6] Huan Wang, Suhas Lohit, Michael N Jones, and Yun Fu.
What makes a"" good"" data augmentation
in knowledge distillation-a statistical perspective. Advances in Neural Information Processing Systems,
35:13456–13469, 2022."
REFERENCES,0.7665198237885462,"[7] Dohyung Kim, Junghyup Lee, and Bumsub Ham. Distance-aware quantization. In Proceedings of the
IEEE/CVF International Conference on Computer Vision, pages 5271–5280, 2021."
REFERENCES,0.7709251101321586,"[8] Yury Nahshan, Brian Chmiel, Chaim Baskin, Evgenii Zheltonozhskii, Ron Banner, Alex M Bronstein,
and Avi Mendelson. Loss aware post-training quantization. Machine Learning, 110(11-12):3245–3262,
2021."
REFERENCES,0.775330396475771,"[9] Haoping Bai, Meng Cao, Ping Huang, and Jiulong Shan. Batchquant: Quantized-for-all architecture
search with robust quantizer. Advances in Neural Information Processing Systems, 34:1074–1085, 2021."
REFERENCES,0.7797356828193832,"[10] Sijie Zhao, Tao Yue, and Xuemei Hu. Distribution-aware adaptive multi-bit quantization. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9281–9290, 2021."
REFERENCES,0.7841409691629956,"[11] Yongkweon Jeon, Chungman Lee, Eulrang Cho, and Yeonju Ro. Mr. biq: Post-training non-uniform
quantization based on minimizing the reconstruction error. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 12329–12338, 2022."
REFERENCES,0.788546255506608,"[12] Alex Finkelstein, Ella Fuchs, Idan Tal, Mark Grobman, Niv Vosco, and Eldad Meller. Qft: Post-training
quantization via fast joint ﬁnetuning of all degrees of freedom. In Computer Vision–ECCV 2022 Work-
shops: Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part VII, pages 115–129. Springer, 2023."
REFERENCES,0.7929515418502202,"[13] Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry. Accurate post training quanti-
zation with small calibration sets. In International Conference on Machine Learning, pages 4466–4475.
PMLR, 2021."
REFERENCES,0.7973568281938326,"[14] Elias Frantar, Sidak Pal Singh, and Dan Alistarh. Optimal Brain Compression: a framework for accurate
post-training quantization and pruning. Advances in Neural Information Processing Systems, 36, 2022."
REFERENCES,0.801762114537445,"[15] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or
down? adaptive rounding for post-training quantization. In International Conference on Machine Learn-
ing, pages 7197–7206. PMLR, 2020."
REFERENCES,0.8061674008810573,"[16] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi
Gu.
Brecq: Pushing the limit of post-training quantization by block reconstruction.
arXiv preprint
arXiv:2102.05426, 2021."
REFERENCES,0.8105726872246696,"[17] Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart Van Baalen, and Tijmen
Blankevoort. A white paper on neural network quantization. arXiv preprint arXiv:2106.08295, 2021."
REFERENCES,0.8149779735682819,"[18] Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efﬁcient inference: A whitepa-
per. arXiv preprint arXiv:1806.08342, 2018."
REFERENCES,0.8193832599118943,"[19] Yunshan Zhong, Mingbao Lin, Mengzhao Chen, Ke Li, Yunhang Shen, Fei Chao, Yongjian Wu, and
Rongrong Ji. Fine-grained data distribution alignment for post-training quantization. In Computer Vision–
ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XI,
pages 70–86. Springer, 2022."
REFERENCES,0.8237885462555066,"[20] Xingchao Liu, Mao Ye, Dengyong Zhou, and Qiang Liu. Post-training quantization with multiple points:
Mixed precision without mixed precision. In Proceedings of the AAAI Conference on Artiﬁcial Intelli-
gence, volume 35, pages 8697–8705, 2021."
REFERENCES,0.8281938325991189,"[21] Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Zeroq:
A novel zero shot quantization framework. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 13169–13178, 2020."
REFERENCES,0.8325991189427313,"[22] Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. Data-free quantization through
weight equalization and bias correction. In Proceedings of the IEEE/CVF International Conference on
Computer Vision, pages 1325–1334, 2019."
REFERENCES,0.8370044052863436,"[23] Edouard Yvinec, Arnaud Dapogny, Matthieu Cord, and Kevin Bailly. Spiq: Data-free per-channel static
input quantization. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer
Vision, pages 3869–3878, 2023."
REFERENCES,0.8414096916299559,"[24] Xu Shoukai, Li Haokun, Zhuang Bohan, Liu Jing, Cao Jiezhang, Liang Chuangrun, and Tan Mingkui.
Generative low-bitwidth data free quantization. In The European Conference on Computer Vision, pages
1–17. Springer, 2020."
REFERENCES,0.8458149779735683,"[25] Yuang Liu, Wei Zhang, and Jun Wang.
Zero-shot adversarial quantization.
In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1512–1521, 2021."
REFERENCES,0.8502202643171806,"[26] Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and Wieland
Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and
robustness. In International Conference on Learning Representations, 2019."
REFERENCES,0.8546255506607929,"[27] Yingwei Li, Qihang Yu, Mingxing Tan, Jieru Mei, Peng Tang, Wei Shen, Alan Yuille, and Cihang Xie.
Shape-texture debiased neural network training. arXiv preprint arXiv:2010.05981, 2020."
REFERENCES,0.8590308370044053,"[28] Bo Gao and Michael W. Spratling. Shape-texture debiased training for robust template matching. SEN-
SORS, 22(17), SEP 2022."
REFERENCES,0.8634361233480177,"[29] Yunshan Zhong, Mingbao Lin, Gongrui Nan, Jianzhuang Liu, Baochang Zhang, Yonghong Tian, and
Rongrong Ji. Intraq: Learning synthetic images with intra-class heterogeneity for zero-shot network
quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pages 12339–12348, 2022."
REFERENCES,0.8678414096916299,"[30] Xiangguo Zhang, Haotong Qin, Yifu Ding, Ruihao Gong, Qinghua Yan, Renshuai Tao, Yuhang Li, Feng-
wei Yu, and Xianglong Liu. Diversifying sample generation for accurate data-free quantization. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15658–15667,
2021."
REFERENCES,0.8722466960352423,"[31] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning
research, 9(11), 2008."
REFERENCES,0.8766519823788547,"[32] Zhenhua Guo, Lei Zhang, and David Zhang. A completed modeling of local binary pattern operator for
texture classiﬁcation. IEEE transactions on image processing, 19(6):1657–1663, 2010."
REFERENCES,0.8810572687224669,"[33] Li Liu, Jie Chen, Paul Fieguth, Guoying Zhao, Rama Chellappa, and Matti Pietikäinen. From bow to cnn:
Two decades of texture representation for texture classiﬁcation. International Journal of Computer Vision,
127:74–109, 2019."
REFERENCES,0.8854625550660793,"[34] Trygve Randen and John Hakon Husoy. Filtering for texture classiﬁcation: A comparative study. IEEE
Transactions on pattern analysis and machine intelligence, 21(4):291–310, 1999."
REFERENCES,0.8898678414096917,"[35] Arden Sagiterry Setiawan, Julian Wesley, Yudy Purnama, et al. Mammogram classiﬁcation using law’s
texture energy measure and neural networks. Procedia Computer Science, 59:92–97, 2015."
REFERENCES,0.8942731277533039,"[36] Will D Gillett. Image classiﬁcation using laws’ texture energy measures. Technical report, Washington
University in St. Louis, 1987."
REFERENCES,0.8986784140969163,"[37] Yuwei Zhang, Peng Zhang, Chun Yuan, and Zhi Wang. Texture and shape biased two-stream networks
for clothing classiﬁcation and attribute recognition. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 13538–13547, 2020."
REFERENCES,0.9030837004405287,"[38] Taye Girma Debelee, Abrham Gebreselasie, Friedhelm Schwenker, Mohammadreza Amirian, and Dereje
Yohannes. Classiﬁcation of mammograms using texture and cnn based extracted features. Journal of
Biomimetics, Biomaterials and Biomedical Engineering, 42:79–97, 2019."
REFERENCES,0.9074889867841409,"[39] Leon Gatys, Alexander S Ecker, and Matthias Bethge.
Texture synthesis using convolutional neural
networks. Advances in neural information processing systems, 28, 2015."
REFERENCES,0.9118942731277533,"[40] Pedro Ballester and Ricardo Araujo. On the performance of googlenet and alexnet applied to sketches. In
Proceedings of the AAAI conference on artiﬁcial intelligence, volume 30, 2016."
REFERENCES,0.9162995594713657,"[41] Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and Wieland
Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and
robustness. arXiv preprint arXiv:1811.12231, 2018."
REFERENCES,0.920704845814978,"[42] Nikolai Kalischek, Rodrigo Caye Daudt, Torben Peters, Reinhard Furrer, Jan D Wegner, and Konrad
Schindler. Biasbed-rigorous texture bias evaluation. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 22221–22230, 2023."
REFERENCES,0.9251101321585903,"[43] Cong Guo, Yuxian Qiu, Jingwen Leng, Xiaotian Gao, Chen Zhang, Yunxin Liu, Fan Yang, Yuhao Zhu,
and Minyi Guo. Squant: On-the-ﬂy data-free quantization via diagonal hessian approximation. arXiv
preprint arXiv:2202.07471, 2022."
REFERENCES,0.9295154185022027,"[44] Eldad Meller, Alexander Finkelstein, Uri Almog, and Mark Grobman. Same, same but different: Re-
covering neural network quantization error through weight factorization. In International Conference on
Machine Learning, pages 4486–4495. PMLR, 2019."
REFERENCES,0.933920704845815,"[45] Yangcheng Gao, Zhao Zhang, Richang Hong, Haijun Zhang, Jicong Fan, and Shuicheng Yan. Towards
feature distribution alignment and diversity enhancement for data-free quantization. In 2022 IEEE Inter-
national Conference on Data Mining (ICDM), pages 141–150. IEEE, 2022."
REFERENCES,0.9383259911894273,"[46] Biao Qian, Yang Wang, Richang Hong, and Meng Wang. Rethinking data-free quantization as a zero-sum
game. arXiv preprint arXiv:2302.09572, 2023."
REFERENCES,0.9427312775330396,"[47] Biao Qian, Yang Wang, Richang Hong, and Meng Wang. Adaptive data-free quantization. arXiv e-prints,
pages arXiv–2303, 2023."
REFERENCES,0.947136563876652,"[48] Kanghyun Choi, Deokki Hong, Noseong Park, Youngsok Kim, and Jinho Lee. Qimera: Data-free quanti-
zation with synthetic boundary supporting samples. Advances in Neural Information Processing Systems,
34:14835–14847, 2021."
REFERENCES,0.9515418502202643,"[49] Timo Ojala, Matti Pietikäinen, and David Harwood. A comparative study of texture measures with clas-
siﬁcation based on featured distributions. Pattern recognition, 29(1):51–59, 1996."
REFERENCES,0.9559471365638766,"[50] Khurram Kamal, Rehan Qayyum, Senthan Mathavan, and Tayyab Zafar. Wood defects classiﬁcation
using laws texture energy measures and supervised learning approach. Advanced Engineering Informatics,
34:125–135, 2017."
REFERENCES,0.960352422907489,"[51] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk
minimization. In International Conference on Learning Representations, 2018."
REFERENCES,0.9647577092511013,"[52] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009."
REFERENCES,0.9691629955947136,"[53] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, An-
drej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge.
International journal of computer vision, 115:211–252, 2015."
REFERENCES,0.973568281938326,"[54] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016."
REFERENCES,0.9779735682819384,"[55] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.
Mo-
bilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 4510–4520, 2018."
REFERENCES,0.9823788546255506,"[56] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al.
Pytorch: An imperative style, high-
performance deep learning library. Advances in neural information processing systems, 32, 2019."
REFERENCES,0.986784140969163,"[57] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. In International
Conference on Learning Representations, 2016."
REFERENCES,0.9911894273127754,"[58] Baozhou Zhu, Peter Hofstee, Johan Peltenburg, Jinho Lee, and Zaid Alars. Autorecon: Neural architecture
search-based reconstruction for data-free. In International Joint Conference on Artiﬁcial Intelligence,
2021."
REFERENCES,0.9955947136563876,"[59] Kanghyun Choi, Hye Yoon Lee, Deokki Hong, Joonsang Yu, Noseong Park, Youngsok Kim, and Jinho
Lee. It’s all in the teacher: Zero-shot quantization brought closer to the teacher. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8311–8321, 2022."
