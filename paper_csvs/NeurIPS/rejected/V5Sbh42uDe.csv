Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001088139281828074,"We consider weakly supervised segmentation where only a fraction of pixels
1"
ABSTRACT,0.002176278563656148,"have ground truth labels (scribbles) and focus on a self-labeling approach where
2"
ABSTRACT,0.003264417845484222,"soft pseudo-labels on unlabeled pixels optimize some relaxation of the standard
3"
ABSTRACT,0.004352557127312296,"unsupervised CRF/Potts loss. While WSSS methods can directly optimize CRF
4"
ABSTRACT,0.00544069640914037,"losses via gradient descent, prior work suggests that higher-order optimization
5"
ABSTRACT,0.006528835690968444,"can lead to better network training by jointly estimating pseudo-labels, e.g. using
6"
ABSTRACT,0.007616974972796518,"discrete graph cut sub-problems. The inability of hard pseudo-labels to represent
7"
ABSTRACT,0.008705114254624592,"class uncertainty motivates the relaxed pseudo-labeling. We systematically evaluate
8"
ABSTRACT,0.009793253536452665,"standard and new CRF relaxations, neighborhood systems, and losses connecting
9"
ABSTRACT,0.01088139281828074,"network predictions with soft pseudo-labels. We also propose a general continuous
10"
ABSTRACT,0.011969532100108813,"sub-problem solver for such pseudo-labels. Soft self-labeling loss combining the
11"
ABSTRACT,0.013057671381936888,"log-quadratic Potts relaxation and collision cross-entropy achieves state-of-the-art
12"
ABSTRACT,0.014145810663764961,"and can outperform full pixel-precise supervision on PASCAL.
13"
INTRODUCTION,0.015233949945593036,"1
Introduction
14"
INTRODUCTION,0.01632208922742111,"Full supervision for semantic segmentation requires thousands of training images with complete pixel-
15"
INTRODUCTION,0.017410228509249184,"accurate ground truth masks. Their high costs explain the interest in weakly-supervised approaches
16"
INTRODUCTION,0.018498367791077257,"based on image-level class tags [21, 4], pixel-level scribbles [26, 36, 35], or boxes [23]. This paper
17"
INTRODUCTION,0.01958650707290533,"is focused on weak supervision with scribbles, which we also call seeds or partial masks. While
18"
INTRODUCTION,0.020674646354733407,"only slightly more expensive than image-level class tags, scribbles on less than 3% of pixels were
19"
INTRODUCTION,0.02176278563656148,"previously shown to achieve accuracy approaching full supervision without any modifications of the
20"
INTRODUCTION,0.022850924918389554,"segmentation models. In contrast, tag supervision typically requires highly specialized systems and
21"
INTRODUCTION,0.023939064200217627,"complex multi-stage training procedures, which are hard to reproduce. Our interest in the scribble-
22"
INTRODUCTION,0.025027203482045703,"based approach is motivated by its practical simplicity and mathematical clarity. The corresponding
23"
INTRODUCTION,0.026115342763873776,"methodologies are focused on the design of unsupervised or self-supervised loss functions and
24"
INTRODUCTION,0.02720348204570185,"stronger optimization algorithms. The corresponding solutions are often general and can be used in
25"
INTRODUCTION,0.028291621327529923,"different weakly-supervised applications.
26"
SCRIBBLE-SUPERVISED SEGMENTATION,0.029379760609358,"1.1
Scribble-supervised segmentation
27"
SCRIBBLE-SUPERVISED SEGMENTATION,0.030467899891186073,"Assume that a set of image pixels is denoted by Ωand a subset of pixels with ground truth labels is
28"
SCRIBBLE-SUPERVISED SEGMENTATION,0.031556039173014146,"S ⊂Ω, which we call seeds or scribbles as subset S is typically marked by mouse-controlled UI for
29"
SCRIBBLE-SUPERVISED SEGMENTATION,0.03264417845484222,"image annotations, e.g. see seeds over an image in Fig.7(a). The ground truth label at any given pixel
30"
SCRIBBLE-SUPERVISED SEGMENTATION,0.03373231773667029,"i ∈S is an integer
31"
SCRIBBLE-SUPERVISED SEGMENTATION,0.03482045701849837,"\ l abe l  { e q:GT_index} \bar {y}_i\in \{1,\dots ,K\} 
(1)"
SCRIBBLE-SUPERVISED SEGMENTATION,0.035908596300326445,"where K is the number of classes including the background. Without much ambiguity, it is convenient
32"
SCRIBBLE-SUPERVISED SEGMENTATION,0.036996735582154515,"to use the same notation ¯yi for the equivalent one-hot distribution
33"
SCRIBBLE-SUPERVISED SEGMENTATION,0.03808487486398259,"\
l
abel
 { e q : G T_o
n e h ot
} \
bar
 {y
} _i  \ ; \;\e q uiv  \;\;(\bar {y}_i^1,\dots ,\bar {y}_i^K)\,\in \Delta ^K_{0,1} \quad \quad \quad \text {for}\quad \bar {y}_i^k := \left [k=\bar {y}_i\right ]\;\in \{0,1\} 
(2)"
SCRIBBLE-SUPERVISED SEGMENTATION,0.03917301414581066,"where [ · ] is the True operator for the condition inside the brackets. Set ∆K
0,1 represents K possible
one-hot distributions, which are vertices of the K-class probability simplex"
SCRIBBLE-SUPERVISED SEGMENTATION,0.04026115342763874,"∆K
:=
{p = (p1, . . . , pK) | pk ≥0, K
X"
SCRIBBLE-SUPERVISED SEGMENTATION,0.041349292709466814,"k=1
pk = 1}"
SCRIBBLE-SUPERVISED SEGMENTATION,0.042437431991294884,"representing all K-categorical distributions. The context of specific expressions should make it
34"
SCRIBBLE-SUPERVISED SEGMENTATION,0.04352557127312296,"obvious if ¯yi is a class index (1) or the corresponding one-hot distribution (2).
35"
SCRIBBLE-SUPERVISED SEGMENTATION,0.04461371055495103,"Loss functions for weakly supervised segmentation with scribbles typically use negative log-
36"
SCRIBBLE-SUPERVISED SEGMENTATION,0.04570184983677911,"likelihoods (NLL) over scribbles i ∈S ⊂Ωwith ground truth labels ¯yi
37 "
SCRIBBLE-SUPERVISED SEGMENTATION,0.046789989118607184,"\la
be l {e
q:NLL} -\sum _{i\in S} \ln \sigma _i^{\bar {y}_i} 
(3)"
SCRIBBLE-SUPERVISED SEGMENTATION,0.04787812840043525,"where σi = (σ1
i , . . . , σK
i ) ∈∆K is the model prediction at pixel i. This loss is a standard in
38"
SCRIBBLE-SUPERVISED SEGMENTATION,0.04896626768226333,"full supervision where the only difference is that S = Ωand usually, no other losses are needed
39"
SCRIBBLE-SUPERVISED SEGMENTATION,0.05005440696409141,"for training. However, in a weakly supervised setting the majority of pixels are unlabeled, and
40"
SCRIBBLE-SUPERVISED SEGMENTATION,0.051142546245919476,"unsupervised losses are needed for i ̸∈S.
41"
SCRIBBLE-SUPERVISED SEGMENTATION,0.05223068552774755,"The most common unsupervised loss in image segmentation is the Potts model and its relaxations.
It is a pairwise loss defined on pairs of neighboring pixels {i, j} ∈N for a given neighborhood
system N ⊂Ω× Ω, typically corresponding to the nearest-neighbor grid (NN) [6, 17], or other
sparse (SN) [38] and dense neighborhoods (DN) [22]. The original Potts model is defined for discrete
segmentation variables, e.g. as in
X"
SCRIBBLE-SUPERVISED SEGMENTATION,0.05331882480957562,"{i,j}∈N
P(σi, σj)
where
P(σi, σj) = [σi ̸= σj]"
SCRIBBLE-SUPERVISED SEGMENTATION,0.0544069640914037,"assuming integer-valued one-hot predictions σi ∈∆K
0,1. This regularization loss encourages smooth-
ness between the pixels. Its popular self-supervised variant is"
SCRIBBLE-SUPERVISED SEGMENTATION,0.055495103373231776,"P(σi, σj) = wi,j · [σi ̸= σj]"
SCRIBBLE-SUPERVISED SEGMENTATION,0.056583242655059846,"where pairwise affinities wij are based on local intensity edges [6, 17, 22]. Of course, in the context
42"
SCRIBBLE-SUPERVISED SEGMENTATION,0.05767138193688792,"of network training, one should use relaxations of P applicable to (soft) predictions σi ∈∆K. Many
43"
SCRIBBLE-SUPERVISED SEGMENTATION,0.058759521218716,"types of its relaxation [33, 42] were studied in segmentation, e.g. quadratic [17], bi-linear [36], total
44"
SCRIBBLE-SUPERVISED SEGMENTATION,0.05984766050054407,"variation [32, 8], and others [14].
45"
SCRIBBLE-SUPERVISED SEGMENTATION,0.060935799782372145,"Another unsupervised loss highly relevant for training segmentation networks is the entropy of
predictions, which is also known as decisiveness [7, 18]
X"
SCRIBBLE-SUPERVISED SEGMENTATION,0.062023939064200215,"i
H(σi)"
SCRIBBLE-SUPERVISED SEGMENTATION,0.06311207834602829,"where H is the Shannon’s entropy function. This loss can improve generalization and the quality of
46"
SCRIBBLE-SUPERVISED SEGMENTATION,0.06420021762785637,"representation by moving (deep) features away from the decision boundaries. Widely known in the
47"
SCRIBBLE-SUPERVISED SEGMENTATION,0.06528835690968444,"context of unsupervised or semi-supervised classification, this loss also matters in weakly-supervised
48"
SCRIBBLE-SUPERVISED SEGMENTATION,0.06637649619151251,"segmentation where it is used explicitly or implicitly1.
49"
SCRIBBLE-SUPERVISED SEGMENTATION,0.06746463547334058,"Other unsupervised losses (e.g. contrastive), clustering criteria (e.g. K-means), or specialized
50"
SCRIBBLE-SUPERVISED SEGMENTATION,0.06855277475516866,"architectures can be found in weakly-supervised segmentation [39, 31, 20, 9]. However, a lot can be
51"
SCRIBBLE-SUPERVISED SEGMENTATION,0.06964091403699674,"achieved simply by combining the basic losses discussed above
52"
SCRIBBLE-SUPERVISED SEGMENTATION,0.07072905331882481,"\lab
el
 
{"
SCRIBBLE-SUPERVISED SEGMENTATION,0.07181719260065289,"eq:
lo ss_w
s
}  
L"
SCRIBBLE-SUPERVISED SEGMENTATION,0.07290533188248095,"_{ws
}(\si g m
a"
SCRIBBLE-SUPERVISED SEGMENTATION,0.07399347116430903,") \
;\;:= \;\; -\sum _{i\in S} \ln \sigma _i^{\bar {y}_i}\;+\;\eta \sum _{i\not \in S} H(\sigma _i) \;+\;\lambda \sum _{ij\in {\cal N}} P(\sigma _i,\sigma _j) 
(4)"
SCRIBBLE-SUPERVISED SEGMENTATION,0.0750816104461371,"which can be optimized directly by gradient descent [36, 38] or using self-labeling techniques
53"
SCRIBBLE-SUPERVISED SEGMENTATION,0.07616974972796518,"[26, 28, 27] incorporating optimization of auxiliary pseudo-labels as sub-problems.
54"
SCRIBBLE-SUPERVISED SEGMENTATION,0.07725788900979326,"1Interestingly, a unary decisiveness-like term is the difference between convex quadratic and tight, but
non-convex, bi-linear relaxations [33, 27] of the discrete pairwise Potts model."
SCRIBBLE-SUPERVISED SEGMENTATION,0.07834602829162132,"1.2
Soft pseudo-labels: motivation and contributions
55"
SCRIBBLE-SUPERVISED SEGMENTATION,0.0794341675734494,"We observe that self-labeling with hard pseudo-labels yi, which is discussed in the Appendix A, is
56"
SCRIBBLE-SUPERVISED SEGMENTATION,0.08052230685527748,"inherently limited as such labels can not represent the uncertainty of class estimates at unlabeled
57"
SCRIBBLE-SUPERVISED SEGMENTATION,0.08161044613710555,"pixels i ∈Ω\S. Instead, we focus on soft pseudo-labels
58"
SCRIBBLE-SUPERVISED SEGMENTATION,0.08269858541893363,"\
lab
e l  { e q :l
a b e l_soft} y_i \;\;=\;\;(y_i^1,\dots ,y_i^K)\,\in \Delta ^K 
(5)"
SCRIBBLE-SUPERVISED SEGMENTATION,0.08378672470076169,"which are general categorical distributions p over K-classes. It is possible that the estimated pseudo-
59"
SCRIBBLE-SUPERVISED SEGMENTATION,0.08487486398258977,"label yi in (5) could be a one-hot distribution, which is a vertex of ∆K. In such a case, one can treat
60"
SCRIBBLE-SUPERVISED SEGMENTATION,0.08596300326441784,"yi as a class index, but we avoid this in the main part of our paper starting Section 2. However, the
61"
SCRIBBLE-SUPERVISED SEGMENTATION,0.08705114254624592,"ground truth labels ¯yi are always hard and we use them either as indices (1) or one-hot distributions
62"
SCRIBBLE-SUPERVISED SEGMENTATION,0.088139281828074,"(2), as convenient.
63"
SCRIBBLE-SUPERVISED SEGMENTATION,0.08922742110990206,"Soft pseudo-labels can be found in prior work on weakly-supervised segmentation [25, 41] using the
64"
SCRIBBLE-SUPERVISED SEGMENTATION,0.09031556039173014,"“soft proposal generation”. In contrast, we formulate soft self-labeling as a principled optimization
65"
SCRIBBLE-SUPERVISED SEGMENTATION,0.09140369967355821,"methodology where network predictions and soft pseudo-labels are variables in a joint loss, which
66"
SCRIBBLE-SUPERVISED SEGMENTATION,0.09249183895538629,"guarantees convergence of the training procedure. Our pseudo-labels are auxiliary variables for
67"
SCRIBBLE-SUPERVISED SEGMENTATION,0.09357997823721437,"ADM-based [5] splitting of the loss (4) into two simpler optimization sub-problems: one focused on
68"
SCRIBBLE-SUPERVISED SEGMENTATION,0.09466811751904244,"the Potts model over unlabeled pixels, and the other on the network training. While similar to [28],
69"
SCRIBBLE-SUPERVISED SEGMENTATION,0.0957562568008705,"instead of hard, we use soft auxiliary variables for the Potts sub-problem. Our work can be seen as a
70"
SCRIBBLE-SUPERVISED SEGMENTATION,0.09684439608269858,"study of the relaxed Potts sub-problem in the context of weakly-supervised semantic segmentation.
71"
SCRIBBLE-SUPERVISED SEGMENTATION,0.09793253536452666,"The related prior work is focused on discrete solvers fundamentally unable to represent class estimate
72"
SCRIBBLE-SUPERVISED SEGMENTATION,0.09902067464635474,"uncertainty. Our contributions can be summarized as follows:
73"
SCRIBBLE-SUPERVISED SEGMENTATION,0.10010881392818281,"• convergent soft self-labeling framework based on a simple joint self-labeling loss
74"
SCRIBBLE-SUPERVISED SEGMENTATION,0.10119695321001088,"• systematic evaluation of Potts relaxations and (cross-) entropy terms in our loss
75"
SCRIBBLE-SUPERVISED SEGMENTATION,0.10228509249183895,"• state-of-the-art in scribble-based semantic segmentation that does not require any modifica-
76"
SCRIBBLE-SUPERVISED SEGMENTATION,0.10337323177366703,"tions of semantic segmentation models and is easy to reproduce
77"
SCRIBBLE-SUPERVISED SEGMENTATION,0.1044613710554951,"• using the same segmentation model, our self-labeling loss with 3% scribbles may outperform
78"
SCRIBBLE-SUPERVISED SEGMENTATION,0.10554951033732318,"standard supervised cross-entropy loss with full ground truth masks.
79"
OUR SOFT SELF-LABELING APPROACH,0.10663764961915125,"2
Our soft self-labeling approach
80"
OUR SOFT SELF-LABELING APPROACH,0.10772578890097932,"First, we apply ADM splitting [5] to weakly supervised loss (4) to formulate our self-labeling loss (6)
incorporating additional soft auxiliary variables, i.e. pseudo-labels (5). It is convenient to introduce
pseudo-labels yi on all pixels in Ωeven though a subset of pixels (seeds) S ⊂Ωhave ground truth
labels ¯yi. We will simply impose a constraint that pseudo-labels and ground truth labels agree on S.
Thus, we assume the following set of pseudo-labels"
OUR SOFT SELF-LABELING APPROACH,0.1088139281828074,"YΩ:= {yi ∈∆K | i ∈Ω, s.t. yi = ¯yi for i ∈S}."
OUR SOFT SELF-LABELING APPROACH,0.10990206746463548,"We split the terms in (4) into two groups: one includes NLL and entropy H terms keeping the original
prediction variables σi and the other includes the Potts relaxation P replacing σi with auxiliary
variables yi. This transforms loss (4) into expression −
X"
OUR SOFT SELF-LABELING APPROACH,0.11099020674646355,"i∈S
ln σ¯yi
i
+ η
X"
OUR SOFT SELF-LABELING APPROACH,0.11207834602829161,"i̸∈S
H(σi) + λ
X"
OUR SOFT SELF-LABELING APPROACH,0.11316648531011969,"ij∈N
P(yi, yj)"
OUR SOFT SELF-LABELING APPROACH,0.11425462459194777,"equivalent to (4) assuming equality σi = yi. The standard approximation is to incorporate constraint
81"
OUR SOFT SELF-LABELING APPROACH,0.11534276387377584,"σi ≈yi directly into the loss, e.g. using KL-divergence. For simplicity, we use weight η for
82"
OUR SOFT SELF-LABELING APPROACH,0.11643090315560392,"KL(σi, yi) to combine it with H(σi) into a single cross-entropy term \
n"
OUR SOFT SELF-LABELING APPROACH,0.117519042437432,"onu
mb er -
\
s u
m"
OUR SOFT SELF-LABELING APPROACH,0.11860718171926006,"_{i
\in S }  
\"
OUR SOFT SELF-LABELING APPROACH,0.11969532100108814,"ln \
sigma _i^"
OUR SOFT SELF-LABELING APPROACH,0.12078346028291621,"{
\b
a r  
{"
OUR SOFT SELF-LABELING APPROACH,0.12187159956474429,"y}_i
} & \ ;+\ ;
\"
OUR SOFT SELF-LABELING APPROACH,0.12295973884657237,"unde
rbrac e {\eta \sum _{i\not \in S} H(\sigma _i)\;+\;\eta \sum _{i\not \in S} KL(\sigma _i,y_i)}\;+\;\lambda \sum _{ij\in {\cal N}} P(y_i,y_j) \\ \nonumber & \quad \quad \quad \quad \quad \eta \sum _{i\not \in S} H(\sigma _i,y_i)"
OUR SOFT SELF-LABELING APPROACH,0.12404787812840043,"defining joint self-labeling loss for both predictions σi and pseudo-labels yi
84"
OUR SOFT SELF-LABELING APPROACH,0.1251360174102285,"\label  {
eq
:
l"
OUR SOFT SELF-LABELING APPROACH,0.12622415669205658,"oss
_s elf}
 
L _
{"
OUR SOFT SELF-LABELING APPROACH,0.12731229597388466,"self
}(\si gma  ,
y"
OUR SOFT SELF-LABELING APPROACH,0.12840043525571274,")\;\
; :=\ ;\; -\sum _{i\in S} \ln \sigma _i^{\bar {y}_i}\;+\;\eta \sum _{i\not \in S} H(\sigma _i,y_i) \;+\;\lambda \sum _{ij\in {\cal N}} P(y_i,y_j) 
(6)"
OUR SOFT SELF-LABELING APPROACH,0.1294885745375408,"bi-linear ∼“graph cut”
quadratic ∼“random walker”
PBL(p, q)
:=
1 −p⊤q
PQ(p, q)
:=
1
2∥p −q∥2"
OUR SOFT SELF-LABELING APPROACH,0.1305767138193689,normalized quadratic
OUR SOFT SELF-LABELING APPROACH,0.13166485310119697,"PNQ(p, q)
:=
1 −
p⊤q
∥p∥∥q∥
≡
1
2
 p"
OUR SOFT SELF-LABELING APPROACH,0.13275299238302501,"∥p∥−
q
∥q∥

2"
OUR SOFT SELF-LABELING APPROACH,0.1338411316648531,"Table 1: Second-order Potts relaxations, see Fig.1(a,b,c)
approximating the original weakly supervised loss (4).
85"
OUR SOFT SELF-LABELING APPROACH,0.13492927094668117,"Iterative minimization of this loss w.r.t. predictions σi (model parameters training) and pseudo-
86"
OUR SOFT SELF-LABELING APPROACH,0.13601741022850924,"labels yi effectively breaks the original optimization problem for (4) into two simpler sub-problems,
87"
OUR SOFT SELF-LABELING APPROACH,0.13710554951033732,"assuming there is a good solver for optimal pseudo-labels. The latter seems plausible since the unary
88"
OUR SOFT SELF-LABELING APPROACH,0.1381936887921654,"term H(σi, yi) is convex for yi and the Potts relaxations were widely studied in image segmentation
89"
OUR SOFT SELF-LABELING APPROACH,0.13928182807399347,"for decades.
90"
OUR SOFT SELF-LABELING APPROACH,0.14036996735582155,"Section 2.1 discusses standard and new relaxations of the Potts model P. Section 2.2 discusses several
91"
OUR SOFT SELF-LABELING APPROACH,0.14145810663764963,"robust variants of cross-entropy H for connecting predictions with uncertain (soft) pseudo-labels yi
92"
OUR SOFT SELF-LABELING APPROACH,0.1425462459194777,"estimated for unlabeled points i ∈Ω\S. Appendix B proposes an efficient general solver for the
93"
OUR SOFT SELF-LABELING APPROACH,0.14363438520130578,"corresponding pseudo-labeling sub-problems.
94"
SECOND-ORDER RELAXATIONS OF THE POTTS MODEL,0.14472252448313383,"2.1
Second-order relaxations of the Potts model
95"
SECOND-ORDER RELAXATIONS OF THE POTTS MODEL,0.1458106637649619,"We focus on second-order relaxations for two reasons. First, to manage the scope of this study.
96"
SECOND-ORDER RELAXATIONS OF THE POTTS MODEL,0.14689880304678998,"Second, this includes several important baseline cases (see Table 1): quadratic, the simplest convex
97"
SECOND-ORDER RELAXATIONS OF THE POTTS MODEL,0.14798694232861806,"relaxation popularized by the random walker algorithm [17], and bi-linear, which is non-convex but
98"
SECOND-ORDER RELAXATIONS OF THE POTTS MODEL,0.14907508161044614,"tight [33] w.r.t. the original discrete Potts model. The latter implies that optimizing it over relaxed
99"
SECOND-ORDER RELAXATIONS OF THE POTTS MODEL,0.1501632208922742,"variables will lead to a solution consistent with a discrete Potts solver, e.g. graph cut [6]. On the
100"
SECOND-ORDER RELAXATIONS OF THE POTTS MODEL,0.1512513601741023,"contrary, the quadratic relaxation will produce a significantly different soft solution. We investigate
101"
SECOND-ORDER RELAXATIONS OF THE POTTS MODEL,0.15233949945593037,"such soft solutions.
102"
SECOND-ORDER RELAXATIONS OF THE POTTS MODEL,0.15342763873775844,"Figure 2 shows two examples illustrating local minima for (a) the bi-linear and (b) quadratic relax-
ations of the Potts loss. In (a) two neighboring pixels attempt to jointly change the common soft
label from yi = yj = (1, 0, 0) to y′′
i = y′′
j = (0, 1, 0), which corresponds to a “move” where the
whole object is reclassified from A to B. This move does not violate smoothness within the region
represented by the Potts model. But, the soft intermediate state y′
i = y′
j = ( 1 2, 1"
SECOND-ORDER RELAXATIONS OF THE POTTS MODEL,0.15451577801958652,"2, 0) will prevent this
move in bi-linear case"
SECOND-ORDER RELAXATIONS OF THE POTTS MODEL,0.15560391730141457,"PBL(y′
i, y′
j) = 1"
SECOND-ORDER RELAXATIONS OF THE POTTS MODEL,0.15669205658324264,"2
>
0 = PBL(yi, yj) = PBL(y′′
i , y′′
j )"
SECOND-ORDER RELAXATIONS OF THE POTTS MODEL,0.15778019586507072,"while quadratic relaxation assigns zero loss for all states during this move. On the other hand, the
example in Figure 2(b) shows a move problematic for the quadratic relaxation. Two neighboring
pixels have labels yi = (1, 0, 0) and yj = (0, 0, 1) corresponding to the boundary of objects A and
C. The second object attempts to change from C to B. This move does not affect the discontinuity
between two pixels, but quadratic relaxation prefers that the second object is stuck in the intermediate
state y′
j = (0, 1 2, 1 2)"
SECOND-ORDER RELAXATIONS OF THE POTTS MODEL,0.1588683351468988,"PQ(yi, y′
j) = 3"
SECOND-ORDER RELAXATIONS OF THE POTTS MODEL,0.15995647442872687,"4
<
1 = PQ(yi, yj) = PQ(yi, y′′
j )"
SECOND-ORDER RELAXATIONS OF THE POTTS MODEL,0.16104461371055495,"while bi-linear relaxation PBL(yi, yj) = 1 remains constant as yj changes.
103"
SECOND-ORDER RELAXATIONS OF THE POTTS MODEL,0.16213275299238303,"We propose a new relaxation, normalized quadratic in Table 1. Normalization leads to equivalence
104"
SECOND-ORDER RELAXATIONS OF THE POTTS MODEL,0.1632208922742111,"between quadratic and bi-linear formulations combining their benefits. As easy to check, normalized
105"
SECOND-ORDER RELAXATIONS OF THE POTTS MODEL,0.16430903155603918,"collision cross entropy
log-quadratic
PCCE(p, q)
:=
−ln p⊤q
PLQ(p, q)
:=
−ln

1 −∥p−q∥2 2
"
SECOND-ORDER RELAXATIONS OF THE POTTS MODEL,0.16539717083786726,collision divergence
SECOND-ORDER RELAXATIONS OF THE POTTS MODEL,0.16648531011969533,"PCD(p, q)
:=
−ln
p⊤q
∥p∥∥q∥
≡
−ln

1 −1"
P,0.16757344940152338,"2
 p"
P,0.16866158868335146,"∥p∥−
q
∥q∥

2"
P,0.16974972796517954,"Table 2: Log-based Potts relaxations, see Fig.1(d,e,f)"
P,0.1708378672470076,Table 1
P,0.1719260065288357,"(a) bi-linear PBL
(b) normalized quadratic PNQ
(c) quadratic PQ"
P,0.17301414581066377,Table 2 (+log)
P,0.17410228509249184,"(d) collision cr. entropy PCCE
(e) collision divergence PCD
(f) log-quadratic PLQ"
P,0.17519042437431992,"Figure 1: Second-order Potts relaxations in Tables 1 and 2: interaction potentials P for pairs of
predictions (σi, σj) in (4) or pseudo-labels (yi, yj) in (6) are illustrated for K = 2 when each
prediction σi or label yi, i.e. distribution in ∆2, can be represented by a single scalar as (x, 1 −x).
The contour maps are iso-levels of P((xi, 1 −xi), (xj, 1 −xj)) over domain (xi, xj) ∈[0, 1]2. The
3D plots above illustrate the potentials P as functions over pairs of “logits” (li, lj) ∈R2 where each
scalar logit li defines binary distribution (xi, 1 −xi) for xi =
1
1+e−2li ∈[0, 1]."
P,0.176278563656148,"quadratic relaxation PNQ does not have local minima in both examples of Figure 2. Table 2 also
106"
P,0.17736670293797607,"proposes “logarithmic” versions of the relaxations in Table 1 composing them with function −ln(1 −
107"
P,0.17845484221980412,"x). As illustrated by Figure 1, the logarithmic versions in (d-f) addresses the “vanishing gradients”
108"
P,0.1795429815016322,"evident in (a-c).
109"
P,0.18063112078346028,"(a) both pixels change (A →B)
(b) only pixel q changes (C →B)"
P,0.18171926006528835,"Figure 2: Examples of ""moves"" for neighboring pixels {i, j} ∈N. Their (soft) pseudo-labels yi and
yj are illustrated on the probability simplex ∆K for K = 3. In (a) both pixels i and j are inside a
region/object changing its label from A to B. In (b) pixels i and j are on the boundary between two
regions/objects; one is fixed to class A and the other changes from class C to B."
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.18280739934711643,"2.2
Cross-entropy and soft pseudo-labels
110"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.1838955386289445,"Shannon’s cross-entropy H(y, σ) is the most common loss for training network predictions σ from
111"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.18498367791077258,"ground truth labels y in the context of classification, semantic segmentation, etc. However, this loss
112"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.18607181719260066,"may not be ideal for applications where the targets y are soft categorical distributions representing
113"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.18715995647442873,"various forms of class uncertainty. For example, this paper is focused on scribble-based segmentation
114"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.1882480957562568,"where the ground truth is not known for most of the pixels, and the network training is done jointly
115"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.1893362350380849,"with estimating pseudo-labels y for the unlabeled pixels. In this case, soft labels y are distributions
116"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.19042437431991294,"representing class uncertainty. We observe that if such y is used as a target in H(y, σ), the network
117"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.191512513601741,"is trained to reproduce the uncertainty, see Figure 3(a). This motivates the discussion of alternative
118"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.1926006528835691,"“cross-entropy” functions where the quotes indicate an informal interpretation of this information-
119"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.19368879216539717,"theoretic concept. Intuitively, such functions should encourage decisiveness, as well as proximity
120"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.19477693144722524,"0
0.5
0.8
0.95
1
σ"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.19586507072905332,"ଵ= ( 0.95, 0.05)"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.1969532100108814,"ଶ= ( 0.8, 0.2)"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.19804134929270947,"ଷ= ( 0.5, 0.5) 0
1
σ"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.19912948857453755,"ଵ= ( 0.95, 0.05)"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.20021762785636563,"ଶ= ( 0.8, 0.2)"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.20130576713819368,"ଷ= ( 0.5, 0.5)
−ln 0.5"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.20239390642002175,−ln 0.2
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.20348204570184983,−ln 0.5
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.2045701849836779,−ln 0.8
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.20565832426550598,−ln 0.95
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.20674646354733406,"0
0.5
1
σ"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.20783460282916214,−ln 0.5
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.2089227421109902,−ln 0.2
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.2100108813928183,−ln 0.5
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.21109902067464636,−ln 0.8
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.21218715995647444,−ln 0.95
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.2132752992383025,"ଵ= ( 0.95, 0.05)"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.21436343852013057,"ଷ= ( 0.5, 0.5)"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.21545157780195864,"ଶ= ( 0.8, 0.2)"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.21653971708378672,"0.0
0.2
0.4
0.6
0.8
corruption level 25% 35% 45% 55% 65% 75% 85% 95%"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.2176278563656148,accuracy
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.21871599564744287,"HCE(y,
)"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.21980413492927095,"HRCE(y,
)"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.22089227421109903,"HCCE(y,
)"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.2219804134929271,"(a) standard HCE(y, σ) (b) reverse HRCE(y, σ) (c) collision HCCE(y, σ) (d) empirical comparison"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.22306855277475518,"Figure 3: Illustration of cross-entropy functions: (a) standard (7), (b) reverse (8), and (c) collision
(9). (d) shows the empirical comparison on the robustness to label uncertainty. The test uses
ResNet-18 architecture on fully-supervised Natural Scene dataset [30] where we corrupted some
labels. The horizontal axis shows the percentage η of training images where the correct ground truth
labels were replaced by a random label. All losses trained the model using soft target distributions
ˆy = η∗u+(1−η)∗y representing the mixture of one-hot distribution y for the observed corrupt label
and the uniform distribution u, following [29]. The vertical axis shows the test accuracy. Training
with the reverse and collision cross-entropy is robust to much higher levels of label uncertainty."
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.22415669205658323,"between the predictions and pseudo-labels, but avoid mimicking the uncertainty in both directions:
121"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.2252448313384113,"from soft pseudo-labels to predictions and vice-versa. We show that the last property can be achieved
122"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.22633297062023938,"in a probabilistically principled manner. The following three paragraphs discuss different cross-
123"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.22742110990206746,"entropy functions that we study in the context of our self-labeling loss (6).
124"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.22850924918389554,"Standard cross-entropy provides the obvious baseline for evaluating two alternative versions that
125"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.2295973884657236,"follow. For completeness, we include its mathematical definition
126"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.2306855277475517,"\labe l {
e
q:D_C E} 
H
_
\"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.23177366702937977,"C
E 
( y_ i,
\sigma _i)\;\;=\;\;H(y_i,\sigma _i) \;\;\equiv \;\;-\sum _k y_i^k \ln \sigma _i^k 
(7)"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.23286180631120784,"and remind the reader that this loss is primarily used with hard or one-hot labels, in which case it is
127"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.23394994559303592,"also equivalent to NLL loss −ln σyi
i previously discussed for ground truth labels (3). As mentioned
128"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.235038084874864,"earlier, Figure 3(a) shows that for soft pseudo-labels like y = (0.5, 0.5), it forces predictions to mimic
129"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.23612622415669204,"or replicate the uncertainty σ ≈y. In fact, label y = (0.5, 0.5) just tells that the class is unknown
130"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.23721436343852012,"and the network should not be supervised by this point. This problem manifests itself in the poor
131"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.2383025027203482,"performance of the standard cross-entropy (7) in our experiment discussed in Figure 3 (d) (red curve).
132"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.23939064200217627,"Reverse cross-entropy switches the order of the label and prediction in (7)
133"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.24047878128400435,"\label  {e
q
:D_re ver
s
e
C"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.24156692056583243,"E
} 
H _\ RC
E (y_i,\sigma _i)\;\;=\;\;H(\sigma _i,y_i)\;\;\equiv \;\;-\sum _k \sigma _i^k \ln y_i^k 
(8)"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.2426550598476605,"which is not too common. Indeed, Shannon’s cross-entropy is not symmetric and the first argument
134"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.24374319912948858,"is normally the target distribution and the second is the estimated distribution. However, in our
135"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.24483133841131666,"case, both distributions are estimated and there is no reason not to try the reverse order. It is worth
136"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.24591947769314473,"noting that our self-labeling formulation (6) suggests that reverse cross-entropy naturally appears
137"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.2470076169749728,"when the ADM approach splits the decisiveness and fairness into separate sub-problems. Moreover,
138"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.24809575625680086,"as Figure 3(b) shows, in this case, the network does not mimic uncertain pseudo-labels, e.g. the
139"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.24918389553862894,"gradient of the blue line is zero. The results for the reverse cross-entropy in Figure 3 (d) (green)
140"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.250272034820457,"are significantly better than for the standard (red). Unfortunately, now pseudo-labels y mimic the
141"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.2513601741022851,"uncertainty in predictions σ.
142"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.25244831338411317,"Collision cross-entropy resolves the problem in a principled way. We define it as
143"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.2535364526659412,"\label  {e
q
: D_
c"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.2546245919477693,"C
E}
 H_
\
C
C E (y_i,\sigma _i)\;\;\equiv \;\;-\ln \sum _k \sigma _i^k y_i^k \;\;\equiv \;\;-\ln \sigma ^\top y 
(9)"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.25571273122959737,"which is symmetric w.r.t. pseudo-labels and predictions. The dot product σ⊤y can be seen as a
probability that random variables represented by the distribution σ, the prediction class C, and the
distribution y, the unknown true class T, are equal. Indeed,"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.25680087051142547,"Pr(C = T) =
X"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.2578890097932535,"k
Pr(C = k) Pr(T = k) = σ⊤y."
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.2589771490750816,"Loss (9) maximizes this “collision” probability rather than the constraint σ = y. Figure 3(c) shows no
144"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.2600652883569097,"mimicking of uncertainty (blue line). However, unlike reverse cross-entropy, this is also valid when y
145"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.2611534276387378,"is estimated from uncertain predictions σ since (9) is symmetric. This leads to the best performance
146"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.2622415669205658,"in Figure 3 (d) (blue). Our extensive experiments are conclusive that collision cross-entropy is the
147"
CROSS-ENTROPY AND SOFT PSEUDO-LABELS,0.26332970620239393,"best option for H in self-labeling loss (6).
148"
EXPERIMENTS,0.264417845484222,"3
Experiments
149"
EXPERIMENTS,0.26550598476605003,"We conducted comprehensive experiments to demonstrate the choice of each element (cross-entropy,
150"
EXPERIMENTS,0.26659412404787813,"pairwise term, and neighborhood) in the loss and compare our method to the state-of-the-art. In
151"
EXPERIMENTS,0.2676822633297062,"Section 3.1, quantitative results are shown to compare different Potts relaxations. The qualitative
152"
EXPERIMENTS,0.2687704026115343,"examples are shown in Figure 7. Then we compare several cross-entropy terms in Section 3.2.
153"
EXPERIMENTS,0.26985854189336234,"Besides, we also compare our soft self-labeling approach on the nearest and dense neighborhood
154"
EXPERIMENTS,0.27094668117519044,"systems in Section 3.3. We summarized the results in Section 3.4. In the last section, we show that
155"
EXPERIMENTS,0.2720348204570185,"our method achieves the SOTA and even can outperform the fully-supervised method. More details
156"
EXPERIMENTS,0.2731229597388466,"on the dataset, implementation, and additional experiments are given in Appendix C.
157"
COMPARISON OF POTTS RELAXATIONS,0.27421109902067464,"3.1
Comparison of Potts relaxations
158"
COMPARISON OF POTTS RELAXATIONS,0.27529923830250275,"To compare different Potts relaxations under the self-labeling framework,
we need to
159"
COMPARISON OF POTTS RELAXATIONS,0.2763873775843308,"choose
one
cross-entropy
term.
Motivated
by
the
properties
and
empirical
results
160"
COMPARISON OF POTTS RELAXATIONS,0.27747551686615884,"scribble length ratio
0
0.3
0.5
0.8
1.0
PBL
56.42
61.74
63.81
65.73
67.24
PNQ
59.01
65.53
67.80
70.63
71.12
PQ
58.92
65.34
67.81
70.43
71.05
PCCE
56.40
61.82
63.81
65.81
67.41
PCD
59.04
65.52
67.84
70.93
71.22
PLQ
59.03
65.44
67.81
70.80
71.21"
COMPARISON OF POTTS RELAXATIONS,0.27856365614798695,"Table 3: Comparison of Potts relaxations with
self-labeling.
mIoUs on validation set are
shown here."
COMPARISON OF POTTS RELAXATIONS,0.279651795429815,"shown in Section 3.2, we use HCCE. The neighbor-
161"
COMPARISON OF POTTS RELAXATIONS,0.2807399347116431,"hood system is the nearest neighbors. The quanti-
162"
COMPARISON OF POTTS RELAXATIONS,0.28182807399347115,"tative results are in Table 3. First, One can see that
163"
COMPARISON OF POTTS RELAXATIONS,0.28291621327529926,"the pairwise terms with logarithm are better than
164"
COMPARISON OF POTTS RELAXATIONS,0.2840043525571273,"those without the logarithm because the logarithm
165"
COMPARISON OF POTTS RELAXATIONS,0.2850924918389554,"may help with the gradient vanishing problem in
166"
COMPARISON OF POTTS RELAXATIONS,0.28618063112078346,"softmax operation. Moreover, the logarithm does
167"
COMPARISON OF POTTS RELAXATIONS,0.28726877040261156,"not like abrupt change across the boundaries, so the
168"
COMPARISON OF POTTS RELAXATIONS,0.2883569096844396,"transition across the boundaries is smoother (see
169"
COMPARISON OF POTTS RELAXATIONS,0.28944504896626766,"Figure 7 in the appendix.). Note that it is reasonable
170"
COMPARISON OF POTTS RELAXATIONS,0.29053318824809576,"to have higher uncertainty around the boundaries.
171"
COMPARISON OF POTTS RELAXATIONS,0.2916213275299238,"Second, the results prefer the normalized version,
172"
COMPARISON OF POTTS RELAXATIONS,0.2927094668117519,"which confirms the points made in Figure 2. Third, the simplest quadratic formulation PQ can be a
173"
COMPARISON OF POTTS RELAXATIONS,0.29379760609357997,"fairly good starting point to obtain decent results. Additionally, we specifically test HQ + PQ due to
174"
COMPARISON OF POTTS RELAXATIONS,0.29488574537540807,"the existing closed-form solution [1, 17]. Since the pseudo-labels generated from this formula tend to
175"
COMPARISON OF POTTS RELAXATIONS,0.2959738846572361,"be overly soft, we explicitly add entropy terms during the training of network parameters and the
176"
COMPARISON OF POTTS RELAXATIONS,0.2970620239390642,"mIoU goes up to 68.97% from 67.8%.
177"
COMPARISON OF CROSS-ENTROPY TERMS,0.2981501632208923,"3.2
Comparison of cross-entropy terms
178"
COMPARISON OF CROSS-ENTROPY TERMS,0.2992383025027203,"In this section, we compare different cross-entropy terms while fixing the pairwise term to
179"
COMPARISON OF CROSS-ENTROPY TERMS,0.3003264417845484,"PQ due to its simplicity and using the nearest neighborhood system.
The results are shown
180"
COMPARISON OF CROSS-ENTROPY TERMS,0.3014145810663765,"0.0
0.2
0.4
0.6
0.8
1.0
ratio of scribble length 56 58 60 62 64 66 68 70 72"
COMPARISON OF CROSS-ENTROPY TERMS,0.3025027203482046,mIoU on validation set
COMPARISON OF CROSS-ENTROPY TERMS,0.30359085963003263,"HCE
HRCE
HCCE"
COMPARISON OF CROSS-ENTROPY TERMS,0.30467899891186073,"Figure 4: Comparison of cross-entropy
terms."
COMPARISON OF CROSS-ENTROPY TERMS,0.3057671381936888,"in Figure 4.
One can see that HCCE performs the
181"
COMPARISON OF CROSS-ENTROPY TERMS,0.3068552774755169,"best consistently across different supervision levels,
182"
COMPARISON OF CROSS-ENTROPY TERMS,0.30794341675734493,"i.e. scribble lengths. Both HCCE and HRCE are con-
183"
COMPARISON OF CROSS-ENTROPY TERMS,0.30903155603917304,"sistently better than standard HCE with a noticeable
184"
COMPARISON OF CROSS-ENTROPY TERMS,0.3101196953210011,"margin because they are more robust, as explained in
185"
COMPARISON OF CROSS-ENTROPY TERMS,0.31120783460282914,"Section 2.2, to the uncertainty in soft pseudo-labels
186"
COMPARISON OF CROSS-ENTROPY TERMS,0.31229597388465724,"when optimizing network parameters. We also test the
187"
COMPARISON OF CROSS-ENTROPY TERMS,0.3133841131664853,"performance of using HCCE + PQ with hard pseudo-
188"
COMPARISON OF CROSS-ENTROPY TERMS,0.3144722524483134,"labels obtained via the argmax operation on the soft
189"
COMPARISON OF CROSS-ENTROPY TERMS,0.31556039173014144,"ones. The mIoU on the validation set is 69.8% under
190"
COMPARISON OF CROSS-ENTROPY TERMS,0.31664853101196955,"the full scribble-length supervision.
191"
COMPARISON OF NEIGHBORHOOD SYSTEMS,0.3177366702937976,"3.3
Comparison of neighborhood systems
192"
COMPARISON OF NEIGHBORHOOD SYSTEMS,0.3188248095756257,"Until now, we only used the four nearest neighbors for the pairwise term. In this section, we also use
193"
COMPARISON OF NEIGHBORHOOD SYSTEMS,0.31991294885745375,"the dense neighborhood and compare the results under the self-labeling framework.
194"
COMPARISON OF NEIGHBORHOOD SYSTEMS,0.32100108813928185,"（a) Image & scribble
(b) Predictions
(c) Pseudo-labels"
COMPARISON OF NEIGHBORHOOD SYSTEMS,0.3220892274211099,for NN
COMPARISON OF NEIGHBORHOOD SYSTEMS,0.32317736670293795,(d) Pseudo-labels
COMPARISON OF NEIGHBORHOOD SYSTEMS,0.32426550598476606,for DN (25)
COMPARISON OF NEIGHBORHOOD SYSTEMS,0.3253536452665941,(e) Pseudo-labels
COMPARISON OF NEIGHBORHOOD SYSTEMS,0.3264417845484222,for DN (100)
COMPARISON OF NEIGHBORHOOD SYSTEMS,0.32752992383025026,Figure 5: Pseudo-labels generated from given network predictions using different neighborhoods.
COMPARISON OF NEIGHBORHOOD SYSTEMS,0.32861806311207836,"Firstly, to optimize the pseudo-labels for the dense neighborhood, we still use the gradient descent
195"
COMPARISON OF NEIGHBORHOOD SYSTEMS,0.3297062023939064,"technique as detailed in Appendix B. The gradient computation employs the bilateral filtering
196"
COMPARISON OF NEIGHBORHOOD SYSTEMS,0.3307943416757345,"technique following [35]. For the pairwise term, we use PQ. The cross-entropy term is HCCE. Note
197"
COMPARISON OF NEIGHBORHOOD SYSTEMS,0.33188248095756256,"that the bilateral filtering technique only supports quadratic pairwise terms, i.e. PBL and PQ. Since
198"
COMPARISON OF NEIGHBORHOOD SYSTEMS,0.33297062023939067,"PBL leads to hard solutions, PQ is the only practical choice for soft self-labeling. We obtained 71.1%
199"
COMPARISON OF NEIGHBORHOOD SYSTEMS,0.3340587595212187,"mIoU on nearest neighbors while only getting 67.9% on dense neighborhoods (bandwidth is 100).
200"
COMPARISON OF NEIGHBORHOOD SYSTEMS,0.33514689880304677,"Some qualitative results are shown in Figure 5. Clearly from this figure one can see that a larger
201"
COMPARISON OF NEIGHBORHOOD SYSTEMS,0.33623503808487487,"neighborhood size induces lower-quality pseudo-labels. A possible explanation is that the Potts
202"
COMPARISON OF NEIGHBORHOOD SYSTEMS,0.3373231773667029,"model gets closer to cardinality/volume potentials when the neighborhood size becomes larger [37].
203"
COMPARISON OF NEIGHBORHOOD SYSTEMS,0.338411316648531,"The nearest neighborhood is better for edge alignment and thus produces cleaner results.
204"
COMPARISON OF NEIGHBORHOOD SYSTEMS,0.3394994559303591,"3.4
Soft self-labeling vs. hard self-labeling vs. gradient descent
205"
COMPARISON OF NEIGHBORHOOD SYSTEMS,0.3405875952121872,"In this section, we give a summary in Table 4 as to what is the best framework for
206"
COMPARISON OF NEIGHBORHOOD SYSTEMS,0.3416757344940152,"the WSSS based on losses regularized by the Potts model.
Firstly, to directly optimize
207"
COMPARISON OF NEIGHBORHOOD SYSTEMS,0.34276387377584333,"N
NN
DN
GD
67.0
69.5∗[36]"
COMPARISON OF NEIGHBORHOOD SYSTEMS,0.3438520130576714,"SL
hard
69.6∗[27]
63.1 [26]
soft
71.1
67.9
Table 4: Summary of comparisons. “∗” stands
for the reproduced results from their code repos-
itory."
COMPARISON OF NEIGHBORHOOD SYSTEMS,0.34494015233949943,"the network parameters via stochastic gradient de-
208"
COMPARISON OF NEIGHBORHOOD SYSTEMS,0.34602829162132753,"scent on the regularized loss, one needs a larger
209"
COMPARISON OF NEIGHBORHOOD SYSTEMS,0.3471164309031556,"neighborhood size. One possible explanation is
210"
COMPARISON OF NEIGHBORHOOD SYSTEMS,0.3482045701849837,"that a larger neighborhood size induces a smoother
211"
COMPARISON OF NEIGHBORHOOD SYSTEMS,0.34929270946681173,"Potts model and it helps the gradient descent [28].
212"
COMPARISON OF NEIGHBORHOOD SYSTEMS,0.35038084874863984,"However, larger neighborhood size is not preferred
213"
COMPARISON OF NEIGHBORHOOD SYSTEMS,0.3514689880304679,"in the self-labeling framework. If we use Potts
214"
COMPARISON OF NEIGHBORHOOD SYSTEMS,0.352557127312296,"model on nearest neighborhoods, the self-labeling
215"
COMPARISON OF NEIGHBORHOOD SYSTEMS,0.35364526659412404,"optimization should be applied and one should use
216"
COMPARISON OF NEIGHBORHOOD SYSTEMS,0.35473340587595215,"soft pseudo-labels instead of hard ones. Note that with proper optimization the advantage of the Potts
217"
COMPARISON OF NEIGHBORHOOD SYSTEMS,0.3558215451577802,"model on small neighborhood size can show up. In Figure 6, we also compare these approaches
218"
COMPARISON OF NEIGHBORHOOD SYSTEMS,0.35690968443960824,"across different scribble lengths.
219"
COMPARISON TO SOTA,0.35799782372143635,"3.5
Comparison to SOTA
220"
COMPARISON TO SOTA,0.3590859630032644,"0.0
0.2
0.4
0.6
0.8
1.0
ratio of scribble length 56 58 60 62 64 66 68 70 72"
COMPARISON TO SOTA,0.3601741022850925,mIoU on validation set
COMPARISON TO SOTA,0.36126224156692055,"Full
GD-dense-PBL
GD-grid-PBL
SL-grid-hard-PBL
SL-grid-soft-HCCE + PQ
SL-grid-soft-HCCE + PCD
SL-dense-hard-HCCE + PQ
SL-dense-soft-HCCE + PQ"
COMPARISON TO SOTA,0.36235038084874865,"Figure 6: Comparison of different methods
using Potts relaxations. The architecture is
DeeplabV3+ with the backbone MobileNetV2."
COMPARISON TO SOTA,0.3634385201305767,"In this section, we use a different network architec-
221"
COMPARISON TO SOTA,0.3645266594124048,"ture, ResNet101, to fairly compare our method with
222"
COMPARISON TO SOTA,0.36561479869423286,"the current state-of-the-art. We only compare the
223"
COMPARISON TO SOTA,0.36670293797606096,"results before applying any post-processing steps.
224"
COMPARISON TO SOTA,0.367791077257889,"The results are shown in Table 5. Note that our
225"
COMPARISON TO SOTA,0.36887921653971706,"results can outperform the fully-supervised method
226"
COMPARISON TO SOTA,0.36996735582154516,"when using 12 as the batch size. We also observe
227"
COMPARISON TO SOTA,0.3710554951033732,"that a larger batch size usually improves the results
228"
COMPARISON TO SOTA,0.3721436343852013,"quite a lot. Our results with 12 batch size can out-
229"
COMPARISON TO SOTA,0.37323177366702937,"perform several SOTA methods which use 16 batch
230"
COMPARISON TO SOTA,0.37431991294885747,"size.
231"
COMPARISON TO SOTA,0.3754080522306855,"Method
Architecture
Batchsize"
COMPARISON TO SOTA,0.3764961915125136,Optimization
COMPARISON TO SOTA,0.37758433079434167,"N
mIoU
GD
SL
hard
soft
Full supervision
Deeplab∗[12]
V3+
16
✓
-
-
-
78.9
Deeplab∗[12]
V3+
12
✓
-
-
-
76.6
Deeplab [11]
V2
12
✓
-
-
-
75.6
Scribble supervision
Architectural modification
BPG [39]
V2
10
✓
-
-
-
73.2
URSS [31]
V2
16
✓
-
-
-
74.6
SPML [20]
V2
16
✓
-
-
-
74.2
PSI [41]
V3+
-
-
-
✓
-
74.9
SEMINAR [9]
V3+
12
✓
-
-
-
76.2
TEL [25]
V3+
16
-
-
✓
-
77.1
Loss modification - Potts relaxations
ScribbleSup [26]
VGG16(V2)
8
-
✓
-
DN
63.1
DenseCRF loss∗[36]
V3+
12
✓
-
-
DN
75.8
GridCRF loss∗[27]
V3+
12
-
✓
-
NN
75.6
NonlocalCRF loss∗[38]
V3+
12
✓
-
-
SN
75.7
HCCE + PQ
V3+
12
-
-
✓
NN
77.5
HCCE + PCD
V3+
12
-
-
✓
NN
77.7
HCCE + PCD (no pretrain)
V3+
12
-
-
✓
NN
76.7
HCCE + PCD
V3+
16
-
-
✓
NN
78.1
HCCE + PCD (no pretrain)
V3+
16
-
-
✓
NN
77.6"
COMPARISON TO SOTA,0.3786724700761698,"Table 5: Comparison to SOTA methods (without CRF postprocessing) on scribble-supervised seg-
mentation. The numbers are mIoU on the validation dataset of Pascal VOC 2012 and use full-length
scribble. The backbone is ResNet101 unless stated otherwise. V2: deeplabV2. V3+: deeplabV3+.
N: neighborhood. “∗”: reproduced results. GD: gradient descent. SL: self-labeling. “no pretrain”
means the segmentation network is not pretrained using cross-entropy on scribbles."
CONCLUSIONS,0.3797606093579978,"4
Conclusions
232"
CONCLUSIONS,0.3808487486398259,"This paper proposed a convergent soft self-labeling framework based on a simple well-motivated loss
233"
CONCLUSIONS,0.381936887921654,"(6) for joint optimization of network predictions and soft pseudo-labels. The latter were motivated
234"
CONCLUSIONS,0.383025027203482,"as auxiliary optimization variables simplifying optimization of weakly-supervised loss (4). Our
235"
CONCLUSIONS,0.38411316648531013,"systematic evaluation of the cross-entropy and the Potts terms in self-labeling loss (6) provides
236"
CONCLUSIONS,0.3852013057671382,"clear recommendations based on the discussed conceptual advantages empirically confirmed by our
237"
CONCLUSIONS,0.3862894450489663,"experiments. Specifically, our work recommends the collision cross-entropy, log-quadratic Potts
238"
CONCLUSIONS,0.38737758433079433,"relaxations, and the earest-neighbor neighborhood. They achieve the best result that may even
239"
CONCLUSIONS,0.38846572361262244,"outperform the fully-supervised method with full pixel-precise masks. Our method does not require
240"
CONCLUSIONS,0.3895538628944505,"any modifications of the semantic segmentation models and it is easy to reproduce. Our general
241"
CONCLUSIONS,0.3906420021762786,"framework and empirical findings can be useful for other weakly-supervised segmentation problems
242"
CONCLUSIONS,0.39173014145810664,"(boxes, class tags, etc.).
243"
REFERENCES,0.3928182807399347,"References
244"
REFERENCES,0.3939064200217628,"[1] Multilabel random walker image segmentation using prior models. In 2005 IEEE computer
245"
REFERENCES,0.39499455930359084,"society conference on computer vision and pattern recognition (CVPR’05), pages 763–770.
246"
REFERENCES,0.39608269858541895,"IEEE, 2005.
247"
REFERENCES,0.397170837867247,"[2] Jiwoon Ahn and Suha Kwak. Learning pixel-level semantic affinity with image-level supervision
248"
REFERENCES,0.3982589771490751,"for weakly supervised semantic segmentation. In Proceedings of the IEEE conference on
249"
REFERENCES,0.39934711643090315,"computer vision and pattern recognition, pages 4981–4990, 2018.
250"
REFERENCES,0.40043525571273125,"[3] Jiwoon Ahn, Sunghyun Cho, and Suha Kwak. Weakly supervised learning of instance seg-
251"
REFERENCES,0.4015233949945593,"mentation with inter-pixel relations. In Proceedings of the IEEE/CVF conference on computer
252"
REFERENCES,0.40261153427638735,"vision and pattern recognition, pages 2209–2218, 2019.
253"
REFERENCES,0.40369967355821545,"[4] Nikita Araslanov and Stefan Roth. Single-stage semantic segmentation from image labels. In
254"
REFERENCES,0.4047878128400435,"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
255"
REFERENCES,0.4058759521218716,"4253–4262, 2020.
256"
REFERENCES,0.40696409140369966,"[5] Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press,
257"
REFERENCES,0.40805223068552776,"2004.
258"
REFERENCES,0.4091403699673558,"[6] Yuri Y Boykov and M-P Jolly. Interactive graph cuts for optimal boundary & region segmenta-
259"
REFERENCES,0.4102285092491839,"tion of objects in nd images. In Proceedings eighth IEEE international conference on computer
260"
REFERENCES,0.41131664853101196,"vision. ICCV 2001, pages 105–112. IEEE, 2001.
261"
REFERENCES,0.41240478781284007,"[7] John Bridle, Anthony Heading, and David MacKay. Unsupervised classifiers, mutual informa-
262"
REFERENCES,0.4134929270946681,"tion and’phantom targets. Advances in neural information processing systems, 4, 1991.
263"
REFERENCES,0.41458106637649617,"[8] Antonin Chambolle and Thomas Pock. A first-order primal-dual algorithm for convex problems
264"
REFERENCES,0.41566920565832427,"with applications to imaging. Journal of mathematical imaging and vision, 40:120–145, 2011.
265"
REFERENCES,0.4167573449401523,"[9] Hongjun Chen, Jinbao Wang, Hong Cai Chen, Xiantong Zhen, Feng Zheng, Rongrong Ji, and
266"
REFERENCES,0.4178454842219804,"Ling Shao. Seminar learning for click-level weakly supervised semantic segmentation. In
267"
REFERENCES,0.41893362350380847,"Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6920–6929,
268"
REFERENCES,0.4200217627856366,"2021.
269"
REFERENCES,0.4211099020674646,"[10] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille.
270"
REFERENCES,0.42219804134929273,"Semantic image segmentation with deep convolutional nets and fully connected crfs. arXiv
271"
REFERENCES,0.4232861806311208,"preprint arXiv:1412.7062, 2014.
272"
REFERENCES,0.4243743199129489,"[11] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille.
273"
REFERENCES,0.42546245919477693,"Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and
274"
REFERENCES,0.426550598476605,"fully connected crfs. IEEE transactions on pattern analysis and machine intelligence, 40(4):
275"
REFERENCES,0.4276387377584331,"834–848, 2017.
276"
REFERENCES,0.42872687704026113,"[12] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam.
277"
REFERENCES,0.42981501632208924,"Encoder-decoder with atrous separable convolution for semantic image segmentation. In
278"
REFERENCES,0.4309031556039173,"Proceedings of the European conference on computer vision (ECCV), pages 801–818, 2018.
279"
REFERENCES,0.4319912948857454,"[13] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo
280"
REFERENCES,0.43307943416757344,"Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic
281"
REFERENCES,0.43416757344940154,"urban scene understanding. In Proc. of the IEEE Conference on Computer Vision and Pattern
282"
REFERENCES,0.4352557127312296,"Recognition (CVPR), 2016.
283"
REFERENCES,0.4363438520130577,"[14] Camille Couprie, Leo Grady, Laurent Najman, and Hugues Talbot. Power watershed: A
284"
REFERENCES,0.43743199129488575,"unifying graph-based optimization framework. IEEE transactions on pattern analysis and
285"
REFERENCES,0.4385201305767138,"machine intelligence, 33(7):1384–1399, 2010.
286"
REFERENCES,0.4396082698585419,"[15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-
287"
REFERENCES,0.44069640914036995,"scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern
288"
REFERENCES,0.44178454842219805,"recognition, pages 248–255. Ieee, 2009.
289"
REFERENCES,0.4428726877040261,"[16] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman.
290"
REFERENCES,0.4439608269858542,"The pascal visual object classes (voc) challenge. International journal of computer vision, 88:
291"
REFERENCES,0.44504896626768226,"303–308, 2009.
292"
REFERENCES,0.44613710554951036,"[17] Leo Grady. Random walks for image segmentation. IEEE transactions on pattern analysis and
293"
REFERENCES,0.4472252448313384,"machine intelligence, 28(11):1768–1783, 2006.
294"
REFERENCES,0.44831338411316646,"[18] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization.
295"
REFERENCES,0.44940152339499456,"Advances in neural information processing systems, 17, 2004.
296"
REFERENCES,0.4504896626768226,"[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
297"
REFERENCES,0.4515778019586507,"recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
298"
REFERENCES,0.45266594124047876,"pages 770–778, 2016.
299"
REFERENCES,0.45375408052230687,"[20] Tsung-Wei Ke, Jyh-Jing Hwang, and Stella X Yu. Universal weakly supervised segmentation
300"
REFERENCES,0.4548422198041349,"by pixel-to-segment contrastive learning. arXiv preprint arXiv:2105.00957, 2021.
301"
REFERENCES,0.455930359085963,"[21] Alexander Kolesnikov and Christoph H Lampert. Seed, expand and constrain: Three principles
302"
REFERENCES,0.45701849836779107,"for weakly-supervised image segmentation. In Computer Vision–ECCV 2016: 14th European
303"
REFERENCES,0.4581066376496192,"Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part IV 14, pages
304"
REFERENCES,0.4591947769314472,"695–711. Springer, 2016.
305"
REFERENCES,0.4602829162132753,"[22] Philipp Krähenbühl and Vladlen Koltun. Efficient inference in fully connected CRFs with
306"
REFERENCES,0.4613710554951034,"Gaussian edge potentials. Advances in neural information processing systems, 24, 2011.
307"
REFERENCES,0.4624591947769314,"[23] V. Kulharia, S. Chandra, A. Agrawal, P. Torr, and A. Tyagi. Box2seg: Attention weighted loss
308"
REFERENCES,0.46354733405875953,"and discriminative feature learning for weakly supervised segmentation. In ECCV’20.
309"
REFERENCES,0.4646354733405876,"[24] Jungbeom Lee, Eunji Kim, Sungmin Lee, Jangho Lee, and Sungroh Yoon. Ficklenet: Weakly
310"
REFERENCES,0.4657236126224157,"and semi-supervised semantic image segmentation using stochastic inference. In Proceedings
311"
REFERENCES,0.46681175190424373,"of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5267–5276,
312"
REFERENCES,0.46789989118607184,"2019.
313"
REFERENCES,0.4689880304678999,"[25] Zhiyuan Liang, Tiancai Wang, Xiangyu Zhang, Jian Sun, and Jianbing Shen. Tree energy
314"
REFERENCES,0.470076169749728,"loss: Towards sparsely annotated semantic segmentation. In Proceedings of the IEEE/CVF
315"
REFERENCES,0.47116430903155604,"Conference on Computer Vision and Pattern Recognition, pages 16907–16916, 2022.
316"
REFERENCES,0.4722524483133841,"[26] Di Lin, Jifeng Dai, Jiaya Jia, Kaiming He, and Jian Sun. Scribblesup: Scribble-supervised
317"
REFERENCES,0.4733405875952122,"convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on
318"
REFERENCES,0.47442872687704024,"computer vision and pattern recognition, pages 3159–3167, 2016.
319"
REFERENCES,0.47551686615886835,"[27] Dmitrii Marin and Yuri Boykov. Robust trust region for weakly supervised segmentation. In
320"
REFERENCES,0.4766050054406964,"Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6608–6618,
321"
REFERENCES,0.4776931447225245,"2021.
322"
REFERENCES,0.47878128400435255,"[28] Dmitrii Marin, Meng Tang, Ismail Ben Ayed, and Yuri Boykov. Beyond gradient descent for
323"
REFERENCES,0.47986942328618065,"regularized segmentation losses. In Proceedings of the IEEE/CVF Conference on Computer
324"
REFERENCES,0.4809575625680087,"Vision and Pattern Recognition, pages 10187–10196, 2019.
325"
REFERENCES,0.4820457018498368,"[29] Rafael Müller, Simon Kornblith, and Geoffrey E Hinton. When does label smoothing help?
326"
REFERENCES,0.48313384113166485,"Advances in neural information processing systems, 32, 2019.
327"
REFERENCES,0.4842219804134929,"[30] NSD.
Natural Scenes Dataset [NSD].
https://www.kaggle.com/datasets/
328"
REFERENCES,0.485310119695321,"nitishabharathi/scene-classification, 2020.
329"
REFERENCES,0.48639825897714906,"[31] Zhiyi Pan, Peng Jiang, Yunhai Wang, Changhe Tu, and Anthony G Cohn. Scribble-supervised
330"
REFERENCES,0.48748639825897716,"semantic segmentation by uncertainty reduction on neural representation and self-supervision
331"
REFERENCES,0.4885745375408052,"on neural eigenspace. In Proceedings of the IEEE/CVF International Conference on Computer
332"
REFERENCES,0.4896626768226333,"Vision, pages 7416–7425, 2021.
333"
REFERENCES,0.49075081610446136,"[32] Thomas Pock, Antonin Chambolle, Daniel Cremers, and Horst Bischof. A convex relaxation
334"
REFERENCES,0.49183895538628947,"approach for computing minimal partitions. In IEEE Conference on Computer Vision and
335"
REFERENCES,0.4929270946681175,"Pattern Recognition, pages 810–817, 2009.
336"
REFERENCES,0.4940152339499456,"[33] Pradeep Ravikumar and John Lafferty. Quadratic programming relaxations for metric labeling
337"
REFERENCES,0.49510337323177367,"and Markov Random Field MAP estimation. In The 23rd International Conference on Machine
338"
REFERENCES,0.4961915125136017,"Learning, page 737–744, 2006.
339"
REFERENCES,0.4972796517954298,"[34] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.
340"
REFERENCES,0.49836779107725787,"Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference
341"
REFERENCES,0.499455930359086,"on computer vision and pattern recognition, pages 4510–4520, 2018.
342"
REFERENCES,0.500544069640914,"[35] Meng Tang, Abdelaziz Djelouah, Federico Perazzi, Yuri Boykov, and Christopher Schroers.
343"
REFERENCES,0.5016322089227421,"Normalized cut loss for weakly-supervised cnn segmentation. In Proceedings of the IEEE
344"
REFERENCES,0.5027203482045702,"conference on computer vision and pattern recognition, pages 1818–1827, 2018.
345"
REFERENCES,0.5038084874863983,"[36] Meng Tang, Federico Perazzi, Abdelaziz Djelouah, Ismail Ben Ayed, Christopher Schroers, and
346"
REFERENCES,0.5048966267682263,"Yuri Boykov. On regularized losses for weakly-supervised cnn segmentation. In Proceedings of
347"
REFERENCES,0.5059847660500544,"the European Conference on Computer Vision (ECCV), pages 507–522, 2018.
348"
REFERENCES,0.5070729053318824,"[37] Olga Veksler. Efficient graph cut optimization for full crfs with quantized edges. IEEE
349"
REFERENCES,0.5081610446137106,"transactions on pattern analysis and machine intelligence, 42(4):1005–1012, 2019.
350"
REFERENCES,0.5092491838955386,"[38] Olga Veksler and Yuri Boykov.
Sparse non-local crf.
In Proceedings of the IEEE/CVF
351"
REFERENCES,0.5103373231773667,"Conference on Computer Vision and Pattern Recognition, pages 4493–4503, 2022.
352"
REFERENCES,0.5114254624591947,"[39] Bin Wang, Guojun Qi, Sheng Tang, Tianzhu Zhang, Yunchao Wei, Linghui Li, and Yongdong
353"
REFERENCES,0.5125136017410229,"Zhang. Boundary perception guidance: A scribble-supervised semantic segmentation approach.
354"
REFERENCES,0.5136017410228509,"In IJCAI International joint conference on artificial intelligence, 2019.
355"
REFERENCES,0.514689880304679,"[40] Changwei Wang, Rongtao Xu, Shibiao Xu, Weiliang Meng, and Xiaopeng Zhang. Treating
356"
REFERENCES,0.515778019586507,"pseudo-labels generation as image matting for weakly supervised semantic segmentation. In
357"
REFERENCES,0.5168661588683352,"Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 755–765,
358"
REFERENCES,0.5179542981501633,"2023.
359"
REFERENCES,0.5190424374319913,"[41] Jingshan Xu, Chuanwei Zhou, Zhen Cui, Chunyan Xu, Yuge Huang, Pengcheng Shen, Shaoxin
360"
REFERENCES,0.5201305767138193,"Li, and Jian Yang. Scribble-supervised semantic segmentation inference. In Proceedings of the
361"
REFERENCES,0.5212187159956474,"IEEE/CVF International Conference on Computer Vision, pages 15354–15363, 2021.
362"
REFERENCES,0.5223068552774756,"[42] Christopher Zach, Christian Häne, and Marc Pollefeys. What is optimized in tight convex
363"
REFERENCES,0.5233949945593036,"relaxations for multi-label problems? In IEEE Conference on Computer Vision and Pattern
364"
REFERENCES,0.5244831338411317,"Recognition, pages 1664–1671, 2012.
365"
REFERENCES,0.5255712731229597,"[43] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba.
366"
REFERENCES,0.5266594124047879,"Scene parsing through ade20k dataset. In Proceedings of the IEEE conference on computer
367"
REFERENCES,0.5277475516866159,"vision and pattern recognition, pages 633–641, 2017.
368"
REFERENCES,0.528835690968444,"[44] Xiaojin Zhu and Zoubin Ghahramani. Learning from labeled and unlabeled data with label
369"
REFERENCES,0.529923830250272,"propagation. ProQuest Number: INFORMATION TO ALL USERS, 2002.
370"
REFERENCES,0.5310119695321001,"A
Self-labeling and hard pseudo-labels
371"
REFERENCES,0.5321001088139282,"One argument motivating self-labeling approaches to weakly-supervised segmentation comes from
372"
REFERENCES,0.5331882480957563,"well-known limitations of gradient descent when optimizing the Potts relaxatons, e.g. [28]. But even
373"
REFERENCES,0.5342763873775843,"when using convex Potts relaxations [17, 32, 8], they are combined with the concave entropy term in
374"
REFERENCES,0.5353645266594124,"(4) making their optimization challenging.
375"
REFERENCES,0.5364526659412405,"Typical self-labeling methods, including one of the first works on scribble-based semantic segmenta-
376"
REFERENCES,0.5375408052230686,"tion [26], introduce a sub-problem focused on the estimation of pseudo-labels over unlabeled points,
377"
REFERENCES,0.5386289445048966,"separately from the network training by such labels. Pseudo-labeling is typically done by optimiza-
378"
REFERENCES,0.5397170837867247,"tion algorithms or heuristics balancing unsupervised or self-supervised criteria, e.g. the Potts, and
379"
REFERENCES,0.5408052230685527,"proximity to current predictions. Then, network fine-tuning from pseudo-labels and pseudo-labeling
380"
REFERENCES,0.5418933623503809,"steps are iterated.
381"
REFERENCES,0.5429815016322089,"We denote pseudo-labels yi slightly differently from the ground truth labels ¯yi by omitting the
382"
REFERENCES,0.544069640914037,"bar. It is important to distinguish them since the ground truth labels ¯yi for i ∈S are given, while
383"
REFERENCES,0.545157780195865,"the pseudo-labels yi for i ∈Ω\S are estimated. The majority of existing self-labeling methods
384"
REFERENCES,0.5462459194776932,"[26, 2, 28, 3, 24, 27, 40] estimate hard pseudo-labels, which could be equivalently represented either
385"
REFERENCES,0.5473340587595212,"by class indices
386"
REFERENCES,0.5484221980413493,"\ lab e l  { eq:label_index} y_i\in \{1,\dots ,K\} 
(10)"
REFERENCES,0.5495103373231773,"or by the corresponding one-hot categorical distributions
387"
REFERENCES,0.5505984766050055,"\
lab
e l  { e q :l
a b e l_
one
hot
} 
y _i  \ ; \;\ e qui v \;\;(y_i^1,\dots ,y_i^K)\,\in \Delta ^K_{0,1} \quad \quad \quad \text {for}\quad y_i^k := \left [k=y_i\right ]\;\in \{0,1\} 
(11)"
REFERENCES,0.5516866158868335,"analogously with the hard ground truth labels in (1) and (2). In part, hard pseudo-labels are motivated
388"
REFERENCES,0.5527747551686616,"by the network training where the default is NLL loss (3) assuming discrete labels. Besides, there are
389"
REFERENCES,0.5538628944504896,"powerful discrete solvers for the Potts model [6, 32, 8]. We discuss the potential advantages of soft
390"
REFERENCES,0.5549510337323177,"pseudo-labels in the next Section 1.2.
391"
REFERENCES,0.5560391730141458,"Joint loss vs “proposal generation”: The majority of self-labeling approaches can be divided into
392"
REFERENCES,0.5571273122959739,"two groups. One group designs pseudo-labeling and the network training sup-problems that are not
393"
REFERENCES,0.558215451577802,"formally related, e.g. [26, 25, 41]. While pseudo-labeling typically depends on the current network
394"
REFERENCES,0.55930359085963,"predictions and the network fine-tuning uses such pseudo-labels, the lack of a formal relation between
395"
REFERENCES,0.5603917301414582,"these sub-problems implies that iterating such steps does not guarantee any form of convergence.
396"
REFERENCES,0.5614798694232862,"Such methods are often referred to as proposal generation heuristics.
397"
REFERENCES,0.5625680087051143,"Alternatively, the pseudo-labeling sub-problem and the network training sub-problem can be formally
398"
REFERENCES,0.5636561479869423,"derived from a weakly-supervised loss like (4), e.g. by ADM splitting [28] or as high-order trust-
399"
REFERENCES,0.5647442872687704,"region method [27]. Such methods often formulate a joint loss function w.r.t network predictions
400"
REFERENCES,0.5658324265505985,"and pseudo-labels and iteratively optimize it in a convergent manner that is guaranteed to decrease
401"
REFERENCES,0.5669205658324266,"the loss. We consider this group of self-labeling methods as better motivated, more principled, and
402"
REFERENCES,0.5680087051142546,"numerically safer.
403"
REFERENCES,0.5690968443960827,"B
Optimization Algorithm
404"
REFERENCES,0.5701849836779108,"In this section, we will focus on the optimization of (6) in steps iterating optimization of y and σ.
405"
REFERENCES,0.5712731229597389,"The network parameters are optimized by standard stochastic gradient descent in all our experiments.
406"
REFERENCES,0.5723612622415669,"Pseudo-labels are also estimated online using a mini-batch. To solve y at given σ, it is a large-scale
407"
REFERENCES,0.573449401523395,"constrained convex problem. While there are existing general solvers to find global optima, such
408"
REFERENCES,0.5745375408052231,"as projected gradient descent, it is often too slow for practical usage. Instead, we reformulate our
409"
REFERENCES,0.5756256800870512,"problem to avoid the simplex constraints so that we can use standard gradient descent in PyTorch
410"
REFERENCES,0.5767138193688792,"library accelerated by GPU. Specifically, instead of directly optimizing y, we optimize a set of new
411"
REFERENCES,0.5778019586507073,"variables {li ∈RK, i ∈Ω} where yi is computed by softmax(li). Now, the simplex constraint on
412"
REFERENCES,0.5788900979325353,"y will be automatically satisfied. Note that the hard constraints on scribble regions still need to be
413"
REFERENCES,0.5799782372143635,"considered because the interaction with unlabeled regions through pairwise terms will influence the
414"
REFERENCES,0.5810663764961915,"optimization process. Inspired by [44], we can reset softmax(li) where i ∈S back to the ground
415"
REFERENCES,0.5821545157780196,"truth at the beginning of each step of the gradient descent.
416"
REFERENCES,0.5832426550598476,"However, the original convex problem now becomes non-convex due to the Softmax operation. Thus,
417"
REFERENCES,0.5843307943416758,"initialization is important to help find better local minima or even the global optima. Empirically, we
418"
REFERENCES,0.5854189336235038,"observed that the network output logit can be a fairly good initialization. The quantitative comparison
419"
REFERENCES,0.5865070729053319,Image & scribble
REFERENCES,0.5875952121871599,"GT mask
predictions σ"
REFERENCES,0.588683351468988,entropy H(σ)
REFERENCES,0.5897714907508161,"input: image and
network predictions σ"
REFERENCES,0.5908596300326442,"without log
with log"
REFERENCES,0.5919477693144722,"quadratic
bilinear
n-quadratic"
REFERENCES,0.5930359085963003,"pseudo-label y
pseudo-label y
entropy H(y)
entropy H(y)"
REFERENCES,0.5941240478781284,Potts relaxations and optimal pseudo-labels y
REFERENCES,0.5952121871599565,"(a) Image, GT & input"
REFERENCES,0.5963003264417845,(b) Pseudo-labels using different Potts relaxation
REFERENCES,0.5973884657236126,"Figure 7: Illustration of the difference among Potts relaxations. The visualization of soft pseudo-
labels uses the convex combination of RGB colors for each class weighted by pseudo-label itself."
REFERENCES,0.5984766050054406,"uses a special quadratic formulation where closed-form solution and efficient solver [1, 17] exist.
420"
REFERENCES,0.5995647442872688,"We compute the standard soft Jaccard index for the pseudo-labels between the solutions given by
421"
REFERENCES,0.6006528835690969,"our solver and the global optima. The soft Jaccard index is 99.2% on average over 100 images.
422"
REFERENCES,0.6017410228509249,"Furthermore, our experimental results for all other formulations in Figure 7, 5, and Section 3 confirm
423"
REFERENCES,0.602829162132753,"the effectiveness of our optimization solver. In all experiments, the number of gradient descent steps
424"
REFERENCES,0.6039173014145811,"for solving y is 200 and the corresponding learning rate is 0.075. To test the robustness of the number
425"
REFERENCES,0.6050054406964092,"of steps here, we decreased 200 to 100 and the mIoU on the validation set just dropped from 71.05
426"
REFERENCES,0.6060935799782372,"by 0.72. This indicates that we can significantly accelerate the training without much sacrifice of
427"
REFERENCES,0.6071817192600653,"accuracy. When using 200 steps, the total time for the training will be about 3 times longer than the
428"
REFERENCES,0.6082698585418934,"SGD with dense Potts [36].
429"
REFERENCES,0.6093579978237215,"C
Experimental settings
430"
REFERENCES,0.6104461371055495,"Dataset and evaluation
We mainly use the standard PASCAL VOC 2012 dataset [16] and scribble-
431"
REFERENCES,0.6115342763873776,"based annotations for supervision [26]. The dataset contains 21 classes including background.
432"
REFERENCES,0.6126224156692056,"Following the common practice [10, 35, 36], we use the augmented version which has 10,582 training
433"
REFERENCES,0.6137105549510338,"images and 1449 images for validation. We employ the standard mean Intersection-over-Union
434"
REFERENCES,0.6147986942328618,"(mIoU) on validation set as the evaluation metric. We also test our method on two additional datasets
435"
REFERENCES,0.6158868335146899,"in Section 3.5. One is Cityscapes [13] which is built for urban scenes and consists of 2975 and 500
436"
REFERENCES,0.6169749727965179,"fine-labeled images for training and validation. There are 19 out of 30 annotated classes for semantic
437"
REFERENCES,0.6180631120783461,"segmentation. The other one is ADE20k [43] which has 150 fine-grained classes. There are 20210
438"
REFERENCES,0.6191512513601741,"and 2000, images for training and validation. Instead of scribble-based supervision, we followed [25]
439"
REFERENCES,0.6202393906420022,"to use the block-wise annotation as a form of weak supervision.
440"
REFERENCES,0.6213275299238302,"Implementation details
We adpoted DeepLabv3+ [12] framework with two backbones, ResNet101
441"
REFERENCES,0.6224156692056583,"[19] and MobileNetV2 [34]. We use ResNet101 in Section 3.5, and use MobileNetV2 in other
442"
REFERENCES,0.6235038084874864,"sections for efficiency. All backbone networks (ResNet-101 and MobileNetV2) are pre-trained on
443"
REFERENCES,0.6245919477693145,"Imagenet [15]. Unless stated explicitly, we use batch 12 as the default across all the experiments.
444"
REFERENCES,0.6256800870511425,"Following [35], we adopt two-stage training, unless otherwise stated, where only the cross-entropy
445"
REFERENCES,0.6267682263329706,"loss on scribbles is used in the first stage. The optimizer for network parameters is SGD. The learning
446"
REFERENCES,0.6278563656147987,"rate is scheduled by a polynomial decay with a power of 0.9. Initial learning is set to 0.007 in the first
447"
REFERENCES,0.6289445048966268,"stage and 0.0007 in the second phase. 60 epochs are used to train the model with different losses
448"
REFERENCES,0.6300326441784548,"where hyperparameters are tuned separately for them. For our best result, we use η = 0.3, λ = 6,
449"
REFERENCES,0.6311207834602829,"HCCE and PCD. The color intensity bandwidth in the Potts model is set to 9 across all the experiments
450"
REFERENCES,0.6322089227421109,"on Pascal VOC 2012 and 3 for Cityscapes and ADE20k datasets.
451"
REFERENCES,0.6332970620239391,"Method
Architecture
Cityscapes
ADE20k
Full supervision
Deeplab [12]
V3+
80.2
44.6
Block-scribble supervision
DenseCRF loss [36]
V3+
69.3
37.4
GridCRF loss∗[27]
V3+
69.5
37.7
TEL [25]
V3+
71.5
39.2
HCCE + PCD
V3+
72.4
39.7"
REFERENCES,0.6343852013057671,"Table 6: Comparison to SOTA methods (without CRF postprocessing) on segmentation with block-
scribble supervision. The numbers are mIoU on the validation dataset of cityscapes [13] and ADE20k
[43] and use 50% of full annotations for supervision following [25]. The backbone is ResNet101.
“∗”: reproduced results. All methods are trained in a single-stage fashion."
REFERENCES,0.6354733405875952,"NeurIPS Paper Checklist
452"
REFERENCES,0.6365614798694232,"The checklist is designed to encourage best practices for responsible machine learning research,
453"
REFERENCES,0.6376496191512514,"addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove
454"
REFERENCES,0.6387377584330794,"the checklist: The papers not including the checklist will be desk rejected. The checklist should
455"
REFERENCES,0.6398258977149075,"follow the references and follow the (optional) supplemental material. The checklist does NOT count
456"
REFERENCES,0.6409140369967355,"towards the page limit.
457"
REFERENCES,0.6420021762785637,"Please read the checklist guidelines carefully for information on how to answer these questions. For
458"
REFERENCES,0.6430903155603918,"each question in the checklist:
459"
REFERENCES,0.6441784548422198,"• You should answer [Yes] , [No] , or [NA] .
460"
REFERENCES,0.6452665941240479,"• [NA] means either that the question is Not Applicable for that particular paper or the
461"
REFERENCES,0.6463547334058759,"relevant information is Not Available.
462"
REFERENCES,0.6474428726877041,"• Please provide a short (1–2 sentence) justification right after your answer (even for NA).
463"
REFERENCES,0.6485310119695321,"The checklist answers are an integral part of your paper submission. They are visible to the
464"
REFERENCES,0.6496191512513602,"reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it
465"
REFERENCES,0.6507072905331882,"(after eventual revisions) with the final version of your paper, and its final version will be published
466"
REFERENCES,0.6517954298150164,"with the paper.
467"
REFERENCES,0.6528835690968444,"The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.
468"
REFERENCES,0.6539717083786725,"While ""[Yes] "" is generally preferable to ""[No] "", it is perfectly acceptable to answer ""[No] "" provided a
469"
REFERENCES,0.6550598476605005,"proper justification is given (e.g., ""error bars are not reported because it would be too computationally
470"
REFERENCES,0.6561479869423286,"expensive"" or ""we were unable to find the license for the dataset we used""). In general, answering
471"
REFERENCES,0.6572361262241567,"""[No] "" or ""[NA] "" is not grounds for rejection. While the questions are phrased in a binary way, we
472"
REFERENCES,0.6583242655059848,"acknowledge that the true answer is often more nuanced, so please just use your best judgment and
473"
REFERENCES,0.6594124047878128,"write a justification to elaborate. All supporting evidence can appear either in the main paper or the
474"
REFERENCES,0.6605005440696409,"supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification
475"
REFERENCES,0.661588683351469,"please point to the section(s) where related material for the question can be found.
476"
REFERENCES,0.6626768226332971,"IMPORTANT, please:
477"
REFERENCES,0.6637649619151251,"• Delete this instruction block, but keep the section heading “NeurIPS paper checklist"",
478"
REFERENCES,0.6648531011969532,"• Keep the checklist subsection headings, questions/answers and guidelines below.
479"
REFERENCES,0.6659412404787813,"• Do not modify the questions and only use the provided macros for your answers.
480"
CLAIMS,0.6670293797606094,"1. Claims
481"
CLAIMS,0.6681175190424374,"Question: Do the main claims made in the abstract and introduction accurately reflect the
482"
CLAIMS,0.6692056583242655,"paper’s contributions and scope?
483"
CLAIMS,0.6702937976060935,"Answer: [Yes]
484"
CLAIMS,0.6713819368879217,"Justification:
485"
CLAIMS,0.6724700761697497,"Guidelines:
486"
CLAIMS,0.6735582154515778,"• The answer NA means that the abstract and introduction do not include the claims
487"
CLAIMS,0.6746463547334058,"made in the paper.
488"
CLAIMS,0.675734494015234,"• The abstract and/or introduction should clearly state the claims made, including the
489"
CLAIMS,0.676822633297062,"contributions made in the paper and important assumptions and limitations. A No or
490"
CLAIMS,0.6779107725788901,"NA answer to this question will not be perceived well by the reviewers.
491"
CLAIMS,0.6789989118607181,"• The claims made should match theoretical and experimental results, and reflect how
492"
CLAIMS,0.6800870511425462,"much the results can be expected to generalize to other settings.
493"
CLAIMS,0.6811751904243744,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
494"
CLAIMS,0.6822633297062024,"are not attained by the paper.
495"
LIMITATIONS,0.6833514689880305,"2. Limitations
496"
LIMITATIONS,0.6844396082698585,"Question: Does the paper discuss the limitations of the work performed by the authors?
497"
LIMITATIONS,0.6855277475516867,"Answer: [No]
498"
LIMITATIONS,0.6866158868335147,"Justification: The training time is longer and more details can be found in the end of
499"
LIMITATIONS,0.6877040261153428,"Appendix B.
500"
LIMITATIONS,0.6887921653971708,"Guidelines:
501"
LIMITATIONS,0.6898803046789989,"• The answer NA means that the paper has no limitation while the answer No means that
502"
LIMITATIONS,0.690968443960827,"the paper has limitations, but those are not discussed in the paper.
503"
LIMITATIONS,0.6920565832426551,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
504"
LIMITATIONS,0.6931447225244831,"• The paper should point out any strong assumptions and how robust the results are to
505"
LIMITATIONS,0.6942328618063112,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
506"
LIMITATIONS,0.6953210010881393,"model well-specification, asymptotic approximations only holding locally). The authors
507"
LIMITATIONS,0.6964091403699674,"should reflect on how these assumptions might be violated in practice and what the
508"
LIMITATIONS,0.6974972796517954,"implications would be.
509"
LIMITATIONS,0.6985854189336235,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
510"
LIMITATIONS,0.6996735582154516,"only tested on a few datasets or with a few runs. In general, empirical results often
511"
LIMITATIONS,0.7007616974972797,"depend on implicit assumptions, which should be articulated.
512"
LIMITATIONS,0.7018498367791077,"• The authors should reflect on the factors that influence the performance of the approach.
513"
LIMITATIONS,0.7029379760609358,"For example, a facial recognition algorithm may perform poorly when image resolution
514"
LIMITATIONS,0.7040261153427638,"is low or images are taken in low lighting. Or a speech-to-text system might not be
515"
LIMITATIONS,0.705114254624592,"used reliably to provide closed captions for online lectures because it fails to handle
516"
LIMITATIONS,0.70620239390642,"technical jargon.
517"
LIMITATIONS,0.7072905331882481,"• The authors should discuss the computational efficiency of the proposed algorithms
518"
LIMITATIONS,0.7083786724700761,"and how they scale with dataset size.
519"
LIMITATIONS,0.7094668117519043,"• If applicable, the authors should discuss possible limitations of their approach to
520"
LIMITATIONS,0.7105549510337323,"address problems of privacy and fairness.
521"
LIMITATIONS,0.7116430903155604,"• While the authors might fear that complete honesty about limitations might be used by
522"
LIMITATIONS,0.7127312295973884,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
523"
LIMITATIONS,0.7138193688792165,"limitations that aren’t acknowledged in the paper. The authors should use their best
524"
LIMITATIONS,0.7149075081610446,"judgment and recognize that individual actions in favor of transparency play an impor-
525"
LIMITATIONS,0.7159956474428727,"tant role in developing norms that preserve the integrity of the community. Reviewers
526"
LIMITATIONS,0.7170837867247007,"will be specifically instructed to not penalize honesty concerning limitations.
527"
THEORY ASSUMPTIONS AND PROOFS,0.7181719260065288,"3. Theory Assumptions and Proofs
528"
THEORY ASSUMPTIONS AND PROOFS,0.719260065288357,"Question: For each theoretical result, does the paper provide the full set of assumptions and
529"
THEORY ASSUMPTIONS AND PROOFS,0.720348204570185,"a complete (and correct) proof?
530"
THEORY ASSUMPTIONS AND PROOFS,0.721436343852013,"Answer: [NA]
531"
THEORY ASSUMPTIONS AND PROOFS,0.7225244831338411,"Justification:
532"
THEORY ASSUMPTIONS AND PROOFS,0.7236126224156693,"Guidelines:
533"
THEORY ASSUMPTIONS AND PROOFS,0.7247007616974973,"• The answer NA means that the paper does not include theoretical results.
534"
THEORY ASSUMPTIONS AND PROOFS,0.7257889009793254,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
535"
THEORY ASSUMPTIONS AND PROOFS,0.7268770402611534,"referenced.
536"
THEORY ASSUMPTIONS AND PROOFS,0.7279651795429815,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
537"
THEORY ASSUMPTIONS AND PROOFS,0.7290533188248096,"• The proofs can either appear in the main paper or the supplemental material, but if
538"
THEORY ASSUMPTIONS AND PROOFS,0.7301414581066377,"they appear in the supplemental material, the authors are encouraged to provide a short
539"
THEORY ASSUMPTIONS AND PROOFS,0.7312295973884657,"proof sketch to provide intuition.
540"
THEORY ASSUMPTIONS AND PROOFS,0.7323177366702938,"• Inversely, any informal proof provided in the core of the paper should be complemented
541"
THEORY ASSUMPTIONS AND PROOFS,0.7334058759521219,"by formal proofs provided in appendix or supplemental material.
542"
THEORY ASSUMPTIONS AND PROOFS,0.73449401523395,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
543"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.735582154515778,"4. Experimental Result Reproducibility
544"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7366702937976061,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
545"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7377584330794341,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
546"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7388465723612623,"of the paper (regardless of whether the code and data are provided or not)?
547"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7399347116430903,"Answer: [Yes]
548"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7410228509249184,"Justification: All the details are given in the Appendix C.
549"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7421109902067464,"Guidelines:
550"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7431991294885746,"• The answer NA means that the paper does not include experiments.
551"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7442872687704026,"• If the paper includes experiments, a No answer to this question will not be perceived
552"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7453754080522307,"well by the reviewers: Making the paper reproducible is important, regardless of
553"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7464635473340587,"whether the code and data are provided or not.
554"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7475516866158868,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
555"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7486398258977149,"to make their results reproducible or verifiable.
556"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.749727965179543,"• Depending on the contribution, reproducibility can be accomplished in various ways.
557"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.750816104461371,"For example, if the contribution is a novel architecture, describing the architecture fully
558"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7519042437431991,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
559"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7529923830250272,"be necessary to either make it possible for others to replicate the model with the same
560"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7540805223068553,"dataset, or provide access to the model. In general. releasing code and data is often
561"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7551686615886833,"one good way to accomplish this, but reproducibility can also be provided via detailed
562"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7562568008705114,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
563"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7573449401523396,"of a large language model), releasing of a model checkpoint, or other means that are
564"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7584330794341676,"appropriate to the research performed.
565"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7595212187159956,"• While NeurIPS does not require releasing code, the conference does require all submis-
566"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7606093579978237,"sions to provide some reasonable avenue for reproducibility, which may depend on the
567"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7616974972796517,"nature of the contribution. For example
568"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7627856365614799,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
569"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.763873775843308,"to reproduce that algorithm.
570"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.764961915125136,"(b) If the contribution is primarily a new model architecture, the paper should describe
571"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.766050054406964,"the architecture clearly and fully.
572"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7671381936887922,"(c) If the contribution is a new model (e.g., a large language model), then there should
573"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7682263329706203,"either be a way to access this model for reproducing the results or a way to reproduce
574"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7693144722524483,"the model (e.g., with an open-source dataset or instructions for how to construct
575"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7704026115342764,"the dataset).
576"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7714907508161044,"(d) We recognize that reproducibility may be tricky in some cases, in which case
577"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7725788900979326,"authors are welcome to describe the particular way they provide for reproducibility.
578"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7736670293797606,"In the case of closed-source models, it may be that access to the model is limited in
579"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7747551686615887,"some way (e.g., to registered users), but it should be possible for other researchers
580"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7758433079434167,"to have some path to reproducing or verifying the results.
581"
OPEN ACCESS TO DATA AND CODE,0.7769314472252449,"5. Open access to data and code
582"
OPEN ACCESS TO DATA AND CODE,0.7780195865070729,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
583"
OPEN ACCESS TO DATA AND CODE,0.779107725788901,"tions to faithfully reproduce the main experimental results, as described in supplemental
584"
OPEN ACCESS TO DATA AND CODE,0.780195865070729,"material?
585"
OPEN ACCESS TO DATA AND CODE,0.7812840043525572,"Answer: [No]
586"
OPEN ACCESS TO DATA AND CODE,0.7823721436343852,"Justification: The code will be released upon acceptance.
587"
OPEN ACCESS TO DATA AND CODE,0.7834602829162133,"Guidelines:
588"
OPEN ACCESS TO DATA AND CODE,0.7845484221980413,"• The answer NA means that paper does not include experiments requiring code.
589"
OPEN ACCESS TO DATA AND CODE,0.7856365614798694,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
590"
OPEN ACCESS TO DATA AND CODE,0.7867247007616975,"public/guides/CodeSubmissionPolicy) for more details.
591"
OPEN ACCESS TO DATA AND CODE,0.7878128400435256,"• While we encourage the release of code and data, we understand that this might not be
592"
OPEN ACCESS TO DATA AND CODE,0.7889009793253536,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
593"
OPEN ACCESS TO DATA AND CODE,0.7899891186071817,"including code, unless this is central to the contribution (e.g., for a new open-source
594"
OPEN ACCESS TO DATA AND CODE,0.7910772578890098,"benchmark).
595"
OPEN ACCESS TO DATA AND CODE,0.7921653971708379,"• The instructions should contain the exact command and environment needed to run to
596"
OPEN ACCESS TO DATA AND CODE,0.7932535364526659,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
597"
OPEN ACCESS TO DATA AND CODE,0.794341675734494,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
598"
OPEN ACCESS TO DATA AND CODE,0.795429815016322,"• The authors should provide instructions on data access and preparation, including how
599"
OPEN ACCESS TO DATA AND CODE,0.7965179542981502,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
600"
OPEN ACCESS TO DATA AND CODE,0.7976060935799782,"• The authors should provide scripts to reproduce all experimental results for the new
601"
OPEN ACCESS TO DATA AND CODE,0.7986942328618063,"proposed method and baselines. If only a subset of experiments are reproducible, they
602"
OPEN ACCESS TO DATA AND CODE,0.7997823721436343,"should state which ones are omitted from the script and why.
603"
OPEN ACCESS TO DATA AND CODE,0.8008705114254625,"• At submission time, to preserve anonymity, the authors should release anonymized
604"
OPEN ACCESS TO DATA AND CODE,0.8019586507072906,"versions (if applicable).
605"
OPEN ACCESS TO DATA AND CODE,0.8030467899891186,"• Providing as much information as possible in supplemental material (appended to the
606"
OPEN ACCESS TO DATA AND CODE,0.8041349292709467,"paper) is recommended, but including URLs to data and code is permitted.
607"
OPEN ACCESS TO DATA AND CODE,0.8052230685527747,"6. Experimental Setting/Details
608"
OPEN ACCESS TO DATA AND CODE,0.8063112078346029,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
609"
OPEN ACCESS TO DATA AND CODE,0.8073993471164309,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
610"
OPEN ACCESS TO DATA AND CODE,0.808487486398259,"results?
611"
OPEN ACCESS TO DATA AND CODE,0.809575625680087,"Answer: [Yes]
612"
OPEN ACCESS TO DATA AND CODE,0.8106637649619152,"Justification: See Appendix C.
613"
OPEN ACCESS TO DATA AND CODE,0.8117519042437432,"Guidelines:
614"
OPEN ACCESS TO DATA AND CODE,0.8128400435255713,"• The answer NA means that the paper does not include experiments.
615"
OPEN ACCESS TO DATA AND CODE,0.8139281828073993,"• The experimental setting should be presented in the core of the paper to a level of detail
616"
OPEN ACCESS TO DATA AND CODE,0.8150163220892275,"that is necessary to appreciate the results and make sense of them.
617"
OPEN ACCESS TO DATA AND CODE,0.8161044613710555,"• The full details can be provided either with the code, in appendix, or as supplemental
618"
OPEN ACCESS TO DATA AND CODE,0.8171926006528836,"material.
619"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8182807399347116,"7. Experiment Statistical Significance
620"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8193688792165397,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
621"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8204570184983678,"information about the statistical significance of the experiments?
622"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8215451577801959,"Answer: [No]
623"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8226332970620239,"Justification: We reported the best following everyone else.
624"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.823721436343852,"Guidelines:
625"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8248095756256801,"• The answer NA means that the paper does not include experiments.
626"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8258977149075082,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
627"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8269858541893362,"dence intervals, or statistical significance tests, at least for the experiments that support
628"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8280739934711643,"the main claims of the paper.
629"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8291621327529923,"• The factors of variability that the error bars are capturing should be clearly stated (for
630"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8302502720348205,"example, train/test split, initialization, random drawing of some parameter, or overall
631"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8313384113166485,"run with given experimental conditions).
632"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8324265505984766,"• The method for calculating the error bars should be explained (closed form formula,
633"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8335146898803046,"call to a library function, bootstrap, etc.)
634"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8346028291621328,"• The assumptions made should be given (e.g., Normally distributed errors).
635"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8356909684439608,"• It should be clear whether the error bar is the standard deviation or the standard error
636"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8367791077257889,"of the mean.
637"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8378672470076169,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
638"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8389553862894451,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
639"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8400435255712732,"of Normality of errors is not verified.
640"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8411316648531012,"• For asymmetric distributions, the authors should be careful not to show in tables or
641"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8422198041349293,"figures symmetric error bars that would yield results that are out of range (e.g. negative
642"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8433079434167573,"error rates).
643"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8443960826985855,"• If error bars are reported in tables or plots, The authors should explain in the text how
644"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8454842219804135,"they were calculated and reference the corresponding figures or tables in the text.
645"
EXPERIMENTS COMPUTE RESOURCES,0.8465723612622416,"8. Experiments Compute Resources
646"
EXPERIMENTS COMPUTE RESOURCES,0.8476605005440696,"Question: For each experiment, does the paper provide sufficient information on the com-
647"
EXPERIMENTS COMPUTE RESOURCES,0.8487486398258978,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
648"
EXPERIMENTS COMPUTE RESOURCES,0.8498367791077258,"the experiments?
649"
EXPERIMENTS COMPUTE RESOURCES,0.8509249183895539,"Answer: [Yes]
650"
EXPERIMENTS COMPUTE RESOURCES,0.8520130576713819,"Justification: See end of Appendix B.
651"
EXPERIMENTS COMPUTE RESOURCES,0.85310119695321,"Guidelines:
652"
EXPERIMENTS COMPUTE RESOURCES,0.8541893362350381,"• The answer NA means that the paper does not include experiments.
653"
EXPERIMENTS COMPUTE RESOURCES,0.8552774755168662,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
654"
EXPERIMENTS COMPUTE RESOURCES,0.8563656147986942,"or cloud provider, including relevant memory and storage.
655"
EXPERIMENTS COMPUTE RESOURCES,0.8574537540805223,"• The paper should provide the amount of compute required for each of the individual
656"
EXPERIMENTS COMPUTE RESOURCES,0.8585418933623504,"experimental runs as well as estimate the total compute.
657"
EXPERIMENTS COMPUTE RESOURCES,0.8596300326441785,"• The paper should disclose whether the full research project required more compute
658"
EXPERIMENTS COMPUTE RESOURCES,0.8607181719260065,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
659"
EXPERIMENTS COMPUTE RESOURCES,0.8618063112078346,"didn’t make it into the paper).
660"
CODE OF ETHICS,0.8628944504896626,"9. Code Of Ethics
661"
CODE OF ETHICS,0.8639825897714908,"Question: Does the research conducted in the paper conform, in every respect, with the
662"
CODE OF ETHICS,0.8650707290533188,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
663"
CODE OF ETHICS,0.8661588683351469,"Answer: [Yes]
664"
CODE OF ETHICS,0.8672470076169749,"Justification:
665"
CODE OF ETHICS,0.8683351468988031,"Guidelines:
666"
CODE OF ETHICS,0.8694232861806311,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
667"
CODE OF ETHICS,0.8705114254624592,"• If the authors answer No, they should explain the special circumstances that require a
668"
CODE OF ETHICS,0.8715995647442872,"deviation from the Code of Ethics.
669"
CODE OF ETHICS,0.8726877040261154,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
670"
CODE OF ETHICS,0.8737758433079434,"eration due to laws or regulations in their jurisdiction).
671"
BROADER IMPACTS,0.8748639825897715,"10. Broader Impacts
672"
BROADER IMPACTS,0.8759521218715995,"Question: Does the paper discuss both potential positive societal impacts and negative
673"
BROADER IMPACTS,0.8770402611534276,"societal impacts of the work performed?
674"
BROADER IMPACTS,0.8781284004352558,"Answer: [NA]
675"
BROADER IMPACTS,0.8792165397170838,"Justification:
676"
BROADER IMPACTS,0.8803046789989118,"Guidelines:
677"
BROADER IMPACTS,0.8813928182807399,"• The answer NA means that there is no societal impact of the work performed.
678"
BROADER IMPACTS,0.8824809575625681,"• If the authors answer NA or No, they should explain why their work has no societal
679"
BROADER IMPACTS,0.8835690968443961,"impact or why the paper does not address societal impact.
680"
BROADER IMPACTS,0.8846572361262242,"• Examples of negative societal impacts include potential malicious or unintended uses
681"
BROADER IMPACTS,0.8857453754080522,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
682"
BROADER IMPACTS,0.8868335146898803,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
683"
BROADER IMPACTS,0.8879216539717084,"groups), privacy considerations, and security considerations.
684"
BROADER IMPACTS,0.8890097932535365,"• The conference expects that many papers will be foundational research and not tied
685"
BROADER IMPACTS,0.8900979325353645,"to particular applications, let alone deployments. However, if there is a direct path to
686"
BROADER IMPACTS,0.8911860718171926,"any negative applications, the authors should point it out. For example, it is legitimate
687"
BROADER IMPACTS,0.8922742110990207,"to point out that an improvement in the quality of generative models could be used to
688"
BROADER IMPACTS,0.8933623503808488,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
689"
BROADER IMPACTS,0.8944504896626768,"that a generic algorithm for optimizing neural networks could enable people to train
690"
BROADER IMPACTS,0.8955386289445049,"models that generate Deepfakes faster.
691"
BROADER IMPACTS,0.8966267682263329,"• The authors should consider possible harms that could arise when the technology is
692"
BROADER IMPACTS,0.8977149075081611,"being used as intended and functioning correctly, harms that could arise when the
693"
BROADER IMPACTS,0.8988030467899891,"technology is being used as intended but gives incorrect results, and harms following
694"
BROADER IMPACTS,0.8998911860718172,"from (intentional or unintentional) misuse of the technology.
695"
BROADER IMPACTS,0.9009793253536452,"• If there are negative societal impacts, the authors could also discuss possible mitigation
696"
BROADER IMPACTS,0.9020674646354734,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
697"
BROADER IMPACTS,0.9031556039173014,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
698"
BROADER IMPACTS,0.9042437431991295,"feedback over time, improving the efficiency and accessibility of ML).
699"
SAFEGUARDS,0.9053318824809575,"11. Safeguards
700"
SAFEGUARDS,0.9064200217627857,"Question: Does the paper describe safeguards that have been put in place for responsible
701"
SAFEGUARDS,0.9075081610446137,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
702"
SAFEGUARDS,0.9085963003264418,"image generators, or scraped datasets)?
703"
SAFEGUARDS,0.9096844396082698,"Answer: [NA]
704"
SAFEGUARDS,0.9107725788900979,"Justification:
705"
SAFEGUARDS,0.911860718171926,"Guidelines:
706"
SAFEGUARDS,0.9129488574537541,"• The answer NA means that the paper poses no such risks.
707"
SAFEGUARDS,0.9140369967355821,"• Released models that have a high risk for misuse or dual-use should be released with
708"
SAFEGUARDS,0.9151251360174102,"necessary safeguards to allow for controlled use of the model, for example by requiring
709"
SAFEGUARDS,0.9162132752992383,"that users adhere to usage guidelines or restrictions to access the model or implementing
710"
SAFEGUARDS,0.9173014145810664,"safety filters.
711"
SAFEGUARDS,0.9183895538628944,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
712"
SAFEGUARDS,0.9194776931447225,"should describe how they avoided releasing unsafe images.
713"
SAFEGUARDS,0.9205658324265505,"• We recognize that providing effective safeguards is challenging, and many papers do
714"
SAFEGUARDS,0.9216539717083787,"not require this, but we encourage authors to take this into account and make a best
715"
SAFEGUARDS,0.9227421109902068,"faith effort.
716"
LICENSES FOR EXISTING ASSETS,0.9238302502720348,"12. Licenses for existing assets
717"
LICENSES FOR EXISTING ASSETS,0.9249183895538629,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
718"
LICENSES FOR EXISTING ASSETS,0.926006528835691,"the paper, properly credited and are the license and terms of use explicitly mentioned and
719"
LICENSES FOR EXISTING ASSETS,0.9270946681175191,"properly respected?
720"
LICENSES FOR EXISTING ASSETS,0.9281828073993471,"Answer: [NA]
721"
LICENSES FOR EXISTING ASSETS,0.9292709466811752,"Justification:
722"
LICENSES FOR EXISTING ASSETS,0.9303590859630033,"Guidelines:
723"
LICENSES FOR EXISTING ASSETS,0.9314472252448314,"• The answer NA means that the paper does not use existing assets.
724"
LICENSES FOR EXISTING ASSETS,0.9325353645266594,"• The authors should cite the original paper that produced the code package or dataset.
725"
LICENSES FOR EXISTING ASSETS,0.9336235038084875,"• The authors should state which version of the asset is used and, if possible, include a
726"
LICENSES FOR EXISTING ASSETS,0.9347116430903155,"URL.
727"
LICENSES FOR EXISTING ASSETS,0.9357997823721437,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
728"
LICENSES FOR EXISTING ASSETS,0.9368879216539717,"• For scraped data from a particular source (e.g., website), the copyright and terms of
729"
LICENSES FOR EXISTING ASSETS,0.9379760609357998,"service of that source should be provided.
730"
LICENSES FOR EXISTING ASSETS,0.9390642002176278,"• If assets are released, the license, copyright information, and terms of use in the package
731"
LICENSES FOR EXISTING ASSETS,0.940152339499456,"should be provided. For popular datasets, paperswithcode.com/datasets has
732"
LICENSES FOR EXISTING ASSETS,0.941240478781284,"curated licenses for some datasets. Their licensing guide can help determine the license
733"
LICENSES FOR EXISTING ASSETS,0.9423286180631121,"of a dataset.
734"
LICENSES FOR EXISTING ASSETS,0.9434167573449401,"• For existing datasets that are re-packaged, both the original license and the license of
735"
LICENSES FOR EXISTING ASSETS,0.9445048966267682,"the derived asset (if it has changed) should be provided.
736"
LICENSES FOR EXISTING ASSETS,0.9455930359085963,"• If this information is not available online, the authors are encouraged to reach out to
737"
LICENSES FOR EXISTING ASSETS,0.9466811751904244,"the asset’s creators.
738"
NEW ASSETS,0.9477693144722524,"13. New Assets
739"
NEW ASSETS,0.9488574537540805,"Question: Are new assets introduced in the paper well documented and is the documentation
740"
NEW ASSETS,0.9499455930359086,"provided alongside the assets?
741"
NEW ASSETS,0.9510337323177367,"Answer: [NA]
742"
NEW ASSETS,0.9521218715995647,"Justification:
743"
NEW ASSETS,0.9532100108813928,"Guidelines:
744"
NEW ASSETS,0.9542981501632208,"• The answer NA means that the paper does not release new assets.
745"
NEW ASSETS,0.955386289445049,"• Researchers should communicate the details of the dataset/code/model as part of their
746"
NEW ASSETS,0.956474428726877,"submissions via structured templates. This includes details about training, license,
747"
NEW ASSETS,0.9575625680087051,"limitations, etc.
748"
NEW ASSETS,0.9586507072905331,"• The paper should discuss whether and how consent was obtained from people whose
749"
NEW ASSETS,0.9597388465723613,"asset is used.
750"
NEW ASSETS,0.9608269858541894,"• At submission time, remember to anonymize your assets (if applicable). You can either
751"
NEW ASSETS,0.9619151251360174,"create an anonymized URL or include an anonymized zip file.
752"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9630032644178455,"14. Crowdsourcing and Research with Human Subjects
753"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9640914036996736,"Question: For crowdsourcing experiments and research with human subjects, does the paper
754"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9651795429815017,"include the full text of instructions given to participants and screenshots, if applicable, as
755"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9662676822633297,"well as details about compensation (if any)?
756"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9673558215451578,"Answer: [NA]
757"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9684439608269858,"Justification:
758"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.969532100108814,"Guidelines:
759"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.970620239390642,"• The answer NA means that the paper does not involve crowdsourcing nor research with
760"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9717083786724701,"human subjects.
761"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9727965179542981,"• Including this information in the supplemental material is fine, but if the main contribu-
762"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9738846572361263,"tion of the paper involves human subjects, then as much detail as possible should be
763"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9749727965179543,"included in the main paper.
764"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9760609357997824,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
765"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9771490750816104,"or other labor should be paid at least the minimum wage in the country of the data
766"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9782372143634385,"collector.
767"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9793253536452666,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
768"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9804134929270947,"Subjects
769"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9815016322089227,"Question: Does the paper describe potential risks incurred by study participants, whether
770"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9825897714907508,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
771"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9836779107725789,"approvals (or an equivalent approval/review based on the requirements of your country or
772"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.984766050054407,"institution) were obtained?
773"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.985854189336235,"Answer: [NA]
774"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9869423286180631,"Justification:
775"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9880304678998912,"Guidelines:
776"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9891186071817193,"• The answer NA means that the paper does not involve crowdsourcing nor research with
777"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9902067464635473,"human subjects.
778"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9912948857453754,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
779"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9923830250272034,"may be required for any human subjects research. If you obtained IRB approval, you
780"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9934711643090316,"should clearly state this in the paper.
781"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9945593035908596,"• We recognize that the procedures for this may vary significantly between institutions
782"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9956474428726877,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
783"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9967355821545157,"guidelines for their institution.
784"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9978237214363439,"• For initial submissions, do not include any information that would break anonymity (if
785"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.998911860718172,"applicable), such as the institution conducting the review.
786"
