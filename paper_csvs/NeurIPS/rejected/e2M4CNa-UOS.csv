Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0017667844522968198,"Effective training of today’s large language models (LLMs) depends on large
1"
ABSTRACT,0.0035335689045936395,"batches and long sequences for throughput and accuracy. To handle variable-length
2"
ABSTRACT,0.00530035335689046,"sequences on hardware accelerators, it is common practice to introduce padding
3"
ABSTRACT,0.007067137809187279,"tokens, so that all sequences in a batch have the same length. We show in this paper
4"
ABSTRACT,0.0088339222614841,"that the variation in sequence lengths in common NLP datasets is such that up to
5"
ABSTRACT,0.01060070671378092,"50% of all tokens can be padding. In less common, but not extreme, cases (e.g.
6"
ABSTRACT,0.012367491166077738,"GLUE-cola with sequence length 128), the ratio is up to 89%. Existing methods
7"
ABSTRACT,0.014134275618374558,"to address the resulting inefﬁciency are complicated by the need to avoid ‘cross-
8"
ABSTRACT,0.015901060070671377,"contamination’ in self-attention, by a reduction in accuracy when sequence ordering
9"
ABSTRACT,0.0176678445229682,"information is lost, or by customized kernel implementations only valid for speciﬁc
10"
ABSTRACT,0.019434628975265017,"accelerators. This paper introduces a new formalization of sequence packing in
11"
ABSTRACT,0.02120141342756184,"the context of the well-studied bin packing problem, and presents new algorithms
12"
ABSTRACT,0.022968197879858657,"based on this formulation which, for example, confer a 2x speedup for phase 2
13"
ABSTRACT,0.024734982332155476,"pre-training in BERT. We show how existing models can be adapted to ensure
14"
ABSTRACT,0.026501766784452298,"mathematical equivalence between the original and packed models, meaning that
15"
ABSTRACT,0.028268551236749116,"packed models can be trained with existing pre-training and ﬁne-tuning practices.
16"
INTRODUCTION,0.030035335689045935,"1
Introduction
17"
INTRODUCTION,0.03180212014134275,"Many language datasets, including the de-facto pre-training dataset for BERT—Wikipedia, have
18"
INTRODUCTION,0.03356890459363958,"a skewed distribution of sequence lengths (see Figure 1). However, typical machine learning
19"
INTRODUCTION,0.0353356890459364,"accelerators, and their corresponding libraries, exhibit poor performance when processing variable-
20"
INTRODUCTION,0.037102473498233215,"length workloads. A simple mitigation is to set a maximum sequence length, and to pad shorter
21"
INTRODUCTION,0.038869257950530034,"sequences with padding tokens. This naive batching is widely used and provided in the vanilla BERT
22"
INTRODUCTION,0.04063604240282685,"implementation as well as the Hugging Face framework [32]. Its effect is enhanced by the ofﬂine
23"
INTRODUCTION,0.04240282685512368,"dataset generation process which, in BERT, attempts to “pack” together sentences so as to ﬁll the
24"
INTRODUCTION,0.044169611307420496,"sequence length as completely as possible [8]. We improve this process at a whole-dataset level.
25"
INTRODUCTION,0.045936395759717315,"We show that, even after this pre-processing, padding tokens represent 50% of all tokens of the
26"
INTRODUCTION,0.04770318021201413,"Wikipedia pre-training dataset at sequence length 512. Thus, by avoiding processing the padding
27"
INTRODUCTION,0.04946996466431095,"tokens one can get a 2x speed-up for phase 2. Overall, the lengths range between 5 tokens up to 512.
28"
INTRODUCTION,0.05123674911660778,"Samples of length 512 represent only 23.5% of the dataset,
29"
INTRODUCTION,0.053003533568904596,"Beyond the simple batching, other solutions have been addressed in the literature, and in open-source
30"
INTRODUCTION,0.054770318021201414,"software implementations. When processing sequences, most libraries and algorithms mention
31"
INTRODUCTION,0.05653710247349823,"packing as reference to concatenating sentences from the same document (BERT) or from different
32"
INTRODUCTION,0.05830388692579505,"documents (BERT, T5 [24], GPT-3 [4], and RoBERTa [16]) as they arrive (GREEDY) from the
33"
INTRODUCTION,0.06007067137809187,"source dataset to generate the training dataset. None of the respective papers addresses the packing
34"
INTRODUCTION,0.061837455830388695,"efﬁciency, i.e., remaining fraction of padding. To “separate” sequences from different documents, a
35"
INTRODUCTION,0.0636042402826855,"separator token is introduced. However, this is not sufﬁcient and can have a signiﬁcant impact on
36"
INTRODUCTION,0.06537102473498234,"performance. This is discussed only in the RoBERTa paper which shows that downstream F1 scores
37"
INTRODUCTION,0.06713780918727916,"get consistently reduced on average by 0.35%. Alternative common approaches to overcome the large
38"
INTRODUCTION,0.06890459363957598,"amount of padding in many datasets are “un-padding” as in Effective Transformer [5] and sorted
39"
INTRODUCTION,0.0706713780918728,"batching (SORT) as in Faster Transformer [21], lingvo [28] fairseq [22], and RoBERTa. However, for
40"
INTRODUCTION,0.07243816254416961,"running efﬁciently on arbitrary accelerators, these approaches require substantial hardware-speciﬁc
41"
INTRODUCTION,0.07420494699646643,"low-level code optimizations only available on GPUs. Further details are in Sections C [1] and 4.4.
42"
INTRODUCTION,0.07597173144876325,"Beyond language models, packing has been also present in other areas of machine learning, however
43"
INTRODUCTION,0.07773851590106007,"with little to no exploration in the literature and mostly hidden in some libraries without any further
44"
INTRODUCTION,0.07950530035335689,"discussion. For example, PyG (PyTorch Geometric) combines multiple small graphs in a batch to
45"
INTRODUCTION,0.0812720848056537,"account for the large variation in size and to optimize the hardware usage when training a Graph
46"
INTRODUCTION,0.08303886925795052,"Neural Network (GNN). Another example is the RNN implementation in PyTorch which introduces a
47"
INTRODUCTION,0.08480565371024736,"“PackedSequence” object and states that “All RNN modules accept packed sequences as inputs” but
48"
INTRODUCTION,0.08657243816254417,"does not address how sequences are packed efﬁciently and how the processing of packed sequences
49"
INTRODUCTION,0.08833922261484099,"is implemented in an efﬁcient manner while avoiding interaction between sequences. Even though
50"
INTRODUCTION,0.09010600706713781,"we focus on BERT [6] and other transformers in this paper, the general principles can be transferred
51"
INTRODUCTION,0.09187279151943463,"to many more machine learning algorithms with differently sized data samples.
52"
INTRODUCTION,0.09363957597173145,"In this paper, we formally frame the packing problem in transformer based models, and provide some
53"
INTRODUCTION,0.09540636042402827,"solutions, showing that sequences can be packed efﬁciently, separator tokens are not required, and
54"
INTRODUCTION,0.09717314487632508,"cross-contamination can be avoided with little overhead.
55"
INTRODUCTION,0.0989399293286219,"In summary, the contributions of the paper are as follows. In Section 2, we produce histograms of a
56"
INTRODUCTION,0.10070671378091872,"variety of datasets showing the high percentage of padding tokens. In Section 3.1, we present two new
57"
INTRODUCTION,0.10247349823321555,"deterministic and efﬁcient packing algorithms based on established solvers which efﬁciently pack
58"
INTRODUCTION,0.10424028268551237,"datasets with millions of sequences in a matter of seconds (or less). In Section 3.2 and Section 3.3, we
59"
INTRODUCTION,0.10600706713780919,"describe ‘cross-contamination’ —the cause of the accuracy reduction which separator tokens do not
60"
INTRODUCTION,0.10777385159010601,"mitigate— and show how the BERT model can be adjusted to show the same convergence behavior
61"
INTRODUCTION,0.10954063604240283,"on packed and unpacked sequences. We empirically show that the proposed packing algorithms
62"
INTRODUCTION,0.11130742049469965,"produce a nearly-optimal packing scheme for Wikipedia pre-training dataset (Section 4.1) and more
63"
INTRODUCTION,0.11307420494699646,"in the Appendix. In Section 4.2, we demonstrate that the convergence of the BERT large model on
64"
INTRODUCTION,0.11484098939929328,"the packed dataset is equivalent to that on the un-packed dataset with 2x throughput increase on the
65"
INTRODUCTION,0.1166077738515901,"Wikipedia sequence length 512 pre-training dataset. Further experiments underline the necessity and
66"
INTRODUCTION,0.11837455830388692,"efﬁciency of our changes.
67"
SEQUENCE LENGTH DISTRIBUTIONS,0.12014134275618374,"2
Sequence length distributions
68"
SEQUENCE LENGTH DISTRIBUTIONS,0.12190812720848057,"Figure 1: Sequence length distributions for different datasets. The three graphics at the top left show
Wikipedia BERT pre-training dataset sequence length histograms (token count excluding padding)
for different maximum sequence lengths based on the Wikipedia article dump from October 1st 2020.
The theoretical speed-up relates to not using any padding tokens and not having any overhead from
processing the different lengths. Top right: GLUE datasets. Bottom from left to right: SQuAD 1.1,
LibriSpeech text labels, LibriSpeech audio token sequence, and QM9 molecules of a graph in a
sequence."
SEQUENCE LENGTH DISTRIBUTIONS,0.12367491166077739,"BERT is pre-trained using masked-language modelling and next-sentence prediction on a large
69"
SEQUENCE LENGTH DISTRIBUTIONS,0.1254416961130742,"corpus of Wikipedia articles. Each sequence is composed of one <CLS> token followed by the
70"
SEQUENCE LENGTH DISTRIBUTIONS,0.127208480565371,"ﬁrst “segment” of sentences, followed by a <SEP> token, and then ﬁnally the second “segment” of
71"
SEQUENCE LENGTH DISTRIBUTIONS,0.12897526501766785,"sentences. Because these “segments” are created in sentence-level increments there is no token-level
72"
SEQUENCE LENGTH DISTRIBUTIONS,0.13074204946996468,"control of sequence length. Furthermore 10% (default value, [7]) of sequences are intentionally
73"
SEQUENCE LENGTH DISTRIBUTIONS,0.13250883392226148,"cut short. This leads to signiﬁcant levels of padding, especially for longer maximum sequence
74"
SEQUENCE LENGTH DISTRIBUTIONS,0.13427561837455831,"lengths (see Figure 1 and Section J[1]). At sequence length 128 (commonly used in phase 1 of
75"
SEQUENCE LENGTH DISTRIBUTIONS,0.13604240282685512,"pre-training) the theoretical speed-up is around 1.2, at sequence length 384 this increases to 1.7, and
76"
SEQUENCE LENGTH DISTRIBUTIONS,0.13780918727915195,"ﬁnally at sequence length 512 (commonly used for phase 2 of pre-training) it is 2.0. Despite the
77"
SEQUENCE LENGTH DISTRIBUTIONS,0.13957597173144876,"widespread use of the Wikipedia dataset for pre-training BERT such histograms have, to the best
78"
SEQUENCE LENGTH DISTRIBUTIONS,0.1413427561837456,"of our knowledge, not been published previously. This has perhaps lead to the underestimation of
79"
SEQUENCE LENGTH DISTRIBUTIONS,0.1431095406360424,"the speed-up opportunity available. To put things into perspective, the sequence length 512 dataset
80"
SEQUENCE LENGTH DISTRIBUTIONS,0.14487632508833923,"contains 8.33 billion tokens, of which 4.17 billion are padding tokens.
81"
SEQUENCE LENGTH DISTRIBUTIONS,0.14664310954063603,"Note that the skewed sequence length distributions are neither limited to Wikipedia, as shown with
82"
SEQUENCE LENGTH DISTRIBUTIONS,0.14840989399293286,"GLUE [30, 31] from Section L[1] and SQuAD 1.1 [25] from Section K[1] (2.2x speed up), to BERT
83"
SEQUENCE LENGTH DISTRIBUTIONS,0.1501766784452297,"training, as shown with LibiSpeech text distributions [23] from Section M[1], nor to text itself,
84"
SEQUENCE LENGTH DISTRIBUTIONS,0.1519434628975265,"given the LibriSpeech audio data distributions, and the QM9 molecular data [27, 26] (1.6x speed-up,
85"
SEQUENCE LENGTH DISTRIBUTIONS,0.15371024734982333,"Section Q[1]). All distributions can be found in Figure 1. Since LibriSpeech audio data is skewed to
86"
SEQUENCE LENGTH DISTRIBUTIONS,0.15547703180212014,"longer sequences, only 1.3x speed-up could be achieved despite the theoretical maximum of 1.6x.
87"
SEQUENCE LENGTH DISTRIBUTIONS,0.15724381625441697,"For all other cases, the algorithms presented in Section 3.1 lead to close to optimal packing.
88"
METHODS,0.15901060070671377,"3
Methods
89"
METHODS,0.1607773851590106,"Our approach consists of three distinct components. Firstly, we pack the n data samples efﬁciently
90"
METHODS,0.1625441696113074,"during pre-processing to make full use of the maximum sequence length, sm (Sections 3.1 and F).
91"
METHODS,0.16431095406360424,"Secondly, we introduce a series of model changes in Section 3.2 that preserve the equivalence with
92"
METHODS,0.16607773851590105,"the original BERT implementation. The changes include a self-attention mask to prevent the model
93"
METHODS,0.16784452296819788,"from attending between different sequences in the same pack (Section 3.2.2), and an adjustment
94"
METHODS,0.1696113074204947,"of the the positional embeddings (Section 3.2.1) to handle packs of sequences. Other components
95"
METHODS,0.17137809187279152,"of the model, such as the feed-forward layer [29], operate on a per-token basis and do not require
96"
METHODS,0.17314487632508835,"modiﬁcation for pre-training. In Section 3.2.3, we also demonstrate how to compute a per-sequence
97"
METHODS,0.17491166077738515,"loss and accuracy for NSP and downstream ﬁne-tuning tasks. Thirdly, we provide suggestions for
98"
METHODS,0.17667844522968199,"hyperparameter adjustment (Section 3.3) that lead to analogous convergence behavior between the
99"
METHODS,0.1784452296819788,"packed and un-packed BERT implementations. Additional videos and animations are provided as
100"
METHODS,0.18021201413427562,"supplemental material.
101"
PACKING ALGORITHMS,0.18197879858657243,"3.1
Packing algorithms
102"
PACKING ALGORITHMS,0.18374558303886926,"The widely studied and well established bin packing problem deals with the assignment of items into
103"
PACKING ALGORITHMS,0.1855123674911661,"bins of a ﬁxed capacity such that the number of utilized bins is minimized. It has been known for
104"
PACKING ALGORITHMS,0.1872791519434629,"decades if not centuries. Since an exact solution is strongly NP-complete [14], numerous approximate
105"
PACKING ALGORITHMS,0.18904593639575973,"solutions have been proposed [12, 15, 13, 36]. Since most existing approximations have a high
106"
PACKING ALGORITHMS,0.19081272084805653,"complexity of at least O(n log n), we propose two new heuristic ofﬂine algorithms that are tailored
107"
PACKING ALGORITHMS,0.19257950530035337,"to the NLP setting applied to the whole dataset. For a detailed introduction to packing see Section F.
108"
PACKING ALGORITHMS,0.19434628975265017,"3.1.1
Shortest-pack-ﬁrst histogram-packing (SPFHP)
109"
PACKING ALGORITHMS,0.196113074204947,"Shortest-pack-ﬁrst histogram-packing (SPFHP) works on the bins in the sequence length histogram
110"
PACKING ALGORITHMS,0.1978798586572438,"(with bin size 1) rather than the individual samples. The histogram is traversed in sorted order from
111"
PACKING ALGORITHMS,0.19964664310954064,"longest to shortest sequences. Then, to pack the data during the traversal, we apply the worst-ﬁt
112"
PACKING ALGORITHMS,0.20141342756183744,"algorithm [12, 36] such that the histogram bin being processed goes to the “pack”1 that has the most
113"
PACKING ALGORITHMS,0.20318021201413428,"space remaining (“shortest-pack-ﬁrst”). If the histogram bin does not ﬁt completely, a new pack is
114"
PACKING ALGORITHMS,0.2049469964664311,"created. We also limit the packing depth, in other words the maximum number of sequences that
115"
PACKING ALGORITHMS,0.2067137809187279,"are allowed in a pack. Therefore, an existing pack is only extended if it is not already at maximum
116"
PACKING ALGORITHMS,0.20848056537102475,"packing depth. The detailed code for the algorithm is provided in Listing 3. The time and space
117"
PACKING ALGORITHMS,0.21024734982332155,complexity of the algorithm are O(n + s2
PACKING ALGORITHMS,0.21201413427561838,m) and O(s2
PACKING ALGORITHMS,0.2137809187279152,"m) (Section G.2[1]).
118"
PACKING ALGORITHMS,0.21554770318021202,"1We avoid the ambiguous terms “bin” and “sample/sequence”and use “pack” instead to refer to the multiple
sequences concatenated during packing."
PACKING ALGORITHMS,0.21731448763250882,"3.1.2
Non-negative least squares histogram-packing (NNLSHP)
119"
PACKING ALGORITHMS,0.21908127208480566,"The proposed NNLSHP algorithm is based on re-stating the packing problem as a (weighted) non-
120"
PACKING ALGORITHMS,0.22084805653710246,"negative least squares problem (NNLS) [3] of the form wAx = wb where x ≥0. The vector b is the
121"
PACKING ALGORITHMS,0.2226148409893993,"histogram containing the counts of all the sequence lengths in the dataset. Next, we deﬁne the A
122"
PACKING ALGORITHMS,0.22438162544169613,"matrix (the “packing matrix“) by ﬁrst generating a list of all possible sequence length combinations
123"
PACKING ALGORITHMS,0.22614840989399293,"(“strategies”) that add up exactly to the maximum sequence length. We focus speciﬁcally on strategies
124"
PACKING ALGORITHMS,0.22791519434628976,"that consist of at most 3 sequences per pack (independent of b) and encode each strategy as a column
125"
PACKING ALGORITHMS,0.22968197879858657,"of the sparse matrix A. For example, a strategy consisting of the sequence length 128, 128, and
126"
PACKING ALGORITHMS,0.2314487632508834,"256 in represented a column vector that has the value 2 at the 128th row, the value 1 at the 256th
127"
PACKING ALGORITHMS,0.2332155477031802,"row, and zero at all other rows. The variable x describes the non-negative repetition count for each
128"
PACKING ALGORITHMS,0.23498233215547704,"strategy. So a 24 in the ith row of x means that the strategy represented by the ith column of A should
129"
PACKING ALGORITHMS,0.23674911660777384,"repeat 24 times. Moreover, in the un-weighted setting, Ax = b states that we would like to “mix” the
130"
PACKING ALGORITHMS,0.23851590106007067,"pre-deﬁned strategies (columns of A) such that the number of samples matches the histogram b, and
131"
PACKING ALGORITHMS,0.24028268551236748,"where each strategy is used x ≥0 times. We use the residual weight w to control the penalization
132"
PACKING ALGORITHMS,0.2420494699646643,"of the Ax −b residual on different sequence lengths (different rows of b). Heuristically, we set
133"
PACKING ALGORITHMS,0.24381625441696114,"the weight of 0.09 for all sequences of length 8 or smaller because they are considered acceptable
134"
PACKING ALGORITHMS,0.24558303886925795,"padding sequences while all other sequence lengths get weight 1. We discuss this heuristic choice of
135"
PACKING ALGORITHMS,0.24734982332155478,"parameters in Section F.4.5 and F.5[1]. The overall efﬁciency of the packing is not greatly inﬂuenced
136"
PACKING ALGORITHMS,0.24911660777385158,"by the weighing (less than 1% extra speed-up).
137"
PACKING ALGORITHMS,0.2508833922261484,"After solving wAx = wb for x ≥0 using an off-the-shelf solver, we obtain a ﬂoating point solution,
138"
PACKING ALGORITHMS,0.25265017667844525,"which means that the repetition counts are not necessarily integers. Since we cannot use a non-natural
139"
PACKING ALGORITHMS,0.254416961130742,"number of strategies, we round the solution ˆx to the nearest integer. The error introduced by this
140"
PACKING ALGORITHMS,0.25618374558303886,"rounding is found to be negligible (a few hundred sequences in the worst case) compared to the size
141"
PACKING ALGORITHMS,0.2579505300353357,"of the dataset (millions of sequences). The time complexity and space complexity of the algorithm
142"
PACKING ALGORITHMS,0.2597173144876325,are O(n + s5
PACKING ALGORITHMS,0.26148409893992935,m) and O(s3
PACKING ALGORITHMS,0.26325088339222613,"m). Further details are provided in Section F.4.
143"
PACKING ALGORITHMS,0.26501766784452296,"3.2
packedBERT: model changes
144"
PACKING ALGORITHMS,0.2667844522968198,"This section describes how any vanilla BERT implementation should be modiﬁed for packed sequence
145"
PACKING ALGORITHMS,0.26855123674911663,"processing, such that the behavior of the model is the same as when processing unpacked sequences.
146"
PACKING ALGORITHMS,0.2703180212014134,"Preserving the mathematical equivalence is necessary to ensure existing BERT pre-training and
147"
PACKING ALGORITHMS,0.27208480565371024,"ﬁne-tuning practices remain valid, as well as being required by benchmarks such as MLPerf™[17].
148"
PACKING ALGORITHMS,0.27385159010600707,"The presented approaches and principles apply to a variety of other models.
149"
ADJUST POSITIONAL EMBEDDINGS,0.2756183745583039,"3.2.1
Adjust positional embeddings
150"
ADJUST POSITIONAL EMBEDDINGS,0.2773851590106007,"The BERT model uses three types of embeddings: token, segment, and positional embeddings. The
151"
ADJUST POSITIONAL EMBEDDINGS,0.2791519434628975,"latter is canonically implemented as a bias add operation, rather than a full embedding look-up. This
152"
ADJUST POSITIONAL EMBEDDINGS,0.28091872791519434,"is possible because the positional indices increase linearly for every sequence. However, when using
153"
ADJUST POSITIONAL EMBEDDINGS,0.2826855123674912,"the packed data format the position index needs to be reset with each new packed sequence. For
154"
ADJUST POSITIONAL EMBEDDINGS,0.284452296819788,"instance, when packing two sequences one of length 2 and one of length 3, the positional embedding
155"
ADJUST POSITIONAL EMBEDDINGS,0.2862190812720848,"indexes that need to be picked up are [0, 1, 0, 1, 2]. To achieve this, the bias add needs to be replaced
156"
ADJUST POSITIONAL EMBEDDINGS,0.2879858657243816,"by an embedding look-up to extract the correct positional embedding for each token in the pack. This
157"
ADJUST POSITIONAL EMBEDDINGS,0.28975265017667845,"also requires keeping an extra input which speciﬁes the position of each token in its sequence. This
158"
ADJUST POSITIONAL EMBEDDINGS,0.2915194346289753,"required adjustment has only a minor impact on absolute accuracy/loss (see Section 4.2 and 4.2.1).
159"
ADJUST ATTENTION MASKING,0.29328621908127206,"3.2.2
Adjust attention masking
160"
ADJUST ATTENTION MASKING,0.2950530035335689,"# input
mask = np.array([[1, 1, 1, 2, 2]])
# 0, 1 mask
zero_one_mask = tf.equal(mask, mask.T)
# for use with softmax:
softmax_mask = tf.where(
    zero_one_mask, 0, -1000) 0"
ADJUST ATTENTION MASKING,0.2968197879858657,"B
B
B
@"
ADJUST ATTENTION MASKING,0.29858657243816256,"1
1
1
0
0
1
1
1
0
0
1
1
1
0
0
0
0
0
1
1
0
0
0
1
1 1"
ADJUST ATTENTION MASKING,0.3003533568904594,"C
C
C
A"
ADJUST ATTENTION MASKING,0.30212014134275617,"Figure 2: Attention mask code [left], respective zero-one mask [middle], and vectorized unpacking
of the sequence loss[right]. White rectangles correspond to padding."
ADJUST ATTENTION MASKING,0.303886925795053,"To maintain an implementation that is consistent with the un-packed version, tokens from different
161"
ADJUST ATTENTION MASKING,0.30565371024734983,"sequences within a pack should not be able to attend to each other. This is typically achieved in
162"
ADJUST ATTENTION MASKING,0.30742049469964666,"other implementations by unpacking the sequences using custom attention kernels and then doing
163"
ADJUST ATTENTION MASKING,0.30918727915194344,"the attention per-sequence [5]. Instead, we propose directly masking the attention matrix with a
164"
ADJUST ATTENTION MASKING,0.31095406360424027,"block-diagonal mask before the attention softmax. This is straightforward to implement in modern
165"
ADJUST ATTENTION MASKING,0.3127208480565371,"frameworks (see Figure 2). Naturally, there is a cost to both the mask construction and applying
166"
ADJUST ATTENTION MASKING,0.31448763250883394,"it to the attention matrix. However, it is required to keep the accuracy (see Table 1, Section 4.1,
167"
ADJUST ATTENTION MASKING,0.31625441696113077,"Section 4.2). See also the code of the deprecated tensor2tensor library and our own provided code.
168"
ADJUST PER-SEQUENCE LOSS AND ACCURACY,0.31802120141342755,"3.2.3
Adjust per-sequence loss and accuracy
169"
ADJUST PER-SEQUENCE LOSS AND ACCURACY,0.3197879858657244,"Canonical implementations of BERT compute the cross-entropy loss for the masked language model
170"
ADJUST PER-SEQUENCE LOSS AND ACCURACY,0.3215547703180212,"on a per-token basis. However other NLP tasks, such as SQuAD, compute the loss and accuracy on
171"
ADJUST PER-SEQUENCE LOSS AND ACCURACY,0.32332155477031804,"a per-sequence basis. This section discusses how to handle such tasks when training with packed
172"
ADJUST PER-SEQUENCE LOSS AND ACCURACY,0.3250883392226148,"sequences. Simply feeding packs of sequences to the same implementation of cross-entropy would
173"
ADJUST PER-SEQUENCE LOSS AND ACCURACY,0.32685512367491165,"result in a per-pack weighted loss. In other words, the overall loss on the micro-batch would sum-up
174"
ADJUST PER-SEQUENCE LOSS AND ACCURACY,0.3286219081272085,"the losses on the individual packs, rather than individual sequences. As a result, the model would
175"
ADJUST PER-SEQUENCE LOSS AND ACCURACY,0.3303886925795053,"converge to a different optimum than when running with the un-packed implementation. For instance,
176"
ADJUST PER-SEQUENCE LOSS AND ACCURACY,0.3321554770318021,"a pack of a single sequence would contribute to the loss with the same weight as a pack of three
177"
ADJUST PER-SEQUENCE LOSS AND ACCURACY,0.3339222614840989,"sequences.
178"
ADJUST PER-SEQUENCE LOSS AND ACCURACY,0.33568904593639576,"To recover the per-sequence averaging behavior of the canonical un-packed BERT implementation,
179"
ADJUST PER-SEQUENCE LOSS AND ACCURACY,0.3374558303886926,"we effectively “unpack” the incoming logits and labels. Once the sequences have been unpacked,
180"
ADJUST PER-SEQUENCE LOSS AND ACCURACY,0.3392226148409894,"we can compute the loss on each sequence separately as usual and then add up the losses. However,
181"
ADJUST PER-SEQUENCE LOSS AND ACCURACY,0.3409893992932862,"rather than looping through the sequences index, we compute on all indexes in parallel (see Figure 2).
182"
ADJUST PER-SEQUENCE LOSS AND ACCURACY,0.34275618374558303,"This minimizes the latency overhead of un-packing the loss calculation. As an example, we show how
183"
ADJUST PER-SEQUENCE LOSS AND ACCURACY,0.34452296819787986,"per-sequence loss can be implemented for the pre-training task. We use the “masked lm weight” [7]
184"
ADJUST PER-SEQUENCE LOSS AND ACCURACY,0.3462897526501767,"input tensor to represent which sequence a given masked token belongs to (0, 1, 2 and so on). This
185"
ADJUST PER-SEQUENCE LOSS AND ACCURACY,0.3480565371024735,"is consistent with the canonical BERT implementation where this input takes a value of either 1
186"
ADJUST PER-SEQUENCE LOSS AND ACCURACY,0.3498233215547703,"(belonging to the sequence) or 0 (belonging to padding). The full methodology is detailed in Listing 5
187"
ADJUST PER-SEQUENCE LOSS AND ACCURACY,0.35159010600706714,"and can be applied to other classiﬁcation or pre-training tasks.
188"
ADJUST HYPERPARAMETERS,0.35335689045936397,"3.3
Adjust hyperparameters
189"
ADJUST HYPERPARAMETERS,0.3551236749116608,"In terms of convergence behavior, the primary consequence of packing is an increase in the effective
190"
ADJUST HYPERPARAMETERS,0.3568904593639576,"batch size (with respect to number of sequences and real tokens) with some added variation over
191"
ADJUST HYPERPARAMETERS,0.3586572438162544,"different iterations. If we look on the sentence level, the number of sentences in one batch increases
192"
ADJUST HYPERPARAMETERS,0.36042402826855124,"by the packing factor. Similarly, the number of tokens in one batch increases. Hence, hyperparameters
193"
ADJUST HYPERPARAMETERS,0.3621908127208481,"that are sensitive to these numbers need to be adjusted.
194"
ADJUST HYPERPARAMETERS,0.36395759717314485,"A direct solution is to reduce the computational batch size by the packing factor (average number of
195"
ADJUST HYPERPARAMETERS,0.3657243816254417,"sequences per pack) and keep all other hyperparameters the same. For example, if the packing factor
196"
ADJUST HYPERPARAMETERS,0.3674911660777385,"is 2, cutting the gradient accumulation count by half is sufﬁcient. The advantage of this strategy is that
197"
ADJUST HYPERPARAMETERS,0.36925795053003535,"no ﬁne-tuning of hyperparameters is required and performance curves are comparable. However, this
198"
ADJUST HYPERPARAMETERS,0.3710247349823322,"approach might be not desirable as it might imply under-utilizing the memory/compute, especially if
199"
ADJUST HYPERPARAMETERS,0.37279151943462896,"the micro batch size needs to be reduced.
200"
ADJUST HYPERPARAMETERS,0.3745583038869258,"Hence to preserve batch size and optimize hardware utilization, we additionally propose an approxi-
201"
ADJUST HYPERPARAMETERS,0.3763250883392226,"mate heuristic for updating the decay parameters of the LAMB optimizer [35] . For a packed dataset
202"
ADJUST HYPERPARAMETERS,0.37809187279151946,"with a packing factor p, we update the decay parameters as: β1 := βp"
ADJUST HYPERPARAMETERS,0.37985865724381623,"1, β2 := βp"
ADJUST HYPERPARAMETERS,0.38162544169611307,"2. For p = 2, this
203"
ADJUST HYPERPARAMETERS,0.3833922261484099,"corresponds to the exact parameters for calculating momentum and velocity, when updating with the
204"
ADJUST HYPERPARAMETERS,0.38515901060070673,"same gradient twice (Section D). A common approach is to scale the learning rate with the batch size.
205"
ADJUST HYPERPARAMETERS,0.3869257950530035,"However, our experiments in Section 4.2 show that this reduces convergence speed.
206"
ADJUST HYPERPARAMETERS,0.38869257950530034,"Since these adjustments are only heuristics the convergence of the model will be comparable but not
207"
ADJUST HYPERPARAMETERS,0.39045936395759717,"identical. In particular, it is unlikely that simply adjusting the hyperparameters will fully undo the
208"
ADJUST HYPERPARAMETERS,0.392226148409894,"impact of the increased batch size. However, with these adjustments, researchers should be able to
209"
ADJUST HYPERPARAMETERS,0.39399293286219084,"continue to use existing conﬁgurations.
210"
EXPERIMENTS,0.3957597173144876,"4
Experiments
211"
BIN PACKING ALGORITHM COMPARISON,0.39752650176678445,"4.1
Bin packing algorithm comparison
212"
BIN PACKING ALGORITHM COMPARISON,0.3992932862190813,"We evaluate our algorithms using the following metrics: number of packs, number of all tokens,
213"
BIN PACKING ALGORITHM COMPARISON,0.4010600706713781,"number of padding tokens, solution time of the packing algorithm (after histogram and strategy
214"
BIN PACKING ALGORITHM COMPARISON,0.4028268551236749,"creation), number of strategies used, packing efﬁciency (the fraction of non-padding tokens in the
215"
BIN PACKING ALGORITHM COMPARISON,0.4045936395759717,"packed dataset), the speed-up achieved compared to not packing (depth 1), and the average number
216"
BIN PACKING ALGORITHM COMPARISON,0.40636042402826855,"of sequences per sample (packing factor). For SPFHP, we analyse different (maximum) packing
217"
BIN PACKING ALGORITHM COMPARISON,0.4081272084805654,"depth, since packing is less efﬁcient with smaller depth and we want to get a general understanding
218"
BIN PACKING ALGORITHM COMPARISON,0.4098939929328622,"on how the packing depth inﬂuences the processing time. For NNLSHP, we focus on packing
219"
BIN PACKING ALGORITHM COMPARISON,0.411660777385159,"depth 3 because it packs the data sufﬁciently well. For the speed-up analysis, we focus on the
220"
BIN PACKING ALGORITHM COMPARISON,0.4134275618374558,"intelligence processing unit (IPU) [11] (IPU-M2000, 16 accelerator chips), BERT phase 2 pretraining
221"
BIN PACKING ALGORITHM COMPARISON,0.41519434628975266,"setup as in Section 4.2. A GPU dynamically loads the code into the accelerator; in contrast, the
222"
BIN PACKING ALGORITHM COMPARISON,0.4169611307420495,"IPU works with a static pre-compiled engine that gets loaded onto the chip at the start of the run.
223"
BIN PACKING ALGORITHM COMPARISON,0.41872791519434627,"While other approaches result in excessive padding or continuous changes of the code, our approach
224"
BIN PACKING ALGORITHM COMPARISON,0.4204946996466431,"can work with the same code for the whole dataset. So in this setting the IPU architecture would
225"
BIN PACKING ALGORITHM COMPARISON,0.42226148409893993,"especially beneﬁt from our approach since it avoids code changes. Nevertheless, it can be applied
226"
BIN PACKING ALGORITHM COMPARISON,0.42402826855123676,"to any implementation on GPU or TPU. For determining the speed-up, we take advantage of the
227"
BIN PACKING ALGORITHM COMPARISON,0.42579505300353354,"precompiled kernel. Since time measurements are quite noisy, we can proﬁle the kernel and how
228"
BIN PACKING ALGORITHM COMPARISON,0.4275618374558304,"many cycles it takes for processing a batch. That way, we can determine the overhead (in cycles)
229"
BIN PACKING ALGORITHM COMPARISON,0.4293286219081272,"from processing the additional attention masking and for unpacking the loss. Combining overhead
230"
BIN PACKING ALGORITHM COMPARISON,0.43109540636042404,"and packing factor, we get the speed-up estimate. No experiment repetitions are required since the
231"
BIN PACKING ALGORITHM COMPARISON,0.43286219081272087,"algorithms and measurements are deterministic.
232"
BIN PACKING ALGORITHM COMPARISON,0.43462897526501765,Table 1: Key performance results of proposed packing algorithms (SPFHP and NNLSHP) on IPU.
BIN PACKING ALGORITHM COMPARISON,0.4363957597173145,"pack.
packing
EFF
p
OH
realized
depth
algorithm
(%)
(%)
speed-up
1
NONE
50.0
1.00
0.000
1.000
1
SORT
99.9
2.00
≫100
⌧1.000
⇡10
GREEDY
⇡78
⇡1.6
⇡4.48
⇡1.5
2
SPFHP
80.5
1.61
4.283
1.544
3
SPFHP
89.4
1.79
4.287
1.716
3
NNLSHP
99.7
2.00
4.287
1.913
4
SPFHP
93.9
1.88
4.294
1.803
8
SPFHP
98.9
1.98
4.481
1.895
max
SPFHP
99.6
1.99
4.477
1.905"
BIN PACKING ALGORITHM COMPARISON,0.4381625441696113,"Packing depth describes the maximum number of packed sequences. NONE is the baseline BERT
implementation, whereas SORT corresponds to sorted batching, and GREEDY concatenates se-
quences as they arrive until they would exceed 512 tokens. Setting no limit resulted in a maximum
packing depth of 16. EFFiciency is the percentage of real tokens in the packed dataset. The packing
factor describes the resulting potential speed-up compared to packing depth 1. With overhead (OH),
we denote the percentage decrease in throughput due to changes to the model to enable packing (such
as the masking scheme introduced in Section 3.2.2). The realized speed-up is the combination of
the speed-up due to packing (the packing factor) and the decrease in throughput due to the overhead
on the IPU. It is used to measure the relative speed-up in throughput and the overhead from masking
and loss adjustment. SORT can be only efﬁcient on GPUs (see Section 4.4)."
BIN PACKING ALGORITHM COMPARISON,0.43992932862190814,"The main results for the performance metric evaluation are displayed in Table 1. The processing
233"
BIN PACKING ALGORITHM COMPARISON,0.4416961130742049,"time for SPFHP on an Intel(R) Xeon(R) Gold 6138 CPU with 2.00GHz, 80 nodes, and 472G RAM
234"
BIN PACKING ALGORITHM COMPARISON,0.44346289752650175,"was around 0.03s and independent from the packing depth. Classical First-Fit-Decreasing requires
235"
BIN PACKING ALGORITHM COMPARISON,0.4452296819787986,"87-120s, a lot of memory, and scales almost linear with the number of samples. We see that the
236"
BIN PACKING ALGORITHM COMPARISON,0.4469964664310954,"overhead slightly increases with packing depth but that the beneﬁts of packing outweigh the cost. The
237"
BIN PACKING ALGORITHM COMPARISON,0.44876325088339225,"best speed-up is obtained with NNLSHP at depth 3 which required 28.4s on the CPU for processing
238"
BIN PACKING ALGORITHM COMPARISON,0.450530035335689,"and ran out of memory for larger depth. With a value of 1.913, it is close to the theoretical upper
239"
BIN PACKING ALGORITHM COMPARISON,0.45229681978798586,"bound of 2.001. The results show that efﬁciency, packing factor, and speed-up can be viewed inter-
240"
BIN PACKING ALGORITHM COMPARISON,0.4540636042402827,"changeably. The amount of time needed to process a sample (a pack of sequences) is barely changed
241"
BIN PACKING ALGORITHM COMPARISON,0.4558303886925795,"relative to the un-packed implementation. The packing factor, or the improvement in efﬁciency,
242"
BIN PACKING ALGORITHM COMPARISON,0.4575971731448763,"effectively provide an accurate estimate of the speed-up. GREEDY packing as used in T5 shows
243"
BIN PACKING ALGORITHM COMPARISON,0.45936395759717313,"to be quite inefﬁcient and sorted batching (SORT) is highly efﬁcient in avoiding padding but the
244"
BIN PACKING ALGORITHM COMPARISON,0.46113074204946997,"resulting different computational graphs cause a major overhead on the IPU that exceeds the beneﬁts
245"
BIN PACKING ALGORITHM COMPARISON,0.4628975265017668,"of avoiding the padding. Since we made our algorithm and code public available, results have been
246"
BIN PACKING ALGORITHM COMPARISON,0.46466431095406363,"reproduced with a different framework on the Habana Gaudi accelerator [10] and conﬁrmed that our
247"
BIN PACKING ALGORITHM COMPARISON,0.4664310954063604,"approach is hardware and software independent giving it a huge advantage over existing approaches.
248"
BIN PACKING ALGORITHM COMPARISON,0.46819787985865724,"4.2
MLPerf™phase 2 pretraining setup: learning curves and hyperparameter adjustment
249"
BIN PACKING ALGORITHM COMPARISON,0.46996466431095407,"For depth 1 (classic BERT) and NNLSHP with depth 3, we additionally evaluate on the MLPerf™ver-
250"
BIN PACKING ALGORITHM COMPARISON,0.4717314487632509,"sion 0.7 BERT pre-training benchmark [17]. Brieﬂy, this involves training from a standard checkpoint
251"
BIN PACKING ALGORITHM COMPARISON,0.4734982332155477,"to a masked-language model accuracy of 71.2% using 3 million sequences with a maximum length of
252"
BIN PACKING ALGORITHM COMPARISON,0.4752650176678445,"512 tokens (refer to [19] for details). Following this standardized benchmark supports reproduction of
253"
BIN PACKING ALGORITHM COMPARISON,0.47703180212014135,"results even on other systems and makes sure that the reproduction effort is moderate and setup rules
254"
BIN PACKING ALGORITHM COMPARISON,0.4787985865724382,"are clearly documented. We compare the resulting speed-up as well as the respective learning curves
255"
BIN PACKING ALGORITHM COMPARISON,0.48056537102473496,"by evaluating the data on a held-out validation dataset. The objective of this additional evaluation is
256"
BIN PACKING ALGORITHM COMPARISON,0.4823321554770318,"to analyse if convergence behavior is changed by the packing strategy and if the theoretical speed-up
257"
BIN PACKING ALGORITHM COMPARISON,0.4840989399293286,"can be achieved in practice.
258"
BIN PACKING ALGORITHM COMPARISON,0.48586572438162545,"With packing, we effectively increase the average batch size by the packing factor (⇡2). However,
259"
BIN PACKING ALGORITHM COMPARISON,0.4876325088339223,"with a different batch size, different hyperparameters are required (see Section 3.3) and there is no
260"
BIN PACKING ALGORITHM COMPARISON,0.48939929328621906,"mapping that will generate exact matching of results but only heuristics. In a ﬁrst comparison, we
261"
BIN PACKING ALGORITHM COMPARISON,0.4911660777385159,"use the same hyperparameters when comparing packed and unpacked training except for cutting the
262"
BIN PACKING ALGORITHM COMPARISON,0.4929328621908127,"accumulation count by half. This way, we make sure that the batch size is constant on average and
263"
BIN PACKING ALGORITHM COMPARISON,0.49469964664310956,"we have the same amount of training steps. In the second comparison, we evaluate our heuristics and
264"
BIN PACKING ALGORITHM COMPARISON,0.49646643109540634,"how they compensate the difference in batch size. This setup is more desirable because it is beneﬁcial
265"
BIN PACKING ALGORITHM COMPARISON,0.49823321554770317,"to use the hardware to its full potential and cutting the batch size by half usually reduces throughput.
266"
BIN PACKING ALGORITHM COMPARISON,0.5,"In the third comparison, we compare two optimized setups. In these two cases, packing takes half the
267"
BIN PACKING ALGORITHM COMPARISON,0.5017667844522968,"amount of training steps.
268"
BIN PACKING ALGORITHM COMPARISON,0.5035335689045937,"The learning curves are displayed in Figure 3. In the ﬁrst setup, we see the curves almost matching
269"
BIN PACKING ALGORITHM COMPARISON,0.5053003533568905,"perfectly when normalizing by the numbers of samples processed. Differences can be explained
270"
BIN PACKING ALGORITHM COMPARISON,0.5070671378091873,"by the variation of the number of sequences in the packing batch, and general noise in the training
271"
BIN PACKING ALGORITHM COMPARISON,0.508833922261484,"process. Especially after the initial phase, the curves show a near-identical match. The second setup
272"
BIN PACKING ALGORITHM COMPARISON,0.5106007067137809,"shows bigger differences since changing the batch size and hyperparameters changes the training
273"
BIN PACKING ALGORITHM COMPARISON,0.5123674911660777,"dynamics. We observe slower convergence early on in training due to the increased batch size. This
274"
BIN PACKING ALGORITHM COMPARISON,0.5141342756183745,"is expected. The adjustment of the learning rate actually decreases performance probably because we
275"
BIN PACKING ALGORITHM COMPARISON,0.5159010600706714,"correct for the increased number of sequences already in the modiﬁed loss. With the adjustment of
276"
BIN PACKING ALGORITHM COMPARISON,0.5176678445229682,"the decay parameter of LAMB, we see matching performance at the later training stages. However,
277"
BIN PACKING ALGORITHM COMPARISON,0.519434628975265,"it is not feasible to completely recover the early convergence behavior of the smaller batch size by
278"
BIN PACKING ALGORITHM COMPARISON,0.5212014134275619,"adjusting the hyperparameters. For instance doubling the batch size of unpacked BERT to 3000
279"
BIN PACKING ALGORITHM COMPARISON,0.5229681978798587,"and adjusting the LAMB decay parameters leads to more of a slow down in convergence than
280"
BIN PACKING ALGORITHM COMPARISON,0.5247349823321554,"when running packed BERT with a batch size of 1500 and a packing factor of 2. n practice, our
281"
BIN PACKING ALGORITHM COMPARISON,0.5265017667844523,"implementations exceeds the estimated 1.913 maximum speed-up. This estimate is based on the
282"
BIN PACKING ALGORITHM COMPARISON,0.5282685512367491,"reduction in the computational work needed to process the dataset. However, packing the data also
283"
BIN PACKING ALGORITHM COMPARISON,0.5300353356890459,"reduces the latency of the transferring the data to the device. Figure 3 shows that the realized total
284"
BIN PACKING ALGORITHM COMPARISON,0.5318021201413428,"speed-up from packing exceeds 2x.
285"
ABLATION STUDY,0.5335689045936396,"4.2.1
Ablation study
286"
ABLATION STUDY,0.5353356890459364,"So far, we have shown that with the introduced adjustments, we can match the accuracy of unpacked
287"
ABLATION STUDY,0.5371024734982333,"BERT. In the following, we analyze in how far the masking adjustment is required. In Figure 4, we
288"
ABLATION STUDY,0.5388692579505301,"can see that without our adjustments, training loss and accuracy worsen drastically and a longer
289"
ABLATION STUDY,0.5406360424028268,"training time does not lead to a recovery. When not adjusting the positional embedding, the loss and
290"
ABLATION STUDY,0.5424028268551236,"accuracy almost match. However, the accuracy stalls at 71.8% and does not reach the target accuracy
291"
ABLATION STUDY,0.5441696113074205,"of 72.1%. So overall, both adjustments are crucial to avoid a reduction in performance.
292"
ABLATION STUDY,0.5459363957597173,"When running packed BERT without the NSP loss but keeping everything else the same in a full
293"
ABLATION STUDY,0.5477031802120141,"training setup, we observed that downstream performance on SQuAD reduced the F1 measure by
294"
ABLATION STUDY,0.549469964664311,"1.31% and EM by 1.15%. Hence, we do not consider removing NSP as done in approaches like
295"
ABLATION STUDY,0.5512367491166078,"RoBERTa and T5 as discussed in Section I.
296"
ABLATION STUDY,0.5530035335689046,"Figure 3: Comparison of learning curves for packed and unpacked processing, where all experiments
converged to the target accuracy within the same number of training samples(3 million). [left] same
effective batch size (ebs is batch size times packing factor), [middle] different heuristic adjustments
of the hyperparameters (batch size 1500 for all runs, such that ebs for packed runs is 1500 ⇤2), and
[right] realized speed-up from packing (in excess of desired 2x). Further learning curves are provided
in Section O."
ABLATION STUDY,0.5547703180212014,"Figure 4: Comparison of learning curves with and without mask or positional embedding adjustment
in our packed BERT approach. The grey accuracy baseline to reach is 72.1%."
ABLATION STUDY,0.5565371024734982,"4.3
Full pretraining and SQuAD ﬁnetuning
297"
ABLATION STUDY,0.558303886925795,"Packing slightly violates the i.i.d. assumption of data. Thus, we have to check that downstream
298"
ABLATION STUDY,0.5600706713780919,"performance is not impacted by packing. This is especially relevant in a full training setup without
299"
ABLATION STUDY,0.5618374558303887,"a starting checkpoint. To this aim, we show that the packed and unpacked SQuAD 1.1 scores are
300"
ABLATION STUDY,0.5636042402826855,"comparable after a full-pretraining of BERT base and large plus ﬁne-tuning. During pre-training,
301"
ABLATION STUDY,0.5653710247349824,"in order to avoid giving an advantage to packing by further hyperparameter tuning, we reduce the
302"
ABLATION STUDY,0.5671378091872792,"gradient accumulation count for the packed BERT training for phase 1 and phase 2 to match, on
303"
ABLATION STUDY,0.568904593639576,"average, the total number of sequences that get processed before each weight update. With this
304"
ABLATION STUDY,0.5706713780918727,"approach, we can use the same hyperparameters and number of training steps but process each batch
305"
ABLATION STUDY,0.5724381625441696,"faster by avoiding the processing of padding. This gives a slight disadvantage to the packed run in
306"
ABLATION STUDY,0.5742049469964664,"terms of machine utilization, as explained in Section 3.3 and is different to the speedup analysis in
307"
ABLATION STUDY,0.5759717314487632,"Section 4.2. For Phase 2, we use sequence length 384 since longer range attention is not relevant
308"
ABLATION STUDY,0.5777385159010601,"for SQuAD 1.1. The respective speed-ups from packing for BERT base and large are shown in
309"
ABLATION STUDY,0.5795053003533569,"Table 2: the realized speed-up, measured as the quotient of the throughputs between the packed
310"
ABLATION STUDY,0.5812720848056537,"and unpacked runs, is slightly lower to the theoretical throughput (i.e. the packing factor) due to
311"
ABLATION STUDY,0.5830388692579506,"the packing overhead. Further learning curves with the loss function and accuracy are provided in
312"
ABLATION STUDY,0.5848056537102474,"Section P. For the ﬁne-tuning training on SQuAD 1.1, we do not use packing. The scores, computed
313"
ABLATION STUDY,0.5865724381625441,"as the median of 10 different seeds, are displayed in Table 3. They are comparable to the reference
314"
ABLATION STUDY,0.588339222614841,"ones in [6]: for BERT base (resp. large) the F1 score is reduced by 0.2% (resp. 0.3%) and the EM
315"
ABLATION STUDY,0.5901060070671378,"score increases by 0.3% (resp. 0.02%).
316"
ABLATION STUDY,0.5918727915194346,"Table 2: Measured speed-ups in BERT
pretraining with packing."
ABLATION STUDY,0.5936395759717314,"Model
Sequence
Packing
Realized
size
length
factor
speed-up"
ABLATION STUDY,0.5954063604240283,"base
128
1.17
1.15
384
1.70
1.68"
ABLATION STUDY,0.5971731448763251,"large
128
1.17
1.15
384
1.70
1.69"
ABLATION STUDY,0.598939929328622,"Table 3: SQuAD 1.1 scores after BERT pretrain-
ing with packing."
ABLATION STUDY,0.6007067137809188,"Model
Conﬁguration
F1
Exact
size
match"
ABLATION STUDY,0.6024734982332155,"base
[6]
88.5
80.8
Packed
88.32
81.03"
ABLATION STUDY,0.6042402826855123,"large
[6]
90.9
84.1
Packed
90.65
84.12"
ABLATION STUDY,0.6060070671378092,"4.4
Scaling analysis: Impact of accelerators count
317"
ABLATION STUDY,0.607773851590106,"A further advantage of packing over competing un-padding approaches is the inherent load balancing
318"
ABLATION STUDY,0.6095406360424028,"provided by packing. So called un-padding approaches rely on dynamically launching custom kernels
319"
ABLATION STUDY,0.6113074204946997,"that ignore padding. A stated advantage of such implementations is the ability to avoid computing
320"
ABLATION STUDY,0.6130742049469965,"the complete (512 x 512) attention matrix. This provides additional computational savings compared
321"
ABLATION STUDY,0.6148409893992933,"to packing, where the attention matrix is computed in its entirety and then masked. Because of
322"
ABLATION STUDY,0.6166077738515902,"these additional savings, un-padding can exceed the theoretical upper bound for speed-up from
323"
ABLATION STUDY,0.6183745583038869,"packing (2.013 on Wikipedia). As a result of the dynamic nature of the approach, the processing
324"
ABLATION STUDY,0.6201413427561837,"time with un-padding is different for each sequence in the batch, and the amount of time required to
325"
ABLATION STUDY,0.6219081272084805,"process a batch of sequences will be determined by the processing time of the longest sequence in
326"
ABLATION STUDY,0.6236749116607774,"the batch (with the sequences being processed in parallel). Furthermore, in the multiple accelerator
327"
ABLATION STUDY,0.6254416961130742,"setting the processing time on each device will vary depending on the sequences in the batch that it
328"
ABLATION STUDY,0.627208480565371,"receives. Devices which ﬁnish early have to wait for the slowest device to ﬁnish before exchanging
329"
ABLATION STUDY,0.6289752650176679,"gradients. This load-imbalance between the devices (and inside the batch) leads to a considerable
330"
ABLATION STUDY,0.6307420494699647,"decrease in the speed-up from un-padding as the number of accelerators is increased (see Figure 5
331"
ABLATION STUDY,0.6325088339222615,"and Section E [1]). In contrast, packing (our approach) is inherently load-balanced. The processing
332"
ABLATION STUDY,0.6342756183745583,"time on each accelerator is independent of the content inside the batch received by the device. Any
333"
ABLATION STUDY,0.6360424028268551,"number of accelerators can therefore operate in unison without having to wait for the slowest batch to
334"
ABLATION STUDY,0.6378091872791519,"process (all per-device batches are equally fast).
335"
ABLATION STUDY,0.6395759717314488,Figure 5: Comparison of the theoretical speed-up as the number of accelerators is increased.
CONCLUSION,0.6413427561837456,"5
Conclusion
336"
CONCLUSION,0.6431095406360424,"Whereas packing is a well known concept, this paper sheds a new light onto it in multiple aspects.
337"
CONCLUSION,0.6448763250883393,"First, we visualize the sequence length distributions of multiple datasets not just from language
338"
CONCLUSION,0.6466431095406361,"domains but also audio and molecular domains to emphasize that packing is beneﬁcial for a lot of
339"
CONCLUSION,0.6484098939929329,"datasets and that in many cases, more than 2x acceleration can be achieved by removing 50% or
340"
CONCLUSION,0.6501766784452296,"more padding. Second, we provide two new highly efﬁcient packing approaches based on established
341"
CONCLUSION,0.6519434628975265,"solvers that leave almost no padding and that can tackle arbitrarily large datasets in a matter of
342"
CONCLUSION,0.6537102473498233,"seconds, in contrast to existing approaches that are slow and suboptimal. Third, we demonstrate that
343"
CONCLUSION,0.6554770318021201,"without adjusting the sequence processing algorithm (e.g., BERT) to the packed sequences, predictive
344"
CONCLUSION,0.657243816254417,"performance is reduced. Thus, we propose several model adjustments that are all necessary to keep
345"
CONCLUSION,0.6590106007067138,"predictive performance. Last but not least, we prove that, thanks to such adjustments, predictive
346"
CONCLUSION,0.6607773851590106,"performance is preserved as if no packing was used — but speed signiﬁcantly increases, especially
347"
CONCLUSION,0.6625441696113075,"since the adjustments come with an overhead of less than 5%. We prove in our experiments that
348"
CONCLUSION,0.6643109540636042,"downstream performance is not impacted by packing and that the anticipated 2x acceleration can be
349"
CONCLUSION,0.666077738515901,"achieved.
350"
CONCLUSION,0.6678445229681979,"In the future, an interesting direction is the packing of images of different sizes to help accelerate
351"
CONCLUSION,0.6696113074204947,"computer-vision applications. This is especially relevant given the recent advances in the use of
352"
CONCLUSION,0.6713780918727915,"transformer-based approaches in the computer vision domain, for example the visual transformer [33].
353"
CONCLUSION,0.6731448763250883,"Note that many images come in different shapes and resolutions and packing them can be a new
354"
CONCLUSION,0.6749116607773852,"approach to tackle this diversity instead of casting them all to the same resolution and shape. Masking
355"
CONCLUSION,0.676678445229682,"out the self-attention within transformers is easier to implement than avoiding cross-contamination of
356"
CONCLUSION,0.6784452296819788,"convolutions applied to packed images. Future work should explore improving the performance of
357"
CONCLUSION,0.6802120141342756,"other models (RoBERTa, GPT-3, T5) by avoiding contamination between non-contiguous segments
358"
CONCLUSION,0.6819787985865724,"from different documents. Even BERT itself might beneﬁt from avoiding contamination between the
359"
CONCLUSION,0.6837455830388692,"two concatenated segments.
360"
REFERENCES,0.6855123674911661,"References
361"
REFERENCES,0.6872791519434629,"[1] ANONYMOUS. Supplemental Material for “Efﬁcient Sequence Packing without Cross-contamination:
362"
REFERENCES,0.6890459363957597,"Accelerating Large Language Models without Impacting Performance’, 2022.
363"
REFERENCES,0.6908127208480566,"[2] BOTTOU, L., CURTIS, F. E., AND NOCEDAL, J. Optimization Methods for Large-Scale Machine Learning.
364"
REFERENCES,0.6925795053003534,"SIAM Review 60, 2 (jan 2018), 223–311.
365"
REFERENCES,0.6943462897526502,"[3] BRO, R., AND DE JONG, S. A fast non-negativity-constrained least squares algorithm. Journal of
366"
REFERENCES,0.696113074204947,"Chemometrics 11, 5 (sep 1997), 393–401.
367"
REFERENCES,0.6978798586572438,"[4] BROWN, T. B., MANN, B., RYDER, N., SUBBIAH, M., KAPLAN, J., DHARIWAL, P., NEELAKANTAN,
368"
REFERENCES,0.6996466431095406,"A., SHYAM, P., SASTRY, G., ASKELL, A., AGARWAL, S., HERBERT-VOSS, A., KRUEGER, G.,
369"
REFERENCES,0.7014134275618374,"HENIGHAN, T., CHILD, R., RAMESH, A., ZIEGLER, D. M., WU, J., WINTER, C., HESSE, C., CHEN,
370"
REFERENCES,0.7031802120141343,"M., SIGLER, E., LITWIN, M., GRAY, S., CHESS, B., CLARK, J., BERNER, C., MCCANDLISH, S.,
371"
REFERENCES,0.7049469964664311,"RADFORD, A., SUTSKEVER, I., AND AMODEI, D. Language Models are Few-Shot Learners. In Advances
372"
REFERENCES,0.7067137809187279,"in Neural Information Processing Systems 33 pre-proceedings (NeurIPS 2020) (may 2020).
373"
REFERENCES,0.7084805653710248,"[5] BYTEDANCE INC.
Effective Transformer.
https://github.com/bytedance/effective_
374"
REFERENCES,0.7102473498233216,"transformer, 2021.
375"
REFERENCES,0.7120141342756183,"[6] DEVLIN, J., CHANG, M. W., LEE, K., AND TOUTANOVA, K. BERT: Pre-training of deep bidirectional
376"
REFERENCES,0.7137809187279152,"transformers for language understanding. NAACL HLT 2019 - 2019 Conference of the North American
377"
REFERENCES,0.715547703180212,"Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings
378"
REFERENCES,0.7173144876325088,"of the Conference 1 (oct 2019), 4171–4186.
379"
REFERENCES,0.7190812720848057,"[7] DEVLIN, J., CHANG, M. W., LEE, K., AND TOUTANOVA, K. BERT: Pre-training of Deep Bidirectional
380"
REFERENCES,0.7208480565371025,"Transformers for Language Understanding. https://github.com/google-research/bert, 2019.
381"
REFERENCES,0.7226148409893993,"[8] DEVLIN, J., CHANG, M. W., LEE, K., AND TOUTANOVA, K.
Pre-training data creation script
382"
REFERENCES,0.7243816254416962,"for BERT. https://github.com/google-research/bert/blob/master/create_pretraining_
383"
REFERENCES,0.726148409893993,"data.py#L243, 2019.
384"
REFERENCES,0.7279151943462897,"[9] FEDUS, W., ZOPH, B., AND SHAZEER, N. Switch Transformers: Scaling to Trillion Parameter Models
385"
REFERENCES,0.7296819787985865,"with Simple and Efﬁcient Sparsity. arXiv (jan 2021).
386"
REFERENCES,0.7314487632508834,"[10] INTEL, 2021.
387"
REFERENCES,0.7332155477031802,"[11] JIA, Z., TILLMAN, B., MAGGIONI, M., AND SCARPAZZA, D. P.
Dissecting the Graphcore IPU
388"
REFERENCES,0.734982332155477,"architecture via microbenchmarking. ArXiv abs/1912.03413 (2019).
389"
REFERENCES,0.7367491166077739,"[12] JOHNSON, D. S. Near-optimal bin packing algorithms. PhD thesis, Massachusetts Institute of Technology,
390"
REFERENCES,0.7385159010600707,"1973.
391"
REFERENCES,0.7402826855123675,"[13] JOHNSON, D. S., AND GAREY, M. R. A 7160 theorem for bin packing. Journal of Complexity 1, 1 (oct
392"
REFERENCES,0.7420494699646644,"1985), 65–106.
393"
REFERENCES,0.7438162544169611,"[14] KORTE, B., AND VYGEN, J. Combinatorial Optimization, vol. 21 of Algorithms and Combinatorics.
394"
REFERENCES,0.7455830388692579,"Springer Berlin Heidelberg, Berlin, Heidelberg, 2012.
395"
REFERENCES,0.7473498233215548,"[15] LEE, C. C., AND LEE, D. T. A Simple On-Line Bin-Packing Algorithm. Journal of the ACM (JACM) 32,
396"
REFERENCES,0.7491166077738516,"3 (jul 1985), 562–572.
397"
REFERENCES,0.7508833922261484,"[16] LIU, Y., OTT, M., GOYAL, N., DU, J., JOSHI, M., CHEN, D., LEVY, O., LEWIS, M., ZETTLEMOYER,
398"
REFERENCES,0.7526501766784452,"L., AND STOYANOV, V. RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv (jul 2019).
399"
REFERENCES,0.7544169611307421,"[17] MATTSON, P., REDDI, V. J., CHENG, C., COLEMAN, C., DIAMOS, G., KANTER, D., MICIKEVICIUS,
400"
REFERENCES,0.7561837455830389,"P., PATTERSON, D., SCHMUELLING, G., TANG, H., WEI, G., AND WU, C. MLPerf: An Industry
401"
REFERENCES,0.7579505300353356,"Standard Benchmark Suite for Machine Learning Performance. IEEE Micro 40, 2 (2020), 8–16.
402"
REFERENCES,0.7597173144876325,"[18] MENG, Q., CHEN, W., WANG, Y., MA, Z. M., AND LIU, T. Y. Convergence analysis of distributed
403"
REFERENCES,0.7614840989399293,"stochastic gradient descent with shufﬂing. Neurocomputing 337 (apr 2019), 46–57.
404"
REFERENCES,0.7632508833922261,"[19] MLCOMMONS. v0.7 Results. https://mlcommons.org/en/training-normal-07/, 2020. Result
405"
REFERENCES,0.765017667844523,"not veriﬁed by MLPerf. Throughput/speedup is not the primary metric of MLPerf. MLPerf name and logo
406"
REFERENCES,0.7667844522968198,"are trademarks. See www.mlperf.org for more information.
407"
REFERENCES,0.7685512367491166,"[20] NVIDIA.
Reference numbers for BERT un-padding results.
https://github.com/mlcommons/
408"
REFERENCES,0.7703180212014135,"training_results_v0.7/blob/master/NVIDIA/results/dgxa100_ngc20.06_pytorch/bert/
409"
REFERENCES,0.7720848056537103,"result_0.txt, 2020. Throughput/speedup is not the primary metric of MLPerf. MLPerf name and logo
410"
REFERENCES,0.773851590106007,"are trademarks. See www.mlperf.org for more information.
411"
REFERENCES,0.7756183745583038,"[21] NVIDIA.
Faster Transformer.
https://github.com/NVIDIA/DeepLearningExamples/tree/
412"
REFERENCES,0.7773851590106007,"master/FasterTransformer/v1, 2021.
413"
REFERENCES,0.7791519434628975,"[22] OTT, M., EDUNOV, S., BAEVSKI, A., FAN, A., GROSS, S., NG, N., GRANGIER, D., AND AULI,
414"
REFERENCES,0.7809187279151943,"M. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of NAACL-HLT 2019:
415"
REFERENCES,0.7826855123674912,"Demonstrations (2019).
416"
REFERENCES,0.784452296819788,"[23] PANAYOTOV, V., CHEN, G., POVEY, D., AND KHUDANPUR, S. Librispeech: an asr corpus based on
417"
REFERENCES,0.7862190812720848,"public domain audio books. In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International
418"
REFERENCES,0.7879858657243817,"Conference on (2015), IEEE, pp. 5206–5210.
419"
REFERENCES,0.7897526501766784,"[24] RAFFEL, C., SHAZEER, N., ROBERTS, A., LEE, K., NARANG, S., MATENA, M., ZHOU, Y., LI, W.,
420"
REFERENCES,0.7915194346289752,"AND LIU, P. J. Exploring the Limits of Transfer Learning with a Uniﬁed Text-to-Text Transformer. Journal
421"
REFERENCES,0.7932862190812721,"of Machine Learning Research 21 (oct 2019).
422"
REFERENCES,0.7950530035335689,"[25] RAJPURKAR, P., ZHANG, J., LOPYREV, K., AND LIANG, P. SQuAD: 100,000+ questions for machine
423"
REFERENCES,0.7968197879858657,"comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language
424"
REFERENCES,0.7985865724381626,"Processing (Austin, Texas, Nov. 2016), Association for Computational Linguistics, pp. 2383–2392.
425"
REFERENCES,0.8003533568904594,"[26] RAMAKRISHNAN, R., DRAL, P. O., RUPP, M., AND VON LILIENFELD, O. A. Quantum chemistry
426"
REFERENCES,0.8021201413427562,"structures and properties of 134 kilo molecules. Scientiﬁc Data 1 (2014).
427"
REFERENCES,0.803886925795053,"[27] RUDDIGKEIT, L., VAN DEURSEN, R., BLUM, L. C., AND REYMOND, J.-L. Enumeration of 166 billion
428"
REFERENCES,0.8056537102473498,"organic small molecules in the chemical universe database gdb-17. Journal of Chemical Information and
429"
REFERENCES,0.8074204946996466,"Modeling 52, 11 (2012), 2864–2875. PMID: 23088335.
430"
REFERENCES,0.8091872791519434,"[28] SHEN, J., NGUYEN, P., WU, Y., CHEN, Z., ET AL. Lingvo: a modular and scalable framework for
431"
REFERENCES,0.8109540636042403,"sequence-to-sequence modeling, 2019.
432"
REFERENCES,0.8127208480565371,"[29] VASWANI, A., SHAZEER, N., PARMAR, N., USZKOREIT, J., JONES, L., GOMEZ, A. N., KAISER, U.,
433"
REFERENCES,0.8144876325088339,"AND POLOSUKHIN, I. Attention is all you need. In Proceedings of the 31st International Conference on
434"
REFERENCES,0.8162544169611308,"Neural Information Processing Systems (Red Hook, NY, USA, 2017), NIPS’17, Curran Associates Inc.,
435"
REFERENCES,0.8180212014134276,"p. 6000–6010.
436"
REFERENCES,0.8197879858657244,"[30] WANG, A., SINGH, A., MICHAEL, J., HILL, F., LEVY, O., AND BOWMAN, S. GLUE: A multi-task
437"
REFERENCES,0.8215547703180212,"benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP
438"
REFERENCES,0.823321554770318,"Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP (Brussels, Belgium, Nov.
439"
REFERENCES,0.8250883392226148,"2018), Association for Computational Linguistics, pp. 353–355.
440"
REFERENCES,0.8268551236749117,"[31] WARSTADT, A., SINGH, A., AND BOWMAN, S. R. Neural network acceptability judgments. arXiv
441"
REFERENCES,0.8286219081272085,"preprint arXiv:1805.12471 (2018).
442"
REFERENCES,0.8303886925795053,"[32] WOLF, T., DEBUT, L., SANH, V., CHAUMOND, J., DELANGUE, C., MOI, A., CISTAC, P., RAULT, T.,
443"
REFERENCES,0.8321554770318021,"LOUF, R., FUNTOWICZ, M., DAVISON, J., SHLEIFER, S., VON PLATEN, P., MA, C., JERNITE, Y., PLU,
444"
REFERENCES,0.833922261484099,"J., XU, C., SCAO, T. L., GUGGER, S., DRAME, M., LHOEST, Q., AND RUSH, A. M. Transformers: State-
445"
REFERENCES,0.8356890459363958,"of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in
446"
REFERENCES,0.8374558303886925,"Natural Language Processing: System Demonstrations (Online, Oct. 2020), Association for Computational
447"
REFERENCES,0.8392226148409894,"Linguistics, pp. 38–45.
448"
REFERENCES,0.8409893992932862,"[33] WU, B., XU, C., DAI, X., WAN, A., ZHANG, P., YAN, Z., TOMIZUKA, M., GONZALEZ, J., KEUTZER,
449"
REFERENCES,0.842756183745583,"K., AND VAJDA, P. Visual transformers: Token-based image representation and processing for computer
450"
REFERENCES,0.8445229681978799,"vision, 2020.
451"
REFERENCES,0.8462897526501767,"[34] XLA, T. XLA: Optimizing Compiler for Machine Learning. https://www.tensorflow.org/xla,
452"
REFERENCES,0.8480565371024735,"2021.
453"
REFERENCES,0.8498233215547704,"[35] YOU, Y., LI, J., REDDI, S., HSEU, J., KUMAR, S., BHOJANAPALLI, S., SONG, X., DEMMEL, J.,
454"
REFERENCES,0.8515901060070671,"KEUTZER, K., AND HSIEH, C.-J. Large Batch Optimization for Deep Learning: Training BERT in 76
455"
REFERENCES,0.8533568904593639,"minutes. arXiv (apr 2019).
456"
REFERENCES,0.8551236749116607,"[36] YUE, M., AND ZHANG, L. A simple proof of the inequality MFFD(L) 71/60OPT(L) + 1, L for
457"
REFERENCES,0.8568904593639576,"the MFFD bin-packing algorithm. Acta Mathematicae Applicatae Sinica 11, 3 (jul 1995), 318–330.
458"
REFERENCES,0.8586572438162544,"Checklist
459"
REFERENCES,0.8604240282685512,"1. For all authors...
460"
REFERENCES,0.8621908127208481,"(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s
461"
REFERENCES,0.8639575971731449,"contributions and scope? [Yes] Our paper has four main claims. First, in Figure 1
462"
REFERENCES,0.8657243816254417,"we show the sequence length distribution of Wikipedia and many other datasets and
463"
REFERENCES,0.8674911660777385,"the excessive padding that they require. Second, in Section 4.1, we show that we
464"
REFERENCES,0.8692579505300353,"can efﬁciently pack the data which can be easily reproduced with the shared data and
465"
REFERENCES,0.8710247349823321,"code [1]. Third, in Figure 3[right], we clearly show the 2x performance gain from
466"
REFERENCES,0.872791519434629,"packing and the related hyperparameter adjustment scheme. Fourth, multiple additional
467"
REFERENCES,0.8745583038869258,"experiments on downstream tasks, ablation studies, and packing variants further verify
468"
REFERENCES,0.8763250883392226,"the validity of our proposed approaches.
469"
REFERENCES,0.8780918727915195,"(b) Did you describe the limitations of your work? [Yes] We see three potential limitations
470"
REFERENCES,0.8798586572438163,"that we discuss in the paper. First, as stated in Section A “Broader Impact” in the
471"
REFERENCES,0.8816254416961131,"appendix [1], our approach is clearly dependent on the sequence length distribution
472"
REFERENCES,0.8833922261484098,"of the dataset. However, we looked into several other datasets beyond Wikipedia and
473"
REFERENCES,0.8851590106007067,"observed even higher potential for acceleration and document this in multiple sections
474"
REFERENCES,0.8869257950530035,"throughout the paper as well as in the appendix [1]. Second, we explain our focus
475"
REFERENCES,0.8886925795053003,"on the IPU hardware in Section 4.1. Our theoretical analysis in Section 4.4 indicates
476"
REFERENCES,0.8904593639575972,"that our approach beneﬁts also GPUs. We also cite other work, that shows that our
477"
REFERENCES,0.892226148409894,"approach is hardware independent. Third, our changes to the network with a modiﬁed
478"
REFERENCES,0.8939929328621908,"attention mask and loss calculation come with some overhead. This is addressed in
479"
REFERENCES,0.8957597173144877,"Table 1 [overhead column] in Section 4.1.
480"
REFERENCES,0.8975265017667845,"(c) Did you discuss any potential negative societal impacts of your work? [Yes] We address
481"
REFERENCES,0.8992932862190812,"this point in Section A “Broader Impact”, third paragraph, in the appendix [1].
482"
REFERENCES,0.901060070671378,"(d) Have you read the ethics review guidelines and ensured that your paper conforms to
483"
REFERENCES,0.9028268551236749,"them? [Yes]
484"
REFERENCES,0.9045936395759717,"2. If you are including theoretical results...
485"
REFERENCES,0.9063604240282686,"(a) Did you state the full set of assumptions of all theoretical results? [Yes] Detailed
486"
REFERENCES,0.9081272084805654,"algorithm explanations, clariﬁcations of assumptions, and proofs are provided in the
487"
REFERENCES,0.9098939929328622,"supplemental material [1].
488"
REFERENCES,0.911660777385159,"(b) Did you include complete proofs of all theoretical results? [Yes] Sections D, E, and
489"
REFERENCES,0.9134275618374559,"G in the supplemental material [1] provide the necessary derivations on theoretical
490"
REFERENCES,0.9151943462897526,"results.
491"
REFERENCES,0.9169611307420494,"3. If you ran experiments...
492"
REFERENCES,0.9187279151943463,"(a) Did you include the code, data, and instructions needed to reproduce the main experi-
493"
REFERENCES,0.9204946996466431,"mental results (either in the supplemental material or as a URL)? [Yes] All packing
494"
REFERENCES,0.9222614840989399,"code is provided in the paper. The packing results on BERT got veriﬁed by multiple
495"
REFERENCES,0.9240282685512368,"independent parties. One party used a draft of this paper to successfully reproduce its
496"
REFERENCES,0.9257950530035336,"main ﬁndings. Links to implementations in three different frameworks will be provided
497"
REFERENCES,0.9275618374558304,"after acceptance, to avoid violating the blind submission rules.
498"
REFERENCES,0.9293286219081273,"(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
499"
REFERENCES,0.931095406360424,"were chosen)? [Yes] In the ﬁrst part, we follow the MLPerf 0.7 benchmark rules.
500"
REFERENCES,0.9328621908127208,"We document the parameters that we changed and why we change them. For the
501"
REFERENCES,0.9346289752650176,"downstream tasks, we follow the reference and report where, how and why we change
502"
REFERENCES,0.9363957597173145,"hyperparameters.
503"
REFERENCES,0.9381625441696113,"(c) Did you report error bars (e.g., with respect to the random seed after running exper-
504"
REFERENCES,0.9399293286219081,"iments multiple times)? [No] The packing algorithms are deterministic and have no
505"
REFERENCES,0.941696113074205,"error. Other experiments are only executed once to compare convergence curves. For
506"
REFERENCES,0.9434628975265018,"downstream tasks, we report repetition details and the median as in the reference.
507"
REFERENCES,0.9452296819787986,"(d) Did you include the total amount of compute and the type of resources used (e.g., type
508"
REFERENCES,0.9469964664310954,"of GPUs, internal cluster, or cloud provider)? [Yes] We used 16 Graphcore Mk2 IPUs
509"
REFERENCES,0.9487632508833922,"for acceleration on an internal cluster.
510"
REFERENCES,0.950530035335689,"4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
511"
REFERENCES,0.9522968197879859,"(a) If your work uses existing assets, did you cite the creators? [Yes] Appropriate references
512"
REFERENCES,0.9540636042402827,"to the BERT authors, all datasets, and the code snippet from the HugginFace inc. are
513"
REFERENCES,0.9558303886925795,"appropriately referenced with citations and links.
514"
REFERENCES,0.9575971731448764,"(b) Did you mention the license of the assets? [Yes] For the only taken code snippet, the
515"
REFERENCES,0.9593639575971732,"license is part of the ﬁle [Listing 7 in [1]]. Dataset licenses like Wikipedia’s “Creative
516"
REFERENCES,0.9611307420494699,"Commons Attribution-ShareAlike 3.0 License” are covered by the references. New
517"
REFERENCES,0.9628975265017667,"materials like packing code and histograms will be provided under an MIT license
518"
REFERENCES,0.9646643109540636,"which will be added over a link to the resources in the ﬁnal paper version.
519"
REFERENCES,0.9664310954063604,"(c) Did you include any new assets either in the supplemental material or as a URL?
520"
REFERENCES,0.9681978798586572,"[Yes] New materials like packing code and histograms are included in the supplement
521"
REFERENCES,0.9699646643109541,"document as well as separate ﬁle. To avoid violating the blind submission rules, they
522"
REFERENCES,0.9717314487632509,"will be linked in the ﬁnal version like many other assets which are already publicly
523"
REFERENCES,0.9734982332155477,"available under MIT license.
524"
REFERENCES,0.9752650176678446,"(d) Did you discuss whether and how consent was obtained from people whose data you’re
525"
REFERENCES,0.9770318021201413,"using/curating? [N/A] We did not curate other people’s data. We only provide a very
526"
REFERENCES,0.9787985865724381,"high level aggregate of the used data.
527"
REFERENCES,0.980565371024735,"(e) Did you discuss whether the data you are using/curating contains personally identiﬁable
528"
REFERENCES,0.9823321554770318,"information or offensive content? [N/A] We did not curate other people’s data.
529"
REFERENCES,0.9840989399293286,"5. If you used crowdsourcing or conducted research with human subjects...
530"
REFERENCES,0.9858657243816255,"(a) Did you include the full text of instructions given to participants and screenshots, if
531"
REFERENCES,0.9876325088339223,"applicable? [N/A] Our experiments did not include crowdsourcing or human subjects.
532"
REFERENCES,0.9893992932862191,"(b) Did you describe any potential participant risks, with links to Institutional Review Board
533"
REFERENCES,0.991166077738516,"(IRB) approvals, if applicable? [N/A] Our experiments did not include crowdsourcing
534"
REFERENCES,0.9929328621908127,"or human subjects.
535"
REFERENCES,0.9946996466431095,"(c) Did you include the estimated hourly wage paid to participants and the total amount
536"
REFERENCES,0.9964664310954063,"spent on participant compensation? [N/A] Our experiments did not include crowd-
537"
REFERENCES,0.9982332155477032,"sourcing or human subjects.
538"
