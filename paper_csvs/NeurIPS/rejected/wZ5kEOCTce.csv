Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0010395010395010396,"In this work, we examine the impact of inter-patch dependencies in the decoder of
1"
ABSTRACT,0.002079002079002079,"masked autoencoders (MAE) on representation learning. We decompose the decod-
2"
ABSTRACT,0.0031185031185031187,"ing mechanism for masked reconstruction into self-attention between mask tokens
3"
ABSTRACT,0.004158004158004158,"and cross-attention between masked and visible tokens. Our findings reveal that
4"
ABSTRACT,0.005197505197505198,"MAE reconstructs coherent images from visible patches not through interactions
5"
ABSTRACT,0.006237006237006237,"between patches in the decoder but by learning a global representation within the
6"
ABSTRACT,0.007276507276507277,"encoder. This discovery leads us to propose a simple visual pretraining framework:
7"
ABSTRACT,0.008316008316008316,"cross-attention masked autoencoders (CrossMAE). This framework employs only
8"
ABSTRACT,0.009355509355509356,"cross-attention in the decoder to independently read out reconstructions for a small
9"
ABSTRACT,0.010395010395010396,"subset of masked patches from encoder outputs, yet it achieves comparable or
10"
ABSTRACT,0.011434511434511435,"superior performance to traditional MAE across models ranging from ViT-S to
11"
ABSTRACT,0.012474012474012475,"ViT-H. By its design, CrossMAE challenges the necessity of interaction between
12"
ABSTRACT,0.013513513513513514,"mask tokens for effective masked pretraining. Code is available here.
13"
INTRODUCTION,0.014553014553014554,"1
Introduction
14"
INTRODUCTION,0.015592515592515593,"Masked image modeling [46, 30, 61, 4] has emerged as a pivotal unsupervised learning technique
15"
INTRODUCTION,0.016632016632016633,"in computer vision. One such recent work following this paradigm is masked autoencoders (MAE):
16"
INTRODUCTION,0.017671517671517672,"given only a small, random subset of visible image patches, the model is tasked to reconstruct the
17"
INTRODUCTION,0.018711018711018712,"missing pixels. By operating mostly on this small subset of visible tokens, MAE can efficiently
18"
INTRODUCTION,0.01975051975051975,"pre-train high-capacity models on large-scale vision datasets, demonstrating impressive results on a
19"
INTRODUCTION,0.02079002079002079,"wide array of downstream tasks [33, 38, 49].
20"
INTRODUCTION,0.02182952182952183,"The MAE framework employs self-attention across the entire model for self-supervised reconstruction
21"
INTRODUCTION,0.02286902286902287,"tasks. In this setup, both masked and visible tokens engage in self-attention, not just with each other
22"
INTRODUCTION,0.02390852390852391,"but also with themselves, aiming to generate a holistic and context-aware representation. However,
23"
INTRODUCTION,0.02494802494802495,"the masked tokens inherently lack information. Intuitively, facilitating information exchange among
24"
INTRODUCTION,0.02598752598752599,"adjacent masked tokens should enable the model to synthesize a more coherent image, thereby
25"
INTRODUCTION,0.02702702702702703,"accomplishing the task of masked reconstruction and improving representation learning. A question
26"
INTRODUCTION,0.028066528066528068,"arises, though: Is this truly the case?
27"
INTRODUCTION,0.029106029106029108,"We decompose the decoding process of each mask token into two parallel components: self-attention
28"
INTRODUCTION,0.030145530145530147,"with other mask tokens, as well as cross-attention to the encoded visible tokens. If MAE relies on
29"
INTRODUCTION,0.031185031185031187,"the self-attention with other mask tokens, its average should be on par with the cross-attention. Yet,
30"
INTRODUCTION,0.032224532224532226,"the quantitative comparison in Figure 1.(b) shows the magnitude of mask token-to-visible token
31"
INTRODUCTION,0.033264033264033266,"cross-attention (1.42) in the MAE decoder evaluated over the entire ImageNet validation set far
32"
INTRODUCTION,0.034303534303534305,"exceeds that of mask token-to-mask token self-attention (0.39).
33"
INTRODUCTION,0.035343035343035345,"This initial observation prompts two questions: 1) Is the self-attention mechanism among mask
34"
INTRODUCTION,0.036382536382536385,"tokens in the decoder necessary for effective representation learning? 2) If not, can each patch be
35"
INTRODUCTION,0.037422037422037424,"Figure 1: Method Overview. (A) Masked autoencoder (MAE) starts by masking random patches of the input
image. (B) To reconstruct a mask token (marked by the blue star), MAE attends to both the masked tokens
(B.Left) and the visible tokens (B.Right). A quantitative comparison over the ImageNet validation set shows
that the masked tokens in MAE disproportionally attend to the visible tokens (1.42 vs 0.39), questioning the
necessity of attention within mask tokens. (C) We propose CrossMAE, the masked patches are reconstructed
from only the cross attention between the masked tokens and the visible tokens. Surprisingly, CrossMAE attains
the same or better performance than MAE on ImageNet classification and COCO instance segmentation."
INTRODUCTION,0.038461538461538464,"Figure 2: Example reconstructions of ImageNet validation images. For each set of 5 images, from left to right,
are the original image, masked image with a mask ratio of 75%, MAE [30], CrossMAE (trained to reconstruct
25% of image tokens, or 1/3 of the mask tokens), and CrossMAE (trained to reconstruct all masked tokens).
Since CrossMAE does not reconstruct them, all model outputs have the visible patches overlaid. Intriguingly,
CrossMAE, when trained for partial reconstruction, can decode all mask tokens in one forward pass (shown
above), indicating that the encoder rather than the decoder effectively captures global image information in
its output tokens. Its comparable reconstruction quality to full-image-trained models suggests that full-image
reconstruction might not be essential for effective representation learning."
INTRODUCTION,0.0395010395010395,"independently read out from the encoder output, allowing the reconstruction of only a small subset of
36"
INTRODUCTION,0.04054054054054054,"masked patches, which in turn, accelerates the pretraining without performance degradation?
37"
INTRODUCTION,0.04158004158004158,"In addressing these questions, we introduce CrossMAE, which diverges from MAE in three ways:
38"
INTRODUCTION,0.04261954261954262,"1. Cross-attention for decoding. Rather than passing a concatenation of mask and visible
39"
INTRODUCTION,0.04365904365904366,"tokens to a self-attention decoder, CrossMAE uses mask tokens as queries to read out the masked
40"
INTRODUCTION,0.0446985446985447,"reconstructions from the visible tokens in a cross-attention decoder. In this setting, mask tokens
41"
INTRODUCTION,0.04573804573804574,"incorporate information from the visible tokens but do not interact with other mask tokens, thereby
42"
INTRODUCTION,0.04677754677754678,"reducing the sequence length for the decoder and cutting down computational costs.
43"
INTRODUCTION,0.04781704781704782,"2. Independent partial reconstruction. With self-attention removed, the decoding of each mask
44"
INTRODUCTION,0.04885654885654886,"token, based on the encoded features from visible tokens, becomes conditionally independent. This
45"
INTRODUCTION,0.0498960498960499,"enables the decoding of only a fraction of masked tokens rather than the entire image.
46"
INTRODUCTION,0.05093555093555094,"3. Inter-block attention. Due to the separation of visible and mask tokens, we can use features
47"
INTRODUCTION,0.05197505197505198,"from different encoder blocks for each decoder block. Empirically, we find solely relying on the last
48"
INTRODUCTION,0.05301455301455302,"encoder feature map for reconstruction, the design present in MAE, hurts feature learning. We propose
49"
INTRODUCTION,0.05405405405405406,"a lightweight inter-block attention mechanism that allows the CrossMAE decoder to leverage a mix
50"
INTRODUCTION,0.0550935550935551,"of low-level and high-level feature maps from the encoder, improving the learned representation.
51"
INTRODUCTION,0.056133056133056136,"The analysis performed on CrossMAE led to a novel way to understand MAE. Even though the
52"
INTRODUCTION,0.057172557172557176,"patches to be reconstructed are independently decoded, our findings demonstrate that coherent
53"
INTRODUCTION,0.058212058212058215,"reconstruction for each masked patch can be independently read out from the encoder output, without
54"
INTRODUCTION,0.059251559251559255,"any interactions among masked tokens in the decoder for consistency (Figure 2). Furthermore, the
55"
INTRODUCTION,0.060291060291060294,"downstream performance of the model remains robust even without these interactions (Figure 1.(c),
56"
INTRODUCTION,0.061330561330561334,"Tables 1 and 2). Both pieces of evidence confirm that the encoder’s output features already encapsulate
57"
INTRODUCTION,0.062370062370062374,"the necessary global context for image reconstruction, while the decoder simply performs a readout
58"
INTRODUCTION,0.06340956340956341,"from the encoder output to reconstruct the pixels at the location of each patch.
59"
INTRODUCTION,0.06444906444906445,"To sum up, our main contributions are the following:
60"
INTRODUCTION,0.06548856548856549,"1. We present a novel understanding of MAE. Our findings show that MAE reconstructs coherent
61"
INTRODUCTION,0.06652806652806653,"images from visible patches not through interactions between patches to be reconstructed in the
62"
INTRODUCTION,0.06756756756756757,"decoder but by learning a global representation within the encoder. This is evidenced by the model’s
63"
INTRODUCTION,0.06860706860706861,"ability to generate coherent images and maintain robust downstream performance without such
64"
INTRODUCTION,0.06964656964656965,"interactions, indicating the encoder effectively captures global image information.
65"
INTRODUCTION,0.07068607068607069,"2. We advocate replacing self-attention layers with a simple cross-attention readout function.
66"
INTRODUCTION,0.07172557172557173,"Given our discovery that the encoder in MAE already captures a comprehensive global representation,
67"
INTRODUCTION,0.07276507276507277,"we propose replacing self-attention layers in the decoder with a more efficient information readout
68"
INTRODUCTION,0.07380457380457381,"function. Specifically, we suggest utilizing cross-attention to aggregate the output tokens of the
69"
INTRODUCTION,0.07484407484407485,"encoder into each input token within the decoder layers independently, thereby eliminating the need
70"
INTRODUCTION,0.07588357588357589,"for token-to-token communication within the decoder.
71"
CROSSMAE ACHIEVES COMPARABLE OR SUPERIOR PERFORMANCE WITH REDUCED COMPUTATIONAL,0.07692307692307693,"3. CrossMAE achieves comparable or superior performance with reduced computational
72"
CROSSMAE ACHIEVES COMPARABLE OR SUPERIOR PERFORMANCE WITH REDUCED COMPUTATIONAL,0.07796257796257797,"costs in image classification and instance segmentation compared to MAE on vision transformer
73"
CROSSMAE ACHIEVES COMPARABLE OR SUPERIOR PERFORMANCE WITH REDUCED COMPUTATIONAL,0.079002079002079,"models ranging from ViT-S to ViT-H. Code is available here.
74"
RELATED WORKS,0.08004158004158005,"2
Related Works
75"
SELF-SUPERVISED LEARNING,0.08108108108108109,"2.1
Self-Supervised Learning
76"
SELF-SUPERVISED LEARNING,0.08212058212058213,"In self-supervised representation learning, a model trains on a pretext task where the supervision
77"
SELF-SUPERVISED LEARNING,0.08316008316008316,"comes from the input data itself without labels. Contrastive learning methods learn representations
78"
SELF-SUPERVISED LEARNING,0.0841995841995842,"by contrasting positive and negative samples, such as SimCLR [11], CPC [44], MoCo [29, 12, 13],
79"
SELF-SUPERVISED LEARNING,0.08523908523908524,"CLD [59] and SwAV [7]. Additionally, in BYOL [26], iBOT [65], DINO [8], DINOv2 [45], and
80"
SELF-SUPERVISED LEARNING,0.08627858627858628,"MaskAlign [62] make a student model to imitate a teacher model without negative pairs.
81"
SELF-SUPERVISED LEARNING,0.08731808731808732,"Generative modeling, focusing on acquiring a generative model capable of capturing the underlying
82"
SELF-SUPERVISED LEARNING,0.08835758835758836,"data distribution, is an alternative method for self-supervised learning. VAE/GAN [35] merges the
83"
SELF-SUPERVISED LEARNING,0.0893970893970894,"strengths of variational autoencoders and generative adversarial networks to acquire disentangled
84"
SELF-SUPERVISED LEARNING,0.09043659043659044,"representations of data. PixelCNN, PixelVAE, and PixelTransformer [55, 27, 54] generate images
85"
SELF-SUPERVISED LEARNING,0.09147609147609148,"pixel by pixel, taking into account the context of previously generated pixels. Masked modeling, a
86"
SELF-SUPERVISED LEARNING,0.09251559251559252,"large subclass of generative modeling, is discussed in the following subsection. After the pre-training
87"
SELF-SUPERVISED LEARNING,0.09355509355509356,"stage, these generative models can be finetuned for many downstream applications.
88"
MASKED MODELING,0.0945945945945946,"2.2
Masked Modeling
89"
MASKED MODELING,0.09563409563409564,"Masked modeling learns representations by reconstructing a masked portion of the input. Pioneering
90"
MASKED MODELING,0.09667359667359668,"works in natural language processing (NLP) present various such pretraining objectives. BERT [19]
91"
MASKED MODELING,0.09771309771309772,"and its extensions [41, 34] use a bidirectional transformer and present few-shot learning capabil-
92"
MASKED MODELING,0.09875259875259876,"ities from masked language modeling. GPT [47, 48, 5], uses autoregressive, causal masking and
93"
MASKED MODELING,0.0997920997920998,"demonstrates multi-task, few-shot, and in-context learning capabilities.
94"
MASKED MODELING,0.10083160083160084,"Early works in computer vision, such as Stacked Denoising Autoencoders [57] and Context En-
95"
MASKED MODELING,0.10187110187110188,"coder [46], investigated masked image modeling as a form of denoising or representation learning.
96"
MASKED MODELING,0.10291060291060292,"Recently, with the widespread use of transformer [20] as a backbone vision architecture, where
97"
MASKED MODELING,0.10395010395010396,"images are patchified and tokenized as sequences, researchers are interested in how to transfer the
98"
MASKED MODELING,0.104989604989605,"success in language sequence modeling to scale vision transformers. BEiT [3], MAE [30], and Sim-
99"
MASKED MODELING,0.10602910602910603,"Figure 3: MAE [30] concatenates all mask tokens with the visible patch features from a ViT encoder and passes
them to a decoder with self-attention blocks to reconstruct the original image. Patches that correspond to visible
tokens are then dropped, and an L2 loss is applied to the rest of the reconstruction as the pretraining objective.
CrossMAE instead uses cross-attention blocks in the decoder to reconstruct only a subset of the masked tokens."
MASKED MODELING,0.10706860706860707,"MIM [61] are a few of the early works that explored BERT-style pretraining of vision transformers.
100"
MASKED MODELING,0.10810810810810811,"Compared to works in NLP, both MAE and SimMIM [30, 61] find that a much higher mask ratio
101"
MASKED MODELING,0.10914760914760915,"compared to works in NLP is necessary to learn good visual representation. Many recent works
102"
MASKED MODELING,0.1101871101871102,"further extend masked pretraining to hierarchical architectures [61, 40] and study data the role of data
103"
MASKED MODELING,0.11122661122661123,"augmentation [9, 21]. Many subsequent works present similar successes of masked pretraining for
104"
MASKED MODELING,0.11226611226611227,"video [52, 58, 22, 28], language-vision and multi-modal pretraining [1, 39, 23] and for learning both
105"
MASKED MODELING,0.11330561330561331,"good representations and reconstruction capabilities [60, 37].
106"
MASKED MODELING,0.11434511434511435,"However, BERT-style pretraining requires heavy use of self-attention, which makes computational
107"
MASKED MODELING,0.11538461538461539,"complexity scale as a polynomial of sequence length. PixelTransformer [54] and DiffMAE [60] both
108"
MASKED MODELING,0.11642411642411643,"use cross-attention for masked image generation and representation learning. Siamese MAE [28]
109"
MASKED MODELING,0.11746361746361747,"uses an asymmetric masking pattern and decodes frames of a video condition on an earlier frame. In
110"
MASKED MODELING,0.11850311850311851,"these settings, all masked patches are reconstructed. In this work, we investigate if learning good
111"
MASKED MODELING,0.11954261954261955,"features necessitates high reconstruction quality and if the entire image needs to be reconstructed to
112"
MASKED MODELING,0.12058212058212059,"facilitate representation learning. PCAE [36] progressively discards redundant mask tokens through
113"
MASKED MODELING,0.12162162162162163,"its network, leading to a few tokens for reconstruction. VideoMAEv2 [58] concatenates randomly
114"
MASKED MODELING,0.12266112266112267,"sampled masked tokens with visible tokens and uses self-attention to reconstruct the masked patches.
115"
MASKED MODELING,0.12370062370062371,"In comparison, we minimally modify MAE with a cross-attention-only decoder and masked tokens
116"
MASKED MODELING,0.12474012474012475,"are decoded in a conditional independent way.
117"
APPLICATIONS OF CROSS-ATTENTION,0.1257796257796258,"2.3
Applications of Cross-Attention
118"
APPLICATIONS OF CROSS-ATTENTION,0.12681912681912683,"In addition to the prevalent use of self-attention in computer vision, cross-attention has shown to be a
119"
APPLICATIONS OF CROSS-ATTENTION,0.12785862785862787,"cost-effective way to perform pooling from a large set of visible tokens. Intuitively, cross-attention
120"
APPLICATIONS OF CROSS-ATTENTION,0.1288981288981289,"can be seen as a parametric form of pooling, which learnably weighs different features. Touvron
121"
APPLICATIONS OF CROSS-ATTENTION,0.12993762993762994,"et al. [53] replace mean pooling with cross-attention pooling and find improvement in ImageNet
122"
APPLICATIONS OF CROSS-ATTENTION,0.13097713097713098,"classification performance. Jaegle et al. [32] uses cross-attention to efficiently process large volumes
123"
APPLICATIONS OF CROSS-ATTENTION,0.13201663201663202,"of multi-modal data. Cross-attention is also widely used for object detection. Carion et al. [6] utilizes
124"
APPLICATIONS OF CROSS-ATTENTION,0.13305613305613306,"query tokens as placeholders for potential objects in the scene. Cheng et al. [16, 15] further extend
125"
APPLICATIONS OF CROSS-ATTENTION,0.1340956340956341,"this concept by introducing additional query tokens to specifically tackle object segmentation in
126"
APPLICATIONS OF CROSS-ATTENTION,0.13513513513513514,"addition to the query tokens for object detection. Distinct from thes prior works, we are interested the
127"
APPLICATIONS OF CROSS-ATTENTION,0.13617463617463618,"role of cross-anttention for representation learning in a self-supervised manner.
128"
CROSSMAE,0.13721413721413722,"3
CrossMAE
129"
CROSSMAE,0.13825363825363826,"We start with an overview of vanilla masked autoencoders in Section 3.1. Next, in Section 3.2, we
130"
CROSSMAE,0.1392931392931393,"introduce the use of cross-attention in place of self-attention in the decoder for testing the necessity
131"
CROSSMAE,0.14033264033264034,"of interaction between mask tokens for representation learning. In Section 3.3, we discuss how
132"
CROSSMAE,0.14137214137214138,"eliminating self-attention in the decoding process enables us to reconstruct only a subset of masked
133"
CROSSMAE,0.14241164241164242,"tokens, leading to faster pretraining. Finally, Section 3.4 presents our inter-block attention mechanism,
134"
CROSSMAE,0.14345114345114346,"which allows decoder blocks to leverage varied encoder features.
135"
CROSSMAE,0.1444906444906445,"3.1
Preliminaries: Masked Autoencoders
136"
CROSSMAE,0.14553014553014554,"Masked Autoencoders (MAE) [30] pretrain Vision Transformers (ViTs) [20]. Each image input is
137"
CROSSMAE,0.14656964656964658,"first patchified, and then a random subset of the patches is selected as the visible patches. As depicted
138"
CROSSMAE,0.14760914760914762,"in Figure 3, the visible patches, concatenated with a learnable class token [CLS], are subsequently
139"
CROSSMAE,0.14864864864864866,"Figure 4: Overview of CrossMAE. (a) The vanilla version of CrossMAE uses the output of the last encoder
block as the keys and queries for cross-attention. The first decoder block takes the sum of mask tokens and their
corresponding positional embeddings as queries, and subsequent layers use the output of the previous decoder
block as queries to reconstruct the masked patches. (b) Unlike the decoder block in [56], the cross-attention
decoder block does not contain self-attention, decoupling the generation of different masked patches. (c)
CrossMAE’s decoder blocks can leverage low-level features for reconstruction via inter-block attention. It
weighs the intermediate feature maps, and the weighted sum of feature maps is used as the key and value for
each decoder block."
CROSSMAE,0.1496881496881497,"fed into the ViT encoder, which outputs a set of feature latents. The latent vectors, concatenated with
140"
CROSSMAE,0.15072765072765074,"the sum of the positional embeddings of the masked patches and the learnable mask token, are passed
141"
CROSSMAE,0.15176715176715178,"into the MAE decoder. The decoder blocks share the same architecture as the encoder blocks (i.e.,
142"
CROSSMAE,0.15280665280665282,"both are transformer blocks with self-attention layers). Note that the number of tokens fed into the
143"
CROSSMAE,0.15384615384615385,"decoder is the same length as the original input, and the decoding process assumes that the decoded
144"
CROSSMAE,0.1548856548856549,"tokens depend on both visible and masked tokens. Decoder outputs pass through a fully connected
145"
CROSSMAE,0.15592515592515593,"layer per patch for image reconstruction. After the reconstruction is generated, the loss is applied
146"
CROSSMAE,0.15696465696465697,"only to the masked positions, while the reconstructions for visible spatial locations are discarded.
147"
CROSSMAE,0.158004158004158,"Recall in Sec. 1 we measure the mean attention value across all attention maps over the ImageNet
148"
CROSSMAE,0.15904365904365905,"validation set to study the properties of MAE. We grouped the attention values by cross-attention
149"
CROSSMAE,0.1600831600831601,"and self-attention between visible and masked tokens. We observed that in the decoding process
150"
CROSSMAE,0.16112266112266113,"of an MAE, mask tokens attend disproportionately to the class token and the visible tokens (see
151"
CROSSMAE,0.16216216216216217,"Figure 1.(b)). This motivates us to make design decisions and conduct experiments specifically to
152"
CROSSMAE,0.1632016632016632,"answer the following question: Can we simplify the decoding process by eliminating self-attention
153"
CROSSMAE,0.16424116424116425,"among masked tokens without compromising the model’s ability to generate coherent images and
154"
CROSSMAE,0.1652806652806653,"perform well on downstream tasks?
155"
RECONSTRUCTION WITH CROSS-ATTENTION,0.16632016632016633,"3.2
Reconstruction with Cross-Attention
156"
RECONSTRUCTION WITH CROSS-ATTENTION,0.16735966735966737,"To address this question, we substitute the self-attention mechanism in the decoder blocks with
157"
RECONSTRUCTION WITH CROSS-ATTENTION,0.1683991683991684,"cross-attention, using it as a readout function to decode the latent embedding from the encoder to raw
158"
RECONSTRUCTION WITH CROSS-ATTENTION,0.16943866943866945,"pixel values. Specifically, the decoder employs multi-head cross-attention where the queries are the
159"
RECONSTRUCTION WITH CROSS-ATTENTION,0.1704781704781705,"output from previous decoder blocks (or the sum of position embedding of the masked patches and
160"
RECONSTRUCTION WITH CROSS-ATTENTION,0.17151767151767153,"mask token for the first decoder block). The keys and values are from the encoded features.
161"
RECONSTRUCTION WITH CROSS-ATTENTION,0.17255717255717257,"In the most basic CrossMAE, the output from the final encoder block is used as the key and value
162"
RECONSTRUCTION WITH CROSS-ATTENTION,0.1735966735966736,"tokens for all layers of the decoder, as illustrated in Fig. 4(a). Further exploration in Sec.3.4 reveals
163"
RECONSTRUCTION WITH CROSS-ATTENTION,0.17463617463617465,"that utilizing a weighted mean of selected encoder feature maps can be beneficial. The residual
164"
RECONSTRUCTION WITH CROSS-ATTENTION,0.17567567567567569,"connections in each decoder block enable iterative refinement of decoded tokens as they progress
165"
RECONSTRUCTION WITH CROSS-ATTENTION,0.17671517671517672,"through decoder blocks.
166"
RECONSTRUCTION WITH CROSS-ATTENTION,0.17775467775467776,"Diverging from the original transformer architecture [56], our decoder omits the causal self-attention
167"
RECONSTRUCTION WITH CROSS-ATTENTION,0.1787941787941788,"layer before the introduction of multi-head cross-attention. This elimination, coupled with the fact
168"
RECONSTRUCTION WITH CROSS-ATTENTION,0.17983367983367984,"that layer normalization and residual connections are only applied along the feature axis but not
169"
RECONSTRUCTION WITH CROSS-ATTENTION,0.18087318087318088,"the token axis, enables the independent decoding of tokens. This design choice is evaluated in the
170"
RECONSTRUCTION WITH CROSS-ATTENTION,0.18191268191268192,"ablation study section to determine its impact on performance.
171"
RECONSTRUCTION WITH CROSS-ATTENTION,0.18295218295218296,"Given the disparity in the dimensions of the encoder and decoder, MAE adapts the visible features to
172"
RECONSTRUCTION WITH CROSS-ATTENTION,0.183991683991684,"the decoder’s latent space using an MLP. However, in CrossMAE, as encoder features are integrated
173"
RECONSTRUCTION WITH CROSS-ATTENTION,0.18503118503118504,"at various decoder blocks, we embed the projection within the multi-head cross-attention module.
174"
RECONSTRUCTION WITH CROSS-ATTENTION,0.18607068607068608,"Cross-attention layers serve as a readout function that decodes the global representation provided
175"
RECONSTRUCTION WITH CROSS-ATTENTION,0.18711018711018712,"in the encoder’s output tokens to the pixel values within each patch to be reconstructed. However,
176"
RECONSTRUCTION WITH CROSS-ATTENTION,0.18814968814968816,"CrossMAE does not restrict the architecture to a single cross-attention block. Instead, we stack
177"
RECONSTRUCTION WITH CROSS-ATTENTION,0.1891891891891892,"multiple cross-attention decoder blocks in a manner more akin to the traditional transformer [56].
178"
PARTIAL RECONSTRUCTION,0.19022869022869024,"3.3
Partial Reconstruction
179"
PARTIAL RECONSTRUCTION,0.19126819126819128,"The fact that CrossMAE uses cross-attention rather than self-attention in the decoder blocks brings
180"
PARTIAL RECONSTRUCTION,0.19230769230769232,"an additional benefit over the original MAE architecture. Recall that mask tokens are decoded inde-
181"
PARTIAL RECONSTRUCTION,0.19334719334719336,"pendently and thus there is no exchange of information between them, to obtain the reconstructions
182"
PARTIAL RECONSTRUCTION,0.1943866943866944,"at a specific spatial location, CrossMAE only needs to pass the corresponding mask tokens to the
183"
PARTIAL RECONSTRUCTION,0.19542619542619544,"cross-attention decoder. This allows partial reconstruction in contrast to the original full-image
184"
PARTIAL RECONSTRUCTION,0.19646569646569648,"reconstruction in the MAE architecture which needs to pass all the masked tokens as the input of the
185"
PARTIAL RECONSTRUCTION,0.19750519750519752,"decoder blocks due to the existence of self-attention in the decoder blocks.
186"
PARTIAL RECONSTRUCTION,0.19854469854469856,"To address the second question in Sec. 3.1, rather than decoding the reconstruction for all masked
187"
PARTIAL RECONSTRUCTION,0.1995841995841996,"locations, we only compute the reconstruction on a random subset of the locations and apply the loss
188"
PARTIAL RECONSTRUCTION,0.20062370062370063,"to the decoded locations. Specifically, we name the ratio of predicted tokens to all image tokens as
189"
PARTIAL RECONSTRUCTION,0.20166320166320167,"prediction ratio (γ), and the mask ratio (p). Then the prediction ratio is bounded between γ ∈(0, p].
190"
PARTIAL RECONSTRUCTION,0.20270270270270271,"Because we are sampling within the masked tokens uniformly at random and the reconstruction
191"
PARTIAL RECONSTRUCTION,0.20374220374220375,"loss is a mean square error on the reconstructed patches, the expected loss is the same as in MAE,
192"
PARTIAL RECONSTRUCTION,0.2047817047817048,"while the variance is (p/γ) times larger than the variance in MAE. Empirically, we find that scaling
193"
PARTIAL RECONSTRUCTION,0.20582120582120583,"the learning rate of MAE (β) to match the variance (i.e. setting the learning rate as γβ/p)) helps
194"
PARTIAL RECONSTRUCTION,0.20686070686070687,"with model performance. Since cross-attention has linear complexity with respect to the number of
195"
PARTIAL RECONSTRUCTION,0.2079002079002079,"masked tokens, this partial reconstruction paradigm decreases computation complexity. Empirically,
196"
PARTIAL RECONSTRUCTION,0.20893970893970895,"we find that the quality of the learned representations is not compromised by this approach.
197"
INTER-BLOCK ATTENTION,0.20997920997921,"3.4
Inter-block Attention
198"
INTER-BLOCK ATTENTION,0.21101871101871103,"MAE combines the feature of the last encoder block with mask tokens as the input to the self-attention
199"
INTER-BLOCK ATTENTION,0.21205821205821207,"decoder, which creates an information bottleneck by making early encoder features inaccessible
200"
INTER-BLOCK ATTENTION,0.2130977130977131,"for the decoder. In contrast, CrossMAE’s cross-attention decoder decouples queries from keys and
201"
INTER-BLOCK ATTENTION,0.21413721413721415,"values. This decoupling allows different cross-attention decoder blocks to take in feature maps from
202"
INTER-BLOCK ATTENTION,0.2151767151767152,"different encoder blocks. This added degree of flexibility comes with a design choice for selecting
203"
INTER-BLOCK ATTENTION,0.21621621621621623,"encoder features for each decoder block. One naive choice is to give the feature of the ith encoder
204"
INTER-BLOCK ATTENTION,0.21725571725571727,"block to the last ith decoder (e.g., feeding the feature of the first encoder to the last decoder), in a
205"
INTER-BLOCK ATTENTION,0.2182952182952183,"U-Net-like fashion. However, this assumes the decoder’s depth matches the depth of the encoder,
206"
INTER-BLOCK ATTENTION,0.21933471933471935,"which is not the case for MAE or CrossMAE.
207"
INTER-BLOCK ATTENTION,0.2203742203742204,"Instead of manually matching each decoder block with an encoder feature map, we make the selection
208"
INTER-BLOCK ATTENTION,0.22141372141372143,"learnable and propose inter-block attention for feature fusion for each decoder block (Figure 4(c)).
209"
INTER-BLOCK ATTENTION,0.22245322245322247,"Analogous to the inter-patch cross-attention that takes a weighted sum of the visible token embeddings
210"
INTER-BLOCK ATTENTION,0.2234927234927235,"across the patch dimensions to update the embeddings of masked tokens, inter-block attention takes
211"
INTER-BLOCK ATTENTION,0.22453222453222454,"a weighted sum of the visible token embeddings across different input blocks at the same spatial
212"
INTER-BLOCK ATTENTION,0.22557172557172558,"location to fuse the input features from multiple blocks into one feature map for each decoder block.
213"
INTER-BLOCK ATTENTION,0.22661122661122662,"Concretely, each decoder block takes a weighted linear combination of encoder feature maps {fi} as
214"
INTER-BLOCK ATTENTION,0.22765072765072766,"keys and values. Specifically, for each key/value token tk in decoder block k in a model with encoder
215"
INTER-BLOCK ATTENTION,0.2286902286902287,"depth n, we initialize a weight wk ∈Rn ∼N(0, 1/n). Then tk is defined as
216 tk = n
X"
INTER-BLOCK ATTENTION,0.22972972972972974,"j=1
wk
j fj.
(1)"
INTER-BLOCK ATTENTION,0.23076923076923078,"217
In addition to feature maps from different encoder blocks, we also include the inputs to the first
218"
INTER-BLOCK ATTENTION,0.23180873180873182,"encoder block to allow the decoder to leverage more low-level information to reconstruct the original
219"
INTER-BLOCK ATTENTION,0.23284823284823286,"Method
ViT-S
ViT-B
ViT-L
ViT-H"
INTER-BLOCK ATTENTION,0.2338877338877339,"Supervised [50]
79.0
82.3
82.6
83.1
DINO [8]
-
82.8
-
-
MoCo v3 [14]
81.4
83.2
84.1
-
BEiT [3]
-
83.2
85.2
-
MultiMAE [2]
-
83.3
-
-
MixedAE [9]
-
83.5
-
-
CIM [21]
81.6
83.3
-
-
MAE [30]
78.9
83.3
85.4
85.8
CrossMAE (25%)
79.2
83.5
85.4
86.3
CrossMAE (75%)
79.3
83.7
85.4
-"
INTER-BLOCK ATTENTION,0.23492723492723494,"Table 1: ImageNet-1K classification accuracy.
CrossMAE performs on par or better than MAE.
All experiments are run with 800 epochs. The best
results are in bold while the second best results are
underlined."
INTER-BLOCK ATTENTION,0.23596673596673598,"APbox
APmask"
INTER-BLOCK ATTENTION,0.23700623700623702,"Method
ViT-B
ViT-L
ViT-B
ViT-L"
INTER-BLOCK ATTENTION,0.23804573804573806,"Supervised [38]
47.6
49.6
42.4
43.8
MoCo v3 [14]
47.9
49.3
42.7
44.0
BEiT [3]
49.8
53.3
44.4
47.1
MixedAE [9]
50.3
-
43.5
-
MAE [38]
51.2
54.6
45.5
48.6
CrossMAE
52.1
54.9
46.3
48.8"
INTER-BLOCK ATTENTION,0.2390852390852391,"Table 2: COCO instance segmentation. Compared to
previous masked visual pretraining works, CrossMAE per-
forms favorably on object detection and instance segmen-
tation tasks."
INTER-BLOCK ATTENTION,0.24012474012474014,"image. We can select a subset of the feature maps from the encoder layers instead of all feature maps.
220"
INTER-BLOCK ATTENTION,0.24116424116424118,"This reduces the computation complexity of the system. We ablate this in Table 3d.
221"
INTER-BLOCK ATTENTION,0.24220374220374222,"We show that using the weighted features rather than simply using the features from the last block
222"
INTER-BLOCK ATTENTION,0.24324324324324326,"greatly improves the performance of CrossMAE. Intriguingly, in the process of learning to achieve
223"
INTER-BLOCK ATTENTION,0.2442827442827443,"better reconstructions, early decoder blocks tend to prioritize information from later encoder blocks,
224"
INTER-BLOCK ATTENTION,0.24532224532224534,"while later decoder blocks focus on earlier encoder block information, as demonstrated in Section 4.5.
225"
EXPERIMENTS,0.24636174636174638,"4
Experiments
226"
EXPERIMENTS,0.24740124740124741,"We perform self-supervised pretraining on ImageNet-1K, following MAE [30]’s hyperparameter
227"
EXPERIMENTS,0.24844074844074845,"settings, only modifying the learning rate and decoder depth. The hyperparameters were initially
228"
EXPERIMENTS,0.2494802494802495,"determined on ViT-Base and then directly applied to ViT-Small, ViT-Large, and ViT-Huge. Both
229"
EXPERIMENTS,0.2505197505197505,"CrossMAE and MAE are trained for 800 epochs. We provide implementation details and more
230"
EXPERIMENTS,0.2515592515592516,"experiments in the appendix.
231"
IMAGENET CLASSIFICATION,0.2525987525987526,"4.1
ImageNet Classification
232"
IMAGENET CLASSIFICATION,0.25363825363825365,"Setup. The model performance is evaluated with end-to-end fine-tuning, with top-1 accuracy used
233"
IMAGENET CLASSIFICATION,0.25467775467775466,"for comparison. Same as in Figure. 2, we compare two versions of CrossMAE: one with a prediction
234"
IMAGENET CLASSIFICATION,0.25571725571725573,"ratio of 25% (1/3 of the mask tokens) and another with 75% (all mask tokens). Both models are
235"
IMAGENET CLASSIFICATION,0.25675675675675674,"trained with a mask ratio of 75% and a decoder depth of 12.
236"
IMAGENET CLASSIFICATION,0.2577962577962578,"Results. As shown in Table 1, CrossMAE outperforms vanilla MAE using the same ViT-B encoder
237"
IMAGENET CLASSIFICATION,0.2588357588357588,"in terms of fine-tuning accuracy. This shows that replacing the self-attention with cross-attention
238"
IMAGENET CLASSIFICATION,0.2598752598752599,"does not degrade the downstream classification performance of the pre-trained model. Moreover,
239"
IMAGENET CLASSIFICATION,0.2609147609147609,"CrossMAE outperforms other self-supervised and masked image modeling baselines, e.g., DINO [8],
240"
IMAGENET CLASSIFICATION,0.26195426195426197,"MoCo v3 [14], BEiT [3], and MultiMAE [2].
241"
OBJECT DETECTION AND INSTANCE SEGMENTATION,0.262993762993763,"4.2
Object Detection and Instance Segmentation
242"
OBJECT DETECTION AND INSTANCE SEGMENTATION,0.26403326403326405,"Setup. We additionally evaluate models pretrained with CrossMAE for object detection and instance
243"
OBJECT DETECTION AND INSTANCE SEGMENTATION,0.26507276507276506,"segmentation, which require deeper spatial understanding than ImageNet classification. Specifically,
244"
OBJECT DETECTION AND INSTANCE SEGMENTATION,0.2661122661122661,"we follow ViTDet [38], a method that leverages a Vision Transformer backbone for object detection
245"
OBJECT DETECTION AND INSTANCE SEGMENTATION,0.26715176715176714,"and instance segmentation. We report box AP for object detection and mask AP for instance
246"
OBJECT DETECTION AND INSTANCE SEGMENTATION,0.2681912681912682,"segmentation, following MAE [30]. We compare against supervised pre-training, MoCo-v3 [14],
247"
OBJECT DETECTION AND INSTANCE SEGMENTATION,0.2692307692307692,"BEiT [4], and MAE [30].
248"
OBJECT DETECTION AND INSTANCE SEGMENTATION,0.2702702702702703,"Results. As listed in Table 2, CrossMAE, with the default 75% prediction ratio, performs better
249"
OBJECT DETECTION AND INSTANCE SEGMENTATION,0.2713097713097713,"compared to these baselines, including vanilla MAE. This suggests that similar to MAE, CrossMAE
250"
OBJECT DETECTION AND INSTANCE SEGMENTATION,0.27234927234927236,"performance on ImageNet positively correlates with instance segmentation. Additionally, Cross-
251"
OBJECT DETECTION AND INSTANCE SEGMENTATION,0.2733887733887734,"MAE’s downstream performance scales similarly to MAE as the model capacity increases from ViT-B
252"
OBJECT DETECTION AND INSTANCE SEGMENTATION,0.27442827442827444,"to ViT-L. This observation also supports our hypothesis that partial reconstruction is suprisingly
253"
OBJECT DETECTION AND INSTANCE SEGMENTATION,0.27546777546777546,"sufficient for learning dense visual representation.
254"
OBJECT DETECTION AND INSTANCE SEGMENTATION,0.2765072765072765,"Method
Acc. (%)
MAE
83.0
CrossMAE
83.3
CrossMAE + Self-Attn
83.3"
OBJECT DETECTION AND INSTANCE SEGMENTATION,0.27754677754677753,"(a)
Attention type in decoder
blocks. Adding back self-attention
between mask tokens does not im-
prove performance."
OBJECT DETECTION AND INSTANCE SEGMENTATION,0.2785862785862786,"Mask Ratio
Acc. (%)
65%
83.5
75%
83.3
85%
83.3"
OBJECT DETECTION AND INSTANCE SEGMENTATION,0.2796257796257796,"(b) Mask ratio. CrossMAE has
consistent performance across high
mask ratios."
OBJECT DETECTION AND INSTANCE SEGMENTATION,0.2806652806652807,"Pred. Ratio
Acc. (%)
15%
83.1
25%
83.2
75%
83.3"
OBJECT DETECTION AND INSTANCE SEGMENTATION,0.2817047817047817,"(c) Prediction ratio. CrossMAE
performs well even when only a
fraction of mask tokens are recon-
structed."
OBJECT DETECTION AND INSTANCE SEGMENTATION,0.28274428274428276,"# Feature
Maps Fused
Acc.
(%)
1
82.9
3
83.3
6
83.5
12
83.3"
OBJECT DETECTION AND INSTANCE SEGMENTATION,0.28378378378378377,"(d) Inter-block attention. A com-
bination of six select encoder fea-
ture maps is best."
OBJECT DETECTION AND INSTANCE SEGMENTATION,0.28482328482328484,"Decoder
Depth
Acc.
(%)
1
83.0
4
83.1
8
83.1
12
83.3"
OBJECT DETECTION AND INSTANCE SEGMENTATION,0.28586278586278585,"(e) Decoder depth. CrossMAE
performance scales with decoder
depth."
OBJECT DETECTION AND INSTANCE SEGMENTATION,0.2869022869022869,"Image
Resolution
Acc.
(%)
224
83.2
448
84.6"
OBJECT DETECTION AND INSTANCE SEGMENTATION,0.28794178794178793,"(f) Input resolution. CrossMAE
scales to longer input sequences."
OBJECT DETECTION AND INSTANCE SEGMENTATION,0.288981288981289,"Table 3: Ablations on CrossMAE. We report fine-tuning performance on ImageNet-1K classification with 400
epochs (i.e., half of the full experiments) with ViT-B/16. MAE performance is reproduced using the official
MAE code. Underline indicates the default setting for CrossMAE. Bold indicates the best hyperparameter
among the tested ones. 1 feature map fused (row 1, Table 3(d)) indicates using only the feature from the last
encoder block. We use 25% prediction ratio for both settings in Table 3(f) to accelerate training."
ABLATIONS,0.29002079002079,"4.3
Ablations
255"
ABLATIONS,0.2910602910602911,"Cross-Attention vs Self-Attention. As shown in Table 3a, CrossMAE, with its cross-attention-
256"
ABLATIONS,0.2920997920997921,"only decoder, outperforms vanilla MAE in downstream tasks as noted in Section 4.1. Additionally,
257"
ABLATIONS,0.29313929313929316,"combining cross-attention with self-attention does not enhance fine-tuning performance, indicating
258"
ABLATIONS,0.29417879417879417,"that cross-attention alone is adequate for effective representation learning.
259"
ABLATIONS,0.29521829521829523,"Mask Ratio and Prediction Ratio. In our experiments with different mask and prediction ratios (i.e.,
260"
ABLATIONS,0.29625779625779625,"the ratio of mask tokens to all tokens and the ratio of reconstructed tokens to all tokens, respectively)
261"
ABLATIONS,0.2972972972972973,"(see Table 3b and Table 3c), we found that our method’s performance is not significantly affected by
262"
ABLATIONS,0.2983367983367983,"variations in the number of masked tokens. Notably, CrossMAE effectively learns representations
263"
ABLATIONS,0.2993762993762994,"by reconstructing as few as 15% of tokens, compared to the 100% required by vanilla MAE, with
264"
ABLATIONS,0.3004158004158004,"minimal impact on downstream fine-tuning performance, which shows that partial reconstruction is
265"
ABLATIONS,0.30145530145530147,"sufficient for effective representation learning.
266"
ABLATIONS,0.3024948024948025,"Inter-block Attention. Our ablation study, detailed in Table 3d, explored the impact of varying the
267"
ABLATIONS,0.30353430353430355,"number of encoder feature maps in our inter-block attention mechanism. We found that using only
268"
ABLATIONS,0.30457380457380456,"the last feature map slightly lowers performance compared to using all 12. However, even a partial
269"
ABLATIONS,0.30561330561330563,"selection of feature maps improves CrossMAE’s performance, with the best results obtained using 6
270"
ABLATIONS,0.30665280665280664,"feature maps. This indicates that CrossMAE does not require all features for optimal performance.
271"
ABLATIONS,0.3076923076923077,"Decoder Depth. Table 3e shows that a 12-block decoder slightly improves performance compared
272"
ABLATIONS,0.3087318087318087,"to shallower ones. Remarkably, CrossMAE achieves similar results to MAE with just one decoder
273"
ABLATIONS,0.3097713097713098,"block, demonstrating its efficiency. Our experiments in Figure 7 that models with lower prediction
274"
ABLATIONS,0.3108108108108108,"ratios benefit more from deeper decoders.
275"
ABLATIONS,0.31185031185031187,"Input Resolution. We extend CrossMAE to longer token lengths by increasing the image resolution
276"
ABLATIONS,0.3128898128898129,"with constant patch size. Escalating the resolution from 224 to 448 increases the token length from
277"
ABLATIONS,0.31392931392931395,"197 to 785, challenging the scalability of current approaches. Thus, we opt for a CrossMAE variant
278"
ABLATIONS,0.31496881496881496,"with a 25% prediction ratio. In Table 3f, we observe that the classification accuracy positively
279"
ABLATIONS,0.316008316008316,"correlates with the input resolution, indicating that CrossMAE can scale to long input sequences.
280"
TRAINING THROUGHPUT AND MEMORY UTILIZATION,0.31704781704781704,"4.4
Training Throughput and Memory Utilization
281"
TRAINING THROUGHPUT AND MEMORY UTILIZATION,0.3180873180873181,"Due to partial reconstruction and confining attention to between mask tokens and visible tokens,
282"
TRAINING THROUGHPUT AND MEMORY UTILIZATION,0.3191268191268191,"CrossMAE improves pre-training efficiency over MAE. Results in Table 10 show that the FLOPs
283"
TRAINING THROUGHPUT AND MEMORY UTILIZATION,0.3201663201663202,"Figure 5: We visualize the output of each decoder block. (a-b) Different decoder blocks play different roles
in the reconstruction, with most details emerging at later decoder blocks, which confirms the motivation for
inter-block attention. (c) Visualizations of inter-block attention shows that different decoder blocks indeed
attend to feature from different encoder blocks, with later blocks focusing on earlier encoder features to
achieve reconstruction. The reconstructions are unnormalized w.r.t ground truth mean and std for each patch."
TRAINING THROUGHPUT AND MEMORY UTILIZATION,0.3212058212058212,"reduction does translate to an 1.54× training throughput and at least 50% reduction in GPU memory
284"
TRAINING THROUGHPUT AND MEMORY UTILIZATION,0.32224532224532226,"utilization compared to MAE.
285"
VISUALIZATIONS,0.3232848232848233,"4.5
Visualizations
286"
VISUALIZATIONS,0.32432432432432434,"Visualizing Per-block Reconstruction. Rather than only visualizing the final reconstruction, we
287"
VISUALIZATIONS,0.32536382536382535,"have two key observations that allow us to visualize the work performed by each decoder block:
288"
VISUALIZATIONS,0.3264033264033264,"1) Transformer blocks have skip connections from their inputs to outputs. 2) The final decoder
289"
VISUALIZATIONS,0.32744282744282743,"block’s output goes through a linear reconstruction head to produce the reconstruction. As detailed in
290"
VISUALIZATIONS,0.3284823284823285,"Appendix D, we can factor out each block’s contribution in the final reconstruction with linearity.
291"
VISUALIZATIONS,0.3295218295218295,"This decomposition allows expressing the reconstruction as an image stack, where summing up all the
292"
VISUALIZATIONS,0.3305613305613306,"levels gives us the final reconstruction. As shown in Figure 5 (a,b), we observe that different decoder
293"
VISUALIZATIONS,0.3316008316008316,"blocks play different roles in reconstruction, with most details emerging at later decoder blocks. This
294"
VISUALIZATIONS,0.33264033264033266,"justifies the need for low-level features from early encoder blocks, motivating inter-block attention.
295"
VISUALIZATIONS,0.33367983367983367,"Visualizing Inter-block Attention Maps. As shown in the visualizations of the attention maps of
296"
VISUALIZATIONS,0.33471933471933474,"inter-block attention in 5(c), CrossMAE naturally leverages the inter-block attention to allow the later
297"
VISUALIZATIONS,0.33575883575883575,"decoder blocks to focus on earlier encoder features to achieve reconstruction and allow the earlier
298"
VISUALIZATIONS,0.3367983367983368,"decoder blocks to focus on later encoder features. This underscores the necessity for different decoder
299"
VISUALIZATIONS,0.33783783783783783,"blocks to attend to different encoder features, correlating with the performance improvements when
300"
VISUALIZATIONS,0.3388773388773389,"inter-block attention is used.
301"
DISCUSSION AND CONCLUSION,0.3399168399168399,"5
Discussion and Conclusion
302"
DISCUSSION AND CONCLUSION,0.340956340956341,"In our study, we present a novel understanding of MAE, demonstrating that coherent image recon-
303"
DISCUSSION AND CONCLUSION,0.341995841995842,"struction is achieved not through interactions between patches in the decoder but by learning a global
304"
DISCUSSION AND CONCLUSION,0.34303534303534305,"representation within the encoder. Based on this insight, we propose replacing self-attention layers
305"
DISCUSSION AND CONCLUSION,0.34407484407484407,"in the decoder with a simple readout function, specifically utilizing cross-attention to aggregate
306"
DISCUSSION AND CONCLUSION,0.34511434511434513,"encoder outputs into each input token within the decoder layers independently. This approach, tested
307"
DISCUSSION AND CONCLUSION,0.34615384615384615,"across models ranging from ViT-S to ViT-H, achieves comparable or better performance in image
308"
DISCUSSION AND CONCLUSION,0.3471933471933472,"classification and instance segmentation with reduced computational requirements, showcasing the
309"
DISCUSSION AND CONCLUSION,0.3482328482328482,"potential for more efficient and scalable visual pretraining methods. Our findings underscore the
310"
DISCUSSION AND CONCLUSION,0.3492723492723493,"efficacy of the encoder’s global representation learning, paving the way for streamlined decoder
311"
DISCUSSION AND CONCLUSION,0.3503118503118503,"architectures in future MAE implementations. CrossMAE’s efficiency and scalability demonstrate
312"
DISCUSSION AND CONCLUSION,0.35135135135135137,"potential for large-scale visual pretraining, particularly on underutilized in-the-wild video datasets.
313"
DISCUSSION AND CONCLUSION,0.3523908523908524,"However, our work has not yet explored scaling to models larger than ViT-H, the largest model
314"
DISCUSSION AND CONCLUSION,0.35343035343035345,"examined in MAE, leaving this for future research.
315"
REFERENCES,0.35446985446985446,"References
316"
REFERENCES,0.35550935550935553,"[1] Roman Bachmann, David Mizrahi, Andrei Atanov, and Amir Zamir. Multimae: Multi-modal multi-task
317"
REFERENCES,0.35654885654885654,"masked autoencoders. arXiv:2204.01678, 2022.
318"
REFERENCES,0.3575883575883576,"[2] Roman Bachmann, David Mizrahi, Andrei Atanov, and Amir Zamir. Multimae: Multi-modal multi-task
319"
REFERENCES,0.3586278586278586,"masked autoencoders. In European Conference on Computer Vision, pages 348–367. Springer, 2022.
320"
REFERENCES,0.3596673596673597,"[3] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv
321"
REFERENCES,0.3607068607068607,"preprint arXiv:2106.08254, 2021.
322"
REFERENCES,0.36174636174636177,"[4] Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. In ICLR, 2022.
323"
REFERENCES,0.3627858627858628,"[5] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
324"
REFERENCES,0.36382536382536385,"Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
325"
REFERENCES,0.36486486486486486,"2020.
326"
REFERENCES,0.3659043659043659,"[6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey
327"
REFERENCES,0.36694386694386694,"Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision,
328"
REFERENCES,0.367983367983368,"pages 213–229. Springer, 2020.
329"
REFERENCES,0.369022869022869,"[7] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsu-
330"
REFERENCES,0.3700623700623701,"pervised learning of visual features by contrasting cluster assignments. Advances in neural information
331"
REFERENCES,0.3711018711018711,"processing systems, 33:9912–9924, 2020.
332"
REFERENCES,0.37214137214137216,"[8] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand
333"
REFERENCES,0.3731808731808732,"Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF
334"
REFERENCES,0.37422037422037424,"international conference on computer vision, pages 9650–9660, 2021.
335"
REFERENCES,0.37525987525987525,"[9] Kai Chen, Zhili Liu, Lanqing Hong, Hang Xu, Zhenguo Li, and Dit-Yan Yeung. Mixed autoencoder for
336"
REFERENCES,0.3762993762993763,"self-supervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer
337"
REFERENCES,0.37733887733887733,"Vision and Pattern Recognition (CVPR), pages 22742–22751, 2023.
338"
REFERENCES,0.3783783783783784,"[10] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.
339"
REFERENCES,0.3794178794178794,"Generative pretraining from pixels. 2020.
340"
REFERENCES,0.3804573804573805,"[11] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
341"
REFERENCES,0.3814968814968815,"contrastive learning of visual representations. In International conference on machine learning, pages
342"
REFERENCES,0.38253638253638256,"1597–1607. PMLR, 2020.
343"
REFERENCES,0.38357588357588357,"[12] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive
344"
REFERENCES,0.38461538461538464,"learning. arXiv preprint arXiv:2003.04297, 2020.
345"
REFERENCES,0.38565488565488565,"[13] Xinlei Chen, Saining Xie, and Kaiming He.
An empirical study of training self-supervised vision
346"
REFERENCES,0.3866943866943867,"transformers, 2021.
347"
REFERENCES,0.3877338877338877,"[14] Xinlei Chen, Saining Xie, and Kaiming He.
An empirical study of training self-supervised vision
348"
REFERENCES,0.3887733887733888,"transformers. arXiv preprint arXiv:2104.02057, 2021.
349"
REFERENCES,0.3898128898128898,"[15] Bowen Cheng, Alexander G. Schwing, and Alexander Kirillov. Per-pixel classification is not all you need
350"
REFERENCES,0.3908523908523909,"for semantic segmentation. 2021.
351"
REFERENCES,0.3918918918918919,"[16] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-
352"
REFERENCES,0.39293139293139295,"attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF conference
353"
REFERENCES,0.39397089397089397,"on computer vision and pattern recognition, pages 1290–1299, 2022.
354"
REFERENCES,0.39501039501039503,"[17] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data
355"
REFERENCES,0.39604989604989604,"augmentation with a reduced search space. arxiv e-prints, page. arXiv preprint arXiv:1909.13719, 4, 2019.
356"
REFERENCES,0.3970893970893971,"[18] Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023.
357"
REFERENCES,0.3981288981288981,"[19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-
358"
REFERENCES,0.3991683991683992,"tional transformers for language understanding. 2019.
359"
REFERENCES,0.4002079002079002,"[20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
360"
REFERENCES,0.40124740124740127,"Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth
361"
REFERENCES,0.4022869022869023,"16x16 words: Transformers for image recognition at scale. In ICLR, 2020.
362"
REFERENCES,0.40332640332640335,"[21] Yuxin Fang, Li Dong, Hangbo Bao, Xinggang Wang, and Furu Wei. Corrupted image modeling for
363"
REFERENCES,0.40436590436590436,"self-supervised visual pre-training. In The Eleventh International Conference on Learning Representations,
364"
REFERENCES,0.40540540540540543,"2023.
365"
REFERENCES,0.40644490644490644,"[22] Christoph Feichtenhofer, Haoqi Fan, Yanghao Li, and Kaiming He. Masked autoencoders as spatiotemporal
366"
REFERENCES,0.4074844074844075,"learners. In Advances in Neural Information Processing Systems, 2022.
367"
REFERENCES,0.4085239085239085,"[23] Xinyang Geng, Hao Liu, Lisa Lee, Dale Schuurams, Sergey Levine, and Pieter Abbeel. Multimodal
368"
REFERENCES,0.4095634095634096,"masked autoencoders learn transferable representations. arXiv preprint arXiv:2205.14204, 2022.
369"
REFERENCES,0.4106029106029106,"[24] Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew
370"
REFERENCES,0.41164241164241167,"Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour.
371"
REFERENCES,0.4126819126819127,"arXiv:1706.02677, 2017.
372"
REFERENCES,0.41372141372141374,"[25] Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew
373"
REFERENCES,0.41476091476091476,"Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv
374"
REFERENCES,0.4158004158004158,"preprint arXiv:1706.02677, 2017.
375"
REFERENCES,0.41683991683991684,"[26] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya,
376"
REFERENCES,0.4178794178794179,"Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your
377"
REFERENCES,0.4189189189189189,"own latent-a new approach to self-supervised learning. Advances in neural information processing systems,
378"
REFERENCES,0.41995841995842,"33:21271–21284, 2020.
379"
REFERENCES,0.420997920997921,"[27] Ishaan Gulrajani, Kundan Kumar, Faruk Ahmed, Adrien Ali Taiga, Francesco Visin, David Vazquez, and
380"
REFERENCES,0.42203742203742206,"Aaron Courville. Pixelvae: A latent variable model for natural images. arXiv preprint arXiv:1611.05013,
381"
REFERENCES,0.4230769230769231,"2016.
382"
REFERENCES,0.42411642411642414,"[28] Agrim Gupta, Jiajun Wu, Jia Deng, and Li Fei-Fei. Siamese masked autoencoders. arXiv preprint
383"
REFERENCES,0.42515592515592515,"arXiv:2305.14344, 2023.
384"
REFERENCES,0.4261954261954262,"[29] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised
385"
REFERENCES,0.42723492723492723,"visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern
386"
REFERENCES,0.4282744282744283,"recognition, pages 9729–9738, 2020.
387"
REFERENCES,0.4293139293139293,"[30] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders
388"
REFERENCES,0.4303534303534304,"are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
389"
REFERENCES,0.4313929313929314,"Recognition (CVPR), pages 16000–16009, 2022.
390"
REFERENCES,0.43243243243243246,"[31] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic
391"
REFERENCES,0.43347193347193347,"depth. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October
392"
REFERENCES,0.43451143451143454,"11–14, 2016, Proceedings, Part IV 14, pages 646–661. Springer, 2016.
393"
REFERENCES,0.43555093555093555,"[32] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding,
394"
REFERENCES,0.4365904365904366,"Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, et al. Perceiver io: A general architecture
395"
REFERENCES,0.4376299376299376,"for structured inputs & outputs. arXiv preprint arXiv:2107.14795, 2021.
396"
REFERENCES,0.4386694386694387,"[33] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao,
397"
REFERENCES,0.4397089397089397,"Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything.
398"
REFERENCES,0.4407484407484408,"In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 4015–4026,
399"
REFERENCES,0.4417879417879418,"2023.
400"
REFERENCES,0.44282744282744285,"[34] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.
401"
REFERENCES,0.44386694386694386,"Albert: A lite bert for self-supervised learning of language representations. In International Conference on
402"
REFERENCES,0.44490644490644493,"Learning Representations, 2020.
403"
REFERENCES,0.44594594594594594,"[35] Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, Hugo Larochelle, and Ole Winther. Autoencoding
404"
REFERENCES,0.446985446985447,"beyond pixels using a learned similarity metric. In International conference on machine learning, pages
405"
REFERENCES,0.448024948024948,"1558–1566. PMLR, 2016.
406"
REFERENCES,0.4490644490644491,"[36] Jin Li, Yaoming Wang, XIAOPENG ZHANG, Yabo Chen, Dongsheng Jiang, Wenrui Dai, Chenglin Li,
407"
REFERENCES,0.4501039501039501,"Hongkai Xiong, and Qi Tian. Progressively compressed auto-encoder for self-supervised representation
408"
REFERENCES,0.45114345114345117,"learning. In The Eleventh International Conference on Learning Representations, 2023.
409"
REFERENCES,0.4521829521829522,"[37] Tianhong Li, Huiwen Chang, Shlok Kumar Mishra, Han Zhang, Dina Katabi, and Dilip Krishnan.
410"
REFERENCES,0.45322245322245325,"Mage: Masked generative encoder to unify representation learning and image synthesis. arXiv preprint
411"
REFERENCES,0.45426195426195426,"arXiv:2211.09117, 2022.
412"
REFERENCES,0.4553014553014553,"[38] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer backbones
413"
REFERENCES,0.45634095634095634,"for object detection. In European Conference on Computer Vision, pages 280–296. Springer, 2022.
414"
REFERENCES,0.4573804573804574,"[39] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He. Scaling language-image
415"
REFERENCES,0.4584199584199584,"pre-training via masking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
416"
REFERENCES,0.4594594594594595,"Recognition, pages 23390–23400, 2023.
417"
REFERENCES,0.4604989604989605,"[40] Jihao Liu, Xin Huang, Jinliang Zheng, Yu Liu, and Hongsheng Li. Mixmae: Mixed and masked autoencoder
418"
REFERENCES,0.46153846153846156,"for efficient pretraining of hierarchical vision transformers. arXiv:2205.13137, 2022.
419"
REFERENCES,0.4625779625779626,"[41] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
420"
REFERENCES,0.46361746361746364,"Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv
421"
REFERENCES,0.46465696465696466,"preprint arXiv:1907.11692, 2019.
422"
REFERENCES,0.4656964656964657,"[42] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. 2017.
423"
REFERENCES,0.46673596673596673,"[43] Ilya Loshchilov and Frank Hutter.
Decoupled weight decay regularization.
arXiv preprint
424"
REFERENCES,0.4677754677754678,"arXiv:1711.05101, 2017.
425"
REFERENCES,0.4688149688149688,"[44] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive
426"
REFERENCES,0.4698544698544699,"coding. arXiv preprint arXiv:1807.03748, 2018.
427"
REFERENCES,0.4708939708939709,"[45] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre
428"
REFERENCES,0.47193347193347196,"Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual
429"
REFERENCES,0.47297297297297297,"features without supervision. arXiv preprint arXiv:2304.07193, 2023.
430"
REFERENCES,0.47401247401247404,"[46] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context encoders:
431"
REFERENCES,0.47505197505197505,"Feature learning by inpainting. In Proceedings of the IEEE conference on computer vision and pattern
432"
REFERENCES,0.4760914760914761,"recognition, pages 2536–2544, 2016.
433"
REFERENCES,0.47713097713097713,"[47] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding
434"
REFERENCES,0.4781704781704782,"by generative pre-training. 2018.
435"
REFERENCES,0.4792099792099792,"[48] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language
436"
REFERENCES,0.4802494802494803,"models are unsupervised multitask learners. 2019.
437"
REFERENCES,0.4812889812889813,"[49] Ilija Radosavovic, Tete Xiao, Stephen James, Pieter Abbeel, Jitendra Malik, and Trevor Darrell. Real-world
438"
REFERENCES,0.48232848232848236,"robot learning with masked visual pre-training. In Conference on Robot Learning, pages 416–426. PMLR,
439"
REFERENCES,0.48336798336798337,"2023.
440"
REFERENCES,0.48440748440748443,"[50] Andreas Peter Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas
441"
REFERENCES,0.48544698544698545,"Beyer. How to train your vit? data, augmentation, and regularization in vision transformers. Transactions
442"
REFERENCES,0.4864864864864865,"on Machine Learning Research, 2022.
443"
REFERENCES,0.4875259875259875,"[51] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the
444"
REFERENCES,0.4885654885654886,"inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and
445"
REFERENCES,0.4896049896049896,"pattern recognition, pages 2818–2826, 2016.
446"
REFERENCES,0.49064449064449067,"[52] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. VideoMAE: Masked autoencoders are data-efficient
447"
REFERENCES,0.4916839916839917,"learners for self-supervised video pre-training. In Advances in Neural Information Processing Systems,
448"
REFERENCES,0.49272349272349275,"2022.
449"
REFERENCES,0.49376299376299376,"[53] Hugo Touvron, Matthieu Cord, Alaaeldin El-Nouby, Piotr Bojanowski, Armand Joulin, Gabriel Synnaeve,
450"
REFERENCES,0.49480249480249483,"and Hervé Jégou. Augmenting convolutional networks with attention-based aggregation, 2021.
451"
REFERENCES,0.49584199584199584,"[54] Shubham Tulsiani and Abhinav Gupta. Pixeltransformer: Sample conditioned signal generation. In
452"
REFERENCES,0.4968814968814969,"Proceedings of the 38th International Conference on Machine Learning, pages 10455–10464. PMLR,
453"
REFERENCES,0.4979209979209979,"2021.
454"
REFERENCES,0.498960498960499,"[55] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional
455"
REFERENCES,0.5,"image generation with pixelcnn decoders. Advances in neural information processing systems, 29, 2016.
456"
REFERENCES,0.501039501039501,"[56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
457"
REFERENCES,0.502079002079002,"Kaiser, and Illia Polosukhin. Attention is all you need. 2017.
458"
REFERENCES,0.5031185031185031,"[57] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine Manzagol, and Léon
459"
REFERENCES,0.5041580041580042,"Bottou. Stacked denoising autoencoders: Learning useful representations in a deep network with a local
460"
REFERENCES,0.5051975051975052,"denoising criterion. Journal of machine learning research, 11(12), 2010.
461"
REFERENCES,0.5062370062370062,"[58] Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, and Yu Qiao.
462"
REFERENCES,0.5072765072765073,"Videomae v2: Scaling video masked autoencoders with dual masking. In Proceedings of the IEEE/CVF
463"
REFERENCES,0.5083160083160083,"Conference on Computer Vision and Pattern Recognition, pages 14549–14560, 2023.
464"
REFERENCES,0.5093555093555093,"[59] Xudong Wang, Ziwei Liu, and Stella X Yu. Unsupervised feature learning by cross-level instance-group
465"
REFERENCES,0.5103950103950103,"discrimination. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,
466"
REFERENCES,0.5114345114345115,"pages 12586–12595, 2021.
467"
REFERENCES,0.5124740124740125,"[60] Chen Wei, Karttikeya Mangalam, Po-Yao Huang, Yanghao Li, Haoqi Fan, Hu Xu, Huiyu Wang, Cihang
468"
REFERENCES,0.5135135135135135,"Xie, Alan Yuille, and Christoph Feichtenhofer. Diffusion models as masked autoencoder. In ICCV, 2023.
469"
REFERENCES,0.5145530145530145,"[61] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu.
470"
REFERENCES,0.5155925155925156,"Simmim: A simple framework for masked image modeling. In Proceedings of the IEEE/CVF Conference
471"
REFERENCES,0.5166320166320166,"on Computer Vision and Pattern Recognition (CVPR), pages 9653–9663, 2022.
472"
REFERENCES,0.5176715176715176,"[62] Hongwei Xue, Peng Gao, Hongyang Li, Yu Qiao, Hao Sun, Houqiang Li, and Jiebo Luo. Stare at what
473"
REFERENCES,0.5187110187110187,"you see: Masked image modeling without reconstruction. In Proceedings of the IEEE/CVF Conference on
474"
REFERENCES,0.5197505197505198,"Computer Vision and Pattern Recognition, pages 22732–22741, 2023.
475"
REFERENCES,0.5207900207900208,"[63] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.
476"
REFERENCES,0.5218295218295218,"Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the
477"
REFERENCES,0.5228690228690228,"IEEE/CVF international conference on computer vision, pages 6023–6032, 2019.
478"
REFERENCES,0.5239085239085239,"[64] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk
479"
REFERENCES,0.524948024948025,"minimization. In International Conference on Learning Representations, 2018.
480"
REFERENCES,0.525987525987526,"[65] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image
481"
REFERENCES,0.527027027027027,"bert pre-training with online tokenizer. arXiv preprint arXiv:2111.07832, 2021.
482"
REFERENCES,0.5280665280665281,"A
Implementation details
483"
REFERENCES,0.5291060291060291,"A.1
Attention Calculation
484"
REFERENCES,0.5301455301455301,"To compare the attention values for mask tokens in vanilla MAE (Figure 1), we trained a ViT-B/16
485"
REFERENCES,0.5311850311850311,"MAE for 800 epochs using the default hyperparameters provided in [30]. For each image, we
486"
REFERENCES,0.5322245322245323,"randomly generate a 75% binary mask (m) for all tokens, with mi = 1 representing a token being
487"
REFERENCES,0.5332640332640333,"masked and mi = 0 otherwise. During the forward pass of the decoder, for each self-attention
488"
REFERENCES,0.5343035343035343,"operation, the attention map is stored. This means that for the default MAE, a total of 8 attention
489"
REFERENCES,0.5353430353430353,"maps, each with 16 attention heads are stored. Based on the mask pattern, we calculate the outer
490"
REFERENCES,0.5363825363825364,"product (m · m⊤) for the self-attention among mask tokens, and m · (1 −m⊤) for the cross-attention
491"
REFERENCES,0.5374220374220374,"from the mask token to the visible tokens. We then calculate the average across all feature maps
492"
REFERENCES,0.5384615384615384,"and attention heads for self-attention and cross-attention to get the image average values. Lastly, we
493"
REFERENCES,0.5395010395010394,"averaged across the entire ImageNet validation set to obtain the final values.
494"
REFERENCES,0.5405405405405406,"A.2
Inter-Block Attention
495"
REFERENCES,0.5415800415800416,"We tried a few implementations for inter-block attention (IBA) and found the following implementa-
496"
REFERENCES,0.5426195426195426,"tion to be the fastest and most memory-efficient. In this implementation, we combine inter-block
497"
REFERENCES,0.5436590436590436,"attention for all encoder layers as a single forward pass of a linear layer. For each decoder block,
498"
REFERENCES,0.5446985446985447,"we index into the output tensor to extract the corresponding feature map, and a layer norm will be
499"
REFERENCES,0.5457380457380457,"applied before the feature map is fed into the decoder block. Other alternatives we tried include 1)
500"
REFERENCES,0.5467775467775468,"performing separate inter-block attentions before each decoder block, and 2) 1x1 convolution on the
501"
REFERENCES,0.5478170478170478,"stacked encoder feature maps.
502"
REFERENCES,0.5488565488565489,"In MAE, there exists a layer norm after the last encoder feature map before feeding into the decoder.
503"
REFERENCES,0.5498960498960499,"In our implementation, we only add layer norm after inter-block attention. We find that adding
504"
REFERENCES,0.5509355509355509,"an additional layer norm before inter-block attention to each encoder feature map does not lead to
505"
REFERENCES,0.5519750519750519,"improvements in model performance but will significantly increase GPU memory usage.
506"
REFERENCES,0.553014553014553,"The pseudo-code of inter-block attention is the following:
507"
CLASS,0.5540540540540541,"1 class
InterBlockAttention ():
508"
DEF,0.5550935550935551,"2
def
__init__(self , num_feat_maps , decoder_depth ):
509"
DEF,0.5561330561330561,"3
self.linear = Linear(num_feat_maps , decoder_depth , bias=False)
510"
DEF,0.5571725571725572,"4
std_dev = 1. / sqrt( num_feat_maps )
511"
DEF,0.5582120582120582,"5
init.normal_(self.linear.weight , mean =0., std=std_dev)
512 6
513"
DEF,0.5592515592515592,"7
def
forward(self , feature_maps : list):
514"
DEF,0.5602910602910602,"8
""""""
515"
DEF,0.5613305613305614,"9
feature_maps: a list of length
num_feat_maps , each with
516"
DEF,0.5623700623700624,"dimension
517"
BATCH,0.5634095634095634,"10
Batch
Size x Num. Tokens x Embedding
Dim.
518"
BATCH,0.5644490644490644,"11
""""""
519"
BATCH,0.5654885654885655,"12
stacked_feature_maps = stack(feature_maps , dim=-1)
520"
RETURN,0.5665280665280665,"13
return
self.linear( stacked_feature_maps )
521"
RETURN,0.5675675675675675,"Additionally, we further investigate the importance of using a cross-attention decoder, where each
522"
RETURN,0.5686070686070686,"decoder block can use different feature maps from the encoder for decoding. In this experiment, we
523"
RETURN,0.5696465696465697,"incorporated IBA into MAE, which uses only a self-attention decoder. Specifically, we concatenate
524"
RETURN,0.5706860706860707,"the interblock attention features with the masked tokens. We then feed the combined features into
525"
RETURN,0.5717255717255717,"MAE’s self-attention decoder. We pre-trained the model and finetuned it for Imagenet classification.
526"
RETURN,0.5727650727650727,"The results are presented in Table. 4, where all models are pre-trained for 400 epochs. We observe that
527"
RETURN,0.5738045738045738,"inter-block attention has negligible performance improvements for MAE, potentially because MAE
528"
RETURN,0.5748440748440748,"only takes in one feature map in its decoder. In contrast, inter-block attention allows cross-attention
529"
RETURN,0.5758835758835759,"layers in CrossMAE to attend to features from different encoder blocks, thanks to its decoupling of
530"
RETURN,0.5769230769230769,"queries with keys and values.
531"
RETURN,0.577962577962578,"A.3
Ablation that Adds Self-Attention
532"
RETURN,0.579002079002079,"In Section 4.3 (a), we propose adding self-attention back to CrossMAE as an ablation. In that
533"
RETURN,0.58004158004158,"particular ablation study, we analyze the effect of self-attention between the masked tokens, which
534"
RETURN,0.581081081081081,"Method
Acc. (%)
MAE
83.0
MAE + IBA
83.0
CrossMAE (25%)
83.2
CrossMAE (75%)
83.3
Table 4: For MAE, inter-block attention has very small differences in terms of finetuning performance, potentially
due to the fact that MAE’s decoder only takes in one set of features."
RETURN,0.5821205821205822,"can be used to improve the consistency for reconstruction. Specifically, we modify the formulation in
535"
RETURN,0.5831600831600832,"the original transformer paper [56], where the mask/query tokens are first passed through a multi-
536"
RETURN,0.5841995841995842,"head self-attention and a residual connection before being used in the multiheaded cross-attention
537"
RETURN,0.5852390852390852,"with the features from the encoder. The primary difference with the vanilla transformer decoder
538"
RETURN,0.5862785862785863,"implementation [56] is we do not perform casual masking in the multi-head self-attention. Please
539"
RETURN,0.5873180873180873,"reference Figure 6 for a more visual presentation of the method.
540"
RETURN,0.5883575883575883,Figure 6: Modification for self-attention ablation
RETURN,0.5893970893970893,"A.4
Ablation on Inter-block Attention
541"
RETURN,0.5904365904365905,"In Table 3d, the following cases are considered. 1 feature map (row 1) does not use inter-block
542"
RETURN,0.5914760914760915,"attention. Each decoder block only takes the last feature map from the encoder as the keys and values.
543"
RETURN,0.5925155925155925,"For scenarios where more than one feature map is used, the output of the patch embedding (input to
544"
RETURN,0.5935550935550935,"the ViT) is also used.
545"
RETURN,0.5945945945945946,"In addition to the simple design of inter-block attention proposed above, we also experimented
546"
RETURN,0.5956340956340956,"with a variant of inter-block attention by further parameterizing the attention with linear projections.
547"
RETURN,0.5966735966735967,"Specifically, rather than directly performing weighted sum aggregation to form the features for each
548"
RETURN,0.5977130977130977,"cross-attention layer in the decoder, we added a linear projection for each encoder feature before the
549"
RETURN,0.5987525987525988,"feature aggregation. We denote this variant as CrossMAE+LP. As shown in the Table. 5 (with ViT-B
550"
RETURN,0.5997920997920998,"pre-trained for 400 epochs, consistent with the setting in Table. 3), adding a linear projection slightly
551"
RETURN,0.6008316008316008,"improves the performance. This indicates that it is possible to design variants of readout functions,
552"
RETURN,0.6018711018711018,"such as through improved inter-block attention, to improve the feature quality of CrossMAE.
553"
RETURN,0.6029106029106029,"Method
Acc. (%)
CrossMAE
83.3
CrossMAE + LP
83.5
Table 5: Improving inter-block attention by adding linear projections to the input features. The performance
gain indicates that it is possible to design variants of readout functions to improve CrossMAE."
RETURN,0.603950103950104,"A.5
Hyperparameters
554"
RETURN,0.604989604989605,"Pre-training: The default setting is in Table 6, which is consistent with the official MAE [30]
555"
RETURN,0.606029106029106,"implementation. As mentioned in Sec. 3.4, we scale the learning rate by the ratio between mask ratio
556"
RETURN,0.6070686070686071,"(p) and prediction ratio (γ) to ensure the variance of the loss is consistent with [30]. Additionally, we
557"
RETURN,0.6081081081081081,"use the linear learning rate scaling rule [25]. This results in lr = γ ∗base_lr ∗batchsize/(256 ∗p).
558"
RETURN,0.6091476091476091,"For Table 1, we use 12 decoder blocks, with mask ratio and prediction ratio both 75%, and interblock
559"
RETURN,0.6101871101871101,"attention takes in all encoder feature maps. For the 400 epochs experiments in Table 2, we scale the
560"
RETURN,0.6112266112266113,"warm-up epochs correspondingly. Other hyperparameters, such as decoder block width, are the same
561"
RETURN,0.6122661122661123,"as MAE.
562"
RETURN,0.6133056133056133,"Finetuning: We use the same hyperparameters as MAE finetuning. We use global average pooling
563"
RETURN,0.6143451143451143,"for finetuning. In MAE, the layer norm for the last encoder feature map is removed for finetuning,
564"
RETURN,0.6153846153846154,which is consistent with our pretraining setup. Please refer to Table 7 for more detail.
RETURN,0.6164241164241164,"Config
Value
optimizer
AdamW [43]
base learning rate
1.5e-4
learning rate schedule
cosine decay [42]
batch size
4096
weight decay
0.05
optimizer momentum
β1, β2 = 0.9, 0.95 [10]
warm up epoch [24]
20, 40
total epochs
400, 800"
RETURN,0.6174636174636174,"augmentation
RandomResizedCrop,
RandomHorizontalFlip"
RETURN,0.6185031185031185,Table 6: Pretraining Hyperparameters 565
RETURN,0.6195426195426196,"A.6
Compute Infrastructure
566"
RETURN,0.6205821205821206,"Each of the pretraining and finetuning experiments is run on 2 or 4 NVIDIA A100 80GB GPUs. The
567"
RETURN,0.6216216216216216,"batch size per GPU is scaled accordingly and we use gradient accumulation to avoid out-of-memory
568"
RETURN,0.6226611226611226,"errors. ViTDet [38] experiments use a single machine equipped with 8 NVIDIA A100 (80GB) GPUs.
569"
RETURN,0.6237006237006237,"We copy the datasets to the shared memory on the machines to accelerate dataloading. We use
570"
RETURN,0.6247401247401247,"FlashAttention-2 [18] to accelerate attention calculation.
571"
RETURN,0.6257796257796258,"Config
Value
optimizer
AdamW
base learning rate
1e-3
learning rate schedule
cosine decay
batch size
1024
weight decay
0.05
optimizer momentum
β1, β2 = 0.9, 0.999
warm up epoch
5
total epochs
100 (B), 50 (L)
augmentation
RandAug (9, 0.5) [17]
label smoothing [51]
0.1
mixup [64]
0.8
cutmix [63]
1.0
drop path [31]
0.1"
RETURN,0.6268191268191268,Table 7: Finetuning Hyperparameters
RETURN,0.6278586278586279,"B
Additional Experiments
572"
RETURN,0.6288981288981289,"B.1
Linear Probe
573"
RETURN,0.6299376299376299,"We provide linear probe comparisons (at 800 epochs) for ViT-Small and ViT-Base in Table. 8. For both
574"
RETURN,0.6309771309771309,"of these experiments, we run CrossMAE with a prediction ratio of 75% (reconstruction of all masked
575"
RETURN,0.632016632016632,"patches). These results show that CrossMAE achieves slightly better linear probe performance than
576"
RETURN,0.6330561330561331,"vanilla MAE.
577"
RETURN,0.6340956340956341,"Method
ViT-S
ViT-B
MAE
49.7
65.1
CrossMAE
51.5
65.4
Table 8: Linear probe experiments of CrossMAE."
RETURN,0.6351351351351351,"B.2
Masking Strategy
578"
RETURN,0.6361746361746362,"Method
Acc. (%)
Grid Masking
83.2
Random Masking
83.3
Table 9: Ablation of masking strategies."
RETURN,0.6372141372141372,"Similar to MAE [30], we here ablate the masking pattern. Instead of random masking, we perform
579"
RETURN,0.6382536382536382,"grid-wise sampling that “keeps one of every four patches” (see MAE Figure 6). The finetuning
580"
RETURN,0.6392931392931392,"performance is reported in Table. 9 for ViT-B (at 400 epochs), which shows that grid masking does
581"
RETURN,0.6403326403326404,"not lead to additional improvements in downstream performance.
582"
RETURN,0.6413721413721414,"C
Runtime and GPU Memory Comparisons with MAE
583"
RETURN,0.6424116424116424,"Method
Memory
(MB/GPU)
Runtime
(min/epoch)
Acc.
(%)"
RETURN,0.6434511434511434,"MAE
OOM (>81920)
5.19∗
83.3
CrossMAE
41177
3.38
83.5"
RETURN,0.6444906444906445,"Table 10: CrossMAE greatly improves the training
throughput and reduces the memory requirements,
lowering the barrier for masked pretraining. Statistics
are measured on 2 NVIDIA A100 80GB GPUs. Please
refer to Appendix C for comparison details. ∗: MAE’s
default batch size exceeds the capacity of 4 GPUs, re-
quiring gradient accumulation for runtime measurement."
RETURN,0.6455301455301455,"Figure 7: We compare ViT-B which is pre-trained
for 800 epochs with different variants of Cross-
MAE v.s. MAE. For CrossMAE, we vary the pre-
diction ratio p and number of decoder blocks n,
and we denote each as (p, n). While all exper-
iments are run with inter-block attention, Cross-
MAE has lower decoder FLOPS than MAE [30]
and performs on par or better."
RETURN,0.6465696465696466,"All experiments in Table 10 are conducted on a server with 4 NVIDIA A100 (80GB) GPUs, with the
584"
RETURN,0.6476091476091476,"standard hyperparameters provided above for pretraining. NVLink is equipped across the GPUs. We
585"
RETURN,0.6486486486486487,"use the default setting for MAE and set the global batch size to 4096. For CrossMAE, we also use
586"
RETURN,0.6496881496881497,"the default setting with a prediction ratio 0.25, and this takes around 41GB memory per GPU without
587"
RETURN,0.6507276507276507,"gradient accumulation (i.e., local batch size is set to 1024 samples per GPU). However, the same
588"
RETURN,0.6517671517671517,"local batch size results in out-of-memory (OOM), which indicates that the total memory requirement
589"
RETURN,0.6528066528066528,"is larger than the available memory for each GPU (80GB). To run MAE on same hardware, we
590"
RETURN,0.6538461538461539,"thus employ gradient accumulation with a local batch size of 512 to maintain the global batch size.
591"
RETURN,0.6548856548856549,"The benchmark runs each method and measures the average per epoch runtime as well as the max
592"
RETURN,0.6559251559251559,"memory allocation for 10 training epochs. Our experiments in Figure 7 show that models with lower
593"
RETURN,0.656964656964657,"prediction ratios benefit more from deeper decoders. Our model performs on par or better when
594"
RETURN,0.658004158004158,"compared to MAE, with up to 3.7× lower decoder FLOPS.
595"
RETURN,0.659043659043659,"D
Visualizing the Contributions per Decoder Block
596"
RETURN,0.66008316008316,"We propose a more fine-grained visualization approach that allows us to precisely understand the
597"
RETURN,0.6611226611226612,"effect and contribution of each decoder block.
598"
RETURN,0.6621621621621622,"Two key observations enable per-block visualization: 1) Transformer blocks have residual connections
599"
RETURN,0.6632016632016632,"from their inputs to outputs. Let fi be the output and gi(·) the residual function of decoder i, so
600"
RETURN,0.6642411642411642,"fi = fi−1 + gi(fi−1). 2) The final decoder block’s output goes through a reconstruction head h,
601"
RETURN,0.6652806652806653,"which is linear, consisting of a layer-norm and a linear layer, to produce the reconstruction. With
602"
RETURN,0.6663201663201663,"D as the decoder depth, f0 the initial input, and y the final output, y is recursively defined as
603"
RETURN,0.6673596673596673,"y = h(fD−1 + gD(fD−1)), which simplifies due to the linearity of h:
604"
RETURN,0.6683991683991684,"y = h(f0 + g1(f0) + · · · + gD(fD−1))
=
h(f0)
| {z }
Pos Embed. + Mask Token"
RETURN,0.6694386694386695,"+ h(g1(f0))
|
{z
}
Block 1"
RETURN,0.6704781704781705,"+ · · · + h(gD(fD−1))
|
{z
}
Block D"
RETURN,0.6715176715176715,"This decomposition allows us to express the reconstruction as an image stack, where the sum of all
605"
RETURN,0.6725571725571725,"the levels gives us the final reconstruction. We present the visualization in Figure 5.
606"
RETURN,0.6735966735966736,"NeurIPS Paper Checklist
607"
CLAIMS,0.6746361746361746,"1. Claims
608"
CLAIMS,0.6756756756756757,"Question: Do the main claims made in the abstract and introduction accurately reflect the
609"
CLAIMS,0.6767151767151767,"paper’s contributions and scope?
610"
CLAIMS,0.6777546777546778,"Answer: [Yes]
611"
CLAIMS,0.6787941787941788,"Justification: The claims in the abstract are justified in the method and the experiments
612"
CLAIMS,0.6798336798336798,"section.
613"
CLAIMS,0.6808731808731808,"Guidelines:
614"
CLAIMS,0.681912681912682,"• The answer NA means that the abstract and introduction do not include the claims
615"
CLAIMS,0.682952182952183,"made in the paper.
616"
CLAIMS,0.683991683991684,"• The abstract and/or introduction should clearly state the claims made, including the
617"
CLAIMS,0.685031185031185,"contributions made in the paper and important assumptions and limitations. A No or
618"
CLAIMS,0.6860706860706861,"NA answer to this question will not be perceived well by the reviewers.
619"
CLAIMS,0.6871101871101871,"• The claims made should match theoretical and experimental results, and reflect how
620"
CLAIMS,0.6881496881496881,"much the results can be expected to generalize to other settings.
621"
CLAIMS,0.6891891891891891,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
622"
CLAIMS,0.6902286902286903,"are not attained by the paper.
623"
LIMITATIONS,0.6912681912681913,"2. Limitations
624"
LIMITATIONS,0.6923076923076923,"Question: Does the paper discuss the limitations of the work performed by the authors?
625"
LIMITATIONS,0.6933471933471933,"Answer: [Yes]
626"
LIMITATIONS,0.6943866943866944,"Justification: The limitations of the work have been discussed in the Discussion and Conclu-
627"
LIMITATIONS,0.6954261954261954,"sion section.
628"
LIMITATIONS,0.6964656964656964,"Guidelines:
629"
LIMITATIONS,0.6975051975051975,"• The answer NA means that the paper has no limitation while the answer No means that
630"
LIMITATIONS,0.6985446985446986,"the paper has limitations, but those are not discussed in the paper.
631"
LIMITATIONS,0.6995841995841996,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
632"
LIMITATIONS,0.7006237006237006,"• The paper should point out any strong assumptions and how robust the results are to
633"
LIMITATIONS,0.7016632016632016,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
634"
LIMITATIONS,0.7027027027027027,"model well-specification, asymptotic approximations only holding locally). The authors
635"
LIMITATIONS,0.7037422037422038,"should reflect on how these assumptions might be violated in practice and what the
636"
LIMITATIONS,0.7047817047817048,"implications would be.
637"
LIMITATIONS,0.7058212058212058,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
638"
LIMITATIONS,0.7068607068607069,"only tested on a few datasets or with a few runs. In general, empirical results often
639"
LIMITATIONS,0.7079002079002079,"depend on implicit assumptions, which should be articulated.
640"
LIMITATIONS,0.7089397089397089,"• The authors should reflect on the factors that influence the performance of the approach.
641"
LIMITATIONS,0.7099792099792099,"For example, a facial recognition algorithm may perform poorly when image resolution
642"
LIMITATIONS,0.7110187110187111,"is low or images are taken in low lighting. Or a speech-to-text system might not be
643"
LIMITATIONS,0.7120582120582121,"used reliably to provide closed captions for online lectures because it fails to handle
644"
LIMITATIONS,0.7130977130977131,"technical jargon.
645"
LIMITATIONS,0.7141372141372141,"• The authors should discuss the computational efficiency of the proposed algorithms
646"
LIMITATIONS,0.7151767151767152,"and how they scale with dataset size.
647"
LIMITATIONS,0.7162162162162162,"• If applicable, the authors should discuss possible limitations of their approach to
648"
LIMITATIONS,0.7172557172557172,"address problems of privacy and fairness.
649"
LIMITATIONS,0.7182952182952183,"• While the authors might fear that complete honesty about limitations might be used by
650"
LIMITATIONS,0.7193347193347194,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
651"
LIMITATIONS,0.7203742203742204,"limitations that aren’t acknowledged in the paper. The authors should use their best
652"
LIMITATIONS,0.7214137214137214,"judgment and recognize that individual actions in favor of transparency play an impor-
653"
LIMITATIONS,0.7224532224532224,"tant role in developing norms that preserve the integrity of the community. Reviewers
654"
LIMITATIONS,0.7234927234927235,"will be specifically instructed to not penalize honesty concerning limitations.
655"
THEORY ASSUMPTIONS AND PROOFS,0.7245322245322245,"3. Theory Assumptions and Proofs
656"
THEORY ASSUMPTIONS AND PROOFS,0.7255717255717256,"Question: For each theoretical result, does the paper provide the full set of assumptions and
657"
THEORY ASSUMPTIONS AND PROOFS,0.7266112266112266,"a complete (and correct) proof?
658"
THEORY ASSUMPTIONS AND PROOFS,0.7276507276507277,"Answer: [NA]
659"
THEORY ASSUMPTIONS AND PROOFS,0.7286902286902287,"Justification: This work offers observations and hypotheses justified with empirical results.
660"
THEORY ASSUMPTIONS AND PROOFS,0.7297297297297297,"Guidelines:
661"
THEORY ASSUMPTIONS AND PROOFS,0.7307692307692307,"• The answer NA means that the paper does not include theoretical results.
662"
THEORY ASSUMPTIONS AND PROOFS,0.7318087318087318,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
663"
THEORY ASSUMPTIONS AND PROOFS,0.7328482328482329,"referenced.
664"
THEORY ASSUMPTIONS AND PROOFS,0.7338877338877339,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
665"
THEORY ASSUMPTIONS AND PROOFS,0.7349272349272349,"• The proofs can either appear in the main paper or the supplemental material, but if
666"
THEORY ASSUMPTIONS AND PROOFS,0.735966735966736,"they appear in the supplemental material, the authors are encouraged to provide a short
667"
THEORY ASSUMPTIONS AND PROOFS,0.737006237006237,"proof sketch to provide intuition.
668"
THEORY ASSUMPTIONS AND PROOFS,0.738045738045738,"• Inversely, any informal proof provided in the core of the paper should be complemented
669"
THEORY ASSUMPTIONS AND PROOFS,0.739085239085239,"by formal proofs provided in appendix or supplemental material.
670"
THEORY ASSUMPTIONS AND PROOFS,0.7401247401247402,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
671"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7411642411642412,"4. Experimental Result Reproducibility
672"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7422037422037422,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
673"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7432432432432432,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
674"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7442827442827443,"of the paper (regardless of whether the code and data are provided or not)?
675"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7453222453222453,"Answer: [Yes]
676"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7463617463617463,"Justification: Our code, which reproduces our results, is provided through an anonymous
677"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7474012474012474,"link in the abstract.
678"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7484407484407485,"Guidelines:
679"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7494802494802495,"• The answer NA means that the paper does not include experiments.
680"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7505197505197505,"• If the paper includes experiments, a No answer to this question will not be perceived
681"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7515592515592515,"well by the reviewers: Making the paper reproducible is important, regardless of
682"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7525987525987526,"whether the code and data are provided or not.
683"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7536382536382537,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
684"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7546777546777547,"to make their results reproducible or verifiable.
685"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7557172557172557,"• Depending on the contribution, reproducibility can be accomplished in various ways.
686"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7567567567567568,"For example, if the contribution is a novel architecture, describing the architecture fully
687"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7577962577962578,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
688"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7588357588357588,"be necessary to either make it possible for others to replicate the model with the same
689"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7598752598752598,"dataset, or provide access to the model. In general. releasing code and data is often
690"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.760914760914761,"one good way to accomplish this, but reproducibility can also be provided via detailed
691"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.761954261954262,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
692"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.762993762993763,"of a large language model), releasing of a model checkpoint, or other means that are
693"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.764033264033264,"appropriate to the research performed.
694"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7650727650727651,"• While NeurIPS does not require releasing code, the conference does require all submis-
695"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7661122661122661,"sions to provide some reasonable avenue for reproducibility, which may depend on the
696"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7671517671517671,"nature of the contribution. For example
697"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7681912681912682,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
698"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7692307692307693,"to reproduce that algorithm.
699"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7702702702702703,"(b) If the contribution is primarily a new model architecture, the paper should describe
700"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7713097713097713,"the architecture clearly and fully.
701"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7723492723492723,"(c) If the contribution is a new model (e.g., a large language model), then there should
702"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7733887733887734,"either be a way to access this model for reproducing the results or a way to reproduce
703"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7744282744282744,"the model (e.g., with an open-source dataset or instructions for how to construct
704"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7754677754677755,"the dataset).
705"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7765072765072765,"(d) We recognize that reproducibility may be tricky in some cases, in which case
706"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7775467775467776,"authors are welcome to describe the particular way they provide for reproducibility.
707"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7785862785862786,"In the case of closed-source models, it may be that access to the model is limited in
708"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7796257796257796,"some way (e.g., to registered users), but it should be possible for other researchers
709"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7806652806652806,"to have some path to reproducing or verifying the results.
710"
OPEN ACCESS TO DATA AND CODE,0.7817047817047817,"5. Open access to data and code
711"
OPEN ACCESS TO DATA AND CODE,0.7827442827442828,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
712"
OPEN ACCESS TO DATA AND CODE,0.7837837837837838,"tions to faithfully reproduce the main experimental results, as described in supplemental
713"
OPEN ACCESS TO DATA AND CODE,0.7848232848232848,"material?
714"
OPEN ACCESS TO DATA AND CODE,0.7858627858627859,"Answer: [Yes]
715"
OPEN ACCESS TO DATA AND CODE,0.7869022869022869,"Justification: Our method is evaluated on open datasets that are publicly available.
716"
OPEN ACCESS TO DATA AND CODE,0.7879417879417879,"Guidelines:
717"
OPEN ACCESS TO DATA AND CODE,0.7889812889812889,"• The answer NA means that paper does not include experiments requiring code.
718"
OPEN ACCESS TO DATA AND CODE,0.7900207900207901,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
719"
OPEN ACCESS TO DATA AND CODE,0.7910602910602911,"public/guides/CodeSubmissionPolicy) for more details.
720"
OPEN ACCESS TO DATA AND CODE,0.7920997920997921,"• While we encourage the release of code and data, we understand that this might not be
721"
OPEN ACCESS TO DATA AND CODE,0.7931392931392931,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
722"
OPEN ACCESS TO DATA AND CODE,0.7941787941787942,"including code, unless this is central to the contribution (e.g., for a new open-source
723"
OPEN ACCESS TO DATA AND CODE,0.7952182952182952,"benchmark).
724"
OPEN ACCESS TO DATA AND CODE,0.7962577962577962,"• The instructions should contain the exact command and environment needed to run to
725"
OPEN ACCESS TO DATA AND CODE,0.7972972972972973,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
726"
OPEN ACCESS TO DATA AND CODE,0.7983367983367984,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
727"
OPEN ACCESS TO DATA AND CODE,0.7993762993762994,"• The authors should provide instructions on data access and preparation, including how
728"
OPEN ACCESS TO DATA AND CODE,0.8004158004158004,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
729"
OPEN ACCESS TO DATA AND CODE,0.8014553014553014,"• The authors should provide scripts to reproduce all experimental results for the new
730"
OPEN ACCESS TO DATA AND CODE,0.8024948024948025,"proposed method and baselines. If only a subset of experiments are reproducible, they
731"
OPEN ACCESS TO DATA AND CODE,0.8035343035343036,"should state which ones are omitted from the script and why.
732"
OPEN ACCESS TO DATA AND CODE,0.8045738045738046,"• At submission time, to preserve anonymity, the authors should release anonymized
733"
OPEN ACCESS TO DATA AND CODE,0.8056133056133056,"versions (if applicable).
734"
OPEN ACCESS TO DATA AND CODE,0.8066528066528067,"• Providing as much information as possible in supplemental material (appended to the
735"
OPEN ACCESS TO DATA AND CODE,0.8076923076923077,"paper) is recommended, but including URLs to data and code is permitted.
736"
OPEN ACCESS TO DATA AND CODE,0.8087318087318087,"6. Experimental Setting/Details
737"
OPEN ACCESS TO DATA AND CODE,0.8097713097713097,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
738"
OPEN ACCESS TO DATA AND CODE,0.8108108108108109,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
739"
OPEN ACCESS TO DATA AND CODE,0.8118503118503119,"results?
740"
OPEN ACCESS TO DATA AND CODE,0.8128898128898129,"Answer: [Yes]
741"
OPEN ACCESS TO DATA AND CODE,0.8139293139293139,"Justification: We follow the hyperparam selection from MAE. The hyperparams introduced
742"
OPEN ACCESS TO DATA AND CODE,0.814968814968815,"by our work, such as the mask ratio and the number of feature maps used, are ablated.
743"
OPEN ACCESS TO DATA AND CODE,0.816008316008316,"Guidelines:
744"
OPEN ACCESS TO DATA AND CODE,0.817047817047817,"• The answer NA means that the paper does not include experiments.
745"
OPEN ACCESS TO DATA AND CODE,0.818087318087318,"• The experimental setting should be presented in the core of the paper to a level of detail
746"
OPEN ACCESS TO DATA AND CODE,0.8191268191268192,"that is necessary to appreciate the results and make sense of them.
747"
OPEN ACCESS TO DATA AND CODE,0.8201663201663202,"• The full details can be provided either with the code, in appendix, or as supplemental
748"
OPEN ACCESS TO DATA AND CODE,0.8212058212058212,"material.
749"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8222453222453222,"7. Experiment Statistical Significance
750"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8232848232848233,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
751"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8243243243243243,"information about the statistical significance of the experiments?
752"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8253638253638254,"Answer: [No]
753"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8264033264033264,"Justification: Error bars are not reported because they would be too computationally expen-
754"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8274428274428275,"sive.
755"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8284823284823285,"Guidelines:
756"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8295218295218295,"• The answer NA means that the paper does not include experiments.
757"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8305613305613305,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
758"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8316008316008316,"dence intervals, or statistical significance tests, at least for the experiments that support
759"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8326403326403327,"the main claims of the paper.
760"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8336798336798337,"• The factors of variability that the error bars are capturing should be clearly stated (for
761"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8347193347193347,"example, train/test split, initialization, random drawing of some parameter, or overall
762"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8357588357588358,"run with given experimental conditions).
763"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8367983367983368,"• The method for calculating the error bars should be explained (closed form formula,
764"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8378378378378378,"call to a library function, bootstrap, etc.)
765"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8388773388773388,"• The assumptions made should be given (e.g., Normally distributed errors).
766"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.83991683991684,"• It should be clear whether the error bar is the standard deviation or the standard error
767"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.840956340956341,"of the mean.
768"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.841995841995842,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
769"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.843035343035343,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
770"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8440748440748441,"of Normality of errors is not verified.
771"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8451143451143451,"• For asymmetric distributions, the authors should be careful not to show in tables or
772"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8461538461538461,"figures symmetric error bars that would yield results that are out of range (e.g. negative
773"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8471933471933472,"error rates).
774"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8482328482328483,"• If error bars are reported in tables or plots, The authors should explain in the text how
775"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8492723492723493,"they were calculated and reference the corresponding figures or tables in the text.
776"
EXPERIMENTS COMPUTE RESOURCES,0.8503118503118503,"8. Experiments Compute Resources
777"
EXPERIMENTS COMPUTE RESOURCES,0.8513513513513513,"Question: For each experiment, does the paper provide sufficient information on the com-
778"
EXPERIMENTS COMPUTE RESOURCES,0.8523908523908524,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
779"
EXPERIMENTS COMPUTE RESOURCES,0.8534303534303534,"the experiments?
780"
EXPERIMENTS COMPUTE RESOURCES,0.8544698544698545,"Answer: [Yes]
781"
EXPERIMENTS COMPUTE RESOURCES,0.8555093555093555,"Justification: We described the compute requirements in Appendix A.6. We do not use
782"
EXPERIMENTS COMPUTE RESOURCES,0.8565488565488566,"GPUs from a cloud provider.
783"
EXPERIMENTS COMPUTE RESOURCES,0.8575883575883576,"Guidelines:
784"
EXPERIMENTS COMPUTE RESOURCES,0.8586278586278586,"• The answer NA means that the paper does not include experiments.
785"
EXPERIMENTS COMPUTE RESOURCES,0.8596673596673596,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
786"
EXPERIMENTS COMPUTE RESOURCES,0.8607068607068608,"or cloud provider, including relevant memory and storage.
787"
EXPERIMENTS COMPUTE RESOURCES,0.8617463617463618,"• The paper should provide the amount of compute required for each of the individual
788"
EXPERIMENTS COMPUTE RESOURCES,0.8627858627858628,"experimental runs as well as estimate the total compute.
789"
EXPERIMENTS COMPUTE RESOURCES,0.8638253638253638,"• The paper should disclose whether the full research project required more compute
790"
EXPERIMENTS COMPUTE RESOURCES,0.8648648648648649,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
791"
EXPERIMENTS COMPUTE RESOURCES,0.8659043659043659,"didn’t make it into the paper).
792"
CODE OF ETHICS,0.8669438669438669,"9. Code Of Ethics
793"
CODE OF ETHICS,0.867983367983368,"Question: Does the research conducted in the paper conform, in every respect, with the
794"
CODE OF ETHICS,0.8690228690228691,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
795"
CODE OF ETHICS,0.8700623700623701,"Answer: [Yes]
796"
CODE OF ETHICS,0.8711018711018711,"Justification: The research conforms to the NeurIPS Code of Ethics.
797"
CODE OF ETHICS,0.8721413721413721,"Guidelines:
798"
CODE OF ETHICS,0.8731808731808732,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
799"
CODE OF ETHICS,0.8742203742203742,"• If the authors answer No, they should explain the special circumstances that require a
800"
CODE OF ETHICS,0.8752598752598753,"deviation from the Code of Ethics.
801"
CODE OF ETHICS,0.8762993762993763,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
802"
CODE OF ETHICS,0.8773388773388774,"eration due to laws or regulations in their jurisdiction).
803"
BROADER IMPACTS,0.8783783783783784,"10. Broader Impacts
804"
BROADER IMPACTS,0.8794178794178794,"Question: Does the paper discuss both potential positive societal impacts and negative
805"
BROADER IMPACTS,0.8804573804573804,"societal impacts of the work performed?
806"
BROADER IMPACTS,0.8814968814968815,"Answer: [Yes]
807"
BROADER IMPACTS,0.8825363825363826,"Justification: This paper aims to advance the field of self-supervised learning. Like other self-
808"
BROADER IMPACTS,0.8835758835758836,"supervised learning methods, our work may have various societal implications. However,
809"
BROADER IMPACTS,0.8846153846153846,"we do not believe any specific consequences need to be highlighted in this context.
810"
BROADER IMPACTS,0.8856548856548857,"Guidelines:
811"
BROADER IMPACTS,0.8866943866943867,"• The answer NA means that there is no societal impact of the work performed.
812"
BROADER IMPACTS,0.8877338877338877,"• If the authors answer NA or No, they should explain why their work has no societal
813"
BROADER IMPACTS,0.8887733887733887,"impact or why the paper does not address societal impact.
814"
BROADER IMPACTS,0.8898128898128899,"• Examples of negative societal impacts include potential malicious or unintended uses
815"
BROADER IMPACTS,0.8908523908523909,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
816"
BROADER IMPACTS,0.8918918918918919,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
817"
BROADER IMPACTS,0.8929313929313929,"groups), privacy considerations, and security considerations.
818"
BROADER IMPACTS,0.893970893970894,"• The conference expects that many papers will be foundational research and not tied
819"
BROADER IMPACTS,0.895010395010395,"to particular applications, let alone deployments. However, if there is a direct path to
820"
BROADER IMPACTS,0.896049896049896,"any negative applications, the authors should point it out. For example, it is legitimate
821"
BROADER IMPACTS,0.8970893970893971,"to point out that an improvement in the quality of generative models could be used to
822"
BROADER IMPACTS,0.8981288981288982,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
823"
BROADER IMPACTS,0.8991683991683992,"that a generic algorithm for optimizing neural networks could enable people to train
824"
BROADER IMPACTS,0.9002079002079002,"models that generate Deepfakes faster.
825"
BROADER IMPACTS,0.9012474012474012,"• The authors should consider possible harms that could arise when the technology is
826"
BROADER IMPACTS,0.9022869022869023,"being used as intended and functioning correctly, harms that could arise when the
827"
BROADER IMPACTS,0.9033264033264033,"technology is being used as intended but gives incorrect results, and harms following
828"
BROADER IMPACTS,0.9043659043659044,"from (intentional or unintentional) misuse of the technology.
829"
BROADER IMPACTS,0.9054054054054054,"• If there are negative societal impacts, the authors could also discuss possible mitigation
830"
BROADER IMPACTS,0.9064449064449065,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
831"
BROADER IMPACTS,0.9074844074844075,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
832"
BROADER IMPACTS,0.9085239085239085,"feedback over time, improving the efficiency and accessibility of ML).
833"
SAFEGUARDS,0.9095634095634095,"11. Safeguards
834"
SAFEGUARDS,0.9106029106029107,"Question: Does the paper describe safeguards that have been put in place for responsible
835"
SAFEGUARDS,0.9116424116424117,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
836"
SAFEGUARDS,0.9126819126819127,"image generators, or scraped datasets)?
837"
SAFEGUARDS,0.9137214137214137,"Answer: [NA]
838"
SAFEGUARDS,0.9147609147609148,"Justification: The paper does not pose such risks.
839"
SAFEGUARDS,0.9158004158004158,"Guidelines:
840"
SAFEGUARDS,0.9168399168399168,"• The answer NA means that the paper poses no such risks.
841"
SAFEGUARDS,0.9178794178794178,"• Released models that have a high risk for misuse or dual-use should be released with
842"
SAFEGUARDS,0.918918918918919,"necessary safeguards to allow for controlled use of the model, for example by requiring
843"
SAFEGUARDS,0.91995841995842,"that users adhere to usage guidelines or restrictions to access the model or implementing
844"
SAFEGUARDS,0.920997920997921,"safety filters.
845"
SAFEGUARDS,0.922037422037422,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
846"
SAFEGUARDS,0.9230769230769231,"should describe how they avoided releasing unsafe images.
847"
SAFEGUARDS,0.9241164241164241,"• We recognize that providing effective safeguards is challenging, and many papers do
848"
SAFEGUARDS,0.9251559251559252,"not require this, but we encourage authors to take this into account and make a best
849"
SAFEGUARDS,0.9261954261954262,"faith effort.
850"
LICENSES FOR EXISTING ASSETS,0.9272349272349273,"12. Licenses for existing assets
851"
LICENSES FOR EXISTING ASSETS,0.9282744282744283,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
852"
LICENSES FOR EXISTING ASSETS,0.9293139293139293,"the paper, properly credited and are the license and terms of use explicitly mentioned and
853"
LICENSES FOR EXISTING ASSETS,0.9303534303534303,"properly respected?
854"
LICENSES FOR EXISTING ASSETS,0.9313929313929314,"Answer: [Yes]
855"
LICENSES FOR EXISTING ASSETS,0.9324324324324325,"Justification: The code and datasets used in this work follow the original MAE work.
856"
LICENSES FOR EXISTING ASSETS,0.9334719334719335,"Guidelines:
857"
LICENSES FOR EXISTING ASSETS,0.9345114345114345,"• The answer NA means that the paper does not use existing assets.
858"
LICENSES FOR EXISTING ASSETS,0.9355509355509356,"• The authors should cite the original paper that produced the code package or dataset.
859"
LICENSES FOR EXISTING ASSETS,0.9365904365904366,"• The authors should state which version of the asset is used and, if possible, include a
860"
LICENSES FOR EXISTING ASSETS,0.9376299376299376,"URL.
861"
LICENSES FOR EXISTING ASSETS,0.9386694386694386,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
862"
LICENSES FOR EXISTING ASSETS,0.9397089397089398,"• For scraped data from a particular source (e.g., website), the copyright and terms of
863"
LICENSES FOR EXISTING ASSETS,0.9407484407484408,"service of that source should be provided.
864"
LICENSES FOR EXISTING ASSETS,0.9417879417879418,"• If assets are released, the license, copyright information, and terms of use in the
865"
LICENSES FOR EXISTING ASSETS,0.9428274428274428,"package should be provided. For popular datasets, paperswithcode.com/datasets
866"
LICENSES FOR EXISTING ASSETS,0.9438669438669439,"has curated licenses for some datasets. Their licensing guide can help determine the
867"
LICENSES FOR EXISTING ASSETS,0.9449064449064449,"license of a dataset.
868"
LICENSES FOR EXISTING ASSETS,0.9459459459459459,"• For existing datasets that are re-packaged, both the original license and the license of
869"
LICENSES FOR EXISTING ASSETS,0.946985446985447,"the derived asset (if it has changed) should be provided.
870"
LICENSES FOR EXISTING ASSETS,0.9480249480249481,"• If this information is not available online, the authors are encouraged to reach out to
871"
LICENSES FOR EXISTING ASSETS,0.9490644490644491,"the asset’s creators.
872"
NEW ASSETS,0.9501039501039501,"13. New Assets
873"
NEW ASSETS,0.9511434511434511,"Question: Are new assets introduced in the paper well documented and is the documentation
874"
NEW ASSETS,0.9521829521829522,"provided alongside the assets?
875"
NEW ASSETS,0.9532224532224532,"Answer: [NA]
876"
NEW ASSETS,0.9542619542619543,"Justification: The paper does not release new assets.
877"
NEW ASSETS,0.9553014553014553,"Guidelines:
878"
NEW ASSETS,0.9563409563409564,"• The answer NA means that the paper does not release new assets.
879"
NEW ASSETS,0.9573804573804574,"• Researchers should communicate the details of the dataset/code/model as part of their
880"
NEW ASSETS,0.9584199584199584,"submissions via structured templates. This includes details about training, license,
881"
NEW ASSETS,0.9594594594594594,"limitations, etc.
882"
NEW ASSETS,0.9604989604989606,"• The paper should discuss whether and how consent was obtained from people whose
883"
NEW ASSETS,0.9615384615384616,"asset is used.
884"
NEW ASSETS,0.9625779625779626,"• At submission time, remember to anonymize your assets (if applicable). You can either
885"
NEW ASSETS,0.9636174636174636,"create an anonymized URL or include an anonymized zip file.
886"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9646569646569647,"14. Crowdsourcing and Research with Human Subjects
887"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9656964656964657,"Question: For crowdsourcing experiments and research with human subjects, does the paper
888"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9667359667359667,"include the full text of instructions given to participants and screenshots, if applicable, as
889"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9677754677754677,"well as details about compensation (if any)?
890"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9688149688149689,"Answer: [NA]
891"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9698544698544699,"Justification: The paper does not involve crowdsourcing or research with human subjects.
892"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9708939708939709,"Guidelines:
893"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9719334719334719,"• The answer NA means that the paper does not involve crowdsourcing nor research with
894"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.972972972972973,"human subjects.
895"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.974012474012474,"• Including this information in the supplemental material is fine, but if the main contribu-
896"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.975051975051975,"tion of the paper involves human subjects, then as much detail as possible should be
897"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9760914760914761,"included in the main paper.
898"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9771309771309772,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
899"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9781704781704782,"or other labor should be paid at least the minimum wage in the country of the data
900"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9792099792099792,"collector.
901"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9802494802494802,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
902"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9812889812889813,"Subjects
903"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9823284823284824,"Question: Does the paper describe potential risks incurred by study participants, whether
904"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9833679833679834,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
905"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9844074844074844,"approvals (or an equivalent approval/review based on the requirements of your country or
906"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9854469854469855,"institution) were obtained?
907"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9864864864864865,"Answer: [NA]
908"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9875259875259875,"Justification: The paper does not involve crowdsourcing or research with human subjects.
909"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9885654885654885,"Guidelines:
910"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9896049896049897,"• The answer NA means that the paper does not involve crowdsourcing nor research with
911"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9906444906444907,"human subjects.
912"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9916839916839917,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
913"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9927234927234927,"may be required for any human subjects research. If you obtained IRB approval, you
914"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9937629937629938,"should clearly state this in the paper.
915"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9948024948024948,"• We recognize that the procedures for this may vary significantly between institutions
916"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9958419958419958,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
917"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9968814968814969,"guidelines for their institution.
918"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.997920997920998,"• For initial submissions, do not include any information that would break anonymity (if
919"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.998960498960499,"applicable), such as the institution conducting the review.
920"
