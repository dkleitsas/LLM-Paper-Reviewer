Section,Section Appearance Order,Paragraph
UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,0.0,"1University of Illinois at Urbana-Champaign
2Carnegie Mellon University
3Lawrence Livermore National Laboratory
4Amazon AWS AI
{fanw6,linyi2,chejian2,lbo}@illinois.edu
huan@huan-zhang.com
kailkhura1@llnl.gov
kkenthapadi@gmail.com
dingzhao@andrew.cmu.edu
* Equal contribution."
ABSTRACT,0.0010964912280701754,ABSTRACT
ABSTRACT,0.0021929824561403508,"As reinforcement learning (RL) has achieved near human-level performance in a
variety of tasks, its robustness has raised great attention. While a vast body of
research has explored test-time (evasion) attacks in RL and corresponding de-
fenses, its robustness against training-time (poisoning) attacks remains largely
unanswered. In this work, we focus on certifying the robustness of ofÔ¨Çine RL
in the presence of poisoning attacks, where a subset of training trajectories could
be arbitrarily manipulated. We propose the Ô¨Årst certiÔ¨Åcation framework, COPA,
to certify the number of poisoning trajectories that can be tolerated regarding dif-
ferent certiÔ¨Åcation criteria. Given the complex structure of RL, we propose two
certiÔ¨Åcation criteria: per-state action stability and cumulative reward bound. To
further improve the certiÔ¨Åcation, we propose new partition and aggregation pro-
tocols to train robust policies. We further prove that some of the proposed certiÔ¨Å-
cation methods are theoretically tight and some are NP-Complete problems. We
leverage COPA to certify three RL environments trained with different algorithms
and conclude: (1) The proposed robust aggregation protocols such as temporal
aggregation can signiÔ¨Åcantly improve the certiÔ¨Åcations; (2) Our certiÔ¨Åcations for
both per-state action stability and cumulative reward bound are efÔ¨Åcient and tight;
(3) The certiÔ¨Åcation for different training algorithms and environments are dif-
ferent, implying their intrinsic robustness properties. All experimental results are
available at https://copa-leaderboard.github.io."
INTRODUCTION,0.003289473684210526,"1
INTRODUCTION"
INTRODUCTION,0.0043859649122807015,"Reinforcement learning (RL) has been widely applied to a range of applications, including
robotics (Kober et al., 2013; Deisenroth et al., 2013; Polydoros & Nalpantidis, 2017) and au-
tonomous vehicles (Shalev-Shwartz et al., 2016; Sallab et al., 2017). In particular, ofÔ¨Çine RL (Levine
et al., 2020) is proposed to leverage previously observed data to train the policy without requiring
expensive interaction with environments or online data collection, and enables the reuse of training
data (Agarwal et al., 2020). However, ofÔ¨Çine RL also raises a great safety concern on poisoning
attacks (Kiourti et al., 2020; Wang et al., 2020; 2021). A recent survey of industry reports that data
poisoning is signiÔ¨Åcantly more concerning than other threats (Kumar et al., 2020). For ofÔ¨Çine RL,
the situation is even worse, and a recent theoretical study shows that the robust ofÔ¨Çine RL against
poisoning attacks is a strictly harder problem than online RL (Zhang et al., 2021)."
INTRODUCTION,0.005482456140350877,"Although there are several empirical and certiÔ¨Åed defenses for classiÔ¨Åcation tasks against poisoning
attacks (Peri et al., 2020; Levine & Feizi, 2021; Weber et al., 2020), it is challenging and shown
ineffective to directly apply them to RL given its complex structure (Kiourti et al., 2020). Thus, ro-
bust ofÔ¨Çine RL against poisoning attacks remains largely unexplored with no mention of robustness
certiÔ¨Åcation. In addition, though some theoretical analyses provide general robustness bounds for
RL, they either assume a bounded distance between the learned and Bellman optimal policies or are
limited to linear MDPs (Zhang et al., 2021). To the best of our knowledge, there is no robust RL"
INTRODUCTION,0.006578947368421052,Published as a conference paper at ICLR 2022
INTRODUCTION,0.007675438596491228,"method that is able to provide practically computable certiÔ¨Åed robustness against poisoning attacks.
In this paper, we tackle this problem by proposing the Ô¨Årst framework of Certifying robust policies
for general ofÔ¨Çine RL against poisoning attacks (COPA)."
INTRODUCTION,0.008771929824561403,"CertiÔ¨Åcation Criteria. One critical challenge in certifying robustness for ofÔ¨Çine RL is the certiÔ¨Å-
cation criteria, since the prediction consistency is no longer the only goal as in classiÔ¨Åcation. We
propose two criteria based on the properties of RL: per-state action stability and cumulative reward
bound. The former guarantees that at a speciÔ¨Åc time, the policy learned with COPA will predict the
same action before and after attacks under certain conditions. This is important for guaranteeing the
safety of the policy at critical states, e.g., braking when seeing pedestrians. For cumulative reward
bound, a lower bound of the cumulative reward for the policy learned with COPA is guaranteed
under certain poisoning conditions. This directly guarantees the worst-case overall performance."
INTRODUCTION,0.009868421052631578,"COPA Framework. COPA is composed of two components: policy partition and aggregation
protocol and robustness certiÔ¨Åcation method. We propose three policy partition aggregation pro-
tocols: PARL (Per-State Partition Aggregation), TPARL (Temporal Partition Aggregation), and
DPARL (Dynamic Temporal Partition Aggregation), and propose certiÔ¨Åcation methods for each of
them corresponding to both proposed certiÔ¨Åcation criteria. In addition, for per-state action stability,
we prove that our certiÔ¨Åcations for PARL and TPARL are theoretically tight. For cumulative reward
bound, we propose an adaptive search algorithm, where we compute the possible action set for each
state under certain poisoning conditions. Concretely, we propose a novel method to compute the
precise action set for PARL and efÔ¨Åcient algorithms to compute a superset of the possible action
set which leads to sound certiÔ¨Åcation for TPARL and DPARL. We further prove that for PARL our
certiÔ¨Åcation is theoretically tight, for TPARL the theoretically tight certiÔ¨Åcation is NP-complete,
and for DPARL it is open whether theoretically tight certiÔ¨Åcation exists."
INTRODUCTION,0.010964912280701754,"Technical Contributions. We take the Ô¨Årst step towards certifying the robustness of ofÔ¨Çine RL
against poisoning attacks, and we make contributions on both theoretical and practical fronts."
INTRODUCTION,0.01206140350877193,"‚Ä¢ We abstract and formulate the robustness certiÔ¨Åcation for ofÔ¨Çine RL against poisoning attacks,
and we propose two certiÔ¨Åcation criteria: per-state action stability and cumulative reward bound.
‚Ä¢ We propose the Ô¨Årst framework COPA for certifying robustness of ofÔ¨Çine RL against poisoning
attacks. COPA includes novel policy aggregation protocols and certiÔ¨Åcation methods.
‚Ä¢ We prove the tightness of the proposed certiÔ¨Åcation methods for the aggregation protocol PARL.
We also prove the computational hardness of the certiÔ¨Åcation for TPARL.
‚Ä¢ We conduct thorough experimental evaluation for COPA on different RL environments with
three ofÔ¨Çine RL algorithms, demonstrating the effectiveness of COPA, together with several
interesting Ô¨Åndings."
RELATED WORK,0.013157894736842105,"2
RELATED WORK"
RELATED WORK,0.01425438596491228,"Poisoning attacks (Nelson et al., 2008; Diakonikolas et al., 2016) are critical threats in machine
learning, which are claimed to be more concerning than other threats (Kumar et al., 2020).
Poisoning attacks widely exist in classiÔ¨Åcation (Schwarzschild et al., 2021), and both empirical
defenses (Liu et al., 2018; Chacon et al., 2019; Peri et al., 2020; Steinhardt et al., 2017) and certiÔ¨Åed
defenses (Weber et al., 2020; Jia et al., 2020; Levine & Feizi, 2021) have been proposed."
RELATED WORK,0.015350877192982455,"After Kiourti et al. (2020) show the existence of effective backdoor poisoning attacks in RL, a
recent work theoretically and empirically validates the existence of reward poisoning in online
RL (Zhang et al., 2020b). Furthermore, Zhang et al. (2021) theoretically prove that the ofÔ¨Çine RL
is more difÔ¨Åcult to be robustiÔ¨Åed against poisoning than online RL considering linear MDP. From
the defense side, Zhang et al. (2021) propose robust variants of the Least-Square Value Iteration
algorithm that provides probabilistic robustness guarantees under linear MDP assumption.
In
addition, Robust RL against reward poisoning is studied in Banihashem et al. (2021), but robust
RL against general poisoning is less explored. In this background, we aim to provide the certiÔ¨Åed
robustness for general ofÔ¨Çine RL algorithms against poisoning attacks, which is the Ô¨Årst work that
achieves the goal. We discuss broader related work in Appendix I."
CERTIFICATION CRITERIA OF COPA,0.01644736842105263,"3
CERTIFICATION CRITERIA OF COPA"
CERTIFICATION CRITERIA OF COPA,0.017543859649122806,"In this section, we propose two robustness certiÔ¨Åcation criteria for ofÔ¨Çine RL against general poi-
soning attacks: per-state action stability and cumulative reward bound."
CERTIFICATION CRITERIA OF COPA,0.01864035087719298,Published as a conference paper at ICLR 2022
CERTIFICATION CRITERIA OF COPA,0.019736842105263157,"OfÔ¨Çine RL.
We model the RL environment by an episodic Ô¨Ånite-horizon Markov decision pro-
cess (MDP) E = (S, A, R, P, H, d0), where S is the set of states, A is the set of discrete actions,
R : S √ó A ‚ÜíR is the reward function, P : S √ó A ‚ÜíP(S) is the stochastic transition function
with P(¬∑) deÔ¨Åning the set of probability measures, H is the time horizon, and d0 ‚ààP(S) is the
distribution of the initial state. At time step t, the RL agent is at state st ‚ààS. After choosing action
at ‚ààA, the agent transitions to the next state st+1 ‚àºP(st, at) and receives reward rt = R(st, at).
After H time steps, the cumulative reward J = PH‚àí1
t=0 rt. We denote a consecutive sequence of all
states between time step l and r as sl:r := [sl, sl+1, . . . , sr]."
CERTIFICATION CRITERIA OF COPA,0.020833333333333332,"Here we focus on ofÔ¨Çine RL, for which the threat of poisoning attacks is practical and more chal-
lenging to deal with (Zhang et al., 2021). Concretely, in ofÔ¨Çine RL, a training dataset D = {œÑi}N
i=1
consists of logged trajectories, where each trajectory œÑ = {(sj, rj, aj, s‚Ä≤
j)}l
j=1 ‚àà(S √ó A √ó R √ó S)l
consists of multiple tuples denoting the transitions (i.e., starting from state sj, taking the action aj,
receiving reward rj, and transitioning to the next state s‚Ä≤
j)."
CERTIFICATION CRITERIA OF COPA,0.021929824561403508,"Poisoning Attacks.
Training dataset D can be poisoned in the following manner. For each tra-
jectory œÑ ‚ààD, the adversary is allowed to replace it with an arbitrary trajectory eœÑ, generating a
manipulated dataset eD. We denote D ‚äñeD = (D\ eD) S( eD\D) as the symmetric difference between
two datasets D and eD. For instance, adding or removing one trajectory causes a symmetric differ-
ence of magnitude 1, while replacing one trajectory with a new one leads to a symmetric difference
of magnitude 2. We refer to the size of the symmetric difference as the poisoning size."
CERTIFICATION CRITERIA OF COPA,0.023026315789473683,"CertiÔ¨Åcation Goal.
To provide the robustness certiÔ¨Åcation against poisoning attacks introduced
above, we aim to certify the test-time performance of the trained policy in a clean environment.
SpeciÔ¨Åcally, in the training phase, the RL training algorithm and our aggregation protocol can be
jointly modeled by M : D ‚Üí(S‚ãÜ‚ÜíA) which provides an aggregated policy, where S‚ãÜdenotes
the set of all consecutive state sequences. Our goal is to provide robustness certiÔ¨Åcation for the
poisoned aggregated policy ÀúœÄ = M( eD), given bounded poisoning size (i.e., |D ‚äñeD| ‚â§K)."
CERTIFICATION CRITERIA OF COPA,0.02412280701754386,"Robustness CertiÔ¨Åcation Criteria: Per-State Action Stability.
We Ô¨Årst aim to certify the ro-
bustness of the poisoned policy in terms of the stability of per-state action during test time."
CERTIFICATION CRITERIA OF COPA,0.025219298245614034,"DeÔ¨Ånition 1 (Robustness CertiÔ¨Åcation for Per-State Action Stability). Given a clean dataset D, we
deÔ¨Åne the robustness certiÔ¨Åcation for per-state action stability as that for any eD satisfying |D‚äñeD| ‚â§
K, the action predictions of the poisoned and clean policies for the state (or state sequence) s are the
same, i.e., ÀúœÄ = M( eD), œÄ = M(D), ÀúœÄ(s) = œÄ(s), under the the tolerable poisoning threshold K.
In an episode, we denote the tolerable poisoning threshold for the state at step t by Kt."
CERTIFICATION CRITERIA OF COPA,0.02631578947368421,"The deÔ¨Ånition encodes the requirement that, for a particular state, any poisoned policy will always
give the same action prediction as the clean one, as long as the poisoning size is within K (K
computed in Section 4). In this deÔ¨Ånition, s could be either a state or a state sequence, since our ag-
gregated policy (deÔ¨Åned in Section 3) may aggregate multiple recent states to make an action choice."
CERTIFICATION CRITERIA OF COPA,0.027412280701754384,"Robustness CertiÔ¨Åcation Criteria: Lower Bound of Cumulative Reward.
We also aim to cer-
tify poisoned policy‚Äôs overall performance in addition to the prediction at a particular state. Here we
measure the overall performance by the cumulative reward J(œÄ) (formally deÔ¨Åned in Appendix A)."
CERTIFICATION CRITERIA OF COPA,0.02850877192982456,Now we are ready to deÔ¨Åne the robustness certiÔ¨Åcation for cumulative reward bound.
CERTIFICATION CRITERIA OF COPA,0.029605263157894735,"DeÔ¨Ånition 2 (Robustness CertiÔ¨Åcation for Cumulative Reward Bound). Robustness certiÔ¨Åcation for
cumulative reward bound is the lower bound of cumulative reward JK such that JK ‚â§J(ÀúœÄ) for any
ÀúœÄ = M( eD) where |D ‚äñeD| ‚â§K, i.e., ÀúœÄ is trained on poisoned dataset eD within poisoning size K."
CERTIFICATION PROCESS OF COPA,0.03070175438596491,"4
CERTIFICATION PROCESS OF COPA"
CERTIFICATION PROCESS OF COPA,0.03179824561403509,"In this section, we introduce our framework COPA, which is composed of training protocols, ag-
gregation protocols, and certiÔ¨Åcation methods. The training protocol combined with an ofÔ¨Çine RL
training algorithm provides subpolicies. The aggregation protocol aggregates the subpolicies as an
aggregated policy. The certiÔ¨Åcation method certiÔ¨Åes the robustness of the aggregated policy against
poisoning attacks corresponding to different certiÔ¨Åcation criteria provided in Section 3."
CERTIFICATION PROCESS OF COPA,0.03289473684210526,Published as a conference paper at ICLR 2022
CERTIFICATION PROCESS OF COPA,0.03399122807017544,"Table 1: Overview of theoretical results in Section 4. ‚ÄúCertiÔ¨Åcation‚Äù columns entail our certiÔ¨Åcation theorems.
‚ÄúAnalysis‚Äù columns entail the analyses of our certiÔ¨Åcation bounds, where ‚Äútight‚Äù means our certiÔ¨Åcation is
theoretically tight, ‚ÄúNP-complete‚Äù means the tight certiÔ¨Åcation problem is NP-complete, and ‚Äúopen‚Äù means
the tight certiÔ¨Åcation problem is still open. Theorems 8 and 13 and proposition 9 are in Appendices C and F.6.2."
CERTIFICATION PROCESS OF COPA,0.03508771929824561,"CertiÔ¨Åcation
Criteria"
CERTIFICATION PROCESS OF COPA,0.03618421052631579,Proposed Aggregation Protocol
CERTIFICATION PROCESS OF COPA,0.03728070175438596,"PARL (œÄP, DeÔ¨Ånition 7)
TPARL (œÄT, DeÔ¨Ånition 8)
DPARL (œÄD, DeÔ¨Ånition 3)
CertiÔ¨Åcation
Analysis
CertiÔ¨Åcation
Analysis
CertiÔ¨Åcation
Analysis"
CERTIFICATION PROCESS OF COPA,0.03837719298245614,"Per-State Action
Theorem 8
tight (Proposition 9)
Theorem 1
tight (Proposition 2)
Theorem 3
open
Cumulative Reward
Theorem 4
tight (Theorem 13)
Theorem 5
NP-complete (Theorem 6)
Theorem 7
open"
CERTIFICATION PROCESS OF COPA,0.039473684210526314,"Overview of Theoretical Results.
In Table 1, we present an overview of our theoretical results:
For each proposed aggregation protocol and certiÔ¨Åcation criteria, we provide the corresponding cer-
tiÔ¨Åcation method and core theorems, and we also provide the tightness analysis for each certiÔ¨Åcation."
PARTITION-BASED TRAINING PROTOCOL,0.04057017543859649,"4.1
PARTITION-BASED TRAINING PROTOCOL"
PARTITION-BASED TRAINING PROTOCOL,0.041666666666666664,"COPA‚Äôs training protocol contains two stages: partitioning and training. We denote D as the entire
ofÔ¨Çine RL training dataset. We abstract an ofÔ¨Çine RL training algorithm (e.g., DQN) by M0 : 2D ‚Üí
Œ†, where 2D is the power set of D, and Œ† = {œÄ : S ‚ÜíA} is the set of trained policies. Each trained
policy in Œ† is a function mapping a given state to the predicted action."
PARTITION-BASED TRAINING PROTOCOL,0.04276315789473684,"Partitioning Stage.
In this stage, we separate the training dataset D into u partitions {Di}u‚àí1
i=0
that satisfy Su‚àí1
i=0 Di = D and ‚àÄi Ã∏= j, Di ‚à©Dj = ‚àÖ. Concretely, when performing partitioning,
for each trajectory œÑ ‚ààD, we deterministically assign it to one unique partition. The assignment
is only dependent on the trajectory œÑ itself, and not impacted by any modiÔ¨Åcation to other parts of
the training set. One design choice of such a deterministic assignment is using a deterministic hash
function h to compute the assignment, i.e., Di = {œÑ ‚ààD | h(œÑ) ‚â°i (mod u)}, ‚àÄi ‚àà[u]."
PARTITION-BASED TRAINING PROTOCOL,0.043859649122807015,"Training Stage.
In this stage, for each training data partition Di, we independently apply an
RL algorithm M0 to train a policy œÄi = M0(Di). Hereinafter, we call these trained polices as
subpolicies to distinguish from the aggregated policies. Concretely, let [u] := {0, 1, . . . , u‚àí1}. For
these subpolicies, the policy indicator 1i,a : S ‚Üí{0, 1} is deÔ¨Åned by 1i,a(s) := 1[œÄi(s) = a],
indicating whether subpolicy œÄi chooses action a at state s. The aggregated action count na : S ‚Üí
N‚â•0 is the number of votes across all the subpolicies for action a given state s: na(s) := |{i|œÄi(s) =
a, i ‚àà[u]}| = Pu‚àí1
i=0 1i,a(s). SpeciÔ¨Åcally, we denote na(sl:r) for Pr
j=l na(sj), i.e., the sum of votes
for states between time step l and r. A detailed algorithm of the training protocol is in Appendix E.1."
PARTITION-BASED TRAINING PROTOCOL,0.044956140350877194,"Now we are ready to introduce the proposed aggregation protocols in COPA (PARL, TPARL,
DPARL) that generate aggregated policies based on subpolicies, and corresponding certiÔ¨Åcation."
PARTITION-BASED TRAINING PROTOCOL,0.046052631578947366,"4.2
AGGREGATION PROTOCOLS: PARL, TPARL, DPARL"
PARTITION-BASED TRAINING PROTOCOL,0.047149122807017545,"With u learned subpolicies {œÄi}u‚àí1
i=0 , we propose three different aggregation protocols in COPA to
form three types of aggregated policies for each certiÔ¨Åcation criteria: PARL, TPARL, and DPARL."
PARTITION-BASED TRAINING PROTOCOL,0.04824561403508772,"Per-State Partition Aggregation (PARL). Inspired by aggregation in classiÔ¨Åcation (Levine &
Feizi, 2021), PARL aggregates subpolicies by choosing actions with the highest votes. We denote
the PARL aggregated policy by œÄP : S ‚ÜíA. When there are multiple highest voting actions, we
break ties deterministically by returning the ‚Äúsmaller‚Äù (<) action, which can be deÔ¨Åned by numeri-
cal order, lexicographical order, etc. Throughout the paper, we assume arg max over A always uses
< operator to break ties. The formal deÔ¨Ånition of the protocol is in Appendix A."
PARTITION-BASED TRAINING PROTOCOL,0.049342105263157895,"The intuition behind PARL is that the poisoning attack within size K can change at most K sub-
policies. Therefore, as long as the margin between the votes for top and runner-up actions is larger
than 2K for the given state, after poisoning, we can guarantee that the aggregated PARL policy will
not change its action choice. We will formally state the robustness guarantee in Section 4.3."
PARTITION-BASED TRAINING PROTOCOL,0.05043859649122807,"Temporal Partition Aggregation (TPARL).
In the sequential decision making process of RL,
it is likely that certain important states are much more vulnerable to poisoning attacks, which we
refer to as bottleneck states. Therefore, the attacker may just change the action predictions for these
bottleneck states to deteriorate the overall performance, say, the cumulative reward. For example,
in Pong game, we may lose the round when choosing an immediate bad action when the ball is
closely approaching the paddle. Thus, to improve the overall certiÔ¨Åed robustness, we need to focus"
PARTITION-BASED TRAINING PROTOCOL,0.051535087719298246,Published as a conference paper at ICLR 2022
PARTITION-BASED TRAINING PROTOCOL,0.05263157894736842,"on improving the tolerable poisoning threshold for these bottleneck states. Given such intuition and
goal, we propose Temporal Partition Aggregation (TPARL) and the aggregated policy is denoted as
œÄT, which is formally deÔ¨Åned in DeÔ¨Ånition 8 in Appendix A."
PARTITION-BASED TRAINING PROTOCOL,0.0537280701754386,"TPARL is based on two insights: (1) Bottleneck states have lower tolerable poisoning threshold,
which is because the vote margin between the top and runner-up actions is smaller at such state;
(2) Some RL tasks satisfy temporal continuity (Legenstein et al., 2010; Veerapaneni et al., 2020),
indicating that good action choices are usually similar across states of adjacent time steps, i.e.,
adjacent states. Hence, we leverage the subpolicies‚Äô votes from adjacent states to enlarge the vote
margin, and thus increase the tolerable poisoning threshold. To this end, in TPARL, we predetermine
a window size W, and choose the action with the highest votes across recent W states."
PARTITION-BASED TRAINING PROTOCOL,0.05482456140350877,"Dynamic Temporal Partition Aggregation (DPARL).
The TPARL uses a Ô¨Åxed window size W
across all states. Since the speciÔ¨Åcation of the window size W requires certain prior knowledge,
plus that the same Ô¨Åxed window size W may not be suitable for all states, it is preferable to perform
dynamic temporal aggregation by using a Ô¨Çexible window size. Therefore, we propose Dynamic
Temporal Partition Aggregation (DPARL), which dynamically selects the window size W towards
maximizing the tolerable poisoning threshold per step. Intuitively, DPARL selects the window size
W such that the average vote margin over selected states is maximized. To guarantee that only
recent states are chosen, we further constrain the maximum window size (Wmax)."
PARTITION-BASED TRAINING PROTOCOL,0.05592105263157895,"DeÔ¨Ånition 3 (Dynamic Temporal Partition Aggregation). Given subpolicies {œÄi}u‚àí1
i=0 and maximum
window size Wmax, at time step t, the Dynamic Temporal Partition Aggregation (DPARL) deÔ¨Ånes
an aggregated policy œÄD : Smin{t+1,Wmax} ‚ÜíA such that
œÄD(smax{t‚àíWmax+1,0}:t) := arg max
a‚ààA
na(st‚àíW ‚Ä≤+1:t), where W ‚Ä≤ =
arg max
1‚â§W ‚â§min{Wmax,t+1}
‚àÜW
t .
(1)"
PARTITION-BASED TRAINING PROTOCOL,0.05701754385964912,"In the above equation, na is deÔ¨Åned in Section 3 and ‚àÜW
t
is given by"
PARTITION-BASED TRAINING PROTOCOL,0.0581140350877193,"‚àÜW
t
:= 1"
PARTITION-BASED TRAINING PROTOCOL,0.05921052631578947,"W (na1(st‚àíW +1:t) ‚àína2(st‚àíW +1:t)) ,"
PARTITION-BASED TRAINING PROTOCOL,0.06030701754385965,"where
a1 = arg max
a‚ààA
na(st‚àíW +1:t), a2 = arg max
a‚ààA,aÃ∏=a1
na(st‚àíW +1:t).
(2)"
PARTITION-BASED TRAINING PROTOCOL,0.06140350877192982,"In the above deÔ¨Ånition, ‚àÜW
t
encodes the average vote margin between top action a1 and runner-up
action a2 if choosing window size W. Thus, W ‚Ä≤ locates the window size with maximum average
vote margin, and its corresponding action is selected. Again, we use the mechanism described in
PARL to break ties. Robustness certiÔ¨Åcation methods for DPARL are in Sections 4.3 and 4.4."
PARTITION-BASED TRAINING PROTOCOL,0.0625,"In Appendix B, we present a concrete example to demonstrate how different aggregation protocols
induce different tolerable poisoning thresholds, and illustrate bottleneck and non-bottleneck states."
CERTIFICATION OF PER-STATE ACTION STABILITY,0.06359649122807018,"4.3
CERTIFICATION OF PER-STATE ACTION STABILITY"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.06469298245614036,"In this section, we present our robustness certiÔ¨Åcation theorems and methods for per-state action.
For each of the aggregation protocols (PARL, TPARL, and DPARL), at each time step t, we will
compute a valid tolerable poisoning threshold K as deÔ¨Åned in DeÔ¨Ånition 1, such that the chosen
action at step t does not change as long as the poisoning size K ‚â§K."
CERTIFICATION OF PER-STATE ACTION STABILITY,0.06578947368421052,"CertiÔ¨Åcation for PARL. Due to the space limit, we defer the robustness certiÔ¨Åcation method for
PARL to Appendix C. The certiÔ¨Åcation method is based on Theorem 8. We further show the theo-
retical tightness of the certiÔ¨Åcation in Proposition 9. All the theorem statements are in Appendix C."
CERTIFICATION OF PER-STATE ACTION STABILITY,0.0668859649122807,CertiÔ¨Åcation for TPARL. We certify the robustness of TPARL following Theorem 1.
CERTIFICATION OF PER-STATE ACTION STABILITY,0.06798245614035088,"Theorem 1. Let D be the clean training dataset; let œÄi = M0(Di), 0 ‚â§i ‚â§u ‚àí1 be the learned
subpolicies according to Section 4.1 from which we deÔ¨Åne na (Section 3); and let œÄT be the Temporal
Partition Aggregation policy: œÄT = M(D) where M abstracts the whole training-aggregation
process. eD is a poisoned dataset and f
œÄT is the poisoned policy: f
œÄT = M( eD)."
CERTIFICATION OF PER-STATE ACTION STABILITY,0.06907894736842106,"For a given state st encountered at time step t during test time, let a := œÄT(smax{t‚àíW +1,0}:t), then
at time step t the tolerable poisoning threshold (see DeÔ¨Ånition 1)"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.07017543859649122,"Kt =
min
a‚Ä≤Ã∏=a,a‚Ä≤‚ààA max ( p p
X"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.0712719298245614,"i=1
h(i)
a,a‚Ä≤ ‚â§Œ¥a,a‚Ä≤ ) (3)"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.07236842105263158,Published as a conference paper at ICLR 2022
CERTIFICATION OF PER-STATE ACTION STABILITY,0.07346491228070176,"where {h(i)
a,a‚Ä≤}u
i=1 is a nonincreasing permutation of
Ô£±
Ô£≤ Ô£≥"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.07456140350877193,"min{W ‚àí1,t}
X"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.0756578947368421,"j=0
1i,a(st‚àíj) + min{W, t + 1} ‚àí"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.07675438596491228,"min{W ‚àí1,t}
X"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.07785087719298246,"j=0
1i,a‚Ä≤(st‚àíj) Ô£º
Ô£Ω Ô£æ u‚àí1 i=0"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.07894736842105263,"=: {hi,a,a‚Ä≤}u‚àí1
i=0 ,
(4)"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.0800438596491228,"and Œ¥a,a‚Ä≤ := na(smax{t‚àíW +1,0}:t) ‚àí(na‚Ä≤(smax{t‚àíW +1,0}:t) + 1[a‚Ä≤ < a]). Here, 1i,a(s) =
1[œÄi(s) = a] (Section 3), and W is the window size.
Remark. We defer the detailed proof to Appendix F.2. The theorem provides a per-state action
certiÔ¨Åcation for TPARL. The detailed algorithm is in Algorithm 3 (Appendix E.2). The certiÔ¨Åcation
time complexity per state is O(|A|u(W +log u)) and can be further optimized to O(|A|u log u) with
proper preÔ¨Åx sum caching across time steps. We prove the certiÔ¨Åcation for TPARL is theoretically
tight in Proposition 2 (proof in Appendix F.3). We also prove that directly extending Theorem 8 for
TPARL (Corollary 10) is loose in Appendix F.4.
Proposition 2. Under the same condition as Theorem 1, for any time step t, there exists an
RL learning algorithm M0, and a poisoned dataset eD, such that |D ‚äñeD| = Kt + 1, and
f
œÄT(smax{t‚àíW +1,0}:t) Ã∏= œÄT(smax{t‚àíW +1,0}:t)."
CERTIFICATION OF PER-STATE ACTION STABILITY,0.08114035087719298,"CertiÔ¨Åcation for DPARL. Theorem 3 provides certiÔ¨Åcation for DPARL.
Theorem 3. Let D be the clean training dataset; let œÄi = M0(Di), 0 ‚â§i ‚â§u ‚àí1 be the learned
subpolicies according to Section 4.1 from which we deÔ¨Åne na (see Section 3); and let œÄD be the
Dynamic Temporal Partition Aggregation: œÄD = M(D) where M abstracts the whole training-
aggregation process. eD is a poisoned dataset and f
œÄD is the poisoned policy: f
œÄD = M( eD)."
CERTIFICATION OF PER-STATE ACTION STABILITY,0.08223684210526316,"For a given state st encountered at time step t during test time, let a := œÄD(smax{t‚àíWmax+1,0}:t)
and W ‚Ä≤ be the chosen time window (according to Equation (1)), then tolerable poisoning threshold"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.08333333333333333,"KD
t = min
"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.08442982456140351,"Kt,
min
1‚â§W ‚àó‚â§min{Wmax,t+1},W ‚àóÃ∏=W ‚Ä≤,a‚Ä≤Ã∏=a,a‚Ä≤‚Ä≤Ã∏=a LW ‚àó,W ‚Ä≤"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.08552631578947369,"a‚Ä≤,a‚Ä≤‚Ä≤ 
(5)"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.08662280701754387,"where Kt is deÔ¨Åned by Equation (3) with W as W ‚Ä≤ and LW ‚àó,W ‚Ä≤"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.08771929824561403,"a,a‚Ä≤‚Ä≤
deÔ¨Åned by the below DeÔ¨Ånition 4."
CERTIFICATION OF PER-STATE ACTION STABILITY,0.08881578947368421,"DeÔ¨Ånition 4 (L in Theorem 3). Under the same condition as Theorem 3, for given W ‚àó, W ‚Ä≤, a, a‚Ä≤, a‚Ä≤‚Ä≤,
we let a# := arg maxa0Ã∏=a‚Ä≤,a0‚ààA na0(st‚àíW ‚àó+1:t), then"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.08991228070175439,"LW ‚àó,W ‚Ä≤"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.09100877192982457,"a‚Ä≤,a‚Ä≤‚Ä≤
:= max ( p p
X"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.09210526315789473,"i=1
g(i) + W ‚Ä≤(nW ‚àó
a‚Ä≤
‚àínW ‚àó"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.09320175438596491,"a# ) ‚àíW ‚àó(nW ‚Ä≤
a
‚àínW ‚Ä≤
a‚Ä≤‚Ä≤ ) ‚àí1[a‚Ä≤ > a] < 0 ) (6)"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.09429824561403509,"where nw
a is a shorthand of na(st‚àíw+1:t) and {g(i)}u
i=1 is a nonincreasing permutation of {gi}u‚àí1
i=0 .
Each gi is deÔ¨Åned by gi :="
CERTIFICATION OF PER-STATE ACTION STABILITY,0.09539473684210527,"max{W ‚àó,W ‚Ä≤}
X"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.09649122807017543,"w=0
max
a0‚ààA œÉw(a0) ‚àíœÉw(œÄi(st‚àíw)), where
(7)"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.09758771929824561,"œÉw(a0) := W ‚Ä≤1[a0 = a‚Ä≤, w ‚â§W ‚àó] ‚àíW ‚Ä≤1[a0 = a#, w ‚â§W ‚àó] ‚àíW ‚àó1[a0 = a, w ‚â§W ‚Ä≤] + W ‚àó1[a0 = a‚Ä≤‚Ä≤, w ‚â§W ‚Ä≤]."
CERTIFICATION OF PER-STATE ACTION STABILITY,0.09868421052631579,"Proof sketch. A successful poisoning attack should change the chosen action from a to an another
a‚Ä≤. If after attack, the chosen window size is still W ‚Ä≤, the poisoning size should be at least larger than
Kt according to Theorem 1. If the chosen window size is not W ‚Ä≤, we Ô¨Ånd out that the poisoning size
is at least mina‚Ä≤‚Ä≤Ã∏=a LW ‚àó,W ‚Ä≤"
CERTIFICATION OF PER-STATE ACTION STABILITY,0.09978070175438597,"a‚Ä≤,a‚Ä≤‚Ä≤
+1 from a greedy-based analysis. Formal proof is in Appendix F.5."
CERTIFICATION OF PER-STATE ACTION STABILITY,0.10087719298245613,"Remark. The theorem provides a valid per-state action certiÔ¨Åcation for DPARL policy. The de-
tailed algorithm is in Algorithm 4 (Appendix E.2). The certiÔ¨Åcation time complexity per state is
O(W 2
max|A|2u+Wmax|A|2u log u), which in practice adds similar overhead compared with TPARL
certiÔ¨Åcation (see Appendix H.3). Unlike certiÔ¨Åcation for PARL and TPARL, the certiÔ¨Åcation given
by Theorem 3 is not theoretically tight. An interesting future work would be providing a tighter
per-state action certiÔ¨Åcation for DPARL."
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.10197368421052631,"4.4
CERTIFICATION OF CUMULATIVE REWARD BOUND
In this section, we present our robustness certiÔ¨Åcation for cumulative reward bound. We assume the
deterministic RL environment throughout the cumulative reward certiÔ¨Åcation for convenience, i.e.,
the transition function is P : S√óA ‚ÜíS and the initial state is a Ô¨Åxed s0 ‚ààS. The certiÔ¨Åcation goal,
as listed in DeÔ¨Ånition 6, is to obtain a lower bound of cumulative reward under poisoning attacks,
given bounded poisoning size K. The cumulative reward certiÔ¨Åcation is based on a novel adaptive
search algorithm COPA-SEARCH inspired from Wu et al. (2022); we tailor the algorithm to certify
against poisoning attacks. We defer detailed discussions and complexity analysis to Appendix E.3."
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.10307017543859649,Published as a conference paper at ICLR 2022
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.10416666666666667,"COPA-SEARCH Algorithm Description.
The pseudocode is in Algorithm 5 (Appendix E.3).
The method starts from the base case: when the poisoning threshold Kcur = 0, the lower bound
of cumulative reward JKcur is exactly the reward without poisoning. The method then gradually
increases the poisoning threshold Kcur, by Ô¨Ånding the immediate larger K‚Ä≤ > Kcur that can expand
the possible action set along the trajectory. With the increase of Kcur ‚ÜêK‚Ä≤, the attack may cause
the poisoned policy ÀúœÄ to take different actions at some states, thus resulting in new trajectories. We
need to Ô¨Ågure out a set of all possible actions to exhaustively traverse all possible trajectories. We
will introduce theorems to compute this set of possible actions. With this set, the method effectively
explores these new trajectories by formulating them as expanded branches of a trajectory tree. Once
all new trajectories are explored, the method examines all leaf nodes of the tree and Ô¨Ågures out the
minimum reward among them, which is the new lower bound of cumulative reward JK‚Ä≤ under new
poisoning size K‚Ä≤. We then repeat this process of increasing poisoning size from K‚Ä≤ and expanding
with new trajectories until we reach a predeÔ¨Åned threshold for poisoning size K.
DeÔ¨Ånition 5 (Possible Action Set). Given previous states s0:t, the subpolicies {œÄi}u‚àí1
i=0 , the aggre-
gation protocol (PARL, TPARL, or DPARL), and the poisoning size K, the possible action set A
at step t is a subset of action space: A ‚äÜA, such that for any poisoned policy eœÄ, as long as the
poisoning size is within K, the chosen action at step t will always be in A, i.e., at = eœÄ(s0:t) ‚ààA."
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.10526315789473684,"Possible Action Set for PARL.
The following theorem gives the possible action set for PARL.
Theorem 4 (Tight PARL Action Set). Under the condition of DeÔ¨Ånition 5, suppose the aggregation
protocol is PARL as deÔ¨Åned in DeÔ¨Ånition 7, then the possible action set at step t"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.10635964912280702,AT (K) = (
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.1074561403508772,"a ‚ààA

X"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.10855263157894737,"a‚Ä≤‚ààA
max{na‚Ä≤(st) ‚àína(st) ‚àíK + 1[a‚Ä≤ < a], 0} ‚â§K ) .
(8)"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.10964912280701754,"We defer the proof to Appendix F.6.1. Furthermore, in Appendix F.6.2 we show that: 1) The theo-
rem gives theoretically tight possible action set; 2) In contrast, directly extending PARL‚Äôs per-state
certiÔ¨Åcation gives loose certiÔ¨Åcation."
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.11074561403508772,"Possible Action Set for TPARL.
The following theorem gives the possible action set for TPARL.
Theorem 5. Under the condition of DeÔ¨Ånition 5, suppose the aggregation protocol is TPARL as
deÔ¨Åned in DeÔ¨Ånition 8, then the possible action set at step t"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.1118421052631579,"A(K) = ( a ‚ààA K
X"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.11293859649122807,"i=1
h(i)
a‚Ä≤,a > Œ¥a‚Ä≤,a, ‚àÄa‚Ä≤ Ã∏= a ) ,
(9)"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.11403508771929824,"where h(i)
a‚Ä≤,a and Œ¥a‚Ä≤,a follow the deÔ¨Ånition in Theorem 1."
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.11513157894736842,"We defer the proof to Appendix F.7. The possible action set here is no longer theoretically tight.
Indeed, the problem of computing a possible action set with minimum cardinality for TPARL is
NP-complete as we shown in the following theorem (proved in Appendix F.8), where we reduce
computing theoretically tight possible action set to the set cover problem (Karp, 1972). This result
can be viewed as the hardness of targeted attack. In other words, the optimal untargeted attack
on TPARL can be found in polynomial time, while the optimal targeted attack on TPARL is NP-
complete, which indicates the robustness property of proposed TPARL.
Theorem 6. Under the condition of DeÔ¨Ånition 5, suppose we use TPARL (DeÔ¨Ånition 8) as the ag-
gregation protocol, then computing a possible action set A(K) such that any possible action set S
satisÔ¨Åes |A(K)| ‚â§|S| is NP-complete.
Possible Action Set for DPARL.
The following theorem gives the possible action set for DPARL.
Theorem 7. Under the condition of DeÔ¨Ånition 5, suppose the aggregation protocol is TPARL as
deÔ¨Åned in DeÔ¨Ånition 3, then the possible action set at step t"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.1162280701754386,"A(K) = {at} ‚à™ Ô£±
Ô£¥
Ô£≤"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.11732456140350878,"Ô£¥
Ô£≥
a‚Ä≤ ‚ààA

min
1‚â§W ‚àó‚â§min{Wmax,t+1},
W ‚àóÃ∏=W ‚Ä≤,a‚Ä≤‚Ä≤Ã∏=at"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.11842105263157894,"LW ‚àó,W ‚Ä≤"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.11951754385964912,"a‚Ä≤,a‚Ä≤‚Ä≤
‚â§K Ô£º
Ô£¥
Ô£Ω Ô£¥
Ô£æ
‚à™ ( a ‚ààA K
X"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.1206140350877193,"i=1
h(i)
a‚Ä≤,a > Œ¥a‚Ä≤,a, ‚àÄa‚Ä≤ Ã∏= a ) (10)"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.12171052631578948,"where at = œÄD(smax{t‚àíWmax+1,0}:t) is the clean policy‚Äôs chosen action, W ‚Ä≤ is deÔ¨Åned by Equa-"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.12280701754385964,"tion (1), LW ‚àó,W ‚Ä≤"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.12390350877192982,"a‚Ä≤,a‚Ä≤‚Ä≤
is deÔ¨Åned by DeÔ¨Ånition 4 with a being replaced by at, and h(i)
a‚Ä≤,a, Œ¥a‚Ä≤,a is deÔ¨Åned in
Theorem 1 with W replaced by W ‚Ä≤."
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.125,"We prove the theorem in Appendix F.8. As summarized in Table 1, we further prove the tightness or
hardness of certiÔ¨Åcation for PARL and TPARL, while for DPARL it is an interesting open problem
on whether theoretical tight certiÔ¨Åcation is possible in polynomial time."
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.12609649122807018,Published as a conference paper at ICLR 2022
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.12719298245614036,"PARL
TPARL (W = 4)
DPARL (Wmax = 5)"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.12828947368421054,"Freeway, u = 50
Breakout, u = 50
Highway, u = 50"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.12938596491228072,"DQN
stability ratio"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.13048245614035087,"0
2
4
6
8 10 12 14 16 18 20 22 24
0.0 0.2 0.4 0.6 0.8 1.0"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.13157894736842105,"0
2
4
6
8 10 12 14 16 18 20 22 24
0.0
0.2
0.4
0.6
0.8
1.0"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.13267543859649122,"0
2
4
6
8 10 12 14 16 18 20 22 24
0.0
0.2
0.4
0.6
0.8
1.0"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.1337719298245614,"QR-DQN
stability ratio"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.13486842105263158,"0
2
4
6
8 10 12 14 16 18 20 22 24
0.0 0.2 0.4 0.6 0.8 1.0"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.13596491228070176,"0
2
4
6
8 10 12 14 16 18 20 22 24
0.0
0.2
0.4
0.6
0.8
1.0"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.13706140350877194,"0
2
4
6
8 10 12 14 16 18 20 22 24
0.0
0.2
0.4
0.6
0.8
1.0"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.13815789473684212,"C51
stability ratio"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.13925438596491227,"0
2
4
6
8 10 12 14 16 18 20 22 24
0.0 0.2 0.4 0.6 0.8 1.0"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.14035087719298245,"0
2
4
6
8 10 12 14 16 18 20 22 24
0.0
0.2
0.4
0.6
0.8
1.0"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.14144736842105263,"0
2
4
6
8 10 12 14 16 18 20 22 24
0.0
0.2
0.4
0.6
0.8
1.0"
CERTIFICATION OF CUMULATIVE REWARD BOUND,0.1425438596491228,"‚â•K
‚â•K
‚â•K
Figure 1: Robustness certiÔ¨Åcation for per-state action stability. We plot the cumulative histogram of the
tolerable poisoning size K for all time steps. We provide the certiÔ¨Åcation for different aggregation protocols
(PARL, TPARL, DPARL) on three environments and #partitions u = 50. The results are averaged over 20 runs
with the vertical bar on top denoting the standard deviation."
EXPERIMENTS,0.14364035087719298,"5
EXPERIMENTS"
EXPERIMENTS,0.14473684210526316,"In this section, we present the evaluation for our COPA framework, speciÔ¨Åcally, the aggregation
protocols (Section 4.2) and the certiÔ¨Åcation methods under different certiÔ¨Åcation criteria (Sec-
tions 4.3 and 4.4). We defer the description of the ofÔ¨Çine RL algorithms (DQN (Mnih et al., 2013),
QR-DQN (Dabney et al., 2018), and C51 (Bellemare et al., 2017)) used for training the subpolicies
to Appendix G.1, and the concrete experimental procedures to Appendix G.2. As a summary, we
obtain similar conclusions from per-state action certiÔ¨Åcation and reward certiÔ¨Åcation: 1) QR-DQN
and C51 are oftentimes more certiÔ¨Åably robust than DQN; 2) temporal aggregation (TPARL
and DPARL) achieves higher certiÔ¨Åcation for environments satisfying temporal continuity, e.g.,
Freeway; 3) larger partition number improves the certiÔ¨Åed robustness; 4) Freeway is the most stable
and robust environment among the three. More interesting discussions are deferred to Appendix H."
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.14583333333333334,"5.1
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.14692982456140352,We provide the robustness certiÔ¨Åcation for per-state action stability based on Section 4.3.
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.14802631578947367,"Experimental Setup and Metrics.
We evaluate the aggregated policies œÄP, œÄT, and œÄD following
Section 4.2. Basically, in each run, we run one trajectory (of maximum length H) using the derived
policy, and compute Kt at each time step t. Given {Kt}H‚àí1
t=0 , we obtain a cumulative histogram‚Äî
for each threshold K, we count the time steps that achieve a threshold no smaller than it and then
normalize, i.e.,
PH‚àí1
t=0 1[Kt‚â•K]/H. We call this quantity stability ratio since it reÔ¨Çects the per-state
action stability w.r.t. given poisoning thresholds. We also compute an average tolerable poisoning
thresholds for a trajectory, deÔ¨Åned as
PH‚àí1
t=0 Kt/H. More details are deferred to Appendix G.2."
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.14912280701754385,"Evaluation Results.
We present the comparison of per-state action certiÔ¨Åcation for different RL
methods and certiÔ¨Åcation methods in Figure 1. We plot partial poisoning thresholds on the x-axes
here, and omit full results in Appendix H.6, where we also report the average tolerable poisoning
thresholds. We additionally report benign empirical reward and the comparisons with standard
training in Appendices H.1 and H.2, as well as more analytical statistics in Appendices H.3 and H.4."
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.15021929824561403,"The cumulative histograms in Figure 1 can be compared in different levels. Basically, we compare
the stability ratio at each tolerable poisoning thresholds K‚Äîhigher ratio at larger poisoning size
indicates stronger certiÔ¨Åed robustness. On the RL algorithm level, QR-DQN and C51 consistently
outperform the baseline DQN, and C51 has a substantial advantage particularly in Highway. On the
aggregation protocol level, we observe different behaviors in different environments. On Freeway,
methods with temporal aggregation (TPARL and DPARL) achieve higher robustness, and DPARL
achieves the highest certiÔ¨Åed robustness in most cases; while on Breakout and Highway, the single-
step aggregation PARL is oftentimes better. This difference is due to the different properties of
environments. Our temporal aggregation is developed based on the assumption of consistent action"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.1513157894736842,Published as a conference paper at ICLR 2022
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.1524122807017544,"u = 30, PARL
u = 50, PARL"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.15350877192982457,"u = 30, TPARL (W = 4)
u = 50, TPARL (W = 4)"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.15460526315789475,"u = 30, DPARL (Wmax = 5)
u = 50, DPARL (Wmax = 5)"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.15570175438596492,"Freeway, H = 200
Freeway, H = 400
Breakout, H = 50
Breakout, H = 75
Highway, H = 30"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.15679824561403508,"DQN
JK"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.15789473684210525,"0
5
10
15
20
25
0 1 2 3"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.15899122807017543,"0
5
10
15
20
25
0 1 2 3 4 5"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.1600877192982456,"0
3
6
9 12 15 18 21
0 1"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.1611842105263158,"0
5
10
15
20
25
0 1 2"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.16228070175438597,"0
5
10
15
20
25 5 10 15 20 25"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.16337719298245615,"QR-DQN
JK"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.16447368421052633,"0
5
10
15
20
25
0 1 2 3"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.16557017543859648,"0
5
10
15
20
25
0 1 2 3 4 5"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.16666666666666666,"0
3
6
9 12 15 18 21 24
0 1"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.16776315789473684,"0
5
10
15
20
25
0 1 2"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.16885964912280702,"0
5
10
15
20
25 5 10 15 20 25"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.1699561403508772,"C51
JK"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.17105263157894737,"0
5
10
15
20
25
0 1 2 3"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.17214912280701755,"0
5
10
15
20
25
0 1 2 3 4 5"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.17324561403508773,"0
3
6
9 12 15 18 21 24 27
0 1"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.17434210526315788,"0
5
10
15
20
25
0 1 2"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.17543859649122806,"0
5
10
15
20
25 5 10 15 20 25"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.17653508771929824,"Poisoning size K
Poisoning size K
Poisoning size K
Poisoning size K
Poisoning size K
Figure 2: Robustness certiÔ¨Åcation for cumulative reward. We plot the lower bound of cumulative reward
bound JK w.r.t. poisoning size K under three aggregation protocols (PARL, TPARL (W = 4), DPARL
(Wmax = 5)) with two #partitions u, evaluated on three environments with different horizon lengths H."
EVALUATION OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY,0.17763157894736842,"selection in adjacent time steps. This assumption is true in Freeway while violated in Breakout and
Highway. A more detailed explanation of environment properties is omitted to Appendix H.5. On
the partition number level, a larger partition number generally allows larger tolerable poisoning
thresholds as shown in Appendix H.6. Finally, on the RL environment level, Freeway achieves
much higher certiÔ¨Åed robustness for per-state action stability than Highway, followed by Breakout,
implying that Freeway is an environment that accommodates more stable and robust policies."
EVALUATION OF ROBUSTNESS CERTIFICATION FOR CUMULATIVE REWARD BOUND,0.1787280701754386,"5.2
EVALUATION OF ROBUSTNESS CERTIFICATION FOR CUMULATIVE REWARD BOUND"
EVALUATION OF ROBUSTNESS CERTIFICATION FOR CUMULATIVE REWARD BOUND,0.17982456140350878,We provide the robustness certiÔ¨Åcation for cumulative reward bound according to Section 4.4.
EVALUATION OF ROBUSTNESS CERTIFICATION FOR CUMULATIVE REWARD BOUND,0.18092105263157895,"Experimental Setup and Metrics.
We evaluate the aggregated policies œÄP, œÄT, and œÄD follow-
ing Theorem 4, Theorem 5 and Theorem 7. We compute the lower bounds of the cumulative reward
JK w.r.t. the poisoning size K using the COPA-SEARCH algorithm introduced in Section 4.4. We
provide details of the evaluated trajectory length along with the rationales in Appendix G.2."
EVALUATION OF ROBUSTNESS CERTIFICATION FOR CUMULATIVE REWARD BOUND,0.18201754385964913,"Evaluation Results.
We present the comparison of reward certiÔ¨Åcation for different RL algorithms
and certiÔ¨Åcation methods in Figure 2. Essentially, at each poisoning size K, we compare the lower
bound of cumulative reward achieved by different RL algorithms and certiÔ¨Åcation methods‚Äîhigher
value of the lower bound implies stronger certiÔ¨Åed robustness. On the RL algorithm level, QR-
DQN and C51 almost invariably outperform the baseline DQN algorithm. On the aggregation
protocol level, methods with temporal aggregation consistently surpass the single-step aggregation
PARL on Freeway but not the other two, as analyzed in Section 5.1. In addition, we note that DPARL
is sometimes not as robust as TPARL. We hypothesize two reasons: 1) the dynamic mechanism is
more susceptible to the attack, e.g., the selected optimal window size is prone to be manipulated;
2) the lower bound is looser for DPARL given the difÔ¨Åculty of computing the possible action set
in DPARL (discussed in Theorem 7). On the partition number level, a larger partition number
(u = 50) demonstrates higher robustness. On the horizon length level, the robustness ranking of
different policies is similar under different horizon lengths with slight differences, corresponding
to the property of Ô¨Ånite-horizon RL. On the RL environment level, Freeway can tolerate a larger
poisoning size than Breakout and Highway. More results and discussions are in Appendix H.7."
CONCLUSIONS,0.18311403508771928,"6
CONCLUSIONS"
CONCLUSIONS,0.18421052631578946,"In this paper, we proposed COPA, the Ô¨Årst framework for certifying robust policies for ofÔ¨Çine RL
against poisoning attacks. COPA includes three policy aggregation protocols. For each aggregation
protocol, COPA provides a sound certiÔ¨Åcation for both per-state action stability and cumulative
reward bound. Experimental evaluations on different environments and different ofÔ¨Çine RL training
algorithms show the effectiveness of our robustness certiÔ¨Åcation in a wide range of scenarios."
CONCLUSIONS,0.18530701754385964,Published as a conference paper at ICLR 2022
CONCLUSIONS,0.18640350877192982,ACKNOWLEDGMENTS
CONCLUSIONS,0.1875,"This work is partially supported by the NSF grant No.1910100, NSF CNS 20-46726 CAR, Alfred P.
Sloan Fellowship, the U.S. Department of Energy by the Lawrence Livermore National Laboratory
under Contract No. DE-AC52-07NA27344 and LLNL LDRD Program Project No. 20-ER-014, and
Amazon Research Award."
ETHICS STATEMENT,0.18859649122807018,"Ethics Statement.
In this paper, we prove the Ô¨Årst framework for certifying robust policies for
ofÔ¨Çine RL against poisoning attacks. On the one hand, such framework provides rigorous guaran-
tees in practice RL tasks and thus signiÔ¨Åcantly alleviates the security vulnerabilities of ofÔ¨Çine RL
algorithms against training-phase poisoning attacks. The evaluation of different RL algorithms also
provides a better understanding about the different degrees of security vulnerabilities across differ-
ent RL algorithms. On the other hand, the robustness guarantee provided by our framework only
holds under speciÔ¨Åc conditions of the attack. SpeciÔ¨Åcally, we require the attack to change only a
bounded number of training trajectories. Therefore, users should be aware of such limitations of
our framework, and should not blindly trust the robustness guarantee when the attack can change a
large number of training instances or modify the training algorithm itself. As a result, we encourage
researchers to understand the potential risks, and evaluate whether our constraints on the attack align
with their usage scenarios when applying our COPA to real-world applications. We do not expect
any ethics issues raised by our work."
REPRODUCIBILITY STATEMENT,0.18969298245614036,"Reproducibility Statement.
All theorem statements are substantiated with rigorous proofs in Ap-
pendix F. In Appendix E, we list the pseudocode for all key algorithms. Our experimental eval-
uation is conducted with publicly available OpenAI Gym toolkit (Brockman et al., 2016). We
introduce all experimental details in both Section 5 and Appendix G. SpeciÔ¨Åcally, we build the
code upon the open-source code base of Agarwal et al. (2020), and we upload the source code at
https://github.com/AI-secure/COPA for reproducibility purpose."
REFERENCES,0.19078947368421054,REFERENCES
REFERENCES,0.19188596491228072,"Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on ofÔ¨Çine
reinforcement learning. In International Conference on Machine Learning, 2020."
REFERENCES,0.19298245614035087,"Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron,
Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, et al. Solving rubik‚Äôs cube with a
robot hand. arXiv preprint arXiv:1910.07113, 2019."
REFERENCES,0.19407894736842105,"Kiarash Banihashem, Adish Singla, and Goran Radanovic. Defense against reward poisoning attacks
in reinforcement learning. arXiv preprint arXiv:2102.05776, 2021."
REFERENCES,0.19517543859649122,"Vahid Behzadan and Arslan Munir. Whatever does not kill deep reinforcement learning, makes it
stronger. arXiv preprint arXiv:1712.09344, 2017."
REFERENCES,0.1962719298245614,"Vahid Behzadan and Arslan Munir. Mitigation of policy manipulation attacks on deep q-networks
with parameter-space noise. In International Conference on Computer Safety, Reliability, and
Security, pp. 406‚Äì417. Springer, 2018."
REFERENCES,0.19736842105263158,"Marc G Bellemare, Will Dabney, and R¬¥emi Munos. A distributional perspective on reinforcement
learning. In International Conference on Machine Learning, pp. 449‚Äì458. PMLR, 2017."
REFERENCES,0.19846491228070176,"Richard Bellman. Dynamic programming. Science, 153(3731):34‚Äì37, 1966."
REFERENCES,0.19956140350877194,"Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016."
REFERENCES,0.20065789473684212,"Henry Chacon, Samuel Silva, and Paul Rad. Deep learning poison data attack detection. In 2019
IEEE 31st International Conference on Tools with ArtiÔ¨Åcial Intelligence (ICTAI), pp. 971‚Äì978.
IEEE, 2019."
REFERENCES,0.20175438596491227,"Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. CertiÔ¨Åed adversarial robustness via randomized
smoothing. In International Conference on Machine Learning, pp. 1310‚Äì1320. PMLR, 2019."
REFERENCES,0.20285087719298245,Published as a conference paper at ICLR 2022
REFERENCES,0.20394736842105263,"Will Dabney, Mark Rowland, Marc G Bellemare, and R¬¥emi Munos. Distributional reinforcement
learning with quantile regression. In Thirty-Second AAAI Conference on ArtiÔ¨Åcial Intelligence,
2018."
REFERENCES,0.2050438596491228,"Marc Peter Deisenroth, Gerhard Neumann, Jan Peters, et al. A survey on policy search for robotics.
Foundations and trends in Robotics, 2(1-2):388‚Äì403, 2013."
REFERENCES,0.20614035087719298,"Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Ankur Moitra, and Alistair Stewart.
Robust estimators in high dimensions without the computational intractability. In 57th Annual
Symposium on Foundations of Computer Science, pp. 655‚Äì664. Institute of Electrical and Elec-
tronics Engineers (IEEE), 2016."
REFERENCES,0.20723684210526316,"Michael Everett, Bj¬®orn L¬®utjens, and Jonathan P How. CertiÔ¨Åable robustness to adversarial state
uncertainty in deep reinforcement learning. IEEE Transactions on Neural Networks and Learning
Systems, 2021."
REFERENCES,0.20833333333333334,"Marc Fischer, Matthew Mirman, Steven Stalder, and Martin Vechev. Online robustness training for
deep reinforcement learning. arXiv preprint arXiv:1911.00887, 2019."
REFERENCES,0.20942982456140352,"Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves,
Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, et al. Noisy networks for exploration.
arXiv preprint arXiv:1706.10295, 2017."
REFERENCES,0.21052631578947367,"Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Ue-
sato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval
bound propagation for training veriÔ¨Åably robust models. arXiv preprint arXiv:1810.12715, 2018."
REFERENCES,0.21162280701754385,"IEEE Computer Society. Standards Committee. Working group of the Microprocessor Stan-
dards Subcommittee and American National Standards Institute.
IEEE standard for binary
Ô¨Çoating-point arithmetic, volume 754. IEEE, 1985."
REFERENCES,0.21271929824561403,"Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adver-
sarial examples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 15262‚Äì15271, 2021."
REFERENCES,0.2138157894736842,"Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks
on neural network policies. arXiv preprint arXiv:1702.02284, 2017."
REFERENCES,0.2149122807017544,"Yunhan Huang and Quanyan Zhu. Deceptive reinforcement learning under adversarial manipula-
tions on cost signals. In International Conference on Decision and Game Theory for Security, pp.
217‚Äì237. Springer, 2019."
REFERENCES,0.21600877192982457,"Jinyuan Jia, Xiaoyu Cao, and Neil Zhenqiang Gong.
CertiÔ¨Åed robustness of nearest neighbors
against data poisoning attacks. arXiv preprint arXiv:2012.03765, 2020."
REFERENCES,0.21710526315789475,"Richard M Karp. Reducibility among combinatorial problems. In Complexity of computer compu-
tations, pp. 85‚Äì103. Springer, 1972."
REFERENCES,0.21820175438596492,"Panagiota Kiourti, Kacper Wardega, Susmit Jha, and Wenchao Li. Trojdrl: evaluation of backdoor
attacks on deep reinforcement learning. In 2020 57th ACM/IEEE Design Automation Conference
(DAC), pp. 1‚Äì6. IEEE, 2020."
REFERENCES,0.21929824561403508,"Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The
International Journal of Robotics Research, 32(11):1238‚Äì1274, 2013."
REFERENCES,0.22039473684210525,"Jernej Kos and Dawn Song.
Delving into adversarial attacks on deep policies.
arXiv preprint
arXiv:1705.06452, 2017."
REFERENCES,0.22149122807017543,"Ram Shankar Siva Kumar, Magnus Nystr¬®om, John Lambert, Andrew Marshall, Mario Goertzel,
Andi Comissoneru, Matt Swann, and Sharon Xia. Adversarial machine learning-industry per-
spectives. In 2020 IEEE Security and Privacy Workshops (SPW), pp. 69‚Äì75. IEEE, 2020."
REFERENCES,0.2225877192982456,"Yann LeCun, L¬¥eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278‚Äì2324, 1998."
REFERENCES,0.2236842105263158,Published as a conference paper at ICLR 2022
REFERENCES,0.22478070175438597,"Robert Legenstein, Niko Wilbert, and Laurenz Wiskott. Reinforcement learning on slow features of
high-dimensional input streams. PLoS computational biology, 6(8):e1000894, 2010."
REFERENCES,0.22587719298245615,"Edouard Leurent. An environment for autonomous driving decision-making. https://github.
com/eleurent/highway-env, 2018."
REFERENCES,0.22697368421052633,"Alexander Levine and Soheil Feizi. Deep partition aggregation: Provable defenses against general
poisoning attacks. In International Conference on Learning Representations, 2020."
REFERENCES,0.22807017543859648,"Alexander Levine and Soheil Feizi. Deep partition aggregation: Provable defenses against gen-
eral poisoning attacks. In International Conference on Learning Representations, 2021. URL
https://openreview.net/forum?id=YUGG2tFuPM."
REFERENCES,0.22916666666666666,"Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. OfÔ¨Çine reinforcement learning: Tuto-
rial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020."
REFERENCES,0.23026315789473684,"Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg.
Fine-pruning: Defending against back-
dooring attacks on deep neural networks. In International Symposium on Research in Attacks,
Intrusions, and Defenses, pp. 273‚Äì294. Springer, 2018."
REFERENCES,0.23135964912280702,"Yuzhe Ma, Xuezhou Zhang, Wen Sun, and Xiaojin Zhu. Policy poisoning in batch reinforcement
learning and control. Advances in Neural Information Processing Systems, 2019."
REFERENCES,0.2324561403508772,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017."
REFERENCES,0.23355263157894737,"Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for prov-
ably robust neural networks. In International Conference on Machine Learning, pp. 3578‚Äì3586.
PMLR, 2018."
REFERENCES,0.23464912280701755,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller.
Playing atari with deep reinforcement learning.
arXiv preprint
arXiv:1312.5602, 2013."
REFERENCES,0.23574561403508773,"Blaine Nelson, Marco Barreno, Fuching Jack Chi, Anthony D Joseph, Benjamin IP Rubinstein,
Udam Saini, Charles Sutton, J Doug Tygar, and Kai Xia. Exploiting machine learning to subvert
your spam Ô¨Ålter. LEET, 8:1‚Äì9, 2008."
REFERENCES,0.23684210526315788,"Tuomas Oikarinen, Tsui-Wei Weng, and Luca Daniel. Robust deep reinforcement learning through
adversarial loss. arXiv preprint arXiv:2008.01976, 2020."
REFERENCES,0.23793859649122806,"Anay Pattanaik, Zhenyi Tang, Shuijing Liu, Gautham Bommannan, and Girish Chowdhary. Ro-
bust deep reinforcement learning with adversarial attacks. In 17th International Conference on
Autonomous Agents and Multiagent Systems, AAMAS 2018, pp. 2040‚Äì2042. International Foun-
dation for Autonomous Agents and Multiagent Systems (IFAAMAS), 2018."
REFERENCES,0.23903508771929824,"Neehar Peri, Neal Gupta, W Ronny Huang, Liam Fowl, Chen Zhu, Soheil Feizi, Tom Goldstein, and
John P Dickerson. Deep k-nn defense against clean-label data poisoning attacks. In European
Conference on Computer Vision, pp. 55‚Äì70. Springer, 2020."
REFERENCES,0.24013157894736842,"Athanasios S Polydoros and Lazaros Nalpantidis. Survey of model-based reinforcement learning:
Applications on robotics. Journal of Intelligent & Robotic Systems, 86(2):153‚Äì173, 2017."
REFERENCES,0.2412280701754386,"Amin Rakhsha, Goran Radanovic, Rati Devidze, Xiaojin Zhu, and Adish Singla. Policy teaching
via environment poisoning: Training-time adversarial attacks against reinforcement learning. In
International Conference on Machine Learning, pp. 7974‚Äì7984. PMLR, 2020."
REFERENCES,0.24232456140350878,"Ahmad EL Sallab, Mohammed Abdou, Etienne Perot, and Senthil Yogamani. Deep reinforcement
learning framework for autonomous driving. Electronic Imaging, 2017(19):70‚Äì76, 2017."
REFERENCES,0.24342105263157895,"Hadi Salman, Greg Yang, Jerry Li, Pengchuan Zhang, Huan Zhang, Ilya Razenshteyn, and S¬¥ebastien
Bubeck. Provably robust deep learning via adversarially trained smoothed classiÔ¨Åers. In Pro-
ceedings of the 33rd International Conference on Neural Information Processing Systems, pp.
11292‚Äì11303, 2019."
REFERENCES,0.24451754385964913,Published as a conference paper at ICLR 2022
REFERENCES,0.24561403508771928,"Avi Schwarzschild, Micah Goldblum, Arjun Gupta, John P Dickerson, and Tom Goldstein. Just
how toxic is data poisoning? a uniÔ¨Åed benchmark for backdoor and data poisoning attacks. In
International Conference on Machine Learning, pp. 9389‚Äì9398. PMLR, 2021."
REFERENCES,0.24671052631578946,"Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement
learning for autonomous driving. arXiv preprint arXiv:1610.03295, 2016."
REFERENCES,0.24780701754385964,"Jacob Steinhardt, Pang Wei Koh, and Percy Liang. CertiÔ¨Åed defenses for data poisoning attacks. In
Proceedings of the 31st International Conference on Neural Information Processing Systems, pp.
3520‚Äì3532, 2017."
REFERENCES,0.24890350877192982,"Yanchao Sun, Da Huo, and Furong Huang. Vulnerability-aware poisoning mechanism for online
{rl} with unknown dynamics. In International Conference on Learning Representations, 2021.
URL https://openreview.net/forum?id=9r30XCjf5Dt."
REFERENCES,0.25,"Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Do-
main randomization for transferring deep neural networks from simulation to the real world. In
2017 IEEE/RSJ international conference on intelligent robots and systems (IROS), pp. 23‚Äì30.
IEEE, 2017."
REFERENCES,0.25109649122807015,"Rishi Veerapaneni, John D Co-Reyes, Michael Chang, Michael Janner, Chelsea Finn, Jiajun Wu,
Joshua Tenenbaum, and Sergey Levine. Entity abstraction in visual model-based reinforcement
learning. In Conference on Robot Learning, pp. 1439‚Äì1456. PMLR, 2020."
REFERENCES,0.25219298245614036,"Lun Wang, Zaynah Javed, Xian Wu, Wenbo Guo, Xinyu Xing, and Dawn Song. Backdoorl: Back-
door attack against competitive reinforcement learning. arXiv preprint arXiv:2105.00579, 2021."
REFERENCES,0.2532894736842105,"Yue Wang, Esha Sarkar, Wenqing Li, Michail Maniatakos, and Saif Eddin Jabari. Stop-and-go: Ex-
ploring backdoor attacks on deep reinforcement learning-based trafÔ¨Åc congestion control systems.
arXiv preprint arXiv:2003.07859, 2020."
REFERENCES,0.2543859649122807,"Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279‚Äì292, 1992."
REFERENCES,0.25548245614035087,"Maurice Weber, Xiaojun Xu, Bojan Karlas, Ce Zhang, and Bo Li. Rab: Provable robustness against
backdoor attacks. arXiv preprint arXiv:2003.08904, 2020."
REFERENCES,0.2565789473684211,"Lily Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane Boning,
and Inderjit Dhillon. Towards fast computation of certiÔ¨Åed robustness for relu networks. In
International Conference on Machine Learning, pp. 5276‚Äì5285. PMLR, 2018."
REFERENCES,0.2576754385964912,"Fan Wu, Linyi Li, Zijian Huang, Yevgeniy Vorobeychik, Ding Zhao, and Bo Li. Crop: Certify-
ing robust policies for reinforcement learning through functional smoothing. In International
Conference on Learning Representations, 2022."
REFERENCES,0.25877192982456143,"Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, and Yang Gao. Mastering atari games
with limited data. Advances in Neural Information Processing Systems, 34, 2021."
REFERENCES,0.2598684210526316,"Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Mingyan Liu, Duane Boning, and Cho-Jui
Hsieh.
Robust deep reinforcement learning against adversarial perturbations on state obser-
vations.
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-
vances in Neural Information Processing Systems, volume 33, pp. 21024‚Äì21037. Curran Asso-
ciates, Inc., 2020a. URL https://proceedings.neurips.cc/paper/2020/file/
f0eb6568ea114ba6e293f903c34d7488-Paper.pdf."
REFERENCES,0.26096491228070173,"Xuezhou Zhang, Yuzhe Ma, Adish Singla, and Xiaojin Zhu. Adaptive reward-poisoning attacks
against reinforcement learning. In International Conference on Machine Learning, pp. 11225‚Äì
11234. PMLR, 2020b."
REFERENCES,0.26206140350877194,"Xuezhou Zhang, Yiding Chen, Jerry Zhu, and Wen Sun. Corruption-robust ofÔ¨Çine reinforcement
learning. arXiv preprint arXiv:2106.06630, 2021."
REFERENCES,0.2631578947368421,Published as a conference paper at ICLR 2022
REFERENCES,0.2642543859649123,"A
OMITTED DEFINITIONS"
REFERENCES,0.26535087719298245,"A.1
CUMULATIVE REWARD"
REFERENCES,0.26644736842105265,"DeÔ¨Ånition 6 (Cumulative Reward). Let P : S √ó A ‚ÜíP(S) be the transition function with P(¬∑)
deÔ¨Åning the set of probability measures. Let R, d0, H be the reward function, initial state distribu-
tion, and time horizon. We denote J(œÄ) as the cumulative reward of the given policy œÄ:"
REFERENCES,0.2675438596491228,"J(œÄ) := E H‚àí1
X"
REFERENCES,0.26864035087719296,"t=0
R(st, at), where at = œÄ(st), st+1 ‚àºP(st, at), s0 ‚àºd0,
(11)"
REFERENCES,0.26973684210526316,"It is natural to adapt this deÔ¨Ånition when there is a discount factor. If policy œÄ considers recent nt
states instead of only st to make action predictions, we can change at = œÄ(st) to at = œÄ(st‚àínt+1:t)
in Equation (11)."
REFERENCES,0.2708333333333333,"A.2
PARL (PER-STATE PARTITION AGGREGATION) PROTOCOL"
REFERENCES,0.2719298245614035,"DeÔ¨Ånition 7 (Per-State Partition Aggregation). Given subpolicies {œÄi}u‚àí1
i=0 , the Per-State Partition
Aggregation (PARL) protocol deÔ¨Ånes an aggregated policy œÄP : S ‚ÜíA such that
œÄP(s) := arg max
a‚ààA
na(s),
(12)"
REFERENCES,0.2730263157894737,where na(s) is deÔ¨Åned in Section 3.
REFERENCES,0.2741228070175439,"As mentioned in Section 4, when the arg max in Equation (12) returns multiple elements, we break
ties deterministically by returning the ‚Äúsmaller‚Äù (<) action, which can be deÔ¨Åned by numerical
order, lexicographical order, etc."
REFERENCES,0.27521929824561403,"A.3
TPARL (TEMPORAL PARTITION AGGREGATION) PROTOCOL"
REFERENCES,0.27631578947368424,"DeÔ¨Ånition 8 (Temporal Partition Aggregation). Given subpolicies {œÄi}u‚àí1
i=0 and window size W,
at time step t, the Temporal Partition Aggregation (TPARL) deÔ¨Ånes an aggregated policy œÄT :
Smin{t+1,W } ‚ÜíA such that
œÄT(smax{t‚àíW +1,0}:t) = arg max
a‚ààA
na(smax{t‚àíW +1,0}:t),
(13)"
REFERENCES,0.2774122807017544,where sl:r and na are deÔ¨Åned in Section 3.
REFERENCES,0.27850877192982454,"In the above deÔ¨Ånition, at time step t, the input of policy œÄT contains all states between time step
max{t ‚àíW + 1, 0} and t within window size W; and the output is the policy prediction with the
highest votes across these states. Recall that in Section 3, we deÔ¨Åne na(sl:r) = Pr
j=l na(sj). We
break ties in the same way as in PARL."
REFERENCES,0.27960526315789475,"B
ILLUSTRATION OF COPA AGGREGATION PROTOCOLS"
REFERENCES,0.2807017543859649,"We present a concrete example to demonstrate how different aggregation protocols induce different
tolerable poisoning thresholds, illustrate bottleneck and non-bottleneck states, and provide addi-
tional discussions."
REFERENCES,0.2817982456140351,"Suppose the action space contains two actions A = {a1, a2}, and there are six subpolicies {œÄi}u‚àí1
i=0
where u = 6. We are at time step t = 7 now. When there is no poisoning, the subpolicies‚Äô action
predictions for state s0 to s7 are shown by Table 2. After aggregation, all aggregated policies will
choose action a1 at t = 7."
REFERENCES,0.28289473684210525,"PARL (DeÔ¨Ånition 7).
The PARL aggregation protocol only uses the current state s7 to aggregate
the votes. On s7, three subpolicies choose action a1 and three others choose action a2. By breaking
the tie with a1 < a2, the PARL policy œÄP(s7) = a1. The tolerable poisoning threshold Kt = 0,
because one action Ô¨Çipping from a1 to a2 by poisoning only one subpolicy can change the aggregated
policy to a2."
REFERENCES,0.28399122807017546,"TPARL (DeÔ¨Ånition 8).
The TPARL aggregation protocol uses window size W = 7. This implies
that, we aggregate all states s1:7 to count the votes and decide the action. Since na1(s1:7) = 36 and"
REFERENCES,0.2850877192982456,Published as a conference paper at ICLR 2022
REFERENCES,0.28618421052631576,"Table 2: A concrete example of action predictions, where ‚Äú1‚Äù means action a1 and ‚Äú2‚Äù means action a2. When
there is no poisoning attack, the corresponding time window spans by PARL, TPARL, and DPARL are shown
by green , blue , and pink respectively. All aggregated policies choose action a1, but have different tolerable
poisoning thresholds as shown in the last column."
REFERENCES,0.28728070175438597,"Action Prediction for
s0
s1
s2
s3
s4
s5
s6
s7
K at s7
œÄ0
1
1
1
1
1
1
1
1
œÄ1
1
1
1
1
1
1
1
1
œÄ2
1
1
1
1
1
1
1
1
œÄ3
1
1
1
1
1
1
1
2
œÄ4
1
1
1
1
1
1
1
2
œÄ5
1
1
1
2
1
2
2
2
PARL (œÄP, DeÔ¨Ånition 7)
1
0
TPARL with W = 7 (œÄT, DeÔ¨Ånition 8)
1
2
DPARL with Wmax = 8 (œÄD, DeÔ¨Ånition 3)
1
1"
REFERENCES,0.2883771929824561,"na2(s1:7) = 6, after poisoning two subpolicies, we still ena1(s1:7) ‚â•ena2(s1:7). Thus, we achieve an
tolerable poisoning threshold Kt = 2."
REFERENCES,0.2894736842105263,"Compared with PARL, the temporal aggregation in TPARL increases the vote margin between a1
and a2 and thus improve the tolerable poisoning threshold at current state."
REFERENCES,0.2905701754385965,"DPARL (DeÔ¨Ånition 3).
The DPARL aggregation protocol chooses the window size W ‚Ä≤ = 8 since
this window size gives the largest average vote margin ‚àÜW ‚Ä≤
t
(deÔ¨Åned in Equation (1)). We can easily
Ô¨Ånd out that with poisoning size K = 1, we will still choose f
W ‚Ä≤ = 8 as the window size and the
resulting votes for a1 can be kept higher than votes for a2. However, when it comes to K = 2, if we
totally Ô¨Çip subpolicies œÄ0 and œÄ1 to let them always predict action a2, then the DPARL aggregated
policy will turn to choose W ‚Ä≤ = 1 as the window size and then choose action a2 as the action output.
Thus, we achieve a tolerable poisoning threshold Kt = 1 this time."
REFERENCES,0.2916666666666667,"Illustrations of Bottleneck and Non-Bottleneck States.
According to our illustration in Sec-
tion 4.2, bottleneck states are those where subpolicies vote for diverse actions but the best action
is the same as previous states, and non-bottleneck states are those where subpolicies mostly vote
for the same action. As we can see, s0 to s6 are non-bottleneck states since the subpolicies almost
unanimously vote for one action. In contrast, s7 is a bottleneck state."
REFERENCES,0.29276315789473684,"From the perspectives of different aggregation protocols, we observe that for the bottleneck state
s7, both TPARL and DPARL exploit temporal aggregation to boost the certiÔ¨Åed robust poisoning
threshold (from 0 to 2 and from 0 to 1 respectively), thus demonstrating the effectiveness of TPARL
and DPARL on improving certiÔ¨Åed robustness. On the other hand, for non-bottleneck states s0 to
s6, we can easily see that different aggregation protocols do not differ much in terms of provided
certiÔ¨Åed robustness levels."
REFERENCES,0.29385964912280704,"Explanations for Bottleneck and Non-Bottleneck States.
We provide additional discussions
regarding why bottleneck states are vulnerable and why our temporal aggregation strategy can ef-
fectively alleviate the problem. We Ô¨Årst explain the existence of the bottleneck states. Given the
property of the bottleneck states that there is high disagreement among different subpolicies on such
states, they may lie close to the decision boundary. This may be a result of poisoning, or simply due
to the intrinsic difÔ¨Åculty of the state. On the other hand, such states naturally exist in the rollouts
(like natural adversarial examples (Hendrycks et al., 2021)), and may induce high instability of the
rollouts during testing. We next discuss additional intuitions for our temporal aggregation. Essen-
tially, temporal aggregation effectively leverages the adjacent non-bottleneck states to rectify the
decisions at the bottleneck states, based on the assumption of temporal continuity which has been
revealed and utilized in several previous works (Legenstein et al., 2010; Veerapaneni et al., 2020; Ye
et al., 2021)."
REFERENCES,0.2949561403508772,Published as a conference paper at ICLR 2022
REFERENCES,0.29605263157894735,"C
CERTIFICATION OF PER-STATE ACTION STABILITY FOR PARL"
REFERENCES,0.29714912280701755,"We certify the robustness of PARL following Theorem 8.
Theorem 8. Let D be the clean training dataset; let œÄi = M0(Di), 0 ‚â§i ‚â§u ‚àí1 be the learned
subpolicies according to Section 4.1 from which we deÔ¨Åne na (see Section 3); and let œÄP be the Per-
State Partition Aggregation policy: œÄP = M(D) where M abstracts the whole training-aggregation
process. eD is a poisoned dataset and f
œÄP is the poisoned policy: f
œÄP = M( eD)."
REFERENCES,0.2982456140350877,"For a given state st encountered at time step t during test time, let a := œÄP(st), then"
REFERENCES,0.2993421052631579,"Kt =
na(st) ‚àímaxa‚Ä≤Ã∏=a (na‚Ä≤(st) + 1[a‚Ä≤ < a]) 2"
REFERENCES,0.30043859649122806,"
.
(14)"
REFERENCES,0.30153508771929827,"Remark. The theorem provides a valid per-state action certiÔ¨Åcation for PARL, since according to
DeÔ¨Ånition 1, as long as the poisoning size is smaller or equal to Kt, i.e., |D ‚äñeD| ‚â§Kt, the action
of the poisoned policy is the same as the clean policy: f
œÄP(st) = œÄP(st) = a. To compute Kt,
according to Equation (14), we rely on the aggregated action count na for state st from subpolicies.
The a‚Ä≤ < a is the ‚Äúsmaller-than‚Äù operator introduced following DeÔ¨Ånition 7."
REFERENCES,0.3026315789473684,"Proof of Theorem 8. Suppose the poisoning size is within K, then after poisoning, the aggregated
action count ena(st) ‚àà[na(st)‚àíK, na(st)+K] where na(st) is such count before poisoning. Thus,
to ensure the chosen action does not change, a necessary condition is that ena(st) ‚â•ena‚Ä≤(st)+1[a‚Ä≤ <
a] for any other a‚Ä≤ < a. Solving K yields Equation (14)."
REFERENCES,0.30372807017543857,"We use the theorem to compute the per-state action certiÔ¨Åcation for each time step t along the tra-
jectory, and the detailed algorithm is in Algorithm 2 (Appendix E). Furthermore, we prove that this
certiÔ¨Åcation is theoretically tight as the following proposition shows. The proof is in Appendix F.1.
Proposition 9. Under the same condition as Theorem 8, for any time step t, there exists an RL learn-
ing algorithm M0, and a poisoned dataset eD, such that |D ‚äñeD| = Kt + 1, and f
œÄP(st) Ã∏= œÄP(st)."
REFERENCES,0.3048245614035088,"D
A TABLE OF POSSIBLE ACTION SET"
REFERENCES,0.3059210526315789,"Table 3: Expressions of possible action set A(K) given poisoning threshold K for different aggregation
protocols. Full theorem statements are in Theorems 4, 5 and 7 (in Section 4.4)."
REFERENCES,0.30701754385964913,"Protocol
A(K) PARL"
REFERENCES,0.3081140350877193,"n
a ‚ààA
 P"
REFERENCES,0.3092105263157895,"a‚Ä≤‚ààA max{na‚Ä≤(st) ‚àína(st) ‚àíK + 1[a‚Ä≤ < a], 0} ‚â§K
o TPARL"
REFERENCES,0.31030701754385964,"n
a ‚ààA
 PK
i=1 h(i)
a‚Ä≤,a > Œ¥a‚Ä≤,a, ‚àÄa‚Ä≤ Ã∏= a
o"
REFERENCES,0.31140350877192985,"DPARL
{at} ‚à™

a‚Ä≤ ‚ààA

min
1‚â§W ‚àó‚â§min{Wmax,t+1},W ‚àóÃ∏=W ‚Ä≤,a‚Ä≤‚Ä≤Ã∏=at LW ‚àó,W ‚Ä≤"
REFERENCES,0.3125,"a‚Ä≤,a‚Ä≤‚Ä≤
‚â§K

‚à™ ( a ‚ààA K
X"
REFERENCES,0.31359649122807015,"i=1
h(i)
a‚Ä≤,a > Œ¥a‚Ä≤,a, ‚àÄa‚Ä≤ Ã∏= a )"
REFERENCES,0.31469298245614036,"For all three aggregation protocols, we summarize the expressions used for computing the possible
action set A(K) given tolerable poisoning threshold K in Table 3."
REFERENCES,0.3157894736842105,"E
ALGORITHM PSEUDOCODE AND DISCUSSION"
REFERENCES,0.3168859649122807,"E.1
COPA TRAINING PROTOCOL"
REFERENCES,0.31798245614035087,"Algorithm 1: COPA training protocol.
Input: training dataset D, number of partitions k, deterministic hash function h
Output: COPA subpolicies {œÄi}k‚àí1
i=0
1 for i ‚àà[k] do"
REFERENCES,0.3190789473684211,"2
Di ‚Üê{œÑ ‚ààD | h(œÑ) ‚â°i (mod k)}
‚ñ∑Separate the training data D into k partitions"
FOR EACH PARTITION DI DO,0.3201754385964912,3 for each partition Di do
FOR EACH PARTITION DI DO,0.32127192982456143,"4
œÄi ‚ÜêM0(Di)
‚ñ∑Subpolicy trained on partition Di with offline RL algorithm M0"
FOR EACH PARTITION DI DO,0.3223684210526316,"5 return {œÄi}k‚àí1
i=0"
FOR EACH PARTITION DI DO,0.32346491228070173,Published as a conference paper at ICLR 2022
FOR EACH PARTITION DI DO,0.32456140350877194,"E.2
COPA PER-STATE ACTION CERTIFICATION"
FOR EACH PARTITION DI DO,0.3256578947368421,Per-State Partition Aggregation (PARL).
FOR EACH PARTITION DI DO,0.3267543859649123,"Algorithm 2: COPA per-state certiÔ¨Åcation algorithm for Per-State Partition Aggrega-
tion (PARL)."
FOR EACH PARTITION DI DO,0.32785087719298245,"Input: environment E = (S, A, R, P, H, d0), subpolicies {œÄi}k‚àí1
i=0
Output: COPA robust size at each time step {Kt}H‚àí1
t=0
1 s0 ‚àºd0
‚ñ∑sample initial state"
FOR EACH PARTITION DI DO,0.32894736842105265,2 for t from 0 to H ‚àí1 do
FOR EACH PARTITION DI DO,0.3300438596491228,"3
for each a ‚ààA do"
FOR EACH PARTITION DI DO,0.33114035087719296,"4
Compute na(st) from subpolicies‚Äô {œÄi}k‚àí1
i=0 decisions
‚ñ∑na is defined in Section 3"
FOR EACH PARTITION DI DO,0.33223684210526316,"5
Determine the chosen action at ‚ÜêœÄP(st) according to PARL (DeÔ¨Ånition 7)"
FOR EACH PARTITION DI DO,0.3333333333333333,"6
Compute Kt according to Equation (14) in Theorem 8"
FOR EACH PARTITION DI DO,0.3344298245614035,"7
st+1 ‚àºP(st, at)"
FOR EACH PARTITION DI DO,0.3355263157894737,"8 return {Kt}H‚àí1
t=0"
FOR EACH PARTITION DI DO,0.3366228070175439,Temporal Partition Aggregation (TPARL).
FOR EACH PARTITION DI DO,0.33771929824561403,"Algorithm 3: COPA per-state certiÔ¨Åcation algorithm for Temporal Partition Aggrega-
tion (TPARL)."
FOR EACH PARTITION DI DO,0.33881578947368424,"Input: environment E = (S, A, R, P, H, d0), subpolicies {œÄi}k‚àí1
i=0 , window size W
Output: COPA robust size at each time step {Kt}H‚àí1
t=0
1 s0 ‚àºd0
‚ñ∑sample initial state"
FOR EACH PARTITION DI DO,0.3399122807017544,2 for t from 0 to H ‚àí1 do
FOR EACH PARTITION DI DO,0.34100877192982454,"3
for each a ‚ààA do"
FOR EACH PARTITION DI DO,0.34210526315789475,"4
Compute na(st) from subpolicies‚Äô {œÄi}k‚àí1
i=0 decisions
‚ñ∑na is defined in Section 3"
FOR EACH PARTITION DI DO,0.3432017543859649,"5
Determine the chosen action at ‚ÜêœÄT(st) according to TPARL (DeÔ¨Ånition 8)"
FOR EACH PARTITION DI DO,0.3442982456140351,"6
pmin ‚Üê‚àû"
FOR EACH PARTITION DI DO,0.34539473684210525,"7
for each a‚Ä≤ ‚ààA, a‚Ä≤ Ã∏= at do"
FOR EACH PARTITION DI DO,0.34649122807017546,"8
for i from 0 to k ‚àí1 do"
FOR EACH PARTITION DI DO,0.3475877192982456,"9
Compute hi,at,a‚Ä≤ according to Equation (4)"
FOR EACH PARTITION DI DO,0.34868421052631576,"10
{h(i)
at,a‚Ä≤}k
i=1 ‚Üêsorted({hi,at,a‚Ä≤}k‚àí1
i=0 , reverse = True)"
FOR EACH PARTITION DI DO,0.34978070175438597,"11
Compute Œ¥at,a‚Ä≤"
FOR EACH PARTITION DI DO,0.3508771929824561,"12
sum ‚Üê0, p ‚Üê0"
FOR EACH PARTITION DI DO,0.3519736842105263,"13
for j from 1 to k do"
FOR EACH PARTITION DI DO,0.3530701754385965,"14
if sum + h(j)
at,a‚Ä≤ > Œ¥at,a‚Ä≤ then"
FOR EACH PARTITION DI DO,0.3541666666666667,"15
p ‚Üêj ‚àí1"
BREAK,0.35526315789473684,"16
break"
BREAK,0.35635964912280704,"17
p ‚Üêj, sum ‚Üêsum + h(j)
at,a‚Ä≤"
BREAK,0.3574561403508772,"18
pmin ‚Üêmin{pmin, p}"
BREAK,0.35855263157894735,"19
Kt ‚Üêpmin"
BREAK,0.35964912280701755,"20
st+1 ‚àºP(st, at)"
BREAK,0.3607456140350877,"21 return {Kt}H‚àí1
t=0"
BREAK,0.3618421052631579,Published as a conference paper at ICLR 2022
BREAK,0.36293859649122806,Dynamic Temporal Partition Aggregation (DPARL).
BREAK,0.36403508771929827,"Algorithm 4: COPA per-state certiÔ¨Åcation algorithm for Dynamic Temporal Partition Aggrega-
tion (DPARL)."
BREAK,0.3651315789473684,"Input: environment E = (S, A, R, P, H, d0), subpolicies {œÄi}k‚àí1
i=0 , maximum window size Wmax
Output: COPA robust size at each time step {Kt}H‚àí1
t=0
1 s0 ‚àºd0
‚ñ∑sample initial state"
BREAK,0.36622807017543857,2 for t from 0 to H ‚àí1 do
BREAK,0.3673245614035088,"3
for each a ‚ààA do"
BREAK,0.3684210526315789,"4
Compute na(st) from subpolicies‚Äô {œÄi}k‚àí1
i=0 decisions given st ‚ñ∑na is defined in Section 3"
BREAK,0.36951754385964913,"5
Determine the chosen action at ‚ÜêœÄD(st) and chosen window size W ‚Ä≤ according to
DPARL (DeÔ¨Ånition 3)"
BREAK,0.3706140350877193,"6
pmin ‚Üê‚àû"
BREAK,0.3717105263157895,"7
Kt ‚ÜêAlgorithm 3
‚ñ∑use Algorithm 3 with W replaced by W ‚Ä≤ to compute Kt"
KD,0.37280701754385964,"8
KD
t ‚ÜêKt"
KD,0.37390350877192985,"9
for each a‚Ä≤ ‚ààA, a‚Ä≤ Ã∏= at do"
KD,0.375,"10
for each a‚Ä≤‚Ä≤ ‚ààA, a‚Ä≤‚Ä≤ Ã∏= at do"
KD,0.37609649122807015,"11
for W ‚àófrom 1 to min{Wmax, t + 1} do"
KD,0.37719298245614036,"12
a# ‚Üêarg maxa0Ã∏=a‚Ä≤,a0‚ààA na0(st‚àíW ‚àó+1:t)"
KD,0.3782894736842105,"13
for w from 1 to max{W ‚Ä≤, W ‚àó} do"
KD,0.3793859649122807,"14
Compute maxa0‚ààA œÉw(a0) according to Equation (7)"
KD,0.38048245614035087,"15
for i from 0 to k ‚àí1 do"
KD,0.3815789473684211,"16
for w from 1 to max{W ‚Ä≤, W ‚àó} do"
KD,0.3826754385964912,"17
Compute œÉw(œÄi(st‚àíw)) according to Equation (7)"
KD,0.38377192982456143,"18
Compute gi according to Equation (7)"
KD,0.3848684210526316,"19
{g(i)}k
i=1 ‚Üêsorted({gi}k‚àí1
i=0 , reverse = True)"
KD,0.38596491228070173,"20
Compute nW ‚àó
a‚Ä≤ , nW ‚àó"
KD,0.38706140350877194,"a# , nW ‚Ä≤
at , nW ‚Ä≤
a‚Ä≤‚Ä≤"
KD,0.3881578947368421,"21
tmp ‚ÜêW ‚Ä≤(nW ‚àó
a‚Ä≤
‚àínW ‚àó"
KD,0.3892543859649123,"a# ) ‚àíW ‚àó(nW ‚Ä≤
at ‚àínW ‚Ä≤
a‚Ä≤‚Ä≤ ) ‚àí1[a‚Ä≤ > at]"
KD,0.39035087719298245,"22
sum ‚Üê0, p ‚Üê0"
KD,0.39144736842105265,"23
for j from 1 to k do"
KD,0.3925438596491228,"24
if sum + tmp ‚â•0 then"
KD,0.39364035087719296,"25
p ‚Üêj ‚àí1"
BREAK,0.39473684210526316,"26
break"
BREAK,0.3958333333333333,"27
p ‚Üêj, sum ‚Üêsum + g(j)"
BREAK,0.3969298245614035,"28
LW ‚àó,W ‚Ä≤"
BREAK,0.3980263157894737,"a‚Ä≤,a‚Ä≤‚Ä≤
‚Üêp, KD
t ‚Üêmin{KD
t , LW ‚àó,W ‚Ä≤"
BREAK,0.3991228070175439,"a‚Ä≤,a‚Ä≤‚Ä≤
}"
BREAK,0.40021929824561403,"29
st+1 ‚àºP(st, at)"
BREAK,0.40131578947368424,"30 return {KD
t }H‚àí1
t=0"
BREAK,0.4024122807017544,"E.3
COPA CUMULATIVE REWARD CERTIFICATION"
BREAK,0.40350877192982454,"COPA-SEARCH alternately executes the procedure of trajectory exploration and expansion and
poisoning threshold growth. In trajectory exploration and expansion, COPA-SEARCH organizes all
possible trajectories in the form of a search tree and progressively grows it. For each node (represent-
ing a state), we leverage Theorems 4, 5 and 7 to compute the Possible Action Set. We then expand
the tree branches corresponding to the actions in the derived set. In poisoning threshold growth,
when all trajectories for the current poisoning threshold are explored, we increase K‚Ä≤ to seek certi-
Ô¨Åcation under larger poisoning sizes, via maintaining a priority queue of all poisoning sizes during
the expansion of the tree. The iterative procedures end when the priority queue becomes empty."
BREAK,0.40460526315789475,Published as a conference paper at ICLR 2022
BREAK,0.4057017543859649,"The highlighted line in the following algorithm needs to inject different algorithms based on the
aggregation protocol: for PARL (œÄP), use Theorem 4; for TPARL (œÄT), use Theorem 5; for
DPARL (œÄD), use Theorem 7."
BREAK,0.4067982456140351,Algorithm 5: COPA-SEARCH: adaptive tree search for cumulative reward certiÔ¨Åcation.
BREAK,0.40789473684210525,"Input: environment E = (S, A, R, P, H, d0), subpolicies {œÄi}k‚àí1
i=0 , aggregated policy œÄP or œÄT (with
W) or œÄT (with Wmax)
Output: a map M that maps poisoning size K to corresponding certiÔ¨Åed lower bound of cumulative
reward JK
‚ñ∑Initialize global variables"
BREAK,0.40899122807017546,"1 p que ‚Üê‚àÖ
‚ñ∑initialize an empty priority queue containing tuples of
(state historys0:t, action a, poisoning size K, reward J) sorted by increasing K"
BREAK,0.4100877192982456,"2 Jglobal ‚Üê‚àû
‚ñ∑initialize global minimum reward"
BREAK,0.41118421052631576,"3 Function GETACTIONS(s0:t, Klim, Jcur):"
BREAK,0.41228070175438597,"4
A ‚ÜêpossibleActions(s0:t, {œÄi}k‚àí1
i=1 , œÄ‚àó, Klim)
‚ñ∑compute the possible action set given
state history, subpolicies, aggregation policy œÄ‚àó, and poisoning size according to
theorems in Section 4.4"
BREAK,0.4133771929824561,"5
a list ‚Üê‚àÖ"
BREAK,0.4144736842105263,"6
for each action a ‚ààA do"
BREAK,0.4155701754385965,"7
if P(s, a) = ‚ä•then"
CONTINUE,0.4166666666666667,"8
continue
‚ñ∑game terminate here, no possible larger or lower cumulative reward
to search"
CONTINUE,0.41776315789473684,"9
a list ‚Üêa list ‚à™{a}
‚ñ∑record possible actions to expand"
CONTINUE,0.41885964912280704,"10
if A Ã∏= A then"
CONTINUE,0.4199561403508772,"11
K‚Ä≤ ‚ÜêminpossibleActions(s0:t,{œÄi}k‚àí1
i=0 ,œÄ‚àó,K)Ã∏=A K"
CONTINUE,0.42105263157894735,"12
Anew ‚ÜêpossibleActions(s0:t, {œÄi}k‚àí1
i=1 , œÄ‚àó, K‚Ä≤) \ A ‚ñ∑compute the immediate actions that
will possibly be chosen if enlarging the poisoning size"
CONTINUE,0.42214912280701755,"13
for each action a ‚ààAnew do"
CONTINUE,0.4232456140350877,"14
p que.push((s0:t, a, K‚Ä≤, Jcur))"
RETURN A LIST,0.4243421052631579,"15
return a list"
RETURN A LIST,0.42543859649122806,"16 Procedure EXPAND(s0:t, Klim, Jcur):"
RETURN A LIST,0.42653508771929827,"17
if Jcur ‚â•Jglobal ‚àß(step reward is non-negative) then"
RETURN,0.4276315789473684,"18
return
‚ñ∑pruning"
RETURN,0.42872807017543857,"19
a list ‚ÜêGETACTIONS(s0:t, {œÄi}k‚àí1
i=0 , œÄ‚àó, Klim)
‚ñ∑compute according to theorems in
Section 4.4"
RETURN,0.4298245614035088,"20
if a list = ‚àÖthen"
RETURN,0.4309210526315789,"21
Jglobal ‚Üêmin{Jglobal, Jcur}"
RETURN,0.43201754385964913,"22
return"
RETURN,0.4331140350877193,"23
for a ‚ààa list do"
RETURN,0.4342105263157895,"24
st+1 ‚ÜêP(st, a)"
RETURN,0.43530701754385964,"25
if st+1 = ‚ä•then"
RETURN,0.43640350877192985,"26
Jglobal ‚Üêmin{Jglobal, Jcur}"
ELSE,0.4375,"27
else"
ELSE,0.43859649122807015,"28
EXPAND (s0:t+1, Klim, Jcur + R(st, a))"
ELSE,0.43969298245614036,29 M ‚Üê‚àÖ
ELSE,0.4407894736842105,30 s0 ‚Üêd0
ELSE,0.4418859649122807,"‚ñ∑initial state is s0
31 EX P AND (s0, Klim = 0, Jcur = 0)
‚ñ∑expand initial trajectory"
WHILE TRUE DO,0.44298245614035087,32 while True do
WHILE TRUE DO,0.4440789473684211,"33
if p que = ‚àÖthen"
BREAK,0.4451754385964912,"34
break
‚ñ∑no state to expand"
BREAK,0.44627192982456143,"35
(s0:t, a, K, J) ‚Üêp que.pop()
‚ñ∑pop out the first element"
BREAK,0.4473684210526316,"36
( , , K‚Ä≤, ) ‚Üêp que.top()
‚ñ∑examine the next element"
BREAK,0.44846491228070173,"37
M(K) ‚ÜêJglobal
‚ñ∑obtain one new point of certification result"
BREAK,0.44956140350877194,"38
st+1 ‚ÜêP(st, a)"
BREAK,0.4506578947368421,"39
EXPAND (s0:t+1, K‚Ä≤, J + R(st, a))
‚ñ∑expand the tree from the new node"
RETURN M,0.4517543859649123,"40 return M
‚ñ∑indeed the algorithm can terminate at any time within the while loop"
RETURN M,0.45285087719298245,Published as a conference paper at ICLR 2022
RETURN M,0.45394736842105265,"Time Complexity.
The complexity of COPA-SEARCH is O(H|Sexplored|(log |Sexplored| +
|A|T)), where |Sexplored| is the number of explored states throughout the search procedure, which is
no larger than cardinality of state set S, H is the horizon length, |A| is the cardinality of action set,
and T is the time complexity of per-state action certiÔ¨Åcation. The main bottleneck of the algorithm
is the large number of possible states, which is in the worse case exponential to state dimension.
However, to provide a deterministic worst-case certiÔ¨Åcation agnostic to environment properties, ex-
ploring all possible states may be inevitable."
RETURN M,0.4550438596491228,"Relation with Wu et al. (2022).
The COPA-SEARCH algorithm is inspired from Wu et al.
(2022, Algorithm 3), which also leverages tree search to explore all possible states and thus derive
the robustness guarantee. However, the major distinction is that their algorithm tries to derive a
probabilistic guarantee of RL robustness against state perturbations, while COPA-SEARCH derives
deterministic guarantee of RL robustness against poisoning attacks. We think this general tree search
methodology can be further extended to provide certiÔ¨Åcation beyond evasion attack (Wu et al., 2022)
and poisoning attack (our work) and we leave it as future work."
RETURN M,0.45614035087719296,"Extension to stochastic MDPs.
In the current version of COPA-SEARCH, the exhaustive search
is enabled by the deterministic MDP assumption. However, we foresee the potential of conveniently
extending COPA-SEARCH to stochastic MDPs and will discuss one concrete method below. In con-
trast to interacting with the environment for one time to obtain the deterministic next state transition
in the deterministic MDP case, in the case of stochastic MDPs, we can leverage sampling (i.e., by
repeatedly taking the same action at the same state) to obtain the set of high probability next state
transitions with high conÔ¨Ådence. In this way, our COPA-SEARCH will be able to yield a proba-
bilistic bound, in comparison with the deterministic bound achieved in this paper enabled by the
deterministic MDP assumption."
RETURN M,0.45723684210526316,"F
PROOFS"
RETURN M,0.4583333333333333,"F.1
TIGHTNESS OF PER-STATE ACTION CERTIFICATION IN PARL"
RETURN M,0.4594298245614035,"Proof of Proposition 9. We prove by construction. Given the state st, we Ô¨Årst locate the subpolicies
whose chosen action is a, and denote the set of them as
B = {i ‚àà[u] | œÄi(st) = a = œÄP(st)}.
We also denote a‚Ä≤ = arg maxa‚Ä≤Ã∏=a na‚Ä≤(st)+1[a‚Ä≤ < a]. According to Kt‚Äôs deÔ¨Ånition (Equation (14)),
|B| = na(st) > na(st)/2 ‚â•Kt.
We now pick an arbitrary subset B‚Ä≤ ‚äÜB such that |B‚Ä≤| = Kt + 1. For each i ‚ààB‚Ä≤, we locate its
corresponding parititioned dataset Di for training subpolicy œÄi. We insert one point pi to Di, such
that our chosen learning algorithm M0 can train a subpolicy ÀúœÄi = M0(Di ‚à™{pi}) that satisÔ¨Åes
ÀúœÄi(st) = a‚Ä≤. For example, the point could be pi = {(st, a‚Ä≤, s‚Ä≤, ‚àû)} and M0 learns the action with
maximum reward for the memorized nearest state, where s‚Ä≤ can be adjusted to make sure the point
is hashed to parition i.1 Then, we construct the poisoned dataset eD = ["
RETURN M,0.4605263157894737,"i‚ààB‚Ä≤
Di ‚à™{pi} ! ‚à™ Ô£´ Ô£≠
["
RETURN M,0.4616228070175439,"i‚àà[u]\B‚Ä≤
Di Ô£∂ Ô£∏."
RETURN M,0.46271929824561403,"Therefore, we have eD ‚äñD = ‚à™i‚ààB‚Ä≤pi = |B‚Ä≤| = Kt + 1."
RETURN M,0.46381578947368424,"On this poisoned dataset, we train subpolicies {ÀúœÄi}u‚àí1
i=0 and get the aggregated policy f
œÄP. To study
f
œÄP(st), we compute the aggregated action count ena on these poisoned subpolicies. We found that"
RETURN M,0.4649122807017544,"1Strictly speaking, we need to choose a determinisitc hash function h for dataset partitioning such that
adjustment on s‚Ä≤ and reward (currently ‚àû, but can be an arbitrary large enough number) can make the point
being partitioned to i, i.e., h(pi) ‚â°i (mod u). Since our adjustment space is inÔ¨Ånite due to inÔ¨Ånite number of
large enough reward, such assumption can be easily achieved. Same applies to other attack constructions in the
following proofs."
RETURN M,0.46600877192982454,Published as a conference paper at ICLR 2022
RETURN M,0.46710526315789475,"ena(st) = na(st) ‚àí|B‚Ä≤| and ena‚Ä≤(st) = na‚Ä≤(st) + |B‚Ä≤|. Therefore,
ena(st) ‚àíena‚Ä≤(st) = na(st) ‚àína‚Ä≤(st) ‚àí2(Kt + 1)"
RETURN M,0.4682017543859649,"= na(st) ‚àína‚Ä≤(st) ‚àí2
jna(st) ‚àína‚Ä≤(st) ‚àí1[a‚Ä≤ < a] 2"
RETURN M,0.4692982456140351,"k
+ 1
"
RETURN M,0.47039473684210525,"< na(st) ‚àína‚Ä≤(st) ‚àí(na(st) ‚àína‚Ä≤(st) ‚àí1[a‚Ä≤ < a]) = 1[a‚Ä≤ < a].
Therefore, if a‚Ä≤ < a, ena(st) ‚â§ena‚Ä≤(st), and a‚Ä≤ has higher priority to be chosen than a; if a‚Ä≤ > a,
ena(st) ‚â§ena‚Ä≤(st) ‚àí1, and a‚Ä≤ still has higher priority to be chosen than a. Hence, f
œÄP(st) Ã∏= a =
œÄP(st). To this point, we conclude the proof with a feasible construction."
RETURN M,0.47149122807017546,"F.2
PER-STATE ACTION CERTIFICATION IN TPARL"
RETURN M,0.4725877192982456,"Proof of Theorem 1. For ease of notation, we let w = min{W, t+1} so w is the actual window size
used at step t. We let t0 = t ‚àíw + 1, i.e., t0 is the actual start time step for TPARL aggregation at
step t. Now we can write the chosen action at step t without poisoning as a = œÄT(st0:t)."
RETURN M,0.47368421052631576,"We prove the theorem by contradiction: We assume that there is a poisoning attack whose poisoning
size K ‚â§Kt where Kt is deÔ¨Åned by Equation (3) in the theorem, and after poisoning, the chosen
action is aT Ã∏= a. We denote {ÀúœÄi}u‚àí1
i=0 to the poisoned subpolicies and f
œÄT to the poisoned TPARL
aggregated policy. From the deÔ¨Ånition of Kt, we have
K
X"
RETURN M,0.47478070175438597,"i=1
h(i)
a,aT ‚â§ Kt
X"
RETURN M,0.4758771929824561,"i=1
h(i)
a,aT ‚â§Œ¥a,aT ,
(15)"
RETURN M,0.4769736842105263,"since K ‚â§Kt, and each h(i)
a,aT is an element of hi‚Ä≤,a,aT for some i‚Ä≤ where"
RETURN M,0.4780701754385965,"hi‚Ä≤,a,aT = w‚àí1
X"
RETURN M,0.4791666666666667,"j=0
1i‚Ä≤,a(st‚àíj) + w ‚àí w‚àí1
X"
RETURN M,0.48026315789473684,"j=0
1i‚Ä≤,aT (st‚àíj) ‚â•0"
RETURN M,0.48135964912280704,"by Equation (4) in the theorem. Since the poisoning attack within threshold K can at most affect K
subpolicies, we let B be the set of affected policies and assume |B| = K without loss of generality.
Formally, B = {i ‚àà[u] | ‚àÉt‚Ä≤ ‚àà[t0, t], ÀúœÄi(st‚Ä≤) Ã∏= œÄi(st‚Ä≤)}. Therefore, according to the monotonicity
of h(i)
a,aT , from Equation (15),
X"
RETURN M,0.4824561403508772,"i‚ààB
hi,a,aT ‚â§ K
X"
RETURN M,0.48355263157894735,"i=1
h(i)
a,aT ‚â§Œ¥a,aT .
(16)"
RETURN M,0.48464912280701755,"According to the assumption of successfully poisoning attack, after attack, the sum of aggregated
vote for aT plus 1[aT < a] should be larger then that of a. Formally, after the poisoning,
enaT (st0:t) + 1[aT < a] > ena(st0:t)
or equivalently
ena(st0:t) ‚àí(enaT (st0:t) + 1[aT < a]) < 0.
From the statement of Theorem 1,
Œ¥a,aT = na(st0:t) ‚àí(naT (st0:t) + 1[aT < a]).
Take the difference of the above two equations, we know that the attack satisÔ¨Åes
na(st0:t) ‚àíena(st0:t) ‚àí(naT (st0:t) ‚àíenaT (st0:t)) > Œ¥a,aT .
(17)
Since the attack only changes the subpolicies in B, we have"
RETURN M,0.4857456140350877,"na(st0:t) ‚àíena(st0:t) =
X i‚ààB Ô£´"
RETURN M,0.4868421052631579,"Ô£≠
w‚àí1
X"
RETURN M,0.48793859649122806,"j=0
1[œÄi(st‚àíj) = a] ‚àí w‚àí1
X"
RETURN M,0.48903508771929827,"j=0
1[ÀúœÄi(st‚àíj) = a] Ô£∂ Ô£∏ ‚â§
X i‚ààB w‚àí1
X"
RETURN M,0.4901315789473684,"j=0
1i,a(st‚àíj),"
RETURN M,0.49122807017543857,"naT (st0:t) ‚àíenaT (st0:t) =
X i‚ààB Ô£´"
RETURN M,0.4923245614035088,"Ô£≠
w‚àí1
X"
RETURN M,0.4934210526315789,"j=0
1[œÄi(st‚àíj) = aT ] ‚àí w‚àí1
X"
RETURN M,0.49451754385964913,"j=0
1[ÀúœÄi(st‚àíj) = aT ] Ô£∂ Ô£∏ ‚â•
X i‚ààB Ô£´"
RETURN M,0.4956140350877193,"Ô£≠
w‚àí1
X"
RETURN M,0.4967105263157895,"j=0
1i,aT (stj) ‚àíw Ô£∂ Ô£∏."
RETURN M,0.49780701754385964,Published as a conference paper at ICLR 2022
RETURN M,0.49890350877192985,"Inject them into Equation (17) yields
X i‚ààB Ô£´"
RETURN M,0.5,"Ô£≠
w‚àí1
X"
RETURN M,0.5010964912280702,"j=0
1i,a(st‚àíj) + w ‚àí w‚àí1
X"
RETURN M,0.5021929824561403,"j=0
1i,aT (st‚àíj) Ô£∂ Ô£∏"
RETURN M,0.5032894736842105,"|
{z
}
hi,a,aT"
RETURN M,0.5043859649122807,"> Œ¥a,aT"
RETURN M,0.5054824561403509,which contradicts Equation (16) and thus concludes the proof.
RETURN M,0.506578947368421,"F.3
TIGHTNESS OF PER-STATE ACTION CERTIFICATION IN TPARL"
RETURN M,0.5076754385964912,"Proof of Proposition 2. For ease of notation, we let w = min{W, t + 1} so w is the actual window
size used at step t. We let t0 = t ‚àíw + 1, i.e., t0 is the actual start time step for TPARL aggregation
at step t. Now we can write the chosen action at step t without poisoning as a = œÄT(st0:t)."
RETURN M,0.5087719298245614,"We prove by construction, i.e., we construct an poisoning attack with poisoning size Kt + 1 that
deviates the prediction of the poisoned policy. SpeciÔ¨Åcally, we aim to craft a poisoned dataset eD
with poisoning size |D‚äñeD| = Kt+1, such that for certain learning algorithm M0, after partitioning
and learning on the poisoned dataset, the poisoned subpolicies ÀúœÄi = M0( eDi) can be aggregated to
produce different action prediction: f
œÄT(st0:t) Ã∏= a."
RETURN M,0.5098684210526315,"Before construction, we Ô¨Årst show that Kt given by Equation (3) satisÔ¨Åes Kt < u. If Kt = u, it
means that for arbitrary a‚Ä≤ Ã∏= a, u
X"
RETURN M,0.5109649122807017,"i=1
h(i)
a,a‚Ä≤ = u‚àí1
X"
RETURN M,0.5120614035087719,"i=0
hi,a,a‚Ä≤ = u‚àí1
X i=0 Ô£´"
RETURN M,0.5131578947368421,"Ô£≠
w‚àí1
X"
RETURN M,0.5142543859649122,"j=0
1i,a(st‚àíj) + w ‚àí w‚àí1
X"
RETURN M,0.5153508771929824,"j=0
1i,a‚Ä≤(st‚àíj) Ô£∂"
RETURN M,0.5164473684210527,Ô£∏= na(st0:t) + uw ‚àína‚Ä≤(st0:t)
RETURN M,0.5175438596491229,"‚â§Œ¥a,a‚Ä≤ = na(st0:t) ‚àína‚Ä≤(st0:t) ‚àí1[a‚Ä≤ < a],
which implies uw ‚â§0 contracting uw > 0. Now, we know Kt < u, so Kt + 1, our poisoning size,
is smaller or equal to u."
RETURN M,0.518640350877193,"We start our construction by choosing an action aT :
aT = arg min
a‚Ä≤Ã∏=a,a‚Ä≤‚ààA
arg max
Pp
i=1 h(i)
a,a‚Ä≤‚â§Œ¥a,a‚Ä≤
p."
RETURN M,0.5197368421052632,"According to the deÔ¨Ånition of Kt in Equation (3), for aT we have"
RETURN M,0.5208333333333334,"Kt+1
X"
RETURN M,0.5219298245614035,"i=1
h(i)
a,aT > Œ¥a,aT .
(18)"
RETURN M,0.5230263157894737,"We locate the subpolicies to poison as
B = {i ‚àà[u] | hi
a,aT is h(j)
a,aT in the nonincreasing permutation in Equation (4), j ‚â§Kt + 1}
(19)
Therefore, |B| = Kt + 1 and
X"
RETURN M,0.5241228070175439,"i‚ààB
hi,a,aT > Œ¥a,aT
(20)"
RETURN M,0.5252192982456141,"by Equation (18). For each of these subpolicies i ‚ààB, we locate its corresponding partitioned
dataset Di for training subpolicy œÄi, and insert one trajectory pi to Di, such that our chosen learning
algorithm M0 can train a subpolicy ÀúœÄi = M(Di ‚à™{pi}) satisfying ÀúœÄi(st‚Ä≤) = aT for any t‚Ä≤ ‚àà
[t0, t]. For example, the trajectory could be pi = {(st‚Ä≤, aT , s‚Ä≤, ‚àû)}t
t‚Ä≤=t0 and M0 learns the action
with maximum reward for the memorized nearest state, where s‚Ä≤ can be adjusted to make sure the
trajectory is hashed to partition i. Then, we construct the poisoned dataset eD = ["
RETURN M,0.5263157894736842,"i‚ààB‚Ä≤
Di ‚à™{pi} ! ‚à™ Ô£´ Ô£≠
["
RETURN M,0.5274122807017544,"i‚àà[u]\B‚Ä≤
Di Ô£∂ Ô£∏."
RETURN M,0.5285087719298246,"Therefore, we have eD ‚äñD = ‚à™i‚ààB‚Ä≤pi = |B‚Ä≤| = Kt + 1. Now, we compare the aggregated votes
for a and aT before and after poisoning:"
RETURN M,0.5296052631578947,"ena(st0:t) ‚àína(st0:t) =
X i‚ààB Ô£´"
RETURN M,0.5307017543859649,"Ô£≠
w‚àí1
X"
RETURN M,0.5317982456140351,"j=0
1[eœÄi(st‚àíj) = a] ‚àí w‚àí1
X"
RETURN M,0.5328947368421053,"j=0
1[œÄi(st‚àíj) = a] Ô£∂ Ô£∏"
RETURN M,0.5339912280701754,"Published as a conference paper at ICLR 2022 = ‚àí
X i‚ààB w‚àí1
X"
RETURN M,0.5350877192982456,"j=0
1[œÄi(st‚àíj) = a],"
RETURN M,0.5361842105263158,"enaT (st0:t) ‚àínaT (st0:t) =
X i‚ààB Ô£´"
RETURN M,0.5372807017543859,"Ô£≠
w‚àí1
X"
RETURN M,0.5383771929824561,"j=0
1[eœÄi(st‚àíj) = aT ] ‚àí w‚àí1
X"
RETURN M,0.5394736842105263,"j=0
1[œÄi(st‚àíj) = aT ] Ô£∂ Ô£∏ =
X i‚ààB Ô£´ Ô£≠w ‚àí w‚àí1
X"
RETURN M,0.5405701754385965,"j=0
1[œÄi(st‚àíj) = aT ] Ô£∂ Ô£∏."
RETURN M,0.5416666666666666,"Now we compare the margin between aggregated votes for a and aT after poisoning:
enaT (st0:t) ‚àíena(st0:t) + 1[aT < a]"
RETURN M,0.5427631578947368,=naT (st0:t) ‚àína(st0:t) + 1[aT < a] + enaT (st0:t) ‚àínaT (st0:t) ‚àí(ena(st0:t) ‚àína(st0:t))
RETURN M,0.543859649122807,"= ‚àíŒ¥a,aT +
X i‚ààB Ô£´"
RETURN M,0.5449561403508771,"Ô£≠
w‚àí1
X"
RETURN M,0.5460526315789473,"j=0
1[œÄi(st‚àíj) = a] + w ‚àí w‚àí1
X"
RETURN M,0.5471491228070176,"j=0
1[œÄi(st‚àíj) = aT ] Ô£∂ Ô£∏"
RETURN M,0.5482456140350878,"= ‚àíŒ¥a,aT +
X"
RETURN M,0.5493421052631579,"i‚ààB
hi,a,aT > 0."
RETURN M,0.5504385964912281,"As a result, after poisoning, aT has higher priority to be chosen than a, i.e., f
œÄT(st0:t) Ã∏= a =
œÄT(st0:t). To this point, we conclude the proof with a feasible attack construction."
RETURN M,0.5515350877192983,"F.4
LOOSE PER-STATE ACTION CERTIFICATION IN TPARL AND COMPARISION WITH TIGHT
ONE"
RETURN M,0.5526315789473685,"The following corollary of Theorem 8 states a loose per-state action certiÔ¨Åcation.
Corollary 10. Under the same condition as Theorem 1, Kt ="
RETURN M,0.5537280701754386,"$
na(smax{t‚àíW +1,0}:t) ‚àímaxa‚Ä≤Ã∏=a
 
na‚Ä≤(smax{t‚àíW +1,0}:t) + 1[a‚Ä≤ < a]
"
RETURN M,0.5548245614035088,"2 min{W, t + 1} % (21)"
RETURN M,0.555921052631579,"is a tolerable poisoning threshold at time step t in DeÔ¨Ånition 1, where W is the window size."
RETURN M,0.5570175438596491,"Proof of Corollary 10. We let w = min{t +1, W} to be the actual aggregation window size at time
step t. After poisoning with size K, at most K subpolicies are affected and each affected policy can
only make ¬±w changes to the aggregated action count. This implies that, after poisoning, for any
action a‚Ä≤ ‚ààA, the aggregated vote count ena‚Ä≤(st‚àíw+1:t) ‚àà[na‚Ä≤(st‚àíw+1:t) ‚àíuw, na‚Ä≤(st‚àíw+1:t) +
uw]. Thus, when K ‚â§Kt where Kt is deÔ¨Åned in Theorem 1, for any a‚Ä≤ Ã∏= a, we have
Àúna(st‚àíw+1:t) ‚àíÀúna‚Ä≤(st‚àíw+1:t) ‚àí1[a‚Ä≤ < a]"
RETURN M,0.5581140350877193,=na(st‚àíw+1:t) ‚àína‚Ä≤(st‚àíw+1:t) ‚àí1[a‚Ä≤ < a] ‚àí2Kw
RETURN M,0.5592105263157895,‚â•na(st‚àíw+1:t) ‚àína‚Ä≤(st‚àíw+1:t) ‚àí1[a‚Ä≤ < a] ‚àí2w ¬∑ Kt
RETURN M,0.5603070175438597,"‚â•na(st‚àíw+1:t) ‚àína‚Ä≤(st‚àíw+1:t) ‚àí1[a‚Ä≤ < a] ‚àí

na(st‚àíw+1:t) ‚àímax
a‚Ä≤‚Ä≤Ã∏=a(na‚Ä≤‚Ä≤(st‚àíw+1:t) + 1[a‚Ä≤‚Ä≤ < a])
"
RETURN M,0.5614035087719298,"= max
a‚Ä≤‚Ä≤Ã∏=a(na‚Ä≤‚Ä≤(st‚àíw+1:t) + 1[a‚Ä≤‚Ä≤ < a]) ‚àí(na‚Ä≤(st‚àíw+1:t) + 1[a‚Ä≤ < a]) ‚â•0."
RETURN M,0.5625,"From the deÔ¨Ånition of TPARL protocol, the poisoned policy still chooses action a, which implies
Kt is a tolerable poisoning threshold."
RETURN M,0.5635964912280702,"In the main text, we mention that the certiÔ¨Åcation from Corollary 10 is looser than that from Theo-
rem 1. This assertion is based on the following two facts:"
RETURN M,0.5646929824561403,"1. According to Proposition 2, the certiÔ¨Åcation given by Theorem 1 is theoretically tight, which
means that any other certiÔ¨Åcation can only be as tight as Theorem 1 or looser than it.
2. There exists examples where the computed Kt from Theorem 1 is larger than that from Corol-
lary 10.
For instance, suppose W = 5, action set A = {a1, a2}, and there are three subpolicies. At time
step t = 4, œÄi for s0 to s4 are [a1, a1, a1, a1, a2] for all subpolicies (i.e., i ‚àà[3]). Thus, the"
RETURN M,0.5657894736842105,Published as a conference paper at ICLR 2022
RETURN M,0.5668859649122807,"benign policy œÄT(s0:4) = a1. By computation, the Kt from Theorem 1 is 1; while the Kt from
Corollary 10 is 0."
RETURN M,0.5679824561403509,"Indeed, Corollary 10 can be viewed as using 2w to upper bound hi,a,a‚Ä≤ = w + Pw‚àí1
j=0 1i,a(st‚àíj) ‚àí
Pw‚àí1
j=0 1i,a‚Ä≤(st‚àíj). Intuitively, Corollary 10 assumes every subpolicy can provide 2w vote margin
shrinkage, and Theorem 1 uses hi,a,a‚Ä≤ to capture the precise worse-case margin shrinkage and thus
provides a tighter certiÔ¨Åcation."
RETURN M,0.569078947368421,"F.5
PER-STATE ACTION CERTIFICATION IN DPARL"
RETURN M,0.5701754385964912,"Proof of Theorem 3. Without loss of generality, we assume Wmax ‚â§t + 1 and otherwise we let
Wmax ‚Üêmin{Wmax, t + 1}. We let t0 = max{t ‚àíWmax + 1, 0} be the start time step of the
maximum possible window. To prove the theorem, our general methodology is to enumerate all
possible cases of a successful attack, and derive the tolerable poisoning threshold for each case
respectively. Taking a minimum over these tolerable poisoning thresholds gives the required result."
RETURN M,0.5712719298245614,"SpeciÔ¨Åcally, we denote P to the predicate of robustness under poisoning attack: P = [f
œÄD(st0:t) =
a], and denote K to the poisoning attack size. Therefore, we can decompose P as such:
P = P(W ‚Ä≤) ‚àß
^"
RETURN M,0.5723684210526315,"1‚â§W ‚àó‚â§Wmax,W ‚àóÃ∏=W ‚Ä≤ a‚Ä≤Ã∏=a"
RETURN M,0.5734649122807017,"¬¨Q(W ‚àó, a‚Ä≤).
(22)"
RETURN M,0.5745614035087719,"Recall that W ‚Ä≤ is the chosen window size by the protocol DPARL with unattacked subpolicies œÄD
(Equation (1)). In Equation (22), the predicate P(W ‚Ä≤) means that after poisoning attack, whether
the prediction under window size W ‚Ä≤ is still a; the predicate Q(W ‚àó, a‚Ä≤) means that after poisoning
attack, whether the chosen action is a‚Ä≤ at window size W ‚àóand average vote margin is larger (or equal
if a‚Ä≤ < a) at window size W ‚àócompared to W ‚Ä≤. Formally, let ena be the aggregated action count after
poisoning and e‚àÜW
t
be the average vote margin after poisoning at window W (see Equation (2)),"
RETURN M,0.5756578947368421,"P(W ‚Ä≤) =

arg max
a‚ààA
ena(st‚àíW ‚Ä≤+1:t) = a

,"
RETURN M,0.5767543859649122,"Q(W ‚àó, a‚Ä≤) =

arg max
a‚ààA
ena(st‚àíW ‚àó+1:t) = a‚Ä≤

‚àß

e‚àÜW ‚àó
t
‚â•e‚àÜW ‚Ä≤
t

‚àß(a‚Ä≤ < a)

‚à®

e‚àÜW ‚àó
t
> e‚àÜW ‚Ä≤
t

‚àß(a‚Ä≤ > a)

. (23)"
RETURN M,0.5778508771929824,"According to Theorem 1,
K ‚â§Kt =‚áíP(W ‚Ä≤)
where Kt is deÔ¨Åned by Equation (3) with W replaced by W ‚Ä≤. The following Lemma 11 shows a
sufÔ¨Åcient condition for Q(W ‚àó, a‚Ä≤). We then aggregate these conditions together with minimum to
obtain a sufÔ¨Åcient condition for P:"
RETURN M,0.5789473684210527,"K ‚â§min
"
RETURN M,0.5800438596491229,"Kt,
min
1‚â§W ‚àó‚â§min{Wmax,t+1},W ‚àóÃ∏=W ‚Ä≤,a‚Ä≤Ã∏=a,a‚Ä≤‚Ä≤Ã∏=a LW ‚àó,W ‚Ä≤"
RETURN M,0.581140350877193,"a‚Ä≤,a‚Ä≤‚Ä≤ "
RETURN M,0.5822368421052632,and thus conclude the proof.
RETURN M,0.5833333333333334,"Lemma 11. Let Q(W ‚àó, a‚Ä≤), K, W ‚Ä≤ be the same as deÔ¨Åned in proof of Theorem 3, then
K ‚â§min
a‚Ä≤‚Ä≤Ã∏=a LW ‚àó,W ‚Ä≤"
RETURN M,0.5844298245614035,"a‚Ä≤,a‚Ä≤‚Ä≤
=‚áí¬¨Q(W ‚àó, a‚Ä≤),
(24)"
RETURN M,0.5855263157894737,"where LW ‚àó,W ‚Ä≤"
RETURN M,0.5866228070175439,"a‚Ä≤,a‚Ä≤‚Ä≤
is deÔ¨Åned in DeÔ¨Ånition 4."
RETURN M,0.5877192982456141,"Proof. We prove the equivalent form:
Q(W ‚àó, a‚Ä≤) =‚áíK > min
a‚Ä≤‚Ä≤Ã∏=a LW ‚àó,W ‚Ä≤"
RETURN M,0.5888157894736842,"a‚Ä≤,a‚Ä≤‚Ä≤
.
(25)"
RETURN M,0.5899122807017544,"Suppose a poisoning attack can successfully achieve Q(W ‚àó, a‚Ä≤), we now induce the requirement on
its poisoning size K. First, we notice that

e‚àÜW ‚àó
t
‚â•e‚àÜW ‚Ä≤
t

‚àß(a‚Ä≤ < a)

‚à®

e‚àÜW ‚àó
t
> e‚àÜW ‚Ä≤
t

‚àß(a‚Ä≤ > a)
"
RETURN M,0.5910087719298246,"‚áê‚áíW ‚àóW ‚Ä≤ e‚àÜW ‚àó
t
‚â•W ‚àóW ‚Ä≤ e‚àÜW ‚Ä≤
t
+ 1[a‚Ä≤ > a]
(26)"
RETURN M,0.5921052631578947,Published as a conference paper at ICLR 2022
RETURN M,0.5932017543859649,since W ‚àóW ‚Ä≤ e‚àÜW ‚àó/W ‚àó
RETURN M,0.5942982456140351,"t
is an integer by deÔ¨Ånition. According to the deÔ¨Ånition and Q(W ‚àó, a‚Ä≤)‚Äôs
assumption that arg maxa‚ààA ena(st‚àíW ‚àó+1:t) = a‚Ä≤,
W ‚àóW ‚Ä≤ e‚àÜW ‚àó
t
‚â§W ‚Ä≤ (ena‚Ä≤(st‚àíW ‚àó+1:t) ‚àíena#(st‚àíW ‚àó+1:t)) ,
where ‚Äú‚â§‚Äù comes from the fact that the margin in e‚àÜW ‚àó
t
should be with respect to the runner-up
class after poisoning, and computing with respect to any other class provides an upper bound. Here
we choose a# = arg maxa0Ã∏=a‚Ä≤,a0‚ààA na0(st‚àíW ‚àó+1:t) (see DeÔ¨Ånition 4), the runner-up class before
poisoning, to empirically shrink the gap between the bound and actual margin. On the other hand,"
RETURN M,0.5953947368421053,"W ‚àóW ‚Ä≤ e‚àÜW ‚Ä≤
t
‚â•W ‚àó

ena(st‚àíW ‚Ä≤+1:t) ‚àímax
a‚Ä≤‚Ä≤Ã∏=a ena‚Ä≤‚Ä≤(st‚àíW ‚Ä≤+1:t)

,"
RETURN M,0.5964912280701754,"where ‚Äú‚â•‚Äù comes from the fact that the margin in e‚àÜW ‚Ä≤
t
should use the top class after poisoning, and
computing with any other class provides a lower bound. Thus, from Equation (26) and the above
two relaxations, we get
Q(W ‚àó, a‚Ä≤)"
RETURN M,0.5975877192982456,"=‚áíW ‚Ä≤ (ena‚Ä≤(st‚àíW ‚àó+1:t) ‚àíena#(st‚àíW ‚àó+1:t)) ‚â•W ‚àó

ena(st‚àíW ‚Ä≤+1:t) ‚àímax
a‚Ä≤‚Ä≤Ã∏=a ena‚Ä≤‚Ä≤(st‚àíW ‚Ä≤+1:t)

+ 1[a‚Ä≤ > a]"
RETURN M,0.5986842105263158,"=‚áí‚àÉa‚Ä≤‚Ä≤ Ã∏= a,"
RETURN M,0.5997807017543859,"W ‚Ä≤ (ena‚Ä≤(st‚àíW ‚àó+1:t) ‚àíena#(st‚àíW ‚àó+1:t)) ‚â•W ‚àó(ena(st‚àíW ‚Ä≤+1:t) ‚àíena‚Ä≤‚Ä≤(st‚àíW ‚Ä≤+1:t)) + 1[a‚Ä≤ > a].
For each a‚Ä≤‚Ä≤ Ã∏= a, now we use the last equation as the condition, and show that K > LW ‚àó,W ‚Ä≤"
RETURN M,0.6008771929824561,"a‚Ä≤,a‚Ä≤‚Ä≤
is a
necessary condition. This proposition is equivalent to
K ‚â§LW ‚àó,W ‚Ä≤"
RETURN M,0.6019736842105263,"a‚Ä≤,a‚Ä≤‚Ä≤"
RETURN M,0.6030701754385965,"=‚áíW ‚Ä≤ (ena‚Ä≤(st‚àíW ‚àó+1:t) ‚àíena#(st‚àíW ‚àó+1:t)) < W ‚àó(ena(st‚àíW ‚Ä≤+1:t) ‚àíena‚Ä≤‚Ä≤(st‚àíW ‚Ä≤+1:t)) + 1[a‚Ä≤ > a].
(27)"
RETURN M,0.6041666666666666,"Suppose a poisoning attack within poisoning size K changes the subpolicies in set B ‚äÜ[u]. Note
that |B| ‚â§K. We inspect the objective in Equation (27):
W ‚Ä≤ (ena‚Ä≤(st‚àíW ‚àó+1:t) ‚àíena#(st‚àíW ‚àó+1:t)) ‚àíW ‚àó(ena(st‚àíW ‚Ä≤+1:t) ‚àíena‚Ä≤‚Ä≤(st‚àíW ‚Ä≤+1:t)) ‚àí1[a‚Ä≤ > a]"
RETURN M,0.6052631578947368,"=W ‚Ä≤ (na‚Ä≤(st‚àíW ‚àó+1:t) ‚àína#(st‚àíW ‚àó+1:t)) ‚àíW ‚àó(na(st‚àíW ‚Ä≤+1:t) ‚àína‚Ä≤‚Ä≤(st‚àíW ‚Ä≤+1:t)) ‚àí1[a‚Ä≤ > a] +
X i‚ààB "
RETURN M,0.606359649122807,"W ‚Ä≤
W ‚àó‚àí1
X"
RETURN M,0.6074561403508771,"w=0
1[ÀúœÄi(st‚àíw+1) = a‚Ä≤] ‚àíW ‚Ä≤
W ‚àó‚àí1
X"
RETURN M,0.6085526315789473,"w=0
1[ÀúœÄi(st‚àíw+1) = a#]"
RETURN M,0.6096491228070176,"‚àíW ‚àó
W ‚Ä≤‚àí1
X"
RETURN M,0.6107456140350878,"w=0
1[ÀúœÄi(st‚àíw+1) = a] + W ‚àó
W ‚Ä≤‚àí1
X"
RETURN M,0.6118421052631579,"w=0
1[ÀúœÄi(st‚àíw+1) = a‚Ä≤‚Ä≤] Ô£∂ Ô£∏ ‚àí
X i‚ààB "
RETURN M,0.6129385964912281,"W ‚Ä≤
W ‚àó‚àí1
X"
RETURN M,0.6140350877192983,"w=0
1[œÄi(st‚àíw+1) = a‚Ä≤] ‚àíW ‚Ä≤
W ‚àó‚àí1
X"
RETURN M,0.6151315789473685,"w=0
1[œÄi(st‚àíw+1) = a#]"
RETURN M,0.6162280701754386,"‚àíW ‚àó
W ‚Ä≤‚àí1
X"
RETURN M,0.6173245614035088,"w=0
1[œÄi(st‚àíw+1) = a] + W ‚àó
W ‚Ä≤‚àí1
X"
RETURN M,0.618421052631579,"w=0
1[œÄi(st‚àíw+1) = a‚Ä≤‚Ä≤] Ô£∂ Ô£∏"
RETURN M,0.6195175438596491,"=W ‚Ä≤ (na‚Ä≤(st‚àíW ‚àó+1:t) ‚àína#(st‚àíW ‚àó+1:t)) ‚àíW ‚àó(na(st‚àíW ‚Ä≤+1:t) ‚àína‚Ä≤‚Ä≤(st‚àíW ‚Ä≤+1:t)) ‚àí1[a‚Ä≤ > a] +
X i‚ààB"
RETURN M,0.6206140350877193,"max{W ‚àó,W ‚Ä≤}
X"
RETURN M,0.6217105263157895,"w=0
œÉw(ÀúœÄi(st‚àíw)) ‚àíœÉw(œÄi(st‚àíw))"
RETURN M,0.6228070175438597,"‚â§W ‚Ä≤ (na‚Ä≤(st‚àíW ‚àó+1:t) ‚àína#(st‚àíW ‚àó+1:t)) ‚àíW ‚àó(na(st‚àíW ‚Ä≤+1:t) ‚àína‚Ä≤‚Ä≤(st‚àíW ‚Ä≤+1:t)) ‚àí1[a‚Ä≤ > a] +
X i‚ààB"
RETURN M,0.6239035087719298,"max{W ‚àó,W ‚Ä≤}
X"
RETURN M,0.625,"w=0
max
a0‚ààA œÉw(a0) ‚àíœÉw(œÄi(st‚àíw))"
RETURN M,0.6260964912280702,"|
{z
}
gi"
RETURN M,0.6271929824561403,"(a)
‚â§W ‚Ä≤ (na‚Ä≤(st‚àíW ‚àó+1:t) ‚àína#(st‚àíW ‚àó+1:t)) ‚àíW ‚àó(na(st‚àíW ‚Ä≤+1:t) ‚àína‚Ä≤‚Ä≤(st‚àíW ‚Ä≤+1:t)) ‚àí1[a‚Ä≤ > a] + K
X"
RETURN M,0.6282894736842105,"i=1
g(i)"
RETURN M,0.6293859649122807,Published as a conference paper at ICLR 2022
RETURN M,0.6304824561403509,"(b)
‚â§W ‚Ä≤ (na‚Ä≤(st‚àíW ‚àó+1:t) ‚àína#(st‚àíW ‚àó+1:t)) ‚àíW ‚àó(na(st‚àíW ‚Ä≤+1:t) ‚àína‚Ä≤‚Ä≤(st‚àíW ‚Ä≤+1:t)) ‚àí1[a‚Ä≤ > a] +"
RETURN M,0.631578947368421,"LW ‚àó,W ‚Ä≤"
RETURN M,0.6326754385964912,"a‚Ä≤,a‚Ä≤‚Ä≤
X"
RETURN M,0.6337719298245614,"i=1
g(i)"
RETURN M,0.6348684210526315,"(c)
<0."
RETURN M,0.6359649122807017,"Thus, Equation (27) is proved. Therefore, K > LW ‚àó,W ‚Ä≤"
RETURN M,0.6370614035087719,"a‚Ä≤,a‚Ä≤‚Ä≤
is a necessary condition for Q(W ‚àó, a‚Ä≤),
i.e., Equation (25)."
RETURN M,0.6381578947368421,"In the above deriviation, the deÔ¨Ånitions of gi, gi, and œÉw are from Equation (7). (a) comes from
the facts that {g(i)}u
i=1 is a nondecreasing permutation of {gi}u‚àí1
i=0 , gi ‚â•0, and |B| ‚â§K. (b)
comes from the assumption K ‚â§LW ‚àó,W ‚Ä≤"
RETURN M,0.6392543859649122,"a,a‚Ä≤‚Ä≤
and also g(i) ‚â•0. (c) comes from the deÔ¨Ånition in
Equation (6)."
RETURN M,0.6403508771929824,"F.6
POSSIBLE ACTION SET IN PARL AND COMPARISON"
RETURN M,0.6414473684210527,"F.6.1
CERTIFICATION"
RETURN M,0.6425438596491229,"Proof of Theorem 4. According to the deÔ¨Ånition of possible action set, we only need to prove the
contrary: for any a ‚ààA \ AT (K), within poisoning size K, the poisoned policy cannot choose a:
f
œÄP(st) Ã∏= a."
RETURN M,0.643640350877193,"According to Equation (8), any a ‚ààA \ AT (K) satisÔ¨Åes
X"
RETURN M,0.6447368421052632,"a‚Ä≤‚ààA
max{na‚Ä≤(st) ‚àína(st) ‚àíK + 1[a‚Ä≤ < a], 0} > K.
(28)"
RETURN M,0.6458333333333334,"Given poisoning size K, since each poisoning size can affect only one subpolicy, we know
ena(st) ‚â§na(st) + K
where ena denotes to the poisoned aggregated action count. We suppose the attack could be success-
ful, then for a‚Ä≤ < a, ena‚Ä≤(st) ‚â§ena(st) ‚àí1, and thus na‚Ä≤(st) ‚àíena‚Ä≤(st) ‚â•na‚Ä≤(st) ‚àína(st) ‚àíK + 1.
Similarly, for a‚Ä≤ > a, ea‚Ä≤(st) ‚â§ena(st), and thus na‚Ä≤(st)‚àíena‚Ä≤(st) ‚â•na‚Ä≤(st)‚àína(st)‚àíK. Also, for
any a‚Ä≤ Ã∏= a after poisoning na‚Ä≤(st) ‚àíena‚Ä≤(st) ‚â•0; otherwise deviating the difference subpolicies‚Äô
decisions‚Äô from a‚Ä≤ to a is strictly no-worse. Given these facts, the amount of votes that need to be
reduced is the LHS of Equation (28) which is larger than K. However, we only have K poisoning
size, i.e., K votes that can be reduced. As a result, our assumption that the attack could be successful
is falsiÔ¨Åed and the poisoned policy cannot choose a."
RETURN M,0.6469298245614035,"Corollary 12 (Loose PARL Action Set). Under the condition of DeÔ¨Ånition 5, suppose the aggrega-
tion protocol is PARL as deÔ¨Åned in DeÔ¨Ånition 7, then the possible action set at step t"
RETURN M,0.6480263157894737,"AL(K) =

a ‚ààA
 max
a‚Ä≤‚ààA na‚Ä≤(st) ‚àína(st) ‚â§2K ‚àí1[a > arg max
a‚Ä≤‚ààA
na‚Ä≤(st)]

.
(29)"
RETURN M,0.6491228070175439,"Proof of Corollary 12. Again, we prove the contrary, for any a ‚ààA\AL(K), within poisoning size
K, the poisoned policy cannot choose a: f
œÄP(st) Ã∏= a."
RETURN M,0.6502192982456141,"According to Equation (29), let am = arg maxa‚Ä≤‚ààA na‚Ä≤(st), then any a ‚ààA \ AL(K) satisÔ¨Åes
nam(st) ‚àína(st) > 2K ‚àí1[a > am].
After poisoning, we thus have
nam(st) ‚àína(st) > ‚àí1[a > am] =‚áínam(st) ‚àína(st) ‚â•1 ‚àí1[a > am]
From the deÔ¨Ånition, a /‚ààAL(K) so a Ã∏= am. If am < a, nam(st) ‚â•na(st); if am > a, nam(st) >
na(st). In both cases, am has higher priority to be chosen than a, and thus the poisoned policy
cannot choose a."
RETURN M,0.6513157894736842,"F.6.2
COMPARISON"
RETURN M,0.6524122807017544,"Theorem 13. Under the condition of DeÔ¨Ånition 5, suppose the aggregation protocol is PARL as de-
Ô¨Åned in DeÔ¨Ånition 7, AT (K), AL(K) are deÔ¨Åned according to Equations (8) and (29) accordingly,
then"
RETURN M,0.6535087719298246,"1. AT (K) ‚äÜAL(K); and there are subpolicies {œÄi}u‚àí1
i=0 and state st such that AT (K) ‚ääAL(K)."
RETURN M,0.6546052631578947,Published as a conference paper at ICLR 2022
RETURN M,0.6557017543859649,"2. Given subpolicies {œÄi}u‚àí1
i=0 and state st, for any a ‚ààAT (K), there exists a poisoned training set
eD whose poisoning size |D ‚äñeD| ‚â§K and some RL training mechanism, such that f
œÄP(st) = a
where f
œÄP is the poisoned PARL policy trained on eD."
RETURN M,0.6567982456140351,Proof of Theorem 13. We prove the two arguments separately.
RETURN M,0.6578947368421053,"1. We Ô¨Årst prove AT (K) ‚äÜAL(K). For any a ‚ààAT (K), let am = arg maxa‚Ä≤‚ààA na‚Ä≤(st), then
X"
RETURN M,0.6589912280701754,"a‚Ä≤‚ààA
max{na‚Ä≤(st) ‚àína(st) ‚àíK + 1[a‚Ä≤ < a], 0} ‚â§K"
RETURN M,0.6600877192982456,"=‚áínam(st) ‚àína(st) ‚àíK + 1[am < a] ‚â§K
=‚áínam(st) ‚àína(st) ‚â§2K ‚àí1[a > am]
where the last proposition is exactly the set selector of AL(K) so a ‚ààAL(K)."
RETURN M,0.6611842105263158,"We then prove that AT (K) ‚ääAL(K) can happen by construction. Suppose that there are three
actions in the action space: A = {a1, a2, a3}. We construct subpolicies for current state st such
that the aggregated action counts are
na1(st) = 10, na2(st) = 9, na3(st) = 1.
Given poisoning size K = 5, we Ô¨Ånd that
na1(st) ‚àína3(st) = 9 ‚â§10 ‚àí1[a3 ‚â•a1] = 9
=‚áía3 ‚ààaL(K),
(na1(st) ‚àína3(st) ‚àíK + 1[a1 < a3]) +"
RETURN M,0.6622807017543859,"(na2(st) ‚àína3(st) ‚àíK + 1[a2 < a3]) = 9 > K = 5
=‚áía3 /‚ààaT (K).
Therefore, AT (K) ‚ääAL(K) for these subpolicies and state st."
RETURN M,0.6633771929824561,"2. We prove by construction. For any a ‚ààAT (K), we construct the set of subpolicies to poison
Ba ‚äÜ[u] such that |Ba| ‚â§K, then describe the corresponding poisoned dataset eDa, and Ô¨Ånally
prove that the poisoned policy f
œÄP
a(st) = a."
RETURN M,0.6644736842105263,"For a ‚ààAT (K), by deÔ¨Ånition (Equation (8)), we know
X"
RETURN M,0.6655701754385965,"a‚Ä≤‚ààA
max{na‚Ä≤(st) ‚àína(st) ‚àíK + 1[a‚Ä≤ < a]
|
{z
}
:=ta,a‚Ä≤"
RETURN M,0.6666666666666666,", 0} ‚â§K.
(30)"
RETURN M,0.6677631578947368,"We now deÔ¨Åne a set of actions Ca ‚äÜA such that
Ca = {a‚Ä≤ ‚ààA | ta,a‚Ä≤ > 0}.
According to this deÔ¨Ånition, a /‚ààCa since ta,a ‚â§0."
RETURN M,0.668859649122807,"Fact F.1. For a ‚ààAT (K) and any a‚Ä≤ ‚ààA, na(st) + K ‚àí1[a‚Ä≤ < a] ‚â•0."
RETURN M,0.6699561403508771,"Proof of Fact F.1. Suppose na(st) + K ‚àí1[a‚Ä≤ < a] ‚â§0, then na(st) = K = 0 and a‚Ä≤ < a.
Then
X"
RETURN M,0.6710526315789473,"a‚Ä≤‚ààA
max{na‚Ä≤(st)‚àína(st)‚àíK+1[a‚Ä≤ < a], 0} ‚â•
X"
RETURN M,0.6721491228070176,"a‚Ä≤‚ààA
na‚Ä≤(st)‚àína(st)‚àíK =
X"
RETURN M,0.6732456140350878,"a‚Ä≤‚ààA
na‚Ä≤(st) = u > 0"
RETURN M,0.6743421052631579,which contradicts the requirement that the LHS of the above inequality should be ‚â§K = 0.
RETURN M,0.6754385964912281,"Give Fact F.1, for any a‚Ä≤ ‚ààCa, na‚Ä≤(st) ‚â•ta,a‚Ä≤. Notice that na‚Ä≤(st) is the number of subpolicies
that vote for action a‚Ä≤ at state st. Therefore, we can pick an arbitrary subset of those subpolicies
whose cardinality is ta,a‚Ä≤. We denote Ba
a‚Ä≤ to such subset:
Ba
a‚Ä≤ ‚äÜ{i ‚àà[u] | œÄi(st) = a‚Ä≤}, |Ba
a‚Ä≤| = ta,a‚Ä≤.
Now deÔ¨Åne Ba
Œ±: Ba
Œ± = S"
RETURN M,0.6765350877192983,"a‚Ä≤‚ààCa Ba
a‚Ä≤. We construct Ba
Œ≤ to be an arbitrary subset of those subpoli-
cies whose prediction is not a and who are not in Ba
Œ±, and limit the Ba
Œ≤‚Äôs cardinality:
Ba
Œ≤ ‚äÜ{i ‚àà[u] | œÄi(st) Ã∏= a} \ Ba
Œ±, |Ba
Œ≤| = min{K ‚àí|Ba
Œ±|, u ‚àína(st) ‚àí|Ba
Œ±|}.
Such Ba
Œ≤ can be selected, because:"
RETURN M,0.6776315789473685,"‚Ä¢ From deÔ¨Ånition, Ba
Œ± ‚äÜ{i ‚àà[u] | œÄi(st) Ã∏= a}, where the cardinality of {i ‚àà[u] | œÄi(st) Ã∏=
a} is u ‚àína(st). So 0 ‚â§u ‚àína(st) ‚àí|Ba
Œ±|.
‚Ä¢ Since"
RETURN M,0.6787280701754386,"|Ba
Œ±| ‚â§
X"
RETURN M,0.6798245614035088,"a‚Ä≤‚ààCa
|Ba
a‚Ä≤| =
X"
RETURN M,0.680921052631579,"a‚Ä≤‚ààCa
ta,a‚Ä≤ =
X"
RETURN M,0.6820175438596491,"a‚Ä≤‚ààA,ta,a‚Ä≤>0
ta,a‚Ä≤
(‚àó)
‚â§K,"
RETURN M,0.6831140350877193,Published as a conference paper at ICLR 2022
RETURN M,0.6842105263157895,"K ‚àí|Ba
Œ±| ‚â•0, and thus |Ba
Œ≤| ‚â•0. Here (‚àó) is due to Equation (30)."
RETURN M,0.6853070175438597,"‚Ä¢ The superset {i ‚àà[u] | œÄi(st) Ã∏= a} \ Ba
Œ± has cardinality u ‚àína(st) ‚àí|Ba
Œ±| and |Ba
Œ≤| ‚â§
u ‚àína(st) ‚àí|Ba
Œ±|."
RETURN M,0.6864035087719298,"To this point, we can deÔ¨Åne the set of subpolicies to poison:
Ba := Ba
Œ± ‚à™Ba
Œ≤,
and |Ba| = |Ba
Œ±| + |Ba
Œ≤| ‚â§K."
RETURN M,0.6875,"In a similar fashion as the attack construction in proof of Proposition 9, for each i ‚ààBa, we
locate its corresponding partitioned dataset Di for training subpolicy œÄa
i . We inset one trajectory
pa
i to Di such that our chosen learning algorithm M0 can train a subpolicy ÀúœÄa
i = M0(Di‚à™{pa
i })
such that ÀúœÄa
i (st) = a. For example, the trajectory could be pa
i = {(st, a, s‚Ä≤, ‚àû)} where s‚Ä≤ is an
arbitrary state that guarantees pa
i is hashed to partition i; and M0 learns the action with maximum
reward for the memorized nearest state. Then, the poisoned dataset eDa = ["
RETURN M,0.6885964912280702,"i‚ààBa
Di ‚à™{pa
i } ! ‚à™ Ô£´ Ô£≠
["
RETURN M,0.6896929824561403,"i‚àà[u]\B‚Ä≤
Di Ô£∂ Ô£∏."
RETURN M,0.6907894736842105,"Thus, eDa is |D ‚äñeDa| = |Ba| ‚â§K, i.e., the constructed attack‚Äôs poisoning size is within K."
RETURN M,0.6918859649122807,"We now analyze the action prediction of the poisoned policy f
œÄP. For action a, after poisoning,
ena(st) = na(st)+|Ba| = na(st)+min{K, u‚àína(st)} = min{na(st)+K, u}. If ena(st) = u,
then all subpolicies vote for a, apparently f
œÄP
a(st) = a; otherwise, ena(st) = na(st) + K. In
this case, for any action a‚Ä≤ ‚ààCa, since we at least choose subpolicies in Ba
a‚Ä≤ and change their
action prediction to a, the aggregated action count after poisoning is ena‚Ä≤(st) ‚â§na‚Ä≤(st)‚àí|Ba
a‚Ä≤| =
na‚Ä≤(st)‚àíta,a‚Ä≤ = na‚Ä≤(st)‚àína‚Ä≤(st)+na(st)+K‚àí1[a‚Ä≤ < a] = na(st)+K‚àí1[a‚Ä≤ < a] = Àúna(st)‚àí
1[a‚Ä≤ < a]. Thus, a‚Ä≤ has lower priority to be chosen than a. For any action a‚Ä≤ /‚ààCa and a‚Ä≤ Ã∏= a,
by the deÔ¨Ånition of Ca, ta,a‚Ä≤ = na‚Ä≤(st)‚àína(st)‚àíK +1[a‚Ä≤ < a] ‚â§0. After poisoning, the vote
of a‚Ä≤ does not increase, i.e., ena‚Ä≤(st) ‚â§na‚Ä≤(st) ‚â§na(st) + K ‚àí1[a‚Ä≤ < a] = ena(st) ‚àí1[a‚Ä≤ < a].
Thus, a‚Ä≤ also has lower priority to be chosen than a. In conclusion, we have f
œÄP
a(st) = a."
RETURN M,0.6929824561403509,"To this point, for any a ‚ààAT (K), we successfully construct the corresponding poisoned dataset
eDa within poisoning size K such that f
œÄP
a(st) = a, thus concludes the proof."
RETURN M,0.694078947368421,"F.7
POSSIBLE ACTION SET IN TPARL"
RETURN M,0.6951754385964912,"Proof of Theorem 5. For ease of notation, we let w = min{W, t + 1}, so w is the actual window
size used at step t. We let t0 = t ‚àíw + 1, i.e., t0 is the actual start time step for TPARL aggregation
at our current step t. Now we can write chosen action at step t without poisoning as œÄT(st0:t)."
RETURN M,0.6962719298245614,"We only need to prove the contrary: for any a ‚ààA \ A(K), within poisoning size K, the poisoned
policy cannot choose a: f
œÄT(st0:t) Ã∏= a."
RETURN M,0.6973684210526315,"According to Equation (9), for such a, there exists a‚Ä≤ Ã∏= a such that
K
X"
RETURN M,0.6984649122807017,"i=1
h(i)
a‚Ä≤,a ‚â§Œ¥a‚Ä≤,a = na‚Ä≤(st0:t) ‚àína(st0:t) ‚àí1[a < a‚Ä≤].
(31)"
RETURN M,0.6995614035087719,"Suppose there exists such poisoning attack that lets f
œÄT(st0:t) = a. This implies that for a‚Ä≤, after
poisoning, we have
ena‚Ä≤(st0:t) ‚àíena‚Ä≤(st0:t) ‚àí1[a‚Ä≤ < a] < 0,
(32)
where ena is the aggregated action count after poisoning. Since the poisoning size is within K, it
can affect at most K subpolicies. We let B ‚äÜ[u], |B| ‚â§K to represent the affected subpolicy set.
Therefore,
ena‚Ä≤(st0:t) ‚àíena(st0:t) ‚àí1[a‚Ä≤ < a]"
RETURN M,0.7006578947368421,"= na‚Ä≤(st0:t) ‚àína(st0:t) ‚àí1[a‚Ä≤ < a]
|
{z
}
Œ¥a‚Ä≤,a"
RETURN M,0.7017543859649122,+(ena‚Ä≤(st0:t) ‚àína‚Ä≤(st0:t)) ‚àí(ena(st0:t) ‚àína(st0:t))
RETURN M,0.7028508771929824,Published as a conference paper at ICLR 2022
RETURN M,0.7039473684210527,"=Œ¥a‚Ä≤,a +
X i‚ààB Ô£´"
RETURN M,0.7050438596491229,"Ô£≠
w‚àí1
X"
RETURN M,0.706140350877193,"j=0
1[ÀúœÄi(st‚àíj) = a‚Ä≤, œÄi(st‚àíj) Ã∏= a‚Ä≤] ‚àí w‚àí1
X"
RETURN M,0.7072368421052632,"j=0
1[ÀúœÄi(st‚àíj) Ã∏= a‚Ä≤, œÄi(st‚àíj) = a‚Ä≤] Ô£∂ Ô£∏ ‚àí
X i‚ààB Ô£´"
RETURN M,0.7083333333333334,"Ô£≠
w‚àí1
X"
RETURN M,0.7094298245614035,"j=0
1[ÀúœÄi(st‚àíj) = a, œÄi(st‚àíj) Ã∏= a] ‚àí w‚àí1
X"
RETURN M,0.7105263157894737,"j=0
1[ÀúœÄi(st‚àíj) Ã∏= a, œÄi(st‚àíj) = a] Ô£∂ Ô£∏"
RETURN M,0.7116228070175439,"‚â•Œ¥a‚Ä≤,a ‚àí
X i‚ààB Ô£´"
RETURN M,0.7127192982456141,"Ô£≠
w‚àí1
X"
RETURN M,0.7138157894736842,"j=0
1[ÀúœÄi(st‚àíj) Ã∏= a‚Ä≤, œÄi(st‚àíj) = a‚Ä≤] + w‚àí1
X"
RETURN M,0.7149122807017544,"j=0
1[ÀúœÄi(st‚àíj) = a, œÄi(st‚àíj) Ã∏= a] Ô£∂ Ô£∏"
RETURN M,0.7160087719298246,"‚â•Œ¥a‚Ä≤,a ‚àí
X i‚ààB Ô£´"
RETURN M,0.7171052631578947,"Ô£≠
w‚àí1
X"
RETURN M,0.7182017543859649,"j=0
1[œÄi(st‚àíj) = a‚Ä≤] + w‚àí1
X"
RETURN M,0.7192982456140351,"j=0
1[œÄi(st‚àíj) Ã∏= a] Ô£∂ Ô£∏"
RETURN M,0.7203947368421053,"=Œ¥a‚Ä≤,a ‚àí
X i‚ààB Ô£´"
RETURN M,0.7214912280701754,"Ô£≠
w‚àí1
X"
RETURN M,0.7225877192982456,"j=0
1i,a‚Ä≤(st‚àíj) + w ‚àí w‚àí1
X"
RETURN M,0.7236842105263158,"j=0
1i,a(st‚àíj) Ô£∂ Ô£∏"
RETURN M,0.7247807017543859,"=Œ¥a‚Ä≤,a ‚àí
X"
RETURN M,0.7258771929824561,"i‚ààB
hi,a‚Ä≤,a"
RETURN M,0.7269736842105263,"(a)
‚â•Œ¥a‚Ä≤,a ‚àí |B|
X"
RETURN M,0.7280701754385965,"i=1
h(i)
a‚Ä≤,a"
RETURN M,0.7291666666666666,"(b)
‚â•Œ¥a‚Ä≤,a ‚àí K
X"
RETURN M,0.7302631578947368,"i=1
h(i)
a‚Ä≤,a"
RETURN M,0.731359649122807,"(c)
‚â•0."
RETURN M,0.7324561403508771,"This contradicts with Equation (32), and thus the assumption is falsiÔ¨Åed, i.e., there is no such poi-
soning attack that let f
œÄT(st0:t) = a. In the above equations, (a) is due to the fact that h(i)
a‚Ä≤,a is a"
RETURN M,0.7335526315789473,"nonincreasing permutation of {hi,a‚Ä≤,a}u‚àí1
i=0 . (b) is due to the facts that |B| ‚â§K and h(i)
a‚Ä≤,a ‚â•0. (c)
comes from Equation (31)."
RETURN M,0.7346491228070176,"F.8
HARDNESS FOR COMPUTING TIGHT POSSIBLE ACTION SET IN TPARL"
RETURN M,0.7357456140350878,"Proof of Theorem 6. By DeÔ¨Ånition 5, the possible action set with minimum cardinality (called min-
imal possible action set hereinafter) is unique. Otherwise, suppose A and B are both minimal
possible action set, but A Ã∏= B, then A ‚à©B is a smaller and valid possible action set. Therefore,
the oracle that returns the minimal possible action set, denoted by MINSET, can tell whether any
action a ‚ààMINSET and thus whether any action a can be chosen by some poisoned policy eœÄ whose
poisoning size is within K. In other words, the problem of determining whether an action a can
be chosen by some poisoned policy ÀúœÄ whose poisoning size is within K, denoted by ATTKACT,
is polynomially equivalent to MINSET: MINSET ‚â°P ATTKACT. Now, we show a polynomial
reduction from the set cover problem to ATTKACT, which implies that our MINSET problem is an
NP-complete problem."
RETURN M,0.7368421052631579,"The decision version of the set cover problem (Karp, 1972), denoted by SETCOVER, is a well-
known NP-complete problem and is deÔ¨Åned as follows. The inputs are"
RETURN M,0.7379385964912281,"1. a universal set of n elements: U = {u1, u2, ¬∑ ¬∑ ¬∑ , un};"
RETURN M,0.7390350877192983,"2. a set of subsets of U: V = {V1, ¬∑ ¬∑ ¬∑ , Vm}, Vi ‚äÜU, 1 ‚â§i ‚â§m, Sm
i=1 Vi = U;"
RETURN M,0.7401315789473685,3. a positive number K ‚ààR+.
RETURN M,0.7412280701754386,"The output is a boolean variable b, indicating that whether there exists a subset W ‚äÜV, |W| ‚â§K,
such that ‚àÄui ‚ààU, ‚àÉV ‚ààW, ui ‚ààV . Given an oracle to ATTKACT, we need to show SETCOVER
can be solved in polynomial time, i.e., SETCOVER ‚â§P ATTKACT."
RETURN M,0.7423245614035088,"‚Ä¢ If K ‚â•n:
We scan all sets Vi ‚ààV. To being with, we have a record set S ‚Üê‚àÖ, and an answer set W ‚Üê‚àÖ.
Whenever we encounter a set that contains a new element ui /‚ààS, we put W ‚ÜêW ‚à™{Vj},
and record this element S ‚ÜêS ‚à™{ui}. After one scan pass, W covers all elements of U (since
Sm
j=1 Vj = U), and |W| ‚â§n ‚â§K. Therefore, W is a valid set cover. Since we can always Ô¨Ånd
such W, we can directly answer true."
RETURN M,0.743421052631579,Published as a conference paper at ICLR 2022
RETURN M,0.7445175438596491,"‚Ä¢ If K ‚â•m:
We can directly return V as a valid set cover, since Sm
j=1 Vj = U and |V| ‚â§m ‚â§K. Thus, we
can answer true."
RETURN M,0.7456140350877193,"‚Ä¢ If K < min{n, m}:
This is the general case which we need to handle. Now we construct the ATTKACT(K) problem
so that we can trigger its oracle to solve SETCOVER."
RETURN M,0.7467105263157895,"1. The poisoning size is K.
2. The action space A = U ‚à™{b} ‚à™Œì where Œì := {#1, . . . , #m2n}. (|A| = n + 1 + m2n.)
The sorting of actions is u1 < u2 < ¬∑ ¬∑ ¬∑ < un < b < #1 < ¬∑ ¬∑ ¬∑ < #m2n.
3. The subpolicies are {œÄa
j }m
j=1 ‚à™{œÄb
i , œÄc
i,j
| 1 ‚â§i ‚â§n, 1 ‚â§j ‚â§K ‚àí1}, where œÄa
j
corresponds to Vj ‚ààV, and œÄb
i , œÄc
i,j correspond to ui ‚ààU. (Number of subpolicies u =
m + Kn ‚â§m + n2.)
4. The current time step is t = nm, and the window size W = nm.
5. The input action is b, i.e., asking whether b can be chosen by some poisoned TPARL policy,
i.e., f
œÄT(s1:t) = b, if the poisoning size is within K."
RETURN M,0.7478070175438597,"Now, we construct the states at each step t (1 ‚â§t ‚â§nm) so that the subpolicies‚Äô action predictions
at these steps are as follows."
RETURN M,0.7489035087719298,"1. Count the appearing time of each ui in V, and denote it by ci: ci = Pm
j=1 1[ui ‚ààVj]. For
each ui, select a Vj0 that contains ui, and in the corresponding œÄa
j0, assign m ‚àíci + 1 steps
to predict ui; for all other Vj that contains ui, in the corresponding œÄa
j , assign one step to
predict ui.
2. After this process, each œÄa
j at least has one time step whose action prediction is ui for each
ui ‚ààVj. Among all {œÄa
j }m
j=1 and all time steps 1 ‚â§t ‚â§nm, mn step-action cells are Ô¨Ålled,
and the remaining (m2n ‚àímn) cells are Ô¨Ålled by #l ‚ààŒì sequentially.
3. For each œÄb
i , arbitrarily select W ‚àím time steps to assign action prediction as ui; and Ô¨Åll in
other m time steps by remaining #l ‚ààŒì sequentially.
4. For each œÄc
i,j, for all time steps, let the action prediction be ui."
RETURN M,0.75,"As we can observe, the number of actions, the number of subpolicies, and the window size are all
bounded by a polynomial of n and m. Therefore, such construction can be done in polynomial
time."
RETURN M,0.7510964912280702,"We then show SETCOVER = true ‚áê‚áí‚àÉK‚Ä≤, b ‚ààATTKACT(K‚Ä≤), 1 ‚â§K‚Ä≤ ‚â§K."
RETURN M,0.7521929824561403,"‚Äì =‚áí:
Suppose the covering set is W ‚äÜV, we denote K‚Ä≤ to |W|, and construct the ATTKACT
problem with poisoning size K‚Ä≤ as described above.
We can construct a poisoning strategy to let f
œÄT(s1:nm) = b, The poisoning strategy is to
Ô¨Ånd out œÄa
j for each Vj ‚ààW, and to let them predict action b throughout all time steps:
eœÄa
j (s‚Ä≤
t) = b, 1 ‚â§t‚Ä≤ ‚â§nm.
After poisoning, the aggregated action count enb(s1:nm) = |W| √ó nm = K‚Ä≤nm. Since W
covers every ui ‚ààU, for each ui ‚ààU there exists a set Vj ‚àãui, whose corresponding eœÄa
j is
poisoned to predict b. Thus, enui(s1:nm) < nui(s1:nm) = m + (W ‚àím) + W √ó (K‚Ä≤ ‚àí1) =
WK‚Ä≤ = K‚Ä≤nm. For any #l ‚ààŒì, en#l(s1:nm) ‚â§n#l(s1:nm) = 1. In summary,
enui(s1:nm) < K‚Ä≤nm, enb(s1:nm) = K‚Ä≤nm, en#l(s1:nm) = 1.
Thus, after TPARL aggregation, the poisoned policy f
œÄT(s1:nm) = b, and therefore b ‚àà
ATTKACT(K‚Ä≤).
‚Äì ‚áê=:
Suppose it is K‚Ä≤ that let b ‚ààATTKACT(K‚Ä≤), which implies that there exists such a poisoning
attack within size K‚Ä≤ that misleads the poisoned policy to b: f
œÄT(s1:nm) = b. Since the
poisoning size is K‚Ä≤, after poisoning the aggregated action count
enb(s1:nm) ‚â§K‚Ä≤W = K‚Ä≤nm.
For each ui ‚ààU, since nui(s1:nm) = m + (W ‚àím) + W √ó (K‚Ä≤ ‚àí1) = K‚Ä≤nm, we always
have"
RETURN M,0.7532894736842105,"enui(s1:nm)
(‚àó)
< enb(s1:nm) ‚â§nui(s1:nm),
(33)"
RETURN M,0.7543859649122807,Published as a conference paper at ICLR 2022
RETURN M,0.7554824561403509,"where (‚àó) is due to the condition of successful attack. We denote the set of poisoned subpoli-
cies by Œ† (|Œ†| ‚â§K‚Ä≤). Therefore, Equation (33) implies that for each ui ‚ààU, there exists at
least one subpolicy
œÄ‚Ä≤
ui ‚ààŒ†, œÄ‚Ä≤
ui ‚àà{œÄa
j | ui ‚ààVj} ‚à™{œÄb
i } ‚à™{œÄc
i,j | 1 ‚â§j ‚â§K ‚àí1}
(34)
that is poisoned by the attack, otherwise the aggregated vote enui cannot change.
We partition Œì by Œìa and Œìbc, where
Œìa = Œì ‚à©{œÄa
j }m
j=1, Œìb = Œì ‚à©{œÄb
i , œÄc
i,j | 1 ‚â§i ‚â§n, 1 ‚â§j ‚â§K ‚àí1}.
We construct additional poisoning set Œìa
+ following this process: In the beginning, Œìa
+ ‚Üê‚àÖ.
For each ui ‚ààU, if Œìa ‚à©{œÄa
j
| ui ‚ààVj} is not empty, skip. Otherwise, according to
Equation (34), Œìb ‚à©{œÄb
i , œÄc
i,j | 1 ‚â§j ‚â§K ‚àí1} is not empty. In this case, we Ô¨Ånd
an arbitrary covering set of ui, namely Vj0 ‚àãui, and put œÄa
j0 into Œìa
+. When the process
terminates, we Ô¨Ånd that for each ui ‚ààU,
(Œìa ‚à™Œìa
+) ‚à©{œÄa
j | ui ‚ààVj} Ã∏= ‚àÖ.
(35)
Following the mapping {œÄa
j }m
j=1 ‚Üê‚Üí{Vj | 1 ‚â§j ‚â§m} = V, the subset (Œìa ‚à™Œìa
+) ‚äÜ
{œÄa
j }m
j=1 can be mapped to (Wa ‚à™Wa
+) ‚äÜV. From Equation (35), (Wa ‚à™Wa
+) is a valid set
cover for U. We now study the cardinality of this set cover. From the process, we know that
every Vj0 ‚ààWa
+ corresponds to a different set in Œìb. Thus, |Wa
+| ‚â§|Œìb|, which implies that
|Wa ‚à™Wa
+| ‚â§|Wa| + |Wa
+| ‚â§|Œìa| + |Œìb| = |Œì| ‚â§K‚Ä≤.
To this point, we successfully construct a set cover within cardinally K‚Ä≤ ‚â§K that covers U,
so SETCOVER = true."
RETURN M,0.756578947368421,"Therefore, we can check whether SETCOVER = true by iterating K‚Ä≤ from 1 to K (<
min{n, m} iterations), constructing the ATTKACT(K‚Ä≤) problem, and querying the oracle.
The whole process can be done in polynomial time assumping O(1) computation time of the
ATTKACT(K‚Ä≤) oracle."
RETURN M,0.7576754385964912,"To this point, we have shown SETCOVER ‚â§P ATTKACT. On the other hand, an undeterminisitic
Turing machine can try different poisoning strategies by branching on whether to poison current
subpolicy and what actions to be assigned to each poisoned subpolicy. The decision of whether the
poisoning is successful can be done in polynomial time and b ‚ààATTKACT(K) corresponds to the
existence of successfully attacked branches. Thus, ATTKACT ‚ààNP. Given that SETCOVER is
an NP-complete problem, so does ATTKACT and MINSET (since MINSET ‚â°P ATTKACT)."
RETURN M,0.7587719298245614,"F.9
POSSIBLE ACTION SET IN DPARL"
RETURN M,0.7598684210526315,"Proof of Theorem 7. For ease of notation, we assume that Wmax ‚â§t + 1, and otherwise we let
Wmax ‚Üêt + 1. We let t0 = max{t ‚àíWmax + 1, 0} be the start time step of the maximum possible
window."
RETURN M,0.7609649122807017,"We only need to prove the contrary: for any a ‚ààA \ A(K), within poisoning size K, the poisoned
policy cannot choose a: f
œÄD(st0:t) Ã∏= a. We prove by contradiction: we assume that there exists
such a poisoning attack within poisoning size K that lets f
œÄD(st0:t) = a. From the expression of
A(K) (Equation (10)), a Ã∏= at = œÄD(st0:t). Suppose the selected time window before the attack is
W ‚Ä≤ (selected according to Equation (1) based on ‚àÜW
t
and na), and the selected time window after
the attack is f
W ‚Ä≤ (selected according to Equation (1) based on e‚àÜW
t
and ena)."
RETURN M,0.7620614035087719,"‚Ä¢ If f
W ‚Ä≤ = W ‚Ä≤:
Suppose we use the TPARL aggregation policy with window size W = W ‚Ä≤ instead of current
DPARL aggregation policy, then we will have œÄT(st‚àíW ‚Ä≤+1:t) = at and f
œÄT(st‚àíW ‚Ä≤+1:t) = a.
Thus, according to the deÔ¨Ånition of possible action set (DeÔ¨Ånition 5), a ‚ààA(K) where A(K) is
deÔ¨Åned by Equation (9) in Theorem 5. This implies that a ‚ààA(K) where A(K) is deÔ¨Åned by
Equation (10), which contradicts the assumption that a ‚ààA \ A(K)."
RETURN M,0.7631578947368421,"‚Ä¢ If f
W ‚Ä≤ Ã∏= W ‚Ä≤:
According to the deÔ¨Ånition in Equation (10),
min
1‚â§W ‚àó‚â§Wmax,W ‚àóÃ∏=W ‚Ä≤,a‚Ä≤‚Ä≤Ã∏=at LW ‚àó,W ‚Ä≤"
RETURN M,0.7642543859649122,"a,a‚Ä≤‚Ä≤
> K."
RETURN M,0.7653508771929824,Published as a conference paper at ICLR 2022
RETURN M,0.7664473684210527,"We deÔ¨Åne a#
t = arg maxa0Ã∏=at ena0(st‚àíW ‚Ä≤+1:t). Then, the above equation implies that"
RETURN M,0.7675438596491229,"L
f
W ‚Ä≤,W ‚Ä≤"
RETURN M,0.768640350877193,"a,a#
t
> K
and thus
L
f
W ‚Ä≤,W ‚Ä≤"
RETURN M,0.7697368421052632,"a,a#
t
X"
RETURN M,0.7708333333333334,"i=1
g(i) + W ‚Ä≤(n
f
W ‚Ä≤
a
‚àín
f
W ‚Ä≤
a# ) ‚àíf
W ‚Ä≤(nW ‚Ä≤
at ‚àínW ‚Ä≤"
RETURN M,0.7719298245614035,"a#
t ) ‚àí1[a > at] < 0."
RETURN M,0.7730263157894737,"Since g(i) ‚â•0 by deÔ¨Ånition (Equation (7)),
K
X"
RETURN M,0.7741228070175439,"i=1
g(i) + W ‚Ä≤(n
f
W ‚Ä≤
a
‚àín
f
W ‚Ä≤
a# ) ‚àíf
W ‚Ä≤(nW ‚Ä≤
at ‚àínW ‚Ä≤"
RETURN M,0.7752192982456141,"a#
t ) ‚àí1[a > at] < 0,
(36)"
RETURN M,0.7763157894736842,"where a# = arg maxa0Ã∏=a‚Ä≤,a0‚ààA na0(st‚àíf
W ‚Ä≤+1:t) and nw
a is a shorthand of na(st‚àíw+1:t)."
RETURN M,0.7774122807017544,"Following the derivation from Equation (27) to (a), we have
K
X"
RETURN M,0.7785087719298246,"i=1
g(i)+W ‚Ä≤(n
f
W ‚Ä≤
a ‚àín
f
W ‚Ä≤
a# )‚àíf
W ‚Ä≤(nW ‚Ä≤
at ‚àínW ‚Ä≤"
RETURN M,0.7796052631578947,"a#
t )‚àí1[a > at] ‚â•W ‚Ä≤(en
f
W ‚Ä≤
a ‚àíen
f
W ‚Ä≤
a# )‚àíf
W ‚Ä≤(enW ‚Ä≤
at ‚àíenW ‚Ä≤"
RETURN M,0.7807017543859649,"a#
t )‚àí1[a > at]."
RETURN M,0.7817982456140351,"Combined with Equation (36),
W ‚Ä≤(en
f
W ‚Ä≤
a
‚àíen
f
W ‚Ä≤
a# ) ‚àíf
W ‚Ä≤(enW ‚Ä≤
at ‚àíenW ‚Ä≤"
RETURN M,0.7828947368421053,"a#
t ) ‚àí1[a > at] < 0.
(37)"
RETURN M,0.7839912280701754,"On the other hand, the successful attack assumption, i.e., f
œÄD(st0:t) = a and œÄD(st0:t) = at,
implies that"
RETURN M,0.7850877192982456,"e‚àÜ
f
W ‚Ä≤
t
= enf
W ‚Ä≤
a
‚àíenf
W ‚Ä≤
a(2)
f
W ‚Ä≤
>
enW ‚Ä≤
aW ‚Ä≤ ‚àíenW ‚Ä≤"
RETURN M,0.7861842105263158,"a(2)
W ‚Ä≤
W ‚Ä≤
= e‚àÜW ‚Ä≤
t
,
(38)"
RETURN M,0.7872807017543859,"where ‚Äú>‚Äù is ‚Äú‚â•‚Äù if a < at. In the above equation,
a(2) = arg max
a0Ã∏=a,a0‚ààA
en
f
W ‚Ä≤
a0 , aW ‚Ä≤ = arg max
a0‚ààA
enW ‚Ä≤
a0 , a(2)
W ‚Ä≤ =
arg max
a0Ã∏=aW ‚Ä≤,a0‚ààA
enW ‚Ä≤
a0 ."
RETURN M,0.7883771929824561,"Intuitively, after poisoning, a(2) is the runner-up action at window f
W ‚Ä≤, aW ‚Ä≤ is the action at window
W ‚Ä≤, and a(2)
W ‚Ä≤ is the runner-up action at window W ‚Ä≤. We rewrite Equation (38) to
W ‚Ä≤(en
f
W ‚Ä≤
a
‚àíen
f
W ‚Ä≤
a(2)) ‚â•f
W ‚Ä≤(enW ‚Ä≤
aW ‚Ä≤ ‚àíenW ‚Ä≤"
RETURN M,0.7894736842105263,"a(2)
W ‚Ä≤) + 1[a > at].
(39)"
RETURN M,0.7905701754385965,We have the following two observations:
ENF,0.7916666666666666,"1. enf
W ‚Ä≤
a
‚àíenf
W ‚Ä≤
a# ‚â•enf
W ‚Ä≤
a
‚àíenf
W ‚Ä≤
a(2), since a is the top action and a(2) is the runner-up action and their
margin should be the smallest."
ENF,0.7927631578947368,"2. enW ‚Ä≤
aW ‚Ä≤ ‚àíenW ‚Ä≤"
ENF,0.793859649122807,"a(2)
W ‚Ä≤ ‚â•enW ‚Ä≤
at ‚àíenW ‚Ä≤"
ENF,0.7949561403508771,"a#
t , because 1) if at = aW ‚Ä≤, LHS equals to RHS; 2) if at Ã∏= aW ‚Ä≤,"
ENF,0.7960526315789473,LHS ‚â•0 and RHS ‚â§0.
ENF,0.7971491228070176,"Plugging these two observations to two sides of Equation (39), we get
W ‚Ä≤(en
f
W ‚Ä≤
a
‚àíen
f
W ‚Ä≤
a# ) ‚â•f
W ‚Ä≤(enW ‚Ä≤
at ‚àíenW ‚Ä≤"
ENF,0.7982456140350878,"a#
t ) + 1[a > at].
(40)
This contradicts with Equation (37)."
ENF,0.7993421052631579,"Since in both cases, we Ô¨Ånd contradictions. Now we can conclude that for any a ‚ààA\A(K), within
poisoning size K, the poisoned policy cannot choose a: f
œÄD(st0:t) Ã∏= a."
ENF,0.8004385964912281,"G
ADDITIONAL EXPERIMENTAL DETAILS"
ENF,0.8015350877192983,"G.1
DETAILS OF THE OFFLINE RL ALGORITHMS AND IMPLEMENTATIONS"
ENF,0.8026315789473685,"We experimented with three ofÔ¨Çine RL algorithms: DQN (Mnih et al., 2013), QR-DQN (Dabney
et al., 2018), and C51 (Bellemare et al., 2017). The Ô¨Årst one is the standard baseline, while the latter
two are distributional RL algorithms which show SOTA results in ofÔ¨Çine RL tasks. We Ô¨Årst brieÔ¨Çy
introduce the algorithm ideas, followed by the implementation details."
ENF,0.8037280701754386,Published as a conference paper at ICLR 2022
ENF,0.8048245614035088,"Algorithm Ideas.
The core of Q-learning (Watkins & Dayan, 1992) is the Bellman optimality
equation (Bellman, 1966)
Q‚ãÜ(s, a) = ER(s, a) + Œ≥Es‚Ä≤‚àºP max
a‚Ä≤‚ààA Q‚ãÜ(s‚Ä≤, a‚Ä≤) ,
(41)"
ENF,0.805921052631579,"where a parameterized QŒ∏ is adopted to approximate the optimal Q‚ãÜand iteratively improved. In
DQN (Mnih et al., 2013) speciÔ¨Åcally, the parameterization is achieved by using a convolutional
neural network (LeCun et al., 1998). In contrast to estimating the mean action value QœÄ(s, a) in
DQN, distributional RL algorithms estimate a density over the values of the state-action pairs. The
distributional Bellman optimality can be expressed as follows:
Z‚ãÜ(s, a)
D= r + Œ≥Z‚ãÜ(s‚Ä≤, argmaxa‚Ä≤‚ààA Q‚ãÜ(s‚Ä≤, a‚Ä≤)) where r ‚àºR(s, a), s‚Ä≤ ‚àºP(¬∑ | s, a).
(42)
Concretely, QR-DQN (Dabney et al., 2018) approximates the density D‚ãÜwith a uniform mixture
of K Dirac delta functions, while C51 (Bellemare et al., 2017) approximates the density using a
categorical distribution over a set of anchor points."
ENF,0.8070175438596491,"Implementation Details.
For training the subpolicies using ofÔ¨Çine RL training algorithms, we use
the code base of Agarwal et al. (2020). The conÔ¨Åguration Ô¨Åles containing the detailed hyperparam-
eters can be found at their public repository https://github.com/google-research/
batch_rl/tree/master/batch_rl/fixed_replay/configs. For the three methods
DQN, QR-DQN, and C51, the names of the conÔ¨Åguration Ô¨Åles are dqn.gin, quantile.gin,
and c51.gin, respectively. On each partition, we train the subpolicy for 50 epochs."
ENF,0.8081140350877193,"G.2
CONCRETE EXPERIMENTAL PROCEDURES"
ENF,0.8092105263157895,"We conduct experiments on two Atari 2600 games, Freeway and Breakout from OpenAI
Gym (Brockman et al., 2016), and one autonomous driving environment Highway (Leurent, 2018),
following Section 4."
ENF,0.8103070175438597,"Concretely, in the training stage, we Ô¨Årst partition the training dataset into u partitions (u = 30, 50,
or 100) by using the hash function h(œÑ) pre-deÔ¨Åned in each environment. Let si be the i-th value in
the state representation and d be the dimensionality of the state. In Atari games where the state is
the game frame, h(œÑ) is deÔ¨Åned as the sum of all pixel values of all frames in the trajectory œÑ, i.e.,
h(œÑ) = P s‚ààœÑ
P"
ENF,0.8114035087719298,"i‚àà[d] si. In Highway where the state is a Ô¨Çoating point number scalar containing
the positions and velocities of all vehicles, we deÔ¨Åne h(œÑ) = P s‚ààœÑ
P"
ENF,0.8125,"i‚àà[d] f(si) where f : R ‚ÜíZ
is a deterministic function that maps the given Ô¨Çoat value to integer space. Concretely, we take f(x)
as the the sum of the higher 16 bits and the lower 16 bits of its 32-bit representation under IEEE
754 standard (group of the Microprocessor Standards Subcommittee & Institute, 1985). We then
train the subpolicies on the partitions with ofÔ¨Çine RL training algorithm M0 ‚àà{DQN (Mnih et al.,
2013), QR-DQN (Dabney et al., 2018), C51 (Bellemare et al., 2017)}. The detailed description of
the algorithms can be found in Appendix G.1."
ENF,0.8135964912280702,"In the aggregation stage, we apply the three proposed aggregation protocols (PARL (Theorem 8),
TPARL (Theorem 1), and DPARL (Theorem 3)) on the trained u subpolicies and derive the aggre-
gated policies œÄP, œÄT, and œÄD accordingly."
ENF,0.8146929824561403,"Finally, in the certiÔ¨Åcation stage, for each aggregated policy, we provide the per-state action and
cumulative reward certiÔ¨Åcation following our theorems. When certifying the per-state action, we
constrain the maximum trajectory length H = 1000 for Atari games and H = 30 for Highway
(which is the full length in Highway) and report results averaged over 20 runs; for the cumulative
reward certiÔ¨Åcation, we adopt the trajectory length H = 400 for evaluating Freeway, H = 75 for
Breakout, and H = 30 for Highway."
ENF,0.8157894736842105,"ConÔ¨Åguration of Trajectory Length H.
For Atari games, we do not evaluate the full episode
length (up to tens of thousands steps), since our goal is to compare the relative certiÔ¨Åed robustness
of different RL algorithms, and the evaluation on relatively short trajectory is sufÔ¨Åcient under af-
fordable computation cost. Moreover, different episodes in Atari games are oftentimes of different
lengths; thus it is necessary that we restrict the episode length to enable a fair comparison. For
Highway, we evaluate on the full episode (H = 30) where we can efÔ¨Åciently achieve effective
comparisons."
ENF,0.8168859649122807,Published as a conference paper at ICLR 2022
ENF,0.8179824561403509,"Table 4: Benign empirical performance of three aggregation protocols (PARL, TPARL, and DPARL) applied
on subpolicies trained using three ofÔ¨Çine RL algorithms (DQN, QR-DQN, and C51), with the number of sub-
policies (i.e., #partitions) u equal to 30 or 50. We report results averaged over 20 runs of varying randomness
in the environment."
ENF,0.819078947368421,"Freeway
u = 30
u = 50"
ENF,0.8201754385964912,"PARL
TPARL
(W = 2)
TPARL
(W = 3)
TPARL
(W = 4)
DPARL
(Wmax = 5)
PARL
TPARL
(W = 2)
TPARL
(W = 3)
TPARL
(W = 4)
DPARL
(Wmax = 5)"
ENF,0.8212719298245614,"DQN
10.90 ¬± 0.77 11.65 ¬± 0.57 11.25 ¬± 0.54 12.00 ¬± 0.45
11.45 ¬± 0.74
10.95 ¬± 0.97 11.60 ¬± 1.02 12.40 ¬± 0.58 11.65 ¬± 0.65
11.90 ¬± 0.89
QR-DQN 11.60 ¬± 0.66 11.85 ¬± 1.15 11.25 ¬± 0.62 11.80 ¬± 0.87
12.10 ¬± 0.99
11.50 ¬± 0.92 11.60 ¬± 1.02 12.15 ¬± 0.57 12.80 ¬± 0.51
11.90 ¬± 0.77
C51
11.20 ¬± 0.60 12.45 ¬± 0.50 12.55 ¬± 0.50 11.40 ¬± 0.66
12.40 ¬± 0.49
11.70 ¬± 1.27 11.80 ¬± 0.75 12.70 ¬± 0.46 11.50 ¬± 0.87
11.95 ¬± 0.92"
ENF,0.8223684210526315,"Breakout
u = 30
u = 50"
ENF,0.8234649122807017,"PARL
TPARL
(W = 2)
TPARL
(W = 3)
TPARL
(W = 4)
DPARL
(Wmax = 5)
PARL
TPARL
(W = 2)
TPARL
(W = 3)
TPARL
(W = 4)
DPARL
(Wmax = 5)"
ENF,0.8245614035087719,"DQN
58.65 ¬± 40.83 38.05 ¬± 10.22 26.00 ¬± 12.79 13.50 ¬± 11.41
36.00 ¬± 13.39
60.25 ¬± 31.99 37.90 ¬± 11.55
25.90 ¬± 9.55
14.95 ¬± 7.24
45.00 ¬± 13.44
QR-DQN 76.50 ¬± 79.76 45.25 ¬± 42.70
22.05 ¬± 9.13
19.05 ¬± 6.91
42.05 ¬± 15.60
62.80 ¬± 28.71
32.10 ¬± 9.27
40.25 ¬± 52.97 17.30 ¬± 7.89
41.00 ¬± 13.87
C51
51.80 ¬± 10.74 37.60 ¬± 11.38 24.75 ¬± 12.77
13.55 ¬± 7.37
34.75 ¬± 10.91
60.55 ¬± 20.44 34.85 ¬± 11.92
26.15 ¬± 9.98
19.65 ¬± 7.30
39.90 ¬± 14.69"
ENF,0.8256578947368421,"Highway
u = 30
u = 50"
ENF,0.8267543859649122,"PARL
TPARL
(W = 2)
TPARL
(W = 3)
TPARL
(W = 4)
DPARL
(Wmax = 5)
PARL
TPARL
(W = 2)
TPARL
(W = 3)
TPARL
(W = 4)
DPARL
(Wmax = 5)"
ENF,0.8278508771929824,"DQN
28.07 ¬± 3.86 19.16 ¬± 10.26 16.83 ¬± 8.35 12.69 ¬± 6.17
23.55 ¬± 9.01
29.05 ¬± 0.62 16.63 ¬± 9.93 15.85 ¬± 8.52 13.43 ¬± 8.29
22.51 ¬± 8.01
QR-DQN 28.52 ¬± 1.22
18.63 ¬± 8.07
15.53 ¬± 7.46 12.77 ¬± 6.39
23.11 ¬± 7.94
27.64 ¬± 3.01 20.59 ¬± 7.70 14.94 ¬± 7.76 14.13 ¬± 7.54
23.49 ¬± 7.95
C51
28.86 ¬± 1.08
20.20 ¬± 9.21
13.30 ¬± 8.65
9.36 ¬± 5.96
21.51 ¬± 10.04
27.44 ¬± 4.32 15.12 ¬± 9.36 16.13 ¬± 8.46 10.61 ¬± 6.52
19.94 ¬± 11.14"
ENF,0.8289473684210527,"H
ADDITIONAL EVALUATION RESULTS AND DISCUSSIONS"
ENF,0.8300438596491229,"H.1
BENIGN EMPIRICAL PERFORMANCE"
ENF,0.831140350877193,"We present the benign empirical cumulative rewards of the three aggregation protocols (PARL,
TPARL, and DPARL) applied on subpolicies trained using three ofÔ¨Çine RL algorithms (DQN, QR-
DQN, and C51) in Table 4. The cumulative reward is obtained by running a trajectory of maximum
length H and accumulating the reward achieved at each time step."
ENF,0.8322368421052632,"We discuss the conclusions on multiple levels. We choose H = 1000 for Atari games and H = 30
for Highway environment and report results averaged over 20 runs. On the RL algorithm level,
we see that QR-DQN achieves the highest score in most cases. On the aggregation protocol level,
temporal aggregation enhances the performance on Freeway while dampens the performance on
Breakout and Highway. We particularly note that the results obtained by using temporal aggregation
(TPARL and DPARL) gives signiÔ¨Åcantly smaller variance compared to the single-step aggregation
PARL, especially in Breakout. This implies that temporal aggregation can help stabilize the policy.
However, the conclusion does not hold in Highway, which is an interesting phenomenon that is
worth investigating. On the partition number level, a larger partition number can give slightly
better results. On the environment level, Freeway is simpler and more stable than Breakout and
Highway."
ENF,0.8333333333333334,"H.2
COMPARISON OF COPA WITH STANDARD TRAINING"
ENF,0.8344298245614035,"We provide additional experimental results on the comparison between the empirical performance
of our COPA and the standard training in terms of the convergence speed (Figure 3) and policy
quality (Table 5)."
ENF,0.8355263157894737,"In Figure 3, we aim to show the comparison of the convergence speed. For our proposed training,
we plot all training curves of the sampled 5 subpolicies in blue, where each subpolicy is trained on
one partition of the dataset. (We do not plot all 50 curves for visual clarity.) For standard training,
we plot the training curve of the standard policy in red, where the single policy is trained on the
entire dataset. We see that on Freeway, most subpolicies converge slower than the standard policy,
but will reach similar convergence value as the standard training one; while there also exist a few
subpolicies that fail to be trained. On Breakout, within 50 epochs, we observe substantial Ô¨Çuctuation
for the training curves of all the policies, as well as large variance for the achieved reward at the last"
ENF,0.8366228070175439,Published as a conference paper at ICLR 2022
ENF,0.8377192982456141,"0
10
20
30
40
50
# epoch 0 5 10 15 20 25 30"
ENF,0.8388157894736842,reward
ENF,0.8399122807017544,training curve (Freeway)
ENF,0.8410087719298246,"subpolicies
standard policy"
ENF,0.8421052631578947,"0
10
20
30
40
50
# epoch 0 50 100 150 200 250 300"
ENF,0.8432017543859649,reward
ENF,0.8442982456140351,training curve (Breakout)
ENF,0.8453947368421053,"subpolicies
standard policy"
ENF,0.8464912280701754,"0
600
1200
1800
2400
3000
# epoch 5 10 15 20 25 30"
ENF,0.8475877192982456,reward
ENF,0.8486842105263158,training curve (Highway)
ENF,0.8497807017543859,"subpolicies
standard policy"
ENF,0.8508771929824561,"Figure 3: The convergence speed of the proposed partition-based training compared with the standard
training. For our proposed training, we plot all training curves of the sampled 5 subpolicies in blue, where
each subpolicy is trained on one partition of the dataset. (We do not plot all 50 curves for visual clarity). For
standard training, we plot the training curve of the standard policy in red, where the single policy is trained on
the entire dataset. In Atari games, we train each policy (or subpolicy) for a Ô¨Åxed number of 50 epochs, where
each epoch consumes 1M randomly sampled training data. In Highway environment, we train each policy (or
subpolicy) for a Ô¨Åxed number of 3000 epochs, where each epoch consumes 1K randomly sampled training
data. The ofÔ¨Çine RL algorithm used is DQN."
ENF,0.8519736842105263,"Table 5: The policy quality measured by empirical cumulative reward of the proposed aggregation proto-
cols (PARL, TPARL, and DPARL) compared with the standard training. In our aggregation, we aggregate
over u subpolicies with u equal to 30 or 50 . We report results averaged over 20 runs of varying randomness
in the environment, where each run is an episode of length at most 1000 for Atari games and 30 for Highway
environment. The ofÔ¨Çine RL algorithm used is DQN."
ENF,0.8530701754385965,"standard
trained
policy"
ENF,0.8541666666666666,"u = 30
u = 50"
ENF,0.8552631578947368,"PARL
TPARL
(W = 4)
DPARL
(Wmax = 5)
PARL
TPARL
(W = 4)
DPARL
(Wmax = 5)"
ENF,0.856359649122807,"Freeway
12.00 ¬± 0.89
10.90 ¬± 0.77
12.00 ¬± 0.45
11.45 ¬± 0.74
10.95 ¬± 0.97
11.65 ¬± 0.65
11.90 ¬± 0.89
Breakout 97.60 ¬± 117.28 58.65 ¬± 40.83 13.50 ¬± 11.41
36.00 ¬± 13.39
60.25 ¬± 31.99 14.95 ¬± 7.24
45.00 ¬± 13.44
Highway
28.90 ¬± 0.87
28.07 ¬± 3.86
12.69 ¬± 6.17
23.55 ¬± 9.01
29.05 ¬± 0.62
13.43 ¬± 8.29
22.51 ¬± 8.01"
ENF,0.8574561403508771,"epoch. Thus, on these two Atari games, we cannot draw conclusions w.r.t. the convergence. Given
more computational resources to run more epochs, we would be able to draw more informative
conclusions. In contrast, on Highway, all subpolicies display similarly good convergence properties,
showing comparable convergence speed and value with the standard training on the entire dataset."
ENF,0.8585526315789473,"In Table 5, we aim to compare the policy quality of the aggregated policy derived in our COPA
framework (i.e., PARL, TPARL, and DPARL) with the policy obtained from standard training. We
see that on Freeway, our three protocols achieve comparable results with the policy obtained by
standard training on the entire dataset; on Breakout, although the quality of our obtained policies is
lower, our policies are much more stable with signiÔ¨Åcantly lower variance than the standard training;
on Highway, only PARL obtains comparable results to the standard training policy, indicating the
lack of temporal continuity in this environment."
ENF,0.8596491228070176,"Table 6: Average window size (i.e., PT
t=1 Wt/T) for the aggregation protocol DPARL (Wmax = 5) ap-
plied on subpolicies trained using three ofÔ¨Çine RL algorithms (DQN, QR-DQN, and C51), with the number of
subpolicies (i.e., #partitions) u equal to 30 or 50. We report results averaged over all time steps in 20 runs."
ENF,0.8607456140350878,"u = 30
u = 50"
ENF,0.8618421052631579,"DQN
QR-DQN
C51
DQN
QR-DQN
C51"
ENF,0.8629385964912281,"Freeway
2.69 ¬± 1.73
2.59 ¬± 1.71
2.64 ¬± 1.74
2.79 ¬± 1.73
2.70 ¬± 1.73
2.72 ¬± 1.73
Breakout
2.28 ¬± 1.46
2.44 ¬± 1.55
2.32 ¬± 1.48
2.33 ¬± 1.48
2.40 ¬± 1.52
2.39 ¬± 1.52
Highway 1.97 ¬± 0.44
2.21 ¬± 0.27
2.16 ¬± 0.37
2.23 ¬± 0.32
2.26 ¬± 0.24
2.18 ¬± 0.37"
ENF,0.8640350877192983,Published as a conference paper at ICLR 2022
ENF,0.8651315789473685,"Table 7: Runtime (unit: seconds) of the aggregation protocol DPARL (Wmax = 5) applied on subpolicies
trained using ofÔ¨Çine RL algorithm DQN, with the number of subpolicies (i.e., partition number) u equal to 30 or
50. We compare with the standard testing which tests the runtime of a single trained DQN policy without using
our framework. We report results averaged over 20 runs of varying randomness in the environment, where each
run is an episode of length at most 1000."
ENF,0.8662280701754386,"standard testing
DPARL (u = 30)
DPARL (u = 50)"
ENF,0.8673245614035088,"Freeway
2.53 ¬± 0.50
56.37 ¬± 1.75
79.05 ¬± 4.27
Breakout
2.00 ¬± 0.56
72.05 ¬± 13.09
108.05 ¬± 8.71"
ENF,0.868421052631579,"H.3
MORE ANALYTICAL STATISTICS FOR DPARL"
ENF,0.8695175438596491,"Selected Window Size.
We provide the mean and variance for the selected window sizes in
DPARL in Table 6. In our experiments, the maximum window size Wmax = 5, while the aver-
age selected window sizes are all around half of the maximum value. Thus, the average certiÔ¨Åcation
time is expected to be much smaller compared with the worst-case time complexity."
ENF,0.8706140350877193,"Running Time.
We provide the running time in Table 7. SpeciÔ¨Åcally, we compare the running
time of DPARL (Wmax = 5) applied on u = 30 or 50 subpolicies trained using ofÔ¨Çine RL algorithm
DQN with the normal testing which tests the runtime of a single trained DQN policy without using
our framework. As we have shown in the remark of Theorem 3 in Section 4.3, the time complexity of
DPARL is O
 
W 2
max|A|2u + Wmax|A|2u log u

. In Table 7, we also see that the runtime of DPARL
scales roughly quasilinearly with the number of subpolicies u, and quadratically with the action set
size |A|, which is 3 for Freeway and 4 for Breakout. We omit the runtime for Highway, since the
horizon length, the environment type, and the neural network size are all different for Highway and
Atari games."
ENF,0.8717105263157895,"H.4
MAXIMUM TOLERABLE POISONING THRESHOLD VS. TOTAL TRAJECTORY NUMBER"
ENF,0.8728070175438597,"We provide the total number of trajectories in the ofÔ¨Çine dataset used for training the environments
Freeway, Breakout and Highway below, as well as the corresponding ratio of the maximum tolerable
poisoning threshold w.r.t. the total number of trajectories. For Freeway, the total number of trajec-
tories in the entire ofÔ¨Çine dataset is 121,892, and the ratio (max Kt / # total trajectories) is 0.0002;
for Breakout, the number is 209,049, and the ratio is 0.0001. We emphasize that, as shown in previ-
ous work on probable defense against poisoning in supervised learning (Levine & Feizi, 2020), the
number of instances they can certify is also not high especially for challenging tasks (e.g., certifying
20 instances on the dataset GTSRB in Levine & Feizi (2020), attaining the ratio 0.0005), which
may imply the intrinsic difÔ¨Åculty of certifying against poisoning attacks. Given such difÔ¨Åculty, we
already achieve reasonable certiÔ¨Åcation as the Ô¨Årst work on certiÔ¨Åed robust RL against poisoning,
and we hope future works can further improve upon our results."
ENF,0.8739035087719298,"H.5
COMPARISON OF FREEWAY, BREAKOUT, AND HIGHWAY"
ENF,0.875,"Freeway and Breakout are two typical types of games with distinctive game properties. Freeway is a
simple game where the agent aims to cross the road and avoid the trafÔ¨Åc. In most of the time steps,
the agent would take the ‚Äúforward‚Äù action; only when there is a need to avoid the trafÔ¨Åc will the agent
stop and wait. In comparison, the game Breakout involves more complicated interactions between
the paddle and the environment. The paddle position and velocity both play important roles in order
to catch and bounce the ball at an appropriate angle, which requires a frequent switch of the paddle
moving direction. Similar to Breakout, as an autonomous driving environment, Highway requires
the agent to accurately analyze the rapidly changing environment around it and react quickly, leading
to frequent changes in vehicle‚Äôs actions."
ENF,0.8760964912280702,"Given the properties of the environments, we note that in Freeway, adjacent time steps may share
consistent action selections, which is not necessarily true in the Breakout and Highway. Thus, it
would be expected to be beneÔ¨Åcial to consider the past history for making the current decision in
Freeway, while the past history may interfere with the action selection in Breakout and Highway."
ENF,0.8771929824561403,Published as a conference paper at ICLR 2022
ENF,0.8782894736842105,"Table 8: Action change ratio (in percentage) of three aggregation protocols (PARL, TPARL, and DPARL)
applied on subpolicies trained in Highway environment using three ofÔ¨Çine RL algorithms (DQN, QR-DQN, and
C51), with the number of subpolicies (i.e., #partitions) u equal to 30, 50, or 100. We report results averaged
over 20 runs of varying randomness in the environment."
ENF,0.8793859649122807,"u
RL
Algorithm"
ENF,0.8804824561403509,Aggregation Protocol
ENF,0.881578947368421,"PARL
TPARL
(W = 4)
DPARL
(Wmax = 5)"
DQN,0.8826754385964912,"30
DQN
59.79 ¬± 8.99
24.89 ¬± 10.47
33.82 ¬± 8.75
QR-DQN
61.00 ¬± 12.74
31.14 ¬± 6.43
37.80 ¬± 7.16
C51
59.00 ¬± 10.39
18.17 ¬± 11.52
35.44 ¬± 8.65"
DQN,0.8837719298245614,"50
DQN
55.50 ¬± 8.84
26.05 ¬± 11.69
40.07 ¬± 13.74
QR-DQN
60.37 ¬± 10.31
28.48 ¬± 6.81
37.78 ¬± 12.15
C51
59.39 ¬± 12.34
28.04 ¬± 10.71
38.21 ¬± 11.23"
DQN,0.8848684210526315,"100
DQN
48.67 ¬± 5.91
22.17 ¬± 7.65
33.45 ¬± 9.70
QR-DQN
61.17 ¬± 8.38
24.12 ¬± 9.85
36.76 ¬± 7.87
C51
61.83 ¬± 10.41
22.91 ¬± 13.34
34.06 ¬± 9.19"
DQN,0.8859649122807017,"Comparisons of Bottleneck states in Atari Games.
In Breakout, the game goal is to control
the paddle so that the ball is bounced to hit the brick. There are clearly very different stages in
the Breakout game, e.g., when the ball is Ô¨Çying towards the paddle and when it is bounced back.
An example of the bottleneck state is when the ball is approaching the paddle. The poisoning
behavior may lead to the disastrous effect that the policy learns to control the paddle to slide in the
opposite direction of the ball at such bottleneck states, while other states may not be as vulnerable.
In comparison, in Freeway, the game goal is to control the agent to cross the road while avoiding
the trafÔ¨Åc. The agent is faced with similar trafÔ¨Åc conditions everywhere on the road, and can make
stable choices regardless of the complex situation. Thus there are very few bottleneck states."
DQN,0.8870614035087719,"A Study on the Frequency of Action Change in Highway.
In Table 8, we present the action
change ratio (# action changes / trajectory length) in Highway environment. Comparing among
the aggregation protocols, we Ô¨Årst note that the single-step aggregation policy œÄP frequently alters
actions as the reaction to the rapidly changing driving environment, while the temporal aggregation
protocol œÄT changes actions less frequently. This is because in TPARL, the agent is explicitly forced
to take stable actions based on a Ô¨Åxed window size. Second, comparing the action change ratio with
the benign empirical performance in Table 4, we observe that œÄP and œÄT achieves the highest and
lowest empirical performance respectively, which indicates the correlation between action change
ratio with the achieved empirical reward. SpeciÔ¨Åcally, in TPARL, the action change is limited as a
result of the enforced window size, leading to the limited benign performance."
DQN,0.8881578947368421,"H.6
FULL RESULTS OF ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION STABILITY"
DQN,0.8892543859649122,"As a complete set of results for per-state action certiÔ¨Åcation apart from Figure 1 in Section 5.1, we
present the entire cumulative histogram of tolerable poisoning thresholds in Figure 4 and Figure 5,
together with the average tolerable poisoning thresholds in Table 9. The deÔ¨Ånitions of the two
metrcis can be found in Section 5.1."
DQN,0.8903508771929824,"Basically, the conclusions in terms of the comparisons on the RL algorithm level, the aggregation
protocol level, the partition number level, and the game level are all similar to that derived in Sec-
tion 5.1."
DQN,0.8914473684210527,"H.7
FULL RESULTS OF ROBUSTNESS CERTIFICATION FOR CUMULATIVE REWARD BOUND"
DQN,0.8925438596491229,"In addition to the evaluation results provided in Figure 2 in section 5.2, we provide a more compre-
hensive set of evaluation results with more settings of trajectory length H here in Figure 6."
DQN,0.893640350877193,"We draw similar conclusions as discussed in Section 5.2. SpeciÔ¨Åcally, in Highway, C51 achieves
higher certiÔ¨Åed lower bounds than other two when the poisoning size is large, which can be explained
by the larger portion of states that can tolerate large poisoning sizes as shown in Figure 5."
DQN,0.8947368421052632,Published as a conference paper at ICLR 2022
DQN,0.8958333333333334,"PARL
TPARL (W = 4)
DPARL (Wmax = 5)"
DQN,0.8969298245614035,"Freeway, u = 30
Freeway, u = 50"
DQN,0.8980263157894737,"DQN
stability ratio"
DQN,0.8991228070175439,"0 1 2 3 4 5 6 7 8 9 1011121314
0.0
0.2
0.4
0.6
0.8
1.0"
DQN,0.9002192982456141,"0 1 2 3 4 5 6 7 8 9 101112131415161718192021222324
0.0
0.2
0.4
0.6
0.8
1.0"
DQN,0.9013157894736842,"QR-DQN
stability ratio"
DQN,0.9024122807017544,"0 1 2 3 4 5 6 7 8 9 1011121314
0.0
0.2
0.4
0.6
0.8
1.0"
DQN,0.9035087719298246,"0 1 2 3 4 5 6 7 8 9 101112131415161718192021222324
0.0
0.2
0.4
0.6
0.8
1.0"
DQN,0.9046052631578947,"C51
stability ratio"
DQN,0.9057017543859649,"0 1 2 3 4 5 6 7 8 9 1011121314
0.0
0.2
0.4
0.6
0.8
1.0"
DQN,0.9067982456140351,"0 1 2 3 4 5 6 7 8 9 101112131415161718192021222324
0.0
0.2
0.4
0.6
0.8
1.0 ‚â•K
‚â•K"
DQN,0.9078947368421053,"(a) Freeway
Breakout, u = 30
Breakout, u = 50"
DQN,0.9089912280701754,"DQN
stability ratio"
DQN,0.9100877192982456,"0 1 2 3 4 5 6 7 8 9 1011121314
0.0
0.2
0.4
0.6
0.8
1.0"
DQN,0.9111842105263158,"0 1 2 3 4 5 6 7 8 9 101112131415161718192021222324
0.0
0.2
0.4
0.6
0.8
1.0"
DQN,0.9122807017543859,"QR-DQN
stability ratio"
DQN,0.9133771929824561,"0 1 2 3 4 5 6 7 8 9 1011121314
0.0
0.2
0.4
0.6
0.8
1.0"
DQN,0.9144736842105263,"0 1 2 3 4 5 6 7 8 9 101112131415161718192021222324
0.0
0.2
0.4
0.6
0.8
1.0"
DQN,0.9155701754385965,"C51
stability ratio"
DQN,0.9166666666666666,"0 1 2 3 4 5 6 7 8 9 1011121314
0.0
0.2
0.4
0.6
0.8
1.0"
DQN,0.9177631578947368,"0 1 2 3 4 5 6 7 8 9 101112131415161718192021222324
0.0
0.2
0.4
0.6
0.8
1.0 ‚â•K
‚â•K"
DQN,0.918859649122807,"(b) Breakout
Figure 4: Robustness certiÔ¨Åcation for per-state action stability on Atari games (full results). We plot the
cumulative histogram of the tolerable poisoning size K for all time steps in one trajectory. We provide results
on two games (Freeway and Breakout), two partition numbers (u = 30 and u = 50), and a comparison of
three certiÔ¨Åcation methods (PARL, TPARL, and DPARL). The results are averaged over 20 runs considering
the randomness in the game environment, and the short vertical bar on top of each bar represents the standard
deviation."
DQN,0.9199561403508771,Published as a conference paper at ICLR 2022
DQN,0.9210526315789473,"PARL
TPARL (W = 4)
DPARL (Wmax = 5)"
DQN,0.9221491228070176,"Highway, u = 30
Highway, u = 50"
DQN,0.9232456140350878,"DQN
stability ratio"
DQN,0.9243421052631579,"0 1 2 3 4 5 6 7 8 9 1011121314
0.0
0.2
0.4
0.6
0.8
1.0"
DQN,0.9254385964912281,"0 1 2 3 4 5 6 7 8 9 101112131415161718192021222324
0.0
0.2
0.4
0.6
0.8
1.0"
DQN,0.9265350877192983,"QR-DQN
stability ratio"
DQN,0.9276315789473685,"0 1 2 3 4 5 6 7 8 9 1011121314
0.0
0.2
0.4
0.6
0.8
1.0"
DQN,0.9287280701754386,"0 1 2 3 4 5 6 7 8 9 101112131415161718192021222324
0.0
0.2
0.4
0.6
0.8
1.0"
DQN,0.9298245614035088,"C51
stability ratio"
DQN,0.930921052631579,"0 1 2 3 4 5 6 7 8 9 1011121314
0.0
0.2
0.4
0.6
0.8
1.0"
DQN,0.9320175438596491,"0 1 2 3 4 5 6 7 8 9 101112131415161718192021222324
0.0
0.2
0.4
0.6
0.8
1.0"
DQN,0.9331140350877193,"‚â•K
‚â•K
Highway, u = 100"
DQN,0.9342105263157895,"DQN
stability ratio"
DQN,0.9353070175438597,"0 1 2 3 4 5 6 7 8 9 10111213141516171819202122232425262728293031323334353637383940414243444546474849
0.0
0.2
0.4
0.6
0.8
1.0"
DQN,0.9364035087719298,"QR-DQN
stability ratio"
DQN,0.9375,"0 1 2 3 4 5 6 7 8 9 10111213141516171819202122232425262728293031323334353637383940414243444546474849
0.0
0.2
0.4
0.6
0.8
1.0"
DQN,0.9385964912280702,"C51
stability ratio"
DQN,0.9396929824561403,"0 1 2 3 4 5 6 7 8 9 10111213141516171819202122232425262728293031323334353637383940414243444546474849
0.0
0.2
0.4
0.6
0.8
1.0"
DQN,0.9407894736842105,"‚â•K
Figure 5: Robustness certiÔ¨Åcation for per-state action stability on Highway environment (full results).
We plot the cumulative histogram of the tolerable poisoning size K for all time steps in one trajectory. We
provide results on Highway environment, three partition numbers (u = 30, u = 50 and u = 100), and a
comparison of three certiÔ¨Åcation methods (PARL, TPARL, and DPARL). The results are averaged over 20 runs
considering the randomness in the environment, and the short vertical bar on top of each bar represents the
standard deviation."
DQN,0.9418859649122807,"u = 30, PARL
u = 50, PARL"
DQN,0.9429824561403509,"u = 30, TPARL (W = 4)
u = 50, TPARL (W = 4)"
DQN,0.944078947368421,"u = 30, DPARL (Wmax = 5)
u = 50, DPARL (Wmax = 5)"
DQN,0.9451754385964912,"Freeway, H = 100 Freeway, H = 200 Freeway, H = 400
Breakout, H = 50
Breakout, H = 75 Highway, H = 30"
DQN,0.9462719298245614,"DQN
JK"
DQN,0.9473684210526315,"0
5
10
15
20
25
0 1"
DQN,0.9484649122807017,"0
5
10
15
20
25
0 1 2 3"
DQN,0.9495614035087719,"0
5
10
15
20
25
0 1 2 3 4 5"
DQN,0.9506578947368421,"0
3
6
9 12 15 18 21
0 1"
DQN,0.9517543859649122,"0
5
10
15
20
25
0 1 2"
DQN,0.9528508771929824,"0
5
10
15
20
25 5 10 15 20 25"
DQN,0.9539473684210527,"QR-DQN
JK"
DQN,0.9550438596491229,"0
5
10
15
20
25
0 1"
DQN,0.956140350877193,"0
5
10
15
20
25
0 1 2 3"
DQN,0.9572368421052632,"0
5
10
15
20
25
0 1 2 3 4 5"
DQN,0.9583333333333334,"0
3
6
9 12 15 18 21 24
0 1"
DQN,0.9594298245614035,"0
5
10
15
20
25
0 1 2"
DQN,0.9605263157894737,"0
5
10
15
20
25 5 10 15 20 25"
DQN,0.9616228070175439,"C51
JK"
DQN,0.9627192982456141,"0
5
10
15
20
25
0 1"
DQN,0.9638157894736842,"0
5
10
15
20
25
0 1 2 3"
DQN,0.9649122807017544,"0
5
10
15
20
25
0 1 2 3 4 5"
DQN,0.9660087719298246,"0
3
6
9 12 15 18 21 24 27
0 1"
DQN,0.9671052631578947,"0
5
10
15
20
25
0 1 2"
DQN,0.9682017543859649,"0
5
10
15
20
25 5 10 15 20 25"
DQN,0.9692982456140351,"Poisoning size K
Poisoning size K
Poisoning size K
Poisoning size K
Poisoning size K
Poisoning size K
Figure 6: Robustness certiÔ¨Åcation for cumulative reward (full results). We plot the lower bound of cumu-
lative reward JK w.r.t. poisoning size K under three different certiÔ¨Åcation methods (PARL, TPARL (W = 4),
DPARL (Wmax = 5)) with two partition numbers (u ‚àà{30, 50}). Each row corresponds to one RL algorithm,
and each column corresponds to one setting of trajectory length H."
DQN,0.9703947368421053,Published as a conference paper at ICLR 2022
DQN,0.9714912280701754,"Table 9:
Average tolerable poisoning thresholds of three aggregation protocols (PARL, TPARL, and
DPARL) applied on subpolicies trained using three ofÔ¨Çine RL algorithms (DQN, QR-DQN, and C51), with
the number of subpolicies (i.e., #partitions) u equal to 30 or 50. We report results averaged over 20 runs of
varying randomness in the environment."
DQN,0.9725877192982456,"Freeway
u = 30
u = 50"
DQN,0.9736842105263158,"PARL
TPARL
(W = 4)
DPARL
(Wmax = 5)
PARL
TPARL
(W = 4)
DPARL
(Wmax = 5)"
DQN,0.9747807017543859,"DQN
9.90 ¬± 0.14
10.21 ¬± 0.09
10.30 ¬± 0.09
16.78 ¬± 0.42
17.16 ¬± 0.29
16.88 ¬± 0.45
QR-DQN 9.87 ¬± 0.20
10.11 ¬± 0.30
10.16 ¬± 0.34
16.79 ¬± 0.52
17.31 ¬± 0.29
17.03 ¬± 0.40
C51
9.57 ¬± 0.24
9.83 ¬± 0.15
10.11 ¬± 0.19
16.82 ¬± 0.40
17.08 ¬± 0.34
17.61 ¬± 0.15"
DQN,0.9758771929824561,"Breakout
u = 30
u = 50"
DQN,0.9769736842105263,"PARL
TPARL
(W = 4)
DPARL
(Wmax = 5)
PARL
TPARL
(W = 4)
DPARL
(Wmax = 5)"
DQN,0.9780701754385965,"DQN
1.08 ¬± 0.09
1.28 ¬± 0.15
0.85 ¬± 0.09
2.07 ¬± 0.22
1.92 ¬± 0.30
1.55 ¬± 0.28
QR-DQN
1.38 ¬± 0.09
1.33 ¬± 0.18
1.18 ¬± 0.10
2.12 ¬± 0.27
1.93 ¬± 0.14
1.71 ¬± 0.19
C51
1.10 ¬± 0.08
1.42 ¬± 0.19
0.93 ¬± 0.13
2.43 ¬± 0.21
2.37 ¬± 0.19
1.90 ¬± 0.19"
DQN,0.9791666666666666,"Highway
u = 30
u = 50"
DQN,0.9802631578947368,"PARL
TPARL
(W = 4)
DPARL
(Wmax = 5)
PARL
TPARL
(W = 4)
DPARL
(Wmax = 5)"
DQN,0.981359649122807,"DQN
4.38 ¬± 0.78
2.99 ¬± 1.08
3.75 ¬± 1.85
7.16 ¬± 1.58
6.27 ¬± 1.61
6.32 ¬± 0.98
QR-DQN 4.18 ¬± 0.91
3.35 ¬± 1.44
2.84 ¬± 0.55
6.52 ¬± 1.17
5.02 ¬± 1.60
4.61 ¬± 1.20
C51
4.97 ¬± 1.09
4.31 ¬± 1.45
4.12 ¬± 0.95
7.95 ¬± 1.46
6.55 ¬± 2.20
6.69 ¬± 1.47"
DQN,0.9824561403508771,"We additionally point out that the value JK achieved at poisoning size K = 0 (e.g., JK = 4
for Freeway, 2 for Breakout, and 28.31 for Highway under œÄP over u = 50 DQN subpolicies)
corresponds to the case where there is no poisoning at all on the training set. Our successive bounds
under larger K are non-vacuous compared to this value."
DQN,0.9835526315789473,"I
A BROADER DISCUSSION ON RELATED WORK"
DQN,0.9846491228070176,"I.1
POISONING ATTACKS IN RL"
DQN,0.9857456140350878,"Below, we provide a brief discussion on the related works on policy poisoning and reward poison-
ing (Ma et al., 2019; Sun et al., 2021; Huang & Zhu, 2019)."
DQN,0.9868421052631579,"Ma et al. (2019) and Huang & Zhu (2019) study reward poisoning where the attacker can modify
the rewards in an ofÔ¨Çine dataset or the reward signals during the online interaction. The attacker‚Äôs
goal is to force learning a particular target policy, or minimize the agent‚Äôs reward in the original
task. Under this framework, Ma et al. (2019) considers two victim learners with speciÔ¨Åc assump-
tions on the learner structure and develops attacks for them, while Huang & Zhu (2019) provides a
theoretical analysis on the conditions for successful attacks against a Q-learning agent. In compari-
son to only reward poisoning in Ma et al. (2019) and Huang et al. (2017), Sun et al. (2021) focuses
on general policy poisoning attacks for policy gradient learners in the online RL setting by using
the vulnerability-awareness metric to decide when to poison, and the adversarial critic to guide the
poisoning. Sun et al. (2021) achieve successful poisoning against policy-based agents in various
complex environments in online RL, but it remains an interesting open problem as to how to poison
the ofÔ¨Çine RL dataset which is agnostic to the learning algorithm."
DQN,0.9879385964912281,"I.2
EMPIRICALLY ROBUST RL"
DQN,0.9890350877192983,"Robust RL against Evasion Attacks.
We brieÔ¨Çy review several categories of RL methods that
demonstrate empirical robustness against evasion attacks."
DQN,0.9901315789473685,Published as a conference paper at ICLR 2022
DQN,0.9912280701754386,"Randomization methods (Tobin et al., 2017; Akkaya et al., 2019) were Ô¨Årst proposed to encourage
exploration. This type of method was later systematically studied for its potential to improve model
robustness. NoisyNet (Fortunato et al., 2017) adds parametric noise to the network‚Äôs weight during
training, providing better resilience to both training-time and test-time attacks (Behzadan & Munir,
2017; 2018), also reducing the transferability of adversarial examples, and enabling quicker recovery
with fewer number of transitions during phase transition."
DQN,0.9923245614035088,"Under the adversarial training framework, Kos & Song (2017) and Behzadan & Munir (2017)
show that re-training with random noise and FGSM perturbations increases the resilience against
adversarial examples. Pattanaik et al. (2018) leverage attacks using an engineered loss function
speciÔ¨Åcally designed for RL to signiÔ¨Åcant increase the robustness to parameter variations. RS-
DQN (Fischer et al., 2019) is an imitation learning based approach that trains a robust student-DQN
in parallel with a standard DQN in order to incorporate the constrains such as SOTA adversarial
defenses (Madry et al., 2017; Mirman et al., 2018)."
DQN,0.993421052631579,"SA-DQN (Zhang et al., 2020a) is a regularization based method that adds regularizers to the training
loss function to encourage the top-1 action to stay unchanged under perturbation."
DQN,0.9945175438596491,"Built on top of the neural network veriÔ¨Åcation algorithms (Gowal et al., 2018; Weng et al., 2018),
Radial-RL (Oikarinen et al., 2020) proposes to minimize an adversarial loss function that incor-
porates the upper bound of the perturbed loss, computed using certiÔ¨Åed bounds from veriÔ¨Åcation
algorithms. CARRL (Everett et al., 2021) aims to compute the lower bounds of action-values under
potential perturbation and select actions according to the worst-case bound, but it relies on linear
bounds (Weng et al., 2018) and is only suitable for low-dimensional environments."
DQN,0.9956140350877193,"These robust RL methods only provide empirical robustness against perturbed state inputs during
test time, but cannot provide theoretical guarantees for the performance of the trained models under
any bounded perturbations."
DQN,0.9967105263157895,"Robust RL against Poisoning Attacks.
Banihashem et al. (2021) considers the threat model of
reward poisoning attacks proposed by Ma et al. (2019); Rakhsha et al. (2020); Zhang et al. (2020b),
where the attacker aims to force learning a target policy while minimizing the cost of reward manip-
ulation. As shown in Ma et al. (2019); Rakhsha et al. (2020); Zhang et al. (2020b), this optimization
problem is feasible and always has a unique optimal solution. Leveraging this property, Banihashem
et al. (2021) formulates the defense task as another optimization problem which aims to optimize the
agent‚Äôs worst case performance among the set of plausible candidates of the true reward function.
They speciÔ¨Åcally consider two settings regarding the agent‚Äôs knowledge of the attack parameter, and
provide lower bounds on the performance of the defense policy. In contrast to Banihashem et al.
(2021) which focuses on a speciÔ¨Åc type of attack, our paper considers the threat model of general
poisoning attacks, where the attacker has the power to manipulate the training trajectories arbitrarily.
Given limited knowledge regarding the attack, our proposed COPA framework is general and appli-
cable to any potential attack. Our robustness is derived from the aggregation over both sub-policy
level and temporal level. One similarity between Banihashem et al. (2021) and our work is that we
both aim to provide lower bounds of the cumulative reward for our proposed method as the provable
guarantee / certiÔ¨Åcation criteria."
DQN,0.9978070175438597,"I.3
ROBUSTNESS CERTIFICATION FOR RL"
DQN,0.9989035087719298,"Wu et al. (2022) provide the Ô¨Årst robustness certiÔ¨Åcation for RL against test-time evasion attacks
following the line of work on randomized smoothing (Cohen et al., 2019; Salman et al., 2019). Con-
cretely, they apply per-state smoothing to achieve the certiÔ¨Åcation for per-state action stability, as
well as trajectory smoothing to obtain the certiÔ¨Åcation for cumulative reward. Notably, Wu et al.
(2022) propose an adaptive tree search algorithm to explore all possible trajectories and thus de-
rive the robustness guarantee for cumulative reward. The relationship between our COPA-SEARCH
and their Algorithm 3 is discussed in detail in Appendix E.3. In comparison, robustness in their
work is derived from smoothing, while our robustness comes from aggregation. We propose aggre-
gation protocols and certiÔ¨Åcation methods that leverage the temporal information by dynamically
aggregating over the past time steps, which is not covered in Wu et al. (2022)."
