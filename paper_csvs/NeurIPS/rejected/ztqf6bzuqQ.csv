Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.00202020202020202,"Representation learning has been evolving from traditional supervised training to
1"
ABSTRACT,0.00404040404040404,"Contrastive Learning (CL) and Masked Image Modeling (MIM). Previous works
2"
ABSTRACT,0.006060606060606061,"have demonstrated their pros and cons in specific scenarios, i.e., CL and supervised
3"
ABSTRACT,0.00808080808080808,"pre-training excel at capturing longer-range global patterns and enabling better
4"
ABSTRACT,0.010101010101010102,"feature discrimination, while MIM can introduce more local and diverse attention
5"
ABSTRACT,0.012121212121212121,"across all transformer layers. In this paper, we explore how to obtain a model
6"
ABSTRACT,0.014141414141414142,"that combines their strengths. We start by examining previous feature distillation
7"
ABSTRACT,0.01616161616161616,"and mask feature reconstruction methods and identify their limitations. We find
8"
ABSTRACT,0.01818181818181818,"that their increasing diversity mainly derives from the asymmetric designs, but
9"
ABSTRACT,0.020202020202020204,"these designs may in turn compromise the discrimination ability. In order to
10"
ABSTRACT,0.022222222222222223,"better obtain both discrimination and diversity, we propose a simple but effective
11"
ABSTRACT,0.024242424242424242,"Hybrid Distillation strategy, which utilizes both the supervised/CL teacher and the
12"
ABSTRACT,0.026262626262626262,"MIM teacher to jointly guide the student model. Hybrid Distill imitates the token
13"
ABSTRACT,0.028282828282828285,"relations of the MIM teacher to alleviate attention collapse, as well as distills the
14"
ABSTRACT,0.030303030303030304,"feature maps of the supervised/CL teacher to enable discrimination. Furthermore, a
15"
ABSTRACT,0.03232323232323232,"progressive redundant token masking strategy is also utilized to reduce the distilling
16"
ABSTRACT,0.03434343434343434,"costs and avoid falling into local optima. Experiment results prove that Hybrid
17"
ABSTRACT,0.03636363636363636,"Distill can achieve superior performance on different benchmarks.
18"
INTRODUCTION,0.03838383838383838,"1
Introduction
19"
INTRODUCTION,0.04040404040404041,"Pre-training followed by fine-tuning has been a common paradigm for computer vision tasks since
20"
INTRODUCTION,0.04242424242424243,"the advent of deep learning. In the past decade, supervised image classification [16, 10, 24] over the
21"
INTRODUCTION,0.044444444444444446,"widely used ImageNet [32] has dominated the pretraining mode. Recently, self-supervised learning
22"
INTRODUCTION,0.046464646464646465,"has emerged as a promising alternative, particularly with two approaches: Contrastive Learning (CL)
23"
INTRODUCTION,0.048484848484848485,"and Masked Image Modeling (MIM). The former one, typical representatives are MoCo [14] and
24"
INTRODUCTION,0.050505050505050504,"SimCLR [4], learns invariant representation for positive views, which are usually defined as different
25"
INTRODUCTION,0.052525252525252523,"augmentations of the same image. Furthermore, CLIP [30] extends CL to a multi-modal manner,
26"
INTRODUCTION,0.05454545454545454,"which utilizes the corresponding text description of the given image as positive pairs. While the
27"
INTRODUCTION,0.05656565656565657,"latter, including MAE [13] and SimMIM [44], aims to reconstruct the masked image patches and has
28"
INTRODUCTION,0.05858585858585859,"become mainstream due to its efficiency brought by mask operations.
29"
INTRODUCTION,0.06060606060606061,"The different pre-training paradigms of CL and MIM facilitate a series of studies [43, 27, 38] that
30"
INTRODUCTION,0.06262626262626263,"aim at understanding their respective properties. These studies point out that CL pre-training behaves
31"
INTRODUCTION,0.06464646464646465,"more similar to supervised pre-training, i.e., it provides models with longer-range global patterns
32"
INTRODUCTION,0.06666666666666667,"targeting object shape, particularly in the last few layers [27], and enables feature representation with
33"
INTRODUCTION,0.06868686868686869,"better discrimination. However, as shown in Fig. 1(a), CL pre-training causes self-attention in the
34"
INTRODUCTION,0.0707070707070707,"last few layers to collapse into homogeneity, with attention distances located within a very small
35"
INTRODUCTION,0.07272727272727272,"distance range. In contrast, MIM pre-training can bring more diverse attention and evenly distributed
36"
INTRODUCTION,0.07474747474747474,"representations to all layers [43, 27], and this diversity contributes to its better generalization on
37"
INTRODUCTION,0.07676767676767676,"downstream fine-tuning. Nevertheless, MIM pre-training is slower to converge and underperforms in
38"
INTRODUCTION,0.07878787878787878,"linear probing, mainly due to its lack of discrimination ability.
39"
INTRODUCTION,0.08080808080808081,"Since discrimination and diversity are both crucial for downstream adaptation, previous methods
40"
INTRODUCTION,0.08282828282828283,"[41, 11, 23, 40, 29] propose to utilize feature distillation to combine the benefits of CL and MIM.
41"
INTRODUCTION,0.08484848484848485,"Among them, dBOT [23] replaces the reconstructing objective of MAE with the feature maps of
42"
INTRODUCTION,0.08686868686868687,"different pre-trained teachers. It finds that feature distillation can bring diverse attention no matter
43"
INTRODUCTION,0.08888888888888889,"what the teacher model is, and the performance is comparable across different teachers, even with
44"
INTRODUCTION,0.09090909090909091,"the randomly initialized ones, after multi-stage distillation. Also observing that distillation can yield
45"
INTRODUCTION,0.09292929292929293,"diversity benefits, FD [41] directly distills feature maps from supervised/CL teachers to relieve the
46"
INTRODUCTION,0.09494949494949495,"attention collapse and achieves considerable downstream performance gains. Although interesting
47"
INTRODUCTION,0.09696969696969697,"and important, we argue that their findings are incomplete.
48"
INTRODUCTION,0.09898989898989899,"This paper re-examines these findings and reconsiders the importance of diversity and discrimination.
49"
INTRODUCTION,0.10101010101010101,"Our study reveals the following observations: (i) The increase in diversity derives from the
50"
INTRODUCTION,0.10303030303030303,"asymmetric architecture designs, rather than feature distillation itself. (Section 2.2) After
51"
INTRODUCTION,0.10505050505050505,"removing the asymmetric attention in [41] and encoder-decoder designs in [23] and keeping the same
52"
INTRODUCTION,0.10707070707070707,"teacher and student structures, we observe a negligible increase (or even a decrease) in attention
53"
INTRODUCTION,0.10909090909090909,"diversity. (ii) The asymmetric decoder de facto harm the discrimination over the encoder
54"
INTRODUCTION,0.1111111111111111,"side, for it migrates the semantic information of the teacher model. (Section 2.3) Due to the
55"
INTRODUCTION,0.11313131313131314,"decomposition of the encoding and decoding functions, student encoders tend to summarize more
56"
INTRODUCTION,0.11515151515151516,"general information, thus gradually losing the semantics obtained from teachers and yielding similar
57"
INTRODUCTION,0.11717171717171718,"results after multi-stage distillation [23]. (iii) Mask reconstruction of high-level semantics does
58"
INTRODUCTION,0.1191919191919192,"not help improve diversity. (Section 2.4) The phenomenon of reconstructing high-level information
59"
INTRODUCTION,0.12121212121212122,"[29, 11, 40] is similar to direct feature distillation and lacks the diversity found in MIM, which
60"
INTRODUCTION,0.12323232323232323,"implies that the attention diversity of MIM mainly comes from low-level reconstruction objectives.
61"
INTRODUCTION,0.12525252525252525,"Based on the above observations, we argue that a better distillation strategy is needed to help student
62"
INTRODUCTION,0.12727272727272726,"models inherit both diversity and discrimination. To this end, we propose a simple but effective
63"
INTRODUCTION,0.1292929292929293,"feature distillation method, termed as Hybrid Distill, to fully exploit the pre-trained model. Unlike
64"
INTRODUCTION,0.13131313131313133,"previous works, Hybrid Distill aims to distill knowledge from both the supervised/CL and MIM
65"
INTRODUCTION,0.13333333333333333,"teacher, allowing the student model to benefit from their respective advantages. To realize this, Hybrid
66"
INTRODUCTION,0.13535353535353536,"Distill makes careful designs for the distilling target and location. Specifically, we find that the
67"
INTRODUCTION,0.13737373737373737,"relational modeling ability of MIM is crucial for preserving token diversity, while the feature
68"
INTRODUCTION,0.1393939393939394,"maps of supervised/CL teachers are beneficial for discrimination. Accordingly, we set the token
69"
INTRODUCTION,0.1414141414141414,"relations of the MIM teacher and the feature maps of the supervised/CL teacher as the distilling
70"
INTRODUCTION,0.14343434343434344,"objectives of Hybrid Distill. The token relations are distilled in layers preceding the final layer where
71"
INTRODUCTION,0.14545454545454545,"attention collapse tends to occur, while the feature maps are distilled in the final layer to preserve
72"
INTRODUCTION,0.14747474747474748,"semantics. Additionally, Hybrid Distill proposes a progressive redundant token masking strategy
73"
INTRODUCTION,0.1494949494949495,"to reduce distilling costs and prevent falling into local optima. Experiment results show that the
74"
INTRODUCTION,0.15151515151515152,"above distilling strategy works surprisingly well even when using MAE and CLIP teachers, i.e., MAE
75"
INTRODUCTION,0.15353535353535352,"pretrained with only 1.28M ImageNet images can also boost the large-scale (400M) pretrained CLIP
76"
INTRODUCTION,0.15555555555555556,"teacher on different downstream tasks.
77"
INTRODUCTION,0.15757575757575756,"In a nutshell, this paper makes the following distribution:
78"
INTRODUCTION,0.1595959595959596,"• We re-examine the findings of previous feature distilling methods and point out that their increas-
79"
INTRODUCTION,0.16161616161616163,"ing diversity mainly arises from the use of asymmetric designs, while these designs may in turn
80"
INTRODUCTION,0.16363636363636364,"compromise the discrimination.
81"
INTRODUCTION,0.16565656565656567,"• We further propose a Hybrid Distill framework that utilized both supervised/CL and MIM teacher
82"
INTRODUCTION,0.16767676767676767,"to provide the student with higher-quality discrimination and diversity. Distilling targets and locations
83"
INTRODUCTION,0.1696969696969697,"are carefully designed in Hybrid Distill to fully exploit the strengths of both teachers.
84"
INTRODUCTION,0.1717171717171717,"• We conduct property analysis to demonstrate that the representations exhibit both discrimination
85"
INTRODUCTION,0.17373737373737375,"and diversity in our Hybrid Distill. Experiments on various downstream tasks, including classification,
86"
INTRODUCTION,0.17575757575757575,"detection, and segmentation, also showcase its superiority.
87"
INTRODUCTION,0.17777777777777778,"2
Model Evaluation: Diversity and Discrimination
88"
INTRODUCTION,0.1797979797979798,"This section re-examines the findings of previous feature distillation or mask feature reconstruction
89"
INTRODUCTION,0.18181818181818182,"works illustrated in Sec. 1 and highlights their limitations in incorporating diversity and discrimination.
90"
INTRODUCTION,0.18383838383838383,"(a) Before distillation. From left to right are: MAE, DeiT and CLIP."
INTRODUCTION,0.18585858585858586,"(b) DeiT distillation. From left to right are: no decoder, linear projection and asymmetric decoder."
INTRODUCTION,0.18787878787878787,"(c) CLIP distillation. From left to right are: no decoder, linear projection and asymmetric decoder."
INTRODUCTION,0.1898989898989899,"Figure 1: Average head distance after feature distillation with various decoders. (a) are the baselines.
(b) use the supervised DeiT model as the teacher. (c) use the CL-based CLIP model as the teacher."
PRELIMINARY,0.1919191919191919,"2.1
Preliminary
91"
PRELIMINARY,0.19393939393939394,"We first introduce the definitions of diversity and discrimination and the evaluation strategies we used.
92"
PRELIMINARY,0.19595959595959597,"Discrimination means that the representations contain more global patterns tailored to object shapes,
93"
PRELIMINARY,0.19797979797979798,"which is beneficial for recognizing objects and distinguishing images. Diversity is a relative concept,
94"
PRELIMINARY,0.2,"which means that the model pays more attention to local information and can achieve more evenly
95"
PRELIMINARY,0.20202020202020202,"distributed representations, particularly in the last few layers.
96"
PRELIMINARY,0.20404040404040405,"We measure these properties by average head distance [41, 10] and normalized mutual information
97"
PRELIMINARY,0.20606060606060606,"(NMI) [33]. The former calculates the average distance between the query tokens and the key tokens
98"
PRELIMINARY,0.2080808080808081,"based on their attention weights, providing insight into whether the attention is global or local. The
99"
PRELIMINARY,0.2101010101010101,"latter measures whether the attention is attending to different tokens or similar ones and is calculated
100"
PRELIMINARY,0.21212121212121213,"following [27]. Specifically, let a uniform distribution p(q) = 1"
PRELIMINARY,0.21414141414141413,"N represent the distribution of query
101"
PRELIMINARY,0.21616161616161617,"tokens, where N is the total token number. The joint distribution of query and key is then computed
102"
PRELIMINARY,0.21818181818181817,"as p(q, k) = π(k|q)p(q), where π(k|q) is the normalized self-attention matrix. Thus, NMI can be
103"
PRELIMINARY,0.2202020202020202,"calculated by
I(q,k)
√"
PRELIMINARY,0.2222222222222222,"H(q)H(k) where I(·, ·) is the mutual information and H(·) is the marginal entropy.
104"
THE INCREASE IN DIVERSITY DERIVES FROM THE ASYMMETRIC DESIGNS,0.22424242424242424,"2.2
The Increase in Diversity Derives from the Asymmetric Designs
105"
THE INCREASE IN DIVERSITY DERIVES FROM THE ASYMMETRIC DESIGNS,0.22626262626262628,"Fig. 1 measures the average head distance after feature distillation with a consistent encoder structure
106"
THE INCREASE IN DIVERSITY DERIVES FROM THE ASYMMETRIC DESIGNS,0.22828282828282828,"(vanilla Vision Transformer (ViT) [10]) for both the teacher and student models, along with various
107"
THE INCREASE IN DIVERSITY DERIVES FROM THE ASYMMETRIC DESIGNS,0.23030303030303031,"decoders only for the student. It can be seen that when the encoder is kept the same, using no decoder
108"
THE INCREASE IN DIVERSITY DERIVES FROM THE ASYMMETRIC DESIGNS,0.23232323232323232,"or linear projection decoder leads to a negligible increase (or even decrease) in attention diversity,
109"
THE INCREASE IN DIVERSITY DERIVES FROM THE ASYMMETRIC DESIGNS,0.23434343434343435,"reflecting that feature distilling itself cannot bring benefits to diversity. Adding some extra attention
110"
THE INCREASE IN DIVERSITY DERIVES FROM THE ASYMMETRIC DESIGNS,0.23636363636363636,"layers to the decoder can make the student encoder more diverse, but it hinders discrimination since
111"
THE INCREASE IN DIVERSITY DERIVES FROM THE ASYMMETRIC DESIGNS,0.2383838383838384,"the last layer no longer captures long-range patterns. Fig. 2(a) further compares NMI using the DeiT
112"
THE INCREASE IN DIVERSITY DERIVES FROM THE ASYMMETRIC DESIGNS,0.2404040404040404,"teacher and the results are in line with the attention visualization, i.e., without asymmetric designs,
113"
THE INCREASE IN DIVERSITY DERIVES FROM THE ASYMMETRIC DESIGNS,0.24242424242424243,"(b) Encoder and decoder 
(a) Various decoders 
(c) Mask feature reconstruction
Figure 2: The normalized mutual information (NMI) of (a) various decoders, (b) encoder and decoder,
and (c) mask feature reconstruction."
THE INCREASE IN DIVERSITY DERIVES FROM THE ASYMMETRIC DESIGNS,0.24444444444444444,"(a) Encoder and decoder 
(b) Mask feature reconstruction
Figure 3: Average head distance of (a) encoder and decoder, and (b) mask feature reconstruction."
THE INCREASE IN DIVERSITY DERIVES FROM THE ASYMMETRIC DESIGNS,0.24646464646464647,"the student collapses into homogeneity and pays attention to similar tokens in the last few layers.
114"
THE INCREASE IN DIVERSITY DERIVES FROM THE ASYMMETRIC DESIGNS,0.24848484848484848,"Conversely, the use of asymmetric decoders greatly reduces discrimination.
115"
THE INCREASE IN DIVERSITY DERIVES FROM THE ASYMMETRIC DESIGNS,0.2505050505050505,"The above discussions focus on varying decoders, while FD [41] introduces asymmetric designs to
116"
THE INCREASE IN DIVERSITY DERIVES FROM THE ASYMMETRIC DESIGNS,0.25252525252525254,"the encoder by adding additional learnable parameters and relative position bias to the attention layers
117"
THE INCREASE IN DIVERSITY DERIVES FROM THE ASYMMETRIC DESIGNS,0.2545454545454545,"of the student. In the appendix, we demonstrate that the increase in diversity observed in FD also
118"
THE INCREASE IN DIVERSITY DERIVES FROM THE ASYMMETRIC DESIGNS,0.25656565656565655,"arises from these designs and the diversity brought by them is not always significant.
119"
THE ASYMMETRIC DECODER HARMS THE ENCODER DISCRIMINATION,0.2585858585858586,"2.3
The Asymmetric Decoder Harms the Encoder Discrimination
120"
THE ASYMMETRIC DECODER HARMS THE ENCODER DISCRIMINATION,0.2606060606060606,"Fig. 3(a) and Fig. 2(b) further measure the average head distance and NMI of the asymmetric
121"
THE ASYMMETRIC DECODER HARMS THE ENCODER DISCRIMINATION,0.26262626262626265,"decoder. Our findings suggest that the decoder has transferred the discrimination of the teacher, as its
122"
THE ASYMMETRIC DECODER HARMS THE ENCODER DISCRIMINATION,0.26464646464646463,"behavior is similar to that of the last few layers of the teacher model where attention collapse occurs.
123"
THE ASYMMETRIC DECODER HARMS THE ENCODER DISCRIMINATION,0.26666666666666666,"Reducing the number of decoder layers does not eliminate this transfer, as further demonstrated
124"
THE ASYMMETRIC DECODER HARMS THE ENCODER DISCRIMINATION,0.2686868686868687,"in the appendix. Since only the student encoder is retained and applied to downstream tasks after
125"
THE ASYMMETRIC DECODER HARMS THE ENCODER DISCRIMINATION,0.27070707070707073,"distillation, the semantic information that the model maintained is weakened, which explains why in
126"
THE ASYMMETRIC DECODER HARMS THE ENCODER DISCRIMINATION,0.2727272727272727,"dBOT, different teachers tend to yield similarly-behaving models after multi-stage distilling. Note
127"
THE ASYMMETRIC DECODER HARMS THE ENCODER DISCRIMINATION,0.27474747474747474,"that dBOT conducts feature distilling in a mask reconstruction way, while we demonstrate in both
128"
THE ASYMMETRIC DECODER HARMS THE ENCODER DISCRIMINATION,0.2767676767676768,"Sec. 2.4 and the visualization in the appendix that it behaves similarly to directly distilling features.
129"
MASK RECONSTRUCTION OF HIGH-LEVEL SEMANTICS DOES NOT HELP IMPROVE DIVERSITY,0.2787878787878788,"2.4
Mask Reconstruction of High-Level Semantics Does not Help Improve Diversity
130"
MASK RECONSTRUCTION OF HIGH-LEVEL SEMANTICS DOES NOT HELP IMPROVE DIVERSITY,0.2808080808080808,"Fig. 3(b) and Fig. 2(c) examine the influence of mask reconstructing high-level information. To
131"
MASK RECONSTRUCTION OF HIGH-LEVEL SEMANTICS DOES NOT HELP IMPROVE DIVERSITY,0.2828282828282828,"eliminate the effect of the asymmetric decoder, we feed both the masks and tokens into the encoder
132"
MASK RECONSTRUCTION OF HIGH-LEVEL SEMANTICS DOES NOT HELP IMPROVE DIVERSITY,0.28484848484848485,"simultaneously and use only linear projection as the decoder. The overall process is thus similar
133"
MASK RECONSTRUCTION OF HIGH-LEVEL SEMANTICS DOES NOT HELP IMPROVE DIVERSITY,0.2868686868686869,"to SimMIM [44], except that we use the high-level information obtained from the supervised/CL
134"
MASK RECONSTRUCTION OF HIGH-LEVEL SEMANTICS DOES NOT HELP IMPROVE DIVERSITY,0.28888888888888886,"teacher as the distilling objective. Fig. 3(b) proves that reconstructing high-level information brings
135"
MASK RECONSTRUCTION OF HIGH-LEVEL SEMANTICS DOES NOT HELP IMPROVE DIVERSITY,0.2909090909090909,"no diversity gains towards directly distilling features, which is consistent with the finding of [45], i.e.,
136"
MASK RECONSTRUCTION OF HIGH-LEVEL SEMANTICS DOES NOT HELP IMPROVE DIVERSITY,0.29292929292929293,"reconstruction is unnecessary for MIM with semantic-rich teachers. This phenomenon also implies
137"
MASK RECONSTRUCTION OF HIGH-LEVEL SEMANTICS DOES NOT HELP IMPROVE DIVERSITY,0.29494949494949496,"that the diversity of MIM mainly arises from the low-level reconstructing objective rather than from
138"
MASK RECONSTRUCTION OF HIGH-LEVEL SEMANTICS DOES NOT HELP IMPROVE DIVERSITY,0.296969696969697,"the reconstruction itself, since diversity is absent in high-level reconstruction.
139"
HYBRID DISTILLATION,0.298989898989899,"3
Hybrid Distillation
140"
HYBRID DISTILLATION,0.301010101010101,"From the above discussion, we conclude that existing distillation pipelines have limitations in
141"
HYBRID DISTILLATION,0.30303030303030304,"providing discrimination and diversity. Thus, we further propose a novel hybrid distillation framework
142"
HYBRID DISTILLATION,0.30505050505050507,"to ensure these important properties, and this section elaborates on its details.
143"
HYBRID DISTILLATION,0.30707070707070705,"Sup/CL
Teacher"
HYBRID DISTILLATION,0.3090909090909091,Masked
HYBRID DISTILLATION,0.3111111111111111,Frozen
HYBRID DISTILLATION,0.31313131313131315,"MIM
Teacher"
HYBRID DISTILLATION,0.3151515151515151,"Student
#L-1 #L-2"
HYBRID DISTILLATION,0.31717171717171716,Features
HYBRID DISTILLATION,0.3191919191919192,"Relations
Relations Tuned"
HYBRID DISTILLATION,0.3212121212121212,Student #L
HYBRID DISTILLATION,0.32323232323232326,Features
HYBRID DISTILLATION,0.32525252525252524,"Token Dropping
Distillation"
HYBRID DISTILLATION,0.32727272727272727,Color Bar
HYBRID DISTILLATION,0.3292929292929293,"More Irrelevant
Unmasked"
HYBRID DISTILLATION,0.33131313131313134,Figure 4: Hybrid Distill pipeline and its effectiveness in ensuring discrimination and diversity.
OVERVIEW,0.3333333333333333,"3.1
Overview
144"
OVERVIEW,0.33535353535353535,"Given a supervised/CL pre-trained model Tc, and a MIM pre-trained model Tm, Hybrid Distill
145"
OVERVIEW,0.3373737373737374,"simultaneously distills knowledge from these two different types of pre-trained teachers, aims at
146"
OVERVIEW,0.3393939393939394,"combining their respective advantages to enhance the new representations in a randomly initialized
147"
OVERVIEW,0.3414141414141414,"student model Sθ where θ is its learnable parameters. ViT [10] is adopted for all the models in Hybrid
148"
OVERVIEW,0.3434343434343434,"Distill, and Tm is provided by MAE [13] while Tc is provided by DeiT [36] or CLIP [30].
149"
OVERVIEW,0.34545454545454546,"Specifically, the Hybrid Distill framework is shown in Fig. 4 and its overall objective is:
150"
OVERVIEW,0.3474747474747475,"max
θ
E
x∼XD {Tc(x) ⊙M, Sθ(M ⊙x)}"
OVERVIEW,0.34949494949494947,"+ αD {T ′
m(x) ⊙M, S′
θ(M ⊙x)} ,
(1)"
OVERVIEW,0.3515151515151515,"where ⊙is an element-wise product operation. M is a mask provided by the teacher model using
151"
OVERVIEW,0.35353535353535354,"the strategy described in Sec. 3.2 and M ⊙x denotes the unmasked patches. D(·, ·) is the distance
152"
OVERVIEW,0.35555555555555557,"measurement, and we use smooth L1 distance in our experiment. α is the hyperparameter that controls
153"
OVERVIEW,0.3575757575757576,"the contribution of the two teacher models. Note that we do not distill the final output features Tm(x)
154"
OVERVIEW,0.3595959595959596,"for the MIM pre-trained model but instead use the token relations in the previous ViT layers, denote
155"
OVERVIEW,0.3616161616161616,"as T ′
m(x), as the learning objective. Details are illustrated in Sec. 3.2.
156"
DISTILLING STRATEGIES,0.36363636363636365,"3.2
Distilling Strategies
157"
DISTILLING STRATEGIES,0.3656565656565657,"What to distill?
Different from previous works [41, 11, 45] that directly distill the features of
158"
DISTILLING STRATEGIES,0.36767676767676766,"teacher models, we analyze that the diversity of MIM pre-trained models arises from their superior
159"
DISTILLING STRATEGIES,0.3696969696969697,"token-level relationship modeling, while supervised/CL pre-trained models excel at image-level
160"
DISTILLING STRATEGIES,0.3717171717171717,"discrimination. Hence, we apply different distilling targets to Tc and Tm to fully utilize their respective
161"
DISTILLING STRATEGIES,0.37373737373737376,"advantages. Specifically, taking Tm as an example, we decompose Tm into T 1
m ◦T 2
m ◦· · · ◦T L
m,
162"
DISTILLING STRATEGIES,0.37575757575757573,"where T i
m is the ith layer of Tm and is composed of a multi-head self-attention (MSA) layer and an
163"
DISTILLING STRATEGIES,0.37777777777777777,"MLP layer. Given xi
m as the input of the ith layer, the calculation in T i
m can be represented as:
164"
DISTILLING STRATEGIES,0.3797979797979798,"Ri
m(xi
m) = Qi
m(xi
m)Ki
m(xi
m)
T ,"
DISTILLING STRATEGIES,0.38181818181818183,"MSAi
m(xi
m) = Softmax

Ri
m(xi
m)/
√"
DISTILLING STRATEGIES,0.3838383838383838,"d

V i
m(xi
m),"
DISTILLING STRATEGIES,0.38585858585858585,"T i
m(xi
m) = xi
m + MLP(xi
m + MSAi
m(xi
m)), (2)"
DISTILLING STRATEGIES,0.3878787878787879,"where Qi
m, Ki
m, and V i
m denotes the linear mappings for xi
m and d equals to the dimension of xi
m.
165"
DISTILLING STRATEGIES,0.3898989898989899,"Then, for MIM pre-trained model Tm, we set the token relation Ri
m(xi
m) as the distilling target, while
166"
DISTILLING STRATEGIES,0.39191919191919194,"for supervised/CL pretrained model Tc, we set the output features T i
c(xi
c) as the target.
167"
DISTILLING STRATEGIES,0.3939393939393939,"Where to distill?
As shown in Fig. 1(a), supervised and CL models tend to collapse into ho-
168"
DISTILLING STRATEGIES,0.39595959595959596,"mogeneity in the last few layers, so Hybrid Distill chooses to distill token relations from Tm in
169"
DISTILLING STRATEGIES,0.397979797979798,"these layers to address this collapse and improve diversity. While for the last layer of S which is
170"
DISTILLING STRATEGIES,0.4,"MAE 
Hybrid Distill"
DISTILLING STRATEGIES,0.402020202020202,"Layer 10
Layer 12
Layer 10
Layer 12"
DISTILLING STRATEGIES,0.40404040404040403,"(a) Average head distance
(b) NMI
(c) Attention visualization
Figure 5: The (a) average head distance, (b) NMI, and (c) attention visualization of the student model
obtained from Hybrid Distill with MAE and CLIP teachers."
DISTILLING STRATEGIES,0.40606060606060607,"crucial for discrimination, Hybrid Distill directly distills knowledge from Tc using the output features.
171"
DISTILLING STRATEGIES,0.4080808080808081,"Specifically, we distill token relations from Tm at the L −1 and L −2 layers and distill features from
172"
DISTILLING STRATEGIES,0.4101010101010101,"Tc at the L layer of ViT. Accordingly, the learning objective Tc(x) and T ′
m(x) in Eq. 1 become:
173"
DISTILLING STRATEGIES,0.4121212121212121,"Tc(x) = T L
c (x),"
DISTILLING STRATEGIES,0.41414141414141414,"T ′
m(x) = [RL−1
m
(x), RL−2
m
(x)].
(3)"
DISTILLING STRATEGIES,0.4161616161616162,"Distillation acceleration via redundant token dropping.
Suppose the input is divided into N
174"
DISTILLING STRATEGIES,0.41818181818181815,"tokens, i.e., x ∈RN×d, Hybrid Distill can directly distill token relations and features using all the N
175"
DISTILLING STRATEGIES,0.4202020202020202,"tokens. However, since some tokens in the image may be redundant, it is promising to mask these
176"
DISTILLING STRATEGIES,0.4222222222222222,"tokens for the student model S to reduce memory and time costs. Furthermore, removing redundant
177"
DISTILLING STRATEGIES,0.42424242424242425,"tokens can play a regulatory role, helping the model avoid local optima during the distillation process.
178"
DISTILLING STRATEGIES,0.4262626262626263,"Specifically, we use the MIM pre-trained teacher Tm to guide the identification of redundant tokens
179"
DISTILLING STRATEGIES,0.42828282828282827,"and provide the token mask. Inspired by [20], we propose a progressive redundant token masking
180"
DISTILLING STRATEGIES,0.4303030303030303,"strategy, which generates token masks at different layers of Tm in a progressive manner. Given xi
m
181"
DISTILLING STRATEGIES,0.43232323232323233,"and the mask M i−1
m
provided by the previous layer, we define the tokens in xi
m ⊙M i−1
m
and are
182"
DISTILLING STRATEGIES,0.43434343434343436,"top K% similar to their average token as redundant tokens in the ith layer and generate a redundant
183"
DISTILLING STRATEGIES,0.43636363636363634,"token mask for them. The above process is denoted as T(xi
m ⊙M i−1
m , K). Next, we update M i
m
184"
DISTILLING STRATEGIES,0.4383838383838384,"using T(xi
m ⊙M i−1
m , K) and M i−1
m
as follows:
185"
DISTILLING STRATEGIES,0.4404040404040404,"M i
m =
M i−1
m
−T(xi
m ⊙M i−1
m , K),
if i ∈I,
M i−1
m
if i /∈I.
(4)"
DISTILLING STRATEGIES,0.44242424242424244,"where I is the set of layers required to update the token mask. For M 0
m, all elements are set to 1.
186"
DISTILLING STRATEGIES,0.4444444444444444,"Finally, we set the mask M for the student model as M = M L
m.
187"
PROPERTY ANALYSIS,0.44646464646464645,"3.3
Property Analysis
188"
PROPERTY ANALYSIS,0.4484848484848485,"Average head distance. Fig. 5(a) visualizes the average head distance of the student model with CLIP
189"
PROPERTY ANALYSIS,0.4505050505050505,"and MAE as teachers, while the visualization of CLIP and MAE teachers themselves are included in
190"
PROPERTY ANALYSIS,0.45252525252525255,"Fig. 1(a). These visualizations demonstrate that Hybrid Distill enhances the discrimination ability of
191"
PROPERTY ANALYSIS,0.45454545454545453,"the student model, compensating for the semantic lacking problem of the MAE teacher. Moreover,
192"
PROPERTY ANALYSIS,0.45656565656565656,"Hybrid Distill avoids succeeding attention collapse from the CLIP teacher and generates more diverse
193"
PROPERTY ANALYSIS,0.4585858585858586,"representations in the last few layers.
194"
PROPERTY ANALYSIS,0.46060606060606063,"Normalized mutual information. Fig. 5(b) further inspects the NMI. The results demonstrate
195"
PROPERTY ANALYSIS,0.4626262626262626,"that the mutual information between tokens is significantly enhanced in the layers where the MAE
196"
PROPERTY ANALYSIS,0.46464646464646464,"token relationships are distilled. Besides, this enhancement does not compromise the discrimination
197"
PROPERTY ANALYSIS,0.4666666666666667,"obtained from CLIP, as evidenced by attention in the final layers still attending to similar tokens.
198"
PROPERTY ANALYSIS,0.4686868686868687,"Attention visualization. Fig. 5(c) further visualizes the attention between a given query and other
199"
PROPERTY ANALYSIS,0.4707070707070707,"keys at different layers to examine behaviors. Compared to MAE, Hybrid Distill exhibits better
200"
PROPERTY ANALYSIS,0.4727272727272727,"discrimination ability, i.e., the query tokens of the last layer have global attention towards the main
201"
PROPERTY ANALYSIS,0.47474747474747475,"object of the images, regardless of their location. Besides, Hybrid Distill also improves the locality of
202"
PROPERTY ANALYSIS,0.4767676767676768,"the model in the 10th layer, where attention collapse is known to occur in the CLIP teacher.
203"
DISCUSSION WITH OTHER DISTILLATION METHODS,0.47878787878787876,"3.4
Discussion with Other Distillation Methods
204"
DISCUSSION WITH OTHER DISTILLATION METHODS,0.4808080808080808,"Compared to previous distillation methods [41, 11, 23, 40, 29], Hybrid Distill stands out by not being
205"
DISCUSSION WITH OTHER DISTILLATION METHODS,0.48282828282828283,"restricted to using a single teacher network. In addition to addressing the limitations of single-teacher
206"
DISCUSSION WITH OTHER DISTILLATION METHODS,0.48484848484848486,"Table 1: Main results on ImageNet-1k classification, COCO detection and instance segmentation,
and ADE20K semantic segmentation. ⋆: using MAE+DeiT teachers. †: using MAE+CLIP teachers."
DISCUSSION WITH OTHER DISTILLATION METHODS,0.4868686868686869,"Method
Backbone
Distill.
IN-1K
COCO
ADE20K
APbox
APMask"
DISCUSSION WITH OTHER DISTILLATION METHODS,0.4888888888888889,DeiT [36] ViT-B
DISCUSSION WITH OTHER DISTILLATION METHODS,0.4909090909090909,"81.8
46.9
41.5
47.0
MoCo v3 [7]
83.2
45.5
40.5
47.1
DINO [2]
83.3
46.8
41.5
47.2
MAE [13]
83.6
48.4
42.6
48.1
CAE [5]
83.3
48.0
42.3
47.7
SdAE [8]
84.1
48.9
43.0
48.6
CLIP [30]
83.6
47.6
42.3
49.6
MAE [13]
ViT-L
85.9
54.0
47.1
53.6
CLIP [30]
86.1
52.7
46.2
54.2
Distill-DeiT"
DISCUSSION WITH OTHER DISTILLATION METHODS,0.49292929292929294,"ViT-B
✓"
DISCUSSION WITH OTHER DISTILLATION METHODS,0.494949494949495,"82.0
47.7
42.1
47.3
Distill-MAE
83.7
49.1
43.1
47.8
Distill-CLIP
84.8
49.5
43.5
50.3
Hybrid Distill⋆"
DISCUSSION WITH OTHER DISTILLATION METHODS,0.49696969696969695,"ViT-B
✓
83.7
50.3
44.2
49.1
Hybrid Distill†
85.1
50.6
44.4
51.5
Hybrid Distill†
ViT-L
✓
88.0
54.6
47.6
56.3"
DISCUSSION WITH OTHER DISTILLATION METHODS,0.498989898989899,"Table 2: Classification results on CIFAR100, Cars and INautralist19. ⋆: using MAE+DeiT teachers.
†: using MAE+CLIP teachers."
DISCUSSION WITH OTHER DISTILLATION METHODS,0.501010101010101,"Method
Backbone
CIFAR100
Cars
INaturalist19
Mean
DeiT [36]
ViT-B
91.4
92.0
77.3
86.9
MAE [13]
ViT-B
89.6
89.5
75.2
84.8
Distill-DeiT
ViT-B
91.2
92.5
78.3
87.3
Distill-MAE
ViT-B
90.3
93.1
79.0
87.5
Distill-CLIP
ViT-B
91.6
94.3
81.6
89.2
Hybrid Distill⋆
ViT-B
91.7
94.1
80.2
88.7
Hybrid Distill†
ViT-B
92.0
94.5
81.9
89.5
Hybrid Distill†
ViT-L
94.5
95.6
85.3
91.8"
DISCUSSION WITH OTHER DISTILLATION METHODS,0.503030303030303,"distillation in enriching diversity (as discussed in Sec. 2), a more direct factor is that single-teacher
207"
DISCUSSION WITH OTHER DISTILLATION METHODS,0.5050505050505051,"distillation cannot create new knowledge, e.g., creating additional discrimination for the student
208"
DISCUSSION WITH OTHER DISTILLATION METHODS,0.5070707070707071,"model when using the MIM teacher. Therefore, we believe that combining and utilizing existing
209"
DISCUSSION WITH OTHER DISTILLATION METHODS,0.509090909090909,"knowledge from various teachers is more effective and convenient. Furthermore, with the growing
210"
DISCUSSION WITH OTHER DISTILLATION METHODS,0.5111111111111111,"availability of large-scale pre-trained models within the community, it becomes increasingly valuable
211"
DISCUSSION WITH OTHER DISTILLATION METHODS,0.5131313131313131,"to explore new ways to utilize these models and combine their strengths. This further enhances the
212"
DISCUSSION WITH OTHER DISTILLATION METHODS,0.5151515151515151,"practical value of our Hybrid Distill, and we hope our work would shed light on new directions.
213"
EXPERIMENTS,0.5171717171717172,"4
Experiments
214"
IMPLEMENTATION DETAILS,0.5191919191919192,"4.1
Implementation Details
215"
IMPLEMENTATION DETAILS,0.5212121212121212,"Hybrid Distill is conducted on 8 V100 GPUs and is built on the codebase of dBOT [23], so most of its
216"
IMPLEMENTATION DETAILS,0.5232323232323233,"settings are in line with dBOT. Specifically, the batch size, learning rate, and weight decay are set to
217"
IMPLEMENTATION DETAILS,0.5252525252525253,"1024 and 6e-4, and 0.05, respectively. AdamW [26] optimizer and cosine decay [25] schedule is used.
218"
IMPLEMENTATION DETAILS,0.5272727272727272,"The input size is 2242. For ViT-B, the distillation is based on ImageNet-1K and the epoch is 300
219"
IMPLEMENTATION DETAILS,0.5292929292929293,"for main results and 100 for ablation studies. For ViT-L, the distillation is based on ImageNet-21K
220"
IMPLEMENTATION DETAILS,0.5313131313131313,"and the epoch is 40. The hyperparameter α is set to 1.0 and the redundant token masking set I is set
221"
IMPLEMENTATION DETAILS,0.5333333333333333,"to [0, L/3, 2L/3] following [20]. The performances are tested on different downstream tasks. For
222"
IMPLEMENTATION DETAILS,0.5353535353535354,"classification, we report results on ImageNet-1K, CIFAR100 [19], Cars [18], and iNaturalist19 [37].
223"
IMPLEMENTATION DETAILS,0.5373737373737374,"For object detection and instance segmentation, we fine-tune the student model on COCO [22] using
224"
IMPLEMENTATION DETAILS,0.5393939393939394,"Mask-RCNN [15] following [5]. For semantic segmentation, the evaluation is conducted on ADE20K
225"
IMPLEMENTATION DETAILS,0.5414141414141415,"[47] using the ViT with UperNet [42] following [5, 8]. More details are included in the appendix.
226"
MAIN RESULTS,0.5434343434343434,"4.2
Main Results
227"
MAIN RESULTS,0.5454545454545454,"This section presents benchmark results of Hybrid Distill on different downstream. We also list results
228"
MAIN RESULTS,0.5474747474747474,"for supervised and self-supervised pre-trained models, as well as 300-epoch uni-distillation baselines
229"
MAIN RESULTS,0.5494949494949495,"Table 3: Different combinations of two teacher
models. Tc(x): DeiT, Tm(x): MAE."
MAIN RESULTS,0.5515151515151515,"Targets
APbox
APmask"
MAIN RESULTS,0.5535353535353535,"Tc(x)
47.5
41.8
Tm(x)
48.9
43.1
Tc(x) + T ′
c(x)
46.8
41.5
Tm(x) + T ′
m(x)
48.9
43.2
Tc(x) + T ′
m(x)
50.0
43.9"
MAIN RESULTS,0.5555555555555556,"Table 4: Different combinations of two teacher
models. Tc(x): CLIP, Tm(x): MAE. ⋆: using the
ImageNet-100 pretrained weights."
MAIN RESULTS,0.5575757575757576,"Targets
APbox
APmask"
MAIN RESULTS,0.5595959595959596,"Tc(x)
49.1
43.1
Tm(x)
48.9
43.1
Tc(x) + T ′
c(x)
49.1
43.2
Tc(x) + T ′
m(x)
50.4
44.1
Tc(x) + T ′
m(x)⋆
49.5
43.5
Table 5: The distilling targets of T ′
m(x). Tc(x):
DeiT, Tm(x): MAE. ⋆means distilling MAE and
DeiT features at the last layer."
MAIN RESULTS,0.5616161616161616,"Targets
APbox
APmask"
MAIN RESULTS,0.5636363636363636,"T i
m
⋆
47.7
42.1
T i
m
49.6
43.5
MSAi
m
49.8
43.7
Ri
m
50.0
43.9"
MAIN RESULTS,0.5656565656565656,"Table 6: The distilling targets of T ′
m(x). Tc(x):
CLIP, Tm(x): MAE."
MAIN RESULTS,0.5676767676767677,"Targets
APbox
APmask"
MAIN RESULTS,0.5696969696969697,"T i
m
49.9
44.0
MSAi
m
50.1
44.0
Ri
m
50.4
44.1"
MAIN RESULTS,0.5717171717171717,"which use the same symmetrical structures as Hybrid Distill, for comparison. As shown in Tab. 1,
230"
MAIN RESULTS,0.5737373737373738,"Hybrid Distill achieves performance gains on all downstream tasks, especially for the dense-level
231"
MAIN RESULTS,0.5757575757575758,"ones. Specifically, although the performance of DeiT is suboptimal, its strength can be complementary
232"
MAIN RESULTS,0.5777777777777777,"to MAE and brings considerable benefits, i.e., when using DeiT and MAE teachers, Hybrid Distill
233"
MAIN RESULTS,0.5797979797979798,"achieves 50.3 APbox and 44.2 APmask on COCO, as well as 49.1 mIoU on ADE20K, surpassing
234"
MAIN RESULTS,0.5818181818181818,"Distill-MAE by 1.2, 1.1, and 1.3, respectively. Similarly, Hybrid Distill achieves 50.6 APbox and
235"
MAIN RESULTS,0.5838383838383838,"44.4 APmask on COCO, as well as 51.5 mIoU on ADE20K when using CLIP and MAE teachers,
236"
MAIN RESULTS,0.5858585858585859,"outperforming Distill-CLIP by 1.1, 0.9, and 1.2, respectively. When using the VIT-L backbone, the
237"
MAIN RESULTS,0.5878787878787879,"performance can be further boosted to 54.6 APbox, 47.6 APmask and 56.3 mIoU on respective tasks.
238"
MAIN RESULTS,0.5898989898989899,"The improvement on ImageNet-1k is not significant, probably because the distillation is performed on
239"
MAIN RESULTS,0.591919191919192,"the same dataset, thus increasing diversity fails to bring further gains. In Tab. 2, we further evaluate
240"
MAIN RESULTS,0.593939393939394,"Hybrid Distill on several small-scale classification datasets and observe more significant gains.
241"
ABLATION STUDY,0.5959595959595959,"4.3
Ablation Study
242"
ABLATION STUDY,0.597979797979798,"This section ablates different variants of Hybrid Distill. The results are reported on dense-level COCO
243"
ABLATION STUDY,0.6,"detection and segmentation tasks, as diversity has a stronger influence on these dense-level tasks [27].
244"
ABLATION STUDY,0.602020202020202,"Different combinations of two teachers.
We first evaluate the benefits of combining two teachers
245"
ABLATION STUDY,0.604040404040404,"for distillation. As shown in Tab. 3, adding additional MAE attention regularization can bring
246"
ABLATION STUDY,0.6060606060606061,"noticeable improvements (2.5 on APbox and 2.1 on APmask) compared to directly distilling from the
247"
ABLATION STUDY,0.6080808080808081,"DeiT teacher. Moreover, the additional attention regularization cannot bring benefits when only using
248"
ABLATION STUDY,0.6101010101010101,"a single DeiT teacher, which suggests that the benefits come from the introduction of MAE teacher.
249"
ABLATION STUDY,0.6121212121212121,"The above conclusions are consistent when using CLIP and MAE teachers, as illustrated in Tab. 4.
250"
ABLATION STUDY,0.6141414141414141,"We also try a much weaker version of MAE teacher which is only pre-trained on ImageNet-100 for
251"
ABLATION STUDY,0.6161616161616161,"100 epochs in Tab. 4. We lower the weight of this teacher to avoid its impact on discrimination. The
252"
ABLATION STUDY,0.6181818181818182,"results are still positive, which reflects the power of the MIM pre-training in modeling diversity.
253"
ABLATION STUDY,0.6202020202020202,"Distilling target of the MIM teacher.
We then examine the distilling target of the MIM teacher.
254"
ABLATION STUDY,0.6222222222222222,"As shown in Tab. 5, distilling the relation Ri
m brings the best detection performance (50.0APbox).
255"
ABLATION STUDY,0.6242424242424243,"Distilling MSAi
m achieves a close performance (49.8APbox) since its essential is also distilling
256"
ABLATION STUDY,0.6262626262626263,"relationships, while directly distilling the feature maps T i
m brings the worst performance (49.6APbox).
257"
ABLATION STUDY,0.6282828282828283,"Nevertheless, all these schemes outperform the DeiT distillation baseline, and the trends are consistent
258"
ABLATION STUDY,0.6303030303030303,"when using CLIP and MAE teachers, as shown in Tab. 6. Besides, we also evaluate a basic setting
259"
ABLATION STUDY,0.6323232323232323,"that directly distills the features of both the MAE and DeiT teachers at the last layer. The result is far
260"
ABLATION STUDY,0.6343434343434343,"from satisfactory, which highlights the effectiveness of the designs in Hybrid Distill.
261"
ABLATION STUDY,0.6363636363636364,"Distilling position of the MIM teacher.
Tab. 7 inspect the distilling position of the MIM teacher.
262"
ABLATION STUDY,0.6383838383838384,"We first experiment with distilling MAE relations at the front, middle, and back layers. Distilling at
263"
ABLATION STUDY,0.6404040404040404,"the back layers achieves better results, i.e., 1.5APbox and 2.4APbox gains towards distilling at the
264"
ABLATION STUDY,0.6424242424242425,Table 7: The distilling position of Tm.
ABLATION STUDY,0.6444444444444445,"Distilling layers
APbox
APmask"
ABLATION STUDY,0.6464646464646465,"1-11
48.8
43.0
1,2,3
47.4
41.9
5,6,7
48.3
42.7
9,10,11
49.8
43.7
10,11
50.0
43.9
11
49.2
43.3"
ABLATION STUDY,0.6484848484848484,Table 8: The token masking strategy.
ABLATION STUDY,0.6505050505050505,"Strategy
Ratio
APbox
APmask"
ABLATION STUDY,0.6525252525252525,"No
100%
50.0
43.9
Random
35%
49.2
43.3
Direct
35%
49.6
43.7
Progressive
13%(50%3)
48.4
42.8
Progressive
34%(70%3)
49.9
43.8
Progressive
73%(90%3)
49.9
43.8"
ABLATION STUDY,0.6545454545454545,"front and middle, respectively. The results are consistent with the fact that attention collapse tends to
265"
ABLATION STUDY,0.6565656565656566,"occur in these back layers. We then ablate the number of distilling layers and find that distilling at the
266"
ABLATION STUDY,0.6585858585858586,"two layers preceding the final layer (i.e., 10,11) contributes to the best results.
267"
ABLATION STUDY,0.6606060606060606,"Token masking strategy.
Tab. 8 studies different masking strategies for the student model. Since
268"
ABLATION STUDY,0.6626262626262627,"we progressive drop the redundant tokens three times, the actual tokens used in the student model are
269"
ABLATION STUDY,0.6646464646464646,"(1 −K)3%. We observe that when dropping 30% tokens at a time, Hybrid Distill achieves very close
270"
ABLATION STUDY,0.6666666666666666,"performance (49.9APbox and 43.8APmask) to the no masking results and outperforms the random
271"
ABLATION STUDY,0.6686868686868687,"masking strategy and the direct masking strategy which only generates token mask at the last layer. In
272"
ABLATION STUDY,0.6707070707070707,"addition, we notice that our token masking strategy also has a regularizing effect, which can prevent
273"
ABLATION STUDY,0.6727272727272727,"the model from falling into a locally optimal when training for longer epochs. Details about this
274"
ABLATION STUDY,0.6747474747474748,"effect are included in the appendix.
275"
RELATED WORK,0.6767676767676768,"5
Related Work
276"
RELATED WORK,0.6787878787878788,"Representation learning. Pre-training on large-scale datasets (e.g., ImageNet [32], JFT [34], Kinetics
277"
RELATED WORK,0.6808080808080809,"[3], etc.) is typically utilized for downstream initialization. Except for the common supervised pre-
278"
RELATED WORK,0.6828282828282828,"training [16, 10, 24], contrastive learning (CL) [4, 14, 6, 12] and masked image modeling (MIM)
279"
RELATED WORK,0.6848484848484848,"[1, 44, 13] dominate the recent research. The former is achieved by pulling close the features of two
280"
RELATED WORK,0.6868686868686869,"different augment views of the input image. While the latter, inspired by masked language modeling
281"
RELATED WORK,0.6888888888888889,"[17, 46] in NLP, is realized by reconstructing the mask part of the input image. Recently multi-model
282"
RELATED WORK,0.6909090909090909,"extensions [30, 9, 21] of the CL pre-training have also been proposed by utilizing the paired text
283"
RELATED WORK,0.692929292929293,"description of the given image. These different types of pre-training frameworks are proven to have
284"
RELATED WORK,0.694949494949495,"different properties [27, 43], and this paper aims to combine their respective excellent properties to
285"
RELATED WORK,0.696969696969697,"boost a student model.
286"
RELATED WORK,0.6989898989898989,"Knowledge distillation. Knowledge distillation [28, 35, 31] utilizes a well-trained teacher to guide
287"
RELATED WORK,0.701010101010101,"the feature learning of the student model, thus transferring its ability to the student. Beyond its
288"
RELATED WORK,0.703030303030303,"success in supervised learning, some recent works [41, 11, 39, 40, 29] utilize it to extend existing
289"
RELATED WORK,0.705050505050505,"pretrained models or paradigms. Feature distillation (FD) [41] finds that distilling the feature map
290"
RELATED WORK,0.7070707070707071,"of the supervised/CL pretrained teacher can bring diverse representation to the student and make it
291"
RELATED WORK,0.7090909090909091,"more friendly for downstream fine-tuning. dBOT [23], MVP [40], and BEiT v2 [29] change the mask
292"
RELATED WORK,0.7111111111111111,"reconstruction object of MIM to the knowledge of the teacher model to boost MIM pre-training with
293"
RELATED WORK,0.7131313131313132,"semantic information. In this paper, we analyze their properties and propose a new hybrid distillation
294"
RELATED WORK,0.7151515151515152,"framework to deal with their deficiencies.
295"
CONCLUSION,0.7171717171717171,"6
Conclusion
296"
CONCLUSION,0.7191919191919192,"This paper proposed a hybrid distillation framework that simultaneously distills knowledge from
297"
CONCLUSION,0.7212121212121212,"both the supervised/CL pre-trained teacher and MIM pre-trained teacher to enhance the diversity and
298"
CONCLUSION,0.7232323232323232,"discrimination of the student. The framework addresses the limitations of single-teacher distillation,
299"
CONCLUSION,0.7252525252525253,"where increasing diversity through the use of asymmetric designs may harm discrimination. Specifi-
300"
CONCLUSION,0.7272727272727273,"cally, Hybrid Distill carefully designs the distilling target and location, i.e., distilling relations from
301"
CONCLUSION,0.7292929292929293,"MIM in layers where attention collapse tends to occur and distilling features from supervised/CL
302"
CONCLUSION,0.7313131313131314,"in the last layer to preserve discrimination. A progressive redundant token masking strategy is also
303"
CONCLUSION,0.7333333333333333,"proposed for reducing the distilling costs. Experiments prove that Hybrid Distill can acquire better
304"
CONCLUSION,0.7353535353535353,"properties and achieve promising results on various downstream. We hope our research would shed
305"
CONCLUSION,0.7373737373737373,"light on a new direction for applying existing large-scale pre-trained models.
306"
REFERENCES,0.7393939393939394,"References
307"
REFERENCES,0.7414141414141414,"[1] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEit: BERT pre-training of image
308"
REFERENCES,0.7434343434343434,"transformers. In International Conference on Learning Representations, 2022.
309"
REFERENCES,0.7454545454545455,"[2] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski,
310"
REFERENCES,0.7474747474747475,"and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings
311"
REFERENCES,0.7494949494949495,"of the International Conference on Computer Vision (ICCV), 2021.
312"
REFERENCES,0.7515151515151515,"[3] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the
313"
REFERENCES,0.7535353535353535,"kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern
314"
REFERENCES,0.7555555555555555,"Recognition (CVPR), 2017.
315"
REFERENCES,0.7575757575757576,"[4] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework
316"
REFERENCES,0.7595959595959596,"for contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020.
317"
REFERENCES,0.7616161616161616,"[5] Xiaokang Chen, Mingyu Ding, Xiaodi Wang, Ying Xin, Shentong Mo, Yunhao Wang, Shumin
318"
REFERENCES,0.7636363636363637,"Han, Ping Luo, Gang Zeng, and Jingdong Wang. Context autoencoder for self-supervised
319"
REFERENCES,0.7656565656565657,"representation learning. arXiv preprint arXiv:2202.03026, 2022.
320"
REFERENCES,0.7676767676767676,"[6] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum
321"
REFERENCES,0.7696969696969697,"contrastive learning. arXiv preprint arXiv:2003.04297, 2020.
322"
REFERENCES,0.7717171717171717,"[7] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised
323"
REFERENCES,0.7737373737373737,"vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer
324"
REFERENCES,0.7757575757575758,"Vision, pages 9640–9649, 2021.
325"
REFERENCES,0.7777777777777778,"[8] Yabo Chen, Yuchen Liu, Dongsheng Jiang, Xiaopeng Zhang, Wenrui Dai, Hongkai Xiong, and
326"
REFERENCES,0.7797979797979798,"Qi Tian. Sdae: Self-distillated masked autoencoder. In ECCV, 2022.
327"
REFERENCES,0.7818181818181819,"[9] Yufeng Cui, Lichen Zhao, Feng Liang, Yangguang Li, and Jing Shao. Democratizing contrastive
328"
REFERENCES,0.7838383838383839,"language-image pre-training: A clip benchmark of data, model, and supervision, 2022.
329"
REFERENCES,0.7858585858585858,"[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
330"
REFERENCES,0.7878787878787878,"Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.
331"
REFERENCES,0.7898989898989899,"An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
332"
REFERENCES,0.7919191919191919,"arXiv:2010.11929, 2020.
333"
REFERENCES,0.793939393939394,"[11] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang,
334"
REFERENCES,0.795959595959596,"Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning
335"
REFERENCES,0.797979797979798,"at scale. arXiv preprint arXiv:2211.07636, 2022.
336"
REFERENCES,0.8,"[12] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H Richemond, Elena
337"
REFERENCES,0.802020202020202,"Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi
338"
REFERENCES,0.804040404040404,"Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. arXiv
339"
REFERENCES,0.806060606060606,"preprint arXiv:2006.07733, 2020.
340"
REFERENCES,0.8080808080808081,"[13] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked
341"
REFERENCES,0.8101010101010101,"autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on
342"
REFERENCES,0.8121212121212121,"Computer Vision and Pattern Recognition, pages 16000–16009, 2022.
343"
REFERENCES,0.8141414141414142,"[14] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
344"
REFERENCES,0.8161616161616162,"unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on
345"
REFERENCES,0.8181818181818182,"Computer Vision and Pattern Recognition (CVPR), pages 9729–9738, 2020.
346"
REFERENCES,0.8202020202020202,"[15] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In ICCV, pages
347"
REFERENCES,0.8222222222222222,"2961–2969, 2017.
348"
REFERENCES,0.8242424242424242,"[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
349"
REFERENCES,0.8262626262626263,"recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition
350"
REFERENCES,0.8282828282828283,"(CVPR), pages 770–778, 2016.
351"
REFERENCES,0.8303030303030303,"[17] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep
352"
REFERENCES,0.8323232323232324,"bidirectional transformers for language understanding. In NAACL-HLT, pages 4171–4186,
353"
REFERENCES,0.8343434343434344,"2019.
354"
REFERENCES,0.8363636363636363,"[18] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-
355"
REFERENCES,0.8383838383838383,"grained categorization. In Proceedings of the IEEE international conference on computer vision
356"
REFERENCES,0.8404040404040404,"workshops, pages 554–561, 2013.
357"
REFERENCES,0.8424242424242424,"[19] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
358"
REFERENCES,0.8444444444444444,"2009.
359"
REFERENCES,0.8464646464646465,"[20] Jin Li, Yaoming Wang, XIAOPENG ZHANG, Yabo Chen, Dongsheng Jiang, Wenrui Dai,
360"
REFERENCES,0.8484848484848485,"Chenglin Li, Hongkai Xiong, and Qi Tian. Progressively compressed auto-encoder for self-
361"
REFERENCES,0.8505050505050505,"supervised representation learning. In The Eleventh International Conference on Learning
362"
REFERENCES,0.8525252525252526,"Representations, 2023.
363"
REFERENCES,0.8545454545454545,"[21] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image
364"
REFERENCES,0.8565656565656565,"pre-training for unified vision-language understanding and generation. In ICML, 2022.
365"
REFERENCES,0.8585858585858586,"[22] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
366"
REFERENCES,0.8606060606060606,"Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Proceedings
367"
REFERENCES,0.8626262626262626,"of the European Conference on Computer Vision (ECCV), pages 1209–1218, 2014.
368"
REFERENCES,0.8646464646464647,"[23] Xingbin Liu, Jinghao Zhou, Tao Kong, Xianming Lin, and Rongrong Ji. Exploring target
369"
REFERENCES,0.8666666666666667,"representations for masked autoencoders. arXiv preprint arXiv:2209.03917, 2022.
370"
REFERENCES,0.8686868686868687,"[24] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining
371"
REFERENCES,0.8707070707070707,"Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings
372"
REFERENCES,0.8727272727272727,"of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021.
373"
REFERENCES,0.8747474747474747,"[25] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv
374"
REFERENCES,0.8767676767676768,"preprint arXiv:1608.03983, 2016.
375"
REFERENCES,0.8787878787878788,"[26] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
376"
REFERENCES,0.8808080808080808,"arXiv:1711.05101, 2017.
377"
REFERENCES,0.8828282828282829,"[27] Namuk Park, Wonjae Kim, Byeongho Heo, Taekyung Kim, and Sangdoo Yun. What do
378"
REFERENCES,0.8848484848484849,"self-supervised vision transformers learn? arXiv preprint arXiv:2305.00729, 2023.
379"
REFERENCES,0.8868686868686869,"[28] Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho. Relational knowledge distillation. In
380"
REFERENCES,0.8888888888888888,"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
381"
REFERENCES,0.8909090909090909,"3967–3976, 2019.
382"
REFERENCES,0.8929292929292929,"[29] Zhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and Furu Wei. BEiT v2: Masked image
383"
REFERENCES,0.8949494949494949,"modeling with vector-quantized visual tokenizers. 2022.
384"
REFERENCES,0.896969696969697,"[30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
385"
REFERENCES,0.898989898989899,"Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
386"
REFERENCES,0.901010101010101,"models from natural language supervision. In International Conference on Machine Learning
387"
REFERENCES,0.9030303030303031,"(ICML), 2021.
388"
REFERENCES,0.9050505050505051,"[31] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and
389"
REFERENCES,0.907070707070707,"Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014.
390"
REFERENCES,0.9090909090909091,"[32] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
391"
REFERENCES,0.9111111111111111,"Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual
392"
REFERENCES,0.9131313131313131,"recognition challenge. International journal of computer vision (IJCV), 115(3):211–252, 2015.
393"
REFERENCES,0.9151515151515152,"[33] Alexander Strehl and Joydeep Ghosh. Cluster ensembles—a knowledge reuse framework for
394"
REFERENCES,0.9171717171717172,"combining multiple partitions. Journal of machine learning research, 3(Dec):583–617, 2002.
395"
REFERENCES,0.9191919191919192,"[34] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable
396"
REFERENCES,0.9212121212121213,"effectiveness of data in deep learning era. In Proceedings of the IEEE International Conference
397"
REFERENCES,0.9232323232323232,"on Computer Vision (ICCV), 2017.
398"
REFERENCES,0.9252525252525252,"[35] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive representation distillation. arXiv
399"
REFERENCES,0.9272727272727272,"preprint arXiv:1910.10699, 2019.
400"
REFERENCES,0.9292929292929293,"[36] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and
401"
REFERENCES,0.9313131313131313,"Herve Jegou. Training data-efficient image transformers & distillation through attention. In
402"
REFERENCES,0.9333333333333333,"International Conference on Machine Learning (ICML), volume 139, pages 10347–10357, July
403"
REFERENCES,0.9353535353535354,"2021.
404"
REFERENCES,0.9373737373737374,"[37] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig
405"
REFERENCES,0.9393939393939394,"Adam, Pietro Perona, and Serge Belongie. The inaturalist species classification and detection
406"
REFERENCES,0.9414141414141414,"dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition,
407"
REFERENCES,0.9434343434343434,"pages 8769–8778, 2018.
408"
REFERENCES,0.9454545454545454,"[38] Shaoru Wang, Jin Gao, Zeming Li, Xiaoqin Zhang, and Weiming Hu. A closer look at self-
409"
REFERENCES,0.9474747474747475,"supervised lightweight vision transformers, 2023.
410"
REFERENCES,0.9494949494949495,"[39] Wenguan Wang, Tianfei Zhou, Fisher Yu, Jifeng Dai, Ender Konukoglu, and Luc Van Gool. Ex-
411"
REFERENCES,0.9515151515151515,"ploring cross-image pixel contrast for semantic segmentation. arXiv preprint arXiv:2101.11939,
412"
REFERENCES,0.9535353535353536,"2021.
413"
REFERENCES,0.9555555555555556,"[40] Longhui Wei, Lingxi Xie, Wengang Zhou, Houqiang Li, and Qi Tian. Mvp: Multimodality-
414"
REFERENCES,0.9575757575757575,"guided visual pre-training. In Computer Vision–ECCV 2022: 17th European Conference, Tel
415"
REFERENCES,0.9595959595959596,"Aviv, Israel, October 23–27, 2022, Proceedings, Part XXX, pages 337–353. Springer, 2022.
416"
REFERENCES,0.9616161616161616,"[41] Yixuan Wei, Han Hu, Zhenda Xie, Zheng Zhang, Yue Cao, Jianmin Bao, Dong Chen, and
417"
REFERENCES,0.9636363636363636,"Baining Guo. Contrastive learning rivals masked image modeling in fine-tuning via feature
418"
REFERENCES,0.9656565656565657,"distillation. Tech Report, 2022.
419"
REFERENCES,0.9676767676767677,"[42] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing
420"
REFERENCES,0.9696969696969697,"for scene understanding. In Proceedings of the European Conference on Computer Vision
421"
REFERENCES,0.9717171717171718,"(ECCV), pages 418–434, 2018.
422"
REFERENCES,0.9737373737373738,"[43] Zhenda Xie, Zigang Geng, Jingcheng Hu, Zheng Zhang, Han Hu, and Yue Cao. Revealing the
423"
REFERENCES,0.9757575757575757,"dark secrets of masked image modeling. arXiv preprint arXiv:2205.13543, 2022.
424"
REFERENCES,0.9777777777777777,"[44] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and
425"
REFERENCES,0.9797979797979798,"Han Hu. Simmim: A simple framework for masked image modeling. In Proceedings of the
426"
REFERENCES,0.9818181818181818,"IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9653–9663, 2022.
427"
REFERENCES,0.9838383838383838,"[45] Hongwei Xue, Peng Gao, Hongyang Li, Yu Qiao, Hao Sun, Houqiang Li, and Jiebo Luo. Stare at
428"
REFERENCES,0.9858585858585859,"what you see: Masked image modeling without reconstruction. arXiv preprint arXiv:2211.08887,
429"
REFERENCES,0.9878787878787879,"2022.
430"
REFERENCES,0.98989898989899,"[46] Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. Ernie:
431"
REFERENCES,0.9919191919191919,"Enhanced language representation with informative entities. In ACL, pages 1441–1451, 2019.
432"
REFERENCES,0.9939393939393939,"[47] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio
433"
REFERENCES,0.9959595959595959,"Torralba. Semantic understanding of scenes through the ade20k dataset. International Journal
434"
REFERENCES,0.997979797979798,"on Computer Vision (IJCV), 127:302–321, 2019.
435"
