Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0034482758620689655,"Saliency methods seek to provide human-interpretable explanations for the output
of machine learning model on a given input. A plethora of saliency methods ex-
ist, as well as an extensive literature on their justiﬁcations/criticisms/evaluations.
This paper focuses on heat maps based saliency methods that often provide expla-
nations that look best to humans. It tries to introduce methods and evaluations for
masked-based saliency methods that are intrinsic — use just the training dataset
and the trained net, and do not use separately trained nets, distractor distributions,
human evaluations or annotations. Since a mask can be seen as a “certiﬁcate”
justifying the net’s answer, we introduce notions of completeness and soundness
(the latter being the new contribution) motivated by logical proof systems. These
notions allow a new evaluation of saliency methods, that experimentally provides
a novel and stronger justiﬁcation for several heuristic tricks in the ﬁeld (T.V. reg-
ularization, upscaling)."
INTRODUCTION,0.006896551724137931,"1
INTRODUCTION"
INTRODUCTION,0.010344827586206896,"Why did the deep net give a certain answer on a particular input, and can we trust the answer?
Saliency methods try to provide explanations, and are thus of great interest from viewpoint of human
explainability, fairness, robustness, etc. This paper restricts attention to methods that return an
importance score for each coordinate of the input —usually visualized as a heat map— that captures
its importance to the ﬁnal decision.1 We refer the reader to Samek et al. (2019) for an extensive
survey and Section 2 for a short account of such methods and controversies."
INTRODUCTION,0.013793103448275862,"There are two important components in the research on saliency: saliency methods that produce such
heat maps for explanations, and saliency evaluation metrics that aim to test and compare saliency
methods. Numerous saliency methods have been proposed to learn such heat maps. Some methods
learn maps through “credit attribution” to individual input coordinates by using methods reminiscent
of backpropagation (Binder et al., 2016; Selvaraju et al., 2019), while some are derived using careful
axiomatization of credit assignment using the idea of Shapley values from cooperative game theory
(Lundberg & Lee, 2017b; Yeh et al., 2020). Some recent methods train another deep net to produce
heat maps (Dabkowski & Gal, 2017; Phang et al., 2020). Given the proliferation of saliency methods,
an ecosystem of evaluation metrics has emerged to evaluate the quality of explanations produced by
saliency methods, either through human evaluations (Adebayo et al., 2018), comparison to certain
ground truth explanations (Zhang et al., 2018) or other evaluations that do not require any external
annotation or supervision (Dabkowski & Gal, 2017) Petsiuk et al. (2018)."
INTRODUCTION,0.017241379310344827,"In this paper we restrict attention to a class of evaluations we refer to as intrinsic, that aim to evaluate
saliency maps based on whether they are good explanations for the model prediction. These only
involve computations using the provided heat map and the net, and do not involve extrinsic factors
such as human judgements or retraining. A popular idea in such evaluations (see Section 3 for
references) is to create a new composite input —or sequence of such inputs— using the heat map
and the original input, and to evaluate the original net on this composite input. For example, if
M is a binary vector with 1’s in the k coordinates with the highest values in the heat map, then
x ⊙M (with ⊙denoting coordinate-wise multiplication) can be viewed as a composite input2 (aka"
INTRODUCTION,0.020689655172413793,"1Heat maps sufﬁce for recognition/classiﬁcation tasks; other tasks may require more complex explanations.
2When x is an image the zeroes in this masked input are often replaced with gray values."
INTRODUCTION,0.02413793103448276,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.027586206896551724,"Figure 1: Masked CIFAR-10 images generated by our procedure with λT V = 0.01 shows the
artifacts exist for masks generated for incorrect labels. More examples can be ﬁnd in Figure 7 in
Appendix. The base classiﬁer outputs the correct label on the original image (ship and bird resp.)
with probability at least 0.99, and assigns probability at most 10−5 for the incorrect label (cat and
frog resp.). With the generated masks, the AUC metric for the correct label remains high (around
0.94 and 0.90), which corresponds to completeness, but AUC metric for the incorrect label rises
tremendously (around 0.18 for cat mask, 0.71 for frog mask.) This suggests violation of soundness."
INTRODUCTION,0.03103448275862069,"“masked input”) that can be fed into the original net. By varying k one obtains a sequence of masks
M. Although there is an issue of distribution shift due to the net never having trained on composite
inputs, in practice, especially for image data, trained nets work ﬁne on masked inputs. (This is also
exploited in Shapley value based saliency methods.)"
INTRODUCTION,0.034482758620689655,"Inspired by logical reasoning, we identity a key component missing in existing intrinsic evaluations:
soundness of the saliency method. In simple terms, checking soundness for mask-based explanations
entails verifying that it is impossible for the method to produce well-performing (in the composite-
input evaluations) heat maps corresponding to the incorrect labels, i.e. labels not predicted as the
most likely by the model. By contrast current masking evaluations focus on a property akin to
completeness3 in logic: the heat map “justiﬁes” the correct label, i.e. the label judged to be most
likely by the model. We argue that soundness can be a useful criterion in addition to completeness."
INTRODUCTION,0.03793103448275862,"The concept of soundness is motivated by observing that the recent mask-based saliency methods
are inherently different from the earlier gradient-based methods. They produce a heatmap that is
interpreted as a masked image that, when fed into the net must make it output the same label that
received the top logit value on the full image. Our paper observes that it is unclear at a logical
level why this should be considered an “explanation”, since to be convinced one must verify that
a different mask could not induce the net to output a label corresponding to other logits. Figure 1
shows that in fact this issue is real; masks do exist that can “justify” the wrong labels. We introduce
soundness as a quantitative measure for this issue (deﬁnitions in Section 3.1), and propose that
evaluations of mask-based saliency methods should account for soundness. Many past works allude
to difﬁculty of the soundness concept, mentioning that computations to ﬁnd the “best” mask end
up ﬁnding artifacts that can even justify labels that are obviously incorrect. To avoid uncovering
artifacts, the methods to ﬁnd masks/heat maps use a suitable regularization (usually TV). However,
Figure 1 illustrates that TV regularization is not a full solution to removing artifacts."
INTRODUCTION,0.041379310344827586,"Another reason for incorporating soundness with completeness in metrics is to implicitly encourage
saliency methods to also produce saliency maps for incorrect labels. When using the classiﬁer in
the wild—as opposed to on a dataset with centered images— we might encounter multiple salient
objects in the image, and the net may not have high conﬁdence in a single label. It would then
be reasonable to have saliency methods ﬁnd evidence for all the labels which the net outputs with
reasonable conﬁdence. We ﬁnd in Figure 2 that our saliency method that produces masks for all la-
bels can produce different and meaningful masks for different labels when there are multiple objects
present in the image4, compared to prior state-of-the-art methods, that output a single mask for all
labels that contains both objects (more on this in Appendix B)."
INTRODUCTION,0.04482758620689655,"3In the literature devoted to saliency methods based upon backpropagation-like procedure, such as GRA-
DIENT ⊙INPUT or LRP, the term completeness is used in a slightly different sense: the sum of the heat map
values has to be equal to the logit value for the label.
4In this paper, multiple objects in one image is not formally considered, as we assume single object clas-
siﬁcation is the prior knowledge. Therefore, either zebra or elephant in Figure 2 is regarded as an incorrect
label. A quick extension to the multi-object case is to deﬁne correct labels as the labels whose model proba-
bility exceeds a certain threshold. Even without the extended deﬁnition of correct labels, Deﬁnition 3.2 with
suitable α, β already allows explanations to exist for both elephant and zebra as long as the model probability
for elephant and zebra are high enough."
INTRODUCTION,0.04827586206896552,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.05172413793103448,"Figure 2: Images containing both elephant(s) and zebra(s), and the corresponding masked ones
generated by our method and the best-performing CA model in Phang et al. (2020). The masks
by Phang et al. (2020) are identical for different labels, and contains both elephant and zebra. In
contrast, our method outputs descent masks for elephant and zebra accordingly."
INTRODUCTION,0.05517241379310345,"Main contributions:
In this paper we make a case for pushing saliency methods and metrics to
incorporate soundness in addition to completeness. We propose metrics and a method and exper-
imentally evaluate them in comparison to previous methods and metrics, leading to some novel
ﬁndings. Our main contributions are as follows:
• A new single number metric named consistency score that evaluates the completeness and sound-
ness simultaneously. It measures the probability that the saliency map for the model prediction
has the best quality compared to all other labels. See Section 3.2 for a formal deﬁnition.
• A saliency method in Section 4 that bears similarity to prior works that learns masks through
optimization, but with subtle differences. One of the differences is based on the logic of soundness:
we learn a mask by trying to maximize the probability for the given label, rather than matching
the probability, and learn the best mask for all labels this way. We ﬁnd in experiments (Section 6)
that our method not only performs comparably to prior masked-based methods on earlier saliency
metrics, but also does better than those methods on the proposed consistency score.
• Previous works proposed “tricks” like TV penalty and upscaling of masks (Fong & Vedaldi, 2017;
Dabkowski & Gal, 2017) that are motivated by their ability to avoid “artifacts” and make the
saliency maps visually look better. These justiﬁcations, however, are not in the spirit of an intrinsic
evaluation of saliency maps. In this paper, we provide a novel intrinsic justiﬁcation for these
choices of TV penalty and upscaling, by showing that they improve the saliency maps by making
them more sound, as empirically demonstrated in Section 6. We complement this ﬁnding with
a theoretical result in Section 5 that proves that TV regularization can help with soundness in a
simple linear classiﬁcation setting.
Paper outline:
Section 2 discusses prior works on saliency methods and evaluations. Section 3
delves further into the concept of completeness and soundness in the context of saliency evaluation,
and describes our proposed metric of consistency score. Section 4 describes our saliency method and
how it differs from prior approaches. Finally Section 5 describes various supportive experiments."
PRIOR APPROACHES,0.05862068965517241,"2
PRIOR APPROACHES"
PRIOR APPROACHES,0.06206896551724138,"We delegate a more thorough description of prior work to Appendix D, but mention some common
saliency and saliency evaluation methods here. Saliency methods aim to explain a model’s decision
about an input. Saliency evaluation methods aim to evaluate the goodness of a saliency method."
PRIOR APPROACHES,0.06551724137931035,"Saliency methods include backpropagation based approaches such as Gradient ⊙Input (Shrikumar
et al., 2017), LRP (Binder et al., 2016), GradCAM (Selvaraju et al., 2019), Smooth-Grad (Smilkov
et al., 2017). Another line of work is masking methods which include techniques based on averaging
over randomly sampled masks (Petsiuk et al., 2018), optimizing over meaningful mask perturbations
(Fong & Vedaldi, 2017), and real time image saliency using a masking network (Dabkowski & Gal,
2017). Pixels that have been removed from the image by the mask may be replaced by greying out,
by Gaussian blurring as in (Fong & Vedaldi, 2017), or with inﬁllers such as CA-GAN (Yu et al.,"
PRIOR APPROACHES,0.06896551724137931,Under review as a conference paper at ICLR 2022
PRIOR APPROACHES,0.07241379310344828,"2018) used in (Chang et al., 2018; Phang et al., 2020), or DFNet (Hong et al., 2019). De Cao et al.
(2020) ﬁnd masks using differentiable masking. Boolean logic is another approach for saliency
methods (Ignatiev et al., 2019b;a; Macdonald et al., 2019; Mu & Andreas, 2020; Zhou et al., 2018)."
PRIOR APPROACHES,0.07586206896551724,"Arguments about saliency. As saliency methods have arisen, discussion about them has occurred
e.g. in (Seo et al., 2018; Fryer et al., 2021; Gu et al., 2018; Sundararajan & Najmi, 2020). Some
works reveal situations where Shapley axioms work against feature selection or where Shapley val-
ues may be calculated in conﬂicting ways (Fryer et al., 2021; Sundararajan & Najmi, 2020). Others
question why noise adding saliency methods work (Seo et al., 2018), with some pointing out that
some explanations are not class discriminative (Gu et al., 2018)."
PRIOR APPROACHES,0.07931034482758621,"Saliency evaluation methods. Extrinsic evaluation metrics include the WSOL metric, and Pointing
Game metric proposed by Zhang et al. (2018) and ROAR (Hooker et al., 2019). Other more intrinsic
methods include early saliency evaluation techniques like MorF and LerF (Samek et al., 2016),
Insertion and Deletion game proposed by Petsiuk et al. (2018), which involve either inserting
pixels in order of most importance or deleting pixels in order of most importance. BAM (Yang
& Kim, 2019) creates saliency maps by pasting object pixels from MSCOCO (Lin et al., 2014)
The Saliency Metric proposed by Dabkowski & Gal (2017) thresholds saliency values above some
α chosen on a holdout set, ﬁnds the smallest bounding box containing these pixels, upsamples
and measures the ratio of bounding box area to model accuracy on the cropped image, s(a, p) =
log(max(a, 0.05)) −log(p) where a is the area of the bounding box and p is the class probability of
the upsampled image."
PRIOR APPROACHES,0.08275862068965517,"Controversies.
Brunke et al. (2020) show that perturbation methods are sensitive to baseline and
Petsiuk et al. (2018) point out that human centric explanations (based on bounding boxes) may not
reveal why the model made a certain decision. Our notion of intrinsic saliency method aligns with
the latter idea, and we introduce a new criteria for evaluating saliency maps: soundness, which
captures the notion that maps should not produce explanations for low probability labels."
PRIOR APPROACHES,0.08620689655172414,"3
MASKING EXPLANATIONS AND COMPLETENESS/SOUNDNESS"
PRIOR APPROACHES,0.0896551724137931,"A running theme in saliency methods/evaluations is that the salient pixels should be sufﬁcient to
convince us about the model’s output, regardless of contents of the other pixels. For instance, grey-
ing out (or setting them to average pixel value) non-salient pixels should have very little effect on the
output. This idea appears in many saliency methods (Dabkowski & Gal, 2017; Phang et al., 2020)
(and evaluation metrics (Petsiuk et al., 2018)), including Shapley values and mask-based explana-
tions. This motives the following deﬁnition of a masked-based explanation."
PRIOR APPROACHES,0.09310344827586207,"Deﬁnition 3.1 (Masking Explanation). A masking explanation for input x is a distribution ∆over
subsets S of input coordinates (“salient sets of x”) as well as an input modiﬁcation process Γ that
generates a distribution of modiﬁed inputs ˜x ∼Γ(x, S) under the constraint that ˜x matches x on
every coordinate in set S. We use Γ(x, ∆) to denote the distribution of ˜x produced by Γ when the
input is x and the set S is sampled from ∆.
A salient set S could be the direct output of any saliency method, or the output heat map passed
through a potentially randomized discretization (e.g., the insertion game in Section 3.3), leading to
a distribution ∆over sets S. Simple examples of the input modiﬁcation process Γ that stay in the
intrinsic framework are: greying out the pixels outside S, replacing them by pixels from a Gaussian
blurring of x (Fong & Vedaldi, 2017). Another example of Γ —albeit non-intrinsic and hence not
used in this paper— uses a conditional image generative model to produce new pixel values in S
conditional on pixels in S being consistent with x (Agarwal & Nguyen, 2020; Chang et al., 2018).
Deﬁnition 3.1 can also be changed to allow Γ to change the values of pixels in S. In our proposed
method in Section 4, we consider another distribution Γ, which replaces pixels outside of S with
pixels from a random image from the training set5. This amounts to grafting salient pixels of x on
top of a random image, reminiscent of BAM evaluations (Yang & Kim, 2019) for saliency methods."
COMPLETENESS AND SOUNDNESS,0.09655172413793103,"3.1
COMPLETENESS AND SOUNDNESS
We now describe completeness and soundness for saliency evaluations. In a multiclass classiﬁcation
setting, denote by f(x, a) the probability for label a returned by the model on input x. The prediction
of the model on input x is ˆy(x) := arg maxa∈C f(x, a), where C denotes all classes. For p ∈[0, 1],"
COMPLETENESS AND SOUNDNESS,0.1,5We think other random image distributions should work too.
COMPLETENESS AND SOUNDNESS,0.10344827586206896,Under review as a conference paper at ICLR 2022
COMPLETENESS AND SOUNDNESS,0.10689655172413794,"a masking explanation (∆, Γ) for input x is said to p-validate the label a ∈C if"
COMPLETENESS AND SOUNDNESS,0.1103448275862069,"E˜x∼Γ(x,∆)[f(˜x, a)] ≥p.
(1)"
COMPLETENESS AND SOUNDNESS,0.11379310344827587,"We say that a masking explanation x p-validates the model prediction if it p-validates ˆy(x).
Deﬁnition 3.2 (Completeness and Soundness). Fix an input modiﬁcation process Γ. A saliency
method given f, x and any label a ∈C produces a distribution ∆(x, a) over salient sets. For
α, β ≤1, the method is α-complete on f, x, a if the masking explanation (∆(x, a), Γ) p-validates
the label a for x with p = α·f(x, a), and the method is β-sound on f, x, a if the masking explanation
(∆(x, a), Γ) cannot p-validate the label a with p > 1"
COMPLETENESS AND SOUNDNESS,0.11724137931034483,"β max{f(x, a), ϵ}6.
Intuitively, when α, β are close to 1, α-completeness means that if the model outputs a high prob-
ability for label a, then the probability for label a after seeing only the coordinates in the salient
sets is also high; β-soundness means that if the model outputs a low probability for label a, then
the probability for label a after seeing only the coordinates in the salient sets is also low. Thus a
desirable saliency method should have α-completeness for labels predicted by the model and β-
completeness for other labels. This captures the idea that we want the method to be able to validate
the model prediction ˆy(x) but not other labels. It is worth noting that the saliency method that de-
clares all pixels to be salient is α-complete and β-sound with α = β = 1, but its size is too large to
provide any useful information. Thus one should ask the saliency method to search over the salient
sets of smaller sizes. But artifacts for incorrect labels may occur at small size (Figure 1), which
hurts soundness. For this reason we put explicit or implicit constraints on the salient sets (e.g., TV
regularization) and ask saliency methods to be α-complete and β-sound with α, β as close to 1 as
possible. Ideally we should adjust the constraints to achieve a good tradeoff between completeness
and soundness. In our experiments, we ﬁnd that standard “tricks” like TV regularization and mask
upscaling provide such good tradeoffs, as observed in Table 3 and Figure 3."
COMPLETENESS AND SOUNDNESS,0.1206896551724138,"Important note.
In the logic setting, from where we borrow the concept of soundness, one has to
search among all possible proofs to ensure that there is indeed no valid proof for any false propo-
sition. Similarly, our soundness metric is non-vacuous only if the saliency method “makes its best
efforts” to ﬁnd masking explanations for every label. Otherwise the saliency method could for in-
stance ignore the value of a return the mask for ˆy which can p-validate ˆy(x) with a high value of p
and thus not validate any other label, leading to a false notion of soundness. If the saliency method
behaves like this on the incorrect label (i.e., is not working at producing an explanation) then as a
backup the evaluation can use some default method to produce saliency explanations for those la-
bels. Our method for mask-based explanations, described later, is simple and not tied to any speciﬁc
philosophy or a-priori deﬁnition of saliency. It can produce explanations for all labels."
CONSISTENCY SCORE FOR EVALUATING COMPLETENESS AND SOUNDNESS,0.12413793103448276,"3.2
CONSISTENCY SCORE FOR EVALUATING COMPLETENESS AND SOUNDNESS"
CONSISTENCY SCORE FOR EVALUATING COMPLETENESS AND SOUNDNESS,0.12758620689655173,"Inspired by the notions of completeness and soundness, we propose to use a single number metric
that evaluates the completeness and soundness simultaneously over a data distribution.
Deﬁnition 3.3 (Consistency Score). Let X be the distribution of input x and Γ be an input modiﬁ-
cation process. For a saliency method that produces salient sets ∆(x, a) on input x and label a, we
deﬁne the “saliency score” for (x, a) as g∆(x, a) = E˜x∼Γ(x,∆(x,a))[f(˜x, a)], i.e. average probability
the model assigns to a labels for modiﬁed versions of x using the label-speciﬁc salient sets ∆(x, a).
The consistency score for the saliency method is deﬁned as the probability that the saliency score is
consistent with the model prediction, i.e.,"
CONSISTENCY SCORE FOR EVALUATING COMPLETENESS AND SOUNDNESS,0.1310344827586207,"Pr
x∼X"
CONSISTENCY SCORE FOR EVALUATING COMPLETENESS AND SOUNDNESS,0.13448275862068965,"
arg max
a
g∆(x, a) = ˆy(x)

.
(2)"
CONSISTENCY SCORE FOR EVALUATING COMPLETENESS AND SOUNDNESS,0.13793103448275862,"It is easy to see that if a saliency method is 1-complete and 1-sound on all inputs, then the consis-
tency score is 1. The following lemma shows that even α-completeness and β-soundness can imply
consistency if there is a gap between the probabilities for the largest and second largest labels.
Lemma 3.4. For an input x, if a saliency method is α-complete on the model prediction ˆy(x) and
β-sound on all other labels, and if the ratio between the output probabilities for the largest label
ˆy(x) and second largest labels
f(x,ˆy(x))
max{maxa̸=ˆy(x){f(x,a)},ϵ} is larger than
1
αβ , then the saliency method
is consistent with the model on input x."
CONSISTENCY SCORE FOR EVALUATING COMPLETENESS AND SOUNDNESS,0.1413793103448276,6The constant ϵ is used to avoid blowing up due to tiny probabilities.
CONSISTENCY SCORE FOR EVALUATING COMPLETENESS AND SOUNDNESS,0.14482758620689656,Under review as a conference paper at ICLR 2022
CONSISTENCY SCORE FOR EVALUATING COMPLETENESS AND SOUNDNESS,0.1482758620689655,"Proof. By α-completeness, the saliency method p-validates ˆy(x) with p = α · f(x, ˆy(x)). Since
f(x,ˆy(x))
max{maxa̸=ˆy(x){f(x,a)},ϵ} >
1
αβ , p has the lower bound p >
1
β max{maxa̸=ˆy(x){f(x, a)}, ϵ} for
all a ̸= ˆy(x). Then β-soundness implies that the saliency method cannot p-validate a. Therefore
E˜x∼Γ(x,∆(x,a))[f(˜x, a)] is maximized when a = ˆy(x), and hence consistency."
CONNECTION TO POPULAR AREA-UNDER-THE-CURVE EVALUATIONS OF HEAT MAPS,0.15172413793103448,"3.3
CONNECTION TO POPULAR AREA-UNDER-THE-CURVE EVALUATIONS OF HEAT MAPS"
CONNECTION TO POPULAR AREA-UNDER-THE-CURVE EVALUATIONS OF HEAT MAPS,0.15517241379310345,"As mentioned, our approach requires the method to output salient sets whereas existing methods of-
ten return a heatmap of saliency values. However, popular Area-Under-the-Curve (AUC) evaluation
metrics (Petsiuk et al., 2018) of saliency methods can be reinterpreted in terms of our completeness
and soundness. An example is the insertion game:"
CONNECTION TO POPULAR AREA-UNDER-THE-CURVE EVALUATIONS OF HEAT MAPS,0.15862068965517243,"AUC of Insertion game: For s = 1 to d take the top s saliency values, and plot the probability
given by model to label a on the input where the top s pixels of x are retained and remaining pixels
are assigned values from the input modiﬁcation process Γ. Return AUC.
Lemma 3.5. Given a saliency heatmap with AUC ρ for label a, we can convert it into a masking
explanation that ρ-validates the label a."
CONNECTION TO POPULAR AREA-UNDER-THE-CURVE EVALUATIONS OF HEAT MAPS,0.16206896551724137,"Proof. Given a heatmap, produce the salient set S by picking s uniformly at random from 1 to d
and letting S be coordinates corresponding to top s saliency values. Let ∆be the distribution of the
produced salient sets. Then E˜x∼Γ(x,∆)[f(˜x, a)] = ρ by deﬁnition."
PROCEDURES TO FIND MASKING EXPLANATIONS,0.16551724137931034,"4
PROCEDURES TO FIND MASKING EXPLANATIONS"
PROCEDURES TO FIND MASKING EXPLANATIONS,0.16896551724137931,"Based on the deﬁnition 3.1 of masking explanations, we propose a very simple method to ﬁnd
saliency masks with good empirical completeness and soundness scores. We introduce our methods
in this section concisely. The detailed intuitions and implementations are provided in Appendix A."
PROCEDURES TO FIND MASKING EXPLANATIONS,0.1724137931034483,"Our method bears similarity to prior work on learning mask based saliency maps, but with subtle
differences. The key difference from Dabkowski & Gal (2017); Phang et al. (2020) is that we do
not use a neural network to learn the mask and the difference from Fong & Vedaldi (2017); Agarwal
& Nguyen (2020); Chang et al. (2018) is that we use a different input modiﬁcation process Γ (see
Deﬁnition 3.1) to inﬁll other pixels during training: given x and S, generate a hybrid inputs whereby
the pixels in the set S match with x and the pixels outside S are set to those of a random image
drawn from the training set X. Replacing with random image pixels instead of gray pixels will
make it harder to ﬁnd a mask. Despite this, we can still learn a mask that predicts the correct label
with high conﬁdence in practice. Furthermore, it achieves superior performance over replacing with
gray pixels, which is likely because the added hardness of task helps increase the robustness of the
output.
The ﬁnal crucial difference is that our method can ﬁnd a mask for every input-label pair
(x, a) and does so by trying to maximize the probability assigned to label a on modiﬁed images, as
opposed to trying to match the model probability for a."
PROCEDURES TO FIND MASKING EXPLANATIONS,0.17586206896551723,"As is standard, we relax the domain of masks M from binary {0, 1}hw to continuous [0, 1]hw, and
optimize M for input x and label a based on the following natural objective7"
PROCEDURES TO FIND MASKING EXPLANATIONS,0.1793103448275862,"L(M; (x, a)) = E¯x∼X [−log(f(M ⊙x + (1 −M) ⊙¯x, a))] + λ1∥M∥1,
(3)"
PROCEDURES TO FIND MASKING EXPLANATIONS,0.18275862068965518,"where the part of x on M is superimposed onto a distractor ¯x ∼X as ˜x = M ⊙x + (1 −M) ⊙¯x,
and the ℓ1 norm penalty on M helps to reduce the size of masks."
PROCEDURES TO FIND MASKING EXPLANATIONS,0.18620689655172415,"We further employ Total-Variation (TV) penalty (Fong & Vedaldi, 2017) and upscaling of the mask
from a lower resolution one (Petsiuk et al., 2018) by learning a low-resolution mask at scale s,
M ∈[0, 1]hw/s2, to minimize the following"
PROCEDURES TO FIND MASKING EXPLANATIONS,0.1896551724137931,"L(M; (x, a)) = E¯x∼X

−log(f(M ×s ⊙x + (1 −M ×s) ⊙¯x, a))

+ λT V TV (M ×s) + λ1∥M ×s∥1
(4)"
PROCEDURES TO FIND MASKING EXPLANATIONS,0.19310344827586207,"where M ×s ∈Rhw is obtained by upscaling M by a factor of s ∈{1, 4} via bilinear interpolation."
PROCEDURES TO FIND MASKING EXPLANATIONS,0.19655172413793104,"While the motivation cited for these two “tricks” is to avoid artifacts, it is not clear whether artifacts
are a bad thing, since they might be relevant to the net’s decision-making. Indeed, in our experi-
ments, we show that while TV penalty or upscaling does produce better looking masks, they lead"
A STANDARD WAY TO MAXIMIZE PROBABILITY IS TO MINIMIZE THE NEGATIVE LOG PROBABILITY,0.2,7A standard way to maximize probability is to minimize the negative log probability
A STANDARD WAY TO MAXIMIZE PROBABILITY IS TO MINIMIZE THE NEGATIVE LOG PROBABILITY,0.20344827586206896,Under review as a conference paper at ICLR 2022
A STANDARD WAY TO MAXIMIZE PROBABILITY IS TO MINIMIZE THE NEGATIVE LOG PROBABILITY,0.20689655172413793,"0.0
0.2
0.4
0.6
0.8
1.0
fraction of pixels used 0.0 0.2 0.4 0.6 0.8 1.0"
A STANDARD WAY TO MAXIMIZE PROBABILITY IS TO MINIMIZE THE NEGATIVE LOG PROBABILITY,0.2103448275862069,model output probability
A STANDARD WAY TO MAXIMIZE PROBABILITY IS TO MINIMIZE THE NEGATIVE LOG PROBABILITY,0.21379310344827587,Best label TV
A STANDARD WAY TO MAXIMIZE PROBABILITY IS TO MINIMIZE THE NEGATIVE LOG PROBABILITY,0.21724137931034482,"0.0
0.001
0.01
0.1"
A STANDARD WAY TO MAXIMIZE PROBABILITY IS TO MINIMIZE THE NEGATIVE LOG PROBABILITY,0.2206896551724138,"0.0
0.2
0.4
0.6
0.8
1.0
fraction of pixels used 0.0 0.2 0.4 0.6 0.8 1.0"
A STANDARD WAY TO MAXIMIZE PROBABILITY IS TO MINIMIZE THE NEGATIVE LOG PROBABILITY,0.22413793103448276,model output probability
A STANDARD WAY TO MAXIMIZE PROBABILITY IS TO MINIMIZE THE NEGATIVE LOG PROBABILITY,0.22758620689655173,Second best label TV
A STANDARD WAY TO MAXIMIZE PROBABILITY IS TO MINIMIZE THE NEGATIVE LOG PROBABILITY,0.23103448275862068,"0.0
0.001
0.01
0.1"
A STANDARD WAY TO MAXIMIZE PROBABILITY IS TO MINIMIZE THE NEGATIVE LOG PROBABILITY,0.23448275862068965,"Figure 3: Plot of model output probability as more pixels from the original image are retained using
learned masks. The remaining pixels are replaced with gray. Different curves correspond to different
values of TV regularization (λT V ). Larger area-under-curve (AUC) for the left ﬁgure (best label)
suggests good completeness, while lower AUC for the right ﬁgure suggests good soundness. Plots
suggest that adding TV signiﬁcantly helps with soundness, while only slightly hurting completeness."
A STANDARD WAY TO MAXIMIZE PROBABILITY IS TO MINIMIZE THE NEGATIVE LOG PROBABILITY,0.23793103448275862,"to a drop in the completeness metric. However we also show that adding such tricks leads to signif-
icant improvement in the soundness metric, thus providing a novel justiﬁcation for the use of such
tricks, beyond just the heuristic argument of getting rid of artifacts. In Section 5 we also provide
theoretical justiﬁcation for why TV penalty can help with soundness, even for the simple case of
linear predictors on non-image data."
CLARIFYING BENEFIT OF TV REGULARIZATION,0.2413793103448276,"5
CLARIFYING BENEFIT OF TV REGULARIZATION"
CLARIFYING BENEFIT OF TV REGULARIZATION,0.24482758620689654,"Our experiments show that the main beneﬁt of TV regularization in saliency methods is that it
improves soundness. Here we sketch a simple example showing how this beneﬁts even for simple
linear models on non-image data. Details appear in Appendix E. Consider S, a dataset of labeled
data (x, y) where labels y are binary, and a linear classiﬁer with weight w and margin γ > 0.
Assume inputs are of unit norm and with bounded ℓ∞norm and so is w. The saliency method has to
return a single subset S of salient coordinates to validate the label. The input modiﬁcation process
Γ will assign 0 to all coordinates outside S. Question is whether the decision can be validated using
a small S. A priori this seems impossible. Furthermore imposing a TV constraint seems to help
nothing, because the solution has no obvious continuity structure."
CLARIFYING BENEFIT OF TV REGULARIZATION,0.2482758620689655,"But now assume we randomly permute the coordinates. This (paradoxically) turns out to make a big
difference! For simplicity we consider salient sets among intervals, in other words sets of TV ≤2.
Theorem 5.1. For any (x, y) ∈S, after random shufﬂe following holds for any L1 = Ω( 1"
CLARIFYING BENEFIT OF TV REGULARIZATION,0.2517241379310345,γ2 log 1
CLARIFYING BENEFIT OF TV REGULARIZATION,0.25517241379310346,"δ ),
L2 = Ω( 1"
CLARIFYING BENEFIT OF TV REGULARIZATION,0.25862068965517243,γ2 log d
CLARIFYING BENEFIT OF TV REGULARIZATION,0.2620689655172414,"δ ), where probability is over the random shufﬂe:"
CLARIFYING BENEFIT OF TV REGULARIZATION,0.2655172413793103,"1. (Completeness) With probability 1 −δ, there is an interval of length L1 that validates y;"
CLARIFYING BENEFIT OF TV REGULARIZATION,0.2689655172413793,"2. (Soundness) With probability 1 −δ, no interval of length L2 can validate −y."
EXPERIMENTS,0.27241379310344827,"6
EXPERIMENTS"
EXPERIMENTS,0.27586206896551724,"In this section, we present results for our mask learning procedures described in Section 4 and our
consistency score metric described in Deﬁnition 3.3. We also analyze the role of TV penalty and
upsampling. We observe from the results that (1) our simple mask learning procedure achieves
competitive performance with existing saliency methods on existing metrics; (2) our procedure does
better on our consistency score metric compared to previous methods, which suggests that consis-
tency score has no major conﬂicts with previous metrics and that our method is more sound; (3) TV
regularization and upscaling signiﬁcantly aid soundness, while only slightly hurting completeness.
Overall, consistency score increases with higher TV regularization and upscaling, thus showing the
ability of our metrics to provide intrinsic explanations for existing “tricks” that previously only had
extrinsic justiﬁcations."
EXPERIMENTS,0.2793103448275862,"Experiments involving consistency score are computationally infeasible to perform on ImageNet
as it contains 1000 classes. Therefore, these experiments are performed either on CIFAR-10 or
Imagenette (Howard, 2020), a 10-class ImageNet subset. Even for CIFAR-10 and Imagenette, since"
EXPERIMENTS,0.2827586206896552,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.28620689655172415,"Table 1: We compute AUC of insertion game with gray inﬁlling as masking explanation. The last
two columns are the mask-model consistency conditioned on whether the base classiﬁer predicts the
ground truth y. The best two of each column are marked bold. From the table, our method with
upscaling factor s = 4 achieves the best consistency."
EXPERIMENTS,0.2896551724137931,"Deletion ↓
Insertion
Saliency
Consistency
Consistency
Consistency
(gray) ↑
Metric ↓
Score ↑
when ˆy = y ↑
when ˆy ̸= y ↑
Gradient ⊙Input
0.42
0.56
0.31
0.841
0.977
0.27
Real time saliency
0.48
0.66
−0.85
0.857
0.980
0.34
Fong & Vedaldi (2017)
0.64
0.63
−0.41
0.845
0.990
0.23
Phang et al. (2020)
0.43
0.76
−0.27
0.871
0.985
0.39
Ours (s = 1)
0.52
0.68
−0.91
0.862
0.968
0.41
Ours (s = 4)
0.47
0.57
−0.66
0.880
0.980
0.46
Random
0.45
0.45
−0.35
0.829
0.964
0.26"
EXPERIMENTS,0.29310344827586204,"consistency score requires computing 10 times more saliency maps than other metrics, we test it on
1000 randomly drawn images from original test set. We also run experiments that do not involving
consistency score on various datasets (including ImageNet and CIFAR-100) with various models.
Results are shown in Appendix C. Details of training procedure (beyond those in Section 4) are also
in the Appendix C."
FULL COMPARISON TO EXISTING METRICS AND METHODS ON IMAGENETTE,0.296551724137931,"6.1
FULL COMPARISON TO EXISTING METRICS AND METHODS ON IMAGENETTE"
FULL COMPARISON TO EXISTING METRICS AND METHODS ON IMAGENETTE,0.3,"We compare our procedure to other methods including Real Time Saliency (Dabkowski & Gal,
2017), Gradient ⊙Input (Shrikumar et al., 2017), Fong & Vedaldi (2017) and Phang et al. (2020)
on Imagenette. Apart from our new consistency score metric, we calculate the Deletion Game and
Insertion Game metrics using the code provided by https://github.com/eclique/RISE,
and Saliency metric (SM), an another intrinsic evaluation metric from Dabkowski & Gal (2017).
Detailed description of the different metrics can be found in the Appendix."
FULL COMPARISON TO EXISTING METRICS AND METHODS ON IMAGENETTE,0.30344827586206896,"For our method, we use the learned mask M from optimizing the objective function in Equation (4)
with λT V = 0.01 and s = {1, 4} as the saliency map. For fairness, we use the identical ResNet50
pretrained on ImageNet as the base classiﬁer for every saliency method. The salient sets used in
our consistency score follows the Insertion game. Insertion scores were calculated by replacing
remaining pixels with gray. We normalize the maps so that all values lie in [0, 1] before use."
FULL COMPARISON TO EXISTING METRICS AND METHODS ON IMAGENETTE,0.30689655172413793,"To get a sense what the numbers mean, we also include the performance of a random mask, where
the score for each pixel is sampled independently from standard normal distribution. The results are
shown in Table 1. For the deletion metric, we note that most methods have comparable or worse
performance than the random mask, which suggests that the metric does not give us much signal
about the goodness of the saliency maps. For the Insertion scores and saliency metric, our method
performs decently since these metrics mostly depend on variety of the completeness condition."
FULL COMPARISON TO EXISTING METRICS AND METHODS ON IMAGENETTE,0.3103448275862069,"For our consistency score metric, most previous methods perform well on consistency score when
the base classiﬁer predicts the ground truth. However, their performance on samples that the base
classiﬁer predicts incorrectly is not as good as ours. Our method shows that soundness can be im-
proved without much affecting performance on other known metrics. Note that our metric assumes
that testing methods take effort to generate different masks for each possible label. Saliency methods
like Phang et al. (2020) actually inherent advantage on our metric by not doing that. Considering
that our method bears similarity to prior work but outperforms them on consistency score even with
arguable disadvantage, it implies there is room for improvement on soundness for previous works."
FULL COMPARISON TO EXISTING METRICS AND METHODS ON IMAGENETTE,0.3137931034482759,"Table 1 also shows that upscaling improves the consistency score. As mentioned in Section 4,
upscaling was used in previous work to avoid “artifacts” and produce better looking masks. Our
experiment shows a new explanation for the upscaling that it improves the soundness of the masking."
FULL COMPARISON TO EXISTING METRICS AND METHODS ON IMAGENETTE,0.31724137931034485,"To verify the connection between completeness/soundness and consistency score, we compute the
completeness and soundness (with ϵ = 10−3) for test samples, and report the average in Table 2. It
shows completeness/soundness and consistency score are highly correlated. Even though saliency
methods like Phang et al. (2020) have inherent advantage in soundness, our method still achieves
the best soundness. Our method also gains better completeness, which is likely contributed by
maximizing the probability assigned to the given label instead of matching the model probability."
FULL COMPARISON TO EXISTING METRICS AND METHODS ON IMAGENETTE,0.32068965517241377,Under review as a conference paper at ICLR 2022
FULL COMPARISON TO EXISTING METRICS AND METHODS ON IMAGENETTE,0.32413793103448274,"Table 2: Completeness and soundness for each saliency methods, averaged over test samples. Com-
pleteness are computed on labels that the model predicts, and soundness are computed on the in-
correct labels (w.r.t. model prediction). For soundness, we compute both the average among the
incorrect labels and the minimum one among them. Consistency scores are listed in the last column
for a comparison. The best two of each column are marked bold. Consistency scores are highly
correlated to completeness/soundness in the table."
FULL COMPARISON TO EXISTING METRICS AND METHODS ON IMAGENETTE,0.3275862068965517,"Completeness of
Average soundness of
Worst soundness of
Consistency
model prediction
the incorrect labels
the incorrect labels
Score
Gradient ⊙Input
0.54
0.98
0.83
0.841
Real time saliency
0.56
0.98
0.85
0.857
Fong & Vedaldi (2017)
0.53
0.95
0.61
0.845
Phang et al. (2020)
0.71
0.98
0.87
0.871
Ours (s = 1)
0.85
0.95
0.61
0.862
Ours (s = 4)
0.83
0.99
0.89
0.880
Random
0.35
0.96
0.70
0.829"
FULL COMPARISON TO EXISTING METRICS AND METHODS ON IMAGENETTE,0.3310344827586207,"Table 3: Consistency scores for different λT V in CIFAR-10. The last two rows are the mask-model
consistency conditioned on whether the base classiﬁer predicts the ground truth. The best one in
each row is marked bold. λT V = 0.1 performs the best in both situations."
FULL COMPARISON TO EXISTING METRICS AND METHODS ON IMAGENETTE,0.33448275862068966,"λT V
0
0.001
0.01
0.1
Consistency Score
0.46
0.37
0.49
0.62
Consistency (correct)
0.47
0.38
0.51
0.64
Consistency (incorrect)
0.24
0.21
0.16
0.36"
EFFECT OF TV REGULARIZATION,0.33793103448275863,"6.2
EFFECT OF TV REGULARIZATION"
EFFECT OF TV REGULARIZATION,0.3413793103448276,"In this subsection, we test our theoretical prediction from Section 5 that TV penalty improves sound-
ness. We ﬁrst visually inspect the masks learned by our procedure as we increase the TV regular-
ization strength8 in Figure 4. Masked images at 10 % sparsity are depicted in odd columns and the
full masks are shown in even columns. We plot the AUC insertion metric with blur evaluated on
the non-sparsely masked images on the y axis of each plot. We ﬁnd that as we increase the TV
regularization strength, the model can still ﬁnd a saliency map for the correct label with high AUC
score, however the AUC score for the mask learned to ﬁt the second most conﬁdent label drops
signiﬁcantly. This, in conjunction with Lemma 3.5, suggests that the TV regularization method has
slightly worse completeness but much higher soundness. Figure 3 plots the model output proba-
bility for CIFAR-10 as more pixels from the original image are retained using the mask. Our high
level ﬁnding is again that adding TV penalty and upsampling signiﬁcantly aid soundness, while only
slightly hurting completeness. To formally justify the effect of TV penalty, we measure our consis-
tency score of different level of TV penalty on CIFAR-10. To further challenge the completeness
and soundness, the salient sets of our consistency score follows the Insertion game but only takes
sets of size 0.2d to 0.6d (d is the number of pixels). Table 3 shows the result."
CONCLUSIONS,0.3448275862068966,"7
CONCLUSIONS"
CONCLUSIONS,0.3482758620689655,"Saliency explanations of ML models has proved nebulous and generated many controversies. By
taking rooted in intrinsic deﬁnitions such as completeness/soundness and consistency score, this
paper has tried to provide greater rigor to the intrinsic approaches to saliency. Other new contri-
butions include clarifying the role of TV regularization (it hurts completeness slightly but greatly
improves soundness); extensive experimental evaluations that bring new understanding using con-
sistency score; and a simple saliency method for producing mask-based explanations whose perfor-
mance is broadly competitive with good existing methods, and sometimes better."
CONCLUSIONS,0.35172413793103446,"The soundness notion is clearly useful for localization of objects in the image, and for classiﬁcation
in the wild –where multiple objects appear over the image. This was hinted in our evaluation on a
prior small dataset (see Appendix B). It may have other applications to handling distribution shift
and other types of robustness, but such explorations are left for future work."
SIMILAR IMAGES FOR SCALING FACTORS CAN BE FOUND IN APPENDIX C,0.35517241379310344,8Similar images for scaling factors can be found in Appendix C
SIMILAR IMAGES FOR SCALING FACTORS CAN BE FOUND IN APPENDIX C,0.3586206896551724,Under review as a conference paper at ICLR 2022
REFERENCES,0.3620689655172414,REFERENCES
REFERENCES,0.36551724137931035,"Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim.
Sanity checks for saliency maps. Advances in Neural Information Processing Systems, 31:9505–
9515, 2018."
REFERENCES,0.3689655172413793,"Chirag Agarwal and Anh Nguyen. Explaining image classiﬁers by removing input features using
generative models. In Proceedings of the Asian Conference on Computer Vision, 2020."
REFERENCES,0.3724137931034483,"Sebastian Bach, Alexander Binder, Gr´egoire Montavon, Frederick Klauschen, Klaus-Robert M¨uller,
and Wojciech Samek. On pixel-wise explanations for non-linear classiﬁer decisions by layer-wise
relevance propagation. PloS one, 10(7):e0130140, 2015."
REFERENCES,0.3758620689655172,"David Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja Hansen, and Klaus-
Robert M¨uller. How to explain individual classiﬁcation decisions. The Journal of Machine Learn-
ing Research, 11:1803–1831, 2010."
REFERENCES,0.3793103448275862,"Alexander Binder, Gr´egoire Montavon, Sebastian Lapuschkin, Klaus-Robert M¨uller, and Wojciech
Samek. Layer-wise relevance propagation for neural networks with local renormalization layers.
In International Conference on Artiﬁcial Neural Networks, pp. 63–71. Springer, 2016."
REFERENCES,0.38275862068965516,"Lukas Brunke, Prateek Agrawal, and Nikhil George. Evaluating input perturbation methods for
interpreting cnns and saliency map comparison. In European Conference on Computer Vision,
pp. 120–134. Springer, 2020."
REFERENCES,0.38620689655172413,"Chun-Hao Chang, Elliot Creager, Anna Goldenberg, and David Duvenaud. Explaining image clas-
siﬁers by counterfactual generation. In International Conference on Learning Representations,
2018."
REFERENCES,0.3896551724137931,"Piotr Dabkowski and Yarin Gal. Real time image saliency for black box classiﬁers. In Proceedings
of the 31st International Conference on Neural Information Processing Systems, pp. 6970–6979,
2017."
REFERENCES,0.3931034482758621,"Nicola De Cao, Michael Sejr Schlichtkrull, Wilker Aziz, and Ivan Titov. How do decisions emerge
across layers in neural models? interpretation with differentiable masking. In Proceedings of the
2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 3243–
3255, 2020."
REFERENCES,0.39655172413793105,"Ruth C Fong and Andrea Vedaldi. Interpretable explanations of black boxes by meaningful perturba-
tion. In Proceedings of the IEEE International Conference on Computer Vision, pp. 3429–3437,
2017."
REFERENCES,0.4,"Daniel Fryer, Inga Str¨umke, and Hien Nguyen. Shapley values for feature selection: The good, the
bad, and the axioms. ICLR, 2021."
REFERENCES,0.40344827586206894,"Jindong Gu, Yinchong Yang, and Volker Tresp. Understanding individual decisions of cnns via
contrastive backpropagation. In Asian Conference on Computer Vision, pp. 119–134. Springer,
2018."
REFERENCES,0.4068965517241379,"Xin Hong, Pengfei Xiong, Renhe Ji, and Haoqiang Fan. Deep fusion network for image completion.
In Proceedings of the 27th ACM International Conference on Multimedia, pp. 2033–2042, 2019."
REFERENCES,0.4103448275862069,"Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. A benchmark for interpretabil-
ity methods in deep neural networks. In NeurIPS, 2019."
REFERENCES,0.41379310344827586,"Jeremy Howard. imagenette, 2019. URL https://github. com/fastai/imagenette, 2020."
REFERENCES,0.41724137931034483,"Alexey Ignatiev, Nina Narodytska, and Joao Marques-Silva. Abduction-based explanations for ma-
chine learning models. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol-
ume 33, pp. 1511–1519, 2019a."
REFERENCES,0.4206896551724138,"Alexey Ignatiev, Nina Narodytska, and Joao Marques-Silva. On relating explanations and adversar-
ial examples. 2019b."
REFERENCES,0.4241379310344828,Under review as a conference paper at ICLR 2022
REFERENCES,0.42758620689655175,"Ashkan Khakzar, Soroosh Baselizadeh, Saurabh Khanduja, Christian Rupprecht, Seong Tae Kim,
and Nassir Navab. Improving feature attribution through input-speciﬁc network pruning. arXiv
e-prints, pp. arXiv–1911, 2019."
REFERENCES,0.43103448275862066,"Beomsu Kim, Junghoon Seo, Seunghyeon Jeon, Jamyoung Koo, Jeongyeol Choe, and Taegyun Jeon.
Why are saliency maps noisy? cause of and solution to noisy saliency maps. In 2019 IEEE/CVF
International Conference on Computer Vision Workshop (ICCVW), pp. 4149–4157. IEEE, 2019."
REFERENCES,0.43448275862068964,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.4379310344827586,"Narine Kokhlikyan, Vivek Miglani, Miguel Martin, Edward Wang, Bilal Alsallakh, Jonathan
Reynolds, Alexander Melnikov, Natalia Kliushkina, Carlos Araya, Siqi Yan, and Orion Reblitz-
Richardson. Captum: A uniﬁed and generic model interpretability library for pytorch, 2020."
REFERENCES,0.4413793103448276,"Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Doll´ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European
conference on computer vision, pp. 740–755. Springer, 2014."
REFERENCES,0.44482758620689655,"Scott M Lundberg and Su-In Lee.
A uniﬁed approach to interpreting model predic-
tions.
In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,
and R. Garnett (eds.), Advances in Neural Information Processing Systems. Curran Asso-
ciates, Inc., 2017a. URL https://proceedings.neurips.cc/paper/2017/file/
8a20a8621978632d76c43dfd28b67767-Paper.pdf."
REFERENCES,0.4482758620689655,"Scott M Lundberg and Su-In Lee. A uniﬁed approach to interpreting model predictions. In Proceed-
ings of the 31st international conference on neural information processing systems, pp. 4768–
4777, 2017b."
REFERENCES,0.4517241379310345,"Jan Macdonald, Stephan W¨aldchen, Sascha Hauch, and Gitta Kutyniok. A rate-distortion framework
for explaining neural network decisions. arXiv e-prints, pp. arXiv–1905, 2019."
REFERENCES,0.45517241379310347,Jesse Mu and Jacob Andreas. Compositional explanations of neurons. 2020.
REFERENCES,0.4586206896551724,"Vitali Petsiuk, Abir Das, and Kate Saenko. Rise: Randomized input sampling for explanation of
black-box models. In Proceedings of the British Machine Vision Conference (BMVC), 2018."
REFERENCES,0.46206896551724136,"Jason Phang, Jungkyu Park, and Krzysztof J Geras. Investigating and simplifying masking-based
saliency methods for model interpretability. arXiv preprint arXiv:2010.09750, 2020."
REFERENCES,0.46551724137931033,"Wojciech Samek, Alexander Binder, Gr´egoire Montavon, Sebastian Lapuschkin, and Klaus-Robert
M¨uller. Evaluating the visualization of what a deep neural network has learned. IEEE transactions
on neural networks and learning systems, 28(11):2660–2673, 2016."
REFERENCES,0.4689655172413793,"Wojciech Samek, Gr´egoire Montavon, Andrea Vedaldi, Lars Kai Hansen, and Klaus-Robert M¨uller.
Explainable AI: interpreting, explaining and visualizing deep learning, volume 11700. Springer
Nature, 2019."
REFERENCES,0.4724137931034483,"Karl Schulz, Leon Sixt, Federico Tombari, and Tim Landgraf. Restricting the ﬂow: Information
bottlenecks for attribution. In International Conference on Learning Representations, 2019."
REFERENCES,0.47586206896551725,"Ramprasaath R Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael Cogswell, Devi Parikh,
and Dhruv Batra. Grad-cam: Why did you say that? International Journal of Computer Vision
(IJCV), 2019."
REFERENCES,0.4793103448275862,"Junghoon Seo, Jeongyeol Choe, Jamyoung Koo, Seunghyeon Jeon, Beomsu Kim, and Taegyun Jeon.
Noise-adding methods of saliency map as series of higher order partial derivative. 2018 ICML
Workshop on Human Interpretability in Machine Learning, pp. arXiv–1806, 2018."
REFERENCES,0.4827586206896552,"Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through
propagating activation differences. In International Conference on Machine Learning, pp. 3145–
3153. PMLR, 2017."
REFERENCES,0.4862068965517241,Under review as a conference paper at ICLR 2022
REFERENCES,0.4896551724137931,"Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Vi´egas, and Martin Wattenberg. Smoothgrad:
removing noise by adding noise. arXiv e-prints, pp. arXiv–1706, 2017."
REFERENCES,0.49310344827586206,"Mukund Sundararajan and Amir Najmi. The many shapley values for model explanation. In Inter-
national Conference on Machine Learning, pp. 9269–9278. PMLR, 2020."
REFERENCES,0.496551724137931,"Saeid Asgari Taghanaki, Mohammad Havaei, Tess Berthier, Francis Dutil, Lisa Di Jorio, Ghassan
Hamarneh, and Yoshua Bengio. Infomask: Masked variational latent representation to localize
chest disease. In International Conference on Medical Image Computing and Computer-Assisted
Intervention, pp. 739–747. Springer, 2019."
REFERENCES,0.5,"Mengjiao Yang and Been Kim. Benchmarking attribution methods with relative feature importance.
arXiv e-prints, pp. arXiv–1907, 2019."
REFERENCES,0.503448275862069,"Chih-Kuan Yeh, Been Kim, Sercan Arik, Chun-Liang Li, Tomas Pﬁster, and Pradeep Ravikumar.
On completeness-aware concept-based explanations in deep neural networks. Advances in Neural
Information Processing Systems, 33, 2020."
REFERENCES,0.506896551724138,"Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S Huang. Generative image
inpainting with contextual attention. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pp. 5505–5514, 2018."
REFERENCES,0.5103448275862069,"Jianming Zhang, Sarah Adel Bargal, Zhe Lin, Jonathan Brandt, Xiaohui Shen, and Stan Sclaroff.
Top-down neural attention by excitation backprop. International Journal of Computer Vision, 126
(10):1084–1102, 2018."
REFERENCES,0.5137931034482759,"Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10
million image database for scene recognition. IEEE transactions on pattern analysis and machine
intelligence, 40(6):1452–1464, 2017."
REFERENCES,0.5172413793103449,"Bolei Zhou, David Bau, Aude Oliva, and Antonio Torralba. Interpreting deep visual representations
via network dissection. IEEE transactions on pattern analysis and machine intelligence, 41(9):
2131–2145, 2018."
REFERENCES,0.5206896551724138,Under review as a conference paper at ICLR 2022
REFERENCES,0.5241379310344828,"A
INTUITIONS AND IMPLEMENTATIONS OF PROCEDURES TO FIND
MASKING EXPLANATIONS"
REFERENCES,0.5275862068965518,"As introduced in Section 3.3, for evaluation we may interest in random binary masks due to its
connection to AUC, but in our method for ﬁnding masking explanations we only focus on deter-
ministic masks. Given a network f, image x ∈Rc×hw and class a, we wish to ﬁnd a binary mask
M ∈{0, 1}hw such that when the part of x on M is superimposed onto a distractor ¯x ∼X as
˜x = M ⊙x + (1 −M) ⊙¯x, the output probability of the model f(˜x, a) is high for the class a. This
corresponds to the case where ∆is a singleton that assigns probability 1 to M and Γ(x, M) is the
distribution of ˜x9. As in Section 3.1 we compute the average probability assigned to class a over
the sampling of the distractor ¯x, i.e. we are interested in making E¯x∼X [f(˜x, a)] high. To avoid the
hard problem of optimizing over the hypercube {0, 1}hw, a typical strategy (also employed in prior
work) is to relax the domain of M to be [0, 1]hw. Since we do not wish to learn masks of very large
size, a ℓ1 norm penalty on M (corresponding to size of the mask), leading to the following natural
objective function10"
REFERENCES,0.5310344827586206,"L(M) = E¯x∼X [−log(f(M ⊙x + (1 −M) ⊙¯x, a))] + λ1∥M∥1
(5)"
REFERENCES,0.5344827586206896,"However most masking-based methods employ additional “tricks” in order to avoid “artifacts” in the
produced saliency maps, like Total-Variation (TV) penalty (Fong & Vedaldi, 2017) and upscaling of
the mask from a lower resolution one (Petsiuk et al., 2018). We also employ the same strategy by
learning a low-resolution mask at scale s, M ∈Rhw/s2, to minimize the following"
REFERENCES,0.5379310344827586,"L(M) = E¯x∼X

−log(f(M ×s ⊙x + (1 −M ×s) ⊙¯x, a))

+ λT V TV (M ×s) + λ1∥M ×s∥1
(6)"
REFERENCES,0.5413793103448276,"where M ×s ∈Rhw is obtained by upscaling M by a factor of s ∈{1, 4} via bilinear interpolation."
REFERENCES,0.5448275862068965,"While the motivation cited for these “trick” is to avoid artifacts, it is not clear whether artifacts are
a bad thing, since they might be relevant to the net’s decision-making. Indeed, we show that while
TV penalty or upscaling does produce better looking masks, they lead to a drop in the completeness
metric. However we show that adding such tricks leads to signiﬁcant improvement in the soundness
metric, thus providing a novel justiﬁcation for the use of such tricks, beyond just the heuristic ar-
gument of getting rid of artifacts. In Section 5 we also provide theoretical justiﬁcation for why TV
penalty can help with soundness, even for the simple case of linear predictors on non-image data."
REFERENCES,0.5482758620689655,"We optimize the objective in Equation (4) by parametrizing M as a sigmoid of real valued weights
W ∈Rhw/s2, i.e. M = σ(W), and run Adam (Kingma & Ba, 2014) optimizer for 2000 steps with
learning rate 0.05 and by sampling 10 distractor images at every step, for different values of λT V
and upscaling factor s. We report the effect of λT V qualitatively in Figure 4 and quantitatively on
various intrinsic saliency metrics in Table 10."
REFERENCES,0.5517241379310345,"B
PRACTICAL BENEFITS OF SOUNDNESS FOR IMAGES OF MULTIPLE
OBJECTS"
REFERENCES,0.5551724137931034,"Images may have multiple plausible labels. In Figure 2, images that previously used in Gu et al. 2018
can have both elephants and zebras present, but it may not be always clear from the model output if
there is such a case, since the model can be much more conﬁdent on one label, e.g., elephant, than
one would expect it to be. For this reason, ﬁnding masking explanations validating other labels, e.g.,
zebra, could provide more information on how the model makes the prediction."
REFERENCES,0.5586206896551724,"C
EXPERIMENTAL DETAILS AND ADDITIONAL EXPERIMENTS"
REFERENCES,0.5620689655172414,"In this section we expand upon the experiments in Section 6 and complement them with more
experiments on the ImageNet, CIFAR-10 and CIFAR-100 datasets. For each of the datasets we
test the following:"
REFERENCES,0.5655172413793104,"9We slightly abuse the use of notation to use M as a set over coordinates rather than a binary mask.
10A standard way to maximize probability is to minimize the negative log probability"
REFERENCES,0.5689655172413793,Under review as a conference paper at ICLR 2022
REFERENCES,0.5724137931034483,"Figure 4: Top: Masking validating the correct label of ImageNet images using Section 4. Pixels out-
side the salient set S are rendered as grey. Size of S (as % of total pixels) appears below the images.
Value of TV regularizer shown above each column corresponding to (0,.001,.01,.1). Original image
rendered in last column. True labels with original model probabilities are shown in the rightmost
column. Insertion metric when the mask is found by the procedure in Section 4 shown on y axis
Bottom: Masking validating the second-best label, which appears on the right along with original
model probability of that label. Insertion metric when the mask is found by ﬁtting the second best
label under the procedure in Section 4 is shown on the y label of each plot. We ﬁnd that increasing
the TV regularizer makes the resulting assignment more sound."
REFERENCES,0.5758620689655173,"• Visualization: For various values of TV regularization (and upscaling for ImageNet), we
visualize the mask and also what part of the image a sparse version of the mask highlights.
We do so for masks learned for the correct label and also for the second most probable
label as predicted by the model. The common trend is that while TV regularization (and
upscaling) make the masks more human interpretable, it also makes it harder to ﬁnd a good
mask (partial assignment) for the incorrect label, thus improving soundness."
REFERENCES,0.5793103448275863,"• AUC curve: We plot the output model probability for a masked input as more pixels
from the original image are selected. The 4 plots denote replacing remaining pixels with
grey pixels or pixels from a random image, and masks to ﬁt the correct or incorrect label.
Again, we ﬁnd the TV regularization and upsampling help with soundness. For mask M, if
¯
M(p) denotes the discrete mask with top p fraction of the pixels from M picked. We plot
Ex

Ex′∼Γ[f( ¯
M(p) ⊙x + (1 −¯
M(p)) ⊙x′, a)]

v/s p, where Γ is either a random image
or a grey image, and a is either the correct label for x or the second best label."
REFERENCES,0.5827586206896552,"• Completeness/soundness: We evaluate completeness and soundness metrics as deﬁned in
Deﬁnition 3.2. In particular, for every input x, we evaluate completeness for the correct
label a and soundness for the incorrect (second best) label a′. For any mask M, we de-
ﬁne AUC as AUC(M; (x, a)) = Ep

Ex′∼Γ[f( ¯
M(p) ⊙x + (1 −¯
M(p)) ⊙x′, a)]

. Com-"
REFERENCES,0.5862068965517241,Under review as a conference paper at ICLR 2022
REFERENCES,0.5896551724137931,pleteness and soundness are deﬁned as
REFERENCES,0.593103448275862,"Cδ(M) = E(x,a)"
REFERENCES,0.596551724137931,"
min
AUC(M; (x, a))"
REFERENCES,0.6,"min{f(x, a), δ} , 1

(7)"
REFERENCES,0.603448275862069,"Sϵ(M) = E(x,a′)"
REFERENCES,0.6068965517241379,"
min
 max{f(x, a′), ϵ}"
REFERENCES,0.6103448275862069,"AUC(M; (x, a′)), 1

(8)"
REFERENCES,0.6137931034482759,"where a is the correct label and a′ is the incorrect label and f(x, a) is the model probability
for label a for input x. δ and ϵ can be any reasonable constant to stabilize the value.
• Intrinsic metrics: We evaluate our masks on other intrinsic metrics from prior work, and
compare to baseline saliency methods. Our baselines include Gradient ⊙Input (Shrikumar
et al., 2017), Smooth-Grad (Smilkov et al., 2017), Real Time Saliency (Dabkowski & Gal,
2017) (for ResNet-50 on ImageNet), and Random indicating a random Gaussian mask as a
control. We use Captum (Kokhlikyan et al., 2020) for Gradient ⊙Input and Smooth-Grad
implementations and the original author code11 for Real Time Saliency. When calculating
the Saliency Metric (SM) (Dabkowski & Gal, 2017) we tune the threshold δ on a holdout
set of size 100 with δ between 0 and 5 in increments of 0.2 as in prior work.
For the saliency method of Fong & Vedaldi (2017) that we only used on the Imagenette, we
adapt the most popular implementation on GitHub12. The implementation contains minor
deviations from the original paper as described on its main page. For Phang et al. (2020),
we used their best CA model pretrained and provided in original author code13."
REFERENCES,0.6172413793103448,"C.1
CIFAR-10 EXPERIMENTS"
REFERENCES,0.6206896551724138,"We also run our method from Section 4 on the CIFAR-10 dataset using a pretrained ResNet-164
architecture14. For all experiments we learn a mask M ∈R32×32, thus using a scaling factor
of s = 1 (no upscaling). We train masks for 1600 images that were correctly classiﬁed by the
pretrained ResNet-164 using regularization parameter λT V ∈{0, 0.001, 0.01, 0.1}. We use a (ﬁxed)
L1 regularization value of .001 for all masks."
REFERENCES,0.6241379310344828,"We visualize the masks learned for the correct label in Figure 6a and in Figure 6b we visualize the
same for the second best label predicted by the ResNet-164 model. We also visualize the masks for
all labels for some randomly picked images in Figure 7 to demonstrate the commonness of artifact,
especially for the incorrect labels. The AUC curves in Figure 5 suggest a similar trend to that of
ImageNet, adding TV regularization results in only a mild drop in completeness, but signiﬁcantly
improves soundness. Evaluation of our masks, compared to some gradient baselines, on intrinsic
metrics can be found in Table 5. We place a downarrow after the name of the metric to indicate
a lower value is considered better and an uparrow when a higher value is considered better. We
evaluate on a randomly selected subset of 1000 data points where the model had correct top 1
accuracy. We report the completeness and soundness results for CIFAR-10 in Table 4 for TV values
in (0.0,0.001,0.01,0.1) calculated using a ResNet-164 model."
REFERENCES,0.6275862068965518,"C.2
CIFAR-100 EXPERIMENTS"
REFERENCES,0.6310344827586207,"We run the same experiment for CIFAR-100 using the corresponding ResNet164 model. We visu-
alize the masks learned for the correct label in Figure 9a and in Figure 9b we visualize the same for
the second best label predicted by the ResNet-164 model. The AUC curves in Figure 13 suggest a
similar trend to that of ImageNet, adding TV regularization results in only a mild drop in complete-
ness, but signiﬁcantly improves soundness. Evaluation of our masks, compared to some gradient
baselines, on intrinsic metrics can be found in Table 7. We place a downarrow after the name of the
metric to indicate a lower value is considered better and an uparrow when a higher value is consid-
ered better. We evaluate on a randomly selected subset of 1000 data points where the model had
correct top 1 accuracy. When calculating the saliency metric we tune the threshold δ on a holdout
set of size 100 with δ between 0 and 5 in increments of 0.2."
REFERENCES,0.6344827586206897,"11https://github.com/PiotrDabkowski/pytorch-saliency
12https://github.com/jacobgil/pytorch-explain-black-box
13https://github.com/zphang/saliency_investigation
14https://github.com/bearpaw/pytorch-classification. The ResNet-110 model in this
repository is actually a ResNet-164 model."
REFERENCES,0.6379310344827587,Under review as a conference paper at ICLR 2022
REFERENCES,0.6413793103448275,"Table 4: Completeness and soundness for a ResNet-164 CIFAR-10 as deﬁned in Equation (8). Each
column contains a tuple (Grey/Noise, TV 0.0/ TV 0.001/ TV 0.01/ TV 0.1). Grey indicates pixels
were greyed during calculation. Noise indicates they were replaced with other images. TV indicates
a TV regularization value of 0.0, 0.001, 0.01, or 0.1."
REFERENCES,0.6448275862068965,"Grey
TV = 0.0
TV= 0.001
TV= 0.01
TV= 0.1
Correct label completeness (C0.8)
0.92
0.91
0.84
0.79
Second label soundness (S0.2)
0.27
0.28
0.34
0.38
Noise
TV = 0.0
TV= 0.001
TV= 0.01
TV= 0.1
Correct label completeness (C0.8)
0.90
0.88
0.81
0.75
Second label soundness (S0.2)
0.30
0.31
0.37
0.42"
REFERENCES,0.6482758620689655,"Figure 5: [CIFAR-10] AUC curves with as the fraction of pixels retained from the original images
based on the mask varies from 0 to 1.0 on the X-axis. The probabilities assigned by the model
(averaged over 1600 images) on the Y-axis. Left: Mask learned for ground truth label, probabilities
for ground truth label while replacing remaining pixels with grey. Center Left:
Mask learned
for ground truth label, probabilities for ground truth label while replacing remaining pixels with
other image pixels. Center Right:
Mask learned for second best label, probabilities for second
best label while replacing remaining pixels with grey. Right: Mask learned for second best label,
probabilities for second best label while replacing remaining pixels with other image pixels. We
see that increasing TV regularization results in only a mild drop in completeness, but signiﬁcantly
improves soundness."
REFERENCES,0.6517241379310345,"Table 5: Performance of our method on CIFAR-10 and some baselines on various intrinsic saliency
metrics proposed in prior work. We ﬁnd that while both our masks (learned with and without TV)
have very good performance on the insertion metric. The deletion and saliency metrics are uninfor-
mative in this case, since all methods are as good (or worse) compared to a random mask."
REFERENCES,0.6551724137931034,"Gradient ⊙Input
Our method
Our Method
Smooth-Grad
Random
(λT V = 0.01)
(λT V = 0)
saliency
Deletion ↓
0.32
0.37
0.59
0.31
0.26
Insertion (blur) ↑
0.60
0.88
0.94
0.66
0.36
Insertion (grey) ↑
0.51
0.83
0.92
0.55
0.26
Saliency Metric ↓
0.22
0.22
0.22
0.23
0.22"
REFERENCES,0.6586206896551724,"We report the completeness and soundness results for CIFAR-100 in Table 6 for TV values in
(0.0,0.001,0.01,0.1) calculated using a ResNet-164 model."
REFERENCES,0.6620689655172414,Under review as a conference paper at ICLR 2022 (a) (b)
REFERENCES,0.6655172413793103,"Figure 6: Details in Appendix C.1 Panel 6a Partial statistical assignments validating the correct
label of CIFAR-10 images using the procedure outlined in Section 4 on ResNet-164. Columns
(1,3,5,7) depict masked images at 30 (retained) % mask sparseness. Columns (2,4,6,8) depict the
original mask. TV values shown above. Original image shown in rightmost column. Model proba-
bility of correct label for masked images on y axis. Panel 6b Partial statistical assignments validating
the second most probable label of CIFAR-10 images using the procedure outlined in Section 4 on
ResNet-164. Columns (1,3,5,7) depict masked images at 30 % mask sparseness. Columns (2,4,6,8)
depict the original mask.TV values shown above. Original image shown in rightmost column. Model
probability of second best label for masked images on y axis."
REFERENCES,0.6689655172413793,"Table 6: Completeness and soundness for a ResNet-164 CIFAR-100 as deﬁned in Equation (8).
Each column contains a tuple (Grey/Noise, TV 0.0/ TV 0.001/ TV 0.01/ TV 0.1). Grey indicates
pixels were greyed during calculation. Noise indicates they were replaced with other images. TV
indicates a TV regularization value of 0.0, 0.001, 0.01, or 0.1."
REFERENCES,0.6724137931034483,"Grey
TV = 0.0
TV= 0.001
TV= 0.01
TV= 0.1
Correct label completeness (C0.8)
0.78
0.74
0.66
0.60
Second label soundness (S0.2)
0.39
0.42
0.50
0.54
Noise
TV = 0.0
TV= 0.001
TV= 0.01
TV= 0.1
Correct label completeness (C0.8)
0.72
0.70
0.65
0.58
Second label soundness (S0.2)
0.46
0.47
0.51
0.56"
REFERENCES,0.6758620689655173,"C.3
EXPERIMENTS ON IMAGENET"
REFERENCES,0.6793103448275862,"C.3.1
RESNET-18"
REFERENCES,0.6827586206896552,"In Figure 10a we depict the the masks for TV values in {0.0, 0.01} for a ResNet-18 model on
ImageNet for the ground truth label and in Figure 10b we depict the same for the second best label.
We also experiment with the effect of upsampling (US) the mask, whereby we learn a mask of size
(56,56) and upsample to size (224,224). We use a ﬁxed L1 regularization value of 2e-5. We depict
our results on ImageNet and ResNet-18 in Table 9"
REFERENCES,0.6862068965517242,Under review as a conference paper at ICLR 2022
REFERENCES,0.6896551724137931,(a) Our method with no TV regularization
REFERENCES,0.6931034482758621,(b) Our method with TV regularization λT V = 0.01
REFERENCES,0.696551724137931,"Figure 7: A demonstration of artifacts created by masking. Pixels (partially) masked out are ﬁlled
with gray based on the fractions they are masked out. Masks generated without or only with low
level regularization can easily produce artifacts. It is more common and/or severe for the incorrect
label than correct label."
REFERENCES,0.7,Under review as a conference paper at ICLR 2022
REFERENCES,0.7034482758620689,"Figure 8: [CIFAR-100] AUC curves with as the fraction of pixels retained from the original images
based on the mask varies from 0 to 1.0 on the X-axis. The probabilities assigned by the model
(averaged over 1600 images) on the Y-axis. Left: Mask learned for ground truth label, probabilities
for ground truth label while replacing remaining pixels with grey. Center Left:
Mask learned
for ground truth label, probabilities for ground truth label while replacing remaining pixels with
other image pixels. Center Right:
Mask learned for second best label, probabilities for second
best label while replacing remaining pixels with grey. Right: Mask learned for second best label,
probabilities for second best label while replacing remaining pixels with other image pixels. We
see that increasing TV regularization results in only a mild drop in completeness, but signiﬁcantly
improves soundness."
REFERENCES,0.7068965517241379,"Table 7: Performance of our method on CIFAR-100 and some baselines on various intrinsic saliency
metrics proposed in prior work. We ﬁnd that while both our masks (learned with and without TV)
have very good performance on the insertion metric. The deletion and saliency metrics are uninfor-
mative in this case, since all methods are as good (or worse) compared to a random mask."
REFERENCES,0.7103448275862069,"Gradient ⊙Input
Our method
Our Method
Smooth-Grad
Random
(λT V = 0.01)
(λT V = 0)
saliency
Deletion ↓
0.10
0.17
0.10
0.29
0.11
Insertion (blur) ↑
0.36
0.71
0.82
0.39
0.20
Insertion (grey) ↑
0.27
0.62
0.76
0.29
0.11
Saliency Metric ↓
0.77
0.77
0.77
0.79
0.77"
REFERENCES,0.7137931034482758,"For the deletion metric, we note that most methods have comparable or worse performance than
the random mask, which suggests that the metric does not give us much signal about the goodness
of the saliency maps. On the insertion metric, we ﬁnd that mask learned by not adding the TV
penalty signiﬁcantly beats other methods. The mask learned using TV penalty, on the other hand,
has impressive performance on both the insertion AUC and saliency metric (SM)."
REFERENCES,0.7172413793103448,"Completeness and Soundness on ImageNet and ResNet-18 We report our results in Table 8 for
TV values in (0, 0.01) for both greying (Grey) and replacing with other image pixels (Noise). Addi-
tionally, we investigate the effect of upsampling (US) where we derive a (56,56) and upsample by a
factor of 4 to a (224, 224) mask."
REFERENCES,0.7206896551724138,"Effect of ensembling. In order to investigate the effect of ensembling we plot maps in Figure 12
as we vary the number of maps that are ensembled over as K ∈{1, 2, 4}, where we learn multiple
masks such that each of them are individually statistical assignments. We do not upsample (using a
scale of 1.0) and we use a ﬁxed L1 regularization of 2e-5 and a ﬁxed TV regularization of 0.0."
REFERENCES,0.7241379310344828,"C.3.2
RESNET-50"
REFERENCES,0.7275862068965517,"We present our results on ImageNet and ResNet-50 in Table 10. Using the same pretrained ResNet-
50 model as Dabkowski & Gal (2017) lets us compare our method to their real-time saliency method"
REFERENCES,0.7310344827586207,Under review as a conference paper at ICLR 2022 (a) (b)
REFERENCES,0.7344827586206897,"Figure 9: Details in Appendix C.2 Panel 9a Partial statistical assignments validating the correct
label of CIFAR-100 images using the procedure outlined in Section 4 on ResNet-164. Columns
(1,3,5,7) depict masked images at 30 (retained) % mask sparseness. Columns (2,4,6,8) depict the
original mask. TV values shown above. Original image shown in rightmost column. Model proba-
bility of correct label for masked images on y axis. Panel 9b Partial statistical assignments validating
the second most probable label of CIFAR-100 images using the procedure outlined in Section 4 on
ResNet-164. Columns (1,3,5,7) depict masked images at 30 % mask sparseness. Columns (2,4,6,8)
depict the original mask.TV values shown above. Original image shown in rightmost column. Model
probability of second best label for masked images on y axis."
REFERENCES,0.7379310344827587,"Table 8: Completeness and soundness for a ResNet-18 model on ImageNet as deﬁned in Equa-
tion (8). Each column contains a tuple (Grey/Noise, TV 0.0/ TV 0.001/ TV 0.01/ TV 0.1). Grey
indicates pixels were greyed during calculation. Noise indicates they were replaced with other im-
ages. no US indicates the full (224,224) mask was derived. US indicates a (56, 56) mask was derived
then upsampled by a factor of 4. TV indicates a TV regularization value of 0.0 or 0.01."
REFERENCES,0.7413793103448276,"Grey
TV = 0.0
TV = 0.01
US TV = 0.0
US TV = 0.01
Correct label completeness (C1)
0.93
0.82
0.75
0.72
Second label soundness (S0)
0.21
0.39
0.51
0.57
Noise
TV = 0.0
TV = 0.01
US TV = 0.0
US TV = 0.01
Correct label completeness (C1)
0.85
0.71
0.61
0.60
Second label soundness (S0)
0.24
0.40
0.53
0.57"
REFERENCES,0.7448275862068966,"directly on intrinsic metrics. Dabkowski & Gal (2017) do not evaluate on insertion/deletion metrics,
so we evaluate these using their pretrained mask model."
REFERENCES,0.7482758620689656,"C.4
EFFECT ON SANITY CHECKS"
REFERENCES,0.7517241379310344,"Inspired by (Adebayo et al., 2018) we randomize the last layer of a ResNet-18 network and visually
inspect the resulting saliency maps in Figure 11. We ﬁnd that the maps appear less coherent than
those of a pre-trained model. We use a ﬁxed L1 regularization of 2e-5 and depict maps with and
without upsampling (US) at TV values of (0, 0.01)."
REFERENCES,0.7551724137931034,Under review as a conference paper at ICLR 2022 (a) (b)
REFERENCES,0.7586206896551724,"Figure 10: Details in Appendix C.3.1. US stands for upsampled mask, where we derive a (56,56)
mask and interpolate to (224,224). Panel 10a Partial statistical assignments validating the correct la-
bel of ImageNet images using the procedure outlined in Section 4 on ResNet-50. Columns (1,3,5,7)
depict masked images at 30 (retained) % mask sparseness. Columns (2,4,6,8) depict the original
mask. TV values shown above. Original image shown in rightmost column. Model probability
of correct label for masked images on y axis. Panel 10b Partial statistical assignments validating
the second most probable label of ImageNet images using the procedure outlined in Section 4 on
ResNet-50. Columns (1,3,5,7) depict masked images at 30 % mask sparseness. Columns (2,4,6,8)
depict the original mask.TV values shown above. Original image shown in rightmost column. Model
probability of second best label for masked images on y axis. We ﬁnd, unsurprisingly, that adding
TV regularization and upsampling make the mask more continuous and “human interpretable” and,
more importantly, make it harder to ﬁnd masks that can get high probability for the second best
label, thus ensuring higher soundness."
REFERENCES,0.7620689655172413,"Table 9: Performance of our method on ImageNet and ResNet-18 model and some baselines on
various intrinsic saliency metrics proposed in prior work. We ﬁnd that while both our masks (learned
with and without TV) have very good performance on the insertion metric, the mask learned with
TV has much better performance on the saliency metric. The deletion metric is uninformative in
most cases, since most methods are as good (or worse) compared to a random mask."
REFERENCES,0.7655172413793103,"Gradient ⊙Input
Our method
Our Method
Smooth-Grad
Random
(λT V = 0.01)
(λT V = 0)
saliency
Deletion ↓
0.1054
0.1337
0.2080
0.0757
0.1336
Insertion (blur) ↑
0.4443
0.7936
0.8507
0.5062
0.3118
Insertion (grey) ↑
0.3056
0.6742
0.9213
0.3518
0.1325
Saliency Metric ↓
0.3149
0.1507
0.3156
0.3157
0.3156"
REFERENCES,0.7689655172413793,Under review as a conference paper at ICLR 2022
REFERENCES,0.7724137931034483,"Figure 11: Results of randomizing the last layer of a ResNet-18 model on ImageNet data for the pro-
cedure described in Section 4. US indicates a (56, 56) map was learned and upsampled to (224, 224).
We ﬁnd the maps of this randomized network are less visually coherent than the analogous maps of
a pre-trained model."
REFERENCES,0.7758620689655172,"Figure 12: Effect of ensembling Partial statistical assignments validating the correct label of Ima-
geNet and ResNet-18 images as we vary K, the number of maps. Details in Appendix C.3.1."
REFERENCES,0.7793103448275862,"Table 10: Performance of our method on ImageNet and ResNet-50 model and some baselines on
various intrinsic saliency metrics proposed in prior work. We ﬁnd that while both our masks (learned
with and without TV) have very good performance on the insertion metric, the mask learned with
TV has much better performance on the saliency metric. The deletion metric is uninformative in
most cases, since most methods are as good (or worse) compared to a random mask."
REFERENCES,0.7827586206896552,"Gradient ⊙Input
Our method
Our Method
Real Time
Random
(λT V = 0.01)
(λT V = 0)
saliency
Deletion ↓
0.1451
0.1832
0.2067
0.1851
0.1843
Insertion (blur) ↑
0.5434
0.8363
0.8673
0.6857
0.3562
Insertion (grey) ↑
0.3716
0.7401
0.8857
0.4873
0.1835
Saliency Metric ↓
0.2549
0.2019
0.2604
0.2943
0.2584"
REFERENCES,0.7862068965517242,"D
ADDITIONAL BACKGROUND INFORMATION"
REFERENCES,0.7896551724137931,"D.1
SALIENCY METHODS"
REFERENCES,0.7931034482758621,Under review as a conference paper at ICLR 2022
REFERENCES,0.7965517241379311,"Figure 13: ImageNet data. Left:
AUC curves as fraction of pixels used varies from 0 to 1.0 for
ground truth label when replacing with grey. Center Left: AUC curves as fraction of pixels used
varies from 0 to 1.0 for ground truth label when replacing with other images. Center Right: AUC
curves as fraction of pixels used varies from 0 to 1.0 for second best label when replacing with
grey. Right: AUC curves as fraction of pixels used varies from 0 to 1.0 for second best label when
replacing with other images."
REFERENCES,0.8,"Backpropagation based explanations shape credit as it is propagated backwards through the neural
network according to certain rules. These approaches include Layerwise Relevance Propagation
(Binder et al., 2016) which satisﬁes completeness, Rect-Grad which thresholds internal neuron
activations (Kim et al., 2019), and DeepLIFT which satisﬁes the summation to delta rule."
REFERENCES,0.803448275862069,"Axiomatic methods.
Axiomatic methods decompose the ouput (typically the logit) according to
certain axioms like fairness in Shapley based methods SHAP (Lundberg & Lee, 2017a) and concept-
SHAP (Yeh et al., 2020). We also include gradient based approaches like Gradient ∂S"
REFERENCES,0.8068965517241379,"∂x (Baehrens
et al., 2010) which calculates the partial derivative of the logit with respect to the input. Gradient
⊙Input (Shrikumar et al., 2017) ∂S"
REFERENCES,0.8103448275862069,"∂x · x, which elementwise multiplies the gradient explanation by
the input, and Grad-CAM (Selvaraju et al., 2019) which takes the gradient of the logit with respect
to the feature map of the last convolutional unit of a DNN. Smooth-Grad (Smilkov et al., 2017),
which averages the Gradient ⊙Input explanation over several noisy copies of the input x + η,
where η is some Gaussian. The previous methods are intrinsic in the sense that they aim to explain
the model decision. The last category of saliency maps, namely masking methods, also aim to ex-
plain the model decision, but they frequently aim to do so in a way that is interpretable by a human.
Contrastive methods, such as contrastive layerwise propagation Gu et al. (2018), also modify LRP
by constructing class speciﬁc saliency maps, with the goal of object localization, i.e. in an image
of an elephant and zebra, the saliency map for elephant should have high overlap with the elephant,
and similarly for the corresponding map for zebra."
REFERENCES,0.8137931034482758,"Masking Methods.
Masking Methods are often evaluated using a pointing game or WSOL metric,
which measures overlap with human labeled bounding boxes or explanations. These masking meth-
ods include techniques based on averaging over randomly sampled masks (Petsiuk et al., 2018), op-
timizing over meaningful mask perturbations (Fong & Vedaldi, 2017), and real time image saliency
using a masking network (Dabkowski & Gal, 2017). Pixels that have been removed from the image
by the mask may be replaced by greying out, by Gaussian blurring as in Fong & Vedaldi (2017),
or with inﬁllers such as CA-GAN (Yu et al., 2018) used in Chang et al. (2018), or DFNet (Hong
et al., 2019). De Cao et al. (2020) ﬁnd masks using differentiable masking. Taghanaki et al. (2019)
introduce a method that results in more accurate localization of discriminatory regions via mutual
information."
REFERENCES,0.8172413793103448,"Pruning and information theory
Khakzar et al. (2019) improve attribution via pruning. Schulz
et al. (2019) improve attribution by adding noise to intermediate feature maps."
REFERENCES,0.8206896551724138,"Saliency and Boolean Logic.
Previous work has also drawn connections between saliency and
notions in logic. Ignatiev et al. (2019b) relates saliency explanations and adversarial examples by a"
REFERENCES,0.8241379310344827,Under review as a conference paper at ICLR 2022
REFERENCES,0.8275862068965517,"generalized form of hitting set duality. Ignatiev et al. (2019a) develops a constraint-agnostic solution
for computing explanations for any ML model. Macdonald et al. (2019) develop a rate distortion
explanation for saliency maps and prove a hardness result. Mu & Andreas (2020) ﬁnd a procedure
for explaining neurons by identifying compositional logical concepts. Zhou et al. (2018) describe
network dissection, which provides labels for the neurons of the hidden representations. We are
unaware of frameworks like Section 3."
REFERENCES,0.8310344827586207,"Arguments about saliency.
For discussion including pro/cons of various methods some starting
points are Seo et al. (2018) Fryer et al. (2021) Gu et al. (2018) Sundararajan & Najmi (2020)."
REFERENCES,0.8344827586206897,"Phang et al. (2020)
We describe separately the masking procedure used by Phang et al. (2020).
They begin by taking a pretrained model on ImageNet. The masker has access to the internal rep-
resentations of the pre-trained model, and tries to maximize masked in accuracy and masked out
entropy. They do not provide the ground truth label to the masker."
REFERENCES,0.8379310344827586,"D.2
SALIENCY EVALUATION METHODS"
REFERENCES,0.8413793103448276,"Saliency evaluation methods attempt to evaluate the quality of a saliency map. Many interpret
the heatmap values as a priority order of saliency. Extrinsic evaluation metrics include the WSOL
metric, which aim to measure overlap of the saliency map with a human annotated bounding box and
the Pointing Game metric proposed by Zhang et al. (2018) in which a pixel count as a hit if it lies
within a bounding box and a miss otherwise, and the metric is
# Hits
# Hits + #Misses. Other more intrinsic
methods include early saliency evaluation techniques like MorF and LerF Samek et al. (2016),
which involve removing pixels either in the order of highest importance or lowest importance and
observing the area of the resulting curve. Insertion and Deletion Games of Petsiuk et al. (2018)
uses this too. The deletion game measures the drop in class probability as important pixels are
removed, while the insertion game measures the rise in class probability as important pixels are
added. (Our AUC discussion in Section 3 relates to this.) Remove and Retrain (ROAR) is a
saliency evaluation method proposed by Hooker et al. (2019). Input features are ranked and then
removed according to a saliency map. A new model is trained on the modiﬁed training set, and
a larger degradation in accuracy on the modiﬁed test set compared to the original model on the
original test set is regarded as a better saliency method. (NB: retraining makes this a non-intrinsic
method.) Previous work has also introduced datasets speciﬁcally designed to test saliency methods.
BAM Yang & Kim (2019) creates saliency maps by pasting object pixels from MSCOCO Lin et al.
(2014) into scene images from MiniPlaces Zhou et al. (2017). The Saliency Metric proposed by
Dabkowski & Gal (2017) thresholds saliency values above some α chosen on a holdout set, ﬁnds
the smallest bounding box containing these pixels, upsamples and measures the ratio of bounding
box area to model accuracy on the cropped image, s(a, p) = log(max(a, 0.05)) −log(p) where a is
the area of the bounding box and p is the class probability of the upsampled image."
REFERENCES,0.8448275862068966,"Controversies.
There is extensive discussion of validity of saliency evaluation methods;
e.g., Brunke et al. (2020)Petsiuk et al. (2018)."
REFERENCES,0.8482758620689655,"D.3
SALIENCY COMPUTATIONS AND UNDERLYING MEANINGS OF SALIENCY"
REFERENCES,0.8517241379310345,"For simplicity this discussion assumes the datapoints are images and the classiﬁer is a deep net. The
heatmap in the saliency method is trying to highlight the contribution of individual pixels to the ﬁnal
answer. This is analogous to how a human may highlight relevant portions of the image with a plan.
(Classic saliency methods in vision are inspired by studies of human cognition.) Saliency methods
operationalize this intuitive deﬁnition in different ways, and we try to roughly categorise these as
follows."
REFERENCES,0.8551724137931035,"Variational interpretation.
These interpret saliency in terms of effect on ﬁnal output due to
change in a single pixel –captured either via partial derivative of output with respect to pixel value
(i.e., effect of inﬁnitesimal change), or via change of output when this pixel is set to 0 or to a ran-
dom (or ”gray”) value. Examples include Gradient, Gradient ⊙Input Shrikumar et al. (2017),
Occlusion"
REFERENCES,0.8586206896551725,"Credit attribution guided by gradient.
These use the gradient to guide the assignment of saliency
values. The gradient is interpreted as propagating values from the output to the input layer, and the
values are partitioned/recombined at internal nodes of the net following some conservation princi-"
REFERENCES,0.8620689655172413,Under review as a conference paper at ICLR 2022
REFERENCES,0.8655172413793103,"ples. A key goal is to ensure completeness, which means that the sum of the attributions equal the
logit value. Examples include LRP, DeepLIFT Shrikumar et al. (2017), Rect-Grad Let al
i be the
activation of some node in layer l, and Rl+1
i
be the backpropagated gradient up to al
i. Rect-grad
replaces the vanilla chain rule, Rl = 1[ai > 0] with the rule that Rl
i = 1[Rl+1
i
ai > τ] for some
threshold τ. Hence, during a backward pass preference is given to nodes with large margin."
REFERENCES,0.8689655172413793,"Ensembling on top of above two ideas.
Ensembling methods combine saliency estimates over
multiple inputs an an attempt to reduce noise in the ﬁnal map. Examples include Smooth-Grad,
Occlusion based methods, etc. We also include Shapley Values in this list."
REFERENCES,0.8724137931034482,"The Shapley value aims to fairly distribute credit among a coalition of N players. In the context
of image saliency, each coordinate of the image input may be seen as a player, and the Shapley
value computes P"
REFERENCES,0.8758620689655172,S⊆N \{i} |S|!(n−|S|−1)!
REFERENCES,0.8793103448275862,"n!
(v(S ∪{i})−v(S)). It can be interpreted as the marginal
contribution of player i, over all possible orderings of the coalition. In this sense, it can be seen as
an ensembling method, as it averages over all possible random permutations."
REFERENCES,0.8827586206896552,"Analysis of saliency methods. Previous work has analyzed ensembling methods like Smooth-grad,
and found that it does not smooth the gradient Seo et al. (2018). They conclude that Smooth-Grad
does not make the gradient of the score function smooth. Rather Smooth-grad is approximately the
sum of a standard saliency map and higher order terms and the standard deviation of the Gaussian
noise. It has also been found that Shapley values, despite having a uniqueness result, can differ
in the way they depend on the model, data, etc Sundararajan & Najmi (2020). Fryer et al. (2021)
highlight several nuances that should be taken into account when considering Shapley values. They
introduce Shapley values as averaging over submodels, and note that ”the performance of a feature
across all submodels may not be indicative of the particular performance of that feature in the set
of optimal submodels.”. They provide speciﬁc cases where satisfying the axioms of Shapley values
works against the goal of feature selection."
REFERENCES,0.8862068965517241,"E
CLARIFYING BENEFIT OF TV REGULARIZATION"
REFERENCES,0.8896551724137931,"This section illustrates Deﬁnitions 3.1 and 3.2 using linear classiﬁers. It also shows how TV regu-
larizers help ensure soundness even in this setting."
REFERENCES,0.8931034482758621,"Let S be a dataset of labeled data (x, y) where the inputs are of unit norm and labels are binary,
i.e., ∥x∥2 = 1, y ∈{±1}. The model in question is a linear classiﬁer f(x) := sgn(⟨w, x⟩)
parameterized by the weight vector w ∈Sd−1, and it achieves the perfect accuracy on the set S with
a margin γ := min(x,y)∈S y ⟨w, x⟩> 0. We assume that the coordinates of x and w are uniformly
bounded by 10
√"
REFERENCES,0.896551724137931,"d, i.e., ∥x∥∞≤10
√"
REFERENCES,0.9,"d, ∥w∥∞≤10
√"
REFERENCES,0.903448275862069,d (10 can be changed to any other constant).
REFERENCES,0.906896551724138,"Let Γ be the input modiﬁcation process that sets all non-salient pixels to 0. We are interested in
masking explanations with deterministic salient set, i.e., ∆assigns probability 1 to some salient set
S. According to Section 3.1, a masking explanation validates label a if E˜x∼Γ(x,∆)[1[f(˜x)=a]] is
high. A simple calculation shows that this expectation equals to 1[a P"
REFERENCES,0.9103448275862069,"i∈S wixi>0], and thus the goal
is to ﬁnd S so that a P"
REFERENCES,0.9137931034482759,i∈S wixi > 0.
REFERENCES,0.9172413793103448,"As we do not consider the full salient set informative, we are interested in masking explanations
with size constraint |S| = L for some 1 ≤L ≤d. There is a simple saliency method that achieves
this goal: Given an input x and a label a ∈{±1}, sort the coordinates according to awixi and take
the highest L coordinates as the salient set S."
REFERENCES,0.9206896551724137,It is easy to see that this method always produces S with a P
REFERENCES,0.9241379310344827,"i∈S wixi > 0. Letting a = y proves the
completeness. However, this method does not satisfy soundness: a salient set S with a P"
REFERENCES,0.9275862068965517,"i∈S wixi >
0 can also be found for a ̸= y!"
REFERENCES,0.9310344827586207,"Now we see how the TV constraint helps to ensure soundness (with good probability). A vec-
tor can be seen as a 1D image, and the TV of a salient set S can be deﬁned by TV(S) :=
Pd−1
i=1
1[i∈S] −1[i+1∈S]
. For simplicity, we consider salient sets with TV at most 2. This means
S is just an interval. Given the size and TV constraints |S| = L, TV(S) ≤2, it is easy to come
out with the following saliency method: search over all the intervals of length L and if an interval"
REFERENCES,0.9344827586206896,Under review as a conference paper at ICLR 2022
REFERENCES,0.9379310344827586,S satisﬁes a P
REFERENCES,0.9413793103448276,"i∈S wixi > 0, return it as the salient set. Fortunately, this method does satisfy both
completeness and soundness, which is justiﬁed by the following theorem."
REFERENCES,0.9448275862068966,"Theorem E.1. For any (x, y) ∈S, if we shufﬂe the coordinates of w and those of x according to
the same random permutation, then"
REFERENCES,0.9482758620689655,1. For L = Ω( 1
REFERENCES,0.9517241379310345,γ2 log 1
REFERENCES,0.9551724137931035,"δ ), with probability 1 −δ, there is an interval of length L that validates
y;"
REFERENCES,0.9586206896551724,2. For L = Ω( 1
REFERENCES,0.9620689655172414,γ2 log d
REFERENCES,0.9655172413793104,"δ ), with probability 1 −δ, no interval of length L can validate −y."
REFERENCES,0.9689655172413794,"Proof. Let S be any ﬁxed interval of length L, then the distribution of P"
REFERENCES,0.9724137931034482,"i∈S wixi is identical to
the distribution of the sum of L i.i.d. random variables drawn from {w1x1, . . . , wdxd} without
replacement. Note that dyw1x1, . . . , dywdxd are d numbers with mean γ, and their absolute values
are bounded by 102 = O(1). By Chernoff bound, Pr ""
1
L X"
REFERENCES,0.9758620689655172,"i∈S
dywixi −γ < −ϵ #"
REFERENCES,0.9793103448275862,≤e−O(ϵ2|S|).
REFERENCES,0.9827586206896551,Set ϵ = γ ensures that y P
REFERENCES,0.9862068965517241,"i∈S wixi > 0 with probability e−O(γ2|S|). We can ﬁx any interval S
to prove Item 1. Taking union bounds over all intervals of length L, the probability of existing an
interval of length L that validates −y should be no greater than P"
REFERENCES,0.9896551724137931,"|S|=L e−O(ϵ2L) ≤d2e−O(γ2L),
which proves Item 2."
REFERENCES,0.993103448275862,"This shows that such masking explanations make sense to humans: if the model predicts y, then we
can ﬁnd an interval of length ˜Ω(1/γ2) so that computing the inner product only in that interval leads
to the same prediction; otherwise if the model does not predict y, such interval cannot be found.
Thus it is sufﬁcient to convince humans that the model predicts y by only revealing the existence of
such interval and the coordinate values in it."
REFERENCES,0.996551724137931,"Although the example given in this section is simple, the conceptual message is clear: saliency
methods may not guarantee soundness in itself, but adding regularity constraints such as TV can
mitigate this soundness issue effectively."
