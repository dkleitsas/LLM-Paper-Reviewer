Section,Section Appearance Order,Paragraph
ZHEJIANG UNIVERSITY,0.0,"1Zhejiang University
2Shanghai Artificial Intelligence Laboratory
3Huawei Noah‚Äôs Ark Lab
xiayan.zju@gmail.com
haihuangcode@outlook.com
jiemingzhu@ieee.org
zhouzhao@zju.edu.cn"
ABSTRACT,0.005649717514124294,Abstract
ABSTRACT,0.011299435028248588,"This paper introduces a novel task called Cross Modal Generalization (CMG),
which addresses the challenge of learning a unified discrete representation from
paired multimodal data during pre-training. Then in downstream tasks, the model
can achieve zero-shot generalization ability in other modalities when only one
modal is labeled. Existing approaches in multimodal representation learning fo-
cus more on coarse-grained alignment or rely on the assumption that information
from different modalities is completely aligned, which is impractical in real-world
scenarios. To overcome this limitation, we propose Uni-Code, which contains
two key contributions: the Dual Cross-modal Information Disentangling (DCID)
module and the Multi-Modal Exponential Moving Average (MM-EMA). These
methods facilitate bidirectional supervision between modalities and align semanti-
cally equivalent information in a shared discrete latent space, enabling fine-grained
unified representation of multimodal sequences. During pre-training, we investi-
gate various modality combinations, including audio-visual, audio-text, and the
tri-modal combination of audio-visual-text. Extensive experiments on various
downstream tasks, i.e., cross-modal event classification, localization, cross-modal
retrieval, query-based video segmentation, and cross-dataset event localization,
demonstrate the effectiveness of our proposed methods. The code is available at
https://github.com/haihuangcode/CMG."
INTRODUCTION,0.01694915254237288,"1
Introduction"
INTRODUCTION,0.022598870056497175,"Although recent years have witnessed significant achievements in many multimodal areas, e.g.,
multi-modality question-answering [1, 2, 3], query-based segmentation [4, 5], audio-visual event
localization [6, 7], labeling these tasks always consumes extensive human resources. Moreover,
the labeling cost for different modalities can vary significantly [8], leading to scenarios where
only a subset of modalities is labeled, while others remain scarce [9]. For example, text-based
visual segmentation is common while the label data for audio-based visual segmentation is rare.
Consequently, models trained on such limited data are restricted to specific scenarios, hindering their
broader applicability. Fortunately, unannotated paired multimodal data is readily accessible, such
as the abundant image-caption [10] and audio-visual pairs [11] available on the Internet. Hence, in
this paper, we develop a new task, Cross-Modal Generalization (CMG), for investigating how to
learn unified discrete representations from these unlabelled multi-modal data pairs. Our objective is
to transfer the knowledge acquired from labeled modalities to other unseen modalities in downstream
tasks, enabling models to generalize effectively."
INTRODUCTION,0.02824858757062147,"‚àóEqual Contribution
‚Ä†Corresponding Author"
INTRODUCTION,0.03389830508474576,"UniÔ¨Åed Discrete Encoder
‚ùÑ"
INTRODUCTION,0.03954802259887006,"Task SpeciÔ¨Åc Decoder
üî•"
INTRODUCTION,0.04519774011299435,"UniÔ¨Åed Discrete Encoder
‚ùÑ
Task SpeciÔ¨Åc Decoder
‚ùÑ"
INTRODUCTION,0.05084745762711865,Downstream Task Training
INTRODUCTION,0.05649717514124294,Downstream Task Inference
INTRODUCTION,0.062146892655367235,"Cross Modal 
Generaliza9on"
INTRODUCTION,0.06779661016949153,Woman Speaking /Dog Barking /Dog Barking / Woman Speaking / Background
INTRODUCTION,0.07344632768361582,Cat Meowing / Dog Barking / Dog Barking / Dog Barking / Background
INTRODUCTION,0.07909604519774012,"Modality A
Modality B"
INTRODUCTION,0.0847457627118644,"Modality C
VQ code"
INTRODUCTION,0.0903954802259887,"Good UniÔ¨Åed 
Representa9on"
INTRODUCTION,0.096045197740113,"Modality A
Modality B"
INTRODUCTION,0.1016949152542373,"Modality C
VQ code"
INTRODUCTION,0.10734463276836158,"Bad UniÔ¨Åed 
Representa9on"
INTRODUCTION,0.11299435028248588,"Figure 1: The overview of our proposed CMG task, different colors in the left and middle parts mean
different semantics. The left part is the illustration of bad multi-modal unified representation, where
features from various modalities sharing the same semantic meaning are mapped into disparate latent
codes, while the good unified representation (middle part) is totally different. The right part shows
that in downstream tasks, the model will be directly transferred to unseen modalities."
INTRODUCTION,0.11864406779661017,"Humans possess a natural ability to associate different modalities with similar semantics, facilitating
cross-modal knowledge transfer based on known modalities. Inspired by this, numerous studies have
explored the integration of diverse multimodal information into a unified semantic space, which
can be categorized into two types: implicit representations [12, 13, 14] and explicit representations
[15, 16, 17]. For implicit representations, several methods utilize a modality-agnostic encoder [12, 13]
to represent different modalities, or employ contrastive learning [18, 19] to bring different modalities
closer in high-dimensional semantic space. In contrast, explicit representations aim to use a unified
codebook [17] or prototype [16] to represent different modalities, serving as a bridge to facilitate
robust alignment across modalities. Furthermore, the use of discrete space allows for the aggregation
of similar input features in a high-dimensional space, enabling complex feature representation with a
reduced number of latent codes. However, these studies predominantly compress sequential features
within modalities into single vectors before quantization [16, 20] or rely on the assumption that
information from different modalities is completely aligned [17, 21]. As a result, these methods are
typically limited to simple tasks such as cross-modal retrieval and demonstrate poor performance in
more complex scenarios."
INTRODUCTION,0.12429378531073447,"As shown in Fig 1, the visual information in unconstrained videos comprises events of cat meowing
and dog barking, while the audio information comprises events of woman speaking and dog barking.
Directly applying previous methods [13, 17, 16, 20] would lead to inaccurate mapping of multimodal
information lacking shared semantics. To address this limitation, our paper primarily focuses on
the implementation of a fine-grained, unified representation for multimodal sequences. We address
this novel problem from two key aspects: 1) extracting information with identical semantics across
different modalities while mitigating the influence of modality-specific details, and 2) representing
these diverse modalities with shared semantics using a unified codebook. Most of the previous works
primarily focus on the second aspect while neglecting the importance of the first aspect, which we
argue is crucial for achieving cross-modal generalization."
INTRODUCTION,0.12994350282485875,"Our work builds upon the advantages of explicit representations and proposes two modules to address
the aforementioned challenges. To address the first aspect, we propose Dual Cross-modal Information
Disentangling (DCID) module, which combines Contrastive Log-ratio Upper Bound (CLUB) [22]
with our proposed Cross-modal Contrastive Predictive Coding [23] (Cross-CPC). Concretely, we
employ CLUB to optimize the upper bound of mutual information, efficiently distinguishing the
modal-agnostic semantic information (conveying primary content or events) from the modal-specific
information (additional details unrelated to semantics, such as light and perspective in vision, or
timbre and pitch in audio, etc.) across various modalities. However, without effective guidance,
it is challenging for the model to identify which aspects of semantic information are useful [24].
Motivated by the complementary guidance nature of multi-modal information [25], we propose a
novel Cross-CPC method, to predict the future information in other modalities based on the known
sequence information of the current modality, which can effectively maximize the fine-grained mutual
information between different modalities. To address the second aspects, we propose Multi-Modal
Exponential Moving Average (MM-EMA), which employs a teacher-student mechanism to facilitate"
INTRODUCTION,0.13559322033898305,"cross-modal guidance during pretraining. This mechanism encourages the aggregation of quantized
vectors encoded from different modalities with shared semantics into the same latent code space.
Otherwise, distinct modalities would aggregate in separate regions of the discrete space, rather than
being mapped together [20, 17]. To summarize, our main contributions are threefold:"
INTRODUCTION,0.14124293785310735,"‚Ä¢ We introduce a new task, CMG, which enhances the applicability of existing models in the
realm of multimodal learning, addressing the challenges arising from modality scarcity and
the substantial cost of annotating certain modalities."
INTRODUCTION,0.14689265536723164,"‚Ä¢ We propose a novel framework named Uni-Code, which can effectively extract shared se-
mantic information from paired multi-modal data and project them into a common quantized
latent space in a fine-grained level."
INTRODUCTION,0.15254237288135594,"‚Ä¢ We investigate the unified representation of various modalities, including audio-visual,
audio-text, visual-text, and even the challenging tri-modal combination of audio-visual-text.
Extensive experiments on various downstream tasks, e.g., multimodal event classification,
localization, cross modal retrieval and video segmentation, demonstrate the effectiveness of
our proposed methods."
RELATED WORK,0.15819209039548024,"2
Related Work"
RELATED WORK,0.1638418079096045,"Implicit Multi-Modal Unified Representation. The past few years have witnessed remarkable
achievements in the implicit multi-modal unified representation, which aims to align diverse modali-
ties within a shared latent space [26, 27, 14] or try to learn a modality-agnostic encoder for extracting
information across various modal [28, 29]. These methods investigate the unified representation for
various modality combinations, i.e., Speech-Text [26, 30], Video-Audio [31, 32, 27, 33], Vision-Text
[18, 14, 29]. To achieve these, different kinds of methods have been proposed. Pedersoli et al. [31]
and Sarkar et al. [27] introduce a cross-modal knowledge distillation to transfer knowledge across
modalities. CLIP-based methods [18, 14] use contrastive loss to learn image-text consistency from a
large paired dataset, and have gained incredible zero-shot ability in various downstream tasks."
RELATED WORK,0.1694915254237288,"Explicit Multi-Modal Unified Representation. Recently, some works investigate how to achieve
multimodal explicit unified representation by utilizing a universal codebook [15, 17, 20] or prototype
[16] to explicitly express multimodal content. Duan et al. [16] apply Optimal Transport to map
the feature vectors extracted from different modalities to the prototypes. Zhao et al. [20] use self-
cross-reconstruction to enhance the mutual information between different modals. However, these
coarse-grained alignment methods [16, 20] are only suitable for simple tasks such as retrieval, and
are unable to accomplish fine-grained comprehension in downstream tasks. In recent years, [34, 21]
align the text and speech temporally with a unified discrete space. Liu et al. [17] use the same scheme
to align short videos and speech/text. However, a key assumption underlying their success is that
the modalities they chose have a one-to-one alignment (i.e. text-to-speech), while in more general
case, it is hard to guarantee the semantic information in the two paired modalities is completely
consistent (i.e. unconstrained video and audio). In this paper, we mainly investigate how to map a
paired multimodal sequence into a common discrete semantic space, where the information in the
multimodal sequence is unconstrained and may not be perfectly aligned."
RELATED WORK,0.1751412429378531,"Mutual Information Estimation. Mutual information (MI) estimation aims to measure the depen-
dence between two random variables. Recent works primarily concentrate on how to combine MI
estimation with deep neural networks [35, 22, 23], including MI maximization and minimization. MI
maximization aims to learn representations that capture meaningful and useful information about the
input data, leading to improved performance in various downstream tasks [36, 37, 38]. In order to
maximize mutual information in a sequence, Contrastive Predictive Coding (CPC) [23] employs an
autoregressive model and contrastive estimation to capture long-term relations while maintaining
local features within a sequence. MI minimization tries to reduce the dependency between two
random variables while preserving the relevant information, has been successfully applied in disen-
tangled representation learning [39, 24]. Belghaz et al. [35] propose a Mutual Information Neural
Estimator (MINE), which builds a neural network to estimate mutual information based on dual
representations of the KL-divergence [40, 41]. Chen et al. [22] introduce Contrastive Log-ratio Upper
Bound (CLUB), combining MI estimation with contrastive learning to approximate the MI upper
bound. Their work is more suitable for MI minimization."
RELATED WORK,0.1807909604519774,"(a)
(b) (c)"
RELATED WORK,0.1864406779661017,"Figure 2: The overview of our proposed Uni-Code framework, we use audio-visual as an example. (a)
The main pipeline of our model. (b) The process of Multi-modal Vector Quantization, which contains
Multi-modal EMA (MM-EMA) and a new commitment loss, notice that the abundant discrete codes
will be reset. (c) The architecture of our proposed Cross-CPC."
CROSS MODAL GENERALIZATION TASK,0.192090395480226,"3
Cross Modal Generalization Task"
CROSS MODAL GENERALIZATION TASK,0.1977401129943503,"Given a set of paired multi-modal data X = {(xA
i , xB
i , xC
i ...)}N
i=1 of size N, where A, B, C, and so
on represent different modalities, the Cross Modal Generalization (CMG) task aims to map these
various modalities into a unified discrete space during the pre-training phase, enabling discrete latent
codes to be shared among different modalities with the same semantic. Subsequently, in downstream
tasks, when only one modality (e.g., mode A) has annotated information, the model can transfer
knowledge learned from A mode to other modalities (e.g., mode B and C) based on the shared discrete
space obtained during pre-training to achieve zero-shot generalization ability."
UNIFIED REPRESENTATION LEARNING,0.2033898305084746,"4
Unified Representation Learning"
UNIFIED REPRESENTATION LEARNING,0.20903954802259886,"Different from previous works which simply extract information from paired modalities and then
direct maps, we argue that the success of unified representation lies in the extraction of modality-
agnostic semantic features. Thus in this paper, we achieve this from two perspectives: first, we
introduce a DCID module, designed to extract fine-grained semantic information and separate it from
the corresponding modality-specific information within each modality. Second, we compress the
extracted semantic features into discrete variables through VQ-VAE, ensuring that the compressed
discrete variables can still contain the original semantic information through reconstruction loss. For
simplicity, we take two modalities as an example to illustrate the process. Fig 2 gives an overview
illustration of our network."
BASELINE,0.21468926553672316,"4.1
Baseline"
BASELINE,0.22033898305084745,"Given two paired modals, {(xa
i , xb
i)}N
i=1, we first utilize two semantic encoders Œ¶a and Œ¶b to extract
modal-agnostic features za
i , zb
i ‚ààRT √óD, and use two modal-specific encoders Œ®a and Œ®b to extract
remain features ¬Øza
i ,¬Øzb
i from modality A and B, respectively:"
BASELINE,0.22598870056497175,"za
i = Œ¶a(xa
i )
¬Øza
i = Œ®a(xa
i )
zb
i = Œ¶b(xb
i)
¬Øzb
i = Œ®b(xb
i),
(1)"
BASELINE,0.23163841807909605,"where T, D represent time and hidden dimension, respectively. The dimension of ¬Øza
i , ¬Øzb
i various for
different modalities. Then we apply vector quantized operation to map the semantic features za
i,t, zb
i,t
to discrete latent codes in fine-grained level, t ‚àà[0, T]:"
BASELINE,0.23728813559322035,"ÀÜzm
i,t = V Q(Œ¶m(xm
i,t)) = V Q(zm
i,t) = el,
where
l = argminj‚à•Œ¶(x) ‚àíej‚à•2,
m ‚àà{a, b}, (2)"
BASELINE,0.24293785310734464,"the latent codebook e ‚ààRL√óD is shared across modality A and B, where L is the the size of the
discrete latent space. Finally, we combine ÀÜzm
i with ¬Øzm
i together to reconstruct original features:"
BASELINE,0.24858757062146894,"L = ‚à•xm
i ‚àíD(ÀÜzm
i ; ¬Øzm
i )‚à•2
2
|
{z
}
reconstruction loss"
BASELINE,0.2542372881355932,"+ ‚à•sg[œïm(xm
i )] ‚àíe‚à•2
2
|
{z
}
VQ loss"
BASELINE,0.2598870056497175,"+ Œ≤‚à•œïm(xm
i ) ‚àísg[e]‚à•2
2
|
{z
}
commitment loss"
BASELINE,0.2655367231638418,",
m ‚àà{a, b}.
(3)"
BASELINE,0.2711864406779661,"where Œ≤ is 0.25 for all our experiments, sg is stop gradient. In this work, we use Exponential Moving
Average (EMA) to replace VQ loss, since EMA is more robust. The reconstruction loss can guarantee
that the compressed latent codes el still maintain the semantic information of different modalities.
In ideal conditions, za
i and zb
i encoded from different modalities with the same semantics should be
mapped to the same discrete latent code. However, without effective supervision, the existence of
modality gap will result in za
i and zb
i converging to separate regions of the codebook [20, 17]. Thus
in this paper, we introduce the following modules to alleviate these problems."
DUAL CROSS-MODAL INFORMATION DISENTANGLING,0.2768361581920904,"4.2
Dual Cross-modal Information Disentangling"
DUAL CROSS-MODAL INFORMATION DISENTANGLING,0.2824858757062147,"We introduce our DCID module from two aspects: MI minimization between modal-agnostic semantic
features and modal-specific features in each modality (CLUB), and MI maximization between modal-
agnostic semantic features across different modalities (Cross-CPC)."
DUAL CROSS-MODAL INFORMATION DISENTANGLING,0.288135593220339,"CLUB-based MI Minimization: Compared to the methods that try to optimize the MI lower bound,
such as InfoNCE [23] and MINE [35], CLUB [22] can effectively optimize the MI upper bound,
demonstrating superior advantages in information disentanglement [24]. Given two variables x and
y, the objective function of CLUB is defined as:"
DUAL CROSS-MODAL INFORMATION DISENTANGLING,0.2937853107344633,"IvCLUB(x; y) := Ep(x,y)[log qŒ∏(y|x)] ‚àíEp(x)Ep(y)[log qŒ∏(y|x)],
(4)"
DUAL CROSS-MODAL INFORMATION DISENTANGLING,0.2994350282485876,"where qŒ∏ is the variational approximation of ground-truth posterior of y given x and can be parame-
terized by a network Œ∏. We use CLUB to optimize the MI upper bound between the semantic features
zm
i and modal-specific features ¬Øzm
i , m ‚àà{a, b}, we modify IvCLUB into temporal version:"
DUAL CROSS-MODAL INFORMATION DISENTANGLING,0.3050847457627119,"ÀÜIvCLUB = 1 N N
X i=1 h 1 T T
X"
DUAL CROSS-MODAL INFORMATION DISENTANGLING,0.3107344632768362,"t=1
log qŒ∏(¬Øzm
i |zm
i ) ‚àí1 N
1
T N
X j=1 T
X"
DUAL CROSS-MODAL INFORMATION DISENTANGLING,0.3163841807909605,"t=1
log qŒ∏(¬Øzm
j |zm
i )
i
,
m ‚àà{a, b}
(5)"
DUAL CROSS-MODAL INFORMATION DISENTANGLING,0.3220338983050847,"The approximation network and the main networks are optimized alternatively during pre-training.
Finally, we can reduce the correlation between semantic information and modal-specific information
in each modality. Nevertheless, merely minimizing mutual information using CLUB presents
a challenge for the model to identify relevant semantic features. Given that paired multimodal
information can provide mutual guidance and serve as supervisory signals for each other, we devise
the Cross-CPC approach to alleviate this issue."
DUAL CROSS-MODAL INFORMATION DISENTANGLING,0.327683615819209,"MI Maximization with Cross-CPC: Contrastive Predictive Coding (CPC) [23] can maximize the
mutual information among the adjacent items within a sequence by predicting the future samples with
powerful autoregressive models, has been widely used in self-supervised learning. While for human,
we can not only predict subsequent scenarios based on the current modality, but are also capable of
associating with potential situations that may occur in other modalities. For instance, people can infer
forthcoming auditory information based on a presently viewed video segment or read text, or envision
a subsequent scene by perceiving an audio portion. Inspired by this, in this paper, we extend CPC
to cross-modal constrictive prediction. Given the semantic features za, zb ‚ààRT √óD, a prediction
of K steps and a random time moment t ‚àà(0, T-K], we first use two single layers unidirectional
LSTM to summarize the information of all za
‚â§t, zb
‚â§t and can obtain two context representations as
cm
t = LSTM(zm
‚â§t) ‚ààRD, m ‚àà{a, b}."
DUAL CROSS-MODAL INFORMATION DISENTANGLING,0.3333333333333333,"For modality A, we first select a set Zb of N-1 random negative samples and one positive sample zb
t+k
from modality B, then we use ca
t to predict k-th future step zb
t+k in modality B, and the InfoNCE
loss for modality A can be optimized as:"
DUAL CROSS-MODAL INFORMATION DISENTANGLING,0.3389830508474576,"La2b
cpc = ‚àí1 K K
X"
DUAL CROSS-MODAL INFORMATION DISENTANGLING,0.3446327683615819,"k=1
log
h
exp(zb
t+kW a
k ca
t )
P"
DUAL CROSS-MODAL INFORMATION DISENTANGLING,0.3502824858757062,"zj‚ààZb exp(zb
jW a
k ca
t )"
DUAL CROSS-MODAL INFORMATION DISENTANGLING,0.3559322033898305,"i
;
Lb2a
cpc = ‚àí1 K K
X"
DUAL CROSS-MODAL INFORMATION DISENTANGLING,0.3615819209039548,"k=1
log
h
exp(za
t+kW b
kcb
t)
P"
DUAL CROSS-MODAL INFORMATION DISENTANGLING,0.3672316384180791,"zj‚ààZa exp(za
j W b
kcb
t) i
,"
DUAL CROSS-MODAL INFORMATION DISENTANGLING,0.3728813559322034,"(6)
where the W a
k is the linear projection matrix for different step k. The optimization Lb2a
cpc for modality
B is vice versa. Based on these, features with the same semantics across different modalities can be
mutually extracted through fine-grained cross-modal prediction. Despite it is difficult to align paired
modalities perfectly in unrestricted scenarios, the model can still predict the possible information in
the next step of the other modality at the fine-grained level through contrastive learning, due to the
summarization of the historical information using an autoregressive model."
MULTI-MODAL EXPONENTIAL MOVING AVERAGE,0.3785310734463277,"4.3
Multi-modal Exponential Moving Average"
MULTI-MODAL EXPONENTIAL MOVING AVERAGE,0.384180790960452,"Previous works [20, 16, 17] struggle to achieve fine-grained cross-modal alignment in unconstrained
scenarios, to tackle their limitations, we propose MM-EMA, which can allow different modalities to
serve as teacher-student iteratively and update each other during the quantization process."
MULTI-MODAL EXPONENTIAL MOVING AVERAGE,0.3898305084745763,"First, we use cross-attention [42] to extract related information from the opposite modality, taking
mode A as an example: rb
i = cross-att(za
i ; zb
i; zb
i), where za
i is query, zb
i is key and value. The vector
rb
i contains the semantic information derived from zb
i, exhibiting a strong correlation with za
i , while
also preserving the intrinsic attributes of modality B. Thus, it can serve as an intermediary facilitating
the alignment of modality A with modality B during EMA procedure."
MULTI-MODAL EXPONENTIAL MOVING AVERAGE,0.3954802259887006,"Given a code vector ei, we can obtain na
i semantic vectors of modality A {za
i,j}na
i
j=1 and nb
i semantic"
MULTI-MODAL EXPONENTIAL MOVING AVERAGE,0.4011299435028249,"vectors of modality B {zb
i,j}nb
i
j=1 that are quantized to ei:"
MULTI-MODAL EXPONENTIAL MOVING AVERAGE,0.4067796610169492,"N (t)
i
= Œ≥N (t‚àí1)
i
+ (1 ‚àíŒ≥)[na(t)
i
+ nb(t)
i
]
e(t)
i
= o(t)
i /N (t)
i
(7)"
MULTI-MODAL EXPONENTIAL MOVING AVERAGE,0.4124293785310734,"o(t)
i
= Œ≥o(t‚àí1)
i
+ (1 ‚àíŒ≥)
h na(t)
iX j=1"
MULTI-MODAL EXPONENTIAL MOVING AVERAGE,0.4180790960451977,"za(t)
i,j + rb(t)
i,j
2
+"
MULTI-MODAL EXPONENTIAL MOVING AVERAGE,0.423728813559322,"nb(t)
iX j=1"
MULTI-MODAL EXPONENTIAL MOVING AVERAGE,0.4293785310734463,"zb(t)
i,j + ra(t)
i,j
2 i
,"
MULTI-MODAL EXPONENTIAL MOVING AVERAGE,0.4350282485875706,"where t is batch sequence in order, Ni and oi are accumulated vector count and volume, respectively.
Besides, we also modify the commitment loss in Eq 3, which can use the code vector eb
i (quantized
from zb
i) as a teacher, to guide the Encoder œïa towards not only approximating ea
i , but also converging
to eb
i with a certain ratio (50% in our setting, Œ≤ is the same as in Eq 3)."
MULTI-MODAL EXPONENTIAL MOVING AVERAGE,0.4406779661016949,"La
commit = Œ≤‚à•œïa(xa
i ) ‚àísg[ea
i ]‚à•2
2 + Œ≤"
MULTI-MODAL EXPONENTIAL MOVING AVERAGE,0.4463276836158192,"2 ‚à•œïa(xa
i ) ‚àísg[eb
i]‚à•2
2
(8)"
MULTI-MODAL EXPONENTIAL MOVING AVERAGE,0.4519774011299435,"The modified commitment loss Lb
commit is also the same process with modality A. During training,
the distance between different modalities in the latent space gradually decreases."
MULTI-MODAL EXPONENTIAL MOVING AVERAGE,0.4576271186440678,"Reset inactivated code: In order to alleviate the codebook collapse problem existing in VQ process,
which implies that only a subset of codes are extensively utilized while the majority of codes remain
inactive, we adopt a code reset strategy: for a given code vector ei, it will be retained only if it has
been selected at least once by both modality A and modality B in Nre consecutive batches, whereas
the remaining codes will be re-initialized. This strategy can effectively mitigate the issue of some
code being perpetually unexploitable due to poor initialization, as well as the problem of generating
redundant code during the process of aligning modalities A and B (see appendix for more details)."
TRAINING AND DOWNSTREAM TASKS,0.4632768361581921,"4.4
Training and Downstream Tasks"
TRAINING AND DOWNSTREAM TASKS,0.4689265536723164,"The full objective of our pre-training framework is the combination of all above objection functions:
L = Lrecon + Lcommit + Lcpc + Lcmcm + LMI, where LMI is ÀÜIvCLUB, Lcmcm is the objective loss
proposed in [17], which can also promote the alignment among modalities. All these objectives
except Lcmcm integrate two modalities. After training, we can obtain a unified discrete latent space."
TRAINING AND DOWNSTREAM TASKS,0.4745762711864407,"In this paper, we investigate the unified representation of various modalities, including audio-visual,
audio-text, visual-text, and even more, the tri-modal combination of audio-visual-text. Then we
can apply the pre-trained multi-modal encoder in various downstream tasks, i.e., cross-modal event
classification, event localization, query-based video segmentation and cross both domain & modal
event localization. Note that during downstream tasks, the parameters of the pre-trained encoder are
frozen."
EXPERIMENTS,0.480225988700565,"5
Experiments"
DATASETS AND TASKS,0.4858757062146893,"5.1
Datasets and Tasks"
DATASETS AND TASKS,0.4915254237288136,"Pre-train: We use VGGsound-AVEL [44, 45] to pre-train our unified representation, and divide it
into several different sizes: 24K, 40K, 81K. Considering that the VGGsound-AVEL dataset only
contains audio, video and event label, we design prompts for these labels and modify them into
descriptive sentences, see Appendix for more details."
DATASETS AND TASKS,0.4971751412429379,"Table 1: Compared with state-of-the-art methods on two downstream tasks. We use precision to
indict the performance of the models on AVE tasks, and use accuracy for AVVP tasks."
DATASETS AND TASKS,0.5028248587570622,"Method
VGGsounds-AVEL 24K
VGGsounds-AVEL 40K
VGGsounds-AVEL 81K
AVE
V‚ÜíA A‚ÜíV
AVVP
V‚ÜíA A‚ÜíV
AVE
V‚ÜíA A‚ÜíV
AVVP
V‚ÜíA A‚ÜíV
AVE
V‚ÜíA A‚ÜíV
AVVP
V‚ÜíA A‚ÜíV
Baseline
4.4
5.9
7.6
8.4
5.5
5.4
6.9
8.7
7.1
9.3
5.6
7.2
S-Mit[43]
12.7
16.9
17.2
22.8
14.4
15.9
19.0
22.3
13.4
17.0
20.9
22.8
MST[13]
13.3
19.0
25.7
29.1
19.5
23.1
22.7
24.5
18.6
20.5
19.1
24.8
CODIS[16]
18.5
22.0
29.4
33.7
20.8
26.4
35.1
37.9
28.5
30.2
34.0
37.8
TURN[20]
17.7
21.0
29.4
32.4
19.1
24.3
36.9
39.3
27.6
31.4
33.8
38.1
CMCM[17]
28.9
35.9
42.6
50.4
32.7
36.8
41.9
45.1
31.1
34.0
39.3
44.8
DCID+S-Mit
28.1
32.3
45.9
49.2
32.2
34.0
47.8
53.0
34.8
37.6
51.9
53.5
DCID+MST
31.2
35.0
50.7
52.1
34.9
37.8
54.4
59.1
33.5
35.4
57.1
59.2
DCID+TURN
29.4
35.3
53.4
56.0
29.7
36.9
55.2
58.2
31.9
36.8
56.2
60.9
DCID+CODIS
33.4
36.0
53.8
60.2
36.7
41.0
52.6
62.0
35.9
40.1
54.3
59.0
DCID+CMCM
34.1
38.8
57.6
60.8
36.4
42.9
58.7
62.8
38.8
41.4
57.5
60.5
Uni-Code
44.0
49.7
61.9
65.7
47.7
52.3
64.0
65.6
41.2
45.6
60.5
61.7"
DATASETS AND TASKS,0.5084745762711864,Table 2: Ablation studies of audio-visual pre-training on AVE and AVVP tasks.
DATASETS AND TASKS,0.5141242937853108,"CLUB
Cross-CPC
MM-EMA
Reset code
Lcmcm
VGGsounds-AVEL 24K
VGGsounds-AVEL 40K
AVE
V‚ÜíA A‚ÜíV
AVVP
V‚ÜíA A‚ÜíV
AVE
V‚ÜíA A‚ÜíV
AVVP
V‚ÜíA A‚ÜíV
-
‚úì
‚úì
‚úì
‚úì
34.9
35.1
50.6
54.0
37.2
40.3
52.9
59.5
‚úì
-
‚úì
‚úì
‚úì
4.6
5.8
10.9
24.6
5.2
7.1
12.3
24.1
-
-
‚úì
‚úì
‚úì
29.8
34.6
30.4
32.5
35.2
36.9
32.3
34.0
‚úì
‚úì
-
‚úì
‚úì
34.1
38.8
57.6
60.8
36.4
42.9
58.7
62.8
‚úì
‚úì
‚úì
-
‚úì
37.8
41.2
59.1
61.5
38.9
40.3
55.4
62.1
‚úì
‚úì
‚úì
‚úì
-
39.7
42.6
58.2
62.1
41.3
46.0
58.7
62.8
‚úì
‚úì
-
-
‚úì
28.2
30.9
41.4
49.2
31.5
33.8
46.0
48.3
‚úì
‚úì
‚úì
‚úì
‚úì
44.0
49.7
61.9
65.7
47.7
52.3
64.0
65.6"
DATASETS AND TASKS,0.519774011299435,"Downstream: we evaluate the pre-trained models on several downstream tasks using different
datasets: Cross-modal event classification (AVE [46]): In AVE, each video contains a primary
event that is present in both audio and visual information. Therefore, we can train an event classifier
using a single modality (e.g., video) and directly evaluate the classifier‚Äôs performance on another
modality (e.g., audio). Cross-modal event localization (AVVP [47]): In AVVP, a portion of the
data is annotated with fine-grained event labels for both audio and visual modalities. Similar to
AVE, we can perform event localization on one modality and then directly transfer the model to the
other modality for testing. Cross both modal and dataset localization/classification: In order to
prove that our model can also achieve cross-modal transfer on different downstream datasets, we use
classification tasks for one modality in AVE to train, and directly test fine-grained event localization
ability of the model on another modality in AVVP (from AVE to AVVP). Also we test the cross-modal
classification tasks between the visual modality in part of UCF-101 (16 classes) and audio modality
in part of VGGSound-AVEL (16 classes). Cross-modal video segmentation (AVSBench-S4 [48]):
In AVS, each piece of AVS data contains a piece of audio, a video clip of the object that emits the
sound, a text label corresponding to the object, and a manually labeled mask. We expand each text
label into a descriptive sentence. We use one modality (e.g., audio) to train the query-based video
segmentation, and directly test the segmentation ability in another modality (e.g., text). Cross-modal
retrieval (VGGSound-AVEL) We select retrieval task to demonstrate that our method can also
be applied to visual-text generalization as well. We use audio as an inter-medium to measure the
generalization ability of our model across these two modalities and implement an X-to-audio retrieval
task. To be detailed, in the first stage, we train visual-text unified representation learning using
VGGSound24K dataset, and then in the second stage, during downstream training, we let the model
learn text(video)-audio retrieval, finally during inference, we directly test the generalization ability of
the model on video(text)-audio retrieval."
BASELINES AND IMPLEMENTATION DETAILS,0.5254237288135594,"5.2
Baselines and Implementation Details"
BASELINES AND IMPLEMENTATION DETAILS,0.5310734463276836,"We use the method depicted in Section 4.1 as our baseline. We also compare our methods with several
state-of-the-art unified representation methods: MST [13], CODIS [16], S-MiT [43], CMCM [17],"
BASELINES AND IMPLEMENTATION DETAILS,0.536723163841808,Table 3: Ablation studies of audio-visual-text pre-training on three downstream tasks.
BASELINES AND IMPLEMENTATION DETAILS,0.5423728813559322,"CLUB
Cross-CPC
MM-EMA
Reset code
Lcmcm
VGGsounds-AVEL 40K
AVE
V‚ÜíA A‚ÜíV
AVVP
V‚ÜíA A‚ÜíV
AVE‚ÜíAVVP
V‚ÜíA A‚ÜíV
UCF(v)‚ÜîVGG(a)
V‚ÜíA A‚ÜíV
-
‚úì
‚úì
‚úì
‚úì
50.2
51.8
62.4
66.2
50.1
51.2
9.87
9.59
‚úì
-
‚úì
‚úì
‚úì
43.8
49.2
59.3
61.1
45.5
50.6
60.6
54.6
‚úì
‚úì
-
‚úì
‚úì
52.9
49.9
62.0
67.3
48.1
46.8
66.5
60.5
‚úì
‚úì
-
-
‚úì
33.0
35.5
56.7
61.2
7.4
12.6
43.3
35.2
‚úì
‚úì
‚úì
-
‚úì
50.8
47.8
56.4
61.1
47.9
50.4
60.0
49.8
‚úì
‚úì
‚úì
‚úì
-
52.4
54.5
57.5
72.9
50.5
48.7
69.9
59.7
‚úì
‚úì
‚úì
‚úì
‚úì
54.1
55.0
63.4
71.0
53.0
52.4
67.1
60.6
Evaluation results of the labeled modality
64.8
65.8
71.0
72.9
-
-
80.0
85.4"
BASELINES AND IMPLEMENTATION DETAILS,0.5480225988700564,"(a) Pretrained on audio-visual.
(b) Pretrained on audio-visual-text."
BASELINES AND IMPLEMENTATION DETAILS,0.5536723163841808,Figure 3: The effect of different codebook sizes on two pre-training tasks.
BASELINES AND IMPLEMENTATION DETAILS,0.559322033898305,"TURN [20]. We implement these methods on our tasks and test their performance on two downstream
tasks. We use the perception to evaluate the performance on AVE, VGGSound-AVEL and UCF101
datasets, and use accuracy (%) to evaluate the performance on AVVP datasets, and use F1-score (%)
to evaluate the performance for AVE to AVVP generalization task, and use mIoU and F-score (as
same as AVS[48]) for AVS-S4 dataset. The implementation details are provided in Appendix."
COMPARED WITH STATE-OF-THE-ART METHODS,0.5649717514124294,"5.3
Compared with State-of-the-art Methods"
COMPARED WITH STATE-OF-THE-ART METHODS,0.5706214689265536,"We mainly compare our model with these leading methods on two downstream tasks, cross-modal
event classification and localization, all these models are pre-trained using audio-visual modalities
on three different sizes of datasets. As we can see in Table 1, our methods can outperform all these
previous works on three settings. The baseline results indicate that without any constraints, it is
challenging for the model to learn the alignment relationship between different modalities during
pre-training, leading to poor performance in downstream tasks. Moreover, the experiment results also
demonstrate that merely employing these state-of-the-art methods in such unconstrained audio-visual
alignment has considerable limitations. However, when incorporated with our proposed DCID
module, these models exhibit substantial improvements in downstream tasks. This further suggests
that our proposed DCID can effectively decouple the semantic information of different modalities."
COMPARED WITH STATE-OF-THE-ART METHODS,0.576271186440678,"In addition, we can observe from Table 1 that when the scale of pre-training data increases from 24K
to 40K, there is a significant improvement in the performance of the model on downstream tasks.
However, when the scale reaches 81K, the performance of the model does not improve much and
even declines. This also indicates that excessive pre-training data may not yield desirable results,
only an appropriate amount of data can enable the model to learn the correct alignment relationships."
ABLATION STUDIES,0.5819209039548022,"5.4
Ablation Studies"
ABLATION STUDIES,0.5875706214689266,"Main Components We use the audio-visual unified representation as pre-training task and test the
performance on the downstream AVE and AVVP tasks by removing each main component of our
model, the results are depicted in Table 2. We can observe that the performances all decline after
removing these components. In the DCID module, the impact of removing Cross-CPC is far greater
than that of removing CLUB, which indicates that without the fine-grained cross-modal alignment
constraints provided by Cross-CPC, it is difficult for CLUB alone to extract effective semantic"
ABLATION STUDIES,0.5932203389830508,"information from the original modal features. When MM-EMA is removed, the model performance
also declines significantly, demonstrating that MM-EMA can effectively utilize the Cross-Attention
mechanism between different modalities to gradually map the relevant information together, helping
to achieve a unified representation. The experimental results of removing the ""reset code"" indicate
that this module can effectively remove the redundancy latent codes caused by MM-EMA."
ABLATION STUDIES,0.5988700564971752,"The Effect of Codebook Size Different codebook sizes may affect the effectiveness of modality
aggregation, thereby influencing the performance of downstream cross-modal generalization tasks.
Here we investigate how the size of codebook influences pre-training performance. From Fig 3, we
can see that the appropriate codebook size can achieve the best pre-training performance, while too
large size (which may cause signals from different modalities to aggregate at different positions in the
codebook) and too small size (where semantically distinct signals are erroneously mapped together
due to limited capacity) will both degrade the performance of pre-trained models."
ABLATION STUDIES,0.6045197740112994,"Unified Representation of Three Modalities Compared with previous works that can only align
two modalities together, our method can disentangle and align the semantic information of three
modalities, as shown in Table 3. From the results we can see that the removal of these modules will
lead to a decline in the performance of the pre-trained model on downstream tasks, similar to the
results in Table 2. However, compared to the unified representation of the two modalities, the removal
of these modules leads to a smaller decline in the model performance. We think the reason is that the
introduction of the third modality can facilitate the other two modalities approaching each other, and
the performance of the model pre-trained on three modalities is significantly higher than that of the
model pre-trained on two, which also illustrate this point."
ABLATION STUDIES,0.6101694915254238,"Results on AVS-S4 As we can see in Table 4, our pre-trained model can effectively transfer the
segmentation ability across modals. The visualization of segmentation results in Fig 5, our model
can accurately identify vocal regions in video frames for unknown modalities. Furthermore, the
performance of our method is closer to the evaluation results of the AVS model on source modality.
More visualization results and analysis can be found in Appendix."
ABLATION STUDIES,0.615819209039548,"Results on Cross Modal Retrieval Tasks We also test the performance of X-to-audio retrieval task
under cross modal generalization setting, where X can be visual or text, the results are as shown in
Table 5. Compared with baseline model, our methods can greatly advance the audio retrieval ability
in both two generalization directions."
ABLATION STUDIES,0.6214689265536724,"(a) baseline
(b) CMCM
(c) Uni-Code"
ABLATION STUDIES,0.6271186440677966,"Figure 4: Visualization of the discrete codes. The blue dot means that the number of visual modal
quantization accounts for more than 80% of the total number of audio and video quantification, the
red dot means that the audio station accounts for more than 80%, and the green dot means that audio
and video each account for more than 20%. As we can seen in (b), the discrete codes obtained by
CMCM still contain a little single modality mapped codes, while ours (c) are almost all green codes
which can be mapped by both two modalities."
QUALITATIVE ANALYSES,0.632768361581921,"5.5
Qualitative Analyses"
QUALITATIVE ANALYSES,0.6384180790960452,"To further illustrate the effectiveness of our model during pre-training phase, we take audio and video
as an example and visualize the latent discrete codes obtained from two modalities. As we can see in
Fig 4(a), when there is no additional supervisory signal, different modalities are difficult to map to
the same discrete code, which also proves why the baseline performs poorly in downstream tasks.
The CMCM [17] method can effectively alleviate this problem, but there are still many codes that
can only be mapped by single modality. However, our model enables information from different
modalities but with the same semantics to be mapped into the same codes. (as shown in Fig 4(c))."
QUALITATIVE ANALYSES,0.6440677966101694,"Table 4: Performance on AVS-S4 datasets (pre-
trained on audio-visual-text modalities)."
QUALITATIVE ANALYSES,0.6497175141242938,"Methods
A2T
T2A
mIoU F-score mIoU F-score"
QUALITATIVE ANALYSES,0.655367231638418,"Baseline
69.8
81.4
69.9
81.3
Our full model
78.0
87.1
77.7
86.7"
QUALITATIVE ANALYSES,0.6610169491525424,"SST [49] (A2A)
60.3
80.1
-
-
AVS [48] (A2A)
78.7
87.9
-
-"
QUALITATIVE ANALYSES,0.6666666666666666,"Table 5: Performance of audio retrieval tasks
under cross modal generalization directions."
QUALITATIVE ANALYSES,0.672316384180791,"Methods
V2T
T2V
R@5 R@10 R@5 R@10"
QUALITATIVE ANALYSES,0.6779661016949152,"Baseline
0.47
1.03
0.62
0.85
Our full model 10.3
21.9
8.47
16.7"
QUALITATIVE ANALYSES,0.6836158192090396,"Figure 5: Visualization results of A2T (left) and T2A (right) of our model on AVS-S4 dataset. We
compare our method with the baseline model."
CONCLUSION AND DISCUSSION,0.6892655367231638,"6
Conclusion and Discussion"
CONCLUSION AND DISCUSSION,0.6949152542372882,"In this paper, we propose a new Cross Modal Generalization task, and bring up two innotative
modules, DCID and MM-EMA, which can achieve multi-modal unified representation. We achieve
unified representation of different multi-modal combinations, including audio-visual, audio-text
and audio-visual-text. Also, we conduct extensive experiments on various downstream tasks. The
extensive experiments demonstrate the effectiveness of our proposed methods. Furthermore, our
model can help existing models to extend into other modalities, achieving more extensive applications.
Limitations and Boarder Impact: In our work we only focus on the unified representation of three
modalities. However, our method can inspire future works to explore the combination of more
modalities. Our work do not contain potential negative social impact."
ACKNOWLEDGMENTS,0.7005649717514124,"7
Acknowledgments"
ACKNOWLEDGMENTS,0.7062146892655368,"This work is supported by National Key R&D Program of China under Grant No.2022ZD0162000,
National Natural Science Foundation of China under Grant No. 62222211 and No.61836002. We
also gratefully acknowledge the support of MindSpore (https://www.mindspore.cn), which is a
new deep learning computing framework."
REFERENCES,0.711864406779661,References
REFERENCES,0.7175141242937854,"[1] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence
Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE
international conference on computer vision, pages 2425‚Äì2433, 2015."
REFERENCES,0.7231638418079096,"[2] Jie Lei, Licheng Yu, Mohit Bansal, and Tamara L Berg. Tvqa: Localized, compositional video
question answering. arXiv preprint arXiv:1809.01696, 2018."
REFERENCES,0.7288135593220338,"[3] Guangyao Li, Yake Wei, Yapeng Tian, Chenliang Xu, Ji-Rong Wen, and Di Hu. Learning
to answer questions in dynamic audio-visual scenarios. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages 19108‚Äì19118, 2022."
REFERENCES,0.7344632768361582,"[4] Kirill Gavrilyuk, Amir Ghodrati, Zhenyang Li, and Cees GM Snoek. Actor and action video
segmentation from a sentence. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 5958‚Äì5966, 2018."
REFERENCES,0.7401129943502824,"[5] Seonguk Seo, Joon-Young Lee, and Bohyung Han. Urvos: Unified referring video object
segmentation network with a large-scale benchmark. In Computer Vision‚ÄìECCV 2020: 16th"
REFERENCES,0.7457627118644068,"European Conference, Glasgow, UK, August 23‚Äì28, 2020, Proceedings, Part XV 16, pages
208‚Äì223. Springer, 2020."
REFERENCES,0.751412429378531,"[6] Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, and Chenliang Xu. Audio-visual event
localization in unconstrained videos. In Proceedings of the European Conference on Computer
Vision (ECCV), pages 247‚Äì263, 2018."
REFERENCES,0.7570621468926554,"[7] Haoming Xu, Runhao Zeng, Qingyao Wu, Mingkui Tan, and Chuang Gan. Cross-modal
relation-aware networks for audio-visual event localization. In Proceedings of the 28th ACM
International Conference on Multimedia, pages 3893‚Äì3901, 2020."
REFERENCES,0.7627118644067796,"[8] Huijuan Xu and Kate Saenko. Ask, attend and answer: Exploring question-guided spatial
attention for visual question answering. In Computer Vision‚ÄìECCV 2016: 14th European
Conference, Amsterdam, The Netherlands, October 11‚Äì14, 2016, Proceedings, Part VII 14,
pages 451‚Äì466. Springer, 2016."
REFERENCES,0.768361581920904,"[9] Tam√°s Matuszka, Iv√°n Barton, √Åd√°m Butykai, P√©ter Hajas, D√°vid Kiss, Domonkos Kov√°cs,
S√°ndor Kuns√°gi-M√°t√©, P√©ter Lengyel, G√°bor N√©meth, Levente PetÀùo, et al. aimotive dataset: A
multimodal dataset for robust autonomous driving with long-range perception. arXiv preprint
arXiv:2211.09445, 2022."
REFERENCES,0.7740112994350282,"[10] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Doll√°r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer
Vision‚ÄìECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,
Proceedings, Part V 13, pages 740‚Äì755. Springer, 2014."
REFERENCES,0.7796610169491526,"[11] Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zisserman. Vggsound: A large-scale
audio-visual dataset. In ICASSP 2020-2020 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), pages 721‚Äì725. IEEE, 2020."
REFERENCES,0.7853107344632768,"[12] Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and
Boqing Gong. Vatt: Transformers for multimodal self-supervised learning from raw video,
audio and text. Advances in Neural Information Processing Systems, 34:24206‚Äì24221, 2021."
REFERENCES,0.7909604519774012,"[13] Haoxuan You, Luowei Zhou, Bin Xiao, Noel Codella, Yu Cheng, Ruochen Xu, Shih-Fu Chang,
and Lu Yuan. Learning visual representation from modality-shared contrastive language-image
pre-training. In Computer Vision‚ÄìECCV 2022: 17th European Conference, Tel Aviv, Israel,
October 23‚Äì27, 2022, Proceedings, Part XXVII, pages 69‚Äì87. Springer, 2022."
REFERENCES,0.7966101694915254,"[14] Alex Andonian, Shixing Chen, and Raffay Hamid. Robust cross-modal representation learning
with progressive self-distillation. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 16430‚Äì16441, 2022."
REFERENCES,0.8022598870056498,"[15] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi.
Unified-io: A unified model for vision, language, and multi-modal tasks. arXiv preprint
arXiv:2206.08916, 2022."
REFERENCES,0.807909604519774,"[16] Jiali Duan, Liqun Chen, Son Tran, Jinyu Yang, Yi Xu, Belinda Zeng, and Trishul Chilimbi.
Multi-modal alignment using representation codebook. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages 15651‚Äì15660, 2022."
REFERENCES,0.8135593220338984,"[17] Alexander H Liu, SouYoung Jin, Cheng-I Jeff Lai, Andrew Rouditchenko, Aude Oliva, and
James Glass. Cross-modal discrete representation learning. arXiv preprint arXiv:2106.05438,
2021."
REFERENCES,0.8192090395480226,"[18] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. In International conference on machine learning,
pages 8748‚Äì8763. PMLR, 2021."
REFERENCES,0.8248587570621468,"[19] Victor Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James Y Zou. Mind
the gap: Understanding the modality gap in multi-modal contrastive representation learning.
Advances in Neural Information Processing Systems, 35:17612‚Äì17625, 2022."
REFERENCES,0.8305084745762712,"[20] Yang Zhao, Chen Zhang, Haifeng Huang, Haoyuan Li, and Zhou Zhao. Towards effective
multi-modal interchanges in zero-resource sounding object localization. Advances in Neural
Information Processing Systems, 35:38089‚Äì38102, 2022."
REFERENCES,0.8361581920903954,"[21] Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing
Li, Yu Zhang, et al. Speecht5: Unified-modal encoder-decoder pre-training for spoken language
processing. arXiv preprint arXiv:2110.07205, 2021."
REFERENCES,0.8418079096045198,"[22] Pengyu Cheng, Weituo Hao, Shuyang Dai, Jiachang Liu, Zhe Gan, and Lawrence Carin. Club:
A contrastive log-ratio upper bound of mutual information. In International conference on
machine learning, pages 1779‚Äì1788. PMLR, 2020."
REFERENCES,0.847457627118644,"[23] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive
predictive coding. arXiv preprint arXiv:1807.03748, 2018."
REFERENCES,0.8531073446327684,"[24] Disong Wang, Liqun Deng, Yu Ting Yeung, Xiao Chen, Xunying Liu, and Helen Meng.
Vqmivc: Vector quantization and mutual information-based unsupervised speech representation
disentanglement for one-shot voice conversion. arXiv preprint arXiv:2106.10132, 2021."
REFERENCES,0.8587570621468926,"[25] Yan Xia and Zhou Zhao. Cross-modal background suppression for audio-visual event localiza-
tion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pages 19989‚Äì19998, 2022."
REFERENCES,0.864406779661017,"[26] Stavros Petridis, Themos Stafylakis, Pingchuan Ma, Georgios Tzimiropoulos, and Maja Pantic.
Audio-visual speech recognition with a hybrid ctc/attention architecture. In 2018 IEEE Spoken
Language Technology Workshop (SLT), pages 513‚Äì520. IEEE, 2018."
REFERENCES,0.8700564971751412,"[27] Pritam Sarkar and Ali Etemad. Xkd: Cross-modal knowledge distillation with domain alignment
for video representation learning. arXiv preprint arXiv:2211.13929, 2022."
REFERENCES,0.8757062146892656,"[28] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng,
and Jingjing Liu. Uniter: Universal image-text representation learning. In Computer Vision‚Äì
ECCV 2020: 16th European Conference, Glasgow, UK, August 23‚Äì28, 2020, Proceedings, Part
XXX, pages 104‚Äì120. Springer, 2020."
REFERENCES,0.8813559322033898,"[29] Teng Wang, Wenhao Jiang, Zhichao Lu, Feng Zheng, Ran Cheng, Chengguo Yin, and Ping
Luo. Vlmixer: Unpaired vision-language pre-training via cross-modal cutmix. In International
Conference on Machine Learning, pages 22680‚Äì22690. PMLR, 2022."
REFERENCES,0.8870056497175142,"[30] Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael Zeng, and
Xuedong Huang. Unispeech: Unified speech representation learning with labeled and unlabeled
data. In International Conference on Machine Learning, pages 10937‚Äì10947. PMLR, 2021."
REFERENCES,0.8926553672316384,"[31] Fabrizio Pedersoli, Dryden Wiebe, Amin Banitalebi, Yong Zhang, and Kwang Moo Yi. Estimat-
ing visual information from audio through manifold learning. arXiv preprint arXiv:2208.02337,
2022."
REFERENCES,0.8983050847457628,"[32] Simon Jenni, Alexander Black, and John Collomosse. Audio-visual contrastive learning with
temporal self-supervision. arXiv preprint arXiv:2302.07702, 2023."
REFERENCES,0.903954802259887,"[33] Adri√† Recasens, Jason Lin, Jo¬Øao Carreira, Drew Jaegle, Luyu Wang, Jean-baptiste Alayrac,
Pauline Luc, Antoine Miech, Lucas Smaira, Ross Hemsley, et al. Zorro: the masked multimodal
transformer. arXiv preprint arXiv:2301.09595, 2023."
REFERENCES,0.9096045197740112,"[34] Chi Han, Mingxuan Wang, Heng Ji, and Lei Li. Learning shared semantic space for speech-to-
text translation. arXiv preprint arXiv:2105.03095, 2021."
REFERENCES,0.9152542372881356,"[35] Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio,
Aaron Courville, and Devon Hjelm. Mutual information neural estimation. In International
conference on machine learning, pages 531‚Äì540. PMLR, 2018."
REFERENCES,0.9209039548022598,"[36] Michael Tschannen, Josip Djolonga, Paul K Rubenstein, Sylvain Gelly, and Mario Lucic. On
mutual information maximization for representation learning. arXiv preprint arXiv:1907.13625,
2019."
REFERENCES,0.9265536723163842,"[37] Lingpeng Kong, Cyprien de Masson d‚ÄôAutume, Wang Ling, Lei Yu, Zihang Dai, and Dani
Yogatama. A mutual information maximization perspective of language representation learning.
arXiv preprint arXiv:1910.08350, 2019."
REFERENCES,0.9322033898305084,"[38] Malik Boudiaf, Imtiaz Ziko, J√©r√¥me Rony, Jos√© Dolz, Pablo Piantanida, and Ismail Ben Ayed.
Information maximization for few-shot learning. Advances in Neural Information Processing
Systems, 33:2445‚Äì2457, 2020."
REFERENCES,0.9378531073446328,"[39] Wei Zhu, Haitian Zheng, Haofu Liao, Weijian Li, and Jiebo Luo. Learning bias-invariant repre-
sentation by cross-sample mutual information minimization. In Proceedings of the IEEE/CVF
International Conference on Computer Vision, pages 15002‚Äì15012, 2021."
REFERENCES,0.943502824858757,"[40] Solomon Kullback. Information theory and statistics. Courier Corporation, 1997."
REFERENCES,0.9491525423728814,"[41] Avraham Ruderman, Mark Reid, Dar√≠o Garc√≠a-Garc√≠a, and James Petterson. Tighter varia-
tional representations of f-divergences via restriction to probability measures. arXiv preprint
arXiv:1206.4664, 2012."
REFERENCES,0.9548022598870056,"[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
processing systems, 30, 2017."
REFERENCES,0.96045197740113,"[43] Mathew Monfort, SouYoung Jin, Alexander Liu, David Harwath, Rogerio Feris, James Glass,
and Aude Oliva. Spoken moments: Learning joint audio-visual representations from video
descriptions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 14871‚Äì14881, 2021."
REFERENCES,0.9661016949152542,"[44] Jinxing Zhou, Dan Guo, and Meng Wang. Contrastive positive sample propagation along the
audio-visual event line. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022."
REFERENCES,0.9717514124293786,"[45] Jinxing Zhou, Liang Zheng, Yiran Zhong, Shijie Hao, and Meng Wang. Positive sample
propagation along the audio-visual event line. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 8436‚Äì8444, 2021."
REFERENCES,0.9774011299435028,"[46] Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, and Chenliang Xu. Audio-visual event
localization in unconstrained videos. In Proceedings of the European Conference on Computer
Vision (ECCV), pages 247‚Äì263, 2018."
REFERENCES,0.9830508474576272,"[47] Yapeng Tian, Dingzeyu Li, and Chenliang Xu. Unified multisensory perception: Weakly-
supervised audio-visual video parsing. In Computer Vision‚ÄìECCV 2020: 16th European
Conference, Glasgow, UK, August 23‚Äì28, 2020, Proceedings, Part III 16, pages 436‚Äì454.
Springer, 2020."
REFERENCES,0.9887005649717514,"[48] Jinxing Zhou, Jianyuan Wang, Jiayi Zhang, Weixuan Sun, Jing Zhang, Stan Birchfield, Dan
Guo, Lingpeng Kong, Meng Wang, and Yiran Zhong. Audio‚Äìvisual segmentation. In Com-
puter Vision‚ÄìECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23‚Äì27, 2022,
Proceedings, Part XXXVII, pages 386‚Äì403. Springer, 2022."
REFERENCES,0.9943502824858758,"[49] Brendan Duke, Abdalla Ahmed, Christian Wolf, Parham Aarabi, and Graham W Taylor. Sstvos:
Sparse spatiotemporal transformers for video object segmentation. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5912‚Äì5921, 2021."
