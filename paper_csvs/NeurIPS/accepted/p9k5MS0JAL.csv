Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0019230769230769232,"Classification is a fundamental task in science and engineering on which machine
learning methods have shown outstanding performances. However, it is challenging
to determine whether such methods have achieved the Bayes error rate, that is,
the lowest error rate attained by any classifier. This is mainly due to the fact that
the Bayes error rate is not known in general and hence, effectively estimating it is
paramount. Inspired by the work by Ishida et al. (2023), we propose an estimator
for the Bayes error rate of supervised multi-class classification problems. We
analyze several theoretical aspects of such estimator, including its consistency,
unbiasedness, convergence rate, variance, and robustness. We also propose a
denoising method that reduces the noise that potentially corrupts the data labels, and
we improve the robustness of the proposed estimator to outliers by incorporating the
median-of-means estimator. Our analysis demonstrates the consistency, asymptotic
unbiasedness, convergence rate, and robustness of the proposed estimators. Finally,
we validate the effectiveness of our theoretical results via experiments both on
synthetic data under various noise settings and on real data."
INTRODUCTION,0.0038461538461538464,"1
Introduction"
INTRODUCTION,0.0057692307692307696,"Supervised classification problems are typical tasks in various fields of science and engineering,
such as machine learning, statistical signal processing, estimation, and detection. In supervised
classification, a dataset consisting of several input feature-label pairs is given. The goal of a supervised
classification task is to design effective classifiers, by leveraging the given dataset, to suitably label
future input features, or equivalently, to classify future input features into one of the classes."
INTRODUCTION,0.007692307692307693,"As the dataset is the only available resource on the data distribution, the performance of a classifier
is typically measured by its empirical misclassification rate on the test dataset (which is a subset of
the given dataset). The best performance of an existing classifier, that is, the so-called state-of-the-
art (SOTA) performance, tends to be the point of reference to measure the improvement of a new
classifier. However, there are no guarantees that the SOTA performance is close to the theoretical
minimum misclassification rate, namely the Bayes error rate (BER). Thus, comparing the empirical
misclassification rate with the SOTA error rate provides only a relative improvement."
INTRODUCTION,0.009615384615384616,"Having the knowledge of the BER is essential in theory and practice. The BER indeed provides
a fundamental limit on the misclassification rate, which is critical for designing high-performing"
INTRODUCTION,0.011538461538461539,"classifiers. Moreover, one can leverage the BER to assess how good the SOTA performance is with
respect to the theoretically optimal error rate. If the SOTA performance is nearly close to the BER,
we can avoid wasting time and effort in designing a new classifier. Furthermore, the BER indicates
the inherent hardness of a task and hence, it can be seen as a benchmark for comparing the hardness
of different tasks [13, 46, 83]. Knowing the BER also brings the opportunity to detect whether test
dataset overfitting occurs (which has sporadically happened [3, 51, 59, 91]); this overfitting can be
detected since the BER provides the minimum misclassification rate and hence, no classifier will
perform strictly better than it. We refer an interested reader to Appendix A for a thorough literature
overview on methods to estimate and bound the BER."
INTRODUCTION,0.013461538461538462,"In this paper, we investigate the problem of estimating the BER of an M-class classification task
directly from a dataset, where M ≥2 is arbitrary. Our technique to estimate the BER is different from
a plug-in approach that first estimates the distribution from which the data is drawn, and then evaluates
the BER. Indeed, our BER estimators, which are proved to be unbiased, consistent and robust to label
noise and outliers, do not require the estimation of the data probability density to perform an effective
BER estimation. We start by assuming that the data labels are soft and real-valued, approximating
the posterior probability of the class. We then relax this assumption on the data labels, and show the
applicability of our estimators on one-hot labels and other noisy datasets."
INTRODUCTION,0.015384615384615385,"Contributions.
Our contribution is summarized as follows. Inspired by [46], in Section 3 we first
propose a BER estimator for the case of soft data labels, and we show that it benefits from several
appealing properties, e.g., it is consistent, unbiased and asymptotically normal. In Section 3, we also
propose a methodology, inspired by the median-of-means estimator [65], to make any BER estimator
robust. Then, in Section 4 we study the performance of the proposed estimators in scenarios where the
soft labels are corrupted by two typical types of noise, i.e., the case of a noise that permutes/shuffles
the labels, and the case of additive noise. For the first type of noise, our estimators have similar
properties as in the noiseless case. However, for the additive noise case, our proposed estimators
are not consistent. Because of this, we propose a denoising method that averages noisy labels
associated with the same feature. The corresponding constructed estimator is shown to be consistent.
In Section 4, we also showcase that the noisy soft label framework can be used to study the case of
one-hot labels, and we provide a denoising method that encompasses the one proposed for the case of
additive noise. In Section 5 we validate the effectiveness of the proposed estimators via experiments
both on synthetic data under various noise settings (e.g., one-hot labels) and on real data using three
different datasets, i.e., CIFAR-10H [4], Fashion-MNIST-H [46] and MovieLens [38]. Finally, in
Section 6 we conclude the paper with some discussion on future research directions, which are worth
further investigation."
INTRODUCTION,0.01730769230769231,"Notation.
Deterministic scalar quantities are denoted by lowercase letters, scalar random variables
are denoted by uppercase letters, vectors are denoted by bold lowercase letters, and random vectors
by bold uppercase letters (e.g., x, X, x, X). We let xi (resp., (xk)i) indicate the i-th value of x
(resp., xk). Calligraphic letters X denote sets, and |X| is the cardinality of X. 1{S} is the indicator
function that yields 1 if S is true and 0 otherwise. [M] := {1, . . . , M}. For X ∈RM, we let Xi:M
be the i-th order statistics [18] of X, i.e., the i-th smallest value of X with i ∈[M]. Finally, In is the"
INTRODUCTION,0.019230769230769232,"identity matrix of dimension n, and
d→(resp.,
d=) denotes convergence (resp., equality) in distribution."
PROBLEM FORMULATION,0.021153846153846155,"2
Problem formulation"
PROBLEM FORMULATION,0.023076923076923078,"We consider an M-class classification task in which an input feature x is classified into a class
c ∈C := [M]. Our goal is to estimate the minimum misclassification probability, that is the BER.
In particular, we seek to estimate the BER based on a dataset D = {(xi, yi)}n
i=1 that follows an"
PROBLEM FORMULATION,0.025,"unknown data distribution, i.e., (xi, yi)
i.i.d.
∼PX,Y, where X ∈X and Y ∈Y ⊆[0, 1]M.1"
PROBLEM FORMULATION,0.026923076923076925,"We assume that the label data y contains the information about the class c of the input feature x,
implying that one can retrieve (x, c) from (x, y). When a classifier ϕ : X →C is employed for a
classification task, the corresponding misclassification probability is defined as E(ϕ) := Pr(ϕ(X) ̸=
C), where C is the true class for X. The minimum value of this misclassification probability is the
so-called BER, denoted by Pe, which is formally defined next."
PROBLEM FORMULATION,0.028846153846153848,"1We assume that X is a finite set, but several of our results easily extend to the case when X is an infinite set."
PROBLEM FORMULATION,0.03076923076923077,"Definition 1 (Bayes error rate [50]). Consider an M-class classification problem, where an input
feature x ∈X has to be classified into a class c ∈C := [M]. The BER is defined as"
PROBLEM FORMULATION,0.032692307692307694,"Pe = inf
ϕ∈Φ E(ϕ) = inf
ϕ∈Φ E [1{ϕ(X) ̸= C}] ,
(1)"
PROBLEM FORMULATION,0.03461538461538462,"where Φ is the set of all measurable functions ϕ : X →C, and the expectation is taken over PX,C."
PROBLEM FORMULATION,0.03653846153846154,"The misclassification probability E(ϕ) depends on the quality of the classifier ϕ and the BER is
obtained by choosing an optimal classifier. In fact, an optimal classifier is theoretically equivalent to
the Maximum a Posteriori (MAP) classifier [50], that is, ϕMAP(x) = arg maxk∈C Pr(C = k|X =
x). Plugging the MAP classifier into (1), the BER can be written as"
PROBLEM FORMULATION,0.038461538461538464,"Pe = E

1 −max
k∈C Pr(C = k|X)

,
(2)"
PROBLEM FORMULATION,0.04038461538461539,"where the expectation is taken with respect to PX. Note that Pe ∈

0, 1 −
1
M

."
PROBLEM FORMULATION,0.04230769230769231,"Our main objective is to effectively estimate the BER from the dataset D. In the remaining of the
paper, we let ψ : D →[0, 1] denote the estimator of the BER."
BER ESTIMATION WITH SOFT LABELS,0.04423076923076923,"3
BER estimation with soft labels"
BER ESTIMATION WITH SOFT LABELS,0.046153846153846156,"Soft labels have several favorable properties (e.g., they help to prevent an overfitting problem, they
lead to a well-structured model, and they improve the prediction performance) that make them
widely used in machine learning. For example, they are essential in label smoothing and knowledge
distillation, which are widely applied methods to improve model performance [81, 97–101]."
BER ESTIMATION WITH SOFT LABELS,0.04807692307692308,"There exist several types of soft labels and here we assume that a label is soft if it represents the
posterior probability. Specifically, we say that D = {(xi, yi)}n
i=1 is a dataset with soft labels if
xi ∈X and yi ∈Y are such that yi =  "
BER ESTIMATION WITH SOFT LABELS,0.05,"Pr(C = 1|X = xi)
Pr(C = 2|X = xi)
...
Pr(C = M|X = xi) "
BER ESTIMATION WITH SOFT LABELS,0.051923076923076926,".
(3)"
BER ESTIMATION WITH SOFT LABELS,0.05384615384615385,"This is a standard and widely adopted assumption; it has indeed been argued [17, 35, 42, 61, 69, 101]
that using soft labels to approximate posterior probabilities enhances model performance.
Definition 2 (Base BER estimator). The BER estimator ψsoft(D) is defined as"
BER ESTIMATION WITH SOFT LABELS,0.05576923076923077,ψsoft(D) = 1 n X
BER ESTIMATION WITH SOFT LABELS,0.057692307692307696,"(x,y)∈D"
BER ESTIMATION WITH SOFT LABELS,0.05961538461538462,"
1 −max
j∈[M] yj"
BER ESTIMATION WITH SOFT LABELS,0.06153846153846154,"
.
(4)"
BER ESTIMATION WITH SOFT LABELS,0.06346153846153846,"We will use the base estimator ψsoft in (4) as a building block for a robust BER estimation. We note
that ψsoft in (4) with M = 2 retrieves the estimator proposed in [46]. The next theorem (proof in
Appendix B.1) provides three important properties of ψsoft in (4).
Theorem 1. Assume that D contains soft labels as defined in (3). Then, ψsoft(D) satisfies the
following properties:"
BER ESTIMATION WITH SOFT LABELS,0.06538461538461539,"1. (Unbiasedness): E[ψsoft(D)] = Pe, that is, ψsoft(D) is an unbiased estimator of the BER;"
BER ESTIMATION WITH SOFT LABELS,0.0673076923076923,"2. (Consistency): For any δ ∈(0, 1), it holds that |ψsoft −Pe| < r (1−1 M )
2"
N,0.06923076923076923,"2n
ln 2"
N,0.07115384615384615,"δ with
probability at least 1 −δ, that is, ψsoft(D) is a consistent estimator of the BER;"
N,0.07307692307692308,"3. (Asymptotic Normality): √n(ψsoft −Pe)
d→N(0, Var(YM:M)) as n →∞."
N,0.075,"Theorem 1 shows that the BER can be effectively estimated directly from a dataset that contains soft
labels as in (3). Moreover, it highlights that the convergence rate of ψsoft(D) is n−1"
N,0.07692307692307693,"2 , which is indeed
the optimal (parametric) convergence rate."
N,0.07884615384615384,"Another important aspect to assess the performance of ψsoft(D) is to measure how far it is from the
BER Pe. Since ψsoft(D) is unbiased (from Theorem 1) this distance can be measured by computing
the variance of ψsoft(D), which is denoted as Var(ψsoft) and provided by the next proposition (proof
in Appendix B.2)."
N,0.08076923076923077,Proposition 1. It holds that Var(ψsoft)= 1
N,0.08269230769230769,nVar(YM:M) and Var(ψsoft)≤(1−1
N,0.08461538461538462,"M )Pe−P 2
e
n
≤(1−1 M )
2"
N,0.08653846153846154,"4n
."
N,0.08846153846153847,"The exact computation of Var(ψsoft) in Proposition 1 requires the knowledge of the order statistics of
Y and hence, of the label distribution PY. The upper bounds on Var(ψsoft) are instead distribution-
independent. In particular, both upper bounds show that the rate of convergence of Var(ψsoft)
is 1/n, which is in line with Theorem 1. Moreover, the first upper bound on Var(ψsoft) implies
that Var(ψsoft) →0 when either Pe →0 (i.e., also known as realizability assumption [76]) or
Pe →1 −
1
M (i.e., labels and features are independent). The first upper bound on Var(ψsoft) also
allows to find an upper bound on Pe which becomes tight when Pe →1 −1/M."
N,0.09038461538461538,"3.1
Robustness of ψsoft"
N,0.09230769230769231,"We consider robustness to outliers, where an outlier is a data sample that is corrupted by high noise.
We use the concept of breakdown point [44, 60] to measure the robustness of ψsoft. The breakdown
point captures how robust an estimator is with respect to outliers.2 In the classical definition of
breakdown point, the worst estimator (in terms of robustness) for a dataset D with outliers is defined
as ψ(D) = ∞. However, in our setting ψsoft(D) ≤1 −
1
M since ψsoft(D) is an estimate of Pe.
Because of this, we next adopt an alternative definition for the breakdown point which is commonly
used for a bounded parameter space Θ [45].
Definition 3 (Breakdown point). Consider an estimator ψ : Ωn →Θ of θ ∈Θ ⊆RL. Let
D(κ) = {D1, . . . , Dκ}, where Di ∈Ω, ∀i ∈[κ], be a clean dataset without outliers, and let
eD(τ) = { eD1, . . . , eDτ} be a noisy dataset that is composed of τ noisy data samples that can be
arbitrarily replaced by outliers. Then, the breakdown point of ψ is defined as"
N,0.09423076923076923,"B(ψ) =
min
τ:1≤τ≤κ"
N,0.09615384615384616,"
τ
κ + τ : sup ∥ψ(D(κ+τ)) −ψ(D(κ) ∪eD(τ))∥≥∥rad(Θ)∥

,
(5)"
N,0.09807692307692308,"where the sup is taken over all D(κ+τ), D(κ) and eD(τ) such that D(κ) ⊂D(κ+τ), and rad(Θ) is the
vector consisting of the L radii of the largest L-dimensional ellipsoid in Θ.3"
N,0.1,"B(ψ) in (5) quantifies the value of the breakdown point for an estimator ψ : Ωτ+κ →Θ. In particular,
an estimator ψ “breaks down” if ψ(D(κ+τ)) changes of at least ∥rad(Θ)∥when τ clean data samples
are replaced with τ outliers. The breakdown point is defined as the minimum ratio between the
number of outliers (τ) and the total number of data samples (κ + τ), such that τ outliers are sufficient
to break the estimator ψ(D(κ+τ)). From Definition 3, it is clear that the higher the value of the
breakdown point, the more robust the estimator is. To measure the breakdown point of ψsoft, we
start by noting that ψsoft is the sample mean of {1 −maxj∈[M](yi)j}n
i=1 (see (4)). Thus, using the
notation as in Definition 3, we have that Θ =

0, 1 −
1
M

, which leads to rad(Θ) = 1"
N,0.10192307692307692,"2 −
1
2M . This
implies that B(ψsoft) = 1"
N,0.10384615384615385,"n (e.g., when D contains an outlier y such that maxj∈[M] yj = ∞) and
hence, ψsoft is not robust to outliers."
N,0.10576923076923077,"We next propose a methodology, inspired by the median-of-means estimator [65], to make any BER
estimator (and hence, also ψsoft) robust.
Definition 4 (Median-of-BERs (MoB) estimator). Consider any BER estimator ψ and a dataset D.
First, partition D into K sub-datasets Dk, k ∈[K] such that |D| = n = cK, where c ∈N is the
number of data samples in Dk. Then, the Median-of-BERs (MoB) estimator is defined as
MoBK(ψ, D) = med({ψ(Dk) : k ∈[K]}),
(6)
where med(S) is the sample median of S."
N,0.1076923076923077,"2The sample mean can fail to estimate the true mean by a single outlier (e.g., a sample x with a value very
different from the true mean). Thus, the sample mean has a breakdown point equal to 1/n, meaning that one
outlier can break the estimate. Differently, the median is more robust since it has a breakdown point of 1/2, i.e.,
half of the samples need to be outliers for breaking the estimate."
N,0.10961538461538461,"3Any L-dimensional ellipsoid (after rotation) can be defined as
x2
1
a2
1 +
x2
2
a2
2 + . . . +
x2
L
a2
L = 1. Then, the L radii
are a1, a2, . . . , aL."
N,0.11153846153846154,"The next theorem (proof in Appendix B.3) provides three important properties of MoBK(ψsoft, D).
Theorem 2. Assume that D contains soft labels as defined in (3). Then, the MoB estimator
MoBK(ψsoft, D) satisfies the following properties:"
N,0.11346153846153846,"1. (Consistency): For all t ≲K, with probability at least 1 −4e−2t, it holds that"
N,0.11538461538461539,"|MoBK(ψsoft, D)−Pe|≤  
1−1 M
3 2
√"
N,0.11730769230769231,"3Var(YM:M)
K
√"
N,0.11923076923076924,"n−K +3
p"
N,0.12115384615384615,tVar(YM:M) ! r
N,0.12307692307692308,"1
n−K , (7)"
N,0.125,"and hence, MoBK(ψsoft, D) is a consistent estimator of the BER;"
N,0.12692307692307692,"2. (Breakdown): Its breakdown point is given by B (MoBK(ψsoft, D)) =
 K+1 2
 1 n;"
N,0.12884615384615383,"3. (Asymptotic Normality):
If K
→
∞and K
=
o(√n) as n
→
∞, then
√n (MoBK(ψsoft, D) −Pe)
d→N
 
0, π"
N,0.13076923076923078,"2 Var(YM:M)

."
N,0.1326923076923077,"Theorem 2 shows that MoBK(ψsoft, D) has the same rate of convergence of n−1"
N,0.1346153846153846,"2 as ψsoft (see
Theorem 1). However, MoBK(ψsoft, D) has a higher breakdown point (and hence, is more robust)
than ψsoft. Nonetheless, this robustness is attained at two expenses: (i) MoBK(ψsoft, D) is not an
unbiased estimator of Pe; and (ii) the variance of MoBK(ψsoft, D) is larger than the one of ψsoft by
a factor of π/2 in the asymptotic regime."
N,0.13653846153846153,"Theorem 2 also points out to an interesting trade-off, with respect to K, between accuracy and
robustness. For example, setting K = √n leads to a higher breakdown point (and hence, is more
robust) than setting K = ln n. However, the concentration bound in (7) (which measures the accuracy
of the estimation) has a much larger value with K = √n than with K = ln n."
BER ESTIMATION WITH NOISY LABELS,0.13846153846153847,"4
BER estimation with noisy labels"
BER ESTIMATION WITH NOISY LABELS,0.14038461538461539,"The soft label data discussed in Section 3 might be challenging to obtain as data labels are often
corrupted by noise or other perturbations. In such scenarios, data labels are unreliable [27, 64, 68]
and several studies have been conducted on them [37, 57, 80, 88, 90, 92, 94, 96]."
BER ESTIMATION WITH NOISY LABELS,0.1423076923076923,"With the goal to broaden the applicability of ψsoft (or its robust version MoBK(ψsoft, D)), we here
study their performance in scenarios where the soft labels are corrupted by various typical types of
noise.4 We denote by eY ∼P e
Y|Y the noisy soft label, i.e., a noisy soft label eY = ˜yi is distributed
according to the conditional probability distribution P e
Y|Y=yi, where yi is the true soft label."
ADDITIVE NOISE ON THE DATA LABELS,0.14423076923076922,"4.1
Additive noise on the data labels"
ADDITIVE NOISE ON THE DATA LABELS,0.14615384615384616,"We here consider the case where the labels are corrupted by additive noise, i.e., we have (x, ˜y) ∈eDA
where ˜y = y + z, where z is an M-dimensional random noise vector. We assume that z has i.i.d.
components each with zero mean."
ADDITIVE NOISE ON THE DATA LABELS,0.14807692307692308,"We start our analysis by showing that ψsoft in (4) applied on eDA is neither a consistent nor an unbiased
estimator of Pe. We, in fact, have that"
ADDITIVE NOISE ON THE DATA LABELS,0.15,"ψsoft( eDA) = 1 −1 n n
X"
ADDITIVE NOISE ON THE DATA LABELS,0.1519230769230769,"i=1
max
j∈[M]{(yi)j + (zi)j},
(8)"
ADDITIVE NOISE ON THE DATA LABELS,0.15384615384615385,"which, by the law of large numbers, converges to"
ADDITIVE NOISE ON THE DATA LABELS,0.15576923076923077,"lim
n→∞ψsoft( eDA) = 1 −E

max
j∈[M] {Yj + Zj}

.
(9)"
ADDITIVE NOISE ON THE DATA LABELS,0.1576923076923077,"Then, the inequalities maxi xi + minj yj ≤maxi{xi + yi} ≤maxi xi + maxj yj imply that"
ADDITIVE NOISE ON THE DATA LABELS,0.1596153846153846,"E[Z1:M] ≤Pe −lim
n→∞ψsoft( eDA) ≤E[ZM:M],
(10)"
ADDITIVE NOISE ON THE DATA LABELS,0.16153846153846155,4We refer an interested reader to Appendix B.4 where a noise that randomly shuffles the labels is also studied.
ADDITIVE NOISE ON THE DATA LABELS,0.16346153846153846,"which follow from (2) and (3). The bounds in (10) demonstrate that with labels corrupted by additive
noise, ψsoft is (in general) an inconsistent estimator of Pe. By following similar steps, it can be easily
proved that ψsoft is also a biased estimator of Pe. In particular, the bounds in (10) show that ψsoft has
a bias which is bounded by the expected value of the smallest (lower bound) and of the largest (upper
bound) order statistics of the noise. In what follows, we provide two examples of distributions for
which bounds on these expected values have been computed."
ADDITIVE NOISE ON THE DATA LABELS,0.16538461538461538,"Example 1. Let Z
i.i.d.
∼Uniform(−a, a). Then, E[Zi:M] =
2ai
M+1 −a, which as n →∞implies that"
ADDITIVE NOISE ON THE DATA LABELS,0.1673076923076923,|Pe −ψsoft( eDA)| ≤a M−1 M+1.
ADDITIVE NOISE ON THE DATA LABELS,0.16923076923076924,"Example 2. Let Z
i.i.d.
∼
γ2-sub-Gaussian.5
Then, we have that E[ZM:M] = −E[Z1:M] and
E[ZM:M] ≤
p"
ADDITIVE NOISE ON THE DATA LABELS,0.17115384615384616,"2γ2 ln M [9] which as n →∞implies that |Pe −ψsoft( eDA)| ≤
p"
ADDITIVE NOISE ON THE DATA LABELS,0.17307692307692307,2γ2 ln M.
ADDITIVE NOISE ON THE DATA LABELS,0.175,"Our analysis above shows that ψsoft( eDA) is (in general) an inconsistent and biased estimator of
Pe. Because of this, we next propose a new BER estimator, which (different from ψsoft) leverages
the features {xi}n
i=1 to denoise ˜yi. We refer to this estimator as ψDN. Our main intuition behind
proposing ψDN stems from the fact that data samples having the same feature x should be labeled
with the same (or at least similar) label. This can be attained by noting that, even if labels are noisy, it
is possible to minimize the effect of a zero-mean noise by averaging the noisy labels associated with
the same feature. We next formally define our denoising BER estimator ψDN : RM →[0, 1]."
ADDITIVE NOISE ON THE DATA LABELS,0.17692307692307693,"Definition 5 (Denoise estimator). For a noisy dataset eDA = {(xi, ˜yi)}n
i=1, let the denoised label
s(xi) for all i ∈[n], be defined as"
ADDITIVE NOISE ON THE DATA LABELS,0.17884615384615385,s(xi) =
ADDITIVE NOISE ON THE DATA LABELS,0.18076923076923077,"Pn
j=1 1{xj = xi}˜yj
Pn
j=1 1{xj = xi} ,
(11)"
ADDITIVE NOISE ON THE DATA LABELS,0.18269230769230768,"and let idx(xi) = arg maxj∈[M]{(s(xi))j}. Then, the denoise BER estimator is defined as"
ADDITIVE NOISE ON THE DATA LABELS,0.18461538461538463,"ψDN( eDA) = 1 n n
X i=1"
ADDITIVE NOISE ON THE DATA LABELS,0.18653846153846154," 
1 −(˜yi)idx(xi)

.
(12)"
ADDITIVE NOISE ON THE DATA LABELS,0.18846153846153846,The next theorem (proof in Appendix B.5) provides some important properties of ψDN( eDA).
ADDITIVE NOISE ON THE DATA LABELS,0.19038461538461537,"Theorem 3. Let eDA = {(xi, ˜yi)}n
i=1 be a dataset that consists of noisy soft labels ˜yi = yi + zi
where zi
i.i.d.
∼PZ is the zero mean noise. Then, ψDN( eDA) is a consistent estimator of Pe."
ADDITIVE NOISE ON THE DATA LABELS,0.19230769230769232,"Moreover, if the noise has bounded support, that is Pr(Z ∈[a, b]M) = 1, then"
ADDITIVE NOISE ON THE DATA LABELS,0.19423076923076923,1. (Asymptotical unbiasedness): ψDN( eDA) is an asymptotically unbiased estimator of Pe;
ADDITIVE NOISE ON THE DATA LABELS,0.19615384615384615,"2. (Denoising Consistency): For any δ ∈(0, 1), it holds that |s(x)−y| < r (1−1"
ADDITIVE NOISE ON THE DATA LABELS,0.19807692307692307,"M −a+b)
2"
NX,0.2,"2nx
ln 2"
NX,0.20192307692307693,"δ
with probability at least 1 −δ, where nx = Pn
j=1 1{xj = x} and s(x) is the denoised
label defined in (11) ."
NX,0.20384615384615384,"The results in Theorem 3 broadens the applicability of an effective BER estimator to a larger class of
datasets, e.g., to scenarios where the dataset includes multiple data samples representing the same
feature. For example, ψDN works well on a noisy dataset where each data sample has multiple labels.
Moreover, as we will see in Section 4.2, ψDN is also useful for one-hot labels. We also expect that
ψDN will work well with privatized datasets where noise was added to enhance privacy [26]."
NX,0.20576923076923076,"Remark 1. Our estimator ψDN only requires the dataset eDA. This is a more practical approach than
the one in [46] which instead also requires the true one-hot label. The estimator in [46], which is
referred to as a genie estimator ψgenie, can be obtained by replacing s(x) in (11) with the true one-hot
label. In Appendix B.6, we provide properties of ψgenie( eDA) that generalize the results in [46] for
any M ≥2. As expected, ψgenie is a consistent and unbiased estimator of Pe."
NX,0.2076923076923077,"5We say that a random variable Z is γ2-sub-Gaussian if E[eλ(Z−E[Z])] ≤e
λ2γ2"
NX,0.20961538461538462,"2
, ∀λ ∈R."
ONE-HOT LABELS,0.21153846153846154,"4.2
One-hot labels"
ONE-HOT LABELS,0.21346153846153845,"A dataset consisting of one-hot labels, denoted as D = {(xi, yi)}n
i=1, contains little information
about the class posterior probability. We assume that the one-hot label is constructed from the
corresponding soft label as follows,"
ONE-HOT LABELS,0.2153846153846154,"Y = ei with probability Yi,
(13)"
ONE-HOT LABELS,0.2173076923076923,"where: (i) Y is the one-hot random vector; (ii) Y is the soft label random vector in (3); and (iii)
ei ∈RM is the standard basis vector with a one in the i-th position and zero in all the other entries."
ONE-HOT LABELS,0.21923076923076923,"We can think of the one-hot label construction in (13) as a noisy label. Specifically, the noise
Z ∈{ei −y}M
i=1 with the probability mass function pZ|Y(z|y) = PM
j=1 yj1{z = ej −y} satisfies
Y = Y + Z. It is not difficult to see that E[Z]=E[E[Z|Y]]=0 and hence, our denoise estimator
ψDN( eDA) in (12), or its robust version MoBK(ψDN, eDA), can estimate the BER also for the case of
one-hot labels. However, we also note that with the above construction, the labels might be too noisy
to estimate the BER in practice. This suggests that a more refined denoising method than the one in
Definition 5 for one-hot labels might be needed for a more effective BER estimation."
ONE-HOT LABELS,0.22115384615384615,"Motivated by the above discussion, we next propose a denoising method for one-hot labels, which
leverages neighbor samples to mitigate the noise.6 Our choice of such a denoising method mainly
stems from the fact that one would expect that neighboring samples should have similar posterior
probabilities. In particular, for each xi, i ∈[n], we consider all of its neighbors (features that are at
most at a distance r from xi) and we average the corresponding noisy labels in a spirit similar to (11).
We next formally define our denoising BER estimator, which we refer to as ψC."
ONE-HOT LABELS,0.2230769230769231,"Definition 6 (Cluster denoise estimator). Given d : X 2 →R+ and r ∈R+, define Clusteri :=
{(x, y) ∈eDA : d(xi, x) ≤r}. Then, the denoised label for ˜yi, for all i ∈[n], is defined as ˆyi = P"
ONE-HOT LABELS,0.225,"(x,˜y)∈Clusteri ˜y"
ONE-HOT LABELS,0.22692307692307692,"|Clusteri|
,
(14)"
ONE-HOT LABELS,0.22884615384615384,and the cluster-based BER estimator is defined as
ONE-HOT LABELS,0.23076923076923078,"ψC( eDA, d, r) = 1 n n
X i=1"
ONE-HOT LABELS,0.2326923076923077,"
1 −max
j∈[M]{(ˆyi)j}

.
(15)"
ONE-HOT LABELS,0.23461538461538461,"Remark 2. Preprocessing the dataset that aggregates data samples of similar features can also be
helpful in improving the BER estimate for noisy labels, and it can be paired with the cluster denoise
estimator in Definition 6. A naive approach would consist of reducing the feature dimensionality and
increasing the number of data samples inside clusters (14), which helps mitigate the effect of the
noise. Some notable examples of data preprocessing are the classical principal component analysis
(PCA) [77] and the representation learning [5] (e.g., variational auto encoder [52]).
Remark 3. Another viewpoint to the denoising method in (14) is the connection to the non-parametric
estimation of the conditional expectation E[Y|X = x], where Y is the true soft label corresponding
to X = x. In particular, estimating E[Y|X = x] is equivalent to denoising the noisy label associated
with x as in (14). Under such a viewpoint, the BER estimator in Definition 6 resembles the
Nadaraya–Watson estimator with Parzen window kernel [63, 89]."
EXPERIMENTS,0.23653846153846153,"5
Experiments"
EXPERIMENTS,0.23846153846153847,"We empirically validate our results using various datasets, namely: 1) a synthetic dataset with different
noises, including one-hot labels; 2) two benchmark datasets CIFAR-10H [4] and Fashion-MNIST-
H [46]; and 3) MovieLens [38], a real-world dataset for movie recommendations. We compare our
estimators with two SOTA BER bounds [72], namely the generalized Henze-Penrose (GHP) BER
bounds [75] and the k-Nearest Neighbor (NN) BER bounds [16]. Details on the k-NN bounds and
GHP bounds, and additional results can be found in Appendix C."
EXPERIMENTS,0.2403846153846154,"6Several works have taken advantage of neighbor samples to estimate the BER or the probability divergence.
We refer an interested reader to [67] and references therein."
EXPERIMENTS,0.2423076923076923,"Figure 1: 4-classes Gaussian data samples. The left figure shows the samples of each class, and the
right figure displays the corresponding soft label information as the maximum value of the soft label."
EXPERIMENTS,0.24423076923076922,"(a) Soft labels.
(b) Noisy soft labels."
EXPERIMENTS,0.24615384615384617,"Figure 2: Comparison of MoBK(ψC) where ψC is defined in (15) with the GHP bounds and the
k-NN bounds with k = 1. We consider n = 2, 000 samples generated as in Figure 1 with noise
Z ∼N
 
0, 1"
EXPERIMENTS,0.24807692307692308,"5I4

. MoBK(ψC) uses K = ⌊√n⌋(with this choice, Theorem 2 ensures the asymptotic
normality of MoBK), the Euclidean distance for d, and r = 1/5."
EXPERIMENTS,0.25,"Synthetic dataset with soft labels and noisy labels.
We consider a 4-class classification problem
with equiprobable classes C ∈Cµ := {(µ, µ), (−µ, µ), (−µ, −µ), (µ, −µ)}, where µ > 0 is a
parameter that controls the classification hardness. We generate the feature X ∈R2 according to
a 2-dimensional Gaussian distribution with mean c (i.e., a realization of C ∈Cµ) and covariance
matrix I2. The corresponding soft labels yi’s are then obtained by the Bayes’ theorem. In particular,
since fX|C ∼N(C, I2) and PC(c) = 1/4 if c ∈Cµ (and zero otherwise), according to (3) we have"
EXPERIMENTS,0.2519230769230769,"that (yi)k =
fX|C(xi|c)PC(c)
P"
EXPERIMENTS,0.25384615384615383,"α∈Cµ fX|C(xi|α)PC(α).7 Figure 1 illustrates n = 1, 000 features (left figure), and the"
EXPERIMENTS,0.25576923076923075,"corresponding soft labels (right figure) for the synthetic dataset generated as explained above; in
particular, for each feature xi, we only reported the label (yi)j⋆where j⋆= arg maxj∈[4](yi)j
which is required for evaluating ψsoft in (4). As expected, a point near the decision boundary (lines
for x1 = 0 or x2 = 0) has a smaller maximum value in its soft label."
EXPERIMENTS,0.25769230769230766,"The results in Figure 2 empirically demonstrate the effectiveness of our estimators, which consistently
outperform the others.8 Moreover, our estimators estimate the exact value of the BER, while the
others derive upper and lower bounds on the BER, which might not be tight."
EXPERIMENTS,0.25961538461538464,"7Without loss of generality, we consider the following mapping: k = 1 if c = (µ, µ); k = 2 if c = (−µ, µ);
k = 3 if c = (−µ, −µ); and k = 4 if c = (µ, −µ).
8Among the proposed estimators, in Figure 2 we only evaluated MoBK(ψC) since we observed that it
performs well with respect to SOTA methods and hence, we omitted the other estimators."
EXPERIMENTS,0.26153846153846155,"(a) Pe.
(b) |ψ −Pe| and variance."
EXPERIMENTS,0.26346153846153847,"Figure 3: Comparison of MoBK(ψC) (evaluated with ψC in (15)) with ψsoft on noisy soft labels.
We consider Z ∼N (0, 1/5I4). To model outliers, we added ˜Z = 0.2 · Ber (1/2) with probability
1/10 to each entry of the soft labels. For different n, the parameters of MoBK(ψC) are chosen as
K = ⌊√n⌋, d is the Euclidean distance, and r = 1/5. We iterate the experiment 50 times for each n."
EXPERIMENTS,0.2653846153846154,"We further conducted experiments to verify the effectiveness of our denoising and robustifying
methods on data samples with label noise and outliers. Figure 3 shows the comparison of the
denoising estimator MoBK(ψC) with the base estimator ψsoft. We observe that ψsoft suffers from the
label noise, which leads to a bias in the estimation. However, our denoising estimator MoBK(ψC)
suitably denoises the noisy labels and effectively estimates the BER. As shown in Figure 3b, in fact,
the absolute error between the estimate and the BER as well as the variance decrease as n grows."
EXPERIMENTS,0.2673076923076923,"Figure 4: Comparison of MoBK(ψC) with
ψsoft for Gaussian samples with one-hot labels."
EXPERIMENTS,0.2692307692307692,"Synthetic dataset with one-hot labels.
We an-
alyze the performance of MoBK(ψC) in the same
Gaussian setting described above, but with one-hot
labels. Each soft label of the samples is mapped
to a one-hot label according to the categorical dis-
tribution with probabilities equal to the soft la-
bels (see (13)). Figure 4 shows that our denoising
method is capable of suitably reducing the noise
in one-hot labels; in fact, it correctly estimates the
BER after around 104 samples when MoBK(ψC)
converges to the BER. From Figure 4, we also ob-
serve that ψsoft performs poorly; this suggests that
denoising methods are essential for the BER esti-
mation task, hence worth further investigation."
EXPERIMENTS,0.27115384615384613,"CIFAR-10H [4] and Fashion-MNIST-H [46].
The CIFAR-10H dataset is a variation of CIFAR-10 [53], constructed by labeling 104 images in the
test dataset of CIFAR-10 by multiple labelers. The Fashion-MNIST-H dataset is populated by 104
images in the test dataset of Fashion-MNIST [93] in a similar manner. These two datasets have 10
classes, but we further categorized these into M ∈{2, 3} classes. In particular, for CIFAR-10H,
we assigned C1 = {airplane, automobile, ship, truck}, C2 = {bird, cat, deer, dog, frog, horse}
when M = 2, and C1 = {airplane, automobile, ship, truck}, C2 = {cat, dog, frog}, C3 =
{bird, deer, horse} when M
= 3.
Similarly, for Fashion-MNIST-H, we assigned C1
=
{t-shirt/top, pullover, dress, coat, shirt}, C2
=
{trouser, sandal, sneaker, bag, ankleboot}
when M = 2, and C1 = {t-shirt/top, pullover, coat, shirt}, C2 = {trouser, dress, bag}, and
C3 = {sandal, sneaker, ankleboot} when M = 3."
EXPERIMENTS,0.27307692307692305,"From Table 1, we observe that the estimates using MoBK(ψDN) with K = ⌊√n⌋are slightly lower
than ψDN, which is due to the robustness of MoBK(ψDN) to outliers.9 We also highlight that the
BER estimates can be leveraged to determine which task is more difficult. Intuitively, performing"
EXPERIMENTS,0.275,"9In general, there might be outliers that lead to low values of the estimated BER. However, since the labels
belong to [0, 1] and the estimated BER values in our examples are very small, it is reasonable to assume that
outliers with large values would be more impactful than outliers with small values."
EXPERIMENTS,0.27692307692307694,Table 1: BER estimate on benchmark and real-world datasets.
EXPERIMENTS,0.27884615384615385,"CIFAR-10H
Fashion-MNIST-H
MovieLens"
EXPERIMENTS,0.28076923076923077,"# classes
2
3
10
2
3
10
2
3"
EXPERIMENTS,0.2826923076923077,"ψDN
0.0050
0.0177
0.0456
0.0348
0.0932
0.2825
0.3063
0.4031
MoBK(ψDN)
0.0044
0.0168
0.0440
0.0347
0.0931
0.2816
0.3065
0.4035"
EXPERIMENTS,0.2846153846153846,"the classification task over CIFAR-10H is much easier than performing it over Fashion-MNIST-H,
and this is supported by the results in Table 1, i.e., the BERs associated with CIFAR-10H are smaller
than those over Fashion-MNIST-H. It is worth noting that test dataset overfitting can happen when
benchmark datasets are considered [3, 46, 51, 59, 91]. This can also be observed by the results
in Table 1, where our BER estimates of the 10-class classifications are larger than the SOTA error
rates of 0.005 on CIFAR-10 by [24] and of 0.0309 on Fashion-MNIST by [82]. We also suspect
that, since the labels are assigned by humans, who might not be experts, the datasets might still have
a considerable amount of label noise, which would lead to incorrect BER estimates. However, no
estimators (including ours) can estimate the BER from a dataset that contains very noisy labels."
EXPERIMENTS,0.2865384615384615,"MovieLens [38].
This dataset consists of 25 million ratings to 62, 000 movies by 162, 000 users.
Each rating ranges from 0.5 to 5 with step size 0.5. We first considered a movie classification
task: a user either likes a movie or not. This is a complex binary classification task with the input
feature being a movie (in general, a two-hour long video with audio) and the class being either
0 (dislike the movie) or 1 (like the movie). To ensure that a label belongs to [0, 1], we applied
min-max normalization to each rating. Moreover, in order to have enough ratings for each movie
when we applied the denoising method, we filtered out some data if the number of ratings was smaller
than 100. Then, ψDN and MoBK(ψDN) estimated the BER of such movie classification task, and
they yielded a BER of around 0.3. We then categorized the ratings into M = 3 classes, i.e., we
assigned C1 = {0.5, 1.0, 1.5}, C2 = {2.0, . . . , 3.5} and C3 = {4.0, 4.5, 5.0}. On this task, the BER
estimate is around 0.4. These BERs are quite large, implying that the movie classification is a hard
task to perform. This may be justified by the fact that ratings of movies are subjective, and movie
recommendations are indeed challenging without users’ information (e.g., content-based [47] or
item-based [74] recommendation systems use users’ information)."
CONCLUSION,0.28846153846153844,"6
Conclusion"
CONCLUSION,0.2903846153846154,"In this paper, we investigated the challenge of estimating the BER for multi-class classifications. We
proposed a BER estimator, ψsoft, which we proved to be unbiased, consistent, and asymptotically
normal when applied to soft-labeled datasets. By leveraging the median-of-mean method, we also
proposed a methodology to make any BER estimate robust. To ensure the applicability of the BER
estimator in practical scenarios, we analyzed the challenges posed by noisy soft labels, including
those with additive noise and one-hot labels. For such noisy labeled datasets, we developed denoising
techniques that effectively mitigate the label noise by using the corresponding features. We showed
that these denoising BER estimators are unbiased and consistent under mild noise assumptions. Our
experimental results, drawn from synthetic and real-world datasets, validated our theoretical results."
CONCLUSION,0.2923076923076923,"Although in this work we assumed that the set of features is finite, several of our results (e.g.,
Theorem 1 and Theorem 2) can be easily shown to hold also for the infinite feature space. A research
direction worth further investigation would consist of proving that the cluster denoise estimator in
Definition 6 has similar appealing properties (see Theorem 3) as the denoise estimator in Definition 5
(which assumes that the set of the features is discrete). Always along these lines, a second interesting
future work would be to analyze the rate of convergence of our BER estimator paired with the
denoising method as a function of the characteristics (e.g., cardinality, distribution) of the feature
space. Such an analysis would indeed provide insights into the difficulty of a classification task.
Another relevant avenue for future research lies in assessing the optimal performance of multi-
label learning, where a single data point can be associated with multiple classes. While adapting
our framework to predict the minimum Hamming loss for multi-label learning might be relatively
straightforward, estimating other metrics (e.g., the F1 score and the exact match ratio) might be a
difficult task, potentially necessitating new methodologies."
REFERENCES,0.29423076923076924,References
REFERENCES,0.29615384615384616,"[1] A. Antos, L. Devroye, and L. Gyorfi. Lower bounds for Bayes error estimation. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 21(7):643–645, 1999."
REFERENCES,0.2980769230769231,"[2] H. Avi-Itzhak and T. Diep. Arbitrarily tight upper and lower bounds on the Bayesian probability of error.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 18(1):89–91, 1996."
REFERENCES,0.3,"[3] B. Barz and J. Denzler. Do we train on test data? purging CIFAR of near-duplicates. Journal of Imaging,
6(6):41, 2020."
REFERENCES,0.3019230769230769,"[4] R. M. Battleday, J. C. Peterson, and T. L. Griffiths. Capturing human categorization of natural images
by combining deep networks and cognitive models. Nature Communications, 11(1):5418, 2020. doi:
10.1038/s41467-020-18946-z. URL https://doi.org/10.1038/s41467-020-18946-z."
REFERENCES,0.3038461538461538,"[5] Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798–1828, 2013. doi: 10.1109/
TPAMI.2013.50."
REFERENCES,0.3057692307692308,"[6] V. Berisha, A. Wisler, A. O. Hero, and A. Spanias. Empirically estimable classification bounds based on
a nonparametric divergence measure. IEEE Transactions on Signal Processing, 64(3):580–591, 2016.
doi: 10.1109/TSP.2015.2477805."
REFERENCES,0.3076923076923077,"[7] R. Bhatia and C. Davis. A better bound on the variance. The American Mathematical Monthly, 107(4):
353–357, 2000."
REFERENCES,0.3096153846153846,"[8] D. E. Boekee and J. C. van der Lubbe. Some aspects of error bounds in feature selection. Pattern
Recognition, 11(5-6):353–360, 1979."
REFERENCES,0.31153846153846154,"[9] S. Boucheron, G. Lugosi, and P. Massart. Concentration Inequalities: A Nonasymptotic Theory of
Independence. Oxford University Press, 2013."
REFERENCES,0.31346153846153846,"[10] L. Buturovi´c. Improving k-nearest neighbor density and error estimates. Pattern Recognition, 26(4):
611–616, 1993."
REFERENCES,0.3153846153846154,"[11] C. Chen. Theoretical comparison of a class of feature selection criteria in pattern recognition. IEEE
Transactions on Computers, C-20(9):1054–1056, 1971. doi: 10.1109/T-C.1971.223402."
REFERENCES,0.3173076923076923,"[12] C. H. Chen. On information and distance measures, error bounds, and feature selection. Information
Sciences, 10(2):159–173, 1976."
REFERENCES,0.3192307692307692,"[13] Q. Chen, F. Cao, Y. Xing, and J. Liang. Evaluating classification model against Bayes error rate. IEEE
Transactions on Pattern Analysis and Machine Intelligence, pages 1–16, 2023. doi: 10.1109/TPAMI.
2023.3240194."
REFERENCES,0.3211538461538462,"[14] H. Chernoff. A measure of asymptotic efficiency for tests of a hypothesis based on the sum of observations.
The Annals of Mathematical Statistics, pages 493–507, 1952."
REFERENCES,0.3230769230769231,"[15] T. M. Cover. Elements of Information Theory. John Wiley & Sons, 1999."
REFERENCES,0.325,"[16] T. M. Cover and P. Hart. Nearest neighbor pattern classification. IEEE Transactions on Information
Theory, 13(1):21–27, 1967. doi: 10.1109/TIT.1967.1053964."
REFERENCES,0.3269230769230769,"[17] T. Dao, G. M. Kamath, V. Syrgkanis, and L. Mackey. Knowledge distillation as semiparametric inference.
In International Conference on Learning Representations, 2021. URL https://openreview.net/
forum?id=m4UCf24r0Y."
REFERENCES,0.32884615384615384,"[18] H. A. David and H. N. Nagaraja. Order Statistics. Wiley Online Library, 2004."
REFERENCES,0.33076923076923076,"[19] P. A. Devijver. On a new class of bounds on Bayes risk in multihypothesis pattern recognition. IEEE
Transactions on Computers, C-23(1):70–80, 1974. doi: 10.1109/T-C.1974.223779."
REFERENCES,0.3326923076923077,"[20] P. A. Devijver and J. Kittler. Pattern Recognition. Prentice Hall, Old Tappan, NJ, July 1982."
REFERENCES,0.3346153846153846,"[21] L. Devroye. On the asymptotic probability of error in nonparametric discrimination. The Annals of
Statistics, 9(6):1320–1327, 1981."
REFERENCES,0.33653846153846156,"[22] L. Devroye, L. Györfi, and G. Lugosi. A Probabilistic Theory of Pattern Recognition, volume 31. Springer
Science & Business Media, 2013."
REFERENCES,0.3384615384615385,"[23] J. L. Doob. Regularity properties of certain families of chance variables. Transactions of the American
Mathematical Society, 47(3):455–486, 1940."
REFERENCES,0.3403846153846154,"[24] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,
M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16 words:
Transformers for image recognition at scale. In International Conference on Learning Representations,
2021. URL https://openreview.net/forum?id=YicbFdNTTy."
REFERENCES,0.3423076923076923,"[25] R. O. Duda, P. E. Hart, et al. Pattern Classification. John Wiley & Sons, 2006."
REFERENCES,0.34423076923076923,"[26] C. Dwork. Differential privacy. In Automata, Languages and Programming: 33rd International Col-
loquium, ICALP 2006, Venice, Italy, July 10-14, 2006, Proceedings, Part II 33, pages 1–12. Springer,
2006."
REFERENCES,0.34615384615384615,"[27] B. Frenay and M. Verleysen. Classification in the presence of label noise: A survey. IEEE Transactions
on Neural Networks and Learning Systems, 25(5):845–869, 2014. doi: 10.1109/TNNLS.2013.2292894."
REFERENCES,0.34807692307692306,"[28] J. H. Friedman and L. C. Rafsky. Multivariate generalizations of the wald-wolfowitz and smirnov
two-sample tests. The Annals of Statistics, pages 697–717, 1979."
REFERENCES,0.35,"[29] K. Fukunaga. Introduction to Statistical Pattern Recognition (2nd Ed.). Academic Press Professional,
Inc., USA, 1990. ISBN 0122698517."
REFERENCES,0.35192307692307695,"[30] K. Fukunaga and L. Hostetler. k-nearest-neighbor Bayes-risk estimation. IEEE Transactions on Informa-
tion Theory, 21(3):285–293, 1975. doi: 10.1109/TIT.1975.1055373."
REFERENCES,0.35384615384615387,"[31] K. Fukunaga and D. M. Hummels. Bayes error estimation using Parzen and k-NN procedures. IEEE
Transactions on Pattern Analysis and Machine Intelligence, (5):634–643, 1987."
REFERENCES,0.3557692307692308,"[32] K. Fukunaga and D. M. Hummels. Bias of nearest neighbor error estimates. IEEE Transactions on
Pattern Analysis and Machine Intelligence, (1):103–112, 1987."
REFERENCES,0.3576923076923077,"[33] K. Fukunaga and D. Kessell. Nonparametric Bayes error estimation using unclassified samples. IEEE
Transactions on Information Theory, 19(4):434–440, 1973. doi: 10.1109/TIT.1973.1055049."
REFERENCES,0.3596153846153846,"[34] F. Garber and A. Djouadi. Bounds on the Bayes classification error based on pairwise risk functions. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 10(2):281–288, 1988. doi: 10.1109/34.3891."
REFERENCES,0.36153846153846153,"[35] V. Grossmann, L. Schmarje, and R. Koch. Beyond hard labels: investigating data label distributions.
arXiv preprint arXiv:2207.06224, 2022."
REFERENCES,0.36346153846153845,"[36] L.-Z. Guo, Z.-Y. Zhang, Y. Jiang, Y.-F. Li, and Z.-H. Zhou. Safe deep semi-supervised learning for
unseen-class unlabeled data. In International Conference on Machine Learning, pages 3897–3906. PMLR,
2020."
REFERENCES,0.36538461538461536,"[37] B. Han, Q. Yao, X. Yu, G. Niu, M. Xu, W. Hu, I. Tsang, and M. Sugiyama. Co-teaching: Robust training
of deep neural networks with extremely noisy labels. Advances in Neural Information Processing Systems,
31, 2018."
REFERENCES,0.36730769230769234,"[38] F. M. Harper and J. A. Konstan. The movielens datasets: History and context. ACM Trans. Interact. Intell.
Syst., 5(4), dec 2015. ISSN 2160-6455. doi: 10.1145/2827872. URL https://doi.org/10.1145/
2827872."
REFERENCES,0.36923076923076925,"[39] W. Hashlamoun, P. Varshney, and V. Samarasooriya. A tight upper bound on the Bayesian probability
of error. IEEE Transactions on Pattern Analysis and Machine Intelligence, 16(2):220–224, 1994. doi:
10.1109/34.273728."
REFERENCES,0.37115384615384617,"[40] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pages 770–778, 2016."
REFERENCES,0.3730769230769231,"[41] M. Hellman and J. Raviv. Probability of error, equivocation, and the Chernoff bound. IEEE Transactions
on Information Theory, 16(4):368–372, 1970. doi: 10.1109/TIT.1970.1054466."
REFERENCES,0.375,"[42] M. Huai, C. Miao, Y. Li, Q. Suo, L. Su, and A. Zhang. Metric learning from probabilistic labels. In
Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data
Mining, pages 1541–1550, 2018."
REFERENCES,0.3769230769230769,"[43] G. Huang, Z. Liu, L. V. D. Maaten, and K. Q. Weinberger. Densely connected convolutional networks.
In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2261–2269,
Los Alamitos, CA, USA, jul 2017. IEEE Computer Society. doi: 10.1109/CVPR.2017.243. URL
https://doi.ieeecomputersociety.org/10.1109/CVPR.2017.243."
REFERENCES,0.37884615384615383,"[44] P. J. Huber. Robust estimation of a location parameter. The Annals of Mathematical Statistics, 35(1):73 –
101, 1964. doi: 10.1214/aoms/1177703732. URL https://doi.org/10.1214/aoms/1177703732."
REFERENCES,0.38076923076923075,"[45] P. J. Huber. Robust statistics. In International Encyclopedia of Statistical Science, pages 1248–1251.
Springer, 2011."
REFERENCES,0.38269230769230766,"[46] T. Ishida, I. Yamane, N. Charoenphakdee, G. Niu, and M. Sugiyama. Is the performance of my deep
network too good to be true? a direct approach to estimating the Bayes error in binary classification. In
The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.
net/forum?id=FZdJQgy05rz."
REFERENCES,0.38461538461538464,"[47] U. Javed, K. Shaukat, I. A. Hameed, F. Iqbal, T. M. Alam, and S. Luo. A review of content-based and
context-based recommendation systems. International Journal of Emerging Technologies in Learning
(iJET), 16(3):274–306, 2021."
REFERENCES,0.38653846153846155,"[48] E. Jeon. ViT-PyTorch, 2020. https://github.com/jeonsworld/ViT-pytorch,."
REFERENCES,0.38846153846153847,"[49] T. Kailath. The divergence and Bhattacharyya distance measures in signal selection. IEEE Transactions
on Communication Technology, 15(1):52–60, 1967. doi: 10.1109/TCOM.1967.1089532."
REFERENCES,0.3903846153846154,"[50] S. M. Kay. Fundamentals of Statistical Signal Processing, vol. 2: Detection Theory. Prentice Hall PTR,
1998."
REFERENCES,0.3923076923076923,"[51] D. Kiela, M. Bartolo, Y. Nie, D. Kaushik, A. Geiger, Z. Wu, B. Vidgen, G. Prasad, A. Singh, P. Ringshia,
Z. Ma, T. Thrush, S. Riedel, Z. Waseem, P. Stenetorp, R. Jia, M. Bansal, C. Potts, and A. Williams.
Dynabench: Rethinking benchmarking in NLP. In Proceedings of the 2021 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Technologies,
pages 4110–4124, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.
naacl-main.324. URL https://aclanthology.org/2021.naacl-main.324."
REFERENCES,0.3942307692307692,"[52] D. P. Kingma and M. Welling. Auto-encoding variational Bayes. arXiv preprint arXiv:1312.6114, 2013."
REFERENCES,0.39615384615384613,"[53] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009."
REFERENCES,0.39807692307692305,"[54] P. Kumar. Moments inequalities of a random variable defined over a finite interval. J. Inequal. Pure Appl.
Math, 3(3):1–24, 2002."
REFERENCES,0.4,"[55] J. Lin. Divergence measures based on the Shannon entropy. IEEE Transactions on Information Theory,
37(1):145–151, 1991. doi: 10.1109/18.61115."
REFERENCES,0.40192307692307694,"[56] T. Lissack and K.-S. Fu. Error estimation in pattern recognition via Lα-distance between posterior
density functions. IEEE Transactions on Information Theory, 22(1):34–45, 1976. doi: 10.1109/TIT.1976.
1055512."
REFERENCES,0.40384615384615385,"[57] S. Liu, J. Niles-Weed, N. Razavian, and C. Fernandez-Granda. Early-learning regularization prevents
memorization of noisy labels. Advances in Neural Information Processing Systems, 33:20331–20342,
2020."
REFERENCES,0.40576923076923077,"[58] G. Loizou and S. J. Maybank. The nearest neighbor and the Bayes error rates. IEEE Transactions
on Pattern Analysis and Machine Intelligence, PAMI-9(2):254–262, 1987. doi: 10.1109/TPAMI.1987.
4767899."
REFERENCES,0.4076923076923077,"[59] H. Mania, J. Miller, L. Schmidt, M. Hardt, and B. Recht. Model similarity mitigates test set overuse.
Advances in Neural Information Processing Systems, 32, 2019."
REFERENCES,0.4096153846153846,"[60] R. A. Maronna, R. D. Martin, V. J. Yohai, and M. Salibián-Barrera. Robust Statistics: Theory and Methods
(with R). John Wiley & Sons, 2019."
REFERENCES,0.4115384615384615,"[61] A. K. Menon, A. S. Rawat, S. Reddi, S. Kim, and S. Kumar. A statistical perspective on distillation. In
M. Meila and T. Zhang, editors, Proceedings of the 38th International Conference on Machine Learning,
volume 139 of Proceedings of Machine Learning Research, pages 7632–7642. PMLR, 18–24 Jul 2021.
URL https://proceedings.mlr.press/v139/menon21a.html."
REFERENCES,0.41346153846153844,"[62] S. Minsker. Distributed statistical estimation and rates of convergence in normal approximation. Electronic
Journal of Statistics, 13(2):5213 – 5252, 2019. doi: 10.1214/19-EJS1647. URL https://doi.org/10.
1214/19-EJS1647."
REFERENCES,0.4153846153846154,"[63] E. A. Nadaraya. On estimating regression. Theory of Probability & Its Applications, 9(1):141–142, 1964."
REFERENCES,0.4173076923076923,"[64] N. Natarajan, I. S. Dhillon, P. K. Ravikumar, and A. Tewari. Learning with noisy labels. Advances in
Neural Information Processing Systems, 26, 2013."
REFERENCES,0.41923076923076924,"[65] A. S. Nemirovskij and D. B. Yudin. Problem Complexity and Method Efficiency in Optimization. Wiley-
Interscience, 1983."
REFERENCES,0.42115384615384616,"[66] J. Neyman and E. S. Pearson. Ix. on the problem of the most efficient tests of statistical hypotheses.
Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical
or Physical Character, 231(694-706):289–337, 1933."
REFERENCES,0.4230769230769231,"[67] M. Noshad, L. Xu, and A. Hero. Learning to benchmark: Determining best achievable misclassification
error from training data. arXiv preprint arXiv:1909.07192, 2019."
REFERENCES,0.425,"[68] G. Patrini, A. Rozza, A. Krishna Menon, R. Nock, and L. Qu. Making deep neural networks robust to
label noise: A loss correction approach. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 1944–1952, 2017."
REFERENCES,0.4269230769230769,"[69] J. C. Peterson, R. M. Battleday, T. L. Griffiths, and O. Russakovsky. Human uncertainty makes classifica-
tion more robust. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages
9617–9626, 2019."
REFERENCES,0.4288461538461538,"[70] H. Phan. PyTorch_CIFAR10, 2021. https://github.com/huyvnphan/PyTorch_CIFAR10,."
REFERENCES,0.4307692307692308,"[71] Z. Ren, R. Yeh, and A. Schwing. Not all unlabeled data are equal: Learning to weight data in semi-
supervised learning. Advances in Neural Information Processing Systems, 33:21786–21797, 2020."
REFERENCES,0.4326923076923077,"[72] C. Renggli, L. Rimanic, N. Hollenstein, and C. Zhang. Evaluating Bayes error estimators on real-world
datasets with FeeBee. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and
Benchmarks Track (Round 2), 2021. URL https://openreview.net/forum?id=uKv5inrWeld."
REFERENCES,0.4346153846153846,"[73] T. Sakai, M. C. Plessis, G. Niu, and M. Sugiyama. Semi-supervised classification based on classification
from positive and unlabeled data. In International Conference on Mmachine Learning, pages 2998–3006.
PMLR, 2017."
REFERENCES,0.43653846153846154,"[74] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. Item-based collaborative filtering recommendation
algorithms. In Proceedings of the 10th International Conference on World Wide Web, WWW ’01, page
285–295, New York, NY, USA, 2001. Association for Computing Machinery. ISBN 1581133480. doi:
10.1145/371920.372071. URL https://doi.org/10.1145/371920.372071."
REFERENCES,0.43846153846153846,"[75] S. Y. Sekeh, B. Oselio, and A. O. Hero. Learning to bound the multi-class Bayes error. IEEE Transactions
on Signal Processing, 68:3793–3807, 2020. doi: 10.1109/TSP.2020.2994807."
REFERENCES,0.4403846153846154,"[76] S. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning: From Theory to Algorithms.
Cambridge University Press, 2014."
REFERENCES,0.4423076923076923,"[77] J. Shlens. A tutorial on principal component analysis. arXiv preprint arXiv:1404.1100, 2014."
REFERENCES,0.4442307692307692,"[78] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition.
pages 1–14. Computational and Biological Learning Society, 2015."
REFERENCES,0.4461538461538462,"[79] A. Singh, R. Nowak, and J. Zhu. Unlabeled data: Now it helps, now it doesn’t. Advances in Neural
Information Processing Systems, 21, 2008."
REFERENCES,0.4480769230769231,"[80] H. Song, M. Kim, D. Park, Y. Shin, and J.-G. Lee. Learning from noisy labels with deep neural networks:
A survey. IEEE Transactions on Neural Networks and Learning Systems, pages 1–19, 2022. doi:
10.1109/TNNLS.2022.3152527."
REFERENCES,0.45,"[81] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architecture for
computer vision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pages 2818–2826, 2016."
REFERENCES,0.4519230769230769,"[82] M. S. Tanveer, M. U. Karim Khan, and C.-M. Kyung. Fine-tuning darts for image classification. In 2020
25th International Conference on Pattern Recognition, pages 4789–4796, 2021. doi: 10.1109/ICPR48806.
2021.9412221."
REFERENCES,0.45384615384615384,"[83] R. Theisen, H. Wang, L. R. Varshney, C. Xiong, and R. Socher. Evaluating state-of-the-art classification
models against Bayes optimality. Advances in Neural Information Processing Systems, 34:9367–9377,
2021."
REFERENCES,0.45576923076923076,"[84] R. L. Thorndike. Who belongs in the family? Psychometrika, 18(4):267–276, 1953. doi: 10.1007/
BF02289263. URL https://doi.org/10.1007/BF02289263."
REFERENCES,0.4576923076923077,"[85] G. T. P. Toussaint. Feature Evaluation Criteria and Contextual Decoding Algorithms in Statistical Pattern
Recognition. PhD thesis, University of British Columbia, 1972."
REFERENCES,0.4596153846153846,"[86] K. Tumer and J. Ghosh. Bayes error rate estimation using classifier ensembles. International Journal of
Smart Engineering System Design, 5(2):95–109, 2003. doi: 10.1080/10255810305042. URL https:
//doi.org/10.1080/10255810305042."
REFERENCES,0.46153846153846156,"[87] J. Unnikrishnan, S. Haghighatshoar, and M. Vetterli. Unlabeled sensing with random linear measurements.
IEEE Transactions on Information Theory, 64(5):3237–3253, 2018."
REFERENCES,0.4634615384615385,"[88] X. Wang, Y. Hua, E. Kodirov, D. A. Clifton, and N. M. Robertson. ProSelfLC: Progressive self label
correction for training robust deep neural networks. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 752–761, 2021."
REFERENCES,0.4653846153846154,"[89] G. S. Watson. Smooth regression analysis. Sankhy¯a: The Indian Journal of Statistics, Series A, pages
359–372, 1964."
REFERENCES,0.4673076923076923,"[90] Q. Wei, L. Feng, H. Sun, R. Wang, C. Guo, and Y. Yin. Fine-grained classification with noisy labels. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11651–
11660, June 2023."
REFERENCES,0.46923076923076923,"[91] R. Werpachowski, A. György, and C. Szepesvári. Detecting overfitting via adversarial examples. Advances
in Neural Information Processing Systems, 32, 2019."
REFERENCES,0.47115384615384615,"[92] X. Xia, T. Liu, B. Han, C. Gong, N. Wang, Z. Ge, and Y. Chang. Robust early-learning: Hindering the
memorization of noisy labels. In International Conference on Learning Representations, 2021."
REFERENCES,0.47307692307692306,"[93] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-MNIST: A novel image dataset for benchmarking machine
learning algorithms. arXiv preprint arXiv:1708.07747, 2017."
REFERENCES,0.475,"[94] Y. Yao, T. Liu, M. Gong, B. Han, G. Niu, and K. Zhang. Instance-dependent label-noise learning under a
structural causal model. Advances in Neural Information Processing Systems, 34:4409–4420, 2021."
REFERENCES,0.47692307692307695,"[95] Y. Yao, L. Peng, and M. Tsakiris. Unlabeled principal component analysis. Advances in Neural
Information Processing Systems, 34:30452–30464, 2021."
REFERENCES,0.47884615384615387,"[96] K. Yi and J. Wu. Probabilistic end-to-end noise correction for learning with noisy labels. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7017–7025, 2019."
REFERENCES,0.4807692307692308,"[97] H. Yuan, N. Xu, Y. Shi, X. Geng, and Y. Rui. Learning from biased soft labels. arXiv preprint
arXiv:2302.08155, 2023."
REFERENCES,0.4826923076923077,"[98] C.-B. Zhang, P.-T. Jiang, Q. Hou, Y. Wei, Q. Han, Z. Li, and M.-M. Cheng. Delving deep into label
smoothing. IEEE Transactions on Image Processing, 30:5984–5996, 2021."
REFERENCES,0.4846153846153846,"[99] H. Zhang, P. Koniusz, S. Jian, H. Li, and P. H. Torr. Rethinking class relations: Absolute-relative
supervised and unsupervised few-shot learning. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 9432–9441, 2021."
REFERENCES,0.48653846153846153,"[100] L. Zhang, J. Song, A. Gao, J. Chen, C. Bao, and K. Ma. Be your own teacher: Improve the performance
of convolutional neural networks via self distillation. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pages 3713–3722, 2019."
REFERENCES,0.48846153846153845,"[101] H. Zhou, L. Song, J. Chen, Y. Zhou, G. Wang, J. Yuan, and Q. Zhang. Rethinking soft labels for
knowledge distillation: A bias–variance tradeoff perspective. In International Conference on Learning
Representations, 2021. URL https://openreview.net/forum?id=gIHd-5X324."
REFERENCES,0.49038461538461536,Appendices
REFERENCES,0.49230769230769234,"A
Related work"
REFERENCES,0.49423076923076925,"It is well known that the BER of a binary classification problem can be achieved by a likelihood
ratio test [66] and the BER of a multiclass classification problem can be attained by a Maximum
a Posteriori (MAP) classifier [50]. The problem of estimating the BER using a dataset has been
extensively investigated from the middle of the 20th century [16] to the present [13], leveraging
different approaches, such as the k-nearest neighbors (NN) method, probabilistic divergence, and
bounds on the BER. In the early stage, the main approach was to derive upper and lower bounds
on the BER, while recent work mainly focuses on directly estimating the BER rather than deriving
bounds on it."
REFERENCES,0.49615384615384617,"BER bounds.
Several bounds on the BER in binary classification tasks have been proposed
using probability divergence and distance metrics. Notable approaches use the Mahalanobis dis-
tance [20, 86], the Bhattacharyya distance between two class conditional distributions [20, 49, 86], the
Chernoff coefficient (a generalization of the Bhattacharyya distance) [14, 25, 29], the Henze-Penrose
divergence [6], the Jensen-Shannon divergence [55], bounds for the min{a, b} function [2, 39] (a gen-
eralization of the equivocation bound [41] and of the Bayesian bound [12]), the M0-distance [85], the
Fano’s inequality [15], and the L2 norm of posterior probabilities [19]. Some of these bounds are tight,
but they still require estimating the data distribution (e.g., class conditional distributions or posterior
probabilities), which may not be practical, particularly for modern data (e.g., images and videos).10
Bounds on the BER were also derived using non-parametric approaches [16, 21, 30, 33], such as
the k-NN classifier. A remarkable advantage of a k-NN classifier for this problem is its distribution-
free property to bound the BER (i.e., it works under no assumption on the data distribution). A
convergence rate analysis for universal BER estimators was developed in [1, 22]."
REFERENCES,0.4980769230769231,"Direct estimation of the BER.
As an alternative approach to using bounds on the BER, some
recent research directly estimates the BER [13, 46, 67, 83]. In particular, the authors in [67] expressed
the BER as a function of the f-divergence and proposed a density ratio estimator to evaluate such
an f-divergence that yields the BER. Under various label assumptions, a direct BER estimator
was proposed in [46]; this estimator is unbiased and consistent for soft labels, and it has desirable
properties. Another direct estimator for the BER was proposed in [13]. In particular, this estimator
uses the relationship between the BER and the miss-classified samples."
REFERENCES,0.5,"Extension to multi-class classification.
The extension of the BER in binary classification to
multi-class classification has been extensively investigated both in terms of generalizing existing
bounds and deriving new approaches. For example, the Chernoff bound [34], the Jensen-Shannon
divergence [55], the Fano’s inequality [15], the general mean distance [8], the L2 norm of posterior
probabilities [19], and the generalized Henze-Penrose divergence [75] can be used to derive bounds
on the BER in multi-class classification problems. Non-parametric methods [31, 58], such as the
k-NN classifier, can also bound the BER in multi-class classification settings. Bounds based on the
error probability of a k-NN classifier in the asymptotic regime were studied in [16], and some bounds
in the non-asymptotic regime can be found in [10, 32]."
REFERENCES,0.5019230769230769,"10Interested readers are referred to [2, 11, 56, 72]. In particular, comparisons of a variety of upper bounds on
the BER were studied in [56, 72], and an arbitrary tight upper bound was proposed in [2]."
REFERENCES,0.5038461538461538,"B
Proofs and auxiliary results"
REFERENCES,0.5057692307692307,"B.1
Proof of Theorem 1"
REFERENCES,0.5076923076923077,"Proof. By taking the expectation of ψsoft in (4) with respect to PX,Y, we obtain"
REFERENCES,0.5096153846153846,E[ψsoft(D)] = E  1 n X
REFERENCES,0.5115384615384615,"(x,y)∈D"
REFERENCES,0.5134615384615384,"
1 −max
j∈[M] yj   = 1 n X"
REFERENCES,0.5153846153846153,"(x,y)∈D
E

1 −max
j∈[M] Pr(C = j|X)
"
REFERENCES,0.5173076923076924,"= 1 −E

max
j∈[M] Pr(C = j|X)
"
REFERENCES,0.5192307692307693,"= Pe,
(16)"
REFERENCES,0.5211538461538462,"where the second equality follows from (3), and the last equality is due to (2). This shows that ψsoft
in (4) is an unbiased estimator of Pe."
REFERENCES,0.5230769230769231,"Next, let Uk
i.i.d.
∼
YM:M for all k ∈[n], and note that Uk ∈
 1"
REFERENCES,0.525,"M , 1

:= U. Define a function
f : Un →

0, 1 −
1
M

as f(U1, . . . , Un) := 1"
REFERENCES,0.5269230769230769,"n
Pn
k=1 (1 −Uk), which is equivalent to ψsoft in (4).
Then, for any uk ∈U and for all k ∈[n], we have that"
REFERENCES,0.5288461538461539,"sup
v∈U
|f(u1, . . . , ui−1, ui, ui+1, . . . , un) −f(u1, ..., ui−1, v, ui+1, ..., un)|"
REFERENCES,0.5307692307692308,"= sup
v∈U"
REFERENCES,0.5326923076923077,"1
n ((1 −ui) −(1 −v)) ≤1 n"
REFERENCES,0.5346153846153846,"
1 −1 M"
REFERENCES,0.5365384615384615,"
:= ci.
(17)"
REFERENCES,0.5384615384615384,"By using McDiarmid’s inequality [23], together with (16) and (17), we arrive at"
REFERENCES,0.5403846153846154,"Pr (|ψsoft −Pe| ≥ε) = Pr (|f(U1, . . . , Un) −E[f(U1, . . . , Un)]| ≥ε)"
REFERENCES,0.5423076923076923,≤2 exp 
REFERENCES,0.5442307692307692,"−
2nε2
 
1 −
1
M
2 !"
REFERENCES,0.5461538461538461,".
(18)"
REFERENCES,0.5480769230769231,"By taking n →∞, we obtain"
REFERENCES,0.55,"lim
n→∞Pr (|ψsoft −Pe| ≥ε) ≤lim
n→∞2 exp "
REFERENCES,0.551923076923077,"−
2nε2
 
1 −
1
M
2 !"
REFERENCES,0.5538461538461539,"= 0,
(19)"
REFERENCES,0.5557692307692308,which shows that ψsoft in (4) is a consistent estimator of Pe.
REFERENCES,0.5576923076923077,"By setting ε = r (1−1 M )
2"
N,0.5596153846153846,"2n
ln 2"
N,0.5615384615384615,"δ in (18) where δ ∈(0, 1), we obtain Pr "
N,0.5634615384615385,|ψsoft −Pe| <
N,0.5653846153846154,"s 
1 −
1
M
2"
N,0.5673076923076923,"2n
ln 2 δ "
N,0.5692307692307692,"> 1 −δ.
(20)"
N,0.5711538461538461,This shows the convergence rate.
N,0.573076923076923,"The estimator ψsoft in (4) can be viewed as the sample mean of 1 −maxj∈[M] yj in D. Since
maxj∈[M] yj ∼YM:M, from the central limit theorem, as n →∞, we obtain that
√n (ψsoft −Pe) →N(0, Var(1 −YM:M))
= N(0, Var(YM:M)).
(21)"
N,0.575,This concludes the proof of Theorem 1.
N,0.5769230769230769,"B.2
Proof of Proposition 1"
N,0.5788461538461539,"Proof. We have that YM:M ∼maxj∈[M] Yj. Then, by letting Uk
i.i.d.
∼
YM:M for all k ∈[n], we
obtain"
N,0.5807692307692308,Var(ψsoft) = Var
N,0.5826923076923077,"1
n n
X"
N,0.5846153846153846,"k=1
(1 −Uk) ! = 1"
N,0.5865384615384616,nVar (1 −Uk) = 1
N,0.5884615384615385,nVar(Uk) = 1
N,0.5903846153846154,"nVar(YM:M).
(22)"
N,0.5923076923076923,"Moreover, by using the Bhatia-Davis inequality [7], we obtain Var (YM:M) = Var (1 −YM:M) ≤
 
1 −
1
M −Pe

Pe since E[ψsoft] = Pe, which gives"
N,0.5942307692307692,Var(ψsoft) ≤
N,0.5961538461538461," 
1 −
1
M

Pe −P 2
e
n
.
(23)"
N,0.5980769230769231,"Finally, we can maximize the term
 
1 −
1
M −Pe

Pe over 0 ≤Pe ≤1 −
1
M . The maximum is"
N,0.6,"indeed (1−1 M )
2"
N,0.6019230769230769,"4
attained by Pe = 1"
N,0.6038461538461538,"2
 
1 −
1
M

. Hence,"
N,0.6057692307692307,Var(ψsoft) ≤
N,0.6076923076923076," 
1 −
1
M

Pe −P 2
e
n ≤"
N,0.6096153846153847," 
1 −
1
M
2"
N,0.6115384615384616,"4n
,
(24)"
N,0.6134615384615385,which concludes the proof of Proposition 1.
N,0.6153846153846154,"B.3
Proof of Theorem 2"
N,0.6173076923076923,"Proof. We start by noting that, by viewing ψsoft in (4) as a sample mean of (1−YM:M), the estimator
MoBK(ψsoft, D) is a median-of-means estimator. It has been shown in [62] that, provided that
E[|1 −YM:M −Pe|3] < ∞, a median-of-means estimator satisfies"
N,0.6192307692307693,"|MoBK(ψsoft, D) −Pe| ≤3(Var(YM:M))
1
2
E[|1 −YM:M −Pe|3]"
N,0.6211538461538462,"Var(YM:M)
3
2
K
n −K +
r
s
n −K"
N,0.6230769230769231,"
, (25)"
N,0.625,"with probability at least 1 −4e−2s, for all s ≲K. From [54, eq. (3.8)], since (1 −YM:M) ∈

0, 1 −
1
M

we have that"
N,0.6269230769230769,E[|1 −YM:M −Pe|3] ≤Pe
N,0.6288461538461538,"
1 −1 M −Pe"
N,0.6307692307692307," 
1 −1"
N,0.6326923076923077,M −2Pe  ≤
N,0.6346153846153846," 
1 −
1
M
3 6
√"
N,0.6365384615384615,"3
,
(26)"
N,0.6384615384615384,"where the last inequality follows by maximizing Pe
 
1 −
1
M −Pe
  
1 −
1
M −2Pe

with respect to
0 ≤Pe ≤1 −
1
M . Substituting (26) into (25) leads to"
N,0.6403846153846153,"|MoBK(ψsoft, D) −Pe| ≤3(Var(YM:M))
1
2"
N,0.6423076923076924," 
1 −
1
M
3 6
√"
N,0.6442307692307693,"3(Var(YM:M))
3
2
K
n −K +
r
s
n −K ! ="
N,0.6461538461538462," 
1 −
1
M
3 2
√"
N,0.6480769230769231,"3Var(YM:M)
K
√"
N,0.65,"n −K + 3
p"
N,0.6519230769230769,sVar(YM:M) ! r
N,0.6538461538461539,"1
n −K .
(27)"
N,0.6557692307692308,"Under an appropriate choice of n and K (e.g., K = o(n)), the upper bound in (27) converges to 0,
which proves the consistency of MoBK(ψsoft, D)."
N,0.6576923076923077,"For the breakdown point of MoBK(ψsoft, D), we start by observing that, if we have
 K+1"
N,0.6596153846153846,"2

outliers,
then at most
 K+1"
N,0.6615384615384615,"2

base estimators ψsoft(Dk)’s in (6) can be affected by these outliers, i.e., they can
be bad. In this case, MoBK(ψsoft, D) gives a bad estimate since more than half of the base estimators
ψsoft(Dk)’s are bad. However, if the number of outliers is smaller than
 K+1"
N,0.6634615384615384,"2

, e.g.,
 K+1"
N,0.6653846153846154,"2

−1,
then the median of ψsoft(Dk), k ∈[K] is not a bad estimate since the number of outliers is smaller
than half of the base estimators. Thus, from Definition 3 with τ =
 K+1"
N,0.6673076923076923,"2

and κ + τ = n, we obtain
B (MoBK(ψsoft, D)) =
 K+1 2
 1 n."
N,0.6692307692307692,"The proof of the asymptotic normality of MoBK(ψsoft, D) follows directly from [62, Theorem 4]. In
particular, if K →∞and K = o(√n) as n →∞, it holds that"
N,0.6711538461538461,"√n (MoBK(ψsoft, D) −Pe)
d→N

0, π"
N,0.6730769230769231,"2 Var(YM:M)

.
(28)"
N,0.675,This concludes the proof of Theorem 2.
N,0.676923076923077,"B.4
Random permutation noise"
N,0.6788461538461539,"One of the most common type of label noises is a noise that randomly permutes/shuffles the data
labels [27], i.e., we have (x, ˜y) ∈eDP where eDP denotes the dataset with randomly permuted labels.
This noise model can be categorized into two types [27], namely instance-independent and instance-
dependent. The former perturbs the label y with a random permutation according to some probability
distribution (the extreme noise case, namely unlabeled data or data without correspondence, where
the label is permuted with probability one using a random permutation matrix, has also been studied
extensively [36, 71, 73, 79, 87, 95]), which is independent of the input features. For example, a
corruption by an instance-independent noise to y = [0.1, 0.7, 0.2]⊤may result in ˜y = [0.1, 0.2, 0.7]⊤.
The other type of noise, the instance-dependent noise, flips/permutes the labels depending on the
instance. For example, the label “car” is more likely to be flipped to “truck” rather than “cat”."
N,0.6807692307692308,"We note that ψsoft in (4) only depends on the maximum value of y, which does not change after that
a random permutation is applied on y. This property ensures that ψsoft is still an effective estimator
of Pe even when the labels are randomly permuted, as stated in the next theorem."
N,0.6826923076923077,"Theorem 4. Let eDP = {(xi, ˜yi)}n
i=1 be a dataset that consists of noisy soft labels ˜yi = Piyi where
Pi, i ∈[M] is any random permutation matrix of size M × M. Then, ψsoft( eDP) satisfies all of the
properties in Theorem 1 and Proposition 1."
N,0.6846153846153846,"Proof. Since maxj∈[M] yj is permutation-invariant with respect to y, it follows that"
N,0.6865384615384615,"ψsoft({(xi, ˜yi)}n
i=1) = ψsoft({(xi, Piyi)}n
i=1)
= ψsoft({(xi, yi)}n
i=1),
(29)"
N,0.6884615384615385,"where the first equality follows since ˜yi = Piyi, and the second equality is due to the permutation-
invariance property of the max function. The results in Theorem 1 and Proposition 1 then prove
Theorem 4."
N,0.6903846153846154,"B.5
Proof of Theorem 3"
N,0.6923076923076923,"Proof. Without loss of generality, we let X = {u1, u2, . . . , uK}, where K = |X| is the cardinality of
X, and uk is the unique feature vector (or tensor). We denote by vk ∈Y the soft label corresponding
to uk, i.e., vk = [Pr(C = 1|X = uk), . . . , Pr(C = M|X = uk)]⊤. With these definitions, we
observe that due to the law of large numbers, we have"
N,0.6942307692307692,"lim
n→∞s(uk) = E[ eY|X = uk]"
N,0.6961538461538461,"(a)
= E[E[ eY|Y]|X = uk]"
N,0.698076923076923,"(b)
= E[Y|X = uk]
= vk,
(30)"
N,0.7,"where (a) follows from the law of total expectation and (b) is due to the fact that E[ eY|Y] = Y since
the noise is zero mean. Now, since for any i ∈[n] there exists a k ∈[K] such that xi = uk, we can
write that"
N,0.7019230769230769,"lim
n→∞idx(xi) = lim
n→∞idx(uk)"
N,0.7038461538461539,"(a)
= arg max
j∈[M]{(vk)j}"
N,0.7057692307692308,"= arg max
j∈[M]{Pr(C = j|X = uk)}"
N,0.7076923076923077,":= c
idx(uk),
(31)"
N,0.7096153846153846,"where the equality in (a) follows from (30). Then, we can write ψDN( eDA) in (12) as"
N,0.7115384615384616,"ψDN( eDA) = K
X k=1"
N,0.7134615384615385,"1
n n
X"
N,0.7153846153846154,"i=1
1{xi = uk}(1 −(˜yi)idx(xi)).
(32)"
N,0.7173076923076923,"When n →∞, it follows that"
N,0.7192307692307692,"lim
n→∞ψDN( eDA)
(a)
= K
X"
N,0.7211538461538461,"k=1
E
h
1{X = uk}

1 −eYc
idx(uk)
i (b)
= K
X"
N,0.7230769230769231,"k=1
E
h
1{X = uk}

1 −E[ eY|Y]c
idx(uk)
i (c)
= K
X"
N,0.725,"k=1
E
h
1{X = uk}

1 −Yc
idx(uk)
i = K
X"
N,0.7269230769230769,"k=1
E

1{X = uk}

1 −max
j∈[M] Pr(C = j|X = uk)
 = K
X"
N,0.7288461538461538,"k=1
E

1 −max
j∈[M] Pr(C = j|X = uk)
 X = uk"
N,0.7307692307692307,"
Pr(X = uk)"
N,0.7326923076923076,"= E

1 −max
j∈[M] Pr(C = j|X)
"
N,0.7346153846153847,"= Pe,
(33)"
N,0.7365384615384616,"where the labeled equalities follow from: (a) the law of large numbers, the assumption that (xi, ˜yi) ∼
PX, e
Y, and using (31); (b) using the law of total expectation; and (c) the fact that E[ eY|Y] = Y since
the noise is zero mean. This proves the consistency of ψDN."
N,0.7384615384615385,"The asymptotic unbiasedness directly follows from the consistency together with the assumption of
bounded support for the noise. Specifically, since Zj ∈[a, b] for all j ∈[M], we have that"
N,0.7403846153846154,"lim
n→∞E
h
ψDN( eDA)
i
= E
h
lim
n→∞ψDN( eDA)
i"
N,0.7423076923076923,"= E[Pe]
= Pe,
(34)"
N,0.7442307692307693,"where the first equality follows by the dominated convergence theorem that can be applied since
|ψDN( eDA)| ≤max{|a|, |1−1"
N,0.7461538461538462,"M +b|}, where a and b are the minimum and the maximum value in the
support of the noise, respectively, and they are both finite. This shows the asymptotic unbiasedness
of ψDN( eDA)."
N,0.7480769230769231,"We are left to prove the denoising consistency. The denoised label s(xi) in (11) is an unbiased
estimator of yi since"
N,0.75,"E[s(Xi)]
(a)
= E"
N,0.7519230769230769,"""Pn
j=1 1{Xi = Xj} eYj
Pn
j=1 1{Xi = Xj} #"
N,0.7538461538461538,"(b)
= E "" E"
N,0.7557692307692307,"""Pn
j=1 1{Xi = Xj} eYj
Pn
j=1 1{Xi = Xj} N, Xi ##"
N,0.7576923076923077,"(c)
= E ""
1
N N
X"
N,0.7596153846153846,"k=1
E
h
eYk
 N, Xi
i#"
N,0.7615384615384615,"(d)
= E ""
1
N N
X"
N,0.7634615384615384,"k=1
E [Yi | Xi] #"
N,0.7653846153846153,"= E[Yi|Xi],
(35)"
N,0.7673076923076924,"where the labeled equalities follow from: (a) letting Xj, Yj, and eYj for all j ∈[n] be independent
copies of X, Y, and eY, respectively; (b) introducing a random variable N ∈[n] that counts the
number of Xj such that Xj = Xi and using the law of total expectation; (c) re-indexing eYk, k ∈[N]
for the N pairs (Xj, eYk) such that Xi = Xj; and (d) the fact that the noise is zero mean."
N,0.7692307692307693,"Note that s(xi) is the average of nxi noisy labels ˜y’s corresponding to xi. By Hoeffding’s inequality,
we then obtain"
N,0.7711538461538462,Pr (|s(xi) −yi| ≥ε) ≤2 exp 
N,0.7730769230769231,"−
2nxiε2
 
1 −
1
M −a + b
2 !"
N,0.775,".
(36)"
N,0.7769230769230769,Setting ε = r (1−1
N,0.7788461538461539,"M −a+b)
2"
NXI,0.7807692307692308,"2nxi
ln 2"
NXI,0.7826923076923077,δ leads to Pr 
NXI,0.7846153846153846,|s(xi) −yi| <
NXI,0.7865384615384615,"s 
1 −
1
M −a + b
2"
NXI,0.7884615384615384,"2nxi
ln 2 δ "
NXI,0.7903846153846154,"> 1 −δ.
(37)"
NXI,0.7923076923076923,This concludes the proof of Theorem 3.
NXI,0.7942307692307692,"B.6
Properties of ψgenie( eDA)"
NXI,0.7961538461538461,The following proposition defines ψgenie and provides some properties of it.
NXI,0.7980769230769231,"Proposition 2. Let eDA = {(xi, ˜yi)}n
i=1 be a dataset that consists of noisy soft labels ˜yi = yi + zi
with zi
i.i.d.
∼PZ such that E[Z] = 0. Let ci = arg maxj∈[M](yi)j. Then, the following estimator"
NXI,0.8,"ψgenie( eDA) = 1 n n
X"
NXI,0.801923076923077,"i=1
(1 −(˜yi)ci)
(38)"
NXI,0.8038461538461539,satisfies the following properties:
NXI,0.8057692307692308,1. (Unbiasedness): It is an unbiased estimator of the BER;
NXI,0.8076923076923077,2. (Consistency): It is a consistent estimator of the BER;
NXI,0.8096153846153846,"3. (Variance): Var

ψgenie( eDA)

= Var(YM:M)+Var(Z) n
;"
NXI,0.8115384615384615,"4. (Asymptotic Normality): √n(ψgenie( eDA)−Pe)
d→N(0, Var(YM:M)+Var(Z)) as n →∞."
NXI,0.8134615384615385,"Proof. Define the random variable C = arg maxj∈[M] Yj that indicates the index of Y having the
largest value, i.e., YC = maxj∈[M] Yj. We have that"
NXI,0.8153846153846154,"E[ψgenie( eDA)] = 1 n n
X"
NXI,0.8173076923076923,"i=1
E[1 −eYC]"
NXI,0.8192307692307692,"(a)
= E[E[1 −eYC|Y]]"
NXI,0.8211538461538461,"(b)
= E[1 −YC]"
NXI,0.823076923076923,"= E

1 −max
j∈[M] Yj "
NXI,0.825,"= Pe,
(39)"
NXI,0.8269230769230769,"where the labeled equalities follow from: (a) the law of total expectation; and (b) the fact that
E[ eY|Y] = Y since E[Z] = 0. This shows the unbiasedness of ψgenie( eDA)."
NXI,0.8288461538461539,"Now, we show the consistency of ψgenie( eDA). Due to the law of large numbers, as n →∞, we
obtain that"
NXI,0.8307692307692308,"lim
n→∞ψgenie( eDA) = lim
n→∞
1
n n
X"
NXI,0.8326923076923077,"i=1
(1 −(˜yi)ci)"
NXI,0.8346153846153846,= E[1 −eYC]
NXI,0.8365384615384616,"= E[E[1 −eYC|Y]]
= E[1 −YC]
= Pe,
(40)"
NXI,0.8384615384615385,which demonstrates the consistency of ψgenie( eDA).
NXI,0.8403846153846154,We now compute the variance of ψgenie( eDA). We have that
NXI,0.8423076923076923,"ψgenie( eDA) = 1 n n
X"
NXI,0.8442307692307692,"i=1
(1 −(˜yi)ci) = 1 n n
X"
NXI,0.8461538461538461,"i=1
(1 −(yi)ci −(zi)ci)"
NXI,0.8480769230769231,"= ψsoft(D) −1 n n
X"
NXI,0.85,"i=1
(zi)ci.
(41)"
NXI,0.8519230769230769,This yields
NXI,0.8538461538461538,"Var

ψgenie( eDA)

= Var "
NXI,0.8557692307692307,"ψsoft(D) −1 n n
X"
NXI,0.8576923076923076,"i=1
Zci !"
NXI,0.8596153846153847,"(a)
= Var (ψsoft(D)) + 1"
NXI,0.8615384615384616,nVar (Z)
NXI,0.8634615384615385,"(b)
= Var(YM:M) n
+ 1"
NXI,0.8653846153846154,"nVar (Z) ,
(42)"
NXI,0.8673076923076923,"where (a) follows from the independence between Y and Z, and the fact that Zj’s (with j ∈[M])
are i.i.d.; and (b) follows by Proposition 1."
NXI,0.8692307692307693,"Finally, to show the asymptotic normality, we observe that from (41), we have that"
NXI,0.8711538461538462,"ψgenie( eDA) = 1 n n
X"
NXI,0.8730769230769231,"i=1
(1 −(yi)ci −(zi)ci) = 1 n n
X i=1"
NXI,0.875,"
1 −max
j∈[M](yi)j −(zi)ci  d= 1 n n
X i=1"
NXI,0.8769230769230769,"
1 −max
j∈[M](yi)j −(zi)1"
NXI,0.8788461538461538,"
,
(43)"
NXI,0.8807692307692307,"where the last equality follows since (zi)k
d= (zi)ℓfor all (k, ℓ) ∈[M]2. Since (43) is the sample
mean of n realizations from 1 −YM:M −Z, the central limit theorem leads to
√n(ψgenie( eDA) −Pe)
d→N(0, Var(1 −YM:M −Z))"
NXI,0.8826923076923077,"d= N(0, Var(YM:M) + Var(Z)),
(44)"
NXI,0.8846153846153846,"where the last equality is due to the independence between YM:M and Z. This concludes the proof of
Proposition 2."
NXI,0.8865384615384615,"C
Details on existing BER estimators / Additional experiments"
NXI,0.8884615384615384,"C.1
Details on existing BER estimators"
NXI,0.8903846153846153,"C.1.1
k-NN BER bounds [16]"
NXI,0.8923076923076924,"Let P NN
e
be the error rate of the 1-nearest neighbor (NN) classifier with n samples. Then, for binary
classification, it follows that for n →∞[16], 1
2"
NXI,0.8942307692307693,"
1 −
q"
NXI,0.8961538461538462,"1 −2P NN
e"
NXI,0.8980769230769231,"
≤Pe ≤P NN
e
.
(45)"
NXI,0.9,"The above bounds were proved by showing the convergence of the conditional posterior probability
of the nearest neighbor to the true conditional posterior probability. For details, we refer an interested
reader to [16]."
NXI,0.9019230769230769,"To generalize (45) to M-classification problems, it was shown that for n →∞[16], M −1 M  1 − r"
NXI,0.9038461538461539,"1 −
M
M −1P NN
e !"
NXI,0.9057692307692308,"≤Pe ≤P NN
e
.
(46)"
NXI,0.9076923076923077,"In our experiments, we chose k = 1 as this value provides the tightest bounds among larger values
of k [16, 72], and we plotted the upper and lower bounds defined in (46). In particular, in order to
evaluate P NN
e
on given data samples, we generated true one-hot labels for all data samples from their
soft labels by taking the index having the maximum value in the soft labels."
NXI,0.9096153846153846,"C.1.2
Generalized Henze-Penrose (GHP) divergence BER bounds [75]"
NXI,0.9115384615384615,"Consider a parameter p ∈(0, 1) and two distributions f0 and f1. Then, the Henze-Penrose (HP)
divergence between f0 and f1 is defined as,"
NXI,0.9134615384615384,"Dp(f0, f1) =
1
4p(1 −p)"
NXI,0.9153846153846154,Z (pf0(x) −(1 −p)f1(x))2
NXI,0.9173076923076923,"pf0(x) + (1 −p)f1(x) dx −(2p −1)2

.
(47)"
NXI,0.9192307692307692,"An appealing feature of Dp is that it can be estimated directly from the data using the generalized
Friedman-Rafsky (FR) statistic [28, 75], which uses the Euclidean minimal spanning tree."
NXI,0.9211538461538461,"Then, for f0 and f1 with prior probabilities p and 1 −p, the BER is bounded as [6],"
NXI,0.9230769230769231,"1
2 −1 2 q"
NXI,0.925,"up(f0, f1) ≤Pe ≤1 2 −1"
NXI,0.926923076923077,"2up(f0, f1),
(48) where"
NXI,0.9288461538461539,"up(f0, f1) = 4p(1 −p)Dp(f0, f1) + (2p −1)2,
(49)"
NXI,0.9307692307692308,"and Dp(f0, f1) can be estimated based on the minimal spanning tree [6]."
NXI,0.9326923076923077,"To generalize the HP bounds in (48), consider an M-class classification problem with p1, . . . , pM
as class prior probabilities and class conditional probability densities given by fk(x) := f(x|c =
k), k = 1, . . . , M. Let f (M)(x) := PM
k=1 pkfk(x). Then, the generalized Henze-Penrose (GHP)-
integral is defined as,"
NXI,0.9346153846153846,"GHP(M)(fi, fj) =
Z S"
NXI,0.9365384615384615,fi(x)fj(x)
NXI,0.9384615384615385,"f (M)(x) dx,
(50)"
NXI,0.9403846153846154,"where S is the support of f (M)(x). Let δ(M)
i,j
:=
R pipjfi(x)fj(x)"
NXI,0.9423076923076923,"f (M)(x)
dx. Then, it was shown in [75] that
the BER upper bound is Pe ≤2 M−1
X i=1 M
X"
NXI,0.9442307692307692,"j=i+1
δ(M)
i,j ,
(51)"
NXI,0.9461538461538461,and the BER lower bound is
NXI,0.948076923076923,Pe ≥M −1 M 
NXI,0.95,"
1 − "
NXI,0.9519230769230769,"1 −
2M
M −1 M−1
X i=1 M
X"
NXI,0.9538461538461539,"j=i+1
δ(M)
i,j   1
2 "
NXI,0.9557692307692308,"
.
(52)"
NXI,0.9576923076923077,"In our experiments, we plotted the GHP BER upper and lower bounds defined in (51) and (52).
Similar to the k-NN BER bounds, we generated true one-hot labels for all data samples before
evaluating the GHP divergence Dp. Computing the above bounds requires to evaluate δ(M)
i,j
and this
can be done by using the generalized FR statistics (for more details, see [75, Section IV.])."
NXI,0.9596153846153846,"C.2
Additional experiments"
NXI,0.9615384615384616,Figure 5: Misclassification error rates of several classifiers on CIFAR-10 and CIFAR-10H datasets.
NXI,0.9634615384615385,"In Figure 5, we compare the estimated BER with the misclassification error rate of a few popular
neural network models on the CIFAR-10 and CIFAR-10H datasets. We chose ResNet [40], VGG [78],"
NXI,0.9653846153846154,"(a) Soft labels.
(b) Noisy soft labels."
NXI,0.9673076923076923,"Figure 6:
Comparison of MoBK(ψC) with the GHP bounds and the k-NN bounds with
k
=
1.
The Gaussian samples are generated according to N(c, σ2I2), where c
∈
{(1, 1), (−1, 1), (−1, −1), (1, −1)} is equally likely. We consider Z ∼N
 
0, 1"
NXI,0.9692307692307692,"5I4

. MoBK(ψC)
uses K = ⌊√n⌋, the Euclidean distance for d, and r = 1/5. We iterate the experiments 10 times and
plot the average of them."
NXI,0.9711538461538461,"DenseNet [43], and Vision Transformer (ViT) [24] as classifiers to compare with and we trained
these models on the CIFAR-10 dataset.11 For the error rate evaluated on the CIFAR-10H dataset, we
followed the same experiment setup as in [46], under which we generated the ground-truth labels
for testset images from the soft labels (i.e., the probabilities for each class being associated with the
input image) and evaluated the testset error rate. We iterated this procedure 20 times and plotted
the average of the error rates. Figure 5 shows that the error rates on CIFAR-10 from the models are
worse than the estimated BER, with the exception of ViT. As noted in [46], the reasons for having
a lower error rate for ViT than the estimated BER are mainly due to 1) testset over-fitting and 2)
various labeling difficulties on the dataset CIFAR-10H that is used for estimating the BER. After
compensating for such difficulties on CIFAR-10H, all models’ error rates (evaluated on CIFAR-10H)
are worse than the estimated BER; this suggests that the estimated BER may be a good estimation
since the error rate of any classifier is larger than the BER."
NXI,0.9730769230769231,"Figure 6 shows the comparison of MoBK(ψC) with the GHP and k-NN bounds on the BER. For each
class c ∈{(1, 1), (−1, 1), (−1, −1), (1, −1)}, we generated 500 Gaussian samples N(c, σ2I2) to
have a total of 2, 000 samples. The label noise was generated according to Z ∼N(0, 1"
NXI,0.975,"5I4). The case
of no outliers is considered in this experiment. We observe that our BER estimator outperforms the
other estimators over σ2 ∈[0.1, 2] in both cases of noiseless soft labels and noisy soft labels."
NXI,0.9769230769230769,"We also made performance comparisons varying different parameters (i.e., µ, σ2, r) in Figures 7, 8, 9,
and 10. Similar to the experiment setting for Figure 6, we generated a total of 2, 000 Gaussian
samples with different parameters that are specified in the caption of each figure. The effect of µ
and σ2 on the BER estimation is shown in Figures 7 and 8 under different choices of the radius r.
We also provide performance comparisons of our estimator with the other estimators for different
values of the noise power σ2
Z in Figures 9 and 10. Choosing an appropriate value for r is critical
in the estimation of the BER as shown in Figure 10. An appropriate value for r can be empirically
determined by using the elbow method [84], which is commonly used for choosing the number of
clusters in k-means clustering algorithms. Using Figure 10, we selected r = 0.2 (elbow point) for the
main experiment in Figure 2. Overall, from these figures, we observe that our estimator outperforms
the GHP and the 1-NN classifier BER bounds in various settings."
NXI,0.9788461538461538,"11We fine-tuned the official ViT-16B model (pre-trained on imagenet21k) on CIFAR-10 dataset; for this we
used the implementation in [48]. Moreover, we trained the other models using the implementation in [70]."
NXI,0.9807692307692307,"(a) r = 0.3.
(b) r = 0.4."
NXI,0.9826923076923076,"Figure 7:
Comparison of MoBK(ψC) with the GHP bounds and the k-NN bounds with
k
=
1.
The Gaussian samples are generated according to N(c, I2),
where c
∈
{(µ, µ), (−µ, µ), (−µ, −µ), (µ, −µ)} with equal probability.
We consider Z ∼N
 
0, 1"
NXI,0.9846153846153847,"5I4

.
MoBK(ψC) uses K = ⌊√n⌋, the Euclidean distance for d, and r ∈{0.3, 0.4}. We iterate the
experiments 10 times and plot the average of them."
NXI,0.9865384615384616,"(a) r = 0.3.
(b) r = 0.4."
NXI,0.9884615384615385,"Figure 8:
Comparison of MoBK(ψC) with the GHP bounds and the k-NN bounds with
k
=
1.
The Gaussian samples are generated according to N(c, σ2I2), where c
∈
{(1, 1), (−1, 1), (−1, −1), (1, −1)} with equal probability.
We consider Z
∼
N
 
0, 1"
NXI,0.9903846153846154,"5I4

.
MoBK(ψC) uses K = ⌊√n⌋, the Euclidean distance for d, and r ∈{0.3, 0.4}. We iterate the
experiments 10 times and plot the average of them. As expected, a larger value of r in Figure 8b
yields better performance than a smaller value of r in Figure 8a."
NXI,0.9923076923076923,"(a) σ2
Z = 0.3.
(b) σ2
Z = 0.4."
NXI,0.9942307692307693,"Figure 9:
Comparison of MoBK(ψC) with the GHP bounds and the k-NN bounds with
k
=
1.
The Gaussian samples are generated according to N(c, I2),
where c
∈
{(µ, µ), (−µ, µ), (−µ, −µ), (µ, −µ)} with equal probability.
We consider Z ∼N
 
0, σ2
ZI4

.
MoBK(ψC) uses K = ⌊√n⌋, the Euclidean distance for d, and r = 0.2. We iterate the experi-
ments 10 times and plot the average of them. In both figures, the performance of our BER estimator
at the extreme values of µ is worse than the performance of the other estimators; this is due to an
inappropriate choice of r."
NXI,0.9961538461538462,"(a) σ2
Z = 0.3.
(b) σ2
Z = 0.4."
NXI,0.9980769230769231,"Figure 10: Comparison of MoBK(ψC) with the GHP bounds and the k-NN bounds with k = 1
with respect to the cluster radius r. The Gaussian samples are generated according to N(c, I2),
where c ∈{(1, 1), (−1, 1), (−1, −1), (1, −1)}. We consider Z ∼N
 
0, σ2
ZI4

. MoBK(ψC) uses
K = ⌊√n⌋and the Euclidean distance for d. We iterate the experiments 10 times and plot the
average of them."
