Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0008984725965858042,"Visual geo-localization demands in-depth knowledge and advanced reasoning skills
1"
ABSTRACT,0.0017969451931716084,"to associate images with real-world geographic locations precisely. In general,
2"
ABSTRACT,0.0026954177897574125,"traditional methods based on data-matching are hindered by the impracticality
3"
ABSTRACT,0.0035938903863432167,"of storing adequate visual records of global landmarks. Recently, Large Vision-
4"
ABSTRACT,0.004492362982929021,"Language Models (LVLMs) have demonstrated the capability of geo-localization
5"
ABSTRACT,0.005390835579514825,"through Visual Question Answering (VQA), enabling a solution that does not
6"
ABSTRACT,0.006289308176100629,"require external geo-tagged image records. However, the performance of a single
7"
ABSTRACT,0.0071877807726864335,"LVLM is still limited by its intrinsic knowledge and reasoning capabilities. Along
8"
ABSTRACT,0.008086253369272238,"this line, in this paper, we introduce a novel visual geo-localization framework
9"
ABSTRACT,0.008984725965858042,"called smileGeo that integrates the inherent knowledge of multiple LVLM agents
10"
ABSTRACT,0.009883198562443846,"via inter-agent communication to achieve effective geo-localization of images.
11"
ABSTRACT,0.01078167115902965,"Furthermore, our framework employs a dynamic learning strategy to optimize the
12"
ABSTRACT,0.011680143755615454,"communication patterns among agents, reducing unnecessary discussions among
13"
ABSTRACT,0.012578616352201259,"agents and improving the efficiency of the framework. To validate the effectiveness
14"
ABSTRACT,0.013477088948787063,"of the proposed framework, we construct GeoGlobe, a novel dataset for visual geo-
15"
ABSTRACT,0.014375561545372867,"localization tasks. Extensive testing on the dataset demonstrates that our approach
16"
ABSTRACT,0.015274034141958671,"significantly outperforms state-of-the-art methods. The source code is available at
17"
ABSTRACT,0.016172506738544475,"https://anonymous.4open.science/r/ViusalGeoLocalization-F8F5/ and the dataset
18"
ABSTRACT,0.017070979335130278,"will also be released after the paper is accepted.
19"
INTRODUCTION,0.017969451931716084,"1
Introduction
20"
INTRODUCTION,0.018867924528301886,"Visual geo-localization, referred to the task of estimating geographical identification for a given
21"
INTRODUCTION,0.019766397124887692,"image, is vital in various fields such as human mobility analysis [1, 2, 3, 4, 5] and robotic navigation
22"
INTRODUCTION,0.020664869721473494,"[6, 7, 8, 9, 10, 11]. In general, accurate visual geo-localization without the help of any localization
23"
INTRODUCTION,0.0215633423180593,"equipment (e.g., GPS sensors) is a complex task that requires abundant geospatial knowledge and
24"
INTRODUCTION,0.022461814914645103,"strong reasoning capabilities. Traditional methods [12, 13, 14, 15] typically formulate it as an image
25"
INTRODUCTION,0.02336028751123091,"retrieval problem where to geo-localize the given image by retrieving similar images with known
26"
INTRODUCTION,0.02425876010781671,"geographical locations. Thus, their effectiveness is limited by the scope and quality of the geo-tagged
27"
INTRODUCTION,0.025157232704402517,"image records.
28"
INTRODUCTION,0.02605570530098832,"Recently, the success of Large Vision-Language Models (LVLMs) has enabled Visual Question
29"
INTRODUCTION,0.026954177897574125,"Answering (VQA) to become a unified paradigm for multi-modal problems [16, 17], providing a
30"
INTRODUCTION,0.027852650494159928,"novel solution for visual geo-localization without the need for external geo-tagged image records.
31"
INTRODUCTION,0.028751123090745734,"However, the performance of a single LVLM on the geo-localization task is still limited by its
32"
INTRODUCTION,0.029649595687331536,"inherent geospatial knowledge and reasoning capabilities. Along this line, in this paper, we introduce
33"
INTRODUCTION,0.030548068283917342,"a novel multi-agent framework, named swarm intelligence Geo-localization (smileGeo), which
34"
INTRODUCTION,0.031446540880503145,"aims to adaptively integrate the inherent knowledge and reasoning capabilities of multiple LVLMs
35"
INTRODUCTION,0.03234501347708895,"to effectively and efficiently geo-localize images. Specifically, for a given image, the framework
36"
INTRODUCTION,0.03324348607367475,"initially elects K suitable LVLM agents as answer agents for initial location analysis. Then, each
37"
INTRODUCTION,0.034141958670260555,"answer agent chooses several review agents via an adaptive social network, which imitates the
38"
INTRODUCTION,0.03504043126684636,"collaborative relationships between agents with a target on the visual geo-localization task, to
39"
INTRODUCTION,0.03593890386343217,"discuss and share their knowledge for refining its location analysis. Finally, our framework conducts
40"
INTRODUCTION,0.036837376460017966,"free discussion among all of the answer agents to reach a consensus. Besides, we also design
41"
INTRODUCTION,0.03773584905660377,"a novel dynamic learning strategy to optimize the election mechanism along with the adaptive
42"
INTRODUCTION,0.03863432165318958,"collaboration social network of agents. We hope that by the effectiveness of the election mechanism
43"
INTRODUCTION,0.039532794249775384,"and the review mechanism, our framework can discover the mode of communication among agents,
44"
INTRODUCTION,0.04043126684636118,"thereby enhancing geo-localization performance through multi-agent collaboration while minimizing
45"
INTRODUCTION,0.04132973944294699,"unnecessary discussions. In summary, our contributions are demonstrated as follows:
46"
INTRODUCTION,0.042228212039532795,"• A novel swarm intelligence geo-localization framework, smileGeo, is proposed to adaptively
47"
INTRODUCTION,0.0431266846361186,"integrate the inherent knowledge and reasoning capability of multiple LVLMs through
48"
INTRODUCTION,0.0440251572327044,"discussion for visual geo-localization tasks.
49"
INTRODUCTION,0.044923629829290206,"• A dynamic learning strategy is introduced to discover the most appropriate discussion mode
50"
INTRODUCTION,0.04582210242587601,"among LVLM agents for enhancing the effectiveness and efficiency of the framework.
51"
INTRODUCTION,0.04672057502246182,"• A new visual geo-localization dataset named GeoGlobe1 is collected, containing a wide
52"
INTRODUCTION,0.047619047619047616,"variety of images globally. The diversity and richness of GeoGlobe allow us to evaluate
53"
INTRODUCTION,0.04851752021563342,"the performance of different models more accurately. Moreover, extensive experiments
54"
INTRODUCTION,0.04941599281221923,"demonstrate our competitive performance compared to state-of-the-art methods.
55"
INTRODUCTION,0.050314465408805034,"The remainder of this paper is organized as follows: Section 2 discusses the related literature. In
56"
INTRODUCTION,0.05121293800539083,"Section 3, the proposed framework is introduced. Section 4 provides the performance evaluation, and
57"
INTRODUCTION,0.05211141060197664,"Section 5 concludes the paper.
58"
RELATED WORK,0.053009883198562445,"2
Related Work
59"
RELATED WORK,0.05390835579514825,"Visual Geo-localization. Recent research in visual geo-localization, commonly referred to as
60"
RELATED WORK,0.05480682839173405,"geo-tagging, primarily focuses on developing image retrieval systems to address this challenge
61"
RELATED WORK,0.055705300988319856,"[3, 18, 19, 20, 21, 22]. These systems utilize learned embeddings generated by a feature extraction
62"
RELATED WORK,0.05660377358490566,"backbone, which includes an aggregation or pooling mechanism [23, 24, 25, 26]. However, the
63"
RELATED WORK,0.05750224618149147,"applicability of these retrieval systems to globally geo-localize landmarks or natural attractions is
64"
RELATED WORK,0.05840071877807727,"often limited by the constraints of the available database knowledge and the restrictions imposed by
65"
RELATED WORK,0.05929919137466307,"national or regional geo-data protection laws. Alternatively, some studies treat visual geo-localization
66"
RELATED WORK,0.06019766397124888,"as a classification problem [27, 28, 29, 30]. These approaches posit that two images from the same
67"
RELATED WORK,0.061096136567834684,"geographical region, despite depicting different scenes, typically share common semantic features.
68"
RELATED WORK,0.06199460916442048,"Practically, these methods organize the geographical area into discrete cells and categorize the
69"
RELATED WORK,0.06289308176100629,"image database accordingly. This cell-based categorization facilitates scaling the problem globally,
70"
RELATED WORK,0.0637915543575921,"provided the number of categories remains manageable. However, while the number of countries
71"
RELATED WORK,0.0646900269541779,"globally remains relatively constant, accurately enumerating cities in real-time at a global scale is
72"
RELATED WORK,0.06558849955076371,"challenging due to frequent administrative changes, such as city reorganizations or mergers, which
73"
RELATED WORK,0.0664869721473495,"reflect shifts in national policies. Additionally, in the context of globalization, this strategy has
74"
RELATED WORK,0.0673854447439353,"inherent limitations. The recent advent of LVLMs offers promising compensatory mechanisms for
75"
RELATED WORK,0.06828391734052111,"the deficiencies observed in traditional geo-localization methodologies, making the exploration of
76"
RELATED WORK,0.06918238993710692,"LVLM-based approaches significantly relevant in current research.
77"
RELATED WORK,0.07008086253369272,"Multi-agent Framework for LLM/LVLMs. LLM/LVLM agents have demonstrated the potential
78"
RELATED WORK,0.07097933513027853,"to act like human [31, 32, 33], and a large number of studies have focused on developing robust
79"
RELATED WORK,0.07187780772686433,"architectures for collaborative LLM/LVLM agents [34, 35, 36, 37, 38]. These architectures enable
80"
RELATED WORK,0.07277628032345014,"each LLM/LVLM agent that endows with unique capabilities to engage in debates or discussions.
81"
RELATED WORK,0.07367475292003593,"For instance, [34] proposes an approach to aggregate multiple LLM/LVLM responses by generating
82"
RELATED WORK,0.07457322551662174,"candidate responses from various LLM/LVLM in a single round and employing pairwise ranking to
83"
RELATED WORK,0.07547169811320754,"synthesize the most effective response. While some studies [34] utilize a static architecture potentially
84"
RELATED WORK,0.07637017070979335,"limiting the performance and generalization of LLM/LVLM, others like [38] have implemented
85"
RELATED WORK,0.07726864330637916,"dynamic interaction architectures that adjust according to the query and incorporate user feedback.
86"
RELATED WORK,0.07816711590296496,"1Because GeoGlobe is relatively large (about 32GB), we are unable to provide it as an attachment during the
double-blind review stage. We will publish it once the paper is accepted."
RELATED WORK,0.07906558849955077,"Recent advancements also demonstrate the augmentation of LLM/LVLM as autonomous agents
87"
RELATED WORK,0.07996406109613657,"capable of utilizing external tools to address challenges in interactive settings. These techniques
88"
RELATED WORK,0.08086253369272237,"include retrieval augmentation [39, 40, 41], mathematical tools [40, 42, 43], and code interpreters
89"
RELATED WORK,0.08176100628930817,"[44, 45]. With these capabilities, LLM/LVLMs are well-suited for various tasks, especially for
90"
RELATED WORK,0.08265947888589398,"geo-localization. However, most LLM/LVLM agent frameworks mandate participation from all
91"
RELATED WORK,0.08355795148247978,"agents in at least one interaction round, leading to significant computational overhead. To address
92"
RELATED WORK,0.08445642407906559,"this issue, our framework introduces a dynamic learning strategy electing only a small number of
93"
RELATED WORK,0.0853548966756514,"agents to geo-localize different images, which significantly enhances the efficiency of LLM/LVLM
94"
RELATED WORK,0.0862533692722372,"agents by reducing unnecessary interactions.
95"
METHODOLOGY,0.08715184186882301,"3
Methodology
96"
METHODOLOGY,0.0880503144654088,"In this section, we first present the overall framework and then introduce each part of smileGeo in
97"
METHODOLOGY,0.0889487870619946,"detail for geo-localization tasks.
98"
MODEL OVERVIEW,0.08984725965858041,"3.1
Model Overview
99"
MODEL OVERVIEW,0.09074573225516622,"In this paper, we denote the social network of LVLM agents by G, where G = {V, E}.V stands for the
100"
MODEL OVERVIEW,0.09164420485175202,"agent set and E presents the edge set. Each agent vi ∈V, i ∈[N] is an LVLM, which is pre-trained
101"
MODEL OVERVIEW,0.09254267744833783,"by massive vision-language data and can infer the possible location Y of a given image X. Besides,
102"
MODEL OVERVIEW,0.09344115004492363,"each edge eij ∈E, i, j ∈[N] is the connection weighted by the improvement effect of agent vi to
103"
MODEL OVERVIEW,0.09433962264150944,"agent vj via discussion regarding the geo-localization performance.
104"
MODEL OVERVIEW,0.09523809523809523,"As illustrated in Figure 1, smileGeo contains the process of the review mechanism in agent discussions
105"
MODEL OVERVIEW,0.09613656783468104,"along with a dynamic learning strategy of agent social networks:
106"
MODEL OVERVIEW,0.09703504043126684,"The review mechanism in agent discussions is a 3-stage anonymous collaboration approach to allow
107"
MODEL OVERVIEW,0.09793351302785265,"LVLM agents to reach a consensus via discussion. In the first stage, for a given image X, our
108"
MODEL OVERVIEW,0.09883198562443846,"framework elects the most suitable K agents as answer agents by agent election probability Lst. In
109"
MODEL OVERVIEW,0.09973045822102426,"the second stage, these answer agents respectively select R review agents by the adaptive collaboration
110"
MODEL OVERVIEW,0.10062893081761007,"social network A to refine their answer via discussion. Finally, our framework facilitates consensus
111"
MODEL OVERVIEW,0.10152740341419586,"among all agents through open discussion to reach a final answer. Both Lst and A are analyzed
112"
MODEL OVERVIEW,0.10242587601078167,"from the given image X, allowing our framework to minimize unnecessary discussions, thereby
113"
MODEL OVERVIEW,0.10332434860736747,"significantly enhancing its efficiency while maintaining its accuracy. Moreover, the multi-stage
114"
MODEL OVERVIEW,0.10422282120395328,"discussion facilitates communication among agents, maximizing the integration of their knowledge
115"
MODEL OVERVIEW,0.10512129380053908,"and reasoning abilities to generate an accurate response Y .
116"
MODEL OVERVIEW,0.10601976639712489,"To get Lst and A, we specifically design a dynamic learning module, which initially deploys the
117"
MODEL OVERVIEW,0.1069182389937107,"encoder component of a pre-trained image variational autoencoder (VAE) to extract features from the
118"
MODEL OVERVIEW,0.1078167115902965,"given image X. The extracted features, combined with agent embeddings Emb, are employed to
119"
MODEL OVERVIEW,0.1087151841868823,"determine the suitability of agents w.r.t. Lst for agent discussions and predict agent collaboration
120"
MODEL OVERVIEW,0.1096136567834681,"connections A in the geo-localization task.
121"
REVIEW MECHANISM IN AGENT DISCUSSIONS,0.1105121293800539,"3.2
Review Mechanism in Agent Discussions
122"
REVIEW MECHANISM IN AGENT DISCUSSIONS,0.11141060197663971,"LLM/LVLM have demonstrated remarkable capabilities in complicated tasks and some pioneering
123"
REVIEW MECHANISM IN AGENT DISCUSSIONS,0.11230907457322552,"works have further proven that the performances can be further enhanced by ensembling multiple
124"
REVIEW MECHANISM IN AGENT DISCUSSIONS,0.11320754716981132,"LLM/LVLM agents. Thus, to improve the geo-localization capability of LVLMs, we propose a
125"
REVIEW MECHANISM IN AGENT DISCUSSIONS,0.11410601976639713,"cooperation framework to effectively integrate the diverse knowledge and reasoning abilities of
126"
REVIEW MECHANISM IN AGENT DISCUSSIONS,0.11500449236298294,"multiple LVLMs. Inspired by the fact that community review mechanisms can improve the quality of
127"
REVIEW MECHANISM IN AGENT DISCUSSIONS,0.11590296495956873,"manuscripts, an iterative 3-stage anonymous reviewing mechanism is proposed for helping agents
128"
REVIEW MECHANISM IN AGENT DISCUSSIONS,0.11680143755615453,"share knowledge and reasoning capability with each other through their collaboration social network:
129"
REVIEW MECHANISM IN AGENT DISCUSSIONS,0.11769991015274034,"i) answer agent election & answering, ii) review agent selection & reviewing, and iii) final answer
130"
REVIEW MECHANISM IN AGENT DISCUSSIONS,0.11859838274932614,"conclusion.
131"
REVIEW MECHANISM IN AGENT DISCUSSIONS,0.11949685534591195,"Stage 1: Answer Agent Election & Answering
132"
REVIEW MECHANISM IN AGENT DISCUSSIONS,0.12039532794249776,"Initially, we select K agents with the highest agent election probabilities Lst as answer agents and
133"
REVIEW MECHANISM IN AGENT DISCUSSIONS,0.12129380053908356,"let them geo-localize independently as the preliminary step for further discussion. By initiating
134"
REVIEW MECHANISM IN AGENT DISCUSSIONS,0.12219227313566937,"the discussion with a limited number of agents, we aim to reduce potential chaos and maintain the
135"
REVIEW MECHANISM IN AGENT DISCUSSIONS,0.12309074573225516,"efficiency of our framework as the number of participating agents increases.
136"
REVIEW MECHANISM IN AGENT DISCUSSIONS,0.12398921832884097,Dynamic Learning Strategy of Agent Collaboration Social Networks
REVIEW MECHANISM IN AGENT DISCUSSIONS,0.12488769092542677,Review Mechanism in Agent Discussions
REVIEW MECHANISM IN AGENT DISCUSSIONS,0.12578616352201258,"Stage 1 
Answer Agent Election & Answering
Stage 2
Review Agent Selection & Reviewing
Stage 3
Final Answer Conclusion"
REVIEW MECHANISM IN AGENT DISCUSSIONS,0.12668463611859837,"It’s located in ...
A!"
REVIEW MECHANISM IN AGENT DISCUSSIONS,0.1275831087151842,Some one says it’s located  ...
REVIEW MECHANISM IN AGENT DISCUSSIONS,0.12848158131176998,"I agree ...
R!!
I disagree ...
R!"""
REVIEW MECHANISM IN AGENT DISCUSSIONS,0.1293800539083558,"Here are others’ comments:
(1) ...            (2) ...
 What do you think?"
REVIEW MECHANISM IN AGENT DISCUSSIONS,0.1302785265049416,"Please answer the location by this format 
<location> <confidence> <explain>..."
REVIEW MECHANISM IN AGENT DISCUSSIONS,0.13117699910152741,"In summary, ...
Y A!"
REVIEW MECHANISM IN AGENT DISCUSSIONS,0.1320754716981132,"A""
Top-K Agent Election (K=2) A! A"" A! A"" R!! R!"" A! A"""
REVIEW MECHANISM IN AGENT DISCUSSIONS,0.132973944294699,"R""!
R"""" A! A"""
REVIEW MECHANISM IN AGENT DISCUSSIONS,0.13387241689128482,"R!!,""""
R""! R!"""
REVIEW MECHANISM IN AGENT DISCUSSIONS,0.1347708894878706,Agent Social Network Learning
REVIEW MECHANISM IN AGENT DISCUSSIONS,0.13566936208445643,"Random
Walk R = 2"
REVIEW MECHANISM IN AGENT DISCUSSIONS,0.13656783468104222,"It’s located in ...
A"""
REVIEW MECHANISM IN AGENT DISCUSSIONS,0.13746630727762804,"Loss = 
ˆ
( ,
) (
gives an answer)
i N"
REVIEW MECHANISM IN AGENT DISCUSSIONS,0.13836477987421383,"v
i
i
v
å
Y Y
D
) ("
REVIEW MECHANISM IN AGENT DISCUSSIONS,0.13926325247079965,"ˆ
MSE( ,
)
+
A A
ˆ
MSE(
,
)
+
Lst Lst"
REVIEW MECHANISM IN AGENT DISCUSSIONS,0.14016172506738545,": It’s should located ...
A!"
"V
Y",0.14106019766397124,"1v
Y"
"V
Y",0.14195867026055706,": It’s must located ...
A"""
"V
Y",0.14285714285714285,"3v
Y ..."
"V
Y",0.14375561545372867,"...
0.95 0.93 0.89 ..."
"V
Y",0.14465408805031446,"Choose?
..."
"V
Y",0.14555256064690028,"Optimal Agent
Social Connections A"
"V
Y",0.14645103324348607,"Agent Election 
 Probability Lst"
"V
Y",0.14734950584007186,"Initialized Agent 
 Social Network"
"V
Y",0.14824797843665768,Image X GNN
"V
Y",0.14914645103324348,"VAE 
Encoder (0)
G A"
"V
Y",0.1500449236298293,"The ground truth is ...
ˆY"
"V
Y",0.1509433962264151,"Agent Social Network Updating A! A"""
"V
Y",0.1518418688230009,"R""!
R"""" Weaken
Edges A! A"" R!! R!"""
"V
Y",0.1527403414195867,"Enhance
Edges A"""
"V
Y",0.15363881401617252,"3v
Y A!"
"V
Y",0.1545372866127583,"1v
Y Lst"
"V
Y",0.1554357592093441,"v!
v""
..."
"V
Y",0.15633423180592992,"...
100%
0% v# 50%
P"
"V
Y",0.15723270440251572,Correct!
"V
Y",0.15813117699910154,Wrong! : ... : ...
"V
Y",0.15902964959568733,"K Agent Election v! v"" v# v$ v%"
"V
Y",0.15992812219227315,Pv!  0.95
"V
Y",0.16082659478885894,"Pv""  0.93"
"V
Y",0.16172506738544473,Pv#  0.89
"V
Y",0.16262353998203055,Pv$  0.75
"V
Y",0.16352201257861634,Pv%  0.69
"V
Y",0.16442048517520216,"Choose?
0.51<Pv!"
"V
Y",0.16531895777178796,"0.94>Pv"""
"V
Y",0.16621743036837378,0.72<Pv# ... ... ...
"V
Y",0.16711590296495957,"rand
( )l
Lst ... ..."
"V
Y",0.16801437556154536,Difference in Training
"V
Y",0.16891284815813118,"Initizalize:        ,
(0)
G
(0)
Lst A! A"""
"V
Y",0.16981132075471697,"R!!,""""
R""! R!"""
"V
Y",0.1707097933513028,Feedback
"V
Y",0.17160826594788858,Some one says it’s located  ...
"V
Y",0.1725067385444744,"I agree ...
R!!
I disagree ...
R!"""
"V
Y",0.1734052111410602,"A!
A""
Maximum Z Rounds 
of Discussion"
"V
Y",0.17430368373764601,": It’s should located ...
A!"
"V
Y",0.1752021563342318,"1v
Y"
"V
Y",0.1761006289308176,": It’s must located ...
A"" 3v3
Y"
"V
Y",0.17699910152740342,"I don’t think so. It has ...
A!"
"V
Y",0.1778975741239892,"I agree with you opinion ...
A"""
"V
Y",0.17879604672057503,"We have a consensus ...
A! Lst G"
"V
Y",0.17969451931716082,"（l+1）
（l+1）"
"V
Y",0.18059299191374664,"Pv""
Pv!
Pv$"
"V
Y",0.18149146451033243,"Figure 1: The framework overview of smileGeo. It contains the process of review mechanism in
agent discussions along with a dynamic learning strategy of agent collaboration social networks. The
first part deploys a review mechanism for LVLMs to discuss and share their knowledge anonymously,
which could enhance the overall performance of geo-localization tasks. The second one mainly
utilizes the GNN-based learning module to improve efficiency by reducing unnecessary discussions
among agents while showing the process of updating the agent collaboration social network during
the training process."
"V
Y",0.18238993710691823,"After the answer agents are elected, we send the image X to all answer agents and let them give the
137"
"V
Y",0.18328840970350405,"primary analysis. Each answer must contain three parts: one location (city, country, and so on), one
138"
"V
Y",0.18418688230008984,"confidence (a percentage number), and a detailed explanation.
139"
"V
Y",0.18508535489667566,"Stage 2: Review Agent Selection & Reviewing
140"
"V
Y",0.18598382749326145,"In this stage, for each answer agent, we choose R review agents by performing a transfer-probability-
141"
"V
Y",0.18688230008984727,"based random walk on the agent collaboration social network G for answer reviewing. The transfer
142"
"V
Y",0.18778077268643306,"probability p(vi, vj) from node vi to node vj can be calculated as follows:
143"
"V
Y",0.18867924528301888,"p(vi, vj) ="
"V
Y",0.18957771787960467,"(
Aij
P"
"V
Y",0.19047619047619047,"k∈N (vi) Aik ,
if eij ∈E"
"V
Y",0.19137466307277629,"0,
otherwise
(1)"
"V
Y",0.19227313566936208,"where N(vi) is the 1-hop neighbor node set of node vi.
144"
"V
Y",0.1931716082659479,"For each selected review agent, it reviews the results as well as the explanations generated by the
145"
"V
Y",0.1940700808625337,"corresponding answer agent and gives its own comments. After that, each answer agent would
146"
"V
Y",0.1949685534591195,"summarize their preliminary analysis and the feedback from all of its review agents to get the final
147"
"V
Y",0.1958670260557053,"answer, which must include three parts as well: one location, one confidence, and an explain.
148"
"V
Y",0.1967654986522911,"Stage 3: Final Answer Conclusion
149"
"V
Y",0.1976639712488769,"In the previous stage, each answer agent produces a refined result based on feedback. When K > 1 in
150"
"V
Y",0.1985624438454627,"Stage 1, the proposed framework generates multiple independent results, which may not be consistent.
151"
"V
Y",0.19946091644204852,"However, we aim to provide a definitive answer rather than multiple options for people to choose
152"
"V
Y",0.20035938903863432,"from. To address this, we allow up to Z rounds of free discussion among those answer agents to
153"
"V
Y",0.20125786163522014,"reach a unified answer:
154"
"V
Y",0.20215633423180593,"First, we maintain a global dialog history list, diag, recording all replies agents respond. In addition,
155"
"V
Y",0.20305480682839172,"discussions are executed asynchronously, which means that any answer agent can always reply based
156"
"V
Y",0.20395327942497754,"on the latest diag, and replies would be added to the end of diag as soon as they are posted. Each
157"
"V
Y",0.20485175202156333,"answer agent is allowed to speak only once in each discussion round, and after Z rounds of free
158"
"V
Y",0.20575022461814915,"discussion, we determine the final result using a minority-majority approach, i.e., we choose the reply
159"
"V
Y",0.20664869721473494,"with the most agreement as the final conclusion. If all agents reach a consensus, we early stop this
160"
"V
Y",0.20754716981132076,"stage and adopt the consensus answer as the final answer. If none of any consensus is reached, we
161"
"V
Y",0.20844564240790656,"only select the reply of the first answer agent elected from Stage 1 as the final result.
162"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.20934411500449238,"3.3
Dynamic Learning Strategy of Agent Collaboration Social Networks
163"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.21024258760107817,"In our framework, choosing the appropriate answer agents and review agents for knowledge sharing
164"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.21114106019766396,"and discussion is vital to its effectiveness and efficiency. Therefore, we propose a dynamic learning
165"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.21203953279424978,"strategy to optimize them. Specifically, for each training sample, i.e., a geo-tagged image, we would
166"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.21293800539083557,"first estimate the optimal answer agent election probability
ˆ
Lst and the optimal collaboration social
167"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.2138364779874214,"network of agent ˆG by its actual location. Then we train an attention-based graph neural network,
168"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.21473495058400718,"which aims to predict Lst and G, by such estimated ground truth.
169"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.215633423180593,"To estimate the optimal
ˆ
Lst and ˆ
A for agents to geo-localize image X, we first initialize the agent
170"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.2165318957771788,"social network G(0) by a fully connected graph with the agent set V. Besides, we initialize the agent
171"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.2174303683737646,"election probability Lst(0) = [0.5, 0.5, · · · ], with all agents having 50% probability of being chose
172"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.2183288409703504,"as answer agents.
173"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.2192273135669362,"Then, we iteratively conduct our 3-stage discussion framework to get the prediction answer. Lst(l)
174"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.22012578616352202,"and G(l) is updated at the end of each round l ∈L by comparing the answers Y (l)
vi from each answer
175"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.2210242587601078,"agent with the ground truth ˆY .
176"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.22192273135669363,"After L rounds of agent discussions, the updated agent election probability for an image X,
ˆ
Lst :=
177"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.22282120395327942,"Lst(L)(X) = [P (L)
v1 , P (L)
v2 , · · · , P (L)
vN ], determines whether an agent vi gives the correct/wrong
178"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.22371967654986524,"answers Y (L)
vi by comparing it with the ground truth ˆY . Here, the definition of P (l)
vi of agent vi at
179"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.22461814914645103,"round l is as follows:
180"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.22551662174303683,"P (l)
vi := 

 
"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.22641509433962265,"0,
if D( ˆY , Y (l)
vi ) > th
1,
if D( ˆY , Y (l)
vi ) ≤th
1
2,
if vi did not participate in the discussion
(2)"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.22731356693620844,"where th is a pre-defined threshold for determining whether the predicted location is close enough
181"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.22821203953279426,"to the actual location. In the distance function D(·), we first deploy geocoding to convert natural
182"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.22911051212938005,"language into location intervals in a Web Mercator coordinate system (WGS84) by utilizing OSM
183"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.23000898472596587,"APIs, and then compute the shortest distance between two two location intervals.
184"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.23090745732255166,"Please note that, rather than electing the top-K answer agents in each round, we choose each agent
185"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.23180592991913745,"with probability Pvi during the training period to ensure that every agent has the opportunity to
186"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.23270440251572327,"participate in the discussion for more accurate estimation, as shown at the left part of the dynamic
187"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.23360287511230907,"learning strategy module of agent collaboration social networks in Figure 1.
188"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.23450134770889489,"In addition, the agent collaboration social network would also be updated by comparing the actual
189"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.23539982030548068,"location with the generated answer of each answer agent at the same time. For l-th round, we
190"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.2362982929020665,"strengthen the link between the correctly answered agent and the corresponding review agents while
191"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.2371967654986523,"weakening the link between the incorrectly answered agent and the corresponding review agents:
192"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.23809523809523808,"ˆ
Aij := A(l)
ij (X) ="
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.2389937106918239,"(
tt+1"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.2398921832884097,"2tt A(l−1)
ij
(X),
if agent vi answers correctly 2tt−1"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.2407906558849955,"2tt A(l−1)
ij
(X),
if agent vi answers incorrectly
(3)"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.2416891284815813,"where A(l−1)
ij
(X) is the weight of the connection between answer agent vi and review agent vj
193"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.24258760107816713,"at round l −1 when geo-locating image X, A(0)
ij (X) = 1, i ̸= j, A(0)
ii (X) = 0, i, j ∈[N], tt
194"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.24348607367475292,"is the number of consecutive times an agent has answered correctly, which is used to attenuate
195"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.24438454627133874,"the connection weights when updating them, preventing the performance of an agent on a certain
196"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.24528301886792453,"portion of the continuous dataset from interfering with the model’s evaluation of the current agent’s
197"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.24618149146451032,"performance on the entire dataset.
198"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.24707996406109614,"Then, we try to learn an attention-based graph neural network to predict the corresponding optimal
199"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.24797843665768193,"agent election probability Lst = h(X, G|Θ) and the optimal agent collaboration connections
200"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.24887690925426775,"A = f(X, V|Θ):
201"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.24977538185085355,"A = AttGNN(F ea, F ea, 1)"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.25067385444743934,= softmax
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.25157232704402516,"F ea · F ea⊤ √dk ! 1,"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.252470799640611,"Lst = σ′ (Linear (Flatten (σ (A · F ea · W )))) ,
F ea = Linear (Emb + VAEEnc(X)) , (4)"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.25336927223719674,"where W , Emb ∈Θ are two learnable parameters, Emb := [Embv1, Embv2, · · · ]⊤is the agent
202"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.25426774483378256,"embedding and W is the weight matrix, σ(·) is the LeakyReLU function, σ′(·) is the Sigmoid
203"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.2551662174303684,"function, VAEEnc(·) is the encoder of the image VAE that compresses and maps the image data
204"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.2560646900269542,"into the latent space. It is used to align the image features with the agent embedding, and dk is the
205"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.25696316262353996,"dimension of the F ea. Our learning target can be formalized as:
206"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.2578616352201258,"arg min
Θ N
X"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.2587601078167116,"i
D( ˆY , Y vi)1(vi gives an answer) + MSE( ˆ
Lst, Lst) + MSE( ˆ
A, A),
(5)"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.25965858041329737,"where D(·) denotes the distance between the places an LVLM agent answered and the ground truth,
207"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.2605570530098832,"1(·) is the indicator function, Y vi := Y (L)
vi
= gvi(X, Y (L−1)
vj
), gvi(·) represent the LVLM agent vi
208"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.261455525606469,"with fixed parameters and Y (0)
vi = gvi(X) is the answer that LVLM agent vi generates at the initial
209"
DYNAMIC LEARNING STRATEGY OF AGENT COLLABORATION SOCIAL NETWORKS,0.26235399820305483,"stage of discussion.
210"
EXPERIMENTS,0.2632524707996406,"4
Experiments
211"
EXPERIMENTS,0.2641509433962264,"To evaluate the performance of our framework, we conducted experiments on the real-world dataset
212"
EXPERIMENTS,0.26504941599281223,"that was gathered from the Internet to answer the following research questions:
213"
EXPERIMENTS,0.265947888589398,"• RQ1: Can smileGeo outperform state-of-the-art methods in open-ended geo-localization tasks?
214"
EXPERIMENTS,0.2668463611859838,"• RQ2: Are LVLM agents with diverse knowledge and reasoning abilities more suitable for building
215"
EXPERIMENTS,0.26774483378256964,"a collaboration social network of agents?
216"
EXPERIMENTS,0.26864330637915546,"• RQ3: How does the setting of hyperparameters affect the performance of smileGeo?
217"
EXPERIMENT SETUP,0.2695417789757412,"4.1
Experiment Setup
218"
EXPERIMENT SETUP,0.27044025157232704,"Datasets. In this paper, we newly construct a geo-localization dataset named GeoGlobe. It contains a
219"
EXPERIMENT SETUP,0.27133872416891286,"variety of man-made landmarks or natural attractions from nearly 150 countries with different cultural
220"
EXPERIMENT SETUP,0.2722371967654987,"and regional styles. The diversity and richness of GeoGlobe allow us to evaluate the performance of
221"
EXPERIMENT SETUP,0.27313566936208444,"different models more accurately. More details can be found in Appendix B.
222"
EXPERIMENT SETUP,0.27403414195867026,"Implemention Details. We select both open-source and close-source LVLMs with different scales
223"
EXPERIMENT SETUP,0.2749326145552561,"trained by different datasets as agents in the proposed framework. As for the open-source LVLMs,
224"
EXPERIMENT SETUP,0.27583108715184185,"we utilize several open-source fine-tuned LVLMs: Infi-MM2, Qwen-VL 3, vip–llava–7b&13b4, llava–
225"
EXPERIMENT SETUP,0.27672955974842767,"2https://huggingface.co/Infi-MM/infimm-zephyr
3https://huggingface.co/Qwen/Qwen-VL
4https://huggingface.co/llava-hf/vip-llava-xxx"
EXPERIMENT SETUP,0.2776280323450135,"1.5–7b–base&mistral&vicuna5, llava–1.6–7b&13b&34b–mistral&vicuna6, CogVLM7. As for the
226"
EXPERIMENT SETUP,0.2785265049415993,"closed-source LVLMs, we chose the models provided by three of the most famous companies in the
227"
EXPERIMENT SETUP,0.27942497753818507,"world: Claude–3–opus8, GPT–4V9, and Gemini–1.5–pro 10. Besides, 99% of images (about 290,000
228"
EXPERIMENT SETUP,0.2803234501347709,"samples) from the original dataset are randomly chosen as training samples. For the open-world
229"
EXPERIMENT SETUP,0.2812219227313567,"geolocation problem, we construct the test dataset using approximately 4,000 samples, of which
230"
EXPERIMENT SETUP,0.2821203953279425,"nearly 66.67% samples reflected different locations not present in the training dataset. More details
231"
EXPERIMENT SETUP,0.2830188679245283,"about the deployment of smileGeo and the related parameter settings can be found in Appendix C.
232"
EXPERIMENT SETUP,0.2839173405211141,"Baselines. In this work, we compare the proposed framework with three kinds of baselines: single
233"
EXPERIMENT SETUP,0.28481581311769993,"LVLMs, LLM/LVLM-based multi-agent frameworks, and image retrieval approaches. Firstly, we use
234"
EXPERIMENT SETUP,0.2857142857142857,"each LVLM alone as an agent directly for the geo-localization task and compute the performance of
235"
EXPERIMENT SETUP,0.2866127583108715,"these single LVLMs under the same dataset. In addition, we experiment with multi-agent collaborative
236"
EXPERIMENT SETUP,0.28751123090745734,"frameworks, including LLM-Blender [34], PHP [35], Reflexion [36], LLM Debate [37], and DyLAN
237"
EXPERIMENT SETUP,0.2884097035040431,"[38]. Finally, several state-of-the-art image retrieval approaches, including NetVLAD [3], GeM
238"
EXPERIMENT SETUP,0.2893081761006289,"[26], and CosPlace [46], are also used to be part of the baselines. We set the training dataset as the
239"
EXPERIMENT SETUP,0.29020664869721474,"geo-tagged image database of each image retrieval system and use images in the test dataset for the
240"
EXPERIMENT SETUP,0.29110512129380056,"retrieval system to generate answers.
241"
EXPERIMENT SETUP,0.2920035938903863,Evaluation Metrics. We use Accuracy (Acc) to evaluate the performance: Accuracy = Ncorrect
EXPERIMENT SETUP,0.29290206648697215,"Ntotal ,
242"
EXPERIMENT SETUP,0.29380053908355797,"where Ncorrect is the number of samples that the proposed framework correctly geo-localizes, and
243"
EXPERIMENT SETUP,0.29469901168014373,"Ntotal refers to the total number of testing samples.
244"
EXPERIMENT SETUP,0.29559748427672955,"In this paper, we first geo-encode the answers with the ground truth, i.e., we transform the addresses
245"
EXPERIMENT SETUP,0.29649595687331537,"described through natural language into latitude-longitude coordinates. Then, we calculate the
246"
EXPERIMENT SETUP,0.2973944294699012,"distance between the two coordinates. When the distance between the two coordinates is less than
247"
EXPERIMENT SETUP,0.29829290206648695,"th = 50km (city-level), we consider the answer of the framework to be correct.
248"
PERFORMANCE COMPARISON,0.2991913746630728,"4.2
Performance Comparison
249"
PERFORMANCE COMPARISON,0.3000898472596586,"We divide the baseline comparison experiment into three parts: i) comparison with single LVLMs,
250"
PERFORMANCE COMPARISON,0.30098831985624436,"ii) comparison with LLM/LVLM-based agent frameworks, and iii) comparison with image retrieval
251"
PERFORMANCE COMPARISON,0.3018867924528302,"systems.
252"
PERFORMANCE COMPARISON,0.302785265049416,Table 1: Results of different single LVLM baselines.
PERFORMANCE COMPARISON,0.3036837376460018,"Without Web Searching
With Web Searching
Natural
ManMade
Overall
Natural
ManMade
Overall
Infi-MM
19.2547
21.4133
20.9883
0.9938
0.3351
0.4648
Qwen-VL
42.4845
37.4657
38.4540
4.9689
11.2093
9.9804
vip-llava-13b
20.6211
15.4127
16.4384
8.323
4.3558
5.137
vip-llava-7b
21.9876
18.4892
19.1781
31.9255
56.5032
51.6634
llava-1.5-7b
17.3913
16.3265
16.5362
27.205
47.2129
43.273
llava-1.6-7b-mistral
0.3727
0.0914
0.1468
0.8696
2.1627
1.908
llava-1.6-7b-vicuna
2.2360
2.0713
2.1037
6.9565
15.8696
14.1145
llava-1.6-13b
10.4348
8.8943
9.1977
12.1739
28.2668
25.0978
llava-1.6-34b
10.3106
9.1379
9.3689
52.795
77.1855
72.3826
CogVLM
7.7019
7.5845
7.6076
6.8323
10.3564
9.6624
claude-3-opus
22.06
37.38
16.5468
33.0435
40.7125
39.2027
GPT-4V
27.5776
35.3443
33.8145
61.9876
87.6028
82.5587
Gemini-1.5-pro
55.6522
60.3107
59.3933
62.2360
82.8206
78.7671
smileGeo
58.6111
64.3968
63.2730
78.0448
87.0069
85.2630
Bold indicates the statistically significant improvements
(i.e., two-sided t-test with p < 0.05) over the best baseline."
PERFORMANCE COMPARISON,0.3045822102425876,"5https://huggingface.co/llava-hf/llava-1.5-xxx
6https://huggingface.co/liuhaotian/llava-v1.6-xxx
7https://github.com/THUDM/CogVLM
8https://anthropic.com/
9https://openai.com/
10https://gemini.google.com/"
PERFORMANCE COMPARISON,0.3054806828391734,"Firstly, the performance of all single LVLM baselines is shown in Table 1, in terms of the metric
253"
PERFORMANCE COMPARISON,0.3063791554357592,"Acc. The data in Table 1 indicate that open-source LVLMs with diverse knowledge and reasoning
254"
PERFORMANCE COMPARISON,0.30727762803234504,"capabilities exhibit significant variations, particularly in geo-localization tasks. This may be due
255"
PERFORMANCE COMPARISON,0.3081761006289308,"to the difference in the overlap between the pre-training datasets used by different LVLMs and
256"
PERFORMANCE COMPARISON,0.3090745732255166,"the dataset we constructed. Therefore, in addition to querying the LVLM locations about images,
257"
PERFORMANCE COMPARISON,0.30997304582210244,"we also incorporated real-time image search results from Google to provide the model with more
258"
PERFORMANCE COMPARISON,0.3108715184186882,"comprehensive information. These results from Internet retrievals are incorporated into the chain-of-
259"
PERFORMANCE COMPARISON,0.31176999101527403,"thoughts (CoT) [47] of LVLMs as external knowledge. At this time, models with larger parameters,
260"
PERFORMANCE COMPARISON,0.31266846361185985,"such as llava–1.6–34b, demonstrate superior reasoning abilities compared to smaller models (7b or
261"
PERFORMANCE COMPARISON,0.31356693620844567,"13b). In addition, closed-source large models also show more consistent performance than their open-
262"
PERFORMANCE COMPARISON,0.31446540880503143,"source counterparts and are more adept at analyzing and utilizing external knowledge for accurate
263"
PERFORMANCE COMPARISON,0.31536388140161725,"inferences. Compared to all single LVLMs, our proposed LVLM agent framework surpasses all
264"
PERFORMANCE COMPARISON,0.31626235399820307,"single LVLM baselines in accuracy. This improvement confirms the effectiveness of different LVLMs
265"
PERFORMANCE COMPARISON,0.31716082659478884,"collaborating by engaging in discussions and analyzing various types of images, thus producing more
266"
PERFORMANCE COMPARISON,0.31805929919137466,"precise results.
267"
PERFORMANCE COMPARISON,0.3189577717879605,Table 2: Results of different agent frameworks without web searching.
PERFORMANCE COMPARISON,0.3198562443845463,"Framework
LLM-
Blender
PHP
Reflexion
LLM
Debate
DyLAN
smileGeo"
PERFORMANCE COMPARISON,0.32075471698113206,Sturcture GNN
PERFORMANCE COMPARISON,0.3216531895777179,"Acc ↑
55.7802%
60.9809%
62.3412%
57.0119%
62.8187%
63.2730%
Tks ↓
23,662
154,520
109,524
260,756
159,320
18,876
’Acc’ stands for the accuracy of the framework;
’Tks’ means the average tokens a framework costs per query (including image tokens).
Secondly, the comparative results across various LLM/LVLM agent frameworks are presented in
268"
PERFORMANCE COMPARISON,0.3225516621743037,"Table 2. It is evident that the majority of LLM/LVLM agent frameworks surpass individual LVLMs
269"
PERFORMANCE COMPARISON,0.32345013477088946,"in terms of geo-localization accuracy. This improvement can primarily be attributed to the ability to
270"
PERFORMANCE COMPARISON,0.3243486073674753,"integrate knowledge from multiple LVLM agents, thereby enhancing the overall precision of these
271"
PERFORMANCE COMPARISON,0.3252470799640611,"frameworks. However, LLM-Blender and LLM Debate exhibit lower accuracy due to statements of
272"
PERFORMANCE COMPARISON,0.3261455525606469,"some agents misleading others during discussions, which impedes the generation of correct outcomes.
273"
PERFORMANCE COMPARISON,0.3270440251572327,"Our framework, smileGeo, guarantees the highest accuracy while being able to accomplish the
274"
PERFORMANCE COMPARISON,0.3279424977538185,"geo-localization task with the lowest token costs. The average number of tokens our framework
275"
PERFORMANCE COMPARISON,0.3288409703504043,"spent per query is 18,876, and it is less than the computational overhead of LLM-Blender (23,662),
276"
PERFORMANCE COMPARISON,0.3297394429469901,"which has the simplest agent framework structure but the lowest accuracy among all baselines. This
277"
PERFORMANCE COMPARISON,0.3306379155435759,"is mainly due to a ’small’ GNN-based dynamic learning model being deployed for agent selection
278"
PERFORMANCE COMPARISON,0.33153638814016173,"stages and significantly reducing unnecessary discussions among agents.
279"
PERFORMANCE COMPARISON,0.33243486073674755,Table 3: Comparison with image retrieval systems.
PERFORMANCE COMPARISON,0.3333333333333333,"Natural
ManMade
Overall
NetVLAD
26.5134
28.9955
28.6047
GeM
23.1022
25.4175
25.0749
CosPlace
28.1688
30.2782
29.8701
smileGeo
58.6111
64.3968
63.2730
Bold indicates the statistically significant improvements
(i.e., two-sided t-test with p < 0.05) over the best baseline."
PERFORMANCE COMPARISON,0.33423180592991913,"Finally, Table 3 presents the comparison be-
280"
PERFORMANCE COMPARISON,0.33513027852650495,"tween the proposed framework and existing
281"
PERFORMANCE COMPARISON,0.3360287511230907,"image retrieval systems.
Our framework,
282"
PERFORMANCE COMPARISON,0.33692722371967654,"smileGeo, consistently outperforms all other
283"
PERFORMANCE COMPARISON,0.33782569631626236,"retrieval-based approaches.
This superior
284"
PERFORMANCE COMPARISON,0.3387241689128482,"performance can be attributed to the fact
285"
PERFORMANCE COMPARISON,0.33962264150943394,"that other image retrieval methods rely on
286"
PERFORMANCE COMPARISON,0.34052111410601976,"a rich geo-tagged image database. In our test
287"
PERFORMANCE COMPARISON,0.3414195867026056,"dataset, however, two-thirds of the images
288"
PERFORMANCE COMPARISON,0.3423180592991914,"are new and localized in completely different areas from those in the training dataset. This highlights
289"
PERFORMANCE COMPARISON,0.34321653189577717,"the shortages of conventional database-based retrieval systems due to the limitations of the geo-tagged
290"
PERFORMANCE COMPARISON,0.344115004492363,"image databases and demonstrates the effectiveness of our proposed framework in solving open-world
291"
PERFORMANCE COMPARISON,0.3450134770889488,"geo-localization tasks.
292"
ABLATION STUDY,0.34591194968553457,"4.3
Ablation Study
293"
ABLATION STUDY,0.3468104222821204,"Number of Agents. We further demonstrate the relationships between the number of agents and the
294"
ABLATION STUDY,0.3477088948787062,"framework performance. We conduct experiments in two ways: i) by calling the same closed-source
295"
ABLATION STUDY,0.34860736747529203,"LVLM API (Here, we use Gemini-1.5-pro because it performs best without the help of the Internet)
296"
ABLATION STUDY,0.3495058400718778,"under different prompts (e.g., You are good at recognizing natural attractions; You’re a traveler around
297"
ABLATION STUDY,0.3504043126684636,"Europe) to simulate different agents, and ii) by using different LVLM backbones to represent distinct
298"
ABLATION STUDY,0.35130278526504943,"agents. The results are shown in Figure 2.
299"
ABLATION STUDY,0.3522012578616352,"1
3
5
7
9
11
13
50 55 60 65"
ABLATION STUDY,0.353099730458221,Number of Agents
ABLATION STUDY,0.35399820305480684,Accuracy (%)
ABLATION STUDY,0.35489667565139266,LLM-Blender
ABLATION STUDY,0.3557951482479784,"PHP
Reflexion
LLM Debate"
ABLATION STUDY,0.35669362084456424,"DyLAN
smileGeo"
ABLATION STUDY,0.35759209344115006,(a) Calling the same LVLM API
ABLATION STUDY,0.3584905660377358,"1
3
5
7
9
11
13
0 20 40 60"
ABLATION STUDY,0.35938903863432164,Number of Agents
ABLATION STUDY,0.36028751123090746,Accuracy (%)
ABLATION STUDY,0.3611859838274933,LLM-Blender
ABLATION STUDY,0.36208445642407905,"PHP
Reflexion
LLM Debate"
ABLATION STUDY,0.36298292902066487,"DyLAN
smileGeo"
ABLATION STUDY,0.3638814016172507,(b) Different LVLM backbones
ABLATION STUDY,0.36477987421383645,"Figure 2: Results of model performance in relation to the number of agents.
As illustrated in Figure 2(a), the framework achieves optimal accuracy with 4 or 5 agents. Beyond
300"
ABLATION STUDY,0.3656783468104223,"this number, the framework’s performance begins to deteriorate. This shows that using models
301"
ABLATION STUDY,0.3665768194070081,"with the same knowledge and reasoning capabilities as different agents has limited improvement
302"
ABLATION STUDY,0.3674752920035939,"in the accuracy of the framework. Despite this decline, the performance of frameworks other than
303"
ABLATION STUDY,0.3683737646001797,"LLM-Blender and LLM Debate remains superior to that of a single agent. LLM-Blender and LLM
304"
ABLATION STUDY,0.3692722371967655,"Debate, however, have a significant decrease in model accuracy when the number of agents exceeds
305"
ABLATION STUDY,0.3701707097933513,"11. This is mainly because both of them involve all LVLMs in every discussion, which suffers from
306"
ABLATION STUDY,0.3710691823899371,"excessive repetitive and redundant discussions. Figure 2(b) reveals that the accuracy of the framework
307"
ABLATION STUDY,0.3719676549865229,"improves with the incorporation of more LVLM backbones, indicating that the diversity of LVLMs
308"
ABLATION STUDY,0.3728661275831087,"can enhance the quality of discussions.
309"
ABLATION STUDY,0.37376460017969454,"1
2
3
4
5
6
7
8 1 2 3 4 5 6 7 8 R K 61 62 63"
ABLATION STUDY,0.3746630727762803,"64
Acc (%)"
ABLATION STUDY,0.3755615453728661,Figure 3: Results under different K and R.
ABLATION STUDY,0.37646001796945194,"Hyperparameter K & R. There are two hyperpa-
310"
ABLATION STUDY,0.37735849056603776,"rameters, K and R, that need to be pre-defined in the
311"
ABLATION STUDY,0.3782569631626235,"proposed framework: K is the number of agents (an-
312"
ABLATION STUDY,0.37915543575920935,"swer agents) that respond in each round of discussion,
313"
ABLATION STUDY,0.38005390835579517,"and R is the number of agents (review agents) used
314"
ABLATION STUDY,0.38095238095238093,"to review answers from answer agents. Therefore, we
315"
ABLATION STUDY,0.38185085354896675,"conduct experiments under different combinations of
316"
ABLATION STUDY,0.38274932614555257,"K ∈[1, 8] and R ∈[1, 8], as shown in Figure 3. The re-
317"
ABLATION STUDY,0.3836477987421384,"sults indicate that optimal performance can be achieved
318"
ABLATION STUDY,0.38454627133872415,"with relatively small values of K or R. However, the
319"
ABLATION STUDY,0.38544474393531,"computational cost, measured in tokens, increases ex-
320"
ABLATION STUDY,0.3863432165318958,"ponentially with higher values of K and R. To balance
321"
ABLATION STUDY,0.38724168912848156,"both the efficiency and the accuracy of smileGeo, for
322"
ABLATION STUDY,0.3881401617250674,"the experiments presented in this paper, we set both K and R equal to 2.
323"
CONCLUSION,0.3890386343216532,"5
Conclusion
324"
CONCLUSION,0.389937106918239,"This work introduces a novel LVLM agent framework, smileGeo, specifically designed for geo-
325"
CONCLUSION,0.3908355795148248,"localization tasks. Inspired by the review mechanism, it integrates various LVLMs to discuss
326"
CONCLUSION,0.3917340521114106,"anonymously and geo-localize images worldwide. Additionally, we have developed a dynamic
327"
CONCLUSION,0.3926325247079964,"learning strategy for agent collaboration social networks, electing appropriate agents to geo-localize
328"
CONCLUSION,0.3935309973045822,"each image with different characteristics. This enhancement reduces the computational burden
329"
CONCLUSION,0.394429469901168,"associated with collaborative discussions among LVLM agents. Moreover, we have constructed a
330"
CONCLUSION,0.3953279424977538,"geo-localization dataset called GeoGlobe and will open-source it. Overall, smileGeo demonstrates
331"
CONCLUSION,0.39622641509433965,"significant improvements in geo-localization tasks, achieving superior performance with lower
332"
CONCLUSION,0.3971248876909254,"computational demands compared to contemporary state-of-the-art LLM/LVLM agent frameworks.
333"
CONCLUSION,0.39802336028751123,"Looking ahead, we aim to expand the capabilities of smileGeo to incorporate more powerful external
334"
CONCLUSION,0.39892183288409705,"tools beyond just web searching. Additionally, we plan to explore extending its application to complex
335"
CONCLUSION,0.3998203054806828,"scenarios, such as high-precision global positioning and navigation for robots, laying the cornerstone
336"
CONCLUSION,0.40071877807726863,"for exploring LVLM agent collaboration to handle different complex open-world tasks efficiently.
337"
REFERENCES,0.40161725067385445,"References
338"
REFERENCES,0.4025157232704403,"[1] B. Huang and K. M. Carley, “A large-scale empirical study of geotagging behavior on twitter,” in
339"
REFERENCES,0.40341419586702604,"ASONAM ’19: International Conference on Advances in Social Networks Analysis and Mining,
340"
REFERENCES,0.40431266846361186,"Vancouver, British Columbia, Canada, 27-30 August, 2019, F. Spezzano, W. Chen, and X. Xiao,
341"
REFERENCES,0.4052111410601977,"Eds.
ACM, 2019, pp. 365–373. [Online]. Available: https://doi.org/10.1145/3341161.3342870
342"
REFERENCES,0.40610961365678344,"[2] J. Luo, D. Joshi, J. Yu, and A. C. Gallagher, “Geotagging in multimedia and computer vision
343"
REFERENCES,0.40700808625336926,"- a survey,” Multim. Tools Appl., vol. 51, no. 1, pp. 187–211, 2011. [Online]. Available:
344"
REFERENCES,0.4079065588499551,"https://doi.org/10.1007/s11042-010-0623-y
345"
REFERENCES,0.4088050314465409,"[3] R. Arandjelovic, P. Gronát, A. Torii, T. Pajdla, and J. Sivic, “Netvlad: CNN architecture for
346"
REFERENCES,0.40970350404312667,"weakly supervised place recognition,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 40, no. 6,
347"
REFERENCES,0.4106019766397125,"pp. 1437–1451, 2018. [Online]. Available: https://doi.org/10.1109/TPAMI.2017.2711011
348"
REFERENCES,0.4115004492362983,"[4] M. Zaffar, S. Garg, M. Milford, J. F. P. Kooij, D. Flynn, K. D. McDonald-Maier, and S. Ehsan,
349"
REFERENCES,0.4123989218328841,"“Vpr-bench: An open-source visual place recognition evaluation framework with quantifiable
350"
REFERENCES,0.4132973944294699,"viewpoint and appearance change,” Int. J. Comput. Vis., vol. 129, no. 7, pp. 2136–2174, 2021.
351"
REFERENCES,0.4141958670260557,"[Online]. Available: https://doi.org/10.1007/s11263-021-01469-5
352"
REFERENCES,0.41509433962264153,"[5] A. Torii, R. Arandjelovic, J. Sivic, M. Okutomi, and T. Pajdla, “24/7 place recognition by view
353"
REFERENCES,0.4159928122192273,"synthesis,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 40, no. 2, pp. 257–271, 2018. [Online].
354"
REFERENCES,0.4168912848158131,"Available: https://doi.org/10.1109/TPAMI.2017.2667665
355"
REFERENCES,0.41778975741239893,"[6] Z. Chen, A. Jacobson, N. Sünderhauf, B. Upcroft, L. Liu, C. Shen, I. D. Reid, and
356"
REFERENCES,0.41868823000898475,"M. Milford, “Deep learning features at scale for visual place recognition,” in 2017
357"
REFERENCES,0.4195867026055705,"IEEE International Conference on Robotics and Automation, ICRA 2017, Singapore,
358"
REFERENCES,0.42048517520215634,"Singapore, May 29 - June 3, 2017.
IEEE, 2017, pp. 3223–3230. [Online]. Available:
359"
REFERENCES,0.42138364779874216,"https://doi.org/10.1109/ICRA.2017.7989366
360"
REFERENCES,0.4222821203953279,"[7] Z. Chen, L. Liu, I. Sa, Z. Ge, and M. Chli, “Learning context flexible attention model for
361"
REFERENCES,0.42318059299191374,"long-term visual place recognition,” IEEE Robotics Autom. Lett., vol. 3, no. 4, pp. 4015–4022,
362"
REFERENCES,0.42407906558849956,"2018. [Online]. Available: https://doi.org/10.1109/LRA.2018.2859916
363"
REFERENCES,0.4249775381850854,"[8] Z. Chen, F. Maffra, I. Sa, and M. Chli, “Only look once, mining distinctive landmarks from
364"
REFERENCES,0.42587601078167114,"convnet for visual place recognition,” in 2017 IEEE/RSJ International Conference on Intelligent
365"
REFERENCES,0.42677448337825696,"Robots and Systems, IROS 2017, Vancouver, BC, Canada, September 24-28, 2017.
IEEE,
366"
REFERENCES,0.4276729559748428,"2017, pp. 9–16. [Online]. Available: https://doi.org/10.1109/IROS.2017.8202131
367"
REFERENCES,0.42857142857142855,"[9] S. Garg, N. Sünderhauf, and M. Milford, “Semantic-geometric visual place recognition: a new
368"
REFERENCES,0.42946990116801437,"perspective for reconciling opposing views,” Int. J. Robotics Res., vol. 41, no. 6, pp. 573–598,
369"
REFERENCES,0.4303683737646002,"2022. [Online]. Available: https://doi.org/10.1177/0278364919839761
370"
REFERENCES,0.431266846361186,"[10] S. Hausler, A. Jacobson, and M. Milford, “Multi-process fusion: Visual place recognition using
371"
REFERENCES,0.43216531895777177,"multiple image processing methods,” IEEE Robotics Autom. Lett., vol. 4, no. 2, pp. 1924–1931,
372"
REFERENCES,0.4330637915543576,"2019. [Online]. Available: https://doi.org/10.1109/LRA.2019.2898427
373"
REFERENCES,0.4339622641509434,"[11] A. Khaliq, S. Ehsan, Z. Chen, M. Milford, and K. D. McDonald-Maier, “A holistic visual
374"
REFERENCES,0.4348607367475292,"place recognition approach using lightweight cnns for significant viewpoint and appearance
375"
REFERENCES,0.435759209344115,"changes,” IEEE Trans. Robotics, vol. 36, no. 2, pp. 561–569, 2020. [Online]. Available:
376"
REFERENCES,0.4366576819407008,"https://doi.org/10.1109/TRO.2019.2956352
377"
REFERENCES,0.43755615453728663,"[12] M. M. ElQadi, M. Lesiv, A. G. Dyer, and A. Dorin, “Computer vision-enhanced selection of
378"
REFERENCES,0.4384546271338724,"geo-tagged photos on social network sites for land cover classification,” Environ. Model. Softw.,
379"
REFERENCES,0.4393530997304582,"vol. 128, p. 104696, 2020. [Online]. Available: https://doi.org/10.1016/j.envsoft.2020.104696
380"
REFERENCES,0.44025157232704404,"[13] M. Campbell and M. Wheeler, “A vision based geolocation tracking system for uav’s,” in AIAA
381"
REFERENCES,0.4411500449236298,"Guidance, Navigation, and Control Conference and Exhibit, 2006, p. 6246.
382"
REFERENCES,0.4420485175202156,"[14] F. Deng, L. Zhang, F. Gao, H. Qiu, X. Gao, and J. Chen, “Long-range binocular
383"
REFERENCES,0.44294699011680144,"vision target geolocation using handheld electronic devices in outdoor environment,”
384"
REFERENCES,0.44384546271338726,"IEEE Trans. Image Process.,
vol. 29,
pp. 5531–5541,
2020. [Online]. Available:
385"
REFERENCES,0.444743935309973,"https://doi.org/10.1109/TIP.2020.2984898
386"
REFERENCES,0.44564240790655885,"[15] L. Zhang, F. Deng, J. Chen, Y. Bi, S. K. Phang, X. Chen, and B. M. Chen,
387"
REFERENCES,0.44654088050314467,"“Vision-based target three-dimensional geolocation using unmanned aerial vehicles,” IEEE
388"
REFERENCES,0.4474393530997305,"Trans. Ind. Electron., vol. 65, no. 10, pp. 8052–8061, 2018. [Online]. Available:
389"
REFERENCES,0.44833782569631625,"https://doi.org/10.1109/TIE.2018.2807401
390"
REFERENCES,0.44923629829290207,"[16] X. Feng, Z.-Y. Chen, Y. Qin, Y. Lin, X. Chen, Z. Liu, and J.-R. Wen, “Large language model-
391"
REFERENCES,0.4501347708894879,"based human-agent collaboration for complex task solving,” arXiv preprint arXiv:2402.12914,
392"
REFERENCES,0.45103324348607365,"2024.
393"
REFERENCES,0.4519317160826595,"[17] W. Wang, Q. Lv, W. Yu, W. Hong, J. Qi, Y. Wang, J. Ji, Z. Yang, L. Zhao, X. Song et al.,
394"
REFERENCES,0.4528301886792453,"“Cogvlm: Visual expert for pretrained language models,” arXiv preprint arXiv:2311.03079,
395"
REFERENCES,0.4537286612758311,"2023.
396"
REFERENCES,0.4546271338724169,"[18] V. Paolicelli, G. M. Berton, F. Montagna, C. Masone, and B. Caputo, “Adaptive-attentive
397"
REFERENCES,0.4555256064690027,"geolocalization from few queries: A hybrid approach,” Frontiers Comput. Sci., vol. 4, p.
398"
REFERENCES,0.4564240790655885,"841817, 2022. [Online]. Available: https://doi.org/10.3389/fcomp.2022.841817
399"
REFERENCES,0.4573225516621743,"[19] Y. Ge, H. Wang, F. Zhu, R. Zhao, and H. Li, “Self-supervising fine-grained region similarities
400"
REFERENCES,0.4582210242587601,"for large-scale image localization,” in Computer Vision - ECCV 2020 - 16th European
401"
REFERENCES,0.4591194968553459,"Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part IV, ser. Lecture Notes in
402"
REFERENCES,0.46001796945193174,"Computer Science, A. Vedaldi, H. Bischof, T. Brox, and J. Frahm, Eds., vol. 12349.
Springer,
403"
REFERENCES,0.4609164420485175,"2020, pp. 369–386. [Online]. Available: https://doi.org/10.1007/978-3-030-58548-8_22
404"
REFERENCES,0.4618149146451033,"[20] H. Jin Kim, E. Dunn, and J.-M. Frahm, “Learned contextual feature reweighting for image
405"
REFERENCES,0.46271338724168914,"geo-localization,” in Proceedings of the IEEE Conference on Computer Vision and Pattern
406"
REFERENCES,0.4636118598382749,"Recognition, 2017, pp. 2136–2145.
407"
REFERENCES,0.46451033243486073,"[21] L. Liu, H. Li, and Y. Dai, “Stochastic attraction-repulsion embedding for large scale image
408"
REFERENCES,0.46540880503144655,"localization,” in 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019,
409"
REFERENCES,0.46630727762803237,"Seoul, Korea (South), October 27 - November 2, 2019.
IEEE, 2019, pp. 2570–2579. [Online].
410"
REFERENCES,0.46720575022461813,"Available: https://doi.org/10.1109/ICCV.2019.00266
411"
REFERENCES,0.46810422282120395,"[22] F. Warburg, S. Hauberg, M. Lopez-Antequera, P. Gargallo, Y. Kuang, and J. Civera, “Mapillary
412"
REFERENCES,0.46900269541778977,"street-level sequences: A dataset for lifelong place recognition,” in Proceedings of the IEEE/CVF
413"
REFERENCES,0.46990116801437554,"conference on computer vision and pattern recognition, 2020, pp. 2626–2635.
414"
REFERENCES,0.47079964061096136,"[23] G. Peng, Y. Yue, J. Zhang, Z. Wu, X. Tang, and D. Wang, “Semantic reinforced attention
415"
REFERENCES,0.4716981132075472,"learning for visual place recognition,” in IEEE International Conference on Robotics and
416"
REFERENCES,0.472596585804133,"Automation, ICRA 2021, Xi’an, China, May 30 - June 5, 2021.
IEEE, 2021, pp. 13 415–13 422.
417"
REFERENCES,0.47349505840071876,"[Online]. Available: https://doi.org/10.1109/ICRA48506.2021.9561812
418"
REFERENCES,0.4743935309973046,"[24] S. Ibrahimi,
N. van Noord,
T. Alpherts,
and M. Worring,
“Inside out visual
419"
REFERENCES,0.4752920035938904,"place recognition,” in 32nd British Machine Vision Conference 2021, BMVC 2021,
420"
REFERENCES,0.47619047619047616,"Online, November 22-25, 2021.
BMVA Press, 2021, p. 362. [Online]. Available:
421"
REFERENCES,0.477088948787062,"https://www.bmvc2021-virtualconference.com/assets/papers/0467.pdf
422"
REFERENCES,0.4779874213836478,"[25] S. Hausler, S. Garg, M. Xu, M. Milford, and T. Fischer, “Patch-netvlad: Multi-scale
423"
REFERENCES,0.4788858939802336,"fusion of locally-global descriptors for place recognition,” in IEEE Conference on
424"
REFERENCES,0.4797843665768194,"Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021.
425"
REFERENCES,0.4806828391734052,"Computer Vision Foundation / IEEE, 2021, pp. 14 141–14 152. [Online]. Available: https:
426"
REFERENCES,0.481581311769991,"//openaccess.thecvf.com/content/CVPR2021/html/Hausler_Patch-NetVLAD_Multi-Scale_
427"
REFERENCES,0.48247978436657685,"Fusion_of_Locally-Global_Descriptors_for_Place_Recognition_CVPR_2021_paper.html
428"
REFERENCES,0.4833782569631626,"[26] F. Radenovic, G. Tolias, and O. Chum, “Fine-tuning CNN image retrieval with no human
429"
REFERENCES,0.48427672955974843,"annotation,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 41, no. 7, pp. 1655–1668, 2019.
430"
REFERENCES,0.48517520215633425,"[Online]. Available: https://doi.org/10.1109/TPAMI.2018.2846566
431"
REFERENCES,0.48607367475292,"[27] M. Izbicki, E. E. Papalexakis, and V. J. Tsotras, “Exploiting the earth’s spherical geometry to
432"
REFERENCES,0.48697214734950584,"geolocate images,” in Machine Learning and Knowledge Discovery in Databases - European
433"
REFERENCES,0.48787061994609165,"Conference, ECML PKDD 2019, Würzburg, Germany, September 16-20, 2019, Proceedings,
434"
REFERENCES,0.4887690925426775,"Part II, ser. Lecture Notes in Computer Science, U. Brefeld, É. Fromont, A. Hotho, A. J.
435"
REFERENCES,0.48966756513926324,"Knobbe, M. H. Maathuis, and C. Robardet, Eds., vol. 11907.
Springer, 2019, pp. 3–19.
436"
REFERENCES,0.49056603773584906,"[Online]. Available: https://doi.org/10.1007/978-3-030-46147-8_1
437"
REFERENCES,0.4914645103324349,"[28] G. Kordopatis-Zilos, P. Galopoulos, S. Papadopoulos, and I. Kompatsiaris, “Leveraging
438"
REFERENCES,0.49236298292902064,"efficientnet and contrastive learning for accurate global-scale location estimation,” in ICMR
439"
REFERENCES,0.49326145552560646,"’21: International Conference on Multimedia Retrieval, Taipei, Taiwan, August 21-24, 2021,
440"
REFERENCES,0.4941599281221923,"W. Cheng, M. S. Kankanhalli, M. Wang, W. Chu, J. Liu, and M. Worring, Eds.
ACM, 2021,
441"
REFERENCES,0.4950584007187781,"pp. 155–163. [Online]. Available: https://doi.org/10.1145/3460426.3463644
442"
REFERENCES,0.49595687331536387,"[29] E. Müller-Budack, K. Pustu-Iren, and R. Ewerth, “Geolocation estimation of photos
443"
REFERENCES,0.4968553459119497,"using a hierarchical model and scene classification,” in Computer Vision - ECCV 2018 -
444"
REFERENCES,0.4977538185085355,"15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part
445"
REFERENCES,0.49865229110512127,"XII, ser. Lecture Notes in Computer Science, V. Ferrari, M. Hebert, C. Sminchisescu,
446"
REFERENCES,0.4995507637017071,"and Y. Weiss, Eds., vol. 11216.
Springer, 2018, pp. 575–592. [Online]. Available:
447"
REFERENCES,0.5004492362982929,"https://doi.org/10.1007/978-3-030-01258-8_35
448"
REFERENCES,0.5013477088948787,"[30] P. H. Seo, T. Weyand, J. Sim, and B. Han, “Cplanet: Enhancing image geolocalization
449"
REFERENCES,0.5022461814914645,"by combinatorial partitioning of maps,” in Computer Vision - ECCV 2018 - 15th
450"
REFERENCES,0.5031446540880503,"European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part
451"
REFERENCES,0.5040431266846361,"X, ser. Lecture Notes in Computer Science, V. Ferrari, M. Hebert, C. Sminchisescu,
452"
REFERENCES,0.504941599281222,"and Y. Weiss, Eds., vol. 11214.
Springer, 2018, pp. 544–560. [Online]. Available:
453"
REFERENCES,0.5058400718778078,"https://doi.org/10.1007/978-3-030-01249-6_33
454"
REFERENCES,0.5067385444743935,"[31] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,
455"
REFERENCES,0.5076370170709793,"K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell,
456"
REFERENCES,0.5085354896675651,"P. Welinder, P. F. Christiano, J. Leike, and R. Lowe, “Training language models to follow
457"
REFERENCES,0.5094339622641509,"instructions with human feedback,” in Advances in Neural Information Processing Systems
458"
REFERENCES,0.5103324348607368,"35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New
459"
REFERENCES,0.5112309074573226,"Orleans, LA, USA, November 28 - December 9, 2022, S. Koyejo, S. Mohamed, A. Agarwal,
460"
REFERENCES,0.5121293800539084,"D. Belgrave, K. Cho, and A. Oh, Eds., 2022. [Online]. Available: http://papers.nips.cc/paper_
461"
REFERENCES,0.5130278526504941,"files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html
462"
REFERENCES,0.5139263252470799,"[32] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T.
463"
REFERENCES,0.5148247978436657,"Lee, Y. Li, S. M. Lundberg, H. Nori, H. Palangi, M. T. Ribeiro, and Y. Zhang, “Sparks of
464"
REFERENCES,0.5157232704402516,"artificial general intelligence: Early experiments with GPT-4,” CoRR, vol. abs/2303.12712,
465"
REFERENCES,0.5166217430368374,"2023. [Online]. Available: https://doi.org/10.48550/arXiv.2303.12712
466"
REFERENCES,0.5175202156334232,"[33] R. Schaeffer, B. Miranda, and S. Koyejo, “Are emergent abilities of large language models
467"
REFERENCES,0.518418688230009,"a mirage?” in Advances in Neural Information Processing Systems 36: Annual Conference
468"
REFERENCES,0.5193171608265947,"on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA,
469"
REFERENCES,0.5202156334231806,"December 10 - 16, 2023, A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and
470"
REFERENCES,0.5211141060197664,"S. Levine, Eds., 2023. [Online]. Available: http://papers.nips.cc/paper_files/paper/2023/hash/
471"
REFERENCES,0.5220125786163522,"adc98a266f45005c403b8311ca7e8bd7-Abstract-Conference.html
472"
REFERENCES,0.522911051212938,"[34] D. Jiang, X. Ren, and B. Y. Lin, “Llm-blender:
Ensembling large language models
473"
REFERENCES,0.5238095238095238,"with pairwise ranking and generative fusion,” in Proceedings of the 61st Annual Meeting
474"
REFERENCES,0.5247079964061097,"of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023,
475"
REFERENCES,0.5256064690026954,"Toronto, Canada, July 9-14, 2023, A. Rogers, J. L. Boyd-Graber, and N. Okazaki, Eds.
476"
REFERENCES,0.5265049415992812,"Association for Computational Linguistics, 2023, pp. 14 165–14 178. [Online]. Available:
477"
REFERENCES,0.527403414195867,"https://doi.org/10.18653/v1/2023.acl-long.792
478"
REFERENCES,0.5283018867924528,"[35] C. Zheng, Z. Liu, E. Xie, Z. Li, and Y. Li, “Progressive-hint prompting improves
479"
REFERENCES,0.5292003593890386,"reasoning in large language models,” CoRR, vol. abs/2304.09797, 2023. [Online]. Available:
480"
REFERENCES,0.5300988319856245,"https://doi.org/10.48550/arXiv.2304.09797
481"
REFERENCES,0.5309973045822103,"[36] N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, and S. Yao, “Reflexion: language agents
482"
REFERENCES,0.531895777178796,"with verbal reinforcement learning,” in Advances in Neural Information Processing Systems
483"
REFERENCES,0.5327942497753818,"36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New
484"
REFERENCES,0.5336927223719676,"Orleans, LA, USA, December 10 - 16, 2023, A. Oh, T. Naumann, A. Globerson, K. Saenko,
485"
REFERENCES,0.5345911949685535,"M. Hardt, and S. Levine, Eds., 2023. [Online]. Available: http://papers.nips.cc/paper_files/
486"
REFERENCES,0.5354896675651393,"paper/2023/hash/1b44b878bb782e6954cd888628510e90-Abstract-Conference.html
487"
REFERENCES,0.5363881401617251,"[37] Y. Du, S. Li, A. Torralba, J. B. Tenenbaum, and I. Mordatch, “Improving factuality and
488"
REFERENCES,0.5372866127583109,"reasoning in language models through multiagent debate,” CoRR, vol. abs/2305.14325, 2023.
489"
REFERENCES,0.5381850853548967,"[Online]. Available: https://doi.org/10.48550/arXiv.2305.14325
490"
REFERENCES,0.5390835579514824,"[38] Z. Liu, Y. Zhang, P. Li, Y. Liu, and D. Yang, “Dynamic llm-agent network: An llm-agent
491"
REFERENCES,0.5399820305480683,"collaboration framework with agent team optimization,” CoRR, vol. abs/2310.02170, 2023.
492"
REFERENCES,0.5408805031446541,"[Online]. Available: https://doi.org/10.48550/arXiv.2310.02170
493"
REFERENCES,0.5417789757412399,"[39] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettlemoyer, and W. Yih,
494"
REFERENCES,0.5426774483378257,"“REPLUG: retrieval-augmented black-box language models,” CoRR, vol. abs/2301.12652, 2023.
495"
REFERENCES,0.5435759209344115,"[Online]. Available: https://doi.org/10.48550/arXiv.2301.12652
496"
REFERENCES,0.5444743935309974,"[40] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. R. Narasimhan, and Y. Cao, “React: Synergizing
497"
REFERENCES,0.5453728661275831,"reasoning and acting in language models,” in The Eleventh International Conference on
498"
REFERENCES,0.5462713387241689,"Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023.
OpenReview.net,
499"
REFERENCES,0.5471698113207547,"2023. [Online]. Available: https://openreview.net/pdf?id=WE_vluYUL-X
500"
REFERENCES,0.5480682839173405,"[41] G. Izacard, P. S. H. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick, J. Dwivedi-Yu,
501"
REFERENCES,0.5489667565139263,"A. Joulin, S. Riedel, and E. Grave, “Atlas: Few-shot learning with retrieval augmented
502"
REFERENCES,0.5498652291105122,"language models,” J. Mach. Learn. Res., vol. 24, pp. 251:1–251:43, 2023. [Online]. Available:
503"
REFERENCES,0.550763701707098,"http://jmlr.org/papers/v24/23-0037.html
504"
REFERENCES,0.5516621743036837,"[42] T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli, E. Hambro, L. Zettlemoyer,
505"
REFERENCES,0.5525606469002695,"N. Cancedda, and T. Scialom, “Toolformer: Language models can teach themselves to
506"
REFERENCES,0.5534591194968553,"use tools,” in Advances in Neural Information Processing Systems 36: Annual Conference
507"
REFERENCES,0.5543575920934412,"on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA,
508"
REFERENCES,0.555256064690027,"December 10 - 16, 2023, A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and
509"
REFERENCES,0.5561545372866128,"S. Levine, Eds., 2023. [Online]. Available: http://papers.nips.cc/paper_files/paper/2023/hash/
510"
REFERENCES,0.5570530098831986,"d842425e4bf79ba039352da0f658a906-Abstract-Conference.html
511"
REFERENCES,0.5579514824797843,"[43] P. Lu, B. Peng, H. Cheng, M. Galley, K. Chang, Y. N. Wu, S. Zhu, and J. Gao,
512"
REFERENCES,0.5588499550763701,"“Chameleon:
Plug-and-play compositional reasoning with large language models,” in
513"
REFERENCES,0.559748427672956,"Advances in Neural Information Processing Systems 36: Annual Conference on Neural
514"
REFERENCES,0.5606469002695418,"Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December
515"
REFERENCES,0.5615453728661276,"10 - 16, 2023, A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and
516"
REFERENCES,0.5624438454627134,"S. Levine, Eds., 2023. [Online]. Available: http://papers.nips.cc/paper_files/paper/2023/hash/
517"
REFERENCES,0.5633423180592992,"871ed095b734818cfba48db6aeb25a62-Abstract-Conference.html
518"
REFERENCES,0.564240790655885,"[44] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig,
519"
REFERENCES,0.5651392632524708,"“PAL: program-aided language models,” in International Conference on Machine Learning,
520"
REFERENCES,0.5660377358490566,"ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, ser. Proceedings of Machine
521"
REFERENCES,0.5669362084456424,"Learning Research, A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and
522"
REFERENCES,0.5678346810422282,"J. Scarlett, Eds., vol. 202.
PMLR, 2023, pp. 10 764–10 799. [Online]. Available:
523"
REFERENCES,0.568733153638814,"https://proceedings.mlr.press/v202/gao23f.html
524"
REFERENCES,0.5696316262353999,"[45] X. Wang, S. Li, and H. Ji, “Code4struct:
Code generation for few-shot structured
525"
REFERENCES,0.5705300988319856,"prediction from natural language,” CoRR, vol. abs/2210.12810, 2022. [Online]. Available:
526"
REFERENCES,0.5714285714285714,"https://doi.org/10.48550/arXiv.2210.12810
527"
REFERENCES,0.5723270440251572,"[46] G. M. Berton, C. Masone, and B. Caputo, “Rethinking visual geo-localization for large-scale
528"
REFERENCES,0.573225516621743,"applications,” in IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR
529"
REFERENCES,0.5741239892183289,"2022, New Orleans, LA, USA, June 18-24, 2022.
IEEE, 2022, pp. 4868–4878. [Online].
530"
REFERENCES,0.5750224618149147,"Available: https://doi.org/10.1109/CVPR52688.2022.00483
531"
REFERENCES,0.5759209344115005,"[47] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V. Le,
532"
REFERENCES,0.5768194070080862,"and D. Zhou, “Chain-of-thought prompting elicits reasoning in large language models,” in
533"
REFERENCES,0.577717879604672,"Advances in Neural Information Processing Systems 35: Annual Conference on Neural
534"
REFERENCES,0.5786163522012578,"Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November
535"
REFERENCES,0.5795148247978437,"28 - December 9, 2022, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and
536"
REFERENCES,0.5804132973944295,"A. Oh, Eds., 2022. [Online]. Available: http://papers.nips.cc/paper_files/paper/2022/hash/
537"
REFERENCES,0.5813117699910153,"9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html
538"
REFERENCES,0.5822102425876011,"A
Notations
539"
REFERENCES,0.5831087151841868,"We summarize all notations in this paper and list them in Table 4.
540"
REFERENCES,0.5840071877807727,"Table 4: Notations in this paper.
Notation
Description
X
The image to be recognized.
Y ( ˆY )
The predicted (ground truth of) geospatial location in the natural language form.
G ( ˆG)
The predicted (ground truth of) LVLM-based agent collaboration social network.
A ( ˆ
A)
The predicted (ground truth of) adjacency matrix of the agent social network.
Lst ( ˆ
Lst)
The predicted (ground truth of) scalar of agent election probability.
V
The set of LLM agents.
E
The set of connections between LLM agents.
N
The number of agents.
K
The number of agents to be elected as answer agent(s).
R
The number of agents to be selected as review agent(s).
L
The number of agent discussion rounds.
Z
The maximum number of rounds in which answer agents harmonize opinions.
Θ
The learnable parameters of the agent social network learning model."
REFERENCES,0.5849056603773585,"B
Dataset Details
541"
REFERENCES,0.5858041329739443,"The images in this dataset are copyright-free images obtained from the Internet via a crawler. We
542"
REFERENCES,0.5867026055705301,"divide the images into two main categories: man-made landmarks as well as natural attractions. Then,
543"
REFERENCES,0.5876010781671159,"we filter out the data samples that could clearly identify the locations of the landmarks or attractions
544"
REFERENCES,0.5884995507637018,"in the images. As a result, we filter out nearly three hundred thousand data samples, and please
545"
REFERENCES,0.5893980233602875,"refer to Table 5 and Figure 4 for details. Due to the fact that a large number of natural attractions in
546"
REFERENCES,0.5902964959568733,"different geographical regions with high similarity are cleaned, the magnitude of the data related to
547"
REFERENCES,0.5911949685534591,"natural attractions in this dataset is smaller than that of man-made attractions.
548"
REFERENCES,0.5920934411500449,"Figure 4:
The data distribution
around the world."
REFERENCES,0.5929919137466307,Table 5: Statistics of the dataset GeoGlobe.
REFERENCES,0.5938903863432166,"Images Cities Countries Attractions
Man-made 253,118 2,313
143
10,492
Natural
40,087
1,044
97
1,849 549"
REFERENCES,0.5947888589398024,"For an open-world geo-localization task, the relationship between the training and test samples in
550"
REFERENCES,0.5956873315363881,"the experiment could greatly affect the results. We label the training samples as Ztrain, and the test
551"
REFERENCES,0.5965858041329739,"sample set as Ztest, and use two metrics, coverage as well as consistency, to portray this relationship:
552"
REFERENCES,0.5974842767295597,coverage = Ztrain ∩Ztest
REFERENCES,0.5983827493261455,"Ztrain
× 100%"
REFERENCES,0.5992812219227314,consistency = Ztrain ∩Ztest
REFERENCES,0.6001796945193172,"Ztest
× 100%
(6)"
REFERENCES,0.601078167115903,"As for the samples in this paper, coverage ≈4.6564%, and consistency ≈33.2957%.
553"
REFERENCES,0.6019766397124887,"C
Implementation Details
554"
REFERENCES,0.6028751123090745,"In all experiments, we employ a variety of LVLMs, encompassing both open-source and closed-source
555"
REFERENCES,0.6037735849056604,"models, to be agents in the proposed framework. Unless specified otherwise, zero-shot prompting is
556"
REFERENCES,0.6046720575022462,"applied. Each open-source LVLM is deployed on a dedicated A800 (80G) GPU server with 200GB
557"
REFERENCES,0.605570530098832,"memory. As for each closed-source LVLM, we cost amounting to billions of tokens by calling APIs
558"
REFERENCES,0.6064690026954178,"as specified by the official website. To avoid the context length issue that occurs in some LVLMs, we
559"
REFERENCES,0.6073674752920036,"truncate the context before submitting it to the agent for questions based on the maximum number of
560"
REFERENCES,0.6082659478885895,Algorithm 1 The smileGeo framework
REFERENCES,0.6091644204851752,"Input: A set of pre-trained LLMs V = {v1, v2, · · · }, the input image X, and the ground truth ˆY (if
has);
Output: The geospatial location Y .
Initialization Stage:
1: Initialize (Load) the parameter of the agent selection model: Θ
2: Calculate: A ←f(X, V|Θ)
3: Initialize the agent collaboration social network: G
4: Calculate: Lst ←f(X, G|Θ)
Stage 1:
5: Elect answer agents: V1 = {v1
a, v1
b, · · · } ←Lst, where |V1| = K
6: for each answer agent v1 do
7:
Obtain the location: Y 1
v1 ←Askv1(X)
8:
Get the confidence percentage: C1
v1 ←Askv1(X, Y 1
v1)
9:
Store the further explanation: T 1
v1 ←Askv1(X, Y 1
v1)
10: end for
Stage 2:
11: for each selected answer agent v1 do
12:
Select the review agents: V2 = {v2
a, v2
b, · · · } ←RandomWalkv1(G), where |V2| = R
13:
for each review agent v2 do
14:
Obtain the comment T 2
v2 ←Reviewv2(X, Y 1
v1, C1
v1)
15:
Get the confidence percentage: C2
v2 ←Askv2(X, T 2
v2)
16:
end for
17: end for
Stage 3:
18: for each selected answer agent v1 do
19:
Summary the final answer: Y 3
v1 ←Summaryv1(Y 1
v1, C1
v1, T 2
v2
1, C2
v2
1, T 2
v2
2, , C2
v2
2, · · · )"
REFERENCES,0.610062893081761,"20:
Get the final confidence percentage: C3
v1 ←Askv1(Y 1
v1, C1
v1, T 2
v2
1, C2
v2
1, T 2
v2
2, , C2
v2
2, · · · )"
REFERENCES,0.6109613656783468,"21: end for
22: Generate the final answer: Y ←DiscussionZ(Y 3
v1
1, C3
v1
2, Y 3
v1
2, C3
v1
2, · · · )
The dynamic learning strategy module:
23: Initialize Lst(0), G(0)"
REFERENCES,0.6118598382749326,"24: for round l in total L rounds do
25:
for each selected answer agent v1 do
26:
Obtain coordinates: Coors ←GeoEmb(Y 3
v1), CoorsTruth ←GeoEmb(Y Truth)
27:
if Dis(Coors, CoorsTruth) ≤th then
28:
A(l) ←Enhance(e|e contains v1, e ∈E)
29:
Update Lst(l)[v1] = 1
30:
else
31:
A(l) ←Weaken(e|e contains v1, e ∈E)
32:
Update Lst(l)[v1] = 0
33:
end if
34:
end for
35: end for
36:
ˆ
A ≈A(L),
ˆ
Lst ≈Lst(L)"
REFERENCES,0.6127583108715184,"37: Update: Θ ←Loss( ˆY , Y , ˆ
A, A, ˆ
Lst, Lst)"
REFERENCES,0.6136567834681043,"tokens that each agent supports. Besides, noting that images are token consuming, we only keep the
561"
REFERENCES,0.6145552560646901,"freshest response for agent discussions.
562"
REFERENCES,0.6154537286612758,"The detailed algorithm of smileGeo is illustrated in Algorithm 1. In the initialization stage, we
563"
REFERENCES,0.6163522012578616,"initialize or load the parameters of the agent social network learning model, as delineated in line 1.
564"
REFERENCES,0.6172506738544474,"Next, we treat each LVLM agent as a node, establishing the LVLM agent collaboration social network
565"
REFERENCES,0.6181491464510332,"and computing the adjacency relationships among LVLM agents as well as the probability that each
566"
REFERENCES,0.6190476190476191,"agent is suited for responding to image X, as shown in line 2. Then, line 3 initializes the agent
567"
REFERENCES,0.6199460916442049,Stage 1: Answer Agent Election & Answering A A
REFERENCES,0.6208445642407907,"Marseille, France, 100%. The building in the image is the CMA CGM 
Tower, which is located in Marseille, France. The logo “CMA CGM” at the top 
of the building is a clear indicator of its location. This tower serves as the head-
quarters for the shipping firm CMA CGM. It’s a notable feature of Marseille’s 
skyline and was completed in September 2011. The unique, angular design of 
the building is also a distinctive feature of this skyscraper."
REFERENCES,0.6217430368373764,"Answer the location of this picture by this format: 
<city>, <country>, <confidence percentage>, <explain>. 
For example, Paris, France, 60%, *. If you need to more 
information about this picture, I have done an internet 
search for you and the top results are listed below:
1. Uptown Tower - Propsearch.ae (source：PropSearch.ae)
2. DUBAI | Uptown Tower | 333m | 1094ft | 77 fl | Com | 
Page 9 | SkyscraperCity Forum (source: SkyscraperCity 
Forum)"
REFERENCES,0.6226415094339622,"Dubai,  United Arab Emirates, 100%. The building in the image is the 
Uptown Tower located in Dubai, United Arab Emirates. This is confirmed by 
the information provided from the internet search results that mention “DUBAI 
| Uptown Tower”. The tower is a notable feature of Dubai’s skyline with its 
unique and modern architectural design. The logo “DMCC” at the top of the 
building indicates that it’s an office building for the Dubai Multi Commodities 
Centre. A A"
REFERENCES,0.6235399820305481,"Optimal Agent 
Social Connections"
REFERENCES,0.6244384546271339,"Agent Social Network Initialization
Agent Social Network Learning"
REFERENCES,0.6253369272237197,"Agent Election
Probability"
REFERENCES,0.6262353998203055,"Initialized Agent 
 Social Network"
REFERENCES,0.6271338724168913,Image X GNN
REFERENCES,0.628032345013477,GPT-4V LLaVA Qwen
REFERENCES,0.6289308176100629,Claude-3
REFERENCES,0.6298292902066487,Gemini
REFERENCES,0.6307277628032345,Stage 2: Review Agent Selection & Reviewing
REFERENCES,0.6316262353998203,"100%.  The building  in  the  image  is  indeed  the  CMA  CGM  Tower,    
which is located in Marseille, France. The logo “CMA CGM” at the top of the 
building is a clear indicator of its location. This tower serves as the headquarters 
for the shipping firm CMA CGM. It’s a notable feature of Marseille’s skyline and 
was completed in September 2011. The unique, angular design of the building is 
also a distinctive feature of this skyscraper. The sky behind is clear and ..."
REFERENCES,0.6325247079964061,"Someone say this picture is located in Dubai, United 
Arab Emirates, with confidence 100% (explain: This is 
confirmed by the information provided from the internet 
search results that mention “DUBAI | Uptown Tower”. The 
tower is a notable feature of Dubai’s skyline with its unique 
and modern architectural design. The logo “DMCC” at the 
top of the building indicates that it’s an office building for 
the Dubai Multi Commodities Centre). What do you think? 
Please use the following format to answer: <confidence 
percentage>, <explain>."
REFERENCES,0.633423180592992,"100%.  I agree with the previous identification.  The  building  in  the 
image is indeed the Uptown Tower located in Dubai, United Arab Emirates. 
This is confirmed by the logo “DMCC” at the top of the building and the unique 
architectural design of the tower, which matches the description of the Uptown 
Tower in Dubai. The information provided from the internet search results that 
mention “DUBAI | Uptown Tower” further supports this identification. The 
tower is a notable feature of Dubai’s skyline and serves as an office building for 
the Dubai Multi Commodities Centre. R"
REFERENCES,0.6343216531895777,Stage 3: Final Answer Conclusion
REFERENCES,0.6352201257861635,"You say this picture is located in Dubai, United Arab 
Emirates, with confidence 100%. Here are others comments 
with the confidence: 
1. I agree with the previous identification. The ...., 100%;
2. This image features the Uptown Tower in Dubai, ...., 100%.
Please summarize all informations by this format: <city>, 
<country>, <confidence percentage>"
REFERENCES,0.6361185983827493,": Dubai, United Arab Emirates, 100%.
A A A"
REFERENCES,0.6370170709793351,"R
R"
REFERENCES,0.637915543575921,"R, R"
REFERENCES,0.6388140161725068,"You say this picture is located in Marseille, France, with 
confidence 100%.  Here are others comments with the confi-
dence: 
1. I cannot judge ...., 60%;
2. This image features the Uptown Tower in Dubai, ...., 90%.
Please summarize all informations by this format: <city>, <coun-
try>, <confidence percentage>"
REFERENCES,0.6397124887690926,": Marseille, France, 80%.
A"
REFERENCES,0.6406109613656783,"VAE 
Encoder"
REFERENCES,0.6415094339622641,"Dubai, United Arab Emirates, 100%.
Y A A"
REFERENCES,0.6424079065588499,"R
R"
REFERENCES,0.6433063791554358,"R,"
REFERENCES,0.6442048517520216,"Discussion & harmonization v! v"" v#
v$ v' v! v"" v#
v$ v' v! v"" v#
v$ v'"
REFERENCES,0.6451033243486074,"Pv!=0.95
Pv#=0.93
Pv'=0.89"
REFERENCES,0.6460017969451932,"Pv$=0.31
Lst"
REFERENCES,0.6469002695417789,"Pv""=0.65"
REFERENCES,0.6477987421383647,"0
0.9
0.5
0.8
0.5
0.9
0
0.5
0.5
0.5
0.5
0.5
0
0.5
0.6
0.8
0.5
0.5
0
0.5
0.5
0.5
0.6
0.5
0"
REFERENCES,0.6486972147349506,"é
ù
ê
ú
ê
ú
ê
ú
=
ê
ú
ê
ú
ê
ú
ë
û A v! v!
v"" v""
v# v# v$ v$ v' v' (0)
G (0)
G"
"V
Y",0.6495956873315364,"1v
Y"
"V
Y",0.6504941599281222,"3v
Y"
"V
Y",0.651392632524708,Figure 5: A case study on the geo-localization process via a given image.
"V
Y",0.6522911051212938,"collaboration social network and line 4 computes the agent election probability. In Stage 1, line 5
568"
"V
Y",0.6531895777178796,"involves electing appropriate answer agents based on the calculated probabilities. Subsequently, lines
569"
"V
Y",0.6540880503144654,"6-10 detail the process through which each chosen answer agent formulates their response. Stage 2
570"
"V
Y",0.6549865229110512,"begins by employing the random walk algorithm to assign review agents to each answer agent, as
571"
"V
Y",0.655884995507637,"depicted in lines 11-12. Lines 13-16 then describe how these review agents generate feedback based
572"
"V
Y",0.6567834681042228,"on the answers provided. In Stage 3, each answer agent consolidates feedback from their assigned
573"
"V
Y",0.6576819407008087,"review agents to finalize their response, as illustrated in lines 18-21. Line 22 concludes the final
574"
"V
Y",0.6585804132973945,"answer with up to Z rounds (we set Z = 10 in experiments) of intra-discussion among all answer
575"
"V
Y",0.6594788858939802,"agents only. The dynamic learning strategy module involves L-round (we set L = 20 in experiments)
576"
"V
Y",0.660377358490566,"comparing the generated answers against the ground truth and updating the connections between the
577"
"V
Y",0.6612758310871518,"answer and review agents accordingly, as shown in lines 23-36. In line 37, the process concludes
578"
"V
Y",0.6621743036837376,"with the updating of the learning parameters of the dynamic agent social network learning model.
579"
"V
Y",0.6630727762803235,"Here, for the agent social network learning model, we first deflate each image to be recognized to
580"
"V
Y",0.6639712488769093,"512x512 pixels and then use the pre-trained VAE model11 to compress the image again (compression
581"
"V
Y",0.6648697214734951,11https://huggingface.co/stabilityai/sd-vae-ft-mse
"V
Y",0.6657681940700808,"Needs to
be localized"
"V
Y",0.6666666666666666,(a) An example of the input image
"V
Y",0.6675651392632524,Answer the location of this pcicture.
"V
Y",0.6684636118598383,"There's the Statue of Liberty and part of the Eiffel Tower. It’s confusing 
that two different object appear in the same picture."
"V
Y",0.6693620844564241,"The image does not provide any clear visual cues about a specific loca- 
tion. Without more context about  the details in the image itself, I do not have 
enough information to confidently determine the city or country."
"V
Y",0.6702605570530099,"New York City, United States. The Statue of Liberty is located on Liberty 
Island in New York Harbor."
"V
Y",0.6711590296495957,"There's the Statue of Liberty. It is located in New York, United States."
"V
Y",0.6720575022461814,"It is a miniature version of the Statue of Liberty, located at the southwest 
end of Swan Island, a long and narrow artificial island in the Seine River. It 
is a gift from the American people to the French people."
"V
Y",0.6729559748427673,(b) Various thoughts about the image
"V
Y",0.6738544474393531,(c) Actual locations of two landmarks
"V
Y",0.6747529200359389,smileGeo
"V
Y",0.6756513926325247,"Paris, France, 95%.
Y"
"V
Y",0.6765498652291105,GPT-4V LLaVA Qwen
"V
Y",0.6774483378256964,Claude-3
"V
Y",0.6783468104222821,"Gemini v! v"" v#
v$ v& G
i i"
"V
Y",0.6792452830188679,(d) The final answer of smileGeo
"V
Y",0.6801437556154537,Figure 6: A case study illustrating the reasoning capabilities of smileGeo.
"V
Y",0.6810422282120395,"ratio 1:8) and extract its representations. We define the embedding dimension of the nodes to be 1024
582"
"V
Y",0.6819407008086253,"and the hidden layer dimension of the network layer to be 1024. we use Adam as an optimizer for
583"
"V
Y",0.6828391734052112,"gradient descent with a learning rate of 1e−5. For each stage of the LVLM agent discussion, we use a
584"
"V
Y",0.683737646001797,"uniform template to ask questions to different LVLM agents and ask them to make a response in the
585"
"V
Y",0.6846361185983828,"specified format. In addition, the performance of our proposed framework is the average of the last
586"
"V
Y",0.6855345911949685,"100 epochs in a total training of 2500 epochs.
587"
"V
Y",0.6864330637915543,"D
Additional Experiments
588"
"V
Y",0.6873315363881402,"D.1
Case Study
589"
"V
Y",0.688230008984726,"Case 1: In Figure 5, we illustrate the application of smileGeo in a visual geo-localization task.
590"
"V
Y",0.6891284815813118,"For this demonstration, we randomly select an image from the test dataset and employ five distinct
591"
"V
Y",0.6900269541778976,"LVLMs: LLaVA, GPT-4V, Claude-3, Gemini, and Qwen. The agent selection model selects two
592"
"V
Y",0.6909254267744834,"answer agents, as depicted in the top part of the figure. Subsequently, stages 1 through 3 detail the
593"
"V
Y",0.6918238993710691,"process of generating the accurate geo-location. Initially, only one answer agent provided the correct
594"
"V
Y",0.692722371967655,"response. However, after several rounds of discussion, the agent that initially responded incorrectly
595"
"V
Y",0.6936208445642408,"revised the confidence level of its answer. During the final internal discussion, this agent aligned its
596"
"V
Y",0.6945193171608266,"response with the correct answer. This outcome validates the efficacy of our proposed framework,
597"
"V
Y",0.6954177897574124,"demonstrating its ability to integrate the knowledge and reasoning capabilities of different agents to
598"
"V
Y",0.6963162623539982,"enhance the overall performance of the proposed LVLM agent framework.
599"
"V
Y",0.6972147349505841,"Case 2: This case study illustrates the need to pinpoint the geographical location of a complete
600"
"V
Y",0.6981132075471698,"image based on only a portion of it, as demonstrated in 6(a). As illustrated in Figure 6(b), all agents
601"
"V
Y",0.6990116801437556,"recognized the Statue of Liberty in Figure 6(a), and some identified the presence of part of the Eiffel
602"
"V
Y",0.6999101527403414,"Tower at the edge of the picture. For instance, GPT-4V concluded that the buildings in these two
603"
"V
Y",0.7008086253369272,"locations appeared in the same image. However, as is known through the knowledge of other agents
604"
"V
Y",0.701707097933513,"(Gemini), a scaled-down version of the Statue of Liberty has been erected on Swan Island, an artificial
605"
"V
Y",0.7026055705300989,"island in the Seine River in France. By marking both the Eiffel Tower and the island on the Open
606"
"V
Y",0.7035040431266847,"Street Map (OSM) manually, as shown in Figure 6(c), it is evident that they are merely 1.3 kilometers
607"
"V
Y",0.7044025157232704,"apart in a straight line. By utilizing the proposed framework, agents discuss and summarize the
608"
"V
Y",0.7053009883198562,"location depicted in the picture to be Paris, France, as shown in Figure 6(d). Thus, without human
609"
"V
Y",0.706199460916442,"intervention, this framework demonstrates the effectiveness of doing geo-localization tasks.
610"
"V
Y",0.7070979335130279,"NeurIPS Paper Checklist
611"
CLAIMS,0.7079964061096137,"1. Claims
612"
CLAIMS,0.7088948787061995,"Question: Do the main claims made in the abstract and introduction accurately reflect the
613"
CLAIMS,0.7097933513027853,"paper’s contributions and scope?
614"
CLAIMS,0.710691823899371,"Answer: [Yes]
615"
CLAIMS,0.7115902964959568,"Justification: Our work proposes a swarm intelligence geo-localization framework, smileGeo,
616"
CLAIMS,0.7124887690925427,"which contains the process of the review mechanism in agent discussions along with a
617"
CLAIMS,0.7133872416891285,"dynamic learning strategy of agent collaboration social network, to achieve open-world
618"
CLAIMS,0.7142857142857143,"geo-localization tasks. In addition, we construct a novel geo-localization dataset, GeoGlobe
619"
CLAIMS,0.7151841868823001,"for evaluation and it will be public. All of the contributions we claimed in both abstract and
620"
CLAIMS,0.7160826594788859,"introduction.
621"
CLAIMS,0.7169811320754716,"Guidelines:
622"
CLAIMS,0.7178796046720575,"• The answer NA means that the abstract and introduction do not include the claims
623"
CLAIMS,0.7187780772686433,"made in the paper.
624"
CLAIMS,0.7196765498652291,"• The abstract and/or introduction should clearly state the claims made, including the
625"
CLAIMS,0.7205750224618149,"contributions made in the paper and important assumptions and limitations. A No or
626"
CLAIMS,0.7214734950584007,"NA answer to this question will not be perceived well by the reviewers.
627"
CLAIMS,0.7223719676549866,"• The claims made should match theoretical and experimental results, and reflect how
628"
CLAIMS,0.7232704402515723,"much the results can be expected to generalize to other settings.
629"
CLAIMS,0.7241689128481581,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
630"
CLAIMS,0.7250673854447439,"are not attained by the paper.
631"
LIMITATIONS,0.7259658580413297,"2. Limitations
632"
LIMITATIONS,0.7268643306379156,"Question: Does the paper discuss the limitations of the work performed by the authors?
633"
LIMITATIONS,0.7277628032345014,"Answer: [Yes]
634"
LIMITATIONS,0.7286612758310872,"Justification: At present, the LVLM agent framework we proposed can only search the
635"
LIMITATIONS,0.7295597484276729,"Internet autonomously. Our agent still has shortcomings in the use of other multiple tools.
636"
LIMITATIONS,0.7304582210242587,"We stated in our future outlook that our follow-up work will solve this problem.
637"
LIMITATIONS,0.7313566936208445,"Guidelines:
638"
LIMITATIONS,0.7322551662174304,"• The answer NA means that the paper has no limitation while the answer No means that
639"
LIMITATIONS,0.7331536388140162,"the paper has limitations, but those are not discussed in the paper.
640"
LIMITATIONS,0.734052111410602,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
641"
LIMITATIONS,0.7349505840071878,"• The paper should point out any strong assumptions and how robust the results are to
642"
LIMITATIONS,0.7358490566037735,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
643"
LIMITATIONS,0.7367475292003594,"model well-specification, asymptotic approximations only holding locally). The authors
644"
LIMITATIONS,0.7376460017969452,"should reflect on how these assumptions might be violated in practice and what the
645"
LIMITATIONS,0.738544474393531,"implications would be.
646"
LIMITATIONS,0.7394429469901168,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
647"
LIMITATIONS,0.7403414195867026,"only tested on a few datasets or with a few runs. In general, empirical results often
648"
LIMITATIONS,0.7412398921832885,"depend on implicit assumptions, which should be articulated.
649"
LIMITATIONS,0.7421383647798742,"• The authors should reflect on the factors that influence the performance of the approach.
650"
LIMITATIONS,0.74303683737646,"For example, a facial recognition algorithm may perform poorly when image resolution
651"
LIMITATIONS,0.7439353099730458,"is low or images are taken in low lighting. Or a speech-to-text system might not be
652"
LIMITATIONS,0.7448337825696316,"used reliably to provide closed captions for online lectures because it fails to handle
653"
LIMITATIONS,0.7457322551662174,"technical jargon.
654"
LIMITATIONS,0.7466307277628033,"• The authors should discuss the computational efficiency of the proposed algorithms
655"
LIMITATIONS,0.7475292003593891,"and how they scale with dataset size.
656"
LIMITATIONS,0.7484276729559748,"• If applicable, the authors should discuss possible limitations of their approach to
657"
LIMITATIONS,0.7493261455525606,"address problems of privacy and fairness.
658"
LIMITATIONS,0.7502246181491464,"• While the authors might fear that complete honesty about limitations might be used by
659"
LIMITATIONS,0.7511230907457322,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
660"
LIMITATIONS,0.7520215633423181,"limitations that aren’t acknowledged in the paper. The authors should use their best
661"
LIMITATIONS,0.7529200359389039,"judgment and recognize that individual actions in favor of transparency play an impor-
662"
LIMITATIONS,0.7538185085354897,"tant role in developing norms that preserve the integrity of the community. Reviewers
663"
LIMITATIONS,0.7547169811320755,"will be specifically instructed to not penalize honesty concerning limitations.
664"
THEORY ASSUMPTIONS AND PROOFS,0.7556154537286612,"3. Theory Assumptions and Proofs
665"
THEORY ASSUMPTIONS AND PROOFS,0.756513926325247,"Question: For each theoretical result, does the paper provide the full set of assumptions and
666"
THEORY ASSUMPTIONS AND PROOFS,0.7574123989218329,"a complete (and correct) proof?
667"
THEORY ASSUMPTIONS AND PROOFS,0.7583108715184187,"Answer: [NA]
668"
THEORY ASSUMPTIONS AND PROOFS,0.7592093441150045,"Justification: This work is a solution to the problem of geo-localization in application
669"
THEORY ASSUMPTIONS AND PROOFS,0.7601078167115903,"scenarios. We have provided the source code and will release the related dataset, as the
670"
THEORY ASSUMPTIONS AND PROOFS,0.7610062893081762,"dataset is relatively large (about 32 GB) and cannot be uploaded as an attachment.
671"
THEORY ASSUMPTIONS AND PROOFS,0.7619047619047619,"Guidelines:
672"
THEORY ASSUMPTIONS AND PROOFS,0.7628032345013477,"• The answer NA means that the paper does not include theoretical results.
673"
THEORY ASSUMPTIONS AND PROOFS,0.7637017070979335,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
674"
THEORY ASSUMPTIONS AND PROOFS,0.7646001796945193,"referenced.
675"
THEORY ASSUMPTIONS AND PROOFS,0.7654986522911051,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
676"
THEORY ASSUMPTIONS AND PROOFS,0.766397124887691,"• The proofs can either appear in the main paper or the supplemental material, but if
677"
THEORY ASSUMPTIONS AND PROOFS,0.7672955974842768,"they appear in the supplemental material, the authors are encouraged to provide a short
678"
THEORY ASSUMPTIONS AND PROOFS,0.7681940700808625,"proof sketch to provide intuition.
679"
THEORY ASSUMPTIONS AND PROOFS,0.7690925426774483,"• Inversely, any informal proof provided in the core of the paper should be complemented
680"
THEORY ASSUMPTIONS AND PROOFS,0.7699910152740341,"by formal proofs provided in appendix or supplemental material.
681"
THEORY ASSUMPTIONS AND PROOFS,0.77088948787062,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
682"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7717879604672058,"4. Experimental Result Reproducibility
683"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7726864330637916,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
684"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7735849056603774,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
685"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7744833782569631,"of the paper (regardless of whether the code and data are provided or not)?
686"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7753818508535489,"Answer: [Yes]
687"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7762803234501348,"Justification: We provided the source code and will release the related dataset once the paper
688"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7771787960467206,"is accepted, as the dataset is relatively large (about 32 GB) and cannot be uploaded as an
689"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7780772686433064,"attachment.
690"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7789757412398922,"Guidelines:
691"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.779874213836478,"• The answer NA means that the paper does not include experiments.
692"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7807726864330637,"• If the paper includes experiments, a No answer to this question will not be perceived
693"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7816711590296496,"well by the reviewers: Making the paper reproducible is important, regardless of
694"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7825696316262354,"whether the code and data are provided or not.
695"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7834681042228212,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
696"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.784366576819407,"to make their results reproducible or verifiable.
697"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7852650494159928,"• Depending on the contribution, reproducibility can be accomplished in various ways.
698"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7861635220125787,"For example, if the contribution is a novel architecture, describing the architecture fully
699"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7870619946091644,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
700"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7879604672057502,"be necessary to either make it possible for others to replicate the model with the same
701"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.788858939802336,"dataset, or provide access to the model. In general. releasing code and data is often
702"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7897574123989218,"one good way to accomplish this, but reproducibility can also be provided via detailed
703"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7906558849955077,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
704"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7915543575920935,"of a large language model), releasing of a model checkpoint, or other means that are
705"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7924528301886793,"appropriate to the research performed.
706"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.793351302785265,"• While NeurIPS does not require releasing code, the conference does require all submis-
707"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7942497753818508,"sions to provide some reasonable avenue for reproducibility, which may depend on the
708"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7951482479784366,"nature of the contribution. For example
709"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7960467205750225,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
710"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7969451931716083,"to reproduce that algorithm.
711"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7978436657681941,"(b) If the contribution is primarily a new model architecture, the paper should describe
712"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7987421383647799,"the architecture clearly and fully.
713"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7996406109613656,"(c) If the contribution is a new model (e.g., a large language model), then there should
714"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8005390835579514,"either be a way to access this model for reproducing the results or a way to reproduce
715"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8014375561545373,"the model (e.g., with an open-source dataset or instructions for how to construct
716"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8023360287511231,"the dataset).
717"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8032345013477089,"(d) We recognize that reproducibility may be tricky in some cases, in which case
718"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8041329739442947,"authors are welcome to describe the particular way they provide for reproducibility.
719"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8050314465408805,"In the case of closed-source models, it may be that access to the model is limited in
720"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8059299191374663,"some way (e.g., to registered users), but it should be possible for other researchers
721"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8068283917340521,"to have some path to reproducing or verifying the results.
722"
OPEN ACCESS TO DATA AND CODE,0.8077268643306379,"5. Open access to data and code
723"
OPEN ACCESS TO DATA AND CODE,0.8086253369272237,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
724"
OPEN ACCESS TO DATA AND CODE,0.8095238095238095,"tions to faithfully reproduce the main experimental results, as described in supplemental
725"
OPEN ACCESS TO DATA AND CODE,0.8104222821203954,"material?
726"
OPEN ACCESS TO DATA AND CODE,0.8113207547169812,"Answer: [Yes]
727"
OPEN ACCESS TO DATA AND CODE,0.8122192273135669,"Justification: We provide the anonymous code link:
https://anonymous.4open.science/
728"
OPEN ACCESS TO DATA AND CODE,0.8131176999101527,"r/ViusalGeoLocalization-F8F5/. In this link, we also provide a small-scale dataset we
729"
OPEN ACCESS TO DATA AND CODE,0.8140161725067385,"collected for people to reproduce the results.
730"
OPEN ACCESS TO DATA AND CODE,0.8149146451033243,"Guidelines:
731"
OPEN ACCESS TO DATA AND CODE,0.8158131176999102,"• The answer NA means that paper does not include experiments requiring code.
732"
OPEN ACCESS TO DATA AND CODE,0.816711590296496,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
733"
OPEN ACCESS TO DATA AND CODE,0.8176100628930818,"public/guides/CodeSubmissionPolicy) for more details.
734"
OPEN ACCESS TO DATA AND CODE,0.8185085354896675,"• While we encourage the release of code and data, we understand that this might not be
735"
OPEN ACCESS TO DATA AND CODE,0.8194070080862533,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
736"
OPEN ACCESS TO DATA AND CODE,0.8203054806828391,"including code, unless this is central to the contribution (e.g., for a new open-source
737"
OPEN ACCESS TO DATA AND CODE,0.821203953279425,"benchmark).
738"
OPEN ACCESS TO DATA AND CODE,0.8221024258760108,"• The instructions should contain the exact command and environment needed to run to
739"
OPEN ACCESS TO DATA AND CODE,0.8230008984725966,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
740"
OPEN ACCESS TO DATA AND CODE,0.8238993710691824,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
741"
OPEN ACCESS TO DATA AND CODE,0.8247978436657682,"• The authors should provide instructions on data access and preparation, including how
742"
OPEN ACCESS TO DATA AND CODE,0.825696316262354,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
743"
OPEN ACCESS TO DATA AND CODE,0.8265947888589398,"• The authors should provide scripts to reproduce all experimental results for the new
744"
OPEN ACCESS TO DATA AND CODE,0.8274932614555256,"proposed method and baselines. If only a subset of experiments are reproducible, they
745"
OPEN ACCESS TO DATA AND CODE,0.8283917340521114,"should state which ones are omitted from the script and why.
746"
OPEN ACCESS TO DATA AND CODE,0.8292902066486972,"• At submission time, to preserve anonymity, the authors should release anonymized
747"
OPEN ACCESS TO DATA AND CODE,0.8301886792452831,"versions (if applicable).
748"
OPEN ACCESS TO DATA AND CODE,0.8310871518418689,"• Providing as much information as possible in supplemental material (appended to the
749"
OPEN ACCESS TO DATA AND CODE,0.8319856244384546,"paper) is recommended, but including URLs to data and code is permitted.
750"
OPEN ACCESS TO DATA AND CODE,0.8328840970350404,"6. Experimental Setting/Details
751"
OPEN ACCESS TO DATA AND CODE,0.8337825696316262,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
752"
OPEN ACCESS TO DATA AND CODE,0.834681042228212,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
753"
OPEN ACCESS TO DATA AND CODE,0.8355795148247979,"results?
754"
OPEN ACCESS TO DATA AND CODE,0.8364779874213837,"Answer: [Yes]
755"
OPEN ACCESS TO DATA AND CODE,0.8373764600179695,"Justification: We explain all the settings in both the main paper (Experiments) and the
756"
OPEN ACCESS TO DATA AND CODE,0.8382749326145552,"appendix (Implementation Details).
757"
OPEN ACCESS TO DATA AND CODE,0.839173405211141,"Guidelines:
758"
OPEN ACCESS TO DATA AND CODE,0.8400718778077269,"• The answer NA means that the paper does not include experiments.
759"
OPEN ACCESS TO DATA AND CODE,0.8409703504043127,"• The experimental setting should be presented in the core of the paper to a level of detail
760"
OPEN ACCESS TO DATA AND CODE,0.8418688230008985,"that is necessary to appreciate the results and make sense of them.
761"
OPEN ACCESS TO DATA AND CODE,0.8427672955974843,"• The full details can be provided either with the code, in appendix, or as supplemental
762"
OPEN ACCESS TO DATA AND CODE,0.8436657681940701,"material.
763"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8445642407906558,"7. Experiment Statistical Significance
764"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8454627133872417,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
765"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8463611859838275,"information about the statistical significance of the experiments?
766"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8472596585804133,"Answer: [Yes]
767"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8481581311769991,"Justification: We deploy a two-sided t-test with p < 0.05 for our baseline experiments.
768"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8490566037735849,"Guidelines:
769"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8499550763701708,"• The answer NA means that the paper does not include experiments.
770"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8508535489667565,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
771"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8517520215633423,"dence intervals, or statistical significance tests, at least for the experiments that support
772"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8526504941599281,"the main claims of the paper.
773"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8535489667565139,"• The factors of variability that the error bars are capturing should be clearly stated (for
774"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8544474393530997,"example, train/test split, initialization, random drawing of some parameter, or overall
775"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8553459119496856,"run with given experimental conditions).
776"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8562443845462714,"• The method for calculating the error bars should be explained (closed form formula,
777"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8571428571428571,"call to a library function, bootstrap, etc.)
778"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8580413297394429,"• The assumptions made should be given (e.g., Normally distributed errors).
779"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8589398023360287,"• It should be clear whether the error bar is the standard deviation or the standard error
780"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8598382749326146,"of the mean.
781"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8607367475292004,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
782"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8616352201257862,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
783"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.862533692722372,"of Normality of errors is not verified.
784"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8634321653189577,"• For asymmetric distributions, the authors should be careful not to show in tables or
785"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8643306379155435,"figures symmetric error bars that would yield results that are out of range (e.g. negative
786"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8652291105121294,"error rates).
787"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8661275831087152,"• If error bars are reported in tables or plots, The authors should explain in the text how
788"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.867026055705301,"they were calculated and reference the corresponding figures or tables in the text.
789"
EXPERIMENTS COMPUTE RESOURCES,0.8679245283018868,"8. Experiments Compute Resources
790"
EXPERIMENTS COMPUTE RESOURCES,0.8688230008984726,"Question: For each experiment, does the paper provide sufficient information on the com-
791"
EXPERIMENTS COMPUTE RESOURCES,0.8697214734950584,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
792"
EXPERIMENTS COMPUTE RESOURCES,0.8706199460916442,"the experiments?
793"
EXPERIMENTS COMPUTE RESOURCES,0.87151841868823,"Answer: [Yes]
794"
EXPERIMENTS COMPUTE RESOURCES,0.8724168912848158,"Justification: We announce the compute resources in the appendix (Implementation Details).
795"
EXPERIMENTS COMPUTE RESOURCES,0.8733153638814016,"Guidelines:
796"
EXPERIMENTS COMPUTE RESOURCES,0.8742138364779874,"• The answer NA means that the paper does not include experiments.
797"
EXPERIMENTS COMPUTE RESOURCES,0.8751123090745733,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
798"
EXPERIMENTS COMPUTE RESOURCES,0.876010781671159,"or cloud provider, including relevant memory and storage.
799"
EXPERIMENTS COMPUTE RESOURCES,0.8769092542677448,"• The paper should provide the amount of compute required for each of the individual
800"
EXPERIMENTS COMPUTE RESOURCES,0.8778077268643306,"experimental runs as well as estimate the total compute.
801"
EXPERIMENTS COMPUTE RESOURCES,0.8787061994609164,"• The paper should disclose whether the full research project required more compute
802"
EXPERIMENTS COMPUTE RESOURCES,0.8796046720575023,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
803"
EXPERIMENTS COMPUTE RESOURCES,0.8805031446540881,"didn’t make it into the paper).
804"
CODE OF ETHICS,0.8814016172506739,"9. Code Of Ethics
805"
CODE OF ETHICS,0.8823000898472596,"Question: Does the research conducted in the paper conform, in every respect, with the
806"
CODE OF ETHICS,0.8831985624438454,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
807"
CODE OF ETHICS,0.8840970350404312,"Answer: [Yes]
808"
CODE OF ETHICS,0.8849955076370171,"Justification: The codes used in our paper are all open source, and the data used in the paper
809"
CODE OF ETHICS,0.8858939802336029,"come from copyright-free images on the Internet.
810"
CODE OF ETHICS,0.8867924528301887,"Guidelines:
811"
CODE OF ETHICS,0.8876909254267745,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
812"
CODE OF ETHICS,0.8885893980233602,"• If the authors answer No, they should explain the special circumstances that require a
813"
CODE OF ETHICS,0.889487870619946,"deviation from the Code of Ethics.
814"
CODE OF ETHICS,0.8903863432165319,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
815"
CODE OF ETHICS,0.8912848158131177,"eration due to laws or regulations in their jurisdiction).
816"
BROADER IMPACTS,0.8921832884097035,"10. Broader Impacts
817"
BROADER IMPACTS,0.8930817610062893,"Question: Does the paper discuss both potential positive societal impacts and negative
818"
BROADER IMPACTS,0.8939802336028752,"societal impacts of the work performed?
819"
BROADER IMPACTS,0.894878706199461,"Answer: [Yes]
820"
BROADER IMPACTS,0.8957771787960467,"Justification: We have an outlook on our research in the section Conclusion, which can be
821"
BROADER IMPACTS,0.8966756513926325,"widely used in robot positioning and navigation in the future.
822"
BROADER IMPACTS,0.8975741239892183,"Guidelines:
823"
BROADER IMPACTS,0.8984725965858041,"• The answer NA means that there is no societal impact of the work performed.
824"
BROADER IMPACTS,0.89937106918239,"• If the authors answer NA or No, they should explain why their work has no societal
825"
BROADER IMPACTS,0.9002695417789758,"impact or why the paper does not address societal impact.
826"
BROADER IMPACTS,0.9011680143755616,"• Examples of negative societal impacts include potential malicious or unintended uses
827"
BROADER IMPACTS,0.9020664869721473,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
828"
BROADER IMPACTS,0.9029649595687331,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
829"
BROADER IMPACTS,0.903863432165319,"groups), privacy considerations, and security considerations.
830"
BROADER IMPACTS,0.9047619047619048,"• The conference expects that many papers will be foundational research and not tied
831"
BROADER IMPACTS,0.9056603773584906,"to particular applications, let alone deployments. However, if there is a direct path to
832"
BROADER IMPACTS,0.9065588499550764,"any negative applications, the authors should point it out. For example, it is legitimate
833"
BROADER IMPACTS,0.9074573225516622,"to point out that an improvement in the quality of generative models could be used to
834"
BROADER IMPACTS,0.9083557951482479,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
835"
BROADER IMPACTS,0.9092542677448338,"that a generic algorithm for optimizing neural networks could enable people to train
836"
BROADER IMPACTS,0.9101527403414196,"models that generate Deepfakes faster.
837"
BROADER IMPACTS,0.9110512129380054,"• The authors should consider possible harms that could arise when the technology is
838"
BROADER IMPACTS,0.9119496855345912,"being used as intended and functioning correctly, harms that could arise when the
839"
BROADER IMPACTS,0.912848158131177,"technology is being used as intended but gives incorrect results, and harms following
840"
BROADER IMPACTS,0.9137466307277629,"from (intentional or unintentional) misuse of the technology.
841"
BROADER IMPACTS,0.9146451033243486,"• If there are negative societal impacts, the authors could also discuss possible mitigation
842"
BROADER IMPACTS,0.9155435759209344,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
843"
BROADER IMPACTS,0.9164420485175202,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
844"
BROADER IMPACTS,0.917340521114106,"feedback over time, improving the efficiency and accessibility of ML).
845"
SAFEGUARDS,0.9182389937106918,"11. Safeguards
846"
SAFEGUARDS,0.9191374663072777,"Question: Does the paper describe safeguards that have been put in place for responsible
847"
SAFEGUARDS,0.9200359389038635,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
848"
SAFEGUARDS,0.9209344115004492,"image generators, or scraped datasets)?
849"
SAFEGUARDS,0.921832884097035,"Answer: [Yes]
850"
SAFEGUARDS,0.9227313566936208,"Justification: The data sets we collect have been manually reviewed twice, and all data
851"
SAFEGUARDS,0.9236298292902066,"containing various types of sensitive information or copyright risks have been filtered out.
852"
SAFEGUARDS,0.9245283018867925,"Guidelines:
853"
SAFEGUARDS,0.9254267744833783,"• The answer NA means that the paper poses no such risks.
854"
SAFEGUARDS,0.9263252470799641,"• Released models that have a high risk for misuse or dual-use should be released with
855"
SAFEGUARDS,0.9272237196765498,"necessary safeguards to allow for controlled use of the model, for example by requiring
856"
SAFEGUARDS,0.9281221922731356,"that users adhere to usage guidelines or restrictions to access the model or implementing
857"
SAFEGUARDS,0.9290206648697215,"safety filters.
858"
SAFEGUARDS,0.9299191374663073,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
859"
SAFEGUARDS,0.9308176100628931,"should describe how they avoided releasing unsafe images.
860"
SAFEGUARDS,0.9317160826594789,"• We recognize that providing effective safeguards is challenging, and many papers do
861"
SAFEGUARDS,0.9326145552560647,"not require this, but we encourage authors to take this into account and make a best
862"
SAFEGUARDS,0.9335130278526504,"faith effort.
863"
LICENSES FOR EXISTING ASSETS,0.9344115004492363,"12. Licenses for existing assets
864"
LICENSES FOR EXISTING ASSETS,0.9353099730458221,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
865"
LICENSES FOR EXISTING ASSETS,0.9362084456424079,"the paper, properly credited and are the license and terms of use explicitly mentioned and
866"
LICENSES FOR EXISTING ASSETS,0.9371069182389937,"properly respected?
867"
LICENSES FOR EXISTING ASSETS,0.9380053908355795,"Answer: [Yes]
868"
LICENSES FOR EXISTING ASSETS,0.9389038634321654,"Justification: We list and acknowledge all other open-source codes we used in the file
869"
LICENSES FOR EXISTING ASSETS,0.9398023360287511,"’README.md’ and we follow the license for existing assets.
870"
LICENSES FOR EXISTING ASSETS,0.9407008086253369,"Guidelines:
871"
LICENSES FOR EXISTING ASSETS,0.9415992812219227,"• The answer NA means that the paper does not use existing assets.
872"
LICENSES FOR EXISTING ASSETS,0.9424977538185085,"• The authors should cite the original paper that produced the code package or dataset.
873"
LICENSES FOR EXISTING ASSETS,0.9433962264150944,"• The authors should state which version of the asset is used and, if possible, include a
874"
LICENSES FOR EXISTING ASSETS,0.9442946990116802,"URL.
875"
LICENSES FOR EXISTING ASSETS,0.945193171608266,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
876"
LICENSES FOR EXISTING ASSETS,0.9460916442048517,"• For scraped data from a particular source (e.g., website), the copyright and terms of
877"
LICENSES FOR EXISTING ASSETS,0.9469901168014375,"service of that source should be provided.
878"
LICENSES FOR EXISTING ASSETS,0.9478885893980233,"• If assets are released, the license, copyright information, and terms of use in the
879"
LICENSES FOR EXISTING ASSETS,0.9487870619946092,"package should be provided. For popular datasets, paperswithcode.com/datasets
880"
LICENSES FOR EXISTING ASSETS,0.949685534591195,"has curated licenses for some datasets. Their licensing guide can help determine the
881"
LICENSES FOR EXISTING ASSETS,0.9505840071877808,"license of a dataset.
882"
LICENSES FOR EXISTING ASSETS,0.9514824797843666,"• For existing datasets that are re-packaged, both the original license and the license of
883"
LICENSES FOR EXISTING ASSETS,0.9523809523809523,"the derived asset (if it has changed) should be provided.
884"
LICENSES FOR EXISTING ASSETS,0.9532794249775381,"• If this information is not available online, the authors are encouraged to reach out to
885"
LICENSES FOR EXISTING ASSETS,0.954177897574124,"the asset’s creators.
886"
NEW ASSETS,0.9550763701707098,"13. New Assets
887"
NEW ASSETS,0.9559748427672956,"Question: Are new assets introduced in the paper well documented and is the documentation
888"
NEW ASSETS,0.9568733153638814,"provided alongside the assets?
889"
NEW ASSETS,0.9577717879604672,"Answer: [Yes]
890"
NEW ASSETS,0.958670260557053,"Justification: In this paper, we provide the algorithm of the code and introduce the dataset in
891"
NEW ASSETS,0.9595687331536388,"detail (in the appendix).
892"
NEW ASSETS,0.9604672057502246,"Guidelines:
893"
NEW ASSETS,0.9613656783468104,"• The answer NA means that the paper does not release new assets.
894"
NEW ASSETS,0.9622641509433962,"• Researchers should communicate the details of the dataset/code/model as part of their
895"
NEW ASSETS,0.963162623539982,"submissions via structured templates. This includes details about training, license,
896"
NEW ASSETS,0.9640610961365679,"limitations, etc.
897"
NEW ASSETS,0.9649595687331537,"• The paper should discuss whether and how consent was obtained from people whose
898"
NEW ASSETS,0.9658580413297394,"asset is used.
899"
NEW ASSETS,0.9667565139263252,"• At submission time, remember to anonymize your assets (if applicable). You can either
900"
NEW ASSETS,0.967654986522911,"create an anonymized URL or include an anonymized zip file.
901"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9685534591194969,"14. Crowdsourcing and Research with Human Subjects
902"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9694519317160827,"Question: For crowdsourcing experiments and research with human subjects, does the paper
903"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9703504043126685,"include the full text of instructions given to participants and screenshots, if applicable, as
904"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9712488769092543,"well as details about compensation (if any)?
905"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.97214734950584,"Answer: [NA]
906"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9730458221024259,"Justification: This paper aims to address visual geo-localization tasks and does not contain
907"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9739442946990117,"any experiments with human subjects.
908"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9748427672955975,"Guidelines:
909"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9757412398921833,"• The answer NA means that the paper does not involve crowdsourcing nor research with
910"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9766397124887691,"human subjects.
911"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.977538185085355,"• Including this information in the supplemental material is fine, but if the main contribu-
912"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9784366576819407,"tion of the paper involves human subjects, then as much detail as possible should be
913"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9793351302785265,"included in the main paper.
914"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9802336028751123,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
915"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9811320754716981,"or other labor should be paid at least the minimum wage in the country of the data
916"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9820305480682839,"collector.
917"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9829290206648698,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
918"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9838274932614556,"Subjects
919"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9847259658580413,"Question: Does the paper describe potential risks incurred by study participants, whether
920"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9856244384546271,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
921"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9865229110512129,"approvals (or an equivalent approval/review based on the requirements of your country or
922"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9874213836477987,"institution) were obtained?
923"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9883198562443846,"Answer: [NA]
924"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9892183288409704,"Justification: This paper does not contain any experiments with human subjects.
925"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9901168014375562,"Guidelines:
926"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9910152740341419,"• The answer NA means that the paper does not involve crowdsourcing nor research with
927"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9919137466307277,"human subjects.
928"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9928122192273136,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
929"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9937106918238994,"may be required for any human subjects research. If you obtained IRB approval, you
930"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9946091644204852,"should clearly state this in the paper.
931"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.995507637017071,"• We recognize that the procedures for this may vary significantly between institutions
932"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9964061096136568,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
933"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9973045822102425,"guidelines for their institution.
934"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9982030548068284,"• For initial submissions, do not include any information that would break anonymity (if
935"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9991015274034142,"applicable), such as the institution conducting the review.
936"
