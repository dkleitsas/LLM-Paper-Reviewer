Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0010775862068965517,"Causal inference from observational data has attracted considerable attention among
1"
ABSTRACT,0.0021551724137931034,"researchers. One main obstacle is the handling of confounders. As direct mea-
2"
ABSTRACT,0.003232758620689655,"surement of confounders may not be feasible, recent methods seek to address the
3"
ABSTRACT,0.004310344827586207,"confounding bias via proxy variables, i.e., covariates postulated to be conducive to
4"
ABSTRACT,0.005387931034482759,"the inference of latent confounders. However, the selected proxies may scramble
5"
ABSTRACT,0.00646551724137931,"both confounders and post-treatment variables in practice, which risks biasing the
6"
ABSTRACT,0.007543103448275862,"estimation by controlling for variables affected by the treatment. In this paper, we
7"
ABSTRACT,0.008620689655172414,"systematically investigate the bias due to latent post-treatment variables, i.e., latent
8"
ABSTRACT,0.009698275862068966,"post-treatment bias, in causal effect estimation. Specifically, we first derive the
9"
ABSTRACT,0.010775862068965518,"bias when selected proxies scramble both confounders and post-treatment variables,
10"
ABSTRACT,0.011853448275862068,"which we demonstrate can be arbitrarily bad. We then propose a novel Confounder-
11"
ABSTRACT,0.01293103448275862,"identifiable VAE (CiVAE) to address the bias. Based on a mild assumption that the
12"
ABSTRACT,0.014008620689655173,"prior of latent variables that generate the proxy belongs to a general exponential
13"
ABSTRACT,0.015086206896551725,"family with at least one invertible sufficient statistic in the factorized part, CiVAE
14"
ABSTRACT,0.016163793103448277,"individually identifies latent confounders and latent post-treatment variables up
15"
ABSTRACT,0.017241379310344827,"to bijective transformations. We then prove that with individual identification,
16"
ABSTRACT,0.018318965517241378,"the intractable disentanglement problem of latent confounders and post-treatment
17"
ABSTRACT,0.01939655172413793,"variables can be transformed into a tractable independence test problem. Finally,
18"
ABSTRACT,0.020474137931034482,"we prove that the true causal effects can be unbiasedly estimated with transformed
19"
ABSTRACT,0.021551724137931036,"confounders inferred by CiVAE. Experiments on both simulated and real-world
20"
ABSTRACT,0.022629310344827586,"datasets demonstrate significantly improved robustness of CiVAE.
21"
INTRODUCTION,0.023706896551724137,"1
Introduction
22"
INTRODUCTION,0.02478448275862069,"Causal inference, which aims to infer cause-and-effect relations from data, has gained increasing
23"
INTRODUCTION,0.02586206896551724,"prominence in various fields, such as social science, economics, and public health [10, 17, 34].
24"
INTRODUCTION,0.02693965517241379,"Traditional methods rely on the golden standard of randomized control trials (RCT) to draw valid
25"
INTRODUCTION,0.028017241379310345,"causal conclusions via experimentation [6]. Recently, more attention has been dedicated to causal
26"
INTRODUCTION,0.029094827586206896,"inference from observational data, where treatments, outcomes, and unit features are passively
27"
INTRODUCTION,0.03017241379310345,"observed, and researchers have no control over the treatment assignment mechanism [36, 37, 40].
28"
INTRODUCTION,0.03125,"One main obstacle to inferring valid causal relations from observational data is the confounding
29"
INTRODUCTION,0.032327586206896554,"bias, which occurs when we fail to account for the systematic difference between the treatment and
30"
INTRODUCTION,0.0334051724137931,"non-treatment group due to variables that causally influence the past treatments and the outcome, i.e.,
31"
INTRODUCTION,0.034482758620689655,"unobserved confounders [16]. If the confounders can be measured, a simple strategy to address the
32"
INTRODUCTION,0.03556034482758621,"bias is to control them via covariate adjustment [33] or propensity score re-weighting [24]. However,
33"
INTRODUCTION,0.036637931034482756,"confounders are not always measurable [23]. Therefore, recent methods seek to adjust for the
34"
INTRODUCTION,0.03771551724137931,"influence of unobserved confounders based on their proxies, which are easily acquirable covariates
35"
INTRODUCTION,0.03879310344827586,"postulated to be causally related with the unobserved confounders [29, 42, 28]. One exemplar work
36"
INTRODUCTION,0.03987068965517242,"(a) SCMAssumed by CEVAE
(c) SCMAssumed by the Proposed CiVAE T
Y C X T
Y C1 X"
INTRODUCTION,0.040948275862068964,"CKC
M1
MK"
INTRODUCTION,0.04202586206896552,"(b) SCMAssumed by TEDVAE T
Y C X I
A"
INTRODUCTION,0.04310344827586207,Instrumental
INTRODUCTION,0.04418103448275862,"Variables
Adjusters"
INTRODUCTION,0.04525862068965517,"Confounders
Post-treatment"
INTRODUCTION,0.04633620689655173,Variables
INTRODUCTION,0.04741379310344827,Confounders
INTRODUCTION,0.04849137931034483,"M
predictive
for only T"
INTRODUCTION,0.04956896551724138,"predictive
for only Y"
INTRODUCTION,0.05064655172413793,correlate with
INTRODUCTION,0.05172413793103448,both T and Y
INTRODUCTION,0.052801724137931036,correlate with
INTRODUCTION,0.05387931034482758,both T and Y
INTRODUCTION,0.05495689655172414,"Figure 1: Comparison between the causal models assumed by CEVAE, TEDVAE, and CiVAE."
INTRODUCTION,0.05603448275862069,"is the causal effect variational auto-encoder (CEVAE) [25], which has demonstrated that confounding
37"
INTRODUCTION,0.057112068965517244,"bias can be mitigated by controlling latent variables inferred from the proxies of confounders.
38"
INTRODUCTION,0.05818965517241379,"Although proxy-based methods have achieved substantial progress in recent years, they may risk
39"
INTRODUCTION,0.059267241379310345,"controlling latent post-treatment variables scrambled in the proxies, where latent post-treatment
40"
INTRODUCTION,0.0603448275862069,"bias can be introduced. Here, we note that the negative effects of controlling observed post-treatment
41"
INTRODUCTION,0.061422413793103446,"variables have been investigated in prior research [1, 9, 21]. For example, Montgomery et al. [30]
42"
INTRODUCTION,0.0625,"found that more than 50% of the papers published in top journals of politics inadvertently control
43"
INTRODUCTION,0.06357758620689655,"post-treatment variables in the experimental setting, even though researchers have complete control
44"
INTRODUCTION,0.06465517241379311,"over which covariates to control for. On this basis, we postulate that the post-treatment bias could
45"
INTRODUCTION,0.06573275862068965,"be even worse for proxy-based methods in the setting of observational study where variables are
46"
INTRODUCTION,0.0668103448275862,"passively recorded. In addition, the post-treatment variables can be latent and scrambled into the
47"
INTRODUCTION,0.06788793103448276,"observed covariates together with the latent confounders, which makes them difficult to disentangle.
48"
INTRODUCTION,0.06896551724137931,"Consider a real-world example from the Company1. We found that changing a job from onsite to
49"
INTRODUCTION,0.07004310344827586,"online mode causes applicants to make different decisions, and we want to estimate the causal effects
50"
INTRODUCTION,0.07112068965517242,"of switching a job from onsite to online mode to the decisions of the applicants (reflected by statistics
51"
INTRODUCTION,0.07219827586206896,"of applicants that apply for the job). In this case, the Company collected two groups of online (treated)
52"
INTRODUCTION,0.07327586206896551,"and onsite (control) jobs, where the statistics of the applicants (e.g., the average age) are calculated as
53"
INTRODUCTION,0.07435344827586207,"the surrogate outcome. Clearly, job seniority is a confounder, since less senior jobs are more likely to
54"
INTRODUCTION,0.07543103448275862,"permit online work, and applicants for these jobs tend to be younger. However, the seniority level of
55"
INTRODUCTION,0.07650862068965517,"a job can be difficult to measure. Therefore, the required skills of the job can be used as the proxy of
56"
INTRODUCTION,0.07758620689655173,"the confounder ""seniority"", as senior jobs tend to require more advanced skills. However, a caveat is
57"
INTRODUCTION,0.07866379310344827,"that switching to an online work mode may also alter the required skills of a job, thereby affecting the
58"
INTRODUCTION,0.07974137931034483,"qualification and, therefore, the decision of the applicants. Consequently, directly using the skills as
59"
INTRODUCTION,0.08081896551724138,"the proxy of the confounder ""seniority"" for adjustment could unintentionally control latent mediators
60"
INTRODUCTION,0.08189655172413793,"(changed skills), which introduces latent post-treatment bias in the causal effect estimation.
61"
INTRODUCTION,0.08297413793103449,"Addressing the latent post-treatment bias faces multi-faceted challenges. First, there lacks a
62"
INTRODUCTION,0.08405172413793104,"theoretical formulation of the bias when selected proxies scramble latent post-treatment variables
63"
INTRODUCTION,0.08512931034482758,"for existing proxy-based methods. In addition, it is difficult to distinguish confounders and post-
64"
INTRODUCTION,0.08620689655172414,"treatment variables in the latent space due to their similar observed behaviors. Existing covariate
65"
INTRODUCTION,0.08728448275862069,"disentanglement-based methods, e.g., TEDVAE [44], focus on an easier task of disentangling latent
66"
INTRODUCTION,0.08836206896551724,"confounders with latent adjusters and instrumental variables, which can be achieved by leveraging
67"
INTRODUCTION,0.0894396551724138,"their different predictive abilities w.r.t. the treatment and outcome. However, since both latent
68"
INTRODUCTION,0.09051724137931035,"confounders and post-treatment variables correlate with the treatment and the outcome, they cannot
69"
INTRODUCTION,0.09159482758620689,"be disentangled by these methods. Finally, even if latent confounders can be distinguished from post-
70"
INTRODUCTION,0.09267241379310345,"treatment variables, since most existing latent variable models have no identifiability guarantee [19],
71"
INTRODUCTION,0.09375,"it is unclear whether controlling the inferred latent variables, which may be arbitrary transformations
72"
INTRODUCTION,0.09482758620689655,"of the true confounders, can provide unbiased estimations of true causal effects.
73"
INTRODUCTION,0.09590517241379311,"To address the aforementioned challenges, we first analyze existing proxy-based methods when se-
74"
INTRODUCTION,0.09698275862068965,"lected proxies scramble both latent confounders and post-treatment variables and show the estimation
75"
INTRODUCTION,0.0980603448275862,"can be arbitrarily biased. We then propose a novel Confounder-identifiable VAE (CiVAE) to address
76"
INTRODUCTION,0.09913793103448276,"the latent post-treatment bias. Specifically, we prove that based on a mild assumption that the prior
77"
INTRODUCTION,0.10021551724137931,"of latent variables that generate the observed proxy (i.e., the latent confounders and post-treatment
78"
INTRODUCTION,0.10129310344827586,"variables) belong to a general exponential family with at least one invertible sufficient statistic in the
79"
INTRODUCTION,0.10237068965517242,"factorized part, latent confounders and latent post-treatment variables can be individually identified up
80"
INTRODUCTION,0.10344827586206896,"to simple bijective transformations. With such identifiability guarantee, based on the causal relations
81"
INTRODUCTION,0.10452586206896551,"among confounders, mediators, and treatment, we further demonstrate that the inferred confounders
82"
INTRODUCTION,0.10560344827586207,1Anonymized due to double-blind review policy.
INTRODUCTION,0.10668103448275862,"(which are actually transformed proxies of the true confounders) could be properly distinguished
83"
INTRODUCTION,0.10775862068965517,"from the latent post-treatment variables with pair-wise conditional independence tests. Finally, we
84"
INTRODUCTION,0.10883620689655173,"prove that the true causal effects can be unbiasedly estimated based on transformed confounders
85"
INTRODUCTION,0.10991379310344827,"inferred by CiVAE. Experiments on both simulated and real-world datasets demonstrate that CiVAE
86"
INTRODUCTION,0.11099137931034483,"shows more robustness to latent post-treatment bias than existing methods.
87"
PROBLEM FORMULATION,0.11206896551724138,"2
Problem Formulation
88"
PROBLEM FORMULATION,0.11314655172413793,"In this paper, we assume the causal model in Fig. 1-(c). We use a binary random variable T to
89"
PROBLEM FORMULATION,0.11422413793103449,"denote the treatment, a random vector X ∈RKX to denote the observed covariates (i.e., the proxy),
90"
PROBLEM FORMULATION,0.11530172413793104,"and a random scalar Y ∈R to denote the outcome. Furthermore, the observed covariates X are
91"
PROBLEM FORMULATION,0.11637931034482758,"assumed to be generated from KC independent latent confounders C ≜[C1, C2..., CKC] causally
92"
PROBLEM FORMULATION,0.11745689655172414,"influencing both T and Y , and KM latent post-treatment variables M ≜[M1, M2..., MKM ] under
93"
PROBLEM FORMULATION,0.11853448275862069,"the causal influence of the treatment (where the relation between M and Y can be arbitrary). We use
94"
PROBLEM FORMULATION,0.11961206896551724,"the random vector Z ≜[C||M] ∈RKZ=KC+KM to denote all latent factors. Our aim is to estimate
95"
PROBLEM FORMULATION,0.1206896551724138,"the average causal effects of treatment T on outcome Y with auxiliary confounder information in X,
96"
PROBLEM FORMULATION,0.12176724137931035,"where the estimation should be devoid of both confounding bias and post-treatment bias.
97"
THEORETICAL ANALYSIS OF LATENT POST-TREATMENT BIAS,0.12284482758620689,"3
Theoretical Analysis of Latent Post-Treatment Bias
98"
PRELIMINARIES AND ASSUMPTIONS,0.12392241379310345,"3.1
Preliminaries and Assumptions
99"
PRELIMINARIES AND ASSUMPTIONS,0.125,"To achieve such a purpose, we first define the (conditional) average treatment effects (C/ATE) when
100"
PRELIMINARIES AND ASSUMPTIONS,0.12607758620689655,"covariates X scramble both latent confounders C and post-treatment variables M. We then define
101"
PRELIMINARIES AND ASSUMPTIONS,0.1271551724137931,"the post-treatment bias when covariates X are directly used as the proxy of confounders. To facilitate
102"
PRELIMINARIES AND ASSUMPTIONS,0.12823275862068967,"the analysis, we make the following assumption regarding the causal generative process.
103"
PRELIMINARIES AND ASSUMPTIONS,0.12931034482758622,"Assumption 1. (Noisy-Injectivity). We assume X = f(C, M) + ϵ, where f is a deterministic
104"
PRELIMINARIES AND ASSUMPTIONS,0.13038793103448276,"function that combines latent confounders C and latent post-treatment variables M into observations
105"
PRELIMINARIES AND ASSUMPTIONS,0.1314655172413793,"X, and ϵ is random noise. In addition, we assume that the function f is injective; beyond injectivity,
106"
PRELIMINARIES AND ASSUMPTIONS,0.13254310344827586,"f can be arbitrarily nonlinear. We use f † : X →[C||M] to denote its left inverse. We use
107"
PRELIMINARIES AND ASSUMPTIONS,0.1336206896551724,"f †
C : X →C and f †
M : X →M to denote the mapping from X to C, M, respectively.
108"
PRELIMINARIES AND ASSUMPTIONS,0.13469827586206898,"Noisy-Injectivity is a common assumption made either explicitly or implicitly in most existing proxy-
109"
PRELIMINARIES AND ASSUMPTIONS,0.13577586206896552,"of-confounder-based causal inference algorithms. For example, if both X and C are categorical,
110"
PRELIMINARIES AND ASSUMPTIONS,0.13685344827586207,"[31] assumes that X has at least the same number of categories as C, whereas the effect restoration
111"
PRELIMINARIES AND ASSUMPTIONS,0.13793103448275862,"algorithm [35] assumes that the matrix of p(C, X) to be full-rank. Although CEVAE [25] makes no
112"
PRELIMINARIES AND ASSUMPTIONS,0.13900862068965517,"explicit injectivity assumption between C and X, it requires that the joint distribution p(C, X, T, Y )
113"
PRELIMINARIES AND ASSUMPTIONS,0.1400862068965517,"can be fully recovered from the observations (X, T, Y ). [2] show that some of the possible identifica-
114"
PRELIMINARIES AND ASSUMPTIONS,0.1411637931034483,"tion criteria for the recovery include 1) having multiple independent views of C in X [8], and 2) C
115"
PRELIMINARIES AND ASSUMPTIONS,0.14224137931034483,"is categorical and X is a mixture of Gaussian components determined by C (that is, X is generated
116"
PRELIMINARIES AND ASSUMPTIONS,0.14331896551724138,"by bijective mapping of C to the mean of the corresponding component with added Gaussian noise).
117"
PRELIMINARIES AND ASSUMPTIONS,0.14439655172413793,"In the following part of this section, we omit the noise ϵ to gain better intuition of latent post-treatment
118"
PRELIMINARIES AND ASSUMPTIONS,0.14547413793103448,"bias (but all the exact conclusions will still hold in the posterior sense [19]). In Section 4, we assume
119"
PRELIMINARIES AND ASSUMPTIONS,0.14655172413793102,"noise exists and demonstrate that our method can still properly identify the latent confounders.
120"
CAUSAL ESTIMAND AND THE TRUE ATE,0.1476293103448276,"3.2
Causal Estimand and the True ATE
121"
CAUSAL ESTIMAND AND THE TRUE ATE,0.14870689655172414,"Based on Assumption 1, we are ready to define the estimand of average treatment effect (ATE)
122"
CAUSAL ESTIMAND AND THE TRUE ATE,0.1497844827586207,"through controlling the covariates X′, as well the as the true (conditional) average treatment effects.
123"
CAUSAL ESTIMAND AND THE TRUE ATE,0.15086206896551724,"Definition 1. (DCEV & DEV). We define the Difference in Conditional Expected Values (DCEV) as:
124"
CAUSAL ESTIMAND AND THE TRUE ATE,0.15193965517241378,"DCEV (x′) = E[Y |T = 1, X′ = x′] −E[Y |T = 0, X′ = x′],
(1)"
CAUSAL ESTIMAND AND THE TRUE ATE,0.15301724137931033,"which is the difference of the expected value of Y for units with variable X′ = x′ in the treatment
125"
CAUSAL ESTIMAND AND THE TRUE ATE,0.1540948275862069,"group and the non-treatment group. Based on DCEV (x′), we define the Difference in Expected
126"
CAUSAL ESTIMAND AND THE TRUE ATE,0.15517241379310345,"Value (DEV) as DEV (X′) = Ep(X′)[DCEV (X′)] as the expectation of DCEV w.r.t. p(X′).
127"
CAUSAL ESTIMAND AND THE TRUE ATE,0.15625,"DEV (X′) denotes the estimand of ATE when X′ is the covariates that we choose to control (i.e.,
128"
CAUSAL ESTIMAND AND THE TRUE ATE,0.15732758620689655,"calculate the expected difference in each stratum of X′ = x′). If X′ = ∅, DEV (∅) represents
129"
CAUSAL ESTIMAND AND THE TRUE ATE,0.1584051724137931,"the naive estimator that directly calculates the expected difference of the outcome Y between the
130"
CAUSAL ESTIMAND AND THE TRUE ATE,0.15948275862068967,"treatment group and the non-treatment group. With the causal estimand DEV (X′) defined, we then
131"
CAUSAL ESTIMAND AND THE TRUE ATE,0.16056034482758622,"derive the true causal effects with the covariates X′ when it scrambles both latent confounders and
132"
CAUSAL ESTIMAND AND THE TRUE ATE,0.16163793103448276,"post-treatment variables according to the generative process described in Assumption 1:
133"
CAUSAL ESTIMAND AND THE TRUE ATE,0.1627155172413793,"Definition 2. Under Assumption 1, we define the Conditional Average Treatment Effect (CATE) for
134"
CAUSAL ESTIMAND AND THE TRUE ATE,0.16379310344827586,"individuals with observed covariates X = x by controlling only the confounder part in X as:
135"
CAUSAL ESTIMAND AND THE TRUE ATE,0.1648706896551724,"CATE(x) = E[Y |T = 1, C = f †
C(x)] −E[Y |T = 0, C = f †
C(x)],
(2)"
CAUSAL ESTIMAND AND THE TRUE ATE,0.16594827586206898,"with the Average Treatment Effect (ATE) of treatment T defined as:
136"
CAUSAL ESTIMAND AND THE TRUE ATE,0.16702586206896552,"ATE = E[Y |do(T = 1)] −E[Y |do(T = 0)] = Ep(C)[E[Y |T = 1, C] −E[Y |T = 0, C]].
(3)"
CAUSAL ESTIMAND AND THE TRUE ATE,0.16810344827586207,"Please note that we only consider the latent confounder component of the observed features X in the
137"
CAUSAL ESTIMAND AND THE TRUE ATE,0.16918103448275862,"definition of CATE in Eq. (2). This is because the causal relationship between the post-treatment
138"
CAUSAL ESTIMAND AND THE TRUE ATE,0.17025862068965517,"variables M and the outcome Y is indeterminate. However, if the specific relationship between M
139"
CAUSAL ESTIMAND AND THE TRUE ATE,0.1713362068965517,"and Y can be further established by the researcher (e.g., all elements of M are latent mediators),
140"
CAUSAL ESTIMAND AND THE TRUE ATE,0.1724137931034483,"more precise forms of CATE can be derived with path-specific counterfactual analysis [5, 14].
141"
LATENT POST-TREATMENT BIAS,0.17349137931034483,"3.3
Latent Post-Treatment Bias
142"
LATENT POST-TREATMENT BIAS,0.17456896551724138,"With DEV (X′) (the ATE estimator that control for the covariates X′), CATE, and ATE defined in
143"
LATENT POST-TREATMENT BIAS,0.17564655172413793,"Section 3.2, in this section, we analyze the latent post-treatment bias of existing proxy-of-confounder-
144"
LATENT POST-TREATMENT BIAS,0.17672413793103448,"based causal inference methods, such as CEVAE, that control for latent variables inferred from
145"
LATENT POST-TREATMENT BIAS,0.17780172413793102,"the covariates X to estimate the ATE of T on Y , when X scrambles both latent confounders and
146"
LATENT POST-TREATMENT BIAS,0.1788793103448276,"post-treatment variables as Assumption 1. In our analysis, Lemma 3.1 will be frequently used.
147"
LATENT POST-TREATMENT BIAS,0.17995689655172414,"Lemma 3.1. For an injective function g, E[Y |X′ = x′] = E[Y |g(X′) = g(x′)] holds.
148"
LATENT POST-TREATMENT BIAS,0.1810344827586207,"The proof when g is differentiable a.e. can be referred to in Appendix C.1. Since the latent variable
149"
LATENT POST-TREATMENT BIAS,0.18211206896551724,"models used in existing methods (such as VAE with factorized Gaussian prior in CEVAE) lack
150"
LATENT POST-TREATMENT BIAS,0.18318965517241378,"identifiability guarantee (i.e., the recovery of the exact latent variables), we assume that these models
151"
LATENT POST-TREATMENT BIAS,0.18426724137931033,"can recover the true latent space Z = [C, M] up to invertible transformations ¯f, where the inference
152"
LATENT POST-TREATMENT BIAS,0.1853448275862069,"process can be represented as ˆZ = ˜f(X) = ¯f ◦f †(X). With such an assumption, we have the
153"
LATENT POST-TREATMENT BIAS,0.18642241379310345,"following theorem regarding the latent post-treatment bias when X mixes post-treatment variables.
154"
LATENT POST-TREATMENT BIAS,0.1875,"Theorem 3.2. If the observed covariates X are generated from latent confounders C and latent
155"
LATENT POST-TREATMENT BIAS,0.18857758620689655,"post-treatment variables M according to Assumption 1, the latent post-treatment bias of a proxy-
156"
LATENT POST-TREATMENT BIAS,0.1896551724137931,"based causal inference algorithm that controls latent variables ˆZ inferred from X via ˜f = ¯f ◦f † :
157"
LATENT POST-TREATMENT BIAS,0.19073275862068967,"RKX →RKC+KM to estimate the ATE can be formulated as follows:
158"
LATENT POST-TREATMENT BIAS,0.19181034482758622,"Bias(X) = ATE −DEV ( ˜f(X)) = ATE −E[E[Y |T = 1, ˜f(X)] −E[Y |T = 0, ˜f(X)]]"
LATENT POST-TREATMENT BIAS,0.19288793103448276,"= ATE −E[E[Y |1, ¯f ◦f †(f(C, M))] −E[Y |0, ¯f ◦f †(f(C, M))]]
= E[E[Y |1, C] −E[Y |0, C]] −E[E[Y |1, C, M] −E[Y |0, C, M]], (4)"
LATENT POST-TREATMENT BIAS,0.1939655172413793,"which can be arbitrarily bad. Therefore, the estimator of existing proxy-of-confounder-based meth-
159"
LATENT POST-TREATMENT BIAS,0.19504310344827586,"ods, i.e., DEV ( ˜f(X)), is an arbitrarily biased estimator of the ATE, when the selected proxy of
160"
LATENT POST-TREATMENT BIAS,0.1961206896551724,"confounders X accidentally mixes in latent post-treatment variables M.
161"
LATENT POST-TREATMENT BIAS,0.19719827586206898,"The final step of Eq. (4) can be proved since f is injective and ¯f bijective, the composite ¯f ◦f † ◦f :
162"
LATENT POST-TREATMENT BIAS,0.19827586206896552,"[C, M] →ˆZ is bijective, so we can use Lemma 3.1 to remove ¯f ◦f † ◦f in the condition.
163"
EXAMPLES IN THE LINEAR CASE,0.19935344827586207,"3.4
Examples in the Linear Case
164"
EXAMPLES IN THE LINEAR CASE,0.20043103448275862,"Generally, the latent post-treatment bias defined in Eq. (4) cannot be simplified, because (i) the
165"
EXAMPLES IN THE LINEAR CASE,0.20150862068965517,"causal relationship between M and Y are indeterminate, and (ii) the causal influence of C, M,
166"
EXAMPLES IN THE LINEAR CASE,0.2025862068965517,"and T on Y can be arbitrary. However, for linear structural causal models with determined causal
167"
EXAMPLES IN THE LINEAR CASE,0.2036637931034483,"relationships between M and Y (e.g., M are mediators, which are post-treatment variables that have
168"
EXAMPLES IN THE LINEAR CASE,0.20474137931034483,"causal influences on the outcomes), stronger conclusions can be drawn as follows:
169"
EXAMPLES IN THE LINEAR CASE,0.20581896551724138,"Corollary 3.3. (Mixed Latent Mediator). For the linear Structural Causal Model (SCM) defined as:
170"
EXAMPLES IN THE LINEAR CASE,0.20689655172413793,"(i) T ←1(αT +
X
βi · Ci > a), (ii) Mj ←αM + γj · T"
EXAMPLES IN THE LINEAR CASE,0.20797413793103448,"(iii) X ←αX + A[M||C], (iv) Y ←αY + τ · T +
X
θj · Mj +
X
κi · Ci,
(5)"
EXAMPLES IN THE LINEAR CASE,0.20905172413793102,"where the mixture function f = A ∈RKX×(KC+KM) is a full column-rank matrix, the CATE, ATE,
171"
EXAMPLES IN THE LINEAR CASE,0.2101293103448276,"and the bias of proxy-of-confounder-based causal inference model that controls the latent variables
172"
EXAMPLES IN THE LINEAR CASE,0.21120689655172414,"ˆZ inferred via ˆZ = ˜f(X) = BT X can be formulated as follows:
173"
EXAMPLES IN THE LINEAR CASE,0.2122844827586207,"ATE = CATE = τ +
X
γj · θj, and DEV ( ˆZ) = E[DCEV ( ˆZ)] = DCEV ( ˆZ) = τ"
EXAMPLES IN THE LINEAR CASE,0.21336206896551724,"Bias( ˆZ) = ATE −DEV ( ˆZ) =
X
γj · θj,
(6)"
EXAMPLES IN THE LINEAR CASE,0.21443965517241378,"where B ∈RKX×(KC+KM) is another full column-rank matrix. Since P γj · θj is arbitrary, the
174"
EXAMPLES IN THE LINEAR CASE,0.21551724137931033,"estimator DEV ( ˆZ) = E[DCEV (BT X)] is arbitrarily biased for ATE estimation.
175"
EXAMPLES IN THE LINEAR CASE,0.2165948275862069,"The proof of Eq. (6) is provided in Appendix C.2. In addition, we show that post-treatment variables
176"
EXAMPLES IN THE LINEAR CASE,0.21767241379310345,"M DO NOT necessarily need to have direct causal effects on the outcome Y to incur arbitrary bias
177"
EXAMPLES IN THE LINEAR CASE,0.21875,"in ATE estimation. In Appendix C.3, we provide another example (i.e., Mixed Latent Correlator) in
178"
EXAMPLES IN THE LINEAR CASE,0.21982758620689655,"the linear case where M is correlated with Y through unobserved confounders U in Corollary C.1.
179"
METHODOLOGY,0.2209051724137931,"4
Methodology
180"
METHODOLOGY,0.22198275862068967,"In this section, we introduce the proposed Confounder-identifiable Variational Auto-Encoder (CiVAE)
181"
METHODOLOGY,0.22306034482758622,"in detail. Specifically, we first prove that if the prior distribution of the true latent variables Z =
182"
METHODOLOGY,0.22413793103448276,"[C, M] satisfies certain weak assumptions, CiVAE individually identify [C, M] up to bijective
183"
METHODOLOGY,0.2252155172413793,"transformations. Then, utilizing the causal relations between C, M, and T, we novelly transform the
184"
METHODOLOGY,0.22629310344827586,"challenging confounder-identifiability problem into a tractable pair-wise conditional independence
185"
METHODOLOGY,0.2273706896551724,"test problem, which can be effectively solved with kernel-based methods. The generalization of
186"
METHODOLOGY,0.22844827586206898,"CiVAE to address the interactions among [C, M] are discussed in Section D of the Appendix.
187"
GENERATIVE PROCESS,0.22952586206896552,"4.1
Generative Process
188"
GENERATIVE PROCESS,0.23060344827586207,"The fundamental work on the identifiability of deep variational inference, i.e., the identifiable VAE
189"
GENERATIVE PROCESS,0.23168103448275862,"(iVAE) [19], makes a strict assumption that the prior of true latent variables Z (i.e., [C, M] in
190"
GENERATIVE PROCESS,0.23275862068965517,"our case) is conditionally factorized given the available covariates. However, since both C and
191"
GENERATIVE PROCESS,0.2338362068965517,"M form fork structures with the outcome Y (see Fig. 1-(c)) [22], Ci, Cj, Mi, and Mj are not
192"
GENERATIVE PROCESS,0.2349137931034483,"independent given Y . Recently, Non-Factorized iVAE (NF-iVAE) [26] was proposed that allows
193"
GENERATIVE PROCESS,0.23599137931034483,"arbitrary dependence among the true latent variables Z in the conditional priors, where Z can be
194"
GENERATIVE PROCESS,0.23706896551724138,"identified up to arbitrary non-linear transformations. However, the transformation is not necessarily
195"
GENERATIVE PROCESS,0.23814655172413793,"invertible, which is risky as multiple values of the confounders may collapse, leading to bias when
196"
GENERATIVE PROCESS,0.23922413793103448,"estimating the ATE by averaging the DCEV calculated in each stratum of the inferred confounders.
197"
GENERATIVE PROCESS,0.24030172413793102,"In contrast to NF-iVAE, CiVAE guarantees the individual and bijective identifiability of Z by putting
198"
GENERATIVE PROCESS,0.2413793103448276,"a general exponential family with at least one invertible sufficient statistic in the factorized part as its
199"
GENERATIVE PROCESS,0.24245689655172414,"prior when conditioning on treatment T and outcome Y , which can be formulated as follows.
200"
GENERATIVE PROCESS,0.2435344827586207,"Assumption 2. Let Z = [C||M] be the random vector for latent variables that causally gen-
201"
GENERATIVE PROCESS,0.24461206896551724,"erate the observed covariates X according to Assumption 1. We assume that the conditional
202"
GENERATIVE PROCESS,0.24568965517241378,"prior of Z given the outcome Y and the treatment T belongs to a general exponential family
203"
GENERATIVE PROCESS,0.24676724137931033,"with parameter vector λ(Y, T) and sufficient statistics S(Z) = [Sf(Z)T , Snf(Z)T ]T . Specif-
204"
GENERATIVE PROCESS,0.2478448275862069,"ically, S(Z) is composed of (i) the sufficient statistics of a factorized exponential family, i.e.,
205"
GENERATIVE PROCESS,0.24892241379310345,"Sf(Z) = [S1(Z1)T , · · · , SKZ(ZKZ)T ]T , where all components Si(Zi) have dimension larger
206"
GENERATIVE PROCESS,0.25,"than or equal to 2 and each Si has at least one invertible dimension, and (ii) Snf(Z), where Snf is
207"
GENERATIVE PROCESS,0.2510775862068966,"a neural network with ReLU activation. The density of the conditional prior can be formulated as:
208"
GENERATIVE PROCESS,0.2521551724137931,"pS,λ(Z|Y, T) = Q(Z)/C(Y, T) exp[S(Z)T λ(Y, T)],
(7)"
GENERATIVE PROCESS,0.25323275862068967,"where Q(Z) is the base measure, and C(Y, T) is the normalizing constant independent of Z.
209"
GENERATIVE PROCESS,0.2543103448275862,"We justify that assumption 2 is weak and practical as follows. (i) Neural networks with ReLU
210"
GENERATIVE PROCESS,0.25538793103448276,"activation have universal approximation ability of distributions [27]. Therefore, Eq. (7) can model
211"
GENERATIVE PROCESS,0.25646551724137934,"arbitrary dependence between true latent confounders C and post-treatment variables M conditional
212"
GENERATIVE PROCESS,0.25754310344827586,"on T and Y . (ii) Although CiVAE makes an extra assumption that ∀i, at least one dimension of Si is
213"
GENERATIVE PROCESS,0.25862068965517243,"invertible, this can be easily satisfied as most commonly used exponential family distributions, such
214"
GENERATIVE PROCESS,0.25969827586206895,"as Gaussian, Bernoulli, etc., has at least one invertible sufficient statistics2.
215"
GENERATIVE PROCESS,0.2607758620689655,"The reason why we use ReLU as the activation is that, the identifiability of iVAE relies on the
216"
GENERATIVE PROCESS,0.26185344827586204,"condition that the sufficient statistics S have zero second-order cross-derivative. The factorized part,
217"
GENERATIVE PROCESS,0.2629310344827586,"i.e., Sf, satisfies it trivially as all cross-derivatives of Sf are zero. In addition, since the ReLU neural
218"
GENERATIVE PROCESS,0.2640086206896552,"networks are linear a.e., all second-order derivatives of Snf are zero. Therefore, identifiability holds
219"
GENERATIVE PROCESS,0.2650862068965517,"after adding Snf in the prior that allows the capturing of arbitrary dependence among Z.
220"
OPTIMIZATION OBJECTIVE,0.2661637931034483,"4.2
Optimization Objective
221"
OPTIMIZATION OBJECTIVE,0.2672413793103448,"Combining Assumptions 1 and 2, the generative process assumed by CiVAE can be formulated as:
222"
OPTIMIZATION OBJECTIVE,0.2683189655172414,"(i) pθ(X, Z | Y, T) = pf(X | Z), (ii) pS,λ(Z | Y, T), (iii) pf(X | Z) = pϵ(X −f(Z)). (8)"
OPTIMIZATION OBJECTIVE,0.26939655172413796,"where θ = (f, λ, S) ∈Θ are the parameters of the generative distribution. Since the generative
223"
OPTIMIZATION OBJECTIVE,0.2704741379310345,"process of CiVAE is parameterized by deep neural networks, the posterior distribution of Z, i.e.,
224"
OPTIMIZATION OBJECTIVE,0.27155172413793105,"pθ(Z | X, Y, T), is intractable. Therefore, we resort to variational inference [4], where we introduce
225"
OPTIMIZATION OBJECTIVE,0.27262931034482757,"an approximate posterior qϕ(Z | X, Y, T) parameterized by a deep neural network with a trainable
226"
OPTIMIZATION OBJECTIVE,0.27370689655172414,"parameter ϕ, and in qϕ(Z|·) finds the one closest to pθ(Z|·) measured by KL divergence. The
227"
OPTIMIZATION OBJECTIVE,0.27478448275862066,"minimization of KL is equivalent to maximization of the evidence lower bound (ELBO):
228"
OPTIMIZATION OBJECTIVE,0.27586206896551724,"L(θ, ϕ) := Eqϕ

log pf(X | Z) + log pS,λ(Z | Y, T) −log qϕ(Z | ·)
|
{z
}
KL of posterior with prior"
OPTIMIZATION OBJECTIVE,0.2769396551724138,"
.
(9)"
OPTIMIZATION OBJECTIVE,0.27801724137931033,"Since the normalization constant C in Eq. (7) is generally intractable, it is infeasible to directly learn
229"
OPTIMIZATION OBJECTIVE,0.2790948275862069,"S, λ by optimizing Eq. (9). Therefore, we substitute the KL term in Eq. (9) with the widely-used
230"
OPTIMIZATION OBJECTIVE,0.2801724137931034,"score matching [13] to learn unnormalized densities instead as follows:
231"
OPTIMIZATION OBJECTIVE,0.28125,"L(S, λ, ϕ) := Eqϕ(Z|·)
h
∥∇Z log qϕ(Z | ·) −∇Z log pS,λ(Z | Y, T)∥2i"
OPTIMIZATION OBJECTIVE,0.2823275862068966,"= Eqϕ(Z|·)   KZ
X j=1"
OPTIMIZATION OBJECTIVE,0.2834051724137931,"""
∂2pS,λ(Z | Y, T)"
OPTIMIZATION OBJECTIVE,0.28448275862068967,"∂Z2
j
+ 1 2"
OPTIMIZATION OBJECTIVE,0.2855603448275862,"∂pS,λ(Z | Y, T) ∂Zj 2#"
OPTIMIZATION OBJECTIVE,0.28663793103448276,"+ const,
(10)"
IDENTIFIABILITY OF CIVAE,0.28771551724137934,"4.3
Identifiability of CiVAE
232"
IDENTIFIABILITY OF CIVAE,0.28879310344827586,"With the generative process and optimization objective of CiVAE discussed in previous sub-sections,
233"
IDENTIFIABILITY OF CIVAE,0.28987068965517243,"we are ready to introduce the final assumption of CiVAE, which, combined with Assumptions 1 and
234"
IDENTIFIABILITY OF CIVAE,0.29094827586206895,"2, leads to the main Theorem of this paper, which states the identifiability of CiVAE.
235"
IDENTIFIABILITY OF CIVAE,0.2920258620689655,"Assumption 3. Assume the following: (i) The set {X ∈X|ϕ(X) = 0} has measure zero, where ϕ
236"
IDENTIFIABILITY OF CIVAE,0.29310344827586204,"is the characteristic function of the density pf in Eq. (8). (ii) The sufficient statistics, Si in Sf are all
237"
IDENTIFIABILITY OF CIVAE,0.2941810344827586,"twice differentiable. (iii) The mixture function f in Eq. (8) has all second-order cross derivatives.
238"
IDENTIFIABILITY OF CIVAE,0.2952586206896552,"(iv) There exist k + 1 distinct points (Y, T)0, · · · , (Y, T)k s.t. the matrix L = [λ((Y, T)1) −
239"
IDENTIFIABILITY OF CIVAE,0.2963362068965517,"λ((Y, T)0), · · · , λ((Y, T)k) −λ((Y, T)0)] of size k × k is invertible, where k = Dim(S).
240"
IDENTIFIABILITY OF CIVAE,0.2974137931034483,"Here, we note that Assumptions (i) - (iii) are trivial for differentiable neural networks. The Assumption
241"
IDENTIFIABILITY OF CIVAE,0.2984913793103448,"(iv) can be intuitively understood as independent samples of (Y, T) are required to identify C and
242"
IDENTIFIABILITY OF CIVAE,0.2995689655172414,"M. The identifiability theorem of CiVAE can be formulated as follows.
243"
IDENTIFIABILITY OF CIVAE,0.30064655172413796,"Theorem 4.1. If Assumptions 1, 2, and 3 hold, and if θ, ˜θ ∈Θ →pθ(X|Y, T) = p ˜θ(X|Y, T), the
244"
IDENTIFIABILITY OF CIVAE,0.3017241379310345,"true latent variables Z are identifiable up to permutation and element-wise bijective transformation.
245"
IDENTIFIABILITY OF CIVAE,0.30280172413793105,"Furthermore, in the case of variational inference, if we denote the true parameter that generates the
246"
IDENTIFIABILITY OF CIVAE,0.30387931034482757,"data as θ∗, if (i) the distribution family qϕ(Z|X, Y, T) contains the posterior pθ(Z|X, Y, T), and
247"
IDENTIFIABILITY OF CIVAE,0.30495689655172414,"qϕ(Z|X, Y, T) > 0, (ii) we optimize Eq. (4) w.r.t. both θ, ϕ, then in the limit of infinite data, true
248"
IDENTIFIABILITY OF CIVAE,0.30603448275862066,"parameters θ∗can be learned up to a permutation and bijective transformation of Z.
249"
IDENTIFIABILITY OF CIVAE,0.30711206896551724,"2There are a few exponential family dist. with no invertible sufficient statistics, e.g., Weibull with even shape
parameter k. However, these distributions are not commonly used in statistics or machine learning."
IDENTIFIABILITY OF CIVAE,0.3081896551724138,"The proof of Theorem 4.1 non trivially extends the NF-iVAE paper [26] by incorporating the new
250"
IDENTIFIABILITY OF CIVAE,0.30926724137931033,"assumption introduced in CiVAE (i.e., each Si has at least one invertible dimension) to ensure that the
251"
IDENTIFIABILITY OF CIVAE,0.3103448275862069,"transformation of each Zi is bijective. The detailed proof is provided in Appendix C.4 for reference.
252"
IDENTIFICATION OF LATENT CONFOUNDERS,0.3114224137931034,"4.4
Identification of Latent Confounders
253"
IDENTIFICATION OF LATENT CONFOUNDERS,0.3125,"Theorem 4.1 ensures that the latent variables ˆZ inferred by CiVAE cannot (i) mix confounders
254"
IDENTIFICATION OF LATENT CONFOUNDERS,0.3135775862068966,"and post-treatment variables in each dimension, or (ii) collapsing of different values of the latent
255"
IDENTIFICATION OF LATENT CONFOUNDERS,0.3146551724137931,"confounders into the same value. To further determine the dimensions of confounder and post-
256"
IDENTIFICATION OF LATENT CONFOUNDERS,0.31573275862068967,"treatment variable in ˆZ, we rely on the causal relations between latent variables ˆZ and the treatment
257"
IDENTIFICATION OF LATENT CONFOUNDERS,0.3168103448275862,"T and the associated marginal/conditional dependence properties, which are discussed as follows.
258"
IDENTIFICATION OF LATENT CONFOUNDERS,0.31788793103448276,"• Case 1. Intra-Confounders. Latent confounders Ci, Cj and the treatment T form the V structure
259"
IDENTIFICATION OF LATENT CONFOUNDERS,0.31896551724137934,"Ci →T ←Cj. Therefore, Ci and Cj are marginally independent, whereas they become
260"
IDENTIFICATION OF LATENT CONFOUNDERS,0.32004310344827586,"dependent when conditioning on the assigned treatment T.
261"
IDENTIFICATION OF LATENT CONFOUNDERS,0.32112068965517243,"• Case 2. Intra-Post Treatment Variables. Latent post-treatment variables Mi, Mj and the treatment
262"
IDENTIFICATION OF LATENT CONFOUNDERS,0.32219827586206895,"T form a Fork-structure Mi ←T →Mj, where Mi, Mj are marginally dependent, but they
263"
IDENTIFICATION OF LATENT CONFOUNDERS,0.3232758620689655,"become independent after conditioning on the assigned treatment T.
264"
IDENTIFICATION OF LATENT CONFOUNDERS,0.32435344827586204,"• Case 3. Cross-Confounder and Post-Treatment Variables. Latent confounder Ci, latent post-
265"
IDENTIFICATION OF LATENT CONFOUNDERS,0.3254310344827586,"treatment variable Mj, and the treatment T forms a Chain structure Ci →T →Mj, where Ci,
266"
IDENTIFICATION OF LATENT CONFOUNDERS,0.3265086206896552,"Mj are marginally dependent, and they become independent after conditioning on T.
267"
IDENTIFICATION OF LATENT CONFOUNDERS,0.3275862068965517,"From the above analysis we can find that, the dependence between two latent variables ˆZi and ˆZj
268"
IDENTIFICATION OF LATENT CONFOUNDERS,0.3286637931034483,"increases after conditioning on the treatment T ONLY in the case of intra-confounders. Therefore,
269"
IDENTIFICATION OF LATENT CONFOUNDERS,0.3297413793103448,"if more than one latent confounder exists, which is highly probable when covariates X are high-
270"
IDENTIFICATION OF LATENT CONFOUNDERS,0.3308189655172414,"dimensional, we can conduct independence test Ind( ˆZi, ˆZj) and CInd( ˆZi, ˆZj|T) for all pairs of
271"
IDENTIFICATION OF LATENT CONFOUNDERS,0.33189655172413796,"inferred latent variables, which can be implemented via kernel-based methods as [43], and select
272"
IDENTIFICATION OF LATENT CONFOUNDERS,0.3329741379310345,"the pairs where the p-value of CInd is larger than that of Ind as latent confounders. Here, we note
273"
IDENTIFICATION OF LATENT CONFOUNDERS,0.33405172413793105,"that the kernel-based (conditional) independence test incurs N 2 × K2
Z complexity in the training
274"
IDENTIFICATION OF LATENT CONFOUNDERS,0.33512931034482757,"phase. However, once the dimensions of the confounders in ˆZ are determined, CiVAE has the same
275"
IDENTIFICATION OF LATENT CONFOUNDERS,0.33620689655172414,"complexity as CEVAE for the estimation of CATE and ATE in the test phase.
276"
ATE ESTIMATOR WITH TRANSFORMED CONFOUNDERS,0.33728448275862066,"4.5
ATE Estimator with Transformed Confounders
277"
ATE ESTIMATOR WITH TRANSFORMED CONFOUNDERS,0.33836206896551724,"Finally, we demonstrate that controlling the transformed confounders ˆC inferred by CiVAE provides
278"
ATE ESTIMATOR WITH TRANSFORMED CONFOUNDERS,0.3394396551724138,"an unbiased estimation of ATE. Specifically, we have the final Theorem show the unbiasedness.
279"
ATE ESTIMATOR WITH TRANSFORMED CONFOUNDERS,0.34051724137931033,"Theorem 4.2. Controlling bijective of confounders is equivalent to original confounders in ATE
280"
ATE ESTIMATOR WITH TRANSFORMED CONFOUNDERS,0.3415948275862069,"estimation, i.e., DEV ( ˜C) = DEV (g(C)) = ATE, if the transformation function g is bijective.
281"
ATE ESTIMATOR WITH TRANSFORMED CONFOUNDERS,0.3426724137931034,"The proof of Theorem 4.2 for discrete C is trivial (where ˆC = g(C) represents a simple relabeling
282"
ATE ESTIMATOR WITH TRANSFORMED CONFOUNDERS,0.34375,"of the stratum that we calculate the DCEV and take the expectation). The proof in the continuous
283"
ATE ESTIMATOR WITH TRANSFORMED CONFOUNDERS,0.3448275862068966,"case where g is differentiable is provided in Appendix C.5. With Theorem 4.2, we can control the
284"
ATE ESTIMATOR WITH TRANSFORMED CONFOUNDERS,0.3459051724137931,"identified latent confounders as true confounders, providing an unbiased estimate of ATE.
285"
EMPIRICAL STUDY,0.34698275862068967,"5
Empirical Study
286"
EMPIRICAL STUDY,0.3480603448275862,"In this section, we provide and analyze the experiments we conduct on both simulated and real-world
287"
EMPIRICAL STUDY,0.34913793103448276,"datasets, where a code demo written in PyTorch and Pyro is provided in this anonymous URL.
288"
DATASETS,0.35021551724137934,"5.1
Datasets
289"
DATASETS,0.35129310344827586,"Simulated Datasets.
We first establish two simulated datasets, i.e., LatentMediator and
290"
DATASETS,0.35237068965517243,"LatentCorrelator, that consider two types of post-treatment variables, i.e., (i) mediators and
291"
DATASETS,0.35344827586206895,"(ii) correlators, i.e., variables that are correlated with the outcome Y via latent confounders U, where
292"
DATASETS,0.3545258620689655,"the causal generative process is under the full control of the experimenter. The generative process of
293"
DATASETS,0.35560344827586204,"the two datasets can be referred to in Corollary 3.3 and Corollary C.1 in the Appendix, respectively.
294"
DATASETS,0.3566810344827586,"In our experiments, C are generated from Gaussian distribution as C ∼Gaussian(0, IKC). For
295"
DATASETS,0.3577586206896552,"(a) Case 1: Intra-Confounder
(b) Case 2: Intra-Mediator
(c) Case 3: Confounder-Mediator
Figure 2: Visualization of p-value of independence test before and after conditioning on treatment T."
DATASETS,0.3588362068965517,"LatentMediator, γ is set as [−1, −1, −1], θ is set as [1, 1, 1], and τ is set as 2, which results in
296"
DATASETS,0.3599137931034483,"ATE = −1. For the LatentCorrelator dataset, we set the same γ and θ as the LatentMediator
297"
DATASETS,0.3609913793103448,"dataset, where parameters ϕ and τ are set to 1, which results in an overall ATE of 1.
298"
DATASETS,0.3620689655172414,"Real-world Datasets. In addition, we build real-world datasets from the Company to estimate the
299"
DATASETS,0.36314655172413796,"ATE of switching a job from onsite to online work mode to the statistics of the applicants. The
300"
DATASETS,0.3642241379310345,"average age and the variance of gender of the applicants are two outcomes of interest. Covariates
301"
DATASETS,0.36530172413793105,"X ∈{0, 1}KX include the required skills of the job. Specifically, we establish a cohort of 3,228
302"
DATASETS,0.36637931034482757,"jobs from the Bay Area in the US, where a preliminary study shows that DEV (∅) ≈2 years3 (i.e.,
303"
DATASETS,0.36745689655172414,"online job applicants are two years younger than onsite job applicants in the collected data), and
304"
DATASETS,0.36853448275862066,"DEV (∅) ≈−0.015 (i.e., online jobs exhibit 0.015 more gender variance than onsite jobs in the
305"
DATASETS,0.36961206896551724,"collected data). To simulate C and M, we first learn a generative model as follows:
306"
DATASETS,0.3706896551724138,"Z ∼Gaussian(0, IKZ), X ∼Multi(NNf(Z)), Y ∼Gaussian(w ⊙Z, 1),
(11)"
DATASETS,0.37176724137931033,"where Multi represents multinomial distribution, NNf is a neural network with softmax activation,
307"
DATASETS,0.3728448275862069,"Z, w ∈RKZ, KZ = 8, and ⊙represents the element-wise product operator, respectively. We
308"
DATASETS,0.3739224137931034,"then treat the first KC = 5 dimensions of Z as the latent confounders C and the remaining
309"
DATASETS,0.375,"KM = KZ −KC dimensions as the latent mediators M. After learning NNf and w according to
310"
DATASETS,0.3760775862068966,"Eq. (11), we draw latent confounders C ∈Gaussian(0, I), latent mediators M = T · γ, and set the
311"
DATASETS,0.3771551724137931,"outcome Y = w ⊙[C||M] + τ · T, where the true ATE can be calculated as sum(γ ⊙w−KM:) + τ.
312"
DISENTANGLE CONFOUNDERS AND POST-TREATMENT VARIABLES,0.37823275862068967,"5.1.1
Disentangle Confounders and Post-treatment Variables
313"
DISENTANGLE CONFOUNDERS AND POST-TREATMENT VARIABLES,0.3793103448275862,"We first show the p-value of the kernel-based pairwise independence test of the true latent variables
314"
DISENTANGLE CONFOUNDERS AND POST-TREATMENT VARIABLES,0.38038793103448276,"before and after conditioning on the assigned treatment T. From Fig. 2, we can find that the distinction
315"
DISENTANGLE CONFOUNDERS AND POST-TREATMENT VARIABLES,0.38146551724137934,"of the intra-confounder case from the other two cases discussed in Subsection 4.4 is significant. Here,
316"
DISENTANGLE CONFOUNDERS AND POST-TREATMENT VARIABLES,0.38254310344827586,"we should note this relies on the assumption that latent confounders are independent. If the latent
317"
DISENTANGLE CONFOUNDERS AND POST-TREATMENT VARIABLES,0.38362068965517243,"confounders are correlated, we can first use causal discovery techniques such as the PC algorithm [39]
318"
DISENTANGLE CONFOUNDERS AND POST-TREATMENT VARIABLES,0.38469827586206895,"to find direct parents of T, and use our algorithm as the refinement to determine the true confounders
319"
DISENTANGLE CONFOUNDERS AND POST-TREATMENT VARIABLES,0.3857758620689655,"C from the misidentified post-treatment variables (Experiments see Section D) in Appendix.
320"
BASELINES,0.38685344827586204,"5.2
Baselines
321"
BASELINES,0.3879310344827586,"The baselines we include for comparisons can be categorized into three classes. (i) Unawareness,
322"
BASELINES,0.3890086206896552,"where no information in X is used for ATE estimation. We implement the naive LR0 estimator, which
323"
BASELINES,0.3900862068965517,"regresses Y on T and uses the coefficient to estimate the ATE [15] (LR0 equals to DEV (∅), i.e., the
324"
BASELINES,0.3911637931034483,"difference of the average outcome between the treatment and non-treatment group). (ii) Control-X,
325"
BASELINES,0.3922413793103448,"which directly controls the covariates X. In this class, LR1 regresses Y on T and X, whereas TarNet
326"
BASELINES,0.3933189655172414,"uses a two-branch neural network to estimate the DEV (X) (iii) Control-Z, which controls latent
327"
BASELINES,0.39439655172413796,"variables Z learned from the covariates X. Methods from this class include the CEVAE [25] and
328"
BASELINES,0.3954741379310345,"covariate disentanglement methods, such as DR-CFR [12], TEDVAE [44], NICE [38], and AFS [41].
329"
RESULTS AND ANALYSIS,0.39655172413793105,"5.2.1
Results and Analysis
330"
RESULTS AND ANALYSIS,0.39762931034482757,"From Table 1, we can find that for all four datasets, CEVAE is worse than the naive LR0 estimator.
331"
RESULTS AND ANALYSIS,0.39870689655172414,"In addition, for the LatentMediator and Company (Age) dataset, all methods except CiVAE fail
332"
RESULTS AND ANALYSIS,0.39978448275862066,"to predict the negativity of the ATE. Covariates disentanglement-based methods, i.e., DR-CFR
333"
RESULTS AND ANALYSIS,0.40086206896551724,"and TEDVAE, inherit the latent post-treatment bias of CEVAE. The reason is that, these methods
334"
RESULTS AND ANALYSIS,0.4019396551724138,"disentangle latent confounders C from latent instrumental variables I and latent adjusters A by
335"
RESULTS AND ANALYSIS,0.40301724137931033,3which leads to 0.178 and -0.105 after standardization of the outcome.
RESULTS AND ANALYSIS,0.4040948275862069,Table 1: Comparison of CiVAE with baselines under latent post-treatment bias on various datasets.
RESULTS AND ANALYSIS,0.4051724137931034,"Dataset
LatentMediator
LatentCorrelator
Company (Age)
Company (Gender)
Method
ATE.
Err.
ATE.
Err.
ATE.
Err.
ATE.
Err."
RESULTS AND ANALYSIS,0.40625,"LR0
0.975 ± 0.032
1.975
2.977 ± 0.032
1.977
0.131 ± 0.015
0.399
-0.105 ± 0.009
-0.213
LR1
1.457 ± 0.167
2.457
3.400 ± 0.130
2.400
0.093 ± 0.029
0.361
-0.175 ± 0.014
-0.256
TarNet
1.461 ± 0.172
2.461
3.414 ± 0.146
2.414
0.112 ± 0.085
0.380
-0.167 ± 0.021
-0.248
CEVAE
1.550 ± 0.292
2.550
3.323 ± 0.167
2.323
0.106 ± 0.078
0.374
-0.180 ± 0.028
-0.261
DR-CFR
1.239 ± 0.324
2.239
3.185 ± 0.319
2.185
0.094 ± 0.089
0.362
-0.159 ± 0.030
-0.240
NICE
1.868 ± 0.530
2.868
1.942 ± 0.524
0.942
0.149 ± 0.126
0.417
-0.186 ± 0.041
-0.267
TEDVAE
1.042 ± 0.315
2.042
3.138 ± 0.281
2.138
0.097 ± 0.093
0.365
-0.143 ± 0.027
-0.224
AFS
1.496 ± 0.825
2.496
3.251 ± 0.398
2.251
0.105 ± 0.102
0.373
-0.163 ± 0.045
-0.244
CiVAE
-0.822 ± 0.753
0.178
1.199 ± 0.765
0.199
-0.140 ±0.137
0.128
-0.106 ± 0.064
-0.187
True ATE
-1.000 ± 0.000
0.000
1.000 ± 0.000
0.000
-0.268 ± 0.000
0.000
-0.081 ± 0.000
0.000"
RESULTS AND ANALYSIS,0.4073275862068966,"utilizing their causal relations with T and Y , i.e., I is predictive only for T, A is predictive only
336"
RESULTS AND ANALYSIS,0.4084051724137931,"for Y , whereas C is predictive for both T and Y . For example, TEDVAE includes three encoders
337"
RESULTS AND ANALYSIS,0.40948275862068967,"to infer three sets of latent variables ˆI, ˆ
A, ˆC from X and adds classification losses p(T| ˆI, ˆC)
338"
RESULTS AND ANALYSIS,0.4105603448275862,"and p(Y |T, ˆC, ˆ
A) on the CEVAE loss. However, since both latent confounders C and latent post-
339"
RESULTS AND ANALYSIS,0.41163793103448276,"treatment variables M are correlated with both T and Y , these methods cannot disentangle C from
340"
RESULTS AND ANALYSIS,0.41271551724137934,"M. An exception is NICE [38], which uses invariant risk minimization (IRM) [3] to find all causal
341"
RESULTS AND ANALYSIS,0.41379310344827586,"parents of the outcome Y as the confounders, which makes it more robust in the LatentCorrelator
342"
RESULTS AND ANALYSIS,0.41487068965517243,"case. However, since mediators M are also the causal parent of Y , the performance degrades
343"
RESULTS AND ANALYSIS,0.41594827586206895,"substantially on the LatentMediator dataset. Although AFS [41] considers the existence of post-
344"
RESULTS AND ANALYSIS,0.4170258620689655,"treatment variables M in the proxy X, it assumes that they can be separated from other variables in
345"
RESULTS AND ANALYSIS,0.41810344827586204,"X in the observational space, and no relationship exists between the post-treatment variables and the
346"
RESULTS AND ANALYSIS,0.4191810344827586,"outcome, so it still has poor performance in our setting since both assumptions are violated.
347"
SENSITIVITY ANALYSIS,0.4202586206896552,"5.3
Sensitivity Analysis
348"
SENSITIVITY ANALYSIS,0.4213362068965517,"2:6
3:5
4:4
5:3
6:2
Ratio 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Error"
SENSITIVITY ANALYSIS,0.4224137931034483,(a) Company (Age)
SENSITIVITY ANALYSIS,0.4234913793103448,"CiVAE
TEDVAE"
SENSITIVITY ANALYSIS,0.4245689655172414,"2:6
3:5
4:4
5:3
6:2
Ratio 0.0 0.1 0.2 0.3 0.4 0.5 0.6 Error"
SENSITIVITY ANALYSIS,0.42564655172413796,(b) Company (Gender)
SENSITIVITY ANALYSIS,0.4267241379310345,"CiVAE
TEDVAE"
SENSITIVITY ANALYSIS,0.42780172413793105,"Figure 3: Error with different ratio of latent confounders and
latent post-treatment variable in the latent space."
SENSITIVITY ANALYSIS,0.42887931034482757,"In this part, we vary the number
349"
SENSITIVITY ANALYSIS,0.42995689655172414,"of confounders and post-treatment
350"
SENSITIVITY ANALYSIS,0.43103448275862066,"variables that generate proxy X in
351"
SENSITIVITY ANALYSIS,0.43211206896551724,"the Company (Age) and Company
352"
SENSITIVITY ANALYSIS,0.4331896551724138,"(Gender) datasets and compare
353"
SENSITIVITY ANALYSIS,0.43426724137931033,"CiVAE with the baseline TEDVAE
354"
SENSITIVITY ANALYSIS,0.4353448275862069,"in Fig. 3. Fig. 3 shows that the er-
355"
SENSITIVITY ANALYSIS,0.4364224137931034,"ror is consistently lower for CiVAE.
356"
SENSITIVITY ANALYSIS,0.4375,"In addition, the error is compara-
357"
SENSITIVITY ANALYSIS,0.4385775862068966,"tively higher when the number of con-
358"
SENSITIVITY ANALYSIS,0.4396551724137931,"founders is low since the misidenti-
359"
SENSITIVITY ANALYSIS,0.44073275862068967,"fication of latent post-treatment vari-
360"
SENSITIVITY ANALYSIS,0.4418103448275862,"ables as confounders can have a com-
361"
SENSITIVITY ANALYSIS,0.44288793103448276,"paratively larger influence on the ATE estimation. In addition, when the number of confounders
362"
SENSITIVITY ANALYSIS,0.44396551724137934,"becomes larger, the performance gap between CiVAE and TEDVAE gracefully shrinks.
363"
CONCLUSIONS,0.44504310344827586,"6
Conclusions
364"
CONCLUSIONS,0.44612068965517243,"In this paper, we systematically investigate the latent post-treatment bias in causal inference from
365"
CONCLUSIONS,0.44719827586206895,"observational data. We first prove that unresolved latent post-treatment variables scrambled in the
366"
CONCLUSIONS,0.4482758620689655,"proxy of confounders can arbitrarily bias the ATE estimation. To address the bias, we proposed
367"
CONCLUSIONS,0.44935344827586204,"the Confounder-identifiable VAE (CiVAE), which, utilizing a mild assumption regarding the prior
368"
CONCLUSIONS,0.4504310344827586,"of latent factors, guarantees the identifiability of latent confounders up to bijective transformations.
369"
CONCLUSIONS,0.4515086206896552,"Finally, we show that controlling the latent confounders inferred by CiVAE can provide an unbiased
370"
CONCLUSIONS,0.4525862068965517,"estimation of the ATE. Experiments on both simulated and real-world datasets demonstrate that
371"
CONCLUSIONS,0.4536637931034483,"CiVAE has superior robustness to latent post-treatment bias compared to state-of-the-art methods.
372"
REFERENCES,0.4547413793103448,"References
373"
REFERENCES,0.4558189655172414,"[1] A. Acharya, M. Blackwell, and M. Sen. Explaining causal findings without bias: Detecting and
374"
REFERENCES,0.45689655172413796,"assessing direct effects. American Political Science Review, 110(3):512–529, 2016.
375"
REFERENCES,0.4579741379310345,"[2] A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and M. Telgarsky. Tensor decompositions for
376"
REFERENCES,0.45905172413793105,"learning latent variable models. Journal of Machine Learning Research, 15:2773–2832, 2014.
377"
REFERENCES,0.46012931034482757,"[3] M. Arjovsky, L. Bottou, I. Gulrajani, and D. Lopez-Paz. Invariant risk minimization. arXiv
378"
REFERENCES,0.46120689655172414,"preprint arXiv:1907.02893, 2019.
379"
REFERENCES,0.46228448275862066,"[4] D. M. Blei, A. Kucukelbir, and J. D. McAuliffe. Variational inference: A review for statisticians.
380"
REFERENCES,0.46336206896551724,"Journal of the American Statistical Association, 112(518):859–877, 2017.
381"
REFERENCES,0.4644396551724138,"[5] L. Cheng, R. Guo, and H. Liu. Causal mediation analysis with hidden confounders. In
382"
REFERENCES,0.46551724137931033,"Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining,
383"
REFERENCES,0.4665948275862069,"pages 113–122, 2022.
384"
REFERENCES,0.4676724137931034,"[6] T. D. Cook, D. T. Campbell, and W. Shadish. Experimental and quasi-experimental designs for
385"
REFERENCES,0.46875,"generalized causal inference. Houghton Mifflin Boston, MA, 2002.
386"
REFERENCES,0.4698275862068966,"[7] P. Ding and L. W. Miratrix. To adjust or not to adjust? sensitivity analysis of m-bias and
387"
REFERENCES,0.4709051724137931,"butterfly-bias. Journal of Causal Inference, 3(1):41–57, 2015.
388"
REFERENCES,0.47198275862068967,"[8] J. K. Edwards, S. R. Cole, and D. Westreich. All your data are always missing: incorporating
389"
REFERENCES,0.4730603448275862,"bias due to measurement error into the potential outcomes framework. International Journal of
390"
REFERENCES,0.47413793103448276,"Epidemiology, 44(4):1452–1459, 2015.
391"
REFERENCES,0.47521551724137934,"[9] F. Elwert and C. Winship. Endogenous selection bias: The problem of conditioning on a collider
392"
REFERENCES,0.47629310344827586,"variable. Annual review of sociology, 40:31–53, 2014.
393"
REFERENCES,0.47737068965517243,"[10] T. A. Glass, S. N. Goodman, M. A. Hernán, and J. M. Samet. Causal inference in public health.
394"
REFERENCES,0.47844827586206895,"Annual Review of Public Health, 34:61–75, 2013.
395"
REFERENCES,0.4795258620689655,"[11] N. Hassanpour and R. Greiner. Counterfactual regression with importance sampling weights.
396"
REFERENCES,0.48060344827586204,"In IJCAI, pages 5880–5887, 2019.
397"
REFERENCES,0.4816810344827586,"[12] N. Hassanpour and R. Greiner.
Learning disentangled representations for counterfactual
398"
REFERENCES,0.4827586206896552,"regression. In International Conference on Learning Representations, 2020.
399"
REFERENCES,0.4838362068965517,"[13] A. Hyvärinen and P. Dayan. Estimation of non-normalized statistical models by score matching.
400"
REFERENCES,0.4849137931034483,"Journal of Machine Learning Research, 6(4), 2005.
401"
REFERENCES,0.4859913793103448,"[14] K. Imai, L. Keele, and D. Tingley. A general approach to causal mediation analysis. Psycholog-
402"
REFERENCES,0.4870689655172414,"ical Methods, 15(4):309, 2010.
403"
REFERENCES,0.48814655172413796,"[15] G. W. Imbens and D. B. Rubin. Causal inference in statistics, social, and biomedical sciences.
404"
REFERENCES,0.4892241379310345,"Cambridge University Press, 2015.
405"
REFERENCES,0.49030172413793105,"[16] K. Jager, C. Zoccali, A. Macleod, and F. Dekker. Confounding: what it is and how to deal with
406"
REFERENCES,0.49137931034482757,"it. Kidney international, 73(3):256–260, 2008.
407"
REFERENCES,0.49245689655172414,"[17] F. Johansson, U. Shalit, and D. Sontag. Learning representations for counterfactual inference.
408"
REFERENCES,0.49353448275862066,"In International Conference on Machine Learning, pages 3020–3029, 2016.
409"
REFERENCES,0.49461206896551724,"[18] M. Kalisch and P. Bühlman. Estimating high-dimensional directed acyclic graphs with the
410"
REFERENCES,0.4956896551724138,"pc-algorithm. Journal of Machine Learning Research, 8(3), 2007.
411"
REFERENCES,0.49676724137931033,"[19] I. Khemakhem, D. Kingma, R. Monti, and A. Hyvarinen. Variational autoencoders and nonlinear
412"
REFERENCES,0.4978448275862069,"ICA: A unifying framework. In International Conference on Artificial Intelligence and Statistics,
413"
REFERENCES,0.4989224137931034,"pages 2207–2217. PMLR, 2020.
414"
REFERENCES,0.5,"[20] G. King. A hard unsolved problem? post-treatment bias in big social science questions. In
415"
REFERENCES,0.5010775862068966,"Hard Problems in Social Science” Symposium, April, volume 10, 2010.
416"
REFERENCES,0.5021551724137931,"[21] G. King and L. Zeng. The dangers of extreme counterfactuals. Political Analysis, 14(2):131–159,
417"
REFERENCES,0.5032327586206896,"2006.
418"
REFERENCES,0.5043103448275862,"[22] D. Koller and N. Friedman. Probabilistic graphical models: principles and techniques. MIT
419"
REFERENCES,0.5053879310344828,"press, 2009.
420"
REFERENCES,0.5064655172413793,"[23] M. Kuroki and J. Pearl. Measurement bias and effect restoration in causal inference. Biometrika,
421"
REFERENCES,0.5075431034482759,"101(2):423–437, 2014.
422"
REFERENCES,0.5086206896551724,"[24] F. Li, K. L. Morgan, and A. M. Zaslavsky. Balancing covariates via propensity score weighting.
423"
REFERENCES,0.509698275862069,"Journal of the American Statistical Association, 113(521):390–400, 2018.
424"
REFERENCES,0.5107758620689655,"[25] C. Louizos, U. Shalit, J. M. Mooij, D. Sontag, R. Zemel, and M. Welling. Causal effect inference
425"
REFERENCES,0.5118534482758621,"with deep latent-variable models. Advances in Neural Information Processing Systems, 30,
426"
REFERENCES,0.5129310344827587,"2017.
427"
REFERENCES,0.5140086206896551,"[26] C. Lu, Y. Wu, J. M. Hernández-Lobato, and B. Schölkopf. Invariant causal representation
428"
REFERENCES,0.5150862068965517,"learning for out-of-distribution generalization. In International Conference on Learning Repre-
429"
REFERENCES,0.5161637931034483,"sentations, 2021.
430"
REFERENCES,0.5172413793103449,"[27] Y. Lu and J. Lu. A universal approximation theorem of deep neural networks for expressing
431"
REFERENCES,0.5183189655172413,"probability distributions. In Advances in Neural Information Processing Systems, pages 3094–
432"
REFERENCES,0.5193965517241379,"3105, 2020.
433"
REFERENCES,0.5204741379310345,"[28] D. Madras, E. Creager, T. Pitassi, and R. Zemel. Fairness through causal awareness: Learning
434"
REFERENCES,0.521551724137931,"causal latent-variable models for biased data. In Proceedings of the Conference on Fairness,
435"
REFERENCES,0.5226293103448276,"Accountability, and Transparency, pages 349–358, 2019.
436"
REFERENCES,0.5237068965517241,"[29] W. Miao, Z. Geng, and E. J. Tchetgen Tchetgen. Identifying causal effects with proxy variables
437"
REFERENCES,0.5247844827586207,"of an unmeasured confounder. Biometrika, 105(4):987–993, 2018.
438"
REFERENCES,0.5258620689655172,"[30] J. M. Montgomery, B. Nyhan, and M. Torres. How conditioning on posttreatment variables
439"
REFERENCES,0.5269396551724138,"can ruin your experiment and what to do about it. American Journal of Political Science,
440"
REFERENCES,0.5280172413793104,"62(3):760–775, 2018.
441"
REFERENCES,0.5290948275862069,"[31] J. Pearl. On measurement bias in causal inference. arXiv preprint arXiv:1203.3504, 2012.
442"
REFERENCES,0.5301724137931034,"[32] J. Pearl. Conditioning on post-treatment variables. Journal of Causal Inference, 3(1):131–137,
443"
REFERENCES,0.53125,"2015.
444"
REFERENCES,0.5323275862068966,"[33] S. J. Pocock, S. E. Assmann, L. E. Enos, and L. E. Kasten. Subgroup analysis, covariate
445"
REFERENCES,0.5334051724137931,"adjustment and baseline comparisons in clinical trial reporting: current practiceand problems.
446"
REFERENCES,0.5344827586206896,"Statistics in Medicine, 21(19):2917–2930, 2002.
447"
REFERENCES,0.5355603448275862,"[34] M. Prosperi, Y. Guo, M. Sperrin, J. S. Koopman, J. S. Min, X. He, S. Rich, M. Wang, I. E.
448"
REFERENCES,0.5366379310344828,"Buchan, and J. Bian. Causal inference and counterfactual prediction in machine learning for
449"
REFERENCES,0.5377155172413793,"actionable healthcare. Nature Machine Intelligence, 2(7):369–375, 2020.
450"
REFERENCES,0.5387931034482759,"[35] K. J. Rothman, S. Greenland, T. L. Lash, et al. Modern epidemiology, volume 3. Wolters
451"
REFERENCES,0.5398706896551724,"Kluwer Health/Lippincott Williams & Wilkins Philadelphia, 2008.
452"
REFERENCES,0.540948275862069,"[36] U. Shalit, F. D. Johansson, and D. Sontag. Estimating individual treatment effect: generalization
453"
REFERENCES,0.5420258620689655,"bounds and algorithms. In International Conference on Machine Learning, pages 3076–3085,
454"
REFERENCES,0.5431034482758621,"2017.
455"
REFERENCES,0.5441810344827587,"[37] C. Shi, D. Blei, and V. Veitch. Adapting neural networks for the estimation of treatment effects.
456"
REFERENCES,0.5452586206896551,"In Advances in Neural Information Processing Systems, 2019.
457"
REFERENCES,0.5463362068965517,"[38] C. Shi, V. Veitch, and D. M. Blei. Invariant representation learning for treatment effect estimation.
458"
REFERENCES,0.5474137931034483,"In Uncertainty in Artificial Intelligence, pages 1546–1555. PMLR, 2021.
459"
REFERENCES,0.5484913793103449,"[39] P. Spirtes, C. N. Glymour, R. Scheines, and D. Heckerman. Causation, prediction, and search.
460"
REFERENCES,0.5495689655172413,"MIT press, 2000.
461"
REFERENCES,0.5506465517241379,"[40] S. Wager and S. Athey. Estimation and inference of heterogeneous treatment effects using
462"
REFERENCES,0.5517241379310345,"random forests. Journal of the American Statistical Association, 113(523):1228–1242, 2018.
463"
REFERENCES,0.552801724137931,"[41] H. Wang, K. Kuang, H. Chi, L. Yang, M. Geng, W. Huang, and W. Yang. Treatment effect
464"
REFERENCES,0.5538793103448276,"estimation with adjustment feature selection.
In Proceedings of the 29th ACM SIGKDD
465"
REFERENCES,0.5549568965517241,"Conference on Knowledge Discovery and Data Mining, pages 2290–2301, 2023.
466"
REFERENCES,0.5560344827586207,"[42] L. Yao, S. Li, Y. Li, M. Huai, J. Gao, and A. Zhang. Representation learning for treatment effect
467"
REFERENCES,0.5571120689655172,"estimation from observational data. In Advances in Neural Information Processing Systems,
468"
REFERENCES,0.5581896551724138,"volume 31, 2018.
469"
REFERENCES,0.5592672413793104,"[43] K. Zhang, J. Peters, D. Janzing, and B. Schölkopf. Kernel-based conditional independence test
470"
REFERENCES,0.5603448275862069,"and application in causal discovery. arXiv preprint arXiv:1202.3775, 2012.
471"
REFERENCES,0.5614224137931034,"[44] W. Zhang, L. Liu, and J. Li. Treatment effect estimation with disentangled latent factors. In
472"
REFERENCES,0.5625,"Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 10923–10930,
473"
REFERENCES,0.5635775862068966,"2021.
474"
REFERENCES,0.5646551724137931,"Appendix
475"
REFERENCES,0.5657327586206896,"A
Broader Impact
476"
REFERENCES,0.5668103448275862,"The proposed CiVAE is a universal model for causal effect estimation with observational data.
477"
REFERENCES,0.5678879310344828,"Although we use the Company job data that estimate the causal effects of online working mode to
478"
REFERENCES,0.5689655172413793,"applicant statistics as a real-world example, proxy-of-confounder-based methods have been heavily
479"
REFERENCES,0.5700431034482759,"used in other observational studies, which may be susceptible to latent post-treatment bias. Therefore,
480"
REFERENCES,0.5711206896551724,"we speculate that the proposed CiVAE will have a broader impact on causal inference community.
481"
REFERENCES,0.572198275862069,"B
Related Work
482"
REFERENCES,0.5732758620689655,"B.1
Post-Treatment Bias in Causal Inference
483"
REFERENCES,0.5743534482758621,"Bias due to accidentally controlling post-treatment variables, i.e., post-treatment bias, has long been
484"
REFERENCES,0.5754310344827587,"recognized as dangerous in causal effect estimation [20]. Back at 2005, Pearl [32] cautioned that
485"
REFERENCES,0.5765086206896551,"controlling more is not better, and uses the collider bias [9] and M-Bias [7] as two examples to
486"
REFERENCES,0.5775862068965517,"show that bias can be increased when controlling the post-treatment variables. Furthermore, [30]
487"
REFERENCES,0.5786637931034483,"show that indirect correlations between post-treatment variable M and outcome Y can still cause
488"
REFERENCES,0.5797413793103449,"bias. Recent works prove that even if M has no causal relationship with Y , controlling it can still
489"
REFERENCES,0.5808189655172413,"increase the variance of estimand [12]. However, most of these works study the post-treatment bias
490"
REFERENCES,0.5818965517241379,"in the observational space, where latent post-treatment variables that are mixed with confounders to
491"
REFERENCES,0.5829741379310345,"generate the observed covariates can be easily ignored by the researcher. Therefore, it motivates us to
492"
REFERENCES,0.584051724137931,"develop CiVAE, which is robust to the latent post-treatment bias under mild assumptions.
493"
REFERENCES,0.5851293103448276,"B.2
Covariate Disentanglement
494"
REFERENCES,0.5862068965517241,"Recently, researchers have realized that directly controlling proxy of confounders X may not be
495"
REFERENCES,0.5872844827586207,"safe, as variables other than confounders could lurk in the proxy and ruin the ATE estimation [12].
496"
REFERENCES,0.5883620689655172,"Traditional methods assume that the variables that generate X are a mixture of confounders, adjusters,
497"
REFERENCES,0.5894396551724138,"and influencers [36], where adjusters should not be controlled as it can increase the estimation
498"
REFERENCES,0.5905172413793104,"variance [11]. Most methods rely on the fact that adjusters are correlated only with the treatment
499"
REFERENCES,0.5915948275862069,"to separate them from other variables [12, 44] (see Fig. (1)). This can also be used to remove post-
500"
REFERENCES,0.5926724137931034,"treatment variables that are not correlated with the outcome, which have similar statistics properties
501"
REFERENCES,0.59375,"with adjustors [41]. Here, a different work is NICE [38], which uses the fact that confounders and
502"
REFERENCES,0.5948275862068966,"influencers are direct causal parents of the outcome to find these variables with invariant learning as
503"
REFERENCES,0.5959051724137931,"the control set [3]. However, since mediators are also direct parents of the outcome, NICE is still not
504"
REFERENCES,0.5969827586206896,"robust to general post-treatment bias. Given that all above methods cannot satisfactorily address the
505"
REFERENCES,0.5980603448275862,"latent post-treatment in general cases, it is imperative to design the CiVAE, where confounders can
506"
REFERENCES,0.5991379310344828,"be identified and distinguished with latent post-treatment variables for unbiased adjustment.
507"
REFERENCES,0.6002155172413793,"C
Theoretical Analysis
508"
REFERENCES,0.6012931034482759,"C.1
Proof of Lemma 3.1.
509"
REFERENCES,0.6023706896551724,"Proof. Let Z = f(X) and z = f(x). If f is injective and differentiable a.e., and f † is the
510"
REFERENCES,0.603448275862069,"left-inverse, we have:
511"
REFERENCES,0.6045258620689655,"fY |f(X)(y|f(x)) = fY |Z(y|z) = fY,Z(y, z)"
REFERENCES,0.6056034482758621,"fZ(z)
= fY,X(y, f †(z))|Jf †(z)|"
REFERENCES,0.6066810344827587,"fX(f †(z))|Jf †(z)|
= fY,X(y, x)"
REFERENCES,0.6077586206896551,"fX(x)
= fY |X(y|x),"
REFERENCES,0.6088362068965517,"(12)
where f· and f·|· represent the marginal and conditional density function, respectively, and Jf †(z) is
512"
REFERENCES,0.6099137931034483,"the Jacobian matrix of function f † evaluated at z. Based on Eq. (12), we have:
513"
REFERENCES,0.6109913793103449,"E[Y |X] =
Z
y·fY |X(y|x)dy =
Z
y·fY |Z(y|z)dy = E[Y |Z = z] = E[Y |f(X) = f(x)]. (13) 514"
REFERENCES,0.6120689655172413,"C.2
Proof of Corollary 3.3.
515"
REFERENCES,0.6131465517241379,"Proof. For X = x, let [c||m] .= [f †
C(x)||f †
M(x)] .= f †(x) = A†(x −αX), where A† is the left
516"
REFERENCES,0.6142241379310345,"inverse of the full column-rank matrix A in Eq. (2), we have:
517"
REFERENCES,0.615301724137931,"CATE(x) = E[Y |T = 1, C = f †
C(x)] −E[Y |T = 0, C = f †
C(x)]
= E[Y |T = 1, C = c] −E[Y |T = 0, C = c]"
REFERENCES,0.6163793103448276,"= E[αY + τ · T +
X
θj · Mj +
X
κi · Ci|T = 1, C = c]"
REFERENCES,0.6174568965517241,"−E[αY + τ · T +
X
θj · Mj +
X
κi · Ci|T = 0, C = c]"
REFERENCES,0.6185344827586207,"= αY + τ · E[T|T = 1, C = c] +
X
θj · E[Mj|T = 1, C = c] +
X
κi · E[Ci|T = 1, C = c]"
REFERENCES,0.6196120689655172,"−αY + τ · E[T|T = 0, C = c] +
X
θj · E[Mj|T = 0, C = c] +
X
κi · E[Ci|T = 0, C = c]"
REFERENCES,0.6206896551724138,"= τ · (1 −0) +
X
θj · (γj · (1 −0)) +
X
κi · (ci −ci)"
REFERENCES,0.6217672413793104,"= τ +
X
θj · γj = E[τ +
X
θj · γj] = ATE,
(14)
where the first equality is due to the definition of CATE in Eq. (2). In addition, the causal estimand
518"
REFERENCES,0.6228448275862069,"and bias of a proxy-of-confounder-based causal inference model that controls the latent variable Z
519"
REFERENCES,0.6239224137931034,"inferred via Z = ¯f(X) = BT X (where B is also a full column-rank matrix) can be formulated as:
520"
REFERENCES,0.625,"DCEV (BT x) = E[Y |T = 1, Z = BT x] −E[Y |T = 0, Z = BT x]"
REFERENCES,0.6260775862068966,"= E[Y |T = 1, Z = BT αX + BT A[c||m]] −E[Y |T = 0, Z = BT αX + BT A[c||m]]"
REFERENCES,0.6271551724137931,"(a)
= E[Y |T = 1, C = c, M = m] −E[Y |T = 0, C = c, M = m]"
REFERENCES,0.6282327586206896,"= αY + τ · 1 +
X
θj · E[Mj|T = 1, C = c, M = m] +
X
κi · E[Ci|T = 1, C = c, M = m]"
REFERENCES,0.6293103448275862,"−αY + τ · 0 +
X
θj · E[Mj|T = 0, C = c, M = m] +
X
κi · E[Ci|T = 0, C = c, M = m]"
REFERENCES,0.6303879310344828,"= τ · (1 −0) +
X
θj · (mj −mj) +
X
κi · (ci −ci)"
REFERENCES,0.6314655172413793,"= τ = E[τ] = E[DCEV (BT X)],
(15)"
REFERENCES,0.6325431034482759,"where step (a) is due to the fact that, since both A and B are full column-rank matrices, BT A is
521"
REFERENCES,0.6336206896551724,"an invertible matrix, and the mapping f = BT αX + BT A is bijective. Therefore, we can invoke
522"
REFERENCES,0.634698275862069,"Lemma 3.1 and apply the left-inverse of f, i.e., f † = (BT A)−1 −BT αX, to the condition of the
523"
REFERENCES,0.6357758620689655,"expectation. The rest steps are based on the structural causal equations defined in Eq. (2).
524"
REFERENCES,0.6368534482758621,"C.3
Another Case of Linear SCM with Latent Correlators
525"
REFERENCES,0.6379310344827587,"Corollary C.1. For another Linear Structural Causal Model defined as follows
526"
REFERENCES,0.6390086206896551,"T ←1(αT +
X
βi · Ci > a)"
REFERENCES,0.6400862068965517,"Mj ←αM + γj · T + ϕj · Uj
X ←αX + A[M||C]"
REFERENCES,0.6411637931034483,Y ←αY + τ · T +
REFERENCES,0.6422413793103449,"X
θj · Uj +
X
κi · Ci, (16)"
REFERENCES,0.6433189655172413,"where f = A ∈RKX×(KC+KM) is a full column-rank matrix, the CATE, ATE, and the bias of
527"
REFERENCES,0.6443965517241379,"proxy-of-confounder-based causal inference model that controls the latent variable Z inferred via
528"
REFERENCES,0.6454741379310345,"Z = ¯f(X) = BT X can be formulated as follows:
529"
REFERENCES,0.646551724137931,ATE = CATE = τ
REFERENCES,0.6476293103448276,"E[DCEV (Z = BT X)] = DCEV (Z = BT X) = τ−
X θj · γj ϕj"
REFERENCES,0.6487068965517241,Bias = ATE −E[DCEV (BT X)] =
REFERENCES,0.6497844827586207,"X θj · γj ϕj , (17)"
REFERENCES,0.6508620689655172,where B ∈RKX×(KC+KM) is another full column-rank matrix. Since P θj·γj
REFERENCES,0.6519396551724138,"ϕj
is arbitrary, the
530"
REFERENCES,0.6530172413793104,"estimator E[DCEV (BT X)] is arbitrarily biased for the estimation of ATE.
531"
REFERENCES,0.6540948275862069,"Proof. The proof of the CATE and ATE is trivial. The causal estimand and the bias of a proxy-
532"
REFERENCES,0.6551724137931034,"of-confounder-based causal inference model that controls the latent variables Z inferred via Z =
533"
REFERENCES,0.65625,"¯f(X) = BT X (where B is also a full column-rank matrix) can be formulated as follows:
534"
REFERENCES,0.6573275862068966,"DCEV (BT x) = E[Y |T = 1, Z = BT x] −E[Y |T = 0, Z = BT x]"
REFERENCES,0.6584051724137931,"= E[Y |T = 1, Z = αX + BT A[c||m]] −E[Y |T = 0, Z = αX + BT A[c||m]]"
REFERENCES,0.6594827586206896,"(a)
= E[Y |T = 1, C = c, M = m] −E[Y |T = 0, C = c, M = m]"
REFERENCES,0.6605603448275862,"= αY + τ · 1 +
X
θj · E[Uj|T = 1, C = c, M = m] +
X
κi · E[Ci|T = 1, C = c, M = m]"
REFERENCES,0.6616379310344828,"−αY + τ · 0 +
X
θj · E[Uj|T = 0, C = c, M = m] +
X
κi · E[Ci|T = 0, C = c, M = m]"
REFERENCES,0.6627155172413793,"= τ · (1 −0) +
X
θj · (ϕ−1
j
· (mj −αM −γj) −ϕ−1
j
· (mj −αM)) +
X
κi · (ci −ci)"
REFERENCES,0.6637931034482759,"= τ −
X θj · γj"
REFERENCES,0.6648706896551724,"ϕj
= E

τ −
X θj · γj ϕj"
REFERENCES,0.665948275862069,"
= E[DCEV (BT X)], (18) 535"
REFERENCES,0.6670258620689655,"where step (a) and the rest of the proof follow the same logic as the proof in Section 3.3.
536"
REFERENCES,0.6681034482758621,"C.4
Proof of Theorem 4.1
537"
REFERENCES,0.6691810344827587,"The strict definitions of the exponential family, strong exponential (which is assumed for the factorized
538"
REFERENCES,0.6702586206896551,"part of the conditional prior), and identifiability follow [19, 26], and can be referred to in Appendix
539"
REFERENCES,0.6713362068965517,"E, F of [26], which we omit to avoid redundancy. The proof of Theorem 4.1 is largely based on the
540"
REFERENCES,0.6724137931034483,"NF-iVAE paper [26], where most of the details can be found, with the new assumption introduced in
541"
REFERENCES,0.6734913793103449,"CiVAE that each Sf,i has at least one invertible dimension incorporated to ensure that each dimension
542"
REFERENCES,0.6745689655172413,"of the inferred latent variables is a bijective transformation of the corresponding true latent variable.
543"
REFERENCES,0.6756465517241379,"C.4.1
PART I
544"
REFERENCES,0.6767241379310345,"Step I. In this step, we transform the equality of noisy conditional marginal distribution of X given
545"
REFERENCES,0.677801724137931,"Y, T of two models with parameter θ, ˜θ ∈Θ into the equality of noise-free distributions.
546"
REFERENCES,0.6788793103448276,"pθ(X | Y, T) = p ˜θ(X | Y, T) =⇒
Z"
REFERENCES,0.6799568965517241,"Z
pf(X | Z)pS,λ(Z | Y, T)dZ =
Z"
REFERENCES,0.6810344827586207,"Z
p ˜
f(X | Z)p ˜
S,˜λ(Z | Y, T)dZ =⇒
Z"
REFERENCES,0.6821120689655172,"Z
pε(X −f(Z))pS,λ(Z | Y, T)dZ =
Z"
REFERENCES,0.6831896551724138,"Z
pε(X −˜f(Z))p ˜
S,˜λ(Z | Y, T)dZ"
REFERENCES,0.6842672413793104,"(a)
=⇒
Z"
REFERENCES,0.6853448275862069,"X
pε(X −X)pS,λ
 
f †(X) | Y, T

vol
 
Jf †(X)

dX =
Z"
REFERENCES,0.6864224137931034,"X
pε(X −X)p ˜
S,˜λ

˜f †(X) | Y, T

vol

J ˜
f †(X)

dX"
REFERENCES,0.6875,"(b)
=⇒
Z"
REFERENCES,0.6885775862068966,"Rd pε(X −X)˜pf,S,λ,Y,T (X)dX =
Z"
REFERENCES,0.6896551724137931,"Rd pε(X −X)˜p ˜
f, ˜
S,˜λ, ˜Y , ˜T (X)dX"
REFERENCES,0.6907327586206896,"=⇒(˜pf,S,λ,Y,T ∗pε) (X) =

˜p ˜
f, ˜
S,˜λ, ˜Y , ˜T ∗pε

(X)"
REFERENCES,0.6918103448275862,"(c)
=⇒F [˜pf,S,λ,Y,T ] (ω)φε(ω) = F
h
˜p ˜
f, ˜
S,˜λ, ˜Y , ˜T
i
(ω)φε(ω)"
REFERENCES,0.6928879310344828,"(d)
=⇒F [˜pf,S,λ,Y,T ] (ω) = F
h
˜p ˜
f, ˜
S,˜λ, ˜Y , ˜T
i
(ω)"
REFERENCES,0.6939655172413793,"=⇒˜pf,S,λ,Y,T (X) = ˜p ˜
f, ˜
S,˜λ, ˜Y , ˜T (X). (19)"
REFERENCES,0.6950431034482759,"Step (a) is based on the rule of change-of-variable, where vol(A) =
r"
REFERENCES,0.6961206896551724,"det

AT A

. In step (b),
547"
REFERENCES,0.697198275862069,"we define ˜pf,S,λ,Y,T (X) ≜pS,λ
 
f †(X) | Y, T

vol
 
Jf †(X)

IX (X). In step (c), we use F[·] to
548"
REFERENCES,0.6982758620689655,"denote the Fourier transform. In step (d), we drop φε(ω) as it is non-zero a.e. (see Assumption 3).
549"
REFERENCES,0.6993534482758621,"Step II. In this step, we transform the equality of the noise-free distributions into the relationship of
550"
REFERENCES,0.7004310344827587,"the sufficient statistics S and ˜S. By taking logarithm of both sides of Eq. (19), we have:
551"
REFERENCES,0.7015086206896551,"log vol
 
Jf †(X)

+ log Q
 
f †(X)

−log C(Y, T) +

S
 
f †(X)

, λ(Y, T)"
REFERENCES,0.7025862068965517,"= log vol

J ˜
f †(X)

+ log ˜Q

˜f †(X)

−log ˜C(Y, T) +
D
˜S

˜f †(X)

, ˜λ(Y, T)
E
.
(20)"
REFERENCES,0.7036637931034483,"Let (Y, T)0, · · · , (Y, T)k be the k + 1 distinct points defined in Assumption 3 - (iv). We obtain k + 1
552"
REFERENCES,0.7047413793103449,"equations by evaluating the Eq. (20) at these points, where the first equation is subtracted from the
553"
REFERENCES,0.7058189655172413,"remaining ones, which leads to the following equation system:
554"
REFERENCES,0.7068965517241379,"S
 
f †(X)

, λ ((Y, T)l) −λ ((Y, T)0)⟩+ log C ((Y, T)0)"
REFERENCES,0.7079741379310345,"C ((Y, T)l)"
REFERENCES,0.709051724137931,"=
D
˜S

˜f †(X)

, ˜λ ((Y, T)l) −˜λ ((Y, T)0)
E
+ log
˜C ((Y, T)0)"
REFERENCES,0.7101293103448276,"˜C ((Y, T)l)
,
l = 1, · · · , k.
(21)"
REFERENCES,0.7112068965517241,"Let L be the invertible matrix defined in Assumption 3 - (iv) and ˜L be the counterpart for ˜λ, if we
555"
REFERENCES,0.7122844827586207,"summarize all terms irrelevant to X into a constant b,we have:
556"
REFERENCES,0.7133620689655172,"LT S
 
f †(X)

= ˜LT ˜S

˜f †(X)

+ b"
REFERENCES,0.7144396551724138,"=⇒S
 
f †(X)

= A ˜S

˜f †(X)

+ c,
(22)"
REFERENCES,0.7155172413793104,"where A = L−T ˜L ∈Rk×k, and c = L−T b ∈Rk.
557"
REFERENCES,0.7165948275862069,"Step III. Ideally, to prove the element-wise bijective identifiability of the latent variables Z, the
558"
REFERENCES,0.7176724137931034,"transformation of the sufficient statistics S derived in Eq. (22) should be bijective. We claim that if
559"
REFERENCES,0.71875,"the conditional prior pS,λ(Z | Y, T) is strongly exponential and L is invertible, ˜L and A must also
560"
REFERENCES,0.7198275862068966,"be invertible. The proof is omitted, and can be referred to in Appendix H.1.1 of [26].
561"
REFERENCES,0.7209051724137931,"C.4.2
PART II
562"
REFERENCES,0.7219827586206896,"In this part, we prove that, if Assumptions 1, 2 and 3 hold, we can identify the factorized part
563"
REFERENCES,0.7230603448275862,"of the sufficient statistics S(Z), i.e., Sf(Z), up to permutation and element-wise transformation.
564"
REFERENCES,0.7241379310344828,"Specifically, if we use v to denote the composite map ˜f † ◦f : Z →Z, Eq. (22) can be rewritten into:
565"
REFERENCES,0.7252155172413793,"S(Z) = A ˜S(v(Z)) + c.
(23)"
REFERENCES,0.7262931034482759,"We aim to prove that A in Eq. (23) is a block permutation matrix.
566"
REFERENCES,0.7273706896551724,"Step I. We start by showing that v is a component-wise function. If we differentiate both sides of Eq.
567"
REFERENCES,0.728448275862069,"(23) with respect to Zs and Zt, where s ̸= t, we have:
568 ∂S(Z)"
REFERENCES,0.7295258620689655,"∂Zs
= A KZ
X i=1"
REFERENCES,0.7306034482758621,∂˜S(v(Z))
REFERENCES,0.7316810344827587,"∂vi(Z)
· ∂vi(Z) ∂Zs"
REFERENCES,0.7327586206896551,"∂2S(Z)
∂Zs∂Zt
= A KZ
X i=1 KZ
X i=1"
REFERENCES,0.7338362068965517,"∂2 ˜S(v(Z))
∂vi(Z)∂vj(Z) · ∂vj(Z)"
REFERENCES,0.7349137931034483,"∂Zt
· ∂vi(Z)"
REFERENCES,0.7359913793103449,"∂Zs
+ A KZ
X i=1"
REFERENCES,0.7370689655172413,∂˜S(v(Z))
REFERENCES,0.7381465517241379,"∂vi(Z)
· ∂2vi(Z)"
REFERENCES,0.7392241379310345,"∂Zs∂Zt
. (24)"
REFERENCES,0.740301724137931,"Note that for the factorized part of the sufficient statistics S, i.e., Sf, all cross-derivatives are zero,
569"
REFERENCES,0.7413793103448276,"and for the non-factorized part of S, i.e., Snf, which is a neural network with ReLU activation (i.e.,
570"
REFERENCES,0.7424568965517241,"linear a.e.), all second-order derivatives are zero. Therefore, the second order cross-derivatives on
571"
REFERENCES,0.7435344827586207,"the LHS. of Eq. (24) are zero, which leads to the following equality:
572 0 = A KZ
X i=1"
REFERENCES,0.7446120689655172,∂2 ˜S(v(Z))
REFERENCES,0.7456896551724138,"∂vi(Z)2
· ∂vi(Z)"
REFERENCES,0.7467672413793104,"∂Zt
· ∂vi(Z)"
REFERENCES,0.7478448275862069,"∂Zs
+ A KZ
X i=1"
REFERENCES,0.7489224137931034,∂˜S(v(Z))
REFERENCES,0.75,"∂vi(Z)
· ∂2vi(Z)"
REFERENCES,0.7510775862068966,"∂Zs∂Zt
.
(25)"
REFERENCES,0.7521551724137931,"Eq. (25) can be written into the matrix-vector product form as follows:
573"
REFERENCES,0.7532327586206896,"0 = A ˜S′′(Z)v′
s,t(Z) + A ˜S′(Z)v′′
s,t(Z),
(26) where"
REFERENCES,0.7543103448275862,˜S′′(Z) =
REFERENCES,0.7553879310344828,"""
∂2 ˜S(v(Z))"
REFERENCES,0.7564655172413793,"∂v1(Z)2 , · · · , ∂2 ˜S(v(Z))"
REFERENCES,0.7575431034482759,∂vKZ(Z)2 #
REFERENCES,0.7586206896551724,"∈Rk×KZ,"
REFERENCES,0.759698275862069,"v′
s,t(Z) =
∂v1(Z)"
REFERENCES,0.7607758620689655,"∂Zt
· ∂v1(Z)"
REFERENCES,0.7618534482758621,"∂Zs
, · · · , ∂vKZ(Z)"
REFERENCES,0.7629310344827587,"∂Zt
· ∂vKZ(Z) ∂Zs"
REFERENCES,0.7640086206896551,"T
∈RKZ, and"
REFERENCES,0.7650862068965517,˜S′(Z) =
REFERENCES,0.7661637931034483,"""
∂˜S(v(Z))"
REFERENCES,0.7672413793103449,"∂v1(Z) , · · · , ∂˜S(v(Z))"
REFERENCES,0.7683189655172413,∂vKZ(Z) #
REFERENCES,0.7693965517241379,"∈Rk×KZ,"
REFERENCES,0.7704741379310345,"v′′
s,t(Z) =
∂2v1(Z)"
REFERENCES,0.771551724137931,"∂Zs∂Zt
, · · · , ∂2vKZ(Z)"
REFERENCES,0.7726293103448276,∂Zs∂Zt
REFERENCES,0.7737068965517241,"T
∈RKZ."
REFERENCES,0.7747844827586207,"If we denote the concatenation as ˜S′′′(Z) =
h
˜S′′(Z), ˜S′(Z)
i
∈Rk×2KZ and v′′
s,t(Z) =
574"
REFERENCES,0.7758620689655172,"
v′
s,t(Z)T , v′′
s,t(Z)T T ∈R2Kz, we have:
575"
REFERENCES,0.7769396551724138,"0 = A ˜S′′′(Z)v′′′
s,t(Z).
(27)"
REFERENCES,0.7780172413793104,"Finally, if we denote the rows of ˜S′′′(Z) that correspond to the factorized part of S by ˜S′′′
f (Z),
576"
REFERENCES,0.7790948275862069,"according to Lemma 5 of the iVAE paper [19] and the assumption that k ≥2KZ, we have that the
577"
REFERENCES,0.7801724137931034,"rank of ˜S′′′
f (Z) is 2KZ. Since k ≥2KZ, the rank of ˜S′′′
f (Z) is also 2KZ. Since the rank of A is k,
578"
REFERENCES,0.78125,"the rank of A ˜S′′′(Z) is 2KZ, which implies that v′′′
s,t(Z) ∈R2KZ is a zero vector. Therefore, we
579"
REFERENCES,0.7823275862068966,"have v′
s,t(Z) = 0, ∀s ̸= t, and we have demonstrated that v is a component-wise function.
580"
REFERENCES,0.7834051724137931,"Step II. Based on Step I, we demonstrate that A is a block permutation matrix. Without loss of gen-
581"
REFERENCES,0.7844827586206896,"erality, we assume that the permutation in v is Identity, where v(Z) = [v1 (Z1) , · · · , vKZ (ZKZ)]T
582"
REFERENCES,0.7855603448275862,"and each vi is a nonlinear univariate scalar function. Since f and ˜f are injective, v is bijective and
583"
REFERENCES,0.7866379310344828,"v−1(Z) =

v−1
1
(Z1) , · · · , v−1
KZ (ZKZ)
T . If we denote S(v(Z)) = ˜S(v(Z)) + A−1c, Eq. (23)
584"
REFERENCES,0.7877155172413793,"can be reformulated as S(Z) = AS(v(Z)). We then apply v−1 to Z on both sides, which gives
585"
REFERENCES,0.7887931034482759,"S
 
v−1(Z)

= AS(Z).
(28)"
REFERENCES,0.7898706896551724,"Let t be the index of an entry in S that corresponds to the factorized part Sf. For all s ̸= t, we have:
586"
REFERENCES,0.790948275862069,"0 = ∂S
 
v−1(Z)
"
REFERENCES,0.7920258620689655,"t
∂Zs
= k
X"
REFERENCES,0.7931034482758621,"j=1
atj
∂S(Z)j"
REFERENCES,0.7941810344827587,"∂Zs
.
(29)"
REFERENCES,0.7952586206896551,"Since the entries of ˜S are linearly independent, atj is zero for any j such that ∂S(Z)j"
REFERENCES,0.7963362068965517,"∂Zs
̸= 0. This
587"
REFERENCES,0.7974137931034483,"includes the entries Sj that correspond to (1) the factorized part that does not depend on Zt; and (2)
588"
REFERENCES,0.7984913793103449,"the non-factorized part Snf. Therefore, when t is the index of an entry in the sufficient statistics S
589"
REFERENCES,0.7995689655172413,"that corresponds to factor i in the factorized part Sf, i.e., Sf,i, the only non-zero atj are the ones that
590"
REFERENCES,0.8006465517241379,"map between Sf,i (Zi) and Sf,i (vi (Zi)). Therefore, we can construct an invertible submatrix A′
i
591"
REFERENCES,0.8017241379310345,"with all non-zero elements atj for all t that corresponds to factor i, such that
592"
REFERENCES,0.802801724137931,"Sf,i (Zi) = A′
iSf,i (vi (Zi)) = A′
i ˜Sf,i (vi (Zi)) + ci,
i = 1, · · · , KZ,
(30)"
REFERENCES,0.8038793103448276,"where ci denotes the corresponding elements of c. Eq. (30) means that for each i = 1, · · · , KZ,
593"
REFERENCES,0.8049568965517241,"the matrix block A′
i of A affinely transforms the i-specific sufficient statistics vector Sf,i (Zi) into
594"
REFERENCES,0.8060344827586207,"˜Sf,i (vi (Zi)). In addition, there is also an additional block A′ that affinely transforms Snf(Z) in
595"
REFERENCES,0.8071120689655172,"into Snf(v(Z)). This completes the proof that A is a block permutation matrix.
596"
REFERENCES,0.8081896551724138,"C.4.3
PART III
597"
REFERENCES,0.8092672413793104,"Let ˜Zi = vi (Zi) = ˜f †(X)i be the ith inferred latent variable. Assume again that the permutation in
598"
REFERENCES,0.8103448275862069,"v is Identity. In this part, we prove that if Assumption 2 holds, each inferred latent variable ˜Zi is the
599"
REFERENCES,0.8114224137931034,"bijective transformation of the true latent variable. The proof is as follows.
600"
REFERENCES,0.8125,"Proof. Plugging ˜Zi into Eq. (30), we have:
601"
REFERENCES,0.8135775862068966,"Sf,i(Zi) = A′
i ¯Sf,i( ˜Zi).
(31)"
REFERENCES,0.8146551724137931,"According to Assumption 2, there exists one dimension of Sf,i, i.e., j, such that Sf,ij is bijective.
602"
REFERENCES,0.8157327586206896,"This implies that Sf,i is injective, and therefore it has a left-inverse S†
f,i. we apply S†
f,i to both sides
603"
REFERENCES,0.8168103448275862,"of Eq. (31), which gives:
604"
REFERENCES,0.8178879310344828,"Zi = S†
f,iA′
i ¯Sf,i( ˜Zi).
(32)"
REFERENCES,0.8189655172413793,"Since A′
i is a block of an invertible block permutation matrix, Ai is also an invertible matrix, and
605"
REFERENCES,0.8200431034482759,"therefore A′
i is a bijective mapping. In addition, since ˜Sf,i is injective, ¯Sf,i is also injective, and
606"
REFERENCES,0.8211206896551724,"therefore the composite map S†
f,iA′
i ¯Sf,i : R →R that applies on ˜Zi is a bijective. This completes
607"
REFERENCES,0.822198275862069,"the proof that each inferred latent variable ˜Zi is the bijective transformation of the true latent variable
608"
REFERENCES,0.8232758620689655,"in the case of no noise, where Z = f †(X) are the true latent variables. If noise ε exists, the posterior
609"
REFERENCES,0.8243534482758621,"distribution of the latent variables can be identified up to an analogous bijective indeterminacy.
610"
REFERENCES,0.8254310344827587,"C.4.4
Consistency
611"
REFERENCES,0.8265086206896551,"Proof. If the family of the variational posterior qϕ(Z|X, Y, T) contains the true posterior
612"
REFERENCES,0.8275862068965517,"pθ(Z|X, Y, T), then by optimizing the loss of Eq. (9) (with the KL term replaced by the score match-
613"
REFERENCES,0.8286637931034483,"ing loss defined in Eq. (10)) over its parameter ϕ, the score matching term will eventually vanish.
614"
REFERENCES,0.8297413793103449,"Therefore, the ELBO term in Eq. (9) will be equal to the log-likelihood. Under this circumstance,
615"
REFERENCES,0.8308189655172413,"CiVAE inherits all the properties of maximum likelihood estimation (MLE). Since the identifiability
616"
REFERENCES,0.8318965517241379,"of CiVAE is guaranteed up to permutation and component-wise bijective transformation of the latent
617"
REFERENCES,0.8329741379310345,"variables, the consistency property of MLE means that the model will converge to the true parameter
618"
REFERENCES,0.834051724137931,"θ∗up to such mild indeterminacy of the latent variables in the limit of infinite data.
619"
REFERENCES,0.8351293103448276,"C.5
Proof of Theorem 4.2
620"
REFERENCES,0.8362068965517241,"Proof. Let C be the true latent confounders and ˜C be the transformed confounders, where the
621"
REFERENCES,0.8372844827586207,"transformation function f is bijective and differentiable a.e. Let f −1 denote its inverse. The ATE
622"
REFERENCES,0.8383620689655172,"estimator that controls transformed confounders ˜C can be formulated as:
623"
REFERENCES,0.8394396551724138,"DEV ( ˜
C) = Ep( ˜
C)[E[Y |T = 1, ˜C = ˜c] −E[Y |T = 0, ˜C = ˜c]].
(33)"
REFERENCES,0.8405172413793104,"Specifically, for the continuous case where density functions exist, for each term, we have:
624"
REFERENCES,0.8415948275862069,"Ep( ˜
C)[E[Y |T = t, ˜C = ˜c]] =
Z
f ˜
C(˜c)
Z
y · fY |T, ˜
C(y|t, ˜c)dyd˜c.
(34)"
REFERENCES,0.8426724137931034,"For the marginal density f ˜
C(˜c), the following equality holds:
625"
REFERENCES,0.84375,"f ˜
C(˜c) = fC(f −1(˜c))|Jf −1(˜c)| = fC(c)|Jf −1(˜c)|.
(35)"
REFERENCES,0.8448275862068966,"As for the conditional density fY |T, ˜
C(y|t, ˜c), since f is bijective, according to Eq. (12), we have:
626"
REFERENCES,0.8459051724137931,"fY |T, ˜
C(y|t, ˜c) = fY |T,C(y|t, c).
(36)"
REFERENCES,0.8469827586206896,"Combining Eqs. (35) and (36), and given that d˜c = |Jf(c)|dc, we have:
627"
REFERENCES,0.8480603448275862,"(34) =
Z
fC(c)|Jf −1(˜c)|
Z
y · fY |T,C(y|t, c)dy|Jf(c)|dc"
REFERENCES,0.8491379310344828,"=|Jf −1(˜c)| · |Jf(c)|
Z
fC(c)
Z
y · fY |T,C(y|t, c)dydc"
REFERENCES,0.8502155172413793,"(a)
=
Z
fC(c)
Z
y · fY |T,C(y|t, c)dydc"
REFERENCES,0.8512931034482759,"=Ep(C)[E[Y |T = t, C = c]], (37)"
REFERENCES,0.8523706896551724,Table 2: Comparison of CiVAE with baselines when intra-interactions among M exist.
REFERENCES,0.853448275862069,"Dataset
LatentMediator
LatentCorrelator
Company (Age)
Company (Gender)
Method
ATE.
Err.
ATE.
Err.
ATE.
Err.
ATE.
Err."
REFERENCES,0.8545258620689655,"CEVAE
1.627 ± 0.549
2.627
2.659 ± 0.302
1.353
0.152 ± 0.027
0.420
-0.225 ± 0.044
-0.144
TEDVAE
1.653 ± 0.511
2.042
2.827 ± 0.259
1.521
0.180 ± 0.047
0.448
-0.189 ± 0.012
-0.108
CiVAE
-0.350 ± 0.695
1.785
1.785 ± 0.481
0.479
-0.073 ±0.101
0.195
-0.136 ± 0.087
-0.055
True ATE
-1.000 ± 0.000
0.000
1.306 ± 0.000
0.000
-0.268 ± 0.000
0.000
-0.081 ± 0.000
0.000"
REFERENCES,0.8556034482758621,Table 3: Comparison of CiVAE with baselines when inter-interactions between C and M exist.
REFERENCES,0.8566810344827587,"Dataset
LatentMediator
LatentCorrelator
Company (Age)
Company (Gender)
Method
ATE.
Err.
ATE.
Err.
ATE.
Err.
ATE.
Err."
REFERENCES,0.8577586206896551,"CEVAE
2.070 ± 0.279
3.070
2.831 ± 0.398
1.831
0.094 ± 0.061
0.362
-0.192 ± 0.015
-0.111
TEDVAE
1.743 ± 0.307
2.743
2.954 ± 0.763
1.954
0.109 ± 0.116
0.377
-0.212 ± 0.019
-0.131
CiVAE
-0.716 ± 0.523
0.284
1.385 ± 0.660
0.385
-0.041 ±0.144
0.227
-0.129 ± 0.064
-0.048
True ATE
-1.000 ± 0.000
0.000
1.000 ± 0.000
0.000
-0.268 ± 0.000
0.000
-0.081 ± 0.000
0.000"
REFERENCES,0.8588362068965517,"where the term |Jf −1(˜c)| · |Jf(c)| vanishes in step (a) as the two factors have the product of one.
628"
REFERENCES,0.8599137931034483,"Therefore, if we plug Eq. (37) into Eq. (33), it leads to the following equality:
629"
REFERENCES,0.8609913793103449,"DEV ( ˜
C) = Ep( ˜
C)[E[Y |T = 1, ˜C = ˜c] −E[Y |T = 0, ˜C = ˜c]]"
REFERENCES,0.8620689655172413,"= Ep(C)[E[Y |T = 1, C = c] −E[Y |T = 0, C = c]] = DEV (C) = ATE,
(38)"
REFERENCES,0.8631465517241379,"where the last step is due to Eq. (2) in Definition 2, which completes our proof that controlling
630"
REFERENCES,0.8642241379310345,"bijectively transformed confounders provides an unbiased estimation of ATE.
631"
REFERENCES,0.865301724137931,"D
Extending CiVAE to address Latent Interactions
632"
REFERENCES,0.8663793103448276,"In this section, we extend CiVAE to more general cases where interactions exist among the latent
633"
REFERENCES,0.8674568965517241,"confounders C and the latent post-treatment variables M. Here, we note that the identification
634"
REFERENCES,0.8685344827586207,"of latent confounders C in CiVAE is achieved in two steps. (i) CiVAE individually identifies
635"
REFERENCES,0.8696120689655172,"latent variables [C, M] that generate X in inferred Z (but which dims of Z correspond to C
636"
REFERENCES,0.8706896551724138,"or M is unknown). (ii) pairwise independence test to identify C. Since Assumption 2 allows
637"
REFERENCES,0.8717672413793104,"arbitrary dependence among C and M, step (i) still holds when interactions among [C, M] exist.
638"
REFERENCES,0.8728448275862069,"To distinguish C in these cases, we can use more general causal discovery algorithms, e.g., the
639"
REFERENCES,0.8739224137931034,"PC algorithm [18] in the second step. In this section, we consider two cases of interaction: (i)
640"
REFERENCES,0.875,"Intra-Interaction among mediators, and (ii) Inter-Interaction among mediators and confounders.
641"
REFERENCES,0.8760775862068966,"D.1
Intra-Interactions among Latent Mediators
642"
REFERENCES,0.8771551724137931,"In this subsection, we discuss the case where latent post-treatment variables M interact with each
643"
REFERENCES,0.8782327586206896,"other. Since in this case, M cannot causally influence the latent confounders C (otherwise C will be
644"
REFERENCES,0.8793103448275862,"post-treatment), and the PC algorithm orients edges in causal graphs via colliders, latent confounders
645"
REFERENCES,0.8803879310344828,"can still be identified from the inferred Z as they form colliders with the treatment T.
646"
REFERENCES,0.8814655172413793,"To empirically verify the claim, we extend the simulated datasets described in Section 5.1, where we
647"
REFERENCES,0.8825431034482759,"make (i) T directly affects M1, (ii) M1 affects M2, and (iii) M1, M2 affect M3. The coefficients are
648"
REFERENCES,0.8836206896551724,"randomly sampled from N(0, 1/3). In step (ii), we use the PC algorithm [18] to identify C from
649"
REFERENCES,0.884698275862069,"the inferred Z. The results in Table 2 demonstrate that the adapted CiVAE is still significantly more
650"
REFERENCES,0.8857758620689655,"robust to latent post-treatment bias compared to CEVAE and TEDVAE, which empirically verify our
651"
REFERENCES,0.8868534482758621,"claim that PC-adapted CiVAE can address the interaction among post-treatment variables.
652"
REFERENCES,0.8879310344827587,"D.2
Inter-Interactions between Latent Mediators and Latent Confounders
653"
REFERENCES,0.8890086206896551,"In this subsection, we discuss another case where inter-interactions exist between latent confounders
654"
REFERENCES,0.8900862068965517,"C and latent post-treatment variables M. Since in this case, M still cannot causally influence C
655"
REFERENCES,0.8911637931034483,"(otherwise C will be post-treatment), and the PC algorithm orients edges in causal graph via colliders,
656"
REFERENCES,0.8922413793103449,"latent confounders C can still be identified from Z as they form colliders with the treatment T.
657"
REFERENCES,0.8933189655172413,"To verify the claim, we extend the simulated datasets described in Section 5.1 to allow each latent
658"
REFERENCES,0.8943965517241379,"confounder Ci ∈R3 to determine M ∈R3. The coefficients are randomly sampled from N(0, 1/3).
659"
REFERENCES,0.8954741379310345,"In step (ii), we use the PC algorithm to identify C from the inferred Z. The results in Table 3
660"
REFERENCES,0.896551724137931,"demonstrate that the PC-adapted CiVAE is still significantly more robust to latent post-treatment bias
661"
REFERENCES,0.8976293103448276,"compared to CEVAE and TEDVAE, which empirically verify our claim that PC-adapted CiVAE can
662"
REFERENCES,0.8987068965517241,"address the case where inter-interactions exist among latent confounders and post-treatment variables.
663"
REFERENCES,0.8997844827586207,"NeurIPS Paper Checklist
664"
CLAIMS,0.9008620689655172,"1. Claims
665"
CLAIMS,0.9019396551724138,"Question: Do the main claims made in the abstract and introduction accurately reflect the
666"
CLAIMS,0.9030172413793104,"paper’s contributions and scope?
667"
CLAIMS,0.9040948275862069,"Answer: [Yes]
668"
CLAIMS,0.9051724137931034,"Justification: The contribution of this paper can be summarized as: We study a critical but
669"
CLAIMS,0.90625,"easily overlooked problem in causal effect estimation: latent post-treatment bias, and we
670"
CLAIMS,0.9073275862068966,"propose a novel framework, i.e., CiVAE, to address the bias. The details are in Section 4.
671"
LIMITATIONS,0.9084051724137931,"2. Limitations
672"
LIMITATIONS,0.9094827586206896,"Question: Does the paper discuss the limitations of the work performed by the authors?
673"
LIMITATIONS,0.9105603448275862,"Answer: [Yes]
674"
LIMITATIONS,0.9116379310344828,"Justification: We have discussed the potential issue of the vanilla when interactions among
675"
LIMITATIONS,0.9127155172413793,"the latent variables exists. However, in Section D we have addressed the issue by extendeding
676"
LIMITATIONS,0.9137931034482759,"our framework.
677"
THEORY ASSUMPTIONS AND PROOFS,0.9148706896551724,"3. Theory Assumptions and Proofs
678"
THEORY ASSUMPTIONS AND PROOFS,0.915948275862069,"Question: For each theoretical result, does the paper provide the full set of assumptions and
679"
THEORY ASSUMPTIONS AND PROOFS,0.9170258620689655,"a complete (and correct) proof?
680"
THEORY ASSUMPTIONS AND PROOFS,0.9181034482758621,"Answer: [Yes]
681"
THEORY ASSUMPTIONS AND PROOFS,0.9191810344827587,"Justification: We have introduced the three mild assumptions required for the identification
682"
THEORY ASSUMPTIONS AND PROOFS,0.9202586206896551,"of causal effects under latent post-treatment bias. In addition, we have provided the proof
683"
THEORY ASSUMPTIONS AND PROOFS,0.9213362068965517,"for all the theorems in the Appendix.
684"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9224137931034483,"4. Experimental Result Reproducibility
685"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9234913793103449,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
686"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9245689655172413,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
687"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9256465517241379,"of the paper (regardless of whether the code and data are provided or not)?
688"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9267241379310345,"Answer: [Yes]
689"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.927801724137931,"Justification: We have provided implementation details in Section 5.1. In addition, we have
690"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9288793103448276,"provided a code demo in an anonymous URL.
691"
OPEN ACCESS TO DATA AND CODE,0.9299568965517241,"5. Open access to data and code
692"
OPEN ACCESS TO DATA AND CODE,0.9310344827586207,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
693"
OPEN ACCESS TO DATA AND CODE,0.9321120689655172,"tions to faithfully reproduce the main experimental results, as described in supplemental
694"
OPEN ACCESS TO DATA AND CODE,0.9331896551724138,"material?
695"
OPEN ACCESS TO DATA AND CODE,0.9342672413793104,"Answer: [Yes]
696"
OPEN ACCESS TO DATA AND CODE,0.9353448275862069,"Justification: See Checklist 4.
697"
OPEN ACCESS TO DATA AND CODE,0.9364224137931034,"6. Experimental Setting/Details
698"
OPEN ACCESS TO DATA AND CODE,0.9375,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
699"
OPEN ACCESS TO DATA AND CODE,0.9385775862068966,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
700"
OPEN ACCESS TO DATA AND CODE,0.9396551724137931,"results?
701"
OPEN ACCESS TO DATA AND CODE,0.9407327586206896,"Answer: [Yes]
702"
OPEN ACCESS TO DATA AND CODE,0.9418103448275862,"Justification: See Checklist 4.
703"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9428879310344828,"7. Experiment Statistical Significance
704"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9439655172413793,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
705"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9450431034482759,"information about the statistical significance of the experiments?
706"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9461206896551724,"Answer: [Yes]
707"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.947198275862069,"Justification: We have reported the error of five independent run for both the proposed
708"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9482758620689655,"CiVAE and all the baselines in the main paper.
709"
EXPERIMENTS COMPUTE RESOURCES,0.9493534482758621,"8. Experiments Compute Resources
710"
EXPERIMENTS COMPUTE RESOURCES,0.9504310344827587,"Question: For each experiment, does the paper provide sufficient information on the com-
711"
EXPERIMENTS COMPUTE RESOURCES,0.9515086206896551,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
712"
EXPERIMENTS COMPUTE RESOURCES,0.9525862068965517,"the experiments?
713"
EXPERIMENTS COMPUTE RESOURCES,0.9536637931034483,"Answer: [Yes]
714"
EXPERIMENTS COMPUTE RESOURCES,0.9547413793103449,"Justification: See Checklist 4.
715"
CODE OF ETHICS,0.9558189655172413,"9. Code Of Ethics
716"
CODE OF ETHICS,0.9568965517241379,"Question: Does the research conducted in the paper conform, in every respect, with the
717"
CODE OF ETHICS,0.9579741379310345,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
718"
CODE OF ETHICS,0.959051724137931,"Answer: [Yes]
719"
CODE OF ETHICS,0.9601293103448276,"Justification: We have carefully read the code of ethics and behaved strictly according to it.
720"
BROADER IMPACTS,0.9612068965517241,"10. Broader Impacts
721"
BROADER IMPACTS,0.9622844827586207,"Question: Does the paper discuss both potential positive societal impacts and negative
722"
BROADER IMPACTS,0.9633620689655172,"societal impacts of the work performed?
723"
BROADER IMPACTS,0.9644396551724138,"Answer: [Yes]
724"
BROADER IMPACTS,0.9655172413793104,"Justification: See Section A of the Appendix.
725"
SAFEGUARDS,0.9665948275862069,"11. Safeguards
726"
SAFEGUARDS,0.9676724137931034,"Question: Does the paper describe safeguards that have been put in place for responsible
727"
SAFEGUARDS,0.96875,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
728"
SAFEGUARDS,0.9698275862068966,"image generators, or scraped datasets)?
729"
SAFEGUARDS,0.9709051724137931,"Answer: [NA]
730"
SAFEGUARDS,0.9719827586206896,"Justification: Our model does not have a high risk for misuse.
731"
LICENSES FOR EXISTING ASSETS,0.9730603448275862,"12. Licenses for existing assets
732"
LICENSES FOR EXISTING ASSETS,0.9741379310344828,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
733"
LICENSES FOR EXISTING ASSETS,0.9752155172413793,"the paper, properly credited and are the license and terms of use explicitly mentioned and
734"
LICENSES FOR EXISTING ASSETS,0.9762931034482759,"properly respected?
735"
LICENSES FOR EXISTING ASSETS,0.9773706896551724,"Answer: [Yes]
736"
LICENSES FOR EXISTING ASSETS,0.978448275862069,"Justification: We have cited the papers of our baselines and honor their license of code.
737"
NEW ASSETS,0.9795258620689655,"13. New Assets
738"
NEW ASSETS,0.9806034482758621,"Question: Are new assets introduced in the paper well documented and is the documentation
739"
NEW ASSETS,0.9816810344827587,"provided alongside the assets?
740"
NEW ASSETS,0.9827586206896551,"Answer: [Yes]
741"
NEW ASSETS,0.9838362068965517,"Justification: We provide the Readme file along side the codes.
742"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9849137931034483,"14. Crowdsourcing and Research with Human Subjects
743"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9859913793103449,"Question: For crowdsourcing experiments and research with human subjects, does the paper
744"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9870689655172413,"include the full text of instructions given to participants and screenshots, if applicable, as
745"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9881465517241379,"well as details about compensation (if any)?
746"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9892241379310345,"Answer: [NA]
747"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.990301724137931,"Justification: No human subjects are involved in our experiments.
748"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9913793103448276,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
749"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9924568965517241,"Subjects
750"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9935344827586207,"Question: Does the paper describe potential risks incurred by study participants, whether
751"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9946120689655172,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
752"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9956896551724138,"approvals (or an equivalent approval/review based on the requirements of your country or
753"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9967672413793104,"institution) were obtained?
754"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9978448275862069,"Answer: [NA]
755"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9989224137931034,"Justification: No human subjects are involved in our experiments.
756"
