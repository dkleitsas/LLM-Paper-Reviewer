Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002421307506053269,"While message passing Graph Neural Networks (GNNs) have become increas-
ingly popular architectures for learning with graphs, recent works have revealed
important shortcomings in their expressive power. In response, several higher-
order GNNs have been proposed that substantially increase the expressive power,
albeit at a large computational cost. Motivated by this gap, we explore alternative
strategies and lower bounds. In particular, we analyze a new recursive pooling
technique of local neighborhoods that allows different tradeoffs of computational
cost and expressive power. First, we prove that this model can count subgraphs of
size k, and thereby overcomes a known limitation of low-order GNNs. Second,
we show how recursive pooling can exploit sparsity to reduce the computational
complexity compared to the existing higher-order GNNs. More generally, we pro-
vide a (near) matching information-theoretic lower bound for counting subgraphs
with graph representations that pool over representations of derived (sub-)graphs.
We also discuss lower bounds on time complexity."
INTRODUCTION,0.004842615012106538,"1
INTRODUCTION"
INTRODUCTION,0.007263922518159807,"Graph Neural Networks (GNNs) are powerful tools for graph representation learning (Scarselli et al.,
2008; Kipf & Welling, 2017; Hamilton et al., 2017), and have been successfully applied to molecule
property prediction, simulating physics, social network analysis, knowledge graphs, trafﬁc predic-
tion and many other domains (Duvenaud et al., 2015; Defferrard et al., 2016; Battaglia et al., 2016;
Jin et al., 2018). The perhaps most widely used class of GNNs, Message Passing Graph Neural Net-
works (MPNNs) (Gilmer et al., 2017; Kipf & Welling, 2017; Hamilton et al., 2017; Xu et al., 2019;
Scarselli et al., 2008), follow an iterative message passing scheme to compute a graph representation."
INTRODUCTION,0.009685230024213076,"Despite the empirical success of MPNNs, their expressive power has been shown to be limited. For
example, their discriminative power, at best, corresponds to the one-dimensional Weisfeiler-Leman
(1-WL) graph isomorphism test (Xu et al., 2019; Morris et al., 2019), so they cannot distinguish
regular graphs, for instance. Likewise, they cannot count any induced subgraph with at least three
vertices (Chen et al., 2020), or learn structural graph parameters such as clique information, diame-
ter, conjoint or shortest cycle (Garg et al., 2020). Yet, in applications like computational chemistry,
materials design or pharmacy (Elton et al., 2019; Sun et al., 2020; Jin et al., 2018), the functions we
aim to learn often depend on the presence or count of speciﬁc substructures, e.g., functional groups."
INTRODUCTION,0.012106537530266344,"The limitations of MPNNs result from their inability to distinguish individual nodes. To resolve this
issue, two routes have been studied: (1) using unique node identiﬁers (Loukas, 2019; Sato et al.,
2019; Abboud et al., 2021), and (2) higher-order GNNs that act on k-tuples of nodes. Node IDs, if
available, enable Turing completeness for sufﬁciently large MPNNs (Loukas, 2019). Higher-order
networks use an encoding of k-tuples and then apply message passing (Morris et al., 2019), or
equivariant tensor operations (Maron et al., 2018)."
INTRODUCTION,0.014527845036319613,"The expressive power of MPNNs is often measured in terms of a hierarchy of graph isomorphism
tests, speciﬁcally the k-Weisfeiler-Leman (k-WL) hierarchy. The k-order models in (Maron et al.,
2018) and (Maron et al., 2019a) are equivalent to the k-WL and (k+1)-WL “tests”, respectively, and
are universal for the corresponding function classes (Azizian & Lelarge, 2020; Maron et al., 2019b;
Keriven & Peyr´e, 2019). Yet, these models are computationally expensive, operating on Θ(nk)"
INTRODUCTION,0.01694915254237288,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.01937046004842615,"tuples and according to current upper bounds requiring up to O(nk) iterations (Kiefer & McKay,
2020). The necessary tradeoffs between expressive power and computational complexity are still
an open question. However, for speciﬁc classes of tasks this full universality may not be needed.
Here, we study such an example of practical interest: counting substructures, as proposed in (Chen
et al., 2020). In particular, we study if it is possible to count given substructures with a GNN whose
complexity is between that of MPNNs and existing higher-order GNNs."
INTRODUCTION,0.021791767554479417,"To this end, we study a generic scheme followed by many GNN architectures, including MPNNs
and higher-order GNNs (Morris et al., 2019; Chen et al., 2020): select a collection of subgraphs
of the input graph, encode these, and apply an aggregation function on this collection. First, we
study the power of pooling by itself, as a multi-set function over node features. We prove that k
recursive applications on each node’s neighborhood allow to count any substructure of size k. This
is in contrast to iterative MPNNs. We call this technique Recursive neighborhood pooling (RNP).
While subgraph pooling relates to the graph reconstruction conjecture, our strategy has important
differences. In particular, we show how the aggregation “augments” local encodings, if they play
together and the subgraphs are selected appropriately, and this reasoning may be of interest for
the design of other, even partially, expressive architectures. Moreover, our results show that the
complexity is adjustable to the counting task of interest and the sparsity of the graph."
INTRODUCTION,0.024213075060532687,"The strategy of pooling subgraph encodings has previously been used for counting in Local Rela-
tional Pooling (LRP) (Chen et al., 2020). LRP relies on an isomorphic encoding of subgraphs, which
is expensive – e.g., the relational pooling it uses requires O(k!) time for a subgraph of size k. Other
higher-order GNNs would be expensive, too, as high orders are needed for complete isomorphism
power. A major difference to our RNP is that our recursion uses subgraphs of varying sizes and
structures, many of them much smaller – adapted to the graph structure and speciﬁc counting task."
INTRODUCTION,0.026634382566585957,"Furthermore, we study lower bounds on GNNs that count motifs. We show an information-theoretic
lower bound on the number of subgraphs to encode, as a function of an encoding complexity. We
also transfer computational lower bounds that apply to any counting GNN. The lower bounds show
that the recursive pooling is close to tight."
INTRODUCTION,0.029055690072639227,"In short, in this paper, we make the following contributions:"
INTRODUCTION,0.031476997578692496,"• We study the power of pooling encodings of subgraphs, and show that pooling, as an injective
multi-set function, is sufﬁcient by itself for counting when applied recursively on appropriate
subgraphs, remarkably without relying on other encoding techniques or node IDs. This is different
from any other strategy we are aware of in the literature.
• We analyze the complexity of recursive pooling, as a function of the task and input graph.
• We provide complexity lower bounds for pooling and general GNN architectures that count motifs.
For instance, we show a lower bound on the number of subgraphs that need to be encoded."
BACKGROUND,0.03389830508474576,"2
BACKGROUND"
BACKGROUND,0.03631961259079903,"Message Passing Graph Neural Networks.
Let G = (V, E, X) be an attributed graph with
|V| = n nodes. Here, Xv ∈X denotes the initial attribute of v ∈V, where X ⊆N is a (countable)
domain.A typical Message Passing Graph Neural Network (MPNN) ﬁrst computes a representation
of each node, and then aggregates the node representations via a readout function into a representa-
tion of the entire graph G. The representation h(i)
v
of each node v ∈V is computed iteratively by
aggregating the representations h(i−1)
u
of the neighboring vertices u:"
BACKGROUND,0.0387409200968523,"m(i)
v
= AGGREGATE(i)
{{h(i−1)
u
: u ∈N(v)}}

,
h(i)
v
= COMBINE(i)
h(i−1)
v
, m(i)
v

,
(1)"
BACKGROUND,0.04116222760290557,"for any v ∈V, for k iterations, and with h(0)
v
= Xv. The AGGREGATE/COMBINE functions are
parametrized, and {{.}} denotes a multi-set, i.e., a set with (possibly) repeating elements. A graph-
level representation can be computed as hG = READOUT
 
{{h(k)
v
: v ∈V}}

, where READOUT
is a learnable aggregation function. For representational power, it is important that the learnable
functions are injective (Xu et al., 2019)."
BACKGROUND,0.043583535108958835,"Higher-Order GNNs.
To increase the representational power of GNNs, several higher-order
GNNs have been proposed. In k-GNN, message passing is applied to k−tuples of nodes, inspired"
BACKGROUND,0.04600484261501211,Under review as a conference paper at ICLR 2022
BACKGROUND,0.048426150121065374,"by k-WL (Morris et al., 2019). At initialization, each k-tuple is labeled such that two k-tuples are
labeled differently if their induced subgraphs have different isomorphism types (Maron et al., 2019a;
Cai et al., 1992). As a result, k-GNNs can count (induced) substructures with at most k vertices even
at initialization. Another class of higher-order networks applies (linear) equivariant operations, in-
terleaved with coordinate-wise nonlinearities, to order-k tensors consisting of the adjacency matrix
and input node attributes (Maron et al., 2018; 2019a;b). These GNNs are at least as powerful as
k−GNNs, and hence they too can count substructures with at most k vertices. All these methods
need Ω(nk) operations. Local Relational Pooling (LRP) (Chen et al., 2020) was designed for count-
ing and applies relational pooling (Murphy et al., 2019b;a) on local neighborhoods, i.e., one pools
over evaluations of a permutation-sensitive function applied to all k! permutations of the nodes in a
k-size neighborhood of each node."
OTHER RELATED WORKS,0.05084745762711865,"3
OTHER RELATED WORKS"
OTHER RELATED WORKS,0.053268765133171914,"Expressive power. Several other works have studied the expressive power of GNNs as function
approximators (Azizian & Lelarge, 2020). Scarselli et al. (2009) extend universal approximation
from feedforward networks to MPNNs, using the notion of unfolding equivalence, i.e., functions on
computation trees. Indeed, graph distinction and function approximation are closely related (Chen
et al., 2019; Azizian & Lelarge, 2020; Keriven & Peyr´e, 2019). Maron et al. (2019b) and Keriven &
Peyr´e (2019) show that higher-order, tensor-based GNNs provably achieve universal approximation
of permutation-invariant functions on graphs, and Loukas (2019) analyzes expressive power under
depth and width restrictions. Studying GNNs from the perspective of local algorithms, Sato et al.
(2019) show that GNNs can approximate solutions to certain combinatorial optimization problems.
For counting substricutres, Arvind et al. (2020); F¨urer (2017) show that 2-WL, which is equivalent
to MPNNs, can count not necessarily induced cycles and paths of up to 7 vertices with O(n2)
complexity,"
OTHER RELATED WORKS,0.05569007263922518,"Subgraphs and GNNs. Having infromation about subgraphs can be quite helpful in various graph
representation algorithms (Liu et al., 2019; Monti et al., 2018; Liu et al., 2020; Yu et al., 2020; Meng
et al., 2018; Cotta et al., 2020; Alsentzer et al., 2020; Huang & Zitnik, 2020). For example, for graph
comparison (i.e., testing whether a given (possibly large) subgraph exists in the given model), Ying
et al. (2020) compare the outputs of GNNs for small subgraphs of the two graphs. To improve the
expressive power of GNNs, Bouritsas et al. (2020) use features that are counts of speciﬁc subgraphs
of interest. Another example is (Vignac et al., 2020), where an MPNN is strengthened by learning
local context matrices around vertices. Recent works have also developed GNNs that pass messages
on ego-nets (You et al., 2021; Sandfelder et al., 2021). With motivation from the reconstruction
conjecture, Cotta et al. (2021) process node-deleted subgraphs with individual MPNNs, and then
pool them with a DeepSets model to get a representation of the original graph."
RECURSIVE NEIGHBORHOOD POOLING,0.05811138014527845,"4
RECURSIVE NEIGHBORHOOD POOLING"
RECURSIVE NEIGHBORHOOD POOLING,0.06053268765133172,"Let G = (V, E, X) be an attributed input graph with |V| = n nodes, and let h(0)
v
= Xv be the initial
representation of each node v. In this work, we study architectures that ﬁrst ﬁnd representations of a
collection of m subgraphs Gi and then aggregate (pool) over these representations with a multi-set
function, i.e.,
AGGREGATE({{ψ(Gi) : i ∈[m]}}),
[m] = {1, . . . , m}.
(2)"
RECURSIVE NEIGHBORHOOD POOLING,0.06295399515738499,"It is clear that if the ψ can count a subgraph structure H, then the entire model can count H. In
particular, we aim to apply this strategy to obtain the representations ψ, too. To appreciate the
challenges in doing so, recall two such examples. First, MPNNs follow this pooling strategy, by
iteratively aggregating over node neighborhoods, and then aggregating all node representations into
a graph representation. However, it is known that MPNNs can count at most star structures or edges
(Arvind et al., 2020). This is because they represent a local computation tree, which loses structural
information about node identities and connectivity. Second, this strategy is at the heart of the Graph
Reconstruction Conjecture (Kelly, 1957), which conjectures that a graph G can be reconstructed
from its subgraphs {{Gv = G \ {v} : v ∈V (G)}} (Appendix E). This, however, is unknown for
general graphs. Although the Gv retain some structure, we lose information about their structural
relationship. In summary, encoding structural information is the key question."
RECURSIVE NEIGHBORHOOD POOLING,0.06537530266343826,"Under review as a conference paper at ICLR 2022 u1
u2 u3 N2(v)"
RECURSIVE NEIGHBORHOOD POOLING,0.06779661016949153,"(N2(v) \ {v}) ∩N2(u1) u11 v
v"
RECURSIVE NEIGHBORHOOD POOLING,0.07021791767554479,"Figure 1: Illustration of a Recursive Neigh-
borhood Pooling GNN (RNP-GNN) with re-
cursion parameters (2, 2, 1). To compute the
representation of node v in the given input
graph (depicted in the top left of the ﬁgure),
we ﬁrst recurse on G(N2(v)\{v}) (top right
of ﬁgure). To do so, we ﬁnd the represen-
tation of each node u ∈G(N2(v) \ {v}).
For instance, to compute the representation
of u1, we apply an RNP-GNN with recursion
parameters (2, 1) and aggregate G((N2(v) \
{v}) ∩(N2(u1) \ {u1})), which is shown in
the bottom left of the ﬁgure. To do so, we
recursively apply an RNP-GNN with recur-
sion parameter (1) on G((N2(v) \ {v}) ∩
(N2(u1) \ {u1}) ∩(N1(u11) \ {u11})), in
the bottom right of the ﬁgure."
RECURSIVE NEIGHBORHOOD POOLING,0.07263922518159806,"Hence, to represent the counting function ψ over potentially large neighborhoods, an MPNN does
not sufﬁce. But, aggregation over input node attributes, along with edge information, can count edge
types, i.e., tiny subgraphs. Hence, we recursively apply aggregation on smaller sub-neighborhoods
while remembering structural information, with node-wise aggregation as the base case. For intu-
ition, suppose we aim to count the occurrence of subgraph H in the r1-neighborhood Nr1(v) of
a node v. To do so, we may count Hv = H \ {v} in the smaller graph Nr1(v) \ {v}. But, to
combine these counts with the presence of v to “complete” H, we must know how the Hv are con-
nected to v in the screened graph. Hence, to retain structure information, we mark the neighbors of
v. We then recursively call neighborhood pooling to process smaller neighborhoods Nr2(u) within
Nr1(v) \ {v}. This could, e.g., learn to count marked versions of Hv. The radii r of neighborhoods
may differ in recursive calls. In Section 5, we relate their size to H."
RECURSIVE NEIGHBORHOOD POOLING,0.07506053268765134,"Recursive neighborhood pooling RNP-GNN(G, {hin
u}u∈V(G), (r1, . . . , rτ)) takes an attributed
graph along with a sequence of neighborhood radii for different recursions, and returns a set of
node encodings {hv}v∈V(G). For any v ∈G, RNP-GNN ﬁrst constructs v’s neighborhood, removes
v and marks its neighbors1:"
RECURSIVE NEIGHBORHOOD POOLING,0.0774818401937046,"Gv ←Nr1(v) \ {v},
hin
u,aug = (hin
u, 1[(u, v) ∈E(Gv)]).
(3)"
RECURSIVE NEIGHBORHOOD POOLING,0.07990314769975787,"Here, we extracted r1−neighborhood of v, removed the central node v, and then added the new
structural information to 1−neighbors of v. Then we aggregate over subgraph representations, i.e.,
the representations of Gv, v ∈[n] when they considered as a set graphs. If τ = 1 (base case), we
use the input features:"
RECURSIVE NEIGHBORHOOD POOLING,0.08232445520581114,"hv ←AGGREGATE(τ)(hin
v , {{hin
u,aug : u ∈Gv}}).
(4)"
RECURSIVE NEIGHBORHOOD POOLING,0.0847457627118644,"In other words, in this case we only combine hidden representations in each Gv without any itera-
tion/recursion. If τ > 1, we recursively represent neighborhoods of nodes in Gv:"
RECURSIVE NEIGHBORHOOD POOLING,0.08716707021791767,"{ˆhv,u}u∈G′ ←RNP-GNN
 
Gv, {hin
u,aug}u∈Gv, (r2, r3, . . . , rτ)

(5)"
RECURSIVE NEIGHBORHOOD POOLING,0.08958837772397095,"hv ←AGGREGATE(τ)
hin
v , {{ˆhu,v : u ∈Gv}}

.
(6)"
RECURSIVE NEIGHBORHOOD POOLING,0.09200968523002422,"Here, we ﬁrst apply an RNP-GNN on each Gv to update the representations of nodes for each node
in Gv. Then, we combine those representations to ﬁnd the representation of v. The fact that Gv has
less nodes than G allows to deﬁne the algorithm in an inductive way; this ensures the algorithm does
not have any logical loop. For aggregation, we can use, e.g., the injective multi-set function from
(Xu et al., 2019):"
RECURSIVE NEIGHBORHOOD POOLING,0.09443099273607748,"AGGREGATE(τ)
hv, {hu}u∈Gv

= MLP(τ)
(1 + ϵ)hv +
X"
RECURSIVE NEIGHBORHOOD POOLING,0.09685230024213075,"u∈Gv
ˆhu

,
(7)"
RECURSIVE NEIGHBORHOOD POOLING,0.09927360774818401,"11[.] is one when the condition is satisﬁed, and otherwise is zero."
RECURSIVE NEIGHBORHOOD POOLING,0.1016949152542373,Under review as a conference paper at ICLR 2022
RECURSIVE NEIGHBORHOOD POOLING,0.10411622276029056,"where we use MLP modules with ReLU activation, and ϵ is an arbitrary irrational constant (Xu et al.,
2019). The ﬁnal readout aggregates over the ﬁnal node representations of the entire graph. Figure 1
illustrates an RNP-GNN with recursion parameters (2, 2, 1), and Appendix F provides pseudocode.
One can optionally do message passing iterations to the representations hv, v ∈[n] to make the
model more expressive; though our results hold even without those iterations."
RECURSIVE NEIGHBORHOOD POOLING,0.10653753026634383,"While MPNNs also encode a representation of a local neighborhood, the recursive representations
differ as they take into account intersections of neighborhoods. As a result, as we will see in Sec-
tion 5, they retain more structural information and are more expressive than MPNNs; one can take
r1 = 1 to simulate one iteration of MPNNs (and then iterate)."
RECURSIVE NEIGHBORHOOD POOLING,0.1089588377723971,"Models like k-GNN and LRP also compute encodings of subgraphs, and then update the resulting
representations via message passing. We can do the same with the neighborhood representations
computed by RNP-GNNs to encode more global information, although our representation results in
Section 5 hold even without that."
EXPRESSIVE POWER OF RECURSIVE POOLING,0.11138014527845036,"5
EXPRESSIVE POWER OF RECURSIVE POOLING"
EXPRESSIVE POWER OF RECURSIVE POOLING,0.11380145278450363,"In this section, we analyze the expressive power of RNP-GNNs. We notice that for full expres-
siveness, we indeed need the identiﬁers in Eq. (3); their addition is an important insight. Still,
RNP-GNN does maintain some expressive power without the augmented identiﬁers. For instance,
consider the graphs G1 = two triangles and G2 = the 6-cycle, which are 1-WL equivalent and thus
cannot be distinguished by MPNNs. An RNP-GNN with r1 = 1 can distinguish these without the
augmented identiﬁers. This is because on the ﬁrst recursion level, the 1-neighborhoods of G1 are
two nodes with an edge between them, and the 1-neighborhoods of G2 consist of two isolated nodes."
EXPRESSIVE POWER OF RECURSIVE POOLING,0.1162227602905569,"5.1
COUNTING (INDUCED) SUBSTRUCTURES"
EXPRESSIVE POWER OF RECURSIVE POOLING,0.11864406779661017,"In contrast to MPNNs, which in general cannot count substructures of three vertices or more (Chen
et al., 2020), in this section we prove that for any set of substructures, there is an RNP-GNN that
provably counts them. We begin with a few deﬁnitions."
EXPRESSIVE POWER OF RECURSIVE POOLING,0.12106537530266344,"Deﬁnition 1. Let G, H be arbitrary, potentially attributed simple graphs, where V is the set of nodes
in G. Also, for any S ⊆V, let G(S) denote the subgraph of G induced by S. The induced subgraph
count function is deﬁned as"
EXPRESSIVE POWER OF RECURSIVE POOLING,0.1234866828087167,"C(G; H) :=
X"
EXPRESSIVE POWER OF RECURSIVE POOLING,0.12590799031476999,"S⊆V 1{G(S) ∼= H},
(8)"
EXPRESSIVE POWER OF RECURSIVE POOLING,0.12832929782082325,"i.e., the number of subgraphs of G isomorphic to H."
EXPRESSIVE POWER OF RECURSIVE POOLING,0.13075060532687652,"To relate the size of encoded neighborhoods to the substructure H, we will need a notion of covering
sequences for graphs."
EXPRESSIVE POWER OF RECURSIVE POOLING,0.13317191283292978,"Deﬁnition 2. Let H = (VH, EH) be a simple connected graph. For any S ⊆VH and v ∈VH,
deﬁne the covering distance of v from S as"
EXPRESSIVE POWER OF RECURSIVE POOLING,0.13559322033898305,"¯dH(v; S) := max
u∈S d(u, v),
(9)"
EXPRESSIVE POWER OF RECURSIVE POOLING,0.13801452784503632,"where d(., .) is the shortest-path distance in H."
EXPRESSIVE POWER OF RECURSIVE POOLING,0.14043583535108958,"Deﬁnition 3. Let H be a simple connected graph on τ + 1 vertices. A permutation of vertices,
such as (v1, v2, . . . , vτ+1), is called a vertex covering sequence with respect to a sequence r =
(r1, r2, . . . , rτ) ∈Nτ called a covering sequence, if and only if"
EXPRESSIVE POWER OF RECURSIVE POOLING,0.14285714285714285,"¯dH′
i(vi; Si) ≤ri,
(10)"
EXPRESSIVE POWER OF RECURSIVE POOLING,0.14527845036319612,"for any i ∈[τ + 1] = {1, 2, . . . , τ + 1}, where Si = {vi, vi+1 . . . , vτ+1} and H′
i = H(Si) is the
subgraph of H induced by the set of vertices Si. We also say that H admits the covering sequence
r = (r1, r2, . . . , rτ) ∈Nτ if there is a vertex covering sequence for H with respect to r."
EXPRESSIVE POWER OF RECURSIVE POOLING,0.14769975786924938,"In particular, in a covering sequence we ﬁrst consider the whole graph as a local neighborhood of
one of its nodes with radius r1. Then, we remove that node and compute the covering sequence of"
EXPRESSIVE POWER OF RECURSIVE POOLING,0.15012106537530268,Under review as a conference paper at ICLR 2022
EXPRESSIVE POWER OF RECURSIVE POOLING,0.15254237288135594,"the remaining graph. Figure 4 shows an example of covering sequence computation. An important
property, which holds by deﬁnition, is that if r is a covering sequence for H, then any r′ ≥r
(coordinate-wise) is also a covering sequence for H."
EXPRESSIVE POWER OF RECURSIVE POOLING,0.1549636803874092,"Note that any connected graph on k nodes admits at least one covering sequence, which is (k −
1, k −2, . . . , 1). To observe this fact, note that in a connected graph, there is at least one node that
can be removed and the remaining graph still remains connected. Therefore, we may take this node
as the ﬁrst element of a vertex covering sequence, and inductively ﬁnd the other elements. Since the
diameter of a connected graph with k vertices is always bounded by k −1, we achieve the desired
result. However, we will see in the next section that, when using covering sequences to identify
sufﬁciently powerful RNP-GNNs, it is desirable to have covering sequences with low r1, since the
complexity of the resulting RNP-GNN depends on r1."
EXPRESSIVE POWER OF RECURSIVE POOLING,0.15738498789346247,"More generally, if H1 and H2 are (possibly attributed) simple graphs on k nodes and H1 ⋐H2, i.e.,
H1 is a subgraph of H2 (not necessarily induced subgraph), then it follows from the deﬁnition that
any covering sequence for H1 is also a covering sequence for H2. As a side remark, as illustrated in
Figure 2, covering sequences need not always to be decreasing. Using covering sequences, we can
show the following result.
Theorem 1. Consider a set of (possibly attributed) graphs H on τ +1 vertices, such that any H ∈H
admits the covering sequence (r1, r2, . . . , rτ). Then, there is an RNP-GNN f(·; θ) with recursion
parameters (r1, r2, . . . , rτ) that can count any H ∈H. In other words, for any two graphs G1, G2,
if there exists H ∈H such that C(G1; H) ̸= C(G2; H), then f(G1; θ) ̸= f(G2; θ). The same result
also holds for the non-induced subgraph count function."
EXPRESSIVE POWER OF RECURSIVE POOLING,0.15980629539951574,"Theorem 1 states that, with appropriate recursion parameters, any set of (possibly attributed) sub-
structures can be counted by an RNP-GNN. Interestingly, induced and non-induced subgraphs can
be both counted in RNP-GNNs2. Also, for a given covering sequence, we can simultaneously count
a set of substructures that each admit it. This means that one RNP-GNN is able to potentially count
many substructures. We prove Theorem 1 in Appendix A.2. The main idea is to show that we
can implement the intuition for recursive pooling outlined in Section 4 formally with the proposed
architecture and multiset functions3."
EXPRESSIVE POWER OF RECURSIVE POOLING,0.162227602905569,"The theorem holds for any covering sequence that is valid for all graphs in H. For any graph, one
can compute a covering sequence by computing a spanning tree, and sequentially pruning the leaves
of the tree. The resulting sequence of nodes is a vertex covering sequence, and the corresponding
covering sequence can be obtained from the tree too (Appendix D). A valid covering sequence for
all the graphs in H is the coordinate-wise maximum of all these sequences."
EXPRESSIVE POWER OF RECURSIVE POOLING,0.16464891041162227,"For large substructures, the sequence (r1, r2, . . . , rτ) can be long or include large numbers, and this
will affect the computational complexity of RNP-GNNs. For small, e.g., constant-size substructures,
the recursion parameters are also small (i.e., ri = O(1) for all i), raising the hope to count these
structures efﬁciently. In particular, r1 is an important parameter. In Section 5.3, we analyze the
complexity of RNP-GNNs in more detail."
A UNIVERSAL APPROXIMATION RESULT FOR LOCAL FUNCTIONS,0.16707021791767554,"5.2
A UNIVERSAL APPROXIMATION RESULT FOR LOCAL FUNCTIONS"
A UNIVERSAL APPROXIMATION RESULT FOR LOCAL FUNCTIONS,0.1694915254237288,"Theorem 1 shows that RNP-GNNs can count substructures if their recursion parameters are chosen
carefully. Next, we provide a universal approximation result, which shows that they can represent
any function related to local neighborhoods or small subgraphs in a graph."
A UNIVERSAL APPROXIMATION RESULT FOR LOCAL FUNCTIONS,0.17191283292978207,"First, we recall that for a graph G, G(S) denotes the subgraph of G induced by the set of nodes S.
Deﬁnition 4. A function ℓ: Gn →Rd is called an r−local graph function if"
A UNIVERSAL APPROXIMATION RESULT FOR LOCAL FUNCTIONS,0.17433414043583534,"ℓ(G) = φ({{ψ(G(S)) : S ⊆V, |S| ≤r}}),
(11)"
A UNIVERSAL APPROXIMATION RESULT FOR LOCAL FUNCTIONS,0.17675544794188863,"where ψ : Gr →Rd′ is a function on graphs, φ is a multi-set function, and V denotes the set of all
nodes."
A UNIVERSAL APPROXIMATION RESULT FOR LOCAL FUNCTIONS,0.1791767554479419,"2For simplicity, we assume that H only contains τ + 1 node graphs. If H includes graphs with strictly less
than τ + 1 vertices, we can simply append a sufﬁcient number of zeros to their covering sequences.
3One can also generalize this theorem to wider classes of graphs; see Remark 1. However, here in this paper
we focus on a special class of graph with covering sequences to keep the results simple and insightful"
A UNIVERSAL APPROXIMATION RESULT FOR LOCAL FUNCTIONS,0.18159806295399517,Under review as a conference paper at ICLR 2022
A UNIVERSAL APPROXIMATION RESULT FOR LOCAL FUNCTIONS,0.18401937046004843,"In other words, a local function only depends on small substructures.
Theorem 2. For any r−local graph function ℓ(.), there exists an RNP-GNN f(.; θ) with recursion
parameters (r −1, r −2, . . . , 1) such that f(G; θ) = ℓ(G) for any G ∈Gn."
A UNIVERSAL APPROXIMATION RESULT FOR LOCAL FUNCTIONS,0.1864406779661017,"To prove the theorem, we use speciﬁc aggregation functions, but since MLPs with ReLU activation
are universal approximators, we can approximate those aggregations and ﬁnd an RNP-GNN f(G; θ)
implemented by MLPs such that |f(G; θ) −ℓ(G)| < ϵ, for arbitrary small ϵ. As a result, we can
provably approximate all the local information in a graph with an appropriate RNP-GNN. Note that
we still need recursions, because the function ψ(.) may be an arbitrarily difﬁcult graph function.
However, to achieve the full generality of such a universal approximation result, we need to consider
large recursion parameters (r1 = r −1) and injective aggregations in the RNP-GNN network. For
universal approximation, we may also need high dimensions if fully connected layers are used for
aggregation (see the proof in Appendix B for more details)."
A UNIVERSAL APPROXIMATION RESULT FOR LOCAL FUNCTIONS,0.18886198547215496,"As a remark, for r = n, achieving universal approximation on graphs implies solving the graph
isomorphism problem. But, in this extreme case, the computational complexity of RNP is in general
not polynomial in n."
COMPUTATIONAL COMPLEXITY,0.19128329297820823,"5.3
COMPUTATIONAL COMPLEXITY v1"
COMPUTATIONAL COMPLEXITY,0.1937046004842615,"v2
v3
v4
v5
v6"
COMPUTATIONAL COMPLEXITY,0.19612590799031476,"Figure
2:
For
the
above
graph,
(v1, v2, . . . , v6) is a vertex covering se-
quence. The corresponding covering se-
quence (1, 4, 3, 2, 1) is not decreasing."
COMPUTATIONAL COMPLEXITY,0.19854721549636803,"The computational complexity of RNP-GNNs is graph-
dependent. For instance, we need to compute the set of
local neighborhoods, which is cheaper for sparse graphs.
Moreover, in the recursions, we use intersections of
neighborhoods which become smaller and sparser.
Theorem 3. Let f(.; θ) : Gn
→Rd be an RNP-
GNN with recursion parameters (r1, r2, . . . , rτ).
As-
sume that the observed graphs G1, G2, . . ., whose rep-
resentations we compute, satisfy the following property:
maxv∈[n] |Nr1(v)| ≤c, for a constant c. Then the num-
ber of node updates in the RNP-GNN is O(ncτ)."
COMPUTATIONAL COMPLEXITY,0.2009685230024213,"In other words, if c = no(1) and τ = O(1), then RNP-
GNN requires relatively few updates (that is, n1+o(1)).
If the maximum degree of the given graphs is ∆, then c = O(r1∆r1). Therefore, similarly, if
∆= no(1) then we can count with at most n1+o(1) updates. Additional gains may arise from rapidly
shrinking neighborhoods, which are not yet accounted for in Theorem 3. To put this in context, the
higher-order GNNs based on tensors or k-WL would operate on tensors of order nτ+1."
COMPUTATIONAL COMPLEXITY,0.2033898305084746,"Table 1: Time complexity of various models.
∆is the max-
degree, and ’−’ means the complexity is not polynomial in n."
COMPUTATIONAL COMPLEXITY,0.20581113801452786,"Model
worst-case
∆= no(1)
∆= O(log(n))
∆= O(1)"
COMPUTATIONAL COMPLEXITY,0.20823244552058112,"LRP
−
−
−
O(n)
k−WL
nk
nk
nk
nk"
COMPUTATIONAL COMPLEXITY,0.2106537530266344,"RNP
nk
n1+o(1)
˜O(n)
O(n)"
COMPUTATIONAL COMPLEXITY,0.21307506053268765,"The above results show that when us-
ing RNP-GNNs with sparse graphs,
we can represent functions of sub-
structures with k nodes without re-
quiring k−order tensors. LRPs also
encode neighborhoods of distance r1
around nodes. In particular, all c! per-
mutations of the nodes in a neighbor-
hood of size c are considered to ob-
tain the representation.
As a result, LRP networks only have polynomial complexity if c =
o(log(n)). Thus, RNP-GNNs can provide an exponential improvement in terms of the tolerable
size c of neighborhoods with distance r1 in the graph."
COMPUTATIONAL COMPLEXITY,0.21549636803874092,"Moreover, Theorem 3 suggests to aim for small r1. The other ri’s may be larger than r1, as shown
in Figure 2, but do not affect the upper bound on the complexity."
AN INFORMATION-THEORETIC LOWER BOUND,0.2179176755447942,"6
AN INFORMATION-THEORETIC LOWER BOUND"
AN INFORMATION-THEORETIC LOWER BOUND,0.22033898305084745,"In this section, we provide a general information-theoretic lower bound for graph representations
that encode a given graph G by ﬁrst encoding a number of (possibly small) graphs G1, G2, . . . , Gt"
AN INFORMATION-THEORETIC LOWER BOUND,0.22276029055690072,Under review as a conference paper at ICLR 2022
AN INFORMATION-THEORETIC LOWER BOUND,0.22518159806295399,"and then aggregating the resulting representations. The sequence of graphs G1, G2, . . . , Gt may be
obtained in an arbitrary way from G. For example, in an MPNN, Gi can be the computation tree
(rooted tree) at node i. As another example, in LRP, Gi is the local neighborhood around node i."
AN INFORMATION-THEORETIC LOWER BOUND,0.22760290556900725,"Formally, consider a graph representation f(.; θ) : Gn →Rd as"
AN INFORMATION-THEORETIC LOWER BOUND,0.23002421307506055,"f(G; θ) = AGGREGATE({{ψ(Gi) : i ∈[t]}}),
[t] = {1, . . . , t}
(12)"
AN INFORMATION-THEORETIC LOWER BOUND,0.2324455205811138,"for any G ∈Gn, where AGGREGATE is a multi-set function, (G1, G2, . . . , Gt) = Ξ(G) where
Ξ(.) : Gn →
  S∞
m=1 Gm
t is a function from one graph to t graphs, and ψ : S∞
m=1 Gm →[s] is a
function on graphs taking s values. In short, we encode t graphs, and each encoding takes one of s
values. We call this graph representation function an (s, t)-good graph representation."
AN INFORMATION-THEORETIC LOWER BOUND,0.23486682808716708,"Theorem 4. Consider a parametrized class of (s, t)−good representations f(.; θ) : Gn →Rd
that is able to count any (not necessarily induced4) substructure with k vertices. More precisely,
for any graph H with k vertices, there exists f(.; θ) such that if C(G1; H) ̸= C(G2; H), then
f(G1; θ) ̸= f(G2; θ). Then5 t = ˜Ω(n
k
s−1 )."
AN INFORMATION-THEORETIC LOWER BOUND,0.23728813559322035,"In particular, for any (s, t)−good graph representation with s = 2, i.e., binary encoding functions,
we need ˜Ω(nk) encoded graphs. This implies that, for s = 2, enumerating all subgraphs and
deciding for each whether it equals H is near optimal. Moreover, if s ≤k, then t = Θ(n) small
graphs would not sufﬁce to enable counting."
AN INFORMATION-THEORETIC LOWER BOUND,0.2397094430992736,"More interestingly, if k, s = O(1), then it is impossible to perform the substructure counting task
with t = O(log(n)). As a result, in this case, considering n encoded graphs (as is done in GNNs or
LRP networks) cannot be exponentially improved."
AN INFORMATION-THEORETIC LOWER BOUND,0.24213075060532688,"The lower bound in this section is information-theoretic and hence applies to any algorithm. It may
be possible to strengthen it by considering computational complexity, too. For binary encodings, i.e.,
s = 2, however, we know that the bound cannot be improved since manual counting of subgraphs
matches the lower bound."
TIME COMPLEXITY LOWER BOUNDS FOR COUNTING SUBGRAPHS,0.24455205811138014,"7
TIME COMPLEXITY LOWER BOUNDS FOR COUNTING SUBGRAPHS"
TIME COMPLEXITY LOWER BOUNDS FOR COUNTING SUBGRAPHS,0.2469733656174334,"In this section, we put our results in the context of known hardness results for subgraph counting.
In general, the subgraph isomorphism problem is known to be NP-complete. Going further, the
Exponential Time Hypothesis (ETH) is a conjecture in complexity theory (Impagliazzo & Paturi,
2001), and states that several NP-complete problems cannot be solved in sub-exponential time.
ETH, as a stronger version of the P ̸= NP problem, is widely believed to hold. Assuming that
ETH holds, the k−clique detection problem requires at least nΩ(k) time (Chen et al., 2005). This
means that if a graph representation can count any subgraph H of size k, then computing it requires
at least nΩ(k) time."
TIME COMPLEXITY LOWER BOUNDS FOR COUNTING SUBGRAPHS,0.24939467312348668,"Corollary 1. Assuming the ETH conjecture holds, any graph representation that can count any
substructure H on k vertices with appropriate parametrization needs nΩ(k) time to compute."
TIME COMPLEXITY LOWER BOUNDS FOR COUNTING SUBGRAPHS,0.25181598062953997,"The above bound matches the O(nk) complexity of the higher-order GNNs. Comparing with The-
orem 4 above, Corollary 1 is more general, while Theorem 4 has fewer assumptions and offers a
reﬁned result for aggregation-based graph representations."
TIME COMPLEXITY LOWER BOUNDS FOR COUNTING SUBGRAPHS,0.2542372881355932,"Given that Corollary 1 is a worst-case bound, a natural question is whether we can do better for
subclasses of graphs. Regarding H, even if H is a random Erd¨os-R´enyi graph, it can only be
counted in nΩ(k/ log k) time (Dalirrooyfard et al., 2019)."
TIME COMPLEXITY LOWER BOUNDS FOR COUNTING SUBGRAPHS,0.2566585956416465,"Regarding the input graph in which we count, consider two classes of sparse graphs: strongly sparse
graphs have maximum degree ∆= O(1), and weakly sparse graphs have average degree ¯∆= O(1).
We argued in Theorem 3 that RNP-GNNs achieve almost linear complexity for the class of strongly
sparse graphs. For weakly sparse graphs, in contrast, the complexity of RNP-GNNs is generally not
linear, but still polynomial, and can be much better than O(nk). One may ask whether it is possible"
TIME COMPLEXITY LOWER BOUNDS FOR COUNTING SUBGRAPHS,0.25907990314769974,"4The theorem also holds for induced subgraphs, with/without node attributes.
5 ˜Ω(m) is Ω(m) up to poly-logarithmic factors."
TIME COMPLEXITY LOWER BOUNDS FOR COUNTING SUBGRAPHS,0.26150121065375304,Under review as a conference paper at ICLR 2022
TIME COMPLEXITY LOWER BOUNDS FOR COUNTING SUBGRAPHS,0.2639225181598063,"to achieve a learnable graph representation such that its complexity for weakly sparse graphs is still
linear. Recent results in complexity theory imply that this is impossible:"
TIME COMPLEXITY LOWER BOUNDS FOR COUNTING SUBGRAPHS,0.26634382566585957,"Corollary 2 (Gishboliner et al. (2020); Bera et al. (2019; 2020)). There is no graph representation
algorithm that runs in linear time on weakly sparse graphs and is able to count any substructure H
on k vertices (with appropriate parametrization)."
TIME COMPLEXITY LOWER BOUNDS FOR COUNTING SUBGRAPHS,0.2687651331719128,"Hence, RNP-GNNs are close to optimal for several cases of counting substructures with
parametrized learnable functions."
EXPERIMENTS,0.2711864406779661,"8
EXPERIMENTS"
EXPERIMENTS,0.2736077481840194,"In this section, we validate our theoretical ﬁndings via numerical experiments. Here, we brieﬂy
describe our experimental setup and results — further experimental details are given in Appendix H."
EXPERIMENTS,0.27602905569007263,"Table 2: Numerical results for counting induced triangles
and non-induced 3-stars, following the setup of Chen et al.
(2020). We report the test MSE divided by variance of the
true counts of each substructure (lower is better). The best
three models for each task are bolded."
EXPERIMENTS,0.2784503631961259,"Erd˝os-Renyi
Random Regular"
EXPERIMENTS,0.28087167070217917,"triangle
3-star
triangle
3-star"
EXPERIMENTS,0.28329297820823246,"GCN
6.78E-1
4.36E-1
1.82
2.63
GIN
1.23E-1
1.62E-4
4.70E-1
3.73E-4
GraphSAGE
1.31E-1
2.40E-10
3.62E-1
8.70E-8
sGNN
9.25E-2
2.36E-3
3.92E-1
2.37E-2
2-IGN
9.83E-2
5.40E-4
2.62E-1
1.19E-2
PPGN
5.08E-8
4.00E-5
1.40E-6
8.49E-5
LRP-1-3
1.56E-4
2.17E-5
2.47E-4
1.88E-6
Deep LRP-1-3
2.81E-5
1.12E-5
1.30E-6
2.07E-6
RNP-GNN
1.39E-5
1.39E-5
2.38E-6
1.50E-4"
EXPERIMENTS,0.2857142857142857,"Table 3: Test accuracy on the EXP dataset
with setup as in Abboud et al. (2021). Re-
sults for baselines taken from Abboud et al.
(2021).
∗Reported PPGN performance dif-
fers in other work (Balcilar et al., 2021)."
EXPERIMENTS,0.288135593220339,"Model
Accuracy (%)"
EXPERIMENTS,0.29055690072639223,"GCN-RNI
98.0 ± 1.85
PPGN∗
50.0
1-2-3-GCN-L
50.0
3-GCN
99.7 ± 0.004
RNP-GNN (r1 = 1)
50.0
RNP-GNN (r1 = 2)
99.8 ± 0.005"
EXPERIMENTS,0.2929782082324455,"Counting substructures. First, we follow the experimental setup of Chen et al. (2020) on tasks for
counting substructures. In Table 2, we report results for learning induced subgraph count of triangles
and non-induced subgraph count of 3-stars. We test on two datasets of 5000 graphs each: one of
Erd˝os-Renyi graphs and one of random regular graphs. Our RNP-GNN model is consistently within
the best performing models for these counting tasks, thus validating our theoretical results. Based on
the baseline results taken from (Chen et al., 2020), RNP-GNN tends to widely outperform MPNNs
(GCN (Kipf & Welling, 2017), GIN (Xu et al., 2019), GraphSAGE (Hamilton et al., 2017)), and
other models not tailored for counting: spectral GNN (Chen et al., 2018), and 2-IGN (Maron et al.,
2018). Also, RNP-GNN often beats higher-order GNNs: PPGN (Maron et al., 2019a) and LRP-1-
3 (Chen et al., 2020). RNP-GNN is mostly comparable to Deep LRP-1-3, though Deep LRP-1-3
outperforms it in a few cases. Recall that Deep LRP-1-3 is a practical version of LRP — we leave
further developments of practical variants of RNP-GNN to future work. The best r parameters for
RNP-GNN were: r = (1, 1, 1, 1) for triangles on Erd˝os-Renyi, r = (1, 1) for stars on Erd˝os-Renyi,
r = (1, 1) for triangles on random regular and r = (1, 1, 1) for stars on random regular."
EXPERIMENTS,0.29539951573849876,"Satisﬁability of propositional formulas. Second, we test the expressiveness of our model in dis-
tinguishing non-isomorphic graphs that 1-WL cannot distinguish. The EXP dataset of 600 graphs
(Abboud et al., 2021) for classifying whether certain propositional formulas are satisﬁable requires
higher than 1-WL expressive power to achieve better than random accuracy. As shown in Table 3,
while our RNP-GNN with r1 = 1 is unable to achieve better than random accuracy, our RNP-GNN
with r1 = 2 (we use r = (2, 1) achieves near perfect accuracy — beating all other models based on
results taken from (Abboud et al., 2021). These other models include universal models with random
node identiﬁers (GCN-RNI (Abboud et al., 2021)), GNNs with 3-WL power (PPGN (Maron et al.,
2019a)), and GNNs that imitate some (possibly weaker) version of 3-WL (1-2-3-GCN-L (Morris
et al., 2019), 3-GCN (Abboud et al., 2021)). Thus, our architecture, which is not developed within
common frameworks for achieving k-WL expressiveness, is in fact powerful at distinguishing non-
isomorphic graphs."
EXPERIMENTS,0.29782082324455206,Under review as a conference paper at ICLR 2022
REFERENCES,0.30024213075060535,REFERENCES
REFERENCES,0.3026634382566586,"Ralph Abboud, ˙Ismail ˙Ilkan Ceylan, Martin Grohe, and Thomas Lukasiewicz. The surprising power
of graph neural networks with random node initialization. In Proceedings of the Thirtieth Inter-
national Joint Conference on Artiﬁcial Intelligence, IJCAI-21, pp. 2112–2118, 8 2021."
REFERENCES,0.3050847457627119,"Emily Alsentzer, Samuel Finlayson, Michelle Li, and Marinka Zitnik. Subgraph neural networks.
Advances in Neural Information Processing Systems, 33, 2020."
REFERENCES,0.3075060532687651,"Vikraman Arvind, Frank Fuhlbr¨uck, Johannes K¨obler, and Oleg Verbitsky. On weisfeiler-leman
invariance: subgraph counts and related graph properties. Journal of Computer and System Sci-
ences, 2020."
REFERENCES,0.3099273607748184,"Wa¨ıss Azizian and Marc Lelarge. Characterizing the expressive power of invariant and equivariant
graph neural networks. arXiv preprint arXiv:2006.15646, 2020."
REFERENCES,0.31234866828087166,"Muhammet Balcilar, Pierre H´eroux, Benoit Ga¨uz`ere, Pascal Vasseur, S´ebastien Adam, and Paul
Honeine. Breaking the limits of message passing graph neural networks. In Proceedings of the
38th International Conference on Machine Learning (ICML), 2021."
REFERENCES,0.31476997578692495,"Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction networks
for learning about objects, relations and physics. In Advances in neural information processing
systems, pp. 4502–4510, 2016."
REFERENCES,0.3171912832929782,"Suman K Bera, Noujan Pashanasangi, and C Seshadhri. Linear time subgraph counting, graph
degeneracy, and the chasm at size six. arXiv preprint arXiv:1911.05896, 2019."
REFERENCES,0.3196125907990315,"Suman K. Bera, Noujan Pashanasangi, and C. Seshadhri. Near-linear time homomorphism counting
in bounded degeneracy graphs: The barrier of long induced cycles, 2020."
REFERENCES,0.3220338983050847,"Giorgos Bouritsas, Fabrizio Frasca, Stefanos Zafeiriou, and Michael M Bronstein.
Improv-
ing graph neural network expressivity via subgraph isomorphism counting.
arXiv preprint
arXiv:2006.09252, 2020."
REFERENCES,0.324455205811138,"Jin-Yi Cai, Martin F¨urer, and Neil Immerman. An optimal lower bound on the number of variables
for graph identiﬁcation. Combinatorica, 12(4):389–410, 1992."
REFERENCES,0.3268765133171913,"Jianer Chen, Benny Chor, Mike Fellows, Xiuzhen Huang, David Juedes, Iyad A Kanj, and Ge Xia.
Tight lower bounds for certain parameterized np-hard problems. Information and Computation,
201(2):216–231, 2005."
REFERENCES,0.32929782082324455,"Zhengdao Chen, Lisha Li, and Joan Bruna. Supervised community detection with line graph neural
networks. In International Conference on Learning Representations, 2018."
REFERENCES,0.33171912832929784,"Zhengdao Chen, Soledad Villar, Lei Chen, and Joan Bruna. On the equivalence between graph
isomorphism testing and function approximation with gnns. In Advances in Neural Information
Processing Systems, pp. 15894–15902, 2019."
REFERENCES,0.3341404358353511,"Zhengdao Chen, Lei Chen, Soledad Villar, and Joan Bruna.
Can graph neural networks count
substructures? arXiv preprint arXiv:2002.04025, 2020."
REFERENCES,0.3365617433414044,"Leonardo Cotta, Carlos H. C. Teixeira, Ananthram Swami, and Bruno Ribeiro.
Unsupervised
joint k-node graph representations with compositional energy-based models.
arXiv preprint
arXiv:2010.04259, 2020."
REFERENCES,0.3389830508474576,"Leonardo Cotta, Christopher Morris, and Bruno Ribeiro. Reconstruction for powerful graph repre-
sentations, 2021."
REFERENCES,0.3414043583535109,"Mina Dalirrooyfard, Thuy Duong Vuong, and Virginia Vassilevska Williams. Graph pattern detec-
tion: Hardness for all induced patterns and faster non-induced cycles. In Proceedings of the 51st
Annual ACM SIGACT Symposium on Theory of Computing, pp. 1167–1178, 2019."
REFERENCES,0.34382566585956414,"Micha¨el Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on
graphs with fast localized spectral ﬁltering. In Advances in neural information processing systems,
pp. 3844–3852, 2016."
REFERENCES,0.34624697336561744,Under review as a conference paper at ICLR 2022
REFERENCES,0.3486682808716707,"David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Al´an
Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular
ﬁngerprints. In Advances in neural information processing systems, pp. 2224–2232, 2015."
REFERENCES,0.35108958837772397,"Daniel C Elton, Zois Boukouvalas, Mark D Fuge, and Peter W Chung. Deep learning for molecular
design—a review of the state of the art. Molecular Systems Design & Engineering, 4(4):828–849,
2019."
REFERENCES,0.35351089588377727,"Paul Erdos, Alfr´ed R´enyi, et al. On the evolution of random graphs. Publ. Math. Inst. Hung. Acad.
Sci, 5(1):17–60, 1960."
REFERENCES,0.3559322033898305,"Martin F¨urer. On the combinatorial power of the weisfeiler-lehman algorithm. In International
Conference on Algorithms and Complexity, pp. 260–271. Springer, 2017."
REFERENCES,0.3583535108958838,"Vikas Garg, Stefanie Jegelka, and Tommi Jaakkola. Generalization and representational limits of
graph neural networks. In Int. Conference on Machine Learning (ICML), pp. 5204–5215. 2020."
REFERENCES,0.36077481840193704,"Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70, pp. 1263–1272, 2017."
REFERENCES,0.36319612590799033,"Lior Gishboliner, Yevgeny Levanzov, and Asaf Shapira. Counting subgraphs in degenerate graphs.
arXiv preprint arXiv:2010.05998, 2020."
REFERENCES,0.36561743341404357,"Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In Advances in neural information processing systems, pp. 1024–1034, 2017."
REFERENCES,0.36803874092009686,"Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4
(2):251–257, 1991."
REFERENCES,0.3704600484261501,"Kurt Hornik, Maxwell Stinchcombe, Halbert White, et al. Multilayer feedforward networks are
universal approximators. Neural networks, 2(5):359–366, 1989."
REFERENCES,0.3728813559322034,"Kexin Huang and Marinka Zitnik.
Graph meta learning via local subgraphs.
arXiv preprint
arXiv:2006.07889, 2020."
REFERENCES,0.37530266343825663,"Russell Impagliazzo and Ramamohan Paturi. On the complexity of k-sat. Journal of Computer and
System Sciences, 62(2):367–375, 2001."
REFERENCES,0.37772397094430993,"Wengong Jin, Kevin Yang, Regina Barzilay, and Tommi Jaakkola. Learning multimodal graph-to-
graph translation for molecule optimization. In International Conference on Learning Represen-
tations, 2018."
REFERENCES,0.3801452784503632,"Paul Kelly. A congruence theorem for trees. Paciﬁc Journal of Mathematics, 7(1):961–968, 1957."
REFERENCES,0.38256658595641646,"Nicolas Keriven and Gabriel Peyr´e. Universal invariant and equivariant graph neural networks. In
Advances in Neural Information Processing Systems (NeurIPS), pp. 7092–7101, 2019."
REFERENCES,0.38498789346246975,"Sandra Kiefer and Brendan D McKay. The iteration number of colour reﬁnement. In 47th Interna-
tional Colloquium on Automata, Languages, and Programming (ICALP 2020). Schloss Dagstuhl-
Leibniz-Zentrum f¨ur Informatik, 2020."
REFERENCES,0.387409200968523,"Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional net-
works. In International Conference on Learning Representations, 2017."
REFERENCES,0.3898305084745763,"Jon Kleinberg and Eva Tardos. Algorithm design. Pearson Education India, 2006."
REFERENCES,0.3922518159806295,"Shengchao Liu, Mehmet F Demirel, and Yingyu Liang. N-gram graph: Simple unsupervised repre-
sentation for graphs, with applications to molecules. In Advances in Neural Information Process-
ing Systems, pp. 8466–8478, 2019."
REFERENCES,0.3946731234866828,"Xin Liu, Haojie Pan, Mutian He, Yangqiu Song, Xin Jiang, and Lifeng Shang. Neural subgraph
isomorphism counting. In Proceedings of the 26th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining, pp. 1959–1969, 2020."
REFERENCES,0.39709443099273606,Under review as a conference paper at ICLR 2022
REFERENCES,0.39951573849878935,"Andreas Loukas. What graph neural networks cannot learn: depth vs width. In International Con-
ference on Learning Representations, 2019."
REFERENCES,0.4019370460048426,"Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph
networks. In International Conference on Learning Representations, 2018."
REFERENCES,0.4043583535108959,"Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph
networks. In Advances in Neural Information Processing Systems (NeurIPS), pp. 2156–2167,
2019a."
REFERENCES,0.4067796610169492,"Haggai Maron, Ethan Fetaya, Nimrod Segol, and Yaron Lipman. On the universality of invariant
networks. In International Conference on Machine Learning, pp. 4363–4371, 2019b."
REFERENCES,0.4092009685230024,"Brendan D McKay. Small graphs are reconstructible. Australasian Journal of Combinatorics, 15:
123–126, 1997."
REFERENCES,0.4116222760290557,"Changping Meng, S Chandra Mouli, Bruno Ribeiro, and Jennifer Neville. Subgraph pattern neural
networks for high-order graph evolution prediction. In AAAI, pp. 3778–3787, 2018."
REFERENCES,0.41404358353510895,"Federico Monti, Karl Otness, and Michael M Bronstein. Motifnet: a motif-based graph convolu-
tional network for directed graphs. In 2018 IEEE Data Science Workshop (DSW), pp. 225–228.
IEEE, 2018."
REFERENCES,0.41646489104116224,"Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav
Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks.
In AAAI, 2019."
REFERENCES,0.4188861985472155,"R Murphy, B Srinivasan, V Rao, and B Riberio. Janossy pooling: Learning deep permutation-
invariant functions for variable-size inputs. In International Conference on Learning Representa-
tions, 2019a."
REFERENCES,0.4213075060532688,"R Murphy, B Srinivasan, V Rao, and B Riberio. Relational pooling for graph representations. In
International Conference on Machine Learning (ICML 2019), 2019b."
REFERENCES,0.423728813559322,"Dylan Sandfelder, Priyesh Vijayan, and William L Hamilton. Ego-gnns: Exploiting ego structures
in graph neural networks. In ICASSP 2021-2021 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), pp. 8523–8527. IEEE, 2021."
REFERENCES,0.4261501210653753,"Ryoma Sato, Makoto Yamada, and Hisashi Kashima. Approximation ratios of graph neural networks
for combinatorial problems. In Advances in Neural Information Processing Systems, pp. 4081–
4090, 2019."
REFERENCES,0.42857142857142855,"F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. Computational capabilities
of graph neural networks. IEEE Transactions on Neural Networks, 20(1):81–102, 2009."
REFERENCES,0.43099273607748184,"Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.
The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61–80, 2008."
REFERENCES,0.43341404358353514,"Angelika Steger and Nicholas C Wormald. Generating random regular graphs quickly. Combina-
torics, Probability and Computing, 8(4):377–396, 1999."
REFERENCES,0.4358353510895884,"Mengying Sun, Sendong Zhao, Coryandar Gilvary, Olivier Elemento, Jiayu Zhou, and Fei Wang.
Graph convolutional networks for computational drug development and discovery. Brieﬁngs in
bioinformatics, 21(3):919–935, 2020."
REFERENCES,0.43825665859564167,"Clement Vignac, Andreas Loukas, and Pascal Frossard. Building powerful and equivariant graph
neural networks with message-passing. arXiv preprint arXiv:2006.15107, 2020."
REFERENCES,0.4406779661016949,"Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.
How powerful are graph neural
networks? In International Conference on Learning Representations, 2019."
REFERENCES,0.4430992736077482,"Rex Ying, Zhaoyu Lou, Jiaxuan You, Chengtao Wen, Arquimedes Canedo, and Jure Leskovec.
Neural subgraph matching. arXiv preprint arXiv:2007.03092, 2020."
REFERENCES,0.44552058111380144,Under review as a conference paper at ICLR 2022
REFERENCES,0.44794188861985473,"Jiaxuan You, Jonathan M Gomes-Selman, Rex Ying, and Jure Leskovec.
Identity-aware graph
neural networks. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 35,
pp. 10737–10745, 2021."
REFERENCES,0.45036319612590797,"Yue Yu, Kexin Huang, Chao Zhang, Lucas M. Glass, Jimeng Sun, and Cao Xiao. Sumgnn: Multi-
typed drug interaction prediction via efﬁcient knowledge graph summarization. arXiv preprint
arXiv:2010.01450, 2020."
REFERENCES,0.45278450363196127,"A
PROOF OF THEOREM 1"
REFERENCES,0.4552058111380145,"A.1
PRELIMINARIES"
REFERENCES,0.4576271186440678,"Let us ﬁrst state a few deﬁnitions about the graph functions. Note that for any graph function
f : Gn →Rd, we have f(G) = f(H) for any G ∼= H.
Deﬁnition 5. Given two graph functions f, g : Gn →Rd, we write f ⊒g, if and only if for any
G1, G2 ∈Gn,
∀G1, G2 ∈Gn : g(G1) ̸= g(G2) =⇒f(G1) ̸= f(G2),
(13)
or, equivalently,
∀G1, G2 ∈Gn : f(G1) = f(G2) =⇒g(G1) = g(G2).
(14)"
REFERENCES,0.4600484261501211,"Proposition 1. Consider graph functions f, g, h : Gn →Rd such that f ⊒g and g ⊒h. Then,
f ⊒h. In other words, ⊒is transitive."
REFERENCES,0.46246973365617433,Proof. The proposition holds by deﬁnition.
REFERENCES,0.4648910411622276,"Proposition 2. Consider graph functions f, g : Gn →Rd such that f ⊒g. Then, there is a function
ξ : Rd →Rd such that ξ ◦f = g."
REFERENCES,0.46731234866828086,"Proof. Let Gn = ⊔i∈NFi be the partitioning induced by the equality relation with respect to the
function f on Gn. Similarly deﬁne Gi, i ∈N for g. Note that due to the deﬁnition, {Fi : i ∈N} is a
reﬁnement for {Gi : i ∈N}. Deﬁne ξ to be the unique mapping from {Fi : i ∈N} to {Gi : i ∈N}
which respects the equality relation. One can observe that such ξ satisﬁes the requirement in the
proposition."
REFERENCES,0.46973365617433416,"Deﬁnition 6. An RNP-GNN is called maximally expressive, if and only if"
REFERENCES,0.4721549636803874,"• all the aggregate functions are injective as mappings from a multi-set on a countable
ground set to their codomain."
REFERENCES,0.4745762711864407,"• all the combine functions are injective mappings.
Proposition 3. Consider two RNP-GNNs f, g with the same recursion parameters r
=
(r1, r2, . . . , rτ) where f is maximally expressive. Then, f ⊒g."
REFERENCES,0.47699757869249393,Proof. The proposition holds by deﬁnition.
REFERENCES,0.4794188861985472,"Proposition 4. Consider a sequence of graph functions f, g1, . . . , gk. If f ⊒gi for all i ∈[k], then f ⊒ k
X"
REFERENCES,0.48184019370460046,"i=1
cigi,
(15)"
REFERENCES,0.48426150121065376,"for any ci ∈R, i ∈N."
REFERENCES,0.48668280871670705,"Proof. Since f ⊒gi, we have
∀G1, G2 ∈Gn : f(G1) = f(G2) =⇒gi(G1) = gi(G2),
(16)
for all i ∈[k]. This means that for any G1, G2 ∈Gn if f(G1) = f(G2) then gi(G1) = gi(G2),
i ∈[k], and consequently Pk
i=1 cigi(G1) = Pk
i=1 cigi(G2). Therefore, from the deﬁnition we
conclude f ⊒Pk
i=1 cigi. Note that the same proof also holds in the case of countable summations
as long as the summation is bounded."
REFERENCES,0.4891041162227603,Under review as a conference paper at ICLR 2022
REFERENCES,0.4915254237288136,"Deﬁnition 7. Let H = (VH, EH, XH) be a attributed connected simple graph with k nodes. For
any attributed graph G = (VG, EG, XG) ∈Gn, the induced subgraph count function C(G; H) is
deﬁned as"
REFERENCES,0.4939467312348668,"C(G; H) :=
X"
REFERENCES,0.4963680387409201,"S⊆[n]
1{G(S) ∼= H}.
(17)"
REFERENCES,0.49878934624697335,"Also, let ¯C(G; H) denote the number of non-induced subgraphs of G which are isomorphic to H. It
can be deﬁned with the homomorphisms from H to G. Formally, if n > k deﬁne"
REFERENCES,0.5012106537530266,"¯C(G; H) :=
X"
REFERENCES,0.5036319612590799,"S⊆[n]
|S|=k"
REFERENCES,0.5060532687651331,"¯C(G(S); H).
(18)"
REFERENCES,0.5084745762711864,"Otherwise, n = k, and we deﬁne"
REFERENCES,0.5108958837772397,"¯C(G; H) :=
X"
REFERENCES,0.513317191283293,"˜
H∈˜
H(H)
c ˜
H,H × 1{G ∼= ˜H},
(19) where"
REFERENCES,0.5157384987893463,"˜H(H) := { ˜H ∈Gk : ˜H ⋑H
	
,
(20)"
REFERENCES,0.5181598062953995,"is deﬁned with respect to the graph isomorphism, and c ˜
H,H ∈N denotes the number of subgraphs
in H identical to ˜H. Note that ˜H(H) is a ﬁnite set and ⋑denotes being a (not necessarily induced)
subgraph."
REFERENCES,0.5205811138014528,"Proposition 5. Let H be a family of graphs. If for any H ∈H, there is an RNP-GNN fH(.; θ) with
recursion parameters (r1, r2, . . . , rτ) such that fH ⊒C(G; H), then there exists an RNP-GNN
f(.; θ) with recursion parameters (r1, r2, . . . , rτ) such that f ⊒P"
REFERENCES,0.5230024213075061,H∈H C(G; H).
REFERENCES,0.5254237288135594,"Proof. Let f(.; θ) be a maximally expressive RNP-GNN. Note that by the deﬁnition f ⊒fH for any
H ∈H. Since ⊒is transitive, f ⊒C(G; H) for all H ∈H, and using Proposition 4, we conclude
that f ⊒P"
REFERENCES,0.5278450363196125,H∈H C(G; H).
REFERENCES,0.5302663438256658,The following proposition shows that it is sufﬁcient to address counting attributed graphs.
REFERENCES,0.5326876513317191,"Proposition 6. Let H0 be an unattributed connected graph. Assume that for any attributed graph
H, which is constructed by adding arbitrary attributes to H0, there exists an RNP-GNN fH(.; θH)
such that fH ⊒C(G; H), then for its unattributed counterpart H0, there exists an RNP-GNN f(.; θ)
with the same recursion parameters as fH(.; θH) such that f ⊒C(G; H0)."
REFERENCES,0.5351089588377724,"Proof. If there exists an RNP-GNN fH(.; θH) such that fH ⊒C(G; H), then for a maximally
expressive RNP-GNN f(.; θ) with the same recursion parameters as fH we also have f ⊒C(G; H).
Let H be the set of all attributed graphs H = (V, E, X) ∈Gk up to graph isomorphism, where
X ∈X k for a countable set X. Note that H = {H1, H2, . . .} is a countable set. Now we write"
REFERENCES,0.5375302663438256,"C(G; H0) =
X"
REFERENCES,0.5399515738498789,"S⊆[n]
|S|=k"
REFERENCES,0.5423728813559322,"1{G(S) ∼= H0}
(21) =
X"
REFERENCES,0.5447941888619855,"S⊆[n]
|S|=k X"
REFERENCES,0.5472154963680388,"i∈N
1{G(S) ∼= Hi}
(22) =
X i∈N X"
REFERENCES,0.549636803874092,"S⊆[n]
|S|=k"
REFERENCES,0.5520581113801453,"1{G(S) ∼= Hi}
(23) =
X"
REFERENCES,0.5544794188861986,"i∈N
C(G; Hi).
(24) (25)"
REFERENCES,0.5569007263922519,Now using Proposition 4 we conclude that f ⊒C(G; H0) since C(G; H0) is always ﬁnite.
REFERENCES,0.559322033898305,Under review as a conference paper at ICLR 2022
REFERENCES,0.5617433414043583,"Deﬁnition 8. Let H be a (possibly attributed) simple connected graph. For any S ⊆VH and
v ∈VH, deﬁne"
REFERENCES,0.5641646489104116,"¯dH(v; S) := max
u∈S d(u, v).
(26)"
REFERENCES,0.5665859564164649,"Deﬁnition 9. Let H be a (possibly attributed) connected simple graph with k = τ + 1 vertices.
A permutation of vertices, such as (v1, v2, . . . , vτ+1), is called a vertex covering sequence, with
respect to a sequence r = (r1, r2, . . . , rτ) ∈Nτ, called a covering sequence, if and only if"
REFERENCES,0.5690072639225182,"¯dH′
i(vi; Si) ≤ri,
(27)"
REFERENCES,0.5714285714285714,"for i ∈[τ + 1], where H′
i = H(Si) and Si = {vi, vi+1, . . . , vτ+1}. Let CH(r) denote the set of all
vertex covering sequences with respect to the covering sequence r for H."
REFERENCES,0.5738498789346247,"Proposition 7. For any G, H ∈Gk, if G ⋑H (non-induced subgraph), then"
REFERENCES,0.576271186440678,"CH(r) ⊆CG(r),
(28)"
REFERENCES,0.5786924939467313,for any sequence r.
REFERENCES,0.5811138014527845,"Proof. The proposition follows from the fact that the function ¯d is decreasing with introducing new
edges."
REFERENCES,0.5835351089588378,"Proposition 8. Assume that Theorem 1 holds for induced-subgraph count functions. Then, it also
holds for the non-induced subgraph count functions."
REFERENCES,0.585956416464891,"Proof. Assume that for a connected (attributed or unattributed) graph H, there exists an RNP-GNN
with appropriate recursion parameters fH(.; θH) such that fH ⊒C(G; H), then we prove there
exists an RNP-GNN f(.; θ) with the same recursion parameters as fH such that f ⊒¯C(G; H)."
REFERENCES,0.5883777239709443,"If there exists an RNP-GNN fH(.; θH) such that fH ⊒C(G; H), then for a maximally expressive
RNP-GNN f(.; θ) with the same recursion parameters as fH we also have f ⊒C(G; H). Note that"
REFERENCES,0.5907990314769975,"¯C(G, H) =
X"
REFERENCES,0.5932203389830508,"S⊆[n]
|S|=k"
REFERENCES,0.5956416464891041,"¯C(G(S); H)
(29) =
X"
REFERENCES,0.5980629539951574,"S⊆[n]
|S|=k X"
REFERENCES,0.6004842615012107,"˜
H∈˜
H(H)
c ˜
H,H × 1{G(S) ∼= ˜H}
(30) =
X"
REFERENCES,0.6029055690072639,"˜
H∈˜
H(H)
c ˜
H,H
X"
REFERENCES,0.6053268765133172,"S⊆[n]
|S|=k"
REFERENCES,0.6077481840193705,"1{G(S) ∼= ˜H}
(31) =
X"
REFERENCES,0.6101694915254238,"i∈N
cHi,H × C(G, Hi),
(32)"
REFERENCES,0.612590799031477,"where ˜H(H) = {H1, H2, . . .}."
REFERENCES,0.6150121065375302,"Claim 1. f ⊒C(G, Hi) for any i."
REFERENCES,0.6174334140435835,"Using Proposition 4 and Claim 1 we conclude that f ⊒¯C(G; H) since ¯C(G; H) is ﬁnite and
f ⊒C(G, Hi) for any i, and the proof is complete. The missing part which we must show here
is that for any Hi the sequence (r1, r2, . . . , rt) which covers H also covers Hi. This follows from
Proposition 7. We are done."
REFERENCES,0.6198547215496368,"At the end of this part, let us introduce an important notation. For any attributed connected simple
graph on k vertices G = (V, E, X), let G∗
v be the resulting induced graph obtained after removing
v ∈V from G with the new attributes deﬁned as"
REFERENCES,0.6222760290556901,"X∗
u := (Xu, 1{(u, v) ∈E}),
(33)"
REFERENCES,0.6246973365617433,"for each u ∈V \ {v}. We may also use X∗v
u for more clariﬁcation."
REFERENCES,0.6271186440677966,Under review as a conference paper at ICLR 2022
REFERENCES,0.6295399515738499,"A.2
PROOF OF THEOREM 1"
REFERENCES,0.6319612590799032,"We utilize an inductive proof on τ, which is the length of the covering sequence of H. Equivalently,
due to the deﬁnition, τ = k −1, where k is the number of vertices in H. First, we note that due
to Proposition 8, without loss of generality, we can assume that H is a simple connected attributed
graph and the goal is to achieve the induced-subgraph count function via an RNP-GNN with appro-
priate recursion parameters. We also consider only maximally expressive networks here to prove the
desired result."
REFERENCES,0.6343825665859564,"Induction base. For the induction base, i.e., τ = 1, H is a two-node graph. This means that we only
need to count the number of a speciﬁc (attributed) edge in the given graph G. Note that in this case
we apply an RNP-GNN with recursion parameter r1 ≥1. Denote the two attributes of the vertices
in H by XH
1 , XH
2 ∈X. The output of an RNP-GNN f(.; θ) is"
REFERENCES,0.6368038740920097,"f(G; θ) = φ({{ψ(XG
v , ϕ({{X∗v
u : u ∈Nr1(v)}})) : v ∈[n]}}),
(34)"
REFERENCES,0.639225181598063,"where we assume that f(.; θ) is maximally expressive. The goal is to show that f ⊒C(G; H).
Using the transitivity of ⊒, we only need to choose appropriate φ, ψ, ϕ to achieve ˆf = C(G; H) as
the ﬁnal representation. Let"
REFERENCES,0.6416464891041163,"φ({{zv : v ∈[n]}}) :=
1
2 + 2 × 1{XH
1 = XH
2 } n
X"
REFERENCES,0.6440677966101694,"i=1
zi
(35)"
REFERENCES,0.6464891041162227,"ψ(X, (z, z′)) := z × 1{X = XH
1 } + z′ × 1{X = XH
2 }
(36)"
REFERENCES,0.648910411622276,"ϕ({{zu : u ∈[n′]}}) :=
 n′
X"
REFERENCES,0.6513317191283293,"i=1
1{zu = (XH
2 , 1)}, n′
X"
REFERENCES,0.6537530266343826,"i=1
1{zu = (XH
1 , 1)

.
(37)"
REFERENCES,0.6561743341404358,"Then, a simple computation shows that"
REFERENCES,0.6585956416464891,"ˆf(G; θ) = φ({{ψ(XG
v , ϕ({{X∗v
u : u ∈Nr1(v)}})) : v ∈[n]}}),
(38)
= C(G; H).
(39)"
REFERENCES,0.6610169491525424,"Since ˆf(.; θ) is an RNP-GNN with recursion parameter r1 and for any maximally expressive RNP-
GNN f(.; θ) with the same recursion parameter as ˆf we have f ⊒ˆf and ˆf ⊒C(G; H), we conclude
that f ⊒C(G; H) and this completes the proof."
REFERENCES,0.6634382566585957,"Induction step. Assume that the desired result holds for τ −1 (τ ≥2). We show that it also holds
for τ. Let us ﬁrst deﬁne"
REFERENCES,0.6658595641646489,"H∗:= {H∗
v1 : ∃v2, . . . , vτ ∈[k] : (v1, v2, . . . , vτ) ∈CH(r)}
(40)"
REFERENCES,0.6682808716707022,"c∗(H0) := 1{H0 ∈H∗} × #{v ∈[k] : H∗
v ∼= H0},
(41)"
REFERENCES,0.6707021791767555,"where H∗
v means the induced subgraph after removing a node, with new attributes (see A.1). Note
that H∗̸= ∅by the assumption. Let"
REFERENCES,0.6731234866828087,"∥H∗∥:=
X"
REFERENCES,0.6755447941888619,"H0∈H∗
c∗(H0).
(42)"
REFERENCES,0.6779661016949152,"For all H0 ∈H∗, using the induction hypothesis, there is a (universal) RNP-GNN ˆf(.; ˆθ) with
recursion parameters (r2, r3, . . . , rτ) such that ˆf ⊒C(G; H0). Using Proposition 4 we conclude"
REFERENCES,0.6803874092009685,"ˆf ⊒
X"
REFERENCES,0.6828087167070218,"u∈[k]:H∗
u∈H∗
C(G; H∗
u).
(43)"
REFERENCES,0.6852300242130751,"Deﬁne a maximally expressive RNP-GNN with the recursion parameters (r1, r2, . . . , rτ) as follows:"
REFERENCES,0.6876513317191283,"f(G; θ) = φ({{ψ(XG
v , ˆf(G∗(Nr1(v)); ˆθ)) : v ∈[n]}}).
(44)"
REFERENCES,0.6900726392251816,"Similar to the proof for τ = 1, here we only need to propose a (not necessarily maximally expressive)
RNP-GNN which achieves the function C(G; H)."
REFERENCES,0.6924939467312349,Under review as a conference paper at ICLR 2022
REFERENCES,0.6949152542372882,Let us deﬁne
REFERENCES,0.6973365617433414,"fH∗
u(G; θ) := φ({{ψH∗
u(XG
v , ξ ◦ˆf(G∗(Nr1(v)); ˆθ)) : v ∈[n]}}),
(45) where"
REFERENCES,0.6997578692493946,"φ({{zv : v ∈[n]}}) :=
1
∥H∗∥ n
X"
REFERENCES,0.7021791767554479,"i=1
zi
(46)"
REFERENCES,0.7046004842615012,"ψH∗
u(X, z) := z × 1{X = XH
u },
(47) (48)"
REFERENCES,0.7070217917675545,"and ξ ◦ˆf = C(G; H∗
u). Note that the existence of such function ξ is guaranteed due to Proposition
2. Now we write"
REFERENCES,0.7094430992736077,"∥H∗∥× C(G; H) = ∥H∗∥
X"
REFERENCES,0.711864406779661,"S⊆[n]
1{G(S) ∼= H}
(49) =
X S⊆[n] X"
REFERENCES,0.7142857142857143,"v∈S
1{∃u ∈[k] : (G(S \ {v}))∗
v ∼= H∗
u ∈H∗∧XG
v = XH
u }
(50) =
X v∈[n] X"
REFERENCES,0.7167070217917676,"v∈S⊆[n]
1{∃u ∈[k] : (G(S \ {v}))∗
v ∼= H∗
u ∈H∗∧XG
v = XH
u } (51) =
X v∈[n] X"
REFERENCES,0.7191283292978208,"v∈S⊆Nr1(v)
1{∃u ∈[k] : (G(S \ {v}))∗
v ∼= H∗
u ∈H∗∧XG
v = XH
u } (52) =
X v∈[n] X"
REFERENCES,0.7215496368038741,v∈S⊆Nr1(v) X
REFERENCES,0.7239709443099274,"u∈[k]:H∗
u∈H∗
1{(G(S \ {v}))∗
v ∼= H∗
u}1{XG
v = XH
u } (53) =
X v∈[n] X"
REFERENCES,0.7263922518159807,"u∈[k]:H∗
u∈H∗
C(G∗(Nr1(v)); H∗
u) × 1{XG
v = XH
u },
(54)"
REFERENCES,0.7288135593220338,"which means that
X"
REFERENCES,0.7312348668280871,"u∈[k]:H∗
u∈H∗
fH∗
u(G; θ) ⊒C(G; H).
(55)"
REFERENCES,0.7336561743341404,"However, for a maximally expressive RNP-GNN f(.; θ) we know that f ⊒fH∗
u for all H∗
u ∈H and
this means that f ⊒C(G; H). The proof is thus complete."
REFERENCES,0.7360774818401937,"B
PROOF OF THEOREM 2"
REFERENCES,0.738498789346247,"For any attributed graph H on r nodes (not necessarily connected) we claim that RNP-GNNs can
count them."
REFERENCES,0.7409200968523002,"Claim 2. Let f(.; θ) : Gn →Rd be a maximally expressive RNP-GNN with recursion parameters
(r −1, r −2, . . . , 1). Then, f ⊒C(G; H)."
REFERENCES,0.7433414043583535,Now consider the function
REFERENCES,0.7457627118644068,"ℓ(G) = φ({{ψ(G(S)) : S ⊆V, |S| ≤r}}).
(56)"
REFERENCES,0.7481840193704601,"We claim that f ⊒ℓ(f is deﬁned in the previous claim) and this completes the proof according to
Proposition 2."
REFERENCES,0.7506053268765133,"To prove the claim, assume that f(G1) = f(G2). Then, we conclude that C(G1; H) = C(G2; H)
for any attributed H (not necessarily connected) with r vertices. Now, we have"
REFERENCES,0.7530266343825666,"ℓ(G) = φ({{ψ(G(S)) : S ⊆V, |S| ≤r}})
(57)
= φ({{ψ(H) : H ∈Gr, the multiplicity of H is C(G; H)}}),
(58)"
REFERENCES,0.7554479418886199,which shows that ℓ(G1) = ℓ(G2).
REFERENCES,0.7578692493946732,Under review as a conference paper at ICLR 2022
REFERENCES,0.7602905569007264,"Proof of Claim 2. To prove the claim, we use an induction on the number of connected components
cH of graph H. If H is connected, i.e., cH = 1, then according to Theorem 1, we know that
f ⊒C(G; H)."
REFERENCES,0.7627118644067796,"Now assume that the claim holds for cH = c −1 ≥1. We show that it also holds for cH = c. Let
H1, H2, . . . , Hc denote the connected components of H. Also assume that Hi ̸∼= Hj for all i ̸= j.
We will relax this assumption later. Let us deﬁne"
REFERENCES,0.7651331719128329,"AG := {(S1, S2, . . . , Sc) : ∀i ∈[c] : Si ⊆[n]; G(Si) ∼= Hi}.
(59)"
REFERENCES,0.7675544794188862,Note that we can write
REFERENCES,0.7699757869249395,"|AG| = c
Y"
REFERENCES,0.7723970944309927,"i=1
C(G; Hi)
(60)"
REFERENCES,0.774818401937046,"= C(G; H) + ∞
X"
REFERENCES,0.7772397094430993,"j=1
c′
jC(G; H′
j),
(61)"
REFERENCES,0.7796610169491526,"where H′
1, H′
2, . . . are all non-isomorphic graphs obtained by adding edges (at least one edge) be-
tween c graphs H1, H2, . . . , Hc, or contracting a number of vertices of them. The constants c′
j
are just used to remove the effect of multiple counting due to the symmetry. Now, since for any
Hi, H′
j the number of connected components is strictly less that c, using the induction, we have
f ⊒C(G; Hi) and f ⊒C(G; H′
j) for all j and all i ∈[c]. According to Proposition 4, we
conclude that f ⊒C(G; H) and this completes the proof. Also, if Hi, i ∈[c], are not pairwise
non-isomorphic, then we can use αC(G; H) in above equation instead of C(G; H), where α > 0
removes the effect of multiple counting by symmetry. The proof is thus complete."
REFERENCES,0.7820823244552058,"Remark 1. As we explained in the above proof, one can modify Theorem 1 to hold for disconnected
graphs. To this end, we need to generalize the notion of covering sequence to hold for this class of
graphs. Since this special case is out of scope of this paper, we only restrict to a special, but more
insightful case."
REFERENCES,0.784503631961259,"C
PROOF OF THEOREM 3"
REFERENCES,0.7869249394673123,"To prove Theorem 3, we need to bound the number of node updates required for an RNP-GNN with
recursion parameters (r1, r2, . . . , rt). First of all, we have n variables used for the ﬁnal represen-
tations of vertices. For each vertex v1 ∈V, we explore the local neighborhood Nr1(v1) and apply
a new RNP-GNN network to that neighborhood. In other words, for the second step we need to
update |Nr1(v1)| nodes. Similarly, for the ith step of the algorithm we have at most"
REFERENCES,0.7893462469733656,"λi := max
v1∈[n]
max
vj+1∈Nrj (vj )"
REFERENCES,0.7917675544794189,"∀j∈[i−1]
|Nr1(v1) ∩Nr2(v2) ∩Nr3(v3) . . . ∩Nri(vi)|,
(62)"
REFERENCES,0.7941888619854721,"updates. Therefore, we can bound the number of node updates as n × τY"
REFERENCES,0.7966101694915254,"i=1
λi.
(63)"
REFERENCES,0.7990314769975787,"Since λi is decreasing in i, the desired result holds."
REFERENCES,0.801452784503632,"D
PROOF OF THEOREM 4"
REFERENCES,0.8038740920096852,Let Kk denote the complete graph on k vertices.
REFERENCES,0.8062953995157385,"Claim 3. For any k, n ∈N, such that n is sufﬁciently large,
{C(G; Kk) : G ∈Gn}
 ≥(cn/(k log(n/k)) −k)k"
REFERENCES,0.8087167070217918,"k!
= ˜Ω(nk),
(64)"
REFERENCES,0.8111380145278451,"where c is a constant which does not depend on k, n."
REFERENCES,0.8135593220338984,Under review as a conference paper at ICLR 2022
REFERENCES,0.8159806295399515,"In particular, we claim that the number of different values that C(G; Kk) can take is nk, up to
poly-logarithmic factors."
REFERENCES,0.8184019370460048,"To prove the theorem, we use the above claim. Consider a class of (s, t)−good graph representations
f(.; θ) which can count any substructure on k vertices. As a result, f ⊒C(G; Kk) for an appropriate"
REFERENCES,0.8208232445520581,"parametrization θ. By the deﬁnition, f(.) must take at least
{C(G; Kk) : G ∈Gn}
 different
values, i.e.,
{f(G; θ) : G ∈Gn}
 ≥
{C(G; Kk) : G ∈Gn}
.
(65)"
REFERENCES,0.8232445520581114,"Also,
{f(G; θ) : G ∈Gn}
 ≤


{{ψ(Gi) : i ∈[t]}} : G ∈Gn
	,
(66)"
REFERENCES,0.8256658595641646,"where (G1, G2, . . . , Gt) = Ξ(G). But, ψ can take only s values. Therefore, we have
{C(G; Kk) : G ∈Gn}
 ≤
{f(G; θ) : G ∈Gn}

(67)"
REFERENCES,0.8280871670702179,"≤


{{ψ(Gi) : i ∈[t]}} : G ∈Gn
	
(68)"
REFERENCES,0.8305084745762712,"≤


{{αi : i ∈[t]}} : ∀i ∈[t] : αi ∈[s]}

(69)"
REFERENCES,0.8329297820823245,"≤(t + 1)s−1.
(70)"
REFERENCES,0.8353510895883777,"As a result, (t + 1)s−1 = ˜Ω(nk) or t = ˜Ω(n
k
s−1 ). To complete the proof, we only need to prove the
claim."
REFERENCES,0.837772397094431,"Proof of Claim 3. Let p1, p2, . . . , pm be distinct prime numbers less than n/k. Using the prime
number theorem, we know that limn→∞
m
n/(k log(n/k)) = 1. In particular, we can choose n large
enough to ensure cn/(k log(n/k)) < m for any constant c < 1."
REFERENCES,0.8401937046004843,"For any B = {b1, b2, . . . , bk} ⊆[m], deﬁne GB as a graph on n vertices such that VGB = V0 ⊔
(⊔i∈[k]Vi), and |Vi| = pbi. Also,"
REFERENCES,0.8426150121065376,"e = (u, v) ∈GB ⇐⇒∃i, j ∈[m], i ̸= j : u ∈Vi & v ∈Vj.
(71)"
REFERENCES,0.8450363196125908,"The graph GB is well-deﬁned since Pk
i=1 pbi ≤k × n/k = n. Note that C(GB; Kk) = Qk
i=1 pbi.
Also, since pi, i ∈[m], are prime numbers, there is a unique bijection"
REFERENCES,0.847457627118644,"B
ϕ
←→C(GB; Kk).
(72)
Therefore,
{C(G; Kk) : G ∈Gn}
 ≥
{C(GB; Kk) : B ⊆[m], |B| = k}

(73)"
REFERENCES,0.8498789346246973,"=
m
k"
REFERENCES,0.8523002421307506,"
(74)"
REFERENCES,0.8547215496368039,≥(m −k)k
REFERENCES,0.8571428571428571,"k!
(75)"
REFERENCES,0.8595641646489104,≥(cn/(k log(n/k)) −k)k
REFERENCES,0.8619854721549637,"k!
.
(76)"
REFERENCES,0.864406779661017,"E
RELATIONSHIP TO THE RECONSTRUCTION CONJECTURE"
REFERENCES,0.8668280871670703,"Theorem 2 provides a universality result for RNP-GNNs. Here, we note that the proposed method is
closely related to the reconstruction conjecture, an old open problem in graph theory. This motivates
us to explain their relationship/differences. First, we need a deﬁnition for unattributed graphs.
Deﬁnition 10. Let Fn ⊆Gn be a set of graphs and let Gv = G(V \ {v}) for any ﬁnite simple
graph G = (V, E), and any v ∈V. Then, we say the set F is reconstructible if and only if there is a
bijection"
REFERENCES,0.8692493946731235,"{{Gv : v ∈V}}
Φ
←→G,
(77)
for any G ∈Fn. In other words, Fn is reconstructible, if and only if the multi-set {{Gv : v ∈V}}
fully identiﬁes G for any G ∈Fn."
REFERENCES,0.8716707021791767,Under review as a conference paper at ICLR 2022
REFERENCES,0.87409200968523,"It is known that the class of disconnected graphs, trees, regular graphs, are reconstructible (Kelly,
1957; McKay, 1997). The general case is still open; however it is widely believed that it is true."
REFERENCES,0.8765133171912833,Conjecture 1 (Kelly (1957)). Gn is reconstructible.
REFERENCES,0.8789346246973365,"For RNP-GNNs, the reconstruction from the subgraphs G∗
v, v ∈[n] is possible, since we relabel
any subgraph (in the deﬁnition of X∗) and this preserves the critical information for the recursion
to the original graph. In the reconstruction conjecture, this part of information is missing, and this
makes the problem difﬁcult. Nonetheless, since in RNP-GNNs we preserve the original node’s
information in the subgraphs with relabeling, the reconstruction conjecture is not required to hold to
show the universality results for RNP-GNNs, although that conjecture is a motivation for this paper.
Moreover, if it can be shown that the reconstruction conjecture it true, it may be also possible to ﬁnd
a simple encoding of subgraphs to an original graph and this may lead to more powerful but less
complex new GNNs."
REFERENCES,0.8813559322033898,"F
THE RNP-GNN ALGORITHM"
REFERENCES,0.8837772397094431,"In this section, we provide pseudocode for RNP-GNNs. The algorithm below computes node rep-
resentations. In the algorithms, we frequently use MLP modules with ReLU activation. For a graph
representation, we can aggregate them with a common readout, e.g., hG ←MLP
 P"
REFERENCES,0.8861985472154964,"v∈V h(k)
v

.
Following (Xu et al., 2019), we use sum pooling here, to ensure that we can represent injective
aggregation functions."
REFERENCES,0.8886198547215496,Algorithm 1 Recursive Neighborhood Pooling-GNN (RNP-GNN)
REFERENCES,0.8910411622276029,"Input: G = (V, E, {xv}v∈V) where V = [n], recursion parameters r1, r2, . . . , rτ ∈N, ϵ(i) ∈R,
i ∈[τ], node features {xv}v∈V.
Output: hv for all v ∈V"
REFERENCES,0.8934624697336562,"hin
v ←xv for all v ∈V
if τ = 1 then"
REFERENCES,0.8958837772397095,"hv ←MLP(τ,1)
(1 + ϵ(1))hin
v +
X"
REFERENCES,0.8983050847457628,"u∈Nr1(v)\{v}
MLP(τ,2)(hin
u, 1(u, v) ∈E)

,"
REFERENCES,0.9007263922518159,"for all v ∈V.
else"
REFERENCES,0.9031476997578692,for all v ∈V do
REFERENCES,0.9055690072639225,"G′
v ←G(Nr1(v) \ {v}), which has node attributes {(hin
u, 1(u, v) ∈E)′}u∈Nr1(v)\{v}
{ˆhv,u}u∈G′v ←RNP-GNN(G′
v, (r2, r3, . . . , rτ), (ϵ(2), . . . , ϵ(τ)))"
REFERENCES,0.9079903147699758,"hv ←MLP(τ)
(1 + ϵ(τ))hin
v + P"
REFERENCES,0.910411622276029,"u∈G′
v ˆhu,v

.
end for
end if
return {hv}v∈V"
REFERENCES,0.9128329297820823,"With this algorithm, one can achieve the expressive power of RNP-GNNs if high dimensional MLPs
are allowed (Xu et al., 2019; Hornik et al., 1989; Hornik, 1991). That said, in practice, smaller
MLPs may be acceptable (Xu et al., 2019)."
REFERENCES,0.9152542372881356,"G
COMPUTING A COVERING SEQUENCE"
REFERENCES,0.9176755447941889,"As we explained in the context of Theorem 1, we need a covering sequence (or an upper bound to
that) to design an RNP-GNN network that can count a given substructure. A covering sequence can
be constructed from a spanning tree of the graph."
REFERENCES,0.9200968523002422,"For reducing complexity, it is desirable to have a covering sequence with minimum r1 (Theorem
3). Here, we suggest an algorithm for obtaining such a covering sequence, shown in Algorithm 2."
REFERENCES,0.9225181598062954,Under review as a conference paper at ICLR 2022
REFERENCES,0.9249394673123487,"For obtaining merely an aribtrary covering sequence, one can compute any minimum spanning tree
(MST), and then proceed as with the MST in Algorithm 2."
REFERENCES,0.927360774818402,"Given an MST, we build a vertex covering sequence by iteratively removing a leaf vi from the tree
and adding the respective node vi to the sequence. This ensures that, at any point, the remaining
graph is connected. At position i corresponding to vi, the covering sequence contains the maximum
distance ri of vi to any node in the remaining graph, or an upper bound on that. For efﬁciency, an
upper bound on the distance can be computed in the tree."
REFERENCES,0.9297820823244553,"To minimize r1 = maxu∈V d(u, v1), we need to ensure that a node in arg minv∈V maxu∈V d(u, v)
is a leaf in the spanning tree. Hence, we ﬁrst compute maxu∈V d(u, v) for all nodes v, e.g., by
running All-Pairs-Shortest-Paths (APSP) (Kleinberg & Tardos, 2006), and sort them in increasing
order by this distance. Going down this list, we try whether it is possible to use the respective node
as v1, and stop when we ﬁnd one."
REFERENCES,0.9322033898305084,"Say v∗is the current node in the list. To compute a spanning tree where v∗is a leaf, we assign a
large weight to all the edges adjacent to v∗, and a very low weight to all other edges. If there exists
such a tree, running an MST with the assigned weights will ﬁnd one. Then, we use v∗as v1 in the
vertex covering sequence. This algorithm runs in polynomial time."
REFERENCES,0.9346246973365617,"Algorithm 2 Computing a covering sequence with minimum r1
Input: H = (V, E, X) where V = [τ + 1]
Output: A minimal covering sequence (r1, r2 . . . , rτ), and its corresponding vertex covering se-
quence (v1, v2, . . . , vτ+1)
For any u, v ∈V, compute d(u, v) using APSP
(u1, u2, . . . , uτ+1) ←all the vertices sorted increasingly in s(v) := maxu∈V d(u, v)
for i = 1 to τ + 1 do"
REFERENCES,0.937046004842615,"Set edge weights w(u, v) = 1 + τ × 1{u = ui ∨v = ui} for all (u, v) ∈E
HT ←the MST of H with weights w
if ui is a leaf in HT then"
REFERENCES,0.9394673123486683,"v1 ←ui
r1 ←s(ui)
break
end if
end for
for i = 2 to τ + 1 do"
REFERENCES,0.9418886198547215,"vi ←one of the leaves of HT
ri ←maxu∈VHT d(u, vi)
HT ←HT after removing vi
end for
return (r1, r2, . . . , rτ) and (v1, v2, . . . , vτ+1)"
REFERENCES,0.9443099273607748,"H
EXPERIMENTAL DETAILS"
REFERENCES,0.9467312348668281,"Table 4: Runtime (in seconds) averaged over 10 epochs of training on the synthetic triangle counting
experiments on Erd˝os-Renyi graphs. Times do not include LRP preprocessing (which takes several
minutes for this dataset)."
REFERENCES,0.9491525423728814,"Model
Parameters
Runtime (s)"
REFERENCES,0.9515738498789347,"RNP-GNN, r = (1, 1)
10210
153.13
RNP-GNN, r = (1, 1, 1)
10834
199.49
RNP-GNN, r = (2, 1)
10210
370.38
RNP-GNN, r = (2, 1, 1)
10834
835.77
GIN
10186
.88
Deep-LRP
10231
24.43"
REFERENCES,0.9539951573849879,Under review as a conference paper at ICLR 2022
REFERENCES,0.9564164648910412,"H.1
DATASET AND TASK DETAILS"
REFERENCES,0.9588377723970944,"For the counting experiments, we follow the setup of Chen et al. (2020). There are two datasets:
one consisting of 5000 Erd˝os-Renyi graphs (Erdos et al., 1960) and one consisting of 5000 noisy
random regular graphs (Steger & Wormald, 1999). Each Erd˝os-Renyi graph has 10 nodes, and each
random regular graph has either 10, 15, 20, or 30 nodes. Also, n random edges are deleted from
each random regular graph, where n is the number of nodes."
REFERENCES,0.9612590799031477,"For the experiments on distinguishing non-isomorphic graphs, we use the EXP dataset (Abboud
et al., 2021). This dataset consists of 600 pairs of graphs (so 1200 graphs in total), where each pair
is 1-WL equivalent but distinguishable by 3-WL, and each pair contains one graph that represents a
satisﬁable formula and one graph that represents an unsatisﬁable formula. On average, each graph
contains about 44 nodes and 110 edges (Balcilar et al., 2021). We report the mean and standard
deviation across 10 cross-validation folds. Additionally, we report the runtimes in Table 4 for the
counting experiments."
REFERENCES,0.9636803874092009,"H.2
RNP-GNN IMPLEMENTATION DETAILS"
REFERENCES,0.9661016949152542,"Here, we detail some speciﬁc design choices we make in implementing our RNP-GNN model. Most
embeddings are computed in Rd for some ﬁxed hidden dimension d. The input node features are
ﬁrst embedded in Rd by an initial linear layer. Then RNP layers are applied to compute node
representations. Finally, a sum pooling across nodes followed by a ﬁnal MLP is used to compute a
graph-level output."
REFERENCES,0.9685230024213075,"An RNP layer for r = (r1, . . . , rτ) is implemented as follows. Note that the input node features to
this layer are in Rd due to our initial linear layer. Also, note that we concatenate an extra feature
dimension due to the augmented indicator feature at each recursion step. To align these feature
dimensions, for l ∈[τ], we parameterize the l-th GIN (Xu et al., 2019) by a feedforward neural
network MLP(l) : Rd+l →Rd+l−1. For instance, the last GIN has a feedforward network MLP(τ) :
Rd+τ →Rd+τ−1, because after τ levels of recursion we have augmented τ features. Dropout and
nonlinear activation functions are only applied in the MLPs."
REFERENCES,0.9709443099273608,"H.3
HYPERPARAMETERS"
REFERENCES,0.9733656174334141,"For all baseline models, we take the results from other papers. Thus, for the counting experiments the
conﬁgurations for the baseline models are from Chen et al. (2020), while for the EXP experiments
the conﬁgurations for the baseline models are from Abboud et al. (2021)."
REFERENCES,0.9757869249394673,"RNP-GNN hyperparameters. For all experiments we ran random search over hyperparameters. In
all cases we used the Adam optimizer with initial learning rate in {.01, .001, .0001, .0005}. We train
for 100 epochs with a batch size in {16, 32, 128}. The number of stacked RNP-GNNs for computing
node representations is in {1, 2}. We use a dropout ratio in {0, .1, .5}. The recursion parameters
used varies for each task. We used two layers for each MLP used in the aggregation function. Also,
the graph-level output obtained after sum-pooling across nodes is computed by a two layer MLP."
REFERENCES,0.9782082324455206,"Speciﬁcally for the counting experiments, the number of hidden dimensions is searched in
{16, 32, 64}. For all tasks we used r1 = 1. In particular, the optimal r parameters for RNP-GNN
were: r = (1, 1, 1, 1) for triangles on Erd˝os-Renyi, r = (1, 1) for stars on Erd˝os-Renyi, r = (1, 1)
for triangles on random regular and r = (1, 1, 1) for stars on random regular. We use ReLU activa-
tions in the MLPs. We either decay the learning rate by half every 25, 50, or ∞epochs (where ∞
means never decaying)."
REFERENCES,0.9806295399515739,"For the EXP experiments, the number of hidden dimensions is searched in {8, 16, 32, 64}. We use
either ELU or ReLU activations in the MLPs. We decay the learning rate by half at the 50th epoch.
The recursion parameters are r = (2, 1)."
REFERENCES,0.9830508474576272,Under review as a conference paper at ICLR 2022
REFERENCES,0.9854721549636803,"I
MORE FIGURES"
REFERENCES,0.9878934624697336,"Figure 3: MPNNs cannot count substructures with three nodes or more (Chen et al., 2020). For
example, the graph with black center vertex on the left cannot be counted, since the two graphs on
the left result in the same node representations as the graph on the right."
REFERENCES,0.9903147699757869,"v2
v1
v3"
REFERENCES,0.9927360774818402,"v5
v4
v6
v5
v4
v6
v5
v4
v6"
REFERENCES,0.9951573849878934,"v2
v1
v3
v2
v1
v3"
REFERENCES,0.9975786924939467,"Figure 4:
Example of a covering sequence computed for the graph on the left.
For this
graph, (v6, v1, v4, v5, v3, v2) is a vertex covering sequence with respect to the covering sequence
(3, 3, 3, 2, 1). The ﬁrst two computations to obtain this covering sequence are depicted in the middle
and on the right."
