Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0007575757575757576,"Policy gradient methods are notorious for having a large variance and high sample
1"
ABSTRACT,0.0015151515151515152,"complexity. To mitigate this, we introduce SoftTreeMax—a generalization of
2"
ABSTRACT,0.0022727272727272726,"softmax that employs planning. In SoftTreeMax, we extend the traditional logits
3"
ABSTRACT,0.0030303030303030303,"with the multi-step discounted cumulative reward, topped with the logits of future
4"
ABSTRACT,0.003787878787878788,"states. We analyze SoftTreeMax and explain how tree expansion helps to reduce
5"
ABSTRACT,0.004545454545454545,"its gradient variance. We prove that the variance decays exponentially with the
6"
ABSTRACT,0.005303030303030303,"planning horizon as a function of the chosen tree-expansion policy. Specifically,
7"
ABSTRACT,0.006060606060606061,"we show that the closer the induced transitions are to being state-independent,
8"
ABSTRACT,0.006818181818181818,"the stronger the decay. With approximate forward models, we prove that the
9"
ABSTRACT,0.007575757575757576,"resulting gradient bias diminishes with the approximation error while retaining
10"
ABSTRACT,0.008333333333333333,"the same variance reduction. Ours is the first result to bound the gradient bias for
11"
ABSTRACT,0.00909090909090909,"an approximate model. In a practical implementation of SoftTreeMax, we utilize
12"
ABSTRACT,0.009848484848484848,"a parallel GPU-based simulator for fast and efficient tree expansion. Using this
13"
ABSTRACT,0.010606060606060607,"implementation in Atari, we show that SoftTreeMax reduces the gradient variance
14"
ABSTRACT,0.011363636363636364,"by three orders of magnitude. This leads to better sample complexity and improved
15"
ABSTRACT,0.012121212121212121,"performance compared to distributed PPO.
16"
INTRODUCTION,0.012878787878787878,"1
Introduction
17"
INTRODUCTION,0.013636363636363636,"Policy Gradient (PG) methods [Sutton et al., 1999] for Reinforcement Learning (RL) are often the
18"
INTRODUCTION,0.014393939393939395,"first choice for environments that allow numerous interactions at a fast pace [Schulman et al., 2017].
19"
INTRODUCTION,0.015151515151515152,"Their success is attributed to several factors: they are easy to distribute to multiple workers, require
20"
INTRODUCTION,0.015909090909090907,"no assumptions on the underlying value function, and have both on-policy and off-policy variants.
21"
INTRODUCTION,0.016666666666666666,"Despite these positive features, PG algorithms are also notoriously unstable due to the high variance
22"
INTRODUCTION,0.017424242424242425,"of the gradients computed over entire trajectories [Liu et al., 2020, Xu et al., 2020]. As a result, PG
23"
INTRODUCTION,0.01818181818181818,"algorithms tend to be highly inefficient in terms of sample complexity. Several solutions have been
24"
INTRODUCTION,0.01893939393939394,"proposed to mitigate the high variance issue, including baseline subtraction [Greensmith et al., 2004,
25"
INTRODUCTION,0.019696969696969695,"Thomas and Brunskill, 2017, Wu et al., 2018], anchor-point averaging [Papini et al., 2018], and other
26"
INTRODUCTION,0.020454545454545454,"variance reduction techniques [Zhang et al., 2021, Shen et al., 2019, Pham et al., 2020].
27"
INTRODUCTION,0.021212121212121213,"A second family of algorithms that achieved state-of-the-art results in several domains is based on
28"
INTRODUCTION,0.02196969696969697,"planning. Planning is exercised primarily in the context of value-based RL and is usually implemented
29"
INTRODUCTION,0.022727272727272728,"using a Tree Search (TS) [Silver et al., 2016, Schrittwieser et al., 2020]. In this work, we combine
30"
INTRODUCTION,0.023484848484848483,"PG with TS by introducing a parameterized differentiable policy that incorporates tree expansion.
31"
INTRODUCTION,0.024242424242424242,"Namely, our SoftTreeMax policy replaces the standard policy logits of a state and action, with the
32"
INTRODUCTION,0.025,"expected value of trajectories that originate from these state and action. We consider two variants of
33"
INTRODUCTION,0.025757575757575757,"SoftTreeMax, one for cumulative reward and one for exponentiated reward.
34"
INTRODUCTION,0.026515151515151516,"Combining TS and PG should be done with care given the biggest downside of PG—its high gradient
35"
INTRODUCTION,0.02727272727272727,"variance. This raises questions that were ignored until this work: (i) How to design a PG method based
36"
INTRODUCTION,0.02803030303030303,"on tree-expansion that is stable and performs well in practice? and (ii) How does the tree-expansion
37"
INTRODUCTION,0.02878787878787879,"policy affect the PG variance? Here, we analyze SoftTreeMax, and provide a practical methodology
38"
INTRODUCTION,0.029545454545454545,"to choose the expansion policy to minimize the resulting variance. Our main result shows that a
39"
INTRODUCTION,0.030303030303030304,"desirable expansion policy is one, under which the induced transition probabilities are similar for
40"
INTRODUCTION,0.03106060606060606,"each starting state. More generally, we show that the gradient variance of SoftTreeMax decays at
41"
INTRODUCTION,0.031818181818181815,"a rate of |λ2|d, where d is the depth of the tree and λ2 is the second eigenvalue of the transition
42"
INTRODUCTION,0.03257575757575758,"matrix induced by the tree expansion policy. This work is the first to prove such a relation between
43"
INTRODUCTION,0.03333333333333333,"PG variance and tree expansion policy. In addition, we prove that the with an approximate forward
44"
INTRODUCTION,0.03409090909090909,"model, the bias of the gradient is bounded proportionally to the approximation error of the model.
45"
INTRODUCTION,0.03484848484848485,"To verify our results, we implemented a practical version of SoftTreeMax that exhaustively searches
46"
INTRODUCTION,0.035606060606060606,"the entire tree and applies a neural network on its leaves. We test our algorithm on a parallelized
47"
INTRODUCTION,0.03636363636363636,"Atari GPU simulator [Dalton et al., 2020]. To enable a tractable deep search, up to depth eight, we
48"
INTRODUCTION,0.037121212121212124,"also introduce a pruning technique that limits the width of the tree. We do so by sampling only the
49"
INTRODUCTION,0.03787878787878788,"most promising nodes at each level. We integrate our SoftTreeMax GPU implementation into the
50"
INTRODUCTION,0.038636363636363635,"popular PPO [Schulman et al., 2017] and compare it to the flat distributed variant of PPO. This allows
51"
INTRODUCTION,0.03939393939393939,"us to demonstrate the potential benefit of utilizing learned models while isolating the fundamental
52"
INTRODUCTION,0.04015151515151515,"properties of TS without added noise. In all tested Atari games, our results outperform the baseline
53"
INTRODUCTION,0.04090909090909091,"and obtain up to 5x more reward. We further show in Section 6 that the associated gradient variance
54"
INTRODUCTION,0.041666666666666664,"is smaller by three orders of magnitude in all games, demonstrating the relation between low gradient
55"
INTRODUCTION,0.04242424242424243,"variance and high reward.
56"
INTRODUCTION,0.04318181818181818,"We summarize our key contributions. (i) We show how to combine two families of SoTA approaches:
57"
INTRODUCTION,0.04393939393939394,"PG and TS by introducing SoftTreeMax: a novel parametric policy that generalizes softmax to
58"
INTRODUCTION,0.0446969696969697,"planning. Specifically, we propose two variants based on cumulative and exponentiated rewards. (ii)
59"
INTRODUCTION,0.045454545454545456,"We prove that the gradient variance of SoftTreeMax in its two variants decays exponentially
60"
INTRODUCTION,0.04621212121212121,"with its tree depth. Our analysis sheds new light on the choice of tree expansion policy. It raises
61"
INTRODUCTION,0.04696969696969697,"the question of optimality in terms of variance versus the traditional regret; e.g., in UCT [Kocsis
62"
INTRODUCTION,0.04772727272727273,"and Szepesvári, 2006]. (iii) We prove that with an approximate forward model, the gradient bias is
63"
INTRODUCTION,0.048484848484848485,"proportional to the approximation error, while retaining the variance decay. This quantifies the
64"
INTRODUCTION,0.04924242424242424,"accuracy required from a learned forward model. (iv) We implement a differentiable deep version
65"
INTRODUCTION,0.05,"of SoftTreeMax that employs a parallelized GPU tree expansion. We demonstrate how its gradient
66"
INTRODUCTION,0.05075757575757576,"variance is reduced by three orders of magnitude over PPO while obtaining up to 5x reward.
67"
PRELIMINARIES,0.051515151515151514,"2
Preliminaries
68"
PRELIMINARIES,0.05227272727272727,"Let ∆U denote simplex over the set U. Throughout, we consider a discounted Markov Decision
69"
PRELIMINARIES,0.05303030303030303,"Process (MDP) M = (S, A, P, r, γ, ν), where S is a finite state space of size S, A is a finite action
70"
PRELIMINARIES,0.05378787878787879,"space of size A, r : S × A →[0, 1] is the reward function, P : S × A →∆S is the transition
71"
PRELIMINARIES,0.05454545454545454,"function, γ ∈(0, 1) is the discount factor, and ν ∈RS is the initial state distribution. We denote
72"
PRELIMINARIES,0.055303030303030305,"the transition matrix starting from state s by Ps ∈[0, 1]A×S, i.e., [Ps]a,s′ = P(s′|a, s). Similarly,
73"
PRELIMINARIES,0.05606060606060606,"let Rs = r(s, ·) ∈RA denote the corresponding reward vector. Separately, let π : S →∆A be a
74"
PRELIMINARIES,0.056818181818181816,"stationary policy. Let P π and Rπ be the induced transition matrix and reward function, respectively,
75"
PRELIMINARIES,0.05757575757575758,"i.e., P π(s′|s) = P"
PRELIMINARIES,0.058333333333333334,"a π(a|s) Pr(s′|s, a) and Rπ(s) = P"
PRELIMINARIES,0.05909090909090909,"a π(a|s)r(s, a). Denote the stationary
76"
PRELIMINARIES,0.059848484848484845,"distribution of P π by µπ ∈RS s.t. µ⊤
π P π = P π, and the discounted state visitation frequency
77"
PRELIMINARIES,0.06060606060606061,"by dπ so that d⊤
π = (1 −γ) P∞
t=0 γtν⊤(P π)t. Also, let V π ∈RS be the value function of π
78"
PRELIMINARIES,0.06136363636363636,"defined by V π(s) = Eπ [P∞
t=0 γtr (st, π(st)) | s0 = s], and let Qπ ∈RS×A be the Q-function
79"
PRELIMINARIES,0.06212121212121212,"such that Qπ(s, a) = Eπ [r(s, a) + γV π(s′)]. Our goal is to find an optimal policy π⋆such that
80"
PRELIMINARIES,0.06287878787878788,"V ⋆(s) ≡V π⋆(s) = maxπ V π(s), ∀s ∈S.
81"
PRELIMINARIES,0.06363636363636363,"For the analysis in Section 4, we introduce the following notation. Denote by Θ ∈RS the vector
82"
PRELIMINARIES,0.06439393939393939,"representation of θ(s) ∀s ∈S. For a vector u, denote by exp(u) the coordinate-wise exponent of
83"
PRELIMINARIES,0.06515151515151515,"u and by D(u) the diagonal square matrix with u in its diagonal. For a matrix A, denote its i-th
84"
PRELIMINARIES,0.0659090909090909,"eigenvalue by λi(A). Denote the k-dimensional identity matrix and all-ones vector by Ik and 1k,
85"
PRELIMINARIES,0.06666666666666667,"respectively. Also, denote the trace operator by Tr . Finally, we treat all vectors as column vectors.
86"
POLICY GRADIENT,0.06742424242424243,"2.1
Policy Gradient
87"
POLICY GRADIENT,0.06818181818181818,"PG schemes seek to maximize the cumulative reward as a function of the policy πθ(a|s) by performing
88"
POLICY GRADIENT,0.06893939393939394,"gradient steps on θ. The celebrated Policy Gradient Theorem [Sutton et al., 1999] states that
89"
POLICY GRADIENT,0.0696969696969697,"∂
∂θν⊤V πθ = Es∼dπθ ,a∼πθ(·|s) [∇θ log πθ(a|s)Qπθ(s, a)] ."
POLICY GRADIENT,0.07045454545454545,"The variance of the gradient is thus
90"
POLICY GRADIENT,0.07121212121212121,"Vars∼dπθ ,a∼πθ(·|s) (∇θ log πθ(a|s)Qπθ(s, a)) .
(1)"
POLICY GRADIENT,0.07196969696969698,"In the notation above, we denote the variance of a vector random variable X by
91"
POLICY GRADIENT,0.07272727272727272,"Varx (X) = Tr
h
Ex
h
(X −ExX)⊤(X −ExX)
ii
,"
POLICY GRADIENT,0.07348484848484849,"similarly as in [Greensmith et al., 2004]. From now on, we drop the subscript from Var in (1)
92"
POLICY GRADIENT,0.07424242424242425,"for brevity. When the action space is discrete, a commonly used parameterized policy is softmax:
93"
POLICY GRADIENT,0.075,"πθ(a|s) ∝exp (θ(s, a)) , where θ : S × A →R is a state-action parameterization.
94"
POLICY GRADIENT,0.07575757575757576,"3
SoftTreeMax: Exponent of trajectories
95"
POLICY GRADIENT,0.07651515151515151,"We introduce a new family of policies called SoftTreeMax, which are a model-based generalization
96"
POLICY GRADIENT,0.07727272727272727,"of the popular softmax. We propose two variants: Cumulative (C-SoftTreeMax) and Exponenti-
97"
POLICY GRADIENT,0.07803030303030303,"ated (E-SoftTreeMax). In both variants, we replace the generic softmax logits θ(s, a) with the
98"
POLICY GRADIENT,0.07878787878787878,"score of a trajectory of horizon d starting from (s, a), generated by applying a behavior policy
99"
POLICY GRADIENT,0.07954545454545454,"πb. In C-SoftTreeMax, we exponentiate the expectation of the logits. In E-SoftTreeMax, we first
100"
POLICY GRADIENT,0.0803030303030303,"exponentiate the logits and then only compute their expectation.
101"
POLICY GRADIENT,0.08106060606060606,"Logits. We define the SoftTreeMax logit ℓs,a(d; θ) to be the random variable depicting the score of a
102"
POLICY GRADIENT,0.08181818181818182,"trajectory of horizon d starting from (s, a) and following the policy πb:
103"
POLICY GRADIENT,0.08257575757575758,"ℓs,a(d; θ) = γ−d
""d−1
X"
POLICY GRADIENT,0.08333333333333333,"t=0
γtrt + γdθ(sd) # .
(2)"
POLICY GRADIENT,0.08409090909090909,"In the above expression, note that s0 = s, a0 = a, at ∼πb(·|st) ∀t ≥1, and rt ≡r (st, at) .
104"
POLICY GRADIENT,0.08484848484848485,"For brevity of the analysis, we let the parametric score θ in (2) be state-based, similarly to a value
105"
POLICY GRADIENT,0.0856060606060606,"function. Instead, one could use a state-action input analogous to a Q-function. Thus, SoftTreeMax
106"
POLICY GRADIENT,0.08636363636363636,"can be integrated into the two types of implementation of RL algorithms in standard packages. Lastly,
107"
POLICY GRADIENT,0.08712121212121213,"the preceding γ−d scales the θ parametrization to correspond to its softmax counerpart.
108"
POLICY GRADIENT,0.08787878787878788,"C-SoftTreeMax. Given an inverse temperature parameter β, we let C-SoftTreeMax be
109"
POLICY GRADIENT,0.08863636363636364,"πC
d,θ(a|s) ∝exp [βEπbℓs,a(d; θ)] .
(3)"
POLICY GRADIENT,0.0893939393939394,"C-SoftTreeMax gives higher weight to actions that result in higher expected returns. While standard
110"
POLICY GRADIENT,0.09015151515151515,"softmax relies entirely on parametrization θ, C-SoftTreeMax also interpolates a Monte-Carlo portion
111"
POLICY GRADIENT,0.09090909090909091,"of the reward.
112"
POLICY GRADIENT,0.09166666666666666,"E-SoftTreeMax. The second operator we propose is E-SoftTreeMax:
113"
POLICY GRADIENT,0.09242424242424242,"πE
d,θ(a|s) ∝Eπb exp [(βℓs,a(d; θ))] ;
(4)"
POLICY GRADIENT,0.09318181818181819,"here, the expectation is taken outside the exponent. This objective corresponds to the exponentiated
114"
POLICY GRADIENT,0.09393939393939393,"reward objective which is often used for risk-sensitive RL [Howard and Matheson, 1972, Fei et al.,
115"
POLICY GRADIENT,0.0946969696969697,"2021, Noorani and Baras, 2021]. The common risk-sensitive objective is of the form log E[exp(δR)],
116"
POLICY GRADIENT,0.09545454545454546,"where δ is the risk parameter and R is the cumulative reward. Similarly to that literature, the exponent
117"
POLICY GRADIENT,0.09621212121212121,"in (4) emphasizes the most promising trajectories.
118"
POLICY GRADIENT,0.09696969696969697,"SoftTreeMax properties. SoftTreeMax is a natural model-based generalization of softmax. For
119"
POLICY GRADIENT,0.09772727272727273,"d = 0, both variants above coincide since (2) becomes deterministic. In that case, for a state-action
120"
POLICY GRADIENT,0.09848484848484848,"parametrization, they reduce to standard softmax. When β →0, both variants again coincide and
121"
POLICY GRADIENT,0.09924242424242424,"sample actions uniformly (exploration). When β →∞, the policies become deterministic and
122"
POLICY GRADIENT,0.1,"greedily optimize for the best trajectory (exploitation). For C-SoftTreeMax, the best trajectory is
123"
POLICY GRADIENT,0.10075757575757575,"defined in expectation, while for E-SoftTreeMax it is defined in terms of the best sample path.
124"
POLICY GRADIENT,0.10151515151515152,"SoftTreeMax convergence. Under regularity conditions, for any parametric policy, PG converges
125"
POLICY GRADIENT,0.10227272727272728,"to local optima [Bhatnagar et al., 2009], and thus also SoftTreeMax. For softmax PG, asymptotic
126"
POLICY GRADIENT,0.10303030303030303,"[Agarwal et al., 2021] and rate results [Mei et al., 2020b] were recently obtained, by showing that
127"
POLICY GRADIENT,0.10378787878787879,"the gradient is strictly positive everywhere [Mei et al., 2020b, Lemmas 8-9]. We conjecture that
128"
POLICY GRADIENT,0.10454545454545454,"SoftTreeMax satisfies the same property, being a generalization of softmax, but formally proving it is
129"
POLICY GRADIENT,0.1053030303030303,"subject to future work.
130"
POLICY GRADIENT,0.10606060606060606,"SoftTreeMax gradient. The two variants of SoftTreeMax involve an expectation taken over Sd
131"
POLICY GRADIENT,0.10681818181818181,"many trajectories from the root state s and weighted according to their probability. Thus, during
132"
POLICY GRADIENT,0.10757575757575757,"the PG training process, the gradient ∇θ log πθ is calculated using a weighted sum of gradients over
133"
POLICY GRADIENT,0.10833333333333334,"all reachable states starting from s. Our method exploits the exponential number of trajectories to
134"
POLICY GRADIENT,0.10909090909090909,"reduce the variance while improving performance. Indeed, in the next section we prove that the
135"
POLICY GRADIENT,0.10984848484848485,"gradient variance of SoftTreeMax decays exponentially fast as a function of the behavior policy πb
136"
POLICY GRADIENT,0.11060606060606061,"and trajectory length d. In the experiments in Section 6, we also show how the practical version
137"
POLICY GRADIENT,0.11136363636363636,"of SoftTreeMax achieves a significant reduction in the noise of the PG process and leads to faster
138"
POLICY GRADIENT,0.11212121212121212,"convergence and higher reward.
139"
THEORETICAL ANALYSIS,0.11287878787878788,"4
Theoretical Analysis
140"
THEORETICAL ANALYSIS,0.11363636363636363,"In this section, we first bound the variance of PG when using the SoftTreeMax policy. Later, we
141"
THEORETICAL ANALYSIS,0.1143939393939394,"discuss how the gradient bias resulting due to approximate forward models diminishes as a function
142"
THEORETICAL ANALYSIS,0.11515151515151516,"of the approximation error, while retaining the same variance decay.
143"
THEORETICAL ANALYSIS,0.1159090909090909,"We show that the variance decreases exponentially with the tree depth, and the rate is determined
144"
THEORETICAL ANALYSIS,0.11666666666666667,"by the second eigenvalue of the transition kernel induced by πb. Specifically, we bound the same
145"
THEORETICAL ANALYSIS,0.11742424242424243,"expression for variance as appears in [Greensmith et al., 2004, Sec. 3.5] and [Wu et al., 2018, Sec. A,
146"
THEORETICAL ANALYSIS,0.11818181818181818,"Eq. (21)]. Other types of analysis could instead have focused on the estimation aspect in the context
147"
THEORETICAL ANALYSIS,0.11893939393939394,"of sampling [Zhang et al., 2021, Shen et al., 2019, Pham et al., 2020]. Indeed, in our implementation
148"
THEORETICAL ANALYSIS,0.11969696969696969,"in Section 5, we manage to avoid sampling and directly compute the expectations in Eqs. (3) and
149"
THEORETICAL ANALYSIS,0.12045454545454545,"(4). As we show later, we do so by leveraging efficient parallel simulation on the GPU in feasible
150"
THEORETICAL ANALYSIS,0.12121212121212122,"run-time. In our application, due to the nature of the finite action space and quasi-deterministic Atari
151"
THEORETICAL ANALYSIS,0.12196969696969696,"dynamics [Bellemare et al., 2013], our expectation estimator is noiseless. We encourage future work
152"
THEORETICAL ANALYSIS,0.12272727272727273,"to account for the finite-sample variance component. We defer all the proofs to Appendix A.
153"
THEORETICAL ANALYSIS,0.12348484848484849,"We begin with a general variance bound that holds for any parametric policy.
154"
THEORETICAL ANALYSIS,0.12424242424242424,"Lemma 4.1 (Bound on the policy gradient variance). Let ∇θ log πθ(·|s) ∈RA×dim(θ) be a matrix
155"
THEORETICAL ANALYSIS,0.125,"whose a-th row is ∇θ log πθ(a|s)⊤. For any parametric policy πθ and function Qπθ : S × A →R,
156"
THEORETICAL ANALYSIS,0.12575757575757576,"Var (∇θ log πθ(a|s)Qπθ(s, a)) ≤max
s,a [Qπθ(s, a)]2 max
s ∥∇θ log πθ(·|s)∥2
F ."
THEORETICAL ANALYSIS,0.12651515151515152,"Hence, to bound (1), it is sufficient to bound the Frobenius norm ∥∇θ log πθ(·|s)∥F for any s.
157"
THEORETICAL ANALYSIS,0.12727272727272726,"Note that SoftTreeMax does not reduce the gradient uniformly, which would have been equivalent
158"
THEORETICAL ANALYSIS,0.12803030303030302,"to a trivial change in the learning rate. While the gradient norm shrinks, the gradient itself scales
159"
THEORETICAL ANALYSIS,0.12878787878787878,"differently along the different coordinates. This scaling occurs along different eigenvectors, as a
160"
THEORETICAL ANALYSIS,0.12954545454545455,"function of problem parameters (P, θ) and our choice of behavior policy (πb), as can be seen in
161"
THEORETICAL ANALYSIS,0.1303030303030303,"the proof of the upcoming Theorem 4.4. This allows SoftTreeMax to learn a good “shrinkage” that,
162"
THEORETICAL ANALYSIS,0.13106060606060607,"while reducing the overall gradient, still updates the policy quickly enough. This reduction in norm
163"
THEORETICAL ANALYSIS,0.1318181818181818,"and variance resembles the idea of gradient clipping Zhang et al. [2019], where the gradient is scaled
164"
THEORETICAL ANALYSIS,0.13257575757575757,"to reduce its variance, thus increasing stability and improving overall performance.
165"
THEORETICAL ANALYSIS,0.13333333333333333,"A common assumption in the RL literature [Szepesvári, 2010] that we adopt for the remainder of
166"
THEORETICAL ANALYSIS,0.1340909090909091,"the section is that the transition matrix P πb, induced by the behavior policy πb, is irreducible and
167"
THEORETICAL ANALYSIS,0.13484848484848486,"aperiodic. Consequently, its second highest eigenvalue satisfies |λ2(P πb)| < 1.
168"
THEORETICAL ANALYSIS,0.13560606060606062,"From now on, we divide the variance results for the two variants of SoftTreeMax into two subsec-
169"
THEORETICAL ANALYSIS,0.13636363636363635,"tions. For C-SoftTreeMax, the analysis is simpler and we provide an exact bound. The case of
170"
THEORETICAL ANALYSIS,0.13712121212121212,"E-SoftTreeMax is more involved and we provide for it a more general result. In both cases, we show
171"
THEORETICAL ANALYSIS,0.13787878787878788,"that the variance decays exponentially with the planning horizon.
172"
VARIANCE OF C-SOFTTREEMAX,0.13863636363636364,"4.1
Variance of C-SoftTreeMax
173"
VARIANCE OF C-SOFTTREEMAX,0.1393939393939394,"We express C-SoftTreeMax in vector form as follows.
174"
VARIANCE OF C-SOFTTREEMAX,0.14015151515151514,"Lemma 4.2 (Vector form of C-SoftTreeMax). For d ≥1, (3) is given by
175"
VARIANCE OF C-SOFTTREEMAX,0.1409090909090909,"πC
d,θ(·|s) =
exp
h
β

Cs,d + Ps (P πb)d−1 Θ
i"
VARIANCE OF C-SOFTTREEMAX,0.14166666666666666,"1⊤
A exp
h
β

Cs,d + Ps (P πb)d−1 Θ
i,
(5)"
VARIANCE OF C-SOFTTREEMAX,0.14242424242424243,"where
176"
VARIANCE OF C-SOFTTREEMAX,0.1431818181818182,"Cs,d = γ−dRs + Ps"
VARIANCE OF C-SOFTTREEMAX,0.14393939393939395,"""d−1
X"
VARIANCE OF C-SOFTTREEMAX,0.14469696969696969,"h=1
γh−d (P πb)h−1
# Rπb."
VARIANCE OF C-SOFTTREEMAX,0.14545454545454545,"The vector Cs,d ∈RA represents the cumulative discounted reward in expectation along the trajectory
177"
VARIANCE OF C-SOFTTREEMAX,0.1462121212121212,"of horizon d. This trajectory starts at state s, involves an initial reward dictated by Rs and an
178"
VARIANCE OF C-SOFTTREEMAX,0.14696969696969697,"initial transition as per Ps. Thereafter, it involves rewards and transitions specified by Rπb and P πb,
179"
VARIANCE OF C-SOFTTREEMAX,0.14772727272727273,"respectively. Once the trajectory reaches depth d, the score function θ(sd) is applied,.
180"
VARIANCE OF C-SOFTTREEMAX,0.1484848484848485,"Lemma 4.3 (Gradient of C-SoftTreeMax). The C-SoftTreeMax gradient is given by
181"
VARIANCE OF C-SOFTTREEMAX,0.14924242424242423,"∇θ log πC
d,θ = β

IA −1A(πC
d,θ)⊤
Ps (P πb)d−1 ,"
VARIANCE OF C-SOFTTREEMAX,0.15,"in RA×S, where for brevity, we drop the s index in the policy above, i.e., πC
d,θ ≡πC
d,θ(·|s).
182"
VARIANCE OF C-SOFTTREEMAX,0.15075757575757576,"We are now ready to present our first main result:
183"
VARIANCE OF C-SOFTTREEMAX,0.15151515151515152,"Theorem 4.4 (Variance decay of C-SoftTreeMax). For every Q : S × A →R, the C-SoftTreeMax
184"
VARIANCE OF C-SOFTTREEMAX,0.15227272727272728,"policy gradient variance is bounded by
185"
VARIANCE OF C-SOFTTREEMAX,0.15303030303030302,"Var
 
∇θ log πC
d,θ(a|s)Q(s, a)

≤2 A2S2β2"
VARIANCE OF C-SOFTTREEMAX,0.15378787878787878,(1 −γ)2 |λ2(P πb)|2(d−1).
VARIANCE OF C-SOFTTREEMAX,0.15454545454545454,"We provide the full proof in Appendix A.4, and briefly outline its essence here.
186"
VARIANCE OF C-SOFTTREEMAX,0.1553030303030303,"Proof outline. Lemma 4.1 allows us to bound the variance using a direct bound on the gradient
187"
VARIANCE OF C-SOFTTREEMAX,0.15606060606060607,"norm. The gradient is given in Lemma 4.3 as a product of three matrices, which we now study from
188"
VARIANCE OF C-SOFTTREEMAX,0.15681818181818183,"right to left. The matrix P πb is a row-stochastic matrix. Because the associated Markov chain is
189"
VARIANCE OF C-SOFTTREEMAX,0.15757575757575756,"irreducible and aperiodic, it has a unique stationary distribution. This implies that P πb has one and
190"
VARIANCE OF C-SOFTTREEMAX,0.15833333333333333,"only one eigenvalue equal to 1; all others have magnitude strictly less than 1. Let us suppose that
191"
VARIANCE OF C-SOFTTREEMAX,0.1590909090909091,"all these other eigenvalues have multiplicity 1 (the general case with repeated eigenvalues can be
192"
VARIANCE OF C-SOFTTREEMAX,0.15984848484848485,"handled via Jordan decompositions as in [Pelletier, 1998, Lemma1]). Then, P πb has the spectral
193"
VARIANCE OF C-SOFTTREEMAX,0.1606060606060606,"decomposition P πb = 1Sµ⊤
πb + PS
i=2 λiviu⊤
i , where λi is the i-th eigenvalue of P πb (ordered in
194"
VARIANCE OF C-SOFTTREEMAX,0.16136363636363638,"descending order according to their magnitude) and ui and vi are the corresponding left and right
195"
VARIANCE OF C-SOFTTREEMAX,0.1621212121212121,"eigenvectors, respectively, and therefore (P πb)d−1 = 1Sµ⊤
πb + PS
i=2 λd−1
i
viu⊤
i .
196"
VARIANCE OF C-SOFTTREEMAX,0.16287878787878787,"The second matrix in the gradient relation in Lemma 4.3, Ps, is a rectangular transition ma-
197"
VARIANCE OF C-SOFTTREEMAX,0.16363636363636364,"trix that translates the vector of all ones from dimension S to A : Ps1S = 1A. Lastly, the
198"
VARIANCE OF C-SOFTTREEMAX,0.1643939393939394,"first matrix
h
IA −1A(πC
d,θ)⊤i
is a projection whose null-space includes the vector 1A, i.e.,
199
h
IA −1A(πC
d,θ)⊤i
1A = 0. Combining the three properties above when multiplying the three matri-
200"
VARIANCE OF C-SOFTTREEMAX,0.16515151515151516,"ces of the gradient, it is easy to see that the first term in the expression for (P πb)d−1 gets canceled,
201"
VARIANCE OF C-SOFTTREEMAX,0.16590909090909092,"and we are left with bounded summands scaled by λi(P πb)d−1. Recalling that |λi(P πb)| < 1 and
202"
VARIANCE OF C-SOFTTREEMAX,0.16666666666666666,"that |λ2| ≥|λ3| ≥. . . for i = 2, . . . , S, we obtain the desired result.
203"
VARIANCE OF C-SOFTTREEMAX,0.16742424242424242,"Theorem 4.4 guarantees that the variance of the gradient decays exponentially with d. It also provides
204"
VARIANCE OF C-SOFTTREEMAX,0.16818181818181818,"a novel insight for choosing the behavior policy πb as the policy that minimizes the absolute second
205"
VARIANCE OF C-SOFTTREEMAX,0.16893939393939394,"eigenvalue of the P πb. Indeed, the second eigenvalue of a Markov chain relates to its connectivity
206"
VARIANCE OF C-SOFTTREEMAX,0.1696969696969697,"and its rate of convergence to the stationary distribution [Levin and Peres, 2017].
207"
VARIANCE OF C-SOFTTREEMAX,0.17045454545454544,"Optimal variance decay. For the strongest reduction in variance, the behavior policy πb should be
208"
VARIANCE OF C-SOFTTREEMAX,0.1712121212121212,"chosen to achieve an induced Markov chain whose transitions are state-independent. In that case, P πb
209"
VARIANCE OF C-SOFTTREEMAX,0.17196969696969697,"is a rank one matrix of the form 1Sµ⊤
πb, and λ2(P πb) = 0. Then, Var (∇θ log πθ(a|s)Q(s, a)) = 0.
210"
VARIANCE OF C-SOFTTREEMAX,0.17272727272727273,"Naturally, this can only be done for pathological MDPs; see Appendix C.1 for a more detailed
211"
VARIANCE OF C-SOFTTREEMAX,0.1734848484848485,"discussion. Nevertheless, as we show in Section 5, we choose our tree expansion policy to reduce the
212"
VARIANCE OF C-SOFTTREEMAX,0.17424242424242425,"variance as best as possible.
213"
VARIANCE OF C-SOFTTREEMAX,0.175,"Worst-case variance decay. In contrast, and somewhat surprisingly, when πb is chosen so that the
214"
VARIANCE OF C-SOFTTREEMAX,0.17575757575757575,"dynamics is deterministic, there is no guarantee that it will decay exponentially fast. For example, if
215"
VARIANCE OF C-SOFTTREEMAX,0.1765151515151515,"P πb is a permutation matrix, then λ2(P πb) = 1, and advancing the tree amounts to only updating the
216"
VARIANCE OF C-SOFTTREEMAX,0.17727272727272728,"gradient of one state for every action, as in the basic softmax.
217"
VARIANCE OF E-SOFTTREEMAX,0.17803030303030304,"4.2
Variance of E-SoftTreeMax
218"
VARIANCE OF E-SOFTTREEMAX,0.1787878787878788,"The proof of the variance bound for E-SoftTreeMax is similar to that of C-SoftTreeMax, but more
219"
VARIANCE OF E-SOFTTREEMAX,0.17954545454545454,"involved. It also requires the assumption that the reward depends only on the state, i.e. r(s, a) ≡r(s).
220"
VARIANCE OF E-SOFTTREEMAX,0.1803030303030303,"This is indeed the case in most standard RL environments such as Atari and Mujoco.
221"
VARIANCE OF E-SOFTTREEMAX,0.18106060606060606,"Lemma 4.5 (Vector form of E-SoftTreeMax). For d ≥1, (4) is given by
222"
VARIANCE OF E-SOFTTREEMAX,0.18181818181818182,"πE
d,θ(·|s) =
Es,d exp(βΘ)
1⊤
AEs,d exp(βΘ),
(6)"
VARIANCE OF E-SOFTTREEMAX,0.18257575757575759,"where
223"
VARIANCE OF E-SOFTTREEMAX,0.18333333333333332,"Es,d = Ps d−1
Y h=1"
VARIANCE OF E-SOFTTREEMAX,0.18409090909090908," 
D
 
exp(βγh−dR)

P πb
."
VARIANCE OF E-SOFTTREEMAX,0.18484848484848485,"The vector R above is the S-dimensional vector whose s-th coordinate is r(s).
224"
VARIANCE OF E-SOFTTREEMAX,0.1856060606060606,"The matrix Es,d ∈RA×S has a similar role to Cs,d from (5), but it represents the exponentiated
225"
VARIANCE OF E-SOFTTREEMAX,0.18636363636363637,"cumulative discounted reward. Accordingly, it is a product of d matrices as opposed to a sum. It
226"
VARIANCE OF E-SOFTTREEMAX,0.18712121212121213,"captures the expected reward sequence starting from s and then iteratively following P πb. After d
227"
VARIANCE OF E-SOFTTREEMAX,0.18787878787878787,"steps, we apply the score function on the last state as in (6).
228"
VARIANCE OF E-SOFTTREEMAX,0.18863636363636363,"Lemma 4.6 (Gradient of E-SoftTreeMax). The E-SoftTreeMax gradient is given by
229"
VARIANCE OF E-SOFTTREEMAX,0.1893939393939394,"∇θ log πE
d,θ = β

IA −1A(πE
d,θ)⊤
×
D

πE
d,θ
−1
Es,dD(exp(βΘ))"
VARIANCE OF E-SOFTTREEMAX,0.19015151515151515,"1⊤
AEs,d exp(βΘ)
∈
RA×S,"
VARIANCE OF E-SOFTTREEMAX,0.19090909090909092,"where for brevity, we drop the s index in the policy above, i.e., πE
d,θ ≡πE
d,θ(·|s).
230"
VARIANCE OF E-SOFTTREEMAX,0.19166666666666668,"This gradient structure is harder to handle than that of C-SoftTreeMax in Lemma 4.3, but here we
231"
VARIANCE OF E-SOFTTREEMAX,0.19242424242424241,"also can bound the decay of the variance nonetheless.
232"
VARIANCE OF E-SOFTTREEMAX,0.19318181818181818,"Theorem 4.7 (Variance decay of E-SoftTreeMax). There exists α ∈(0, 1) such that,
233"
VARIANCE OF E-SOFTTREEMAX,0.19393939393939394,"Var
 
∇θ log πE
d,θ(a|s)Q(s, a)

∈O
 
β2α2d
,"
VARIANCE OF E-SOFTTREEMAX,0.1946969696969697,"for every Q. Further, if P πb is reversible or if the reward is constant, then α = |λ2(P πb)|.
234"
VARIANCE OF E-SOFTTREEMAX,0.19545454545454546,"Theory versus Practice. We demonstrate the above result in simulation. We draw a random finite
235"
VARIANCE OF E-SOFTTREEMAX,0.1962121212121212,"MDP, parameter vector Θ ∈RS
+, and behavior policy πb. We then empirically compute the PG
236"
VARIANCE OF E-SOFTTREEMAX,0.19696969696969696,"variance of E-SoftTreeMax as given in (1) and compare it to |λ2(P πb)|d. We repeat this experiment
237"
VARIANCE OF E-SOFTTREEMAX,0.19772727272727272,"three times for different P πb : (i) close to uniform, (ii) drawn randomly, and (iii) close to a permutation
238"
VARIANCE OF E-SOFTTREEMAX,0.1984848484848485,"matrix. As seen in Figure 1, the empirical variance and our bound match almost identically. This
239"
VARIANCE OF E-SOFTTREEMAX,0.19924242424242425,"also suggests that α = |λ2(P πb)| in the general case and not only when P πb is reversible or when
240"
VARIANCE OF E-SOFTTREEMAX,0.2,"the reward is constant.
241"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.20075757575757575,"4.3
Bias with an Approximate Forward Model
242"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.2015151515151515,"The definition of the two SoftTreeMax variants involves the knowledge of the underlying environment,
243"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.20227272727272727,"in particular the value of P and r. However, in practice, we often can only learn approximations of
244"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.20303030303030303,"the dynamics from interactions, e.g., using NNs [Ha and Schmidhuber, 2018, Schrittwieser et al.,
245"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.2037878787878788,"2020]. Let ˆP and ˆr denote the approximate kernel and reward functions, respectively. In this section,
246"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.20454545454545456,"we study the consequences of the approximation error on the C-SoftTreeMax gradient.
247"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.2053030303030303,"2
4
6
8
10
Depth d 10
34 10
28 10
22 10
16 10
10 10
4 102"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.20606060606060606,SoftTreeMax
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.20681818181818182,Gradient variance
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.20757575757575758,"Permutation: Empirical variance
Permutation: Variance bound
Random: Empirical variance
Random: Variance bound
Uniform: Empirical variance
Uniform: Variance bound"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.20833333333333334,"Figure 1: A comparison of the empirical PG
variance and our bound for E-SoftTreeMax
on randomly drawn MDPs. We present three
cases for P πb : (i) close to uniform, (ii) drawn
randomly, and (iii) close to a permutation ma-
trix.
This experiment verifies the optimal
and worse-case rate decay cases. The vari-
ance bounds here are taken from Theorem 4.7
where we substitute α = |λ2(P πb)|. To ac-
count for the constants, we match the values
for the first point in d = 1."
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.20909090909090908,"𝑊(𝑆!""# $,$ )"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.20984848484848484,"logits 
for"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.2106060606060606,"logits 
for"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.21136363636363636,"logits 
for 𝑎!""$"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.21212121212121213,"($)
Policy 
network 𝑎!""$ (() 𝑎!""$ ($) 𝑎!""$ (() 𝑎!""$ ($) 𝑎!""$ (() 𝑎!"") ($) 𝑎!"") (#) 𝑎!"") (() 𝑎!""# (%) 𝑎!""# (') 𝑎!""# (() 𝑆!"") 𝑆!""$ ($) 𝑆!""$ (#) 𝑆!""$ (() 𝑆!""# ($,$) 𝑆!""# (#,$) 𝑆!""# ($,() 𝑆!""# (#,() 𝑆!""# ((,$) 𝑆!""# ((,()"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.2128787878787879,"𝑊(𝑆!""# $,( )"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.21363636363636362,"𝑊(𝑆!""# #,$ )"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.2143939393939394,"𝑊(𝑆!""# #,( )"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.21515151515151515,"𝑊(𝑆!""# (,$ )"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.2159090909090909,"𝑊(𝑆!""# (,( )"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.21666666666666667,"Figure 2: SoftTreeMax policy. Our exhaus-
tive parallel tree expansion iterates on all ac-
tions at each state up to depth d (= 2 here).
The leaf state of every trajectory is used as
input to the policy network. The output is
then added to the trajectory’s cumulative re-
ward as described in (2). I.e., instead of the
standard softmax logits, we add the cumula-
tive discounted reward to the policy network
output. This policy is differentiable and can
be easily integrated into any PG algorithm. In
this work, we build on PPO and use its loss
function to train the policy network."
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.21742424242424244,"Let ˆπC
d,θ be the C-SoftTreeMax policy defined given the approximate forward model introduced
248"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.21818181818181817,"above. That is, let ˆπC
d,θ be defined exactly as in (5), but using ˆRs, ˆPs, ˆRπb and ˆP πb, instead of their
249"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.21893939393939393,"unperturbed counterparts from Section 2. Then, the variance of the corresponding gradient again
250"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.2196969696969697,"decays exponentially with a decay rate of λ2( ˆP πb). However, a gradient bias is introduced. In the
251"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.22045454545454546,"following, we bound this bias in terms of the approximation error and other problem parameters. The
252"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.22121212121212122,"proof is provided in Appendix A.9.
253"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.22196969696969698,"Theorem 4.8. Let ϵ be the maximal model mis-specification, i.e., let max{∥P −ˆP∥, ∥r −ˆr∥} = ϵ.
254"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.22272727272727272,"Then the policy gradient bias due to ˆπC
d,θ satisfies
255 ∂
∂θ"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.22348484848484848,"
ν⊤V πC
d,θ

−∂ ∂θ"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.22424242424242424,"
ν⊤V ˆπC
d,θ
 =
O

1
(1 −γ)2 Sβ2dϵ

.
(7)"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.225,"To the best of our knowledge, Theorem 4.8 is the first result that bounds the bias of the gradient
256"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.22575757575757577,"of a parametric policy due to an approximate model. It states that if the learned model is accurate
257"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.2265151515151515,"enough, we expect similar convergence properties for C-SoftTreeMax as we would have obtained
258"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.22727272727272727,"with the true dynamics. It also suggests that higher temperature (lower β) reduces the bias. In this
259"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.22803030303030303,"case, the logits get less weight, with the extreme of β = 0 corresponding to a uniform policy that has
260"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.2287878787878788,"no bias. Lastly, the error scales linearly with d : the policy suffers from cumulative error as it relies
261"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.22954545454545455,"on further-looking states in the approximate model.
262"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.23030303030303031,"5
SoftTreeMax: Deep Parallel Implementation
263"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.23106060606060605,"Following impressive successes of deep RL [Mnih et al., 2015, Silver et al., 2016], using deep NNs
264"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.2318181818181818,"in RL is standard practice. Depending on the RL algorithm, a loss function is defined and gradients
265"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.23257575757575757,"on the network weights can be calculated. In PG methods, the scoring function used in the softmax is
266"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.23333333333333334,"commonly replaced by a neural network Wθ: πθ(a|s) ∝exp (Wθ(s, a)) . Similarly, we implement
267"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.2340909090909091,"SoftTreeMax by replacing θ(s) in (2) with a neural network Wθ(s). Although both variants of
268"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.23484848484848486,"SoftTreeMax from Section 3 involve computing an expectation, this can be hard in general. One
269"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.2356060606060606,"approach to handle it is with sampling, though these introduce estimation variance into the process.
270"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.23636363636363636,"We leave the question of sample-based theory and algorithmic implementations for future work.
271"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.23712121212121212,"Instead, in finite action space environments such as Atari, we compute the exact expectation in
272"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.23787878787878788,"SoftTreeMax with an exhaustive TS of depth d. Despite the exponential computational cost of
273"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.23863636363636365,"spanning the entire tree, recent advancements in parallel GPU-based simulation allow efficient
274"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.23939393939393938,"expansion of all nodes at the same depth simultaneously [Dalal et al., 2021, Rosenberg et al., 2022].
275"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.24015151515151514,"This is possible when a simulator is implemented on GPU [Dalton et al., 2020, Makoviychuk
276"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.2409090909090909,"et al., 2021, Freeman et al., 2021], or when a forward model is learned [Kim et al., 2020, Ha and
277"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.24166666666666667,"Schmidhuber, 2018]. To reduce the complexity to be linear in depth, we apply tree pruning to a
278"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.24242424242424243,"limited width in all levels. We do so by sub-sampling only the most promising branches at each level.
279"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.2431818181818182,"Limiting the width drastically improves runtime, and enables respecting GPU memory limits, with
280"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.24393939393939393,"only a small sacrifice in performance.
281"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.2446969696969697,"To summarize, in the practical SoftTreeMax algorithm we perform an exhaustive tree expansion with
282"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.24545454545454545,"pruning to obtain trajectories up to depth d. We expand the tree with equal weight to all actions, which
283"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.24621212121212122,"corresponds to a uniform tree expansion policy πb. We apply a neural network on the leaf states, and
284"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.24696969696969698,"accumulate the result with the rewards along each trajectory to obtain the logits in (2). Finally, we
285"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.24772727272727274,"aggregate the results using C-SoftTreeMax. We leave experiments E-SoftTreeMax for future work
286"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.24848484848484848,"on risk-averse RL. During training, the gradient propagates to the NN weights of Wθ. When the
287"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.24924242424242424,"gradient ∇θ log πd,θ is calculated at each time step, it updates Wθ for all leaf states, similarly to
288"
BIAS WITH AN APPROXIMATE FORWARD MODEL,0.25,"Siamese networks [Bertinetto et al., 2016]. An illustration of the policy is given in Figure 2.
289"
EXPERIMENTS,0.25075757575757573,"6
Experiments
290"
EXPERIMENTS,0.2515151515151515,"We conduct our experiments on multiple games from the Atari simulation suite [Bellemare et al.,
291"
EXPERIMENTS,0.25227272727272726,"2013]. As a baseline, we train a PPO [Schulman et al., 2017] agent with 256 GPU workers in parallel
292"
EXPERIMENTS,0.25303030303030305,"[Dalton et al., 2020]. For the tree expansion, we employ a GPU breadth-first as in [Dalal et al., 2021].
293"
EXPERIMENTS,0.2537878787878788,"We then train C-SoftTreeMax 1 for depths d = 1 . . . 8, with a single worker. For depths d ≥3,
294"
EXPERIMENTS,0.2545454545454545,"we limited the tree to a maximum width of 1024 nodes and pruned trajectories with low estimated
295"
EXPERIMENTS,0.2553030303030303,"weights. Since the distributed PPO baseline advances significantly faster in terms of environment
296"
EXPERIMENTS,0.25606060606060604,"steps, for a fair comparison, we ran all experiments for one week on the same machine. For more
297"
EXPERIMENTS,0.25681818181818183,"details see Appendix B.
298"
EXPERIMENTS,0.25757575757575757,"In Figure 3, we plot the reward and variance of SoftTreeMax for each game, as a function of depth.
299"
EXPERIMENTS,0.25833333333333336,"The dashed lines are the results for PPO. Each value is taken after convergence, i.e., the average
300"
EXPERIMENTS,0.2590909090909091,"over the last 20% of the run. The numbers represent the average over five seeds per game. The plot
301"
EXPERIMENTS,0.25984848484848483,"conveys three intriguing conclusions. First, in all games, SoftTreeMax achieves significantly higher
302"
EXPERIMENTS,0.2606060606060606,"reward than PPO. Its gradient variance is also orders of magnitude lower than that of PPO. Second,
303"
EXPERIMENTS,0.26136363636363635,"the reward and variance are negatively correlated and mirror each other in almost all games. This
304"
EXPERIMENTS,0.26212121212121214,"phenomenon demonstrates the necessity of reducing the variance of PG for improving performance.
305"
EXPERIMENTS,0.2628787878787879,"Lastly, each game has a different sweet spot in terms of optimal tree depth. Recall that we limit the
306"
EXPERIMENTS,0.2636363636363636,"run-time in all experiments to one week The deeper the tree, the slower each step and the run consists
307"
EXPERIMENTS,0.2643939393939394,"of less steps. This explains the non-monotone behavior as a function of depth. For a more thorough
308"
EXPERIMENTS,0.26515151515151514,"discussion on the sweet spot of different games, see Appendix B.3.
309"
RELATED WORK,0.26590909090909093,"7
Related Work
310"
RELATED WORK,0.26666666666666666,"Softmax Operator. The softmax policy became a canonical part of PG to the point where theoretical
311"
RELATED WORK,0.2674242424242424,"results of PG focus specifically on it [Zhang et al., 2021, Mei et al., 2020b, Li et al., 2021, Ding et al.,
312"
RELATED WORK,0.2681818181818182,"2022]. Even though we focus on a tree extension to the softmax policy, our methodology is general
313"
RELATED WORK,0.2689393939393939,"and can be easily applied to other discrete or continuous parameterized policies as in [Mei et al.,
314"
RELATED WORK,0.2696969696969697,"2020a, Miahi et al., 2021, Silva et al., 2019]. Tree Search. One famous TS algorithm is Monte-Carlo
315"
RELATED WORK,0.27045454545454545,"TS (MCTS; [Browne et al., 2012]) used in AlphaGo [Silver et al., 2016] and MuZero [Schrittwieser
316"
RELATED WORK,0.27121212121212124,"et al., 2020]. Other algorithms such as Value Iteration, Policy Iteration and DQN were also shown to
317"
RELATED WORK,0.271969696969697,"1We also experimented with E-SoftTreeMax and the results were almost identical. This is due to the quasi-
deterministic nature of Atari, which causes the trajectory logits (2) to have almost no variability. We encourage
future work on E-SoftTreeMax using probabilistic environments that are risk-sensitive."
RELATED WORK,0.2727272727272727,"Figure 3: Reward and Gradient variance: GPU SoftTreeMax (single worker) vs PPO (256 GPU
workers). The blue reward plots show the average of 50 evaluation episodes. The red variance plots
show the average gradient variance of the corresponding training runs, averaged over five seeds. The
dashed lines represent the same for PPO. Note that the variance y-axis is in log-scale."
RELATED WORK,0.2734848484848485,"give an improved performance with a tree search extensions [Efroni et al., 2019, Dalal et al., 2021].
318"
RELATED WORK,0.27424242424242423,"Parallel Environments. In this work we used accurate parallel models that are becoming more
319"
RELATED WORK,0.275,"common with the increasing popularity of GPU-based simulation [Makoviychuk et al., 2021, Dalton
320"
RELATED WORK,0.27575757575757576,"et al., 2020, Freeman et al., 2021]. Alternatively, in relation to Theorem 4.8, one can rely on recent
321"
RELATED WORK,0.2765151515151515,"works that learn the underlying model [Ha and Schmidhuber, 2018, Schrittwieser et al., 2020] and
322"
RELATED WORK,0.2772727272727273,"use an approximation of the true dynamics. Risk Aversion. Previous work considered exponential
323"
RELATED WORK,0.278030303030303,"utility functions for risk aversion [Chen et al., 2007, Garcıa and Fernández, 2015, Fei et al., 2021].
324"
RELATED WORK,0.2787878787878788,"This utility function is the same as E-SoftTreeMax formulation from (4), but we have it directly
325"
RELATED WORK,0.27954545454545454,"in the policy instead of the objective. Reward-free RL. We showed that the gradient variance is
326"
RELATED WORK,0.2803030303030303,"minimized when the transitions induced by the behavior policy πb are uniform. This is expressed by
327"
RELATED WORK,0.28106060606060607,"the second eigenvalue of the transition matrix P πb. This notion of uniform exploration is common to
328"
RELATED WORK,0.2818181818181818,"the reward-free RL setup [Jin et al., 2020]. Several such works also considered the second eigenvalue
329"
RELATED WORK,0.2825757575757576,"in their analysis [Liu and Brunskill, 2018, Tarbouriech and Lazaric, 2019].
330"
DISCUSSION,0.2833333333333333,"8
Discussion
331"
DISCUSSION,0.2840909090909091,"In this work, we introduced for the first time a differentiable parametric policy that combines TS with
332"
DISCUSSION,0.28484848484848485,"PG. We proved that SoftTreeMax is essentially a variance reduction technique and explained how to
333"
DISCUSSION,0.2856060606060606,"choose the expansion policy to minimize the gradient variance. It is an open question whether optimal
334"
DISCUSSION,0.2863636363636364,"variance reduction corresponds to the appealing regret properties the were put forward by UCT
335"
DISCUSSION,0.2871212121212121,"[Kocsis and Szepesvári, 2006]. We believe that this can be answered by analyzing the convergence
336"
DISCUSSION,0.2878787878787879,"rate of SoftTreeMax, relying on the bias and variance results we obtained here.
337"
DISCUSSION,0.28863636363636364,"As the learning process continues, the norm of the gradient and the variance both become smaller.
338"
DISCUSSION,0.28939393939393937,"On the face of it, one can ask if the gradient becomes small as fast as the variance or even faster can
339"
DISCUSSION,0.29015151515151516,"there be any meaningful learning? As we showed in the experiments, learning happens because the
340"
DISCUSSION,0.2909090909090909,"variance reduces fast enough (a variance of 0 represents deterministic learning, which is fastest).
341"
DISCUSSION,0.2916666666666667,"Finally, our work can be extended to infinite action spaces. The analysis can be extended to infinite-
342"
DISCUSSION,0.2924242424242424,"dimension kernels that retain the same key properties used in our proofs. In the implementation, the
343"
DISCUSSION,0.29318181818181815,"tree of continuous actions can be expanded by maintaining a parametric distribution over actions that
344"
DISCUSSION,0.29393939393939394,"depend on θ. This approach can be seen as a tree adaptation of MPPI [Williams et al., 2017].
345"
REPRODUCIBILITY AND LIMITATIONS,0.2946969696969697,"9
Reproducibility and Limitations
346"
REPRODUCIBILITY AND LIMITATIONS,0.29545454545454547,"In this submission, we include the code as part of the supplementary material. We also include a
347"
REPRODUCIBILITY AND LIMITATIONS,0.2962121212121212,"docker file for setting up the environment and a README file with instructions on how to run both
348"
REPRODUCIBILITY AND LIMITATIONS,0.296969696969697,"training and evaluation. The environment engine is an extension of Atari-CuLE [Dalton et al., 2020],
349"
REPRODUCIBILITY AND LIMITATIONS,0.29772727272727273,"a CUDA-based Atari emulator that runs on GPU. Our usage of a GPU environment is both a novelty
350"
REPRODUCIBILITY AND LIMITATIONS,0.29848484848484846,"and a current limitation of our work.
351"
REFERENCES,0.29924242424242425,"References
352"
REFERENCES,0.3,"A. Agarwal, S. M. Kakade, J. D. Lee, and G. Mahajan. On the theory of policy gradient methods:
353"
REFERENCES,0.3007575757575758,"Optimality, approximation, and distribution shift. J. Mach. Learn. Res., 22(98):1–76, 2021.
354"
REFERENCES,0.3015151515151515,"M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An
355"
REFERENCES,0.30227272727272725,"evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253–279,
356"
REFERENCES,0.30303030303030304,"2013.
357"
REFERENCES,0.3037878787878788,"L. Bertinetto, J. Valmadre, J. F. Henriques, A. Vedaldi, and P. H. Torr. Fully-convolutional siamese
358"
REFERENCES,0.30454545454545456,"networks for object tracking. In European conference on computer vision, pages 850–865. Springer,
359"
REFERENCES,0.3053030303030303,"2016.
360"
REFERENCES,0.30606060606060603,"S. Bhatnagar, R. S. Sutton, M. Ghavamzadeh, and M. Lee. Natural actor–critic algorithms. Automatica,
361"
REFERENCES,0.3068181818181818,"45(11):2471–2482, 2009.
362"
REFERENCES,0.30757575757575756,"C. B. Browne, E. Powley, D. Whitehouse, S. M. Lucas, P. I. Cowling, P. Rohlfshagen, S. Tavener,
363"
REFERENCES,0.30833333333333335,"D. Perez, S. Samothrakis, and S. Colton. A survey of monte carlo tree search methods. IEEE
364"
REFERENCES,0.3090909090909091,"Transactions on Computational Intelligence and AI in games, 4(1):1–43, 2012.
365"
REFERENCES,0.3098484848484849,"S. Chatterjee and E. Seneta. Towards consensus: Some convergence theorems on repeated averaging.
366"
REFERENCES,0.3106060606060606,"Journal of Applied Probability, 14(1):89–97, 1977.
367"
REFERENCES,0.31136363636363634,"X. Chen, M. Sim, D. Simchi-Levi, and P. Sun. Risk aversion in inventory management. Operations
368"
REFERENCES,0.31212121212121213,"Research, 55(5):828–842, 2007.
369"
REFERENCES,0.31287878787878787,"G. Dalal, A. Hallak, S. Dalton, S. Mannor, G. Chechik, et al. Improve agents without retraining:
370"
REFERENCES,0.31363636363636366,"Parallel tree search with off-policy correction. Advances in Neural Information Processing Systems,
371"
REFERENCES,0.3143939393939394,"34:5518–5530, 2021.
372"
REFERENCES,0.3151515151515151,"S. Dalton et al. Accelerating reinforcement learning through gpu atari emulation. Advances in Neural
373"
REFERENCES,0.3159090909090909,"Information Processing Systems, 33:19773–19782, 2020.
374"
REFERENCES,0.31666666666666665,"Y. Ding, J. Zhang, and J. Lavaei. On the global optimum convergence of momentum-based policy
375"
REFERENCES,0.31742424242424244,"gradient. In International Conference on Artificial Intelligence and Statistics, pages 1910–1934.
376"
REFERENCES,0.3181818181818182,"PMLR, 2022.
377"
REFERENCES,0.31893939393939397,"Y. Efroni, G. Dalal, B. Scherrer, and S. Mannor. How to combine tree-search methods in reinforcement
378"
REFERENCES,0.3196969696969697,"learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages
379"
REFERENCES,0.32045454545454544,"3494–3501, 2019.
380"
REFERENCES,0.3212121212121212,"Y. Fei, Z. Yang, Y. Chen, and Z. Wang. Exponential bellman equation and improved regret bounds
381"
REFERENCES,0.32196969696969696,"for risk-sensitive reinforcement learning. Advances in Neural Information Processing Systems, 34:
382"
REFERENCES,0.32272727272727275,"20436–20446, 2021.
383"
REFERENCES,0.3234848484848485,"C. D. Freeman, E. Frey, A. Raichuk, S. Girgin, I. Mordatch, and O. Bachem. Brax-a differen-
384"
REFERENCES,0.3242424242424242,"tiable physics engine for large scale rigid body simulation. In Thirty-fifth Conference on Neural
385"
REFERENCES,0.325,"Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021.
386"
REFERENCES,0.32575757575757575,"J. Garcıa and F. Fernández. A comprehensive survey on safe reinforcement learning. Journal of
387"
REFERENCES,0.32651515151515154,"Machine Learning Research, 16(1):1437–1480, 2015.
388"
REFERENCES,0.32727272727272727,"E. Greensmith, P. L. Bartlett, and J. Baxter. Variance reduction techniques for gradient estimates in
389"
REFERENCES,0.328030303030303,"reinforcement learning. Journal of Machine Learning Research, 5(9), 2004.
390"
REFERENCES,0.3287878787878788,"D. Ha and J. Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.
391"
REFERENCES,0.32954545454545453,"R. A. Howard and J. E. Matheson. Risk-sensitive markov decision processes. Management science,
392"
REFERENCES,0.3303030303030303,"18(7):356–369, 1972.
393"
REFERENCES,0.33106060606060606,"C. Jin, A. Krishnamurthy, M. Simchowitz, and T. Yu. Reward-free exploration for reinforcement
394"
REFERENCES,0.33181818181818185,"learning. In International Conference on Machine Learning, pages 4870–4879. PMLR, 2020.
395"
REFERENCES,0.3325757575757576,"S. W. Kim, Y. Zhou, J. Philion, A. Torralba, and S. Fidler. Learning to simulate dynamic environments
396"
REFERENCES,0.3333333333333333,"with gamegan. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
397"
REFERENCES,0.3340909090909091,"Recognition, pages 1231–1240, 2020.
398"
REFERENCES,0.33484848484848484,"L. Kocsis and C. Szepesvári. Bandit based monte-carlo planning. In European conference on machine
399"
REFERENCES,0.33560606060606063,"learning, pages 282–293. Springer, 2006.
400"
REFERENCES,0.33636363636363636,"D. A. Levin and Y. Peres. Markov chains and mixing times, volume 107. American Mathematical
401"
REFERENCES,0.3371212121212121,"Soc., 2017.
402"
REFERENCES,0.3378787878787879,"G. Li, Y. Wei, Y. Chi, Y. Gu, and Y. Chen. Softmax policy gradient methods can take exponential
403"
REFERENCES,0.3386363636363636,"time to converge. In Conference on Learning Theory, pages 3107–3110. PMLR, 2021.
404"
REFERENCES,0.3393939393939394,"Y. Liu and E. Brunskill. When simple exploration is sample efficient: Identifying sufficient conditions
405"
REFERENCES,0.34015151515151515,"for random exploration to yield pac rl algorithms. arXiv preprint arXiv:1805.09045, 2018.
406"
REFERENCES,0.3409090909090909,"Y. Liu, K. Zhang, T. Basar, and W. Yin. An improved analysis of (variance-reduced) policy gradient
407"
REFERENCES,0.3416666666666667,"and natural policy gradient methods. Advances in Neural Information Processing Systems, 33:
408"
REFERENCES,0.3424242424242424,"7624–7636, 2020.
409"
REFERENCES,0.3431818181818182,"V. Makoviychuk, L. Wawrzyniak, Y. Guo, M. Lu, K. Storey, M. Macklin, D. Hoeller, N. Rudin,
410"
REFERENCES,0.34393939393939393,"A. Allshire, A. Handa, et al. Isaac gym: High performance gpu-based physics simulation for robot
411"
REFERENCES,0.3446969696969697,"learning. arXiv preprint arXiv:2108.10470, 2021.
412"
REFERENCES,0.34545454545454546,"A. S. Mathkar and V. S. Borkar. Nonlinear gossip. SIAM Journal on Control and Optimization, 54
413"
REFERENCES,0.3462121212121212,"(3):1535–1557, 2016.
414"
REFERENCES,0.346969696969697,"J. Mei, C. Xiao, B. Dai, L. Li, C. Szepesvári, and D. Schuurmans. Escaping the gravitational pull of
415"
REFERENCES,0.3477272727272727,"softmax. Advances in Neural Information Processing Systems, 33:21130–21140, 2020a.
416"
REFERENCES,0.3484848484848485,"J. Mei, C. Xiao, C. Szepesvari, and D. Schuurmans. On the global convergence rates of softmax
417"
REFERENCES,0.34924242424242424,"policy gradient methods. In International Conference on Machine Learning, pages 6820–6829.
418"
REFERENCES,0.35,"PMLR, 2020b.
419"
REFERENCES,0.35075757575757577,"E. Miahi, R. MacQueen, A. Ayoub, A. Masoumzadeh, and M. White. Resmax: An alternative
420"
REFERENCES,0.3515151515151515,"soft-greedy operator for reinforcement learning. 2021.
421"
REFERENCES,0.3522727272727273,"V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Ried-
422"
REFERENCES,0.353030303030303,"miller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement
423"
REFERENCES,0.35378787878787876,"learning. nature, 518(7540):529–533, 2015.
424"
REFERENCES,0.35454545454545455,"E. Noorani and J. S. Baras. Risk-sensitive reinforce: A monte carlo policy gradient algorithm for
425"
REFERENCES,0.3553030303030303,"exponential performance criteria. In 2021 60th IEEE Conference on Decision and Control (CDC),
426"
REFERENCES,0.3560606060606061,"pages 1522–1527. IEEE, 2021.
427"
REFERENCES,0.3568181818181818,"M. Papini, D. Binaghi, G. Canonaco, M. Pirotta, and M. Restelli. Stochastic variance-reduced policy
428"
REFERENCES,0.3575757575757576,"gradient. In International conference on machine learning, pages 4026–4035. PMLR, 2018.
429"
REFERENCES,0.35833333333333334,"M. Pelletier. On the almost sure asymptotic behaviour of stochastic algorithms. Stochastic processes
430"
REFERENCES,0.35909090909090907,"and their applications, 78(2):217–244, 1998.
431"
REFERENCES,0.35984848484848486,"N. Pham, L. Nguyen, D. Phan, P. H. Nguyen, M. Dijk, and Q. Tran-Dinh. A hybrid stochastic
432"
REFERENCES,0.3606060606060606,"policy gradient algorithm for reinforcement learning. In International Conference on Artificial
433"
REFERENCES,0.3613636363636364,"Intelligence and Statistics, pages 374–385. PMLR, 2020.
434"
REFERENCES,0.3621212121212121,"A. Raffin, A. Hill, M. Ernestus, A. Gleave, A. Kanervisto, and N. Dormann. Stable baselines3, 2019.
435"
REFERENCES,0.36287878787878786,"A. Rosenberg, A. Hallak, S. Mannor, G. Chechik, and G. Dalal. Planning and learning with adaptive
436"
REFERENCES,0.36363636363636365,"lookahead. arXiv preprint arXiv:2201.12403, 2022.
437"
REFERENCES,0.3643939393939394,"J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A. Guez, E. Lockhart,
438"
REFERENCES,0.36515151515151517,"D. Hassabis, T. Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned
439"
REFERENCES,0.3659090909090909,"model. Nature, 588(7839):604–609, 2020.
440"
REFERENCES,0.36666666666666664,"J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization
441"
REFERENCES,0.36742424242424243,"algorithms. arXiv preprint arXiv:1707.06347, 2017.
442"
REFERENCES,0.36818181818181817,"Z. Shen, A. Ribeiro, H. Hassani, H. Qian, and C. Mi. Hessian aided policy gradient. In International
443"
REFERENCES,0.36893939393939396,"conference on machine learning, pages 5729–5738. PMLR, 2019.
444"
REFERENCES,0.3696969696969697,"A. Silva, T. Killian, I. D. J. Rodriguez, S.-H. Son, and M. Gombolay. Optimization methods for inter-
445"
REFERENCES,0.3704545454545455,"pretable differentiable decision trees in reinforcement learning. arXiv preprint arXiv:1903.09338,
446"
REFERENCES,0.3712121212121212,"2019.
447"
REFERENCES,0.37196969696969695,"D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser,
448"
REFERENCES,0.37272727272727274,"I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural
449"
REFERENCES,0.3734848484848485,"networks and tree search. nature, 529(7587):484–489, 2016.
450"
REFERENCES,0.37424242424242427,"R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement
451"
REFERENCES,0.375,"learning with function approximation. Advances in neural information processing systems, 12,
452"
REFERENCES,0.37575757575757573,"1999.
453"
REFERENCES,0.3765151515151515,"C. Szepesvári. Algorithms for reinforcement learning. Synthesis lectures on artificial intelligence
454"
REFERENCES,0.37727272727272726,"and machine learning, 4(1):1–103, 2010.
455"
REFERENCES,0.37803030303030305,"J. Tarbouriech and A. Lazaric. Active exploration in markov decision processes. In The 22nd
456"
REFERENCES,0.3787878787878788,"International Conference on Artificial Intelligence and Statistics, pages 974–982. PMLR, 2019.
457"
REFERENCES,0.3795454545454545,"P. S. Thomas and E. Brunskill. Policy gradient methods for reinforcement learning with function
458"
REFERENCES,0.3803030303030303,"approximation and action-dependent baselines. arXiv preprint arXiv:1706.06643, 2017.
459"
REFERENCES,0.38106060606060604,"G. Williams, N. Wagener, B. Goldfain, P. Drews, J. M. Rehg, B. Boots, and E. A. Theodorou.
460"
REFERENCES,0.38181818181818183,"Information theoretic mpc for model-based reinforcement learning. In 2017 IEEE International
461"
REFERENCES,0.38257575757575757,"Conference on Robotics and Automation (ICRA), pages 1714–1721. IEEE, 2017.
462"
REFERENCES,0.38333333333333336,"C. Wu, A. Rajeswaran, Y. Duan, V. Kumar, A. M. Bayen, S. Kakade, I. Mordatch, and P. Abbeel.
463"
REFERENCES,0.3840909090909091,"Variance reduction for policy gradient with action-dependent factorized baselines. In International
464"
REFERENCES,0.38484848484848483,"Conference on Learning Representations, 2018.
465"
REFERENCES,0.3856060606060606,"P. Xu, F. Gao, and Q. Gu. An improved convergence analysis of stochastic variance-reduced policy
466"
REFERENCES,0.38636363636363635,"gradient. In Uncertainty in Artificial Intelligence, pages 541–551. PMLR, 2020.
467"
REFERENCES,0.38712121212121214,"J. Zhang, T. He, S. Sra, and A. Jadbabaie. Why gradient clipping accelerates training: A theoretical
468"
REFERENCES,0.3878787878787879,"justification for adaptivity. arXiv preprint arXiv:1905.11881, 2019.
469"
REFERENCES,0.3886363636363636,"J. Zhang, C. Ni, C. Szepesvari, M. Wang, et al. On the convergence and sample efficiency of
470"
REFERENCES,0.3893939393939394,"variance-reduced policy gradient method. Advances in Neural Information Processing Systems, 34:
471"
REFERENCES,0.39015151515151514,"2228–2240, 2021.
472"
REFERENCES,0.39090909090909093,"Appendix
473"
REFERENCES,0.39166666666666666,"A
Proofs
474"
REFERENCES,0.3924242424242424,"A.1
Proof of Lemma 4.1 – Bound on the policy gradient variance
475"
REFERENCES,0.3931818181818182,"For any parametric policy πθ and function Q : S × A →R,
476"
REFERENCES,0.3939393939393939,"Var (∇θ log πθ(a|s)Q(s, a)) ≤max
s,a [Q(s, a)]2 max
s
∥∇θ log πθ(·|s)∥2
F ,"
REFERENCES,0.3946969696969697,"where ∇θ log πθ(·|s) ∈RA×dim(θ) is a matrix whose a-th row is ∇θ log πθ(a|s)⊤.
477"
REFERENCES,0.39545454545454545,"Proof. The variance for a parametric policy πθ is given as follows:
478"
REFERENCES,0.39621212121212124,"Var (∇θ log πθ(a|s)Q(a, s)) =Es∼dπθ ,a∼πθ(·|s)

∇θ log πθ(a|s)⊤∇θ log πθ(a|s)Q(s, a)2
−"
REFERENCES,0.396969696969697,"Es∼dπθ ,a∼πθ(·|s) [∇θ log πθ(a|s)Q(s, a)]⊤Es∼dπθ ,a∼πθ(·|s) [∇θ log πθ(a|s)Q(s, a)] ,"
REFERENCES,0.3977272727272727,"where Q(s, a) is the currently estimated Q-function and dπθ is the discounted state visitation frequency
479"
REFERENCES,0.3984848484848485,"induced by the policy πθ. Since the second term we subtract is always positive (it is of quadratic form
480"
REFERENCES,0.39924242424242423,"v⊤v) we can bound the variance by the first term:
481"
REFERENCES,0.4,"Var (∇θ log πθ(a|s)Q(a, s)) ≤Es∼dπθ ,a∼πθ(·|s)

∇θ log πθ(a|s)⊤∇θ log πθ(a|s)Q(s, a)2 =
X"
REFERENCES,0.40075757575757576,"s
dπθ(s)
X"
REFERENCES,0.4015151515151515,"a
πθ(a|s)∇θ log πθ(a|s)⊤∇θ log πθ(a|s)Q(s, a)2"
REFERENCES,0.4022727272727273,"≤max
s,a"
REFERENCES,0.403030303030303,"h
[Q(s, a)]2 πθ(a|s)
i X"
REFERENCES,0.4037878787878788,"s
dπθ(s)
X"
REFERENCES,0.40454545454545454,"a
∇θ log πθ(a|s)⊤∇θ log πθ(a|s)"
REFERENCES,0.4053030303030303,"≤max
s,a [Q(s, a)]2 max
s X"
REFERENCES,0.40606060606060607,"a
∇θ log πθ(a|s)⊤∇θ log πθ(a|s)"
REFERENCES,0.4068181818181818,"= max
s,a [Q(s, a)]2 max
s
∥∇θ log πθ(·|s)∥2
F . 482"
REFERENCES,0.4075757575757576,"A.2
Proof of Lemma 4.2 – Vector form of C-SoftTreeMax
483"
REFERENCES,0.4083333333333333,"In vector form, (3) is given by
484"
REFERENCES,0.4090909090909091,"πC
d,θ(·|s) =
exp
h
β

Cs,d + Ps (P πb)d−1 Θ
i"
REFERENCES,0.40984848484848485,"1⊤
A exp
h
β

Cs,d + Ps (P πb)d−1 Θ
i,
(8)"
REFERENCES,0.4106060606060606,"where
485"
REFERENCES,0.4113636363636364,"Cs,d = γ−dRs + Ps"
REFERENCES,0.4121212121212121,"""d−1
X"
REFERENCES,0.4128787878787879,"h=1
γh−d (P πb)h−1
#"
REFERENCES,0.41363636363636364,"Rπb.
(9)"
REFERENCES,0.41439393939393937,"Proof. Consider the vector ℓs,· ∈R|A|. Its expectation satisfies
486"
REFERENCES,0.41515151515151516,"Eπbℓs,·(d; θ) = Eπb
""d−1
X"
REFERENCES,0.4159090909090909,"t=0
γt−drt + θ(sd) #"
REFERENCES,0.4166666666666667,"= γ−dRs + d−1
X"
REFERENCES,0.4174242424242424,"t=1
γt−dPs(P πb)t−1Rπb + Ps(P πb)d−1Θ."
REFERENCES,0.41818181818181815,"As required.
487"
REFERENCES,0.41893939393939394,"A.3
Proof of Lemma 4.3 – Gradient of C-SoftTreeMax
488"
REFERENCES,0.4196969696969697,"The C-SoftTreeMax gradient of dimension A × S is given by
489"
REFERENCES,0.42045454545454547,"∇θ log πC
d,θ = β

IA −1A(πC
d,θ)⊤
Ps (P πb)d−1 ,"
REFERENCES,0.4212121212121212,"where for brevity, we drop the s index in the policy above, i.e., πC
d,θ ≡πC
d,θ(·|s).
490"
REFERENCES,0.421969696969697,"Proof. The (j, k)-th entry of ∇θ log πC
d,θ satisifes
491"
REFERENCES,0.42272727272727273,"[∇θ log πC
d,θ]j,k =
∂log(πC
d,θ(aj|s))"
REFERENCES,0.42348484848484846,∂θ(sk)
REFERENCES,0.42424242424242425,"= β[Ps(P πb)d−1]j,k − P"
REFERENCES,0.425,"a
h
exp
h
β

Cs,d + Ps (P πb)d−1 Θ
ii"
REFERENCES,0.4257575757575758,"a β

Ps(P πb)d−1 a,k"
REFERENCES,0.4265151515151515,"1⊤
A exp
h
β

Cs,d + Ps (P πb)d−1 Θ
i"
REFERENCES,0.42727272727272725,"= β[Ps(P πb)d−1]j,k −β
X"
REFERENCES,0.42803030303030304,"a
πC
d,θ(a|s)

Ps(P πb)d−1 a,k"
REFERENCES,0.4287878787878788,"= β[Ps(P πb)d−1]j,k −β

(πC
d,θ)⊤Ps(P πb)d−1"
REFERENCES,0.42954545454545456,"k
= β[Ps(P πb)d−1]j,k −β

1A(πC
d,θ)⊤Ps(P πb)d−1 j,k ."
REFERENCES,0.4303030303030303,"Moving back to matrix form, we obtain the stated result.
492"
REFERENCES,0.43106060606060603,"A.4
Proof of Theorem 4.4 – Exponential variance decay of C-SoftTreeMax
493"
REFERENCES,0.4318181818181818,"The C-SoftTreeMax policy gradient is bounded by
494"
REFERENCES,0.43257575757575756,"Var
 
∇θ log πC
d,θ(a|s)Q(s, a)

≤2 A2S2β2"
REFERENCES,0.43333333333333335,(1 −γ)2 |λ2(P πb)|2(d−1).
REFERENCES,0.4340909090909091,"Proof. We use Lemma 4.1 directly. First of all, it is know that when the reward is bounded in [0, 1],
495"
REFERENCES,0.4348484848484849,"the maximal value of the Q-function is
1
1−γ as the sum as infinite discounted rewards. Next, we
496"
REFERENCES,0.4356060606060606,"bound the Frobenius norm of the term achieved in Lemma 4.3, by applying the eigen-decomposition
497"
REFERENCES,0.43636363636363634,"on P πb:
498"
REFERENCES,0.43712121212121213,"P πb = 1Sµ⊤+ S
X"
REFERENCES,0.43787878787878787,"i=2
λiuiv⊤
i ,
(10)"
REFERENCES,0.43863636363636366,"where µ is the stationary distribution of P πb, and ui and vi are left and right eigenvectors correspond-
499"
REFERENCES,0.4393939393939394,"ingly.
500"
REFERENCES,0.4401515151515151,"∥β
 
IA,A −1Aπ⊤
Ps(P πb)d−1∥F = β∥
 
IA,A −1Aπ⊤
Ps  1Sµ⊤+ S
X"
REFERENCES,0.4409090909090909,"i=2
λd−1
i
uiv⊤
i ! ∥F"
REFERENCES,0.44166666666666665,"(Ps is stochastic)
= β∥
 
IA,A −1Aπ⊤ 1Aµ⊤+ S
X"
REFERENCES,0.44242424242424244,"i=2
λd−1
i
Psuiv⊤
i ! ∥F"
REFERENCES,0.4431818181818182,"(projection nullifies 1Aµ⊤)
= β∥
 
IA,A −1Aπ⊤
 S
X"
REFERENCES,0.44393939393939397,"i=2
λd−1
i
Psuiv⊤
i ! ∥F"
REFERENCES,0.4446969696969697,"(triangle inequality)
≤β S
X"
REFERENCES,0.44545454545454544,"i=2
∥
 
IA,A −1Aπ⊤  
λd−1
i
Psuiv⊤
i

∥F"
REFERENCES,0.4462121212121212,"(matrix norm sub-multiplicativity)
≤β|λd−1
2
| S
X"
REFERENCES,0.44696969696969696,"i=2
∥IA,A −1Aπ⊤∥F ∥Ps∥F ∥uiv⊤
i ∥F"
REFERENCES,0.44772727272727275,"= β|λd−1
2
|(S −1)∥IA,A −1Aπ⊤∥F ∥Ps∥F ."
REFERENCES,0.4484848484848485,"Now, we can bound the norm ∥IA,A −1Aπ⊤∥F by direct calculation:
501"
REFERENCES,0.4492424242424242,"∥IA,A −1Aπ⊤∥2
F = Tr
h 
IA,A −1Aπ⊤  
IA,A −1Aπ⊤⊤i
(11)"
REFERENCES,0.45,"= Tr
h
IA,A −1Aπ⊤−π1⊤
A + π⊤π1A1⊤
A
i
(12)"
REFERENCES,0.45075757575757575,"= A −1 −1 + Aπ⊤π
(13)
≤2A.
(14)"
REFERENCES,0.45151515151515154,"From the Cauchy-Schwartz inequality,
502"
REFERENCES,0.45227272727272727,"∥Ps∥2
F =
X a X"
REFERENCES,0.453030303030303,"s
[[Ps]a,s]2 =
X"
REFERENCES,0.4537878787878788,"a
∥[Ps]a,·∥2
2 ≤
X"
REFERENCES,0.45454545454545453,"a
∥[Ps]a,·∥1∥[Ps]a,·∥∞≤A."
REFERENCES,0.4553030303030303,"So,
503"
REFERENCES,0.45606060606060606,"Var
 
∇θ log πC
d,θ(a|s)Q(s, a)

≤max
s,a [Q(s, a)]2 max
s
∥∇θ log πC
d,θ(·|s)∥2
F"
REFERENCES,0.45681818181818185,"≤
1
(1 −γ)2 ∥β
 
IA,A −1Aπ⊤
Ps(P πb)d−1∥2
F"
REFERENCES,0.4575757575757576,"≤
1
(1 −γ)2 β2|λ2(P πb)|2(d−1)S2(2A2),"
REFERENCES,0.4583333333333333,"which obtains the desired bound.
504"
REFERENCES,0.4590909090909091,"A.5
A lower bound on C-SoftTreeMax gradient (result not in the paper)
505"
REFERENCES,0.45984848484848484,"For completeness we also supply a lower bound on the Frobenius norm of the gradient. Note that
506"
REFERENCES,0.46060606060606063,"this result does not translate to the a lower bound on the variance since we have no lower bound
507"
REFERENCES,0.46136363636363636,"equivalence of Lemma 4.1.
508"
REFERENCES,0.4621212121212121,"Lemma A.1. The Frobenius norm on the gradient of the policy is lower-bounded by:
509"
REFERENCES,0.4628787878787879,"∥∇θ log πC
d,θ(·|s)∥F ≥C · β|λ2(P πb)|(d−1).
(15)"
REFERENCES,0.4636363636363636,"Proof. We begin by moving to the induced l2 norm by norm-equivalence:
510"
REFERENCES,0.4643939393939394,"∥β
 
IA,A −1Aπ⊤
Ps(P πb)d−1∥F ≥∥β
 
IA,A −1Aπ⊤
Ps(P πb)d−1∥2."
REFERENCES,0.46515151515151515,"Now, taking the vector u to be the eigenvector of the second eigenvalue of P πb:
511"
REFERENCES,0.4659090909090909,"∥β
 
IA,A −1Aπ⊤
Ps(P πb)d−1∥2 ≥∥β
 
IA,A −1Aπ⊤
Ps(P πb)d−1u∥2
= β∥
 
IA,A −1Aπ⊤
Psu∥2"
REFERENCES,0.4666666666666667,"= β|λ2(P πb)|(d−1)∥
 
IA,A −1Aπ⊤
Psu∥2."
REFERENCES,0.4674242424242424,"Note that even though Psu can be 0, that is not the common case since we can freely change πb (and
512"
REFERENCES,0.4681818181818182,"therefore the eigenvectors of P πb).
513"
REFERENCES,0.46893939393939393,"A.6
Proof of Lemma 4.5 – Vector form of E-SoftTreeMax
514"
REFERENCES,0.4696969696969697,"For d ≥1, (4) is given by
515"
REFERENCES,0.47045454545454546,"πE
d,θ(·|s) =
Es,d exp(βΘ)
1⊤
AEs,d exp(βΘ),
(16)"
REFERENCES,0.4712121212121212,"where
516"
REFERENCES,0.471969696969697,"Es,d = Ps d−1
Y h=1"
REFERENCES,0.4727272727272727," 
D
 
exp[βγh−dR]

P πb
(17)"
REFERENCES,0.4734848484848485,"with R being the |S|-dimensional vector whose s-th coordinate is r(s).
517"
REFERENCES,0.47424242424242424,"Proof. Recall that
518"
REFERENCES,0.475,"ℓs,a(d; θ) = γ−d
"""
REFERENCES,0.47575757575757577,"r(s) + d−1
X"
REFERENCES,0.4765151515151515,"t=1
γtr(st) + γdθ(sd) #"
REFERENCES,0.4772727272727273,".
(18)"
REFERENCES,0.478030303030303,"and, hence,
519"
REFERENCES,0.47878787878787876,"exp[βℓs,a(d; θ)] = exp "" βγ−d"
REFERENCES,0.47954545454545455,"r(s) + d−1
X"
REFERENCES,0.4803030303030303,"t=1
γtr(st) + γdθ(sd) !#"
REFERENCES,0.4810606060606061,".
(19)"
REFERENCES,0.4818181818181818,"Therefore,
520"
REFERENCES,0.4825757575757576,"E[exp βℓs,a(d; θ)] = E "" exp "" βγ−d"
REFERENCES,0.48333333333333334,"r(s) + d−1
X"
REFERENCES,0.48409090909090907,"t=1
γtr(st) !#"
REFERENCES,0.48484848484848486,"E [exp [β (θ(sd))]|s1, . . . , sd−1] # (20) = E "" exp "" βγ−d"
REFERENCES,0.4856060606060606,"r(s) + d−1
X"
REFERENCES,0.4863636363636364,"t=1
γtr(st) !#"
REFERENCES,0.4871212121212121,P πb(·|sd−1) #
REFERENCES,0.48787878787878786,"exp(βΘ)
(21) = E "" exp "" βγ−d"
REFERENCES,0.48863636363636365,"r(s) + d−2
X"
REFERENCES,0.4893939393939394,"t=1
γtr(st) !#"
REFERENCES,0.49015151515151517,exp[βγ−1r(sd−1)]P πb(·|sd−1) #
REFERENCES,0.4909090909090909,exp(βΘ). (22)
REFERENCES,0.49166666666666664,"By repeatedly using iterative conditioning as above, the desired result follows.
Note that
521"
REFERENCES,0.49242424242424243,"exp(βγ−dr(s)) does not depend on the action and is therefore cancelled out with the denomi-
522"
REFERENCES,0.49318181818181817,"nator.
523"
REFERENCES,0.49393939393939396,"A.7
Proof of Lemma 4.6 – Gradient of E-SoftTreeMax
524"
REFERENCES,0.4946969696969697,"The E-SoftTreeMax gradient of dimension A × S is given by
525"
REFERENCES,0.4954545454545455,"∇θ log πE
d,θ = β

IA −1A(πE
d,θ)⊤ D

πE
d,θ
−1
Es,dD(exp(βΘ))"
REFERENCES,0.4962121212121212,"1⊤
AEs,d exp(βΘ)
,"
REFERENCES,0.49696969696969695,"where for brevity, we drop the s index in the policy above, i.e., πE
d,θ ≡πE
d,θ(·|s).
526"
REFERENCES,0.49772727272727274,"Proof. The (j, k)-th entry of ∇θ log πE
d,θ satisfies
527"
REFERENCES,0.4984848484848485,"[∇θ log πE
d,θ]j,k =
∂log(πE
d,θ(aj|s))
∂θ(sk)"
REFERENCES,0.49924242424242427,"=
∂
∂θ(sk)"
REFERENCES,0.5,"
log[(Es,d)⊤
j exp(βΘ)] −log[1⊤
AEs,d exp(βΘ)]
"
REFERENCES,0.5007575757575757,"= β(Es,d)j,k exp(βθ(sk))"
REFERENCES,0.5015151515151515,"(Es,d)⊤
j exp(βΘ)
−β1⊤
AEs,dek exp(βθ(sk))"
REFERENCES,0.5022727272727273,"1⊤
AEs,d exp(βΘ)"
REFERENCES,0.503030303030303,"= β(Es,dek exp(βθ(sk)))j"
REFERENCES,0.5037878787878788,"(Es,d)⊤
j exp(βΘ)
−β1⊤
AEs,dek exp(βθ(sk))"
REFERENCES,0.5045454545454545,"1⊤
AEs,d exp(βΘ) = β"
REFERENCES,0.5053030303030303,"""
e⊤
j
e⊤
j Es,d exp(βΘ) −
1⊤
A
1⊤
AEs,d exp(βΘ) #"
REFERENCES,0.5060606060606061,"Es,dek exp(βθ(sk))."
REFERENCES,0.5068181818181818,"Hence,
528"
REFERENCES,0.5075757575757576,"[∇θ log πE
d,θ]·,k = β
h
D(Es,d exp(βΘ))−1 −(1⊤
AEs,d exp(βΘ))−11A1⊤
A
i
Es,dek exp(βθ(sk))"
REFERENCES,0.5083333333333333,"From this, it follows that
529"
REFERENCES,0.509090909090909,"∇θ log πE
d,θ = β
h
D
 
πE
d,θ
−1 −1A1⊤
A
i Es,dD(exp(βΘ))"
REFERENCES,0.5098484848484849,"1⊤
AEs,d exp(βΘ)
.
(23)"
REFERENCES,0.5106060606060606,"The desired result is now easy to see.
530"
REFERENCES,0.5113636363636364,"A.8
Proof of Theorem 4.7 — Exponential variance decay of E-SoftTreeMax
531"
REFERENCES,0.5121212121212121,"There exists α ∈(0, 1) such that, for any function Q : S × A →R,
532"
REFERENCES,0.5128787878787879,"Var
 
∇θ log πE
d,θ(a|s)Q(s, a)

∈O
 
β2α2d
."
REFERENCES,0.5136363636363637,"If all rewards are equal (r ≡const), then α = |λ2(P πb)|.
533"
REFERENCES,0.5143939393939394,"Proof outline. Recall that thanks to Lemma 4.1, we can bound the PG variance using a direct bound
534"
REFERENCES,0.5151515151515151,"on the gradient norm. The definition of the induced norm is
535"
REFERENCES,0.5159090909090909,"∥∇θ log πE
d,θ∥= max
z:∥z∥=1 ∥∇θ log πE
d,θz∥,"
REFERENCES,0.5166666666666667,"with ∇θ log πE
d,θ given in Lemma 4.6. Let z ∈RS be an arbitrary vector such that ∥z∥= 1. Then,
536"
REFERENCES,0.5174242424242425,"z = PS
i=1 cizi, where ci are scalar coefficients and zi are vectors spanning the S-dimensional space.
537"
REFERENCES,0.5181818181818182,"In the full proof, we show our specific choice of zi and prove they are linearly independent given that
538"
REFERENCES,0.5189393939393939,"choice. We do note that z1 = 1S.
539"
REFERENCES,0.5196969696969697,"The first part of the proof relies on the fact that (∇θ log πE
d,θ)z1 = 0. This is easy to verify using"
REFERENCES,0.5204545454545455,"Lemma 4.6 together with (6), and because
h
IA −1A(πE
d,θ)⊤i
is a projection matrix whose null-space
is spanned by 1S. Thus,"
REFERENCES,0.5212121212121212,"∇θ log πE
d,θz = ∇θ log πE
d,θ S
X"
REFERENCES,0.521969696969697,"i=2
cizi."
REFERENCES,0.5227272727272727,"In the second part of the proof, we focus on Es,d from (6), which appears within ∇θ log πE
d,θ. Notice
540"
REFERENCES,0.5234848484848484,"that Es,d consists of the product Qd−1
h=1
 
D
 
exp(βγh−dR

P πb
. Even though the elements in this
541"
REFERENCES,0.5242424242424243,"product are not stochastic matrices, in the full proof we show how to normalize each of them to a
542"
REFERENCES,0.525,"stochastic matrix Bh. We thus obtain that
543"
REFERENCES,0.5257575757575758,"Es,d = PsD(M1) d−1
Y"
REFERENCES,0.5265151515151515,"h=1
Bh,"
REFERENCES,0.5272727272727272,"where M1 ∈RS is some strictly positive vector. Then, we can apply a result by Mathkar and Borkar
544"
REFERENCES,0.5280303030303031,"[2016], which itself builds on [Chatterjee and Seneta, 1977]. The result states that the product of
545"
REFERENCES,0.5287878787878788,"stochastic matrices Qd−1
h=1 Bh of our particular form converges exponentially fast to a matrix of the
546"
REFERENCES,0.5295454545454545,"form 1Sµ⊤s.t. ∥1Sµ⊤−Qd−1
h=1 Bh∥≤Cαd for some constant C.
547"
REFERENCES,0.5303030303030303,"Lastly, 1Sµ⊤
πb gets canceled due to our choice of zi, i = 2, . . . , S. This observation along with the
548"
REFERENCES,0.531060606060606,"above fact that the remainder decays then shows that ∇θ log πE
d,θ
PS
i=2 zi = O(αd), which gives the
549"
REFERENCES,0.5318181818181819,"desired result.
550"
REFERENCES,0.5325757575757576,"Full technical proof. Let d ≥2. Recall that
551"
REFERENCES,0.5333333333333333,"Es,d = Ps d−1
Y h=1"
REFERENCES,0.5340909090909091," 
D
 
exp[βγh−dR]

P πb
,
(24)"
REFERENCES,0.5348484848484848,"and that R refers to the S-dimensional vector whose s-th coordinate is r(s). Define
552"
REFERENCES,0.5356060606060606,"Bi =
P πb
if i = d −1,
D−1(P πbMi+1)P πbD(Mi+1)
if i = 1, . . . , d −2,
(25)"
REFERENCES,0.5363636363636364,"and the vector
553"
REFERENCES,0.5371212121212121,"Mi =
exp(βγ−1R)
if i = d −1,
exp(βγi−dR) ◦P πbMi+1
if i = 1, . . . , d −2,
(26)"
REFERENCES,0.5378787878787878,"where ◦denotes the element-wise product. Then,
554"
REFERENCES,0.5386363636363637,"Es,d = PsD(M1) d−1
Y"
REFERENCES,0.5393939393939394,"i=1
Bi.
(27)"
REFERENCES,0.5401515151515152,"It is easy to see that each Bi is a row-stochastic matrix, i.e., all entries are non-negative and
555"
REFERENCES,0.5409090909090909,"Bi1S = 1S.
556"
REFERENCES,0.5416666666666666,"Next, we prove that all non-zeros entries of Bi are bounded away from 0 by a constant. This is
557"
REFERENCES,0.5424242424242425,"necessary to apply the next result from Chatterjee and Seneta [1977]. The j-th coordinate of Mi
558"
REFERENCES,0.5431818181818182,"satisfies
559"
REFERENCES,0.543939393939394,"(Mi)j = exp[βγi−dRj]
X"
REFERENCES,0.5446969696969697,"k
[P πb]j,k(Mi+1)k ≤∥exp[βγi−dR]∥∞∥Mi+1∥∞.
(28)"
REFERENCES,0.5454545454545454,"Separately, observe that ∥Md−1∥∞≤∥exp(βγ−1R)∥∞. Plugging these relations in (26) gives
560"
REFERENCES,0.5462121212121213,"∥M1∥∞≤ d−1
Y"
REFERENCES,0.546969696969697,"h=1
∥exp[βγh−dR]∥∞= d−1
Y"
REFERENCES,0.5477272727272727,"h=1
∥exp[βγ−dR]∥γh
∞= ∥exp[βγ−dR]∥"
REFERENCES,0.5484848484848485,"Pd−1
h=1 γh
∞
≤∥exp[βγ−dR]∥"
REFERENCES,0.5492424242424242,"1
1−γ
∞. (29)"
REFERENCES,0.55,"Similarly, for every 1 ≤i ≤d −1, we have that
561"
REFERENCES,0.5507575757575758,"∥Mi∥∞≤ d−1
Y"
REFERENCES,0.5515151515151515,"h=i
∥exp[βγ−dR]∥γh
∞≤∥exp[βγ−dR]∥"
REFERENCES,0.5522727272727272,"1
1−γ
∞.
(30)"
REFERENCES,0.553030303030303,"The jk-th entry of Bi = D−1(P πbMi+1)P πbD(Mi+1) is
562"
REFERENCES,0.5537878787878788,"(Bi)jk =
P πb
jk [Mi+1]k
P|S|
ℓ=1 P πb
jℓ[Mi+1]ℓ
≥
P πb
jk
P|S|
ℓ=1 P πb
jℓ[Mi+1]ℓ
≥
P πb
jk"
REFERENCES,0.5545454545454546,∥exp[βγ−dR]∥
REFERENCES,0.5553030303030303,"1
1−γ
∞
.
(31)"
REFERENCES,0.556060606060606,"Hence, for non-zero P πb
jk , the entries are bounded away from zero by the same. We can now proceed
563"
REFERENCES,0.5568181818181818,"with applying the following result.
564"
REFERENCES,0.5575757575757576,"Now, by [Chatterjee and Seneta, 1977, Theorem 5] (see also (14) in [Mathkar and Borkar, 2016]),
565"
REFERENCES,0.5583333333333333,"limd→∞
Qd−1
i=1 Bi exists and is of the form 1Sµ⊤for some probability vector µ. Furthermore, there
566"
REFERENCES,0.5590909090909091,"is some α ∈(0, 1) such that ε(d) :=
Qd−1
i=1 Bi

−1S µ⊤satisfies
567"
REFERENCES,0.5598484848484848,"∥ε(d)∥= O(αd).
(32)"
REFERENCES,0.5606060606060606,"Pick linearly independent vectors w2, . . . , wS such that
568"
REFERENCES,0.5613636363636364,"µ⊤wi = 0 for i = 2, . . . , d.
(33)"
REFERENCES,0.5621212121212121,"Since PS
i=2 αiwi is perpendicular to µ for any α2, . . . αS and because µ⊤exp(βΘ) > 0, there
569"
REFERENCES,0.5628787878787879,"exists no choice of α2, . . . , αS such that PS
i=2 αiwi = exp(βΘ). Hence, if we let z1 = 1S and
570"
REFERENCES,0.5636363636363636,"zi = D(exp(βΘ))−1wi for i = 2, . . . , S, then it follows that {z1, . . . , zS} is linearly independent.
571"
REFERENCES,0.5643939393939394,"In particular, it implies that {z1, . . . , zS} spans RS.
572"
REFERENCES,0.5651515151515152,"Now consider an arbitrary unit norm vector z := PS
i=1 cizi ∈RS s.t. ∥z∥2 = 1. Then,
573"
REFERENCES,0.5659090909090909,"∇θ log πE
d,θz = ∇θ log πE
d,θ S
X"
REFERENCES,0.5666666666666667,"i=2
cizi
(34)"
REFERENCES,0.5674242424242424,"= β

IA −1A(πE
d,θ)⊤ D

πE
d,θ
−1
Es,dD(exp(βΘ))"
REFERENCES,0.5681818181818182,"1⊤
AEs,d exp(βΘ) S
X"
REFERENCES,0.568939393939394,"i=2
cizi
(35)"
REFERENCES,0.5696969696969697,"= β

IA −1A(πE
d,θ)⊤ D

πE
d,θ
−1
Es,d"
REFERENCES,0.5704545454545454,"1⊤
AEs,d exp(βΘ) S
X"
REFERENCES,0.5712121212121212,"i=2
ciwi
(36)"
REFERENCES,0.571969696969697,"= β

IA −1A(πE
d,θ)⊤ D

πE
d,θ
−1 
1Sµ⊤+ ε(d)
"
REFERENCES,0.5727272727272728,"1⊤
AEs,d exp(βΘ) S
X"
REFERENCES,0.5734848484848485,"i=2
ciwi
(37)"
REFERENCES,0.5742424242424242,"= β

IA −1A(πE
d,θ)⊤ D

πE
d,θ
−1
ε(d)"
REFERENCES,0.575,"1⊤
AEs,d exp(βΘ) S
X"
REFERENCES,0.5757575757575758,"i=2
ciwi
(38)"
REFERENCES,0.5765151515151515,"= β

IA −1A(πE
d,θ)⊤ D

πE
d,θ
−1
ε(d)D(exp(βΘ))"
REFERENCES,0.5772727272727273,"1⊤
AEs,d exp(βΘ)
(z −c11S),
(39)"
REFERENCES,0.578030303030303,"where (34) follows from the fact that ∇θ log πE
d,θz1 = ∇θ log πE
d,θ1S = 0, (35) follows from
574"
REFERENCES,0.5787878787878787,"Lemma 4.6, (36) holds since zi = D(exp(βΘ))−1wi, (38) because µ is perpendicular wi for each i,
575"
REFERENCES,0.5795454545454546,"while (39) follows by reusing zi = D(exp(βΘ))−1wi relation along with the fact that z1 = 1S.
576"
REFERENCES,0.5803030303030303,"From (39), it follows that
577"
REFERENCES,0.581060606060606,"∥∇θ log πE
d,θz∥≤β∥ε(d)∥ "
REFERENCES,0.5818181818181818,"
IA −1A(πE
d,θ)⊤
D

πE
d,θ
−1"
REFERENCES,0.5825757575757575,"1⊤
AEs,d exp(βΘ)"
REFERENCES,0.5833333333333334,∥D(exp(βΘ))∥∥z −c11S∥ (40)
REFERENCES,0.5840909090909091,"≤βαd(∥IA∥+ ∥1A(πE
d,θ)⊤∥) "
REFERENCES,0.5848484848484848,"D

πE
d,θ
−1"
REFERENCES,0.5856060606060606,"1⊤
AEs,d exp(βΘ)"
REFERENCES,0.5863636363636363,"exp(β max
s
θ(s))∥z −c11S∥ (41)"
REFERENCES,0.5871212121212122,"≤βαd(1 +
√ A) "
REFERENCES,0.5878787878787879,"D

πE
d,θ
−1"
REFERENCES,0.5886363636363636,"1⊤
AEs,d exp(βΘ)"
REFERENCES,0.5893939393939394,"exp(β max
s
θ(s))∥z −c11S∥
(42)"
REFERENCES,0.5901515151515152,"≤βαd(1 +
√"
REFERENCES,0.5909090909090909,"A)
D−1(Es,d exp(βΘ))
 exp(β max
s
θ(s))∥z −c11S∥
(43)"
REFERENCES,0.5916666666666667,"≤βαd(1 +
√"
REFERENCES,0.5924242424242424,"A)
1
mins[Es,d exp(βΘ]s
exp(β max
s
θ(s))∥z −c11S∥
(44)"
REFERENCES,0.5931818181818181,"≤βαd(1 +
√"
REFERENCES,0.593939393939394,"A)
exp(β maxs θ(s))
exp(β mins θ(s)) mins |M1|∥z −c11S∥
(45)"
REFERENCES,0.5946969696969697,"≤βαd(1 +
√"
REFERENCES,0.5954545454545455,"A)
exp(β maxs θ(s))
exp(β mins θ(s)) exp(β mins r(s))∥z −c11S∥
(46)"
REFERENCES,0.5962121212121212,"≤βαd(1 +
√"
REFERENCES,0.5969696969696969,"A) exp(β[max
s
θ(s) −min
s
θ(s) −min
s
r(s)])∥z −c11S∥.
(47)"
REFERENCES,0.5977272727272728,"Lastly, we prove that ∥z −c11S∥is bounded independently of d. First, denote by c = (c1, . . . , cS)⊤
578"
REFERENCES,0.5984848484848485,"and ˜c = (0, c2, . . . , cS)⊤. Also, denote by Z the matrix with zi as its i-th column. Now,
579"
REFERENCES,0.5992424242424242,"∥z −c11S∥= ∥ S
X"
REFERENCES,0.6,"i=2
cizi∥
(48)"
REFERENCES,0.6007575757575757,"= ∥Z˜c∥
(49)
≤∥Z∥∥˜c∥
(50)
≤∥Z∥∥c∥
(51)"
REFERENCES,0.6015151515151516,"= ∥Z∥∥Z−1z∥
(52)"
REFERENCES,0.6022727272727273,"≤∥Z∥∥Z−1∥,
(53)"
REFERENCES,0.603030303030303,"where the last relation is due to z being a unit vector. All matrix norms here are l2-induced norms.
580"
REFERENCES,0.6037878787878788,"Next, denote by W the matrix with wi in its i-th column. Recall that in (33) we only defined
581"
REFERENCES,0.6045454545454545,"w2, . . . , wS. We now set w1 = exp(βΘ). Note that w1 is linearly independent of {w2, . . . , wS}
582"
REFERENCES,0.6053030303030303,"because of (33) together with the fact that µ⊤w1 > 0. We can now express the relation between Z
583"
REFERENCES,0.6060606060606061,"and W by Z = D−1(exp(βΘ))W. Substituting this in (53), we have
584"
REFERENCES,0.6068181818181818,"∥z −c11S∥≤∥D−1(exp(βΘ))W∥∥W −1D(exp(βΘ))∥
(54)"
REFERENCES,0.6075757575757575,"≤∥W∥∥W −1∥∥D(exp(βΘ))∥∥D−1(exp(βΘ))∥.
(55)"
REFERENCES,0.6083333333333333,"It further holds that
585"
REFERENCES,0.6090909090909091,"∥D(exp(βΘ))∥≤max
s
exp (βθ(s)) ≤max{1, exp[β max
s
θ(s)])},
(56)"
REFERENCES,0.6098484848484849,"where the last relation equals 1 if θ(s) < 0 for all s. Similarly,
586"
REFERENCES,0.6106060606060606,"∥D−1(exp(βΘ))∥≤
1
mins exp (βθ(s)) ≤
1
min{1, exp[β mins θ(s)])}.
(57)"
REFERENCES,0.6113636363636363,"Furthermore, by the properties of the l2-induced norm,
587"
REFERENCES,0.6121212121212121,"∥W∥2 ≤
√"
REFERENCES,0.6128787878787879,"S∥W∥1
(58) =
√"
REFERENCES,0.6136363636363636,"S max
1≤i≤S ∥wi∥1
(59) =
√"
REFERENCES,0.6143939393939394,"S max{exp(βΘ), max
2≤i≤S ∥wi∥1}
(60) ≤
√"
REFERENCES,0.6151515151515151,"S max{1, exp[β max
s
θ(s)], max
2≤i≤S ∥wi∥1)}.
(61)"
REFERENCES,0.615909090909091,"Lastly,
588"
REFERENCES,0.6166666666666667,"∥W −1∥=
1
σmin(W)
(62) ≤ S−1
Y i=1"
REFERENCES,0.6174242424242424,σmax(W) σi(W)
REFERENCES,0.6181818181818182,"!
1
σmin(W)
(63)"
REFERENCES,0.6189393939393939,= (σmax(W))S−1
REFERENCES,0.6196969696969697,"QS
i=1 σi(W)
(64)"
REFERENCES,0.6204545454545455,= ∥W∥S−1
REFERENCES,0.6212121212121212,"| det(W)|.
(65)"
REFERENCES,0.621969696969697,"The determinant of W is a sum of products involving its entries. To upper bound (65) independently
589"
REFERENCES,0.6227272727272727,"of d, we lower bound its denominator by upper and lower bounds on the entries [W]i,1 that are
590"
REFERENCES,0.6234848484848485,"independent of d, depending on their sign:
591"
REFERENCES,0.6242424242424243,"min{1, exp[β min
s
θ(s)])} ≤[W]i,1 ≤max{1, exp[β max
s
θ(s)])}.
(66)"
REFERENCES,0.625,"Using this, together with (53), (55), (56), (57), and (61), we showed that ∥z−c11S∥is upper bounded
592"
REFERENCES,0.6257575757575757,"by a constant independent of d. This concludes the proof.
593"
REFERENCES,0.6265151515151515,"A.9
Bias Estimates
594"
REFERENCES,0.6272727272727273,"Lemma A.2. For any matrix A and ˆA,"
REFERENCES,0.628030303030303,"ˆAk −Ak = k
X"
REFERENCES,0.6287878787878788,"h=1
ˆAh−1( ˆA −A)Ak−h."
REFERENCES,0.6295454545454545,"Proof. The proof follows from first principles:
595 k
X"
REFERENCES,0.6303030303030303,"h=1
ˆAh−1( ˆA −A)Ak−h = k
X"
REFERENCES,0.6310606060606061,"h=1
ˆAh−1 ˆAAk−h − k
X"
REFERENCES,0.6318181818181818,"h=1
ˆAh−1AAk−h
(67) = k
X"
REFERENCES,0.6325757575757576,"h=1
ˆAhAk−h − k
X"
REFERENCES,0.6333333333333333,"h=1
ˆAh−1Ak−h+1
(68)"
REFERENCES,0.634090909090909,"= ˆAk −Ak + k−1
X"
REFERENCES,0.6348484848484849,"h=1
ˆAhAk−h − k
X"
REFERENCES,0.6356060606060606,"h=2
ˆAh−1Ak−h+1
(69)"
REFERENCES,0.6363636363636364,"= ˆAk −Ak.
(70) 596"
REFERENCES,0.6371212121212121,"Henceforth, ∥· ∥will refer to ∥· ∥∞, i.e. the induced infinity norm. Also, for brevity, we denote πC
d,θ
597"
REFERENCES,0.6378787878787879,"and ˆπC
d,θ by πθ and ˆπθ, respectively. Similarly, we use dπθ and dˆπθ to denote dπC
d,θ and dˆπC
d,θ. As for
598"
REFERENCES,0.6386363636363637,"the induced norm of the matrix P and its perturbed counterpart ˆP, which are of size S × A × S,
599"
REFERENCES,0.6393939393939394,"we slightly abuse notation and denote ∥P −ˆP∥= maxs{∥Ps −ˆPs∥}, where Ps is as defined in
600"
REFERENCES,0.6401515151515151,"Section 2.
601"
REFERENCES,0.6409090909090909,"Definition A.3. Let ϵ be the maximal model mis-specification, i.e., max{∥P −ˆP∥, ∥r −ˆr∥} = ϵ.
602"
REFERENCES,0.6416666666666667,"Lemma A.4. Recall the definitions of Rs, Ps, Rπb and P πb from Section 2, and respectively denote
603"
REFERENCES,0.6424242424242425,"their perturbed counterparts by ˆRs, ˆPs, ˆRπb and ˆP πb. Then, for ϵ defined in Definition A.3,
604"
REFERENCES,0.6431818181818182,"max{∥Rs −ˆRs∥, ∥Ps −ˆPs∥, ∥Rπb −ˆRπb∥, ∥P πb −ˆP πb∥} = O(ϵ).
(71)"
REFERENCES,0.6439393939393939,"Proof. The proof follows easily from the fact that the differences above are convex combinations of
605"
REFERENCES,0.6446969696969697,"P −ˆP and r −ˆr.
606"
REFERENCES,0.6454545454545455,"Lemma A.5. Let πθ be as in (5), and let ˆπθ also be defined as in (5), but with Rs, Ps, P πb replaced
607"
REFERENCES,0.6462121212121212,"by their perturbed counterparts ˆRs, ˆPs, ˆP πb throughout. Then,
608"
REFERENCES,0.646969696969697,"∥πC
d,θ −ˆπC
d,θ∥= O(βdϵ).
(72)"
REFERENCES,0.6477272727272727,"Proof. To prove the desired result, we work with (5) to bound the error between Rs, Ps, P πb, Rπb
609"
REFERENCES,0.6484848484848484,"and their perturbed versions.
610"
REFERENCES,0.6492424242424243,"First, we apply Lemma A.2 together with Lemma A.4 to obtain that ∥(P πb)k −( ˆP πb)k∥= O(kϵ).
Next, denote by M the argument in the exponent in (5), i.e."
REFERENCES,0.65,"M := β[Cs,d + Ps(P πb)d−1Θ]."
REFERENCES,0.6507575757575758,"Similarly, let ˆ
M be the corresponding perturbed sum that relies on ˆP and ˆr. Combining the bounds
611"
REFERENCES,0.6515151515151515,"from Lemma A.4, and using the triangle inequality, we have that ∥ˆ
M −M∥= O(βdϵ).
612"
REFERENCES,0.6522727272727272,"Eq. (5) states that the C-SoftTreeMax policy in the true environment is πθ = exp(M)/(1⊤exp(M)).
Similarly define ˆπθ using ˆ
M for the approximate model. Then,"
REFERENCES,0.6530303030303031,"ˆπθ = (πθ ◦exp(M −ˆ
M))1⊤exp(M)/(1⊤exp( ˆ
M)),"
REFERENCES,0.6537878787878788,"where ◦denotes element-wise multiplication. Using the above relation, we have that ∥ˆπθ −πθ∥=
613"
REFERENCES,0.6545454545454545,"∥πθ∥∥exp(M−ˆ
M)1⊤exp(M)
1⊤exp( ˆ
M)
−1∥. Using the relation |ex −1| = O(x) as x →0, the desired result
614"
REFERENCES,0.6553030303030303,"follows.
615 616"
REFERENCES,0.656060606060606,"Theorem A.6. Let ϵ be as in Definition A.3. Further let ˆπC
d,θ being the corresponding approximate
617"
REFERENCES,0.6568181818181819,"policy as given in Lemma 4.2. Then, the policy gradient bias is bounded by
618"
REFERENCES,0.6575757575757576,"∂
∂θ
 
ν⊤V πθ
−∂"
REFERENCES,0.6583333333333333,"∂θ
 
ν⊤V ˆπθ = O

1
(1 −γ)2 Sβ2dϵ

.
(73)"
REFERENCES,0.6590909090909091,"We first provide a proof outline for conciseness, and only after it the complete proof.
619"
REFERENCES,0.6598484848484848,"Proof outline. First, we prove that max{∥Rs−ˆRs∥, ∥Ps−ˆPs∥, ∥Rπb−ˆRπb∥, ∥P πb−ˆP πb∥} = O(ϵ).
This follows from the fact that the differences above are suitable convex combinations of either the
rows of P −ˆP or r −ˆr. We use the above observation along with the definitions of πC
d,θ and ˆπC
d,θ
given in (5) to show that ∥πC
d,θ −ˆπC
d,θ∥= O(βdϵ). The proof for the latter builds upon two key facts:"
REFERENCES,0.6606060606060606,"(a) ∥(P πb)k −( ˆP πb)k∥≤Pk
h=1 ∥ˆP πb∥h−1∥ˆP πb −P πb∥∥pπb∥k−h = O(kϵ) for any k ≥0, and (b)
|ex −1| = O(x) as x →0. Next, we decompose the LHS of (7) to get X s"
Y,0.6613636363636364,"4
Y"
Y,0.6621212121212121,"i=1
Xi(s) −"
Y,0.6628787878787878,"4
Y"
Y,0.6636363636363637,"i=1
ˆXi(s) ! =
X s"
X,0.6643939393939394,"4
X"
X,0.6651515151515152,"i=1
ˆX1(s) · · · ˆXi−1(s)

Xi(s) −ˆXi(s)

×Xi+1(s) · · · X4(s),"
X,0.6659090909090909,"where X1(s) = dπC
d,θ(s) ∈R, X2(s) = (∇θ log πC
d,θ(·|s))⊤∈RS×A, X3(s) = D(πC
d,θ(·|s)) ∈
620"
X,0.6666666666666666,"RA×A, X4(s) = QπC
d,θ(s, ·) ∈RA×A, and ˆX1(s), . . . , ˆX4(s) are similarly defined with πC
d,θ re-
621"
X,0.6674242424242425,"placed by ˆπC
d,θ. Then, we show that, for i = 1, . . . , 4, (i) ∥Xi(s) −ˆXi(s)∥= O(ϵ) and (ii)
622"
X,0.6681818181818182,"max{∥Xi∥, ∥ˆXi∥} is bounded by problem parameters. From this, the desired result follows.
623"
X,0.668939393939394,"Proof. We have
624"
X,0.6696969696969697,"∂
∂θ
 
ν⊤V πθ
−∂ ∂θ"
X,0.6704545454545454,"
ν⊤V π′
θ

(74)"
X,0.6712121212121213,"= Es∼dπθ ,a∼πθ(·|s) [∇θ log πθ(a|s)Qπθ(s, a)] −Es∼dˆπθ ,a∼ˆπθ(·|s)

∇θ log ˆπθ(a|s)Qˆπθ(s, a)
 (75) =
X s,a"
X,0.671969696969697," 
dπθ(s)πθ(a|s)∇θ log πθ(a|s)Qπθ(s, a) −dˆπθ(s)ˆπθ(a|s)∇θ log ˆπθ(a|s)Qˆπθ(s, a)

(76) =
X s"
X,0.6727272727272727,"
dπθ(s)(∇θ log πθ(·|s))⊤D(πθ(·|s))Qπθ(s, ·)
(77)"
X,0.6734848484848485,"−dˆπθ(s)(∇θ log ˆπθ(·|s))⊤D(ˆπθ(·|s))Qˆπθ(s, ·)

(78) =
X s"
Y,0.6742424242424242,"4
Y"
Y,0.675,"i=1
Xi(s) −"
Y,0.6757575757575758,"4
Y"
Y,0.6765151515151515,"i=1
ˆXi(s) ! (79) =
X s"
X,0.6772727272727272,"4
X"
X,0.678030303030303,"i=1
ˆX1(s) · · · ˆXi−1(s)

Xi(s) −ˆXi(s)

Xi+1(s) · · · X4(s),
(80)"
X,0.6787878787878788,"where X1(s) = dπθ(s) ∈R, X2(s) = (∇θ log πθ(·|s))⊤∈RS×A, X3(s) = D(πθ(·|s)) ∈RA×A,
625"
X,0.6795454545454546,"X4(s) = Qπθ(s, ·) ∈RA×A, and ˆX1(s), . . . , ˆX4(s) are similarly defined with πθ replaced by ˆπθ.
626"
X,0.6803030303030303,"Therefore,
627

∂
∂θ
 
ν⊤V πθ
−∂ ∂θ"
X,0.681060606060606,"
ν⊤V π′
θ
 ≤

max
s
Γ(s)

S,
(81)"
X,0.6818181818181818,"where
628"
X,0.6825757575757576,"Γ(s) = ∥
X s"
X,0.6833333333333333,"4
X"
X,0.6840909090909091,"i=1
ˆX1(s) · · · ˆXi−1(s)

Xi(s) −ˆXi(s)

Xi+1(s) · · · X4(s)∥.
(82)"
X,0.6848484848484848,"Next, since dπθ, dˆπθ, πθ, and ˆπθ are all distributions, we have
629"
X,0.6856060606060606,"max{|X1(s)|, | ˆ
X1(s)|, |X3(s, a)|, | ˆ
X3(s, a)|} ≤1.
(83)"
X,0.6863636363636364,"Separately, using Lemma 4.3, we have
630"
X,0.6871212121212121,"∥X2∥= ∥∇θ log πθ(a|s)∥≤β(∥IA∥+ ∥1Aπ⊤
θ ∥)∥Ps∥∥(P πb)d−1∥.
(84)"
X,0.6878787878787879,"Since all rows of the above matrices have non-negative entries that add up to 1, we get
631"
X,0.6886363636363636,"∥Y ∥≤2β.
(85)"
X,0.6893939393939394,"In the rest of the proof, we bound each of ∥X1 −ˆ
X1∥, . . . , ∥X4 −ˆ
X4∥.
632"
X,0.6901515151515152,"Finally,
633"
X,0.6909090909090909,"∥X4∥≤
1
1 −γ .
(86)"
X,0.6916666666666667,"Similarly, the same bounds hold for ˆ
X1, ˆ
X2, ˆ
X3 and ˆ
X4.
634"
X,0.6924242424242424,"From, we have
635"
X,0.6931818181818182,"∥X1 −ˆ
X1∥≤(1 −γ) ∞
X"
X,0.693939393939394,"t=0
γt∥ν⊤(P πθ)t −ν⊤(P ˆπθ)t∥
(87)"
X,0.6946969696969697,"≤(1 −γ)∥ν∥
X"
X,0.6954545454545454,"t=0
γttdϵ
(88)"
X,0.6962121212121212,"≤(1 −γ)dϵ ∞
X"
X,0.696969696969697,"t=0
γtt
(89) = γdϵ"
X,0.6977272727272728,"1 −γ .
(90)"
X,0.6984848484848485,"The last relation follows from the fact that (1 −γ)−1 = P∞
t=0 γt, which in turn implies
636 γ ∂ ∂γ"
X,0.6992424242424242,"
1
1 −γ 
= ∞
X"
X,0.7,"t=0
tγt.
(91)"
X,0.7007575757575758,"From Lemma A.5, it follows that
637"
X,0.7015151515151515,"∥X3 −ˆ
X3∥= O(βdϵ).
(92)"
X,0.7022727272727273,"Next, recall that from Lemma 4.3 that
638"
X,0.703030303030303,"X2(s, ·) = β

IA −1A(πθ)⊤
Ps (P πb)d−1 ."
X,0.7037878787878787,"Then,
639"
X,0.7045454545454546,"∥X2(s, ·) −ˆ
X2(s, ·)∥≤∥β

IA −1A(πθ)⊤
Ps∥∥(P πb)d−1 −

ˆP πb
d−1
∥
(93)"
X,0.7053030303030303,"+ ∥β

IA −1A(πθ)⊤
∥∥Ps −ˆPs∥∥

ˆP πb
d−1
∥
(94)"
X,0.706060606060606,"+ β∥1A(πθ)⊤−1A(ˆπθ)⊤∥∥ˆPs

ˆP πb
d−1
∥.
(95)"
X,0.7068181818181818,"Following the same argument as in (85) and applying Lemma A.2, we have that (93) is O(βdϵ).
640"
X,0.7075757575757575,"Similarly, from the argument of (85), Eq. (94) is O(βϵ). Lastly, (95) is O(βdϵ) due to Lemma A.5.
641"
X,0.7083333333333334,"Putting the above three terms together, we have that
642"
X,0.7090909090909091,"∥X2(s, ·) −ˆ
X2(s, ·)∥= O(βdϵ).
(96)"
X,0.7098484848484848,"Since the state-action value function satisfies the Bellman equation, we have
643"
X,0.7106060606060606,"Qπθ = r + γPQπθ
(97)"
X,0.7113636363636363,"and
644"
X,0.7121212121212122,"Qˆπθ = ˆr + γ ˆPQˆπθ.
(98)
Consequently,
645"
X,0.7128787878787879,"∥Qπθ −Qˆπθ∥≤∥r −ˆr∥+ γ∥PQπθ −PQˆπθ∥+ γ∥PQˆπθ −ˆPQˆπθ∥
(99)"
X,0.7136363636363636,"≤ϵ + γ∥P∥∥Qπθ −Qˆπθ∥+ γ∥P −ˆP∥∥Qˆπθ∥
(100)"
X,0.7143939393939394,"≤ϵ + γ∥Qπθ −Qˆπθ∥+
γ
1 −γ ϵ,
(101)"
X,0.7151515151515152,"which finally shows that
646"
X,0.7159090909090909,"∥X4 −ˆ
X4∥= ∥Qπθ −Qˆπθ∥≤
ϵ
(1 −γ)2 .
(102) 647"
X,0.7166666666666667,"B
Experiments
648"
X,0.7174242424242424,"B.1
Implementation Details
649"
X,0.7181818181818181,"The environment engine is the highly efficient Atari-CuLE [Dalton et al., 2020], a CUDA-based
650"
X,0.718939393939394,"version of Atari that runs on GPU. Similarly, we use Atari-CuLE for the GPU-based breadth-first TS
651"
X,0.7196969696969697,"as done in Dalal et al. [2021]: In every tree expansion, the state St is duplicated and concatenated
652"
X,0.7204545454545455,"with all possible actions. The resulting tensor is fed into the GPU forward model to generate the
653"
X,0.7212121212121212,"tensor of next states (S0
t+1, . . . , SA−1
t+1 ). The next-state tensor is then duplicated and concatenated
654"
X,0.7219696969696969,"again with all possible actions, fed into the forward model, etc. This procedure is repeated until the
655"
X,0.7227272727272728,"final depth is reached, for which Wθ(s) is applied per state.
656"
X,0.7234848484848485,"We train SoftTreeMax for depths d = 1 . . . 8, with a single worker. We use five seeds for each
657"
X,0.7242424242424242,"experiment.
658"
X,0.725,"For the implementation, we extend Stable-Baselines3 [Raffin et al., 2019] with all parameters taken
659"
X,0.7257575757575757,"as default from the original PPO paper [Schulman et al., 2017]. For depths d ≥3, we limited the
660"
X,0.7265151515151516,"tree to a maximum width of 1024 nodes and pruned non-promising trajectories in terms of estimated
661"
X,0.7272727272727273,"weights. Since the distributed PPO baseline advances significantly faster in terms of environment
662"
X,0.728030303030303,"steps, for a fair comparison, we ran all experiments for one week on the same machine and use the
663"
X,0.7287878787878788,"wall-clock time as the x-axis. We use Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz equipped with
664"
X,0.7295454545454545,"one NVIDIA Tesla V100 32GB.
665"
X,0.7303030303030303,"B.2
Time-Based Training Curves
666"
X,0.7310606060606061,"We provide the training curves in Figure 4. For brevity, we exclude a few of the depths from the plots.
667"
X,0.7318181818181818,"As seen, there is a clear benefit for SoftTreeMax over distributed PPO with the standard softmax
668"
X,0.7325757575757575,"policy. In most games, PPO with the SoftTreeMax policy shows very high sample efficiency: it
669"
X,0.7333333333333333,"achieves higher episodic reward although it observes much less episodes, for the same running time.
670"
X,0.7340909090909091,"Figure 4: Training curves: GPU SoftTreeMax (single worker) vs PPO (256 GPU workers). The
plots show average reward and standard deviation over 5 seeds. The x-axis is the wall-clock time.
The runs ended after one week with varying number of time-steps. The training curves correspond to
the evaluation runs in Figure 3."
X,0.7348484848484849,"B.3
Step-Based Training Curves
671"
X,0.7356060606060606,"In Figure 5 we also provide the same convergence plots where the x-axis is now the number of online
672"
X,0.7363636363636363,"interactions with the environment, thus excluding the tree expansion complexity. As seen, due to the
673"
X,0.7371212121212121,"complexity of the tree expansion, less steps are conducted during training (limited to one week) as
674"
X,0.7378787878787879,"the depth increases. In this plot, the monotone improvement of the reward with increasing tree depth
675"
X,0.7386363636363636,"is noticeable in most games.
676"
X,0.7393939393939394,"Figure 5: Training curves: GPU SoftTreeMax (single worker) vs PPO (256 GPU workers). The
plots show average reward and standard deviation over 5 seeds. The x-axis is the number of online
interactions with the environment. The runs ended after one week with varying number of time-steps.
The training curves correspond to the evaluation runs in Figure 3."
X,0.7401515151515151,"We note that not for all games we see monotonicity. Our explanation for this phenomenon relates to
677"
X,0.740909090909091,"how immediate reward contributes to performance compared to the value. Different games benefit
678"
X,0.7416666666666667,"differently from long-term as opposed to short-term planning. Games that require longer-term
679"
X,0.7424242424242424,"planning need a better value estimate. A good value estimate takes longer to obtain with larger depths,
680"
X,0.7431818181818182,"in which we apply the network to states that are very different from the ones observed so far in the
681"
X,0.7439393939393939,"buffer (recall that as in any deep RL algorithm, we train the model only on states in the buffer). If
682"
X,0.7446969696969697,"the model hasn’t learned a good enough value function yet, and there is no guiding dense reward
683"
X,0.7454545454545455,"along the trajectory, the policy becomes noisier, and can take more steps to converge – even more
684"
X,0.7462121212121212,"than those we run in our week-long experiment.
685"
X,0.746969696969697,"For a concrete example, let us compare Breakout to Gopher. Inspecting Fig. 5, we observe that
686"
X,0.7477272727272727,"Breakout quickly (and monotonically) gains from large depths since it relies on the short term goal
687"
X,0.7484848484848485,"of simply keeping the paddle below the moving ball. In Gopher, however, for large depths (>=5),
688"
X,0.7492424242424243,"learning barely started even by the end of the training run. Presumably, this is because the task in
689"
X,0.75,"Gopher involves multiple considerations and steps: the agent needs to move to the right spot and
690"
X,0.7507575757575757,"then hit the mallet the right amount of times, while balancing different locations. This task requires
691"
X,0.7515151515151515,"long-term planning and thus depends more strongly on the accuracy of the value function estimate.
692"
X,0.7522727272727273,"In that case, for depth 5 or more, we would require more train steps for the value to “kick in” and
693"
X,0.753030303030303,"become beneficial beyond the gain from the reward in the tree.
694"
X,0.7537878787878788,"The figures above convey two key observations that occur for at least some non-zero depth: (1) The
695"
X,0.7545454545454545,"final performance with the tree is better than PPO (Fig. 3); and (2) the intermediate step-based results
696"
X,0.7553030303030303,"with the tree are better than PPO (Fig. 5). This leads to our main takeaway from this work — there
697"
X,0.7560606060606061,"is no reason to believe that the vanilla policy gradient algorithm should be better than a multi-step
698"
X,0.7568181818181818,"variant. Indeed, we show that this is not the case.
699"
X,0.7575757575757576,"C
Further discussion
700"
X,0.7583333333333333,"C.1
The case of λ2(P πb) = 0
701"
X,0.759090909090909,"When P πb is rank one, it is not only its variance that becomes 0, but also the norm of the gradient
702"
X,0.7598484848484849,"itself (similarly to the case of d →∞). Note that such a situation will happen rarely, in degenerate
703"
X,0.7606060606060606,"MDPs. This is a local minimum for SoftTreeMax and it would cause the PG iteration to get stuck,
704"
X,0.7613636363636364,"and to the optimum in the (desired but impractical) case where πb is the optimal policy. However,
705"
X,0.7621212121212121,"a similar phenomenon was also discovered in the standard softmax with deterministic policies:
706"
X,0.7628787878787879,"θ(s, a) →∞for one a per s. PG with softmax would suffer very slow convergence near these
707"
X,0.7636363636363637,"local equilibria, as observed in Mei et al. [2020a]. To see this, note that the softmax gradient is
708"
X,0.7643939393939394,"∇θ log πθ(a|s) = ea −πθ(·|s), where ea ∈[0, 1]A is the vector with 0 everywhere except for the
709"
X,0.7651515151515151,"a-th coordinate. I.e., it will be zero for a deterministic policy. SoftTreeMax avoids these local optima
710"
X,0.7659090909090909,"by integrating the reward into the policy itself (but may get stuck in another, as discussed above).
711"
X,0.7666666666666667,"NeurIPS Paper Checklist
712"
CLAIMS,0.7674242424242425,"1. Claims
713"
CLAIMS,0.7681818181818182,"Question: Do the main claims made in the abstract and introduction accurately reflect the
714"
CLAIMS,0.7689393939393939,"paper’s contributions and scope?
715"
CLAIMS,0.7696969696969697,"Answer: [Yes]
716"
CLAIMS,0.7704545454545455,"Justification: [NA]
717"
CLAIMS,0.7712121212121212,"Guidelines:
718"
CLAIMS,0.771969696969697,"• The answer NA means that the abstract and introduction do not include the claims
719"
CLAIMS,0.7727272727272727,"made in the paper.
720"
CLAIMS,0.7734848484848484,"• The abstract and/or introduction should clearly state the claims made, including the
721"
CLAIMS,0.7742424242424243,"contributions made in the paper and important assumptions and limitations. A No or
722"
CLAIMS,0.775,"NA answer to this question will not be perceived well by the reviewers.
723"
CLAIMS,0.7757575757575758,"• The claims made should match theoretical and experimental results, and reflect how
724"
CLAIMS,0.7765151515151515,"much the results can be expected to generalize to other settings.
725"
CLAIMS,0.7772727272727272,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
726"
CLAIMS,0.7780303030303031,"are not attained by the paper.
727"
LIMITATIONS,0.7787878787878788,"2. Limitations
728"
LIMITATIONS,0.7795454545454545,"Question: Does the paper discuss the limitations of the work performed by the authors?
729"
LIMITATIONS,0.7803030303030303,"Answer: [Yes]
730"
LIMITATIONS,0.781060606060606,"Justification: We included a relevant section at the end of the paper.
731"
LIMITATIONS,0.7818181818181819,"Guidelines:
732"
LIMITATIONS,0.7825757575757576,"• The answer NA means that the paper has no limitation while the answer No means that
733"
LIMITATIONS,0.7833333333333333,"the paper has limitations, but those are not discussed in the paper.
734"
LIMITATIONS,0.7840909090909091,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
735"
LIMITATIONS,0.7848484848484848,"• The paper should point out any strong assumptions and how robust the results are to
736"
LIMITATIONS,0.7856060606060606,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
737"
LIMITATIONS,0.7863636363636364,"model well-specification, asymptotic approximations only holding locally). The authors
738"
LIMITATIONS,0.7871212121212121,"should reflect on how these assumptions might be violated in practice and what the
739"
LIMITATIONS,0.7878787878787878,"implications would be.
740"
LIMITATIONS,0.7886363636363637,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
741"
LIMITATIONS,0.7893939393939394,"only tested on a few datasets or with a few runs. In general, empirical results often
742"
LIMITATIONS,0.7901515151515152,"depend on implicit assumptions, which should be articulated.
743"
LIMITATIONS,0.7909090909090909,"• The authors should reflect on the factors that influence the performance of the approach.
744"
LIMITATIONS,0.7916666666666666,"For example, a facial recognition algorithm may perform poorly when image resolution
745"
LIMITATIONS,0.7924242424242425,"is low or images are taken in low lighting. Or a speech-to-text system might not be
746"
LIMITATIONS,0.7931818181818182,"used reliably to provide closed captions for online lectures because it fails to handle
747"
LIMITATIONS,0.793939393939394,"technical jargon.
748"
LIMITATIONS,0.7946969696969697,"• The authors should discuss the computational efficiency of the proposed algorithms
749"
LIMITATIONS,0.7954545454545454,"and how they scale with dataset size.
750"
LIMITATIONS,0.7962121212121213,"• If applicable, the authors should discuss possible limitations of their approach to
751"
LIMITATIONS,0.796969696969697,"address problems of privacy and fairness.
752"
LIMITATIONS,0.7977272727272727,"• While the authors might fear that complete honesty about limitations might be used by
753"
LIMITATIONS,0.7984848484848485,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
754"
LIMITATIONS,0.7992424242424242,"limitations that aren’t acknowledged in the paper. The authors should use their best
755"
LIMITATIONS,0.8,"judgment and recognize that individual actions in favor of transparency play an impor-
756"
LIMITATIONS,0.8007575757575758,"tant role in developing norms that preserve the integrity of the community. Reviewers
757"
LIMITATIONS,0.8015151515151515,"will be specifically instructed to not penalize honesty concerning limitations.
758"
THEORY ASSUMPTIONS AND PROOFS,0.8022727272727272,"3. Theory Assumptions and Proofs
759"
THEORY ASSUMPTIONS AND PROOFS,0.803030303030303,"Question: For each theoretical result, does the paper provide the full set of assumptions and
760"
THEORY ASSUMPTIONS AND PROOFS,0.8037878787878788,"a complete (and correct) proof?
761"
THEORY ASSUMPTIONS AND PROOFS,0.8045454545454546,"Answer: [Yes]
762"
THEORY ASSUMPTIONS AND PROOFS,0.8053030303030303,"Justification: All proofs can be found in the appendix.
763"
THEORY ASSUMPTIONS AND PROOFS,0.806060606060606,"Guidelines:
764"
THEORY ASSUMPTIONS AND PROOFS,0.8068181818181818,"• The answer NA means that the paper does not include theoretical results.
765"
THEORY ASSUMPTIONS AND PROOFS,0.8075757575757576,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
766"
THEORY ASSUMPTIONS AND PROOFS,0.8083333333333333,"referenced.
767"
THEORY ASSUMPTIONS AND PROOFS,0.8090909090909091,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
768"
THEORY ASSUMPTIONS AND PROOFS,0.8098484848484848,"• The proofs can either appear in the main paper or the supplemental material, but if
769"
THEORY ASSUMPTIONS AND PROOFS,0.8106060606060606,"they appear in the supplemental material, the authors are encouraged to provide a short
770"
THEORY ASSUMPTIONS AND PROOFS,0.8113636363636364,"proof sketch to provide intuition.
771"
THEORY ASSUMPTIONS AND PROOFS,0.8121212121212121,"• Inversely, any informal proof provided in the core of the paper should be complemented
772"
THEORY ASSUMPTIONS AND PROOFS,0.8128787878787879,"by formal proofs provided in appendix or supplemental material.
773"
THEORY ASSUMPTIONS AND PROOFS,0.8136363636363636,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
774"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8143939393939394,"4. Experimental Result Reproducibility
775"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8151515151515152,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
776"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8159090909090909,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
777"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8166666666666667,"of the paper (regardless of whether the code and data are provided or not)?
778"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8174242424242424,"Answer: [Yes]
779"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8181818181818182,"Justification: Yes. We also attached the repository, together with a docker environment, as
780"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.818939393939394,"supplementary material.
781"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8196969696969697,"Guidelines:
782"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8204545454545454,"• The answer NA means that the paper does not include experiments.
783"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8212121212121212,"• If the paper includes experiments, a No answer to this question will not be perceived
784"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.821969696969697,"well by the reviewers: Making the paper reproducible is important, regardless of
785"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8227272727272728,"whether the code and data are provided or not.
786"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8234848484848485,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
787"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8242424242424242,"to make their results reproducible or verifiable.
788"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.825,"• Depending on the contribution, reproducibility can be accomplished in various ways.
789"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8257575757575758,"For example, if the contribution is a novel architecture, describing the architecture fully
790"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8265151515151515,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
791"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8272727272727273,"be necessary to either make it possible for others to replicate the model with the same
792"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.828030303030303,"dataset, or provide access to the model. In general. releasing code and data is often
793"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8287878787878787,"one good way to accomplish this, but reproducibility can also be provided via detailed
794"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8295454545454546,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
795"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8303030303030303,"of a large language model), releasing of a model checkpoint, or other means that are
796"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.831060606060606,"appropriate to the research performed.
797"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8318181818181818,"• While NeurIPS does not require releasing code, the conference does require all submis-
798"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8325757575757575,"sions to provide some reasonable avenue for reproducibility, which may depend on the
799"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8333333333333334,"nature of the contribution. For example
800"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8340909090909091,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
801"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8348484848484848,"to reproduce that algorithm.
802"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8356060606060606,"(b) If the contribution is primarily a new model architecture, the paper should describe
803"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8363636363636363,"the architecture clearly and fully.
804"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8371212121212122,"(c) If the contribution is a new model (e.g., a large language model), then there should
805"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8378787878787879,"either be a way to access this model for reproducing the results or a way to reproduce
806"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8386363636363636,"the model (e.g., with an open-source dataset or instructions for how to construct
807"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8393939393939394,"the dataset).
808"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8401515151515152,"(d) We recognize that reproducibility may be tricky in some cases, in which case
809"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8409090909090909,"authors are welcome to describe the particular way they provide for reproducibility.
810"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8416666666666667,"In the case of closed-source models, it may be that access to the model is limited in
811"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8424242424242424,"some way (e.g., to registered users), but it should be possible for other researchers
812"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8431818181818181,"to have some path to reproducing or verifying the results.
813"
OPEN ACCESS TO DATA AND CODE,0.843939393939394,"5. Open access to data and code
814"
OPEN ACCESS TO DATA AND CODE,0.8446969696969697,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
815"
OPEN ACCESS TO DATA AND CODE,0.8454545454545455,"tions to faithfully reproduce the main experimental results, as described in supplemental
816"
OPEN ACCESS TO DATA AND CODE,0.8462121212121212,"material?
817"
OPEN ACCESS TO DATA AND CODE,0.8469696969696969,"Answer: [Yes]
818"
OPEN ACCESS TO DATA AND CODE,0.8477272727272728,"Justification: We attached the repository, together with a docker environment, as supplemen-
819"
OPEN ACCESS TO DATA AND CODE,0.8484848484848485,"tary material.
820"
OPEN ACCESS TO DATA AND CODE,0.8492424242424242,"Guidelines:
821"
OPEN ACCESS TO DATA AND CODE,0.85,"• The answer NA means that paper does not include experiments requiring code.
822"
OPEN ACCESS TO DATA AND CODE,0.8507575757575757,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
823"
OPEN ACCESS TO DATA AND CODE,0.8515151515151516,"public/guides/CodeSubmissionPolicy) for more details.
824"
OPEN ACCESS TO DATA AND CODE,0.8522727272727273,"• While we encourage the release of code and data, we understand that this might not be
825"
OPEN ACCESS TO DATA AND CODE,0.853030303030303,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
826"
OPEN ACCESS TO DATA AND CODE,0.8537878787878788,"including code, unless this is central to the contribution (e.g., for a new open-source
827"
OPEN ACCESS TO DATA AND CODE,0.8545454545454545,"benchmark).
828"
OPEN ACCESS TO DATA AND CODE,0.8553030303030303,"• The instructions should contain the exact command and environment needed to run to
829"
OPEN ACCESS TO DATA AND CODE,0.8560606060606061,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
830"
OPEN ACCESS TO DATA AND CODE,0.8568181818181818,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
831"
OPEN ACCESS TO DATA AND CODE,0.8575757575757575,"• The authors should provide instructions on data access and preparation, including how
832"
OPEN ACCESS TO DATA AND CODE,0.8583333333333333,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
833"
OPEN ACCESS TO DATA AND CODE,0.8590909090909091,"• The authors should provide scripts to reproduce all experimental results for the new
834"
OPEN ACCESS TO DATA AND CODE,0.8598484848484849,"proposed method and baselines. If only a subset of experiments are reproducible, they
835"
OPEN ACCESS TO DATA AND CODE,0.8606060606060606,"should state which ones are omitted from the script and why.
836"
OPEN ACCESS TO DATA AND CODE,0.8613636363636363,"• At submission time, to preserve anonymity, the authors should release anonymized
837"
OPEN ACCESS TO DATA AND CODE,0.8621212121212121,"versions (if applicable).
838"
OPEN ACCESS TO DATA AND CODE,0.8628787878787879,"• Providing as much information as possible in supplemental material (appended to the
839"
OPEN ACCESS TO DATA AND CODE,0.8636363636363636,"paper) is recommended, but including URLs to data and code is permitted.
840"
OPEN ACCESS TO DATA AND CODE,0.8643939393939394,"6. Experimental Setting/Details
841"
OPEN ACCESS TO DATA AND CODE,0.8651515151515151,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
842"
OPEN ACCESS TO DATA AND CODE,0.865909090909091,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
843"
OPEN ACCESS TO DATA AND CODE,0.8666666666666667,"results?
844"
OPEN ACCESS TO DATA AND CODE,0.8674242424242424,"Answer: [Yes]
845"
OPEN ACCESS TO DATA AND CODE,0.8681818181818182,"Justification: [NA]
846"
OPEN ACCESS TO DATA AND CODE,0.8689393939393939,"Guidelines:
847"
OPEN ACCESS TO DATA AND CODE,0.8696969696969697,"• The answer NA means that the paper does not include experiments.
848"
OPEN ACCESS TO DATA AND CODE,0.8704545454545455,"• The experimental setting should be presented in the core of the paper to a level of detail
849"
OPEN ACCESS TO DATA AND CODE,0.8712121212121212,"that is necessary to appreciate the results and make sense of them.
850"
OPEN ACCESS TO DATA AND CODE,0.871969696969697,"• The full details can be provided either with the code, in appendix, or as supplemental
851"
OPEN ACCESS TO DATA AND CODE,0.8727272727272727,"material.
852"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8734848484848485,"7. Experiment Statistical Significance
853"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8742424242424243,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
854"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.875,"information about the statistical significance of the experiments?
855"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8757575757575757,"Answer: [Yes]
856"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8765151515151515,"Justification: [NA]
857"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8772727272727273,"Guidelines:
858"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.878030303030303,"• The answer NA means that the paper does not include experiments.
859"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8787878787878788,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
860"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8795454545454545,"dence intervals, or statistical significance tests, at least for the experiments that support
861"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8803030303030303,"the main claims of the paper.
862"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8810606060606061,"• The factors of variability that the error bars are capturing should be clearly stated (for
863"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8818181818181818,"example, train/test split, initialization, random drawing of some parameter, or overall
864"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8825757575757576,"run with given experimental conditions).
865"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8833333333333333,"• The method for calculating the error bars should be explained (closed form formula,
866"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.884090909090909,"call to a library function, bootstrap, etc.)
867"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8848484848484849,"• The assumptions made should be given (e.g., Normally distributed errors).
868"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8856060606060606,"• It should be clear whether the error bar is the standard deviation or the standard error
869"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8863636363636364,"of the mean.
870"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8871212121212121,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
871"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8878787878787879,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
872"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8886363636363637,"of Normality of errors is not verified.
873"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8893939393939394,"• For asymmetric distributions, the authors should be careful not to show in tables or
874"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8901515151515151,"figures symmetric error bars that would yield results that are out of range (e.g. negative
875"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8909090909090909,"error rates).
876"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8916666666666667,"• If error bars are reported in tables or plots, The authors should explain in the text how
877"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8924242424242425,"they were calculated and reference the corresponding figures or tables in the text.
878"
EXPERIMENTS COMPUTE RESOURCES,0.8931818181818182,"8. Experiments Compute Resources
879"
EXPERIMENTS COMPUTE RESOURCES,0.8939393939393939,"Question: For each experiment, does the paper provide sufficient information on the com-
880"
EXPERIMENTS COMPUTE RESOURCES,0.8946969696969697,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
881"
EXPERIMENTS COMPUTE RESOURCES,0.8954545454545455,"the experiments?
882"
EXPERIMENTS COMPUTE RESOURCES,0.8962121212121212,"Answer: [Yes]
883"
EXPERIMENTS COMPUTE RESOURCES,0.896969696969697,"Justification: All relevant information can be found in the paper and the appendix.
884"
EXPERIMENTS COMPUTE RESOURCES,0.8977272727272727,"Guidelines:
885"
EXPERIMENTS COMPUTE RESOURCES,0.8984848484848484,"• The answer NA means that the paper does not include experiments.
886"
EXPERIMENTS COMPUTE RESOURCES,0.8992424242424243,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
887"
EXPERIMENTS COMPUTE RESOURCES,0.9,"or cloud provider, including relevant memory and storage.
888"
EXPERIMENTS COMPUTE RESOURCES,0.9007575757575758,"• The paper should provide the amount of compute required for each of the individual
889"
EXPERIMENTS COMPUTE RESOURCES,0.9015151515151515,"experimental runs as well as estimate the total compute.
890"
EXPERIMENTS COMPUTE RESOURCES,0.9022727272727272,"• The paper should disclose whether the full research project required more compute
891"
EXPERIMENTS COMPUTE RESOURCES,0.9030303030303031,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
892"
EXPERIMENTS COMPUTE RESOURCES,0.9037878787878788,"didn’t make it into the paper).
893"
CODE OF ETHICS,0.9045454545454545,"9. Code Of Ethics
894"
CODE OF ETHICS,0.9053030303030303,"Question: Does the research conducted in the paper conform, in every respect, with the
895"
CODE OF ETHICS,0.906060606060606,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
896"
CODE OF ETHICS,0.9068181818181819,"Answer: [Yes]
897"
CODE OF ETHICS,0.9075757575757576,"Justification: [NA]
898"
CODE OF ETHICS,0.9083333333333333,"Guidelines:
899"
CODE OF ETHICS,0.9090909090909091,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
900"
CODE OF ETHICS,0.9098484848484848,"• If the authors answer No, they should explain the special circumstances that require a
901"
CODE OF ETHICS,0.9106060606060606,"deviation from the Code of Ethics.
902"
CODE OF ETHICS,0.9113636363636364,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
903"
CODE OF ETHICS,0.9121212121212121,"eration due to laws or regulations in their jurisdiction).
904"
BROADER IMPACTS,0.9128787878787878,"10. Broader Impacts
905"
BROADER IMPACTS,0.9136363636363637,"Question: Does the paper discuss both potential positive societal impacts and negative
906"
BROADER IMPACTS,0.9143939393939394,"societal impacts of the work performed?
907"
BROADER IMPACTS,0.9151515151515152,"Answer: [NA]
908"
BROADER IMPACTS,0.9159090909090909,"Justification: The paper has no scietal impact.
909"
BROADER IMPACTS,0.9166666666666666,"Guidelines:
910"
BROADER IMPACTS,0.9174242424242425,"• The answer NA means that there is no societal impact of the work performed.
911"
BROADER IMPACTS,0.9181818181818182,"• If the authors answer NA or No, they should explain why their work has no societal
912"
BROADER IMPACTS,0.918939393939394,"impact or why the paper does not address societal impact.
913"
BROADER IMPACTS,0.9196969696969697,"• Examples of negative societal impacts include potential malicious or unintended uses
914"
BROADER IMPACTS,0.9204545454545454,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
915"
BROADER IMPACTS,0.9212121212121213,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
916"
BROADER IMPACTS,0.921969696969697,"groups), privacy considerations, and security considerations.
917"
BROADER IMPACTS,0.9227272727272727,"• The conference expects that many papers will be foundational research and not tied
918"
BROADER IMPACTS,0.9234848484848485,"to particular applications, let alone deployments. However, if there is a direct path to
919"
BROADER IMPACTS,0.9242424242424242,"any negative applications, the authors should point it out. For example, it is legitimate
920"
BROADER IMPACTS,0.925,"to point out that an improvement in the quality of generative models could be used to
921"
BROADER IMPACTS,0.9257575757575758,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
922"
BROADER IMPACTS,0.9265151515151515,"that a generic algorithm for optimizing neural networks could enable people to train
923"
BROADER IMPACTS,0.9272727272727272,"models that generate Deepfakes faster.
924"
BROADER IMPACTS,0.928030303030303,"• The authors should consider possible harms that could arise when the technology is
925"
BROADER IMPACTS,0.9287878787878788,"being used as intended and functioning correctly, harms that could arise when the
926"
BROADER IMPACTS,0.9295454545454546,"technology is being used as intended but gives incorrect results, and harms following
927"
BROADER IMPACTS,0.9303030303030303,"from (intentional or unintentional) misuse of the technology.
928"
BROADER IMPACTS,0.931060606060606,"• If there are negative societal impacts, the authors could also discuss possible mitigation
929"
BROADER IMPACTS,0.9318181818181818,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
930"
BROADER IMPACTS,0.9325757575757576,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
931"
BROADER IMPACTS,0.9333333333333333,"feedback over time, improving the efficiency and accessibility of ML).
932"
SAFEGUARDS,0.9340909090909091,"11. Safeguards
933"
SAFEGUARDS,0.9348484848484848,"Question: Does the paper describe safeguards that have been put in place for responsible
934"
SAFEGUARDS,0.9356060606060606,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
935"
SAFEGUARDS,0.9363636363636364,"image generators, or scraped datasets)?
936"
SAFEGUARDS,0.9371212121212121,"Answer: [NA]
937"
SAFEGUARDS,0.9378787878787879,"Justification: The paper poses no such risks.
938"
SAFEGUARDS,0.9386363636363636,"Guidelines:
939"
SAFEGUARDS,0.9393939393939394,"• The answer NA means that the paper poses no such risks.
940"
SAFEGUARDS,0.9401515151515152,"• Released models that have a high risk for misuse or dual-use should be released with
941"
SAFEGUARDS,0.9409090909090909,"necessary safeguards to allow for controlled use of the model, for example by requiring
942"
SAFEGUARDS,0.9416666666666667,"that users adhere to usage guidelines or restrictions to access the model or implementing
943"
SAFEGUARDS,0.9424242424242424,"safety filters.
944"
SAFEGUARDS,0.9431818181818182,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
945"
SAFEGUARDS,0.943939393939394,"should describe how they avoided releasing unsafe images.
946"
SAFEGUARDS,0.9446969696969697,"• We recognize that providing effective safeguards is challenging, and many papers do
947"
SAFEGUARDS,0.9454545454545454,"not require this, but we encourage authors to take this into account and make a best
948"
SAFEGUARDS,0.9462121212121212,"faith effort.
949"
LICENSES FOR EXISTING ASSETS,0.946969696969697,"12. Licenses for existing assets
950"
LICENSES FOR EXISTING ASSETS,0.9477272727272728,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
951"
LICENSES FOR EXISTING ASSETS,0.9484848484848485,"the paper, properly credited and are the license and terms of use explicitly mentioned and
952"
LICENSES FOR EXISTING ASSETS,0.9492424242424242,"properly respected?
953"
LICENSES FOR EXISTING ASSETS,0.95,"Answer: [Yes]
954"
LICENSES FOR EXISTING ASSETS,0.9507575757575758,"Justification: [NA]
955"
LICENSES FOR EXISTING ASSETS,0.9515151515151515,"Guidelines:
956"
LICENSES FOR EXISTING ASSETS,0.9522727272727273,"• The answer NA means that the paper does not use existing assets.
957"
LICENSES FOR EXISTING ASSETS,0.953030303030303,"• The authors should cite the original paper that produced the code package or dataset.
958"
LICENSES FOR EXISTING ASSETS,0.9537878787878787,"• The authors should state which version of the asset is used and, if possible, include a
959"
LICENSES FOR EXISTING ASSETS,0.9545454545454546,"URL.
960"
LICENSES FOR EXISTING ASSETS,0.9553030303030303,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
961"
LICENSES FOR EXISTING ASSETS,0.956060606060606,"• For scraped data from a particular source (e.g., website), the copyright and terms of
962"
LICENSES FOR EXISTING ASSETS,0.9568181818181818,"service of that source should be provided.
963"
LICENSES FOR EXISTING ASSETS,0.9575757575757575,"• If assets are released, the license, copyright information, and terms of use in the
964"
LICENSES FOR EXISTING ASSETS,0.9583333333333334,"package should be provided. For popular datasets, paperswithcode.com/datasets
965"
LICENSES FOR EXISTING ASSETS,0.9590909090909091,"has curated licenses for some datasets. Their licensing guide can help determine the
966"
LICENSES FOR EXISTING ASSETS,0.9598484848484848,"license of a dataset.
967"
LICENSES FOR EXISTING ASSETS,0.9606060606060606,"• For existing datasets that are re-packaged, both the original license and the license of
968"
LICENSES FOR EXISTING ASSETS,0.9613636363636363,"the derived asset (if it has changed) should be provided.
969"
LICENSES FOR EXISTING ASSETS,0.9621212121212122,"• If this information is not available online, the authors are encouraged to reach out to
970"
LICENSES FOR EXISTING ASSETS,0.9628787878787879,"the asset’s creators.
971"
NEW ASSETS,0.9636363636363636,"13. New Assets
972"
NEW ASSETS,0.9643939393939394,"Question: Are new assets introduced in the paper well documented and is the documentation
973"
NEW ASSETS,0.9651515151515152,"provided alongside the assets?
974"
NEW ASSETS,0.9659090909090909,"Answer: [NA]
975"
NEW ASSETS,0.9666666666666667,"Justification: The paper does not release new assets.
976"
NEW ASSETS,0.9674242424242424,"Guidelines:
977"
NEW ASSETS,0.9681818181818181,"• The answer NA means that the paper does not release new assets.
978"
NEW ASSETS,0.968939393939394,"• Researchers should communicate the details of the dataset/code/model as part of their
979"
NEW ASSETS,0.9696969696969697,"submissions via structured templates. This includes details about training, license,
980"
NEW ASSETS,0.9704545454545455,"limitations, etc.
981"
NEW ASSETS,0.9712121212121212,"• The paper should discuss whether and how consent was obtained from people whose
982"
NEW ASSETS,0.9719696969696969,"asset is used.
983"
NEW ASSETS,0.9727272727272728,"• At submission time, remember to anonymize your assets (if applicable). You can either
984"
NEW ASSETS,0.9734848484848485,"create an anonymized URL or include an anonymized zip file.
985"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9742424242424242,"14. Crowdsourcing and Research with Human Subjects
986"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.975,"Question: For crowdsourcing experiments and research with human subjects, does the paper
987"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9757575757575757,"include the full text of instructions given to participants and screenshots, if applicable, as
988"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9765151515151516,"well as details about compensation (if any)?
989"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9772727272727273,"Answer: [NA]
990"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.978030303030303,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
991"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9787878787878788,"Guidelines:
992"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9795454545454545,"• The answer NA means that the paper does not involve crowdsourcing nor research with
993"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9803030303030303,"human subjects.
994"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9810606060606061,"• Including this information in the supplemental material is fine, but if the main contribu-
995"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9818181818181818,"tion of the paper involves human subjects, then as much detail as possible should be
996"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9825757575757575,"included in the main paper.
997"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9833333333333333,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
998"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9840909090909091,"or other labor should be paid at least the minimum wage in the country of the data
999"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9848484848484849,"collector.
1000"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9856060606060606,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
1001"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9863636363636363,"Subjects
1002"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9871212121212121,"Question: Does the paper describe potential risks incurred by study participants, whether
1003"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9878787878787879,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
1004"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9886363636363636,"approvals (or an equivalent approval/review based on the requirements of your country or
1005"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9893939393939394,"institution) were obtained?
1006"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9901515151515151,"Answer: [NA]
1007"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.990909090909091,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
1008"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9916666666666667,"Guidelines:
1009"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9924242424242424,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1010"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9931818181818182,"human subjects.
1011"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9939393939393939,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
1012"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9946969696969697,"may be required for any human subjects research. If you obtained IRB approval, you
1013"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9954545454545455,"should clearly state this in the paper.
1014"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9962121212121212,"• We recognize that the procedures for this may vary significantly between institutions
1015"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.996969696969697,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
1016"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9977272727272727,"guidelines for their institution.
1017"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9984848484848485,"• For initial submissions, do not include any information that would break anonymity (if
1018"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9992424242424243,"applicable), such as the institution conducting the review.
1019"
