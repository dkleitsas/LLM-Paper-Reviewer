Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.005747126436781609,"In contrast to object recognition models, humans do not blindly trust their percep-
tion when building representations of the world, instead recruiting metacognition
to detect percepts that are unreliable or false, such as when we realize that we mis-
took one object for another. We propose METAGEN, an unsupervised model that
enhances object recognition models through a metacognition. Given noisy output
from an object-detection model, METAGEN learns a meta-representation of how
its perceptual system works and uses it to infer the objects in the world responsi-
ble for the detections. METAGEN achieves this by conditioning its inference on
basic principles of objects that even human infants understand (known as Spelke
principles: object permanence, cohesion, and spatiotemporal continuity). We test
METAGEN on a variety of state-of-the-art object detection neural networks. We
ﬁnd that METAGEN quickly learns an accurate metacognitive representation of
the neural network, and that this improves detection accuracy by ﬁlling in objects
that the detection model missed and removing hallucinated objects. This approach
enables generalization to out-of-sample data and outperforms comparison models
that lack a metacognition."
INTRODUCTION,0.011494252873563218,"1
INTRODUCTION"
INTRODUCTION,0.017241379310344827,"Learning accurate representations of the world is critical for prediction, inference, and planning in
complex environments (Lake et al., 2017). In human vision, these representations are generated by a
perceptual system that transforms light entering the retina into representations of the physical space
and the objects in it (Kar & DiCarlo, 2021; G¨uc¸l¨u & van Gerven, 2017). While human perception
is generally robust and reliable, it nonetheless suffers from errors, such as when we fail to detect an
object in a crowded visual scene, when we confuse one object for another, or when we experience a
visual illusion. Critically, in these situations, people have a metacognition that helps them recognize
that their visual system has failed them and adjust their reasoning accordingly."
INTRODUCTION,0.022988505747126436,"End-to-end object recognition models have revolutionized computer vision in the past decade
(Krizhevsky et al., 2012; Redmon et al., 2016; Zou et al., 2019), but they face a similar challenge:
how can we identify when a non-existent object is incorrectly detected (hallucinations), or when a
real object is not reliably recognized (misses)? Traditional approaches to this problem have focused
on increasing the training data (Lin et al., 2014; Deng et al., 2009) or improving the model’s ar-
chitecture (Carion et al., 2020; Liu et al., 2021). While these methods are generally successful, the
possibility of misses and hallucinations remains—particularly when confronted with out-of-sample
data—and a foolproof visual model must be able to identify when this occurs. We propose that aug-
menting object recognition models with a metacognition—an external meta-representation of their
own computational processes—can allow these systems to monitor their percepts and ﬂexibly decide
when to trust or doubt proposed representations, much like humans do."
INTRODUCTION,0.028735632183908046,"In this paper, we propose a formulation of metacognition that can be learned online, requiring no
human annotation or feedback. Given a pre-trained object detection model and a data stream from
videos or scene viewpoints, our approach is to perform a joint inference over the objects generat-
ing the percepts, and a meta-representation of the system’s performance. Our model, METAGEN,
achieves this by representing its metacognition as a generative model that captures the joint distri-
bution between objects in a scene and the model’s pattern of detections. Critically, we make this
problem tractable by drawing on an insight from cognitive science. Infants come into the world"
INTRODUCTION,0.034482758620689655,Under review as a conference paper at ICLR 2021
INTRODUCTION,0.040229885057471264,"equipped with a basic form of ‘core knowledge’ or ‘start-up sofware’ (Lake et al., 2017; Carey,
2009). These priors constrain the representations of world states that humans consider possible and
support drawing richer inferences with less data. Speciﬁcally, we focus on early object constraints
known as Spelke principles (Spelke & Kinzler, 2007; Smith et al., 2019): objects persist in time, they
cannot occupy the same position as another object, and they move continuously in space. Condition-
ing inference on these basic principles enables METAGEN to learn a metacognitive representation
of the object detection model by analyzing and resolving patterns of percepts that violate the Spelke
principles of objects."
INTRODUCTION,0.04597701149425287,"We evaluate METAGEN in a variety of ways and against several comparison models, focused on
performance on out-of-sample data. Using scenes rendered in in the ThreeDWorld (TDW) virtual
environment (Gan et al., 2020), we ﬁrst examine METAGEN’s capacity to learn accurate metacog-
nition online for a variety of state-of-the-art object detection networks that span different dominant
paradigms (single-stage, two-stage, and transformer models). We ﬁnd that, during learning, META-
GEN already produces marked improvements over key comparison models: the object detection
model without a metacognition and the object detection models with a conﬁdence threshold ﬁt to
the data. We then evaluate METAGEN after learning by conditioning on its learned metacognition
and testing its accuracy on a new set of data. We ﬁnd that METAGEN also outperforms comparison
models on a set of novel scenes."
INTRODUCTION,0.05172413793103448,"Overall, our work makes two main contributions. First, we propose a novel approach to improv-
ing object detection models through a metacognition that helps monitor a model’s performance.
This distinction between perceptual processing systems and high-level cognitive systems mirrors
the architecture of the human mind (Firestone & Scholl, 2016) where both processes work jointly
to build accurate representations of the world. Second, we present METAGEN, a model that learns
a metacognition for object detection models. We show that METAGEN can learn an accurate rep-
resentation of object detection models without feedback and leads to rapid improvements in the
system’s accuracy."
RELATED WORK,0.05747126436781609,"2
RELATED WORK"
RELATED WORK,0.06321839080459771,"Metacognition in AI. Previous work has argued for the importance of metacognition for machine
learning and AI (Cox, 2005). Models that use metacognition during learning have shown promise
for improving classiﬁcation accuracy (Babu & Suresh, 2012; Subramanian et al., 2013). This work
has focused on engineering an inﬂexible metacognition to guide learning. In this paper, we focus on
a complimentary problem: learning a metacognition without feedback that helps monitor a neural
network’s detections."
RELATED WORK,0.06896551724137931,"Computational cognitive science. Our core idea—learning a metacognitive representation of a
perceptual system—is inspired by research in cognitive science showing that human reasoning is
structured around mental models of the physical world (Ullman et al., 2017; Battaglia et al., 2013),
of the social world (Jara-Ettinger et al., 2019; Baker et al., 2017), and of ourselves (Gopnik, 1993;
Nisbett & Wilson, 1977). Our work is also related to computational models of human core knowl-
edge (Smith et al., 2019; Kemp & Xu, 2009). The difference is that our work uses object principles
to learn a metacognition, whereas past work has worked on modeling object principles themselves."
RELATED WORK,0.07471264367816093,"Uncertainty-aware AI. The spirit of our work relates more closely to uncertainty-aware AI. This
work focuses on building end-to-end systems that express uncertainty in their inferences (Sensoy
et al., 2018; Kaplan et al., 2018; Ivanovska et al., 2015). Our work focuses instead on how to learn
a model of uncertainty over a pre-trained model. These two approaches complement each other. In
humans, metacognitive uncertainty supplements the intrinsic uncertainty in visual perception."
RELATED WORK,0.08045977011494253,"Meta-Learning. A major challenge for machine learning algorithms is generalizing to new datasets
and tasks that they were not trained on. Recent and exciting work in machine learning has explored
the possibility of meta-learning (Hospedales et al., 2020), which involves applying a model trained
on one task to a new setting (Finn et al., 2017). At ﬁrst glance, our work may appear to fall within the
ﬁeld of meta-learning. However, in contrast to meta-learning, METAGEN does not improve the ﬁrst-
level learning algorithm (the neural network for object detection). Instead, METAGEN functions at a
level above learning algorithm, taking the outputs of the learning algorithm as unchangeable inputs."
RELATED WORK,0.08620689655172414,Under review as a conference paper at ICLR 2021
METAGEN,0.09195402298850575,"3
METAGEN"
METAGEN,0.09770114942528736,"Figure 1: Schematic depicting the forward gener-
ative model. Gt is the prior over metacognition
(Vt) at time t; Wt is the world state; and Ot are
the observations that are generated. Wt, Ot, and
Gt collectively inﬂuence Gt+1, the prior over Vt1."
METAGEN,0.10344827586206896,"In this section we present our METAGEN
model, applied to the problem of inferring what
objects are in a 3D scene, given a video from
a moving camera. METAGEN captures the re-
lationship between the objects in a scene and
the detections obtained by its perceptual mod-
ule (i.e., the output from an object recognition
model) through a hierarchical generative model
with two levels (Figure 1).
The lower level
captures the joint distribution between world
states and object detections, as determined by
a metacognitive representation V that summa-
rizes its perception module’s performance. The
upper level describes the dynamics of the prior
over V, capturing the idea that a metacognitive
representation can—and should—change with
experience."
METAGEN,0.10919540229885058,"Given a collection of observations ⃗o obtained
from a scene, our approach is to perform a joint inference over the unobservable world state W
and the metacognitive representation V , conditioned on Spelke principles of objects. We begin by
presenting the generative model (3.1) and then turn to how we perform joint inference over the world
state and the system’s metacognition (3.2)."
GENERATIVE MODEL,0.11494252873563218,"3.1
GENERATIVE MODEL"
WORLD-STATE REPRESENTATION AND SPELKE PRINCIPLES,0.1206896551724138,"3.1.1
WORLD-STATE REPRESENTATION AND SPELKE PRINCIPLES"
WORLD-STATE REPRESENTATION AND SPELKE PRINCIPLES,0.12643678160919541,"Let W be the space of possible world states, where W ∈W consists of a collection of objects, each
associated with a category and with a 3D position in space. Throughout, we use Wt to refer the
world state from the t-th scene encountered by METAGEN, and ⃗W = {W0, . . . , WT } to refer to
the complete sequence of world states."
WORLD-STATE REPRESENTATION AND SPELKE PRINCIPLES,0.13218390804597702,"In our approach, Spelke principles constrain possible world states. Speciﬁcally, the assumption of
object permanence implies that each world state has a ﬁxed set of objects that does not change.
Next, the assumption that two objects cannot occupy the same physical position is implemented as
a prior over the locations of the objects in a world state. The prior over the location of an object is a
uniform over 3-D space, except near another object, where the probability of placement decreases to
0. Finally, the assumption of spatio-temporal continuity implies that any moving object must exhibit
a continuous trajectory. Our Experimental task focuses on inferring permanent ﬁxtures in a room, so
we therefore do not implement this constraint, although doing so is a solved problem (Smith et al.,
2019; Kemp & Xu, 2009). Implementation details are available in the Appendix."
PERCEPTUAL MODULE,0.13793103448275862,"3.1.2
PERCEPTUAL MODULE"
PERCEPTUAL MODULE,0.14367816091954022,"The goal of METAGEN is to build a metacognitive representation of any object detection model
that can transform images into object detections, each associated with a category and a position in
the 2D image. Although METAGEN does not have access to ground truth (W), we assume access
to the camera’s state cs, which determines its position in space and orientation. This helps with
inferring and tracking the location of different objects (as this is not the main focus of our work). We
additionally assume access to video boundaries, enabling METAGEN to reset object representations
across scenes. Critically, the perceptual models functionally serves as a black box, and METAGEN
does not have any access to the internal state of the model, knowledge about its performance, or any
feedback about the ﬁnal inferences."
PERCEPTUAL MODULE,0.14942528735632185,"Formally, let Ot = ⃗o be the collection of observations associated with the t-th scene. Each observa-
tion ocs ∈Ot is a percept associated with camera state cs (from the complete camera trajectory cj),"
PERCEPTUAL MODULE,0.15517241379310345,Under review as a conference paper at ICLR 2021
PERCEPTUAL MODULE,0.16091954022988506,"and consists of a set of point detections obtained from the object recognition model, each containing
the object’s category and 2-D location on an image: d = (c, x, y)."
METACOGNITIVE REPRESENTATION,0.16666666666666666,"3.1.3
METACOGNITIVE REPRESENTATION"
METACOGNITIVE REPRESENTATION,0.1724137931034483,"The metacognitive representation of the perceptual model, V , consists of two probability distri-
butions per object category. The ﬁrst distribution captures the model’s propensity to hallucinate
objects, represented through a Poisson distribution with rate λc for object category c. The second
distribution captures the model’s propensity to correctly detect an object that’s in view. We represent
this through a Geometric distribution with rate pc, which helps us account for the fact that object
models can sometimes detect the same object multiple times. Under this formulation, the model’s
miss rate for an object in category c is 1 −pc."
METACOGNITIVE REPRESENTATION,0.1781609195402299,"Because each distribution is captured by a single parameter, the perceptual module’s metacognition
V can then be represented as a |C|-by-2 matrix storing each category’s hallucation rate λc and miss
rate 1 −pc. This deﬁnes a generative model such that V (O, W, cs) describes the probability that the
visual system would produce percept O when processing world state W ∈W with the camera state
cs. This is calculated by processing every object in W that is visible from camera state cs, through
the metacognitive representation captured in V ."
METACOGNITIVE DYNAMICS,0.1839080459770115,"3.1.4
METACOGNITIVE DYNAMICS"
METACOGNITIVE DYNAMICS,0.1896551724137931,"The generative model described so far captures how METAGEN learns a metarepresentation of its
perceptual module’s performance. However, an object detection model’s propensity to hallucinate or
miss objects can vary across scenes, and leaving ﬂexibility in V to be adjusted is a desirable property.
At the same time, experience in a previous scene includes critical information about the perceptual
module that should inform expectations about its performance in a new scene. Our generative model
therefore includes an evolving kernel Gt (Figure 1), capturing changing priors over V ."
METACOGNITIVE DYNAMICS,0.19540229885057472,"When t = 0, each category’s initial hallucination rate λc is initialized from a Gamma distribution
with parameters α = 1 and β = 1. After inference in a scene, the prior over λc evolves by computing
the inferred number of hallucinations at time t (based on the difference between world state Wt
and observations Ot), updating α and β as the conjugate prior over the Poisson distribution in the
metacognition."
METACOGNITIVE DYNAMICS,0.20114942528735633,"Beliefs about the miss rates evolve in an analogous manner. When t = 0, the detection rate pc for
each category is initialized from a Beta distribution with parameters α = 1 and β = 1 (equivalent to
a uniform distribution). The parameters from the Beta distribution are then updated as the conjugate
prior over the Geometric distribution, based on the inferred missed detections (by comparing Wt
against Ot)."
INFERENCE,0.20689655172413793,"3.2
INFERENCE"
INFERENCE,0.21264367816091953,"Given a collection of observations ⃗O = {Ot}T
t=1 (i.e., multiple sets of percepts from multiple world
states) and the corresponding camera trajectories ⃗cj, our goal is to infer Pr(⃗V , ⃗W| ⃗O, ⃗cj), given by"
INFERENCE,0.21839080459770116,"Pr

⃗V , ⃗W | ⃗O, ⃗cj

∝ T
Y"
INFERENCE,0.22413793103448276,"t=1
Pr (Ot|Wt, Vt, cjt) Pr(Vt)Pr(Wt)
(1)"
INFERENCE,0.22988505747126436,"The posterior, Eq. 1, is approximated via Sequential Monte-Carlo using a particle ﬁlter. An estimate
of the joint posterior can be sequentially approximated via:"
INFERENCE,0.23563218390804597,"Pr

⃗V , ⃗W| ⃗O, ⃗cj

≈Pr

ˆV0
0 T
Y"
INFERENCE,0.2413793103448276,"t=1
Pr

Ot| ˆVt
t, ˆ
Wt
t, cjt

Pr

ˆ
Wt
t
Pr

ˆVt
t|
ˆ
Wt−1
t,
ˆ
Ot−1
t (2)"
INFERENCE,0.2471264367816092,Under review as a conference paper at ICLR 2021      plant tv
INFERENCE,0.25287356321839083,umbrella   plant plant
INFERENCE,0.25862068965517243,umbrella
INFERENCE,0.26436781609195403,umbrella 
INFERENCE,0.27011494252873564,umbrella  plant  plant
INFERENCE,0.27586206896551724,"Joint inference over 
and P(M) P(H)"
INFERENCE,0.28160919540229884,NN(     ) plant plant
INFERENCE,0.28735632183908044,Figure 2: Conceptual ﬁgure depicting inference.
INFERENCE,0.29310344827586204,"where ˆ
W1
T , . . . , ˆ
WT
T is the estimate W1, . . . , WT after T observations, and ˆV1
T , . . . , ˆ
VT
T is the
estimate V1, . . . , VT after T observations. Here the transition kernel, Pr( ˆVt
t|
ˆ
Wt−1
t,
ˆ
Ot−1
t) is gov-
erned by the belief dynamics described in section 3.1.4."
INFERENCE PROCEDURE DETAILS,0.2988505747126437,"3.2.1
INFERENCE PROCEDURE DETAILS"
INFERENCE PROCEDURE DETAILS,0.3045977011494253,"We sequentially approximate the joint posterior given in eq. 2 using a particle ﬁlter with 100 parti-
cles. The solution space to the inference problem that we consider is sparse, and Sequential Monte-
Carlo methods can suffer from degeneracy and loss of diversity in these situations. Our inference
approach solves these problems by implementing particle rejuvenation over objects, locations, and
beliefs about V . Rejuvenation is conducted using a series of Metropolis-Hastings moves with data-
driven proposals designed to obtain samples from Pr(⃗V , ⃗W| ⃗O, ⃗cj)."
INFERENCE PROCEDURE DETAILS,0.3103448275862069,"During object rejuvenation, a new state W ′
t is proposed by either adding or removing an object
from the current state Wt. Objects in Wt have an equal chance of being removed to produce W ′
t,
and new objects are added based on a data-driven distribution. This distribution samples object
categories from a categorical distribution biased towards categories observed in the current scene
t (see Appendix). With probability 0.5, location is sampled from a 3-D uniform distribution, and
otherwise sampled from a data-driven function with location biased toward 3D points likely to have
caused the 2D detections (see Appendix). The proposal for adding a new object or removing and
existing object from the world state is accepted or rejected according to the MH algorithm."
INFERENCE PROCEDURE DETAILS,0.3160919540229885,"After a proposed change to W, a second rejuvenation step is performed on locations, wherein an
object in W is randomly selected (with equal probability) to have a new location proposed. With
probability 0.5, the new location is drawn from a multivariate normal distribution centered on the
previous location, and it is otherwise sampled from a data-driven proposal (see Appendix). The
proposed world-state W ′ with a perturbed location is then accepted or rejected according to the MH
algorithm."
INFERENCE PROCEDURE DETAILS,0.3218390804597701,"Finally, a new metacognition V ′ is proposed by perturbing V . Each parameter in the |C|-by-2 matrix
is perturbed, with new values generated for V (i, j)t and for V (i, j)t−1 ∀i, j. The new value for each
V (i, j)t is sampled from the appropriate distribution (Beta for misses, Gamma for hallucinations)
with αt and βt, while the new value for V (i, j)t−1 is sampled with αt−1 and βt−1. These new
values are accepted or rejected according to the MH algorithm."
INFERENCE PROCEDURE DETAILS,0.3275862068965517,"This three-step rejuvenation process for world states, locations, and metacognition is done for each
particle for 500 iterations and the last state reached in the chain is used as the new rejuvenated
particle."
INFERENCE PROCEDURE DETAILS,0.3333333333333333,Under review as a conference paper at ICLR 2021
ESTIMATING V,0.3390804597701149,"3.2.2
ESTIMATING V"
ESTIMATING V,0.3448275862068966,"After all T time steps in the particle ﬁlter, we estimate VT by taking the expectation of the marginal
distribution by averaging across particles weighted by their likelihood l: ˆV T
T,µ = E[ ˆV T
T | ⃗O] ="
"M
PM",0.3505747126436782,"1
M
PM
m=1( ˆV T
T,m ∗lm), where m indexes the particles. This ˆV T
T,µ is the ﬁnal estimate of the be-
lief about the true V after all observations have been observed. Given ˆV T
T,µ, the posterior predictive
distribution is deﬁned as:"
"M
PM",0.3563218390804598,"Pr
 ˆ⃗W| ⃗O, ⃗cj, ⃗V = ˆV T
T,µ

∝ T
Y"
"M
PM",0.3620689655172414,"t=1
Pr

Ot| ˆWt, cjt, Vt = ˆV T
T,µ

Pr

ˆWt

(3)"
"M
PM",0.367816091954023,"This posterior predictive distribution can then be used to make better inferences about world states
for novel scenes, WT +1, . . ., or for reassessing previous world states W1, . . . , WT , that were origi-
nally inferring using a less informed V ."
EXPERIMENTS,0.3735632183908046,"4
EXPERIMENTS"
EXPERIMENTS,0.3793103448275862,"To evaluate METAGEN, we created a dataset of trajectories navigating simulated indoor scenes,
sampling several viewpoints per trajectory. We ran several object detection models on these images
and tested METAGEN’s ability to infer the underlying world states and properties of the object
detection models causing the detections."
DATASET,0.3850574712643678,"4.1
DATASET"
DATASET,0.39080459770114945,"We evaluate METAGEN on a dataset of simulated indoor scenes observed from multiple perspec-
tives by a moving agent (Figure 2). We generate scenes and render frames in the ThreeDWorld
physical simulation platform (Gan et al., 2020). Each scene is initialized as an empty 13-unit-by-
13-unit room. The number of objects in the scene is uniformly drawn from the counting numbers
up to 4 and each object is uniformly assigned one of 5 object categories: potted plant, chair, bowl,
tv, or umbrella. We chose a canonical object model for each category. We sequentially place objects
at locations drawn from a uniform distribution, resampling if two objects are within 3 units of each
other or the walls so as to avoid collisions."
DATASET,0.39655172413793105,"The camera trajectory consists of camera positions and focal points. Roughly, the camera circles the
room looking toward the center, with noise from Gaussian processes added to the camera position
and focal points. The details and parameters are discussed in the Appendix. We generate frames by
querying the camera for an image at 20 linearly-spaced times."
DATASET,0.40229885057471265,"We generated 100 videos in total, randomly split into two sets of 50 videos. The ﬁrst set was used
to test METAGEN’s performance during online learning (Training), and the second set was used to
test METAGEN’s performance on a novel set of scenes after halting inference over V (Test)."
DATASET,0.40804597701149425,"METAGEN’s performance during online learning could be affected by the order in which the videos
are processed (for instance, METAGEN might beneﬁt if the most challenging videos appear later in
the sequence). Our results therefore average METAGEN’s performance across four different order-
ings of the videos. Videos were ﬁrst randomly labeled from 1 to 50, and the four counterbalanced
orders were: {1 ... 50}, {50 ... 1}, {26 ... 50, 1 ... 25}, and {25 ... 1, 26 ... 50}."
OBJECT DETECTION MODELS,0.41379310344827586,"4.2
OBJECT DETECTION MODELS"
OBJECT DETECTION MODELS,0.41954022988505746,"To test METAGEN’s capacity to learn and use a metacognition, we tested its performance using
three modern neural networks for object detection as its perception module. The three networks
were selected to represent a variety of state-of-the-art architectures. In particular, we use RetinaNet,
a one-stage detector (Lin et al., 2017); Faster R-CNN, a two-stage detector (Ren et al., 2015), and
DETR, a vision transformer (Carion et al., 2020). We process the outputs of the object detectors by
ﬁltering for the 5 object categories present in the scenes and performed Non-Maximum Suppression
(NMS) with an IoU (Intersection over Union) threshold of 0.4 (applied only for RetinaNet and"
OBJECT DETECTION MODELS,0.42528735632183906,Under review as a conference paper at ICLR 2021
OBJECT DETECTION MODELS,0.43103448275862066,"Faster R-CNN, as it is not typically used for DETR). As input to METAGEN, we took the top ﬁve
highest-conﬁdence detections per frame."
METRICS USED,0.4367816091954023,"4.3
METRICS USED"
METRICS USED,0.4425287356321839,"We evaluate METAGEN’s performance in two ways: 1) by testing whether it can learn an accurate
metacognition of the object detection module and 2) by testing whether the learned metacognition
leads to improved accuracy."
METRICS USED,0.4482758620689655,"To measure how well METAGEN learned a metacognition, we calculated the mean squared error
(MSE) between the inferred visual system ˆV and the true V . The true V was calculated for each
object detection system. The MSE is given by"
METRICS USED,0.4540229885057471,"MSE =
1
2|C| X c∈C"
METRICS USED,0.45977011494252873,"
(λc −ˆλc)2 + (pc −ˆpc)2
(4)"
METRICS USED,0.46551724137931033,where C is the the set of object classes.
METRICS USED,0.47126436781609193,"To measure METAGEN’s object detection accuracy, we could not use the standard mean Average
Precision (mAP) metric because the outputs of METAGEN are points in 3D space rather than 2D
bounding boxes. But we still needed a metric by which to compare METAGEN’s object detection
accuracy to those of the neural networks without a metacognition."
METRICS USED,0.47701149425287354,"To solve this problem, we projected METAGEN’s inferences about the 3D location of objects to a
point on each 2D image, and for the neural networks, we took the centroid of the bounding boxes.
This way, METAGEN and the neural networks’ detections have the same format: an object label
and a point on the image."
METRICS USED,0.4827586206896552,"To assess object detection accuracy on a given frame, we took the Jaccard similarity coefﬁcient (the
Intersection over Union for sets) of the object labels that were detected and the objects that were
actually present in a give frame. The Jaccard similarity is given by:"
METRICS USED,0.4885057471264368,"J(D, G) = |D ∩G|"
METRICS USED,0.4942528735632184,"|D ∪G|
(5)"
METRICS USED,0.5,"where D is the set of object labels that were detected (i.e. D = {chair, chair, bowl}) and G is the set
of objects in the ground-truth world state (i.e. G = {chair, bowl}). (In this example, J(D, G) = 2 3.)"
METRICS USED,0.5057471264367817,"A limitation of this metric is that it does not directly assess the spatial accuracy of the detections:
only the categorical accuracy per frame. That said, highly inaccurate inferences about a 3D location
would result in 2D projections onto the wrong frames, so this metric indirectly captures spatial
accuracy. Averaging across frames yields an accuracy per video, and averaging across videos yields
an overall accuracy."
COMPARISON MODELS,0.5114942528735632,"5
COMPARISON MODELS"
COMPARISON MODELS,0.5172413793103449,"Throughout, we test METAGEN’s performance while it is learning a metacognition as it processes
videos (METAGEN Online Learning), and its performance when learning is halted and all inferences
about world states are conditioned on the learned metacognition ˆV (METAGEN Learned ˆV ). We test
METAGEN Online Learning on our Training set, and then test METAGEN Learned ˆV on both the
Training set (assessing if its accuracy changes when it can retrospectively re-evaluate its inferences
after learning a metacognition), and the Test set."
COMPARISON MODELS,0.5229885057471264,"To test if the learned metacognition confers a beneﬁt beyond what can be obtained from the neural
networks alone, we used two comparison models. The ﬁrst comparison model (NN Output / META-
GEN Input) consists of the processed neural network output, which served as an input to METAGEN
(Section 4.2)."
COMPARISON MODELS,0.5287356321839081,"Our second comparison model (NN Output with Fitted Threshold) served as a more stringent test
that compared METAGEN against the Neural Network output, ﬁt to maximize accuracy. For each
neural network, we ﬁt the conﬁdence threshold so as to maximize accuracy on the ﬁrst 50 videos
using a grid-search. This required access to the ground-truth object labels, giving an advantage to"
COMPARISON MODELS,0.5344827586206896,Under review as a conference paper at ICLR 2021
COMPARISON MODELS,0.5402298850574713,Training Set
COMPARISON MODELS,0.5459770114942529,Avg. Accuracy 0.01 0.02 0.03 0.04
COMPARISON MODELS,0.5517241379310345,"0
10
20
30
40
50
Num videos observed MSE −0.50 −0.25 0.00 0.25 0.50"
COMPARISON MODELS,0.5574712643678161,Avg. Diff in Accuracy
COMPARISON MODELS,0.5632183908045977,Test Set 0.00 0.50 1.00 0.75 0.25
COMPARISON MODELS,0.5689655172413793,"NN Output Fitted Threshold
NN Output / METAGEN Input
METAGEN Online Learning"
COMPARISON MODELS,0.5747126436781609,"METAGEN Learned
Training Set
=      -        
Test Set"
COMPARISON MODELS,0.5804597701149425,=      -
COMPARISON MODELS,0.5862068965517241,"Figure 3: Results for METAGEN and comparison models with DETR. (A) METAGEN Online
Learning MSE as a function of scenes observed. (B) Average accuracy for all models in Training
Set and Test Set. (C) Accuracy difference between METAGEN Learned ˆV and the two baseline
models. Positive values indicated METAGEN outperforms the comparison models."
COMPARISON MODELS,0.5919540229885057,"this comparison model over METAGEN. The values of the ﬁtted threshold were 0.20 for DETR,
0.14 for RetinaNet, and 0.00 for Faster R-CNN."
RESULTS,0.5977011494252874,"6
RESULTS"
RESULTS,0.603448275862069,"To assess METAGEN’s ability to learn an accurate metacognitive representation V , we examined
the MSE as a function of the number of observed videos. Figure 3A shows the dynamics of the MSE
for V for the DETR model. 95% CIs over the MSE were calculated for each of the four runs by
bootstrapping over the 100 particles. After observing just 10 videos, the MSE dropped from 0.035
to 0.014, and after training videos, the MSE was at 0.009. Results were similar for RetinaNet (ﬁnal
MSE at 0.023) and Faster R-CNN (ﬁnal MSE at 0.010). This demonstrates the speed with which
METAGEN is able to learn an accurate metacognition V , without access to ground-truth world
states."
RESULTS,0.6091954022988506,"Does the learned metacognition enable METAGEN to make better inferences about what’s in a
scene? Figure 3B shows a barplot depicting the accuracies of all four models (Section 5) using
DETR. We ﬁnd that METAGEN outperforms the neural networks even with the ﬁtted conﬁdence
threshold. The error bars represent 95% conﬁdence intervals calculated by bootstrapping over the
videos. METAGEN outperformed the two comparison models for all three object detection models
(Table 1)."
RESULTS,0.6149425287356322,"To evaluate the magnitude of the difference between METAGEN with Learned ˆV and the two Neural
Network comparison models (NN Output / METAGEN input and NN Output with Fitted Thresh-
old), we calculated average accuracy difference in accuracy and boostrapped the mean difference to
obtain a 95% conﬁdence intervals over performance difference. METAGEN showed a substantial
improvement against both comparison models (Figure 3C for DETR; Table 2 for full results)."
RESULTS,0.6206896551724138,"Finally, to conﬁrm that METAGEN’s success can be attributed to its learned metacognition, rather
than purely to the Spelke object principles, we tested METAGEN with an erroneous metacognition
consisting of low (1e−12) miss rates and high (1.0) hallucination rates. Under these parameters,
METAGEN can only distinguish real detections from hallucinations based on whether the pattern of
detections across frames adheres to Spelke principles. This lesioned METAGEN led to a degenerate
solution where all detections were always treated as hallucinations, for all object detection models,
with an accuracy of 0.228, conﬁrming that Spelke principles alone cannot account for METAGEN’s
success."
DISCUSSION,0.6264367816091954,"7
DISCUSSION"
DISCUSSION,0.632183908045977,"We proposed that metacognition—a metarepresentation of one’s own computational processes—
can be a powerful tool for increasing a system’s robustness, enabling it to decide when to accept or
reject proposed representations obtained from pre-trained systems. We presented an implementation"
DISCUSSION,0.6379310344827587,Under review as a conference paper at ICLR 2021
DISCUSSION,0.6436781609195402,"Table 1: Main results
Perceptual
Model
Version
Avg. Accuracy
Avg. Accuracy
Module
Training
Test"
DISCUSSION,0.6494252873563219,"DETR
NN Output
Fitted threshold
0.361 (0.300, 0.422)
0.361 (0.313, 0.412)
METAGEN input
0.315 (0.257, 0.373)
0.292 (0.241, 0.346)
METAGEN
Online learning
0.609 (0.552, 0.667)
-
Learned ˆV
0.562 (0.494, 0.627)
0.631 (0.569, 0.695)
RetinaNet
NN Output
Fitted threshold
0.582 (0.512, 0.650)
0.543 (0.462, 0.568)
METAGEN input
0.510 (0.452, 0.568)
0.539 (0.468, 0.609)
METAGEN
Online learning
0.671 (0.595,0.744)
-
Learned ˆV
0.666 (0.594, 0.735)
0.647 (0.562, 0.730)
FasterR-CNN
NN Output
Fitted threshold
0.656 (0.589, 0.724)
0.630 (0.554, 0.705)
METAGEN input
0.656 (0.589, 0.724)
0.630 (0.554, 0.705)
METAGEN
Online learning
0.711 (0.639,0.775)
-
Learned ˆV
0.697 (0.628, 0.762)
0.650 (0.563, 0.732)"
DISCUSSION,0.6551724137931034,Table 2: Accuracy difference between METAGEN learned V and NN output with ﬁtted threshold
DISCUSSION,0.6609195402298851,"Perceputal Module
Avg. Difference on Test Set
% Increase in Accuracy"
DISCUSSION,0.6666666666666666,"DETR
0.270 (0.195, 0.350)
74.8%
RetinaNet
0.104 (0.0782, 0.130)
19.2%
Faster R-CNN
0.020 (0.000, 0.004)
3.1%"
DISCUSSION,0.6724137931034483,"of this idea, METAGEN, applied to the context of inferring what objects were in a scene. Given a
set of observations by an object detection model with unknown performance, METAGEN performed
joint inference over a metacognitive representation of the system and over the objects causing the
detections. We tested METAGEN using a variety of modern object detection models, and found
that METAGEN can quickly learn an accurate metacognition and use it to correct errors from the
classiﬁcation system, improving the system’s overall accuracy."
DISCUSSION,0.6781609195402298,"That improvement in accuracy was observed on a custom dataset of synthetic images, in some ways
quite different from those the object detection systems were trained on. Thus, our results show that
METAGEN can be particularly helpful in improving the ability of an object detection system to
generalize to out-of-sample data, especially when considering that METAGEN does not require any
internal access to the model or knowledge of its performance metrics. By learning a metacognition
for how the detection system performs on a new dataset, METAGEN is able ﬁlter the outputs of the
detection system to be tailored to a new, out-of-sample data."
DISCUSSION,0.6839080459770115,"While our focus was on object detection, we believe that similar ideas can be applied to related
domains. For instance, face-detection models could be augmented with a metacognition that learns
to represent its own accuracy, conditioned on an expectation that all faces should be, in principle,
equally discriminable. This could allow a METAGEN model to learn its biases (e.g., such as learn-
ing that it has poor performance for faces from people of color). From this standpoint, METAGEN
can also support explainable AI by generating a simpliﬁed metarepresentations of a model’s perfor-
mance, which can be easily analyzed. Similarly, because METAGEN can help infer the presence
of missed objects, or ﬂag hallucinations in an image, this approach could be fruitful as a way of
generating additional training data for self-supervised learning."
DISCUSSION,0.6896551724137931,"Finally, our work highlights how computational solutions that appear in humans can be fruitful
for approaching related problems in machine learning and AI. In recent years, models that capture
human reasoning have received substantial attention (e.g., Smith et al., 2019; Baker et al., 2017; Jara-
Ettinger et al., 2019; Battaglia et al., 2013). By understanding how humans leverage representations
of their cognitive processes to create more nuanced and accurate representations of the world, we
may also be better able to design human-like artiﬁcial cognition."
DISCUSSION,0.6954022988505747,Under review as a conference paper at ICLR 2021
DISCUSSION,0.7011494252873564,REPRODUCIBILITY
DISCUSSION,0.7068965517241379,"To ensure reproducibility, we provide all of the code for the METAGEN model, producing the
dataset, and the analyses conducted. Please see the anonymous git repo linked below for the code
and for a demo:"
DISCUSSION,0.7126436781609196,https://anonymous.4open.science/r/MetaGen-0391/
REFERENCES,0.7183908045977011,REFERENCES
REFERENCES,0.7241379310344828,"Giduthuri Sateesh Babu and Sundaram Suresh. Sequential projection-based metacognitive learn-
ing in a radial basis function network for classiﬁcation problems. IEEE transactions on neural
networks and learning systems, 24(2):194–206, 2012."
REFERENCES,0.7298850574712644,"Chris L Baker, Julian Jara-Ettinger, Rebecca Saxe, and Joshua B Tenenbaum. Rational quantitative
attribution of beliefs, desires and percepts in human mentalizing. Nature Human Behaviour, 1(4):
1–10, 2017."
REFERENCES,0.735632183908046,"Peter W Battaglia, Jessica B Hamrick, and Joshua B Tenenbaum. Simulation as an engine of physical
scene understanding. Proceedings of the National Academy of Sciences, 110(45):18327–18332,
2013."
REFERENCES,0.7413793103448276,"Susan Carey. The origin of concepts. Oxford university press, 2009."
REFERENCES,0.7471264367816092,"Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and
Sergey Zagoruyko. End-to-end object detection with transformers. In Andrea Vedaldi, Horst
Bischof, Thomas Brox, and Jan-Michael Frahm (eds.), Computer Vision – ECCV 2020, pp. 213–
229, Cham, 2020. Springer International Publishing. ISBN 978-3-030-58452-8."
REFERENCES,0.7528735632183908,"Michael T Cox. Metacognition in computation: A selected research review. Artiﬁcial intelligence,
169(2):104–141, 2005."
REFERENCES,0.7586206896551724,"Marco F. Cusumano-Towner, Feras A. Saad, Alexander K. Lew, and Vikash K. Mansinghka. Gen: A
general-purpose probabilistic programming system with programmable inference. In Proceedings
of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation,
PLDI 2019, pp. 221–236, New York, NY, USA, 2019. ACM. ISBN 978-1-4503-6712-7. doi: 10.
1145/3314221.3314642. URL http://doi.acm.org/10.1145/3314221.3314642."
REFERENCES,0.764367816091954,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-
erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248–255. Ieee, 2009."
REFERENCES,0.7701149425287356,"Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks, 2017."
REFERENCES,0.7758620689655172,"Chaz Firestone and Brian J Scholl. Cognition does not affect perception: Evaluating the evidence
for “top-down” effects. Behavioral and brain sciences, 39, 2016."
REFERENCES,0.7816091954022989,"Chuang Gan, Jeremy Schwartz, Seth Alter, Martin Schrimpf, James Traer, Julian De Freitas, Jonas
Kubilius, Abhishek Bhandwaldar, Nick Haber, Megumi Sano, et al. Threedworld: A platform for
interactive multi-modal physical simulation. arXiv preprint arXiv:2007.04954, 2020."
REFERENCES,0.7873563218390804,"Alison Gopnik. How we know our minds: The illusion of ﬁrst-person knowledge of intentionality.
Behavioral and Brain sciences, 16(1):1–14, 1993."
REFERENCES,0.7931034482758621,"Umut G¨uc¸l¨u and Marcel AJ van Gerven. Increasingly complex representations of natural movies
across the dorsal stream are shared between subjects. NeuroImage, 145:329–336, 2017."
REFERENCES,0.7988505747126436,"Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. Meta-learning in neural
networks: A survey, 2020."
REFERENCES,0.8045977011494253,"Magdalena Ivanovska, Audun Jøsang, Lance Kaplan, and Francesco Sambo. Subjective networks:
Perspectives and challenges. In International Workshop on Graph Structures for Knowledge Rep-
resentation and Reasoning, pp. 107–124. Springer, 2015."
REFERENCES,0.8103448275862069,Under review as a conference paper at ICLR 2021
REFERENCES,0.8160919540229885,"Julian Jara-Ettinger, Laura Schulz, and Josh Tenenbaum. The naive utility calculus as a uniﬁed,
quantitative framework for action understanding. PsyArXiv, 2019."
REFERENCES,0.8218390804597702,"Lance Kaplan, Federico Cerutti, Murat Sensoy, Alun Preece, and Paul Sullivan. Uncertainty aware
ai ml: why and how. arXiv preprint arXiv:1809.07882, 2018."
REFERENCES,0.8275862068965517,"Kohitij Kar and James J DiCarlo. Fast recurrent processing via ventrolateral prefrontal cortex is
needed by the primate ventral stream for robust core visual object recognition. Neuron, 109(1):
164–176, 2021."
REFERENCES,0.8333333333333334,"Charles Kemp and Fei Xu. An ideal observer model of infant object perception. In Advances in
Neural Information Processing Systems, pp. 825–832, 2009."
REFERENCES,0.8390804597701149,"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep con-
volutional neural networks. Advances in neural information processing systems, 25:1097–1105,
2012."
REFERENCES,0.8448275862068966,"Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman.
Building
machines that learn and think like people. Behavioral and brain sciences, 40, 2017."
REFERENCES,0.8505747126436781,"Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Doll´ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European
conference on computer vision, pp. 740–755. Springer, 2014."
REFERENCES,0.8563218390804598,"Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He, and Piotr Doll´ar. Focal loss for dense
object detection. 2017 IEEE International Conference on Computer Vision (ICCV), pp. 2999–
3007, 2017."
REFERENCES,0.8620689655172413,"Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.
Swin transformer: Hierarchical vision transformer using shifted windows, 2021."
REFERENCES,0.867816091954023,"Richard E Nisbett and Timothy D Wilson. Telling more than we can know: verbal reports on mental
processes. Psychological review, 84(3):231, 1977."
REFERENCES,0.8735632183908046,"Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Uniﬁed,
real-time object detection. In Proceedings of the IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), June 2016."
REFERENCES,0.8793103448275862,"Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object
detection with region proposal networks. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 28. Curran As-
sociates, Inc., 2015. URL https://proceedings.neurips.cc/paper/2015/file/
14bfa6bb14875e45bba028a21ed38046-Paper.pdf."
REFERENCES,0.8850574712643678,"Murat Sensoy, Lance Kaplan, and Melih Kandemir. Evidential deep learning to quantify classiﬁca-
tion uncertainty. In Advances in Neural Information Processing Systems, pp. 3179–3189, 2018."
REFERENCES,0.8908045977011494,"Kevin Smith, Lingjie Mei, Shunyu Yao, Jiajun Wu, Elizabeth Spelke, Josh Tenenbaum, and Tomer
Ullman. Modeling expectation violation in intuitive physics with coarse probabilistic object rep-
resentations. In Advances in Neural Information Processing Systems, pp. 8983–8993, 2019."
REFERENCES,0.896551724137931,"Elizabeth S Spelke and Katherine D Kinzler. Core knowledge. Developmental science, 10(1):89–96,
2007."
REFERENCES,0.9022988505747126,"Kartick Subramanian, Sundaram Suresh, and Narasimhan Sundararajan. A metacognitive neuro-
fuzzy inference system (mcﬁs) for sequential classiﬁcation problems.
IEEE Transactions on
Fuzzy Systems, 21(6):1080–1095, 2013."
REFERENCES,0.9080459770114943,"Tomer D Ullman, Elizabeth Spelke, Peter Battaglia, and Joshua B Tenenbaum. Mind games: Game
engines as an architecture for intuitive physics. Trends in cognitive sciences, 21(9):649–665,
2017."
REFERENCES,0.9137931034482759,"Zhengxia Zou, Zhenwei Shi, Yuhong Guo, and Jieping Ye. Object detection in 20 years: A survey,
2019."
REFERENCES,0.9195402298850575,Under review as a conference paper at ICLR 2021
REFERENCES,0.9252873563218391,"A
APPENDIX"
REFERENCES,0.9310344827586207,"The METAGEN model and inference were in implemented in the Julia-based probablistic program-
ming language Gen (Cusumano-Towner et al., 2019)."
REFERENCES,0.9367816091954023,"A.1
ADDITIONAL GENERATIVE MODEL PARAMETERS"
REFERENCES,0.9425287356321839,"Implementation of Spelke principles was as follows. World states without object persistence were
not included in W, which is equivalent to implicitly setting their prior probability to 0. The as-
sumption that two objects cannot occupy the same region in space was implemented through a prior
over W where the probability of having one object near another decreased according to a Gaussian
distribution with σ2 = 1."
REFERENCES,0.9482758620689655,"To account for noise in a network’s location detection Ot, each detection was modeled as having 2D
spatial noise, following a Gaussian with σx,y = 40 pixels."
REFERENCES,0.9540229885057471,"Finally, the full generative model requires specifying a prior distribution over camera positions and
focal points (although these are observable), set as uniform over 3D space, and a prior distribution
over expected number of objects in a scene, sampled from a geometric distribution with parameter
p = 0.9 (with a uniform prior over category type)."
REFERENCES,0.9597701149425287,"A.2
DETAILS ABOUT THE INFERENCE PROCEDURE"
REFERENCES,0.9655172413793104,"During rejuvenation, new objects are proposed to be added to W ′
t using a data-driven distribution.
The category of the object is comes from a categorical distribution, where 10% of the weight is
divided evenly among the categories, and the other 90% is divided proportionally to the number of
times that object was observed in the scene t. With probability 0.5, the location is sampled from a
3-D uniform distribution, and otherwise sampled from a data-driven function. Using the data-driven
function, the point is sampled based on proximity to the line-segment that, when projected onto the
2D image, would result in the point where the detection was observed. The probability of proposing
a particular point decreases with the distance from this line segment, following a Gaussian with
σ2 = 0.01."
REFERENCES,0.9712643678160919,"In the second rejuvenation step a new location is proposed for an object. With probability 0.5, the
new location is drawn from a multivariate normal distribution centered on the previous location, with
σx,y,z = 0.01. Otherwise, it is sampled as described in the previous paragraph (based on proximity
to the line segment that would results in the detection’s 2D location)."
REFERENCES,0.9770114942528736,"A.3
DETAILS ABOUT THE CAMERA TRAJECTORY IN THE DATASET"
REFERENCES,0.9827586206896551,"In our dataset of simulated indoor scenes, the agent’s trajectory consists of a series of camera po-
sitions and focal points. The camera trajectory is a circular path around the periphery of the room
with noise generated by a Gaussian process with an RBF kernel with σ = 0.7 and ℓ= 2.5. The
height of the camera is held constant at y = 2. The focal point trajectory is a Gaussian process with
a mean above the center of the ﬂoor and component-wise parameters σ = 0.7 and ℓ= 2. Figure 4
shows some example trajectories."
REFERENCES,0.9885057471264368,Under review as a conference paper at ICLR 2021
REFERENCES,0.9942528735632183,"Figure 4: Three sampled agent trajectories. The
wide circular patterns at the top are camera posi-
tions, and the smaller patterns near the bottom are
camera focal points."
