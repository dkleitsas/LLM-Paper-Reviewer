Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002036659877800407,"Understanding how activity in neural circuits reshapes following task learning
could reveal fundamental mechanisms of learning. Thanks to the recent advances
in neural imaging technologies, high-quality recordings can be obtained from hun-
dreds of neurons over multiple days or even weeks. However, the complexity
and dimensionality of population responses pose signiﬁcant challenges for analy-
sis. Existing methods of studying neuronal adaptation and learning often impose
strong assumptions on the data or model, resulting in biased descriptions that do
not generalize. In this work, we use a variant of deep generative models called
– cycle-consistent adversarial networks, to learn the unknown mapping between
pre- and post-learning neuronal activities recorded in vivo. To do so, we develop
an end-to-end pipeline to preprocess, train and evaluate calcium ﬂuorescence sig-
nals, and a procedure to interpret the resulting deep learning models. To assess
the validity of our method, we ﬁrst test our framework on a synthetic dataset with
known ground-truth transformation. Subsequently, we applied our method to neu-
ronal activities recorded from the primary visual cortex of behaving mice, where
the mice transition from novice to expert-level performance in a visual-based vir-
tual reality experiment. We evaluate model performance on generated calcium
imaging signals and their inferred spike trains. To maximize performance, we de-
rive a novel approach to pre-sort neurons such that convolutional-based networks
can take advantage of the spatial information that exists in neuronal activities. In
addition, we incorporate visual explanation methods to improve the interpretabil-
ity of our work and gain insights into the learning process as manifested in the
cellular activities. Together, our results demonstrate that analyzing neuronal learn-
ing processes with data-driven deep unsupervised methods holds the potential to
unravel changes in an unbiased way."
INTRODUCTION,0.004073319755600814,"1
INTRODUCTION"
INTRODUCTION,0.006109979633401222,"One of the main objectives in computational neuroscience is to study the dynamics of neural pro-
cessing and how neural activity reshapes in the course of learning. A major hurdle was the difﬁculty
in obtaining high-quality neural recordings of the same set of neurons across multiple experiments,
though such limitation in recording techniques has seen tremendous improvements in recent years.
With the advent of modern neural imaging technologies, it is now possible to monitor a large pop-
ulation of neurons over days or even weeks (Williams et al., 2018a; Steinmetz et al., 2021), thus
allowing experimentalists to obtain in vivo recordings from the same set of neurons across different
learning stages. Signiﬁcant efforts have been put into extracting interpretable and unbiased descrip-
tions of how cortical responses change with experience. Proposed approaches to model changes
in neuronal activity include linear latent variable models such as PCA, TCA, GPFA, GPFADS and
PSID (Cunningham & Byron, 2014; Williams et al., 2018b; Sani et al., 2021; Yu et al., 2009; Rutten
et al., 2020). Methods employing deep learning models but with linear changes or mapping include
LFADS and PfLDS (Pandarinath et al., 2018; Gao et al., 2016). While these methods enabled sub-
stantial progress in understanding the structure of neuronal activity, they do have strong assumptions
inherent in the modelling technique or the analysis, such as the linearity assumption in linear latent
variable models. Therefore, making sense of the unknown mapping between pre- and post-learning
neural activity in an unbiased manner remains a signiﬁcant challenge, and a data-driven method to
interpret the circuit dynamics in learning is highly desirable."
INTRODUCTION,0.008146639511201629,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.010183299389002037,"Thanks to their ability to self-identity and self-learn features from complex data, deep neural net-
works (DNNs) have seen tremendous success in many biomedical applications (Cao et al., 2018;
Zemouri et al., 2019; Piccialli et al., 2021). Speciﬁcally, deep generative networks have shown
promising results in analyzing and synthesizing neuronal activities in recent years. Pandarinath et al.
(2018) developed a variational autoencoder (VAE) to learn latent dynamics from single-trial spik-
ing activities and Prince et al. (2020) extended the framework to work with calcium imaging data.
Numerous work have demonstrated generative adversarial networks (GAN) are capable of synthe-
sizing neuronal activities that capture the low-level statistics of recordings obtained from behaving
animals (Molano-Mazon et al., 2018; Ramesh et al., 2019; Li et al., 2020)."
INTRODUCTION,0.012219959266802444,"In this work, we explore the use of cycle-consistent adversarial networks (Zhu et al., 2017), or Cy-
cleGAN, to learn the mapping between pre- and post-learning neuronal activities in an unsupervised
and data-driven manner. In other words, given the neural recordings of a novice animal, can we
translate the neuronal activities that correspond to the animal with expert-level performance, and
vice versa? The resulting transformation summarizes these changes in response characteristics in
a compact form and is obtained in a fully data-driven way. Such a transformation can be useful in
follow-up studies to 1) identify neurons that are particularly important for describing the changes
in the overall response statistics, not limited to ﬁrst or second order statistics; 2) detect response
patterns relevant for changes from pre- to post-learning; and 3) determine what experimental details
are of particular interest for learning."
INTRODUCTION,0.014256619144602852,"To learn the transformation, we derive a standardized procedure to train, evaluate and interpret the
CycleGAN framework. To improve the explainability of our work, we incorporate a self-attention
mechanism into our generator models and also employ a feature-importance visualization method
into our pipeline so that we can visualize and identify the input that the networks deemed relevant
in their decision making process. In addition, we introduced a novel neuron ordering method to im-
prove the learning performance of convolutional neural networks (CNN). To quantify the capability
of the proposed unsupervised learning method, we evaluate our method on two datasets: 1) an arti-
ﬁcially constructed dataset with a handcrafted transformation, and 2) recordings obtained from the
primary visual cortex of a behaving animal across multiple days. We then compare several metrics
and statistics between the recorded and translated calcium traces and their inferred spike trains."
METHODS,0.016293279022403257,"2
METHODS"
"ANIMAL EXPERIMENT
TO OBTAIN NEURONAL ACTIVITIES THAT CAN DEMONSTRATE PRE- AND POST-",0.018329938900203666,"2.1 ANIMAL EXPERIMENT
To obtain neuronal activities that can demonstrate pre- and post-
learning responses, we conducted a visual-based experiment which follows a similar procedure as
Pakan et al. (2018) and Henschke et al. (2020). Brieﬂy, a head-ﬁxed mouse was placed on a linear
treadmill that allows it to move forward and backward. A lick spout and two monitors were placed in
front of the treadmill and a virtual corridor with deﬁned grating pattern was shown to the mouse. A
reward (water drop) would be made available if the mouse licked within the predeﬁned reward loca-
tion (at 120-140 cm), in which a black screen is displayed as a visual clue. Figure A.1 illustrates the
experiment setup. The mouse should learn to utilize both visual information and self-motion feed-
back to maximize reward. The same set of neurons in the primary visual cortex were labelled with
GCaMP6 calcium indicator and monitored throughout 4 days of experiment, the relative changes in
ﬂuorescence (∆F/F0) over time were used as a proxy for an action potential. 4 mice were used in
the virtual-corridor experiment and all mice transitioned from novice to expert in the behaviour task
within 4 days of training. Mouse 1 took on average 6.94s per trial on day 1 and 4.43s per trial on
day 4, Table A.1 and A.2 shows the trial information of all the mice. Hence, this dataset can provide
excellent insights into how cortical responses reshape with experience."
CYCLEGAN,0.020366598778004074,"2.2 CYCLEGAN
CycleGAN (Zhu et al., 2017) is a GAN-based unsupervised framework that
learns the mapping between two unpaired distributions X and Y via the adversarial training and
cycle-consistency optimization. The framework has shown excellent results in a number of unsuper-
vised translation tasks, including natural language translation (Gomez et al., 2018) and molecular
optimization (Maziarka et al., 2020), to name a few."
CYCLEGAN,0.02240325865580448,"Let X and Y be two distributions with unknown mappings that correspond to (novice) pre- and
(expert) post-learning neuronal activity, respectively. CycleGAN consists of four DNNs: generator
G : X →Y that maps novice activities to expert activities and generator F : Y →X that
maps expert activities to novice activities; discriminator DX : X →[0, 1] and discriminator DY :
Y →[0, 1] that learn to distinguish novice and expert neural activities, respectively. In a forward"
CYCLEGAN,0.024439918533604887,Under review as a conference paper at ICLR 2022
CYCLEGAN,0.026476578411405296,"cycle step (X →Y →X, illustrated in Figure B.1), we ﬁrst sample a novice recording x from
distribution X and apply transformation G to obtain ˆy = G(x). We expect ˆy to resembles data
from the expert distribution Y , hence DY learns to minimize (1) LDY = −Ey∼Y [(DY (y) −1)2] +
Ex∼X[DY (G(x))2]. Similar to a typical GAN, generator G learns to deceive DY with the objective
of (2) LG = −Ex∼X[(DY (G(x))−1)2]. Note that these are same objectives in LSGAN (Mao et al.,
2017). However, DY can only verify if ˆy ∈Y , though cannot ensure that ˆy is the corresponding
expert activity of the novice recording x. Moreover, X and Y are not paired hence we cannot directly
compare ˆy with samples in Y . To tackle this issue, CycleGAN applies another transformation to
reconstruct the novice recording ¯x = F(ˆy) where the distance ∥x −¯x ∥or ∥x −F(G(x)) ∥
should be minimal. Therefore, the generators also optimize this cycle-consistent loss (3) Lcycle =
Ex∼X[∥x −F(G(x)) ∥] + Ey∼Y [∥y −G(F(y)) ∥]. Mean absolute error (MAE) was used as the
distance function, though other distance functions can also be employed. In addition, we would
expect ˆx = F(x) and ˆy = G(y) to be in distributions X and Y given that F : Y →X and
G : X →Y , hence the identity loss objective (4) LG
identity = Ey∼Y [∥y −G(y) ∥]."
CYCLEGAN,0.028513238289205704,"Taken all together, G optimizes the following objectives: (5) LG
total = LG + λcycleLcycle +
λidentityLG
identity where λidentity and λcycle are hyper-parameters for identity and cycle loss coefﬁcients.
All four networks are trained jointly where LF
total and LDX are similar to LG
total and LDY though in
opposite directions. In this work, we adapt the CycleGAN framework to learn the unknown map-
ping between pre- and post-learning neuronal activities recorded from the primary visual cortex of
behaving mice. In addition, we experiment with different GANs objective formulations on top of the
original LSGAN objective, including GAN (Goodfellow et al., 2014), WGANGP (Arjovsky et al.,
2017) and DRAGAN (Kodali et al., 2017). Table C.2 shows their exact formulations in CycleGAN."
MODEL PIPELINE,0.03054989816700611,"2.3 MODEL PIPELINE
We devise a consistent analysis framework, including data preprocessing
and augmentation, networks interpretation, and evaluation of the generated calcium ﬂuorescence
signals and their inferred spike trains. Figure C.1 illustrates the complete pipeline of our work.1"
MODEL PIPELINE,0.032586558044806514,"We denote the day 1 (pre-learning) and day 4 (post-learning) recording distributions to be X and Y .
With Mouse 1, W = 102 neurons from the primary visual cortex were monitored, as well as trial
information such as the virtual distance, licks and rewards. In total, 21471 and 21556 samples were
recorded on day 1 and 4. Since we want the generators and discriminators to identify patterns rele-
vant to the animal experiment in a data-driven manner, we do not incorporate any trial information
into the training data. We ﬁrst segment the two datasets with a sliding window of size H = 2048
along the temporal dimension (around 85 s in wall-time), resulting in data with shape (N, H, W)
for X and Y where N is the total number of segments. We select a stride size that space out each
segment evenly so that we obtained a sufﬁcient number of samples while keeping the correlations
between samples reasonably low. In order to take advantage of the spatiotemporal information in the
neuronal activities in a 2D CNN, we further convert the two sets to have shape (N, H, W, C) where
C = 1. Finally, we normalize each set to the range [0, 1], and divide them into train, validation and
test set with 3000, 200 and 200 samples respectively."
MODEL PIPELINE,0.034623217922606926,"To evaluate the transformation results of G and F, we can compare the cycle-consistency
MAE(X, F(G(X))) and MAE(Y, G(F(Y ))), as well as the identity losses MAE(X, F(X)) and
MAE(Y, G(Y )) (e.g. we expect F to apply no transformation to a novice sample x). We also evalu-
ate the generated data in terms of spike activities in the following distribution combinations: novice
against translated novice (X | F(Y )), novice against reconstructed novice (X | F(G(X))), ex-
pert against translated expert (Y | G(X)) and expert against reconstructed expert (Y | G(F(Y ))).
We use Cascade (Rupprecht et al., 2021) to infer spike trains from the recorded and generated cal-
cium signals to assess the credibility of the generated signals. We measure the following com-
monly used spike train similarities and statistics: 1) mean ﬁring rate for evaluating single neuron
statistics; 2) pairwise Pearson correlation for evaluating pairwise statistics; 3) pairwise van Rossum
distance (Rossum, 2001) for evaluating general spike train similarity. We evaluate these quantities
across the whole population for each neuron or neuron pairs and compare the resulting distributions
over these quantities obtained from the recorded and generated data. We, therefore, validate the
whole spatiotemporal ﬁrst and second-order statistics as well as general spike train similarities."
MODEL PIPELINE,0.03665987780040733,"To improve the explainability of this work we introduce a number of recently proposed model inter-
pretation methods into our pipeline. We design a self-attention generator architecture which allows"
MODEL PIPELINE,0.038696537678207736,1The software codebase will be made publicly available upon acceptance.
MODEL PIPELINE,0.04073319755600815,Under review as a conference paper at ICLR 2022
MODEL PIPELINE,0.04276985743380855,"the network to learn a set of attention masks such that it encourages the network to better focus on
speciﬁc areas of interest in the input and also enables us to visually inspect the learned attention
maps. In addition, we use GradCAM (Selvaraju et al., 2017), a method to visualize discriminative
region(s) learned by a CNN classiﬁer w.r.t to the input, to extract localization maps from the gen-
erators and discriminators. The self-attention mechanism and GradCAM visualization allow us to
verify and interpret that the networks are learning meaningful features. Moreover, these extracted
attention maps can reveal neurons or activity patterns that are informative in the neuronal learning
process. A detail description of the model architectures are available in Section D."
"NEURON ORDERING
CNNS WITH A SMALLER KERNEL CAN OFTEN PERFORM AS WELL OR EVEN BETTER",0.04480651731160896,"2.3.1 NEURON ORDERING
CNNs with a smaller kernel can often perform as well or even better
than models with larger kernels while consisting of fewer trainable parameters (He et al., 2016a;
Li et al., 2021). Nevertheless, a smaller kernel can also limit the receptive ﬁeld of the model, or
the region in the input that the model is exposed to in each convolution step (Araujo et al., 2019).
In addition, the recordings obtained from the virtual-corridor experiment were annotated based on
how visible the neurons were in the calcium image, rather than ordered in a particular manner (see
Figure A.1). This could potentially restrict CNNs with small receptive ﬁeld to learn meaningful
spatial-temporal information from the population responses. To mitigate this issue, we derive a
procedure to pre-sort X and Y , such that neurons that are highly correlated or relevant are nearby
in their ordering. A naive approach is to sort the neurons by their ﬁring rate or average pairwise
correlation, where the neuron with the highest ﬁring rate or the neuron that, on average, is most
correlated to other neurons is ranked ﬁrst in the data matrix. However, it is possible that not all high-
ﬁring neurons or most correlated neurons are the most inﬂuential in the learning process. Therefore,
we also explore a data-driven approach. Deep autoencoders have shown excellent results in feature
extraction and representation learning (Gondara, 2016; Wang et al., 2016; Tschannen et al., 2018),
and we can take advantage of its unsupervised feature learning ability."
"NEURON ORDERING
CNNS WITH A SMALLER KERNEL CAN OFTEN PERFORM AS WELL OR EVEN BETTER",0.04684317718940937,"We employ a deep autoencoder AE which learns to reconstruct calcium signals in X and Y
jointly. AE consists of 3 convolution down-sampling blocks, followed by a bottleneck layer, then
3 transposed-convolution up-sampling blocks. The down-sampling block consists of a convolution
layer followed by Instance Normalization (Ulyanov et al., 2016), Leaky ReLU (LReLU) activa-
tion (Maas et al., 2013) and Spatial Dropout (Tompson et al., 2015), whereas a transpose convolution
is used in the up-sampling block instead. We optimize the mean-squared error (MSE) reconstruction
loss on the training set of X and Y , then we use the per-neuron reconstruction error on the test set to
sort the neurons (in ascending order): order = argsort(0.5×[ MSE(X, AE(X))+MSE(Y, AE(Y )) ]).
The neuron sorting process is part of the data preprocessing step and is independent from the Cycle-
GAN framework."
SYNTHETIC DATA,0.048879837067209775,"2.3.2 SYNTHETIC DATA
CycleGAN was originally introduced for image-to-image translation.
Albeit the two image distributions are not aligned hence cannot be directly compared easily, one
could still visually inspect whether or not ˆx = F(y) and ˆy = G(x) are reasonable transformations.
However, it would be difﬁcult to visually inspect the two transformations with calcium signals. To
this end, we introduce an additional dataset Y = Φ(X) with a known transformation Φ, such that
G : X →Y = Φ(X) and F : Y = Φ(X) →X. We can then verify G(x) = ˆy = Φ(x) and
F(y) = ˆx = x. We deﬁned the spatiotemporal transformation Φ that can be identiﬁed visually as
follows: (6) Φ(x) = mdiagonalx + 0.5η, where mdiagonal is a diagonal mask to zero-out the lower
left corners of the signals and η ∼N(µx, σ2
x). µx and σx are the per-neuron mean and standard
deviation of X. Figure 1 shows an augmented example. Importantly, we shufﬂe the train set after the
augmentation procedure so that X and Y appears to be unpaired to the model. Whereas the test set
remains in its original paired arrangement so that we can compare ∥X−F(Y ) ∥and ∥Y −G(X) ∥."
SYNTHETIC DATA,0.05091649694501019,"Figure 1: (Left) original x and (Right) augmented y = Φ(x) calcium traces, where the bottom left
corner (yellow dashed triangle) in y has been masked out and noise being added to the segment."
SYNTHETIC DATA,0.05295315682281059,Under review as a conference paper at ICLR 2022
RESULTS,0.054989816700611,"3
RESULTS"
RESULTS,0.05702647657841141,"We assessed the CycleGAN framework on synthetic data with known ground truth and on experi-
mental data where we recovered trial information. We also experimented with different GAN objec-
tive formulations as well as different neuron ordering methods. All models presented below were
trained with the Adam optimizer (Kingma & Ba, 2014) for 200 epochs where all models converged.
We trained all CycleGAN models on a single NVIDIA A100 GPU which on average took 15 hours
to complete. It took an additional hour to train the autoencoder in the case where we pre-sort neurons
according to the AE reconstruction loss. Table C.1 details the hyper-parameters used."
"SYNTHETIC DATA
TO SHOW THAT OUR METHOD IS CAPABLE OF LEARNING SUBTLE DIFFERENCES IN CAL-",0.059063136456211814,"3.1 SYNTHETIC DATA
To show that our method is capable of learning subtle differences in cal-
cium traces, we ﬁrst ﬁt our model on the synthetic dataset. Figure 2 shows calcium signals of the
forward and backward cycle transformation of neuron 75 from a randomly selected test segment,
where AGResNet generators were trained with LSGAN objectives (more examples in Figure F.1).
Without paired samples, F(y) made a reasonable attempt in reconstructing the augmented region
in y, whereas G(x) was able to learn to mask out the appropriate regions in x. Since Φ(X) = Y
performed a systematic spatiotemporal transformation to X, one would expect the networks to learn
features that focus on the augmented region of the data. We, therefore, use GradCAM (Selvaraju
et al., 2017) localization maps to visualize regions of interest learned by the discriminators. The
localization map of discriminator DY (Y ) when given an augmented sample y = Φ(x), shown in
Figure 3, demonstrates a high level of attention around the edge of the diagonal region. This indi-
cates that DY learned to distinguish whether or not a given sample is from distribution Y = Φ(X)
by predominantly focusing on the edge of the masking area. On the other hand, since no augmen-
tation was done on the input to discriminator DX, the localization map does not appear to have a
particular structural area of focus at ﬁrst (c.f. Figure F.3). Interestingly, once we overlay the reward
zones on the input, we observe that the area of focus learned by DX is loosely aligned with the
reward zones. Note that reward zones are external task-relevant regions that are expected to shape
the neural activity in the primary visual cortex as the visual patterns change when the mouse enters
the reward zone. Our ﬁndings therefore suggest that DX learned distinctive patterns from highly
ranked neurons around the reward zones. Figure 3 shows the AG sigmoid masks from G(x). Both
attention masks ignored the augmentation region (i.e. bottom left corner), as information in that area
is not relevant in the G : X →Φ(X) transformation. Similar, F which should learn Φ(X) →X
also allocated less focus in the masked region in its reconstruction process, as it contains no useful
information. (see Figure F.3)."
"SYNTHETIC DATA
TO SHOW THAT OUR METHOD IS CAPABLE OF LEARNING SUBTLE DIFFERENCES IN CAL-",0.06109979633401222,"Since X and Y = Φ(X) are paired in the test set, this allows us to compute MAE(Y, G(X)) and
MAE(X, F(Y )) hence providing a good testbed to compare different generator architectures, GAN
objective formulations and neuron ordering methods. We also added the identity models as baseline,
which should have perfect cycle-consistent loss as F(G(x)) and G(F(y)) perform no operation on
the data. Nevertheless, despite the fact that Φ is a relatively simple augmentation, one would expect
the difference between X and Φ(X) to be small. Table 1 shows the direct comparison results of
different combinations of objective formulations, generator architectures and neuron ordering meth-
ods. Both ResNet and AGResNet achieved signiﬁcantly better results than the identity model. To
mitigate the issues of vanishing gradient and mode collapse, we used gradient penalty regulariza-
tion to enforce the 1-Lipschitz condition in the discriminator. We, therefore, tested 4 popular GAN
objectives with the CycleGAN framework. Interestingly, the LSGAN objectives achieved slightly
better results than GAN objectives while both performed better than identity. The two objectives
with gradient penalty obtained lower cycle-consistent errors than GAN and LSGAN, yet performed
signiﬁcantly worse in the intermediate transformations F(Y ) and G(X). This suggests that the dis-
criminators could be overpowered by the generators when trained with WGANGP and DRAGAN,
in which DX(F(Y )) and DY (G(X)) are neither informative nor impactful to the overall objective.
This is likely because the gradient penalty regularization further complicates the already perplexing
CycleGAN objectives. We employed 3 different methods to pre-sort neurons in the data, including
ﬁring rate, pairwise correlation and autoencoder reconstruction loss. In addition, to demonstrate
that 2D convolution can indeed better learn the spatial structure in neuronal activities, we trained
a 1D variant of AGResNet (denoted as 1D-AGResNet) as baseline which disregards all spatial
information. Overall, models trained on sorted neurons achieved better results compared to un-
ordered neurons and in most cases, sorting neurons according to the autoencoder reconstruction loss
performed the best. Moreover, 1D-AGResNet performed signiﬁcantly worse than its 2D coun-
terparts, suggesting that the spatial structure in the neural activities is indeed important. In the"
"SYNTHETIC DATA
TO SHOW THAT OUR METHOD IS CAPABLE OF LEARNING SUBTLE DIFFERENCES IN CAL-",0.06313645621181263,Under review as a conference paper at ICLR 2022
"SYNTHETIC DATA
TO SHOW THAT OUR METHOD IS CAPABLE OF LEARNING SUBTLE DIFFERENCES IN CAL-",0.06517311608961303,"remaining work, we use the LSGAN objective to train the generators with the AGResNet archi-
tecture along with neurons ordered based on autoencoder reconstruction loss as this combination
achieved the best overall results on the synthetic data."
"SYNTHETIC DATA
TO SHOW THAT OUR METHOD IS CAPABLE OF LEARNING SUBTLE DIFFERENCES IN CAL-",0.06720977596741344,"Figure 2: Forward and backward cycle steps of neuron 75 from a randomly selected test segment.
G should learn to translate signals in the yellow solid box to yellow dotted box, and F from green
dotted box to green solid box. We expect the signals in the green solid box resemble signals in the
yellow solid box and yellow dotted box to green dotted box. More examples are shown in Figure F.1"
"SYNTHETIC DATA
TO SHOW THAT OUR METHOD IS CAPABLE OF LEARNING SUBTLE DIFFERENCES IN CAL-",0.06924643584521385,"Figure 3: (Left) Learned attention masks AG1 and AG2 in AGResNet G given a random test
segment x ∼X. AG1 and AG2 both learned to ignore information in (to-be) masked region in x.
Note that AG1 and AG2 are 4 and 2 times lower-dimensional than the original input dimension.
(Right) GradCAM localization map of DY given a randomly select test segment y = Φ(x) ∼X.
The top panel shows the original input, where yellow and orange dotted lines mark the start and
end of each reward zone. The second panel shows the GradCAM localization map superimposed on
the input. Neurons were ordered based on AE reconstruction loss, the exact ordering is available in
Table E.1. Figure F.3 shows the attention gates and GradCAM map of F(y) and DX(x)."
"RECORDED DATA
AS OUR PROPOSED METHOD HAS SUCCESSFULLY LEARNED THE UNPAIRED TRANSFORMA-",0.07128309572301425,"3.2 RECORDED DATA
As our proposed method has successfully learned the unpaired transforma-
tions in the synthetic dataset, we now move on to the recordings obtained from the virtual-corridor
experiment where we attempt to learn the unknown mapping between pre- and post-learning neu-
ronal activity. Figure 4 shows the cycle transformation of neuron 50 from a randomly selected test
segment. Visually, G and F seems to be able to reconstruct ¯x = F(G(x)) and ¯y = G(F(y)),
and that the two generators are not simply passing through x and y in intermediate step ˆy = G(x)
and ˆx = F(y). To better analyse the transformation performance, we ﬁrst compare the generated
calcium ﬂorescence signals with the recorded test set data. The cycle-consistent loss on the test set
achieved a values of MAE(X, F(G(X))) = 0.0733 and MAE(Y, G(F(Y ))) = 0.0737. The identity
losses for MAE(Y, G(Y )) and MAE(Y, G(Y )) are also minimal, with values of 0.0101 and 0.0069,
respectively. For reference, MAE(X, Y ) = 0.3674. This suggest G and F are not simply passing
through the data without any processing. In addition, the low identity loss indicates that the gener-
ators can correctly identify whether or not the given input is already part of its target distribution.
Table G.1 reports the cycle-consistent and identity loss with different neuron ordering methods."
"RECORDED DATA
AS OUR PROPOSED METHOD HAS SUCCESSFULLY LEARNED THE UNPAIRED TRANSFORMA-",0.07331975560081466,"Since we lack paired data in the in vivo recordings, we cannot directly compare MAE(X, F(Y )) nor
MAE(Y, F(X)), in contrast to Section 3.1. In order to better analyse the two intermediate transfor-
mations ˆy = G(x) and ˆx = F(y), and show that G and F can indeed translate x and y into their
respective distributions ˆy ∼Y and ˆx ∼X, we also compare a set of spike train statistics. Section H"
"RECORDED DATA
AS OUR PROPOSED METHOD HAS SUCCESSFULLY LEARNED THE UNPAIRED TRANSFORMA-",0.07535641547861507,Under review as a conference paper at ICLR 2022
"RECORDED DATA
AS OUR PROPOSED METHOD HAS SUCCESSFULLY LEARNED THE UNPAIRED TRANSFORMA-",0.07739307535641547,"|X −F(Y )|
|X −F(G(X))|
|Y −G(X)|
|Y −G(F(Y ))|"
"RECORDED DATA
AS OUR PROPOSED METHOD HAS SUCCESSFULLY LEARNED THE UNPAIRED TRANSFORMA-",0.07942973523421588,(A) DIFFERENT MODELS WITH LSGAN OBJECTIVE
"RECORDED DATA
AS OUR PROPOSED METHOD HAS SUCCESSFULLY LEARNED THE UNPAIRED TRANSFORMA-",0.0814663951120163,"IDENTITY
0.4234 ± 0.0172
0
0.4234 ± 0.0172
0
RESNET
0.1617 ± 0.0071
0.1173 ± 0.0043
0.3743 ± 0.0391
0.1247 ± 0.0067
AGRESNET
0.1508 ± 0.0089
0.1107 ± 0.0051
0.2520 ± 0.0262
0.1467 ± 0.0084"
"RECORDED DATA
AS OUR PROPOSED METHOD HAS SUCCESSFULLY LEARNED THE UNPAIRED TRANSFORMA-",0.0835030549898167,(B) DIFFERENT OBJECTIVES WITH AGRESNET
"RECORDED DATA
AS OUR PROPOSED METHOD HAS SUCCESSFULLY LEARNED THE UNPAIRED TRANSFORMA-",0.0855397148676171,"GAN
0.1611 ± 0.0063
0.0948 ± 0.0069
0.2513 ± 0.0350
0.1491 ± 0.0050
LSGAN
0.1508 ± 0.0089
0.1107 ± 0.0051
0.2520 ± 0.0262
0.1467 ± 0.0084
WGANGP
0.2381 ± 0.0123
0.1600 ± 0.0098
0.3186 ± 0.0096
0.1960 ± 0.0093
DRAGAN
0.3832 ± 0.0115
0.0434 ± 0.0021
0.4012 ± 0.0207
0.0568 ± 0.0027"
"RECORDED DATA
AS OUR PROPOSED METHOD HAS SUCCESSFULLY LEARNED THE UNPAIRED TRANSFORMA-",0.08757637474541752,(C) DIFFERENT NEURON ORDERING WITH AGRESNET AND LSGAN OBJECTIVE
D-AGRESNET,0.08961303462321792,"1D-AGRESNET
0.2724 ± 0.0101
0.1878 ± 0.0115
0.3151 ± 0.0445
0.1655 ± 0.0115
ORIGINAL
0.1508 ± 0.0089
0.1107 ± 0.0051
0.2520 ± 0.0262
0.1467 ± 0.0084
FIRING RATE
0.1578 ± 0.0079
0.0722 ± 0.0044
0.1304 ± 0.0306
0.0842 ± 0.0036
CORRELATION
0.1556 ± 0.0044
0.0852 ± 0.0042
0.1369 ± 0.0209
0.0930 ± 0.0034
AUTOENCODER
0.1433 ± 0.0083
0.0639 ± 0.0032
0.1227 ± 0.0135
0.0671 ± 0.0030"
D-AGRESNET,0.09164969450101833,"Table 1: MAE comparison between synthetic and generated calcium signals. Results of (A) identity,
ResNet and AGResNet generators trained with LSGAN objective, (B) AGResNet generators
trained with different objectives and (C) neurons ordered by original annotation, ﬁring rate, pairwise
correlation and autoencoder reconstruction loss. We also trained a 1D variant of AGResNet as a
baseline which disregards the neuron spatial structure. Lowest values marked in bold. -0.21 0.49 1.19 ∆F/F"
D-AGRESNET,0.09368635437881874,"x
G(x)
F(G(x))"
D-AGRESNET,0.09572301425661914,"0
21
42
63
85
-0.28 0.39"
Y,0.09775967413441955,"1.06
y"
Y,0.09979633401221996,"0
21
42
63
85
Time (s) F(y)"
Y,0.10183299389002037,"0
21
42
63
85"
Y,0.10386965376782077,G(F(y))
Y,0.10590631364562118,"Figure 4: Forward and backward cycle steps of neuron 50 from a randomly selected test segment.
More examples are shown in Figure G.1 and Figure G.2"
Y,0.1079429735234216,"shows that per-neuron and per-segment comparison. We ﬁrst compare the ﬁring rate distribution of
each neuron between recorded and translated data (e.g. X vs F(Y ) and X vs F(G(X))). Examples
of the distribution comparisons are available in Section H. Since we expect that the distribution of
the generated data resemble of those from the recorded data, we can compare the KL divergence
for each neuron to quantify the transformation performance. The ﬁring rate distributions of F(Y )
and G(X) closely matched the distributions of X and Y , with average KL divergence of 1.1648
and 1.0697, respectively. Similarly, we can compute the pairwise correlation of each neuron w.r.t
the population and compare the distribution between translated and recorded data. X | F(Y ) and
Y |G(X) achieved an average KL divergence value of 0.0479 and 0.0493 in the pairwise correlation
comparison, both were signiﬁcantly better results than the baseline identity model. In addition, we
measure the van Rossum distance between X and F(Y ) for each neuron across 200 test samples,
and represent the results in the form of a heatmap. We can observe a clear diagonal line of low-
intensity values in the heatmaps for most neurons (e.g. Figure H.3 and H.4 for G and F). Hence,
there exists a spike train in X and Y that corresponds to a translated spike train in F(Y ) and G(X).
Table G.2 summaries the average KL divergence of the 3 spike statistics in different distribution
combinations, the results indicate that the generators can indeed learn the distribution translation
from pre- to post-learning neuronal activities, and vice-versa. We additionally trained separate mod-
els on the activities recorded from the other mice and obtained similar results, which are available
in Section I, J and K."
Y,0.109979633401222,Under review as a conference paper at ICLR 2022
Y,0.1120162932790224,"In the previous section, we were able to identify and interpret the learned features in a relatively
straightforward manner due to the systematic augmentation we introduced into the data. However,
visualizing and interpreting the attention maps on pre- and post-learning data could be more chal-
lenging as there would not be obvious patterns in the inputs to anticipate. Nevertheless, we would
expect a higher level of activities in the V1 neurons when the mouse is about to enter or inside the re-
ward zone, where the grating pattern on the virtual walls turn to black. Subsequently, the generators
and discriminators should learn meaningful features from responses surrounding the reward zones.
We ﬁrst visualize the sigmoid masks in AGResNet. Figure 5 shows the learned attention masks of
G superimposed on the latent inputs (see Figure G.3 for F). When the neurons were ordered, either
by ﬁring rate or autoencoder, we observe that the generators allocate more attention toward neurons
that rank higher. This suggest that by grouping neurons in a meaningful manner, the convolutional
layers in the generators can extract relevant features more effectively as compared to when neurons
were randomly ordered. The spike analysis showed that ordering neurons in a structured manner
does indeed yield better results across the board. In most cases, ordering the neurons based on the
reconstruction error achieved the best results."
Y,0.11405295315682282,"We then inspect the GradCAM localization maps of the discriminators. Similar to DX in the syn-
thetic dataset, we observed regions of high attention surrounding the reward zones in both DX and
DY (see Figure G.3 and 5). To better visualize the relationship between the area of focus learned
by the model and the virtual-corridor, we generate positional attention maps as shown in Figure 6.
We ﬁrst compute GradCAM maps for all test samples, then we average the activation value for each
neuron at each virtual position (160 cm in total) and plot the average activation value against dis-
tance. Effectively, these maps should represent the average attention learned by the models w.r.t. the
visual location of the animal. Importantly, the only objective the discriminators had was to distin-
guish if a given sample is from a particular distribution. Thus, the discriminators could have learned
trivial features. Instead, DX focused on a speciﬁc group of neurons at 100 - 130 cm in the virtual
environment, which coincides with the beginning of the reward zone. Moreover, DY learned to
focus on two groups of neurons with attention patterns that were also in alignment with the reward
zone in the virtual-corridor experiment. Similarly, we can extract these positional attention maps
for G(X) and F(Y ) following the same procedure, where we monitor the change in gradient in
the last residual block RB9 (bottom row in Figure 6). Interestingly, both generators focused on the
ﬁrst few neurons in their transformation operations. G focused on activities at the beginning of the
trial as well as activities in the reward zone; whereas with F, it paid higher level of attention to
activities right before the reward zone. This suggests that to learn the transformation from post- to
pre-learning responses, the activities the mouse exhibit as it approaches the reward zone is deemed
more important by the networks. Note that no trial information was incorporated into the training
data nor was it formulated in the objective function. Hence, these interesting patterns we observe
here were learned entirely by the networks themselves via the adversarial process."
Y,0.11608961303462322,"Figure 5: (Left) AG sigmoid masks in AGResNet G given a random test segment x ∼X. The
histograms on the right show the neuron-level attention. Note that AG1 and AG2 are at 4 and 2
times lower dimension than the input. (Right) GradCAM localization map of DY given a randomly
select test segment y ∼Y . The top panel shows the original input, where yellow and orange dotted
lines mark the start and end of each reward zones. The second panel shows the localization map
superimposed on the input. Neurons were ordered based on AE reconstruction loss. The exact
ordering is available in Table E.1. Plots of F and DX are shown in Figure G.3"
Y,0.11812627291242363,Under review as a conference paper at ICLR 2022
Y,0.12016293279022404,"Figure 6: Positional attention maps of (Top Left) DX, (Top Right) DY , (Bottom Left) G and (Bot-
tom Right) F w.r.t virtual position in the animal experiment. Yellow and orange dotted lines indicate
the start and end of the reward zone. Neurons were ordered based on AE reconstruction loss. The
exact ordering is available in Table E.1. The pre- and post-learning activities of top 10 highlighted
neurons learned by DX and DY are available in Figure G.5 and Figure G.6b, respectively."
DISCUSSION,0.12219959266802444,"4
DISCUSSION"
DISCUSSION,0.12423625254582485,"We demonstrated that the CycleGAN (Zhu et al., 2017) framework is a capable data-driven method
to model the translation between pre- and post-learning responses recorded in vivo. With self-
attention and feature-importance visualization methods, we are able to visualize information that
the networks deemed important in their translation and discrimination process. Intriguingly, without
providing trial information in the training process, the networks self-identiﬁed activities surrounding
the reward zone in the virtual-corridor experiment to be highly inﬂuential, which aligns with our
understanding that the responses in the visual cortex were shaped by the change of visual cues. In
addition, we introduced a novel and simple to implement neuron ordering method enabling more
effective learning by convolutional-based networks."
DISCUSSION,0.12627291242362526,"A signiﬁcant portion of the neuronal activity validation in Section 3.2 was performed in spike trains
inferred from the recorded and generated calcium ﬂuorescent signals using Cascade (Rupprecht
et al., 2021), which is a recently introduced method that has outperformed existing model-based
algorithms. However, reliable spike inference from ﬂuorescent calcium indicators signals remains an
active area of research (Theis et al., 2016). For instance, Vanwalleghem et al. (2020) demonstrated
that spiking activities could be missed due to the implicit non-negativity assumption in calcium
imaging data which exists in many deconvolution algorithms, including Cascade. Nonetheless, we
would like to emphasize that Cascade was used to deconvolve calcium signals for all distributions
of data and therefore all inferred spike trains experienced the same bias. Another notable constraint
in our method is the fundamental one-to-one mapping limitation in the CycleGAN framework. The
generators learn a deterministic mapping between the two domains and only associate each input
with a single output. However, most cross-domain relationships consist of one-to-many or many-
to-many mappings. More recently proposed methods, such as Augmented CycleGAN (Almahairi
et al., 2018), aim to address such fundamental limitations by introducing auxiliary noise to the two
distributions, and are thus able to generate outputs with variations. Nevertheless, these methods are
most effective when trained in a semi-supervised manner which is not possible with our unpaired
neural activity."
DISCUSSION,0.12830957230142567,"All in all, as deep unsupervised methods have become more expressive and explainable, and neu-
ronal activities in different learning phases from behaving animals have become more readily avail-
able, there is potential for novel insights into fundamental learning mechanisms. Future directions
include sorting neurons in 2D space, as they were recorded, such that the model can take advantage
of both vertical and horizontal spatial information."
DISCUSSION,0.13034623217922606,Under review as a conference paper at ICLR 2022
REFERENCES,0.13238289205702647,REFERENCES
REFERENCES,0.13441955193482688,"Amjad Almahairi, Sai Rajeshwar, Alessandro Sordoni, Philip Bachman, and Aaron Courville. Aug-
mented cyclegan: Learning many-to-many mappings from unpaired data. In International Con-
ference on Machine Learning, pp. 195–204. PMLR, 2018."
REFERENCES,0.1364562118126273,"Andr´e Araujo, Wade Norris, and Jack Sim. Computing receptive ﬁelds of convolutional neural net-
works. Distill, 2019. doi: 10.23915/distill.00021. https://distill.pub/2019/computing-receptive-
ﬁelds."
REFERENCES,0.1384928716904277,"Martin Arjovsky, Soumith Chintala, and L´eon Bottou.
Wasserstein gan.
arXiv preprint
arXiv:1701.07875, 2017."
REFERENCES,0.14052953156822812,"Chensi Cao, Feng Liu, Hai Tan, Deshou Song, Wenjie Shu, Weizhong Li, Yiming Zhou, Xiaochen
Bo, and Zhi Xie. Deep learning and its applications in biomedicine. Genomics, proteomics &
bioinformatics, 16(1):17–32, 2018."
REFERENCES,0.1425661914460285,"John P Cunningham and M Yu Byron. Dimensionality reduction for large-scale neural recordings.
Nature neuroscience, 17(11):1500–1509, 2014."
REFERENCES,0.1446028513238289,"Yuanjun Gao, Evan W Archer, Liam Paninski, and John P Cunningham. Linear dynamical neural
population models through nonlinear embeddings. Advances in neural information processing
systems, 29:163–171, 2016."
REFERENCES,0.14663951120162932,"Aidan N Gomez, Sicong Huang, Ivan Zhang, Bryan M Li, Muhammad Osama, and Lukasz Kaiser.
Unsupervised cipher cracking using discrete gans. arXiv preprint arXiv:1801.04883, 2018."
REFERENCES,0.14867617107942974,"Lovedeep Gondara. Medical image denoising using convolutional denoising autoencoders. In 2016
IEEE 16th international conference on data mining workshops (ICDMW), pp. 241–246. IEEE,
2016."
REFERENCES,0.15071283095723015,"Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information
processing systems, 27, 2014."
REFERENCES,0.15274949083503056,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778, 2016a."
REFERENCES,0.15478615071283094,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European conference on computer vision, pp. 630–645. Springer, 2016b."
REFERENCES,0.15682281059063136,"Julia U Henschke, Evelyn Dylda, Danai Katsanevaki, Nathalie Dupuy, Stephen P Currie, Theok-
litos Amvrosiadis, Janelle MP Pakan, and Nathalie L Rochefort. Reward association enhances
stimulus-speciﬁc representations in primary visual cortex. Current Biology, 2020."
REFERENCES,0.15885947046843177,"Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 4700–4708, 2017."
REFERENCES,0.16089613034623218,"Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with
conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 1125–1134, 2017."
REFERENCES,0.1629327902240326,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.164969450101833,"Naveen Kodali, Jacob Abernethy, James Hays, and Zsolt Kira. On convergence and stability of gans.
arXiv preprint arXiv:1705.07215, 2017."
REFERENCES,0.1670061099796334,"Bryan M Li, Theoklitos Amvrosiadis, Nathalie Rochefort, and Arno Onken. Calciumgan: A gen-
erative adversarial network model for synthesising realistic calcium imaging data of neuronal
populations. arXiv preprint arXiv:2009.02707, 2020."
REFERENCES,0.1690427698574338,Under review as a conference paper at ICLR 2022
REFERENCES,0.1710794297352342,"Zewen Li, Fan Liu, Wenjie Yang, Shouheng Peng, and Jun Zhou. A survey of convolutional neural
networks: analysis, applications, and prospects. IEEE Transactions on Neural Networks and
Learning Systems, 2021."
REFERENCES,0.17311608961303462,"Andrew L Maas, Awni Y Hannun, Andrew Y Ng, et al. Rectiﬁer nonlinearities improve neural
network acoustic models. In Proc. icml, volume 30, pp. 3. Citeseer, 2013."
REFERENCES,0.17515274949083504,"Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley.
Least squares generative adversarial networks. In Proceedings of the IEEE international confer-
ence on computer vision, pp. 2794–2802, 2017."
REFERENCES,0.17718940936863545,"Łukasz Maziarka, Agnieszka Pocha, Jan Kaczmarczyk, Krzysztof Rataj, Tomasz Danel, and Michał
Warchoł. Mol-cyclegan: a generative model for molecular optimization. Journal of Cheminfor-
matics, 12(1):1–18, 2020."
REFERENCES,0.17922606924643583,"Manuel Molano-Mazon, Arno Onken, Eugenio Piasini*, and Stefano Panzeri*. Synthesizing real-
istic neural population activity patterns using generative adversarial networks. In International
Conference on Learning Representations, 2018. URL https://openreview.net/forum?
id=r1VVsebAZ."
REFERENCES,0.18126272912423624,"Ozan Oktay, Jo Schlemper, Loic Le Folgoc, Matthew Lee, Mattias Heinrich, Kazunari Misawa,
Kensaku Mori, Steven McDonagh, Nils Y Hammerla, Bernhard Kainz, et al. Attention u-net:
Learning where to look for the pancreas. arXiv preprint arXiv:1804.03999, 2018."
REFERENCES,0.18329938900203666,"Janelle MP Pakan, Stephen P Currie, Lukas Fischer, and Nathalie L Rochefort. The impact of visual
cues, reward, and motor feedback on the representation of behaviorally relevant spatial locations
in primary visual cortex. Cell reports, 24(10):2521–2528, 2018."
REFERENCES,0.18533604887983707,"Chethan Pandarinath, Daniel J O’Shea, Jasmine Collins, Rafal Jozefowicz, Sergey D Stavisky,
Jonathan C Kao, Eric M Trautmann, Matthew T Kaufman, Stephen I Ryu, Leigh R Hochberg,
et al. Inferring single-trial neural population dynamics using sequential auto-encoders. Nature
methods, pp. 1, 2018."
REFERENCES,0.18737270875763748,"Francesco Piccialli, Vittorio Di Somma, Fabio Giampaolo, Salvatore Cuomo, and Giancarlo Fortino.
A survey on deep learning in medicine: Why, how and when? Information Fusion, 66:111–137,
2021."
REFERENCES,0.1894093686354379,"Luke Yuri Prince, Shahab Bakhtiari, Colleen J Gillon, and Blake Aaron Richards. Calfads: latent
factor analysis of dynamical systems in calcium imaging data. 2020."
REFERENCES,0.19144602851323828,"Poornima Ramesh, Mohamad Atayi, and Jakob H Macke. Adversarial training of neural encoding
models on population spike trains. 2019."
REFERENCES,0.1934826883910387,"Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedi-
cal image segmentation. In International Conference on Medical image computing and computer-
assisted intervention, pp. 234–241. Springer, 2015."
REFERENCES,0.1955193482688391,"MCW van Rossum. A novel spike distance. Neural computation, 13(4):751–763, 2001."
REFERENCES,0.1975560081466395,"Peter Rupprecht, Stefano Carta, Adrian Hoffmann, Mayumi Echizen, Antonin Blot, Alex C Kwan,
Yang Dan, Sonja B Hofer, Kazuo Kitamura, Fritjof Helmchen, et al. A database and deep learning
toolbox for noise-optimized, generalized spike inference from calcium imaging. Nature Neuro-
science, pp. 1–14, 2021."
REFERENCES,0.19959266802443992,"Virginia Rutten, Alberto Bernacchia, Maneesh Sahani, and Guillaume Hennequin. Non-reversible
gaussian processes for identifying latent dynamical structure in neural data. Advances in Neural
Information Processing Systems, 2020."
REFERENCES,0.20162932790224034,"Omid G Sani, Hamidreza Abbaspourazad, Yan T Wong, Bijan Pesaran, and Maryam M Shanechi.
Modeling behaviorally relevant neural dynamics enabled by preferential subspace identiﬁcation.
Nature Neuroscience, 24(1):140–149, 2021."
REFERENCES,0.20366598778004075,Under review as a conference paper at ICLR 2022
REFERENCES,0.20570264765784113,"Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh,
and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based local-
ization. In Proceedings of the IEEE international conference on computer vision, pp. 618–626,
2017."
REFERENCES,0.20773930753564154,"Nicholas A Steinmetz, Cagatay Aydin, Anna Lebedeva, Michael Okun, Marius Pachitariu, Marius
Bauza, Maxime Beau, Jai Bhagat, Claudia B¨ohm, Martijn Broux, et al. Neuropixels 2.0: A
miniaturized high-density probe for stable, long-term brain recordings. Science, 372(6539), 2021."
REFERENCES,0.20977596741344195,"Lucas Theis, Philipp Berens, Emmanouil Froudarakis, Jacob Reimer, Miroslav Rom´an Ros´on, Tom
Baden, Thomas Euler, Andreas S Tolias, and Matthias Bethge. Benchmarking spike rate inference
in population calcium imaging. Neuron, 90(3):471–482, 2016."
REFERENCES,0.21181262729124237,"Jonathan Tompson, Ross Goroshin, Arjun Jain, Yann LeCun, and Christoph Bregler. Efﬁcient object
localization using convolutional networks. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 648–656, 2015."
REFERENCES,0.21384928716904278,"Michael Tschannen, Olivier Bachem, and Mario Lucic.
Recent advances in autoencoder-based
representation learning. arXiv preprint arXiv:1812.05069, 2018."
REFERENCES,0.2158859470468432,"Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing in-
gredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016."
REFERENCES,0.21792260692464357,"Gilles Vanwalleghem, Lena Constantin, and Ethan K Scott.
Calcium imaging and the curse of
negativity. Frontiers in neural circuits, 14, 2020."
REFERENCES,0.219959266802444,"Yasi Wang, Hongxun Yao, and Sicheng Zhao. Auto-encoder based dimensionality reduction. Neu-
rocomputing, 184:232–242, 2016."
REFERENCES,0.2219959266802444,"Alex H. Williams, Tony Hyun Kim, Forea Wang, Saurabh Vyas, Stephen I. Ryu, Krishna V. Shenoy,
Mark Schnitzer, Tamara G. Kolda, and Surya Ganguli.
Unsupervised discovery of demixed,
low-dimensional neural dynamics across multiple timescales through tensor component anal-
ysis.
Neuron, 98(6):1099–1115.e8, 2018a.
ISSN 0896-6273.
doi: https://doi.org/10.1016/j.
neuron.2018.05.015. URL https://www.sciencedirect.com/science/article/
pii/S0896627318303878."
REFERENCES,0.2240325865580448,"Alex H Williams, Tony Hyun Kim, Forea Wang, Saurabh Vyas, Stephen I Ryu, Krishna V Shenoy,
Mark Schnitzer, Tamara G Kolda, and Surya Ganguli. Unsupervised discovery of demixed, low-
dimensional neural dynamics across multiple timescales through tensor component analysis. Neu-
ron, 98(6):1099–1115, 2018b."
REFERENCES,0.22606924643584522,"Byron M Yu, John P Cunningham, Gopal Santhanam, Stephen I Ryu, Krishna V Shenoy, and Ma-
neesh Sahani. Gaussian-process factor analysis for low-dimensional single-trial analysis of neural
population activity. Journal of neurophysiology, 102(1):614–635, 2009."
REFERENCES,0.22810590631364563,"Ryad Zemouri, Noureddine Zerhouni, and Daniel Racoceanu. Deep learning in the biomedical
applications: Recent and future status. Applied Sciences, 9(8):1526, 2019."
REFERENCES,0.23014256619144602,"Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation
using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference
on computer vision, pp. 2223–2232, 2017."
REFERENCES,0.23217922606924643,Under review as a conference paper at ICLR 2022
REFERENCES,0.23421588594704684,APPENDIX
REFERENCES,0.23625254582484725,"A
ANIMAL EXPERIMENT"
REFERENCES,0.23828920570264767,"DAY
NUM. TRIALS
EXPERIMENT DURATION
AVG. TRIAL DURATION
LICKS
REWARDS"
REFERENCES,0.24032586558044808,"1
129
894.73S
6.94S
2813
140
2
177
898.68S
5.08S
2364
182
3
192
897.16S
4.67S
2217
198
4
203
898.45S
4.43S
1671
213"
REFERENCES,0.24236252545824846,"Table A.1: Trial information of mouse 1 in the virtual-corridor experiment across 4 days of training,
which include the number of trials, average duration of each trial, total number of licks and the total
reward received by the mouse. The mouse achieved “expert” level by day 4 where it had a success
rate of > 75% at the task. All data were recorded at a sampling rate of 24Hz. Note that the same
mouse was used in the experiment."
REFERENCES,0.24439918533604887,"MOUSE
NUM. NEURONS
DAY 1 LICKS
DAY 1 REWARDS
DAY 4 LICKS
DAY 4 REWARDS"
REFERENCES,0.24643584521384929,"1
102
2813
140
1671
213
2
59
1038
75
1069
157
3
21
919
98
1065
302
4
32
1239
192
2493
230"
REFERENCES,0.2484725050916497,"Table A.2: The number of licks and rewards the 4 mice exhibit on day 1 and 4 in the virtual-corridor
experiment (see Section 2.1)."
REFERENCES,0.2505091649694501,Ca 2+  imaging
REFERENCES,0.2525458248472505,"n 1 n 2 n 3 
n 102"
REFERENCES,0.2545824847250509,"(N, 2048, 102, 1)"
REFERENCES,0.25661914460285135,"Figure A.1: (Left) illustration of the mouse virtual-environment setup. A deﬁned grating pattern is
displayed on the monitors and the mouse can move forward and backward in the virtual-corridor.
When the mouse approaches the reward zone, which was set at 120 cm to 140 cm from the initial
start point, the grating pattern would disappear and be replaced with a blank screen. If the mouse
licked within the virtual reward zone, then a droplet of water was given to the mouse as a reward.
Trials reset at 160 cm. The ﬁgure is based on Figure 1 in Pakan et al. (2018). (Right) original
coordinates and annotation order of the 102 recorded neurons. i.e. neuron #1 here would be at
index 0 in the data matrix, and neuron #65 would be at index 64. Neurons followed the same order
across all experiments."
REFERENCES,0.25865580448065173,Under review as a conference paper at ICLR 2022
REFERENCES,0.2606924643584521,"B
CYCLEGAN (a)"
REFERENCES,0.26272912423625255,cycle consistent loss (b)
REFERENCES,0.26476578411405294,"Figure B.1: Illustration of (a) the data ﬂow and (b) the cycle-consistent loss in a forward cycle
X →Y →X. G and F are generators that learn the transformation of X →Y and Y →X
respectively. We ﬁrst sample x ∼X, then apply transformation G to obtain ˆy = G(x). To ensure
ˆy resemble distribution Y , we train discriminator DY to distinguish generated samples from real
samples. However, even if ˆy is of distribution Y , we cannot verify that ˆy is the direct correspondent
of x. Hence, we apply transformation F which convert ¯x = G(ˆy) back to domain X. If both F
and G are reasonable transformations, then the cycle-consistency |x −¯x| should be minimal. The
backward cycle Y →X →X is a mirrored but opposite operation that run concurrently with the
forward cycle. Illustration re-created from Figure 3 in Zhu et al. (2017)."
REFERENCES,0.2668024439918534,"C
METHODS"
REFERENCES,0.26883910386965376,"HYPER-PARAMETERS
GAN
LSGAN
WGANGP
DRAGAN"
REFERENCES,0.2708757637474542,"FILTERS
32
KERNEL SIZE
4
REDUCTION FACTOR
2
ACTIVATION
LRELU
NORMALIZATION
INSTANCENORM
SPATIAL DROPOUT
0.25
WEIGHT INITIALIZATION
RANDOM NORMAL N(0, 0.02)
λ CYCLE
10
λ IDENTITY
5
λ GP
N/A
N/A
10
10
c
N/A
N/A
N/A
10
NUM. DIS UPDATE
1
1
5
1
αG
0.0001
αD
0.0004
DISTANCE FUNCTION
MEAN ABSOLUTE ERROR"
REFERENCES,0.2729124236252546,"Table C.1: The hyper-parameters used for each objective formulation. NUM. DIS UPDATE is the
number of discriminator updates for every generator update, such procedure was introduced in op-
timizing WGANGP Arjovsky et al. (2017). αG and αD denotes the learning rates of the generators
and discriminators. λ GP is the gradient penalty coefﬁcient for WGANGP and DRAGAN and c is
the Gaussian variance hyper-parameter in DRAGAN."
REFERENCES,0.27494908350305497,Under review as a conference paper at ICLR 2022
REFERENCES,0.2769857433808554,"MODEL
LOSS FUNCTIONS OF G AND DY"
REFERENCES,0.2790224032586558,"GAN
LG
= −E
x∼X"
REFERENCES,0.28105906313645623,"h
log(DY (G(x)
i"
REFERENCES,0.2830957230142566,"LDY
= −E
y∼Y"
REFERENCES,0.285132382892057,"h
log(DY (y))
i
−
E
x∼X"
REFERENCES,0.28716904276985744,"h
log(1 −DY (G(x)))
i"
REFERENCES,0.2892057026476578,"LSGAN
LG
= −E
x∼X"
REFERENCES,0.29124236252545826,"h
(DY (G(x) −1)2i"
REFERENCES,0.29327902240325865,"LDY
= −E
y∼Y"
REFERENCES,0.2953156822810591,"h
(DY (y) −1)2i
+
E
x∼X"
REFERENCES,0.2973523421588595,"h
DY (G(x))2i"
REFERENCES,0.29938900203665986,"WGANGP
LG
= −E
x∼X"
REFERENCES,0.3014256619144603,"h
DY (G(x))
i"
REFERENCES,0.3034623217922607,"LDY
=
E
x∼X"
REFERENCES,0.3054989816700611,"h
DY (G(x))
i
−E
y∼Y"
REFERENCES,0.3075356415478615,"h
DY (y)
i"
REFERENCES,0.3095723014256619,"+ λGP
E
x∼X,y∼Y"
REFERENCES,0.31160896130346233,"h 
∥∇D(ϵy + (1 −ϵ)G(x)) ∥2 −1
2i"
REFERENCES,0.3136456211812627,"DRAGAN
LG
=
E
x∼X"
REFERENCES,0.31568228105906315,"h
log(1 −DY (G(x)))
i"
REFERENCES,0.31771894093686354,"LDY
= −E
y∼Y"
REFERENCES,0.319755600814664,"h
log(Dy(y))
i
−
E
x∼X"
REFERENCES,0.32179226069246436,"h
log(1 −DY (G(x)))
i"
REFERENCES,0.32382892057026474,"+ λGP
E
y∼Y,z∼N (0,c)"
REFERENCES,0.3258655804480652,"h 
∥∇D(y + z) ∥2 −1
2i"
REFERENCES,0.32790224032586557,"Table C.2: The objective functions of the generator G and discriminator DY in GAN (Goodfellow
et al., 2014), LSGAN (Mao et al., 2017), WGANGP (Arjovsky et al., 2017) and DRAGAN (Kodali
et al., 2017) formulations. The loss functions for F and DX are symmetric to G and DY shown
above. λGP denotes the gradient penalty coefﬁcient in WGANGP and DRAGAN, ϵ is the [0, 1] linear
interpolation coefﬁcient for WGANGP and c is the Gaussian standard deviation for DRAGAN.
Note that the LG listed in the table are the generator loss, and the total generator loss remains
LG
total = LG + λcycleLcycle + λidentityLG
identity."
REFERENCES,0.329938900203666,Under review as a conference paper at ICLR 2022
REFERENCES,0.3319755600814664,cycle-consistent loss
REFERENCES,0.3340122199592668,"Autoencoder 
Preprocessing"
REFERENCES,0.3360488798370672,segementation
REFERENCES,0.3380855397148676,order neurons
REFERENCES,0.34012219959266804,augmentation
REFERENCES,0.3421588594704684,normalization
REFERENCES,0.34419551934826886,"Generator G 
Generator F"
REFERENCES,0.34623217922606925,"Discriminator D X 
Discriminator D Y 1 2 3 4 3 5 6 6"
REFERENCES,0.34826883910386963,Anaylsis
REFERENCES,0.35030549898167007,Ca 2+  comparsion
REFERENCES,0.35234215885947046,feature visualization
REFERENCES,0.3543788187372709,spike inference
REFERENCES,0.3564154786150713,spike analysis
REFERENCES,0.35845213849287166,CycleGAN
REFERENCES,0.3604887983706721,"Figure C.1: Illustration of the complete pipeline used in this work. Black directed lines represent the
ﬂow of data and the numbers indicate its order. Note that only the forward cycle step X →Y →X
is shown here for better readability."
REFERENCES,0.3625254582484725,Under review as a conference paper at ICLR 2022
REFERENCES,0.3645621181262729,"D
NETWORKS ARCHITECTURE"
REFERENCES,0.3665987780040733,"The generator architecture used in this work, shown in Figure D.1, is based on the ResNet-like (He
et al., 2016a) generator in CycleGAN with a number of modiﬁcations. Generally, the model consists
of 2 down-sampling blocks (DS1 and DS2), followed by 9 residual blocks (RBi for 1 ≤i ≤9), then
2 up-sampling blocks (US1 and US2). Each down-sampling block uses a 2D strided convolution
layer to reduce the spatiotemporal dimensions by factor of 2, which is then follows by Instance Nor-
malization, LReLU activation and Spatial Dropout. Each up-sampling block has the same structure
as the down-sampling blocks but with a transposed convolution layer instead. Each residual block
consists of two convolution blocks with padding added to offset the dimensionality reduction and a
skip connection that connect the input to the block with the output of the last convolution block via
element-wise addition. A convolution layer with a ﬁlter size of 1 then compresses the channel of the
output from US1, followed by a sigmoid activation to scale the ﬁnal output to have range [0, 1]."
REFERENCES,0.36863543788187375,"Residual connections are known to improve gradient ﬂow in CNN, thus mitigating the issue of
vanishing gradients and allowing deeper networks to be trained effectively (He et al., 2016a;b;
Huang et al., 2017). Therefore, shortcut connections are added between the down-sampling and
up-sampling blocks of the same level. For instance, the output of down-sampling block DS2 is
concatenated with the output of residual block RB9, then passes the resultant vector to the next up-
sampling block US1, such level-wise residual connection was ﬁrst introduced in Ronneberger et al.
(2015). We denote the level-wise residual connected network as ResNet."
REFERENCES,0.37067209775967414,"Furthermore, we adapted the Additive Attention Gate (AG) module in Oktay et al. (2018) as a
replacement for the concatenation operation in the residual connection described above. The yellow
block in Figure D.1 illustrates the AG structure. AG takes two inputs q and a, both with height HAG
and width WAG but varying channels, where q is the output of the previous processing block and a
is a shortcut connection from the down-sampling block of the same level. In AG1 for instance, q and
a are the output of RB9 and DS2 respectively. Both q and a are processed by two separate 1 × 1
convolution layers followed by Instance Normalization. The two vectors are then summed element-
wise such that overlapping regions from the two vectors would have higher intensity. We then apply
ReLU activation to eliminate negative values, followed by a 1 × 1 convolution layer with 1 ﬁlter
and Instance Normalization, resulting in a vector with shape (HAG, WAG, 1). Sigmoid activation is
applied to obtain a [0, 1] attention mask σ, where units closer to 1 indicate regions that are more
relevant. We apply the sigmoid mask to a, and concatenate it with q. Since q is a set of high-level
features processed by the stack of residual blocks, whereas a is the low-dimensional representation
of the original input. Therefore, the sigmoid attention mask should learn to eliminate information in
the input that is less relevant to the output. Moreover, as the attention mask is of the same dimension
of the input q, we can later superimpose the attention mask onto q to visualize the region of interest
learned by the model. We denote the attention-gated ResNet as AGResNet."
REFERENCES,0.3727087576374745,"We use a PatchGAN-based (Isola et al., 2017) discriminator architecture in this work, as it provides
more ﬁne-grained discrimination information to the generators instead of the single value discrim-
ination in the discriminator in vanilla GAN. DX and DY contain 3 down-sampling blocks where
each block reduces the spatiotemporal dimension by a factor of 2, like the down-sampling blocks in
the generators. For an input sample with shape (H = 2048, W = 102, C = 1), the discriminator
outputs a sigmoid activated vector with shape (256, 13, 1). Each element has range [0, 1] where a
value closer to 1 suggests that the corresponding patch is a real sample."
REFERENCES,0.37474541751527496,Under review as a conference paper at ICLR 2022
REFERENCES,0.37678207739307534,Padding
REFERENCES,0.3788187372708758,CONV Block ×9 Input
REFERENCES,0.38085539714867617,CONV Block DS 1
REFERENCES,0.38289205702647655,"CONV 
InstanceNorm"
REFERENCES,0.384928716904277,"LReLU 
2D Dropout"
REFERENCES,0.3869653767820774,CONV Block DS 2
REFERENCES,0.3890020366598778,Residual Block RB i
REFERENCES,0.3910386965376782,Attention Gate AG 1
REFERENCES,0.39307535641547864,CONV Block +
REFERENCES,0.395112016293279,"Padding 
CONV Block"
REFERENCES,0.3971486761710794,Padding
REFERENCES,0.39918533604887985,CONV Block US 1
REFERENCES,0.40122199592668023,Attention Gate AG 2
REFERENCES,0.40325865580448067,CONV Block
REFERENCES,0.40529531568228105,CONV Block
REFERENCES,0.4073319755600815,Padding
REFERENCES,0.4093686354378819,Sigmoid
REFERENCES,0.41140529531568226,Output q +
REFERENCES,0.4134419551934827,InstanceNorm
REFERENCES,0.4154786150712831,1×1 CONV
REFERENCES,0.4175152749490835,InstanceNorm
REFERENCES,0.4195519348268839,1×1 CONV a
REFERENCES,0.4215885947046843,InstanceNorm
REFERENCES,0.42362525458248473,1×1 CONV ReLU
REFERENCES,0.4256619144602851,Sigmoid ×
REFERENCES,0.42769857433808556,Concat
REFERENCES,0.42973523421588594,"Figure D.1: Architecture diagram of generator G and F. + and × denotes addition and element-wise
multiplication respectively. Note that the Attention Gate (AG) block can be replaced by a concate-
nation operation between the output of the previous block and the output from the down-sampling
block from the same-level. e.g. if AG is not used, then the input to US1 is concat(DS2, RB9)."
REFERENCES,0.4317718940936864,Under review as a conference paper at ICLR 2022
REFERENCES,0.43380855397148677,"E
NEURON ORDERING 1 2 3"
REFERENCES,0.43584521384928715,"45
6
7
8 9"
REFERENCES,0.4378818737270876,"10
11
12 13 14 15"
REFERENCES,0.439918533604888,"16
17
18 19 20 21 22 23
24 25 26 27 28 29 30 31 32 33 34 3536 37
38 39"
REFERENCES,0.4419551934826884,"40
41
42 43 4445"
REFERENCES,0.4439918533604888,"4647
48 4950 51 52
53 54 55 56 57
58 59 60 61 62
63 64 65 66 67 68 69
70 71 72 73 74
75 76 77
78 79 80 81"
REFERENCES,0.4460285132382892,"82
83
84 85 86 87 88 89 90
91 92 93 94 95
96 97 98 99 100 101 102"
REFERENCES,0.4480651731160896,"(a) Original 1 2 3 4 5 6 7 8 9 10 11 12 1314 15 16 17 18 19
20 21 22 23 24 25 26 27 28 29 30 31 32 33
34 35 36
37 38 39 40 41 42
43 44 45 46 47 48 49 50
51 52 53
54 55 56 57 58 59
60 61 62
63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78
79 80 81 82
83 84 85 86 87 88 89 90
91 92 93 94 95
96 97 98 99 100"
REFERENCES,0.45010183299389,"101
102"
REFERENCES,0.45213849287169044,"(b) Firing rate 1 2 3 4 5 6 7
8
9 10 11 12 13 14 15 16 17
18 19 20 21 22 23 24
25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45
46 47 48
49 50 51 52 53 54 55 56 57 58 59 60
61 62 63 64
65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81
82 83 84 85 86 87 88 89 90 91 92
93 94
95 96 97 98 99 100 101 102"
REFERENCES,0.45417515274949083,(c) Autoencoder
REFERENCES,0.45621181262729127,"Figure E.1: Neuron ordering based on (a) original annotation, (b) ﬁring rate and (c) autoencoder
reconstruction loss. The original order was based on how visible the neuron were in the calcium
imaging data, hence not sorted in a particular manner. One naive approach is to sort neurons base
on their overall ﬁring rate, such that active neurons can be closer in space thus allow more efﬁcient
learning by convolutional-based networks. We proposed to train an autoencoder AE which learns to
reconstruct X and Y jointly, and sort neurons base on the average reconstruction error on the test
set. See Section 2.3.1 for detail regarding different neuron ordering methods and their motivations."
REFERENCES,0.45824847250509165,"METHOD
ORDER"
REFERENCES,0.46028513238289204,"(A) N/A
1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,
25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45,
46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66,
67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87,
88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102
(B) FIRING RATE
18, 14, 12, 30, 8, 15, 36, 4, 21, 19, 3, 7, 43, 33, 20, 42, 13, 6, 11, 39, 2, 22, 75,
28, 55, 100, 31, 62, 10, 67, 63, 54, 17, 40, 52, 46, 99, 88, 61, 77, 57, 34, 85, 41,
27, 98, 84, 47, 65, 73, 5, 1, 44, 101, 58, 80, 16, 29, 87, 9, 26, 83, 92, 74, 24, 45,
49, 23, 97, 48, 68, 60, 71, 76, 59, 53, 70, 89, 25, 93, 32, 56, 66, 81, 72, 94, 38,
64, 79, 82, 50, 51, 96, 90, 37, 86, 95, 91, 102, 69, 35, 78
(C) CORRELATION
36, 27, 46, 28, 39, 30, 42, 20, 92, 10, 18, 11, 67, 14, 4, 33, 19, 77, 75, 13, 24, 99,
8, 43, 65, 101, 63, 7, 25, 44, 12, 76, 80, 9, 47, 3, 34, 71, 87, 52, 22, 1, 85, 61, 84,
29, 45, 31, 93, 100, 5, 58, 57, 17, 74, 21, 96, 55, 82, 91, 2, 48, 6, 56, 83, 62, 49,
16, 26, 81, 97, 53, 73, 94, 89, 59, 40, 95, 23, 32, 54, 66, 98, 72, 35, 88, 15, 41,
50, 60, 90, 70, 78, 68, 69, 86, 38, 51, 64, 79, 37, 102
(D) AUTOENCODER
89, 52, 100, 97, 83, 59, 64, 51, 93, 37, 96, 50, 99, 38, 61, 81, 87, 60, 53, 62, 55,
35, 40, 94, 21, 86, 95, 17, 32, 23, 72, 69, 3, 54, 41, 58, 49, 22, 91, 84, 90, 36, 92,
82, 39, 68, 66, 8, 73, 88, 71, 4, 46, 18, 11, 44, 70, 78, 25, 85, 29, 56, 20, 80, 28,
9, 26, 101, 65, 24, 5, 98, 1, 57, 43, 10, 12, 31, 63, 33, 75, 77, 19, 47, 45, 76, 27,
74, 42, 102, 30, 48, 7, 34, 13, 67, 16, 79, 2, 15, 14, 6"
REFERENCES,0.4623217922606925,"Table E.1: Neuron ordering based on (a) original annotation, (b) ﬁring rate, (c) average pairwise
correlation and (d) autoencoder reconstruction loss with respect to the original annotation order
recorded on Day 4 data. The physical location of each neuron is available in Figure E.1."
REFERENCES,0.46435845213849286,Under review as a conference paper at ICLR 2022
REFERENCES,0.4663951120162933,"F
RESULTS IN SYNTHETIC DATA -0.24 1.56 3.36"
REFERENCES,0.4684317718940937,"x
G(x)
F(G(x)) -0.15 0.31 0.78 ∆F/F"
REFERENCES,0.47046843177189407,"0
21
42
63
85
-0.29 1.29 2.87"
REFERENCES,0.4725050916496945,"0
21
42
63
85
Time (s)"
REFERENCES,0.4745417515274949,"0
21
42
63
85 25"
REFERENCES,0.47657841140529533,"Neuron
50
75"
REFERENCES,0.4786150712830957,(a) Forward cycle: X →Y →X -0.25 1.71 3.67
REFERENCES,0.48065173116089616,"y
F(y)
G(F(y)) -0.11 0.3 0.7 ∆F/F"
REFERENCES,0.48268839103869654,"0
21
42
63
85
-0.33 1.35 3.04"
REFERENCES,0.4847250509164969,"0
21
42
63
85
Time (s)"
REFERENCES,0.48676171079429736,"0
21
42
63
85 25"
REFERENCES,0.48879837067209775,"Neuron
50
75"
REFERENCES,0.4908350305498982,(b) Backward cycle: Y →X →Y
REFERENCES,0.49287169042769857,"Figure F.1: (a) forward and (b) backward cycle of neuron 25, 50 and 75 from AGResNet trained
with LSGAN objectives. Since X and Y in the test set are paired (see Section 2.3.2), we expect
x ≈F(y) and y ≈G(x). Notice that neurons with a higher index (e.g. neuron 75 in y) would
have more units being masked out and replaced by noise. We can see that generator F was able to
reconstruct the masked out regions (e.g. neuron 50 and 75 in F(y)) that resemble the traces in X."
REFERENCES,0.49490835030549896,Under review as a conference paper at ICLR 2022 1 26 51 76 102 1 26 51 76 102
REFERENCES,0.4969450101832994,Neuron
REFERENCES,0.4989816700610998,"0
21
42
63
85
Time (s) 1 26 51 76 102"
REFERENCES,0.5010183299389002,"x
G(x)
F(G(x))"
REFERENCES,0.5030549898167006,"(a) Forward cycle: X →Y →X
1 26 51 76 102 1 26 51 76 102"
REFERENCES,0.505091649694501,Neuron
REFERENCES,0.5071283095723014,"0
21
42
63
85
Time (s) 1 26 51 76 102"
REFERENCES,0.5091649694501018,"y
F(y)
G(F(y))"
REFERENCES,0.5112016293279023,(b) Backward cycle: Y →X →Y
REFERENCES,0.5132382892057027,"Figure F.2: (a) forward and (b) backward cycle of the entire 102 neurons from a randomly selected
test segment. Model was trained with AGResNet using the LSGAN objectives on the synthetic
dataset where Y = Φ(X), see Section 2.3.2 for detail."
REFERENCES,0.515274949083503,Under review as a conference paper at ICLR 2022
REFERENCES,0.5173116089613035,"Figure F.3: (Left) Self-learned attention masks in AG1 and AG2 in AGResNet G given a random
test segment y = Φ(x) ∼X. AG1 and AG2 were less focused in the masked region as it is
ﬁlled with Gaussian noise and it not informative in the reconstruction process. Note that AG1 and
AG2 are at 4 and 2 times lower-dimensional than the original input dimension (see Section D).
(Right) GradCAM localization maps of DX given a randomly select test segment x ∼X. The
top panel shows the original input, where yellow and orange dotted lines mark the start and end
of each reward zones. The second panel shows the GradCAM localization map superimposed on
the input. We observe that DX focused on neuronal activities surrounding the reward zone areas in
its discrimination process, which is expected since the activities in the visual cortex are shaped by
the visual-clues in the virtual-corridor experiment. Note that trial information such as reward zone
locations were not provided to the networks, the pattern observed here was learned by the models
themselves."
REFERENCES,0.5193482688391039,Under review as a conference paper at ICLR 2022
REFERENCES,0.5213849287169042,"G
RESULTS IN RECORDED DATA -0.24 0.48 1.21"
REFERENCES,0.5234215885947047,"x
G(x)
F(G(x)) -0.21 0.49 1.19 ∆F/F"
REFERENCES,0.5254582484725051,"0
21
42
63
85
-0.25 0.54 1.32"
REFERENCES,0.5274949083503055,"0
21
42
63
85
Time (s)"
REFERENCES,0.5295315682281059,"0
21
42
63
85 25"
REFERENCES,0.5315682281059063,"Neuron
50
75"
REFERENCES,0.5336048879837068,(a) Forward cycle: X →Y →X -0.24 0.47
Y,0.5356415478615071,"1.17
y
F(y)
G(F(y)) -0.28 0.39 1.06 ∆F/F"
Y,0.5376782077393075,"0
21
42
63
85
-0.25 0.61 1.47"
Y,0.539714867617108,"0
21
42
63
85
Time (s)"
Y,0.5417515274949084,"0
21
42
63
85 25"
Y,0.5437881873727087,"Neuron
50
75"
Y,0.5458248472505092,(b) Backward cycle: Y →X →Y
Y,0.5478615071283096,"Figure G.1: (a) forward and (b) backward cycle of neuron 6, 27 and 75 from a randomly selected
segment. Model was trained with AGResNet using the LSGAN objective on the recorded dataset.
Note that, unlike the synthetic dataset, the traces presented here are not unpaired. Hence, we cannot
directly compare x with F(y) nor y with G(x)."
Y,0.5498981670061099,Under review as a conference paper at ICLR 2022
Y,0.5519348268839104,"ORDER
|X −F(G(X))|
|X −F(X)|
|Y −G(F(Y ))|
|Y −G(Y )|"
D-AGRESNET,0.5539714867617108,"1D-AGRESNET
0.1806 ± 0.0077
0.1502 ± 0.0064
0.1811 ± 0.0163
0.1463 ± 0.0149
ORIGINAL
0.0874 ± 0.0037
0.0123 ± 0.0015
0.0766 ± 0.0025
0.0101 ± 0.0010
FIRING RATE
0.0760 ± 0.0030
0.0108 ± 0.0013
0.0752 ± 0.0028
0.0070 ± 0.0005
CORRELATION
0.0778 ± 0.0028
0.0111 ± 0.0012
0.0757 ± 0.0024
0.0089 ± 0.0022
AUTOENCODER
0.0733 ± 0.0025
0.0101 ± 0.0012
0.0737 ± 0.0027
0.0069 ± 0.0007"
D-AGRESNET,0.5560081466395111,"Table G.1: Cycle-consistent and identity loss in the test set of Mouse 1 recordings, where neu-
rons were ordered by 1) original annotation, 2) ﬁring rate 3) pairwise correlation and 4) autoen-
coder reconstruction loss. We also trained a 1D variant of the model as an additional baseline
(1D-AGResNet in the table) such that all spatial information of the neurons is disregarded. The
AGResNet generator architecture was used for G and F, and were optimized with LSGAN objec-
tives. The lowest loss in each category is marked in bold. For reference, |X −Y | = 0.3674±0.0236
in the test set."
D-AGRESNET,0.5580448065173116,"KL(X, F(Y ))
KL(X, F(G(X)))
KL(Y, G(X))
KL(Y, G(F(Y )))"
D-AGRESNET,0.560081466395112,(A) PAIRWISE CORRELATION
D-AGRESNET,0.5621181262729125,"IDENTITY
0.0875 ± 0.0549
0
0.0821 ± 0.0471
0
1D-AGRESNET
0.2027 ± 0.1040
0.4715 ± 0.2051
0.1901 ± 0.1003
0.4149 ± 0.2194
ORIGINAL
0.0552 ± 0.0419
0.0754 ± 0.0353
0.0583 ± 0.0553
0.0174 ± 0.0110
FIRING RATE
0.0507 ± 0.0358
0.0266 ± 0.0146
0.0504 ± 0.0438
0.0267 ± 0.0176
CORRELATION
0.0539 ± 0.0329
0.0339 ± 0.0176
0.0534 ± 0.0474
0.0205 ± 0.0133
AUTOENCODER
0.0479 ± 0.0372
0.0329 ± 0.0163
0.0493 ± 0.0448
0.0283 ± 0.0206"
D-AGRESNET,0.5641547861507128,(B) FIRING RATE
D-AGRESNET,0.5661914460285132,"IDENTITY
8.0705 ± 6.5500
0
7.7781 ± 6.7338
0
1D-AGRESNET
3.5688 ± 3.8895
7.9101 ± 5.3517
3.0572 ± 3.1114
8.3185 ± 5.5950
ORIGINAL
1.5401 ± 1.2491
2.0442 ± 2.0936
1.8527 ± 1.3563
1.4697 ± 1.1412
FIRING RATE
1.3402 ± 1.0450
1.2658 ± 1.0784
1.6994 ± 1.4170
1.4152 ± 1.2221
CORRELATION
1.4006 ± 1.1079
1.5450 ± 1.0786
1.4088 ± 1.0828
1.4674 ± 1.3505
AUTOENCODER
1.1648 ± 0.7934
1.4022 ± 1.2734
1.0697 ± 0.7689
1.2705 ± 1.1148"
D-AGRESNET,0.5682281059063137,(C) PAIRWISE VAN ROSSUM DISTANCE
D-AGRESNET,0.570264765784114,"IDENTITY
0.5510 ± 0.2960
0
0.3053 ± 0.1211
0
1D-AGRESNET
0.3613 ± 0.1597
0.8045 ± 0.1846
0.3764 ± 0.1565
1.3897 ± 0.8256
ORIGINAL
0.2790 ± 0.2186
0.1878 ± 0.0477
0.3216 ± 0.1352
0.1581 ± 0.0664
FIRING RATE
0.2539 ± 0.1708
0.1003 ± 0.0514
0.3080 ± 0.1173
0.1536 ± 0.0663
CORRELATION
0.2629 ± 0.1877
0.1905 ± 0.0485
0.2953 ± 0.1230
0.1797 ± 0.0696
AUTOENCODER
0.2387 ± 0.1488
0.1041 ± 0.0376
0.3031 ± 0.1138
0.1328 ± 0.0592"
D-AGRESNET,0.5723014256619144,"Table G.2: The average KL divergence between generated and recorded distributions of Mouse 1 in
(a) pairwise correlation, (b) ﬁring rate and (c) pairwise van Rossum distance. We trained AGResNet
with neurons ordered according to the following methods: 1) original annotation, 2) ﬁring rate, 3)
pairwise correlation and 4) autoencoder reconstruction loss. We also trained a 1D variant of the
model (denoted as 1D-AGResNet) such that all spatial information of the neurons is disregarded.
Note that we added the identity model (ﬁrst row of each sub-table) as a baseline where we should
obtain perfect cycle reconstruction. Entries with the lowest value are marked in bold."
D-AGRESNET,0.5743380855397149,Under review as a conference paper at ICLR 2022 1 26 51 76 102 1 26 51 76 102
D-AGRESNET,0.5763747454175153,Neuron
D-AGRESNET,0.5784114052953157,"0
21
42
63
85
Time (s) 1 26 51 76 102"
D-AGRESNET,0.5804480651731161,"x
G(x)
F(G(x))"
D-AGRESNET,0.5824847250509165,"(a) Forward cycle: X →Y →X
1 26 51 76 102 1 26 51 76 102"
D-AGRESNET,0.5845213849287169,Neuron
D-AGRESNET,0.5865580448065173,"0
21
42
63
85
Time (s) 1 26 51 76 102"
D-AGRESNET,0.5885947046843177,"y
F(y)
G(F(y))"
D-AGRESNET,0.5906313645621182,(b) Backward cycle: Y →X →Y
D-AGRESNET,0.5926680244399185,"Figure G.2: (a) forward and (b) backward cycle of the entire 102 neurons from a randomly se-
lected segment. Model was trained with AGResNet generators using the LSGAN objective on the
recorded dataset."
D-AGRESNET,0.594704684317719,Under review as a conference paper at ICLR 2022
D-AGRESNET,0.5967413441955194,"Figure G.3: (Left) self-learned sigmoid attention masks AG1 and AG2 in AGResNet F given a
random test segment y ∼Y , where neurons were sorted by the autoencoder AE reconstruction loss.
(Right) GradCAM localization map of DX given a randomly select test segment x ∼X. The top
panel shows the original input, where yellow and orange dotted lines mark the start and end of each
reward zones. The second panel shows the localization map superimposed on the input. Again, we
observe attention patterns that loosely align with the reward zones. Note that trial information such
as reward zone locations were not provided to the networks, the pattern observed here was learned
by the models themselves."
D-AGRESNET,0.5987780040733197,"0
511
1023
1535
2047 1 26 51 76 102 Input"
D-AGRESNET,0.6008146639511202,"0
127
255
383
511 1 7 13 19 26 AG1"
D-AGRESNET,0.6028513238289206,"0
255
511
767
1023 1 13 26 38 51 AG2"
D-AGRESNET,0.604887983706721,"0
511
1023
1535
2047
Time-step 1 26 51 76 102"
D-AGRESNET,0.6069246435845214,Output
D-AGRESNET,0.6089613034623218,(a) G(x)
D-AGRESNET,0.6109979633401222,"0
511
1023
1535
2047 1 26 51 76 102 Input"
D-AGRESNET,0.6130346232179226,"0
127
255
383
511 1 7 13 19 26 AG1"
D-AGRESNET,0.615071283095723,"0
255
511
767
1023 1 13 26 38 51 AG2"
D-AGRESNET,0.6171079429735234,"0
511
1023
1535
2047
Time-step 1 26 51 76 102"
D-AGRESNET,0.6191446028513238,Output
D-AGRESNET,0.6211812627291242,(b) F(y)
D-AGRESNET,0.6232179226069247,"Figure G.4: Self-learned sigmoid attention masks AG1 and AG2 in AGResNet (a) G given a
random test segment x ∼X and (b) F for a given random test segment y ∼Y . Top panels shows
the original input, where yellow and orange dotted lines mark the start and end of each reward zones.
Bottom panels show the generated outputs of G(x) and F(y). The learned sigmoid masks shown
here did not exhibit strong patterns as compare to Figure 5 and Figure G.3."
D-AGRESNET,0.6252545824847251,Under review as a conference paper at ICLR 2022 0.05 0.10 0.1 0.2 0.3 0.05 0.10 0.05 0.10 0.05 0.10 0.05 0.10 0.15 0.05 0.10 0.05 0.10 0.1 0.2
D-AGRESNET,0.6272912423625254,"0
21
42
64
85
Time (s) 0.05 0.10"
D-AGRESNET,0.6293279022403259,"N64
N51
N93
N37
N96
N50
N99
N38
N61
N81 ∆F/F"
D-AGRESNET,0.6313645621181263,(a) Pre-learning 0.25 0.50 0.2 0.4 0.05 0.10 0.15 0.05 0.10 0.1 0.2 0.05 0.10 0.05 0.10 0.05 0.10 0.1 0.2
D-AGRESNET,0.6334012219959266,"0
21
42
64
85
Time (s) 0.1 0.2"
D-AGRESNET,0.6354378818737271,"N64
N51
N93
N37
N96
N50
N99
N38
N61
N81 ∆F/F"
D-AGRESNET,0.6374745417515275,(b) Post-learning
D-AGRESNET,0.639511201629328,"Figure G.5: The (a) pre-learning and (b) post-learning traces of the top 10 neurons that DX paid the
most attention to. The positional attention maps of DX is available in Figure 6 (Top Left). Note that
the pre-learning and post-learning activities are not paired."
D-AGRESNET,0.6415478615071283,Under review as a conference paper at ICLR 2022 0.05 0.10 0.05 0.10 0.15 0.05 0.10 0.05 0.10 0.1 0.2 0.05 0.10 0.050 0.075 0.25 0.50 0.75 0.1 0.2 0.3
D-AGRESNET,0.6435845213849287,"0
21
42
64
85
Time (s) 0.05 0.10"
D-AGRESNET,0.6456211812627292,"N96
N50
N99
N38
N86
N95
N17
N32
N23
N72 ∆F/F"
D-AGRESNET,0.6476578411405295,(a) Pre-learning 0.1 0.2 0.05 0.10 0.05 0.10 0.05 0.10 0.05 0.10 0.05 0.10 0.1 0.2 0.050 0.075 0.05 0.10
D-AGRESNET,0.6496945010183299,"0
21
42
64
85
Time (s) 0.2 0.4"
D-AGRESNET,0.6517311608961304,"N96
N50
N99
N38
N86
N95
N17
N32
N23
N72 ∆F/F"
D-AGRESNET,0.6537678207739308,(b) Post-learning
D-AGRESNET,0.6558044806517311,"Figure G.6: The (a) pre-learning and (b) post-learning traces of the top 10 neurons that DY paid the
most attention to. The positional attention maps of DY is available in Figure 6 (Top Right). Note
that the pre-learning and post-learning activities are not paired."
D-AGRESNET,0.6578411405295316,Under review as a conference paper at ICLR 2022
D-AGRESNET,0.659877800407332,"H
SPIKE ANALYSIS"
D-AGRESNET,0.6619144602851323,"-0.4
-0.0
0.3
0.7
1.0
Pair-wise correlation 0 588 1176 1764 2353 Count"
D-AGRESNET,0.6639511201629328,Trial 0 x
D-AGRESNET,0.6659877800407332,x = F(y)
D-AGRESNET,0.6680244399185336,"-0.5
-0.1
0.2
0.6
1.0
Pair-wise correlation 0 747 1495 2243 2991 Count"
D-AGRESNET,0.670061099796334,Trial 5 x
D-AGRESNET,0.6720977596741344,x = F(y)
D-AGRESNET,0.6741344195519349,"-0.5
-0.1
0.3
0.6
1.0
Pair-wise correlation 0 744 1488 2232 2976 Count"
D-AGRESNET,0.6761710794297352,Trial 10 x
D-AGRESNET,0.6782077393075356,x = F(y)
D-AGRESNET,0.6802443991853361,"0.0
0.1
0.2
0.3
0.3
KL divergence 0 9 18 27 37 Count"
D-AGRESNET,0.6822810590631364,Correlation KL
D-AGRESNET,0.6843177189409368,"0.1
0.6
1.1
1.7
2.2
Hz 0 7 14 21 28 Count"
D-AGRESNET,0.6863543788187373,Neuron 25 x
D-AGRESNET,0.6883910386965377,x = F(y)
D-AGRESNET,0.6904276985743381,"0.0
0.2
0.4
0.6
0.8
Hz 0 15 30 45 61 Count"
D-AGRESNET,0.6924643584521385,Neuron 50 x
D-AGRESNET,0.6945010183299389,x = F(y)
D-AGRESNET,0.6965376782077393,"0.1
0.6
1.1
1.6
2.0
Hz 0 5 11 16 22 Count"
D-AGRESNET,0.6985743380855397,Neuron 75 x
D-AGRESNET,0.7006109979633401,x = F(y)
D-AGRESNET,0.7026476578411406,"0.0
1.0
2.0
3.0
4.0
KL divergence 0 3 7 11 15 Count"
D-AGRESNET,0.7046843177189409,Firing Rate
D-AGRESNET,0.7067209775967414,"132
92
17
14
64
177
7
147
102
196
180
98
x = F(y) 153 56 168 188 106 30 148 162 159 150 71 51 x"
D-AGRESNET,0.7087576374745418,Neuron 25 8 18 28 38 48 58
D-AGRESNET,0.7107942973523421,"1
88
13
40
44
142
51
166
143
199
141
178
x = F(y) 99 63 106 153 112 51 180 29 187 38 35 20 x"
D-AGRESNET,0.7128309572301426,Neuron 50 0 6 12 18 24 31
D-AGRESNET,0.714867617107943,"77
152
177
150
31
168
11
132
125
136
40
57
x = F(y) 104 106 137 157 109 85 29 60 146 51 101 65 x"
D-AGRESNET,0.7169042769857433,Neuron 75 11 20 30 40 50 60
D-AGRESNET,0.7189409368635438,"0.1
0.3
0.6
0.8
1.1
KL divergence 0 9 19 28 38 Count"
D-AGRESNET,0.7209775967413442,van-Rossum distance KL
D-AGRESNET,0.7230142566191446,"Figure H.1: Spike statistics of (top) ﬁring rate of 3 randomly selected neurons, (middle) pairwise
correlation of 3 randomly selected segments and (bottom) van Rossum distance of 3 randomly se-
lected segments between X and F(Y ) where X was ordered by autoencoder reconstruction loss.
The right columns show the KL divergence of each metrics and Table G.2 shows the mean and
standard deviation of the KL divergence comparisons."
D-AGRESNET,0.725050916496945,Under review as a conference paper at ICLR 2022
D-AGRESNET,0.7270875763747454,"-0.3
-0.0
0.3
0.7
1.0
Pair-wise correlation 0 853 1706 2559 3412 Count"
D-AGRESNET,0.7291242362525459,Trial 0
D-AGRESNET,0.7311608961303462,"x
x = F(G(x))"
D-AGRESNET,0.7331975560081466,"-0.5
-0.1
0.2
0.6
1.0
Pair-wise correlation 0 847 1694 2541 3388 Count"
D-AGRESNET,0.7352342158859471,Trial 5
D-AGRESNET,0.7372708757637475,"x
x = F(G(x))"
D-AGRESNET,0.7393075356415478,"-0.3
0.0
0.4
0.7
1.0
Pair-wise correlation 0 834 1668 2502 3337 Count"
D-AGRESNET,0.7413441955193483,Trial 10
D-AGRESNET,0.7433808553971487,"x
x = F(G(x))"
D-AGRESNET,0.745417515274949,"0.0
0.0
0.1
0.1
0.1
KL divergence 0 5 11 17 23 Count"
D-AGRESNET,0.7474541751527495,Correlation KL
D-AGRESNET,0.7494908350305499,"0.1
0.6
1.2
1.7
2.3
Hz 0 6 12 18 24 Count"
D-AGRESNET,0.7515274949083504,Neuron 25
D-AGRESNET,0.7535641547861507,"x
x = F(G(x))"
D-AGRESNET,0.7556008146639511,"0.0
0.2
0.3
0.5
0.7
Hz 0 15 31 46 62 Count"
D-AGRESNET,0.7576374745417516,Neuron 50
D-AGRESNET,0.7596741344195519,"x
x = F(G(x))"
D-AGRESNET,0.7617107942973523,"0.1
0.6
1.0
1.4
1.9
Hz 0 5 11 16 22 Count"
D-AGRESNET,0.7637474541751528,Neuron 75
D-AGRESNET,0.7657841140529531,"x
x = F(G(x))"
D-AGRESNET,0.7678207739307535,"0.0
1.4
2.8
4.3
5.7
KL divergence 0 4 9 13 18 Count"
D-AGRESNET,0.769857433808554,Firing Rate
D-AGRESNET,0.7718940936863544,"153
160
168
27
39
192
194
182
145
150
172
51
x = F(G(x)) 153 160 168 27 39 192 194 182 145 150 172 51 x"
D-AGRESNET,0.7739307535641547,Neuron 25 1 12 24 36 47 59
D-AGRESNET,0.7759674134419552,"1
137
169
2
112
51
180
29
187
38
35
20
x = F(G(x)) 99 63 106 153 112 51 180 29 187 38 35 20 x"
D-AGRESNET,0.7780040733197556,Neuron 50 0 5 11 17 23 29
D-AGRESNET,0.780040733197556,"104
22
134
157
45
148
127
74
121
94
101
65
x = F(G(x)) 104 22 134 157 45 148 127 74 121 94 101 65 x"
D-AGRESNET,0.7820773930753564,Neuron 75 1 11 22 33 43 54
D-AGRESNET,0.7841140529531568,"0.0
0.1
0.1
0.2
0.2
KL divergence 0 4 8 12 17 Count"
D-AGRESNET,0.7861507128309573,van-Rossum distance KL
D-AGRESNET,0.7881873727087576,"Figure H.2: Spike statistics of (top) ﬁring rate of 3 randomly selected neurons, (middle) pairwise
correlation of 3 randomly selected segments and (bottom) van Rossum distance of 3 randomly se-
lected segments between X and F(G(X)) where X was ordered by autoencoder reconstruction
loss. The right columns show the KL divergence of each metrics and Table G.2 shows the mean and
standard deviation of the KL divergence comparisons."
D-AGRESNET,0.790224032586558,Under review as a conference paper at ICLR 2022
D-AGRESNET,0.7922606924643585,"-0.5
-0.1
0.3
0.6
1.0
Pair-wise correlation 0 895 1790 2685 3581 Count"
D-AGRESNET,0.7942973523421588,Trial 0 y
D-AGRESNET,0.7963340122199593,y = G(x)
D-AGRESNET,0.7983706720977597,"-0.4
-0.1
0.3
0.6
1.0
Pair-wise correlation 0 938 1877 2816 3755 Count"
D-AGRESNET,0.8004073319755601,Trial 5 y
D-AGRESNET,0.8024439918533605,y = G(x)
D-AGRESNET,0.8044806517311609,"-0.4
-0.0
0.3
0.7
1.0
Pair-wise correlation 0 820 1641 2461 3282 Count"
D-AGRESNET,0.8065173116089613,Trial 10 y
D-AGRESNET,0.8085539714867617,y = G(x)
D-AGRESNET,0.8105906313645621,"0.0
0.1
0.1
0.2
0.3
KL divergence 0 10 21 31 42 Count"
D-AGRESNET,0.8126272912423625,Correlation KL
D-AGRESNET,0.814663951120163,"0.0
0.2
0.3
0.5
0.6
Hz 0 7 15 23 31 Count"
D-AGRESNET,0.8167006109979633,Neuron 25 y
D-AGRESNET,0.8187372708757638,y = G(x)
D-AGRESNET,0.8207739307535642,"0.0
0.5
1.0
1.5
2.1
Hz 0 22 45 68 91 Count"
D-AGRESNET,0.8228105906313645,Neuron 50 y
D-AGRESNET,0.824847250509165,y = G(x)
D-AGRESNET,0.8268839103869654,"0.0
0.2
0.3
0.5
0.6
Hz 0 16 32 48 64 Count"
D-AGRESNET,0.8289205702647657,Neuron 75 y
D-AGRESNET,0.8309572301425662,y = G(x)
D-AGRESNET,0.8329938900203666,"0.0
1.9
3.7
5.6
7.5
KL divergence 0 4 8 12 17 Count"
D-AGRESNET,0.835030549898167,Firing Rate
D-AGRESNET,0.8370672097759674,"18
15
194
47
86
155
169
36
49
128
184
197
y = G(x) 186 185 193 49 130 9 57 149 170 124 39 85 y"
D-AGRESNET,0.8391038696537678,Neuron 25 0 7 13 20 26 33
D-AGRESNET,0.8411405295315683,"0
161
33
90
9
199
99
45
186
129
114
108
y = G(x) 199 48 120 184 163 57 157 131 27 116 78 134 y"
D-AGRESNET,0.8431771894093686,Neuron 50 0 13 26 39 52 65
D-AGRESNET,0.845213849287169,"1
35
60
93
91
127
25
122
55
31
177
116
y = G(x) 133 110 37 161 154 112 84 41 188 78 145 40 y"
D-AGRESNET,0.8472505091649695,Neuron 75 0 6 13 20 27 34
D-AGRESNET,0.8492871690427699,"0.1
0.3
0.4
0.6
0.8
KL divergence 0 5 11 17 23 Count"
D-AGRESNET,0.8513238289205702,van-Rossum distance KL
D-AGRESNET,0.8533604887983707,"Figure H.3: Spike statistics of (top) ﬁring rate of 3 randomly selected neurons, (middle) pairwise
correlation of 3 randomly selected segments and (bottom) van Rossum distance of 3 randomly se-
lected segments between Y and G(X) where Y was ordered by autoencoder reconstruction loss.
The right columns show the KL divergence of each metrics and Table G.2 shows the mean and
standard deviation of the KL divergence comparisons."
D-AGRESNET,0.8553971486761711,Under review as a conference paper at ICLR 2022
D-AGRESNET,0.8574338085539714,"-0.5
-0.1
0.3
0.6
1.0
Pair-wise correlation 0 892 1784 2676 3569 Count"
D-AGRESNET,0.8594704684317719,Trial 0
D-AGRESNET,0.8615071283095723,"y
y = G(F(y))"
D-AGRESNET,0.8635437881873728,"-0.4
-0.1
0.3
0.6
1.0
Pair-wise correlation 0 1017 2034 3051 4069 Count"
D-AGRESNET,0.8655804480651731,Trial 5
D-AGRESNET,0.8676171079429735,"y
y = G(F(y))"
D-AGRESNET,0.869653767820774,"-0.4
-0.1
0.3
0.6
1.0
Pair-wise correlation 0 957 1914 2871 3828 Count"
D-AGRESNET,0.8716904276985743,Trial 10
D-AGRESNET,0.8737270875763747,"y
y = G(F(y))"
D-AGRESNET,0.8757637474541752,"0.0
0.0
0.1
0.1
0.1
KL divergence 0 10 21 31 42 Count"
D-AGRESNET,0.8778004073319755,Correlation KL
D-AGRESNET,0.879837067209776,"0.0
0.2
0.3
0.5
0.7
Hz 0 6 12 18 25 Count"
D-AGRESNET,0.8818737270875764,Neuron 25
D-AGRESNET,0.8839103869653768,"y
y = G(F(y))"
D-AGRESNET,0.8859470468431772,"0.0
0.5
1.0
1.5
2.1
Hz 0 22 45 67 90 Count"
D-AGRESNET,0.8879837067209776,Neuron 50
D-AGRESNET,0.890020366598778,"y
y = G(F(y))"
D-AGRESNET,0.8920570264765784,"0.0
0.2
0.4
0.6
0.8
Hz 0 21 42 63 84 Count"
D-AGRESNET,0.8940936863543788,Neuron 75
D-AGRESNET,0.8961303462321792,"y
y = G(F(y))"
D-AGRESNET,0.8981670061099797,"0.0
1.6
3.3
4.9
6.5
KL divergence 0 3 6 9 13 Count"
D-AGRESNET,0.90020366598778,Firing Rate
D-AGRESNET,0.9022403258655805,"98
90
115
59
133
99
18
120
157
55
191
85
y = G(F(y)) 98 185 115 59 133 99 18 120 157 55 191 85 y"
D-AGRESNET,0.9042769857433809,Neuron 25 1 7 14 21 28 35
D-AGRESNET,0.9063136456211812,"2
56
95
128
22
57
157
131
27
116
78
134
y = G(F(y)) 199 48 120 184 163 57 157 131 27 116 78 134 y"
D-AGRESNET,0.9083503054989817,Neuron 50 0 14 28 42 56 70
D-AGRESNET,0.9103869653767821,"3
101
66
125
194
112
84
131
188
78
145
40
y = G(F(y)) 133 110 37 161 154 112 84 41 188 78 145 40 y"
D-AGRESNET,0.9124236252545825,Neuron 75 0 7 15 22 30 37
D-AGRESNET,0.9144602851323829,"0.0
0.1
0.2
0.3
0.4
KL divergence 0 5 11 16 22 Count"
D-AGRESNET,0.9164969450101833,van-Rossum distance KL
D-AGRESNET,0.9185336048879837,"Figure H.4: Spike statistics of (top) ﬁring rate of 3 randomly selected neurons, (middle) pairwise
correlation of 3 randomly selected segments and (bottom) van Rossum distance of 3 randomly se-
lected segments between Y and G(F(Y )) where Y was ordered by autoencoder reconstruction loss.
The right columns show the KL divergence of each metrics and Table G.2 shows the mean and
standard deviation of the KL divergence comparisons."
D-AGRESNET,0.9205702647657841,Under review as a conference paper at ICLR 2022
D-AGRESNET,0.9226069246435845,"I
MOUSE 2 RECORDED DATA"
D-AGRESNET,0.924643584521385,"ORDER
|X −F(G(X))|
|X −F(X)|
|Y −G(F(Y ))|
|Y −G(Y )|"
D-AGRESNET,0.9266802443991853,"NONE
0.5875 ± 0.1050
0.1292 ± 0.0168
0.4416 ± 0.0763
0.0923 ± 0.0064
FIRING RATE
0.5794 ± 0.1055
0.1276 ± 0.0152
0.4396 ± 0.0793
0.0894 ± 0.0048
AUTOENCODER
0.5692 ± 0.1008
0.1030 ± 0.0099
0.4378 ± 0.0769
0.0101 ± 0.0018"
D-AGRESNET,0.9287169042769857,"Table I.1: Cycle-consistent and identity loss of AGResNet on Mouse 2 recordings, where neurons
were ordered by 1) original annotation, 2) ﬁring rate and 3) autoencoder reconstruction loss. For
reference, | X −Y | = 0.6057 ± 0.1146 in the test set. The lowest loss in each category marked in
bold."
D-AGRESNET,0.9307535641547862,"KL(X, F(Y ))
KL(X, F(G(X)))
KL(Y, G(X))
KL(Y, G(F(Y )))"
D-AGRESNET,0.9327902240325866,(A) PAIRWISE CORRELATION
D-AGRESNET,0.9348268839103869,"IDENTITY
0.6528 ± 0.4980
0
0.4583 ± 0.4366
0
N/A
0.5523 ± 0.4251
0.1617 ± 0.0715
0.1212 ± 0.0833
0.0499 ± 0.0266
FIRING RATE
0.5639 ± 0.4679
0.1951 ± 0.1031
0.1126 ± 0.0831
0.0399 ± 0.0231
AUTOENCODER
0.5209 ± 0.5554
0.0582 ± 0.0361
0.1231 ± 0.0988
0.0352 ± 0.0228"
D-AGRESNET,0.9368635437881874,(B) FIRING RATE
D-AGRESNET,0.9389002036659878,"IDENTITY
8.3096 ± 6.1580
0
5.5783 ± 5.8451
0
N/A
1.2881 ± 1.1147
2.5786 ± 2.7222
1.5782 ± 1.2217
1.6722 ± 1.3286
FIRING RATE
1.2181 ± 0.9909
2.4912 ± 2.5037
1.3656 ± 1.1475
1.1767 ± 1.0625
AUTOENCODER
0.8087 ± 0.5764
1.1326 ± 1.3149
1.2521 ± 0.9649
1.0592 ± 1.0722"
D-AGRESNET,0.9409368635437881,(C) PAIRWISE VAN ROSSUM DISTANCE
D-AGRESNET,0.9429735234215886,"IDENTITY
1.3894 ± 2.0529
0
1.1240 ± 1.5159
0
N/A
1.3392 ± 1.6653
0.5782 ± 0.9743
0.6043 ± 0.5250
0.2497 ± 0.2443
FIRING RATE
1.2464 ± 1.7505
0.5946 ± 0.9352
0.5638 ± 0.4181
0.1977 ± 0.1234
AUTOENCODER
0.6946 ± 0.5687
0.1996 ± 0.3232
0.5287 ± 0.3897
0.1775 ± 0.0959"
D-AGRESNET,0.945010183299389,"Table I.2: The average KL divergence between generated and recorded distributions of Mouse 2
in (a) pairwise correlation, (b) ﬁring rate and (c) population pairwise van Rossum distance. We
compare AGResNet results with different neuron ordering including 1) original annotation, 2) ﬁring
rate and 3) autoencoder reconstruction loss. Note that we added the identity model (ﬁrst row of each
sub-table) as a baseline where we should obtain perfect cycle reconstruction. Entries with the lowest
value are marked in bold."
D-AGRESNET,0.9470468431771895,Under review as a conference paper at ICLR 2022
D-AGRESNET,0.9490835030549898,"J
MOUSE 3 RECORDED DATA"
D-AGRESNET,0.9511201629327902,"ORDER
|X −F(G(X))|
|X −F(X)|
|Y −G(F(Y ))|
|Y −G(Y )|"
D-AGRESNET,0.9531568228105907,"NONE
0.2684 ± 0.0290
0.0656 ± 0.0037
0.3229 ± 0.0476
0.0796 ± 0.0047
FIRING RATE
0.2679 ± 0.0309
0.0585 ± 0.0034
0.3192 ± 0.0477
0.0777 ± 0.0043
AUTOENCODER
0.2677 ± 0.0282
0.0554 ± 0.0023
0.3199 ± 0.0487
0.0672 ± 0.0034"
D-AGRESNET,0.955193482688391,"Table J.1: Cycle-consistent and identity loss of AGResNet on Mouse 3 recordings, where neurons
were ordered by 1) original annotation, 2) ﬁring rate and 3) autoencoder reconstruction loss. For
reference, | X −Y | = 0.4764 ± 0.1520 in the test set. The lowest loss in each category marked in
bold."
D-AGRESNET,0.9572301425661914,"KL(X, F(Y ))
KL(X, F(G(X)))
KL(Y, G(X))
KL(Y, G(F(Y )))"
D-AGRESNET,0.9592668024439919,(A) PAIRWISE CORRELATION
D-AGRESNET,0.9613034623217923,"IDENTITY
1.0188 ± 0.5731
0
0.7363 ± 0.3732
0
N/A
0.5361 ± 0.2817
0.5678 ± 0.3145
0.6975 ± 0.2202
0.7381 ± 0.2977
FIRING RATE
0.5021 ± 0.2596
0.5184 ± 0.2536
0.6281 ± 0.2830
0.6616 ± 0.2850
AUTOENCODER
0.5140 ± 0.2538
0.4751 ± 0.2421
0.6137 ± 0.2997
0.4625 ± 0.2443"
D-AGRESNET,0.9633401221995926,(B) FIRING RATE
D-AGRESNET,0.9653767820773931,"IDENTITY
12.2077 ± 7.3556
0
12.4075 ± 7.3156
0
N/A
1.0164 ± 0.7129
1.8203 ± 1.9280
1.2904 ± 0.9448
1.4786 ± 1.4374
FIRING RATE
0.9371 ± 0.6735
1.7893 ± 2.5419
1.0712 ± 0.7793
1.2805 ± 1.5136
AUTOENCODER
0.8936 ± 0.5655
1.1152 ± 0.6797
1.2114 ± 0.7281
0.6928 ± 0.4643"
D-AGRESNET,0.9674134419551935,(C) PAIRWISE VAN ROSSUM DISTANCE
D-AGRESNET,0.9694501018329938,"IDENTITY
4.2704 ± 2.0834
0
4.9623 ± 1.4393
0
N/A
3.0412 ± 1.8467
2.0246 ± 1.3422
4.6059 ± 2.0664
3.0293 ± 1.5854
FIRING RATE
2.9009 ± 1.7587
1.6458 ± 1.2375
4.1910 ± 1.7950
2.8613 ± 1.7788
AUTOENCODER
2.8383 ± 1.5942
1.4747 ± 1.1150
3.9709 ± 1.7732
1.4767 ± 1.0195"
D-AGRESNET,0.9714867617107943,"Table J.2: The average KL divergence between generated and recorded distributions of Mouse 3
in (a) pairwise correlation, (b) ﬁring rate and (c) population pairwise van Rossum distance. We
compare AGResNet results with different neuron ordering including 1) original annotation, 2) ﬁring
rate and 3) autoencoder reconstruction loss. Note that we added the identity model (ﬁrst row of each
sub-table) as a baseline comparison and should obtain perfect cycle reconstruction. Entries with the
lowest value are marked in bold."
D-AGRESNET,0.9735234215885947,Under review as a conference paper at ICLR 2022
D-AGRESNET,0.9755600814663951,"K
MOUSE 4 RECORDED DATA"
D-AGRESNET,0.9775967413441955,"ORDER
|X −F(G(X))|
|X −F(X)|
|Y −G(F(Y ))|
|Y −G(Y )|"
D-AGRESNET,0.9796334012219959,"NONE
0.2538 ± 0.0399
0.0443 ± 0.0015
0.2403 ± 0.0395
0.0808 ± 0.0061
FIRING RATE
0.2511 ± 0.0389
0.0376 ± 0.0015
0.2388 ± 0.0406
0.0764 ± 0.0067
AUTOENCODER
0.2489 ± 0.0381
0.0382 ± 0.0012
0.2367 ± 0.0396
0.0764 ± 0.0053"
D-AGRESNET,0.9816700610997964,"Table K.1: Cycle-consistent and identity loss of AGResNet on Mouse 4 recordings, where neurons
were ordered by 1) original annotation, 2) ﬁring rate and 3) autoencoder reconstruction loss. For
reference, | X −Y | = 0.4383 ± 0.2354 in the test set. The lowest loss in each category marked in
bold."
D-AGRESNET,0.9837067209775967,"KL(X, F(Y ))
KL(X, F(G(X)))
KL(Y, G(X))
KL(Y, G(F(Y )))"
D-AGRESNET,0.9857433808553971,(A) PAIRWISE CORRELATION
D-AGRESNET,0.9877800407331976,"IDENTITY
0.3724 ± 0.2169
0
0.5124 ± 0.3238
0
N/A
0.2849 ± 0.1552
0.1735 ± 0.0918
0.3536 ± 0.2541
0.5750 ± 0.2883
FIRING RATE
0.2482 ± 0.1502
0.1478 ± 0.0848
0.3482 ± 0.2561
0.5471 ± 0.2577
AUTOENCODER
0.2096 ± 0.1155
0.1587 ± 0.0867
0.3460 ± 0.2568
0.4795 ± 0.2457"
D-AGRESNET,0.9898167006109979,(B) FIRING RATE
D-AGRESNET,0.9918533604887984,"IDENTITY
5.8031 ± 4.8030
0
5.1383 ± 5.4684
0
N/A
1.3062 ± 1.0097
0.6034 ± 0.6294
1.4253 ± 1.5599
2.9196 ± 3.1077
FIRING RATE
1.0818 ± 0.9274
0.5480 ± 0.5043
1.2120 ± 1.2971
2.8206 ± 2.7266
AUTOENCODER
1.0564 ± 1.1415
0.5474 ± 0.5223
1.1570 ± 1.0830
2.1015 ± 2.2399"
D-AGRESNET,0.9938900203665988,(C) PAIRWISE VAN ROSSUM DISTANCE
D-AGRESNET,0.9959266802443992,"IDENTITY
2.2670 ± 1.2707
0
2.8134 ± 1.5536
0
N/A
1.8698 ± 1.1525
0.5625 ± 0.4399
2.4011 ± 1.4879
3.3849 ± 1.7608
FIRING RATE
1.5416 ± 0.9327
0.3931 ± 0.2821
2.1379 ± 1.4338
3.3865 ± 1.9320
AUTOENCODER
1.3246 ± 0.8537
0.4578 ± 0.3639
2.2134 ± 1.3838
2.6537 ± 1.6526"
D-AGRESNET,0.9979633401221996,"Table K.2: The average KL divergence between generated and recorded distributions of Mouse
4 in (a) pairwise correlation, (b) ﬁring rate and (c) population pairwise van Rossum distance. We
compare AGResNet results with different neuron ordering including 1) original annotation, 2) ﬁring
rate and 3) autoencoder reconstruction loss. Note that we added the identity model (ﬁrst row of each
sub-table) as a baseline comparison and should obtain perfect cycle reconstruction. Entries with the
lowest value are marked in bold."
