Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0010224948875255625,"In this work, we propose Retentive Network (RETNET) as a foundation architecture
1"
ABSTRACT,0.002044989775051125,"for large language models, simultaneously achieving training parallelism, low-cost
2"
ABSTRACT,0.003067484662576687,"inference, and good performance. We theoretically derive the connection between
3"
ABSTRACT,0.00408997955010225,"recurrence and attention. Then we propose the retention mechanism for sequence
4"
ABSTRACT,0.005112474437627812,"modeling, which supports three computation paradigms, i.e., parallel, recurrent,
5"
ABSTRACT,0.006134969325153374,"and chunkwise recurrent. Specifically, the parallel representation allows for training
6"
ABSTRACT,0.007157464212678937,"parallelism. The recurrent representation enables low-cost O(1) inference, which
7"
ABSTRACT,0.0081799591002045,"improves decoding throughput, latency, and GPU memory without sacrificing
8"
ABSTRACT,0.009202453987730062,"performance. The chunkwise recurrent representation facilitates efficient long-
9"
ABSTRACT,0.010224948875255624,"sequence modeling with linear complexity, where each chunk is encoded parallelly
10"
ABSTRACT,0.011247443762781187,"while recurrently summarizing the chunks. Experimental results on language
11"
ABSTRACT,0.012269938650306749,"modeling show that RETNET achieves favorable scaling results, parallel training,
12"
ABSTRACT,0.013292433537832311,"low-cost deployment, and efficient inference.
13"
INTRODUCTION,0.014314928425357873,"1
Introduction
14"
INTRODUCTION,0.015337423312883436,"Transformer [51] has become the de facto architecture for large language models, which was initially
15"
INTRODUCTION,0.016359918200409,"proposed to overcome the sequential training issue of recurrent models [25]. However, training
16"
INTRODUCTION,0.017382413087934562,"parallelism of Transformers is at the cost of inefficient inference, because of the O(N) complexity per
17"
INTRODUCTION,0.018404907975460124,"step and memory-bound key-value cache [42], which renders Transformers unfriendly to deployment.
18"
INTRODUCTION,0.019427402862985686,"The growing sequence length increases GPU memory consumption as well as latency and reduces
19"
INTRODUCTION,0.02044989775051125,"inference speed. Numerous efforts have continued to develop the next-generation architecture, aiming
20"
INTRODUCTION,0.02147239263803681,"at retaining training parallelism and competitive performance as Transformers while having efficient
21"
INTRODUCTION,0.022494887525562373,"O(1) inference. It is challenging to achieve the above goals simultaneously.
22"
INTRODUCTION,0.023517382413087935,"There have been three main strands of research. First, linearized attention [27, 37] approximates
23"
INTRODUCTION,0.024539877300613498,"standard attention scores exp(q ¬∑ k) with kernels œï(q) ¬∑ œï(k), so that autoregressive inference can
24"
INTRODUCTION,0.02556237218813906,"be rewritten in a recurrent form. However, the modeling capability and performance are worse than
25"
INTRODUCTION,0.026584867075664622,"Transformers, which hinders the method‚Äôs popularity. The second strand returns to recurrent models
26"
INTRODUCTION,0.027607361963190184,"for efficient inference while sacrificing training parallelism. As a remedy, element-wise operators [36]
27"
INTRODUCTION,0.028629856850715747,"are used for acceleration, however, representation capacity and performance are harmed. The third
28"
INTRODUCTION,0.02965235173824131,"line explores replacing attention with other mechanisms, such as S4 [20], and its variants [11, 38].
29"
INTRODUCTION,0.03067484662576687,"None of the previous work can achieve strong performance and efficient inference at the same time
30"
INTRODUCTION,0.03169734151329243,"compared to Transformers.
31"
INTRODUCTION,0.032719836400818,"In this work, we propose retentive networks (RetNet), achieving low-cost inference, efficient long-
32"
INTRODUCTION,0.03374233128834356,"sequence modeling, Transformer-comparable performance, and parallel model training simultane-
33"
INTRODUCTION,0.034764826175869123,"ously. Specifically, we introduce a multi-scale retention mechanism to substitute multi-head attention,
34"
INTRODUCTION,0.03578732106339468,"which has three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent repre-
35"
INTRODUCTION,0.03680981595092025,"sentations. First, the parallel representation empowers training parallelism to utilize GPU devices
36"
INTRODUCTION,0.03783231083844581,"fully. Second, the recurrent representation enables efficient O(1) inference in terms of memory
37"
INTRODUCTION,0.03885480572597137,"and computation. The deployment cost and latency can be significantly reduced. Moreover, the
38"
INTRODUCTION,0.03987730061349693,"implementation is greatly simplified without key-value cache tricks. Third, the chunkwise recurrent
39"
INTRODUCTION,0.0408997955010225,"representation can perform efficient long-sequence modeling. We parallelly encode each local block
40"
INTRODUCTION,0.041922290388548056,"for computation speed while recurrently encoding the global blocks to save GPU memory.
41"
INTRODUCTION,0.04294478527607362,"We compare RetNet with Transformer and its variants. Experimental results on language modeling
42"
INTRODUCTION,0.04396728016359918,"show that RetNet is consistently competitive in terms of both scaling curves and in-context learning.
43"
INTRODUCTION,0.044989775051124746,"Moreover, the inference cost of RetNet is length-invariant. For a 7B model and 8k sequence
44"
INTRODUCTION,0.046012269938650305,"length, RetNet decodes 8.4√ó faster and saves 70% of memory than Transformers with key-value
45"
INTRODUCTION,0.04703476482617587,"caches. During training, RetNet also achieves 3√ó acceleration than standard Transformer with
46"
INTRODUCTION,0.04805725971370143,"highly-optimized FlashAttention-2 [10]. Besides, RetNet‚Äôs inference latency is insensitive to batch
47"
INTRODUCTION,0.049079754601226995,"size, allowing enormous throughput. The intriguing properties make RetNet a potential candidate to
48"
INTRODUCTION,0.050102249488752554,"replace Transformer for large language models.
49"
RETENTIVE NETWORK,0.05112474437627812,"2
Retentive Network
50"
RETENTIVE NETWORK,0.05214723926380368,"Retentive network (RetNet) is stacked with L identical blocks, which follows a similar layout (i.e.,
51"
RETENTIVE NETWORK,0.053169734151329244,"residual connection, and pre-LayerNorm) as in Transformer [51]. Each RetNet block contains two
52"
RETENTIVE NETWORK,0.0541922290388548,"modules: a multi-scale retention (MSR) module, and a feed-forward network (FFN) module. We
53"
RETENTIVE NETWORK,0.05521472392638037,"introduce the MSR module in the following sections. Given an input sequence x = x1 ¬∑ ¬∑ ¬∑ x|x|,
54"
RETENTIVE NETWORK,0.05623721881390593,"RetNet encodes the sequence in an autoregressive way. The input vectors {xi}|x|
i=1 is first packed
55"
RETENTIVE NETWORK,0.05725971370143149,"into X0 = [x1, ¬∑ ¬∑ ¬∑ , x|x|] ‚ààR|x|√ódmodel, where dmodel is hidden dimension. Then we compute
56"
RETENTIVE NETWORK,0.05828220858895705,"contextualized vector representations Xl = RetNetl(Xl‚àí1), l ‚àà[1, L].
57"
RETENTION,0.05930470347648262,"2.1
Retention
58"
RETENTION,0.06032719836400818,"In this section, we introduce the retention mechanism that has a dual form of recurrence and
59"
RETENTION,0.06134969325153374,"parallelism. So we can train the models in a parallel way while recurrently conducting inference.
60"
RETENTION,0.06237218813905931,"Consider a sequence modeling problem that maps v(n) 7‚Üío(n) through states sn. Let vn, on denote
61"
RETENTION,0.06339468302658487,"v(n), o(n) for simplicity. We formulate the mapping in a recurrent manner:
62"
RETENTION,0.06441717791411043,"sn = Asn‚àí1 + K‚ä∫
nvn,
A ‚ààRd√ód,
Kn ‚ààR1√ód"
RETENTION,0.065439672801636,"on = Qnsn = n
X"
RETENTION,0.06646216768916155,"m=1
QnAn‚àímK‚ä∫
mvm,
Qn ‚ààR1√ód
(1)"
RETENTION,0.06748466257668712,"where we map vn to the state vector sn, and then implement a linear transform to encode sequence
63"
RETENTION,0.06850715746421268,"information recurrently. Next, we make the projection Qn, Kn content-aware:
64"
RETENTION,0.06952965235173825,"Q = XWQ,
K = XWK
(2)"
RETENTION,0.0705521472392638,"where WQ, WK ‚ààRd√ód are learnable matrices.
65"
RETENTION,0.07157464212678936,"We diagonalize the matrix A = Œõ(Œ≥eiŒ∏)Œõ‚àí1, where Œ≥, Œ∏ ‚ààRd.
Then we obtain An‚àím =
66"
RETENTION,0.07259713701431493,"Œõ(Œ≥eiŒ∏)n‚àímŒõ‚àí1. By absorbing Œõ into WQ and WK, we can rewrite Equation (1) as:
67 on = n
X"
RETENTION,0.0736196319018405,"m=1
Qn(Œ≥eiŒ∏)n‚àímK‚ä∫
mvm = n
X"
RETENTION,0.07464212678936605,"m=1
(Qn(Œ≥eiŒ∏)n)(Km(Œ≥eiŒ∏)‚àím)‚ä∫vm (3)"
RETENTION,0.07566462167689161,"where Qn(Œ≥eiŒ∏)n, Km(Œ≥eiŒ∏)‚àím is known as xPos [45], i.e., a relative position embedding proposed
68"
RETENTION,0.07668711656441718,"for Transformer. We further simplify Œ≥ as a scalar, Equation (3) becomes:
69 on = n
X"
RETENTION,0.07770961145194274,"m=1
Œ≥n‚àím(QneinŒ∏)(KmeimŒ∏)‚Ä†vm
(4)"
RETENTION,0.0787321063394683,"where ‚Ä† is the conjugate transpose. The formulation is easily parallelizable within training instances.
70"
RETENTION,0.07975460122699386,"In summary, we start with recurrent modeling as shown in Equation (1), and then derive its parallel
71"
RETENTION,0.08077709611451943,"formulation in Equation (4). We consider the original mapping v(n) 7‚Üío(n) as vectors and obtain
72"
RETENTION,0.081799591002045,"the retention mechanism as follows.
73 ùëã ùêæ
ùëÑ
ùëâ ùëÇ GN"
RETENTION,0.08282208588957055,ùëÑùêæ‚ä∫‚äôùê∑ùëâ
RETENTION,0.08384458077709611,(a) Parallel representation. ùëãùëõ
RETENTION,0.08486707566462168,"ùëÜùëõ‚àí1
ùëÜùëõ
ùõæ"
RETENTION,0.08588957055214724,"ùêæùëõ
ùëâùëõ
ùëÑùëõ ùëÇùëõ"
RETENTION,0.0869120654396728,Recurrent State Input
RETENTION,0.08793456032719836,Output GN
RETENTION,0.08895705521472393,(b) Recurrent representation.
RETENTION,0.08997955010224949,"Figure 1: RetNet has three equivalent computation paradigms, i.e., parallel, recurrent, and chunkwise
recurrent representations. Given the same input, three paradigms obtain the same output. ‚ÄúGN‚Äù is
short for GroupNorm."
RETENTION,0.09100204498977506,"The Parallel Representation of Retention As shown in Figure 1a, the retention layer is defined as:
74"
RETENTION,0.09202453987730061,"Q = (XWQ) ‚äôŒò,
K = (XWK) ‚äôŒò,
V = XWV"
RETENTION,0.09304703476482618,"Œòn = einŒ∏,
Dnm =
Œ≥n‚àím,
n ‚â•m
0,
n < m
Retention(X) = (QK‚ä∫‚äôD)V (5)"
RETENTION,0.09406952965235174,"where D ‚ààR|x|√ó|x| combines causal masking and exponential decay along relative distance as one
75"
RETENTION,0.0950920245398773,"matrix, and Œò is the complex conjugate of Œò. In practice, we map Q, K ‚ààRd ‚ÜíCd/2, add the
76"
RETENTION,0.09611451942740286,"complex position embedding Œò, then map them back to Rd, following the implementation trick as in
77"
RETENTION,0.09713701431492842,"LLaMA [48, 44]. Similar to self-attention, the parallel representation enables us to train the models
78"
RETENTION,0.09815950920245399,"with GPUs efficiently.
79"
RETENTION,0.09918200408997956,"The Recurrent Representation of Retention As shown in Figure 1b, the proposed mechanism can
80"
RETENTION,0.10020449897750511,"also be written as recurrent neural networks (RNNs), which is favorable for inference. For the n-th
81"
RETENTION,0.10122699386503067,"timestep, we recurrently obtain the output as:
82"
RETENTION,0.10224948875255624,"Sn = Œ≥Sn‚àí1 + K‚ä∫
nVn
Retention(Xn) = QnSn,
n = 1, ¬∑ ¬∑ ¬∑ , |x|
(6)"
RETENTION,0.1032719836400818,"where Q, K, V, Œ≥ are the same as in Equation (5).
83"
RETENTION,0.10429447852760736,"The Chunkwise Recurrent Representation of Retention
A hybrid form of parallel representation
84"
RETENTION,0.10531697341513292,"and recurrent representation is available to accelerate training, especially for long sequences. We
85"
RETENTION,0.10633946830265849,"divide the input sequences into chunks. Within each chunk, we follow the parallel representation
86"
RETENTION,0.10736196319018405,"(Equation (5)) to conduct computation. In contrast, cross-chunk information is passed following the
87"
RETENTION,0.1083844580777096,"recurrent representation (Equation (6)). Specifically, let B denote the chunk length. We compute the
88"
RETENTION,0.10940695296523517,"retention output of the i-th chunk via:
89"
RETENTION,0.11042944785276074,"Q[i] = QBi:B(i+1),
K[i] = KBi:B(i+1),
V[i] = VBi:B(i+1)
Ri = K‚ä∫
[i](V[i] ‚äôŒ∂) + Œ≥BRi‚àí1,
Œ∂ij = Œ≥B‚àíi‚àí1"
RETENTION,0.1114519427402863,"Retention(X[i]) = (Q[i]K‚ä∫
[i] ‚äôD)V[i]
|
{z
}
Inner-Chunk"
RETENTION,0.11247443762781185,"+ (Q[i]Ri‚àí1) ‚äôŒæ
|
{z
}
Cross-Chunk"
RETENTION,0.11349693251533742,",
Œæij = Œ≥i+1
(7)"
RETENTION,0.11451942740286299,"where [i] indicates the i-th chunk, i.e., x[i] = [x(i‚àí1)B+1, ¬∑ ¬∑ ¬∑ , xiB]. The proof of the equivalence
90"
RETENTION,0.11554192229038855,"between recurrent representation and chunkwise recurrent representation is described in Appendix B.
91"
GATED MULTI-SCALE RETENTION,0.1165644171779141,"2.2
Gated Multi-Scale Retention
92"
GATED MULTI-SCALE RETENTION,0.11758691206543967,"We use h = dmodel/d retention heads in each layer, where d is the head dimension. The heads use
93"
GATED MULTI-SCALE RETENTION,0.11860940695296524,"different parameter matrices WQ, WK, WV ‚ààRd√ód. Moreover, multi-scale retention (MSR) assigns
94"
GATED MULTI-SCALE RETENTION,0.1196319018404908,def ParallelRetention(
GATED MULTI-SCALE RETENTION,0.12065439672801637,"q, k, v, # bsz ‚àónum_head ‚àólen ‚àóqkv_dim
decay_mask): # num_head ‚àólen ‚àólen
retention = q @ k.transpose(‚àí1, ‚àí2)
retention = retention ‚àódecay_mask
output = retention @ v
output = group_norm(output)
return output"
GATED MULTI-SCALE RETENTION,0.12167689161554192,def RecurrentRetention(
GATED MULTI-SCALE RETENTION,0.12269938650306748,"q, k, v, # bsz ‚àónum_head ‚àóqkv_dim
past_kv, # bsz ‚àónum_head ‚àóqk_dim ‚àóv_dim
decay): # num_head ‚àó1 ‚àó1
current_kv = decay ‚àópast_kv + k.unsqueeze(‚àí1) ‚àóv."
GATED MULTI-SCALE RETENTION,0.12372188139059305,"unsqueeze(‚àí2)
output = torch.sum(q.unsqueeze(‚àí1) ‚àócurrent_kv,"
GATED MULTI-SCALE RETENTION,0.12474437627811862,"dim=‚àí2)
output = group_norm(output)
return output, current_kv"
GATED MULTI-SCALE RETENTION,0.12576687116564417,def ChunkwiseRetention(
GATED MULTI-SCALE RETENTION,0.12678936605316973,"q, k, v, # bsz ‚àónum_head ‚àóchunk_size ‚àó"
GATED MULTI-SCALE RETENTION,0.1278118609406953,"qkv_dim
past_kv, # bsz ‚àónum_head ‚àóqk_dim ‚àó"
GATED MULTI-SCALE RETENTION,0.12883435582822086,"v_dim
decay_mask, # num_head ‚àóchunk_size ‚àó"
GATED MULTI-SCALE RETENTION,0.12985685071574643,"chunk_size
chunk_decay, # num_head ‚àó1 ‚àó1
inner_decay): # num_head ‚àóchunk_size
retention = q @ k.transpose(‚àí1, ‚àí2)
retention = retention ‚àódecay_mask
inner_retention = retention @ v
cross_retention = (q @ past_kv) ‚àó"
GATED MULTI-SCALE RETENTION,0.130879345603272,"inner_decay
retention = inner_retention +"
GATED MULTI-SCALE RETENTION,0.13190184049079753,"cross_retention
output = group_norm(retention)
current_kv = chunk_decay ‚àópast_kv + k."
GATED MULTI-SCALE RETENTION,0.1329243353783231,"transpose(‚àí1, ‚àí2) @ v
return output, current_kv"
GATED MULTI-SCALE RETENTION,0.13394683026584867,"Figure 2: Pseudocode for the three computation paradigms of retention. Parallel implementation
enables training parallelism to fully utilize GPUs. Recurrent paradigm enables low-cost inference.
Chunkwise retention combines the above advantages (i.e., parallel within each chunk and recurrent
across chunks), which has linear memory complexity for long sequences."
GATED MULTI-SCALE RETENTION,0.13496932515337423,"different Œ≥ for each head. For simplicity, we set Œ≥ identical among different layers and keep them
95"
GATED MULTI-SCALE RETENTION,0.1359918200408998,"fixed. In addition, we add a swish gate [23, 40] to increase the non-linearity of retention layers.
96"
GATED MULTI-SCALE RETENTION,0.13701431492842536,"Formally, given input X, we define the layer as:
97"
GATED MULTI-SCALE RETENTION,0.13803680981595093,"Œ≥ = 1 ‚àí2‚àí5‚àíarange(0,h) ‚ààRh"
GATED MULTI-SCALE RETENTION,0.1390593047034765,"headi = Retention(X, Œ≥i)
Y = GroupNormh(Concat(head1, ¬∑ ¬∑ ¬∑ , headh))
MSR(X) = (swish(XWG) ‚äôY )WO (8)"
GATED MULTI-SCALE RETENTION,0.14008179959100203,"where WG, WO ‚ààRdmodel√ódmodel are learnable parameters, and GroupNorm [53] normalizes the
98"
GATED MULTI-SCALE RETENTION,0.1411042944785276,"output of each head, following SubLN proposed in [43]. Notice that the heads use multiple Œ≥ scales,
99"
GATED MULTI-SCALE RETENTION,0.14212678936605316,"which results in different variance statistics. So we normalize the head outputs separately.
100"
GATED MULTI-SCALE RETENTION,0.14314928425357873,"The pseudocode of retention is summarized in Figure 2.
101"
GATED MULTI-SCALE RETENTION,0.1441717791411043,"Retention Score Normalization We utilize the scale-invariant nature of GroupNorm to improve the
102"
GATED MULTI-SCALE RETENTION,0.14519427402862986,"numerical precision of retention layers. Specifically, multiplying a scalar value within GroupNorm
103"
GATED MULTI-SCALE RETENTION,0.14621676891615543,"does not affect outputs and backward gradients, i.e., GroupNorm(Œ±‚àóheadi) = GroupNorm(headi).
104"
GATED MULTI-SCALE RETENTION,0.147239263803681,"We implement three normalization factors in Equation (5). First, we normalize QK‚ä∫as QK‚ä∫/
‚àö"
GATED MULTI-SCALE RETENTION,0.14826175869120656,"d.
105"
GATED MULTI-SCALE RETENTION,0.1492842535787321,"Second, we replace D with ÀúDnm = Dnm/‚àöPn
i=1 Dni. Third, let R denote the retention scores
106"
GATED MULTI-SCALE RETENTION,0.15030674846625766,"R = QK‚ä∫‚äôD, we normalize it as ÀúRnm = Rnm/max(Pn
i=1 |Rni|,1). Then the retention output
107"
GATED MULTI-SCALE RETENTION,0.15132924335378323,"becomes Retention(X) = ÀúRV . The above tricks do not affect the final results while stabilizing the
108"
GATED MULTI-SCALE RETENTION,0.1523517382413088,"numerical flow of both forward and backward passes, because of the scale-invariant property.
109"
OVERALL ARCHITECTURE OF RETENTION NETWORKS,0.15337423312883436,"2.3
Overall Architecture of Retention Networks
110"
OVERALL ARCHITECTURE OF RETENTION NETWORKS,0.15439672801635992,"For an L-layer retention network, we stack multi-scale retention (MSR) and feed-forward network
111"
OVERALL ARCHITECTURE OF RETENTION NETWORKS,0.1554192229038855,"(FFN) to build the model. Formally, the input sequence {xi}|x|
i=1 is transformed into vectors by a
112"
OVERALL ARCHITECTURE OF RETENTION NETWORKS,0.15644171779141106,"word embedding layer. We use the packed embeddings X0 = [x1, ¬∑ ¬∑ ¬∑ , x|x|] ‚ààR|x|√ódmodel as the
113"
OVERALL ARCHITECTURE OF RETENTION NETWORKS,0.1574642126789366,"input and compute the model output XL:
114"
OVERALL ARCHITECTURE OF RETENTION NETWORKS,0.15848670756646216,Y l = MSR(LN(Xl)) + Xl
OVERALL ARCHITECTURE OF RETENTION NETWORKS,0.15950920245398773,"Xl+1 = FFN(LN(Y l)) + Y l
(9)"
OVERALL ARCHITECTURE OF RETENTION NETWORKS,0.1605316973415133,"where LN(¬∑) is LayerNorm [3]. The FFN part is computed as FFN(X) = gelu(XW1)W2, where
115"
OVERALL ARCHITECTURE OF RETENTION NETWORKS,0.16155419222903886,"W1, W2 are parameter matrices.
116"
OVERALL ARCHITECTURE OF RETENTION NETWORKS,0.16257668711656442,"Training We use the parallel (Equation (5)) and chunkwise recurrent (Equation (7)) representations
117"
OVERALL ARCHITECTURE OF RETENTION NETWORKS,0.16359918200409,"during the training process. The parallelization within sequences or chunks efficiently utilizes
118"
OVERALL ARCHITECTURE OF RETENTION NETWORKS,0.16462167689161555,"GPUs to accelerate computation. More favorably, chunkwise recurrence is especially useful for
119"
OVERALL ARCHITECTURE OF RETENTION NETWORKS,0.1656441717791411,"long-sequence training, which is efficient in terms of both FLOPs and memory consumption.
120"
OVERALL ARCHITECTURE OF RETENTION NETWORKS,0.16666666666666666,"Inference The recurrent representation (Equation (6)) is employed during inference, which nicely
121"
OVERALL ARCHITECTURE OF RETENTION NETWORKS,0.16768916155419222,"fits autoregressive decoding. The O(1) complexity reduces memory and inference latency while
122"
OVERALL ARCHITECTURE OF RETENTION NETWORKS,0.1687116564417178,"achieving equivalent results.
123"
EXPERIMENTS,0.16973415132924335,"3
Experiments
124"
EXPERIMENTS,0.17075664621676892,"We perform language modeling experiments to evaluate RetNet. First, we present the scaling curves
125"
EXPERIMENTS,0.17177914110429449,"of Transformer and RetNet. Second, we follow the training settings of StableLM-4E1T [50] to
126"
EXPERIMENTS,0.17280163599182005,"compare with open-source Transformer models in downstream benchmarks. Moreover, for training
127"
EXPERIMENTS,0.1738241308793456,"and inference, we compare speed, memory consumption, and latency. The training corpus is a curated
128"
EXPERIMENTS,0.17484662576687116,"compilation of The Pile [16], C4 [14], and The Stack [29].
129"
COMPARISON WITH TRANSFORMER VARIANTS,0.17586912065439672,"3.1
Comparison with Transformer Variants
130"
COMPARISON WITH TRANSFORMER VARIANTS,0.1768916155419223,"We compare RetNet with various efficient Transformer variants, including RWKV [36], H3 [11],
131"
COMPARISON WITH TRANSFORMER VARIANTS,0.17791411042944785,"Hyena [38], and Mamba [19]. We use LLaMA [48] architecture, including RMSNorm [59] and
132"
COMPARISON WITH TRANSFORMER VARIANTS,0.17893660531697342,"SwiGLU [40, 7] module, as the Transformer backbone, which shows better performance and stability.
133"
COMPARISON WITH TRANSFORMER VARIANTS,0.17995910020449898,"Consequently, other variants follow these settings. Specifically, Mamba does not have FFN layers so
134"
COMPARISON WITH TRANSFORMER VARIANTS,0.18098159509202455,"we only implement RMSNorm. For RetNet, the FFN intermediate dimension is 5"
D AND THE VALUE,0.18200408997955012,"3d and the value
135"
D AND THE VALUE,0.18302658486707565,"dimensions in WG, WV , WO are also 5"
D AND THE VALUE,0.18404907975460122,"3d, where the overall parameters are still 12d2. All models
136"
D AND THE VALUE,0.18507157464212678,"have 400M parameters with 24 layers and a hidden dimension of 1024. For H3, we set the head
137"
D AND THE VALUE,0.18609406952965235,"dimension to 8. For RWKV, we use the TimeMix module to substitute self-attention layers while
138"
D AND THE VALUE,0.18711656441717792,"keeping FFN layers consistent with other models for fair comparisons. We train the models with 40k
139"
D AND THE VALUE,0.18813905930470348,"steps with a batch size of 0.25M tokens.
140"
D AND THE VALUE,0.18916155419222905,"Fine-Grained Language Modeling Evaluation As shown in Table 1, we first report the language
141"
D AND THE VALUE,0.1901840490797546,"modeling perplexity of validation sets. Besides the overall validation set, following [2], we divide
142"
D AND THE VALUE,0.19120654396728015,"perplexity into ‚ÄúAR-Hit‚Äù and ‚ÄúFirst Occur‚Äù. Specifically, AR-Hit contains the predicted tokens that
143"
D AND THE VALUE,0.19222903885480572,"are previously seen bigrams in the previous context, which evaluates the associative recall ability.
144"
D AND THE VALUE,0.19325153374233128,"‚ÄúFirst Occur‚Äù has the predicted tokens that can not be recalled from the context. Among various
145"
D AND THE VALUE,0.19427402862985685,"Transformer variants, RetNet outperforms previous methods on both ‚ÄúAR-Hit‚Äù and ‚ÄúFirst Occur‚Äù
146"
D AND THE VALUE,0.19529652351738241,"splits, which is important for real-world use cases.
147"
D AND THE VALUE,0.19631901840490798,"Knowledge-Intensive Tasks
We also evaluate Massive Multitask Language Understanding
148"
D AND THE VALUE,0.19734151329243355,"(MMLU; [24]) answer perplexity to evaluate models on knowledge-intensive tasks. We report
149"
D AND THE VALUE,0.1983640081799591,"the average perplexity of the correct answers, i.e., given input [Question, ‚ÄúAnswer:‚Äù, Correct
150"
D AND THE VALUE,0.19938650306748465,"Answer], we calculate the perplexity of the ‚ÄúCorrect Answer‚Äù part. RetNet achieves competitive
151"
D AND THE VALUE,0.20040899795501022,"results among the architectures.
152"
D AND THE VALUE,0.20143149284253578,"Language Modeling
MMLU
Valid. Set
AR-Hit
First-Occur
STEMs
Humanites
Social-Sci.
Others
Avg"
D AND THE VALUE,0.20245398773006135,"Transformer [51]
3.320
1.118
3.826
0.584
0.229
0.279
0.402
0.356"
D AND THE VALUE,0.2034764826175869,"Transformer Variants
Hyena [38]
3.545
1.799
3.947
1.125
0.576
0.654
0.819
0.767
RWKV [36]
3.497
1.706
3.910
1.156
0.609
0.617
0.781
0.768
Mamba [19]
3.379
1.322
3.852
0.668
0.288
0.300
0.425
0.403
H3 [11]
3.563
1.722
3.986
1.169
0.532
0.637
0.792
0.752
RetNet
3.360
1.264
3.843
0.577
0.263
0.280
0.384
0.362"
D AND THE VALUE,0.20449897750511248,"Table 1: Perplexity results on language modeling and MMLU [24] answers. We use the augmented
Transformer architecture proposed in LLaMA [48] for reference. For language modeling, we
report perplexity on both the overall validation set and fine-grained diagnosis sets [2], i.e., ‚ÄúAR-Hit‚Äù
evaluates the associative recall capability, and ‚ÄúFirst-Occur‚Äù indicates the regular language modeling
performance. Besides, we evaluate the answer perplexity of MMLU subsets."
LANGUAGE MODELING EVALUATION WITH VARIOUS MODEL SIZES,0.20552147239263804,"3.2
Language Modeling Evaluation with Various Model Sizes
153"
B,0.2065439672801636,"1.3B
2.7B
6.7B
Model Size 6 8 10 12 14 16 18"
B,0.20756646216768918,Validation PPL
B,0.2085889570552147,"Transformer
RetNet"
B,0.20961145194274028,"Figure 3: Validation perplexity (PPL) de-
creases along with scaling up the model
size."
B,0.21063394683026584,"We train language models with various sizes (i.e., 1.3B,
154"
B,0.2116564417177914,"2.7B, and 6.7B) from scratch. The training batch size
155"
B,0.21267893660531698,"is 4M tokens with 2048 maximal length. We train the
156"
B,0.21370143149284254,"models with 25k steps. The detailed hyper-parameters are
157"
B,0.2147239263803681,"described in Appendix E. We train the models with 512
158"
B,0.21574642126789367,"AMD MI200 GPUs.
159"
B,0.2167689161554192,"Figure 3 reports perplexity on the validation set for the
160"
B,0.21779141104294478,"language models based on Transformer and RetNet. We
161"
B,0.21881390593047034,"present the scaling curves with three model sizes, i.e.,
162"
B,0.2198364008179959,"1.3B, 2.7B, and 6.7B. RetNet achieves comparable results
163"
B,0.22085889570552147,"with Transformers. More importantly, the results indicate
164"
B,0.22188139059304704,"that RetNet is favorable in terms of size scaling. In addi-
165"
B,0.2229038854805726,"tion to performance, RetNet training is quite stable in our
166"
B,0.22392638036809817,"experiments. Experimental results show that RetNet is a
167"
B,0.2249488752556237,"strong competitor to Transformer for large language mod-
168"
B,0.22597137014314927,"els. Empirically, we find that RetNet starts to outperform
169"
B,0.22699386503067484,"Transformer when the model size is larger than 2B.
170"
LONG-CONTEXT EVALUATION,0.2280163599182004,"3.3
Long-Context Evaluation
171"
LONG-CONTEXT EVALUATION,0.22903885480572597,"We evaluate long-context modeling on the ZeroSCROLLS [41] benchmark. We train a hybrid model
172"
LONG-CONTEXT EVALUATION,0.23006134969325154,"of size 2.7B, RetNet+, which stacks the attention and retention layers. Specifically, we insert one
173"
LONG-CONTEXT EVALUATION,0.2310838445807771,"attention layer after every 3 retention layers. We follow most configurations of the 2.7B model as in
174"
LONG-CONTEXT EVALUATION,0.23210633946830267,"Section 3.2. We scale the number of training tokens to 420B tokens. The batch size is 4M tokens.
175"
LONG-CONTEXT EVALUATION,0.2331288343558282,"We first train the model with 4K length and then extend the sequence length to 16K for the last 50B
176"
LONG-CONTEXT EVALUATION,0.23415132924335377,"training tokens. The rotation base scaling [55] is used for length extension.
177"
LONG-CONTEXT EVALUATION,0.23517382413087934,"Figure 4 reports the answer perplexity given various lengths of input document. It shows that both
178"
LONG-CONTEXT EVALUATION,0.2361963190184049,"Transformer and RetNet+ perform better with longer input documents. The results indicate that the
179"
LONG-CONTEXT EVALUATION,0.23721881390593047,"language models successfully utilize the long-distance context. Notice that the 12K and 16K results
180"
LONG-CONTEXT EVALUATION,0.23824130879345604,"in Qasper are similar because the lengths of most documents are shorter than 16K. Moreover, RetNet+
181"
LONG-CONTEXT EVALUATION,0.2392638036809816,"obtains competitive results compared with Transformer for long-context modeling. Meanwhile,
182"
LONG-CONTEXT EVALUATION,0.24028629856850717,"retention has better training and inference efficiency.
183"
LONG-CONTEXT EVALUATION,0.24130879345603273,"4096
8192
12288 16384
Context Length 1.5 1.6 1.7"
LONG-CONTEXT EVALUATION,0.24233128834355827,Answer PPL
LONG-CONTEXT EVALUATION,0.24335378323108384,GovReport
LONG-CONTEXT EVALUATION,0.2443762781186094,"4096
8192
12288 16384
Context Length 1.7 1.8 1.9"
LONG-CONTEXT EVALUATION,0.24539877300613497,Answer PPL
LONG-CONTEXT EVALUATION,0.24642126789366053,Qasper
LONG-CONTEXT EVALUATION,0.2474437627811861,"4096
8192
12288 16384
Context Length 3.25 3.50 3.75 4.00 4.25"
LONG-CONTEXT EVALUATION,0.24846625766871167,Answer PPL
LONG-CONTEXT EVALUATION,0.24948875255623723,NarrativeQA
LONG-CONTEXT EVALUATION,0.2505112474437628,"Transformer
RetNet+"
LONG-CONTEXT EVALUATION,0.25153374233128833,"Figure 4: Answer perplexity decreases along with longer input documents. Transformer and RetNet+
obtain comparable performance for long-context modeling on the ZeroSCROLLS [41] benchmark."
INFERENCE COST,0.25255623721881393,"3.4
Inference Cost
184"
INFERENCE COST,0.25357873210633947,"As shown in Figure 5, we compare memory cost, throughput, and latency of Transformer and RetNet
185"
INFERENCE COST,0.254601226993865,"during inference. Transformers reuse KV caches of previously decoded tokens. RetNet uses the
186"
INFERENCE COST,0.2556237218813906,"recurrent representation as described in Equation (6). We evaluate the 6.7B model on the A100-80GB
187"
INFERENCE COST,0.25664621676891614,"GPU. Figure 5 shows that RetNet outperforms Transformer in terms of inference cost.
188"
INFERENCE COST,0.25766871165644173,"Memory
As shown in Figure 5a, the memory cost of Transformer increases linearly due to KV
189"
INFERENCE COST,0.25869120654396727,"caches. In contrast, the memory consumption of RetNet remains consistent even for long sequences,
190"
INFERENCE COST,0.25971370143149286,"2048
3072
4096
5120
6144
7168
8192
Sequence Length 20 30 40"
INFERENCE COST,0.2607361963190184,GPU Memory (GB)
INFERENCE COST,0.261758691206544,"Model Weights
RetNet
Transformer"
INFERENCE COST,0.26278118609406953,"(a) GPU memory cost with varying
sequence length."
INFERENCE COST,0.26380368098159507,"2048
3072
4096
5120
6144
7168
8192
Sequence Length 50 100 150 200 250 300"
INFERENCE COST,0.26482617586912066,Throughput (wps)
INFERENCE COST,0.2658486707566462,"RetNet
Transformer"
INFERENCE COST,0.2668711656441718,"(b) Inference throughput with vary-
ing sequence length."
INFERENCE COST,0.26789366053169733,"2
4
6
8
Batch Size 100 200 300"
INFERENCE COST,0.2689161554192229,Latency (ms)
INFERENCE COST,0.26993865030674846,"Transformer (1024)
Transformer (2048)
Transformer (4096)
Transformer (8192)
RetNet (8192)"
INFERENCE COST,0.27096114519427406,"(c) Inference latency with different
batch sizes."
INFERENCE COST,0.2719836400817996,"Figure 5: Inference cost of Transformer and RetNet with a model size of 6.7B. RetNet outperforms
Transformers in terms of memory consumption, throughput, and latency."
INFERENCE COST,0.27300613496932513,"requiring much less GPU memory to host RetNet. The additional memory consumption of RetNet is
191"
INFERENCE COST,0.2740286298568507,"almost negligible (i.e., about 3%) while the model weights occupy 97%.
192"
INFERENCE COST,0.27505112474437626,"Throughput
As presented in Figure 5b, the throughput of Transformer drops along with the
193"
INFERENCE COST,0.27607361963190186,"decoding length increases. In comparison, RetNet has higher and length-invariant throughput during
194"
INFERENCE COST,0.2770961145194274,"decoding, by utilizing the recurrent representation of retention.
195"
INFERENCE COST,0.278118609406953,"Latency
Latency is an important metric in deployment that greatly affects the user experience.
196"
INFERENCE COST,0.2791411042944785,"We report the decoding latency in Figure 5c. Experimental results show that increasing batch size
197"
INFERENCE COST,0.28016359918200406,"renders the Transformer‚Äôs latency larger. Moreover, the latency of Transformers grows faster with
198"
INFERENCE COST,0.28118609406952966,"longer input. In order to make latency acceptable, we have to restrict the batch size, which harms the
199"
INFERENCE COST,0.2822085889570552,"overall inference throughput of Transformers. By contrast, RetNet‚Äôs decoding latency outperforms
200"
INFERENCE COST,0.2832310838445808,"Transformers and stays almost the same across different batch sizes and input lengths.
201"
TRAINING THROUGHPUT,0.2842535787321063,"3.5
Training Throughput
202"
TRAINING THROUGHPUT,0.2852760736196319,"8192
16384
32768
65536
Sequence Length 4000 6000 8000 10000 12000 14000"
TRAINING THROUGHPUT,0.28629856850715746,Throughput (wps)
TRAINING THROUGHPUT,0.28732106339468305,"Transformer (FlashAttn-2)
RetNet (Triton)
RetNet (PyTorch)"
TRAINING THROUGHPUT,0.2883435582822086,"Figure 6:
Training throughput (word
per second; wps) of Transformer with
FlashAttention-2 [10] and RetNet."
TRAINING THROUGHPUT,0.2893660531697341,"Figure 6 compares the training throughput of Trans-
203"
TRAINING THROUGHPUT,0.2903885480572597,"former and RetNet, where the training sequence lengths
204"
TRAINING THROUGHPUT,0.29141104294478526,"range from 8192 to 65536. The model size is 3.5B,
205"
TRAINING THROUGHPUT,0.29243353783231085,"where the hidden dimension is 3072 and the layer size
206"
TRAINING THROUGHPUT,0.2934560327198364,"is 28. We use highly optimized FlashAttention-2 [10]
207"
TRAINING THROUGHPUT,0.294478527607362,"for Transformers. In comparison, we implement chunk
208"
TRAINING THROUGHPUT,0.2955010224948875,"recurrent representation (Equation (7)) using Triton [46],
209"
TRAINING THROUGHPUT,0.2965235173824131,"where the computation is both memory-friendly and
210"
TRAINING THROUGHPUT,0.29754601226993865,"computationally efficient. The chunk size is set to 256.
211"
TRAINING THROUGHPUT,0.2985685071574642,"We evaluate the results with eight Nvidia H100-80GB
212"
TRAINING THROUGHPUT,0.2995910020449898,"GPUs because FlashAttention-2 is highly optimized for
213"
TRAINING THROUGHPUT,0.3006134969325153,"H100 cards.
214"
TRAINING THROUGHPUT,0.3016359918200409,"Experimental results show that RetNet has higher train-
215"
TRAINING THROUGHPUT,0.30265848670756645,"ing throughput than Transformers. The acceleration ratio increases as the sequence length is longer.
216"
TRAINING THROUGHPUT,0.30368098159509205,"When the training length is 64k, RetNet‚Äôs throughput is about 3 times than Transformer‚Äôs.
217"
ZERO-SHOT AND FEW-SHOT EVALUATION ON DOWNSTREAM TASKS,0.3047034764826176,"3.6
Zero-Shot and Few-Shot Evaluation on Downstream Tasks
218"
ZERO-SHOT AND FEW-SHOT EVALUATION ON DOWNSTREAM TASKS,0.3057259713701431,"We also compare the language models on a wide range of downstream tasks. We evaluate zero-shot
219"
ZERO-SHOT AND FEW-SHOT EVALUATION ON DOWNSTREAM TASKS,0.3067484662576687,"and 4-shot learning with the 6.7B models. As shown in Table 2, the datasets include HellaSwag
220"
ZERO-SHOT AND FEW-SHOT EVALUATION ON DOWNSTREAM TASKS,0.30777096114519426,"(HS; [57]), BoolQ [8], COPA [52], PIQA [6], Winograd, Winogrande [30], and StoryCloze (SC; [34]).
221"
ZERO-SHOT AND FEW-SHOT EVALUATION ON DOWNSTREAM TASKS,0.30879345603271985,"The accuracy numbers are consistent with language modeling perplexity presented in Figure 3. RetNet
222"
ZERO-SHOT AND FEW-SHOT EVALUATION ON DOWNSTREAM TASKS,0.3098159509202454,"achieves comparable performance with Transformer on zero-shot and in-context learning settings.
223"
ABLATION STUDIES,0.310838445807771,"3.7
Ablation Studies
224"
ABLATION STUDIES,0.3118609406952965,"We ablate various design choices of RetNet and report the language modeling results in Table 3. The
225"
ABLATION STUDIES,0.3128834355828221,"evaluation settings and metrics are the same as in Section 3.1.
226"
ABLATION STUDIES,0.31390593047034765,"HS
BoolQ
COPA
PIQA
Winograd
Winogrande
SC
Avg"
ABLATION STUDIES,0.3149284253578732,"Zero-Shot Performance
Transformer
55.9
62.0
69.0
74.6
69.5
56.5
75.0
66.07
RetNet
60.7
62.2
77.0
75.4
77.2
58.1
76.0
69.51"
ABLATION STUDIES,0.3159509202453988,"Few-shot Performance (4-Shot)
Transformer
55.8
58.7
71.0
75.0
71.9
57.3
75.4
66.44
RetNet
60.5
60.1
78.0
76.0
77.9
59.9
75.9
69.76"
ABLATION STUDIES,0.3169734151329243,Table 2: Zero-shot and few-shot learning performance. The language model size is 6.7B.
ABLATION STUDIES,0.3179959100204499,"Architecture
We ablate the swish gate and GroupNorm as described in Equation (8). Table 3
227"
ABLATION STUDIES,0.31901840490797545,"shows that the above two components improve performance. First, the gating module is essential
228"
ABLATION STUDIES,0.32004089979550104,"for enhancing non-linearity and improving model capability. Notice that we use the same parameter
229"
ABLATION STUDIES,0.3210633946830266,"allocation as in Transformers after removing the gate. Second, group normalization in retention
230"
ABLATION STUDIES,0.3220858895705521,"balances the variances of multi-head outputs, which improves training stability and language modeling
231"
ABLATION STUDIES,0.3231083844580777,"results.
232"
ABLATION STUDIES,0.32413087934560325,"Multi-Scale Decay Equation (8) shows that we use different Œ≥ as the decay rates for the retention
233"
ABLATION STUDIES,0.32515337423312884,"heads. In the ablation studies, we examine removing Œ≥ decay (i.e., ‚Äú‚àíŒ≥ decay‚Äù) and applying the
234"
ABLATION STUDIES,0.3261758691206544,"same decay rate across heads (i.e., ‚Äú‚àímulti-scale decay‚Äù). Specifically, ablating Œ≥ decay is equivalent
235"
ABLATION STUDIES,0.32719836400818,"to Œ≥ = 1. In the second setting, we set Œ≥ = 1 ‚àí2‚àí6.5 for all heads. Table 3 indicates that both the
236"
ABLATION STUDIES,0.3282208588957055,"decay mechanism and using multiple decay rates can improve the language modeling performance.
237"
ABLATION STUDIES,0.3292433537832311,"Head Dimension As indicated by the recurrent perspective of Equation (1), the head dimension
238"
ABLATION STUDIES,0.33026584867075665,"implies the memory capacity of hidden states. In ablation, we reduce the default head dimension from
239"
ABLATION STUDIES,0.3312883435582822,"256 to 64, i.e., 64 for queries and keys, and ‚åä5"
ABLATION STUDIES,0.3323108384458078,"3 √ó64‚åã‚âà108 for values. We keep the hidden dimension
240"
ABLATION STUDIES,0.3333333333333333,"dmodel the same. Accordingly, we adjust the multi-scale decay as Œ≥ = 1 ‚àí2‚àí5‚àíarange(0,h)/4 to keep
241"
ABLATION STUDIES,0.3343558282208589,"the same decay range. Table 3 shows that the larger head dimension achieves better performance.
242"
ABLATION STUDIES,0.33537832310838445,"Language Modeling
MMLU
Valid. Set
AR-Hit
First-Occur
STEMs
Humanites
Social-Sci.
Others
Avg"
ABLATION STUDIES,0.33640081799591004,"RetNet
3.360
1.264
3.843
0.577
0.263
0.280
0.384
0.362
‚àíswish gate
3.509
1.366
4.002
0.599
0.285
0.315
0.421
0.390
‚àíGroupNorm
3.367
1.302
3.843
0.630
0.295
0.327
0.438
0.406
‚àíŒ≥ decay
3.920
2.122
4.334
0.958
0.566
0.571
0.694
0.681
‚àímulti-scale decay
3.524
1.768
3.928
0.921
0.433
0.471
0.590
0.582
Reduce head dim.
3.397
1.331
3.872
0.637
0.272
0.294
0.393
0.384"
ABLATION STUDIES,0.3374233128834356,"Table 3: Perplexity results on language modeling and MMLU [24] answers. For language modeling,
we report perplexity on both the overall validation set and fine-grained diagnosis sets [2], i.e., ‚ÄúAR-Hit‚Äù
evaluates the associative recall capability, and ‚ÄúFirst-Occur‚Äù indicates the regular language modeling
performance. Besides, we evaluate the answer perplexity of the MMLU subsets."
RESULTS ON VISION TASKS,0.33844580777096117,"3.8
Results on Vision Tasks
243"
RESULTS ON VISION TASKS,0.3394683026584867,"We also compare RetNet with vision Transformers [15, 47] in Table 4, where bidirectional en-
244"
RESULTS ON VISION TASKS,0.34049079754601225,"coders are evaluated. Unlike causal language models, the vision encoders do not require recurrent
245"
RESULTS ON VISION TASKS,0.34151329243353784,"representations. Specifically, we use retention as follows:
246"
RESULTS ON VISION TASKS,0.3425357873210634,"Q = (XWQ) ‚äôŒò,
K = (XWK) ‚äôŒò,
V = XWV
Retention(X) = (QK‚ä∫)V = Q(K‚ä∫V )"
RESULTS ON VISION TASKS,0.34355828220858897,"where multi-scale decay is removed in bidirectional computation. Notice that we can compute
247"
RESULTS ON VISION TASKS,0.3445807770961145,"retention in different orders. Similar to linear attention [27], the Q(K‚ä∫V ) paradigm is an efficient
248"
RESULTS ON VISION TASKS,0.3456032719836401,"operator in bidirectional settings, especially for high-resolution images.
249"
RESULTS ON VISION TASKS,0.34662576687116564,"We perform experiments on ImageNet-1K classification [13], COCO object detection [32], and
250"
RESULTS ON VISION TASKS,0.3476482617586912,"ADE20K semantic segmentation [60]. We compare RetNet with DeiT [47] which is a well-tuned
251"
RESULTS ON VISION TASKS,0.3486707566462168,"vision Transformer. Besides, we follow [21] and plug in a depth-wise convolution in experiments.
252"
RESULTS ON VISION TASKS,0.3496932515337423,"We adopt the DeiT-M size, which has about 38M parameters. For ImageNet-1K image classification,
253"
RESULTS ON VISION TASKS,0.3507157464212679,"ImageNet
COCO
ADE20K
Acc
APb
APb
50
APb
75
mIoU
mAcc"
RESULTS ON VISION TASKS,0.35173824130879344,"DeiT [47]
80.76
0.458
0.678
0.502
43.52
55.08
RetNet
81.57
0.457
0.669
0.488
44.13
56.12"
RESULTS ON VISION TASKS,0.35276073619631904,"Table 4: Results on vision tasks, i.e., image classification (ImageNet), object detection (COCO), and
semantic segmentation (ADE20K). RetNet achieves competitive performance with DeiT, which is a
well-tuned vision Transformer."
RESULTS ON VISION TASKS,0.3537832310838446,"we use AdamW [33] for 300 epochs, and 20 epochs of linear warm-up. The learning rate is 1 √ó 10‚àí3,
254"
RESULTS ON VISION TASKS,0.35480572597137017,"the batch size is 1024, and the weight decay is 0.05. For COCO object detection, we use Mask
255"
RESULTS ON VISION TASKS,0.3558282208588957,"R-CNN [22] as the task head, and the above models pre-trained on ImageNet as the backbone with
256"
RESULTS ON VISION TASKS,0.35685071574642124,"3x schedules. In ADE20K experiments, we use UperNet [54] as the segmentation head. The detailed
257"
RESULTS ON VISION TASKS,0.35787321063394684,"configuration can be found in Appendix H.
258"
RESULTS ON VISION TASKS,0.3588957055214724,"Table 4 shows the results across various vision tasks. RetNet is competitive compared with DeiT.
259"
RESULTS ON VISION TASKS,0.35991820040899797,"For classification and segmentation, RetNet is slightly better than DeiT, where RetNet achieves
260"
RESULTS ON VISION TASKS,0.3609406952965235,"0.81% accuracy improvement on ImageNet and 0.61% mIoU improvement on ADE20K. For object
261"
RESULTS ON VISION TASKS,0.3619631901840491,"detection, the results are comparable.
262"
RELATED WORK,0.36298568507157464,"4
Related Work
263"
RELATED WORK,0.36400817995910023,"Numerous efforts are focused on reducing the quadratic complexity of attention mechanisms. Linear
264"
RELATED WORK,0.36503067484662577,"attention [27] uses various kernels œï(qi)œï(kj)/P|x|
n=1 œï(qi)œï(kn) to replace the softmax function. In
265"
RELATED WORK,0.3660531697341513,"contrast, we reexamine sequence modeling from scratch, rather than aiming at approximating
266"
RELATED WORK,0.3670756646216769,"softmax. AFT [58] simplifies dot-product attention to element-wise and moves softmax to key
267"
RELATED WORK,0.36809815950920244,"vectors. RWKV [36] replaces AFT‚Äôs position embeddings with exponential decay and runs the
268"
RELATED WORK,0.36912065439672803,"models recurrently for training and inference. In comparison, retention preserves high-dimensional
269"
RELATED WORK,0.37014314928425357,"states to encode sequence information, which contributes to expressive ability and better performance.
270"
RELATED WORK,0.37116564417177916,"S4 [20] unifies convolution and recurrence format and achieves O(N log N) training complexity
271"
RELATED WORK,0.3721881390593047,"leveraging the FFT kernel. Unlike Equation (2), if Qn and Kn are content-unaware, the formulation
272"
RELATED WORK,0.37321063394683024,"can be degenerated to S4 [20]. Hyena [38] generates the convolution kernels, achieving sub-quadratic
273"
RELATED WORK,0.37423312883435583,"training efficiency but keeping O(N) complexity in single-step inference. Recently, most related
274"
RELATED WORK,0.37525562372188137,"work has focused on modifying Œ≥ in Equation (6) as a data-dependent variable, such as Mamba [19],
275"
RELATED WORK,0.37627811860940696,"GLA [56], Gateloop [28], and xLSTM [4]. Another strand explores hybrid architectures [31, 12] that
276"
RELATED WORK,0.3773006134969325,"interleave the above components with attention layers.
277"
RELATED WORK,0.3783231083844581,"In addition, we discuss the training and inference efficiency of some related methods. Let D denote
278"
RELATED WORK,0.37934560327198363,"the hidden dimension, H the head dimension, and N the sequence length. For training, RWKV‚Äôs
279"
RELATED WORK,0.3803680981595092,"token-mixing complexity is O(DN), and Mamba‚Äôs complexity is O(DHN) with optimized CUDA
280"
RELATED WORK,0.38139059304703476,"kernels. Hyena‚Äôs is O(DN log N) with Fast Fourier Transform acceleration. In comparison, the
281"
RELATED WORK,0.3824130879345603,"chunk-wise recurrent representation is O(DN(B + H)), where B is the chunk size, and we usually
282"
RELATED WORK,0.3834355828220859,"set H = 256, B ‚â§512. However, chunk-wise computation is highly parallelized, enabling efficient
283"
RELATED WORK,0.38445807770961143,"hardware usage. For large model size (i.e., larger D) or sequence length, the additional b + h has
284"
RELATED WORK,0.38548057259713703,"negligible effects. For inference, among the efficient architectures compared, Hyena has the same
285"
RELATED WORK,0.38650306748466257,"complexity (i.e., O(N) per step) as Transformer, while the others can perform O(1) decoding.
286"
CONCLUSION,0.38752556237218816,"5
Conclusion
287"
CONCLUSION,0.3885480572597137,"We propose retentive networks (RetNet) for sequence modeling, which enables various representa-
288"
CONCLUSION,0.3895705521472393,"tions, i.e., parallel, recurrent, and chunkwise recurrent. RetNet achieves significantly better inference
289"
CONCLUSION,0.39059304703476483,"efficiency (in terms of memory, speed, and latency), favorable training parallelization, and competitive
290"
CONCLUSION,0.39161554192229037,"performance compared with Transformers. The above advantages make RetNet an ideal successor to
291"
CONCLUSION,0.39263803680981596,"Transformers for large language models, especially considering the deployment benefits brought by
292"
CONCLUSION,0.3936605316973415,"the O(1) inference complexity. In the future, we are interested in deploying RetNet on various edge
293"
CONCLUSION,0.3946830265848671,"devices, such as mobile phones.
294"
REFERENCES,0.39570552147239263,"References
295"
REFERENCES,0.3967280163599182,"[1] J. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyanskiy, F. Lebr√≥n, and S. Sanghai. GQA: Training
296"
REFERENCES,0.39775051124744376,"generalized multi-query Transformer models from multi-head checkpoints. arXiv preprint
297"
REFERENCES,0.3987730061349693,"arXiv:2305.13245, 2023.
298"
REFERENCES,0.3997955010224949,"[2] S. Arora, S. Eyuboglu, A. Timalsina, I. Johnson, M. Poli, J. Zou, A. Rudra, and C. R√©. Zoology:
299"
REFERENCES,0.40081799591002043,"Measuring and improving recall in efficient language models. arXiv preprint arXiv:2312.04927,
300"
REFERENCES,0.401840490797546,"2023.
301"
REFERENCES,0.40286298568507156,"[3] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450,
302"
REFERENCES,0.40388548057259716,"2016.
303"
REFERENCES,0.4049079754601227,"[4] M. Beck, K. P√∂ppel, M. Spanring, A. Auer, O. Prudnikova, M. Kopp, G. Klambauer, J. Brand-
304"
REFERENCES,0.4059304703476483,"stetter, and S. Hochreiter.
xLSTM: Extended long short-term memory.
arXiv preprint
305"
REFERENCES,0.4069529652351738,"arXiv:2405.04517, 2024.
306"
REFERENCES,0.40797546012269936,"[5] J. Berant, A. Chou, R. Frostig, and P. Liang. Semantic parsing on Freebase from question-
307"
REFERENCES,0.40899795501022496,"answer pairs.
In Proceedings of the 2013 Conference on Empirical Methods in Natural
308"
REFERENCES,0.4100204498977505,"Language Processing, pages 1533‚Äì1544, Seattle, Washington, USA, Oct. 2013. Association for
309"
REFERENCES,0.4110429447852761,"Computational Linguistics.
310"
REFERENCES,0.4120654396728016,"[6] Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi. Piqa: Reasoning about physical com-
311"
REFERENCES,0.4130879345603272,"monsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelligence,
312"
REFERENCES,0.41411042944785276,"2020.
313"
REFERENCES,0.41513292433537835,"[7] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W.
314"
REFERENCES,0.4161554192229039,"Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko, J. Maynez, A. B. Rao,
315"
REFERENCES,0.4171779141104294,"P. Barnes, Y. Tay, N. M. Shazeer, V. Prabhakaran, E. Reif, N. Du, B. C. Hutchinson, R. Pope,
316"
REFERENCES,0.418200408997955,"J. Bradbury, J. Austin, M. Isard, G. Gur-Ari, P. Yin, T. Duke, A. Levskaya, S. Ghemawat,
317"
REFERENCES,0.41922290388548056,"S. Dev, H. Michalewski, X. Garc√≠a, V. Misra, K. Robinson, L. Fedus, D. Zhou, D. Ippolito,
318"
REFERENCES,0.42024539877300615,"D. Luan, H. Lim, B. Zoph, A. Spiridonov, R. Sepassi, D. Dohan, S. Agrawal, M. Omernick,
319"
REFERENCES,0.4212678936605317,"A. M. Dai, T. S. Pillai, M. Pellat, A. Lewkowycz, E. O. Moreira, R. Child, O. Polozov, K. Lee,
320"
REFERENCES,0.4222903885480573,"Z. Zhou, X. Wang, B. Saeta, M. D√≠az, O. Firat, M. Catasta, J. Wei, K. S. Meier-Hellstern,
321"
REFERENCES,0.4233128834355828,"D. Eck, J. Dean, S. Petrov, and N. Fiedel. PaLM: Scaling language modeling with pathways.
322"
REFERENCES,0.42433537832310836,"ArXiv, abs/2204.02311, 2022.
323"
REFERENCES,0.42535787321063395,"[8] C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova. BoolQ:
324"
REFERENCES,0.4263803680981595,"Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019
325"
REFERENCES,0.4274028629856851,"Conference of the North American Chapter of the Association for Computational Linguistics,
326"
REFERENCES,0.4284253578732106,"pages 2924‚Äì2936, 2019.
327"
REFERENCES,0.4294478527607362,"[9] T. Computer. Redpajama-data: An open source recipe to reproduce llama training dataset, 2023.
328"
REFERENCES,0.43047034764826175,"URL https://github.com/togethercomputer/RedPajama-Data.
329"
REFERENCES,0.43149284253578735,"[10] T. Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. arXiv
330"
REFERENCES,0.4325153374233129,"preprint arXiv:2307.08691, 2023.
331"
REFERENCES,0.4335378323108384,"[11] T. Dao, D. Y. Fu, K. K. Saab, A. W. Thomas, A. Rudra, and C. R√©. Hungry hungry hippos:
332"
REFERENCES,0.434560327198364,"Towards language modeling with state space models. arXiv preprint arXiv:2212.14052, 2022.
333"
REFERENCES,0.43558282208588955,"[12] S. De, S. L. Smith, A. Fernando, A. Botev, G. Cristian-Muraru, A. Gu, R. Haroun, L. Berrada,
334"
REFERENCES,0.43660531697341515,"Y. Chen, S. Srinivasan, G. Desjardins, A. Doucet, D. Budden, Y. W. Teh, R. Pascanu, N. D.
335"
REFERENCES,0.4376278118609407,"Freitas, and C. Gulcehre. Griffin: Mixing gated linear recurrences with local attention for
336"
REFERENCES,0.4386503067484663,"efficient language models. 2024.
337"
REFERENCES,0.4396728016359918,"[13] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical
338"
REFERENCES,0.44069529652351735,"image database. In 2009 IEEE conference on computer vision and pattern recognition, pages
339"
REFERENCES,0.44171779141104295,"248‚Äì255. Ieee, 2009.
340"
REFERENCES,0.4427402862985685,"[14] J. Dodge, A. Marasovi¬¥c, G. Ilharco, D. Groeneveld, M. Mitchell, and M. Gardner. Documenting
341"
REFERENCES,0.4437627811860941,"large webtext corpora: A case study on the colossal clean crawled corpus. In Conference on
342"
REFERENCES,0.4447852760736196,"Empirical Methods in Natural Language Processing, 2021.
343"
REFERENCES,0.4458077709611452,"[15] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,
344"
REFERENCES,0.44683026584867075,"M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for
345"
REFERENCES,0.44785276073619634,"image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.
346"
REFERENCES,0.4488752556237219,"[16] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite,
347"
REFERENCES,0.4498977505112474,"N. Nabeshima, et al. The Pile: An 800GB dataset of diverse text for language modeling. arXiv
348"
REFERENCES,0.450920245398773,"preprint arXiv:2101.00027, 2020.
349"
REFERENCES,0.45194274028629855,"[17] L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu,
350"
REFERENCES,0.45296523517382414,"A. Le Noac‚Äôh, H. Li, K. McDonell, N. Muennighoff, C. Ociepa, J. Phang, L. Reynolds,
351"
REFERENCES,0.4539877300613497,"H. Schoelkopf, A. Skowron, L. Sutawika, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou. A
352"
REFERENCES,0.4550102249488753,"framework for few-shot language model evaluation, 12 2023. URL https://zenodo.org/
353"
REFERENCES,0.4560327198364008,"records/10256836.
354"
REFERENCES,0.4570552147239264,"[18] X. Geng and H. Liu. Openllama: An open reproduction of llama, May 2023. URL https:
355"
REFERENCES,0.45807770961145194,"//github.com/openlm-research/open_llama.
356"
REFERENCES,0.4591002044989775,"[19] A. Gu and T. Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv
357"
REFERENCES,0.4601226993865031,"preprint arXiv:2312.00752, 2023.
358"
REFERENCES,0.4611451942740286,"[20] A. Gu, K. Goel, and C. R√©. Efficiently modeling long sequences with structured state spaces.
359"
REFERENCES,0.4621676891615542,"arXiv preprint arXiv:2111.00396, 2021.
360"
REFERENCES,0.46319018404907975,"[21] D. Han, X. Pan, Y. Han, S. Song, and G. Huang. Flatten Transformer: Vision Transformer
361"
REFERENCES,0.46421267893660534,"using focused linear attention. In Proceedings of the IEEE/CVF International Conference on
362"
REFERENCES,0.4652351738241309,"Computer Vision, pages 5961‚Äì5971, 2023.
363"
REFERENCES,0.4662576687116564,"[22] K. He, G. Gkioxari, P. Doll√°r, and R. Girshick. Mask r-cnn. In Proceedings of the IEEE
364"
REFERENCES,0.467280163599182,"international conference on computer vision, pages 2961‚Äì2969, 2017.
365"
REFERENCES,0.46830265848670755,"[23] D. Hendrycks and K. Gimpel. Gaussian error linear units (GELUs). arXiv: Learning, 2016.
366"
REFERENCES,0.46932515337423314,"[24] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring
367"
REFERENCES,0.4703476482617587,"massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.
368"
REFERENCES,0.47137014314928427,"[25] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9:1735‚Äì1780,
369"
REFERENCES,0.4723926380368098,"Nov. 1997.
370"
REFERENCES,0.4734151329243354,"[26] W. Hua, Z. Dai, H. Liu, and Q. Le. Transformer quality in linear time. In International
371"
REFERENCES,0.47443762781186094,"Conference on Machine Learning, pages 9099‚Äì9117. PMLR, 2022.
372"
REFERENCES,0.4754601226993865,"[27] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. Transformers are rnns: Fast autoregressive
373"
REFERENCES,0.47648261758691207,"transformers with linear attention. In International Conference on Machine Learning, pages
374"
REFERENCES,0.4775051124744376,"5156‚Äì5165. PMLR, 2020.
375"
REFERENCES,0.4785276073619632,"[28] T. Katsch. Gateloop: Fully data-controlled linear recurrence for sequence modeling. arXiv
376"
REFERENCES,0.47955010224948874,"preprint arXiv:2311.01927, 2023.
377"
REFERENCES,0.48057259713701433,"[29] D. Kocetkov, R. Li, L. Ben Allal, J. Li, C. Mou, C. Mu√±oz Ferrandis, Y. Jernite, M. Mitchell,
378"
REFERENCES,0.4815950920245399,"S. Hughes, T. Wolf, D. Bahdanau, L. von Werra, and H. de Vries. The Stack: 3TB of permissively
379"
REFERENCES,0.48261758691206547,"licensed source code. Preprint, 2022.
380"
REFERENCES,0.483640081799591,"[30] H. Levesque, E. Davis, and L. Morgenstern. The winograd schema challenge. In Thirteenth
381"
REFERENCES,0.48466257668711654,"International Conference on the Principles of Knowledge Representation and Reasoning, 2012.
382"
REFERENCES,0.48568507157464214,"[31] O. Lieber, B. Lenz, H. Bata, G. Cohen, J. Osin, I. Dalmedigos, E. Safahi, S. Meirom, Y. Belinkov,
383"
REFERENCES,0.4867075664621677,"S. Shalev-Shwartz, et al. Jamba: A hybrid Transformer-Mamba language model. arXiv preprint
384"
REFERENCES,0.48773006134969327,"arXiv:2403.19887, 2024.
385"
REFERENCES,0.4887525562372188,"[32] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll√°r, and C. L. Zitnick.
386"
REFERENCES,0.4897750511247444,"Microsoft COCO: Common objects in context. In Computer Vision‚ÄìECCV 2014: 13th European
387"
REFERENCES,0.49079754601226994,"Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740‚Äì755.
388"
REFERENCES,0.4918200408997955,"Springer, 2014.
389"
REFERENCES,0.49284253578732107,"[33] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In International Conference
390"
REFERENCES,0.4938650306748466,"on Learning Representations, 2019.
391"
REFERENCES,0.4948875255623722,"[34] N. Mostafazadeh, M. Roth, A. Louis, N. Chambers, and J. Allen. Lsdsem 2017 shared task: The
392"
REFERENCES,0.49591002044989774,"story cloze test. In Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential
393"
REFERENCES,0.49693251533742333,"and Discourse-level Semantics, pages 46‚Äì51, 2017.
394"
REFERENCES,0.49795501022494887,"[35] A. Orvieto, S. L. Smith, A. Gu, A. Fernando, C. Gulcehre, R. Pascanu, and S. De. Resurrecting
395"
REFERENCES,0.49897750511247446,"recurrent neural networks for long sequences. ArXiv, abs/2303.06349, 2023.
396"
REFERENCES,0.5,"[36] B. Peng, E. Alcaide, Q. G. Anthony, A. Albalak, S. Arcadinho, H. Cao, X. Cheng, M. Chung,
397"
REFERENCES,0.5010224948875256,"M. Grella, G. Kranthikiran, X. He, H. Hou, et al. RWKV: Reinventing RNNs for the Transformer
398"
REFERENCES,0.5020449897750511,"era. ArXiv, abs/2305.13048, 2023.
399"
REFERENCES,0.5030674846625767,"[37] H. Peng, N. Pappas, D. Yogatama, R. Schwartz, N. A. Smith, and L. Kong. Random feature
400"
REFERENCES,0.5040899795501023,"attention. arXiv preprint arXiv:2103.02143, 2021.
401"
REFERENCES,0.5051124744376279,"[38] M. Poli, S. Massaroli, E. Nguyen, D. Y. Fu, T. Dao, S. Baccus, Y. Bengio, S. Ermon, and
402"
REFERENCES,0.5061349693251533,"C. R√©. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint
403"
REFERENCES,0.5071574642126789,"arXiv:2302.10866, 2023.
404"
REFERENCES,0.5081799591002045,"[39] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. SQuAD: 100,000+ questions for machine
405"
REFERENCES,0.50920245398773,"comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in
406"
REFERENCES,0.5102249488752556,"Natural Language Processing, pages 2383‚Äì2392, Austin, Texas, Nov. 2016. Association for
407"
REFERENCES,0.5112474437627812,"Computational Linguistics. doi: 10.18653/v1/D16-1264.
408"
REFERENCES,0.5122699386503068,"[40] P. Ramachandran, B. Zoph, and Q. V. Le. Swish: a self-gated activation function. arXiv: Neural
409"
REFERENCES,0.5132924335378323,"and Evolutionary Computing, 2017.
410"
REFERENCES,0.5143149284253579,"[41] U. Shaham, M. Ivgi, A. Efrat, J. Berant, and O. Levy. ZeroSCROLLS: A zero-shot benchmark
411"
REFERENCES,0.5153374233128835,"for long text understanding. arXiv preprint arXiv:2305.14196, 2023.
412"
REFERENCES,0.516359918200409,"[42] N. M. Shazeer.
Fast Transformer decoding: One write-head is all you need.
ArXiv,
413"
REFERENCES,0.5173824130879345,"abs/1911.02150, 2019.
414"
REFERENCES,0.5184049079754601,"[43] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro. Megatron-LM:
415"
REFERENCES,0.5194274028629857,"Training multi-billion parameter language models using model parallelism. arXiv preprint
416"
REFERENCES,0.5204498977505112,"arXiv:1909.08053, 2019.
417"
REFERENCES,0.5214723926380368,"[44] J. Su, Y. Lu, S. Pan, B. Wen, and Y. Liu. Roformer: Enhanced transformer with rotary position
418"
REFERENCES,0.5224948875255624,"embedding. arXiv preprint arXiv:2104.09864, 2021.
419"
REFERENCES,0.523517382413088,"[45] Y. Sun, L. Dong, B. Patra, S. Ma, S. Huang, A. Benhaim, V. Chaudhary, X. Song, and F. Wei. A
420"
REFERENCES,0.5245398773006135,"length-extrapolatable transformer. In Proceedings of the 61st Annual Meeting of the Association
421"
REFERENCES,0.5255623721881391,"for Computational Linguistics (Volume 1: Long Papers), pages 14590‚Äì14604, Toronto, Canada,
422"
REFERENCES,0.5265848670756647,"July 2023. Association for Computational Linguistics.
423"
REFERENCES,0.5276073619631901,"[46] P. Tillet and D. Cox. Triton: An intermediate language and compiler for tiled neural network
424"
REFERENCES,0.5286298568507157,"computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine
425"
REFERENCES,0.5296523517382413,"Learning and Programming Languages, pages 10‚Äì19, 2019.
426"
REFERENCES,0.5306748466257669,"[47] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. J√©gou. Training data-efficient
427"
REFERENCES,0.5316973415132924,"image transformers & distillation through attention. In International conference on machine
428"
REFERENCES,0.532719836400818,"learning, pages 10347‚Äì10357. PMLR, 2021.
429"
REFERENCES,0.5337423312883436,"[48] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi√®re, N. Goyal,
430"
REFERENCES,0.5347648261758691,"E. Hambro, F. Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv
431"
REFERENCES,0.5357873210633947,"preprint arXiv:2302.13971, 2023.
432"
REFERENCES,0.5368098159509203,"[49] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,
433"
REFERENCES,0.5378323108384458,"P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv
434"
REFERENCES,0.5388548057259713,"preprint arXiv:2307.09288, 2023.
435"
REFERENCES,0.5398773006134969,"[50] J. Tow, M. Bellagente, D. Mahan, and C. Riquelme. StableLM 3B 4E1T. https://aka.ms/
436"
REFERENCES,0.5408997955010225,"StableLM-3B-4E1T, 2023.
437"
REFERENCES,0.5419222903885481,"[51] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and
438"
REFERENCES,0.5429447852760736,"I. Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems
439"
REFERENCES,0.5439672801635992,"30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017,
440"
REFERENCES,0.5449897750511248,"Long Beach, CA, USA, pages 6000‚Äì6010, 2017.
441"
REFERENCES,0.5460122699386503,"[52] A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill, O. Levy, and S. R.
442"
REFERENCES,0.5470347648261759,"Bowman. SuperGLUE: A stickier benchmark for general-purpose language understanding
443"
REFERENCES,0.5480572597137015,"systems. arXiv preprint arXiv:1905.00537, 2019.
444"
REFERENCES,0.549079754601227,"[53] Y. Wu and K. He. Group normalization. In Proceedings of the European conference on computer
445"
REFERENCES,0.5501022494887525,"vision (ECCV), pages 3‚Äì19, 2018.
446"
REFERENCES,0.5511247443762781,"[54] T. Xiao, Y. Liu, B. Zhou, Y. Jiang, and J. Sun. Unified perceptual parsing for scene understanding.
447"
REFERENCES,0.5521472392638037,"In Proceedings of the European conference on computer vision (ECCV), pages 418‚Äì434, 2018.
448"
REFERENCES,0.5531697341513292,"[55] W. Xiong, J. Liu, I. Molybog, H. Zhang, P. Bhargava, R. Hou, L. Martin, R. Rungta, K. A.
449"
REFERENCES,0.5541922290388548,"Sankararaman, B. Oguz, et al. Effective long-context scaling of foundation models. arXiv
450"
REFERENCES,0.5552147239263804,"preprint arXiv:2309.16039, 2023.
451"
REFERENCES,0.556237218813906,"[56] S. Yang, B. Wang, Y. Shen, R. Panda, and Y. Kim. Gated linear attention transformers with
452"
REFERENCES,0.5572597137014315,"hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023.
453"
REFERENCES,0.558282208588957,"[57] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. Hellaswag: Can a machine really
454"
REFERENCES,0.5593047034764826,"finish your sentence?
In Proceedings of the 57th Annual Meeting of the Association for
455"
REFERENCES,0.5603271983640081,"Computational Linguistics, 2019.
456"
REFERENCES,0.5613496932515337,"[58] S. Zhai, W. Talbott, N. Srivastava, C. Huang, H. Goh, R. Zhang, and J. Susskind. An attention
457"
REFERENCES,0.5623721881390593,"free transformer. arXiv preprint arXiv:2105.14103, 2021.
458"
REFERENCES,0.5633946830265849,"[59] B. Zhang and R. Sennrich. Root mean square layer normalization. Advances in Neural
459"
REFERENCES,0.5644171779141104,"Information Processing Systems, 32, 2019.
460"
REFERENCES,0.565439672801636,"[60] B. Zhou, H. Zhao, X. Puig, T. Xiao, S. Fidler, A. Barriuso, and A. Torralba.
Semantic
461"
REFERENCES,0.5664621676891616,"understanding of scenes through the ADE20k dataset. International Journal of Computer Vision,
462"
REFERENCES,0.5674846625766872,"127:302‚Äì321, 2019.
463"
REFERENCES,0.5685071574642127,"A
Scaling Up Number of Training Tokens
464"
REFERENCES,0.5695296523517382,"We scale up the number of training tokens to 350B for the 3B-size models. We compare with strong
465"
REFERENCES,0.5705521472392638,"Transformer checkpoints including OpenLLaMA [18] and StableLM [50]. Moreover, we reproduce a
466"
REFERENCES,0.5715746421267893,"Transformer language model (named TransformerRepro) for apple-to-apple comparison.
467"
REFERENCES,0.5725971370143149,"Our model RetNet+ follows the same configuration as in Section 3.3, which is a hybrid model. The
468"
REFERENCES,0.5736196319018405,"model‚Äôs hidden size is 3072, and the number of layers is 28. Without vocabulary embedding, the total
469"
REFERENCES,0.5746421267893661,"number of parameters is 3.17B, which is between StableLM-3B-4E1T (2.7B) and OpenLLaMA-3B-
470"
REFERENCES,0.5756646216768916,"v1 (3.19B). The batch size is 4M tokens. The training length is 4k. The learning rate is 3.2 √ó 10‚àí4
471"
REFERENCES,0.5766871165644172,"with 1000 warm-up steps and linear learning rate decay. The training corpus includes The Pile [16]
472"
REFERENCES,0.5777096114519428,"and RedPajama [9]. TransformerRepro follows the exact same setting.
473"
REFERENCES,0.5787321063394683,"Table 5 reports accuracy numbers on the Harness-Eval benchmark [17]. We directly follow the evalua-
474"
REFERENCES,0.5797546012269938,"tion protocol. The results show that RetNet+ achieves a performance comparable to TransformerRepro
475"
REFERENCES,0.5807770961145194,"on language tasks. Notice that OpenLLaMA-3B-v1 and StableLM-3B use different learning rate
476"
REFERENCES,0.581799591002045,"schedules. The results of these two models are used for reference purposes.
477"
REFERENCES,0.5828220858895705,"Model
ARC-C
ARC-Cnorm
ARC-E
ARC-Enorm
Hellaswag
Hellaswagnorm
OpenLLaMA-3B-v1
0.303
0.323
0.641
0.599
0.449
0.608
StableLM-3B
‚Äî
‚Äî
0.649
0.610
‚Äî
‚Äî
TransformerRepro
0.322
0.354
0.668
0.633
0.476
0.633
RetNet+
0.321
0.347
0.675
0.613
0.478
0.639"
REFERENCES,0.5838445807770961,"Model
OBQA
OBQAnorm
PIQA
PIQAnorm
Winogrande
Avg"
REFERENCES,0.5848670756646217,"OpenLLaMA-3B-v1
0.222
0.348
0.713
0.724
0.594
0.502
StableLM-3B
‚Äî
‚Äî
0.759
0.763
0.608
‚Äî
TransformerRepro
0.258
0.358
0.746
0.755
0.612
0.529
RetNet+
0.258
0.362
0.750
0.763
0.614
0.529"
REFERENCES,0.5858895705521472,"Table 5: Accuracy on the Harness-Eval benchmark. All models are trained with 350B tokens with a
batch size of 4M tokens. The results of OpenLLaMA-3B-v1 are taken from their official repository
(https://bit.ly/openllama-350b-results), and StableLM-3B from their technical report
(https://bit.ly/StableLM-3B-4E1T)."
REFERENCES,0.5869120654396728,"B
Equivalence Between Chunk-wise Recurrent Representation and
478"
REFERENCES,0.5879345603271984,"Recurrent Representation
479"
REFERENCES,0.588957055214724,"We illustrate the equivalence between the recurrent representation and the chunk-wise recurrent
480"
REFERENCES,0.5899795501022495,"representation. Specifically, let B denote the chunk length. For the output On, n can be divided as
481"
REFERENCES,0.591002044989775,"n = kB + r where B is the chunk size. Following Equation 6, we have:
482 On = n
X"
REFERENCES,0.5920245398773006,"m=1
Œ≥n‚àímQnK‚ä∫
mVm"
REFERENCES,0.5930470347648262,"= (QnK‚ä∫
kB+1:n ‚äôŒì)VkB+1:n + (QnŒ≥r) k‚àí1
X c=0 B
X"
REFERENCES,0.5940695296523517,"m=1
(K‚ä∫
m+cBVm+cBŒ≥B‚àím)Œ≥(k‚àí1‚àíc)B"
REFERENCES,0.5950920245398773,"= (QnK‚ä∫
kB+1:n ‚äôŒì)VkB+1:n + (QnŒ≥r) k
X"
REFERENCES,0.5961145194274029,"c=1
(K‚ä∫
[c](V[c] ‚äôŒ∂))Œ≥(k‚àíc)B"
REFERENCES,0.5971370143149284,"= (QnK‚ä∫
kB+1:n ‚äôŒì)VkB+1:n + (QnŒ≥r)Ri‚àí1 (10)"
REFERENCES,0.598159509202454,"where Œìi = Œ≥n‚àíi, Œ∂ij = Œ≥B‚àím, and [i] indicates the i-th chunk, i.e., x[i] = [x(i‚àí1)B+1, ¬∑ ¬∑ ¬∑ , xiB].
483"
REFERENCES,0.5991820040899796,"Then we write Rn as a recurrent function and compute the retention output of the i-th chunk via:
484"
REFERENCES,0.6002044989775052,"Ri =K‚ä∫
[i](V[i] ‚äôŒ∂) + Œ≥BRi‚àí1"
REFERENCES,0.6012269938650306,"Œ∂ij = Œ≥B‚àíi,
Œæij = Œ≥i"
REFERENCES,0.6022494887525562,"Retention(X[i]) = (Q[i]K‚ä∫
[i] ‚äôD)V[i]
|
{z
}
Inner-Chunk"
REFERENCES,0.6032719836400818,"+ (Q[i] ‚äôŒæ)Ri‚àí1
|
{z
}
Cross-Chunk (11)"
REFERENCES,0.6042944785276073,"Finally, we show that the chunkwise recurrent representation is equivalent to the other representations.
485"
REFERENCES,0.6053169734151329,"C
Results with Different Context Lengths
486"
REFERENCES,0.6063394683026585,"As shown in Table 6, we report the results of language modeling with different context lengths. In
487"
REFERENCES,0.6073619631901841,"order to make the numbers comparable, we use 2048 text chunks as evaluation data and only compute
488"
REFERENCES,0.6083844580777096,"the perplexity for the last 128 tokens. Experimental results show that RetNet performs comparably
489"
REFERENCES,0.6094069529652352,"with Transformer in different context lengths.
490"
REFERENCES,0.6104294478527608,"Model
512
1024
2048"
REFERENCES,0.6114519427402862,"Transformer
13.55
12.56
12.35
RetNet
13.09
12.14
11.98"
REFERENCES,0.6124744376278118,"Table 6: Language modeling perplexity of RetNet and Transformer with different context length. The
results show that RetNet has a consistent advantage across sequence length."
REFERENCES,0.6134969325153374,"D
Hyperparameters Used in Section 3.1
491"
REFERENCES,0.614519427402863,"We use LLaMA [48] architecture, including RMSNorm [59] and SwiGLU [40, 7] module, as
492"
REFERENCES,0.6155419222903885,"the Transformer backbone, which shows better performance and stability. The weights of word
493"
REFERENCES,0.6165644171779141,"embedding and softmax projection are shared. Consequently, other variants follow these settings.
494"
REFERENCES,0.6175869120654397,"For RetNet, the FFN intermediate dimension is 5"
REFERENCES,0.6186094069529653,"3d and the value dimensions in WG, WV , WO are
495"
REFERENCES,0.6196319018404908,also 5
REFERENCES,0.6206543967280164,"3d, where the overall parameters are still 12d2.
496"
REFERENCES,0.621676891615542,"For H3, we set the head dimension to 8. For RWKV, we use the TimeMix module to substitute
497"
REFERENCES,0.6226993865030674,"self-attention layers while keeping FFN layers consistent with other models for fair comparisons.
498"
REFERENCES,0.623721881390593,"For Mamba, we follow all the details in the paper [19], where double-SSM layers are implemented
499"
REFERENCES,0.6247443762781186,"instead of ‚ÄúSSM + SwiGLU‚Äù. In addition to RetNet and Mamba, the FFN intermediate dimension is
500 all 8"
REFERENCES,0.6257668711656442,"3d. All models have 400M parameters, 24 layers, and a hidden dimension of 1024. We train the
501"
REFERENCES,0.6267893660531697,"models with 40k steps and a batch size of 0.25M tokens.
502"
REFERENCES,0.6278118609406953,"Params
Values"
REFERENCES,0.6288343558282209,"Layers
24
Hidden size
1024
Vocab size
100,288
Heads
24
Adam Œ≤
(0.9, 0.98)
LR
1.5 √ó 10‚àí4
Batch size
0.25M
Warmup steps
375
Weight decay
0.05
Dropout
0.0"
REFERENCES,0.6298568507157464,Table 7: Hyperparamters used for the architecture comparison in Section 3.1.
REFERENCES,0.630879345603272,"E
Hyperparameters Used in Section 3.2
503"
REFERENCES,0.6319018404907976,"We re-allocate the parameters in MSR and FFN for fair comparisons. Let d denote dmodel for simplicity
504"
REFERENCES,0.6329243353783232,"here. In Transformers, there are about 4d2 parameters in self-attention where WQ, WK, WV , WO ‚àà
505"
REFERENCES,0.6339468302658486,"Rd√ód, and 8d2 parameters in FFN where the intermediate dimension is 4d. In comparison, RetNet
506"
REFERENCES,0.6349693251533742,"has 8d2 parameters in retention, where WQ, WK ‚ààRd√ód, WG, WV ‚ààRd√ó2d, WO ‚ààR2d√ód. Notice
507"
REFERENCES,0.6359918200408998,"that the head dimension of V is twice Q, K, similar to GAU [26]. The widened dimension is
508"
REFERENCES,0.6370143149284253,"projected back to d by WO. In order to keep the parameter number the same as Transformer, the FFN
509"
REFERENCES,0.6380368098159509,"intermediate dimension in RetNet is 2d. Meanwhile, we set the head dimension to 256, i.e., 256 for
510"
REFERENCES,0.6390593047034765,"queries and keys, and 512 for values. For fair comparison, we keep Œ≥ identical among different model
511"
REFERENCES,0.6400817995910021,"sizes, where Œ≥ = 1 ‚àíelinspace(log 1/32,log 1/512,h) ‚ààRh instead of the default value in Equation (8).
512"
REFERENCES,0.6411042944785276,"Hyperparameters
1.3B
2.7B
6.7B"
REFERENCES,0.6421267893660532,"Layers
24
32
32
Hidden size
2048
2560
4096
FFN size
4096
5120
8192
Heads
8
10
16"
REFERENCES,0.6431492842535788,"Learning rate
6 √ó 10‚àí4
3 √ó 10‚àí4
3 √ó 10‚àí4
LR scheduler
Linear decay
Warm-up steps
375
Tokens per batch
4M
Adam Œ≤
(0.9, 0.98)
Training steps
25,000"
REFERENCES,0.6441717791411042,"Gradient clipping
2.0
Dropout
0.1
Weight decay
0.05"
REFERENCES,0.6451942740286298,Table 8: Hyperparamters used for language modeling in Section 3.2.
REFERENCES,0.6462167689161554,"F
Results on Open-Ended Generation Tasks
513"
REFERENCES,0.647239263803681,"Table 9 presents one-shot performance on two open-ended question-answering tasks, including
514"
REFERENCES,0.6482617586912065,"SQUAD [39] and WebQS [5], with 6.7B models as follows. We report the recall metric in the table,
515"
REFERENCES,0.6492842535787321,"i.e., whether the answers are contained in the generated response.
516"
REFERENCES,0.6503067484662577,"Dataset
SQUAD
WebQS"
REFERENCES,0.6513292433537833,"Transformer
67.7
36.4
RetNet
72.7
40.4"
REFERENCES,0.6523517382413088,Table 9: Answer recall of RetNet and Transformer on open-ended question answering.
REFERENCES,0.6533742331288344,"G
Inference Cost of Grouped-Query Retention
517"
REFERENCES,0.65439672801636,"We compare with grouped-query attention [1] and evaluate the method in the context of RetNet.
518"
REFERENCES,0.6554192229038854,"Grouped-query attention makes a trade-off between performance and efficiency, which has been
519"
REFERENCES,0.656441717791411,"successfully verified in LLaMA2 34B/70B [49]. The method reduces the overhead of key/value cache
520"
REFERENCES,0.6574642126789366,"during inference. Moreover, the performance of grouped-query attention is better than multi-query
521"
REFERENCES,0.6584867075664622,"attention [42], overcoming the quality degradation brought by using one-head key value.
522"
REFERENCES,0.6595092024539877,"As shown in Table 10, we compare the inference cost with grouped-query attention and apply the
523"
REFERENCES,0.6605316973415133,"method for RetNet. For the LLaMA2 70B model, the number of key/value heads is reduced by 8√ó,
524"
REFERENCES,0.6615541922290389,"where the query head number is 64 while the key/value head number is 8. For RetNet-70B, the
525"
REFERENCES,0.6625766871165644,"parameter allocation is identical to LLaMA [48], where the dimension is 8192, and the head number
526"
REFERENCES,0.66359918200409,"is 32 for RetNet. For RetNet-70B-GQ2, the key-value head number is 16, where grouped-query
527"
REFERENCES,0.6646216768916156,"retention is applied. We run the inference with four A100 GPUs without quantization.
528"
REFERENCES,0.6656441717791411,"When the batch size is 256, LLaMA2 runs out of memory while RetNet without group query still
529"
REFERENCES,0.6666666666666666,"has a high throughput. When equipped with grouped-query retention, RetNet-70B achieves 38%
530"
REFERENCES,0.6676891615541922,"acceleration and saves 30% memory.
531"
REFERENCES,0.6687116564417178,"We evaluate LLaMA2 under 2k and 8k lengths separately. The batch size is reduced to 8 so that
532"
REFERENCES,0.6697341513292433,"LLaMA2 can run without out of memory. Table 10 shows that the inference cost of Transformers
533"
REFERENCES,0.6707566462167689,"increases with the sequence length. In contrast, RetNet is length-invariant. Moreover, RetNet-70B-
534"
REFERENCES,0.6717791411042945,"GQ2 achieves better latency, throughput, and GPU memory than LLaMA2-70B-2k/8k equipped
535"
REFERENCES,0.6728016359918201,"with grouped-query attention. Notice that the evaluation metrics are averaged over positions of
536"
REFERENCES,0.6738241308793456,"different sequence lengths for a fair comparison, rather than only considering the inference cost of
537"
REFERENCES,0.6748466257668712,"the maximum length.
538"
REFERENCES,0.6758691206543967,"Model
Batch Size
Latency (ms)‚Üì
Throughput (wps)‚Üë
Memory (GB)‚Üì"
REFERENCES,0.6768916155419223,"LLaMA2-70B-2k
256
‚Äî
‚Äî
OOM
LLaMA2-70B-8k
256
‚Äî
‚Äî
OOM
RetNet-70B
256
639.1
410.19
72.469
RetNet-70B-GQ2
256
461.8
567.66
52.726"
REFERENCES,0.6779141104294478,"LLaMA2-70B-2k
8
184.5
44.42
33.374
LLaMA2-70B-8k
8
277.7
29.50
37.386
RetNet-70B-GQ2
8
106.2
77.02
32.301"
REFERENCES,0.6789366053169734,"Table 10: Inference cost of RetNet and LLaMA2-70B with difference batch size and length. LLaMA2-
70B is equipped with grouped-query attention, reducing key/value heads by 8√ó. ‚Äú-GQ2‚Äù means
grouped-query retention, which reduces half of key/value heads. ‚Äú-2k‚Äù and ‚Äú-8k‚Äù indicate sequence
length for LLaMA2, while RetNet is length-invariant. RetNet is capable of large-batch inference and
is favourable in terms of latency, throughput, and GPU memory."
REFERENCES,0.679959100204499,"H
Hyperparameters Used in Section 3.8
539"
REFERENCES,0.6809815950920245,"Hyperparameters
DeiT
RetNet"
REFERENCES,0.6820040899795501,"Layers
12
12
Hidden size
512
512
Patch size
16
16
FFN size
2048
1024
Heads
8
2"
REFERENCES,0.6830265848670757,"Learning rate
1 √ó 10‚àí3
LR scheduler
Cosine decay
Batch size
1024
Epochs
300
Warmup epochs
5
Smoothing
0.1
Weight decay
0.05
Drop path
0.3
Table 11: Hyperparamters used for the ImageNet experiments in Section 3.8."
REFERENCES,0.6840490797546013,"NeurIPS Paper Checklist
540"
CLAIMS,0.6850715746421268,"1. Claims
541"
CLAIMS,0.6860940695296524,"Question: Do the main claims made in the abstract and introduction accurately reflect the
542"
CLAIMS,0.6871165644171779,"paper‚Äôs contributions and scope?
543"
CLAIMS,0.6881390593047034,"Answer: [Yes]
544"
CLAIMS,0.689161554192229,"Justification: The abstract and introduction is carefully written.
545"
CLAIMS,0.6901840490797546,"Guidelines:
546"
CLAIMS,0.6912065439672802,"‚Ä¢ The answer NA means that the abstract and introduction do not include the claims
547"
CLAIMS,0.6922290388548057,"made in the paper.
548"
CLAIMS,0.6932515337423313,"‚Ä¢ The abstract and/or introduction should clearly state the claims made, including the
549"
CLAIMS,0.6942740286298569,"contributions made in the paper and important assumptions and limitations. A No or
550"
CLAIMS,0.6952965235173824,"NA answer to this question will not be perceived well by the reviewers.
551"
CLAIMS,0.696319018404908,"‚Ä¢ The claims made should match theoretical and experimental results, and reflect how
552"
CLAIMS,0.6973415132924335,"much the results can be expected to generalize to other settings.
553"
CLAIMS,0.6983640081799591,"‚Ä¢ It is fine to include aspirational goals as motivation as long as it is clear that these goals
554"
CLAIMS,0.6993865030674846,"are not attained by the paper.
555"
LIMITATIONS,0.7004089979550102,"2. Limitations
556"
LIMITATIONS,0.7014314928425358,"Question: Does the paper discuss the limitations of the work performed by the authors?
557"
LIMITATIONS,0.7024539877300614,"Answer: [Yes]
558"
LIMITATIONS,0.7034764826175869,"Justification: Limitations are discussed in the paper.
559"
LIMITATIONS,0.7044989775051125,"Guidelines:
560"
LIMITATIONS,0.7055214723926381,"‚Ä¢ The answer NA means that the paper has no limitation while the answer No means that
561"
LIMITATIONS,0.7065439672801636,"the paper has limitations, but those are not discussed in the paper.
562"
LIMITATIONS,0.7075664621676891,"‚Ä¢ The authors are encouraged to create a separate ""Limitations"" section in their paper.
563"
LIMITATIONS,0.7085889570552147,"‚Ä¢ The paper should point out any strong assumptions and how robust the results are to
564"
LIMITATIONS,0.7096114519427403,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
565"
LIMITATIONS,0.7106339468302658,"model well-specification, asymptotic approximations only holding locally). The authors
566"
LIMITATIONS,0.7116564417177914,"should reflect on how these assumptions might be violated in practice and what the
567"
LIMITATIONS,0.712678936605317,"implications would be.
568"
LIMITATIONS,0.7137014314928425,"‚Ä¢ The authors should reflect on the scope of the claims made, e.g., if the approach was
569"
LIMITATIONS,0.7147239263803681,"only tested on a few datasets or with a few runs. In general, empirical results often
570"
LIMITATIONS,0.7157464212678937,"depend on implicit assumptions, which should be articulated.
571"
LIMITATIONS,0.7167689161554193,"‚Ä¢ The authors should reflect on the factors that influence the performance of the approach.
572"
LIMITATIONS,0.7177914110429447,"For example, a facial recognition algorithm may perform poorly when image resolution
573"
LIMITATIONS,0.7188139059304703,"is low or images are taken in low lighting. Or a speech-to-text system might not be
574"
LIMITATIONS,0.7198364008179959,"used reliably to provide closed captions for online lectures because it fails to handle
575"
LIMITATIONS,0.7208588957055214,"technical jargon.
576"
LIMITATIONS,0.721881390593047,"‚Ä¢ The authors should discuss the computational efficiency of the proposed algorithms
577"
LIMITATIONS,0.7229038854805726,"and how they scale with dataset size.
578"
LIMITATIONS,0.7239263803680982,"‚Ä¢ If applicable, the authors should discuss possible limitations of their approach to
579"
LIMITATIONS,0.7249488752556237,"address problems of privacy and fairness.
580"
LIMITATIONS,0.7259713701431493,"‚Ä¢ While the authors might fear that complete honesty about limitations might be used by
581"
LIMITATIONS,0.7269938650306749,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
582"
LIMITATIONS,0.7280163599182005,"limitations that aren‚Äôt acknowledged in the paper. The authors should use their best
583"
LIMITATIONS,0.7290388548057259,"judgment and recognize that individual actions in favor of transparency play an impor-
584"
LIMITATIONS,0.7300613496932515,"tant role in developing norms that preserve the integrity of the community. Reviewers
585"
LIMITATIONS,0.7310838445807771,"will be specifically instructed to not penalize honesty concerning limitations.
586"
THEORY ASSUMPTIONS AND PROOFS,0.7321063394683026,"3. Theory Assumptions and Proofs
587"
THEORY ASSUMPTIONS AND PROOFS,0.7331288343558282,"Question: For each theoretical result, does the paper provide the full set of assumptions and
588"
THEORY ASSUMPTIONS AND PROOFS,0.7341513292433538,"a complete (and correct) proof?
589"
THEORY ASSUMPTIONS AND PROOFS,0.7351738241308794,"Answer: [NA]
590"
THEORY ASSUMPTIONS AND PROOFS,0.7361963190184049,"Justification: There is no theoretical result in this paper.
591"
THEORY ASSUMPTIONS AND PROOFS,0.7372188139059305,"Guidelines:
592"
THEORY ASSUMPTIONS AND PROOFS,0.7382413087934561,"‚Ä¢ The answer NA means that the paper does not include theoretical results.
593"
THEORY ASSUMPTIONS AND PROOFS,0.7392638036809815,"‚Ä¢ All the theorems, formulas, and proofs in the paper should be numbered and cross-
594"
THEORY ASSUMPTIONS AND PROOFS,0.7402862985685071,"referenced.
595"
THEORY ASSUMPTIONS AND PROOFS,0.7413087934560327,"‚Ä¢ All assumptions should be clearly stated or referenced in the statement of any theorems.
596"
THEORY ASSUMPTIONS AND PROOFS,0.7423312883435583,"‚Ä¢ The proofs can either appear in the main paper or the supplemental material, but if
597"
THEORY ASSUMPTIONS AND PROOFS,0.7433537832310838,"they appear in the supplemental material, the authors are encouraged to provide a short
598"
THEORY ASSUMPTIONS AND PROOFS,0.7443762781186094,"proof sketch to provide intuition.
599"
THEORY ASSUMPTIONS AND PROOFS,0.745398773006135,"‚Ä¢ Inversely, any informal proof provided in the core of the paper should be complemented
600"
THEORY ASSUMPTIONS AND PROOFS,0.7464212678936605,"by formal proofs provided in appendix or supplemental material.
601"
THEORY ASSUMPTIONS AND PROOFS,0.7474437627811861,"‚Ä¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
602"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7484662576687117,"4. Experimental Result Reproducibility
603"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7494887525562373,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
604"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7505112474437627,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
605"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7515337423312883,"of the paper (regardless of whether the code and data are provided or not)?
606"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7525562372188139,"Answer: [Yes]
607"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7535787321063395,"Justification: The experiment can be easily reproduced based on the model description,
608"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.754601226993865,"hyperparameter, and any well-known pre-training corpus.
609"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7556237218813906,"Guidelines:
610"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7566462167689162,"‚Ä¢ The answer NA means that the paper does not include experiments.
611"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7576687116564417,"‚Ä¢ If the paper includes experiments, a No answer to this question will not be perceived
612"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7586912065439673,"well by the reviewers: Making the paper reproducible is important, regardless of
613"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7597137014314929,"whether the code and data are provided or not.
614"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7607361963190185,"‚Ä¢ If the contribution is a dataset and/or model, the authors should describe the steps taken
615"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7617586912065439,"to make their results reproducible or verifiable.
616"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7627811860940695,"‚Ä¢ Depending on the contribution, reproducibility can be accomplished in various ways.
617"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7638036809815951,"For example, if the contribution is a novel architecture, describing the architecture fully
618"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7648261758691206,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
619"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7658486707566462,"be necessary to either make it possible for others to replicate the model with the same
620"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7668711656441718,"dataset, or provide access to the model. In general. releasing code and data is often
621"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7678936605316974,"one good way to accomplish this, but reproducibility can also be provided via detailed
622"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7689161554192229,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
623"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7699386503067485,"of a large language model), releasing of a model checkpoint, or other means that are
624"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7709611451942741,"appropriate to the research performed.
625"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7719836400817995,"‚Ä¢ While NeurIPS does not require releasing code, the conference does require all submis-
626"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7730061349693251,"sions to provide some reasonable avenue for reproducibility, which may depend on the
627"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7740286298568507,"nature of the contribution. For example
628"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7750511247443763,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
629"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7760736196319018,"to reproduce that algorithm.
630"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7770961145194274,"(b) If the contribution is primarily a new model architecture, the paper should describe
631"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.778118609406953,"the architecture clearly and fully.
632"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7791411042944786,"(c) If the contribution is a new model (e.g., a large language model), then there should
633"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7801635991820041,"either be a way to access this model for reproducing the results or a way to reproduce
634"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7811860940695297,"the model (e.g., with an open-source dataset or instructions for how to construct
635"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7822085889570553,"the dataset).
636"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7832310838445807,"(d) We recognize that reproducibility may be tricky in some cases, in which case
637"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7842535787321063,"authors are welcome to describe the particular way they provide for reproducibility.
638"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7852760736196319,"In the case of closed-source models, it may be that access to the model is limited in
639"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7862985685071575,"some way (e.g., to registered users), but it should be possible for other researchers
640"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.787321063394683,"to have some path to reproducing or verifying the results.
641"
OPEN ACCESS TO DATA AND CODE,0.7883435582822086,"5. Open access to data and code
642"
OPEN ACCESS TO DATA AND CODE,0.7893660531697342,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
643"
OPEN ACCESS TO DATA AND CODE,0.7903885480572597,"tions to faithfully reproduce the main experimental results, as described in supplemental
644"
OPEN ACCESS TO DATA AND CODE,0.7914110429447853,"material?
645"
OPEN ACCESS TO DATA AND CODE,0.7924335378323109,"Answer: [Yes]
646"
OPEN ACCESS TO DATA AND CODE,0.7934560327198364,"Justification: Code will be released in camera-ready version. All of the data we use is
647"
OPEN ACCESS TO DATA AND CODE,0.7944785276073619,"public-available.
648"
OPEN ACCESS TO DATA AND CODE,0.7955010224948875,"Guidelines:
649"
OPEN ACCESS TO DATA AND CODE,0.7965235173824131,"‚Ä¢ The answer NA means that paper does not include experiments requiring code.
650"
OPEN ACCESS TO DATA AND CODE,0.7975460122699386,"‚Ä¢ Please see the NeurIPS code and data submission guidelines (https://nips.cc/
651"
OPEN ACCESS TO DATA AND CODE,0.7985685071574642,"public/guides/CodeSubmissionPolicy) for more details.
652"
OPEN ACCESS TO DATA AND CODE,0.7995910020449898,"‚Ä¢ While we encourage the release of code and data, we understand that this might not be
653"
OPEN ACCESS TO DATA AND CODE,0.8006134969325154,"possible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not
654"
OPEN ACCESS TO DATA AND CODE,0.8016359918200409,"including code, unless this is central to the contribution (e.g., for a new open-source
655"
OPEN ACCESS TO DATA AND CODE,0.8026584867075665,"benchmark).
656"
OPEN ACCESS TO DATA AND CODE,0.803680981595092,"‚Ä¢ The instructions should contain the exact command and environment needed to run to
657"
OPEN ACCESS TO DATA AND CODE,0.8047034764826176,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
658"
OPEN ACCESS TO DATA AND CODE,0.8057259713701431,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
659"
OPEN ACCESS TO DATA AND CODE,0.8067484662576687,"‚Ä¢ The authors should provide instructions on data access and preparation, including how
660"
OPEN ACCESS TO DATA AND CODE,0.8077709611451943,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
661"
OPEN ACCESS TO DATA AND CODE,0.8087934560327198,"‚Ä¢ The authors should provide scripts to reproduce all experimental results for the new
662"
OPEN ACCESS TO DATA AND CODE,0.8098159509202454,"proposed method and baselines. If only a subset of experiments are reproducible, they
663"
OPEN ACCESS TO DATA AND CODE,0.810838445807771,"should state which ones are omitted from the script and why.
664"
OPEN ACCESS TO DATA AND CODE,0.8118609406952966,"‚Ä¢ At submission time, to preserve anonymity, the authors should release anonymized
665"
OPEN ACCESS TO DATA AND CODE,0.8128834355828221,"versions (if applicable).
666"
OPEN ACCESS TO DATA AND CODE,0.8139059304703476,"‚Ä¢ Providing as much information as possible in supplemental material (appended to the
667"
OPEN ACCESS TO DATA AND CODE,0.8149284253578732,"paper) is recommended, but including URLs to data and code is permitted.
668"
OPEN ACCESS TO DATA AND CODE,0.8159509202453987,"6. Experimental Setting/Details
669"
OPEN ACCESS TO DATA AND CODE,0.8169734151329243,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
670"
OPEN ACCESS TO DATA AND CODE,0.8179959100204499,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
671"
OPEN ACCESS TO DATA AND CODE,0.8190184049079755,"results?
672"
OPEN ACCESS TO DATA AND CODE,0.820040899795501,"Answer: [Yes]
673"
OPEN ACCESS TO DATA AND CODE,0.8210633946830266,"Justification: Hyperparameters are attached in the appendix.
674"
OPEN ACCESS TO DATA AND CODE,0.8220858895705522,"Guidelines:
675"
OPEN ACCESS TO DATA AND CODE,0.8231083844580777,"‚Ä¢ The answer NA means that the paper does not include experiments.
676"
OPEN ACCESS TO DATA AND CODE,0.8241308793456033,"‚Ä¢ The experimental setting should be presented in the core of the paper to a level of detail
677"
OPEN ACCESS TO DATA AND CODE,0.8251533742331288,"that is necessary to appreciate the results and make sense of them.
678"
OPEN ACCESS TO DATA AND CODE,0.8261758691206544,"‚Ä¢ The full details can be provided either with the code, in appendix, or as supplemental
679"
OPEN ACCESS TO DATA AND CODE,0.8271983640081799,"material.
680"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8282208588957055,"7. Experiment Statistical Significance
681"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8292433537832311,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
682"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8302658486707567,"information about the statistical significance of the experiments?
683"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8312883435582822,"Answer: [No]
684"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8323108384458078,"Justification: For large language models, the variance between different runs is negligible.
685"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8333333333333334,"Moreover, the evaluation pipeline is deterministic.
686"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8343558282208589,"Guidelines:
687"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8353783231083844,"‚Ä¢ The answer NA means that the paper does not include experiments.
688"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.83640081799591,"‚Ä¢ The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
689"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8374233128834356,"dence intervals, or statistical significance tests, at least for the experiments that support
690"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8384458077709611,"the main claims of the paper.
691"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8394683026584867,"‚Ä¢ The factors of variability that the error bars are capturing should be clearly stated (for
692"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8404907975460123,"example, train/test split, initialization, random drawing of some parameter, or overall
693"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8415132924335378,"run with given experimental conditions).
694"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8425357873210634,"‚Ä¢ The method for calculating the error bars should be explained (closed form formula,
695"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.843558282208589,"call to a library function, bootstrap, etc.)
696"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8445807770961146,"‚Ä¢ The assumptions made should be given (e.g., Normally distributed errors).
697"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.84560327198364,"‚Ä¢ It should be clear whether the error bar is the standard deviation or the standard error
698"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8466257668711656,"of the mean.
699"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8476482617586912,"‚Ä¢ It is OK to report 1-sigma error bars, but one should state it. The authors should
700"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8486707566462167,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
701"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8496932515337423,"of Normality of errors is not verified.
702"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8507157464212679,"‚Ä¢ For asymmetric distributions, the authors should be careful not to show in tables or
703"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8517382413087935,"figures symmetric error bars that would yield results that are out of range (e.g. negative
704"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.852760736196319,"error rates).
705"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8537832310838446,"‚Ä¢ If error bars are reported in tables or plots, The authors should explain in the text how
706"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8548057259713702,"they were calculated and reference the corresponding figures or tables in the text.
707"
EXPERIMENTS COMPUTE RESOURCES,0.8558282208588958,"8. Experiments Compute Resources
708"
EXPERIMENTS COMPUTE RESOURCES,0.8568507157464212,"Question: For each experiment, does the paper provide sufficient information on the com-
709"
EXPERIMENTS COMPUTE RESOURCES,0.8578732106339468,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
710"
EXPERIMENTS COMPUTE RESOURCES,0.8588957055214724,"the experiments?
711"
EXPERIMENTS COMPUTE RESOURCES,0.8599182004089979,"Answer: [Yes]
712"
EXPERIMENTS COMPUTE RESOURCES,0.8609406952965235,"Justification: The corresponding resources are stated in the paper.
713"
EXPERIMENTS COMPUTE RESOURCES,0.8619631901840491,"Guidelines:
714"
EXPERIMENTS COMPUTE RESOURCES,0.8629856850715747,"‚Ä¢ The answer NA means that the paper does not include experiments.
715"
EXPERIMENTS COMPUTE RESOURCES,0.8640081799591002,"‚Ä¢ The paper should indicate the type of compute workers CPU or GPU, internal cluster,
716"
EXPERIMENTS COMPUTE RESOURCES,0.8650306748466258,"or cloud provider, including relevant memory and storage.
717"
EXPERIMENTS COMPUTE RESOURCES,0.8660531697341514,"‚Ä¢ The paper should provide the amount of compute required for each of the individual
718"
EXPERIMENTS COMPUTE RESOURCES,0.8670756646216768,"experimental runs as well as estimate the total compute.
719"
EXPERIMENTS COMPUTE RESOURCES,0.8680981595092024,"‚Ä¢ The paper should disclose whether the full research project required more compute
720"
EXPERIMENTS COMPUTE RESOURCES,0.869120654396728,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
721"
EXPERIMENTS COMPUTE RESOURCES,0.8701431492842536,"didn‚Äôt make it into the paper).
722"
CODE OF ETHICS,0.8711656441717791,"9. Code Of Ethics
723"
CODE OF ETHICS,0.8721881390593047,"Question: Does the research conducted in the paper conform, in every respect, with the
724"
CODE OF ETHICS,0.8732106339468303,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
725"
CODE OF ETHICS,0.8742331288343558,"Answer: [Yes]
726"
CODE OF ETHICS,0.8752556237218814,"Justification: We follow the NeurIPS Code of Ethics in the research.
727"
CODE OF ETHICS,0.876278118609407,"Guidelines:
728"
CODE OF ETHICS,0.8773006134969326,"‚Ä¢ The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
729"
CODE OF ETHICS,0.878323108384458,"‚Ä¢ If the authors answer No, they should explain the special circumstances that require a
730"
CODE OF ETHICS,0.8793456032719836,"deviation from the Code of Ethics.
731"
CODE OF ETHICS,0.8803680981595092,"‚Ä¢ The authors should make sure to preserve anonymity (e.g., if there is a special consid-
732"
CODE OF ETHICS,0.8813905930470347,"eration due to laws or regulations in their jurisdiction).
733"
BROADER IMPACTS,0.8824130879345603,"10. Broader Impacts
734"
BROADER IMPACTS,0.8834355828220859,"Question: Does the paper discuss both potential positive societal impacts and negative
735"
BROADER IMPACTS,0.8844580777096115,"societal impacts of the work performed?
736"
BROADER IMPACTS,0.885480572597137,"Answer: [NA]
737"
BROADER IMPACTS,0.8865030674846626,"Justification: We work on fundamental research that has no direct societal impact.
738"
BROADER IMPACTS,0.8875255623721882,"Guidelines:
739"
BROADER IMPACTS,0.8885480572597138,"‚Ä¢ The answer NA means that there is no societal impact of the work performed.
740"
BROADER IMPACTS,0.8895705521472392,"‚Ä¢ If the authors answer NA or No, they should explain why their work has no societal
741"
BROADER IMPACTS,0.8905930470347648,"impact or why the paper does not address societal impact.
742"
BROADER IMPACTS,0.8916155419222904,"‚Ä¢ Examples of negative societal impacts include potential malicious or unintended uses
743"
BROADER IMPACTS,0.8926380368098159,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
744"
BROADER IMPACTS,0.8936605316973415,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
745"
BROADER IMPACTS,0.8946830265848671,"groups), privacy considerations, and security considerations.
746"
BROADER IMPACTS,0.8957055214723927,"‚Ä¢ The conference expects that many papers will be foundational research and not tied
747"
BROADER IMPACTS,0.8967280163599182,"to particular applications, let alone deployments. However, if there is a direct path to
748"
BROADER IMPACTS,0.8977505112474438,"any negative applications, the authors should point it out. For example, it is legitimate
749"
BROADER IMPACTS,0.8987730061349694,"to point out that an improvement in the quality of generative models could be used to
750"
BROADER IMPACTS,0.8997955010224948,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
751"
BROADER IMPACTS,0.9008179959100204,"that a generic algorithm for optimizing neural networks could enable people to train
752"
BROADER IMPACTS,0.901840490797546,"models that generate Deepfakes faster.
753"
BROADER IMPACTS,0.9028629856850716,"‚Ä¢ The authors should consider possible harms that could arise when the technology is
754"
BROADER IMPACTS,0.9038854805725971,"being used as intended and functioning correctly, harms that could arise when the
755"
BROADER IMPACTS,0.9049079754601227,"technology is being used as intended but gives incorrect results, and harms following
756"
BROADER IMPACTS,0.9059304703476483,"from (intentional or unintentional) misuse of the technology.
757"
BROADER IMPACTS,0.9069529652351738,"‚Ä¢ If there are negative societal impacts, the authors could also discuss possible mitigation
758"
BROADER IMPACTS,0.9079754601226994,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
759"
BROADER IMPACTS,0.908997955010225,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
760"
BROADER IMPACTS,0.9100204498977505,"feedback over time, improving the efficiency and accessibility of ML).
761"
SAFEGUARDS,0.911042944785276,"11. Safeguards
762"
SAFEGUARDS,0.9120654396728016,"Question: Does the paper describe safeguards that have been put in place for responsible
763"
SAFEGUARDS,0.9130879345603272,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
764"
SAFEGUARDS,0.9141104294478528,"image generators, or scraped datasets)?
765"
SAFEGUARDS,0.9151329243353783,"Answer: [NA]
766"
SAFEGUARDS,0.9161554192229039,"Justification: The paper does not pose safety risks.
767"
SAFEGUARDS,0.9171779141104295,"Guidelines:
768"
SAFEGUARDS,0.918200408997955,"‚Ä¢ The answer NA means that the paper poses no such risks.
769"
SAFEGUARDS,0.9192229038854806,"‚Ä¢ Released models that have a high risk for misuse or dual-use should be released with
770"
SAFEGUARDS,0.9202453987730062,"necessary safeguards to allow for controlled use of the model, for example by requiring
771"
SAFEGUARDS,0.9212678936605317,"that users adhere to usage guidelines or restrictions to access the model or implementing
772"
SAFEGUARDS,0.9222903885480572,"safety filters.
773"
SAFEGUARDS,0.9233128834355828,"‚Ä¢ Datasets that have been scraped from the Internet could pose safety risks. The authors
774"
SAFEGUARDS,0.9243353783231084,"should describe how they avoided releasing unsafe images.
775"
SAFEGUARDS,0.9253578732106339,"‚Ä¢ We recognize that providing effective safeguards is challenging, and many papers do
776"
SAFEGUARDS,0.9263803680981595,"not require this, but we encourage authors to take this into account and make a best
777"
SAFEGUARDS,0.9274028629856851,"faith effort.
778"
LICENSES FOR EXISTING ASSETS,0.9284253578732107,"12. Licenses for existing assets
779"
LICENSES FOR EXISTING ASSETS,0.9294478527607362,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
780"
LICENSES FOR EXISTING ASSETS,0.9304703476482618,"the paper, properly credited and are the license and terms of use explicitly mentioned and
781"
LICENSES FOR EXISTING ASSETS,0.9314928425357873,"properly respected?
782"
LICENSES FOR EXISTING ASSETS,0.9325153374233128,"Answer: [Yes]
783"
LICENSES FOR EXISTING ASSETS,0.9335378323108384,"Justification: We carefully follow the licenses of open-source code, data, and models.
784"
LICENSES FOR EXISTING ASSETS,0.934560327198364,"Guidelines:
785"
LICENSES FOR EXISTING ASSETS,0.9355828220858896,"‚Ä¢ The answer NA means that the paper does not use existing assets.
786"
LICENSES FOR EXISTING ASSETS,0.9366053169734151,"‚Ä¢ The authors should cite the original paper that produced the code package or dataset.
787"
LICENSES FOR EXISTING ASSETS,0.9376278118609407,"‚Ä¢ The authors should state which version of the asset is used and, if possible, include a
788"
LICENSES FOR EXISTING ASSETS,0.9386503067484663,"URL.
789"
LICENSES FOR EXISTING ASSETS,0.9396728016359919,"‚Ä¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
790"
LICENSES FOR EXISTING ASSETS,0.9406952965235174,"‚Ä¢ For scraped data from a particular source (e.g., website), the copyright and terms of
791"
LICENSES FOR EXISTING ASSETS,0.941717791411043,"service of that source should be provided.
792"
LICENSES FOR EXISTING ASSETS,0.9427402862985685,"‚Ä¢ If assets are released, the license, copyright information, and terms of use in the
793"
LICENSES FOR EXISTING ASSETS,0.943762781186094,"package should be provided. For popular datasets, paperswithcode.com/datasets
794"
LICENSES FOR EXISTING ASSETS,0.9447852760736196,"has curated licenses for some datasets. Their licensing guide can help determine the
795"
LICENSES FOR EXISTING ASSETS,0.9458077709611452,"license of a dataset.
796"
LICENSES FOR EXISTING ASSETS,0.9468302658486708,"‚Ä¢ For existing datasets that are re-packaged, both the original license and the license of
797"
LICENSES FOR EXISTING ASSETS,0.9478527607361963,"the derived asset (if it has changed) should be provided.
798"
LICENSES FOR EXISTING ASSETS,0.9488752556237219,"‚Ä¢ If this information is not available online, the authors are encouraged to reach out to
799"
LICENSES FOR EXISTING ASSETS,0.9498977505112475,"the asset‚Äôs creators.
800"
NEW ASSETS,0.950920245398773,"13. New Assets
801"
NEW ASSETS,0.9519427402862985,"Question: Are new assets introduced in the paper well documented and is the documentation
802"
NEW ASSETS,0.9529652351738241,"provided alongside the assets?
803"
NEW ASSETS,0.9539877300613497,"Answer: [NA]
804"
NEW ASSETS,0.9550102249488752,"Justification: The paper does not release new assets.
805"
NEW ASSETS,0.9560327198364008,"Guidelines:
806"
NEW ASSETS,0.9570552147239264,"‚Ä¢ The answer NA means that the paper does not release new assets.
807"
NEW ASSETS,0.9580777096114519,"‚Ä¢ Researchers should communicate the details of the dataset/code/model as part of their
808"
NEW ASSETS,0.9591002044989775,"submissions via structured templates. This includes details about training, license,
809"
NEW ASSETS,0.9601226993865031,"limitations, etc.
810"
NEW ASSETS,0.9611451942740287,"‚Ä¢ The paper should discuss whether and how consent was obtained from people whose
811"
NEW ASSETS,0.9621676891615542,"asset is used.
812"
NEW ASSETS,0.9631901840490797,"‚Ä¢ At submission time, remember to anonymize your assets (if applicable). You can either
813"
NEW ASSETS,0.9642126789366053,"create an anonymized URL or include an anonymized zip file.
814"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9652351738241309,"14. Crowdsourcing and Research with Human Subjects
815"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9662576687116564,"Question: For crowdsourcing experiments and research with human subjects, does the paper
816"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.967280163599182,"include the full text of instructions given to participants and screenshots, if applicable, as
817"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9683026584867076,"well as details about compensation (if any)?
818"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9693251533742331,"Answer: [NA]
819"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9703476482617587,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
820"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9713701431492843,"Guidelines:
821"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9723926380368099,"‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with
822"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9734151329243353,"human subjects.
823"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9744376278118609,"‚Ä¢ Including this information in the supplemental material is fine, but if the main contribu-
824"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9754601226993865,"tion of the paper involves human subjects, then as much detail as possible should be
825"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.976482617586912,"included in the main paper.
826"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9775051124744376,"‚Ä¢ According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
827"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9785276073619632,"or other labor should be paid at least the minimum wage in the country of the data
828"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9795501022494888,"collector.
829"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9805725971370143,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
830"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9815950920245399,"Subjects
831"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9826175869120655,"Question: Does the paper describe potential risks incurred by study participants, whether
832"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.983640081799591,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
833"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9846625766871165,"approvals (or an equivalent approval/review based on the requirements of your country or
834"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9856850715746421,"institution) were obtained?
835"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9867075664621677,"Answer: [NA]
836"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9877300613496932,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
837"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9887525562372188,"Guidelines:
838"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9897750511247444,"‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with
839"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.99079754601227,"human subjects.
840"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9918200408997955,"‚Ä¢ Depending on the country in which research is conducted, IRB approval (or equivalent)
841"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9928425357873211,"may be required for any human subjects research. If you obtained IRB approval, you
842"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9938650306748467,"should clearly state this in the paper.
843"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9948875255623721,"‚Ä¢ We recognize that the procedures for this may vary significantly between institutions
844"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9959100204498977,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
845"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9969325153374233,"guidelines for their institution.
846"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9979550102249489,"‚Ä¢ For initial submissions, do not include any information that would break anonymity (if
847"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9989775051124744,"applicable), such as the institution conducting the review.
848"
