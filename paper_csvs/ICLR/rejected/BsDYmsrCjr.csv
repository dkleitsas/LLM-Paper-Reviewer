Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0020491803278688526,"Federated averaging, the most popular aggregation approach in federated learn-
ing, is known to be vulnerable to failures and adversarial updates from clients
that wish to disrupt training. While median aggregation remains one of the most
popular alternatives to improve training robustness, relying on secure multi-party
computation (MPC) to na¨ıvely compute the median is unscalable. To this end, we
propose a secure and efﬁcient approximate median aggregation with MPC privacy
guarantees in the multi-silo setting, e.g., across hospitals, with two semi-honest
non-colluding servers. The proposed method protects the conﬁdentiality of client
gradient updates against both semi-honest clients and servers. Asymptotically, the
cost of our approach scales only linearly with the number of clients, whereas the
na¨ıve MPC median scales quadratically. Moreover, we prove that the convergence
of the proposed federated learning method is robust to a wide range of failures
and attacks. Empirically, we show that our method displays similar robustness
properties as the median while converging faster than the na¨ıve MPC median for
even a small number of clients."
INTRODUCTION,0.004098360655737705,"1
Introduction"
INTRODUCTION,0.006147540983606557,"Federated Learning (FL) (McMahan et al., 2017) has emerged as a leading approach for training
shared models among clients who do not wish to release their datasets publicly. While FL avoids
releasing client data, revealing client models in the clear creates privacy vulnerabilities, e.g., using
model inversion attacks (Fredrikson et al., 2015). To this end, global model aggregation is often
implemented using Secure Multi-Party Computation (MPC) to improve privacy (Bonawitz et al.,
2016). MPC (Yao, 1986; Goldreich et al., 1987) is a cryptographic primitive that makes it possible to
evaluate a function on encrypted inputs while ensuring that the only information revealed throughout
the computation is the ﬁnal output of the function. Thus, to avoid revealing each client’s local
weights, clients encrypt their weights before sending them to the central server. MPC allows the
central server to recover a single aggregate update that it sends back to individual clients. Specialized
MPC protocols, that allow the server to recover the average of client updates have been widely
deployed for Federated Learning."
INTRODUCTION,0.00819672131147541,"In addition to privacy, robustness is a major concern in federated learning implementations (Kairouz
et al., 2019). The distributed nature of the computation creates a variety of vulnerabilities to ad-
versarial training attacks and client failures such as hardware, software, data, and communication
errors (Xie et al., 2019a; 2021). One way to improve robustness is to replace an aggregation proce-
dure that computes means with a more robust aggregate such as the median (Yin et al., 2018). Using
the median, up to half of inputs can be from a faulty distribution before the median fails as a robust
average estimator. While median aggregation alone is insufﬁcient to guarantee Byzantine robustness
(Xie et al., 2020) – median-based robustness guarantees are sufﬁcient against most published attacks
and failures. Unfortunately, the best known approaches for median aggregation with MPC are slow,
requiring computation that grows quadratically with the number of clients (Tueno et al., 2019)."
INTRODUCTION,0.010245901639344262,"Our work is motivated by applications to multi-silo federated learning (Kairouz et al., 2019), de-
signed for learning across institutions. For example, an individual hospital may be limited to using
only their own patients’ data, and sharing patient information among hospitals is generally difﬁcult
due to privacy and intellectual property concerns. Here, federated learning can provide a mechanism
for widely distributed medical institutions to jointly train a model."
INTRODUCTION,0.012295081967213115,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.014344262295081968,"The main contribution of this work is a new approximate-median aggregation for robust, privacy-
preserving federated learning in the multi-silo, two-server setting. Our approach is the ﬁrst to have
all of the following features:
• Robustness against failures. By relying on (an approximation of) the median of client updates,
we achieve robust estimates even in settings where updates sent by up to half the clients are faulty.
In addition, we empirically evaluate robustness against noise distributions that take the form of bit
ﬂips, label switching, and Gaussian noise in up to half of the clients.
• Preserving Client Privacy via Secure Computation. We prove that any subset of clients col-
luding with each other and with one of the two servers obtains no information about the private
inputs of other clients, beyond the aggregated (approximate) median that is returned at each step.
This provides meaningful privacy to every client and is a meaningful defense, e.g., against model-
inversion attacks that take advantage of individual client updates.
• Scalability. The running time of our protocol increases linearly with the number of clients; match-
ing the order-wise scaling (with number of clients) of the standard averaging aggregation. This
is in contrast to a quadratic factor blowup for a federated learning system that implements me-
dian na¨ıvely in MPC. We empirically evaluate the convergence of our system on various datasets,
models and under different types of faults."
RELATED WORK,0.01639344262295082,"2
Related Work"
RELATED WORK,0.018442622950819672,"Federated Learning (McMahan et al., 2017) is a widely adopted technique for training models with
data from edge devices. In its plainest form, client updates are sent without encryption to the central
server at each round, and the global model is sent back to each client. However, this basic imple-
mentation of a distributed machine learning system is not robust against privacy attacks or failures.
For instance, (Wang et al., 2019; Geiping et al., 2020) show how model inversion attacks can be
successful in a federated learning setting. (Bonawitz et al., 2017; 2016) provide a protocol for se-
cure aggregation of the client updates and computation of the global model in both a semi-honest
and malicious setting, with increased runtime and communication overhead. In addition to encryp-
tion of client updates, existing work has also considered robustness to failures and Byzantine client
attacks, e.g., (Xie et al., 2019a; 2021) prune faulty client updates by assigning each update a score,
and only aggregating the updates that have the top scores. Other methods of providing byzantine
robustness including trimmed mean (Xie et al., 2019b), Krum (He et al., 2020; So et al., 2021), and
norm-based elimination (Gupta et al., 2021) have been studied. In this paper, we strengthen the re-
striction on client faults and only consider semi-honest clients. Although a semi-honest setting may
seem restrictive by forcing parties to follow the dictated protocol, it applies to settings where parties
are assumed to have no ill intentions. Furthermore, developing semi-honest protocols are a step
towards developing protocols in the malicious security model, where more expensive cryptographic
primitives such as zero-knowledge proofs are added on top of the existing semi-honest protocol."
RELATED WORK,0.020491803278688523,"Secure Multi-Party Computation (MPC) provides a way for multiple parties to compute a function
on their individual data and only reveal the ﬁnal output of the function. (Bonawitz et al., 2016; 2017)
provide a method for federated learning where client updates have a pairwise additive mask, such
that when all the encrypted client updates are summed together, the masks cancel out. In (Tueno
et al., 2019), the authors propose a rank computation system where clients send a pairwise masked
version of their data point to the central server. The central server uses pairwise comparisons in MPC
to calculate the desired ranked element in the clients’ combined data. However, pairwise compar-
isons are expensive in MPC, as additional values for each unique comparison must be generated in
advance. Furthermore, agreeing upon pairwise additive masks grows quadratically with the number
of parties."
RELATED WORK,0.022540983606557378,"The generalization of a central server to two non-colluding central servers is not a new idea in
privacy-preserving analytics and machine learning. For instance, (Corrigan-Gibbs & Boneh, 2017)
introduce a cryptographic protocol called secret-shared non-interactive proofs, and show that these
can be used to construct an efﬁcient privacy preserving system for the release of aggregate statis-
tics with guarantees on security in the malicious setting. The setting of the paper consists of a
small number of servers jointly computing aggregate statistics over private client data. (Mohassel &
Zhang, 2017) introduce efﬁcient distributed ML protocols that utilize a two server abstraction and
MPC versions of ML computations. However they do not seek to provide robustness. To our knowl-
edge, (He et al., 2020) is the ﬁrst work to propose an FL system that combines both robustness and
privacy, using private calculation of the distance-based aggregation. They also make the assumption"
RELATED WORK,0.02459016393442623,Under review as a conference paper at ICLR 2022
RELATED WORK,0.02663934426229508,"of the central server being two non-colluding central servers, and utilize client side secret sharing of
models and server side two party MPC. The two central server setting allows their protocol to scale
well with number of parties, compared to a single central server setting. However, their protocol
leaks the pairwise distance of client updates to one of the servers, which may be undesirable."
RELATED WORK,0.028688524590163935,"Our proposed approach uses the 2-Party MPC comparison protocols presented in (Rathee et al.,
2020). We also adopt a pairwise comparison based computation of the k-th ranked element proposed
in (Tueno et al., 2019) and a bucketing approximate median idea proposed in (Corrigan-Gibbs &
Boneh, 2017), and apply them to the setting of distributed machine learning. Finally, we note that
this work focuses on security and robustness of centralized model training. Existing work has shown
that personalization and mitigating data heterogeneity can be addressed by ﬁne-tuning the robust
centralized model (Li et al., 2021). These details are left for future work."
PRELIMINARIES,0.030737704918032786,"3
Preliminaries"
PRELIMINARIES,0.03278688524590164,"Federated Learning.
We denote N as the set of positive integers and R+ as the set of non-negative
real numbers. Given n ∈N, we denote [n] = {1, . . . , n}. We use bold lowercase letters, e.g.,
w ∈Rd, to denote vectors, and w(k) to denote its value at the kth dimension. We consider a two-
server multi-client FL system, let S0, S1 denote the two central severs, and ci for i ∈[n] represent
the clients, where n is the total number of clients. Client ci holds dataset Di. The agreed upon model
has d parameters. wg denotes the global model sent from servers to clients, while wi denotes the
local model held by client i. All parties agree beforehand to run r cycles of FL. The loss function
for client ci is ˆFi : Rd →R+. Usually, because clients have different data, the loss functions ˆFi are
different. Therefore, the goal of our setting is to minimize the averaged loss across all clients, i.e.,
arg minw
1
n
Pn
i=1 ˆFi(w)."
PRELIMINARIES,0.03483606557377049,"We ﬁrst detail a basic federated learning system with one central server S and n clients. Clients are
generally considered to be edge nodes and do not communicate with each other. In one iteration, the
system follows the following generic protocol:"
PRELIMINARIES,0.036885245901639344,"1. For i ∈[n], client ci receives global model wg from central server S.
2. For i ∈[n], client ci performs step(s) of Stochastic Gradient Descent of wg on its dataset
Di to obtain new local model wi. They then send this model to S.
3. Upon receiving all client updates, S aggregates them to obtain the new global model. The
new global model is computed as w′
g = 1"
PRELIMINARIES,0.0389344262295082,"n
Pn
i=1 wi.
4. S sends w′
g to all the clients."
PRELIMINARIES,0.040983606557377046,"Secure Multi-Party Computation.
Secure Multi-party Computation (MPC) (Goldreich et al.,
1987) is a cryptographic primitive which allows n parties, each with input value xi, to jointly com-
pute a mutually agreed upon function f(x1, x2 . . . xn), with the guarantee that no party learns any-
thing other than what can be inferred from its own input and the joint output."
PRELIMINARIES,0.0430327868852459,"One of the most prominent techniques used in MPC is secret sharing. A (t, n) secret sharing in-
volves splitting a secret s into n shares, then distributing a share to each party, such that any subset
of t −1 parties cannot learn anything about s, while any t or more parties can jointly reconstruct s.
In this paper, we make use of (2, 2) additive secret sharing over power-of-two sized integer rings,
which involves splitting s ∈Z2ℓinto s1, s2 ∈Z2ℓs.t. s1 + s2 = s, and individually si, i ∈{0, 1}
has a uniform random distribution. We use ⟨s⟩to denote secret sharing of s over Z2ℓ, and ⟨s⟩0, ⟨s⟩1
to denote the shares of the two parties. We use Fℓ
Comp, Fℓ
Equal to refer to the “greater than” and
“equals to” ideal functionalities respectively, i.e. where parties have access to trusted functionalities
computing the said functions and use the same to describe our protocols. To implement the pro-
tocols for Fℓ
Comp, Fℓ
Equal, we make use of protocols proposed in CrypTFlow2 (Rathee et al., 2020).
CrypTFlow2 provides a customized 2-party protocol for solving the millionaire’s, and hence also
the comparison/equality problems in Z2ℓ(more details on all this in Appendix C)."
PROBLEM DESCRIPTION,0.045081967213114756,"4
Problem Description"
PROBLEM DESCRIPTION,0.0471311475409836,"In this section, we introduce a baseline solution to secure and robust FL. Median provides well-
studied robustness against outliers compared to mean (Xie et al., 2018). In order to make our MPC
protocols for this efﬁcient, we abstract the central server in FL to two non-colluding central servers.
The servers each receive one of the (2, 2) secret shares of the client updates, and perform 2-party
based secure median computation. Figure 1 provides a representation of our model."
PROBLEM DESCRIPTION,0.04918032786885246,Under review as a conference paper at ICLR 2022
PROBLEM DESCRIPTION,0.05122950819672131,"Figure 1: Our proposed two server FL system. Hospitals compute local models wi, and secret share
them among the two servers. Servers securely compute the dimension-wise median and end up with
the global model. Each hospital receives the global model from S0, and repeats the cycle."
SECURITY THREAT MODEL,0.05327868852459016,"4.1
Security Threat Model"
SECURITY THREAT MODEL,0.055327868852459015,"We model the adversary using the semi-honest/honest-but-curious model, whereby it is assumed that
the parties controlled by the adversary will follow the protocol speciﬁcations, but try to learn more
information about other parties’ data using the protocol transcript. We also assume static corruption,
i.e., that the adversary chooses the parties to corrupt at the beginning of the protocol and cannot
change them once protocol execution starts. In addition, we assume the two servers S0, S1 do not
collude with each other. The adversary is then allowed to corrupt any arbitrary subset of the clients
and at most one of the servers - and once corrupted gets to see their internal state as well as the
transcript of their communication. Furthermore, we are concerned with the multi-silo setting, where
clients are assumed to be large institutions and thus client drop out or limitations on computational
resources are not considered."
BASELINE SOLUTION,0.05737704918032787,"4.2
Baseline solution"
BASELINE SOLUTION,0.05942622950819672,"We propose a baseline solution for computing the exact median in our two-server multi-client setting.
In order to send their updates to the servers, clients additively secret-share their updates and sends
one share to each server. Once all the clients have done so, the two servers now have a (2, 2)
additive secret share of all the clients’ updates and perform a MPC based median computation.
We take inspiration from the ideas in Tueno et al. (2019) to design a 2-party secure protocol for
median computation by breaking it up into pairwise comparisons between all elements and some
number of equality checks. In Appendix A, we provide a formal description of our algorithm in the
Fℓ
Comp, Fℓ
Equal hybrid (described in Algorithm 2), along with a correctness sketch and other details for
the same. Our ﬁnal protocol is then obtained by replacing Fℓ
Comp, Fℓ
Equal with CrypTFlow2 (Rathee
et al., 2020) based secure comparison/equality protocols (described in detail in Appendix C). We
refer to this protocol as “pairwise comparison based median” in the rest of the paper."
BASELINE SOLUTION,0.06147540983606557,"Complexity. The complexity of this protocol is at least dn(n −1)/2 calls to Fℓ
Comp, in addition to
dn calls to Fℓ
Equal. Note that the number of secure comparisons/equalities grows quadratically with
the number of clients n. In the next section, we will introduce our approximate median protocol,
whose number of secure operations is independent of n. We provide a brief summary in Appendix C
on why we use number of secure comparisons as a metric for complexity."
BUCKETING BASED MEDIAN COMPUTATION,0.06352459016393443,"5
Bucketing based median computation"
BUCKETING BASED MEDIAN COMPUTATION,0.06557377049180328,"In this section, we propose an approximate median, which provides increased computational efﬁ-
ciency, to serve as the replacement of the exact median computation. We provide an abridged version
of the protocol in Algorithm 1 and a formal description of the protocol in Algorithm 3."
BUCKETING BASED MEDIAN COMPUTATION,0.06762295081967214,"Summary of the key idea. Divide the set of possible values that client updates may take into a ﬁxed
set of buckets. Let b designate the number of buckets. Next, instead of sending actual updates to the
server, each client will send a unit vector that has a 1 in the bucket containing the client’s update, and
0 everywhere else. Upon receiving all client updates, the (two) servers build a frequency distribution
of client updates and use this to ﬁnd the bucket which contains the median value. The median is then"
BUCKETING BASED MEDIAN COMPUTATION,0.06967213114754098,Under review as a conference paper at ICLR 2022
BUCKETING BASED MEDIAN COMPUTATION,0.07172131147540983,"Algorithm 1 Succinct version of our bucketed FL system
Input: For i ∈[n], ci holds dataset Di.
d = model size, b = number of buckets, p are public parameters.
∀i ∈[n], client ci does:"
BUCKETING BASED MEDIAN COMPUTATION,0.07377049180327869,1. Receive global model wg and bucket ranges B from server S0.
BUCKETING BASED MEDIAN COMPUTATION,0.07581967213114754,"2. Compute local model wi as SGD(wg, Di)."
BUCKETING BASED MEDIAN COMPUTATION,0.0778688524590164,"3. Compute Bucketize(wg, B, b, wi) and obtain bucketized local model ei."
BUCKETING BASED MEDIAN COMPUTATION,0.07991803278688525,"4. Creates (2, 2) shares ⟨ei⟩0, ⟨ei⟩1 of ei."
BUCKETING BASED MEDIAN COMPUTATION,0.08196721311475409,"5. Send ⟨ei⟩0 to S0, send ⟨ei⟩1 to S1."
BUCKETING BASED MEDIAN COMPUTATION,0.08401639344262295,"∀j ∈{0, 1}, server Sj does:"
BUCKETING BASED MEDIAN COMPUTATION,0.0860655737704918,"1. Receive share ⟨ei⟩j from client ci, ∀i ∈[n]."
BUCKETING BASED MEDIAN COMPUTATION,0.08811475409836066,"2. Add shares ⟨e1⟩j . . . ⟨en⟩j to obtain ⟨Pn
i=1 ei⟩j."
BUCKETING BASED MEDIAN COMPUTATION,0.09016393442622951,"3. Call 2-party secure protocol Fℓ
Comp(⟨Pn
i=1 ei⟩j, ⌈n"
BUCKETING BASED MEDIAN COMPUTATION,0.09221311475409837,"2 ⌉) and obtain d-length vector m, where
m(k) indicates which bucket holds the median for dimension k ∈[d].
4. S0 calls Quantize(wg, B, b, m) and obtains new global model w′
g."
BUCKETING BASED MEDIAN COMPUTATION,0.0942622950819672,"5. S0 computes new bucket range B′ as 2∥(w′
g −wg)∥1 + p."
BUCKETING BASED MEDIAN COMPUTATION,0.09631147540983606,"6. S0 releases w′
g and B′ to all clients."
BUCKETING BASED MEDIAN COMPUTATION,0.09836065573770492,"approximated by the middle value of the range of this bucket. All this while, servers only obtain
(2, 2) additive secret shares of client vectors, and rely on MPC to perform the computation outlined
above and ﬁnd the median. We now describe these steps in additional detail."
BUCKETING BASED MEDIAN COMPUTATION,0.10040983606557377,"We “bucketize” clients’ updates by dividing each dimension of the model into b buckets. This
enables the servers to build a secret sharing of the frequency distribution of clients’ updates and ﬁnd
the bucket which contains the median value. The approximate median is then chosen as the middle
value in that bucket."
BUCKETING BASED MEDIAN COMPUTATION,0.10245901639344263,"At the beginning of a given round each client ci receives bucket range B and global model wg from
the central servers, and follow by running SGD on its dataset Di to obtain local model wi. The
bucket range implicitly ﬁxes the buckets around the global model wg, i.e. in any given dimension
k ∈[d], the b buckets are centered around wg(k) with total range of B. The ending buckets are
taken to represent all the remaining range outside B - i.e. the ﬁrst and the last buckets represent the
range (−∞, wg(k) −B"
BUCKETING BASED MEDIAN COMPUTATION,0.10450819672131148,"2 ], [wg(k) + B"
BUCKETING BASED MEDIAN COMPUTATION,0.10655737704918032,"2 , +∞). The middle b −2 buckets evenly divide the range
wg(k) ± B"
BUCKETING BASED MEDIAN COMPUTATION,0.10860655737704918,"2 . ci then creates, ∀k ∈[d], a unit vector ei(k) ∈{0, 1}b with 1 in place for the bucket
which contains wi(k); we refer to this transformation as “bucketizing”. ci then sends secret shares
of ei to both servers"
BUCKETING BASED MEDIAN COMPUTATION,0.11065573770491803,"The servers add all the client updates to get a secret-sharing of the frequency distribution of the
clients updates and compute the bucket containing the median value for each dimension. This is
done by performing a secure comparison to compare each bucket’s share of frequency against ⌈n"
BUCKETING BASED MEDIAN COMPUTATION,0.11270491803278689,"2 ⌉.
The comparison result is then revealed to learn which bucket contains the median value and the
servers take the middle value of that bucket to be the value of the new global model g′
w for that
dimension. We refer to this transformation of converting a bucketed model back to a model where
each dimension has a numerical value associated with it as “quantizing”. The bucket range is then
updated as 2∥w′
g −wg∥1 + p, where p is a parameter agreed by all parties."
BUCKETING BASED MEDIAN COMPUTATION,0.11475409836065574,"Complexity For each dimension, the servers need to only perform b calls to Fℓ
Comp - hence a total of
db calls. Compared to dn(n−1)/2 calls for pairwise comparison median computation, we note that
if b is a constant = 10, then while pairwise comparison median needs number of secure comparisons
that grows quadratically with the number of clients, the number of calls in our method is independent
of the number of clients."
BUCKETING BASED MEDIAN COMPUTATION,0.1168032786885246,Under review as a conference paper at ICLR 2022
THEORETICAL GUARANTEES,0.11885245901639344,"6
Theoretical guarantees"
THEORETICAL GUARANTEES,0.12090163934426229,"In this section, we theoretically analyze the proposed method in terms of security, robustness, and
convergence guarantees."
ROBUSTNESS AND CONVERGENCE,0.12295081967213115,"6.1
Robustness and Convergence"
ROBUSTNESS AND CONVERGENCE,0.125,"We ﬁrst introduce the notation and settings speciﬁc to the robustness and convergence analysis. The
proofs in this subsection are provided in section D in the appendix. Given a data distribution D, we
assume data samples ξ∼D are i.i.d. sampled. Data individual loss function is denoted as f(· ; ξ) :
Rd →R+, i.e., f(w; ξ) ≥0 given a parameter w ∈Rd. Based on the individual loss function, we
denote the population loss function F = Eξ∼D[f(· ; ξ)], and we assume it is differentiable. Draw N
i.i.d. samples for each of the n clients (in total nN samples): {ξi,j}i∈[n],j∈[N]. Therefore the client
empirical loss function is"
ROBUSTNESS AND CONVERGENCE,0.12704918032786885,"ˆFi = 1 N N
X"
ROBUSTNESS AND CONVERGENCE,0.1290983606557377,"j=1
f(· ; ξi,j)
for i ∈[n].
(1)"
ROBUSTNESS AND CONVERGENCE,0.13114754098360656,"The
goal
of
our
setting
is
to
minimize
the
averaged
loss
across
all
clients,
i.e.,
arg minw
1
n
Pn
i=1 ˆFi(w). In the convergence analysis, we consider gradient descent with step
size µ. The range of the buckets at iteration t is denoted as [−Bt/2, Bt/2], and the model parameter
are denoted as wt ∈Rd. As detailed in Algorithm 3, we consider the bucket range updated by"
ROBUSTNESS AND CONVERGENCE,0.13319672131147542,"B0 ←p0,
Bt ←η∥wt −wt−1∥1 + p1,
(2)"
ROBUSTNESS AND CONVERGENCE,0.13524590163934427,"where p0, p1, η > 0 are constants. There is a convex parameter set W ⊆Rd such that the pa-
rameters during gradient descent for all clients are within W. Given a function g : Rd →Rd and
p, q ∈[1, ∞], we denote the (p, q)-norm of g as ∥g∥p,q =
 R"
ROBUSTNESS AND CONVERGENCE,0.13729508196721313,"W ∥g(w)∥p
q dµ(w)
1/p . In particular,
∥g∥∞,q = supw∈W ∥g(w)∥q."
ROBUSTNESS AND CONVERGENCE,0.13934426229508196,"The coordinate-wise median operator Med(·) takes a set of vectors {vi ∈Rd}n
i=1 and outputs a
vector v ∈Rd such that for each dimension k, v(k) is the standard median of {vi(k) ∈R}n
i=1.
The bucket median is calculated as described in Algorithm 3, and we denote it as BucketMed(·).
Therefore, in the convergence analysis, we consider the global parameter update be"
ROBUSTNESS AND CONVERGENCE,0.1413934426229508,"wt+1 ←wt −BucketMed({µ∇ˆFi(wt)}i∈[n]; B, b),"
ROBUSTNESS AND CONVERGENCE,0.14344262295081966,where B is the bucket range and b is the number of buckets.
ROBUSTNESS AND CONVERGENCE,0.14549180327868852,We make the following assumptions for the robustness and convergence analysis.
ROBUSTNESS AND CONVERGENCE,0.14754098360655737,"Assumption 1 (β∞-smoothness). F is β∞-smooth. Formally, there exists β ≥0 such that for
∀w1, w2 ∈W we have"
ROBUSTNESS AND CONVERGENCE,0.14959016393442623,"∥∇F(w1) −∇F(w2)∥∞≤β∥w1 −w2∥1,"
ROBUSTNESS AND CONVERGENCE,0.15163934426229508,Note that ∥· ∥1 is the dual norm of ∥· ∥∞.
ROBUSTNESS AND CONVERGENCE,0.15368852459016394,"Assumption 2 (Weak Law of Large Number). We assume ∇F = Eξ∼D[∇f(· ; ξ)] exists and the
sample mean converges to ∇F in probability in ∥· ∥∞,∞. Formally, let ξj ∼D be i.i.d. sampled,"
ROBUSTNESS AND CONVERGENCE,0.1557377049180328,"and denote δN,ϵ := Pr
∇F −1"
ROBUSTNESS AND CONVERGENCE,0.15778688524590165,"N
PN
j=1 ∇f(· ; ξj)

∞,∞> ϵ

. We assume"
ROBUSTNESS AND CONVERGENCE,0.1598360655737705,"∀ϵ > 0 :
lim
N→∞δN,ϵ = 0."
ROBUSTNESS AND CONVERGENCE,0.16188524590163936,"Therefore, by deﬁnition, δN,ϵ1 ≥δN,ϵ2 if ϵ1 ≤ϵ2. Denote ϵN,δ = inf{ϵ : δN,ϵ ≤δ}, and we can
see that ϵN,δ1 ≥ϵN,δ2 if δ1 ≤δ2. Moreover, we have the following proposition."
ROBUSTNESS AND CONVERGENCE,0.16393442622950818,"Proposition 6.1. With Assumption 2, we have ∀δ > 0 :
limN→∞ϵN,δ = 0."
ROBUSTNESS AND CONVERGENCE,0.16598360655737704,"With the above assumptions, the following result shows that the proposed bucket median is robust to
some extent if ratio of adversarial clients is α < 1/2. We provide similar results for dimension-wise
median Med(·) in Theorem D.1 in the appendix. Note that the previous analysis on dimension-wise
median in the FL setting (Yin et al., 2018) requires α to be smaller than an easily negative value. We
prove the robustness in a new way which holds for any α < 1/2."
ROBUSTNESS AND CONVERGENCE,0.1680327868852459,Under review as a conference paper at ICLR 2022
ROBUSTNESS AND CONVERGENCE,0.17008196721311475,"Theorem 6.1 (Robustness). With Assumption 1&2, consider n1 i.i.d. samples {∇ˆFi}n1
i=1 (equa-
tion 1) and n2 adversarial vectors {vi ∈Rd}n2
i=1. Denote n := n1 + n2 and α := n2/n being
the ratio of adversaries. Given the bucket range B > 0, the number of buckets b and the gradient
descent step size µ. If α < 1/2 then for ∀ϵ > ϵN, 1−2α"
ROBUSTNESS AND CONVERGENCE,0.1721311475409836,"2(1−α) with probability at least 1 −(δN,ϵ)
n
2 (1−2α),
we have
∀{vi ∈Rd}n2
i=1 : ∥BucketMed({µ∇ˆFi(w)}n1
i=1 ∪{vi}n2
i=1; B, b) −µ∇F(w)∥∞≤µϵ + B"
ROBUSTNESS AND CONVERGENCE,0.17418032786885246,"2b,
for ∀w ∈W satisfying µ∥∇ˆFi(w)∥∞≤B for all i ∈[n1]. With the bucket median being robust,
we can prove the convergence guarantee for smooth (possibly non-convex) function."
ROBUSTNESS AND CONVERGENCE,0.1762295081967213,"Note that as N →∞, both δN,ϵ →0 and ϵN,δ →0.
Theorem 6.2 (Robust Convergence). With Assumption 1&2, consider n1 normal clients with i.i.d.
samples {∇ˆFi}n1
i=1 (equation 1) and n2 faulty clients that can send arbitrary vectors following the
proposed protocol. Denote n := n1 +n2 and α := n2/n being the ratio of faulty clients. The bucket
range adaption is deﬁned by equation 2, where we assume Bt ≤B for all iteration t for a constant
B > 0, and p0 ≥
1
mβ ∥∇ˆFi(w0)∥∞for ∀i ∈[n1]. If α < 1/2, for ∀ϵ > ϵN, 1−2α"
ROBUSTNESS AND CONVERGENCE,0.17827868852459017,"2(1−α) and ∀ϵ′ > 0, the"
ROBUSTNESS AND CONVERGENCE,0.18032786885245902,"proposed method with parameters µ =
1
dβ , p1 ≥2µϵ + B"
ROBUSTNESS AND CONVERGENCE,0.18237704918032788,"2b, η ≥1 + βµ and b ≥dBβ"
ROBUSTNESS AND CONVERGENCE,0.18442622950819673,2ϵ′ can achieve
ROBUSTNESS AND CONVERGENCE,0.1864754098360656,"min
0≤t≤T
1
m∥∇F(wt)∥2
2 ≤2βF(w0)"
ROBUSTNESS AND CONVERGENCE,0.1885245901639344,"T + 1
+ (ϵ + ϵ′)2,"
ROBUSTNESS AND CONVERGENCE,0.19057377049180327,"with probability at least 1 −(δN,ϵ)
n
2 (1−2α)."
ROBUSTNESS AND CONVERGENCE,0.19262295081967212,"Thus, with sufﬁcient number of buckets b and number of data N, the proposed method can converge
w.r.t. the true loss function F with high probability, while α < 1/2 of clients send incorrect local
updates."
OVERVIEW OF SECURITY ANALYSIS,0.19467213114754098,"6.2
Overview of Security Analysis"
OVERVIEW OF SECURITY ANALYSIS,0.19672131147540983,"We argue the security of our bucketed protocol by means of the standard real/ideal simulation
paradigm in cryptography (deﬁned formally in Appendix B). As stated in section 4.1, we assume a
semi-honest adversary statically corrupting up to n −1 clients and at most one server, and prove the
security of one epoch of the FL training using the simulation paradigm. The security of the entire
protocol then follows by applying the sequential composition theorem to the security of each epoch."
OVERVIEW OF SECURITY ANALYSIS,0.1987704918032787,"We provide a brief overview of the security argument. The use of secure 2-party computation be-
tween the servers allows them to compute the desired approximate median without revealing any-
thing other than the input/output of the functionality. At the beginning of each round, the clients
secret-share their inputs between the two servers and thereafter, the two servers use secure 2-party
protocols to compute the shares of a unit vector with 1 for the bucket in which the median value lies.
Revealing this unit vector allows the servers to learn the median bucket index and ﬁnd the median
value, which is then sent to clients. An adversary corrupting one of the servers only sees random
shares throughout the protocol (guaranteed by the secure protocol implementing Fℓ
Comp, Fℓ
Equal, de-
scribed in Appendix C) in addition to bucket range (which is computed solely using public values)
and median bucket index, which it sees in the clear. Additionally corrupting any number of clients
does not allow an adversary to recover any honest client’s information, since honest client’s data are
only secret-shared between the servers and not shared in plain between clients."
OVERVIEW OF SECURITY ANALYSIS,0.20081967213114754,We revisit the simulation paradigm and provide the formal security proof in Appendix B.
EXPERIMENTAL RESULTS,0.2028688524590164,"7
Experimental Results"
EXPERIMENTAL RESULTS,0.20491803278688525,"In this section, we evaluate our proposed bucketed median scheme on different models and datasets.
First, we compare convergence behaviour over epoch of FL of bucketed median, pairwise compari-
son median, and mean methods. We also detail the quadratic versus linear relationship the pairwise
comparison and bucketed median methods respectively have with the number of clients. In the next
subsection, we formally describe the failures that faulty clients can exhibit. We then compare the
robustness of median and our bucketed median against mean. In Appendix F, we detail a breakdown
of one FL cycle and analyze sensitivity of parameters."
EXPERIMENTAL RESULTS,0.2069672131147541,"Experiments were conducted on an Intel(R) Xeon(R) Platinum 8280 CPU @ 2.70GHz virtual ma-
chine with 112 cores. All parties involved in the FL process were separate processes on this machine,"
EXPERIMENTAL RESULTS,0.20901639344262296,Under review as a conference paper at ICLR 2022
EXPERIMENTAL RESULTS,0.21106557377049182,"(a) Time taken per number of clients
(b) Communication cost in megabytes per
number of clients
Figure 3: Single-threaded median computation over 1000 dimensions."
EXPERIMENTAL RESULTS,0.21311475409836064,"(a) MLP, bitﬂip failure
(b) MLP, label ﬂip failure
(c) MLP, Gaussian noise failure
Figure 4: Training loss convergence behaviour for 3 clients FL system using different server-side
methods."
EXPERIMENTAL RESULTS,0.2151639344262295,"with a bandwidth cap of 377 MBps to simulate a LAN network. We test on the following combina-
tions of (model, dataset): (MLP, MNIST), (CNNMnist, Mnist), and (CNN, CIFAR-10). The MLP
model has consits of 1 hidden layer with 200 units with ReLu activation for 199, 210 dimensions.
We use the same CNNMnist as McMahan et al. (2017), which has 1,663,370 dimensions. The CNN
model we use consists of two 5 × 5 convolution layers, the ﬁrst followed by a 2 × 2 max pooling,
and the second followed by three linear layers for a total of 581,866 dimensions. Model training was
done using the CPU. Training batch size is set to 20, learning rate is set to 0.01, and client datasets
are sampled i.i.d. from the global dataset. Number of buckets b = 8."
CONVERGENCE RESULTS,0.21721311475409835,"7.1
Convergence Results"
CONVERGENCE RESULTS,0.2192622950819672,"We ﬁrst test convergence behaviour when all parties are benign. We show that our bucketed median
FL system exhibits similar training loss convergence as our baseline: a mean based FL system. For
all three methods, we run the same number of iterations of FL and note the average training loss
when all clients are benign."
CONVERGENCE RESULTS,0.22131147540983606,"(a) 20 iter. of (MLP, MNIST)
(b) 10 iter. of (CNN, CIFAR-10)
(c) 10 iter. of (CNNMnist, MNIST)
Figure 2: Results of training loss convergence on 3 benign parties."
CONVERGENCE RESULTS,0.22336065573770492,"We also show the relationship between number of parties and the time taken for secure median
computation in both the bucketing and pairwise median methods. We test for number of clients
n ∈[2, 10] for dimension-wise median when d = 1000. Figure 3 depicts the constant and quadratic
relationships between time and communication costs with number of clients for both bucketing and
pairwise methods. In Appendix F, we provide scalability results with larger number of clients."
CONVERGENCE RESULTS,0.22540983606557377,Under review as a conference paper at ICLR 2022
CONVERGENCE RESULTS,0.22745901639344263,"(a) CNN, bitﬂip failure
(b) CNN, label ﬂip failure
(c) CNN, Gaussian noise failure
Figure 5: Training loss convergence behaviour for 3 clients FL system using different server-side
methods."
CONVERGENCE RESULTS,0.22950819672131148,"(a) CNNMnist, bitﬂip failure
(b) CNNMnist, label ﬂip failure
(c) CNNMnist, Gaussian noise fail-
ure
Figure 6: Training loss convergence behaviour for 3 clients FL system using different server-side
methods."
CLIENT FAILURE TYPES,0.23155737704918034,"7.2
Client Failure Types"
CLIENT FAILURE TYPES,0.2336065573770492,"We assume the updates sent from clients can be corrupted with errors arising from machine noise,
network transmission errors, data poisoning etc. The failures we test with are:"
CLIENT FAILURE TYPES,0.23565573770491804,"• Bit Flip Failure: The most signiﬁcant bit, or the bit that controls the sign is ﬂipped. The client
sends the negative gradient instead of the true gradient to the servers.
• Label Switching Failure: Clients compute gradients with switched labels on the training data.
For example, label ∈{0, ..., 9} is replaced by 9 −label.
• Gaussian Noise Failure: Instead of sending the correct gradient to the server, clients send an
update with values sampled from a Gaussian distribution with mean of zero and standard deviation
of 200."
ROBUSTNESS RESULTS,0.23770491803278687,"7.3
Robustness Results"
ROBUSTNESS RESULTS,0.23975409836065573,"In this subsection, we empirically show that the robustness of our bucketed median FL system
behaves similarly to a median based FL system. We show that for bit-ﬂip, label-ﬂip, and Gaussian
noise errors, our system still converges. We test with three clients, one of which exhibits failures
starting from the third epoch. Figures 4, 5, 6 shows results of our bucketed median method versus
a median and mean method. In our implementation, the faulty client performs label ﬂipping during
the training process, while bitﬂip and Gaussian noise are manually computed on the local model
returned from model training. The mean method results in loss of convergence for both bitﬂip and
Gaussian noise failures, but performs well against label ﬂip failures. However, both the median and
bucketed median methods show continued training loss decrease against all three failures."
CONCLUSION,0.24180327868852458,"8
Conclusion"
CONCLUSION,0.24385245901639344,"We presented a dual central-server FL system that is robust and private in the semi-honest setting.
We propose a bucketing approximation technique in order to sacriﬁce accuracy for computational
efﬁciency, which also leads to better scalability with the number of clients. The advantage our work
provides are both privacy and robustness in distributed ML."
CONCLUSION,0.2459016393442623,Under review as a conference paper at ICLR 2022
ETHICS STATEMENT,0.24795081967213115,Ethics Statement
ETHICS STATEMENT,0.25,"Our work focuses on theoretically and empirically studying robust federated learning.
All the
datasets and packages we use are open-sourced. Potential ethical concerns are primarily in the
observation that robust federated learning tends to remove outliers, and it can be hard to determine
if data from minority groups are outliers or adversarial."
REPRODUCIBILITY STATEMENT,0.2520491803278688,Reproducibility Statement
REPRODUCIBILITY STATEMENT,0.2540983606557377,"We have tried our best to provide training details to facilitate reproducing our results. We pro-
vide detailed results on how to train and evaluate our model. We will open-source our code once
accepted."
REFERENCES,0.25614754098360654,References
REFERENCES,0.2581967213114754,"Cryptﬂow2: Practical 2-Party Secure Inference. https://github.com/mpc-msri/EzPC/
tree/master/SCI, 2021."
REFERENCES,0.26024590163934425,"K. A. Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H. Brendan McMahan, Sarvar
Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for federated
learning on user-held data. In NIPS Workshop on Private Multi-Party Machine Learning, 2016.
URL https://arxiv.org/abs/1611.04482."
REFERENCES,0.26229508196721313,"Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H. Brendan McMahan, Sarvar
Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for privacy-
preserving machine learning. In Proceedings of the 2017 ACM SIGSAC Conference on Computer
and Communications Security, CCS ’17, pp. 1175–1191, New York, NY, USA, 2017. Associa-
tion for Computing Machinery. ISBN 9781450349468. doi: 10.1145/3133956.3133982. URL
https://doi.org/10.1145/3133956.3133982."
REFERENCES,0.26434426229508196,"Henry Corrigan-Gibbs and Dan Boneh.
Prio:
Private, robust, and scalable computation of
aggregate statistics.
In 14th USENIX Symposium on Networked Systems Design and Im-
plementation (NSDI 17), pp. 259–282, Boston, MA, March 2017. USENIX Association.
ISBN 978-1-931971-37-9.
URL https://www.usenix.org/conference/nsdi17/
technical-sessions/presentation/corrigan-gibbs."
REFERENCES,0.26639344262295084,"Ivan Damg˚ard, Matthias Fitzi, Eike Kiltz, Jesper Buus Nielsen, and Tomas Toft. Unconditionally
secure constant-rounds multi-party computation for equality, comparison, bits and exponentiation.
In Shai Halevi and Tal Rabin (eds.), Theory of Cryptography, pp. 285–304, Berlin, Heidelberg,
2006. Springer Berlin Heidelberg. ISBN 978-3-540-32732-5."
REFERENCES,0.26844262295081966,"Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit conﬁ-
dence information and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC Confer-
ence on Computer and Communications Security, CCS ’15, pp. 1322–1333, New York, NY, USA,
2015. Association for Computing Machinery. ISBN 9781450338325. doi: 10.1145/2810103.
2813677. URL https://doi.org/10.1145/2810103.2813677."
REFERENCES,0.27049180327868855,"Jonas Geiping, Hartmut Bauermeister, Hannah Dr¨oge, and Michael Moeller. Inverting gradients –
how easy is it to break privacy in federated learning?, 2020."
REFERENCES,0.2725409836065574,"O. Goldreich, S. Micali, and A. Wigderson. How to play any mental game. In Proceedings of
the Nineteenth Annual ACM Symposium on Theory of Computing, STOC ’87, pp. 218–229, New
York, NY, USA, 1987. Association for Computing Machinery. ISBN 0897912217. doi: 10.1145/
28395.28420. URL https://doi.org/10.1145/28395.28420."
REFERENCES,0.27459016393442626,"Oded Goldreich. The Foundations of Cryptography - Volume 2: Basic Applications. Cambridge
University Press, 2004. ISBN 0-521-83084-2. doi: 10.1017/CBO9780511721656. URL http:
//www.wisdom.weizmann.ac.il/%7Eoded/foc-vol2.html."
REFERENCES,0.2766393442622951,"Nirupam Gupta, Shuo Liu, and Nitin H. Vaidya. Byzantine fault-tolerant distributed machine learn-
ing using stochastic gradient descent (sgd) and norm-based comparative gradient elimination
(cge), 2021."
REFERENCES,0.2786885245901639,Under review as a conference paper at ICLR 2022
REFERENCES,0.2807377049180328,"Lie He, Sai Praneeth Karimireddy, and Martin Jaggi. Secure byzantine-robust machine learning,
2020."
REFERENCES,0.2827868852459016,"Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aur´elien Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Kallista A. Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael
G. L. D’Oliveira, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adri`a
Gasc´on, Badih Ghazi, Phillip B. Gibbons, Marco Gruteser, Za¨ıd Harchaoui, Chaoyang He, Lie
He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail
Khodak, Jakub Koneˇcn´y, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo, Tancr`ede
Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer ¨Ozg¨ur, Rasmus Pagh,
Mariana Raykova, Hang Qi, Daniel Ramage, Ramesh Raskar, Dawn Song, Weikang Song, Se-
bastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tram`er, Praneeth Vepakomma,
Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao.
Ad-
vances and open problems in federated learning. CoRR, abs/1912.04977, 2019. URL http:
//arxiv.org/abs/1912.04977."
REFERENCES,0.2848360655737705,"Nishant Kumar, Mayank Rathee, Nishanth Chandran, Divya Gupta, Aseem Rastogi, and Rahul
Sharma. Cryptﬂow: Secure tensorﬂow inference. In 2020 IEEE Symposium on Security and
Privacy (SP), pp. 336–353. IEEE, 2020."
REFERENCES,0.28688524590163933,"Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated
learning through personalization, 2021."
REFERENCES,0.2889344262295082,"H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Ag¨uera y Arcas.
Communication-efﬁcient learning of deep networks from decentralized data, 2017."
REFERENCES,0.29098360655737704,"Payman Mohassel and Peter Rindal. Aby3: A mixed protocol framework for machine learning. In
Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security,
pp. 35–52, 2018."
REFERENCES,0.2930327868852459,"Payman Mohassel and Yupeng Zhang. Secureml: A system for scalable privacy-preserving machine
learning. In 2017 IEEE Symposium on Security and Privacy (SP), pp. 19–38, 2017. doi: 10.1109/
SP.2017.12."
REFERENCES,0.29508196721311475,"Takashi Nishide and Kazuo Ohta. Multiparty computation for interval, equality, and comparison
without bit-decomposition protocol. In Tatsuaki Okamoto and Xiaoyun Wang (eds.), Public Key
Cryptography – PKC 2007, pp. 343–360, Berlin, Heidelberg, 2007. Springer Berlin Heidelberg.
ISBN 978-3-540-71677-8."
REFERENCES,0.29713114754098363,"Deevashwer Rathee, Mayank Rathee, Nishant Kumar, Nishanth Chandran, Divya Gupta, Aseem
Rastogi, and Rahul Sharma. Cryptﬂow2: Practical 2-party secure inference. Proceedings of
the 2020 ACM SIGSAC Conference on Computer and Communications Security, Oct 2020. doi:
10.1145/3372297.3417274. URL http://dx.doi.org/10.1145/3372297.3417274."
REFERENCES,0.29918032786885246,"Jinhyun So, Basak Guler, and A. Salman Avestimehr. Byzantine-resilient secure federated learning,
2021."
REFERENCES,0.3012295081967213,"Anselme Tueno, Florian Kerschbaum, Stefan Katzenbeisser, Yordan Boev, and Mubashir Qureshi.
Secure computation of the kth-ranked element in a star network, 2019."
REFERENCES,0.30327868852459017,"Zhibo Wang, Song Mengkai, Zhifei Zhang, Yang Song, Qian Wang, and Hairong Qi.
Beyond
inferring class representatives: User-level privacy leakage from federated learning. pp. 2512–
2520, 04 2019. doi: 10.1109/INFOCOM.2019.8737416."
REFERENCES,0.305327868852459,"Cong Xie, Oluwasanmi Koyejo, and Indranil Gupta. Generalized byzantine-tolerant sgd, 2018."
REFERENCES,0.3073770491803279,"Cong Xie, Sanmi Koyejo, and Indranil Gupta.
Zeno: Distributed stochastic gradient descent
with suspicion-based fault-tolerance. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.),
Proceedings of the 36th International Conference on Machine Learning, volume 97 of Pro-
ceedings of Machine Learning Research, pp. 6893–6901. PMLR, 09–15 Jun 2019a.
URL
http://proceedings.mlr.press/v97/xie19b.html."
REFERENCES,0.3094262295081967,"Cong Xie, Sanmi Koyejo, and Indranil Gupta. Slsgd: Secure and efﬁcient distributed on-device
machine learning, 2019b."
REFERENCES,0.3114754098360656,Under review as a conference paper at ICLR 2022
REFERENCES,0.3135245901639344,"Cong Xie, Oluwasanmi Koyejo, and Indranil Gupta. Fall of empires: Breaking byzantine-tolerant
sgd by inner product manipulation. In Uncertainty in Artiﬁcial Intelligence, pp. 261–270. PMLR,
2020."
REFERENCES,0.3155737704918033,"Cong Xie, Sanmi Koyejo, and Indranil Gupta. Zeno++: Robust fully asynchronous sgd, 2021."
REFERENCES,0.3176229508196721,"Andrew Chi-Chih Yao. How to generate and exchange secrets (extended abstract). In 27th Annual
Symposium on Foundations of Computer Science, Toronto, Canada, 27-29 October 1986, pp.
162–167. IEEE Computer Society, 1986. doi: 10.1109/SFCS.1986.25. URL https://doi.
org/10.1109/SFCS.1986.25."
REFERENCES,0.319672131147541,"Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Byzantine-robust distributed
learning: Towards optimal statistical rates. In International Conference on Machine Learning,
pp. 5650–5659. PMLR, 2018."
REFERENCES,0.32172131147540983,Under review as a conference paper at ICLR 2022
REFERENCES,0.3237704918032787,"A
Full Description of our algorithms"
REFERENCES,0.32581967213114754,"Here we provide full descriptions of both our median based and approximate bucketed median based
FL algorithms (Algorithm 2, Algorithm 3), along with more details and correctness sketch for the
same."
REFERENCES,0.32786885245901637,"A.1
Median based FL system"
REFERENCES,0.32991803278688525,This is formally speciﬁed in full in Algorithm 2.
REFERENCES,0.3319672131147541,"Correctness sketch.
We sketch the idea behind the protocol, which is inspired from similar ideas
used in (Tueno et al., 2019). To compute the median of n values - {x1, x2 . . . xn}, we ﬁrst com-
pute all pairwise comparison results - bij = (xi > xj), ∀i, j ∈[n], i ̸= j.
This allows us
to compute for each element xi, its position in the sorted list of {x1, x2 . . . xn}, by computing
posi = P"
REFERENCES,0.33401639344262296,"k∈[n],k̸=i bik. Finally, we ﬁnd element for which posi = ⌈n/2⌉."
REFERENCES,0.3360655737704918,"In our actual computation, we only compute bij, ∀i, j ∈[n] s.t. j > i. Then bji is taken as 1 −bij.
The correctness of this median depends on all the values being different, which we assume to happen
in our protocol (and abort if this is not true)."
REFERENCES,0.33811475409836067,"Float-to-ﬁxed conversion.
Our protocol for exact median involves mapping the client’s local
ﬂoating point values to integers, since it is well known that most cryptographic protocols are much
more efﬁcient when working over integer rings than directly over ﬂoating point values (Mohassel
& Zhang, 2017; Mohassel & Rindal, 2018; Kumar et al., 2020). Previous work in this area (Mo-
hassel & Zhang, 2017; Mohassel & Rindal, 2018; Kumar et al., 2020), has made use of ﬁxed point
representation for the same, where real number r is mapped to integer ρr = ⌊r ∗2s⌋, where s is
a parameter called the scaling factor. Hence, given a real number r, we map it to its ﬁxed point
representation ρr, which is then mapped to an integer ring like Z2ℓ. In the following, we use wiInt
to denote ﬁxed point version of wi."
REFERENCES,0.3401639344262295,"F2Iℓ,s and I2Fℓ,s are functions which are used for the mapping real numbers to (ﬁxed-point) integers
and vice versa respectively. In particular, F2Iℓ,s : R →Z2ℓ, is deﬁned as F2Iℓ,s(x) = ⌊x ∗2s⌋.
I2Fℓ,s : Z2ℓ→R is deﬁned as I2Fℓ,s(x) = x/2s (where x ∈Z2ℓis ﬁrst interpreted as a signed
integer and then the division is performed)."
REFERENCES,0.3422131147540984,"Since we use ℓ= 64, s = 24 and 32-bit ﬂoating point values throughout and which have only 23
bits of precision (and since we are only performing addition over the ﬁxed point values/the values
involved at any intermediate stage are small) we don’t suffer any accuracy loss/convergence rate loss
due to the usage of ﬁxed point values."
REFERENCES,0.3442622950819672,"A.2
Bucketed median based FL"
REFERENCES,0.3463114754098361,In Algorithm 3 we describe in full our algorithm to compute the bucketed median.
REFERENCES,0.3483606557377049,Under review as a conference paper at ICLR 2022
REFERENCES,0.35040983606557374,"Algorithm 2 One training epoch of exact median based FL
Input: ∀i ∈[n], ci holds dataset Di
Parameters: Bitlength ℓ, ﬁxed point scale s
∀i ∈[n], client ci does:"
REFERENCES,0.3524590163934426,"1. Receive ﬁxed-point shares of global model ⟨wInt
g ⟩j ∈Z2ℓfrom server Sj, ∀j ∈{0, 1}."
COMPUTE WINT,0.35450819672131145,"2. Compute wInt
g = ⟨wInt
g ⟩0 + ⟨wInt
g ⟩1 and wg = I2Fℓ,s(wInt
g )."
COMPUTE WINT,0.35655737704918034,"3. Compute local model wi = SGD(wg, Di)."
COMPUTE WINT,0.35860655737704916,"4. Convert back to ﬁxed point as wInt
i
= F2Iℓ,s(wi)."
COMPUTE WINT,0.36065573770491804,"5. Create fresh (2, 2) secret shares of wInt
i
by choosing ⟨wInt
i ⟩0 at random from Zd
2ℓand com-
puting ⟨wInt
i ⟩1 = wInt
i −⟨wInt
i ⟩0."
COMPUTE WINT,0.36270491803278687,"6. ∀j ∈{0, 1}, send ⟨wInt
i ⟩j to server Sj."
COMPUTE WINT,0.36475409836065575,"∀j ∈{0, 1}, server Sj performs the following:"
COMPUTE WINT,0.3668032786885246,"1. ∀i ∈[n], receive share ⟨wInt
i ⟩j from client ci."
COMPUTE WINT,0.36885245901639346,"2. ∀k ∈[d], compute shares of median in dimension j across all the n clients as follows:"
COMPUTE WINT,0.3709016393442623,"(a) ∀x ∈[n], y ∈[x + 1, n], compute ⟨zk
x,y⟩j where zk
x,y = (wInt
x (k) > wInt
y (k)) by
calling Fℓ
Comp with inputs ⟨wInt
x (k)⟩j, ⟨wInt
y (k)⟩j. Set ⟨zk
y,x⟩j = j −⟨zk
x,y⟩j."
COMPUTE WINT,0.3729508196721312,"(b) ∀i ∈[n], compute ⟨posi⟩j = P
x∈[n],x̸=i⟨zk
i,x⟩j."
COMPUTE WINT,0.375,"(c) Set mid = (n + 1)/2 if n is odd, else set mid = n/2.
(d) ∀i ∈[n], check if posi = mid by calling Fℓ
Equal with inputs ⟨posi⟩j, j ∗mid and
receiving output ⟨eqi⟩j in return.
(e) ∀i ∈[n], send ⟨eqi⟩j to server S1−j and compute eqi = ⟨eqi⟩0 + ⟨eqi⟩1."
COMPUTE WINT,0.3770491803278688,"(f) Find i ∈[n] s.t. eqi(k) = 1 and set ⟨wInt
g
′(k)⟩j = ⟨wInt
i (k)⟩j."
COMPUTE WINT,0.3790983606557377,"3. Send ⟨wInt
g
′⟩j to each client ci, ∀i ∈[n]."
COMPUTE WINT,0.38114754098360654,Under review as a conference paper at ICLR 2022
COMPUTE WINT,0.3831967213114754,"Algorithm 3 One training epoch of bucketed median based FL
Input: ∀i ∈[n], clients ci holds dataset Di
Parameters: Number of buckets b, bitlength ℓ, total epochs r, constant values p0, p1, pt = p1"
COMPUTE WINT,0.38524590163934425,"t , where
t is the current epoch number.
[h] ∀i ∈[n], client ci does:"
COMPUTE WINT,0.38729508196721313,1. Receive global model wg and bucket range B from server S0.
COMPUTE WINT,0.38934426229508196,"2. Compute local model wi = SGD(wg, Di)."
COMPUTE WINT,0.39139344262295084,"3. Bucketize wi to get ei ∈{0, 1}d×b. Speciﬁcally, for each dimension k in [d], compute the
kth row of bucketed model ei as follows:"
COMPUTE WINT,0.39344262295081966,• If wi(k) ≤wg(k) −B
COMPUTE WINT,0.39549180327868855,"2 , set ei(k) as [1, 0, . . . , 0]."
COMPUTE WINT,0.3975409836065574,• If wi(k) ≥wg(k) + B
COMPUTE WINT,0.39959016393442626,"2 , set ei(k) as [0, . . . , 0, 1]."
COMPUTE WINT,0.4016393442622951,"• Else, let z = ⌊wi(k)−(wg(k)−B/2)"
COMPUTE WINT,0.4036885245901639,"B/(b−2)
⌋+ 1. Set ei(k) as [0, . . . , 0, 1, 0, . . . , 0], where
ei(k, z) is set to 1."
COMPUTE WINT,0.4057377049180328,"4. Create (2, 2) secret shares of ei by choosing ⟨ei⟩0 at random from Zd×b
2ℓ
and computing
⟨ei⟩1 = ei −⟨ei⟩0."
COMPUTE WINT,0.4077868852459016,"5. ∀j ∈{0, 1}, send ⟨ei⟩j to server Sj."
COMPUTE WINT,0.4098360655737705,"∀j ∈{0, 1}, server Sj performs the following:"
COMPUTE WINT,0.41188524590163933,"1. ∀i ∈[n], receive share ⟨ei⟩j from client ci."
COMPUTE WINT,0.4139344262295082,2. Set ⟨h⟩j = P
COMPUTE WINT,0.41598360655737704,i∈[n]⟨ei⟩j.
COMPUTE WINT,0.4180327868852459,"3. For each dimension k in [d], calculate the approximate median:"
COMPUTE WINT,0.42008196721311475,"(a) ∀y ∈[2, b], set ⟨h(k, y)⟩j = P"
COMPUTE WINT,0.42213114754098363,"z∈[y]⟨h(k, z)⟩j.
(b) Find the bucket that contains the median:"
COMPUTE WINT,0.42418032786885246,"i. Compute ⟨m(k)⟩j, where m(k) is of form [0, . . . , 1, . . . , 0], by calling Fℓ
Comp
with S0’s input being ⟨h(k)⟩0, [⌈n"
COMPUTE WINT,0.4262295081967213,"2 ⌉]d×b and S1’s input being ⟨h(k)⟩1, [0]d×b. Set
⟨m(k)⟩j = [j]b −⟨m(k)⟩j.
ii. Sj shares ⟨m⟩j with S1−j.
iii. S0 computes m = ⟨m(k)⟩0
L⟨m(k)⟩1.
iv. S0 selects the bucket y that contains the median as miny∈[0,b−1])[m(k, y) ≡1].
(c) S0 quantizes the approximate median value w′
g(k) as follows:"
COMPUTE WINT,0.42827868852459017,"• If y == 0, w′
g(k) = wg(k) −B"
COMPUTE WINT,0.430327868852459,"2
• If y == b −1, w′
g(k) = wg(k) + B"
COMPUTE WINT,0.4323770491803279,"2
• Else, w′
g(k) =

(wg(k) −B"
COMPUTE WINT,0.4344262295081967,2 + (y −1) ∗( B
COMPUTE WINT,0.4364754098360656,b−2)) + (wg(k) −B
COMPUTE WINT,0.4385245901639344,2 + y ∗( B
COMPUTE WINT,0.4405737704918033,"b−2))

/2."
COMPUTE WINT,0.4426229508196721,"4. S0 creates new bucket range B′ = 2∥w′
g −wg∥1 + p1,t, where p1,t is predetermined."
COMPUTE WINT,0.444672131147541,"5. S0 sends w′
g, B′ to client ci, ∀i ∈[n]."
COMPUTE WINT,0.44672131147540983,"To initialize FL, server S0 performs the following:"
COMPUTE WINT,0.4487704918032787,1. Initialize wg as model with parameter values of 0.
COMPUTE WINT,0.45081967213114754,2. Set B = p0.
COMPUTE WINT,0.45286885245901637,"3. S0 sends wg, B to each client ci, ∀i ∈[n]."
COMPUTE WINT,0.45491803278688525,"B
Formal security Proof"
COMPUTE WINT,0.4569672131147541,"In this work, we use the semi-honest or honest-but-curious adversarial model, where it is assumed
that the adversary follows the protocol speciﬁcations, but tries to glean information about the other
party’s inputs using the protocol transcript. As stated previously, we assume a semi-honest adver-
sary, statically (i.e. before starting the protocol) corrupting up to n−1 clients and one of the servers,
and prove full security of the protocol using the simulation paradigm."
COMPUTE WINT,0.45901639344262296,Under review as a conference paper at ICLR 2022
COMPUTE WINT,0.4610655737704918,"B.1
Deﬁning Secure Multiparty Computation"
COMPUTE WINT,0.46311475409836067,"Here, we provide a formal deﬁnition of secure multiparty computation. Parts of this section have
been borrowed verbatim from (Goldreich, 2004)."
COMPUTE WINT,0.4651639344262295,"A multi-party protocol is cast by specifying a random process that maps pairs of inputs to pairs of
outputs (one for each party). We refer to such a process as a functionality. The security of a protocol
is deﬁned with respect to a functionality f. In particular, let n denote the number of parties. A
non-reactive n-party functionality f is a (possibly randomized) mapping of n inputs to n outputs. A
multiparty protocol with security parameter λ for computing a non-reactive functionality f is a pro-
tocol running in time polynomial in λ and satisfying the following correctness requirement: if parties
P1, . . . , Pn with inputs (x1, . . . , xn) respectively all run an honest execution of the protocol, then
the joint distribution of the outputs y1, . . . , yn of the parties is statistically close to f(x1, . . . , xn).
A reactive functionality f is a sequence of non-reactive functionalities f = (f1, . . . , fℓ) computed
in a stateful fashion in a series of phases. Let xj
i denote the input of Pi in phase j, and let sj denote
the state of the computation after phase j. Computation of f proceeds by setting s0 equal to the
empty string and then computing (yj
1, . . . , yj
n, sj) ←fj(sj−1, xj
1, . . . , xj
n) for j ∈[ℓ], where yj
i
denotes the output of Pi at the end of phase j. A multi-party protocol computing f also runs in
ℓphases, at the beginning of which each party holds an input and at the end of which each party
obtains an output. (Note that parties may wait to decide on their phase-j input until the beginning of
that phase.) Parties maintain state throughout the entire execution. The correctness requirement is
that, in an honest execution of the protocol, the joint distribution of all the outputs {yj
1, . . . , yj
n}ℓ
j=1
of all the phases is statistically close to the joint distribution of all the outputs of all the phases in a
computation of f on the same inputs used by the parties."
COMPUTE WINT,0.4672131147540984,"Deﬁning Security.
The security of a protocol (with respect to a functionality f) is deﬁned by
comparing the real-world execution of the protocol with an ideal-world evaluation of f by a trusted
party. More concretely, it is required that for every adversary A, which attacks the real execution
of the protocol, there exist an adversary Sim, also referred to as a simulator, which can achieve the
same effect in the ideal-world. Next, we specify what each of these worlds mean."
COMPUTE WINT,0.4692622950819672,"The real execution
In the real execution of the n-party protocol π for computing f is executed in
the presence of an adversary A. The honest parties follow the instructions of π. The adversary A
takes as input the security parameter k, the set I ⊂[n] of corrupted parties, the inputs of the cor-
rupted parties, and an auxiliary input z. A obtains the local randomness and the state of all corrupted
parties, but all corrupted parties do follow the strategy outlined in the protocol speciﬁcation."
COMPUTE WINT,0.4713114754098361,"The interaction of A with a protocol π deﬁnes a random variable REALπ,A(z),I(λ, ⃗x) whose value is
determined by the coin tosses of the adversary and the honest players. This random variable contains
the output of the adversary (which may be an arbitrary function of its view, which includes the state
of all corrupted parties) as well as the outputs of the uncorrupted parties. We let REALπ,A(z),I
denote the distribution ensemble {REALΠ,A(z),I(λ, ⃗x)}k∈N,⟨⃗x,z⟩∈{0,1}∗."
COMPUTE WINT,0.4733606557377049,"The ideal execution – security with abort.
In this second variant of the ideal model, fairness and
output delivery are no longer guaranteed. This is the standard relaxation used when a strict majority
of honest parties is not assumed. In this case, an ideal execution for a function f proceeds as follows:"
COMPUTE WINT,0.47540983606557374,"• Send inputs to the trusted party: As before, the parties send their inputs to the trusted
party, and we let x′
i denote the value sent by Pi. Once again, for a semi-honest adversary
we require x′
i = xi for all i ∈I."
COMPUTE WINT,0.4774590163934426,"• Trusted party sends output to the adversary:
The trusted party computes
f(x′
1, . . . , x′
n) = (y1, . . . , yn) and sends {yi}i∈I to the adversary."
COMPUTE WINT,0.47950819672131145,"• Adversary instructs trusted party to abort or continue: This is formalized by having
the adversary send either a continue or abort message to the trusted party. (A semi-honest
adversary never aborts.) In the latter case, the trusted party sends to each uncorrupted party
Pi its output value yi. In the former case, the trusted party sends the special symbol ⊥to
each uncorrupted party."
COMPUTE WINT,0.48155737704918034,"• Outputs: Sim outputs an arbitrary function of its view, and the honest parties output the
values obtained from the trusted party."
COMPUTE WINT,0.48360655737704916,Under review as a conference paper at ICLR 2022
COMPUTE WINT,0.48565573770491804,"The interaction of Sim with the trusted party deﬁnes a random variable IDEALf,A(z)(λ, ⃗x) as
above,and we let {IDEALf,A(z),I(λ, ⃗x)}k∈N,⟨⃗x,z⟩∈{0,1}∗. Having deﬁned the real and the ideal
worlds, we now proceed to deﬁne our notion of security, where a semi-honest adversary is deﬁned
as an (interactive) Turing Machine whose speciﬁcations match those of the protocol."
COMPUTE WINT,0.48770491803278687,"Deﬁnition 1. Let k be the security parameter. Let f be an n-party randomized functionality, and
Π be an n-party protocol for n ∈N. We say that Π t-securely computes f in the presence of semi-
honest adversaries if for every semi-honest adversary A there exists a semi-honest adversary Sim
such that for any I ⊂[n] with |I| ≤t the following quantity is negligible:"
COMPUTE WINT,0.48975409836065575,"|Pr[REALΠ,A(z),I(λ, ⃗x) = 1] −Pr[IDEALf,A(z),I(λ, ⃗x) = 1]|"
COMPUTE WINT,0.4918032786885246,"where ⃗x = {xi}i∈[n] ∈{0, 1}∗and z ∈{0, 1}∗."
COMPUTE WINT,0.49385245901639346,"B.2
Simulation"
COMPUTE WINT,0.4959016393442623,"Since our model is semi-honest and our protocols use secure 2-party computation, the simulator in
our case is straightforward. Given the inputs of the adversary and the bucket index containing the
median value across clients, the simulator sends random shares to the corrupted server on behalf of
the honest clients. Finally, during secure bucketed median computation, it invokes the simulator for
the secure protocol implementing Fℓ
Comp, and to this simulator it provides as input the bucket index
of the median value."
COMPUTE WINT,0.4979508196721312,"C
Protocols implementing Fℓ
Comp, Fℓ
Equal"
COMPUTE WINT,0.5,"For exposition purposes, we described our protocols for median computation by writing them in a
“hybrid” model, where parties have access to trusted functionalities Fℓ
Comp, Fℓ
Equal, computing secret
sharing of comparison and equality of two inputs respectively (described in Figure 7 and Figure 8
respectively). In this section, we provide protocols implementing the said functionalities. Our ﬁnal
protocol is then obtained by composing these with our own protocols. We then provide reasoning
on why we choose secure comparisons as the metric for protocol complexity."
COMPUTE WINT,0.5020491803278688,"Fℓ
Comp
Parties - P0, P1
• ∀b ∈{0, 1}, Pb sends xb, yb ∈Z2ℓto Fℓ
Comp.
• Fℓ
Comp computes x = x0 + x1 ∈Z2ℓ, y = y0 + y1 ∈Z2ℓand bit z = (x > y).
• Fℓ
Comp randomly chooses z0 ∈Z2ℓand computes z1 = z −z0 ∈Z2ℓand sends zb to
Pb, ∀b ∈{0, 1}."
COMPUTE WINT,0.5040983606557377,"Figure 7: Ideal functionality for 2-party comparisons: given ⟨x⟩, ⟨y⟩compute ⟨x > y⟩."
COMPUTE WINT,0.5061475409836066,"Fℓ
Equal
Parties - P0, P1
• ∀b ∈{0, 1}, Pb sends xb, yb ∈Z2ℓto Fℓ
Equal.
• Fℓ
Equal computes x = x0 + x1 ∈Z2ℓ, y = y0 + y1 ∈Z2ℓand bit z = (x = y).
• Fℓ
Equal randomly chooses z0 ∈Z2ℓand computes z1 = z −z0 ∈Z2ℓand sends zb to
Pb, ∀b ∈{0, 1}."
COMPUTE WINT,0.5081967213114754,"Figure 8: Ideal functionality for 2-party equality: given ⟨x⟩, ⟨y⟩compute ⟨x = y⟩."
COMPUTE WINT,0.5102459016393442,"Notation
Before going further, lets setup some notation. For a boolean variable z, we use notation
1{z} to indicate the indicator variable, which is 1 iff z is true. While ⟨x⟩0, ⟨x⟩1 denote (2, 2) additive
secret shares of x over Z2ℓ, we use notation ⟨x⟩B
0 , ⟨x⟩B
1 to denote the (2, 2) additive secret shares of
x over Z2."
COMPUTE WINT,0.5122950819672131,"To implement Fℓ
Comp, Fℓ
Equal, we use and modify a bit the protocols proposed in CrypTFlow2 (Rathee
et al., 2020). We describe the protocols for these in the following hybrids:"
COMPUTE WINT,0.514344262295082,Under review as a conference paper at ICLR 2022
COMPUTE WINT,0.5163934426229508,"• Fℓ
Mill: This is described in Figure 9. It computes the solution to the millionaire’s problem
over Z2ℓand provides boolean secret shares of the answer bit to the two parties."
COMPUTE WINT,0.5184426229508197,"• Fℓ
B2A: This is described in Figure 10. It takes in boolean shares of a bit from the two parties
and provides shares of the same over Z2ℓ."
COMPUTE WINT,0.5204918032786885,"• Fℓ
Mill,eq: This is described in Figure 11. It is similar to Fℓ
Mill, except instead of doing
comparison, it checks equality of the two received values."
COMPUTE WINT,0.5225409836065574,"Each of the above hybrids can be implemented directly using protocols from CrypTFlow2 (Rathee
et al., 2020). In more detail, while protocols for Fℓ
Mill, Fℓ
B2A are described directly in (Rathee et al.,
2020), the protocol for Fℓ
Mill,eq can be obtained directly by a small tweak1 to the protocol for Fℓ
Mill.
Since these protocols follow directly from (Rathee et al., 2020), we skip providing detailed descrip-
tions and proofs of them here and refer the reader to CrypTFlow2 (Rathee et al., 2020) for the same."
COMPUTE WINT,0.5245901639344263,"Given the above three hybrids, we provide the protocols for implementing Fℓ
Comp, Fℓ
Equal in Algo-
rithm 4, Algorithm 5 respectively. We then have the following theorems (where “securely” refers to
simulation-based security according to Deﬁnition 1)."
COMPUTE WINT,0.5266393442622951,"Theorem C.1. Πℓ
Comp securely implements Fℓ
Comp in the Fℓ−1
Mill , Fℓ
B2A hybrid as long as the signed
inputs of the two parties x, y (whose secret-shares they input into the protocol) satisfy |x| + |y| <
2ℓ−1."
COMPUTE WINT,0.5286885245901639,"Theorem C.2. Πℓ
Equal securely implements Fℓ
Equal in the Fℓ
Mill,eq, Fℓ
B2A hybrid."
COMPUTE WINT,0.5307377049180327,"Composing the corresponding implementation of Fℓ
Mill, Fℓ
B2A, Fℓ
Mill,eq (as described above), gives us
secure protocols for Fℓ
Comp, Fℓ
Equal."
COMPUTE WINT,0.5327868852459017,"Fℓ
Mill
Parties - P0, P1
• P0 sends x ∈{0, 1}ℓto Fℓ
Mill, while P1 sends y ∈{0, 1}ℓto Fℓ
Mill.
• Fℓ
Mill computes z = (x < y), chooses ⟨z⟩B
0 ←{0, 1} at random and computes ⟨z⟩B
1 =
⟨z⟩B
0 ⊕z. Fℓ
Mill sends ⟨z⟩B
b to Pb, ∀b ∈{0, 1}."
COMPUTE WINT,0.5348360655737705,Figure 9: Ideal functionality for solving the Millionaire’s problem
COMPUTE WINT,0.5368852459016393,"Fℓ
B2A
Parties - P0, P1
• ∀b ∈{0, 1}, Pb sends ⟨x⟩B
b to Fℓ
B2A.
• Fℓ
B2A computes x = ⟨x⟩B
0 ⊕⟨x⟩B
1 , chooses ⟨x⟩0 ←Z2ℓat random and computes ⟨x⟩1 ∈Z2ℓ
as ⟨x⟩1 = x −⟨x⟩0. It sends ⟨x⟩b to Pb, ∀b ∈{0, 1}."
COMPUTE WINT,0.5389344262295082,Figure 10: Ideal functionality for converting boolean to arithmetic shares
COMPUTE WINT,0.5409836065573771,"1(Rathee et al., 2020) provides protocols for F ℓ
Mill and F ℓ
B2A directly. The protocol for F ℓ
Mill works by
breaking up the ℓbits of inputs into a balanced binary tree and computing at each level the answer of both
comparison and equality checks of the inputs corresponding to that subtree. Hence, while not stated directly in
the protocol in (Rathee et al., 2020), a protocol for F ℓ
Mill,eq can be easily obtained from the protocol for F ℓ
Mill. In
addition, the public implementation of CrypTFlow2 (cry, 2021), contains functions which allow us to compute
the same without tweaking anything underlying."
COMPUTE WINT,0.5430327868852459,Under review as a conference paper at ICLR 2022
COMPUTE WINT,0.5450819672131147,"Fℓ
Mill,eq
Parties - P0, P1
• P0 sends x ∈{0, 1}ℓto Fℓ
Mill,eq, while P1 sends y ∈{0, 1}ℓto Fℓ
Mill,eq.
• Fℓ
Mill,eq computes z = (x = y), chooses ⟨z⟩B
0 ←{0, 1} at random and computes ⟨z⟩B
1 =
⟨z⟩B
0 ⊕z. Fℓ
Mill,eq sends ⟨z⟩B
b to Pb, ∀b ∈{0, 1}."
COMPUTE WINT,0.5471311475409836,Figure 11: Ideal functionality checking if two parties hold the same value
COMPUTE WINT,0.5491803278688525,"Algorithm 4 ℓbit signed comparison protocol, Πℓ
Comp."
COMPUTE WINT,0.5512295081967213,"Input: ∀b ∈{0, 1}, Pb holds ⟨x⟩b, ⟨y⟩b.
Output: ∀b ∈{0, 1}, Pb holds ⟨1{x > y}⟩b"
COMPUTE WINT,0.5532786885245902,"1. ∀b ∈{0, 1}, Pb computes ⟨z⟩b = ⟨x⟩b −⟨y⟩b."
COMPUTE WINT,0.555327868852459,2. Compute boolean shares of msb(z) as follows:
COMPUTE WINT,0.5573770491803278,"(a) ∀b ∈{0, 1}, Pb parses ⟨z⟩b as msbb||yb.
(b) P0 and P1 invoke Fℓ−1
Mill with P0’s input 2ℓ−1 −y0 and P1’s input y1. ∀b ∈{0, 1}, Pb
learns ⟨carry⟩B
b .
(c) ∀b ∈{0, 1}, Pb computes ⟨t⟩B
b = msbb ⊕⟨carry⟩B
b ."
COMPUTE WINT,0.5594262295081968,"3. P0 and P1 invoke Fℓ
B2A with inputs ⟨t⟩B
b to learn ⟨t⟩b."
COMPUTE WINT,0.5614754098360656,"Algorithm 5 ℓbit equality protocol, Πℓ
Equal."
COMPUTE WINT,0.5635245901639344,"Input: ∀b ∈{0, 1}, Pb holds ⟨x⟩b, ⟨y⟩b.
Output: ∀b ∈{0, 1}, Pb holds ⟨1{x = y}⟩b"
COMPUTE WINT,0.5655737704918032,"1. ∀b ∈{0, 1}, Pb computes ⟨z⟩b = ⟨x⟩b −⟨y⟩b."
COMPUTE WINT,0.5676229508196722,2. Check if z = 0 by doing the following:
COMPUTE WINT,0.569672131147541,"(a) P0 and P1 invoke computes Fℓ
Mill,eq with P0’s input ⟨z⟩0, while P1’s input being
(−⟨z⟩1) mod 2ℓ. ∀b ∈{0, 1}, Pb learns ⟨eq⟩B
b .
(b) P0 and P1 invoke Fℓ
B2A with inputs ⟨eq⟩B
b to learn ⟨eq⟩b."
COMPUTE WINT,0.5717213114754098,"C.1
Why we use secure comparison as complexity metric"
COMPUTE WINT,0.5737704918032787,"MPC is efﬁcient on linear operations when using arithmetic shares. The sum of two arithmetic
shares ⟨a⟩+ ⟨b⟩result in share ⟨a + b⟩. However, this is not the case for non-linear operations such
as ⟨a⟩> ⟨b⟩, since the operation performed and the type of secret sharing are not of the same nature."
COMPUTE WINT,0.5758196721311475,"These type of non-linear operations such as multiplication, comparisons, and equality require in-
creased computations, such as generation of correlated randomness between parties. There have
been many works on improving the efﬁciency of secure comparisons. (Damg˚ard et al., 2006) pro-
vide a secure comparison protocol which uses O(ℓlog ℓ) secure multiplications, where ℓ= log p
and p is the prime used for the ﬁeld the secret shares are computed over. (Nishide & Ohta, 2007)
improved the complexity of secure comparison to O(ℓ) secure multiplications."
COMPUTE WINT,0.5778688524590164,"Thus a large portion of the evaluation of a function in MPC is due to the computation of non-linear
operations. Computing median intuitively involves expensive comparison operations, and therefore
we choose the number of secure comparisons needed as a metric for computational complexity of
the protocols."
COMPUTE WINT,0.5799180327868853,"D
Detailed Robustness and Convergence Analysis"
COMPUTE WINT,0.5819672131147541,"Proposition D.1 (Proposition 6.1 Restated). With Assumption 2, we have"
COMPUTE WINT,0.5840163934426229,"∀δ > 0 :
lim
N→∞ϵN,δ = 0."
COMPUTE WINT,0.5860655737704918,Under review as a conference paper at ICLR 2022
COMPUTE WINT,0.5881147540983607,"Proof. Given a δ > 0, for ∀ϵ > 0, by Assumption 2 there exists N such that ∀M : M > N we have
δM,ϵ < δ/2. Therefore, ϵM,δ ≤ϵM,δ/2 = inf{ϵ′ : δM,ϵ′ ≤δ/2} ≤ϵ."
COMPUTE WINT,0.5901639344262295,"Theorem D.1. With Assumption 1&2, consider n1 i.i.d. samples {∇ˆFi}n1
i=1 (equation 1) and n2
adversarial vectors {vi ∈Rd}n2
i=1. Denote n := n1 + n2 and α := n2/n being the ratio of"
COMPUTE WINT,0.5922131147540983,"adversaries. If α < 1/2, then for ∀ϵ > ϵN, 1−2α"
COMPUTE WINT,0.5942622950819673,"2(1−α) with probability at least 1 −(δN,ϵ)
n
2 (1−2α), we
have
∀{vi ∈Rd}n2
i=1 :
∥Med({∇ˆFi(w)}n1
i=1 ∪{vi}n2
i=1) −∇F(w)∥∞≤ϵ,
for ∀w ∈W. Note that as N →∞, both δN,ϵ →0 and ϵN,δ →0."
COMPUTE WINT,0.5963114754098361,"Proof. By deﬁnition of the median operation, we have"
COMPUTE WINT,0.5983606557377049,"∥Med({∇ˆFi(w)}n1
i=1 ∪{vi}n2
i=1) −∇F(w)∥∞"
COMPUTE WINT,0.6004098360655737,"=∥Med({∇ˆFi(w) −∇F(w)}n1
i=1 ∪{vi −∇F(w)}n2
i=1)∥∞.
(3)
Invoking Lemma E.2, we have"
COMPUTE WINT,0.6024590163934426,"(3) ≤Med({∥∇ˆFi(w) −∇F(w)∥∞}n1
i=1 ∪{∥vi −∇F(w)∥∞}n2
i=1)"
COMPUTE WINT,0.6045081967213115,"= Med({∥∇ˆFi(w) −∇F(w)∥∞}n1
i=1 ∪{ai}n2
i=1),
(4)
where ai = ∥vi −∇F(w)∥∞∈R."
COMPUTE WINT,0.6065573770491803,"As we can see, (4) depends on the variance of ∇ˆFi =
1
N
PN
j=1 ∇f(· ; ξi,j), where ξi,j ∼D are
i.i.d. sampled. Denote
ˆXi,ϵ := 1{∥∇F −∇ˆFi∥∞,∞≤ϵ},"
COMPUTE WINT,0.6086065573770492,"where 1(·) is the indicator function. By Assumption 2, we can see that ˆXi is a Bernoulli random
variable with"
COMPUTE WINT,0.610655737704918,"Pr
n
ˆXi,ϵ = 1
o
= 1 −δN,ϵ."
COMPUTE WINT,0.6127049180327869,"Therefore, ˆXϵ := Pn1
i=1 ˆXi,ϵ forms a binomial distribution. By the Chernoff bound for binomial
distribution, for k ∈N and k ≤n1(1 −δN,ϵ), we have"
COMPUTE WINT,0.6147540983606558,"Pr( ˆXϵ ≤k) ≤e
−n1DKL"
COMPUTE WINT,0.6168032786885246,"
k
n1
(1−δN,ϵ)
"
COMPUTE WINT,0.6188524590163934,",
(5)
where DKL is the Kullback–Leibler divergence, i.e., for q1, q2 ∈(0, 1)"
COMPUTE WINT,0.6209016393442623,"DKL(q1∥q2) := q1 log
q1 q2"
COMPUTE WINT,0.6229508196721312,"
+ (1 −q1) log
1 −q1 1 −q2 "
COMPUTE WINT,0.625,≥q1 log q1 + (1 −q1) log(1 −q1) −(1 −q1) log(1 −q2)
COMPUTE WINT,0.6270491803278688,"≥(1 −q1) log
1
1−q2 ,"
COMPUTE WINT,0.6290983606557377,"where the ﬁrst inequality is due to that q2 < 1 and the last inequality is because that q1 log q1 + (1 −
q1) log(1 −q1) is the entropy which is non-negative."
COMPUTE WINT,0.6311475409836066,"Combining the above inequality with equation 5, we have"
COMPUTE WINT,0.6331967213114754,"Pr( ˆXϵ ≤k) ≤e
−n1(1−k"
COMPUTE WINT,0.6352459016393442,"n1 ) log
1
δN,ϵ = (δN,ϵ)n1−k .
(6)"
COMPUTE WINT,0.6372950819672131,Denoting event
COMPUTE WINT,0.639344262295082,Eϵ := { ˆXϵ > n1 + n2
COMPUTE WINT,0.6413934426229508,"2
},
(7)"
COMPUTE WINT,0.6434426229508197,"we can see that if Eϵ happens then by Lemma E.1 we have (4) ≤ϵ. It left to show the probability
of event Eϵ happens. Therefore, when n1+n2"
COMPUTE WINT,0.6454918032786885,"2n1
≤(1 −δN,ϵ), we can apply equation 6 to have"
COMPUTE WINT,0.6475409836065574,Pr(Eϵ) = 1 −Pr( ˆXϵ ≤n1 + n2
COMPUTE WINT,0.6495901639344263,"2
) ≥1 −(δN,ϵ)
n
2 (1−2α).
(8)"
COMPUTE WINT,0.6516393442622951,The condition of n1+n2
COMPUTE WINT,0.6536885245901639,"2n1
≤(1−δN,ϵ) is equivalent to δN,ϵ ≤
1−2α
2(1−α), and it has a sufﬁcient condition
of ϵ > ϵN, 1−2α"
COMPUTE WINT,0.6557377049180327,2(1−α) .
COMPUTE WINT,0.6577868852459017,Under review as a conference paper at ICLR 2022
COMPUTE WINT,0.6598360655737705,"Theorem D.2 (Theorem 6.1 Restated). With Assumption 1&2, consider n1 i.i.d. samples {∇ˆFi}n1
i=1
(equation 1) and n2 adversarial vectors {vi ∈Rd}n2
i=1. Denote n := n1 + n2 and α := n2/n being
the ratio of adversaries. Given the bucket range B > 0, the number of buckets b and the gradient
descent step size µ. If α < 1/2 then for ∀ϵ > ϵN, 1−2α"
COMPUTE WINT,0.6618852459016393,"2(1−α) with probability at least 1 −(δN,ϵ)
n
2 (1−2α),
we have"
COMPUTE WINT,0.6639344262295082,"∀{vi ∈Rd}n2
i=1 : ∥BucketMed({µ∇ˆFi(w)}n1
i=1 ∪{vi}n2
i=1; B, b) −µ∇F(w)∥∞≤µϵ + B 2b,"
COMPUTE WINT,0.6659836065573771,for ∀w ∈W satisfying µ∥∇ˆFi(w)∥∞≤B for all i ∈[n].
COMPUTE WINT,0.6680327868852459,"Proof. First, by the deﬁnition of BucketMed(·), for any input vector vi ∈Rd to the BucketMed(·),
there exists a vector v′
i ∈Rd with ∥v′
i∥∞≤B such that"
COMPUTE WINT,0.6700819672131147,"BucketMed({µ∇ˆFi(w)}n1
i=1 ∪{vi}n2
i=1; B, b) = BucketMed({µ∇ˆFi(w)}n1
i=1 ∪{v′
i}n2
i=1; B, b)."
COMPUTE WINT,0.6721311475409836,Denote Eϵ as the event deﬁned in equation 7 By equation 8 we can see that
COMPUTE WINT,0.6741803278688525,"Pr(Eϵ) ≥1 −(δN,ϵ)
n
2 (1−2α).
Given the event Eϵ, by Theorem D.1 we can see that
∥Med({µ∇ˆFi(w)}n1
i=1 ∪{v′
i}n2
i=1) −µ∇F(w)∥∞≤µϵ,
(9)"
COMPUTE WINT,0.6762295081967213,"for ∀w ∈W. Moreover, for ∀w ∈W satisfying µ∥∇ˆFi(w)∥∞≤B for all i ∈[n1], by Lemma E.3
we have"
COMPUTE WINT,0.6782786885245902,"∥BucketMed({µ∇ˆFi(w)}n1
i=1 ∪{v′
i}n2
i=1; B, b) −Med({µ∇ˆFi(w)}n1
i=1 ∪{v′
i}n2
i=1)∥∞≤B 2b."
COMPUTE WINT,0.680327868852459,"(10)
Therefore, combining equation 9 and equation 10 gives the theorem."
COMPUTE WINT,0.6823770491803278,"Theorem D.3 (Theorem 6.2 Restated). With Assumption 1&2, consider n1 normal clients with i.i.d.
samples {∇ˆFi}n1
i=1 (equation 1) and n2 faulty clients that can send arbitrary vectors following the
proposed protocol. Denote n := n1 +n2 and α := n2/n being the ratio of faulty clients. The bucket
range adaption is deﬁned by equation 2, where we assume Bt ≤B for all iteration t for a constant
B > 0, and p0 ≥
1
mβ ∥∇ˆFi(w0)∥∞for ∀i ∈[n1]. If α < 1/2, for ∀ϵ > ϵN, 1−2α"
COMPUTE WINT,0.6844262295081968,"2(1−α) and ∀ϵ′ > 0, the"
COMPUTE WINT,0.6864754098360656,"proposed method with parameters µ =
1
dβ , p1 ≥2µϵ + B"
COMPUTE WINT,0.6885245901639344,"2b, η ≥1 + βµ and b ≥dBβ"
COMPUTE WINT,0.6905737704918032,2ϵ′ can achieve
COMPUTE WINT,0.6926229508196722,"min
0≤t≤T
1
m∥∇F(wt)∥2
2 ≤2βF(w0)"
COMPUTE WINT,0.694672131147541,"T + 1
+ (ϵ + ϵ′)2,"
COMPUTE WINT,0.6967213114754098,"with probability at least 1 −(δN,ϵ)
n
2 (1−2α)."
COMPUTE WINT,0.6987704918032787,Proof. Denote Eϵ := { ˆXϵ > n1+n2
COMPUTE WINT,0.7008196721311475,"2
} as the event deﬁned in equation 7, where"
COMPUTE WINT,0.7028688524590164,"ˆXϵ := n1
X"
COMPUTE WINT,0.7049180327868853,"i=1
1{∥∇F −∇ˆFi∥∞,∞≤ϵ}."
COMPUTE WINT,0.7069672131147541,By equation 8 we have
COMPUTE WINT,0.7090163934426229,"Pr(Eϵ) ≥1 −(δN,ϵ)
n
2 (1−2α).
Now condition on the event Eϵ, and denote the set of clients
I := {i ∈[n1] : ∥∇F −∇ˆFi∥∞,∞≤ϵ},
(11)
and we can see that |I| > n1+n2 2
."
COMPUTE WINT,0.7110655737704918,"Recall that the training procedure of Algorithm 3 is
wt = wt−1 −BucketMed({µ∇ˆFi(wt−1)}n1
i=1 ∪{vi,t−1}n2
i=1; Bt−1, b),
where vi,t−1 are sent from the faulty clients i at iteration t. In the following, for notational ease we
denote
gt−1 := BucketMed({µ∇ˆFi(wt−1)}n1
i=1 ∪{vi,t−1}n2
i=1; Bt−1, b).
Moreover recall the bucket range Bt at iteration t is updated by
Bt = η∥wt −wt−1∥1 + p1 = η∥gt−1∥1 + p1.
Now, given any i ∈I, we make the following claim."
COMPUTE WINT,0.7131147540983607,Under review as a conference paper at ICLR 2022
COMPUTE WINT,0.7151639344262295,"Claim 1. Conditioned on event Eϵ, for any iteration step t ≥0 we have"
COMPUTE WINT,0.7172131147540983,"µ∥∇ˆFi(wt)∥∞≤Bt,
(12)"
COMPUTE WINT,0.7192622950819673,∥µ∇F(wt) −gt∥∞≤µϵ + B
COMPUTE WINT,0.7213114754098361,"2b.
(13)"
COMPUTE WINT,0.7233606557377049,"Proof of Claim 1. Note that equation 12 is true for t = 0 by assumption, and we prove the claim by
induction. Assuming equation 12 holds for iteration t −1, we prove equation 12 for iteration t, and
prove equation 13 for iteration t −1 by the way. We begin by"
COMPUTE WINT,0.7254098360655737,µ∥∇ˆFi(wt)∥∞
COMPUTE WINT,0.7274590163934426,=µ∥∇ˆFi(wt) −∇F(wt) + ∇F(wt) −∇F(wt−1) + ∇F(wt−1) −gt−1/µ + gt−1/µ∥∞
COMPUTE WINT,0.7295081967213115,"≤µ∥∇ˆFi(wt) −∇F(wt)∥∞
|
{z
}
a1"
COMPUTE WINT,0.7315573770491803,"+ µ∥∇F(wt) −∇F(wt−1)∥∞
|
{z
}
a2
+ ∥µ∇F(wt−1) −gt−1∥∞
|
{z
}
a3"
COMPUTE WINT,0.7336065573770492,"+ ∥gt−1∥∞
|
{z
}
a4"
COMPUTE WINT,0.735655737704918,",
(14)"
COMPUTE WINT,0.7377049180327869,"where the last step is by triangle inequality. We will show that a1, a2, a4 have straight-forward upper
bounds, respectively. For the a1, by equation 11 we have"
COMPUTE WINT,0.7397540983606558,a1 ≤µϵ.
COMPUTE WINT,0.7418032786885246,"By Assumption 1, we have"
COMPUTE WINT,0.7438524590163934,a2 ≤µβ∥wt −wt−1∥1 = µβ∥gt−1∥1.
COMPUTE WINT,0.7459016393442623,"For a4, by deﬁnition of the ℓ1 norm and ℓ∞norm, we directly have"
COMPUTE WINT,0.7479508196721312,a4 ≤∥gt−1∥1.
COMPUTE WINT,0.75,"Note that a3 is exactly we want to prove for equation 13. For a3, we need to prove a slightly
different version of Theorem 6.1, however, using similar techniques. Recall the deﬁnition of gt−1
and I ⊆[n1], we have"
COMPUTE WINT,0.7520491803278688,"a3 = ∥µ∇F(wt−1) −BucketMed({µ∇ˆFi(wt−1)}n1
i=1 ∪{vi,t−1}n2
i=1; Bt−1, b)∥∞"
COMPUTE WINT,0.7540983606557377,"= ∥µ∇F(wt−1) −BucketMed({µ∇ˆFi(wt−1)}i∈I ∪{µ∇ˆFi(w)}i∈[n1]\I ∪{vi,t−1}n2
i=1; Bt−1, b)∥∞."
COMPUTE WINT,0.7561475409836066,"Note that we have assumed that ∥µ∇ˆFi(wt−1)∥≤Bt−1. For {µ∇ˆFi(w)}i∈[n1]\I ∪{vi,t−1}n2
i=1,
there exists n1 + n2 −|I| vectors v′
i ∈Rd satisfying ∥v′
i∥∞≤Bt−1 for i ∈[n1 + n2 −|I|] such
that"
COMPUTE WINT,0.7581967213114754,"BucketMed({µ∇ˆFi(wt−1)}n1
i=1 ∪{vi,t−1}n2
i=1; Bt−1, b)"
COMPUTE WINT,0.7602459016393442,"= BucketMed({µ∇ˆFi(wt−1)}i∈I ∪{µ∇ˆFi(w)}i∈[n1]\I ∪{vi,t−1}n2
i=1; Bt−1, b)"
COMPUTE WINT,0.7622950819672131,"= BucketMed({µ∇ˆFi(wt−1)}i∈I ∪{v′
i}n1+n2−|I|
i=1
; Bt−1, b)
(15)"
COMPUTE WINT,0.764344262295082,"Therefore, denoting V := {µ∇ˆFi(wt−1)}i∈I ∪{v′
i}n1+n2−|I|
i=1
, by Lemma E.3, we have"
COMPUTE WINT,0.7663934426229508,"∥BucketMed(V; Bt−1, b) −Med(V)∥∞≤Bt−1"
B,0.7684426229508197,"2b
≤B"
B,0.7704918032786885,"2b.
(16)"
B,0.7725409836065574,"Moreover, since |I| > n1+n2"
B,0.7745901639344263,"2
and ∥∇F(wt−1) −∇ˆF(wt−1)∥∞≤ϵ for any i ∈I, by Lemma E.1
we have"
B,0.7766393442622951,"∥Med(V) −∇F(wt−1)∥∞= ∥Med({µ∇ˆFi(wt−1)}i∈I ∪{v′
i}n1+n2−|I|
i=1
) −∇F(wt−1)∥∞
≤µϵ.
(17)"
B,0.7786885245901639,"Combining equation 16 and equation 17 by triangle inequality, we have"
B,0.7807377049180327,"∥BucketMed(V; Bt−1, b) −µ∇F(wt−1)∥∞≤µϵ + B 2b."
B,0.7827868852459017,"Noting that V = {µ∇ˆFi(wt−1)}i∈I ∪{v′
i}n1+n2−|I|
i=1
, we apply equation 15 to have"
B,0.7848360655737705,"∥BucketMed({µ∇ˆFi(wt−1)}n1
i=1 ∪{vi,t−1}n2
i=1; Bt−1, b) −µ∇F(wt−1)∥∞≤µϵ + B 2b,"
B,0.7868852459016393,Under review as a conference paper at ICLR 2022
B,0.7889344262295082,"which proves equation 13 for iteration t −1, and equivalently a3 ≤µϵ + B"
B,0.7909836065573771,"2b. Collecting our upper
bounds for a1, a2, a3 and a4, we ﬁnally have"
B,0.7930327868852459,(14) = a1 + a2 + a3 + a4 ≤µϵ + µβ∥gt−1∥1 + µϵ + B
B,0.7950819672131147,2b + ∥gt−1∥1
B,0.7971311475409836,= 2µϵ + B
B,0.7991803278688525,2b + (1 + µβ)∥gt−1∥1.
B,0.8012295081967213,Recall the condition that p1 ≥2µϵ + B
B,0.8032786885245902,"2b and η ≥1 + βµ. We can see that
(14) ≤η∥gt−1∥1 + p1 = Bt.
By induction, it concludes the claim."
B,0.805327868852459,"Given the Claim 1, we continue the proof of Theorem 6.2. By the smoothness assumption (Assump-
tion 1), and given that the parameter domain W is deﬁned to be convex, it is known that
F(wt+1) −F(wt)"
B,0.8073770491803278,"≤⟨∇F(wt), wt+1 −wt⟩+ β"
B,0.8094262295081968,"2 ∥wt+1 −wt∥2
1"
B,0.8114754098360656,"= −⟨∇F(wt), gt⟩+ β"
B,0.8135245901639344,"2 ∥gt∥2
1"
B,0.8155737704918032,"≤−⟨∇F(wt), gt⟩+ dβ"
B,0.8176229508196722,"2 ∥gt∥2
2"
B,0.819672131147541,"= −⟨∇F(wt), gt −µ∇F(wt) + µ∇F(wt)⟩+ dβ"
B,0.8217213114754098,"2 ∥gt −µ∇F(wt) + µ∇F(wt)∥2
2"
B,0.8237704918032787,"= −µ∥∇F(wt)∥2
2 −⟨∇F(wt), gt −µ∇F(wt)⟩ + dβ"
B,0.8258196721311475,"2 (µ2∥∇F(wt)∥2
2 + ∥gt −µ∇F(wt)∥2
2 + µ⟨∇F(wt), gt −2µ∇F(wt)⟩)."
B,0.8278688524590164,"(18)
Substituting into µ =
1
dβ , we can continue as"
B,0.8299180327868853,(18) = −1
B,0.8319672131147541,"2dβ ∥∇F(wt)∥2
2 + dβ"
B,0.8340163934426229,"2 ∥gt −µ∇F(wt)∥2
2 ≤−1"
B,0.8360655737704918,"2dβ ∥∇F(wt)∥2
2 + dβ"
B,0.8381147540983607,"2 ∥gt −µ∇F(wt)∥2
∞."
B,0.8401639344262295,"By Claim 1, we have"
B,0.8422131147540983,(18) ≤−1
B,0.8442622950819673,"2dβ ∥∇F(wt)∥2
2 + dβ 2"
B,0.8463114754098361,"
µϵ + B"
B,0.8483606557377049,2b 2 = −1
B,0.8504098360655737,"2dβ ∥∇F(wt)∥2
2 +"
B,0.8524590163934426,"ϵ
√2β + r β"
DB,0.8545081967213115,"2
dB"
B,0.8565573770491803,2b !2 .
B,0.8586065573770492,"Therefore, rearranging the inequality, and denote"
B,0.860655737704918,ρ := 2β
B,0.8627049180327869,"ϵ
√2β + r β"
DB,0.8647540983606558,"2
dB"
B,0.8668032786885246,2b !2
B,0.8688524590163934,"=

ϵ + dBβ"
B,0.8709016393442623,"2b 2
,"
B,0.8729508196721312,"we have
1
d∥∇F(wt)∥2
2 ≤2β(F(wt) −F(wt+1)) + ρ."
B,0.875,"Accordingly, for number of iterations T ≥1:"
B,0.8770491803278688,"1
T + 1 T
X t=0"
B,0.8790983606557377,"1
d∥∇F(wt)∥2
2 ≤
2β
T + 1(F(w0) −F(wT +1)) + ρ"
B,0.8811475409836066,≤2βF(w0)
B,0.8831967213114754,"T + 1
+ ρ.
(19)"
B,0.8852459016393442,Given any ϵ′ > 0 and choosing b > dBβ
B,0.8872950819672131,"2ϵ′ , equation 19 implies"
B,0.889344262295082,"min
0≤t≤T
1
d∥∇F(wt)∥2
2 ≤2βF(w0)"
B,0.8913934426229508,"T + 1
+ (ϵ + ϵ′)2."
B,0.8934426229508197,Under review as a conference paper at ICLR 2022
B,0.8954918032786885,"E
Auxiliary Lemmas"
B,0.8975409836065574,"Lemma E.1. Given a set of real number A = {xi ∈R}n
i=1 and an interval [a, b] ⊂R, if |A ∩
[a, b]| > |A ∩[a, b]c| then we have Med(A) ∈[a, b]."
B,0.8995901639344263,"Proof. Without loss of generality, we assume A is sorted, i.e, xi ≤xi+1 for ∀i ∈[n −1]. By
the deﬁnition of median, there are at least ⌈n"
B,0.9016393442622951,"2 ⌉numbers in A that are less or equal to Med(A).
Therefore, if Med(A) < a, we can see that there are at least ⌈n"
B,0.9036885245901639,"2 ⌉numbers in A that are outside
[a, b], which contradicts to the condition of |A ∩[a, b]| > |A ∩[a, b]c|. Hence, Med(A) ≥a.
Similarly, we can show Med(A) ≤b. That being said, Med(A) ∈[a, b]."
B,0.9057377049180327,"Lemma E.2. Given n vectors {vi ∈Rd}n
i=1, we have"
B,0.9077868852459017,"∥Med({vi}n
i=1)∥∞≤Med({∥vi∥∞}n
i=1)."
B,0.9098360655737705,"Proof. Denoting vi(k) as the kth dimension of vi ∈Rd, we have"
B,0.9118852459016393,"∥Med({vi}n
i=1)∥∞= max
k∈[m] |Med({vi(k)}n
i=1)| ≤max
k∈[m] Med({|vi(k)|}n
i=1).
(20)"
B,0.9139344262295082,"Denote k⋆∈arg maxk∈[m] Med({|vi(k)|}n
i=1), and we have"
B,0.9159836065573771,"(20) = Med({|vi(k⋆)|}n
i=1) ≤Med({max
k∈[m] |vi(k)|}n
i=1) = Med({∥vi∥∞}n
i=1)."
B,0.9180327868852459,"Lemma E.3. Given the bucket range B > 0, the number of buckets b, for any n vectors {vi}n
i=1 in
Rd, if ∥vi∥∞≤B for all i ∈[n], then we have"
B,0.9200819672131147,"∥BucketMed({vi}n
i=1; B, b) −Med({vi}n
i=1)∥∞≤B 2b."
B,0.9221311475409836,"Proof. Since ∥vi∥∞≤B for all i ∈[n] are within the bucket range, the BucketMed({vi}n
i=1; B, b)
is the same as Med({v′
i}n
i=1) for some v′
i ∈Rd satisfying"
B,0.9241803278688525,"∥v′
i −vi∥∞≤B 2b."
B,0.9262295081967213,Denote
B,0.9282786885245902,"wl
i = vi −B"
B,0.930327868852459,"2b,
wr
i = vi + B 2b,"
B,0.9323770491803278,where the ± B
B,0.9344262295081968,"2b is done on every dimension of v. We can see that wl
i ≤v′
i ≤wr
i , where the ≤is
also dimension-wise. Noting that"
B,0.9364754098360656,"Med({vi}n
i=1) −B"
B,0.9385245901639344,"2b = Med({wl
i}n
i=1) ≤Med({v′
i}n
i=1) ≤Med({wr
i }n
i=1) = Med({vi}n
i=1) + B 2b,"
B,0.9405737704918032,we can conclude that
B,0.9426229508196722,"∥Med({v′
i}n
i=1) −Med({vi}n
i=1)∥≤B 2b."
B,0.944672131147541,"Replacing the Med({v′
i}n
i=1) by BucketMed({vi}n
i=1; B, b) gives the lemma."
B,0.9467213114754098,"F
Computation Breakdown and Parameter Sensitivity"
B,0.9487704918032787,"In this subsection, we detail a temporal breakdown of a round of federated learning in our approach
and the pairwise comparison median. We show the relationship between server-side computation
time and increasing number of clients. In the bucketing technique, parameters are agreed upon at
the start of the FL protocol to initiate values for the bucketing ranges. We empirically show that these
parameters are not sensitive, and that convergence still holds while varying the order of magnitude
of these parameter values."
B,0.9508196721311475,Under review as a conference paper at ICLR 2022
B,0.9528688524590164,"Figure 12: Relationship between number of clients and 1. total server-side computation time after
receiving all client updates, 2. bucketed median computation time."
B,0.9549180327868853,"Computation Breakdown for one round of FL
Insecure
Me-
dian"
B,0.9569672131147541,"Time
(s)"
B,0.9590163934426229,"Pairwise Compar-
ison Median"
B,0.9610655737704918,"Time
(s)"
B,0.9631147540983607,"Bucketed Median
Time
(s)
Clients train local
model"
"CLIENTS TRAIN LOCAL
MODEL",0.9651639344262295,"24
Clients train local
model"
"CLIENTS TRAIN LOCAL
MODEL",0.9672131147540983,"24
Clients train local
model 24"
"CLIENTS TRAIN LOCAL
MODEL",0.9692622950819673,"Clients
bucketize
model 5"
"CLIENTS TRAIN LOCAL
MODEL",0.9713114754098361,"Clients
compute
shares"
"CLIENTS
COMPUTE
SHARES",0.9733606557377049,"1
Clients
compute
shares 1"
"CLIENTS
COMPUTE
SHARES",0.9754098360655737,"Server insecurely
computes median"
"CLIENTS
COMPUTE
SHARES",0.9774590163934426,"6
Servers calls C++
backend to compute
median"
"CLIENTS
COMPUTE
SHARES",0.9795081967213115,"∼500
Servers calls C++
backend to compute
median buckets ∼125"
"CLIENTS
COMPUTE
SHARES",0.9815573770491803,"Server
quantizes
median buckets 5"
"CLIENTS
COMPUTE
SHARES",0.9836065573770492,"Total
30
Total
525
Total
160
Table 1: CNNMnist, 8 clients, simulated LAN network"
"CLIENTS
COMPUTE
SHARES",0.985655737704918,"In Table 1, we detail the time in seconds each part of the FL process takes for both the pairwise com-
parison based median method, and our proposed bucketed median method. The bucketed method
had the additional overhead on client and server side of converting to and from a bucketed format,
and this takes around 5 seconds per transformation for a CNNMnist model. The time for both
methods are dominated by the two server secure computation of median. We see that the pairwise
comparison based median FL system increases by a factor of 17.5, and the bucketed median FL
system increases by a factor of 5.5."
"CLIENTS
COMPUTE
SHARES",0.9877049180327869,"In Figure 12, we show the scalability of our protocol when the number of clients range from 10 to 160
in increments of 30. We detail the number of clients on the x-axis and the server-side computation
time (after receiving all client updates) in milliseconds on the y-axis. The server-side computation
consists of the following parts: 1. adding client shares to compute a histogram for each dimension,
2. performing MPC computation of the bucketed median for each dimension, 3. converting the
chosen median buckets to a global model. Steps 2. and 3. are independent of the number of clients.
We test on the MLP model. We see that the time for bucketed median computation stays constant,
yet the time taken for steps 1-3 roughly increases when number of clients increase due to the time
taken for step 1."
"CLIENTS
COMPUTE
SHARES",0.9897540983606558,"In Figure 13, we show the performance of our bucketed protocol with different parameter values.
In our protocol, we determine bucket range as B = {p0, ∥w′
g −wg∥1 + p1,t}, where p0 is chosen
during the initial epoch, and ∥w′
g −wg∥1 + p1,t is chosen in subsequent epochs. In our convergence
analysis, we use p1 and prove convergence for this choice. In our experiments, we use pt = p1"
"CLIENTS
COMPUTE
SHARES",0.9918032786885246,"t ,
where t is the current epoch count. Both choices empirically result in decreasing training loss per
epoch, albeit at slightly different rates. We note that in our implementation, we set both p0 and p1 as"
"CLIENTS
COMPUTE
SHARES",0.9938524590163934,Under review as a conference paper at ICLR 2022
"CLIENTS
COMPUTE
SHARES",0.9959016393442623,"Figure 13: Variable parameter values for bucket range tested on a three client FL system and CNN-
Mnist model."
"CLIENTS
COMPUTE
SHARES",0.9979508196721312,"equivalent values, and use the CNNMnist model. For parameter values less than 1, we see that our
bucketed mechanism results in a similar rate of training loss convergence. For each model, one can
solve for speciﬁc p0, p1 values for the best convergence rate, but as the convergence behaviour does
not strictly depend on the parameter values, we do not focus our attention towards this optimization."
