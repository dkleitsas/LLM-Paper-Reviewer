Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0009587727708533077,"To study the resilience of distributed learning, the “Byzantine” literature consid-
ers a strong threat model where workers can report arbitrary gradients to the pa-
rameter server. While this model helped generate several fundamental results, it
has however sometimes been considered unrealistic, when the workers are mostly
trustworthy machines. In this paper, we show a surprising equivalence between
this model and data poisoning, a threat considered much more realistic. More
speciﬁcally, we prove that any gradient attack can be reduced to data poisoning in
a personalized federated learning system that provides PAC guarantees (which we
show are both desirable and realistic in various personalized federated learning
contexts such as linear regression and classiﬁcation). Maybe most importantly,
we derive a simple and practical attack that may be constructed against classi-
cal personalized federated learning models, and we show both theoretically and
empirically the effectiveness of this attack."
INTRODUCTION,0.0019175455417066154,"1
INTRODUCTION"
INTRODUCTION,0.0028763183125599234,"Learning algorithms now typically leverage data generated by a large number of users (Smith et al.,
2013; Wang et al., 2019a;b) to often learn a common model that ﬁts a large population (Konecn´y
et al., 2015), but also sometimes to construct a personalized model for each individual (Ricci et al.,
2011). Autocompletion (Lehmann & Buschek, 2021), conversational (Shum et al., 2018) and rec-
ommendation (Ie et al., 2019) algorithms are examples of such algorithms deployed at scale. To be
effective, besides huge amounts of (distributed) data (Brown et al., 2020; Fedus et al., 2021), these
algorithms require a high level of customization. This has motivated research into personalized
federated learning (Fallah et al., 2020; Hanzely et al., 2020; Dinh et al., 2020)."
INTRODUCTION,0.003835091083413231,"However, in applications such as content recommendation, activists, companies, and politicians have
strong incentives to promote certain views, products or ideologies (Hoang, 2020; Hoang et al., 2021).
Remember for instance that, on YouTube, two views out of three result from algorithmic recommen-
dations (Solsman, 2018). Perhaps unsurprisingly, this has led to vast amounts of fabricated activities
to bias algorithms (Bradshaw & Howard, 2019; Neudert et al., 2019), like “fake reviews” (Wu et al.,
2020). The scale of this phenomenon is well illustrated by the case of Facebook which, in 2019
alone, reported the removal of around 6 billion fake accounts from its platform (Fung & Garcia,
2019). This is particularly concerning in the era of “stochastic parrots” (Bender et al., 2021): cli-
mate denialists are incentivized to pollute textual datasets with claims like “climate change is a
hoax”, as autocompletion, conversational and recommendation algorithms trained on such data will
more likely spread these views (McGufﬁe & Newhouse, 2020). This raises serious concerns about
the vulnerability of personalized federated learning to such misleading data. Data poisoning attacks
clearly constitute now a major machine learning security issue (Kumar et al., 2020)."
INTRODUCTION,0.004793863854266539,"Overall, in highly adversarial environments like social media, given the advent of deep fakes (John-
son & Diakopoulos, 2021), we should expect that most data are strategically crafted and labeled. In
this context, the authentication of the data provider seems critical. In particular, the safety of learn-
ing algorithms arguably demands that they be trained solely on cryptographically signed data, that
is, data that provably come from a known source. But even signed data cannot be wholeheartedly
trusted since users usually have preferences over what ought to be recommended to others. They
thus have incentives to behave strategically in order to promote certain views or products."
INTRODUCTION,0.005752636625119847,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.006711409395973154,"To address data poisoning, the Byzantine learning literature usually considers that each federated
learning worker may behave arbitrarily (Blanchard et al., 2017; Mhamdi et al., 2018; El-Mhamdi
et al., 2021). Recall that at each iteration of the federated learning stochastic gradient descent, each
worker is given the updated model, and is asked to compute the gradient of the loss function with
respect to (a batch of) its local data. Byzantine learning usually assumes that a malicious (Byzantine)
worker may report any gradient; without having to justify whether such a gradient could have been
generated through data poisoning. In fact, the gradient attack threat model has sometimes been
claimed to be unrealistic in practical federated learning (Shejwalkar et al., 2021), especially when
the workers are machines owned by trusted entities (cross-silo FL (Kairouz et al., 2021))."
INTRODUCTION,0.007670182166826462,"We prove in this paper an equivalence between gradients attacks and fabricated data injection, in a
general and desirable collaborative learning framework. Thereby, our paper provides the ﬁrst prac-
tically compelling argument for the necessity to protect federated learning against gradient attacks."
INTRODUCTION,0.00862895493767977,"Contributions.
As a preamble of our main result, we formalize local PAC* learning1 (Valiant,
1984) for personalized learning, and prove that a simple and general solution to personalized fed-
erated linear regression and classiﬁcation is indeed locally PAC* learning. Our proof leverages a
new concept called gradient-PAC* learning, which is of independent interest. We prove that it is
sufﬁcient to guarantee local PAC* learning, and that it is veriﬁed by basic learning algorithms, like
linear and logistic regression. This is an important and highly nontrivial contribution of this paper."
INTRODUCTION,0.009587727708533078,"Our main contribution is then to prove that local PAC* learning in personalized federated learning
essentially implies an equivalence between data poisoning and gradient attacks. More precisely, we
show how any (converging) gradient attack can be turned into a data poisoning attack, with the same
resulting harm. Given how easy it generally is to create fake accounts on web platforms and to inject
data poisoning by generating fake activities, this result should arguably greatly increase the concerns
over the vulnerabilities of federated learning with user-generated data."
INTRODUCTION,0.010546500479386385,"Finally, we propose a simple but very general strategic gradient attack, called the counter-gradient
attack (CGA), which any participant to federated learning can deploy to bias the global model
towards any target model that better suits their interest. We prove the effectiveness of this attack
under fairly general assumptions, which apply to many proposed personalized learning frameworks
including Hanzely et al. (2020); Dinh et al. (2020). We then show empirically how this gradient
attack can be turned into a devastating data poisoning attack, with remarkably few data."
INTRODUCTION,0.011505273250239693,"Related work.
Byzantine learning has provided both negative and positive results in Byzantine
resilience (Blanchard et al., 2017; Mhamdi et al., 2018; Baruch et al., 2019; Xie et al., 2019), some
of which apply almost straightforwardly to personalized federated learning (El-Mhamdi et al., 2020;
2021). Such results study the resilience against a minority of adversarial users. Our paper how-
ever focuses on a different kind of malicious users. Namely, like Suya et al. (2021), we study the
resilience against strategic users, who aim to bias the learned models towards a speciﬁc target model."
INTRODUCTION,0.012464046021093002,"The study of the resilience against strategic users is part of the research on strategyproof learning.
Many special cases of strategyproofness have been tackled, including regression (Chen et al., 2018b;
Dekel et al., 2010; Perote & Perote-Pe˜na, 2004; Ben-Porat & Tennenholtz, 2017), classiﬁcation
(Meir et al., 2012; Chen et al., 2020; Meir et al., 2011; Hardt et al., 2016), statistical estimation
(Cai et al., 2015), and clustering (Perote & Sevilla, 2003). However, none of these papers tackles a
general personalized federated learning scheme. Typically, for linear regression, Chen et al. (2018b)
and Perote & Perote-Pe˜na (2004) assume that each user can only provide a single data point. This
greatly restricts the users’ ability to contribute to the learning model. And while Dekel et al. (2010)
allows multiple contributions, they either require payments, which might not be possible (e.g., due to
ethical reasons), or they restrict the model to one dimension or a constant function in Rd. Conversely,
Suya et al. (2021) show how to arbitrarily manipulate convex learning models through multiple data
injections, in the case where a single model is learned from all data at once."
INTRODUCTION,0.013422818791946308,"A large literature has focused on data poisoning, with either a focus on backdoor (Dai et al., 2019;
Zhao et al., 2020; Severi et al., 2021; Truong et al., 2020; Schwarzschild et al., 2021) or triggerless
attacks (Biggio et al., 2012; Mu˜noz-Gonz´alez et al., 2017; Shafahi et al., 2018; Zhu et al., 2019;"
INTRODUCTION,0.014381591562799617,"1We omit complexity considerations for the sake of generality. We deﬁne PAC* to be PAC without such
considerations."
INTRODUCTION,0.015340364333652923,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.016299137104506232,"Huang et al., 2020; Barreno et al., 2006; Aghakhani et al., 2021; Geiping et al., 2021). However,
most of this research analyzed data poisoning without signed data. One noteworthy exception is
Mahloujifar et al. (2019), whose universal attack ampliﬁes the probability of a (bad) property."
INTRODUCTION,0.01725790987535954,"Collaborative PAC learning was previously introduced by Blum et al. (2017), and then extensively
studied (Chen et al., 2018a; Nguyen & Zakynthinou, 2018), sometimes with the presence of Byzan-
tine collaborating users (Qiao, 2018; Jain & Orlitsky, 2020; Konstantinov et al., 2020). We stress
however that this line of work assumes that all honest users have the same labeling function. In other
words, given any query, they agree on how the query should be answered. This is a very unrealistic
assumption in many critical applications, like content moderation or language processing. In fact, in
such applications, removing outliers can be argued to amount to ignoring minorities’ views, which
would be highly unethical. The very deﬁnition of PAC learning must then be adapted, which is what
we do in this paper (also, we adapt it to parameterized models)."
INTRODUCTION,0.01821668264621285,"Structure of the paper.
The rest of the paper is organized as follows. Section 2 presents a general
model of personalized learning, formalizes local PAC* learning and deﬁnes a general federated
gradient descent algorithm. Section 3 proves the equivalence between data poisoning and gradient
attacks, under local PAC* learning. Section 4 proves the local PAC* learning properties for federated
linear regression and classiﬁcation. Section 5 then describes a simple and general data poisoning
attack, whose effectiveness against ℓ2
2 is proved theoretically and empirically. Section 6 concludes."
A GENERAL PERSONALIZED FEDERATED LEARNING FRAMEWORK,0.019175455417066157,"2
A GENERAL PERSONALIZED FEDERATED LEARNING FRAMEWORK"
A GENERAL PERSONALIZED FEDERATED LEARNING FRAMEWORK,0.020134228187919462,"We consider a set [N] = {1, . . . , N} of users. Each user n ∈[N] has a local signed dataset Dn, and
aims to learn a local model θn ∈Rd. Users may collaborate to improve their models. Personalized
learning must then input a tuple of users’ local datasets ⃗D ≜(D1, . . . , DN), and output a tuple of
local models ⃗θ∗≜(θ∗
1, . . . , θ∗
N). Like many others, we assume that the users perform federated
learning to do so, by leveraging the computation of a common global model ρ ∈Rd. Intuitively, the
global model is an aggregate of all users’ local models, which users can leverage to improve their
local models. The common global model will typically allow users with too few data to obtain an
effective local model, while it may be mostly discarded by users whose local datasets are large."
A GENERAL PERSONALIZED FEDERATED LEARNING FRAMEWORK,0.02109300095877277,"More formally, we consider a very general personalized learning framework which generalizes the
models proposed by Dinh et al. (2020) and Hanzely et al. (2020). Namely, we consider that the
personalized learning algorithm outputs a global minimum (ρ∗, ⃗θ∗) of a global loss given by"
A GENERAL PERSONALIZED FEDERATED LEARNING FRAMEWORK,0.02205177372962608,"LOSS(ρ, ⃗θ, ⃗D) ≜
X"
A GENERAL PERSONALIZED FEDERATED LEARNING FRAMEWORK,0.023010546500479387,"n∈[N]
Ln(θn, Dn) +
X"
A GENERAL PERSONALIZED FEDERATED LEARNING FRAMEWORK,0.023969319271332695,"n∈[N]
R(ρ, θn),
(1)"
A GENERAL PERSONALIZED FEDERATED LEARNING FRAMEWORK,0.024928092042186004,"where R is a regularization, typically with a minimum at θn = ρ. For instance, Hanzely et al. (2020)
and Dinh et al. (2020) deﬁne R(ρ, θn) ≜λ ∥ρ −θn∥2
2, which we shall call the ℓ2
2 regularization.
But other regularizations may be considered, like the ℓ2 regularization R(ρ, θn) ≜λ ∥ρ −θn∥2, or"
A GENERAL PERSONALIZED FEDERATED LEARNING FRAMEWORK,0.02588686481303931,"the smooth-ℓ2 regularization R(ρ, θn) ≜λ
q"
A GENERAL PERSONALIZED FEDERATED LEARNING FRAMEWORK,0.026845637583892617,"1 + ∥ρ −θn∥2
2. Note that, for all such regularizations,
the limit λ →∞essentially yields the classical non-personalized federated learning framework."
A GENERAL PERSONALIZED FEDERATED LEARNING FRAMEWORK,0.027804410354745925,"2.1
LOCAL PAC* LEARNING"
A GENERAL PERSONALIZED FEDERATED LEARNING FRAMEWORK,0.028763183125599234,"In this paper, we focus on personalized learning algorithms that provably recover a user n’s preferred
model θ†
n, if the user provides a large enough honest dataset Dn, i.e. constructed with θ†
n. Such
honest datasets Dn could typically be obtained by repeatedly drawing random queries (or features),
and by using the user’s preferred model θ†
n to provide (potentially noisy) answers (or labels). We
refer to Section 4 for examples. The model recovery condition is then formalized as follows."
A GENERAL PERSONALIZED FEDERATED LEARNING FRAMEWORK,0.029721955896452542,"Deﬁnition 1. A personalized learning algorithm is locally PAC* learning if, for any subset H ⊂[N]
of honest users, any preferred models ⃗θ†, any ε, δ > 0, and any datasets ⃗D−H from users n /∈H,
there exists I such that, if all users h ∈H provide honest datasets Dh with at least |Dh| ≥I data"
A GENERAL PERSONALIZED FEDERATED LEARNING FRAMEWORK,0.030680728667305847,"points, then, with probability at least 1 −δ, we have
θ∗
h

⃗D

−θ†
h

2 ≤ε for all users h ∈H."
A GENERAL PERSONALIZED FEDERATED LEARNING FRAMEWORK,0.031639501438159155,Under review as a conference paper at ICLR 2022
A GENERAL PERSONALIZED FEDERATED LEARNING FRAMEWORK,0.032598274209012464,"Local PAC* learning is arguably a very desirable property. Indeed, it guarantees that any honest
active user will not be discouraged to participate in federated learning as they will eventually learn
their preferred model by providing more and more data. In Section 4, we will show how local PAC*
learning can be achieved in practice, by considering speciﬁc local loss functions Ln."
FEDERATED GRADIENT DESCENT,0.03355704697986577,"2.2
FEDERATED GRADIENT DESCENT"
FEDERATED GRADIENT DESCENT,0.03451581975071908,"While the computation of ρ∗and ⃗θ∗could be done by a single machine, which ﬁrst collects the
datasets ⃗D and then minimizes the global loss LOSS, modern machine learning deployments often
rather rely on federated (stochastic) gradient descent (or variants), with a central trusted parameter
server. In this setting, each user n keeps their data Dn locally. At each iteration t, the parameter
server sends the latest global model ρt to the users. Each user n is then expected to update its local
model given the global model ρt, either by solving θt
n ≜arg minθn Ln(θn, Dn)+R(ρt, θn) (which
is what we assume in the theory part, in the manner of Dinh et al. (2020)), or by making a (stochastic)
gradient step from the previous local model θt−1
n
(which is what is done in our experiments, in
the manner of Hanzely & Richt´arik (2021)). User n is then expected to report the gradient gt
n =
∇ρR(ρt, θt
n) to the parameter server. The parameter server then updates the global model, using a
gradient step, i.e. it computes ρt+1 ≜ρt −ηt
P"
FEDERATED GRADIENT DESCENT,0.03547459252157239,"n∈[N] gt
n. For simplicity, here, and since our goal
is to show the vulnerability of personalized federated learning even in good conditions, we assume
that the network is synchronous and that no node can crash. Note also that our setting could be
generalized to fully decentralized collaborative learning, as was done by El-Mhamdi et al. (2021)."
FEDERATED GRADIENT DESCENT,0.0364333652924257,"We assume that the users are only allowed to send plausible gradient gradients. More precisely, we
denote GRAD(ρ) ≜{∇ρR(ρ, θ) | θ ∈Rd} the closure set of plausible (sub)gradients at ρ. If user n’s
gradient gt
n is not in the set GRAD(ρt), the parameter server can easily detect the malicious behavior
and gt
n will be ignored by the parameter server at iteration t. In the case of an ℓ2
2 regularization,
where R(ρ, θ) = λ ∥ρ −θ∥2
2, we clearly have GRAD(ρ) = Rd for all ρ ∈Rd. It can be easily
shown that, for ℓ2 and smooth-ℓ2 regularizations, GRAD(ρ) is the closed ball B(0, λ)."
FEDERATED GRADIENT DESCENT,0.037392138063279005,"Nevertheless, in this setting, a strategic user s ∈[N] can deviate from its expected behavior, to bias
the global model in their favor. We identify in particular three sorts of attacks."
FEDERATED GRADIENT DESCENT,0.038350910834132314,"Data poisoning: Instead of collecting an honest dataset, s fabricates any strategically crafted
dataset Ds, and then performs all other operations as expected."
FEDERATED GRADIENT DESCENT,0.039309683604985615,"Model attack: At each iteration t, s ﬁxes θt
s ≜θ♠
s , where θ♠
s is any strategically crafted model.
All other operations would then be executed as expected.
Gradient attack: At each iteration t, s sends any (plausible) strategically crafted gradient gt
s."
FEDERATED GRADIENT DESCENT,0.040268456375838924,"Gradient attacks are intuitively the most harmful attacks, as the strategic user can then adapt their
attack based on what they observe during the learning process. However, because of this, gradient
attacks are more likely to be ﬂagged as suspicious behaviors. At the other end of the spectrum,
data poisoning is a much safer attack, as the strategic user can always report their entire dataset,
and prove that they rigorously performed the expected computations. In fact, data poisoning can be
executed, even if users directly provide the data to a (trusted) central authority, which is then tasked
to execute (federated) gradient descent. This is typically what is done to construct recommendation
algorithms on social medias, where users’ data are their online activities (what they view, like and
share). Crucially, especially in applications with no clear ground truth, such as content moderation
or language processing, the strategic user can always argue that their dataset is an “honest” dataset;
not a strategically crafted one. Ignoring the strategic user’s data on the basis that it is an “outlier”
may then be regarded as unethical, as it can be argued to amount to rejecting minorities’ viewpoints."
THE EQUIVALENCE BETWEEN DATA POISONING AND GRADIENT ATTACKS,0.04122722914669223,"3
THE EQUIVALENCE BETWEEN DATA POISONING AND GRADIENT ATTACKS"
THE EQUIVALENCE BETWEEN DATA POISONING AND GRADIENT ATTACKS,0.04218600191754554,"We now present our main result. The threat model we considered is closely related to “model-
targeted attacks”, studied in Suya et al. (2021). Recall also that, in this theory part, at each iteration,
local models θt
n are fully optimized, given ρt, in the manner of Dinh et al. (2020).
Theorem 1 (Equivalence between gradient attacks and data poisoning). Assume local PAC* learn-
ing, and ℓ2
2 or smooth-ℓ2 regularization. Assume also that Ln is convex and L-smooth for all users"
THE EQUIVALENCE BETWEEN DATA POISONING AND GRADIENT ATTACKS,0.04314477468839885,Under review as a conference paper at ICLR 2022
THE EQUIVALENCE BETWEEN DATA POISONING AND GRADIENT ATTACKS,0.04410354745925216,"n ∈[N], and that the learning rate ηt is constant and small enough. Consider any datasets ⃗D−s
provided by users n ̸= s. Then, for any target model θ†
s ∈Rd, there exists a converging gradient
attack of strategic user s (i.e. gt
s converges) such that ρt →θ†
s, if and only if, for any ε > 0, there
exists a dataset Ds such that
ρ∗( ⃗D) −θ†
s

2 ≤ε."
THE EQUIVALENCE BETWEEN DATA POISONING AND GRADIENT ATTACKS,0.045062320230105465,"Note that smoothness is used as a sufﬁcient condition to prove the convergence of federated gradient
descent. We now present our proof, which goes through the intermediary step of model attacks."
EQUIVALENCE BETWEEN DATA POISONING AND MODEL ATTACKS,0.046021093000958774,"3.1
EQUIVALENCE BETWEEN DATA POISONING AND MODEL ATTACKS"
EQUIVALENCE BETWEEN DATA POISONING AND MODEL ATTACKS,0.04697986577181208,"In this section, we prove the equivalence between data poisoning and model attacks."
EQUIVALENCE BETWEEN DATA POISONING AND MODEL ATTACKS,0.04793863854266539,"Lemma 1 (Reduction from model attack to data poisoning). Consider any data ⃗D and user s ∈[N].
Assume the global loss with datasets ⃗D has a global minimum (ρ∗, ⃗θ∗). Then (ρ∗, ⃗θ∗
−s) is also a
global minimum of the modiﬁed loss with datasets ⃗D−s and strategic reporting θ♠
s ≜θ∗
s( ⃗D)."
EQUIVALENCE BETWEEN DATA POISONING AND MODEL ATTACKS,0.0488974113135187,Proof. The proof is almost straightforward. It is given in Appendix B.1.
EQUIVALENCE BETWEEN DATA POISONING AND MODEL ATTACKS,0.04985618408437201,"Now, intuitively, by virtue of local PAC* learning, strategic user s can essentially guarantee that the
personalized learning framework will be learning θ∗
s ≈θ♠
s . But, a priori, it may seem unclear if this
will imply a biasing of the global model essentially identical to the one obtained through the model
attack that imposes θ∗
s = θ♠
s . In the sequel, we show that the answer is yes.
Lemma 2 (Reduction from data poisoning to model attack). Assume ℓ2
2, ℓ2 or smooth-ℓ2 regular-
ization, and assume local PAC* learning. Consider any datasets D−s and any attack model θ♠
s such
that the modiﬁed loss LOSSs has a unique minimum ρ∗(θ♠
s , ⃗D−s), ⃗θ∗
−s(θ♠
s , ⃗D−s). Then, for any
ε > 0, there exists a dataset Ds such that we have
ρ∗( ⃗D) −ρ∗(θ♠
s , ⃗D−s)

2 ≤ε and ∀n ̸= s,
θ∗
n( ⃗D) −θ∗
n(θ♠
s , ⃗D−s)

2 ≤ε.
(2)"
EQUIVALENCE BETWEEN DATA POISONING AND MODEL ATTACKS,0.05081495685522531,"Our proof in fact holds for all continuous regularizations R with R(ρ, θ) →∞as ∥ρ −θ∥2 →∞.
Moreover, note that the approximation guarantee holds for local models too."
EQUIVALENCE BETWEEN DATA POISONING AND MODEL ATTACKS,0.05177372962607862,"Sketch of proof. Given local PAC* learning, for a large dataset Ds constructed from θ♠
s , strategic
user s can guarantee θ∗
s( ⃗D) ≈θ♠
s . By carefully bounding the effect of the approximation on the loss
using the Heine-Cantor theorem, we show that this implies ρ∗( ⃗D) ≈ρ∗(θ♠
s , ⃗D−s) and θ∗
n( ⃗D) ≈
θ∗
n(θ♠
s , ⃗D−s) for all n ̸= s too. The precise analysis, given in Appendix B.2, is nontrivial."
EQUIVALENCE BETWEEN MODEL ATTACKS AND GRADIENT ATTACKS,0.052732502396931925,"3.2
EQUIVALENCE BETWEEN MODEL ATTACKS AND GRADIENT ATTACKS"
EQUIVALENCE BETWEEN MODEL ATTACKS AND GRADIENT ATTACKS,0.053691275167785234,"For the last few years, gradient attacks have been widely studied by the Byzantine learning literature.
Recently, Shejwalkar et al. (2021) argued that they are not a realistic threat model. Below, we prove
that, in our setting, they are actually as concerning as model attacks (and, thus, as data poisoning).
Lemma 3 (Reduction from model attack to gradient attack). Assume that Ln is convex and L-
smooth for all nodes n ∈[N], and that we use ℓ2
2 or smooth-ℓ2 regularization. If gt
s converges and
if ηt = η is a constant small enough, then ρt will converge too. Denote ρ∞its limit. Then for any
ε > 0, there is θ♠
s ∈Rd such that
ρ∞−ρ∗(θ♠
s , ⃗D−s)

2 ≤ε."
EQUIVALENCE BETWEEN MODEL ATTACKS AND GRADIENT ATTACKS,0.05465004793863854,"Sketch of proof. Denote g∞
s the limit of gt
s. Gradient descent then behaves as though it was minimiz-
ing the loss plus ρT g∞
s (and ignoring R(ρ, θs)). Essentially, classical gradient descent theory then
guarantees the convergence of ρt to ρ∞, though the precise proof is nontrivial. Then, since GRAD
is closed and g∞
s
∈GRAD, we can construct θ♠
s which approximately yields the gradient g∞
s . The
full proof (which also yields the necessary upper bound on η) is given in Appendix B.3."
EQUIVALENCE BETWEEN MODEL ATTACKS AND GRADIENT ATTACKS,0.05560882070949185,"Since any model attack can clearly be achieved by the corresponding honest gradient attack, we
conclude that model attacks and gradient attacks are essentially equivalent. In light of our previous
results, this implies that gradient attacks are essentially equivalent to data poisoning (Theorem 1)."
EQUIVALENCE BETWEEN MODEL ATTACKS AND GRADIENT ATTACKS,0.05656759348034516,Under review as a conference paper at ICLR 2022
EQUIVALENCE BETWEEN MODEL ATTACKS AND GRADIENT ATTACKS,0.05752636625119847,"4
EXAMPLES OF LOCALLY PAC* LEARNING SYSTEMS"
EQUIVALENCE BETWEEN MODEL ATTACKS AND GRADIENT ATTACKS,0.058485139022051776,"To the best of our knowledge, though similar to collaborative PAC learning (Blum et al., 2017),
local PAC* learnability is a new concept in the context of personalized federated learning. It is thus
important to show that it is not meaningless. To achieve this, in this section, we provide sufﬁcient
conditions for a personalized learning model to be locally PAC* learnable. First, we construct local
losses Ln as sums of losses per input, i.e."
EQUIVALENCE BETWEEN MODEL ATTACKS AND GRADIENT ATTACKS,0.059443911792905084,"Ln(θn, Dn) = ν ∥θn∥2
2 +
X"
EQUIVALENCE BETWEEN MODEL ATTACKS AND GRADIENT ATTACKS,0.06040268456375839,"x∈Dn
ℓ(θn, x),
(3)"
EQUIVALENCE BETWEEN MODEL ATTACKS AND GRADIENT ATTACKS,0.061361457334611694,"for some “loss per input” function ℓand some weight ν > 0. Appendix C gives theoretical and
empirical arguments are provided for using such a sum (as opposed to an expectation). Remarkably,
for linear or logistic regression, given such a loss, local PAC* learning can then be guaranteed.
Theorem 2 (Personalized least square linear regression is locally PAC* learning). Consider ℓ2
2, ℓ2
or smooth-ℓ2 regularization. Assume that, to generate a data xi, a user with preferred parameter
θ† ∈Rd ﬁrst independently draws a random vector query Qi ∈Rd from a sub-Gaussian query
distribution ˜Q, with parameter σQ and positive deﬁnite matrix Σ = E

QiQT
i

. Assume that the
user labels Qi with a real-valued answer Ai = QT
i θ† + ξi, where ξi is a zero-mean sub-Gaussian
random noise with parameter σξ, independent from Qi and other data points. Finally, assume that
ℓ(θ, Qi, Ai) = 1"
EQUIVALENCE BETWEEN MODEL ATTACKS AND GRADIENT ATTACKS,0.062320230105465,2(θT Qi −Ai)2. Then the personalized learning algorithm is locally PAC* learning.
EQUIVALENCE BETWEEN MODEL ATTACKS AND GRADIENT ATTACKS,0.06327900287631831,"Theorem 3 (Personalized logistic regression is locally PAC*-learning). Consider ℓ2
2, ℓ2 or smooth-
ℓ2 regularization. Assume that, to generate a data xi, a user with preferred parameter θ† ∈Rd"
EQUIVALENCE BETWEEN MODEL ATTACKS AND GRADIENT ATTACKS,0.06423777564717162,"ﬁrst independently draws a random vector query Qi ∈Rd from a query distribution ˜Q, whose
support SUPP( ˜Q) is bounded and spans the full vector space Rd. Assume that the user then labels
Qi with answer Ai = 1 with probability σ(QT
i θ†), and labels it Ai = −1 otherwise, where σ(z) =
(1+e−z)−1 is the sigmoid logistic function. Finally, assume that ℓ(θ, Qi, Ai) = −ln(σ(AiθT Qi)).
Then the personalized learning algorithm is locally PAC* learning."
PROOF SKETCH,0.06519654841802493,"4.1
PROOF SKETCH"
PROOF SKETCH,0.06615532118887824,"In this section, we outline the proofs of theorems 2 and 3, as they are interesting in their own sake.
The proofs both leverage the following notion, which intuitively means “robust PAC* learning”.
Deﬁnition 2 (Gradient-PAC*). Denote E(D, θ†, I, A, B, α) the event"
PROOF SKETCH,0.06711409395973154,"∀θ ∈Rd,
 
θ −θ†T ∇L (θ, D) ≥AI min
nθ −θ†
2 ,
θ −θ†2 2"
PROOF SKETCH,0.06807286673058485,"o
−BIα θ −θ†
2 .
(4)"
PROOF SKETCH,0.06903163950143816,"The loss ℓis gradient-PAC* if, for any K > 0, there exist constants AK, BK > 0 and αK < 1, such
that for any preferred model θ† ∈Rd with
θ†
2 ≤K, assuming that the dataset D is obtained by
honestly collecting and labeling I data points according to the preferred model θ†, the probability
of the event E(D, θ†, I, AK, BK, αK) goes to 1 as I →∞."
PROOF SKETCH,0.06999041227229147,"Intuitively, this deﬁnition asserts that, as we collect more data from an honest user, then, with high
probability, the gradient of the loss at any point θ too far from θ† will point away from θ†. In
particular, gradient descent is then essentially guaranteed to draw θ closer to θ†. The right-hand
side of equation 4 is subtly chosen to be strong enough to yield local PAC* learning guarantees, and
weak enough to be veriﬁed by linear and logistic regression, as proved by the following lemma.
Lemma 4. Logistic and linear regression, deﬁned in theorems 2 and 3, are gradient PAC* learning."
PROOF SKETCH,0.07094918504314478,"Sketch of proof. In the case of linear regression, remarkably, the discrepancy between the em-
pirical and the expected loss functions depends only on a few key random variables, such as
min SP

1
I
P QiQT
i

and P ξiQi, which can be controlled by appropriate concentration bounds."
PROOF SKETCH,0.07190795781399809,"Meanwhile, for logistic regression, for |b| ≤K, we observe that (a −b)(σ(a) −σ(b)) ≥
cK min(|a −b| , |a −b|2). Essentially, this proves that gradient-PAC* would hold if the empiri-
cal loss was replaced by the expected loss (times I). The actual proofs, however, are nontrivial,
especially in the case of logistic regression, which leverages topological considerations to derive a
critical uniform concentration bound. The full proofs are given in appendices D.2 and D.3."
PROOF SKETCH,0.0728667305848514,Under review as a conference paper at ICLR 2022
PROOF SKETCH,0.0738255033557047,"Now, under very mild assumptions on the regularization R (not even convexity!), which are veriﬁed
by the ℓ2
2, ℓ2 and smooth-ℓ2 regularizations, we prove that the gradient-PAC* learnability through ℓ
sufﬁces to guarantee that personalized learning will be locally PAC* learning.
Lemma 5. Assume R is (sub)differentiable and nonnegative, and R(ρ, θ) →∞as ∥ρ −θ∥2 →∞.
If ℓis gradient-PAC* and nonnegative, then personalized learning is locally PAC*-learning."
PROOF SKETCH,0.07478427612655801,"Sketch of proof. Once other users’ datasets are ﬁxed, the learning of an honest user’s model has a
ﬁxed biased due to R. But as the user provides more data, by gradient-PAC*, the local loss becomes
predominant, which guarantees local PAC*-learning. Appendix E provides a full proof."
PROOF SKETCH,0.07574304889741132,"Combining the two lemmas clearly yields theorems 2 and 3 as special cases. Note that our result
actually applies to a more general set of regularizations and a more general set of local losses."
THE CASE OF NEURAL NETWORKS,0.07670182166826463,"4.2
THE CASE OF NEURAL NETWORKS"
THE CASE OF NEURAL NETWORKS,0.07766059443911794,"Neural networks generally do not verify gradient PAC* learning. After all, because of symmetries
like neuron swapping, different values of the parameters compute the same neural network function.
Thus the notion of a “preferred model” θ† is arguably ill-deﬁned for neural networks2. Nevertheless,
we may consider a strategic user who only aims to bias the parameters of the last layer. In particular,
assuming that all layers but the last one of a neural network are pretrained and ﬁxed, then our theory
may apply to the parameters of the last layer, if it performs classiﬁcation or regression."
A PRACTICAL DATA POISONING ATTACK,0.07861936720997123,"5
A PRACTICAL DATA POISONING ATTACK"
A PRACTICAL DATA POISONING ATTACK,0.07957813998082454,"We now construct a practical data poisoning attack. We do so by introducing a new effective gradient
attack, and by then leveraging our equivalence to turn it into a data poisoning attack."
THE COUNTER-GRADIENT ATTACK,0.08053691275167785,"5.1
THE COUNTER-GRADIENT ATTACK"
THE COUNTER-GRADIENT ATTACK,0.08149568552253116,"We now deﬁne a simple, general and practical gradient attack, which we call the counter-gradient
attack (CGA). Intuitively, this attack estimates the sum g†,t
−s of the gradients of other users based on
its value at the previous iteration, which can be inferred from the way the global model ρt−1 was
updated into ρt. More precisely, apart from initialization ˆg1
−s ≜0, CGA makes the estimation"
THE COUNTER-GRADIENT ATTACK,0.08245445829338446,"ˆgt
−s ≜ρt−1 −ρt"
THE COUNTER-GRADIENT ATTACK,0.08341323106423777,"ηt−1
−gt−1
s
= g†,t−1
−s
.
(5)"
THE COUNTER-GRADIENT ATTACK,0.08437200383509108,"Strategic user s then reports the plausible gradient that moves the global model closest to the user’s
target model θ†
s, assuming others report ˆgt
−s. In other words, at every iteration, CGA reports"
THE COUNTER-GRADIENT ATTACK,0.08533077660594439,"gt
s ∈arg min
g∈GRAD(ρ)"
THE COUNTER-GRADIENT ATTACK,0.0862895493767977,"ρt −ηt(ˆgt
−s + g) −θ†
s

2 .
(6)"
THE COUNTER-GRADIENT ATTACK,0.087248322147651,"Note that, to perform this attack, user s only needs to know the previous and current learning rates
ηt−1 and ηt, the previous and current global models ρt−1 and ρt, and their target model θ†
s."
THE COUNTER-GRADIENT ATTACK,0.08820709491850431,"Computation of CGA.
Deﬁne ht
s ≜gt−1
s
+ ρt−θ†
s
ηt
−ρt−1−ρt"
THE COUNTER-GRADIENT ATTACK,0.08916586768935762,"ηt−1
. For convex sets GRAD(ρt), it
is straightforward to see that CGA boils down to computing the orthogonal projection of ht
s on
GRAD(ρt). This yields very simple computations for ℓ2
2, ℓ2 and smooth-ℓ2 regularizations.
Proposition 1. For ℓ2
2 regularization, CGA reports gt
s = ht
s. For ℓ2 or smooth-ℓ2 regularization,
CGA reports gt
s = ht
s min {1, λ/ ∥ht
s∥2}."
THE COUNTER-GRADIENT ATTACK,0.09012464046021093,"Proof. Equation (6) boils down to minimizing the distance between ρt−θ†
s
ηt
−ˆgt
−s and GRAD(ρ),
which is the (convex) ball B(0, λ). This minimum is the orthogonal projection on B(0, λ)."
THE COUNTER-GRADIENT ATTACK,0.09108341323106424,"2Evidently, our deﬁnition could be easily modiﬁed to give importance to the computed function, rather than
to the parameters of the model."
THE COUNTER-GRADIENT ATTACK,0.09204218600191755,Under review as a conference paper at ICLR 2022
THE COUNTER-GRADIENT ATTACK,0.09300095877277086,"Theoretical analysis.
We now prove that CGA is perfectly successful against ℓ2
2 regularization.
To do so, we suppose that, at each iteration t and for each user n ̸= s, the local models θn are fully
optimized with respect to ρt, and the honest gradients of g†,t
n are used for the gradient descent of ρ."
THE COUNTER-GRADIENT ATTACK,0.09395973154362416,"Theorem 4. Consider ℓ2
2 regularization. Assume that ℓis convex and Lℓ-smooth, and that ηt = η
is small enough. Then CGA is converging and optimal, as ρt →θ†
s."
THE COUNTER-GRADIENT ATTACK,0.09491850431447747,"Sketch of proof. The main challenge is to guarantee that the other users’ gradients g†,t
n
for n ̸= s
remain sufﬁciently stable over time to guarantee convergence, which can be done by leveraging
L-smoothness. The full proof, with the necessary upper-bound on η, is given in Appendix F."
THE COUNTER-GRADIENT ATTACK,0.09587727708533078,"The analysis of the convergence against smooth-ℓ2 is unfortunately signiﬁcantly more challenging.
Here, we simply make a remark about what CGA yields in this case, if it converges."
THE COUNTER-GRADIENT ATTACK,0.09683604985618409,"Proposition 2. If CGA against smooth-ℓ2 regularization converges for ηt = η, then it either
achieves perfect manipulation, or it is eventually partially honest, in the sense that the gradient
by CGA correctly points towards θ†
s."
THE COUNTER-GRADIENT ATTACK,0.0977948226270374,"Proof. Denote P the projection onto the closed ball B(0, λ). If CGA converges, then, by Proposi-"
THE COUNTER-GRADIENT ATTACK,0.0987535953978907,"tion 1, P

g∞
s + ρ∞−θ†
s
η

= g∞
s . This implies in particular that ρ∞−θ†
s and g∞
s must be colinear."
THE COUNTER-GRADIENT ATTACK,0.09971236816874401,"If perfect manipulation is not achieved (i.e. ρ∞̸= θ†
s), then we must have g∞
s = λ
ρ∞−θ†
s
∥ρ∞−θ†
s∥2
."
THE COUNTER-GRADIENT ATTACK,0.10067114093959731,"0
25
50
75
100
125
150
175
200
Epochs 0.0 0.2 0.4 0.6 0.8 1.0"
THE COUNTER-GRADIENT ATTACK,0.10162991371045062,Accuracy
THE COUNTER-GRADIENT ATTACK,0.10258868648130393,"L2 squared
L2"
THE COUNTER-GRADIENT ATTACK,0.10354745925215723,"Figure 1: Accuracy of the global
model under attack by CGA."
THE COUNTER-GRADIENT ATTACK,0.10450623202301054,"Empirical evaluation of CGA.
We deployed CGA to bias the
federated learning of MNIST. We consider a strategic user whose
target model is one that labels 0’s as 1’s, 1’s as 2’s, and so on,
until 9’s that are labeled as 0’s. In particular, this target model
has a nil accuracy. Figure 1 shows that such a user effectively
hacks the ℓ2
2 regularization against 10 honest users who each
have 6,000 data points of MNIST, in the case where local mod-
els only undergo a single gradient step at each iteration, but fails
to hack the ℓ2 regularization. Further details on the experiment
are given in Appendix G. We also ran a similar successful attack
on the last layer of a deep neural network trained on cifar-10,
which is detailed in Appendix J. The Appendix also discusses
the extent to which the attack may be turned into data poisoning."
THE COUNTER-GRADIENT ATTACK,0.10546500479386385,"5.2
FROM GRADIENT ATTACK TO MODEL ATTACK AGAINST ℓ2
2"
THE COUNTER-GRADIENT ATTACK,0.10642377756471716,"We now show how to turn a gradient attack into model attack, against ℓ2
2 regularization. Assume
that we found a gradient g∞
s such that ρ∞= θ†
s. It is trivial to transform it into a model attack by
setting θ♠
s ≜θ†
s −1"
THE COUNTER-GRADIENT ATTACK,0.10738255033557047,"2g∞
s , as guaranteed by the following result, and as depicted by Figure 2."
THE COUNTER-GRADIENT ATTACK,0.10834132310642378,"0
50
100
150
200
250
Epochs 5 10 15 20 25 30 35"
THE COUNTER-GRADIENT ATTACK,0.10930009587727708,l2 norm
THE COUNTER-GRADIENT ATTACK,0.11025886864813039,Model attack
THE COUNTER-GRADIENT ATTACK,0.1112176414189837,"l2_dist
l2_norm
target_dist"
THE COUNTER-GRADIENT ATTACK,0.11217641418983701,"(a) Distance between the global model ρt and
the target model θ†
s (target dist)."
THE COUNTER-GRADIENT ATTACK,0.11313518696069032,"0
50
100
150
200
250
Epochs 0.0 0.2 0.4 0.6 0.8 1.0"
THE COUNTER-GRADIENT ATTACK,0.11409395973154363,Accuracy
THE COUNTER-GRADIENT ATTACK,0.11505273250239693,Model attack
THE COUNTER-GRADIENT ATTACK,0.11601150527325024,acc_glob
THE COUNTER-GRADIENT ATTACK,0.11697027804410355,"(b) Accuracy of ρt according to θ†
s (which rela-
bels 0 →1 →2 →... →9 →0)."
THE COUNTER-GRADIENT ATTACK,0.11792905081495686,"Figure 2: Successful model attack against ℓ2
2 by combining CGA and Proposition 3."
THE COUNTER-GRADIENT ATTACK,0.11888782358581017,Under review as a conference paper at ICLR 2022
THE COUNTER-GRADIENT ATTACK,0.11984659635666348,"Proposition 3. Consider the ℓ2
2 regularization. Suppose that gt
s →g∞
s
and ρt →θ†
s, with a
constant learning rate ηt = η. Then, under the model attack θ♠
s ≜θ†
s −
1
2λ g∞
s , the gradient at"
THE COUNTER-GRADIENT ATTACK,0.12080536912751678,"ρ = θ†
s vanishes, i.e. ∇ρLOSS(θ†
s, ⃗θ∗
−s(θ†
s, ⃗D−s), θ♠
s , D−s) = 0."
THE COUNTER-GRADIENT ATTACK,0.12176414189837009,"Proof. Given that the learning rate is constant, the convergence ρt →θ†
s implies that the sum of
honest users’ gradients at ρ = θ†
s must equal −g∞
s . Therefore, to achieve ρ∗= θ†
s with a model
attack, it sufﬁces to send a model θ♠
s such that the gradient of λ
ρ −θ♠
s
2
2 with respect to ρ at
ρ = θ†
s equals g∞
s . Since the gradient is λ(θ†
s −θ♠
s ), θ♠
s ≜θ†
s −
1
2λ g∞
s does the trick."
THE COUNTER-GRADIENT ATTACK,0.12272291466922339,"5.3
FROM MODEL ATTACK TO DATA POISONING AGAINST ℓ2
2"
THE COUNTER-GRADIENT ATTACK,0.1236816874400767,"The case of linear regression.
In linear regression, any model attack can be turned into a single
data poisoning attack, as proved by the following theorem whose proof is given in Appendix H."
THE COUNTER-GRADIENT ATTACK,0.12464046021093,"Theorem 5. Consider the ℓ2
2 regularization and linear regression. For any data D−s and any target
value θ†
s, there is a datapoint (Q, A) to be injected by user s such that ρ∗({(Q, A)} , D−s) = θ†
s."
THE COUNTER-GRADIENT ATTACK,0.12559923298178333,"The case of linear classiﬁcation.
We now consider linear classiﬁcation, with the case of MNIST.
By Theorem 2, any model attack can be turned into data poisoning, provided sufﬁciently many
(random) data points are labeled by the strategic user. However, this may require creating too many
data labelings, especially if the norm of θ♠
s is large (which is the case if s is alone against many
active users), as suggested by Theorem 3."
THE COUNTER-GRADIENT ATTACK,0.12655800575263662,"For efﬁcient data poisoning, deﬁne the indifference afﬁne subspace V ⊂Rd as the set of images
with equiprobable labels. Intuitively, labeling images close to V is very informative, as it informs us
directly about the separating hyperplanes. To generate images, we ﬁrst draw random images, which
we then project orthogonally on V . We then add a small noise, before probabilistically labeling the
image with model θ♠
s . Note that this leads us to consider images not in [0, 1]d. Figure 3 shows the
effectiveness of the resulting data poisoning attack, with only 2,000 data points, as opposed to the
other nodes’ 6,000 data points. More details and explanations are provided in Appendix I."
THE COUNTER-GRADIENT ATTACK,0.12751677852348994,"0
50
100
150
200
250
Epochs 5 10 15 20 25 30 35"
THE COUNTER-GRADIENT ATTACK,0.12847555129434324,l2 norm
THE COUNTER-GRADIENT ATTACK,0.12943432406519656,Data attack
THE COUNTER-GRADIENT ATTACK,0.13039309683604985,"l2_dist
l2_norm
target_dist"
THE COUNTER-GRADIENT ATTACK,0.13135186960690318,"(a) Distance between the global model ρt and
the target model θ†
s (target dist)."
THE COUNTER-GRADIENT ATTACK,0.13231064237775647,"0
50
100
150
200
250
Epochs 0.0 0.2 0.4 0.6 0.8 1.0"
THE COUNTER-GRADIENT ATTACK,0.1332694151486098,Accuracy
THE COUNTER-GRADIENT ATTACK,0.1342281879194631,Data attack
THE COUNTER-GRADIENT ATTACK,0.13518696069031638,acc_glob
THE COUNTER-GRADIENT ATTACK,0.1361457334611697,"(b) Accuracy of ρt according to θ†
s (which rela-
bels 0 →1 →2 →... →9 →0)."
THE COUNTER-GRADIENT ATTACK,0.137104506232023,"Figure 3: Successful data attack against ℓ2
2 by the efﬁcient data generation scheme."
CONCLUDING REMARKS,0.13806327900287632,"6
CONCLUDING REMARKS"
CONCLUDING REMARKS,0.13902205177372962,"We showed in this paper that, unlike what has been argued by, e.g., Shejwalkar et al. (2021), the gra-
dient attack threat is not unrealistic. For personalized federated learning with local PAC* guarantees,
gradient attacks are in fact just as realistic and harmful as strategic data reporting. More generally,
our work stresses how critical Byzantine learning research is. For instance, El-Mhamdi et al. (2021)
proved lower bounds on what any collaborative learning algorithm can guarantee in heterogeneous
environments, under Byzantine gradient attacks. Our work implies that, at least for certain person-
alized federated learning problems, these lower bounds also hold for data poisoning attacks, which
are known to be common for many high-risk applications. Arguably, a lot more security measures
are urgently needed to make large-scale learning algorithms safe."
CONCLUDING REMARKS,0.13998082454458294,Under review as a conference paper at ICLR 2022
ETHICS STATEMENT,0.14093959731543623,ETHICS STATEMENT
ETHICS STATEMENT,0.14189837008628955,"The safety of algorithms is arguably a prerequisite to their ethics. After all, an arbitrarily manip-
ulable large-scale algorithm will unavoidably endanger the targets of the entities that successfully
design such algorithms. Typically, unsafe large-scale recommendation algorithms may be hacked by
health disinformation campaigns that aim to promote non-certiﬁed products, e.g., by falsely pretend-
ing that they cure COVID-19. Such algorithms must not be regarded as ethical, even if they were
designed with the best intentions. We believe that our work helps understand the vulnerabilities of
such algorithms, and will motivate further research in the ethics and security of machine learning."
REPRODUCIBILITY STATEMENT,0.14285714285714285,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.14381591562799617,"All our experiments are run on the classical datasets MNIST and FashionMNIST. We provide all of
the source codes to reproduce the experiments:"
REPRODUCIBILITY STATEMENT,0.14477468839884947,"• The sum versus expectation experiments can be run by executing this ﬁle:
https://www.dropbox.com/sh/qdgmz9air24nhyr/AAAtycEkxc_
1hGbvU5YG18z4a?dl=0
• The counter-gradient attack experiments can be run by executing this ﬁle:
https://www.dropbox.com/sh/bycqkccgmk4muzn/
AACRD1yeTglLSHEd1OOAzmVqa?dl=0
• The data poisoning attack experiments can be run by executing this ﬁle:
https://www.dropbox.com/sh/qodnl6ivzti8hch/
AADgX4EYuSOotiMCAHyTIiGMa?dl=0
• The cifar10 on VGG 13-BN experiments can be run by executing this ﬁle:
https://www.dropbox.com/sh/5mw5c9dt25eiav7/AAC0jP3e2kuh_
zvLudWbmDl-a?dl=0"
REPRODUCIBILITY STATEMENT,0.1457334611697028,"The experiments are seeded and the CuDNN backend is conﬁgured in deterministic mode in order to
reduce the sources of non-determinism. We also turn of the benchmark mode. Executing the codes
will generate the ﬁgures and statistics of our main paper, and most of the ﬁgures of our Appendix.
Our other ﬁgures can be obtained by adjusting the hyperparameters of our codes."
REPRODUCIBILITY STATEMENT,0.14669223394055608,"The full description of the architecture and optimisation algorithm used is described in Appendix C.
The experimental setup details of each experiment are provided in the Appendix, along with addi-
tional results. The Appendix also contains the full proofs of our theorems."
REFERENCES,0.1476510067114094,REFERENCES
REFERENCES,0.1486097794822627,"Hojjat Aghakhani, Dongyu Meng, Yu-Xiang Wang, Christopher Kruegel, and Giovanni Vigna.
Bullseye polytope: A scalable clean-label poisoning attack with improved transferability, 2021."
REFERENCES,0.14956855225311602,"Marco Barreno, Blaine Nelson, Russell Sears, Anthony D. Joseph, and J. D. Tygar. Can machine
learning be secure? In Proceedings of the 2006 ACM Symposium on Information, Computer and
Communications Security, ASIACCS ’06, pp. 16–25, New York, NY, USA, 2006. Association
for Computing Machinery. ISBN 1595932720. doi: 10.1145/1128817.1128824. URL https:
//doi.org/10.1145/1128817.1128824."
REFERENCES,0.15052732502396932,"Gilad Baruch, Moran Baruch, and Yoav Goldberg. A little is enough: Circumventing defenses for
distributed learning. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran As-
sociates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
ec1c59141046cd1866bbbcdfb6ae31d4-Paper.pdf."
REFERENCES,0.15148609779482264,"Omer Ben-Porat and Moshe Tennenholtz.
Best response regression.
In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 30. Curran Asso-
ciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
1ce927f875864094e3906a4a0b5ece68-Paper.pdf."
REFERENCES,0.15244487056567593,Under review as a conference paper at ICLR 2022
REFERENCES,0.15340364333652926,"Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the
dangers of stochastic parrots: Can language models be too big?
In Madeleine Clare Elish,
William Isaac, and Richard S. Zemel (eds.), FAccT ’21: 2021 ACM Conference on Fairness,
Accountability, and Transparency, Virtual Event / Toronto, Canada, March 3-10, 2021, pp. 610–
623. ACM, 2021. doi: 10.1145/3442188.3445922. URL https://doi.org/10.1145/
3442188.3445922."
REFERENCES,0.15436241610738255,"Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against support vector ma-
chines.
In Proceedings of the 29th International Conference on Machine Learning, ICML
2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012. icml.cc / Omnipress, 2012.
URL
http://icml.cc/2012/papers/880.pdf."
REFERENCES,0.15532118887823587,"Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer.
Machine
learning with adversaries:
Byzantine tolerant gradient descent.
In Isabelle Guyon, Ul-
rike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan,
and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30:
An-
nual Conference on Neural Information Processing Systems 2017, 4-9 December 2017,
Long Beach, CA, USA, pp. 119–129, 2017.
URL http://papers.nips.cc/paper/
6617-machine-learning-with-adversaries-byzantine-tolerant-gradient-descent."
REFERENCES,0.15627996164908917,"Avrim Blum, Nika Haghtalab, Ariel D. Procaccia, and Mingda Qiao. Collaborative PAC learning. In
Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vish-
wanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30:
Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long
Beach, CA, USA, pp. 2392–2401, 2017.
URL https://proceedings.neurips.cc/
paper/2017/hash/186a157b2992e7daed3677ce8e9fe40f-Abstract.html."
REFERENCES,0.15723873441994246,"Samantha Bradshaw and Philip N Howard. The global disinformation order: 2019 global inventory
of organised social media manipulation. Project on Computational Propaganda, 2019."
REFERENCES,0.15819750719079578,"Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-
wal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,
Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler,
Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCan-
dlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
Language models are few-shot
learners.
In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan,
and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html."
REFERENCES,0.15915627996164908,"Yang Cai, Constantinos Daskalakis, and Christos H. Papadimitriou. Optimum statistical estimation
with strategic data sources. In Peter Gr¨unwald, Elad Hazan, and Satyen Kale (eds.), Proceedings
of The 28th Conference on Learning Theory, COLT 2015, Paris, France, July 3-6, 2015, vol-
ume 40 of JMLR Workshop and Conference Proceedings, pp. 280–296. JMLR.org, 2015. URL
http://proceedings.mlr.press/v40/Cai15.html."
REFERENCES,0.1601150527325024,"Jiecao Chen, Qin Zhang, and Yuan Zhou.
Tight bounds for collaborative PAC learn-
ing via multiplicative weights.
In Samy Bengio, Hanna M. Wallach, Hugo Larochelle,
Kristen Grauman, Nicol`o Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neu-
ral Information Processing Systems 31:
Annual Conference on Neural Information Pro-
cessing Systems 2018,
NeurIPS 2018,
December 3-8,
2018,
Montr´eal,
Canada,
pp.
3602–3611, 2018a. URL https://proceedings.neurips.cc/paper/2018/hash/
ed519dacc89b2bead3f453b0b05a4a8b-Abstract.html."
REFERENCES,0.1610738255033557,"Yiling Chen, Chara Podimata, Ariel D. Procaccia, and Nisarg Shah. Strategyproof linear regres-
sion in high dimensions. In Proceedings of the 2018 ACM Conference on Economics and Com-
putation, EC ’18, pp. 9–26, New York, NY, USA, 2018b. Association for Computing Machin-
ery. ISBN 9781450358293. doi: 10.1145/3219166.3219175. URL https://doi.org/10.
1145/3219166.3219175."
REFERENCES,0.16203259827420902,Under review as a conference paper at ICLR 2022
REFERENCES,0.1629913710450623,"Yiling Chen, Yang Liu, and Chara Podimata.
Learning strategy-aware linear classiﬁers.
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances
in Neural Information Processing Systems, volume 33, pp. 15265–15276. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
ae87a54e183c075c494c4d397d126a66-Paper.pdf."
REFERENCES,0.16395014381591563,"Jiazhu Dai, Chuanshuai Chen, and Yufeng Li. A backdoor attack against lstm-based text classiﬁ-
cation systems. IEEE Access, 7:138872–138878, 2019. doi: 10.1109/ACCESS.2019.2941376.
URL https://doi.org/10.1109/ACCESS.2019.2941376."
REFERENCES,0.16490891658676893,"Ofer Dekel, Felix Fischer, and Ariel D. Procaccia. Incentive compatible regression learning. Jour-
nal of Computer and System Sciences, 76(8):759–777, 2010. ISSN 0022-0000. doi: https://doi.
org/10.1016/j.jcss.2010.03.003.
URL https://www.sciencedirect.com/science/
article/pii/S0022000010000309."
REFERENCES,0.16586768935762225,"Canh T. Dinh, Nguyen H. Tran, and Tuan Dung Nguyen. Personalized federated learning with
moreau envelopes. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Bal-
can, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
f4f1f13c8289ac1b1ee0ff176b56fc60-Abstract.html."
REFERENCES,0.16682646212847554,"El-Mahdi El-Mhamdi, Rachid Guerraoui, Arsany Guirguis, Lˆe Nguyˆen Hoang, and S´ebastien
Rouault.
Genuinely distributed byzantine machine learning.
In Yuval Emek and Christian
Cachin (eds.), PODC ’20: ACM Symposium on Principles of Distributed Computing, Virtual
Event, Italy, August 3-7, 2020, pp. 355–364. ACM, 2020. doi: 10.1145/3382734.3405695. URL
https://doi.org/10.1145/3382734.3405695."
REFERENCES,0.16778523489932887,"El-Mahdi El-Mhamdi, Sadegh Farhadkhani, Rachid Guerraoui, Arsany Guirguis, Lˆe Nguyˆen Hoang,
and S´ebastien Rouault. Collaborative learning in the jungle (decentralized, byzantine, heteroge-
neous, asynchronous and nonconvex learning). In Advances in Neural Information Processing
Systems 34: Annual Conference on Neural Information Processing Systems 2021, December 6-
14, 2021, 2021."
REFERENCES,0.16874400767018216,"El-Mahdi El-Mhamdi, Rachid Guerraoui, and S´ebastien Rouault.
Distributed momentum for
byzantine-resilient stochastic gradient descent.
In 9th International Conference on Learning
Representations, ICLR 2021, Vienna, Austria, May 4–8, 2021. OpenReview.net, 2021.
URL
https://openreview.net/forum?id=H8UHdhWG6A3."
REFERENCES,0.16970278044103548,"Alireza Fallah, Aryan Mokhtari, and Asuman E. Ozdaglar.
Personalized federated learn-
ing with theoretical guarantees:
A model-agnostic meta-learning approach.
In Hugo
Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien
Lin (eds.), Advances in Neural Information Processing Systems 33:
Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
virtual,
2020.
URL https://proceedings.neurips.cc/paper/2020/hash/
24389bfe4fe2eba8bf9aa9203a44cdad-Abstract.html."
REFERENCES,0.17066155321188878,"William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion param-
eter models with simple and efﬁcient sparsity. CoRR, abs/2101.03961, 2021. URL https:
//arxiv.org/abs/2101.03961."
REFERENCES,0.1716203259827421,"Brian Fung and Ahiza Garcia. Facebook has shut down 5.4 billion fake accounts this year. CNN
Business, 2019."
REFERENCES,0.1725790987535954,"Jonas Geiping, Liam H Fowl, W. Ronny Huang, Wojciech Czaja, Gavin Taylor, Michael Moeller,
and Tom Goldstein. Witches’ brew: Industrial scale data poisoning via gradient matching. In
International Conference on Learning Representations, 2021. URL https://openreview.
net/forum?id=01olnfLIbD."
REFERENCES,0.17353787152444872,"Filip Hanzely and Peter Richt´arik. Federated learning of a mixture of global and local models, 2021."
REFERENCES,0.174496644295302,Under review as a conference paper at ICLR 2022
REFERENCES,0.17545541706615533,"Filip Hanzely, Slavom´ır Hanzely, Samuel Horv´ath, and Peter Richt´arik.
Lower bounds
and
optimal
algorithms
for
personalized
federated
learning.
In
Hugo
Larochelle,
Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.),
Advances in Neural Information Processing Systems 33:
Annual Conference on Neu-
ral Information Processing Systems 2020,
NeurIPS 2020,
December 6-12,
2020,
vir-
tual,
2020.
URL
https://proceedings.neurips.cc/paper/2020/hash/
187acf7982f3c169b3075132380986e4-Abstract.html."
REFERENCES,0.17641418983700863,"Moritz Hardt, Nimrod Megiddo, Christos Papadimitriou, and Mary Wootters.
Strategic classi-
ﬁcation.
In Proceedings of the 2016 ACM Conference on Innovations in Theoretical Com-
puter Science, ITCS ’16, pp. 111–122, New York, NY, USA, 2016. Association for Com-
puting Machinery.
ISBN 9781450340571.
doi: 10.1145/2840728.2840730.
URL https:
//doi.org/10.1145/2840728.2840730."
REFERENCES,0.17737296260786195,"Lˆe Nguyˆen Hoang. Science communication desperately needs more aligned recommendation algo-
rithms. Frontiers in Communication, 5:115, 2020."
REFERENCES,0.17833173537871524,"Lˆe Nguyˆen Hoang, Louis Faucon, and El-Mahdi El-Mhamdi. Recommendation algorithms, a ne-
glected opportunity for public health. Revue M´edecine et Philosophie, 4(2):16–24, 2021."
REFERENCES,0.17929050814956854,"Roger A. Horn and Charles R. Johnson. Matrix Analysis. Cambridge University Press, 2 edition,
2012. doi: 10.1017/9781139020411."
REFERENCES,0.18024928092042186,"W. Ronny Huang, Jonas Geiping, Liam Fowl, Gavin Taylor, and Tom Goldstein.
Metapoi-
son:
Practical
general-purpose
clean-label
data
poisoning.
In
Hugo
Larochelle,
Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.),
Advances in Neural Information Processing Systems 33:
Annual Conference on Neu-
ral Information Processing Systems 2020,
NeurIPS 2020,
December 6-12,
2020,
vir-
tual,
2020.
URL
https://proceedings.neurips.cc/paper/2020/hash/
8ce6fc704072e351679ac97d4a985574-Abstract.html."
REFERENCES,0.18120805369127516,"Eugene Ie, Vihan Jain, Jing Wang, Sanmit Narvekar, Ritesh Agarwal, Rui Wu, Heng-Tze Cheng,
Tushar Chandra, and Craig Boutilier.
Slateq: A tractable decomposition for reinforcement
learning with recommendation sets.
In Sarit Kraus (ed.), Proceedings of the Twenty-Eighth
International Joint Conference on Artiﬁcial Intelligence, IJCAI 2019, Macao, China, August
10-16, 2019, pp. 2592–2599. ijcai.org, 2019.
doi: 10.24963/ijcai.2019/360.
URL https:
//doi.org/10.24963/ijcai.2019/360."
REFERENCES,0.18216682646212848,"Ayush Jain and Alon Orlitsky.
A general method for robust learning from batches.
In
Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-
Tien Lin (eds.), Advances in Neural Information Processing Systems 33:
Annual Con-
ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
f7a82ce7e16d9687e7cd9a9feb85d187-Abstract.html."
REFERENCES,0.18312559923298177,"Deborah G. Johnson and Nicholas Diakopoulos. What to do about deepfakes. Commun. ACM, 64
(3):33–35, 2021. doi: 10.1145/3447255. URL https://doi.org/10.1145/3447255."
REFERENCES,0.1840843720038351,"Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aur´elien Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G. L.
D’Oliveira, Hubert Eichner, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Garrett,
Adri`a Gasc´on, Badih Ghazi, Phillip B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He,
Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi,
Mikhail Khodak, Jakub Koneˇcn´y, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo,
Tancr`ede Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer ¨Ozg¨ur, Rasmus
Pagh, Mariana Raykova, Hang Qi, Daniel Ramage, Ramesh Raskar, Dawn Song, Weikang Song,
Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tram`er, Praneeth Vepakomma,
Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. Advances
and open problems in federated learning, 2021."
REFERENCES,0.1850431447746884,"Jakub Konecn´y, Brendan McMahan, and Daniel Ramage. Federated optimization: Distributed op-
timization beyond the datacenter. CoRR, abs/1511.03575, 2015. URL http://arxiv.org/
abs/1511.03575."
REFERENCES,0.1860019175455417,Under review as a conference paper at ICLR 2022
REFERENCES,0.186960690316395,"Nikola Konstantinov, Elias Frantar, Dan Alistarh, and Christoph Lampert. On the sample complexity
of adversarial multi-source PAC learning. In Proceedings of the 37th International Conference
on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings
of Machine Learning Research, pp. 5416–5425. PMLR, 2020. URL http://proceedings.
mlr.press/v119/konstantinov20a.html."
REFERENCES,0.18791946308724833,"Ram Shankar Siva Kumar, Magnus Nystr¨om, John Lambert, Andrew Marshall, Mario Goertzel,
Andi Comissoneru, Matt Swann, and Sharon Xia. Adversarial machine learning-industry per-
spectives. In 2020 IEEE Security and Privacy Workshops, SP Workshops, San Francisco, CA,
USA, May 21, 2020, pp. 69–75. IEEE, 2020.
doi: 10.1109/SPW50608.2020.00028.
URL
https://doi.org/10.1109/SPW50608.2020.00028."
REFERENCES,0.18887823585810162,"Florian Lehmann and Daniel Buschek. Examining autocompletion as a basic concept for interaction
with generative AI. i-com, 19(3):251–264, 2021. doi: 10.1515/icom-2020-0025. URL https:
//doi.org/10.1515/icom-2020-0025."
REFERENCES,0.18983700862895495,"Saeed Mahloujifar, Mohammad Mahmoody, and Ameer Mohammed.
Data poisoning attacks
in multi-party learning.
In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceed-
ings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June
2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Re-
search, pp. 4274–4283. PMLR, 2019.
URL http://proceedings.mlr.press/v97/
mahloujifar19a.html."
REFERENCES,0.19079578139980824,"Guangcan Mai, Kai Cao, Pong C. Yuen, and Anil K. Jain. On the reconstruction of face images
from deep face templates.
IEEE Trans. Pattern Anal. Mach. Intell., 41(5):1188–1202, 2019.
doi: 10.1109/TPAMI.2018.2827389. URL https://doi.org/10.1109/TPAMI.2018.
2827389."
REFERENCES,0.19175455417066156,"Kris McGufﬁe and Alex Newhouse. The radicalization risks of GPT-3 and advanced neural language
models. CoRR, abs/2009.06807, 2020. URL https://arxiv.org/abs/2009.06807."
REFERENCES,0.19271332694151486,"Reshef Meir, Shaull Almagor, Assaf Michaely, and Jeffrey S. Rosenschein. Tight bounds for strat-
egyproof classiﬁcation. In The 10th International Conference on Autonomous Agents and Multi-
agent Systems - Volume 1, AAMAS ’11, pp. 319–326, Richland, SC, 2011. International Founda-
tion for Autonomous Agents and Multiagent Systems. ISBN 0982657153."
REFERENCES,0.19367209971236818,"Reshef Meir, Ariel D. Procaccia, and Jeffrey S. Rosenschein. Algorithms for strategyproof classiﬁ-
cation. Artiﬁcial Intelligence, 186:123–156, 2012. ISSN 0004-3702. doi: https://doi.org/10.1016/
j.artint.2012.03.008. URL https://www.sciencedirect.com/science/article/
pii/S000437021200029X."
REFERENCES,0.19463087248322147,"El Mahdi El Mhamdi, Rachid Guerraoui, and S´ebastien Rouault. The hidden vulnerability of dis-
tributed learning in byzantium. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the
35th International Conference on Machine Learning, ICML 2018, Stockholmsm¨assan, Stockholm,
Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 3518–
3527. PMLR, 2018. URL http://proceedings.mlr.press/v80/mhamdi18a.html."
REFERENCES,0.1955896452540748,"Luis Mu˜noz-Gonz´alez, Battista Biggio, Ambra Demontis, Andrea Paudice, Vasin Wongrassamee,
Emil C. Lupu, and Fabio Roli. Towards poisoning of deep learning algorithms with back-gradient
optimization.
In Bhavani M. Thuraisingham, Battista Biggio, David Mandell Freeman, Brad
Miller, and Arunesh Sinha (eds.), Proceedings of the 10th ACM Workshop on Artiﬁcial Intelli-
gence and Security, AISec@CCS 2017, Dallas, TX, USA, November 3, 2017, pp. 27–38. ACM,
2017.
doi: 10.1145/3128572.3140451.
URL https://doi.org/10.1145/3128572.
3140451."
REFERENCES,0.1965484180249281,"Lisa-Maria Neudert, Philip Howard, and Bence Kollanyi.
Sourcing and automation of polit-
ical news and information during three european elections.
Social Media+ Society, 5(3):
2056305119863147, 2019."
REFERENCES,0.1975071907957814,"Huy L. Nguyen and Lydia Zakynthinou. Improved algorithms for collaborative PAC learning. In
Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol`o Cesa-Bianchi, and"
REFERENCES,0.1984659635666347,Under review as a conference paper at ICLR 2022
REFERENCES,0.19942473633748803,"Roman Garnett (eds.), Advances in Neural Information Processing Systems 31: Annual Con-
ference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018,
Montr´eal, Canada, pp. 7642–7650, 2018. URL https://proceedings.neurips.cc/
paper/2018/hash/3569df159ec477451530c4455b2a9e86-Abstract.html."
REFERENCES,0.20038350910834132,"Javier Perote and Juan Perote-Pe˜na.
Strategy-proof estimators for simple regression.
Math-
ematical Social Sciences, 47(2):153–176, 2004.
ISSN 0165-4896.
doi: https://doi.org/10.
1016/S0165-4896(03)00085-4.
URL https://www.sciencedirect.com/science/
article/pii/S0165489603000854."
REFERENCES,0.20134228187919462,"Javier Perote and Olavide Sevilla. The impossibility of strategy-proof clustering. Economics Bul-
letin, 2003."
REFERENCES,0.20230105465004794,"Huy Phan. huyvnphan/pytorch cifar10, January 2021. URL https://doi.org/10.5281/
zenodo.4431043."
REFERENCES,0.20325982742090123,"Mingda Qiao. Do outliers ruin collaboration? In Jennifer G. Dy and Andreas Krause (eds.), Proceed-
ings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm¨assan,
Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Re-
search, pp. 4177–4184. PMLR, 2018.
URL http://proceedings.mlr.press/v80/
qiao18a.html."
REFERENCES,0.20421860019175456,"Francesco Ricci, Lior Rokach, and Bracha Shapira. Introduction to recommender systems hand-
book.
In Francesco Ricci, Lior Rokach, Bracha Shapira, and Paul B. Kantor (eds.), Recom-
mender Systems Handbook, pp. 1–35. Springer, 2011. doi: 10.1007/978-0-387-85820-3\ 1. URL
https://doi.org/10.1007/978-0-387-85820-3_1."
REFERENCES,0.20517737296260785,"Avi Schwarzschild, Micah Goldblum, Arjun Gupta, John P Dickerson, and Tom Goldstein.
Just how toxic is data poisoning? a uniﬁed benchmark for backdoor and data poisoning at-
tacks.
In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Con-
ference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp.
9389–9398. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/
schwarzschild21a.html."
REFERENCES,0.20613614573346117,"Giorgio Severi, Jim Meyer, Scott Coull, and Alina Oprea.
Explanation-guided backdoor poi-
soning attacks against malware classiﬁers.
In Michael Bailey and Rachel Greenstadt (eds.),
30th USENIX Security Symposium, USENIX Security 2021, August 11-13, 2021, pp. 1487–
1504. USENIX Association, 2021.
URL https://www.usenix.org/conference/
usenixsecurity21/presentation/severi."
REFERENCES,0.20709491850431447,"Ali Shafahi, W. Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras,
and Tom Goldstein. Poison frogs! targeted clean-label poisoning attacks on neural networks.
In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol`o Cesa-Bianchi,
and Roman Garnett (eds.), Advances in Neural Information Processing Systems 31: Annual Con-
ference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018,
Montr´eal, Canada, pp. 6106–6116, 2018. URL https://proceedings.neurips.cc/
paper/2018/hash/22722a343513ed45f14905eb07621686-Abstract.html."
REFERENCES,0.2080536912751678,"Virat Shejwalkar, Amir Houmansadr, Peter Kairouz, and Daniel Ramage. Back to the drawing
board: A critical evaluation of poisoning attacks on federated learning. CoRR, abs/2108.10241,
2021. URL https://arxiv.org/abs/2108.10241."
REFERENCES,0.20901246404602108,"Heung-Yeung Shum, Xiaodong He, and Di Li. From eliza to xiaoice: challenges and opportunities
with social chatbots. Frontiers Inf. Technol. Electron. Eng., 19(1):10–26, 2018. doi: 10.1631/
FITEE.1700826. URL https://doi.org/10.1631/FITEE.1700826."
REFERENCES,0.2099712368168744,"Jason R. Smith, Herve Saint-Amand, Magdalena Plamada, Philipp Koehn, Chris Callison-Burch,
and Adam Lopez. Dirt cheap web-scale parallel text from the common crawl. In Proceedings of
the 51st Annual Meeting of the Association for Computational Linguistics, ACL 2013, 4-9 August
2013, Soﬁa, Bulgaria, Volume 1: Long Papers, pp. 1374–1383. The Association for Computer
Linguistics, 2013. URL https://www.aclweb.org/anthology/P13-1135/."
REFERENCES,0.2109300095877277,"Joan E. Solsman. Youtube’s ai is the puppet master over most of what you watch. CNET, 2018."
REFERENCES,0.21188878235858102,Under review as a conference paper at ICLR 2022
REFERENCES,0.21284755512943432,"Fnu Suya, Saeed Mahloujifar, Anshuman Suri, David Evans, and Yuan Tian. Model-targeted poi-
soning attacks with provable convergence. In Marina Meila and Tong Zhang (eds.), Proceedings
of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual
Event, volume 139 of Proceedings of Machine Learning Research, pp. 10000–10010. PMLR,
2021. URL http://proceedings.mlr.press/v139/suya21a.html."
REFERENCES,0.21380632790028764,"Loc Truong, Chace Jones, Brian Hutchinson, Andrew August, Brenda Praggastis, Robert
Jasper, Nicole Nichols, and Aaron Tuor.
Systematic evaluation of backdoor data poison-
ing attacks on image classiﬁers.
In 2020 IEEE/CVF Conference on Computer Vision and
Pattern Recognition, CVPR Workshops 2020, Seattle, WA, USA, June 14-19, 2020, pp. 3422–
3431. Computer Vision Foundation / IEEE, 2020.
doi: 10.1109/CVPRW50498.2020.00402.
URL
https://openaccess.thecvf.com/content_CVPRW_2020/html/w47/
Truong_Systematic_Evaluation_of_Backdoor_Data_Poisoning_Attacks_
on_Image_Classifiers_CVPRW_2020_paper.html."
REFERENCES,0.21476510067114093,"Leslie G. Valiant. A theory of the learnable. Commun. ACM, 27(11):1134–1142, 1984. doi: 10.
1145/1968.1972. URL https://doi.org/10.1145/1968.1972."
REFERENCES,0.21572387344199426,"Roman Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge university press, 2018."
REFERENCES,0.21668264621284755,"Martin J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge Series
in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019. doi: 10.1017/
9781108627771."
REFERENCES,0.21764141898370087,"Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill,
Omer Levy, and Samuel R. Bowman.
Superglue: A stickier benchmark for general-purpose
language understanding systems.
In Hanna M. Wallach, Hugo Larochelle, Alina Beygelz-
imer, Florence d’Alch´e-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neu-
ral Information Processing Systems 32:
Annual Conference on Neural Information Pro-
cessing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp.
3261–3275, 2019a. URL https://proceedings.neurips.cc/paper/2019/hash/
4496bf24afe7fab6f046bf4923da8de6-Abstract.html."
REFERENCES,0.21860019175455417,"Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.
GLUE: A multi-task benchmark and analysis platform for natural language understanding. In
7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA,
May 6-9, 2019. OpenReview.net, 2019b. URL https://openreview.net/forum?id=
rJ4km2R5t7."
REFERENCES,0.2195589645254075,"Shurun Wang, Shiqi Wang, Xinfeng Zhang, Shanshe Wang, Siwei Ma, and Wen Gao. Scalable
facial image compression with deep feature reconstruction. In 2019 IEEE International Confer-
ence on Image Processing, ICIP 2019, Taipei, Taiwan, September 22-25, 2019, pp. 2691–2695.
IEEE, 2019c. doi: 10.1109/ICIP.2019.8803255. URL https://doi.org/10.1109/ICIP.
2019.8803255."
REFERENCES,0.22051773729626079,"Yuanyuan Wu, Eric W. T. Ngai, Pengkun Wu, and Chong Wu. Fake online reviews: Literature
review, synthesis, and directions for future research. Decis. Support Syst., 132:113280, 2020. doi:
10.1016/j.dss.2020.113280. URL https://doi.org/10.1016/j.dss.2020.113280."
REFERENCES,0.2214765100671141,"Cong Xie, Oluwasanmi Koyejo, and Indranil Gupta. Fall of empires: Breaking byzantine-tolerant
SGD by inner product manipulation. In Amir Globerson and Ricardo Silva (eds.), Proceedings of
the Thirty-Fifth Conference on Uncertainty in Artiﬁcial Intelligence, UAI 2019, Tel Aviv, Israel,
July 22-25, 2019, volume 115 of Proceedings of Machine Learning Research, pp. 261–270. AUAI
Press, 2019. URL http://proceedings.mlr.press/v115/xie20a.html."
REFERENCES,0.2224352828379674,"Matthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In
David J. Fleet, Tom´as Pajdla, Bernt Schiele, and Tinne Tuytelaars (eds.), Computer Vision - ECCV
2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part
I, volume 8689 of Lecture Notes in Computer Science, pp. 818–833. Springer, 2014. doi: 10.1007/
978-3-319-10590-1\ 53.
URL https://doi.org/10.1007/978-3-319-10590-1_
53."
REFERENCES,0.2233940556088207,Under review as a conference paper at ICLR 2022
REFERENCES,0.22435282837967402,"Shihao Zhao, Xingjun Ma, Xiang Zheng, James Bailey, Jingjing Chen, and Yu-Gang Jiang. Clean-
label backdoor attacks on video recognition models. In 2020 IEEE/CVF Conference on Computer
Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pp. 14431–
14440. Computer Vision Foundation / IEEE, 2020.
doi: 10.1109/CVPR42600.2020.01445.
URL
https://openaccess.thecvf.com/content_CVPR_2020/html/Zhao_
Clean-Label_Backdoor_Attacks_on_Video_Recognition_Models_CVPR_
2020_paper.html."
REFERENCES,0.2253116011505273,"Chen Zhu, W. Ronny Huang, Hengduo Li, Gavin Taylor, Christoph Studer, and Tom Goldstein.
Transferable clean-label poisoning attacks on deep neural nets. In Kamalika Chaudhuri and Rus-
lan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learn-
ing, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of
Machine Learning Research, pp. 7614–7623. PMLR, 2019. URL http://proceedings.
mlr.press/v97/zhu19a.html."
REFERENCES,0.22627037392138064,Under review as a conference paper at ICLR 2022
REFERENCES,0.22722914669223393,Appendices
REFERENCES,0.22818791946308725,"A
CONVEXITY LEMMAS"
REFERENCES,0.22914669223394055,"A.1
GENERAL LEMMAS"
REFERENCES,0.23010546500479387,"Deﬁnition 3. We say that f : Rd →R is locally strongly convex if, for any convex compact set
C ⊂Rd, there exists µ > 0 such that f is µ-strongly convex on C, i.e. for any x, y ∈C and any
λ ∈[0, 1], we have"
REFERENCES,0.23106423777564716,f(λx + (1 −λ)y) ≤λf(x) + (1 −λ)f(y) −µ
REFERENCES,0.23202301054650049,"2 λ(1 −λ) ∥x −y∥2
2 .
(7)"
REFERENCES,0.23298178331735378,"It
is
well-known
that
if
f
is
differentiable,
this
condition
amounts
to
saying
that
∥∇f(x) −∇f(y)∥2 ≥µ ∥x −y∥2 for all x, y ∈C. And if f is twice differentiable, then it amounts
to saying ∇2f(x) ⪰µI for all x ∈C.
Lemma 6. If f is locally strongly convex and g is convex, then f + g is locally strongly convex."
REFERENCES,0.2339405560882071,"Proof. Indeed, (f + g)(λx + (1 −λ)y) ≤λf(x) + (1 −λ)f(y) −µ"
REFERENCES,0.2348993288590604,"2 λ(1 −λ) ∥x −y∥2
2 + λg(x) +
(1 −λ)g(y) = λ(f + g)(x) + (1 −λ)(f + g)(y) −µ"
REFERENCES,0.23585810162991372,"2 λ(1 −λ) ∥x −y∥2
2."
REFERENCES,0.236816874400767,"Deﬁnition 4. We say that f : Rd →R is L-smooth if it is differentiable and if its gradient is
L-Lipschitz continuous, i.e. for any x, y ∈Rd,"
REFERENCES,0.23777564717162034,"∥∇f(x) −∇f(y)∥2 ≤L ∥x −y∥2 .
(8)"
REFERENCES,0.23873441994247363,"Lemma 7. If f is Lf-smooth and g is Lg-smooth, then f + g is (Lf + Lg)-smooth."
REFERENCES,0.23969319271332695,"Proof. Indeed, ∥∇(f + g)(x) −∇(f + g)(y)∥2 ≤∥∇f(x) −∇f(y)∥2 + ∥∇g(x) −∇g(y)∥2 ≤
Lf ∥x −y∥2 + Lg ∥x −y∥2 = (Lf + Lg) ∥x −y∥2 ."
REFERENCES,0.24065196548418025,"Lemma 8. Suppose that f : Rd × Rd′ 7→R is locally strongly convex and L-smooth, and that, for
any x ∈X, where X ⊂Rd is a convex compact subset, the map y 7→f(x, y) has a minimum y∗(x).
Note that local strong convexity guarantees the uniqueness of this minimum. Then, there exists K
such that the function y∗is K-Lipschitz continuous on X."
REFERENCES,0.24161073825503357,"Proof. The existence and uniqueness of y∗(x) hold by strong convexity. Fix x, x′. By optimality of
y∗, we know that ∇yf(x, y∗(x)) = ∇yf(x′, y∗(x′)) = 0. We then have the following bounds"
REFERENCES,0.24256951102588686,"µ ∥y∗(x) −y∗(x′)∥2 ≤∥∇yf(x, y∗(x)) −∇yf(x, y∗(x′))∥2 = ∥∇yf(x, y∗(x′))∥2
(9)"
REFERENCES,0.24352828379674019,"= ∥∇yf(x, y∗(x′)) −∇yf(x′, y∗(x′))∥2
(10)"
REFERENCES,0.24448705656759348,"≤∥∇f(x, y∗(x′)) −∇f(x′, y∗(x′))∥2
(11)"
REFERENCES,0.24544582933844677,"≤L ∥(x −x′, y∗(x′) −y∗(x′))∥2 = L ∥x −x′∥2 ,
(12)"
REFERENCES,0.2464046021093001,"where we ﬁrst used the local strong convexity assumption, then the fact that ∇yf(x, y∗(x)) = 0,
then the fact that ∇yf(x′, y∗(x′)) = 0, and then the L-smooth assumption."
REFERENCES,0.2473633748801534,"Lemma 9. Suppose that f : Rd × Rd′ 7→R is locally strongly convex and L-smooth, and that,
for any x ∈X, where X ⊂Rd is a convex compact subset, the map y 7→f(x, y) has a minimum
y∗(x). Deﬁne g(x) ≜miny∈Y f(x, y). Then g is convex and differentiable on X and ∇g(x) =
∇xf(x, y∗(x))."
REFERENCES,0.2483221476510067,"Proof. First we prove that g is convex. Let x1, x2 ∈Rd, and λ1, λ2 ∈[0, 1] with λ1 + λ2 = 1. For
any y1, y2 ∈Rd′, we have"
REFERENCES,0.24928092042186,"g(λ1x1 + λ2x2) = min
y∈Rd′ f(λ1x1 + λ2x2, y)
(13)"
REFERENCES,0.25023969319271333,"≤f(λ1x1 + λ2x2, λ1y1 + λ2y2)
(14)
≤λ1f(x1, y1) + λ2f(x2, y2).
(15)"
REFERENCES,0.25119846596356665,Under review as a conference paper at ICLR 2022
REFERENCES,0.2521572387344199,"Taking the inﬁmum of the right-hand side over y1 and y2 yields g(λ1x1 + λ2x2) ≤λ1g(x1) +
λ2g(x2), which proves the convexity of g."
REFERENCES,0.25311601150527324,"Now denote h(x) = ∇xf(x, y∗(x)). We aim to show that ∇g(x) = h(x). Let ε ∈Rd small enough
so that x + ε ∈X. Now note that we have"
REFERENCES,0.25407478427612656,"g(x + ε) = min
y∈Rd′ f(x + ε, y) ≤f(x + ε, y∗(x))
(16)"
REFERENCES,0.2550335570469799,"= f(x, y∗(x)) + εT ∇xf(x, y∗(x)) + o(∥ε∥2)
(17)"
REFERENCES,0.25599232981783315,"= g(x) + εT h(x) + o(∥ε∥2),
(18)"
REFERENCES,0.2569511025886865,"which shows that h(x) is a superderivative of g at x. We now show that it is also a subderivative. To
do so, ﬁrst note that its value at x + ε is approximately the same, i.e."
REFERENCES,0.2579098753595398,"∥h(x + ε) −h(x)∥2 ≤∥∇xf(x + ε, y∗(x + ε)) −∇xf(x, y∗(x + ε))∥2
+ ∥∇xf(x, y∗(x + ε)) −∇xf(x, y∗(x))∥2
(19)"
REFERENCES,0.2588686481303931,"≤L ∥ε∥2 + L ∥y∗(x + ε) −y∗(x)∥2 ≤

L + L2 µ"
REFERENCES,0.2598274209012464,"
∥ε∥2 ,
(20)"
REFERENCES,0.2607861936720997,where we used the L-smoothness of f and Lemma 8. Now notice that
REFERENCES,0.26174496644295303,"g(x) = min
y∈Rd′ f(x, y) ≤f(x, y∗(x + ε)) = f((x + ε) −ε, y∗(x + ε))
(21)"
REFERENCES,0.26270373921380635,"= f(x + ε, y∗(x + ε)) −εT ∇xf(x + ε, y∗(x + ε)) + o(∥ε∥2)
(22)"
REFERENCES,0.2636625119846596,"= g(x + ε) −εT h(x) −εT (h(x + ε) −h(x)) + o(∥ε∥2),
(23)"
REFERENCES,0.26462128475551294,But we know that ∥h(x + ε) −h(x)∥2 = O(∥ε∥2). Rearranging the terms then yields
REFERENCES,0.26558005752636626,"g(x + ε) ≥g(x) + εT h(x) −o(∥ε∥2),
(24)"
REFERENCES,0.2665388302972196,"which shows that h(x) is also a subderivative. Therefore, we know that g(x+ε) = g(x)+εT h(x)+
o(∥ε∥2), which boils down to saying that g is differentiable in x ∈X, and that ∇g(x) = h(x)."
REFERENCES,0.26749760306807285,"Lemma 10. Suppose that f : X × Rd′ →R is µ-strongly convex, where X ⊂Rd is closed and
convex. Then g : X →R, deﬁned by g(x) = infy∈Y f(x, y), is well-deﬁned and µ-strongly convex
too."
REFERENCES,0.2684563758389262,"Proof. The function y 7→f(x, y) is still strongly convex, which means that it is at least equal to
a quadratic approximation around 0, which is a function that goes to inﬁnity in all directions as
∥y∥2 →∞. This proves that the inﬁmum must be reached within a compact set, which implies the
existence of a minimum. Thus g is well-deﬁned. Moreover, for any x1, x2 ∈X, y1, y2 ∈Rd′, and
λ1, λ2 ≥0 with λ1 + λ2 = 1, we have"
REFERENCES,0.2694151486097795,"g(λ1x1 + λ2x2) = inf
y f(λ1x1 + λ2x2, y)
(25)"
REFERENCES,0.27037392138063276,"≤f(λ1x1 + λ2x2, λ1y1 + λ2y2)
(26)"
REFERENCES,0.2713326941514861,"≤λ1f(x1, y1) + λ2f(x2, y2) −µ"
REFERENCES,0.2722914669223394,"2 λ1λ2 ∥(x1 −x2, y1 −y2)∥2
2
(27)"
REFERENCES,0.27325023969319273,"≤λ1f(x1, y1) + λ2f(x2, y2) −µ"
REFERENCES,0.274209012464046,"2 λ1λ2 ∥x1 −x2∥2
2 ,
(28)"
REFERENCES,0.2751677852348993,"where we used the µ-strong convexity of f. Taking the inﬁmum over y1, y2 implies the µ-strong
convexity of g."
REFERENCES,0.27612655800575264,"A.2
APPLICATIONS TO LOSS"
REFERENCES,0.27708533077660596,"Now instead of proving our theorems for different cases separately, we make the following assump-
tions on the components of the global loss that encompasses both ℓ2
2 and smooth-ℓ2 regularization,
a well as linear regression and logistic regression.
Assumption 1. Assume that ℓis convex and Lℓ-smooth, and that R(ρ, θ) = R0(ρ −θ), where
R0 : Rd →R is locally strongly convex (i.e. strongly convex on any convex compact set), LR0-
smooth and satisfy R0(z) = Ω(∥z∥2) as ∥z∥2 →∞."
REFERENCES,0.27804410354745923,Under review as a conference paper at ICLR 2022
REFERENCES,0.27900287631831255,"Lemma 11. Under Assumption 1, LOSS is locally strongly convex and L-smooth."
REFERENCES,0.2799616490891659,"Proof. All terms of LOSS are L0-smooth, for an appropriate value of L0. By Lemma 7, their sum
is thus also L-smooth, for an appropriate value of L. Now, given Lemma 6, to prove that LOSS is
locally strongly convex, it sufﬁces to prove that ν P ∥θn∥2
2 +R0(ρ−θ1) is locally strongly convex.
Consider any convex compact set C ⊂Rd×(1+N). Since R0 is locally strongly convex, we know
that there exists µ > 0 such that ∇2R0 ⪰µI. As a result,"
REFERENCES,0.2809204218600192,"(ρ, ⃗θ)T  
∇2LOSS

(ρ, ⃗θ) ≥ν
X"
REFERENCES,0.28187919463087246,"n∈[N]
∥θn∥2
2 + µ ∥ρ −θ1∥2
2
(29)"
REFERENCES,0.2828379674017258,"= ν ∥θ1∥2
2 + µ ∥ρ∥2
2 + µ ∥θ1∥2
2 −2µρT θ1 + ν
X"
REFERENCES,0.2837967401725791,"n̸=1
∥θn∥2
2 .
(30)"
REFERENCES,0.28475551294343243,"Now deﬁne α ≜
q"
REFERENCES,0.2857142857142857,"2µ
ν+2µ. Clearly, 0 < α < 1. Moreover, 0 ≤
 1"
REFERENCES,0.286673058485139,"αθ1 −αρ
2
2 =
1
α2 ∥θ1∥2
2 +"
REFERENCES,0.28763183125599234,"α2 ∥ρ∥2
2 −2ρT θ1. Therefore 2ρT θ1 ≤α2 ∥ρ∥2
2 +
1
α2 ∥θ1∥2
2, which thus implies"
REFERENCES,0.28859060402684567,"(ρ, ⃗θ)T  
∇2LOSS

(ρ, ⃗θ) ≥
 
ν + µ
 
1 −α−2
∥θ1∥2
2 + µ
 
1 −α2
∥ρ∥2
2 + ν
X"
REFERENCES,0.28954937679769893,"n̸=1
∥θn∥2
2
(31) ≥ν"
REFERENCES,0.29050814956855225,"2 ∥θ1∥2
2 +
2νµ
ν + 2µ ∥ρ∥2
2 + ν
X"
REFERENCES,0.2914669223394056,"n̸=1
∥θn∥2
2 ≥min
ν"
REFERENCES,0.29242569511025884,"2,
2νµ
ν + 2µ"
REFERENCES,0.29338446788111217," (ρ, ⃗θ)

2"
REFERENCES,0.2943432406519655,"2 ,
(32)"
REFERENCES,0.2953020134228188,"which proves that ∇2LOSS ⪰κI, with κ > 0. This shows that LOSS is locally strongly convex."
REFERENCES,0.2962607861936721,"Lemma 12. Under Assumption 1, ρ 7→⃗θ∗(ρ, ⃗D) is Lipchitz continuous on any compact set."
REFERENCES,0.2972195589645254,"Proof. Deﬁne fn(ρ, θn) ≜ν ∥θn∥2
2 + P"
REFERENCES,0.2981783317353787,"x∈Dn ℓ(θn, x) + λ ∥ρ −θn∥2
2. If ℓis L-smooth, then fn
is clearly (|Dn| L + ν + λ)-smooth. Moreover, if ℓis convex, then for any ρ, the function θn 7→
fn(ρ, θn) is at least ν-strongly convex. Thus Lemma 8 applies, which guarantees that ρ 7→⃗θ∗(ρ, ⃗D)
is Lipchitz."
REFERENCES,0.29913710450623204,"Lemma 13. Under Assumption 1, ρ 7→LOSS(ρ, ⃗θ∗(ρ, ⃗D), ⃗D) is L-smooth and locally strongly
convex."
REFERENCES,0.3000958772770853,"Proof. By Lemma 11, the global loss is known to be L-smooth, for some value of L and locally
strongly convex. Denoting f : ρ 7→LOSS(ρ, ⃗θ∗(ρ, ⃗D), ⃗D), we then have"
REFERENCES,0.30105465004793863,"∥∇f(ρ) −∇f(ρ′)∥2 ≤
∇ρLOSS(ρ, ⃗θ∗(ρ, ⃗D), ⃗D) −∇ρLOSS(ρ′, ⃗θ∗(ρ′, ⃗D), ⃗D)

2
(33)"
REFERENCES,0.30201342281879195,"≤L
(ρ, ⃗θ∗(ρ, ⃗D)) −(ρ′, ⃗θ∗(ρ′, ⃗D))

2
(34)"
REFERENCES,0.3029721955896453,"≤L ∥ρ −ρ′∥2 ,
(35)
which proves that f is L-smooth."
REFERENCES,0.30393096836049854,"For strong convexity, note that since the global loss function is locally strongly convex, for any com-
pact convex set C, there exists µ such that LOSS(ρ, ⃗θ, ⃗D) is µ-strongly convex on C = (C1, C2) ⊂
(Rd, RN×d), therefore, by Lemma 10, f(ρ) will also be µ-strongly convex on C1 which means that
f(ρ) is locally strongly convex."
REFERENCES,0.30488974113135187,"B
PROOF OF THE EQUIVALENCE"
REFERENCES,0.3058485139022052,"B.1
PROOF OF THE REDUCTION FROM MODEL ATTACK TO DATA POISONING"
REFERENCES,0.3068072866730585,"Proof of Lemma 1. We omit making the dependence of the optima on ⃗D explicit, and we consider
any other models ρ and ⃗θ−s. We have the following inequalities:"
REFERENCES,0.3077660594439118,"LOSSs(ρ∗, ⃗θ∗
−s, θ♠
s , ⃗D) = LOSS(ρ∗, ⃗θ∗, ⃗D) −L(θ∗
s, Ds)
(36)"
REFERENCES,0.3087248322147651,"≤LOSS(ρ, (⃗θ−s, θ∗
s), ⃗D) −L(θ∗
s, Ds) = LOSSs(ρ, ⃗θ−s, θ♠
s , ⃗D),
(37)"
REFERENCES,0.3096836049856184,Under review as a conference paper at ICLR 2022
REFERENCES,0.31064237775647174,"where we used the optimality of (ρ∗, ⃗θ∗) in the second line, and where we repeatedly used the fact
that θ∗
s = θ♠
s . This proves that (ρ∗, ⃗θ∗
−s) is a global minimum of the modiﬁed loss."
REFERENCES,0.311601150527325,"B.2
PROOF OF THE REDUCTION FROM DATA POISONING TO MODEL ATTACK"
REFERENCES,0.31255992329817833,"First, we deﬁne the following modiﬁed loss function:"
REFERENCES,0.31351869606903165,"LOSSs(ρ, ⃗θ−s, θ♠
s , ⃗D−s) ≜LOSS(ρ, (θ♠
s , ⃗θ−s), (∅, ⃗D−s))
(38)"
REFERENCES,0.3144774688398849,"where ⃗θ−s and ⃗D−s are variables and datasets for users n ̸= s. We then deﬁne ρ∗(θ♠
s , ⃗D−s) and
⃗θ∗
−s(θ♠
s , ⃗D−s) as a minimum of the modiﬁed loss function, and θ∗
s(θ♠
s , ⃗D−s) ≜θ♠
s . We now prove
a slightly more general version of Lemma 2, which applies to a larger class of regularizations. It
also shows how to construct the strategic’s user data poisoning attack.
Lemma 14 (Reduction from data poisoning to model attack). Assume local PAC* learning. Sup-
pose also that R is continuous and that R(ρ, θ) →∞when ∥ρ −θ∥2 →∞. Consider any
datasets D−s and any attack model θ♠
s such that the modiﬁed loss LOSSs has a unique minimum
ρ∗(θ♠
s , ⃗D−s), ⃗θ∗
−s(θ♠
s , ⃗D−s). Then, for any ε, δ > 0, there exists I such that if user s’s dataset Ds
contains at least I inputs drawn from model θ♠
s , then, with probability at least 1 −δ, we have
ρ∗( ⃗D) −ρ∗(θ♠
s , ⃗D−s)

2 ≤ε and ∀n ̸= s,
θ∗
n( ⃗D) −θ∗
n(θ♠
s , ⃗D−s)

2 ≤ε.
(39)"
REFERENCES,0.31543624161073824,"Clearly, ℓ2
2, ℓ2 and smooth-ℓ2 are continuous regularizations, and verify R(ρ, θ) →∞when
∥ρ −θ∥2 →∞. Moreover, setting δ ≜1/2 shows that the probability that the dataset Ds satis-
ﬁes the inequalities of Lemma 14 is positive. This implies in particular that there must be a dataset
Ds that satisﬁes these inequalities. ALl in all, this shows that Lemma 14 implies Lemma 2."
REFERENCES,0.31639501438159157,"Proof of Lemma 14. Let ε, δ > 0 and θ♠
s ∈Rd. Denote ρ♠≜ρ∗(θ♠
s , ⃗D−s) and ⃗θ♠≜⃗θ∗(θ♠
s , ⃗D−s)
the result of strategic user s’s model attack. We deﬁne the compact set C by"
REFERENCES,0.3173537871524449,"C ≜
n
ρ, ⃗θ−s

ρ −ρ♠
2 ≤ε ∧∀n ̸= s,
θn −θ♠
n

2 ≤ε
o
(40)"
REFERENCES,0.31831255992329816,"We deﬁne D ≜Rd×N −C the closure of the complement of C. Clearly, ρ♠, ⃗θ♠
−s /∈D. We aim
to show that, when strategic user s reveals a large dataset Ds whose answers are provided using the
attack model θ♠
s , then the same holds for any global minimum of the global loss ρ∗( ⃗D), ⃗θ∗
−s( ⃗D) ∈
C. Note that, to prove this, it sufﬁces to prove that the modiﬁed loss takes too large values, even
when θ♠
s is replaced by θ∗
s( ⃗D)."
REFERENCES,0.3192713326941515,"Let us now formalize this. Denote L♠≜LOSSs(ρ♠, ⃗θ♠
−s, θ♠
s , ⃗D−s). We deﬁne"
REFERENCES,0.3202301054650048,"η ≜
inf
ρ,⃗θ−s∈D
LOSSs(ρ, ⃗θ−s, θ♠
s , ⃗D−s) −L♠.
(41)"
REFERENCES,0.3211888782358581,"By a similar argument as that of Lemma 5, using the assumption R →∞at inﬁnity, we know that
the inﬁmum is actually a minimum. Moreover, given that the minimum of the modiﬁed loss LOSSs
is unique, we know that the value of the loss function at this minimum is different from its value at
ρ♠, ⃗θ♠
−s. As a result, we must have η > 0."
REFERENCES,0.3221476510067114,"Now, since the function R is differentiable, it must be continuous. By the Heine–Cantor theorem,
it is thus uniformly continuous on all compact sets. Thus, there must exist κ > 0 such that, for all
models θs satisfying
θs −θ♠
s

2 ≤κ, we have
R(θs, ρ♠) −R(θ♠
s , ρ♠)
 ≤η/3.
(42)"
REFERENCES,0.3231064237775647,"Now, Lemma 5 guarantees the existence of I such that, if user s provides a dataset Ds of least I
answers with the model θ♠
s , then with probability at least 1 −δ, we will have
θ∗
s( ⃗D) −θ♠
s

2 ≤"
REFERENCES,0.32406519654841803,"min(κ, ε). Under this event, we then have"
REFERENCES,0.32502396931927136,"LOSSs

ρ♠, ⃗θ♠
−s, θ∗
s( ⃗D), ⃗D−s

≤L♠+ η/3.
(43)"
REFERENCES,0.3259827420901246,Under review as a conference paper at ICLR 2022 Then
REFERENCES,0.32694151486097794,"inf
ρ,⃗θ−s∈D
LOSSs(ρ, ⃗θ−s, θ∗
s( ⃗D), ⃗D−s) ≥
inf
ρ,⃗θ−s∈D
LOSSs(ρ, ⃗θ−s, θ♠
s , ⃗D−s) −η/3
(44)"
REFERENCES,0.32790028763183127,"≥L♠+ η −η/3 ≥L♠+ 2η/3
(45)"
REFERENCES,0.3288590604026846,"> LOSSs

ρ♠, ⃗θ♠
−s, θ∗
s( ⃗D), ⃗D−s

.
(46)"
REFERENCES,0.32981783317353786,"This shows that there is a high probability event under which the minimum of ρ, ⃗θ−s
7→
LOSSs

ρ, ⃗θ−s, θ∗
s( ⃗D), ⃗D−s

cannot be reached in D. This is equivalent to what the theorem we
needed to prove states."
REFERENCES,0.3307766059443912,"B.3
PROOF OF REDUCTION FROM MODEL ATTACK TO GRADIENT ATTACK"
REFERENCES,0.3317353787152445,"In this section, we prove a slightly more general result than Lemma 3. Namely, instead of work-
ing with speciﬁc regularizations, we consider a more general class of regularizations, identiﬁed by
Assumption 1.
Lemma 15 (Reduction from model attack to gradient attack). Under Assumption 1, if gt
s converges
and if ηt = η is a constant small enough, then ρt will converge too. Denote ρ∞its limit. Then for
any ε > 0, there is θ♠
s ∈Rd such that
ρ∞−ρ∗(θ♠
s , ⃗D−s)

2 ≤ε."
REFERENCES,0.3326941514860978,"Note that since ℓ2
2 and smooth-ℓ2 regularizations satisfy Assumption 1, Lemma 15 clearly implies
Lemma 3. We now introduce the key objects of the proof of Lemma 15."
REFERENCES,0.3336529242569511,"Denote g∞
s the limit of the attack gradients gt
s. We now deﬁne"
REFERENCES,0.3346116970278044,"LOSS1
s(ρ) ≜inf
⃗θ−s"
REFERENCES,0.33557046979865773,"n
LOSS(ρ, ⃗θ, ⃗D) −Ls(θs, Ds) −R(ρ, θs)
o
+ ρT g∞
s
(47)"
REFERENCES,0.336529242569511,"= inf
⃗θ−s 
  X"
REFERENCES,0.3374880153403643,"n̸=s
Ln(θn, Dn) +
X"
REFERENCES,0.33844678811121764,"n̸=s
R(ρ, θn) 
"
REFERENCES,0.33940556088207097,"+ ρT g∞
s ,
(48)"
REFERENCES,0.34036433365292423,"and prove that ρt will converge to the minimizer of LOSS1
s(ρ).
By Lemma 13, we show that
LOSS1
s(ρ) is both locally strongly convex and L-smooth."
REFERENCES,0.34132310642377756,"Now deﬁne ζt
s ≜gt
s −g∞
s . We then know ζt
s →0 and ∇LOSS1
s(ρt) is the sum of all gradient
vectors received from all users assuming the strategic user s sends the vector g∞
s
in all iterations.
Thus, at iteration t of the optimization algorithm, we will take one step in the direction Gt ≜
∇LOSS1
s(ρt) + ζt
s, i.e.,
ρt+1 = ρt −ηtGt.
(49)"
REFERENCES,0.3422818791946309,"We now prove the following lemma that bounds the difference between the function value in two
successive iterations.
Lemma 16. If LOSS1
s(ρ) is L-smooth and ηt ≤1/L, we have"
REFERENCES,0.3432406519654842,"LOSS1
s(ρt+1) −LOSS1
s(ρt) ≤−ηt 2"
REFERENCES,0.34419942473633747,"Gt2
2 + ηtζt
s
T Gt.
(50)"
REFERENCES,0.3451581975071908,"Proof. Since LOSS1
s is L-smooth, we have"
REFERENCES,0.3461169702780441,"LOSS1
s(ρt+1) ≤LOSS1
s(ρt) + (ρt+1 −ρt)T ∇LOSS1
s(ρt) + L 2"
REFERENCES,0.34707574304889743,"ρt+1 −ρt2
2 .
(51)"
REFERENCES,0.3480345158197507,"Now plugging ρt+1 −ρt = −ηtGt and ∇LOSS1
s(ρt) = Gt −ζt
s into the inequality implies"
REFERENCES,0.348993288590604,"LOSS1
s(ρt+1) −LOSS1
s(ρt) ≤
 
−ηtGtT  
Gt −ζt
s

+ L 2"
REFERENCES,0.34995206136145734,"−ηtGt2
2
(52) ≤−ηt 2"
REFERENCES,0.35091083413231067,"Gt2
2 + ηtζt
s
T Gt,
(53)"
REFERENCES,0.35186960690316393,where we used the fact ηt ≤1/L.
REFERENCES,0.35282837967401726,Under review as a conference paper at ICLR 2022
REFERENCES,0.3537871524448706,"B.3.1
THE GLOBAL MODEL REMAINS BOUNDED"
REFERENCES,0.3547459252157239,"Lemma 17. There is M such that, for all t, LOSS1
s(ρt) ≤M."
REFERENCES,0.35570469798657717,"Proof. Consider the closed ball B(ρ∗, 1) centered on ρ∗and of radius 1. By Lemma 13, we know
that LOSS1
s is locally strongly convex and thus there exists a µ1 > 0 such that LOSS1
s is µ1-strongly
convex on B(ρ∗, 1). Now consider a point ρ1 on the boundary of B(ρ∗, 1). By strong convexity we
have
∇LOSS1
s(ρ1)
2
2 ≥(ρ1 −ρ∗)T ∇LOSS1
s(ρ1) ≥µ1 ∥ρ1 −ρ∗∥2
2 = µ1.
(54)"
REFERENCES,0.3566634707574305,"Now similarly, by the convexity of LOSS1
s on Rd, for any ρ
∈
Rd −B(ρ∗, 1), we have
∇LOSS1
s(ρ1)

2 ≥√µ1. Now since ζt
s →0, there exists an iteration T1 after which (t ≥T1),
we have ∥ζt
s∥2 ≤1"
REFERENCES,0.3576222435282838,"4
√µ1, and thus ∥Gt∥2 ≥
∇LOSS1
s(ρt)

2 −∥ζt
s∥2 ≥3"
REFERENCES,0.3585810162991371,"4
√µ1. Thus, Lemma 16
implies that for t ≥T1, if ∥ρt −ρ∗∥2 ≥1, then"
REFERENCES,0.3595397890699904,"LOSS1
s(ρt+1) −LOSS1
s(ρt) ≤−η 2"
REFERENCES,0.3604985618408437,"Gt2
2 + ηζt
s
T Gt
(55) ≤−η 2"
REFERENCES,0.36145733461169705,"Gt2
2 + η
ζt
s

2
Gt
2
(56) ≤−η 2"
REFERENCES,0.3624161073825503,"Gt
2
 Gt
2 −2
ζt
s

2

(57) ≤−η"
REFERENCES,0.36337488015340363,"2
3
4
√µ1 3"
REFERENCES,0.36433365292425696,"4
√µ1 −2 4
√µ1"
REFERENCES,0.3652924256951103,"
≤−3η"
REFERENCES,0.36625119846596355,"32µ1 < 0.
(58)"
REFERENCES,0.36720997123681687,"Thus, for ∥ρt −ρ∗∥2 ≥1, the loss cannot increase at the next iteration."
REFERENCES,0.3681687440076702,"Now consider the case ∥ρt −ρ∗∥2
< 1 for t ≥T1.
The smoothness of LOSS1
s implies
∇LOSS1
s(ρt)

2 < L. Therefore,
ρt+1 −ρ∗
2 =
ρt −η(∇LOSS1
s(ρt) + ζt
s) −ρ∗
2
(59)"
REFERENCES,0.3691275167785235,"≤
ρt+1 −ρ∗
2 + η(L + 1"
REFERENCES,0.3700862895493768,"4
√µ1) ≤1 + η(L + 1"
REFERENCES,0.3710450623202301,"4
√µ1).
(60)"
REFERENCES,0.3720038350910834,"Now we deﬁne M1 ≜maxρ∈B(ρ∗,1+η(L+ 1"
REFERENCES,0.37296260786193675,"4
√µ1)) LOSS1
s(ρ), the maximum function value in the"
REFERENCES,0.37392138063279,"closed ball B
 
ρ∗, 1 + η(L + 1"
REFERENCES,0.37488015340364333,"4
√µ1)

. Therefore, we have LOSS1
s(ρt+1) ≤M1. So far we proved
that for t ≥T1, in each iteration of gradient descent either the function value will not increase
or it will be upper-bounded by M1. This implies that for all t, the function value LOSS1
s(ρt) is
upper-bounded by"
REFERENCES,0.37583892617449666,"M ≜max

max
t≤T1"
REFERENCES,0.37679769894535,"
LOSS1
s(ρt)
	
, M1"
REFERENCES,0.37775647171620325,"
.
(61)"
REFERENCES,0.37871524448705657,This concludes the proof.
REFERENCES,0.3796740172579099,"Lemma 18. There is a compact set X such that, for all t, ρt ∈X."
REFERENCES,0.38063279002876316,"Proof. Now since LOSS1
s is µ1-strongly convex in B(ρ∗, 1), for any point ρ ∈Rd such that
∥ρ −ρt∥2 = 1, we have"
REFERENCES,0.3815915627996165,"LOSS1
s(ρ) ≥LOSS1
s(ρ∗) + µ1"
REFERENCES,0.3825503355704698,"2 ∥ρ −ρ∗∥2
2 = LOSS1
s(ρ∗) + µ1"
REFERENCES,0.3835091083413231,"2 .
(62)"
REFERENCES,0.3844678811121764,"But now by the convexity of LOSS1
s in Rd, for any ρ such that ∥ρ −ρ∗∥2 ≥1, we have"
REFERENCES,0.3854266538830297,"LOSS1
s(ρ) ≥LOSS1
s(ρ∗) + ∥ρ −ρ∗∥2
µ1"
REFERENCES,0.38638542665388304,"2 .
(63)"
REFERENCES,0.38734419942473636,"This implies that if ∥ρt −ρ∗∥2 >
2
µ1
 
M2 −LOSS1
s(ρ∗)

, then LOSS1
s(ρt) > M2. Therefore, we
must have ∥ρt −ρ∗∥2 ≤
2
µ1
 
M2 −LOSS1
s(ρ∗)

, for all t ≥0. This describes a closed ball, which
is a compact set."
REFERENCES,0.3883029721955896,Under review as a conference paper at ICLR 2022
REFERENCES,0.38926174496644295,"B.3.2
CONVERGENCE OF THE GLOBAL MODEL UNDER CONVERGING GRADIENT ATTACK"
REFERENCES,0.39022051773729627,"Lemma 19. Suppose ut ≥0 veriﬁes ut+1 ≤αut + δt, with δt →0. Then ut →0."
REFERENCES,0.3911792905081496,"Proof. We now show that for any ε > 0, there exists an iteration T(ε), such that for t ≥T(ε), we
have ut ≤ε. For this, note that by induction, we observe that, for all t ≥0,"
REFERENCES,0.39213806327900286,"ut+1 ≤u0αt+1 + t
X"
REFERENCES,0.3930968360498562,"τ=0
ατδt−τ.
(64)"
REFERENCES,0.3940556088207095,"Since δt →0, there exists an iteration T2(ε) such that for all t ≥T2(ε), we have δt ≤ε(1−α)"
REFERENCES,0.3950143815915628,"2
.
Therefore, for t ≥T2(ε), we have"
REFERENCES,0.3959731543624161,ut+1 ≤u0αt+1 +
REFERENCES,0.3969319271332694,"t−T2(ε)
X"
REFERENCES,0.39789069990412274,"τ=0
ατδt−τ + t
X"
REFERENCES,0.39884947267497606,"τ=t−T2(ε)+1
ατδt−τ
(65)"
REFERENCES,0.3998082454458293,≤u0αt+1 + ε(1 −α) 2
REFERENCES,0.40076701821668265,"t−T2(ε)
X"
REFERENCES,0.40172579098753597,"τ=0
ατ +"
REFERENCES,0.40268456375838924,"T2(ε)−1
X"
REFERENCES,0.40364333652924256,"s=0
αt−sδs
(66) ≤  u0 +"
REFERENCES,0.4046021093000959,"T2(ε)−1
X"
REFERENCES,0.4055608820709492,"s=0
α−s−1δs "
REFERENCES,0.40651965484180247,"αt+1 + ε(1 −α) 2 ∞
X"
REFERENCES,0.4074784276126558,"τ=0
ατ.
(67)"
REFERENCES,0.4084372003835091,"Denoting M0(ε) ≜PT2(ε)−1
s=0
α−s−1δs, we then have"
REFERENCES,0.40939597315436244,ut+1 ≤(u0 + M0(ε)) αt+1 + ε
REFERENCES,0.4103547459252157,"2.
(68)"
REFERENCES,0.411313518696069,"Therefore, for t ≥
ln
ε
2(u0+M0(ε))"
REFERENCES,0.41227229146692235,"ln α
, we have"
REFERENCES,0.41323106423777567,ut+1 ≤ε 2 + ε
REFERENCES,0.41418983700862894,"2 = ε.
(69)"
REFERENCES,0.41514860977948226,This proves that ut →0.
REFERENCES,0.4161073825503356,We ﬁrst prove the ﬁrst part of Lemma 3.
REFERENCES,0.4170661553211889,"Lemma 20. Under Assumption 1 and ηt = η ≤1/L, if gt
s converges, then so does ρt."
REFERENCES,0.41802492809204217,"Proof. Deﬁne X based on Lemma 18. Since LOSS1
s is locally strongly convex, there exists µ2 > 0
such that LOSS1
s is µ2-strongly convex in a convex compact set X containing ρt for all t ≥0. By
the strong convexity of LOSS1
s(ρ), we have"
REFERENCES,0.4189837008628955,"LOSS1
s(ρt) −LOSS1
s(ρ∗) ≤(ρt −ρ∗)T ∇LOSS1
s(ρt) −µ2 2"
REFERENCES,0.4199424736337488,"ρt −ρ∗2
2
(70)"
REFERENCES,0.42090124640460214,"= (ρt −ρ∗)T  
Gt −ζt
s

−µ2 2"
REFERENCES,0.4218600191754554,"ρt −ρ∗2
2 .
(71)"
REFERENCES,0.4228187919463087,"Now, using the fact"
REFERENCES,0.42377756471716205,(ρt −ρ∗)T Gt = 1
REFERENCES,0.4247363374880153,"η (ρt −ρ∗)T (ρt −ρt+1)
(72) = 1 2η"
REFERENCES,0.42569511025886864,"ρt −ρ∗2
2 +
ρt −ρt+12
2 −
ρt+1 −ρ∗2
2"
REFERENCES,0.42665388302972196,"
(73) = 1 2η"
REFERENCES,0.4276126558005753,"
η2 Gt2
2 +
ρt −ρ∗2
2 −
ρt+1 −ρ∗2
2"
REFERENCES,0.42857142857142855,"
(74) = η 2"
REFERENCES,0.42953020134228187,"Gt2
2 + 1 2η"
REFERENCES,0.4304889741131352,"ρt −ρ∗2
2 −
ρt+1 −ρ∗2
2"
REFERENCES,0.4314477468839885,"
,
(75)"
REFERENCES,0.4324065196548418,Under review as a conference paper at ICLR 2022
REFERENCES,0.4333652924256951,"we have
LOSS1
s(ρt) −LOSS1
s(ρ∗) ≤
(76)
η
2"
REFERENCES,0.4343240651965484,"Gt2
2 + 1 2η"
REFERENCES,0.43528283796740175,"ρt −ρ∗2
2 −
ρt+1 −ρ∗2
2"
REFERENCES,0.436241610738255,"
−(ρt −ρ∗)T ζt
s −µ2 2"
REFERENCES,0.43720038350910834,"ρt −ρ∗2
2 .
(77)"
REFERENCES,0.43815915627996166,"But now note that LOSS1
s(ρt)−LOSS1
s(ρ∗) ≥LOSS1
s(ρt)−LOSS1
s(ρt+1). Thus, combining Equation
(77) and Lemma 16 yields"
REFERENCES,0.439117929050815,"−ηζt
s
T Gt ≤1 2η"
REFERENCES,0.44007670182166825,"ρt −ρ∗2
2 −
ρt+1 −ρ∗2
2"
REFERENCES,0.44103547459252157,"
−(ρt −ρ∗)T ζt
s −µ2 2"
REFERENCES,0.4419942473633749,"ρt −ρ∗2
2 .
(78)"
REFERENCES,0.4429530201342282,"By rearranging the terms, we then have
ρt+1 −ρ∗2
2 ≤(1 −µ2η)
ρt −ρ∗2
2 −η
 
ρt+1 −ρ∗T ζt
s
(79)"
REFERENCES,0.4439117929050815,"≤(1 −µ2η)
ρt −ρ∗2
2 + η
ρt+1 −ρ∗
2
ζt
s

2 .
(80)"
REFERENCES,0.4448705656759348,"Now note that η ≤1/L < 1/µ2 and thus 0 < 1 −µ2η < 1. We now deﬁne two sequences
ut ≜∥ρt −ρ∗∥2 and δt = η ∥ζt
s∥2. We already know that δt →0, and we want to show ut also
converges to 0. By Equation (80), we have
u2
t+1 ≤(1 −ηµ2)u2
t + δtut+1,
(81)
which implies

ut+1 −δt 2"
REFERENCES,0.4458293384467881,"2
= u2
t+1 −ut+1δt + δ2
t
4 ≤(1 −ηµ2)u2
t + δ2
t
4 ,
(82)"
REFERENCES,0.4467881112176414,and thus
REFERENCES,0.4477468839884947,ut+1 ≤ r
REFERENCES,0.44870565675934804,"(1 −ηµ2)u2
t + δ2
t
4 + δt 2 ≤
q"
REFERENCES,0.44966442953020136,"(1 −ηµ2)u2
t + δt"
REFERENCES,0.4506232023010546,2 + δt
REFERENCES,0.45158197507190795,"2 ≤

1 −ηµ2 2"
REFERENCES,0.45254074784276127,"
ut + δt.
(83)"
REFERENCES,0.4534995206136146,Lemma 19 allows to conclude.
REFERENCES,0.45445829338446786,"B.3.3
REDUCTION FROM MODEL ATTACK TO CONVERGING GRADIENT ATTACK"
REFERENCES,0.4554170661553212,"Proof of Lemma 15. Lemma 20 already provided the convergence part of Lemma 3. We now move
forward to proving the second part of the theorem, showing that for any ε > 0, there exists θ♠
s ∈Rd"
REFERENCES,0.4563758389261745,"such that
ρ∞−ρ∗(θ♠
s , ⃗D−s)

2 ≤ε. We deﬁne"
REFERENCES,0.4573346116970278,"LOSS2
s(ρ, θs) ≜inf
⃗θ−s"
REFERENCES,0.4582933844678811,"n
LOSS(ρ, ⃗θ, ⃗D) −Ls(θs, Ds)
o
(84)"
REFERENCES,0.4592521572387344,"= LOSS1
s(ρ) + R(ρ, θs) −ρT g∞
s ,
(85)
and ρ∗(θs), its minimizer. Therefore, we have"
REFERENCES,0.46021093000958774,"∇ρLOSS2
s(ρ, θs) = ∇ρLOSS1
s(ρ) + ∇ρR(ρ, θs) −g∞
s .
(86)"
REFERENCES,0.46116970278044106,"By Lemma 13, we know that LOSS2
s is locally strongly convex. Therefore, there exists µ3 > 0 such
that LOSS2
s(ρ) is µ3-strongly convex in {ρ| ∥ρ −ρ∗(θs)∥2 ≤1}. Therefore, for any 0 < ε < 1, if
∥ρ∞−ρ∗(θs)∥2 > ε, we then have"
REFERENCES,0.4621284755512943,"ε
∇ρLOSS2
s(ρ∞, θ♠
s )

2 ≥(ρ∞−ρ∗(θs))T ∇ρLOSS2
s(ρ∞, θ♠
s )
(87)"
REFERENCES,0.46308724832214765,"≥µ3 ∥ρ∞−ρ∗(θs)∥2
2 = µ3 ≥µ3ε2,
(88)"
REFERENCES,0.46404602109300097,"and thus
∇ρLOSS2
s(ρ∞, θ♠
s )

2 ≥µ3ε."
REFERENCES,0.4650047938638543,"Now since g∞
s
∈GRAD(ρ∞) there exists θ♠
s ∈Rd such that3 ∇ρR(ρ∞, θ♠
s ) −g∞
s

2 ≤
µ3ε"
REFERENCES,0.46596356663470756,"2
which yields ∇ρLOSS2
s(ρ∞, θ♠
s )

2 =
∇ρLOSS1
s(ρ∞) + ∇ρR(ρ∞, θ♠
s ) −g∞
s

2
(89)"
REFERENCES,0.4669223394055609,"=
∇ρR(ρ∞, θ♠
s ) −g∞
s

2 ≤µ3ε"
REFERENCES,0.4678811121764142,"2 ,
(90)"
REFERENCES,0.46883988494726747,"which is a contradiction. Therefore, we must have
ρ∞−ρ∗(θ♠
s , ⃗D−s)

2 ≤ε."
REFERENCES,0.4697986577181208,"3In fact, if g∞
s belongs to the interior of GRAD(ρ∞), we can guarantee ∇ρR(ρ∞, θ♠
s ) = g∞
s ."
REFERENCES,0.4707574304889741,Under review as a conference paper at ICLR 2022
REFERENCES,0.47171620325982744,"C
SUM OVER EXPECTATIONS"
REFERENCES,0.4726749760306807,"In this section, we provide both theoretical and empirical results to argue for using a sum-based local
loss over an expectation-based local loss."
REFERENCES,0.473633748801534,"C.1
THEORETICAL ARGUMENTS"
REFERENCES,0.47459252157238735,"Indeed, intuitively, if one considers an expectation Ex∼Dn [ℓ(θn, x)] rather than a sum, as is done by
Hanzely et al. (2020), Dinh et al. (2020) and El-Mhamdi et al. (2021), then the weight of an honest
active user’s local loss will not increase as a node provides more and more data, which will hinder
the ability of θn to ﬁt the user’s local data. In fact, intuitively, using an expectation wrongly yields
the same inﬂuence to any two nodes, even when one (honest) node provides a much larger dataset
Dn than the other, and should thus intuitively be regarded as “more reliable”."
REFERENCES,0.47555129434324067,"There is another theoretical argument for using the sum rather than the expectation.
Namely,
if the loss is regarded as a Bayesian negative log-posterior,
given a prior
exp

−P"
REFERENCES,0.47651006711409394,n∈[N] ν ∥θn∥2 −P
REFERENCES,0.47746883988494726,"n∈[N] R(ρ, θn)

on the local and global models, then the term that"
REFERENCES,0.4784276126558006,"ﬁts local data should equal the negative log-likelihood of the data, given the models (ρ, ⃗θ). Assum-
ing that the distribution of each data point x ∈Dn is independent from all other data points, and
depends only on the local model θn, this negative log-likelihood yields a sum over data points; not
an expectation."
REFERENCES,0.4793863854266539,"C.2
EMPIRICAL RESULTS"
REFERENCES,0.48034515819750717,"We also empirically compared the performances of sum as opposed to the expectation. To do so, we
constructed a setting where 10 “idle” users draw randomly 10 data points from the FashionMNIST
dataset, while one “active” user has all of the FashionMNIST dataset (60,000 data points). We
then learned local and global models, with R(ρ, θ) ≜λ ∥ρ −θ∥2
2, λ = 1. We compared two
different classiﬁers to which we refer as a “linear model” and “2-layers neural network”, both using
CrossEntropy loss. The linear model has (784+1)×10 parameters. The neural network has 2 layers
of 784 parameters with bias, with ReLU activation in between, adding up to ((784 + 1) × 784 +
(784 + 1) × 10."
REFERENCES,0.4813039309683605,"Note also that, in all our experiments, we did not consider any local regularization, i.e. we set ν ≜0.
All our experiments are seeded with seed 999."
REFERENCES,0.4822627037392138,"C.2.1
NOISY FASHIONMNIST"
REFERENCES,0.48322147651006714,"To see a strong difference between sum and average, we made the FashionMNIST dataset harder
to learn, by randomly labeling 60% of the training set. Table 1 reports the accuracy of local and
global models in the different settings. Our results clearly and robustly indicate that the use of sums
outperforms the use of expectations."
REFERENCES,0.4841802492809204,"EL
ΣL
ENN
ΣNN
idle user’s model
0.52
0.80
0.55
0.79
active user’s model
0.58
0.80
0.56
0.79
global model
0.55
0.80
0.58
0.79"
REFERENCES,0.4851390220517737,"Table 1: Accuracy of trained models, depending on the use of expectation (denoted E) or sum (Σ),
and on the use of linear classiﬁer (L) or a 2-layer neural net (NN). Here, all users are honest and
an ℓ2
2 regularization is used, but there is a large heterogeneity in the amount of data per user."
REFERENCES,0.48609779482262705,"On each of the following plots, we display the top-1 accuracy on the MNIST test dataset (10 000
images) for the active user, for the global model and for one of the idle users (in Table 1, the mean
accuracy for idle users is reported), as we vary the value of λ. Intuitively, λ models how much we
want the local models to be similar."
REFERENCES,0.48705656759348037,Under review as a conference paper at ICLR 2022
REFERENCES,0.48801534036433364,"In the case of learning FashionMNIST, given that the data is i.i.d., larger values of λ are more
meaningful (though our experiments show that they may hinder convergence speed). However,
in settings where users have different data distributions, e.g. because the labels depend on users’
preferences, then smaller values of λ may be more relevant."
REFERENCES,0.48897411313518696,"Note that the use of a common value of λ in both cases is slightly misleading, as using the sum
intuitively decreases the comparative weight of the regularization term. To reduce this effect, for
this experiment only, we divide the local losses by the average of the number of data points per
node for the sum version. This way, if the number of points is equal for all nodes, the two losses
will be exactly the same. What’s more, our experiments seem to robustly show that using the sum
consistently outperforms the expectation, for both a linear classiﬁer and a 2-layer neural network,
for the problem of noisy FashionMNIST classiﬁcation."
REFERENCES,0.4899328859060403,"0
25
50
75
100
125
150
175
200
Epochs 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.49089165867689355,Accuracy
REFERENCES,0.49185043144774687,"acc_big
acc_small
acc_glob"
REFERENCES,0.4928092042186002,(a) Using the average
REFERENCES,0.4937679769894535,"0
25
50
75
100
125
150
175
200
Epochs 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.4947267497603068,Accuracy
REFERENCES,0.4956855225311601,"acc_big
acc_small
acc_glob"
REFERENCES,0.4966442953020134,(b) Using the sum
REFERENCES,0.49760306807286675,"Figure 4: Linear model on noisy FashionMNIST, for λ = 0.01."
REFERENCES,0.49856184084372,"0
25
50
75
100
125
150
175
200
Epochs 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.49952061361457334,Accuracy
REFERENCES,0.5004793863854267,"acc_big
acc_small
acc_glob"
REFERENCES,0.50143815915628,(a) Using the average
REFERENCES,0.5023969319271333,"0
25
50
75
100
125
150
175
200
Epochs 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.5033557046979866,Accuracy
REFERENCES,0.5043144774688398,"acc_big
acc_small
acc_glob"
REFERENCES,0.5052732502396932,(b) Using the sum
REFERENCES,0.5062320230105465,"Figure 5: 2-layer neural network on noisy FashionMNIST, for λ = 0.01."
REFERENCES,0.5071907957813998,Under review as a conference paper at ICLR 2022
REFERENCES,0.5081495685522531,"0
25
50
75
100
125
150
175
200
Epochs 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.5091083413231065,Accuracy
REFERENCES,0.5100671140939598,"acc_big
acc_small
acc_glob"
REFERENCES,0.5110258868648131,(a) Using the average
REFERENCES,0.5119846596356663,"0
25
50
75
100
125
150
175
200
Epochs 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.5129434324065196,Accuracy
REFERENCES,0.513902205177373,"acc_big
acc_small
acc_glob"
REFERENCES,0.5148609779482263,(b) Using the sum
REFERENCES,0.5158197507190796,"Figure 6: Linear model on noisy FashionMNIST, for λ = 0.1."
REFERENCES,0.5167785234899329,"0
25
50
75
100
125
150
175
200
Epochs 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.5177372962607862,Accuracy
REFERENCES,0.5186960690316395,"acc_big
acc_small
acc_glob"
REFERENCES,0.5196548418024928,(a) Using the average
REFERENCES,0.5206136145733461,"0
25
50
75
100
125
150
175
200
Epochs 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.5215723873441994,Accuracy
REFERENCES,0.5225311601150527,"acc_big
acc_small
acc_glob"
REFERENCES,0.5234899328859061,(b) Using the sum
REFERENCES,0.5244487056567594,"Figure 7: 2-layer neural network on noisy FashionMNIST, for λ = 0.1."
REFERENCES,0.5254074784276127,"0
25
50
75
100
125
150
175
200
Epochs 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.5263662511984659,Accuracy
REFERENCES,0.5273250239693192,"acc_big
acc_small
acc_glob"
REFERENCES,0.5282837967401726,(a) Using the average
REFERENCES,0.5292425695110259,"0
25
50
75
100
125
150
175
200
Epochs 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.5302013422818792,Accuracy
REFERENCES,0.5311601150527325,"acc_big
acc_small
acc_glob"
REFERENCES,0.5321188878235859,(b) Using the sum
REFERENCES,0.5330776605944392,"Figure 8: Linear model on noisy FashionMNIST, for λ = 1."
REFERENCES,0.5340364333652924,Under review as a conference paper at ICLR 2022
REFERENCES,0.5349952061361457,"0
25
50
75
100
125
150
175
200
Epochs 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.535953978906999,Accuracy
REFERENCES,0.5369127516778524,"acc_big
acc_small
acc_glob"
REFERENCES,0.5378715244487057,(a) Using the average
REFERENCES,0.538830297219559,"0
25
50
75
100
125
150
175
200
Epochs 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.5397890699904123,Accuracy
REFERENCES,0.5407478427612655,"acc_big
acc_small
acc_glob"
REFERENCES,0.5417066155321189,(b) Using the sum
REFERENCES,0.5426653883029722,"Figure 9: 2-layer neural network on noisy FashionMNIST, for λ = 1."
REFERENCES,0.5436241610738255,"0
25
50
75
100
125
150
175
200
Epochs 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.5445829338446788,Accuracy
REFERENCES,0.5455417066155321,"acc_big
acc_small
acc_glob"
REFERENCES,0.5465004793863855,(a) Using the average
REFERENCES,0.5474592521572388,"0
25
50
75
100
125
150
175
200
Epochs 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.548418024928092,Accuracy
REFERENCES,0.5493767976989453,"acc_big
acc_small
acc_glob"
REFERENCES,0.5503355704697986,(b) Using the sum
REFERENCES,0.551294343240652,"Figure 10: Linear model on noisy FashionMNIST, for λ = 10."
REFERENCES,0.5522531160115053,"0
25
50
75
100
125
150
175
200
Epochs 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.5532118887823586,Accuracy
REFERENCES,0.5541706615532119,"acc_big
acc_small
acc_glob"
REFERENCES,0.5551294343240653,(a) Using the average
REFERENCES,0.5560882070949185,"0
25
50
75
100
125
150
175
200
Epochs 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.5570469798657718,Accuracy
REFERENCES,0.5580057526366251,"acc_big
acc_small
acc_glob"
REFERENCES,0.5589645254074784,(b) Using the sum
REFERENCES,0.5599232981783318,"Figure 11: 2-layer neural network on noisy FashionMNIST, for λ = 10."
REFERENCES,0.5608820709491851,Under review as a conference paper at ICLR 2022
REFERENCES,0.5618408437200384,"0
25
50
75
100
125
150
175
200
Epochs 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.5627996164908916,Accuracy
REFERENCES,0.5637583892617449,"acc_big
acc_small
acc_glob"
REFERENCES,0.5647171620325983,(a) Using the average
REFERENCES,0.5656759348034516,"0
25
50
75
100
125
150
175
200
Epochs 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.5666347075743049,Accuracy
REFERENCES,0.5675934803451582,"acc_big
acc_small
acc_glob"
REFERENCES,0.5685522531160115,(b) Using the sum
REFERENCES,0.5695110258868649,"Figure 12: Linear model on noisy FashionMNIST, for λ = 100."
REFERENCES,0.5704697986577181,"0
25
50
75
100
125
150
175
200
Epochs 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.5714285714285714,Accuracy
REFERENCES,0.5723873441994247,"acc_big
acc_small
acc_glob"
REFERENCES,0.573346116970278,(a) Using the average
REFERENCES,0.5743048897411314,"0
25
50
75
100
125
150
175
200
Epochs 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.5752636625119847,Accuracy
REFERENCES,0.576222435282838,"acc_big
acc_small
acc_glob"
REFERENCES,0.5771812080536913,(b) Using the sum
REFERENCES,0.5781399808245445,"Figure 13: 2-layer neural network on noisy FashionMNIST, for λ = 100."
REFERENCES,0.5790987535953979,"C.2.2
FASHIONMNIST WITHOUT NOISE"
REFERENCES,0.5800575263662512,"Recall that we introduced noise into FashionMNIST to make the problem harder to learn and ob-
serve a clear difference between the average and the sum. In this section, we present results of our
experiments when the noise is removed."
REFERENCES,0.5810162991371045,"0
200
400
600
800
1000
Epochs 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.5819750719079578,Accuracy
REFERENCES,0.5829338446788112,"acc_big
acc_small
acc_glob"
REFERENCES,0.5838926174496645,(a) Using the average
REFERENCES,0.5848513902205177,"0
200
400
600
800
1000
Epochs 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.585810162991371,Accuracy
REFERENCES,0.5867689357622243,"acc_big
acc_small
acc_glob"
REFERENCES,0.5877277085330777,(b) Using the sum
REFERENCES,0.588686481303931,"Figure 14: Linear model on FashionMNIST (without noise), for λ = 1."
REFERENCES,0.5896452540747843,Under review as a conference paper at ICLR 2022
REFERENCES,0.5906040268456376,"0
250
500
750
1000
1250
1500
1750
2000
Epochs 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.5915627996164909,Accuracy
REFERENCES,0.5925215723873442,"acc_big
acc_small
acc_glob"
REFERENCES,0.5934803451581975,(a) Using the average
REFERENCES,0.5944391179290508,"0
250
500
750
1000
1250
1500
1750
2000
Epochs 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.5953978906999041,Accuracy
REFERENCES,0.5963566634707574,"acc_big
acc_small
acc_glob"
REFERENCES,0.5973154362416108,(b) Using the sum
REFERENCES,0.5982742090124641,"Figure 15: 2-layer neural network on FashionMNIST (without noise), for λ = 1."
REFERENCES,0.5992329817833174,"Even without noise, the difference between using the sum and using the expectation still seems
important. We acknowledge, however, that the plots suggest that even though we ran this experiment
for 10 times more (and 5 times more for the linear model) than other experiments, we might not have
reached convergence yet, and that the use of the expectation might still eventually gets closer to the
case of sum. We believe that the fact that the difference between sum and expectation in the absence
of noise is weak is due to the fact that the FashionMNIST dataset is sufﬁciently linearly separable.
Thus, we achieve a near-zero loss in both cases, which make the sum and the expectation close at
optimum."
REFERENCES,0.6001917545541706,"Even in this case, however, we observed that the sum clearly outperforms the expectation especially,
in the ﬁrst epochs. We argue that the reason for this is the following. By taking the average in local
losses, the weights of the data of idle nodes are essentially blown out of proportion. As a result, the
optimizer will very quickly ﬁt these data. However, the signal from the data of the active node will
then be too weak, so that the optimizer has to ﬁrst almost perfectly ﬁt the idle nodes’ data before it
can catch the signal of the active node’s data and hence the average achieves weaker convergence
performances than the sum."
REFERENCES,0.6011505273250239,"D
LINEAR REGRESSION AND CLASSIFICATION ARE GRADIENT PAC*"
REFERENCES,0.6021093000958773,"Throughout this section, we use the following terminology."
REFERENCES,0.6030680728667306,"Deﬁnition 5. Consider a parameterized event E(I). We say that the event E occurs with high
probability if P [E(I)] →1 as I →∞."
REFERENCES,0.6040268456375839,"D.1
PRELIMINARIES"
REFERENCES,0.6049856184084372,"Deﬁne ∥Σ∥2 ≜max∥x∥2̸=0(∥Σx∥2 / ∥x∥2) the ℓ2 operator norm of the matrix Σ. For symmetric
matrices Σ, this is also the largest eigenvalue in absolute value."
REFERENCES,0.6059443911792906,"Theorem 6 (Covariance concentration, Theorem 6.5 in Wainwright (2019)). Denote Σ
=
E

QiQT
i

, where Qi ∈Rd is from a σQ-sub-Gaussian random distribution ˜Q. Then, there are
universal constants c1, c2 and c3 such that, for any set {Qi}i∈[I] of i.i.d. samples from ˜Q, and any"
REFERENCES,0.6069031639501438,"δ > 0, the sample covariance bΣ = 1"
REFERENCES,0.6078619367209971,"I
P QiQT
i satisﬁes the bound P"
REFERENCES,0.6088207094918504,"""
1
σ2
Q"
REFERENCES,0.6097794822627037,"ˆΣ −Σ

2 ≥c1 r"
REFERENCES,0.610738255033557,"d
I + d I ! + δ #"
REFERENCES,0.6116970278044104,"≤c2 exp
 
−c3I min(δ, δ2)

.
(91)"
REFERENCES,0.6126558005752637,"Theorem 7 (Weyl’s Theorem, Theorem 4.3.1 in Horn & Johnson (2012)). Let A and B be Hermi-
tian4 and let the respective eigenvalues of A and B and A + B be {λi(A)}d
i=1, {λi(B)}d
i=1, and"
REFERENCES,0.613614573346117,"4For real matrices, Hermitian is the same as symmetric."
REFERENCES,0.6145733461169702,Under review as a conference paper at ICLR 2022
REFERENCES,0.6155321188878236,"{λi(A + B)}d
i=1, each increasingly ordered. Then"
REFERENCES,0.6164908916586769,"λi(A + B) ≤λi+j(A) + λd−j(B),
j = 0, 1, ..., d −i,
(92)"
REFERENCES,0.6174496644295302,"and
λi+j(A) + λj+1(B) ≤λi(A + B),
j = 0, ..., i −1,
(93)
for each i = 1, ..., d.
Lemma 21. Consider two symmetric deﬁnite positive matrices S and Σ. Denote ρmin and λmin
their minimal eigenvalues. Then |ρmin −λmin| ≤∥S −Σ∥2."
REFERENCES,0.6184084372003835,"Proof. This is a direct consequence of Theorem 7, for A = S, B = Σ −S, i = 1, and j = 0."
REFERENCES,0.6193672099712368,"Corollary 1. There are universal constants c1, c2 and c3 such that, for any σQ-sub-Gaussian vector
distribution ˜Q ∈Rd and any δ > 0, the sample covariance bΣ = 1"
REFERENCES,0.6203259827420902,"I
P QiQT
i satisﬁes the bound P"
REFERENCES,0.6212847555129435,"""
1
σ2
Q"
REFERENCES,0.6222435282837967,"min SP(ˆΣ) −min SP(Σ)
 ≥c1 r"
REFERENCES,0.62320230105465,"d
I + d I ! + δ #"
REFERENCES,0.6241610738255033,"≤c2 exp
 
−c3I min(δ, δ2)

,
(94)"
REFERENCES,0.6251198465963567,where min SP(ˆΣ) and min SP(Σ) are the minimal eigenvalues of ˆΣ and Σ.
REFERENCES,0.62607861936721,Proof. This follows from Theorem 6 and Lemma 21.
REFERENCES,0.6270373921380633,"Lemma 22. With high probability, min SP(ˆΣ) ≥min SP(Σ)/2."
REFERENCES,0.6279961649089166,"Proof. Denote λmin ≜min SP(Σ) and bλmin ≜min SP(ˆΣ). Since each Qi is drawn i.i.d. from a
σQ-sub-Gaussian, we can apply Corollary 1. Namely, there are constants c1, c2 and c3, such that for
any δ > 0, we have P"
REFERENCES,0.6289549376797698,"""bλmin −λmin
 ≥c1σ2
Q r"
REFERENCES,0.6299137104506232,"d
I + d I !"
REFERENCES,0.6308724832214765,"+ δσ2
Q #"
REFERENCES,0.6318312559923298,"≤c2 exp (−c3I min

δ, δ2	
).
(95)"
REFERENCES,0.6327900287631831,"We now set δ
≜λmin/(4σ2
Q) and we consider I large enough so that c1
q"
REFERENCES,0.6337488015340365,"d
I + d I

≤"
REFERENCES,0.6347075743048898,"λmin/(4σ2
Q). With high probability, we then have bλmin ≥λmin/2."
REFERENCES,0.6356663470757431,"D.2
LINEAR REGRESSION IS GRADIENT-PAC*"
REFERENCES,0.6366251198465963,"In this section, we prove the ﬁrst part of Lemma 4. Namely, we prove that linear regression is
gradient-PAC* learning."
REFERENCES,0.6375838926174496,"D.2.1
LEMMAS FOR LINEAR REGRESSION"
REFERENCES,0.638542665388303,"Before moving to the main proof that linear regression is gradient-PAC*, we ﬁrst prove a few useful
lemmas. These lemmas will rest on the following well-known theorems.
Theorem 8 (Lemma 2.7.7 in Vershynin (2018)). If X and Y are sub-Gaussian, then XY is sub-
exponential.
Theorem 9 (Equation 2.18 in Wainwright (2019)). If X1, . . . , XI are iid sub-exponential variables,
then there exist constants c4, c5 such that, for all I, we have"
REFERENCES,0.6395014381591563,"∀t ∈[0, c4], P [|X −E [X]| ≥tI] ≤2 exp
 
−c5It2
.
(96)"
REFERENCES,0.6404602109300096,"Lemma 23. For all j ∈[d], the random variables Xi ≜ξiQi[j] are iid, sub-exponential and have
zero mean."
REFERENCES,0.6414189837008629,"Proof. The fact that these variables are iid follows straightforwardly from the fact that the noises
ξi are iid, and the queries Qi are also iid. Moreover, both are sub-Gaussian, and by Theorem 8,
the product of sub-Gaussian variables is sub-exponential. Finally, we have E [X] = E [ξQ[j]] =
E [ξ] E [Q[j]] = 0, using the independence of the noise and the query, and the fact that noises have
zero mean (E [ξ] = 0)."
REFERENCES,0.6423777564717162,Under review as a conference paper at ICLR 2022
REFERENCES,0.6433365292425696,"Lemma 24. There exists B such that
P"
REFERENCES,0.6442953020134228,"i∈I ξiQi

2 ≤BI3/4 with high probability."
REFERENCES,0.6452540747842761,"Proof. By Lemma 23, the terms ξiQi[j] are iid, sub-exponential and have zero mean. Therefore, by
Theorem 9, there exist constants c4 and c5 such that for any coordinate j ∈[d] of ξiQi and for all
0 ≤u ≤c4, we have P "" X"
REFERENCES,0.6462128475551294,"i∈I
ξiQi[j] ≥Iu #"
REFERENCES,0.6471716203259827,"≤2 exp (−c5Iu2).
(97)"
REFERENCES,0.6481303930968361,"Plugging u = vI(−1/4) into the inequality for some small enough constant v, and using union bound
then yields P    X"
REFERENCES,0.6490891658676894,"i∈I
ξiQi"
REFERENCES,0.6500479386385427,"2
≥I(3/4)v
√ d  ≤P    X"
REFERENCES,0.6510067114093959,"i∈I
ξiQi"
REFERENCES,0.6519654841802492,"∞
≥I(3/4)v "
REFERENCES,0.6529242569511026,"≤2d exp (−c5
p"
REFERENCES,0.6538830297219559,"Iv2).
(98)"
REFERENCES,0.6548418024928092,"Deﬁning B ≜v
√"
REFERENCES,0.6558005752636625,d yields the lemma.
REFERENCES,0.6567593480345159,"D.2.2
PROOF THAT LINEAR REGRESSION IS GRADIENT-PAC*"
REFERENCES,0.6577181208053692,We now move on to proving that least square linear regression is gradient-PAC*.
REFERENCES,0.6586768935762224,"Proof of Theorem 2. Note that ∇θℓ(θ, Q, A) = (θT Q −A)Q. Thus, on input i ∈[I], we have"
REFERENCES,0.6596356663470757,"∇θℓ(θ, Qi, A(Qi, θ†)) =
 
(θ −θ†)T Qi

Qi −ξiQi.
(99)"
REFERENCES,0.660594439117929,"Moreover, we have"
REFERENCES,0.6615532118887824,"(θ −θ†)T ∇θ

ν ∥θ∥2
2

= 2ν(θ −θ†)T θ = 2ν
θ −θ†2"
REFERENCES,0.6625119846596357,"2 + 2ν(θ −θ†)T θ†.
(100)"
REFERENCES,0.663470757430489,"As a result, we have"
REFERENCES,0.6644295302013423,"(θ −θ†)T ∇θL(θ, D) =
(101)"
REFERENCES,0.6653883029721956,"I(θ −θ†)T bΣ(θ −θ†) −(θ −θ†)T
 X"
REFERENCES,0.6663470757430489,"i∈I
ξiQi !"
REFERENCES,0.6673058485139022,"+ 2ν
θ −θ†2"
REFERENCES,0.6682646212847555,"2 + 2ν(θ −θ†)T θ†.
(102)"
REFERENCES,0.6692233940556088,"But now, with high probability, we have (θ−θ†)T bΣ(θ−θ†) ≥(λmin/2)
θ −θ†2
2 (Lemma 22) and
P"
REFERENCES,0.6701821668264621,"i∈I ξiQi

2 ≤BI(3/4) (Lemma 24). Using the fact that
θ†
2 ≤K and the Cauchy-Schwarz
inequality, we have"
REFERENCES,0.6711409395973155,"(θ −θ†)T ∇θL(θ, D) ≥(λmin"
REFERENCES,0.6720997123681688,"2
I + ν)
θ −θ†2"
REFERENCES,0.673058485139022,"2 −(BI(3/4) + 2νK)
θ −θ†
2 .
(103)"
REFERENCES,0.6740172579098753,Denoting AK ≜λmin
REFERENCES,0.6749760306807286,"2
and BK ≜B + 2νK and using the fact that I ≥1, we then have"
REFERENCES,0.675934803451582,"(θ −θ†)T ∇θL(θ, D) ≥AKI
θ −θ†2"
REFERENCES,0.6768935762224353,"2 −BKI(3/4) θ −θ†
2
(104)"
REFERENCES,0.6778523489932886,"≥AKI min
nθ −θ†
2 ,
θ −θ†2 2"
REFERENCES,0.6788111217641419,"o
−BKI(3/4) θ −θ†
2 ,
(105)"
REFERENCES,0.6797698945349953,with high probability. This corresponds to saying Assumption 2 is satisﬁed for α = 3/4.
REFERENCES,0.6807286673058485,"D.3
LOGISTIC REGRESSION"
REFERENCES,0.6816874400767018,"In this section, we now prove the second part of Lemma 4. Namely, we prove that logistic regression
is gradient-PAC* learning."
REFERENCES,0.6826462128475551,Under review as a conference paper at ICLR 2022
REFERENCES,0.6836049856184084,"D.3.1
LEMMAS ABOUT THE SIGMOID FUNCTION"
REFERENCES,0.6845637583892618,We ﬁrst prove two useful lemmas about the following logistic distance function.
REFERENCES,0.6855225311601151,"Deﬁnition 6. We deﬁne the logistic distance function by ∆(a, b) ≜(a −b) (σ(a) −σ(b))."
REFERENCES,0.6864813039309684,"Lemma 25. If a, b ∈R such that for some k > 0, |a| ≤k and |b| ≤k, then there exists some
constant ck > 0 such that
∆(a, b) ≥ck |a −b|2 .
(106)"
REFERENCES,0.6874400767018217,"Proof. Note that the derivative of σ(z) is strictly positive, symmetric (σ′(z) = σ′(−z)) and mono-
tonically decreasing for z ≥0. Therefore, for any z ∈[−k, k], we know σ′(z) ≥ck ≜σ′(k). Thus,
by the mean value theorem, we have"
REFERENCES,0.6883988494726749,σ(a) −σ(b)
REFERENCES,0.6893576222435283,"a −b
≥ck.
(107)"
REFERENCES,0.6903163950143816,Multiplying both sides by (a −b)2 then yields the lemma.
REFERENCES,0.6912751677852349,"Lemma 26. If b ∈R, and |b| ≤k, for some k > 0, then there exists a constant dk, such that for any
a ∈R, we have
∆(a, b) ≥dk |a −b| −dk
(108)"
REFERENCES,0.6922339405560882,"Proof. Assume |a −b| ≥1 and deﬁne dk ≜σ(k + 1) −σ(k). If b ≥0, since σ′(z) is decreasing
for z ≥0, we have σ(b) −σ(b −1) ≥σ(b + 1) −σ(b) ≥dk, and by symmetry, a similar argument
holds for b ≤0. Thus, we have"
REFERENCES,0.6931927133269415,"|σ(a) −σ(b)| ≥min {σ(b) −σ(b −1), σ(b + 1) −σ(b)} ≥dk.
(109)"
REFERENCES,0.6941514860977949,"Therefore,
(a −b) (σ(a) −σ(b)) ≥dk |a −b| ≥dk |a −b| −dk.
(110)"
REFERENCES,0.6951102588686481,"For the case of |a −b| ≤1, we also have (a −b) (σ(a) −σ(b)) ≥0 ≥dk |a −b| −dk."
REFERENCES,0.6960690316395014,"D.3.2
A UNIFORM LOWER BOUND"
REFERENCES,0.6970278044103547,"Deﬁnition 7. Denote Sd−1 ≜

u ∈Rd  ∥u∥2 = 1
	
the hypersphere in Rd."
REFERENCES,0.697986577181208,"Lemma 27. Assume SUPP( ˜Q) spans Rd. Then, for all u ∈Sd−1, E
QT u

> 0."
REFERENCES,0.6989453499520614,"Proof. Let u ∈Sd−1. We know that there exists Q1, . . . , Qd ∈SUPP( ˜Q) and α1, . . . , αd ∈R
such that u is colinear with P αjQj. In particular, we then have uT P αjQj = P αj(QT
j u) ̸= 0.
Therefore, there must be a query Q∗∈SUPP( ˜Q) such that QT
∗u ̸= 0, which implies a ≜
QT
∗u
 >
0 By continuity of the scalar product, there must then also exist ε > 0 such that, for any Q ∈
B(Q∗, ε), we have
QT u
 ≥a/2, where B(Q∗, ε) is an Euclidean ball centered on Q∗and of radius
ε."
REFERENCES,0.6999041227229147,"But now, by deﬁnition of the support, we know that p ≜P [Q ∈B(Q∗, ε)] > 0. By the law of total
expectation, we then have"
REFERENCES,0.700862895493768,"E
QT u

= E
QT u
  Q ∈B(Q∗, ε)

P [Q ∈B(Q∗, ε)]"
REFERENCES,0.7018216682646213,"+ E
QT u
  Q /∈B(Q∗, ε)

P [Q /∈B(Q∗, ε)]
(111)"
REFERENCES,0.7027804410354745,"≥ap/2 + 0 > 0,
(112)"
REFERENCES,0.7037392138063279,which is the lemma.
REFERENCES,0.7046979865771812,"Lemma 28. Assume that, for all unit vectors u ∈Sd−1, we have E
QT u

> 0, and that SUPP( ˜Q)
is bounded by MQ. Then there exists C > 0 such that, with high probability,"
REFERENCES,0.7056567593480345,"∀u ∈Sd−1,
X i∈I"
REFERENCES,0.7066155321188878,"QT
i u
 ≥CI.
(113)"
REFERENCES,0.7075743048897412,Under review as a conference paper at ICLR 2022
REFERENCES,0.7085330776605945,"Proof. By continuity of the scalar product and the expectation operator, and by compactness of
Sd−1, we know that
C0 ≜inf
u∈Rd E
QT u

> 0.
(114)"
REFERENCES,0.7094918504314478,Now deﬁne ε ≜C0/4MQ. Note that Sd−1 ⊂S
REFERENCES,0.710450623202301,"u∈Sd−1 B(u, ε). Thus we have a covering of
the hypersphere by open sets. But since Sd−1 is compact, we know that we can extract a ﬁnite
covering. In other words, there exists a ﬁnite subset S ⊂Sd−1 such that Sd−1 ⊂S"
REFERENCES,0.7114093959731543,"u∈S B(u, ε). Put
differently, for any v ∈Sd−1, there exists u ∈S such that ∥u −v∥2 ≤ε."
REFERENCES,0.7123681687440077,"Now consider u ∈S. Given that SUPP( ˜Q) is bounded, we know that
QT
i u
 ∈[0, MQ]. Moreover,
such variables
QT
i u
 are iid. By Hoeffding’s inequality, for any t > 0, we have P "" X i∈I"
REFERENCES,0.713326941514861,"QT
i u
 −IE
QT u

 ≥It #"
REFERENCES,0.7142857142857143,"≤2 exp
−2It2 MQ"
REFERENCES,0.7152444870565676,"
.
(115)"
REFERENCES,0.716203259827421,"Choosing t = C0/2 then yields P ""X i∈I"
REFERENCES,0.7171620325982742,"QT
i u
 ≤C0I 2 # ≤P "" X i∈I"
REFERENCES,0.7181208053691275,"QT
i uθ−θ†
 −IE
QT uθ−θ†

 ≥IC0 2 # (116)"
REFERENCES,0.7190795781399808,"≤2 exp
−IC0
2"
MQ,0.7200383509108341,2MQ
MQ,0.7209971236816874,"
.
(117)"
MQ,0.7219558964525408,"Taking a union bound for u ∈S then guarantees P """
MQ,0.7229146692233941,"∀u ∈S,
X i∈I"
MQ,0.7238734419942474,"QT
i u
 ≥C0I 2 #"
MQ,0.7248322147651006,"≥1 −2 |S| exp
−IC0
2"
MQ,0.725790987535954,2MQ
MQ,0.7267497603068073,"
,
(118)"
MQ,0.7277085330776606,"which clearly goes to 1 as I →∞. Thus ∀u ∈S, P"
MQ,0.7286673058485139,"i∈I
QT
i u
 ≥C0I"
MQ,0.7296260786193672,"2
holds with high probability."
MQ,0.7305848513902206,"Now consider v ∈Sd−1. We know that there exists u ∈S such that ∥u −v∥2 ≤ε. Then, we have
X i∈[I]"
MQ,0.7315436241610739,"QT
i v
 =
X i∈[I]"
MQ,0.7325023969319271,"QT
i u + QT
i (v −u)

(119) ≥
X i∈[I]"
MQ,0.7334611697027804,"QT
i u
 −IMQ ∥v −u∥2
(120) ≥C0I"
MQ,0.7344199424736337,"2
−IMQ
C0
4MQ
= C0I"
MQ,0.7353787152444871,"4 ,
(121)"
MQ,0.7363374880153404,which proves the lemma.
MQ,0.7372962607861937,"D.3.3
LOWER BOUND ON THE DISCREPANCY BETWEEN PREFERRED AND REPORTED
ANSWERS"
MQ,0.738255033557047,"Lemma 29. Assume that ˜Q has a bounded support, whose interior contains the origin. Suppose
also that
θ†
2 ≤K. Then there exists AK such that, with high probability, we have
X"
MQ,0.7392138063279002,"i∈[I]
∆(QT
i θ, QT
i θ†) ≥AKI min
nθ −θ†
2 ,
θ −θ†2 2"
MQ,0.7401725790987536,"o
.
(122)"
MQ,0.7411313518696069,"Proof. Note that by Cauchy-Schwarz inequality we have
QT
i θ† ≤∥Qi∥2
θ†
2 ≤MQK.
(123)"
MQ,0.7420901246404602,"Thus, Lemma 26 implies the existence of a positive constant dK, such that for all θ ∈Rd, we have
X"
MQ,0.7430488974113135,"i∈I
∆
 
QT
i θ, QT
i θ†
≥
X i∈I"
MQ,0.7440076701821668," 
dK
QT
i θ −QT
i θ† −dK

(124)"
MQ,0.7449664429530202,"= −dKI + dK
θ −θ†
2
X i∈I"
MQ,0.7459252157238735,"QT
i uθ−θ†
 ,
(125)"
MQ,0.7468839884947267,Under review as a conference paper at ICLR 2022
MQ,0.74784276126558,"where uθ−θ† ≜(θ −θ†)/
θ −θ†
2 is the unit vector in the direction of θ −θ†."
MQ,0.7488015340364333,"Now, by Lemma 28, we know that, with high probability, for all unit vectors u ∈Sd−1, we have
P QT
i u
 ≥CI. Thus, for I sufﬁciently large, for any θ ∈Rd, with high probability, we have X"
MQ,0.7497603068072867,"i∈I
∆(QT
i θ, QT
i θ†) ≥dKCmin"
I,0.75071907957814,"2
I
θ −θ†
2 −dKI.
(126)"
I,0.7516778523489933,Deﬁning eK ≜dKCmin
I,0.7526366251198466,"4
, and fK ≜
4
Cmin , for
θ −θ†
2 > fK, we then have X"
I,0.7535953978907,"i∈I
∆(QT
i θ, QT
i θ†) ≥eKI
θ −θ†
2 .
(127)"
I,0.7545541706615532,"We now focus on the case of
θ −θ†
2 ≤fK. The triangle inequality yields ∥θ∥2 ≤
θ −θ†
2 +
θ†
2 ≤fK + K. By Cauchy-Schwarz inequality, we then have
QT
i θ
 ≤(fK + K)MQ ≜gK and
QT
i θ† ≤KMQ ≤gK. Thus, by Lemma 25, we know there exists some constant cK such that X i∈I"
I,0.7555129434324065," 
σ(QT
i θ) −σ(QT
i θ†)

(QT
i θ −QT
i θ†) ≥
X"
I,0.7564717162032598,"i∈I
cK
QT
i θ −QT
i θ†2
(128) =
X"
I,0.7574304889741131,"i∈I
cK(θ −θ†)T QiQT
i (θ −θ†)
(129)"
I,0.7583892617449665,"= cK(θ −θ†)T
 X"
I,0.7593480345158198,"i∈I
QiQT
i !"
I,0.7603068072866731,"(θ −θ†).
(130)"
I,0.7612655800575263,"Since distribution ˜Q is bounded (and thus sub-Gaussian), by Theorem 6, with high probability, we
have"
I,0.7622243528283796,"(θ −θ†)T
 X"
I,0.763183125599233,"i∈I
QiQT
i !"
I,0.7641418983700863,(θ −θ†) ≥λmin
I,0.7651006711409396,"2
I
θ −θ†2"
I,0.7660594439117929,"2 ,
(131)"
I,0.7670182166826462,"where λmin is the smallest eigenvalue of E

QiQT
i

. Thus, for
θ −θ†
2 ≤fK, we have X i∈I"
I,0.7679769894534996," 
σ(QT
i θ) −σ(QT
i θ†)

(QT
i θ −QT
i θ†) ≥λmincK"
I,0.7689357622243528,"2
I
θ −θ†2"
I,0.7698945349952061,"2 .
(132)"
I,0.7708533077660594,"Combining this with (127), and deﬁning AK ≜min
 λmincK"
I,0.7718120805369127,"2
, eK
	
, we then obtain the lemma."
I,0.7727708533077661,"D.3.4
PROOF THAT LOGISTIC REGRESSION IS GRADIENT-PAC*"
I,0.7737296260786194,Now we proceed with the proof that logistic regression is gradient-PAC*.
I,0.7746883988494727,Proof of Theorem 3. Note that σ(−z) = e−zσ(z) = 1−σ(z) and σ′(z) = e−zσ2(z). We then have
I,0.775647171620326,"∇θℓ(θ, Q, A) = −σ′(AQT θ)AQ"
I,0.7766059443911792,"σ(AQT θ)
= −e−AQT θσ(AQT θ)AQ
(133)"
I,0.7775647171620326,"= −σ(−AQT θ)AQ =
 
σ(QT θ) −1 [A = 1]

Q,
(134)"
I,0.7785234899328859,Under review as a conference paper at ICLR 2022
I,0.7794822627037392,"where 1 [A = 1] is the indicator function that outputs 1 if A = 1, and 0 otherwise. As a result,"
I,0.7804410354745925,"(θ −θ†)T ∇θL(θ, D) =
(135)"
I,0.7813998082454459,"(θ −θ†)T
 X i∈I"
I,0.7823585810162992," 
σ(QT
i θ) −1 [Ai = 1]

Qi !"
I,0.7833173537871524,"+ 2ν(θ −θ†)T θ
(136)"
I,0.7842761265580057,"= (θ −θ†)T
 X i∈I"
I,0.785234899328859," 
σ(QT
i θ) −σ(QT
i θ†) + σ(QT
i θ†) −1 [Ai = 1]

Qi ! (137)"
I,0.7861936720997124,"+ 2ν
θ −θ†2"
I,0.7871524448705657,"2 + 2ν(θ −θ†)T θ†
(138) =
X"
I,0.788111217641419,"i∈[I]
∆
 
QT
i θ, QT
i θ†
+ (θ −θ†)T
 X i∈I"
I,0.7890699904122723," 
σ(QT
i θ†) −1 [Ai = 1]

Qi ! (139)"
I,0.7900287631831256,"+ 2ν
θ −θ†2"
I,0.7909875359539789,"2 + 2ν(θ −θ†)T θ†.
(140)"
I,0.7919463087248322,"By Lemma 29, with high probability, we have
X"
I,0.7929050814956855,"i∈[I]
∆
 
QT
i θ, QT
i θ†
≥AKI min
nθ −θ†
2 ,
θ −θ†2 2"
I,0.7938638542665388,"o
.
(141)"
I,0.7948226270373921,"To control the second term of (139), note that the random vectors Zi ≜
 
σ(QT
i θ†) −1 [Ai = 1]

Qi
are iid with norm at most MQ. Moreover, since E [1 [Ai = 1] |Qi] = σ(QT
i θ†), by the tower rule,
we have E [Zi] = E [E [Zi|Qi]] = 0. Therefore, by applying Hoeffding’s bound to every coordinate
of Zi, and then taking a union bound, for any B > 0, we have P    X"
I,0.7957813998082455,"i∈I
Zi"
I,0.7967401725790988,"2
≥BI3/4 "
I,0.7976989453499521,"≤2d exp

−B2√I"
I,0.7986577181208053,"2dM 2
Q"
I,0.7996164908916586,"
.
(142)"
I,0.800575263662512,"Applying now Cauchy-Schwarz inequality, with high probability, we have
(θ −θ†)T
 X i∈I"
I,0.8015340364333653," 
σ(QT
i θ†) −1 [Ai = 1]

Qi"
I,0.8024928092042186,"! ≤BI3/4 θ −θ†
2 ."
I,0.8034515819750719,"Combining this with (132) and using
θ†2
2 ≤K, we then have"
I,0.8044103547459253,"(θ −θ†)T ∇θL(θ, D)
(143)"
I,0.8053691275167785,"≥(AKI + ν)
nθ −θ†
2 ,
θ −θ†2 2"
I,0.8063279002876318,"o
−(BI(3/4) + 2νK)
θ −θ†
2
(144)"
I,0.8072866730584851,"≥AKI
nθ −θ†
2 ,
θ −θ†2 2"
I,0.8082454458293384,"o
−BKI(3/4) θ −θ†
2 ,
(145)"
I,0.8092042186001918,"where BK = B + 2νK. This shows that Assumption 2 is satisﬁed for logistic loss for α = 3/4, and
AK and BK as previously deﬁned."
I,0.8101629913710451,"E
PROOFS OF LOCAL PAC*-LEARNABILITY"
I,0.8111217641418984,"Let us now prove Lemma 5. To do so, consider the preferred models ⃗θ† and a subset H ⊂[N]
of honest users. Denote ⃗D−H the datasets provided by users n ∈[N] −H. Each honest user
h ∈H provides an honest dataset Dh of cardinality at least I ≥1. Consider the bound KH ≜
maxh∈H
θ†
h

2 on the parameter norm of honest active users h ∈H."
I,0.8120805369127517,"E.1
BOUNDS ON THE OPTIMA"
I,0.8130393096836049,"Before proving the theorem, we prove a useful lemma that bounds the set of possible values for the
global model and honest local models."
I,0.8139980824544583,Under review as a conference paper at ICLR 2022
I,0.8149568552253116,"Lemma 30. Assume that R and ℓare nonnegative. For I large enough, if all honest active nodes
h ∈H provide at least I data, then, with high probability, ⃗θ∗
H must lie in a compact subset of Rd×H
that does not depend on I."
I,0.8159156279961649,"Proof. Denote L0 ≜LOSS(0, (⃗θ†
H, 0−H), (∅, ⃗D−H)). Essentially, we will show that, if ⃗θ∗
H is too
far from ⃗θ†
H, then the loss will take values strictly larger than L0."
I,0.8168744007670182,"Assumption 2 implies the existence of an event E that occurs with probability at least P0 ≜
P(KH, I)|H|, under which, for any θh ∈Rd, we have"
I,0.8178331735378715,"
θh −θ†
h
T
∇Lh (θh) ≥AKHI min
θh −θ†
h

2 ,
θh −θ†
h

2 2"
I,0.8187919463087249,"
−BKHIα θh −θ†
h

2 , (146)"
I,0.8197507190795782,which implies
I,0.8207094918504314,"uT
(θh−θ†
h)∇Lh (θh) ≥AKHI min
n
1,
θh −θ†
h

2"
I,0.8216682646212847,"o
−BKHIα.
(147)"
I,0.822627037392138,"Note also that P0 →1 as I →∞. We now integrate both sides over the line segment from θ†
h to θh.
The fundamental theorem of calculus for line integrals then yields"
I,0.8235858101629914,"Lh (θh) −Lh

θ†
h

=
θh −θ†
h

2 Z 1"
I,0.8245445829338447,"t=0
uT
(θh−θ†
h)∇L

θ†
h + t(θh −θ†
h)

dt
(148)"
I,0.825503355704698,"≥
θh −θ†
h

2 Z 1 t=0"
I,0.8264621284755513,"
AKHI min
n
1, t
θh −θ†
h

2"
I,0.8274209012464045,"o
−BKHIα
dt
(149)"
I,0.8283796740172579,"=
θh −θ†
h

2 Z 1 t=0"
I,0.8293384467881112,"
AKHI min
n
1, t
θh −θ†
h

2"
I,0.8302972195589645,"o
dt −BKHIα θh −θ†
h

2 .
(150)"
I,0.8312559923298178,"Now, if
θh −θ†
h

2 > 2, we then have"
I,0.8322147651006712,"Lh (θh) −Lh

θ†
h

≥
AKHI"
I,0.8331735378715245,"2
−BKHIα
 θh −θ†
h

2
(151)"
I,0.8341323106423778,"≥AKHI −2BKHIα.
(152)"
I,0.835091083413231,"Now for I > I1 ≜max
n
2L0/AKH, (4BKH/AKH)
1
1−α
o
, we have"
I,0.8360498561840843,"Lh (θh) −Lh

θ†
h

> L0.
(153)"
I,0.8370086289549377,"This implies that if
θh −θ†
h

2 > 2 for any h ∈H, then we have"
I,0.837967401725791,"LOSS(0, (⃗θ†
H, 0−H), ⃗D) < LOSS(ρ, (⃗θH, ⃗θ−H), ⃗D),
(154)"
I,0.8389261744966443,"regardless of ρ and θ−H. Therefore, we must have
θ†
h −θ∗
h

2 ≤2. Such inequalities describe a"
I,0.8398849472674976,"bounded closed subset of Rd×H, which is thus compact."
I,0.840843720038351,"Lemma 31. Assume that R(ρ, θ) →∞as ∥ρ −θ∥2 →∞, and that
θ†
h −θ∗
h

2 ≤2 for all honest"
I,0.8418024928092043,users h ∈H. Then ρ∗must lie in a compact subset of Rd that does not depend on I.
I,0.8427612655800575,"Proof. Consider an honest user h′. Given our assumption on R →∞, we know that there exists
DKH such that if ∥ρ −θ∗
h′∥2 ≥DKH, then R(ρ, θ∗
h′) ≥L0 + 1. Thus any global optimum ρ∗must"
I,0.8437200383509108,"satisfy
ρ∗−θ†
h′

2 ≤∥ρ∗−θ∗
h′∥2 +
θ∗
h′ −θ†
h′

2 ≤DKH + 2."
I,0.8446788111217641,Under review as a conference paper at ICLR 2022
I,0.8456375838926175,"E.2
PROOF OF LEMMA 5"
I,0.8465963566634708,"Proof of Lemma 5. Fix ε, δ > 0. We want to show the existence of some value of I(ε, δ, ⃗D−H, ⃗θ†)
that will guarantee (ε, δ)-locally PAC* learning for honest users."
I,0.8475551294343241,"By lemmas 30 and 31, we know that the set C of possible values for (ρ∗, ⃗θ∗
H) is compact. Now, we
deﬁne
EKH ≜max
(ρ,θ)∈C ∥∇θR(ρ, θ)∥2
(155)"
I,0.8485139022051774,"the maximum of the norm of achievable gradients at the optimum. We know this maximum exists
since C is compact."
I,0.8494726749760306,"Using the optimality of (ρ∗, ⃗θ∗), for all h ∈H, we have"
I,0.850431447746884,"0 ∈(θ∗
h −θ†
h)T ∇θhLOSS(ρ∗, ⃗θ∗)
(156)"
I,0.8513902205177373,"= (θ∗
h −θ†
h)T ∇Lh(θ∗
h) + (θ∗
h −θ†
h)T ∇θhR(ρ∗, θ∗
h)
(157)"
I,0.8523489932885906,"≥(θ∗
h −θ†
h)T ∇Lh(θ∗
h) −
θ∗
h −θ†
h

2 ∥∇θhR(ρ∗, θ∗
h)∥2
(158)"
I,0.8533077660594439,"≥(θ∗
h −θ†
h)T ∇Lh(θ∗
h) −EKH
θ∗
h −θ†
h

2 .
(159)"
I,0.8542665388302972,"We now apply assumption 2 for θ = θ∗
h (for h ∈H). Thus, there exists some other event E′ with
probability at least P0, under which, for all h ∈H, we have"
I,0.8552253116011506,"0 ≥AKHI min
θ∗
h −θ†
h

2 ,
θ∗
h −θ†
h

2 2"
I,0.8561840843720039,"
−BKHIα θ∗
h −θ†
h

2 −EKH
θ∗
h −θ†
h

2 . (160)"
I,0.8571428571428571,"Now if I > I2 ≜max
n
2EKH/AKH, (2BKH/AKH)
1
1−α
o
this inequality cannot hold for
θ∗
h −θ†
h

2 ≥1. Therefore, for I > I2, we have
θ∗
h −θ†
h

2 < 1, and thus,"
I,0.8581016299137104,"0 ≥AKHI
θ∗
h −θ†
h

2"
I,0.8590604026845637,"2 −BKHIα θ∗
h −θ†
h

2 −EKH
θ∗
h −θ†
h

2
(161)"
I,0.8600191754554171,"and thus,
θ∗
h −θ†
h

2 ≤BKHIα + EKH"
I,0.8609779482262704,"AKHI
.
(162)"
I,0.8619367209971237,"Now note that P [E ∧E′] = 1 −P [¬E ∨¬E′] ≥1 −P [¬E] −P [¬E′] = 2P0 −1. It now sufﬁces
to consider I larger than I2 and large enough so that P(KH, I)|H| ≥1 −δ/2 (whose existence is
guaranteed by Assumption 2, and which guarantees 2P0 −1 ≥1−δ) and so that
BKH Iα+EKH"
I,0.862895493767977,"AKH I
≤ε
to obtain the theorem."
I,0.8638542665388304,"F
CONVERGENCE OF CGA AGAINST ℓ2
2"
I,0.8648130393096836,"To write our proof, we deﬁne LOSSρ
−s : Rd →R by"
I,0.8657718120805369,"LOSSρ
−s(ρ) ≜inf
⃗θ"
I,0.8667305848513902,"n
LOSS(ρ, ⃗θ, ⃗D) −Ls(θs, Ds) −R(ρ, θs)
o
(163)"
I,0.8676893576222435,"= inf
⃗θ X"
I,0.8686481303930969,"n̸=s
L(θn, Dn) + λ
X"
I,0.8696069031639502,"n̸=s
∥ρ −θn∥2
2 .
(164)"
I,0.8705656759348035,"In other words, it is the loss when local models are optimized, and when the data of strategic user s
are removed.
Lemma 32. Assuming ℓ2
2 regularization and convex loss-per-input functions ℓ, for any datasets ⃗D,
LOSS is strongly convex. As a result, so is LOSSρ
−s."
I,0.8715244487056567,"Proof. Note that the global loss can be written as a sum of convex function, and of ν P ∥θn∥2
2 +
∥ρ −θ1∥2
2. Using tricks similar to the proof of Lemma 11, we see that the loss is strongly convex.
The latter part of the lemma is then a straightforward application of Lemma 10."
I,0.87248322147651,Under review as a conference paper at ICLR 2022
I,0.8734419942473634,"We now move on to the proof of Theorem 4. Note that our statement of the proof was not fully
explicit, especially about the upper bound on the constant learning rate η. Here, we prove that it
holds for ηt = η ≤1/3L, where L is a constant such that LOSSρ
−s is L-smooth. The existence of L
is guaranteed by Lemma 13."
I,0.8744007670182167,"Proof of Theorem 4. Note that by Lemma 9, LOSSρ
−s is convex, differentiable and L-smooth, and
∇LOSSρ
−s(ρt) = g†,t
−s. For ℓ2
2 regularization, we have GRAD(ρ) = Rd for all ρ ∈Rd. Then the"
I,0.87535953978907,"minimum of equation 6 is zero, which is obtained when gt
s ≜ρt−θ†
s
η
−ˆgt
−s = gt−1
s
+ ρt−θ†
s
η
+ ρt−ρt−1"
I,0.8763183125599233,"η
.
Note that"
I,0.8772770853307766,"ρt+1 = ρt −ηg†,t
−s −ηgt
s
(165)"
I,0.87823585810163,"= ρt −ηg†,t
−s −(ρt −θ†
s) + (ρt−1 −ρt) −ηgt−1
s
(166)"
I,0.8791946308724832,"= θ†
s −ηt(g†,t
−s + gt−1
s
) + η(g†,t−1
−s
+ gt−1
s
)
(167)"
I,0.8801534036433365,"= θ†
s −η(g†,t
−s −g†,t−1
−s
).
(168)"
I,0.8811121764141898,"Therefore, ρt+1 −ρt = η(g†,t
−s −g†,t−1
−s
) −η(g†,t−1
−s
−g†,t−2
−s
)."
I,0.8820709491850431,"Then, using the L-smoothness of LOSSρ
−s, and denoting ut ≜
ρt+1 −ρt
2, we have ut+1 ≤
Lηtut + Lηt−1ut−1. Now assume that η ≤1/3L. Then ut+1 ≤1"
I,0.8830297219558965,"3(ut + ut−1). We then know that
ut+2 ≤1"
I,0.8839884947267498,3(ut+1 + ut) ≤1 3( 1
I,0.8849472674976031,3(ut + ut−1) + ut) = 4
I,0.8859060402684564,9ut + 1
I,0.8868648130393096,9ut−1.
I,0.887823585810163,"Now deﬁne vt ≜ut + ut−1.
We then have vt+2 ≤ut+2 + ut+1 ≤
7
9ut + 4"
I,0.8887823585810163,"9ut−1 ≤
7
9(ut + ut−1)
≤
7
9vt.
By induction, we know that vt
≤
(7/9)(t−1)/2 max {v0, v1}
≤
(
√"
I,0.8897411313518696,"7/3)t  
(
√"
I,0.8906999041227229,"7/3) max {v0, v1}

. Thus, deﬁning α ≜
√"
I,0.8916586768935763,"7/3 < 1, there exists C > 0 such that
ut ≤vt ≤Cαt. This implies that P ρt+1 −ρt
2 ≤P Cαt < ∞. Thus P(ρt+1 −ρt) con-
verges, which implies the convergence of ρt to a limit ρ∞. By L-smoothness, we know that g†,t
−s
must converge too. Taking equation 168 to the limit then implies ρ∞= θ†
s. This shows that the
strategic user achieves precisely what they want with CGA. It is thus optimal."
I,0.8926174496644296,"G
CGA ON MNIST"
I,0.8935762224352828,"In this section, CGA is executed against 10 honest users, each one having 6,000 randomly and data
points of MNIST, drawn randomly and independently. CGA is run by a strategic user whose target
model θ†
s labels 0’s as 1’s, 1’s as 2’s, and so on, until 9’s as 0’s. We learn θ†
s by relabeling the
MNIST training dataset and learning from the relabeled data. We use λ = 1, Adam optimizer and a
decreasing learning rate."
I,0.8945349952061361,"0
25
50
75
100
125
150
175
200
Epochs 0 5 10 15 20 25 30 35"
I,0.8954937679769894,l2 norm
I,0.8964525407478428,"l2_dist
l2_norm
target_dist"
I,0.8974113135186961,"(a) Using ℓ2
2"
I,0.8983700862895494,"0
25
50
75
100
125
150
175
200
Epochs 0 5 10 15 20 25 30 35"
I,0.8993288590604027,l2 norm
I,0.900287631831256,"l2_dist
l2_norm
target_dist"
I,0.9012464046021093,(b) Using ℓ2
I,0.9022051773729626,"Figure 16: Norm of global model, distance to initialisation and distance to target, under attack by
CGA. In particular, we see that the attack against ℓ2
2 is successful, as the distance between the global
model and the target model goes to zero."
I,0.9031639501438159,Under review as a conference paper at ICLR 2022
I,0.9041227229146692,"H
SINGLE DATA POISONING FOR LEAST SQUARE LINEAR REGRESSION"
I,0.9050814956855225,Proof of Theorem 5. We deﬁne the minimized loss with respect to ρ and without strategic user s by
I,0.9060402684563759,"LOSS∗
−s(ρ, ⃗D−s) ≜
min
⃗θ−s∈Rd×(N−1) 
  X"
I,0.9069990412272292,"n̸=s
Ln(θn, Dn) +
X"
I,0.9079578139980825,"n̸=s
λ ∥θn −ρ∥2
2 
"
I,0.9089165867689357,".
(169)"
I,0.909875359539789,"Now consider a subgradient g ∈∇ρLOSS∗
−s(θ†
s, ⃗D−s) of the minimized loss at θ†
s. For x ≜−g 2λ ,"
I,0.9108341323106424,"then have −g ∈∇

λ ∥x∥2
2

. We then deﬁne θ♠
s ≜θ†
s −x."
I,0.9117929050814957,"0 = g −g ∈∇ρLOSS∗
−s(θ†
s, ⃗D−s) + ∇ρ

λ
θ♠
s −θ†
s
2 2"
I,0.912751677852349,"
(170)"
I,0.9137104506232023,"= ∇ρLOSSs(θ†
s, ⃗θ∗
−s(θ♠
s , ⃗D−s), θ♠
s , ⃗D−s),
(171)"
I,0.9146692233940557,"where LOSSs is deﬁned by (38). Now consider the data point (Q, A) = (g, gT θ♠
s −1). For
Ds = {(Q, A)}, we then have ∇Ls(θ♠
s , Ds) = g, which implies"
I,0.9156279961649089,"∇θsLOSS(θ†
s, (θ♠
s , ⃗θ∗
−s(θ♠
s , ⃗D−s), ⃗D) = 0.
(172)"
I,0.9165867689357622,Combining it all together with the uniqueness of the solution then yields
I,0.9175455417066155,"arg min
(ρ,⃗θ)"
I,0.9185043144774688,"n
LOSS(ρ, ⃗θ, ⃗D)
o
=

θ†
s,

θ♠
s , ⃗θ∗
−s(θ♠
s , ⃗D−s)

,
(173)"
I,0.9194630872483222,which is what we wanted.
I,0.9204218600191755,"I
DATA POISONING AGAINST LINEAR CLASSIFICATION"
I,0.9213806327900288,"I.1
GENERATING EFFICIENT POISONING DATA"
I,0.9223394055608821,"For every label a ∈{1, . . . , 9}, we deﬁne ya ≜θ♠
a −θ♠
0 , and ca ≜−(θ♠
a0 −θ♠
00) (where θ♠
a0 is the
bias of the linear classiﬁer). The indifference subspace V is then the set of images Q ∈Rd such
that QT ya = ca for all a ∈{1, . . . , 9}."
I,0.9232981783317353,"To project any image X ∈Rd on V , let us ﬁrst construct an orthogonal basis of the vector space
orthogonal to V , using the Gram-Schmidt algorithm. Namely, we ﬁrst deﬁne z1 ≜y1. Then, for
any answer a ∈{1, . . . , 9}, we deﬁne"
I,0.9242569511025887,"za ≜ya −
X"
I,0.925215723873442,"b<a
yT
a zb
zb
∥zb∥2
2
.
(174)"
I,0.9261744966442953,"It is easy to check that for b < a, we have zT
a zb = 0. Moreover, if Q ∈V , then"
I,0.9271332694151486,"zT
a Q = yT
a Q −
X b<a"
I,0.9280920421860019,"(yT
a zb)(zT
b Q)"
I,0.9290508149568553,"∥zb∥2
2
= ca −
X b<a"
I,0.9300095877277086,"(yT
a zb)(zT
b Q)"
I,0.9309683604985618,"∥zb∥2
2
.
(175)"
I,0.9319271332694151,"By induction, we see that zT
a Q is a constant independent from Q. Indeed, for a = 1, this is clear
as zT
1 Q = yT
1 Q = c1. Moreover, for a > 1, then, in the computation of zT
a Q, Q always appear as
zT
b Q for b < a. Moreover, denoting c′
a the constant such that zT
a Q = c′
a for all a ∈{1, . . . 9}, we
see that these constants can be computed by"
I,0.9328859060402684,"c′
a = ca −
X b<a"
I,0.9338446788111218,"yT
a zb
∥zb∥2
2
c′
b.
(176)"
I,0.9348034515819751,"Finally, we can simply perform repeated projection onto the hyperplanes where a is equally probable
as the answer 0. To do this, we ﬁrst deﬁne the orthogonal projection P(X, y, c) of X ∈Rd on the
hyperplane xT y = c, which is given by"
I,0.9357622243528284,"P(X, y, c) = X −(XT y −c)
y"
I,0.9367209971236817,"∥y∥2
2
.
(177)"
I,0.9376797698945349,Under review as a conference paper at ICLR 2022
I,0.9386385426653883,"It is straightforward to verify that P(X, y, c)T y = c and that P(P(X, y, c), y, c) = P(X, y, c). We
then canonically deﬁne repeated projection by induction, as"
I,0.9395973154362416,"P(X, (y1, . . . , yk+1), (c1, . . . , ck+1)) ≜P(P(X, (y1, . . . , yk), (c1, . . . , ck)), yk+1, ck+1). (178)"
I,0.9405560882070949,Now consider any image X ∈Rd. Its projection can be obtained by setting
I,0.9415148609779482,"Q ≜P(X, (z1, . . . , z9), (c′
1, . . . c′
9)) + ξ.
(179)"
I,0.9424736337488016,"Note that to avoid being exactly on the boundary, and thus retrieve information about the scales of
θ♠and on which side of the boundary favors which label, we add a small noise ξ, to make sure Q
does not lie exactly on V (which would lead to multiple solutions for the learning), but small enough
so that the probabilities of the different label remain close to 0.1 (the equiprobable probability)."
I,0.9434324065196549,"We acknowledge that images obtained this way may not be in [0, 1]d, like the images of the MNIST
dataset. In general, one could search for points Q ∈V ∩[0, 1]d. Note that in theory, by Theorem 3
(or a generalization of it), labeling random images in [0, 1]d should sufﬁce. However, in the case
where V ∩[0, 1]d is empty (typically if no image in [0, 1] is argued by model θ♠
s as realistically a 9),
this procedure may require the labeling of signiﬁcantly more images to be successful."
I,0.9443911792905082,"I.2
A BRIEF THEORY OF DATA POISONING FOR LINEAR CLASSIFICATION"
I,0.9453499520613614,"Using the efﬁcient poisoning data fabrication, we thus have a set of images (Q, p(Q)), where pa(Q)
is the probability assigned to image Q and label a. This deﬁnes the following local loss for the
strategic node:
Ls(θs, Ds) =
X"
I,0.9463087248322147,"(Q,p(Q))∈Ds X"
I,0.947267497603068,"a∈{0,1,...,9}
pa(Q) ln σa(θs, Q),
(180)"
I,0.9482262703739214,"where σa(θs, Q) =
exp(θT
saQ+θsa0)
P exp(θT
sbQ+θsb0) is the probability that image Q has label a, according to the
model θs. We acknowledge that such labelings of queries is unusual. Evidently, in practice, an image
may be labeled N times, and the number of labels Na it received can be set to be approximately
Na ≈Npa(Q)."
I,0.9491850431447747,"It is noteworthy that the gradient of the loss function is then given by
 
θs −θ♠
s
T ∇θsLs(θs, Ds) =
X Q∈Ds X"
I,0.950143815915628,"a∈{0,1,...,9}"
I,0.9511025886864813," 
σa(θs, Q) −σa(θ♠
s , Q)
  
θsa −θ♠
sa
T Q+,"
I,0.9520613614573347,"(181)
where we deﬁned Q+ ≜(1, Q) (which allows to factor in the bias of the model. This shows
that ∇θsLs(θs, Ds) points systematically away from θ♠
s , and thus that gradient descent will move
towards θ♠
s ."
I,0.9530201342281879,"In fact, if the set of images Q cover all dimensions (which occurs if there are Ω(d) images, which is
the case for 2,000 images, since d = 784), then gradient descent will always move the model in the
direction of θ♠
s , which will be the minimum. Moreover, by overweighting each data (Q, p(Q)) by
a factor α (as though the image Q was labeled α times), we can guarantee gradient-PAC* learning,
which means that we will have θ∗
s ≈θ♠
s , even in the personalized federated learning framework.
This shows why data poisoning should work in theory, with relatively few data injections."
I,0.9539789069990412,"Note that the number of other users does make learning harder. Indeed, the gradient of the regu-
larization R(ρ, θs) at ρ = θ†
s and θs = θ♠
s is equal to 2λ
θ†
s −θ♠
s

2. As the number N −1 of
other users grows, we should expect this distance to grow roughly proportionally to N. In order to
make strategic user s robustly learn θ♠
s , the norm of the gradient of the local loss Ls at θ†
s must be
vastly larger than 2λ
θ†
s −θ♠
s

2. This means that the value of α (or, equivalently, the number of
data injected in Ds) must also grow proportionally to N."
I,0.9549376797698945,"I.3
INITIALIZATION OF THE LEARNING ALGORITHM"
I,0.9558964525407478,"The convergence to the optimum is slow. But given that the problem is convex, we focus here
mostly on showing that the minimum is indeed a poisoned model. To boost the convergence, we
initialize our learning algorithm at a point close to what we expect to be the minimum, by taking
this minimum and adding a Gaussian noise, and then we observe the convergence to this minimum."
I,0.9568552253116012,Under review as a conference paper at ICLR 2022
I,0.9578139980824545,"J
CIFAR-10 ON VGG 13-BN EXPERIMENTS"
I,0.9587727708533078,"We considered VGG 13-BN, which was pretrained on cifar-10 by Phan (2021). We now assume that
10 nodes are given part of the cifar-10 database, while a strategic node also joins to the personalized
federated gradient descent algorithm. The strategic node’s goal is to bias the global model towards
a target model, which misclassiﬁes the cifar-10 data, by reclassifying 0 into 1, 1 into 2... and 9 into
0."
I,0.959731543624161,"J.1
COUNTER-GRADIENT ATTACK"
I,0.9606903163950143,"We ﬁrst show the result of performing counter-gradient attack on the last layer of the neural network.
Essentially, images are now reduced to their vector embedding, and the last layer performs a simple
linear classiﬁcation akin to the case of MNIST (see Appendix G)."
I,0.9616490891658677,"0
200
400
600
800
1000
Epochs 0.0 0.2 0.4 0.6 0.8 1.0"
I,0.962607861936721,Accuracy
I,0.9635666347075743,Gradient attack
I,0.9645254074784276,acc_glob
I,0.965484180249281,(a) Accuracy according to attacker’s objective
I,0.9664429530201343,"0
200
400
600
800
1000
Epochs 0 5 10 15 20 25 30 35 40"
I,0.9674017257909875,l2 norm
I,0.9683604985618408,Gradient attack
I,0.9693192713326941,"honest_dist
l2_norm
target_dist
attack_dist"
I,0.9702780441035475,(b) Distances
I,0.9712368168744008,Figure 17: CGA on cifar-10.
I,0.9721955896452541,"J.2
RECONSTRUCTING A MODEL ATTACK"
I,0.9731543624161074,"Reconstructing an attack model whose effect is equivalent to the counter-gradient attack is identical
to what was done in the case of MNIST (see Section 5.2)."
I,0.9741131351869607,"0
100
200
300
400
500
Epochs 0.0 0.2 0.4 0.6 0.8 1.0"
I,0.975071907957814,Accuracy
I,0.9760306807286673,Model attack
I,0.9769894534995206,acc_glob
I,0.9779482262703739,(a) Accuracy according to attacker’s objective
I,0.9789069990412272,"0
100
200
300
400
500
Epochs 0 5 10 15 20 25 30 35 40"
I,0.9798657718120806,l2 norm
I,0.9808245445829339,Model attack
I,0.9817833173537871,"honest_dist
l2_norm
target_dist
attack_dist"
I,0.9827420901246404,(b) Distances
I,0.9837008628954937,Figure 18: Model attack on cifar-10.
I,0.9846596356663471,"J.3
RECONSTRUCTING DATA POISONING"
I,0.9856184084372004,"This last step is however nontrivial. On one hand, we could simply use the attack model to label a
large number of random images. However, this solution would likely require a large sample com-"
I,0.9865771812080537,Under review as a conference paper at ICLR 2022
I,0.987535953978907,"plexity. For a more efﬁcient data poisoning, we can construct vector embeddings on the indifference
afﬁne subspace V , as was done for MNIST in Section 5.3. This is what is shown below."
I,0.9884947267497604,"0
200
400
600
800
1000
1200
1400
Epochs 0.0 0.2 0.4 0.6 0.8 1.0"
I,0.9894534995206136,Accuracy
I,0.9904122722914669,Data attack
I,0.9913710450623202,acc_glob
I,0.9923298178331735,(a) Accuracy according to attacker’s objective
I,0.9932885906040269,"0
200
400
600
800
1000
1200
1400
Epochs 0 5 10 15 20 25 30 35 40"
I,0.9942473633748802,l2 norm
I,0.9952061361457335,Data attack
I,0.9961649089165868,"honest_dist
l2_norm
target_dist
attack_dist"
I,0.99712368168744,(b) Distances
I,0.9980824544582934,Figure 19: Data poisoning on cifar-10.
I,0.9990412272291467,"We acknowledge however that this does not quite correspond to data poisoning, as it requires re-
porting a vector embedding and its label, rather than an actual image and its label. The challenge
is then to reconstruct an image that has a given vector embedding. We note that, while this is not a
straightforward task in general, this has been shown to be at least somewhat possible for some neural
networks, especially when they are designed to be interpretable (Zeiler & Fergus, 2014; Wang et al.,
2019c; Mai et al., 2019)."
