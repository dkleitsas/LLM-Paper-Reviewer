Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.004761904761904762,"Stochastic gradient descent (SGD) with momentum is widely used for training
modern deep learning architectures. While it is well understood that using momen-
tum can lead to faster convergence rate in various settings, it has also been observed
that momentum yields higher generalization. Prior work argue that momentum
stabilizes the SGD noise during training and this leads to higher generalization. In
this paper, we take the opposite view to this result and ﬁrst empirically show that
gradient descent with momentum (GD+M) signiﬁcantly improves generalization
comparing to gradient descent (GD) in many deep learning tasks. From this obser-
vation, we formally study how momentum improves generalization in deep learning.
We devise a binary classiﬁcation setting where a two-layer (over-parameterized)
convolutional neural network trained with GD+M provably generalizes better than
the same network trained with vanilla GD, when both algorithms start from the
same random initialization. The key insight in our analysis is that momentum is
beneﬁcial in datasets where the examples share some features but differ in their
margin. Contrary to the GD model that memorizes the small margin data, GD+M
can still learn the features in these data thanks to its historical gradients. We also
empirically verify this learning process of momentum in real-world settings."
INTRODUCTION,0.009523809523809525,"1
INTRODUCTION"
INTRODUCTION,0.014285714285714285,"It is commonly accepted that adding momentum to an optimization algorithm is required to optimally
train a large-scale deep network. Most of the modern architectures maintain during the training
process a heavy momentum close to 1 (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014;
He et al., 2016; Zagoruyko & Komodakis, 2016). Indeed, it has been empirically observed that
architectures trained with momentum outperform those which are trained without (Sutskever et al.,
2013). Several papers have attempted to explain this phenomenon. From the optimization perspective,
Defazio (2020) assert that momentum yields faster convergence of the training loss since, at the
early stages, it cancels out the noise from the stochastic gradients. On the other hand, Leclerc &
Madry (2020) empirically observes that momentum yields faster training convergence only when
the learning rate is small. While these works shed light on how momentum acts on neural network
training, they fail to capture the generalization improvement induced by momentum (Sutskever et al.,
2013). Besides, the noise reduction property of momentum advocated by Defazio (2020) seems to
even contradict the observation that, in deep learning, having a large noise in the training improves
generalization (Li et al., 2019; HaoChen et al., 2020). To the best of our knowledge, there is no
existing work which theoretically explains how momentum improves generalization in deep learning.
Therefore, this paper aims to close this gap and addresses the following question:"
INTRODUCTION,0.01904761904761905,"Is the higher generalization induced by momentum tied to the stochastic noise of the gradient? If not,
what is the underlying mechanism of momentum improving generalization in deep learning?"
INTRODUCTION,0.023809523809523808,"In this paper, we empirically verify that the generalization improvement induced by momentum is
not tied to the stochasticity of the gradient. Indeed, as reported in Figure 1, momentum improves
generalization more signiﬁcantly for full batch GD than for SGD in CIFAR object recognition tasks.
Motivated by this empirical observation and the fact that the stochastic noise inﬂuences generalization,
we theoretically study how gradient descent with momentum (GD+M) can generalize better than
vanilla gradient descent (GD). We therefore only focus on the contribution of momentum of the true
gradient on generalization."
INTRODUCTION,0.02857142857142857,"The question we address concerns algorithmic regularization which characterizes the generalization
of an optimization algorithm when multiple global solutions exist in over-parameterized deep learning"
INTRODUCTION,0.03333333333333333,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.0380952380952381,"CIFAR-10
CIFAR-100
Test
Ratio
Test
Ratio
R18
75.83/84.68
1.11
43.32/51.99
1.20
WR16
75.02/84.48
1.12
42.95/51.33
1.20
(a)"
INTRODUCTION,0.04285714285714286,"CIFAR-10
CIFAR-100
Test
Ratio
Test
Ratio
R18
86.15/85.91
0.99
53.81/58.01
1.08
WR16
84.83/87.85
1.04
55.09/60.83
1.10
(b)"
INTRODUCTION,0.047619047619047616,"0
50
100
150
200
250
300"
INTRODUCTION,0.05238095238095238,Number of epochs 0.0 0.5 1.0 1.5 2.0
INTRODUCTION,0.05714285714285714,Training loss
INTRODUCTION,0.06190476190476191,"SGD (0.00)
SGD+M (0.00)
GD (0.00)
GD+M (0.01) (c)"
INTRODUCTION,0.06666666666666667,"0
50
100
150
200
250
300"
INTRODUCTION,0.07142857142857142,Number of epochs 20 40 60 80
INTRODUCTION,0.0761904761904762,Test Accuracy
INTRODUCTION,0.08095238095238096,"SGD (86.15)
SGD+M (85.91)
GD (75.83)
GD+M (84.68) (d)"
INTRODUCTION,0.08571428571428572,"Figure 1: Test accuracy obtained with Resnet-18 (R18) and WideResnet16 (WR16) on CIFAR-10 and CIFAR-
100. The architectures are trained using GD/GD+M (a) and SGD/ SGD+M (b) for 300 epochs to ensure zero
training error. (c)-(d) respectively display the training loss and test accuracy by R18 with GD/GD+M on
CIFAR-10. To isolate the effect of momentum, we turn off data augmentation, dropout and batch normalization.
GD and SGD respectively refer to stochastic gradient descent with batch sizes 50k (full batch) and 128. We grid
searched the best (scheduled) learning rate and weight decay for each individual algorithm separately. Results
are averaged over 3 runs and we only report the mean (see Appendix for complete table)."
INTRODUCTION,0.09047619047619047,"model Soudry et al. (2018); Lyu & Li (2019); Ji & Telgarsky (2019); Chizat & Bach (2020); Gunasekar
et al. (2018); Arora et al. (2019). This regularization arises in deep learning mainly due to the non-
convexity of the objective function. Indeed, this latter can create multiple global minima scattered
in the space that vastly differ in terms of generalization. Algorithmic regularization is induced by
and depends on many factors such as learning rate and batch size (Goyal et al., 2017; Hoffer et al.,
2017; Keskar et al., 2016; Smith et al., 2018), initialization Allen-Zhu & Li (2020), adaptive step-size
(Kingma & Ba, 2014; Neyshabur et al., 2015; Wilson et al., 2017), batch normalization (Arora et al.,
2018; Hoffer et al., 2019; Ioffe & Szegedy, 2015) and dropout (Srivastava et al., 2014; Wei et al.,
2020). However, none of these works theoretically analyzes the regularization induced by momentum.
We therefore start our investigation by raising the following question:"
INTRODUCTION,0.09523809523809523,Does momentum unconditionally improve generalization in deep learning?
INTRODUCTION,0.1,"This question could be positively answered given the success of momentum for learning distinct
architectures such as ResNets (He et al., 2016) or BERT (Devlin et al., 2018). However, we here
empirically give a negative answer through the following synthetic example in deep learning. We
consider a binary classiﬁcation problem where data-points are generated from a standard normal
distribution and labels are outputs of teacher networks. Starting from the same initialization, we
train different over-parametrized student networks using GD and GD+M. Based on Table 1, whether
the target function is simple (linear) or complex (neural network), momentum does not improve
generalization even when using a non-linear neural network as learner. The same observation holds
for SGD/SGD+M as shown in the Appendix. Therefore, momentum does not always lead to a higher
generalization in deep learning. Instead, such beneﬁt seems to heavily depend on both the structure
of the data and the learning problem.
On which data set does momentum help generalization?
In this paper, in order to deter-
mine the underlying mechanism produced by momentum to improve generalization, we de-
sign a binary classiﬁcation problem with a simple data structure where training a two-layer (over-
parameterized) convolutional network with momentum provably improves generalization in deep
learning. It is built upon a data distribution that relies on the concepts of feature and margin. Infor-
mally, each example in this distribution is a 1D image having P patches. One of the patches (the
signal patch) contains a feature we want to learn and all the others are Gaussian random noise with
small variance."
INTRODUCTION,0.10476190476190476,"Mathematically, one can think of a feature as a vector w∗∈Rd. We assume that our training examples
are divided into large margin data where the signal is αw∗with α constant and small margin data
where the signal is βw∗with β ≪1. Intuitively, the second type of data is inherently noisier as the
margin is small and therefore, a classiﬁer would struggle more to generalize on this type of data. We"
INTRODUCTION,0.10952380952380952,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.11428571428571428,"Figure 2: Dataset equation (D) 2D. Each data-point is Xi = [ci · w∗, di · n] ∈R4 for some ci, di ∈R. We
project these points in the 2D space (span(w∗), span(n)). The feature is w∗and the noisy patch is in span(n).
The large margin data (squares) have large component along w∗and relatively small noise component and are
thus roughly equal to αw∗. The small margin data (circles) have relatively large noise component and thus,
these data are well-spread on the span of n."
INTRODUCTION,0.11904761904761904,"XXXXXXXXX
Student
Teacher
Linear
1-MLP
2-MLP
1-CNN
2-CNN"
-MLP,0.12380952380952381,"1-MLP
93.48/93.25
92.32/92.18
84.3/83.68
94.18/94.12
76.04/76.12
2-MLP
93.45/92.85
91.02/91.78
83.82/83.25
94.14/94.20
75.50/75.56
1-CNN
92.21/92.34
92.31/92.33
83.39/83.44
94.39/94.39
79.44/78.32
2-CNN
91.04/91.22
91.51/91.56
82.44/82.12
93.91/93.79
80.86/78.56"
-MLP,0.12857142857142856,"Table 1: Test accuracy obtained using GD/GD+M on a Gaussian synthetic dataset trained using neural network
with ReLU activations. The training dataset consists in 500 data points in dimension 30 and test set in 5000
points. The student networks are trained for 1000 epochs to ensure zero training error. The results are averaged
over 3 runs and we only report the mean (see Appendix for complete table).
underline that all the examples share the same feature but differ in the intensity of the signal. We
consider a training dataset of size N with the following split for ˆµ ≪1 :
(1 −ˆµ)N datapoints are with large margin,
ˆµN datapoints are with small margin data.
(D)"
-MLP,0.13333333333333333,"Figure 2 sketches equation (D) in a 2D setting. We emphasize that datasets having similar features
and different margins are common in the real-world. Examples include object-recognition datasets
such as CIFAR (Krizhevsky et al., 2009) or Imagenet (Deng et al., 2009) (for example, the “wheel
feature” of a car can be strong or weak depending on the orientation of the car). More speciﬁcally, we
believe that the dataset (D) can be viewed as a simpliﬁed model of these object-recognition datasets.
In this context, the following informal theorems characterize the generalization of the GD and GD+M
models. They dramatically simplify Theorem 3.1 and Theorem 3.2 but highlight the intuitions behind
our results.
Theorem 1.1 (Informal, GD+M). There exists a dataset of the form (D) with size N such that a
two-layer (over-parameterized) convolutional network trained with GD+M:"
-MLP,0.1380952380952381,1. initially only learns large margin data from the (1 −ˆµ)N examples.
-MLP,0.14285714285714285,2. has large historical gradients that contain the feature w∗present in small margin data.
-MLP,0.14761904761904762,"3. keeps learning the feature in the small margin data using its momentum historical gradients.
The model thus reaches zero training error and perfectly classify large and small margin data at test.
Theorem 1.2 (Informal, GD). There exists a dataset of the form (D) with size N such that a two-layer
(over-parameterized) convolutional network trained with GD:"
-MLP,0.1523809523809524,1. initially only learns large margin data from the from the (1 −ˆµ)N examples.
-MLP,0.15714285714285714,2. has small gradient after learning these data.
-MLP,0.1619047619047619,3. memorizes the remaining small margin data from the ˆµN examples using the noises.
-MLP,0.16666666666666666,"The model thus reaches zero training and manages to classify the large margin data at test. However,
it fails to classify the small margin data because of the memorization step during training.
Why does GD+M generalize better than GD?
Since the large margin data are dominant, GD
focus in priority on these examples to decrease its training loss. However, after ﬁtting this data, it
signiﬁcantly lowers its gradient. The gradient is thus not large enough for learning the small margin
data. Similarly, GD+M ﬁts the large margin data and subsequently gets a small gradient. However,"
-MLP,0.17142857142857143,Under review as a conference paper at ICLR 2022
-MLP,0.1761904761904762,"(a)
(b)
(c)
Figure 3: (a): Training loss (b) test accuracy on large margin data and (c) test accuracy on the small margin data
in the synthetic setting. While GD and GD+M get zero training loss, GD+M generalizes better on small margin
data than GD. Setting: 20000 training data, 2000 test data, d=30, number of neurons=5, number of patches=5."
-MLP,0.18095238095238095,"0
100
200
300"
-MLP,0.18571428571428572,Number of epochs 40 60 80 100
-MLP,0.19047619047619047,Training Accuracy
-MLP,0.19523809523809524,"GD (100.00)
GD+M (100.00) (a)"
-MLP,0.2,"0
50
100
150
200
250
300"
-MLP,0.20476190476190476,Number of epochs 20 40 60 80
-MLP,0.20952380952380953,Test Accuracy
-MLP,0.21428571428571427,"GD (73.69)
GD+M (83.82)
GD (SM) (65.14)
GD+M (SM) (80.36) (b)"
-MLP,0.21904761904761905,"Figure 4: Training (a) and test (b) accuracy obtained with Resnet-18 on CIFAR-10 dataset with artiﬁcially
generated small margin data. The architectures are trained using GD/GD+M for 300 epochs to ensure zero
training error. Data augmentation, dropout and batch normalization are turned off. (SM) stands for the test
accuracy obtained by the algorithm on the small margin data. Results are averaged over 5 runs with best
scheduled learning rate and weight decay for each individual algorithm separately."
-MLP,0.22380952380952382,"contrary to GD, GD+M has large historical gradients in his momentum gradient. These gradients
helped to learn the feature in the large margin data. They also help to learn small margin data since all
the examples share the same feature. GD+M therefore uses his momentum to learn the small margin
data. We name this process historical feature ampliﬁcation and believe that it is key to understand
why momentum improves generalization.
Empirical justiﬁcation.
We also provide an empirical justiﬁcation that such phenomenon does
happen in a real-world setting as reported in Figure 4. In this experiment, we create small margin
data in the CIFAR-10 dataset by respectively lowering the resolution of 10% of the training and
test images, adding Gaussian noise of variance 0.005 and randomly shufﬂing the RGB channels.
Figure 4 shows that even though both algortihms reach zero training error and 100% training accuracy,
GD+M gets higher generalization than GD on this decimated dataset. Above all, at test, GD+M
performs as well on small and large margin data while GD does relatively worse on small margin
data.Indeed, the relative accuracy drop for GD+M is 80.36/83.32 = 0.97 while for GD is equal to
65.14/73.69 = 0.88."
-MLP,0.22857142857142856,"Our paper is organized as follows. In Section 2, we formally deﬁne the data distribution equation (D),
the model and algorithms we use to learn it. Lastly, Section 3 presents our main theorems and provide
a proof sketch in Section 4 and Section 5. Additional experiments can be found in the Appendix."
-MLP,0.23333333333333334,"MORE RELATED WORK
Momentum in convex setting.
GD+M (a.k.a. heavy ball or Polyak momentum) consists in calcu-
lating the exponentially weighted average of the past gradients and using it to update the weights.
For convex functions near a strict twice-differentiable minimum, GD+M is optimal regarding local
convergence rate Polyak (1963; 1964); Nemirovskij & Yudin (1983); Nesterov (2003). However, it
may fail to converge globally for general strongly convex twice-differentiable functions Lessard et al.
(2015) and is no longer optimal for the class of smooth convex functions. In the stochastic setting,
GD+M is more sensitive to noise in the gradients; that is, to preserve their improved convergence
rates, signiﬁcantly less noise is required d’Aspremont (2008); Schmidt et al. (2011); Devolder et al.
(2014); Kidambi et al. (2018). Finally, other momentum methods are extensively used for convex
functions such as Nesterov’s accelerated gradient Nesterov (1983). Our paper focuses on the use of
GD+M and contrary to the aforementioned papers, our setting is non-convex and we mainly focus on"
-MLP,0.23809523809523808,Under review as a conference paper at ICLR 2022
-MLP,0.24285714285714285,"the generalization of the model learned by GD and GD+M when both methods converge to global
optimal. We underline that contrary to the non-convex world, generalization is typically disentangled
with optimization for (strictly) convex functions.
Non-convex optimization with momentum.
A long line of work consists in understanding the
convergence speed of momentum methods when optimizing non-convex functions. Mai & Johansson
(2020); Liu et al. (2020); Cutkosky & Mehta (2020); Defazio (2020) show that SGD+M reaches
a stationary point as fast as SGD under diverse assumptions. Besides, Leclerc & Madry (2020)
empirically shows that momentum accelerates neural network training for small learning rates and
slows it down otherwise. Our paper differs from these works as we work in the batch setting and
theoretically investigate the generalization beneﬁts brought by momentum (and not the training ones).
Generalization with momentum.
Momentum-based methods such as SGD+M, RMSProp (Tiele-
man & Hinton, 2012) and Adam (Kingma & Ba, 2014) are standard in deep learning training since
the seminal work of Sutskever et al. (2013). Although its well accepted that Momentum improve
generalization in deep learning, only a few works formally investigate the role of momentum in
generalization. Leclerc & Madry (2020) empirically reports that momentum yields higher general-
ization when using a large learning rate. However, they assert that this beneﬁt can be obtained by
applying an even larger learning rate on vanilla SGD. We suspect that this observation is due to batch
normalization (BN) which is known to dramatically bias the algorithm’s generalization (Lyu & Li,
2019). In Appendix, we report that BN reduces the generalization gain of momentum comparing to
without BN. To our knowledge, our work is ﬁrst that theoretically investigate the generalization of
momentum in deep learning.
2
SETTING AND ALGORITHMS
In this section, we ﬁrst introduce a formal deﬁnition of the data distribution equation (D) and the
neural network model we use to learn it. We ﬁnally present the GD and GD+M algorithms.
General notations.
For a matrix W ∈Rm×d, we denote by wr its r-th row. For a function
f : Rm×d →R, we denote by ∇wrf(W) the gradient of f with respect to wr and ∇f(W) the
gradient with respect to W. For an optimization algorithm updating a vector w, w(t) represents its
iterate at time t. We use Id for the d × d identity matrix and 1m the all-ones vector of dimension
m. Finally, we use the asymptotic complexity notations when deﬁning the different constants in the
paper. We use ˜O, ˜Θ, ˜Ωto hide logarithmic dependency on d.
Data distribution.
We deﬁne our data distribution D as follows."
-MLP,0.24761904761904763,Each sample from D consists in an input data X and a label y that are generated as:
-MLP,0.2523809523809524,"1. The label y is uniformly sampled from {−1, 1}."
-MLP,0.2571428571428571,"2. Each data-point X = (X[1], . . . , X[P]) consists in P patches where each X[j] ∈Rd."
-MLP,0.2619047619047619,"3. Signal patch: for one patch P(X) ∈[P], we have X[P(X)] = cw∗, where c ∈R,
w∗∈Rd and ∥w∗∥2 = 1.
(D)"
-MLP,0.26666666666666666,4. The distribution of c satisﬁes that
-MLP,0.2714285714285714,"c =
αy
with probability 1 −µ
βy
with probability µ
."
-MLP,0.2761904761904762,"5. Noisy patches: for all the other patches j ∈[P]\{P(X)}, X[j] ∼N(0, (I−w∗w∗⊤)σ2Id)."
-MLP,0.28095238095238095,"We precise that we sample the noisy patches in the orthogonal complement of w∗to have a simpler
analysis. To present the simplest result, we assume that the values in equation (D) satisfy α = d0.49,
β =
1
polylog(d)
√"
-MLP,0.2857142857142857,"dα, σ =
1
√"
-MLP,0.2904761904761905,"d and P ∈[2, polylog(d)]."
-MLP,0.29523809523809524,"Using this model, we generate a training dataset Z = {(Xi, yi)}i∈[N] where Xi = (Xi[j])j∈[P ]. We"
-MLP,0.3,"focus on the case where µ = 1/poly(d) and N = Θ

1
µ

. We let Z to be partitioned in two sets Z1
and Z2 such that Z1 gathers the large margin data while Z2 the small margin ones. Lastly, we deﬁne
ˆµ = |Z2|"
-MLP,0.3047619047619048,"N
the fraction of small margin data.
Learner model.
We use a two-layer convolutional neural network with cubic activation to learn
the training dataset Z. This model is the simplest non-linear network since a quadratic activation"
-MLP,0.30952380952380953,Under review as a conference paper at ICLR 2022
-MLP,0.3142857142857143,"would only output positive labels and mismatch our labeling function. The ﬁrst layer weights are
W ∈Rm×d and the second layer is ﬁxed to 1m. Given a input data X, the output of the model is"
-MLP,0.319047619047619,"fW (X) = m
X r=1 P
X"
-MLP,0.3238095238095238,"j=1
⟨wr, X[j]⟩3.
(CNN)"
-MLP,0.32857142857142857,"The number of neurons is set as m = polylog(d) to ensure that (CNN) is mildly over-parametrized.
Training objective.
We ﬁt the training dataset Z using (CNN) and solve the logistic regression
problem
min
W ∈Rm×d
1
N N
X"
-MLP,0.3333333333333333,"i=1
log (1 + exp (−yifW (Xi))) + λ"
-MLP,0.3380952380952381,"2 ∥W∥2
2 := bL(W).
(P)"
-MLP,0.34285714285714286,"(P) sheds light on our choice of cubic activation in (CNN). Indeed, it is the smallest polynomial
degree that makes the training objective (P) non-convex and compatible with our dataset. Linear or
quadratic activations would respectively make the problem convex or all the labels positive. Here, we
pick λ ∈
h
0,
1
poly(d)N
i
."
-MLP,0.3476190476190476,"Importance of non-convexity.
When λ > 0, if the loss
1
N
PN
i=1 log (1 + exp (−yifW (Xi))) is
convex, then there is a unique global optimal solution, so the choice of optimization algorithm does
not matter. In our case, due to the non-convexity of the training objective, GD + M converges to a
different (approximate) global optimal comparing to GD, with better generalization properties."
-MLP,0.3523809523809524,"Test error.
We assess the quality of a predictor c
W using the classical 0-1 loss used in bi-
nary classiﬁcation. Given a sample (X, y), the individual test (classiﬁcation) error is deﬁned
as L (X, y) = 1{fc
W (X)y < 0}. While L measures the error of fW on an individual data-point,
we are interested in the test error that measures the average loss over data points generated from (D)
and deﬁned as
L (fc
W ) := E(X,y)∼D[L (fc
W (X), y)].
(TE)"
-MLP,0.35714285714285715,Algorithms. We solve the training problem equation (P) using GD and GD+M. GD is deﬁned by
-MLP,0.3619047619047619,"W (t+1) = W (t) −η∇bL(W (t)), for t ≥0,
(GD)"
-MLP,0.36666666666666664,"where η > 0 is the learning rate. On the other hand, GD+M is deﬁned by the update rule
(
g(t+1)
= γg(t) + (1 −γ)∇bL(W (t))
W (t+1)
= W (t) −ηg(t+1)
, for t ≥0.
(GD+M)"
-MLP,0.37142857142857144,"where γ ∈(0, 1) is momentum factor. We now detail how to set parameters in (GD) and (GD+M).
Parametrization 2.1. When running GD and GD+M on equation (P), the number of iterations is
T ∈

poly(d)N/(η), dO(log d)/(η)

. For both algorithms, the weights w(0)
1 , . . . , w(0)
m are initialized
using independent samples from a normal distribution N(0, σ2
0Id) where σ2
0 =
polylog(d)"
-MLP,0.3761904761904762,"d
. The
learning rate is set as:"
-MLP,0.38095238095238093,"1. GD: the learning rate may take any reasonable value η ∈(0, ˜O(1)]."
-MLP,0.38571428571428573,2. GD+M: the learning rate is a large learning rate: η = ˜Θ(1).1
-MLP,0.3904761904761905,"Lastly, the momentum factor in GD+M is set to be γ = 1 −polylog(d) d
."
-MLP,0.3952380952380952,"Our Parametrization 2.1 matches with the parameters used in practice as the weights are generally
initialized from Gaussian with small variance and momentum is set close to 1 (Sutskever et al., 2013).
3
MAIN RESULTS
We now formally state our main theorems regarding the generalization of models trained using
equation (GD) and equation (GD+M) on the training set Z generated by equation (D). As announced
in the introduction, we show that the GD+M model incurs a generalization error that is dramatically
smaller than the GD model. Before introducing the main result, we deﬁne some notations:"
THIS IS CONSISTENT WITH THE EMPIRICAL OBSERVATION THAT ONLY MOMENTUM WITH LARGE LEARNING RATE IMPROVES,0.4,"1This is consistent with the empirical observation that only momentum with large learning rate improves
generalization (Sutskever et al., 2013)"
THIS IS CONSISTENT WITH THE EMPIRICAL OBSERVATION THAT ONLY MOMENTUM WITH LARGE LEARNING RATE IMPROVES,0.40476190476190477,Under review as a conference paper at ICLR 2022
THIS IS CONSISTENT WITH THE EMPIRICAL OBSERVATION THAT ONLY MOMENTUM WITH LARGE LEARNING RATE IMPROVES,0.4095238095238095,"Main objects.
Let r ∈[m], i ∈[N], j ∈P\{P(Xi)}, γ ∈(0, 1) and t ≥0. We are mainly
interested in w(t)
r , the r-th weight of the network, ∇wr bL(W (t)) the gradient of the training loss w.r.t.
wr, g(t)
r
the momentum gradient deﬁned by g(t+1)
r
= γg(t)
r
+ (1 −γ)∇wr bL(W (t)). The analysis lies
on the projection of these objects on the feature w∗and on noisy patches Xi[j]. We introduce the
following notations for the component of the learned weights along feature and noise directions:"
THIS IS CONSISTENT WITH THE EMPIRICAL OBSERVATION THAT ONLY MOMENTUM WITH LARGE LEARNING RATE IMPROVES,0.4142857142857143,"– Projection on w∗: c(t)
r
= ⟨w(t)
r , w∗⟩."
THIS IS CONSISTENT WITH THE EMPIRICAL OBSERVATION THAT ONLY MOMENTUM WITH LARGE LEARNING RATE IMPROVES,0.41904761904761906,"– Projection on Xi[j] : Ξ(t)
i,j,r = ⟨w(t)
r , Xi[j]⟩."
THIS IS CONSISTENT WITH THE EMPIRICAL OBSERVATION THAT ONLY MOMENTUM WITH LARGE LEARNING RATE IMPROVES,0.4238095238095238,"– Total noise: Ξ(t)
i
= Pm
r=1
P"
THIS IS CONSISTENT WITH THE EMPIRICAL OBSERVATION THAT ONLY MOMENTUM WITH LARGE LEARNING RATE IMPROVES,0.42857142857142855,"j∈[P ]\{P (Xi)}⟨w(t)
r , Xi[j]⟩3."
THIS IS CONSISTENT WITH THE EMPIRICAL OBSERVATION THAT ONLY MOMENTUM WITH LARGE LEARNING RATE IMPROVES,0.43333333333333335,"– Maximum signal: let rmax = argmaxr∈[m]c(t)
r , c(t) = c(t)
rmax"
THIS IS CONSISTENT WITH THE EMPIRICAL OBSERVATION THAT ONLY MOMENTUM WITH LARGE LEARNING RATE IMPROVES,0.4380952380952381,"Theorem 3.1. Assume that we run GD on (P) for T iterations with parameters set as in Parametriza-
tion 2.1. With probability at least 1 −o(1), the weights learned by GD"
THIS IS CONSISTENT WITH THE EMPIRICAL OBSERVATION THAT ONLY MOMENTUM WITH LARGE LEARNING RATE IMPROVES,0.44285714285714284,"1. partially learn the feature: for all r ∈[m], |c(T )
r
| ≤˜O(1/α)."
THIS IS CONSISTENT WITH THE EMPIRICAL OBSERVATION THAT ONLY MOMENTUM WITH LARGE LEARNING RATE IMPROVES,0.44761904761904764,"2. memorize from small margin data: for all i ∈Z2, Ξ(t)
i
≥˜Ω(1)."
THIS IS CONSISTENT WITH THE EMPIRICAL OBSERVATION THAT ONLY MOMENTUM WITH LARGE LEARNING RATE IMPROVES,0.4523809523809524,"Consequently, the training error is smaller than O(µ/poly(d)) and the test error is at least ˜Ω(µ)."
THIS IS CONSISTENT WITH THE EMPIRICAL OBSERVATION THAT ONLY MOMENTUM WITH LARGE LEARNING RATE IMPROVES,0.45714285714285713,"Intuitively, the training process of the GD model is described as follows. Since the large margin
data are dominant in Z, the gradient points mainly in the direction of the feature w∗. Therefore, GD
eventually learns the feature in Z1 (Lemma 4.1) and the gradients from Z1 quickly become small.
Afterwards, the gradient is dominated by the gradients from Z2 (Lemma 4.2). Because Z2 has small
margin, the full gradient is now directed by the noisy patches. It implies that GD memorizes noise in
Z2 (Lemma 4.4). Since these gradients also control the amount of remaining feature to be learned
(Lemma 4.3), we conclude that the GD model partially learns the feature and introduces a huge noise
component in the learned weights. We provide a proof sketch of Theorem 3.1 in Section 4."
THIS IS CONSISTENT WITH THE EMPIRICAL OBSERVATION THAT ONLY MOMENTUM WITH LARGE LEARNING RATE IMPROVES,0.46190476190476193,"Theorem 3.2. Assume that we run GD+M on equation (P) for T iterations with parameters set as in
Parametrization 2.1. With probability at least 1 −o(1), the weights learned by GD+M"
THIS IS CONSISTENT WITH THE EMPIRICAL OBSERVATION THAT ONLY MOMENTUM WITH LARGE LEARNING RATE IMPROVES,0.4666666666666667,1. (at least for one of them) is highly correlated with the feature: c(T ) > ˜Ω(1/β).
THIS IS CONSISTENT WITH THE EMPIRICAL OBSERVATION THAT ONLY MOMENTUM WITH LARGE LEARNING RATE IMPROVES,0.4714285714285714,"2. are barely correlated with noise: for all r ∈[m], for all i ∈[N] and j ∈[P]. |Ξ(T )
i,j,r| ≤˜O(σ0)."
THIS IS CONSISTENT WITH THE EMPIRICAL OBSERVATION THAT ONLY MOMENTUM WITH LARGE LEARNING RATE IMPROVES,0.47619047619047616,"Consequently, the training loss and the test error are at most O(µ/poly(d))."
THIS IS CONSISTENT WITH THE EMPIRICAL OBSERVATION THAT ONLY MOMENTUM WITH LARGE LEARNING RATE IMPROVES,0.48095238095238096,"Intuitively, the GD+M model follows this training process. Similarly to GD, it ﬁrst ﬁts the Z1
(Lemma 5.1). Contrary to GD, the momentum gradient is still highly correlated with w∗after this
step (Lemma 5.2). Indeed, the key difference is that momentum accumulates historical gradients.
Since these gradients were accumulated when learning large margin data, the direction of momentum
gradient is highly biased towards w∗. Therefore, the GD+M model ampliﬁes the feature from these
historical gradients to learn the feature in small margin data (Lemma 5.3). Subsequently, the gradient
becomes small (Lemma 5.4) and the weights are no longer updated. Therefore, the GD+M model
manages to ignore the noisy patches (Lemma 5.5) and learns the feature from both Z1 and Z2. We
provide a proof sketch of Theorem 3.2 in Section 5."
THIS IS CONSISTENT WITH THE EMPIRICAL OBSERVATION THAT ONLY MOMENTUM WITH LARGE LEARNING RATE IMPROVES,0.4857142857142857,"To state the proof, we further decompose the gradients along signal and noise directions."
THIS IS CONSISTENT WITH THE EMPIRICAL OBSERVATION THAT ONLY MOMENTUM WITH LARGE LEARNING RATE IMPROVES,0.49047619047619045,"– Projection on w∗: G (t)
r
= ⟨∇wr bL(Wt), w∗⟩and G(t)
r
= ⟨g(t)
r , w∗⟩."
THIS IS CONSISTENT WITH THE EMPIRICAL OBSERVATION THAT ONLY MOMENTUM WITH LARGE LEARNING RATE IMPROVES,0.49523809523809526,"– Projection on Xi[j] : G(t)
i,j,r = ⟨∇wr bL(W (t)), Xi[j]⟩, G(t)
i,j,r = ⟨g(t)
r , Xi[j]⟩."
THIS IS CONSISTENT WITH THE EMPIRICAL OBSERVATION THAT ONLY MOMENTUM WITH LARGE LEARNING RATE IMPROVES,0.5,"– Maximum signal: let rmax = argmaxr∈[m]c(t)
r , c(t) = c(t)
rmax and G(t) = G(t)
rmax."
THIS IS CONSISTENT WITH THE EMPIRICAL OBSERVATION THAT ONLY MOMENTUM WITH LARGE LEARNING RATE IMPROVES,0.5047619047619047,"Signal and noise iterates.
Our analysis is build upon a decomposition of the updates equation (GD)
and equation (GD+M) on w∗and Xi[j]. These decompositions are respectively deﬁned as follows:
c(t+1)
r
= c(t)
r
−ηG (t)
r
(GD-S)
Ξ(t+1)
i,j,r
= Ξ(t)
i,j,r −ηG(t)
i,j,r
(GD-N)"
THIS IS CONSISTENT WITH THE EMPIRICAL OBSERVATION THAT ONLY MOMENTUM WITH LARGE LEARNING RATE IMPROVES,0.5095238095238095,Under review as a conference paper at ICLR 2022
THIS IS CONSISTENT WITH THE EMPIRICAL OBSERVATION THAT ONLY MOMENTUM WITH LARGE LEARNING RATE IMPROVES,0.5142857142857142,"(
G(t+1)
r
= γG(t)
r
+ (1 −γ)G (t)
r
c(t+1)
r
= c(t)
r
−ηG(t+1)
r
(GDM-S)"
THIS IS CONSISTENT WITH THE EMPIRICAL OBSERVATION THAT ONLY MOMENTUM WITH LARGE LEARNING RATE IMPROVES,0.5190476190476191,"(
G(t+1)
i,j,r
= γG(t)
i,j,r + (1 −γ)G(t)
i,j,r
Ξ(t+1)
i,j,r
= Ξ(t)
i,j,r −G(t+1)
i,j,r
(GDM-N)
We detail how to use these dynamics to analyze GD+M and GD in Section 4 and Section 5. Our anal-
ysis heavily depends on the gradients of the training loss which involve sigmoid(x) = (1 + e−x)−1 .
We deﬁne the derivative of a data-point i as ℓ(t)
i
= sigmoid(−yifW (t)(Xi)), the derivatives
ν(t)
k
= 1"
THIS IS CONSISTENT WITH THE EMPIRICAL OBSERVATION THAT ONLY MOMENTUM WITH LARGE LEARNING RATE IMPROVES,0.5238095238095238,"N
P
i∈Zk ℓ(t)
i
for k ∈{1, 2} and the full derivative ν(t) = ν(t)
1
+ ν(t)
2 ."
ANALYSIS OF GD,0.5285714285714286,"4
ANALYSIS OF GD"
ANALYSIS OF GD,0.5333333333333333,"In this section, we provide a proof sketch for Theorem 3.1 that reﬂects the behavior of GD with
λ = 0. A more detailed proof (extending to λ > 0) can be found in the Appendix."
ANALYSIS OF GD,0.5380952380952381,"Step 1: Learning Z1.
At the beginning of the learning process, the gradient is mostly dominated
by the gradients coming from the Z1 samples. Since these data have large margin, the gradient is
thus highly correlated with w∗and c(t)
r
increases as shown in the following Lemma.
Lemma 4.1. For all r ∈[m] and t ≥0, equation (GD-S) is simpliﬁed as:"
ANALYSIS OF GD,0.5428571428571428,"c(t+1)
r
≥c(t)
r
+ ˜Θ(η)α3(c(t)
r )2 · sigmoid(−Pt
s=1 α3(c(t)
s )3)."
ANALYSIS OF GD,0.5476190476190477,"Consequently, after T0 = ˜Θ

1
ηα3σ0"
ANALYSIS OF GD,0.5523809523809524,"
iterations, for all t ∈[T0, T], we have c(t) ≥˜Ω(1/α)."
ANALYSIS OF GD,0.5571428571428572,"Intuitively, the increment in the update in Lemma 4.1 is non-zero when the sigmoid is not too small
which is equivalent to c(t) ≤˜O(1/α). Therefore, c(t) keeps increasing until reaching this threshold.
After this step, the Z1 data have small gradient and therefore, GD has learned these data."
ANALYSIS OF GD,0.5619047619047619,"Lemma 4.2. Let T0 = ˜Θ

1
ηα3σ0"
ANALYSIS OF GD,0.5666666666666667,"
. After t ∈[T0, T] iterations, the Z1 derivative is bounded as"
ANALYSIS OF GD,0.5714285714285714,"ν(t)
1
≤˜O

1
η(t−T0+1)α

+ ˜O

β3"
ANALYSIS OF GD,0.5761904761904761,"α

ν(t)
2 . The full derivative is ν(t) ≤˜O

1
η(t−T0+1)α +

1 + β3"
ANALYSIS OF GD,0.580952380952381,"α

ν(t)
2

."
ANALYSIS OF GD,0.5857142857142857,"By our choice of parameter, Lemma 4.2 indicates that the full gradient is dominated by the gradients
from Z2 data after T0 = ˜Ω

1
ˆµηα

. Consequently, ν(t)
2
also rules the amount of feature learnt by GD."
ANALYSIS OF GD,0.5904761904761905,"Lemma 4.3. Let T0 = ˜Θ

1
ηα3σ0"
ANALYSIS OF GD,0.5952380952380952,"
. For t ∈[T0, T], equation (GD-S) becomes c(t+1) ≤˜O(1/α) +"
ANALYSIS OF GD,0.6,"˜O(ηβ3/α) Pt
τ=T0 ν(τ)
2 ."
ANALYSIS OF GD,0.6047619047619047,"Lemma 4.3 implies that quantifying the decrease rate of ν(t)
2
provides an estimate on the quantity
of feature learnt by the model. We remark that ν(t)
2
= sigmoid(β3 Pm
s=1(c(t)
s )3 + Ξ(t)
i ) for some
i ∈Z2. We thus need to determine whether the feature or the noise terms dominates in the sigmoid."
ANALYSIS OF GD,0.6095238095238096,"Step 2: Memorizing Z2.
We now show that the total correlation between the weights and the noise
in Z2 data increases until being large."
ANALYSIS OF GD,0.6142857142857143,"Lemma 4.4. Let t ≥0 and i ∈Z2. Assume that Ξ(t)
i
≤˜O(1). Then, equation (GD-N) can be
simpliﬁed as:"
ANALYSIS OF GD,0.6190476190476191,"yiΞ(t+1)
i,j,r
≥yiΞ(0)
i,j,r +
˜Θ(ησ2d) N t
X"
ANALYSIS OF GD,0.6238095238095238,"τ=0
(Ξ(τ)
i,j,r)2 −˜O Pσ2√ d
α ! ."
ANALYSIS OF GD,0.6285714285714286,"Let T1 = ˜O

N
σ0σ
√ dσ2d"
ANALYSIS OF GD,0.6333333333333333,"
. Therefore, Ξ(t)
i
≥˜Ω(1), for t ∈[T1, T] and thus GD memorizes."
ANALYSIS OF GD,0.638095238095238,"By Lemma 4.4, in the gradient of Z2 data, the noise term dominates the feature term (which scales as
˜O(β3)). Consequently, the algorithm memorizes the Z2 data which implies a fast decay of ν(t)
2 ."
ANALYSIS OF GD,0.6428571428571429,Under review as a conference paper at ICLR 2022
ANALYSIS OF GD,0.6476190476190476,"Lemma 4.5. Let T1 = ˜O

N
σ0σ
√ dσ2d"
ANALYSIS OF GD,0.6523809523809524,"
. For t ∈[T1, T], we have Pt
τ=0 ν(τ)
2
≤˜O

1
ησ0 
."
ANALYSIS OF GD,0.6571428571428571,"Combining Lemma 4.5 and Lemma 4.3, we prove that GD partially learns the feature.
Lemma 4.6. For t ≤T, the signal component satisﬁes c(t) ≤˜O(1/α)."
ANALYSIS OF GD,0.6619047619047619,"Lemma 4.4 and Lemma 4.6 respectively yield the ﬁrst two items in Theorem 3.1. Bounds on the
training and test errors are respectively obtained by plugging these results in (P) and (TE)."
ANALYSIS OF GD,0.6666666666666666,"5
ANALYSIS OF GD+M"
ANALYSIS OF GD,0.6714285714285714,"In this section, we provide a proof sketch for Theorem 3.2 that reﬂects the behavior of GD+M with
λ = 0. A more detailed proof (also extending to λ > 0) can be found in the Appendix."
ANALYSIS OF GD,0.6761904761904762,"Step 1: Learning Z1.
Similarly to GD, by our initialization choice, the early gradients and so,
momentum gradients are large. They are also spanned by the feature w∗and therefore, the GD+M
model also increases its correlation with w∗.
Lemma 5.1. For all r ∈[m] and t ≥0, as long as c(t) ≤˜O(1/α), the momentum update
equation (GDM-S) is simpliﬁed as:
−G(t+1)
r
= −γG(t)
r
+ (1 −γ)Θ(α3)(c(t)
r )2"
ANALYSIS OF GD,0.680952380952381,"Consequently, after T0 = ˜Θ

1
σ0α2 +
1
1−γ

iterations, for all t ∈[T0, T], we have c(t) ≥˜Ω(1/α)."
ANALYSIS OF GD,0.6857142857142857,"Step 2: Learning Z2.
Contrary to GD, GD+M has a large momentum that contains w∗after Step 1."
ANALYSIS OF GD,0.6904761904761905,"Lemma 5.2. Let T0 = ˜Θ

1
σ0α3 +
1
1−γ

. For t ∈[T0, T], we have G(t) ≥˜Ω(√1 −γ/α)."
ANALYSIS OF GD,0.6952380952380952,"Lemma 5.2 hints an important distinction between GD and GD+M: while the current gradient along
w∗is small at time T0, the momentum gradient stores historical gradients that are spanned by w∗. It
ampliﬁes the feature present in previous gradients to learn the feature from small margin data."
ANALYSIS OF GD,0.7,"Lemma 5.3. Let T0 = ˜Θ

1
σ0α3 +
1
1−γ

. After T1 = T0 + ˜Θ

1
1−γ

iterations, for t ∈[T1, T], we"
ANALYSIS OF GD,0.7047619047619048,"have c(t) ≥˜Ω

1
√1−γα

. Our choice of parameter in Section 2, this implies c(t) ≥˜Ω(1/β)."
ANALYSIS OF GD,0.7095238095238096,"Lemma 5.3 states that at least one of the weights that is highly correlated with the feature compared
to GD where c(t) = ˜O(1). This result implies that ν(t) converges fast."
ANALYSIS OF GD,0.7142857142857143,"Lemma 5.4. Let T0 = ˜Θ

1
ησ0α3 +
1
1−γ

. After T1 = T0 + ˜Θ

1
1−γ

iterations, for t ∈[T1, T],"
ANALYSIS OF GD,0.719047619047619,"ν(t) ≤˜O

1
η(t−T1+1)β

."
ANALYSIS OF GD,0.7238095238095238,"With this fast convergence, Lemma 5.4 implies that the correlation of the weights with the noisy
patches does not have enough time to increase and thus, remains small.
Lemma 5.5. Let i ∈[N], j ∈[P]\{P(Xi)} and r ∈[m]. For t ≥0, equation (GDM-N) can be
rewritten as |G(t+1)
i,j,r | ≤γ|G(t)
i,j,r| + (1 −γ) ˜O(σ2
0σ4d2)ν(t). As a consequence, after t ∈[T1, T]"
ANALYSIS OF GD,0.7285714285714285,"iterations, we thus have |Ξ(t)
i,j,r| ≤˜O(σ0σ
√ d)."
ANALYSIS OF GD,0.7333333333333333,Lemma 5.3 and Lemma 5.5 respectively yield the two ﬁrst items in Theorem 3.2.
DISCUSSION,0.7380952380952381,"6
DISCUSSION"
DISCUSSION,0.7428571428571429,"Our work is a ﬁrst step towards understanding the algorithmic regularization of momentum and leaves
room for improvements. We constructed a data distribution where historical feature ampliﬁcation
may explain the generalization improvement of momentum. However, it would be interesting to
understand whether this phenomenon is the only reason or whether there are other mechanisms
explaining momentum’s beneﬁts.An interesting setting for this question is NLP where momentum is
used to train large models as BERT (Devlin et al., 2018). Lastly, our analysis is in the batch setting
to isolate the generalization induced by momentum. It would be interesting to understand how the
stochastic noise and the momentum together contribute to the generalization of a neural network."
DISCUSSION,0.7476190476190476,Under review as a conference paper at ICLR 2022
REFERENCES,0.7523809523809524,REFERENCES
REFERENCES,0.7571428571428571,"Zeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillation and
self-distillation in deep learning. arXiv preprint arXiv:2012.09816, 2020."
REFERENCES,0.7619047619047619,"Sanjeev Arora, Zhiyuan Li, and Kaifeng Lyu. Theoretical analysis of auto rate-tuning by batch
normalization. arXiv preprint arXiv:1812.03981, 2018."
REFERENCES,0.7666666666666667,"Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix
factorization. arXiv preprint arXiv:1905.13655, 2019."
REFERENCES,0.7714285714285715,"Anthony Carbery and James Wright. Distributional and lq norm inequalities for polynomials over
convex bodies in Rn. Mathematical research letters, 8(3):233–248, 2001."
REFERENCES,0.7761904761904762,"Lenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks
trained with the logistic loss. In Conference on Learning Theory, pp. 1305–1338. PMLR, 2020."
REFERENCES,0.780952380952381,"Ashok Cutkosky and Harsh Mehta. Momentum improves normalized sgd. In International Conference
on Machine Learning, pp. 2260–2268. PMLR, 2020."
REFERENCES,0.7857142857142857,"Alexandre d’Aspremont. Smooth optimization with approximate gradient. SIAM Journal on Opti-
mization, 19(3):1171–1183, 2008."
REFERENCES,0.7904761904761904,"Aaron Defazio. Understanding the role of momentum in non-convex optimization: Practical insights
from a lyapunov analysis. arXiv preprint arXiv:2010.00406, 2020."
REFERENCES,0.7952380952380952,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248–255. Ieee, 2009."
REFERENCES,0.8,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018."
REFERENCES,0.8047619047619048,"Olivier Devolder, François Glineur, and Yurii Nesterov. First-order methods of smooth convex
optimization with inexact oracle. Mathematical Programming, 146(1):37–75, 2014."
REFERENCES,0.8095238095238095,"Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola,
Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017."
REFERENCES,0.8142857142857143,"Suriya Gunasekar, Blake Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nathan Srebro.
Implicit regularization in matrix factorization. In 2018 Information Theory and Applications
Workshop (ITA), pp. 1–10. IEEE, 2018."
REFERENCES,0.819047619047619,"Jeff Z HaoChen, Colin Wei, Jason D Lee, and Tengyu Ma. Shape matters: Understanding the implicit
bias of the noise covariance. arXiv preprint arXiv:2006.08680, 2020."
REFERENCES,0.8238095238095238,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770–778, 2016."
REFERENCES,0.8285714285714286,"Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the gen-
eralization gap in large batch training of neural networks. arXiv preprint arXiv:1705.08741,
2017."
REFERENCES,0.8333333333333334,"Elad Hoffer, Ron Banner, Itay Golan, and Daniel Soudry. Norm matters: efﬁcient and accurate
normalization schemes in deep networks, 2019."
REFERENCES,0.8380952380952381,"Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International conference on machine learning, pp. 448–456.
PMLR, 2015."
REFERENCES,0.8428571428571429,"Ziwei Ji and Matus Telgarsky. The implicit bias of gradient descent on nonseparable data. In
Conference on Learning Theory, pp. 1772–1798. PMLR, 2019."
REFERENCES,0.8476190476190476,Under review as a conference paper at ICLR 2022
REFERENCES,0.8523809523809524,"Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter
Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv
preprint arXiv:1609.04836, 2016."
REFERENCES,0.8571428571428571,"Rahul Kidambi, Praneeth Netrapalli, Prateek Jain, and Sham Kakade. On the insufﬁciency of existing
momentum schemes for stochastic optimization. In 2018 Information Theory and Applications
Workshop (ITA), pp. 1–9. IEEE, 2018."
REFERENCES,0.861904761904762,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.8666666666666667,"Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009."
REFERENCES,0.8714285714285714,"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep con-
volutional neural networks. Advances in neural information processing systems, 25:1097–1105,
2012."
REFERENCES,0.8761904761904762,"Guillaume Leclerc and Aleksander Madry. The two regimes of deep network training. arXiv preprint
arXiv:2002.10376, 2020."
REFERENCES,0.8809523809523809,"Laurent Lessard, Benjamin Recht, and Andrew Packard. Analysis and design of optimization
algorithms via integral quadratic constraints, 2015."
REFERENCES,0.8857142857142857,"Yuanzhi Li, Colin Wei, and Tengyu Ma. Towards explaining the regularization effect of initial large
learning rate in training neural networks. arXiv preprint arXiv:1907.04595, 2019."
REFERENCES,0.8904761904761904,"Yanli Liu, Yuan Gao, and Wotao Yin. An improved analysis of stochastic gradient descent with
momentum. arXiv preprint arXiv:2007.07989, 2020."
REFERENCES,0.8952380952380953,"Shachar Lovett. An elementary proof of anti-concentration of polynomials in gaussian variables.
Electron. Colloquium Comput. Complex., 17:182, 2010."
REFERENCES,0.9,"Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks.
arXiv preprint arXiv:1906.05890, 2019."
REFERENCES,0.9047619047619048,"Vien Mai and Mikael Johansson. Convergence of a stochastic gradient method with momentum
for non-smooth non-convex optimization. In International Conference on Machine Learning, pp.
6630–6639. PMLR, 2020."
REFERENCES,0.9095238095238095,"Arkadij Semenoviˇc Nemirovskij and David Borisovich Yudin. Problem complexity and method
efﬁciency in optimization. 1983."
REFERENCES,0.9142857142857143,"Yurii Nesterov. A method for unconstrained convex minimization problem with the rate of conver-
gence o (1/kˆ 2). In Doklady an ussr, volume 269, pp. 543–547, 1983."
REFERENCES,0.919047619047619,"Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2003."
REFERENCES,0.9238095238095239,"Behnam Neyshabur, Ruslan Salakhutdinov, and Nathan Srebro. Path-sgd: Path-normalized optimiza-
tion in deep neural networks. arXiv preprint arXiv:1506.02617, 2015."
REFERENCES,0.9285714285714286,"Boris T Polyak. Gradient methods for the minimisation of functionals. USSR Computational
Mathematics and Mathematical Physics, 3(4):864–878, 1963."
REFERENCES,0.9333333333333333,"Boris T Polyak. Some methods of speeding up the convergence of iteration methods. Ussr computa-
tional mathematics and mathematical physics, 4(5):1–17, 1964."
REFERENCES,0.9380952380952381,"Mark Schmidt, Nicolas Le Roux, and Francis Bach. Convergence rates of inexact proximal-gradient
methods for convex optimization. arXiv preprint arXiv:1109.2415, 2011."
REFERENCES,0.9428571428571428,"Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014."
REFERENCES,0.9476190476190476,"Samuel L. Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc V. Le. Don’t decay the learning rate,
increase the batch size, 2018."
REFERENCES,0.9523809523809523,Under review as a conference paper at ICLR 2022
REFERENCES,0.9571428571428572,"Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit
bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):
2822–2878, 2018."
REFERENCES,0.9619047619047619,"Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overﬁtting. The journal of machine
learning research, 15(1):1929–1958, 2014."
REFERENCES,0.9666666666666667,"Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization
and momentum in deep learning. In International conference on machine learning, pp. 1139–1147.
PMLR, 2013."
REFERENCES,0.9714285714285714,"Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26–31,
2012."
REFERENCES,0.9761904761904762,"Roman Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge university press, 2018."
REFERENCES,0.9809523809523809,"Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cam-
bridge University Press, 2019."
REFERENCES,0.9857142857142858,"Colin Wei, Sham Kakade, and Tengyu Ma. The implicit and explicit regularization effects of dropout,
2020."
REFERENCES,0.9904761904761905,"Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. arXiv preprint arXiv:1705.08292, 2017."
REFERENCES,0.9952380952380953,"Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146,
2016."
