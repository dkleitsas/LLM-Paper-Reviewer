Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003278688524590164,"In large-scale supervised learning, after a model is trained with an initial dataset,
a common challenge is how to exploit new incremental data without re-training
the model from scratch.
Motivated by this problem, we revisit the canonical
problem of dynamic least-squares regression (LSR), where the goal is to learn
a linear model over incremental training data.
In this setup, data and labels
(A(t), b(t)) 2 Rt⇥d⇥Rt evolve in an online fashion (t ≫d), and the goal is to ef-
ﬁciently maintain an (approximate) solution of minx(t) kA(t)x(t) −b(t)k2 for all
t 2 [T]. Our main result is a dynamic data structure which maintains an arbitrarily
small constant approximate solution to dynamic LSR with amortized update time
O(d1+o(1)), almost matching the running time of the static (sketching-based) so-
lution. By contrast, for exact (or 1/ poly(n)-accuracy) solutions, we show a sepa-
ration between the models, namely, that dynamic LSR requires ⌦(d2−o(1)) amor-
tized update time under the OMv Conjecture (Henzinger et al., STOC’15). Our
data structure is fast, conceptually simple, easy to implement, and our experiments
demonstrate their practicality on both synthetic and real-world datasets."
INTRODUCTION,0.006557377049180328,"1
INTRODUCTION"
INTRODUCTION,0.009836065573770493,"The problem of least-squares regression (LSR) dates back to Gauss in 1821 (Stigler, 1981), and
is the backbone of statistical inference (Hastie et al., 2001), signal processing (Rabiner & Gold,
1975), convex optimization (Bubeck, 2015), control theory (Chui, 1990) and network routing (Lee &
Sidford, 2014; Madry, 2013). Given an overdetermined (n ≫d) linear system A 2 Rn⇥d, b 2 Rn,
the goal is to ﬁnd the solution vector x that minimizes the mean squared error (MSE)"
INTRODUCTION,0.013114754098360656,"min
x2Rn kAx −bk2.
(1)"
INTRODUCTION,0.01639344262295082,"Among many other loss functions (e.g., `p) that have been studied for linear regression, `2-regression
has been the most popular choice as it is at the same time robust to outliers, and admits a high-
accuracy efﬁcient solution."
INTRODUCTION,0.019672131147540985,"The computational task of least-squares regression arises naturally in high-dimensional statistics
and has been the central of focus. The exact closed-form solution is given by the well-known
Normal equation x? = (A>A)−1A>b, which requires O(nd2) time to compute using naive matrix-
multiplication, or O(nd!−1) ⇡O(nd1.37) time using fast matrix-multiplication (FMM) (Strassen,
1969) for the current FMM exponent of ! ⇡2.37 (Le Gall, 2014; Alman & Williams, 2021)."
INTRODUCTION,0.022950819672131147,"Despite the elegance and simplicity of this closed-form solution, in practice the latter runtime is
often too slow, especially in modern data analysis applications where both the dimension of the
feature space (d) and the size of datasets (n) are overwhelmingly large. A more modest objective in
attempt to circumvent this computational overhead, is to seek an ✏-accurate solution that satisﬁes"
INTRODUCTION,0.02622950819672131,kAx −bk2 (1 + ✏) min
INTRODUCTION,0.029508196721311476,x2Rd kAx −bk2.
INTRODUCTION,0.03278688524590164,"This was the primary motivation behined the development of the sketch-and-solve paradigm, where
the idea is to ﬁrst compress the matrix into one with fewer (⇠d/✏2) rows and then to compute the
standard LSR solution but over the smaller matrix. A long line of developments on this framework
culminates in algorithms that run in close to input-sparsity time (Sarlos, 2006; Clarkson & Woodruff,
2017; Nelson & Nguyên, 2013; Chepurko et al., 2021). In particular, a direct application of sketch-
and-solve yields an algorithm runs in eO(nnz(A)✏−1 + d!) 1, which is near optimal in the “low"
INTRODUCTION,0.036065573770491806,"1In this paper we use eO(·) to hide polylogarithmic terms, and we use O✏(·) to hide poly(log d, ✏−1) terms."
INTRODUCTION,0.03934426229508197,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.04262295081967213,"precision” regime (✏= 1/ poly(log d)). Interestingly, when combined with more sophisticated
ideas of preconditioning and (conjugate) gradient descent, the runtime of this algorithm in terms
of the error ✏can be further improved to eO(nnz(A) log(1/✏) + d!), which yields a high precision
algorithm, i.e., it can efﬁciently solve the problem to within polynomial accuracy ✏= 1/ poly(d)."
INTRODUCTION,0.04590163934426229,"Dynamic least-squares
In many real-world scenarios of the aforementioned applications, data is
evolving in an online fashion either by nature or by design, and such applications require maintaining
the solution (1) adaptively, where rows of the data matrix and their corresponding labels (A(t), b(t))
arrive one-by-one incrementally. This is known as the dynamic least-squares regression problem."
INTRODUCTION,0.04918032786885246,"The origins of dynamic least-squares regression was in control theory of the 1950’s (Plackett, 1950),
in the context of dynamical linear systems. In this setup, the data matrix [A(t), b(t)] corresponds to
the set of measurement and it evolves in an online (incremental) fashion, and the goal is to efﬁciently
maintain the (exact) solution to a noisy linear system b := A(t)x(t) + ⇠(t) without recomputing
the LSR solution from scratch. The recursive least-squares (RLS) framework and the celebrated
Kalman ﬁlter (Kalman, 1960) provide a rather simple update rule for maintaining an exact solu-
tion for this problem, by maintaining the sample covariance matrix and using Woodburry’s identity
(which assert that an incremental update to [A(t), b(t)] translates into a rank-1 update to the sample
covariance matrix), and hence each update can be computed in O(d2) time (Kalman, 1960)."
INTRODUCTION,0.05245901639344262,"Beyond this classic motivation for dynamic LSR, a more timely motivation comes from modern
deep learning applications: Most neural networks need to be frequently re-trained upon arrival on
new training data, in order to improve prediction accuracy, and it is desirable to avoid recomputing
weights from scratch. This problem of efﬁcient incremental training of DNNs has been studied
before in elastic machine learning (Liberty et al., 2020) and in the context of continual learning
(Parisi et al., 2019). Our work sheds light on this question by analyzing the minimal computational
resources required for `2 loss-minimization."
INTRODUCTION,0.05573770491803279,"Despite the rich and versatile literature on static LSR, the understanding of the dynamic counterpart
was so far quite limited: The previous best known result requires O(d2) amortized update time (by
a direct application of the Woodbury identity). The basic questions we address in this papers are:"
INTRODUCTION,0.05901639344262295,"Is it possible to achieve faster update time for maintaining an exact solution? How about a small-
approximate solution – Is it then possible to achieve amortized O(d) or even input-sparsity time?"
INTRODUCTION,0.06229508196721312,"In this paper, we settle both of these questions and present an essentially complete characterization
of the dynamic complexity of LSR."
OVERVIEW OF OUR RESULTS,0.06557377049180328,"1.1
OVERVIEW OF OUR RESULTS"
OVERVIEW OF OUR RESULTS,0.06885245901639345,"Our ﬁrst result is a negative answer to the ﬁrst question above of maintaining exact (or polynomial-
accuracy) LSR solutions in the dynamic setting – We prove that Kalman’s approach is essentially
optimal, assuming the popular Online Matrix-Vector (OMv) Conjecture (Henzinger et al., 2015) 2:"
OVERVIEW OF OUR RESULTS,0.07213114754098361,"Theorem 1.1 (Hardness of exact dynamic LSR, informal). Assuming the OMv Conjecture, any
dynamic algorithm that maintains an ✏= 1/ poly(d)-approximate solution for the dynamic LSR
problem over T = poly(d) iterations, must have ⌦(d2−o(1)) amortized update time per iteration."
OVERVIEW OF OUR RESULTS,0.07540983606557378,"Theorem 1.1 separates the static and the dynamic complexities of the exact LSR problem: As men-
tioned above, the static problem can be solved by batching rows together using FMM in time
O(Td!−1), whereas the dynamic problem requires ⌦(Td2) by Theorem 1.1. Indeed, the impli-
cation Theorem 1.1 is stronger, it also separates the static and dynamic complexity of approximate
LSR problem under the high precision regime, it asserts that a polylogarithmic dependence on the
precision (i.e. d poly(log(1/✏))) on update time is impossible (assuming OMv), in sharp contrast to
the static case."
OVERVIEW OF OUR RESULTS,0.07868852459016394,"We next focus on an approximate version of this classic online problem, dynamic ✏-LSR, where
the goal is to efﬁciently maintain, during all iterations t 2 [T], an ✏-approximate solution under
incremental row-updates to A(t) and labels b(t), where efﬁciency is measured by the amortized"
OVERVIEW OF OUR RESULTS,0.08196721311475409,"2This conjecture postulates that multiplying a ﬁxed d ⇥d matrix A with an online matrix B, column-by-
column (ABi), requires d3−o(1) time, in sharp contrast to the batch setting where this can be done using FMM
in d! ⌧d3 time. See Section 4."
OVERVIEW OF OUR RESULTS,0.08524590163934426,Under review as a conference paper at ICLR 2022
OVERVIEW OF OUR RESULTS,0.08852459016393442,"update time for inserting a new row. A natural complexity benchmark for this dynamic problem is
the aforementioned best static sketch-and-solve solution, which for n = T is eO(nnz(A(T ))✏−1 +
d!) = eO(nnz(A)✏−1) for T ≫d. Our main result is a provably efﬁcient and practical dynamic
data structure, whose total running time essentially matches the complexity of the ofﬂine problem:
Theorem 1.2 (Main result, informal version of Theorem 3.1). For any accuracy parameter ✏> 0,
there is a randomized dynamic data structure which, with probability at least 0.9, maintains an ✏-
approximate solution to the dynamic LSR problem simultaneously for all iterations t 2 [T], with
total update time"
OVERVIEW OF OUR RESULTS,0.09180327868852459,O(✏−2 nnz(A(T )) log(T) + ✏−6d3 log5(T)).
OVERVIEW OF OUR RESULTS,0.09508196721311475,"Theorem 1.2 almost matches the fastest static sketching-based solution, up to polylogarithmic terms
and the additive FMM term. When T ≫d, this theorem shows that amortized update time of our
algorithm is O(d1+o(1))."
RELATED WORK,0.09836065573770492,"1.2
RELATED WORK"
RELATED WORK,0.10163934426229508,"Sketching and sampling
The least squares regression as a fundamental problem has been exten-
sively studied in the literature. A long line of work (Ailon & Chazelle, 2006; Clarkson & Woodruff,
2017; Nelson & Nguyên, 2013; Avron et al., 2017; Cohen et al., 2015; Woodruff, 2014; 2021) have
focused on using dimension reduction technique (sketching or sampling) to speedup the computation
task, culminates into algorithms that run in eO(nnz(A) log(1/✏) + d!) time (Clarkson & Woodruff,
2017). See Appendix A for detailed discussions."
RELATED WORK,0.10491803278688525,"Regression in online, streaming, and sliding window models
Least-squares regressions have
also been studied in various computational models, though the focus of these models are generally
not the (amortized) running time. Our algorithm uses techniques developed by Cohen et al. (2020),
where they study the regression problem in the online model, with the goal of maintaining a spec-
tral approximation of data matrix in the online stream. Their algorithm only needs to store O✏(d)
rows but the amortized running time is still ⌦(d2) (see Section 3.1 for detailed discussions). In
the streaming model, the main focus is the space complexity, and a direct application of random
Gaussian sketch or count sketch reduces the space complexity to eO(d2✏−1) and it is shown to be
tight (Clarkson & Woodruff, 2009). Recent work of (Braverman et al., 2020) studies regressions
and other numerical linear algebra tasks in the sliding window model, where data come in an online
stream and only the most recent updates form the underlying data set. The major focus of a sliding
window model is still the space complexity, and there is no amortized running time guarantee."
RELATED WORK,0.10819672131147541,"Disparity from online learning
Our work crucially differs from online learning literature (Hazan,
2019), in that the main bottleneck in online regret-minimization and bandit problems is information-
theoretic, whereas the challenge in our loss-minimization problem is purely computational. See
Appendix A for detailed discussions."
PROBLEM FORMULATION,0.11147540983606558,"2
PROBLEM FORMULATION"
PROBLEM FORMULATION,0.11475409836065574,"In a dynamic least-squares regression problem, initially, we are given a matrix A(0) 2 Rn0⇥d"
PROBLEM FORMULATION,0.1180327868852459,"together with a vector b(0) 2 Rn0. At the t-th step, a new data of form ((a(t))>, β(t)) 2 Rd ⇥R
arrives, and the goal is to maintain an ✏-approximate solution. A formal description is provided
below, where we assume n0 = d + 1 for simplicity (see Remark 2.3).
Deﬁnition 2.1 (Dynamic least-squares regression). Let d 2 N+ and ✏2 [0, 1) be two ﬁxed parame-
ters. We say an algorithm solves ✏-approximate dynamic least squares regression if"
PROBLEM FORMULATION,0.12131147540983607,• The data structure is given a matrix A(0) 2 R(d+1)⇥d and a vector b(0) 2 Rd+1 in the
PROBLEM FORMULATION,0.12459016393442623,preprocessing phase.
PROBLEM FORMULATION,0.12786885245901639,"• For each iteration t 2 [T], the algorithm receives updates a(t) 2 Rd and β(t) 2 R. Deﬁne"
PROBLEM FORMULATION,0.13114754098360656,"A(t) := [(A(t−1))>, a(t)]> 2 R(d+t+1)⇥d to be A(t−1) appended with a new row (a(t))>,
and b(t) := [(b(t−1))>, β(t)]> 2 Rd+t+1 to be b(t−1) appended with a new entry β(t).
After this update, the algorithm outputs an ✏-approximate solution x(t) 2 Rd:"
PROBLEM FORMULATION,0.13442622950819672,kA(t)x(t) −b(t)k2 (1 + ✏) min
PROBLEM FORMULATION,0.1377049180327869,x2Rd kA(t)x −b(t)k2.
PROBLEM FORMULATION,0.14098360655737704,Under review as a conference paper at ICLR 2022
PROBLEM FORMULATION,0.14426229508196722,"We write [0 : T] = {0, 1, . . . , T}, and for any t 2 [0 : T], we denote M(t) := [A(t), b(t)] 2
R(d+t+1)⇥(d+1). We make the following assumptions.
Assumption 2.2. We assume 1. Each data have bounded `2 norm, i.e., 8i 2 [T +d+1], the i-th row
of M(T ) satisﬁes kM(T )"
PROBLEM FORMULATION,0.14754098360655737,"i,⇤k2 D. 2. The initial matrix M(0) has full rank, and its smallest singular
value is bounded by σd+1(M(0)) ≥σ for some polynomially small σ 2 (0, 1).
Remark 2.3. We remark that these assumptions are essentially w.l.o.g. for the following reasons: 1.
Real world data inherently have bounded `2 norm, and in applications like machine learning, data
are often normalized. 2. We can assume the initial matrix A(0) has d+1 rows because brute-forcely
adding these d + 1 initial rows would only take O(d3) time, and this is within our desired total
running time of O✏(nnz(A(T )) + d3). 3. To satisfy the assumption that σd+1(M(0)) ≥σ for some
polynomially small σ, we could let the initial matrix M(0) = σ · Id+1. This is equivalent to adding
a small regularization term of σ · kxk2 and this incurs only a polynomially small additive error."
PROBLEM FORMULATION,0.15081967213114755,"3
DYNAMIC ✏-LSR DATA STRUCTURE"
PROBLEM FORMULATION,0.1540983606557377,"In this section, we provide an approximation algorithm for the dynamic least squares regression.
Notably, our algorithm maintains an ✏-approximate solution in near input sparsity time.
Theorem 3.1 (Data structure for dynamic least squares regression). Let ✏> 0, d, T 2 N. There ex-
ists a randomized algorithm for dynamic least-squares regression (Algorithm 1–4). With probability
at least 0.9, the algorithm maintains an ✏-approximation solution for all iterations t 2 [T] and the
total update time over T iterations is at most O """
PROBLEM FORMULATION,0.15737704918032788,✏−2 nnz(A(T )) log(T)+✏−6d3 log5(TD/σ) #
PROBLEM FORMULATION,0.16065573770491803,". Our
data structure uses at most O """
PROBLEM FORMULATION,0.16393442622950818,✏−2d2 · log2(TD/σ) #
PROBLEM FORMULATION,0.16721311475409836,"space.
Notations
We use a superscript (t) to denote the matrix/vector/scaler maintained by the data
structure at the end of the t-th iterations. In particular, the superscript (0) represents the variables
after the preprocessing step. For any matrix A 2 Rn⇥d, i 2 [n] we deﬁne its leverage score
⌧(A) 2 Rn as ⌧i(A) := a>"
PROBLEM FORMULATION,0.17049180327868851,"i (A>A)†ai. We deﬁne the generalized leverage score (same as Cohen
et al. (2015)) of A 2 Rn⇥d with respect to another matrix B 2 Rn0⇥d as : ⌧B"
PROBLEM FORMULATION,0.1737704918032787,i (A) := a>
PROBLEM FORMULATION,0.17704918032786884,"i (B>B)†ai.
For more properties of the leverage scores see Section B.1."
TECHNIQUE OVERVIEW,0.18032786885245902,"3.1
TECHNIQUE OVERVIEW
Our approach is formally described in Algorithm 1–4, we ﬁrst overview the ideas behind it."
TECHNIQUE OVERVIEW,0.18360655737704917,"From a high level view, our approach follows the online row sampling framework (Cohen et al.,
2015; 2020; Braverman et al., 2020): When a new row arrives, we sample and keep the new row
with probability proportional to the (approximated version of) online leverage score"
TECHNIQUE OVERVIEW,0.18688524590163935,⌧M(t−1)
TECHNIQUE OVERVIEW,0.1901639344262295,d+t+1 (M(t)) = (m(t))>((M(t−1))>M(t−1))−1m(t).
TECHNIQUE OVERVIEW,0.19344262295081968,"The sampled matrix is a spectral approximation to the true data matrix. We maintain an approximate
least-squares regression solution using this sampled matrix."
TECHNIQUE OVERVIEW,0.19672131147540983,"Naively computing the online leverage score takes O(d2) time. In order to accelerate this computa-
tion, we use two approximations:"
TECHNIQUE OVERVIEW,0.2,"1. Similar to Cohen et al. (2020), we compute the online leverage scores with respect to the"
TECHNIQUE OVERVIEW,0.20327868852459016,"sampled matrix instead of the true data matrix. However, this idea alone is still not enough
to achieve sub-quadratic time.
2. We use a JL-embedding3 trick (Spielman & Srivastava, 2011) to compress the size of the"
TECHNIQUE OVERVIEW,0.20655737704918034,"d⇥d matrix to ⇡✏−2 ⇥d. In this way, in each iteration it only takes O✏(d) time to compute
the approximate online leverage score."
TECHNIQUE OVERVIEW,0.2098360655737705,We further use an inductive analysis to bound the overall error (Lemma 3.4).
TECHNIQUE OVERVIEW,0.21311475409836064,"Finally, we adopt a similar strategy as Cohen et al. (2020) to prove that sampling according to the
approximate online leverage score still keeps at most O✏(d) sampled rows (Lemma 3.7). Whenever
a row is sampled, it takes O(d2) time to update the maintained matrices using Woodburry identity.
Hence, the amortized update time of the sampled rows is O✏(d3/T) = o(d) when T ≫d."
TECHNIQUE OVERVIEW,0.21639344262295082,"3Johnson-Lindenstrauss (JL) Lemma shows a way to embed high-dimensional vectors to low-dimensional
space while preserving the distances between the vectors. A rigorous statement is shown in Lemma B.4."
TECHNIQUE OVERVIEW,0.21967213114754097,Under review as a conference paper at ICLR 2022
TECHNIQUE OVERVIEW,0.22295081967213115,"Remark 3.2 (Difference from sketching-based solutions). Our approach crucially differs from the
sketching-based solutions, which do not provide any speedup over the direct application of Wood-
burry identity (O(d2) time per iteration). A sketching-based solution maintains a sketched matrix
SM 2 RO✏(d)⇥d, where S is a sketching matrix (e.g. SRHT (Ailon & Chazelle, 2006) or Count
Sketch (Clarkson & Woodruff, 2017)) that mixes the rows of M. When a new row of M arrives, at
least one row of the sketched matrix SM needs to be updated, in contrast to our sampling-based
approach where the sampled matrix is not updated in most of the iterations."
TECHNIQUE OVERVIEW,0.2262295081967213,"Implementation
We explain the detailed implementation of our algorithm. At the beginning of the
t-th iteration, a sampling matrix D(t−1) is derived based on the online leverage score, and the sub-
sampled matrix N(t−1) = D(t−1)M(t−1) maintains a spectral approximation on the column space
of M(t−1) = [A(t−1), b(t−1)]. Let s(t−1) denote the number of sampled rows. To obtain spectral
approximation, we maintain the approximate covariance matrices H(t−1) = ((N(t−1))>N(t−1))−1"
TECHNIQUE OVERVIEW,0.22950819672131148,"and B(t−1) = N(t−1)H(t−1). The online leverage score ⌧(t) of a new row m(t) = ((a(t))>, β(t))
can be approximated as kB(t−1)m(t−1)k2. To efﬁciently compute the leverage score of a new row,
we left-multiply by a JL matrix J(t−1) and maintain a proxy eB(t−1) = J(t−1)B(t−1), with the
guarantee that keB(t−1)mk2 ⇡kB(t−1)mk2 for any m 2 Rd+1 with high probability."
TECHNIQUE OVERVIEW,0.23278688524590163,"When a new row m(t) is inserted at the t-th iteration, we sample it via the approximate online
leverage score (Line 3 of SAMPLE).
We only perform update if the new row is sampled.
In
that case, we renew the JL matrix and perform a series of careful updates on all the variables
that we maintain (See UPDATEMEMBERS).
To obtain the ﬁnal solution x(t) 2 Rd, we solve
minx2Rd kD(t)A(t)x −D(t)b(t)k2, which has the closed-form solution of x(t) = G(t) · u(t)"
TECHNIQUE OVERVIEW,0.2360655737704918,"and can be efﬁciently maintained by taking G(t) = ((A(t))>(D(t))2A(t))−1 2 Rd⇥d and
u(t) = (A(t))>(D(t))2b(t)."
TECHNIQUE OVERVIEW,0.23934426229508196,"Algorithm 1 PREPROCESS (A, b, ✏, T)"
TECHNIQUE OVERVIEW,0.24262295081967214,"1: M  [A, b]
. M 2 R(d+1)⇥(d+1)"
TECHNIQUE OVERVIEW,0.2459016393442623,2: D  Id+1
TECHNIQUE OVERVIEW,0.24918032786885247,"# Spectral approximation
3: s  d + 1
4: N  D · M
. N 2 Rs⇥(d+1)"
TECHNIQUE OVERVIEW,0.25245901639344265,5: H  ((N)>N)−1 . H 2 R(d+1)⇥(d+1)
TECHNIQUE OVERVIEW,0.25573770491803277,"6: B  N · H
. B 2 Rs⇥(d+1)
# JL approximation
7: δ  O(1/T 2), k  O(✏−2 log(T/δ))
8: J  JL(s, ✏, δ, T) . JL matrix J 2 Rk⇥s"
TECHNIQUE OVERVIEW,0.25901639344262295,"9: eB  J · B
. eB 2 Rk⇥(d+1)
# Maintain solution
10: G  (A>D>DA)−1
. G 2 Rd⇥d"
TECHNIQUE OVERVIEW,0.26229508196721313,"11: u  A>D2b
. u 2 Rd"
TECHNIQUE OVERVIEW,0.26557377049180325,"12: x  G · u
. x 2 Rd"
TECHNIQUE OVERVIEW,0.26885245901639343,"Algorithm 2 UPDATE (a, β)"
TECHNIQUE OVERVIEW,0.2721311475409836,"1: m  [a>, β]>
. m 2 Rd+1"
TECHNIQUE OVERVIEW,0.2754098360655738,"2: ⌫ SAMPLE(m)
. ⌫2 R 3: D "
TECHNIQUE OVERVIEW,0.2786885245901639,"D
0
0
⌫ %"
TECHNIQUE OVERVIEW,0.2819672131147541,"4: if ⌫6= 0 then UPDATEMEMBERS(m)
5: return x"
TECHNIQUE OVERVIEW,0.28524590163934427,Algorithm 3 SAMPLE (m)
TECHNIQUE OVERVIEW,0.28852459016393445,1: ⌧ keB · mk2
TECHNIQUE OVERVIEW,0.29180327868852457,"2
2: p  min{3(1 + ✏)2✏−2⌧log(1/δ), 1}
3: ⌫ 1/pp with probability p, and ⌫ 0"
TECHNIQUE OVERVIEW,0.29508196721311475,otherwise
TECHNIQUE OVERVIEW,0.2983606557377049,Algorithm 4 UPDATEMEMBERS (m)
TECHNIQUE OVERVIEW,0.3016393442622951,"# Update spectral approximation
1: s  s + 1
2: ∆H  −Hmm>H/p"
TECHNIQUE OVERVIEW,0.30491803278688523,"1+m>Hm/p
3: H  H + ∆H
4: B  [(B + N · ∆H)>, H · m/pp]>"
TECHNIQUE OVERVIEW,0.3081967213114754,"5: N  [N>, m/pp]>"
TECHNIQUE OVERVIEW,0.3114754098360656,"# Update JL approximation
6: J  JL(s, ✏, δ, T)
7: eB  J · B"
TECHNIQUE OVERVIEW,0.31475409836065577,# Update solution
TECHNIQUE OVERVIEW,0.3180327868852459,8: G  G −Gaa>G/p
TECHNIQUE OVERVIEW,0.32131147540983607,"1+a>Ga/p
9: u  u + β · a/p
10: x  G · u"
TECHNIQUE OVERVIEW,0.32459016393442625,"We outline the proof of Theorem 3.1, and defer the detailed proof to Appendix C due to space limits."
TECHNIQUE OVERVIEW,0.32786885245901637,Under review as a conference paper at ICLR 2022
CORRECTNESS,0.33114754098360655,"3.2
CORRECTNESS
We show the correctness of our algorithm and prove it maintains an ✏-approximate solution for all
iterations with high probability. We start with closed-form formulas for all the variables we maintain."
CORRECTNESS,0.3344262295081967,"Lemma 3.3 (Closed-form formulas). At the t-th iteration of Algorithm 1 – 4, we have"
CORRECTNESS,0.3377049180327869,"1. M(t) = [A(t), b(t)] 2 R(d+t+1)⇥(d+1)."
CORRECTNESS,0.34098360655737703,2. D(t) 2 R(d+t+1)⇥(d+t+1) is a diagonal matrix with s(t) non-zero entries.
CORRECTNESS,0.3442622950819672,"3. N(t) = (D(t)M(t))S(t),⇤2 Rs(t)⇥(d+1), where S(t) ⇢[d + t + 1] is deﬁned as the set of"
CORRECTNESS,0.3475409836065574,non-zero entries of D(t).
CORRECTNESS,0.35081967213114756,"4. H(t) = """
CORRECTNESS,0.3540983606557377,(N(t))>N(t)#−1 2 R(d+1)⇥(d+1).
CORRECTNESS,0.35737704918032787,5. B(t) = N(t)H(t) 2 Rs(t)⇥(d+1).
CORRECTNESS,0.36065573770491804,"6. eB(t) = J(t) · B(t) 2 Rk⇥(d+1), where k = O(✏−2 log(T/δ))."
CORRECTNESS,0.3639344262295082,"7. G(t) = """
CORRECTNESS,0.36721311475409835,(A(t))>(D(t))2A(t)#−1 2 Rd⇥d.
CORRECTNESS,0.3704918032786885,8. u(t) = (A(t))>(D(t))2b(t) 2 Rd.
CORRECTNESS,0.3737704918032787,"9. x(t) = """
CORRECTNESS,0.3770491803278688,(A(t))>(D(t))2A(t)#−1 · (A(t))>(D(t))2b(t) 2 Rd.
CORRECTNESS,0.380327868852459,"The following lemma is key for our correctness analysis. It shows that we maintain a good approxi-
mation on online leverage scores and a spectral approximation of M(t) throughout all iterations.
Lemma 3.4 (Spectral approximation via leverage score maintenance). With probability at least
1 −2Tδ,"
CORRECTNESS,0.3836065573770492,(1 −✏)2⌧M(t−1)
CORRECTNESS,0.38688524590163936,d+t+1 (M(t)) ⌧(t) (1 + ✏)2⌧M(t−1)
CORRECTNESS,0.3901639344262295,"d+t+1 (M(t)), 8t 2 [T],
(2)
and"
CORRECTNESS,0.39344262295081966,"(M(t))>(D(t))2M(t) ⇡✏(M(t))>M(t), 8t 2 [0 : T].
(3)"
CORRECTNESS,0.39672131147540984,"Proof Sketch. We prove by induction and show that with probability 1 −2tδ, Eq. (3) holds for all
t0 2 [0 : t] and Eq. (2) holds for all t0 2 [t]. The base case t = 0 holds trivially, as D(0) = Id+1,
and therefore, (M(0))>(D(0))2M(0) = (M(0))>M(t). Given the induction hypothesis upon t −1,
we proceed in the following three steps."
CORRECTNESS,0.4,• We ﬁrst use the induction hypothesis to prove that kB(t−1)m(t)k is a good estimate on the
CORRECTNESS,0.40327868852459015,"online leverage score, that is"
CORRECTNESS,0.4065573770491803,(1 −✏)⌧M(t−1)
CORRECTNESS,0.4098360655737705,d+t+1 (M(t)) kB(t−1) · m(t)k2
CORRECTNESS,0.4131147540983607,2 (1 + ✏)⌧M(t−1)
CORRECTNESS,0.4163934426229508,d+t+1 (M(t)).
CORRECTNESS,0.419672131147541,"• We then use the JL lemma (Lemma B.4) to show that with probability 1 −δ, the sketched"
CORRECTNESS,0.42295081967213116,covariance matrix keB(t−1)m(t)k returns good estimation kB(t−1)m(t)k. That is
CORRECTNESS,0.4262295081967213,(1 −✏)2 · ⌧M(t−1)
CORRECTNESS,0.42950819672131146,d+t+1 (M(t)) keB(t−1) · m(t)k2
CORRECTNESS,0.43278688524590164,2 (1 + ✏)2 · ⌧M(t−1)
CORRECTNESS,0.4360655737704918,"d+t+1 (M(t)).
(4)"
CORRECTNESS,0.43934426229508194,"• Finally, we wrap up the proof by proving the second part of induction. In particular, we"
CORRECTNESS,0.4426229508196721,"show that conditioned on Eq. (4) holds, we have"
CORRECTNESS,0.4459016393442623,(M(t))>(D(t))2M(t) ⇡✏(M(t))>M(t).
CORRECTNESS,0.4491803278688525,The proof then follows by an union bound over failure events.
CORRECTNESS,0.4524590163934426,"It is well known that spectral approximations of (M(t))>M(t) give approximate solutions to least
squares regressions (Woodruff, 2014), so we have proved the correctness of our algorithm.
Lemma 3.5 (Correctness of Algorithm 1–4). With probability at least 1−O(1/T), in each iteration,
UPDATE of Algorithm 2 outputs a vector x(t) 2 Rd such that"
CORRECTNESS,0.4557377049180328,kA(t)x(t) −b(t)k2 (1 + ✏) min
CORRECTNESS,0.45901639344262296,x2Rd kA(t)x −b(t)k2.
CORRECTNESS,0.46229508196721314,Under review as a conference paper at ICLR 2022
TIME ANALYSIS,0.46557377049180326,"3.3
TIME ANALYSIS"
TIME ANALYSIS,0.46885245901639344,"Next, we bound the overall update time of our algorithm. We ﬁrst compute the worst case update
time of Algorithm 2. When ⌫(t) = 0, i.e., the t-th row is not sampled, the UPDATE procedure
only needs to compute the approximate leverage score ⌧(t) (SAMPLE, Algorithm 3), and it takes
O(k · nnz(m(t))) time. When ⌫(t) 6= 0, i.e., the t-th row is sampled, the UPDATE procedure makes
a call to UPDATEMEMBERS (Algorithm 4), and it takes O(k · s(t) · d) time. Plugging in the value of
k, we have the following lemma."
TIME ANALYSIS,0.4721311475409836,"Lemma 3.6 (Worst case update time). At the t-th iteration of the UPDATE procedure (Algorithm 2),"
TIME ANALYSIS,0.47540983606557374,"• If ⌫(t) = 0, then UPDATE takes O """
TIME ANALYSIS,0.4786885245901639,✏−2 log(T/δ) · nnz(a(t)) # time.
TIME ANALYSIS,0.4819672131147541,"• If ⌫(t) 6= 0, then UPDATE takes O """
TIME ANALYSIS,0.4852459016393443,✏−2s(t)d log(T/δ) # time.
TIME ANALYSIS,0.4885245901639344,"To bound the amortized update time, we need to bound the total number of sampled rows, and this
is closely related to the sum of online leverage scores. Such an upper bound was already established
by Cohen et al. (2020), here we present a slightly generalized version of it."
TIME ANALYSIS,0.4918032786885246,"Lemma 3.7 (Sum of online leverage scores, generalization of Theorem 2.2 of (Cohen et al., 2020)).
If the matrix M(T ) satisfy Assumption 2.2, then T
X t=1"
TIME ANALYSIS,0.49508196721311476,⌧M(t−1)
TIME ANALYSIS,0.49836065573770494,d+t+1 (M(t)) O(d log(TD/σ)).
TIME ANALYSIS,0.5016393442622951,Now we are ready to bound the amortized update time of our algorithm.
TIME ANALYSIS,0.5049180327868853,"Lemma 3.8 (Amortized update time). With probability at least 0.99, the total running time of
UPDATE over T iterations is at most O """
TIME ANALYSIS,0.5081967213114754,✏−2 nnz(A(T )) log(T) + ✏−6d3 log5(TD/σ) # .
TIME ANALYSIS,0.5114754098360655,"Proof Sketch. In this proof sketch we simplify the second term as d3 · poly(✏−1 log(TD/σ)). The
ﬁrst term comes from the computation cost of querying leverage score, which takes O """
TIME ANALYSIS,0.5147540983606558,"✏−2 log(T/δ)·
nnz(a(t)) #"
TIME ANALYSIS,0.5180327868852459,"time in the t-th iteration even if the t-th row is not sampled. The second term bounds the
total update time for the sampled rows:"
TIME ANALYSIS,0.521311475409836,"• From Lemma 3.4 and Lemma 3.7, with high probability the sum of the approximate online"
TIME ANALYSIS,0.5245901639344263,leverage scores ⌧(t) are bounded by O(d log(TD/σ)).
TIME ANALYSIS,0.5278688524590164,"• Using Markov inequality, the total number of sampled rows is bounded by"
TIME ANALYSIS,0.5311475409836065,"s(T ) = O( T
X i=1"
TIME ANALYSIS,0.5344262295081967,"p(t)) = O(✏−2 · log(1/δ) · T
X t=1"
TIME ANALYSIS,0.5377049180327869,"⌧(t)) O """
TIME ANALYSIS,0.5409836065573771,d · poly(✏−1 log(TD/σ)) # .
TIME ANALYSIS,0.5442622950819672,"• Since there are s(T ) sampled rows, and for each sampled row we update data structure"
TIME ANALYSIS,0.5475409836065573,"members in O """
TIME ANALYSIS,0.5508196721311476,s(T )d · poly(✏−1 log(TD/σ)) #
TIME ANALYSIS,0.5540983606557377,"time, the total update time for sampled
rows is"
TIME ANALYSIS,0.5573770491803278,"s(T ) · O """
TIME ANALYSIS,0.5606557377049181,"s(T )d · poly(✏−1 log(TD/σ)) # = O """
TIME ANALYSIS,0.5639344262295082,d3 · poly(✏−1 log(TD/σ)) # .
HARDNESS RESULT,0.5672131147540984,"4
HARDNESS RESULT"
HARDNESS RESULT,0.5704918032786885,"We prove a ⌦(d2−o(1)) amortized time lower bound for dynamic least squares regression with high
precision, assuming the OMv conjecture. The OMv conjecture was originally proposed by Hen-
zinger et al. (2015), and it is widely accepted in the theoretical computer science community."
HARDNESS RESULT,0.5737704918032787,"Conjecture 4.1 (OMv conjecture, (Henzinger et al., 2015)). Let d 2 N, T = poly(d). Let γ > 0
be any constant. B 2 {0, 1}d⇥d is a Boolean matrix. 8t 2 [T], a Boolean vector z(t) 2 {0, 1}d
is revealed at the t-th step. We say an algorithm solves the OMv problem if it returns the Boolean
matrix-vector product Bz(t) 2 Rd at every time step. The conjectures states that there is no al-
gorithm that solves the OMv problem using poly(d) preprocessing time and O(d2−γ) amortized
running time, and has an error probability 1/3."
HARDNESS RESULT,0.5770491803278689,Under review as a conference paper at ICLR 2022
HARDNESS RESULT,0.580327868852459,"The results in this section are all under the Word RAM model where the word size w = O(log d).
Our main result is formally stated below."
HARDNESS RESULT,0.5836065573770491,"Theorem 4.2 (Hardness of dynamic-least squares regression with high precision). Let d 2 N, T =
poly(d), ✏=
1
d8T 2 = 1/ poly(d), and let γ > 0 be any constant. Assuming the OMv conjecture is
true, any dynamic algorithm that maintains an ✏-approximate solution of the least squares regression
requires at least ⌦(d2−γ) amortized time per update."
HARDNESS RESULT,0.5868852459016394,"Our lower bound is proved by ﬁrst reducing the standard OMv conjecture for Boolean matrices to
OMv-hardness for well-conditioned positive semideﬁnite (PSD) matrices over real numbers. Then
we use this new OMv-hardness result to prove our lower bound for dynamic least squares regression.
We only provide a proof sketch here and detailed proof are delayed to Section D."
HARDNESS RESULT,0.5901639344262295,"OMv-hardness for well-conditioned PSD matrix
The OMv conjecture asserts the hardness of
solving online Boolean matrix-vector product exactly. We extend it to solving online real-valued
matrix-vector product for well-conditioned PSD matrices, while allowing polynomially small error."
HARDNESS RESULT,0.5934426229508196,"Lemma 4.3 (Hardness of approximate real-valued OMv). Let d 2 N, T = poly(d). Let γ > 0 be
any constant. Let H 2 Rd⇥d be a symmetric matrix whose eigenvalues satisfy 1 λd(H) · · · 
λ1(H) 3. For any t 2 [T], z(t) 2 Rd is revealed at the t-th step, and kz(t)k2 1. Assuming the
OMv conjecture is true, then there is no algorithm with poly(d) preprocessing time and O(d2−γ)
amortized running time that can return an O(1/d2)-approximate answer to Hz(t) for all t, i.e., a
vector y(t) 2 Rd s.t. ky(t) −Hz(t)k2 ✏, and has an error probability 1/3."
HARDNESS RESULT,0.5967213114754099,"Proof Sketch. Given a Boolean matrix B 2 {0, 1}d⇥d in the OMv conjecture, we construct a PSD"
HARDNESS RESULT,0.6,matrix H = 
ID,0.6032786885245902,"2Id
1
dB
1
dB>
2Id %"
ID,0.6065573770491803,2 R2d⇥2d. We note that H is symmetric and 1 λd(H) λ1(H) 3.
ID,0.6098360655737705,"Given a binary OMv query vector z(t), we construct z(t) = (0d, z(t)) 2 R2d. Since H has a constant
condition number, we can prove that rounding an ✏⇠1/d2-approximate answer by ⇡✏H · z(t) still
gives the correct binary answer to Bz(t)."
ID,0.6131147540983607,"Reducing OMv to dynamic least-squares regression
We next wrap up the proof of Theorem 4.2
by reducing OMv to dynamic ✏-LSR."
ID,0.6163934426229508,Proof Sketch of Theorem 4.2. Given a PSD matrix H and a sequence of query {Hz(t)}T
ID,0.6196721311475409,"t=1 of the
problem in Lemma 4.3, we reduce it to a dynamic ✏-LSR, where the initial A is such that A>A =
H−1 (this preprocessing step of the reduction takes ⇠d! time), and the label is 0 for the initial
d data. For each t 2 [T], the incoming row a(t) is a small scaled version of z(t), i.e, a(t) = 1
d2p"
ID,0.6229508196721312,"T · z(t) 2 Rd, and the label is 1. Let x(t)"
ID,0.6262295081967213,"?
be the optimal solution at the t-th step, H(t) :="
ID,0.6295081967213115,"((A(t))>A(t))−2, the reduction is complete via the following three steps:"
ID,0.6327868852459017,• Step 1. x(t) and x(t)
ID,0.6360655737704918,"?
are close, i.e., x(t) = x(t)"
ID,0.639344262295082,"? ± O(
1
d4p T )."
ID,0.6426229508196721,"• Step 2. x(t) −x(t−1) recovers H(t−1)a(t), i.e., x(t) −x(t−1) = H(t−1)a(t) ± O(
1
d4p T )."
ID,0.6459016393442623,"• Step 3. H(t−1)a(t) is close to Ha(t), i.e., H(t−1)a(t) = Ha(t) ± O(
1
d6p T )."
ID,0.6491803278688525,"In particular, let y(t) = d2p"
ID,0.6524590163934426,"T(x(t) −x(t−1)), Step 2 and 3 directly implies ky −Hz(t)k2 
O(1/d2). This completes the proof."
EXPERIMENTS,0.6557377049180327,"5
EXPERIMENTS"
EXPERIMENTS,0.659016393442623,"Our method is most suitable for data distributions that are non-uniform. Indeed, if the data has
low coherence (they are all similar to each other), then the naive uniform sampling is as good as
leverage score sampling. We perform empirical evaluations on our algorithm over both synthetic
and real-world datasets."
EXPERIMENTS,0.6622950819672131,"Synthetic dataset
We follow the empirical study of (Dobriban & Liu, 2019) and generate data
from the elliptical model. In this model a(t) = w(t)⌃z(t), where z(t) ⇠N(0, Id) is a random
Gaussian vector, ⌃2 Rd⇥d is a PSD matrix, and w(t) is a scaler. The label is generated as b(t) ="
EXPERIMENTS,0.6655737704918033,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.6688524590163935,"ha(t), x?i + w(t)⇠(t), where x? 2 Rd is a hidden vector and ⇠⇠N(0, 1) is standard Gaussian
noise. This model has a long history in multivariate statistics, see e.g. (Martin & Maes, 1979). In
our experiments, we set ⌃= Id for simplicity. In order to make the dataset non-trivial, we set w(t)"
EXPERIMENTS,0.6721311475409836,to be large (= p
EXPERIMENTS,0.6754098360655738,"T) for a few (= d/10) iterations, and small (= 1) for the rest of the iterations. We
set T = 400000 and d = 500."
EXPERIMENTS,0.6786885245901639,"Real-world dataset
We use the VirusShare dataset from the UCI Machine Learning Repository4.
We select this dataset because it has a large number of features and data points, and has low errors
when ﬁtted by a linear model. The dataset is collected from Nov 2010 to Jul 2014 by VirusShare
(an online platform for malware detection). It has T = 107888 data points and d = 482 features."
EXPERIMENTS,0.6819672131147541,"Baseline algorithms
We compare with three baseline methods. 1. Kalman’s approach makes use
of Woodburry identiy and gives an exact solution. 2. The uniform sampling approach samples new
rows uniformly at random. 3. The row sampling approach samples new rows according to the exact
online leverage scores. (Cohen et al., 2020)"
EXPERIMENTS,0.6852459016393443,"Our experiments are executed on an Apple M1 CPU with codes written in MATLAB. We repeat all
experiments for at least 5 times and take the mean. On both datasets, we initiate the model based
on the ﬁrst 10% of the data. The experiment results are formally presented in Figure 1 and more
details can be found in Appendix E. Our algorithm consistently outperforms baseline methods: Our
algorithm runs faster when achieving comparable error rates."
EXPERIMENTS,0.6885245901639344,"(a) Synthetic dataset
(b) VirusShare"
EXPERIMENTS,0.6918032786885245,"Figure 1: Experiment results. The x-axis shows the running time (unit: seconds), and the y-axis
shows the relative error (err/errstd −1), where err is the error of the particular approach, and errstd
is the error of the static Normal equation. The y-axis is on a log scale. For uniform sampling, we
take sampling probability p = 0.05, 0.1, 0.2, 0.5. For row sampling and our algorithm, we take the
error parameter ✏= 0.1, 0.2, 0.5, 1. Kalman’s approach has a relative error of 0."
CONCLUSION,0.6950819672131148,"6
CONCLUSION"
CONCLUSION,0.6983606557377049,"We provide the ﬁrst practical and provably fast data structure for dynamic least-squares regression,
obtaining nearly tight upper and lower bounds for this fundamental problem. On the algorithmic
side, we design an ✏-approximation dynamic algorithm whose total update time almost matches the
input sparsity of the (online) matrix. On the lower bound side, we prove that it is impossible to
maintain an exact (or even high-accuracy) solution with ⌧d2−o(1) amortized update time under the
OMv conjecture. As such, this result exhibits the ﬁrst separation between the static and the dynamic
LSR problems."
CONCLUSION,0.7016393442622951,"Our paper sets forth several interesting future directions. On the theoretical side, a very interest-
ing question is whether it is possible to reduce the additive term d3 of our algorithm to matrix-
multiplication time d!? A second open problem—of interest in both theory and practice—is whether
it is possible to achieve input-sparsity amortized update time in the fully dynamic setting, i.e., when
allowing both addition and deletion of data rows? Finally, it would be interesting to ﬁnd connections
between dynamic least-squares regression and incremental training of more complicated models,
such as dynamic Kernel-ridge regression and to deep neural networks."
CONCLUSION,0.7049180327868853,4https://archive.ics.uci.edu/ml/datasets.php
CONCLUSION,0.7081967213114754,Under review as a conference paper at ICLR 2022
REFERENCES,0.7114754098360656,REFERENCES
REFERENCES,0.7147540983606557,"Thomas D Ahle, Michael Kapralov, Jakob BT Knudsen, Rasmus Pagh, Ameya Velingker, David P"
REFERENCES,0.7180327868852459,"Woodruff, and Amir Zandieh. Oblivious sketching of high-degree polynomial kernels. In Pro-
ceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms, pp. 141–160.
SIAM, 2020."
REFERENCES,0.7213114754098361,Nir Ailon and Bernard Chazelle. Approximate nearest neighbors and the fast johnson-lindenstrauss
REFERENCES,0.7245901639344262,"transform. In Proceedings of the thirty-eighth annual ACM symposium on Theory of computing,
pp. 557–563, 2006."
REFERENCES,0.7278688524590164,Josh Alman and Virginia Vassilevska Williams. A reﬁned laser method and faster matrix multipli-
REFERENCES,0.7311475409836066,"cation. In Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA), pp.
522–539. SIAM, 2021."
REFERENCES,0.7344262295081967,"Haim Avron, Kenneth L Clarkson, and David P Woodruff. Faster kernel ridge regression using"
REFERENCES,0.7377049180327869,"sketching and preconditioning. SIAM Journal on Matrix Analysis and Applications, 38(4):1116–
1138, 2017."
REFERENCES,0.740983606557377,"Christos Boutsidis, David P Woodruff, and Peilin Zhong. Optimal principal component analysis in"
REFERENCES,0.7442622950819672,"distributed and streaming models. In Proceedings of the forty-eighth annual ACM symposium on
Theory of Computing, pp. 236–249, 2016."
REFERENCES,0.7475409836065574,"van den Jan Brand, Yin-Tat Lee, Danupon Nanongkai, Richard Peng, Thatchaphol Saranurak, Aaron"
REFERENCES,0.7508196721311475,"Sidford, Zhao Song, and Di Wang. Bipartite matching in nearly-linear time on moderately dense
graphs. In 2020 IEEE 61st Annual Symposium on Foundations of Computer Science (FOCS), pp.
919–930. IEEE, 2020a."
REFERENCES,0.7540983606557377,"van den Jan Brand, Yin Tat Lee, Aaron Sidford, and Zhao Song. Solving tall dense linear programs"
REFERENCES,0.7573770491803279,"in nearly linear time. In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of
Computing (STOC), pp. 775–788, 2020b."
REFERENCES,0.760655737704918,"van den Jan Brand, Yin Tat Lee, Yang P Liu, Thatchaphol Saranurak, Aaron Sidford, Zhao Song, and"
REFERENCES,0.7639344262295082,"Di Wang. Minimum cost ﬂows, mdps, and `1-regression in nearly linear time for dense instances.
In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing (STOC),
pp. 859–869, 2021a."
REFERENCES,0.7672131147540984,"van den Jan Brand, Binghui Peng, Zhao Song, and Omri Weinstein. Training (overparametrized)"
REFERENCES,0.7704918032786885,"neural networks in near-linear time. In 12th Innovations in Theoretical Computer Science Con-
ference (ITCS 2021), 2021b."
REFERENCES,0.7737704918032787,"Vladimir Braverman, Petros Drineas, Cameron Musco, Christopher Musco, Jalaj Upadhyay, David P"
REFERENCES,0.7770491803278688,"Woodruff, and Samson Zhou.
Near optimal linear algebra in the online and sliding window
models. In 2020 IEEE 61st Annual Symposium on Foundations of Computer Science (FOCS), pp.
517–528. IEEE, 2020."
REFERENCES,0.780327868852459,Sébastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends in
REFERENCES,0.7836065573770492,"Machine Learning, 8(3-4):231–357, 2015."
REFERENCES,0.7868852459016393,"Nadiia Chepurko, Kenneth L Clarkson, Praneeth Kacham, and David P Woodruff.
Near-
optimal algorithms for linear algebra in the current matrix multiplication time. arXiv preprint
arXiv:2107.08090, 2021."
REFERENCES,0.7901639344262295,"Charles K. Chui. Estimation, control, and the discrete kalman ﬁlter (donald e. calin). SIAM Re-"
REFERENCES,0.7934426229508197,"view, 32(3):493–494, 1990. doi: 10.1137/1032097. URL https://doi.org/10.1137/
1032097."
REFERENCES,0.7967213114754098,Kenneth L Clarkson and David P Woodruff. Numerical linear algebra in the streaming model. In
REFERENCES,0.8,"Proceedings of the forty-ﬁrst annual ACM symposium on Theory of computing, pp. 205–214,
2009."
REFERENCES,0.8032786885245902,Kenneth L Clarkson and David P Woodruff. Low-rank approximation and regression in input spar-
REFERENCES,0.8065573770491803,"sity time. Journal of the ACM (JACM), 63(6):1–45, 2017."
REFERENCES,0.8098360655737705,Under review as a conference paper at ICLR 2022
REFERENCES,0.8131147540983606,"Michael B Cohen, Yin Tat Lee, Cameron Musco, Christopher Musco, Richard Peng, and Aaron"
REFERENCES,0.8163934426229508,"Sidford. Uniform sampling for matrix approximation. In Proceedings of the 2015 Conference on
Innovations in Theoretical Computer Science (ITCS), pp. 181–190. ACM, 2015."
REFERENCES,0.819672131147541,"Michael B Cohen, Cameron Musco, and Jakub Pachocki. Online row sampling. Theory OF Com-"
REFERENCES,0.8229508196721311,"puting, 16(15):1–25, 2020."
REFERENCES,0.8262295081967214,Edgar Dobriban and Sifan Liu. Asymptotics for sketching in least squares. In Proceedings of the
REFERENCES,0.8295081967213115,"33rd International Conference on Neural Information Processing Systems, pp. 3675–3685, 2019."
REFERENCES,0.8327868852459016,"Petros Drineas, Michael W. Mahoney, and S. Muthukrishnan. Sampling algorithms for l 2 regression"
REFERENCES,0.8360655737704918,"and applications. In Proceedings of the seventeenth annual ACM-SIAM symposium on Discrete
algorithm (SODA), pp. 1127–1136, 2006a."
REFERENCES,0.839344262295082,"Petros Drineas, Michael W. Mahoney, and S. Muthukrishnan.
Subspace sampling and relative-
error matrix approximation: Column-based methods. In Approximation, Randomization, and
Combinatorial Optimization. (APPROX-RANDOM), pp. 316–326, 2006b."
REFERENCES,0.8426229508196721,"Petros Drineas, Michael W. Mahoney, and S. Muthukrishnan.
Subspace sampling and relative-
error matrix approximation: Column-row-based methods. In European Symposium on Algorithms
(ESA), pp. 304–314, 2006c."
REFERENCES,0.8459016393442623,"Trevor Hastie, Jerome H. Friedman, and Robert Tibshirani. The Elements of Statistical Learning:"
REFERENCES,0.8491803278688524,"Data Mining, Inference, and Prediction. Springer Series in Statistics. Springer, 2001. ISBN 978-
1-4899-0519-2.
doi: 10.1007/978-0-387-21606-5.
URL https://doi.org/10.1007/
978-0-387-21606-5."
REFERENCES,0.8524590163934426,"Elad Hazan. Introduction to online convex optimization. arXiv preprint arXiv:1909.05207, 2019."
REFERENCES,0.8557377049180328,"Monika Henzinger, Sebastian Krinninger, Danupon Nanongkai, and Thatchaphol Saranurak. Unify-"
REFERENCES,0.8590163934426229,"ing and strengthening hardness for dynamic problems via the online matrix-vector multiplication
conjecture. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing,
pp. 21–30, 2015."
REFERENCES,0.8622950819672132,"Shunhua Jiang, Zhao Song, Omri Weinstein, and Hengjie Zhang. A faster algorithm for solving"
REFERENCES,0.8655737704918033,"general lps. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing
(STOC), pp. 823–832, 2021."
REFERENCES,0.8688524590163934,William B Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings into a hilbert space.
REFERENCES,0.8721311475409836,"Contemporary mathematics, 26(189-206):1, 1984."
REFERENCES,0.8754098360655738,Rudolph Emil Kalman. A new approach to linear ﬁltering and prediction problems. Journal of Basic
REFERENCES,0.8786885245901639,"Engineering, 82(1):35–45, 1960."
REFERENCES,0.8819672131147541,François Le Gall. Powers of tensors and fast matrix multiplication. In Proceedings of the 39th
REFERENCES,0.8852459016393442,"international symposium on symbolic and algebraic computation, pp. 296–303, 2014."
REFERENCES,0.8885245901639345,"Yin Tat Lee and Aaron Sidford.
Path ﬁnding methods for linear programming: Solving linear
programs in eO( p"
REFERENCES,0.8918032786885246,"rank) iterations and faster algorithms for maximum ﬂow. In 2014 IEEE 55th
Annual Symposium on Foundations of Computer Science, pp. 424–433. IEEE, 2014."
REFERENCES,0.8950819672131147,"Yin Tat Lee, Zhao Song, and Qiuyi Zhang. Solving empirical risk minimization in the current matrix"
REFERENCES,0.898360655737705,"multiplication time. In Annual Conference on Learning Theory (COLT), 2019."
REFERENCES,0.9016393442622951,"Edo Liberty, Zohar Karnin, Bing Xiang, Laurence Rouesnel, Baris Coskun, Ramesh Nallapati, Julio"
REFERENCES,0.9049180327868852,"Delgado, Amir Sadoughi, Yury Astashonok, Piali Das, et al. Elastic machine learning algorithms
in amazon sagemaker. In Proceedings of the 2020 ACM SIGMOD International Conference on
Management of Data, pp. 731–737, 2020."
REFERENCES,0.9081967213114754,"Aleksander Madry. Navigating central path with electrical ﬂows: From ﬂows to matchings, and"
REFERENCES,0.9114754098360656,"back. In 2013 IEEE 54th Annual Symposium on Foundations of Computer Science (FOCS), pp.
253–262. IEEE, 2013."
REFERENCES,0.9147540983606557,"Nick Martin and Hermine Maes. Multivariate analysis. Academic press London, 1979."
REFERENCES,0.9180327868852459,Under review as a conference paper at ICLR 2022
REFERENCES,0.921311475409836,Xiangrui Meng and Michael W Mahoney. Low-distortion subspace embeddings in input-sparsity
REFERENCES,0.9245901639344263,"time and applications to robust linear regression. In Proceedings of the forty-ﬁfth annual ACM
symposium on Theory of computing, pp. 91–100, 2013."
REFERENCES,0.9278688524590164,Jelani Nelson and Huy L Nguyên. Osnap: Faster numerical linear algebra algorithms via sparser
REFERENCES,0.9311475409836065,"subspace embeddings. In 2013 ieee 54th annual symposium on foundations of computer science,
pp. 117–126. IEEE, 2013."
REFERENCES,0.9344262295081968,"Rasmus Pagh.
Compressed matrix multiplication.
ACM Transactions on Computation Theory
(TOCT), 5(3):1–17, 2013."
REFERENCES,0.9377049180327869,"German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual"
REFERENCES,0.940983606557377,"lifelong learning with neural networks: A review. Neural Networks, 113:54–71, 2019."
REFERENCES,0.9442622950819672,"R. L. Plackett.
Some theorems in least squares.
Biometrika, 37(1/2):149–157, 1950.
ISSN
00063444. URL http://www.jstor.org/stable/2332158."
REFERENCES,0.9475409836065574,Lawrence R Rabiner and Bernard Gold. Theory and application of digital signal processing. Engle-
REFERENCES,0.9508196721311475,"wood Cliffs: Prentice-Hall, 1975."
REFERENCES,0.9540983606557377,"Ilya Razenshteyn, Zhao Song, and David P Woodruff. Weighted low rank approximations with"
REFERENCES,0.9573770491803278,"provable guarantees. In Proceedings of the forty-eighth annual ACM symposium on Theory of
Computing, pp. 250–263, 2016."
REFERENCES,0.9606557377049181,Vladimir Rokhlin and Mark Tygert. A fast randomized algorithm for overdetermined linear least-
REFERENCES,0.9639344262295082,"squares regression. Proceedings of the National Academy of Sciences, 105(36):13212–13217,
2008."
REFERENCES,0.9672131147540983,Tamas Sarlos. Improved approximation algorithms for large matrices via random projections. In
REFERENCES,0.9704918032786886,"2006 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS’06), pp. 143–
152. IEEE, 2006."
REFERENCES,0.9737704918032787,"Daniel A Spielman and Nikhil Srivastava.
Graph sparsiﬁcation by effective resistances.
SIAM
Journal on Computing, 40(6):1913–1926, 2011."
REFERENCES,0.9770491803278688,"Daniel A Spielman and Shang-Hua Teng. Nearly-linear time algorithms for graph partitioning,"
REFERENCES,0.980327868852459,"graph sparsiﬁcation, and solving linear systems. In Proceedings of the thirty-sixth annual ACM
symposium on Theory of computing, pp. 81–90, 2004."
REFERENCES,0.9836065573770492,"Stephen M. Stigler.
Gauss and the Invention of Least Squares.
The Annals of Statistics, 9(3):
465–474, 1981. doi: 10.1214/aos/1176345451. URL https://doi.org/10.1214/aos/
1176345451."
REFERENCES,0.9868852459016394,"Volker Strassen. Gaussian elimination is not optimal. Numerische mathematik, 13(4):354–356, 1969."
REFERENCES,0.9901639344262295,"David Woodruff.
A very sketchy talk (invited talk).
In 48th International Colloquium on Au-
tomata, Languages, and Programming (ICALP 2021). Schloss Dagstuhl-Leibniz-Zentrum fuer
Informatik, 2021."
REFERENCES,0.9934426229508196,David P. Woodruff. Sketching as a tool for numerical linear algebra. Foundations and Trends in
REFERENCES,0.9967213114754099,"Theoretical Computer Science, 10(1-2):1–157, 2014."
