Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003968253968253968,"Multimodal learning has achieved great successes in many scenarios. Compared
with unimodal learning, it can effectively combine the information from different
modalities to improve the performance of learning tasks. In reality, the multimodal
data may have missing modalities due to various reasons, such as sensor failure
and data transmission error. In previous works, the information of the modality-
missing data has not been well exploited. To address this problem, we propose
an efﬁcient approach based on maximum likelihood estimation to incorporate the
knowledge in the modality-missing data. Speciﬁcally, we design a likelihood
function to characterize the conditional distributions of the modality-complete
data and the modality-missing data, which is theoretically optimal. Moreover,
we develop a generalized form of the softmax function to effectively implement
maximum likelihood estimation in an end-to-end manner. Such training strategy
guarantees the computability of our algorithm capably. Finally, we conduct a series
of experiments on real-world multimodal datasets. Our results demonstrate the
effectiveness of the proposed approach, even when 95% of the training data has
missing modality."
INTRODUCTION,0.007936507936507936,"1
INTRODUCTION"
INTRODUCTION,0.011904761904761904,"Multimodal learning is an important research area, which builds models to process and relate
information between different modalities (Ngiam et al., 2011; Srivastava & Salakhutdinov, 2014;
Baltrušaitis et al., 2018). Compared with unimodal learning, multimodal learning can achieve
better performance by properly utilizing the multimodal data. It has been successfully used in
many applications, such as multimodal emotion recognition (Soleymani et al., 2011; Mittal et al.,
2020), multimedia event detection (Li et al., 2020), and visual question-answering (Yu et al., 2019).
With the emergence of big data, multimodal learning becomes more and more important to combine
the multimodal data from different sources."
INTRODUCTION,0.015873015873015872,"A number of previous works (Tzirakis et al., 2017; Zhang et al., 2017; Elliott et al., 2017; Kim et al.,
2020; Zhang et al., 2020) have achieved great successes based on complete observations during
the training process. However, in practice, the multimodal data may have missing modalities (Du
et al., 2018; Ma et al., 2021a;b). This may be caused by various reasons. For instance, the sensor
that collects the multimodal data is damaged or the network transmission fails. Examples of the
multimodal data are shown in Figure 1."
INTRODUCTION,0.01984126984126984,"In the past years, different approaches have been proposed to deal with modality missing. A simple
and typical way (Hastie et al., 2009) is to directly discard the data with missing modalities. Since
the information contained in the modality-missing data is neglected, such method often has limited
performance. In addition, researchers (Tran et al., 2017; Chen & Zhang, 2020; Liu et al., 2021; Ma
et al., 2021b) have proposed approaches to heuristically combine the information of the modality-
missing data. However, most of these works lack theoretical explanations, and these empirical
methods are often implemented using multiple training stages rather than an end-to-end manner,
which lead to the information of the modality-missing data not being well exploited."
INTRODUCTION,0.023809523809523808,"To tackle above issues, we propose an efﬁcient approach based on maximum likelihood estimation
to effectively utilize the modality-missing data. To be speciﬁc, we present a likelihood function to
characterize the conditional distributions of the modality-complete data and the modality-missing
data, which is theoretically optimal. Furthermore, we adopt a generalized form of the softmax"
INTRODUCTION,0.027777777777777776,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.031746031746031744,"(a) Complete
(b) Visual Missing
(c) Audio Missing"
INTRODUCTION,0.03571428571428571,"…
…
…
Visual:"
INTRODUCTION,0.03968253968253968,Audio:
INTRODUCTION,0.04365079365079365,"Figure 1: Examples of the multimodal data: (a) complete observations, (b) observations which may
have missing visual modality, and (c) observations which may have missing audio modality."
INTRODUCTION,0.047619047619047616,"function to efﬁciently implement our maximum likelihood estimation algorithm. Such training
strategy guarantees the computability of our framework in an end-to-end scheme. In this way, our
approach can effectively leverage the information of the modality-missing data during the training
process, Finally, we perform several experiments on real-world multimodal datasets, including
eNTERFACE’05 (Martin et al., 2006) and RAVDESS (Livingstone & Russo, 2018). The results show
the effectiveness of our approach in handling the problem of modality missing. To summarize, our
contribution is three-fold:"
INTRODUCTION,0.051587301587301584,"• We design a likelihood function to learn the conditional distributions of the modality-
complete data and the modality-missing data, which is theoretically optimal."
INTRODUCTION,0.05555555555555555,"• We develop a generalized form of the softmax function to implement our maximum likeli-
hood estimation framework in an end-to-end manner, which is more effective than previous
works."
INTRODUCTION,0.05952380952380952,"• We conduct a series of experiments on real-world multimodal datasets. The results validate
the effectiveness of our approach, even when 95% of the training data has missing modality."
METHODOLOGY,0.06349206349206349,"2
METHODOLOGY"
METHODOLOGY,0.06746031746031746,"Our goal is to deal with the problem of modality missing in multimodal learning based on maximum
likelihood estimation. In the following, we ﬁrst show the problem formulation, and then describe the
details of our framework."
PROBLEM FORMULATION,0.07142857142857142,"2.1
PROBLEM FORMULATION"
PROBLEM FORMULATION,0.07539682539682539,"In this paper, we consider that the multimodal data has two modalities. Here, the random variables
corresponding to these two modalities and their category labels are denoted as X, Y , and Z, re-
spectively. In the training process, we assume that there are two independently observed datasets:
modality-complete and modality-missing. We use DXY Z =

(x(i)
c , y(i)
c , z(i)
c ) | z(i)
c
∈Z =
{1, 2, · · · , |Z|}
	nc
i=1 to represent the modality-complete dataset, where x(i)
c
and y(i)
c
represent the"
PROBLEM FORMULATION,0.07936507936507936,"two modalities of the i-th sample of DXY Z respectively, z(i)
c
is their corresponding category label,
and the size of DXY Z is nc. We then use DXZ =

(x(i)
m , z(i)
m ) | z(i)
m ∈Z = {1, 2, · · · , |Z|}
	nm
i=1
to represent the modality-missing dataset, where the size of DXZ is nm. In addition, we adopt
[DXY Z]XY to represent

(x(i)
c , y(i)
c )
	nc
i=1. [DXY Z]Z, [DXZ]X, and [DXZ]Z are expressed in the
same way. The multimodal data of DXY Z and DXZ are assumed to be i.i.d. generated from an
unknown underlying joint distribution. By utilizing the knowledge of the modality-complete data and
the modality-missing data, we hope our framework can predict the category labels correctly."
MAXIMUM LIKELIHOOD ESTIMATION FOR MISSING MODALITY,0.08333333333333333,"2.2
MAXIMUM LIKELIHOOD ESTIMATION FOR MISSING MODALITY"
MAXIMUM LIKELIHOOD ESTIMATION FOR MISSING MODALITY,0.0873015873015873,"In this section, we ﬁrst present how to design a likelihood function to learn the conditional distributions
of the modality-complete data and the modality-missing data. Then, we show that by adopting a
generalized form of the softmax function, we design a training strategy to implement our algorithm."
MAXIMUM LIKELIHOOD ESTIMATION FOR MISSING MODALITY,0.09126984126984126,Under review as a conference paper at ICLR 2022
MAXIMUM LIKELIHOOD ESTIMATION FOR MISSING MODALITY,0.09523809523809523,"Visual Network
Audio Network"
MAXIMUM LIKELIHOOD ESTIMATION FOR MISSING MODALITY,0.0992063492063492,Label Network
MAXIMUM LIKELIHOOD ESTIMATION FOR MISSING MODALITY,0.10317460317460317,Log-likelihood Function x y L f g h
MAXIMUM LIKELIHOOD ESTIMATION FOR MISSING MODALITY,0.10714285714285714,“Anger”
MAXIMUM LIKELIHOOD ESTIMATION FOR MISSING MODALITY,0.1111111111111111,“Disgust”
MAXIMUM LIKELIHOOD ESTIMATION FOR MISSING MODALITY,0.11507936507936507,“Happiness”
MAXIMUM LIKELIHOOD ESTIMATION FOR MISSING MODALITY,0.11904761904761904,“Sadness” ···
MAXIMUM LIKELIHOOD ESTIMATION FOR MISSING MODALITY,0.12301587301587301,Class Label z
MAXIMUM LIKELIHOOD ESTIMATION FOR MISSING MODALITY,0.12698412698412698,"Figure 2: Our proposed framework for multimodal learning with missing modality. In the training
process, we propose a log-likelihood function L, as shown in Equation (2), to learn the conditional
distributions of the modality-complete data and the modality-missing data. By developing a general-
ized form of the softmax function, we implement our maximum likelihood estimation algorithm in an
end-to-end manner."
LIKELIHOOD FUNCTION ANALYSES,0.13095238095238096,"2.2.1
LIKELIHOOD FUNCTION ANALYSES"
LIKELIHOOD FUNCTION ANALYSES,0.1349206349206349,"Maximum likelihood estimation is a statistical method of using the observed data to estimate the
distribution by maximizing the likelihood function. The estimated distribution makes the observed
data most likely (Myung, 2003). With this idea, we study the likelihood function on datasets DXY Z
and DXZ. For the classiﬁcation task, the conditional likelihood is commonly used. Inspired by this,
we use a model QXY Z to learn the underlying joint distribution of DXY Z and DXZ. The conditional
likelihood can be represented as:"
LIKELIHOOD FUNCTION ANALYSES,0.1388888888888889,"ℓ≜P ([DXY Z]Z, [DXZ]Z | [DXY Z]XY , [DXZ]X; QXY Z)"
LIKELIHOOD FUNCTION ANALYSES,0.14285714285714285,"a= P ([DXY Z]Z | [DXY Z]XY ; QXY Z) · P ([DXZ]Z | [DXZ]X; QXY Z) b=
Y"
LIKELIHOOD FUNCTION ANALYSES,0.14682539682539683,"(x,y,z)∈DXY Z
QZ|XY (z|xy) ·
Y"
LIKELIHOOD FUNCTION ANALYSES,0.15079365079365079,"(x,z)∈DXZ
QZ|X(z|x)
(1)"
LIKELIHOOD FUNCTION ANALYSES,0.15476190476190477,"where the step a follows from the fact that datasets DXY Z and DXZ are observed independently,
and the step b is due to that samples in each dataset are i.i.d. QZ|XY and QZ|X are conditional
distributions of QXY Z. In this way, we show the likelihood function using the information of DXY Z
and DXZ. Then, we use the negative log-likelihood as the loss function to train our deep learning
model, i.e.,"
LIKELIHOOD FUNCTION ANALYSES,0.15873015873015872,"L ≜−log ℓ= −
X"
LIKELIHOOD FUNCTION ANALYSES,0.1626984126984127,"(x,y,z)∈DXY Z
log QZ|XY (z|xy) −
X"
LIKELIHOOD FUNCTION ANALYSES,0.16666666666666666,"(x,z)∈DXZ
log QZ|X(z|x)
(2)"
LIKELIHOOD FUNCTION ANALYSES,0.17063492063492064,"It is worth noting that in (Daniels, 1961; Lehmann, 2004), maximum likelihood estimation is proved
to be an asymptotically-efﬁcient strategy, which guarantees the theoretical optimality of our method
to deal with modality missing."
LIKELIHOOD FUNCTION ANALYSES,0.1746031746031746,"To optimize L, we use deep neural networks to extract the k-dimensional feature representations
from the observation (x, y, z), which are represented as f(x) = [f1(x), f2(x), · · · , fk(x)]T, g(y) =
[g1(y), g2(y), · · · , gk(y)]T, and h(z) = [h1(z), h2(z), · · · , hk(z)]T, respectively. We then utilize
these features to learn QZ|XY and QZ|X in L. Our framework is shown in Figure 2."
LIKELIHOOD FUNCTION ANALYSES,0.17857142857142858,"In this way, we show the log-likelihood function L. By characterizing the conditional distributions
of the modality-complete data and the modality-missing data, it leverages the underlying structure
information behind the multimodal data, which constitutes the theoretical basis of our framework."
MAXIMUM LIKELIHOOD ESTIMATION IMPLEMENTATION,0.18253968253968253,"2.2.2
MAXIMUM LIKELIHOOD ESTIMATION IMPLEMENTATION"
MAXIMUM LIKELIHOOD ESTIMATION IMPLEMENTATION,0.1865079365079365,"In fact, it is not easy to optimize the log-likelihood function L in Equation (2) by designing neural
networks, which is mainly due to two reasons. Firstly, the representations of the high-dimensional
data and the procedure to model them are complicated. Secondly, since QZ|XY and QZ|X in L are"
MAXIMUM LIKELIHOOD ESTIMATION IMPLEMENTATION,0.19047619047619047,"Under review as a conference paper at ICLR 2022 M
O"
MAXIMUM LIKELIHOOD ESTIMATION IMPLEMENTATION,0.19444444444444445,"(a) Addition
(b) Concatenation
(c) Outer product f
f
f g
g
g"
MAXIMUM LIKELIHOOD ESTIMATION IMPLEMENTATION,0.1984126984126984,"Figure 3: Three forms of φ are studied: (a) addition (Wang et al., 2019), i.e., φ(f, g) ≜f + g, (b)
concatenation (Chandar et al., 2016), i.e., φ(f, g) ≜[f T, gT]T, and (c) outer product (Zadeh et al.,
2017), i.e., φ(f, g) ≜vec(f ⊗g), where vec represents the vectorization of outer product."
MAXIMUM LIKELIHOOD ESTIMATION IMPLEMENTATION,0.20238095238095238,"related, how to build models to learn their relationships is difﬁcult. To address these two issues, we
develop a generalized form of the softmax function to describe QXY Z as follows 1:"
MAXIMUM LIKELIHOOD ESTIMATION IMPLEMENTATION,0.20634920634920634,"QXY Z(x, y, z) =
RX(x)RY (y)RZ(z) exp(φT(f(x), g(y))h(z))
P"
MAXIMUM LIKELIHOOD ESTIMATION IMPLEMENTATION,0.21031746031746032,"x′,y′,z′ RX(x′)RY (y′)RZ(z′) exp(φT(f(x′), g(y′))h(z′))
(3)"
MAXIMUM LIKELIHOOD ESTIMATION IMPLEMENTATION,0.21428571428571427,"where φ(f, g) represents the function to fuse features f and g. We study three forms of φ to
investigate its effect in our framework, as shown in Figure 3. RX, RY , and RZ represent the
underlying marginal distributions of the variables X, Y , and Z, respectively. Their use makes the
denominator of Equation (3) expressed in the form of the mean over RX, RY , and RZ, which serves
as the normalization to make QXY Z a valid distribution and is helpful for our further derivation. In
addition, the generalized softmax function we propose can be regarded as a generalization of softmax
learning in (Xu et al., 2018) from unimodal learning to multimodal learning."
MAXIMUM LIKELIHOOD ESTIMATION IMPLEMENTATION,0.21825396825396826,"In this way, we show the distribution QXY Z by adopting a generalized form of the softmax function,
which has the following two beneﬁts. Firstly, by depicting the representation of QXY Z, we can further
derive QZ|XY and QZ|X. It makes our approach a uniﬁed framework to combine the information
of the modality-complete data and the modality-missing data. Secondly, it avoids modeling the
relationship between QZ|XY and QZ|X. In fact, the correlation between the high-dimensional data
can be rather complex."
MAXIMUM LIKELIHOOD ESTIMATION IMPLEMENTATION,0.2222222222222222,"Then, we derive conditional distributions QZ|XY and QZ|X from Equation (3):"
MAXIMUM LIKELIHOOD ESTIMATION IMPLEMENTATION,0.2261904761904762,"QZ|XY (z|xy) = RZ(z)
exp(φT(f(x), g(y))h(z))
P"
MAXIMUM LIKELIHOOD ESTIMATION IMPLEMENTATION,0.23015873015873015,"z′ RZ(z′) exp(φT(f(x), g(y))h(z′))
(4) and"
MAXIMUM LIKELIHOOD ESTIMATION IMPLEMENTATION,0.23412698412698413,QZ|X(z|x) = RZ(z)
MAXIMUM LIKELIHOOD ESTIMATION IMPLEMENTATION,0.23809523809523808,"P
y′ RY (y′) exp(φT(f(x), g(y′))h(z))
P"
MAXIMUM LIKELIHOOD ESTIMATION IMPLEMENTATION,0.24206349206349206,z′ RZ(z′) P
MAXIMUM LIKELIHOOD ESTIMATION IMPLEMENTATION,0.24603174603174602,"y′ RY (y′) exp(φT(f(x), g(y′))h(z′))
(5)"
MAXIMUM LIKELIHOOD ESTIMATION IMPLEMENTATION,0.25,"We can observe that by introducing RX, RY , and RZ into QXY Z, the derived QZ|XY and QZ|X are
expressed in the form of the mean over RY and RZ. In practice, we can use the empirical mean as an
estimation. Correspondingly, by plugging Equations (4) and (5) into Equation (2), we can summarize
the detailed steps to compute our objective function L, as shown in Algorithm 1. It is worth pointing
out that when we compute QZ|X, we need to use the information of the modality y. Since in the
training process, the modality y of the dataset DXZ is missing, we utilize samples of the modality y
of the dataset DXY Z to compute QZ|X."
MAXIMUM LIKELIHOOD ESTIMATION IMPLEMENTATION,0.25396825396825395,"Finally, we utilize neural networks to extract features f, g, and h from the modality-complete data and
the modality-missing data to optimize our log-likelihood function L. It performs classiﬁcation directly,
which does not need to explicitly complement the modality-missing data before the classiﬁcation
task."
MAXIMUM LIKELIHOOD ESTIMATION IMPLEMENTATION,0.25793650793650796,"1Strictly speaking, RX and RY are probability density functions, and RZ is a probability mass function. The
denominator of Equation (3) should be integrated over RX and RY . We use summation here for the simplicity
of exposition."
MAXIMUM LIKELIHOOD ESTIMATION IMPLEMENTATION,0.2619047619047619,Under review as a conference paper at ICLR 2022
MAXIMUM LIKELIHOOD ESTIMATION IMPLEMENTATION,0.26587301587301587,Algorithm 1 Compute our objective function on a mini-batch.
MAXIMUM LIKELIHOOD ESTIMATION IMPLEMENTATION,0.2698412698412698,Input:
MAXIMUM LIKELIHOOD ESTIMATION IMPLEMENTATION,0.27380952380952384,"A modality-complete batch

(x(i)
c , y(i)
c , z(i)
c )
	n1
i=1, where n1 is the batch size."
MAXIMUM LIKELIHOOD ESTIMATION IMPLEMENTATION,0.2777777777777778,"A modality-missing batch

(x(i)
m , z(i)
m )
	n2
i=1, where n2 is the batch size.
Neural networks with k output units: f, g, and h.
Output:"
MAXIMUM LIKELIHOOD ESTIMATION IMPLEMENTATION,0.28174603174603174,"The value of our objective L.
1: Compute empirical label distribution ˆRZ:
ˆRZ(z) ←"
MAXIMUM LIKELIHOOD ESTIMATION IMPLEMENTATION,0.2857142857142857,"Pn1
i=1 1(z(i)
c
=z)+Pn2
i=1 1(z(i)
m =z)
n1+n2
, z = 1, 2, · · · , |Z|
2: Compute QZ|XY :"
MAXIMUM LIKELIHOOD ESTIMATION IMPLEMENTATION,0.2896825396825397,"QZ|XY (z(i)
c |x(i)
c , y(i)
c ) ←ˆRZ(z(i)
c )
exp(φT(f(x(i)
c ),g(y(i)
c
))h(z))
P|Z|
z′=1 RZ(z′) exp(φT(f(x(i)
c ),g(y(i)
c
))h(z′)), i = 1, · · · , n1"
MAXIMUM LIKELIHOOD ESTIMATION IMPLEMENTATION,0.29365079365079366,3: Compute QZ|X:
MAXIMUM LIKELIHOOD ESTIMATION IMPLEMENTATION,0.2976190476190476,"QZ|X(z(i)
m |x(i)
m ) ←ˆRZ(z(i)
m )"
MAXIMUM LIKELIHOOD ESTIMATION IMPLEMENTATION,0.30158730158730157,"1
n1
Pn1
j=1 exp(φT(f(x(i)
m ),g(y(j)
c
))h(z(i)
m ))
P|Z|
z′=1 RZ(z′) 1"
MAXIMUM LIKELIHOOD ESTIMATION IMPLEMENTATION,0.3055555555555556,"n1
Pn1
j=1 exp(φT(f(x(i)
m ),g(y(j)
c
))h(z′)), i = 1, · · · , n2"
MAXIMUM LIKELIHOOD ESTIMATION IMPLEMENTATION,0.30952380952380953,"4: Compute our empirical objective L:
−Pn1
i=1 log QZ|XY (z(i)
c |x(i)
c , y(i)
c ) −Pn2
i=1 log QZ|X(z(i)
m |x(i)
m )"
EXPERIMENTS,0.3134920634920635,"3
EXPERIMENTS"
EXPERIMENTS,0.31746031746031744,"In this section, we ﬁrst describe the real-world multimodal datasets used in our experiment, then
explain the experimental settings and baseline methods, and ﬁnally give the experimental results to
show the effectiveness of our approach."
DATASETS,0.32142857142857145,"3.1
DATASETS"
DATASETS,0.3253968253968254,"We perform experiments on two public real-world multimodal datasets: eNTERFACE’05 (Martin
et al., 2006) and RAVDESS (Livingstone & Russo, 2018). eNTERFACE’05 is an audio-visual
emotion database in English. It contains 42 subjects eliciting the six basic emotions: anger, disgust,
fear, happiness, sadness, and surprise. There are 213 videos for happiness, and 216 videos for each
of the remaining emotions. Following (Ma et al., 2020), we extract 30 segment samples from each
video and then obtain a processed dataset with 38,790 samples."
DATASETS,0.32936507936507936,"RAVDESS is a multimodal database of emotional speech and song, which consists of 24 professional
actors in a neutral North American accent. Here, we use the speech part, which includes calm,
happy, sad, angry, fearful, surprise, and disgust expressions. Each recording is also in the video form.
Similar to the eNTERFACE’05 dataset, we only consider six basic emotions, each of which has 5,760
segment samples."
EXPERIMENTAL SETTINGS,0.3333333333333333,"3.2
EXPERIMENTAL SETTINGS"
EXPERIMENTAL SETTINGS,0.3373015873015873,"We perform experiments on the processed eNTERFACE’05 and RAVDESS datasets. Each segment of
these two datasets has a duration of 0.5 seconds. As shown in ((Ma et al., 2020)), consecutive frames
within 0.5 seconds usually contain the same emotion in a similar way, which inspired us to choose
the central frame of each segment as the visual modality. This technique not only makes that the
visual data contains enough emotional information, but also avoids the redundancy in multiple frames.
Besides, the log Mel-spectrogram is extracted from each segment as the audio modality, which is
similar to the RGB image. We then feed these data into our framework to obtain the classiﬁcation
result. ResNet-50 (He et al., 2016) is used as the backbone of visual network f and audio network
g to extract features from visual and audio modalities, respectively. In addition, we transform the
corresponding label into the one-hot form and then extract the label feature using label network h
with a fully connected layer. f, g, and h are trained together."
EXPERIMENTAL SETTINGS,0.3412698412698413,"On each processed dataset, we split all data into three parts: training set, validation set, and test set.
Their proportions are 70%, 15%, and 15%. In practice, modality missing often occurs with a high"
EXPERIMENTAL SETTINGS,0.34523809523809523,Under review as a conference paper at ICLR 2022
EXPERIMENTAL SETTINGS,0.3492063492063492,Table 1: The classiﬁcation performance with missing modality on the eNTERFACE’05 dataset.
EXPERIMENTAL SETTINGS,0.3531746031746032,"Method
Visual Missing
Audio Missing
95%
90%
80%
95%
90%
80%"
EXPERIMENTAL SETTINGS,0.35714285714285715,"Lower Bound
Addition
26.39
35.26
46.91
27.53
35.26
50.93
Concatenation
27.11
36.39
46.49
27.84
33.71
46.29
Outer product
26.91
37.53
42.78
28.56
34.95
48.14"
EXPERIMENTAL SETTINGS,0.3611111111111111,"AE
Addition
41.24
47.11
50.31
42.78
45.77
51.75
Concatenation
43.92
46.39
50.00
42.16
47.42
50.72
Outer product
49.79
52.16
55.15
47.73
53.09
53.51"
EXPERIMENTAL SETTINGS,0.36507936507936506,"HGR MC
Addition
41.34
59.69
58.97
54.95
74.12
77.32
Concatenation
41.34
57.84
63.51
57.42
75.67
79.18
Outer product
49.90
59.69
64.64
55.46
76.29
77.94"
EXPERIMENTAL SETTINGS,0.36904761904761907,"ZP
Addition
58.66
67.84
69.07
76.49
78.35
80.41
Concatenation
60.21
67.11
68.76
76.70
78.25
80.93"
EXPERIMENTAL SETTINGS,0.373015873015873,"Ours
Addition
66.29
71.65
72.37
79.38
80.31
81.24
Concatenation
64.74
70.82
72.27
79.79
80.21
81.24
Outer product
66.08
71.13
72.06
80.31
81.03
81.65"
EXPERIMENTAL SETTINGS,0.376984126984127,"missing rate (Suo et al., 2019; Ma et al., 2021b). Here, in the training stage, we study three kinds
of missing rates: 80%, 90%, and 95%. The case where the audio modality is missing and the case
where the visual modality is missing are investigated respectively. Following (Yu et al., 2020; Chen
& Zhang, 2020; Du et al., 2021), we set modality missing arising during the training phase to show
that a large amount of unimodal data can assist the training of our multimodal learning framework.
In the inference phase, we use Equation (4) to predict the class label of the given test data."
EXPERIMENTAL SETTINGS,0.38095238095238093,"Finally, we run each experiment ﬁve times and report average test accuracies to evaluate the perfor-
mance of our approach and baseline methods. Adam optimizer (Kingma & Ba, 2015) is used to train
neural networks with the learning rate of 0.0001. Both the size of modality-complete batch and the
size of modality-missing batch are set to 90. The number of epochs is set to 100. All experiments are
implemented by Pytorch (Paszke et al., 2019) on a NVIDIA TITAN V GPU card."
BASELINE METHODS,0.38492063492063494,"3.3
BASELINE METHODS"
BASELINE METHODS,0.3888888888888889,"To show the effectiveness of our method, we compare our approach with the following methods
which can also handle modality missing to some extent."
BASELINE METHODS,0.39285714285714285,"• Discarding Modality-incomplete Data (Lower Bound): One simple strategy to handle
modality missing is to directly discard the modality-incomplete data, and then only use the
modality-complete data for the classiﬁcation task. This method does not use the information
of the data with missing modalities. In our maximum likelihood estimation model, this is
equivalent to calculating QZ|XY without calculating QZ|X. Therefore, this method can also
be used as the ablation study of our approach.
• Hirschfeld-Gebelein-Renyi Maximal Correlation (Hirschfeld, 1935; Gebelein, 1941; Rényi,
1959) (HGR MC): HGR MC is a statistical measure which calculates the dependence
between different random variables. It has been successfully used for multimodal learning
(Ma et al., 2021a; 2020; Wang et al., 2019; Xu & Huang, 2020). Here, we use it further
to deal with modality missing. For the modality-complete data, we learn the maximal
correlation between x, y, and z. For the modality-missing data, we learn the maximal
correlation between x and z.
• Zero Padding (ZP): Padding the feature representation of the missing modality with zero is
another widely used way to cope with incomplete modalities (Jo et al., 2019; Chen et al.,
2020; Shen et al., 2020). For ZP, we consider two forms of φ to fuse features f and g:
addition and concatenation. The reason why the form of outer product is not studied here is
that if the feature of one modality is zero, the outer product of it and the non-zero feature of
another modality is also zero, which makes the modality-missing data useless.
• Autoencoder (AE): An autoencoder is a neural network framework used to learn the represen-
tation from the training data. Some previous approaches apply autoencoders to complement"
BASELINE METHODS,0.3968253968253968,Under review as a conference paper at ICLR 2022
BASELINE METHODS,0.4007936507936508,Table 2: The classiﬁcation performance with missing modality on the RAVDESS dataset.
BASELINE METHODS,0.40476190476190477,"Method
Visual Missing
Audio Missing
95%
90%
80%
95%
90%
80%"
BASELINE METHODS,0.4087301587301587,"Lower Bound
Addition
43.01
48.79
62.66
37.11
50.40
67.63
Concatenation
41.27
49.94
62.66
35.61
49.48
67.63
Outer product
37.34
45.90
63.35
36.87
47.86
68.32"
BASELINE METHODS,0.4126984126984127,"AE
Addition
52.60
58.03
67.75
87.39
88.44
90.17
Concatenation
53.29
58.03
67.75
87.39
88.21
90.29
Outer product
53.53
57.34
66.24
87.86
89.25
90.52"
BASELINE METHODS,0.4166666666666667,"HGR MC
Addition
38.49
48.78
64.51
42.43
53.87
73.75
Concatenation
42.08
48.90
65.20
43.70
52.60
73.17
Outer product
37.34
46.59
62.43
43.01
51.33
70.98"
BASELINE METHODS,0.42063492063492064,"ZP
Addition
44.74
52.72
67.40
83.81
86.24
90.17
Concatenation
43.70
52.02
65.89
82.31
86.47
90.63"
BASELINE METHODS,0.4246031746031746,"Ours
Addition
56.64
60.34
70.06
89.72
90.52
91.33
Concatenation
58.84
61.27
69.83
89.25
89.94
91.91
Outer product
54.45
58.26
67.51
90.06
91.21
91.68"
BASELINE METHODS,0.42857142857142855,"the missing modality (Liu et al., 2021; Jaques et al., 2017; Pereira et al., 2020; Shi et al.,
2019). Following these works, for the modality-complete data, we use modality x as the
input of the autoencoder to reconstruct modality y. Then we use the trained autoencoder to
predict the modality y to impute the modality-missing data. Then we use the imputed data
to perform the classiﬁcation task. It is noted that the AE method has several stages while
our method is end-to-end."
BASELINE METHODS,0.43253968253968256,"For a fair comparison, we make that each method has the same network architecture and training
strategy, and report the classiﬁcation results of each method after the same number of repeated
experiments."
EXPERIMENTAL RESULTS,0.4365079365079365,"3.4
EXPERIMENTAL RESULTS"
EXPERIMENTAL RESULTS,0.44047619047619047,"We ﬁrst conduct classiﬁcation experiments on the eNTERFACE’05 and RAVDESS datasets by
comparing our framework with other methods. The experimental setting is shown in Section 3.2. We
report the classiﬁcation accuracy of each method in each setting. The results are shown in Table 1
and Table 2."
EXPERIMENTAL RESULTS,0.4444444444444444,"We have the following summarizations from Table 1 and Table 2: (1) The methods of AE, HGR MC,
ZP, and ours can improve the classiﬁcation accuracy compared to the Lower Bound method which
only uses the modality-complete data. Our method achieves the highest classiﬁcation performance
among all methods under different settings. The higher the missing rate, the more obvious the
gap between other methods and our method. These show that our maximum likelihood estimation
approach are more effective to tackle modality missing compared with other methods. (2) Different
forms of φ will affect the classiﬁcation performance. For example, for our approach, addition and
outer product perform better than concatenation on the eNTERFACE’05 dataset. However, on the
RAVDESS dataset, the concatenation form of φ achieves higher classiﬁcation performance than
the addition and outer product forms under some settings. This indicates that in different settings,
the discrimination ability of the learned feature representations is different. We need to design the
appropriate form of φ to fuse features of the multimodal data. (3) When the visual modality is
missing, the classiﬁcation accuracy is lower than that when the audio modality is missing, indicating
that the visual modality has a more signiﬁcant contribution to the classiﬁcation performance, which
is consistent with previous works (Zhang et al., 2017; Ma et al., 2020)."
EXPERIMENTAL RESULTS,0.44841269841269843,"In addition, we show the classiﬁcation confusion matrices using the methods of AE, HGR MC, ZP,
and ours when the missing rate of visual modality reaches 95% on the eNTERFACE’05 dataset, as
shown in Figure 4. It can be seen that the classiﬁcation accuracy of each emotion using AE or HGR
MC is not high, which indicates that they can only deal with modality missing to a certain extent.
The overall classiﬁcation performance of ZP is lower than ours, but the classiﬁcation accuracy of"
EXPERIMENTAL RESULTS,0.4523809523809524,Under review as a conference paper at ICLR 2022 anger
EXPERIMENTAL RESULTS,0.45634920634920634,disgust fear
EXPERIMENTAL RESULTS,0.4603174603174603,happiness
EXPERIMENTAL RESULTS,0.4642857142857143,sadness
EXPERIMENTAL RESULTS,0.46825396825396826,surprise anger
EXPERIMENTAL RESULTS,0.4722222222222222,disgust fear
EXPERIMENTAL RESULTS,0.47619047619047616,happiness
EXPERIMENTAL RESULTS,0.4801587301587302,sadness
EXPERIMENTAL RESULTS,0.48412698412698413,surprise
EXPERIMENTAL RESULTS,0.4880952380952381,"36.36
12.73
26.06
7.27
6.06
11.52"
EXPERIMENTAL RESULTS,0.49206349206349204,"10.62
52.50
9.38
17.50
5.00
5.00"
EXPERIMENTAL RESULTS,0.49603174603174605,"13.66
9.32
36.65
7.45
17.39
15.53"
EXPERIMENTAL RESULTS,0.5,"3.75
19.38
8.13
57.50
1.24
10.00"
EXPERIMENTAL RESULTS,0.503968253968254,"8.59
6.75
28.22
7.98
38.04
10.42"
EXPERIMENTAL RESULTS,0.5079365079365079,"15.53
6.83
24.84
13.66
12.42
26.72"
EXPERIMENTAL RESULTS,0.5119047619047619,(a) AE anger
EXPERIMENTAL RESULTS,0.5158730158730159,disgust fear
EXPERIMENTAL RESULTS,0.5198412698412699,happiness
EXPERIMENTAL RESULTS,0.5238095238095238,sadness
EXPERIMENTAL RESULTS,0.5277777777777778,surprise anger
EXPERIMENTAL RESULTS,0.5317460317460317,disgust fear
EXPERIMENTAL RESULTS,0.5357142857142857,happiness
EXPERIMENTAL RESULTS,0.5396825396825397,sadness
EXPERIMENTAL RESULTS,0.5436507936507936,surprise
EXPERIMENTAL RESULTS,0.5476190476190477,"54.55
6.06
13.33
7.88
8.48
9.70"
EXPERIMENTAL RESULTS,0.5515873015873016,"13.13
36.88
10.00
21.88
13.75
4.36"
EXPERIMENTAL RESULTS,0.5555555555555556,"13.04
6.83
26.09
19.88
23.60
10.56"
EXPERIMENTAL RESULTS,0.5595238095238095,"7.50
14.37
7.50
53.13
10.62
6.88"
EXPERIMENTAL RESULTS,0.5634920634920635,"9.20
6.75
17.79
5.52
49.70
11.04"
EXPERIMENTAL RESULTS,0.5674603174603174,"16.77
7.45
13.04
15.53
19.88
27.33"
EXPERIMENTAL RESULTS,0.5714285714285714,(b) HGR MC anger
EXPERIMENTAL RESULTS,0.5753968253968254,disgust fear
EXPERIMENTAL RESULTS,0.5793650793650794,happiness
EXPERIMENTAL RESULTS,0.5833333333333334,sadness
EXPERIMENTAL RESULTS,0.5873015873015873,surprise anger
EXPERIMENTAL RESULTS,0.5912698412698413,disgust fear
EXPERIMENTAL RESULTS,0.5952380952380952,happiness
EXPERIMENTAL RESULTS,0.5992063492063492,sadness
EXPERIMENTAL RESULTS,0.6031746031746031,surprise
EXPERIMENTAL RESULTS,0.6071428571428571,"71.53
5.45
6.06
7.27
1.21
8.48"
EXPERIMENTAL RESULTS,0.6111111111111112,"11.25
48.75
10.00
17.50
6.88
5.62"
EXPERIMENTAL RESULTS,0.6150793650793651,"15.53
6.21
51.55
3.11
15.53
8.07"
EXPERIMENTAL RESULTS,0.6190476190476191,"6.87
6.87
5.00
70.00
5.63
5.63"
EXPERIMENTAL RESULTS,0.623015873015873,"5.52
4.29
15.95
3.07
63.19
7.98"
EXPERIMENTAL RESULTS,0.626984126984127,"13.66
4.35
14.91
6.83
13.66
46.59"
EXPERIMENTAL RESULTS,0.6309523809523809,(c) ZP anger
EXPERIMENTAL RESULTS,0.6349206349206349,disgust fear
EXPERIMENTAL RESULTS,0.6388888888888888,happiness
EXPERIMENTAL RESULTS,0.6428571428571429,sadness
EXPERIMENTAL RESULTS,0.6468253968253969,surprise anger
EXPERIMENTAL RESULTS,0.6507936507936508,disgust fear
EXPERIMENTAL RESULTS,0.6547619047619048,happiness
EXPERIMENTAL RESULTS,0.6587301587301587,sadness
EXPERIMENTAL RESULTS,0.6626984126984127,surprise
EXPERIMENTAL RESULTS,0.6666666666666666,"87.28
3.03
1.82
2.42
3.03
2.42"
EXPERIMENTAL RESULTS,0.6706349206349206,"13.13
57.50
5.63
9.38
4.36
10.00"
EXPERIMENTAL RESULTS,0.6746031746031746,"14.91
5.59
59.01
1.86
10.56
8.07"
EXPERIMENTAL RESULTS,0.6785714285714286,"8.12
9.38
8.13
66.87
2.50
5.00"
EXPERIMENTAL RESULTS,0.6825396825396826,"7.36
3.07
12.27
1.23
73.62
2.45"
EXPERIMENTAL RESULTS,0.6865079365079365,"11.80
8.70
11.80
4.96
9.94
52.80"
EXPERIMENTAL RESULTS,0.6904761904761905,(d) Ours
EXPERIMENTAL RESULTS,0.6944444444444444,Figure 4: The confusion matrices of different methods on the eNTERFACE’05 dataset.
EXPERIMENTAL RESULTS,0.6984126984126984,"“happiness” is slightly higher than ours. This shows that different emotions have different clues
for the classiﬁcation task."
EXPERIMENTAL RESULTS,0.7023809523809523,"We then investigate the effect of the backbone in coping with modality missing. In the above
experiments, we use ResNet-50 as the backbone of different methods to extract feature representations.
Here, we replace ResNet-50 with ResNet-34 (He et al., 2016) and VGG-16 (Simonyan & Zisserman,
2015) respectively, and conduct experiments to compare the performance of different backbones
when 95% of the training data has missing visual modality on the RAVDESS dataset, as shown in
Figure 5. We can observe that compared with VGG-16 and ResNet-34, ResNet-50 achieves the
highest performance. In addition, no matter what kind of backbone is based on, the classiﬁcation
accuracy using our method is the highest, followed by using AE, ZP and HGR MC, and the lowest
using Lower Bound, which shows that our approach can take effect for different backbones."
RELATED WORKS,0.7063492063492064,"4
RELATED WORKS"
RELATED WORKS,0.7103174603174603,"Multimodal learning has achieved great successes in many applications. An important topic in this
ﬁeld is multimodal representations (Baltrušaitis et al., 2018; Zhu et al., 2020), which learn feature
representations from the multimodal data by using the information of different modalities. How to
learn good representations is investigated in (Ngiam et al., 2011; Wu et al., 2014; Pan et al., 2016; Xu
et al., 2015). Another important topic is multimodal fusion (Atrey et al., 2010; Poria et al., 2017),
which combines the information from different modalities to make predictions. Feature-based fusion
is one of the most common types of multimodal fusion. It concatenates the feature representations
extracted from different modalities. This fusion approach is adopted by previous works (Tzirakis
et al., 2017; Zhang et al., 2017; Castellano et al., 2008; Zhang et al., 2016). Modality missing is a key
challenge in applying multimodal learning to the real world."
RELATED WORKS,0.7142857142857143,Under review as a conference paper at ICLR 2022
RELATED WORKS,0.7182539682539683,"Lower Bound
AE
HGR MC
ZP
Ours
0 10 20 30 40"
RELATED WORKS,0.7222222222222222,(a) VGG-16
RELATED WORKS,0.7261904761904762,"Lower Bound
AE
HGR MC
ZP
Ours
0 10 20 30 40 50"
RELATED WORKS,0.7301587301587301,(b) ResNet-34
RELATED WORKS,0.7341269841269841,"Lower Bound
AE
HGR MC
ZP
Ours
0 10 20 30 40 50 60"
RELATED WORKS,0.7380952380952381,(c) ResNet-50
RELATED WORKS,0.7420634920634921,Figure 5: The performance comparison of different backbones on the RAVDESS dataset.
RELATED WORKS,0.746031746031746,"To cope with the problem of modality missing, a few methods have been proposed. For example,
Ma et al. (2021b) propose a Bayesian meta learning framework to perturb the latent feature space so
that embeddings of single modality can approximate embeddings of full modality. Tran et al. (2017)
propose a cascaded residual autoencoder for imputation with missing modalities, which is composed
of a set of stacked residual autoencoders that iteratively model the residuals. Chen & Zhang (2020)
propose a heterogeneous graph-based multimodal fusion approach to enable multimodal fusion of
incomplete data within a heterogeneous graph structure. Liu et al. (2021) propose an autoencoder
framework to complement the missing data in the kernel space while taking into account the structural
information of data and the inherent association between multiple views."
RELATED WORKS,0.75,"The above approaches can combine the information of the modality-missing data to some extent.
Our work is signiﬁcantly different from them. The reason lies in the following two facts. Firstly, by
exploiting the likelihood function to learn the conditional distributions of the modality-complete data
and the modality-missing data, our method has a theoretical guarantee, which is skipped by previous
works. Secondly, the training process of our approach is in an end-to-end manner, while the training
processes of most above methods are relatively cumbersome."
CONCLUSION,0.753968253968254,"5
CONCLUSION"
CONCLUSION,0.7579365079365079,"Multimodal learning is a hot topic in the academic and industry communities, of which a key challenge
is modality missing. In practice, the multimodal data may not be complete due to various reasons.
Most previous works cannot effectively utilize the modality-missing data for the learning task. To
address this problem, we propose an efﬁcient approach to leverage the knowledge in the modality-
missing data during the training stage. Speciﬁcally, we present a framework based on maximum
likelihood estimation to characterize the conditional distributions of the modality-complete data and
the modality-missing data, which has a theoretical guarantee. Furthermore, we develop a generalized
form of the softmax function to effectively implement our maximum likelihood estimation framework
in an end-to-end way. We conduct experiments on the eNTERFACE’05 dataset and the RAVDESS
dataset for multimodal learning to demonstrate the effectiveness of our approach. In the future, we
can further extend our framework to other multimodal learning domains."
REPRODUCIBILITY STATEMENT,0.7619047619047619,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.7658730158730159,"We provide our code in “supplement.zip”. In this folder, “eNTERFACE_preprocess.py” and
“RAVDESS_preprocess.py” extract segment samples from the original videos of the eNTER-
FACE’05 dataset and the RAVDESS dataset, respectively. “mle.py” shows the function to compute
our maximum likelihood estimation algorithm."
REFERENCES,0.7698412698412699,REFERENCES
REFERENCES,0.7738095238095238,"Pradeep K Atrey, M Anwar Hossain, Abdulmotaleb El Saddik, and Mohan S Kankanhalli. Multimodal
fusion for multimedia analysis: a survey. Multimedia systems, 16(6):345–379, 2010."
REFERENCES,0.7777777777777778,Under review as a conference paper at ICLR 2022
REFERENCES,0.7817460317460317,"Tadas Baltrušaitis, Chaitanya Ahuja, and Louis-Philippe Morency. Multimodal machine learning:
A survey and taxonomy. IEEE transactions on pattern analysis and machine intelligence, 41(2):
423–443, 2018."
REFERENCES,0.7857142857142857,"Ginevra Castellano, Loic Kessous, and George Caridakis. Emotion recognition through multiple
modalities: face, body gesture, speech. In Affect and emotion in human-computer interaction, pp.
92–103. Springer, 2008."
REFERENCES,0.7896825396825397,"Sarath Chandar, Mitesh M Khapra, Hugo Larochelle, and Balaraman Ravindran. Correlational neural
networks. Neural computation, 28(2):257–285, 2016."
REFERENCES,0.7936507936507936,"Jiayi Chen and Aidong Zhang. Hgmf: Heterogeneous graph-based fusion for multimodal data with
incompleteness. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining, pp. 1295–1305, 2020."
REFERENCES,0.7976190476190477,"Zhengyang Chen, Shuai Wang, and Yanmin Qian. Multi-modality matters: A performance leap on
voxceleb. Proc. Interspeech 2020, pp. 2252–2256, 2020."
REFERENCES,0.8015873015873016,"HE Daniels. The asymptotic efﬁciency of a maximum likelihood estimator. In Fourth Berkeley
Symposium on Mathematical Statistics and Probability, volume 1, pp. 151–163. University of
California Press Berkeley, 1961."
REFERENCES,0.8055555555555556,"Changde Du, Changying Du, Hao Wang, Jinpeng Li, Wei-Long Zheng, Bao-Liang Lu, and Huiguang
He. Semi-supervised deep generative modelling of incomplete multi-modality emotional data. In
Proceedings of the 26th ACM international conference on Multimedia, pp. 108–116, 2018."
REFERENCES,0.8095238095238095,"Changde Du, Changying Du, and Huiguang He. Multimodal deep generative adversarial models for
scalable doubly semi-supervised learning. Information Fusion, 68:118–130, 2021."
REFERENCES,0.8134920634920635,"Desmond Elliott, Stella Frank, Loïc Barrault, Fethi Bougares, and Lucia Specia. Findings of the
second shared task on multimodal machine translation and multilingual image description. In
Proceedings of the Second Conference on Machine Translation, pp. 215–233, 2017."
REFERENCES,0.8174603174603174,"Hans Gebelein. Das statistische problem der korrelation als variations-und eigenwertproblem und
sein zusammenhang mit der ausgleichsrechnung. ZAMM-Journal of Applied Mathematics and
Mechanics/Zeitschrift für Angewandte Mathematik und Mechanik, 21(6):364–379, 1941."
REFERENCES,0.8214285714285714,"Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The elements of statistical learning: data
mining, inference, and prediction. Springer Science & Business Media, 2009."
REFERENCES,0.8253968253968254,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.
770–778, 2016."
REFERENCES,0.8293650793650794,"Hermann O Hirschfeld. A connection between correlation and contingency. In Mathematical Pro-
ceedings of the Cambridge Philosophical Society, volume 31, pp. 520–524. Cambridge University
Press, 1935."
REFERENCES,0.8333333333333334,"Natasha Jaques, Sara Taylor, Akane Sano, and Rosalind Picard. Multimodal autoencoder: A deep
learning approach to ﬁlling in missing sensor data and enabling better mood prediction. In 2017
Seventh International Conference on Affective Computing and Intelligent Interaction (ACII), pp.
202–208. IEEE, 2017."
REFERENCES,0.8373015873015873,"Dae Ung Jo, ByeongJu Lee, Jongwon Choi, Haanju Yoo, and Jin Young Choi. Cross-modal variational
auto-encoder with distributed latent spaces and associators. arXiv preprint arXiv:1905.12867,
2019."
REFERENCES,0.8412698412698413,"Eun-Sol Kim, Woo Young Kang, Kyoung-Woon On, Yu-Jung Heo, and Byoung-Tak Zhang. Hyper-
graph attention networks for multimodal learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 14581–14590, 2020."
REFERENCES,0.8452380952380952,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations (ICLR), 2015."
REFERENCES,0.8492063492063492,Under review as a conference paper at ICLR 2022
REFERENCES,0.8531746031746031,"Erich Leo Lehmann. Elements of large-sample theory. Springer Science & Business Media, 2004."
REFERENCES,0.8571428571428571,"Manling Li, Alireza Zareian, Qi Zeng, Spencer Whitehead, Di Lu, Heng Ji, and Shih-Fu Chang.
Cross-media structured common space for multimedia event extraction. In Proceedings of the 58th
Annual Meeting of the Association for Computational Linguistics, pp. 2557–2568, 2020."
REFERENCES,0.8611111111111112,"Yanbei Liu, Lianxi Fan, Changqing Zhang, Tao Zhou, Zhitao Xiao, Lei Geng, and Dinggang Shen.
Incomplete multi-modal representation learning for alzheimer’s disease diagnosis. Medical Image
Analysis, 69:101953, 2021."
REFERENCES,0.8650793650793651,"Steven R Livingstone and Frank A Russo. The ryerson audio-visual database of emotional speech
and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american
english. PloS one, 13(5):e0196391, 2018."
REFERENCES,0.8690476190476191,"Fei Ma, Wei Zhang, Yang Li, Shao-Lun Huang, and Lin Zhang. Learning better representations for
audio-visual emotion recognition with common information. Applied Sciences, 10(20), 2020."
REFERENCES,0.873015873015873,"Fei Ma, Shao-Lun Huang, and Lin Zhang. An efﬁcient approach for audio-visual emotion recognition
with missing labels and missing modalities. In 2021 IEEE International Conference on Multimedia
and Expo (ICME), 2021a."
REFERENCES,0.876984126984127,"Mengmeng Ma, Jian Ren, Long Zhao, Sergey Tulyakov, Cathy Wu, and Xi Peng. Smil: Multimodal
learning with severely missing modality. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, volume 35, pp. 2302–2310, 2021b."
REFERENCES,0.8809523809523809,"O. Martin, I. Kotsia, B. Macq, and I. Pitas. The enterface’ 05 audio-visual emotion database. In 22nd
International Conference on Data Engineering Workshops (ICDEW’06), pp. 8–8, 2006."
REFERENCES,0.8849206349206349,"Trisha Mittal, Pooja Guhan, Uttaran Bhattacharya, Rohan Chandra, Aniket Bera, and Dinesh Manocha.
Emoticon: Context-aware multimodal emotion recognition using frege’s principle. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14234–14243,
2020."
REFERENCES,0.8888888888888888,"In Jae Myung. Tutorial on maximum likelihood estimation. Journal of mathematical Psychology, 47
(1):90–100, 2003."
REFERENCES,0.8928571428571429,"Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Y Ng. Multi-
modal deep learning. In ICML, 2011."
REFERENCES,0.8968253968253969,"Yingwei Pan, Tao Mei, Ting Yao, Houqiang Li, and Yong Rui. Jointly modeling embedding and
translation to bridge video and language. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 4594–4602, 2016."
REFERENCES,0.9007936507936508,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. In NeurIPS, pp. 8026–8037, 2019."
REFERENCES,0.9047619047619048,"Ricardo Cardoso Pereira, Miriam Seoane Santos, Pedro Pereira Rodrigues, and Pedro Henriques
Abreu. Reviewing autoencoders for missing data imputation: Technical trends, applications and
outcomes. Journal of Artiﬁcial Intelligence Research, 69:1255–1285, 2020."
REFERENCES,0.9087301587301587,"Soujanya Poria, Erik Cambria, Rajiv Bajpai, and Amir Hussain. A review of affective computing:
From unimodal analysis to multimodal fusion. Information Fusion, 37:98–125, 2017."
REFERENCES,0.9126984126984127,"Alfréd Rényi. On measures of dependence. Acta Mathematica Academiae Scientiarum Hungarica,
10(3-4):441–451, 1959."
REFERENCES,0.9166666666666666,"Guangyao Shen, Xin Wang, Xuguang Duan, Hongzhi Li, and Wenwu Zhu. Memor: A dataset for
multimodal emotion reasoning in videos. In Proceedings of the 28th ACM International Conference
on Multimedia, pp. 493–502, 2020."
REFERENCES,0.9206349206349206,"Yuge Shi, Brooks Paige, Philip Torr, et al. Variational mixture-of-experts autoencoders for multi-
modal deep generative models. Advances in Neural Information Processing Systems, 32:15718–
15729, 2019."
REFERENCES,0.9246031746031746,Under review as a conference paper at ICLR 2022
REFERENCES,0.9285714285714286,"Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. International Conference on Learning Representations (ICLR), 2015."
REFERENCES,0.9325396825396826,"Mohammad Soleymani, Maja Pantic, and Thierry Pun. Multimodal emotion recognition in response
to videos. IEEE transactions on affective computing, 3(2):211–223, 2011."
REFERENCES,0.9365079365079365,"Nitish Srivastava and Ruslan Salakhutdinov. Multimodal learning with deep boltzmann machines.
Journal of Machine Learning Research, 15(84):2949–2980, 2014."
REFERENCES,0.9404761904761905,"Qiuling Suo, Weida Zhong, Fenglong Ma, Ye Yuan, Jing Gao, and Aidong Zhang. Metric learning on
healthcare data with incomplete modalities. In IJCAI, pp. 3534–3540, 2019."
REFERENCES,0.9444444444444444,"Luan Tran, Xiaoming Liu, Jiayu Zhou, and Rong Jin. Missing modalities imputation via cascaded
residual autoencoder. In 2017 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 4971–4980, 2017."
REFERENCES,0.9484126984126984,"Panagiotis Tzirakis, George Trigeorgis, Mihalis A Nicolaou, Björn W Schuller, and Stefanos Zafeiriou.
End-to-end multimodal emotion recognition using deep neural networks. IEEE Journal of Selected
Topics in Signal Processing, 11(8):1301–1309, 2017."
REFERENCES,0.9523809523809523,"Lichen Wang, Jiaxiang Wu, Shao-Lun Huang, Lizhong Zheng, Xiangxiang Xu, Lin Zhang, and
Junzhou Huang. An efﬁcient approach to informative feature extraction from multimodal data. In
Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pp. 5281–5288, 2019."
REFERENCES,0.9563492063492064,"Zuxuan Wu, Yu-Gang Jiang, Jun Wang, Jian Pu, and Xiangyang Xue. Exploring inter-feature and
inter-class relationships with deep neural networks for video classiﬁcation. In Proceedings of the
22nd ACM international conference on Multimedia, pp. 167–176, 2014."
REFERENCES,0.9603174603174603,"Ran Xu, Caiming Xiong, Wei Chen, and Jason Corso. Jointly modeling deep video and compositional
text to bridge vision and language in a uniﬁed framework. In Proceedings of the AAAI Conference
on Artiﬁcial Intelligence, volume 29, 2015."
REFERENCES,0.9642857142857143,"Xiangxiang Xu and Shao-Lun Huang. Maximal correlation regression. IEEE Access, 8:26591–26601,
2020."
REFERENCES,0.9682539682539683,"Xiangxiang Xu, Shao-Lun Huang, Lizhong Zheng, and Lin Zhang. The geometric structure of
generalized softmax learning. In 2018 IEEE Information Theory Workshop (ITW), pp. 1–5, 2018."
REFERENCES,0.9722222222222222,"Guan Yu, Quefeng Li, Dinggang Shen, and Yufeng Liu. Optimal sparse linear prediction for block-
missing multi-modality data without imputation. Journal of the American Statistical Association,
115(531):1406–1419, 2020."
REFERENCES,0.9761904761904762,"Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian. Deep modular co-attention networks for
visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 6281–6290, 2019."
REFERENCES,0.9801587301587301,"Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. Tensor
fusion network for multimodal sentiment analysis. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Processing, pp. 1103–1114, 2017."
REFERENCES,0.9841269841269841,"Huaizheng Zhang, Linsen Dong, Guanyu Gao, Han Hu, Yonggang Wen, and Kyle Guan. Deepqoe: A
multimodal learning framework for video quality of experience (qoe) prediction. IEEE Transactions
on Multimedia, 22(12):3210–3223, 2020."
REFERENCES,0.9880952380952381,"Shiqing Zhang, Shiliang Zhang, Tiejun Huang, and Wen Gao. Multimodal deep convolutional neural
network for audio-visual emotion recognition. In Proceedings of the 2016 ACM on International
Conference on Multimedia Retrieval, pp. 281–284, 2016."
REFERENCES,0.9920634920634921,"Shiqing Zhang, Shiliang Zhang, Tiejun Huang, Wen Gao, and Qi Tian. Learning affective features
with a hybrid deep model for audio–visual emotion recognition. IEEE Transactions on Circuits
and Systems for Video Technology, 28(10):3030–3043, 2017."
REFERENCES,0.996031746031746,"Wenwu Zhu, Xin Wang, and Wen Gao. Multimedia intelligence: When multimedia meets artiﬁcial
intelligence. IEEE Transactions on Multimedia, 22(7):1823–1835, 2020."
