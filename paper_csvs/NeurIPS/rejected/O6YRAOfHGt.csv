Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0009009009009009009,"Unsupervised Environment Design (UED) is a paradigm that automatically gen-
1"
ABSTRACT,0.0018018018018018018,"erates a curriculum of training environments, enabling agents trained in these
2"
ABSTRACT,0.002702702702702703,"environments to develop general capabilities, i.e., achieving good zero-shot transfer
3"
ABSTRACT,0.0036036036036036037,"performance. However, existing UED approaches focus primarily on the random
4"
ABSTRACT,0.0045045045045045045,"generation of environments for open-ended agent training. This is impractical
5"
ABSTRACT,0.005405405405405406,"in resource-limited scenarios where there is a constraint on the number of envi-
6"
ABSTRACT,0.006306306306306306,"ronments that can be generated. In this paper, we introduce a hierarchical MDP
7"
ABSTRACT,0.007207207207207207,"framework for environment design under resource constraints. It consists of an
8"
ABSTRACT,0.008108108108108109,"upper-level RL teacher agent that generates suitable training environments for a
9"
ABSTRACT,0.009009009009009009,"lower-level student agent. The RL teacher can leverage previously discovered
10"
ABSTRACT,0.00990990990990991,"environment structures and generate environments at the frontier of the student’s
11"
ABSTRACT,0.010810810810810811,"capabilities by observing the student policy’s representation. Additionally, to alle-
12"
ABSTRACT,0.011711711711711712,"viate the time-consuming process of collecting the experience of the upper-level
13"
ABSTRACT,0.012612612612612612,"teacher, we utilize recent advances in generative modeling to synthesize a trajec-
14"
ABSTRACT,0.013513513513513514,"tory dataset for training the teacher agent. Our method significantly reduces the
15"
ABSTRACT,0.014414414414414415,"resource-intensive interactions between agents and environments, and empirical
16"
ABSTRACT,0.015315315315315315,"experiments across various domains demonstrate the effectiveness of our approach.
17"
INTRODUCTION,0.016216216216216217,"1
Introduction
18"
INTRODUCTION,0.017117117117117116,"The advances of reinforcement learning (RL) [17] have promoted research into the problem of
19"
INTRODUCTION,0.018018018018018018,"training autonomous agents that are capable of accomplishing complex tasks. One interesting, yet
20"
INTRODUCTION,0.01891891891891892,"underexplored, area is training agents to perform well in unseen environments, a concept referred to
21"
INTRODUCTION,0.01981981981981982,"as zero-shot transfer performance. To this end, Unsupervised Environment Design (UED) [3] has
22"
INTRODUCTION,0.02072072072072072,"emerged as a promising paradigm to address this problem. The objective of UED is to automatically
23"
INTRODUCTION,0.021621621621621623,"generate environments in a curriculum-based manner, and training agents in these sequentially
24"
INTRODUCTION,0.02252252252252252,"generated environments can equip agents with general capabilities, enabling agents to learn robust
25"
INTRODUCTION,0.023423423423423424,"and adaptive behaviors that can be transferred to new scenarios without explicit exposure during
26"
INTRODUCTION,0.024324324324324326,"training.
27"
INTRODUCTION,0.025225225225225224,"Existing approaches in UED primarily focus on building an adaptive curriculum for the environment
28"
INTRODUCTION,0.026126126126126126,"generation process to train the generally capable agent. Dennis et al. [3] formalize the problem of
29"
INTRODUCTION,0.02702702702702703,"finding adaptive curricula through a game involving an adversarial environment generator (teacher
30"
INTRODUCTION,0.027927927927927927,"agent), an antagonist agent (expert agent), and the protagonist agent (student agent). The RL-based
31"
INTRODUCTION,0.02882882882882883,"teacher is designed to generate environments that maximize regret, defined as the difference between
32"
INTRODUCTION,0.02972972972972973,"the protagonist and antagonist agent’s expected rewards. They show that these agents will reach
33"
INTRODUCTION,0.03063063063063063,"a Nash Equilibrium where the student agent learns the minimax regret policy. However, since the
34"
INTRODUCTION,0.03153153153153153,"teacher agent adapts solely based on the regret feedback, it is inherently difficult to adapt to student
35"
INTRODUCTION,0.032432432432432434,"policy changes. Meanwhile, training such an RL-based teacher remains a challenge because of the
36"
INTRODUCTION,0.03333333333333333,"high computational cost of training an expert antagonist agent for each environment.
37"
INTRODUCTION,0.03423423423423423,"In contrast, domain randomization [19] based approaches circumvent the overhead of developing
38"
INTRODUCTION,0.03513513513513514,"an RL teacher by training agents in randomly generated environments, resulting in good empirical
39"
INTRODUCTION,0.036036036036036036,"performances. Building upon this, Jiang et al. [7] introduce an emergent curriculum by sampling
40"
INTRODUCTION,0.036936936936936934,"randomly generated environments with high regret value 1 to train the agent. Parker-Holder et al.
41"
INTRODUCTION,0.03783783783783784,"[10] then propose the adaptive curricula by manually designing a principled, regret-based curriculum,
42"
INTRODUCTION,0.03873873873873874,"which involves generating random environments with increasing complexity. While these domain
43"
INTRODUCTION,0.03963963963963964,"randomization-based algorithms have demonstrated good zero-shot transfer performance, they face
44"
INTRODUCTION,0.04054054054054054,"limitations in efficiently exploring large environment design spaces and exploiting the inherent
45"
INTRODUCTION,0.04144144144144144,"structure of previously discovered environments. Moreover, existing UED approaches typically
46"
INTRODUCTION,0.04234234234234234,"rely on open-ended learning, necessitating a long training horizon, which is unrealistic in the real
47"
INTRODUCTION,0.043243243243243246,"world due to resource constraints. Our goal is to develop a teacher policy capable of generating
48"
INTRODUCTION,0.044144144144144144,"environments that are perfectly matched to the current skill levels of student agents, thereby allowing
49"
INTRODUCTION,0.04504504504504504,"students to achieve optimal general capability within a strict budget for the number of environments
50"
INTRODUCTION,0.04594594594594595,"generated and within a shorter training time horizon.
51"
INTRODUCTION,0.04684684684684685,"In this paper, we address these challenges by introducing a novel, adaptive environment design
52"
INTRODUCTION,0.047747747747747746,"framework. The core idea involves using a hierarchical Markov Decision Process (MDP) to simul-
53"
INTRODUCTION,0.04864864864864865,"taneously formulate the evolution of an upper-level teacher agent, tasked with generating suitable
54"
INTRODUCTION,0.04954954954954955,"environments to train the lower-level student agent to achieve general capabilities. To accurately
55"
INTRODUCTION,0.05045045045045045,"guide the generation of environments at the frontier of the student agent’s current capabilities, we
56"
INTRODUCTION,0.051351351351351354,"propose approximating the student agent’s policy/capability by its performances across a set of diverse
57"
INTRODUCTION,0.05225225225225225,"evaluation environments, which acts as the state abstraction for the teacher’s decision-making process.
58"
INTRODUCTION,0.05315315315315315,"The transitions in the teacher’s state represent the trajectories of the student agent’s capability after
59"
INTRODUCTION,0.05405405405405406,"training in the generated environment. However, collecting experience for the upper-level teacher
60"
INTRODUCTION,0.054954954954954956,"agent is slow and resource-intensive, since each upper-level MDP transition evolves a complete
61"
INTRODUCTION,0.055855855855855854,"training cycle of the student agent on the generated environment. To accelerate the collection of
62"
INTRODUCTION,0.05675675675675676,"upper-level MDP experiences, we utilize advances in diffusion models that can generate new data
63"
INTRODUCTION,0.05765765765765766,"points capturing complex distribution properties, such as skewness and multi-modality, exhibited
64"
INTRODUCTION,0.05855855855855856,"in the collected dataset [11]. Specifically, we employ diffusion probabilistic model [15, 6] to learn
65"
INTRODUCTION,0.05945945945945946,"the evolution trajectory of student policy/capability and generate synthetic experiences to enhance
66"
INTRODUCTION,0.06036036036036036,"the training efficiency of the teacher agent. Our method, called Synthetically-enhanced Hierarchical
67"
INTRODUCTION,0.06126126126126126,"Environment Design (SHED), automatically generates increasingly complex environments suited to
68"
INTRODUCTION,0.062162162162162166,"the current capabilities of student agents.
69"
INTRODUCTION,0.06306306306306306,"In summary, we make the following contributions:
70"
INTRODUCTION,0.06396396396396396,"• We develop a novel hierarchical MDP framework for UED that introduces a straightforward method
71"
INTRODUCTION,0.06486486486486487,"to represent the current capability level of the student agent.
72"
INTRODUCTION,0.06576576576576576,"• We introduce SHED, which utilizes diffusion-based techniques to generate synthetic experiences.
73"
INTRODUCTION,0.06666666666666667,"This method can accelerate the training of the off-policy teacher agent.
74"
INTRODUCTION,0.06756756756756757,"• We demonstrate that our method outperforms existing UED approaches (i.e., achieving a better
75"
INTRODUCTION,0.06846846846846846,"general capability under resource constraints) in different task domains.
76"
PRELIMINARIES,0.06936936936936937,"2
Preliminaries
77"
PRELIMINARIES,0.07027027027027027,"In this section, we provide an overview of two main research areas upon which our work is based.
78"
UNSUPERVISED ENVIRONMENT DESIGN,0.07117117117117117,"2.1
Unsupervised Environment Design
79"
UNSUPERVISED ENVIRONMENT DESIGN,0.07207207207207207,"The objective of UED is to generate a sequence of environments that effectively train the student
agent to achieve a general capability. Dennis et al. [3] first model UED with an Underspecified
Partially Observable Markov Decision Process (UPOMDP), which is a tuple"
UNSUPERVISED ENVIRONMENT DESIGN,0.07297297297297298,"M =< A, O, Θ, SM, PM, IM, RM, γ >"
UNSUPERVISED ENVIRONMENT DESIGN,0.07387387387387387,1They approximate the regret value by the Generalized Advantage Estimate [12].
UNSUPERVISED ENVIRONMENT DESIGN,0.07477477477477477,". The UPOMDP has a set Θ representing the free parameters of the environments, which are
80"
UNSUPERVISED ENVIRONMENT DESIGN,0.07567567567567568,"determined by the teacher agent and can be distinct to generate the next new environment. Further,
81"
UNSUPERVISED ENVIRONMENT DESIGN,0.07657657657657657,"these parameters are incorporated into the environment-dependent transition function PM : S × A ×
82"
UNSUPERVISED ENVIRONMENT DESIGN,0.07747747747747748,"Θ →S. Here A represents the set of actions, S is the set of states. Similarly, IM : S →O is the
83"
UNSUPERVISED ENVIRONMENT DESIGN,0.07837837837837838,"environment-dependent observation function, RM is the reward function, and γ is the discount factor.
84"
UNSUPERVISED ENVIRONMENT DESIGN,0.07927927927927927,"Specifically, given the environment parameters ⃗θ ∈Θ, we denote the corresponding environment
85"
UNSUPERVISED ENVIRONMENT DESIGN,0.08018018018018018,"instance as M⃗θ. The student policy π is trained to maximize the cumulative rewards V M⃗θ(π) =
86
PT
t=0 γtrt in the given environment M⃗θ under a time horizon T, and rt are the collected rewards
87"
UNSUPERVISED ENVIRONMENT DESIGN,0.08108108108108109,"in M⃗θ. Existing works on UED consist of two main strands: the RL-based environment generation
88"
UNSUPERVISED ENVIRONMENT DESIGN,0.08198198198198198,"approach and the domain randomization-based environment generation approach.
89"
UNSUPERVISED ENVIRONMENT DESIGN,0.08288288288288288,"The RL-based generation approach was first formalized by Dennis et al. [3] as a self-supervised RL
90"
UNSUPERVISED ENVIRONMENT DESIGN,0.08378378378378379,"paradigm for generating environments. This approach involves co-evolving an environment generator
91"
UNSUPERVISED ENVIRONMENT DESIGN,0.08468468468468468,"policy (teacher) with an agent policy π (student), where the teacher’s role is to generate environment
92"
UNSUPERVISED ENVIRONMENT DESIGN,0.08558558558558559,"instances that best support the student agent’s continual learning. The teacher is trained to produce
93"
UNSUPERVISED ENVIRONMENT DESIGN,0.08648648648648649,"challenging yet solvable environments that maximize the regret measure, which is defined as the
94"
UNSUPERVISED ENVIRONMENT DESIGN,0.08738738738738738,"performance difference between the current student agent and a well-trained expert agent π∗within
95"
UNSUPERVISED ENVIRONMENT DESIGN,0.08828828828828829,"the current environment: RegretM⃗θ(π, π∗) = V M⃗θ(π∗) −V M⃗θ(π).
96"
UNSUPERVISED ENVIRONMENT DESIGN,0.0891891891891892,"The domain randomization-based generation approach, on the other hand, involves randomly generat-
97"
UNSUPERVISED ENVIRONMENT DESIGN,0.09009009009009009,"ing environments. Jiang et al. [7] propose to collect encountered environments with high learning
98"
UNSUPERVISED ENVIRONMENT DESIGN,0.09099099099099099,"potentials, which are approximated by the Generalized Advantage Estimation (GAE) [12], and then
99"
UNSUPERVISED ENVIRONMENT DESIGN,0.0918918918918919,"the student agent can selectively train in these environments, resulting in an emergent curriculum
100"
UNSUPERVISED ENVIRONMENT DESIGN,0.09279279279279279,"of increasing difficulty. Additionally, Parker-Holder et al. [10] adopt a different strategy by using
101"
UNSUPERVISED ENVIRONMENT DESIGN,0.0936936936936937,"predetermined starting points for the environment generation process and gradually increasing com-
102"
UNSUPERVISED ENVIRONMENT DESIGN,0.0945945945945946,"plexity. They manually divide the environment design space into different difficulty levels and employ
103"
UNSUPERVISED ENVIRONMENT DESIGN,0.09549549549549549,"human-defined edits to generate similar environments with high learning potentials. Their algorithm,
104"
UNSUPERVISED ENVIRONMENT DESIGN,0.0963963963963964,"ACCEL, is currently the state-of-the-art (SOTA) in the field, and we use an edited version of ACCEL
105"
UNSUPERVISED ENVIRONMENT DESIGN,0.0972972972972973,"as a baseline in our experiments.
106"
DIFFUSION PROBABILISTIC MODELS,0.0981981981981982,"2.2
Diffusion Probabilistic Models
107"
DIFFUSION PROBABILISTIC MODELS,0.0990990990990991,"Diffusion models [15] are a specific type of generative model that learns the data distribution.
108"
DIFFUSION PROBABILISTIC MODELS,0.1,"Recent advances in diffusion-based models, including Langevin dynamics and score-based generative
109"
DIFFUSION PROBABILISTIC MODELS,0.1009009009009009,"models, have shown promising results in various applications, such as time series forecasting [18],
110"
DIFFUSION PROBABILISTIC MODELS,0.1018018018018018,"robust learning [9], anomaly detection [21] as well as synthesizing high-quality images from text
111"
DIFFUSION PROBABILISTIC MODELS,0.10270270270270271,"descriptions [8, 11]. These models can be trained using standard optimization techniques, such as
112"
DIFFUSION PROBABILISTIC MODELS,0.1036036036036036,"stochastic gradient descent, making them highly scalable and easy to implement.
113"
DIFFUSION PROBABILISTIC MODELS,0.1045045045045045,"In a diffusion probabilistic model, we assume a d-dimensional random variable x0 ∈Rd with an
114"
DIFFUSION PROBABILISTIC MODELS,0.10540540540540541,"unknown distribution q(x0). Diffusion Probabilistic model involves two Markov chains: a predefined
115"
DIFFUSION PROBABILISTIC MODELS,0.1063063063063063,"forward chain q(xk|xk−1) that perturbs data to noise, and a trainable reverse chain pϕ(xk−1|xk) that
116"
DIFFUSION PROBABILISTIC MODELS,0.10720720720720721,"converts noise back to data. The forward chain is typically designed to transform any data distribution
117"
DIFFUSION PROBABILISTIC MODELS,0.10810810810810811,"into a simple prior distribution (e.g., standard Gaussian) by considering perturb data with Gaussian
118"
DIFFUSION PROBABILISTIC MODELS,0.109009009009009,"noise of zero mean and a fixed variance schedule {βk}K
k=1 for K steps:
119"
DIFFUSION PROBABILISTIC MODELS,0.10990990990990991,"q(xk|xk−1) = N(xk;
p"
DIFFUSION PROBABILISTIC MODELS,0.11081081081081082,"1 −βkxk−1, βtI)
and
q(x1:K|x0) = ΠK
k=1q(xk|xk−1),
(1)"
DIFFUSION PROBABILISTIC MODELS,0.11171171171171171,"where k ∈{1, . . . , K}, and 0 < β1:K < 1 denote the noise scale scheduling. As K →∞, xK
120"
DIFFUSION PROBABILISTIC MODELS,0.11261261261261261,"will converge to isometric Gaussian noise: xK →N(0, I). According to the rule of the sum of
121"
DIFFUSION PROBABILISTIC MODELS,0.11351351351351352,"normally distributed random variables, the choice of Gaussian noise provides a closed-form solution
122"
DIFFUSION PROBABILISTIC MODELS,0.11441441441441441,"to generate arbitrary time-step xk through:
123"
DIFFUSION PROBABILISTIC MODELS,0.11531531531531532,"xk = √¯αkx0 +
√"
DIFFUSION PROBABILISTIC MODELS,0.11621621621621622,"1 −¯αkϵ,
where
ϵ ∼N(0, I).
(2)"
DIFFUSION PROBABILISTIC MODELS,0.11711711711711711,"Here αk = 1 −βk and ¯αk = Qk
s=1 αs. The reverse chain pϕ(xk−1|xk) reverses the forward process
124"
DIFFUSION PROBABILISTIC MODELS,0.11801801801801802,"by learning transition kernels parameterized by deep neural networks. Specifically, considering the
125"
DIFFUSION PROBABILISTIC MODELS,0.11891891891891893,"Markov chain parameterized by ϕ, denoising arbitrary Gaussian noise into clean data samples can be
126"
DIFFUSION PROBABILISTIC MODELS,0.11981981981981982,"written as:
127"
DIFFUSION PROBABILISTIC MODELS,0.12072072072072072,"pϕ(xk−1|xk) = N(xk−1; µϕ(xk, k), Σϕ(xk, k))
(3)
It uses the Gaussian form pϕ(xk−1|xk) because the reverse process has the identical function form as
128"
DIFFUSION PROBABILISTIC MODELS,0.12162162162162163,"the forward process when βt is small [15]. Ho et al. [6] consider the following parameterization of
129"
DIFFUSION PROBABILISTIC MODELS,0.12252252252252252,"pϕ(xk−1|xk):
130"
DIFFUSION PROBABILISTIC MODELS,0.12342342342342343,"µϕ(xk, k) = 1 αk"
DIFFUSION PROBABILISTIC MODELS,0.12432432432432433,"
xk −
βk
√1 −αk
ϵϕ(xk, k)

and Σϕ(xk, k) = ˜β1/2
k
where ˜βk ="
DIFFUSION PROBABILISTIC MODELS,0.12522522522522522,( 1−αk−1
DIFFUSION PROBABILISTIC MODELS,0.12612612612612611,"1−αk βk
k > 1
β1
k = 1
(4)
ϵϕ is a trainable function to predict the noise vector ϵ from xk. Ho et al. [6] show that training
131"
DIFFUSION PROBABILISTIC MODELS,0.12702702702702703,"the reverse chain to maximize the log-likelihood
R
q(x0) log pϕ(x0)dx0 is equivalent to minimizing
132"
DIFFUSION PROBABILISTIC MODELS,0.12792792792792793,"re-weighted evidence lower bound (ELBO) that fits the noise. They derive the final simplified
133"
DIFFUSION PROBABILISTIC MODELS,0.12882882882882882,"optimization objective:
134"
DIFFUSION PROBABILISTIC MODELS,0.12972972972972974,"L(ϕ) = Ex0,k,ϵ

∥ϵ −ϵϕ(√¯αkx0 +
√"
DIFFUSION PROBABILISTIC MODELS,0.13063063063063063,"1 −¯αkϵ, k)∥2
.
(5)"
DIFFUSION PROBABILISTIC MODELS,0.13153153153153152,"Once the model is trained, new data points can be subsequently generated by first sampling a random
135"
DIFFUSION PROBABILISTIC MODELS,0.13243243243243244,"vector from the prior distribution, followed by ancestral sampling through the reverse Markov chain
136"
DIFFUSION PROBABILISTIC MODELS,0.13333333333333333,"in Equation 3.
137"
APPROACH,0.13423423423423422,"3
Approach
138"
APPROACH,0.13513513513513514,"In this section, we formally describe our method, Synthetically-enhanced Hierarchical Environment
139"
APPROACH,0.13603603603603603,"Design (SHED), which is a novel framework for UED under resource constraints. The SHED
140"
APPROACH,0.13693693693693693,"incorporates two key components that differentiate it from existing UED approaches:
141"
APPROACH,0.13783783783783785,"• A hierarchical MDP framework to generate suitable environments,
142"
APPROACH,0.13873873873873874,"• A generative model to generate the synthetic trajectories.
143"
APPROACH,0.13963963963963963,"SHED uses a hierarchical MDP framework where an RL teacher leverages the observed student’s
144"
APPROACH,0.14054054054054055,"policy representation to generate environments at the student’s capabilities frontier. Such targeted
145"
APPROACH,0.14144144144144144,"environment generation process enhances the student’s general capability by utilizing the underlying
146"
APPROACH,0.14234234234234233,"structure of previously discovered environments, rather than relying on the open-ended random
147"
APPROACH,0.14324324324324325,"generation. Besides, SHED leverages advances in generative models to generate synthetic trajectories
148"
APPROACH,0.14414414414414414,"that can be used to train the off-policy teacher agent, which significantly reduces the costly interactions
149"
APPROACH,0.14504504504504503,"between the agents and the environments. The overall framework is shown in Figure 1, and the
150"
APPROACH,0.14594594594594595,"pseudo-code is provided in Algorithm 1.
151"
HIERARCHICAL ENVIRONMENT DESIGN,0.14684684684684685,"3.1
Hierarchical Environment Design
152"
HIERARCHICAL ENVIRONMENT DESIGN,0.14774774774774774,"The objective is to generate a limited number of environments that are designed to enhance the general
153"
HIERARCHICAL ENVIRONMENT DESIGN,0.14864864864864866,"capability of the student agent. Inspired by the principles of PAIRED [3], we adopt an RL-based
154"
HIERARCHICAL ENVIRONMENT DESIGN,0.14954954954954955,"approach for the environment generation process. To better generate suitable environments tailored
155"
HIERARCHICAL ENVIRONMENT DESIGN,0.15045045045045044,"to the current student skill level, SHED uses the hierarchical MDP framework, consisting of an
156"
HIERARCHICAL ENVIRONMENT DESIGN,0.15135135135135136,"upper-level RL teacher policy Λ and a lower-level student policy π. Specifically, the teacher policy,
157"
HIERARCHICAL ENVIRONMENT DESIGN,0.15225225225225225,"Λ : Π →Θ, maps from the space of all potential student policies Π to the space of environment
158"
HIERARCHICAL ENVIRONMENT DESIGN,0.15315315315315314,"parameters Θ. Existing RL-based methods (e.g., PARIED) rely solely on regret feedback and
159"
HIERARCHICAL ENVIRONMENT DESIGN,0.15405405405405406,"fail to effectively capture the nuances of the student policy. To address this challenge, SHED
160"
HIERARCHICAL ENVIRONMENT DESIGN,0.15495495495495495,"enhances understanding by encoding the student policy π into a vector that serves as the state
161"
ABSTRACT,0.15585585585585585,"abstraction for teacher Λ. Rather than compressing the knowledge in the student policy network, we
162"
ABSTRACT,0.15675675675675677,"approximate the embedding of the student policy π by assessing performance across a set of diverse
163"
ABSTRACT,0.15765765765765766,"evaluation environments. This performance vector, denoted as p(π), gives us a practical estimate
164"
ABSTRACT,0.15855855855855855,"of the student’s current general capabilities, enabling the teacher to customize the next training
165"
ABSTRACT,0.15945945945945947,"environments accordingly. In our hierarchical framework, the environment generation process is
166"
ABSTRACT,0.16036036036036036,"governed by discrete-time dynamics. We delve into the specifics below.
167"
ABSTRACT,0.16126126126126125,"Upper-level teacher MDP. The upper-level teacher operates at a coarser layer of student policy
168"
ABSTRACT,0.16216216216216217,"abstraction and generates environments to train the lower-level student agent. This process can be
169"
ABSTRACT,0.16306306306306306,"formally modeled as an MDP by the tuple < Su, Au, P u, Ru, γu >:
170"
ABSTRACT,0.16396396396396395,"• Su represents the upper-level state space. Typically, su = p(π) = [p1, . . . , pm] denotes the
171"
ABSTRACT,0.16486486486486487,"student performance vector across m diverse evaluation environments. This vector serves as the
172"
ABSTRACT,0.16576576576576577,"representation of the student policy π and is observed by the teacher.
173"
ABSTRACT,0.16666666666666666,"Algorithm 1 SHED
Input: real data ratio ψ ∈[0, 1], evaluate environment
set θeval, reward function R;
1: Initialize: diffusion model D, teacher policy Λ,
real and synthetic replay buffer Breal, Bsyn = ∅;
2: for episode ep = 1, . . . , K do
3:
Initialize student policy π
4:
Evaluate π on θeval and get state su = p(π)
5:
for Budget t = 1, . . . , T do
6:
generate ⃗θ ∼Λ, and create M⃗θ(π)"
ABSTRACT,0.16756756756756758,"7:
train π on M⃗θ to maximize V ⃗θ(π)
8:
evaluate π on θeval and get next state s′"
ABSTRACT,0.16846846846846847,"9:
compute teacher’s reward rt according to R
10:
add experience (su
t , ⃗θ, ru
t , su,′
t ) to Breal
11:
train D with samples from Breal
12:
generate synthetic experiences from D and
add them to Bsyn
13:
train Λ on samples from Breal
S Bsyn mixed
with ratio ψ
14:
set s = s′;
15:
end for
16: end for
Output: Λ, π, D"
ABSTRACT,0.16936936936936936,Figure 1: The overall framework of SHED.
ABSTRACT,0.17027027027027028,"Figure 2: The illustration of the environment
generation process."
ABSTRACT,0.17117117117117117,"• Au is the upper-level action space. The teacher observes the abstraction of the student policy,
174"
ABSTRACT,0.17207207207207206,"su and produces an upper-level action au which is the environment parameters ⃗θ. ⃗θ (au) is then
175"
ABSTRACT,0.17297297297297298,"used to generate specific environment instances M⃗θ. Thus the upper-level action space Au is the
176"
ABSTRACT,0.17387387387387387,"environment parameter space Θ.
177"
ABSTRACT,0.17477477477477477,"• P u denotes the action-dependent transition dynamics of the upper-level state. The general capability
178"
ABSTRACT,0.17567567567567569,"of the student policy evolves due to training the student agent on the generated environments.
179"
ABSTRACT,0.17657657657657658,"• Ru provides the upper-level reward to the teacher at the end of training the student on the generated
180"
ABSTRACT,0.17747747747747747,"environment. The design of Ru will be discussed in Section 3.3.
181"
ABSTRACT,0.1783783783783784,"As shown in Figure 2, given the student policy π, the teacher Λ first observes the representation
182"
ABSTRACT,0.17927927927927928,"of the student policy, su = [p1, . . . , pm]. Then teacher produces an upper-level action au which
183"
ABSTRACT,0.18018018018018017,"corresponds to the environment parameters. These environment parameters are subsequently used
184"
ABSTRACT,0.1810810810810811,"to generate specific environment instances. The lower-level student policy π will be trained on the
185"
ABSTRACT,0.18198198198198198,"generated environments for C training steps. The upper-level teacher collects and stores the student
186"
ABSTRACT,0.18288288288288287,"policy evolution transition (su, au, ru, su,′) every C times steps for off-policy training. The teacher
187"
ABSTRACT,0.1837837837837838,"agent is trained to maximize the cumulative reward giving the budget for the number of generated
188"
ABSTRACT,0.18468468468468469,"environments. The choice of the evaluation environments will be discussed in Section 3.3.
189"
ABSTRACT,0.18558558558558558,"Lower-level student MDP. The generated environment is fully specified for the student, characterized
190"
ABSTRACT,0.1864864864864865,"by a Partially Observable Markov Decision Process (POMDP), which is defined by a tuple M⃗θ =<
191"
ABSTRACT,0.1873873873873874,"A, O, S⃗θ, P⃗θ, I⃗θ, R⃗θ, γ >, where A represents the set of actions, O is the set of observations, S⃗θ
192"
ABSTRACT,0.18828828828828828,"is the set of states determined by the environment parameters ⃗θ, similarly, P⃗θ is the environment-
193"
ABSTRACT,0.1891891891891892,"dependent transition function, and I⃗θ : ⃗θ →O is the environment-dependent observation function,
194"
ABSTRACT,0.1900900900900901,"R⃗θ is the reward function, and γ is the discount factor. At each time step t, the environment produces a
195"
ABSTRACT,0.19099099099099098,"state observation st ∈S⃗θ, the student agent samples the action at ∼A and interacts with environment
196"
ABSTRACT,0.1918918918918919,"⃗θ. The environment yields a reward rt according to the reward function R⃗θ. The student agent is
197"
ABSTRACT,0.1927927927927928,"trained to maximize their cumulative reward V ⃗θ(π) = PC
t=0 γtrt for the current environment under
198"
ABSTRACT,0.19369369369369369,"a finite time horizon C. The student agent will learn a good general capability from training on a
199"
ABSTRACT,0.1945945945945946,"sequence of generated environments.
200"
ABSTRACT,0.1954954954954955,"The hierarchical framework enables the teacher agent to systematically measure and enhance the
201"
ABSTRACT,0.1963963963963964,"general capability of the student agent and to adapt the training process accordingly. However, it’s
202"
ABSTRACT,0.1972972972972973,"worth noting that collecting student policy evolution trajectories (su, au, ru, su,′) to train the teacher
203"
ABSTRACT,0.1981981981981982,"agent is notably slow and resource-intensive, since each transition in the upper-level teacher MDP
204"
ABSTRACT,0.1990990990990991,"encompasses a training horizon of C timesteps for the student in the generated environment. Thus, it
205"
ABSTRACT,0.2,"is essential to reduce the need for costly collection of upper-level teacher experiences.
206"
GENERATIVE TRAJECTORY MODELING,0.2009009009009009,"3.2
Generative Trajectory Modeling
207"
GENERATIVE TRAJECTORY MODELING,0.2018018018018018,"In this section, we will formally introduce a generative model designed to ease the collection of upper-
208"
GENERATIVE TRAJECTORY MODELING,0.20270270270270271,"level MDP experience. This will allow us to train our teacher policy more efficiently. In particular, we
209"
GENERATIVE TRAJECTORY MODELING,0.2036036036036036,"first utilize a diffusion model to learn the conditional data distribution from the collected experiences
210"
GENERATIVE TRAJECTORY MODELING,0.2045045045045045,"τ = {(su
t , au
t , ru
t , sp,′
t )}. Later we can use the reverse chain in the diffusion model to generate the
211"
GENERATIVE TRAJECTORY MODELING,0.20540540540540542,"synthetic trajectories that can be used to help train the teacher agent, thereby alleviating the need
212"
GENERATIVE TRAJECTORY MODELING,0.2063063063063063,"for extensive and time-consuming collection of upper-level teacher experiences. We deal with two
213"
GENERATIVE TRAJECTORY MODELING,0.2072072072072072,"different types of timesteps in this section: one for the diffusion process and the other for the upper-
214"
GENERATIVE TRAJECTORY MODELING,0.20810810810810812,"level teacher agent, respectively. We use subscripts k ∈1, . . . , K to represent diffusion timesteps
215"
GENERATIVE TRAJECTORY MODELING,0.209009009009009,"and subscripts t ∈1, . . . , T to represent trajectory timesteps in the teacher’s experience.
216"
GENERATIVE TRAJECTORY MODELING,0.2099099099099099,"In the image domain, the diffusion process is implemented across all pixel values of the image. In our
217"
GENERATIVE TRAJECTORY MODELING,0.21081081081081082,"setting, we diffuse over the next state su,′ conditioned the given state su and action au. We construct
218"
GENERATIVE TRAJECTORY MODELING,0.21171171171171171,"our generative model according to the conditional diffusion process:
219"
GENERATIVE TRAJECTORY MODELING,0.2126126126126126,"q(su,′
k |su,′
k−1),
pϕ(su,′
k−1|su,′
k , su, au)"
GENERATIVE TRAJECTORY MODELING,0.21351351351351353,"As usual, q(su,′
k |su,′
k−1) is the predefined forward noising process while pϕ(su,′
k−1|su,′
k , su, au) is the
220"
GENERATIVE TRAJECTORY MODELING,0.21441441441441442,"trainable reverse denoising process. We begin by randomly sampling the collected experiences
221"
GENERATIVE TRAJECTORY MODELING,0.2153153153153153,"τ = {(su
t , au
t , ru
t , su,′
t )} from the real experience buffer Breal. Giving the observed state su and
222"
GENERATIVE TRAJECTORY MODELING,0.21621621621621623,"action au, we use the reverse process pϕ to represent the generation of the next state su,′:
223"
GENERATIVE TRAJECTORY MODELING,0.21711711711711712,"pϕ(su,′
0:K|su, au) = N(su,′
K ; 0, I) K
Y"
GENERATIVE TRAJECTORY MODELING,0.218018018018018,"k=1
pϕ(su,′
k−1|su,′
k , su, au)"
GENERATIVE TRAJECTORY MODELING,0.21891891891891893,"At the end of the reverse chain, the sample su,′
0 , is the generated next state su,′. Similar to Ho et al.
[6], we parameterize pϕ(s′
k−1|s′
k, su, au) as a noise prediction model with the covariance matrix
fixed as Σϕ(su,′
k , su, au, k) = βiI, and the mean is"
GENERATIVE TRAJECTORY MODELING,0.21981981981981982,"µϕ(su,′
i , su, au, k) =
1
√αk"
GENERATIVE TRAJECTORY MODELING,0.22072072072072071,"
su,′
k −
βk
√1 −¯αk
ϵϕ(su,′
k , su, au, k)
"
GENERATIVE TRAJECTORY MODELING,0.22162162162162163,"ϵϕ(su,′
k , su, au, k) is the trainable denoising function, which aims to estimate the noise ϵ in the noisy
224"
GENERATIVE TRAJECTORY MODELING,0.22252252252252253,"input su,′
k
at step k.
225"
GENERATIVE TRAJECTORY MODELING,0.22342342342342342,"Training objective. We employ a similar simplified objective to train the conditional ϵ- model:
226"
GENERATIVE TRAJECTORY MODELING,0.22432432432432434,"L(ϕ) = E(su,au,su,′)∼τ,k∼U,ϵ∼N(0,I)

∥ϵ −ϵϕ(su,′
k , su, au, k)∥2
(6)"
GENERATIVE TRAJECTORY MODELING,0.22522522522522523,"Where su,′
k
= √¯αksu,′ + √1 −¯αkϵ. The intuition for the loss function L(ϕ) is to predict the noise
227"
GENERATIVE TRAJECTORY MODELING,0.22612612612612612,"ϵ ∼N(0, I) at the denoising step k, and the diffusion model is essentially learning the student
228"
GENERATIVE TRAJECTORY MODELING,0.22702702702702704,"policy involution trajectories collected in the real experience buffer Breals. Note that the reverse
229"
GENERATIVE TRAJECTORY MODELING,0.22792792792792793,"process necessitates a substantial number of steps K [15]. Recent research by Xiao et al. [22] has
230"
GENERATIVE TRAJECTORY MODELING,0.22882882882882882,"demonstrated that enabling denoising with large steps can reduce the total number of denoising steps
231"
GENERATIVE TRAJECTORY MODELING,0.22972972972972974,"K. To expedite the relatively slow reverse sampling process (as it requires computing ϵϕ networks
232"
GENERATIVE TRAJECTORY MODELING,0.23063063063063063,"K times), we use a small value of K. Similar to Wang et al. [20], while simultaneously setting
233"
GENERATIVE TRAJECTORY MODELING,0.23153153153153153,"βmin = 0.1 and βmax = 10.0, we define:
234"
GENERATIVE TRAJECTORY MODELING,0.23243243243243245,"βk = 1 −exp

βmin × 1"
GENERATIVE TRAJECTORY MODELING,0.23333333333333334,K −0.5(βmax −βmin)2k −1 K2 
GENERATIVE TRAJECTORY MODELING,0.23423423423423423,"This noise schedule is derived from the variance-preserving Stochastic Differential Equation by Song
235"
GENERATIVE TRAJECTORY MODELING,0.23513513513513515,"et al. [16].
236"
GENERATIVE TRAJECTORY MODELING,0.23603603603603604,"Generate synthetic trajectories.Once the diffusion model has been trained, it can be used to generate
237"
GENERATIVE TRAJECTORY MODELING,0.23693693693693693,"synthetic experience data by starting with a draw from the prior su,′
K ∼N(0, I) and successively
238"
GENERATIVE TRAJECTORY MODELING,0.23783783783783785,"generating denoised next state, conditioned on the given su and au through the reverse chain pϕ.
239"
GENERATIVE TRAJECTORY MODELING,0.23873873873873874,"Note that the giving condition action a can either be randomly sampled from the action space or use
240"
GENERATIVE TRAJECTORY MODELING,0.23963963963963963,"another diffusion model to learn the action distribution giving the initial state su. This new diffusion
241"
GENERATIVE TRAJECTORY MODELING,0.24054054054054055,"model is essentially a behavior-cloning model that aims to learn the teacher policy Λ(au|su). This
242"
GENERATIVE TRAJECTORY MODELING,0.24144144144144145,"process is similar to the work of Wang et al. [20]. We discuss this process in detail in the appendix.
243"
GENERATIVE TRAJECTORY MODELING,0.24234234234234234,"In this paper, we randomly sample au as it is straightforward and can also increase the diversity in
244"
GENERATIVE TRAJECTORY MODELING,0.24324324324324326,"the generated synthetic experience to help train a more robust teacher agent.
245"
GENERATIVE TRAJECTORY MODELING,0.24414414414414415,"After obtaining the generated next state su,′ conditioned on su, au, we compute reward ru using
246"
GENERATIVE TRAJECTORY MODELING,0.24504504504504504,"teacher’s reward function R(su, au, su,′). The specifics of how the reward function is chosen are
247"
GENERATIVE TRAJECTORY MODELING,0.24594594594594596,"explained in the following section.
248"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.24684684684684685,"3.3
Rewards and Choice of evaluate environments
249"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.24774774774774774,"Selection of evaluation environments. The upper-level teacher generates environments tailored
250"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.24864864864864866,"for the lower-level student to improve its general capability. Thus it is important to select a set of
251"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.24954954954954955,"diverse suitable evaluation environments as the performance vector reflects the student agent’s general
252"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.25045045045045045,"capabilities and serves as an approximation of the policy’s embedding. Fontaine and Nikolaidis
253"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.25135135135135134,"[5] propose the use of quality diversity (QD) optimization to collect high-quality environments that
254"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.25225225225225223,"exhibit diversity for the agent behaviors. Similarly, Bhatt et al. [1] introduce a QD-based algorithm for
255"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.2531531531531532,"dynamically designing such evaluation environments based on the current agent’s behavior. However,
256"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.25405405405405407,"it’s worth noting that this QD-based approach can be tedious and time-consuming, and the collected
257"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.25495495495495496,"evaluation environments heavily rely on the given agent policy.
258"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.25585585585585585,"Given these considerations, it is natural to take advantage of the domain randomization algorithm,
259"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.25675675675675674,"as it has demonstrated compelling results in generating diverse environments and training generally
260"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.25765765765765763,"capable agents. In our approach, we first discretize the environment parameters into different ranges,
261"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.2585585585585586,"then randomly sample from these ranges, and combine these parameters to generate evaluation
262"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.2594594594594595,"environments. This method can generate environments that may induce a diverse performance for the
263"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.26036036036036037,"same policy, and it shows promising empirical results in the final experiments.
264"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.26126126126126126,"Reward design. We define the reward function for the upper-level teacher policy as a parameterized
265"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.26216216216216215,"function based on the improvement in student performance in the evaluation environments after
266"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.26306306306306304,"training in the generated environment:
267"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.263963963963964,"R(su, au, su,′) = m
X"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.2648648648648649,"i=1
(p′
i −pi)"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.26576576576576577,"This reward function gives positive rewards to the upper-level teacher for taking action to create
268"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.26666666666666666,"the right environment to improve the overall performance of students across diverse environments.
269"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.26756756756756755,"However, it may encourage the teacher to obtain higher rewards by sacrificing student performance
270"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.26846846846846845,"in one subset of evaluation environments to improve student performance in another subset, which
271"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.2693693693693694,"conflicts with our objective to develop a student agent with general capabilities. Therefore, we need
272"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.2702702702702703,"to consider fairness in the reward function to ensure that the generated environment can improve
273"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.2711711711711712,"student’s general capabilities. Similar to [4], we build our fairness metric on top of the change
274"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.27207207207207207,"in student’s performance in each evaluation environment, denoted as ωi = p′
i −pi, and we have
275"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.27297297297297296,"¯ω =
1
m
Pm
i=1 ωi. We then measure the fairness of the teacher’s action using the coefficient of
276"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.27387387387387385,"variation of student performances:
277"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.2747747747747748,"cv(su, au, su,′) = s"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.2756756756756757,"1
m −1 X i"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.2765765765765766,(ωi −¯ω)2
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.2774774774774775,"¯ω2
(7)"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.27837837837837837,"A teacher is considered to be fair if and only if the cv is smaller. As a result, our reward function is:
278"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.27927927927927926,"R(su, au, su,′) = m
X"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.2801801801801802,"i=1
(p′
i −pi) −η · cv(su, au, su,′)
(8)"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.2810810810810811,"Here η is the coefficient that balances the weight of fairness in the reward function (We set a small
279"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.281981981981982,"value to η). This reward function motivates the teacher to generate training environments that can
280"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.2828828828828829,"improve student’s general capability.
281"
REWARDS AND CHOICE OF EVALUATE ENVIRONMENTS,0.28378378378378377,"Figure 3: Left: The average zero-shot transfer performances on the test environments in the Lunar
lander environment (mean and standard error). Right: The average zero-shot transfer performances
on the test environments in the BipedalWalker (mean and standard error)."
EXPERIMENTS,0.28468468468468466,"4
Experiments
282"
EXPERIMENTS,0.2855855855855856,"In this section, we conduct experiments to compare SHED to other leading approaches on three
283"
EXPERIMENTS,0.2864864864864865,"domains: Lunar Lander, maze and a modified BipedalWalker environment. Experimental details and
284"
EXPERIMENTS,0.2873873873873874,"hyperparameters can be found in the Appendix. Specifically, our primary comparisons involve SHED
285"
EXPERIMENTS,0.2882882882882883,"and h-MDP (our proposed hierarchical approach without diffusion model aiding in training) against
286"
EXPERIMENTS,0.2891891891891892,"four baselines: domain randomization [19], ACCEL, [10], Edited ACCEL(with slight modifications
287"
EXPERIMENTS,0.29009009009009007,"that it does not revisit the previously generated environments), PAIRED [3]. In all cases, we
288"
EXPERIMENTS,0.290990990990991,"train a student agent via Proximal Policy Optimization (PPO [13], and train the teacher agent via
289"
EXPERIMENTS,0.2918918918918919,"Deterministic policy gradient algorithms(DDPG [14]), because DDPG is an off-policy algorithm and
290"
EXPERIMENTS,0.2927927927927928,"can learn from both real experiences and the synthetic experiences.
291"
EXPERIMENTS,0.2936936936936937,"Setup. For each domain, we construct a set of evaluation environments and a set of test environments.
292"
EXPERIMENTS,0.2945945945945946,"The vector of student performances in the evaluation environments is used as the approximation of
293"
EXPERIMENTS,0.2954954954954955,"the student policy (as the observation to teacher agent), and the performances in the test environments
294"
EXPERIMENTS,0.2963963963963964,"are used to represent the student’s zero-shot transfer performances (general capabilities). Note that in
295"
EXPERIMENTS,0.2972972972972973,"order to obtain a fair comparison of zero-shot transfer performance, the evaluation environments and
296"
EXPERIMENTS,0.2981981981981982,"test environments do not share the same environment and they are not present during training.
297"
EXPERIMENTS,0.2990990990990991,"Lunar Lander.
This is a classic rocket trajectory optimization problem. In this domain, student
298"
EXPERIMENTS,0.3,"agents are tasked with controlling a lander’s engine to safely land the vehicle. Before the start of each
299"
EXPERIMENTS,0.3009009009009009,"episode, teacher algorithms determine the environment parameters that are used to generate environ-
300"
EXPERIMENTS,0.30180180180180183,"ments in a given play-through, which includes gravity, wind power, and turbulence power. These
301"
EXPERIMENTS,0.3027027027027027,"parameters directly alter the difficulty of landing the vehicle safely. The state is an 8-dimensional
302"
EXPERIMENTS,0.3036036036036036,"vector, which includes the coordinates of the lander, its linear velocities, its angle, its angular velocity,
303"
EXPERIMENTS,0.3045045045045045,"and two booleans that represent whether each leg is in contact with the ground or not.
304"
EXPERIMENTS,0.3054054054054054,"We train the student agent for 1e6 environment time steps and periodically test the agent in test
305"
EXPERIMENTS,0.3063063063063063,"environments. The parameters for the test environments are randomly generated and fixed during
306"
EXPERIMENTS,0.30720720720720723,"training. We report the experiment results on the left side of Figure 3. As we can see, student
307"
EXPERIMENTS,0.3081081081081081,"agents trained under SHED consistently outperform other baselines and have minimal variance in
308"
EXPERIMENTS,0.309009009009009,"transfer performance. During training, the baselines, except h-MDP, show a performance dip in the
309"
EXPERIMENTS,0.3099099099099099,"middle. This phenomenon could potentially be attributed to the inherent challenge of designing the
310"
EXPERIMENTS,0.3108108108108108,"appropriate environment instance in the large environment parameter space. This further demonstrates
311"
EXPERIMENTS,0.3117117117117117,"the effectiveness of our hierarchical design (SHED and h-MDP), which can successfully create
312"
EXPERIMENTS,0.31261261261261264,"environments that are appropriate to the current skill level of the students.
313"
EXPERIMENTS,0.31351351351351353,"Bipedalwalker.
We also evaluate SHED in the modified BipedalWalker from Parker-Holder et al.
314"
EXPERIMENTS,0.3144144144144144,"[10]. In this domain, the student agent is required to control a bipedal vehicle and navigate across the
315"
EXPERIMENTS,0.3153153153153153,"terrain, and the student receives a 24-dimensional proprioceptive state with respect to its lidar sensors,
316"
EXPERIMENTS,0.3162162162162162,"angles, and contacts. The teacher is tasked to select eight variables (including ground roughness, the
317"
EXPERIMENTS,0.3171171171171171,"number of stairs steps, min/max range of pit gap width, min/max range of stump height, and min/max
318"
EXPERIMENTS,0.31801801801801804,"range of stair height) to generate the corresponding terrain.
319"
EXPERIMENTS,0.31891891891891894,"We use similar experiment settings in prior UED works, we train all the algorithms for 1e7 environ-
320"
EXPERIMENTS,0.31981981981981983,"ment time steps, and then evaluate their generalization ability on ten distinct test environments in
321"
EXPERIMENTS,0.3207207207207207,"Bipedal-Walker domain. The parameters for the test environments are randomly generated and fixed
322"
EXPERIMENTS,0.3216216216216216,"during training. As shown in Figure 3, our proposed method SHED surpasses all other baselines and
323"
EXPERIMENTS,0.3225225225225225,"achieves performance levels nearly on par with the SOTA (ACCEL). Meanwhile, SHED maintains a
324"
EXPERIMENTS,0.32342342342342345,"slight edge in terms of stability and overall performance and PAIRED suffers from a considerable
325"
EXPERIMENTS,0.32432432432432434,"degree of variance in its performance.
326"
EXPERIMENTS,0.32522522522522523,"Partially observable Maze.
Here we study navigation tasks, where an agent must explore to find a
327"
EXPERIMENTS,0.3261261261261261,"goal while navigating around obstacles. The environment is partially observable, and the agent’s field
328"
EXPERIMENTS,0.327027027027027,"of view is limited to a 3 × 3 grid area. Unlike the previously mentioned domains, maze environments
329"
EXPERIMENTS,0.3279279279279279,"are non-parametric and cannot be directly represented by compact parameter vectors due to their
330"
EXPERIMENTS,0.32882882882882886,"high complexity. To solve this challenge, we propose a novel method to generate maze by leveraging
331"
EXPERIMENTS,0.32972972972972975,"advances in large language models (e.g., ChatGPT). Specifically, we implement a retrieval-augmented
332"
EXPERIMENTS,0.33063063063063064,"generation (RAG) process to optimize the ChatGPT’s output such that it can generate desired maze
333"
EXPERIMENTS,0.33153153153153153,"environments. This process ensures that large language models reference authoritative knowledge
334"
EXPERIMENTS,0.3324324324324324,"bases to generate feasible mazes. To simplify the teacher’s action space, we extracted several key
335"
EXPERIMENTS,0.3333333333333333,"factors that constitute the teacher’s action space (environmental parameters) for maze generation.
336"
EXPERIMENTS,0.3342342342342342,"Details on maze generation are provided in Appendix D.3, and prompt are included in Appendix D.4.
337"
EXPERIMENTS,0.33513513513513515,"Figure 4: Average zero-shot transfer performance on the test
environments in the maze environments."
EXPERIMENTS,0.33603603603603605,"The average zero-shot transfer perfor-
338"
EXPERIMENTS,0.33693693693693694,"mances are reported in Figure 4. No-
339"
EXPERIMENTS,0.33783783783783783,"tably, SHED demonstrates the highest
340"
EXPERIMENTS,0.3387387387387387,"performance, consistently improving
341"
EXPERIMENTS,0.3396396396396396,"and achieving the highest cumulative
342"
EXPERIMENTS,0.34054054054054056,"rewards. The performance of h-MDP
343"
EXPERIMENTS,0.34144144144144145,"steadily improves but does not reach
344"
EXPERIMENTS,0.34234234234234234,"the highest levels, which further high-
345"
EXPERIMENTS,0.34324324324324323,"lights the advantages of incorporat-
346"
EXPERIMENTS,0.3441441441441441,"ing the generated synthetic datasets
347"
EXPERIMENTS,0.345045045045045,"to train an effective RL teacher agent.
348"
EXPERIMENTS,0.34594594594594597,"Meanwhile, Accel-Edit and Accel
349"
EXPERIMENTS,0.34684684684684686,"show higher variances in performance,
350"
EXPERIMENTS,0.34774774774774775,"indicating that random teachers are
351"
EXPERIMENTS,0.34864864864864864,"less stable in finding a suitable envi-
352"
EXPERIMENTS,0.34954954954954953,"ronment to train student agents.
353"
EXPERIMENTS,0.3504504504504504,"Ablation and additional Experi-
354"
EXPERIMENTS,0.35135135135135137,"ments
In Appendix C, we evaluate
355"
EXPERIMENTS,0.35225225225225226,"the ability of the diffusion model to generate the synthetic student policy involution trajectories. We
356"
EXPERIMENTS,0.35315315315315315,"further provide ablation studies to assess the impact of different design choices in Appendix E.1.
357"
EXPERIMENTS,0.35405405405405405,"Additionally, in Appendix E.2, we conduct experiments to show how the algorithm performs under
358"
EXPERIMENTS,0.35495495495495494,"different settings, including scenarios with a larger budget constraint on the number of generated
359"
EXPERIMENTS,0.35585585585585583,"environments or a larger weight assigned to CV fairness rewards. Notably, all results consistently
360"
EXPERIMENTS,0.3567567567567568,"demonstrate the effectiveness of our approach.
361"
CONCLUSION,0.35765765765765767,"5
Conclusion
362"
CONCLUSION,0.35855855855855856,"In this paper, we introduce an adaptive approach for efficiently training a generally capable agent
363"
CONCLUSION,0.35945945945945945,"under resource constraints. Our approach is general, utilizing an upper-level MDP teacher agent
364"
CONCLUSION,0.36036036036036034,"that can guide the training of the lower-level MDP student agent agent. The hierarchical framework
365"
CONCLUSION,0.36126126126126124,"can incorporate techniques from existing UED works, such as prioritized level replay (revisiting
366"
CONCLUSION,0.3621621621621622,"environments with high learning potential). Furthermore, we have described a method to assist the
367"
CONCLUSION,0.3630630630630631,"experience collection for the teacher when it is trained in an off-policy manner. Our experiment
368"
CONCLUSION,0.36396396396396397,"demonstrates that our method outperforms existing UED methods, highlighting its effectiveness as a
369"
CONCLUSION,0.36486486486486486,"curriculum-based learning approach within the UED framework.
370"
REFERENCES,0.36576576576576575,"References
371"
REFERENCES,0.36666666666666664,"[1] Varun Bhatt, Bryon Tjanaka, Matthew Fontaine, and Stefanos Nikolaidis. Deep surrogate
372"
REFERENCES,0.3675675675675676,"assisted generation of environments. Advances in Neural Information Processing Systems, 35:
373"
REFERENCES,0.3684684684684685,"37762–37777, 2022.
374"
REFERENCES,0.36936936936936937,"[2] Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes,
375"
REFERENCES,0.37027027027027026,"Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative
376"
REFERENCES,0.37117117117117115,"interactive environments. arXiv preprint arXiv:2402.15391, 2024.
377"
REFERENCES,0.37207207207207205,"[3] Michael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre Bayen, Stuart Russell, Andrew
378"
REFERENCES,0.372972972972973,"Critch, and Sergey Levine. Emergent complexity and zero-shot transfer via unsupervised
379"
REFERENCES,0.3738738738738739,"environment design. Advances in neural information processing systems, 33:13049–13061,
380"
REFERENCES,0.3747747747747748,"2020.
381"
REFERENCES,0.37567567567567567,"[4] Salma Elmalaki. Fair-iot: Fairness-aware human-in-the-loop reinforcement learning for har-
382"
REFERENCES,0.37657657657657656,"nessing human variability in personalized iot. In Proceedings of the International Conference
383"
REFERENCES,0.37747747747747745,"on Internet-of-Things Design and Implementation, pages 119–132, 2021.
384"
REFERENCES,0.3783783783783784,"[5] Matthew Fontaine and Stefanos Nikolaidis. Differentiable quality diversity. Advances in Neural
385"
REFERENCES,0.3792792792792793,"Information Processing Systems, 34:10040–10052, 2021.
386"
REFERENCES,0.3801801801801802,"[6] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances
387"
REFERENCES,0.3810810810810811,"in neural information processing systems, 33:6840–6851, 2020.
388"
REFERENCES,0.38198198198198197,"[7] Minqi Jiang, Edward Grefenstette, and Tim Rocktäschel. Prioritized level replay. In Interna-
389"
REFERENCES,0.38288288288288286,"tional Conference on Machine Learning, pages 4940–4950. PMLR, 2021.
390"
REFERENCES,0.3837837837837838,"[8] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,
391"
REFERENCES,0.3846846846846847,"Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing
392"
REFERENCES,0.3855855855855856,"with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.
393"
REFERENCES,0.3864864864864865,"[9] Weili Nie, Brandon Guo, Yujia Huang, Chaowei Xiao, Arash Vahdat, and Anima Anandkumar.
394"
REFERENCES,0.38738738738738737,"Diffusion models for adversarial purification. arXiv preprint arXiv:2205.07460, 2022.
395"
REFERENCES,0.38828828828828826,"[10] Jack Parker-Holder, Minqi Jiang, Michael Dennis, Mikayel Samvelyan, Jakob Foerster, Edward
396"
REFERENCES,0.3891891891891892,"Grefenstette, and Tim Rocktäschel. Evolving curricula with regret-based environment design.
397"
REFERENCES,0.3900900900900901,"arXiv preprint arXiv:2203.01302, 2022.
398"
REFERENCES,0.390990990990991,"[11] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton,
399"
REFERENCES,0.3918918918918919,"Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.
400"
REFERENCES,0.3927927927927928,"Photorealistic text-to-image diffusion models with deep language understanding. Advances in
401"
REFERENCES,0.39369369369369367,"Neural Information Processing Systems, 35:36479–36494, 2022.
402"
REFERENCES,0.3945945945945946,"[12] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-
403"
REFERENCES,0.3954954954954955,"dimensional continuous control using generalized advantage estimation.
arXiv preprint
404"
REFERENCES,0.3963963963963964,"arXiv:1506.02438, 2015.
405"
REFERENCES,0.3972972972972973,"[13] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal
406"
REFERENCES,0.3981981981981982,"policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
407"
REFERENCES,0.3990990990990991,"[14] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
408"
REFERENCES,0.4,"Deterministic policy gradient algorithms. In International conference on machine learning,
409"
REFERENCES,0.4009009009009009,"pages 387–395. Pmlr, 2014.
410"
REFERENCES,0.4018018018018018,"[15] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper-
411"
REFERENCES,0.4027027027027027,"vised learning using nonequilibrium thermodynamics. In International conference on machine
412"
REFERENCES,0.4036036036036036,"learning, pages 2256–2265. PMLR, 2015.
413"
REFERENCES,0.4045045045045045,"[16] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and
414"
REFERENCES,0.40540540540540543,"Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv
415"
REFERENCES,0.4063063063063063,"preprint arXiv:2011.13456, 2020.
416"
REFERENCES,0.4072072072072072,"[17] Richard S Sutton, Andrew G Barto, et al. Introduction to reinforcement learning, volume 135.
417"
REFERENCES,0.4081081081081081,"MIT press Cambridge, 1998.
418"
REFERENCES,0.409009009009009,"[18] Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. Csdi: Conditional score-based
419"
REFERENCES,0.4099099099099099,"diffusion models for probabilistic time series imputation. Advances in Neural Information
420"
REFERENCES,0.41081081081081083,"Processing Systems, 34:24804–24816, 2021.
421"
REFERENCES,0.4117117117117117,"[19] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel.
422"
REFERENCES,0.4126126126126126,"Domain randomization for transferring deep neural networks from simulation to the real world.
423"
REFERENCES,0.4135135135135135,"In 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS), pages
424"
REFERENCES,0.4144144144144144,"23–30. IEEE, 2017.
425"
REFERENCES,0.4153153153153153,"[20] Zhendong Wang, Jonathan J Hunt, and Mingyuan Zhou. Diffusion policies as an expressive
426"
REFERENCES,0.41621621621621624,"policy class for offline reinforcement learning. In The Eleventh International Conference on
427"
REFERENCES,0.41711711711711713,"Learning Representations, 2023. URL https://openreview.net/forum?id=AHvFDPi-FA.
428"
REFERENCES,0.418018018018018,"[21] Julian Wyatt, Adam Leach, Sebastian M Schmon, and Chris G Willcocks. Anoddpm: Anomaly
429"
REFERENCES,0.4189189189189189,"detection with denoising diffusion probabilistic models using simplex noise. In Proceedings of
430"
REFERENCES,0.4198198198198198,"the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 650–656, 2022.
431"
REFERENCES,0.4207207207207207,"[22] Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the generative learning trilemma
432"
REFERENCES,0.42162162162162165,"with denoising diffusion gans. arXiv preprint arXiv:2112.07804, 2021.
433"
REFERENCES,0.42252252252252254,"A
Theorem
434"
REFERENCES,0.42342342342342343,"Theorem 1 There exists a finite evaluation environment set that can capture the student’s general
435"
REFERENCES,0.4243243243243243,"capabilities and the performance vector [p1, . . . , pm] is a good representation of the student policy.
436"
REFERENCES,0.4252252252252252,"To prove this, we first provide the following Assumption:
437"
REFERENCES,0.4261261261261261,"Assumption 1 Let p(π, ⃗θ) denote the performance of student policy π in an environment ⃗θ. For ∀i-th
438"
REFERENCES,0.42702702702702705,"dimension of the environment parameters, denoted as θi, when changing the θi to θ′
i to get a new
439"
REFERENCES,0.42792792792792794,"environment ⃗θ′ while keeping other environment parameters fixed, there ∃δi > 0, if |θ′
i −θi| ≤δi, we
440"
REFERENCES,0.42882882882882883,"have |p(π, ⃗θ′) −p(π, ⃗θ)| ≤ϵi, where ϵi →0.
441"
REFERENCES,0.4297297297297297,"If this is true, we then can construct a finite set of environments, and the student performances in
442"
REFERENCES,0.4306306306306306,"those environments can represent the performances in all potential environments generated within
443"
REFERENCES,0.4315315315315315,"the certain environment parameters open interval combinations, and the set of those open intervals
444"
REFERENCES,0.43243243243243246,"combinations cover the environment parameter space Θ.
445"
REFERENCES,0.43333333333333335,"We begin from the simplest case where we only consider using one environment parameter to generate
446"
REFERENCES,0.43423423423423424,"environments, denoted as θi. We can construct a finite environment parameter set for environment
447"
REFERENCES,0.43513513513513513,"parameters, which is {θmin
i
+ 1/2 ∗δi, θmin
i
+ 3/2 ∗δi, θmin
i
+ 7/2 ∗δi, . . . , θmax
i
−δi/2}. Assume
448"
REFERENCES,0.436036036036036,"the set size is Li. We let the set {⃗θi}Li
i=1 denote the corresponding generated environments. This is
449"
REFERENCES,0.4369369369369369,"served as the representative environment set. Then the student performances in those environments
450"
REFERENCES,0.43783783783783786,"are denoted as {p(π, ⃗θi)}Li
i=1, which we call it as representative performance vector set. We can
451"
REFERENCES,0.43873873873873875,"divide the space for θi into a finite set of open intervals with size Li, which is {[θmin
i
, θmin
i
+ 3/2 ∗
452"
REFERENCES,0.43963963963963965,"δi), (θmin
i
+1/2∗δi, θmin
i
+5/2δi), (θmin
i
+5/2∗δi, θmin
i
+9/2∗δi), . . . , (θmax
i
−3/2∗δi, θmax
i
]},
453"
REFERENCES,0.44054054054054054,"which we call it as representative parameter interval set, also denoted as {(θi −δ, θi + δ)}Li
i=1.
454"
REFERENCES,0.44144144144144143,"For any environment generated in those intervals, denoted as ⃗θ′
i, the performance p(π, ⃗θ′
i) can always
455"
REFERENCES,0.4423423423423423,"be represented by the p(π, ⃗θi) which is in the same interval, as |p(π, ⃗θ′
i) −p(π, ⃗θi)| ≤ϵi, where
456"
REFERENCES,0.44324324324324327,"ϵi →0. In such cases, the finite set of environmental parameter intervals {θmin
i
+ 1/2 ∗δi, θmin
i
+
457"
REFERENCES,0.44414414414414416,"3/2 ∗δi, θmin
i
+ 7/2 ∗δi, . . . , θmax
i
−δi/2} fully covers the entire parameter space Θ. We can find
458"
REFERENCES,0.44504504504504505,"a representative environment set {⃗θi}Li
i=1 that is capable of approximating the performance of the
459"
REFERENCES,0.44594594594594594,"student policy within the open parameter intervals combination. This set effectively characterizes the
460"
REFERENCES,0.44684684684684683,"general performance capabilities of the student policy π.
461"
REFERENCES,0.4477477477477477,"Then we extend to two environment parameter design space cases. Let’s assume that the environment
462"
REFERENCES,0.4486486486486487,"is generated by two-dimension environment parameters. Then, for each environment parameter,
463"
REFERENCES,0.44954954954954957,"θi ∈{θ1, θ2}. We can find the same open interval set for each parameter. Specifically, for each θi,
464"
REFERENCES,0.45045045045045046,"there exists a δi, such that if |θ′
i −θi| ≤δi, we have |p(π, ⃗θ′) −p(π, ⃗θ)| ≤ϵi, where ϵi →0. Hence,
465"
REFERENCES,0.45135135135135135,"we let δ = min{δ1, δ2} and ϵ = ϵ1 + ϵ2. Thus the new representative environment set is the set
466"
REFERENCES,0.45225225225225224,"that includes the any combination of {[θ1, θ2]} where θ1 ∈{⃗θi}L1
i=1 and θ2 ∈{⃗θj}L2
j=1. We can get
467"
REFERENCES,0.45315315315315313,"the representative performance vector set as {p(π, [⃗θi, ⃗θj])}i∈[1,L1],j∈[1,L2]. We then can construct
468"
REFERENCES,0.4540540540540541,"the representative parameter interval set as {[(θi −δ, θi + δ), (θj −δ, θj + δ)]}i∈[1,L1],j∈[1,Lj].
469"
REFERENCES,0.45495495495495497,"As a result, for any new environments [⃗θ′
i, ⃗θ′
j], we can find the representative environment whose
470"
REFERENCES,0.45585585585585586,"environment parameters are in the same parameter interval [⃗θi, ⃗θj], such that their performance
471"
REFERENCES,0.45675675675675675,"difference is smaller than ϵ = ϵ1 + ϵ2 for all ∀i ∈[1, L1], ∀j ∈[1, L2]:
472"
REFERENCES,0.45765765765765765,"|p(π, [⃗θ′
i, ⃗θ′
j]) −p(π, [⃗θi, ⃗θj])| = |p(π, [⃗θ′
i, ⃗θ′
j]) −p(π, [⃗θ′
i, ⃗θj]) + p(π, [⃗θ′
i, ⃗θj]) −p(π, [⃗θi, ⃗θj])|"
REFERENCES,0.45855855855855854,"≤|p(π, [⃗θ′
i, ⃗θ′
j]) −p(π, [⃗θ′
i, ⃗θj])| + |p(π, [⃗θ′
i, ⃗θj]) −p(π, [⃗θi, ⃗θj])|"
REFERENCES,0.4594594594594595,"≤δj + δi
= δ
(9)"
REFERENCES,0.4603603603603604,"In such cases, the finite set of environmental parameter intervals {[(θi −δ, θi + δ), (θj −δ, θj +
473"
REFERENCES,0.46126126126126127,"δ)]}i∈[1,L1],j∈[1,Lj] fully covers the entire parameter space Θ. We can find a representative environ-
474"
REFERENCES,0.46216216216216216,"ment set {⃗θi}Li
i=1 that is capable of approximating the performance of the student policy within the
475"
REFERENCES,0.46306306306306305,"Table 1: The teacher policies corresponding to the three approaches for UED. U(Θ) is a uniform
distribution over environment parameter space, ˜Dπ is a baseline distribution, ¯θπ is the trajectory
which maximizes regret of π, and vπ is the value above the baseline distribution that π achieves on
that trajectory, cπ is the negative of the worst-case regret of π. Details are described in PAIRED [3]."
REFERENCES,0.46396396396396394,"UED Approaches
Teacher Policy
Decision Rule"
REFERENCES,0.4648648648648649,"DR [19]
Λ(π) = U(Θ)
Randomly sample
PARIED [3]
Λ(π) = {¯θπ : cπ"
REFERENCES,0.4657657657657658,"vπ , ˜Dπ : otherwise}
Minimax Regret
SHED (ours)
Λ(π) = arg max
⃗θ∈Θ
Qπ(s = π, a = ⃗θ)
Maximize reward"
REFERENCES,0.4666666666666667,"open parameter intervals combination. This set effectively characterizes the general performance
476"
REFERENCES,0.46756756756756757,"capabilities of the student policy π.
477"
REFERENCES,0.46846846846846846,"Similarly, we can show this still holds when the environment is constructed by a larger dimension
478"
REFERENCES,0.46936936936936935,"environment parameters, where we set δ = min{δi}, and ϵ = P"
REFERENCES,0.4702702702702703,"i ϵi, and we have δ > 0, ϵ →0. The
479"
REFERENCES,0.4711711711711712,"overall logic is that we can find a finite set, which is called representative environment set, and
480"
REFERENCES,0.4720720720720721,"we can use performances in this set to represent any performances in the environments generated
481"
REFERENCES,0.47297297297297297,"in the representative parameter interval set, which is called representative performance vector
482"
REFERENCES,0.47387387387387386,"set. Finally, we can show that representative parameter interval set fully covers the environment
483"
REFERENCES,0.47477477477477475,"parameter space. Thus there exists a finite evaluation environment set that can capture the student’s
484"
REFERENCES,0.4756756756756757,"general capabilities and the performance vector, called representative performance vector set,
485"
REFERENCES,0.4765765765765766,"[p1, . . . , pm] is a good representation of the student policy.
486"
REFERENCES,0.4774774774774775,"B
Details about the Generative model
487"
REFERENCES,0.4783783783783784,"B.1
Generative model to generate synthetic next state
488"
REFERENCES,0.47927927927927927,"Here, we describe how to leverage the diffusion model to learn the conditional data distribution in the
489"
REFERENCES,0.48018018018018016,"collected experiences τ = {(su
t , au
t , ru
t , su,′
t )}. Later we can use the trainable reverse chain in the
490"
REFERENCES,0.4810810810810811,"diffusion model to generate the synthetic trajectories that can be used to help train the teacher agent,
491"
REFERENCES,0.481981981981982,"resulting in reducing the resource-intensive and time-consuming collection of upper-level teacher
492"
REFERENCES,0.4828828828828829,"experiences. We deal with two different types of timesteps in this section: one for the diffusion
493"
REFERENCES,0.4837837837837838,"process and the other for the upper-level teacher agent, respectively. We use subscripts k ∈1, . . . , K
494"
REFERENCES,0.4846846846846847,"to represent diffusion timesteps and subscripts t ∈1, . . . , T to represent trajectory timesteps in the
495"
REFERENCES,0.48558558558558557,"teacher’s experience.
496"
REFERENCES,0.4864864864864865,"In the image domain, the diffusion process is implemented across all pixel values of the image. In our
497"
REFERENCES,0.4873873873873874,"setting, we diffuse over the next state su,′ conditioned the given state su and action au. We construct
498"
REFERENCES,0.4882882882882883,"our generative model according to the conditional diffusion process:
499"
REFERENCES,0.4891891891891892,"q(su,′
k |su,′
k−1),
pϕ(su,′
k−1|su,′
k , su, au)"
REFERENCES,0.4900900900900901,"As usual, q(su,′
k |su,′
k−1) is the predefined forward noising process while pϕ(su,′
k−1|su,′
k , su, au) is the
500"
REFERENCES,0.49099099099099097,"trainable reverse denoising process. We begin by randomly sampling the collected experiences
501"
REFERENCES,0.4918918918918919,"τ = {(su
t , au
t , ru
t , su,′
t )} from the real experience buffer Breal.
502"
REFERENCES,0.4927927927927928,"We drop the superscript u here for ease of explanation. Giving the observed state s and action a, we
503"
REFERENCES,0.4936936936936937,"use the reverse process pϕ to represent the generation of the next state s′:
504"
REFERENCES,0.4945945945945946,"pϕ(s′
0:K|s, a) = N(s′
K; 0, I) K
Y"
REFERENCES,0.4954954954954955,"k=1
pϕ(s′
k−1|s′
k, s, a)
(10)"
REFERENCES,0.4963963963963964,"At the end of the reverse chain, the sample s′
0, is the generated next state s′.
As
shown in Section 2.2,
pϕ(s′
k−1|, s′
k, s, a) could be modeled as a Gaussian distribution
N(s′
k−1; µθ(s′
k, s, a, k), Σθ(s′
k, s, a, k)). Similar to Ho et al. [6], we parameterize pϕ(s′
k−1|s′
k, s, a)
as a noise prediction model with the covariance matrix fixed as"
REFERENCES,0.4972972972972973,"Σθ(s′
k, s, a, k) = βiI"
REFERENCES,0.4981981981981982,and mean is
REFERENCES,0.4990990990990991,"µθ(s′
i, s, a, k) =
1
√αk"
REFERENCES,0.5,"
s′
k −
βk
√1 −¯αk
ϵθ(s′
k, s, a, k)
"
REFERENCES,0.5009009009009009,"Where ϵθ(s′
k, s, a, k) is the trainable denoising function, which aims to estimate the noise ϵ in the
505"
REFERENCES,0.5018018018018018,"noisy input s′
k at step k. Specifically, giving the sampled experience (s, a, s′), we begin by sampling
506"
REFERENCES,0.5027027027027027,"s′
K ∼N(0, I) and then proceed with the reverse diffusion chain pϕ(s′
k−1|, s′
k, s, a) for k = K, . . . , 1.
507"
REFERENCES,0.5036036036036036,"The detailed expression for s′
k−1 is as follows:
508"
REFERENCES,0.5045045045045045,"s′
k
√αk
−
βk
p"
REFERENCES,0.5054054054054054,"αk(1 −¯αk)
ϵθ(s′
k, s, a, k) +
p"
REFERENCES,0.5063063063063064,"βkϵ,
(11)"
REFERENCES,0.5072072072072072,"where ϵ ∼N(0, I). Note that ϵ = 0 when k = 1.
509"
REFERENCES,0.5081081081081081,"Training objective.
We employ a similar simplified objective, as proposed by Ho et al. [6] to train
510"
REFERENCES,0.509009009009009,"the conditional ϵ- model through the following process:
511"
REFERENCES,0.5099099099099099,"L(θ) = E(s,a,s′)∼τ,k∼U,ϵ∼N(0,I)

∥ϵ −ϵϕ(s′
k, s, a, k)∥2
(12)"
REFERENCES,0.5108108108108108,"Where s′
k = √¯αks′+√1 −¯αkϵ. U represents a uniform distribution over the discrete set {1, . . . , K}.
512"
REFERENCES,0.5117117117117117,"The intuition for the loss function L(θ) tries to predict the noise ϵ ∼N(0, I) at the denoising step k,
513"
REFERENCES,0.5126126126126126,"and the diffusion model is essentially learning the student policy involution trajectories collected in
514"
REFERENCES,0.5135135135135135,"the real experience buffer Breals. Note that the reverse process necessitates a substantial number of
515"
REFERENCES,0.5144144144144144,"steps K, as the Gaussian assumption holds true primarily under the condition of the infinitesimally
516"
REFERENCES,0.5153153153153153,"limit of small denoising steps [15]. Recent research by Xiao et al. [22] has demonstrated that enabling
517"
REFERENCES,0.5162162162162162,"denoising with large steps can reduce the total number of denoising steps K. To expedite the relatively
518"
REFERENCES,0.5171171171171172,"slow reverse sampling process outlined in Equation 3.2 (as it requires computing ϵϕ networks K
519"
REFERENCES,0.5180180180180181,"times), we use a small value of K, while simultaneously setting βmin = 0.1 and βmax = 10.0.
520"
REFERENCES,0.518918918918919,"Similar to Wang et al. [20], we define:
521"
REFERENCES,0.5198198198198198,βk = 1 −αk
REFERENCES,0.5207207207207207,"= 1 −exp

βmin × 1"
REFERENCES,0.5216216216216216,K −0.5(βmax −βmin)2k −1 K2 
REFERENCES,0.5225225225225225,"This noise schedule is derived from the variance-preserving Stochastic Differential Equation by Song
522"
REFERENCES,0.5234234234234234,"et al. [16].
523"
REFERENCES,0.5243243243243243,"Generate synthetic trajectories.
Once the diffusion model has been trained, it can be used
524"
REFERENCES,0.5252252252252252,"to generate synthetic experience data by starting with a draw from the prior s′
K ∼N(0, I) and
525"
REFERENCES,0.5261261261261261,"successively generating denoised next state, conditioned on the given s and a through the reverse
526"
REFERENCES,0.527027027027027,"chain pϕ in Equation 3.2. Note that the giving condition action a can either be randomly sampled
527"
REFERENCES,0.527927927927928,"from the action space (which is also the environment parameter space) or use another diffusion model
528"
REFERENCES,0.5288288288288289,"to learn the action distribution giving the initial state s. In such case, this new diffusion model is
529"
REFERENCES,0.5297297297297298,"essentially a behavior-cloning model that aims to learn the teacher policy Λ(a|s). This process is
530"
REFERENCES,0.5306306306306307,"similar to the work of Wang et al. [20]. We discuss this process in detail in the appendix. In this paper,
531"
REFERENCES,0.5315315315315315,"we randomly sample a as it is straightforward and can also increase the diversity in the generated
532"
REFERENCES,0.5324324324324324,"synthetic experience to help train a more robust teacher agent.
533"
REFERENCES,0.5333333333333333,"B.2
Generative model to generate synthetic action
534"
REFERENCES,0.5342342342342342,"Once the diffusion model has been trained, it can be used to generate synthetic experience data
535"
REFERENCES,0.5351351351351351,"by starting with a draw from the prior s′
K ∼N(0, I) and successively generating denoised next
536"
REFERENCES,0.536036036036036,"state, conditioned on the given s and a through the reverse chain pϕ in Equation 3.2. Note that the
537"
REFERENCES,0.5369369369369369,"giving condition action a can either be randomly sampled from the action space (which is also the
538"
REFERENCES,0.5378378378378378,"environment parameter space) or we can train another diffusion model to learn the action distribution
539"
REFERENCES,0.5387387387387388,"giving the initial state s, and then use the trained new diffusion model to sample the action a giving
540"
REFERENCES,0.5396396396396397,"the state s. This process is similar to the work of Wang et al. [20].
541"
REFERENCES,0.5405405405405406,"In particular, We construct another conditional diffusion model as:
542"
REFERENCES,0.5414414414414415,"q(ak|ak−1),
pϕ(ak−1|ak, s)"
REFERENCES,0.5423423423423424,"Figure 5: The distribution of the real s′ and the synthetic s′ conditioned on (s, a)."
REFERENCES,0.5432432432432432,"As usual, q(ak|ak−1) is the predefined forward noising process while pϕ(ak−1|ak, s) is the trainable
543"
REFERENCES,0.5441441441441441,"reverse denoising process. we represent the action generation process via the reverse chain of the
544"
REFERENCES,0.545045045045045,"conditional diffusion model as
545"
REFERENCES,0.5459459459459459,"pϕ(a0:K|s) = N(aK; 0, I) K
Y"
REFERENCES,0.5468468468468468,"k=1
pϕ(ak−1|ak, s)
(13)"
REFERENCES,0.5477477477477477,"At the end of the reverse chain, the sample a0, is the generated action a for the giving state s.
Similarly, we parameterize pϕ(ak−1|ak, s) as a noise prediction model with the covariance matrix
fixed as
Σθ(ak, s, k) = βiI
and mean is"
REFERENCES,0.5486486486486486,"µθ(ai, s, k) =
1
√αk"
REFERENCES,0.5495495495495496,"
ak −
βk
√1 −¯αk
ϵθ(ak, s, k)
"
REFERENCES,0.5504504504504505,"Similarly, the simplified loss function is
546"
REFERENCES,0.5513513513513514,"La(θ) = E(s,a)∼τ,k∼U,ϵ∼N(0,I)

∥ϵ −ϵϕ(ak, s, k)∥2
(14)"
REFERENCES,0.5522522522522523,"Where ak = √¯αka+√1 −¯αkϵ. U represents a uniform distribution over the discrete set {1, . . . , K}.
547"
REFERENCES,0.5531531531531532,"The intuition for the loss function La(θ) tries to predict the noise ϵ ∼N(0, I) at the denoising step k,
548"
REFERENCES,0.5540540540540541,"and the diffusion model is essentially a behavior cloning model to learn the student policy collected
549"
REFERENCES,0.554954954954955,"in the real experience buffer Breals.
550"
REFERENCES,0.5558558558558558,"Once this new diffusion model is trained, the generation of the synthetic experience can be formulated
551"
REFERENCES,0.5567567567567567,"as:
552"
REFERENCES,0.5576576576576576,"• we first randomly sample the state from the collected real trajectories s ∼τ;
553"
REFERENCES,0.5585585585585585,"• we use the new diffusion model discussed above to mimic the teacher’s policy to generate
554"
REFERENCES,0.5594594594594594,"the actions a;
555"
REFERENCES,0.5603603603603604,"• giving the state s and action a, we use the first diffusion model presented in the main paper
556"
REFERENCES,0.5612612612612613,"to generate the next state s′;
557"
REFERENCES,0.5621621621621622,"• we compute the reward r according to the reward function, and add the final generated
558"
REFERENCES,0.5630630630630631,"synthetic experience (s, a, r, s′) to the synthetic experience buffer Bsyn to help train the
559"
REFERENCES,0.563963963963964,"teacher agent.
560"
REFERENCES,0.5648648648648649,"Figure 6: The distribution of the real [s′
1, s′
2, s′
3](red) and the synthetic [s′
1, s′
2, s′
3](blue) giving the
fixed (su, au). Specifically, the noise ε in f(su, au) is (i).left figure: ε = ϵ, (ii).middle figure:
ε = 3 ∗ϵ, (iii).right figure: ε = 10 ∗ϵ, where ϵ ∼N(0, 1)."
REFERENCES,0.5657657657657658,"C
Empirical analysis of generative model
561"
REFERENCES,0.5666666666666667,"C.1
Ability to generate good synthetic trajectories
562"
REFERENCES,0.5675675675675675,"We begin by investigating SHED’s ability to assist in collecting experiences for the upper-level MDP
563"
REFERENCES,0.5684684684684684,"teacher. This involves the necessity for SHED to prove its ability to accurately generate synthetic
564"
REFERENCES,0.5693693693693693,"experiences for teacher agents. To check the quality of these generated synthetic experiences, we
565"
REFERENCES,0.5702702702702702,"employ a diffusion model to simulate some data for validation (even though Diffusion models have
566"
REFERENCES,0.5711711711711712,"demonstrated remarkable success across vision and NLP tasks).
567"
REFERENCES,0.5720720720720721,"We design the following experiment: given the teacher’s observed state su = [p1, p2, p3, p4, p5],
568"
REFERENCES,0.572972972972973,"where pi denotes the student performance on i-th evaluation environment. and given the teacher’s
569"
REFERENCES,0.5738738738738739,"action au = [a1, a2, a3], which is the environment parameters and are used to generate corresponding
570"
REFERENCES,0.5747747747747748,"environment instances. We use a neural network f(su, au) to mimic the involution trajectories of
571"
REFERENCES,0.5756756756756757,"the student policy π. That is, with the input of the state su and action au into the neural network, it
572"
REFERENCES,0.5765765765765766,"outputs the next observed state su,′ = [p′
1, p′
2, p′
3, p′
4, p′
5], indicating the updated student performance
573"
REFERENCES,0.5774774774774775,"vector on the evaluation environments after training in the environment generated by au. In particular,
574"
REFERENCES,0.5783783783783784,"we add a noise ε into su,′ to represent the uncertainty in the transition. We first train our diffusion
575"
REFERENCES,0.5792792792792792,"model on the real dataset (su, au, su,′) generated by neural network f(su, au). We then set a fixed
576"
REFERENCES,0.5801801801801801,"(su, au) pair and input them into f(su, au) to generate 200 samples of real su,′. The trained diffusion
577"
REFERENCES,0.581081081081081,"model is then used to generate 200 synthetic su,′ conditioned on the fixed (su, au) pair.
578"
REFERENCES,0.581981981981982,"The results are presented in Figure 6, we can see that the generative model can effectively capture
579"
REFERENCES,0.5828828828828829,"the distribution of real experience even if there is a large uncertainty in the transition, indicated by
580"
REFERENCES,0.5837837837837838,"the value of ε. This provides evidence that the diffusion model can generate useful experiences
581"
REFERENCES,0.5846846846846847,"conditioned on (su, au). It is important to note that the marginal distribution derived from the reverse
582"
REFERENCES,0.5855855855855856,"diffusion chain provides an implicit, expressive distribution, such distribution has the capability to
583"
REFERENCES,0.5864864864864865,"capture complex distribution properties, including skewness and multi-modality.
584"
REFERENCES,0.5873873873873874,"C.2
addition experiments on diffusion model
585"
REFERENCES,0.5882882882882883,"We further provide more results to show the ability of our generative model to generate synthetic
586"
REFERENCES,0.5891891891891892,"trajectories where the noise is extremely small. In such cases, the actual next state s′ will converge to
587"
REFERENCES,0.5900900900900901,"a certain value, and the synthetic next state ssyn,′ generated by the diffusion model should also be
588"
REFERENCES,0.590990990990991,"very close to that value, then the diffusion model has the ability to sample the next state ssyn,′
0
which
589"
REFERENCES,0.5918918918918918,"can accurately represent the next state. We present the results in Figure 5. Specifically, this figure
590"
REFERENCES,0.5927927927927928,"shows when the noise is very small in the actual next state, which is 0.05∗ϵ, and ϵ ∼N(0, 1). Giving
591"
REFERENCES,0.5936936936936937,"any condition (s, a) pair, we selectively report on (si, ai), where x-axis is the ai value, and y-axis
592"
REFERENCES,0.5945945945945946,"is the si value. The student policy with initial performance vector s is trained on the environments
593"
REFERENCES,0.5954954954954955,"generated by the teacher’s action a. We report the new performance s′
i of student policy on i-th
594"
REFERENCES,0.5963963963963964,"environments after training in the z-axis. In particular, if two points s′
i and ssyn,′
i
are close, it indicates
595"
REFERENCES,0.5972972972972973,"that the diffusion model can successfully generate the actual next state. As we can see, when the
596"
REFERENCES,0.5981981981981982,"noise is extremely small, our diffusion model can accurately predict the next state of s′
i giving any
597"
REFERENCES,0.5990990990990991,"condition (s, a) pair.
598"
REFERENCES,0.6,"Figure 7: Left: The ablation study in the Lunar lander environment which investigates the effect of
the size of the evaluation environment set. We provide the average zero-shot transfer performances
on the test environments (mean and standard error). Right: Zero-shot transfer performance on the test
environments under a longer time horizon in Lunar lander environments(mean and standard error)."
REFERENCES,0.6009009009009009,"D
Additional Experiment Details
599"
REFERENCES,0.6018018018018018,"D.1
Hyperparameters
600"
REFERENCES,0.6027027027027027,"We set the learning rate 1e−3 for actor, and 3e−3 for critic, we set gamma γ = 0.999, λ = 0.95, and
601"
REFERENCES,0.6036036036036037,"set coefficient for the entropy bonus (to encourage exploration) as 0.01. For each environment, we
602"
REFERENCES,0.6045045045045045,"conduct 50 PPO updates for the student agent, and We can train on up to 50 environments, including
603"
REFERENCES,0.6054054054054054,"replay. For our diffusion model, the diffusion discount is 0.99, and batch size is 64, τ is 0.005,
604"
REFERENCES,0.6063063063063063,"learning rate is 3e −4. The synthetic buffer size is 1000, and the ratio is 0.25.
605"
REFERENCES,0.6072072072072072,"D.2
Experiments Compute Resources
606"
REFERENCES,0.6081081081081081,"All the models were trained on a single NVIDIA GeForce RTX 3090 GPU and 16 CPUs.
607"
REFERENCES,0.609009009009009,"D.3
Maze document
608"
REFERENCES,0.6099099099099099,"Here we provide the document shows the instruction to generate feasible maze environments.
609"
REFERENCES,0.6108108108108108,"There are several factors that can affect the difficulty of a maze. Here are
610"
REFERENCES,0.6117117117117117,"some key factors to consider:
611"
REFERENCES,0.6126126126126126,"1. Maze Size: Larger mazes generally increase the complexity and difficulty
612"
REFERENCES,0.6135135135135135,"as the agent has more states to explore. Typically, the maze size should be
613"
REFERENCES,0.6144144144144145,"larger than 4x4 and smaller than 15*15.
614"
REFERENCES,0.6153153153153154,"- If the size is 7*7 or smaller, the maze size is considered easy.
615"
REFERENCES,0.6162162162162163,"- If the size is larger than 7*7 but smaller than 10*10, the maze size is
616"
REFERENCES,0.6171171171171171,"considered medium.
617"
REFERENCES,0.618018018018018,"- If the maze size is larger than 10x10 but smaller than 15*15, the maze
618"
REFERENCES,0.6189189189189189,"size is considered hard.
619"
REFERENCES,0.6198198198198198,"2. Maze Structure: The complexity of the paths, including the number of twists,
620"
REFERENCES,0.6207207207207207,"turns, and dead-ends, can significantly impact navigation strategies. The
621"
REFERENCES,0.6216216216216216,"presence of narrow corridors versus wide-open spaces also plays a role.
622"
REFERENCES,0.6225225225225225,"- If there are fewer than 2 turns in the feasible path from the start position
623"
REFERENCES,0.6234234234234234,"to the end position, the maze structure is considered easy.
624"
REFERENCES,0.6243243243243243,"- If there are more than 2 turns but fewer than 4 turns in the path from the
625"
REFERENCES,0.6252252252252253,"start position to the end position, the maze structure is considered medium.
626"
REFERENCES,0.6261261261261262,"- If there are 4 or more turns in the path from the start position to the end
627"
REFERENCES,0.6270270270270271,"position, the maze structure is considered hard.
628"
REFERENCES,0.627927927927928,"3. Goal Location: The distance from the starting position to the end position
629"
REFERENCES,0.6288288288288288,"also affects difficulty.
630"
REFERENCES,0.6297297297297297,"- If the path from the start position to the end position requires fewer than
631"
REFERENCES,0.6306306306306306,"5 steps, the goal location is considered easy.
632"
REFERENCES,0.6315315315315315,"- If the path from the start position to the end position requires 5 to 10
633"
REFERENCES,0.6324324324324324,"steps, the goal location is considered medium.
634"
REFERENCES,0.6333333333333333,"- If the path from the start position to the end position requires more than
635"
REFERENCES,0.6342342342342342,"10 steps, the goal location is considered hard.
636"
REFERENCES,0.6351351351351351,"4. Start Location: The starting position can also affect the difficulty of
637"
REFERENCES,0.6360360360360361,"the maze. The starting position is categorized into five levels:
638"
REFERENCES,0.636936936936937,"- If the start position is close to 1, it means it should be located as close
639"
REFERENCES,0.6378378378378379,"to the top left of the maze.
640"
REFERENCES,0.6387387387387388,"- If the start position is close to 2, it means it should be located as close
641"
REFERENCES,0.6396396396396397,"to the top right of the maze.
642"
REFERENCES,0.6405405405405405,"- If the start position is close to 3, it means it should be located as close
643"
REFERENCES,0.6414414414414414,"to the bottom left of the maze.
644"
REFERENCES,0.6423423423423423,"- If the start position is close to 4, it means it should be located as close
645"
REFERENCES,0.6432432432432432,"to the bottom right of the maze.
646"
REFERENCES,0.6441441441441441,"- If the start position is close to 5, it means it should be located as close
647"
REFERENCES,0.645045045045045,"to the center of the maze.
648"
REFERENCES,0.6459459459459459,"Please note that the generated maze uses -1 to represent blocks, 0 to
649"
REFERENCES,0.6468468468468469,"represent the feasible path, 1 to represent the start position, and 2 to represent
650"
REFERENCES,0.6477477477477478,"the end position. Must ensure that there is a feasible path in the generated maze!
651"
REFERENCES,0.6486486486486487,"A feasible path means that 1 and 2 are connected directly through 0s, or 1 and 2
652"
REFERENCES,0.6495495495495496,"are connected directly. For example:
653"
REFERENCES,0.6504504504504505,"Feasible Maze:
654"
REFERENCES,0.6513513513513514,"Maze = [
655"
REFERENCES,0.6522522522522523,"[0, -1, -1, 2],
656"
REFERENCES,0.6531531531531531,"[1, -1, 0, 0],
657"
REFERENCES,0.654054054054054,"[0, -1, 0, -1],
658"
REFERENCES,0.6549549549549549,"[0, 0, 0, -1],
659 ]
660"
REFERENCES,0.6558558558558558,"Non-Feasible Mazes:
661"
REFERENCES,0.6567567567567567,"Maze = [
662"
REFERENCES,0.6576576576576577,"[0, -1, -1, 2],
663"
REFERENCES,0.6585585585585586,"[1, -1, 0, 0],
664"
REFERENCES,0.6594594594594595,"[0, -1, -1, 0],
665"
REFERENCES,0.6603603603603604,"[0, 0, 0, -1],
666 ]
667"
REFERENCES,0.6612612612612613,"Or
668"
REFERENCES,0.6621621621621622,"Maze = [
669"
REFERENCES,0.6630630630630631,"[1, -1],
670"
REFERENCES,0.663963963963964,"[-1, 2]
671 ]
672"
REFERENCES,0.6648648648648648,"These second example does not have any feasible path.
673 674 675"
REFERENCES,0.6657657657657657,"D.4
Prompt for RAG
676"
REFERENCES,0.6666666666666666,"We provide our prompt for the Retrieval Augmented Generation as follows:
677"
REFERENCES,0.6675675675675675,"Please refer to the document, and generate a maze with feasible path. The
678"
REFERENCES,0.6684684684684684,"difficulty level for the maze size is {maze_size_level}, and the difficulty
679"
REFERENCES,0.6693693693693694,"level for the maze structure is {maze_structure_level}, he difficulty level
680"
REFERENCES,0.6702702702702703,"for the goal location is {goal_location_level}, he difficulty level for
681"
REFERENCES,0.6711711711711712,"the start location is {start_position_level}.
682"
REFERENCES,0.6720720720720721,"E
Additional experiments
683"
REFERENCES,0.672972972972973,"E.1
Additional experiments about ablation studies
684"
REFERENCES,0.6738738738738739,"We also provide ablation analysis to evaluate the impact of different design choices in Lunar lander
685"
REFERENCES,0.6747747747747748,"domain, including (a) a larger evaluation environment set; (b) a bigger budget for constraint on the
686"
REFERENCES,0.6756756756756757,"number of generated environments (which incurs a longer training time horizon). The results are
687"
REFERENCES,0.6765765765765765,"reported in Figure 7.
688"
REFERENCES,0.6774774774774774,"We explore the impact of introducing the diffusion model in collecting synthetic teacher’s experience
689"
REFERENCES,0.6783783783783783,"and varying the size of the evaluation environment set. Specifically, as we can see from the right side
690"
REFERENCES,0.6792792792792792,"of Figure 7, the SHED consistently outperforms h-MDP, indicating the effectiveness of introducing
691"
REFERENCES,0.6801801801801802,"the generative model to help train the upper-level teacher policy. Furthermore, we find that when
692"
REFERENCES,0.6810810810810811,"increasing the size of the evaluation environment set, we can have a better result in the student
693"
REFERENCES,0.681981981981982,"transfer performances. The intuition is that a larger evaluation environment set, encompassing a more
694"
REFERENCES,0.6828828828828829,"diverse range of environments, provides a better approximation of the student policy according to the
695"
REFERENCES,0.6837837837837838,"Theorem 1. However, the reason why SHED with 30 evaluation environments slightly outperforms
696"
REFERENCES,0.6846846846846847,"SHED with 40 evaluation environments is perhaps attributed to the increase in the dimension of the
697"
REFERENCES,0.6855855855855856,"student performance vector, which amplifies the challenge of training an effective diffusion model
698"
REFERENCES,0.6864864864864865,"with a limited dataset.
699"
REFERENCES,0.6873873873873874,"We conduct experiments in Lunar lander under a longer time horizon. The results are provided on the
700"
REFERENCES,0.6882882882882883,"right side of Figure 7. As we can see, our proposed algorithm SHED can efficiently train the student
701"
REFERENCES,0.6891891891891891,"agent to achieve the general capability in a shorter time horizon, This observation indicates that
702"
REFERENCES,0.69009009009009,"our proposed environment generation process can better generate the suitable environments for the
703"
REFERENCES,0.690990990990991,"current student policy, thereby enhancing its general capability, especially when there is a constraint
704"
REFERENCES,0.6918918918918919,"on the number of generated environments.
705"
REFERENCES,0.6927927927927928,"E.2
Additional experiments on Lunar lander
706"
REFERENCES,0.6936936936936937,"we also conduct experiments to show how the algorithm performs under different settings, such
707"
REFERENCES,0.6945945945945946,"as a larger weight of cv fairness rewards (η = 10). The results are provided in Figure 8. We
708"
REFERENCES,0.6954954954954955,"noticed an interesting finding: when fairness reward has a high weightage, our algorithm tends to
709"
REFERENCES,0.6963963963963964,"generate environments at the onset that lead to a rapid decline and subsequent improvement in student
710"
REFERENCES,0.6972972972972973,"performance across all test environments. This is done to avoid acquiring a substantial negative
711"
REFERENCES,0.6981981981981982,"fairness reward and thereby maximize the teacher’s cumulative reward. Notably, the student’s final
712"
REFERENCES,0.6990990990990991,performance still surpasses other baselines at the end of training.
REFERENCES,0.7,"Figure 8: Zero-shot transfer performance on the test environments with a larger cv value coefficient
in Lunar lander environments. 713"
REFERENCES,0.7009009009009008,"We further show in detail how the performance of different methods changes in each testing environ-
714"
REFERENCES,0.7018018018018019,"ment during training (see Figure 9 and Figure 10 ).
715"
REFERENCES,0.7027027027027027,"Figure 9: Detail how the performance of different methods changes in each testing environment
during training (mean and error)"
REFERENCES,0.7036036036036036,"E.3
Additional experiments on Maze
716"
REFERENCES,0.7045045045045045,"We selectively report some results of zero-shot transfer performances in maze environments. The
717"
REFERENCES,0.7054054054054054,"results are provided in Figure
718"
REFERENCES,0.7063063063063063,"F
Discussion
719"
REFERENCES,0.7072072072072072,"F.1
Limitations
720"
REFERENCES,0.7081081081081081,"The limitation of this work comes from the UED framework, as UED is limited to the use of
721"
REFERENCES,0.709009009009009,"parameterized environments. This results in our experimental domain being relatively simple.
722"
REFERENCES,0.7099099099099099,"Figure 10: Detail how the performance of different methods changes in each testing environment
during training (mean and error)"
REFERENCES,0.7108108108108108,"(f) small maze
(g) medium maze
(h) large maze
(i) four rooms maze
(j) corridor maze"
REFERENCES,0.7117117117117117,Figure 11: Zeros-shot transfer performance on test environments in maze environemnts
REFERENCES,0.7126126126126127,"However, our work proposes a new hierarchical structure, and our policy representation is not only of
723"
REFERENCES,0.7135135135135136,"great help for UED, but also has certain inspirations for hierarchical RL. Additionally, in the world
724"
REFERENCES,0.7144144144144144,"model of UED (Genie [2]), the environment generator (teacher) focuses on creating video games, a
725"
REFERENCES,0.7153153153153153,"domain that is compatible with our proposed application of upsampling the teacher agent’s experience
726"
REFERENCES,0.7162162162162162,"using a diffusion model (since the state is image-based).
727"
REFERENCES,0.7171171171171171,"NeurIPS Paper Checklist
728"
CLAIMS,0.718018018018018,"1. Claims
729"
CLAIMS,0.7189189189189189,"Question: Do the main claims made in the abstract and introduction accurately reflect the
730"
CLAIMS,0.7198198198198198,"paper’s contributions and scope?
731"
CLAIMS,0.7207207207207207,"Answer: [Yes]
732"
CLAIMS,0.7216216216216216,"Justification: Yes, the main claims made in the abstract and introduction accurately reflect
733"
CLAIMS,0.7225225225225225,"the paper’s contributions and scope.
734"
CLAIMS,0.7234234234234235,"Guidelines:
735"
CLAIMS,0.7243243243243244,"• The answer NA means that the abstract and introduction do not include the claims
736"
CLAIMS,0.7252252252252253,"made in the paper.
737"
CLAIMS,0.7261261261261261,"• The abstract and/or introduction should clearly state the claims made, including the
738"
CLAIMS,0.727027027027027,"contributions made in the paper and important assumptions and limitations. A No or
739"
CLAIMS,0.7279279279279279,"NA answer to this question will not be perceived well by the reviewers.
740"
CLAIMS,0.7288288288288288,"• The claims made should match theoretical and experimental results, and reflect how
741"
CLAIMS,0.7297297297297297,"much the results can be expected to generalize to other settings.
742"
CLAIMS,0.7306306306306306,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
743"
CLAIMS,0.7315315315315315,"are not attained by the paper.
744"
LIMITATIONS,0.7324324324324324,"2. Limitations
745"
LIMITATIONS,0.7333333333333333,"Question: Does the paper discuss the limitations of the work performed by the authors?
746"
LIMITATIONS,0.7342342342342343,"Answer: [Yes]
747"
LIMITATIONS,0.7351351351351352,"Justification: The limitations of this work is discussed in Appendix F.1.
748"
LIMITATIONS,0.7360360360360361,"Guidelines:
749"
LIMITATIONS,0.736936936936937,"• The answer NA means that the paper has no limitation while the answer No means that
750"
LIMITATIONS,0.7378378378378379,"the paper has limitations, but those are not discussed in the paper.
751"
LIMITATIONS,0.7387387387387387,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
752"
LIMITATIONS,0.7396396396396396,"• The paper should point out any strong assumptions and how robust the results are to
753"
LIMITATIONS,0.7405405405405405,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
754"
LIMITATIONS,0.7414414414414414,"model well-specification, asymptotic approximations only holding locally). The authors
755"
LIMITATIONS,0.7423423423423423,"should reflect on how these assumptions might be violated in practice and what the
756"
LIMITATIONS,0.7432432432432432,"implications would be.
757"
LIMITATIONS,0.7441441441441441,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
758"
LIMITATIONS,0.7450450450450451,"only tested on a few datasets or with a few runs. In general, empirical results often
759"
LIMITATIONS,0.745945945945946,"depend on implicit assumptions, which should be articulated.
760"
LIMITATIONS,0.7468468468468469,"• The authors should reflect on the factors that influence the performance of the approach.
761"
LIMITATIONS,0.7477477477477478,"For example, a facial recognition algorithm may perform poorly when image resolution
762"
LIMITATIONS,0.7486486486486487,"is low or images are taken in low lighting. Or a speech-to-text system might not be
763"
LIMITATIONS,0.7495495495495496,"used reliably to provide closed captions for online lectures because it fails to handle
764"
LIMITATIONS,0.7504504504504504,"technical jargon.
765"
LIMITATIONS,0.7513513513513513,"• The authors should discuss the computational efficiency of the proposed algorithms
766"
LIMITATIONS,0.7522522522522522,"and how they scale with dataset size.
767"
LIMITATIONS,0.7531531531531531,"• If applicable, the authors should discuss possible limitations of their approach to
768"
LIMITATIONS,0.754054054054054,"address problems of privacy and fairness.
769"
LIMITATIONS,0.7549549549549549,"• While the authors might fear that complete honesty about limitations might be used by
770"
LIMITATIONS,0.7558558558558559,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
771"
LIMITATIONS,0.7567567567567568,"limitations that aren’t acknowledged in the paper. The authors should use their best
772"
LIMITATIONS,0.7576576576576577,"judgment and recognize that individual actions in favor of transparency play an impor-
773"
LIMITATIONS,0.7585585585585586,"tant role in developing norms that preserve the integrity of the community. Reviewers
774"
LIMITATIONS,0.7594594594594595,"will be specifically instructed to not penalize honesty concerning limitations.
775"
THEORY ASSUMPTIONS AND PROOFS,0.7603603603603604,"3. Theory Assumptions and Proofs
776"
THEORY ASSUMPTIONS AND PROOFS,0.7612612612612613,"Question: For each theoretical result, does the paper provide the full set of assumptions and
777"
THEORY ASSUMPTIONS AND PROOFS,0.7621621621621621,"a complete (and correct) proof?
778"
THEORY ASSUMPTIONS AND PROOFS,0.763063063063063,"Answer: [Yes]
779"
THEORY ASSUMPTIONS AND PROOFS,0.7639639639639639,"Justification: See the theoretical result in Appendix 1.
780"
THEORY ASSUMPTIONS AND PROOFS,0.7648648648648648,"Guidelines:
781"
THEORY ASSUMPTIONS AND PROOFS,0.7657657657657657,"• The answer NA means that the paper does not include theoretical results.
782"
THEORY ASSUMPTIONS AND PROOFS,0.7666666666666667,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
783"
THEORY ASSUMPTIONS AND PROOFS,0.7675675675675676,"referenced.
784"
THEORY ASSUMPTIONS AND PROOFS,0.7684684684684685,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
785"
THEORY ASSUMPTIONS AND PROOFS,0.7693693693693694,"• The proofs can either appear in the main paper or the supplemental material, but if
786"
THEORY ASSUMPTIONS AND PROOFS,0.7702702702702703,"they appear in the supplemental material, the authors are encouraged to provide a short
787"
THEORY ASSUMPTIONS AND PROOFS,0.7711711711711712,"proof sketch to provide intuition.
788"
THEORY ASSUMPTIONS AND PROOFS,0.7720720720720721,"• Inversely, any informal proof provided in the core of the paper should be complemented
789"
THEORY ASSUMPTIONS AND PROOFS,0.772972972972973,"by formal proofs provided in appendix or supplemental material.
790"
THEORY ASSUMPTIONS AND PROOFS,0.7738738738738739,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
791"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7747747747747747,"4. Experimental Result Reproducibility
792"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7756756756756756,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
793"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7765765765765765,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
794"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7774774774774775,"of the paper (regardless of whether the code and data are provided or not)?
795"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7783783783783784,"Answer: [Yes]
796"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7792792792792793,"Justification: We disclose all the information needed to reproduce the main experimental
797"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7801801801801802,"results of the paper to the extent that it affects the main claims and conclusions of the paper,
798"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7810810810810811,"detailed in Section 3 and Appendix D.1.
799"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.781981981981982,"Guidelines:
800"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7828828828828829,"• The answer NA means that the paper does not include experiments.
801"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7837837837837838,"• If the paper includes experiments, a No answer to this question will not be perceived
802"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7846846846846847,"well by the reviewers: Making the paper reproducible is important, regardless of
803"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7855855855855856,"whether the code and data are provided or not.
804"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7864864864864864,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
805"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7873873873873873,"to make their results reproducible or verifiable.
806"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7882882882882883,"• Depending on the contribution, reproducibility can be accomplished in various ways.
807"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7891891891891892,"For example, if the contribution is a novel architecture, describing the architecture fully
808"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7900900900900901,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
809"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.790990990990991,"be necessary to either make it possible for others to replicate the model with the same
810"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7918918918918919,"dataset, or provide access to the model. In general. releasing code and data is often
811"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7927927927927928,"one good way to accomplish this, but reproducibility can also be provided via detailed
812"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7936936936936937,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
813"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7945945945945946,"of a large language model), releasing of a model checkpoint, or other means that are
814"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7954954954954955,"appropriate to the research performed.
815"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7963963963963964,"• While NeurIPS does not require releasing code, the conference does require all submis-
816"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7972972972972973,"sions to provide some reasonable avenue for reproducibility, which may depend on the
817"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7981981981981981,"nature of the contribution. For example
818"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7990990990990992,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
819"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8,"to reproduce that algorithm.
820"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8009009009009009,"(b) If the contribution is primarily a new model architecture, the paper should describe
821"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8018018018018018,"the architecture clearly and fully.
822"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8027027027027027,"(c) If the contribution is a new model (e.g., a large language model), then there should
823"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8036036036036036,"either be a way to access this model for reproducing the results or a way to reproduce
824"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8045045045045045,"the model (e.g., with an open-source dataset or instructions for how to construct
825"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8054054054054054,"the dataset).
826"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8063063063063063,"(d) We recognize that reproducibility may be tricky in some cases, in which case
827"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8072072072072072,"authors are welcome to describe the particular way they provide for reproducibility.
828"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8081081081081081,"In the case of closed-source models, it may be that access to the model is limited in
829"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.809009009009009,"some way (e.g., to registered users), but it should be possible for other researchers
830"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.80990990990991,"to have some path to reproducing or verifying the results.
831"
OPEN ACCESS TO DATA AND CODE,0.8108108108108109,"5. Open access to data and code
832"
OPEN ACCESS TO DATA AND CODE,0.8117117117117117,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
833"
OPEN ACCESS TO DATA AND CODE,0.8126126126126126,"tions to faithfully reproduce the main experimental results, as described in supplemental
834"
OPEN ACCESS TO DATA AND CODE,0.8135135135135135,"material?
835"
OPEN ACCESS TO DATA AND CODE,0.8144144144144144,"Answer: [Yes]
836"
OPEN ACCESS TO DATA AND CODE,0.8153153153153153,"Justification: The code is provided in the supplementary marterial.
837"
OPEN ACCESS TO DATA AND CODE,0.8162162162162162,"Guidelines:
838"
OPEN ACCESS TO DATA AND CODE,0.8171171171171171,"• The answer NA means that paper does not include experiments requiring code.
839"
OPEN ACCESS TO DATA AND CODE,0.818018018018018,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
840"
OPEN ACCESS TO DATA AND CODE,0.8189189189189189,"public/guides/CodeSubmissionPolicy) for more details.
841"
OPEN ACCESS TO DATA AND CODE,0.8198198198198198,"• While we encourage the release of code and data, we understand that this might not be
842"
OPEN ACCESS TO DATA AND CODE,0.8207207207207208,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
843"
OPEN ACCESS TO DATA AND CODE,0.8216216216216217,"including code, unless this is central to the contribution (e.g., for a new open-source
844"
OPEN ACCESS TO DATA AND CODE,0.8225225225225226,"benchmark).
845"
OPEN ACCESS TO DATA AND CODE,0.8234234234234235,"• The instructions should contain the exact command and environment needed to run to
846"
OPEN ACCESS TO DATA AND CODE,0.8243243243243243,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
847"
OPEN ACCESS TO DATA AND CODE,0.8252252252252252,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
848"
OPEN ACCESS TO DATA AND CODE,0.8261261261261261,"• The authors should provide instructions on data access and preparation, including how
849"
OPEN ACCESS TO DATA AND CODE,0.827027027027027,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
850"
OPEN ACCESS TO DATA AND CODE,0.8279279279279279,"• The authors should provide scripts to reproduce all experimental results for the new
851"
OPEN ACCESS TO DATA AND CODE,0.8288288288288288,"proposed method and baselines. If only a subset of experiments are reproducible, they
852"
OPEN ACCESS TO DATA AND CODE,0.8297297297297297,"should state which ones are omitted from the script and why.
853"
OPEN ACCESS TO DATA AND CODE,0.8306306306306306,"• At submission time, to preserve anonymity, the authors should release anonymized
854"
OPEN ACCESS TO DATA AND CODE,0.8315315315315316,"versions (if applicable).
855"
OPEN ACCESS TO DATA AND CODE,0.8324324324324325,"• Providing as much information as possible in supplemental material (appended to the
856"
OPEN ACCESS TO DATA AND CODE,0.8333333333333334,"paper) is recommended, but including URLs to data and code is permitted.
857"
OPEN ACCESS TO DATA AND CODE,0.8342342342342343,"6. Experimental Setting/Details
858"
OPEN ACCESS TO DATA AND CODE,0.8351351351351352,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
859"
OPEN ACCESS TO DATA AND CODE,0.836036036036036,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
860"
OPEN ACCESS TO DATA AND CODE,0.8369369369369369,"results?
861"
OPEN ACCESS TO DATA AND CODE,0.8378378378378378,"Answer: [Yes]
862"
OPEN ACCESS TO DATA AND CODE,0.8387387387387387,"Justification: We provide the training and test details Section 3 and Appendix D.1 and
863"
OPEN ACCESS TO DATA AND CODE,0.8396396396396396,"Appendix D.2.
864"
OPEN ACCESS TO DATA AND CODE,0.8405405405405405,"Guidelines:
865"
OPEN ACCESS TO DATA AND CODE,0.8414414414414414,"• The answer NA means that the paper does not include experiments.
866"
OPEN ACCESS TO DATA AND CODE,0.8423423423423423,"• The experimental setting should be presented in the core of the paper to a level of detail
867"
OPEN ACCESS TO DATA AND CODE,0.8432432432432433,"that is necessary to appreciate the results and make sense of them.
868"
OPEN ACCESS TO DATA AND CODE,0.8441441441441442,"• The full details can be provided either with the code, in appendix, or as supplemental
869"
OPEN ACCESS TO DATA AND CODE,0.8450450450450451,"material.
870"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.845945945945946,"7. Experiment Statistical Significance
871"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8468468468468469,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
872"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8477477477477477,"information about the statistical significance of the experiments?
873"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8486486486486486,"Answer: [Yes]
874"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8495495495495495,"Justification: The proposed method is thoroughly evaluated on three domains, and the results
875"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8504504504504504,"are reported based on a statistical analysis in Section 4 and Appendix E.
876"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8513513513513513,"Guidelines:
877"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8522522522522522,"• The answer NA means that the paper does not include experiments.
878"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8531531531531531,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
879"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8540540540540541,"dence intervals, or statistical significance tests, at least for the experiments that support
880"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.854954954954955,"the main claims of the paper.
881"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8558558558558559,"• The factors of variability that the error bars are capturing should be clearly stated (for
882"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8567567567567568,"example, train/test split, initialization, random drawing of some parameter, or overall
883"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8576576576576577,"run with given experimental conditions).
884"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8585585585585586,"• The method for calculating the error bars should be explained (closed form formula,
885"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8594594594594595,"call to a library function, bootstrap, etc.)
886"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8603603603603603,"• The assumptions made should be given (e.g., Normally distributed errors).
887"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8612612612612612,"• It should be clear whether the error bar is the standard deviation or the standard error
888"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8621621621621621,"of the mean.
889"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.863063063063063,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
890"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8639639639639639,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
891"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8648648648648649,"of Normality of errors is not verified.
892"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8657657657657658,"• For asymmetric distributions, the authors should be careful not to show in tables or
893"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8666666666666667,"figures symmetric error bars that would yield results that are out of range (e.g. negative
894"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8675675675675676,"error rates).
895"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8684684684684685,"• If error bars are reported in tables or plots, The authors should explain in the text how
896"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8693693693693694,"they were calculated and reference the corresponding figures or tables in the text.
897"
EXPERIMENTS COMPUTE RESOURCES,0.8702702702702703,"8. Experiments Compute Resources
898"
EXPERIMENTS COMPUTE RESOURCES,0.8711711711711712,"Question: For each experiment, does the paper provide sufficient information on the com-
899"
EXPERIMENTS COMPUTE RESOURCES,0.872072072072072,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
900"
EXPERIMENTS COMPUTE RESOURCES,0.8729729729729729,"the experiments?
901"
EXPERIMENTS COMPUTE RESOURCES,0.8738738738738738,"Answer: [Yes]
902"
EXPERIMENTS COMPUTE RESOURCES,0.8747747747747747,"Justification: The detailed configuration of the experiments is listed with required computa-
903"
EXPERIMENTS COMPUTE RESOURCES,0.8756756756756757,"tional resources.
904"
EXPERIMENTS COMPUTE RESOURCES,0.8765765765765766,"Guidelines:
905"
EXPERIMENTS COMPUTE RESOURCES,0.8774774774774775,"• The answer NA means that the paper does not include experiments.
906"
EXPERIMENTS COMPUTE RESOURCES,0.8783783783783784,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
907"
EXPERIMENTS COMPUTE RESOURCES,0.8792792792792793,"or cloud provider, including relevant memory and storage.
908"
EXPERIMENTS COMPUTE RESOURCES,0.8801801801801802,"• The paper should provide the amount of compute required for each of the individual
909"
EXPERIMENTS COMPUTE RESOURCES,0.8810810810810811,"experimental runs as well as estimate the total compute.
910"
EXPERIMENTS COMPUTE RESOURCES,0.881981981981982,"• The paper should disclose whether the full research project required more compute
911"
EXPERIMENTS COMPUTE RESOURCES,0.8828828828828829,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
912"
EXPERIMENTS COMPUTE RESOURCES,0.8837837837837837,"didn’t make it into the paper).
913"
CODE OF ETHICS,0.8846846846846846,"9. Code Of Ethics
914"
CODE OF ETHICS,0.8855855855855855,"Question: Does the research conducted in the paper conform, in every respect, with the
915"
CODE OF ETHICS,0.8864864864864865,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
916"
CODE OF ETHICS,0.8873873873873874,"Answer: [Yes]
917"
CODE OF ETHICS,0.8882882882882883,"Justification: We confirm that the research conducted in the paper conform, in every respect,
918"
CODE OF ETHICS,0.8891891891891892,"with the NeurIPS Code of Ethics, and all the authors preserve anonymity.
919"
CODE OF ETHICS,0.8900900900900901,"Guidelines:
920"
CODE OF ETHICS,0.890990990990991,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
921"
CODE OF ETHICS,0.8918918918918919,"• If the authors answer No, they should explain the special circumstances that require a
922"
CODE OF ETHICS,0.8927927927927928,"deviation from the Code of Ethics.
923"
CODE OF ETHICS,0.8936936936936937,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
924"
CODE OF ETHICS,0.8945945945945946,"eration due to laws or regulations in their jurisdiction).
925"
BROADER IMPACTS,0.8954954954954955,"10. Broader Impacts
926"
BROADER IMPACTS,0.8963963963963963,"Question: Does the paper discuss both potential positive societal impacts and negative
927"
BROADER IMPACTS,0.8972972972972973,"societal impacts of the work performed?
928"
BROADER IMPACTS,0.8981981981981982,"Answer: [Yes]
929"
BROADER IMPACTS,0.8990990990990991,"Justification: The broader impacts of our paper are presented in Section F.1.
930"
BROADER IMPACTS,0.9,"Guidelines:
931"
BROADER IMPACTS,0.9009009009009009,"• The answer NA means that there is no societal impact of the work performed.
932"
BROADER IMPACTS,0.9018018018018018,"• If the authors answer NA or No, they should explain why their work has no societal
933"
BROADER IMPACTS,0.9027027027027027,"impact or why the paper does not address societal impact.
934"
BROADER IMPACTS,0.9036036036036036,"• Examples of negative societal impacts include potential malicious or unintended uses
935"
BROADER IMPACTS,0.9045045045045045,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
936"
BROADER IMPACTS,0.9054054054054054,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
937"
BROADER IMPACTS,0.9063063063063063,"groups), privacy considerations, and security considerations.
938"
BROADER IMPACTS,0.9072072072072072,"• The conference expects that many papers will be foundational research and not tied
939"
BROADER IMPACTS,0.9081081081081082,"to particular applications, let alone deployments. However, if there is a direct path to
940"
BROADER IMPACTS,0.909009009009009,"any negative applications, the authors should point it out. For example, it is legitimate
941"
BROADER IMPACTS,0.9099099099099099,"to point out that an improvement in the quality of generative models could be used to
942"
BROADER IMPACTS,0.9108108108108108,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
943"
BROADER IMPACTS,0.9117117117117117,"that a generic algorithm for optimizing neural networks could enable people to train
944"
BROADER IMPACTS,0.9126126126126126,"models that generate Deepfakes faster.
945"
BROADER IMPACTS,0.9135135135135135,"• The authors should consider possible harms that could arise when the technology is
946"
BROADER IMPACTS,0.9144144144144144,"being used as intended and functioning correctly, harms that could arise when the
947"
BROADER IMPACTS,0.9153153153153153,"technology is being used as intended but gives incorrect results, and harms following
948"
BROADER IMPACTS,0.9162162162162162,"from (intentional or unintentional) misuse of the technology.
949"
BROADER IMPACTS,0.9171171171171171,"• If there are negative societal impacts, the authors could also discuss possible mitigation
950"
BROADER IMPACTS,0.918018018018018,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
951"
BROADER IMPACTS,0.918918918918919,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
952"
BROADER IMPACTS,0.9198198198198199,"feedback over time, improving the efficiency and accessibility of ML).
953"
SAFEGUARDS,0.9207207207207208,"11. Safeguards
954"
SAFEGUARDS,0.9216216216216216,"Question: Does the paper describe safeguards that have been put in place for responsible
955"
SAFEGUARDS,0.9225225225225225,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
956"
SAFEGUARDS,0.9234234234234234,"image generators, or scraped datasets)?
957"
SAFEGUARDS,0.9243243243243243,"Answer: [NA]
958"
SAFEGUARDS,0.9252252252252252,"Justification: Our paper poses no such risks.
959"
SAFEGUARDS,0.9261261261261261,"Guidelines:
960"
SAFEGUARDS,0.927027027027027,"• The answer NA means that the paper poses no such risks.
961"
SAFEGUARDS,0.9279279279279279,"• Released models that have a high risk for misuse or dual-use should be released with
962"
SAFEGUARDS,0.9288288288288288,"necessary safeguards to allow for controlled use of the model, for example by requiring
963"
SAFEGUARDS,0.9297297297297298,"that users adhere to usage guidelines or restrictions to access the model or implementing
964"
SAFEGUARDS,0.9306306306306307,"safety filters.
965"
SAFEGUARDS,0.9315315315315316,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
966"
SAFEGUARDS,0.9324324324324325,"should describe how they avoided releasing unsafe images.
967"
SAFEGUARDS,0.9333333333333333,"• We recognize that providing effective safeguards is challenging, and many papers do
968"
SAFEGUARDS,0.9342342342342342,"not require this, but we encourage authors to take this into account and make a best
969"
SAFEGUARDS,0.9351351351351351,"faith effort.
970"
LICENSES FOR EXISTING ASSETS,0.936036036036036,"12. Licenses for existing assets
971"
LICENSES FOR EXISTING ASSETS,0.9369369369369369,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
972"
LICENSES FOR EXISTING ASSETS,0.9378378378378378,"the paper, properly credited and are the license and terms of use explicitly mentioned and
973"
LICENSES FOR EXISTING ASSETS,0.9387387387387387,"properly respected?
974"
LICENSES FOR EXISTING ASSETS,0.9396396396396396,"Answer: [Yes]
975"
LICENSES FOR EXISTING ASSETS,0.9405405405405406,"Justification: All the assets, used in our paper, are properly credited and we explicitly
976"
LICENSES FOR EXISTING ASSETS,0.9414414414414415,"mention and properly respect the license and terms of use.
977"
LICENSES FOR EXISTING ASSETS,0.9423423423423424,"Guidelines:
978"
LICENSES FOR EXISTING ASSETS,0.9432432432432433,"• The answer NA means that the paper does not use existing assets.
979"
LICENSES FOR EXISTING ASSETS,0.9441441441441442,"• The authors should cite the original paper that produced the code package or dataset.
980"
LICENSES FOR EXISTING ASSETS,0.945045045045045,"• The authors should state which version of the asset is used and, if possible, include a
981"
LICENSES FOR EXISTING ASSETS,0.9459459459459459,"URL.
982"
LICENSES FOR EXISTING ASSETS,0.9468468468468468,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
983"
LICENSES FOR EXISTING ASSETS,0.9477477477477477,"• For scraped data from a particular source (e.g., website), the copyright and terms of
984"
LICENSES FOR EXISTING ASSETS,0.9486486486486486,"service of that source should be provided.
985"
LICENSES FOR EXISTING ASSETS,0.9495495495495495,"• If assets are released, the license, copyright information, and terms of use in the
986"
LICENSES FOR EXISTING ASSETS,0.9504504504504504,"package should be provided. For popular datasets, paperswithcode.com/datasets
987"
LICENSES FOR EXISTING ASSETS,0.9513513513513514,"has curated licenses for some datasets. Their licensing guide can help determine the
988"
LICENSES FOR EXISTING ASSETS,0.9522522522522523,"license of a dataset.
989"
LICENSES FOR EXISTING ASSETS,0.9531531531531532,"• For existing datasets that are re-packaged, both the original license and the license of
990"
LICENSES FOR EXISTING ASSETS,0.9540540540540541,"the derived asset (if it has changed) should be provided.
991"
LICENSES FOR EXISTING ASSETS,0.954954954954955,"• If this information is not available online, the authors are encouraged to reach out to
992"
LICENSES FOR EXISTING ASSETS,0.9558558558558559,"the asset’s creators.
993"
NEW ASSETS,0.9567567567567568,"13. New Assets
994"
NEW ASSETS,0.9576576576576576,"Question: Are new assets introduced in the paper well documented and is the documentation
995"
NEW ASSETS,0.9585585585585585,"provided alongside the assets?
996"
NEW ASSETS,0.9594594594594594,"Answer: [NA]
997"
NEW ASSETS,0.9603603603603603,"Justification: This paper does not release new assets.
998"
NEW ASSETS,0.9612612612612612,"Guidelines:
999"
NEW ASSETS,0.9621621621621622,"• The answer NA means that the paper does not release new assets.
1000"
NEW ASSETS,0.9630630630630631,"• Researchers should communicate the details of the dataset/code/model as part of their
1001"
NEW ASSETS,0.963963963963964,"submissions via structured templates. This includes details about training, license,
1002"
NEW ASSETS,0.9648648648648649,"limitations, etc.
1003"
NEW ASSETS,0.9657657657657658,"• The paper should discuss whether and how consent was obtained from people whose
1004"
NEW ASSETS,0.9666666666666667,"asset is used.
1005"
NEW ASSETS,0.9675675675675676,"• At submission time, remember to anonymize your assets (if applicable). You can either
1006"
NEW ASSETS,0.9684684684684685,"create an anonymized URL or include an anonymized zip file.
1007"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9693693693693693,"14. Crowdsourcing and Research with Human Subjects
1008"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9702702702702702,"Question: For crowdsourcing experiments and research with human subjects, does the paper
1009"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9711711711711711,"include the full text of instructions given to participants and screenshots, if applicable, as
1010"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.972072072072072,"well as details about compensation (if any)?
1011"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.972972972972973,"Answer: [NA]
1012"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9738738738738739,"Justification: Our paper does not involve crowdsourcing nor research with human subjects.
1013"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9747747747747748,"Guidelines:
1014"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9756756756756757,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1015"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9765765765765766,"human subjects.
1016"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9774774774774775,"• Including this information in the supplemental material is fine, but if the main contribu-
1017"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9783783783783784,"tion of the paper involves human subjects, then as much detail as possible should be
1018"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9792792792792793,"included in the main paper.
1019"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9801801801801802,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
1020"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.981081081081081,"or other labor should be paid at least the minimum wage in the country of the data
1021"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9819819819819819,"collector.
1022"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9828828828828828,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
1023"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9837837837837838,"Subjects
1024"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9846846846846847,"Question: Does the paper describe potential risks incurred by study participants, whether
1025"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9855855855855856,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
1026"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9864864864864865,"approvals (or an equivalent approval/review based on the requirements of your country or
1027"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9873873873873874,"institution) were obtained?
1028"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9882882882882883,"Answer: [NA]
1029"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9891891891891892,"Justification: Our paper does not involve crowdsourcing nor research with human subjects.
1030"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9900900900900901,"Guidelines:
1031"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.990990990990991,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1032"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9918918918918919,"human subjects.
1033"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9927927927927928,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
1034"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9936936936936936,"may be required for any human subjects research. If you obtained IRB approval, you
1035"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9945945945945946,"should clearly state this in the paper.
1036"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9954954954954955,"• We recognize that the procedures for this may vary significantly between institutions
1037"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9963963963963964,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
1038"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9972972972972973,"guidelines for their institution.
1039"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9981981981981982,"• For initial submissions, do not include any information that would break anonymity (if
1040"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9990990990990991,"applicable), such as the institution conducting the review.
1041"
