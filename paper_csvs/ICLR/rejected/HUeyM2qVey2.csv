Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.00202020202020202,"We analyze neural networks composed of bijective ﬂows and injective expansive
elements. We ﬁnd that such networks universally approximate a large class of
manifolds simultaneously with densities supported on them. Among others, our
results apply to the well-known coupling and autoregressive ﬂows. We build on
the work of Teshima et al. 2020 on bijective ﬂows and study injective architectures
proposed in Brehmer et al. 2020 and Kothari et al. 2021. Our results leverage a
new theoretical device called the embedding gap, which measures how far one
continuous manifold is from embedding another. We relate the embedding gap
to a relaxation of universally we call the manifold embedding property, capturing
the geometric part of universality. Our proof also establishes that optimality of a
network can be established “in reverse,” resolving a conjecture made in Brehmer
et al. 2020 and opening the door for simple layer-wise training schemes. Finally,
we show that the studied networks admit an exact layer-wise projection result,
Bayesian uncertainty quantiﬁcation, and black-box recovery of network weights."
INTRODUCTION,0.00404040404040404,"1
INTRODUCTION"
INTRODUCTION,0.006060606060606061,"In the past several years, invertible ﬂow networks emerged as powerful deep learning models to
learn maps between distributions (Durkan et al., 2019a; Grathwohl et al., 2018; Huang et al., 2018;
Jaini et al., 2019; Kingma et al., 2016; Kingma & Dhariwal, 2018; Kobyzev et al., 2020; Kruse et al.,
2019; Papamakarios et al., 2019). They generate excellent samples (Kingma & Dhariwal, 2018) and
facilitate solving scientiﬁc inference problems (Brehmer & Cranmer, 2020; Kruse et al., 2021)."
INTRODUCTION,0.00808080808080808,"By design, invertible ﬂows are bijective and, hence, may not be a natural choice when the target
distribution has low-dimensional support. This problem can be overcome by combining bijective
ﬂows with expansive, injective layers, which map to higher dimensions (Brehmer & Cranmer, 2020;
Cunningham et al., 2020; Kothari et al., 2021). Despite their empirical success, the theoretical
aspects of such globally injective architectures are not well understood."
INTRODUCTION,0.010101010101010102,"In this work, we present approximation-theoretic properties of injective ﬂows whose architecture
combines bijective ﬂows and expansive, injective layers, with an emphasis on approximating mea-
sures with low-dimensional support. We state conditions under which these networks are universal
approximators and describe how their design enables applications to inference and inverse problems."
PRIOR WORK,0.012121212121212121,"1.1
PRIOR WORK"
PRIOR WORK,0.014141414141414142,"The idea to combine invertible (coupling) layers with expansive layers has been explored by
Brehmer & Cranmer (2020) and Kothari et al. (2021). Brehmer & Cranmer (2020) combine two
ﬂow networks with a simple expansive element (in the sense made precise in Section 2.1) and ob-
tain a network that paramterizes probability distributions supported on manifolds. They suggest that
such constructions may be universal but neither they not Kothari et al. (2021) derive theoretical re-
sults. We discuss in detail the connection between these two empirical works and the approximation
results derived here in Appendix B.1."
PRIOR WORK,0.01616161616161616,"Kothari et al. (2021) propose expansive coupling layers and build networks similar to that of
Brehmer & Cranmer (2020) but with an arbitrary number of expressive and expansive elements.
They observe that the resulting network trains much faster with a smaller memory footprint, while
producing high-quality samples on a variety of benchmark datasets."
PRIOR WORK,0.01818181818181818,Under review as a conference paper at ICLR 2022
PRIOR WORK,0.020202020202020204,"While to the best of our knowledge, there are no approximation-theoretic results for injective ﬂows,
there exists a body of work on universality of invertible ﬂows; see Kobyzev et al. (2020) for an
overview. Several works show that certain bijective ﬂow architectures are distributionally universal.
This was proved for autoregressive ﬂows with sigmoidal activations by Huang et al. (2018) and for
sum-of-squares polynomial ﬂows by Jaini et al. (2019). Teshima et al. (2020) show that several
ﬂow networks including those from Huang et al. (2018) and Jaini et al. (2019) are also universal
approximators of diffeomorphisms."
PRIOR WORK,0.022222222222222223,"The injective ﬂows considered here have key applications in inference and inverse problems; for an
overview of deep learning approaches to inverse problems, see Arridge et al. (2019). Bora et al.
(2017) proposed to regularize compressed sensing problems by constraining the recovery to the
range of (pre-trained) generative models. Injective ﬂows with efﬁcient inverses as generative mod-
els gives an algorithmic projection1 on the range, which facilitates implementation of reconstruction
algorithms. An alternative approach is Bayesian, where ﬂows are used to obtain tractable varia-
tional approximations of posterior distributions over parameters of interest, via supervised training
on labeled input-output data pairs. Ardizzone et al. (2018) encode the dimension-reducing forward
process by an invertible neural network (INN), with additional outputs used to encode posterior
variability. Invertibility guarantees that a model of the inverse process is learned implicitly. For a
given measurement, the inverse pass of the INN approximates the posterior over parameters. Sun &
Bouman (2020) propose variational approximations of the posterior using an untrained deep genera-
tive model. They train a normalizing ﬂow which produces samples from the posterior, with the prior
and the noise model given implicitly by the regularized misﬁt functional. In Kothari et al. (2021) this
procedure is adapted to priors speciﬁed by injective ﬂows which yields signiﬁcant improvements in
computational efﬁciency."
OUR CONTRIBUTION,0.024242424242424242,"1.2
OUR CONTRIBUTION"
OUR CONTRIBUTION,0.026262626262626262,"We derive new approximation results for neural networks composed of bijective ﬂows and injec-
tive expansive layers, including those introduced by Brehmer & Cranmer (2020) and Kothari et al.
(2021). We show that these networks universally jointly approximate a large class of manifolds and
densities supported on them."
OUR CONTRIBUTION,0.028282828282828285,"We build on the results of Teshima et al. (2020) and develop a new theoretical device which we refer
to as the embedding gap. This gap is a measure of how nearly a mapping from Ro →Rm embeds an
n-dimensional manifold in Rm, where n ≤o. We ﬁnd a natural relationship between the embedding
gap and the problem of approximating probability measures with low-dimensional support."
OUR CONTRIBUTION,0.030303030303030304,"We then relate the embedding gap to a relaxation of universality we call manifold embedding prop-
erty. We show that this property captures the essential geometric aspects of universality and uncover
important topological restrictions on the approximation power of these networks, to our knowledge,
heretofore unknown in the literature. We give an example of an absolutely continuous measure µ
and embedding f : R2 →R3 such that f #µ can not be approximated with combinations of ﬂow
layers and linear expansive layers. This may be surprising since it was previously thought that net-
works such as those of Brehmer & Cranmer (2020) can approximate any “nice” density supported
on a “nice” manifold. We establish universality for manifolds with suitable topology, described in
terms of extendable embeddings. Our proof shows that optimality of the approximating network can
be established in reverse: optimality of a given layer can be established without optimality of pre-
ceding layers. This settles a (generalization of a) conjecture posed for a two-layer case in (Brehmer
& Cranmer, 2020). Finally, we show that these universal architectures are also practical and admit
exact layer-wise projections, as well as other properties discussed in Section 3.4."
DESCRIPTION OF THE ARCHITECTURE,0.03232323232323232,"2
DESCRIPTION OF THE ARCHITECTURE"
DESCRIPTION OF THE ARCHITECTURE,0.03434343434343434,"Let C(X, Y ) denote the space of continuous functions X →Y . We study networks in F ⊂
C(X, Y ) that are of the form:"
DESCRIPTION OF THE ARCHITECTURE,0.03636363636363636,"F = T nL
L
◦RnL−1,nL
L
◦· · · ◦T n1
1
◦Rn0,n1
1
◦T n0
0
(1)"
DESCRIPTION OF THE ARCHITECTURE,0.03838383838383838,1Idempotent but in general not orthogonal.
DESCRIPTION OF THE ARCHITECTURE,0.04040404040404041,Under review as a conference paper at ICLR 2022
DESCRIPTION OF THE ARCHITECTURE,0.04242424242424243,"where Rnℓ−1,nℓ
ℓ
⊂C(Rnℓ−1, Rnℓ), T nℓ
ℓ
⊂C(Rnℓ, Rnℓ), L ∈N is the number of networks, n0 = n,
nL = m, and nℓ≥nℓ−1 for ℓ= 1, . . . , L. We introduce a well-tuned shorthand notation and write
H ◦G := {h ◦g: h ∈H, g ∈G} throughout the paper."
DESCRIPTION OF THE ARCHITECTURE,0.044444444444444446,"We identify R with the expansive layers and T with the bijective ﬂows. Loosely speaking, the pur-
pose of the expansive layers is to allow the network to parameterize high-dimensional functions by
low-dimensional coordinates in an injective way. The ﬂow networks give the network the expressiv-
ity necessary for universal approximation of manifold-supported distributions."
EXPANSIVE LAYERS,0.046464646464646465,"2.1
EXPANSIVE LAYERS"
EXPANSIVE LAYERS,0.048484848484848485,"The expansive elements transform an n-dimensional manifold M embedded in Rnℓ−1, and embed
it in a higher dimensional space Rnℓ. To preserve the topology of the manifold, this must be done
injectively. We thus make the following assumptions about the expansive elements:"
EXPANSIVE LAYERS,0.050505050505050504,"Deﬁnition 1 (Expansive Element). Let ℓ= 1, . . . , L, and Rnℓ−1,nℓ
ℓ
be a family of functions from
Rnℓ−1 →Rnℓ. Rnℓ−1,nℓ
ℓ
is a family of expansive elements if every R ∈Rnℓ−1,nℓ
ℓ
is injective and
Lipschitz."
EXPANSIVE LAYERS,0.052525252525252523,Examples of expansive elements include
EXPANSIVE LAYERS,0.05454545454545454,"(R1) Zero padding: R(x) =

xT , 0(m−n)T where 0(m−n) is the zero vector (Brehmer & Cran-
mer, 2020)."
EXPANSIVE LAYERS,0.05656565656565657,"(R2) Multiplication by an arbitrary full-rank matrix, or one-by-one convolution:"
EXPANSIVE LAYERS,0.05858585858585859,"R(x) = Wx,
or
R(x) = w ⋆x
(2)"
EXPANSIVE LAYERS,0.06060606060606061,"where W ∈Rm×n and rank(W) = n (Cunningham et al., 2020), and w is a convolution
kernel ⋆denotes convolution Kingma & Dhariwal (2018)."
EXPANSIVE LAYERS,0.06262626262626263,"(R3) Injective ReLU layers:
R
= ReLU(Wx), W
=

BT , −DBT , M T T , R(x) =
ReLU
 
wT , −wT 
⋆x

for matrix B ∈GLn(R), positive diagonal matrix D ∈Rn×n,
and arbitrary matrix M ∈R(m−2n)×n (Puthawala et al., 2020)."
EXPANSIVE LAYERS,0.06464646464646465,"(R4) Injective ReLU networks (Puthawala et al., 2020, Theorem 15). These are functions R :
Rn →Rm of the form R(x) = WL+1 ReLU(. . . ReLU(W1x+b1) . . . )+bL where Wℓare
nℓ+1 × nℓmatrices and bℓare the bias vectors in Rnℓ+1. The weight matrices WL satisfy
the Directed Spanning Set (DSS) condition for ℓ≤L (that make all layers injective) and
WL+1 is a generic matrix which makes the map R : Rn →Rm injective. Note that the DSS
condition requires that nℓ≥2nℓ−1 + 1 for ℓ≤L and we have n1 = n and nL+1 = m."
BIJECTIVE FLOW NETWORKS,0.06666666666666667,"2.2
BIJECTIVE FLOW NETWORKS"
BIJECTIVE FLOW NETWORKS,0.06868686868686869,"The bulk of our theoretical analysis is devoted to expressive elements. The expressive elements bend
the range of the expansive elements into the correct shape. We make the following assumptions
about the expressive elements:"
BIJECTIVE FLOW NETWORKS,0.0707070707070707,"Deﬁnition 2 (Bijective Flow Network). Let ℓ= 0, . . . , L be given and let T nℓ
ℓ
⊂C(Rnℓ, Rnℓ).
T nℓ
ℓ
is a family of bijective ﬂow networks if every T ∈T nℓ
ℓ
is Lipschitz continuous and bijective."
BIJECTIVE FLOW NETWORKS,0.07272727272727272,Examples of bijective ﬂow networks include
BIJECTIVE FLOW NETWORKS,0.07474747474747474,"(T1) Coupling ﬂows, introduced by Dinh et al. (2014) consider R(x) = Hk ◦· · · ◦H1(x) where"
BIJECTIVE FLOW NETWORKS,0.07676767676767676,"Hi(x) =

hi
 
[x]1:d , gi
 
[x]d+1:n
"
BIJECTIVE FLOW NETWORKS,0.07878787878787878,[x]d+1:n
BIJECTIVE FLOW NETWORKS,0.08080808080808081,"
.
(3)"
BIJECTIVE FLOW NETWORKS,0.08282828282828283,"In Eqn. 3, hi : Rd × Re →Rd is invertible w.r.t. the ﬁrst argument given the second, and
gi : Rn−d →Re is arbitrary. Typically in practice the operation in Eqn. 3 is combined with
additional invertible operations such as permutations, masking or convolutions Dinh et al.
(2014; 2016); Kingma & Dhariwal (2018)."
BIJECTIVE FLOW NETWORKS,0.08484848484848485,Under review as a conference paper at ICLR 2022
BIJECTIVE FLOW NETWORKS,0.08686868686868687,"(T2) Autoregressive ﬂows, introduced by Kingma et al. (2016) are generalizations of triangular
ﬂows A: Rn →Rn where for i = 1, . . . , n the i’th value of A is given by of the form"
BIJECTIVE FLOW NETWORKS,0.08888888888888889,"[A]i (x) = hi
 
[x]i , gi
 
[x]1:i−1

(4)"
BIJECTIVE FLOW NETWORKS,0.09090909090909091,"In Eqn. 4, hi : R × Rm →R where again hi is invertible w.r.t. the ﬁrst argument given
the second, and gi : Ri−1 →Rm is arbitrary except for g1 = 0. In Huang et al. (2018), the
authors choose hi(x, y), where y ∈Rm, to be a multi-layer perceptron (MLP) of the form"
BIJECTIVE FLOW NETWORKS,0.09292929292929293,"hi(x, y) = φ ◦Wp,y ◦· · · ◦φ ◦W1,y(x)
(5)"
BIJECTIVE FLOW NETWORKS,0.09494949494949495,where φ is a sigmoidal increasing non-linear activation function.
MAIN RESULTS,0.09696969696969697,"3
MAIN RESULTS"
EMBEDDING GAP,0.09898989898989899,"3.1
EMBEDDING GAP"
EMBEDDING GAP,0.10101010101010101,"We call a function f an embedding and denote it by f ∈emb(X, Y ) if f : X →Y is continuous,
injective, and f −1 : f(X) →X is continuous2. Also we denote embk(Rn, Rm) = emb(Rn, Rm)∩
Ck(Rn, Rm). In order to set up our result concerning embedding of manifolds, we ﬁrst need a way to
measure the degree to which a mapping g ∈emb(Ro, Rm) nearly embeds a manifold M = f(K)
for compact K ⊂Rn and f ∈emb(K, Rm). With this in mind we introduce embedding gap
BK,W (f, g), a non-symmetric notion of distance between f and g. Later in the paper, f will be the
function to be approximated, and g a ﬂow-network to be the approximator."
EMBEDDING GAP,0.10303030303030303,"Deﬁnition 3 (Embedding Gap). Let K ⊂Rn be compact and non-empty, W ⊂Ro contain the
closure of set U which is open in the subspace topology of some vector subspace V of dimension p,
where n ≤p ≤o ≤m, f ∈emb(K, Rm), and g ∈emb(W, Rm). Then we deﬁne the Embedding
Gap between f and g on sets K and W as"
EMBEDDING GAP,0.10505050505050505,"BK,W (f, g) =
inf
r∈emb(f(K),g(W )) ∥I −r∥L∞(f(K))
(6)"
EMBEDDING GAP,0.10707070707070707,"where I : f(K) →f(K) is the identity function and ∥h∥L∞(X) = ess supx∈X ∥h(x)∥2 for h: X →
Y . We refer to the embedding gap between f and g without specifying K and W when it is clear
from context."
EMBEDDING GAP,0.10909090909090909,"Remark 1. As W ⊂Ro contains U, an open set in V , there is an afﬁne map A : Rn →V such that
A(K) ⊂W. Then, the map r0 = g ◦A ◦f −1 : f(K) →g(W) is an injective continuous map from
a compact set to its range and hence r0 ∈emb(f(K), g(W)) . Thus, in the above inﬁmum the set
emb(f(K), g(W)) is non-empty."
EMBEDDING GAP,0.1111111111111111,"Before giving properties of BK,W (f, g), we brieﬂy describe its interpretation and meaning. We
denote by P(X) the set of probability measures over X. If the embedding gap between two func-
tions is small, then g is nearly an embedding of the range of f into Ro. BK,W (f, g) is constructed
expressly to serve as an upper bound"
EMBEDDING GAP,0.11313131313131314,"inf
µo∈P(W ) W2 (f #µn, g#µo) ≤BK,W (f, g)"
EMBEDDING GAP,0.11515151515151516,"where µn ∈P(K) is given, and W2 (ν1, ν2) denotes the Wasserstein-2 distance with ℓ2 ground
metric (Villani, 2008), as shown in Lemma 7 part 5. The above result has a simple meaning in the
context of machine learning. Suppose we want to learn a generative model g to (approximately)
sample from a probability measure ν with low-dimensional support, by applying g to samples from
a base distribution µo. Suppose further that ν is a pushforward of some (known or unknown) dis-
tribution µn via f. The embedding gap BK,W (f, g) then upper bounds the 2-Wasserstein distance
between ν and g#µ0 for the best possible choice of µo.3"
EMBEDDING GAP,0.11717171717171718,"2Note that if X is a compact set, then continuity of the of f −1 : f(X) →X is automatic, and need not be
assumed (Sutherland, 2009, Cor. 13.27). Moreover, if f : Rn →Rm is a continuous injective map that satisﬁes
|f(x)| →∞as |x| →∞, then by (Mukherjee, 2015, Cor. 2.1.23) the map f −1 : f(Rn) →Rn is continuous.
3The choice of p-Wasserstein distance is suitable for measures with mismatched low-dimensional support;
this has been widely exploited in training generative models (Arjovsky et al., 2017)."
EMBEDDING GAP,0.1191919191919192,Under review as a conference paper at ICLR 2022
EMBEDDING GAP,0.12121212121212122,"Figure 1: A visualization of the embedding gap. In all three ﬁgures we plot f and gi for Left: i = 1,
Center: i = 2 and Right: i = 3. Visually, we see that gi approaches f as increases, and we compute
BK,W (f, g1) > BK,W (f, g2) > BK,W (f, g3) = 0."
EMBEDDING GAP,0.12323232323232323,"The embedding r can be interpreted as a candidate transport map from any measure pushed forward
by f, that can be pulled back through g. Loosely speaking, for µ′
o = g−1 ◦r ◦f #µn, r is a valid
Wasserstein transport map that transports f #µn to g#µ′
o with cost no more than ∥I −r∥L∞(f(K)).
See Fig. 1 for a visualization of the embedding gap between two toy functions. Further, the em-
bedding gap satisﬁes inequalities useful for studying networks of the form of Eqn. 1, see Lemma
7."
MANIFOLD EMBEDDING PROPERTY,0.12525252525252525,"3.2
MANIFOLD EMBEDDING PROPERTY"
MANIFOLD EMBEDDING PROPERTY,0.12727272727272726,"We now introduce a central concept, the manifold embedding property (MEP). A family of net-
works satisﬁes the MEP if it nearly embeds a large class of manifolds of certain dimension and
regularity, as measured by the embedding gap. The MEP is a property of a family of functions
Eo,m ⊂emb(W, Rm) where W ⊂Ro. For this manuscript, Eo,m will always be formed by taking
Eo,m := T m ◦Ro,m."
MANIFOLD EMBEDDING PROPERTY,0.1292929292929293,"We note here that the MEP is closely related to the question of whether or not a given n-dimensional
manifold M that is an image of an embedding f : K →Rm, that is, M = f(K), K ⊂Rn, can be
approximated by the images of neural networks E : K →Rm of a given type. In particular, we will
consider neural networks that are compositions where R : Ro →Rm are ‘simple’ non-universal
functions, and ﬂow-maps T : Rm →Rm that are diffeomorphism. This choice of applying non-
universal expansive layers, and then diffromorphisms has some topological consequences, which we
discuss below."
MANIFOLD EMBEDDING PROPERTY,0.13131313131313133,TOPOLOGICAL OBSTRUCTIONS TO MANIFOLD LEARNING WITH NEURAL NETWORKS
MANIFOLD EMBEDDING PROPERTY,0.13333333333333333,"We ﬁnd that using non-universal expansive layers, followed by layers imposes some topological
conditions on what can be approximated. We illustrate this with the following problem. When
n = 2, m = 3, and K = S1 ⊂R2 is the circle. We consider maps E = T ◦R : Rn →Rm where
R : Rn →Rm is, e.g., a linear map of rank n, and T : Rm →Rm is a coupling ﬂow which is a
homeomorphism. Let f ∈emb(K, R3) be an embedding that maps K to a trefoil knot M = f(S1),
see Fig. 2. Such a function f can not be written as a restriction T ◦R on S1. In Sec. C.2.2 we
prove this fact and build a related example where a measure, µ ∈P(R2), supported on an annulus
is pushed forward to a measure supported on a knotted ribbon in R3 by an embedding g: R2 →R3.
For this measure, there are no E := T ◦R, with linear injective R and embedding T, such that
g#µ = E#µ."
MANIFOLD EMBEDDING PROPERTY,0.13535353535353536,"To sidestep this fundamental difﬁculty, we deﬁne the MEP property for a certain subclass of mani-
folds {f(K) : f ∈F}. Finally, when considering ﬂow networks which are universal approximators
of C2 diffeomorphisms, we restrict the class of manifolds to be approximated even further. This is
because manifolds that are homeomorphic are not necessarily diffeomorphic; for example exotic
spheres. These are topological structures that are homeomorphic, but not diffeomorphic, to the
sphere Milnor (1956). Moreover, it is known that general homeomorphisms F : Rm →Rm can"
MANIFOLD EMBEDDING PROPERTY,0.13737373737373737,Under review as a conference paper at ICLR 2022 ↑
MANIFOLD EMBEDDING PROPERTY,0.1393939393939394,"Figure 2: An illustration of the case when n = 2, m = 3, and K = S1 is the
circle. Here f : S1 →R3 is an embedding such that the curve M = f(S1) is
a trefoil knot. Due to knot theoretical reasons, there are no map E = T ◦R :
R2 →R3 such that E(S1) = M, where R : R2 →R3 is a full rank linear
map and T : R3 →R3 is a homeomorphism. This shows that a combination of
linear maps and coupling ﬂow maps can not represent all embedded manifolds.
For this reason, we deﬁne the class I(Rn, Rm) of extendable embeddings f.
A similar 2-dimensional example can be obtained to a knotted ribbon, see Sec.
C.2.2."
MANIFOLD EMBEDDING PROPERTY,0.1414141414141414,"not be approximated in C0-topology by C2-smooth diffeomorphisms T : Rm →Rm. See M¨uller
(2014) for a precise statement. However, all C1-smooth diffeomorphims F : Rm →Rm can be
approximated in the strong topology of C1 by C2-smooth diffeomorphims ˜F : Rm →Rm, ℓ≥k,
see Hirsch (2012), Ch. 2, Theorem 2.7. Because of this, we have to pay attention to the smoothness
of the maps in the subset F ⊂emb(K, Rm)."
MANIFOLD EMBEDDING PROPERTY,0.14343434343434344,"For these reasons, when we refer to the MEP, we consider it with respect to a class of functions F ⊂
emb(Rn, Rm). The MEP can be interpreted as a density statement, saying that our networks Eo,m
are dense in the ‘BK,W distance’ in the set F ⊂emb(Rn, Rm). Two examples we are particularly
interested in are the following. First, when F = Φ ◦A where A : Rn →Rm are linear maps of rank
n and Φ : Rm →Rm are Ck diffeomorphisms with k ≥1. Second, when F ∈emb(Rn, Rm).
Deﬁnition 4. Let Eo,m ⊂emb(Ro, Rm) and Fn,m ⊂emb(Rn, Rm) be two families of functions.
We say that Eo,m has the m, n, o Manifold Embedding Property (MEP) w.r.t. Fn,m if for every
compact non-empty set K ⊂Rn, f ∈Fn,m, and ϵ > 0, there is an E ∈Eo,m and a compact set
W ⊂Ro such that the restriction of f in K and the restriction of E in W satisfy"
MANIFOLD EMBEDDING PROPERTY,0.14545454545454545,"BK,W (f, E) < ϵ.
(7)"
MANIFOLD EMBEDDING PROPERTY,0.14747474747474748,"When it is clear from the context, we abbreviate the m, n, o MEP w.r.t. Fn,m simply by the m, n, o
MEP, or simply the MEP."
MANIFOLD EMBEDDING PROPERTY,0.1494949494949495,"We also note here that if a model from Ro →Rm is a uniform universal approximator of Fo,m =
C0(Rn, Rm) on compact sets, such as those considered in Yarotsky (2017; 2018), then it has the
m, n, o MEP w.r.t Fn,mfor any n ≤o. Thus, networks that are uniform universal approximators
automatically possess the MEP, as shown in Lemma 1. However, we have a result that applies to
network which are uniform universal approximators.
Deﬁnition 5 (Uniform Universal Approximator). For a non-empty subset Fn,m ⊂C(Rn, Rm), a
family En,m ⊂C(Rn, Rm) is said to be a uniform universal approximator of Fn,m if for every
f ∈Fn,m, every non-empty compact K ⊂Rn, and each ϵ > 0, there is an E ∈En,m satisfying:"
MANIFOLD EMBEDDING PROPERTY,0.15151515151515152,"sup
x∈K
∥f(x) −E(x)∥2 < ϵ.
(8)"
MANIFOLD EMBEDDING PROPERTY,0.15353535353535352,"We also have the following function class, which is key to our analysis.
Deﬁnition 6 (Extendable Embeddings). Let I(Rn, Rm) be the set of functions F : Rn →Rm that
are compositions F = Φ ◦R where R : Rn →Rm is a linear map of rank n and Φ : Rm →Rm
is a C1-smooth diffeomorphism. We call I(Rn, Rm) be the set of extendable embeddings. We also
denote Ik(Rn, Rm) = I(Rn, Rm) ∩Ck(Rn, Rm). We observe that I(Rn, Rm) ⊂emb(Rn, Rm).
Lemma 1.
(i) If Ro,m is a universal approximator of C(Rn, Rm) and I ∈T m where I is the
identity map, then Eo,m := T m ◦Ro,m has the MEP w.r.t. emb(Rn, Rm)."
MANIFOLD EMBEDDING PROPERTY,0.15555555555555556,"(ii) If Ro,m is such that there is an injective R ∈Ro,m and open set U ⊂Ro such that R"
MANIFOLD EMBEDDING PROPERTY,0.15757575757575756,"U is
linear, and T m is a sup universal approximator in the space of Diff2(Rm, Rm), in the sense
of Teshima et al. (2020), of the C2-smooth diffeomorphisms, then Eo,m := T m ◦Ro,m has
the MEP w.r.t. F = I(Rn, Rm)."
MANIFOLD EMBEDDING PROPERTY,0.1595959595959596,"The proof of Lemma 1 is in Appendix C.2.1. It has the following implications for the architectures
studied in Section 2.
Example 1. Let E := T m ◦Ro,m, then"
MANIFOLD EMBEDDING PROPERTY,0.16161616161616163,Under review as a conference paper at ICLR 2022
MANIFOLD EMBEDDING PROPERTY,0.16363636363636364,"(i) If T m is either (T1) or (T2) and Ro,m is (R4), then Eo,m has the m, n, o MEP w.r.t.
emb(Rn, Rm)."
MANIFOLD EMBEDDING PROPERTY,0.16565656565656567,"(ii) If T m is (T2) with sigmoidal activations Huang et al. (2018), then if Ro,m is any of (R1),
..., (R4), then Eo,m has the m, n, o MEP w.r.t. I(Rn, Rm)."
MANIFOLD EMBEDDING PROPERTY,0.16767676767676767,The proof of Example 1 is in Appendix C.2.3.
MANIFOLD EMBEDDING PROPERTY,0.1696969696969697,"We add a remark here showing that if m is sufﬁciently larger than n, and subject to some regularity
assumptions, The two classes considered in Example 1 coincide."
MANIFOLD EMBEDDING PROPERTY,0.1717171717171717,"Lemma 2. When m ≥3n + 1 and k ≥1, for any Ck embedding f ∈embk(Rn, Rm) and compact
set K ⊂Rn, there is a map in the closures of the ﬂow type neural network E ∈Ik(Rn, Rm) such
that E(K) = f(K). Moreover,"
MANIFOLD EMBEDDING PROPERTY,0.17373737373737375,"Ik(K, Rm) = embk(K, Rm)
(9)"
MANIFOLD EMBEDDING PROPERTY,0.17575757575757575,"This means that the topological structure of a manifold to approximate M in Rm is diffeomorphic
to the set K ⊂Rn, and thus some ﬂow type network can approximate M."
MANIFOLD EMBEDDING PROPERTY,0.17777777777777778,The proof of Lemma 2 in Appendix C.2.4.
MANIFOLD EMBEDDING PROPERTY,0.1797979797979798,"We ﬁnally have two more lemmas to present before moving on to our discussion of universality as
it ties to the MEP.
Lemma 3. Let Fn,o ⊂emb(Rn, Ro) and Fo,m ⊂emb(R0, Rm) Ep,o
1
⊂emb(Rp, Ro) have the
o, n, p MEP w.r.t. Fn,o ⊂emb(Rn, Ro) and Eo,m
2
⊂emb(Ro, Rm) have the m, o, o MEP w.r.t.
Fo,m ⊂emb(Ro, Rm). If each Eo,m
2
∈Eo,m
2
is locally Lipschitz, then Eo,m
2
◦Ep,o
1
has the m, n, p
MEP w.r.t. Fo,m ◦Fn,o."
MANIFOLD EMBEDDING PROPERTY,0.18181818181818182,The proof of Lemma 3 is in Appendix C.2.5.
MANIFOLD EMBEDDING PROPERTY,0.18383838383838383,"We note that when the elements of Eo,m
2
are differentiable, local Lipschitzness is automatic, and
need not be assumed, see e.g. (Tao, 2009, Ex. 10.2.6). We also record a near-converse (proved in
C.2.6) of Lemma 3 that shows that if Eo,m
2
◦Ep,o
1
has the m, n, p MEP, then Eo,m
2
has the m, n, o
MEP.
Lemma 4. Let F ⊂emb(Rn, Rm). Let Ep,o
1
⊂emb(Rp, Ro) and Eo,m
2
⊂emb(Ro, Rm) be such
that Eo,m
2
◦Ep,o
1
has the m, n, p MEP with respect to family F. Then Eo,m
2
has the m, n, o MEP with
respect to family F."
UNIVERSALITY,0.18585858585858586,"3.3
UNIVERSALITY"
UNIVERSALITY,0.18787878787878787,"We now present our universal approximation result for networks given in Eqn. Eq. 1 and a decou-
pling property.
Theorem 1 (Qualitative Universality for Embeddings). Let n0 = n, nL = m, µ ∈P(K) be
an absolutely continuous measure w.r.t. Lebesgue measure. Further let, for each ℓ= 1, . . . , L,
Enℓ−1,nℓ
ℓ
:= T nℓ
ℓ
◦Rnℓ−1,nℓ
ℓ
where Rnℓ−1,nℓ
ℓ
is a family of injective expansive elements that contains
a linear map, and T nℓ
ℓ
is a family of bijective family networks. Finally let T n
0 be distributionally
universal, i.e. for any absolutely continuous µ ∈P(Rn) and ν ∈P(Rn), there is a {Ti}∞
i=1 such
that Ti#µ →ν in distribution. Suppose that one of the following two cases hold:"
UNIVERSALITY,0.1898989898989899,"(i) Let F ∈FnL−1,m
L
◦· · ·◦Fn,n1
1
and Enℓ−1,nℓ
ℓ
have the the nℓ, n, nℓ−1 MEP for ℓ= 1, . . . , L with
respect to Fnℓ−1,nℓ
ℓ
."
UNIVERSALITY,0.1919191919191919,"(ii) Let F ∈emb1(Rn, Rm) be a C1-smooth embedding, and assume that for ℓ= 1, . . . , L it holds
that nℓ≥3nℓ−1 + 1 and the families T nℓ
ℓ
are dense in the space of C2-diffeomorphism Diff2(Rnℓ)."
UNIVERSALITY,0.19393939393939394,"Then, there is a sequence of {Ei}i=1,...,∞⊂EnL−1,m
L
◦· · · ◦En1,n
1
◦T n
0 such that"
UNIVERSALITY,0.19595959595959597,"lim
i→∞W2 (F #µ, Ei#µ) = 0.
(10)"
UNIVERSALITY,0.19797979797979798,"The proof of Theorem 1 is in Appendix C.3.1. As discussed in above and in Figure 2, there are
topological obstructions for Theorem 1 with a general embedding F : Rn →Rm. When n = 2,"
UNIVERSALITY,0.2,Under review as a conference paper at ICLR 2022
UNIVERSALITY,0.20202020202020202,"m = 3, L = 1, and µ is the uniform measure on an annulus K ⊂R2 target measure F#µ is
the uniform measure on a knotted ribbon M = F(K) ⊂R3. There are no injective linear maps
R : R2 →R3 and diffeomorphisms T : R3 →R3 such that E = T ◦R would satisfy M = E(K)
and E#µ = F#µ. This happen even though the set of diffeomorphism T : R3 →R3 are universal
distributional approximators. In this case the condition m = n1 ≥3n + 1 is not valid."
UNIVERSALITY,0.20404040404040405,"We remark here that our networks are designed expressly to approximate manifolds, and hence
injectivity is key. This separates our results from, e.g. (Lee et al., 2017, Theorem 3.1) or (Lu & Lu,
2020, Theorem 2.1), where universality results of ReLU networks are also obtained."
UNIVERSALITY,0.20606060606060606,"The previous theorem shows that the entire network is universalif it can be broken into pieces that
have the MEP. The following lemma, proved in Appendix C.3.2, shows that if En,m = Ho,m ◦Gn,o,
then Ho,m must have the m, n, o MEP if En,m is universal."
UNIVERSALITY,0.2080808080808081,"Lemma 5. Suppose that En,m
=
Ho,m ◦Gn,o where En,m
⊂
emb(Rn, Rm), Ho,m
⊂
emb(Ro, Rm), and Gn,o ⊂emb(Rn, Ro).
If Ho,m does not have the m, n, o MEP w.r.t.
F,
then there exists a f ∈F, compact K ⊂Rn and ϵ > 0 such that for all E ∈En,m, and
r ∈emb(f(K), E(W))"
UNIVERSALITY,0.2101010101010101,"∥I −r∥L∞(K) ≥ϵ.
(11)"
UNIVERSALITY,0.21212121212121213,"The proof of Theorem 1 also implies that, loosely speaking, later layers decouple from earlier ones.
That is, given a sequence of functions that has the MEP on the last L −˜ℓlayers, there are always
functions in the ﬁrst ˜ℓlayers so that the entire network is end-to-end optimal."
UNIVERSALITY,0.21414141414141413,"Corollary 1. Let Fn,o ⊂emb(Rn, Ro), Fo,m ⊂emb(Ro, Rm), and let Eo,m ⊂emb(Ro, Rm)
have the m, n, o MEP w.r.t. Fo,m ◦Fn,o. For every F ∈Fo,m ◦Fn,o then there is a compact
K ⊂Rn and {Ei}∞
i=1 ⊂Eo,m such that"
UNIVERSALITY,0.21616161616161617,"lim
i→∞BK,W (F, Ei) = 0.
(12)"
UNIVERSALITY,0.21818181818181817,"Further, if En,o ⊂emb(W ′, Ro) has the o, n, n MEP w.r.t. Fn,o, and T n is a universal approx-
imator for distributions, then for any µ ∈P(K) where K ⊂Rn is compact, there is a sequence
{E′
i}i=1,...,∞⊂En,o and {Ti}i=1,...,∞⊂T n so that"
UNIVERSALITY,0.2202020202020202,"lim
i→∞W2 (F #µ, Ei ◦E′
i ◦Ti#µ) = 0.
(13)"
UNIVERSALITY,0.2222222222222222,"The proof of Corollary 1 is in Appendix C.3.3. Approximation results for neural networks are typi-
cally given in terms of the network end-to-end. Corollary 1 shows that the layers of approximating
networks can in fact be built one at a time. It is related to an observation made in (Brehmer & Cran-
mer, 2020, Section B) about training strategies, where the authors remark that they ‘expect faster and
more robust training of a network’ of the form in Eqn. 1 when L = 1, that is F = T m
1 ◦Rn,m
1
◦T n
0 .
Corollary 1 shows that there exists a minimizing sequence in T m
1
that need only minimize Eqn. 12;
the T n
0 layers can be minimized after. We can further combine Lemma 5 and Cor. 1 to prove that not
only can the network from Brehmer & Cranmer (2020) be trained layerwise, but that any universal
network can necessarily be trained layerwise in reverse order, provided that it can be written as a
composition of two smaller layers."
UNIVERSALITY,0.22424242424242424,"3.4
LAYER-WISE INVERSION, UNCERTAINTY QUANTIFICATION AND RECOVERY OF WEIGHTS"
UNIVERSALITY,0.22626262626262628,"In this subsection, we describe how our network can be augmented with more useful properties if the
architecture satisﬁes a few more assumptions without affecting universal approximation. We focus
on a new layerwise projection result, with a further discussion of Bayesian uncertainty quantiﬁca-
tion, and black-box recovery of our network’s weights in Appendices D.2 & D.3."
UNIVERSALITY,0.22828282828282828,"Given a point y ∈Rm that does not lie in the range of the network, projecting y onto the range of the
network is a practical problem without an obvious answer. The crux of the problem is inverting the
injective (but non-invertible) R layers when R. When R contains only full-rank matrices as in (R1)
or (R2) then we can compute a least-squares solution. If, however, R contains layers which are only
piecewise linear, as in (R3), then the problem of computing a least squares solution is more difﬁcult,
see Fig. 3. Nevertheless, we ﬁnd that if R is (R3) we can still compute a least-squares solution."
UNIVERSALITY,0.23030303030303031,Under review as a conference paper at ICLR 2022
UNIVERSALITY,0.23232323232323232,"Figure 3: A schematic showing that, for a toy problem, the least-squares
projection to a piecewise afﬁne range can be discontinuous. Left: A
partitioning of R2 into classes with gray boundaries. Two points y, y′
are in the same class if they are both closest to the same afﬁne piece of
R(R), the range of R. The three points y1, y2 and y3 are each projected
to the closest three points on R(R) yielding ˜y1, ˜y2 and ˜y3. Note that the
projection operation is continuous within each section, but discontinuous
across gray boundaries between section."
UNIVERSALITY,0.23434343434343435,"Assumption 1. Let R be given by one of (R1) or (R2), or else (R3) when m = 2n."
UNIVERSALITY,0.23636363636363636,"If R only contains linear operators, then the least-squares problem can be computed by solving the
normal equations (see (Golub, 1996, Section 5.3).) This includes cases (R1) or (R2). For (R3) we
have the following result for D = In×n and M is an empty matrix."
UNIVERSALITY,0.2383838383838384,"Deﬁnition 7. Let W =
Bt
−DBtt ∈R2n×n and y ∈R2n be given, and let R(x) =
ReLU(Wx). Then deﬁne"
UNIVERSALITY,0.2404040404040404,"c(y) ∈R2n
c(y) := max

In×n
−In×n
−In×n
In×n"
UNIVERSALITY,0.24242424242424243,"
y, 0

(14)"
UNIVERSALITY,0.24444444444444444,"∆y ∈Rn×n
[∆y]i,i :=
0
if [c(y)]i+n = 0
1
if [c(y)]i+n > 0
[∆y]ij = 0 if i ̸= j
(15)"
UNIVERSALITY,0.24646464646464647,"My ∈Rn×2n
My :=
(In×n −∆y)
∆y

(16)"
UNIVERSALITY,0.24848484848484848,where the max in Eqn. 14 is taken element-wise.
UNIVERSALITY,0.2505050505050505,"Theorem 2. Let y ∈R2n. If for i = 1, . . . , n, [y]i ̸= [y]i+n then"
UNIVERSALITY,0.25252525252525254,"R†(y) := (MyW)−1 Myy = argmin
x∈Rn ∥y −R(x)∥2 .
(17)"
UNIVERSALITY,0.2545454545454545,"Further, if there is a i ∈{1, . . . , n} such that [y]i = [y]i+n, then there are multiple minimizers of
∥y −R(x)∥2, one of which is R†(y)."
UNIVERSALITY,0.25656565656565655,"The proof of Theorem 2 is given in Appendix D.1.
Remark 2. We note that Theorem 2 is different from many of the existing work on inverting expan-
sive layers, e.g. Aberdam et al. (2020); Bora et al. (2017); Lei et al. (2019), our result gives a direct
inversion algorithm that is provably the least-squares minimizer. Further, if each expansive layer is
any combination of (R1), (R2), or (R3) then the entire network can be inverted end-to-end by using
either the above result or solving the normal equations directly."
CONCLUSION,0.2585858585858586,"4
CONCLUSION"
CONCLUSION,0.2606060606060606,"Bijective ﬂow networks are a powerful tool for learning a push-forward mappings in a space of ﬁxed
dimension. Increasingly, these ﬂow networks have been used in combination with networks that
increase dimension in order to produce networks which are purportedly universal."
CONCLUSION,0.26262626262626265,"In this work, we have studied the theory underpinning these ﬂow and expansive networks by in-
troducing two new notions, the embedding gap and the manifold embedding property. We show
that these notions are both necessary and sufﬁcient for proving universality, but require important
topological and geometrical considerations which are, heretofore, under-explored in the literature.
We also ﬁnd that optimality of the studied networks can be established ‘in reverse,’ by minimizing
the embedding gap, which we expect opens the door to convergence of layer-wise training schemes.
Without compromising universality, we can also use speciﬁc expansive layers with a new layer-
wise projection result. Moreover, we show that the studied networks provide Bayesian uncertainty
quantiﬁcation and allow black-box recovery of their weights."
CONCLUSION,0.26464646464646463,Under review as a conference paper at ICLR 2022
REFERENCES,0.26666666666666666,REFERENCES
REFERENCES,0.2686868686868687,"Aviad Aberdam, Dror Simon, and Michael Elad. When and how can deep generative models be
inverted? arXiv preprint arXiv:2006.15555, 2020."
REFERENCES,0.27070707070707073,"Luigi Ambrosio, Nicola Gigli, and Giuseppe Savar´e. Gradient ﬂows: in metric spaces and in the"
REFERENCES,0.2727272727272727,"space of probability measures. Springer Science & Business Media, 2008."
REFERENCES,0.27474747474747474,"Lynton Ardizzone, Jakob Kruse, Sebastian Wirkert, Daniel Rahner, Eric W Pellegrini, Ralf S
Klessen, Lena Maier-Hein, Carsten Rother, and Ullrich K¨othe. Analyzing inverse problems with
invertible neural networks. arXiv preprint arXiv:1808.04730, 2018."
REFERENCES,0.2767676767676768,"Martin Arjovsky, Soumith Chintala, and L´eon Bottou.
Wasserstein gan.
arXiv preprint
arXiv:1701.07875, 2017."
REFERENCES,0.2787878787878788,"Simon Arridge, Peter Maass, Ozan ¨Oktem, and Carola-Bibiane Sch¨onlieb. Solving inverse problems
using data-driven models. Acta Numerica, 28:1–174, 2019."
REFERENCES,0.2808080808080808,"Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. Compressed sensing using genera-
tive models. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pp. 537–546. JMLR. org, 2017."
REFERENCES,0.2828282828282828,"Johann Brehmer and Kyle Cranmer. Flows for simultaneous manifold learning and density estima-
tion. arXiv preprint arXiv:2003.13913, 2020."
REFERENCES,0.28484848484848485,"Phuong Bui Thi Mai and Christoph Lampert. Functional vs. parametric equivalence of relu net-
works. In 8th International Conference on Learning Representations, 2020."
REFERENCES,0.2868686868686869,"Edmond Cunningham, Renos Zabounidis, Abhinav Agrawal, Ina Fiterau, and Daniel Sheldon. Nor-
malizing ﬂows across dimensions. arXiv preprint arXiv:2006.13070, 2020."
REFERENCES,0.28888888888888886,"Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components esti-
mation. arXiv preprint arXiv:1410.8516, 2014."
REFERENCES,0.2909090909090909,"Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv"
REFERENCES,0.29292929292929293,"preprint arXiv:1605.08803, 2016."
REFERENCES,0.29494949494949496,"Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. Cubic-spline ﬂows. arXiv"
REFERENCES,0.296969696969697,"preprint arXiv:1906.02145, 2019a."
REFERENCES,0.298989898989899,"Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios.
Neural spline ﬂows.
Advances in Neural Information Processing Systems, 32:7511–7522, 2019b."
REFERENCES,0.301010101010101,"Gene H Golub. Matrix computations. Johns Hopkins University Press, 1996."
REFERENCES,0.30303030303030304,"Aidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger B Grosse. The reversible residual net-
work: Backpropagation without storing activations. In Advances in neural information processing
systems, pp. 2214–2224, 2017."
REFERENCES,0.30505050505050507,"Will Grathwohl, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. Ffjord:
Free-form continuous dynamics for scalable reversible generative models.
arXiv preprint
arXiv:1810.01367, 2018."
REFERENCES,0.30707070707070705,"Morris W Hirsch. Differential topology, volume 33. Springer Science & Business Media, 2012."
REFERENCES,0.3090909090909091,"Chin-Wei Huang, David Krueger, Alexandre Lacoste, and Aaron Courville. Neural autoregressive
ﬂows. In International Conference on Machine Learning, pp. 2078–2087. PMLR, 2018."
REFERENCES,0.3111111111111111,"Michael F Hutchinson. A stochastic estimator of the trace of the inﬂuence matrix for laplacian
smoothing splines. Communications in Statistics-Simulation and Computation, 18(3):1059–1076,
1989."
REFERENCES,0.31313131313131315,"J¨orn-Henrik Jacobsen, Arnold Smeulders, and Edouard Oyallon. i-revnet: Deep invertible networks."
REFERENCES,0.3151515151515151,"arXiv preprint arXiv:1802.07088, 2018."
REFERENCES,0.31717171717171716,Under review as a conference paper at ICLR 2022
REFERENCES,0.3191919191919192,"Priyank Jaini, Kira A Selby, and Yaoliang Yu. Sum-of-squares polynomial ﬂow. In International"
REFERENCES,0.3212121212121212,"Conference on Machine Learning, pp. 3009–3018. PMLR, 2019."
REFERENCES,0.32323232323232326,"Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max
Welling.
Improving variational inference with inverse autoregressive ﬂow.
arXiv preprint
arXiv:1606.04934, 2016."
REFERENCES,0.32525252525252524,Durk P Kingma and Prafulla Dhariwal. Glow: Generative ﬂow with invertible 1x1 convolutions. In
REFERENCES,0.32727272727272727,"Advances in Neural Information Processing Systems, pp. 10215–10224, 2018."
REFERENCES,0.3292929292929293,"Ivan Kobyzev, Simon Prince, and Marcus Brubaker. Normalizing ﬂows: An introduction and review
of current methods. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020."
REFERENCES,0.33131313131313134,"Konik Kothari, AmirEhsan Khorashadizadeh, Maarten de Hoop, and Ivan Dokmani´c. Trumpets:
Injective ﬂows for inference and inverse problems. arXiv preprint arXiv:2102.10461, 2021."
REFERENCES,0.3333333333333333,"Jakob Kruse, Gianluca Detommaso, Robert Scheichl, and Ullrich K¨othe.
Hint:
Hierarchi-
cal invertible neural transport for density estimation and bayesian inference.
arXiv preprint
arXiv:1905.10687, 2019."
REFERENCES,0.33535353535353535,"Jakob Kruse, Lynton Ardizzone, Carsten Rother, and Ullrich K¨othe. Benchmarking invertible archi-
tectures on inverse problems. arXiv preprint arXiv:2101.10763, 2021."
REFERENCES,0.3373737373737374,"Holden Lee, Rong Ge, Tengyu Ma, Andrej Risteski, and Sanjeev Arora. On the ability of neural
nets to express distributions. In Conference on Learning Theory, pp. 1271–1296. PMLR, 2017."
REFERENCES,0.3393939393939394,"Qi Lei, Ajil Jalal, Inderjit S Dhillon, and Alexandros G Dimakis. Inverting deep generative models,
one layer at a time. In Advances in Neural Information Processing Systems, pp. 13910–13919,
2019."
REFERENCES,0.3414141414141414,"Yulong Lu and Jianfeng Lu. A universal approximation theorem of deep neural networks for ex-
pressing distributions. arXiv preprint arXiv:2004.08867, 2020."
REFERENCES,0.3434343434343434,"Ib H Madsen, Jxrgen Tornehave, et al. From calculus to cohomology: de Rham cohomology and"
REFERENCES,0.34545454545454546,"characteristic classes. Cambridge university press, 1997."
REFERENCES,0.3474747474747475,"John Milnor. On manifolds homeomorphic to the 7-sphere. Annals of Mathematics, pp. 399–405,
1956."
REFERENCES,0.34949494949494947,"Amiya
Mukherjee.
Differential
topology.
Hindustan
Book
Agency,
New
Delhi;
Birkh¨auser/Springer, Cham, second edition, 2015.
ISBN 978-3-319-19044-0; 978-3-319-
19045-7.
doi:
10.1007/978-3-319-19045-7.
URL https://doi.org/10.1007/
978-3-319-19045-7."
REFERENCES,0.3515151515151515,Stefan M¨uller. Uniform approximation of homeomorphisms by diffeomorphisms. Topology and its
REFERENCES,0.35353535353535354,"Applications, 178:315–319, 2014."
REFERENCES,0.35555555555555557,"Kunio Murasugi.
Knot theory & its applications.
Modern Birkh¨auser Classics. Birkh¨auser
Boston, Inc., Boston, MA, 2008. ISBN 978-0-8176-4718-6. doi: 10.1007/978-0-8176-4719-3.
URL https://doi.org/10.1007/978-0-8176-4719-3. Translated from the 1993
Japanese original by Bohdan Kurpita, Reprint of the 1996 translation [MR1391727]."
REFERENCES,0.3575757575757576,"George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lak-
shminarayanan.
Normalizing ﬂows for probabilistic modeling and inference.
arXiv preprint
arXiv:1912.02762, 2019."
REFERENCES,0.3595959595959596,"Michael Puthawala, Konik Kothari, Matti Lassas, Ivan Dokmani´c, and Maarten de Hoop. Globally
injective relu networks. arXiv preprint arXiv:2006.08464, 2020."
REFERENCES,0.3616161616161616,David Rolnick and Konrad K¨ording. Reverse-engineering deep relu networks. In International
REFERENCES,0.36363636363636365,"Conference on Machine Learning, pp. 8178–8187. PMLR, 2020."
REFERENCES,0.3656565656565657,"Carlo H. S´equin. Tori story. In Reza Sarhangi and Carlo H. S´equin (eds.), Proceedings of Bridges"
REFERENCES,0.36767676767676766,"2011: Mathematics, Music, Art, Architecture, Culture, pp. 121–130. Tessellations Publishing,
2011. ISBN 978-0-9846042-6-5."
REFERENCES,0.3696969696969697,Under review as a conference paper at ICLR 2022
REFERENCES,0.3717171717171717,"He Sun and Katherine L Bouman. Deep probabilistic imaging: Uncertainty quantiﬁcation and multi-
modal solution characterization for computational imaging. arXiv preprint arXiv:2010.14462,
2020."
REFERENCES,0.37373737373737376,"Wilson A. Sutherland. Introduction to metric and topological spaces. Oxford University Press,
Oxford, 2009. ISBN 978-0-19-956308-1. Second edition [of MR0442869], Companion web site:
www.oup.com/uk/companion/metric."
REFERENCES,0.37575757575757573,"Terence Tao. Analysis, volume 185. Springer, 2009."
REFERENCES,0.37777777777777777,"Takeshi Teshima, Isao Ishikawa, Koichi Tojo, Kenta Oono, Masahiro Ikeda, and Masashi Sugiyama.
Coupling-based invertible neural networks are universal diffeomorphism approximators. arXiv
preprint arXiv:2006.11469, 2020."
REFERENCES,0.3797979797979798,"C´edric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media,
2008."
REFERENCES,0.38181818181818183,"Dmitry Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks, 94:
103–114, 2017."
REFERENCES,0.3838383838383838,Dmitry Yarotsky. Optimal approximation of continuous functions by very deep relu networks. In
REFERENCES,0.38585858585858585,"Conference on Learning Theory, pp. 639–649. PMLR, 2018."
REFERENCES,0.3878787878787879,"APPENDIX A
SUMMARY OF NOTATION"
REFERENCES,0.3898989898989899,Throughout the paper we make heavy use of the following notation.
REFERENCES,0.39191919191919194,"1. Unless otherwise stated, X and Y always refer to subsets of Euclidean space, and K and
W always refer to compact subsets of Euclidian space."
REFERENCES,0.3939393939393939,"2. f ∈C(X, Y ) means that f : X →Y is continuous."
REFERENCES,0.39595959595959596,"3. For families of functions F and G where each F ∋f : X →Y and G ∋g: Y →Z, then
we deﬁne G ◦F = {g ◦f : X →Z : f ∈F, g ∈G}."
REFERENCES,0.397979797979798,"4. f ∈emb(X, Y ) means that f ∈C(X, Y ) is continuous and injective on the range of f,
i.e. an embedding, and furthermore that f −1 : f(X) →X is continuous."
REFERENCES,0.4,5. µ ∈P(X) means that µ is a probability measure over X.
REFERENCES,0.402020202020202,"6. W2 (µ, ν) for µ, ν ∈P(X) refers to the Wasserstein-2 distance, always with ℓ2 ground
metric."
REFERENCES,0.40404040404040403,"7. ∥·∥Lp(X) refers to the Lp norm of functions, from X to R."
REFERENCES,0.40606060606060607,"8. For vector-valued f : X →Y , ∥f∥L∞(X) = ess supx∈X ∥f∥2. Note that Y is always ﬁnite
dimensional, and so all discrete 1 ≤q ≤∞norms are equivalent."
REFERENCES,0.4080808080808081,9. Lip(g) refers to the Lipschitz constant of f.
REFERENCES,0.4101010101010101,"10. For x ∈Rn, [x]i ∈R is the i’th component of x. Similarly, for matrix A ∈Rm×n, [A]ij
refers to the j’th element in the i’th column."
REFERENCES,0.4121212121212121,"APPENDIX B
DETAILED COMPARISON TO PRIOR WORK"
REFERENCES,0.41414141414141414,"B.1
CONNECTION TO BREHMER & CRANMER (2020)"
REFERENCES,0.4161616161616162,"In Brehmer & Cranmer (2020), the authors introduce manifold-learning ﬂows as an invertible
method for learning probability density supported on a low-dimensional manifold. Their model
can be written as"
REFERENCES,0.41818181818181815,"F = T m
1
◦Rn,m ◦T n
0
(18)"
REFERENCES,0.4202020202020202,Under review as a conference paper at ICLR 2022
REFERENCES,0.4222222222222222,"where T m
1
⊂C(Rm, Rm), T m
0
⊂C(Rn, Rn), and R =

In×n"
REFERENCES,0.42424242424242425,0(m−n)×n
REFERENCES,0.4262626262626263,"
is a zero-padding (R1)."
REFERENCES,0.42828282828282827,"They invert F ∈F in two different ways. For manifold-learning ﬂows (M-ﬂows) they restrict T m
1
to be an invertible ﬂow, and for manifold-learning ﬂows with separate encoder (Me-ﬂows) they
place no such restrictions on T m
1
and instead train a separate neural network e to invert elements of
T m
1 ."
REFERENCES,0.4303030303030303,"Our results apply out-of-the-box to the architectures used in Experiment A of Brehmer & Cranmer
(2020). The architecture described in Eqn. 18 is of the form of Eqn. 1 where L = 1. Further,
although they are not studied here, our analysis can also be applied to quadratic ﬂows."
REFERENCES,0.43232323232323233,"The network used in (Brehmer & Cranmer, 2020, Experiment 4.A) uses coupling networks, (T1),
where T m
1
and T n
0 are both 5 layers deep. For (Brehmer & Cranmer, 2020, Experiments 4.B and
4.C) the authors choose expressive elements T that are rational quadratic ﬂows Durkan et al. (2019b)
for both T m
1
and T n
0 . In Experiment 4.B they let T1 and T0 again be 5 layers deep, and in 4.C
they again let T1 by 20 layers deep and T0 15 layers. For the ﬁnal experiment, 4.D, the choose
more complicated expressive elements that combine Glow Kingma & Dhariwal (2018) and Real
NVP Dinh et al. (2016) architectures. These elements include the actnorm, 1 × 1 convolutions and
rational-quadratic coupling transformations along with a multi-scale transformation."
REFERENCES,0.43434343434343436,"The authors mention universality of their network without our proof, but our universality results in
Theorem 1 apply to their networks from Experiment A wholesale. Further in their work the authors
describe how training can be split into a manifold phase and density phase, wherein the manifold
phase T m
1
is trained to learn the manifold, and in the density phase T m
1
if ﬁxed and T n
0 is trained to
learn the density thereupon. This statement is made formal and proven by our Cor. 1."
REFERENCES,0.43636363636363634,"B.2
CONNECTION TO KOTHARI ET AL. (2021)"
REFERENCES,0.4383838383838384,"In Kothari et al. (2021), the authors introduce the ‘Trumpet’ architecture, for it’s architecture,
which has many alternating ﬂow networks & expansive layers with many ﬂow-networks in the low-
dimensional early stages of the network, which gives the architecture a shape similar to the titular
instrument."
REFERENCES,0.4404040404040404,"The architecture studied in Kothari et al. (2021) is precisely of the form of Eqn. 1, where the bijective
ﬂow networks are revnets Gomez et al. (2017); Jacobsen et al. (2018) architecture, and the expansive
elements are 1 × 1 convolutions, as in (R2). To out knowledge, there are no results that show that
the revnets used are universal approximators, but if they revnets are substituted with either (T1) or
(T2), then the, we could apply Theorem 1 to the resulting architecture."
REFERENCES,0.44242424242424244,"The authors of Kothari et al. (2021) also remark that their entire network can be designed in order
to facilitate uncertainty quantiﬁcation, a remark that we too can apply to our networks, as discussed
further in Section D.2."
REFERENCES,0.4444444444444444,"APPENDIX C
PROOFS"
REFERENCES,0.44646464646464645,"C.1
MAIN RESULTS"
REFERENCES,0.4484848484848485,"C.1.1
HELPER LEMMA"
REFERENCES,0.4505050505050505,"Before presenting the proof of Lemma 7, we present the following three helper inequalities"
REFERENCES,0.45252525252525255,"Lemma 6. For all of the following results, f ∈emb(K, Rm) and g ∈emb(W, Rm) and n ≤o ≤
m. 1."
REFERENCES,0.45454545454545453,"BK,W (f, g) ≥sup
xn∈K
inf
xo∈W ∥g(xo) −f(xn)∥2 .
(19)"
REFERENCES,0.45656565656565656,"2. Let X, Y ⊂W, let g be Lipschitz on W, and r ∈emb(X, Y ). Then, there is a r′ ∈
emb(g(X), g(Y )) such that g ◦r = r′ ◦g and ∥I −r′∥L∞(g(X)) ≤∥I −r∥L∞(X) Lip(g)."
REFERENCES,0.4585858585858586,Under review as a conference paper at ICLR 2022 3.
REFERENCES,0.46060606060606063,"∥I −r∥L∞(K) =
I −r−1
L∞(r(K))
(20)"
REFERENCES,0.4626262626262626,"4. Let K ⊂Rn, X ⊂Rp and W ⊂Ro be compact sets. Also, let f ∈emb(K, W) and
h ∈emb(X, W), and let g ∈emb(W, Rm) be a Lipschitz map. Then"
REFERENCES,0.46464646464646464,"BK,X(g ◦f, g ◦h) ≤Lip(g)BK,X(f, h).
(21)"
REFERENCES,0.4666666666666667,"Proof.
1. Let r ∈C(f(K), g(W)), then"
REFERENCES,0.4686868686868687,"∥I −r∥L∞(f(K)) = sup
xn∈K
∥(I −r)f(xn)∥2 = sup
xn∈K
∥f(xn) −r ◦f(xn)∥2"
REFERENCES,0.4707070707070707,"= sup
xn∈K
∥f(xn) −g(xo)∥2 where xo = g−1 ◦r ◦f(xn)"
REFERENCES,0.4727272727272727,"≥sup
xn∈K
inf
xo∈W ∥f(xn) −g(xo)∥2 ."
REFERENCES,0.47474747474747475,"2. g is injective on X, hence we can deﬁne r′ such that r′ = g ◦r ◦g−1 : g(X) →g(r(X)) ⊂
g(Y ) such that r′ ∈emb(g(X), g(Y )), and thus ∀x ∈X,"
REFERENCES,0.4767676767676768,"∥(I −r′) ◦g(x)∥2 = ∥g(x) −g ◦r(x)∥2 ≤Lip(g) ∥I −r∥L∞(X)
(22)"
REFERENCES,0.47878787878787876,where we have used ∥r(x) −x∥2 ≤∥I −r∥L∞(X).
REFERENCES,0.4808080808080808,"3. For every x ∈r(K), we have a y ∈K such that x = r(y), thus ∀x ∈r(K),
 
I −r−1
(x)

2 = ∥(r −I) (y)∥2 .
(23)"
REFERENCES,0.48282828282828283,"But r is clearly surjective onto it’s range, hence taking the supremum over all x ∈X yields
I −r−1
L∞(r(K)) = ∥I −r∥L∞(K)
(24)"
REFERENCES,0.48484848484848486,"4. As g ∈emb(W, Rm), the map g : W →g(W) is a homeomorphism and there is g−1 ∈
emb(g(W), W). For a map r ∈emb(g ◦f(K), g ◦h(X)), we see that ˆr = g−1 ◦r ◦
g ∈emb(f(K), h(X)). Also, the opposite is valid as if ˆr ∈emb(f(K), h(X)) then
r = g ◦ˆr ◦g−1 ∈emb(g ◦f(K), g ◦h(X)). Thus"
REFERENCES,0.4868686868686869,"BK,X(g ◦f, g ◦h) =
inf
r∈emb(g◦f(K),g◦h(X)) ∥I −r∥L∞(g◦f(K))"
REFERENCES,0.4888888888888889,"=
inf
r=g◦ˆr◦g−1∈emb(g◦f(K),g◦h(X)) ∥I −g ◦ˆr ◦g−1∥L∞(g◦f(K))"
REFERENCES,0.4909090909090909,"=
inf
ˆr∈emb(f(K),h(X)) ∥g ◦(I −ˆr) ◦g−1∥L∞(g◦f(K))"
REFERENCES,0.49292929292929294,"≤Lip(g)
inf
ˆr∈emb(f(K),h(X)) ∥(I −ˆr) ◦g−1∥L∞(g◦f(K))"
REFERENCES,0.494949494949495,"≤Lip(g)
inf
ˆr∈emb(f(K),h(X)) ∥I −ˆr∥L∞(f(K))"
REFERENCES,0.49696969696969695,"≤Lip(g) BK,X(f, h)"
REFERENCES,0.498989898989899,"C.1.2
LEMMA 7, USEFUL EMBEDDING GAP INEQUALITIES"
REFERENCES,0.501010101010101,"Lemma 7. Let f ∈emb(K, Rm) and g ∈emb(W, Rm) and n ≤o ≤m. Then the following hold:"
REFERENCES,0.503030303030303,"1. BK,W (f, g) ≤supx∈K ∥g ◦h(x) −f(x)∥2 where h ∈emb(K, Ro) is a map satisfying
h(K) ⊂W."
REFERENCES,0.5050505050505051,"2. For any X that is the closure of an open set , if h ∈emb(X, W) then"
REFERENCES,0.5070707070707071,"BK,W (f, g) ≤BK,X(f, g ◦h)
(25)"
REFERENCES,0.509090909090909,Under review as a conference paper at ICLR 2022
REFERENCES,0.5111111111111111,"3. For any r ∈emb(f(K), Rm),"
REFERENCES,0.5131313131313131,"BK,W (f, g) ≤∥I −r∥L∞(f(K)) + BK,W (r ◦f, g).
(26)"
REFERENCES,0.5151515151515151,"4. For any r ∈emb(f(K), g(W)) and h ∈emb(X, W) where X ⊂Rp is the closure of a
compact set where n ≤p ≤o we have that"
REFERENCES,0.5171717171717172,"BK,X(f, g ◦h) ≤∥I −r∥L∞(f(K)) + Lip(g)BK,X(g−1 ◦r ◦f, h)
(27)"
REFERENCES,0.5191919191919192,where Lip(g) denotes the Lipschitz constant of g.
REFERENCES,0.5212121212121212,"5. For any µn ∈P(K), there is a µo ∈P(W) such that"
REFERENCES,0.5232323232323233,"W2 (f #µn, g#µo) ≤BK,W (f, g).
(28)"
REFERENCES,0.5252525252525253,"The proof of Lemma 7.
1. If we let r := g ◦h ◦f −1, then r ∈emb(f(K), g(W)), and"
REFERENCES,0.5272727272727272,"BK,W (f, g) ≤∥∥(I −r) ◦f(x)∥2∥L∞(K)
(29)"
REFERENCES,0.5292929292929293,"=
f(x) −g ◦h ◦f −1 ◦f(x)

2

L∞(K) ≤sup
x∈K
∥f(x) −g ◦h(x)∥2 . (30)"
REFERENCES,0.5313131313131313,"2. Given that g ◦h(X) ⊂g(W), we have that emb(f(K), g ◦h(X)) ⊂emb(f(K), g(W)),
thus the inﬁmum in Eqn. 6 is taken over a smaller set, thus BK,W (f, g) ≤BK,X(f, g ◦h)."
REFERENCES,0.5333333333333333,"3. Note that for any r′ ∈emb(r ◦f(K), g(W)), r′ ◦r ∈emb(f(K), g(W)), and so we have"
REFERENCES,0.5353535353535354,"BK,W (f, g) ≤∥I −r′ ◦r∥L∞(f(K)) ≤∥I −r∥L∞(f(K)) + ∥r −r′ ◦r∥L∞(f(K)) (31)"
REFERENCES,0.5373737373737374,"= ∥I −r∥L∞(f(K)) + ∥I −r′∥L∞(r◦f(K))
(32)"
REFERENCES,0.5393939393939394,"where we have used that r is injective for the ﬁnal equality. This holds for all possible r′,
hence we have the result."
RECALL THAT F,0.5414141414141415,"4. Recall that f
∈
emb(K, W), g
∈
emb(W, Rm), h
∈
emb(X, W) and r
∈
emb(f(K), g(W)). Then g−1 ∈emb(g(W), W). As r ◦f(K) ⊂g(W), we see that"
RECALL THAT F,0.5434343434343434,r ◦f = g ◦g−1 ◦r ◦f.
RECALL THAT F,0.5454545454545454,"This, Lemma 7, point 3 and Lemma 6, point 4 yield that"
RECALL THAT F,0.5474747474747474,"BK,X(f, g ◦h) ≤∥I −r∥L∞(f(K)) + BK,X(r ◦f, g ◦h)"
RECALL THAT F,0.5494949494949495,"≤∥I −r∥L∞(f(K)) + BK,X(g ◦g−1 ◦r ◦f, g ◦h)"
RECALL THAT F,0.5515151515151515,"≤∥I −r∥L∞(f(K)) + Lip(g) BK,X(g−1 ◦r ◦f, h),"
RECALL THAT F,0.5535353535353535,which proves the claim.
RECALL THAT F,0.5555555555555556,"5. Let r ∈emb(f(K), g(W)) be such that ∥I −r∥L∞(Range(f)) ≤BK,W (f, g) + ϵ for
some ϵ > 0, then for every x ∈K, there exists y ∈W such that g(y) = r ◦f(x). From
injectivity of g, we have that y = g−1◦r◦f(x). Note that g−1◦r◦f ∈emb(K, W), hence
K′ := g−1◦r◦f(K) is compact. Deﬁne µ′ := (g−1 ◦r ◦f)#µ. Clearly g#µ′ = r ◦f #µ,
and thus"
RECALL THAT F,0.5575757575757576,"W2 (g#µ′, f #µ) ≤W2 (g#µ′, r ◦f #µ) + W2 (r ◦f #µ, f #µ)
(33)"
RECALL THAT F,0.5595959595959596,"but W2 (g#µ′, r ◦f #µ) = 0 and"
RECALL THAT F,0.5616161616161616,"W2 (r ◦f #µ, f #µ) ≤
Z"
RECALL THAT F,0.5636363636363636,"K
∥I −r∥2
2 df #µ
1/2
≤BK,W (f, g) + ϵ.
(34)"
RECALL THAT F,0.5656565656565656,"This is true for every ϵ > 0, hence taking the inﬁmum over all r ∈emb(f(K), g(K))
establishes W2 (g#µ′, f #µ) ≤BK,W (f, g) + ϵ for every ϵ > 0, and thus we have that
W2 (g#µ′, f #µ) ≤BK,W (f, g)."
RECALL THAT F,0.5676767676767677,Under review as a conference paper at ICLR 2022
RECALL THAT F,0.5696969696969697,"C.2
MANIFOLD EMBEDDING PROPERTY"
RECALL THAT F,0.5717171717171717,"C.2.1
THE PROOF OF LEMMA 1"
RECALL THAT F,0.5737373737373738,"The proof of Lemma 1.
(i) Let us consider ϵ > 0, a compact set K
⊂Rn and f
∈
emb(Rn, Rm).
Let W = K × {0}o−n and F : Ro →Rm be the map given by
F(x, y) = f(x), (x, y) ∈Rn × Ro−n. Because Ro,m is a uniform universal approxi-
mator of C(Rn, Rm), there is an R ∈Ro,m such that ∥F −R∥L∞(W ) < ϵ. Then for the
map E = I ◦R we have that BK,W (f, E) < ϵ. This is true for every ϵ > 0, and so Eo,m
has the MEP property w.r.t. the family emb(Rn, Rm)."
RECALL THAT F,0.5757575757575758,"(ii) Recall that f := Φ0 ◦R0 for Φ0 ∈Diff1(Rm, Rm) and linear R0 : Rn →Rm, and that
R ∈R is such that R"
RECALL THAT F,0.5777777777777777,"U is linear for open U. We present the proof in the case when n = o,
and we make the assumption that R

K is linear. In this case, we have the existence of an
afﬁne map A: Rm →Rm so that R0 = A ◦R so that ˜K := R0(K) = A(R(K)). Let
ϵ > 0 be given. By (Hirsch, 2012, Chapter 2, Theorem 2.7), the space Diff2(Rm, Rm) is
dense in the space Diff1(Rm, Rm), and so there is some Φ1 ∈Diff2(Rm, Rm) such that"
RECALL THAT F,0.5797979797979798,"∥Φ1| ˜
K −Φ0| ˜
K∥L∞( ˜
K;Rm) < ϵ 2."
RECALL THAT F,0.5818181818181818,"Then, let T ∈T m be such that ∥T −Φ1 ◦A∥L∞(R(K);Rm) < ϵ"
THEN WE HAVE THAT,0.5838383838383838,2. Then we have that
THEN WE HAVE THAT,0.5858585858585859,"∥T ◦R −f∥L∞(K) = ∥T ◦R −Φ0 ◦R0∥L∞(K)
≤∥T ◦R −Φ1 ◦A ◦R∥L∞(K) + ∥Φ1 ◦A ◦R −Φ0 ◦R0∥L∞(K)
≤∥T −Φ1 ◦A∥L∞(R(K)) + ∥Φ1 ◦A ◦R −Φ0 ◦A ◦R∥L∞(K) < ϵ 2 + ϵ"
THEN WE HAVE THAT,0.5878787878787879,2 = ϵ.
THEN WE HAVE THAT,0.5898989898989899,"Hence, if we let r = T ◦R◦f −1 ∈emb(f(K), T ◦R(K)) then we obtain that BK,K(f, T ◦
R) < ϵ. This holds for any ϵ, and hence we have that T ◦R has the MEP for I(Rn, Rm)."
THEN WE HAVE THAT,0.591919191919192,"The proof in the case that o ≥n follows with minor modiﬁcation, and applying Lemma 7
point 1."
THEN WE HAVE THAT,0.593939393939394,"C.2.2
S1 CAN NOT BE MAPPED EXTENDABLY TO THE TREFOIL KNOT"
THEN WE HAVE THAT,0.5959595959595959,"We ﬁrst show that there are no maps E := T ◦R where R: R2 →R3 such that T is a homeo-
morphism and E(S1) is a trefoil knot. We use the fact that the trivial knot S1 and the trefoil knot
M = f(S1) are not equivalent, that is, there are no homeomorphisms in R3 that map S1 to M. In-
deed, by (Murasugi, 2008, Section 3.2), the trefoil knot M and its mirror image are not equivalent,
whereas the trivial knot S1 and its mirror image are equivalent. Hence, M and R(S1) are not equiv-
alent knots in R3. Thus by (Murasugi, 2008, Deﬁnition 1.3.1 and Theorem 1.3.1), we see that there
is no orientation preserving homeomorphism T : R3 →R3 such that T(R3 \ R(S1)) = R3 \ M.
As the orientation of the map T can be changed by composing T with the reﬂection J : R3 →R3
across the plane Range(R) that deﬁnes a homeomorphim J : R3 \ R(S1) →R3 \ R(S1), we see
that there is no homeomorphism T : R3 →R3 such that T(R3 \ R(S1)) = R3 \ M."
THEN WE HAVE THAT,0.597979797979798,"This example shows that the composition E = T ◦R of a linear map R and a coupling ﬂow T
cannot have the property that E(S1) = f(S1) for this embedding f. Moreover, the complement
R3 \ E(S1) is never homeomorphic to R3 \ f(S1) for any such map E."
THEN WE HAVE THAT,0.6,"We now construct another example, similar to Figure 2, where an annulus that is mapped to a knotted
ribbon in R3. To do this, replace the circle S1 by an annulus K = {x ∈R2 : 1/2 ≤|x| ≤3/2},
that in the polar coordinates is {(r, θ) : 1/2 ≤r ≤3/2} and deﬁne a map F : K →R3 by deﬁning
in the polar coordinates
F(r, θ) = f(θ) + a(r −1)v(θ)
where f : S1 →Σ1 ⊂R3 is an smooth embedding of S1 to a trefoil knot Σ1 and v(θ) ∈R3 is a
unit vector normal to Σ1 at the point f(θ) such that v(θ) is a smooth function of θ, and a > 0 is"
THEN WE HAVE THAT,0.602020202020202,Under review as a conference paper at ICLR 2022
THEN WE HAVE THAT,0.604040404040404,"a small number. In this case, M1 = F(K) is a 2-dimensional submanifold of R3 with boundary,
which can visualizes M1 as a knotted ribbon."
THEN WE HAVE THAT,0.6060606060606061,"We now show that there are no maps E = T ◦R such that E(K) = F(K) where T : R3 →R3 is an
embedding, and R: R2 →R3 injective and linear. The key insight is that if such a T existed, then
this implies that the trefoil knot is equivalent to S1 in R3, which is known to be false."
THEN WE HAVE THAT,0.6080808080808081,"Let Uρ(A) denote the ρ-neighborhood of the set A in R3. It is easy to see that R2 \ ({0} × [−1, 1])
is homeomorphic to R2 \ BR2(0, 1), which is further homeomorphic to R2 \ {0}. Thus, using
tubular coordinates near Σ1 and a sufﬁciently small ρ > 0, we see that R3 \ M1 is homeomorphic to
R3 \ Uρ(Σ1), which is further homeomorphic to R3 \ Σ1. Also, when R : R2 →R3 is an injective
linear map, we see that M2 = R(K) is a un-knotted band in R3 and R3 \ M2 is homeomorphic
to R3 \ Σ2. If R3 \ M1 and R3 \ M2 would be homeomorphic, then also R3 \ Σ1 and R3 \
Σ2 would be homeomorphic that is not possible by knot theory, see (Murasugi, 2008, Deﬁnition
1.3.1 and Theorem 1.3.1). This shows that there are no injective linear maps R : R2 →R3 and
homeomorphisms Φ : R3 →R3 such that (Φ ◦R)(K) = M1."
THEN WE HAVE THAT,0.6101010101010101,"Similar examples can be obtained in a higher dimensional case by using a knotted torus S´equin
(2011)4 and their Cartesian products."
THEN WE HAVE THAT,0.6121212121212121,"C.2.3
THE PROOF OF EXAMPLE 1"
THEN WE HAVE THAT,0.6141414141414141,"Proof.
(i) From (Puthawala et al., 2020, Theorem 15) we have that Ro,m can approximate
any continuous function f ∈emb(Rn, Rm). Further, clearly (T1) and (T2) both contain
the identity map, thus Lemma 1 (i) applies."
THEN WE HAVE THAT,0.6161616161616161,"(ii) Let T m be the family autoregressive ﬂows with sigmoidal activations deﬁned in (Huang
et al., 2018). By (Teshima et al., 2020, App. G, Theorem 1 and Proposition 7), T m are
sup-universal approximators in the space Diff2(Rm, Rm) of C2-smooth diffeomorphisms
Φ : Rm →Rm. When Ro,m is one of (R1) or (R2) the network is always linear, hence the
conditions are satisﬁed. If Ro,m is (R4), then Ro,m contains linear mappings, and if (R3),
then we can shift the origin, so that R(x) is linear on K. In all cases, Lemma 1 part (ii)
applies."
THEN WE HAVE THAT,0.6181818181818182,"C.2.4
THE PROOF OF LEMMA 2"
THEN WE HAVE THAT,0.6202020202020202,"Given an f ∈emb∞(K, Rm) we ﬁrst show that for m ≥2n+1 there are always a diffeomorphisms
Ψ: Rm →Rm so that Ψ ◦f : Rn →{0}n × Rm−n. The existence of such a Ψ borrows ideas from
Whitney’s embedding theorem (Hirsch, 2012, Theorems 3.4 & 3.5) and is constructed by iteratively
constructing an injective projection."
THEN WE HAVE THAT,0.6222222222222222,"Next if m −n ≥2n + 1, then we can apply (Madsen et al., 1997, Lemma 7.6), a consequence of the
Tietze extension theorem, to show that Ψ: M →{0}n×Rm−n can be extended to a diffeomorphism
on the entire space, h: Rm →Rm. Hence f(x) = Ψ−1 ◦h ◦R(x) for diffeomorphism Ψ−1 ◦
h: Rm →Rm and zero-padding operator R: Rn →Rm, and thus f ∈I∞(K, Rm)."
THEN WE HAVE THAT,0.6242424242424243,"This fact that for m sufﬁciently large compared to n such a diffeomorphism can always be extended
is related to the fact that in 4-dimensions, all knots can be opened. This can be contrasted with the
case in Figure 2."
THEN WE HAVE THAT,0.6262626262626263,Now we present our proof.
THEN WE HAVE THAT,0.6282828282828283,Proof. Let use next prove Eq. 9 when m ≥3n + 1.Let
THEN WE HAVE THAT,0.6303030303030303,"f ∈embk(Rn, Rm)
(35)"
THEN WE HAVE THAT,0.6323232323232323,be a Ck map and M = f(Rn) be an embedded submanifold of Rm.
"ON
THE
KNOTTED",0.6343434343434343,"4On
the
knotted
torus,
see
http://gallery.bridgesmathart.org/exhibitions/
2011-bridges-conference/sequin."
"ON
THE
KNOTTED",0.6363636363636364,Under review as a conference paper at ICLR 2022
"ON
THE
KNOTTED",0.6383838383838384,We have that m ≥3n + 1 > 2n + 1. Let Sm−1 be the unit sphere of Rm and let
"ON
THE
KNOTTED",0.6404040404040404,"SRm = {(x, v) ∈Rm × Rm : ∥v∥= 1}"
"ON
THE
KNOTTED",0.6424242424242425,"be the sphere bundle of Rm that is a manifold of dimension 2m −1. By the proof’s of Whitney’s
embedding theorem, by Hirsch, (Hirsch, 2012, Chapter 1, Theorems 3.4 and 3.5), there is a set
of ‘problem points’ H1 ⊂Sm−1 of Hausdorff dimension 2n such that for all w ∈Rm \ H1 the
orthogonal projection
Pw : Rm →{w}⊥= {y ∈Rm : y ⊥w}
has a restriction Pw|M on M deﬁnes an injective map"
"ON
THE
KNOTTED",0.6444444444444445,Pw|M : M →{w}⊥.
"ON
THE
KNOTTED",0.6464646464646465,"Moreover, let TxM be the tangent space of manifold M at the point x and let us deﬁne another set
of ‘problem points’ as
H2 = {v ∈Sm−1 : ∃x ∈M, v ∈TxM}.
For w ∈Sm−1 \ H2 the map
Pw|M : M →{w}⊥⊂Rm"
"ON
THE
KNOTTED",0.6484848484848484,"is an immersion, that is, it has an injective differential. The sphere tangent bundle SM of M has
dimension 2n −1, and the set H2 has the Hausdorff dimension at most 2n −1. Thus H = H1 ∪H2
has Hausdorff dimension at most 2n < m −1 and hence the set Sm−1 \ H is non-empty. For
w ∈Sm−1 \ H the map Pw|M : M →{w}⊥is a Ck injective immersion and thus"
"ON
THE
KNOTTED",0.6505050505050505,˜N = Pw(M) ⊂{w}⊥
"ON
THE
KNOTTED",0.6525252525252525,is a Ck submanifold.
"ON
THE
KNOTTED",0.6545454545454545,Let Z : Pw(M) →M be the Ck function deﬁned by
"ON
THE
KNOTTED",0.6565656565656566,"Z(y) ∈M,
Pw(Z(y)) = y,"
"ON
THE
KNOTTED",0.6585858585858586,"that is it is the inverse of Pw|M : M →Pw(M), where Pw(M) ⊂{w}⊥. Let g : ˜N = Pw(M) →
R be the function
g(y) = (Z(y) −y) · w,
y ∈Pw(M)."
"ON
THE
KNOTTED",0.6606060606060606,Then ˜N is a n-dimensional Ck submanifold of (m −1)-dimensional Euclidean space H = {w}⊥
"ON
THE
KNOTTED",0.6626262626262627,"and g is a Ck function deﬁned on it. By deﬁnition of a Ck submanifold of H, any point x ∈˜N
has a neighborhood U ⊂H with local Ck coordinates ψ : U →Rm such that ψ( ˜N ∩U) =
({0}m−1−n ×Rn)∩ψ(U). Using these coordinates, we see that g can be extended to a Ck function
in U. Using a suitable partition of unity, we see that there is a Ck map G : {w}⊥→R that a Ck
extension of g that is, G| ˜
N = g."
"ON
THE
KNOTTED",0.6646464646464646,"Then the map
Φ1 : Rm →Rm,
Φ1(x) = x −G(Pw(x))w
is a Ck diffeomorphism of Rm that maps M to m −1 dimensional space {w}⊥, that is"
"ON
THE
KNOTTED",0.6666666666666666,Φ1(M) ⊂{w}⊥.
"ON
THE
KNOTTED",0.6686868686868687,"In the case when m ≥3n + 1, we can repeat this construction n times. This is possible as m −n ≥
2n + 1. Then we obtain Ck diffeomorphisms Φj : Rm →Rm, j = 1, . . . , n such that their
composition Φn ◦· · · ◦Φ1 : Rm →Rm is a Ck-diffeomorphism such that which"
"ON
THE
KNOTTED",0.6707070707070707,"Φn ◦· · · ◦Φ1(M) ⊂Y ′,"
"ON
THE
KNOTTED",0.6727272727272727,"where Y ′ ⊂Rm is a m −n dimensional linear space. By letting Ψ = Q ◦Φn ◦· · · ◦Φ1 for
rotation matrix Q ∈Rm×m, we have that Y := Q(Y ′) = {0}n × Rm−n. Also, letting φ : X =
Rn × {0}m−n →Rm be the map"
"ON
THE
KNOTTED",0.6747474747474748,"φ(x, 0) = Ψ(f(x)) ∈Y,"
"ON
THE
KNOTTED",0.6767676767676768,"where f is the function given in Eq. 35, we have that φ : X →Y is a Ck-diffeomorphism. We
observe that m −n ≥2n + 1 and so we can apply (Madsen et al., 1997, Lemma 7.6) to extend φ to
a Ck-diffeomorphism
h : Rm →Rm"
"ON
THE
KNOTTED",0.6787878787878788,Under review as a conference paper at ICLR 2022
"ON
THE
KNOTTED",0.6808080808080809,"such that h|X = φ. Note that (Madsen et al., 1997, Lemma 7.6) concerns an extension of a home-
omorphism, but as the extension h is given by an explicit formula which is locally a ﬁnite sum of
Ck functions, the same proof gives a Ck-diffeomorphic extension h to a diffeomorphism φ. This
technique is called the ‘clean trick’."
"ON
THE
KNOTTED",0.6828282828282828,"Finally, to obtain the claim, we observe that when R : Rn →Rm, R(x) = (x, 0) ∈{0}n × Rm−n
is the zero padding operator, we have"
"ON
THE
KNOTTED",0.6848484848484848,"f(x) = Ψ−1(φ(R(x))),
x ∈Rn."
"ON
THE
KNOTTED",0.6868686868686869,"As h|X = φ and R(x) ∈X, this yields"
"ON
THE
KNOTTED",0.6888888888888889,"f(x) = Ψ−1(h(R(x))),
x ∈Rn,"
"ON
THE
KNOTTED",0.6909090909090909,"that is,
f = E ◦R
where E = Ψ−1 ◦h : Rm →Rm is a Ck diffeomorphism. Thus f ∈Ik(Rn, Rm). This proves
Eq. 9 when m ≥3n + 1."
"ON
THE
KNOTTED",0.692929292929293,"C.2.5
THE PROOF OF LEMMA 3"
"ON
THE
KNOTTED",0.694949494949495,"The proof of Lemma 3. Let f = F2 ◦F1 where F2 ∈Fo,m and F1 ∈Fn,o and ϵ > 0 be given, and
let Eo,m. Clearly, BK,W (f, E) ≤BK,W (F2, E) and so by the m, o, o MEP of Eo,m with respect
to Fo,m, we have the existence of an rm ∈emb(f(K), Eo,m) such that ∥I −r∥L∞(f(K)) < ϵ."
"ON
THE
KNOTTED",0.696969696969697,"Ko := (Eo,m)−1 ◦r ◦f(K) is compact, hence Eo,m is Lipschitz on Ko, so we can apply Lemma 7
point 4, so"
"ON
THE
KNOTTED",0.6989898989898989,"BK,W (f, Eo,m ◦Ep,o) ≤∥I −r∥L∞(f(K)) + Lip(Eo,m)BK,W ((Eo,m)−1 ◦r ◦f, Ep,o).
(36)"
"ON
THE
KNOTTED",0.701010101010101,"But, because f ∈Fo,m◦Fn,o, we can choose a Ep,o ∈Ep,o
1
so that BK,W ((Eo,m)−1◦r◦f, Ep,o) ≤
ϵ
2 Lip(Eo,m) which, combined with Eqn. 36, proves the result."
"ON
THE
KNOTTED",0.703030303030303,"C.2.6
THE PROOF OF LEMMA 4"
"ON
THE
KNOTTED",0.705050505050505,"The proof of Lemma 4. Recall that F ⊂emb(Rn, Rm).
Suppose that Eo,m
2
does not have the
m, n, o MEP with respect to F, then there are some ϵ > 0 and f ∈F so that"
"ON
THE
KNOTTED",0.7070707070707071,"∀Eo,m ∈Eo,m
2
∀W1 ⊂⊂Ro,
BK,W1(f, Eo,m
2
) ≥ϵ.
(37)"
"ON
THE
KNOTTED",0.7090909090909091,"From Lemma 7 point 2, we have that"
"ON
THE
KNOTTED",0.7111111111111111,"ϵ ≤BK,W1(f, Eo,m
2
) ≤BK,W (f, Eo,m
2
◦Ep,o
1 )
(38)"
"ON
THE
KNOTTED",0.7131313131313132,"for all Ep,o
1
∈Ep,o
1
and for all compact sets W ⊂Rp that satisfy Ep,o
1 (W1) ⊂W. We observe that
if W ′ ⊂Rp is a compact set such that W ′ ⊂W, we have"
"ON
THE
KNOTTED",0.7151515151515152,"BK,W (f, Eo,m
2
◦Ep,o
1 ) ≤BK,W ′(f, Eo,m
2
◦Ep,o
1 )"
"ON
THE
KNOTTED",0.7171717171717171,"Thus, inequality Eq. 38 holds for all Ep,o
1
∈Ep,o
1
and for all compact sets W ⊂Rp. Summarising,
we have seen that there are f ∈F and ϵ > 0 such that for all Ep,o
1
∈Ep,o
1
and for all compact sets
W ⊂Rp we have ϵ ≤BK,W (f, Eo,m
2
◦Ep,o
1 ). Hence Eo,m
2
does not have the m, n, o MEP with
respect to F, and we have obtained a contradiction, which proves the result."
"ON
THE
KNOTTED",0.7191919191919192,"C.3
UNIVERSALITY"
"ON
THE
KNOTTED",0.7212121212121212,"C.3.1
THE PROOF OF THEOREM 1"
"ON
THE
KNOTTED",0.7232323232323232,The proof of Theorem 1. First we prove the claim under the assumptions (i).
"ON
THE
KNOTTED",0.7252525252525253,First we prove the claim under assumption (i).
"ON
THE
KNOTTED",0.7272727272727273,Let W ⊂Rn be an open relatively compact set. From Lemma 3 we have that
"ON
THE
KNOTTED",0.7292929292929293,"En,m := EnL−1,m
L
◦· · · ◦En,n1
1
(39)"
"ON
THE
KNOTTED",0.7313131313131314,Under review as a conference paper at ICLR 2022
"ON
THE
KNOTTED",0.7333333333333333,"has the m, n, n MEP w.r.t. F := FnL−1,m
L
◦· · ·◦Fn,n1
1
. Thus for any ϵ1 > 0, we have an ˜E ∈En,m"
"ON
THE
KNOTTED",0.7353535353535353,"s.t. BK,W (F, ˜E) < ϵ1."
"ON
THE
KNOTTED",0.7373737373737373,"From Lemma 7 point 5, we have the existence of a µ′ ∈P(W) so that W2

F #µ, ˜E#µ′
< ϵ1."
"ON
THE
KNOTTED",0.7393939393939394,"By convolving µ′ with a suitable molliﬁer φ, we can obtain a measure µ′′ = µ′ ∗φ ∈P(W) that is
absolutely continuous with respect to the Lebesgue measure so that"
"ON
THE
KNOTTED",0.7414141414141414,"W2 (µ′, µ′′) <
ϵ1
1 + Lip( ˜E)
,"
"ON
THE
KNOTTED",0.7434343434343434,"see (Ambrosio et al., 2008, Lemma 7.1.10.), and so W2

˜E#µ′, ˜E#µ′′
< ϵ1. Hence,"
"ON
THE
KNOTTED",0.7454545454545455,"W2

F #µ, ˜E#µ′′
< 2ϵ1.
(40)"
"ON
THE
KNOTTED",0.7474747474747475,"Next, from universality of T n
0
for any ϵ2 > 0, we have the existence of a T0 ∈T n
0
so that
W2 (µ′′, T0#µ) < ϵ2. From Lemma 7 points 3 and 4 we have that"
"ON
THE
KNOTTED",0.7494949494949495,"W2

F #µ, ˜E ◦T0#µ

≤2ϵ1 + ϵ2 Lip( ˜E).
(41)"
"ON
THE
KNOTTED",0.7515151515151515,"For a given ϵ > 0, choosing ϵ1 < ϵ"
"ON
THE
KNOTTED",0.7535353535353535,"4 and ϵ2 <
ϵ
2(1+Lip( ˜
E)) yields that the map E = ˜E ◦T0 ∈E is
such that W2 (F #µ, E#µ) < ϵ. This yields the result."
"ON
THE
KNOTTED",0.7555555555555555,"Next we prove the claim under the assumptions (ii). By our assumptions, in the weak topology
of the space C2(Rnj, Rnj), the closure of the set T nj ⊂C2(Rnj, Rnj) contains the space of
Diff2(Rnj, Rnj). Moreover, by our assumptions Rnj−1,nj contains a linear map R. We observe
that as Rnj−1,nj is a space of expansive elements, the map R is injective. and hecne by Lemma 1,
the family
Enj−1,nj
j
= T nj ◦Rnj−1,nj"
"ON
THE
KNOTTED",0.7575757575757576,"has the MEP w.r.t. F = I1(Rn, Rm). By Lemma 2, we have that I1(Rn, Rm) coincides with
the space emb1(Rn, Rm).
Finally, by the assumption that T n0
0
is dense in the space of C2-
diffeomorphism Diff2(Rnℓ) implies that T n0
0
is a Lp-universal approximator for the set of C∞-
smooth triangular maps for all p < ∞. Hence by Lemma 3 in Appendix A of Teshima et al. (2020),
T n0
0
is a distributionally universal. From these the claim in the case (ii) follows in the same way as
the case (i) using the family F = emb1(Rn, Rm)."
"ON
THE
KNOTTED",0.7595959595959596,"C.3.2
THE PROOF OF LEMMA 5"
"ON
THE
KNOTTED",0.7616161616161616,"The proof of Lemma 5. The proof follows from taking the logical negation of the MEP for F. If the
MEP is not satisﬁed, then there is some f ∈F so that BK,W (f, E) is never smaller than ϵ > 0 for
all E ∈E. Applying the deﬁnition of BK,W (f, E) from Eqn. 6 yields the result."
"ON
THE
KNOTTED",0.7636363636363637,"C.3.3
THE PROOF OF COR. 1"
"ON
THE
KNOTTED",0.7656565656565657,The proof of Cor. 1. The proof of Eqn 12 follows from the deﬁnition of the MEP.
"ON
THE
KNOTTED",0.7676767676767676,"The proof of Eqn. 13 follows from applying Lemma 7 point 4, and applying the same arguments
as in the proof of Theorem 1 so that limi→∞
1
Lip(Ei)BK,W (E−1
i
◦r ◦f, E′
i) →0, and so that
W2 (F #µ, Ei ◦E′
i#µ′) = ϵi where ϵi →0 as i →∞.. From the distributional universality of T n
0 ,
and continuity of Ei ◦E′
i we have the existence of a Ti ∈T n
0 so that W2 (F #µ, Ei ◦E′
i ◦Ti#µ) =
2ϵi. Choosing ϵi so that limi→∞ϵi yields the result."
"ON
THE
KNOTTED",0.7696969696969697,"APPENDIX D
ADDITIONAL PROPERTIES"
"ON
THE
KNOTTED",0.7717171717171717,"D.1
LAYER-WISE PROJECTION"
"ON
THE
KNOTTED",0.7737373737373737,"Here we provide the details of our closed-form layerwise projection algorithm The ﬂow layers are
injective, and are often implemented to be numerically easy to invert. Thus, the crux of the algorithm"
"ON
THE
KNOTTED",0.7757575757575758,Under review as a conference paper at ICLR 2022
"ON
THE
KNOTTED",0.7777777777777778,"comes from inverting the injective expansive layers, R. The range of the ReLU layer is piece-wise
afﬁne, hence the inversion follows a two-step program. First, identify which afﬁne piece (described
algebraically, onto which sign pattern) to project. Second, project to this point using a standard
least-squares solver."
"ON
THE
KNOTTED",0.7797979797979798,"The second step is always straight-forward to analyze, but the ﬁrst is more complicated."
"ON
THE
KNOTTED",0.7818181818181819,"The key step in our algorithm is the fact that for the speciﬁc choice of weight matrix W =

B
−DB 
,"
"ON
THE
KNOTTED",0.7838383838383839,"given any y ∈R2n, we can always solve the least-squares inversion problem exactly."
"ON
THE
KNOTTED",0.7858585858585858,We prove this result in several parts given below.
"ON
THE
KNOTTED",0.7878787878787878,"1. For any y ∈R2n, MyW ∈Rn×n is full-rank."
"ON
THE
KNOTTED",0.7898989898989899,"2. If [y]i ̸= [y]i+n for each i = 1, . . . , n, then the argmin in Eqn. 17 is well deﬁned, i.e. that
there is a unique minimizer. Otherwise there are 2I minimizers, where I is the number of
distinct i such that [y]i = [y]i+n."
"ON
THE
KNOTTED",0.7919191919191919,"3. If ˜
My =
∆y
(In×n −∆y)
, then"
"ON
THE
KNOTTED",0.793939393939394,"min
x∈Rn ∥y −R(x)∥2
2 = min
x∈Rn ∥My (y −Wx)∥2
2 +
 ˜
Myy

2"
"ON
THE
KNOTTED",0.795959595959596,"2 .
(42)"
"ON
THE
KNOTTED",0.797979797979798,4. We verify Eqn. 17.
"ON
THE
KNOTTED",0.8,"The proof of Theorem 2.
1. Using the deﬁnition of My, we have, My"
"ON
THE
KNOTTED",0.802020202020202,"
B
−DB"
"ON
THE
KNOTTED",0.804040404040404,"
=
 
In×n −∆y

B −∆yDB =
 
In×n −∆y −∆yD

B.
(43)"
"ON
THE
KNOTTED",0.806060606060606,"But, (In×n −∆y −∆yD) is a full-rank diagonal matrix (with entries either 1 or [D]i,i),"
"ON
THE
KNOTTED",0.8080808080808081,"and B is full rank by assumption, hence My"
"ON
THE
KNOTTED",0.8101010101010101,"
B
−DB"
"ON
THE
KNOTTED",0.8121212121212121,"
is too."
"ON
THE
KNOTTED",0.8141414141414142,"2. Because B is square and full rank there exists a basis5 n
ˆbi
o"
"ON
THE
KNOTTED",0.8161616161616162,"i=1,...,n of Rn such that"
"ON
THE
KNOTTED",0.8181818181818182,"D
ˆbj, bi
E
=
1
if i = j
0
if i ̸= j .
(44)"
"ON
THE
KNOTTED",0.8202020202020202,"For an x ∈Rn, let αi = ⟨x, bi⟩for i = 1, . . . , n be the expansion of x in the ˆbi basis."
"ON
THE
KNOTTED",0.8222222222222222,"min
x∈Rn ∥y −R(x)∥2
2 = min
x∈Rn"
"N
X",0.8242424242424242,"2n
X"
"N
X",0.8262626262626263,"i=1
[y −R(x)]2
i
(45) = n
X"
"N
X",0.8282828282828283,"i=1
min
xi∈R ([y]i −max(⟨x, bi⟩, 0))2 +
 
[y]i+n −max(⟨x, −[D]ii bi⟩, 0)
2 (46)"
"N
X",0.8303030303030303,"We now consider minizing Eqn. 46 by minimizing the basis expansion in terms of αi, n
X"
"N
X",0.8323232323232324,"i=1
min
αi∈R ([y]i −max(αi, 0))2 +
 
[y]i+n −max(−[D]ii αi, 0)
2
(47)"
"N
X",0.8343434343434344,"Eqn. 47 is clearly minimized by minizing each term in the sum, hence we search for a
minimizer of the i’th term"
"N
X",0.8363636363636363,"min
αi∈R ([y]i −max(αi, 0))2 +
 
[y]i+n −max(−[D]ii αi, 0)
2
(48)"
"N
X",0.8383838383838383,5Namely the columns of the matrix B−1
"N
X",0.8404040404040404,Under review as a conference paper at ICLR 2022
"N
X",0.8424242424242424,"Noting f(αi) as the quantity inside the minimum of Eqn. 48, we consider the positive,
negative and zero αi cases of Eqn. 48 separately and we get"
"N
X",0.8444444444444444,"min
αi∈R+ f(αi) = min
αi∈R+ ([y]i −αi)2 + [y]2
i+n = [y]2
i+n
(49)"
"N
X",0.8464646464646465,"min
αi∈R−f(αi) = min
αi∈R+ [y]2
i +
 
[y]i+n + [D]ii αi
2 = [y]2
i
(50)"
"N
X",0.8484848484848485,"f(0) = [y]2
i + [y]2
i+n .
(51)"
"N
X",0.8505050505050505,"If [y]i+n > [y]i, then the minimizer of Eqn. 48 is αi = −
[y]2
i+n
[D]ii
< 0. Conversely if
[y]i+n < [y]i then the minimzer of Eqn. 48 is αi = [y]i > 0. This argument applies all
i = 1, . . . , n, and hence if [y]i ̸= [y]i+1 for all i = 1, . . . , n then the minimizing x is
unique."
"N
X",0.8525252525252526,"If [y]i = [y]i+1 then there are exactly two minimizers of f(αi), −
[y]2
i+n
[D]ii and [y]i, for both"
"N
X",0.8545454545454545,"of which f(αi) = [y]2
i = [y]2
i+n."
"N
X",0.8565656565656565,"3. If we suppose that [y]i+n −[y]i > 0, then [c(y)]i = 0 and [c(y)]i+n > 0, thus [∆y]ii = 1,
hence if we let xmin be the minimizing x from part 1, then"
"N
X",0.8585858585858586,"([y]i −max(⟨xmin, bi⟩, 0))2 +
 
[y]i+n −max(⟨xmin, −[D]ii bi⟩, 0)
2
(52)"
"N
X",0.8606060606060606,"= [y]2
i +
 
[y]i+n −max(⟨xmin, −[D]ii bi⟩, 0)
2
(53)"
"N
X",0.8626262626262626,"=
h
˜
Myy
i2"
"N
X",0.8646464646464647,"i + [My (y −Wxmin)]2
i
(54)"
"N
X",0.8666666666666667,If [y]i+n −[y]i ≤0 then we have
"N
X",0.8686868686868687,"([y]i −max(⟨xmin, bi⟩, 0))2 +
 
[y]i+n −max(⟨xmin, −[D]ii bi⟩, 0)
2
(55)"
"N
X",0.8707070707070707,"= ([y]i −max(⟨xmin, bi⟩, 0))2 + [y]2
i+n
(56)"
"N
X",0.8727272727272727,"= [My (y −Wxmin)]2
i +
h
˜
Myy
i2"
"N
X",0.8747474747474747,"i .
(57)"
"N
X",0.8767676767676768,"Thus combining Eqn.s 45, 46, 54 and 57 for each i = 1, . . . , n, we have that"
"N
X",0.8787878787878788,"min
x∈Rn ∥y −R(x)∥2
2 = min
x∈Rn ∥My (y −Wx)∥2
2 + ∥Myy∥2
2 .
(58)"
"N
X",0.8808080808080808,"4. For the ﬁnal point, combining all of the above points we have"
"N
X",0.8828282828282829,"min
x∈Rn ∥y −R(x)∥2
2 = min
x∈Rn ∥My (y −Wx)∥2
2 .
(59)"
"N
X",0.8848484848484849,"Further we have from Point 1 that MyW is full rank, hence (MyW)−1 Myy = R†(y) is a
minimizer of Eqn. 59. If [y]i ̸= [y]i+n for all i = 1, . . . , n then Part 2 applies, and R†(y)
is the unique minimizer of ∥y −R(x)∥2
2. In either case, we have that R†(y) is a minimizer."
"N
X",0.8868686868686869,"D.2
BAYESIAN UNCERTAINTY QUANTIFICATION"
"N
X",0.8888888888888888,"We now discuss the process for making the network amenable to Bayesian Uncertainty Quantiﬁca-
tion (Bayesian UQ)."
"N
X",0.8909090909090909,"Assumption 2. Let y0 ∈Rn be given, let y1 = T0y0 and for each ℓ= 2, . . . , L, then let yℓ:=
Rℓ(yℓ−1/2) and yℓ−1/2 := Tℓ−1(yℓ−1). We assume that for ℓ= 1, . . . , L −1, Tℓ(yℓ), Rℓ(yℓ−1/2)
and T0(y0) are differentiable."
"N
X",0.8929292929292929,Under review as a conference paper at ICLR 2022
"N
X",0.8949494949494949,"Lemma 8. If a network F of the form of Eqn. 1 satisﬁes Assumption 2, then we have the following
upper bound on the log-likelihood,"
"N
X",0.896969696969697,"log py(y0) ≤log pz(F −1(y0)) + 1 2 L
X"
"N
X",0.898989898989899,"ℓ=1
log"
"N
X",0.901010101010101,"det
∂Rℓ
∂yℓ−1/2"
"N
X",0.9030303030303031,"T
(yℓ−1/2)
∂Rℓ
∂yℓ−1/2
(yℓ−1/2)  + L
X"
"N
X",0.9050505050505051,"ℓ=0
log
det ∂Tℓ"
"N
X",0.907070707070707,"∂yℓ
(yℓ)
 .
(60)"
"N
X",0.9090909090909091,"Proof. The proof of Lemma 8 follows from computing the log of,"
"N
X",0.9111111111111111,py(y0) = pz(F −1(y0))
"N
X",0.9131313131313131,"det
∂F −1"
"N
X",0.9151515151515152,"∂y0

F (Rn) (61)"
"N
X",0.9171717171717172,and then applying deﬁnition 1 and expanding term using the laws of logarithms we obtain.
"N
X",0.9191919191919192,"log py(y) ≤log pz(F −1(y)) + 1 2 L
X"
"N
X",0.9212121212121213,"ℓ=1
log"
"N
X",0.9232323232323232,"det
∂Rℓ
∂yℓ−1/2"
"N
X",0.9252525252525252,"T
(yℓ−1/2)
∂Rℓ
∂yℓ−1/2
(yℓ−1/2)"
"N
X",0.9272727272727272,"|
{z
}
R + L
X"
"N
X",0.9292929292929293,"ℓ=0
log
det ∂Tℓ"
"N
X",0.9313131313131313,"∂yℓ
(yℓ)

|
{z
}
T"
"N
X",0.9333333333333333,".
(62)"
"N
X",0.9353535353535354,"for ℓ= 1, . . . , L, see (Kothari et al., 2021, Appendix B) for details."
"N
X",0.9373737373737374,"We note here that in Equation 60 we have an inequality rather than an equality. The inequality
comes from the expansive layers, as the ﬂow-layers are all endomorphisms, i.e. Rnℓ→Rnℓ. We
now remark on the ease of approximating, or at least bounding, T and R in Eqn. 62."
"N
X",0.9393939393939394,"Values for R when R is (R1) or (R2) are straightforward to compute or approximate6. When Rn,m
is (R3), R ∈R are not differentiable, but are differentiable a.e.. Further, ∇Rℓ(xnℓ) can a.e. be
computed by applying a simple mask 0 and 1 mask to the weight matrices for an exact computation
of R . Additionally, from Hutchinson (1989); Kothari et al. (2021) we have the following formula
that approximates R , log"
"N
X",0.9414141414141414,"det ∂Rℓ ∂yℓ T ∂Rℓ ∂yℓ = −Tr  1 k ∞
X k=1 "
"N
X",0.9434343434343434,I −β ∂Rℓ ∂yℓ T ∂Rℓ ∂yℓ !k
"N
X",0.9454545454545454,"−d log β ≈−Eν  1 k ∞
X"
"N
X",0.9474747474747475,"k=1
νT"
"N
X",0.9494949494949495,I −β ∂Rℓ ∂yℓ T ∂Rℓ ∂yℓ !k ν 
"N
X",0.9515151515151515,"−d log β.
(63)"
"N
X",0.9535353535353536,"where η ∈Rnℓ. Computation of the T term is often easier, comparatively. This is because these
networks are often designed to make this value as easy to compute as possible. For example the
NICE Dinh et al. (2014), Real-NVP Dinh et al. (2016), and GLOW Kingma & Dhariwal (2018)
architectures are all designed so that T can be computed form a short scalar product."
"N
X",0.9555555555555556,"D.3
BLACK-BOX RECOVERY"
"N
X",0.9575757575757575,"We now discuss assumptions that enable black-box recovery of the weights of our entire network
post-training."
"N
X",0.9595959595959596,"Assumption 3. For each ℓ= 1, . . . , L, Rℓis an afﬁne ReLU layer. Each Tℓand T0 is constructed
from a ﬁnite number of afﬁne ReLU layers."
"N
X",0.9616161616161616,"6for example from a LU decomposition, see (Golub, 1996, Section 3.2)"
"N
X",0.9636363636363636,Under review as a conference paper at ICLR 2022
"N
X",0.9656565656565657,"Remark 3. If a network F of the form of Eqn. 1 satisﬁes Assumption 3, then given the range of the
network, the range of the network can be recovered exactly."
"N
X",0.9676767676767677,"Further, if the linear region assumption from Rolnick & K¨ording (2020) is satisﬁed, then the exact
weights are recovered, subject to two natural isometries discussed below.
Remark 4. The ReLU part of Assumption 3 is for all examples in Sec. 2.1. Further it is also
satisﬁed by both ﬂows considered in Sec. 2.2, provided that the various gi are given by layers of
afﬁne ReLU’s."
"N
X",0.9696969696969697,"In Rolnick & K¨ording (2020), the authors show that, although ReLU networks depend on the value
of their weight matrix in non-linear ways, it is still possible to recover the exact weights of a given
ReLU network in a black-box way, subject to natural isometrics. The authors show that this is
possible not only in theory, but in numerical applications as well."
"N
X",0.9717171717171718,"The works of Rolnick & K¨ording (2020); Bui Thi Mai & Lampert (2020) imply that provided the ac-
tivation functions of the expressive elements are ReLU then the entire network can be recovered in a
black-box way. Further, provided that either the ‘linear region assumption’ from Rolnick & K¨ording
(2020) or the generality assumption from Bui Thi Mai & Lampert (2020) is satisﬁed, then the entire
network can be recovered uniquely modulo the natural isometries of rescaling and permutation of
weight matrices."
"N
X",0.9737373737373738,"First we describe the two natural isometries of scaling and permutation. Consider the following
function
f(x) = W2φ(W1x)
(64)"
"N
X",0.9757575757575757,"where φ is coordinate-wise homogeneous degree 1 (such as ReLU) and W1 ∈Rn1×n2 and W2 ∈
Rn2×n3. If we let P ∈Rn2×n2 be any permutation matrix, and D+ be a diagonal matrix with
strictly positive elements, then we can write
f(x) = W2P ′D−1
+ φ(D+PW1x)
(65)
as well. Thus ReLU networks can only ever be uniquely given subject to these two isometries.
When describe unique recovery in the rest of this section, we mean modulo these two isometries."
"N
X",0.9777777777777777,"In Rolnick & K¨ording (2020), the authors describe how all parameters of a ReLU network can be
recovered uniquely (called reverse engineered in Rolnick & K¨ording (2020)), subject to the so called
‘linear7 region assumption’, LRA."
"N
X",0.9797979797979798,"The input space Rn can be partitioned into a ﬁnite number of open {Si}ni
i=1, where for each k,
F(x) = Wki + bi, i.e. the network corresponds to an afﬁne polyhedron in the output space. The
algorithms (Rolnick & K¨ording, 2020, Alg.s 1 & 2) are roughly described below."
"N
X",0.9818181818181818,"First, identify at least one point within each afﬁne polyhedra {Hj}nj
j=1. Then identify the boundaries
between polyhedra. The boundaries between sections are always one afﬁne ‘piece’ of piecewise
hyperplanes {Hj}nj
j=1. These {Hj}nj
j=1 are the central objects which indicate the (de)activation of
an element of a ReLU somewhere in the network. If the Hj are full hyperplanes, then the ReLU
that is (de)activates occurs in the ﬁrst layer of the network. If Hj is not a full hyperplane, then it
necessarily has a bend where it intersects another hyperplane Hj′. Further, except for a Lebesgue
measure 0 set, when Hj intersects Hj′ the latter does not have a bend. If this is the case, then
Hj′ corresponds to a ReLU (de)activation in an earlier layer than Hj. In this way the activation
functions of every layer can be deduced. Once this is done, the normals of the hyperplanes can be
used to infer the row-vectors of the various weight matrices, letting one recover the entire network."
"N
X",0.9838383838383838,"The above algorithm recovers all of the weights exactly provided that the LRA is satisﬁed. The
LRA is satisﬁed if for every distinct Si and Si′, either Wi ̸= Wi′ or bi ̸= bi′. That is, different
sign patterns produce different afﬁne sections in the output space. This is a natural assumption, as
the algorithm as described above reconstruction works by ﬁrst detecting the boundaries between
adjacent afﬁne polyhedra, which is only possible if the LRA holds."
"N
X",0.9858585858585859,"Given the weights of a network there is currently no simple way to detect if the LRA is satisﬁed, to
our knowledge. Nevertheless the authors of Rolnick & K¨ording (2020) show that if it is satisﬁed,"
"N
X",0.9878787878787879,"7The use of ‘linear’ in this context is somewhat non-standard, and instead means afﬁne. In this section we
use the term ‘linear region assumption’, but use ‘afﬁne’ where Rolnick & K¨ording (2020) would use ‘linear’ to
preserve mathematical meaning."
"N
X",0.98989898989899,Under review as a conference paper at ICLR 2022
"N
X",0.9919191919191919,"then unique recovery follows. Nevertheless recovery of the range of the entire network is possible,
but this recovery may not be unique."
"N
X",0.9939393939393939,"In Bui Thi Mai & Lampert (2020) the authors also consider the problem of recovering weights of a
ReLU neural network, however the authors therein study the question of when there exist isometries
beyond the two natural ones described above. In particular the main result (Bui Thi Mai & Lampert,
2020, Theorem 1) shows the following. Let En0,nL be a ReLU network that is L layers deep and
non-increasing. Suppose that E1, E2 ∈En0,nL, E1 and E2 are general8 and for all x ∈Rn0
E1(x) = E2(x), then E1 is parametrically identical to E2 subject to the two natural isometries."
"N
X",0.9959595959595959,"This work provides the stronger result, however does not apply to the networks that we consider
out of the box. It does apply to our expressive elements (provided that they use ReLU activation
functions, and are non-increasing), but not necessarily apply to the network on the whole."
"N
X",0.997979797979798,8A set it in a topology is general if it’s complement is closed and nowhere dense.
