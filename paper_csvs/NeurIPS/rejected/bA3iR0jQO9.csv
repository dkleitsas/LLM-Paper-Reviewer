Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001692047377326565,"Learning Nash equilibrium (NE) in complex zero-sum games with multi-agent
1"
ABSTRACT,0.00338409475465313,"reinforcement learning (MARL) can be extremely computationally expensive. Cur-
2"
ABSTRACT,0.005076142131979695,"riculum learning is an effective way to accelerate learning, but an under-explored
3"
ABSTRACT,0.00676818950930626,"dimension for generating a curriculum is the difﬁculty-to-learn of the subgames –
4"
ABSTRACT,0.008460236886632826,"games induced by starting from a speciﬁc state. In this work, we present a novel
5"
ABSTRACT,0.01015228426395939,"subgame curriculum learning framework for zero-sum games. It adopts an adaptive
6"
ABSTRACT,0.011844331641285956,"initial state distribution by resetting agents to some previously visited states where
7"
ABSTRACT,0.01353637901861252,"they can quickly learn to improve performance. Building upon this framework,
8"
ABSTRACT,0.015228426395939087,"we derive a subgame selection metric that approximates the squared distance to
9"
ABSTRACT,0.01692047377326565,"NE values and further adopt a particle-based state sampler for subgame genera-
10"
ABSTRACT,0.018612521150592216,"tion. Integrating these techniques leads to our new algorithm, Subgame Automatic
11"
ABSTRACT,0.02030456852791878,"Curriculum Learning (SACL), which is a realization of the subgame curriculum
12"
ABSTRACT,0.021996615905245348,"learning framework. SACL can be combined with any MARL algorithm such as
13"
ABSTRACT,0.023688663282571912,"MAPPO. Experiments in the particle-world environment and Google Research
14"
ABSTRACT,0.025380710659898477,"Football environment show SACL produces much stronger policies than baselines.
15"
ABSTRACT,0.02707275803722504,"In the challenging hide-and-seek quadrant environment, SACL produces all four
16"
ABSTRACT,0.028764805414551606,"emergent stages and uses only half the samples of MAPPO with self-play. The
17"
ABSTRACT,0.030456852791878174,"project website is at https://sites.google.com/view/sacl-neurips.
18"
INTRODUCTION,0.032148900169204735,"1
Introduction
19"
INTRODUCTION,0.0338409475465313,"Applying reinforcement learning (RL) to zero-sum games has led to enormous success, with trained
20"
INTRODUCTION,0.03553299492385787,"agents defeating professional humans in Go [41], StarCraft II [46], and Dota 2 [5]. To ﬁnd an
21"
INTRODUCTION,0.03722504230118443,"approximate Nash equilibrium (NE) in complex games, these works often require a tremendous
22"
INTRODUCTION,0.038917089678511,"amount of training resources including hundreds of GPUs and weeks or even months of time. The
23"
INTRODUCTION,0.04060913705583756,"unaffordable cost prevents RL from more real-world applications beyond these ﬂagship projects
24"
INTRODUCTION,0.04230118443316413,"supported by big companies and makes it important to develop algorithms that can learn close-to-
25"
INTRODUCTION,0.043993231810490696,"equilibrium strategies in a substantially more efﬁcient manner.
26"
INTRODUCTION,0.04568527918781726,"One way to accelerate training is curriculum learning – training agents in tasks from easy to hard.
27"
INTRODUCTION,0.047377326565143825,"Many existing works in solving zero-sum games with MARL generate a curriculum by choosing
28"
INTRODUCTION,0.049069373942470386,"whom to play with. They often use self-play to provide a natural policy curriculum as the agents
29"
INTRODUCTION,0.050761421319796954,"are trained against increasingly stronger opponents [4, 2]. The self-play framework can be further
30"
INTRODUCTION,0.05245346869712352,"extended to population-based training (PBT) by maintaining a policy pool and iteratively training new
31"
INTRODUCTION,0.05414551607445008,"best responses to mixtures of previous policies [31, 23]. Such a policy-level curriculum generation
32"
INTRODUCTION,0.05583756345177665,"paradigm is very different from the paradigm commonly used in goal-conditioned RL [29, 35].
33"
INTRODUCTION,0.05752961082910321,"Most curriculum learning methods for goal-conditioned problems directly reset the goal or initial
34"
INTRODUCTION,0.05922165820642978,"states for each training episode to ensure the current task is of suitable difﬁculty for the learning
35"
INTRODUCTION,0.06091370558375635,"agent. In contrast, the policy-level curriculum in zero-sum games only provides increasingly stronger
36"
INTRODUCTION,0.06260575296108291,"opponents, and the agents are still trained by playing the full game starting from a ﬁxed initial state
37"
INTRODUCTION,0.06429780033840947,"distribution, which is often very challenging.
38"
INTRODUCTION,0.06598984771573604,"In this paper, we propose a general subgame curriculum learning framework to further accelerate
39"
INTRODUCTION,0.0676818950930626,"MARL training for zero-sum games. It leverages ideas from goal-conditioned RL. Complementary
40"
INTRODUCTION,0.06937394247038917,"to policy-level curriculum methods like self-play and PBT, our framework generates subgames (i.e.,
41"
INTRODUCTION,0.07106598984771574,"games induced by starting from a speciﬁc state) with growing difﬁculty for agents to learn and
42"
INTRODUCTION,0.0727580372250423,"eventually solve the full game. We provide justiﬁcations for our proposal by analyzing a simple
43"
INTRODUCTION,0.07445008460236886,"iterated Rock-Paper-Scissors game. We show that in this game, vanilla MARL requires exponentially
44"
INTRODUCTION,0.07614213197969544,"many samples to learn the NE. However, by using a buffer to store the visited states and choosing an
45"
INTRODUCTION,0.077834179357022,"adaptive order of state-induced subgames to learn, the NE can be learned with linear samples.
46"
INTRODUCTION,0.07952622673434856,"A key challenge in our framework is to choose which subgame to train on. This is non-trivial in
47"
INTRODUCTION,0.08121827411167512,"zero-sum games since there does not exist a clear progression metric like the success rate in goal-
48"
INTRODUCTION,0.0829103214890017,"conditioned problems. While the squared difference between the current state value and the NE value
49"
INTRODUCTION,0.08460236886632826,"can measure the progress of learning, it is impossible to calculate this value during training as the NE
50"
INTRODUCTION,0.08629441624365482,"is generally unknown. We derive an alternative metric that approximates the squared difference with
51"
INTRODUCTION,0.08798646362098139,"a bias term and a variance term. The bias term measures how fast the state value changes and the
52"
INTRODUCTION,0.08967851099830795,"variance term measures how uncertain the current value is. We use the combination of the two terms
53"
INTRODUCTION,0.09137055837563451,"as the sampling weights for states and prioritize subgames with fast change and high uncertainty.
54"
INTRODUCTION,0.09306260575296109,"Instantiating our framework with the state selection metric and a non-parametric subgame sampler,
55"
INTRODUCTION,0.09475465313028765,"we develop an automatic curriculum learning algorithm for zero-sum games, i.e., Subgame Automatic
56"
INTRODUCTION,0.09644670050761421,"Curriculum Learning (SACL). SACL can adopt any MARL algorithm as its backbone and preserve
57"
INTRODUCTION,0.09813874788494077,"the overall convergence property. In our implementation, we choose the MAPPO algorithm [52] for
58"
INTRODUCTION,0.09983079526226735,"the best empirical performances.
59"
INTRODUCTION,0.10152284263959391,"We ﬁrst evaluate SACL in the Multi-Agent Particle Environment and Google Research Football,
60"
INTRODUCTION,0.10321489001692047,"where SACL learns stronger policies with lower exploitability than existing MARL algorithms for
61"
INTRODUCTION,0.10490693739424704,"zero-sum games given the same amount of environment interactions. We then stress-test the efﬁciency
62"
INTRODUCTION,0.1065989847715736,"of SACL in the challenging hide-and-seek environment. SACL leads to the emergence of all four
63"
INTRODUCTION,0.10829103214890017,"phases of different strategies and uses 50% fewer samples than MAPPO with self-play.
64"
PRELIMINARY,0.10998307952622674,"2
Preliminary
65"
MARKOV GAME,0.1116751269035533,"2.1
Markov game
66"
MARKOV GAME,0.11336717428087986,"A Markov game [25] is deﬁned by a tuple MG = (N, S, A, P, R, γ, ⇢), where N = {1, 2, · · · , N}
67"
MARKOV GAME,0.11505922165820642,"is the set of agents, S is the state space, A = ⇧N"
MARKOV GAME,0.116751269035533,"i=1Ai is the joint action space with Ai being the action
68"
MARKOV GAME,0.11844331641285956,"space of agent i, P : S ⇥A ! ∆(S) is the transition probability function, R = (R1, R2, · · · , RN) :
69"
MARKOV GAME,0.12013536379018612,"S ⇥A ! Rn is the joint reward function with Ri being the reward function for agent i, γ is the
70"
MARKOV GAME,0.1218274111675127,"discount factor, and ⇢is the distribution of initial states. Given the current state s and the joint action
71"
MARKOV GAME,0.12351945854483926,"a = (a1, a2, · · · , aN) of all agents, the game moves to the next state s0 with probability P(s0|s, a)
72"
MARKOV GAME,0.12521150592216582,"and agent i receives a reward Ri(s, a).
73"
MARKOV GAME,0.12690355329949238,"For inﬁnite-horizon Markov games, a subgame MG(s) is deﬁned as the Markov game induced by
74"
MARKOV GAME,0.12859560067681894,"starting from state s, i.e., ⇢(s) = 1. Selecting subgames is therefore equivalent to setting the Markov
75"
MARKOV GAME,0.13028764805414553,"game’s initial states. The subgames of ﬁnite-horizon Markov games are deﬁned similarly and have an
76"
MARKOV GAME,0.1319796954314721,"additional variable to denote the current step t.
77"
MARKOV GAME,0.13367174280879865,"We focus on two-player zero-sum Markov games, i.e., N = 2 and R1(s, a) + R2(s, a) = 0 for all
78"
MARKOV GAME,0.1353637901861252,"state-action pairs (s, a) 2 S ⇥A. We use the subscript i to denote variables of player i and the
79"
MARKOV GAME,0.13705583756345177,"subscript −i to denote variables of the player other than i. Each player uses a policy ⇡i : S ! Ai
80"
MARKOV GAME,0.13874788494077833,"to produce actions and maximize its own accumulated reward. Given the joint policy ⇡= (⇡1, ⇡2),
81"
MARKOV GAME,0.1404399323181049,"each player’s value function of state s and Q-function of state-action pair (s, a) are deﬁned as
82 V ⇡"
MARKOV GAME,0.14213197969543148,"i (s) = Eat⇠⇡(·|st),st+1⇠P (·|st,at) h X t"
MARKOV GAME,0.14382402707275804,"γtRi(st, at)"
MARKOV GAME,0.1455160744500846,"###s0 = s i ,
(1) Q⇡"
MARKOV GAME,0.14720812182741116,"i (s, a) = Eat⇠⇡(·|st),st+1⇠P (·|st,at) h X t"
MARKOV GAME,0.14890016920473773,"γtRi(st, at)"
MARKOV GAME,0.1505922165820643,"###s0 = s, a0 = a i .
(2)"
MARKOV GAME,0.15228426395939088,round 1 0
MARKOV GAME,0.15397631133671744,"round 2
!! wins"
MARKOV GAME,0.155668358714044,!! loses or draws 0
MARKOV GAME,0.15736040609137056,round n 0
MARKOV GAME,0.15905245346869712,"1
……
!! wins
!! wins
!! wins"
MARKOV GAME,0.16074450084602368,"""""
""!
""#$!"
MARKOV GAME,0.16243654822335024,Figure 1: Illustration of the iterated Rock-Paper-Scissors game.
MARKOV GAME,0.16412859560067683,"The solution concept of two-player zero-sum Markov games is Nash equilibrium (NE), which is a
83"
MARKOV GAME,0.1658206429780034,"joint policy where no player can get a higher value by changing its policy alone.
84"
MARKOV GAME,0.16751269035532995,"Deﬁnition 1 (NE). A joint policy ⇡⇤= (⇡⇤ 1, ⇡⇤"
MARKOV GAME,0.1692047377326565,"2) is a Nash equilibrium of a Markov game if for all
85"
MARKOV GAME,0.17089678510998307,"initial states s0 with ⇢(s0) > 0, the following condition holds
86 ⇡⇤"
MARKOV GAME,0.17258883248730963,i = arg max ⇡i V
MARKOV GAME,0.17428087986463622,"(⇡i,⇡⇤"
MARKOV GAME,0.17597292724196278,"−i)
i
(s0), 8i 2 {1, 2}.
(3)"
MARKOV GAME,0.17766497461928935,We use V ⇤
MARKOV GAME,0.1793570219966159,i (·) to denote the NE value function of player i and Q⇤
MARKOV GAME,0.18104906937394247,"i (·, ·) to denote the NE Q-function of
87"
MARKOV GAME,0.18274111675126903,"player i, and the following equations hold by deﬁnition and the minimax nature of zero-sum games.
88 V ⇤"
MARKOV GAME,0.1844331641285956,i (s) = max
MARKOV GAME,0.18612521150592218,⇡i min
MARKOV GAME,0.18781725888324874,⇡−i Ea⇠⇡(·|s) [Q⇤
MARKOV GAME,0.1895093062605753,"i (s, a)] ,
(4) Q⇤"
MARKOV GAME,0.19120135363790186,"i (s, a) = Ri(s, a) + γ · Es0⇠P (·|s,a) [V ⇤"
MARKOV GAME,0.19289340101522842,"i (s0)] .
(5)"
MARL ALGORITHMS IN ZERO-SUM GAMES,0.19458544839255498,"2.2
MARL algorithms in zero-sum games
89"
MARL ALGORITHMS IN ZERO-SUM GAMES,0.19627749576988154,"MARL methods have been applied to zero-sum games tracing back to the TD-Gammon project [45].
90"
MARL ALGORITHMS IN ZERO-SUM GAMES,0.19796954314720813,"A large body of work [54, 6, 42, 16] is based on regret minimization, and a well-known result is
91"
MARL ALGORITHMS IN ZERO-SUM GAMES,0.1996615905245347,"that the average of policies produced by self-play of regret-minimizing algorithms converges to the
92"
MARL ALGORITHMS IN ZERO-SUM GAMES,0.20135363790186125,"NE policy of zero-sum games [15]. Another notable line of work [25, 17, 23, 34] combines RL
93"
MARL ALGORITHMS IN ZERO-SUM GAMES,0.20304568527918782,"algorithms with game-theoretic approaches. These works typically use self-play or population-based
94"
MARL ALGORITHMS IN ZERO-SUM GAMES,0.20473773265651438,"training to collect samples and then apply RL methods like Q-learning [51] and PPO [39] to learn the
95"
MARL ALGORITHMS IN ZERO-SUM GAMES,0.20642978003384094,"NE value functions and policies, and have recently achieved great success [41, 20, 46, 5].
96"
MARL ALGORITHMS IN ZERO-SUM GAMES,0.20812182741116753,"For the analysis in the next section, we introduce a classic MARL algorithm named minimax-Q
97"
MARL ALGORITHMS IN ZERO-SUM GAMES,0.2098138747884941,"learning [25] that extends Q-learning to zero-sum games. Initializing functions Qi(·, ·) with zero
98"
MARL ALGORITHMS IN ZERO-SUM GAMES,0.21150592216582065,"values, minimax-Q uses an exploration policy induced by the current Q-functions to collect a batch
99"
MARL ALGORITHMS IN ZERO-SUM GAMES,0.2131979695431472,"of samples {(st, at, rt"
MARL ALGORITHMS IN ZERO-SUM GAMES,0.21489001692047377,"i, st+1)}T"
MARL ALGORITHMS IN ZERO-SUM GAMES,0.21658206429780033,"t=0 and uses these samples to update the Q-functions by
100"
MARL ALGORITHMS IN ZERO-SUM GAMES,0.2182741116751269,"Qi(st, at)  (1 −↵) · Qi(st, at) + ↵· % rt"
MARL ALGORITHMS IN ZERO-SUM GAMES,0.21996615905245348,i + γ · max
MARL ALGORITHMS IN ZERO-SUM GAMES,0.22165820642978004,⇡i min
MARL ALGORITHMS IN ZERO-SUM GAMES,0.2233502538071066,⇡−i Ea⇠⇡(·|s) ⇥
MARL ALGORITHMS IN ZERO-SUM GAMES,0.22504230118443316,"Qi(st+1, a) ⇤( ,
(6)"
MARL ALGORITHMS IN ZERO-SUM GAMES,0.22673434856175972,"where ↵is the learning rate. This sample-and-update process continues until the Q-functions converge.
101"
MARL ALGORITHMS IN ZERO-SUM GAMES,0.22842639593908629,"Under the assumptions that the state-action sets are discrete and ﬁnite and are visited an inﬁnite
102"
MARL ALGORITHMS IN ZERO-SUM GAMES,0.23011844331641285,"number of times, it is proved that the stochastic updates by Eq. (6) leads to the NE Q-functions [43].
103"
A MOTIVATING EXAMPLE,0.23181049069373943,"3
A motivating example
104"
A MOTIVATING EXAMPLE,0.233502538071066,"In this section, we show by a simple illustrative example that vanilla MARL methods like minimax-Q
105"
A MOTIVATING EXAMPLE,0.23519458544839256,"require exponentially many samples to derive the NE. However, if we can dynamically set the initial
106"
A MOTIVATING EXAMPLE,0.23688663282571912,"state distribution and induce an appropriate order of subgames to learn, the sample complexity can
107"
A MOTIVATING EXAMPLE,0.23857868020304568,"be substantially reduced from exponential to linear. Such an observation motivates our proposed
108"
A MOTIVATING EXAMPLE,0.24027072758037224,"algorithm described in later sections.
109"
ITERATED ROCK-PAPER-SCISSORS GAME,0.24196277495769883,"3.1
Iterated Rock-Paper-Scissors game
110"
ITERATED ROCK-PAPER-SCISSORS GAME,0.2436548223350254,"We introduce an iterated variant of the Rock-Paper-Scissor (RPS) game, denoted as RPS(n). As
111"
ITERATED ROCK-PAPER-SCISSORS GAME,0.24534686971235195,"shown in Fig. 1, P1 and P2 play the RPS game for up to n rounds. If P1 wins all rounds, it gets a
112"
ITERATED ROCK-PAPER-SCISSORS GAME,0.2470389170896785,"reward of 1 and P2 gets a reward of −1. If P1 loses or draws in any round, the game ends immediately
113"
ITERATED ROCK-PAPER-SCISSORS GAME,0.24873096446700507,"without playing the remaining rounds and both players get zero rewards. Note that the RPS(n) game
114"
ITERATED ROCK-PAPER-SCISSORS GAME,0.25042301184433163,"Figure 2: Number of samples used to learn the
NE Q-values of RPS(n) games."
ITERATED ROCK-PAPER-SCISSORS GAME,0.2521150592216582,"Algorithm 1: Subgame curriculum learning
Input: state sampler oracle(·).
Initialize policy ⇡;
repeat"
ITERATED ROCK-PAPER-SCISSORS GAME,0.25380710659898476,"Sample s0 ⇠oracle(S);
Rollout ⇡in MG(s0);
Train ⇡via MARL;
until ⇡converges;
Output: ﬁnal policy ⇡."
ITERATED ROCK-PAPER-SCISSORS GAME,0.25549915397631134,"is different from playing the RPS game repeatedly for n times because players can play less than n
115"
ITERATED ROCK-PAPER-SCISSORS GAME,0.2571912013536379,"rounds and they only receive a non-zero reward if P1 wins in all rounds. We use sk to denote the
116"
ITERATED ROCK-PAPER-SCISSORS GAME,0.25888324873096447,"state where players have already played k RPS games and are at the k + 1 round. It is easy to verify
117"
ITERATED ROCK-PAPER-SCISSORS GAME,0.26057529610829105,"that the NE policy for both players is to play Rock, Paper, or Scissors with equal probability at each
118"
ITERATED ROCK-PAPER-SCISSORS GAME,0.2622673434856176,"state. Under this joint NE policy, P1 can win one RPS game with 1/3 probability, and the probability
119"
ITERATED ROCK-PAPER-SCISSORS GAME,0.2639593908629442,"for P1 to win all n rounds and get a non-zero reward is 1/3n.
120"
ITERATED ROCK-PAPER-SCISSORS GAME,0.2656514382402707,"Consider using standard minimax-Q learning to solve the RPS(n) game. With Q-functions initialized
121"
ITERATED ROCK-PAPER-SCISSORS GAME,0.2673434856175973,"to zero, we execute the exploration policy to collect samples and perform the update in Eq. (6). Note
122"
ITERATED ROCK-PAPER-SCISSORS GAME,0.26903553299492383,"all state-actions pairs are required to be visited to guarantee convergence to the NE. Therefore, in
123"
ITERATED ROCK-PAPER-SCISSORS GAME,0.2707275803722504,"this sparse-reward game, random exploration will clearly take O(3n) steps to get a non-zero reward.
124"
ITERATED ROCK-PAPER-SCISSORS GAME,0.272419627749577,"Moreover, even if the exploration policy is perfectly set to the NE policy of RPS(n), the probability
125"
ITERATED ROCK-PAPER-SCISSORS GAME,0.27411167512690354,"for P1 to get the non-zero reward by winning all RPS games is still O(1/3n), requiring at least O(3n)
126"
ITERATED ROCK-PAPER-SCISSORS GAME,0.27580372250423013,"samples to learn the NE Q-values of the RPS(n) game.
127"
FROM EXPONENTIAL TO LINEAR COMPLEXITY,0.27749576988155666,"3.2
From exponential to linear complexity
128"
FROM EXPONENTIAL TO LINEAR COMPLEXITY,0.27918781725888325,"An important observation is that the states in later rounds become exponentially rare in the samples
129"
FROM EXPONENTIAL TO LINEAR COMPLEXITY,0.2808798646362098,"generated by starting from the ﬁxed initial state. If we can directly reset the game to these states
130"
FROM EXPONENTIAL TO LINEAR COMPLEXITY,0.2825719120135364,"and design a smart order of minimax-Q updates on the subgames induced by these states, the NE
131"
FROM EXPONENTIAL TO LINEAR COMPLEXITY,0.28426395939086296,"learning can be accelerated signiﬁcantly. Note that RPS(n) can be regarded as the composition
132"
FROM EXPONENTIAL TO LINEAR COMPLEXITY,0.2859560067681895,"of n individual RPS(1) games, a suitable order of learning would be from the easiest subgame
133"
FROM EXPONENTIAL TO LINEAR COMPLEXITY,0.2876480541455161,"RPS(1) starting from state sn−1 to the full game RPS(n) starting from state s0. Assuming we have
134"
FROM EXPONENTIAL TO LINEAR COMPLEXITY,0.2893401015228426,"full access to the state space, we ﬁrst reset the game to sn−1 and use minimax-Q to solve subgame
135"
FROM EXPONENTIAL TO LINEAR COMPLEXITY,0.2910321489001692,"RPS(1) with O(1) samples. Given that the NE Q-values of RPS(k) are learned, the next subgame
136"
FROM EXPONENTIAL TO LINEAR COMPLEXITY,0.2927241962774958,"RPS(k + 1) is equivalent to an RPS(1) game where the winning reward is the value of state sn−k.
137"
FROM EXPONENTIAL TO LINEAR COMPLEXITY,0.29441624365482233,"By sequentially applying minimax-Q to solve all n subgames from RPS(1) to RPS(n), the number
138"
FROM EXPONENTIAL TO LINEAR COMPLEXITY,0.2961082910321489,"of samples required to learn the NE Q-values is reduced substantially from O(3n) to O(n).
139"
FROM EXPONENTIAL TO LINEAR COMPLEXITY,0.29780033840947545,"In practice, we usually do not have access to the entire state space and cannot directly start from the
140"
FROM EXPONENTIAL TO LINEAR COMPLEXITY,0.29949238578680204,"last subgame RPS(1). Instead, we can use a buffer to store all visited states and gradually span the
141"
FROM EXPONENTIAL TO LINEAR COMPLEXITY,0.3011844331641286,"state space. By resetting games to the newly visited states, the number of samples required to cover the
142"
FROM EXPONENTIAL TO LINEAR COMPLEXITY,0.30287648054145516,"full state space is still O(n), and we can then apply minimax-Q from RPS(1) to RPS(n). Therefore,
143"
FROM EXPONENTIAL TO LINEAR COMPLEXITY,0.30456852791878175,"the total number of samples is still O(n). The detailed analysis can be found in Appendix A.1. We
144"
FROM EXPONENTIAL TO LINEAR COMPLEXITY,0.3062605752961083,"validate our analysis by running experiments on RPS(n) games for n = 1, · · · , 10 and the results
145"
FROM EXPONENTIAL TO LINEAR COMPLEXITY,0.3079526226734349,"averaged over ten seeds are shown in Fig. 2. It can be seen that the sample complexity reduces from
146"
FROM EXPONENTIAL TO LINEAR COMPLEXITY,0.3096446700507614,"exponential to linear by running minimax-Q over a smart order of subgames, and the result of using a
147"
FROM EXPONENTIAL TO LINEAR COMPLEXITY,0.311336717428088,"state buffer in practice is comparable to the result with full access.
148"
METHOD,0.3130287648054145,"4
Method
149"
METHOD,0.3147208121827411,"The motivating example suggests that NE learning can be largely accelerated by running MARL
150"
METHOD,0.3164128595600677,"algorithms in a smart order over states. Inspired by this insight, we present a general framework to
151"
METHOD,0.31810490693739424,"accelerate NE learning in zero-sum games by training over a curriculum of subgames. We further
152"
METHOD,0.3197969543147208,"propose two practical techniques to instantiate the framework and present the overall algorithm.
153"
SUBGAME CURRICULUM LEARNING,0.32148900169204736,"4.1
Subgame curriculum learning
154"
SUBGAME CURRICULUM LEARNING,0.32318104906937395,"The key issue of the standard sample-and-update framework is that the rollout trajectories always start
155"
SUBGAME CURRICULUM LEARNING,0.3248730964467005,"from the ﬁxed initial state distribution ⇢, so visiting states that are most critical for efﬁcient learning
156"
SUBGAME CURRICULUM LEARNING,0.32656514382402707,"can consume a large number of samples. To accelerate training, we can directly reset the environment
157"
SUBGAME CURRICULUM LEARNING,0.32825719120135366,"to those critical states. Suppose we have an oracle state sampler oracle(·) that can initiate suitable
158"
SUBGAME CURRICULUM LEARNING,0.3299492385786802,"states for the current policy to learn, i.e., generate appropriate induced subgames, we can derive a
159"
SUBGAME CURRICULUM LEARNING,0.3316412859560068,"general-purpose framework in Alg. 1, which we call subgame curriculum learning. Note that this
160"
SUBGAME CURRICULUM LEARNING,0.3333333333333333,"framework is compatible with any MARL algorithm for zero-sum Markov games.
161"
SUBGAME CURRICULUM LEARNING,0.3350253807106599,"A desirable feature of subgame curriculum learning is that it does not change the convergence property
162"
SUBGAME CURRICULUM LEARNING,0.33671742808798644,"of the backbone MARL algorithm, as discussed below.
163"
SUBGAME CURRICULUM LEARNING,0.338409475465313,"Proposition 1. If all initial states s0 with ⇢(s0) > 0 are sampled inﬁnitely often, and the backbone
164"
SUBGAME CURRICULUM LEARNING,0.3401015228426396,"MARL algorithm is guaranteed to converge to an NE in zero-sum Markov games, then subgame
165"
SUBGAME CURRICULUM LEARNING,0.34179357021996615,"curriculum learning also produces an NE of the original Markov game.
166"
SUBGAME CURRICULUM LEARNING,0.34348561759729274,"The proof can be found in Appendix A.2. Note that such a requirement is easy to satisfy. For example,
167"
SUBGAME CURRICULUM LEARNING,0.34517766497461927,"given any state sampler oracle(·), we can construct a valid mixed sampler by sampling from oracle(·)
168"
SUBGAME CURRICULUM LEARNING,0.34686971235194586,"for probability 0 < p < 1 and sampling from ⇢for probability 1 −p.
169"
SUBGAME CURRICULUM LEARNING,0.34856175972927245,"Remark. With a given state sampler, the only requirement of our subgame curriculum learning
170"
SUBGAME CURRICULUM LEARNING,0.350253807106599,"framework is that the environment can be reset to a desired state to generate the induced game. This
171"
SUBGAME CURRICULUM LEARNING,0.35194585448392557,"is a standard assumption in the curriculum learning literature [13, 29, 35] and is feasible in many RL
172"
SUBGAME CURRICULUM LEARNING,0.3536379018612521,"environments. For environments that do not support this feature, we can simply reimplement the reset
173"
SUBGAME CURRICULUM LEARNING,0.3553299492385787,"function to make them compatible with our framework.
174"
SUBGAME SAMPLING METRIC,0.3570219966159052,"4.2
Subgame sampling metric
175"
SUBGAME SAMPLING METRIC,0.3587140439932318,"A key question is how to instantiate the oracle sampler, i.e., which subgame should we train on for
176"
SUBGAME SAMPLING METRIC,0.3604060913705584,"faster convergence? Intuitively, for a particular state s, if its value has converged to the NE value,
177"
SUBGAME SAMPLING METRIC,0.36209813874788493,"that is, Vi(s) = V ⇤"
SUBGAME SAMPLING METRIC,0.3637901861252115,"i (s), we should no longer train on the subgame induced by it. By contrast, if the
178"
SUBGAME SAMPLING METRIC,0.36548223350253806,"gap between its current value and the NE value is substantial, we should probably train more on the
179"
SUBGAME SAMPLING METRIC,0.36717428087986465,"induced subgame. Thus, a simple way is to use the squared difference of the current value and the
180"
SUBGAME SAMPLING METRIC,0.3688663282571912,"NE value as the weight for a state and sample states with probabilities proportional to the weights.
181"
SUBGAME SAMPLING METRIC,0.37055837563451777,"Concretely, the state weight can be written as
182"
SUBGAME SAMPLING METRIC,0.37225042301184436,w(s) = 1 2
X,0.3739424703891709,"2
X i=1 (V ⇤"
X,0.3756345177664975,"i (s) −Vi(s))2
(7) = Ei ⇥ (V ⇤"
X,0.377326565143824,1 (s) −˜Vi(s))2⇤ (8) = Ei ⇥ V ⇤
X,0.3790186125211506,1 (s) −˜Vi(s)
X,0.38071065989847713,⇤2 + Vari ⇥ V ⇤
X,0.3824027072758037,"1 (s) −˜Vi(s) ⇤ ,
(9)"
X,0.3840947546531303,"where ˜V1(s) = V1(s) and ˜V2(s) = −V2(s). The second equality holds because the game is zero-sum
183"
X,0.38578680203045684,and V ⇤
X,0.38747884940778343,2 (s) = −V ⇤
X,0.38917089678510997,"1 (s). With random initialization and different training samples, { ˜Vi}2"
X,0.39086294416243655,"i=1 can be
184"
X,0.3925549915397631,"regarded as an ensemble of two value functions and the weight w(s) becomes the expectation over the
185"
X,0.3942470389170897,"ensemble. The last equality further expands the expectation to a bias term and a variance term, and
186"
X,0.39593908629441626,we sample state with probability P(s) = w(s)/ P
X,0.3976311336717428,"s0 w(s0). For the motivating example of RPS(n)
187"
X,0.3993231810490694,"game, the NE value decreases exponentially from the last state sn−1 to the initial state s0. With value
188"
X,0.4010152284263959,"functions initialized close to zero, the prioritized subgames throughout training will move gradually
189"
X,0.4027072758037225,"from the last round to the ﬁrst round, which is approximately the optimal order.
190"
X,0.40439932318104904,"However, Eq. (9) is very hard to compute in practice because the NE value is generally unknown.
191"
X,0.40609137055837563,"Inspired by Eq. (9), we propose the following alternative state weight
192"
X,0.4077834179357022,˜w(s) = ↵· Ei
X,0.40947546531302875,⇥˜V (t)
X,0.41116751269035534,"i
(s) −˜V (t−1) i
(s)"
X,0.4128595600676819,⇤2 + Vari
X,0.41455160744500846,⇥˜Vi(s) ⇤
X,0.41624365482233505,",
(10)"
X,0.4179357021996616,"which takes a hyperparameter ↵and uses the difference between two consecutive value function
193"
X,0.4196277495769882,"checkpoints instead of the difference between the NE value and the current value in Eq. (9). The ﬁrst
194"
X,0.4213197969543147,"term in Eq. (10) measures how fast the value functions change over time. If this term is large, the
195"
X,0.4230118443316413,"value functions are changing constantly and still far from the NE value; if this term is marginal, the
196"
X,0.42470389170896783,"value functions are probably close to the converged NE value. The second term in Eq. (10) measures
197"
X,0.4263959390862944,"Algorithm 2: Subgame Automatic Curriculum Learning (SACL)
Input: state buffers M with capacity K, probability p to sample initial state from the state buffer.
Randomly initialize policy ⇡i and value function Vi for player i = 1, 2;
repeat V 0"
X,0.428087986463621,"i  Vi, i = 1, 2;
// Select subgame and train policy."
X,0.42978003384094754,"Sample s0 ⇠sampler(M) with probability p, else s0 ⇠⇢(·);
Rollout in MG(s0) and train {⇡i, Vi}2"
X,0.43147208121827413,"i=1 via MARL;
// Compute weight by Eq. (10) and update state buffer."
X,0.43316412859560066,˜wt  ↵· E[ ˜Vi(st) −˜V 0
X,0.43485617597292725,i (st)]2 + Var({ ˜Vi(st)}2
X,0.4365482233502538,"i=1), t = 0, · · · , T;
M  M [ {(st, ˜wt)}T"
X,0.43824027072758037,"t=0;
if kMk > K then"
X,0.43993231810490696,"M  FPS(M, K);"
X,0.4416243654822335,"until (⇡1, ⇡2) converges;
Output: ﬁnal policy (⇡1, ⇡2)."
X,0.4433164128595601,"the uncertainty of the current learned values and is the same as the variance term in Eq. (9) because
198 V ⇤"
X,0.4450084602368866,"1 (s) is a constant. If ↵= 1, Eq. (10) approximates Eq. (9) as t increases. It is also possible to
199"
X,0.4467005076142132,"train an ensemble of value functions for each player to further improve the empirical performance.
200"
X,0.44839255499153974,"Additional analysis can be found in Appendix A.3.
201"
X,0.4500846023688663,"Since Eq. (10) does not require the unknown NE value to compute, it can be used in practice as
202"
X,0.4517766497461929,"the weight for state sampling and can be implemented for most MARL algorithms. By selecting
203"
X,0.45346869712351945,"states with fast value change and high uncertainty, our framework prioritizes subgames where agents’
204"
X,0.45516074450084604,"performance can quickly improve through learning.
205"
PARTICLE-BASED SUBGAME SAMPLER,0.45685279187817257,"4.3
Particle-based subgame sampler
206"
PARTICLE-BASED SUBGAME SAMPLER,0.45854483925549916,"With the sample weight at hand, we can generate subgames by sampling initial states from the state
207"
PARTICLE-BASED SUBGAME SAMPLER,0.4602368866328257,"space. But it is impractical to sample from the entire space which is usually unavailable and can be
208"
PARTICLE-BASED SUBGAME SAMPLER,0.4619289340101523,"exponentially large for complex games. Typical solutions include training a generative adversarial
209"
PARTICLE-BASED SUBGAME SAMPLER,0.46362098138747887,"network (GAN) [11] or using a parametric Gaussian mixture model (GMM) [35] to generate states
210"
PARTICLE-BASED SUBGAME SAMPLER,0.4653130287648054,"for automatic curriculum learning. However, parametric models require a large number of samples
211"
PARTICLE-BASED SUBGAME SAMPLER,0.467005076142132,"to ﬁt accurately and cannot adapt instantly to the ever-changing weight in our case. Moreover, the
212"
PARTICLE-BASED SUBGAME SAMPLER,0.4686971235194585,"distribution of weights is highly multi-modal, which is hard to capture for many generative models.
213"
PARTICLE-BASED SUBGAME SAMPLER,0.4703891708967851,"We instead adopt a particle-based approach and maintain a large state buffer M using all visited states
214"
PARTICLE-BASED SUBGAME SAMPLER,0.4720812182741117,"throughout training to approximate the state space. Since the size of the buffer is limited while the
215"
PARTICLE-BASED SUBGAME SAMPLER,0.47377326565143824,"state space can be inﬁnitely large, it is important to keep representative samples that are sufﬁciently
216"
PARTICLE-BASED SUBGAME SAMPLER,0.4754653130287648,"far from each other to ensure good coverage of the state space. When the number of states exceeds the
217"
PARTICLE-BASED SUBGAME SAMPLER,0.47715736040609136,"buffer’s capacity K, we use farthest point sampling (FPS) [36] which iteratively selects the farthest
218"
PARTICLE-BASED SUBGAME SAMPLER,0.47884940778341795,"point from the current set of points. In our implementation, we ﬁrst normalize each dimension of the
219"
PARTICLE-BASED SUBGAME SAMPLER,0.4805414551607445,"state and then use the deep graph library package to utilize GPUs for fast and stable FPS results.
220"
OVERALL ALGORITHM,0.48223350253807107,"4.4
Overall algorithm
221"
OVERALL ALGORITHM,0.48392554991539766,"Combining the subgame sampling metric and the particle-based sampler, we present a realization
222"
OVERALL ALGORITHM,0.4856175972927242,"of the subgame curriculum learning framework, i.e., the Subgame Automatic Curriculum Learning
223"
OVERALL ALGORITHM,0.4873096446700508,"(SACL) algorithm, which is summarized in Alg. 2.
224"
OVERALL ALGORITHM,0.4890016920473773,"When each episode resets, we use the particle-based sampler to generate suitable initial states s0
225"
OVERALL ALGORITHM,0.4906937394247039,"for the current policy to learn. To satisfy the requirements in Proposition 1, we also reset the game
226"
OVERALL ALGORITHM,0.49238578680203043,"according to the initial state distribution ⇢(·) with 0.3 probability. After collecting a number of
227"
OVERALL ALGORITHM,0.494077834179357,"samples, we train the policies and value functions using MARL. The weights for the newly collected
228"
OVERALL ALGORITHM,0.4957698815566836,"states are computed according to Eq. (10) and used to update the state buffer M. If the capacity of
229"
OVERALL ALGORITHM,0.49746192893401014,"the state buffer is exceeded, we use FPS to select representative states-weight pairs and delete the
230"
OVERALL ALGORITHM,0.49915397631133673,"others. An overview of SACL in the hide-and-seek game is illustrated in Fig. 3.
231"
OVERALL ALGORITHM,0.5008460236886633,"high
low"
OVERALL ALGORITHM,0.5025380710659898,sample
OVERALL ALGORITHM,0.5042301184433164,weight
OVERALL ALGORITHM,0.505922165820643,initial states
OVERALL ALGORITHM,0.5076142131979695,update
OVERALL ALGORITHM,0.5093062605752962,self-play reset
OVERALL ALGORITHM,0.5109983079526227,"Fort Building
Running and Chasing
Ramp Use
Ramp Defense"
OVERALL ALGORITHM,0.5126903553299492,sample
OVERALL ALGORITHM,0.5143824027072758,weight
OVERALL ALGORITHM,0.5160744500846024,initial states
OVERALL ALGORITHM,0.5177664974619289,update
OVERALL ALGORITHM,0.5194585448392555,self-play reset
OVERALL ALGORITHM,0.5211505922165821,"low
high"
OVERALL ALGORITHM,0.5228426395939086,"agent
agent
agent
environment
agent
agent
agent
environment"
OVERALL ALGORITHM,0.5245346869712352,"Figure 3: Illustration of SACL in the hide-and-seek environment. In the Fort Building stage, the
states with hiders near the box have high weights (red) and agents can easily learn to build a fort
by practicing on these subgames, while the states with randomly spawned hiders have low weights
(green) and contribute less to learning. By sampling initial states with respect to the approximate
squared distance to NE values, agents can proceed to new stages more efﬁciently."
EXPERIMENT,0.5262267343485617,"5
Experiment
232"
EXPERIMENT,0.5279187817258884,"We evaluate SACL in three different zero-sum environments: Multi-Agent Particle Environment
233"
EXPERIMENT,0.5296108291032149,"(MPE) [28], Google Research Football (GRF) [22], and the hide-and-seek (HnS) environment [2].
234"
EXPERIMENT,0.5313028764805414,"We use a state-of-the-art MARL algorithm MAPPO [52] as the backbone in all experiments.
235"
EXPERIMENT,0.5329949238578681,"In zero-sum games, because the performance of one player’s policy depends on the other player’s
236"
EXPERIMENT,0.5346869712351946,"policy, the return curve throughout training is no longer a good evaluation method. One way to
237"
EXPERIMENT,0.5363790186125211,"compare the performance of different policies is to use cross-play, which uses a tournament-style
238"
EXPERIMENT,0.5380710659898477,"match between any two policies and records the results in a payoff matrix. However, due to the
239"
EXPERIMENT,0.5397631133671743,"non-transitivity of many zero-sum games [3], winning other policies does not necessarily mean being
240"
EXPERIMENT,0.5414551607445008,"close to NE policies, so a better way to evaluate the performance of policies is to use exploitability.
241"
EXPERIMENT,0.5431472081218274,"Given a pair of policies (⇡1, ⇡2), the exploitability is deﬁned as
242"
EXPERIMENT,0.544839255499154,"exploitability(⇡1, ⇡2) ="
X,0.5465313028764806,"2
X i=1 max ⇡0 i"
X,0.5482233502538071,Es0⇠⇢(·) h V (⇡0
X,0.5499153976311336,"i,⇡−i)
i
(s0) i"
X,0.5516074450084603,".
(11)"
X,0.5532994923857868,"Exploitability can be roughly interpreted as the “distance” to the joint NE policy. In complex
243"
X,0.5549915397631133,"environments like the ones we use, the exact exploitability cannot be calculated because we cannot
244"
X,0.55668358714044,traverse the policy space to ﬁnd ⇡0
X,0.5583756345177665,"i that maximizes the value. Instead, we compute the approximate
245"
X,0.560067681895093,exploitability by training an approximate best response ˜⇡0
X,0.5617597292724196,"i of the ﬁxed policy ⇡i using MARL.
246"
MAIN RESULTS,0.5634517766497462,"5.1
Main results
247"
MAIN RESULTS,0.5651438240270727,"We ﬁrst compare the performance of SACL in three environments against the following baselines for
248"
MAIN RESULTS,0.5668358714043993,"solving zero-sum games: self-play (SP), two popular variants including Fictitious Self-Play (FSP) [17]
249"
MAIN RESULTS,0.5685279187817259,"and Neural replicator dynamics (NeuRD) [19], and a population-based training method policy-space
250"
MAIN RESULTS,0.5702199661590525,"response oracles (PSRO) [23]. More implementation details can be found in Appendix B.
251"
MAIN RESULTS,0.571912013536379,"Multi-Agent Particle Environment. We consider the predator-prey scenario in MPE, where three
252"
MAIN RESULTS,0.5736040609137056,"slower cooperating predators chase one faster prey in a square space with two obstacles. In the default
253"
MAIN RESULTS,0.5752961082910322,"setting, all agents are spawned uniformly in the square. We also consider a harder setting where the
254"
MAIN RESULTS,0.5769881556683587,"predators are spawned in the top-right corner and the prey is spawned in the bottom-left corner. All
255"
MAIN RESULTS,0.5786802030456852,"algorithms are trained for 40M environment samples and the curves of approximate exploitability
256"
MAIN RESULTS,0.5803722504230119,"w.r.t. sample over three seeds are shown in Fig. 4(a) and 4(b). SACL converges faster and achieves
257"
MAIN RESULTS,0.5820642978003384,"lower exploitability than all baselines in both settings, and its advantage is more obvious in the hard
258"
MAIN RESULTS,0.583756345177665,"scenario. This is because the initial state distribution in corners makes the full game challenging
259"
MAIN RESULTS,0.5854483925549916,"to solve, while SACL generates an adaptive state distribution and learns on increasingly harder
260"
MAIN RESULTS,0.5871404399323181,"subgames to accelerate NE learning. More results and discussions can be found in Appendix C.
261"
MAIN RESULTS,0.5888324873096447,"Google Research Football. We evaluate SACL in three GRF academy scenarios, namely pass and
262"
MAIN RESULTS,0.5905245346869712,"shoot, run pass and shoot, and 3 vs 1 with keeper. In all scenarios, the left team’s agents cooperate
263"
MAIN RESULTS,0.5922165820642978,"(a) MPE: exploitability.
(b) MPE hard: exploitability.
(c) HnS: number of samples."
MAIN RESULTS,0.5939086294416244,"Figure 4: Main experiment results in (a) MPE, (b) MPE hard, and (c) Hide-and-seek."
MAIN RESULTS,0.5956006768189509,"Scenario
SACL
SP
FSP
PSRO
NeuRD"
MAIN RESULTS,0.5972927241962775,"pass and shoot
3.79 (0.87)
4.17 (1.45)
4.73 (2.64)
4.68 (2.46)
9.18 (1.89)
run pass and shoot
4.05 (1.22)
4.45 (1.22)
4.62 (0.02)
8.40 (0.48)
9.27 (0.35)
3 vs 1 with keeper
5.49 (0.93)
7.76 (0.67)
6.23 (1.14)
7.43 (1.49)
8.72 (0.15)"
MAIN RESULTS,0.5989847715736041,Table 1: Approximate exploitability of learned policies in different GRF scenarios.
MAIN RESULTS,0.6006768189509306,"to score a goal and the right team’s agents try to defend them. The ﬁrst two scenarios are trained
264"
MAIN RESULTS,0.6023688663282571,"for 50M environment samples and the last scenario is trained for 100M samples. Table 1 lists the
265"
MAIN RESULTS,0.6040609137055838,"approximate exploitabilities of different methods’ policies over three seeds, and SACL achieves the
266"
MAIN RESULTS,0.6057529610829103,"lowest exploitability. Additional cross-play results and discussions can be found in Appendix C.
267"
MAIN RESULTS,0.6074450084602369,"Hide-and-seek environment. HnS is a challenging zero-sum game with known NE policies, which
268"
MAIN RESULTS,0.6091370558375635,"makes it possible for us to directly evaluate the number of samples used for NE convergence. We
269"
MAIN RESULTS,0.61082910321489,"consider the quadrant scenario where there is a room with a door in the lower right corner. Two
270"
MAIN RESULTS,0.6125211505922166,"hiders, one box, and one ramp are spawned uniformly in the environment, and one seeker is spawned
271"
MAIN RESULTS,0.6142131979695431,"uniformly outside the room. Both the box and the ramp can be moved and locked by agents. The
272"
MAIN RESULTS,0.6159052453468697,"hiders aim to avoid the lines of sight from the seeker while the seeker aims to ﬁnd the hiders.
273"
MAIN RESULTS,0.6175972927241963,"There is a total of four stages of emergent stages in HnS, i.e., Running and Chasing, Fort Building,
274"
MAIN RESULTS,0.6192893401015228,"Ramp Use, and Ramp Defense. As shown in Fig. 4(c), SACL with MAPPO backbone produces all
275"
MAIN RESULTS,0.6209813874788495,"four stages and converges to the NE policy with only 50% the samples of MAPPO with self-play.
276"
MAIN RESULTS,0.622673434856176,"We also visualize the initial state distribution to show how SACL selects appropriate subgames for
277"
MAIN RESULTS,0.6243654822335025,"agents to learn. Fig. 5(a) depicts the distribution of hiders’ position in the Fort Building stage. The
278"
MAIN RESULTS,0.626057529610829,"probabilities of states with hiders inside the room are much higher than states with hiders outside,
279"
MAIN RESULTS,0.6277495769881557,"making it easier for hiders to learn to build a fort with the box. Similarly, the distribution of the
280"
MAIN RESULTS,0.6294416243654822,"seeker’s position in the Ramp Use stage is shown in Fig. 5(b), and the most sampled subgames start
281"
MAIN RESULTS,0.6311336717428088,"from states where the seeker is close to the walls and is likely to use the ramp.
282"
ABLATION STUDY,0.6328257191201354,"5.2
Ablation study
283"
ABLATION STUDY,0.6345177664974619,"We perform ablation studies to examine the effectiveness of the proposed sampling metric and
284"
ABLATION STUDY,0.6362098138747885,"particle-based sampler. All experiments are done in the hard predator-prey scenario of MPE and the
285"
ABLATION STUDY,0.637901861252115,"results are averaged over three seeds. More ablation studies on state buffer size, subgame sample
286"
ABLATION STUDY,0.6395939086294417,"probability, and other hyperparameters can be found in Appendix C.
287"
ABLATION STUDY,0.6412859560067682,"Subgame sampling metric. The sampling metric used in SACL follows Eq. (10) which consists of a
288"
ABLATION STUDY,0.6429780033840947,"bias term and a variance term. We compare it with four other metrics including a uniform metric,
289"
ABLATION STUDY,0.6446700507614214,"a bias-only metric, a variance-only metric, and a temporal difference (TD) error metric. The last
290"
ABLATION STUDY,0.6463620981387479,"metric uses the TD error |δt| = |rt + γV (st+1) −V (st)| as the weight, which can be regarded as an
291"
ABLATION STUDY,0.6480541455160744,"estimation of value uncertainty. The results are shown in Fig. 5(c) and the sampling metric used by
292"
ABLATION STUDY,0.649746192893401,"SACL achieves the best results and outperforms both the bias-only metric and variance-only metric.
293"
ABLATION STUDY,0.6514382402707276,"State generator and buffer update method. We substitute the particle-based sampler with other
294"
ABLATION STUDY,0.6531302876480541,"state generators including using GAN from the work [11] and using GMM from the work [35]. We
295"
ABLATION STUDY,0.6548223350253807,"also replace the FPS buffer update method with a uniform one that randomly keeps states and a
296"
ABLATION STUDY,0.6565143824027073,"greedy one that keeps states with the highest weights. Results in Fig. 5(c) show that our particle-based
297"
ABLATION STUDY,0.6582064297800339,"sampler with FPS update leads to the fastest convergence and lowest exploitability.
298"
ABLATION STUDY,0.6598984771573604,"(a) Fort Building.
(b) Ramp Use.
(c) Ablation on metric.
(d) Ablation on generator."
ABLATION STUDY,0.6615905245346869,Figure 5: Visualization of the state distributions in HnS (a-b) and ablation studies (c-d).
RELATED WORK,0.6632825719120136,"6
Related work
299"
RELATED WORK,0.6649746192893401,"A large number of works achieve faster convergence in zero-sum games by playing against an
300"
RELATED WORK,0.6666666666666666,"increasingly stronger policy. The most popular methods are self-play and its variants [18, 1, 21, 34].
301"
RELATED WORK,0.6683587140439933,"Self-play creates a natural curriculum and leads to emergent complex skills and behaviors [4, 2].
302"
RELATED WORK,0.6700507614213198,"Population-based training like double oracle [31] and policy-space response oracles (PSRO) [23]
303"
RELATED WORK,0.6717428087986463,"extend self-play by training a pool of policies. Some follow-up works further accelerate training by
304"
RELATED WORK,0.6734348561759729,"constructing a smart mixing strategy over the policy pool according to the policy landscape [3, 33,
305"
RELATED WORK,0.6751269035532995,"26, 12]. [30] extends PSRO to extensive-form games by building policy mixtures at all states rather
306"
RELATED WORK,0.676818950930626,"than only the initial states, but it still directly solves the full game starting from some ﬁxed states.
307"
RELATED WORK,0.6785109983079526,"In addition to policy-level curriculum learning methods, other works to accelerate training in zero-
308"
RELATED WORK,0.6802030456852792,"sum games usually adopt heuristics and domain knowledge like the number of agents [27, 49] or
309"
RELATED WORK,0.6818950930626058,"environment speciﬁcations [5, 40, 44]. By contrast, our method automatically generates a curriculum
310"
RELATED WORK,0.6835871404399323,"over subgames without domain knowledge and only requires the environments can be reset to desired
311"
RELATED WORK,0.6852791878172588,"states. Subgame-solving technique [7] is also used in online strategy reﬁnement to improve the
312"
RELATED WORK,0.6869712351945855,"blueprint strategy of a simpliﬁed abstract game. Another closely related work to our method is [9]
313"
RELATED WORK,0.688663282571912,"which combines backward induction with policy learning, but this method requires knowledge of the
314"
RELATED WORK,0.6903553299492385,"game topology and can only be applied to ﬁnite-horizon Markov games.
315"
RELATED WORK,0.6920473773265652,"Besides zero-sum games, curriculum learning is also studied in cooperative settings. The problem
316"
RELATED WORK,0.6937394247038917,"is often formalized as goal-conditioned RL where the agents need to reach a speciﬁc goal in each
317"
RELATED WORK,0.6954314720812182,"episode. Curriculum learning methods design or train a smart sampler to generate proper task
318"
RELATED WORK,0.6971235194585449,"conﬁgurations or goals that are most suitable for training advances w.r.t. some progression metric [10,
319"
RELATED WORK,0.6988155668358714,"14, 13, 37, 29, 35, 11]. Such a metric typically relies on an explicit signal, such as the goal-reaching
320"
RELATED WORK,0.700507614213198,"reward, success rates, or the expected value of the testing tasks. However, in the setting of zero-sum
321"
RELATED WORK,0.7021996615905245,"games, these explicit progression metrics become no longer valid since the value associated with
322"
RELATED WORK,0.7038917089678511,"a Nash equilibrium can be arbitrary. A possible implicit metric is value disagreement [53] used in
323"
RELATED WORK,0.7055837563451777,"goal-reaching tasks, which can be regarded as the variance term in our metric. By adding a bias term,
324"
RELATED WORK,0.7072758037225042,"our metric approximates the squared distance to NE values and gives better results in ablation studies.
325"
RELATED WORK,0.7089678510998308,"Our work adopts a non-parametric subgame sampler which is fast to learn and naturally multi-modal,
326"
RELATED WORK,0.7106598984771574,"instead of training an expensive deep generative model like GAN [13]. Such an idea has been
327"
RELATED WORK,0.7123519458544839,"recently popularized in the literature. Some representative samplers are Gaussian mixture model [50],
328"
RELATED WORK,0.7140439932318104,"Stein variational inference [8], Gaussian process [32], or simply evolutionary computation [47, 48].
329"
RELATED WORK,0.7157360406091371,"Technically, our method is also related to prioritized experience replay [38, 14, 24] with the difference
330"
RELATED WORK,0.7174280879864636,"that we maintain a buffer [50] to approximate the uniform distribution over the state space.
331"
CONCLUSION,0.7191201353637902,"7
Conclusion
332"
CONCLUSION,0.7208121827411168,"We present SACL, a general algorithm for accelerating MARL training in zero-sum Markov games
333"
CONCLUSION,0.7225042301184433,"based on the subgame curriculum learning framework. We propose to use the approximate squared
334"
CONCLUSION,0.7241962774957699,"distance to NE values as the sampling metric and use a particle-based sampler for subgames generation.
335"
CONCLUSION,0.7258883248730964,"Instead of starting from the ﬁxed initial states, RL agents trained with SACL can practice more on
336"
CONCLUSION,0.727580372250423,"subgames that are most suitable for the current policy to learn, thus boosting training efﬁciency. We
337"
CONCLUSION,0.7292724196277496,"report appealing experiment results that SACL efﬁciently discovers all emergent strategies in the
338"
CONCLUSION,0.7309644670050761,"challenging hide-and-seek environment and uses only half the samples of MAPPO with self-play. We
339"
CONCLUSION,0.7326565143824028,"hope SACL can be helpful to speed up prototype development and help make MARL training on
340"
CONCLUSION,0.7343485617597293,"complex zero-sum games more affordable to the community.
341"
REFERENCES,0.7360406091370558,"References
342"
REFERENCES,0.7377326565143824,"[1] Yu Bai, Chi Jin, and Tiancheng Yu.
Near-optimal reinforcement learning with self-play.
343"
REFERENCES,0.739424703891709,"Advances in neural information processing systems, 33:2159–2170, 2020.
344"
REFERENCES,0.7411167512690355,"[2] Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, and
345"
REFERENCES,0.7428087986463621,"Igor Mordatch. Emergent tool use from multi-agent autocurricula. In International Conference
346"
REFERENCES,0.7445008460236887,"on Learning Representations, 2020.
347"
REFERENCES,0.7461928934010152,"[3] David Balduzzi, Marta Garnelo, Yoram Bachrach, Wojciech Czarnecki, Julien Perolat, Max
348"
REFERENCES,0.7478849407783418,"Jaderberg, and Thore Graepel. Open-ended learning in symmetric zero-sum games. In Interna-
349"
REFERENCES,0.7495769881556683,"tional Conference on Machine Learning, pages 434–443. PMLR, 2019.
350"
REFERENCES,0.751269035532995,"[4] Trapit Bansal, Jakub Pachocki, Szymon Sidor, Ilya Sutskever, and Igor Mordatch. Emergent
351"
REFERENCES,0.7529610829103215,"complexity via multi-agent competition. In International Conference on Learning Representa-
352"
REFERENCES,0.754653130287648,"tions, 2018.
353"
REFERENCES,0.7563451776649747,"[5] Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław Debiak, Christy
354"
REFERENCES,0.7580372250423012,"Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large
355"
REFERENCES,0.7597292724196277,"scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.
356"
REFERENCES,0.7614213197969543,"[6] Noam Brown, Adam Lerer, Sam Gross, and Tuomas Sandholm. Deep counterfactual regret
357"
REFERENCES,0.7631133671742809,"minimization. In International conference on machine learning, pages 793–802. PMLR, 2019.
358"
REFERENCES,0.7648054145516074,"[7] Noam Brown and Tuomas Sandholm.
Safe and nested subgame solving for imperfect-
359"
REFERENCES,0.766497461928934,"information games. Advances in neural information processing systems, 30, 2017.
360"
REFERENCES,0.7681895093062606,"[8] Jiayu Chen, Yuanxin Zhang, Yuanfan Xu, Huimin Ma, Huazhong Yang, Jiaming Song, Yu Wang,
361"
REFERENCES,0.7698815566835872,"and Yi Wu. Variational automatic curriculum learning for sparse-reward cooperative multi-agent
362"
REFERENCES,0.7715736040609137,"problems. Advances in Neural Information Processing Systems, 34:9681–9693, 2021.
363"
REFERENCES,0.7732656514382402,"[9] Weizhe Chen, Zihan Zhou, Yi Wu, and Fei Fang. Temporal induced self-play for stochastic
364"
REFERENCES,0.7749576988155669,"bayesian games. arXiv preprint arXiv:2108.09444, 2021.
365"
REFERENCES,0.7766497461928934,"[10] Xi Chen, Diederik P Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya
366"
REFERENCES,0.7783417935702199,"Sutskever, and Pieter Abbeel. Variational lossy autoencoder. arXiv preprint arXiv:1611.02731,
367"
REFERENCES,0.7800338409475466,"November 2016.
368"
REFERENCES,0.7817258883248731,"[11] Patrick Dendorfer, Aljosa Osep, and Laura Leal-Taixé. Goal-gan: Multimodal trajectory
369"
REFERENCES,0.7834179357021996,"prediction based on goal position estimation. In Proceedings of the Asian Conference on
370"
REFERENCES,0.7851099830795262,"Computer Vision, 2020.
371"
REFERENCES,0.7868020304568528,"[12] Xidong Feng, Oliver Slumbers, Ziyu Wan, Bo Liu, Stephen McAleer, Ying Wen, Jun Wang,
372"
REFERENCES,0.7884940778341794,"and Yaodong Yang. Neural auto-curricula in two-player zero-sum games. Advances in Neural
373"
REFERENCES,0.7901861252115059,"Information Processing Systems, 34:3504–3517, 2021.
374"
REFERENCES,0.7918781725888325,"[13] Carlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel. Automatic goal generation
375"
REFERENCES,0.7935702199661591,"for reinforcement learning agents. In International conference on machine learning, pages
376"
REFERENCES,0.7952622673434856,"1515–1528. PMLR, 2018.
377"
REFERENCES,0.7969543147208121,"[14] Carlos Florensa, David Held, Markus Wulfmeier, Michael Zhang, and Pieter Abbeel. Reverse
378"
REFERENCES,0.7986463620981388,"curriculum generation for reinforcement learning. In Conference on robot learning, pages
379"
REFERENCES,0.8003384094754653,"482–495. PMLR, 2017.
380"
REFERENCES,0.8020304568527918,"[15] Yoav Freund and Robert E Schapire.
Game theory, on-line prediction and boosting.
In
381"
REFERENCES,0.8037225042301185,"Proceedings of the ninth annual conference on Computational learning theory, pages 325–332,
382"
REFERENCES,0.805414551607445,"1996.
383"
REFERENCES,0.8071065989847716,"[16] Audr¯unas Gruslys, Marc Lanctot, Rémi Munos, Finbarr Timbers, Martin Schmid, Julien Perolat,
384"
REFERENCES,0.8087986463620981,"Dustin Morrill, Vinicius Zambaldi, Jean-Baptiste Lespiau, John Schultz, et al. The advantage
385"
REFERENCES,0.8104906937394247,"regret-matching actor-critic. arXiv preprint arXiv:2008.12234, 2020.
386"
REFERENCES,0.8121827411167513,"[17] Johannes Heinrich, Marc Lanctot, and David Silver. Fictitious self-play in extensive-form
387"
REFERENCES,0.8138747884940778,"games. In International conference on machine learning, pages 805–813. PMLR, 2015.
388"
REFERENCES,0.8155668358714044,"[18] Johannes Heinrich and David Silver. Deep reinforcement learning from self-play in imperfect-
389"
REFERENCES,0.817258883248731,"information games. arXiv preprint arXiv:1603.01121, 2016.
390"
REFERENCES,0.8189509306260575,"[19] Daniel Hennes, Dustin Morrill, Shayegan Omidshaﬁei, Rémi Munos, Julien Perolat, Marc
391"
REFERENCES,0.8206429780033841,"Lanctot, Audrunas Gruslys, Jean-Baptiste Lespiau, Paavo Parmas, Edgar Duéñez-Guzmán, et al.
392"
REFERENCES,0.8223350253807107,"Neural replicator dynamics: Multiagent learning via hedging policy gradients. In Proceedings
393"
REFERENCES,0.8240270727580372,"of the 19th International Conference on Autonomous Agents and MultiAgent Systems, pages
394"
REFERENCES,0.8257191201353637,"492–501, 2020.
395"
REFERENCES,0.8274111675126904,"[20] Max Jaderberg, Wojciech M Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia
396"
REFERENCES,0.8291032148900169,"Castaneda, Charles Beattie, Neil C Rabinowitz, Ari S Morcos, Avraham Ruderman, Nicolas
397"
REFERENCES,0.8307952622673435,"Sonnerat, Tim Green, Louise Deason, Joel Z Leibo, David Silver, Demis Hassabis, Koray
398"
REFERENCES,0.8324873096446701,"Kavukcuoglu, and Thore Graepel. Human-level performance in ﬁrst-person multiplayer games
399"
REFERENCES,0.8341793570219966,"with population-based deep reinforcement learning. arXiv preprint arXiv:1807.01281, July
400"
REFERENCES,0.8358714043993232,"2018.
401"
REFERENCES,0.8375634517766497,"[21] Chi Jin, Qinghua Liu, Yuanhao Wang, and Tiancheng Yu. V-learning–a simple, efﬁcient,
402"
REFERENCES,0.8392554991539763,"decentralized algorithm for multiagent rl. arXiv preprint arXiv:2110.14555, 2021.
403"
REFERENCES,0.8409475465313029,"[22] Karol Kurach, Anton Raichuk, Piotr Sta´nczyk, Michał Zaj ˛ac, Olivier Bachem, Lasse Espeholt,
404"
REFERENCES,0.8426395939086294,"Carlos Riquelme, Damien Vincent, Marcin Michalski, Olivier Bousquet, et al. Google research
405"
REFERENCES,0.8443316412859561,"football: A novel reinforcement learning environment. In Proceedings of the AAAI Conference
406"
REFERENCES,0.8460236886632826,"on Artiﬁcial Intelligence, volume 34, pages 4501–4510, 2020.
407"
REFERENCES,0.8477157360406091,"[23] Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien
408"
REFERENCES,0.8494077834179357,"Pérolat, David Silver, and Thore Graepel. A uniﬁed game-theoretic approach to multiagent
409"
REFERENCES,0.8510998307952623,"reinforcement learning. Advances in neural information processing systems, 30, 2017.
410"
REFERENCES,0.8527918781725888,"[24] Yunfei Li, Tao Kong, Lei Li, and Yi Wu. Learning design and construction with varying-sized
411"
REFERENCES,0.8544839255499154,"materials via prioritized memory resets. In 2022 International Conference on Robotics and
412"
REFERENCES,0.856175972927242,"Automation (ICRA), pages 7469–7476, 2022.
413"
REFERENCES,0.8578680203045685,"[25] Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In
414"
REFERENCES,0.8595600676818951,"Proceedings of the eleventh international conference on machine learning, volume 157, pages
415"
REFERENCES,0.8612521150592216,"157–163, 1994.
416"
REFERENCES,0.8629441624365483,"[26] Xiangyu Liu, Hangtian Jia, Ying Wen, Yujing Hu, Yingfeng Chen, Changjie Fan, Zhipeng
417"
REFERENCES,0.8646362098138748,"Hu, and Yaodong Yang. Towards unifying behavioral and response diversity for open-ended
418"
REFERENCES,0.8663282571912013,"learning in zero-sum games. Advances in Neural Information Processing Systems, 34:941–952,
419"
REFERENCES,0.868020304568528,"2021.
420"
REFERENCES,0.8697123519458545,"[27] Qian Long, Zihan Zhou, Abhinav Gupta, Fei Fang, Yi Wu, and Xiaolong Wang. Evolution-
421"
REFERENCES,0.871404399323181,"ary population curriculum for scaling multi-agent reinforcement learning. In International
422"
REFERENCES,0.8730964467005076,"Conference on Learning Representations, 2020.
423"
REFERENCES,0.8747884940778342,"[28] Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-
424"
REFERENCES,0.8764805414551607,"critic for mixed cooperative-competitive environments. In Proceedings of the 31st International
425"
REFERENCES,0.8781725888324873,"Conference on Neural Information Processing Systems, 2017.
426"
REFERENCES,0.8798646362098139,"[29] Tambet Matiisen, Avital Oliver, Taco Cohen, and John Schulman. Teacher-student curriculum
427"
REFERENCES,0.8815566835871405,"learning. IEEE transactions on neural networks and learning systems, 2019.
428"
REFERENCES,0.883248730964467,"[30] Stephen McAleer, John B Lanier, Kevin A Wang, Pierre Baldi, and Roy Fox. Xdo: A double
429"
REFERENCES,0.8849407783417935,"oracle algorithm for extensive-form games. Advances in Neural Information Processing Systems,
430"
REFERENCES,0.8866328257191202,"34:23128–23139, 2021.
431"
REFERENCES,0.8883248730964467,"[31] H Brendan McMahan, Geoffrey J Gordon, and Avrim Blum. Planning in the presence of cost
432"
REFERENCES,0.8900169204737732,"functions controlled by an adversary. In Proceedings of the 20th International Conference on
433"
REFERENCES,0.8917089678510999,"Machine Learning (ICML-03), pages 536–543, 2003.
434"
REFERENCES,0.8934010152284264,"[32] Bhairav Mehta, Manfred Diaz, Florian Golemo, Christopher J Pal, and Liam Paull. Active
435"
REFERENCES,0.8950930626057529,"domain randomization. In Conference on Robot Learning, pages 1162–1176. PMLR, 2020.
436"
REFERENCES,0.8967851099830795,"[33] Nicolas Perez-Nieves, Yaodong Yang, Oliver Slumbers, David H Mguni, Ying Wen, and Jun
437"
REFERENCES,0.8984771573604061,"Wang. Modelling behavioural diversity for learning in open-ended games. In International
438"
REFERENCES,0.9001692047377327,"Conference on Machine Learning, pages 8514–8524. PMLR, 2021.
439"
REFERENCES,0.9018612521150592,"[34] Julien Perolat, Bart de Vylder, Daniel Hennes, Eugene Tarassov, Florian Strub, Vincent de Boer,
440"
REFERENCES,0.9035532994923858,"Paul Muller, Jerome T Connor, Neil Burch, Thomas Anthony, et al. Mastering the game of
441"
REFERENCES,0.9052453468697124,"stratego with model-free multiagent reinforcement learning. arXiv preprint arXiv:2206.15378,
442"
REFERENCES,0.9069373942470389,"2022.
443"
REFERENCES,0.9086294416243654,"[35] Rémy Portelas, Cédric Colas, Katja Hofmann, and Pierre-Yves Oudeyer. Teacher algorithms
444"
REFERENCES,0.9103214890016921,"for curriculum learning of deep rl in continuously parameterized environments. In Conference
445"
REFERENCES,0.9120135363790186,"on Robot Learning, pages 835–853. PMLR, 2020.
446"
REFERENCES,0.9137055837563451,"[36] Charles R. Qi, Li Yi, Hao Su, and Leonidas J. Guibas. Pointnet++: Deep hierarchical feature
447"
REFERENCES,0.9153976311336718,"learning on point sets in a metric space. Advances in Neural Information Processing Systems,
448"
REFERENCES,0.9170896785109983,"30, 2017.
449"
REFERENCES,0.9187817258883249,"[37] Sebastien Racaniere, Andrew K Lampinen, Adam Santoro, David P Reichert, Vlad Firoiu, and
450"
REFERENCES,0.9204737732656514,"Timothy P Lillicrap. Automated curricula through setter-solver interactions. arXiv preprint
451"
REFERENCES,0.922165820642978,"arXiv:1909.12892, 2019.
452"
REFERENCES,0.9238578680203046,"[38] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay.
453"
REFERENCES,0.9255499153976311,"arXiv preprint arXiv:1511.05952, 2015.
454"
REFERENCES,0.9272419627749577,"[39] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal
455"
REFERENCES,0.9289340101522843,"policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
456"
REFERENCES,0.9306260575296108,"[40] Jack Serrino, Max Kleiman-Weiner, David C Parkes, and Josh Tenenbaum. Finding friend and
457"
REFERENCES,0.9323181049069373,"foe in multi-agent games. Advances in Neural Information Processing Systems, 32, 2019.
458"
REFERENCES,0.934010152284264,"[41] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driess-
459"
REFERENCES,0.9357021996615905,"che, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al.
460"
REFERENCES,0.937394247038917,"Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484,
461"
REFERENCES,0.9390862944162437,"2016.
462"
REFERENCES,0.9407783417935702,"[42] Eric Steinberger, Adam Lerer, and Noam Brown. Dream: Deep regret minimization with
463"
REFERENCES,0.9424703891708968,"advantage baselines and model-free learning. arXiv preprint arXiv:2006.10410, 2020.
464"
REFERENCES,0.9441624365482234,"[43] Csaba Szepesvári and Michael L Littman.
A uniﬁed analysis of value-function-based
465"
REFERENCES,0.9458544839255499,"reinforcement-learning algorithms. Neural computation, 11(8):2017–2060, 1999.
466"
REFERENCES,0.9475465313028765,"[44] Zhenggang Tang, Chao Yu, Boyuan Chen, Huazhe Xu, Xiaolong Wang, Fei Fang, Simon
467"
REFERENCES,0.949238578680203,"Du, Yu Wang, and Yi Wu. Discovering diverse multi-agent strategic behavior via reward
468"
REFERENCES,0.9509306260575296,"randomization. arXiv preprint arXiv:2103.04564, 2021.
469"
REFERENCES,0.9526226734348562,"[45] Gerald Tesauro et al. Temporal difference learning and td-gammon. Communications of the
470"
REFERENCES,0.9543147208121827,"ACM, 38(3):58–68, 1995.
471"
REFERENCES,0.9560067681895094,"[46] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Jun-
472"
REFERENCES,0.9576988155668359,"young Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster
473"
REFERENCES,0.9593908629441624,"level in StarCraft II using multi-agent reinforcement learning. Nature, 575(7782):350–354,
474"
REFERENCES,0.961082910321489,"2019.
475"
REFERENCES,0.9627749576988156,"[47] Rui Wang, Joel Lehman, Jeff Clune, and Kenneth O Stanley. Poet: open-ended coevolution of
476"
REFERENCES,0.9644670050761421,"environments and their optimized solutions. In Proceedings of the Genetic and Evolutionary
477"
REFERENCES,0.9661590524534687,"Computation Conference, pages 142–151, 2019.
478"
REFERENCES,0.9678510998307953,"[48] Rui Wang, Joel Lehman, Aditya Rawal, Jiale Zhi, Yulun Li, Jeffrey Clune, and Kenneth Stanley.
479"
REFERENCES,0.9695431472081218,"Enhanced poet: Open-ended reinforcement learning through unbounded invention of learning
480"
REFERENCES,0.9712351945854484,"challenges and their solutions. In International Conference on Machine Learning, pages
481"
REFERENCES,0.9729272419627749,"9940–9951. PMLR, 2020.
482"
REFERENCES,0.9746192893401016,"[49] Weixun Wang, Tianpei Yang, Yong Liu, Jianye Hao, Xiaotian Hao, Yujing Hu, Yingfeng Chen,
483"
REFERENCES,0.9763113367174281,"Changjie Fan, and Yang Gao. From few to more: Large-scale dynamic multiagent curriculum
484"
REFERENCES,0.9780033840947546,"learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pages
485"
REFERENCES,0.9796954314720813,"7293–7300, 2020.
486"
REFERENCES,0.9813874788494078,"[50] David Warde-Farley, Tom Van de Wiele, Tejas Kulkarni, Catalin Ionescu, Steven Hansen, and
487"
REFERENCES,0.9830795262267343,"Volodymyr Mnih. Unsupervised control through non-parametric discriminative rewards. In
488"
REFERENCES,0.9847715736040609,"International Conference on Learning Representations, 2019.
489"
REFERENCES,0.9864636209813875,"[51] Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3):279–292, 1992.
490"
REFERENCES,0.988155668358714,"[52] Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising
491"
REFERENCES,0.9898477157360406,"effectiveness of ppo in cooperative, multi-agent games. arXiv preprint arXiv:2103.01955, 2021.
492"
REFERENCES,0.9915397631133672,"[53] Yunzhi Zhang, Pieter Abbeel, and Lerrel Pinto. Automatic curriculum learning through value
493"
REFERENCES,0.9932318104906938,"disagreement. Advances in Neural Information Processing Systems, 33:7648–7659, 2020.
494"
REFERENCES,0.9949238578680203,"[54] Martin Zinkevich, Michael Johanson, Michael Bowling, and Carmelo Piccione. Regret mini-
495"
REFERENCES,0.9966159052453468,"mization in games with incomplete information. Advances in neural information processing
496"
REFERENCES,0.9983079526226735,"systems, 20, 2007.
497"
