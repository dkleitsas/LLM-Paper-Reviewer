Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.001053740779768177,"Prior work has proposed various reweighting algorithms to improve the worst-
group performance of machine learning models for fairness. However, Sagawa
et al. (2020a) empirically found that these algorithms overﬁt easily in practice un-
der the overparameterized setting, where the number of model parameters is much
greater than the number of samples. In this work, we provide a theoretical back-
ing to the empirical results above, and prove the pessimistic result that reweighting
algorithms always overﬁt. Speciﬁcally we prove that with reweighting, an over-
parameterized model always converges to the same ERM interpolator that ﬁts all
training samples, and consequently its worst-group test performance will drop to
the same level as ERM in the long run. That is, we cannot hope for reweighting
algorithms to converge to a different interpolator than ERM with potentially better
worst-group performance. Then, we analyze whether adding regularization helps
ﬁx the issue, and we prove that for regularization to work, it must be large enough
to prevent the model from achieving small training error. Our results suggest that
large regularization (or early stopping) and data augmentation are necessary for
reweighting algorithms to achieve high worst-group test performance."
INTRODUCTION,0.002107481559536354,"1
INTRODUCTION"
INTRODUCTION,0.003161222339304531,"It has been well established by prior work that overparameterized models, whose number of pa-
rameters is much larger than the number of training samples, can empirically achieve high test
performance on a variety of tasks, in contrast to the theory that models with too many parameters
could have large generalization error."
INTRODUCTION,0.004214963119072708,"This high performance however is on average; a large body of prior work (Hovy & Søgaard, 2015;
Blodgett et al., 2016; Tatman, 2017) showed that these models tend to learn spurious features, such
as learning the background in image classiﬁcation instead of the object, and learning keywords like
“not” in language sentiment analysis instead of really understanding the sentences. Consequently,
these models are unfair, i.e. they fail on certain minority groups (such as positive sentences con-
taining “not”) while still having high average-case performance. To solve this problem, people have
proposed various reweighting algorithms to improve the model’s worst-group performance, such as
upweighting the minority groups or using distributionally robust optimization (DRO) based methods
(Shimodaira, 2000; Hashimoto et al., 2018; Duchi & Namkoong, 2018; Sagawa et al., 2020a)."
INTRODUCTION,0.005268703898840885,"While reweighting algorithms in principle can improve the worst-group performance compared to
vanilla empirical risk minimization (ERM), previous work empirically found that when applied to
modern overparameterized models, these methods could overﬁt very easily, so that they have poor
test worst-group performance. For example, Sagawa et al. (2020a) studied a reweighting algorithm
called group DRO. They found that compared to ERM, group DRO does improve the worst-group
test accuracy by a large margin at the early stage of training. However, if no regularization is
applied, then as training goes on, the worst-group test accuracy of group DRO will drop signiﬁ-
cantly and eventually to a level almost the same as ERM. Some previous work tried to explain why
reweighting algorithms can overﬁt so easily. For instance, Sagawa et al. (2020b) argued that with
these algorithms, an overparameterized model would typically memorize all training samples in the
minority groups while still learning the spurious features from the majority groups."
INTRODUCTION,0.006322444678609062,"In this work, we aim to understand the overﬁtting phenomenon in reweighting algorithms by study-
ing their implicit biases. Speciﬁcally, we prove for a family of overparameterized neural networks"
INTRODUCTION,0.007376185458377239,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.008429926238145416,"that for almost all reweighting algorithms, the model always converges to the same interpolator that
ﬁts all training samples, no matter the reweighting. Since ERM is a special case of such reweight-
ing algorithms (where each sample receives the same weight), this means that the implicit biases
of all reweighting algorithms are equivalent to that of ERM. Consequently, the model trained by
any reweighting algorithm always overﬁts to the ERM interpolator, so we cannot hope for its worst-
group test performance to be better than ERM. In short, reweighting algorithms always overﬁt."
INTRODUCTION,0.009483667017913594,"Given this pessimistic result, we analyze whether regularization can help mitigate overﬁtting, as pro-
posed by Sagawa et al. (2020a). We ﬁnd that a necessary condition for regularization to work is that
it considerably lowers the training performance. Speciﬁcally, we prove that if the overparameterized
model trained by a reweighting algorithm with regularization can still perform almost perfectly on
the training set, then overﬁtting is still inevitable. This explains why in practice we need very large
regularization that prevents the model from achieving nearly zero training error to avoid overﬁtting."
INTRODUCTION,0.01053740779768177,"Our results have two important consequences for practice: (i) We should always use large regular-
ization or early stopping when optimizing for worst-group performance; (ii) We should always try
to obtain more training samples, e.g. with strong data augmentation or semi-supervised learning."
RELATED WORK,0.011591148577449948,"1.1
RELATED WORK"
RELATED WORK,0.012644889357218124,"Group fairness.
Group fairness in machine learning was ﬁrst studied in Hardt et al. (2016) and
Zafar et al. (2017), where they required the model to perform equally well over all groups. Later,
Hashimoto et al. (2018) studied another type of group fairness called Rawlsian max-min fairness
(Rawls, 2001), which does not require equal performance but rather requires high performance on
the worst-off group. The problem we study in this paper is most closely related to Rawlsian max-min
fairness. A large body of recent work in machine learning have studied how to improve this worst-
group performance (Duchi & Namkoong, 2018; Oren et al., 2019; Xu et al., 2020; Liu et al., 2021;
Zhai et al., 2021). Recent work however observe that these approaches, when used with modern
overparameterized models, easily overﬁt (Sagawa et al., 2020a;b). Apart from group fairness, there
are also other notions of fairness, such as individual fairness (Dwork et al., 2012; Zemel et al., 2013)
and counterfactual fairness (Kusner et al., 2017), which we do not study in this work."
RELATED WORK,0.0136986301369863,"Implicit bias under the overparameterized setting.
For overparameterized models, there could
be many model parameters which all minimize the training loss. In such cases, it is of interest to
study the implicit bias of speciﬁc optimization algorithms such as gradient descent i.e. to what train-
ing loss minimizer the model parameters will converge to (Du et al., 2019; Allen-Zhu et al., 2019).
Our results use the NTK formulation of wide neural networks (Jacot et al., 2018), and speciﬁcally
we use linearized neural networks to approximate such wide neural networks following Lee et al.
(2019). There is some criticism of this line of work, e.g. Chizat et al. (2019) argued that inﬁnitely
wide neural networks fall in the “lazy training” regime and results might not be transferable to gen-
eral neural networks. Nonetheless such wide neural networks are being widely studied in recent
years, since they provide considerable insights into the behavior of more general neural networks,
which are typically intractable to analyze otherwise."
PRELIMINARIES,0.014752370916754479,"2
PRELIMINARIES"
PRELIMINARIES,0.015806111696522657,"Consider a data domain X × Y ⊆Rd × R that consists of K groups (subdomains)1, where each
data point belongs to one of the groups2. We assume that the input space X is a subset of the unit
ball of Rd, such that any x ∈X satisﬁes ∥x∥2 ≤1. We are given a training set {(xi, yi)}n
i=1 i.i.d.
sampled from some underlying distribution P over X ×Y. Let the K groups be D1, · · · , DK where
each Di is a subset of X × Y. Let Pk(z) = P(z|z ∈Dk) be the conditional data distribution
over Dk, where z = (x, y). Denote X = (x1, · · · , xn) ∈Rd×n, and Y = (y1, · · · , yn) ∈Rn;
for any function g : X 7→R, we overload notation and use g(X) = (g(x1), · · · , g(xn)) . Let
the loss function be ℓ: Y × Y →[0, 1]. In vanilla training, the goal is to minimize the expected
risk denoted by R(f; P) = Ez∼P [ℓ(f(x), y)], which is done by minimizing the empirical risk
ˆR(f) = 1"
PRELIMINARIES,0.01685985247629083,"n
Pn
i=1 ℓ(f(xi), yi)."
PRELIMINARIES,0.01791359325605901,"1We prove our results for Y ⊆R, but our results can be easily extended to the multi-class scenario Y ⊆Rm.
2This is the non-overlapping setting. There is also the overlapping setting where groups can overlap with
each other. We focus on the non-overlapping setting in this paper."
PRELIMINARIES,0.018967334035827187,Under review as a conference paper at ICLR 2022
PRELIMINARIES,0.020021074815595362,"For tasks requiring high worst-group performance, the goal is to train a model f : X →Y that
performs well over every Pk, which can be achieved by minimizing the worst-group risk deﬁned as"
PRELIMINARIES,0.02107481559536354,"Rmax(f; P) =
max
k=1,··· ,K R(f; Pk) =
max
k=1,··· ,K Ez∼P [ℓ(f(x), y)|z ∈Dk]
(1)"
REWEIGHTING ALGORITHMS,0.022128556375131718,"2.1
REWEIGHTING ALGORITHMS"
REWEIGHTING ALGORITHMS,0.023182297154899896,"Most existing methods that minimize the worst-group risk are reweighting algorithms that assign
each sample with a weight during training and minimize the weighted average risk. At time t, we
assign a weight q(t)
i
to sample zi, and minimize the weighted empirical risk:"
REWEIGHTING ALGORITHMS,0.02423603793466807,"ˆRq(t)(f) = n
X"
REWEIGHTING ALGORITHMS,0.02528977871443625,"i=1
q(t)
i ℓ(f(xi), yi)
(2)"
REWEIGHTING ALGORITHMS,0.026343519494204427,"where q(t) = (q(t)
1 , · · · , q(t)
n ) and q(t)
1
+ · · · + q(t)
n = 1."
REWEIGHTING ALGORITHMS,0.0273972602739726,"A static reweighting algorithm assigns to each zi = (xi, yi) a ﬁxed weight qi that does not change
during training, i.e. q(t)
i
≡qi. A famous example is Importance Weighting (IW, Shimodaira (2000)),
in which if zi ∈Dk and the size of Dk is nk, then qi = (Knk)−1. Under IW, each group has the
same weight, and the reweighted empirical risk is a simple (unweighted) average of the empirical
risk over each group, so that each group has an equal contribution to the overall risk objective. Note
that ERM is also a special case of static reweighting algorithms: by assigning q1 = · · · = qn = 1/n."
REWEIGHTING ALGORITHMS,0.02845100105374078,"On the other hand, in a dynamic reweighting algorithm, q(t) changes with t. Speciﬁcally, it up-
weights samples over which the model has a high risk in order to help the model learn “hard”
samples. A popular dynamic reweighting algorithm is Group DRO (Sagawa et al., 2020a). De-
note the empirical risk over group k by ˆRk(f), and the model at time t by f (t). Group DRO sets
q(t)
i
= g(t)
k /nk for all zi ∈Dk where g(t)
k
is the group weight that is updated by"
REWEIGHTING ALGORITHMS,0.029504741833508957,"g(t)
k
∝g(t−1)
k
exp

ν ˆRk(f (t−1))

(∀k = 1, · · · , K)
(3)"
REWEIGHTING ALGORITHMS,0.030558482613277135,"for some ν > 0, and then normalized so that q(t)
1
+ · · · + q(t)
n
= 1. Sagawa et al. (2020a) proved a
convergence rate theorem (their Proposition 2) showing that in the convex setting, the worst-group
training risk of Group DRO converges to the global minimum with the rate O(t−1/2)."
REWEIGHTING ALGORITHMS,0.03161222339304531,"There are many other reweighting algorithms. Particularly, all variants of DRO and DRO-based
methods like CVaR and χ2-DRO are reweighting algorithms. See Appendix A for more examples."
REWEIGHTING ALGORITHMS CAN EASILY OVERFIT,0.032665964172813484,"2.2
REWEIGHTING ALGORITHMS CAN EASILY OVERFIT"
REWEIGHTING ALGORITHMS CAN EASILY OVERFIT,0.03371970495258166,"In this section, we will empirically demonstrate that while IW and Group DRO can achieve higher
worst-group test performances than ERM at the early stage of training, they can easily overﬁt after
a number of training epochs."
REWEIGHTING ALGORITHMS CAN EASILY OVERFIT,0.03477344573234984,"Following Sagawa et al. (2020a), we conduct the experiment on two datasets: Waterbirds and
CelebA. Each dataset contains a binary confounding variable a and a binary target variable y, di-
viding the dataset into four groups (four combinations of (a, y)). In Waterbirds y is the type of the
bird and a is the background; In CelebA y is whether the person has blond hair and a is whether the
person is male. On each dataset, a model trained by ERM always exhibits a very strong empirical
correlation between y and a, so its performance on one of the groups is extremely poor. The goal
is to make the model perform well on every group. See Appendix C.1 of Sagawa et al. (2020a) for
detailed information of these datasets."
REWEIGHTING ALGORITHMS CAN EASILY OVERFIT,0.03582718651211802,"On each dataset, we use the ResNet18 model as the classiﬁer and optimize it with momentum SGD.
We run each of the three algorithms: ERM, IW and group DRO (GDRO), for 500 epochs on Wa-
terbirds and 200 epochs on CelebA, and plot the average training/test and worst-group (WG) train-
ing/test accuracy curves throughout training in Figure 1. From the plots we can conclude that:"
REWEIGHTING ALGORITHMS CAN EASILY OVERFIT,0.0368809272918862,"• All algorithms can achieve and maintain high average training/test accuracy throughout
training, i.e. there is almost no overﬁtting in the average test accuracy."
REWEIGHTING ALGORITHMS CAN EASILY OVERFIT,0.037934668071654375,Under review as a conference paper at ICLR 2022
REWEIGHTING ALGORITHMS CAN EASILY OVERFIT,0.03898840885142255,"0
100
200
300
400
500
Epochs 0.4 0.6 0.8 1"
REWEIGHTING ALGORITHMS CAN EASILY OVERFIT,0.040042149631190724,Average Training Accuracy
REWEIGHTING ALGORITHMS CAN EASILY OVERFIT,0.0410958904109589,"ERM
IW
GDRO"
REWEIGHTING ALGORITHMS CAN EASILY OVERFIT,0.04214963119072708,(a) Avg train acc.
REWEIGHTING ALGORITHMS CAN EASILY OVERFIT,0.04320337197049526,"0
100
200
300
400
500
Epochs 0.4 0.6 0.8 1"
REWEIGHTING ALGORITHMS CAN EASILY OVERFIT,0.044257112750263436,Average Test Accuracy
REWEIGHTING ALGORITHMS CAN EASILY OVERFIT,0.045310853530031614,"ERM
IW
GDRO"
REWEIGHTING ALGORITHMS CAN EASILY OVERFIT,0.04636459430979979,(b) Avg test acc.
REWEIGHTING ALGORITHMS CAN EASILY OVERFIT,0.04741833508956796,"0
100
200
300
400
500
Epochs 0 0.2 0.4 0.6 0.8 1"
REWEIGHTING ALGORITHMS CAN EASILY OVERFIT,0.04847207586933614,Worst-group Training Accuracy
REWEIGHTING ALGORITHMS CAN EASILY OVERFIT,0.04952581664910432,"ERM
IW
GDRO"
REWEIGHTING ALGORITHMS CAN EASILY OVERFIT,0.0505795574288725,(c) WG train acc.
REWEIGHTING ALGORITHMS CAN EASILY OVERFIT,0.051633298208640675,"0
100
200
300
400
500
Epochs 0 0.2 0.4 0.6 0.8 1"
REWEIGHTING ALGORITHMS CAN EASILY OVERFIT,0.05268703898840885,Worst-group Test Accuracy
REWEIGHTING ALGORITHMS CAN EASILY OVERFIT,0.05374077976817703,"ERM
IW
GDRO"
REWEIGHTING ALGORITHMS CAN EASILY OVERFIT,0.0547945205479452,(d) WG test acc.
REWEIGHTING ALGORITHMS CAN EASILY OVERFIT,0.05584826132771338,"0
50
100
150
200
Epochs 0.8 0.85 0.9 0.95 1"
REWEIGHTING ALGORITHMS CAN EASILY OVERFIT,0.05690200210748156,Average Training Accuracy
REWEIGHTING ALGORITHMS CAN EASILY OVERFIT,0.05795574288724974,"ERM
IW
GDRO"
REWEIGHTING ALGORITHMS CAN EASILY OVERFIT,0.059009483667017915,(e) Avg train acc.
REWEIGHTING ALGORITHMS CAN EASILY OVERFIT,0.06006322444678609,"0
50
100
150
200
Epochs 0.85 0.9 0.95"
REWEIGHTING ALGORITHMS CAN EASILY OVERFIT,0.06111696522655427,Average Test Accuracy
REWEIGHTING ALGORITHMS CAN EASILY OVERFIT,0.06217070600632244,"ERM
IW
GDRO"
REWEIGHTING ALGORITHMS CAN EASILY OVERFIT,0.06322444678609063,(f) Avg test acc.
REWEIGHTING ALGORITHMS CAN EASILY OVERFIT,0.0642781875658588,"0
50
100
150
200
Epochs 0 0.2 0.4 0.6 0.8 1"
REWEIGHTING ALGORITHMS CAN EASILY OVERFIT,0.06533192834562697,Worst-group Training Accuracy
REWEIGHTING ALGORITHMS CAN EASILY OVERFIT,0.06638566912539515,"ERM
IW
GDRO"
REWEIGHTING ALGORITHMS CAN EASILY OVERFIT,0.06743940990516333,(g) WG train acc.
REWEIGHTING ALGORITHMS CAN EASILY OVERFIT,0.0684931506849315,"0
50
100
150
200
Epochs 0.1 0.3 0.5 0.7 0.9"
REWEIGHTING ALGORITHMS CAN EASILY OVERFIT,0.06954689146469968,Worst-group Test Accuracy
REWEIGHTING ALGORITHMS CAN EASILY OVERFIT,0.07060063224446786,"ERM
IW
GDRO"
REWEIGHTING ALGORITHMS CAN EASILY OVERFIT,0.07165437302423604,"(h) WG test acc.
Figure 1: Performances of ERM, IW and Group DRO. First row: Waterbirds. Second row: CelebA."
REWEIGHTING ALGORITHMS CAN EASILY OVERFIT,0.07270811380400422,"• Regarding the worst-group test accuracy, while the two reweighting algorithms outperform
ERM by a large margin at the early epochs, they overﬁt very quickly. On CelebA after
roughly 100 epochs, the worst-group test accuracies of the two reweighting algorithms
become the same as ERM. On Waterbirds, the worst-group test performances of IW and
Group DRO drop signiﬁcantly after around 30 epochs though they are still better than ERM."
IMPLICIT BIASES OF REWEIGHTING ALGORITHMS,0.0737618545837724,"3
IMPLICIT BIASES OF REWEIGHTING ALGORITHMS"
IMPLICIT BIASES OF REWEIGHTING ALGORITHMS,0.07481559536354057,"In the previous section, we empirically demonstrated that the worst-group test performances of
reweighting algorithms converge to the same level as ERM. To theoretically understand why this
happens in practice, we analyze the implicit biases of reweighting algorithms. Our main theorem
(Theorem 6) states that almost all reweighting algorithms (including ERM) have equivalent implicit
biases, in the sense that they converge to the same interpolator. Meanwhile, it is observed in practice
that the ERM interpolator has a poor worst-group test performance. This leads to the pessimistic
result that reweighting algorithms always overﬁt. All proofs can be found in Appendix B."
LINEAR MODELS,0.07586933614330875,"3.1
LINEAR MODELS"
LINEAR MODELS,0.07692307692307693,"We ﬁrst demonstrate this pessimistic result on simple linear models to provide our readers with a
key intuition, and later we will apply this same intuition to neural networks. Let the linear model be
f(x) = ⟨θ, x⟩, where θ ∈Rd. In the overparameterized setting, we have d > n. Consider using the
squared loss ℓ(ˆy, y) = 1"
LINEAR MODELS,0.0779768177028451,"2(ˆy−y)2, and minimizing the weighted empirical risk with gradient descent:"
LINEAR MODELS,0.07903055848261328,"θ(t+1) = θ(t) −η n
X"
LINEAR MODELS,0.08008429926238145,"i=1
q(t)
i ∇θℓ(f (t)(xi), yi)
(4)"
LINEAR MODELS,0.08113804004214963,"where η > 0 is the learning rate. For a linear model with the squared loss, the update rule is"
LINEAR MODELS,0.0821917808219178,"θ(t+1) = θ(t) −η n
X"
LINEAR MODELS,0.08324552160168598,"i=1
q(t)
i xi(f (t)(xi) −yi)
(5)"
LINEAR MODELS,0.08429926238145416,"It is a well known result that under the overparameterization setting where d > n, if x1, · · · , xn are
linearly independent, then with a sufﬁciently small η, a linear model trained by ERM can always
converge to an interpolator which ﬁts all training samples (i.e. θ(t) →θ∗such that ⟨θ∗, xi⟩= yi
for all i). Here the linear independence is necessary, because otherwise in the extreme case where
x1 = x2 but y1 ̸= y2, the model cannot ﬁt (x1, y1) and (x2, y2) simultaneously."
LINEAR MODELS,0.08535300316122234,"In this section, we aim to extend this ERM convergence analysis to general reweighting algorithms.
Our results require the following assumption:"
LINEAR MODELS,0.08640674394099052,"Assumption 1. There exist constants q1, · · · , qn such that for all i, q(t)
i
→qi as t →∞. And
mini qi = q∗> 0."
LINEAR MODELS,0.0874604847207587,Under review as a conference paper at ICLR 2022
LINEAR MODELS,0.08851422550052687,"This assumption avoids the scenario where there is some i such that q(t)
i
≈0 for all t, in which case
the model could never ﬁt zi. Assumption 1 empirically holds for Group DRO on Waterbirds and
CelebA (see Appendix D.2). Under this assumption, we can prove that the model always converges
to an interpolator:"
LINEAR MODELS,0.08956796628029505,"Theorem 1. For any reweighting algorithm satisfying Assumption 1, if x1, · · · , xn are linearly
independent, then there exists an η0 > 0 such that for any η ≤η0, as t →∞, θ(t) converges to
some interpolator θ∗such that for all i, ⟨θ∗, xi⟩= yi."
LINEAR MODELS,0.09062170706006323,"We now make the following key observation regarding the update rule (5): θ(t+1) −θ(t) is a lin-
ear combination of x1, · · · , xn for all t, and thus θ(t) −θ(0) always lies in the linear subspace
span(x1, · · · , xn). Note that this is an n-dimensional linear subspace if x1, · · · , xn are linearly in-
dependent, and by Cramer’s rule, there is exactly one ˜θ in this subspace such that ⟨˜θ +θ(0), xi⟩= yi
for all i, which implies that θ∗= ˜θ + θ(0) is unique. Together with Theorem 1, this leads to:"
LINEAR MODELS,0.0916754478398314,"Theorem 2. If x1, · · · , xn are linearly independent, then there exists η0 > 0 such that for any
reweighting algorithm satisfying Assumption 1, and any η ≤η0, θ(t) converges to the same interpo-
lator θ∗that does not depend on q(t)
i ."
LINEAR MODELS,0.09272918861959958,"Note that ERM is also a reweighting algorithm satisfying Assumption 1. Therefore, we have essen-
tially proved the following result: The implicit bias of any reweighting algorithm satisfying Assump-
tion 1 is equivalent to ERM, so reweighting algorithms always overﬁt3."
LINEAR MODELS,0.09378292939936776,"The key intuition here is that no matter what reweighting algorithm we use, θ(t) −θ(0) always lies
in a low-dimensional subspace, in which the interpolator is unique. Therefore, as long as a model
trained by the algorithm converges to some interpolator, it must converge to that unique interpolator,
which means that the implicit bias of the algorithm is equivalent to ERM."
LINEARIZED NEURAL NETWORKS,0.09483667017913593,"3.2
LINEARIZED NEURAL NETWORKS"
LINEARIZED NEURAL NETWORKS,0.0958904109589041,"Now we prove the same result for neural networks. Of course it would be very hard to prove it
for all neural networks. However, we can prove the result for a family of overparameterized neural
networks that can be approximated by their linearized counterparts Lee et al. (2019). Denote the
neural network at time t by f (t)(x) = f(x; θ(t)) which is parameterized by θ(t) ∈Rp where p is
the number of parameters. The linearized neural network of f (t)(x) is deﬁned as"
LINEARIZED NEURAL NETWORKS,0.09694415173867228,"f (t)
lin (x) = f (0)(x) + ⟨θ(t) −θ(0), ∇θf (0)(x)⟩
(6)"
LINEARIZED NEURAL NETWORKS,0.09799789251844046,"where we use the shorthand ∇θf (0)(x) := ∇θf(x; θ)

θ=θ0. Consider training f (t)
lin (x) via gradient
descent on the reweighted risk (as in (4)) using the squared loss. Given a training set {(xi, yi)}n
i=1,
we can construct a new training set {
 
∇θf (0)(xi), yi −f (0)(xi)

}n
i=1, so that training a linearized
neural network on the original training set is equivalent to training a linear model on the new training
set. Based on this observation, we have the following corollary of Theorem 2:"
LINEARIZED NEURAL NETWORKS,0.09905163329820864,"Corollary 3. If ∇θf (0)(x1), · · · , ∇θf (0)(xn) are linearly independent, then there exists η0 > 0
such that for any reweighting algorithm satisfying Assumption 1, and any η ≤η0, θ(t) converges to
the same interpolator θ∗that does not depend on qi."
LINEARIZED NEURAL NETWORKS,0.10010537407797682,"Here we are still using the key intuition: θ(t) −θ(0) always lies in the n-dimensional linear subspace
span
 
∇θf (0)(x1), · · · , ∇θf (0)(xn)

. By Cramer’s rule, there is a unique interpolator θ∗such that
θ∗−θ(0) ∈span
 
∇θf (0)(x1), · · · , ∇θf (0)(xn)

, and θ(t) always converges to that θ∗. Thus, we
have essentially proved that for linearized neural networks, reweighting algorithms always overﬁt."
LINEARIZED NEURAL NETWORKS,0.101159114857745,"Now let us delve deeper into the training dynamics of a linearized neural network.
Note that
∇θf (t)
lin (X) = ∇θf (0)(X) ∈Rp×n, so the change in the training function value vector is"
LINEARIZED NEURAL NETWORKS,0.10221285563751317,"f (t+1)
lin
(X) −f (t)
lin (X) = −η ∇θf (0)(X)⊤∇θf (0)(X)Q(t) ∇ˆyℓ(f (t)
lin (X), Y )
(7)"
LINEARIZED NEURAL NETWORKS,0.10326659641728135,"3By overﬁt, we are saying that the training error of the model trained by the reweighting algorithm will
converge to zero, but the worst-group test performance will converge to the same low level as ERM."
LINEARIZED NEURAL NETWORKS,0.10432033719704953,Under review as a conference paper at ICLR 2022
LINEARIZED NEURAL NETWORKS,0.1053740779768177,"where Q(t) = diag(q(t)
1 , · · · , q(t)
n ). The function value vector moves along the kernel gradient with
respect to Θ(0)
q(t) = ∇θf (0)(X)⊤∇θf (0)(X)Q(t). Meanwhile, the neural tangent kernel (NTK,"
LINEARIZED NEURAL NETWORKS,0.10642781875658588,"Jacot et al. (2018)) is Θ(0)(x, x′) = ∇θf (0)(x)⊤∇θf (0)(x′) , and the Gram matrix is Θ(0) =
Θ(0)(X, X), so Θ(0)
q(t) = Θ(0)Q(t). We can thus extend our result for gradient descent on linearized
neural networks to a kernel gradient descent algorithm as above."
WIDE FULLY-CONNECTED NEURAL NETWORKS,0.10748155953635406,"3.3
WIDE FULLY-CONNECTED NEURAL NETWORKS"
WIDE FULLY-CONNECTED NEURAL NETWORKS,0.10853530031612224,"Now we prove the result for sufﬁciently wide fully-connected neural networks, which can be ap-
proximated by the linearized neural networks. First we deﬁne a fully-connected neural network
with L hidden layers (we always assume L ≥1 so there is at least one hidden layer). Let hl and
xl be the pre- and post-activation outputs of layer l, and dl be the width of layer l. Let x0 = x and
d0 = d. Deﬁne the neural network as


 
"
WIDE FULLY-CONNECTED NEURAL NETWORKS,0.1095890410958904,hl+1 = W l
WIDE FULLY-CONNECTED NEURAL NETWORKS,0.11064278187565858,"√dl
xl + βbl"
WIDE FULLY-CONNECTED NEURAL NETWORKS,0.11169652265542676,"xl+1 = σ(hl+1)
(l = 0, · · · , L)
(8)"
WIDE FULLY-CONNECTED NEURAL NETWORKS,0.11275026343519494,"where σ is a non-linear activation function, W l ∈Rdl+1×dl and W L ∈R1×dL. The parameters θ
consist of W 0, · · · , W L and b0, · · · , bL (θ is the concatenation of all ﬂattened weights and biases).
The ﬁnal output of the neural network is f(x) = hL+1. And let the neural network be initialized as
(
W l(0)
i,j
∼N(0, 1)"
WIDE FULLY-CONNECTED NEURAL NETWORKS,0.11380400421496312,"bl(0)
j
∼N(0, 1)
(l = 0, · · · , L −1)
and"
WIDE FULLY-CONNECTED NEURAL NETWORKS,0.1148577449947313,"(
W L(0)
i,j
= 0"
WIDE FULLY-CONNECTED NEURAL NETWORKS,0.11591148577449947,"bL(0)
j
∼N(0, 1)
(9)"
WIDE FULLY-CONNECTED NEURAL NETWORKS,0.11696522655426765,We also need the following assumption for our approximation theorem:
WIDE FULLY-CONNECTED NEURAL NETWORKS,0.11801896733403583,"Assumption 2. σ is differentiable everywhere, and both σ and ˙σ are Lipschitz.4"
WIDE FULLY-CONNECTED NEURAL NETWORKS,0.11907270811380401,"Difference from Jacot et al. (2018).
Our initialization (9) is different from the original one in
Jacot et al. (2018) in the last (output) layer. For the output layer, we use the zero initialization
W L(0)
i,j
= 0 instead of the Gaussian initialization W L(0)
i,j
∼N(0, 1). This modiﬁcation enables us
to accurately approximate the neural network with its linearized counterpart (6), as we notice that
the proofs in Lee et al. (2019) (particularly the proofs of their Theorem 2.1 and their Lemma 1 in
Appendix G) are ﬂawed. In Appendix C we will explain what goes wrong in their proofs and how
we manage to ﬁx the proofs with our modiﬁcation."
WIDE FULLY-CONNECTED NEURAL NETWORKS,0.12012644889357219,"For our new initialization, we still have the following NTK theorem:"
WIDE FULLY-CONNECTED NEURAL NETWORKS,0.12118018967334036,"Theorem 4. If σ is Lipschitz and dl →∞for l = 1, · · · , L sequentially, then Θ(0)(x, x′) converges
in probability to a non-degenerated5 deterministic limiting kernel Θ(x, x′)."
WIDE FULLY-CONNECTED NEURAL NETWORKS,0.12223393045310854,"The kernel Gram matrix Θ = Θ(X, X) ∈Rn×n is a positive semi-deﬁnite symmetric matrix.
Denote its largest and smallest eigenvalues by λmax and λmin. Note that Θ is non-degenerated,
so we assume that λmin > 0 (which holds almost surely in the overparameterized setting where
dL ≫n). Then, we can prove the following approximation theorem:"
WIDE FULLY-CONNECTED NEURAL NETWORKS,0.1232876712328767,"Theorem 5 (Approximation Theorem). Let η∗= (λmin + λmax)−1. For a fully-connected neural
network f (t) that satisﬁes Assumption 2 and is trained by any reweighting algorithm satisfying
Assumption 1, let f (t)
lin be its linearized neural network which is trained by the same reweighting
algorithm (i.e. ∀i, t, q(t)
i
are the same for both networks). If d1 = d2 = · · · = dL = ˜d and
λmin > 0, then for any δ > 0, there exists ˜D > 0 and a constant C such that as long as η ≤η∗and
˜d ≥˜D, for any test point x ∈Rd such that ∥x∥2 ≤1, with probability at least 1 −δ over random
initialization,
sup
t≥0"
WIDE FULLY-CONNECTED NEURAL NETWORKS,0.12434141201264488,"f (t)
lin (x) −f (t)(x)
 ≤C ˜d−1/4
(10)"
WIDE FULLY-CONNECTED NEURAL NETWORKS,0.12539515279241306,"4f is Lipschitz if there exists a constant L > 0 such that for any x1, x2, |f(x1)−f(x2)| ≤L ∥x1 −x2∥2.
5Non-degenerated means that Θ(x, x′) depends on x and x′ and is not a constant."
WIDE FULLY-CONNECTED NEURAL NETWORKS,0.12644889357218125,Under review as a conference paper at ICLR 2022
WIDE FULLY-CONNECTED NEURAL NETWORKS,0.12750263435194942,"Remark. We can easily extend this theorem to the case where there exists αl > 0 for each of
l = 2, · · · , L such that dl/d1 →αl and d1 →∞."
WIDE FULLY-CONNECTED NEURAL NETWORKS,0.1285563751317176,"Combining all the above results altogether, we achieve our main theorem:
Theorem 6. Under the conditions of Theorem 5, there exists an η1 > 0 such that if η ≤η1 and
∇θf (0)(x1), · · · , ∇θf (0)(xn) are linearly independent, then as ˜d →∞, for any test point x ∈Rd
such that ∥x∥2 ≤1, with probability close to 1 over random initialization,"
WIDE FULLY-CONNECTED NEURAL NETWORKS,0.12961011591148577,"lim sup
t→∞"
WIDE FULLY-CONNECTED NEURAL NETWORKS,0.13066385669125394,"f (t)(x) −f (t)
ERM(x)
 = O( ˜d−1/4) →0
(11)"
WIDE FULLY-CONNECTED NEURAL NETWORKS,0.13171759747102213,"where f (t) is trained by the reweighting algorithm and f (t)
ERM is trained by ERM."
WIDE FULLY-CONNECTED NEURAL NETWORKS,0.1327713382507903,"The main theorem shows that at any test point x, the gap between the function values of the two
models converges to an inﬁnitely small term, so the worst-group test performance of the reweighting
algorithm will converge to the same level as ERM. Therefore, we have proved that for sufﬁciently
wide fully-connected neural networks, reweighting algorithms always overﬁt."
WIDE FULLY-CONNECTED NEURAL NETWORKS,0.13382507903055849,"Our key intuition tells us that the change in the model parameters always lies in an n-dimensional
subspace. Thus, one possible way to improve the worst-group test performance is to enlarge this
subspace by adding more training samples, e.g. via data augmentation or semi-supervised learning.
However, even if we have more training samples, as long as the model is still overparameterized,
and all ∇θf (0)(xi) are linearly independent, then our result still says that no reweighting algorithm
can do better than ERM in the long run (though the performance of ERM itself might be improved)."
WIDE FULLY-CONNECTED NEURAL NETWORKS,0.13487881981032665,"Moreover, our theoretical results can explain the surprising empirical observation in Sagawa et al.
(2020b) that removing some samples from the majority groups to match the group sizes can some-
times achieve even higher worst-group test performance than reweighting even though it wastes lots
of data (see their Section 6). When training samples are removed, the model will converge to an in-
terpolator of the smaller training set which is different from the interpolator of the original training
set, so there is a chance that the performance of the new interpolator is actually higher."
WIDE FULLY-CONNECTED NEURAL NETWORKS,0.13593256059009484,"4
DOES REGULARIZATION REALLY HELP?"
WIDE FULLY-CONNECTED NEURAL NETWORKS,0.136986301369863,"In the previous section, we proved the pessimistic result that reweighting algorithms always overﬁt,
i.e. in the long run their worst-group test performances always drop to the same level as ERM. And
even if we use strong data augmentation or semi-supervised learning, reweighting algorithms still
cannot outperform ERM if the training set is not sufﬁciently enlarged."
WIDE FULLY-CONNECTED NEURAL NETWORKS,0.1380400421496312,"Sagawa et al. (2020a) proposed to tackle the overﬁtting problem of reweighting algorithms via regu-
larization. In particular, they empirically demonstrated with experiments that large regularization is
required to prevent reweighting algorithms such as group DRO from overﬁtting. With a large regu-
larization, the model can maintain a high test worst-group performance, but it cannot obtain perfect
training accuracy, in contrast to the case where no regularization is applied."
WIDE FULLY-CONNECTED NEURAL NETWORKS,0.13909378292939936,"In this section, we study the necessary conditions for regularization to maintain high worst-group
test performance. Speciﬁcally, we will show that regularization will not work if it is not large enough
to prevent the model from obtaining nearly zero training error. In other words, lowering the training
performance is the key to keeping a high worst-group test performance. Note that the results in this
section do not require Assumption 1, so the results hold for all reweighting algorithms."
THEORETICAL ANALYSIS,0.14014752370916755,"4.1
THEORETICAL ANALYSIS"
THEORETICAL ANALYSIS,0.14120126448893572,"Consider a reweighting algorithm with sample weights q(t)
i . Following Sagawa et al. (2020a), we
consider adding L2 penalty to the weighted empirical risk (2):"
THEORETICAL ANALYSIS,0.1422550052687039,"ˆRµ
q(t)(f) = n
X"
THEORETICAL ANALYSIS,0.14330874604847207,"i=1
q(t)
i ℓ(f(xi), yi) + µ 2"
THEORETICAL ANALYSIS,0.14436248682824027,"θ −θ(0)
2"
THEORETICAL ANALYSIS,0.14541622760800843,"2
(12)"
THEORETICAL ANALYSIS,0.1464699683877766,"Given that sufﬁciently wide neural networks can be approximated by linearized ones, we ﬁrst focus
on linearized neural networks. We will use the subscript “reg” to refer to a regularized model (which"
THEORETICAL ANALYSIS,0.1475237091675448,Under review as a conference paper at ICLR 2022
THEORETICAL ANALYSIS,0.14857744994731295,"is trained trained by minimizing the regularized risk (12)). Let f (t)
linreg be a regularized linearized"
THEORETICAL ANALYSIS,0.14963119072708114,"neural network trained by some reweighting algorithm, and f (t)
linERM be an unregularized linearized
neural network trained by ERM. As before, we consider training the models with gradient descent
under the squared loss ℓ(ˆy, y) = 1"
THEORETICAL ANALYSIS,0.1506849315068493,"2(ˆy −y)2. The following result shows that these two models are
very close if f (t)
linreg can achieve low training error:"
THEORETICAL ANALYSIS,0.1517386722866175,"Theorem 7. If there is a constant M0 > 0 such that
∇θf (0)(x)

2 ≤M0 for all ∥x∥2 ≤1,"
THEORETICAL ANALYSIS,0.15279241306638566,"∇θf (0)(x1), · · · , ∇θf (0)(xn) are linearly independent, and the empirical training risk of f (t)
linreg
satisﬁes
lim sup
t→∞
ˆR(f (t)
linreg) < ϵ,
(13)"
THEORETICAL ANALYSIS,0.15384615384615385,"for some ϵ > 0, then for any test point x such that ∥x∥2 ≤1 we have"
THEORETICAL ANALYSIS,0.15489989462592202,"lim sup
t→∞"
THEORETICAL ANALYSIS,0.1559536354056902,"f (t)
linreg(x) −f (t)
linERM(x)
 = O(√ϵ).
(14)"
THEORETICAL ANALYSIS,0.15700737618545837,"The proof of this theorem also follows the key intuition: we can show that even with the L2 penalty
added, θ(t) −θ(0) is still limited in a low-dimensional subspace. And although we cannot prove
that θ(t) always converges to the ERM interpolator, we can prove that it can get very close to that
interpolator if its training error is very low, so the resulting model is very close to the ERM model."
THEORETICAL ANALYSIS,0.15806111696522657,"Then, we can extend this result to sufﬁciently wide fully-connected neural networks:
Theorem 8. If λmin > 0 and µ > 0, then let η∗= (µ+λmin+λmax)−1. For a wide fully-connected
neural network f (t)
reg deﬁned by (8) and (9) and satisfying Assumption 2, and any reweighting algo-
rithm, if d1 = d2 = · · · = dL = ˜d, η ≤η∗, ∇θf (0)(x1), · · · , ∇θf (0)(xn) are linearly independent,
and the empirical training risk of f (t)
reg satisﬁes"
THEORETICAL ANALYSIS,0.15911485774499473,"lim sup
t→∞
ˆR(f (t)
reg ) < ϵ
(15)"
THEORETICAL ANALYSIS,0.1601685985247629,"for some ϵ > 0, then as ˜d →∞, with probability close to 1 over random initialization, for any test
point x such that ∥x∥2 ≤1 we have"
THEORETICAL ANALYSIS,0.1612223393045311,"lim sup
t→∞"
THEORETICAL ANALYSIS,0.16227608008429925,"f (t)
reg (x) −f (t)
ERM(x)
 = O( ˜d−1/4 + √ϵ) →O(√ϵ)
(16)"
THEORETICAL ANALYSIS,0.16332982086406744,"The result shows that a regularized model trained by any reweighting algorithm will get very close
to an unregularized ERM model at any test point x if the training error of the former is nearly zero.
Thus, regularization only helps when it is large enough to keep the training error of the model away
from zero by a margin."
THEORETICAL ANALYSIS,0.1643835616438356,"Our results explain the empirical observation of Sagawa et al. (2020a) that by using large regular-
ization, the model can maintain a high worst-group test performance, but it cannot achieve perfect
training accuracy. If smaller regularization is applied and the model can achieve nearly perfect
training accuracy, then its worst-group test performance will still signiﬁcantly drop."
EMPIRICAL STUDY,0.1654373024236038,"4.2
EMPIRICAL STUDY"
EMPIRICAL STUDY,0.16649104320337196,"In this section, we validate our theoretical results above with experiments on Waterbirds and CelebA.
We run ERM, IW and group DRO under different levels of weight decay for 500 epochs on Water-
birds and 250 epochs on CelebA. Note that we do not strictly follow our L2 penalty formulation
(12), but we study the L2 weight decay regularization which is most widely used in practice. We re-
peat each experiment ﬁve times with different random seeds and report the 95% conﬁdence interval
of the mean average training and worst-group test accuracies of the last 10 training epochs in Table
1. To compare with early stopping, we also report the mean accuracies of epochs 11-20 with no
regularization in blue. Moreover, we plot the average training and worst-group test accuracy curves
throughout training for IW and Group DRO with one of the random seeds in Figure 2."
EMPIRICAL STUDY,0.16754478398314016,"On both datasets, early stopping achieve the best performances. Particularly, on Waterbirds, there
is no clear sign that regularization could help prevent overﬁtting. When the regularization is small,"
EMPIRICAL STUDY,0.16859852476290832,Under review as a conference paper at ICLR 2022
EMPIRICAL STUDY,0.1696522655426765,"Table 1: Mean average training accuracy and worst-group test accuracy (%) of the last 10 training
epochs of ERM, IW and Group DRO under different levels of weight decay (WD). Each entry is
Average training accuracy / Worst-group test accuracy. Blue entries are mean accuracies of epochs
11-20 with no weight decay. Each experiment is repeated ﬁve times with different random seeds."
EMPIRICAL STUDY,0.17070600632244468,"Dataset
WD
ERM
IW
Group DRO"
EMPIRICAL STUDY,0.17175974710221287,Waterbirds
EMPIRICAL STUDY,0.17281348788198103,"0
100.0 ± 0.0/56.3 ± 1.8
100.0 ± 0.0/67.6 ± 1.1
100.0 ± 0.0/64.5 ± 1.6
(11-20)
(Early stopping)
92.4 ± 0.4/83.7 ± 0.6
92.9 ± 0.4/79.9 ± 2.1
0.05
100.0 ± 0.0/71.0 ± 1.9
100.0 ± 0.0/63.5 ± 2.6
0.1
100.0 ± 0.0/67.7 ± 0.7
100.0 ± 0.0/54.7 ± 2.7
0.15
99.0 ± 0.7/53.7 ± 2.7
99.4 ± 0.6/52.5 ± 2.5
0.2
91.6 ± 2.0/35.9 ± 6.9
94.8 ± 0.9/38.0 ± 7.5"
EMPIRICAL STUDY,0.17386722866174922,CelebA
EMPIRICAL STUDY,0.1749209694415174,"0
99.0 ± 0.2/40.2 ± 5.6
99.4 ± 0.1/42.7 ± 1.7
99.4 ± 0.1/49.5 ± 1.9
(11-20)
(Early stopping)
92.1 ± 0.3/78.2 ± 3.2
90.5 ± 0.5/85.2 ± 1.7
0.01
97.9 ± 0.2/50.0 ± 2.8
96.5 ± 0.5/67.2 ± 1.7
0.03
95.0 ± 0.2/62.8 ± 2.4
88.9 ± 1.1/83.1 ± 2.2
0.1
89.4 ± 2.0/76.0 ± 2.4
75.1 ± 9.5/50.6 ± 15.9"
EMPIRICAL STUDY,0.17597471022128555,"0
50
100
150
200
250
Epochs 0.6 0.7 0.8 0.9 1"
EMPIRICAL STUDY,0.17702845100105374,Average Training Accuracy
EMPIRICAL STUDY,0.1780821917808219,"wd = 0
wd = 0.01
wd = 0.03
wd = 0.1"
EMPIRICAL STUDY,0.1791359325605901,(a) IW: Avg Train Acc.
EMPIRICAL STUDY,0.18018967334035826,"0
50
100
150
200
250
Epochs 0.2 0.4 0.6 0.8 1"
EMPIRICAL STUDY,0.18124341412012646,Worst-group Test Accuracy
EMPIRICAL STUDY,0.18229715489989462,"wd = 0
wd = 0.01
wd = 0.03
wd = 0.1"
EMPIRICAL STUDY,0.1833508956796628,(b) IW: WG Test Acc.
EMPIRICAL STUDY,0.18440463645943098,"0
50
100
150
200
250
Epochs 0 0.2 0.4 0.6 0.8 1"
EMPIRICAL STUDY,0.18545837723919917,Average Training Accuracy
EMPIRICAL STUDY,0.18651211801896733,"wd = 0
wd = 0.01
wd = 0.03
wd = 0.1"
EMPIRICAL STUDY,0.18756585879873552,(c) GDRO: Avg Train Acc.
EMPIRICAL STUDY,0.1886195995785037,"0
50
100
150
200
250
Epochs 0 0.2 0.4 0.6 0.8 1"
EMPIRICAL STUDY,0.18967334035827185,Worst-group Test Accuracy
EMPIRICAL STUDY,0.19072708113804004,"wd = 0
wd = 0.01
wd = 0.03
wd = 0.1"
EMPIRICAL STUDY,0.1917808219178082,(d) GDRO: WG Test Acc.
EMPIRICAL STUDY,0.1928345626975764,"Figure 2: Average training accuracy and worst-group (WG) test accuracy of IW and Group DRO
(GDRO) under different L2 weight decay levels on CelebA."
EMPIRICAL STUDY,0.19388830347734456,"the training accuracy is still 100% and the algorithm continues to overﬁt. However, when the regu-
larization is large enough to lower the training accuracy, the worst-group test accuracy drops more
because the model cannot learn the samples well under such a large regularization. Thus, perhaps
not surprisingly, a lower training performance is only a necessary condition but not sufﬁcient."
EMPIRICAL STUDY,0.19494204425711276,"On CelebA, regularization does help mitigate overﬁtting, but a useful regularization must be large
enough to lower the training accuracy. We observe that Group DRO overﬁts more slowly than IW,
as it still has over 70% worst-group test accuracy after 70 epochs. However, as Figure 2d clearly
shows, its worst-group test accuracy will still drop to the ERM level at 200 epochs. We also notice
that Group DRO requires a smaller regularization than IW: for IW we need the weight decay level
to be as large as 0.1 to achieve a similar performance as early stopping, but for Group DRO it only
needs to be 0.01, and using 0.1 is actually harmful."
EMPIRICAL STUDY,0.19599578503688092,"Overall, we ﬁnd that early stopping achieves a markedly better performance. On the other hand,
using large regularization could result in training instability, as well as a loss in overall performance,
and there may or may not be a small band for the regularization parameter where the worst-group
test performance is better."
CONCLUSION,0.1970495258166491,"5
CONCLUSION"
CONCLUSION,0.19810326659641728,"In this work, we theoretically studied why reweighting algorithms overﬁt in practice by analyzing
their implicit biases. Speciﬁcally, we proved the pessimistic result that reweighting algorithms al-
ways overﬁt. Our proof was based on the key intuition that the change in model parameters always
lies in a low-dimensional subspace, so that even with reweighting, the model still converges to the
same unique interpolator. When regularization is applied, we proved that the regularization must
be large enough to keep the model from achieving nearly zero training error in order to prevent
overﬁtting. We empirically validated our theoretical results on real datasets, and our results can
also explain the empirical observations in previous work. Our results are especially important for
large-scale machine learning tasks, where early stopping is not always possible in order to achieve
high performances. Practitioners shooting for high worst-group performances in those tasks must
be very careful about to what extent overﬁtting affects reweighting algorithms."
CONCLUSION,0.19915700737618547,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.20021074815595363,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.20126448893572182,"To guarantee the reproducibility of all our empirical results, in all our experiments we use a ﬁxed
set of random seeds, and we run some of the experiments twice with the same random seed to
make sure that the outputs are the same. See Appendix D.1 for experiment details. After this
paper is deanonymized, we will provide a GitHub repository that contains all the codes, datasets,
hyperparameters, random seeds, machine speculations and anaconda environment speculations that
are sufﬁcient to exactly reproduce our empirical results."
REFERENCES,0.20231822971549,REFERENCES
REFERENCES,0.20337197049525815,"Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242–252. PMLR, 2019."
REFERENCES,0.20442571127502634,"Su Lin Blodgett, Lisa Green, and Brendan O’Connor. Demographic dialectal variation in social
media: A case study of African-American English. In Proceedings of the 2016 Conference on
Empirical Methods in Natural Language Processing, pp. 1119–1130, Austin, Texas, November
2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1120."
REFERENCES,0.2054794520547945,"L´ena¨ıc Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.
In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett (eds.),
Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019."
REFERENCES,0.2065331928345627,"Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent ﬁnds global
minima of deep neural networks. In International Conference on Machine Learning, pp. 1675–
1685. PMLR, 2019."
REFERENCES,0.20758693361433087,"John Duchi and Hongseok Namkoong. Learning models with uniform performance via distribution-
ally robust optimization. arXiv preprint arXiv:1810.08750, 2018."
REFERENCES,0.20864067439409906,"Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel.
Fairness
through awareness. In Proceedings of the 3rd innovations in theoretical computer science confer-
ence, pp. 214–226, 2012."
REFERENCES,0.20969441517386722,"Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. In D. Lee,
M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information
Processing Systems, volume 29, pp. 3315–3323. Curran Associates, Inc., 2016."
REFERENCES,0.2107481559536354,"Tatsunori Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness with-
out demographics in repeated loss minimization.
In Jennifer Dy and Andreas Krause (eds.),
International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning
Research, pp. 1929–1938, Stockholmsm¨assan, Stockholm Sweden, 10–15 Jul 2018. PMLR."
REFERENCES,0.21180189673340358,"Dirk Hovy and Anders Søgaard. Tagging performance correlates with author age. In Proceedings
of the 53rd annual meeting of the Association for Computational Linguistics and the 7th interna-
tional joint conference on natural language processing (volume 2: Short papers), pp. 483–488,
2015."
REFERENCES,0.21285563751317177,"Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31.
Curran Associates, Inc., 2018."
REFERENCES,0.21390937829293993,"Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. In Advances
in neural information processing systems, pp. 4066–4076, 2017."
REFERENCES,0.21496311907270813,"Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. Advances in neural information processing systems, 32:8572–8583, 2019."
REFERENCES,0.2160168598524763,"Evan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa,
Percy Liang, and Chelsea Finn. Just train twice: Improving group robustness without training
group information. In International Conference on Machine Learning, pp. 6781–6792. PMLR,
2021."
REFERENCES,0.21707060063224448,Under review as a conference paper at ICLR 2022
REFERENCES,0.21812434141201265,"Yonatan Oren, Shiori Sagawa, Tatsunori Hashimoto, and Percy Liang. Distributionally robust lan-
guage modeling. In Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-
guage Processing and the 9th International Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), pp. 4227–4237, Hong Kong, China, November 2019. Association for Com-
putational Linguistics. doi: 10.18653/v1/D19-1432."
REFERENCES,0.2191780821917808,"John Rawls. Justice as fairness: A restatement. Harvard University Press, 2001."
REFERENCES,0.220231822971549,"Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust
neural networks for group shifts: On the importance of regularization for worst-case generaliza-
tion. In International Conference on Learning Representations, 2020a."
REFERENCES,0.22128556375131717,"Shiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and Percy Liang. An investigation of why over-
parameterization exacerbates spurious correlations. In Hal Daum´e III and Aarti Singh (eds.),
Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceed-
ings of Machine Learning Research, pp. 8346–8356. PMLR, 13–18 Jul 2020b."
REFERENCES,0.22233930453108536,"Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-
likelihood function. Journal of statistical planning and inference, 90(2):227–244, 2000."
REFERENCES,0.22339304531085352,"Rachael Tatman. Gender and dialect bias in youtube’s automatic captions. In Proceedings of the
First ACL Workshop on Ethics in Natural Language Processing, pp. 53–59, 2017."
REFERENCES,0.2244467860906217,"Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
arXiv:1011.3027, 2010."
REFERENCES,0.22550052687038988,"Ziyu Xu, Chen Dan, Justin Khim, and Pradeep Ravikumar. Class-weighted classiﬁcation: Trade-
offs and robust approaches. In Hal Daum´e III and Aarti Singh (eds.), Proceedings of the 37th
International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning
Research, pp. 10544–10554. PMLR, 13–18 Jul 2020."
REFERENCES,0.22655426765015807,"Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Fair-
ness beyond disparate treatment & disparate impact: Learning classiﬁcation without disparate
mistreatment. In Proceedings of the 26th international conference on world wide web, pp. 1171–
1180, 2017."
REFERENCES,0.22760800842992623,"Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations.
In International Conference on Machine Learning, pp. 325–333, 2013."
REFERENCES,0.22866174920969443,"Runtian Zhai, Chen Dan, Zico Kolter, and Pradeep Ravikumar. Doro: Distributional and outlier ro-
bust optimization. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International
Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research,
pp. 12345–12355. PMLR, 18–24 Jul 2021."
REFERENCES,0.2297154899894626,Under review as a conference paper at ICLR 2022
REFERENCES,0.23076923076923078,"A
OTHER REWEIGHTING ALGORITHMS"
REFERENCES,0.23182297154899895,"In this section, we will review some other previously proposed reweighting algorithms. First, we
will look at DRO-based methods, where DRO stands for Distributionally Robust Optimization."
REFERENCES,0.2328767123287671,"DRO is designed for tasks with distributional shift, where the training distribution and the test distri-
bution are different, and there are some constraints on the distance between these two distributions
(typically described by a divergence function D). Since the real test distribution is unknown, DRO
minimizes the model’s risk over the worst distribution that satisﬁes the distance constraints, which
is an upper bound of the model’s real test error. Formally speaking, given a training distribution P,
DRO minimizes the expected risk over the worst-case distribution Q in a ball w.r.t. divergence D
around the training distribution P. For group shift problems which require high worst-group per-
formance, Q also needs to be absolutely continuous with respect to P, i.e. Q ≪P. Overall, DRO
minimizes the following expected DRO risk:"
REFERENCES,0.2339304531085353,"RD,ρ(θ; P) = sup
Q≪P
{EQ[ℓ(θ; Z)] : D(Q ∥P) ≤ρ}
(17)"
REFERENCES,0.23498419388830347,"The expected DRO risk is typically minimized in the following way: for each epoch t, we ﬁrst ﬁnd
the worst Q that maximizes EQ[ℓ(θ; Z)] and satisﬁes D(Q ∥P) ≤ρ, Q ≪P, and then minimize
the model’s expected risk over this Q with gradient descent. The rationale behind this algorithm is
the famous Danskin’s Theorem, which says that if F(x) is the maximum of a family of functions,
then its gradient at point x is equal to the gradient of the function that attains the maximum value at
x."
REFERENCES,0.23603793466807166,"Note that in practice we only have a ﬁnite set of training samples {z1, · · · , zn}, so P is always
chosen as the empirical distribution, i.e. uniform distribution over z1, · · · , zn. Then, note that
Q ≪P, which implies that the support of Q must be a subset of the support of P, which is
{z1, · · · , zn}. This means that Q must be a distribution over z1, · · · , zn, i.e. it is a reweighting
over the training samples. Thus, we have essentially showed that DRO is a reweighting algorithm,
and in fact almost all methods based on DRO are reweighting algorithms."
REFERENCES,0.23709167544783982,"Two widely used variants of DRO are CVaR (Conditional Value at Risk) and χ2-DRO. In CVaR, for
a ﬁxed α ∈(0, 1), we let D(Q ∥P) = sup log dQ"
REFERENCES,0.23814541622760801,"dP and ρ = −log α. As a result, suppose that αn
is an integer, then CVaR will assign weight
1
αn to αn training samples that incur the highest losses,
and weight 0 to the rest of the samples, so we can easily see that CVaR is a reweighting algorithm.
χ2-DRO was ﬁrst used in Hashimoto et al. (2018) to deal with fairness tasks where the group labels
are unknown, where D(Q ∥P) = 1"
R,0.23919915700737618,"2
R
(dQ/dP −1)2dP and ρ = 1 2( 1"
R,0.24025289778714437,"α −1)2. χ2-DRO is also a
reweighting algorithm."
R,0.24130663856691253,"There are many other previously proposed methods of maximizing the worst-group performance
that are also reweighting algorithms. For instance, Xu et al. (2020) studied the imbalanced class
problem where a standard trained model always has high performance over classes with many train-
ing samples and low performance over minority classes. They proposed to balance the classes with
Label CVaR, which is based on DRO and is a reweighting algorithm."
R,0.24236037934668073,"Liu et al. (2021) proposed a two-stage training process called JTT: in the ﬁrst identiﬁcation stage
they trained a model with ERM to identify training samples that are hard to learn, and in the second
upweighting stage they trained a new model with the hard samples upweighted, so that the model
could learn all samples equally well. As the process itself suggests, JTT is a reweighting algorithm."
R,0.2434141201264489,"Finally, Zhai et al. (2021) argued that DRO-based methods are very sensitive to outliers in the
training set because they upweight training samples with high losses and outliers tend to incur high
losses. They proposed the DORO algorithm which at each iteration removes the samples with the
highest losses, and then performs DRO on the rest of the samples. DORO is a reweighting algorithm."
R,0.24446786090621708,"B
PROOFS"
R,0.24552160168598525,"Notations.
In all of the proofs, for a matrix A, we will use ∥A∥2 to denote its spectral norm and
∥A∥F to denote its Frobenius norm."
R,0.2465753424657534,Under review as a conference paper at ICLR 2022
R,0.2476290832455216,"B.1
PROOF OF THEOREM 1"
R,0.24868282402528977,"To help our readers understand the proof more easily, we will ﬁrst prove the result for static reweight-
ing algorithms where q(t)
i
= qi for all t, and then we will prove the result for dynamic reweighting
algorithms that satisfy q(t)
i
→qi as t →∞."
R,0.24973656480505796,"B.1.1
STATIC REWEIGHTING ALGORITHMS"
R,0.2507903055848261,We ﬁrst prove the result for all static reweighting algorithms such that mini qi = q∗> 0.
R,0.2518440463645943,"We will use a standard optimization proof technique called smoothness. Denote A = Pn
i=1 ∥xi∥2
2.
The empirical risk of the linear model f(x) = ⟨θ, x⟩is"
R,0.2528977871443625,"F(θ) = n
X"
R,0.25395152792413067,"i=1
qi(x⊤
i θ −yi)2
(18)"
R,0.25500526870389884,whose Hessian is
R,0.256059009483667,"∇2
θF(θ) = 2 n
X"
R,0.2571127502634352,"i=1
qixix⊤
i
(19)"
R,0.2581664910432034,"So for any unit vector v ∈Rd, we have (since qi ∈[0, 1])"
R,0.25922023182297155,"v⊤∇2
θF(θ)v = 2 n
X"
R,0.2602739726027397,"i=1
qi(x⊤
i v)2 ≤2 n
X"
R,0.2613277133825079,"i=1
qi ∥xi∥2
2 ≤2A
(20)"
R,0.2623814541622761,"which implies that F(θ) is 2A-smooth. Thus, we have the following upper quadratic bound: for any
θ1, θ2 ∈Rd,
F(θ2) ≤F(θ1) + ⟨∇θF(θ1), θ2 −θ1⟩+ A ∥θ2 −θ1∥2
2
(21)"
R,0.26343519494204426,"Denote g(θ(t)) = √Q(X⊤θ(t) −Y ) ∈Rn where √Q = diag(√q1, · · · , √qn). We can see that
g(θ(t))
2
2 = F(θ(t)), so that ∇F(θ(t)) = 2X√Qg(θ(t)). The update rule of a static reweighting
algorithm with gradient descent and the squared loss is:"
R,0.2644889357218124,"θ(t+1) = θ(t) −η n
X"
R,0.2655426765015806,"i=1
qixi(f (t)(xi) −yi) = θ(t) −ηX
p"
R,0.2665964172813488,"Qg(θ(t))
(22)"
R,0.26765015806111697,Substituting θ1 and θ2 in (21) with θ(t) and θ(t+1) yields
R,0.26870389884088514,F(θ(t+1)) ≤F(θ(t)) −2ηg(θ(t))⊤p
R,0.2697576396206533,"Q
⊤X⊤X
p"
R,0.2708113804004215,"Qg(θ(t)) + A
ηX
p"
R,0.2718651211801897,"Qg(θ(t))

2"
R,0.27291886195995785,"2
(23)"
R,0.273972602739726,"Since x1, · · · , xn are linearly independent, X⊤X is a positive deﬁnite matrix. Denote the smallest
eigenvalue of X⊤X by λmin > 0. And
√Qg(θ(t))

2 ≥√q∗g(θ(t))

2 =
p"
R,0.27502634351949423,"q∗F(θ(t)), so we"
R,0.2760800842992624,"have g(θ(t))⊤√Q
⊤X⊤X√Qg(θ(t)) ≥q∗λminF(θ(t)). Thus,"
R,0.27713382507903056,"F(θ(t+1)) ≤F(θ(t)) −2ηq∗λminF(θ(t)) + Aη2 X
p"
R,0.2781875658587987,"Q

2 2"
R,0.2792413066385669,"g(θ(t))

2 2"
R,0.2802950474183351,"≤F(θ(t)) −2ηq∗λminF(θ(t)) + Aη2 X
p"
R,0.2813487881981033,"Q

2"
R,0.28240252897787144,F F(θ(t))
R,0.2834562697576396,"≤F(θ(t)) −2ηq∗λminF(θ(t)) + Aη2 ∥X∥2
F F(θ(t))"
R,0.2845100105374078,= (1 −2ηq∗λmin + A2η2)F(θ(t)) (24)
R,0.285563751317176,Let η0 = q∗λmin
R,0.28661749209694415,"A2
. For any η ≤η0, we have F(θ(t+1)) ≤(1 −ηq∗λmin)F(θ(t)) for all t, which"
R,0.2876712328767123,"implies that limt→∞F(θ(t)) = 0. Moreover,
p"
R,0.28872497365648053,"F(θ(t+1)) ≤(1 −ηq∗λmin 2
)
p"
R,0.2897787144362487,"F(θ(t)) due to
√1 −x ≤1 −x/2."
R,0.29083245521601686,Under review as a conference paper at ICLR 2022
R,0.291886195995785,"The convergence in F(θ) implies the convergence in θ. This is because
θ(t+1) −θ(t)
2"
R,0.2929399367755532,"2 = η2 X
p"
R,0.2939936775553214,"Qg(θ(t))

2"
R,0.2950474183350896,"2 ≤η2 X
p"
R,0.29610115911485774,"Q

2 F"
R,0.2971548998946259,"g(θ(t))

2 2"
R,0.2982086406743941,"≤η2 ∥X∥2
F
g(θ(t))

2"
R,0.2992623814541623,"2 = Aη2F(θ(t))
(25)"
R,0.30031612223393045,"which implies that for any η ≤η0, ∞
X t=T"
R,0.3013698630136986,"θ(t+1) −θ(t)
2 ≤
p"
R,0.30242360379346683,"Aη2
∞
X t=T q"
R,0.303477344573235,"F(θ(t)) ≤
2A
q∗λmin q"
R,0.30453108535300316,"F(θ(T ))
(26)"
R,0.3055848261327713,"Therefore, limT →∞
P∞
t=T
θ(t+1) −θ(t)
2 = 0, which means that θ(t) converges, and it converges
to some interpolator."
R,0.3066385669125395,"B.1.2
DYNAMIC REWEIGHTING ALGORITHMS"
R,0.3076923076923077,"Now we prove the result for all dynamic reweighting algorithms satisfying Assumption1. By As-
sumption 1, for any ϵ > 0, there exists tϵ such that for all t ≥tϵ and all i,"
R,0.3087460484720759,"q(t)
i
∈(qi −ϵ, qi + ϵ)
(27)"
R,0.30979978925184404,"This is because for all i, there exists ti such that for all t ≥ti, q(t)
i
∈(qi −ϵ, qi + ϵ). Then,
we can deﬁne tϵ = max{t1, · · · , tn}. Denote the largest and smallest eigenvalues of X⊤X by
λmax and λmin, and because X is full-rank, we have λmin > 0. Select and a ﬁx an ϵ such that
0 < ϵ < max{ q∗"
R,0.3108535300316122,"3 , (q∗λmin)2"
R,0.3119072708113804,"12λmax 2 }, and then tϵ is also ﬁxed."
R,0.3129610115911486,"We still denote Q = diag(q1, · · · , qn). When t ≥tϵ, the update rule of a dynamic reweighting
algorithm with gradient descent and the squared loss is:"
R,0.31401475237091675,"θ(t+1) = θ(t) −ηXQ(t)
ϵ (X⊤θ(t) −Y )
(28)"
R,0.3150684931506849,"where Q(t)
ϵ
= Q(t), and we use the subscript ϵ to indicate that
Q(t)
ϵ
−Q

2 < ϵ. Then, note that"
R,0.31612223393045313,"we can rewrite Q(t)
ϵ
as Q(t)
ϵ
=
q"
R,0.3171759747102213,"Q(t)
3ϵ ·√Q for all ϵ < q∗/3. This is because qi +ϵ <
p"
R,0.31822971548998946,"(qi + 3ϵ)qi
and qi −ϵ >
p"
R,0.3192834562697576,"(qi −3ϵ)qi for all ϵ < qi/3, and qi ≥q∗. Thus, we have"
R,0.3203371970495258,"θ(t+1) = θ(t) −ηX
q"
R,0.321390937829294,"Q(t)
3ϵ g(θ(t))
where Q(t)
ϵ
=
q"
R,0.3224446786090622,"Q(t)
3ϵ ·
p"
R,0.32349841938883034,"Q
(29)"
R,0.3245521601685985,"Again, substituting θ1 and θ2 in (21) with θ(t) and θ(t+1) yields"
R,0.3256059009483667,F(θ(t+1)) ≤F(θ(t)) −2ηg(θ(t))⊤p
R,0.3266596417281349,"Q
⊤X⊤X
q"
R,0.32771338250790305,"Q(t)
3ϵ g(θ(t)) + A
ηX
q"
R,0.3287671232876712,"Q(t)
3ϵ g(θ(t)) 2"
R,0.32982086406743943,"2
(30)"
R,0.3308746048472076,"Then, note that
g(θ(t))⊤p"
R,0.33192834562697576,"Q
⊤X⊤X
q"
R,0.3329820864067439,"Q(t)
3ϵ −
p"
R,0.3340358271865121,"Q

g(θ(t))"
R,0.3350895679662803,"≤

p"
R,0.3361433087460485,"Q
⊤X⊤X
q"
R,0.33719704952581664,"Q(t)
3ϵ −
p"
R,0.3382507903055848,"Q

2"
R,0.339304531085353,"g(θ(t))

2 2"
R,0.3403582718651212,"≤

p"
R,0.34141201264488935,"Q

2"
R,0.3424657534246575,"X⊤X

2  q"
R,0.34351949420442573,"Q(t)
3ϵ −
p"
R,0.3445732349841939,"Q

2"
R,0.34562697576396206,"g(θ(t))

2 2"
R,0.3466807165437302,≤λmax√
R,0.34773445732349845,3ϵF(θ(t)) (31)
R,0.3487881981032666,"where the last step comes from the following fact: for all ϵ < qi/3,
p"
R,0.3498419388830348,"qi + 3ϵ −√qi ≤
√"
R,0.35089567966280294,"3ϵ
and
√qi −
p"
R,0.3519494204425711,"qi −3ϵ ≤
√"
R,0.3530031612223393,"3ϵ
(32)"
R,0.3540569020021075,Under review as a conference paper at ICLR 2022
R,0.35511064278187565,"And as proved before, we also have"
R,0.3561643835616438,g(θ(t))⊤p
R,0.35721812434141204,"Q
⊤X⊤X
p"
R,0.3582718651211802,"Qg(θ(t)) ≥q∗λminF(θ(t))
(33)"
R,0.35932560590094836,Since ϵ ≤(q∗λmin)2
R,0.36037934668071653,"12λmax 2 , we have"
R,0.36143308746048475,g(θ(t))⊤p
R,0.3624868282402529,"Q
⊤X⊤X
q"
R,0.3635405690200211,"Q(t)
3ϵ g(θ(t)) ≥

q∗λmin −λmax√"
R,0.36459430979978924,"3ϵ

F(θ(t)) ≥1"
R,0.3656480505795574,"2q∗λminF(θ(t))
(34) Thus,"
R,0.3667017913593256,"F(θ(t+1)) ≤F(θ(t)) −ηq∗λminF(θ(t)) + Aη2
X
q"
R,0.3677555321390938,"Q(t)
3ϵ  2 2"
R,0.36880927291886195,"g(θ(t))

2 2"
R,0.3698630136986301,≤(1 −ηq∗λmin + A2η2(1 + 3ϵ))F(θ(t))
R,0.37091675447839834,≤(1 −ηq∗λmin + 2A2η2)F(θ(t)) (35)
R,0.3719704952581665,for all ϵ < 1/3. Let η0 = q∗λmin
R,0.37302423603793466,"4A2 . For any η ≤η0, we have F(θ(t+1)) ≤(1−ηq∗λmin/2)F(θ(t)) for
all t ≥tϵ, which implies that limt→∞F(θ(t)) = 0. As before, we can prove that the convergence
in F(θ) implies the convergence in θ. Thus, θ converges to some interpolator."
R,0.37407797681770283,"B.2
PROOF OF THEOREM 4"
R,0.37513171759747105,"Note that the ﬁrst l layers (except the output layer) of the original NTK formulation and our new
formulation are the same, so we still have the following proposition:
Proposition 9 (Proposition 1 in Jacot et al. (2018)). If σ is Lipschitz and dl →∞for l = 1, · · · , L
sequentially, then for all l = 1, · · · , L, the distribution of a single element of hl converges in
probability to a zero-mean Gaussian process of covariance Σl that is deﬁned recursively by:"
R,0.3761854583772392,"Σ1(x, x′) = 1"
R,0.3772391991570074,"d0
x⊤x′ + β2"
R,0.37829293993677554,"Σl(x, x′) = Ef[σ(f(x))σ(f(x′))] + β2
(36)"
R,0.3793466807165437,where f is sampled from a zero-mean Gaussian process of covariance Σ(l−1).
R,0.3804004214963119,"Now we show that for an inﬁnitely wide neural network with L ≥1 hidden layers, Θ(0) converges
in probability to the following non-degenerated deterministic limiting kernel"
R,0.3814541622760801,"Θ = Ef∼ΣL[σ(f(x))σ(f(x′))] + β2
(37)"
R,0.38250790305584825,"Consider the output layer hL+1 = W L
√"
R,0.3835616438356164,˜d σ(hL) + βbL. We can see that for any parameter θi before
R,0.38461538461538464,"the output layer,"
R,0.3856691253951528,∇θihL+1 = diag( ˙σ(hL))W L⊤
R,0.38672286617492097,"√dL
∇θihL = 0
(38)"
R,0.38777660695468913,"And for W L and bL, we have"
R,0.38883034773445735,"∇W LhL+1 =
1
√dL
σ(hL)
and
∇bLhL+1 = β
(39)"
R,0.3898840885142255,Then we can achieve (37) by the law of large numbers.
R,0.3909378292939937,"B.3
PROOF OF THEOREM 5"
R,0.39199157007376184,"We will use the following short-hand in the proof:


 
"
R,0.39304531085353,g(θ(t)) = f (t)(X) −Y
R,0.3940990516332982,J(θ(t)) = ∇θf(X; θ(t)) ∈Rp×n
R,0.3951527924130664,Θ(t) = J(θ(t))⊤J(θ(t)) (40)
R,0.39620653319283455,Under review as a conference paper at ICLR 2022
R,0.3972602739726027,"For any ϵ > 0, there exists tϵ such that for all t ≥tϵ and all i, q(t)
i
∈(qi −ϵ, qi + ϵ). Like what we"
R,0.39831401475237094,"have done in (29), we can rewrite Q(t) = Q(t)
ϵ
=
q"
R,0.3993677555321391,"Q(t)
3ϵ · √Q, where Q = diag(q1, · · · , qn)."
R,0.40042149631190727,"The update rule of a reweighting algorithm with gradient descent and the squared loss for the wide
neural network is:
θ(t+1) = θ(t) −ηJ(θ(t))Q(t)g(θ(t))
(41)
and for t ≥tϵ, it can be rewritten as"
R,0.40147523709167543,"θ(t+1) = θ(t) −ηJ(θ(t))
q"
R,0.40252897787144365,"Q(t)
3ϵ
hp"
R,0.4035827186512118,"Qg(θ(t))
i
(42)"
R,0.40463645943098,"First, we will prove the following theorem:
Theorem 10. There exist constants M > 0 and ϵ0 > 0 such that for all ϵ ∈(0, ϵ0], η ≤η∗and
any δ > 0, there exist R0 > 0, ˜D > 0 and B > 1 such that for any ˜d ≥˜D, the following (i) and
(ii) hold with probability at least (1 −δ) over random initialization when applying gradient descent
with learning rate η:"
R,0.40569020021074814,"(i) For all t ≤tϵ, there is
g(θ(t))

2 ≤BtR0
(43) t
X j=1"
R,0.4067439409905163,"θ(j) −θ(j−1)
2 ≤ηMR0 t
X"
R,0.4077976817702845,"j=1
Bj−1 < MBtϵR0"
R,0.4088514225500527,"B −1
(44)"
R,0.40990516332982085,"(ii) For all t ≥tϵ, we have

p"
R,0.410958904109589,"Qg(θ(t))

2 ≤

1 −ηq∗λmin 3"
R,0.41201264488935724,"t−tϵ
BtϵR0
(45) t
X"
R,0.4130663856691254,j=tϵ+1
R,0.41412012644889357,"θ(j) −θ(j−1)
2 ≤η
√"
R,0.41517386722866173,"1 + 3ϵMBtϵR0 t
X"
R,0.41622760800842995,j=tϵ+1
R,0.4172813487881981,"
1 −ηq∗λmin 3 j−tϵ"
R,0.4183350895679663,< 3√1 + 3ϵMBtϵR0
R,0.41938883034773444,q∗λmin (46)
R,0.4204425711275026,"Proof.
The proof is based on the following lemma:
Lemma 11 (Local Lipschitzness of the Jacobian). Under Assumption 2, there is a constant M > 0
such that for any C0 > 0 and any δ > 0, there exists a ˜D such that: If ˜d ≥˜D, then with probability
at least (1 −δ) over random initialization, for any x such that ∥x∥2 ≤1,









"
R,0.4214963119072708,"







"
R,0.422550052687039,"∇θf(x; θ) −∇θf(x; ˜θ)

2 ≤M"
P,0.42360379346680715,4p ˜d
P,0.4246575342465753,"θ −˜θ

2"
P,0.42571127502634354,"∥∇θf(x; θ)∥2 ≤M
J(θ) −J(˜θ)

F ≤M"
P,0.4267650158061117,4p ˜d
P,0.42781875658587987,"θ −˜θ

2"
P,0.42887249736564803,∥J(θ)∥F ≤M
P,0.42992623814541625,",
∀θ, ˜θ ∈B(θ(0), C0)
(47)"
P,0.4309799789251844,"where B(θ(0), R) = {θ :
θ −θ(0)
2 < R}."
P,0.4320337197049526,"The proof can be found in Appendix B.4. Note that for any x, f (0)(x) = βbL where bL is sampled
from the standard Gaussian distribution. Thus, for any δ > 0, there exists a constant R0 such that
with probability at least (1 −δ/3) over random initialization,
g(θ(0))

2 < R0
(48)"
P,0.43308746048472074,"And by Theorem 4, there exists D2 ≥0 such that for any ˜d ≥D2, with probability at least (1−δ/3),
Θ −Θ(0)
F ≤q∗λmin"
P,0.43414120126448896,"3
(49)"
P,0.4351949420442571,Under review as a conference paper at ICLR 2022
P,0.4362486828240253,Let M be the constant in Lemma 11. Let ϵ0 = (q∗λmin)2
P,0.43730242360379346,"108M 4 . Let B = 1 + η∗M 2, and C0 = MBtϵR0 B−1
+"
P,0.4383561643835616,3√1+3ϵMBtϵR0
P,0.43940990516332984,"q∗λmin
. By Lemma 11, there exists D1 > 0 such that with probability at least (1 −δ/3),"
P,0.440463645943098,"for any ˜d ≥D1, (47) is true for all θ, ˜θ ∈B(θ(0), C0)."
P,0.44151738672286617,"By union bound, with probability at least (1 −δ), (47), (48) and (49) are all true. Now we assume
that all of them are true, and prove (43) and (44) by induction. (43) is true for t = 0 due to (48), and
(44) is always true for t = 0. Suppose (43) and (44) are true for t, then for t + 1 we have
θ(t+1) −θ(t)
2 ≤η
J(θ(t))Q(t)
2"
P,0.44257112750263433,"g(θ(t))

2 ≤η
J(θ(t))Q(t)
F"
P,0.44362486828240255,"g(θ(t))

2"
P,0.4446786090621707,"≤η
J(θ(t))

F"
P,0.4457323498419389,"g(θ(t))

2 ≤MηBtR0
(50)"
P,0.44678609062170704,"So (44) is also true for t + 1. And we also have
g(θ(t+1))

2 =
g(θ(t+1)) −g(θ(t)) + g(θ(t))

2"
P,0.44783983140147526,"=
J(˜θ(t))⊤(θ(t+1) −θ(t)) + g(θ(t))

2"
P,0.4488935721812434,"=
−ηJ(˜θ(t))⊤J(θ(t))Q(t)g(θ(t)) + g(θ(t))

2"
P,0.4499473129610116,"≤
I −ηJ(˜θ(t))⊤J(θ(t))Q(t)
2"
P,0.45100105374077976,"g(θ(t))

2"
P,0.4520547945205479,"≤

1 +
ηJ(˜θ(t))⊤J(θ(t))Q(t)
2"
P,0.45310853530031614," g(θ(t))

2"
P,0.4541622760800843,"≤

1 + η
J(˜θ(t))

F"
P,0.45521601685985247,"J(θ(t))

F"
P,0.45626975763962063," g(θ(t))

2"
P,0.45732349841938885,"≤(1 + η∗M 2)
g(θ(t))

2 ≤Bt+1R0 (51)"
P,0.458377239199157,"Therefore, (43) and (44) are true for all t ≤tϵ, which implies that
√Qg(θ(tϵ))

2 ≤
g(θ(tϵ))

2 ≤
BtϵR0, so (45) is true for t = tϵ. And (46) is obviously true for t = tϵ. Now, let us prove (ii) by
induction. Note that when t ≥tϵ, we have the alternative update rule (42). If (45) and (46) are true
for t, then for t + 1, there is"
P,0.4594309799789252,"θ(t+1) −θ(t)
2 ≤η
J(θ(t))
q"
P,0.46048472075869334,"Q(t)
3ϵ 2 p"
P,0.46153846153846156,"Qg(θ(t))

2 ≤η
J(θ(t))
q"
P,0.46259220231822973,"Q(t)
3ϵ F p"
P,0.4636459430979979,"Qg(θ(t))

2 ≤η
√"
P,0.46469968387776606,"1 + 3ϵ
J(θ(t))

F p"
P,0.4657534246575342,"Qg(θ(t))

2 ≤Mη
√"
P,0.46680716543730244,"1 + 3ϵ

1 −ηq∗λmin 3"
P,0.4678609062170706,"t−tϵ
BtϵR0 (52)"
P,0.46891464699683877,"So (46) is true for t + 1. And we also have

p"
P,0.46996838777660693,"Qg(θ(t+1))

2 =

p"
P,0.47102212855637515,"Qg(θ(t+1)) −
p"
P,0.4720758693361433,"Qg(θ(t)) +
p"
P,0.4731296101159115,"Qg(θ(t))

2"
P,0.47418335089567965,"=

p"
P,0.47523709167544786,"QJ(˜θ(t))⊤(θ(t+1) −θ(t)) +
p"
P,0.47629083245521603,"Qg(θ(t))

2"
P,0.4773445732349842,"=
−η
p"
P,0.47839831401475236,"QJ(˜θ(t))⊤J(θ(t))Q(t)g(θ(t)) +
p"
P,0.4794520547945205,"Qg(θ(t))

2"
P,0.48050579557428874,"≤
I −η
p"
P,0.4815595363540569,"QJ(˜θ(t))⊤J(θ(t))
q"
P,0.48261327713382507,"Q(t)
3ϵ 2 p"
P,0.48366701791359323,"Qg(θ(t))

2"
P,0.48472075869336145,"≤
I −η
p"
P,0.4857744994731296,"QJ(˜θ(t))⊤J(θ(t))
q"
P,0.4868282402528978,"Q(t)
3ϵ 2"
P,0.48788198103266595,"
1 −ηq∗λmin 3 t
R0 (53)"
P,0.48893572181243417,"where ˜θ(t) is some linear interpolation between θ(t) and θ(t+1). Now we prove that
I −η
p"
P,0.48998946259220233,"QJ(˜θ(t))⊤J(θ(t))
q"
P,0.4910432033719705,"Q(t)
3ϵ"
P,0.49209694415173866,"2
≤1 −ηq∗λmin"
P,0.4931506849315068,"3
(54)"
P,0.49420442571127504,Under review as a conference paper at ICLR 2022
P,0.4952581664910432,"For any unit vector v ∈Rn, we have"
P,0.49631190727081137,"v⊤(I −η
p QΘ
p"
P,0.49736564805057953,"Q)v = 1 −ηv⊤p QΘ
p"
P,0.49841938883034775,"Qv
(55)
√Qv

2 ∈[√q∗, 1], so for any η ≤η∗, v⊤(I −η√QΘ√Q)v ∈[0, 1 −ηλminq∗], which implies
that
I −η√QΘ√Q

2 ≤1 −ηλminq∗. Thus,"
P,0.4994731296101159,"I −η
p"
P,0.5005268703898841,"QJ(˜θ(t))⊤J(θ(t))
p"
P,0.5015806111696522,"Q

2"
P,0.5026343519494204,"≤
I −η
p QΘ
p"
P,0.5036880927291886,"Q

2 + η

p"
P,0.5047418335089568,"Q(Θ −Θ(0))
p"
P,0.505795574288725,"Q

2 + η

p"
P,0.5068493150684932,"Q(J(θ(0))⊤J(θ(0)) −J(˜θ(t))⊤J(θ(t)))
p"
P,0.5079030558482613,"Q

2"
P,0.5089567966280295,"≤1 −ηλminq∗+ η

p"
P,0.5100105374077977,"Q(Θ −Θ(0))
p"
P,0.5110642781875658,"Q

F + η

p"
P,0.512118018967334,"Q(J(θ(0))⊤J(θ(0)) −J(˜θ(t))⊤J(θ(t)))
p"
P,0.5131717597471022,"Q

F"
P,0.5142255005268704,"≤1 −ηλminq∗+ η
Θ −Θ(0)
F + η
J(θ(0))⊤J(θ(0)) −J(˜θ(t))⊤J(θ(t))

F"
P,0.5152792413066386,≤1 −ηλminq∗+ ηq∗λmin
P,0.5163329820864068,"3
+ ηM 2"
P,0.5173867228661749,4p ˜d
P,0.5184404636459431,"θ(t) −θ(0)
2 +
˜θ(t) −θ(0)
2"
P,0.5194942044257113,"
≤1 −ηq∗λmin"
P,0.5205479452054794,"2
(56)"
P,0.5216016859852476,"for all ˜d ≥max

D1, D2,

12M 2C0"
P,0.5226554267650158,"q∗λmin
4
, which implies that"
P,0.523709167544784,"I −η
p"
P,0.5247629083245522,"QJ(˜θ(t))⊤J(θ(t))
q"
P,0.5258166491043204,"Q(t)
3ϵ 2"
P,0.5268703898840885,≤1 −ηq∗λmin
P,0.5279241306638567,"2
+
η
p"
P,0.5289778714436248,"QJ(˜θ(t))⊤J(θ(t))
q"
P,0.530031612223393,"Q(t)
3ϵ −
p"
P,0.5310853530031612,"Q

2"
P,0.5321390937829295,≤1 −ηq∗λmin
P,0.5331928345626976,"2
+ ηM 2√"
P,0.5342465753424658,3ϵ ≤1 −ηq∗λmin
P,0.5353003161222339,"3
(due to (32)) (57)"
P,0.5363540569020021,"for all ϵ ≤ϵ0. Thus, (45) is also true for t + 1. In conclusion, (45) and (46) are true with probability"
P,0.5374077976817703,"at least (1 −δ) for all ˜d ≥˜D = max

D1, D2,

12M 2C0"
P,0.5384615384615384,"q∗λmin
4
."
P,0.5395152792413066,"Returning back to the proof of Theorem 5.
Choose and ﬁx an ϵ such that ϵ
<"
P,0.5405690200210748,"min{ϵ0, 1"
P,0.541622760800843,"3

q∗λmin"
P,0.5426765015806112,"3λmax+q∗λmin
2
}, where ϵ0 is deﬁned by Theorem 10. Then, tϵ is also ﬁxed. There"
P,0.5437302423603794,"exists ˜D ≥0 such that for any ˜d ≥˜D, with probability at least (1 −δ), Theorem 10 and Lemma 11
are true and
Θ −Θ(0)
F ≤q∗λmin"
P,0.5447839831401475,"3
(58)"
P,0.5458377239199157,"which immediately implies that
Θ(0)
2 ≤∥Θ∥2 +
Θ −Θ(0)
F ≤λmax + q∗λmin"
P,0.5468914646996839,"3
(59)"
P,0.547945205479452,We still denote B = 1 + η∗M 2 and C0 = MBtϵR0
P,0.5489989462592202,"B−1
+ 3√1+3ϵMBtϵR0"
P,0.5500526870389885,"q∗λmin
. Theorem 10 ensures that for
all t, θ(t) ∈B(θ(0), C0). Then we have
I −η
p"
P,0.5511064278187566,QΘ(0)p
P,0.5521601685985248,"Q

2 ≤
I −η
p QΘ
p"
P,0.553213909378293,"Q

2 + η

p"
P,0.5542676501580611,"Q(Θ −Θ(0))
p"
P,0.5553213909378293,"Q

2"
P,0.5563751317175974,≤1 −ηλminq∗+ ηq∗λmin
P,0.5574288724973656,"3
= 1 −2ηq∗λmin 3 (60)"
P,0.5584826132771338,"so it follows that
I −η
p"
P,0.559536354056902,"QΘ(0)
q"
P,0.5605900948366702,"Q(t)
3ϵ"
P,0.5616438356164384,"2
≤
I −η
p"
P,0.5626975763962065,QΘ(0)p
P,0.5637513171759747,"Q

2 +
η
p"
P,0.5648050579557429,"QΘ(0)
q"
P,0.565858798735511,"Q(t)
3ϵ −
p"
P,0.5669125395152792,"Q

2"
P,0.5679662802950474,≤1 −2ηq∗λmin
P,0.5690200210748156,"3
+ η(λmax + q∗λmin 3
)
√ 3ϵ (61)"
P,0.5700737618545838,Under review as a conference paper at ICLR 2022
P,0.571127502634352,"Thus, for all ϵ < 1"
P,0.5721812434141201,"3

q∗λmin"
P,0.5732349841938883,"3λmax+q∗λmin
2
, there is"
P,0.5742887249736565,"I −η
p"
P,0.5753424657534246,"QΘ(0)
q"
P,0.5763962065331928,"Q(t)
3ϵ"
P,0.5774499473129611,"2
≤1 −ηq∗λmin"
P,0.5785036880927292,"3
(62)"
P,0.5795574288724974,The update rule of the reweighting algorithm for the linearized neural network is:
P,0.5806111696522656,"θ(t+1)
lin
= θ(t)
lin −ηJ(θ(0))Q(t)glin(θ(t))
(63)"
P,0.5816649104320337,"where we use the subscript “lin” to denote the linearized neural network, and with a slight abuse of
notion denote glin(θ(t)) = g(θ(t)
lin )."
P,0.5827186512118019,"First, let us consider the training data X. Denote ∆t = glin(θ(t)) −g(θ(t)). We have
(
glin(θ(t+1)) −glin(θ(t)) = −ηJ(θ(0))⊤J(θ(0))Q(t)glin(θ(t))"
P,0.58377239199157,"g(θ(t+1)) −g(θ(t)) = −ηJ(˜θ(t))⊤J(θ(t))Q(t)g(θ(t))
(64)"
P,0.5848261327713382,"where ˜θ(t) is some linear interpolation between θ(t) and θ(t+1). Thus,"
P,0.5858798735511064,"∆t+1 −∆t =η
h
J(˜θ(t))⊤J(θ(t)) −J(θ(0))⊤J(θ(0))
i
Q(t)g(θ(t))"
P,0.5869336143308747,"−ηJ(θ(0))⊤J(θ(0))Q(t)∆t
(65)"
P,0.5879873551106428,"By Lemma 11, we have
J(˜θ(t))⊤J(θ(t)) −J(θ(0))⊤J(θ(0))

F"
P,0.589041095890411,"≤


J(˜θ(t)) −J(θ(0))
⊤
J(θ(t))

F
+
J(θ(0))⊤
J(θ(t)) −J(θ(0))

F"
P,0.5900948366701791,≤2M 2C0 ˜d−1/4 (66)
P,0.5911485774499473,"which implies that for all t < tϵ,"
P,0.5922023182297155,"∥∆t+1∥2 ≤

h
I −ηJ(θ(0))⊤J(θ(0))Q(t)i
∆t

2 +
η
h
J(˜θ(t))⊤J(θ(t)) −J(θ(0))⊤J(θ(0))
i
Q(t)g(θ(t))

2"
P,0.5932560590094836,"≤
I −ηJ(θ(0))⊤J(θ(0))Q(t)
F ∥∆t∥2 + η
J(˜θ(t))⊤J(θ(t)) −J(θ(0))⊤J(θ(0))

F"
P,0.5943097997892518,"g(θ(t))

2
≤(1 + ηM 2) ∥∆t∥2 + 2ηM 2C0BtR0 ˜d−1/4"
P,0.59536354056902,≤B ∥∆t∥2 + 2ηM 2C0BtR0 ˜d−1/4 (67)
P,0.5964172813487882,"Therefore, we have"
P,0.5974710221285564,"B−(t+1) ∥∆t+1∥2 ≤B−t ∥∆t∥2 + 2ηM 2C0B−1R0 ˜d−1/4
(68)"
P,0.5985247629083246,"Since ∆0 = 0, it follows that for all t ≤tϵ,"
P,0.5995785036880927,"∥∆t∥2 ≤2tηM 2C0Bt−1R0 ˜d−1/4
(69)"
P,0.6006322444678609,"and particularly we have

p"
P,0.6016859852476291,"Q∆tϵ

2 ≤∥∆tϵ∥2 ≤2tϵηM 2C0Btϵ−1R0 ˜d−1/4
(70)"
P,0.6027397260273972,"For t ≥tϵ, we have the alternative update rule (42). Thus, p"
P,0.6037934668071654,"Q∆t+1 −
p"
P,0.6048472075869337,"Q∆t =η
p"
P,0.6059009483667018,"Q
h
J(˜θ(t))⊤J(θ(t)) −J(θ(0))⊤J(θ(0))
i q"
P,0.60695468914647,"Q(t)
3ϵ
hp"
P,0.6080084299262382,"Qg(θ(t))
i −η
p"
P,0.6090621707060063,"QJ(θ(0))⊤J(θ(0))
q"
P,0.6101159114857745,"Q(t)
3ϵ
hp"
P,0.6111696522655427,"Q∆t
i
(71)"
P,0.6122233930453108,Under review as a conference paper at ICLR 2022
P,0.613277133825079,"Let A = I −η√QJ(θ(0))⊤J(θ(0))
q"
P,0.6143308746048473,"Q(t)
3ϵ = I −η√QΘ(0)
q"
P,0.6153846153846154,"Q(t)
3ϵ . Then, we have p"
P,0.6164383561643836,"Q∆t+1 = A
p"
P,0.6174920969441517,"Q∆t+η
p"
P,0.6185458377239199,"Q
h
J(˜θ(t))⊤J(θ(t)) −J(θ(0))⊤J(θ(0))
i q"
P,0.6195995785036881,"Q(t)
3ϵ
p"
P,0.6206533192834562,"Qg(θ(t))

(72)"
P,0.6217070600632244,Let γ = 1 −ηq∗λmin
P,0.6227608008429927,"3
< 1. Combining with Theorem 10 and (62), the above leads to p"
P,0.6238145416227608,"Q∆t+1

2 ≤∥A∥2

p"
P,0.624868282402529,"Q∆t

2 + η

p"
P,0.6259220231822972,"Q
h
J(˜θ(t))⊤J(θ(t)) −J(θ(0))⊤J(θ(0))
i q"
P,0.6269757639620653,"Q(t)
3ϵ 2 p"
P,0.6280295047418335,"Qg(θ(t))

2"
P,0.6290832455216017,"≤γ

p"
P,0.6301369863013698,"Q∆t

2 + η
J(˜θ(t))⊤J(θ(t)) −J(θ(0))⊤J(θ(0))

F √"
P,0.631190727081138,1 + 3ϵγt−tϵBtϵR0
P,0.6322444678609063,"≤γ

p"
P,0.6332982086406744,"Q∆t

2 + 2ηM 2C0
√"
P,0.6343519494204426,1 + 3ϵγt−tϵBtϵR0 ˜d−1/4
P,0.6354056902002108,"(73)
This implies that"
P,0.6364594309799789,"γ−(t+1) 
p"
P,0.6375131717597471,"Q∆t+1

2 ≤γ−t 
p"
P,0.6385669125395153,"Q∆t

2 + 2ηM 2C0
√"
P,0.6396206533192834,"1 + 3ϵγ−1−tϵBtϵR0 ˜d−1/4
(74)"
P,0.6406743940990516,"Combining with (70), it implies that for all t ≥tϵ, p"
P,0.6417281348788199,"Q∆t

2 ≤2γt−tϵηM 2C0BtϵR0

tϵB−1 +
√"
P,0.642781875658588,"1 + 3ϵγ−1(t −tϵ)
 ˜d−1/4
(75)"
P,0.6438356164383562,"Next, we consider an arbitrary test point x such that ∥x∥2 ≤1. Denote δt = f (t)
lin (x) −f (t)(x).
Then we have"
P,0.6448893572181243,"(
f (t+1)
lin
(x) −f (t)
lin (x) = −η∇θf(x; θ(0))⊤J(θ(0))Q(t)glin(θ(t))"
P,0.6459430979978925,"f (t+1)(x) −f (t)(x) = −η∇θf(x; ˜θ(t))⊤J(θ(t))Q(t)g(θ(t))
(76)"
P,0.6469968387776607,which yields
P,0.6480505795574288,"δt+1 −δt =η
h
∇θf(x; ˜θ(t))⊤J(θ(t)) −∇θf(x; θ(0))⊤J(θ(0))
i
Q(t)g(θ(t))"
P,0.649104320337197,"−η∇θf(x; θ(0))⊤J(θ(0))Q(t)∆t
(77)"
P,0.6501580611169653,"For t ≤tϵ, we have"
P,0.6512118018967334,"∥δt∥2 ≤η t−1
X s=0"
P,0.6522655426765016,"h
∇θf(x; ˜θ(s))⊤J(θ(s)) −∇θf(x; θ(0))⊤J(θ(0))
i
Q(s)
2"
P,0.6533192834562698,"g(θ(s))

2 + η t−1
X s=0"
P,0.6543730242360379,"∇θf(x; θ(0))⊤J(θ(0))Q(s)
2 ∥∆s∥2 ≤η t−1
X s=0"
P,0.6554267650158061,"∇θf(x; ˜θ(s))⊤J(θ(s)) −∇θf(x; θ(0))⊤J(θ(0))

F"
P,0.6564805057955743,"g(θ(s))

2 + η t−1
X s=0"
P,0.6575342465753424,"∇θf(x; θ(0))

2"
P,0.6585879873551106,"J(θ(0))

F ∥∆s∥2"
P,0.6596417281348789,"≤2ηM 2C0 ˜d−1/4
t−1
X"
P,0.660695468914647,"s=0
BsR0 + ηM 2
t−1
X"
P,0.6617492096944152,"s=0
(2sηM 2C0Bs−1R0 ˜d−1/4) (78)"
P,0.6628029504741834,Under review as a conference paper at ICLR 2022
P,0.6638566912539515,"So we can see that there exists a constant C1 such that ∥δtϵ∥2 ≤C1 ˜d−1/4. Then, for t > tϵ, we have"
P,0.6649104320337197,"∥δt∥2 −∥δtϵ∥2 ≤η t−1
X s=tϵ"
P,0.6659641728134879,"h
∇θf(x; ˜θ(s))⊤J(θ(s)) −∇θf(x; θ(0))⊤J(θ(0))
i q"
P,0.667017913593256,"Q(s)
3ϵ 2 p"
P,0.6680716543730242,"Qg(θ(s))

2 + η t−1
X s=tϵ"
P,0.6691253951527925,"∇θf(x; θ(0))⊤J(θ(0))
q"
P,0.6701791359325606,"Q(s)
3ϵ 2 p"
P,0.6712328767123288,"Q∆s

2"
P,0.672286617492097,≤2ηM 2C0 ˜d−1/4√
P,0.6733403582718651,"1 + 3ϵ t−1
X"
P,0.6743940990516333,"s=tϵ
γs−tϵBtϵR0"
P,0.6754478398314014,+ ηM 2√
P,0.6765015806111696,"1 + 3ϵ t−1
X s=tϵ"
P,0.6775553213909379,"
2γs−tϵηM 2C0BtϵR0

tϵB−1 +
√"
P,0.678609062170706,"1 + 3ϵγ−1(s −tϵ)
 ˜d−1/4 (79)"
P,0.6796628029504742,"Note that P∞
t=0 tγt is ﬁnite as long as γ ∈(0, 1). Therefore, there is a constant C such that for any
t, ∥δt∥2 ≤C ˜d−1/4 with probability at least (1 −δ) for any ˜d ≥˜D."
P,0.6807165437302424,"B.4
PROOF OF LEMMA 11"
P,0.6817702845100105,"We will use the following theorem regarding the eigenvalues of random Gaussian matrices:
Theorem 12 (Corollary 5.35 in Vershynin (2010)). If A ∈Rp×q is a random matrix whose entries
are independent standard normal random variables, then for every t ≥0, with probability at least
1 −2 exp(−t2/2),
√p −√q −t ≤λmin(A) ≤λmax(A) ≤√p + √q + t
(80)"
P,0.6828240252897787,"By this theorem, and also note that W L is a vector, we can see that for any δ, there exist ˜D > 0 and
M1 > 0 such that if ˜d ≥˜D, then with probability at least (1 −δ), for all θ ∈B(θ(0), C0), we have
W l
2 ≤3
p"
P,0.6838777660695469,"˜d
(∀0 ≤l ≤L −1)
and
W L
2 ≤C0 ≤3
4p"
P,0.684931506849315,"˜d
(81)"
P,0.6859852476290832,"as well as
βbl
2 ≤M1
p"
P,0.6870389884088515,"˜d
(∀l = 0, · · · , L)
(82)"
P,0.6880927291886196,"Now we assume that (81) and (82) are true. Then, for any x such that ∥x∥2 ≤1,
h1
2 =

1
√d0
W 0x + βb0

2
≤
1
√d0"
P,0.6891464699683878,"W 0
2 ∥x∥2 +
βb0
2 ≤( 3
√d0
+ M1)
p ˜d"
P,0.690200210748156,"hl+1
2 ="
P,0.6912539515279241,"1
p"
P,0.6923076923076923,"˜d
W lxl + βbl

2
≤
1
p ˜d"
P,0.6933614330874605,"W l
2
xl
2 +
βbl
2
(∀l ≥1)"
P,0.6944151738672286,"xl
2 =
σ(hl) −σ(0l) + σ(0l)

2 ≤L0
hl
2 + σ(0)
p"
P,0.6954689146469969,"˜d
(∀l ≥1) (83)"
P,0.6965226554267651,"where L0 is the Lipschitz constant of σ and σ(0l) = (σ(0), · · · , σ(0)) ∈Rdl. By induction, there"
P,0.6975763962065332,"exists an M2 > 0 such that
xl
2 ≤M2
p"
P,0.6986301369863014,"˜d and
hl
2 ≤M2
p"
P,0.6996838777660696,"˜d for all l = 1, · · · , L."
P,0.7007376185458377,"Denote αl = ∇hlf(x) = ∇hlhL+1. For all l = 1, · · · , L, we have αl = diag( ˙σ(hl)) W l⊤
√"
P,0.7017913593256059,˜d αl+1
P,0.702845100105374,"where ˙σ(x)
≤
L0 for all x
∈
R since σ is L0-Lipschitz, αL+1
=
1 and
αL
2
=
diag( ˙σ(hL)) W L⊤
√ ˜d"
P,0.7038988408851422,"2
≤
3
4√"
P,0.7049525816649105,"˜dL0.
Then, we can easily prove by induction that there exists an"
P,0.7060063224446786,"M3 > 1 such that
αl
2 ≤M3/
4p"
P,0.7070600632244468,"˜d for all l = 1, · · · , L (note that this is not true for L + 1
because αL+1 = 1)."
P,0.708113804004215,"For l = 0, ∇W 0f(x) =
1
√d0 x0α1⊤, so ∥∇W lf(x)∥2 ≤
1
√d0
x0
2
α1
2 ≤
1
√d0 M3/
4p"
P,0.7091675447839831,"˜d. And
for any l = 1, · · · , L, ∇W lf(x) =
1
√"
P,0.7102212855637513,"˜dxlαl+1, so ∥∇W lf(x)∥2 ≤
1
√ ˜d"
P,0.7112750263435195,"xl
2
αl+1
2 ≤M2M3."
P,0.7123287671232876,Under review as a conference paper at ICLR 2022
P,0.7133825079030558,"(Note that if M3 > 1, then
αL+1
2 ≤M3; and since ˜d ≥1, there is
αl
2 ≤M3 for l ≤L.)
Moreover, for l = 0, · · · , L, ∇blf(x) = βαl+1, so ∥∇blf(x)∥2 ≤βM3. Thus, if (81) and (82) are
true, then there exists an M4 > 0, such that ∥∇θf(x)∥2 ≤M4/√n. And since ∥xi∥2 ≤1 for all i,
so ∥J(θ)∥F ≤M4."
P,0.7144362486828241,"Next, we consider the difference in ∇θf(x) between θ and ˜θ. Let ˜f, ˜W, ˜b, ˜x, ˜h, ˜α be the function
and the values corresponding to ˜θ. There is"
P,0.7154899894625922,"h1 −˜h1
2 =

1
√d0
(W 0 −˜W 0)x + β(b0 −˜b0)

2"
P,0.7165437302423604,"≤
1
√d0"
P,0.7175974710221286,"W 0 −˜W 0
2 ∥x∥2 + β
b0 −˜b0
2 ≤
 1
√d0
+ β
 θ −˜θ

2
hl+1 −˜hl+1
2 ="
P,0.7186512118018967,"1
p"
P,0.7197049525816649,"˜d
W l(xl −˜xl) +
1
p"
P,0.7207586933614331,"˜d
(W l −˜W l)˜xl + β(bl −˜bl) 2 ≤
1
p ˜d"
P,0.7218124341412012,"W l
2
xl −˜xl
2 +
1
p ˜d"
P,0.7228661749209695,"W l −˜W l
2"
P,0.7239199157007377,"˜xl
2 + β
bl −˜bl
2"
P,0.7249736564805058,"≤3
xl −˜xl
2 + (M2 + β)
θ −˜θ

2
(∀l ≥1)
xl −˜xl
2 =
σ(hl) −σ(˜hl)

2 ≤L0
hl −˜hl
2
(∀l ≥1) (84)"
P,0.726027397260274,"By induction, there exists an M5 > 0 such that
xl −˜xl
2 ≤M5
θ −˜θ

2 for all l."
P,0.7270811380400422,"For αl, we have αL+1 = ˜αL+1 = 1, and for all l ≥1,"
P,0.7281348788198103,"αl −˜αl
2 ="
P,0.7291886195995785,diag( ˙σ(hl))W l⊤ p
P,0.7302423603793466,"˜d
αl+1 −diag( ˙σ(˜hl))
˜W l⊤
p"
P,0.7312961011591148,"˜d
˜αl+1

2 ≤"
P,0.7323498419388831,diag( ˙σ(hl))W l⊤ p
P,0.7334035827186512,"˜d
(αl+1 −˜αl+1) 2
+"
P,0.7344573234984194,diag( ˙σ(hl))(W l −˜W l)⊤ p
P,0.7355110642781876,"˜d
˜αl+1

2 +"
P,0.7365648050579557,"diag(( ˙σ(hl) −˙σ(˜hl)))
˜W l⊤
p"
P,0.7376185458377239,"˜d
˜αl+1

2"
P,0.7386722866174921,"≤3L0
αl+1 −˜αl+1
2 +

M3L0 ˜d−1/2 + 3M3M5L1 ˜d−1/4 θ −˜θ

2
(85)"
P,0.7397260273972602,"where L1 is the Lipschitz constant of ˙σ. Particularly, for l = L, though ˜αL+1 = 1, since
 ˜W L
2 ≤"
P,0.7407797681770284,"3 ˜d1/4, (85) is still true. By induction, there exists an M6 > 0 such that
αl −˜αl
2 ≤M6 4√ ˜d"
P,0.7418335089567967,"θ −˜θ

2
for all l ≥1 (note that this is also true for l = L + 1)."
P,0.7428872497365648,"Thus, if (81) and (82) are true, then for all θ, ˜θ ∈B(θ(0), C0), any x such that ∥x∥2 ≤1, we have"
P,0.743940990516333,"∇W 0f(x) −∇˜
W 0 ˜f(x)

2 =
1
√d0"
P,0.7449947312961012,"xα1⊤−x ˜α1⊤
2"
P,0.7460484720758693,"≤
1
√d0"
P,0.7471022128556375,"α1 −˜α1
2"
P,0.7481559536354057,"≤
1
√d0 M6"
P,0.7492096944151738,4p ˜d
P,0.7502634351949421,"θ −˜θ

2 (86)"
P,0.7513171759747103,"and for l = 1, · · · , L, we have"
P,0.7523709167544784,Under review as a conference paper at ICLR 2022
P,0.7534246575342466,"∇W lf(x) −∇˜
W l ˜f(x)

2 =
1
p ˜d"
P,0.7544783983140148,"xlαl+1⊤−˜xl ˜αl+1⊤
2 ≤
1
p ˜d"
P,0.7555321390937829," xl
2
αl+1 −˜αl+1
2 +
xl −˜xl
2
 ˜αl+1
2
 ≤ M2M6"
P,0.7565858798735511,4p
P,0.7576396206533192,"˜d
+ M5M3
p ˜d"
P,0.7586933614330874,"! θ −˜θ

2 (87)"
P,0.7597471022128557,"Moreover, for any l = 0, · · · , L, there is
∇blf(x) −∇˜bl ˜f(x)

2 = β
αl+1 −˜αl+1
2 ≤βM6"
P,0.7608008429926238,4p ˜d
P,0.761854583772392,"θ −˜θ

2
(88)"
P,0.7629083245521602,"Overall, we can see that there exists a constant M7 > 0 such that
∇θf(x) −∇˜θ ˜f(x)

2 ≤"
P,0.7639620653319283,"M7
√n·
4√ ˜d"
P,0.7650158061116965,"θ −˜θ

2, so that
J(θ) −J(˜θ)

F ≤M7 4√ ˜d"
P,0.7660695468914647,"θ −˜θ

2."
P,0.7671232876712328,"B.5
PROOF OF THEOREM 6"
P,0.768177028451001,"Let η1 = min{η0, η∗}, where η0 is deﬁned in Corollary 3 and η∗is deﬁned in Theorem 5. Let
f (t)
lin (x) and f (t)
linERM(x) be the linearized neural networks of f (t)(x) and f (t)
ERM(x), respectively. By
Theorem 5, for any δ > 0, there exists ˜D > 0 and a constant C such that



 

"
P,0.7692307692307693,"sup
t≥0"
P,0.7702845100105374,"f (t)
lin (x) −f (t)(x)
 ≤C ˜d−1/4"
P,0.7713382507903056,"sup
t≥0"
P,0.7723919915700738,"f (t)
linERM(x) −f (t)
ERM(x)
 ≤C ˜d−1/4
(89)"
P,0.7734457323498419,"By Corollary 3, we have
lim
t→∞"
P,0.7744994731296101,"f (t)
lin (x) −f (t)
linERM(x)
 = 0
(90)"
P,0.7755532139093783,Summing the above yields
P,0.7766069546891464,"lim sup
t→∞"
P,0.7776606954689147,"f (t)(x) −f (t)
ERM(x)
 ≤2C ˜d−1/4
(91)"
P,0.7787144362486829,which is the result we want.
P,0.779768177028451,"B.6
PROOF OF THEOREM 7"
P,0.7808219178082192,"To minimize the regularized risk (12) with gradient descent, the update rule is"
P,0.7818756585879874,"θ(t+1) = θ(t) −η n
X"
P,0.7829293993677555,"i=1
q(t)
i ∇θℓ(f (t)(xi), yi) −ηµ(θ(t) −θ(0))
(92)"
P,0.7839831401475237,"We can see that under the new rule, θ(t) −θ(0) ∈span(∇θf (0)(x1), · · · , ∇θf (0)(xn)) is still true
for all t. Let θ∗be the interpolator in span(∇θf (0)(x1), · · · , ∇θf (0)(xn)), then the empirical risk
of θ is
1
2n
Pn
i=1⟨θ −θ∗, ∇θf (0)(xi)⟩2 =
1
2n
∇θf (0)(X)⊤(θ −θ∗)
2
2. Thus, there exists T > 0
such that for any t ≥T,
∇θf (0)(X)⊤(θ(t) −θ∗)

2"
P,0.7850368809272918,"2 ≤2nϵ
(93)"
P,0.78609062170706,"Let the smallest singular value of
1
√n∇θf (0)(X) be smin, and we have smin > 0. Note that the"
P,0.7871443624868283,"column space of ∇θf (0)(X) is exactly span(∇θf (0)(x1), · · · , ∇θf (0)(xn)). Deﬁne H ∈Rp×n"
P,0.7881981032665965,Under review as a conference paper at ICLR 2022
P,0.7892518440463646,"such that its columns form an orthonormal basis of this subspace, then there exists G ∈Rn×n such
that ∇θf (0)(X) = HG, and the smallest singular value of
1
√nG is also smin. Since θ(t) −θ(0)"
P,0.7903055848261328,"is also in this subspace, there exists v ∈Rn such that θ(t) −θ∗= Hv. Then we have
√"
P,0.7913593256059009,"2nϵ ≥
G⊤H⊤Hv

2 =
G⊤v

2. Thus, ∥v∥2 ≤
√"
P,0.7924130663856691,"2ϵ
smin , which implies
θ(t) −θ∗
2 ≤ √"
P,0.7934668071654373,"2ϵ
smin
(94)"
P,0.7945205479452054,"By Corollary 3, if we minimize the unregularized risk with ERM, then θ always converges to the
interpolator θ∗. So for any t ≥T and any test point x such that ∥x∥2 ≤1,"
P,0.7955742887249737,"|f (t)
linreg(x) −f (t)
linERM(x)| = |⟨θ(t) −θ∗, ∇θf (0)(x)⟩| ≤M0
√"
P,0.7966280295047419,"2ϵ
smin
(95)"
P,0.79768177028451,which implies (14).
P,0.7987355110642782,"B.7
PROOF OF THEOREM 8"
P,0.7997892518440464,"First of all, with some simple linear algebra analysis, we can prove the following proposition:
Proposition 13. For any positive deﬁnite symmetric matrix H ∈Rn×n, denote its largest and
smallest eigenvalues by λmax and λmin. Then, for any q ∈Rn
+ and Q = diag(q1, · · · , qn), HQ
has n positive eigenvalues that are all in [mini qi · λmin, maxi qi · λmax]."
P,0.8008429926238145,"Proof.
H is a positive deﬁnite symmetric matrix, so there exists A ∈Rn×n such that H = A⊤A,
and A is full-rank. First, any eigenvalue of AQA⊤is also an eigenvalue of A⊤AQ and vice
versa, because for any eigenvalue λ of AQA⊤we have some v ̸= 0 such that AQA⊤v = λv.
Multiplying both sides by A⊤on the left yields A⊤AQ(A⊤v) = λ(A⊤v) which implies that λ
is also an eigenvalue of A⊤AQ because A⊤v ̸= 0 as λv ̸= 0. We can prove the other direction
similarly."
P,0.8018967334035827,"Second, by condition we know that the eigenvalues of A⊤A are all in [λmin, λmax] where λmin > 0,
which implies for any unit vector v, v⊤A⊤Av ∈[λmin, λmax], which is equivalent to ∥Av∥2 ∈
[
√"
P,0.8029504741833509,"λmin,
√"
P,0.804004214963119,"λmax]. Thus, we have v⊤A⊤QAv ∈[λmin mini qi, λmax maxi qi], which implies that
the eigenvalues of A⊤QA are all in [λmin mini qi, λmax maxi qi]."
P,0.8050579557428873,"Thus, the eigenvalues of HQ = A⊤AQ are all in [λmin mini qi, λmax maxi qi]."
P,0.8061116965226555,"Now return back to the proof of Theorem 8. We still use the shorthand (40). With L2 penalty, the
update rule of the reweighting algorithm for the neural network is:"
P,0.8071654373024236,"θ(t+1) = θ(t) −ηJ(θ(t))Q(t)g(θ(t)) −ηµ(θ(t) −θ(0))
(96)"
P,0.8082191780821918,And the update rule for the linearized neural network is:
P,0.80927291886196,"θ(t+1)
lin
= θ(t)
lin −ηJ(θ(0))Q(t)glin(θ(t)) −ηµ(θ(t)
lin −θ(0))
(97)"
P,0.8103266596417281,"First, we need to prove that there exists D0 such that for all ˜d ≥D0, supt≥0
θ(t) −θ(0)
2 is
bounded with high probability. Denote at = θ(t) −θ(0). By (96) we have"
P,0.8113804004214963,at+1 =(1 −ηµ)at −η[J(θ(t)) −J(θ(0))]Q(t)g(θ(t))
P,0.8124341412012644,"−ηJ(θ(0))Q(t)[g(θ(t)) −g(θ(0))] −ηJ(θ(0))Q(t)g(θ(0))
(98)"
P,0.8134878819810326,which implies
P,0.8145416227608009,"∥at+1∥2 ≤
(1 −ηµ)I −ηJ(θ(0))Q(t)J(˜θ(t))⊤
2 ∥at∥2"
P,0.815595363540569,"+ η
J(θ(t)) −J(θ(0))

F"
P,0.8166491043203372,"g(θ(t))

2 + η
J(θ(0))

F"
P,0.8177028451001054,"g(θ(0))

2 (99)"
P,0.8187565858798735,"where ˜θ(t) is some linear interpolation between θ(t) and θ(0). Our choice of η ensures that ηµ < 1.
Similar to (48), we can show that for any δ > 0, there exists a constant R0 > 0 such that with"
P,0.8198103266596417,Under review as a conference paper at ICLR 2022
P,0.8208640674394099,"probability at least (1 −δ/3),
g(θ(0))

2 < R0. Let M be as deﬁned in Lemma 11. Denote
A = ηMR0, and let C0 =
4A
ηµ in Lemma 116. By Lemma 11, there exists D1 such that for all
˜d ≥D1, with probability at least (1 −δ/3), (47) is true."
P,0.821917808219178,"Now we prove by induction that ∥at∥2 < C0. It is true for t = 0, so we need to prove that if
∥at∥2 < C0, then ∥at+1∥2 < C0."
P,0.8229715489989463,"For the ﬁrst term on the right-hand side of (99), we have
(1 −ηµ)I −ηJ(θ(0))Q(t)J(˜θ(t))⊤
2 ≤(1 −ηµ)
I −
η
1 −ηµJ(θ(0))Q(t)J(θ(0))⊤

2"
P,0.8240252897787145,"+ η
J(θ(0))

F"
P,0.8250790305584826,"J(˜θ(t)) −J(θ(0))

F (100)"
P,0.8261327713382508,"Like what we have done before, we can show that all non-zero eigenvalues of J(θ(0))Q(t)J(θ(0))⊤"
P,0.827186512118019,"are eigenvalues of J(θ(0))⊤J(θ(0))Q(t). This is because for any λ ̸= 0, if J(θ(0))Q(t)J(θ(0))⊤v =
λv, then J(θ(0))⊤J(θ(0))Q(t)(J(θ(0))⊤v) = λ(J(θ(0))⊤v), and J(θ(0))⊤v ̸= 0 since λv ̸=
0, so λ is also an eigenvalue of J(θ(0))⊤J(θ(0))Q(t).
On the other hand, by Theorem 4,
J(θ(0))⊤J(θ(0))Q(t) converges in probability to ΘQ(t) whose eigenvalues are all in [0, λmax]
by Proposition 13.
So there exists D2 such that for all ˜d ≥D2, with probability at least
(1 −δ/3), the eigenvalues of J(θ(0))Q(t)J(θ(0))⊤are all in [0, λmax + λmin] for all t. Since
η/(1 −ηµ) ≤(λmin + λmax)−1 by our choice of η, we have
I −
η
1 −ηµJ(θ(0))Q(t)J(θ(0))⊤

2
≤1
(101)"
P,0.8282402528977871,"On the other hand, we can use (47) since ∥at∥2 < C0, so
J(θ(0))

F"
P,0.8292939936775553,"J(˜θ(t)) −J(θ(0))

F ≤ M 2 4√"
P,0.8303477344573235,"˜dC0. Therefore, there exists D3 such that for all ˜d ≥D3,"
P,0.8314014752370916,"(1 −ηµ)I −ηJ(θ(0))Q(t)J(˜θ(t))⊤
2 ≤1 −ηµ"
P,0.8324552160168599,"2
(102)"
P,0.8335089567966281,"For the second term, we have
g(θ(t))

2 ≤
g(θ(t)) −g(θ(0))

2 +
g(θ(0))

2"
P,0.8345626975763962,"≤
J(˜θ(t))

2"
P,0.8356164383561644,"θ(t) −θ(0)
2 + R0 ≤MC0 + R0
(103)"
P,0.8366701791359326,"And for the third term, we have"
P,0.8377239199157007,"η
J(θ(0))

F"
P,0.8387776606954689,"g(θ(0))

2 ≤ηMR0 = A
(104)"
P,0.839831401475237,"Thus, we have"
P,0.8408851422550052,"∥at+1∥2 ≤

1 −ηµ 2"
P,0.8419388830347735,"
∥at∥2 + ηM(MC0 + R0)"
P,0.8429926238145417,4p
P,0.8440463645943098,"˜d
+ A
(105)"
P,0.845100105374078,"So there exists D4 such that for all ˜d ≥D4, ∥at+1∥2 ≤
 
1 −ηµ"
P,0.8461538461538461,"2

∥at∥2 + 2A. This shows that if
∥at∥2 < C0 is true, then ∥at+1∥2 < C0 will also be true."
P,0.8472075869336143,"In conclusion, by union bound, we have proved that for any δ > 0, with probability at least (1 −δ)
for all ˜d ≥D0 = max{D1, D2, D3, D4},
θ(t) −θ(0)
2 < C0 is true for all t. This also implies
that for C1 = MC0 + R0, we have
g(θ(t))

2 ≤C1 for all t by (103)."
P,0.8482613277133825,"6Note that Lemma 11 only depends on the network structure and does not depend on the update rule, so we
can use this lemma here."
P,0.8493150684931506,Under review as a conference paper at ICLR 2022
P,0.8503688092729189,"Second, let ∆t = θ(t)
lin −θ(t). Then we have"
P,0.8514225500526871,"∆t+1 −∆t = η(J(θ(t))Q(t)g(θ(t)) −J(θ(0))Q(t)glin(θ(t)) −µ∆t)
(106)"
P,0.8524762908324552,which implies
P,0.8535300316122234,"∆t+1 =
h
(1 −ηµ)I −ηJ(θ(0))Q(t)J(˜θ(t))⊤i
∆t + η(J(θ(t)) −J(θ(0)))Q(t)g(θ(t))
(107)"
P,0.8545837723919916,"By (102), with probability at least (1 −δ) for all ˜d ≥D0, we have"
P,0.8556375131717597,"∥∆t+1∥2 ≤
(1 −ηµ)I −ηJ(θ(0))Q(t)J(˜θ(t))⊤
2 ∥∆t∥2 + η
J(θ(t)) −J(θ(0))

F"
P,0.8566912539515279,"g(θ(t))

2"
P,0.8577449947312961,"≤

1 −ηµ 2"
P,0.8587987355110642,"
∥∆t∥2 + η M"
P,0.8598524762908325,4p
P,0.8609062170706007,"˜d
C0C1 (108)"
P,0.8619599578503688,"Again, as ∆0 = 0, we can prove by induction that for all t,"
P,0.863013698630137,∥∆t∥2 < 2MC0C1
P,0.8640674394099052,"µ
˜d−1/4
(109)"
P,0.8651211801896733,"For any test point x such that ∥x∥2 ≤1, we have
f (t)
reg (x) −f (t)
linreg(x)
 =
f(x; θ(t)) −flin(x; θ(t)
lin )"
P,0.8661749209694415,"≤
f(x; θ(t)) −flin(x; θ(t))
 +
flin(x; θ(t)) −flin(x; θ(t)
lin )"
P,0.8672286617492097,"≤
f(x; θ(t)) −flin(x; θ(t))
 +
∇θf(x; θ(0))

2"
P,0.8682824025289779,"θ(t) −θ(t)
lin

2"
P,0.8693361433087461,"≤
f(x; θ(t)) −flin(x; θ(t))
 + M ∥∆t∥2 (110)"
P,0.8703898840885143,"For the ﬁrst term, note that
(
f(x; θ(t)) −f(x; θ(0)) = ∇θf(x; ˜θ(t))(θ(t) −θ(0))"
P,0.8714436248682824,"flin(x; θ(t)) −flin(x; θ(0)) = ∇θf(x; θ(0))(θ(t) −θ(0))
(111)"
P,0.8724973656480506,"where ˜θ(t) is some linear interpolation between θ(t) and θ(0). Since f(x; θ(0)) = flin(x; θ(0)),
f(x; θ(t)) −flin(x; θ(t))
 ≤
∇θf(x; ˜θ(t)) −∇θf(x; θ(0))

2"
P,0.8735511064278187,"θ(t) −θ(0)
2 ≤M"
P,0.8746048472075869,4p
P,0.8756585879873551,"˜d
C2
0
(112)"
P,0.8767123287671232,"Thus, we have shown that for all ˜d ≥D0, with probability at least (1 −δ) for all t and all x,
f (t)
reg (x) −f (t)
linreg(x)
 ≤

MC2
0 + 2M 2C0C1 µ"
P,0.8777660695468915,"
˜d−1/4 = O( ˜d−1/4)
(113)"
P,0.8788198103266597,"Given that ˆR(f (t)
linreg) < ϵ for sufﬁciently large t, this also implies that
 ˆR(f (t)
linreg) −ˆR(f (t)
reg )
 = O( ˜d−1/4√ϵ + ˜d−1/2)
(114)"
P,0.8798735511064278,"So for a ﬁxed ϵ, there exists D > 0 such that for all d ≥D, for sufﬁciently large t,"
P,0.880927291886196,"ˆR(f (t)
reg ) < ϵ ⇒ˆR(f (t)
linreg) < 2ϵ
(115)"
P,0.8819810326659642,"By Theorem 5, we have"
P,0.8830347734457323,"sup
t≥0"
P,0.8840885142255005,"f (t)
linERM(x) −f (t)
ERM(x)
 = O( ˜d−1/4)
(116)"
P,0.8851422550052687,Combining Theorem 7 with (113) and (116) derives
P,0.8861959957850368,"lim sup
t→∞"
P,0.8872497365648051,"f (t)
reg (x) −f (t)
ERM(x)
 = O( ˜d−1/4 + √ϵ)
(117)"
P,0.8883034773445733,Letting ˜d →∞leads to the result we need.
P,0.8893572181243414,Under review as a conference paper at ICLR 2022
P,0.8904109589041096,"C
A NOTE ON THE PROOFS IN LEE ET AL. (2019)"
P,0.8914646996838778,"We have mentioned that the proofs in Lee et al. (2019), particularly the proofs of their Theorem 2.1
and Lemma 1 in their Appendix G, are ﬂawed. In order to ﬁx their proof, we change the network
initialization to (9). In this section, we will demonstrate what goes wrong in the proofs in Lee et al.
(2019), and how we manage to ﬁx the proof. For clarity, we are referring to the following version of
the paper: https://arxiv.org/pdf/1902.06720v4.pdf."
P,0.8925184404636459,"To avoid confusion, in this section we will still use the notations used in our paper."
P,0.8935721812434141,"C.1
THEIR PROBLEMS"
P,0.8946259220231823,"Lee et al. (2019) claimed in their Theorem 2.1 that under the conditions of our Theorem 5, for any
δ > 0, there exist ˜D > 0 and a constant C such that for any ˜d ≥˜D, with probability at least (1−δ),
the gap between the output of a sufﬁciently wide fully-connected neural network and the output of
its linearized neural network at any test point x can be uniformly bounded by"
P,0.8956796628029505,"sup
t≥0"
P,0.8967334035827187,"f (t)(x) −f (t)
lin (x)
 ≤C ˜d−1/2
(claimed)
(118)"
P,0.8977871443624869,"where they used the original NTK formulation and initialization in Jacot et al. (2018):


 
"
P,0.898840885142255,hl+1 = W l
P,0.8998946259220232,"√dl
xl + βbl"
P,0.9009483667017913,"xl+1 = σ(hl+1)
and"
P,0.9020021074815595,"(
W l(0)
i,j
∼N(0, 1)"
P,0.9030558482613277,"bl(0)
i
∼N(0, 1)
(∀l = 0, · · · , L)
(119)"
P,0.9041095890410958,"where x0 = x and f(x) = hL+1. However, in their proof in their Appendix G, they did not directly
prove their result for the NTK formulation, but instead they proved another result for the following
formulation which they called the standard formulation:"
P,0.9051633298208641,"(
hl+1 = W lxl + βbl"
P,0.9062170706006323,"xl+1 = σ(hl+1)
and 
 "
P,0.9072708113804004,"W l(0)
i,j
∼N(0, 1 dl
)"
P,0.9083245521601686,"bl(0)
i
∼N(0, 1)
(∀l = 0, · · · , L)
(120)"
P,0.9093782929399368,"See their Appendix F for the deﬁnition of their standard formulation. In the original formulation,
they also included two constants σw and σb for standard deviations, and for simplicity we omit
these constants here. Note that the outputs of the NTK formulation and the standard formulation at
initialization are actually the same. The only difference is that the norm of the weight W l and the
gradient of the model output with respect to W l are different for all l."
P,0.9104320337197049,"In their Appendix G, they claimed that if a network with the standard formulation is trained by
minimizing the squared loss with gradient descent and learning rate η′ = η/ ˜d, where η is our
learning rate in Theorem 5 and also their learning rate in their Theorem 2.1, then (118) is true for
this network, so it is also true for a network with the NTK formulation because the two formulations
have the same network output. And then they claimed in their equation (S37) that applying learning
rate η′ to the standard formulation is equivalent to applying the following learning rates"
P,0.9114857744994731,"ηl
W =
dl
dmax
η
and
ηl
b =
1
dmax
η
(121)"
P,0.9125395152792413,"to W l and bl of the NTK formulation, where dmax = max{d0, · · · , dL}."
P,0.9135932560590094,"To avoid confusion, in the following discussions we will still use the NTK formulation and initial-
ization if not stated otherwise."
P,0.9146469968387777,"Problem 1.
Claim (121) is true, but it leads to two problems. The ﬁrst problem is that ηl
b =
O(d−1
max) since η = O(1), while their Theorem 2.1 needs the learning rate to be O(1). Nevertheless,
this problem can be simply ﬁxed by modifying their standard formulation as hl+1 = W lxl+β√dlbl"
P,0.9157007376185459,"where bl(0)
i
∼N(0, d−1
l
). The real problem that is non-trivial to ﬁx is that by (121), there is"
P,0.916754478398314,Under review as a conference paper at ICLR 2022
P,0.9178082191780822,"η0
W =
d0
dmax η. However, note that d0 is a constant since it is the dimension of the input space, while
dmax goes to inﬁnity. With that being said, in (121) they were essentially using a very small learning
rate for the ﬁrst layer W 0 but a normal learning rate for the rest of the layers, which deﬁnitely does
not match with their claim in their Theorem 2.1."
P,0.9188619599578504,"Problem 2.
Another big problem is that the proof of their Lemma 1 in their Appendix G is erro-
neous, and consequently their Theorem 2.1 is unsound as it heavily depends on their Lemma 1. In
their Lemma 1, they claimed that for some constant M > 0, for any two models with the parameters
θ and ˜θ such that θ, ˜θ ∈B(θ(0), C0) for some constant C0, there is
J(θ) −J(˜θ)

F ≤M
p ˜d"
P,0.9199157007376185,"θ −˜θ

2
(claimed)
(122)"
P,0.9209694415173867,"Note that the original claim in their paper was
J(θ) −J(˜θ)

F ≤M
p"
P,0.9220231822971549,"˜d
θ −˜θ

2. This is because
they were proving this result for their standard formulation. Compared to the standard formulation,
in the NTK formulation θ is
p"
P,0.9230769230769231,"˜d times larger, while the Jacobian J(θ) is
p"
P,0.9241306638566913,"˜d times smaller. This
is also why here we have θ, ˜θ ∈B(θ(0), C0) instead of θ, ˜θ ∈B(θ(0), C0 ˜d−1/2) for the NTK
formulation. Therefore, equivalently they were claiming (122) for the NTK formulation."
P,0.9251844046364595,"However, their proof of (122) in incorrect. Speciﬁcally, the right-hand side of their inequality (S86)
is incorrect. Using the notations in our Appendix B.4, their (S86) essentially claimed that"
P,0.9262381454162276,"αl −˜αl
2 ≤M
p ˜d"
P,0.9272918861959958,"θ −˜θ

2
(claimed)
(123)"
P,0.928345626975764,"for any θ, ˜θ ∈B(θ(0), C0), where αl = ∇hlhL+1 and ˜αl is the same gradient for the second"
P,0.9293993677555321,"model. Note that their (S86) does not have the
p"
P,0.9304531085353003,"˜d in the denominator which appears in (123). This
is because for their standard formulation, θ is
p"
P,0.9315068493150684,"˜d times smaller than the original NTK formulation,
while
αl
2 has the same order in the two formulations because all hl are the same."
P,0.9325605900948367,"However, it is actually impossible to prove (123). Consider the following counterexample: Since θ
and ˜θ are arbitrarily chosen, we can choose them such that they only differ in bl
1 for some 1 ≤l < L.
Then,
θ −˜θ

2 =
bl
1 −˜bl
1
. We can see that hl+1 and ˜hl+1 only differ in the ﬁrst element, and
hl+1
1
−˜hl+1
1
 =
β(bl
1 −˜bl
1)
. Moreover, we have W l+1 = ˜W l+1, so there is"
P,0.9336143308746049,αl+1 −˜αl+1 =diag( ˙σ(hl+1))W l+1⊤ p
P,0.934668071654373,"˜d
αl+2 −diag( ˙σ(˜hl+1))
˜W l+1⊤ p"
P,0.9357218124341412,"˜d
˜αl+2"
P,0.9367755532139094,"=
h
diag( ˙σ(hl+1)) −diag( ˙σ(˜hl+1))
i W l+1⊤ p"
P,0.9378292939936775,"˜d
αl+2"
P,0.9388830347734457,+ diag( ˙σ(˜hl+1))W l+1⊤ p
P,0.9399367755532139,"˜d
(αl+2 −˜αl+2) (124)"
P,0.9409905163329821,"Then we can lower bound
αl+1 −˜αl+1
2 by"
P,0.9420442571127503,"αl+1 −˜αl+1
2 ≥ "
P,0.9430979978925185,"h
diag( ˙σ(hl+1)) −diag( ˙σ(˜hl+1))
i W l+1⊤ p"
P,0.9441517386722866,"˜d
αl+2

2 −"
P,0.9452054794520548,diag( ˙σ(˜hl+1))W l+1⊤ p
P,0.946259220231823,"˜d
(αl+2 −˜αl+2) 2 (125)"
P,0.9473129610115911,"The ﬁrst term on the right-hand side is equal to

h
˙σ(hl+1
1
) −˙σ(˜hl+1
1
)
i
⟨W l+1
1
/
p"
P,0.9483667017913593,"˜d, αl+2⟩
 where"
P,0.9494204425711275,"W l+1
1
is the ﬁrst row of W l+1. We know that
W l+1
1

2 = Θ
p"
P,0.9504741833508957,"˜d

with high probability as its"
P,0.9515279241306639,Under review as a conference paper at ICLR 2022
P,0.9525816649104321,"elements are sampled from N(0, 1), and in their (S85) they claimed that
αl+2
2 = O(1), which
is true. In addition, they assumed that ˙σ is Lipschitz. Hence, we can see that"
P,0.9536354056902002,"h
diag( ˙σ(hl+1)) −diag( ˙σ(˜hl+1))
i W l+1⊤ p"
P,0.9546891464699684,"˜d
αl+2

2
= O
hl+1
1
−˜hl+1
1


= O
θ −˜θ

2  (126)"
P,0.9557428872497366,"On the other hand, suppose that claim (123) is true, then
αl+2 −˜αl+2
2 = O

˜d−1/2 θ −˜θ

2 
."
P,0.9567966280295047,"Then we can see that the second term on the right-hand side is O

˜d−1/2 θ −˜θ

2"
P,0.9578503688092729,"
because
W l+1
2 = O(
p"
P,0.958904109589041,"˜d) and ˙σ(x) is bounded by a constant as σ is Lipschitz. Thus, for a very large ˜d,
the second-term is an inﬁnitely small term compared to the ﬁrst term, so we can only prove that
αl+1 −˜αl+1
2 = O
θ −˜θ

2"
P,0.9599578503688093,"
(127)"
P,0.9610115911485775,"which is different from (123) because it lacks a critical ˜d−1/2 and thus leads to a contradiction.
Hence, we cannot prove (123) with the ˜d−1/2 factor, and consequently we cannot prove (122) with
the
p"
P,0.9620653319283456,"˜d in the denominator on the right-hand side. As a result, their Lemma 1 and Theorem 2.1
cannot be proved without this critical ˜d−1/2. Similarly, we can also construct a counterexample
where θ and ˜θ only differ in the ﬁrst row of some W l."
P,0.9631190727081138,"C.2
OUR FIXES"
P,0.964172813487882,"Regarding Problem 1, we can still use an O(1) learning rate for the ﬁrst layer in the NTK formulation
given that ∥x∥2 ≤1. This is because for the ﬁrst layer, we have"
P,0.9652265542676501,"∇W 0f(x) =
1
√d0
x0α1⊤=
1
√d0
xα1⊤
(128)"
P,0.9662802950474183,"For all l ≥1, we have
xl
2 = O( ˜d1/2). However, for l = 0, we instead have
x0
2 = O(1).
Thus, we can prove that the norm of ∇W 0f(x) has the same order as the gradient with respect to
any other layer, so there is no need to use a smaller learning rate for the ﬁrst layer."
P,0.9673340358271865,"Regarding Problem 2, in our formulation (8) and initialization (9), the initialization of the last layer
of the NTK formulation is changed from the Gaussian initialization W L(0)
i,j
∼N(0, 1) to the zero"
P,0.9683877766069547,"initialization W L(0)
i,j
= 0. Now we show how this modiﬁcation solves Problem 2."
P,0.9694415173867229,"The main consequence of changing the initialization of the last layer is that (81) becomes different:
instead of
W L
2 ≤3
p"
P,0.9704952581664911,"˜d, we now have
W L
2 ≤C0 ≤3
4p"
P,0.9715489989462592,"˜d. In fact, for any r ∈(0, 1/2), we
can prove that
W L
2 ≤3 ˜dr for sufﬁciently large ˜d. In our proof we choose r = 1/4."
P,0.9726027397260274,"Consequently, instead of
αl
2 ≤M3, we can now prove that
αl
2 ≤M3 ˜dr−1/2 for all l ≤L by"
P,0.9736564805057956,"induction. So now we can prove
αl −˜αl
2 = O

˜dr−1/2 θ −˜θ

2"
P,0.9747102212855637,"
instead of O
θ −˜θ

2"
P,0.9757639620653319,"
,
because"
P,0.9768177028451,"• For l < L, we now have
αl+1
2 = O( ˜dr−1/2) instead of O(1), so we can have the
additional ˜dr−1/2 factor in the bound.
• For l = L, although
αL+1
2 = 1, note that
W L
2 now becomes O( ˜dr) instead of
O( ˜d1/2), so again we can decrease the bound by a factor of ˜dr−1/2."
P,0.9778714436248683,"Then, with this critical ˜dr−1/2, we can prove the approximation theorem with the form"
P,0.9789251844046365,"sup
t≥0"
P,0.9799789251844047,"f (t)(x) −f (t)
lin (x)
 ≤C ˜dr−1/2
(129)"
P,0.9810326659641728,"for any r ∈(0, 1/2), though we cannot really prove the O( ˜d−1/2) bound as originally claimed in
(118). So this is how we solve Problem 2."
P,0.982086406743941,Under review as a conference paper at ICLR 2022
P,0.9831401475237092,"One caveat of changing the initialization to zero initialization is whether we can still safely assume
that λmin > 0 where λmin is the smallest eigenvalue of Θ, the kernel matrix of our new formulation.
The answer is yes. In fact, in our Theorem 4 we proved that Θ is non-degenerated (which means
that Θ(x, x′) still depends on x and x′), and under the overparameterized setting where dL ≫n,
chances are high that Θ is full-rank. Hence, we can still assume that λmin > 0."
P,0.9841938883034773,"As a ﬁnal remark, one key reason why we need to initialize W L as zero is that the dimension of the
output space (i.e. the dimension of hL+1) is ﬁnite, and in our case it is 1. Suppose we allow the
dimension of hL+1 to be ˜d which goes to inﬁnity, then using the same proof techniques, for the NTK
formulation we can prove that supt
hL+1(t) −hL+1(t)
lin

2 ≤C, i.e. the gap between two vectors
of inﬁnite dimension is always bounded by a ﬁnite constant. This is the approximation theorem we
need for the inﬁnite-dimensional output space. However, when the dimension of the output space is
ﬁnite, supt
hL+1(t) −hL+1(t)
lin

2 ≤C no longer sufﬁces, so we need to decrease the order of the"
P,0.9852476290832455,norm of W L in order to obtain a smaller bound.
P,0.9863013698630136,"D
EXPERIMENT DETAILS AND ADDITIONAL EXPERIMENTS"
P,0.9873551106427819,"D.1
EXPERIMENT DETAILS"
P,0.9884088514225501,"All experiments are conducted on a Ubuntu 18.04.6 machine with NVIDIA Geforce GTX 1080ti
GPUs. Each model is trained with one GPU. On each of Waterbirds and CelebA, we use a ResNet18
as the model. The model is trained with SGD with momentum = 0.9. On Waterbirds the learning
rate is 10−4, and on CelebA it is 10−3. For Group DRO, ν is selected as 0.01 (see the deﬁnition of
ν in (3)). The batch size used for Waterbirds is 128, and for CelebA it is 400. Data augmentation
including random cropping, random horizontal ﬂip and normalization is performed on both datasets."
P,0.9894625922023182,"D.2
SAMPLE WEIGHTS CONVERGE IN GROUP DRO"
P,0.9905163329820864,"The results in Section 3 require Assumption 1 which states that each sample weight q(t)
i
converges
to some positive value as t →∞. Our readers might wonder how strong this assumption is, and
whether reweighting algorithms satisfy this assumption in practice. In this section we empirically
demonstrate that for Group DRO, the dynamic reweighting algorithm we experiment on, this as-
sumption is satisﬁed on Waterbirds and CelebA."
P,0.9915700737618546,"Recall that in Section 2.2 we empirically showed that reweighting algorithms could easily overﬁt
without regularization. Here using the same experimental settings, we keep track of the weight of
each group gk during training, and we plot the group weight curves in Figure 3. We also train the
models longer (1000 epochs on Waterbirds and 300 epochs on CelebA). Clearly we can see that as
the training accuracy converges to 100%, the group weights also converge to an equilibrium. Note
that q(t)
i
= g(t)
k /nk for all zi ∈Dk, so the sample weights also converge."
P,0.9926238145416227,"0
200
400
600
800
1000
Epochs 0 0.1 0.2 0.3 0.4 0.5"
P,0.9936775553213909,Group Weight
P,0.9947312961011591,(a) Waterbirds.
P,0.9957850368809273,"0
100
200
300
Epochs 0 0.2 0.4 0.6"
P,0.9968387776606955,Group Weight
P,0.9978925184404637,(b) CelebA.
P,0.9989462592202318,"Figure 3: Weights of each group in Group DRO on Waterbirds and CelebA. The four curves corre-
spond to the four groups."
