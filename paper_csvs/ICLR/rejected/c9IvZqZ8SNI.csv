Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002109704641350211,"From learning to play the piano to speaking a new language, reusing and recom-
bining previously acquired representations enables us to master complex skills and
easily adapt to new environments. Inspired by the Gestalt principle of grouping by
proximity and theories of chunking in cognitive science, we propose a hierarchi-
cal chunking model (HCM). HCM learns representations from non-i.i.d sequential
data from the ground up by ﬁrst discovering the minimal atomic sequential units
as chunks. As learning progresses, a hierarchy of chunk representation is acquired
by chunking previously learned representations into more complex representations
guided by sequential dependence. We provide learning guarantees on an idealized
version of HCM, and demonstrate that HCM learns meaningful and interpretable
representations in visual, temporal, visual-temporal domains and language data.
Furthermore, the interpretability of the learned chunks enables ﬂexible transfer be-
tween environments that share partial representational structure. Taken together,
our results show how cognitive science in general and theories of chunking in
particular could inform novel and more interpretable approaches to representation
learning."
INTRODUCTION,0.004219409282700422,"1
INTRODUCTION"
INTRODUCTION,0.006329113924050633,"The last decade has witnessed a meteoric rise in the abilities of artiﬁcial systems, in particular deep
learning models (LeCun et al., 2015). From beating the world champion of Go (Silver et al., 2016)
to predicting the structure of the human proteome (Jumper et al., 2021), deep learning models have
accomplished impressive end-to-end learning achievements. Yet, researchers were quick to point out
shortcomings (Marcus, 2018; Lake et al., 2017). Two such shortcomings concern the hierarchical
structure and interpretability of the representations learned by neural network architectures. Specif-
ically, deep learning models suffer from a lack of interpretability since ANNs are sub-symbolic,
nested, non-linear structures. This means they the way predictions are generated can be difﬁcult to
understand (Samek et al., 2017; Ribeiro et al., 2016; Doshi-Velez & Kim, 2017). Furthermore, deep
learning models can also struggle to learn hierarchical representations altogether (Lake & Baroni,
2018; Fodor & Pylyshyn, 1988). To address these shortcomings, it has been suggested that machine
learning researchers could seek inspiration from cognitive science and construct models that resem-
ble the hierarchical and interpretable representations observed in human learners (Chollet, 2017;
Lake et al., 2015)."
INTRODUCTION,0.008438818565400843,"We take inspiration from the cognitive phenomenon of chunking and the Gestalt principle of group-
ing by proximity. To get an intuition for chunking, try to read through the following sequence of
letters: “DFJKJKJKDFDFJKJKDFDF”. Upon reaching the end, if you were tasked to repeat the
letters from memory, you might recall fragments of the sequence such as “DF” or “JK”. By parsing
the sequence of letters only once, one detects frequently occurring patterns within and memorize of
them together as units, i.e. chunks. Chunking has been observed in a range of sensory modalities.
We perceive units and structures when learning a language (Perruchet et al., 2014; McCauley &
Christiansen, 2017), in action sequences (Penhune & Steele, 2012; Rosenbaum et al., 1983), and in
visual structures Hinton (1979); Brady et al. (2009); Egan & Schwartz (1979). The extracted chunks
have been argued to facilitate memory compression (Gobet et al., 2001; Miller, 1956), compositional"
INTRODUCTION,0.010548523206751054,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.012658227848101266,"Figure 1: Schematic of the Hierarchical Chunking Model. a) Example of a hierarchical model
generating training sequences. b) Intermediate representation of learned marginal and transition
matrices. The most frequent transition that violates the independence testing criterion is marked in
red and can be turned into a new chunk. c) HCM combines the two chunks cL and cR to form a new
chunk. d) As HCM observes longer sequences, it gradually learns a hierarchical representation of
chunks. e) HCM arrives at the ﬁnally chunk hierarchy isomorphic to the generative hierarchy."
INTRODUCTION,0.014767932489451477,"generalization (Schulz et al., 2017), predictive processing (Koch & Hoffmann, 2000; Müssgens &
Ullén, 2015), and communication (Schulz et al., 2020)."
INTRODUCTION,0.016877637130801686,"In the current work, we propose a hierarchical chunking model (HCM) that learns chunks from
non-i.i.d sequential data with a hierarchical structure. HCM starts out learning an atomic set of
chunks to explain the sequence and gradually combines them into increasingly larger and more
complex chunks, thereby learning interpretable hierarchical structures. The output of the model
is a dynamical graph that is a trace of the evolving representation. The resulting representations
are therefore easy to interpret, and ﬂexibly reusable (e.g. we can choose to re-use speciﬁc parts).
We derive learning guarantees on an idealized generative model and demonstrate convergence on
sequential data coming from this generative model. Furthermore, we show that our model can beneﬁt
from having learned previous structures with shared components, leading to ﬂexible transfer and
generalization. Finally, we demonstrate that HCM can return interpretable representation in discrete
sequential, visual, and language domains, in several small-scale experiments. Taken together, our
results show how cognitive science in general and theories of chunking and grouping by proximity
in particular can inform novel and more interpretable approaches to representation learning."
HIERARCHICAL CHUNKING MODEL,0.0189873417721519,"2
HIERARCHICAL CHUNKING MODEL"
HIERARCHICAL CHUNKING MODEL,0.02109704641350211,"When talking about chunks in a sequence, a chunk is deﬁned as a unit created by concatenating sev-
eral atomic sequential units together. Take the training sequence shown in Figure 1a as an example.
The sequence consists of discrete, size-one atomic units from an atomic alphabet A0: in this case
A0 = {0, 1, 2}. A chunk c is made up of a combination of one or more atomic units in A0\{0}. 0
denotes an empty observation in the sequence."
HIERARCHICAL CHUNKING MODEL,0.023206751054852322,"Intuitively, if the training sequence contains inherent hierarchical structure, then there are patterns
which span several sequential units sharing these internal structures, like repeated melodies and sub-
melodies in a piece of music. If the pattern occurs in the sequence, observations between sequential
units within the pattern will be correlated. In this case, chunking patterns within a sequence as
units simpliﬁes perceptual processing in the sense that the sequence can be perceived one chunk at"
HIERARCHICAL CHUNKING MODEL,0.02531645569620253,Under review as a conference paper at ICLR 2022
HIERARCHICAL CHUNKING MODEL,0.027426160337552744,"a time, instead of one sequential unit at a time. Furthermore, the acquired “primary” chunks serve
as building blocks to discover larger chunks that are embedded within the intricate hierarchy of the
sequential structure."
HIERARCHICAL CHUNKING MODEL,0.029535864978902954,"More formally, HCM acquires a belief set B of chunks, and uses chunks from the belief set
to parse the sequence one chunk at a time.
HCM assumes that a sequence is generated from
samples of independently occurring chunks with probability of PB(c) evaluated on the belief set
B.
The probability of observing a sequence of parsed chunks c1, c2, ..., cN can be denoted as
P(c1, c2, ..., cN) = Q"
HIERARCHICAL CHUNKING MODEL,0.03164556962025317,"ci∈B PB(ci). Chunks as perceiving units serve as independent factors that
disentangle observations in the sequence."
HIERARCHICAL CHUNKING MODEL,0.03375527426160337,"From the beginning of the sequence, HCM ‘perceives’ sequential units one chunk at a time, in other
words, the training sequence is parsed by HCM into chunks. At every parsing step, the longest chunk
in the belief set that is consistent with the upcoming sequence is chosen to explain the up-coming
sequential observations. The end of the previous chunk parse initiates the next parse."
HIERARCHICAL CHUNKING MODEL,0.035864978902953586,"Observing a hierarchically structured sequence as illustrated in Figure 1a, HCM gradually builds up
a hierarchy of chunks starting from an empty belief set. It ﬁrst identiﬁes a set of atomic chunks to
construct its initial belief set B. Initially, these will be chunks of length one, yielding one-by-one
processing of the primitive elements."
HIERARCHICAL CHUNKING MODEL,0.0379746835443038,"For one belief set B, HCM keeps track of the marginal parsing frequency M(ci) for each chunk ci
in B, a vector with size |B| and the transition frequency T between chunk ci followed by chunk cj,
as illustrated in Figure 1b. Entries in M and T are used to test the hypothesis that consecutive chunk
parses have a correlated consecutive occurrence within the sequence. If two chunks cL and cR have
a signiﬁcant adjacency dependence based on their entries in M and T , they are chunked together to
become cL ⊕cR, which augments the belief set B by one. One example of chunk merging is shown
in Figure 1c."
HIERARCHICAL CHUNKING MODEL,0.04008438818565401,"There are two different versions of HCM. The Rational Chunk Learning HCM produces chunks
in an idealized way which we use to study learning guarantees. The online version of HCM is an
approximation to the rational HCM that can be adapted to different environments and processes
sequences online. Pseudo-code for both algorithms can be found in Algorithm 1 and 2 in the SI."
HIERARCHICAL CHUNKING MODEL,0.04219409282700422,"Rational Chunk Learning: HCM as an Ideal Observer The model is initiated with an empty
belief set, and it ﬁrst ﬁnds a minimally complete belief set after the ﬁrst sequence parse. In each
iteration, the entire sequence is parsed to evaluate M and T , which are used to ﬁnd consecutive
chunk parses in the existing belief set fulﬁlling the dependence testing criterion. From these depen-
dent chunk pairs, the pair with the largest estimated joint probability is combined into a new chunk.
The new chunk enlarges the belief set by one. The chunks in the new belief set are used to parse the
sequence in the next iteration. This process repeats until all of the chunks in the belief set pass an
independence testing criterion."
HIERARCHICAL CHUNKING MODEL,0.04430379746835443,"Online Chunk Learning The online chunk learning HCM approximates the ideal observer HCM
by learning new chunks when the training sequence is processed online. To have a feature that
encourages adaptation to new environmental statistics, entries in M and T can be subject to memory
decay. We use the ideal observer model to demonstrate learning guarantees, but use the online model
to learn representations in realistic and more complex set-ups."
HCM BUILDS REPRESENTATIONS FROM THE GROUND UP,0.046413502109704644,"2.1
HCM BUILDS REPRESENTATIONS FROM THE GROUND UP"
HCM BUILDS REPRESENTATIONS FROM THE GROUND UP,0.04852320675105485,"As HCM learns from a sequence, it starts with no representation and gradually builds up the rep-
resentations which can be readily interpreted at any learning stage. The representation can be de-
scribed by a chunk hierarchy graph ˆG with the vertex set being the chunks and edges pointing from
chunk constituents to the constructed chunks. Shown in Figure 1d is the gradual build-up of one
such chunk hierarchy graph as the model learns from a training sequence coming from the genera-
tive hierarchy displayed in Figure 1a. At t = 10, HCM learns only the atomic chunks, at t = 60,
HCM has already constructed two additional chunks; when t = 100, two more additional chunks
are constructed based on the previously acquired chunks, and at t = 150, HCM arrives at the ﬁnal
chunk hierarchy."
HCM BUILDS REPRESENTATIONS FROM THE GROUND UP,0.05063291139240506,Under review as a conference paper at ICLR 2022
HCM BUILDS REPRESENTATIONS FROM THE GROUND UP,0.052742616033755275,"Figure 2: a) Example graph generated from the hierarchical generative model with a depth of d = 3.
b) Learning performance of HCM and RNN with increasing training length and for different depths.
Performance was averaged over 30 randomly-generated graphs."
HIERARCHICAL GENERATIVE MODEL,0.05485232067510549,"3
HIERARCHICAL GENERATIVE MODEL"
HIERARCHICAL GENERATIVE MODEL,0.056962025316455694,"To study learning guarantee of the HCM, a generative model with a random hierarchy of chunks is
constructed. The relation between chunks and their constructive components in the generative model
is described by a chunk hierarchy graph Gd with vertex set VAd and edge set EAd. One example is
illustrated in Figure 1a. Ad is the set of chunks used to construct the sequence. The depth d speciﬁes
the number of chunks created in the generative process."
HIERARCHICAL GENERATIVE MODEL,0.05907172995780591,"Starting with an initial set of atomic chunks A0, at the i-th iteration, two chunks cL, cR are randomly
chosen from the current set of chunks Ai and are concatenated into a new chunk cL⊕cR, augmenting
Ai by one to Ai+1. Meanwhile, an independent occurrence probability is assigned to each chunk
under the condition that the probability of occurrence for every new chunk ci in the construction
process evaluated on the support set Ai carries the largest probability mass."
HIERARCHICAL GENERATIVE MODEL,0.06118143459915612,"To generate a sequence from a constructed hierarchical graph, chunks are sampled independently
from the set of all chunks and appended after each other, under the constraint that no two chunks
with a child chunk should be sampled consecutively."
LEARNING GUARANTEE,0.06329113924050633,"3.1
LEARNING GUARANTEE"
LEARNING GUARANTEE,0.06540084388185655,"Theorem: As the length of the sequence approaches inﬁnity, HCM learns a hierarchical chunking
graph ˆG isomorphic to the generative hierarchical graph G."
LEARNING GUARANTEE,0.06751054852320675,"Proof Sketch: We approach this proof by induction. Further details can be found in SI. Base step: The
ﬁrst step of the rational chunking algorithm is to ﬁnd the minimally complete atomic set of chunks
to form its initial belief set. This procedure guarantees that ˆG0 = G0. Additionally, the probability
mass of the learning model at step 0 and the generative model at step 0 is asymptotically the same
as the sequence length approaches inﬁnity. Induction hypothesis: Assume that the learned belief
set Bi at step i contains the same chunks as the alphabet set Ai in the generative model, the chunk
combination pair with the biggest evaluated joint occurrence probability violating the independence
test is picked to be concatenated into a chunk to extend the belief set: this chunk is the same chunk
node created by the hierarchical generative model. End step: The chunk learning process stops once
the independence criterion is passed. This is the case once the chunk learning algorithm has learned
a belief set Bd = Ad."
LEARNING CONVERGENCE,0.06962025316455696,"3.2
LEARNING CONVERGENCE"
LEARNING CONVERGENCE,0.07172995780590717,"HCM’s learning performance with increasing sequence length is evaluated and shown in Figure 2.
For this, HCM was trained on random graph hierarchies constructed from the hierarchical generative
model over 3000 trials while also varying the depth d. Figure 2a displays an example of a random
generative hierarchy with a depth of d = 3. We used the Kullback-Leibler divergence to evaluate
the deviation of HCM’s learned representations from the generative hierarchical model. This was
done by using HCM’s representation to produce “imagined” sequences, which were then compared
to the ground truth probability for each chunk in the used generative model. Figure 2b shows the
KL-divergence between the learned and ground-truth distribution for different depths d of the gener-
ative graphs. For each depth, the KL-divergence is evaluated on 30 random generative models with
sequence length increasing from 50 to 3000 in each model. Overall, the KL-divergence decreased as
the length of training sequence increased and converged with larger training sequences. This shows
empirically that HCM learns representations bearing closer resemblance to the generative model."
LEARNING CONVERGENCE,0.07383966244725738,Under review as a conference paper at ICLR 2022
LEARNING CONVERGENCE,0.0759493670886076,"Figure 3: a) Example of a representation learned by an HCM. b) An environment facilitative to the
learned representation. Gray shadows mark the chunks that can be directly transferred. c) Average
performance over the ﬁrst 500 trials after environment change in the facilitative environment. d) In-
terfering environment. Gray shadows marks chunks that the learned representation needs to acquire
from scratch. e) Average performance over the ﬁrst 500 trials after environment changes into an the
interfering environment."
LEARNING CONVERGENCE,0.07805907172995781,"We used the same sequences to train a 3-layer Recurrent Neural Network (RNN) with 40 hidden
units and the same method to measure the KL-divergence by using “imagined” sequences produced
by the RNN evaluated on the support set of the generative hierarchy. As the length of the train-
ing sequence increased, the KL-divergence also converged but at a much slower rate. Note that
HCM’s competitive advantage increased further as the depth of the generative hierarchy increased.
Thus, HCM learns quickly about the hierarchical generative model and does so faster than a neural
network."
"HCM PERMITS TRANSFER BETWEEN ENVIRONMENTS WITH OVERLAPPING
STRUCTURE",0.08016877637130802,"3.3
HCM PERMITS TRANSFER BETWEEN ENVIRONMENTS WITH OVERLAPPING
STRUCTURE"
"HCM PERMITS TRANSFER BETWEEN ENVIRONMENTS WITH OVERLAPPING
STRUCTURE",0.08227848101265822,"After training on a sequence, HCM acquires an interpretable representation. Knowing what the
model has learned enables us to directly know what type of hierarchical environment would facilitate
or interfere with the learned representations. This is impossible to do using standard neural networks
because we generally would not know what representation they have learned, and therefore do not
have ﬁne-grained control over retaining or removing parts of it."
"HCM PERMITS TRANSFER BETWEEN ENVIRONMENTS WITH OVERLAPPING
STRUCTURE",0.08438818565400844,"More formally, two HCM models might have acquired different hierarchical chunking graphs Gi and
Gj from their past experience. These might lie on the graph construction path (G0, G1, G2, ..., Gd).
The HCM with a chunk hierarchy graph ‘closer’ to the ground truth chunk Gd on the path, takes
fewer iteration to arrive at Gd. This also applies when the chunk hierarchies starting out are not
along the graph construction path but only showing partial overlap."
"HCM PERMITS TRANSFER BETWEEN ENVIRONMENTS WITH OVERLAPPING
STRUCTURE",0.08649789029535865,"Similarly, the chunk hierarchy Gi learned by an HCM might facilitate its performance in a new
environment where Gi lies along the graph construction path to the true Gd, i.e. there is partial
overlap between the chunk hierarchies. We demonstrate this in Figure 3. Here, HCM was trained
on a random hierarchical generative model and acquired the representation shown in Figure 3a.
Knowing this representation, we also know that an environment with a hierarchical generative graph
as shown in Figure 3b allows for positive transfer, i.e. the HCM that already contains the chunk
hierarchy Figure 3a would learn faster than a naive one by transferring its previously learned chunks.
As shown in Figure 3c, the trained HCM learns faster than a naive HCM which had to learn about
the structure from scratch."
"HCM PERMITS TRANSFER BETWEEN ENVIRONMENTS WITH OVERLAPPING
STRUCTURE",0.08860759493670886,"Vice versa, we might have a situation in which transfer is detrimental. For example, there is no
overlap in the chunk hierarchy learned by the HCM in Figure 3a with the graph shown in Figure 3d.
The shaded chunks need to be learned anew by the HCM trained on Figure 3a, yet the previously
acquired chunks might mislead HCM causing it to adapt to the new environment more slowly. As a
result, the performance of the pre-trained HCM suffers more from an interfering environment than a
naive HCM (Figure 3e). This is similar to catastrophic interferences in neural networks (Sharkey &
Sharkey, 1995), where performance on one task interferes with others. The advantage of the HCM
is that with visibility into the chunks learned, we have a better a priori sense of whether transfer
will be facilitative or interfering. This means that we can examine whether to re-use the previously
learned chunks, or to start from scratch with a naive model."
"HCM PERMITS TRANSFER BETWEEN ENVIRONMENTS WITH OVERLAPPING
STRUCTURE",0.09071729957805907,Under review as a conference paper at ICLR 2022
"HCM PERMITS TRANSFER BETWEEN ENVIRONMENTS WITH OVERLAPPING
STRUCTURE",0.09282700421940929,"Figure 4: a) A designed visual hierarchical model where elementary components compose more
complex images. b) Initial, intermediate, and complex chunks learned by HCM trained on sequences
of images generated by the visual hierarchical model."
"GENERALIZING TO VISUAL TEMPORAL CHUNKS VIA THE PRINCIPLE OF
PROXIMAL GROUPING",0.0949367088607595,"4
GENERALIZING TO VISUAL TEMPORAL CHUNKS VIA THE PRINCIPLE OF
PROXIMAL GROUPING"
"GENERALIZING TO VISUAL TEMPORAL CHUNKS VIA THE PRINCIPLE OF
PROXIMAL GROUPING",0.0970464135021097,"Humans are not only able to identify sequential chunks but also to ﬁnd structure in visual-temporal
stimuli and group visual points as a whole. For example, despite the fact that our retina receives
pixel-wise visual inputs that vary across temporal slices, we are able to perceive complex moving
objects. The Gestalt principle of grouping by proximity states that objects that are close to one an-
other appear to form groups (Wagemans et al., 2012). This principle has been argued to play a key
role in human perceptual grouping (Compton & Logan, 1993) and beneﬁt working memory (Peter-
son & Berryhill, 2013) and the reduction of visual complexity (Donderi, 2006). Indeed, in humans
and other animals, learning of adjacent relationships prevails over non-adjacent ones (Malassis et al.,
2018). Therefore, the adjacent dependency structure can be seen as the primary driver of chunking
in visual temporal domains (Froyen et al., 2015). To emulate this ability of chunking via proximal
grouping in visual temporal perception, we extend HCM to also learn visual temporal chunks."
"GENERALIZING TO VISUAL TEMPORAL CHUNKS VIA THE PRINCIPLE OF
PROXIMAL GROUPING",0.09915611814345991,"Visual temporal chunks not only subsume temporal length but also varying visual slices in each tem-
poral slice. One can imagine visual temporal chunks as having a 3D shape — the ﬁrst two height
and width dimensions are the visual part of the chunk, the length of the object is the temporal part,
made of stacked visual-temporal pixels. Within each temporal slice are the visual features identiﬁed
by the chunk. Since nearby points are more likely to be in the same chunk, this assumption is an
implementation of the principle of grouping by proximity in the visual-temporal domain. As the
model iterates through data across its temporal slice, the chunk that attains the biggest visual tempo-
ral volume is chosen to explain parts of the visual-temporal observations. Multiple visual temporal
chunks can be identiﬁed to occur simultaneously. Starting at the visual temporal time point marked
by the previous chunk, chunks are identiﬁed and stored in M. The transition matrix T is modiﬁed
to account for the temporal lag-difference between adjacent chunk pairs and records the frequency
that one chunk transitions into another one with a given time-lag. A hypothesis test is conducted
every time when a pair of adjacent chunks are identiﬁed, chunks that violate the hypothesis test are
grouped together to augment the belief set which is then used to parse future sequences."
LEARNING PART-WHOLE RELATIONSHIP BETWEEN VISUAL COMPONENTS,0.10126582278481013,"4.1
LEARNING PART-WHOLE RELATIONSHIP BETWEEN VISUAL COMPONENTS"
LEARNING PART-WHOLE RELATIONSHIP BETWEEN VISUAL COMPONENTS,0.10337552742616034,"We let HCM learn chunks in a visual domain by learning from a sequence of images. To this end, a
hierarchical generative model in the pixel-wise image domain shown in Figure 4a was constructed
to test HCM’s visual chunking ability. Speciﬁcally, a set of elementary visual units in the lowest
level of the hierarchy are combined to construct intermediate and more complex visual units higher
up in the hierarchy. An empty image is included to denote pauses. All of the constructed elements
in the hierarchy occurred independently according to a draw from a Dirichlet ﬂat distribution. To
generate the sequence which was used to train HCM, images in the hierarchy were sampled from
the generative distribution and appended to the end of the sequence. In this way, despite the visual
correlations in each image described by the hierarchy, each image slice was temporally independent
from other slices."
LEARNING PART-WHOLE RELATIONSHIP BETWEEN VISUAL COMPONENTS,0.10548523206751055,Under review as a conference paper at ICLR 2022
LEARNING PART-WHOLE RELATIONSHIP BETWEEN VISUAL COMPONENTS,0.10759493670886076,"Figure 5: a) A GIF of a moving squid used as a sequence to train HCM. b) Examples of temporal
chunks learned by HCM. c) Examples of visual chunks learned by HCM."
LEARNING PART-WHOLE RELATIONSHIP BETWEEN VISUAL COMPONENTS,0.10970464135021098,"HCM learns a hierarchy of visual chunks from a sequence of visually correlated but temporally
independent images sampled from the visual hierarchical model. Shown in Figure 4b are the chunk
representations learned by HCM at different stages. Having no knowledge about the image parts
before starting to learn, HCM acquires the individual pixels as chunks to explain the observations.
As HCM proceeds with learning, visual correlations among the pixels are discovered and larger
chunks are formed. As the number of observations increases, the model learns more sophisticated
yet still interpretable chunks."
LEARNING VISUAL-TEMPORAL MOVEMENT HIERARCHIES,0.11181434599156118,"4.2
LEARNING VISUAL-TEMPORAL MOVEMENT HIERARCHIES"
LEARNING VISUAL-TEMPORAL MOVEMENT HIERARCHIES,0.11392405063291139,"Instead of seeing one image after another sampled from an independent, identically distributed dis-
tribution, real world experiences contain correlations in both the visual and temporal dimension.
From observing object movements across space and time, the visual system learns structures from
correlated visual and temporal observations, decomposes motion structure and groups moving ob-
jects together as a whole (Bill et al., 2020). To emulate this type of environment as a learning task, an
animated GIF of a squid swimming in the sea as shown in Figure 5a was used as a visual-temporal
sequence to train HCM. As learning advances, HCM learns chunks spanning both the visual and
temporal domain. Examples of such visual-temporal chunks are shown in Figure 5b and c. There
are visual-temporal chunks that mark movements of a tentacle and the rising-up motion of a bubble.
Additionally, there are visual chunks that resemble a part of the squid’s eye and face."
LEARNING CHUNKS FROM REALISTIC LANGUAGE DATA,0.1160337552742616,"5
LEARNING CHUNKS FROM REALISTIC LANGUAGE DATA"
LEARNING CHUNKS FROM REALISTIC LANGUAGE DATA,0.11814345991561181,"We so far have only shown demonstrations of HCM learning chunks on simple sequences. Thus,
one step further is to run HCM on real world datasets that contain complex hierarchical structures.
One immediate testbed containing such structures is natural language. To this end, we trained HCM
to learn chunks on the ﬁrst book of The Hunger Games."
LEARNING CHUNKS FROM REALISTIC LANGUAGE DATA,0.12025316455696203,"HCM starts with learning individual English letters and punctuation. After having seen 10,000
characters of the book, HCM acquires frequently used chunks that resemble common English pre-
and sufﬁxes such as “ity”, “ing” , “re”, and “ith”; deﬁnite and indeﬁnite articles such as “a”, “an”,
“the”; conjunctions such as “and”, “but”, “that”, and “as”; prepositions such as “of, “to”, “in”, “at”,
variants of “is”, “are”, “was”; and pronouns such as “he”, “my”, “me”, “we”, “she”."
LEARNING CHUNKS FROM REALISTIC LANGUAGE DATA,0.12236286919831224,"After having seen 100,000 characters of the book, HCM learns intermediate chunks that include
various commonly used verbs such as “come”, “sing”, “leave”, “try”; nouns such as “bed”, “day”,
“mother”; common word combinations such as “in the”, “lose to”, “the wood”. Additionally, HCM
has acquired words speciﬁc to The Hunger Games such as “prim”, “death”, “hunger”, and “district
12”."
LEARNING CHUNKS FROM REALISTIC LANGUAGE DATA,0.12447257383966245,"After having seen 300,000 characters of the book, HCM learns more complex phrases from the
previously learned words, such as the commonly used phrases “it is not just”, “in the school”, “our
district”, and “cause of the”."
LEARNING CHUNKS FROM REALISTIC LANGUAGE DATA,0.12658227848101267,Under review as a conference paper at ICLR 2022
LEARNING CHUNKS FROM REALISTIC LANGUAGE DATA,0.12869198312236288,"Table 1: Example Chunks Learned from The Hunger Games
Simple Chunks
’an’, ’in ’, ’be’, ’at’, ’me’, ’le’, ’a ’, ’ar’, ’re’, ’and ’, ’ve’, ’ing ’, ’on’,
’st’, ’se’, ’to ’, ’of ’, ’he’, ’my ’, ’te’, ’pe’, ’ou’, ’we’, ’ad’, ’de’, ’li’,
’the’, ’ce’, ’is’, ’as’, ’il’, ’ch’, ’al’, ’no’, ’she’, ’ing’, ’am’, ’ack’, ’we’,
’raw’, ’on the’, ’day’, ’ear’, ’oug’, ’bea’, ’tree’, ’sin’, ’that’, ’log’, ’ters’,
’wood’, ’now’, ’was’, ’even’, ’leven’, ’ater’, ’ever’, ’but’, ’ith’, ’ity’
Intermediate Chunks
’this’, ’pas’, ’eak’, ’if’, ’sing’, ’bed’, ’men’, ’raw’, ’day’, ’in the’,
’link’, ’for’, ’one’, ’the wood’, ’bell’, ’other’, ’...’, ’lose to’, ’hunger’,
’mother’, ’death’, ’would’, ’district 12’, ’try’, ’under’, ’prim’, ’beg’,
’then’, ’into’, ’gale’, ’read’, ’come’, ’he want’, ’leave’, ’where’, ’older’,
’says’, ’might’, ’dont’, ’add’, ’know’, ’man who’, ’of the’
Complex Chunks
’out of’, ’it out’, ’our district’, ’capitol’, ’reaping’, ’fair’, ’berries’, ’the
last’, ’ﬁsh’, ’again’, ’as well’, ’the square’, ’scomers’, ’fully’, ’, but the
’, ’in the school’, ’at the’, ’you can’, ’tribute’, ’to remember’, ’it is not
just’, ’I can’, ’peace’, ’feel’, ’you have to’, ’I know’, ’bother’, ’in our’,
’kill’, ’cause of the’, ’the pig’, ’to the baker’, ’I have’, ’what was’"
RELATED WORK,0.1308016877637131,"6
RELATED WORK"
RELATED WORK,0.13291139240506328,"Our model is successor to decades of work on different approaches to chunk learning. In cognitive
science, researchers have put forward models that produce qualitatively similar chunks as humans
learning a natural or artiﬁcial languages (Servan-Schreiber & Anderson, 1990; Perruchet & Vinter,
1998), as well as models that can extract chunks from visual inputs (Mareschal & French, 2016).
HCM can be viewed as a principled version of these earlier cognitive models because it uses hy-
pothesis testing in its decision to chunk elements together instead of using chunking heuristics.
Therefore, HCM comes with learning guarantees for a fairly general class of hierarchically struc-
tured environments. Additionally, HCM extends past cognitive models to the higher dimensional
domain of visual-temporal chunking."
RELATED WORK,0.1350210970464135,"The primary approach to chunk learning in the language domain were n-gram models, dating all the
way back to Shannon (1948). An n-gram model learns the marginal probabilities of all chunks of
size n. A major limitation of this approach is that the number of chunks grows exponentially as a
function of chunk length. For instance, with an alphabet of 26 letters, the number of possible 5-letter
words already goes beyond 10 million. Due to the large vocabulary size, building word-level n-gram
models is virtually unfeasible. To this end, a Bayesian non-parametric extension of n-gram models
has been put forward (Teh, 2006). A Bayesian non-parametric n-gram model builds up structure as
evidence is accumulated. That is, it ﬂexibly reduces to a 1-gram model if chunks are not present or
‘opens up’ higher and higher n-gram levels depending on the size of chunks. Teh (2006)’s model is
different from HCM in several regards. Instead of using the chunks to ‘look forward’ and parse the
sequence in large steps, it ‘looks back’ and predicts only one element conditioning on the context
of the previous elements. Then, instead of storing an explicit bag of chunks, it represents a chunk
distribution weighted by evidence. For prediction, it smooths over the evidence of all chunks that
are consistent with the current context. A shortcoming of Teh (2006)’s model and n-gram models
in general is that they do not leverage the concatenation process observed in humans but the chunks
are built up primitive element-wise."
RELATED WORK,0.1371308016877637,"Hierarchical hidden Markov models (Fine et al., 1998) were developed in a similar vein, extend-
ing hidden Markov models to be able to capture multi-scale sequential structure. Although these
models are able to capture larger patterns than non-hierarchical versions while maintaining the com-
putational tractability of simple Markov processes, they still lack the adaptive recombination and
reuse of pre-existing components. Fragment grammars (O’Donnell et al., 2009) address this by bal-
ancing the creation and re-use of chunks by Bayesian principles, while also preserving the symbolic
interpretability. However, fragment grammars are intractable and their inference is exceptionally
costly."
RELATED WORK,0.13924050632911392,"In the era of deep learning, both natural language processing and image processing became in-
creasingly dominated by neural networks, with one of their primary tasks being the extraction and
prediction of chunks (Zhai et al., 2017; Si et al., 2020; Ortmann, 2021). Commonly, these so-called"
RELATED WORK,0.14135021097046413,Under review as a conference paper at ICLR 2022
RELATED WORK,0.14345991561181434,"sequence chunks are used as units of segmentation for other downstream tasks such as text trans-
lation. However, in these approaches, the architecture needs to be pre-speciﬁed before training as
compared to HCM, where no such speciﬁcation is required. HCM can therefore be seen as an
interpretable and transparent alternative to neural networks for some of these tasks."
RELATED WORK,0.14556962025316456,"A ﬁnal related line of research attempts to learn explicit representations by inducing programs (Lake
et al., 2015). Fore example, in a recent approach to this challenge, the interpretable structures re-
turned by program induction algorithms was combined with a deep neural network to learn mean-
ingful representations from data (Ellis et al., 2021). Yet in these approaches the retrieved represen-
tations are highly dependent on the initial set of building blocks which are usually speciﬁed by the
experimenter. HCM, in comparsion, is task-agnostic and requires no primitive program description."
DISCUSSION,0.14767932489451477,"7
DISCUSSION"
DISCUSSION,0.14978902953586498,"We have proposed the Hierarchical Chunking Model (HCM) as a cognitively-inspired method for
learning representations from the ground up. HCM starts out by learning atomic units from se-
quential data which it then chunks together, gradually building up a hierarchical representation that
can be expressed as a dynamical and intepretable graph. We have provided learning guarantees for
an idealized version of HCM, shown how HCM’s interpretability can facilitate generalization, and
demonstrated that HCM learns meaningful representations from visual, temporal, visual-temporal,
and language data."
DISCUSSION,0.1518987341772152,"Although we have showcased HCM’s abilities across a set of diverse experiments, some challenges
remain. First of all, HCM is currently not computationally efﬁcient, such that we needed to run
the online version for most of the presented tasks. There are several directions that could enhance
HCM’s computational efﬁciency. One method to speed up learning could be to stitch multiple
chunks together in one decision. Another direction could be to have the chunk decision process
between all of the acquired chunks separated from the process of parsing observations, which could
be used for parallelized implementations. We believe that scaling HCM up will be beneﬁcial to learn
in increasingly more complex data sets than the ones we have applied here."
DISCUSSION,0.1540084388185654,"Secondly, all patterns of chunks in the current project came from adjacent events in the sequences.
This feature was inspired by the Gestalt principle of grouping by proximity. Yet many patterns ob-
served in natural data sets might exhibit non-adjacent dependencies in space or time. The adjacency
assumption therefore limits the model from detecting such patterns. How to relax the adjacency
assumption as a grouping criterion to allow for non-adjacent relationships to be chunked together
remains an open challenge. Furthermore, HCM currently assumes that there is a hierarchy of chunks
which occur independently in the observational sequence. This set-up was intended to be a simpli-
fying assumption for a ﬁrst approach toward building a cognitively plausible hierarchical chunking
model. Nonetheless, our approach can and should be extended to more sophisticated assumptions
such as higher order Markovian dependencies between chunks. Finally, HCM passes visual data
“as is” and does not take into consideration any additional assumptions about visual inputs such as
translation or rotational symmetries, which humans can detect when perceiving visual structures."
DISCUSSION,0.15611814345991562,"There are also several avenues for future investigations. One immediate step is to run HCM on other,
more complex data sets such as musical scores, neural data, and large natural language corpora, to
name but a few. Furthermore, we intend to not only run HCM on raw visual inputs directly but also
to employ neural networks to compress inputs into a latent space and then train HCM on these latent
dimensions (Franklin et al., 2020). Additionally, one could also use deep learning models to label
parts of objects and then train HCM on the movements of these labelled parts (Insafutdinov et al.,
2016). Finally, we are currently only testing for independence when deciding on whether or not to
chunk, although other statistical tests are conceivable. One general class of tests could be to assess
whether or not chunks are causally related with each other, in an attempt to ﬁnd the best causal
structure to explain the sequential, observational data Heinze-Deml et al. (2018). This would make
HCM a useful model of causal representation learning (Schölkopf et al., 2021)."
DISCUSSION,0.15822784810126583,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.16033755274261605,"8
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.16244725738396623,"Detailed information about the HCM algorithm, proof, generative model, independence test and
experimental details and results can be found in the supplementary information section. The code
used for the algorithm and experiments will be available as a comment to the reviewers and area
chairs as a link to an anonymous repository as soon as the discussion forum for all submitted papers
is open."
REFERENCES,0.16455696202531644,REFERENCES
REFERENCES,0.16666666666666666,"Johannes Bill, Hrag Pailian, Samuel J. Gershman, and Jan Drugowitsch. Hierarchical structure is
employed by humans during visual motion perception. Proceedings of the National Academy of
Sciences, 117(39):24581–24589, 2020. ISSN 0027-8424. doi: 10.1073/pnas.2008961117. URL
https://www.pnas.org/content/117/39/24581."
REFERENCES,0.16877637130801687,"Timothy F. Brady, Talia Konkle, and George A. Alvarez. Compression in Visual Working Mem-
ory: Using Statistical Regularities to Form More Efﬁcient Memory Representations. Journal of
Experimental Psychology: General, 138(4), 2009. ISSN 00963445. doi: 10.1037/a0016797."
REFERENCES,0.17088607594936708,"Francois Chollet. Deep Learning with Python. Manning Publications Co., USA, 1st edition, 2017.
ISBN 1617294438."
REFERENCES,0.1729957805907173,"Brian J Compton and Gordon D Logan. Evaluating a computational model of perceptual grouping
by proximity. Perception & Psychophysics, 53(4):403–421, 1993."
REFERENCES,0.1751054852320675,"Don C Donderi. Visual complexity: a review. Psychological bulletin, 132(1):73, 2006."
REFERENCES,0.17721518987341772,"Finale Doshi-Velez and Been Kim. Towards a rigorous science of interpretable machine learning.
arXiv preprint arXiv:1702.08608, 2017."
REFERENCES,0.17932489451476794,"Dennis E. Egan and Barry J. Schwartz. Chunking in recall of symbolic drawings. Memory &
Cognition, 7(2), 1979. ISSN 0090502X. doi: 10.3758/BF03197595."
REFERENCES,0.18143459915611815,"Kevin Ellis, Catherine Wong, Maxwell Nye, Mathias Sablé-Meyer, Lucas Morales, Luke Hewitt,
Luc Cary, Armando Solar-Lezama, and Joshua B. Tenenbaum. Dreamcoder: Bootstrapping in-
ductive program synthesis with wake-sleep library learning. In Proceedings of the 42nd ACM SIG-
PLAN International Conference on Programming Language Design and Implementation, PLDI
2021, pp. 835–850, New York, NY, USA, 2021. Association for Computing Machinery. ISBN
9781450383912.
doi: 10.1145/3453483.3454080.
URL https://doi.org/10.1145/
3453483.3454080."
REFERENCES,0.18354430379746836,"Shai Fine, Yoram Singer, and Naftali Tishby. The hierarchical hidden markov model: Analysis and
applications. Machine learning, 32(1):41–62, 1998."
REFERENCES,0.18565400843881857,"Jerry A. Fodor and Zenon W. Pylyshyn. Connectionism and cognitive architecture: A critical anal-
ysis. Cognition, 28(1-2):3–71, 1988. doi: 10.1016/0010-0277(88)90031-5."
REFERENCES,0.1877637130801688,"Nicholas T Franklin, Kenneth A Norman, Charan Ranganath, Jeffrey M Zacks, and Samuel J Ger-
shman. Structured event memory: A neuro-symbolic model of event cognition. Psychological
Review, 127(3):327, 2020."
REFERENCES,0.189873417721519,"Vicky Froyen, Jacob Feldman, and Manish Singh.
Bayesian hierarchical grouping: Perceptual
grouping as mixture estimation. Psychological Review, 122(4):575, 2015."
REFERENCES,0.19198312236286919,"Fernand Gobet, Peter C.R. Lane, Steve Croker, Peter C.H. Cheng, Gary Jones, Iain Oliver, and
Julian M. Pine. Chunking mechanisms in human learning. Trends in Cognitive Sciences, 5(6),
2001. ISSN 13646613. doi: 10.1016/S1364-6613(00)01662-4."
REFERENCES,0.1940928270042194,"Christina Heinze-Deml, Marloes H. Maathuis, and Nicolai Meinshausen. Causal Structure Learn-
ing. Annual Review of Statistics and Its Application, 2018. ISSN 2326-8298. doi: 10.1146/
annurev-statistics-031017-100630."
REFERENCES,0.1962025316455696,Under review as a conference paper at ICLR 2022
REFERENCES,0.19831223628691982,"Geoffrey Hinton.
Some demonstrations of the effects of structural descriptions in men-
tal imagery*.
Cognitive Science, 3(3):231–250, 1979.
doi:
https://doi.org/10.1207/
s15516709cog0303\_3.
URL https://onlinelibrary.wiley.com/doi/abs/10.
1207/s15516709cog0303_3."
REFERENCES,0.20042194092827004,"Eldar Insafutdinov, Leonid Pishchulin, Bjoern Andres, Mykhaylo Andriluka, and Bernt Schiele.
Deepercut: A deeper, stronger, and faster multi-person pose estimation model.
In European
Conference on Computer Vision, pp. 34–50. Springer, 2016."
REFERENCES,0.20253164556962025,"John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger,
Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al. Highly accurate
protein structure prediction with alphafold. Nature, 596(7873):583–589, 2021."
REFERENCES,0.20464135021097046,"Iring Koch and Joachim Hoffmann. Patterns, chunks, and hierarchies in serial reaction-time tasks.
Psychological Research, 63(1), 2000. ISSN 14302772. doi: 10.1007/PL00008165."
REFERENCES,0.20675105485232068,"B. Lake and Marco Baroni. Generalization without systematicity: On the compositional skills of
sequence-to-sequence recurrent networks. In ICML, 2018."
REFERENCES,0.2088607594936709,"B. Lake, R. Salakhutdinov, and J. Tenenbaum. Human-level concept learning through probabilistic
program induction. Science, 350:1332 – 1338, 2015."
REFERENCES,0.2109704641350211,"Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, and Samuel J. Gershman. Building
machines that learn and think like people. Behavioral and Brain Sciences, 40:e253, 2017. doi:
10.1017/S0140525X16001837."
REFERENCES,0.21308016877637131,"Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436–444,
2015."
REFERENCES,0.21518987341772153,"Raphaëlle Malassis, Arnaud Rey, and Joël Fagot. Non-adjacent dependencies processing in human
and non-human primates. Cognitive Science, 42(5):1677–1699, 2018."
REFERENCES,0.21729957805907174,"G. Marcus. Deep learning: A critical appraisal. ArXiv, abs/1801.00631, 2018."
REFERENCES,0.21940928270042195,"Denis Mareschal and Robert French. Tracx2: a connectionist autoencoder using graded chunks
to model infant visual statistical learning. Philosophical Transactions of the Royal Society B:
Biological Sciences, 372:20160057, 11 2016. doi: 10.1098/rstb.2016.0057."
REFERENCES,0.22151898734177214,"Stewart M McCauley and Morten H Christiansen.
Computational investigations of multiword
chunks in language learning. Topics in Cognitive Science, 9(3):637–652, 2017."
REFERENCES,0.22362869198312235,"George A. Miller. The magical number seven, plus or minus two: some limits on our capacity for
processing information. Psychological Review, 1956. ISSN 0033295X. doi: 10.1037/h0043158."
REFERENCES,0.22573839662447256,"Diana M Müssgens and Fredrik Ullén. Transfer in Motor Sequence Learning: Effects of Practice
Schedule and Sequence Context. 9(November), 2015. doi: 10.3389/fnhum.2015.00642."
REFERENCES,0.22784810126582278,"Timothy J O’Donnell, Joshua B Tenenbaum, and Noah D Goodman. Fragment grammars: Exploring
computation and reuse in language. 2009."
REFERENCES,0.229957805907173,"Katrin Ortmann. Chunking historical german. In NODALIDA, 2021."
REFERENCES,0.2320675105485232,"Virginia B. Penhune and Christopher J. Steele. Parallel contributions of cerebellar, striatal and M1
mechanisms to motor sequence learning, 2012. ISSN 01664328."
REFERENCES,0.23417721518987342,"Pierre Perruchet and Annie Vinter. Parser: A model for word segmentation. Journal of Mem-
ory and Language, 39(2):246 – 263, 1998.
ISSN 0749-596X.
doi: https://doi.org/10.1006/
jmla.1998.2576. URL http://www.sciencedirect.com/science/article/pii/
S0749596X98925761."
REFERENCES,0.23628691983122363,"Pierre Perruchet, Bénédicte Poulin-Charronnat, Barbara Tillmann, and Ronald Peereman.
New
evidence for chunk-based models in word segmentation. Acta psychologica, 149:1–8, 2014."
REFERENCES,0.23839662447257384,"Dwight J Peterson and Marian E Berryhill. The gestalt principle of similarity beneﬁts visual working
memory. Psychonomic bulletin & review, 20(6):1282–1289, 2013."
REFERENCES,0.24050632911392406,Under review as a conference paper at ICLR 2022
REFERENCES,0.24261603375527427,"Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ""why should i trust you?"": Explaining the
predictions of any classiﬁer. In Proceedings of the 22nd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, KDD ’16, pp. 1135–1144, New York, NY, USA,
2016. Association for Computing Machinery. ISBN 9781450342322. doi: 10.1145/2939672.
2939778. URL https://doi.org/10.1145/2939672.2939778."
REFERENCES,0.24472573839662448,"David A. Rosenbaum, Sandra B. Kenny, and Marcia A. Derr. Hierarchical control of rapid movement
sequences. Journal of Experimental Psychology: Human Perception and Performance, 1983.
ISSN 00961523. doi: 10.1037/0096-1523.9.1.86."
REFERENCES,0.2468354430379747,"Wojciech Samek, Thomas Wiegand, and Klaus-Robert Müller. Explainable artiﬁcial intelligence:
Understanding, visualizing and interpreting deep learning models. ITU Journal: ICT Discoveries
- Special Issue 1 - The Impact of Artiﬁcial Intelligence (AI) on Communication Networks and
Services, 1:1–10, 10 2017."
REFERENCES,0.2489451476793249,"Bernhard Schölkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner,
Anirudh Goyal, and Yoshua Bengio.
Toward causal representation learning.
Proceedings of
the IEEE, 109(5):612–634, 2021."
REFERENCES,0.2510548523206751,"Eric Schulz, Joshua B. Tenenbaum, David Duvenaud, Maarten Speekenbrink, and Samuel J. Ger-
shman. Compositional inductive biases in function learning. Cognitive Psychology, 2017. ISSN
00100285. doi: 10.1016/j.cogpsych.2017.11.002."
REFERENCES,0.25316455696202533,"Eric Schulz, Francisco Quiroga, and Samuel J Gershman. Communicating compositional patterns.
Open Mind, 4:25–39, 2020."
REFERENCES,0.2552742616033755,"Emile Servan-Schreiber and John Anderson. Learning artiﬁcial grammars with competitive chunk-
ing. Journal of Experimental Psychology: Learning, Memory, and Cognition, 16:592–608, 07
1990. doi: 10.1037/0278-7393.16.4.592."
REFERENCES,0.25738396624472576,"Claude Elwood Shannon. A mathematical theory of communication. The Bell system technical
journal, 27(3):379–423, 1948."
REFERENCES,0.25949367088607594,"Noel E Sharkey and Amanda JC Sharkey. An analysis of catastrophic interference. Connection
Science, 1995."
REFERENCES,0.2616033755274262,"Sun Si, Chenyan Xiong, Zhenghao Liu, Zhiyuan Liu, and Jie Bao. Joint keyphrase chunking and
salience ranking with bert, 04 2020."
REFERENCES,0.26371308016877637,"David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484–489, 2016."
REFERENCES,0.26582278481012656,"Yee Whye Teh. A hierarchical bayesian language model based on pitman-yor processes. In Proceed-
ings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting
of the Association for Computational Linguistics, pp. 985–992, 2006."
REFERENCES,0.2679324894514768,"Johan Wagemans, Jacob Feldman, Sergei Gepshtein, Ruth Kimchi, James R Pomerantz, Peter A
Van der Helm, and Cees Van Leeuwen. A century of gestalt psychology in visual perception: Ii.
conceptual and theoretical foundations. Psychological bulletin, 138(6):1218, 2012."
REFERENCES,0.270042194092827,"Feifei Zhai, Saloni Potdar, Bing Xiang, and Bowen Zhou. Neural models for sequence chunking. In
AAAI, 2017."
REFERENCES,0.2721518987341772,"A
INDEPENDENCE TEST"
REFERENCES,0.2742616033755274,"We use hypothesis testing on the assumption of independence to decide whether there exists an
association between consecutive parses of any two chunks cL and cR in the current belief set. If the
independence test is violated, then the two chunks are combined together. Another independence
test is used to evaluate if there are still statistical associations between chunk observations for each
possible chunks in the current belief set; this is used as a criterion to continue or halt the chunking
process."
REFERENCES,0.27637130801687765,Under review as a conference paper at ICLR 2022
REFERENCES,0.27848101265822783,"We use a χ2-test of independence to assess if the consecutive parses of cL and cR observed in T
violate the independence criterion. Observations of cL and cR in parses are categorical variables
and can be represented as rows and columns of a contingency table. The number of observations
that cL = 1 or any other observations (cL = 0) consists of the row entries, indicating observations
of cL, while the number of observations cR = 1 and cR = 0 make up the column entries. The table,
therefore, consists of two rows and two columns."
REFERENCES,0.2805907172995781,"The null hypothesis is that the occurrence of the consecutive observations is statistically indepen-
dent. Given the hypothesis of independence, the theoretical frequency for observing cL followed
by cR is E[cL = 1, cR = 1] = Np(cL = 1)p(cR = 1), with N being the total number of parses.
p(cL = 1) = N(cL)"
REFERENCES,0.28270042194092826,"N
= N(cL→cR)"
REFERENCES,0.2848101265822785,"N
+ N(cL→¬cR)"
REFERENCES,0.2869198312236287,"N
, p(cR = 1) = N(cR)"
REFERENCES,0.2890295358649789,"N
= N(cL→cR)"
REFERENCES,0.2911392405063291,"N
+ N(¬cL→cR) N"
REFERENCES,0.29324894514767935,"χ2 =
X"
REFERENCES,0.29535864978902954,"l={0,1} X"
REFERENCES,0.2974683544303797,"r={0,1}"
REFERENCES,0.29957805907172996,"N(cL = l, cR = r) −E[cL = l, cR = r]"
REFERENCES,0.30168776371308015,"E[cL = l, cR = r] = N
X"
REFERENCES,0.3037974683544304,"l={0,1} X"
REFERENCES,0.3059071729957806,"r={0,1}
p(cL = l)p(cR = r)(( N(cL=l,cR=r)"
REFERENCES,0.3080168776371308,"N
) −p(cL = l)p(cR = r)
p(cL = l)p(cR = r)
) (1)"
REFERENCES,0.310126582278481,"The degree of freedom for this test is 1. A χ2-probability of less than or equal to 0.05 is used as a
criterion for rejecting the hypothesis of independence."
REFERENCES,0.31223628691983124,"The independence test is also employed to evaluate whether there are still statistical associations
between chunk observations for each possible chunk in the current belief set, which we use as a
criterion to continue or to halt the chunking process. For this test, the contingency table contains
rows and columns corresponding to all possible chunks in the current belief set, and the χ2-statistic
is calculated as:"
REFERENCES,0.3143459915611814,"χ2 =
X cL∈sB X cR=sB"
REFERENCES,0.31645569620253167,"N(cL, cR) −E[cL, cR]"
REFERENCES,0.31856540084388185,"E[cL, cR]
= N
X cL∈sB X"
REFERENCES,0.3206751054852321,"cR=sB
p(cL)p(cR)("
REFERENCES,0.3227848101265823,"N(cL,cR)"
REFERENCES,0.32489451476793246,"N
−p(cL)p(cR)
p(cL)p(cR)
)"
REFERENCES,0.3270042194092827,"(2)
The degrees of freedom are (|sB| −1) ∗(|sB| −1), and a p-value of p ≤0.05 is again used as a
criterion to reject the null hypothesis and therefore as evidence to continue the chunking process."
REFERENCES,0.3291139240506329,"B
RATIONAL CHUNKING ALGORITHM"
REFERENCES,0.33122362869198313,"Algorithm 1: Rational Chunking Algorithm
input : Seq, maxIter
output: Bd, ˆGd, Td, Md
d ←0, iter ←0;
Bd, Md, Td = getSingleElementSets(Seq);
/* minimally complete atomic set
*/
while !IndependenceTest(Md, Td) or iter ≤maxiter do"
REFERENCES,0.3333333333333333,"Md, Td = Parse(Seq, Bd);
cL, cR ←None; MaxChunk, MaxChunkP ←None; PreCk = {};
for (ci, cj) ∈Bd\{0} × Bd\{0} do"
REFERENCES,0.33544303797468356,"Pd(ci ⊕cj) = CalculateJoint(Md, Td, ci, cj);
Pd+1(ci ⊕cj) =
Pd(ci⊕cj)
1−Pd(ci⊕cj);
if Pd+1(ci ⊕cj) ≥MaxChunkP and ci ⊕cj /∈PreCk and !IndependenceTest(ci, cj)
then"
REFERENCES,0.33755274261603374,"cL ←ci, cR ←cj;
MaxChunkP ←Pd+1(ci ⊕cj), MaxChunk ←ci ⊕cj
end
end
c ←cL ⊕cR; Bd+1 ←Bd ∪c; ˆGd+1 ←AugmentGraph( ˆGd, (cL, c), (cR, c));
PreCk.add(c);
end"
REFERENCES,0.339662447257384,Under review as a conference paper at ICLR 2022
REFERENCES,0.34177215189873417,"C
ONLINE HCM WITH GENERALIZATION TO VISUAL-TEMPORAL
SEQUENCES"
REFERENCES,0.3438818565400844,"HCM learns a chunk hierarchy graph ˆG by training on a visual-temporal sequence. The visual-
temporal chunks can retain various continuous shapes and may not ﬁll the entire space, which means
that there are no jumps from any visual temporal pixel within a chunk to another. Within a chunk,
there is always a path that connects any two visual temporal pixels."
REFERENCES,0.3459915611814346,"The input chunk hierarchy graph ˆG could be an empty graph which corresponds to the case that the
HCM model has not been trained yet, or it can be a chunk hierarchy graph that HCM has acquired
from training. M contains the frequency count of each chunk in the belief set B. T stores pairs of
parsed adjacent chunks and their difference in temporal lag. The temporal lag is the time difference
between the end of the previous chunk and start of the following chunk. The generative model of
the world assumed by the chunk learning model is such that each visual temporal chunk occurs
independently for each parse. It then keeps checking whether adjacent chunks in the visual and
temporal domain violate the independence testing criterion. If so, then the visual/temporal adjacent
chunks are grouped together. The constituent parts of a chunk remains in the belief set."
REFERENCES,0.34810126582278483,Algorithm 2: Visual-Temporal Chunking
REFERENCES,0.350210970464135,"input : Seq, ˆG, θ, DT
output: ˆG
M, T ←ˆG.M, ˆG.T;
PreviousChunkBoundaryRecord ←[];
/* Record Chunk Endings */
ChunkTerminationTime.setall(-1);
while Sequence not over do"
REFERENCES,0.35232067510548526,"CurrentChunks, ChunkTerminationTime =
IdentifyTheLatestChunks(ChunkTerminationTime);
ObservationToExplain ←refactor(Seq, ChunkTerminationTime);
for Chunk in CurrentChunks do"
REFERENCES,0.35443037974683544,for CandidateAdjacentChunk in PreviousChunkBoundaryRecord do
REFERENCES,0.35654008438818563,"if CheckAdjacency(Chunk, CandidateAdjacentChunk) then"
REFERENCES,0.35864978902953587,"M, T , B, ˆG ←LearnChunking(Chunk, CandidateAdjacentChunk.
M, T , B, ˆG);
end
end
ChunkTerminationTime.update(CurrentChunks)
end
PreviousChunkBoundaryRecord.add(CurrentChunks);
Forgetting(M, T , B, ˆG, θ, DT, PreviousChunkBoundaryRecord);
end"
REFERENCES,0.36075949367088606,"The pseudocode for the Visual-Temporal HCM is shown in Algorithm 2. The input can be a visual-
temporal sequence, an i.i.d visual sequence, or a temporal sequence."
REFERENCES,0.3628691983122363,"To process and update chunks online, HCM iterates through the visual temporal sequence, identiﬁes
chunks, and marks the termination time corresponding to each visual dimension stored in Chunk-
TerminationTime. ChunkTerminationTime is a matrix with its size being the same as the visual
dimension that stores the end point in each visual dimension that the previous chunk ﬁnished. Be-
cause chunks could ﬁnish at different time points in each visual temporal dimension, the algorithm
explain chunks starting at the end point in each visual temporal dimension when the last identiﬁed
chunk is ﬁnished up until the current time point to identify current, on-going chunks. The chunks
with the biggest visual temporal volume are prioritized to explain the relevant observations up to the
current time point. As multiple visual temporal chunks can be identiﬁed to occur simoutaneously,
CurrentChunks is a set that stores the identiﬁed chunk that has not reached its end point."
REFERENCES,0.3649789029535865,"Once one or several chunks are identiﬁed to be ending at a time point, they are stored inside the Pre-
viousChunkBoundaryRecord and their ﬁnishing time is updated for each visual pixel in ChunkTer-
minationTime. Corresponding entries in M are updated upon once a chunk has ended. The chunks"
REFERENCES,0.3670886075949367,Under review as a conference paper at ICLR 2022
REFERENCES,0.3691983122362869,"that ﬁnishes after the start of the current chunk is checked with each current chunk on whether there
is a visual temporal adjacency. Additionally, the independence test between adjacent chunk pairs
are carried out for the decision to possibly merge chunks."
REFERENCES,0.37130801687763715,"Once a pair of adjacent chunks cL cR violates the independence testing criterion, they are combined
into one chunk cL ⊕cR. A new entry is created in M with the estimated joint occurrence frequency
for cL ⊕cR, which is subtracted from the marginal record of cL and cR. The transition entries
associated with cL ⊕cR are initialized to be 1. Other chunks inherit the transition entries from cR
to other chunks."
REFERENCES,0.37341772151898733,At each time step the visual temporal chunking algorithm does the following things:
REFERENCES,0.3755274261603376,"1. Identiﬁes the chunks biggest in volume that explain observation in each dimension from
the time point when the last chunk ended to the current time point, mark their time point,
and store them in the set of current chunks.
2. Identiﬁes adjacent chunks in previous chunks with each of the currently ending chunk and
updates their marginal and transition counts.
3. Modiﬁes the set of chunks used to parse the sequence based on their adjacencies."
REFERENCES,0.37763713080168776,"Entries in M and T are subject to memory decay at the rate of θ. If any entry goes below the
deletion threshold DT, their corresponding entries in M, T , B and ˆG are deleted."
REFERENCES,0.379746835443038,"D
DETAILS TO THE PROOF OF RECOVERABILITY"
REFERENCES,0.3818565400843882,"D.1
DEFINITIONS"
REFERENCES,0.38396624472573837,"An observational sequence is made up of discrete, integer valued, size-one elementary observational
unit coming from an atomic alphabet set A0, where 0 represents the empty observation set."
REFERENCES,0.3860759493670886,One example of such an observational sequence S is:
REFERENCES,0.3881856540084388,010021002112000...
REFERENCES,0.39029535864978904,"The elementary observation units such as ‘0’, ‘1’, and ‘2’ come from the atomic alphabet set A0 =
{0, 1, 2}."
REFERENCES,0.3924050632911392,Deﬁnition 1 (Chunk)
REFERENCES,0.39451476793248946,"A chunk is composed of several non-empty, concatenated elementary observations, embedded in the
sequence. That is, a chunk can be made up from any combination of elements in A0 \ {0}"
REFERENCES,0.39662447257383965,"Examples of chunks from the observational sequence can be ‘1’, ‘21’, ‘211’, ‘12’, ‘2112’, ... etc. 0
represents an empty observation in the sequence."
REFERENCES,0.3987341772151899,Deﬁnition 2 (Belief Set)
REFERENCES,0.4008438818565401,"A belief set is the set of chunks that the HCM uses to parse training sequences. The belief set is
denoted as B."
REFERENCES,0.4029535864978903,"An example of a belief set that that the model has learned from S could be B
=
{0, 1, 21, 211, 12, 2112}."
REFERENCES,0.4050632911392405,Deﬁnition 3 (Parsing)
REFERENCES,0.40717299578059074,"The parsing of chunks initiates from the beginning of the sequence. At every parsing step, the biggest
chunk in the belief set that matches the upcoming sequence is chosen to explain the observation. The
end of the previous chunk parse initiates the next parse."
REFERENCES,0.4092827004219409,"The sequence S parsed by the model that uses the belief set {0, 1, 21, 211, 12, 2112} results in the
following partition. 0 1 0 0 21 0 0 2112 0 0 0."
REFERENCES,0.41139240506329117,"We say that a belief set is complete if at any point when the model parses the sequence, the upcoming
observations can be explained at least by one chunk within the belief set."
REFERENCES,0.41350210970464135,Under review as a conference paper at ICLR 2022
REFERENCES,0.41561181434599154,Figure 6: Illustration of Visual Temporal Chunks
REFERENCES,0.4177215189873418,"In this work, we only refer to complete belief sets."
REFERENCES,0.41983122362869196,Deﬁnition 4 (Parsing Length NB)
REFERENCES,0.4219409282700422,"A parsing length NB of a sequence parsed by a belief set B is the length of the parsing result after
the sequence has been parsed by chunks within B."
REFERENCES,0.4240506329113924,Deﬁnition 5 (Count Function NB(c))
REFERENCES,0.42616033755274263,"NB(c) denotes the count of how many times the chunk c in the belief set B appears in the parsed
sequence."
REFERENCES,0.4282700421940928,"The parsing length and the count function for all of the chunks c on a belief set B satisfy the following
relation:"
REFERENCES,0.43037974683544306,"NB =
X"
REFERENCES,0.43248945147679324,"c∈B
NB(c)
(3)"
REFERENCES,0.4345991561181435,Deﬁnition 6 (NB(x →y))
REFERENCES,0.43670886075949367,"The number of times chunk x is being parsed following the parse of chunk y. x and y are both
chunks in the belief set B."
REFERENCES,0.4388185654008439,"For any chunk x within any belief set B, one can relate NB(x) with NB(x →y) by:"
REFERENCES,0.4409282700421941,"NB(x) =
X"
REFERENCES,0.4430379746835443,"y∈B
NB(x →y)
(4)"
REFERENCES,0.4451476793248945,"When the length of the sequence becomes inﬁnite, it is easier to work with probabilities instead of
the count function. One can deﬁne a probability space over the belief set:"
REFERENCES,0.4472573839662447,Deﬁnition 7 (Probability space of a belief set)
REFERENCES,0.44936708860759494,"With a belief set B, one can deﬁne a associated probability space (SB, FB, PB). SB is the sample
space representing all of the possible outcomes of a chunk parse.
An event space F is the space for all possible sets of events. F contains all the subsets of SB.
Additionally, the probability function PAB : FB →R is deﬁned on the event space SB. The proba-
bility function PAB satisﬁes the basic axioms of probability:"
REFERENCES,0.45147679324894513,"• PAB(E) ≥0 ∀E ∈F. For any subset in the event space, the probability of an observation
being in the subset is positive."
REFERENCES,0.45358649789029537,"• M, N ∈F, and M ∩N = E, then P(M ∪N) = P(M)+P(N). For two non-intersecting
subsets in the event space, the probability of observing any element that falls within the
union of the two subsets is the sum of the probability of observing any event within one
subset and the probability of observing any event from the other subset."
REFERENCES,0.45569620253164556,• P(S) = 1. The probability of observing any event that belongs to the sample space is one.
REFERENCES,0.4578059071729958,Under review as a conference paper at ICLR 2022
REFERENCES,0.459915611814346,"The probability of a chunk c on a support set of chunks B, is the limiting case of this ratio when NB
goes to inﬁnity."
REFERENCES,0.4620253164556962,"PB(c) =
lim
NB→∞
NB(c)"
REFERENCES,0.4641350210970464,"NB
(5)"
REFERENCES,0.46624472573839665,"A learning model keeps track of the occurrence probability associated with each chunk in the be-
lief set. For a current belief set, the model assumes that the chunks within the belief set occurs
independently."
REFERENCES,0.46835443037974683,"The occurrence probability of chunk ci with the belief set Bd is PBd(ci), which refers to the nor-
malized frequency of observing chunk ci when the number of parsing the sequence using chunks
from the belief set goes to inﬁnity. In this way, the probability of observing a sequence of chunks
c1, c2, ....cN can be denoted as P(c1, c2, ....cN). The joint probability of observing any chunk in the
generative process is:"
REFERENCES,0.4704641350210971,"P(c1, c2, ....cN) =
Y"
REFERENCES,0.47257383966244726,"ci∈Bd
PBd(ci)
(6)"
REFERENCES,0.47468354430379744,"In this formulation, chunks as observation units serve as independent factors that disentangle obser-
vations in the sequence."
REFERENCES,0.4767932489451477,Deﬁnition 8 (Marginal Parsing Frequency Md)
REFERENCES,0.47890295358649787,A vector that stores the number of parses for each chunk c in the belief set Bd.
REFERENCES,0.4810126582278481,"Md contains size |Bd|. Additionally the model keeps track of the transition probability from one
chunk to another, as they are used to test whether two chunks have immediate temporal adjacency
association."
REFERENCES,0.4831223628691983,Deﬁnition 9 (Td)
REFERENCES,0.48523206751054854,The set of transition frequency from any chunk ci ∈Bd to cj ∈Bd
REFERENCES,0.4873417721518987,Deﬁnition 10 (Chunk Hierarchy Graph Gd)
REFERENCES,0.48945147679324896,"The relation between chunks and their constructive components in the generative model is described
by a chunk hierarchy graph Gd with vertex set VAd and edge set EAd. One example of a chunk
hierarchy graph is illustrated in Figure 1 a). In this hierarchical generative model, d is the depth of
the graph and Ad is the set of chunks used as atomic units to construct the sequence. Each vertex in
VAd is a chunk, and edges connect the parent chunk vertices to their child chunk vertices."
REFERENCES,0.49156118143459915,"As the belief set B keeps changing when one modiﬁes the chunks in a sequence, so does the parsing
length NB and the probability associated with the belief set PAB. Based on the deﬁnition of N
on how chunks are parsed when the support set changes, this change of N changes a set of
constraints on the probabilities deﬁned on the new, augmented support set. To do this, we
need to:"
REFERENCES,0.4936708860759494,• Formulate the deﬁnition of probability based on N.
REFERENCES,0.4957805907172996,• Identify all relevant changes of N before and after the chunk update.
REFERENCES,0.4978902953586498,• Translate this change of N to the constraints on probability updates.
REFERENCES,0.5,"We derive the relation between the probabilities when two chunks cL and cR ∈Ad are concatenated
together to form a new chunk cL ⊕cR and update the alphabet to Ad+1."
REFERENCES,0.5021097046413502,"D.2
IDENTIFY N UPDATES"
REFERENCES,0.5042194092827004,"This is how the count function changes before and after the update. We start with the update of the
summary N when the alphabet goes from Ad to Ad+1, and proceed to the update for the marginal N,
and then the transitional N."
REFERENCES,0.5063291139240507,Under review as a conference paper at ICLR 2022
REFERENCES,0.5084388185654009,"D.2.1
SUMMARY N"
REFERENCES,0.510548523206751,"When going from Ad to Ad+1, cL and cR are both chunks in Ad and merged together as a new chunk
to augment Ad."
REFERENCES,0.5126582278481012,"The number of parses changes only in places where the cL and cR associated with the new chunk
occurs. The chunks in Ad can be divided into three groups, cL, cR, and Ad \ {cL, cR}. The count
function associated with chunks from the set Ad \ {cL, cR} does not change when Ad updates to
Ad+1."
REFERENCES,0.5147679324894515,"Since with the chunk update Ad+1, cL and cR are chunked together, they are recognized together
as a whole, so every time when they are recognized together as a whole, the count reduces twofold.
Nd+1(cL) and Nd+1(cR) are the number of counts for cL and cR when they do not occur together.
This count is different from Nd(cL) and Nd(cR) because the occasions when they occur one after
another is taken into the count by Nd+1(cL ⊕cR). The relation between Nd and Nd+1 is:"
REFERENCES,0.5168776371308017,"Nd+1 = ""
X"
REFERENCES,0.5189873417721519,"c∈Ad−cL−cR
Nd(c) #"
REFERENCES,0.5210970464135021,"+ Nd+1(cL) + Nd+1(cR) + Nd+1(cL ⊕cR)
(7)"
REFERENCES,0.5232067510548524,"Nd+1(cL) could occur in the case where cR does not follow cL.
Nd+1(cL ⊕cR) = Nd(cL →cR)
(8)
Nd+1(cL) = Nd(cL) −Nd(cL →cR)
(9)
Nd+1(cR) = Nd(cR) −Nd(cL →cR)
(10)
Nd+1(cL ⊕cR) ∗2 + Nd+1(cL) + Nd+1(cR) = Nd(cL) + Nd(cR)
(11)
Chunking reduces the number of times sub-chunks are being parsed when sub-chunks occur right
after each other by twofold."
REFERENCES,0.5253164556962026,"Nd =
X"
REFERENCES,0.5274261603375527,"c∈Ad
Nd(c) = ""
X"
REFERENCES,0.5295358649789029,"c∈Ad−cL−cR
Nd(c) #"
REFERENCES,0.5316455696202531,"+ Nd(cL) + Nd(cR)
(12)"
REFERENCES,0.5337552742616034,"Comparing the above two equations we arrive at
Nd −Nd(cL) −Nd(cR) = Nd+1 −Nd+1(cL) −Nd+1(cR) −Nd+1(cL ⊕cR)
(13)"
REFERENCES,0.5358649789029536,"Since:
Nd(cL) + Nd(cR) = Nd+1(cL) + Nd+1(cR) + 2Nd+1(cL ⊕cR)
(14)"
REFERENCES,0.5379746835443038,"This is related to:
Nd(cL →cR) = Nd+1(cL ⊕cR)
(15)
Because both are counting the number of times they occur consecutively.
Nd+1(cL) + Nd+1(cR) = Nd(cL) + Nd(cR) −2Nd(cL →cR)
(16)"
REFERENCES,0.540084388185654,One can deﬁne Nd+1 in terms of counts in Nd as:
REFERENCES,0.5421940928270043,"Nd+1 = ""
X"
REFERENCES,0.5443037974683544,"c∈Ad−cL−cR
Nd(c) #"
REFERENCES,0.5464135021097046,+ Nd(cL) + Nd(cR) −2Nd(cL →cR) + Nd(cL →cR)
REFERENCES,0.5485232067510548,"Nd+1 = ""
X"
REFERENCES,0.5506329113924051,"c∈Ad−cL−cR
Nd(c) #"
REFERENCES,0.5527426160337553,+ Nd(cL) + Nd(cR) −Nd(cL →cR) (17)
REFERENCES,0.5548523206751055,Under review as a conference paper at ICLR 2022
REFERENCES,0.5569620253164557,"Generally, the result is also the case with the total count Nd and Nd+1 when one switches from the
alphabet set Ad to Ad+1 by chunking cL and cR in Ad together."
REFERENCES,0.5590717299578059,"Nd+1 = ""
X"
REFERENCES,0.5611814345991561,"c∈Ad−cL−cR
Nd(c) #"
REFERENCES,0.5632911392405063,"+ Nd(cL) + Nd(cR) −Nd(cL →cR)
(18)"
REFERENCES,0.5654008438818565,"Nd+1 = Nd −Nd(cL →cR)
(19)"
REFERENCES,0.5675105485232067,"D.2.2
MARGINAL N"
REFERENCES,0.569620253164557,"To proceed into formulating the joint and conditional probability given a particular belief space, we
need to formulate how the count of N(c) for a chunk changes when the belief space when switching
from Ad to Ad+1, with the same division as before."
REFERENCES,0.5717299578059072,"Of course, the count function should be ﬁxed. However, the probability function associated with the
chunks will change based on the update of the belief set. We use the update of the count function to
ﬁnd the relation between the probability updates."
REFERENCES,0.5738396624472574,"For all x in Ad −{cL, cR, cL ⊕cR}:"
REFERENCES,0.5759493670886076,"Nd+1(x) = Nd(x)
(20)"
REFERENCES,0.5780590717299579,"Nd+1(cR) = Nd(cR) −Nd(cL →cR)
(21)"
REFERENCES,0.580168776371308,"Nd+1(cL) = Nd(cL) −Nd(cL →cR)
(22)"
REFERENCES,0.5822784810126582,"Nd+1(cL ⊕cR) = Nd(cL →cR)
(23)"
REFERENCES,0.5843881856540084,"D.2.3
TRANSITIONAL N"
REFERENCES,0.5864978902953587,The following relationship needs to hold:
REFERENCES,0.5886075949367089,"for all x, y in Ad \ {cL, cR, cL ⊕cR}:"
REFERENCES,0.5907172995780591,"Nd+1(x →y) = Nd(x →y)
(24)"
REFERENCES,0.5928270042194093,"Nd+1(x →cL) = Nd(x →cL) −Nd+1(x →cL ⊕cR)
(25)"
REFERENCES,0.5949367088607594,"Nd+1(x →cR) = Nd(x →cR)
(26)"
REFERENCES,0.5970464135021097,"Nd+1(x →cL ⊕cR) = Nd+1(x) −Nd+1(x →cR) −Nd+1(x →cL) −Nd+1(x →y)
(27)"
REFERENCES,0.5991561181434599,"Nd+1(x →cL ⊕cR) ≤Nd(cL →cR)
(28)"
REFERENCES,0.6012658227848101,From cL we have this relation.
REFERENCES,0.6033755274261603,"Nd+1(cL →x) = Nd(cL →x)
(29)"
REFERENCES,0.6054852320675106,"Nd+1(cL →cL) = Nd(cL →cL) −Nd+1(cL →cL ⊕cR)
(30)"
REFERENCES,0.6075949367088608,"Nd+1(cL →cR) = 0
(31)"
REFERENCES,0.609704641350211,Nd+1(cL →cL ⊕cR) = Nd+1(cL)−Nd+1(cL →cR)−Nd+1(cL →cL)−Nd+1(cL →x) (32)
REFERENCES,0.6118143459915611,"Nd+1(cL →cL ⊕cR) ≤Nd(cL →cR)
(33)"
REFERENCES,0.6139240506329114,"Nd+1(cL →cL ⊕cR) ≤Nd(cL →cR)
(34)"
REFERENCES,0.6160337552742616,From cR we have the following relation:
REFERENCES,0.6181434599156118,"Nd+1(cR →y) = Nd(cR →y) −Nd+1(cL ⊕cR →y)
(35)"
REFERENCES,0.620253164556962,"Nd+1(cR →cL) = Nd(cR →cL) −Nd+1(cL ⊕cR →cL)
(36)"
REFERENCES,0.6223628691983122,"Nd+1(cR →cR) = Nd(cR →cR) −Nd+1(cL ⊕cR →cR)
(37)"
REFERENCES,0.6244725738396625,Nd+1(cR →cL ⊕cR) = Nd+1(cR)−Nd+1(cR →cR)−Nd+1(cR →cL)−Nd+1(cR →y) (38)
REFERENCES,0.6265822784810127,"Nd+1(cR →cL ⊕cR) ≤Nd(cL ⊕cR)
(39)"
REFERENCES,0.6286919831223629,Under review as a conference paper at ICLR 2022
REFERENCES,0.630801687763713,From the chunked unit cL ⊕cR we have the following relation:
REFERENCES,0.6329113924050633,"Nd+1(cL ⊕cR →x) ≤min(Nd(cL ⊕cR), Nd+1(cR ⊕x))
(40)"
REFERENCES,0.6350210970464135,"Nd+1(cL ⊕cR →cL) ≤min(Nd(cL ⊕cR), Nd(cR ⊕cL))
(41)
Nd+1(cL ⊕cR →cR) ≤min(Nd+1(cL ⊕cR), Nd+1(cR ⊕cR))
(42)
Nd+1(cL ⊕cR →cL ⊕cR) ≤min(Nd+1(cL ⊕cR), Nd(cR ⊕cL))
(43)
Finally, we need to satisfy the marginal constraint, that is:"
REFERENCES,0.6371308016877637,"Nd+1(cL⊕cR) = Nd+1(cL⊕cR →cL)+Nd+1(cL⊕cR →x)+Nd+1(cL⊕cR →cR)+Nd+1(cL⊕cR →cL⊕cR)
(44)"
REFERENCES,0.6392405063291139,"D.3
PROBABILITY DENSITY SWITCH WHEN Ad EXPANDS TO Ad+1"
REFERENCES,0.6413502109704642,"The constraint is: the number of counts N for all chunks deﬁned for the support set Ad must remain
the same for the support set sAd+1, so that the deﬁnition of PAd for all relevant chunks within Ad
remains the same when Ad expands to Ad+1."
REFERENCES,0.6434599156118144,"Relating the number of counts to probability: The probability of a chunk occurring in the alphabet
set Ad is deﬁned as:"
REFERENCES,0.6455696202531646,"PAd(c) =
lim
Nd→∞
Nd(c)"
REFERENCES,0.6476793248945147,"Nd
(45)"
REFERENCES,0.6497890295358649,"Because Nd and Nd+1 are only a constant away, both go to inﬁnity if one of them does, so there is
a relation between the deﬁnition of probability PAd(c) and PAd+1(c). For any chunk x in Ad that is
not cL and cR, Nd+1(x) = Nd(x):"
REFERENCES,0.6518987341772152,"PAd+1(x) =
lim
Nd+1→∞
Nd+1(x)"
REFERENCES,0.6540084388185654,"Nd+1
=
lim
Nd→∞
Nd(x)
Nd −Nd(cL →cR)
(46)"
REFERENCES,0.6561181434599156,"That is, the probability of a chunk of this category at d and d+1 satisﬁes this relationship:"
REFERENCES,0.6582278481012658,"lim
Nd+1→∞PAd+1(x)Nd+1 =
lim
Nd→∞PAd(x)Nd
(47)"
REFERENCES,0.6603375527426161,"PAd+1(x) = PAd(x)
limNd→∞Nd
limNd+1→∞Nd+1
(48)"
REFERENCES,0.6624472573839663,"PAd+1(x) = PAd(x)
limNd→∞Nd
limNd+1→∞Nd −Nd(cL →cR)
(49)"
REFERENCES,0.6645569620253164,For cL and cR in Ad+1:
REFERENCES,0.6666666666666666,"PAd+1(cL) =
lim
Nd+1→∞
Nd+1(cL)"
REFERENCES,0.6687763713080169,"Nd+1
(50)"
REFERENCES,0.6708860759493671,"PAd(cL) =
lim
Nd+1→∞
Nd(cL)"
REFERENCES,0.6729957805907173,"Nd
(51)"
REFERENCES,0.6751054852320675,"PAd+1(cR) =
lim
Nd+1→∞
Nd+1(cR)"
REFERENCES,0.6772151898734177,"Nd+1
(52)"
REFERENCES,0.679324894514768,"PAd(cR) =
lim
Nd→∞
Nd(cR)"
REFERENCES,0.6814345991561181,"Nd
(53)"
REFERENCES,0.6835443037974683,"Because:
Nd+1(cL) = Nd(cL) −Nd(cL ⊕cR)
(54)"
REFERENCES,0.6856540084388185,"PAd+1(cL) =
lim
Nd+1→∞
Nd(cL) −Nd(cL ⊕cR)"
REFERENCES,0.6877637130801688,"Nd+1
(55)"
REFERENCES,0.689873417721519,"PAd+1(cL) =
lim
Nd→∞
Nd(cL) −Nd(cL →cR)"
REFERENCES,0.6919831223628692,"Nd −Nd(cL →cR)
(56)"
REFERENCES,0.6940928270042194,Under review as a conference paper at ICLR 2022
REFERENCES,0.6962025316455697,"PAd+1(cR) =
lim
Nd→∞
Nd(cR) −Nd(cL →cR)"
REFERENCES,0.6983122362869199,"Nd −Nd(cL →cR)
(57)"
REFERENCES,0.70042194092827,Finally:
REFERENCES,0.7025316455696202,"PAd+1(cL ⊕cR) =
lim
Nd+1→∞
Nd+1(cL ⊕cR)"
REFERENCES,0.7046413502109705,"Nd+1
(58)"
REFERENCES,0.7067510548523207,"PAd(cL ⊕cR) =
lim
Nd→∞
Nd(cL →cR)"
REFERENCES,0.7088607594936709,"Nd
(59)"
REFERENCES,0.7109704641350211,"since Nd(cL →cR) = Nd+1(cL ⊕cR), we have"
REFERENCES,0.7130801687763713,"PAd+1(cL ⊕cR) =
lim
Nd→∞
PAd(cL ⊕cR)Nd"
REFERENCES,0.7151898734177216,"Nd+1
(60)"
REFERENCES,0.7172995780590717,"D.3.1
SUMMARY PROBABILITIES"
REFERENCES,0.7194092827004219,"Nd+1 = Nd −Nd(cL ⊕cR) = Nd −NdPd(cL ⊕cR)
(61)
Nd+1"
REFERENCES,0.7215189873417721,"Nd
= 1 −Pd(cL ⊕cR)
(62)"
REFERENCES,0.7236286919831224,"D.3.2
MARGINAL PROBABILITIES"
REFERENCES,0.7257383966244726,"The constraints on marginal probabilities when the support set changes from Ad to Ad+1, derived
from the constraints on the marginal counts, are the following:"
REFERENCES,0.7278481012658228,"Pd+1(x) =
Pd(x)
1 −Pd(cL ⊕cR)
(63)"
REFERENCES,0.729957805907173,Pd+1(cR) = Pd(cR) −Pd(cL ⊕cR)
REFERENCES,0.7320675105485233,"1 −Pd(cL ⊕cR)
(64)"
REFERENCES,0.7341772151898734,Pd+1(cL) = Pd(cL) −Pd(cL ⊕cR)
REFERENCES,0.7362869198312236,"1 −Pd(cL ⊕cR)
(65)"
REFERENCES,0.7383966244725738,"Pd+1(cL ⊕cR) =
Pd(cL ⊕cR)
1 −Pd(cL ⊕cR)
(66)"
REFERENCES,0.740506329113924,"With this set of formulations, we have deﬁned the next level marginal probability measures as a
function of the previous level observations and their implied probability measures."
REFERENCES,0.7426160337552743,"D.3.3
TRANSITIONAL PROBABILITIES"
REFERENCES,0.7447257383966245,"for all x, y in Ad −{cL, cR, cL ⊕cR}, from x we have this relation:"
REFERENCES,0.7468354430379747,"0 ≤Pd+1(·|x) ≤1
(67)"
REFERENCES,0.7489451476793249,"Pd+1(y|x) = Pd(y|x)
(68)"
REFERENCES,0.7510548523206751,"Pd+1(cR|x) = Pd(cR|x)
(69)"
REFERENCES,0.7531645569620253,"Pd+1(cL|x) + Pd+1(cL ⊕cR|x) = Pd(cL|x)
(70)"
REFERENCES,0.7552742616033755,"This equation is sampled so that both Pd+1(cL|x) and Pd+1(cL ⊕cR|x) satisfy the following con-
straints:"
REFERENCES,0.7573839662447257,Pd+1(cL|x) ≤Pd+1(cL)
REFERENCES,0.759493670886076,"Pd+1(x)
(71)"
REFERENCES,0.7616033755274262,Pd+1(cL ⊕cR|x) ≤Pd+1(cL ⊕cR)
REFERENCES,0.7637130801687764,"Pd+1(x)
(72)"
REFERENCES,0.7658227848101266,Under review as a conference paper at ICLR 2022
REFERENCES,0.7679324894514767,"Basically, Pd+1(cL|x) is sampled from the range
h
0, min{1, Pd+1(cL)"
REFERENCES,0.770042194092827,"Pd+1(x) , Pd(cL|x)}
i
."
REFERENCES,0.7721518987341772,"Additionally,
Pd+1(cL
⊕
cR|x)
is
constrained
to
be
within
this
range:
h
0, min{1, Pd+1(cL⊕cR)"
REFERENCES,0.7742616033755274,"Pd+1(x)
, Pd(cL|x)}
i
."
REFERENCES,0.7763713080168776,From cL we have this relation:
REFERENCES,0.7784810126582279,"Pd+1(x|cL) =
Pd(x|cL)
1 −Pd(cR|cL)
(73)"
REFERENCES,0.7805907172995781,"Pd+1(cR|cL) = 0
(74)"
REFERENCES,0.7827004219409283,Pd+1(cL|cL) = Pd(cL|cL) −(1 −Pd(cR|cL))Pd+1(cL ⊕cR|cL)
REFERENCES,0.7848101265822784,"1 −Pd(cR|cL)
(75)"
REFERENCES,0.7869198312236287,Put into simpliﬁed terms:
REFERENCES,0.7890295358649789,"Pd+1(cL|cL) + Pd+1(cL ⊕cR|cL) =
Pd(cL|cL)
1 −Pd(cR|cL)
(76)"
REFERENCES,0.7911392405063291,Pd+1(cL|cL) ≤Pd+1(cL)
REFERENCES,0.7932489451476793,"Pd+1(cL) = 1
(77)"
REFERENCES,0.7953586497890295,Pd+1(cL ⊕cR|cL) ≤Pd+1(cL ⊕cR)
REFERENCES,0.7974683544303798,"Pd+1(cL)
(78)"
REFERENCES,0.79957805907173,"Additionally, Pd+1(cL|cL) is constrained to fall within this range:
h
0, min{1,
Pd(cL|cL)
1−Pd(cR|cL)}
i
."
REFERENCES,0.8016877637130801,"Pd+1(cL ⊕cR|cL) within the range
h
0, min{1, Pd+1(cL⊕cR)"
REFERENCES,0.8037974683544303,"Pd+1(cL)
,
Pd(cL|cL)
1−Pd(cR|cL)}
i"
REFERENCES,0.8059071729957806,From cR we have the following relation:
REFERENCES,0.8080168776371308,"0 ≤Pd+1(·|cR) ≤1
(79)"
REFERENCES,0.810126582278481,Pd+1(y|cR) = Pd(y|cR)Pd(cR) −Pd+1(y|cL ⊕cR)Pd(cL ⊕cR)
REFERENCES,0.8122362869198312,"Pd(cR) −Pd(cL ⊕cR)
(80)"
REFERENCES,0.8143459915611815,Under review as a conference paper at ICLR 2022
REFERENCES,0.8164556962025317,Pd+1(cL|cR) = Pd(cL|cR)Pd(cR) −Pd+1(cL|cL ⊕cR)Pd(cL ⊕cR)
REFERENCES,0.8185654008438819,"Pd(cR) −Pd(cL ⊕cR)
(81)"
REFERENCES,0.820675105485232,Pd+1(cR|cR) = Pd(cR|cR)Pd(cR) −Pd+1(cR|cL ⊕cR)Pd(cL ⊕cR)
REFERENCES,0.8227848101265823,"Pd(cR) −Pd(cL ⊕cR)
(82)"
REFERENCES,0.8248945147679325,"Pd+1(·|cR) ≥Pd+1(·|cL ⊕cR)
(83)"
REFERENCES,0.8270042194092827,"Additionally, Pd+1(cL ⊕cR|cR) and Pd+1(cL ⊕cR|cL ⊕cR) needs to satisfy:"
REFERENCES,0.8291139240506329,"Pd+1(cL ⊕cR|cR) = 1 −Pd+1(y|cR) −Pd+1(cR|cR) −Pd+1(cL|cR)
(84)"
REFERENCES,0.8312236286919831,Pd+1(cL ⊕cR|cL ⊕cR) = 1 −Pd+1(y|cL ⊕cR) −Pd+1(cR|cL ⊕cR) −Pd+1(cL|cL ⊕cR) (85)
REFERENCES,0.8333333333333334,"For the generative model, when the alphabet set goes from Ad to Ad + 1 by chunking cL and cR
together, the above constraints associated with chunks in Ad and Ad+1 need to be satisﬁed."
REFERENCES,0.8354430379746836,"D.4
HIERARCHICAL GENERATIVE MODEL"
REFERENCES,0.8375527426160337,"At the beginning of the generative process, the atomic alphabet set A0 is speciﬁed. Another param-
eter, d, speciﬁes the number of additional chunks that are created in the process of generating the
hierarchical chunks. Starting from the alphabet A0 with initialized elementary chunks ci from the
alphabet, the probability associated with each chunk ci in A0 needs to satisfy the following criterion:
X"
REFERENCES,0.8396624472573839,"ci∈A0
PA0(ci) = 1
(86)"
REFERENCES,0.8417721518987342,"Meanwhile, P(ci) ≥0, ∀ci ∈A0."
REFERENCES,0.8438818565400844,"We assume that at each step the marginal and transitional probability of the previous steps are known.
The next chunk is chosen as the combined chunks with the biggest probability. The order of con-
struction in the generative model follows the rule that the combined chunk with the biggest proba-
bility on the support set of pre-existing chunk sets is chosen to be added to the set of chunks."
REFERENCES,0.8459915611814346,"cL ⊕cR =
arg max
cL,cR∈Ad\{0}
PAd(cL ⊕cR)
(87)"
REFERENCES,0.8481012658227848,Under the constraint that:
REFERENCES,0.8502109704641351,"PAd(cL)PAd(cR) ≤PAd(cL ⊕cR) ≤min{PAd(cL), PAd(cR)}
(88)"
REFERENCES,0.8523206751054853,This can be calculated from the transitional and marginal probability of the previous step.
REFERENCES,0.8544303797468354,"cL ⊕cR =
arg max
cL,cR∈Ad\{0}
PAd(cL ⊕cR) =
arg max
cL,cR∈Ad\{0}
PAd(cL)PAd(cR|cL)
(89)"
REFERENCES,0.8565400843881856,"Once the chunk of the next step: cL ⊕cR is chosen, the support set changes from Ad to Ad+1, which
is one size bigger. On the new support set, the marginal probability is speciﬁed by the marginal
update. That is, the marginal and transitional probability deﬁned on the support set Ad fully spec-
iﬁes the marginal probability of the support set Ad. Going from generating a new chunk from the
support set Ad to the set Ad+1, the transitional probabilities between chunks in Ad need to satisfy
the following constraints:"
REFERENCES,0.8586497890295358,"• PAd+1(cL|x) + PAd+1(cL ⊕cR|x) = PAd(cL|x) for all x in Ad
• PAd+1(cL|cL)(1 −PAd(cR|cL)) + PAd+1(cL ⊕cR|cL)(1 −PAd(cR|cL)) = PAd(cL|cL)
• PAd+1(y|cR)(PAd(cR) −PAd(cL ⊕cR)) + PAd+1(y|cL ⊕cR)PAd(cL ⊕cR)
=
PAd(y|cR)PAd(cR)
• PAd+1(cL|cR)(PAd(cR) −PAd(cL ⊕cR)) + PAd+1(cL|cL ⊕cR)PAd(cL ⊕cR)
=
PAd(cL|cR)PAd(cR)
• PAd+1(cR|cR)(PAd(cR) −PAd(cL ⊕cR)) + PAd+1(cR|cL ⊕cR)PAd(cL ⊕cR)
=
PAd(cR|cR)PAd(cR)"
REFERENCES,0.8607594936708861,Under review as a conference paper at ICLR 2022
REFERENCES,0.8628691983122363,• Pd+1(cL ⊕cR|cR) = 1 −Pd+1(y|cR) −Pd+1(cR|cR) −Pd+1(cL|cR)
REFERENCES,0.8649789029535865,• Pd+1(cL ⊕cR|cL ⊕cR) = 1−Pd+1(y|cL ⊕cR)−Pd+1(cR|cL ⊕cR)−Pd+1(cL|cL ⊕cR)
REFERENCES,0.8670886075949367,"In total, there are |x| + |y| + 3 degrees of freedom. Since |x| = |y| = |Ad| −2, at each step, there
are 2|Ad| −1 number of values to be speciﬁed."
REFERENCES,0.869198312236287,"In practice, after the chunks are speciﬁed in Ad, the probability value associated with chunks in A0
are sampled from a ﬂat Dirichlet distribution, which is then sorted so that the smaller sized chunks
contain more of the probability mass and the null-chunk carries the biggest probability mass. Then,
the above constraint is checked for the assigned probability on each of the newly generated chunk
with their associated alphabet set Ai. This process repeats until the probability drawn satisﬁes the
condition for every newly created chunk."
REFERENCES,0.8713080168776371,"Theorem 1 (Marginal Probability Space Conservation). After the addition of cd,i ⊕cd,j and the
change of probability, PAd is still a valid probability distribution."
REFERENCES,0.8734177215189873,"Proof:
X"
REFERENCES,0.8755274261603375,"cd,k∈Ad
PAd(cd,k) =
X"
REFERENCES,0.8776371308016878,"cd,k∈Ad−1−cd−1,i−cd−1,j
PAd−1(cd−1,k)+"
REFERENCES,0.879746835443038,"+ PAd−1(cd−1,i) −PAd−1(cd−1,j|cd−1,i)PAd−1(cd−1,i)"
REFERENCES,0.8818565400843882,"+ PAd−1(cd−1,j) + PAd−1(cd−1,j|cd−1,i)PAd−1(cd−1,i) = 1 (90) □"
REFERENCES,0.8839662447257384,"Theorem 2 (Conditional Probability Space Preservation). The conditional probability Pd+1(z|c)
for any c ∈Ad+1 after sampling is still a valid distribution."
REFERENCES,0.8860759493670886,"Proof:
Show by manipulating the equations, that the sum of the conditions on c sums to 1, and
each of them is bigger than 0.
□"
REFERENCES,0.8881856540084389,"Theorem 3 (Measure Space Preservation). Given that at the end of the generative process with
depth d one ends up having an alphabet set Ad, the probability space deﬁned on Ai, which includes
the marginal and joint probability of any chunk and combinations of chunks in Ai, i = 0, 1, 2, . . . d,
which are predecessor alphabet sets of Ad, all values in the set Md and Td remain the same no
matter how the future support set changes according to the generative model."
REFERENCES,0.890295358649789,"Proof:
By induction."
REFERENCES,0.8924050632911392,"• Base case: starting from the initialized alphabet set A0, the probability of PA0(c), c ∈A0,
and the probability of PA1(xy), x, y ∈A0, for all valid c, x, y, when the alphabet is A1.
Going from A0 to A1, N0(c), N0 and N0(x →y) does not change, therefore PA0(c) and
PA0(x →y) at the alphabet A1 is the same as that when the alphabet is A0."
REFERENCES,0.8945147679324894,"• Induction Step: starting from the initialized alphabet set Ad, the probability of PAd(c), c ∈
Ad, and the probability of PAd(xy), x, y ∈Ad, for all valid c, x, y, when the alphabet is
Ad+1. Going from Ad to Ad+1, Nd(c), Nd and Nd(x →y) does not change, therefore
PAd(c) and PAd(x →y) at the alphabet Ad+1 is the same as that when the alphabet is Ad. □"
REFERENCES,0.8966244725738397,"Theorem 4. The order of PAi(xy), x, y ∈Ai for any i = 0, 1, 2, . . . d at any previous belief space
is preserved throughout the update."
REFERENCES,0.8987341772151899,"Proof:
At the end of the generative process with depth d, one ends up having such an alphabet set:
Ad. The probability space deﬁned on Ai, which includes the marginal and joint probability of any
chunk and combinations of chunks in Ai, i = 0, 1, 2, . . . d is preserved, hence the order is preserved. □"
REFERENCES,0.9008438818565401,"The generative process can be described by a graph update path. The speciﬁcation of the initial set
of atomic chunks A0 corresponds to an initial graph G0 with the atomic chunks as its vertices. At
the i-th iteration, as the generative graph goes from GAi to graph GAi+1, two none zero chunks cL,
cR chosen from the pre-existent set of chunks Ai and are concatenated into a new chunk cL ⊕cR,"
REFERENCES,0.9029535864978903,Under review as a conference paper at ICLR 2022
REFERENCES,0.9050632911392406,"augmenting Ai by one to Ai+1. The vertex set also increments from VAi to VAi+1 = VAi ∪cL ⊕cR.
Moreover, two directed edges connecting the parental chunks to the newly-created chunk are added
to the set of edges: EAi to EAi = EAi ∪(cL, cL ⊕cR) ∪(cR, cL ⊕cR). The series of graphs created
during the chunk construction process going from GA0 to the ﬁnal graph GAd with d constructed
chunks can be denoted as a graph generating path P(GA0, GA0) = (GA0, GA1, GA2, ..., GAd)."
REFERENCES,0.9071729957805907,"D.5
LEARNING THE HIERARCHY"
REFERENCES,0.9092827004219409,"The rational chunking model is initialized with one minimally complete belief set, the learning
algorithm ranks the joint probability of every possible new chunk concatenated by its pre-existing
belief set, and picks the one with the maximal occurrence joint probability on the basis of the current
set of chunks as the next new chunk to enlarge the belief set. With the one-step agglomerated belief
set, the learning model parses the sequence again. This process repeats until the chunks in the belief
set pass the independence testing criterion.
Theorem 5 (Learning Guarantees on the Hierarchical Generative Model). As N →∞, the chunk
construction graph learned by the model ˆG is the same as the chunk construction graph of the
generative model: ˆG = G, which entails that they have the same vertex set: ˆV = VG and the same
edge set: ˆE = EG. Additionally, the belief set learned by the chunk learning model Bd = Ad, and
the marginal probability evaluated on the learned belief set MBd associated with each chunk is the
same as the marginal probability imposed by the generative model on the generative belief set MAd.
Proof:
Given that all of the empirical estimates are the same as the true probabilities deﬁned by
the generative model, we prove that starting with B0, the learning algorithm will learn BD = AD.
AD is the belief set imposed by the generative model. We approach this proof by induction."
REFERENCES,0.9113924050632911,"Base Step: As the chunk learner acquires a minimal set of atomic chunks that can be used to explain
the sequence at ﬁrst, the set of elementary atomic chunks learned by the model is the same as the
elementary alphabet imposed by the generative model, i.e. B0 = A0. Hence, the root of the graph,
which contains the nodes without their parents, is the same, ˆG = G; put differently, ˆV0 = V0"
REFERENCES,0.9135021097046413,"Additionally, the learning model approximates the probability of a speciﬁc atomic chunk as ˆPA0(ai).
As n →∞, for all chunks c in the set of atomic elementary chunks in B0, the empirical probability
evaluated on the support set is the same as the true probability assigned in the generative model with
the alphabet set A0:
ˆPB0(c) = lim
n→∞
N0(c)"
REFERENCES,0.9156118143459916,"N0
= PA0(c)
(91)"
REFERENCES,0.9177215189873418,"Induction hypothesis: Assume that the learned belief set Bd at step d contains the same chunks as
the alphabet set Ad in the generative model."
REFERENCES,0.919831223628692,"The HCM, by keeping track of the transition probability between any pairs of chunks, calculates
ˆPBd(ci|cj) for all ci, cj in Bd. Afterwards, it ﬁnds the pair of chunks ci, cj, such that the chunk
created by combining ci and cj together contains the maximum joint probability violating the inde-
pendence test as candidate chunks to be combined together."
REFERENCES,0.9219409282700421,"ˆPBd(ci ⊕cj) =
sup
ci,cj∈Bd
ˆPBd(ci) ˆPBd(cj|ci)
(92)"
REFERENCES,0.9240506329113924,"We know that in the generative step the supremum of the joint probability with the support set Ad is
being picked to form the next chunk in the representation graph, so each step of the process at step
d satisﬁes the condition that:"
REFERENCES,0.9261603375527426,"PAd(ci ⊕cj) =
sup
ci,cj∈Ad
PAd(ci)PAd(cj|ci)
(93)"
REFERENCES,0.9282700421940928,"Since PAd(cj|ci) = ˆPBd(cj|ci), PAd(ci) = ˆPBd(ci), the chunks ci and cj chosen by the learning
model will be the same ones as those created in the generative model."
REFERENCES,0.930379746835443,"End step: The chunk learning process stops once an independence test has been passed, which
means that the sequence is better explained by the current set of chunks than any of the other possible
next-step chunk combinations. This is the case once the chunk learning algorithm has learned a
belief set Bd that is the same as the generative alphabet set Ad. At this point ˆG = G
□"
REFERENCES,0.9324894514767933,Under review as a conference paper at ICLR 2022
REFERENCES,0.9345991561181435,"E
EXPERIMENT DETAIL: CHUNK RECOVERY AND CONVERGENCE"
REFERENCES,0.9367088607594937,"To test the model’s learning behavior on this type of sequential data, random graphs of chunk hi-
erarchies with an associated occurrence probability for each chunk are speciﬁed by the hierarchical
generative process. To do so, an initial set of speciﬁed atomic chunks A0 and a pre-speciﬁed level
of depth (new chunks) d is used to initiate the generation of a random hierarchical generative graph
G. In the end, the generative model generates a set of chunks A. In total, there are |A0| + d number
of chunks in the generative alphabet A, with chunk c from the alphabet set having an occurrence
probability of PA(c) on the sample space A."
REFERENCES,0.9388185654008439,"Once the hierarchical generative model is speciﬁed, it is then used to produce training sequences
with varying length N to test the chunk recovery."
REFERENCES,0.9409282700421941,"The rational chunk model is trained on the the sequence S with increasing sizes (from 100 to 3000
with steps of 100) generated by the hierarchical generative graph, and it learns a hierarchical chunk-
ing graph ˆG. To test how good the representation learned by the chunking graph is compared to
the ground truth generative model G, a discrete version of Kullback–Leibler divergence is used to
compare the ground truth probability PA(c) of every chunk on the sample space of the ground truth
alphabet, to the learned probability Q(c) of the chunking model."
REFERENCES,0.9430379746835443,"KL(P||Q) =
X"
REFERENCES,0.9451476793248945,"c∈A
PA(c) log2( PA(c)"
REFERENCES,0.9472573839662447,"QA(c))
(94)"
REFERENCES,0.9493670886075949,"While PA is clearly deﬁned by the generative model, QA(c) needs to be calculated from what the
model has learned. Note that the set of chunks B learned by HCM may or may not be the same as
A, as B also augments or shrinks in different learning stages."
REFERENCES,0.9514767932489452,"To evaluate QA(c), the hierarchical chunk graph with their occurrence probability associated with
each chunk is used to produce “imagined” sequences of length 1000. Imagined sequences are the
sequences that HCM produces based on its learned representations. After that, the occurrence prob-
ability of each chunk c in A(c) is used to evaluate Q, comparing the HCM’s learned representation
with the ground truth."
REFERENCES,0.9535864978902954,"KL divergence is used to evaluate the deviation of learned representations in the hierarchical chunk-
ing model from the original representations on the corresponding support set of the original repre-
sentations."
REFERENCES,0.9556962025316456,"For the comparsion, we used the same sequence used for training HCM to train a 3 layer recurrent
neural network (one embedding layer with 40 hidden units, one LSTM with drop-out rate = 0.2, and
one fully connected layer with batch size = 5, sequence length = 3, epoch = 1, so that the data used
for training is the same as N) of the training sequence, and used it to generate imagined sequences
which are then used to calculate the KL divergence as before."
REFERENCES,0.9578059071729957,"To generate the KL divergence plot for Figure 2, sequences with varying size N were used to train
HCM. HCM was then used to generate imaginative sequence so that KL measurements can be taken."
REFERENCES,0.959915611814346,"F
EXPERIMENT DETAIL: VISUAL HIERARCHICAL CHUNKS"
REFERENCES,0.9620253164556962,"The visual hierarchical chunks are crafted by hand as binary arrays. The dark pixels correspond to
having a array value of 1 and background a value of 0. Each image in the generative hierarchy is
25 dimensional (5 x 5) in the visual domain and size 1 in the temporal domain. An empty array is
included to denote no observation. The alphabet A of the generative model include all 14 the images
in the generative hierarchy."
REFERENCES,0.9641350210970464,"The probability of occurrance for each generative visual chunk is drawn from a ﬂat dirichlet distri-
bution with the empty observation retaining the highest mass, this is to emulate the process that real
world observatipons are mostly sparse in the environment."
REFERENCES,0.9662447257383966,Under review as a conference paper at ICLR 2022
REFERENCES,0.9683544303797469,"The parameters (α1, .., αK) with K = |A| are all set to one, and PA(c), c1, ..., cK ∈A are sampled
from the probability density function"
REFERENCES,0.9704641350210971,"f(x1, ..., xk; α1, ..., αK) =
1
B(a) K
Y"
REFERENCES,0.9725738396624473,"i=1
xαi−1
i
)
(95)"
REFERENCES,0.9746835443037974,Where the beta function when expressed using gamma function is: B(a) =
REFERENCES,0.9767932489451476,"QK
i=1 Γ(αi)
Γ(PK
i αi) , and a ="
REFERENCES,0.9789029535864979,"(α1, .., αK)."
REFERENCES,0.9810126582278481,"To generate the sequence, images in the hierarchy are sampled from the occurrence distribution and
appended to the end of the sequence. As a result, there are visual correlations in the sequence deﬁned
by the hierarchy, but temporally, each image slice is i.i.d. from the dirichlet distribution."
REFERENCES,0.9831223628691983,"Examples of chunk representations learned by the hierarchical chunking model at different stages is
shown are collected at the learning stages of t = 10, t = 100, t = 1000 respectively."
REFERENCES,0.9852320675105485,"G
EXPERIMENT DETAIL: GIF MOVEMENT"
REFERENCES,0.9873417721518988,"The gif ﬁle is converted into an [T x H x W] sized tensor. With T being the temporal dimension of
the spatial vision sequence. The entire moment is 10 frames of 25 x 25 images. Each color in the
gif ﬁle is mapped to a unique integer, with the background having a value of 0. TO construct the
training sequence for spatial temporal chunks. In this way, the gif ﬁle is converted into a tensor with
size 10 x 25 x 25. The entire movement is repeated 100 times and trained on HCM."
REFERENCES,0.989451476793249,"H
EXPERIMENT DETAIL: THE HUNGER GAMES"
REFERENCES,0.9915611814345991,"The ﬁrst book of The Hunger Games is stored as ’test_data.txt’ with the code in the supple-
mentary material. The text ﬁle contains approximately 520,000 characters in total."
REFERENCES,0.9936708860759493,"To convert the book into a temporal sequences, a unique mapping between each character and an
integer is created. This sequence of integer is used to train the online HCM, with a forgetting rate
of 0.99 and a chunk deletion threshold of 1e−5. Empty spaces are also mapped onto a nonzero
integer to enable the model to learn chunks with the inclusion of empty spaces. The HCM trains
on sequences of 1,000 characters in length at each step. More examples of learned chunks taken
from M at different stages of learning are displayed in Table 2. Simple representation examples are
taken from chunks learned after 10,000 characters in the book. Intermediate examples are taken after
100,000 characters have been parsed, and Complex chunks are taken when HCM reaches 300,000
characters of the book."
REFERENCES,0.9957805907172996,Under review as a conference paper at ICLR 2022
REFERENCES,0.9978902953586498,"Table 2: Representation Learned in Hunger Games
Simple Chunks
’an’, ’in ’, ’be’, ’at’, ’me’, ’le’, ’a ’, ’ar’, ’re’, ’and ’, ’ve’, ’ing’, ’on’,
’st’, ’se’, ’to ’, ’i ’, ’n ’, ’of ’, ’he’, ’my ’, ’te’, ’pe’, ’ou’,’we’, ’ad’, ’de’,
’li’, ’oo’, ’bu’, ’fo’, ’ave’, ’the’, ’ce’, ’is’, ’as’,’il’, ’ch’, ’al’, ’no’, ’she’,
’ing’, ’am’, ’ack’, ’we’, ’raw’, ’on the’,’day’, ’ear’, ’oug’, ’bea’, ’tree’,
’sin’, ’that’, ’log’, ’ters’, ’wood’,’now’, ’was’, ’even’, ’leven’, ’ater’,
’ever’, ’but’, ’ith’, ’ity’, ’if’,’the wood’, ’bell’, ’other’
Intermediate Chunks
’old’, ’gather’, ’as’, ’under’, ’way.’, ’day’, ’hunger’, ’very’, ’death’,
’ping’, ’the seam’, ’add’, ’ally’, ’king’, ’lose’, ’sing’, ’loser’, ’money’,
’man who’, ’in the’, ’says’, ’tome’, ’might’, ’rave’, ’even’, ’ick’,
’wood’, ’he want’, ’for’, ’into’, ’leave’, ’reg’, ’lose to’, ’lock’, ’where’,
’up’, ’gale’, ’older’, ’ask’, ’come’, ’raw’, ’real’, ’bed’, ’ing for’, ’from’,
’link’, ’few’, ’close’, ’arrow’, ’ull’, ’cater’, ’this’, ’one’, ’almo’, ’lack’,
’shop’, ’year’, ’ring’, ’cause’, ’is the’, ’ugh’, ’eve’, ’are’, ’the leap’,
’lly’, ’still’, ’heal’, ’tow’, ’never’, ’try’, ’prim’, ’iting’, ’bread’, ’ould’, ’,
but’, ’Now’, ’beg’, ’liday’, ’arm’, ’quick’, ’hot’, ’men’, ’know’, ’then’,
’bell’, ’pan’, ’mother’, ’only’, ’war’, ’eak’, ’high’, ’read’, ’district 12’,
’can’, ’would’, ’pas’,
Complex Chunks
’arent’, ’he may’, ’I want’, ’gather’, ’capitol’, ’been’, ’trip’, ’a baker’,
’, but’, ’madge’, ’heir’, ’mouth’, ’you can’, ’a few’, ’berries’, ’fully’,
’tribute’, ’I can’, ’cause of the’, ’feel’, ’to the baker’, ’its not just’,
’he want’, ’slim’, ’hand’, ’the ball’, ’quick’, ’green’, ’the last’, ’peace’,
’off’, ’you have to’, ’kill’, ’of the’, ’the back’, ’they’, ’scomers’, ’have
been’, ’reaping’, ’as well’, ’, but the ’, ’cent’, ’thing’, ’I remem-
ber’, ’ally’, ’though’, ’again’, ’dont’, ’need’, ’in the school’, ’the pig’,
’in our’, ’them’, ’to remember’, ’fair’, ’bother’, ’at the’, ’older’, ’the
square’, ’I know’, ’house’, ’its not’, ’once’, ’what was’, ’out of’, ’it is
not just’, ’our district’, ’too’, ’I have’, ’it out’"
