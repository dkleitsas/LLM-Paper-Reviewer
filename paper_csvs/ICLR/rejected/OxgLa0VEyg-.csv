Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0026246719160104987,"Generalising robustly to distribution shift is a major challenge that is pervasive
across most real-world applications of machine learning. A recent study highlighted
that many advanced algorithms proposed to tackle such domain generalisation (DG)
fail to outperform a properly tuned empirical risk minimisation (ERM) baseline.
We take a different approach, and explore the impact of the ERM loss function on
out-of-domain generalisation. In particular, we introduce a novel meta-learning
approach to loss function search based on implicit gradient. This enables us
to discover a general purpose parametric loss function that provides a drop-in
replacement for cross-entropy. Our loss can be used in standard training pipelines
to efÔ¨Åciently train robust models using any neural architecture on new datasets.
The results show that it clearly surpasses cross-entropy, enables simple ERM to
outperform some more complicated prior DG methods, and provides state-of-the-art
performance across a variety of DG benchmarks. Furthermore, unlike most existing
DG approaches, our setup applies to the most practical setting of single-source
domain generalisation, on which we show signiÔ¨Åcant improvement."
INTRODUCTION,0.005249343832020997,"1
INTRODUCTION"
INTRODUCTION,0.007874015748031496,"Deep learning is highly successful when the training and testing samples meet the i.i.d. assumption.
However, this assumption is violated in many practical applications of machine learning from medical
imaging to earth observation imaging (Koh et al., 2021). This has led a large number of studies to
investigate approaches to training models with increased robustness to distribution shift at testing-
time, a problem setting known as Domain Generalisation (DG). Despite the volume of research
in this area (Zhou et al., 2021a), a recent careful benchmarking exercise, DomainBed (Gulrajani
& Lopez-Paz, 2021) showed that simple empirical risk minimisation (ERM) on a combination of
training domains is a very strong baseline when properly tuned. State-of-the-art alternatives based
on sophisticated architectures, regularisers, and data augmentation schemes failed to reliably beat
ERM (Gulrajani & Lopez-Paz, 2021)."
INTRODUCTION,0.010498687664041995,"Rather than propose an alternative to ERM for DG, we investigate a previously unstudied hyper-
parameter of ERM, namely the choice of loss function‚Äîwhich has been ubiquitously taken to be
standard cross-entropy (CE) in prior DG work. Loss function choice has been shown to impact
calibration (Mukhoti et al., 2020), overÔ¨Åtting (Gonzalez & Miikkulainen, 2019), and label-noise
robustness (Wang et al., 2019) in standard supervised learning, so it is intuitive that it would impact
robustness to domain-shift. However, it has not yet been studied in this context. Our preliminary
experiments showed that equipping ERM with some recent robust loss functions in place of CE does
lead to improvements in DG performance where sophisticated alternatives have failed (Gulrajani &
Lopez-Paz, 2021). This raises the question: can one design a loss function specialised for DG?"
INTRODUCTION,0.013123359580052493,"To answer this question, we deÔ¨Åne a meta-learning algorithm to learn a parametric (white-box) loss
function suitable for DG. Our desiderata are: (1) Performing ERM with this loss on a source domain
should lead to good performance when tested on out-of-domain target data; and (2) It should provide
a ‚Äòplug-and-play‚Äô drop-in replacement for cross-entropy that, once learned, can be used without
further modiÔ¨Åcation or computational expense with any new dataset or model architecture. While
there has been growing interest in meta-learning for loss function design (Li et al., 2019a), they
mostly fail to meet these criteria. They learn problem-speciÔ¨Åc‚Äîrather than re-usable‚Äîlosses. If
applied to DG, this would imply replacing simple ERM learning with sophisticated meta-learning"
INTRODUCTION,0.015748031496062992,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.01837270341207349,"pipelines to train a loss on a per-problem basis. In contrast, our discovered loss provides a drop in
replacement for CE that leads standard training pipelines to produce more robust models."
INTRODUCTION,0.02099737532808399,"To train a general purpose robust loss function we need a search space that is Ô¨Çexible enough to
include interesting new losses, but simple enough to generalise across tasks without overÔ¨Åtting to the
problem used for loss learning. We choose a 12-dimensional space of fourth order Taylor polynomials.
Furthermore, we need a loss that is suitable for all stages of training. This precludes the majority
of approaches based on online meta-learning which update the loss and base model iteratively (Li
et al., 2019a;c), and also suffer from short-horizon bias (Wu et al., 2018). Evolutionary methods
(Gonzalez & Miikkulainen, 2019) and reinforcement-learning (Li et al., 2019a) could support loss
learning in principle, but are too slow to be feasible. Therefore we develop the Ô¨Årst implicit-gradient
based approach to loss learning. This allows us to tractably compute meta-gradients of the target
recognition performance with respect to the loss used for training in the source domain."
INTRODUCTION,0.023622047244094488,"We use a simple DG task (RotatedMNIST) to train our robust loss, termed Implicit Taylor Loss (ITL),
to replace CE in ERM. Subsequent experiments show that ERM with ITL surpasses CE across a range
of DG benchmarks, and leads to state of the art performance, despite being much simpler and faster
than competitor DG methods. While the majority of existing DG methods require multiple source
domains to conduct data augmentation or feature alignment strategies, ITL improves single-source
domain generalisation, a crucial problem setting which has been minimally studied thus far."
INTRODUCTION,0.026246719160104987,"To summarise our contributions: (i) We provide the Ô¨Årst study on the signiÔ¨Åcance of supervised
loss function choice in DG (ii) We demonstrate the Ô¨Årst efÔ¨Åcient solution to loss-learning based on
meta-gradients computed by the Implicit Function Theorem. (iii) Empirically, we show that our
learned ITL loss enhances simple ERM and achieves state of the art DG performance across a range
of benchmarks, including the challenging single-source DG scenario."
RELATED WORK,0.028871391076115485,"2
RELATED WORK"
RELATED WORK,0.031496062992125984,"Domain Generalisation: Domain Generalisation aims to learn a model using data from one or more
source domains, but with the further requirement that it is robust to testing on novel target domain
data‚Äîwithout accessing target data during training. DG is now a well studied (Zhou et al., 2021a) area
with diverse approaches including data augmentation (Shankar et al., 2018; Zhou et al., 2021b), robust
training algorithms such as domain alignment objectives (Li et al., 2018b), and other regularisers
(Li et al., 2019c; Balaji et al., 2018). Most DG studies have assumed the multi-source setting, which
enables new data-augmentation strategies (Zhou et al., 2021b), and allows generalisation-promoting
design features to be tuned by domain-wise cross-validation. In particular, a few studies (Li et al.,
2019c; Balaji et al., 2018) have considered meta-learning based DG, where a regulariser applied in a
training domain is tuned by meta-gradients from the resulting validation-domain performance. The
resulting model is then deployed to the true target domain within the same family. These methods
require regulariser meta-learning for each given multi-source DG problem family. In contrast, we
propose to learn a simple loss function once, which then provides a drop-in replacement for CE in
any single-, or multi-source DG problem. A recent criticism of the DG literature showed that no
method consistently outperformed a well tuned ERM baseline on the carefully designed DomainBed
benchmark (Gulrajani & Lopez-Paz, 2021). Rather than competing with ERM, we simply enhance
the ERM loss function and this leads to a clear improvement on DomainBed."
RELATED WORK,0.03412073490813648,"Loss Function learning: Loss Function learning aims to discover new losses that improve model
optimisation from various perspectives including conventional generalisation performance, (Gonzalez
& Miikkulainen, 2019; Liu et al., 2021), optimisation efÔ¨Åciency (Li et al., 2019a; Gonzalez &
Miikkulainen, 2019; Wang et al., 2020; Bechtle et al., 2020), and noise robustness (Li et al., 2019a).
Key dichotomies are in the search space of black box (neural) (Bechtle et al., 2020; Li et al., 2019c)
vs white-box (human-readable) (Li et al., 2019a; Gonzalez & Miikkulainen, 2019; Wang et al., 2020)
losses; whether learned losses are problem speciÔ¨Åc (Li et al., 2019a; Wang et al., 2020) or reusable
(Gonzalez & Miikkulainen, 2019); the meta-optimisation algorithm used‚Äîevolution (Liu et al., 2021;
Gonzalez & Miikkulainen, 2019), RL (Li et al., 2019a; Wang et al., 2020; Bechtle et al., 2020), or
gradient (Li et al., 2019c); and whether the loss is updated ofÔ¨Çine (Liu et al., 2021; Gonzalez &
Miikkulainen, 2019) (long inner loop, typically intractable), or online (Li et al., 2019a; Wang et al.,
2020) (short inner loop, efÔ¨Åcient but suffers from short-horizon bias (Wu et al., 2018)). No studies
have thus far investigated loss learning for domain-shift robustness. In order to learn a reusable"
RELATED WORK,0.03674540682414698,Under review as a conference paper at ICLR 2022
RELATED WORK,0.03937007874015748,"robust loss we use a white-box loss search space of Taylor polynomials proposed in Gonzalez &
Miikkulainen (2020), and ofÔ¨Çine/long inner loop meta-learning. To make this meta-optimisation
tractable, we exploit the Implicit Function Theorem, to efÔ¨Åciently generate accurate hypergradients
of the validation domain performance with respect to the training domain loss function parameters.
Besides being the Ô¨Årst demonstration of loss learning for DG, to our knowledge it is also the Ô¨Årst
demonstration of any implicit gradient-based loss learning."
METHOD,0.04199475065616798,"3
METHOD"
METHOD,0.04461942257217848,"The need for Domain Generalisation arises when one is using machine learning to build a model
where the available training data is not representative of the data that will be observed by the model
once it has been deployed. In particular, it is assumed that there is an underlying distribution over
domains, P, from which we can sample several source domain distributions, {p(s)
1 , ..., p(s)
n
‚àºP},
to make use of during training. We can construct a training set for each of these source domain
distributions by sampling K data points, D(s)
i
= {(x(s,j)
i
, y(s,j)
i
) ‚àºp(s)
i }K
j=1, and use the union of"
METHOD,0.047244094488188976,"all these sets as the full training set, D(s) = Sn
i=1 D(s)
i . Empirical Risk Minimisation (ERM) then
simply Ô¨Ånds the model parameters, Œ∏, that minimise the loss measured on this training set,"
METHOD,0.049868766404199474,"min
Œ∏
1
n n
X i=1"
K,0.05249343832020997,"1
K K
X"
K,0.05511811023622047,"j=1
L(fŒ∏(x(j)
i ), y(j)
i ),
(1)"
K,0.05774278215223097,"where L(¬∑, ¬∑) is a loss function (typically cross entropy) measuring how well the predicted labels
match the ground truth labels. One can empirically check the resulting model‚Äôs robustness to domain
shift by sampling one or more target domain distributions, {p(t)
1 , ..., p(t)
m ‚àºP}, from the same
distribution over domains that was used to generate the training data. Data can then be sampled for
each of these target domains, yielding a test dataset D(t) = Sm
i=1 D(t)
i . Standard evaluation metrics
such as accuracy can then be computed using this data."
META-LEARNING LOSSES FOR DG,0.06036745406824147,"3.1
META-LEARNING LOSSES FOR DG"
META-LEARNING LOSSES FOR DG,0.06299212598425197,"Our goal is to replace the standard CE loss typically used in ERM with a learned loss function. We
are motivated by recent work showing that learned losses can enable models to perform better for a
variety of other problem settings, such as training with label noise (Wang et al., 2019) and improving
calibration (Mukhoti et al., 2020). We formulate the task of learning the parameters, œâ, of a loss
function, Lœâ, as a bilevel optimisation problem. The outer objective is to Ô¨Ånd the œâ that maximises
the performance of a model evaluated on the target domain data, and the inner problem is to train
a model to minimise the value of Lœâ measured on the source domain data. The loss parameters
are optimised using gradient-based methods that take advantage of the implicit function theorem to
efÔ¨Åciently compute gradients for the outer optimisation problem. Crucially, once the optimal loss
function œâ‚àóhas been found, new DG problems can be solved via ERM on the Lœâ‚àóloss."
META-LEARNING LOSSES FOR DG,0.06561679790026247,The bilevel optimisation we use to formalise the meta-learning process is given by
META-LEARNING LOSSES FOR DG,0.06824146981627296,"œâ‚àó= arg min
œâ
1
m m
X i=1"
K,0.07086614173228346,"1
K K
X"
K,0.07349081364829396,"j=1
M(fŒ∏‚àó(œâ)(x(t,j)
i
), y(t,j)
i
)
(2)"
K,0.07611548556430446,"s.t.
Œ∏‚àó(œâ) = arg min
Œ∏"
N,0.07874015748031496,"1
n n
X i=1"
K,0.08136482939632546,"1
K K
X"
K,0.08398950131233596,"j=1
Lœâ(fŒ∏(x(s,j)
i
), y(s,j)
i
)
(3)"
K,0.08661417322834646,"where M is a loss function used to measure the performance of the model on the target domains,
typically chosen to be cross entropy."
K,0.08923884514435695,"Optimising œâ is challenging due to the need to backpropagate through the long inner loop optimi-
sation of Œ∏. Existing approaches for learning loss functions typically resort to slow evolutionary
or reinforcement learning updates (Li et al., 2019a; Gonzalez & Miikkulainen, 2019; Wang et al.,
2020) in the outer loop, or to an online approximation based on alternating steps on œâ and Œ∏ (Li et al.,
2019a; Wang et al., 2020). The latter approach leads to a loss function œâ‚àóthat cannot be transferred"
K,0.09186351706036745,Under review as a conference paper at ICLR 2022
K,0.09448818897637795,"to new tasks, as it suffers from a short-horizon bias (Wu et al., 2018). To solve this problem, we
use the Implicit Function Theorem (IFT) to compute ‚àÇM"
K,0.09711286089238845,"‚àÇœâ without truncating the inner optimisation
problem to approximate Œ∏‚àó(œâ)."
IMPLICIT GRADIENT,0.09973753280839895,"3.2
IMPLICIT GRADIENT"
IMPLICIT GRADIENT,0.10236220472440945,"The conceptually simplest way to optimise œâ is to store all the intermediate iterates generated by
the optimiser when training the network in the inner loop, and to then backpropagate through all of
these weight updates (Maclaurin et al., 2015). This becomes prohibitively expensive in both memory
and computation. Instead, after Ô¨Ånding Œ∏‚àó(œâ) we compute the gradient using the Implicit Function
Theorem (IFT). The implicit gradient computation takes advantage of the fact that ‚àÇLœâ"
IMPLICIT GRADIENT,0.10498687664041995,"‚àÇŒ∏ = 0, because
we have found locally optimal model parameters for the inner problem. The gradient we want to
compute is given by
‚àÇM"
IMPLICIT GRADIENT,0.10761154855643044,‚àÇœâ = ‚àÇM
IMPLICIT GRADIENT,0.11023622047244094,"‚àÇŒ∏
‚àÇŒ∏
‚àÇœâ"
IMPLICIT GRADIENT,0.11286089238845144,"œâ,Œ∏‚àó(œâ)
,
(4)"
IMPLICIT GRADIENT,0.11548556430446194,and the IFT can be used to obtain
IMPLICIT GRADIENT,0.11811023622047244,"‚àÇŒ∏
‚àÇœâ = ‚àí
h ‚àÇ2Lœâ"
IMPLICIT GRADIENT,0.12073490813648294,"‚àÇŒ∏ ‚àÇŒ∏T
| {z }
|Œ∏|√ó|Œ∏|"
IMPLICIT GRADIENT,0.12335958005249344,"i‚àí1
√ó
‚àÇ2Lœâ
‚àÇŒ∏ ‚àÇœâT
|
{z
}
|Œ∏|√ó|œâ| .
(5)"
IMPLICIT GRADIENT,0.12598425196850394,"The inverse of the Hessian can be rephrased in terms of a Neumann series,"
IMPLICIT GRADIENT,0.12860892388451445,h ‚àÇ2Lœâ
IMPLICIT GRADIENT,0.13123359580052493,‚àÇŒ∏ ‚àÇŒ∏T
IMPLICIT GRADIENT,0.13385826771653545,"i‚àí1
= lim
i‚Üí‚àû i
X"
IMPLICIT GRADIENT,0.13648293963254593,"j=0
[I ‚àí‚àÇ2Lœâ"
IMPLICIT GRADIENT,0.13910761154855644,"‚àÇŒ∏ ‚àÇŒ∏T ]j,
(6)"
IMPLICIT GRADIENT,0.14173228346456693,"and approximated by truncating the summation to a Ô¨Ånite number of terms. In practice, one can
make use of vector-Jacobian products to avoid explicitly constructing the Hessian in the summation.
Further details can be found in Lorraine et al. (2020), but we provide pseudo-code for computing the
implicit gradient in Algorithm 2."
IMPLICIT GRADIENT,0.14435695538057744,Algorithm 1 IFT-based loss learning for DG.
IMPLICIT GRADIENT,0.14698162729658792,"1: Input: P, œâ
2: Output: œâ‚àó"
IMPLICIT GRADIENT,0.14960629921259844,"3: Init œâ
4: while not converged or reached max steps do
5:
sample p1, ..., pn from P
6:
sample D1, ..., Dn from p1, ..., pn
7:
Init H = 0 ‚ààRn√ó|œâ|"
IMPLICIT GRADIENT,0.15223097112860892,"8:
for all Di do
9:
Init Œ∏i {Get random network weights}
10:
Ds
= {D1, ..., Dn}/Di,
Dt
= Di {Construct
source/target splits}
11:
Œ∏‚àó
i = arg minŒ∏ Lœâ(Œ∏i, Ds) {Train the network}
12:
hi = Hypergradient(Lœâ, M, (œâ, Œ∏‚àó
i ), Œ±)
13:
H[i, :] = hi
14:
end for
15:
h = grad-surgery(H)
16:
œâ = œâ ‚àíŒ∑h {Update the loss function}
17: end while"
IMPLICIT GRADIENT,0.15485564304461943,"Algorithm 2 Computing the hyper-
gradient of the meta-objective M,
with respect to the loss œâ.
The
grad(¬∑, ¬∑, ¬∑) function from PyTorch
computes a Jacobian-vector prod-
uct when called with a non-scalar
Ô¨Årst argument.
Inspired by Lor-
raine et al. (2020), we use this to
efÔ¨Åciently compute the Hessian re-
quired for approximating the Neu-
mann series."
IMPLICIT GRADIENT,0.15748031496062992,"Input: Lœâ, M, (œâ, Œ∏‚àó), Œ±
Output:‚àíp ‚àÇ2Lœâ"
IMPLICIT GRADIENT,0.16010498687664043,"‚àÇŒ∏‚àÇœâ
v = p = ‚àÇM"
IMPLICIT GRADIENT,0.16272965879265092,"‚àÇŒ∏ |(œâ,Œ∏‚àó)
for all j = 1, ..., J do"
IMPLICIT GRADIENT,0.16535433070866143,v‚àí= Œ± ¬∑ grad( ‚àÇLœâ
IMPLICIT GRADIENT,0.1679790026246719,"‚àÇŒ∏ , Œ∏, v)
p += v
end for"
ROBUST GRADIENT ESTIMATION,0.17060367454068243,"3.3
ROBUST GRADIENT ESTIMATION"
ROBUST GRADIENT ESTIMATION,0.1732283464566929,"Algorithm 1 summarizes the gradient estimation procedure. To obtain high quality gradient estimates
in each outer loop iteration, we employ a leave-one-domain-out strategy. The DG task, P, used for
meta-training the loss parameters has m domains associated with it. In each iteration of the outer
loop, we train m networks with the prospective loss (i.e., we instantiate m different copies of the"
ROBUST GRADIENT ESTIMATION,0.17585301837270342,Under review as a conference paper at ICLR 2022
ROBUST GRADIENT ESTIMATION,0.1784776902887139,"ùúÉ‚àó= arg min "" ‚Ñí#"
ROBUST GRADIENT ESTIMATION,0.18110236220472442,Inner Loop
ROBUST GRADIENT ESTIMATION,0.1837270341207349,Outer Loop
ROBUST GRADIENT ESTIMATION,0.18635170603674542,"Meta-
Train ùúÉ‚àó"
ROBUST GRADIENT ESTIMATION,0.1889763779527559,ùúî‚àó= arg min #
ROBUST GRADIENT ESTIMATION,0.19160104986876642,"ùëÄ(ùëì""‚àó(#))"
ROBUST GRADIENT ESTIMATION,0.1942257217847769,Source
ROBUST GRADIENT ESTIMATION,0.1968503937007874,Target
ROBUST GRADIENT ESTIMATION,0.1994750656167979,Deploy Loss ‚Ñí#‚àó
ROBUST GRADIENT ESTIMATION,0.2020997375328084,"ùúÉ‚àó= arg min "" ‚Ñí#"
ROBUST GRADIENT ESTIMATION,0.2047244094488189,"Inner Loop
Meta-
test ùúÉ‚àó"
ROBUST GRADIENT ESTIMATION,0.2073490813648294,Source
ROBUST GRADIENT ESTIMATION,0.2099737532808399,Target
ROBUST GRADIENT ESTIMATION,0.2125984251968504,"ùëì""‚àó
Guitar"
ROBUST GRADIENT ESTIMATION,0.2152230971128609,"Figure 1: Algorithm schematic. Loss Lœâ is trained to optimize held-out domain performance on
R-MNIST and then deployed on novel datasets."
ROBUST GRADIENT ESTIMATION,0.2178477690288714,"inner loop), where each network has a different target domain and the remainder of the domains are
used to train the network. We can then compute a gradient for each of the m networks and aggregate
them together in order to perform an update to the loss parameters. Rather than using the mean
gradient, we found aggregation using gradient surgery (Yu et al., 2020), which reduces the gradient
noise caused by different source/target domain splits in the inner loop, to work better in practice."
TAYLOR POLYNOMIAL REPRESENTATION,0.2204724409448819,"3.4
TAYLOR POLYNOMIAL REPRESENTATION"
TAYLOR POLYNOMIAL REPRESENTATION,0.2230971128608924,"The choice of loss function parameterisation is a crucial factor in our framework. One must balance the
ability to represent a sufÔ¨Åciently broad range of loss functions, with the susceptibility to overÔ¨Åtting the
data used to learn the loss. The search space we consider is based on the truncated Taylor polynomials
used by the evolutionary optimisation loss learning approach of (Gonzalez & Miikkulainen, 2020).
This family of loss functions treats the point around which the Taylor polynomial is centred, and
also the value of the derivatives at this point, as learnable parameters. In this sense, it is a variational
learning method‚Äîthough it should be stressed it is not a variational Bayesian method. The family of
Œ≤ order multivariate Taylor polynomials has the form"
TAYLOR POLYNOMIAL REPRESENTATION,0.22572178477690288,"‚Ñì(z) = Œ≤
X n=0"
TAYLOR POLYNOMIAL REPRESENTATION,0.2283464566929134,"1
n!‚àán‚Ñì(c)T (z ‚àíc)n,
(7)"
TAYLOR POLYNOMIAL REPRESENTATION,0.23097112860892388,"where c is a Ô¨Åxed point around which function is being expanded. Because c is Ô¨Åxed, the values of
the derivative at this point are also Ô¨Åxed. As such, we can replace c and ‚àán‚Ñì(c) with meta-learnable
parameters. This allows us to parameterize the learned loss function in terms of the gradients it should
have at a meta-learnable point. We deÔ¨Åne our learnable loss as"
TAYLOR POLYNOMIAL REPRESENTATION,0.2335958005249344,"Lœâ(ÀÜy, y) = 1 C C
X"
TAYLOR POLYNOMIAL REPRESENTATION,0.23622047244094488,"i=1
‚Ñìœâ(ÀÜyi, yi),
‚Ñìœâ(ÀÜyi, yi) = Œ≤
X n=0"
TAYLOR POLYNOMIAL REPRESENTATION,0.2388451443569554,"1
n!‚àán‚Ñì([œâ0, œâ1])T ([ÀÜyi, yi]‚àí[œâ0, œâ1])n, (8)"
TAYLOR POLYNOMIAL REPRESENTATION,0.24146981627296588,"where each ‚àán‚Ñì([œâ0, œâ1]) can actually be replaced by introducing more meta-parameters to œâ. Please
see Appendix A.4 for an expanded deÔ¨Ånition of this loss function."
ALGORITHM SUMMARY,0.2440944881889764,"3.5
ALGORITHM SUMMARY"
ALGORITHM SUMMARY,0.24671916010498687,"Meta-train:
Given a set of training domains, the loss function search space in Section 3.4, and
efÔ¨Åcient update strategy in Section 3.2 and Algorithm 1, we are able to train a robust loss function
Lœâ. We conduct such loss function learning only once using a small dataset, and then evaluate the
resulting loss on a variety of larger datasets that are unseen during meta-training. Meta-test: Given
the learned loss function Lœâ‚àó, we Ô¨Åx it and use it together with the ERM algorithm for novel DG
tasks. Each target problem is trained from scratch and has not been seen during loss learning. An
overview of the algorithm, and the learning curve of the meta-train phase, are given in Figure 1."
EXPERIMENTS,0.24934383202099739,"4
EXPERIMENTS"
EXPERIMENTS,0.25196850393700787,"4.1
DATASET AND IMPLEMENTATION DETAILS."
EXPERIMENTS,0.2545931758530184,"Meta-train stage: We aim to learn a general purpose loss function that can be used in diverse DG
problems, but we Ô¨Årst need to select a dataset for initial loss learning. We chose RotatedMNIST (Ghi-"
EXPERIMENTS,0.2572178477690289,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.25984251968503935,Table 1: Resnet18 Cross-domain recognition accuracy (%) on PACS.
EXPERIMENTS,0.26246719160104987,"Target set
Art
Cartoon
Photo
Sketch
Avg."
EXPERIMENTS,0.2650918635170604,"Epi-FCR (Li et al., 2019b)
82.1
77.0
93.9
73.0
81.5
JiGen (Carlucci et al., 2019)
79.4
75.3
96.0
71.6
80.5
MASF (Dou et al., 2019)
80.3
77.2
95.0
71.7
81.0
CrossGrad (Shankar et al., 2018)
79.8
76.8
96.0
70.2
80.7
Entropy (Zhao et al., 2020)
80.7
76.4
96.7
71.8
81.4
L2A-OT (Zhou et al., 2020)
83.3
78.2
96.2
73.6
82.8
RSC (reported in Huang et al. (2020))
83.43
80.31
95.99
80.85
85.15"
EXPERIMENTS,0.2677165354330709,"Mixstyle (rs) (Zhou et al., 2021b)
82.3 ¬± 0.2
79.0 ¬± 0.3
96.3 ¬± 0.3
73.8 ¬± 0.9
82.8
Mixstyle (dl) (Zhou et al., 2021b)
84.1 ¬± 0.4
78.8 ¬± 0.4
96.1 ¬± 0.3
75.9 ¬± 0.9
83.7
RSC (our rerun their code)
79.25 ¬± 0.69
77.63 ¬± 0.50
93.61 ¬± 0.37
78.11 ¬± 1.40
81.91
RSC + ITL
81.67 ¬± 0.80
76.56 ¬± 0.50
95.57 ¬± 0.22
77.05 ¬± 0.56
82.71
ERM + BCE
71.19 ¬± 0.81
70.82 ¬± 0.29
93.11 ¬± 0.78
57.65 ¬± 0.98
73.19
ERM + CE
76.9 ¬± 0.6
76.5 ¬± 0.7
93.3 ¬± 0.1
68.8 ¬± 0.6
78.9
ITL-Net (ERM+ITL)
83.9 ¬± 0.4
78.9 ¬± 0.6
94.8 ¬± 0.2
80.1 ¬± 0.6
84.4"
EXPERIMENTS,0.27034120734908135,"fary et al., 2015), which contains six different domains that are all derived from MNIST (LeCun
& Cortes, 2010) but with different rotations: 0%, 15%, 30%, 45%, 60%, and 75%. The leave-
one-domain-out strategy for robust implicit gradient estimation, therefore, results in six inner loop
instantiations for each outer loop iteration. For efÔ¨Åciency, we use 2-layer MLPs as the base model,
which contains 1024-256-10 units from the input layer to the output one with ReLU as the activation
function. The learning rates in the inner loop and outer loop are both 0.01, a batch size of 32 is used
for the inner loop, and the Neumann series used for approximating the inverse Hessian is truncated at
15 iterations. The result is a set of 12 parameters that deÔ¨Åne the learned fourth-order polynomial loss
function and an example of learned loss is shown in Appendix A.3. The meta-train compute cost is
descripted in Appendix A.6."
EXPERIMENTS,0.27296587926509186,"Meta-test (deployment) stage: We now evaluate our learned ITL using ERM, but with our learned
loss instead of cross entropy. Models trained with our method are denoted by ITL-Net. We evaluate
the learned loss function on the four common DG benchmarks: VLCS (Fang et al., 2013), PACS (Li
et al., 2018a), OfÔ¨ÅceHome (Venkateswara et al., 2017), and Terra Incognita (Beery et al., 2018). Two
sets of experiments are conducted: (i) We evaluate the conventional PACS benchmark, as it is the
most widely used in the DG literature, and enables comparison against the most recent state-of-the-art
competitors. (ii) We evaluate all four benchmarks using the recent DomainBed platform, which is
designed to enforce fair and consistent hyperparameter tuning across different methods."
RESULTS,0.2755905511811024,"4.2
RESULTS"
RESULTS,0.2782152230971129,"PACS: Setup
A pre-trained ResNet18 backbone is used throughout, together with the source and
target domain split described in (Li et al., 2018a). We train ResNet-18 with ITL on the training split
and perform model selection using the validation set. We use same set of hyperparameters (learning
rate of 0.001, weight decay of 0.00001, and batch size of 32 for each domain) for the baseline ERM
with Cross-Entropy (ERM +CE) and Binary Cross-Entropy (BCE) to train our model. We compare
ITL-Net with the existing state-of-the-art methods on this benchmark, including RSC (Huang et al.,
2020), data augmentation-based L2A-OT (Zhou et al., 2021b), Mixstyle (Zhou et al., 2020) including
random shufÔ¨Çe (rs) and domain label (dl), regulariser-based Entropy (Zhao et al., 2020), adversarial
gradient-based CrossGrad (Shankar et al., 2018), meta learning-based MASF (Dou et al., 2019) and
Epi-FCR (Li et al., 2019b) and self-supervision-based JiGen (Carlucci et al., 2019)."
RESULTS,0.28083989501312334,"PACS: Results
We Ô¨Årst conduct experiments using the classic PACS protocol to facilitate com-
parison against many recent competitors that were not evaluated on DomainBed. Table 1 compares
our ITL-Net performance vs state-of-the-art methods. From the results we can see that: (i) Simply
swapping out the loss in ERM from CE to ITL, leads to a signiÔ¨Åcant 5.5% improvement. (ii) Overall
our ITL-Net leads to state-of-the-art performance on this benchmark, surpassing the most recent and
sophisticated competitors such as Mixstyle."
RESULTS,0.28346456692913385,"DomainBed: Setup
We next evaluate ITL using the DomainBed platform, which enforces careful
and fair evaluation by ensuring that all competitors use the same hyper-parameter tuning strategy"
RESULTS,0.28608923884514437,Under review as a conference paper at ICLR 2022
RESULTS,0.2887139107611549,"Table 2: DomainBed Cross-domain recognition accuracy (%) with ResNet50 on ColoredMNIST
VLCS, PACS, TerraIncognita, OfÔ¨ÅceHome and DomainNet. Bottom: Results of Wilcoxon signed-
rank hypothesis test comparing ITL-Net against competitors."
RESULTS,0.29133858267716534,"Models
Dataset
ERM
SagNet
CORAL
CDANN
RSC
ITL-Net"
RESULTS,0.29396325459317585,"ColoredMNIST
51.5
51.7
51.5
51.7
51.7
52.0
VLCS
77.5
77.8
78.8
77.5
77.1
78.9
PACS
85.5
86.3
86.2
82.6
85.2
86.4
TerraIncoginita
46.1
48.6
47.6
45.8
46.6
51.0
OfÔ¨ÅceHome
66.5
68.1
68.7
65.8
65.5
69.3
DomainNet
40.9
40.3
41.5
38.3
38.9
41.6"
RESULTS,0.29658792650918636,"Avg. Rank
4.17
2.67
3.00
5.17
5.00
1.00"
RESULTS,0.2992125984251969,"p-value (H0)
Reject (0.016)
Reject (0.016)
Reject (0.016)
Reject (0.016)
Reject (0.016)"
RESULTS,0.30183727034120733,"Table 3: Cross-domain recognition accuracy (%) on DomainBed-PACS-Resnet50. Comparison
with alternative manually-designed robust losses."
RESULTS,0.30446194225721784,"Loss
ERM+CE
ERM+FOCAL
ERM+SCE
ERM+GCE
EMR+LS
ERM+ITL"
RESULTS,0.30708661417322836,"Avg Perf
83.9 ¬± 0.5
84.6 ¬± 0.8
84.2 ¬± 0.5
83.0 ¬± 0.2
84.9 ¬± 0.6
86.4 ¬± 0.5"
RESULTS,0.30971128608923887,"(random search, driven by source domain validation performance), and the same number of hyperpa-
rameter search iterations. We follow the standard DomainBed protocol and use a ResNet-50, with
experiments conducted on VLCS, PACS, OfÔ¨ÅceHome, and Terra Incognita."
RESULTS,0.3123359580052493,"DomainBed: Results
The results in Table 2 compare ITL-Net with ERM and some of the most
competitive published alternatives: SagNet (Nam et al., 2019), CORAL (Sun & Saenko, 2016),
CDANN (Li et al., 2018c) and RSC (Huang et al., 2020) in the original DomainBed paper (Gulrajani &
Lopez-Paz, 2021). A detailed comparison is given in Appendix 6. The conclusion of the DomainBed
study was that existing methods did not reliably beat ERM under this hyperparameter tuning protocol.
In contrast, we can see that ITL-Net provides a clear improvement on ERM and matches or improves
on the strongest existing competitor in each case, especially on TerraIncognita and OfÔ¨ÅceHome. To
formally compare ITL-Net with competitors, we perform signiÔ¨Åcance testing using the Wilcoxon
signed-rank test, where the p-value is set as 0.025 and the sample size is number of the Domain
datasets applied. For example, when comparing the performance of ITL-Net and ERM, the null
hypothesis (H0) is that ITL-Net has equal performance to ERM on DomainBed and the alternative
hypothesis (H1) that ITL-Net is statistically signiÔ¨Åcantly better than ERM on DomainBed.
The
results of the hypothesis tests comparing ITL to each competitor are summarized at the bottom of
Table 2, and conÔ¨Årm that ITL-Net outperforms them all."
RESULTS,0.31496062992125984,"Single Source DG: Setup
Most existing DG methods rely on the availability of multiple source
domains in some form: For example to synthesise new domains for data augmentation (Zhou et al.,
2021b), or perform feature alignment among training domains (Sun & Saenko, 2016). A unique
feature of our ITL-Net is that, since it is only a small modiÔ¨Åcation to ERM, it can be used to learn on
a single source domain. Although this setting is not well explored in the literature, it is obviously
highly practical as multiple source domains are often not available in practice. To explore this setting,
we modify the DomainBed benchmark to train on a single source at a time and average over each
source‚Üítarget combination, rather than training on the conjunction of all sources."
RESULTS,0.31758530183727035,"Single Source DG: Results
From the results in Table 4, we can see that performance drops across
the board compared to multi-source training (Table 2), as expected. However, the state of the art
alternatives CORAL and SagNet are no longer as competitive compared to ERM as they were in
the multi-source case (Table 2)‚Äîthis is expected as they are designed to exploit cues from multiple
source domains. In contrast, our ITL-Net maintains a clear lead over the conventional ERM with
cross entropy baseline in this setting. This is a signiÔ¨Åcant achievement as existing work has not
produced algorithms that improve robustness under the single-source setting."
RESULTS,0.32020997375328086,Under review as a conference paper at ICLR 2022
RESULTS,0.3228346456692913,"Table 4: Cross-domain recognition accuracy (%) on DomainBed with single source domain. The
heading of the table denotes the single source domain, and results average across all target domains."
RESULTS,0.32545931758530183,"Source Dataset
VLCS
PACS
OfÔ¨ÅceHome
TerraIncognita"
RESULTS,0.32808398950131235,"ERM
64.08
51.85
53.57
32.13
CORAL
64.07
51.84
53.51
32.13
SagNet
61.78
53.00
51.30
33.93
Mixup
59.01
54.92
52.70
30.80"
RESULTS,0.33070866141732286,"ITL-Net
62.17
56.54
55.04
35.09"
RESULTS,0.3333333333333333,"0
6
12 18 24 30 36 42 48 54 60
Training iteration x100 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6"
RESULTS,0.3359580052493438,Entropy
RESULTS,0.33858267716535434,Classified Entropy
RESULTS,0.34120734908136485,"CE
SCE"
RESULTS,0.3438320209973753,"FOCAL
LS ITL"
RESULTS,0.3464566929133858,"0
6
12 18 24 30 36 42 48 54 60
Training iteration x100 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6"
RESULTS,0.34908136482939633,Entropy
RESULTS,0.35170603674540685,Misclassified Entropy
RESULTS,0.3543307086614173,"CE
SCE"
RESULTS,0.3569553805774278,"FOCAL
LS ITL"
RESULTS,0.35958005249343833,"Figure 2: The evolution of posterior entropy for target domain test samples during training. Left:
Correctly classiÔ¨Åed samples. Right: MisclassiÔ¨Åed samples."
FURTHER ANALYSIS,0.36220472440944884,"4.3
FURTHER ANALYSIS"
FURTHER ANALYSIS,0.3648293963254593,"Qualitative comparison and impact on learning
To understand our learned loss, we visually
compare it Figure 5 with several other loss functions: CE, SCE (Wang et al., 2019), Label Smoothing
(LS) (Pereyra et al., 2017), and FOCAL (Mukhoti et al., 2020). Compared to the standard cross entropy
loss, we can see that ITL has softer penalties for severe misclassiÔ¨Åcation, and stronger penalties for
moderate misclassiÔ¨Åcation. In addition, it can be noticed that FOCAL has the most similar shape
to our ITL. The properties of the FOCAL loss for probability calibration were recently studied in
(Mukhoti et al., 2020). Motivated by this, we report the evolution of the target-domain entropy
of correctly and incorrectly classiÔ¨Åed samples during training in Figure 2(left) and Figure 2(right)
respectively. Clearly, a byproduct of ITL compared to CE and SCE is a (desirable) increase in
uncertainty for misclassiÔ¨Åed instances. FOCAL has similar behaviour to ITL in general, especially
the entropy of prediction distributions for misclassiÔ¨Åed instances."
FURTHER ANALYSIS,0.3674540682414698,"Quantitative comparison to other robust losses
We next investigate whether the good perfor-
mance of ITL in cross-domain robustness can be easily replicated by applying existing robust loss
functions, or whether our meta-learning pipeline has learned something new in terms of robust model
training. SCE (Wang et al., 2019) and GCE (Zhang & Sabuncu, 2018) were designed with label-noise
robustness in mind, while Focal (Mukhoti et al., 2020; Lin et al., 2017) was designed for class
imbalance and calibration. Label-smooth (LS) (Pereyra et al., 2017) is for improving generalisation
and reducing overconÔ¨Ådence. From the results in Table 3, we can see that while some losses improve
on CE, ITL leads to the clearest improvement. The somewhat similar behaviour of ITL and FOCAL
in terms of induced entropy from Figure 2 does not carry over similar cross-domain recognition
accuracy. Note that all experiments in Table 3 were run by us, while competitor performance in
Table 2 is taken from (Gulrajani & Lopez-Paz, 2021)."
FURTHER ANALYSIS,0.3700787401574803,"Loss landscape analysis and Perturbation analysis
One of the factors that affect model general-
isation is the loss landscape at convergence. (Keskar et al., 2017; Chaudhari et al., 2019) observed
that Ô¨Çatter loss landscapes lead to good generalisation of the learned model. To this end, we compared
a 1D slice through the loss landscape of ITL-Net with that of ERM on both source domain and target
domains. Namely, we perturb the converged paraeters by moving it around though gradient direction
which generated by the evigenvector of the Hessian matrix . From Figure 4, we can see that for each
held out target domain, ITL-Nets have Ô¨Çatter loss landscapes compared with models trained by CE
with respect to the source domains. To further analyse the quality of the minimas provided by CE"
FURTHER ANALYSIS,0.37270341207349084,Under review as a conference paper at ICLR 2022
FURTHER ANALYSIS,0.3753280839895013,"Table 5: Cross-domain recognition accuracy (%) on OfÔ¨ÅceHome: Impact of meta-train seed (¬±
standard deviation), and choice of pre-training dataset."
FURTHER ANALYSIS,0.3779527559055118,"Target set
Artistic
Clipart
Product
Real World
Avg.
ITL-NET (RotatedMNIST)
64.22 ¬± 0.84
56.25 ¬± 0.38
77.52 ¬± 0.32
78.12 ¬± 0.32
69.03 ¬± 0.18
ITL-NET (RotatedKMNIST)
64.28 ¬± 0.30
55.84 ¬± 0.29
76.89 ¬± 1.04
77.96 ¬± 0.30
68.74 ¬± 0.35"
FURTHER ANALYSIS,0.3805774278215223,"None
0.01
0.03
0.05
Gaussian noise std 56 58 60 62 64 66"
FURTHER ANALYSIS,0.38320209973753283,Accuracy
FURTHER ANALYSIS,0.3858267716535433,Held-out domain: Art
FURTHER ANALYSIS,0.3884514435695538,"ITL-Net
ERM"
FURTHER ANALYSIS,0.3910761154855643,"None
0.01
0.03
0.05
Gaussian noise std 46 48 50 52 54 56"
FURTHER ANALYSIS,0.3937007874015748,Accuracy
FURTHER ANALYSIS,0.3963254593175853,Held-out domain: Clipart
FURTHER ANALYSIS,0.3989501312335958,"ITL-Net
ERM"
FURTHER ANALYSIS,0.4015748031496063,"None
0.01
0.03
0.05
Gaussian noise std"
FURTHER ANALYSIS,0.4041994750656168,"71
72
73
74
75
76
77"
FURTHER ANALYSIS,0.4068241469816273,Accuracy
FURTHER ANALYSIS,0.4094488188976378,Held-out domain: Product
FURTHER ANALYSIS,0.4120734908136483,"ITL-Net
ERM"
FURTHER ANALYSIS,0.4146981627296588,"None
0.01
0.03
0.05
Gaussian noise std"
FURTHER ANALYSIS,0.41732283464566927,"72
73
74
75
76
77
78"
FURTHER ANALYSIS,0.4199475065616798,Accuracy
FURTHER ANALYSIS,0.4225721784776903,Held-out domain: Real-World
FURTHER ANALYSIS,0.4251968503937008,"ITL-Net
ERM"
FURTHER ANALYSIS,0.42782152230971127,"Figure 3: Perturbation analysis on OfÔ¨ÅceHome: ITL-Net vs ERM. Multiplicative Gaussian noise
with mean 1 and std: 0.01, 0.05, 0.08 is added to network weights."
FURTHER ANALYSIS,0.4304461942257218,"and ITL, we follow the perturbation analysis routine in (Keskar et al., 2017; Zhang et al., 2018) by
adding multiplicative noise to the weights of the converged models. From the results in Figure 3, it
can be see that ITL-Nets outperform ERM at every noise ratio."
FURTHER ANALYSIS,0.4330708661417323,"0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5"
FURTHER ANALYSIS,0.4356955380577428,ERM Loss
FURTHER ANALYSIS,0.43832020997375326,"-0.8 -0.4
0.0
0.4
0.8
Perturbation"
FURTHER ANALYSIS,0.4409448818897638,"0.0
0.2
0.4
0.6
0.8
1.0
1.2"
FURTHER ANALYSIS,0.4435695538057743,ITL Loss
FURTHER ANALYSIS,0.4461942257217848,Held-out domain: Art
FURTHER ANALYSIS,0.44881889763779526,"ITL-NET
ERM"
FURTHER ANALYSIS,0.45144356955380577,"0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5"
FURTHER ANALYSIS,0.4540682414698163,ERM Loss
FURTHER ANALYSIS,0.4566929133858268,"-0.8 -0.4 0.0
0.4
0.8
Perturbation -10.0 -8.0 -6.0 -4.0 -2.0"
FURTHER ANALYSIS,0.45931758530183725,ITL Loss
FURTHER ANALYSIS,0.46194225721784776,Held-out domain: Clipart
FURTHER ANALYSIS,0.4645669291338583,"ITL-NET
ERM"
FURTHER ANALYSIS,0.4671916010498688,"0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5"
FURTHER ANALYSIS,0.46981627296587924,ERM Loss
FURTHER ANALYSIS,0.47244094488188976,"-0.8 -0.4
0.0
0.4
0.8
Perturbation"
FURTHER ANALYSIS,0.47506561679790027,"0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5"
FURTHER ANALYSIS,0.4776902887139108,ITL Loss
FURTHER ANALYSIS,0.48031496062992124,Held-out domain: Art
FURTHER ANALYSIS,0.48293963254593175,"ITL-NET
ERM"
FURTHER ANALYSIS,0.48556430446194226,"2.0
4.0
6.0
8.0
10.0
12.0
14.0
16.0"
FURTHER ANALYSIS,0.4881889763779528,ERM Loss
FURTHER ANALYSIS,0.49081364829396323,"-0.8 -0.4 0.0
0.4
0.8
Perturbation -10.5 -10.0 -9.5 -9.0 -8.5"
FURTHER ANALYSIS,0.49343832020997375,ITL Loss
FURTHER ANALYSIS,0.49606299212598426,Held-out domain: Clipart
FURTHER ANALYSIS,0.49868766404199477,"ITL-NET
ERM"
FURTHER ANALYSIS,0.5013123359580053,"Figure 4: 1D Loss Landscape: ITL-Net vs ERM on OfÔ¨ÅceHome. Left two: Source domain loss
landscape. Right two: Target domain loss landscape."
FURTHER ANALYSIS,0.5039370078740157,"Repeatability analysis
Our message thus far is that a single loss produced by our pipeline can be
re-used as a plug-and-play modiÔ¨Åcation to improve vanilla ERM+CE on a wide variety of held-out
downstream DG tasks. That said, one might reasonably wonder about the reliability of the loss
function learning procedure itself. To investigate this, we repeat our entire pipeline including the
meta-train stage Ô¨Åve times. We then evaluate the consistency of the resulting Ô¨Åve loss functions on
the downstream ColoredMNIST task. Furthermore, to evaluate the dependence of our result on the
choice of meta-training dataset, we repeat the above experiment on RotatedKMNIST (Clanuwat et al.,
2018) to replace the RotatedMNIST used previously. From the results in Table 5, we can see that
performance is quite consistent over trials (small standard deviation). It also differs little with choice
of pre-training dataset - with both options performing well compared to competitors in Table 2."
FURTHER ANALYSIS,0.5065616797900262,"Limitations
ITL improves ERM+CE for DG tasks in general, but in some cases, the margin over
SOTA is small, since other state-of-the-art competitors may beat ERM+CE. It would be more inter-
esting if ITL is highly complementary to SOTA methods based on other architectural, augmentation,
or domain-alignment improvements ‚Äì but this remains for future work to determine."
CONCLUSION,0.5091863517060368,"5
CONCLUSION"
CONCLUSION,0.5118110236220472,"We provided the Ô¨Årst study of the effect of ERM loss functions on Domain Generalisation. We
observe that models trained by ERM with existing robust loss functions can improve performance on
Domain Generalisation compared with those trained by Cross-Entropy. To discover the best loss for
DG, we perform meta-learning to Ô¨Ånd a re-usable white-box loss function. This is tractably solved
using IFT to obtain gradients of the target domain performance with respect to the source domain
loss parameters. This also provides the Ô¨Årst demonstration of IFT-based loss learning in the literature.
The results show that a simple modiÔ¨Åcation to the standard ERM pipeline improves both multi-source
and single source DG, and even surpasses the purpose-designed state of the art models."
CONCLUSION,0.5144356955380578,Under review as a conference paper at ICLR 2022
REFERENCES,0.5170603674540682,REFERENCES
REFERENCES,0.5196850393700787,"Yogesh Balaji, Swami Sankaranarayanan, and Rama Chellappa. Metareg: Towards domain general-
ization using meta-regularization. NeurIPS, 2018."
REFERENCES,0.5223097112860893,"Sarah Bechtle, Artem Molchanov, Yevgen Chebotar, Edward Grefenstette, Ludovic Righetti, Gaurav
Sukhatme, and Franziska Meier. Meta-learning via learned loss. ICPR, 2020."
REFERENCES,0.5249343832020997,"Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In ECCV, 2018."
REFERENCES,0.5275590551181102,"Fabio M Carlucci, Antonio D‚ÄôInnocente, Silvia Bucci, Barbara Caputo, and Tatiana Tommasi. Domain
generalization by solving jigsaw puzzles. In CVPR, 2019."
REFERENCES,0.5301837270341208,"Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs,
Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing gradient descent
into wide valleys. Journal of Statistical Mechanics: Theory and Experiment, 2019(12):124018,
2019."
REFERENCES,0.5328083989501312,"Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David
Ha. Deep learning for classical japanese literature. In NeurIPS (Workshop), 2018."
REFERENCES,0.5354330708661418,"Qi Dou, Daniel C. Castro, Konstantinos Kamnitsas, and Ben Glocker. Domain generalization via
model-agnostic learning of semantic features. In NeurIPS, 2019."
REFERENCES,0.5380577427821522,"Chen Fang, Ye Xu, and Daniel N Rockmore. Unbiased metric learning: On the utilization of multiple
datasets and web images for softening bias. In ICCV, 2013."
REFERENCES,0.5406824146981627,"Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, and David Balduzzi. Domain generalization
for object recognition with multi-task autoencoders. In ICCV, 2015."
REFERENCES,0.5433070866141733,"Santiago Gonzalez and Risto Miikkulainen. Improved training speed, accuracy, and data utilization
through loss function optimization. arXiv preprint arXiv:1905.11528, 2019."
REFERENCES,0.5459317585301837,"Santiago Gonzalez and Risto Miikkulainen. Optimizing loss functions through multivariate taylor
polynomial parameterization. arXiv preprint arXiv:2002.00059, 2020."
REFERENCES,0.5485564304461942,"Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In International
Conference on Learning Representations, 2021."
REFERENCES,0.5511811023622047,"Zeyi Huang, Haohan Wang, Eric P Xing, and Dong Huang. Self-challenging improves cross-domain
generalization. In Computer Vision‚ÄìECCV 2020: 16th European Conference, Glasgow, UK, August
23‚Äì28, 2020, Proceedings, Part II 16, pp. 124‚Äì140. Springer, 2020."
REFERENCES,0.5538057742782152,"Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter
Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In ICLR,
2017."
REFERENCES,0.5564304461942258,"Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsub-
ramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne
David, Ian Stavness, Wei Guo, Berton Earnshaw, Imran Haque, Sara M Beery, Jure Leskovec, An-
shul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. Wilds: A benchmark
of in-the-wild distribution shifts. In ICML, 2021."
REFERENCES,0.5590551181102362,Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010.
REFERENCES,0.5616797900262467,"Chuming Li, Xin Yuan, Chen Lin, Minghao Guo, Wei Wu, Junjie Yan, and Wanli Ouyang. Am-lfs:
Automl for loss function search. In ICCV, 2019a."
REFERENCES,0.5643044619422573,"Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy Hospedales. Learning to generalize: Meta-learning
for domain generalization. In AAAI, 2018a."
REFERENCES,0.5669291338582677,"Da Li, Jianshu Zhang, Yongxin Yang, Cong Liu, Yi-Zhe Song, and Timothy M Hospedales. Episodic
training for domain generalization. In ICCV, 2019b."
REFERENCES,0.5695538057742782,Under review as a conference paper at ICLR 2022
REFERENCES,0.5721784776902887,"Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot. Domain generalization with adversarial
feature learning. In CVPR, 2018b."
REFERENCES,0.5748031496062992,"Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao.
Deep domain generalization via conditional invariant adversarial networks. In ECCV, 2018c."
REFERENCES,0.5774278215223098,"Yiying Li, Yongxin Yang, Wei Zhou, and Timothy Hospedales. Feature-critic networks for heteroge-
neous domain generalization. In ICML, 2019c."
REFERENCES,0.5800524934383202,"Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll√°r. Focal loss for dense object
detection. In ICCV, 2017."
REFERENCES,0.5826771653543307,"Peidong Liu, Gengwei Zhang, Bochao Wang, Hang Xu, Xiaodan Liang, Yong Jiang, and Zhenguo
Li. Loss function discovery for object detection via convergence-simulation driven search. In
International Conference on Learning Representations, 2021."
REFERENCES,0.5853018372703412,"Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by
implicit differentiation. In AISTATS, 2020."
REFERENCES,0.5879265091863517,"Dougal Maclaurin, David Duvenaud, and Ryan P. Adams. Gradient-based hyperparameter optimiza-
tion through reversible learning. In ICML, 2015."
REFERENCES,0.5905511811023622,"Jishnu Mukhoti, Viveka Kulharia, Amartya Sanyal, Stuart Golodetz, Philip HS Torr, and Puneet K
Dokania. Calibrating deep neural networks using focal loss. NeurIPS, 2020."
REFERENCES,0.5931758530183727,"Hyeonseob Nam, HyunJae Lee, Jongchan Park, Wonjun Yoon, and Donggeun Yoo. Reducing domain
gap via style-agnostic networks. arXiv preprint arXiv:1910.11645, 2019."
REFERENCES,0.5958005249343832,"Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. 2017."
REFERENCES,0.5984251968503937,"Gabriel Pereyra, George Tucker, Jan Chorowski, ≈Åukasz Kaiser, and Geoffrey Hinton. Regularizing
neural networks by penalizing conÔ¨Ådent output distributions. In ICLR, 2017."
REFERENCES,0.6010498687664042,"Shiv Shankar, Vihari Piratla, Soumen Chakrabarti, Siddhartha Chaudhuri, Preethi Jyothi, and Sunita
Sarawagi. Generalizing across domains via cross-gradient training. In ICLR, 2018."
REFERENCES,0.6036745406824147,"Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In
European conference on computer vision, pp. 443‚Äì450. Springer, 2016."
REFERENCES,0.6062992125984252,"Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep
hashing network for unsupervised domain adaptation. In CVPR, 2017."
REFERENCES,0.6089238845144357,"Xiaobo Wang, Shuo Wang, Cheng Chi, Shifeng Zhang, and Tao Mei. Loss function search for face
recognition. In ICML, 2020."
REFERENCES,0.6115485564304461,"Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey. Symmetric cross
entropy for robust learning with noisy labels. In CVPR, 2019."
REFERENCES,0.6141732283464567,"Lijun Wu, Fei Tian, Yingce Xia, Yang Fan, Tao Qin, Lai Jian-Huang, and Tie-Yan Liu. Learning to
teach with dynamic loss functions. In NeurIPS, 2018."
REFERENCES,0.6167979002624672,"Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn.
Gradient surgery for multi-task learning. In NeurIPS, 2020."
REFERENCES,0.6194225721784777,"Ying Zhang, Tao Xiang, Timothy M Hospedales, and Huchuan Lu. Deep mutual learning. In CVPR,
2018."
REFERENCES,0.6220472440944882,"Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks
with noisy labels. In NeurIPS, 2018."
REFERENCES,0.6246719160104987,"Shanshan Zhao, Mingming Gong, Tongliang Liu, Huan Fu, and Dacheng Tao. Domain generalization
via entropy regularization. NeurIPS, 2020."
REFERENCES,0.6272965879265092,Under review as a conference paper at ICLR 2022
REFERENCES,0.6299212598425197,"Kaiyang Zhou, Yongxin Yang, Timothy Hospedales, and Tao Xiang. Learning to generate novel
domains for domain generalization. In ECCV, 2020."
REFERENCES,0.6325459317585301,"Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization: A
survey, 2021a."
REFERENCES,0.6351706036745407,"Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Domain generalization with mixstyle. In
International Conference on Learning Representations, 2021b."
REFERENCES,0.6377952755905512,Under review as a conference paper at ICLR 2022
REFERENCES,0.6404199475065617,"A
APPENDIX"
REFERENCES,0.6430446194225722,"A.1
THE DETAILED RESULTS FOR DOMAINBED"
REFERENCES,0.6456692913385826,"In Table 2 of the main paper, we summarized performance across held out target domains for the
standard multi-source DG problem. In Table 6 we now give the detailed results of ITL-Net for each
target domain."
REFERENCES,0.6482939632545932,"Table 6: DomainBed Cross-domain recognition accuracy (%) with ResNet50 on ColoredMNIST
VLCS, PACS, TerraIncognita, OfÔ¨ÅceHome and DomainNet."
REFERENCES,0.6509186351706037,ColoredMNIST
REFERENCES,0.6535433070866141,"Target set
+90%
+80%
-90%
Avg."
REFERENCES,0.6561679790026247,"ERM
71.7 ¬± 0.1
72.9 ¬± 0.2
10.0 ¬± 0.1
51.5
SagNet
71.8 ¬± 0.2
73.0 ¬± 0.2
10.3 ¬± 0.0
51.7
CORAL
71.6 ¬± 0.3
73.1 ¬± 0.1
9.9 ¬± 0.1
51.5
CDANN
72.0 ¬± 0.3
73.0 ¬± 0.2
10.2 ¬± 0.1
51.7
RSC
71.9 ¬± 0.3
72.9 ¬± 0.4
10.2 ¬± 0.2
51.8
ITL-Net
71.3 ¬± 0.6
73.4 ¬± 0.1
11.3 ¬± 0.7
52.0 VLCS"
REFERENCES,0.6587926509186351,"Target set
Caltech
Labelme
Sun
V-Pascal
Avg."
REFERENCES,0.6614173228346457,"ERM
97.7 ¬± 0.4
64.3 ¬± 0.9
73.4 ¬± 0.5
77.3 ¬± 1.3
77.5
SagNet
97.9 ¬± 0.4
64.5 ¬± 0.5
71.4 ¬± 1.3
77.5 ¬± 0.5
77.8
CORAL
98.3 ¬± 0.1
66.1 ¬± 1.2
73.4 ¬± 0.3
77.5 ¬± 1.2
78.8
CDANN
97.3 ¬± 0.3
65.1 ¬± 1.2
70.7 ¬± 0.8
77.1 ¬± 1.5
77.5
RSC
97.9 ¬± 0.1
62.5 ¬± 0.7
72.3 ¬± 1.2
75.6 ¬± 0.8
77.1
ITL-Net
98.3 ¬± 0.4
65.4 ¬± 0.7
75.1 ¬± 0.6
76.8 ¬± 1.2
78.9 PACS"
REFERENCES,0.6640419947506562,"Target set
Art
Cartoon
Photo
Sketch
Avg."
REFERENCES,0.6666666666666666,"ERM
84.7 ¬± 0.4
80.8 ¬± 0.6
97.2 ¬± 0.3
79.3 ¬± 1.0
85.5
SagNet
87.4 ¬± 1.0
80.7 ¬± 0.6
97.1 ¬± 0.1
80.0 ¬± 0.4
86.3
CORAL
88.3 ¬± 0.2
80.0 ¬± 0.5
97.5 ¬± 0.3
78.8 ¬± 1.3
86.2
CDANN
84.6 ¬± 1.8
75.5 ¬± 0.9
96.8 ¬± 0.3
73.5 ¬± 0.6
82.6
RSC
85.4 ¬± 0.8
79.1 ¬± 0.6
96.9 ¬± 0.5
77.7 ¬± 1.7
84.9
ITL-Net
87.1 ¬± 0.4
83.3 ¬± 0.6
96.1 ¬± 0.4
79.3 ¬± 0.6
86.4"
REFERENCES,0.6692913385826772,TerraIncognita
REFERENCES,0.6719160104986877,"Target set
L100
L38
L43
L46
Avg."
REFERENCES,0.6745406824146981,"ERM
49.8 ¬± 4.4
42.1 ¬± 1.4
56.9 ¬± 1.8
35.7 ¬± 3.9
46.1
SagNet
53.0 ¬± 2.9
43.0 ¬± 2.5
57.9 ¬± 0.6
40.4 ¬± 1.3
48.6
CORAL
51.6 ¬± 2.4
42.2 ¬± 1.0
57.0 ¬± 1.0
39.8 ¬± 2.9
47.6
CDANN
47.0 ¬± 1.9
41.3 ¬± 4.8
54.9 ¬± 1.7
39.8 ¬± 0.8
45.8
RSC
50.2 ¬± 2.2
39.2 ¬± 1.4
56.3 ¬± 1.4
40.8 ¬± 0.6
46.6
ITL-Net
58.4 ¬± 3.7
46.2 ¬± 1.8
58.5 ¬± 0.9
40.9 ¬± 1.8
51.0"
REFERENCES,0.6771653543307087,OfÔ¨ÅceHome
REFERENCES,0.6797900262467191,"Target set
Artistic
Clipart
Product
Real World
Avg."
REFERENCES,0.6824146981627297,"ERM
61.3 ¬± 0.7
52.4 ¬± 0.3
75.8 ¬± 0.1
76.6 ¬± 0.3
66.5
SagNet
63.4 ¬± 0.2
54.8 ¬± 0.4
75.8 ¬± 0.4
78.3 ¬± 0.3
68.1
CORAL
65.3 ¬± 0.4
54.4 ¬± 0.5
76.5 ¬± 0.1
78.4 ¬± 0.5
68.7
CDANN
61.0 ¬± 1.4
50.4 ¬± 2.4
74.4 ¬± 0.9
76.6 ¬± 0.8
65.8
RSC
60.7 ¬± 1.4
51.4 ¬± 0.3
74.8 ¬± 1.1
75.1 ¬± 1.3
65.5
ITL-Net
65.6 ¬± 0.4
55.6 ¬± 0.4
77.5 ¬± 0.3
78.6 ¬± 0.4
69.3"
REFERENCES,0.6850393700787402,DomainNet
REFERENCES,0.6876640419947506,"Target set
Clipart
Infograph
Painting
Quickdraw
Real
Sketch
Avg."
REFERENCES,0.6902887139107612,"ERM
58.1 ¬± 0.3
18.8 ¬± 0.3
46.7 ¬± 0.3
12.2 ¬± 0.4
59.6 ¬± 0.1
49.8 ¬± 0.4
40.9
SagNet
57.7 ¬± 0.3
19.0 ¬± 0.2
45.3 ¬± 0.3
12.7 ¬± 0.5
58.1 ¬± 0.5
48.8 ¬± 0.2
40.3
CORAL
59.2 ¬± 0.1
19.7 ¬± 0.2
46.6 ¬± 0.3
13.4 ¬± 0.4
59.8 ¬± 0.2
50.1 ¬± 0.6
41.5
CDANN
54.6 ¬± 0.4
17.3 ¬± 0.1
43.7 ¬± 0.9
12.1 ¬± 0.7
56.2 ¬± 0.4
45.9 ¬± 0.5
38.3
RSC
55.0 ¬± 1.2
18.3 ¬± 0.5
44.4 ¬± 0.6
12.2 ¬± 0.2
55.7 ¬± 0.7
47.8 ¬± 0.9
38.9
ITL-Net
63.5 ¬± 0.3
19.4 ¬± 0.1
46.3 ¬± 0.1
13.7 ¬± 0.4
53.2 ¬± 0.6
53.5 ¬± 0.3
41.6"
REFERENCES,0.6929133858267716,"Competitors
ERM
SagNet
CORAL
RSC
CDANN
H0 (p-value)
Reject (0.016)
Reject (0.016)
Reject (0.016)
Reject (0.016)
Reject (0.016)"
REFERENCES,0.6955380577427821,Under review as a conference paper at ICLR 2022
REFERENCES,0.6981627296587927,"A.2
DETAILED RESULTS FOR SINGLE SOURCE DOMAIN EXPERIMENT"
REFERENCES,0.7007874015748031,"In Table 4 of the main paper, we reported single-source DG results, summarising performance across
choice of source domain and over all target domains. In Table 7 we now give the detailed results of
ITL-Net for each choice of source and target domain."
REFERENCES,0.7034120734908137,"Table 7: DomainBed Single source domain recognition accuracy (%) with ResNet50 on VLCS,
PACS, TerraIncognita and OfÔ¨ÅceHome. Each cell reports the accuracy for a set of target domains,
and the source domain used for training corresponding to the column. The performance of target
domains is separated by ‚Äò/‚Äò. Average over target domains for a given source domain is given at the
bottom of the cell. VLCS"
REFERENCES,0.7060367454068242,"Source
set
Caltech
Labelme
Sun
V-Pascal
Avg."
REFERENCES,0.7086614173228346,"ERM
47.81/55.58/59.72
70.00/57.61/65.90
52.93/62.27/60.30
96.75/63.29/76.81
54.37
64.5
58.5
78.95
64.08"
REFERENCES,0.7112860892388452,"CORAL
47.81/55.58/59.72
70.00/57.61/65.90
52.93/62.27/60.30
96.75/63.29/76.81
54.37
64.5
58.5
78.95
64.08"
REFERENCES,0.7139107611548556,"SagNet
48.76/53.14/56.93
35.69/53.53/63.33
67.28/62.05/66.70
96.96/62.88/75.20
52.94
50.85
65.34
78.35
61.87"
REFERENCES,0.7165354330708661,"ITL-Net
43.19/39.85/51.01
89.82/55.18/60.63
48.90/61.78/60.01
97.31/60.84/77.51
44.68
68.54
56.9
78.55
62.17 PACS"
REFERENCES,0.7191601049868767,"Source
set
Art
Cartoon
Photo
Sketch
Avg."
REFERENCES,0.7217847769028871,"ERM
65.36/96.23/45.41
70.17/86.17/66.02
68.07/20.05/16.62
24.76/36.05/27.25
69.0
74.12
34.91
29.35
51.85"
REFERENCES,0.7244094488188977,"CORAL
65.36/96.21/45.41
70.20/86.15/66.01
68.03/20.04/16.62
24.70/36.05/27.24
68.99
74.12
34.9
29.33
51.84"
REFERENCES,0.7270341207349081,"SagNet
66.60/93.41/55.61
61.42/79.76/64.93
69.04/30.38/25.88
26.17/36.86/25.93
71.87
68.7
41.77
29.65
53.0"
REFERENCES,0.7296587926509186,"ITL-Net
66.30/94.67/57.29
74.85/86.52/75.06
62.60/45.82/51.44
17.87/26.45/19.64
72.75
78.81
53.29
21.32
56.54"
REFERENCES,0.7322834645669292,TerraIncognita
REFERENCES,0.7349081364829396,"Source
set
L100
L38
L43
L46
Avg."
REFERENCES,0.7375328083989501,"ERM
43.82/60.93/69.15
39.97/51.27/54.40
40.96/39.11/64.70
58.26/46.53/73.73
57.97
48.55
48.26
59.51
53.57"
REFERENCES,0.7401574803149606,"CORAL
43.80/60.90/69.17
40.00/51.12/54.20
40.87/40.12/64.23
58.70/45.43/73.62
57.96
48.44
48.41
59.25
53.51"
REFERENCES,0.7427821522309711,"SagNet
40.71/56.95/68.19
37.95/50.44/53.71
33.99/34.41/59.01
59.54/45.93/74.72
55.28
47.37
42.47
60.06
51.3"
REFERENCES,0.7454068241469817,"ITL-Net
44.79/55.71/67.62
44.66/53.62/58.07
40.97/39.29/66.35
61.68/51.36/76.41
56.04
52.12
48.87
63.15
55.04"
REFERENCES,0.7480314960629921,OfÔ¨ÅceHome
REFERENCES,0.7506561679790026,"Source
set
Artistic
Clipart
Product
Real World
Avg."
REFERENCES,0.7532808398950132,"ERM
44.75/23.36/21.09
39.50/18.66/20.11
37.45/26.00/39.55
27.67/31.44/56.00
29.73
26.09
34.33
38.37
32.13"
REFERENCES,0.7559055118110236,"CORAL
44.75/23.36/21.09
39.49/18.67/20.11
37.45/26.01/39.55
27.77/31.46/55.98
29.73
26.09
34.34
38.4
32.14"
REFERENCES,0.7585301837270341,"SagNet
47.98/22.33/17.85
46.40/23.98/14.89
34.84/34.34/41.62
33.30/35.41/54.25
29.39
28.42
36.93
40.99
33.93"
REFERENCES,0.7611548556430446,"ITL-Net
31.63/21.57/21.53
54.72/21.79/30.99
37.38/38.75/36.69
33.71/35.64/56.64
24.91
35.83
37.61
42.0
35.09"
REFERENCES,0.7637795275590551,Under review as a conference paper at ICLR 2022
REFERENCES,0.7664041994750657,"A.3
THE LEARNED LOSS FUNCTION"
REFERENCES,0.7690288713910761,"Recall that our experimental design trained a single loss function on RotatedMNIST, and then
evaluated it from different perspectives across all our main experiments. The speciÔ¨Åc parameters of
our learned ITL (as visualised in Fig. 5(right)) are given in Table A.3. Then reader can plug-and-play
for their own Domain Generalisation problems."
REFERENCES,0.7716535433070866,Table 8: The parameters of the learned ITL
REFERENCES,0.7742782152230971,"Loss type
parameters
œâ0, œâ2, ..., œâ11
ITL
-2.0193, -1.2234, 0.1363, 0.1269, -0.4566, -0.1016, -0.2545, 1.0971, -0.9203, 0.2368, 0.4795, 0.9975"
REFERENCES,0.7769028871391076,"A.4
THE PARAMETERISATION OF THE LEARNABLE LOSS FUNCTION"
REFERENCES,0.7795275590551181,"We apply fourth order bi-variate Taylor polynomial to parameterise the learnable loss function. The
terms only contain yi are removed from the polynomial since these do not generate gradients with
respect to the prediction of the network. The Ô¨Ånal form, only containing 12 learnable parameters, is
given as:"
REFERENCES,0.7821522309711286,"L(i)
œâ (ÀÜyi, yi) = œâ2(ÀÜyi ‚àíœâ0) + œâ3(ÀÜyi ‚àíœâ0)2 + œâ4(ÀÜyi ‚àíœâ0)3 + œâ5(ÀÜyi ‚àíœâ0)4"
REFERENCES,0.7847769028871391,+ œâ6(ÀÜyi ‚àíœâ0)(yi ‚àíœâ1) + œâ7(ÀÜyi ‚àíœâ0)(yi ‚àíœâ1)2 + œâ8(ÀÜyi ‚àíœâ0)2(yi ‚àíœâ1)
REFERENCES,0.7874015748031497,+ œâ9(ÀÜyi ‚àíœâ0)3(yi ‚àíœâ1) + œâ10(ÀÜyi ‚àíœâ0)(yi ‚àíœâ1)3 + œâ11(ÀÜyi ‚àíœâ0)2(yi ‚àíœâ1)2.
REFERENCES,0.7900262467191601,"A.5
ROBUST LOSS FUNCITON ILLUSTRATION"
REFERENCES,0.7926509186351706,"0.0
0.2
0.4
0.6
0.8
1.0
predicted ground truth category probability 0 2 4 6 8"
REFERENCES,0.7952755905511811,loss value
REFERENCES,0.7979002624671916,"Given label = true
Given label = false"
REFERENCES,0.800524934383202,"0.0
0.2
0.4
0.6
0.8
1.0
predicted ground truth category probability 0 5 10 15 20 25"
REFERENCES,0.8031496062992126,loss value
REFERENCES,0.8057742782152231,"Given label = true
Given label = false"
REFERENCES,0.8083989501312336,"0.0
0.2
0.4
0.6
0.8
1.0
predicted ground truth category probability 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.8110236220472441,loss value
REFERENCES,0.8136482939632546,"Given label = true
Given label = false"
REFERENCES,0.8162729658792651,"0.0
0.2
0.4
0.6
0.8
1.0
predicted ground truth category probability 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.8188976377952756,loss value
REFERENCES,0.821522309711286,"Given label = true
 Given label = false"
REFERENCES,0.8241469816272966,"Figure 5: Comparison of loss functions (from left to right): CE, SCE, FOCAL and ITL. The range of
ITL is normalised between 0 and 1."
REFERENCES,0.8267716535433071,"A.6
META-TRAIN COMPUTE COST"
REFERENCES,0.8293963254593176,"Due to the efÔ¨Åciency of implicit gradient, training our ITL required using PyTorch (Paszke et al.,
2017) only required 8 hours on a single V100 GPU to complete 200 gradient descent steps on œâ.
While the goals and base learning problems are not directly comparable, this is dramatically faster
than alternatives that require an entire cluster (Li et al., 2019a), and where even very recent fast
methods require about 12 GPU-days (Liu et al., 2021)."
REFERENCES,0.8320209973753281,"A.7
META-TRAINING CONVERGENCE"
REFERENCES,0.8346456692913385,"Figure 6 shows the convergence of ITL during meta-training on R-MNIST. The x-axis shows outer
loop iterations/loss function updates. The lines graph (i) the inner loop accuracy (mean over all source
domains) after model training with the current loss function, and (ii) and the outer loop accuracy
(held out domain accuracy). The convergence process is quite smooth."
REFERENCES,0.8372703412073491,"A.8
FULL LOSS LANDSCAPE PLOTS"
REFERENCES,0.8398950131233596,"Figure 7 shows the landscape for all four domains, of which two were shown in Figure 4 of the main
manuscript."
REFERENCES,0.84251968503937,Under review as a conference paper at ICLR 2022
REFERENCES,0.8451443569553806,"0
25
50
75
100
125
150
175
200
Outer loop Iteration 0.0 0.2 0.4 0.6 0.8"
REFERENCES,0.847769028871391,Accuracy
REFERENCES,0.8503937007874016,Learning Curve (meta-train)
REFERENCES,0.8530183727034121,"Inner loop 
(mean over all the source domain)
Outer loop 
(holding out domain)"
REFERENCES,0.8556430446194225,Figure 6: The learning curve for ITL meta-training stage.
REFERENCES,0.8582677165354331,"0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5"
REFERENCES,0.8608923884514436,ERM Loss
REFERENCES,0.863517060367454,"-0.8
-0.4
0.0
0.4
0.8
Perturbation"
REFERENCES,0.8661417322834646,"0.0
0.2
0.4
0.6
0.8
1.0
1.2"
REFERENCES,0.868766404199475,ITL Loss
REFERENCES,0.8713910761154856,Held-out domain: Art
REFERENCES,0.8740157480314961,"ITL-NET
ERM"
REFERENCES,0.8766404199475065,"0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5"
REFERENCES,0.8792650918635171,ERM Loss
REFERENCES,0.8818897637795275,"-0.8 -0.4 0.0
0.4
0.8
Perturbation -10.0 -8.0 -6.0 -4.0 -2.0"
REFERENCES,0.884514435695538,ITL Loss
REFERENCES,0.8871391076115486,Held-out domain: Clipart
REFERENCES,0.889763779527559,"ITL-NET
ERM 0.0 2.0 4.0 6.0 8.0 10.0"
REFERENCES,0.8923884514435696,ERM Loss
REFERENCES,0.89501312335958,"-0.8 -0.4 0.0
0.4
0.8
Perturbation -10.0 -9.0 -8.0 -7.0"
REFERENCES,0.8976377952755905,ITL Loss
REFERENCES,0.9002624671916011,Held-out domain: Product
REFERENCES,0.9028871391076115,"ITL-NET
ERM"
REFERENCES,0.905511811023622,"0.0
5.0
10.0
15.0
20.0
25.0"
REFERENCES,0.9081364829396326,ERM Loss
REFERENCES,0.910761154855643,"-0.8 -0.4 0.0
0.4
0.8
Perturbation -10.0"
REFERENCES,0.9133858267716536,"-8.0
-6.0
-4.0
-2.0"
REFERENCES,0.916010498687664,"0.0
2.0"
REFERENCES,0.9186351706036745,ITL Loss
REFERENCES,0.9212598425196851,Held-out domain: Real-World
REFERENCES,0.9238845144356955,"ITL-NET
ERM"
REFERENCES,0.926509186351706,"0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5"
REFERENCES,0.9291338582677166,ERM Loss
REFERENCES,0.931758530183727,"-0.8 -0.4
0.0
0.4
0.8
Perturbation"
REFERENCES,0.9343832020997376,"0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5"
REFERENCES,0.937007874015748,ITL Loss
REFERENCES,0.9396325459317585,Held-out domain: Art
REFERENCES,0.9422572178477691,"ITL-NET
ERM"
REFERENCES,0.9448818897637795,"2.0
4.0
6.0
8.0
10.0
12.0
14.0
16.0"
REFERENCES,0.94750656167979,ERM Loss
REFERENCES,0.9501312335958005,"-0.8 -0.4 0.0
0.4
0.8
Perturbation -10.5 -10.0 -9.5 -9.0 -8.5"
REFERENCES,0.952755905511811,ITL Loss
REFERENCES,0.9553805774278216,Held-out domain: Clipart
REFERENCES,0.958005249343832,"ITL-NET
ERM"
REFERENCES,0.9606299212598425,"0.0
2.0
4.0
6.0
8.0
10.0
12.0
14.0
16.0"
REFERENCES,0.963254593175853,ERM Loss
REFERENCES,0.9658792650918635,"-0.8
-0.4
0.0
0.4
0.8
Perturbation"
REFERENCES,0.968503937007874,"0.0
0.2
0.4
0.6
0.8
1.0"
REFERENCES,0.9711286089238845,ITL Loss
REFERENCES,0.973753280839895,Held-out domain: Product
REFERENCES,0.9763779527559056,"ITL-NET
ERM"
REFERENCES,0.979002624671916,"0.0
5.0
10.0
15.0
20.0
25.0
30.0
35.0"
REFERENCES,0.9816272965879265,ERM Loss
REFERENCES,0.984251968503937,"-0.8 -0.4
0.0
0.4
0.8
Perturbation -1.0"
REFERENCES,0.9868766404199475,"0.0
1.0
2.0
3.0
4.0"
REFERENCES,0.989501312335958,ITL Loss
REFERENCES,0.9921259842519685,Held-out domain: Real-World
REFERENCES,0.994750656167979,"ITL-NET
ERM"
REFERENCES,0.9973753280839895,"Figure 7: 1D Loss Landscape: ITL-Net vs ERM on OfÔ¨ÅceHome. Top row: Source domain loss
landscape. Bottom row: Target domain loss landscape."
