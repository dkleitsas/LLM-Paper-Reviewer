Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0020242914979757085,"Deep neural networks are vulnerable to adversarial noise. Adversarial training (AT)
1"
ABSTRACT,0.004048582995951417,"has been demonstrated to be the most effective defense strategy to protect neural
2"
ABSTRACT,0.006072874493927126,"networks from being fooled. However, we find AT omits to learning robust features,
3"
ABSTRACT,0.008097165991902834,"resulting in poor performance of adversarial robustness. To address this issue, we
4"
ABSTRACT,0.010121457489878543,"highlight two characteristics of robust representation: (1) exclusion: the feature of
5"
ABSTRACT,0.012145748987854251,"natural examples keeps away from that of other classes; (2) alignment: the feature
6"
ABSTRACT,0.01417004048582996,"of natural and corresponding adversarial examples is close to each other. These
7"
ABSTRACT,0.016194331983805668,"motivate us to propose a generic framework of AT to gain robust representation,
8"
ABSTRACT,0.018218623481781375,"by the asymmetric negative contrast and reverse attention. Specifically, we design
9"
ABSTRACT,0.020242914979757085,"an asymmetric negative contrast based on predicted probabilities and generate
10"
ABSTRACT,0.022267206477732792,"adversarial negative examples by the targeted attack, to push away examples of
11"
ABSTRACT,0.024291497975708502,"different classes in the feature space. Moreover, we propose to weight feature by
12"
ABSTRACT,0.02631578947368421,"parameters of the linear classifier as the reverse attention, to obtain class-aware
13"
ABSTRACT,0.02834008097165992,"feature and pull close the feature of the same class. Empirical evaluations on three
14"
ABSTRACT,0.030364372469635626,"benchmark datasets show our methods greatly advance the robustness of AT and
15"
ABSTRACT,0.032388663967611336,"achieve the state-of-the-art performance.
16"
INTRODUCTION,0.03441295546558704,"1
Introduction
17"
INTRODUCTION,0.03643724696356275,"Deep neural networks (DNNs) have achieved great success in academia and industry, but they
18"
INTRODUCTION,0.038461538461538464,"are easily fooled by carefully crafted adversarial examples to output incorrect results [13], which
19"
INTRODUCTION,0.04048582995951417,"leads to potential threats and insecurity in application. Given a well-trained DNN and a natural
20"
INTRODUCTION,0.04251012145748988,"example, an adversarial example can be generated by adding small perturbation that is invisible to
21"
INTRODUCTION,0.044534412955465584,"the human eyes to the natural example. The natural example can be correctly classified before the
22"
INTRODUCTION,0.0465587044534413,"perturbation and the adversarial example is incorrectly classified after the perturbation. In recent
23"
INTRODUCTION,0.048582995951417005,"years, there are many researches exploring the generation of adversarial examples to cheat models in
24"
INTRODUCTION,0.05060728744939271,"various fields, including image classification [13, 26, 5, 9], object detection [33, 8], natural language
25"
INTRODUCTION,0.05263157894736842,"processing [27, 2], semantic segmentation [28, 25], etc. The vulnerability of DNNs has aroused
26"
INTRODUCTION,0.05465587044534413,"common concerns on adversarial robustness.
27"
INTRODUCTION,0.05668016194331984,"Many empirical defense methods have been proposed to protect DNNs from adversarial perturbation,
28"
INTRODUCTION,0.058704453441295545,"such as adversarial training (AT) [26, 36, 30, 18, 39, 37, 31], image denoising [24], defensive
29"
INTRODUCTION,0.06072874493927125,"distillation [38, 6] and so on. The mainstream view is that AT is the most effective defense, which has
30"
INTRODUCTION,0.06275303643724696,"a training process of a two-sided game. The ""attacker"" crafts perturbation dynamically to generate
31"
INTRODUCTION,0.06477732793522267,"adversarial data to cheat the ""defender"", and the ""defender"" minimizes the loss function against
32"
INTRODUCTION,0.06680161943319839,"adversarial samples to improve robustness of models. Existing work [38, 6, 37, 11, 18, 20, 39] has
33"
INTRODUCTION,0.06882591093117409,"improved the effectiveness of AT in many aspects, but few studies pay attention to learning robust
34"
INTRODUCTION,0.0708502024291498,"feature. The overlook may lead to potential threats in the feature space of AT models, which does
35"
INTRODUCTION,0.0728744939271255,"harm to robust classification. Besides, there are no criteria for robust feature. In addition, adversarial
36"
INTRODUCTION,0.07489878542510121,"(a)
(b)
(c)
(d)"
INTRODUCTION,0.07692307692307693,"Figure 1: Frequency histograms of the L2 distance and cosine similarity of the feature that belongs to
natural examples, AEs and OEs. The four figures show the cosine similarity of the feature between
natural examples and OEs (a), the L2 distance of the feature between natural examples and OEs (b),
the cosine similarity of the feature between natural examples and AEs (c), the L2 distance of the
feature between natural examples and AEs (d), respectively. The feature denotes the feature vector z
before the linear layer. We train ResNet-18 [15] models on CIFAR-10 [22] with three AT methods:
PDG-AT [26], TRADES [36] and MART [30]. In the calculation, we use all samples labeled as class
0 in the test set as natural examples and generate AEs by PGD-10 [26]."
INTRODUCTION,0.07894736842105263,"contrastive learning (ACL) and robust feature selection (RFS) are techniques to optimize feature
37"
INTRODUCTION,0.08097165991902834,"distribution. ACL [21, 12, 35] is a kind of contrast learning (CL) [7, 17, 14] that extends to AT. RFS
38"
INTRODUCTION,0.08299595141700405,"mostly modifies the architecture of models [32, 1, 34] to select important feature. However, the target
39"
INTRODUCTION,0.08502024291497975,"problems of them are not to learn robust feature.
40"
INTRODUCTION,0.08704453441295547,"To demonstrate AT is indeed deficient in the representation which causes limited adversarial robust-
41"
INTRODUCTION,0.08906882591093117,"ness, we conduct a simple experiment. We choose the L2 distance and cosine similarity as metrics.
42"
INTRODUCTION,0.09109311740890688,"And we measure the distance and similarity of the feature between natural examples, adversarial
43"
INTRODUCTION,0.0931174089068826,"examples (AEs) and examples of other classes (OEs). The frequency histograms of the distance and
44"
INTRODUCTION,0.0951417004048583,"similarity is shown in Figure 1. Figure 1 (a) and Figure 1 (b) show that the cosine similarity of the
45"
INTRODUCTION,0.09716599190283401,"feature between natural examples and OEs shows a Gaussian distribution between 0.4 and 0.8, and
46"
INTRODUCTION,0.09919028340080972,"the L2 distance shows a skewed distribution between 2.0 and 12.0, which indicates there are very
47"
INTRODUCTION,0.10121457489878542,"close pairs of natural examples and OEs that are not distinguished in the feature space. In Figure 1
48"
INTRODUCTION,0.10323886639676114,"(c) and Figure 1 (d), it is shown that there are a skewed distribution between 0.9 and 0.99 for the
49"
INTRODUCTION,0.10526315789473684,"cosine similarity of the feature between natural examples and AEs, and a skewed distribution between
50"
INTRODUCTION,0.10728744939271255,"0.5 and 2.5 for the L2 distance, which indicates that the feature of natural examples and AEs is not
51"
INTRODUCTION,0.10931174089068826,"adequately aligned. Thus, there is still large room for optimization of the feature of AT.
52"
INTRODUCTION,0.11133603238866396,"Based on the observation, we propose two characteristics of robust feature: exclusion: the feature
53"
INTRODUCTION,0.11336032388663968,"of natural examples keeps away from that of other classes; alignment: the feature of natural and
54"
INTRODUCTION,0.11538461538461539,"corresponding adversarial samples is close to each other. First, exclusion confirms the separability
55"
INTRODUCTION,0.11740890688259109,"between different classes and avoids confusion in the feature space, which makes it hard to fool the
56"
INTRODUCTION,0.1194331983805668,"model because the feature of different classes keep a large distance. Second, alignment insures the
57"
INTRODUCTION,0.1214574898785425,"feature of natural examples is aligned with adversarial one, which guarantees the predicted results of
58"
INTRODUCTION,0.12348178137651822,"the natural and adversarial examples of the same instances are also highly consistent. And it helps to
59"
INTRODUCTION,0.12550607287449392,"narrow the gap between robust accuracy and clean accuracy.
60"
INTRODUCTION,0.12753036437246965,"To address the issue, we propose an AT framework to concentrate on robust representation with the
61"
INTRODUCTION,0.12955465587044535,"guidance of the two characteristics. Specifically, we suggest two strategies to meet the characteristics,
62"
INTRODUCTION,0.13157894736842105,"respectively. Treat a natural example and corresponding AE as a positive pair (PP), and treat a natural
63"
INTRODUCTION,0.13360323886639677,"example and corresponding OE as a negative pair (PP). For exclusion, we propose an asymmetric
64"
INTRODUCTION,0.13562753036437247,"negative contrast based on predicted probabilities, which freezes natural examples and pushes away
65"
INTRODUCTION,0.13765182186234817,"OEs by reducing the confidence of predicted class when predicted classes of NPs are consistent. In
66"
INTRODUCTION,0.1396761133603239,"particular, we find OEs generated by the targeted attack are more beneficial for correct classification
67"
INTRODUCTION,0.1417004048582996,"than those selected carefully. For alignment, we use the reverse attention to weight the feature of PPs
68"
INTRODUCTION,0.1437246963562753,"by partial parameters of the linear classifier, which contains the importance of feature to target classes
69"
INTRODUCTION,0.145748987854251,"during classification. Because the feature of the same class gets the same weighting and feature of
70"
INTRODUCTION,0.14777327935222673,"different classes is weighted disparately, PPs are aligned and each example of PPs becomes close
71"
INTRODUCTION,0.14979757085020243,"to each other in the feature space. Empirical evaluations show that AT methods combined with our
72"
INTRODUCTION,0.15182186234817813,"framework can greatly enhance robustness, which means the neglect of learning robust feature is
73"
INTRODUCTION,0.15384615384615385,"one of the main reasons for poor robust performance of AT. In a word, we propose a generic AT
74"
INTRODUCTION,0.15587044534412955,"framework with the Asymmetric Negative Contrast and Reverse Attention (ANCRA), to learn robust
75"
INTRODUCTION,0.15789473684210525,"representation and advance robustness. Our main contributions are summarized as follows:
76"
INTRODUCTION,0.15991902834008098,"• We suggest improving adversarial training from the perspective of learning robust feature,
77"
INTRODUCTION,0.16194331983805668,"and two characteristics are highlighted as criteria of optimizing robust representation.
78"
INTRODUCTION,0.16396761133603238,"• We propose a generic framework of adversarial training, termed as ANCRA, to obtain robust
79"
INTRODUCTION,0.1659919028340081,"feature by the asymmetric negative contrast and reverse attention, with the guidance of two
80"
INTRODUCTION,0.1680161943319838,"characteristics of robust feature. It can be easily combined with other defense methods.
81"
INTRODUCTION,0.1700404858299595,"• Empirical evaluations show our framework can obtain robust feature and greatly improve
82"
INTRODUCTION,0.1720647773279352,"adversarial robustness, which achieves the of state-of-the-art performances on CIFAR-10,
83"
INTRODUCTION,0.17408906882591094,"CIFAR-100 and Tiny-ImageNet.
84"
RELATED WORK,0.17611336032388664,"2
Related work
85"
RELATED WORK,0.17813765182186234,"Adversarial training
Madry et al. [26] propose PGD attack and PGD-based adversarial training,
86"
RELATED WORK,0.18016194331983806,"forcing the model to correctly classify adversarial samples within the epsilon sphere during training
87"
RELATED WORK,0.18218623481781376,"to obtain robustness, which is the pioneer of adversarial learning. Zhang et al. [36] propose to learn
88"
RELATED WORK,0.18421052631578946,"both natural and adversarial samples and reduce the divergence of classification distribution of both
89"
RELATED WORK,0.1862348178137652,"to reduce the difference between robust accuracy and natural accuracy. Wang et al. [30] find that
90"
RELATED WORK,0.1882591093117409,"misclassified samples during training have a negative impact on robustness significantly, and propose
91"
RELATED WORK,0.1902834008097166,"to improve the model’s attention to misclassification by adaptive weights. Zhang et al. [37] propose
92"
RELATED WORK,0.19230769230769232,"to replace fixed attack steps with attack steps that just cross the decision boundary, and improved the
93"
RELATED WORK,0.19433198380566802,"natural accuracy by appropriately reducing the number of attack iterations. Huang et al. [18] replace
94"
RELATED WORK,0.19635627530364372,"labels with soft labels predicted by the model and adaptively reduce the weight of misclassification
95"
RELATED WORK,0.19838056680161945,"loss to alleviate robust overfitting problem. Dong et al. [11] also propose a similar idea of softening
96"
RELATED WORK,0.20040485829959515,"label and explain the different effects of hard and soft labels on robustness by investigating the
97"
RELATED WORK,0.20242914979757085,"memory behavior of the model for random noisy labels. Chen et al. [6] propose random weight
98"
RELATED WORK,0.20445344129554655,"smoothing and self-training based on knowledge distillation, which greatly improve the natural and
99"
RELATED WORK,0.20647773279352227,"robust accuracy. Zhou et al. [39] embed a label transition matrix into models to infer natural labels
100"
RELATED WORK,0.20850202429149797,"from adversarial noise. However, little work has been done to improve AT from the perspective
101"
RELATED WORK,0.21052631578947367,"of robust feature learning. Our work shows AT indeed has defects in the feature distribution, and
102"
RELATED WORK,0.2125506072874494,"strategies proposed to learn robust feature can greatly advance robustness, which indicates the neglect
103"
RELATED WORK,0.2145748987854251,"of robust representation results in poor robust performance of AT.
104"
RELATED WORK,0.2165991902834008,"Adversarial contrastive learning
Kim et al. [21] propose an adversarial training method of
105"
RELATED WORK,0.21862348178137653,"maximizing and minimizing the contrastive loss. Fan et al. [12] notice that the robustness of ACL
106"
RELATED WORK,0.22064777327935223,"relies on fine-tuning, and pseudo labels and high-frequency information can advance robustness. Kucer
107"
RELATED WORK,0.22267206477732793,"et al. [23] find that the direct combination of self-supervised learning and AT penalizes non-robust
108"
RELATED WORK,0.22469635627530365,"accuracy. Bui et al. [3] propose some strategies to select positive and negative examples based on
109"
RELATED WORK,0.22672064777327935,"predicted classes and labels. Yu et al. [35] find the instance-level identity confusion problem brought
110"
RELATED WORK,0.22874493927125505,"by positive contrast and address it by asymmetric methods. The idea of these methods motivates us
111"
RELATED WORK,0.23076923076923078,"to further consider how to obtain robust feature by contrast mechanism. We design a new negative
112"
RELATED WORK,0.23279352226720648,"contrast to push away NPs and mitigate the confusion caused by negative contrast.
113"
RELATED WORK,0.23481781376518218,"Robust feature selection
Xiao et al. [32] take the maximum k feature values in each activation
114"
RELATED WORK,0.23684210526315788,"layer to increase adversarial robustness. Zoran et al. [40] use a spatial attention mechanism to identify
115"
RELATED WORK,0.2388663967611336,"important regions of the feature map. Bai et al. [1] propose to suppress redundant feature channels and
116"
RELATED WORK,0.2408906882591093,"dynamically activate feature channels with the parameters of additional components. Yan et al. [34]
117"
RELATED WORK,0.242914979757085,"propose to amplify the top-k activated feature channels. Existing work has shown enlarging import
118"
RELATED WORK,0.24493927125506074,"feature channels is beneficial for robustness, but most approaches rely on extra model components and
119"
RELATED WORK,0.24696356275303644,"do not explain the reason. We proposes the reverse attention to weight feature by class information
120"
RELATED WORK,0.24898785425101214,"without any extra components, and explain it by alignment of feature.
121"
METHODOLOGY,0.25101214574898784,"3
Methodology
122"
METHODOLOGY,0.25303643724696356,"This section explains the instantiation of the our AT framework from the perspective of the two
123"
METHODOLOGY,0.2550607287449393,"characteristics of robust feature. To meet exclusion, we design an asymmetric negative contrast based
124"
METHODOLOGY,0.25708502024291496,"on predicted probabilities and propose to craft OEs by the targeted attack, to push away the feature of
125"
METHODOLOGY,0.2591093117408907,"NPs. To confirm alignment, we propose the reverse attention to weight the feature of the same class,
126"
METHODOLOGY,0.2611336032388664,"by the corresponding weight of target class in parameters of the linear classifier, so that the feature of
127"
METHODOLOGY,0.2631578947368421,"PPs is aligned and the gap of the feature between natural examples and AEs becomes small.
128"
NOTATIONS,0.2651821862348178,"3.1
Notations
129"
NOTATIONS,0.26720647773279355,"In this paper, capital letters indicate random variables or vectors, while lowercase letters represent
130"
NOTATIONS,0.2692307692307692,"their realisations. We define the function for classification as f(·). It can be parameterized by
131"
NOTATIONS,0.27125506072874495,"DNNs. Linear(·) is the linear classifier with a weight of Ω(C, R), in which C denotes the class
132"
NOTATIONS,0.2732793522267207,"number and R denotes the channel number of the feature map. g(·) is the feature extractor, i.e.,
133"
NOTATIONS,0.27530364372469635,"the rest model without Linear(·). Let B = {xi, yi}N
i
be a batch of natural samples where xi
134"
NOTATIONS,0.2773279352226721,"is labeled by yi. Given an adversarial transformation Ta from an adversary A (e.g., PGD attack
135"
NOTATIONS,0.2793522267206478,"in [26]), and a strategy To for selection or generation of OEs. For data, we consider a positive pair
136"
NOTATIONS,0.2813765182186235,"PP={xi, xa
i |xi ∈B, xa
i = Ta(xi)}N
i , and a negative pair NP={xi, xo
i |xi ∈B, xo
i = To(xi)}N
i . Let
137"
NOTATIONS,0.2834008097165992,"N(x, ϵ) represent the neighborhood of x : {˜x : ∥˜x −x∥≤ϵ}, where ϵ is the perturbation budget. For
138"
NOTATIONS,0.2854251012145749,"an input xi, we consider its feature zi before Linear(·), the probability vector pi = softmax(f(xi))
139"
NOTATIONS,0.2874493927125506,"and predicted class hi = argmax(pi), respectively.
140"
ADVERSARIAL TRAINING WITH ASYMMETRIC NEGATIVE CONTRAST,0.2894736842105263,"3.2
Adversarial training with asymmetric negative contrast
141"
ADVERSARIAL TRAINING WITH ASYMMETRIC NEGATIVE CONTRAST,0.291497975708502,"Firstly, we promote AT to learn robust representation that meets exclusion. We notice that ACL has
142"
ADVERSARIAL TRAINING WITH ASYMMETRIC NEGATIVE CONTRAST,0.2935222672064777,"the contrastive loss [29] to maximize the consistency between PPs and to minimize the consistency
143"
ADVERSARIAL TRAINING WITH ASYMMETRIC NEGATIVE CONTRAST,0.29554655870445345,"between NPs. Motivated by the contrast mechanism, we consider to design a new negative-contrast
144"
ADVERSARIAL TRAINING WITH ASYMMETRIC NEGATIVE CONTRAST,0.2975708502024291,"term and combine it with AT loss, which creates a repulsive action between NPs when minimize the
145"
ADVERSARIAL TRAINING WITH ASYMMETRIC NEGATIVE CONTRAST,0.29959514170040485,"whole loss. Thus, we propose a generic pattern of AT loss with a negative contrast. Let TRADES
146"
ADVERSARIAL TRAINING WITH ASYMMETRIC NEGATIVE CONTRAST,0.3016194331983806,"[36] represent AT in the following paper as a example.
147"
ADVERSARIAL TRAINING WITH ASYMMETRIC NEGATIVE CONTRAST,0.30364372469635625,"LCAL(x, y, xa, xo) = LTRADES + Sim (x, xo) = LCE(x, y) + DKL(x, xa) + Sim (x, xo) , (1)"
ADVERSARIAL TRAINING WITH ASYMMETRIC NEGATIVE CONTRAST,0.305668016194332,"Where x denotes natural examples with labels y, xa are AEs generated by untargeted PGD [26],
148"
ADVERSARIAL TRAINING WITH ASYMMETRIC NEGATIVE CONTRAST,0.3076923076923077,"xo are negative examples of other classes (OEs), Sim is a similarity function, LCE denotes the
149"
ADVERSARIAL TRAINING WITH ASYMMETRIC NEGATIVE CONTRAST,0.3097165991902834,"cross-entropy loss and DKL denotes divergence of Kullback-Leibler. AEs generated by maximizing
150"
ADVERSARIAL TRAINING WITH ASYMMETRIC NEGATIVE CONTRAST,0.3117408906882591,"LCE typically have wrong predicted classes, given by:
151"
ADVERSARIAL TRAINING WITH ASYMMETRIC NEGATIVE CONTRAST,0.31376518218623484,"xa
t+1 :=
Π
N(x,ϵ) (xa
t + ϵ sign (∇xLCE ((f(xa
t ) , y))) ,
(2)"
ADVERSARIAL TRAINING WITH ASYMMETRIC NEGATIVE CONTRAST,0.3157894736842105,"where ϵ denotes the L∞-norm of perturbation, xa
t denotes adversarial positive samples after the tth
152"
ADVERSARIAL TRAINING WITH ASYMMETRIC NEGATIVE CONTRAST,0.31781376518218624,"attack iteration, Π denotes a clamp function, sign denotes a sign function and ∇xLCE denotes the
153"
ADVERSARIAL TRAINING WITH ASYMMETRIC NEGATIVE CONTRAST,0.31983805668016196,"gradient of LCE with respect to x. When minimizing the loss in Eq 1, LTRADES learns to classify
154"
ADVERSARIAL TRAINING WITH ASYMMETRIC NEGATIVE CONTRAST,0.32186234817813764,"natural examples and AEs correctly, and additional negative contrast prompts the inconsistency of
155"
ADVERSARIAL TRAINING WITH ASYMMETRIC NEGATIVE CONTRAST,0.32388663967611336,"NPs, which keeps the feature of NPs far away from each other. The whole loss guides the model to
156"
ADVERSARIAL TRAINING WITH ASYMMETRIC NEGATIVE CONTRAST,0.3259109311740891,"learn correct classification from TRADES and push away NPs from each other to ensure exclusion.
157"
ADVERSARIAL TRAINING WITH ASYMMETRIC NEGATIVE CONTRAST,0.32793522267206476,"Although we have a generic pattern of AT loss with a negative contrast, there are several problems
158"
ADVERSARIAL TRAINING WITH ASYMMETRIC NEGATIVE CONTRAST,0.3299595141700405,"about details to address. To refine the negative contrast and address problems, we further propose a
159"
ADVERSARIAL TRAINING WITH ASYMMETRIC NEGATIVE CONTRAST,0.3319838056680162,"method to calculate the negative contrast and strategy to generate OEs.
160"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.3340080971659919,"3.2.1
Asymmetric negative contrast based on probabilities
161"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.3360323886639676,"The work in [35] has indicated that when the predicted classes of the adversarial positive examples
162"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.33805668016194335,"(i.e., AEs) and negative samples (i.e., OEs) are the same, the positive contrast may lead to a conflict
163"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.340080971659919,"between the positive and negative contrast, resulting in wrong classification. On the basis, we find
164"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.34210526315789475,"a similar conflict can also be caused by the negative contrast when the predicted classes of AEs
165"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.3441295546558704,"and OEs are different, which we named by class confusion. As shown in Figure 2, when AEs and
166"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.34615384615384615,"OEs have different predicted classes, natural examples are subject to the attraction of AEs and the
167"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.3481781376518219,"repulsion of OEs at the same time. And it is likely to move near the decision boundary or even into
168"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.35020242914979755,"the wrong class space under the actions, which does harm to exclusion.
169"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.3522267206477733,"In order to alleviate the problem of class confusion, We should reasonably control the effect of the
170"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.354251012145749,"repulsion of negative contrast between natural examples and OEs. we propose an asymmetric method
171"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.3562753036437247,"of the negative contrast, Simα(x, xo), to decouple the repulsive force into an one-side push from the
172"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.3582995951417004,"natural example to the OE and an one-side push from the OE to the natural example, given by:
173"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.3603238866396761,"(a)
(b)"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.3623481781376518,Predicted as “cat”
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.3643724696356275,Predicted as “car”
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.36639676113360325,Predicted as “dog”
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.3684210526315789,Predicted as “cat”
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.37044534412955465,Predicted as “car”
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.3724696356275304,Predicted as “dog”
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.37449392712550605,"Pull
Push
Natural sample
Positive sample
Negative sample"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.3765182186234818,"Figure 2: Illustrations of class confusion when the classes of positive examples (i.e., AEs) and
negative examples (i.e., OEs) are different. (a) shows the normal situation before the optimization.
(b) shows the situation of class confusion after the optimization. In each circle, data points have the
same predicted class. In (a), AEs locate in the wrong predicted class different from natural example
and OEs. The TRADES loss narrow the gap of classification between natural examples and AEs,
and thus AEs in the wrong class pull natural examples to move toward the wrong class and the
negative contrast pushes natural examples to leave from the original class. With these actions, natural
examples come to the decision boundary and even into the wrong class easily as (b) shows."
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.3785425101214575,"Simα(x, xo) = α · Sim(x, xo) + (1 −α) · Sim(xo, x),
(3)"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.3805668016194332,"where Sim(x, xo) denotes the one-sided similarity of x and xo. When minimizing Sim(x, xo), we
174"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.3825910931174089,"stop the back-propagation gradient of x and only move xo away from x. α denotes the weighting
175"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.38461538461538464,"factor to adjust the magnitude of the two repulsive forces. When α = 0, OEs are frozen and only the
176"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.3866396761133603,"feature of natural samples is optimized to push far away from the feature of OEs. As α increases, the
177"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.38866396761133604,"natural sample becomes more repulsive to the OE and the OE pushes the natural example less. To
178"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.39068825910931176,"mitigate the class confusion problem, we should choose α that tends to 1 to reduce the repulsive force
179"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.39271255060728744,"from the OE to the natural example, to prevent the natural example from being pushed into the wrong
180"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.39473684210526316,"class. Experiments show that α =1 leads to the best performance provided in our supplementary
181"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.3967611336032389,"material), which pushes away NPs by only pushing off OEs and follows what we have expected.
182"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.39878542510121456,"Then we propose the negative contrast based on predicted probabilities, Simα
cc(x, xo), to measure
183"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.4008097165991903,"the repulsive force of NPs pushing away from each other. It pushes away NPs by decreasing the
184"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.402834008097166,"corresponding probabilities of the predicted classes when the predicted classes of NPs are consistent.
185"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.4048582995951417,"Simα
cc(x, xo) =
1
∥Bi∥ n
X"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.4068825910931174,"i=1
I (hi = ho
i ) ·

α
q"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.4089068825910931,"ˆpi(hi) · po
i (hi) + (1 −α)
q"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.4109311740890688,"pi(hi) · ˆpo
i (hi)

,
(4)"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.41295546558704455,"where ∥Bi∥denotes the batch size, I(·) denotes the Indicator function and ˆp denotes freezing the
186"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.4149797570850202,"back-propagation gradient of p. hi and hn
i denote the predicted classes of the NP. And pi and pn
i
187"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.41700404858299595,"denote the probability vectors of the NP. Under the negative contrast, the model pushes the natural
188"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.4190283400809717,"example in the direction away from the predicted class of the OE and push the OE in the direction
189"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.42105263157894735,"away from the predicted class of the natural example when and only when two predicted classes of
190"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.4230769230769231,"the NP are consistent. This ensures that the action of exclusion not only pushes away the feature of
191"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.4251012145748988,"NPs in the feature space, but also reduces the probabilities of NPs in the incorrect class. Since the
192"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.4271255060728745,"negative contrast has only directions to reduce the confidence and no explicit directions to increase
193"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.4291497975708502,"the confidence, it does not create any actions to push the natural example into the feature space of
194"
ASYMMETRIC NEGATIVE CONTRAST BASED ON PROBABILITIES,0.4311740890688259,"wrong classes even in the scenario of class confusion, which can effectively alleviate the problem.
195"
GENERATE NEGATIVE SAMPLES BY TARGETED ATTACK,0.4331983805668016,"3.2.2
Generate negative samples by targeted attack
196"
GENERATE NEGATIVE SAMPLES BY TARGETED ATTACK,0.4352226720647773,"To obtain OEs, previous negative sampling strategies [19] simply screen natural samples and pick up
197"
GENERATE NEGATIVE SAMPLES BY TARGETED ATTACK,0.43724696356275305,"the negatives from them, but rarely consider generating special negative samples to assist learning.
198"
GENERATE NEGATIVE SAMPLES BY TARGETED ATTACK,0.4392712550607287,"We innovatively propose a strategy to craft OEs by the targeted attack: natural negative examples with
199"
GENERATE NEGATIVE SAMPLES BY TARGETED ATTACK,0.44129554655870445,"labels that is different from those of natural examples are attacked to the labeled classes of natural
200"
GENERATE NEGATIVE SAMPLES BY TARGETED ATTACK,0.4433198380566802,"examples by targeted PGD-10 [26], to manufacture hard negatives containing adversarial noise.
201"
GENERATE NEGATIVE SAMPLES BY TARGETED ATTACK,0.44534412955465585,"xo
t+1 :=
Π
N(xo,ϵ) (xo
t −ϵ sign (∇xoLCE ((f(xo
t) , y))) ,
(5)"
GENERATE NEGATIVE SAMPLES BY TARGETED ATTACK,0.4473684210526316,"Where ∇xoLCE denotes the gradient of LCE with respect to xo. By this strategy, clean OEs randomly
202"
GENERATE NEGATIVE SAMPLES BY TARGETED ATTACK,0.4493927125506073,"chosen from other classes are attacked to the labeled classes of natural examples and become negative
203"
GENERATE NEGATIVE SAMPLES BY TARGETED ATTACK,0.451417004048583,"adversarial examples. The motivation makes intuitive sense. 1) The negative adversarial sample
204"
GENERATE NEGATIVE SAMPLES BY TARGETED ATTACK,0.4534412955465587,"generated by the targeted attack will be classified as the labeled class of the natural example with
205"
GENERATE NEGATIVE SAMPLES BY TARGETED ATTACK,0.45546558704453444,"high confidence, but its ground truth label is not that, which makes it a very hard negative sample
206"
GENERATE NEGATIVE SAMPLES BY TARGETED ATTACK,0.4574898785425101,"and is beneficial for the negative contrast. 2) The negative adversarial sample contains adversarial
207"
GENERATE NEGATIVE SAMPLES BY TARGETED ATTACK,0.45951417004048584,"noise, which is special feature that natural negative samples do not have. And this feature helps the
208"
GENERATE NEGATIVE SAMPLES BY TARGETED ATTACK,0.46153846153846156,"model learn the paradigm of adversarial noise and improve the robust performance. In particular, we
209"
GENERATE NEGATIVE SAMPLES BY TARGETED ATTACK,0.46356275303643724,"demonstrate that negative samples with adversarial noise do improve robustness better in Table 4.
210"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.46558704453441296,"3.3
Adversarial training with reverse Attention
211"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.4676113360323887,"Secondly, we continue to improve TRADES to learn robust representation that meets alignment.
212"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.46963562753036436,"Consider the calculating process of the model f(·). First, the feature vector z is obtained by g(x),
213"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.4716599190283401,"and then the output vector Ωz is obtained by a linear mapping Linear(z). Each element zi in z
214"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.47368421052631576,"represents the activation level of the feature channel that may be helpful for classification, with larger
215"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.4757085020242915,"values representing more feature information extracted from that channel; ωi,j in Ωrepresents the
216"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.4777327935222672,"importance of the ith feature channel to the jth class, with higher values representing the greater
217"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.4797570850202429,"contribution of the feature channel to the class. Motivated by [1, 34], we exploit the importance
218"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.4817813765182186,"of feature channels to target classes to align the feature of examples of the same classes and pull
219"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.48380566801619435,"close the feature of PPs, which is named by reverse attention. To be specific, we take the Hadamard
220"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.48582995951417,"product (Kronecker product) of partial weight of the classifier Ωj and the feature vector z. It can
221"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.48785425101214575,"weight feature channel by channel according to its contribution to being classified as the target class
222"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.4898785425101215,"j, and gain a class-aware feature vector z′ containing the information of the target class j.
223"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.49190283400809715,"z′
i =
zi ⊙ωi,y,
(training phase)
zi ⊙ωi,h(x),
(testing phase)
(6)"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.4939271255060729,"where ⊙denotes the Hadamard product operation, which is the method of multiplying two matrices of
224"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.4959514170040486,"the same size element by element to obtain a new matrix of the same size. To ensure parameters used
225"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.4979757085020243,"for weighting have the correct feature-to-class importance, we use the unweighted feature vector z to
226"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.5,"go through Linear(·) to obtain the auxiliary probability vector p , and z′ to get the final probability
227"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.5020242914979757,"vector p′ . Finally, we use both p and p′ to train the model. During the training phase, we use the true
228"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.5040485829959515,"label y as an indicator to determine the importance of channels, i.e., Ωj = Ωy. And in the testing
229"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.5060728744939271,"phase, since the true label is not available, we simply choose a sub-vector of the linear weight by the
230"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.5080971659919028,"predicted class h(x) as the importance of channels. We add the reverse attention to the last feature
231"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.5101214574898786,"layer in the model, which generally contains two blocks. The model with the reverse attention does
232"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.5121457489878543,"not need any extra modules, but module interactions are changed.
233"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.5141700404858299,"Let’ s make a detailed analysis and explanation of the principle of this method. The class information
234"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.5161943319838057,"from labels guides the input image to be mapped from the feature to the classification vector during
235"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.5182186234817814,"training, establishing an feature-to-class mapping relationship. In the model, the feature extractor
236"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.520242914979757,"captures the representation that is helpful for classification until the feature vector contains enough
237"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.5222672064777328,"information that allows the classifier to classify the sample as the target class. Among all the modules,
238"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.5242914979757085,"the classifier is the closest to labels and learns which feature channel plays an important role in being
239"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.5263157894736842,"classified as the target class (i.e., the feature importance). Since the classifier is unique, the importance
240"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.52834008097166,"of the feature channels of one example is exactly the same with that of the other samples in the same
241"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.5303643724696356,"class, benefiting the generalization and robustness of the model in the target class. We propose the
242"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.5323886639676113,"reverse attention to utilize this information to improve feature rather than classification. The feature
243"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.5344129554655871,"vectors are weighted by partial parameters of the linear layer that belong to the target class, which
244"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.5364372469635628,"can change the activation of each channel adaptively according to the feature importance, acting as an
245"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.5384615384615384,"attention with the guidance of the class information. After the attention, the important channels in the
246"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.5404858299595142,"feature vector are boosted and the redundant channels are weakened, i.e., the information contributes
247"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.5425101214574899,"to the target class will become larger and more significant, which is helpful for correct classification.
248"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.5445344129554656,"Considering from the perspective of the feature distribution, the weighted feature has gained extra
249"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.5465587044534413,"class information, which induces changes in the feature distribution. Feature vectors with the same
250"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.548582995951417,"target class get the same weighting, and thus the weighted feature becomes more similar. Moreover,
251"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.5506072874493927,"feature vectors with different target classes are weighted according to different weights, and the
252"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.5526315789473685,"weighted feature distributions become more inconsistent. Therefore, the reverse attention guides
253"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.5546558704453441,"the alignment of the feature of the examples in the same class, pulling the feature of PPs closer and
254"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.5566801619433198,"pushing the feature of NPs far away, which benefits alignment and drops by to promote exclusion and
255"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.5587044534412956,"classification. Aligned feature has similar activations in every feature channel, which helps the model
256"
ADVERSARIAL TRAINING WITH REVERSE ATTENTION,0.5607287449392713,"narrows the gap between feature of natural examples and AEs.
257"
EXPERIMENTS,0.562753036437247,"4
Experiments
258"
EXPERIMENTS,0.5647773279352226,"To demonstrate the effectiveness of the proposed approach, we show feature distribution of trained
259"
EXPERIMENTS,0.5668016194331984,"models firstly. Then we evaluate our framework against white-box attacks and adaptive attacks, and
260"
EXPERIMENTS,0.5688259109311741,"make a compare with other defense methods. We conduct experiments across different datasets
261"
EXPERIMENTS,0.5708502024291497,"and models. Because our methods are compatible with existing AT techniques and can be easily
262"
EXPERIMENTS,0.5728744939271255,"incorporated in a plug-and-play manner, we choose three baselines [26, 36, 30] to combine with our
263"
EXPERIMENTS,0.5748987854251012,"framework for evaluation: PGD-AT-ANCRA, TRADES-ANCRA, and MART-ANCRA.
264"
SETTINGS,0.5769230769230769,"4.1
Settings
265"
SETTINGS,0.5789473684210527,"Implementation
On CIFAR-10 and CIFAR-100 [22], we train ResNet18 [15] with a weight
266"
SETTINGS,0.5809716599190283,"decay of 2.0 × 10−4. On Tiny-ImageNet [10], we use PreActResNet18 [16]with a weight decay of
267"
SETTINGS,0.582995951417004,"5.0 × 10−4. We adopt the SGD optimizer with a learning rate of 0.01, a momentum of 0.9, epochs of
268"
SETTINGS,0.5850202429149798,"120 and a batch size of 128 as [30]. For the trade-off hyperparameters β, we use 6.0 in TRADES
269"
SETTINGS,0.5870445344129555,"and 5.0 in MART, following the original setting in their papers. For other hyperparameters, we tune
270"
SETTINGS,0.5890688259109311,"the values based on TRADES-ANCRA. We generate adversarial example for training by L∞-norm
271"
SETTINGS,0.5910931174089069,"PGD [26], with a step size of 0.007, an attack iterations of 10 and perturbation budget of 8/255. We
272"
SETTINGS,0.5931174089068826,"use single NVIDIA A100 and two GTX 2080 Ti in the experiments.
273"
SETTINGS,0.5951417004048583,"Baseline
We compare the proposed PGD-AT-ANCRA, TRADES-ANCRA, and MART-ANCRA
274"
SETTINGS,0.597165991902834,"with the popular baselines: PGD-AT [26], TRADES [36], MART [30] and SAT [18]. Moreover, we
275"
SETTINGS,0.5991902834008097,"also choose three state-of-the-art methods: AWP [31], S2O [20] and UDR [4]. We keep the same
276"
SETTINGS,0.6012145748987854,"settings among all the baselines with our settings and follow their original hyperparameters.
277"
SETTINGS,0.6032388663967612,"Evaluation
We choose several adversarial attacks to attack the target models, including PGD [26],
278"
SETTINGS,0.6052631578947368,"FGSM [13], C&W [5] and AutoAttack [9] which is a powerful and reliable attack and an ensemble
279"
SETTINGS,0.6072874493927125,"attack with three white-box attacks and one black-box attack. We notice that our methods use the
280"
SETTINGS,0.6093117408906883,"auxiliary probability vector p in the training and testing phase, so we design two scenaios: 1) train
281"
SETTINGS,0.611336032388664,"with p and test without p; 2) train with p and test with p. 1) denotes evaluation against white-box
282"
SETTINGS,0.6133603238866396,"attacks and 2) denotes evaluation against adaptive attacks. Following the default setting of AT, the
283"
SETTINGS,0.6153846153846154,"max perturbation strength is set as 8. / 255. for all attack methods under the L∞. The attack iterations
284"
SETTINGS,0.6174089068825911,"of PGD and C&W is 40 (i.e., PGD-40), and the step size of FGSM is 8. / 255. unlike 0.007 for other
285"
SETTINGS,0.6194331983805668,"attacks. The clean accuracy and robust accuracy are used as the evaluation metrics.
286"
COMPARISON RESULTS OF FEATURE DISTRIBUTION,0.6214574898785425,"4.2
Comparison results of feature distribution
287"
COMPARISON RESULTS OF FEATURE DISTRIBUTION,0.6234817813765182,"(a)
(b)
(c)
(d)"
COMPARISON RESULTS OF FEATURE DISTRIBUTION,0.6255060728744939,"Figure 3: Frequency histograms of the L2 distance and cosine similarity of feature of natural examples,
AEs and OEs. We train ResNet-18 models on CIFAR-10 with four defense techniques: PDG-AT,
TRADES, MART and TRADES-ANCRA. Other details are the same with Figure 1
Frequency histograms of feature distribution is shown in Figure 3. It is shown that our methods can
288"
COMPARISON RESULTS OF FEATURE DISTRIBUTION,0.6275303643724697,"greatly improve feature distribution, which follows the characteristics of exclusion and alignment. In
289"
COMPARISON RESULTS OF FEATURE DISTRIBUTION,0.6295546558704453,"Figure 3 (a) and Figure 3 (b), it shows that the cosine similarity of the model trained by our method
290"
COMPARISON RESULTS OF FEATURE DISTRIBUTION,0.631578947368421,"between natural examples and OEs shows a skewed distribution between -0.05 and 0.1, and the L2
291"
COMPARISON RESULTS OF FEATURE DISTRIBUTION,0.6336032388663968,"distance with our method shows a Gaussian distribution between 5.5 and 10.0, which indicates natural
292"
COMPARISON RESULTS OF FEATURE DISTRIBUTION,0.6356275303643725,"examples and OEs have been fully distinguished in the feature space and exclusion has been met. In
293"
COMPARISON RESULTS OF FEATURE DISTRIBUTION,0.6376518218623481,"Figure 3 (c) and Figure 3 (d), it shows that in the model trained by our method there are a uniform
294"
COMPARISON RESULTS OF FEATURE DISTRIBUTION,0.6396761133603239,"distribution between 0.95 and 0.99 for the cosine similarity of the feature between natural examples
295"
COMPARISON RESULTS OF FEATURE DISTRIBUTION,0.6417004048582996,"and AEs, and a skewed distribution between 0.05 and 1.5 for the L2 distance of the feature, which
296"
COMPARISON RESULTS OF FEATURE DISTRIBUTION,0.6437246963562753,"indicates the feature between natural examples and AEs is very close to each other and alignment has
297"
COMPARISON RESULTS OF FEATURE DISTRIBUTION,0.645748987854251,"been confirmed. Thus, our framework successfully helps AT to obtain robust feature.
298"
COMPARISON RESULTS AGAINST WHITE-BOX ATTACKS,0.6477732793522267,"4.3
Comparison results against white-box attacks
299"
COMPARISON RESULTS AGAINST WHITE-BOX ATTACKS,0.6497975708502024,"Table 1: Robustness (%) against white-box attacks. Nat denotes clean accuracy. PGD denotes robust
accuracy against PGD-40. FGSM denotes robust accuracy against FGSM. C&W denotes robust
accuracy against C&W. AA denotes robust accuracy against AutoAttack. Mean denotes average
robust accuracy against these four attacks. We show the most successful defense with bold."
COMPARISON RESULTS AGAINST WHITE-BOX ATTACKS,0.6518218623481782,"Defense
CIFAR-10
CIFAR-100"
COMPARISON RESULTS AGAINST WHITE-BOX ATTACKS,0.6538461538461539,"Nat
PGD FGSM C&W
AA
Mean
Nat
PGD FGSM C&W
AA
Mean
PGD-AT
80.90 44.35 58.41 46.72 42.14 47.91 56.21 19.41 30.00 41.76 17.76 27.23
TRADES
78.92 48.40 59.60 47.59 45.44 50.26 53.46 25.37 32.97 43.59 21.35 30.82
MART
79.03 48.90 60.86 45.92 43.88 49.89 53.26 25.06 33.35 38.07 21.04 29.38
SAT
63.28 43.57 50.13 47.47 39.72 45.22 42.55 23.30 28.36 41.03 18.73 27.86
AWP
76.38 48.88 57.47 48.22 44.65 49.81 54.53 27.35 34.47 44.91 21.98 31.18
S2O
40.09 24.05 29.76 47.00 44.00 36.20 26.66 13.11 16.83 43.00 21.00 23.49
UDR
57.80 39.79 45.02 46.92 34.73 41.62 33.63 20.61 24.19 33.77 16.41 23.75"
COMPARISON RESULTS AGAINST WHITE-BOX ATTACKS,0.6558704453441295,"PGD-AT-ANCRA 85.10
85.10
85.10 89.03
89.03
89.03 87.00 89.23
89.23
89.23 59.15 81.10 59.73 58.10 58.45 58.58 34.44 52.39
TRADES-ANCRA 81.70 82.96
82.96
82.96 82.74 83.01 59.70
59.70
59.70 77.10 53.73 51.24 52.17 52.55 35.81
35.81
35.81 47.94
MART-ANCRA
84.88 88.56 87.95
87.95
87.95 88.77 59.62 81.23
81.23
81.23 60.10
60.10
60.10 58.40
58.40
58.40 58.74
58.74
58.74 59.41
59.41
59.41 35.05 52.90
52.90
52.90"
COMPARISON RESULTS AGAINST WHITE-BOX ATTACKS,0.6578947368421053,"We train ResNet-18 by different defense on CIFAR-10 and CIFAR-100 to evaluate them under
300"
COMPARISON RESULTS AGAINST WHITE-BOX ATTACKS,0.659919028340081,"white-box attacks. And more results in PreActResNet18 on Tiny-ImageNet are provided in our
301"
COMPARISON RESULTS AGAINST WHITE-BOX ATTACKS,0.6619433198380567,"supplementary material. The results on CIFAR-10 and CIFAR-100 are shown in Table 1. First, on
302"
COMPARISON RESULTS AGAINST WHITE-BOX ATTACKS,0.6639676113360324,"CIFAR-10, our approaches improve the clean accuracy of based approaches by 5.2%, 3.2% and 5.9%,
303"
COMPARISON RESULTS AGAINST WHITE-BOX ATTACKS,0.6659919028340081,"and also improves the robust performance under all the attacks (e.g., increase by 44.7%, 34.6% and
304"
COMPARISON RESULTS AGAINST WHITE-BOX ATTACKS,0.6680161943319838,"39.7% against PGD). Compared with state-of-the-art defense, the robust accuracy against different
305"
COMPARISON RESULTS AGAINST WHITE-BOX ATTACKS,0.6700404858299596,"attacks of our methods is almost two times as large than theirs (e.g., 81.23% VS 49.81%). Second, on
306"
COMPARISON RESULTS AGAINST WHITE-BOX ATTACKS,0.6720647773279352,"CIFAR-100, our approaches also greatly improve the robustness and advance the clean accuracy. The
307"
COMPARISON RESULTS AGAINST WHITE-BOX ATTACKS,0.6740890688259109,"clean accuracy of our methods has been increased by 3.5%, 0.3% and 6.8% compared with based
308"
COMPARISON RESULTS AGAINST WHITE-BOX ATTACKS,0.6761133603238867,"methods, and the lowest average robust accuracy of ours is larger than the best one among other
309"
COMPARISON RESULTS AGAINST WHITE-BOX ATTACKS,0.6781376518218624,"methods by 16.8%. In general, our three approaches gain the best performance both in the natural and
310"
COMPARISON RESULTS AGAINST WHITE-BOX ATTACKS,0.680161943319838,"attacked scenaios. To our surprise, MART-ANCRA and PGD-ANCRA rather than TRADES-ANCRA
311"
COMPARISON RESULTS AGAINST WHITE-BOX ATTACKS,0.6821862348178138,"gain the best performance in a lot of cases without hyper-parameter tuning. Besides, our approaches
312"
COMPARISON RESULTS AGAINST WHITE-BOX ATTACKS,0.6842105263157895,"not only improves robustness but also enhances clean accuracy, though there is always a trade-off
313"
COMPARISON RESULTS AGAINST WHITE-BOX ATTACKS,0.6862348178137652,"between clean and robust accuracy. These results indicate that our approaches can vastly boost the
314"
COMPARISON RESULTS AGAINST WHITE-BOX ATTACKS,0.6882591093117408,"robustness of models against white-box attacks.
315"
COMPARISON RESULTS AGAINST ADAPTIVE ATTACKS,0.6902834008097166,"4.4
Comparison results against adaptive attacks
316"
COMPARISON RESULTS AGAINST ADAPTIVE ATTACKS,0.6923076923076923,"We train several ResNet18 models on CIFAR-10 by PGD-AT-ANCRA, TRADES-ANCRA, MART-
317"
COMPARISON RESULTS AGAINST ADAPTIVE ATTACKS,0.694331983805668,"ANCRA and test the same models without p. In addition, we report vanilla based approaches as
318"
COMPARISON RESULTS AGAINST ADAPTIVE ATTACKS,0.6963562753036437,"baseline. Results are in Table 2. It indicates that our approaches can still maintain superb performance
319"
COMPARISON RESULTS AGAINST ADAPTIVE ATTACKS,0.6983805668016194,"after adaptive attacks, e.g., the robust accuracy against PGD of our methods without p are larger than
320"
COMPARISON RESULTS AGAINST ADAPTIVE ATTACKS,0.7004048582995951,"those of baseline by 13.28%, 10.08% and 8.06%.
321"
ABLATION STUDIES,0.7024291497975709,"4.5
Ablation studies
322"
ABLATION STUDIES,0.7044534412955465,"Two defense methods.
We train four models by TRADES, TRADES with the asymmetric nega-
323"
ABLATION STUDIES,0.7064777327935222,"tive contrast (TRADES-ANC), TRADES with the reverse attention (TRADES-RA) and TRADES-
324"
ABLATION STUDIES,0.708502024291498,"ANCRA, respectively. The results of evaluation against adaptive attacks are shown in Table 3. First,
325"
ABLATION STUDIES,0.7105263157894737,Table 2: Robustness(%) of ResNet-18 trained with our approaches and attacked with or without p.
ABLATION STUDIES,0.7125506072874493,"Approach
Nat
Attack with p
Attack without p"
ABLATION STUDIES,0.7145748987854251,"PGD
FGSM
C&W
PGD
FGSM
C&W
Vanilla TRADES
78.92
\
\
\
48.40
59.60
47.59
TRADES-ANCRA
81.70
61.68
61.56
72.36
82.96
82.74
83.01
Vanilla PGD-AT
80.90
\
\
\
44.35
58.41
46.72
PGD-AT-ANCRA
85.10
54.43
58.23
66.36
89.03
87.00
89.23
Vanilla MART
79.09
\
\
\
48.90
60.86
45.92
MART-ANCRA
84.88
56.96
60.43
71.06
88.56
87.95
88.77"
ABLATION STUDIES,0.7165991902834008,"when incorporating the asymmetric negative contrast only, the performance of robustness against all
326"
ABLATION STUDIES,0.7186234817813765,"the attacks and clean accuracy have been improved compared with vanilla TRADES (e.g., 48.36% VS
327"
ABLATION STUDIES,0.7206477732793523,"54.18% against PGD-40). Next, when incorporating the reverse attention only, the performance on
328"
ABLATION STUDIES,0.7226720647773279,"clean and adversarial data is also improved greatly compared with TRADES (e.g., 48.36% VS 61.69%
329"
ABLATION STUDIES,0.7246963562753036,"against PGD-40). Thus, it shows each method contributes to robustness and generalization. Besides,
330"
ABLATION STUDIES,0.7267206477732794,"when Trdeas-ANCRA is compared with TRADES-RA, the clean accuracy and robust accuracy
331"
ABLATION STUDIES,0.728744939271255,"against all the attacks except AA have been enhanced, which indicates that the two strategies are
332"
ABLATION STUDIES,0.7307692307692307,"compatible and the combination can alleviate the side effect of independent methods.
333"
ABLATION STUDIES,0.7327935222672065,"Strategy of negative samples
We compare our strategy of the targeted attack with other strategies
334"
ABLATION STUDIES,0.7348178137651822,"to select negative samples, including Random, Soft-LS and Hard-LS proposed by Bui et al. [3]. The
335"
ABLATION STUDIES,0.7368421052631579,"details of them are provided in our supplementary material. The results are shown in Table 4. To make
336"
ABLATION STUDIES,0.7388663967611336,"a comprehensive compare, we show results of both the best models and last models with different
337"
ABLATION STUDIES,0.7408906882591093,"strategies. It shows that our strategy have the best performance of robustness and clean accuracy in
338"
ABLATION STUDIES,0.742914979757085,"the last models, and achieve the best robust accuracy in the best models.
339"
ABLATION STUDIES,0.7449392712550608,"Table 3: Clean and robust accuracy (%) of ResNet-18 trained by TRADES, TRADES-ANC, TRADES-
RA and TRADES-ANCRA on CIFAR-10 against various attacks."
ABLATION STUDIES,0.7469635627530364,"Defense
Nat
PGD
FGSM
C&W
AA
TRADES
78.92
48.40
59.60
47.59
45.44
TRADES-ANC
80.77
54.18
63.44
49.84
48.51
TRADES-RA
80.46
61.59
61.48
72.15
61.02
TRADES-ANCRA
81.70
61.68
61.56
72.36
59.70"
ABLATION STUDIES,0.7489878542510121,"Table 4: Results of the best and last with four strategies of negative example. Best- denotes results in
the best models and Last- denotes results in the last models. We show the best results with bold."
ABLATION STUDIES,0.7510121457489879,"Strategy
Best-Nat
Best-PGD
Last-Nat
Last-PGD
Random
81.44
62.64
81.78
61.71
Soft-LS
82.10
61.83
80.62
58.47
Hard-LS
82.30
82.30
82.30
62.53
82.13
60.98
Targeted attack
81.36
63.08
63.08
63.08
82.18
82.18
82.18
62.02
62.02
62.02"
CONCLUSION,0.7530364372469636,"5
Conclusion
340"
CONCLUSION,0.7550607287449392,"This work addresses an overlook of robust representation learning in the adversarial training by a
341"
CONCLUSION,0.757085020242915,"generic AT framework with the asymmetric negative contrast and reverse attention. We propose
342"
CONCLUSION,0.7591093117408907,"two characteristics of robust feature to guide the improvement of AT, i.e., exclusion and alignment.
343"
CONCLUSION,0.7611336032388664,"Specifically, the asymmetric negative contrast based on probabilities fixes natural examples, and only
344"
CONCLUSION,0.7631578947368421,"pushes away adversarial examples of other classes in the feature space. Besides, the reverse attention
345"
CONCLUSION,0.7651821862348178,"weights feature by parameters of the linear classifier, to provide class information and align feature of
346"
CONCLUSION,0.7672064777327935,"the same class. Our framework can be used in a plug-and-play manner with other defense methods.
347"
CONCLUSION,0.7692307692307693,"Analysis and empirical evaluations demonstrate that our framework can obtain robust feature and
348"
CONCLUSION,0.771255060728745,"greatly improve robustness and generalization.
349"
REFERENCES,0.7732793522267206,"References
350"
REFERENCES,0.7753036437246964,"[1] Yang Bai, Yuyuan Zeng, Yong Jiang, Shu-Tao Xia, Xingjun Ma, and Yisen Wang. Improving adversarial
351"
REFERENCES,0.7773279352226721,"robustness via channel-wise activation suppressing. arXiv preprint arXiv:2103.08307, 2021.
352"
REFERENCES,0.7793522267206477,"[2] Nicholas Boucher, Ilia Shumailov, Ross Anderson, and Nicolas Papernot. Bad characters: Imperceptible
353"
REFERENCES,0.7813765182186235,"nlp attacks. In 2022 IEEE Symposium on Security and Privacy (SP), pages 1987–2004, 2022. doi:
354"
REFERENCES,0.7834008097165992,"10.1109/SP46214.2022.9833641.
355"
REFERENCES,0.7854251012145749,"[3] Anh Bui, Trung Le, He Zhao, Paul Montague, Seyit Camtepe, and Dinh Phung. Understanding and achiev-
356"
REFERENCES,0.7874493927125507,"ing efficient robustness with adversarial supervised contrastive learning. arXiv preprint arXiv:2101.10027,
357"
REFERENCES,0.7894736842105263,"2021.
358"
REFERENCES,0.791497975708502,"[4] Tuan Anh Bui, Trung Le, Quan Hung Tran, He Zhao, and Dinh Q. Phung.
A unified wasserstein
359"
REFERENCES,0.7935222672064778,"distributional robustness framework for adversarial training. CoRR, abs/2202.13437, 2022. URL https:
360"
REFERENCES,0.7955465587044535,"//arxiv.org/abs/2202.13437.
361"
REFERENCES,0.7975708502024291,"[5] Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks. 2017 IEEE
362"
REFERENCES,0.7995951417004049,"Symposium on Security and Privacy (SP), pages 39–57, 2016.
363"
REFERENCES,0.8016194331983806,"[6] Tianlong Chen, Zhenyu Zhang, Sijia Liu, Shiyu Chang, and Zhangyang Wang. Robust overfitting may be
364"
REFERENCES,0.8036437246963563,"mitigated by properly learned smoothening. In International Conference on Learning Representations,
365"
REFERENCES,0.805668016194332,"2021.
366"
REFERENCES,0.8076923076923077,"[7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
367"
REFERENCES,0.8097165991902834,"contrastive learning of visual representations. In Hal Daumé III and Aarti Singh, editors, Proceedings of
368"
REFERENCES,0.8117408906882592,"the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning
369"
REFERENCES,0.8137651821862348,"Research, pages 1597–1607. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/v119/
370"
REFERENCES,0.8157894736842105,"chen20j.html.
371"
REFERENCES,0.8178137651821862,"[8] Xiangning Chen, Cihang Xie, Mingxing Tan, Li Zhang, Cho-Jui Hsieh, and Boqing Gong. Robust and
372"
REFERENCES,0.819838056680162,"accurate object detection via adversarial learning. In Proceedings of the IEEE/CVF conference on computer
373"
REFERENCES,0.8218623481781376,"vision and pattern recognition, pages 16622–16631, 2021.
374"
REFERENCES,0.8238866396761133,"[9] Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of
375"
REFERENCES,0.8259109311740891,"diverse parameter-free attacks. In International Conference on Machine Learning, 2020.
376"
REFERENCES,0.8279352226720648,"[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, K. Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
377"
REFERENCES,0.8299595141700404,"image database. 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248–255,
378"
REFERENCES,0.8319838056680162,"2009.
379"
REFERENCES,0.8340080971659919,"[11] Yinpeng Dong, Ke Xu, Xiao Yang, Tianyu Pang, Zhijie Deng, Hang Su, and Jun Zhu.
Exploring
380"
REFERENCES,0.8360323886639676,"memorization in adversarial training. arXiv preprint arXiv:2106.01606, 2021.
381"
REFERENCES,0.8380566801619433,"[12] Lijie Fan, Sijia Liu, Pin-Yu Chen, Gaoyuan Zhang, and Chuang Gan. When does contrastive learning
382"
REFERENCES,0.840080971659919,"preserve adversarial robustness from pretraining to finetuning? Advances in neural information processing
383"
REFERENCES,0.8421052631578947,"systems, 34:21480–21492, 2021.
384"
REFERENCES,0.8441295546558705,"[13] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
385"
REFERENCES,0.8461538461538461,"examples. CoRR, abs/1412.6572, 2014.
386"
REFERENCES,0.8481781376518218,"[14] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya,
387"
REFERENCES,0.8502024291497976,"Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray
388"
REFERENCES,0.8522267206477733,"Kavukcuoglu, Rémi Munos, and Michal Valko. Bootstrap your own latent a new approach to self-supervised
389"
REFERENCES,0.854251012145749,"learning. In Proceedings of the 34th International Conference on Neural Information Processing Systems,
390"
REFERENCES,0.8562753036437247,"NIPS’20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546.
391"
REFERENCES,0.8582995951417004,"[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
392"
REFERENCES,0.8603238866396761,"In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016.
393"
REFERENCES,0.8623481781376519,"doi: 10.1109/CVPR.2016.90.
394"
REFERENCES,0.8643724696356275,"[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks.
395"
REFERENCES,0.8663967611336032,"In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors, Computer Vision – ECCV 2016, pages
396"
REFERENCES,0.868421052631579,"630–645, Cham, 2016. Springer International Publishing. ISBN 978-3-319-46493-0.
397"
REFERENCES,0.8704453441295547,"[17] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised
398"
REFERENCES,0.8724696356275303,"visual representation learning. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition
399"
REFERENCES,0.8744939271255061,"(CVPR), pages 9726–9735, 2020. doi: 10.1109/CVPR42600.2020.00975.
400"
REFERENCES,0.8765182186234818,"[18] Lang Huang, Chao Zhang, and Hongyang Zhang. Self-adaptive training: beyond empirical risk minimiza-
401"
REFERENCES,0.8785425101214575,"tion. Advances in neural information processing systems, 33:19365–19376, 2020.
402"
REFERENCES,0.8805668016194332,"[19] Tri Huynh, Simon Kornblith, Matthew R Walter, Michael Maire, and Maryam Khademi. Boosting
403"
REFERENCES,0.8825910931174089,"contrastive self-supervised learning with false negative cancellation. In Proceedings of the IEEE/CVF
404"
REFERENCES,0.8846153846153846,"winter conference on applications of computer vision, pages 2785–2795, 2022.
405"
REFERENCES,0.8866396761133604,"[20] Gaojie Jin, Xinping Yi, Wei Huang, Sven Schewe, and Xiaowei Huang. Enhancing adversarial training
406"
REFERENCES,0.888663967611336,"with second-order statistics of weights. In Proceedings of the IEEE/CVF Conference on Computer Vision
407"
REFERENCES,0.8906882591093117,"and Pattern Recognition, pages 15273–15283, 2022.
408"
REFERENCES,0.8927125506072875,"[21] Minseon Kim, Jihoon Tack, and Sung Ju Hwang. Adversarial self-supervised contrastive learning. Advances
409"
REFERENCES,0.8947368421052632,"in Neural Information Processing Systems, 33:2983–2994, 2020.
410"
REFERENCES,0.8967611336032388,"[22] Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.
411"
REFERENCES,0.8987854251012146,"[23] Michal Kucer, Diane Oyen, and Garrett Kenyon. When does visual self-supervision aid adversarial training
412"
REFERENCES,0.9008097165991903,"in improving adversarial robustness?
413"
REFERENCES,0.902834008097166,"[24] Fangzhou Liao, Ming Liang, Yinpeng Dong, Tianyu Pang, Xiaolin Hu, and Jun Zhu. Defense against
414"
REFERENCES,0.9048582995951417,"adversarial attacks using high-level representation guided denoiser. In 2018 IEEE/CVF Conference on
415"
REFERENCES,0.9068825910931174,"Computer Vision and Pattern Recognition, pages 1778–1787, 2018. doi: 10.1109/CVPR.2018.00191.
416"
REFERENCES,0.9089068825910931,"[25] Yawei Luo, Ping Liu, Liang Zheng, Tao Guan, Junqing Yu, and Yi Yang. Category-level adversarial
417"
REFERENCES,0.9109311740890689,"adaptation for semantic segmentation using purified features. IEEE Transactions on Pattern Analysis and
418"
REFERENCES,0.9129554655870445,"Machine Intelligence, 44(8):3940–3956, 2022. doi: 10.1109/TPAMI.2021.3064379.
419"
REFERENCES,0.9149797570850202,"[26] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards
420"
REFERENCES,0.917004048582996,"deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.
421"
REFERENCES,0.9190283400809717,"[27] John Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, and Yanjun Qi. TextAttack: A framework
422"
REFERENCES,0.9210526315789473,"for adversarial attacks, data augmentation, and adversarial training in NLP. In Proceedings of the 2020
423"
REFERENCES,0.9230769230769231,"Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages
424"
REFERENCES,0.9251012145748988,"119–126, Online, October 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.
425"
REFERENCES,0.9271255060728745,"emnlp-demos.16. URL https://aclanthology.org/2020.emnlp-demos.16.
426"
REFERENCES,0.9291497975708503,"[28] Federico Nesti, Giulio Rossolini, Saasha Nair, Alessandro Biondi, and Giorgio Buttazzo. Evaluating the
427"
REFERENCES,0.9311740890688259,"robustness of semantic segmentation for autonomous driving against real-world adversarial patch attacks.
428"
REFERENCES,0.9331983805668016,"In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2280–2289,
429"
REFERENCES,0.9352226720647774,"2022.
430"
REFERENCES,0.937246963562753,"[29] Aäron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive
431"
REFERENCES,0.9392712550607287,"coding. ArXiv, abs/1807.03748, 2018.
432"
REFERENCES,0.9412955465587044,"[30] Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu. Improving adver-
433"
REFERENCES,0.9433198380566802,"sarial robustness requires revisiting misclassified examples. In International Conference on Learning
434"
REFERENCES,0.9453441295546559,"Representations, 2020.
435"
REFERENCES,0.9473684210526315,"[31] Dongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust generalization.
436"
REFERENCES,0.9493927125506073,"Advances in Neural Information Processing Systems, 33:2958–2969, 2020.
437"
REFERENCES,0.951417004048583,"[32] Chang Xiao, Peilin Zhong, and Changxi Zheng. Enhancing adversarial defense by k-winners-take-all.
438"
REFERENCES,0.9534412955465587,"arXiv preprint arXiv:1905.10510, 2019.
439"
REFERENCES,0.9554655870445344,"[33] Cihang Xie, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou, Lingxi Xie, and Alan Yuille. Adversarial examples
440"
REFERENCES,0.9574898785425101,"for semantic segmentation and object detection. In Proceedings of the IEEE international conference on
441"
REFERENCES,0.9595141700404858,"computer vision, pages 1369–1378, 2017.
442"
REFERENCES,0.9615384615384616,"[34] Hanshu Yan, Jingfeng Zhang, Gang Niu, Jiashi Feng, Vincent Tan, and Masashi Sugiyama. Cifs: Improving
443"
REFERENCES,0.9635627530364372,"adversarial robustness of cnns via channel-wise importance-based feature selection. In International
444"
REFERENCES,0.9655870445344129,"Conference on Machine Learning, pages 11693–11703. PMLR, 2021.
445"
REFERENCES,0.9676113360323887,"[35] Qiying Yu, Jieming Lou, Xianyuan Zhan, Qizhang Li, Wangmeng Zuo, Yang Liu, and Jingjing Liu.
446"
REFERENCES,0.9696356275303644,"Adversarial contrastive learning via asymmetric infonce. In Computer Vision–ECCV 2022: 17th European
447"
REFERENCES,0.97165991902834,"Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part V, pages 53–69. Springer, 2022.
448"
REFERENCES,0.9736842105263158,"[36] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theo-
449"
REFERENCES,0.9757085020242915,"retically principled trade-off between robustness and accuracy. In International conference on machine
450"
REFERENCES,0.9777327935222672,"learning, pages 7472–7482. PMLR, 2019.
451"
REFERENCES,0.979757085020243,"[37] Jingfeng Zhang, Xilie Xu, Bo Han, Gang Niu, Lizhen Cui, Masashi Sugiyama, and Mohan Kankanhalli.
452"
REFERENCES,0.9817813765182186,"Attacks which do not kill training make adversarial learning stronger. In International conference on
453"
REFERENCES,0.9838056680161943,"machine learning, pages 11278–11287. PMLR, 2020.
454"
REFERENCES,0.9858299595141701,"[38] Shiji Zhao, Jie Yu, Zhenlong Sun, Bo Zhang, and Xingxing Wei. Enhanced accuracy and robustness via
455"
REFERENCES,0.9878542510121457,"multi-teacher adversarial distillation. In European Conference on Computer Vision, 2022.
456"
REFERENCES,0.9898785425101214,"[39] Dawei Zhou, Nannan Wang, Bo Han, and Tongliang Liu. Modeling adversarial noise for adversarial
457"
REFERENCES,0.9919028340080972,"training. In International Conference on Machine Learning, pages 27353–27366. PMLR, 2022.
458"
REFERENCES,0.9939271255060729,"[40] Daniel Zoran, Mike Chrzanowski, Po-Sen Huang, Sven Gowal, Alex Mott, and Pushmeet Kohli. Towards
459"
REFERENCES,0.9959514170040485,"robust image classification using sequential attention models. In Proceedings of the IEEE/CVF conference
460"
REFERENCES,0.9979757085020243,"on computer vision and pattern recognition, pages 9483–9492, 2020.
461"
