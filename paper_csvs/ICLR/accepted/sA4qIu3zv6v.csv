Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0013513513513513514,"This paper considers two-player zero-sum finite-horizon Markov games with si-
multaneous moves. The study focuses on the challenging settings where the value
function or the model is parameterized by general function classes. Provably effi-
cient algorithms for both decoupled and coordinated settings are developed. In the
decoupled setting where the agent controls a single player and plays against an ar-
bitrary opponent, we propose a new model-free algorithm. The sample complexity
is governed by the Minimax Eluder dimension—a new dimension of the function
class in Markov games. As a special case, this method improves the state-of-the-art
algorithm by a
√"
ABSTRACT,0.002702702702702703,"d factor in the regret when the reward function and transition
kernel are parameterized with d-dimensional linear features. In the coordinated
setting where both players are controlled by the agent, we propose a model-based
algorithm and a model-free algorithm. In the model-based algorithm, we prove that
sample complexity can be bounded by a generalization of Witness rank to Markov
games. The model-free algorithm enjoys a
√"
ABSTRACT,0.004054054054054054,"K-regret upper bound where K is the
number of episodes."
INTRODUCTION,0.005405405405405406,"1
INTRODUCTION"
INTRODUCTION,0.006756756756756757,"In competitive reinforcement learning, there are two agents competing against each other by taking
actions. Their actions together determine the state evolutions and rewards. Function approximation,
especially deep neural networks (LeCun et al., 2015), contributes to the success of RL in many real
world applications, such as Atari (Mnih et al., 2013), Go (Silver et al., 2015), autonomous driving
(Shalev-Shwartz et al., 2016), Texas holdem poker (Sandholm, 2017), and Dota (Berner et al., 2019)."
INTRODUCTION,0.008108108108108109,"The goal of competitive reinforcement learning aims to learn the Nash Equilibrium (NE) policy in a
trial-and-error fashion. In a NE, no agent can be better off by unilaterally deviating from her policy.
Most of the existing sample efficient competitive RL algorithms focus on the tabular or linear function
approximation cases (Pérolat et al., 2017; Bai & Jin., 2020; Bai et al., 2021; 2020; Xie et al., 2020;
Zhang et al., 2020). The huge empirical success of neural network based RL methods has remained
largely unexplained. Thus, there exists a wide gap between theory and practice in competitive RL
with general function approximation. One demanding question to ask is:"
INTRODUCTION,0.00945945945945946,Can we establish provably efficient competitive RL algorithms with general function approximation?
INTRODUCTION,0.010810810810810811,Published as a conference paper at ICLR 2022
INTRODUCTION,0.012162162162162163,"In this paper, we make a step towards answering this question by providing structural conditions
and complexity measures of Markov Games and function classes that allow for efficient learning.
We focus on episodic zero-sum Markov Game (MG) with simultaneous move, where each episode
consists of H time steps and two players act simultaneously at each time step. The state space is
arbitrarily large and can be infinite. We consider two function approximation settings: 1) use a
general function class F to approximate the action-value function (Q function); 2) use a general
function class M to approximate the environment."
INTRODUCTION,0.013513513513513514,"To this end, we introduce algorithms based on optimistic principles. Due to subtle game-theoretic
issues, naive optimism is no longer working in Markov games where minimax optimization is per-
formed. To deal with this issue, we use the algorithmic idea called ‘alternate optimism’. Specifically,
we develop algorithms for both coordinated and decoupled setting. In the decoupled setting, the agent
controls one player and plays against an arbitrary and potentially adversarial opponent. The sample
efficiency is measured by the gap between the learned value and value of the Nash Equilibrium. In
the coordinated setting, the agent controls both players and the goal is to find the approximate Nash
Equilibrium, i.e. with small duality gap. We identify key complexity measures of function classes to
examine the effectiveness of elimination. By doing so, we prove upper bounds on sample complexity
and regrets of the presented procedures that are independent of the number of states."
INTRODUCTION,0.014864864864864866,Our contributions are summarized into the following two folds:
INTRODUCTION,0.016216216216216217,"• In the decoupled setting, we introduce Minimax Eluder dimension–a new complexity
measure for competitive RL problems. We propose an algorithm that incurs at most
eO(H√dEK log NF) 1 regret in K episodes where dE denotes the Minimax Eluder di-
mension NF denotes the covering number of function class, with probability at least 1 −p.
As a special case, this result improves Xie et al. (2020) by a
√"
INTRODUCTION,0.01756756756756757,"d multiplicative factor in their
setting when the reward function and transition kernel are linearly parameterized and d is
the dimension of feature mapping."
INTRODUCTION,0.01891891891891892,"• In coordinated settings, we propose both model-based and model free algorithms. In the
model-based setting, we generalize the witness rank (Sun et al., 2019a) to competitive
RL. We prove that eO(H3W 2/ϵ2) samples are enough to learn a policy ϵ-close to the
Nash Equilibrium, where W is witness rank. In the model-free setting, we develop algo-
rithm for agnostic learning with a candidate policy class Π. The algorithm incurs at most
eO(H
p"
INTRODUCTION,0.02027027027027027,"dK log(NFNΠ)) regret. Here d is a variant of Minimax Eluder dimension and NΠ
denotes the covering number of policy class."
RELATED WORKS,0.021621621621621623,"1.1
RELATED WORKS"
RELATED WORKS,0.022972972972972974,"There is a rich literature studying the learning and decision-making of Markov Games (Littman &
Szepesvari, 1996; Greenwald et al., 2003; Grau-Moya et al., 2018; Pérolat et al., 2018; Srinivasan
et al., 2018; Sidford et al., 2020; Wei et al., 2017; Pérolat et al., 2017; Bai & Jin., 2020; Bai et al.,
2021; 2020; Zhang et al., 2020; Zhao et al., 2021). The most related to us are perhaps (Xie et al.,
2020; Chen et al., 2021; Jin et al., 2021b), where the authors address the challenge of exploration-
exploitation tradeoff in large state spaces. Due to space constraint, a detailed literature discussion is
deferred to Appendix A."
PRELIMINARIES,0.024324324324324326,"2
PRELIMINARIES"
PRELIMINARIES,0.025675675675675677,"We consider two-player zero-sum simultaneous-moves episodic Markov game, defined by the tuple
(S, A1, A2, r, P, H), where S is the state space, Ai is a finite set of actions that player i ∈{1, 2} can
take, r is the reward function, P is the transition kernel and H is the number of time steps. At each
time step h ∈[H], player P1 and P2 take actions a ∈A1 and b ∈A2 respectively upon observing
the state x ∈S, and then both receive the reward rh(x, a, b). The system then transitions to a new
state x′ ∼Ph(·|x, a, b) according to the transition kernel P. Throughout this paper, we assume for
simplicity that A1 = A2 = A and that the rewards rh(x, a, b) are deterministic functions of the tuple
(x, a, b) taking values in [−1, 1]. Turn-based games are special cases of simultaneous games in the"
PRELIMINARIES,0.02702702702702703,"1We use eO to hide logarithmic terms in H, 1/p and K."
PRELIMINARIES,0.02837837837837838,Published as a conference paper at ICLR 2022
PRELIMINARIES,0.02972972972972973,"sense that at each state the reward and transition are independent of one player’s action (Xie et al.,
2020). Generalizations to A1 ̸= A2 and stochastic reward are also straightforward."
PRELIMINARIES,0.031081081081081083,"Denote by ∆:= ∆(A) the probability simplex over the action space A. A stochastic policy of P1 is
a length-H sequence of functions π := {πh : S 7→∆}h∈[H]. At each step h ∈[H] and state x ∈S,
P1 takes an action sampled from the distribution πh(x) over A. Similarly, a stochastic policy of P2
is given by the sequence ν := {νh : S 7→∆}h∈[H]. The learning happens in K episodes. In each
episode k, each player P1 and P2 proposes a policy πk and νk respectively based on history up to the
end of episode k −1 and then executes the policy to observe the trajectories {xk
h, ak
h, bk
h}H
h=1."
PRELIMINARIES,0.032432432432432434,"For a fixed policy pair (π, ν), the value function and Q functions for zero-sum game is defined by"
PRELIMINARIES,0.033783783783783786,"V π,ν
h0 (x) := E ""
H
X"
PRELIMINARIES,0.03513513513513514,"h=h0
rh(xh, ah, bh)|xh0 = x # ,"
PRELIMINARIES,0.03648648648648649,"Qπ,ν
h0 (x, a, b) := E ""
H
X"
PRELIMINARIES,0.03783783783783784,"h=h0
rh(xh, ah, bh)|xh0 = x, ah0 = a, bh0 = b # ,"
PRELIMINARIES,0.03918918918918919,"where the expectation is over ah ∼πh(xh), bh ∼νh(xh) and xh+1 ∼Ph(·|xh, ah, bh). V π,ν
1
and
Qπ,ν
1
are often abbreviated to V π,ν and Qπ,ν. In zero-sum games, PI aims to maximize V π,ν(x1)
and P2 aims to minimize V π,ν(x1). Given policy π of P1, the best response policy of P2 is defined
by ν∗
π := argminνV π,ν(x1). Similarly given policy ν of P2, the best response policy of P1 is defined
by π∗
ν := argmaxπV π,ν(x1). Then from definitions,"
PRELIMINARIES,0.04054054054054054,"V π,ν∗
π(x1) ≤V π∗,ν∗(x1) ≤V π∗
ν,ν(x1)"
PRELIMINARIES,0.041891891891891894,"and the equality holds for the Nash Equilibrium (NE) of the game (π∗, ν∗). We abbreviate V π∗,ν∗(x1)
as V ∗and Qπ∗,ν∗(x1) as Q∗. V ∗is often referred to as the value of the game. As common in Markov
games literature (Pérolat et al., 2015), we will use the following Bellman operator
ThQh+1(x, a, b) := r(x, a, b) +
E
x′∼Ph(·|x,a,b)[max
π′ min
ν′ Eπ′,ν′Qh+1(x′, ·, ·)],
(1)"
PRELIMINARIES,0.043243243243243246,"and the following Bellman operator for fixed P1’s policy
T π
h Qh+1(x, a, b) := r(x, a, b) +
E
x′∼Ph(·|x,a,b)[min
ν′ Eπ,ν′Qh+1(x′, ·, ·)].
(2)"
PRELIMINARIES,0.0445945945945946,"Notations
For a integer H, [H] denotes the set {1, 2, . . . , H}. For a finite set S, |S| denotes its
cardinality. For a matrix A ∈Rd, Ai,∗and A∗,j denote the i-th row and j-th column of A respectively.
For a function f : S 7→R, ∥f∥∞denotes sups∈S |f(s)|. We use N(F, ϵ) to denote the ϵ-covering
number of F under metric d(f, g) = maxh ∥fh −gh∥∞."
DECOUPLED SETTING,0.04594594594594595,"3
DECOUPLED SETTING"
DECOUPLED SETTING,0.0472972972972973,"In decoupled setting, the algorithm controls P1 to play against P2. In k-th episode, P1 chooses a
policy πk and P2 chooses a policy νk based on πk. Then P1 and P2 interact in the Markov Game by
playing πk and νk respectively, and observe the rewards and states in this episode. Notice that P2
can be adversarial and its policy is never revealed, meaning that P1 can only maximize its reward by
playing π∗. The goal is to thus minimize the following ‘value regret’"
DECOUPLED SETTING,0.04864864864864865,"Reg(K) = K
X"
DECOUPLED SETTING,0.05,"k=1
[V ∗
1 (x1) −V πk,νk
1
(x1)].
(3)"
DECOUPLED SETTING,0.051351351351351354,"A small value regret indicates that we learn the value of MG. If an algorithm obtains an upper bound
on value regret that scales sublinearly with K for all νk, then it can not be exploited by opponents."
FUNCTION APPROXIMATION,0.052702702702702706,"3.1
FUNCTION APPROXIMATION"
FUNCTION APPROXIMATION,0.05405405405405406,"We consider function class F = F1 × · · · × FH where Fh ⊂{f : S × A1 × A2 7→[0, 1]} to provide
function approximations for Q∗— the Q-function of Nash Equilibrium. For a policy pair (π, ν) and
function f we abuse the notations and use f(x, π, ν) = Ea∼π,b∼ν[f(x, a, b)] to denote the expected
reward of executing (π, ν) in function f. We make the following assumptions about F."
FUNCTION APPROXIMATION,0.05540540540540541,Published as a conference paper at ICLR 2022
FUNCTION APPROXIMATION,0.05675675675675676,"Assumption 3.1 (Realizability). We assume Q∗
h ∈Fh for all h ∈[H].
Assumption 3.2 (Completeness). We assume for all h ∈[H] and f ∈Fh+1, Thf ∈Fh."
FUNCTION APPROXIMATION,0.05810810810810811,"Realizability means the function class contains the target Q function. Completeness says the function
class is closed under the Bellman operator. These assumptions are common in value function
approximation literature (e.g. Jin et al. (2020); Wang et al. (2020b); Jin et al. (2021a)). Note that they
are weaker than Xie et al. (2020), as their setting satisfies Thf ∈Fh for any action-value function f."
FUNCTION APPROXIMATION,0.05945945945945946,"For a function f ∈F, we denote the max-min policy of P1 by πf. Specifically,"
FUNCTION APPROXIMATION,0.060810810810810814,"(πf)h(x) := argmaxπmin
ν
fh(x, π, ν), ∀h ∈[H]."
FUNCTION APPROXIMATION,0.062162162162162166,"Similarly we denote the min-max policy of P2 by νf. Given policy π of P1, the best response policy
of P2 in terms of function f is defined by"
FUNCTION APPROXIMATION,0.06351351351351352,"(νf
π)h(x) := argminνfh(x, πh, ν), ∀h ∈[H]."
FUNCTION APPROXIMATION,0.06486486486486487,"Similarly given policy ν of P2, the best response policy of P1 in terms of function f is denoted by πf
ν .
When there is no confusion, we will drop the subscript h for simplicity."
FUNCTION APPROXIMATION,0.06621621621621622,"Learnability requires the function class to have bounded complexity. We introduce Minimax Eluder
dimension to capture the structural complexity of function classes in zero-sum games. To define,
we use ϵ-independence of distributions and Distributional Eluder dimension (DE) from Russo &
Van Roy (2013); Jin et al. (2021a).
Definition 3.3 (ϵ-independence of distributions). Let G be a function class defined on X, and
ν, µ1, . . . , µn be probability measures over X. We say that ν is ϵ-independent of {µ1, . . . , µn} with
respect to G if there exists g ∈G such that
pPn
i=1(Eµi[g])2 ≤ϵ but | Eν[g]| ≥ϵ.
Definition 3.4 (Distributional Eluder dimension (DE)). Let G be a function class defined on X, and
D be a family of probability measures over X. The distributional Eluder dimension dimDE(G, D, ϵ)
is the length of the longest sequence {ρ1, . . . , ρn} ⊂D such that there exists ϵ′ ≥ϵ where ρi is
ϵ′-independent of {ρ1, . . . , ρi−1} for all i ∈[n]."
FUNCTION APPROXIMATION,0.06756756756756757,Now we introduce Minimax Eluder dimension. Recall the minimax Bellman operator from Eq (1):
FUNCTION APPROXIMATION,0.06891891891891892,"Thfh+1(x, a, b) := r(x, a, b) +
E
x′∼Ph(·|x,a,b)[max
π′ min
ν′ fh+1(x′, π′, ν′)]"
FUNCTION APPROXIMATION,0.07027027027027027,"thus Minimax Eluder dimension is the Eluder dimension on the Bellman residues with respect to the
above minimax Bellman operator.
Definition 3.5 (Minimax Eluder dimension). Let (I −Th)F := {fh −Thfh+1 : f ∈F} be the class
of minimax residuals induced by function class F at level h and D∆= {D∆,h}h∈[H] where D∆,h :=
{δ(x, a, b) : x ∈S, a ∈A1, b ∈A2} is the set of Dirac measures on state-action pair (x, a, b). The ϵ-
Minimax Eluder dimension of F is defined by dimME(F, ϵ) := maxh∈[H] dimDE((I −Th)F, D∆, ϵ)."
OPTIMISTIC NASH ELIMINATION FOR MARKOV GAMES,0.07162162162162163,"3.2
OPTIMISTIC NASH ELIMINATION FOR MARKOV GAMES"
OPTIMISTIC NASH ELIMINATION FOR MARKOV GAMES,0.07297297297297298,"Now we introduce Optimistic Nash Elimination for Markov Games (ONEMG), presented in Algo-
rithm 1. This algorithm maintains a confidence set Vk that always contains the Q∗, and sequentially
eliminates inaccurate hypothesis from it. In k-th episode, P1 first optimally chooses a value function
f k in Vk−1 that maximizes the value of the game. Intuitively, this step performs optimistic planning
in the pessimistic scenarios, i.e. assuming P2 plays the best response. Next, it plays against P2 and
augments the data from this episode into replay buffer B. The confidence set Vk is then updated by
keeping the functions f with small minimax Bellman errors fh −Thfh+1, in Line 12. To estimate
fh −Thfh+1, we use EBh(fh, fh+1) −infg∈Fh EBh(g, fh+1), a standard variance reduction trick
to avoid the double-sample issue. It can be shown that when the value function class is complete,
EBh(fh, fh+1) −infg∈Fh EBh(g, fh+1) is an unbiased estimator of fh −Thfh+1 (Lemma B.1)."
OPTIMISTIC NASH ELIMINATION FOR MARKOV GAMES,0.07432432432432433,"Notice that unlike many previous works that add optimistic bonus on every state-action pairs (Xie
et al., 2020; Chen et al., 2021), Algorithm 1 only takes optimism in the initial state. Instead, the
constraint set contains every function that has low Bellman residue on all trajectories in the replay
buffer. This ‘global optimism’ technique trades computational efficiency for better sample complexity,
and can be found in many previous work in Markov Decision process (Zanette et al., 2020; Jin et al.,
2021a). As we will see later, it is also useful in the coordinated setting."
OPTIMISTIC NASH ELIMINATION FOR MARKOV GAMES,0.07567567567567568,Published as a conference paper at ICLR 2022
OPTIMISTIC NASH ELIMINATION FOR MARKOV GAMES,0.07702702702702703,Algorithm 1 Optimistic Nash Elimination for Markov Games (ONEMG)
OPTIMISTIC NASH ELIMINATION FOR MARKOV GAMES,0.07837837837837838,"1: Input: Function class F
2: Initialize Bh = ∅, h ∈[H], V0 ←F
3: Set β ←C log(N(F, 1/K) · HK/p) for some large constant C
4: for k = 1, 2, . . . , K do
5:
Compute f k ←argmaxf∈Vk−1f1(x1, πf, νf), let πk ←πf k
6:
for step h = 1, 2, . . . , H do
7:
Interact with environment by playing action ak
h ∼πk
h(xk
h)
8:
Observe reward rk
h, opponent’s action bk
h and move to next state xk
h+1
9:
end for
10:
Update Bh ←Bh ∪{(xk
h, ak
h, bk
h, rk
h, xk
h+1)}, ∀h ∈[H]
11:
For all ξ, ζ ∈F and h ∈[H], let"
OPTIMISTIC NASH ELIMINATION FOR MARKOV GAMES,0.07972972972972973,"EBh(ξh, ζh+1) = k
X"
OPTIMISTIC NASH ELIMINATION FOR MARKOV GAMES,0.08108108108108109,"τ=1
[ξh(xτ
h, aτ
h, bτ
h) −rτ
h −ζh+1(xτ
h+1, πζh+1, νζh+1)]2"
OPTIMISTIC NASH ELIMINATION FOR MARKOV GAMES,0.08243243243243244,"12:
Set
Vk ←{f ∈F : EBh(fh, fh+1) ≤inf
g∈Fh EBh(g, fh+1) + β, ∀h ∈[H]}"
OPTIMISTIC NASH ELIMINATION FOR MARKOV GAMES,0.08378378378378379,13: end for
THEORETICAL RESULTS,0.08513513513513514,"3.3
THEORETICAL RESULTS"
THEORETICAL RESULTS,0.08648648648648649,"In this section we present the theoretical guarantee for Algorithm 1. The proof is deferred to
Appendix B.
Theorem 3.6. Under Assumption 3.1 and Assumption 3.2, the regret Eq (3) of Algorithm 1 is upper
bounded by"
THEORETICAL RESULTS,0.08783783783783784,"Reg(K) ≤O

H
p"
THEORETICAL RESULTS,0.0891891891891892,"K · dME · log(HKζ)
"
THEORETICAL RESULTS,0.09054054054054055,"with probability at least 1 −p. Here dME = dimME(F,
p"
THEORETICAL RESULTS,0.0918918918918919,"1/K) and ζ = N(F, 1/K)/p."
THEORETICAL RESULTS,0.09324324324324325,"As this theorem suggests, as long as the function class has finite Minimax Eluder dimension and
covering number, P1 can achieve
√"
THEORETICAL RESULTS,0.0945945945945946,"K-regret against P2. Notice that when P2 always plays the best
response of P1’s policies, this regret guarantee indicates that algorithm eventually learns the Nash
Equilibrium. In this case, the algorithm takes eO(H2d log(|F|)/ϵ2) samples to learn a policy π that is
ϵ-close to π∗."
THEORETICAL RESULTS,0.09594594594594595,"When the opponent plays dummy actions, the Markov Game can be seen as an MDP and the value
of the Markov Game is the maximum cumulative sum of rewards one can achieve in this MDP. In
this case, our result reduces to eO(H ·
p"
THEORETICAL RESULTS,0.0972972972972973,"K · dBE log(N(F, 1/K))) where dBE is the Bellman eluder
dimension in Jin et al. (2021a)."
THEORETICAL RESULTS,0.09864864864864865,"In particular, when the function class is finite, the regret becomes O(H
p"
THEORETICAL RESULTS,0.1,"KdME log(HK|F|/p))
that depends logarithmically on the cardinality of F. Moreover, when the reward and transition
kernel have linear structures, i.e. rh(x, a, b) = ϕ(x, a, b)⊤θh and Ph(·|x, a, b) = ϕ(x, a, b)⊤µh(·)
where ϕ(x, a, b) ∈Rd, then dME(F,
p"
THEORETICAL RESULTS,0.10135135135135136,"1/K) = log N(F, 1/K) = eO(d) and the regret becomes
eO
 
Hd
√"
THEORETICAL RESULTS,0.10270270270270271,"K
 2. Thus we improve Xie et al. (2020) by a
√"
THEORETICAL RESULTS,0.10405405405405406,"d factor. We also provide a simpler algorithm
tailored for this setting, see Appendix B.1 for details."
THEORETICAL RESULTS,0.10540540540540541,"Algorithm 1 solves a non-convex optimization problem in Line 5 with highly non-convex constraint
set (Line 12). In general, it is computationally inefficient. Even when reduced to linear MDP,
i.e. the opponent plays dummy actions and the transition matrices and the rewards have low rank
structures, it is not known if the same eO(H
√"
THEORETICAL RESULTS,0.10675675675675676,"d2K) regret can be achieved with computationally
efficient algorithms (Zanette et al., 2020). Designing computationally efficient algorithms with
near-optimal sample efficiency is an interesting further direction."
THEORETICAL RESULTS,0.10810810810810811,"2Here we use eO to omit the logarithm factors in K, d, H and 1/p."
THEORETICAL RESULTS,0.10945945945945947,Published as a conference paper at ICLR 2022
COORDINATED SETTING,0.11081081081081082,"4
COORDINATED SETTING"
COORDINATED SETTING,0.11216216216216217,"In the coordinated setting, the agent can control both P1 and P2. The goal is to find the ϵ-approximate
Nash equilibrium (π, ν) in the sense that"
COORDINATED SETTING,0.11351351351351352,"V π∗
ν,ν(x1) −V π,ν∗
π(x1) ≤ϵ
(4)"
COORDINATED SETTING,0.11486486486486487,"by playing P1 and P2 against each other in the Markov game. In the following sections, we propose
both model-based and model-free methods to deal with this problem."
MODEL-BASED ALGORITHM,0.11621621621621622,"4.1
MODEL-BASED ALGORITHM"
MODEL-BASED ALGORITHM,0.11756756756756757,"In model-based methods, we make use of a model class M of candidate models to approximate
the true model M ∗. We are also given a test function class G to track model misfit. Given a
model M ∈M, we use QM(x, a, b), rM(x, a, b) and PM(·|x, a, b) to denote the Q-function, reward
function and transition kernel of executing action a and b at state x in model M. For simplicity, we
use (r, x) ∼M to represent r ∼rM(x, a, b) and x ∼PM(·|x, a, b). We denote the NE policy in
model M as πM and νM. For policy π, ν and model M, we use νM
π to denote the best response of π
in model M and πM
ν is defined similarly."
MODEL-BASED ALGORITHM,0.11891891891891893,"4.1.1
ALTERNATE OPTIMISTIC MODEL ELIMINATION (AOME)"
MODEL-BASED ALGORITHM,0.12027027027027028,"The model-based method, Alternate Optimistic Model Elimination (AOME), is presented in Algo-
rithm 4. It maintains a constraint set Mk of candidate models. Throughout K iterations, AOME
makes sure that the true model M ∗always belongs Mk for all k ∈[K], and sequentially eliminates
incorrect models from Mk."
MODEL-BASED ALGORITHM,0.12162162162162163,"However, the idea of being optimistic only at initial state (Zanette et al., 2020; Jin et al., 2021a)
is not directly applicable to this scenario. Since the objective to be optimized is the duality gap,
the target policies being considered are thus policies pairs (π∗
ν, ν) and (π, ν∗
π). However, π∗
ν and
ν∗
π are not available to the agents. Therefore, the performances of proposed policies are evaluated
on out-of-distribution trajectories, meaning that optimism can not be guaranteed. This causes the
distribution shift issue. In worst cases, low Bellman errors in the collected trajectories provide no
information for the trajectories collected by π∗
ν, ν and π, ν∗
π."
MODEL-BASED ALGORITHM,0.12297297297297298,"To address this issue, the algorithm performs global planning by applying the idea of alternate
optimism to model-based methods. In k-th episode, the algorithm first solves the optimization
problem M k
1 = arg maxM∈M QM(x1, πM, νM) and let πk = πM k
1 . This optimization problem
corresponds to finding the optimistic estimate of the value of the game. Indeed, notice that M ∗∈Mk"
MODEL-BASED ALGORITHM,0.12432432432432433,"implies QM k
1 (x1, πk, νk) ≥QM k
1 (x1, πk, νM k
1
πk ) ≥V ∗(x1), by optimality of M k
1 . Next, it solves"
MODEL-BASED ALGORITHM,0.12567567567567567,"the second optimization problem M k
2 = arg minM∈M QM(x1, πM k
1 , νM) and let νk ←νM k
2
πk . This"
MODEL-BASED ALGORITHM,0.12702702702702703,"corresponds to finding the pessimistic estimate of V πk,ν∗
πk (x1). In fact, we see that M ∗∈Mk"
MODEL-BASED ALGORITHM,0.12837837837837837,"implies QM k
2 (x1, πk, νk) ≤V πk,ν∗
πk (x1), by optimality of M k
2 . This approach appears in Wei et al.
(2017) to guarantee convergence to Nash equilibrium, where it is referred to as ‘Maximin-EVI’."
MODEL-BASED ALGORITHM,0.12972972972972974,"In Line 8, the algorithm checks if the values of model M k
1 and M k
2 are close to the value of the true
model M ∗when executing policy πk and νk. If the condition holds, we have"
MODEL-BASED ALGORITHM,0.13108108108108107,"V ∗(x1) −V πk,ν∗
πk (x1) ≤QM k
1 (x1, πk, νk) −QM k
2 (x1, πk, νk) ≤ϵ,"
MODEL-BASED ALGORITHM,0.13243243243243244,"which means that AOME finds policy πk that is ϵ-close the NE. One can also switch the order of
alternate optimism and obtain νk that is ϵ-close the NE. We then terminate the process and output πk
and νk. If the algorithm does not terminate in Line 8, it applies the witnessed model misfit checking
method in Sun et al. (2019a). It starts by computing the empirical Bellman error of Markov games
defined as follows"
MODEL-BASED ALGORITHM,0.13378378378378378,"bL(M1, M2, M, h) := n1
X i=1"
MODEL-BASED ALGORITHM,0.13513513513513514,"1
n1
[QM
h (xi
h, ai
h, bi
h) −(ri
h + QM
h (xi
h+1, πM1, νM2
πM1)].
(5)"
MODEL-BASED ALGORITHM,0.13648648648648648,Published as a conference paper at ICLR 2022
MODEL-BASED ALGORITHM,0.13783783783783785,"As proved in Appendix D, in this case bL(M k
1 , M k
2 , M k
j , hk) must be greater than ϵ/(8H) for some
j ∈[2] and hk ∈[H]. Then the algorithm samples additional trajectories at level hk and shrinks the
constraint set by eliminating models with large empirical model misfit, which is defined as follow"
MODEL-BASED ALGORITHM,0.13918918918918918,"bE(M k
1 , M k
2 , M, hk) = sup
g∈G n
X i=1"
"N
E",0.14054054054054055,"1
n
E
(rh,xh)∼M[g(xi
h, ai
h, bi
h, rh, xh) −g(xi
h, ai
h, bi
h, ri
h, xi
h+1)]. (6)"
"N
E",0.14189189189189189,"We will show that this constraint set maintains more and more accurate models of the environment.
Due to space constraints, the algorithm and theory of our model-based method are deferred to
Appendix C."
MODEL FREE ALGORITHM,0.14324324324324325,"4.2
MODEL FREE ALGORITHM"
MODEL FREE ALGORITHM,0.1445945945945946,"In this section we propose a model-free algorithm. We consider a more general agnostic learning,
where The algorithm is given a policy class Π = Π1 × · · · ΠH with Πh ⊂{πh : S 7→∆}, h ∈
[H]. Notice by letting Π to be the class of NE policies induced by the action-value function
class F, we recover the original setup. The goal is to find policy π and ν from Π to minimize
the duality gap: maxπ′∈Π V π′,ν −minν′∈Π V π,ν′, by playing only policies from Π. Since only
policies from Π are considered, we overload the notation and define the optimal solution in Π as
π∗= arg maxπ∈Π minν′∈Π V π,ν′ and ν∗= arg minν∈Π maxπ′∈Π V π′,ν. When Π contains all
possible policies, π∗and ν∗is then the Nash equilibrium."
MODEL FREE ALGORITHM,0.14594594594594595,"Similar to Section 3, we consider general function class F = F1 × · · · × FH where Fh ⊂
{f : S × A1 × A2 7→[0, 1]}.
We overload the notations in Section 3 and use νf
π to de-
note the best response of π from Π, namely νf
π(x) := argminν∈Πf(x, π, ν). Similarly, we use
πf(x) := argmaxπ∈Πminν∈Π f(x, π, ν). We also use ν∗
π(x) := argminν∈ΠV π,ν to denote the best
response of π in the true model."
MODEL FREE ALGORITHM,0.1472972972972973,"4.2.1
ALTERNATE OPTIMISTIC VALUE ELIMINATION (AOVE)"
MODEL FREE ALGORITHM,0.14864864864864866,Notice that the duality gap can be decomposed as follows
MODEL FREE ALGORITHM,0.15,"gap = V π∗
ν,ν −V π∗
ν∗,ν∗
|
{z
}
P2
+ V π∗
ν∗,ν∗−V π∗,ν∗
π∗
|
{z
}
Optimal
+ V π∗,ν∗
π∗−V π,ν∗
π
|
{z
}
P1"
MODEL FREE ALGORITHM,0.15135135135135136,"where the optimal part V π∗
ν∗,ν∗−V π∗,ν∗
π∗is fixed. Therefore we can consider only the P1 part here
and the P2 follows by symmetry. We are interested in the following policy regret"
MODEL FREE ALGORITHM,0.1527027027027027,"Reg(K) := K
X"
MODEL FREE ALGORITHM,0.15405405405405406,"k=1
(V π∗,ν∗
π∗−V πk,ν∗
πk ).
(7)"
MODEL FREE ALGORITHM,0.1554054054054054,"Policy regret measures the extent that P1 can be exploited by policies in Π. Our algorithm Alternate
Optimistic Value Elimination (AOVE), presented in Algorithm 2, works on minimizing this regret.
Similar to Algorithm 1, it also uses optimism in the initial state to address exploration and exploitation
trade-off and constructs a series of confidence sets of hypotheses with small Bellman errors. The
differences primarily lie in a different choice of optimism and a different series of confidence sets
constructed with the Bellman operator in Eq 2. Since the policy class Π is taken into consideration,
the algorithm maintains a constraint set B ⊂Π × F of policy-function pairs. We use the Bellman
operator with regard to fixed P1’s policy, in Eq (2). Thus intuitively, this constraint set maintains
(π, f) pairs such that fh −T π
h fh+1 is small for all h ∈[H]."
MODEL FREE ALGORITHM,0.15675675675675677,"We apply the ‘alternate optimistic’ principle, presented in Line 5 and Line 6. Intuitively, the algorithm
finds an optimistic planning of π and pessimistic planning of ν, together corresponding to a max-min
procedure. As such, we form an upper bound f1(x1, π, νf
π) −g1(x1, πk, νg
πk) of the duality gap.
Note that it is different from the ‘alternate optimistic’ used in model-based settings Wei et al. (2017)
in that the hypothesis chosen in Line 6 is constrained by the policy chosen in Line 5. This is because
in model-based methods, ‘plausible’ models in the confidence set guarantee small simulation errors
(Lemma C.6). However, this is not true for ‘plausible’ value functions in the confidence set."
MODEL FREE ALGORITHM,0.1581081081081081,"The rest of the algorithm aims at minimizing this upper bound sequentially by eliminating bad
hypothesis in the constraint set.
The elimination process occurs in Line 13, where the algo-
rithm uses history data to eliminate functions with large Bellman error. To estimate Bellman"
MODEL FREE ALGORITHM,0.15945945945945947,Published as a conference paper at ICLR 2022
MODEL FREE ALGORITHM,0.1608108108108108,"error, we use the similar method as in Algorithm 1, i.e. subtracting the variance of T π
h fh+1 by
g = arg infg∈Fh EDh(g, fh+1, π). Notice that a smaller value of fh −T π
h fh+1 indicates that function
approximation f is not easily exploited by P2 when P1 plays π."
MODEL FREE ALGORITHM,0.16216216216216217,"As shown in Appendix D, (π∗, Q∗) is always in the constraint set, and the Bellman errors of the
rest hypothesis keep shrinking. Thus in the presented algorithm of AOVE, the regret of P1 part
V π∗,ν∗
π∗−V π,ν∗
π is controlled. By symmetry, we can perform the same procedures on P2 part and
obtain an upper bound on V π∗
ν,ν −V π∗
ν∗,ν∗. Combining these together, we show sublinear regret rate
of V π∗
ν∗,ν∗−V π∗,ν∗
π∗. By regret to PAC conversion, this means that we find the policies that is the
best achievable in Π."
MODEL FREE ALGORITHM,0.1635135135135135,Algorithm 2 Alternate Optimistic Value Elimination (AOVE)
MODEL FREE ALGORITHM,0.16486486486486487,"1: Input: Function class F, policy class Π
2: Initialize B0 ←Π × F
3: Set β ←C log[N(F, 1/K) · N(Π, 1/K) · HK/p] for some large constant C
4: for k = 1, 2, . . . , K do
5:
Find (πk, f k) ←argmax(π,f)∈Vk−1f1(x1, π, νf
π)"
MODEL FREE ALGORITHM,0.1662162162162162,"6:
Let g ←
argmin
g:(πk,g)∈Vk−1g1(x1, πk, νg
πk) and set νk ←νg
πk"
MODEL FREE ALGORITHM,0.16756756756756758,"7:
for step h = 1, 2, . . . , H do
8:
P1 takes action ak
h ∼πk
h(xk
h), P2 takes action bk
h ∼νk
h(xk
h)
9:
Observe reward rk
h and move to next state xk
h+1
10:
end for
11:
Update Bh ←Bh ∪{(xk
h, ak
h, bk
h, rk
h, xk
h+1)}, ∀h ∈[H]
12:
For all ξ, ζ ∈F, π ∈Π and h ∈[H], let"
MODEL FREE ALGORITHM,0.16891891891891891,"EBh(ξh, ζh+1, π) = k
X"
MODEL FREE ALGORITHM,0.17027027027027028,"τ=1
[ξh(xτ
h, aτ
h, bτ
h) −rτ
h −ζh+1(xτ
h+1, π, νζh+1
π
)]2"
MODEL FREE ALGORITHM,0.17162162162162162,"13:
Update"
MODEL FREE ALGORITHM,0.17297297297297298,"Vk ←{(π, f) ∈Π × F : EBh(fh, fh+1, π) ≤inf
g∈Fh EBh(g, fh+1, π) + β, ∀h ∈[H]}"
MODEL FREE ALGORITHM,0.17432432432432432,14: end for
THEORETICAL RESULTS,0.17567567567567569,"4.2.2
THEORETICAL RESULTS"
THEORETICAL RESULTS,0.17702702702702702,"This section presents our theoretical guarantees of Algorithm 2. First, we introduce two assumptions
that take the policy class Π into consideration. Variants of these assumptions are also common in
batch RL (Jin et al., 2021c; Xie et al., 2021).
Assumption 4.1 (Π-realizability). For all π ∈Π, Qπ,ν∗
π ∈F.
Assumption 4.2 (Π-completeness). For all h ∈[H], π ∈Π, and f ∈Fh+1, T π
h f ∈Fh holds."
THEORETICAL RESULTS,0.1783783783783784,"Notice that Assumption 4.2 reduces to Q∗realizability in MDP. Both two assumptions hold for
linear-parameterized Markov games studied in Xie et al. (2020). In fact, Xie et al. (2020) satisfies a
stronger property: for any policy pair π, ν and any h ∈[H], there exists a vector w ∈Rd such that"
THEORETICAL RESULTS,0.17972972972972973,"Qπ,ν
h (x, a, b) = w⊤ϕ(x, a, b), ∀(x, a, b) ∈S × A1 × A2."
THEORETICAL RESULTS,0.1810810810810811,"Now we present the theory of Algorithm 2. First, we introduce the complexity measure in coordinated
setting, which is a variant of Minimax Eluder dimension by replacing the Bellman operator Eq (1)
with Eq (2). This Minimax Eluder dimension also allows a distribution family induced by function
class F which is cheap to evaluate in large state space (Jin et al., 2021a)."
THEORETICAL RESULTS,0.18243243243243243,"Definition 4.3 (Minimax Eluder dimension in Coordinated setting). Let (I −T Πh+1
h
)F :=
{fh −T π
h fh+1 : f
∈F, π ∈Πh+1} be the class of residuals induced by the Bellman
operator T π
h .
Consider the following distribution families: 1) D∆= {D∆,h}h∈[H] where
D∆,h := {δ(x, a, b) : x ∈S, a ∈A1, b ∈A2} is the set of Dirac measures on state-action"
THEORETICAL RESULTS,0.1837837837837838,Published as a conference paper at ICLR 2022
THEORETICAL RESULTS,0.18513513513513513,"pair (x, a, b); 2) DF = {DF,h}h∈[H] which is generated by executing (πf, νg
πf ) for f, g ∈F.
The ϵ-Minimax Eluder dimension (in coordinated setting) of F is defined by dimME′(F, ϵ) :=
maxh∈[H] minD∈{D∆,DF} dimDE((I −T Πh+1
h
)F, D, ϵ)."
THEORETICAL RESULTS,0.1864864864864865,"Now we present our main theory of Algorithm 2, where we make use of a variant of Minimax Eluder
dimension that depends on policy class Π. We use N(Π, ϵ) to denote the covering number of Π under
metric d(π, π′) := maxh∈[H] maxf∈Fh,x∈S,b∈A |f(x, π, b) −f(x, π′, b)|.
Theorem 4.4. Suppose Assumption 4.1 and Assumption 4.2 holds. With probability at least 1 −p we
have the regret (Eq. (7)) of Algorithm 2 upper bounded by"
THEORETICAL RESULTS,0.18783783783783783,"Reg(K) ≤O

H
p"
THEORETICAL RESULTS,0.1891891891891892,"K · dME′ · log[HKζ]
"
THEORETICAL RESULTS,0.19054054054054054,"where dME′ = dimME′(F,
p"
THEORETICAL RESULTS,0.1918918918918919,"1/K) and ζ = N(F, 1/K)N(Π, 1/K)/p."
THEORETICAL RESULTS,0.19324324324324324,"As shown in this theorem, the regret is sub-linear in number of episodes K and Eluder dimen-
sion dME′ which match the regret bound in Markov decision process, e.g. in Jin et al. (2021a).
Through regret-to-PAC conversion, with high probability the algorithm can find ϵ-optimal policy
with O
 
H2KdME′ log[N(F, 1/K)N(Π, 1/K)HK/p]/ϵ2
sample complexity. When the reward
and transition kernel have linear structures, we also recover the eO(
√"
THEORETICAL RESULTS,0.1945945945945946,d3K) regret of Xie et al. (2020).
THEORETICAL RESULTS,0.19594594594594594,"Although our upper bound depends on the logarithm of covering number of Π, there are some cases
when it is small. For example, when the value-function class F is finite and Π is the induced policy
class of F as defined by Π = {πf, νg
πf : f, g ∈F}, then log NΠ(ϵ) = log |F| and the regret is"
THEORETICAL RESULTS,0.1972972972972973,"eO(H
q"
THEORETICAL RESULTS,0.19864864864864865,"KdME′ log2 |F|). Similarly, if the model class M is finite and Π is induced policy class"
THEORETICAL RESULTS,0.2,"of M as defined by Π = {πM, νM ′
πM : M, M ′ ∈M}, then log NΠ(ϵ) = log |M| and the regret"
THEORETICAL RESULTS,0.20135135135135135,"is eO(H
q"
THEORETICAL RESULTS,0.20270270270270271,"KdME′ log2 |M|). In agnostic setting which allows Π to have arbitrary structure, the
term log N(Π, 1/K) reflects the difficulty of learning with complex candidate policy sets and we
conjecture that this term is inevitable."
THEORETICAL RESULTS,0.20405405405405405,"Similar to Algorithm 1, Algorithm 2 solves a non-convex optimization problem in Line 5-6 with
highly non-convex constraint set (Line 13). This step is particularly difficult when the function
class has complex structures. Algorithm 2 is in general computationally inefficient, even in cases of
linear function approximations. Indeed, even when the model is known, solving for the NE is PPAD
complete. Developing decentralized and provably efficient RL algorithm for multi-agent Markov
game seems a challenging but interesting future direction."
DISCUSSION,0.20540540540540542,"5
DISCUSSION"
DISCUSSION,0.20675675675675675,"In this paper we study function approximations in zero-sum simultaneous move Markov games. We
design sample efficient algorithms for both decoupled and coordinated learning settings and propose
Minimax Eluder dimension to characterize the complexity of decision making with value function
approximation. The analysis shows
√"
DISCUSSION,0.20810810810810812,"T regret bounds and half-order dependence on the Minimax
Eluder dimension, matching the results in MDPs and improving the linear function approximations.
Our algorithms for coordinated setting are based on the idea of the ‘alternate optimism’, which also
shows applicability in model-based methods."
DISCUSSION,0.20945945945945946,ACKNOWLEDGEMENTS
DISCUSSION,0.21081081081081082,"BH is supported by the Elite Undergraduate Training Program of School of Mathematical Sciences
at Peking University. JDL acknowledges support of the ARO under MURI Award W911NF-11-1-
0304, the Sloan Research Fellowship, NSF CCF 2002272, NSF IIS 2107304, and an ONR Young
Investigator Award."
REFERENCES,0.21216216216216216,REFERENCES
REFERENCES,0.21351351351351353,"Yasin Abbasi-Yadkori, Nevena Lazic, Csaba Szepesvari, and Gellert Weisz. Exploration-enhanced
POLITEX. arXiv preprint arXiv:1908.10479, 2019."
REFERENCES,0.21486486486486486,Published as a conference paper at ICLR 2022
REFERENCES,0.21621621621621623,"Alekh Agarwal, Miroslav Dudík, Satyen Kale, John Langford, and Robert E. Schapire. Contextual
bandit learning with predictable rewards. In Proceedings of the 15th International Conference on
Artificial Intelligence and Statistics (AISTATS), 2012."
REFERENCES,0.21756756756756757,"Alekh Agarwal, Sham M. Kakade, Jason D. Lee, and Gaurav Mahajan. On the theory of policy gradi-
ent methods: Optimality, approximation, and distribution shift. arXiv preprint arXiv:1908.00261,
2020."
REFERENCES,0.21891891891891893,"Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning (ICML), pp. 242–252, 2019."
REFERENCES,0.22027027027027027,"Mohammad Gheshlaghi Azar, Ian Osband, and Rémi Munos. Minimax regret bounds for reinforce-
ment learning. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pp. 263–272. JMLR. org, 2017."
REFERENCES,0.22162162162162163,"Yu Bai and Chi Jin. Provable self-play algorithms for competitive reinforcement learning. arXiv
preprint arXiv:2002.04017, 2020."
REFERENCES,0.22297297297297297,"Yu Bai, Chi Jin, and Tiancheng Yu. Near-optimal reinforcement learning with self-play. arXiv
preprint arXiv:2006.12007, 2020."
REFERENCES,0.22432432432432434,"Yu Bai, Chi Jin, Huan Wang, and Caiming Xiong. Sample-efficient learning of stackelberg equilibria
in general-sum games. arXiv preprint arXiv:2102.11494, 2021."
REFERENCES,0.22567567567567567,"Leemon Baird. Residual algorithms: Reinforcement learning with function approximation. In
Proceedings of the 12th International Conference on Machine Learning, 1995."
REFERENCES,0.22702702702702704,"Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy
Dennison, David Farhi, Quirin Fischer, Shariq Hashme, and Chris Hesse. Dota 2 with large scale
deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019."
REFERENCES,0.22837837837837838,"Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and deep
neural networks. In Advances in neural information processing systems (NeurIPS), 2019."
REFERENCES,0.22972972972972974,"Zixiang Chen, Dongruo Zhou, and Quanquan Gu. Almost optimal algorithms for two-player markov
games with linear function approximation. arXiv preprint arXiv:2102.07404, 2021."
REFERENCES,0.23108108108108108,"Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning (ICML), pp.
1675–1685. PMLR, 2019a."
REFERENCES,0.23243243243243245,"Simon S. Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and John Langford.
Provably efficient RL with rich observations via latent state decoding. In International Conference
on Machine Learning, pp. 1665–1674. PMLR, 2019b."
REFERENCES,0.23378378378378378,"Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations
(ICLR), 2019c."
REFERENCES,0.23513513513513515,"Simon S. Du, Sham M. Kakade, Jason D. Lee, Shachar Lovett, Gaurav Mahajan, Wen Sun, and
Ruosong Wang. Bilinear classes: A structural framework for provable generalization in rl. arXiv
preprint arXiv:2103.10897, 2021."
REFERENCES,0.23648648648648649,"Jianqing Fan, Zhaoran Wang, Yuchen Xie, and Zhuoran Yang. A theoretical analysis of deep
q-learning. In Learning for Dynamics and Control, pp. 486–489. PMLR, 2020."
REFERENCES,0.23783783783783785,"Jordi Grau-Moya, Felix Leibfried, and Haitham Bou-Ammar. Balancing two-player stochastic games
with soft Q-learning. In Proceedings of the 27th International Joint Conference on Artificial
Intelligence, pp. 268–274, 2018."
REFERENCES,0.2391891891891892,"Amy Greenwald, Keith Hall, and Roberto Serrano. Correlated Q-learning. In International Conference
on Machine Learning, volume 20, pp. 242, 2003."
REFERENCES,0.24054054054054055,Published as a conference paper at ICLR 2022
REFERENCES,0.2418918918918919,"Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems (NeurIPS),
pp. 8571–8580, 2018."
REFERENCES,0.24324324324324326,"Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement
learning. In Journal of Machine Learning Research, volume 11, pp. 1563–1600, 2010."
REFERENCES,0.2445945945945946,"Zeyu Jia, Lin F. Yang, and Mengdi Wang. Feature-based Q-learning for two-player stochastic games.
arXiv preprint arXiv:1906.00423, 2019."
REFERENCES,0.24594594594594596,"Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E. Schapire. Contex-
tual decision processes with low bellman rank are pac-learnable. arXiv preprint arXiv:1610.09512,
2016."
REFERENCES,0.2472972972972973,"Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Con-
textual decision processes with low bellman rank are PAC-learnable. In Proceedings of the 34th
International Conference on Machine Learning-Volume 70, pp. 1704–1713. JMLR. org, 2017."
REFERENCES,0.24864864864864866,"Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I. Jordan. Is Q-learning provably
efficient? In Advances in Neural Information Processing Systems, pp. 4863–4873, 2018."
REFERENCES,0.25,"Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement
learning with linear function approximation. In Conference on Learning Theory, pp. 2137–2143.
PMLR, 2020."
REFERENCES,0.25135135135135134,"Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. Bellman eluder dimension: New rich classes of rl
problems, and sample-efficient algorithms. arXiv preprint arXiv:2102.00875, 2021a."
REFERENCES,0.2527027027027027,"Chi Jin, Qinghua Liu, and Tiancheng Yu. The power of exploiter: Provable multi-agent rl in large
state spaces. arXiv preprint arXiv:2106.03352, 2021b."
REFERENCES,0.25405405405405407,"Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline rl? In
International Conference on Machine Learning (ICML), 2021c."
REFERENCES,0.2554054054054054,"Michail G. Lagoudakis and Ronald Parr. Value function approximation in zero-sum markov games.
In Proceedings of the Eighteenth conference on Uncertainty in artificial intelligence, pp. 283–292.
Morgan Kaufmann Publishers Inc., 2002."
REFERENCES,0.25675675675675674,"Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436–444,
2015."
REFERENCES,0.2581081081081081,"Michael L. Littman and Csaba Szepesvari. A generalized reinforcement-learning model: Convergence
and applications. In 13th International Conference on Machine Learning, 1996."
REFERENCES,0.2594594594594595,"Boyi Liu, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural proximal/trust region policy optimization
attains globally optimal policy. arXiv preprint arXiv:1906.10306, 2019."
REFERENCES,0.2608108108108108,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, DaanWier-
stra, and Martin Riedmiller. Playing atari with deep reinforcement learningep reinforcement
learning. arXiv preprint arXiv:1312.5602, 2013."
REFERENCES,0.26216216216216215,"Ian Osband and Benjamin Van Roy. Model-based reinforcement learning and the eluder dimension.
In Advances in Neural Information Processing Systems (NeurIPS), 2014."
REFERENCES,0.2635135135135135,"Julien Pérolat, Bruno Scherrer, Bilal Piot, and Olivier Pietquin. Approximate dynamic programming
for two-player zero-sum Markov games. In International Conference on Machine Learning, pp.
1321–1329, 2015."
REFERENCES,0.2648648648648649,"Julien Pérolat, Florian Strub, Bilal Piot, and Olivier Pietquin. Learning nash equilibrium for general-
sum markov games from batch data. In Artificial Intelligence and Statistics, pp. 232–241. PMLR,
2017."
REFERENCES,0.2662162162162162,"Julien Pérolat, Bilal Piot, and Olivier Pietquin. Actor-critic fictitious play in simultaneous move
multistage games. In International Conference on Artificial Intelligence and Statistics, pp. 919–928,
2018."
REFERENCES,0.26756756756756755,Published as a conference paper at ICLR 2022
REFERENCES,0.2689189189189189,"Daniel Russo. Worst-case regret bounds for exploration via randomized value functions. In Advances
in Neural Information Processing Systems, pp. 14410–14420, 2019."
REFERENCES,0.2702702702702703,"Daniel Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic
exploration. In Advances in Neural Information Processing Systems, 2013."
REFERENCES,0.2716216216216216,"Tuomas Sandholm. Super-human ai for strategic reasoning: Beating top pros in heads-up no-limit
texas hold’em. In International Joint Conference on Artificial Intelligence, pp. 24–25, 2017."
REFERENCES,0.27297297297297296,"Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement
learning for autonomous driving. arXiv preprint arXiv:1610.03295, 2016."
REFERENCES,0.2743243243243243,"Aaron Sidford, Mengdi Wang, Lin Yang, and Yinyu Ye. Solving discounted stochastic two-player
games with near-optimal time and sample complexity. In International Conference on Artificial
Intelligence and Statistics, pp. 2992–3002. PMLR, 2020."
REFERENCES,0.2756756756756757,"David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, and Marc Lanctot. Mastering
the game of go with deep neural networks and tree search. Applied and Computational Harmonic
Analysis, 529(7587):484–489, 2015."
REFERENCES,0.27702702702702703,"Sriram Srinivasan, Marc Lanctot, Vinicius Zambaldi, Julien Pérolat, Karl Tuyls, Rémi Munos, and
Michael Bowling. Actor-critic policy optimization in partially observable multiagent environments.
In Advances in Neural information Processing Systems, pp. 3422–3435, 2018."
REFERENCES,0.27837837837837837,"Alexander L Strehl, Lihong Li, Eric Wiewiora, John Langford, and Michael L Littman. PAC model-
free reinforcement learning. In Proceedings of the 23rd International Conference on Machine
Learning, pp. 881–888, 2006."
REFERENCES,0.2797297297297297,"Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Model-based
rl in contextual decision processes: Pac bounds and exponential improvements over model-free
approaches. arXiv preprint arXiv:1811.08540, 2019a."
REFERENCES,0.2810810810810811,"Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Model-based
rl in contextual decision processes: Pac bounds and exponential improvements over model-free
approaches. In Conference on Learning Theory (COLT). arXiv preprint arXiv:1811:08540, 2019b."
REFERENCES,0.28243243243243243,"Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural policy gradient methods: Global
optimality and rates of convergence. arXiv preprint arXiv:1909.01150, 2020a."
REFERENCES,0.28378378378378377,"Ruosong Wang, Ruslan Salakhutdinov, and Lin F Yang. Provably efficient reinforcement learning
with general value function approximation. arXiv preprint arXiv:2005.10804, 2020b."
REFERENCES,0.2851351351351351,"Yining Wang, Ruosong Wang, Simon S. Du, and Akshay Krishnamurthy. Optimism in reinforcement
learning with generalized linear function approximation. In International Conference on Learning
Representations, 2021."
REFERENCES,0.2864864864864865,"Chen-Yu Wei, Yi-Te Hong, and Chi-Jen Lu. Online reinforcement learning in stochastic games. In
Advances in Neural Information Processing Systems, pp. 4987–4997, 2017."
REFERENCES,0.28783783783783784,"Qiaomin Xie, Yudong Chen, Zhaoran Wang, and Zhuoran Yang. Learning zero-sum simultaneous-
move markov games using function approximation and correlated equilibrium. In Conference on
Learning Theory (COLT). arXiv preprint arXiv:2002.07066, 2020."
REFERENCES,0.2891891891891892,"Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent
pessimism for offline reinforcement learning. In Advances in neural information processing
systems (NeurIPS), 2021."
REFERENCES,0.2905405405405405,"Lin Yang and Mengdi Wang. Reinforcement learning in feature space: Matrix bandit, kernels, and
regret bound. In International Conference on Machine Learning, pp. 10746–10756. PMLR, 2020."
REFERENCES,0.2918918918918919,"Zhuoran Yang, Chi Jin, Zhaoran Wang, Mengdi Wang, and Michael I Jordan. On function approx-
imation in reinforcement learning: Optimism in the face of large state spaces. arXiv preprint
arXiv:2011.04622, 2020."
REFERENCES,0.29324324324324325,Published as a conference paper at ICLR 2022
REFERENCES,0.2945945945945946,"Andrea Zanette and Emma Brunskill. Tighter problem-dependent regret bounds in reinforcement
learning without domain knowledge using value function bounds. In International Conference on
Machine Learning, pp. 7304–7312, 2019."
REFERENCES,0.2959459459459459,"Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill. Learning near
optimal policies with low inherent bellman error. arXiv preprint arXiv:2003.00153, 2020."
REFERENCES,0.2972972972972973,"Kaiqing Zhang, Sham M Kakade, Tamer Ba¸sar, and Lin F Yang. Model-based multi-agent rl in
zero-sum markov games with near-optimal sample complexity. arXiv preprint arXiv:2007.07461,
2020."
REFERENCES,0.29864864864864865,"Yulai Zhao, Yuandong Tian, Jason D. Lee, and Simon S. Du. Provably efficient policy gradient
methods for two-player zero-sum markov games. arXiv preprint arXiv:2102.08903, 2021."
REFERENCES,0.3,Published as a conference paper at ICLR 2022
REFERENCES,0.3013513513513513,"A
ADDITIONAL RELATED WORKS"
REFERENCES,0.3027027027027027,"There is a rich literature on sample complexity for single agent RL, mostly focusing on the tabular
cases (Strehl et al., 2006; Jaksch et al., 2010; Azar et al., 2017; Jin et al., 2018; Russo, 2019;
Zanette & Brunskill, 2019) and the linear function approximations (Yang & Wang, 2020; Wang et al.,
2021; Abbasi-Yadkori et al., 2019; Jin et al., 2020; Du et al., 2019b). For neural network function
approximations, Agarwal et al. (2020); Liu et al. (2019); Wang et al. (2020a); Yang et al. (2020) builds
provably efficient algorithms on over-parameterized models. The issue is complicated for general
function approximation, as the Q function is hard to learn due to the double-sample issue (Baird,
1995). In recent years, a line of work studies the structural properties and corresponding complexity
measures that allow generalization in RL. These work can be roughly grouped with two catagories.
One studies structures with low information gain. Jiang et al. (2017) proposes Bellman rank and
designs a sample efficient algorithm for a large family of MDPs. Later, Sun et al. (2019b) proposes
Witness rank and uses this notion to provide a provably efficient model-based RL algorithm. Recently,
Du et al. (2021) proposes Bilinear family and the corresponding Bilinear rank that encapsulates
a wide variety of structural properties, spanning from both model-based to model-free function
approximations. The other studies structures with low Eluder dimension (Russo & Van Roy, 2013).
Osband & Van Roy (2014) extends Eluder dimension to reinforcement learning and provides a unified
analysis of model based methods where the regret is controlled by the Kolmogorov dimension and
Eluder dimension of model function class. Wang et al. (2020b) designs a model-free algorithm and
shows that regret can be controlled by the Eluder dimension of the value function class. Recently, Jin
et al. (2021a) proposes Bellman Eluder dimension and builds a new framework to capture a large
amount of structures. It is noted that although lots of structures, such as linear MDP (Jin et al., 2020),
fit in both two viewpoints, they are not equivalent. Among the above work, Jin et al. (2021a); Zanette
et al. (2020) are more related. The method of placing uncertainty quantification on only initial states,
previously appeared in Zanette et al. (2020); Jin et al. (2021a), greatly simplifies the argument and
improves sample complexity. The Minimax Eluder dimension is inspired by the Bellman Eluder
dimension in Jin et al. (2021a)."
REFERENCES,0.30405405405405406,"In competitive RL, the setting with access to a sampling oracle or well explored policies is well
studied (Littman & Szepesvari, 1996; Greenwald et al., 2003; Grau-Moya et al., 2018; Pérolat et al.,
2018; Srinivasan et al., 2018; Sidford et al., 2020). Under these assumptions, many works consider
function approximations (Lagoudakis & Parr, 2002; Pérolat et al., 2015; Fan et al., 2020; Jia et al.,
2019; Zhao et al., 2021). However, without strong sampling model or a well explored policy, the issue
of exploration-exploitation tradeoff must be addressed. Most of these work focus on tabular setting
(Wei et al., 2017; Pérolat et al., 2017; Bai & Jin., 2020; Bai et al., 2021; 2020; Zhang et al., 2020; Zhao
et al., 2021) or linear function approximation settings (Xie et al., 2020; Chen et al., 2021). Among
them, (Wei et al., 2017; Xie et al., 2020) are more related. The regret-decomposition method used in
the decoupled setting is inspired by Xie et al. (2020). The ‘alternate optimism’ used in Algorithm 4
was previously used as ‘Maximin-EVI’ in Wei et al. (2017), although the ‘alternate optimism’ used
in Algorithm 2 is different. Moreover, the concurrent work of Jin et al. (2021b) studies competitive
RL with general function approximation. They reach a sublinear eO(
√"
REFERENCES,0.3054054054054054,"K) regret for MGs with low
minimax Eluder dimensions in the decoupled setting. Their results in the coordinated setting are
based on different assumptions and complexity measures. Specifically, they assume finite function
class and an additional class of exploiters. It is noted that by letting the policy class be the class of
optimal policies induced by the value function class, we have |Π| = |F| and our algorithm achieves
on-par guarantee in their setup in terms of the covering numbers of function classes. In addition, we
propose a first provably sample efficient and model-based algorithm for the coordinated setting with
general function approximation."
REFERENCES,0.30675675675675673,"A.1
TECHNICAL CHALLENGES"
REFERENCES,0.3081081081081081,"Previous work (Xie et al., 2020) imposes optimistic bonus on the action-value functions in every state-
action pairs and performs planning by the Coarse Correlated Equilibrium (CCE) on the optimistic
value functions. To achieve improved rates, we leverage the idea of ‘global optimism’ (Zanette et al.,
2020; Jin et al., 2021a; Du et al., 2021), which maintains a constraint set of candidate functions that
do not deviate much from the empirical estimates and performs optimistic planning on the initial
state. However, going beyond MDPs towards MGs, two problems arise. First, the concentration
property of functions in constraint set is hard to characterize due to multi-agent interplay. For this,"
REFERENCES,0.30945945945945946,Published as a conference paper at ICLR 2022
REFERENCES,0.3108108108108108,"we use the concentration methods in Jin et al. (2021a) and extend it from MDPs to MGs. The second
and more prominent issue is the exploration and exploitation tradeoff. Since ‘global optimism’ only
obtains optimism along the trajectories of behaviour policies, it may not guarantee optimism on the
trajectories of target policies (i.e. NE). As a result, directly using CCE to plan will cause the duality
gaps to diverge. To deal with this problem, we apply ‘alternate optimism’ to guide explorations,
which was previously used in Wei et al. (2017) for model-based methods. The ‘alternate optimism’
used in this work is slightly different for value-based methods. We prove two regret decomposition
lemmata to support this optimism principle."
REFERENCES,0.31216216216216214,"B
PROOF OF THEOREM 3.6"
REFERENCES,0.31351351351351353,"We prove the Theorem 3.6 in the following five steps. In the first step, we show that Bellman error is
small for function in the constraint set. In the second step, we show that Q∗stays in the constraint set
throughout the algorithm. In the third step, we decompose the regret into a summation of Bellman
error and Martingale difference. We can then bound the summation of Bellman error using Minimax
Eluder dimension in step 4. Finally we combine the aforementioned steps and complete the proof of
Theorem 3.6."
REFERENCES,0.31486486486486487,We define ‘optimistic’ value function in step h and episode k as follows
REFERENCES,0.3162162162162162,"V k
h (xk
h) := f k
h(xk
h, πk
h, bνk
h)"
REFERENCES,0.31756756756756754,"where bνk
h is the best response of πk
h in value function f k, i.e., bνk
h := argminνf k
h(xk
h, πk
h, ν). We use
νk = {νk
h}H
h=1 to denote the policy adopted by P2 in the k-th episode. Notice that the agent obtains
knowledge from ν only through its actions bk
h, h ∈[H]."
REFERENCES,0.31891891891891894,"Step 1: Low Bellman error in the constraint set
We first use a standard concentration procedure
(Agarwal et al., 2012; Jin et al., 2021a) to bound the Bellman error in constraint sets.
Lemma B.1 (Concentration on Bellman error). Let ρ > 0 be an arbitrary fixed number. With
probability at least 1 −p for all (k, h) ∈[K] × [H] we have k−1
X i=1"
REFERENCES,0.3202702702702703," 
f k
h(xi
h, ai
h, bi
h) −ri
h −
E
x∼P(·|xi
h,ai
h,bi
h) f k
h+1(x, πk
h+1, bνk
h+1)
2 ≤O(β)."
REFERENCES,0.3216216216216216,"Proof. Consider a fixed (k, h, f) sample, let"
REFERENCES,0.32297297297297295,"Ut(h, f) :=
 
fh(xt
h, at
h, bt
h) −rt
h −min
ν′ max
π′ fh+1(xt
h+1, π′, ν′)
2"
REFERENCES,0.32432432432432434,"−
 
Thfh+1(xt
h, at
h, bt
h) −rt
h −min
ν′ max
π′ fh+1(xt
h+1, π′, ν′)
2"
REFERENCES,0.3256756756756757,"and Ft,h be the filtration induced by {xi
1, ai
1, bi
1, ri
1, . . . , xi
H}t−1
i=1 ∪{xt
1, at
1, bt
1, rt
1, . . . , xt
h, at
h, bt
h}.
Recall the minimax Bellman operator
Thfh+1(x, a, b) := r(x, a, b) +
E
x′∼Ph(·|x,a,b)[min
ν′ max
π′ fh+1(x′, π′, ν′)]."
REFERENCES,0.327027027027027,"We have
E[Ut(h, f)|Ft,h]"
REFERENCES,0.32837837837837835,"= [(fh −Thfh+1)(xt
h, at
h, bt
h)]"
REFERENCES,0.32972972972972975,"· E[fh(xt
h, at
h, bt
h) + Thfh+1(xt
h, at
h, bt
h) −2rt
h −2 min
ν′ max
π′ fh+1(xt
h+1, π′, ν′)|Ft,h]"
REFERENCES,0.3310810810810811,"= [(fh −Thfh+1)(xt
h, at
h, bt
h)]2"
REFERENCES,0.3324324324324324,"and
Var[Ut(h, f)|Ft,h] ≤E[(Ut(h, f))2|Ft,h]"
REFERENCES,0.33378378378378376,"= [(fh −Thfh+1)(xt
h, at
h, bt
h)]2"
REFERENCES,0.33513513513513515,"· E[
 
fh(xt
h, at
h, bt
h) + Thfh+1(xt
h, at
h, bt
h) −2rt
h −2 min
ν′ max
π′ fh+1(xt
h+1, π′, ν′)
2|Ft,h]"
REFERENCES,0.3364864864864865,"≤36[(fh −Thfh+1)(xt
h, at
h, bt
h)]2 = 36 E[Ut(h, f)|Ft,h]."
REFERENCES,0.33783783783783783,Published as a conference paper at ICLR 2022
REFERENCES,0.33918918918918917,"By Freedman’s inequality, we have with probability at least 1 −p, k
X"
REFERENCES,0.34054054054054056,"t=1
Ut(h, f) − k
X"
REFERENCES,0.3418918918918919,"t=1
E[Ut(h, f)|Ft,h]"
REFERENCES,0.34324324324324323,"≤O
v
u
u
tlog(1/p) k
X"
REFERENCES,0.34459459459459457,"t=1
E[Ut|Ft,h] + log(1/p)

."
REFERENCES,0.34594594594594597,"Let N (F, ρ) be a ρ-cover of F. Taking a union bound for all (k, h, g) ∈[K] × [H] × N (F, ρ),
then with probability at least 1 −p for all (k, h, g) ∈[K] × [H] × N (F, ρ) k
X"
REFERENCES,0.3472972972972973,"t=1
Ut(h, g) − k
X"
REFERENCES,0.34864864864864864,"t=1
[(gh −Thgh+1)(xt
h, at
h, bt
h)]2"
REFERENCES,0.35,"≤O
v
u
u
tι k
X"
REFERENCES,0.35135135135135137,"t=1
[(gh −Thgh+1)(xt
h, at
h, bt
h)]2 + ι

(8)"
REFERENCES,0.3527027027027027,"where ι = log(HK|N (F, ρ)|/p). Let gk = argming∈N (F,ρ) maxh∈[H] ∥f k
h −gk
h∥∞. We immedi-
ately have k
X"
REFERENCES,0.35405405405405405,"t=1
Ut(h, gk) − k
X"
REFERENCES,0.3554054054054054,"t=1
[(gk
h −Thgk
h+1)(xt
h, at
h, bt
h)]2"
REFERENCES,0.3567567567567568,"≤O
v
u
u
tι k
X"
REFERENCES,0.3581081081081081,"t=1
[(gk
h −Thgk
h+1)(xt
h, at
h, bt
h)]2 + ι

.
(9)"
REFERENCES,0.35945945945945945,"For all (h, k) ∈[H] × [K], by the definition of Vk and f k ∈Vk we have k
X"
REFERENCES,0.3608108108108108,"t=1
Ut(h, f k) = k
X t=1"
REFERENCES,0.3621621621621622," 
f k
h(xt
h, at
h, bt
h) −rt
h −min
ν′ max
π′ fh+1(xt
h+1, π′, ν′)
2 − k
X t=1"
REFERENCES,0.3635135135135135," 
Thf k
h(xt
h, at
h, bt
h) −rt
h −min
ν′ max
π′ fh+1(xt
h+1, π′, ν′)
2 ≤ k
X t=1"
REFERENCES,0.36486486486486486," 
f k
h(xt
h, at
h, bt
h) −rt
h −min
ν′ max
π′ fh+1(xt
h+1, π′, ν′)
2"
REFERENCES,0.3662162162162162,"−inf
g∈F k
X t=1"
REFERENCES,0.3675675675675676," 
gh(xt
h, at
h, bt
h) −rt
h −min
ν′ max
π′ fh+1(xt
h+1, π′, ν′)
2"
REFERENCES,0.3689189189189189,"≤O(ι + kρ).
(10)"
REFERENCES,0.37027027027027026,"It thus follows that, k
X"
REFERENCES,0.3716216216216216,"t=1
[(f k
h −Thf k
h+1)(xt
h, at
h, bt
h)]2 ≤ k
X"
REFERENCES,0.372972972972973,"t=1
[(gk
h −Thgk
h+1)(xt
h, at
h, bt
h)]2 + O(ι + kρ) ≤O( k
X"
REFERENCES,0.37432432432432433,"t=1
Ut(h, gk) + ι + kρ) ≤O( k
X"
REFERENCES,0.37567567567567567,"t=1
Ut(h, f k) + ι + kρ)"
REFERENCES,0.377027027027027,≤O(ι + kρ)
REFERENCES,0.3783783783783784,"where the first step is due to gk is an ρ-approximation to f k, the second step comes from Eq (9), the
third step is due to gk is an ρ-approximation to f k, and the final step comes from Eq (10)."
REFERENCES,0.37972972972972974,"By definition of πk and bνk, we have"
REFERENCES,0.3810810810810811,"Thf k
h+1(x, a, b) := r(x, a, b) +
E
x′∼Ph(·|x,a,b)[fh+1(x′, πk
h+1, bνk
h+1)]"
REFERENCES,0.3824324324324324,thus we conclude the proof.
REFERENCES,0.3837837837837838,Published as a conference paper at ICLR 2022
REFERENCES,0.38513513513513514,"Step 2: Q∗is always in constraint set.
Lemma B.2. With probability at least 1 −p, we have Q∗∈Vk for all k ∈[K]."
REFERENCES,0.3864864864864865,"Proof. Consider a fixed (k, h, f) sample, let"
REFERENCES,0.3878378378378378,"Ut(h, f) :=
 
fh(xt
h, at
h, bt
h) −rt
h −min
ν′ max
π′ Q∗
h+1(xt
h+1, π′, ν′)
2"
REFERENCES,0.3891891891891892,"−
 
Q∗
h(xt
h, at
h, bt
h) −rt
h −min
ν′ max
π′ Q∗
h+1(xt
h+1, π′, ν′)
2"
REFERENCES,0.39054054054054055,"and Ft,h be the filtration induced by {xi
1, ai
1, bi
1, ri
1, . . . , xi
H}t−1
i=1 ∪{xt
1, at
1, bt
1, rt
1, . . . , xt
h, at
h, bt
h}."
REFERENCES,0.3918918918918919,We have
REFERENCES,0.3932432432432432,"E[Ut(h, f)|Ft,h] = [(fh −Q∗
h)(xt
h, at
h, bt
h)]2 and"
REFERENCES,0.3945945945945946,"Var[Ut(h, f)|Ft,h] ≤36[(f k
h −Thf k
h+1)(xt
h, at
h, bt
h)]2 = 36 E[Ut(h, f)|Ft,h]."
REFERENCES,0.39594594594594595,"Let N (F, ρ) be a ρ-cover of F. By Freedman’s inequality, for all (k, h, f) ∈[K] × [H] × N (F, ρ)
we have with probability at least 1 −p, k
X"
REFERENCES,0.3972972972972973,"t=1
Ut(h, f) − k
X"
REFERENCES,0.39864864864864863,"t=1
[(fh −Q∗
h)(xt
h, at
h, bt
h)]2"
REFERENCES,0.4,"≤O
v
u
u
tlog(1/p) k
X"
REFERENCES,0.40135135135135136,"t=1
[(fh −Q∗
h)(xt
h, at
h, bt
h)]2 + O(ι)
"
REFERENCES,0.4027027027027027,"where ι = log(HK|N (F, ρ)|/p). Since [(fh −Q∗
h)(xt
h, at
h, bt
h)]2 ≥0, this implies − k
X"
REFERENCES,0.40405405405405403,"t=1
Ut(h, g) ≤O(ι)."
REFERENCES,0.40540540540540543,"Therefore for all (k, h, g) ∈[K] × [H] × N (F, ρ) k
X t=1"
REFERENCES,0.40675675675675677," 
Q∗
h(xt
h, at
h, bt
h) −rt
h −min
ν′ max
π′ Q∗
h+1(xt
h+1, π′, ν′)
2 ≤ k
X t=1"
REFERENCES,0.4081081081081081," 
fh(xt
h, at
h, bt
h) −rt
h −min
ν′ max
π′ Q∗
h+1(xt
h+1, π′, ν′)
2 + O(ι + kρ)."
REFERENCES,0.40945945945945944,"Thus we have proven that Q∗∈Vk, ∀k ∈[K] with probability at least 1 −p."
REFERENCES,0.41081081081081083,Remark B.3. This step implies that
REFERENCES,0.41216216216216217,"V ∗≤V k
1 (xk
1)
(11)"
REFERENCES,0.4135135135135135,which ensures optimism from the beginning state (‘global optimism’).
REFERENCES,0.41486486486486485,"Step 3: Regret decomposition
The following result is key to our analysis. It decomposes the
deviation from the value of the game into Bellman errors across time steps and a martingale difference
sequence.
Lemma B.4. Define"
REFERENCES,0.41621621621621624,"δk
h := V k
h (xk
h) −V πk,νk"
REFERENCES,0.4175675675675676,"h
(xk
h)"
REFERENCES,0.4189189189189189,"ζk
h := E[δk
h+1|xk
h, ak
h, bk
h] −δk
h+1
γk
h :=
E
a∼πk
h(xk
h)[f k
h(xk
h, a, bk
h)] −f k
h(xk
h, ak
h, bk
h)"
REFERENCES,0.42027027027027025,"bγk
h :=
E
a∼πk
h(xk
h),b∼νk
h(xk
h)[Qπk,νk"
REFERENCES,0.42162162162162165,"h
(xk
h, a, b)] −Qπk,νk"
REFERENCES,0.422972972972973,"h
(xk
h, ak
h, bk
h)"
REFERENCES,0.4243243243243243,Published as a conference paper at ICLR 2022
REFERENCES,0.42567567567567566,Then we have
REFERENCES,0.42702702702702705,"δk
h ≤δk
h+1 −ζk
h + γk
h −bγk
h + ϵk
h
(12)"
REFERENCES,0.4283783783783784,"where ϵk
h is the bellman error defined by"
REFERENCES,0.4297297297297297,"ϵk
h := f k
h(xk
h, ak
h, bk
h) −rk
h −
E
x∼P(·|xk
h,ak
h,bk
h) f k
h+1(x, πk
h+1, bνk
h+1)."
REFERENCES,0.43108108108108106,"Proof. By definition of bνk
h = argminνf k
h(xk
h, πk
h, ν), we have"
REFERENCES,0.43243243243243246,"f k
h(xk
h, πk
h, bνk
h) ≤f k
h(xk
h, πk
h, bk
h)"
REFERENCES,0.4337837837837838,"= f k
h(xk
h, ak
h, bk
h) + γk
h."
REFERENCES,0.43513513513513513,It thus follows that
REFERENCES,0.43648648648648647,"δk
h ≤f k
h(xk
h, ak
h, bk
h) −Qπk,νk"
REFERENCES,0.43783783783783786,"h
(xk
h, ak
h, bk
h) + γk
h −bγk
h
= rk
h +
E
x∼P(·|xk
h,ak
h,bk
h) f k
h+1(x, πk
h+1, bνk
h+1) + ϵk
h"
REFERENCES,0.4391891891891892,"−(rk
h +
E
x∼P(·,xk
h,ak
h,bk
h)[V πk,νk"
REFERENCES,0.44054054054054054,"h+1
(x)|xk
h, ak
h, bk
h]) + γk
h −bγk
h"
REFERENCES,0.4418918918918919,"= f k
h(xk
h+1, πk
h+1, bνk
h+1) −V πk,νk"
REFERENCES,0.44324324324324327,"h+1
(xk
h+1) −ζk
h + ϵk
h + γk
h −bγk
h
= δk
h+1 −ζk
h + γk
h −bγk
h + ϵk
h."
REFERENCES,0.4445945945945946,Therefore we complete the proof.
REFERENCES,0.44594594594594594,"Step 4: Bounding cumulative Bellman error using Minimax Eluder dimension.
Recall that
Lemma B.1 gives: k−1
X i=1"
REFERENCES,0.4472972972972973," 
f k
h(xi
h, ai
h, bi
h) −ri
h −
E
x∼P(·|xi
h,ai
h,bi
h) f k
h+1(x, πk
h+1, bνk
h+1)
2 ≤O(β)."
REFERENCES,0.4486486486486487,Combining this and Lemma F.1 with
REFERENCES,0.45,"gk = f k
h(·, ·, ·) −rk
h −
E
x∼P(·|·,·,·) f k
h+1(x, πk
h+1, bνk
h+1)"
REFERENCES,0.45135135135135135,"and ρk = δ(xk
n, ak
h, bk
h), we have come to the following result.
Lemma B.5. For any (k, h) ∈[K] × [H] we have K
X"
REFERENCES,0.4527027027027027,"k=1
|f k
h(xk
h, ak
h, bk
h) −rk
h −
E
x∼P(·|xk
h,ak
h,bk
h) f k
h+1(x, πk
h+1, bνk
h+1)| ≤O
q"
REFERENCES,0.4540540540540541,"K · dimME(F,
p"
REFERENCES,0.4554054054054054,"1/K) log(HKN(F, 1/K)/p)

. Thus K
X"
REFERENCES,0.45675675675675675,"k=1
ϵk
h ≤O
q"
REFERENCES,0.4581081081081081,"K · dimME(F,
p"
REFERENCES,0.4594594594594595,"1/K) log(HKN(F, 1/K)/p)

.
(13)"
REFERENCES,0.4608108108108108,"Step 5: Putting everything together
By Eq (11) and Eq (12) K
X"
REFERENCES,0.46216216216216216,"k=1
[V ∗
1 (x1) −V πk,νk
1
(x1)] ≤ K
X"
REFERENCES,0.4635135135135135,"k=1
[V k
1 (x1) −V πk,νk
1
(x1)] ≤ K
X k=1 H
X"
REFERENCES,0.4648648648648649,"h=1
(−ζk
h + γk
h −bγk
h + ϵk
h) = H
X h=1 K
X"
REFERENCES,0.46621621621621623,"k=1
(−ζk
h + γk
h −bγk
h) + H
X h=1 K
X"
REFERENCES,0.46756756756756757,"k=1
ϵk
h."
REFERENCES,0.4689189189189189,Published as a conference paper at ICLR 2022
REFERENCES,0.4702702702702703,"Notice that PH
h=1
PK
k=1(−ζk
h + γk
h −bγk
h) is a martingale difference sequence and can be bounded
by O(H
p"
REFERENCES,0.47162162162162163,KH log(HK/p)) with probability at least 1 −p.
REFERENCES,0.47297297297297297,"By Eq (13) the term PH
h=1
PK
k=1 ϵk
h can be bounded by"
REFERENCES,0.4743243243243243,"O

H
q"
REFERENCES,0.4756756756756757,"K · dimME(F,
p"
REFERENCES,0.47702702702702704,"1/K) log(HKN(F, 1/K)/p)
"
REFERENCES,0.4783783783783784,with probability at least 1 −p.
REFERENCES,0.4797297297297297,It thus follows that regret can be bounded by
REFERENCES,0.4810810810810811,"Reg(K) ≤O

H
q"
REFERENCES,0.48243243243243245,"K · dimME(F,
p"
REFERENCES,0.4837837837837838,"1/K) log(HKN(F, 1/K)/p)

."
REFERENCES,0.4851351351351351,"B.1
LINEAR FUNCTION APPROXIMATION"
REFERENCES,0.4864864864864865,"In the linear function approximation setting of Xie et al. (2020),"
REFERENCES,0.48783783783783785,"rh(x, a, b) = ϕ(x, a, b)⊤θh, Ph(·|x, a, b) = ϕ(x, a, b)⊤µh(·)"
REFERENCES,0.4891891891891892,"where ϕ(·, ·, ·) ∈Rd is known feature vector and θh ∈Rd and µh(·) ∈Rd are unknown with
∥θ∥2 ≤
√"
REFERENCES,0.4905405405405405,"d, ∥νh(·)∥2 ≤
√"
REFERENCES,0.4918918918918919,"d and ∥ϕ(·, ·)∥2 ≤1. In this case Algorithm 1 reduces to the following
Algorithm 3. Notice that this algorithm can also be seen as a generalization of Zanette et al. (2020) to
Markov games."
REFERENCES,0.49324324324324326,Algorithm 3 ONEMG for linear function approximation.
REFERENCES,0.4945945945945946,"1: Input: Function class F
2: Set β ←C
√"
REFERENCES,0.49594594594594593,"d log(HK/p) for some large constant C
3: for episode k = 1, 2, . . . , K do
4:
Receive initial state xk
1
5:
Set wk
H+1 = θk
H+1 = 0
6:
Find maxwk
h,θk
h,h∈[H] V k
1 (xk
1) such that for all h ∈[H]:"
REFERENCES,0.4972972972972973,"(1) wk
h = (Λk
h)−1
k−1
X"
REFERENCES,0.49864864864864866,"τ=1
ϕ(xτ
h, aτ
h, bτ
h)[rτ
h + V k
h+1(xτ
h+1)]"
REFERENCES,0.5,"(2) ∥θk
h −wk
h∥Λk
h ≤C · Hβ"
REFERENCES,0.5013513513513513,"(3) Qk
h(·, ·, ·) = (θk
h)⊤ϕ(·, ·, ·)"
REFERENCES,0.5027027027027027,"(4) (πk
h, B0) = NE(Qk
h)"
REFERENCES,0.504054054054054,"(5) V k
h (·) =
E
a∼πk
h,b∼B0
[Qk
h(·, a, b)]"
REFERENCES,0.5054054054054054,"7:
for step h = 1, 2, . . . , H do
8:
P1 plays action ak
h ∼πk
h(xk
h)
9:
P2 takes action bk
h
10:
Observe reward rk
h and move to next state xk
h+1
11:
end for
12: end for"
REFERENCES,0.5067567567567568,"We will prove the following theoretical result of Algorithm 3. Notice the dependency on d of
Algorithm 3 is d while the dependency on d of Xie et al. (2020) is d3/2, thus this result improves
theirs by
√"
REFERENCES,0.5081081081081081,d factor.
REFERENCES,0.5094594594594595,"Theorem B.6. Regret (Eq (3)) in Algorithm 3 is bounded by O(d
p"
REFERENCES,0.5108108108108108,"H2K log(dHK/p)) with proba-
bility at least 1 −p."
REFERENCES,0.5121621621621621,"B.1.1
PROOF OF THEOREM B.6"
REFERENCES,0.5135135135135135,We prove Theorem B.6 in the following five steps similar to the previous section.
REFERENCES,0.5148648648648648,Published as a conference paper at ICLR 2022
REFERENCES,0.5162162162162162,"Step 1: Low Bellman error in the constraint set
We use some standard results from previous
work. Notice that the N(F, ϵ) = O(d log(1/ϵ)) and our inherent Bellman error is zero, thus these
lemmata can be directly adapted into our setting."
REFERENCES,0.5175675675675676,"Lemma B.7 (Concentration, adapted from Lemma 1 of Zanette et al. (2020)). With probability at
least 1 −p, the following holds for all (k, h) ∈[K] × [H], X"
REFERENCES,0.518918918918919,"τ∈[k−1]
ϕτ
h[V k
h+1(xτ
h+1) −
E
x∼P(·|xτ
h,aτ
h,bτ
h) V k
h+1(x)]"
REFERENCES,0.5202702702702703,"(Λk
h)−1
≤Hβ"
REFERENCES,0.5216216216216216,"where β = Ω(
p"
REFERENCES,0.522972972972973,d log(dKH/p)).
REFERENCES,0.5243243243243243,"Lemma B.8 (Least square error bound, adapted from Lemma 3 of Xie et al. (2020)). On the event of
Lemma B.7, the following holds for all (x, a, b, h, k) ∈S × A × A × [H] × [K] and any policy pair
(π, ν)
⟨ϕ(x, a, b), θk
h⟩−Qπ,ν
h (x, a, b) −
E
x∼P(·|x,a,b)(V k
h+1 −V π,ν
h+1)(x)
 ≤β∥ϕ(x, a, b)∥(Λk
h)−1"
REFERENCES,0.5256756756756756,Step 2: Q∗always stays in the constraint set
REFERENCES,0.527027027027027,"Lemma B.9 (Optimism). Let Q∗
h(·, ·, ·) = ϕ(·, ·, ·)⊤θ∗
h, then on the event of Lemma B.8 and
Lemma B.7 there exists w∗
h, h ∈[H] such that w∗
h, θ∗
h, h ∈[H] satisfies the constraints of Line 6 in
Algorithm 3."
REFERENCES,0.5283783783783784,"Proof. We prove by backward induction on h. The claim trivially holds for h = H + 1. Now assume
the inductive hypothesis holds at h + 1. Let"
REFERENCES,0.5297297297297298,"w∗
h = (Λk
h)−1
k−1
X"
REFERENCES,0.5310810810810811,"τ=1
ϕ(xτ
h, aτ
h, bτ
h)[rτ
h + V k
h+1(xτ
h+1)] = ( k−1
X"
REFERENCES,0.5324324324324324,"τ=1
ϕτ
h(ϕτ
h)⊤+ I)−1
k−1
X"
REFERENCES,0.5337837837837838,"τ=1
ϕτ
h(rτ
h +
E
x∼P(·|xτ
h,aτ
h,bτ
h) V k
h+1(x) + ητ
h)"
REFERENCES,0.5351351351351351,"where ϕτ
h = ϕ(xτ
h, aτ
h, bτ
h) and ητ
h = V k
h+1(xτ
h+1) −Ex∼P(·|xτ
h,aτ
h,bτ
h) V k
h+1(x). Notice that inductive
hypothesis implies rτ
h + Ex∼P(·|xτ
h,aτ
h,bτ
h) V k
h+1(x) = ϕ(xτ
h, aτ
h, bτ
h)⊤θ∗
h, therefore"
REFERENCES,0.5364864864864864,"w∗
h = ( k−1
X"
REFERENCES,0.5378378378378378,"τ=1
ϕτ
h(ϕτ
h)⊤+ I)−1
k−1
X"
REFERENCES,0.5391891891891892,"τ=1
ϕτ
h
 
(ϕτ
h)⊤θ∗
h + ητ
h
"
REFERENCES,0.5405405405405406,"= θ∗
h −(Λk
h)−1θ∗
h + (Λk
h)−1
k−1
X"
REFERENCES,0.5418918918918919,"τ=1
ϕτ
hητ
h."
REFERENCES,0.5432432432432432,It thus suffices to bound the following
REFERENCES,0.5445945945945946,"∥(Λk
h)−1θ∗
h + (Λk
h)−1
k−1
X"
REFERENCES,0.5459459459459459,"τ=1
ϕτ
hητ
h∥(Λk
h) ≤∥θ∗
h∥(Λk
h)−1 + ∥ k−1
X"
REFERENCES,0.5472972972972973,"τ=1
ϕτ
hητ
h∥(Λk
h)−1 ≤
√"
REFERENCES,0.5486486486486486,"d + Hβ
≤C · Hβ"
REFERENCES,0.55,"where the penultimate step comes from ∥θ∗
h∥2 ≤
√"
REFERENCES,0.5513513513513514,d and Lemma B.7.
REFERENCES,0.5527027027027027,Remark B.10. This fact implies that
REFERENCES,0.5540540540540541,"V ∗(xk
1) ≤V k
1 (xk
1).
(14)"
REFERENCES,0.5554054054054054,Published as a conference paper at ICLR 2022
REFERENCES,0.5567567567567567,"Step 3: Recursive decomposition of regret
Lemma B.11 (Regret decomposition). Define"
REFERENCES,0.5581081081081081,"δk
h := V k
h (xk
h) −V πk,νk"
REFERENCES,0.5594594594594594,"h
(xk
h)"
REFERENCES,0.5608108108108109,"ζk
h := E[δk
h+1|xk
h, ak
h, bk
h] −δk
h+1
γk
h :=
E
a∼πk
h(xk
h)[Qk
h(xk
h, a, bk
h)] −Qk
h(xk
h, ak
h, bk
h)"
REFERENCES,0.5621621621621622,"bγk
h :=
E
a∼πk
h(xk
h),b∼νk
h(xk
h)[Qπk,νk"
REFERENCES,0.5635135135135135,"h
(xk
h, a, b)] −Qπk,νk"
REFERENCES,0.5648648648648649,"h
(xk
h, ak
h, bk
h)"
REFERENCES,0.5662162162162162,Then we have
REFERENCES,0.5675675675675675,"δk
h ≤δk
h+1 −ζk
h + γk
h −bγk
h + ϵk
h
(15)"
REFERENCES,0.5689189189189189,"where ϵk
h is the width defined by"
REFERENCES,0.5702702702702702,"ϵk
h := 2β
q"
REFERENCES,0.5716216216216217,"(ϕk
h)⊤(Λk
h)−1ϕk
h."
REFERENCES,0.572972972972973,"Proof. By definition of B0 = argminνf k
h(xk
h, πk
h, ν), we have"
REFERENCES,0.5743243243243243,"V k
h (xk
h) = Qk
h(xk
h, πk
h, B0)"
REFERENCES,0.5756756756756757,"≤Qk
h(xk
h, πk
h, bk
h)"
REFERENCES,0.577027027027027,"= Qk
h(xk
h, ak
h, bk
h) + γk
h."
REFERENCES,0.5783783783783784,It thus follows that
REFERENCES,0.5797297297297297,"δk
h ≤Qk
h(xk
h, ak
h, bk
h) −Qπk,νk"
REFERENCES,0.581081081081081,"h
(xk
h, ak
h, bk
h) + γk
h −bγk
h"
REFERENCES,0.5824324324324325,"≤
E
x∼P(·|xk
h,ak
h,bk
h)[V k
h+1(x)] −
E
x∼P(·|xk
h,ak
h,bk
h)[V πk,νk"
REFERENCES,0.5837837837837838,"h+1
(x)] + ϵk
h + γk
h −bγk
h"
REFERENCES,0.5851351351351352,"= V k
h+1(xk
h+1) −V πk,νk"
REFERENCES,0.5864864864864865,"h+1
(xk
h+1) −ζk
h + ϵk
h + γk
h −bγk
h
= δk
h+1 −ζk
h + γk
h −bγk
h + ϵk
h"
REFERENCES,0.5878378378378378,"where the second step comes from Lemma B.8 by letting (x, a, b) = (xk
h, ak
h, bk
h) (notice that
⟨ϕ(xk
h, ak
h, bk
h), θk
h⟩= Qk
h(xk
h, ak
h, bk
h))."
REFERENCES,0.5891891891891892,"Step 4: Bounding width by Elliptical Potential Lemma
This step directly makes use of the
following standard result.
Lemma B.12 (Elliptical Potential Lemma, Lemma 10 of Xie et al. (2020)). Suppose {ϕt}t≥0 is
a sequence in Rd satisfying ∥ϕt∥≤1, ∀t. Let Λ0 ∈Rd×d be a positive definite matrix, and
Λt = Λ0 + P"
REFERENCES,0.5905405405405405,"i∈[t] ϕiϕ⊤
i . If the smallest eigenvalue of Λ0 is lower bounded by 1, then"
REFERENCES,0.5918918918918918,log( detΛt
REFERENCES,0.5932432432432433,"detΛ0
) ≤
X"
REFERENCES,0.5945945945945946,"i∈[t]
ϕ⊤
i Λ−1
j−1ϕi ≤2 log( detΛt"
REFERENCES,0.595945945945946,"detΛ0
)."
REFERENCES,0.5972972972972973,"Step 5: Putting everything together
By Eq (14) and Eq (15) K
X"
REFERENCES,0.5986486486486486,"k=1
[V ∗
1 (x1) −V πk,νk
1
(x1)] ≤ K
X"
REFERENCES,0.6,"k=1
[V k
1 (x1) −V πk,νk
1
(x1)] ≤ K
X k=1 H
X"
REFERENCES,0.6013513513513513,"h=1
(−ζk
h + γk
h −bγk
h + ϵk
h) = H
X h=1 K
X"
REFERENCES,0.6027027027027027,"k=1
(−ζk
h + γk
h −bγk
h) + H
X h=1 K
X"
REFERENCES,0.6040540540540541,"k=1
ϵk
h."
REFERENCES,0.6054054054054054,Published as a conference paper at ICLR 2022
REFERENCES,0.6067567567567568,"Notice that PH
h=1
PK
k=1(−ζk
h + γk
h −bγk
h) is a martingale difference sequence and can be bounded
by H
p"
REFERENCES,0.6081081081081081,KH log(HK/p) with probability at least 1 −p.
REFERENCES,0.6094594594594595,"By Lemma B.12 the term PH
h=1
PK
k=1 ϵk
h can be bounded as follows H
X h=1 K
X"
REFERENCES,0.6108108108108108,"k=1
ϵk
h = H
X h=1 K
X"
REFERENCES,0.6121621621621621,"k=1
2β
q"
REFERENCES,0.6135135135135135,"(ϕk
h)⊤(Λk
h)−1ϕk
h ≤ H
X h=1 √ K ·"
REFERENCES,0.6148648648648649,"v
u
u
t K
X"
REFERENCES,0.6162162162162163,"k=1
2β(ϕk
h)⊤(Λk
h)−1ϕk
h ≤2β H
X h=1 √ K · s"
REFERENCES,0.6175675675675676,"2 log(detΛk
h
detΛ0
h
) ≤2β H
X h=1 √ K · r"
REFERENCES,0.6189189189189189,"2 log (λ + K maxk ∥ϕk
h∥2)d λd"
REFERENCES,0.6202702702702703,"≤O(dH
p"
REFERENCES,0.6216216216216216,K log(dHK/p)).
REFERENCES,0.6229729729729729,with probability at least 1 −p.
REFERENCES,0.6243243243243243,"It thus follows that with probability at least 1 −p regret can be upper bounded by O(d
p"
REFERENCES,0.6256756756756757,H2K log(dHK/p)).
REFERENCES,0.6270270270270271,"C
MODEL-BASED METHOD FOR THE COORDINATED SETTING"
REFERENCES,0.6283783783783784,"C.1
ALTERNATE OPTIMISTIC MODEL ELIMINATION (AOME)"
REFERENCES,0.6297297297297297,Algorithm 4 Alternate Optimistic Model Elimination (AOME)
REFERENCES,0.6310810810810811,"1: Input: Model class M, test function class G, precision ϵ, failure probability p
2: Initialize M0 ←M
3: for k = 1, 2, . . . , K do
4:
Find M k
1 ←argmaxM∈MQM(x1, πM, νM), let πk ←πM k
1"
REFERENCES,0.6324324324324324,"5:
Find M k
2 ←argminM∈MQM(x1, πk, νM
πk), let νk ←νM k
1
πk
6:
Execute πk, νk and collect n1 rewards {ri
h}h∈[H],i∈[n1]
7:
Let bV k ←
1
n1
Pn1
i=1(PH
h=1 ri
h)"
REFERENCES,0.6337837837837837,"8:
if max{|bV k −QM k
1 (x1, πk, νk)|, |bV k −QM k
2 (x1, πk, νk)|} ≤ϵ/2 then
9:
Terminate and output πk, νk.
10:
else
11:
Find hk such that maxi∈[2] | bL(M k
1 , M k
2 , M k
i , hk)| ≥ϵ/(4H) by Eq (5).
12:
Collect n trajectories {(xi
h, ai
h, bi
h, ri
h)H
h=1}n
i=1 by executing ah, bh ∼πk
h, νk
h, ∀h ∈[H]"
REFERENCES,0.6351351351351351,"13:
Update constraint set, where bE is given by Eq (6)"
REFERENCES,0.6364864864864865,"Mk ←{M ∈Mk−1 : bE(M k
1 , M k
2 , M, hk) ≤ϕ}"
REFERENCES,0.6378378378378379,"14:
end if
15: end for"
REFERENCES,0.6391891891891892,"C.2
THEORY FOR ALTERNATE OPTIMISTIC MODEL ELIMINATION (AOME)"
REFERENCES,0.6405405405405405,"In this section we prove that Algorithm 4 terminates in finite rounds. The technique bulk is then
showing that in Step 13 the constraint set can only shrink for finite times. This is dependent on the
complexity of model class M and discriminator function class G. We first generalize Witness rank
(Sun et al., 2019b) to Markov games."
REFERENCES,0.6418918918918919,Published as a conference paper at ICLR 2022
REFERENCES,0.6432432432432432,"Definition C.1 (Witnessed model misfit in Markov games). For discriminator class G : S × A1 ×
A2 ×R×S 7→R, models M1, M2, M ∈M and level h ∈[H], the witnessed model misfit of Markov
game at level h is defined as follow
E(M1, M2, M, h) := sup
g∈G
E
(x,a,b)∼PM2
M1
E
(r,x)∼M[g(xh, ah, bh, r, x) −g(xh, ah, bh, rh, xh+1)]. (16)"
REFERENCES,0.6445945945945946,"where (x, a, b) ∼PM2
M1 denotes xτ+1 ∼P(·|xτ, aτ, bτ), aτ ∼πM1 and bτ ∼νM2
πM1 for all τ ∈[h]."
REFERENCES,0.6459459459459459,"Using g(xh, ah, bh, r, x) = r(xh, ah, bh) + QM(x, πM1, νM2
πM1), we can also give a generalization of
Bellman error (Jiang et al., 2016) in Markov games by the following"
REFERENCES,0.6472972972972973,"L(M1, M2, M, h) :=
E
(x,a,b)∼PM2
M1"
REFERENCES,0.6486486486486487,"
E
(r,x)∼M[g(xh, ah, bh, r, x))] −
E
(r,x)∼M ∗[g(xh, ah, bh, r, x)]

. (17)"
REFERENCES,0.65,"The final components in our theory are two assumptions from Sun et al. (2019b). Define rank(A, β)
to be the smallest integer k such that A = UV ⊤with U, V ∈Rn×k and ∥Ui,∗∥2 · ∥Vj,∗∥2 ≤β for
any pair of rows Ui,∗, Vj,∗.
Assumption C.2 (Realizability). Assume M ∗∈M.
Assumption C.3 (Witness misfit domination). Assume G is finite, ∥g∥∞≤2, ∀g ∈G and
∀M1, M2, M ∈M : E(M1, M2, M, h) ≥L(M1, M2, M, h)."
REFERENCES,0.6513513513513514,"Now we introduce the Witness rank for Markov games.
Definition C.4 (Witness rank for Markov games). Given a model class M, a test function class
G and κ ∈(0, 1]. For h ∈[H], we define Nκ,h ⊂R|M|2×|M| as the set of matrices such that any
matrix A ∈Nκ,h satisfies
κ|L(M1, M2, M, h)| ≤A((M1, M2), M) ≤E(M1, M2, M, h), ∀M1, M2, M ∈M.
The Witness rank for Markov games is defined by
W(κ, β, M, G, h) :=
min
A∈Nκ,h rank(A, β)."
REFERENCES,0.6527027027027027,"We are in the position to state the main theorem of this section.
Theorem C.5. Under Assumption C.2 and Assumption C.3, set ϕ =
κϵ
100H√Wκ where Wκ =
maxh∈[H] W(κ, β, M, G, h), let T = HWκ log(β/2ϕ), set n1 = C · H2 log(HT/p)/ϵ2 and n =
C · H2Wκ|A| log(T|M||G|/p)/(κϵ)2 for large enough constant C. Then for all ϵ, p, κ ∈(0, 1], with
probability at least 1 −p Algorithm 4 outputs a policy π such that V ∗(x1) −V π,ν∗
π(x1) ≤ϵ within
at most O( H3W 2
κ|A|
κ2ϵ2
log(T|G||M|/p)) trajectories."
REFERENCES,0.654054054054054,"As seen in the above theorem, Algorithm 4 has sample complexity that scales quadratic with Witness
rank and logarithmically with cardinality of model class. This matches the sample complexity results
in the Markov decision process setting (Sun et al., 2019b). When the test function is Q-function,
the Witnessed model misfit reduced to Bellman error as seen in Eq (17) and witness rank reduce to
Bellman rank."
REFERENCES,0.6554054054054054,"C.3
PROOFS"
REFERENCES,0.6567567567567567,"We prove Theorem C.5 in the following steps. In the first step, we show a simulation lemma that
generalize Jiang et al. (2017) to Markov games. In the second step we show concentration of empirical
estimates of Bellman error, model misfit and value functions. In the third step we show that M ∗
always stays in the constraint set. We examine the conditions when Algorithm 4 terminates or not in
the fourth step. Using these conditions, we bound the number of rounds in the fifth step. Finally we
combine everything and complete the proof of Theorem C.5."
REFERENCES,0.6581081081081082,"From Definition C.4 we know that there exists Ah ∈Nκ,h such that
κ|L(M1, M2, M, h)| ≤Ah((M1, M2), M) ≤E(M1, M2, M, h), ∀M1, M2, M ∈M.
and
we
can
factorize
Ah((M1, M2), M)
=
⟨ζh(M1, M2), χh(M)⟩
where
∥ζh(M1, M2)∥2, ∥χh(M)∥2 ≤β."
REFERENCES,0.6594594594594595,Published as a conference paper at ICLR 2022
REFERENCES,0.6608108108108108,"Step 1: Simulation Lemma
The analysis begins with a useful Lemma that decouples the value
difference into a sum of Bellman errors across time steps.
Lemma C.6 (Simulation Lemma). Fix model M1, M2, M ∈M. Let π = πM1, ν = νM2
πM1. Under
Assumption C.3, we have"
REFERENCES,0.6621621621621622,"QM(x1, π, ν) −V π,ν
1
(x1) = H
X"
REFERENCES,0.6635135135135135,"h=1
L(M1, M2, M, h)"
REFERENCES,0.6648648648648648,"Proof. Let (x, a, b) ∼PM2
M1 denote xτ+1 ∼P(·|xτ, aτ, bτ), aτ ∼πM1 and bτ ∼νM2
πM1 for all
τ ∈[h]. Using definition of Bellman error in Eq (17), we have
QM(x1, π, ν) −V π,ν
1
(x1)"
REFERENCES,0.6662162162162162,"=
E
(x,a,b)∼PM2
M1"
REFERENCES,0.6675675675675675,"
E
(r1,x2)∼M[r1 + QM(x2, π, ν)] −
E
(r1,x2)∼M ∗[r1 + QM ∗(x2, π, ν)]
"
REFERENCES,0.668918918918919,"=
E
(x,a,b)∼PM2
M1"
REFERENCES,0.6702702702702703,"
E
(r1,x2)∼M[r1 + QM(x2, π, ν)] −
E
(r1,x2)∼M ∗[r1 + QM(x2, π, ν)]
"
REFERENCES,0.6716216216216216,"+
E
(x,a,b)∼PM2
M1"
REFERENCES,0.672972972972973,"
E
(r1,x2)∼M ∗[r1 + QM(x2, π, ν)] −
E
(r1,x2)∼M ∗[r1 + QM ∗(x2, π, ν)]
"
REFERENCES,0.6743243243243243,"= L(M1, M2, M, 1) +
E
(x,a,b)∼PM2
M1
[QM(x2, π, ν) −V π,ν
2
(x2)]"
REFERENCES,0.6756756756756757,"= · · · = H
X"
REFERENCES,0.677027027027027,"h=1
L(M1, M2, M, h)."
REFERENCES,0.6783783783783783,We thus complete the proof.
REFERENCES,0.6797297297297298,"Step 2: Concentrations
In this part we show some standard concentration results.
Lemma C.7. With probability at least 1 −p, we have the following event 1."
REFERENCES,0.6810810810810811,"E(M k
1 , M k
2 , M, h) −bE(M k
1 , M k
2 , M, h)
 ≤ϕ for all M ∈M and h ∈[H], 2."
REFERENCES,0.6824324324324325,"bL(M k
1 , M k
2 , M k
i , h) −L(M k
1 , M k
2 , M k
i , h)
 ≤
ϵ
8H for all h ∈[H] and i ∈[2], 3."
REFERENCES,0.6837837837837838,"V πk,νk −bV k ≤ϵ/8"
REFERENCES,0.6851351351351351,holds for all k ≤T rounds.
REFERENCES,0.6864864864864865,"Proof. Fix one iteration. By Hoeffding’s inequality, with probability at least 1 −p/3,
 bL(M k
1 , M k
2 , M k
i , h) −L(M k
1 , M k
2 , M k
i , h)
 ≤ s"
REFERENCES,0.6878378378378378,log(2H/p)
REFERENCES,0.6891891891891891,"n1
.
(18)"
REFERENCES,0.6905405405405406,"Thus (2) follows from n1 = C · H2 log(HT/p)/ϵ2 and union bound on all T iterations. Similarly
we can verify (3). For (1), fix model M ∈M and g ∈G, by Hoeffding’s inequality, with probability
at least 1 −p/3, 
E
(x,a,b)∼PM2
M1
E
(r,x)∼M[g(xh, ah, bh, r, x) −g(xh, ah, bh, rh, xh+1)]
(19) − n
X i=1"
"N
E",0.6918918918918919,"1
n
E
(r,x)∼M[g(xi
h, ai
h, bi
h, r, x′) −g(xi
h, ai
h, bi
h, ri
h, xi
h+1)]

(20) ≤ r"
"N
E",0.6932432432432433,log(2/p)
"N
E",0.6945945945945946,"n
.
(21)"
"N
E",0.6959459459459459,Published as a conference paper at ICLR 2022
"N
E",0.6972972972972973,"Thus (1) follows from n = C · H2Wκ|A| log(T|M||G|/p)/(κϵ)2 and union bound on all k ∈[T],
M ∈M and g ∈G. We complete the proof."
"N
E",0.6986486486486486,"Step 3: M ∗stays in the constraint set while it shrinks
In this step, we show that the constraint
set always contains the true environment. The result is displayed in the following.
Lemma C.8. Suppose for each round k, the event in Lemma C.7 holds. Then M ∗∈Mk for all
k. Furthermore, let c
Mk = {M ∈c
Mk−1 : Ahk(M k
1 , M k
2 , M) ≤2ϕ} with c
M0 = M. We have
Mk ⊂c
Mk for all k."
"N
E",0.7,"Proof. Since E(M k
1 , M k
2 , M ∗, h)
=
0, ∀h
∈
[H],
we have
bE(M k
1 , M k
2 , M ∗, hk)
≤
E(M k
1 , M k
2 , M ∗, hk) + ϕ ≤ϕ, Thus M∗is not eliminated.
We prove the second result by
induction.
Firstly c
M0 = M0.
Assume Mk−1 ⊂
c
Mk−1.
For all M ∈Mk, we have
Ahk(M k
1 , M k
2 , M) ≤E(M k
1 , M k
2 , M, hk) ≤bE(M k
1 , M k
2 , M, hk) + ϕ ≤2ϕ by Line 13 of Al-
gorithm 4. Therefore M ∈c
Mk, we complete the proof."
"N
E",0.7013513513513514,"Step 4: Terminate or rank increase
The following Lemma is key to the ‘alternate pessimism
principle’. Intuitively, it indicates that the algorithm either terminates and outputs a pair of policies
with small duality gap, or detects a time step that provides useful information (large Bellman error).
Lemma C.9. Suppose for round k, the event in Lemma C.7 holds and M ∗is not eliminated. Then if
the algorithm terminate, it outputs a policy pair πk such that V ∗(x1) −V πk,ν∗
πk (x1) ≤ϵ; otherwise
Ahk((M k
1 , M k
2 ), M k
i ) ≥κϵ"
"N
E",0.7027027027027027,8H for some i ∈[2].
"N
E",0.7040540540540541,"Proof. Consider the situation that the algorithm does not terminate. Then with out loss of generality
we can assume |bV k −QM k
1 (x1, πk, νk)| ≥ϵ/2. By Lemma C.7 and Lemma C.6, we have H
X"
"N
E",0.7054054054054054,"h=1
L(M k
1 , M k
2 , M k
1 , h)"
"N
E",0.7067567567567568,"= |QM(x1, π, ν) −V π,ν
1
(x1)|"
"N
E",0.7081081081081081,"≥|bV k −QM k
1 (x1, πk, νk)| −
V πk,νk −bV k"
"N
E",0.7094594594594594,≥3ϵ/8.
"N
E",0.7108108108108108,"By pigeonhole principle, these exists h ∈[H] such that |L(M k
1 , M k
2 , M k
1 , h)| ≥
3ϵ
8H . For this h we
have | bL(M k
1 , M k
2 , M k
1 , h)| ≥
3ϵ
8H −
ϵ
8H =
ϵ
4H , thus the hk in Line 11 is well defined and we have
| bL(M k
1 , M k
2 , M k
1 , hk)| ≥ϵ/(4H). Using Lemma C.7 again, we have"
"N
E",0.7121621621621622,"Ahk((M k
1 , M k
2 ), M k
i ) ≥κ · |L(M k
1 , M k
2 , M k
1 , hk)| ≥κϵ 8H ."
"N
E",0.7135135135135136,"If the algorithm terminates, we have"
"N
E",0.7148648648648649,"QM k
1 (x1, πk, νk) ≥QM k
1 (x1, πk, νM k
1
πk ) ≥V ∗(x1)"
"N
E",0.7162162162162162,"and QM k
2 (x1, πk, νk) ≤V πk,ν∗
πk (x1). Therefore"
"N
E",0.7175675675675676,"V ∗(x1) −V πk,ν∗
πk (x1) ≤QM k
1 (x1, πk, νk) −QM k
2 (x1, πk, νk)"
"N
E",0.7189189189189189,"≤|QM k
1 (x1, πk, νk) −bV πk,νk| + |bV πk,νk −QM k
2 (x1, πk, νk)|
≤ϵ."
"N
E",0.7202702702702702,We thus complete the proof.
"N
E",0.7216216216216216,"Step 5: Bounding the number of iterations.
This step uses the technique of Jiang et al. (2017);
Sun et al. (2019a), which uses the volume of minimum volume enclosing ellipsoid as a potential
function to bound the number of iterations. The key result is presented in Lemma C.10."
"N
E",0.722972972972973,"Lemma C.10. Suppose for every round k, the event in Lemma C.7 holds, then the number of iterations
of Algorithm 4 is at most HWκ log( β"
"N
E",0.7243243243243244,2ϕ)/ log(5/3) for certain i ∈[2].
"N
E",0.7256756756756757,Published as a conference paper at ICLR 2022
"N
E",0.727027027027027,"Proof. If
the
algorithm
does
not
terminate
at
round
k,
then
by
Lemma
C.9,
⟨ζhk(M k
1 , M k
2 ), χhk(M k
i )⟩= Ahk((M k
1 , M k
2 ), M k
i ) ≥
κϵ
8H
= 6√Wκϕ.
Let Ok
h denote the
origin-centered minimum volume enclosing ellipsoid of {χh(M) : M ∈c
Mk}. Let Ok−1,+
h
denote
the origin-centered minimum volume enclosing ellipsoid of {v ∈Ok−1
h
: ⟨ζhk(M k
1 , M k
2 ), v⟩≤2ϕ}.
By Lemma F.2, we have"
"N
E",0.7283783783783784,"vol(Ok
hk)"
"N
E",0.7297297297297297,"vol(Ok−1
hk )
≤vol(Ok−1,+
hk
)"
"N
E",0.731081081081081,"vol(Ok−1
hk )
≤3/5."
"N
E",0.7324324324324324,"Define Φ = supM1,M2∈M,h∈[H] ∥ζh(M1, M2)∥2 and Ψ = supM∈M,h∈[H] ∥χh(M)∥2. Then we
have vol(O0
h) ≤VWκ(1) · ΨWκ where VWκ(R) is the volume of Euclidean ball in RWκ with radius
R. On the other hand, we have"
"N
E",0.7337837837837838,"vol(Ok
h) ≥vol({q ∈RWκ : ∥q∥2 ≤2ϕ/Φ}) ≥VWκ(1) · (2ϕ/Φ)Wκ."
"N
E",0.7351351351351352,It thus follows that the number of iterations for each h is at most Wκ log( ΦΨ
"N
E",0.7364864864864865,"2ϕ )/ log(5/3). Using
β ≥ΦΨ, this completes the proof."
"N
E",0.7378378378378379,"Step 6: Putting everything together.
From Lemma C.7, with high probability the events holds.
Therefore for the first T rounds, we can apply Lemma C.10 and know that the algorithm indeed
terminates in at most T rounds. The number of trajectories is thus upper bounded by (n1 + n)T =
O( H3W 2
κ|A|
κ2ϵ2
log(T|G||M|/p))."
"N
E",0.7391891891891892,"D
THEORY FOR ALTERNATE OPTIMISTIC VALUE ELIMINATION (AOVE)"
"N
E",0.7405405405405405,"We prove Theorem 4.4 in the following five steps. In the first step we show that the functions in
the constraint set has low Bellman error. Notice that different from Lemma B.1, here we consider a
different Bellman operator in Eq (2) and consider all policies in policy class Π. In the second step
we show that Qπ,ν∗
π belongs to the constraint set for all π ∈Π and in particular Q∗belongs to the
constraint set. In the third step we perform a regret decomposition. Notice that here we consider
two Bellman errors. We then bound the summation of these Bellman errors by Minimax Eluder
dimension in step 4 and complete the proof in step 5."
"N
E",0.7418918918918919,Step 1: Low Bellman errors in the constraint set.
"N
E",0.7432432432432432,"Lemma D.1 (Concentration on Bellman error). Let ρ > 0 be an arbitrary fixed number. With
probability at least 1 −p for all (k, h) ∈[K] × [H] we have (a) k−1
X i=1"
"N
E",0.7445945945945946," 
f k
h(xi
h, ai
h, bi
h) −ri
h −
E
x∼P(·|xi
h,ai
h,bi
h) f k
h+1(x, πk
h+1, (νf k"
"N
E",0.745945945945946,"πk)h+1))
2 ≤O(β) (b) k−1
X i=1"
"N
E",0.7472972972972973," 
gk
h(xi
h, ai
h, bi
h) −ri
h −
E
x∼P(·|xi
h,ai
h,bi
h) gk
h+1(x, πk
h+1, (νgk"
"N
E",0.7486486486486487,"πk)h+1))
2 ≤O(β)"
"N
E",0.75,"Proof. For simplicity we only proof (a), and (b) follows similarly."
"N
E",0.7513513513513513,"Consider a fixed (k, h, f, π) tuple, let"
"N
E",0.7527027027027027,"Ut(h, f, π) :=
 
fh(xt
h, at
h, bt
h) −rt
h −fh+1(xt
h+1, π, νf
π)
2"
"N
E",0.754054054054054,"−
 
rt
h +
E
x∼P(·|xt
h,at
h,bt
h) fh+1(x, π, νf
π) −rt
h −fh+1(xt
h+1, π, νf
π)
2"
"N
E",0.7554054054054054,"and Ft,h be the filtration induced by {xi
1, ai
1, bi
1, ri
1, . . . , xi
H}t−1
i=1 ∪{xt
1, at
1, bt
1, rt
1, . . . , xt
h, at
h, bt
h}."
"N
E",0.7567567567567568,Published as a conference paper at ICLR 2022
"N
E",0.7581081081081081,We have
"N
E",0.7594594594594595,"E[Ut(h, f, π)|Ft,h]"
"N
E",0.7608108108108108,"= [(fh(xt
h, at
h, bt
h) −rt
h −
E
x∼P(·|·,·,·) fh+1(x, π, νf
π))(xt
h, at
h, bt
h)]"
"N
E",0.7621621621621621,"· E[fh(xt
h, at
h, bt
h) + rt
h +
E
x∼P(·|xt
h,at
h,bt
h) fh+1(x, π, νf
π) −2rt
h −2fh+1(xt
h+1, π, νf
π)|Ft,h]"
"N
E",0.7635135135135135,"= [fh(xt
h, at
h, bt
h) −rt
h −
E
x∼P(·|xt
h,at
h,bt
h) fh+1(x, π, νf
π)]2 and"
"N
E",0.7648648648648648,"Var[Ut(h, f, π)|Ft,h] ≤E[(Ut(h, f, π))2|Ft,h]"
"N
E",0.7662162162162162,"= [(fh −rh −
E
x∼P(·|·,·,·) fh+1(x, π, νf
π))(xt
h, at
h, bt
h)]2"
"N
E",0.7675675675675676,"· E[
 
fh(xt
h, at
h, bt
h) + rt
h +
E
x∼P(·|xt
h,at
h,bt
h) fh+1(x, π, νf
π) −2rt
h −2fh+1(xt
h+1, π, νf
π)
2|Ft,h]"
"N
E",0.768918918918919,"≤36[fh(xt
h, at
h, bt
h) −rt
h −
E
x∼P(·|xt
h,at
h,bt
h) fh+1(x, π, νf
π)]2 = 36 E[Ut(h, f, π)|Ft,h]."
"N
E",0.7702702702702703,"By Freedman’s inequality, we have with probability at least 1 −p,  k
X"
"N
E",0.7716216216216216,"t=1
Ut(h, f, π) − k
X"
"N
E",0.772972972972973,"t=1
E[Ut(h, f, π)|Ft,h]"
"N
E",0.7743243243243243,"≤O
v
u
u
tlog(1/p) k
X"
"N
E",0.7756756756756756,"t=1
E[Ut|Ft,h] + log(1/p)

."
"N
E",0.777027027027027,"Let Nρ × Yρ be a ρ-cover of F × Π. The covering of Π is in-terms of the distance of two policies
defined as
d(π, π′) :=
max
f∈F,x∈S,b∈A |f(x, π, b) −f(x, π′, b)|."
"N
E",0.7783783783783784,"Taking a union bound for all (k, h, f, p) ∈[K] × [H] × Nρ × Yρ, we have that with probability at
least 1 −p for all (k, h, f, p) ∈[K] × [H] × Nρ × Yρ k
X"
"N
E",0.7797297297297298,"t=1
Ut(h, f, p) − k
X"
"N
E",0.7810810810810811,"t=1
[fh(xt
h, at
h, bt
h) −rt
h −
E
x∼P(·|xt
h,at
h,bt
h) fh+1(x, p, νf
p)]2"
"N
E",0.7824324324324324,"≤O
v
u
u
tι k
X"
"N
E",0.7837837837837838,"t=1
[fh(xt
h, at
h, bt
h) −rt
h −
E
x∼P(·|xt
h,at
h,bt
h) fh+1(x, p, νf
p)]2 + ι

(22)"
"N
E",0.7851351351351351,"where ι = log(HK|Nρ||Yρ|/p). For an arbitrary (h, k) ∈[H] × [K] pair, by the definition of Vk"
"N
E",0.7864864864864864,"and (πk, f k) ∈Vk we have k
X"
"N
E",0.7878378378378378,"t=1
Ut(h, f k, πk) − k
X t=1"
"N
E",0.7891891891891892," 
rt
h +
E
x∼P(·|xt
h,at
h,bt
h) f k
h+1(x, πk, νf k"
"N
E",0.7905405405405406,"πk) −rt
h −f k
h+1(xt
h+1, πk, νf k"
"N
E",0.7918918918918919,"πk)
2 ≤ k
X t=1"
"N
E",0.7932432432432432," 
f k
h(xt
h, at
h, bt
h) −rt
h −f k
h+1(xt
h+1, πk, νf k"
"N
E",0.7945945945945946,"πk)
2"
"N
E",0.7959459459459459,"−inf
g∈F k
X t=1"
"N
E",0.7972972972972973," 
gk
h(xt
h, at
h, bt
h) −rt
h −f k
h+1(xt
h+1, πk, νf k"
"N
E",0.7986486486486486,"πk)
2"
"N
E",0.8,"≤O(ι + kρ)
(23)"
"N
E",0.8013513513513514,where in the first step we use Assumption 3.2.
"N
E",0.8027027027027027,Published as a conference paper at ICLR 2022
"N
E",0.8040540540540541,"Define fk = argminf∈Nρ maxh∈[H] ∥f k
h −fk
h∥∞, pk = argminp∈Yρd(πk, p). Since Nρ is a ρ-cover
of F, k
X"
"N
E",0.8054054054054054,"t=1
Ut(h, f k, πk) − k
X"
"N
E",0.8067567567567567,"t=1
Ut(h, fk, pk)"
"N
E",0.8081081081081081,"≤O(kρ).
(24)"
"N
E",0.8094594594594594,"Therefore combining Eq (23) and Eq (24) k
X"
"N
E",0.8108108108108109,"t=1
Ut(h, fk, pk) ≤O(ι + kρ).
(25)"
"N
E",0.8121621621621622,"Recall Eq (22) indicates k
X"
"N
E",0.8135135135135135,"t=1
Ut(h, fk, pk) − k
X"
"N
E",0.8148648648648649,"t=1
[fk
h(xt
h, at
h, bt
h) −rt
h −
E
x∼P(·|xt
h,at
h,bt
h) fk
h+1(x, pk, νfk pk)]2"
"N
E",0.8162162162162162,"≤O
v
u
u
tι k
X"
"N
E",0.8175675675675675,"t=1
[fk
h(xt
h, at
h, bt
h) −rt
h −
E
x∼P(·|xt
h,at
h,bt
h) fk
h+1(x, pk, νfk
pk)]2 + ι

(26)"
"N
E",0.8189189189189189,"Combining Eq (25) and Eq (26) we have k
X"
"N
E",0.8202702702702702,"t=1
[fk
h(xt
h, at
h, bt
h) −rt
h −
E
x∼P(·|xt
h,at
h,bt
h) fk
h+1(x, pk, νfk"
"N
E",0.8216216216216217,pk)]2 ≤O(ι + kρ).
"N
E",0.822972972972973,"Since fk is an ρ-approximation to f k and pk is an ρ-approximation to πk, we can conclude the proof
of (a) with k−1
X i=1"
"N
E",0.8243243243243243," 
f k
h(xi
h, ai
h, bi
h) −ri
h −
E
x∼P(·|xi
h,ai
h,bi
h) f k
h+1(x, πk
h+1, (νf k"
"N
E",0.8256756756756757,"πk)h+1))
2 ≤O(β)."
"N
E",0.827027027027027,"Step 2: (π, Qπ,ν∗
π), ∀π ∈Π is always in constraint set.
Lemma D.2. With probability at least 1 −p, we have (π∗, Q∗) and (π, Qπ,∗), ∀π ∈Π all belong to
Vk for all k ∈[K]."
"N
E",0.8283783783783784,"Proof. Consider a fixed (k, h, f, π) tuple, let"
"N
E",0.8297297297297297,"Ut(h, f, π) :=
 
fh(xt
h, at
h, bt
h) −rt
h −Qπ,ν∗
π
h+1 (xt
h+1, π, ν∗
π)
2"
"N
E",0.831081081081081,"−
 
Qπ,ν∗
π
h
(xt
h, at
h, bt
h) −rt
h −Qπ,ν∗
π
h+1 (xt
h+1, π, ν∗
π
2"
"N
E",0.8324324324324325,"and Ft,h be the filtration induced by {xi
1, ai
1, bi
1, ri
1, . . . , xi
H}t−1
i=1 ∪{xt
1, at
1, bt
1, rt
1, . . . , xt
h, at
h, bt
h}."
"N
E",0.8337837837837838,We have
"N
E",0.8351351351351352,"E[Ut(h, f, π)|Ft,h] = [(fh −Qπ,ν∗
π
h
)(xt
h, at
h, bt
h)]2 and"
"N
E",0.8364864864864865,"Var[Ut(h, f, π)|Ft,h] ≤36[(fh −Qπ,ν∗
π
h
)(xt
h, at
h, bt
h)]2 = 36 E[Ut(h, f, π)|Ft,h]."
"N
E",0.8378378378378378,"By Freedman’s inequality, we have with probability at least 1 −p, k
X"
"N
E",0.8391891891891892,"t=1
Ut(h, f, π) − k
X"
"N
E",0.8405405405405405,"t=1
[(fh −Qπ,ν∗
π
h
)(xt
h, at
h, bt
h)]2"
"N
E",0.8418918918918918,"≤O
v
u
u
tlog(1/p) k
X"
"N
E",0.8432432432432433,"t=1
[(fh −Qπ,ν∗
π
h
)(xt
h, at
h, bt
h)]2 + log(1/p)

."
"N
E",0.8445945945945946,Published as a conference paper at ICLR 2022
"N
E",0.845945945945946,"Let Nρ ×Yρ be a ρ-cover of F ×Π. Taking a union bound for al (k, h, f, p) ∈[K]×[H]×Nρ ×Yρ,
we have that with probability at least 1 −p for all (k, h, f, p) ∈[K] × [H] × Nρ × Yρ − k
X"
"N
E",0.8472972972972973,"t=1
Ut(h, f, p) ≤O(ι)"
"N
E",0.8486486486486486,"where ι = log(HK|Nρ|/p). This implies for all (k, h, f, p) ∈[K] × [H] × Nρ × Yρ k
X t=1"
"N
E",0.85," 
Q
p,ν∗
p
h
(xt
h, at
h, bt
h) −rt
h −Q
p,ν∗
p
h+1(xt
h+1, p, ν∗
p)
2 ≤ k
X t=1"
"N
E",0.8513513513513513," 
fh(xt
h, at
h, bt
h) −rt
h −Q
p,ν∗
p
h+1(xt
h+1, p, ν∗
p)
2 + O(ι + kρ)."
"N
E",0.8527027027027027,"Since Nρ × Yρ be a ρ-cover of F × Π, for all pk ∈Yρ k
X t=1"
"N
E",0.8540540540540541," 
Q
p,ν∗
p
h
(xt
h, at
h, bt
h) −rt
h −Q
p,ν∗
p
h+1(xt
h+1, p, ν∗
p)
2"
"N
E",0.8554054054054054,"≤inf
g∈F k
X t=1"
"N
E",0.8567567567567568," 
gh(xt
h, at
h, bt
h) −rt
h −Q
p,ν∗
p
h+1(xt
h+1, p, ν∗
p)
2 + O(ι + kρ)."
"N
E",0.8581081081081081,"Again using Nρ × Yρ be a ρ-cover of F × Π, we have for all π ∈Π k
X t=1"
"N
E",0.8594594594594595," 
Qπ,ν∗
π
h
(xt
h, at
h, bt
h) −rt
h −Qπ,ν∗
π
h+1 (xt
h+1, π, ν∗
π)
2"
"N
E",0.8608108108108108,"≤inf
g∈F k
X t=1"
"N
E",0.8621621621621621," 
gh(xt
h, at
h, bt
h) −rt
h −Qπ,ν∗
π
h+1 (xt
h+1, π, ν∗
π)
2 + O(ι + kρ)."
"N
E",0.8635135135135135,"Therefore we have (π, Qπ,ν∗
π), ∀π ∈Π all belong to Vk for all k ∈[K] with probability at least
1 −p."
"N
E",0.8648648648648649,Remark D.3. An important implication of this fact is the following
"N
E",0.8662162162162163,"V ∗−V πk,ν∗
πk ≤f k
1 (xk
1, πk, νf k"
"N
E",0.8675675675675676,"πk) −gk
1(xk
1, πk, νk)"
"N
E",0.8689189189189189,which gives an upper bound of the one-side duality gap.
"N
E",0.8702702702702703,"Step 3: Bounding the regret by Bellman error and martingale.
Lemma D.4. Define"
"N
E",0.8716216216216216,"δk
h := f k
h(xk
h, πk
h, (νf k"
"N
E",0.8729729729729729,"πk)h) −gk
h(xk
h, πk
h, νk
h)"
"N
E",0.8743243243243243,"ζk
h := E[δk
h+1|xk
h, ak
h, bk
h] −δk
h+1
γk
h :=
E
a∼πk
h,b∼νk
h
[f k
h(xk
h, a, b)] −f k
h(xk
h, ak
h, bk
h)"
"N
E",0.8756756756756757,"bγk
h :=
E
a∼πk
h,b∼νk
h
[gk
h(xk
h, a, b)] −gk
h(xk
h, ak
h, bk
h)"
"N
E",0.8770270270270271,Then we have
"N
E",0.8783783783783784,"δk
h ≤δk
h+1 + ζk
h + ϵk
h −bϵk
h + γk
h −bγk
h"
"N
E",0.8797297297297297,"where ϵk
h,bϵk
h are the bellman error defined by"
"N
E",0.8810810810810811,"ϵk
h := f k
h(xk
h, ak
h, bk
h) −rk
h −
E
x∼P(·|xk
h,ak
h,bk
h) f k
h(x, πk
h+1, (νf k"
"N
E",0.8824324324324324,πk)h+1)
"N
E",0.8837837837837837,"bϵk
h := gk
h(xk
h, ak
h, bk
h) −rk
h −
E
x∼P(·|xk
h,ak
h,bk
h) gk
h(x, πk
h+1, νk
h+1)."
"N
E",0.8851351351351351,Published as a conference paper at ICLR 2022
"N
E",0.8864864864864865,Proof. By definition of (νf k
"N
E",0.8878378378378379,"πk)h = argminν∈Πhf k
h(xk
h, πk
h, ν), we have"
"N
E",0.8891891891891892,"f k
h(xk
h, πk
h, (νf k"
"N
E",0.8905405405405405,"πk)h) ≤f k
h(xk
h, πk
h, νk
h)"
"N
E",0.8918918918918919,"= f k
h(xk
h, ak
h, bk
h) + γk
h
It thus follows that"
"N
E",0.8932432432432432,"δk
h = f k
h(xk
h, πk
h, (νf k"
"N
E",0.8945945945945946,"πk)h) −gk
h(xk
h, πk
h, νk
h)"
"N
E",0.8959459459459459,"≤f k
h(xk
h, ak
h, bk
h) −gk
h(xk
h, ak
h, bk
h) + γk
h −bγk
h"
"N
E",0.8972972972972973,"=
 
rk
h +
E
x∼P(·|xk
h,ak
h,bk
h) f k
h(x, πk
h+1, (νf k"
"N
E",0.8986486486486487,"πk)h+1) + ϵk
h
"
"N
E",0.9,"−
 
rk
h +
E
x∼P(·|xk
h,ak
h,bk
h) gk
h(x, πk
h+1, νk
h+1) + bϵk
h

+ γk
h −bγk
h"
"N
E",0.9013513513513514,"=
E
x∼P(·|xk
h,ak
h,bk
h) f k
h(x, πk
h+1, (νf k"
"N
E",0.9027027027027027,"πk)h+1) −
E
x∼P(·|xk
h,ak
h,bk
h) gk
h(x, πk
h+1, νk
h+1)"
"N
E",0.904054054054054,"+ ϵk
h −bϵk
h + γk
h −bγk
h
= δk
h+1 + ζk
h + ϵk
h −bϵk
h + γk
h −bγk
h."
"N
E",0.9054054054054054,Thus we complete the proof.
"N
E",0.9067567567567567,"Step 4: Bounding cumulative Bellman error using Minimax Eluder dimension.
Lemma D.5 (Bounding Bellman error). For any (k, h) ∈[K] × [H] we have k
X"
"N
E",0.9081081081081082,"t=1
|ϵt
h| ≤O
q"
"N
E",0.9094594594594595,"k · dimME′(F,
p"
"N
E",0.9108108108108108,"1/K) log[N(F, 1/K) · N(Π, 1/K) · HK/p]
 k
X"
"N
E",0.9121621621621622,"t=1
|bϵt
h| ≤O
q"
"N
E",0.9135135135135135,"k · dimME′(F,
p"
"N
E",0.9148648648648648,"1/K) log[N(F, 1/K) · N(Π, 1/K) · HK/p]

."
"N
E",0.9162162162162162,Proof. Let
"N
E",0.9175675675675675,"lt(·, ·, ·) = f t
h(·, ·, ·) −rh(·, ·, ·) −
E
x∼P(·,·,·) f t
h+1(x, πt, νf t πt)"
"N
E",0.918918918918919,"and ρt = δ(xt
h, at
h, bt
h). Similarly, we can also let ρt be generated by (πt, νt). Then Lemma D.1
indicates k
X"
"N
E",0.9202702702702703,"t=1
l2
k(ρt) ≤O(β)."
"N
E",0.9216216216216216,"Based on this, we can apply Lemma F.1 to obtain the first inequality. The second follows similarly."
"N
E",0.922972972972973,"Step 5: Putting them all together.
We can upper bound the regret as follows: K
X"
"N
E",0.9243243243243243,"k=1
[V ∗−V πk,ν∗
πk ] ≤ K
X"
"N
E",0.9256756756756757,"k=1
[f k
1 (xk
1, πk, νf k"
"N
E",0.927027027027027,"πk) −gk
1(xk
1, πk, νk)] ≤ K
X k=1 H
X"
"N
E",0.9283783783783783,"h=1
(ζk
h + ϵk
h −bϵk
h + γk
h −bγk
h) = H
X h=1 K
X"
"N
E",0.9297297297297298,"k=1
(ζk
h + γk
h −bγk
h) + H
X h=1 K
X"
"N
E",0.9310810810810811,"k=1
(ϵk
h −bϵk
h)
(27)"
"N
E",0.9324324324324325,"where the first step comes from Lemma D.2, the second step comes from Lemma D.4 and the last
step comes from Fubini’s theorem. Notice that the first term in Eq (27) is a martingale difference
sequence and can be bounded by O(H
p"
"N
E",0.9337837837837838,"KH log(HK/p)) with probability at least 1 −p and the
second term can be bounded by Lemma D.5. We thus complete the proof."
"N
E",0.9351351351351351,Published as a conference paper at ICLR 2022
"N
E",0.9364864864864865,"E
EXAMPLES"
"N
E",0.9378378378378378,"In this section we give several examples of Markov games that possess low Minimax Eluder dimension,
including tabular Markov Games, linear function approximation, reproducing kernel Hilbert space
(RKHS) function approximation, overparameterized neural networks, and generalized linear models.
We first consider reproducing kernel Hilbert space (RKHS) function approximation, which captures
many existing models."
"N
E",0.9391891891891891,"Reproducing kernel Hilbert space (RKHS) function approximation. In this setting, the function
class of interest is given by"
"N
E",0.9405405405405406,"Fh = {⟨ϕh(x, a, b), θh⟩: θ, ϕ(·, ·, ·) ∈BR(H)}."
"N
E",0.9418918918918919,"Here ϕ(·, ·, ·) : S × A × A 7→H is a feature map to H, θ ∈H, and BR(H) is the ball centered at
zero with radius R in a Hilbert space H. The effective dimension of a set D ⊂H is characterized
by the critical information gain ed(γ, D). Specifically, for γ > 0 and D ⊂H, we define dn(γ, D) =
maxx1,...,xn∈D log det(I + 1"
"N
E",0.9432432432432433,"γ
Pn
i=1 xix⊤
i ). The critical information gain (Du et al., 2021) is thus"
"N
E",0.9445945945945946,"defined by ed(γ, D) = mink∈N:k≥dn(γ,D) k. The main result is given in the following theorem."
"N
E",0.9459459459459459,"Theorem E.1. In kernel function approximation, the Minimax Eluder dimensions dimME(F, ϵ) and
dimME′(F, ϵ) are upper bounded by maxh∈[H] eγ( ϵ"
"N
E",0.9472972972972973,"9R, Φh), where Φh = {ϕh(x, a, b) : x ∈S, a, b ∈
A}."
"N
E",0.9486486486486486,"Proof. We only show dimME′(F, ϵ) ≤maxh∈[H] eγ( ϵ"
"N
E",0.95,"9R, Φh), and the other part is totally sim-
ilar.
Fix an h ∈[H].
Due to completeness, for any π ∈Π and f ∈F we have (fh −
T π
h fh+1)(x, a, b) = ⟨ϕh(x, a, b), θh −T π
h θh+1⟩where T π
h θh+1 is the unique feature in H such
that T π
h fh+1 = ⟨ϕh(x, a, b), T π
h θh+1⟩. Consider the longest sequence ν1, . . . , νk ∈D∆such that
each one item is ϵ-independent of its precedents, distinguished by a sequence of features θ(1)
h , . . . , θ(k)
h .
To simplify notations, we define ϕi = Eνi[ϕh(x, a, b)] and θi = θ(i)
h −T π
h θ(i)
h+1 for all i ∈[k]. Let"
"N
E",0.9513513513513514,"Λi = Pi−1
τ=1 ϕτϕ⊤
τ +
ϵ2
9R2 I. Then Definition 3.3 gives"
"N
E",0.9527027027027027,"∥ϕi∥Λ−1
i
≥1"
"N
E",0.9540540540540541,"ϵ · ∥ϕi∥Λ−1
i
· ∥θi∥Λi ≥1"
"N
E",0.9554054054054054,"ϵ · |⟨ϕi, θi⟩| ≥1."
"N
E",0.9567567567567568,"where the first step comes from θi ∈BR(H) and Pi−1
j=1⟨ϕj, θj⟩2 ≤ϵ, the second step comes from
Hölder’s inequality and the final step comes from |⟨ϕi, θi⟩| ≥ϵ. Thus we have k ≤ k
X"
"N
E",0.9581081081081081,"i=1
log(1 + ∥ϕi∥Λ−1
i ) = log det(I + 9R2 ϵ2 k
X"
"N
E",0.9594594594594594,"i=1
ϕiϕ⊤
i )."
"N
E",0.9608108108108108,"By the definition of critical information gain, k must be less or equal than eγ( ϵ"
"N
E",0.9621621621621622,"9R, Φh)."
"N
E",0.9635135135135136,This result implies several important cases in general function approximation.
"N
E",0.9648648648648649,"• Linear function approximation.
In this setting rh(x, a, b) = ϕ(x, a, b)⊤θh and
Ph(·|x, a, b) = ϕ(x, a, b)⊤µh(·) where ϕ(x, a, b) ∈Rd. We have shown that this set-
ting satisfies all the realizability and completeness assumptions in the previous sections. By
standard Ellipsoid potential arguments, the critical information gain is upper bounded by
O(d · log(R/ϵ)). Therefore, this setting also has low Minimax Eluder dimensions.
• Tabular Markov Games.
This setting can be seen as a special case of linear function
approximation where the feature vectors are chosen to be one-hot representations of state-
actions pairs. In this case, d = |S| · |A|2 and thus Minimax Eluder dimensions is upper
bounded by eO(|S| · |A|2).
• Overparameterized neural networks. In overparameterized neural network function
approximations, the functions are given by h(ϕ(x, a, b)) where h is the Neural Tangent
Random Feature space defined in Cao & Gu (2019). We rewrite the information gain
dn(γ, D) as maxx1,...,xn∈D log det(I + 1"
"N
E",0.9662162162162162,"γ Kn) where (Kn)i,j = K(xi, xj) and K is the
Neural Tangent Kernel. It has been shown (Jacot et al., 2018; Du et al., 2019c;a; Allen-Zhu"
"N
E",0.9675675675675676,Published as a conference paper at ICLR 2022
"N
E",0.9689189189189189,"et al., 2019) that with proper initialization and learning rates, the function approximations
approximately lie in the Reproducing kernel Hilbert space given by the neural tangent
feature as above. Therefore the Minimax Eluder dimensions are bounded by eγ( ϵ"
"N
E",0.9702702702702702,"9R, Φh) + λ,
where eγ is the critical information gain defined by the dn(γ, D) rewritten above and λ is a
term that vanishes as width tends to infinity."
"N
E",0.9716216216216216,"Generalized linear models. Finally, we consider generalized linear model as an extension of the
linear function approximations.
Theorem E.2. Suppose (fh −T π
h fh+1)(x, a, b) = g(ϕ(x, a, b)⊤θh) for any π ∈Π and f ∈F,
where g is a differentiable and strictly increasing function and ϕ, θh ∈Rd. Assume g′ ∈(c1, c2) and
∥ϕ∥2, ∥θh∥2 ≤R for c1, c2, R > 0. Then dimME′(F, ϵ) ≤O(d(c2/c1)2 log( c2R"
"N
E",0.972972972972973,c1ϵ )).
"N
E",0.9743243243243244,"Proof. The proof mirrors those in Theorem E.1. For the longest sequence ν1, . . . , νk ∈D∆such
that each one item is ϵ-independent of its precedents, distinguished by a sequence of features
θ(1)
h , . . . , θ(k)
h , we define ϕi = Eνi[ϕh(x, a, b)] and θi = θ(i)
h
−T π
h θ(i)
h+1 for all i ∈[k]. Let"
"N
E",0.9756756756756757,"Λi = Pi−1
τ=1 ϕτϕ⊤
τ +
ϵ2"
"N
E",0.977027027027027,"9R2c2
1 I. Then Definition 3.3 gives"
"N
E",0.9783783783783784,"∥ϕi∥Λ−1
i
≥c1"
"N
E",0.9797297297297297,"ϵ · ∥ϕi∥Λ−1
i
· ∥θi∥Λi ≥c1"
"N
E",0.981081081081081,"ϵ · |⟨ϕi, θi⟩| ≥c1 c2
."
"N
E",0.9824324324324324,"This means det(Λk) ≥(1 + c2
1
c2
2 )k−1. But by Elliptic potential lemma, det(Λk) ≤( R2k"
"N
E",0.9837837837837838,"d
+
ϵ2"
"N
E",0.9851351351351352,"9R2c2
1 )d."
"N
E",0.9864864864864865,Combining these two inequalities gives k ≤O(d(c2/c1)2 log( c2R
"N
E",0.9878378378378379,c1ϵ )).
"N
E",0.9891891891891892,"F
TECHNICAL CLAIMS"
"N
E",0.9905405405405405,"This section lists the facts we use from the previous work.
Lemma F.1 (Cumulative error control, Lemma 26 of Jin et al. (2021a)). Given a function class G
defined on X with |g(x)| ≤C for all (g, x) ∈G × X and a family of probability measures D over X.
Suppose sequence {gk}K
k=1 ⊂G and {ρk}H
k=1 ⊂D such that for all k ∈[K], Pk−1
t=1 (Eρt[gk])2 ≤β.
Then for all k ∈[K] and ω > 0, k
X"
"N
E",0.9918918918918919,"t=1
| E
ρt[gt]| ≤O
p"
"N
E",0.9932432432432432,"dimDE(G, D, ϵ)βk + min{k, dimDE(G, D, ϵ)}C + kω

."
"N
E",0.9945945945945946,"Lemma F.2 (Lemma 11 of Jiang et al. (2017)). Consider a closed and bounded set V ⊂Rd
and a vector p ∈Rd. Let B be any origin-centered enclosing ellipsoid of V . Suppose there
exists v ∈V such that p⊤v ≥κ and define B+ as the minimum volume enclosing ellipsoid of
{v ∈B : |p⊤v| ≤
κ
3
√"
"N
E",0.995945945945946,"d}. With vol(·) denoting the (Lebesgue) volume, we have:"
"N
E",0.9972972972972973,vol(B+)
"N
E",0.9986486486486487,vol(B) ≤3 5.
