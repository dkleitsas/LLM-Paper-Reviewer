Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0011614401858304297,"The test loss of well-trained neural networks often follows precise power-law
scaling relations with either the size of the training dataset or the number of
parameters in the network. We propose a theory that explains and connects these
scaling laws. We identify variance-limited and resolution-limited scaling behavior
for both dataset and model size, for a total of four scaling regimes. The variance-
limited scaling follows simply from the existence of a well-behaved inﬁnite data
or inﬁnite width limit, while the resolution-limited regime can be explained by
positing that models are effectively resolving a smooth data manifold. In the
large width limit, this can be equivalently obtained from the spectrum of certain
kernels, and we present evidence that large width and large dataset resolution-
limited scaling exponents are related by a duality. We exhibit all four scaling
regimes in the controlled setting of large random feature and pretrained models and
test the predictions empirically on a range of standard architectures and datasets.
We also observe several empirical relationships between datasets and scaling
exponents: super-classing image tasks does not change exponents, while changing
input distribution (via changing datasets or adding noise) has a strong effect. We
further explore the effect of architecture aspect ratio on scaling exponents."
SCALING LAWS FOR NEURAL NETWORKS,0.0023228803716608595,"1
SCALING LAWS FOR NEURAL NETWORKS"
SCALING LAWS FOR NEURAL NETWORKS,0.003484320557491289,"For a large variety of models and datasets, neural network performance has been empirically observed
to scale as a power-law with model size and dataset size (Hestness et al., 2017; Kaplan et al., 2020;
Rosenfeld et al., 2020b; Henighan et al., 2020). These exponents determine how quickly performance
improves with more data and larger models. We would like to understand why these power-laws
emerge, and what features of the data and models determine the values of the power law exponents."
SCALING LAWS FOR NEURAL NETWORKS,0.004645760743321719,"In this work, we present a theoretical framework for understanding scaling laws in trained neural
networks. We identify four related scaling regimes with respect to the number of model parameters
P and the dataset size D. With respect to each of D, P, there is both a variance-limited regime and a
resolution-limited regime."
SCALING LAWS FOR NEURAL NETWORKS,0.005807200929152149,"Variance-Limited Regime
In the limit of inﬁnite data or an arbitrarily wide model, some aspects
of neural network training simplify. Speciﬁcally, if we ﬁx one of D, P and study scaling with respect
to the other parameter as it becomes arbitrarily large, then the difference between the ﬁnite test loss
and its limiting value scales as 1/x, i.e. as a power-law with exponent 1, with x = D or
√"
SCALING LAWS FOR NEURAL NETWORKS,0.006968641114982578,"P ∝width
in deep networks and x = D or P in linear models."
SCALING LAWS FOR NEURAL NETWORKS,0.008130081300813009,"Resolution-Limited Regime
In this regime, one of D or P is effectively inﬁnite, and we study
scaling as the other parameter increases. In this case, a variety of works have empirically observed
power-law scalings 1/xα, typically with 0 < α < 1 for both x = P or D. We derive exponents in
this regime precisely in the setting of random feature models (c.f. next section). Empirically, we ﬁnd
that our theoretical predictions for exponents hold in pretrained, ﬁne-tuned models even though these
lie outside our theoretical setting."
SCALING LAWS FOR NEURAL NETWORKS,0.009291521486643438,Under review as a conference paper at ICLR 2022
SCALING LAWS FOR NEURAL NETWORKS,0.010452961672473868,"101
102
103
104"
SCALING LAWS FOR NEURAL NETWORKS,0.011614401858304297,"Dataset size (D) 10
8 10
6 10
4 10
2 100"
SCALING LAWS FOR NEURAL NETWORKS,0.012775842044134728,"Loss - Loss(
)"
SCALING LAWS FOR NEURAL NETWORKS,0.013937282229965157,"Variance-limited : Theory 
D = 1"
SCALING LAWS FOR NEURAL NETWORKS,0.015098722415795587,D: 1.02
SCALING LAWS FOR NEURAL NETWORKS,0.016260162601626018,D: 1.01
SCALING LAWS FOR NEURAL NETWORKS,0.017421602787456445,D: 0.98 (CNN)
SCALING LAWS FOR NEURAL NETWORKS,0.018583042973286876,D: 1.01
SCALING LAWS FOR NEURAL NETWORKS,0.019744483159117306,D: 1.02
SCALING LAWS FOR NEURAL NETWORKS,0.020905923344947737,D: 1.10
SCALING LAWS FOR NEURAL NETWORKS,0.022067363530778164,D: 1.01
SCALING LAWS FOR NEURAL NETWORKS,0.023228803716608595,D: 1.11 (SGD)
SCALING LAWS FOR NEURAL NETWORKS,0.024390243902439025,"103
104"
SCALING LAWS FOR NEURAL NETWORKS,0.025551684088269456,"Dataset size (D) 10
1 100 Loss"
SCALING LAWS FOR NEURAL NETWORKS,0.026713124274099883,Resolution-limited
SCALING LAWS FOR NEURAL NETWORKS,0.027874564459930314,D: 0.26
SCALING LAWS FOR NEURAL NETWORKS,0.029036004645760744,D: 0.37
SCALING LAWS FOR NEURAL NETWORKS,0.030197444831591175,D: 0.40
SCALING LAWS FOR NEURAL NETWORKS,0.0313588850174216,D: 0.58
SCALING LAWS FOR NEURAL NETWORKS,0.032520325203252036,"0.0
0.2
0.4
0.6
0.8
1.0
Interpolating Between Training Points in 4-dimensions 12.4 12.5 12.6 12.7"
SCALING LAWS FOR NEURAL NETWORKS,0.03368176538908246,Predictions
SCALING LAWS FOR NEURAL NETWORKS,0.03484320557491289,Teacher
SCALING LAWS FOR NEURAL NETWORKS,0.036004645760743324,"101
102 Width 10
1 100 Loss"
SCALING LAWS FOR NEURAL NETWORKS,0.03716608594657375,Resolution-limited
SCALING LAWS FOR NEURAL NETWORKS,0.03832752613240418,W: 0.46
SCALING LAWS FOR NEURAL NETWORKS,0.03948896631823461,W: 0.34
SCALING LAWS FOR NEURAL NETWORKS,0.04065040650406504,W: 0.62
SCALING LAWS FOR NEURAL NETWORKS,0.041811846689895474,W: 0.40
SCALING LAWS FOR NEURAL NETWORKS,0.0429732868757259,"102
103
104 Width 10
5 10
4 10
3 10
2 10
1 100"
SCALING LAWS FOR NEURAL NETWORKS,0.04413472706155633,"Loss - Loss(
)"
SCALING LAWS FOR NEURAL NETWORKS,0.04529616724738676,"Variance-limited : Theory 
W = 1"
SCALING LAWS FOR NEURAL NETWORKS,0.04645760743321719,W: 0.98 (MSE)
SCALING LAWS FOR NEURAL NETWORKS,0.047619047619047616,W: 1.03
SCALING LAWS FOR NEURAL NETWORKS,0.04878048780487805,W: 1.02 (ERF)
SCALING LAWS FOR NEURAL NETWORKS,0.04994192799070848,W: 1.01
SCALING LAWS FOR NEURAL NETWORKS,0.05110336817653891,W: 1.00
SCALING LAWS FOR NEURAL NETWORKS,0.05226480836236934,W: 1.03 (ERF)
SCALING LAWS FOR NEURAL NETWORKS,0.053426248548199766,"2
4
6
8 10 12 14 16 18 20 22 24 26"
SCALING LAWS FOR NEURAL NETWORKS,0.0545876887340302,"Dimension 0 5 10 15 20 25 4/
D"
SCALING LAWS FOR NEURAL NETWORKS,0.05574912891986063,"4/
D
2/
D 102 103"
SCALING LAWS FOR NEURAL NETWORKS,0.056910569105691054,Dataset Size
SCALING LAWS FOR NEURAL NETWORKS,0.05807200929152149,"Teacher-Student
CIFAR-10
CIFAR-100
SVHN
FashionMNIST
MNIST"
SCALING LAWS FOR NEURAL NETWORKS,0.059233449477351915,"(a)
(b)"
SCALING LAWS FOR NEURAL NETWORKS,0.06039488966318235,"Figure 1: (a) Four scaling regimes Here we exhibit the four regimes we focus on in this work. (top-left,
bottom-right) Variance-limited scaling of under-parameterized models with dataset size and over-parameterized
models with number of parameters (width) exhibit universal scaling (αD = αW = 1) independent of the
architecture or underlying dataset. (top-right, bottom-left) Resolution-limited over-parameterized models with
dataset or under-parameterized models with model size exhibit scaling with exponents that depend on the details
of the data distribution. These four regimes are also found in random feature (Figure 2a) and pretrained models
(see supplement). (b) Resolution-limited models interpolate the data manifold Linear interpolation between
two training points in a four-dimensional input space (top). We show a teacher model and four student models,
each trained on different sized datasets. In all cases teacher and student approximately agree on the training
endpoints, but as the training set size increases they increasingly match everywhere. (bottom) We show 4/αD
versus the data manifold dimension (input dimension for teacher-student models, intrinsic dimension for standard
datasets). We ﬁnd that the teacher-student models follow the 4/αD (dark dashed line), while the relationship for
a four layer CNN (solid) and WRN (hollow) on standard datasets is less clear."
SCALING LAWS FOR NEURAL NETWORKS,0.06155632984901278,"For more general nonlinear models, we propose a reﬁnement of naive bounds into estimates via
expansions that hold asymptotically. These rely on the idea that additional data (in the inﬁnite
model-size limit) or added model parameters (in the inﬁnite data limit) are used by the model to
carve up the data manifold into smaller components. For smooth manifolds, loss, and network, the
test loss will depend on the linear size of a sub-region, while it is the d-dimensional sub-region
volume that scales inversely with P or D, giving rise to α ∝1/d.1 To test this empirically, we make
measurements of the resolution-limited exponents in neural networks and intrinsic dimension of the
data manifold, shown in Figure 1b."
SCALING LAWS FOR NEURAL NETWORKS,0.0627177700348432,"Explicit Derivation
We derive the scaling laws for these four regimes explicitly in the setting of
random feature teacher-student models, which also applies to neural networks in the large width limit.
This setting allows us to solve for the test error directly in terms of the feature covariance (kernel).
The scaling of the test loss then follows from the asymptotic decay of the spectrum of the covariance
matrix. For generic continuous kernels on a d-dimensional manifold, we can further relate this to the
dimension of the data manifold."
SCALING LAWS FOR NEURAL NETWORKS,0.06387921022067364,Summary of Contributions:
SCALING LAWS FOR NEURAL NETWORKS,0.06504065040650407,"1. We propose four scaling regimes for neural networks. The variance-limited and resolution-
limited regimes originate from different mechanisms, which we identify. To our knowledge,"
SCALING LAWS FOR NEURAL NETWORKS,0.06620209059233449,"1A visualization of this successively better approximation with dataset size is shown in Figure 1b for models
trained to predict data generated by a random fully-connected network."
SCALING LAWS FOR NEURAL NETWORKS,0.06736353077816493,Under review as a conference paper at ICLR 2022
SCALING LAWS FOR NEURAL NETWORKS,0.06852497096399536,"this categorization has not been previously exhibited. We provide empirical support for all four
regimes in deep networks on standard datasets."
SCALING LAWS FOR NEURAL NETWORKS,0.06968641114982578,2. We derive the variance-limited regime under simple yet general assumptions (Theorem 1).
SCALING LAWS FOR NEURAL NETWORKS,0.07084785133565621,"3. We present a hypothesis for resolution-limited scaling through reﬁnement of naive bounds (Theo-
rems 2, 3), for general nonlinear models. We empirically test the dependence of the estimates on
intrinsic dimension of the data manifold for deep networks on standard datasets (Figure 1b)."
SCALING LAWS FOR NEURAL NETWORKS,0.07200929152148665,"4. In the setting of random feature teacher-student networks, we derive both variance-limited and
resolution-limited scaling exponents exactly. In the latter case, we relate this to the spectral decay
of kernels. We identify a novel duality that exists between model and dataset size scaling."
SCALING LAWS FOR NEURAL NETWORKS,0.07317073170731707,"5. We empirically investigate predictions from the random features setting in pretrained, ﬁne-tuned
models on standard datasets and ﬁnd they give excellent agreement."
SCALING LAWS FOR NEURAL NETWORKS,0.0743321718931475,"6. We study the dependence of the scaling exponent on changes in architecture and data, ﬁnding that
(i) changing the input distribution via switching datasets and (ii) the addition of noise have strong
effects on the exponent, while (iii) changing the target task via superclassing does not."
SCALING LAWS FOR NEURAL NETWORKS,0.07549361207897794,"Related Works: There have been a number of recent works demonstrating empirical scaling laws
(Hestness et al., 2017; Kaplan et al., 2020; Rosenfeld et al., 2020b; Henighan et al., 2020; Rosenfeld
et al., 2020a) in deep neural networks, including scaling laws with model size, dataset size, compute,
and other observables such as mutual information and pruning. Some precursors (Ahmad & Tesauro,
1989; Cohn & Tesauro, 1991) can be found in earlier literature. Recently, scaling laws have also
played a signiﬁcant role in motivating work on the largest models that have yet been developed
(Brown et al., 2020; Fedus et al., 2021)."
SCALING LAWS FOR NEURAL NETWORKS,0.07665505226480836,"There has been comparatively little work on theoretical ideas (Sharma & Kaplan, 2020; Bisla et al.,
2021) that match and explain empirical ﬁndings in generic deep neural networks. In the particular
case of large width, deep neural networks behave as random feature models (Neal, 1994; Lee et al.,
2018; Matthews et al., 2018; Jacot et al., 2018; Lee et al., 2019; Dyer & Gur-Ari, 2020), and known
results on the loss scaling of kernel methods can be applied (Spigler et al., 2020; Bordelon et al.,
2020). Though not in the original, Bordelon et al. (2020) analyze resolution-limited dataset size
scaling for power-law spectra in later versions."
SCALING LAWS FOR NEURAL NETWORKS,0.07781649245063879,"During the completion of this work, Hutter (2021) presented a speciﬁc solvable model of learning
exhibiting non-trivial power-law scaling for power-law (Zipf) distributed features. This does not
directly relate to the setups studied in this work, or present bounds that supersede our results.
Concurrent to our work, Bisla et al. (2021) presented a derivation of the resolution-limited scaling
with dataset size, also stemming from nearest neighbor distance scaling on data manifolds. However,
they do not discuss requirements on model versus dataset size or how this scaling behavior ﬁts into
other asymptotic scaling regimes."
SCALING LAWS FOR NEURAL NETWORKS,0.07897793263646923,"In the variance-limited regime, scaling laws in the context of random feature models (Rahimi &
Recht, 2008; Hastie et al., 2019; d’Ascoli et al., 2020), deep linear models (Advani & Saxe, 2017;
Advani et al., 2020), one-hidden-layer networks (Mei & Montanari, 2019; Adlam & Pennington,
2020a;b), and wide neural networks treated as Gaussian processes or trained in the NTK regime
(Lee et al., 2019; Dyer & Gur-Ari, 2020; Andreassen & Dyer, 2020; Geiger et al., 2020) have been
studied. In particular, this behavior was used in (Kaplan et al., 2020) to motivate a particular ansatz
for simultaneous scaling with data and model size. The resolution-limited analysis can perhaps be
viewed as an attempt to quantify the ideal-world generalization error of Nakkiran et al. (2021)."
SCALING LAWS FOR NEURAL NETWORKS,0.08013937282229965,"This work makes use of classic results connecting the spectrum of a smooth kernel to the geometry
it is deﬁned over (Weyl, 1912; Reade, 1983; K¨uhn, 1987; Ferreira & Menegatto, 2009) and on the
scaling of iteratively reﬁned approximations to smooth manifolds (Stein, 1999; Bickel et al., 2007;
de Laat, 2011)."
SCALING LAWS FOR NEURAL NETWORKS,0.08130081300813008,Under review as a conference paper at ICLR 2022
FOUR SCALING REGIMES,0.08246225319396051,"2
FOUR SCALING REGIMES"
FOUR SCALING REGIMES,0.08362369337979095,"Throughout this work we will be interested in how the average test loss L(D, P) depends on the
dataset size D and the number of model parameters P. Unless otherwise noted, L denotes the test
loss averaged over initialization of the parameters and draws of a size D training set. Some of our
results only pertain directly to the scaling with width w ∝
√"
FOUR SCALING REGIMES,0.08478513356562137,"P, but we expect many of the intuitions
apply more generally. We use the notation αD, αP , and αW to indicate scaling exponents with
respect to dataset size, parameter count, and width. All proofs appear in the supplement."
VARIANCE-LIMITED EXPONENTS,0.0859465737514518,"2.1
VARIANCE-LIMITED EXPONENTS"
VARIANCE-LIMITED EXPONENTS,0.08710801393728224,"In the limit of large D the outputs of an appropriately trained network approach a limiting form with
corrections which scale as D−1. Similarly, recent work shows that wide networks have a smooth
large P limit (Jacot et al., 2018), where ﬂuctuations scale as 1/
√"
VARIANCE-LIMITED EXPONENTS,0.08826945412311266,"P. If the loss is sufﬁciently smooth
then its value will approach the asymptotic loss with corrections proportional to the variance, (1/D or
1/
√"
VARIANCE-LIMITED EXPONENTS,0.08943089430894309,"P). In Theorem 1 we present sufﬁcient conditions on the loss to ensure this variance dominated
scaling. We note, these conditions are satisﬁed by mean squared error and cross entropy loss, though
we conjecture the result holds even more generally."
VARIANCE-LIMITED EXPONENTS,0.09059233449477352,"Theorem 1. Let ℓ(f) be the test loss as a function of network output, (L = E [ℓ(f)]), and let
fT be the network output after T training steps, thought of as a random variable over weight
initialization, draws of the training dataset, and optimization seed. Further let fT be concentrating
with E[(fT −E[fT ])k] = O (ϵ) ∀k ≥2. If ℓis a ﬁnite degree polynomial, or has bounded second
derivative, or is 2-H¨older, then E [ℓ(fT )] −ℓ(E [fT ]) = O(ϵ)."
VARIANCE-LIMITED EXPONENTS,0.09175377468060394,"Dataset scaling
Consider a neural network, and its associated training loss Ltrain(θ). For every
value of the weights, the training loss, thought of as a random variable over draws of a training set
of size D, concentrates around the population loss, with a variance which scales as O
 
D−1
. If
the optimization procedure is sufﬁciently smooth, the trained weights, network output, and higher
moments, will approach their inﬁnite D values, ED
h
(fT −ED [fT ])ki
= O
 
D−1
. Here, the
subscript D on the expectation indicates an average over draws of the training set. This scaling
together with Theorem 1 gives the variance limited scaling of loss with dataset size."
VARIANCE-LIMITED EXPONENTS,0.09291521486643438,"This concentration result with respect to dataset size has appeared for linear models in Rahimi
& Recht (2008) and for single hidden layer networks with high-dimensional input data in Mei &
Montanari (2019); Adlam & Pennington (2020a;b). In the supplement we prove this for GD and SGD
with polynomial loss as well as present informal arguments more generally. Additionally, we present
examples violating the smoothness assumption and exhibiting different scaling."
VARIANCE-LIMITED EXPONENTS,0.09407665505226481,"Large Width Scaling
We can make a very similar argument in the w →∞limit. It has been
shown that the predictions from an inﬁnitely wide network, either under Bayesian inference (Neal,
1994; Lee et al., 2018), or when trained via gradient descent (Jacot et al., 2018; Lee et al., 2019)
approach a limiting distribution at large width equivalent to a linear model. Furthermore, corrections
to the inﬁnite width behavior are controlled by the variance of the full model around the linear model
predictions. This variance (and higher moments) have been shown to scale as 1/w (Dyer & Gur-Ari,"
VARIANCE-LIMITED EXPONENTS,0.09523809523809523,"2020; Yaida, 2020; Andreassen & Dyer, 2020), Ew
h
(fT −Ew [fT ])ki
= O
 
w−1
. Theorem 1 then"
VARIANCE-LIMITED EXPONENTS,0.09639953542392567,implies the loss will differ from its w = ∞limit by a term proportional to 1/w.
VARIANCE-LIMITED EXPONENTS,0.0975609756097561,"We note that there has also been work studying the combined large depth and large width limit, where
Hanin & Nica (2020) found a well-deﬁned inﬁnite size limit with controlled ﬂuctuations. In any such
context where the model predictions concentrate, we expect the loss to scale with the variance of
the model output. In the case of linear models, studied below, the variance is O(P −1) rather than
O(
√"
VARIANCE-LIMITED EXPONENTS,0.09872241579558652,"P), and we see the associated variance scaling in this case."
VARIANCE-LIMITED EXPONENTS,0.09988385598141696,Under review as a conference paper at ICLR 2022
RESOLUTION-LIMITED EXPONENTS,0.10104529616724739,"2.2
RESOLUTION-LIMITED EXPONENTS"
RESOLUTION-LIMITED EXPONENTS,0.10220673635307782,"In this section we consider training and test data drawn uniformly from a compact d-dimensional
manifold, x ∈Md, and targets given by some smooth function y = F(x) on this manifold."
RESOLUTION-LIMITED EXPONENTS,0.10336817653890824,Over-Parameterized Dataset Scaling
RESOLUTION-LIMITED EXPONENTS,0.10452961672473868,"Consider the double limit of an over-parameterized model with large training set size, P ≫D ≫1.
We further consider well-trained models, i.e. models that interpolate all training data. The goal is to
understand L(D). If we assume that the learned model f is sufﬁciently smooth, then the dependence
of the loss on D can be bounded in terms of the dimension of the data manifold Md."
RESOLUTION-LIMITED EXPONENTS,0.10569105691056911,"Informally, if our train and test data are drawn i.i.d. from the same manifold, then the distance from a
test point to the closest training data point decreases as we add more and more training data points. In
particular, this distance scales as O(D−1/d) (Levina & Bickel, 2005). Furthermore, if f, F are both
sufﬁciently smooth, they cannot differ too much over this distance. If in addition the loss function, L,
is a smooth function vanishing when f = F, we have L = O(D−1/d). This is summarized in the
following theorem."
RESOLUTION-LIMITED EXPONENTS,0.10685249709639953,"Theorem 2. Let L(f), f and F be Lipschitz with constants KL, Kf, and KF. Further let D
be a training dataset of size D sampled i.i.d from Md and let f(x) = F(x), ∀x ∈D then
L(D) = O
 
KLmax(Kf, KF)D−1/d
."
RESOLUTION-LIMITED EXPONENTS,0.10801393728222997,Under-Parameterized Parameter Scaling
RESOLUTION-LIMITED EXPONENTS,0.1091753774680604,"We will again assume that F varies smoothly on an underlying compact d-dimensional manifold
Md. We can obtain a bound on L(P) if we imagine that f approximates F as a piecewise function
with roughly P regions (see Sharma & Kaplan (2020)). Here, we instead make use of the argument
from the over-parameterized, resolution-limited regime above. If we construct a sufﬁciently smooth
estimator for F by interpolating among P randomly chosen points from the (arbitrarily large) training
set, then by the argument above the loss will be bounded by O(P −1/d)."
RESOLUTION-LIMITED EXPONENTS,0.11033681765389082,"Theorem 3. Let L(f), f and F be Lipschitz with constants KL, Kf, and KF. Further let f(x) =
F(x) for P points sampled i.i.d from Md then L(P) = O
 
KLmax(Kf, KF)P −1/d
."
RESOLUTION-LIMITED EXPONENTS,0.11149825783972125,From Bounds to Estimates
RESOLUTION-LIMITED EXPONENTS,0.11265969802555169,"Theorems 2 and 3 are phrased as bounds, but we expect the stronger statement that these bounds also
generically serve as estimates, so that eg L(D) = Ω(D−c/d) for c ≥2, and similarly for parameter
scaling. If we assume that F and f are analytic functions on Md and that the loss function L(f, F) is
analytic in f −F and minimized at f = F, then the loss at a given test input, xtest, can be expanded
around the nearest training point, ˆxtrain, L(xtest) = P∞
m=n≥2 am(ˆxtrain)(xtest −ˆxtrain)m,2 where the
ﬁrst term is of ﬁnite order n ≥2 because the loss vanishes at the training point. As the typical
distance between nearest neighbor points scales as D−1/d on a d-dimensional manifold, the loss
will be dominated by the leading term, L ∝D−n/d, at large D. Note that if the model provides an
accurate piecewise linear approximation, we will generically ﬁnd n ≥4."
EXPLICIT REALIZATION IN LINEAR MODELS,0.11382113821138211,"2.3
EXPLICIT REALIZATION IN LINEAR MODELS"
EXPLICIT REALIZATION IN LINEAR MODELS,0.11498257839721254,"In the proceeding sections we have conjectured typical case scaling relations for a model’s test loss.
We have further given intuitive arguments for this behavior which relied on smoothness assumptions
on the loss and training procedure. In this section, we provide a concrete realization of all four scaling
regimes within the context of linear models. Of particular interest is the resolution-limited regime,
where the scaling of the loss is a consequence of the linear model kernel spectrum – the scaling of
over-parameterized models with dataset size and under-parameterized models with parameters is a
consequence of a classic result, originally due to Weyl (1912), bounding the spectrum of sufﬁciently
smooth kernel functions by the dimension of the manifold they act on."
EXPLICIT REALIZATION IN LINEAR MODELS,0.11614401858304298,2For simplicity we have used a very compressed notation for multi-tensor contractions in higher order terms.
EXPLICIT REALIZATION IN LINEAR MODELS,0.1173054587688734,Under review as a conference paper at ICLR 2022
EXPLICIT REALIZATION IN LINEAR MODELS,0.11846689895470383,"103
104
105"
EXPLICIT REALIZATION IN LINEAR MODELS,0.11962833914053426,"Dataset size (D) 10
5 10
4 10
3 10
2 10
1 100"
EXPLICIT REALIZATION IN LINEAR MODELS,0.1207897793263647,"Loss - Loss(
)"
EXPLICIT REALIZATION IN LINEAR MODELS,0.12195121951219512,Variance-limited
EXPLICIT REALIZATION IN LINEAR MODELS,0.12311265969802555,D: 1.01
EXPLICIT REALIZATION IN LINEAR MODELS,0.12427409988385599,D: 1.01
EXPLICIT REALIZATION IN LINEAR MODELS,0.1254355400696864,D: 1.01
EXPLICIT REALIZATION IN LINEAR MODELS,0.12659698025551683,D: 1.01
EXPLICIT REALIZATION IN LINEAR MODELS,0.12775842044134728,D: 1.01
EXPLICIT REALIZATION IN LINEAR MODELS,0.1289198606271777,D: 1.01
EXPLICIT REALIZATION IN LINEAR MODELS,0.13008130081300814,D: 1.00
EXPLICIT REALIZATION IN LINEAR MODELS,0.13124274099883856,D: 1.00
EXPLICIT REALIZATION IN LINEAR MODELS,0.13240418118466898,"101
102
103
104"
EXPLICIT REALIZATION IN LINEAR MODELS,0.13356562137049943,Dataset size (D)
EXPLICIT REALIZATION IN LINEAR MODELS,0.13472706155632985,"10
7
10
6
10
5
10
4
10
3
10
2
10
1
100
101
102 Loss"
EXPLICIT REALIZATION IN LINEAR MODELS,0.13588850174216027,Resolution-limited
EXPLICIT REALIZATION IN LINEAR MODELS,0.13704994192799072,D: 0.34
EXPLICIT REALIZATION IN LINEAR MODELS,0.13821138211382114,D: 0.43
EXPLICIT REALIZATION IN LINEAR MODELS,0.13937282229965156,D: 0.52
EXPLICIT REALIZATION IN LINEAR MODELS,0.140534262485482,D: 0.61
EXPLICIT REALIZATION IN LINEAR MODELS,0.14169570267131243,D: 0.67
EXPLICIT REALIZATION IN LINEAR MODELS,0.14285714285714285,D: 0.76
EXPLICIT REALIZATION IN LINEAR MODELS,0.1440185830429733,D: 0.85
EXPLICIT REALIZATION IN LINEAR MODELS,0.14518002322880372,D: 1.22
EXPLICIT REALIZATION IN LINEAR MODELS,0.14634146341463414,"100
101
102
103 i 10
6 10
4 10
2 100 102 i"
EXPLICIT REALIZATION IN LINEAR MODELS,0.14750290360046459,Kernel spectrum
EXPLICIT REALIZATION IN LINEAR MODELS,0.148664343786295,K: 0.34
EXPLICIT REALIZATION IN LINEAR MODELS,0.14982578397212543,K: 0.42
EXPLICIT REALIZATION IN LINEAR MODELS,0.15098722415795587,K: 0.50
EXPLICIT REALIZATION IN LINEAR MODELS,0.1521486643437863,K: 0.51
EXPLICIT REALIZATION IN LINEAR MODELS,0.15331010452961671,K: 0.55
EXPLICIT REALIZATION IN LINEAR MODELS,0.15447154471544716,K: 0.60
EXPLICIT REALIZATION IN LINEAR MODELS,0.15563298490127758,K: 0.71
EXPLICIT REALIZATION IN LINEAR MODELS,0.156794425087108,K: 1.25
EXPLICIT REALIZATION IN LINEAR MODELS,0.15795586527293845,"101
102
103
104"
EXPLICIT REALIZATION IN LINEAR MODELS,0.15911730545876887,Parameter count (P)
EXPLICIT REALIZATION IN LINEAR MODELS,0.1602787456445993,"10
7
10
6
10
5
10
4
10
3
10
2
10
1
100
101
102 Loss"
EXPLICIT REALIZATION IN LINEAR MODELS,0.16144018583042974,Resolution-limited
EXPLICIT REALIZATION IN LINEAR MODELS,0.16260162601626016,P: 0.34
EXPLICIT REALIZATION IN LINEAR MODELS,0.16376306620209058,P: 0.44
EXPLICIT REALIZATION IN LINEAR MODELS,0.16492450638792103,P: 0.52
EXPLICIT REALIZATION IN LINEAR MODELS,0.16608594657375145,P: 0.63
EXPLICIT REALIZATION IN LINEAR MODELS,0.1672473867595819,P: 0.70
EXPLICIT REALIZATION IN LINEAR MODELS,0.16840882694541232,P: 0.79
EXPLICIT REALIZATION IN LINEAR MODELS,0.16957026713124274,P: 0.92
EXPLICIT REALIZATION IN LINEAR MODELS,0.17073170731707318,P: 1.31
EXPLICIT REALIZATION IN LINEAR MODELS,0.1718931475029036,"102
103
104"
EXPLICIT REALIZATION IN LINEAR MODELS,0.17305458768873402,"Parameter count (P) 10
4 10
3 10
2 10
1 100 101 102"
EXPLICIT REALIZATION IN LINEAR MODELS,0.17421602787456447,"Loss - Loss(
)"
EXPLICIT REALIZATION IN LINEAR MODELS,0.1753774680603949,Variance-limited
EXPLICIT REALIZATION IN LINEAR MODELS,0.1765389082462253,P: 1.14
EXPLICIT REALIZATION IN LINEAR MODELS,0.17770034843205576,P: 1.14
EXPLICIT REALIZATION IN LINEAR MODELS,0.17886178861788618,P: 1.15
EXPLICIT REALIZATION IN LINEAR MODELS,0.1800232288037166,P: 1.14
EXPLICIT REALIZATION IN LINEAR MODELS,0.18118466898954705,P: 1.15
EXPLICIT REALIZATION IN LINEAR MODELS,0.18234610917537747,P: 1.15
EXPLICIT REALIZATION IN LINEAR MODELS,0.1835075493612079,P: 1.14
EXPLICIT REALIZATION IN LINEAR MODELS,0.18466898954703834,P: 1.15
EXPLICIT REALIZATION IN LINEAR MODELS,0.18583042973286876,"0.4
0.6
0.8
1.0
1.2 D 0.4 0.6 0.8 1.0 1.2"
EXPLICIT REALIZATION IN LINEAR MODELS,0.18699186991869918,Fit exponents P K 2 4 6 8 10 12 14
EXPLICIT REALIZATION IN LINEAR MODELS,0.18815331010452963,pool size
EXPLICIT REALIZATION IN LINEAR MODELS,0.18931475029036005,"(a)
(b)"
EXPLICIT REALIZATION IN LINEAR MODELS,0.19047619047619047,"Figure 2: (a) Random feature models exhibit all four scaling regimes Here we consider linear teacher-
student models with random features trained with MSE loss to convergence. We see both variance-limited
scaling (top-left, bottom-right) and resolution-limited scaling (top-right, bottom-left). Data is varied by
downsampling MNIST by the speciﬁed pool size. (b) Duality and spectra in random feature models Here
we show the relation between the decay of the kernel spectra, αK, and the scaling of the loss with number of
data points, αD, and with number of parameters, αP (top) The spectra of random FC kernels on pooled MNIST
(bottom) appear well described by a power law decay. The theoretical relation αD = αP = αK is given by the
black dashed line."
EXPLICIT REALIZATION IN LINEAR MODELS,0.1916376306620209,"Linear predictors serve as a model system for learning. Such models are used frequently in practice
when more expressive models are unnecessary or infeasible (McCullagh & Nelder, 1989; Rifkin &
Lippert, 2007; Hastie et al., 2009) and also serve as an instructive test bed to study training dynam-
ics (Advani et al., 2020; Goh, 2017; Hastie et al., 2019; Nakkiran, 2019; Grosse, 2021). Furthermore,
in the large width limit, randomly initialized neural networks become Gaussian Processes (Neal,
1994; Lee et al., 2018; Matthews et al., 2018; Novak et al., 2019; Garriga-Alonso et al., 2019; Yang,
2019), and in the low-learning rate regime (Lee et al., 2019; Lewkowycz et al., 2020; Huang et al.,
2020) neural networks train as linear models at inﬁnite width (Jacot et al., 2018; Lee et al., 2019;
Chizat et al., 2019)."
EXPLICIT REALIZATION IN LINEAR MODELS,0.19279907084785133,"Here we discuss linear models in general terms, though the results immediately hold for the special
cases of wide neural networks. In this section we focus on teacher-student models with weights
initialized to zero and trained with mean squared error (MSE) loss to their global optimum."
EXPLICIT REALIZATION IN LINEAR MODELS,0.19396051103368175,"We consider a linear teacher, F, and student f, F(x) = PS
M=1 ωMFM(x), f(x) = PP
µ=1 θµfµ(x) .
Here {FM} are a (potentially inﬁnite) pool of features and the teacher weights, ωM are taken to be
normal distributed, ω ∼N(0, 1/S). The student model is built out of a subset of the teacher features.
To vary the number of parameters in this simple model, we construct P features, fµ=1,...,P , by
introducing a projector P onto a P-dimensional subspace of the teacher features, fµ = P"
EXPLICIT REALIZATION IN LINEAR MODELS,0.1951219512195122,M PµMFM.
EXPLICIT REALIZATION IN LINEAR MODELS,0.19628339140534262,"We train by sampling a training set of size D and minimizing the MSE loss, Ltrain
=
1
2D
PD
a=1 (f(xa) −F(xa))2. We are interested in the test loss averaged over draws of our teacher
and training dataset. The inﬁnite data test loss, L(P) := limD→∞L(D, P), takes the form."
EXPLICIT REALIZATION IN LINEAR MODELS,0.19744483159117304,L(P) = 1
"S TR
H",0.1986062717770035,"2S Tr
h
C −CPT  
PCPT −1 PC
i
.
(1)"
"S TR
H",0.1997677119628339,"Here we have introduced the feature-feature second moment-matrix, C = Ex

F(x)F T (x)

."
"S TR
H",0.20092915214866433,"If the teacher and student features had the same span, this would vanish, but due to the mismatch
the loss is non-zero. On the other hand, if we keep a ﬁnite number of training points, but allow the"
"S TR
H",0.20209059233449478,Under review as a conference paper at ICLR 2022
"S TR
H",0.2032520325203252,"student to use all of the teacher features, the test loss, L(D) := limP →S L(D, P), takes the form,"
"S TR
H",0.20441347270615565,L(D) = 1
"EX
H",0.20557491289198607,"2Ex
h
K(x, x) −⃗K(x) ¯K−1 ⃗K(x)
i
.
(2)"
"EX
H",0.2067363530778165,"Here, K(x, x′) is the data-data second moment matrix, ⃗K indicates restricting one argument to the D
training points, while ¯K indicates restricting both. This test loss vanishes as the number of training
points becomes inﬁnite but is non-zero for ﬁnite training size."
"EX
H",0.20789779326364694,"We present a full derivation of these expressions in the supplement. In the remainder of this section,
we explore the scaling of the test loss with dataset and model size."
VARIANCE-LIMITED EXPONENTS,0.20905923344947736,"2.3.1
VARIANCE-LIMITED EXPONENTS"
VARIANCE-LIMITED EXPONENTS,0.21022067363530778,"To derive the limiting expressions (1) and (2) for the loss one makes use of the fact that the sample
expectation of the second moment matrix over the ﬁnite dataset, and ﬁnite feature set is close to the
full covariance, 1"
VARIANCE-LIMITED EXPONENTS,0.21138211382113822,"D
PD
a=1 F(xa)F T (xa) = C + δC, 1"
VARIANCE-LIMITED EXPONENTS,0.21254355400696864,"P f T (x)f(x′), = K + δK, with the ﬂuctuations
satisfying ED

δC2
= O(D−1) and EP

δK2
= O(P −1), where expectations are taken over
draws of a dataset of size D and over feature sets. Using these expansions yields the variance-limited
scaling, L(D, P) −L(P) = O(D−1), L(D, P) −L(D) = O(P −1) in the under-parameterized and
over-parameterized settings respectively."
VARIANCE-LIMITED EXPONENTS,0.21370499419279906,"In Figure 2a we see evidence of these scaling relations for features built from randomly initialized
ReLU networks on pooled MNIST independent of the pool size. In the supplement we provide an
in-depth derivation of this behavior and expressions for the leading contributions to L(D, P) −L(P)
and L(D, P) −L(D)."
RESOLUTION-LIMITED EXPONENTS,0.2148664343786295,"2.3.2
RESOLUTION-LIMITED EXPONENTS"
RESOLUTION-LIMITED EXPONENTS,0.21602787456445993,"We now would like to analyze the scaling behavior of our linear model in the resolution-limited
regimes, that is the scaling with P when 1 ≪P ≪D and the scaling with D when 1 ≪D ≪P.
In these cases, the scaling is controlled by the shared spectrum of C or K. This spectrum is often
well described by a power-law, where eigenvalues λi satisfy λi =
1
i1+αK . See Figure 2b for example
spectra on pooled MNIST. In this case, we will argue that the losses also obey a power law scaling,
with the exponents controlled by the spectral decay factor, 1 + αK."
RESOLUTION-LIMITED EXPONENTS,0.21718931475029035,"L(D) ∝D−αK ,
L(P) ∝P −αK .
(3)"
RESOLUTION-LIMITED EXPONENTS,0.2183507549361208,"In other words, in this setting, αP = αD = αK. This is supported empirically in Figure 2b. We
then argue that when the kernel function, K is sufﬁciently smooth on a manifold of dimension d,
αK ∝d−1, thus realizing the more general resolution-limited picture described above."
RESOLUTION-LIMITED EXPONENTS,0.21951219512195122,"From spectra to scaling laws for the loss
To be concrete let us focus on the over-parameterized
loss.
If we introduce the notation ei for the eigenvectors of C and ¯ei for the eignvectors of
1
D
PD
a=1 F(xa)F T (xa), the loss becomes,"
RESOLUTION-LIMITED EXPONENTS,0.22067363530778164,"L(D) = 1 2 S
X"
RESOLUTION-LIMITED EXPONENTS,0.2218350754936121,"i=1
λi(1 − D
X"
RESOLUTION-LIMITED EXPONENTS,0.2229965156794425,"j=1
(ei · ¯ej)2) .
(4)"
RESOLUTION-LIMITED EXPONENTS,0.22415795586527293,"Before discussing the general asymptotic behavior of (4), we can gain some intuition by considering
the case of large αK. In this case, ¯ej ≈ej (see e.g. Loukas (2017)), we can simplify (4) to,"
RESOLUTION-LIMITED EXPONENTS,0.22531939605110338,"L(D) ∝ ∞
X D+1"
RESOLUTION-LIMITED EXPONENTS,0.2264808362369338,"1
i1+αK = αKD−αK + O(D−αK−1) .
(5)"
RESOLUTION-LIMITED EXPONENTS,0.22764227642276422,"More generally in the supplement, following Bordelon et al. (2020); Canatar et al. (2021), we use
replica theory methods to derive, L(D) ∝D−αK and L(P) ∝P −αK, without requiring the large
αK limit."
RESOLUTION-LIMITED EXPONENTS,0.22880371660859466,Under review as a conference paper at ICLR 2022
RESOLUTION-LIMITED EXPONENTS,0.22996515679442509,"103
104"
RESOLUTION-LIMITED EXPONENTS,0.2311265969802555,"Dataset size (D) 10
1 100 Loss"
RESOLUTION-LIMITED EXPONENTS,0.23228803716608595,Super-classed CIFAR-100
RESOLUTION-LIMITED EXPONENTS,0.23344947735191637,D=0.39
RESOLUTION-LIMITED EXPONENTS,0.2346109175377468,D=0.39
RESOLUTION-LIMITED EXPONENTS,0.23577235772357724,D=0.39
RESOLUTION-LIMITED EXPONENTS,0.23693379790940766,D=0.38
RESOLUTION-LIMITED EXPONENTS,0.23809523809523808,D=0.41
RESOLUTION-LIMITED EXPONENTS,0.23925667828106853,D=0.42
RESOLUTION-LIMITED EXPONENTS,0.24041811846689895,D=0.40
RESOLUTION-LIMITED EXPONENTS,0.2415795586527294,"103
104"
RESOLUTION-LIMITED EXPONENTS,0.24274099883855982,"Dataset size (D) 10
1 100 Loss"
RESOLUTION-LIMITED EXPONENTS,0.24390243902439024,Corrupted CIFAR-10
RESOLUTION-LIMITED EXPONENTS,0.2450638792102207,D=0.58
RESOLUTION-LIMITED EXPONENTS,0.2462253193960511,D=0.46
RESOLUTION-LIMITED EXPONENTS,0.24738675958188153,D=0.41
RESOLUTION-LIMITED EXPONENTS,0.24854819976771197,D=0.37
RESOLUTION-LIMITED EXPONENTS,0.2497096399535424,D=0.33
RESOLUTION-LIMITED EXPONENTS,0.2508710801393728,"D=0.29
5
10 20 50"
NCLASS,0.25203252032520324,"100
Nclass 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175"
STDDEV,0.25319396051103366,"0.200
stddev"
STDDEV,0.25435540069686413,"Figure 3: Effect of data distribution on scaling exponents For CIFAR-100 superclassed to N classes (left),
we ﬁnd that the number of target classes does not have a visible effect on the scaling exponent. (right) For
CIFAR-10 with the addition of Gaussian noise to inputs, we ﬁnd the strength of the noise has a strong effect on
performance scaling with dataset size. All models are WRN-28-10."
STDDEV,0.25551684088269455,"Data Manifolds and Kernels
In Section 2.2, we discussed a simple argument that resolution-
limited exponents α ∝1/d, where d is the dimension of the data manifold. Our goal now is to
explain how this connects with the linearized models and kernels discussed above: how does the
spectrum of eigenvalues of a kernel relate to the dimension of the data manifold?"
STDDEV,0.25667828106852497,"The key point is that sufﬁciently smooth kernels must have an eigenvalue spectrum with a bounded
tail. Speciﬁcally, a Ct kernel on a d-dimensional space must have eigenvalues λn ≲
1
n1+t/d (K¨uhn,
1987). In the generic case where the covariance matrices we have discussed can be interpreted as
kernels on a manifold, and they have spectra saturating the bound, linearized models will inherit
scaling exponents given by the dimension of the manifold."
STDDEV,0.2578397212543554,"As a simple example, consider a d-torus.
In this case we can study the Fourier series de-
composition, and examine the case of a kernel K(x −y).
This must take the form K =
P"
STDDEV,0.2590011614401858,"nI [anI sin(nI · (x −y)) + bnI cos(nI · (x −y))], where nI = (n1, · · · , nd) are integer indices,
and anI, bnI are the overall Fourier coefﬁcients. To guarantee that K is a Ct function, we must have
anI, bnI ≲
1
nd+t where nd = N indexes the number of anI in decreasing order. But this means that
in this simple case, the tail eigenvalues of the kernel must be bounded by
1
N1+t/d as N →∞."
DUALITY,0.2601626016260163,"2.4
DUALITY"
DUALITY,0.2613240418118467,"We argued above that for kernels with pure power law spectra, the asymptotic scaling of the under-
parameterized loss with respect to model size and the over-parameterized loss with respect to
dataset size share a common exponent. In the linear setup at hand, the relation between the under-
parameterized parameter dependence and over-parameterized dataset dependence is even stronger.
The under-parameterized and over-parameterized losses are directly related by exchanging the
projection onto random features with the projection onto random training points. Note, sample-wise
double descent observed in Nakkiran (2019) is a concrete realization of this duality for a simple data
distribution. In the supplement, we present examples exhibiting the duality of the loss dependence on
model and dataset size outside of the asymptotic regime."
EXPERIMENTS,0.26248548199767713,"3
EXPERIMENTS"
DEEP TEACHER-STUDENT MODELS,0.26364692218350755,"3.1
DEEP TEACHER-STUDENT MODELS"
DEEP TEACHER-STUDENT MODELS,0.26480836236933797,"Our theory can be tested very directly in the teacher-student framework, in which a teacher deep
neural network generates synthetic data used to train a student network. Here, it is possible to generate
unlimited training samples and, crucially, controllably tune the dimension of the data manifold. We"
DEEP TEACHER-STUDENT MODELS,0.2659698025551684,Under review as a conference paper at ICLR 2022
DEEP TEACHER-STUDENT MODELS,0.26713124274099886,"accomplish the latter by scanning over the dimension of the inputs to the teacher. We have found that
when scanning over both model size and dataset size, the interpolation exponents closely match the
prediction of 4/d. The dataset size scaling is shown in Figure 1, while model size scaling experiments
appear in the supplement and have previously been observed in Sharma & Kaplan (2020)."
VARIANCE-LIMITED SCALING IN THE WILD,0.2682926829268293,"3.2
VARIANCE-LIMITED SCALING IN THE WILD"
VARIANCE-LIMITED SCALING IN THE WILD,0.2694541231126597,"Variance-limited scaling, (Section 2.1), can be universally observed in real datasets. Figure 1a
(top-left, bottom-right) measures the variance-limited dataset scaling exponent αD and width scaling
exponent αW . In both cases, we ﬁnd striking agreement with the theoretically predicted values
αD, αW = 1 across a variety of dataset, network architecture, stochastic batch size and loss type.
Our testbed includes deep fully-connected and convolutional networks with Relu or Erf nonlinearities
and MSE or softmax-cross-entropy losses. The supplement contains experimental details."
RESOLUTION-LIMITED SCALING IN THE WILD,0.2706155632984901,"3.3
RESOLUTION-LIMITED SCALING IN THE WILD"
RESOLUTION-LIMITED SCALING IN THE WILD,0.27177700348432055,"In addition to teacher-student models, we explored resolution-limited scaling behavior in the context
of standard classiﬁcation datasets. Wide ResNet (WRN) models (Zagoruyko & Komodakis, 2016)
were trained for a ﬁxed number of steps with cosine decay. In Figure 1b we also include data from
a four hidden layer CNN detailed in the supplement. As detailed above, we ﬁnd dataset dependent
scaling behavior in this context."
RESOLUTION-LIMITED SCALING IN THE WILD,0.27293844367015097,"We further investigated the effect of the data distribution on the resolution-limited exponent, αD, by
tuning the number of target classes and input noise (Figure 3). To probe the effect of the number of
classes, we constructed tasks derived from CIFAR-100 by grouping classes into broader semantic
categories. We found that performance depends on the number of categories, but αD is insensitive
to this number. In contrast, the addition of Gaussian noise had a more pronounced effect on αD.
This suggest a picture in which the network learns to model the input data manifold, independent of
the classiﬁcation task, consistent with observations in Nakkiran & Bansal (2020); Grathwohl et al.
(2020)."
RESOLUTION-LIMITED SCALING IN THE WILD,0.27409988385598144,"We also explored the effect of aspect ratio on dataset scaling, ﬁnding that the exponent magnitude
increases with width up to a critical width, while the dependence on depth is milder (see supplement)."
DISCUSSION,0.27526132404181186,"4
DISCUSSION"
DISCUSSION,0.2764227642276423,"We have presented a framework for categorizing neural scaling laws, along with derivations that
help to explain their very general origins. Crucially, our predictions agree with empirical ﬁndings in
settings which have often proven challenging for theory – deep neural networks on real datasets. The
variance-scaling regime yields, for smooth test losses, a universal prediction of αD = 1 (for D ≫P)
and αW = 1 (for w ≫D). The resolution-limited regime yields exponents whose numerical value is
variable and data and model dependent."
DISCUSSION,0.2775842044134727,"There are many intriguing directions for future work; amongst these, we highlight one in particular.
The invariance of the dataset scaling exponent to superclassing (Figure 3) suggests that deep networks
may be largely learning properties of the input data manifold – akin to unsupervised learning –
rather than signiﬁcant task-speciﬁc structure, which may shed light on the versatility of learned deep
network representations for different downstream tasks. This begs to be explored further."
DISCUSSION,0.2787456445993031,"Limitations
One limitation is that our theoretical results are asymptotic, while experiments are
performed with ﬁnite models and datasets. This is apparent in the resolution-limited regime which
requires a hierarchy (D ≫P or P ≫D). In Figures 1a and 2a top-right (bottom-left), we see
a breakdown of the predicted scaling behavior as D (P) become large and the hierarchy is lost.
Furthermore in the resolution-limited regime for deep networks, our theoretical tools rely on positing
the existence of a data manifold. A precise deﬁnition of the data manifold, however, is lacking forcing
us to use imperfect proxies, such as nearest neighbor distances of ﬁnal embedding layers."
DISCUSSION,0.27990708478513354,Under review as a conference paper at ICLR 2022
ETHICS STATEMENT,0.281068524970964,"Ethics Statement
Work on scaling laws provides an opportunity for discussion on how to deﬁne and
measure progress in machine learning. The values of exponents allow us to estimate expected gains
that come from increases in scale of dataset, model, and compute. Applying similar considerations
to other metrics (i.e. transfer, bias, robustness) in principle allows one to quantify whether and how
models are improving or degrading with scale and at what environmental or computational cost. On
the other hand, one may require that truly non-trivial progress in machine learning be progress that
occurs modulo scale: namely, improvements in performance across different tasks that are not simple
extrapolations of existing behavior. And perhaps the right combinations of algorithmic, model, and
dataset improvements can lead to emergent behavior at new scales. Large language models such as
GPT-3 (Fig. 1.2 in Brown et al. (2020)) have exhibited this in the context of few-shot learning. We
hope our work spurs further research in understanding and controlling neural scaling laws."
REFERENCES,0.28222996515679444,REFERENCES
REFERENCES,0.28339140534262486,"Ben Adlam and Jeffrey Pennington. The Neural Tangent Kernel in high dimensions: Triple descent
and a multi-scale theory of generalization. In International Conference on Machine Learning, pp.
74–84. PMLR, 2020a."
REFERENCES,0.2845528455284553,"Ben Adlam and Jeffrey Pennington. Understanding double descent requires a ﬁne-grained bias-
variance decomposition. Advances in Neural Information Processing Systems, 33, 2020b."
REFERENCES,0.2857142857142857,"Madhu S Advani and Andrew M Saxe. High-dimensional dynamics of generalization error in neural
networks. arXiv preprint arXiv:1710.03667, 2017."
REFERENCES,0.2868757259001161,"Madhu S Advani, Andrew M Saxe, and Haim Sompolinsky. High-dimensional dynamics of general-
ization error in neural networks. Neural Networks, 132:428–446, 2020."
REFERENCES,0.2880371660859466,"Subutai Ahmad and Gerald Tesauro. Scaling and generalization in neural networks: a case study. In
Advances in neural information processing systems, pp. 160–168, 1989."
REFERENCES,0.289198606271777,"Alnur Ali, J Zico Kolter, and Ryan J Tibshirani. A continuous-time view of early stopping for least
squares regression. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics,
pp. 1370–1378, 2019."
REFERENCES,0.29036004645760743,"Anders Andreassen and Ethan Dyer. Asymptotics of wide convolutional neural networks. arxiv
preprint arXiv:2008.08675, 2020."
REFERENCES,0.29152148664343785,"Peter J Bickel, Bo Li, et al. Local polynomial regression on unknown manifolds. In Complex datasets
and inverse problems, pp. 177–186. Institute of Mathematical Statistics, 2007."
REFERENCES,0.2926829268292683,"Devansh Bisla, Apoorva Nandini Saridena, and Anna Choromanska. A theoretical-empirical approach
to estimating sample complexity of dnns. arXiv preprint arXiv:2105.01867, 2021."
REFERENCES,0.2938443670150987,"Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan. Spectrum dependent learning curves in
kernel regression and wide neural networks. In International Conference on Machine Learning,
pp. 1024–1034. PMLR, 2020."
REFERENCES,0.29500580720092917,"James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and
Qiao Zhang.
JAX: composable transformations of Python+NumPy programs, 2018.
URL
http://github.com/google/jax."
REFERENCES,0.2961672473867596,"Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv preprint arXiv:2005.14165, 2020."
REFERENCES,0.29732868757259,"Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan. Spectral bias and task-model align-
ment explain generalization in kernel regression and inﬁnitely wide neural networks. Nature
communications, 12(1):1–12, 2021."
REFERENCES,0.29849012775842043,Under review as a conference paper at ICLR 2022
REFERENCES,0.29965156794425085,"Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.
In Advances in Neural Information Processing Systems, pp. 2937–2947, 2019."
REFERENCES,0.3008130081300813,"Omry Cohen, Or Malka, and Zohar Ringel. Learning curves for deep neural networks: a gaussian
ﬁeld theory perspective. arXiv preprint arXiv:1906.05301, 2019."
REFERENCES,0.30197444831591175,"David Cohn and Gerald Tesauro. Can neural networks do better than the vapnik-chervonenkis
bounds? In Advances in Neural Information Processing Systems, pp. 911–917, 1991."
REFERENCES,0.30313588850174217,"David de Laat. Approximating manifolds by meshes: asymptotic bounds in higher codimension.
Master’s Thesis, University of Groningen, Groningen, 2011."
REFERENCES,0.3042973286875726,"Ethan Dyer and Guy Gur-Ari. Asymptotics of wide networks from feynman diagrams. In International
Conference on Learning Representations, 2020. URL https://openreview.net/forum?
id=S1gFvANKDS."
REFERENCES,0.305458768873403,"St´ephane d’Ascoli, Maria Reﬁnetti, Giulio Biroli, and Florent Krzakala. Double trouble in double
descent: Bias and variance (s) in the lazy regime. In International Conference on Machine
Learning, pp. 2280–2290. PMLR, 2020."
REFERENCES,0.30662020905923343,"William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter
models with simple and efﬁcient sparsity, 2021."
REFERENCES,0.3077816492450639,"JC Ferreira and VA Menegatto. Eigenvalues of integral operators deﬁned by smooth positive deﬁnite
kernels. Integral Equations and Operator Theory, 64(1):61–81, 2009."
REFERENCES,0.3089430894308943,"Adri`a Garriga-Alonso, Laurence Aitchison, and Carl Edward Rasmussen. Deep convolutional
networks as shallow gaussian processes. In International Conference on Learning Representations,
2019."
REFERENCES,0.31010452961672474,"Mario Geiger, Arthur Jacot, Stefano Spigler, Franck Gabriel, Levent Sagun, St´ephane d’Ascoli,
Giulio Biroli, Cl´ement Hongler, and Matthieu Wyart. Scaling description of generalization with
number of parameters in deep learning. Journal of Statistical Mechanics: Theory and Experiment,
2020(2):023401, 2020."
REFERENCES,0.31126596980255516,"Federica Gerace, Bruno Loureiro, Florent Krzakala, Marc M´ezard, and Lenka Zdeborov´a. General-
isation error in learning with random features and the hidden manifold model. In International
Conference on Machine Learning, pp. 3452–3462. PMLR, 2020."
REFERENCES,0.3124274099883856,"Gabriel Goh. Why momentum really works. Distill, 2017. doi: 10.23915/distill.00006. URL
http://distill.pub/2017/momentum."
REFERENCES,0.313588850174216,"Will Grathwohl, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi,
and Kevin Swersky. Your classiﬁer is secretly an energy based model and you should treat
it like one. In International Conference on Learning Representations, 2020. URL https:
//openreview.net/forum?id=Hkxzx0NtDB."
REFERENCES,0.3147502903600465,"Roger Grosse. University of Toronto CSC2541 winter 2021 neural net training dynamics, lecture
notes, 2021. URL https://www.cs.toronto.edu/˜rgrosse/courses/csc2541_
2021."
REFERENCES,0.3159117305458769,"Boris Hanin and Mihai Nica. Finite depth and width corrections to the neural tangent kernel. In
International Conference on Learning Representations, 2020. URL https://openreview.
net/forum?id=SJgndT4KwB."
REFERENCES,0.3170731707317073,"Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The elements of statistical learning: data
mining, inference, and prediction. Springer Science & Business Media, 2009."
REFERENCES,0.31823461091753774,"Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani.
Surprises in high-
dimensional ridgeless least squares interpolation. arXiv preprint arXiv:1903.08560, 2019."
REFERENCES,0.31939605110336816,Under review as a conference paper at ICLR 2022
REFERENCES,0.3205574912891986,"Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas
Steiner, and Marc van Zee. Flax: A neural network library and ecosystem for JAX, 2020. URL
http://github.com/google/flax."
REFERENCES,0.32171893147502906,"Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo
Jun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec Radford,
Aditya Ramesh, Nick Ryder, Daniel M. Ziegler, John Schulman, Dario Amodei, and Sam Mc-
Candlish. Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701,
2020."
REFERENCES,0.3228803716608595,"Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad,
Md Patwary, Mostofa Ali, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable,
empirically. arXiv preprint arXiv:1712.00409, 2017."
REFERENCES,0.3240418118466899,"Wei Huang, Weitao Du, Richard Yi Da Xu, and Chunrui Liu. Implicit bias of deep linear networks in
the large learning rate phase. arXiv preprint arXiv:2011.12547, 2020."
REFERENCES,0.3252032520325203,"Marcus Hutter. Learning curve theory. arXiv preprint arXiv:2102.04074, 2021."
REFERENCES,0.32636469221835074,"Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural Tangent Kernel: Convergence and
generalization in neural networks. In Advances in Neural Information Processing Systems, 2018."
REFERENCES,0.32752613240418116,"Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott
Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models.
arXiv preprint arXiv:2001.08361, 2020."
REFERENCES,0.32868757259001163,"Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,
and Neil Houlsby. Big transfer (bit): General visual representation learning. arXiv preprint
arXiv:1912.11370, 6(2):8, 2019."
REFERENCES,0.32984901277584205,"Simon Kornblith, Jonathon Shlens, and Quoc V Le. Do better imagenet models transfer better?
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
2661–2671, 2019."
REFERENCES,0.3310104529616725,"Thomas K¨uhn. Eigenvalues of integral operators with smooth positive deﬁnite kernels. Archiv der
Mathematik, 49(6):525–534, 1987."
REFERENCES,0.3321718931475029,"Jaehoon Lee, Yasaman Bahri, Roman Novak, Sam Schoenholz, Jeffrey Pennington, and Jascha
Sohl-dickstein. Deep neural networks as Gaussian processes. In International Conference on
Learning Representations, 2018."
REFERENCES,0.3333333333333333,"Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In Advances in Neural Information Processing Systems, 2019."
REFERENCES,0.3344947735191638,"Jaehoon Lee, Samuel Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak, and
Jascha Sohl-Dickstein. Finite versus inﬁnite neural networks: an empirical study. Advances in
Neural Information Processing Systems, 33, 2020."
REFERENCES,0.3356562137049942,"Elizaveta Levina and Peter J Bickel. Maximum likelihood estimation of intrinsic dimension. In
Advances in neural information processing systems, pp. 777–784, 2005."
REFERENCES,0.33681765389082463,"Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari. The large
learning rate phase of deep learning: the catapult mechanism. arXiv preprint arXiv:2003.02218,
2020."
REFERENCES,0.33797909407665505,"Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv
preprint arXiv:1608.03983, 2016."
REFERENCES,0.33914053426248547,Under review as a conference paper at ICLR 2022
REFERENCES,0.3403019744483159,"Andreas Loukas. How close are the eigenvectors of the sample and actual covariance matrices? In
International Conference on Machine Learning, pp. 2228–2237. PMLR, 2017."
REFERENCES,0.34146341463414637,"D¨orthe Malzahn and Manfred Opper. Learning curves for gaussian processes regression: A framework
for good approximations. Advances in neural information processing systems, pp. 273–279, 2001."
REFERENCES,0.3426248548199768,"D¨orthe Malzahn and Manfred Opper. A variational approach to learning curves. In T. Dietterich,
S. Becker, and Z. Ghahramani (eds.), Advances in Neural Information Processing Systems, vol-
ume 14, pp. 463–469. MIT Press, 2002. URL https://proceedings.neurips.cc/
paper/2001/file/26f5bd4aa64fdadf96152ca6e6408068-Paper.pdf."
REFERENCES,0.3437862950058072,"D¨orthe Malzahn and Manfred Opper. Learning curves and bootstrap estimates for inference with
gaussian processes: A statistical mechanics study. Complexity, 8(4):57–63, 2003."
REFERENCES,0.34494773519163763,"Alexander G. de G. Matthews, Jiri Hron, Mark Rowland, Richard E. Turner, and Zoubin Ghahramani.
Gaussian process behaviour in wide deep neural networks. In International Conference on Learning
Representations, 2018."
REFERENCES,0.34610917537746805,"P McCullagh and John A Nelder. Generalized Linear Models, volume 37. CRC Press, 1989."
REFERENCES,0.34727061556329847,"Song Mei and Andrea Montanari. The generalization error of random features regression: Precise
asymptotics and double descent curve. arXiv preprint arXiv:1908.05355, 2019."
REFERENCES,0.34843205574912894,"Preetum Nakkiran. More data can hurt for linear regression: Sample-wise double descent. arXiv
preprint arXiv:1912.07242, 2019."
REFERENCES,0.34959349593495936,"Preetum Nakkiran and Yamini Bansal. Distributional generalization: A new kind of generalization.
arXiv preprint arXiv:2009.08092, 2020."
REFERENCES,0.3507549361207898,"Preetum Nakkiran, Behnam Neyshabur, and Hanie Sedghi. The deep bootstrap framework: Good
online learners are good ofﬂine generalizers. In International Conference on Learning Representa-
tions, 2021. URL https://openreview.net/forum?id=guetrIHLFGI."
REFERENCES,0.3519163763066202,"Radford M. Neal. Bayesian Learning for Neural Networks. PhD thesis, University of Toronto, Dept.
of Computer Science, 1994."
REFERENCES,0.3530778164924506,"Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron, Daniel A. Abolaﬁa,
Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional networks with many
channels are gaussian processes. In International Conference on Learning Representations, 2019."
REFERENCES,0.35423925667828104,"Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander A. Alemi, Jascha Sohl-Dickstein,
and Samuel S. Schoenholz. Neural Tangents: Fast and easy inﬁnite neural networks in python. In
International Conference on Learning Representations, 2020. URL https://github.com/
google/neural-tangents."
REFERENCES,0.3554006968641115,"Giorgio Parisi. A sequence of approximated solutions to the SK model for spin glasses. Journal of
Physics A: Mathematical and General, 13(4):L115, 1980."
REFERENCES,0.35656213704994194,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. Advances in Neural Information Processing Systems, 32:
8026–8037, 2019."
REFERENCES,0.35772357723577236,"Ali Rahimi and Benjamin Recht. Weighted sums of random kitchen sinks: replacing minimization
with randomization in learning. In Nips, pp. 1313–1320. Citeseer, 2008."
REFERENCES,0.3588850174216028,"JB Reade. Eigenvalues of positive deﬁnite kernels. SIAM Journal on Mathematical Analysis, 14(1):
152–157, 1983."
REFERENCES,0.3600464576074332,"Ryan M Rifkin and Ross A Lippert. Notes on regularized least squares, 2007."
REFERENCES,0.3612078977932636,Under review as a conference paper at ICLR 2022
REFERENCES,0.3623693379790941,"Sam Ritchie, Ambrose Slone, and Vinay Ramasesh.
Caliban: Docker-based job manager for
reproducible workﬂows. Journal of Open Source Software, 5(53):2403, 2020. doi: 10.21105/joss.
02403. URL https://doi.org/10.21105/joss.02403."
REFERENCES,0.3635307781649245,"Jonathan S. Rosenfeld, Jonathan Frankle, Michael Carbin, and Nir Shavit. On the predictability of
pruning across scales. arXiv preprint arXiv:2006.10621, 2020a."
REFERENCES,0.36469221835075494,"Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive prediction
of the generalization error across scales. In International Conference on Learning Representations,
2020b."
REFERENCES,0.36585365853658536,"Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information
propagation. International Conference on Learning Representations, 2017."
REFERENCES,0.3670150987224158,"Vaishaal Shankar, Alex Chengyu Fang, Wenshuo Guo, Sara Fridovich-Keil, Ludwig Schmidt,
Jonathan Ragan-Kelley, and Benjamin Recht. Neural kernels without tangents. In International
Conference on Machine Learning, 2020."
REFERENCES,0.3681765389082462,"Utkarsh Sharma and Jared Kaplan. A neural scaling law from the dimension of the data manifold.
arXiv preprint arXiv:2004.10802, 2020."
REFERENCES,0.3693379790940767,"Peter Sollich. Learning curves for gaussian processes. In Proceedings of the 11th International
Conference on Neural Information Processing Systems, pp. 344–350, 1998."
REFERENCES,0.3704994192799071,"Peter Sollich and Anason Halees. Learning curves for gaussian process regression: Approximations
and bounds. Neural computation, 14(6):1393–1428, 2002."
REFERENCES,0.3716608594657375,"Stefano Spigler, Mario Geiger, and Matthieu Wyart. Asymptotic learning curves of kernel methods:
empirical data versus teacher–student paradigm. Journal of Statistical Mechanics: Theory and
Experiment, 2020(12):124001, 2020."
REFERENCES,0.37282229965156793,"Michael L Stein. Interpolation of Spatial Data: Some Theory for Kriging. Springer Science &
Business Media, 1999."
REFERENCES,0.37398373983739835,"Mingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model scaling for convolutional neural networks.
In International Conference on Machine Learning, pp. 6105–6114. PMLR, 2019."
REFERENCES,0.37514518002322883,"Matthew J Urry and Peter Sollich. Replica theory for learning curves for gaussian processes on
random graphs. Journal of Physics A: Mathematical and Theoretical, 45(42):425005, 2012."
REFERENCES,0.37630662020905925,"Hermann Weyl. Das asymptotische verteilungsgesetz der eigenwerte linearer partieller differentialgle-
ichungen (mit einer anwendung auf die theorie der hohlraumstrahlung). Mathematische Annalen,
71(4):441–479, 1912."
REFERENCES,0.37746806039488967,"Christopher KI Williams and Francesco Vivarelli. Upper and lower bounds on the learning curve for
gaussian processes. Machine Learning, 40(1):77–102, 2000."
REFERENCES,0.3786295005807201,"Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel Schoenholz, and Jeffrey Penning-
ton. Dynamical isometry and a mean ﬁeld theory of CNNs: How to train 10,000-layer vanilla
convolutional neural networks. In International Conference on Machine Learning, 2018."
REFERENCES,0.3797909407665505,"Sho Yaida. Non-Gaussian processes and neural networks at ﬁnite widths. In Mathematical and
Scientiﬁc Machine Learning Conference, 2020."
REFERENCES,0.38095238095238093,"Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior,
gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760,
2019."
REFERENCES,0.3821138211382114,"Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In British Machine Vision
Conference, 2016."
REFERENCES,0.3832752613240418,Under review as a conference paper at ICLR 2022
REFERENCES,0.38443670150987225,"A
EXPERIMENTAL SETUP"
REFERENCES,0.38559814169570267,Figure 1 (top-left)
REFERENCES,0.3867595818815331,"Experiments utilize relatively small models, with the number of trainable parameteters P ∼O(1000),
trained with full-batch gradient descent (GD) and small learning rate on datasets of size D ≫P. Each
data point in the ﬁgure represents an average over subsets of size D sampled from the full dataset.
Experiments are done using Neural Tangents (Novak et al., 2020) based on JAX (Bradbury et al.,
2018). All experiment except denoted as (CNN), use 3-layer, width-8 fully-connected networks. CNN
architecture used is Myrtle-5 network (Shankar et al., 2020) with 8 channels. Relu activation function
with critical initialization (Schoenholz et al., 2017; Lee et al., 2018; Xiao et al., 2018) was used.
Unless speciﬁed softmax-cross-entropy loss was used. We performed full-batch gradient descent
update for all dataset sizes without L2 regularization. 20 different training data sampling seeds were
averaged for each point. For fully-connected networks, input pooling of size 4 was performed for
CIFAR-10/100 dataset and pooling of size 2 was performed for MNIST and Fashion-MNIST dataset.
This was to reduce number of parameters in the input layer (# of pixels × width) which can be quite
large even for small width networks."
REFERENCES,0.3879210220673635,"Figure 1 (top-right) All experiments were performed using a Flax (Heek et al., 2020) implementation
of Wide ResNet 28-10 (Zagoruyko & Komodakis, 2016), and performed using the Caliban experiment
manager (Ritchie et al., 2020). Models were trained for 78125 total steps with a cosine learning rate
decay (Loshchilov & Hutter, 2016) and an augmentation policy consisting of random ﬂips and crops.
We report ﬁnal loss, though we found no qualitative difference between using ﬁnal loss, best loss,
ﬁnal accuracy or best accuracy (see Figure S1)."
REFERENCES,0.389082462253194,"Figure 1 (bottom-left) The setup was identical to Figure 1 (top-right) except that the model consid-
ered was a depth 10 residual network with varying width."
REFERENCES,0.3902439024390244,Figure 1 (bottom-right)
REFERENCES,0.3914053426248548,"Experiments are done using Neural Tangents. All experiments use 100 training samples and two-
hidden layer fully-connected networks of varying width (ranging from w = 64 to W = 11, 585) with
Relu nonlinearities unless speciﬁed as Erf. Full-batch gradient descent and cross-entropy loss were
used unless speciﬁed as MSE, and the ﬁgure shows curves from a random assortment of training
times ranging from 100 to 500 steps (equivalently, epochs). Training was done with learning rates
small enough so as to avoid catapult dynamics (Lewkowycz et al., 2020) and no L2 regularization; in
such a setting, the inﬁnite-width learning dynamics is known to be equivalent to that of linearized
models (Lee et al., 2019). Consequently, for each random initialization of the parameters, the test
loss of the ﬁnite-width linearized model was additionally computed in the identical training setting.
This value approximates the limiting behavior L(∞) known theoretically and is subtracted off from
the ﬁnal test loss of the (nonlinear) neural network before averaging over 50 random initializations to
yield each of the individual data points in the ﬁgure."
REFERENCES,0.39256678281068524,"A.1
DEEP TEACHER-STUDENT MODELS"
REFERENCES,0.39372822299651566,"The teacher-student scaling with dataset size (ﬁgure S2) was performed with fully-connected teacher
and student networks with two hidden layers and widths 96 and 192, respectively, using PyTorch
(Paszke et al., 2019). The inputs were random vectors sampled uniformly from a hypercube of
dimension d = 2, 3, · · · , 9. To mitigate noise, we ran the experiment on eight different random seeds,
ﬁxing the random seed for the teacher and student as we scanned over dataset sizes. We also used a
ﬁxed test dataset, and a ﬁxed training set, which was sub-sampled for the experiments with smaller D.
The student networks were trained using MSE loss and Adam optimizer with a maximum learning
rate of 3 × 10−3, a cosine learning rate decay, and a batch size of 64, and 40, 000 steps of training.
The test losses were measured with early stopping. We combine test losses from different random
seeds by averaging the logarithm of the loss from each seed."
REFERENCES,0.3948896631823461,Under review as a conference paper at ICLR 2022
REFERENCES,0.39605110336817656,"102
103
104 10
1 100"
REFERENCES,0.397212543554007,CIFAR-10
REFERENCES,0.3983739837398374,"final loss
best loss
final error
best error"
REFERENCES,0.3995354239256678,"102
103
104 100"
REFERENCES,0.40069686411149824,CIFAR-100
REFERENCES,0.40185830429732866,"final loss
best loss
final error
best error"
REFERENCES,0.40301974448315914,"103
104
105 10
1 SVHN"
REFERENCES,0.40418118466898956,"final loss
best loss
final error
best error"
REFERENCES,0.40534262485482,"103
104 10
1"
REFERENCES,0.4065040650406504,FashionMNIST
REFERENCES,0.4076655052264808,"final loss
best loss
final error
best error"
REFERENCES,0.4088269454123113,"Figure S1: Alternate metrics and stopping conditions We ﬁnd similar scaling behavior for both the loss and
error, and for ﬁnal and best (early stopped) metrics."
REFERENCES,0.4099883855981417,"In our experiments, we always use inputs that are uniformly sampled from a d-dimensional hypercube,
following the setup of Sharma & Kaplan (2020). They also utilized several intrisic dimension (ID)
estimation methods and found the estimates were close to the input dimension, so we simply use the
latter for comparisons. For the dataset size scans we used randomly initialized teachers with width
96, and students with width 192. We found similar results with other network sizes."
REFERENCES,0.41114982578397213,"The ﬁnal scaling exponents and input dimensions are show in the bottom of Figure 1b. We used the
same experiments for the top of that ﬁgure, interpolating the behavior of both teacher and a set of
students between two ﬁxed training points. The students only differed by the size of their training
sets, but had the same random seeds and were trained in the same way. In that ﬁgure the input space
dimension was four."
REFERENCES,0.41231126596980255,"Finally, we also used a similar setup to study variance-limited exponents and scaling. In that case we
used much smaller models, with 16-dimensional hidden layers, and a correspondingly larger learning
rate. We then studied scaling with D again, with results pictured in Figure 1a."
REFERENCES,0.413472706155633,"A.2
CNN ARCHITECTURE FOR RESOLUTION-LIMITED SCALING"
REFERENCES,0.4146341463414634,"Figure 1b includes data from CNN architectures trained on image datasets. The architectures are
summarized in Table 1. We used Adam optimizer for training, with cross-entropy loss. Each network
was trained for long enough to achieve either a clear minimum or a plateau in test loss. Speciﬁcally,
CIFAR10, MNIST and Fashion MNIST were trained for 50 epochs, CIFAR100 was trained for 100
epochs and SVHN was trained for 10 epochs. The default Keras training parameters were used. In
case of SVHN we included the additional images as training data. We averaged (in log space) over"
REFERENCES,0.41579558652729387,Under review as a conference paper at ICLR 2022
REFERENCES,0.4169570267131243,"26
27
28
29
210
211
212
213"
REFERENCES,0.4181184668989547,"Dataset Size 10
9 10
8 10
7 10
6 10
5 10
4 Loss"
REFERENCES,0.41927990708478513,Teacher/Student Dataset Size 2 3 4 5 6 7 8 9
REFERENCES,0.42044134727061555,Input Dimension
REFERENCES,0.42160278745644597,"Figure S2: This ﬁgure shows scaling trends of MSE loss with dataset size for teacher/student models. The
exponents extracted from these ﬁts and their associated input-space dimensionalities are shown in Figure 1."
REFERENCES,0.42276422764227645,"Layer
Width
CNN window (3, 3)
50
2D Max Pooling (2, 2)"
REFERENCES,0.42392566782810687,"CNN window (3, 3)
100
2D Max Pooling (2, 2)"
REFERENCES,0.4250871080139373,"CNN window (3, 3)
100
Dense
64
Dense
10"
REFERENCES,0.4262485481997677,"Layer
Width
CNN window (3, 3)
50
2D Max Pooling (2, 2)"
REFERENCES,0.4274099883855981,"CNN window (3,3)
100
2D Max Pooling (2, 2)"
REFERENCES,0.42857142857142855,"CNN window (3, 3)
200
Dense
256
Dense
100
Layer
Width
CNN window (3, 3)
64
2D Max Pooling (2, 2)"
REFERENCES,0.429732868757259,"CNN window (3, 3)
64
2D Max Pooling (2, 2)"
REFERENCES,0.43089430894308944,"Dense
128
Dense
10"
REFERENCES,0.43205574912891986,"Table 1: CNN architectures for CIFAR10, MNIST, Fashion MNIST (left), CIFAR100 (center) and SVHN
(right)"
REFERENCES,0.4332171893147503,"20 runs for CIFAR100 and CIFAR10, 16 runs for MNIST, 12 runs for Fashion MNIST, and 5 runs
for SVHN. The results of these experiments are shown in Figure S3."
REFERENCES,0.4343786295005807,"The measurement of input-space dimensionality for these experiments was done using the nearest-
neighbour algorithm, described in detail in Appendix B and C in Sharma & Kaplan (2020). We used
2, 3 and 4 nearest neighbors and averaged over the three."
REFERENCES,0.4355400696864111,"A.3
TEACHER-STUDENT EXPERIMENT FOR SCALING OF LOSS WITH MODEL SIZE"
REFERENCES,0.4367015098722416,"We replicated the teacher-student setup in Sharma & Kaplan (2020) to demonstrate the scaling of
loss with model size. The resulting variation of −4/αP with input-space dimensionality is shown in
ﬁgure S4. In our implementation we averaged (in log space) over 15 iterations, with a ﬁxed, randomly
generated teacher."
REFERENCES,0.437862950058072,"B
EFFECT OF ASPECT RATIO ON SCALING EXPONENTS"
REFERENCES,0.43902439024390244,"We trained Wide ResNet architectures of various widths and depths on CIFAR-10 accross dataset
sizes. We found that the effect of depth on dataset scaling was mild for the range studied, while the
effect of width impacted the scaling behavior up until a saturating width, after which the scaling
behavior ﬁxed. See Figure S5."
REFERENCES,0.44018583042973286,Under review as a conference paper at ICLR 2022
REFERENCES,0.4413472706155633,"102
103
104"
REFERENCES,0.4425087108013937,Dataset Size 100
REFERENCES,0.4436701509872242,2 × 100
REFERENCES,0.4448315911730546,3 × 100
REFERENCES,0.445993031358885,Test Loss
REFERENCES,0.44715447154471544,CIFAR10: Loss scaling with Dataset Size
REFERENCES,0.44831591173054586,D : 0.198
REFERENCES,0.44947735191637633,"103
104"
REFERENCES,0.45063879210220675,Dataset Size
REFERENCES,0.4518002322880372,"3 × 10
1"
REFERENCES,0.4529616724738676,"4 × 10
1"
REFERENCES,0.454123112659698,"6 × 10
1"
REFERENCES,0.45528455284552843,Test Loss
REFERENCES,0.4564459930313589,FashionMNIST: Loss scaling with Dataset Size
REFERENCES,0.45760743321718933,D : 0.207
REFERENCES,0.45876887340301975,"103
104"
REFERENCES,0.45993031358885017,"Dataset Size 10
1"
REFERENCES,0.4610917537746806,Test Loss
REFERENCES,0.462253193960511,MNIST: Loss scaling with Dataset Size
REFERENCES,0.4634146341463415,D : 0.397
REFERENCES,0.4645760743321719,"103
104"
REFERENCES,0.4657375145180023,Dataset Size
REFERENCES,0.46689895470383275,3 × 100
REFERENCES,0.46806039488966317,4 × 100
REFERENCES,0.4692218350754936,5 × 100
REFERENCES,0.47038327526132406,Test Loss
REFERENCES,0.4715447154471545,CIFAR100: Loss scaling with Dataset Size
REFERENCES,0.4727061556329849,D : 0.164
REFERENCES,0.4738675958188153,"103
104
105"
REFERENCES,0.47502903600464574,Dataset Size 100
REFERENCES,0.47619047619047616,Test Loss
REFERENCES,0.47735191637630664,SVHN: Loss scaling with Dataset Size
REFERENCES,0.47851335656213706,D : 0.242
REFERENCES,0.4796747967479675,"Figure S3: This ﬁgure shows scaling trends of CE loss with dataset size for various image datasets. The
exponents extracted from these ﬁts and their associated input-space dimensionalities are shown in Figure 1."
REFERENCES,0.4808362369337979,"4
6
8
10
12
Dimension 4 6 8 10 12 14 4/
P"
REFERENCES,0.4819976771196283,Teacher/Student Model Size Exponents
REFERENCES,0.4831591173054588,"d =
4/
P"
REFERENCES,0.4843205574912892,"Figure S4: This ﬁgure shows the variation of αP with the input-space dimension. The exponent αP is the
scaling exponent of loss with model size for teacher-student setup."
REFERENCES,0.48548199767711964,Under review as a conference paper at ICLR 2022
REFERENCES,0.48664343786295006,"103
104"
REFERENCES,0.4878048780487805,Dataset size (D) 100 Loss
REFERENCES,0.4889663182346109,CIFAR-10 varying width (d=28)
REFERENCES,0.4901277584204414,D: 0.42
REFERENCES,0.4912891986062718,D: 0.50
REFERENCES,0.4924506387921022,D: 0.54
REFERENCES,0.49361207897793263,D: 0.58
REFERENCES,0.49477351916376305,D: 0.58
REFERENCES,0.4959349593495935,"103
104"
REFERENCES,0.49709639953542395,Dataset size (D) 100 Loss
REFERENCES,0.49825783972125437,CIFAR-10 varying depth (k=10)
REFERENCES,0.4994192799070848,D: 0.48
REFERENCES,0.5005807200929152,D: 0.55
REFERENCES,0.5017421602787456,D: 0.58
REFERENCES,0.502903600464576,D: 0.58 1 2 4 10
WIDTH FACTOR,0.5040650406504065,"12
Width factor 10 16 28"
DEPTH,0.5052264808362369,"40
Depth"
DEPTH,0.5063879210220673,"Figure S5: Effect of aspect ratio on dataset scaling We ﬁnd that for WRN-d-k trained on CIFAR-10, varying
depth from 10 to 40 has a relatively mild effect on scaling behavior, while varying the width multiplier, k, from 1
to 12 has a more noticeable effect, up until a saturating width."
DEPTH,0.5075493612078978,"C
PROOF OF THEOREM 1"
DEPTH,0.5087108013937283,We now prove Theorem 1 repeated below for convenience.
DEPTH,0.5098722415795587,"Theorem 1. Let ℓ(f) be the test loss as a function of network output, (L = E [ℓ(f)]), and let
fT be the network output after T training steps, thought of as a random variable over weight
initialization, draws of the training dataset, and optimization seed. Further let fT be concentrating
with E[(fT −E[fT ])k] = O (ϵ) ∀k ≥2. If ℓis a ﬁnite degree polynomial, or has bounded second
derivative, or is 2-H¨older, then E [ℓ(fT )] −ℓ(E [fT ]) = O(ϵ)."
DEPTH,0.5110336817653891,"Proof. Case 1 – ﬁnite degree polynomial: In this case, we can write,"
DEPTH,0.5121951219512195,"ℓ(fT ) −ℓ(E [fT ]) = K
X k=1"
DEPTH,0.5133565621370499,ℓ(k) (E [fT ])
DEPTH,0.5145180023228804,"k!
(fT −E [fT ])k ,
(S1)"
DEPTH,0.5156794425087108,"where K is the polynomial degree and ℓ(k) is the k-th derivative of ℓ. Taking the expectation of (S1)
and using the moment scaling proves the result."
DEPTH,0.5168408826945412,"Case 2 – bounded second derivative: The quadratic mean value theorem states that for any fT , there
exists a c such that,"
DEPTH,0.5180023228803716,ℓ(fT ) −ℓ(E [fT ]) = (fT −E [fT ]) ℓ′(E [fT ]) + 1
DEPTH,0.519163763066202,"2ℓ′′(c) (fT −E [fT ])2 .
(S2)"
DEPTH,0.5203252032520326,Taking the expectation of (S2) and using the fact that f ′′(c) is bounded yields the desired result.
DEPTH,0.521486643437863,"Case 3 – 2-H¨older: Lastly, the loss being 2-H¨older means we may write,"
DEPTH,0.5226480836236934,"ℓ(fT ) −ℓ(E [fT ]) ≤|ℓ(fT ) −ℓ(E [fT ])| ≤Kℓ(fT −E [fT ])2 .
(S3)"
DEPTH,0.5238095238095238,"Again, taking the expectation of this inequality completes the proof."
DEPTH,0.5249709639953543,"A note on loss variance
Theorem 1 concerns the mean loss, however we would also like to
understand if this scaling holds for typical instances. This can be understood by examining how the
variance of the loss or altetnatively how E [|ℓ(fT ) −ℓ(E [fT ])|] scales."
DEPTH,0.5261324041811847,"For Case 3 – 2-H¨older loss, we can rerun the argument of Theorem 1, using (S3) to yield
E [|ℓ(fT ) −ℓ(E [fT ])|] = O (ϵ)."
DEPTH,0.5272938443670151,"For Cases 1 and 2, we can attempt to apply the same argument as in the proof. This almost works. In
particular, using H¨older’s inequality, E[(fT −E[fT ])k] = O (ϵ) ∀k ≥2 implies E[|fT −E[fT ]|k] ="
DEPTH,0.5284552845528455,Under review as a conference paper at ICLR 2022
DEPTH,0.5296167247386759,O (ϵ) ∀k ≥2. Taking the absolute value and expectation of (S1) or (S2) then gives
DEPTH,0.5307781649245064,"E [|ℓ(fT ) −ℓ(E [fT ])|] ≤|ℓ′ (E [fT ])| E [|fT −E [fT ]|] + O (ϵ) .
(S4)"
DEPTH,0.5319396051103368,"In general, the above assumptions on ℓand fT imply only that E [|fT −E [fT ]|] = O (√ϵ) and thus
typical instances of the loss will exhibit a less dramatic scaling with ϵ than the mean. If we further
assume, however, that fT on average has been trained such as to be sufﬁciently close to a local
minimum of the loss, such that |ℓ′ (E [fT ])| = O (√ϵ), then typical instances will also obey the O (ϵ)
scaling."
DEPTH,0.5331010452961672,"D
VARIANCE-LIMITED DATASET SCALING"
DEPTH,0.5342624854819977,"In this section, we expand on our discussion of the variance-limited dataset scaling, L(D) −
limD→∞L(D) = O
 
D−1
. We ﬁrst explain some intuition for why this behavior might be
expected for sufﬁciently smooth loss. We then derive it explicitly for losses that are polynomial in the
weights. Finally, we present non-smooth examples where the scaling can be violated either by having
unbounded loss, or ﬁrst derivative."
DEPTH,0.5354239256678281,"D.1
INTUITION"
DEPTH,0.5365853658536586,"At a high level, the intuition is as follows. For any ﬁxed value of weights, θ, the training loss with
D training points (thought of as a random variable over draws of the dataset), Ltrain[θ] concentrates
around the population loss Lpop[θ], with variance that scales as O
 
D−1
."
DEPTH,0.537746806039489,"Our optimization procedure can be thought of as a map from initial weights and training loss to
ﬁnal weights Op : (θ0, Ltrain[θ]) →θT . If this map is sufﬁciently smooth – for instance satisfying
the assumptions of Theorem 1 or well approximated by a Taylor series about all ED [Ltrain[θt]]
– then the output, θT , will also concentrate around its inﬁnite D limit with variance scaling as
O
 
D−1
. Finally, if the population loss is also sufﬁciently smooth, the test loss for a model
trained on D data points averaged over draws of the dataset, L(D) = ED [Lpop[θT ]], satisﬁes
L(D) −limD→∞L(D) = O
 
D−1
. We now walk through this in a little more detail."
DEPTH,0.5389082462253194,"Early time
We can follow this intuition a bit more explicitly for the ﬁrst few steps of gradient
descent. As the training loss at initialization, Ltrain[θ0], is a sample average over D i.i.d draws, it
concentrates around the population loss Lpop[θ0] with variance O
 
D−1
. As a result, the initial
gradient, g0 = ∂Ltrain"
DEPTH,0.5400696864111498,"∂θ0 will also concentrate with O
 
D−1
variance and so will the weights at time 1,
θ1 = θ0 −ηg0. The training loss at time step 1, is then given by"
DEPTH,0.5412311265969802,"Ltrain[θ1] = Ltrain[θ0 −g0] .
(S5)"
DEPTH,0.5423925667828107,"If Ltrain is sufﬁciently smooth around θ0 −ED [g0], then we get that Ltrain[θ1] concentrates around
Ltrain[θ1] with O
 
D−1
variance. We can keep bouncing back and forth between gradient (or
equivalently weights) and training loss for any number of steps T which does not scale with D.
Plugging this ﬁnal θT into the population loss and taking the expectation over draws of the training
set, L(D) = ED [Lpop[θT ]]. If Lpop is also sufﬁciently smooth, this yields L(D) −limD→∞L(D) =
O
 
D−1
."
DEPTH,0.5435540069686411,"Here we have used the term sufﬁciently smooth. A sufﬁcient set of criteria are given in Theorem 1;
however this is likely too restrictive. Indeed, any set of train and population loss for which a Taylor
series (or asymptomatic series with optimal truncation) give an O
 
D−1
error around the training
points ED [θt=0...T ] will have this behavior."
DEPTH,0.5447154471544715,"Local minimum
The above intuition relied on training for a number of steps that was ﬁxed as D is
taken large. Here we present some alternative intuition for the variance-limited scaling at late times,
as training approaches a local minimum in the loss. For simplicity we discuss a one-dimensional loss."
DEPTH,0.5458768873403019,"Consider a local minimum, θ∗, of the population loss. As D is taken large, with high probability, the
training loss will have a local minimum, ¯θ∗, such that |θ∗−¯θ∗| = O
 
D−1
. One way to see this,"
DEPTH,0.5470383275261324,Under review as a conference paper at ICLR 2022
DEPTH,0.5481997677119629,"is to note that for a generic local minimum the ﬁrst derivative changes sign, i.e. we can ﬁnd θ1, θ2
such that θ1 < θ∗< θ2 and either L′
pop[θ1] < 0, L′
pop[θ2] > 0 or L′
pop[θ2] < 0, L′
pop[θ1] > 0. To be
concrete let’s focus on the ﬁrst case (the argument will be identical in either case). As D becomes
large, the probability that the training loss at θ1 and θ2 differs signiﬁcantly from the population loss
approaches zero. This can be seen from Markov’s inequality, where, P
 L′
train[θ] −L′
pop[θ]
 > a

≤"
DEPTH,0.5493612078977933,"VarD(L′
train[θ])
a2
, or more dramatically from Hoeffding’s inequality (assuming bounded Ltrain −Lpop
lying in an interval of size I)"
DEPTH,0.5505226480836237,"P
 L′
train[θ] −L′
pop[θ]
 > a

≤2e−2"
DEPTH,0.5516840882694541,"I D2a2 .
(S6)"
DEPTH,0.5528455284552846,"Here to have non-vanishing probability as we take D large, L′
train[θ1] and L′
train[θ2] must be closer
than O
 
D−1
. If θ1 and θ2 are taken to be O
 
D0
, then L′
train must change sign, indicating an
extremum of Ltrain; however we can do even better. If we assume Ltrain is Lipshitz about θ∗then we
can still ensure a sign change even if |θ1 −θ∗|, |θ2 −θ∗| = O
 
D−1
. Using concentration of L′′
train[θ]
ensures the extremum is a local minimum. For non-generic minimum (i.e. vanishing ﬁrst derivatives)
we can apply the same arguments to higher order derivatives (assuming they exist) of Lpop. Thus for
a local minimum of Lpop, with high probability Ltrain will have a corresponding minimum within a
distance O
 
D−1"
DEPTH,0.554006968641115,"If we now consider an initialization procedure, θ0, and training procedure such that training converges
to the local minimum of the training loss, ¯θ∗, and that the population loss is sufﬁciently smooth about
θ∗(e.g. Lipshitz), then ED[Ltrain[¯θ∗]−Lpop[θ∗]] = ED[Ltrain[¯θ∗]−Lpop[¯θ∗]]+ED[Lpop[¯θ∗]−Lpop[θ∗]].
The ﬁrst term vanishes, while the second is O(D−1). If we further assume that this happens on
average over choices of θ0 then we expect L(D) −limD→∞L(D) = O
 
D−1
."
DEPTH,0.5551684088269454,"SGD
At ﬁrst blush it may be surprising that the variance-limited scaling holds even for mini-batch
training. Indeed in this case, there is batch noise that comes in at a much higher scale than any
variance due to the ﬁnite training set size. Indeed, the effect of mini-batching changes the ﬁnal test
loss, however if we ﬁx the SGD procedure or average over SGD seeds, as we take D large, we can
still ask how the training loss for a model trained under SGD on a training set of size D differs from
that for a model trained under SGD on an inﬁnite training set."
DEPTH,0.5563298490127758,"To see this, we ﬁst consider averaging over minibatches of size B, but where points are drawn i.i.d.
with replacement. If we denote the batch at step t by Bt and the average over independent draws of
this batch by EB [•], then note we can translate moments with respect to batch draws with empirical
averages over the entire training set. Explicitly, consider ca and da potentially correlated, but each
drawn i.i.d. within a batch. We have that, EB ""
1
B X"
DEPTH,0.5574912891986062,"a∈Bt
ca # = 1 D D
X"
DEPTH,0.5586527293844367,"a=1
ca EB"
DEPTH,0.5598141695702671,""" 
1
B X"
DEPTH,0.5609756097560976,"a∈Bt
ca"
DEPTH,0.562137049941928,"!  
1
B X"
DEPTH,0.5632984901277585,"a′∈Bt
da′ !#"
DEPTH,0.5644599303135889,"=

1 −1 B"
DEPTH,0.5656213704994193,"  
1
D D
X"
DEPTH,0.5667828106852497,"a=1
ca"
DEPTH,0.5679442508710801,"!  
1
D D
X"
DEPTH,0.5691056910569106,"a′=1
da′ ! + 1 B
1
D D
X"
DEPTH,0.570267131242741,"a=1
cada . (S7)"
DEPTH,0.5714285714285714,"This procedure means, after taking an average over draws of SGD batch, rather than thinking about
a function of mini-batch averages, we can equivalently consider a modiﬁed function, with explicit
dependence on the batch size, but that is only a function of empirical means over the training set. We
can thus recycle the above intuition for the scaling of smooth functions of empirical means."
DEPTH,0.5725900116144018,"The above relied on independently drawing every sample from every batch. At the other extreme, we
can consider drawing batches without shufﬂing and increasing training set size by B datapoints at
a time, so as to keep the initial set of batches in an epoch ﬁxed. In this case, the ﬁrst deviation in
training between a dataset of size D and one of size D + B happens at the last batch in the ﬁrst epoch
after processing D datapoints."
DEPTH,0.5737514518002322,Under review as a conference paper at ICLR 2022
DEPTH,0.5749128919860628,"As an extreme example, consider the case where D > BT. In this case, as we only take T
steps, the loss is constant for all D > BT and so limD→∞L(D; T; B) = L(BT; T; B) and thus
L(D > BT) −limD→∞L(D) = 0 (and in particular is trivially O
 
D−1
)."
DEPTH,0.5760743321718932,"D.2
POLYNOMIAL LOSS"
DEPTH,0.5772357723577236,"Before discussing neural network training we review the concentration behavior of polynomials of
sample means."
DEPTH,0.578397212543554,"Lemma 1. Let ¯c(i) =
1
D
PD
a=1 c(i)
a
for i = 0 . . . J be empirical means, over D i.i.d. draws of
c(i)
a and let c(i) denote the distributional mean. Further, let X = (¯c(0))k0(¯c(1))k1 · · · (¯c(J))kJ be a
monomial in the sample means. Then X concentrates with moments O
 
D−1
,"
DEPTH,0.5795586527293844,"ED
h
X −(c(0))k0(c(1))k1 · · · (c(J))kJni
= O
 
D−1
.
(S8)"
DEPTH,0.5807200929152149,"Here, ED [•] denotes the average over independent draws of D samples."
DEPTH,0.5818815331010453,Proof. To establish this we can proceed by direct computation.
DEPTH,0.5830429732868757,"ED
h
X −(c(0))k0(c(1))k1 · · · (c(J))kJni = n
X"
DEPTH,0.5842044134727061,"p=0
(−1)n−p
 n
p"
DEPTH,0.5853658536585366,"
ED [Xp]

(c(0))k0(c(1))k1 · · · (c(J))kJn−p
(S9)"
DEPTH,0.586527293844367,Each term in the sum can be computed using
DEPTH,0.5876887340301974,"ED [Xp] = ED
h
(¯c(0))pk0(¯c(1))pk1 · · · (¯c(J))pkJi =
1"
DEPTH,0.5888501742160279,"D(p PJ
i=0 ki)
X"
DEPTH,0.5900116144018583,"{a(i)
α }
ED"
DEPTH,0.5911730545876888,"
c(0)"
DEPTH,0.5923344947735192,"a(0)
1
· · · c(0)"
DEPTH,0.5934959349593496,"a(0)
pk0"
DEPTH,0.59465737514518," 
c(1)"
DEPTH,0.5958188153310104,"a(1)
1
· · · c(1)"
DEPTH,0.5969802555168409,"a(1)
pk1"
DEPTH,0.5981416957026713,"
· · ·

c(J)"
DEPTH,0.5993031358885017,"a(J)
1
· · · c(J)"
DEPTH,0.6004645760743321,"a(J)
pkJ  =
1"
DEPTH,0.6016260162601627,"D(p PJ
i=0 ki)
X"
DEPTH,0.6027874564459931,"{a(i)
α ̸=a(j)
β }
ED"
DEPTH,0.6039488966318235,"
c(0)"
DEPTH,0.6051103368176539,"a(0)
1
· · · c(0)"
DEPTH,0.6062717770034843,"a(0)
pk0"
DEPTH,0.6074332171893148," 
c(1)"
DEPTH,0.6085946573751452,"a(1)
1
· · · c(1)"
DEPTH,0.6097560975609756,"a(1)
pk1"
DEPTH,0.610917537746806,"
· · ·

c(J)"
DEPTH,0.6120789779326364,"a(J)
1
· · · c(J)"
DEPTH,0.6132404181184669,"a(J)
pkJ "
DEPTH,0.6144018583042973,"+ O
 
D−1"
DEPTH,0.6155632984901278,"= D(D −1) · · · (D −(p PJ
i=0 ki −1))"
DEPTH,0.6167247386759582,"D(p PJ
i=0 ki)"
DEPTH,0.6178861788617886,"
c(0)pk0 
c(1)pk1
· · ·

c(J)pkJ
+ O
 
D−1"
DEPTH,0.6190476190476191,"=

c(0)k0 
c(1)k1
· · ·

c(J)kJp
+ O
 
D−1
."
DEPTH,0.6202090592334495,Plugging this into (S9) establishes the lemma.
DEPTH,0.6213704994192799,"In the above, we use the multi-index notation {a(i)
α } for the collection of indices on the ci and the
notation {a(i)
α ̸= a(j)
β } for the subset of terms in the sum where all indices take different values."
DEPTH,0.6225319396051103,"Lemma 1 immediately implies that the mean of polynomials of ¯c(i) concentrate around their inﬁnite
data limit."
DEPTH,0.6236933797909407,"ED
h
g

¯c(0), ¯c(1), . . . , ¯c(K)
−g

c(0), c(1), . . . , c(K)ni
= O
 
D−1
,
(S10)"
DEPTH,0.6248548199767712,"for g ∈PK

¯c(0), ¯c(1), . . . , ¯c(K)
."
DEPTH,0.6260162601626016,"With this out of the way, we can proceed to analysing the scaling of trained neural networks. Here
we consider the simpliﬁed setting where the network map, f, and loss ℓevaluated on each training
example, xa = (xa, ya), are polynomial of degree J and K in the weights, θµ,"
DEPTH,0.627177700348432,"f(x) = J
X"
DEPTH,0.6283391405342624,"i=1
b(i)
µ1µ2...µi(x)θµ1θµ2 · · · θµi
ℓ(xa) = K
X"
DEPTH,0.629500580720093,"i=1
c(i)
µ1µ2...µi(xa)θµ1θµ2 · · · θµi .
(S11)"
DEPTH,0.6306620209059234,Under review as a conference paper at ICLR 2022
DEPTH,0.6318234610917538,"The training loss can then be written as,"
DEPTH,0.6329849012775842,"Ltrain = K
X"
DEPTH,0.6341463414634146,"i=1
¯c(i)
µ1µ2...µiθµ1θµ2 · · · θµi ,
¯c(i) = 1 D D
X"
DEPTH,0.6353077816492451,"a=1
c(i)(xa) .
(S12)"
DEPTH,0.6364692218350755,Here we have used the convention that the repeated weight indices µj are summed over.
DEPTH,0.6376306620209059,"Gradient Descent
As a result of the gradient descent weight update, θt+1 = θt −η ∂Ltrain"
DEPTH,0.6387921022067363,"∂θ , the
weights at time T are a polynomial of degree (K −1)T in the ¯c(i)."
DEPTH,0.6399535423925667,"θT ∈P(K−1)T
h
¯c(0), ¯c(1), . . . , ¯c(K)i
.
(S13)"
DEPTH,0.6411149825783972,"The coefﬁcients of this polynomial depend on the initial weights, θ0. Plugging these weights back
into the network output, we have that the network function at time T is again a polynomial in ¯c(i),
now with degree J (K −1)T ."
DEPTH,0.6422764227642277,"fT (x) ∈PJ(K−1)T
h
¯c(0), ¯c(1), . . . , ¯c(K)i
.
(S14)"
DEPTH,0.6434378629500581,"Thus, again using Lemma 1, fT concentrates with variance O(D−1)."
DEPTH,0.6445993031358885,"ED
h
(fT −ED [fT ])2i
= O
 
D−1
.
(S15)"
DEPTH,0.645760743321719,and by Theorem 1 the loss will obey they variance-limited scaling.
DEPTH,0.6469221835075494,"Stochastic Gradient Descent
We now consider the same setup of polynomial loss, but now trained
via stochastic gradient descent (SGD). We consider SGD batches drawn i.i.d. with replacement and
are interested in the test loss averaged over SGD draws, with ﬁxed batch size, B."
DEPTH,0.6480836236933798,"We proceed by proving the following lemma, which allows us to reuse a similar argument to the GD
case."
DEPTH,0.6492450638792102,Lemma 2. Let ˜c(i;t) = 1
DEPTH,0.6504065040650406,"B
P
a∈Bt c(i)
a for i = 0 . . . J be mini-batch averages, over B i.i.d. draws of"
DEPTH,0.6515679442508711,"c(i)
a . Further, let X = (˜c(0;t0))k0(˜c(1;t1))k1 · · · (˜c(J;tJ))kJ be a monomial in the mini-batch means."
DEPTH,0.6527293844367015,"Then EB [X] ∈PPJ
i=0 ki
h
¯d(0), ¯d(1), . . . , ¯d(QJ
i=0(ki+1)−1)i
, where ¯d(i) are empirical means over the
full training set of i.i.d. random variables as in Lemma 1 and EB [•] denotes the expectation over
draws of SGD batches of size B."
DEPTH,0.6538908246225319,"Proof. Expectations over draws of batches at different time steps are independent. Thus, WLOG, we
can consider t := t0 = t1 = · · · = tJ. We can again proceed by direct computation, expanding the
mini-batch sums."
DEPTH,0.6550522648083623,"EB [X] =
1"
DEPTH,0.6562137049941928,"B
PJ
i=0 ki EB  
X"
DEPTH,0.6573751451800233,"{a(i)
α }∈Bt"
DEPTH,0.6585365853658537,"
c(0)"
DEPTH,0.6596980255516841,"a(0)
1
· · · c(0)"
DEPTH,0.6608594657375145,"a(0)
k0"
DEPTH,0.662020905923345," 
c(1)"
DEPTH,0.6631823461091754,"a(1)
1
· · · c(1)"
DEPTH,0.6643437862950058,"a(1)
k1"
DEPTH,0.6655052264808362,"
· · ·

c(J)"
DEPTH,0.6666666666666666,"a(J)
1
· · · c(J)"
DEPTH,0.667828106852497,"a(J)
kJ  . (S16)"
DEPTH,0.6689895470383276,"To proceed, we must keep track of terms in the sum where the a(i)
α take the same or different values.
If all a(i)
α are different, the expectation over batch draws fully factorizes. More generally (S16) can
be decomposed as a sum over products."
DEPTH,0.670150987224158,"One way of keeping track of the index combinatorics is to introduce a set of graphs, Γ, where each
graph γ ∈Γ has k0 vertices of type 0, k1 vertices of type 1, . . . , and kJ vertices of type J (one vertex
for each a(i)
α index). Any pair of vertices may have zero or one edge between them. For any set of
three vertices, v1, v2, and v3 with edges (v1, v2) and (v2, v3) there must also be an edge (v1, v3). The
set Γ consists of all possible ways of connecting these vertices consistent with these rules."
DEPTH,0.6713124274099884,Under review as a conference paper at ICLR 2022
DEPTH,0.6724738675958188,"For each graph, γ, we denote connected components by σ and denote the number of vertices of type i
within the connected component σ by m(i)
σ . With this we can write the sum, (S16) as"
DEPTH,0.6736353077816493,"EB [X] =
X"
DEPTH,0.6747967479674797,"γ∈Γ
Sγ(B)
Y"
DEPTH,0.6759581881533101,"σ∈γ
EB ""
1
B X a∈Bt"
DEPTH,0.6771196283391405,"
c(0)
a
m(0)
σ 
c(1)
a
m(1)
σ · · ·

c(J)
a
m(J)
σ
# =
X"
DEPTH,0.6782810685249709,"γ∈Γ
Sγ(B)
Y σ∈γ"
D,0.6794425087108014,"1
D D
X a=1"
D,0.6806039488966318,"
c(0)
a
m(0)
σ 
c(1)
a
m(1)
σ · · ·

c(J)
a
m(J)
σ =
X"
D,0.6817653890824622,"γ∈Γ
Sγ(B)
Y"
D,0.6829268292682927,"σ∈γ
¯d({m(0)
σ ,m(1)
σ ,...,m(J)
σ
}) . (S17)"
D,0.6840882694541232,"Here Sγ(B) is a combinatoric factor associated to each graph, not relevant for the argument. The
m(i)
σ take on values 0 to ki, so the multi-index, can take on QJ
i=1(ki + 1) different values, which we
re-index to ¯d(0), ¯d(1), . . . , ¯d(QJ
i=1(ki+1)−1). Meanwhile, the degree of (S17) in ¯d(i) is bounded by the
number of total vertices in each graph, i.e. PJ
i=0 ki. This establishes the lemma."
D,0.6852497096399536,"For a polynomial loss of degree K, the mini-batch training loss at each time step takes the form"
D,0.686411149825784,"L(t)
train = K
X"
D,0.6875725900116144,"i=1
˜c(i;t)
µ1µ2...µiθµ1θµ2 · · · θµi ,
˜c(i;t) = 1 B X"
D,0.6887340301974448,"a=∈Bt
c(i)(xa) .
(S18)"
D,0.6898954703832753,"The update rule, θt+1 = θt −η ∂L(t+1)
train
∂θ
ensures that θT is a polynomial of degree (K −1)T in the
˜c(i;0), ˜c(i;1), · · · , ˜c(i;T )"
D,0.6910569105691057,"θT ∈P(K−1)T
h
˜c(0;0), ˜c(0;1), . . . , ˜c(0;T ), ˜c(1;0), ˜c(1;1), . . . , ˜c(1;T ), . . . , ˜c(K;0), ˜c(K;1), . . . , ˜c(K;T )i
, (S19)"
D,0.6922183507549361,"and consequently, denoting the test loss evaluated at θT by L[θT ],"
D,0.6933797909407665,"L[θT ] ∈PK(K−1)T
h
˜c(0;0), ˜c(0;1), . . . , ˜c(0;T ), ˜c(1;0), ˜c(1;1), . . . , ˜c(1;T ), . . . , ˜c(K;0), ˜c(K;1), . . . , ˜c(K;T )i
. (S20)"
D,0.6945412311265969,"Using Lemma 2, the expectation of L[θT ] over draws of SGD batches is given by"
D,0.6957026713124274,"EB [L[θT ]] ∈PK(K−1)T
h
¯d(0), . . . , ¯d(KK(K−1)T K)i
.
(S21)"
D,0.6968641114982579,"Finally, denoting ED [EB [L[θT ]]] by L(D; B) and applying Lemma 1 gives"
D,0.6980255516840883,"L(D; B) −lim
D→∞L(D; B) = O
 
D−1
.
(S22)"
D,0.6991869918699187,"D.3
NON-SMOOTH EXAMPLES"
D,0.7003484320557491,"Here we present two worked examples where non-bounded or non-smooth loss leads to violations of
the variance dominated scaling. In example one, the system obeys the variance dominated scaling
at early times, but exhibits different behavior for times larger than the dataset size. In the second
example, the system violates the variance dominated scaling even for two gradient descent steps, as a
result of an unbounded derivative in the loss.3"
D,0.7015098722415796,"Example 1 – unbounded loss at late times
Consider a dataset with two varieties of data points,
drawn with probabilities α and 1 −α, and one-dimensional quadratic losses, ℓ1 (concave up) and ℓ2
(concave down), on these two varieties."
D,0.70267131242741,ℓ1(θ) = 1
D,0.7038327526132404,"2θ2 ,
ℓ2(θ) = −1"
D,0.7049941927990708,"2θ2 .
(S23)"
D,0.7061556329849012,3We thank Anonymous for suggesting these two types of examples.
D,0.7073170731707317,Under review as a conference paper at ICLR 2022
D,0.7084785133565621,"If, in a slight abuse of notation, we further denote the training loss on a sample with n1 points of type
1 and D −n1 points of type two by ℓn1 and the population loss at a given value of the weight by Lpop,
we have"
D,0.7096399535423926,"ℓn1 =
n1 D −1 2"
D,0.710801393728223,"
θ2 ,
Lpop =

α −1 2"
D,0.7119628339140535,"
θ2 .
(S24)"
D,0.7131242740998839,"For this example we take α > 1/2. In this case, the minimum of the population loss is at zero, while
the minimum of the training loss can be at zero, or at ±∞depending on whether the training sample
has n1 greater than or less than D/2. We can thus create a situation where at late training times, θT
does not concentrate around the minimum of the population loss."
D,0.7142857142857143,"As we work through this example explicitly, we will see the following. (i) A mismatch larger than
O
 
D−1
between the population minimum and the minimum found by training on a sample set of
size D requires times T larger than a constant multiple of D. (ii) The quantity we study throughout
this work is the difference between the inﬁnite data limit of the test loss, and the ﬁnite data value,
L(D) −limD→∞L(D). The minimum of the inﬁnite data limit of the test loss is not the same as
the minimum of the population loss, min limD→∞L(D) ̸= minθ Lpop. In this example one diverges,
while the other is ﬁnite. In particular this example evades the scaling result by L(D) for times larger
than D having a diverging limit."
D,0.7154471544715447,"Explicitly, we study the evolution of the model under gradient ﬂow."
D,0.7166085946573751,"˙θ = −2
n1 D −1 2"
D,0.7177700348432056,"
θ ,
θT = e−2(
n1 D −1"
D,0.718931475029036,"2)T θ0 .
(S25)"
D,0.7200929152148664,The test loss averaged over draws of the dataset is given by
D,0.7212543554006968,L(D; T) = En1
D,0.7224157955865272,"
α −1 2"
D,0.7235772357723578,"
θ2
T"
D,0.7247386759581882,"
= e2T

α −1 2"
D,0.7259001161440186," 
1 −α

1 −e−4T"
D,0.727061556329849,"D
D
θ2
0
(S26)"
D,0.7282229965156795,If we consider this loss at large D and ﬁxed T we get
D,0.7293844367015099,L(D; T) = e−4(α−1
D,0.7305458768873403,"2)T

α −1 2"
D,0.7317073170731707,"
θ2
0"
D,0.7328687572590011,"
1 + 8T 2α(1 −α)"
D,0.7340301974448316,"D
+ O
 
D−2
,
(S27)"
D,0.735191637630662,"and thus L(D; T) −limD→∞L(D; T) = O
 
D−1
as expected."
D,0.7363530778164924,If on the other hand we consider taking T ≫D we have
D,0.7375145180023229,"L(D; T ≫D) = e2T

α −1 2"
D,0.7386759581881533,"
(1 −α)D θ2
0 ,
(S28)"
D,0.7398373983739838,"the limit limD,T →∞L(D; T ≫D) diverges."
D,0.7409988385598142,"Lastly, we note that if we take T = βD with β < | log(1 −α)|/2 we can approach the large D limit
with non-generic, tuneable exponential convergence."
D,0.7421602787456446,"Example 2 – unbounded derivative
Again, consider a two variety setup, this case with equal
probabilities and per sample losses,"
D,0.743321718931475,ℓ1(θ) = 1
D,0.7444831591173054,2θ2 + 1
D,0.7456445993031359,"2α|θ|α ,
ℓ2(θ) = 1"
D,0.7468060394889663,2θ2 −1
D,0.7479674796747967,"2α|θ|α .
(S29)"
D,0.7491289198606271,"We will consider different values of α > 0. The train loss and population loss are then,"
D,0.7502903600464577,ℓn1 = 1
D,0.7514518002322881,2θ2 + 1 α n1 D −1 2
D,0.7526132404181185,"
|θ|α ,
Lpop = 1"
D,0.7537746806039489,"2θ2 .
(S30)"
D,0.7549361207897793,"We consider a model initialized to θ0 = 1 and trained for two steps of gradient descent with learning
rate 1."
D,0.7560975609756098,"gt = θt +
n1 D −1 2"
D,0.7572590011614402,"
θt|θt|α−2 ,
θt+1 = θt −gt .
(S31)"
D,0.7584204413472706,Under review as a conference paper at ICLR 2022
D,0.759581881533101,Two update steps gives
D,0.7607433217189314,"θ2 =

n1 D −1 2 "
D,0.7619047619047619,"α
.
(S32)"
D,0.7630662020905923,The test loss is given by the population loss evaluated at θ2 averaged over test set draws.
D,0.7642276422764228,"L(D) = En1 1 2θ2
2"
D,0.7653890824622532,"
=
1
2D+1 D
X n1=0"
D,0.7665505226480837," D
n1"
D,0.7677119628339141," 
n1 D −1 2  2α = r D
2π Z ∞"
D,0.7688734030197445,"−∞
e−2D(x−1"
D,0.7700348432055749,"2)
2 x −1 2 "
D,0.7711962833914053,"2α
+ O
 
D−1"
D,0.7723577235772358,"= Γ
 
α + 1 2
"
D,0.7735191637630662,"21+α√π D−α + O
 
D−1
. (S33)"
D,0.7746806039488966,"Here we have approximated the binomial distribution at large D with a normal distribution using
Stirling’s approximation."
D,0.775842044134727,"Note that if α ≥1 then L(D) −limD→∞L(D) = O
 
D−1
i.e. the ﬁnite sample loss approaches
the inﬁnite data loss with the predicted variance-limited scaling. For 0 < α < 1, we get a different
scaling controlled by α. Note that the gradient, expression (S31) is singular at the origin for α
precisely in this range."
D,0.7770034843205574,"In summary, this example achieves a different scaling exponent through a diverging gradient."
D,0.778164924506388,"E
PROOF OF THEOREMS 2 AND 3"
D,0.7793263646922184,"In this section we detail the proof of Theorems 2 and 3. The key observation is to make use of the
fact that nearest neighbor distances for D points sampled i.i.d. from a d-dimensional manifold have
mean ED,x [|x −ˆx|] = O
 
D−1/d
, where ˆx is the nearest neighbor of x and the expectation is the
mean over data-points and draws of the dataset see e.g. (Levina & Bickel, 2005)."
D,0.7804878048780488,"The theorem statements are copied for convenience. In the main, in an abuse of notation, we used
L(f) to indicate the value of the test loss as a function of the network f, and L(D) to indicate the
test loss averaged over the population, draws of the dataset, model initializations and training. To be
more explicit below, we will use the notation ℓ(f(x)) to indicate the test loss for a single network
evaluated at single test point."
D,0.7816492450638792,"Theorem 2. Let ℓ(f), f and F be Lipschitz with constants KL, Kf, and KF and ℓ(F) = 0. Further
let D be a training dataset of size D sampled i.i.d from Md and let f(x) = F(x), ∀x ∈D then
L(D) = O
 
KLmax(Kf, KF)D−1/d
."
D,0.7828106852497096,"Proof. Consider a network trained on a particular draw of the training data. For each training
point, x, let ˆx denote the neighboring training data point. Then by the above Lipschitz assump-
tions and the vanishing of the loss on the true target, we have ℓ(f(x)) ≤KL |f(x) −F(x)| ≤
KL (Kf + KF) |x −ˆx|. With this, the average test loss is bounded as"
D,0.7839721254355401,"L(D) ≤KL (Kf + KF) ED,x [|x −ˆx|] = O

KLmax(Kf, KF)D−1/d
.
(S34)"
D,0.7851335656213705,"In the last equality, we used the above mentioned scaling of nearest neighbor distances."
D,0.7862950058072009,"Theorem 3. Let ℓ(f), f and F be Lipschitz with constants KL, Kf, and KF. Further let f(x) =
F(x) for P points sampled i.i.d from Md then L(P) = O
 
KLmax(Kf, KF)P −1/d
."
D,0.7874564459930313,"Proof. Denote by P the P points, z, for which f(z) = F(z). For each test point x let ˆx denote the
closest point in P, ˆx = argminP (|x −z|). Adopting this notation, the result follows by the same
argument as Theorem 2."
D,0.7886178861788617,Under review as a conference paper at ICLR 2022
D,0.7897793263646922,"F
RANDOM FEATURE MODELS"
D,0.7909407665505227,"Here we present random feature models in more detail. We begin by reviewing exact expressions
for the loss. We then go onto derive its asymptotic properties. We again consider training a
model f(x) = PP
µ=1 θµfµ(x), where fµ are drawn from some larger pool of features, {FM},"
D,0.7921022067363531,"fµ(x) = PS
M=1 PµMFM(x)."
D,0.7932636469221835,"Note, if {FM(x)} form a complete set of functions over the data distribution, than any target function,
y(x), can be expressed as y = PS
M=1 ωMFM(x). The extra constraint in a teacher-student model is
specifying the distribution of the ωM. The variance-limited scaling goes through with or without the
teacher-student assumption, however it is crucial for analysing the variance-limited behavior."
D,0.794425087108014,"As in Section 2.3 we consider models with weights initialized to zero and trained to convergence with
mean squared error loss."
D,0.7955865272938444,"Ltrain =
1
2D D
X"
D,0.7967479674796748,"a=1
(f(xa) −ya)2 .
(S35)"
D,0.7979094076655052,"The data and feature second moments play a central role in our analysis. We introduce the notation,"
D,0.7990708478513356,"C = Ex

F(x)F T (x)

,
¯C = 1 D D
X"
D,0.8002322880371661,"a=1
F(xa)F T (xa) ,
C = PCPT ,
¯C = P ¯CPT ."
D,0.8013937282229965,"K(x, x′) = 1"
D,0.8025551684088269,"S F T (x)F(x′) ,
¯K = K

Dtrain ,
K(x, x′) = 1"
D,0.8037166085946573,"P f T (x)f(x′) ,
¯K = K

Dtrain . (S36)"
D,0.8048780487804879,"Here the script notation indicates the full feature space while the block letters are restricted to the
student features. The bar represents restriction to the training dataset. We will also indicate kernels
with one index in the training set as ⃗K(x) := K(x, xa=1...D) and ⃗K(x) := K(x, xa=1...D). After
this notation spree, the test loss can be written for under-parameterized models, P ≤D as"
D,0.8060394889663183,"L(D, P) = 1"
S ED,0.8072009291521487,"2S ED

Tr
 
C + ¯CPT ¯C−1C ¯C−1P ¯C −2 ¯CPT ¯C−1PC

.
(S37)"
S ED,0.8083623693379791,"and for over-parameterized models (at the unique minimum found by GD, SGD, or projected Newton’s
method),"
S ED,0.8095238095238095,"L(D, P) = 1"
S ED,0.81068524970964,"2Ex,D
h
K(x, x) + ⃗K(x)T ¯K−1 ¯K ¯K−1 ⃗K(x) −2 ⃗K(x)T ¯K−1 ⃗K(x)
i
.
(S38)"
S ED,0.8118466898954704,"Here the expectation ED [•] is an expectation with respect to iid draws of a dataset of size D from the
input distribution, while Ex [•] is an ordinary expectation over the input distribution. Note, expression
(S37) is also valid for over-parameterized models and (S38) is valid for under-parameterized models
if the inverses are replaces with the Moore-Penrose pseudo-inverse. Also note, the two expressions
can be related by echanging the projections onto ﬁnite features with the projection onto the training
dataset and the sums of teacher features with the expectation over the data manifold. This realizes the
duality between dataset and features discussed above."
S ED,0.8130081300813008,"F.1
ASYMPTOTIC EXPRESSIONS"
S ED,0.8141695702671312,We are interested in (S37) and (S38) in the limits of large P and D.
S ED,0.8153310104529616,"Variance-limited scaling
We begin with the under-parameterized case. In the limit of lots of data
the sample estimate of the feature feature second moment matrix, ¯C, approaches the true second
moment matrix, C. Explicitly, if we deﬁne the difference, δC by ¯C = C + δC. We have"
S ED,0.8164924506387921,ED [δC] = 0
S ED,0.8176538908246226,ED [δCM1N1δCM2N2] = 1
S ED,0.818815331010453,D (Ex [FM1(x)FN1(x)FM2(x)FN2(x)] −CM1N1CM2N2)
S ED,0.8199767711962834,"ED [δCM1N1 · · · δCMnNn] = O
 
D−2
∀n > 2 . (S39)"
S ED,0.8211382113821138,Under review as a conference paper at ICLR 2022
S ED,0.8222996515679443,The key takeaway from (S39) is that the dependence on D is manifest.
S ED,0.8234610917537747,Using these expressions in (S37) yields.
S ED,0.8246225319396051,"L(D, P) = 1"
S TR,0.8257839721254355,"2S Tr
 
C −CPT C−1PC
"
S TR,0.826945412311266,"+
1
2DS P
X"
S TR,0.8281068524970964,"M1,2N1,2=1
TM1N1M2N2
h
δM1M2
 
PT C−1P
"
S TR,0.8292682926829268,"N1N2 + (C−1PC2PT C−1)M1M2C−1
N1N2"
S TR,0.8304297328687572,"−2
 
CPT C−1P
"
S TR,0.8315911730545877,"M1M2
 
PT C−1P
 N1N2"
S TR,0.8327526132404182,"i
+ O
 
D−2
. (S40)"
S TR,0.8339140534262486,"Here we have introduced the notation, TM1N1M2N2 = Ex [FM1(x)FN1(x)FM2(x)FN2(x)]."
S TR,0.835075493612079,"As above, deﬁning"
S TR,0.8362369337979094,"L(P) := lim
D→∞L(D, P) = 1"
S TR,0.8373983739837398,"2S Tr
 
C −CPT C−1PC

.
(S41)"
S TR,0.8385598141695703,"we see that though L(D, P) −L(P) is a somewhat cumbersome quantity to compute, involving the
average of a quartic tensor over the data distribution, its dependence on D is simple."
S TR,0.8397212543554007,"For the over-parameterized case, we can similarly expand (S38) using K = K+δK. With ﬂuctuations
satisfying,"
S TR,0.8408826945412311,EP [δK] = 0
S TR,0.8420441347270615,EP [δKa1b1δKa2b2] = 1
S TR,0.8432055749128919,P (EP [fµ(xa1)fµ(xb1)fµ(xa2)fµ(xb2)] −Ka1b1Ka2b2)
S TR,0.8443670150987224,"EP [δKa1a1 · · · δKanan] = O
 
P −2
∀n > 2 . (S42)"
S TR,0.8455284552845529,This gives the expansion
S TR,0.8466898954703833,"L(D, P) = 1"
S TR,0.8478513356562137,"2Ex,D
h
K(x, x) −⃗K(x)T ¯K−1 ⃗K(x)
i
+ O(P −1) ,
(S43) and"
S TR,0.8490127758420442,L(D) = 1
S TR,0.8501742160278746,"2Ex,D
h
K(x, x) −⃗K(x)T ¯K−1 ⃗K(x)
i
.
(S44)"
S TR,0.851335656213705,"Resolution-limited scaling
We now move onto studying the parameter scaling of L(P) and dataset
scaling of L(D). We explicitly analyse the dataset scaling of L(D), with the parameter scaling
following via the dataset parameter duality."
S TR,0.8524970963995354,"Much work has been devoted to evaluating the expression, (S44) (Williams & Vivarelli, 2000;
Malzahn & Opper, 2002; Sollich & Halees, 2002). One approach is to use the replica trick – a tool
originating in the study of disordered systems which computes the expectation of a logarithm of a
random variable via simpler moment contributions and analyticity assumption (Parisi, 1980). The
replica trick has a long history as a technique to study the generalization properties of kernel methods
(Sollich, 1998; Malzahn & Opper, 2001; 2003; Urry & Sollich, 2012; Cohen et al., 2019; Gerace
et al., 2020; Bordelon et al., 2020). We will most closely follow the work of Canatar et al. (2021)
who use the replica method to derive an expression for the test loss of linear feature models in terms
of the eigenvalues of the kernel C and ¯ω, the coefﬁcient vector of the target labels in terms of the
model features."
S TR,0.8536585365853658,"L(D) =
κ2 1 −γ X i"
S TR,0.8548199767711963,"λi¯ω2
i
(κ + Dλi)2 , κ =
X i"
S TR,0.8559814169570267,"κλi
κ + Dλi
,
γ =
X i"
S TR,0.8571428571428571,"Dλ2
i
(κ + Dλi)2 .
(S45)"
S TR,0.8583042973286876,"This is the ridge-less, noise-free limit of equation (4) of Canatar et al. (2021). Here we analyze the
asymptotic behavior of these expressions for eigenvalues satisfying a power-law decay, λi = i−(1+αK)"
S TR,0.859465737514518,"and for targets coming from a teacher-student setup, w ∼N(0, 1/S)."
S TR,0.8606271777003485,Under review as a conference paper at ICLR 2022
S TR,0.8617886178861789,"101
102
103"
S TR,0.8629500580720093,"Feature size (P, Solid) , Dataset size (D, Dashed) 10
1 100 101 102 Loss"
S TR,0.8641114982578397,Fixed Low Regularization
S TR,0.8652729384436701,"101
102
103"
S TR,0.8664343786295006,"Feature size (P, Solid) , Dataset size (D, Dashed) 10
1"
S TR,0.867595818815331,"2 × 10
2"
S TR,0.8687572590011614,"3 × 10
2"
S TR,0.8699186991869918,"4 × 10
2"
S TR,0.8710801393728222,"6 × 10
2"
S TR,0.8722415795586528,Tuned Regularization 101 102 103 P / D
S TR,0.8734030197444832,"Figure S6: Duality between dataset size vs feature number in pretrained features Using pre-
trained embedding features of EfﬁcientNet-B5 (Tan & Le, 2019) for different levels of regularization,
we see that loss as function of dataset size or loss as a function of the feature dimension track each
other both for small regularization (left) and for tuned regularization (right). Note that regularization
strength with trained-feature kernels can be mapped to inverse training time (Ali et al., 2019; Lee
et al., 2020). Thus (left) corresponds to long training time and exhibits double descent behavior,
while (right) corresponds to optimal early stopping."
S TR,0.8745644599303136,"To begin, we note that for teacher-student models in the limit of many features, the overlap coefﬁcients
¯ω are equal to the teacher weights, up to a rotation ¯ωi = OiMwM. As we are choosing an isotropic
Gaussian initialization, we are insensitive to this rotation and, in particular, Ew

¯ω2
i

= 1/S. See
Figure S8 for empirical support of the average constancy of ¯ωi for the teacher-student setting and
contrast with realistic labels."
S TR,0.875725900116144,"With this simpliﬁcation, we now compute the asymptotic scaling of (S45) by approximating the sums
with integrals and expanding the resulting expressions in large D. We use the identities: Z ∞"
DX,0.8768873403019745,"1
dx
x−n(1+α)
 
κ + Dx−(1+α)m = κ−m
Γ

n −
1
1+α
"
DX,0.8780487804878049,"(1 + α)Γ

n +
α
1+α
 2F1"
DX,0.8792102206736353,"
m, n −
1
1 + α, n +
α
1 + α, −D κ "
DX,0.8803716608594657,"2F1 (a, b, c, −y) ∝y−a + By−b + . . . ,
(S46)"
DX,0.8815331010452961,"Here 2F1 is the hypergeometric function and the second line gives its asymptotic form at large y. B is
a constant which does not effect the asymptotic scaling."
DX,0.8826945412311266,Using these relations yields
DX,0.883855981416957,"κ ∝D−αK,
γ ∝D0,
and
L(D) ∝D−αK ,
(S47)"
DX,0.8850174216027874,"as promised. Here we have dropped sub-leading terms at large D. Scaling behavior for parameter
scaling L(P) follow via the dataset parameter duality."
DX,0.8861788617886179,"F.2
DUALITY BEYOND ASYMPTOTICS"
DX,0.8873403019744484,"Expressions (S37) and (S38) are related by changing projections onto ﬁnite feature set, and ﬁnite
dataset even without taking any asymptotic limits. We thus expect the dependence of test loss on
parameter count and dataset size to be related quite generally in linear feature models. See Section G
for further details."
DX,0.8885017421602788,"G
LEARNED FEATURES"
DX,0.8896631823461092,"In this section, we consider linear models with features coming from pretrained neural networks.
Such features are useful for transfer learning applications (e.g. Kornblith et al. (2019); Kolesnikov
et al. (2019)). In Figures S6 and S7, we take pretrained embedding features from an EfﬁcientNet-B5"
DX,0.8908246225319396,Under review as a conference paper at ICLR 2022
DX,0.89198606271777,"101
102
103
104"
DX,0.8931475029036005,"Dataset size (D) 10
5 10
4 10
3 10
2 10
1 100 101 102"
DX,0.8943089430894309,"Loss - Loss (
)"
DX,0.8954703832752613,"Variance-limited: Theory 
D = 1"
DX,0.8966318234610917,"P=10, 
D=1.13
P=25, 
D=1.25"
DX,0.8977932636469221,"P=35, 
D=1.28"
DX,0.8989547038327527,"101
102
103
104"
DX,0.9001161440185831,"Dataset size (D) 10
2 10
1 100 Loss"
DX,0.9012775842044135,"Resolution-limited: Theory 
D =
P"
DX,0.9024390243902439,"P=1156, 
D=0.23
P=1587, 
D=0.25"
DX,0.9036004645760743,"P=2048, 
D=0.26"
DX,0.9047619047619048,"101
102
103"
DX,0.9059233449477352,"Feature size (P) 10
1 100 Loss"
DX,0.9070847851335656,"Resolution-limited: Theory 
P =
D"
DX,0.908246225319396,"D=1156, 
P=0.25"
DX,0.9094076655052264,"D=1587, 
P=0.27
D=2048, 
P=0.28"
DX,0.9105691056910569,"101
102
103"
DX,0.9117305458768873,"Feature size (P) 10
3 10
2 10
1 100 101 102"
DX,0.9128919860627178,"Loss - Loss (
)"
DX,0.9140534262485482,"Variance-limited: Theory 
P = 1"
DX,0.9152148664343787,"D=10, 
P=1.27"
DX,0.9163763066202091,"D=25, 
P=1.35
D=35, 
P=1.41"
DX,0.9175377468060395,"Dataset size (D) 10
5 10
4 10
3 10
2 10
1"
DX,0.9186991869918699,"Loss - Loss (
)"
DX,0.9198606271777003,"Variance-limited: Theory 
D = 1"
DX,0.9210220673635308,"P=10, 
D=1.09"
DX,0.9221835075493612,"P=25, 
D=1.04
P=35, 
D=1.00"
DX,0.9233449477351916,"Dataset size (D) 10
2 Loss"
DX,0.924506387921022,"Resolution-limited: Theory 
D =
P"
DX,0.9256678281068524,"P=1156, 
D=0.29"
DX,0.926829268292683,"P=1587, 
D=0.30
P=2048, 
D=0.30"
DX,0.9279907084785134,"101
102
103"
DX,0.9291521486643438,Feature size (P)
DX,0.9303135888501742,"2 × 10
2"
DX,0.9314750290360047,"3 × 10
2"
DX,0.9326364692218351,"4 × 10
2"
DX,0.9337979094076655,"6 × 10
2 Loss"
DX,0.9349593495934959,"Resolution-limited: Theory 
P =
D"
DX,0.9361207897793263,"D=1156, 
P=0.28
D=1587, 
P=0.28
D=2048, 
P=0.29"
DX,0.9372822299651568,"101
102
103"
DX,0.9384436701509872,"Feature size (P) 10
3 10
2 10
1"
DX,0.9396051103368177,"Loss - Loss (
)"
DX,0.9407665505226481,"Variance-limited: Theory 
P = 1"
DX,0.9419279907084785,"D=10, 
P=1.13
D=25, 
P=1.07
D=35, 
P=1.05"
DX,0.943089430894309,"Figure S7: Four scaling regimes exhibited by pretrained embedding features Using pretrained
embedding features of EfﬁcientNet-B5 (Tan & Le, 2019) for ﬁxed low regularization (left) and tuned
regularization (right), we can identify four regimes of scaling using real CIFAR-10 labels."
DX,0.9442508710801394,"model (Tan & Le, 2019) using TF hub4. The EfﬁcientNet model is pretrained using the ImageNet
dataset with input image size of (456, 456). To extract features for the (32, 32) CIFAR-10 images,
we use bilinear resizing. We then train a linear classiﬁer on top of the penultimate pretrained features.
To explore the effect feature size, P, and dataset size D, we randomly subset the feature dimension
and training dataset size and average over 5 random seeds. Prediction on test points are obtained as a
kernel ridge regression problem with linear kernel. We note that the regularization ridge parameter
can be mapped to an inverse early-stopping time (Ali et al., 2019; Lee et al., 2020) of a corresponding
ridgeless model trained via gradient descent. Inference with low regularization parameter denotes
training for long time while tuned regularization parameter is equivalent to optimal early stopping."
DX,0.9454123112659698,"In Figure S7 we see evidence of all four scaling regimes for low regularization (left four) and optimal
regularization (right four). We speculate that the deviation from the predicted variance-limited
exponent αP = αD = 1 for the case of ﬁxed low regularization (late time) is possibly due to the
double descent resonance at D = P which interferes with the power law ﬁt."
DX,0.9465737514518002,"In Figure S6, we observe the duality between dataset size D (solid) and feature size P (dashed) – the
loss as a function of the number of features is identical to the loss as function of dataset size for both
the optimal loss (tuned regularization) or late time loss (low regularization)."
DX,0.9477351916376306,"In Figure S8, we also compare properties of random features (using the inﬁnite-width limit) and
learned features from trained WRN 28-10 models. We note that teacher-student models, where the
feature class matches the target function and ordinary, fully trained models on real data (Figure 1a),
have signiﬁcantly larger exponents than models with ﬁxed features and realistic targets."
DX,0.9488966318234611,"The measured ¯ωi – the coefﬁcient of the task labels under the i-th feature (S45) are approximately
constant as function of index i for all teacher-student settings. However for real targets, ¯ωi are only
constant for the well-performing Myrtle-10 and WRN trained features (last two columns)."
DX,0.9500580720092915,4https://www.tensorﬂow.org/hub
DX,0.9512195121951219,Under review as a conference paper at ICLR 2022
DX,0.9523809523809523,"101
102
103
104"
DX,0.9535423925667829,"Dataset size (D) 10
1 Loss FC"
DX,0.9547038327526133,"TS: 
D = 0.20
Real: 
D = 0.05"
DX,0.9558652729384437,"101
102
103
104"
DX,0.9570267131242741,"Dataset size (D) 10
1"
DX,0.9581881533101045,"3 × 10
2
4 × 10
2"
DX,0.959349593495935,"6 × 10
2"
DX,0.9605110336817654,"2 × 10
1"
DX,0.9616724738675958,CNN-VEC
DX,0.9628339140534262,"TS: 
D = 0.23
Real: 
D = 0.07"
DX,0.9639953542392566,"101
102
103
104"
DX,0.9651567944250871,"Dataset size (D) 10
4 10
3 10
2"
DX,0.9663182346109176,"10
1
Myrtle-10"
DX,0.967479674796748,"TS: 
D = 0.41
Real: 
D = 0.15"
DX,0.9686411149825784,"101
102
103
104"
DX,0.9698025551684089,"Dataset size (D) 10
12 10
9 10
6 10
3"
DX,0.9709639953542393,WRN pretrained
DX,0.9721254355400697,"TS: 
D = 2.52
Real: 
D = 0.25"
DX,0.9732868757259001,"100
101
102
103
104 i 10
2 10
1 100 iC"
DX,0.9744483159117305,"i
i/
0"
DX,0.975609756097561,K = 0.26
DX,0.9767711962833914,"100
101
102
103
104 i 10
2 10
1 100"
DX,0.9779326364692218,K = 0.26
DX,0.9790940766550522,"100
101
102
103
104 i 10
4 10
3 10
2 10
1 100"
DX,0.9802555168408827,K = 0.46
DX,0.9814169570267132,"100
101
102
103
104 i 10
14 10
10 10
6"
DX,0.9825783972125436,"10
2
K = 1.31"
DX,0.983739837398374,"100
101
102
103 i 10
8 10
5 10
2 101"
DX,0.9849012775842044,Normalized Value
DX,0.9860627177700348,"2
TS  0.03"
DX,0.9872241579558653,"2
Real  1.41 i"
DX,0.9883855981416957,"100
101
102
103 i 10
8 10
5 10
2 101"
DX,0.9895470383275261,"2
TS  0.03"
DX,0.9907084785133565,"2
Real  1.37 i"
DX,0.991869918699187,"100
101
102
103 i 10
8 10
5 10
2 101"
DX,0.9930313588850174,"2
TS  -0.07"
DX,0.9941927990708479,"2
Real  -0.45 i"
DX,0.9953542392566783,"100
101
102
103 i 10
10 10
7 10
4 10
1 102"
DX,0.9965156794425087,"2
TS  -0.07"
DX,0.9976771196283392,"2
Real  -0.40 i"
DX,0.9988385598141696,"Figure S8: Loss on the teacher targets scale better than real targets for both untrained and
trained features The ﬁrst three columns are inﬁnite width kernels while the last column is a kernel
built out of features from the penultimate layer of pretrained WRN 28-10 models on CIFAR-10. The
ﬁrst row is the loss as a function of dataset size D for teacher-student targets vs real targets. The
observed dataset scaling exponent is denoted in the legend. The second row is the normalized partial
sum of kernel eigenvalues. The partial sum’s scaling exponent is measured to capture the effect of the
ﬁnite dataset size when empirical αK is close to zero. The third row shows ¯ωi for teacher-student
and real target compared against the kernel eigenvalue decay. We see the teacher-student ¯ωi are
approximately constant."
