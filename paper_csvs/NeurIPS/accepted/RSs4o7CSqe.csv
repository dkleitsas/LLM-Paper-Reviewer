Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0029940119760479044,"Image fusion aims to integrate complementary information from multiple input
images acquired through various sources to synthesize a new fused image. Existing
methods usually employ distinct constraint designs tailored to specific scenes,
forming fixed fusion paradigms. However, this data-driven fusion approach is
challenging to deploy in varying scenarios, especially in rapidly changing environ-
ments. To address this issue, we propose a conditional controllable fusion (CCF)
framework for general image fusion tasks without specific training. Due to the
dynamic differences of different samples, our CCF employs specific fusion con-
straints for each individual in practice. Given the powerful generative capabilities
of the denoising diffusion model, we first inject the specific constraints into the
pre-trained DDPM as adaptive fusion conditions. The appropriate conditions are
dynamically selected to ensure the fusion process remains responsive to the specific
requirements in each reverse diffusion stage. Thus, CCF enables conditionally
calibrating the fused images step by step. Extensive experiments validate our
effectiveness in general fusion tasks across diverse scenarios against the competing
methods without additional training. The code is publicly available.‚Ä†"
INTRODUCTION,0.005988023952095809,"1
Introduction"
INTRODUCTION,0.008982035928143712,"Image fusion aims at integrating complementary information from multi-source images, fusing a
new composite image containing richer details [1]. It has been applied in various scenarios that
single image contains incomplete information, such as multi-modal fusion (MMF) [2, 3], multi-
exposure fusion (MEF) [4, 5], multi-focus fusion (MFF) [6], and remote sensing fusion [2]. The
fused image inherits the strengths of both modalities, resulting in a composite with enhanced visual
effects [7]. These fusion tasks have diverse downstream applications in computer vision, including
object detection [8‚Äì10], semantic segmentation [11, 12], and medical diagnosis [13] because the
comprehensive representation of images with multi-scene information contributes to the improved
performance of applications."
INTRODUCTION,0.011976047904191617,"Recently, numerous image fusion methods [14‚Äì17] have been proposed, such as traditional fusion
methods [18], CNN-based fusion methods [19, 20] and GAN-based methods [21]. While these
methods produce acceptable fused images in certain scenarios, they are also accompanied by sig-
nificant drawbacks and limitations: (i) They are often tailored for specific scenarios or individual
tasks, limiting their adaptability across diverse applications; (ii) These methods necessitate training
and consume substantial computational resources, posing limitations in terms of time and resource
requirements. Lately, denoising diffusion probabilistic models (DDPM) have emerged as an iterative"
INTRODUCTION,0.014970059880239521,"‚àóCorresponding author.
‚Ä†https://github.com/jehovahxu/CCF"
INTRODUCTION,0.017964071856287425,Selection frequency
INTRODUCTION,0.020958083832335328,Low-frequency
INTRODUCTION,0.023952095808383235,Enhanced conditions dynamic injection
INTRODUCTION,0.02694610778443114,High-frequency
INTRODUCTION,0.029940119760479042,Spatial Frequency Edge
INTRODUCTION,0.03293413173652695,Edge Intensity
INTRODUCTION,0.03592814371257485,Standard Deviation SSIM MSE
INTRODUCTION,0.038922155688622756,"3
4 ùëá
1
4 ùëá
0"
INTRODUCTION,0.041916167664670656,"1
2 ùëá
ùëá ùëá"
INTRODUCTION,0.04491017964071856,Random
INTRODUCTION,0.04790419161676647,Content
INTRODUCTION,0.05089820359281437,Detail
INTRODUCTION,0.05389221556886228,"Figure 1:
The conditions selection statistics during the sampling process of the LLVIP dataset.
The distinct process of sampling has different favor of the conditions. The crucial role that diverse
conditions play in controlling various image generation processes. Throughout the diffusion sampling,
different conditions are dynamically selected to best suit the generation requirement at each stage.
generation framework, showcasing impressive capabilities in unconditional generation. Inspiringly,
numerous researchers explored its controllable aspects. ILVR [22] proposed iterative latent variable
refinement with a reference image to control image translation. Some recent works [23, 24] employed
the diffusion model for image fusion, which fuses images in fixed fusion paradigms (fixed fusion
conditions) by using its inherent reconstruction capacity. However, these approaches are not qualified
for sample-customized fusion with dynamic conditions. At present, general image fusion with
controllable diffusion models is still a challenging problem, warranting further exploration."
INTRODUCTION,0.05688622754491018,"In this paper, we propose a diffusion-based controllable conditional image fusion (CCF) framework,
which controls the fusion process by adaptively selecting optimization conditions. We construct a
condition bank of generally used conditions, categorizing them into basic, enhanced, and task-specific
conditions. CCF dynamically assigns fusion conditions from the condition bank and continuously
injects them into the sampling process of diffusion. To enable flexible integrated conditions, we
further propose a sampling-adaptive condition selection (SCS) mechanism that tailors condition
selection at different denoising steps. The iterative refinements of the sampling are based on the
pre-trained diffusion model without additional training. It is worth noting that the estimated fused
images are conditionally controllable during the iterative denoising process. The diffusion process
seamlessly integrates these conditions during the sampling process, decreasing potential impacts. As
illustrated in Fig. 1, the generation process emphasizes different aspects at various sampling steps.
In the initial stages, the condition selection is influenced by random noise, resulting in a random
selection. During the intermediate stages, there is a shift towards content components. In the final
stage, the emphasis moves to generating and selecting texture details. These various conditional
factors contribute to different aspects of fusion results and demonstrate the necessity and effectiveness
of introducing specific conditions in different stages. To the best of our knowledge, we for the first
time propose a conditional controllable framework for image fusion. The main contributions are
summarized as follows:"
INTRODUCTION,0.059880239520958084,"‚Ä¢ We propose a pioneering conditional controllable image fusion (CCF) framework with a
condition bank, achieving controllability in various image fusion scenarios and facilitating
the capability of dynamic controllable image fusion."
INTRODUCTION,0.06287425149700598,"‚Ä¢ We propose a sampling-adaptive condition selection mechanism to subtly integrate the
condition bank into denoising steps, allowing adaptive condition selection on the fly without
additional training and ensuring the dynamic adaptability of the fusion process."
INTRODUCTION,0.0658682634730539,"‚Ä¢ Extensive experiments on various fusion tasks have confirmed our superior fusion perfor-
mance against the competing methods. Furthermore, our approach qualifies for interactive
manipulation of the fusion results, demonstrating our applicability and efficacy."
RELATED WORK,0.0688622754491018,"2
Related Work"
RELATED WORK,0.0718562874251497,"Image fusion focuses on producing a unified image that amalgamates complementary information
sourced from multiple source images [25]."
RELATED WORK,0.0748502994011976,"Specialized. Focus on specialized tasks such as VIF, several early approaches [26‚Äì28] relied on
CNNs to address challenges across various scenarios. GTF [29] defined the objective of image fusion
as preserving both intensity information in infrared images and gradient information in visible images.
Besides that, researchers started artificially incorporating prior knowledge to aid in the fusion process.
CDDFuse [30] introduced the concept of high and low-frequency decomposition with dual-branch as
prior information. Diverging from approaches tailored to single scenarios, numerous methods are
now exploring the development of a unified fusion framework.DDFM [24] represents the pioneering
training-free method that employs a diffusion model for multi-modal image fusion."
RELATED WORK,0.07784431137724551,"Generalized. Not limited to specialized applications, researchers aim to extend its use to generalized
tasks. U2Fusion [31] introduced a unified framework capable of adaptively preserving information
and facilitating joint training across various task scenarios. Additionally, SwinFusion [32] proposed
a cross-domain distance learning method that has been extended to form a unified framework
encompassing diverse task scenarios. Defusion [33] employs self-supervised learning techniques
to decompose images and subsequently adaptively fuse them. TC-MoA [34] proposed a novel
task-customized mixture of adapters for generating image fusion with a unified model, enabling
adaptive prompting for various fusion tasks."
RELATED WORK,0.08083832335329341,"Nevertheless, these methods cannot control image fusion for adaptation to different scenarios. There-
fore, we propose a method that enables image control, manipulating the fused image through existing
conditions on our condition bank."
PRELIMINARY,0.08383233532934131,"3
Preliminary"
PRELIMINARY,0.08682634730538923,"Denosing diffusion probabilistic models (DDPM) is a class of likelihood-based models that shows
remarkable performance [35] with a stable training objective in unconditional image generation. The
diffusion process entails incrementally introducing Gaussian noise to the data until it reaches a state
of random noise. For a clean sample x0 ‚àºq(x0) each step within the diffusion process constitutes a
Markov Chain, encompassing a total of T steps, relying on the data derived from the preceding step.
Gaussian noise is added as follows:"
PRELIMINARY,0.08982035928143713,"q(xt|xt‚àí1) = N(xt;
p"
PRELIMINARY,0.09281437125748503,"1 ‚àíŒ≤txt‚àí1, Œ≤tI),
(1)"
PRELIMINARY,0.09580838323353294,"where {Œ≤t}T
t=1 is the variance schedule of each diffusion step which is fixed and predefined. The
generative process learns the inverse of the DDPM forward (diffusion) process, sampling from a
distribution by reversing a gradual denoising process. We can directly sample xt at any t step based
on the original data xt ‚àºq(xt|x0) and via the reparameterization, it can be redefined:"
PRELIMINARY,0.09880239520958084,"xt = ‚àö¬ØŒ±tx0 +
‚àö"
PRELIMINARY,0.10179640718562874,"1 ‚àí¬ØŒ±tœµ,
(2)"
PRELIMINARY,0.10479041916167664,"where defined Œ±t := 1 ‚àíŒ≤t and ¬ØŒ± := Qt
i=1 Œ±i. The diffusion process introduces noise to the data,
whereas the inverse process represents a denoising procedure called sampling. In particular, stats
with a noise xT ‚àºN(0, I), the diffusion model learns to produce slightly less-noisy sample xT ‚àí1,
the process can be formulate by:"
PRELIMINARY,0.10778443113772455,"pŒ∏(xt‚àí1|xt) = N(xt‚àí1; ¬µŒ∏(xt, t), Œ£Œ∏(xt, t)).
(3)"
PRELIMINARY,0.11077844311377245,"Utilizing the properties of Markov chains, decomposing ¬µŒ∏ and Œ£Œ∏, the process of generation is
expressed as:"
PRELIMINARY,0.11377245508982035,"xt‚àí1 =
1
‚àöŒ±t
(xt ‚àí1 ‚àíŒ±t
‚àö1 ‚àí¬ØŒ±t
œµŒ∏(xt, t)) + œÉ2
Œ∏(t)I,
(4)"
PRELIMINARY,0.11676646706586827,"where, œÉ2
Œ∏(t) = Œ£Œ∏(t) = (1‚àíŒ±t)(1‚àí¬ØŒ±t‚àí1)"
PRELIMINARY,0.11976047904191617,"1‚àí¬ØŒ±t
, œµŒ∏ signifies the output of a neural network, commonly a
U-Net. This neural network predicts the noise œµŒ∏ at each step, which is utilized for the denoising
procedure. It can be observed that variance is a fixed quantity, because of diffusion process parameters
being constant, whereas the mean is a function dependent on x0 and xt. However, the stochastic
process poses challenges in controlling the generative process."
PRELIMINARY,0.12275449101796407,"Condition Bank
Input"
PRELIMINARY,0.12574850299401197,Sampling
PRELIMINARY,0.12874251497005987,"Basic Conditions
Task-specific"
PRELIMINARY,0.1317365269461078,"ùëá
ùë°
∆∏ùë°
0
‚Ä¶
‚Ä¶"
PRELIMINARY,0.1347305389221557,"ùëìùë°
·àòùëìùë°"
PRELIMINARY,0.1377245508982036,"Conditions 
Embedding"
PRELIMINARY,0.1407185628742515,Top-K Routing
PRELIMINARY,0.1437125748502994,Enhanced Conditions
PRELIMINARY,0.1467065868263473,Detection
PRELIMINARY,0.1497005988023952,Segmentation Depth
PRELIMINARY,0.15269461077844312,Classification ‚Ä¶
PRELIMINARY,0.15568862275449102,"Spatial 
Frequency
SSIM"
PRELIMINARY,0.15868263473053892,"Cross 
Entropy
Average 
Gradient"
PRELIMINARY,0.16167664670658682,Entropy
PRELIMINARY,0.16467065868263472,"Standard 
Deviation"
PRELIMINARY,0.16766467065868262,Feature Mutual
PRELIMINARY,0.17065868263473055,Information
PRELIMINARY,0.17365269461077845,"Mutual 
Information
‚Ä¶
‚Ä¶ Gate MSE"
PRELIMINARY,0.17664670658682635,Low-frequency Edge
PRELIMINARY,0.17964071856287425,High-frequency
PRELIMINARY,0.18263473053892215,Output
PRELIMINARY,0.18562874251497005,"Figure 2: Illustrates the pipeline of the proposed CCF. The framework comprises two components: a
sampling process utilizing a pre-trained DDPM and a condition bank with SCS."
METHOD,0.18862275449101795,"4
Method"
METHOD,0.19161676646706588,"Leveraging the reconstruction capability of unconditional DDPM, we introduced a new controllable
conditional image fusion (CCF) framework. Our approach accomplishes dynamically controllable
image fusion via progressive condition embedding. In particular, we introduced a condition bank
that regulates the incorporation of fusion information using conditions. It allows for combining the
dynamic selection of multiple conditions to achieve sampling-adaptive fusion effects. As shown in
Fig. 2, we illustrate our CCF framework in detail with visible-infrared image fusion (VIF). The goal is
to generate a fused image f ‚ààRH√óW √óN from visible v ‚ààRH√óW √óN and infrared i ‚ààRH√óW √óN
images, where H, W and N denote height, width, and channel numbers, respectively."
CONTROLLABLE CONDITIONS,0.19461077844311378,"4.1
Controllable Conditions"
CONTROLLABLE CONDITIONS,0.19760479041916168,"Firstly, we provide the notation for the model formulation. For each sampling instance, a pre-trained
DDPM represents unconditional transition pŒ∏(xt‚àí1|xt). Our method facilitates the inclusion of
conditional c during the sampling step of unconditional transformation, without no additional training.
For this purpose, we sample images from the conditional distribution p(x0|c) given condition c:"
CONTROLLABLE CONDITIONS,0.20059880239520958,"pŒ∏(x0|c) =
Z
dx(1:T )pŒ∏(x(0:T )|c),"
CONTROLLABLE CONDITIONS,0.20359281437125748,"pŒ∏(x(0:T )|c) = p(xT ) T
Y"
CONTROLLABLE CONDITIONS,0.20658682634730538,"t=1
pŒ∏(xt‚àí1|xt, c).
(5)"
CONTROLLABLE CONDITIONS,0.20958083832335328,"Each transition pŒ∏(xt‚àí1|xt, c) of the generative process depends on the condition c. From the property
of the forward process that latent variable xt can be sampled from x0 in closed-form, denoised data
x0 can be approximated with model prediction œµŒ∏(xt, t):"
CONTROLLABLE CONDITIONS,0.2125748502994012,"x0|t ‚âàfŒ∏(xt, t) = (xt ‚àí‚àö1 ‚àí¬ØŒ±tœµŒ∏(xt, t))
‚àö¬ØŒ±t
.
(6)"
CONTROLLABLE CONDITIONS,0.2155688622754491,"To compute p(xt|c), we can derive it from the Stochastic Differential Equation (SDE) [36]. For
brevity, x0|t is abbreviated as x0, and the expression is given by:"
CONTROLLABLE CONDITIONS,0.218562874251497,‚àáxt log p(xt) = ‚àíxt ‚àí‚àö¬ØŒ±tx0
CONTROLLABLE CONDITIONS,0.2215568862275449,"1 ‚àí¬ØŒ±t
.
(7)"
CONTROLLABLE CONDITIONS,0.2245508982035928,"Classifier Guidance [37] can be intuitively elucidated via the score function, which logarithmically
decomposes the conditional generation probability using Bayes‚Äô theorem:"
CONTROLLABLE CONDITIONS,0.2275449101796407,"‚àálog p(xt|c) = ‚àálog p(xt) + ‚àálog p(c|xt).
(8)"
CONTROLLABLE CONDITIONS,0.23053892215568864,"Figure 3: Qualitative comparisons of our CCF and the competing methods on VIF fusion task of
LLVIP dataset.
Diffusion posterior sampling can yield a more favorable generative trajectory, especially in noisy
settings, [38] estimation log p(c|xt) as follows:"
CONTROLLABLE CONDITIONS,0.23353293413173654,"‚àálog p(c|xt) ‚âà‚àálog p(c|ÀÜx0).
(9)"
CONTROLLABLE CONDITIONS,0.23652694610778444,"Further, as demonstrated in [39], we can express the score function as:"
CONTROLLABLE CONDITIONS,0.23952095808383234,"‚àálog p(c|xt) ‚âà‚àálog p(c|ÀÜx0) = fŒ∏(xt, t) ‚àíŒª‚àá||C ‚àíA(x0)||2.
(10)"
CONTROLLABLE CONDITIONS,0.24251497005988024,"Here, A(¬∑) can be linear or nonlinear operation. We represent ||C ‚àíA(x0)|| as Œ¥C. Now, the objective
is to obtain ÀÜx0 and incorporate the condition into it. We can minimize Œ¥C to regulate the sampling
within the diffusion process. In the following section, we will provide a detailed explanation of how
to build a condition bank and how to select conditions."
CONDITION BANK,0.24550898203592814,"4.2
Condition Bank"
CONDITION BANK,0.24850299401197604,"We empirically construct a condition bank and divide the image constraints into three categories:
basic fusion conditions, enhanced fusion conditions, and task-specific fusion conditions. Basic fusion
conditions are utilized throughout the entire sampling process, while enhanced fusion conditions
are dynamically selected. Task-specific fusion conditions are manually optional, tailored to specific
tasks, and may possess unique attributes that can be customized for various task scenarios. All
conditions can be part of the enhanced condition set, enabling dynamic selection. The condition bank
presented in this paper includes some common conditions, but additional conditions can be explored
and utilized in other scenarios."
CONDITION BANK,0.25149700598802394,"In the above formulation, each conditional Markov transition with the given condition c is shown
in Eq. 5. In particular, we constructed a condition bank that allows us to select required conditions
C = {c1, c2, ..., cn}, subsequently integrating them into the unconditional DDPM for executing
conditional image fusion. Let C represent a condition bank comprising a series of conditions. The
function Œ¥C represents the difference between the source images with the given condition. In every
sampling step t, the difference function Œ¥ci can be minimized using gradient descent. These conditions
help regulate the image information within each modality involved in the fusion process."
CONDITION BANK,0.25449101796407186,"Basic Conditions. As shown in Fig. 2, basic conditions are essential to select for a basic generation.
The basic conditions aim to synthesize a foundational fused image, offering an initial, coarse
representation. The fused image serves as a primary fusion output, capturing essential features from
the source images, though it may suffer from detail loss or texture blur. Notably, different scenarios
may require adjustments to the basic condition, as the specific requirements of each fusion task, such
as clarity, contrast, and other priorities, can influence its selection. Tailoring the basic condition to
align with the unique demands of each task thus ensures an effective fusion process."
CONDITION BANK,0.25748502994011974,"Enhanced Conditions. Besides basic conditions, we added enhancement conditions for refining the
image generation process. The condition bank contains a variety of enhanced conditions, inspired
by various Image Quality Assessments (IQA) such as SSIM, and standard deviation(SD). These
conditions can be integrated into the CCF generation process to improve the quality of the generated
images. The enhanced conditions can be selected with SCS algorithm, allowing different steps of the
diffusion sampling process to be optimized with different conditions. This targeted approach ensures"
CONDITION BANK,0.26047904191616766,"that each phase of the image generation is adapted to the specific requirements of that stage, resulting
in higher quality fused images."
CONDITION BANK,0.2634730538922156,"Task-specific Conditions. The purpose of image fusion is to facilitate various downstream tasks. To
meet the specific requirements of these tasks, we offer task-specific conditions. These conditions can
be manually added to ensure that the fusion process is tailored to suit the downstream applications
better. For example, a detection condition can be introduced by utilizing the feature extracted by
a detection network F = D(x). The detection condition is formulated as ||F(x), F(M)||2, where
X ‚àà{x0}m
i and M is the set of m modalities. Other task-specific conditions can be similarly tailored
to optimize the fusion process for different tasks. By integrating these task-specific conditions, the
image fusion process can be precisely aligned with the demands of various applications, enhancing
the effectiveness and utility of the generated images."
SAMPLING-ADAPTIVE CONDITION SELECTION,0.26646706586826346,"4.3
Sampling-adaptive Condition Selection"
SAMPLING-ADAPTIVE CONDITION SELECTION,0.2694610778443114,"During the diffusion sampling process, it is crucial to focus on generating distinct aspects of the
image. To address this, we designed an algorithm that dynamically selects the appropriate condition
from the condition bank to fit each sampling stage. This selection process can be denoted as
Copt = Topk{Gate(C)}. The Copt is the selected condition, and Gate is the gate of selection. We
hypothesize that rapidly changing conditions during the sampling process should be prioritized as
they indicate greater significance at that generation stage. Inspired by multi-task learning [40], the
gate of conditions can be calculated using the following formula:
Gate = [œâ1, ..œâc], œâi(t) = œâi(t ‚àí1) ‚àí‚ñΩœâi.
(11)
The œâi(t ‚àí1) represents the condition gradient from the previous step, and the ‚ñΩœâi is calculated as:"
SAMPLING-ADAPTIVE CONDITION SELECTION,0.27245508982035926,"‚ñΩœâi =
X"
SAMPLING-ADAPTIVE CONDITION SELECTION,0.2754491017964072,"i
|Gi
W (t) ‚àíE[Gi
W (t)] √ó Œ±i(t)|1,
(12)"
SAMPLING-ADAPTIVE CONDITION SELECTION,0.27844311377245506,"where, GW = ||œâi(t)Li(t)||2, Li(t) represents the condition gradient at step t, and the value can be
calculated using a gradient descent algorithm in minimize Œ¥ci. The Œ±i(t) is defined as:"
SAMPLING-ADAPTIVE CONDITION SELECTION,0.281437125748503,"Œ±i(t) = [
eLi
E[eLi]
]Œ∏,
(13)"
SAMPLING-ADAPTIVE CONDITION SELECTION,0.2844311377245509,"where, eLi =
Li(t)
Li(t‚àí1), Œ∏ is a hyper-parameter. By incorporating this SCS algorithm, we can efficiently
choose the most relevant condition for each step in the diffusion process, thereby enhancing the
quality of the conditional image fusion."
EXPERIMENTS,0.2874251497005988,"5
Experiments"
EXPERIMENTS,0.2904191616766467,"Datasets. We conduct experiments in three image fusion scenarios: multi-modal, multi-focus, and
multi-exposure image fusion. For multi-modal image fusion task, we conducted experiments on the
LLVIP [41] dataset and referred to the test set outlined in Zhu et al. [34]. For MEF and MFF, our
testing procedure followed the test setting in MFFW dataset [42] and MEFB dataset [43], respectively.
Additionally, we test our method on the TNO dataset and Harvard medical dataset to assess our
method‚Äôs performance within the multi-modal fusion domain, detailed in App. B and H."
EXPERIMENTS,0.2934131736526946,"Implementation Details. Our method utilizes a pre-trained diffusion model as our foundational
model [44]. This model was directly applied without any subsequent fine-tuning for specific task
requirements during our experiments. The experiments are conducted on Huawei Atlas 800 Training
Server with CANN and NVIDIA RTX 3090 GPU. Experimental settings are shown in App. A."
EXPERIMENTS,0.2964071856287425,"Evaluation Metrics. We evaluated the fusion results in both quantitative and qualitative. Qualitative
evaluation primarily hinges on subjective visual assessments conducted by individuals. We expect
that the fused image will exhibit rich texture details and abundant color saturability. Objective
evaluation primarily focuses on measuring the quality assessments of individual fused images and
their deviations from the source images. For different task scenarios, the different evaluation metrics
used, specifically, we employ six metrics including Structural Similarity (SSIM), Mean squared error
(MSE), correlation coefficient (CC), peak signal-to-noise ratio (PSNR), modified fusion artifacts
measure (Nabf). In the MFF and MEF tasks, considering the different task scenarios, we employ
standard deviation (SD), average gradient (AG), spatial frequency (SF), and sum of the correlations
of differences (SCD) for evaluation metrics."
EXPERIMENTS,0.2994011976047904,"Source 1
Source 1
U2Fusion
DeFusion"
EXPERIMENTS,0.3023952095808383,"DDFM
Text-IF
TC-MoA
Ours(CCF)"
EXPERIMENTS,0.30538922155688625,"Overexposure
Underexposure
U2Fusion
DeFusion"
EXPERIMENTS,0.3083832335329341,"DDFM
Text-IF
TC-MoA
Ours(CCF)"
EXPERIMENTS,0.31137724550898205,Figure 4: Qualitative comparisons of various methods in MFF task from MFFW dataset.
EVALUATION ON MULTI-MODAL IMAGE FUSION,0.3143712574850299,"5.1
Evaluation on Multi-Modal Image Fusion"
EVALUATION ON MULTI-MODAL IMAGE FUSION,0.31736526946107785,"For multi-modal image fusion, we compare our method with the state-of-the-art methods: Swin-
Fusion [32], DIVFusion [45], MUFusion [46], CDDFuse [30], DDFM [24], Text-IF [47], and
TC-MoA [34] and on the LLVIP dataset. More datasets and comparison methods are shown in
App. B. Note that our method, like DDFM, does not require additional tuning."
EVALUATION ON MULTI-MODAL IMAGE FUSION,0.3203592814371258,"Table 1: Comparison with SOTAs in the LLVIP dataset. The
red/blue/green indicates the best, runner-up and third best."
EVALUATION ON MULTI-MODAL IMAGE FUSION,0.32335329341317365,"LLVIP Dataset
SSIM‚Üë
MSE‚Üì
CC‚Üë
PSNR‚Üë
Nabf‚Üì"
EVALUATION ON MULTI-MODAL IMAGE FUSION,0.3263473053892216,"SwinFusion [32]
0.81
2845
0.668
32.33
0.023
DIVFusion [45]
0.82
6450
0.655
21.60
0.044
MUFusion [46]
1.10
2069
0.648
31.64
0.030
CDDFuse [30]
1.18
2545
0.670
32.13
0.016
DDFM [24]
1.18
2056
0.668
36.10
0.004
Text-IF [47]
1.20
2135
0.669
31.97
0.023
TC-MoA [34]
1.20
2790
0.666
33.00
0.017
CCF (ours)
1.22
1694
0.705
32.71
0.005"
EVALUATION ON MULTI-MODAL IMAGE FUSION,0.32934131736526945,"Quantitative Comparisons. We em-
ploy five quantitative metrics to eval-
uate our model, as shown in Ta-
ble 1. Our method demonstrated ex-
ceptional performance across various
evaluation metrics.
On the LLVIP
dataset, our method achieved the best
results in SSIM, MSE, and CC indica-
tors. Specifically, our method outper-
formed others in SSIM and CC, with
improvements of 0.02 and 0.035 over
the second-best results, respectively.
Additionally, lower MSE values indi-
cate better performance, with our method showing a reduction of 362 compared to the second-best
methods in these metrics. This indicates that our method retains more information from the source
images. The results show suboptimal performance in Nabf but are close to the best values, indicating
the fused image with less noise. Notably, our method does not necessitate turning and holds its
ground against methods requiring training. Compared to existing LLM tuning methods, our model
performs slightly worse in terms of PSNR. This demonstrates the excellent performance of our model,
achieving high performance across almost all indicators in a tuning-free model."
EVALUATION ON MULTI-MODAL IMAGE FUSION,0.3323353293413174,"Qualitative Comparisons. Furthermore, the incorporation of the basic condition and enhanced
conditions enables effective preservation of the background and texture. This comparison underscores
our model‚Äôs efficacy in image fusion, resulting in outstanding visual outcomes. As shown in Fig. 3,
our method showcases superior visual quality compared to other approaches. Specifically, our method
excels in preserving intricate texture details well lid in low light (Fig. 3 red box). Although TC-MoA
and MUFusion approach our method in retaining details, they exhibit visible artifacts, blur, and low
contrast‚Äîcharacteristics absent in CCF (Fig. 3, green box). CCF exhibits the highest contrast, the
clearest details, and the most information content, further highlighting its superiority in preserving
texture details. Its excellent detail retention and clear background generation further demonstrate the
effectiveness of our proposed method."
EVALUATION ON MULTI-FOCUS FUSION,0.33532934131736525,"5.2
Evaluation on Multi-Focus Fusion"
EVALUATION ON MULTI-FOCUS FUSION,0.3383233532934132,"For multi-focus image fusion, we compare our CCF with five general image fusion methods:
U2Fusion [31], DeFusion [33], DDFM [24], Text-IF [47], and TC-MoA [34]."
EVALUATION ON MULTI-FOCUS FUSION,0.3413173652694611,"Quantitative Comparisons. We employ four quantitative metrics to evaluate our model, as shown
in Table 2 (left). Our method significantly outperforms the comparison methods, achieving SOTA
across all metrics. Specifically, the SD is 11.19 higher than the suboptimal value, indicating higher"
EVALUATION ON MULTI-FOCUS FUSION,0.344311377245509,"Table 2: Comparison with SOTAs. The red/blue/green indicates the best, runner-up and third best."
EVALUATION ON MULTI-FOCUS FUSION,0.3473053892215569,"MFFW Dataset
MEFB Dataset
SD ‚Üë
AG ‚Üë
SF ‚Üë
SCD ‚Üë
SD ‚Üë
AG ‚Üë
SF ‚Üë
SCD ‚Üë"
EVALUATION ON MULTI-FOCUS FUSION,0.3502994011976048,"U2Fusion [31]
67.83
8.08
22.19
2.67
64.88
5.56
18.74
2.05
DeFusion [33]
54.75
4.76
12.72
0.50
52.75
4.32
14.12
‚àí0.97
DDFM [24]
56.34
4.47
12.21
0.90
67.30
3.82
13.40
2.12
Text-IF [47]
66.27
7.72
21.58
2.27
62.51
4.78
17.26
1.46
TC-MoA [34]
57.55
6.95
20.67
1.03
50.27
4.82
15.64
0.42
CCF (ours)
79.02
8.18
24.30
2.78
71.88
5.70
20.28
3.00"
EVALUATION ON MULTI-FOCUS FUSION,0.3532934131736527,"contrast and clearer images. The SCD is 0.11 higher than the suboptimal value, suggesting a lower
error between source images and fused images. The AG and SF also rank first demonstrating the
retention of more texture details. These results showcase that our method effectively preserves details
from the source images and produces high-quality fused images."
EVALUATION ON MULTI-FOCUS FUSION,0.3562874251497006,"Qualitative Comparisons. As illustrated in Fig. 4, our proposed method demonstrates outstanding
visual performance, particularly in preserving intricate details. We have carefully selected specific
conditions that allow our approach to effectively handle the blurring caused by multi-focus scenes
while retaining the original image‚Äôs lighting and color information. In comparison, other DDPM-
based methods such as DDFM are unable to achieve the same level of effectiveness as our approach.
In Fig. 4 (red), our method excels in preserving the details of the watch hand. The closest result to
ours is U2Fusion, but it loses texture and color fidelity, appearing blurry, in Fig. 4 (green). In short,
our method performs well in maintaining both color and authentic details."
EVALUATION ON MULTI-EXPOSURE FUSION,0.3592814371257485,"5.3
Evaluation on Multi-Exposure Fusion"
EVALUATION ON MULTI-EXPOSURE FUSION,0.36227544910179643,"For multi-exposure image fusion, we compare our model with five general image fusion methods,
i.e., U2Fusion [31], DeFusion [33], DDFM [24], Text-IF [47], and TC-MoA [34]."
EVALUATION ON MULTI-EXPOSURE FUSION,0.3652694610778443,"Quantitative Comparisons. As demonstrated in Table 2 (right), our method achieved SOTA on
the MEF index, analogous to the results observed for the MFF task. Notably, the metrics SD, AG,
and SF signify the highest image quality, while SCD exhibits the highest correlation. Each of these
metrics attained state-of-the-art performance levels, underscoring the efficacy of our approach. This
consistent performance across multiple metrics illustrates the robustness and versatility of our method
in enhancing image quality and fidelity."
EVALUATION ON MULTI-EXPOSURE FUSION,0.36826347305389223,"Qualitative Comparisons. Fig. 4 demonstrates the excellent visual performance of our method.
Our approach effectively addresses the issue of overexposure while preserving crucial details. In
contrast, DDFM, which relies on finding the middle value of two images, struggles to maintain texture
details. Similarly, Text-IF tends to result in higher average brightness, which can lead to content
loss in overexposed scenes. Defusion and TC-MoA exhibit similar visual performance, with more
blurred edges compared to our method. In comparison, our method strikes a balance between these
challenges, resulting in superior visual fidelity and saturation compared to other existing methods."
TASK-SPECIFIC CONDITIONS,0.3712574850299401,"5.4
Task-specific Conditions"
TASK-SPECIFIC CONDITIONS,0.37425149700598803,"Figure 5: The visualization of the w/o and with
task-specific conditions."
TASK-SPECIFIC CONDITIONS,0.3772455089820359,"The task-specific conditions can be manually se-
lected. In this section, we use the detection con-
dition as an example. The detection model em-
ployed is YOLOv5 [48], trained on the LLVIP
dataset. We randomly select 287 images from
the LLVIP test dataset to validate our method
with the detection condition. Before adding the
detection condition, the fused image achieved
mAP.5 = 0.737, mAP.5 : .95 = 0.509, and a
recall of 0.737. After incorporating the detec-
tion condition, the mAP.5 increased by 0.049 to
0.907, mAP.5 : .95 increased by 0.054 to 0.563,
and recall significantly improved to 0.832. Fig. 5
visualizes several cases detected using YOLOv5."
TASK-SPECIFIC CONDITIONS,0.38023952095808383,"Figure 6: Ablation study of condition bank. (a) is infrared image, (b) is visible image, (c) is basic
condition, (d) is CCF w/o SCS, (e) is CCF."
TASK-SPECIFIC CONDITIONS,0.38323353293413176,"The fused images with the detection condition exhibit higher confidence (columns 1 and 2) and recall
(columns 1, 2, and 3). This demonstrates that task-specific conditions can enhance the performance
of fused images in downstream tasks. By integrating task-specific conditions, the fused image can be
precisely aligned with the demands of various applications, enhancing the overall effectiveness and
utility of the generated images. More detail shows in App. F."
ABLATION STUDY,0.38622754491017963,"5.5
Ablation Study"
ABLATION STUDY,0.38922155688622756,"Numerous ablation experiments were conducted to assess the effectiveness of different components
of our model. The aforementioned four metrics were utilized to evaluate fusion performance across
different experimental configurations. The quantitative results are presented in App. G."
ABLATION STUDY,0.39221556886227543,"To validate the effectiveness of the condition bank, we systematically add basic and enhanced
conditions individually and then verify the effectiveness of SCS. Fig. 6 illustrates a gradual variation
in performance metrics with the progressive addition of conditions."
ABLATION STUDY,0.39520958083832336,"In Fig. 6(a), using only the basic condition results in image fusion. When all enhanced conditions are
injected together without SCS, the metrics all reduced (Fig. 6(b)), likely due to the conflict between
different conditions, as evidenced by the messy lines and random noise in Fig. 6(d). This demonstrates
that injecting conditions unevenly without DCS leads to suboptimal results. After introducing DCS,
both the metrics and visual results (Fig. 6(e)) are well-balanced, with conditions being injected as
needed during the generation process. This indicates the effectiveness of the condition bank and DCS
in dynamically and appropriately selecting the conditions."
DISCUSSION ON CONTROLLABLE FUSION,0.39820359281437123,"5.6
Discussion on Controllable Fusion"
DISCUSSION ON CONTROLLABLE FUSION,0.40119760479041916,"Our method enables the dynamic selection of conditions, allowing for the adaptive customization
of conditions for each sample. As illustrated in Fig. 1, the statistics show the condition selection at
various diffusion sampling stages. Initially, the process is stochastic due to random noise (around
0 to 1"
DISCUSSION ON CONTROLLABLE FUSION,0.4041916167664671,"4T steps). As denoising progresses, the content of the image starts to be dominant in fusion,
and conditions like MSE, SSIM, and SD are most frequently chosen (around 1"
DISCUSSION ON CONTROLLABLE FUSION,0.40718562874251496,4T to 3
DISCUSSION ON CONTROLLABLE FUSION,0.4101796407185629,"4T steps).
As the process continues, and the fusion model tends to generate more texture details, conditions
like EI and edge become more dominant (around 3"
DISCUSSION ON CONTROLLABLE FUSION,0.41317365269461076,"4T to T steps). SSIM also remains selected to
constrain the structure, whereas the frequency of SD selection decreases. This demonstrates the
crucial role of dynamically selecting conditions during the diffusion sampling process. Our CCF
adaptively decomposes diverse conflict fusion conditions into different denoising steps, which is
significantly compatible with the reconstruction preferences of different steps in the denoising model.
This dynamic fusion paradigm ensures appropriate condition selection at each stage of the sampling."
CONCLUSION,0.4161676646706587,"6
Conclusion"
CONCLUSION,0.41916167664670656,"In this paper, we introduce a learning-free approach for conditional controllable image fusion (CCF),
utilizing a condition bank to regulate joint information with a pre-trained DDPM. We capitalize
on the remarkable reconstruction abilities of DDPM and integrate them into the sampling steps.
Sample-adaptive condition selection facilitates fusion in dynamic scenarios. Varied fusion images
can personalize their conditions to emphasize different aspects. Empirical findings demonstrate that
CCF surpasses the competing methods in achieving superior performance for general image fusion
tasks. In the future, we will further explore automatic manners to distinguish basic and enhanced
conditions to reduce empirical intervention, thereby enabling more robust and reliable image fusion."
CONCLUSION,0.4221556886227545,Acknowledgements.
CONCLUSION,0.4251497005988024,"This work was sponsored by National Science and Technology Major Project (No. 2022ZD0116500),
National Natural Science Foundation of China (No.s 62476198, 62436002, 62222608, U23B2049,
62106171 and 61925602), Tianjin Natural Science Funds for Distinguished Young Scholar (No.
23JCJQJC00270), the Zhejiang Provincial Natural Science Foundation of China (No. LD24F020004),
and CCF-Baidu Open Fund. This work was also sponsored by CAAI-CANN Open Fund, developed
on OpenI Community."
REFERENCES,0.4281437125748503,References
REFERENCES,0.4311377245508982,"[1] Shutao Li, Xudong Kang, Leyuan Fang, Jianwen Hu, and Haitao Yin. Pixel-level image fusion:
A survey of the state of the art. information Fusion, 33:100‚Äì112, 2017."
REFERENCES,0.4341317365269461,"[2] Jiaxin Li, Danfeng Hong, Lianru Gao, Jing Yao, Ke Zheng, Bing Zhang, and Jocelyn Chanussot.
Deep learning in multimodal remote sensing data fusion: A comprehensive review. International
Journal of Applied Earth Observation and Geoinformation, 112:102926, 2022."
REFERENCES,0.437125748502994,"[3] Xingchen Zhang, Ping Ye, and Gang Xiao. Vifb: A visible and infrared image fusion benchmark.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
Workshops, pages 104‚Äì105, 2020."
REFERENCES,0.44011976047904194,"[4] Fang Xu, Jinghong Liu, Yueming Song, Hui Sun, and Xuan Wang. Multi-exposure image fusion
techniques: A comprehensive review. Remote Sensing, 14(3):771, 2022."
REFERENCES,0.4431137724550898,"[5] Jinyuan Liu, Jingjie Shang, Risheng Liu, and Xin Fan. Attention-guided global-local adversarial
learning for detail-preserving multi-exposure image fusion. IEEE Transactions on Circuits and
Systems for Video Technology, 32(8):5026‚Äì5040, 2022."
REFERENCES,0.44610778443113774,"[6] Yu Liu, Lei Wang, Juan Cheng, Chang Li, and Xun Chen. Multi-focus image fusion: A survey
of the state of the art. Information Fusion, 64:71‚Äì91, 2020."
REFERENCES,0.4491017964071856,"[7] Bing Cao, Yiming Sun, Pengfei Zhu, and Qinghua Hu. Multi-modal gated mixture of local-
to-global experts for dynamic image fusion. In Proceedings of the IEEE/CVF International
Conference on Computer Vision (ICCV), pages 23555‚Äì23564, October 2023."
REFERENCES,0.45209580838323354,"[8] Yiming Sun, Bing Cao, Pengfei Zhu, and Qinghua Hu. Detfusion: A detection-driven infrared
and visible image fusion network. In Proceedings of the 30th ACM International Conference on
Multimedia, pages 4003‚Äì4011, 2022."
REFERENCES,0.4550898203592814,"[9] Chenxiao Zhang, Peng Yue, Deodato Tapete, Liangcun Jiang, Boyi Shangguan, Li Huang,
and Guangchao Liu. A deeply supervised image fusion network for change detection in high
resolution bi-temporal remote sensing images. ISPRS Journal of Photogrammetry and Remote
Sensing, 166:183‚Äì200, 2020."
REFERENCES,0.45808383233532934,"[10] Jinyuan Liu, Xin Fan, Zhanbo Huang, Guanyao Wu, Risheng Liu, Wei Zhong, and Zhongxuan
Luo. Target-aware dual adversarial learning and a multi-scenario multi-modality benchmark to
fuse infrared and visible for object detection. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 5802‚Äì5811, 2022."
REFERENCES,0.46107784431137727,"[11] Chaoben Du and Shesheng Gao. Image segmentation-based multi-focus image fusion through
multi-scale convolutional neural network. IEEE access, 5:15750‚Äì15761, 2017."
REFERENCES,0.46407185628742514,"[12] Jinyuan Liu, Zhu Liu, Guanyao Wu, Long Ma, Risheng Liu, Wei Zhong, Zhongxuan Luo,
and Xin Fan. Multi-interactive feature learning and a full-time multi-modality benchmark for
image fusion and segmentation. In Proceedings of the IEEE/CVF International Conference on
Computer Vision, pages 8115‚Äì8124, 2023."
REFERENCES,0.46706586826347307,"[13] Abeer D Algarni. Automated medical diagnosis system based on multi-modality image fusion
and deep learning. Wireless Personal Communications, 111:1033‚Äì1058, 2020."
REFERENCES,0.47005988023952094,"[14] Yiming Sun, Bing Cao, Pengfei Zhu, and Qinghua Hu. Detfusion: A detection-driven infrared
and visible image fusion network. In Proceedings of the 30th ACM International Conference on
Multimedia, 2022."
REFERENCES,0.47305389221556887,"[15] Han Xu and Jiayi Ma. Emfusion: An unsupervised enhanced medical image fusion network.
Information Fusion, 76:177‚Äì186, 2021."
REFERENCES,0.47604790419161674,"[16] Kanika Bhalla, Deepika Koundal, Bhisham Sharma, Yu-Chen Hu, and Atef Zaguia. A fuzzy
convolutional neural network for enhancing multi-focus image fusion. Journal of Visual
Communication and Image Representation, 84:103485, 2022."
REFERENCES,0.47904191616766467,"[17] Han Xu, Jiayi Ma, and Xiao-Ping Zhang. Mef-gan: Multi-exposure image fusion via generative
adversarial networks. IEEE Transactions on Image Processing, 29:7203‚Äì7216, 2020."
REFERENCES,0.4820359281437126,"[18] Krista Amolins, Yun Zhang, and Peter Dare. Wavelet based image fusion techniques‚Äîan
introduction, review and comparison. ISPRS Journal of photogrammetry and Remote Sensing,
62(4):249‚Äì263, 2007."
REFERENCES,0.48502994011976047,"[19] Yu Liu, Xun Chen, Hu Peng, and Zengfu Wang.
Multi-focus image fusion with a deep
convolutional neural network. Information Fusion, 36:191‚Äì207, 2017."
REFERENCES,0.4880239520958084,"[20] Jinyuan Liu, Runjia Lin, Guanyao Wu, Risheng Liu, Zhongxuan Luo, and Xin Fan. Coconet:
Coupled contrastive learning network with multi-level feature ensemble for multi-modality
image fusion. International Journal of Computer Vision, 132(5):1748‚Äì1775, 2024."
REFERENCES,0.49101796407185627,"[21] Jiayi Ma, Wei Yu, Chen Chen, Pengwei Liang, Xiaojie Guo, and Junjun Jiang. Pan-gan: An
unsupervised pan-sharpening method for remote sensing image fusion. Information Fusion, 62:
110‚Äì120, 2020."
REFERENCES,0.4940119760479042,"[22] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. Ilvr:
Conditioning method for denoising diffusion probabilistic models. In 2021 IEEE/CVF Interna-
tional Conference on Computer Vision (ICCV), 2021."
REFERENCES,0.49700598802395207,"[23] Mining Li, Ronghao Pei, Tianyou Zheng, Yang Zhang, and Weiwei Fu. Fusiondiff: Multi-focus
image fusion using denoising diffusion probabilistic models. Expert Systems with Applications,
238:121664, 2024."
REFERENCES,0.5,"[24] Zixiang Zhao, Haowen Bai, Yuanzhi Zhu, Jiangshe Zhang, Shuang Xu, Yulun Zhang, Kai
Zhang, Deyu Meng, Radu Timofte, and Luc Van Gool. Ddfm: denoising diffusion model for
multi-modality image fusion. arXiv preprint arXiv:2303.06840, 2023."
REFERENCES,0.5029940119760479,"[25] Jinyuan Liu, Xin Fan, Ji Jiang, Risheng Liu, and Zhongxuan Luo. Learning a deep multi-scale
feature ensemble and an edge-attention guidance for image fusion. IEEE Transactions on
Circuits and Systems for Video Technology, 32(1):105‚Äì119, 2021."
REFERENCES,0.5059880239520959,"[26] Hui Li, Xiao-Jun Wu, and Josef Kittler. Infrared and visible image fusion using a deep learning
framework. In 2018 24th international conference on pattern recognition (ICPR), pages
2705‚Äì2710. IEEE, 2018."
REFERENCES,0.5089820359281437,"[27] Mostafa Amin-Naji, Ali Aghagolzadeh, and Mehdi Ezoji. Ensemble of cnn for multi-focus
image fusion. Information fusion, 51:201‚Äì214, 2019."
REFERENCES,0.5119760479041916,"[28] Yu Liu, Xun Chen, Juan Cheng, and Hu Peng. A medical image fusion method based on
convolutional neural networks. In 2017 20th international conference on information fusion
(Fusion), pages 1‚Äì7. IEEE, 2017."
REFERENCES,0.5149700598802395,"[29] Jiayi Ma, Chen Chen, Chang Li, and Jun Huang. Infrared and visible image fusion via gradient
transfer and total variation minimization. Information Fusion, page 100‚Äì109, Sep 2016."
REFERENCES,0.5179640718562875,"[30] Zixiang Zhao, Haowen Bai, Jiangshe Zhang, Yulun Zhang, Shuang Xu, Zudi Lin, Radu Timofte,
and Luc Van Gool. Cddfuse: Correlation-driven dual-branch feature decomposition for multi-
modality image fusion. In Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition, pages 5906‚Äì5916, 2023."
REFERENCES,0.5209580838323353,"[31] Han Xu, Jiayi Ma, Junjun Jiang, Xiaojie Guo, and Haibin Ling. U2fusion: A unified unsuper-
vised image fusion network. IEEE Transactions on Pattern Analysis and Machine Intelligence,
44(1):502‚Äì518, 2020."
REFERENCES,0.5239520958083832,"[32] Jiayi Ma, Linfeng Tang, Fan Fan, Jun Huang, Xiaoguang Mei, and Yong Ma. Swinfusion:
Cross-domain long-range learning for general image fusion via swin transformer. IEEE/CAA
Journal of Automatica Sinica, 9(7):1200‚Äì1217, 2022."
REFERENCES,0.5269461077844312,"[33] Pengwei Liang, Junjun Jiang, Xianming Liu, and Jiayi Ma. Fusion from decomposition: A self-
supervised decomposition approach for image fusion. In European Conference on Computer
Vision, pages 719‚Äì735. Springer, 2022."
REFERENCES,0.5299401197604791,"[34] Pengfei Zhu, Yang Sun, Bing Cao, and Qinghua Hu. Task-customized mixture of adapters for
general image fusion. arXiv preprint arXiv:2403.12494, 2024."
REFERENCES,0.5329341317365269,"[35] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances
in neural information processing systems, 33:6840‚Äì6851, 2020."
REFERENCES,0.5359281437125748,"[36] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and
Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv
preprint arXiv:2011.13456, 2020."
REFERENCES,0.5389221556886228,"[37] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis.
Advances in neural information processing systems, 34:8780‚Äì8794, 2021."
REFERENCES,0.5419161676646707,"[38] Hyungjin Chung, Jeongsol Kim, Michael T Mccann, Marc L Klasky, and Jong Chul Ye. Diffu-
sion posterior sampling for general noisy inverse problems. arXiv preprint arXiv:2209.14687,
2022."
REFERENCES,0.5449101796407185,"[39] Hyungjin Chung, Jeongsol Kim, MichaelT. Mccann, MarcL. Klasky, and JongChul Ye. Dif-
fusion posterior sampling for general noisy inverse problems. The Eleventh International
Conference on Learning Representations, Sep 2023."
REFERENCES,0.5479041916167665,"[40] Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gra-
dient normalization for adaptive loss balancing in deep multitask networks. In International
conference on machine learning, pages 794‚Äì803. PMLR, 2018."
REFERENCES,0.5508982035928144,"[41] Xinyu Jia, Chuang Zhu, Minzhen Li, Wenqi Tang, and Wenli Zhou. Llvip: A visible-infrared
paired dataset for low-light vision. In Proceedings of the IEEE/CVF international conference
on computer vision, pages 3496‚Äì3504, 2021."
REFERENCES,0.5538922155688623,"[42] Xingchen Zhang. Benchmarking and comparing multi-exposure image fusion algorithms.
Information Fusion, 74:111‚Äì131, 2021."
REFERENCES,0.5568862275449101,"[43] Xingchen Zhang. Deep learning-based multi-focus image fusion: A survey and a comparative
study. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(9):4819‚Äì4838,
2021."
REFERENCES,0.5598802395209581,"[44] Prafulla Dhariwal and AlexanderQuinn Nichol. Diffusion models beat gans on image synthesis.
Neural Information Processing Systems,Neural Information Processing Systems, Dec 2021."
REFERENCES,0.562874251497006,"[45] Linfeng Tang, Xinyu Xiang, Hao Zhang, Meiqi Gong, and Jiayi Ma. Divfusion: Darkness-free
infrared and visible image fusion. Information Fusion, 91:477‚Äì493, 2023."
REFERENCES,0.5658682634730539,"[46] Chunyang Cheng, Tianyang Xu, and Xiao-Jun Wu. Mufusion: A general unsupervised image
fusion network based on memory unit. Information Fusion, 92:80‚Äì92, 2023."
REFERENCES,0.5688622754491018,"[47] Xunpeng Yi, Han Xu, Hao Zhang, Linfeng Tang, and Jiayi Ma. Text-if: Leveraging semantic text
guidance for degradation-aware and interactive image fusion. arXiv preprint arXiv:2403.16387,
2024."
REFERENCES,0.5718562874251497,"[48] Glenn Jocher. Yolov5 by ultralytics, 2020. URL https://github.com/ultralytics/
yolov5. doi:10.5281/zenodo.3908559."
REFERENCES,0.5748502994011976,"[49] Wenlong Zhang, Xiaolin Liu, Wuchao Wang, and Yujun Zeng. Multi-exposure image fu-
sion based on wavelet transform. International Journal of Advanced Robotic Systems, 15(2):
1729881418768939, 2018."
REFERENCES,0.5778443113772455,"[50] Jianming Zhang, Mingshuang Wu, Wei Cao, and Zi Xing. Partition-based image exposure
correction via wavelet-based high frequency restoration.
In International Conference on
Intelligent Computing, pages 452‚Äì463. Springer, 2024."
REFERENCES,0.5808383233532934,"[51] Linfeng Tang, Hao Zhang, Han Xu, and Jiayi Ma. Rethinking the necessity of image fusion
in high-level vision tasks: A practical infrared and visible image fusion network based on
progressive semantic injection and scene fidelity. Information Fusion, page 101870, 2023."
REFERENCES,0.5838323353293413,"[52] Hui Li and Xiao-Jun Wu. Densefuse: A fusion approach to infrared and visible images. IEEE
Transactions on Image Processing, 28(5):2614‚Äì2623, 2018."
REFERENCES,0.5868263473053892,"[53] Hui Li, Xiao-Jun Wu, and Josef Kittler. Rfn-nest: An end-to-end residual fusion network for
infrared and visible images. Information Fusion, 73:72‚Äì86, 2021."
REFERENCES,0.5898203592814372,"[54] Di Wang, Jinyuan Liu, Xin Fan, and Risheng Liu. Unsupervised misaligned infrared and
visible image fusion via cross-modality image generation and registration. arXiv preprint
arXiv:2205.11876, 2022."
REFERENCES,0.592814371257485,"[55] Wei Tang, Fazhi He, and Yu Liu. Ydtr: Infrared and visible image fusion via y-shape dynamic
transformer. IEEE Transactions on Multimedia, 2022."
REFERENCES,0.5958083832335329,"[56] Linfeng Tang, Jiteng Yuan, Hao Zhang, Xingyu Jiang, and Jiayi Ma. Piafusion: A progressive
infrared and visible image fusion network based on illumination aware. Information Fusion, 83:
79‚Äì92, 2022."
REFERENCES,0.5988023952095808,"[57] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn++: Improving the
pixelcnn with discretized logistic mixture likelihood and other modifications. arXiv preprint
arXiv:1701.05517, 2017."
REFERENCES,0.6017964071856288,"[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
processing systems, 30, 2017."
REFERENCES,0.6047904191616766,"[59] Alex Pappachen James and Belur V Dasarathy. Medical image fusion: A survey of the state of
the art. Information fusion, 19:4‚Äì19, 2014."
REFERENCES,0.6077844311377245,"[60] Keith A. Johnson and J. Alex Becker. Harvard medical website, 2023. URL http://www.med.
harvard.edu/AANLIB/home.html."
REFERENCES,0.6107784431137725,Appendix
REFERENCES,0.6137724550898204,"A
Experimental Settings"
REFERENCES,0.6167664670658682,"Our method not only excels in individual settings to generate customized images but also demonstrates
robust performance in general settings. In this section, we elaborate on numerous experiments for
image fusion tasks to demonstrate the superiority of our method. For different tasks, the basic
conditions vary. Specifically, in the VIF task, MSE [24, 46, 31, 34] serves as the basic condition,
where cMSE can be expressed as ||MSE(i, x0) + MSE(v, x0)||. In contrast, for the MEF and MFF
tasks, to achieve clearer and higher fidelity images and inspired by [29, 6, 49, 50], the basic conditions
include MSE, frequency and edge conditions, as detailed in the App. C. To ensure convenience and
fairness, we select eight enhanced conditions from the condition bank: SSIM, MSE, Edge, Low-
frequency, High-frequency, Spatial Frequency, Edge Intensity, and Standard Deviation while setting
k = 3 across all tasks."
REFERENCES,0.6197604790419161,"B
Experiments on More Comparisons"
REFERENCES,0.6227544910179641,"In our evaluation using the TNO dataset, we referred to the test set outlined in the work by Tang et
al. [51] We evaluated the fusion results in both quantitative and qualitative. Qualitative evaluation
primarily hinges on subjective visual assessments conducted by individuals. We expect that the fused
image will exhibit rich texture details and abundant color saturability. For different task scenarios, the
different evaluation metrics used, specifically, we employ six metrics including standard deviation
(SD), entropy (EN), spatial frequency (SF), sum of the correlations of differences (SCD), Structural
Similarity (SSIM), and edge intensity (EI). In the MFF and MEF tasks, considering the different task
scenarios, we employ SD, AG, SF, SCD, and MSE for evaluation metrics. For multi-modal image
fusion, we compare our method with four task-specific methods: DenseFuse [52], RFN-Nest [53],
UMF-CMGR [54], YDTR [55], and three general approaches U2Fusion [31], DeFusion [33]and
DDFM [24] on the LLVIP dataset and TNO dataset."
REFERENCES,0.625748502994012,"Quantitative Comparisons. We employ 6 quantitative metrics to evaluate our model, as shown
in Table 3. Our method demonstrated exceptional performance across various evaluation metrics.
On the TNO dataset, our method achieves the best result on the SD, EN, and SCD indicators. The
result shows suboptimal performance in SF and is close to the best values. Notably, our method does
not necessitate turning and holds its ground against methods requiring training. Compared with the
learning-free DDFM and DeFusion, our method shows better results. This demonstrates the excellent
performance of our model, achieving high performance across almost all indicators in a tuning-free
model."
REFERENCES,0.6287425149700598,"Qualitative Comparisons. The incorporation of both basic and enhanced conditions enables effective
preservation of background and texture. This comparison underscores our model‚Äôs efficacy in image
fusion, resulting in outstanding visual outcomes. As shown in Fig.7, our method demonstrates
superior visual quality compared to other approaches. Specifically, it excels in preserving intricate
texture details and color fidelity, such as license plate numbers (highlighted in the red box in Fig.7).
The DDFM and DeFusion approaches retain fewer texture details. As depicted in Fig. 7, CCF exhibits
the highest contrast, the clearest details, and the most comprehensive information content, further
highlighting its superiority in preserving texture details. Its excellent detail retention ability and clear
background generation further prove the effectiveness of our proposed method."
REFERENCES,0.6317365269461078,"C
Basic Conditions of the MEF and MFF"
REFERENCES,0.6347305389221557,"High-frequency It‚Äôs commonly understood that the high-frequency information in both modalities
is distinctive to each modality. For instance, texture and detailed information are specific to visible
images, while thermal radiation information pertains to infrared images. Specifically, we utilizes
wavelet transform to extract high-frequency information from the image. For example, 2D discrete
wavelet transformation with Haar wavelets to transform the input image (IM) into four sub-bands can
be expressed as:
{LLk, HLk, LHk, HHk} = Wk(LLk‚àí1)
(14)"
REFERENCES,0.6377245508982036,"Figure 7: Qualitative comparisons of our CCF and the competing methods on VIF fusion task of
TNO dataset"
REFERENCES,0.6407185628742516,"where LLk, HLk, LHk, HHk ‚ààR
H
2k √ó W"
REFERENCES,0.6437125748502994,"2k √óc, k ‚àà[1, K], signify the averages of the input image
and the high-frequency details in the vertical, horizontal, and diagonal directions, respectively, at the
first-level wavelet decomposition. Denote that the high-frequency details HF (IM) from IM. So we
can see the high-frequency condition below:"
REFERENCES,0.6467065868263473,"g
HF = HF (x0) ‚àíF(Œªh|HF (i)|, (1 ‚àíŒªh)|HF (v)|)
(15)"
REFERENCES,0.6497005988023952,"where |¬∑| stands for the absolute operation and F(¬∑, ¬∑) can represent a variety of customized functions.
In this paper, specifically, we employ the max function. The restored average coefficient and the
high-frequency coefficient at scale k are correspondingly converted into the output at scale k ‚àí1 by
employing W‚àí1 the 2D inverse discrete wavelet transform:"
REFERENCES,0.6526946107784432,"Œ¥ch = W‚àí1{LLk, g
HF k}
(16)"
REFERENCES,0.655688622754491,"Low-Frequency Condition. Low-frequency components generally encapsulate information common
to both modes. For simplicity, we directly utilize the low-frequency information obtained via wavelet
transform. However, each mode may possess slightly varying amounts of low-frequency information.
Our conditions allow for the adjustment of the low-frequency LF fusion ratio with hyperparameters,
i.e.,"
REFERENCES,0.6586826347305389,"f
LL = LF (x0) ‚àí(ŒªlLF (i) + (1 ‚àíŒªl)LF (v))
(17)"
REFERENCES,0.6616766467065869,"Œ¥cl = W‚àí1{ f
LLk, HFk}
(18)"
REFERENCES,0.6646706586826348,"Edge Condition. We aim at the fused image to retain the more intricate texture details from both
infrared and visible images. To achieve this, we integrated edge conditions into the set of conditions"
REFERENCES,0.6676646706586826,"Table 3: Comparison with SOTAs. Red indicates the best, blue indicates the second best, and green
indicates the third best."
REFERENCES,0.6706586826347305,"TNO Dataset
SD ‚Üë
EN ‚Üë
SF ‚Üë
SSIM ‚Üë
SCD ‚Üë
EI‚Üë"
REFERENCES,0.6736526946107785,"DenseFuse [52]
34.83
6.82
8.99
1.38
1.78
8.75
RFN-Nest [53]
36.90
6.96
5.87
1.31
1.78
7.13
UMF-CMGR [54]
29.97
6.53
8.17
1.43
1.64
7.32
YDTR [55]
28.03
6.43
7.62
1.41
1.56
6.81"
REFERENCES,0.6766467065868264,"U2Fusion [56]
37.70
7.00
11.86
1.28
1.78
12.78
DeFusion [31]
30.38
6.58
6.20
1.38
1.53
6.54
DDFM [24]
34.45
6.85
7.27
1.08
1.54
7.89
CCF (ours)
40.11
7.01
10.20
1.38
1.84
8.29"
REFERENCES,0.6796407185628742,Figure 8: Framework of our neural network architecture.
REFERENCES,0.6826347305389222,to enhance the edge texture in the fused image. The condition is defined as :
REFERENCES,0.6856287425149701,"Œ¥ce = |‚àá(x0)| ‚àíF(Œªe|‚àá(i)|, (1 ‚àíŒªe)|‚àá(v)|)
(19)"
REFERENCES,0.688622754491018,"where ‚àáindicates the Sobel gradient operator, which measures the texture detail information of an
image."
REFERENCES,0.6916167664670658,"MSE Condition. We incorporate the information from the original image into the fused image as
a supplementary content condition. Essentially, we utilize the image reconstruction capability of
DDPM to supplement lost details and enhance the informational content pertaining to the target in
the fused image, i.e."
REFERENCES,0.6946107784431138,"Œ¥cMSE = x0 ‚àíŒ®‚àí1(ŒªMSEŒ®(x0) ‚àí(1 ‚àíŒªMSE)Œ®(y))
(20)"
REFERENCES,0.6976047904191617,"where Œ® and Œ®‚àí1 represent the downsample and upsample operator, respectively, while y can refer
to either the infrared or visible image."
REFERENCES,0.7005988023952096,"Totally, these conditions can be combined to govern the generation of the final fused image:"
REFERENCES,0.7035928143712575,"Œ¥c = Œ∑hŒ¥ch + Œ∑lŒ¥cl + Œ∑eŒ¥ce + Œ∑contŒ¥cMSE
(21)"
REFERENCES,0.7065868263473054,"D
More Details of CCF"
REFERENCES,0.7095808383233533,"Algorithm 1 CCF
Input: : i,v
Output: : f"
REFERENCES,0.7125748502994012,"1: Œ¥C ‚ÜêC(i, v)
2: Sample xT ‚àºN(0, I)
3: for t=T, .., 1 do
4:
z ‚àºN(0, I)"
REFERENCES,0.7155688622754491,"5:
x0 = xt‚àí‚àö1‚àí¬ØŒ±tœµŒ∏(xt‚àí1|xt)
‚àö¬ØŒ±t
6:
for k=N,..., 1 do
7:
ÀÜx0 ‚Üêx0 ‚àíŒª ‚ñΩŒ¥ck
8:
end for
9: end for"
REFERENCES,0.718562874251497,"In the sampling process, the neural network ar-
chitecture is similar to PixelCNN++ [57], con-
sisting of a U-Net backbone with group nor-
malization. The diffusion step t is defined by
incorporating the Transformer [58] sinusoidal
position embedding into each residual block. As
shown in Fig. 8, six feature map resolutions are
utilized, with two convolutional residual blocks
per resolution level. Additionally, self-attention
blocks are incorporated at the 16√ó16 resolution
between the convolutional blocks."
REFERENCES,0.7215568862275449,"Representing a process of reverse diffusion (sam-
pling), starting from a standard normal distribu-
tion N(0, I), T sampling steps are performed
to generate the final fused image. However, the generation with unconditional DDPM [35] is uncon-
trollable. Therefore, conditions selected from the condition bank we constructed are introduced to
control the sampling process. More precisely, the conditions can rectify each step‚Äôs estimation of x0.
Finally, after the T step, the final fused image can be generated. The algorithm of CCF is presented
in Algorithm 1."
REFERENCES,0.7245508982035929,"E
More About Enhanced Conditions"
REFERENCES,0.7275449101796407,"We follow the recent works [30, 24, 46], and adopt the eight most commonly used constraints as
conditions in this paper. However, as shown in Table 4, our approach is not restricted to these"
REFERENCES,0.7305389221556886,"Table 4: Comparison across different numbers of enhancement conditions. For the 4 conditions,
conditions include SSIM, MSE, Edge, and SD. The 8 conditions expand to include SSIM, MSE,
Edge, SD, Low-frequency, High-frequency, SF, and EI. The 12 conditions incorporate the previous 8
conditions with four additional conditions: CC, MMS-SSIM, SCD, and VIF. Bold indicates the best
results"
REFERENCES,0.7335329341317365,"Number of conditions
SSIM ‚Üë
MSE ‚Üì
CC ‚Üë
PSNR ‚Üë
Nabf ‚Üì
Avg. Runtime(s)"
REFERENCES,0.7365269461077845,"4
1.12
1820
0.699
32.20
0.034
188
8 (ours)
1.22
1694
0.705
32.58
0.005
222
12
1.20
1545
0.719
33.21
0.002
454"
REFERENCES,0.7395209580838323,"Figure 9: Example of enhanced conditions selection in denoising iteration for rapidly changing
scenarios."
REFERENCES,0.7425149700598802,"constraints; fewer conditions can be randomly selected (here we select SSIM, MSE, Edge, and SD),
and additional enhanced conditions [31, 34](Correlation Coefficient (CC), Multi-Scale Structural
Similarity (MS-SSIM), the Sum of the Correlations of Differences (SCD), and the Visual Information
Fidelity (VIF)) can also be incorporated. Additionally, we evaluate the runtime across different
numbers of conditions to assess their impact on efficiency. While adding more conditions slightly
improves performance, it also increases inference runtime."
REFERENCES,0.7455089820359282,"Fig. 9 illustrates the selection of enhanced conditions at each denoising step in the same location
under both daylight and nighttime scenarios, representing a typical rapidly changing environment. It
demonstrates that CCF can adapt to these changing environments by selecting different conditions at
various denoising steps to respond effectively to dynamic environmental characteristics."
REFERENCES,0.7485029940119761,"F
More About Task-specific Conditions"
REFERENCES,0.7514970059880239,"The task-specific conditions are incorporated to guide the fusion process throughout the denoising
procedure. For instance, we take the Euclidean distance of features extracted by the object detection
model as the detection condition. In our experiments, we employ YOLOv5, which is pretrained
on the visible modality, to extract features from both the estimated x0 at each step and the visible
image. The Euclidean distance is minimized in the inverse diffusion process iteratively. Consequently,
the final fused image progressively integrates the object-specific information, enhancing the fusion
performance. We provide additional visualizations of the detection conditions, as shown in Fig. 10,
directly fused images appear to have a lot of missed detection in raw 1-4 and the detection scores are
generally higher. The last row of Fig.10 demonstrates a reduction in false-positive boxes when the
detection condition is applied."
REFERENCES,0.7544910179640718,"Table 5 also supports this, indicating that metrics such as mAP.5, mAP.5:95, and recall have increased
remarkably, showing that our method can customize conditions to fit the downstream task effectively.
By tailoring conditions to specific tasks, our approach effectively improves the applicability of
fused images for particular applications, as evidenced by the improved detection metrics. We
further explored the use of different levels of features obtained with YoloV5 as detection conditions.
Specifically, we evaluate in three scales and each of the scales is evaluated individually, and all
features combined involve using all three scale features simultaneously, averaged, to add task-specific
perception to enhance task performance. Furthermore, Table 5 indicates that there is no significant"
REFERENCES,0.7574850299401198,"Table 5: Comparison of task-specific conditions across different scales of features in YOLOv5s.The
Bold indicates the best results."
REFERENCES,0.7604790419161677,"Scale of feature
Position
mAP.5
mAP.5:95
Recall"
REFERENCES,0.7634730538922155,"w/o Feature
-
0.858
0.509
0.737"
REFERENCES,0.7664670658682635,"Feature 1 (160√ó128)
Before
0.884
0.537
0.832
Feature 2 (80√ó64)
Before
0.890
0.540
0.826
Feature 3 (40√ó32) (ours)
Before
0.907
0.563
0.832
All Features ( 1"
PI,0.7694610778443114,"3
Pi
3 Feature i)
Before
0.882
0.541
0.837
Final performance
After
0.894
0.537
0.824"
PI,0.7724550898203593,w/o detection
PI,0.7754491017964071,condition
PI,0.7784431137724551,with detection
PI,0.781437125748503,condition
PI,0.7844311377245509,w/o detection
PI,0.7874251497005988,condition
PI,0.7904191616766467,with detection
PI,0.7934131736526946,condition
PI,0.7964071856287425,Figure 10: Qualitative Comparisons of w/o and with the detection condition.
PI,0.7994011976047904,"difference between using different features. The constraint primarily enhances the implicit expressive
ability of the image, enabling it to better adapt to downstream tasks."
PI,0.8023952095808383,"G
Ablation"
PI,0.8053892215568862,"The quantitative results of the ablation study compare different scenarios: without DDPM, with
only basic conditions, with enhanced conditions but without SCS, and the complete CCF results.
With the reconstruction capability of DDPM, the fusion performance significantly improved after the
conditions were integrated. Some metrics for the enhanced conditions without SCS show a decline
due to conflicts among the various focusing aspect conditions. However, after incorporating SCS,
most metrics improve, demonstrating that SCS can enhance the image fusion process, producing
high-quality images."
PI,0.8083832335329342,Table 6: Ablation studies on the LLVIP dataset. Bold indicates the best.
PI,0.811377245508982,"DDPM
Basic Conditions
Enhanced conditions
SCS
SSIM‚Üë
MSE‚Üì
CC‚Üë
PSNR‚Üë
Nabf‚Üì"
PI,0.8143712574850299,"‚úì
0.28
2947
0.452
27.64
0.131
‚úì
‚úì
1.16
1785
0.683
33.06
0.032
‚úì
‚úì
‚úì
1.16
1779
0.677
32.17
0.072
‚úì
‚úì
‚úì
‚úì
1.22
1694
0.705
32.58
0.005"
PI,0.8173652694610778,Figure 11: Visualization of medical image fusion.
PI,0.8203592814371258,"H
Experiments on Medical Image Fusion"
PI,0.8233532934131736,"Medical image fusion (MIF)[59] experiments were conducted to validate the efficacy of our proposed
method using the Harvard Medical Image Dataset[60]. We visualized fused medical images, focusing
on MRI-SPECT, MRI-CT, and MRI-PET fusion, with the results presented in Fig. 11. Observing
our fused images, it is evident that the preservation of both the skull‚Äôs shape and intensity is notably
well-maintained. Within the brain region, the CCF method effectively combines intricate details and
structures from the two modalities."
PI,0.8263473053892215,"I
Limitations and Broader Impacts"
PI,0.8293413173652695,"Even though the proposed CCF model achieves superior performance over existing methods and
demonstrates advanced generalization with dynamic condition selection, there are still some potential
limitations. We provide a condition bank, but it needs to be constructed empirically. Most of
the conditions are inspired by Image Quality Assessment (IQA), but classifying these conditions
is challenging due to the complexity of IQA. Therefore, it is necessary to propose a method to
automatically classify the conditions, reducing empirical intervention. Additionally, our method relies
on a pre-trained diffusion model, which limits its efficiency and makes the generation process time-
consuming. Exploring more effective sampling processes would be valuable to improve efficiency
and further enhance the model‚Äôs performance. For potential social impact, it is difficult to ensure the
effectiveness of selected conditions for all scenarios, which may be risky in high-risk scenarios such
as medical imaging and autonomous driving."
PI,0.8323353293413174,NeurIPS Paper Checklist
CLAIMS,0.8353293413173652,1. Claims
CLAIMS,0.8383233532934131,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper‚Äôs contributions and scope?
Answer: [Yes]
Justification: The main claims made in the abstract and introduction accurately reflect the
contributions and scope.
Guidelines:"
CLAIMS,0.8413173652694611,"‚Ä¢ The answer NA means that the abstract and introduction do not include the claims
made in the paper.
‚Ä¢ The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
‚Ä¢ The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
‚Ä¢ It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2. Limitations"
CLAIMS,0.844311377245509,"Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: The limitation is discussed in the App. I.
Guidelines:"
CLAIMS,0.8473053892215568,"‚Ä¢ The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
‚Ä¢ The authors are encouraged to create a separate ""Limitations"" section in their paper.
‚Ä¢ The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
‚Ä¢ The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
‚Ä¢ The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
‚Ä¢ The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
‚Ä¢ If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
‚Ä¢ While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren‚Äôt acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3. Theory Assumptions and Proofs"
CLAIMS,0.8502994011976048,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [NA]"
CLAIMS,0.8532934131736527,"Justification: This paper does not provide theoretical results.
Guidelines:"
CLAIMS,0.8562874251497006,"‚Ä¢ The answer NA means that the paper does not include theoretical results.
‚Ä¢ All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
‚Ä¢ All assumptions should be clearly stated or referenced in the statement of any theorems.
‚Ä¢ The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
‚Ä¢ Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
‚Ä¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
4. Experimental Result Reproducibility"
CLAIMS,0.8592814371257484,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We provide implementation details in Sec. 5 and experimental setting in
App. A.
Guidelines:"
CLAIMS,0.8622754491017964,"‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢ If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
‚Ä¢ If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
‚Ä¢ Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
‚Ä¢ While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5. Open access to data and code"
CLAIMS,0.8652694610778443,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
CLAIMS,0.8682634730538922,"Answer: [Yes]
Justification: The source code is provided in https://github.com/jehovahxu/CCF.
Guidelines:"
CLAIMS,0.8712574850299402,"‚Ä¢ The answer NA means that paper does not include experiments requiring code.
‚Ä¢ Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
‚Ä¢ While we encourage the release of code and data, we understand that this might not be
possible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
‚Ä¢ The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
‚Ä¢ The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
‚Ä¢ The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
‚Ä¢ At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
‚Ä¢ Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6. Experimental Setting/Details"
CLAIMS,0.874251497005988,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We provide all the implementation details in Sec. 5 and App. A.
Guidelines:"
CLAIMS,0.8772455089820359,"‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢ The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
‚Ä¢ The full details can be provided either with the code, in appendix, or as supplemental
material.
7. Experiment Statistical Significance"
CLAIMS,0.8802395209580839,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [No]
Justification: All the experiments and comparisons are performed with the same settings.
Guidelines:"
CLAIMS,0.8832335329341318,"‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢ The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
‚Ä¢ The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
‚Ä¢ The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
‚Ä¢ The assumptions made should be given (e.g., Normally distributed errors).
‚Ä¢ It should be clear whether the error bar is the standard deviation or the standard error
of the mean."
CLAIMS,0.8862275449101796,"‚Ä¢ It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
‚Ä¢ For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
‚Ä¢ If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text."
EXPERIMENTS COMPUTE RESOURCES,0.8892215568862275,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.8922155688622755,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.8952095808383234,Answer: [Yes]
EXPERIMENTS COMPUTE RESOURCES,0.8982035928143712,Justification: We provide that in Sec. 5 and App. E.
EXPERIMENTS COMPUTE RESOURCES,0.9011976047904192,Guidelines:
EXPERIMENTS COMPUTE RESOURCES,0.9041916167664671,"‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢ The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
‚Ä¢ The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
‚Ä¢ The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn‚Äôt make it into the paper)."
CODE OF ETHICS,0.907185628742515,9. Code Of Ethics
CODE OF ETHICS,0.9101796407185628,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?"
CODE OF ETHICS,0.9131736526946108,Answer: [Yes]
CODE OF ETHICS,0.9161676646706587,"Justification: Our research conducted in the paper conforms, in every respect, with the
NeurIPS Code of Ethics."
CODE OF ETHICS,0.9191616766467066,Guidelines:
CODE OF ETHICS,0.9221556886227545,"‚Ä¢ The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
‚Ä¢ If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
‚Ä¢ The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction)."
BROADER IMPACTS,0.9251497005988024,10. Broader Impacts
BROADER IMPACTS,0.9281437125748503,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?"
BROADER IMPACTS,0.9311377245508982,Answer: [Yes]
BROADER IMPACTS,0.9341317365269461,Justification: We provide that in App. I.
BROADER IMPACTS,0.937125748502994,Guidelines:
BROADER IMPACTS,0.9401197604790419,"‚Ä¢ The answer NA means that there is no societal impact of the work performed.
‚Ä¢ If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
‚Ä¢ Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations."
BROADER IMPACTS,0.9431137724550899,"‚Ä¢ The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
‚Ä¢ The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
‚Ä¢ If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML)."
SAFEGUARDS,0.9461077844311377,11. Safeguards
SAFEGUARDS,0.9491017964071856,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?"
SAFEGUARDS,0.9520958083832335,Answer: [NA]
SAFEGUARDS,0.9550898203592815,Justification: All the generated images are carefully checked.
SAFEGUARDS,0.9580838323353293,Guidelines:
SAFEGUARDS,0.9610778443113772,"‚Ä¢ The answer NA means that the paper poses no such risks.
‚Ä¢ Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
‚Ä¢ Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
‚Ä¢ We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort."
LICENSES FOR EXISTING ASSETS,0.9640718562874252,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.9670658682634731,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
LICENSES FOR EXISTING ASSETS,0.9700598802395209,Answer: [Yes]
LICENSES FOR EXISTING ASSETS,0.9730538922155688,Justification: We have checked the related requirements.
LICENSES FOR EXISTING ASSETS,0.9760479041916168,Guidelines:
LICENSES FOR EXISTING ASSETS,0.9790419161676647,"‚Ä¢ The answer NA means that the paper does not use existing assets.
‚Ä¢ The authors should cite the original paper that produced the code package or dataset.
‚Ä¢ The authors should state which version of the asset is used and, if possible, include a
URL.
‚Ä¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
‚Ä¢ For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
‚Ä¢ If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
‚Ä¢ For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided."
LICENSES FOR EXISTING ASSETS,0.9820359281437125,"‚Ä¢ If this information is not available online, the authors are encouraged to reach out to
the asset‚Äôs creators.
13. New Assets"
LICENSES FOR EXISTING ASSETS,0.9850299401197605,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: No new assets are provided.
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9880239520958084,"‚Ä¢ The answer NA means that the paper does not release new assets.
‚Ä¢ Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
‚Ä¢ The paper should discuss whether and how consent was obtained from people whose
asset is used.
‚Ä¢ At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14. Crowdsourcing and Research with Human Subjects"
LICENSES FOR EXISTING ASSETS,0.9910179640718563,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: This paper does not study human subjects.
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9940119760479041,"‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
‚Ä¢ Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
‚Ä¢ According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: This paper does not conduct experiments with human participants.
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9970059880239521,"‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
‚Ä¢ Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
‚Ä¢ We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
‚Ä¢ For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
