Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0018656716417910447,"The cross-entropy objective has proved to be an all-purpose training objective for
1"
ABSTRACT,0.0037313432835820895,"autoregressive language models (LMs). However, without considering the penal-
2"
ABSTRACT,0.005597014925373134,"ization of problematic tokens, LMs trained using cross-entropy exhibit text degen-
3"
ABSTRACT,0.007462686567164179,"eration. To address this, unlikelihood training has been proposed to reduce the
4"
ABSTRACT,0.009328358208955223,"probability of unlikely tokens predicted by LMs. But unlikelihood does not con-
5"
ABSTRACT,0.011194029850746268,"sider the relationship between the label tokens and unlikely token candidates, thus
6"
ABSTRACT,0.013059701492537313,"showing marginal improvements in degeneration. We propose a new contrastive
7"
ABSTRACT,0.014925373134328358,"token learning objective that inherits the advantages of cross-entropy and unlikeli-
8"
ABSTRACT,0.016791044776119403,"hood training and avoids their limitations. The key idea is to teach a LM to gener-
9"
ABSTRACT,0.018656716417910446,"ate high probabilities for label tokens and low probabilities of negative candidates.
10"
ABSTRACT,0.020522388059701493,"Comprehensive experiments on language modeling and open-domain dialogue
11"
ABSTRACT,0.022388059701492536,"generation tasks show that the proposed contrastive token objective yields much
12"
ABSTRACT,0.024253731343283583,"less repetitive texts, with a higher generation quality than baseline approaches,
13"
ABSTRACT,0.026119402985074626,"achieving the new state-of-the-art performance on text degeneration.
14"
INTRODUCTION,0.027985074626865673,"1
Introduction
15"
INTRODUCTION,0.029850746268656716,"Autoregressive language models (LMs), such as OpenAI GPT-3 [1], have achieved impressive re-
16"
INTRODUCTION,0.03171641791044776,"sults on various natural language processing (NLP) tasks. The goal of training LMs is to learn the
17"
INTRODUCTION,0.033582089552238806,"true distribution of a text corpus, and this is usually achieved through next word prediction. Specif-
18"
INTRODUCTION,0.03544776119402985,"ically, a standard approach to training LMs is to minimize the cross-entropy loss between the true
19"
INTRODUCTION,0.03731343283582089,"distribution and the model prediction. Unfortunately, LMs trained using the cross-entropy objec-
20"
INTRODUCTION,0.03917910447761194,"tive have been observed to exhibit text degeneration problems, where token, phrase, and sentence
21"
INTRODUCTION,0.041044776119402986,"level repetition is a common symptom [6, 9, 27]. Such repeated texts differ markedly from those
22"
INTRODUCTION,0.04291044776119403,"generated by humans.1 To analyze the reasons for degeneration, our work views the vocabulary of
23"
INTRODUCTION,0.04477611940298507,"LMs as being composed of three sets of tokens at each time step, i.e., positive tokens (label tokens),
24"
INTRODUCTION,0.04664179104477612,"negative tokens (incorrectly repeating tokens), and irrelevant tokens (all the others). Based on this
25"
INTRODUCTION,0.048507462686567165,"taxonomy, we stress that cross-entropy is in fact a contrastive learning objective that contrasts posi-
26"
INTRODUCTION,0.05037313432835821,"tive tokens with negative and irrelevant tokens. While it is necessary for LMs to learn how to rank
27"
INTRODUCTION,0.05223880597014925,"positive tokens higher than other tokens in the predicted distribution, negative tokens are treated
28"
INTRODUCTION,0.054104477611940295,"equally to irrelevant tokens (whose number is usually much larger) by the cross-entropy objective.
29"
INTRODUCTION,0.055970149253731345,"As a consequence, negative tokens may not be suppressed hard enough.
30"
INTRODUCTION,0.05783582089552239,"To address the above issue, Welleck et al. [27] have proposed unlikelihood training to penalize
31"
INTRODUCTION,0.05970149253731343,"certain negative tokens, i.e., tokens being incorrectly repeated. The key idea behind unlikelihood
32"
INTRODUCTION,0.061567164179104475,"training is to lower the probability of negative tokens assigned by LMs. Despite its success, the
33"
INTRODUCTION,0.06343283582089553,"unlikelihood objective penalizes negative tokens by decreasing their predicted probability but does
34"
INTRODUCTION,0.06529850746268656,"1Readers are referrred to Table 4 for some concrete examples. The degeneration problem even exists in
large-scale, state-of-the-art, pre-trained language models such as GPT-3 [18]."
INTRODUCTION,0.06716417910447761,Hidden state
INTRODUCTION,0.06902985074626866,Positive token
INTRODUCTION,0.0708955223880597,Negative token
INTRODUCTION,0.07276119402985075,Contrastive token
INTRODUCTION,0.07462686567164178,"be
more there"
INTRODUCTION,0.07649253731343283,"more
contrast
distinction"
INTRODUCTION,0.07835820895522388,Cross-entropy
INTRODUCTION,0.08022388059701492,"be
..."
INTRODUCTION,0.08208955223880597,"All non-label tokens
Preceding"
INTRODUCTION,0.08395522388059702,"tokens
Label
token ... ..."
INTRODUCTION,0.08582089552238806,Unlikelihood training there be
INTRODUCTION,0.08768656716417911,All preceding
INTRODUCTION,0.08955223880597014,tokens
INTRODUCTION,0.0914179104477612,Language model Let
INTRODUCTION,0.09328358208955224,"there
be more ."
INTRODUCTION,0.09514925373134328,"be
there"
INTRODUCTION,0.09701492537313433,contrast
INTRODUCTION,0.09888059701492537,"Label
token"
INTRODUCTION,0.10074626865671642,contrast more more
INTRODUCTION,0.10261194029850747,Irrelevant token
INTRODUCTION,0.1044776119402985,"Push away
Pull together
Figure 1: Illustrating the differences between our proposed contrastive token learning, unlikelihood
training, and the cross-entropy objective for LMs. For contrastive token learning, we use the label
token as the positive token and the preceding M tokens as the negative tokens at each decoding step."
INTRODUCTION,0.10634328358208955,"not consider the relationship between positive and negative tokens. Unlikelihood training also unin-
35"
INTRODUCTION,0.10820895522388059,"tentionally boosts the probability of other irrelevant tokens. Moreover, all previous context tokens
36"
INTRODUCTION,0.11007462686567164,"are used as negative candidates per generation step. Such an objective not only introduces a consid-
37"
INTRODUCTION,0.11194029850746269,"erable amount of noise, but also results in sub-optimal repetition reduction, thus affecting the ﬁnal
38"
INTRODUCTION,0.11380597014925373,"generation performance.
39"
INTRODUCTION,0.11567164179104478,"In this paper, we introduce a simple yet effective contrastive token learning (CT for short) objective
40"
INTRODUCTION,0.11753731343283583,"that integrates the best of cross-entropy and unlikelihood training, penalizing negative tokens by
41"
INTRODUCTION,0.11940298507462686,"contrasting them with positive tokens. The commonalities and differences between cross-entropy,
42"
INTRODUCTION,0.12126865671641791,"unlikelihood training, and CT are illustrated in Figure 1. Brieﬂy, (i) without distinguishing between
43"
INTRODUCTION,0.12313432835820895,"negative and irrelevant tokens, cross-entropy cannot effectively suppress negative tokens; (ii) due to
44"
INTRODUCTION,0.125,"the lack of contrast between negative and positive tokens, it is difﬁcult for unlikelihood training to
45"
INTRODUCTION,0.12686567164179105,"penalize negative tokens; and (iii) through its more focused contrast between positive and negative
46"
INTRODUCTION,0.1287313432835821,"tokens, CT can take goal-directed actions rather than just predicting label tokens, i.e., explicitly
47"
INTRODUCTION,0.13059701492537312,"teaching the LM to assign negative tokens with a lower probability than positive tokens. In this
48"
INTRODUCTION,0.13246268656716417,"work, we combine the CT and cross-entropy objectives to train LMs, where cross-entropy performs
49"
INTRODUCTION,0.13432835820895522,"on the label tokens so that they are assigned the highest probability, and CT effectively suppresses
50"
INTRODUCTION,0.13619402985074627,"negative tokens from being generated.
51"
INTRODUCTION,0.13805970149253732,"We perform evaluations on the tasks of language modeling and open-domain dialogue generation.2
52"
INTRODUCTION,0.13992537313432835,"Our empirical evidence demonstrates that LMs trained with the proposed CT objective can generate
53"
INTRODUCTION,0.1417910447761194,"much less repetitive texts using standard greedy or beam search and achieve superior text generation
54"
INTRODUCTION,0.14365671641791045,"performance under both automatic and human evaluations. CT has a minor negative inﬂuence on
55"
INTRODUCTION,0.1455223880597015,"the perplexity of LMs, but thanks to the reduced repetition rates, in our case studies we observe
56"
INTRODUCTION,0.14738805970149255,"substantial improvements regarding the quality of generated text.
57"
BACKGROUND,0.14925373134328357,"2
Background
58"
BACKGROUND,0.15111940298507462,"LMs aim to learn the true distribution over variable-length text sequences in a text corpus X =
59"
BACKGROUND,0.15298507462686567,"(x1, x2, . . . , x|X|) with |X| tokens. A popular approach to this task is next word prediction, i.e.,
60"
BACKGROUND,0.15485074626865672,"predicting a distribution over the next word following a given context. To train such a language
61"
BACKGROUND,0.15671641791044777,"model, cross-entropy and unlikelihood training are two representative objectives. In this section,
62"
BACKGROUND,0.15858208955223882,"we ﬁrst review cross-entropy and unlikelihood training. We then provide an analysis of the text
63"
BACKGROUND,0.16044776119402984,"degeneration problem.
64"
BACKGROUND,0.1623134328358209,"2Our source code, including data pre-processing scripts, our trained models, and an interactive Google
Colab notebook, is available at https://anonymous.4open.science/r/lit-seq."
BACKGROUND,0.16417910447761194,"Table 1: The inﬂuence comparison of different learning objectives over the positive (label), negative
(incorrectly repeating), and irrelevant tokens (all the others) for the LMs."
BACKGROUND,0.166044776119403,Relevant tokens
BACKGROUND,0.16791044776119404,"Loss
Positive
Negative
Irrelevant tokens Contrast"
BACKGROUND,0.16977611940298507,"Cross-entropy (CE)
Promote Suppress
Suppress
Yes
Unlikelihood training (UL) Promote Suppress/Promote Promote
No
Contrastive token (CT)
Promote Suppress
Unchanged
Yes"
CROSS ENTROPY,0.17164179104477612,"2.1
Cross entropy
65"
CROSS ENTROPY,0.17350746268656717,"A standard approach to training a LM is to minimize the expected cross-entropy loss between the
66"
CROSS ENTROPY,0.17537313432835822,"true distribution and the model prediction [28]. Speciﬁcally, the cross-entropy loss for each time
67"
CROSS ENTROPY,0.17723880597014927,"step t is deﬁned as:
68"
CROSS ENTROPY,0.1791044776119403,"Lt
CE = −log p(xt|x<t)
(1)"
CROSS ENTROPY,0.18097014925373134,"= −log
exp(hT
t Wxt)
∑"
CROSS ENTROPY,0.1828358208955224,"ˆxt∈V exp(hT
t Wˆxt)
(2) = log "
CROSS ENTROPY,0.18470149253731344,"1 +
∑"
CROSS ENTROPY,0.1865671641791045,"ˆxt∈V,ˆxt̸=xt
exp(hT
t Wˆxt −hT
t Wxt) "
CROSS ENTROPY,0.1884328358208955,",
(3)"
CROSS ENTROPY,0.19029850746268656,"where ht is the model hidden state at time t, W is the embedding matrix, and Wxt denotes the word
69"
CROSS ENTROPY,0.1921641791044776,"embedding of token xt. Through some simple transformations from Eq. (1)–(3), we can see that
70"
CROSS ENTROPY,0.19402985074626866,"Eq. (3) is similar to the N-pair contrastive loss [24] for visual object recognition. In other words,
71"
CROSS ENTROPY,0.1958955223880597,"cross-entropy effectively trains LMs to contrast the label tokens (positive examples) xt with all the
72"
CROSS ENTROPY,0.19776119402985073,"other non-label tokens (negative and irrelevant examples) ˆxt ∈V, ˆxt ̸= xt in the whole vocabulary.
73"
UNLIKELIHOOD TRAINING,0.19962686567164178,"2.2
Unlikelihood training
74"
UNLIKELIHOOD TRAINING,0.20149253731343283,"To address the repetition issue of cross-entropy, Welleck et al. [27] have proposed unlikelihood
75"
UNLIKELIHOOD TRAINING,0.20335820895522388,"training to penalize the likelihood of negative tokens (UL-T). The unlikelihood loss for time step t
76"
UNLIKELIHOOD TRAINING,0.20522388059701493,"is deﬁned as:
77"
UNLIKELIHOOD TRAINING,0.20708955223880596,"Lt
UL = −
∑"
UNLIKELIHOOD TRAINING,0.208955223880597,"x−
t ∈Ct
log(1 −p(x−
t |x<t)),
(4)"
UNLIKELIHOOD TRAINING,0.21082089552238806,"where Ct = {x1, . . . , xt−1}\{xt} is the set of negative tokens at time t, i.e., all previous context
78"
UNLIKELIHOOD TRAINING,0.2126865671641791,"tokens. In this paper, we refer to this set of negative tokens as the preceding tokens set. As we will
79"
UNLIKELIHOOD TRAINING,0.21455223880597016,"see in §2.3, UL-T does not work well as it can increase the probability of irrelevant tokens. Welleck
80"
UNLIKELIHOOD TRAINING,0.21641791044776118,"et al. [27] have also proposed a more effective sequence-level unlikelihood objective (UL-S) that
81"
UNLIKELIHOOD TRAINING,0.21828358208955223,"uses unlikelihood on decoded continuations during training time. We omit the details here as our
82"
UNLIKELIHOOD TRAINING,0.22014925373134328,"proposed CT is more closely related to UL-T, but we do compare CT to UL-S in our experiments.
83"
DISCUSSION,0.22201492537313433,"2.3
Discussion
84"
DISCUSSION,0.22388059701492538,"The main difference between Eq. (3) and the N-pair contrastive loss is that, in Eq. (3), negative and
85"
DISCUSSION,0.22574626865671643,"irrelevant tokens are treated equally by cross-entropy.3 These negative tokens need to be penalized
86"
DISCUSSION,0.22761194029850745,"harder than irrelevant tokens, otherwise, negative tokens may be incorrectly repeated in later time
87"
DISCUSSION,0.2294776119402985,"steps. This explains why LMs trained by cross-entropy have high repetition rates.
88"
DISCUSSION,0.23134328358208955,"Although UL-T penalizes negative tokens, it does not work well enough, and as can be seen from
89"
DISCUSSION,0.2332089552238806,"Table 1, the reasons are twofold. First, each negative token is not deﬁnitely penalized because it
90"
DISCUSSION,0.23507462686567165,"depends on the inﬂuence of other negative tokens, which can be seen from the gradient analysis
91"
DISCUSSION,0.23694029850746268,"of UL-T (Eq. (11) in Appendix D). Second, the formulation of UL-T unintentionally boosts the
92"
DISCUSSION,0.23880597014925373,"probability of other irrelevant tokens and may make them surface as repeated tokens. We detail this
93"
DISCUSSION,0.24067164179104478,"analysis in §3.3.
94"
DISCUSSION,0.24253731343283583,"3Albeit with different strengths, as seen in Eq. (10) in Appendix D."
METHOD,0.24440298507462688,"3
Method
95"
METHOD,0.2462686567164179,"To address the issues discussed above and inherit the advantages of cross-entropy and unlikelihood
96"
METHOD,0.24813432835820895,"training, in this section, we present a novel contrastive token learning (CT) objective for LMs. We
97"
METHOD,0.25,"ﬁrst deﬁne the CT loss for each time step. Then we introduce a positive and negative token selection
98"
METHOD,0.251865671641791,"strategy. Finally, we discuss the differences and connections of CT with respect to cross-entropy
99"
METHOD,0.2537313432835821,"and unlikelihood training.
100"
CONTRASTIVE TOKEN LEARNING,0.2555970149253731,"3.1
Contrastive token learning
101"
CONTRASTIVE TOKEN LEARNING,0.2574626865671642,"The key idea of CT is to promote positive (label) tokens in the ranking at each step, while lowering
102"
CONTRASTIVE TOKEN LEARNING,0.2593283582089552,"negative (incorrectly repeating) tokens, and leave other irrelevant tokens untouched. To this end, we
103"
CONTRASTIVE TOKEN LEARNING,0.26119402985074625,"formulate the CT loss for step t as:
104"
CONTRASTIVE TOKEN LEARNING,0.2630597014925373,"Lt
CT = log "
CONTRASTIVE TOKEN LEARNING,0.26492537313432835,"1 +
∑"
CONTRASTIVE TOKEN LEARNING,0.2667910447761194,"x−
t ∈St
N"
CONTRASTIVE TOKEN LEARNING,0.26865671641791045,"exp(hT
t Wx−
t −hT
t Wxt) "
CONTRASTIVE TOKEN LEARNING,0.27052238805970147,",
(5)"
CONTRASTIVE TOKEN LEARNING,0.27238805970149255,"where St
N is the negative token set and xt is the positive token (i.e., label token) at time t. We detail
105"
CONTRASTIVE TOKEN LEARNING,0.27425373134328357,"the token selection mechanism of St
N below.
106"
CONTRASTIVE TOKEN LEARNING,0.27611940298507465,"During the training phase, we combine the CT loss with the cross-entropy loss for each time step as
107"
CONTRASTIVE TOKEN LEARNING,0.27798507462686567,"follows:
108"
CONTRASTIVE TOKEN LEARNING,0.2798507462686567,"Lt = Lt
CE + Lt
CT ,
(6)
where Lt
CE aims to promote label tokens, training models to assign the highest probabilities to such
109"
CONTRASTIVE TOKEN LEARNING,0.28171641791044777,"tokens. On the other hand, Lt
CT focuses on contrasting positive tokens and negative tokens, so that
110"
CONTRASTIVE TOKEN LEARNING,0.2835820895522388,"the LMs can learn to effectively rank negative tokens lower than their positive counterparts.
111"
NEGATIVE TOKEN SELECTION STRATEGY,0.28544776119402987,"3.2
Negative token selection strategy
112"
NEGATIVE TOKEN SELECTION STRATEGY,0.2873134328358209,"Following [27], we use the preceding tokens set without requiring additional supervision as our
113"
NEGATIVE TOKEN SELECTION STRATEGY,0.2891791044776119,"negative tokens St
N. However, using all preceding tokens (as in [27]) may bring too much noise to
114"
NEGATIVE TOKEN SELECTION STRATEGY,0.291044776119403,"the training process, especially for later time steps in a sequence. Hence, we instead propose to use
115"
NEGATIVE TOKEN SELECTION STRATEGY,0.292910447761194,"the preceding M tokens set to decide the negative tokens, with M being a hyper-parameter. The set
116"
NEGATIVE TOKEN SELECTION STRATEGY,0.2947761194029851,"St
N is deﬁned as:
117"
NEGATIVE TOKEN SELECTION STRATEGY,0.2966417910447761,"St
N = {xt−M, . . . , xt−1}\{xt}.
(7)
Another difference with the preceding tokens set [27] is that, St
N is a multiset that does not remove
118"
NEGATIVE TOKEN SELECTION STRATEGY,0.29850746268656714,"redundant occurrences. Intuitively, minimizing the CT loss with the preceding M tokens set makes
119"
NEGATIVE TOKEN SELECTION STRATEGY,0.3003731343283582,"more frequently repeated tokens less likely to be predicted.
120"
GRADIENT ANALYSIS,0.30223880597014924,"3.3
Gradient analysis
121"
GRADIENT ANALYSIS,0.3041044776119403,"To see how loss functions inﬂuence the positive, negative and irrelevant tokens during training, we
122"
GRADIENT ANALYSIS,0.30597014925373134,"derive the gradient functions of each loss function with respect to these tokens in Appendix D. Table
123"
GRADIENT ANALYSIS,0.30783582089552236,"1 is an intuitive summary of the inﬂuences, from which one can observe that: (i) Cross-entropy
124"
GRADIENT ANALYSIS,0.30970149253731344,"trains to promote label tokens in rankings at each time-step, while suppressing all the other tokens
125"
GRADIENT ANALYSIS,0.31156716417910446,"including negative and irrelevant tokens. (ii) It cannot be decided for unlikelihood training whether
126"
GRADIENT ANALYSIS,0.31343283582089554,"the negative tokens are promoted or suppressed by the gradient function (cf. Eq. (11) in Appendix D,
127"
GRADIENT ANALYSIS,0.31529850746268656,"the valid region for the corresponding gradient function contains both positive and negative values),
128"
GRADIENT ANALYSIS,0.31716417910447764,"and irrelevant tokens are promoted, both of which are problematic. (iii) With contrastive token
129"
GRADIENT ANALYSIS,0.31902985074626866,"learning, CT promotes positive tokens and suppresses negative tokens, and it is the only objective
130"
GRADIENT ANALYSIS,0.3208955223880597,"that does not affect irrelevant tokens (cf. the gradient functions in Appendix D).
131"
GRADIENT ANALYSIS,0.32276119402985076,"When using CT together with CE, as we do for our ﬁnal loss function, negatives are suppressed both
132"
GRADIENT ANALYSIS,0.3246268656716418,"in CT and in CE, while irrelevant tokens are only suppressed in CE. Therefore, our CT objective is
133"
GRADIENT ANALYSIS,0.32649253731343286,"able to better restrain incorrectly repeated tokens.
134"
RELATED WORK,0.3283582089552239,"4
Related work
135"
RELATED WORK,0.3302238805970149,"We review two lines of related work, i.e., neural text degeneration and contrastive learning.
136"
RELATED WORK,0.332089552238806,"Neural text degeneration.
With large-scale pre-training, state-of-the-art neural LMs are able to
137"
RELATED WORK,0.333955223880597,"generate human-like texts [1, 28]. However, they suffer from the text degeneration problem, where
138"
RELATED WORK,0.3358208955223881,"model-generated texts are dull and repetitive [6, 7, 27]. The text degeneration problem is especially
139"
RELATED WORK,0.3376865671641791,"serious with open-ended generation tasks, such as dialogue generation [9, 23] and language model-
140"
RELATED WORK,0.33955223880597013,"ing [6, 27]. Some decoding approaches have been proposed to address this problem, by introducing
141"
RELATED WORK,0.3414179104477612,"randomness [4, 6] or disparity [23, 25] at inference time. Some other work suggests that the de-
142"
RELATED WORK,0.34328358208955223,"generation problem is caused by defects of the likelihood training objective, and improved training
143"
RELATED WORK,0.3451492537313433,"objectives have been proposed [8, 25, 27].
144"
RELATED WORK,0.34701492537313433,"Our proposed contrastive token learning approach belongs to the training objective family. Com-
145"
RELATED WORK,0.34888059701492535,"pared to unlikelihood training [27], we address the suppression of repetitive tokens by contrasting
146"
RELATED WORK,0.35074626865671643,"them with positive tokens.
147"
RELATED WORK,0.35261194029850745,"Contrastive learning. In computer vision, contrastive learning has been widely employed to learn
148"
RELATED WORK,0.35447761194029853,"representations [2, 10, 24]. Noise-contrastive estimation [5] has been proved successful for training
149"
RELATED WORK,0.35634328358208955,"word embeddings [16]. In recent years, contrastive learning has gained more attention in the area of
150"
RELATED WORK,0.3582089552238806,"natural language processing too. Most work builds contrasts at the sequence or document level by
151"
RELATED WORK,0.36007462686567165,"corrupting the ground truth sequence [3, 12, 14, 29] or mining positive/negative samples [17, 19].
152"
RELATED WORK,0.3619402985074627,"Existing token-level contrastive learning frameworks contrast model representations from different
153"
RELATED WORK,0.36380597014925375,"positions [25, 30]. Differently, we contrast word embeddings while using the hidden representations
154"
RELATED WORK,0.3656716417910448,"as anchor points similar to the triplet contrastive loss [22]. Our formulation effectively contrasts
155"
RELATED WORK,0.3675373134328358,"logits output by the model for positive and negative tokens, thus it is more direct than unlikelihood
156"
RELATED WORK,0.3694029850746269,"training on addressing the repetitive degeneration problem. To the best of our knowledge, our pro-
157"
RELATED WORK,0.3712686567164179,"posed contrastive token learning is the ﬁrst to use token embeddings as positive/negative examples
158"
RELATED WORK,0.373134328358209,"in a contrastive framework for the text degeneration problem.
159"
EXPERIMENTAL SETUP,0.375,"5
Experimental setup
160"
EXPERIMENTAL SETUP,0.376865671641791,"We compare CT with baseline approaches on the language modeling and open-domain dialogue
161"
EXPERIMENTAL SETUP,0.3787313432835821,"generation task. Since our experimental results on the dialogue task show a similar pattern as on the
162"
EXPERIMENTAL SETUP,0.3805970149253731,"language modeling task, we will focus on the language modeling task in the body of the paper and
163"
EXPERIMENTAL SETUP,0.3824626865671642,"postpone the setup and analyses of the dialogue task to Appendix I.
164"
EXPERIMENTAL SETUP,0.3843283582089552,"Baselines and implementation. We implement several state-of-the-art baselines and use them with
165"
EXPERIMENTAL SETUP,0.38619402985074625,"GPT-2 [20]: (i) The vanilla cross-entropy (CE) objective; (ii) decoding-based methods: banning
166"
EXPERIMENTAL SETUP,0.3880597014925373,"3-grams [21], top-k sampling [4], nucleus sampling [6] and contrastive search (SimCTG-CS) [25];
167"
EXPERIMENTAL SETUP,0.38992537313432835,"and (iii) learning-based methods: unlikelihood training [27], SimCTG [25], and noise-contrastive
168"
EXPERIMENTAL SETUP,0.3917910447761194,"estimation (NCE; detailed in Appendix C) [5]. More details can be found in Appendix E.
169"
EXPERIMENTAL SETUP,0.39365671641791045,"Dataset, training and inference details. At training time, we ﬁne-tune GPT-2 small on the widely-
170"
EXPERIMENTAL SETUP,0.39552238805970147,"used Wikitext-103 dataset [15] with each learning-based approach (including the CE baseline) for
171"
EXPERIMENTAL SETUP,0.39738805970149255,"50K steps with 3K warm-up steps. As suggested in [27], for sequence-level unlikelihood training,
172"
EXPERIMENTAL SETUP,0.39925373134328357,"we ﬁrst ﬁne-tune the language model using UL-T for 48.5K steps, and then switch to the UL-S
173"
EXPERIMENTAL SETUP,0.40111940298507465,"objective for another 1.5K steps, resulting in UL-TS. Best model checkpoints for each task are
174"
EXPERIMENTAL SETUP,0.40298507462686567,"selected according to the lowest validation CE loss with an evaluation interval of 1K training steps.
175"
EXPERIMENTAL SETUP,0.4048507462686567,"We use trunks of 512 tokens, and a training batch size of 4. All models are trained using the Adam
176"
EXPERIMENTAL SETUP,0.40671641791044777,"optimizer [11] with a learning rate of 1e-5. For UL-TS, we had to use a smaller learning rate of
177"
EXPERIMENTAL SETUP,0.4085820895522388,"1e-6, otherwise the generated texts contain massive ungrammatical repetitions (continuous token
178"
EXPERIMENTAL SETUP,0.41044776119402987,"repetitions, as can be seen in Table 5 of Appendix F).
179"
EXPERIMENTAL SETUP,0.4123134328358209,"At inference time, we compare the performance of each approach to text degeneration using both
180"
EXPERIMENTAL SETUP,0.4141791044776119,"greedy search and beam search. We use k = 50 for top-k sampling, and p = 0.9 for deciding the
181"
EXPERIMENTAL SETUP,0.416044776119403,"sampling pool of the nucleus method. We follow Welleck et al. [27] to use 50 tokens as the input
182"
EXPERIMENTAL SETUP,0.417910447761194,"preﬁx and let the model generate 100 tokens as a continuation.
183"
EXPERIMENTAL SETUP,0.4197761194029851,"Evaluation metrics. We measure the perplexity (ppl) of different approaches. For measuring gen-
184"
EXPERIMENTAL SETUP,0.4216417910447761,"erative repetition, we follow Welleck et al. [27] to use 1-gram to 4-gram repetition rates (rep-1
185"
EXPERIMENTAL SETUP,0.42350746268656714,"– rep-4), which are deﬁned as the number of repeated n-grams divided by the total number of
186"
EXPERIMENTAL SETUP,0.4253731343283582,"generated n-grams in each sequence, micro-averaged over the whole dataset. We also report the
187"
EXPERIMENTAL SETUP,0.42723880597014924,"generation diversity at the dataset level, which is measured by distinct 1-gram rates (dist-1) [13]
188"
EXPERIMENTAL SETUP,0.4291044776119403,"and unique 1-gram counts (uniq-1). We adopt human evaluation for measuring the quality of
189"
EXPERIMENTAL SETUP,0.43097014925373134,"Table 2: Results on the test set of Wikitext-103 for the language modeling task. ↑/↓arrows denote
whether higher or lower is better for a metric. The best result for either type of approach (decoding-
based vs. learning-based) under each metric is highlighted in bold face. ‡ Does not count as the best.
† For this experiment, we use a beam size of 5 as suggested in its original paper [25]."
EXPERIMENTAL SETUP,0.43283582089552236,"ppl↓
ppl-s↓
search
rep-1↓
rep-2↓
rep-3↓
rep-4↓
dist-1↑
uniq-1↑"
EXPERIMENTAL SETUP,0.43470149253731344,"GPT-2
18.01
25.95
greedy
71.03
60.12
54.77
50.93
1.15
12787
beam
77.02
69.70
65.49
61.69
1.12
12545"
EXPERIMENTAL SETUP,0.43656716417910446,decoding-based
-GRAM BAN,0.43843283582089554,"3-gram ban
18.01
25.95
greedy
50.09
18.31
0.00‡
0.00‡
1.52
16940
beam
40.91
10.40
0.00‡
0.00‡
1.35
15114"
-GRAM BAN,0.44029850746268656,"Top-k
18.01
25.95
greedy
34.80
9.38
3.86
1.73
2.23
24840
beam
73.47
64.38
59.31
54.88
1.19
13280"
-GRAM BAN,0.44216417910447764,"Nucleus
18.01
25.95
greedy
38.41
12.10
5.50
2.78
2.06
23038
beam
74.28
65.70
60.86
56.58
1.17
13004"
-GRAM BAN,0.44402985074626866,"SimCTG-CS
18.12
26.10
greedy
70.23
58.92
53.44
49.54
1.17
13005
beam†
31.93
6.52
2.23
0.94
1.77
19746"
-GRAM BAN,0.4458955223880597,learning-based
-GRAM BAN,0.44776119402985076,"SimCTG
18.12
26.10
greedy
70.23
58.92
53.44
49.54
1.17
13005
beam
75.87
68.02
63.54
59.52
1.15
12835"
-GRAM BAN,0.4496268656716418,"NCE
18.60
32.88
greedy
57.23
41.59
35.50
31.75
1.32
14774
beam
56.02
40.99
34.73
30.48
1.28
14322"
-GRAM BAN,0.45149253731343286,"UL-T
18.93
26.63
greedy
60.91
45.15
38.31
33.90
1.26
14071
beam
67.39
55.95
49.85
44.78
1.15
12874"
-GRAM BAN,0.4533582089552239,"UL-TS
18.88
27.41
greedy
51.98
29.17
19.71
14.42
1.29
14378
beam
45.81
23.96
15.60
10.41
1.27
14141"
-GRAM BAN,0.4552238805970149,"CT
18.72
64.01
greedy
22.09
4.02
1.49
0.80
2.05
22832
beam
27.18
9.71
5.73
3.77
1.68
18697"
-GRAM BAN,0.457089552238806,"Human
–
–
–
29.92
7.25
2.81
1.14
3.41
19034"
-GRAM BAN,0.458955223880597,"model generated texts. We randomly select 100 preﬁxes from the test set of Wikitext-103, and com-
190"
-GRAM BAN,0.4608208955223881,"pare the continuations generated using CT with those by the best-performing baselines according to
191"
-GRAM BAN,0.4626865671641791,"the automatic evaluation results. Since it does not make much sense to compare continuations with
192"
-GRAM BAN,0.46455223880597013,"either side having excessive repetitions, we ﬁlter out such pairs using a threshold of rep-4 ≤0.05
193"
-GRAM BAN,0.4664179104477612,"to make the comparisons more competitive. Then we display the preﬁx and two continuations from
194"
-GRAM BAN,0.46828358208955223,"different systems (side-by-side, in a random order) to three crowd workers and ask them to select
195"
-GRAM BAN,0.4701492537313433,"the winner in terms of repetition, coherence, ﬂuency, and overall quality. Ties are allowed for all
196"
-GRAM BAN,0.47201492537313433,"aspects. We use majority voting to decide the ﬁnal winner. Details about our question form design
197"
-GRAM BAN,0.47388059701492535,"and the instructions to crowd workers can be found in Appendix G.
198"
EVALUATION RESULTS,0.47574626865671643,"6
Evaluation results
199"
EVALUATION RESULTS,0.47761194029850745,"We conduct extensive experiments to demonstrate the advantages of our proposed CT. In this section,
200"
EVALUATION RESULTS,0.47947761194029853,"we discuss how CT compares to SOTA methods under both the automatic and human evaluations as
201"
EVALUATION RESULTS,0.48134328358208955,"well as showing some visualization analysis on its generation probability.
202"
BASELINE COMPARISON,0.4832089552238806,"6.1
Baseline comparison
203"
BASELINE COMPARISON,0.48507462686567165,"The performance comparisons between our CT and the baselines on the language modeling task are
204"
BASELINE COMPARISON,0.4869402985074627,"shown in Table 2. For models, the repetition and diversity results are calculated on model-generated
205"
BASELINE COMPARISON,0.48880597014925375,"continuations of 100 tokens, using 50 tokens of human-created text as the preﬁx. For the human
206"
BASELINE COMPARISON,0.4906716417910448,"performance, we calculate the metrics on trunks of 100 tokens for a fair comparison. The ppl
207"
BASELINE COMPARISON,0.4925373134328358,"metric is for 512-token sequences to comply with the training sequence length. To be comparable to
208"
BASELINE COMPARISON,0.4944029850746269,"existing work [25, 27], we also report ppl-s for short sequences of 50 tokens. We use a sequence
209"
BASELINE COMPARISON,0.4962686567164179,"length of 150 tokens and M = 60 as the negative window size for CT. Justiﬁcations for such hyper-
210"
BASELINE COMPARISON,0.498134328358209,"parameter selections can be found in Appendix F.2.
211"
BASELINE COMPARISON,0.5,"0.0
0.2
0.4
0.6
0.8
1.0
Rep-1 rates 0 1000 2000 3000 4000 5000 6000 7000 8000"
BASELINE COMPARISON,0.5018656716417911,#Examples
BASELINE COMPARISON,0.503731343283582,"SimCTG
CE
NCE
UL-T
CT
UL-TS"
BASELINE COMPARISON,0.5055970149253731,"0.0
0.2
0.4
0.6
0.8
1.0
Rep-4 rates 0 2000 4000 6000 8000 10000 p"
BASELINE COMPARISON,0.5074626865671642,"SimCTG
CE
NCE
UL-T
CT
UL-TS"
BASELINE COMPARISON,0.5093283582089553,"Figure 2: Histograms for rep-1 (left) and rep-4 (right) rates of each method, on the Wikitext-103
test set."
BASELINE COMPARISON,0.5111940298507462,"CT compared to learning-based approaches. One can observe that CT performs the best and
212"
BASELINE COMPARISON,0.5130597014925373,"even outperforms humans according to rep-* rates and unique token counts (uniq-1) when us-
213"
BASELINE COMPARISON,0.5149253731343284,"ing greedy search. However, the repetition problem is still not yet solved, because when looking
214"
BASELINE COMPARISON,0.5167910447761194,"at speciﬁc cases, models trained by CT still occasionally generate texts with excessive repetitions,
215"
BASELINE COMPARISON,0.5186567164179104,"though being much rarer than baseline methods. To see how each method performs at every repe-
216"
BASELINE COMPARISON,0.5205223880597015,"tition level, we group the rep-1 and rep-4 rates of model-generated texts in to 5 bins, and plot
217"
BASELINE COMPARISON,0.5223880597014925,"their histograms in Figure 2, from which we can see that CT generates substantially less degener-
218"
BASELINE COMPARISON,0.5242537313432836,"ated continuations (with rep-1≥0.4 and rep-4≥0.2). For UL-TS, we were able to achieve
219"
BASELINE COMPARISON,0.5261194029850746,"lower repetition rates with a larger learning rate of 1e-5 during training. However, the trained LM
220"
BASELINE COMPARISON,0.5279850746268657,"often generates ungrammatical repetitions. This problem does not exist with CT when trained with
221"
BASELINE COMPARISON,0.5298507462686567,"a learning rate as large as 1e-4. The comparisons are shown in Table 5 in Appendix F, and in §6.3
222"
BASELINE COMPARISON,0.5317164179104478,"we show that this is caused by UL-TS being uncertain about its predictions at later time steps.
223"
BASELINE COMPARISON,0.5335820895522388,"The diversity improvements brought by CT are the largest among all learning-based methods, espe-
224"
BASELINE COMPARISON,0.5354477611940298,"cially when using greedy search. CT increases the second highest uniq-1 count (NCE) by 55%.
225"
BASELINE COMPARISON,0.5373134328358209,"When comparing NCE and UL-T, one can see that utilizing the contrast between positive and neg-
226"
BASELINE COMPARISON,0.539179104477612,"ative tokens works better than solely penalizing negative tokens. The primary difference between
227"
BASELINE COMPARISON,0.5410447761194029,"CT and NCE is that the positive and negative tokens of CT interact with each other, while those
228"
BASELINE COMPARISON,0.542910447761194,"of NCE do not (Table 1, more details in Appendix D). This explains the lower rep-* rates and
229"
BASELINE COMPARISON,0.5447761194029851,"higher diversity of CT, which also concurs with the observation made by Sohn [24] that interactive
230"
BASELINE COMPARISON,0.5466417910447762,"contrastive losses work better than non-interactive counterparts.
231"
BASELINE COMPARISON,0.5485074626865671,"The ppl increase brought by CT is minor, with 0.71 points. When calculated on short sequences,
232"
BASELINE COMPARISON,0.5503731343283582,"due to the length mismatch of training and test sequences, ppl-s scores are higher than ppl for all
233"
BASELINE COMPARISON,0.5522388059701493,"approaches. Among them, contrastive objectives (NCE and CT) have larger ppl-s increases than
234"
BASELINE COMPARISON,0.5541044776119403,"other methods. Although CT has the highest increase on ppl-s, our case study (Table 4) shows
235"
BASELINE COMPARISON,0.5559701492537313,"that the generation quality of CT is not harmed, but on the contrary is improved due to the lower
236"
BASELINE COMPARISON,0.5578358208955224,"repetition and higher diversity of the generated texts.
237"
BASELINE COMPARISON,0.5597014925373134,"CT compared to decoding-based approaches. Although CT is a learning-based method, we still
238"
BASELINE COMPARISON,0.5615671641791045,"compare it against decoding approaches for a more comprehensive understanding of its performance.
239"
BASELINE COMPARISON,0.5634328358208955,"When greedy search is used, CT outperforms the best decoding method (Top-k) in terms of rep-*
240"
BASELINE COMPARISON,0.5652985074626866,"rates, which again proves the effectiveness of contrastive learning. When using beam search, all
241"
BASELINE COMPARISON,0.5671641791044776,"but SimCTG-CS perform signiﬁcantly worse than CT, both in terms of repetition rates and diversity.
242"
BASELINE COMPARISON,0.5690298507462687,"SimCTG-CS is effective at reducing repetition as it explicitly requires a disparity among different
243"
BASELINE COMPARISON,0.5708955223880597,"time steps at inference time. This can harm the generation quality, especially the coherence and
244"
BASELINE COMPARISON,0.5727611940298507,"ﬂuency, as we see in §6.2. It is also worth noting that SimCTG-CS only works together with its
245"
BASELINE COMPARISON,0.5746268656716418,"SimCTG training objective and with beam search [25]. In summary, one can see that the repetition
246"
BASELINE COMPARISON,0.5764925373134329,"problem can be better addressed from the model learning perspective, in which case a simple greedy
247"
BASELINE COMPARISON,0.5783582089552238,"decoding strategy sufﬁces.
248"
HUMAN EVALUATION,0.5802238805970149,"6.2
Human evaluation
249"
HUMAN EVALUATION,0.582089552238806,"Human evaluation results are shown in Table 3. Regarding the overall quality, CT performs signiﬁ-
250"
HUMAN EVALUATION,0.5839552238805971,"cantly better than Top-k and SimCTG-CS, two decoding based approaches. Instead of purely learn-
251"
HUMAN EVALUATION,0.585820895522388,"ing generation policies from data, decoding approaches exert heuristics at inference time, which
252"
HUMAN EVALUATION,0.5876865671641791,"Table 3: Win/lose rates (%) of CT compared to baselines under human evalutaions. For a competi-
tive comparison, we ﬁltered out highly repetitive examples of either model in the pair. * indicates
statistical signiﬁcance as determined with a sign test (p < 0.05)."
HUMAN EVALUATION,0.5895522388059702,"Overall
Repetition
Coherence
Fluency"
HUMAN EVALUATION,0.5914179104477612,"Comparison
Win
Lose
Win
Lose
Win
Lose
Win
Lose"
HUMAN EVALUATION,0.5932835820895522,"CT vs Top-k
58*
36
40*
23
56*
36
45
36
CT vs SimCTG-CS
55*
35
46*
18
52
36
54*
28
CT vs UL-TS
48
43
43
28
39
45
47
38
CT vs Human
27
67*
30
35
23
67*
27
57*"
HUMAN EVALUATION,0.5951492537313433,"may prevent the language model from performing naturally. This explains the worse performance of
253"
HUMAN EVALUATION,0.5970149253731343,"decoding approaches on coherence and ﬂuency. CT performs generally better than UL-TS except on
254"
HUMAN EVALUATION,0.5988805970149254,"coherence, but none of these differences are statistically signiﬁcant. This suggests that CT has a sim-
255"
HUMAN EVALUATION,0.6007462686567164,"ilar generation quality as UL-TS on low-repetitive examples, but CT has much lower repetition rates
256"
HUMAN EVALUATION,0.6026119402985075,"as reported in Table 2. This result is expected, as both CT and UL-TS are learning-based approaches
257"
HUMAN EVALUATION,0.6044776119402985,"for training data-driven models, and on normal cases such as low-repetitive generations, they should
258"
HUMAN EVALUATION,0.6063432835820896,"perform similarly. Compared to human performance, there is still a large margin for machine learn-
259"
HUMAN EVALUATION,0.6082089552238806,"ing models before they have a comparable performance on the language modeling task. Although
260"
HUMAN EVALUATION,0.6100746268656716,"CT performs on par with humans regarding repetition, its generations are far less coherent and ﬂuent
261"
HUMAN EVALUATION,0.6119402985074627,"than those of humans. This may be mitigated by using larger models such as GPT-2 large or GPT-3.
262"
HUMAN EVALUATION,0.6138059701492538,"However, we could not perform such experiments due to a lack of computational resources.
263"
VISUALIZATION ANALYSIS OF THE GENERATION PROBABILITY,0.6156716417910447,"6.3
Visualization analysis of the generation probability
264"
VISUALIZATION ANALYSIS OF THE GENERATION PROBABILITY,0.6175373134328358,"We also conduct analyses to understand the predicted probability of model-generated tokens at infer-
265"
VISUALIZATION ANALYSIS OF THE GENERATION PROBABILITY,0.6194029850746269,"ence time. As shown in Figure 3, diagonal cells represent the probability of generated tokens at the
266"
VISUALIZATION ANALYSIS OF THE GENERATION PROBABILITY,0.621268656716418,"corresponding time steps; off-diagonal cells represent the probability of context tokens. The plots
267"
VISUALIZATION ANALYSIS OF THE GENERATION PROBABILITY,0.6231343283582089,"are averaged over 10 random instances from the test set of Wikitext-103.
268"
VISUALIZATION ANALYSIS OF THE GENERATION PROBABILITY,0.625,"0
10 20 30 40 50 60 70 80 90 CT"
VISUALIZATION ANALYSIS OF THE GENERATION PROBABILITY,0.6268656716417911,"0
10
20
30
40
50
60
70
80
90"
VISUALIZATION ANALYSIS OF THE GENERATION PROBABILITY,0.628731343283582,"0
10 20 30 40 50 60 70 80 90 CE"
VISUALIZATION ANALYSIS OF THE GENERATION PROBABILITY,0.6305970149253731,"0
10
20
30
40
50
60
70
80
90"
VISUALIZATION ANALYSIS OF THE GENERATION PROBABILITY,0.6324626865671642,"0
10 20 30 40 50 60 70 80 90 UL-TS"
VISUALIZATION ANALYSIS OF THE GENERATION PROBABILITY,0.6343283582089553,"0
10
20
30
40
50
60
70
80
90 0.1 0.2 0.3 0.4 0.5"
VISUALIZATION ANALYSIS OF THE GENERATION PROBABILITY,0.6361940298507462,"Figure 3: Heat maps for the generation probability of CT, CE and, UL-TS, at inference time. Row
and column labels represent model-generated tokens at each time step, and the saturation of each
cell represents the corresponding probability of each token. Please refer to §6.3 for a more detailed
description. Heat maps for NCE, UL-T and SimCTG look similar to that of CE, and can be found
in Appendix F, Figure 4."
VISUALIZATION ANALYSIS OF THE GENERATION PROBABILITY,0.6380597014925373,"We have the following key observations from Figure 3: (i) The heat map of CT shows a high variance
269"
VISUALIZATION ANALYSIS OF THE GENERATION PROBABILITY,0.6399253731343284,"in the diagonal, meaning that the model becomes certain and uncertain from time to time. As noted
270"
VISUALIZATION ANALYSIS OF THE GENERATION PROBABILITY,0.6417910447761194,"by Holtzman et al. [6], human-created texts also show such a pattern when fed through pretrained
271"
VISUALIZATION ANALYSIS OF THE GENERATION PROBABILITY,0.6436567164179104,"language models. (ii) In comparison, the heat map for CE shows clear stripes, which stand for
272"
VISUALIZATION ANALYSIS OF THE GENERATION PROBABILITY,0.6455223880597015,"excessive repetition of context n-grams. Besides, the diagonal cells are increasingly darker from
273"
VISUALIZATION ANALYSIS OF THE GENERATION PROBABILITY,0.6473880597014925,"top to bottom, revealing that the language model is becoming more and more certain about its later
274"
VISUALIZATION ANALYSIS OF THE GENERATION PROBABILITY,0.6492537313432836,"predictions, and it seems to positively correlate with the heavier repetition in the later halves of
275"
VISUALIZATION ANALYSIS OF THE GENERATION PROBABILITY,0.6511194029850746,"sequences. (iii) Contrary to CE, the heat map for UL-TS is almost white at the lower and the
276"
VISUALIZATION ANALYSIS OF THE GENERATION PROBABILITY,0.6529850746268657,"right parts of the heat map, indicating the language model is uncertain about any prediction in later
277"
VISUALIZATION ANALYSIS OF THE GENERATION PROBABILITY,0.6548507462686567,"stages, and the generated tokens just win marginally over other candidates. This is expected, since
278"
VISUALIZATION ANALYSIS OF THE GENERATION PROBABILITY,0.6567164179104478,"Table 4: Continuations generated using UL-TS have heavier repetition than those generated using
CT. Greedy search is used. More comparisons to other approaches can be seen in Table 6 in Ap-
pendix F. rep-1"
VISUALIZATION ANALYSIS OF THE GENERATION PROBABILITY,0.6585820895522388,"Preﬁx
the American lobster, H. americanus. It may grow to a length of 60 cm ( 24 in ) and a mass of 6 kilograms ( 13 lb ),
and bears a conspicuous pair of claws. In life, the lobsters are blue,
UL-TS
with a white spot on the underside. The eggs are laid in a single, white sac, and hatch in the spring. The adult is about
1 @.5 2 cm ( 0 @.@ 8 1 @.@ 9 in ) long, and weighs about 1 @.5 2 @.@ 5 kg ( 2 @.5 3 @.@ 8 lb ). The eggs are
laid in a single, white @ brownish @ brown shell, with a white margin 0.55"
VISUALIZATION ANALYSIS OF THE GENERATION PROBABILITY,0.6604477611940298,"CT
yellow, or greenish @-@ brown with short pointed teeth. The male lays eggs on top of the female’s abdomen, which
are incubated by means of tubes attached to the skin. After three weeks, the eggs hatch into adult males. = = Taxonomy
= = The genus H. americanus has been described by several authors since its discovery in 1887. Most notably John
Browning ( 1827 - 1901 ), who wrote about it in his book On Lobsters 0.18"
VISUALIZATION ANALYSIS OF THE GENERATION PROBABILITY,0.6623134328358209,"Preﬁx
He married around 752, and by 757 the couple had had ﬁve children three sons and two daughters but one of the
sons died in infancy in 755. From 754 he
UL-TS
was a prominent ﬁgure in the Church of England, and in 758 he was appointed vicar of St John’s, and in 759 he was
appointed vicar of the Church of Scotland. He died in 761, and was succeeded by his son, John, who died in 762.
= = Early life = = John was born in the parish of St John’s, in the parish of St John’s, in the parish of St John’s
parish, 0.61"
VISUALIZATION ANALYSIS OF THE GENERATION PROBABILITY,0.664179104477612,"CT
continued to live at St. John’s Church, near Bath, where he received instruction from William de Montfort on how
to build a dam for irrigation purposes. The ﬁrst mention of this work came in 757 when it was discovered that a
large portion of the earth beneath the riverbed had been washed away by ﬂoods caused by wind gusts. This led to
speculation that it might be connected to the Norman invasion of England. In 758, however, Henry VIII granted
permission for construction of a 0.21"
VISUALIZATION ANALYSIS OF THE GENERATION PROBABILITY,0.6660447761194029,"UL-TS penalizes repetitions unilaterally, and repetitions are more common in the later half of a
279"
VISUALIZATION ANALYSIS OF THE GENERATION PROBABILITY,0.667910447761194,"model-generated sequence. Even though UL-TS is able to effectively reduce repetition rates, its
280"
VISUALIZATION ANALYSIS OF THE GENERATION PROBABILITY,0.6697761194029851,"heat map shows that the language model trained by UL-TS may subject to frequent grammatical
281"
VISUALIZATION ANALYSIS OF THE GENERATION PROBABILITY,0.6716417910447762,"errors, as can be seen in Appendix F, Table 5.
282"
CASE STUDY,0.6735074626865671,"6.4
Case study
283"
CASE STUDY,0.6753731343283582,"To intuitively see how well CT performs, we selected some example generations of CT, and compare
284"
CASE STUDY,0.6772388059701493,"them with those generated using UL-TS in Table 4. More often than not, continuations generated by
285"
CASE STUDY,0.6791044776119403,"CT are less repetitive and make more sense than those generated by UL-TS. The reason for the poor
286"
CASE STUDY,0.6809701492537313,"quality of UL-TS is that sequence-level unlikelihood training penalizes repeated 4-grams generated
287"
CASE STUDY,0.6828358208955224,"by LMs, making LMs uncertain about their predictions as suggested in Figure 3.
288"
CONCLUSION AND DISCUSSION,0.6847014925373134,"7
Conclusion and discussion
289"
CONCLUSION AND DISCUSSION,0.6865671641791045,"In this paper we studied the neural text degeneration problem. By integrating the best of cross-
290"
CONCLUSION AND DISCUSSION,0.6884328358208955,"entropy and unlikelihood training objectives, we obtain a simple and effective contrastive token
291"
CONCLUSION AND DISCUSSION,0.6902985074626866,"learning (CT) framework. The main novelty of this work is adapting contrastive learning to the
292"
CONCLUSION AND DISCUSSION,0.6921641791044776,"token level of autoregressive language model training. As far as we are aware, our work is the ﬁrst
293"
CONCLUSION AND DISCUSSION,0.6940298507462687,"to use model hidden states as the anchor points and tokens as the positive and negative examples to
294"
CONCLUSION AND DISCUSSION,0.6958955223880597,"formulate the contrastive loss. By contrasting the preceding M tokens at a training step with the label
295"
CONCLUSION AND DISCUSSION,0.6977611940298507,"token, LMs learn to not repeat such tokens, thus alleviating the repetition problem. Although the idea
296"
CONCLUSION AND DISCUSSION,0.6996268656716418,"of negative tokens is similar to UL, our formulation of contrastive objective is more effective and
297"
CONCLUSION AND DISCUSSION,0.7014925373134329,"safer to use. Experiments on the open-ended text generation and open-domain dialogue generation
298"
CONCLUSION AND DISCUSSION,0.7033582089552238,"tasks show that CT beats UL-TS, the previous state-of-the-art approach to tackling the repetitive text
299"
CONCLUSION AND DISCUSSION,0.7052238805970149,"degeneration problem. CT not only achieves the lowest repetition rates and the highest generation
300"
CONCLUSION AND DISCUSSION,0.707089552238806,"diversity, but also higher generation quality according to our human evaluation.
301"
CONCLUSION AND DISCUSSION,0.7089552238805971,"We performed experiments on ﬁne-tuning LMs for reducing their repetition rates, which can be
302"
CONCLUSION AND DISCUSSION,0.710820895522388,"beneﬁcial for related tasks such as abstractive summarization, machine translation, and image cap-
303"
CONCLUSION AND DISCUSSION,0.7126865671641791,"tioning. Our early experiments show that CT can be safely integrated when training a language
304"
CONCLUSION AND DISCUSSION,0.7145522388059702,"model from scratch, which can be helpful for future pre-training of large language models. In this
305"
CONCLUSION AND DISCUSSION,0.7164179104477612,"work, we used CT with decoder-only (GPT2) and encoder-decoder (BlenderBot) language models,
306"
CONCLUSION AND DISCUSSION,0.7182835820895522,"but we note that CT can also be used with encoder language models (e.g., BERT [26]) to potentially
307"
CONCLUSION AND DISCUSSION,0.7201492537313433,"improve the model performance such as prediction accuracy. The repetitive degeneration problem
308"
CONCLUSION AND DISCUSSION,0.7220149253731343,"is still not fully solved as occasional, excessive phrase repetitions remain in the generated texts. We
309"
CONCLUSION AND DISCUSSION,0.7238805970149254,"leave these research directions as future work.
310"
REFERENCES,0.7257462686567164,"References
311"
REFERENCES,0.7276119402985075,"[1] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
312"
REFERENCES,0.7294776119402985,"wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-
313"
REFERENCES,0.7313432835820896,"wal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,
314"
REFERENCES,0.7332089552238806,"Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler,
315"
REFERENCES,0.7350746268656716,"Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCan-
316"
REFERENCES,0.7369402985074627,"dlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot
317"
REFERENCES,0.7388059701492538,"learners. In Advances in Neural Information Processing Systems 33: Annual Conference on
318"
REFERENCES,0.7406716417910447,"Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.
319"
REFERENCES,0.7425373134328358,"[2] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. 2020. A sim-
320"
REFERENCES,0.7444029850746269,"ple framework for contrastive learning of visual representations. In Proceedings of the 37th
321"
REFERENCES,0.746268656716418,"International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event,
322"
REFERENCES,0.7481343283582089,"volume 119 of Proceedings of Machine Learning Research, pages 1597–1607. PMLR.
323"
REFERENCES,0.75,"[3] Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020. ELECTRA:
324"
REFERENCES,0.7518656716417911,"pre-training text encoders as discriminators rather than generators. In 8th International Con-
325"
REFERENCES,0.753731343283582,"ference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.
326"
REFERENCES,0.7555970149253731,"OpenReview.net.
327"
REFERENCES,0.7574626865671642,"[4] Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In
328"
REFERENCES,0.7593283582089553,"ACL, pages 889–898.
329"
REFERENCES,0.7611940298507462,"[5] Michael Gutmann and Aapo Hyvärinen. 2010. Noise-contrastive estimation: A new estima-
330"
REFERENCES,0.7630597014925373,"tion principle for unnormalized statistical models. In Proceedings of the thirteenth interna-
331"
REFERENCES,0.7649253731343284,"tional conference on artiﬁcial intelligence and statistics, pages 297–304. JMLR Workshop and
332"
REFERENCES,0.7667910447761194,"Conference Proceedings.
333"
REFERENCES,0.7686567164179104,"[6] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of
334"
REFERENCES,0.7705223880597015,"neural text degeneration. In 8th International Conference on Learning Representations, ICLR
335"
REFERENCES,0.7723880597014925,"2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.
336"
REFERENCES,0.7742537313432836,"[7] Shaojie Jiang and Maarten de Rijke. 2018. Why are sequence-to-sequence models so dull? un-
337"
REFERENCES,0.7761194029850746,"derstanding the low-diversity problem of chatbots. In Proceedings of the 2018 EMNLP Work-
338"
REFERENCES,0.7779850746268657,"shop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI. ACL.
339"
REFERENCES,0.7798507462686567,"[8] Shaojie Jiang, Pengjie Ren, Christof Monz, and Maarten de Rijke. 2019. Improving neural
340"
REFERENCES,0.7817164179104478,"response diversity with frequency-aware cross-entropy loss. In The Web Conference 2019,
341"
REFERENCES,0.7835820895522388,"pages 2879–2885. ACM.
342"
REFERENCES,0.7854477611940298,"[9] Shaojie Jiang, Thomas Wolf, Christof Monz, and Maarten de Rijke. 2020. TLDR: token loss
343"
REFERENCES,0.7873134328358209,"dynamic reweighting for reducing repetitive utterance generation. CoRR, abs/2003.11963.
344"
REFERENCES,0.789179104477612,"[10] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola,
345"
REFERENCES,0.7910447761194029,"Aaron Maschinot, Ce Liu, and Dilip Krishnan. 2020. Supervised contrastive learning. In
346"
REFERENCES,0.792910447761194,"Advances in Neural Information Processing Systems 33: Annual Conference on Neural Infor-
347"
REFERENCES,0.7947761194029851,"mation Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.
348"
REFERENCES,0.7966417910447762,"[11] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. In
349"
REFERENCES,0.7985074626865671,"Proceedings of the 3rd International Conference on Learning Representations (ICLR).
350"
REFERENCES,0.8003731343283582,"[12] Seanie Lee, Dong Bok Lee, and Sung Ju Hwang. 2021. Contrastive learning with adversarial
351"
REFERENCES,0.8022388059701493,"perturbations for conditional text generation. In 9th International Conference on Learning
352"
REFERENCES,0.8041044776119403,"Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.
353"
REFERENCES,0.8059701492537313,"[13] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2016. A diversity-
354"
REFERENCES,0.8078358208955224,"promoting objective function for neural conversation models.
In Proceedings of the 2016
355"
REFERENCES,0.8097014925373134,"Conference of the North American Chapter of the Association for Computational Linguistics:
356"
REFERENCES,0.8115671641791045,"Human Language Technologies, pages 110–119.
357"
REFERENCES,0.8134328358208955,"[14] Yu Meng, Chenyan Xiong, Payal Bajaj, Saurabh Tiwary, Paul Bennett, Jiawei Han, and Xia
358"
REFERENCES,0.8152985074626866,"Song. 2021. COCO-LM: correcting and contrasting text sequences for language model pre-
359"
REFERENCES,0.8171641791044776,"training. Advances in Neural Information Processing Systems, abs/2102.08473.
360"
REFERENCES,0.8190298507462687,"[15] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2017. Pointer sentinel
361"
REFERENCES,0.8208955223880597,"mixture models. In 5th International Conference on Learning Representations, ICLR 2017,
362"
REFERENCES,0.8227611940298507,"Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net.
363"
REFERENCES,0.8246268656716418,"[16] Tomás Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efﬁcient estimation of word
364"
REFERENCES,0.8264925373134329,"representations in vector space. In 1st International Conference on Learning Representations,
365"
REFERENCES,0.8283582089552238,"ICLR 2013, Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings.
366"
REFERENCES,0.8302238805970149,"[17] Thong Nguyen and Anh Tuan Luu. 2021. Contrastive learning for neural topic model. CoRR,
367"
REFERENCES,0.832089552238806,"abs/2110.12764.
368"
REFERENCES,0.8339552238805971,"[18] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin,
369"
REFERENCES,0.835820895522388,"Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language
370"
REFERENCES,0.8376865671641791,"models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.
371"
REFERENCES,0.8395522388059702,"[19] Xiao Pan, Mingxuan Wang, Liwei Wu, and Lei Li. 2021. Contrastive learning for many-to-
372"
REFERENCES,0.8414179104477612,"many multilingual neural machine translation. In Proceedings of the 59th Annual Meeting of
373"
REFERENCES,0.8432835820895522,"the Association for Computational Linguistics and the 11th International Joint Conference on
374"
REFERENCES,0.8451492537313433,"Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event,
375"
REFERENCES,0.8470149253731343,"August 1-6, 2021, pages 244–258. Association for Computational Linguistics.
376"
REFERENCES,0.8488805970149254,"[20] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019.
377"
REFERENCES,0.8507462686567164,"Language models are unsupervised multitask learners.
378"
REFERENCES,0.8526119402985075,"[21] Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu,
379"
REFERENCES,0.8544776119402985,"Myle Ott, Eric Michael Smith, Y-Lan Boureau, and Jason Weston. 2021. Recipes for building
380"
REFERENCES,0.8563432835820896,"an open-domain chatbot. In Proceedings of the 16th Conference of the European Chapter of
381"
REFERENCES,0.8582089552238806,"the Association for Computational Linguistics: Main Volume, EACL 2021, Online, April 19 -
382"
REFERENCES,0.8600746268656716,"23, 2021, pages 300–325. Association for Computational Linguistics.
383"
REFERENCES,0.8619402985074627,"[22] Florian Schroff, Dmitry Kalenichenko, and James Philbin. 2015. Facenet: A uniﬁed embed-
384"
REFERENCES,0.8638059701492538,"ding for face recognition and clustering. In IEEE Conference on Computer Vision and Pattern
385"
REFERENCES,0.8656716417910447,"Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages 815–823. IEEE Computer
386"
REFERENCES,0.8675373134328358,"Society.
387"
REFERENCES,0.8694029850746269,"[23] Abigail See, Stephen Roller, Douwe Kiela, and Jason Weston. 2019. What makes a good con-
388"
REFERENCES,0.871268656716418,"versation? how controllable attributes affect human judgments. In Proceedings of the 2019
389"
REFERENCES,0.8731343283582089,"Conference of the North American Chapter of the Association for Computational Linguistics:
390"
REFERENCES,0.875,"Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019,
391"
REFERENCES,0.8768656716417911,"Volume 1 (Long and Short Papers), pages 1702–1723. Association for Computational Linguis-
392"
REFERENCES,0.878731343283582,"tics.
393"
REFERENCES,0.8805970149253731,"[24] Kihyuk Sohn. 2016. Improved deep metric learning with multi-class n-pair loss objective.
394"
REFERENCES,0.8824626865671642,"In Advances in Neural Information Processing Systems 29: Annual Conference on Neural
395"
REFERENCES,0.8843283582089553,"Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 1849–
396"
REFERENCES,0.8861940298507462,"1857.
397"
REFERENCES,0.8880597014925373,"[25] Yixuan Su, Tian Lan, Yan Wang, Dani Yogatama, Lingpeng Kong, and Nigel Collier. 2022. A
398"
REFERENCES,0.8899253731343284,"contrastive framework for neural text generation. CoRR, abs/2202.06417.
399"
REFERENCES,0.8917910447761194,"[26] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
400"
REFERENCES,0.8936567164179104,"Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In NeurIPS, pages 6000–
401"
REFERENCES,0.8955223880597015,"6010.
402"
REFERENCES,0.8973880597014925,"[27] Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston.
403"
REFERENCES,0.8992537313432836,"2020. Neural text generation with unlikelihood training. In 8th International Conference on
404"
REFERENCES,0.9011194029850746,"Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenRe-
405"
REFERENCES,0.9029850746268657,"view.net.
406"
REFERENCES,0.9048507462686567,"[28] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and
407"
REFERENCES,0.9067164179104478,"Quoc V. Le. 2019. Xlnet: Generalized autoregressive pretraining for language understand-
408"
REFERENCES,0.9085820895522388,"ing. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural
409"
REFERENCES,0.9104477611940298,"Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC,
410"
REFERENCES,0.9123134328358209,"Canada, pages 5754–5764.
411"
REFERENCES,0.914179104477612,"[29] Zonghan Yang, Yong Cheng, Yang Liu, and Maosong Sun. 2019. Reducing word omission er-
412"
REFERENCES,0.9160447761194029,"rors in neural machine translation: A contrastive learning approach. In Proceedings of the 57th
413"
REFERENCES,0.917910447761194,"Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July
414"
REFERENCES,0.9197761194029851,"28- August 2, 2019, Volume 1: Long Papers, pages 6191–6196. Association for Computational
415"
REFERENCES,0.9216417910447762,"Linguistics.
416"
REFERENCES,0.9235074626865671,"[30] Tong Zhang, Wei Ye, Baosong Yang, Long Zhang, Xingzhang Ren, Dayiheng Liu, Jinan Sun,
417"
REFERENCES,0.9253731343283582,"Shikun Zhang, Haibo Zhang, and Wen Zhao. 2021. Frequency-aware contrastive learning for
418"
REFERENCES,0.9272388059701493,"neural machine translation. CoRR, abs/2112.14484.
419"
REFERENCES,0.9291044776119403,"Checklist
420"
REFERENCES,0.9309701492537313,"1. For all authors...
421"
REFERENCES,0.9328358208955224,"(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s
422"
REFERENCES,0.9347014925373134,"contributions and scope? [Yes]
423"
REFERENCES,0.9365671641791045,"(b) Did you describe the limitations of your work? [Yes] See Section 7.
424"
REFERENCES,0.9384328358208955,"(c) Did you discuss any potential negative societal impacts of your work? [Yes] See
425"
REFERENCES,0.9402985074626866,"Appendix A.
426"
REFERENCES,0.9421641791044776,"(d) Have you read the ethics review guidelines and ensured that your paper conforms to
427"
REFERENCES,0.9440298507462687,"them? [Yes] See Appendix A.
428"
REFERENCES,0.9458955223880597,"2. If you are including theoretical results...
429"
REFERENCES,0.9477611940298507,"(a) Did you state the full set of assumptions of all theoretical results? [N/A]
430"
REFERENCES,0.9496268656716418,"(b) Did you include complete proofs of all theoretical results? [N/A]
431"
REFERENCES,0.9514925373134329,"3. If you ran experiments...
432"
REFERENCES,0.9533582089552238,"(a) Did you include the code, data, and instructions needed to reproduce the main experi-
433"
REFERENCES,0.9552238805970149,"mental results (either in the supplemental material or as a URL)? [Yes] See Section 1.
434"
REFERENCES,0.957089552238806,"(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
435"
REFERENCES,0.9589552238805971,"were chosen)? [Yes] See Section 5, the README ﬁle in our source code (link or the
436"
REFERENCES,0.960820895522388,".zip ﬁle in our supplementary material) and Appendix F.2.
437"
REFERENCES,0.9626865671641791,"(c) Did you report error bars (e.g., with respect to the random seed after running experi-
438"
REFERENCES,0.9645522388059702,"ments multiple times)? [No] We train the models by ﬁnetuning, and we observed them
439"
REFERENCES,0.9664179104477612,"to be insensitive to different random seeds.
440"
REFERENCES,0.9682835820895522,"(d) Did you include the total amount of compute and the type of resources used (e.g., type
441"
REFERENCES,0.9701492537313433,"of GPUs, internal cluster, or cloud provider)? [Yes] See Section 5.
442"
REFERENCES,0.9720149253731343,"4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
443"
REFERENCES,0.9738805970149254,"(a) If your work uses existing assets, did you cite the creators? [Yes] See Appendix E.
444"
REFERENCES,0.9757462686567164,"(b) Did you mention the license of the assets? [Yes] See Appendix E.
445"
REFERENCES,0.9776119402985075,"(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]
446 447"
REFERENCES,0.9794776119402985,"(d) Did you discuss whether and how consent was obtained from people whose data
448"
REFERENCES,0.9813432835820896,"you’re using/curating? [N/A] We only used public and credible datasets in this work.
449"
REFERENCES,0.9832089552238806,"(e) Did you discuss whether the data you are using/curating contains personally identiﬁ-
450"
REFERENCES,0.9850746268656716,"able information or offensive content? [Yes] Please see Appendix A.
451"
REFERENCES,0.9869402985074627,"5. If you used crowdsourcing or conducted research with human subjects...
452"
REFERENCES,0.9888059701492538,"(a) Did you include the full text of instructions given to participants and screenshots, if
453"
REFERENCES,0.9906716417910447,"applicable? [Yes] Please see Appendix F.
454"
REFERENCES,0.9925373134328358,"(b) Did you describe any potential participant risks, with links to Institutional Review
455"
REFERENCES,0.9944029850746269,"Board (IRB) approvals, if applicable? [N/A]
456"
REFERENCES,0.996268656716418,"(c) Did you include the estimated hourly wage paid to participants and the total amount
457"
REFERENCES,0.9981343283582089,"spent on participant compensation? [Yes] See Appendix A.
458"
