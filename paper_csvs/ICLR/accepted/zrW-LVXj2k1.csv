Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0012468827930174563,"We advocate for a practical Maximum Likelihood Estimation (MLE) approach
towards designing loss functions for regression and forecasting, as an alternative
to the typical approach of direct empirical risk minimization on a speciﬁc target
metric. The MLE approach is better suited to capture inductive biases such as prior
domain knowledge in datasets, and can output post-hoc estimators at inference time
that can optimize different types of target metrics. We present theoretical results
to demonstrate that our approach is competitive with any estimator for the target
metric under some general conditions. In two example practical settings, Poisson
and Pareto regression, we show that our competitive results can be used to prove
that the MLE approach has better excess risk bounds than directly minimizing
the target metric. We also demonstrate empirically that our method instantiated
with a well-designed general purpose mixture likelihood family can obtain superior
performance for a variety of tasks across time-series forecasting and regression
datasets with different data distributions."
INTRODUCTION,0.0024937655860349127,"1
INTRODUCTION"
INTRODUCTION,0.003740648379052369,"The task of ﬁtting a regression model for a response variable y against a covariate vector x ∈Rd
is ubiquitous in supervised learning in both linear and non-linear settings (Lathuilière et al., 2020;
Mohri et al., 2018) as well as non-i.i.d settings like multi-variate forecasting (Salinas et al., 2020;
Wang et al., 2019). The end goal in regression and forecasting problems is often to use the resulting
model to obtain good performance in terms of some target metric of interest on the population level
(usually measured on a previously unseen test set). The mean-squared error or the mean absolute
deviation are examples of common target metrics."
INTRODUCTION,0.004987531172069825,"In this paper, our focus is on the choice of loss function that is used to train such models, which is an
important question that is often overlooked, especially in the deep neural networks context where the
emphasis is more on the choice of network architecture (Lathuilière et al., 2020)."
INTRODUCTION,0.006234413965087282,"Perhaps the most common method used by practitioners for choosing the loss function for a particular
regression model is to directly use the target metric of interest as the loss function for empirical risk
minimization (ERM) over a function class on the training set. We denote this approach for choosing a
loss function as Target Metric Optimization (TMO). This is especially more common with the advent
of powerful general purpose function optimizers like deep networks and has also been rigorously
analyzed for simpler function classes (Mohri et al., 2018)."
INTRODUCTION,0.007481296758104738,"Target Metric Optimization seems like a reasonable approach - if the practitioner knows about the
target metric of interest for prediction using the model, it seems intuitive to optimize for the same
objective on the training data. Prior work (both theoretical and applied) has both advocated for and
argued against TMO for regression problems. Many prominent works on regression (Goldberger
et al., 1964; Lecue & Mendelson, 2016) use the TMO approach, though most of them assume that
the data is well behaved (e.g. sub-Gaussian noise). In terms of applications, many recent works on
time-series forecasting (Wu et al., 2020; Oreshkin et al., 2019; Sen et al., 2019) also use the TMO
approach directly on the target metric. On the other hand, the robust regression literature has long
advocated for not using the target metric directly for ERM in the case of contamination or heavy
tailed response/covariate behaviour (Huber, 1992; Hsu & Sabato, 2016; Zhu & Zhou, 2021; Lugosi
& Mendelson, 2019a; Audibert et al., 2011; Brownlees et al., 2015) on account of its suboptimal"
INTRODUCTION,0.008728179551122194,Published as a conference paper at ICLR 2022
INTRODUCTION,0.00997506234413965,"high-probability risk bounds. However, as noted in (Prasad et al., 2020), many of these methods
are either not practical (Lugosi & Mendelson, 2019a; Brownlees et al., 2015) or have sub-optimal
empirical performance (Hsu & Sabato, 2016). Even more practical methods such as (Prasad et al.,
2020) would lead to sufﬁciently more computational overhead over standard TMO."
INTRODUCTION,0.011221945137157107,"Another well known approach for designing a loss function is Maximum Likelihood Estimation (MLE).
Here one assumes that the conditional distribution of y given x belongs to a family of distributions
p(y|x; θ) parameterized by θ ∈Θ (McCullagh & Nelder, 2019). Then one can choose the negative
log likelihood as the loss function to optimize using the training set, to obtain an estimate ˆθθθmle. This
approach is sometimes used in the forecasting literature (Salinas et al., 2020; Davis & Wu, 2009)
where the choice of a likelihood can encode prior knowledge about the data. For instance a negative
binomial distribution can be used to model count data. During inference, given a new instance x′,
one can output the statistic from p(y|x′; ˆθθθmle) that optimizes the target metric, as the prediction
value (Gneiting, 2011). MLE also seems like a reasonable approach for loss function design - it is
folklore that the MLE is asymptotically optimal for parameter estimation, in terms of having the
smallest asymptotic variance among all estimators (Heyde, 1978; Rao, 1963), when the likelihood is
well-speciﬁed. However, much less is known about ﬁnite-sample, ﬁxed-dimension analysis of MLE,
which is the typical regime of interest for the regression problems we consider in this paper. An
important practical advantage for MLE is that model training is agnostic to the choice of the target
metric - the same trained model can output estimators for different target metrics at inference time.
Perhaps the biggest argument against the MLE approach is the requirement of knowing the likelihood
distribution family. We address both these topics in Section 5."
INTRODUCTION,0.012468827930174564,"Both TMO and MLE can be viewed as offering different approaches to selecting the loss function
for a given regression model. In this paper, we argue that in several settings, both from a practical
and theoretical perspective, MLE might be a better approach than TMO. This result might not be
immediately obvious apriori - while MLE does beneﬁt from prior knowledge of the distribution,
TMO also beneﬁts from prior knowledge of the target metric at training time."
INTRODUCTION,0.01371571072319202,Our main contributions are as follows:
INTRODUCTION,0.014962593516209476,"Competitiveness of MLE: In Section 3, we prove that under some general conditions on the family
of distributions and a property of interest, the MLE approach is competitive with any estimator for
the property. We show that this result can be applied to ﬁxed design regression problems in order to
prove that MLE can be competitive (up to logarithmic terms) with any estimator in terms of excess
square loss risk, under some assumptions."
INTRODUCTION,0.016209476309226933,"Example Applications: In Section 4.1, we apply our general theorem to prove an excess square loss
bound for the the MLE estimator for Poisson regression with the identity link (Nelder & Wedderburn,
1972; Lawless, 1987). We show that these bounds can be better than those of the TMO estimator,
which in this case is least-squares regression. Then in Section 4.2, we show a similar application
in the context of Pareto regression i.e y|x follows a Pareto distribution. We show that MLE can be
competitive with robust estimators like the one in (Hsu & Sabato, 2016) and therefore can be better
than TMO (least-squares)."
INTRODUCTION,0.017456359102244388,"Empirical Results:
We propose the use of a general purpose mixture likelihood family (see
Section 5) that can capture a wide variety of prior knowledge across datasets, including zero-inﬂated
or bursty data, count data, sub-Gaussian continuous data as well as heavy tailed data, through different
choices of (learnt) mixture weights. Then we empirically show that the MLE approach with this
likelihood can outperform ERM for many different commonly used target metrics like WAPE, MAPE
and RMSE (Hyndman & Koehler, 2006) for two popular forecasting and two regression datasets.
Moreover the MLE approach is also shown to have better probabilistic forecasts (measured by
quantile losses (Wen et al., 2017)) than quantile regression (Koenker & Bassett Jr, 1978; Gasthaus
et al., 2019; Wen et al., 2017) which is the TMO approach in this case."
PRIOR WORK ON MLE,0.018703241895261846,"2
PRIOR WORK ON MLE"
PRIOR WORK ON MLE,0.0199501246882793,"Maximum likelihood estimators (MLE) have been studied extensively in statistics starting with
the work of Wald (1949); Redner (1981), who showed the maximum likelihood estimates are
asymptotically consistent for parametric families. Fahrmeir & Kaufmann (1985) showed asymptotic"
PRIOR WORK ON MLE,0.02119700748129676,Published as a conference paper at ICLR 2022
PRIOR WORK ON MLE,0.022443890274314215,"normality for MLE for generalized linear models. It is also known that under some regularity
assumptions, MLE achieves the Cramer-Rao lower bound asymptotically (Lehmann & Casella, 2006).
However, we note that none of these asymptotic results directly yield ﬁnite sample guarantees."
PRIOR WORK ON MLE,0.02369077306733167,"Finite sample guarantees have been shown for certain problem scenarios. Geer & van de Geer (2000);
Zhang (2006) provided uniform convergence bounds in Hellinger distance for maximum likelihood
estimation. These ideas were recently used by Foster & Krishnamurthy (2021) to provide algorithms
for contextual bandits. There are other works which study MLE for non-parametric distributions e.g.,
Dümbgen & Ruﬁbach (2009) showed convergence rates for log-concave distributions. There has
been some works (Sur & Candès, 2019; Bean et al., 2013; Donoho & Montanari, 2016; El Karoui,
2018) that show that MLE can be sub-optimal for high dimensional regression i.e when the dimension
grows with the number of samples. In this work we focus on the setting where the dimension does
not scale with the number of samples."
PRIOR WORK ON MLE,0.02493765586034913,"Our MLE results differ from the above work as we provide ﬁnite sample competitiveness guarantees.
Instead of showing that the maximum likelihood estimator converges in some distance metric, we
show that under some mild assumptions it can work as well as other estimators. Hence, our methods
are orthogonal to known well established results in statistics."
PRIOR WORK ON MLE,0.026184538653366583,"Perhaps the closest to our work is the competitiveness result of Acharya et al. (2017), who showed
that MLE is competitive when the size of the output alphabet is bounded and applied to proﬁle
maximum likelihood estimation. In contrast, our work applies for unbounded output alphabets and
can provide stronger guarantees in many scenarios."
COMPETITIVENESS OF MLE,0.02743142144638404,"3
COMPETITIVENESS OF MLE"
COMPETITIVENESS OF MLE,0.028678304239401497,"In this section, we will show that under some reasonable assumptions on the likelihood family, the
MLE is competitive with any estimator in terms of estimating any property of a distribution from the
family. We will then show that this result can be applied to derive bounds on the MLE in some ﬁxed
design regression settings that can be better than that of TMO. We will ﬁrst setup some notation."
COMPETITIVENESS OF MLE,0.029925187032418952,"Notation: Given a positive semi-deﬁnite symmetric matrix M, ∥x∥M := xT Mx is the matrix
norm of the vector x. λ(M) denotes an eigenvalue of a symmetric square matrix M; speciﬁcally
λmax(M) and λmin(M) denote the maximum and minimum eigenvalues respectively. The letter f
is used to denote general probability densities. We use p to denote the conditional probability density
of the response given the covariate. ∥·∥1 will be overloaded to denote the ℓ1 norm between two
probability distributions i.e ∥p −p′∥1 :=
R
|p(z) −p′(z)|dz. DKL(p1; p2) will be used to denote the
KL-divergence between the two distributions. If Z is a set equipped with a norm ∥·∥, then N(ϵ, Z)
will denote an ϵ-net i.e any point z ∈Z has a corresponding point z′ ∈N(ϵ, Z) s.t ∥z −z′∥≤ϵ.
Bd
r denotes the ball centered at the origin with radius r and Sd−1
r
denotes its surface. We deﬁne
[n] := {1, 2, · · · , n}. | · | denotes the size of the enclosed set."
COMPETITIVENESS OF MLE,0.03117206982543641,"General Competitiveness:
We ﬁrst consider a general family of distributions F over the space
Z. For a sample z ∼f (for z ∈Z and f ∈F), the MLE distribution is deﬁned as fz =
argmaxf∈F f(z). We are interested in estimating a property π : F →W of these distributions from
an observed sample. The following deﬁnition will be required to impose some joint conditions on the
distribution family and the property being estimated, that are needed for our result.
Deﬁnition 1. The tuple (F, π), where F is a set of distributions and π : F →W a property
of those distributions, is said to be (T, ϵ, δ1, δ2)-approximable, if there exists a set of distribu-
tions ˜F ⊆F s.t | ˜F| ≤T and for every f ∈F, there exists a ˜f such that ∥f −˜f∥1 ≤δ1 and"
COMPETITIVENESS OF MLE,0.032418952618453865,"Prz∼f
π(fz) −π( ˜fz)
 ≥ϵ

≤δ2, where ˜fz = argmax ˜
f∈˜
F ˜f(z) and W has a norm ∥·∥."
COMPETITIVENESS OF MLE,0.03366583541147132,"The above deﬁnition states that the set of distributions F has a ﬁnite δ-cover, ˜F in terms of the ℓ1
distance. Moreover the cover is such that solving MLE on the cover and applying the property π on
the result of the MLE is not too far from π applied on the MLE over the whole set F. This property
is satisﬁed trivially if F is ﬁnite. We note that it is also satisﬁed by some commonly used set of
distributions and corresponding properties. Now we state our main result.
Theorem 1. Let ˆπ be an estimator such that for any f ∈F and z ∼f, Pr(∥π(f) −ˆπ(z)∥≥ϵ) ≤δ.
Let Ff be a subset of F that contains f such that with probability at least 1−δ, fz ∈Ff and (Ff, π)"
COMPETITIVENESS OF MLE,0.034912718204488775,Published as a conference paper at ICLR 2022
COMPETITIVENESS OF MLE,0.03615960099750624,"is (T, ϵ, δ1, δ2)-approximable. Then the MLE satisﬁes the following bound,"
COMPETITIVENESS OF MLE,0.03740648379052369,Pr(∥π(f) −π(fz)∥≥3ϵ) ≤(T + 3)δ + δ1 + δ2.
COMPETITIVENESS OF MLE,0.03865336658354115,"We provide the proof of Theorem 1 in Appendix A. We also provide a simpler version of this result
for ﬁnite distribution families as Theorem 3 in Appendix A for the beneﬁt of the reader."
COMPETITIVENESS OF MLE,0.0399002493765586,"Competitiveness in Fixed Design Regression:
Theorem 1 can be used to show that MLE is
competitive with respect to any estimator for square loss minimization in ﬁxed design regression. We
will ﬁrst formally introduce the setting. Consider a ﬁxed design matrix X ∈Rn×d where n is the
number of samples and d the feature dimension. We will work in a setting where n ≫d. The target
vector is a random vector given by yn ∈Rn. Let yi be the i-th coordinate of yn and xi denote the
i-th row of the design matrix. We assume that the target is generated from the conditional distribution
given xi such that,"
COMPETITIVENESS OF MLE,0.04114713216957606,"yi ∼p(·|xi;θθθ∗), θθθ∗∈Θ."
COMPETITIVENESS OF MLE,0.04239401496259352,"We are interested in optimizing a target metric ℓ(·, ·) given an instance of the random vector yn. The
ﬁnal objective is to optimize,"
COMPETITIVENESS OF MLE,0.043640897755610975,"min
h∈H Eyi∼p(·|xi;θθθ∗) ""
1
n n
X"
COMPETITIVENESS OF MLE,0.04488778054862843,"i=1
ℓ(yi, h(xi)) # ,"
COMPETITIVENESS OF MLE,0.046134663341645885,"where H is a class of functions. In this context, we are interested in comparing two approaches."
COMPETITIVENESS OF MLE,0.04738154613466334,"TMO (see (Mohri et al., 2018)).
This is standard empirical risk minimization on the tar-
get metric where given an instance of the random vector yn one outputs the estimator ˆh =
minh∈H 1"
COMPETITIVENESS OF MLE,0.048628428927680795,"n
Pn
i=1 ℓ(yi, h(xi))."
COMPETITIVENESS OF MLE,0.04987531172069826,"MLE and post-hoc inference (see (Gneiting, 2011)). In this method one ﬁrst solves for the parameter
in the distribution family that best explains the empirical data by MLE i.e.,"
COMPETITIVENESS OF MLE,0.05112219451371571,"ˆθθθmle := argmin
θ∈Θ
L(yn; θ), where L(yn; θ) := n
X"
COMPETITIVENESS OF MLE,0.05236907730673317,"i=1
−log p(yi|xi; θ)"
COMPETITIVENESS OF MLE,0.05361596009975062,"Then
during
inference
given
a
sample
xi
the
predictor
is
deﬁned
as,
˜h(xi)
:=
argminˆy Ey∈p(·|xi;ˆθθθmle)[ℓ(y, ˆy)] or in other words we output the statistic from the MLE distribu-"
COMPETITIVENESS OF MLE,0.05486284289276808,"tion that optimizes the loss function of interest. For instance if ℓis the square loss, then ˜h(xi) will be
the mean of the conditional distribution p(y|xi; ˆθθθmle)."
COMPETITIVENESS OF MLE,0.05610972568578554,"We will prove a general result using Theorem 1 when the target metric ℓis the square loss and H is a
linear function class. Moreover, the true distribution p(·|xi;θθθ∗) is such that E[yi] = ⟨θθθ∗, xi⟩for all
i ∈[n] i.e we are in the linear realizable setting."
COMPETITIVENESS OF MLE,0.057356608478802994,"In this case the quantity of interest is the excess square loss risk given by,"
COMPETITIVENESS OF MLE,0.05860349127182045,"E(θ) := 1 n n
X"
COMPETITIVENESS OF MLE,0.059850374064837904,"i=1
Eyn∥yn −Xθ∥2
2 −1 n n
X"
COMPETITIVENESS OF MLE,0.06109725685785536,"i=1
Eyn∥yn −Xθθθ∗∥2
2 = ∥θ −θθθ∗∥2
Σ,
(1)"
COMPETITIVENESS OF MLE,0.06234413965087282,where Σ := (P
COMPETITIVENESS OF MLE,0.06359102244389027,"i xixT
i )/n is the normalized covariance matrix, and θθθ∗is the population minimizer
of the target metric over the linear function class. Now we are ready to state the main result."
COMPETITIVENESS OF MLE,0.06483790523690773,"Theorem 2. Consider a ﬁxed design regression setting where the likelihood family is parameterized
by θ ∈Θ ⊆Bd
w and |N(ϵ, Θ ∩Bd
w)| ≤|N(ϵ, Bd
w)| for a small enough ϵ. Further the following
conditions hold,"
COMPETITIVENESS OF MLE,0.06608478802992519,"1. DKL(p(yi; θ); p(yi; θ′)) ≤L∥θ −θ′∥2.
2. The negative log-likelihood L(yn; θ) as a function of θ is α-strongly convex and β-smooth, w.p.
at least 1 −δ."
COMPETITIVENESS OF MLE,0.06733167082294264,"Further suppose there exists an estimator θest such that E(θest) ≤(c1 + c2 log(1/δ))η/n, where
c1, c2 are problem dependent quantities and η > 0. Then the MLE estimator also satisﬁes,"
COMPETITIVENESS OF MLE,0.0685785536159601,Published as a conference paper at ICLR 2022
COMPETITIVENESS OF MLE,0.06982543640897755,E(ˆθθθmle) = O
COMPETITIVENESS OF MLE,0.07107231920199501," 
c1 + c2d
 
log n + log(wLλmax(Σ)) + log(β/α) + log 1 δ
η n !"
COMPETITIVENESS OF MLE,0.07231920199501247,w.p at least 1 −δ.
COMPETITIVENESS OF MLE,0.07356608478802992,"We provide the proof in Appendix C. The proof involves proving the conditions in Theorem 1 and
bounding the size of the cover T."
COMPETITIVENESS OF MLE,0.07481296758104738,"In order to better understand Theorem 2, let us consider a typical case where there exists a possibly
complicated estimator such that E(θest) = O((d + log(1/δ))/n). In this case the above theorem
implies that MLE will be competitive with this estimator up to a log n factor. In many cases the MLE
might be much simpler to implement than the original estimator but would essentially match the
same error bound. We now provide speciﬁc examples in subsequent sections."
APPLICATIONS OF COMPETITIVENESS RESULT,0.07605985037406483,"4
APPLICATIONS OF COMPETITIVENESS RESULT"
APPLICATIONS OF COMPETITIVENESS RESULT,0.0773067331670823,"In this section we will specialize to two examples, Poisson regression and Pareto regression, where
we show that MLE can be better than TMO through the use of our competitive result in Theorem 2."
POISSON REGRESSION,0.07855361596009976,"4.1
POISSON REGRESSION"
POISSON REGRESSION,0.0798004987531172,"We work in the ﬁxed design setting in Section 3 and assume that the conditional distribution of y|x is
Poisson i.e,"
POISSON REGRESSION,0.08104738154613467,"p(yi = k|xi;θθθ∗) = µk
i e−µi"
POISSON REGRESSION,0.08229426433915212,"k!
where, µi = ⟨θθθ∗, xi⟩> 0,
(2)"
POISSON REGRESSION,0.08354114713216958,"for all i ∈[n]. Poisson regression is a popular model for studying count data regression which
naturally appears in many applications like demand forecasting (Lawless, 1987). Note that here we
study the version of Poisson regression with the identity link function (Nelder & Wedderburn, 1972),
while another popular variant is the one with exponential link function (McCullagh & Nelder, 2019).
We choose the identity link function for a fair comparison of the two approaches as it is realizable
for both the approaches under the linear function class i.e the globally optimal estimator in terms
of population can be obtained by both approaches. The exponential link function would make the
problem non-realizable under a linear function class for the TMO approach."
POISSON REGRESSION,0.08478802992518704,"We make the following natural assumptions. Let Σ = (Pn
i=1 xixT
i )/n be the design covariance
matrix as before and M = (Pn
i=1 µiuiuT
i )/n, where ui = xi/∥xi∥2. Let χ and ζ be the condition
numbers of the matrices M and Σ respectively.
Assumption 1. The parameter space Θ and the design matrix X satisfy the following,"
POISSON REGRESSION,0.08603491271820449,"• (A1) The parameter space Θ = {θ ∈Rd : ∥θ∥2 ≤w, min(∥θ∥2
2, ⟨θ, xi⟩) ≥γ > 0, ∀i ∈[n]}.
• (A2) The design matrix is such that λmin(Σ) > 0 and ∥xi∥2 ≤R for all i ∈[n]."
POISSON REGRESSION,0.08728179551122195,"• (A3) Let λmin(M) ≥
R2
4nγ2 (d log(24χ) + log(1/δ)) and
p"
POISSON REGRESSION,0.0885286783042394,"λmax(M)(d log(24χ) + log(1/δ)) ≤
√nλmin(M)/16, for a small δ ∈(0, 1) 1."
POISSON REGRESSION,0.08977556109725686,"The above assumptions are fairly mild. For instance λmin is ˜Ω(1/d) for random Gaussian covariance
matrices (Bai & Yin, 2008). The other part merely requires that λmin(M) = ˜Ω(
p"
POISSON REGRESSION,0.09102244389027432,dλmax(M)/n).
POISSON REGRESSION,0.09226932668329177,"We are interested in comparing MLE with TMO for the square loss which is just the least-squares
estimator i.e ˆθθθls := argminθ∈Θ
1
n∥yn −Xθ∥2
2. Note that it is apriori unclear as to which approach
would be better in terms of the target metric because on one hand the MLE method knows the
distribution family but on the other hand TMO is explicitly geared towards minimizing the square
loss during training."
POISSON REGRESSION,0.09351620947630923,"Least squares analysis is typically provided for regression under sub-Gaussian noise. By adapting
existing techniques (Hsu et al., 2012), we show the following guarantee for Poisson regression with
least square loss. We provide a proof in Appendix D for completeness."
POISSON REGRESSION,0.09476309226932668,1Note that the constants can be further tightened in our analysis.
POISSON REGRESSION,0.09600997506234414,Published as a conference paper at ICLR 2022
POISSON REGRESSION,0.09725685785536159,"Lemma 1. Let µmax = maxi µi. The least squares estimator ˆθθθls satisﬁes the following loss bounds
w.p. at least 1 −δ,"
POISSON REGRESSION,0.09850374064837905,E(ˆθθθls) =
POISSON REGRESSION,0.09975062344139651,"(
O
  µmax"
POISSON REGRESSION,0.10099750623441396,"n
 
log 1"
POISSON REGRESSION,0.10224438902743142,"δ + d

if µmax ≥(log(1/δ) + d log 6)/2"
POISSON REGRESSION,0.10349127182044887,"O

1
n
 
log 1"
POISSON REGRESSION,0.10473815461346633,"δ + d
2
otherwise"
POISSON REGRESSION,0.1059850374064838,"Now we present our main result in this section which uses the competitiveness bound in Theorem 2
coupled with the existence of a superior estimator compared to TMO, to show that the MLE estimator
can have a better bound than TMO."
POISSON REGRESSION,0.10723192019950124,"In Theorem 4 (in Appendix F), under some mild assumptions on the covariates, we construct an
estimator θest with the following bound for the Poisson regression setting,"
POISSON REGRESSION,0.1084788029925187,"E(θest) ≤c · ∥θθθ∗∥2 · λmax(Σ)
d + log( 1 δ )
n"
POISSON REGRESSION,0.10972568578553615,"
.
(3)"
POISSON REGRESSION,0.11097256857855362,"The construction of the estimator is median-of-means tournament based along the lines of (Lugosi &
Mendelson, 2019a) and therefore the estimator might not be practical. However, this immediately
gives the following bound on the MLE as a corollary of Theorem 2.
Corollary 1. Under assumption 1 and the conditions of Theorem 4, the MLE estimator for the
Poisson regression setting satisﬁes w.p. at least 1 −δ,"
POISSON REGRESSION,0.11221945137157108,"E(ˆθθθmle) = O

∥θθθ∗∥2 · λmax(Σ)d(log n + log(wRλmax(Σ)) + log χ + log 1 δ )
n 
."
POISSON REGRESSION,0.11346633416458853,"The bound in Corollary 1 can be better than the bound for ˆθθθls in Lemma 1. In the sub-Gaussian
region, the bound in Lemma 1 scales linearly with µmax which can be prohibitively large even when
a few covariates have large norms. The bound for the MLE estimator in Corollary 1 has no such
dependence. Further, in the sub-Exponential region the bound in Lemma 1 scales as ˜O(d2/n) while
the bound in Corollary 1 has a ˜O(d/n) dependency, up to log-factors. In Appendix G, we also show
that when the covariates are one-dimensional, an even sharper analysis is possible, that shows that
the MLE estimator is always better than least squares in terms of excess risk. In Appendix I.8, we
perform a simulated experiment that shows a linear growth of the test error w.r.t λmax(Σ), further
showing the efﬁcacy of our theoretical bounds."
PARETO REGRESSION,0.11471321695760599,"4.2
PARETO REGRESSION"
PARETO REGRESSION,0.11596009975062344,"Now we will provide an example of a heavy tailed regression setting where it is well-known that
TMO for the square loss does not perform well (Lugosi & Mendelson, 2019a). We will consider the
Pareto regression setting given by,"
PARETO REGRESSION,0.1172069825436409,"p(yi|xi) = bmb
i
yb+1
i
, mi = b −1"
PARETO REGRESSION,0.11845386533665836,"b
⟨θθθ∗, xi⟩
for yi ≥mi
(4)"
PARETO REGRESSION,0.11970074812967581,"provided ⟨θθθ∗, xi⟩> γ for all i ∈[n]. Thus yi is Pareto given xi and E[yi|xi] = µi := ⟨θθθ∗, xi⟩. We
will assume that b > 4 such that 4 + ϵ-moment exists for ϵ > 0. As in the Poisson setting, we choose
this parametrization for a fair comparison between TMO and MLE i.e in the limit of inﬁnite samples
θθθ∗lies in the linear solution space for both TMO (least squares) and MLE."
PARETO REGRESSION,0.12094763092269327,"As before, to apply Theorem 2 we need an estimator with a good risk bound. We use the estimator in
Theorem 4 of (Hsu & Sabato, 2016), which in the ﬁxed design pareto regression setting yields,"
PARETO REGRESSION,0.12219451371571072,"E(θest) =

1 + O
d log 1 δ
n"
PARETO REGRESSION,0.12344139650872818," ∥θ∗∥2
Σ
b(b −2)."
PARETO REGRESSION,0.12468827930174564,"Note that the above estimator might not be easily implementable, however this yields the following
corollary of Theorem 2, which is a bound on the performance of the MLE estimator.
Corollary 2. Under assumptions of our Pareto regression setting, the MLE estimator satisﬁes w.p.
at least 1 −δ,"
PARETO REGRESSION,0.1259351620947631,E(ˆθθθmle) = 1 + O 
PARETO REGRESSION,0.12718204488778054,"
d2
log n + log ζ + log bwRλmax(Σ)"
PARETO REGRESSION,0.128428927680798,"γ
+ log 1 δ
 n "
PARETO REGRESSION,0.12967581047381546,"∥θ∗∥2
Σ
b(b −2)."
PARETO REGRESSION,0.13092269326683292,Published as a conference paper at ICLR 2022
PARETO REGRESSION,0.13216957605985039,"The proof is provided in Appendix H. It involves verifying the two conditions in Theorem 2 in the
Pareto regression setting."
PARETO REGRESSION,0.13341645885286782,"The above MLE guarantee is expected to be much better than what can be achieved by TMO which
is least-squares. It is well established in the literature (Hsu & Sabato, 2016; Lugosi & Mendelson,
2019a) that ERM on square loss cannot achieve a O(log(1/δ)) dependency in a heavy tailed regime;
instead it can achieve only a O(
p"
PARETO REGRESSION,0.13466334164588528,1/δ) rate.
CHOICE OF LIKELIHOOD AND INFERENCE METHODS,0.13591022443890274,"5
CHOICE OF LIKELIHOOD AND INFERENCE METHODS"
CHOICE OF LIKELIHOOD AND INFERENCE METHODS,0.1371571072319202,"In this section we discuss some practical considerations for MLE, such as adapting to a target metric
of interest at inference time, and the choice of the likelihood family."
CHOICE OF LIKELIHOOD AND INFERENCE METHODS,0.13840399002493767,"Inference for different target metrics:
In most practical settings, the trained regression model
is used at inference time to predict the response variable on some test set to minimize some target
metric. For the MLE based approach, once the distribution parameters are learnt, this involves using
an appropriate statistic of the learnt distribution at inference time (see Section 3). For mean squared
error and mean absolute error, the estimator corresponds to the mean and median of the distribution,
but for several other commonly used loss metrics in the forecasting domain such as Mean Absolute
Percentage Error (MAPE) and Relative Error (RE) (Gneiting, 2011; Hyndman & Koehler, 2006),
this estimator corresponds to a median of a transformed distribution (Gneiting, 2011). Please see
Appendix I for more details. This ability to optimize the estimator at inference time for different
target metrics using a single trained model is another advantage that MLE based approaches have
over TMO models that are trained individually for speciﬁc target metrics."
CHOICE OF LIKELIHOOD AND INFERENCE METHODS,0.1396508728179551,"Mixture Likelihood:
An important practical question when performing MLE-based regression
is to decide which distribution family to use for the response variable. The goal is to pick a
distribution family that can capture the inductive biases present in the data. It is well known that a
misspeciﬁed distribution family for MLE might adversely affect generalization error of regression
models (Heagerty & Kurland, 2001). At the same time, it is also desirable for the distribution family
to be generic enough to cater to diverse datasets with potentially different types of inductive biases,
or even datasets for which no distributional assumptions can be made in advance."
CHOICE OF LIKELIHOOD AND INFERENCE METHODS,0.14089775561097256,"A simple approach that we observe to work particularly well in practice with regression models using
deep neural networks is to assume the response variable comes from a mixture distribution, where
each mixture component belongs to a different distribution family and the mixture weights are learnt
along with the parameters of the distribution. More speciﬁcally, we consider a mixture distribution of
k components p(y|x; θ1, . . . , θk, w1, . . . , wk) = Pk
j=1 wjpj(y|x; θj), where each pj characterizes
a different distribution family, and the mixture weights wj and distribution parameters θj are learnt
together. We would like to have a mixture distribution that can handle different situations like sparse
data, sub-Exponential and sub-Gaussian tails, count data as well as heavy tailed data. Moreover it
should be applicable to continuous valued datasets in general."
CHOICE OF LIKELIHOOD AND INFERENCE METHODS,0.14214463840399003,"We use a three component mixture of (i) the constant 0 (zero-inﬂation for dealing with bi-modal
sparse data), (ii) a continuous version of negative binomial where n and p are learnt and (iii) a
Pareto distribution where the scale parameter is learnt. We provide more details about the continuous
version of negative binomial distribution in Appendix I.2. Our experiments in Section 6 show that
this mixture shows promising performance on a variety of datasets."
CHOICE OF LIKELIHOOD AND INFERENCE METHODS,0.1433915211970075,"This will increase the number of parameters and the resulting likelihood might require non-convex
optimization. However, we empirically observed that with sufﬁciently over-parameterized networks
and gradient-based optimizers, this is not a problem in practice (Fig. 4 shows a convergence curve)."
EMPIRICAL RESULTS,0.14463840399002495,"6
EMPIRICAL RESULTS"
EMPIRICAL RESULTS,0.14588528678304238,"We present empirical results on two time-series forecasting problems and two regression problems
using neural networks. We will ﬁrst describe our models and baselines. Our goal is to compare the
MLE approach with the TMO approach for three target metrics per dataset."
EMPIRICAL RESULTS,0.14713216957605985,Published as a conference paper at ICLR 2022
EMPIRICAL RESULTS,0.1483790523690773,"Model
Favorita
M5"
EMPIRICAL RESULTS,0.14962593516209477,"MAPE
WAPE
RMSE
MAPE
WAPE
RMSE"
EMPIRICAL RESULTS,0.15087281795511223,"TMO(MSE)
0.6121±0.0075
0.2891±0.0023
175.3782±0.8235
0.5045±0.004
0.2839±0.0008
7.507±0.023
TMO(MAE)
0.3983±0.0012
0.2258±0.0006
161.4919±0.4748
0.4452±0.0005
0.266±0.0001
7.0503±0.0094
TMO(MAPE)
0.3199±0.0011
0.2528±0.0016
192.3823±1.3871
0.3892±0.0001
0.3143±0.0007
11.3799±0.1965
TMO(Huber)
0.432±0.0033
0.2366±0.0018
164.7006±0.7178
0.4722±0.0007
0.269±0.0002
7.093±0.0133
MLE(ZNBP)
0.3139±0.0011
0.2238±0.0009
164.6521±1.5185
0.3864±0.0001
0.2677±0.0002
7.2133±0.0152"
EMPIRICAL RESULTS,0.15211970074812967,"Table 1: We provide the MAPE, WAPE and RMSE metrics for all the models on the test set of two time-series datasets. The conﬁdence
intervals provided are one standard error over 50 experiments, for each entry. TMO(<loss>) refers to TMO using the <loss>. For the MLE row,
we only train one model per dataset. The same model is used to output a different statistic for each column during inference. For MAPE, we
output the optimizer of MAPE given in Section I.5. For WAPE we output the median and for RMSE we output the mean."
EMPIRICAL RESULTS,0.15336658354114713,"Model
Bicycle Share
Gas Turbine"
EMPIRICAL RESULTS,0.1546134663341646,"MAPE
WAPE
RMSE
MAPE
WAPE
RMSE"
EMPIRICAL RESULTS,0.15586034912718205,"TMO(MSE)
0.2503±0.0008
0.1421±0.0003
878.5815±1.3059
0.8884±0.0118
0.3496±0.0041
1.5628±0.0071
TMO(MAE)
0.2594±0.0011
0.1436±0.0003
901.1357±1.4943
0.774±0.0054
0.3389±0.0019
1.5789±0.0067
TMO(MAPE)
0.2382±0.0012
0.1469±0.0012
899.9163±4.8219
0.8108±0.0009
0.8189±0.001
3.0573±0.0019
TMO(Huber)
0.2536±0.0011
0.1414±0.0004
889.1173±1.9654
0.902±0.0128
0.3598±0.0049
1.5992±0.0082
MLE(ZNBP)
0.1969±0.0018
0.1235±0.001
767.4368±7.1274
0.9877±0.0019
0.3379±0.0004
1.4547±0.0054"
EMPIRICAL RESULTS,0.1571072319201995,"Table 2: We provide the MAPE, WAPE and RMSE metrics for all the models on the test set of two regression datasets. The conﬁdence
intervals provided are one standard error over 50 experiments, for each entry. TMO(<loss>) refers to TMO using the <loss>. For the MLE row,
we only train one model per dataset. The same model is used to output a different statistic for each column during inference. For MAPE, we
output the optimizer of MAPE given in Section I.5. For WAPE we output the median and for RMSE we output the mean."
EMPIRICAL RESULTS,0.15835411471321695,"Common Experimental Protocol: Now we describe the common experimental protocol on all the
datasets (we get into dataset related speciﬁcs and architectures subsequently). For a fair comparison
the architecture is kept the same for TMO and MLE approaches. For each dataset, we tune the
hyper-parameters for the TMO(MSE) objective. Then these hyper-parameters are held ﬁxed for all
models for that dataset i.e only the output layer and the loss function is modiﬁed. We provide all the
details in Appendix I. Our code will be available here."
EMPIRICAL RESULTS,0.1596009975062344,"For the MLE approach, the output layer of the models map to the MLE parameters of the mixture
distribution introduced in Section 5, through link functions. The MLE output has 6 parameters, three
for mixture weights, two for negative binomial component and one for the scale parameter in Pareto.
The choice of the link functions and more details are speciﬁed in Appendix I.3. The loss function
used is the negative log-likelihood implemented in Tensorﬂow Probability (Dillon et al., 2017). Note
that for the MLE approach only one model is trained per dataset and during inference we output
the statistic that optimizes the target metric in question. We refer to our MLE based models that
employ the mixture likelihood from Section 5 as MLE(ZNBP)loss, where ZNBP refers to the mixture
components Zero, Negative-Binomial and Pareto."
EMPIRICAL RESULTS,0.16084788029925187,"For TMO, the output layer of the models map to ˆy and we directly minimize the target metric in
question. Note that this means we need to train a separate model for every target metric. Thus we
have one model each for target metrics in {’MSE’, ’MAE’, ’MAPE’}. Further we also train a model
using the Huber loss 2. In order to keep the number of parameters the same as that of MLE, we add
an additional 6 neurons to the TMO models."
EXPERIMENTS ON FORECASTING DATASETS,0.16209476309226933,"6.1
EXPERIMENTS ON FORECASTING DATASETS"
EXPERIMENTS ON FORECASTING DATASETS,0.1633416458852868,We perform our experiments on two well-known forecasting datasets used in Kaggle competitions.
EXPERIMENTS ON FORECASTING DATASETS,0.16458852867830423,"1. The M5 dataset (M5, 2020) consists of time series data of product sales from 10 Walmart stores in
three US states. The data consists of two different hierarchies: the product hierarchy and store
location hierarchy. For simplicity, in our experiments we use only the product hierarchy consisting
of 3K individual time-series and 1.8K time steps.
2. The Favorita dataset (Favorita, 2017) is a similar dataset, consisting of time series data from
Corporación Favorita, a South-American grocery store chain. As above, we use the product
hierarchy, consisting of 4.5k individual time-series and 1.7k time steps."
EXPERIMENTS ON FORECASTING DATASETS,0.1658354114713217,"The task is to predict the values for the last 14 days all at once. The preceding 14 days are used for
validation. We provide more details about the dataset generation for reproducibility in Appendix I."
EXPERIMENTS ON FORECASTING DATASETS,0.16708229426433915,"2The Huber loss is commonly used in robust regression (Huber, 1992; Lugosi & Mendelson, 2019a)"
EXPERIMENTS ON FORECASTING DATASETS,0.16832917705735662,Published as a conference paper at ICLR 2022
EXPERIMENTS ON FORECASTING DATASETS,0.16957605985037408,"Model
p10QL
p90QL"
EXPERIMENTS ON FORECASTING DATASETS,0.1708229426433915,"TMO (Quantile)
0.0973±0.0002
0.0628±0.0019"
EXPERIMENTS ON FORECASTING DATASETS,0.17206982543640897,"MLE(ZNBP)
0.0788±0.0008
0.0536±0.0007"
EXPERIMENTS ON FORECASTING DATASETS,0.17331670822942644,"Table 3: The MLE model predicts the empirical quantile of inter-
est during inference. It is compared with Quantile regression (TMO
based). The results, averaged over 50 runs along with the correspond-
ing conﬁdence intervals are presented."
EXPERIMENTS ON FORECASTING DATASETS,0.1745635910224439,"Model
MAPE
WAPE
RMSE"
EXPERIMENTS ON FORECASTING DATASETS,0.17581047381546136,"MLE(NB)
0.3314+/-0.0016
0.2521+/-0.002
175.501+/-1.1928"
EXPERIMENTS ON FORECASTING DATASETS,0.1770573566084788,"MLE(ZNB)
0.3186+/-0.0011
0.2453+/-0.002
170.0075+/-1.282"
EXPERIMENTS ON FORECASTING DATASETS,0.17830423940149626,"MLE(ZNBP)
0.3139±0.0011
0.2238±0.0009
164.6521+/-1.5185"
EXPERIMENTS ON FORECASTING DATASETS,0.17955112219451372,"Table 4: We perform an ablation study on the Favorita dataset,
where we progressively add the components of our mixture distribu-
tion. There are three MLE models in the progression: Negative Bino-
mial (NB), Zero-Inﬂated Negative Binomial (ZNB) and ﬁnally ZNBP."
EXPERIMENTS ON FORECASTING DATASETS,0.18079800498753118,"The base architecture for the baselines as well as our model is a seq-2-seq model (Sutskever et al.,
2014). The encoders and decoders both are LSTM cells (Hochreiter & Schmidhuber, 1997). The
architecture is illustrated in Figure 1 and described in more detail in Appendix I."
EXPERIMENTS ON FORECASTING DATASETS,0.18204488778054864,"We present our experimental results in Table 1. On both the datasets the MLE model with the
appropriate inference-time estimator for a metric is always better than TMO trained on the same
target metric, except for WAPE in M5 where MLE’s performance is only marginally worse. Note
that the MLE model is always the best or second best performing model on all metrics, among all
TMO models. For TMO the best performance is not always achieved for the same target metric. For
instance, TMO(MAE) performs better than TMO(MSE) for the RMSE metric on the Favorita dataset.
In Table 4 we perform an ablation study on the Favorita dataset, where we progressively add mixture
components resulting in three MLE models: Negative Binomial, Zero-Inﬂated Negative Binomial
and ﬁnally ZNBP. This shows that each of the components add value in this dataset."
EXPERIMENTS ON REGRESSION DATASETS,0.18329177057356608,"6.2
EXPERIMENTS ON REGRESSION DATASETS"
EXPERIMENTS ON REGRESSION DATASETS,0.18453865336658354,"We perform our experiments on two standard regression datasets,"
EXPERIMENTS ON REGRESSION DATASETS,0.185785536159601,"1. The Bicyle Share dataset (Bicycle, 2017) has daily counts of the total number of rental bikes.
The features include time features as well as weather conditions such as temperature, humidity,
windspeed etc. A random 10% of the dataset is used as test and the rest for training and validation.
The dataset has a total of 730 samples.
2. The Gas Turbine dataset (Kaya et al., 2019) has 11 sensor measurements per example (hourly)
from a gas turbine in Turkey. We consider the level of NOx as our target variable and the rest
as predictors. There are 36733 samples in total. We use the ofﬁcial train/test split. A randomly
chosen 20% of the training set is used for validation. The response variable is continuous."
EXPERIMENTS ON REGRESSION DATASETS,0.18703241895261846,"For all our models, the model architecture is a fully connected DNN with one hidden layer that has
32 neurons. Note that for categorical variables, the input is ﬁrst passed through an embedding layer
(one embedding layer per feature), that is jointly trained. We provide further details like the shape of
the embedding layers in Appendix I. The architecture is illustrated in Figure 2."
EXPERIMENTS ON REGRESSION DATASETS,0.1882793017456359,"We present our results in Table 2. On the Bicycle Share dataset, the MLE(ZNBP)based model
performs optimally in all metrics and often outperforms the TMO models by a large margin even
though TMO is a separate model per target metric. On the Gas Turbine dataset, the MLE based model
is optimal for WAPE and RMSE, however it does not perform that well for the MAPE metric."
EXPERIMENTS ON REGRESSION DATASETS,0.18952618453865336,"In Table 3, we compare the MLE based approach versus quantile regression (TMO based) on the
Bicycle Share dataset, where the metric presented is the normalized quantile loss (Wang et al., 2019).
We train the TMO model for the corresponding quantile loss directly and the predictions are evaluated
on normalized quantile losses as shown in the table. The MLE based model is trained by minimizing
the negative log-likelihood and then during inference we output the corresponding empirical quantile
from the predicted distribution. MLE(ZNBP)outperforms TMO(Quantile) signiﬁcantly."
EXPERIMENTS ON REGRESSION DATASETS,0.19077306733167082,"Discussion: We compare the approaches of direct ERM on the target metric (TMO) and MLE
followed by post-hoc inference time optimization for regression and forecasting problems. We prove
a general competitiveness result for the MLE approach and also show theoretically that it can be
better than TMO in the Poisson and Pareto regression settings. Our empirical results show that our
proposed general purpose likelihood function employed in the MLE approach can uniformly perform
well on several tasks across four datasets. Even though this addresses some of the concerns about
choosing the correct likelihood for a dataset, some limitations still remain for example concerns about
the non-convexity of the log-likelihood. We provide a more in-depth discussion in Appendix J."
EXPERIMENTS ON REGRESSION DATASETS,0.19201995012468828,Published as a conference paper at ICLR 2022
REFERENCES,0.19326683291770574,REFERENCES
REFERENCES,0.19451371571072318,"Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorﬂow: A system for large-scale
machine learning. In 12th USENIX symposium on operating systems design and implementation
(OSDI 16), pp. 265–283, 2016."
REFERENCES,0.19576059850374064,"Jayadev Acharya, Hirakendu Das, Alon Orlitsky, and Ananda Theertha Suresh. A uniﬁed maximum
likelihood approach for estimating symmetric properties of discrete distributions. In International
Conference on Machine Learning, pp. 11–21. PMLR, 2017."
REFERENCES,0.1970074812967581,"Jean-Yves Audibert, Olivier Catoni, et al. Robust linear least squares regression. The Annals of
Statistics, 39(5):2766–2794, 2011."
REFERENCES,0.19825436408977556,"Zhi-Dong Bai and Yong-Qua Yin. Limit of the smallest eigenvalue of a large dimensional sample
covariance matrix. In Advances In Statistics, pp. 108–127. World Scientiﬁc, 2008."
REFERENCES,0.19950124688279303,"Derek Bean, Peter J Bickel, Noureddine El Karoui, and Bin Yu. Optimal m-estimation in high-
dimensional regression. Proceedings of the National Academy of Sciences, 110(36):14563–14568,
2013."
REFERENCES,0.20074812967581046,"Bicycle.
Bicycle
share
dataset.
https://www.kaggle.com/contactprad/
bike-share-daily-data/, 2017."
REFERENCES,0.20199501246882792,"Christian Brownlees, Emilien Joly, and Gábor Lugosi. Empirical risk minimization for heavy-tailed
losses. The Annals of Statistics, 43(6), Dec 2015. ISSN 0090-5364. doi: 10.1214/15-aos1350.
URL http://dx.doi.org/10.1214/15-AOS1350."
REFERENCES,0.20324189526184538,"Yunshun Chen, Aaron TL Lun, and Gordon K Smyth. From reads to genes to pathways: differential
expression analysis of rna-seq experiments using rsubread and the edger quasi-likelihood pipeline.
F1000Research, 5, 2016."
REFERENCES,0.20448877805486285,"Richard A Davis and Rongning Wu. A negative binomial model for time series of counts. Biometrika,
96(3):735–749, 2009."
REFERENCES,0.2057356608478803,"Joshua V Dillon, Ian Langmore, Dustin Tran, Eugene Brevdo, Srinivas Vasudevan, Dave Moore,
Brian Patton, Alex Alemi, Matt Hoffman, and Rif A Saurous. Tensorﬂow distributions. arXiv
preprint arXiv:1711.10604, 2017."
REFERENCES,0.20698254364089774,"David Donoho and Andrea Montanari. High dimensional robust m-estimation: Asymptotic variance
via approximate message passing. Probability Theory and Related Fields, 166(3):935–969, 2016."
REFERENCES,0.2082294264339152,"Lutz Dümbgen and Kaspar Ruﬁbach. Maximum likelihood estimation of a log-concave density and
its distribution function: Basic properties and uniform consistency. Bernoulli, 15(1):40–68, 2009."
REFERENCES,0.20947630922693267,"Noureddine El Karoui. On the impact of predictor geometry on the performance on high-dimensional
ridge-regularized generalized robust regression estimators. Probability Theory and Related Fields,
170(1):95–175, 2018."
REFERENCES,0.21072319201995013,"Ludwig Fahrmeir and Heinz Kaufmann. Consistency and asymptotic normality of the maximum
likelihood estimator in generalized linear models. The Annals of Statistics, 13(1):342–368, 1985."
REFERENCES,0.2119700748129676,"Favorita.
Favorita
forecasting
dataset.
https://www.kaggle.com/c/
favorita-grocery-sales-forecasting/, 2017."
REFERENCES,0.21321695760598502,"Dylan J Foster and Akshay Krishnamurthy. Efﬁcient ﬁrst-order contextual bandits: Prediction,
allocation, and triangular discrimination. arXiv preprint arXiv:2107.02237, 2021."
REFERENCES,0.2144638403990025,"Jan Gasthaus, Konstantinos Benidis, Yuyang Wang, Syama Sundar Rangapuram, David Salinas,
Valentin Flunkert, and Tim Januschowski. Probabilistic forecasting with spline quantile function
rnns. In The 22nd international conference on artiﬁcial intelligence and statistics, pp. 1901–1910.
PMLR, 2019."
REFERENCES,0.21571072319201995,"Sara A Geer and Sara van de Geer. Empirical Processes in M-estimation, volume 6. Cambridge
university press, 2000."
REFERENCES,0.2169576059850374,Published as a conference paper at ICLR 2022
REFERENCES,0.21820448877805487,"Tilmann Gneiting. Making and evaluating point forecasts. Journal of the American Statistical
Association, 106(494):746–762, 2011."
REFERENCES,0.2194513715710723,"Arthur Stanley Goldberger et al. Econometric theory. Econometric theory., 1964."
REFERENCES,0.22069825436408977,"Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT Press, 2016."
REFERENCES,0.22194513715710723,"Patrick J Heagerty and Brenda F Kurland. Misspeciﬁed maximum likelihood estimates and gener-
alised linear mixed models. Biometrika, 88(4):973–985, 2001."
REFERENCES,0.2231920199501247,"C.C. Heyde.
On an optimal asymptotic property of the maximum likelihood estimator of
a parameter from a stochastic process.
Stochastic Processes and their Applications, 8(1):
1–9, 1978.
ISSN 0304-4149.
URL https://www.sciencedirect.com/science/
article/pii/0304414978900649."
REFERENCES,0.22443890274314215,"Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735–1780, 1997."
REFERENCES,0.2256857855361596,"Daniel Hsu and Sivan Sabato. Loss minimization and parameter estimation with heavy tails. Journal
of Machine Learning Research, 17(18):1–40, 2016. URL http://jmlr.org/papers/v17/
14-273.html."
REFERENCES,0.22693266832917705,"Daniel Hsu, Sham M Kakade, and Tong Zhang. Random design analysis of ridge regression. In
Conference on learning theory, pp. 9–1. JMLR Workshop and Conference Proceedings, 2012."
REFERENCES,0.2281795511221945,"Peter J Huber. Robust estimation of a location parameter. In Breakthroughs in statistics, pp. 492–518.
Springer, 1992."
REFERENCES,0.22942643391521197,"Rob John Hyndman and Ann B Koehler. Another look at measures of forecast accuracy. International
Journal of Forecasting, 22(4):679–688, 2006."
REFERENCES,0.23067331670822944,"Heysem Kaya, PINAR TÜFEKC˙I, and Erdinc Uzun. Predicting co and no x emissions from gas
turbines: novel data and a benchmark pems. Turkish Journal of Electrical Engineering & Computer
Sciences, 27(6):4783–4796, 2019."
REFERENCES,0.23192019950124687,"Bernhard Klar. Bounds on tail probabilities of discrete distributions. Probability in the Engineering
and Informational Sciences, 14(2):161–171, 2000."
REFERENCES,0.23316708229426433,"Roger Koenker and Gilbert Bassett Jr. Regression quantiles. Econometrica: journal of the Economet-
ric Society, pp. 33–50, 1978."
REFERENCES,0.2344139650872818,"Stéphane Lathuilière, Pablo Mesejo, Xavier Alameda-Pineda, and Radu Horaud. A comprehensive
analysis of deep regression. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42
(9):2065–2081, 2020. doi: 10.1109/TPAMI.2019.2910523."
REFERENCES,0.23566084788029926,"Jerald F Lawless. Negative binomial and mixed poisson regression. The Canadian Journal of
Statistics/La Revue Canadienne de Statistique, pp. 209–225, 1987."
REFERENCES,0.23690773067331672,"G. Lecue and S. Mendelson. Learning subgaussian classes: Upper and minimax bounds. In S.
Boucheron and N. Vayatis, editors, Topics in Learning Theory. Societe Mathematique de France,
2016."
REFERENCES,0.23815461346633415,"Erich L Lehmann and George Casella. Theory of point estimation. Springer Science & Business
Media, 2006."
REFERENCES,0.23940149625935161,"Gábor Lugosi and Shahar Mendelson. Mean estimation and regression under heavy-tailed distribu-
tions: A survey. Foundations of Computational Mathematics, 19(5):1145–1190, 2019a."
REFERENCES,0.24064837905236908,"Gábor Lugosi and Shahar Mendelson. Sub-gaussian estimators of the mean of a random vector. The
annals of statistics, 47(2):783–794, 2019b."
REFERENCES,0.24189526184538654,"M5.
M5
forecasting
dataset.
https://www.kaggle.com/c/
m5-forecasting-accuracy/, 2020."
REFERENCES,0.243142144638404,Published as a conference paper at ICLR 2022
REFERENCES,0.24438902743142144,"Davis J McCarthy, Yunshun Chen, and Gordon K Smyth.
Differential expression analysis of
multifactor rna-seq experiments with respect to biological variation. Nucleic acids research, 40
(10):4288–4297, 2012."
REFERENCES,0.2456359102244389,"Peter McCullagh and John A Nelder. Generalized linear models. Routledge, 2019."
REFERENCES,0.24688279301745636,"Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT
press, 2018."
REFERENCES,0.24812967581047382,"John Ashworth Nelder and Robert WM Wedderburn. Generalized linear models. Journal of the
Royal Statistical Society: Series A (General), 135(3):370–384, 1972."
REFERENCES,0.24937655860349128,"Ryan O’Donnell. Analysis of boolean functions. Cambridge University Press, 2014."
REFERENCES,0.2506234413965087,"Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-beats: Neural basis
expansion analysis for interpretable time series forecasting. arXiv preprint arXiv:1905.10437,
2019."
REFERENCES,0.2518703241895262,"Adarsh Prasad, Arun Sai Suggala, Sivaraman Balakrishnan, and Pradeep Ravikumar. Robust estima-
tion via robust gradient estimation. Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 82(3):601–627, 2020."
REFERENCES,0.25311720698254364,"C.R. Rao. Criteria of estimation in large samples. Sankhy¯a, 25, Ser A, 1963."
REFERENCES,0.2543640897755611,"Richard Redner. Note on the consistency of the maximum likelihood estimate for nonidentiﬁable
distributions. The Annals of Statistics, pp. 225–228, 1981."
REFERENCES,0.25561097256857856,"Philippe Rigollet.
High-dimensional statistics.
https://ocw.mit.edu/courses/
mathematics/18-s997-high-dimensional-statistics-spring-2015/
lecture-notes/MIT18_S997S15_Chapter2.pdf, 2015."
REFERENCES,0.256857855361596,"Alesandro Rinaldo.
Sub-exponential concentration.
http://www.stat.cmu.edu/
~arinaldo/Teaching/36709/S19/Scribed_Lectures/Feb5_Aleksandr.pdf,
2019."
REFERENCES,0.2581047381546135,"Mark D Robinson and Gordon K Smyth. Small-sample estimation of negative binomial dispersion,
with applications to sage data. Biostatistics, 9(2):321–332, 2008."
REFERENCES,0.2593516209476309,"David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. Deepar: Probabilistic
forecasting with autoregressive recurrent networks. International Journal of Forecasting, 36(3):
1181–1191, 2020."
REFERENCES,0.26059850374064836,"Rajat Sen, Hsiang-Fu Yu, and Inderjit Dhillon. Think globally, act locally: A deep neural network
approach to high-dimensional time series forecasting. arXiv preprint arXiv:1905.03806, 2019."
REFERENCES,0.26184538653366585,"Niranjan Srinivas, Andreas Krause, Sham M Kakade, and Matthias Seeger. Gaussian process opti-
mization in the bandit setting: No regret and experimental design. arXiv preprint arXiv:0912.3995,
2009."
REFERENCES,0.2630922693266833,"Pragya Sur and Emmanuel J Candès. A modern maximum-likelihood theory for high-dimensional
logistic regression. Proceedings of the National Academy of Sciences, 116(29):14516–14525,
2019."
REFERENCES,0.26433915211970077,"Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
arXiv preprint arXiv:1409.3215, 2014."
REFERENCES,0.2655860349127182,"Abraham Wald. Note on the consistency of the maximum likelihood estimate. The Annals of
Mathematical Statistics, 20(4):595–601, 1949."
REFERENCES,0.26683291770573564,"Yuyang Wang, Alex Smola, Danielle Maddix, Jan Gasthaus, Dean Foster, and Tim Januschowski.
Deep factors for forecasting. In International Conference on Machine Learning, pp. 6607–6617.
PMLR, 2019."
REFERENCES,0.26807980049875313,"Ruofeng Wen Wen, Kari Torkkola, and Balakrishnan Narayanaswamy. A multi-horizon quantile
recurrent forecaster. In NIPS Time Series Workshop, 2017."
REFERENCES,0.26932668329177056,Published as a conference paper at ICLR 2022
REFERENCES,0.27057356608478805,"Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, Xiaojun Chang, and Chengqi Zhang. Connecting
the dots: Multivariate time series forecasting with graph neural networks. In Proceedings of the
26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp.
753–763, 2020."
REFERENCES,0.2718204488778055,"Tong Zhang. From epsilon-entropy to kl-entropy: Analysis of minimum information complexity
density estimation. The Annals of Statistics, 34(5):2180–2210, 2006."
REFERENCES,0.2730673316708229,"Ziwei Zhu and Wenjing Zhou. Taming heavy-tailed features by shrinkage. In International Conference
on Artiﬁcial Intelligence and Statistics, pp. 3268–3276. PMLR, 2021."
REFERENCES,0.2743142144638404,Published as a conference paper at ICLR 2022
REFERENCES,0.27556109725685785,"A
PROOF FOR COMPETITIVENESS OF MLE"
REFERENCES,0.27680798004987534,"Theorem 3. Let ˆπ be an estimator for a property π, such that for any f ∈F and z ∼f, Pr(∥π(f)−
ˆπ(z)∥≥ϵ) ≤δ. Then the MLE based estimator satisﬁes the following bound,"
REFERENCES,0.27805486284289277,Pr(∥π(f) −π(fz)∥≥2ϵ) ≤(|F| + 1)δ.
REFERENCES,0.2793017456359102,"Proof of Theorem 3. By triangle inequality and by the properties,"
REFERENCES,0.2805486284289277,∥π(f) −π(fz)∥≤∥π(f) −ˆπ(z)∥+ ∥ˆπ(z) −π(fz)∥.
REFERENCES,0.2817955112219451,"Hence,
1∥π(f)−π(fz)∥≥2ϵ ≤1∥π(f)−ˆπ(z)∥≥ϵ + 1∥ˆπ(z)−π(fz)∥≥ϵ."
REFERENCES,0.2830423940149626,"We now take expectation of both LHS and RHS of the above equation with respect to the distribution
f. For the LHS, observe that"
REFERENCES,0.28428927680798005,E[1∥π(f)−π(fz)∥≥2ϵ] = Pr(∥π(f) −π(fz)∥≥2ϵ).
REFERENCES,0.2855361596009975,For the ﬁrst term in the RHS
REFERENCES,0.286783042394015,"E[1∥π(f)−ˆπ(z)∥≥ϵ] ≤Pr(∥π(f) −ˆπ(z)∥≥ϵ) ≤δ,"
REFERENCES,0.2880299251870324,by the assumption in the theorem. Combining the above three equations yield that
REFERENCES,0.2892768079800499,Pr(∥π(f) −π(fz)∥≥2ϵ) ≤E[1∥ˆπ(z)−π(fz)∥≥ϵ] + δ.
REFERENCES,0.29052369077306733,"We now prove that E[1∥ˆπ(z)−π( ˜
fz)∥≥ϵ] ≤|F|δ."
REFERENCES,0.29177057356608477,"E[1∥ˆπ(z)−π(fz)∥≥ϵ] =
Z"
REFERENCES,0.29301745635910226,"z
f(z)1∥ˆπ(z)−π(fz)∥≥ϵ"
REFERENCES,0.2942643391521197,"(a)
≤
Z"
REFERENCES,0.2955112219451372,"z
fz(z)1∥ˆπ(z)−π(fz)∥≥ϵ ≤
Z z X"
REFERENCES,0.2967581047381546,"f
f(z)1∥ˆπ(z)−π(f)∥≥ϵ"
REFERENCES,0.29800498753117205,"(b)
=
X f Z"
REFERENCES,0.29925187032418954,"z
f(z)1∥ˆπ(z)−π(f)∥≥ϵ =
X"
REFERENCES,0.300498753117207,"f
Pr
z∼f(∥ˆπ(z) −π(f)∥≥ϵ) ≤
X"
REFERENCES,0.30174563591022446,"f
δ = |F|δ,"
REFERENCES,0.3029925187032419,"where (a) uses the fact that fz is the MLE estimate over set F and hence fz(z) ≥f(z). (b) follows
from Fubini’s theorem."
REFERENCES,0.30423940149625933,Now we prove an extension of our last theorem that can deal with inﬁnite likelihood families.
REFERENCES,0.3054862842892768,"Proof of Theorem 1. Let ˜F be the cover of Ff that satisﬁes assumptions in Deﬁnition 1. By triangle
inequality and by the properties of ˜F, with probability at least 1 −δ2 −δ,"
REFERENCES,0.30673316708229426,∥π(f) −π(fz)∥≤∥π(f) −ˆπ(z)∥+ ∥ˆπ(z) −π( ˜fz)∥+ ∥π( ˜fz) −π(fz)∥
REFERENCES,0.30798004987531175,≤∥π(f) −ˆπ(z)∥+ ∥ˆπ(z) −π( ˜fz)∥+ ϵ
REFERENCES,0.3092269326683292,"We have used the fact that fz ∈Ff w.p at least 1 −δ. Hence with probability at least 1 −δ2 −δ,"
REFERENCES,0.3104738154613466,"1∥π(f)−π(fz)∥≥3ϵ ≤1∥π(f)−ˆπ(z)∥≥ϵ + 1∥ˆπ(z)−π( ˜
fz)∥≥ϵ."
REFERENCES,0.3117206982543641,Published as a conference paper at ICLR 2022
REFERENCES,0.31296758104738154,"We now take expectation of both LHS and RHS of the above equation with respect to the distribution
f. For the LHS, observe that"
REFERENCES,0.314214463840399,E[1∥π(f)−π(fz)∥≥3ϵ] = Pr(∥π(f) −π(fz)∥≥3ϵ).
REFERENCES,0.31546134663341646,For the ﬁrst term in the RHS
REFERENCES,0.3167082294264339,"E[1∥π(f)−ˆπ(z)∥≥ϵ] ≤Pr(∥π(f) −ˆπ(z)∥≥ϵ) ≤δ,"
REFERENCES,0.3179551122194514,by the assumption in the theorem. Combining the above three equations yield that
REFERENCES,0.3192019950124688,"Pr(∥π(f) −π(fz)∥≥3ϵ) ≤E[1∥ˆπ(z)−π( ˜
fz)∥≥ϵ] + 2δ + δ2."
REFERENCES,0.3204488778054863,"We now prove that E[1∥ˆπ(z)−π( ˜
fz)∥≥ϵ] ≤Tδ + δ1. Let ˜f be the distribution in ˜Ff that is at most δ1
away from f. Then,"
REFERENCES,0.32169576059850374,"E[1∥ˆπ(z)−π( ˜
fz)∥≥ϵ] =
Z"
REFERENCES,0.3229426433915212,"z
f(z)1∥ˆπ(z)−π( ˜
fz)∥≥ϵ"
REFERENCES,0.32418952618453867,"(a)
≤
Z"
REFERENCES,0.3254364089775561,"z
˜f(z)1∥ˆπ(z)−π( ˜
fz)∥≥ϵ + ∥f −˜f∥1"
REFERENCES,0.3266832917705736,"(b)
≤
Z"
REFERENCES,0.327930174563591,"z
˜f(z)1∥ˆπ(z)−π( ˜
fz)∥≥ϵ + δ1"
REFERENCES,0.32917705735660846,"(c)
≤
Z"
REFERENCES,0.33042394014962595,"z
˜fz(z)1∥ˆπ(z)−π( ˜
fz)∥≥ϵ + δ1 ≤
Z z X"
REFERENCES,0.3316708229426434,"˜
f∈˜
F"
REFERENCES,0.3329177057356609,"˜f(z)1∥ˆπ(z)−π( ˜
fz)∥≥ϵ + δ1"
REFERENCES,0.3341645885286783,"(d)
=
X"
REFERENCES,0.33541147132169574,"˜
f∈˜
F Z"
REFERENCES,0.33665835411471323,"z
˜f(z)1∥ˆπ(z)−π( ˜
f)∥≥ϵ + δ1 =
X"
REFERENCES,0.33790523690773067,"˜
f∈˜
F
Pr
z∼˜
f
(∥π( ˜f) −ˆπ(z)∥≥ϵ) + δ1 ≤(
X"
REFERENCES,0.33915211970074816,"˜
f∈˜
F
δ) + δ1 = Tδ + δ1,"
REFERENCES,0.3403990024937656,"where the (a) follows from the deﬁnition of ℓ1 distance between distributions, (b) follows from
the properties of the cover, (c) uses the fact that ˜fz is the MLE estimate over set ˜F and hence
˜fz(z) ≥˜f(z). (d) follows from Fubini’s Theorem."
REFERENCES,0.341645885286783,"B
OTHER GENERAL RESULTS FOR THE MLE"
REFERENCES,0.3428927680798005,"In this section, we prove that the log likelihood of the MLE distribution at the observed data point is
is close to the log-likelihood of the ground truth at the data point, upto an additive factor that depends
on the Shtarkov sum of the family.
Lemma 2. Let z ∼f. The maximum likelihood estimator satisﬁes the following inequality with
probability at least 1 −δ,"
REFERENCES,0.34413965087281795,log f(z) ≤log fz(z) ≤log f(z) + log X
REFERENCES,0.34538653366583544,"z∈Z
fz(z) !"
REFERENCES,0.34663341645885287,"+ log 1 δ ,"
REFERENCES,0.3478802992518703,where P
REFERENCES,0.3491271820448878,"z∈Z fz(z) is the Shtarkov sum of the family F. Furthermore, if F is ﬁnite, then"
REFERENCES,0.35037406483790523,"log f(z) ≤log fz(z) ≤log f(z) + log |F| + log 1 δ ,"
REFERENCES,0.3516209476309227,"Further is the distribution family is ﬁnite, then"
REFERENCES,0.35286783042394015,log f(z) ≤log fz(z) ≤log f(z) + log|F| + log 1 δ .
REFERENCES,0.3541147132169576,Published as a conference paper at ICLR 2022
REFERENCES,0.3553615960099751,"Proof. Let K = 1 δ
P"
REFERENCES,0.3566084788029925,z∈Z fz(z).
REFERENCES,0.35785536159601,"Pr(f(z) ≤fz(z)/K) = Pr
 fz(z)"
REFERENCES,0.35910224438902744,"Kf(z) ≥1
"
REFERENCES,0.36034912718204487,"(a)
≤E
 fz(z) Kf(z)  = 1 K X"
REFERENCES,0.36159600997506236,"z∈Z
fz(z) = δ,"
REFERENCES,0.3628428927680798,"where (a) follows from Markov’s inequality. The ﬁrst part of the lemma follows by taking the
logarithm and substituting the value of K. For the second part, observe that if F is ﬁnite then,
X"
REFERENCES,0.3640897755610973,"z∈Z
fz(z) ≤
X z∈Z X"
REFERENCES,0.3653366583541147,"f∈F
f(z) =
X f∈F X"
REFERENCES,0.36658354114713215,"z∈Z
f(z) =
X"
REFERENCES,0.36783042394014964,"f∈F
1 = |F|."
REFERENCES,0.3690773067331671,"We now prove a result of Shtarkov’s sum, which will be useful in other scenarios."
REFERENCES,0.37032418952618457,"Lemma 3. Let w = π(z) for some property π.
X"
REFERENCES,0.371571072319202,"w
fw(w) ≤
X"
REFERENCES,0.37281795511221943,"z
fz(z)."
REFERENCES,0.3740648379052369,Proof. X
REFERENCES,0.37531172069825436,"z
fz(z) ≥
X"
REFERENCES,0.3765586034912718,"z
pg(z)(z) =
X w X"
REFERENCES,0.3778054862842893,"z:g(z)=w
fw(z) =
X"
REFERENCES,0.3790523690773067,"w
fw(w)."
REFERENCES,0.3802992518703242,"C
GENERAL FIXED DESIGN RESULT"
REFERENCES,0.38154613466334164,"In this section, we will prove Theorem 2 which is an application of Theorem 1 to the ﬁxed design
regression setting with the square loss as the target metric. The theorem holds under some reasonable
assumptions. We will ﬁst prove some intermediate lemmas."
REFERENCES,0.3827930174563591,"Lemma 4. Let f(·; θ) = Qn
i=1 p(·|xi; θ) in the ﬁxed design setting. If ∥θ −θ′∥2 ≤δ then,"
REFERENCES,0.38403990024937656,"∥f(·; θ) −f(·; θ′)∥1 ≤
√ 2nLδ."
REFERENCES,0.385286783042394,under the assumptions of Theorem 2.
REFERENCES,0.3865336658354115,"Proof. By Pinskers’ inequality we have the following chain,"
REFERENCES,0.3877805486284289,"∥f(·; θ) −f(·; θ′)∥1 ≤
p"
REFERENCES,0.38902743142144636,2DKL(f(·; θ); f(·; θ′)) ≤
REFERENCES,0.39027431421446385,"v
u
u
t2 n
X"
REFERENCES,0.3915211970074813,"i=1
DKL(p(·|xi; θ); p(·|xi; θ′)) ≤
√ 2nLδ"
REFERENCES,0.39276807980049877,"Lemma 5. Assume the conditions of Theorem 2 holds. Let ˜θθθmle := argmaxθ∈N(˜ϵ,Θ) L(yn, θ), in
the Poisson regression setting. Then with probability at least 1 −δ we have,"
REFERENCES,0.3940149625935162,"|E(ˆθθθmle) −E(˜θθθmle)| ≤4wλmax(Σ)˜ϵ n r β
α."
REFERENCES,0.39526184538653364,Published as a conference paper at ICLR 2022
REFERENCES,0.39650872817955113,Proof. We assume that the event in (2) of Theorem 2 holds.
REFERENCES,0.39775561097256856,|L(yn; ˆθθθmle) −L(yn; θc)| ≤β
REFERENCES,0.39900249376558605,"2 ˜ϵ2,"
REFERENCES,0.4002493765586035,"where θc is the closest point in N(˜ϵ, Θ) to ˆθθθmle. Therefore, by deﬁnition"
REFERENCES,0.4014962593516209,L(yn; ˜θθθmle) ≤L(yn; ˆθθθmle) + (β/2)˜ϵ2.
REFERENCES,0.4027431421446384,"By virtue of the strong-convexity we have,"
REFERENCES,0.40399002493765584,"ˆθθθmle −˜θθθmle

2 2 ≤β α˜ϵ2."
REFERENCES,0.40523690773067333,"Now by triangle inequality we have, |
q"
REFERENCES,0.40648379052369077,"E(ˆθθθmle) −
q"
REFERENCES,0.4077306733167082,"E(˜θθθmle)| = |
ˆθθθmle −θθθ∗
Σ −
˜θθθmle −θθθ∗
Σ|"
REFERENCES,0.4089775561097257,"≤
ˆθθθmle −˜θθθmle

Σ ≤ r"
REFERENCES,0.4102244389027431,"λmax(Σ)β α
˜ϵ"
REFERENCES,0.4114713216957606,"Now we can use the fact |a −b| ≤2 max(√a,
√"
REFERENCES,0.41271820448877805,"b)|√a −
√"
REFERENCES,0.4139650872817955,"b| and that Θ ⊆Bd
w to conclude,"
REFERENCES,0.415211970074813,"|E(ˆθθθmle) −E(˜θθθmle)| ≤4wλmax(Σ)˜ϵ r β
α."
REFERENCES,0.4164588528678304,"Proof of Theorem 2. Consider the net N(φ, θ). If φ = δ2/(2nL), then by Lemma 4 the net forms a
δ cover on F. Note that under the conditions of Theorem 2 |N(φ, θ)| ≤(3w/φ)d."
REFERENCES,0.4177057356608479,"Next note that in the application of Theorem 1, we should set"
REFERENCES,0.41895261845386533,"ϵ = (c1 + c2 log(1/δ))η n
."
REFERENCES,0.42019950124688277,"Thus if we set φ in place of ˜ϵ in Lemma 5 we would need the following to apply Theorem 1,"
REFERENCES,0.42144638403990026,4wλmax(Σ)φ r
REFERENCES,0.4226932668329177,"β
α = (c1 + c2 log(1/δ))η n
."
REFERENCES,0.4239401496259352,"Thus, φ can be such that, log 1"
REFERENCES,0.4251870324189526,"φ ≤max

log(2nL), 0.5 log β"
REFERENCES,0.42643391521197005,"α + log(4wλmax(Σ)) + log(n)

."
REFERENCES,0.42768079800498754,"From above we can apply Theorem 1 with the above ϵ, δ1 = δ2 = δ and T = (3w/φ)d. Therefore,
we get"
REFERENCES,0.428927680798005,"P

E(ˆθθθmle) ≥3(c1 + c2 log(1/δ))η n"
REFERENCES,0.43017456359102246,"
≤(T + 5)δ.
(5)"
REFERENCES,0.4314214463840399,"We can set δ = δ/(T + 5) to then conclude that w.p at least 1 −δ,"
REFERENCES,0.43266832917705733,"E(ˆθθθmle) = O
(c1 + c2d(log(w) + log(1/φ)))η n 
."
REFERENCES,0.4339152119700748,Substituting the bound on log 1
REFERENCES,0.43516209476309226,φ yields the result.
REFERENCES,0.43640897755610975,Published as a conference paper at ICLR 2022
REFERENCES,0.4376558603491272,"D
LEAST SQUARES FOR POISSON"
REFERENCES,0.4389027431421446,"We begin by restating the following general statement about OLS.
Lemma 6 (see Theorem 2.2 in (Rigollet, 2015)). Suppose r = rank(XT X) and φφφ ∈Rn×r be an
orthonormal basis for the column-span of X. Then we have the following result,"
REFERENCES,0.4401496259351621,E(ˆθθθls) ≤4
REFERENCES,0.44139650872817954,"n
sup
u∈Sr−1
1
(uT˜ϵϵϵ)2,
(6)"
REFERENCES,0.442643391521197,"where ˜ϵϵϵ = φφφTϵϵϵ. Here, ϵ = yn −Xθθθ∗for the given response sample yn."
REFERENCES,0.44389027431421446,Now are at a position to prove Lemma 1.
REFERENCES,0.4451371571072319,"Proof of Lemma 1. We are interested in tail bounds on the RHS of (6). We will ﬁrst analyze tail
bounds for a ﬁxed u ∈Sr−1
1
. We have the following chain,"
REFERENCES,0.4463840399002494,"E[exp{s⟨u,˜ϵϵϵ⟩}] = E[exp{s⟨φu, ϵ⟩}] := E[exp{s⟨v, ϵ⟩}] = n
Y"
REFERENCES,0.4476309226932668,"i=1
E[exp{sviϵi}] = n
Y"
REFERENCES,0.4488778054862843,"i=1
E[exp(svi(yi −µi))]"
REFERENCES,0.45012468827930174,"where µi = ⟨θθθ∗, xi⟩. Now bounding each term separately we have,"
REFERENCES,0.4513715710723192,E[exp(svi(yi −µi))] = exp(−sviµi) exp(µi(exp(svi) −1))
REFERENCES,0.45261845386533667,"We have, n
Y"
REFERENCES,0.4538653366583541,"i=1
exp(µi(exp(svi) −1)) = exp n
X"
REFERENCES,0.4551122194513716,"i=1
µi(exp(svi) −1) ! = exp  
n
X i=1  
∞
X j=1 µi"
REFERENCES,0.456359102244389,"j! (svi)j    ≤exp  
n
X i=1 "
REFERENCES,0.45760598503740646,"µisvi + ∞
X"
REFERENCES,0.45885286783042395,"j=2
µmax
1
j!(svi)j     ≤exp X"
REFERENCES,0.4600997506234414,"i
µisvi + µmax(exp(s) −s −1) ! ,
(7)"
REFERENCES,0.4613466334164589,"where we have used the fact that ||v||p ≤||v||2 = 1 for all p ≥2. Thus we have,"
REFERENCES,0.4625935162094763,"E[exp{s⟨u,˜ϵϵϵ⟩}] ≤exp(µmax(exp(s) −s −1))."
REFERENCES,0.46384039900249374,"This means that the RV ⟨u,˜ϵϵϵ⟩is sub-exponential with parameter ν2 = 2µmax and α = 0.56. Thus if
t ≤4µmax then we have that for a ﬁxed u,"
REFERENCES,0.46508728179551123,"P(⟨u,˜ϵϵϵ⟩} ≥t) ≤exp

−
t2 4µmax"
REFERENCES,0.46633416458852867,"
.
(8)"
REFERENCES,0.46758104738154616,"Thus by a union bound over the ϵ-net with ϵ = 2, we have that wp 1 −δ,"
REFERENCES,0.4688279301745636,"sup
u ⟨u,˜ϵϵϵ⟩≤
p"
REFERENCES,0.470074812967581,8µmax(log(1/δ) + r log 6).
REFERENCES,0.4713216957605985,"Thus we get the risk bound wp 1 −δ,"
REFERENCES,0.47256857855361595,E(ˆθθθls) ≤32µmax
REFERENCES,0.47381546134663344,"n
(log(1/δ) + r log 6).
(9)"
REFERENCES,0.47506234413965087,"For the sub-exponential region when t ≥µmax we get the bound,"
REFERENCES,0.4763092269326683,E(ˆθθθls) ≤16
REFERENCES,0.4775561097256858,"n max

(log(1/δ) + r log 6)2, µ2
max

."
REFERENCES,0.47880299251870323,We get the bound by setting r = d.
REFERENCES,0.4800498753117207,Published as a conference paper at ICLR 2022
REFERENCES,0.48129675810473815,"E
COMPETITIVENESS FOR POISSON REGRESSION"
REFERENCES,0.4825436408977556,"Lemma 7. Let f(·; θ) = Qn
i=1 p(·|xi; θ) where p is deﬁned in Eq. (2). If ∥θ −θ′∥2 ≤δ then,"
REFERENCES,0.4837905236907731,"∥f(·; θ) −f(·; θ′)∥1 ≤R
r2nw γ
δ."
REFERENCES,0.4850374064837905,"Proof. By Pinskers’ inequality we have the following chain,"
REFERENCES,0.486284289276808,"∥f(·; θ) −f(·; θ′)∥1 ≤
p"
REFERENCES,0.48753117206982544,2DKL(f(·; θ); f(·; θ′)) ≤
REFERENCES,0.48877805486284287,"v
u
u
t2 n
X"
REFERENCES,0.49002493765586036,"i=1
DKL(p(·|xi; θ); p(·|xi; θ′)) ≤"
REFERENCES,0.4912718204488778,"v
u
u
t2 n
X"
REFERENCES,0.4925187032418953,"i=1
(⟨θ, xi⟩(log(⟨θ, xi⟩) −log(⟨θ′, xi⟩)) −(⟨θ, xi⟩−⟨θ′, xi⟩))"
REFERENCES,0.4937655860349127,"Notice that the absolute value of the derivative of a log x −x wrt x, is upper bounded by a/γ −1 if
x ≥γ > 0 and also a ≥γ. Therefore, following from above we have,"
REFERENCES,0.49501246882793015,∥f(·; θ) −f(·; θ′)∥1 ≤
REFERENCES,0.49625935162094764,"v
u
u
t2 n
X i=1"
REFERENCES,0.4975062344139651,θT xi
REFERENCES,0.49875311720698257,"γ
−1

|⟨θ, xi⟩−⟨θ′, xi⟩| ≤"
REFERENCES,0.5,"v
u
u
t2 n
X i=1"
REFERENCES,0.5012468827930174,θT xi
REFERENCES,0.5024937655860349,"γ
−1

∥θ −θ′∥2R ≤ s 2nwR2"
REFERENCES,0.5037406483790524,"γ
∥θ −θ′∥2,"
REFERENCES,0.5049875311720698,thus concluding the proof.
REFERENCES,0.5062344139650873,"Now we prove high probability bounds on the strong convexity and smoothness of the likelihood in
the context of Poisson regression."
REFERENCES,0.5074812967581047,"Lemma 8. Let χ be the condition number of the matrix M, where ui = xi/∥xi∥2. Then with
probability at least 1 −2δ, we have"
REFERENCES,0.5087281795511222,"λmin
 
∇2
θL(yn; θ)

≥
n"
REFERENCES,0.5099750623441397,"2∥θ∥2 λmin(M),
(10)"
REFERENCES,0.5112219451371571,"provided nλmin(M) ≥
1
4γ2 (d log(24χ) + log(1/δ)) and
p"
REFERENCES,0.5124688279301746,"λmax(M)(d log(24χ) + log(1/δ)) ≤
√nλmin(M)/16."
REFERENCES,0.513715710723192,"Proof. We start with the expression of the Hessian of L(yn; θ),"
REFERENCES,0.5149625935162094,"∇2
θL = n
X i=1"
REFERENCES,0.516209476309227,"yixixT
i
⟨θ, xi⟩2 ≽ n
X i=1"
REFERENCES,0.5174563591022444,"yiuiuT
i
∥θ∥2
:= L(θ),"
REFERENCES,0.5187032418952618,where ui = xi/∥xi∥2.
REFERENCES,0.5199501246882793,"For a ﬁxed unit vector u ∈Sd−1
1
let us deﬁne,"
REFERENCES,0.5211970074812967,"L(u, θ) := n
X i=1"
REFERENCES,0.5224438902743143,"yiuT uiuT
i u"
REFERENCES,0.5236907730673317,"∥θ∥2
:= n
X"
REFERENCES,0.5249376558603491,"i=1
yivi,"
REFERENCES,0.5261845386533666,"where vi = ⟨ui, u⟩2/∥θ∥2."
REFERENCES,0.527431421446384,Published as a conference paper at ICLR 2022
REFERENCES,0.5286783042394015,"Following the deﬁnition of sub-exponential RV in (Rinaldo, 2019), X"
REFERENCES,0.529925187032419,"i
yivi ∈SE "
REFERENCES,0.5311720698254364,"ν2 =
X"
REFERENCES,0.5324189526184538,"i
ν2
i , α = 0.56 max
i
vi !"
REFERENCES,0.5336658354114713,",
(11)"
REFERENCES,0.5349127182044888,"where νi = 2µiv2
i . This implies that wp atleast 1 −2δ/C we have, |
X"
REFERENCES,0.5361596009975063,"i
yivi −
X"
REFERENCES,0.5374064837905237,"i
µivi| ≤2 s"
REFERENCES,0.5386533665835411,"X
µiv2
i log
C δ  if,
s"
REFERENCES,0.5399002493765586,"X
µiv2
i log
C δ"
REFERENCES,0.5411471321695761,"
≤2(
X
µiv2
i )/(max
i
vi)."
REFERENCES,0.5423940149625935,"Note that the above condition is mild and is satisﬁed when λmin
 P"
REFERENCES,0.543640897755611,"i µiuiuT
i

≥log(C/δ)/4γ2, as
minu
P"
REFERENCES,0.5448877805486284,"i µivi ≥λmin
 P"
REFERENCES,0.5461346633416458,"i µiuiuT
i

. Here, C is a problem dependent constant that will be chosen
later."
REFERENCES,0.5473815461346634,"Now consider an ϵ-net in ℓ2 norm over the surface unit sphere denoted by N(ϵ, d). It is well known
that |N(ϵ, d)| ≤(3/ϵ)d. Now any z ∈Sd−1
1
can be written as z = x + u where x ∈N(ϵ, d) and
u ∈Bd−1
ϵ
. Therefore, we have that"
REFERENCES,0.5486284289276808,"L(z, θ) ≥L(x, θ) −λmax(L(θ))ϵ"
REFERENCES,0.5498753117206983,"A similar argument as above gives us,"
REFERENCES,0.5511221945137157,"λmax(L(θ)) = max
u∈Bd−1
1
L(u, θ) ≤
max
x∈N(ϵ,d) L(x, θ) + 1"
MAX,0.5523690773067331,"2 max
u∈Bd−1
1
L(u, θ)"
MAX,0.5536159600997507,"=⇒λmax(L(θ)) ≤2
max
x∈N(ϵ,d) L(x, θ)"
MAX,0.5548628428927681,"Thus by an union bound over the net we have wp 1 −2δ and other conditions,"
MAX,0.5561097256857855,"min
z∈Sd−1
1
L(z, θ) ≥
1"
MAX,0.557356608478803,"∥θ∥2
2
λmin X"
MAX,0.5586034912718204,"i
µiuiuT
i ! −
2ϵ"
MAX,0.559850374064838,"∥θ∥2
2
λmax X"
MAX,0.5610972568578554,"i
µiuiuT
i !"
MAX,0.5623441396508728,"−max
u∈Sd−1
1
2(1 + 2ϵ)"
MAX,0.5635910224438903,"v
u
u
tX i"
MAX,0.5648379052369077,"µi⟨u, ui⟩2"
MAX,0.5660847880299252,"∥θ∥4
log
C δ 
."
MAX,0.5673316708229427,"if the conditions above hold with C = (3/ϵ)d. We can now set ϵ = 1/(8∗χ) where χ is the condition
number of the matrix P"
MAX,0.5685785536159601,"i µiuiuT
i . Now by virtue of that fact that
p"
MAX,0.5698254364089775,"λmax(P µiuiuT
i ) log(C/δ) ≤
λmin
 P"
MAX,0.571072319201995,"i µiuiuT
i

/16 we have the result."
MAX,0.5723192019950125,"Now we will prove a result on the smoothness of the negative log likelihood for the Poisson regression
setting.
Lemma 9. With probability at least 1 −2δ, we have"
MAX,0.57356608478803,"λmax
 
∇2
θL(yn; θ)

≤2nR2"
MAX,0.5748129675810474,"γ2
λmax(M),
(12)"
MAX,0.5760598503740648,if λmin(M) ≥R2(d log 6 + log(1/δ))/4nγ2.
MAX,0.5773067331670823,"Proof. We start by writing out the Hessian again but this time upper bounding it in semi-deﬁnite
ordering,"
MAX,0.5785536159600998,"∇2
θL = n
X i=1"
MAX,0.5798004987531172,"yixixT
i
⟨θ, xi⟩2 ≼ n
X i=1"
MAX,0.5810473815461347,"yixixT
i
γ2
≼ n
X i=1"
MAX,0.5822942643391521,"yiR2uiuT
i
γ2
:= L(θ)"
MAX,0.5835411471321695,Published as a conference paper at ICLR 2022
MAX,0.5847880299251871,"For a ﬁxed unit vector u ∈Sd−1
1
let us deﬁne,"
MAX,0.5860349127182045,"L(u, θ) := n
X i=1"
MAX,0.587281795511222,"yiR2uT uiuT
i u
γ2
:= n
X"
MAX,0.5885286783042394,"i=1
yivi,"
MAX,0.5897755610972568,"where vi = R2⟨ui, u⟩2/γ2. Proceeding as in Lemma 8 we conclude that, X"
MAX,0.5910224438902744,"i
yivi ∈SE "
MAX,0.5922693266832918,"ν2 =
X"
MAX,0.5935162094763092,"i
ν2
i , α = 0.56 max
i
vi !"
MAX,0.5947630922693267,",
(13)"
MAX,0.5960099750623441,"where νi = 2µiv2
i . This implies that wp atleast 1 −2δ/C we have, |
X"
MAX,0.5972568578553616,"i
yivi −
X"
MAX,0.5985037406483791,"i
µivi| ≤2 s"
MAX,0.5997506234413965,"X
µiv2
i log
C δ  if,
s"
MAX,0.600997506234414,"X
µiv2
i log
C δ"
MAX,0.6022443890274314,"
≤2(
X
µiv2
i )/(max
i
vi)."
MAX,0.6034912718204489,"Note that the above condition is mild and is satisﬁed when λmin
 P"
MAX,0.6047381546134664,"i µiuiuT
i

≥R2 log(C/δ)/4γ2,
as minu
P"
MAX,0.6059850374064838,"i µivi ≥λmin
 P"
MAX,0.6072319201995012,"i µiuiuT
i

. Here, C is a problem dependent constant that will be chosen
later."
MAX,0.6084788029925187,"Now consider an 1/2-net in ℓ2 norm over the surface unit sphere denoted by N(0.5, d). It is well
known that |N(ϵ, d)| ≤(3/ϵ)d. Now any z ∈Sd−1
1
can be written as z = x+u where x ∈N(0.5, d)
and u ∈Bd−1
0.5 . Therefore, we have that,"
MAX,0.6097256857855362,"λmax(L(θ)) = max
u∈Bd−1
1
L(u, θ) ≤
max
x∈N(ϵ,d) L(x, θ) + 1"
MAX,0.6109725685785536,"2 max
u∈Bd−1
1
L(u, θ)"
MAX,0.6122194513715711,"=⇒λmax(L(θ)) ≤2
max
x∈N(ϵ,d) L(x, θ)"
MAX,0.6134663341645885,"Thus we set C = 6d and obtain wp at least 1 −2δ,"
MAX,0.614713216957606,λmax(L(θ)) ≤2nR2
MAX,0.6159600997506235,"γ2 λmax(M),"
MAX,0.6172069825436409,"provided 4
p"
MAX,0.6184538653366584,λmax(M) log(C/δ) ≤λmax(M)/√n.
MAX,0.6197007481296758,Proof of Corollary 1. We show that the conditions of Theorem 2 hold.
MAX,0.6209476309226932,"Creating Nets: First we need to create an ϵ-net over the parameter space Θ. We start by creating an
ϵ-net over the sphere with radius w. Now suppose, ϵ < γ/2R. Then we remove all centers θc if ∃i
s.t ⟨θc, xi⟩< γ/2. This is a valid ϵ-net over Θ as all net partitions that are removed do not have any
points lying in Θ. In subsequent section, we will always follow this strategy to create ϵ-nets over
subsets of Θ."
MAX,0.6221945137157108,"Strong Convexity and Smoothness: From Lemma 8 and 9 we have that,"
MAX,0.6234413965087282,"β
α = w2R2 γ2
χ."
MAX,0.6246882793017456,with probability 1 −O(δ).
MAX,0.6259351620947631,KL Divergence: The L in Theorem 2 is bounded by 2wR2/γ according to Lemma 7.
MAX,0.6271820448877805,Combining the above into Theorem 2 and using the estimator in Section F we get our result.
MAX,0.628428927680798,Published as a conference paper at ICLR 2022
MAX,0.6296758104738155,"F
MEDIAN OF MEANS ESTIMATOR FOR POISSON"
MAX,0.6309226932668329,"In this section we will design a median of means estimator for the Poisson regression model based
on the estimator proposed in the work of Lugosi & Mendelson (2019b). Recall that we have a ﬁxed
design matrix X ∈Rn×d with rows x1, . . . , xn, and for each i ∈[n], yi is drawn from a Poisson
distribution with mean µi = θθθ∗· xi. We will further assume that the design matrix is chosen from an
(L, 4) hyper-contractive distribution. Mathematically this implies that for any unit vector u"
MAX,0.6321695760598504,"E[
 
Σ−1"
MAX,0.6334164588528678,"2 x · u
4] ≤L · E[
 
Σ−1"
MAX,0.6346633416458853,"2 x · u
2]2."
MAX,0.6359102244389028,"For simplicity we will assume that L = O(1). In the above deﬁnition note that E is the empirical
expectation over teh ﬁxed design. This is a benign assumption and for instance would be satisﬁed if
the design matrix is drawn from a sub-Gaussian distribution. In the general case, the obtained bounds
will scale with L. We have the following guarantee associated with Algorithm 1."
MAX,0.6371571072319202,"Algorithm 1: Median of Means Estimator
Input: Samples S = {(x1, y1), . . . , (xn, yn)}, conﬁdence parameter δ.
Step 1: Compute Σ = E[xx⊤]. Form S′ = {x′
1 = y1Σ−1"
MAX,0.6384039900249376,"2 x1, . . . , x′
n = ynΣ−1"
MAX,0.6396508728179551,"2 xn}.
Step 2: Randomly partition S′ into k blocks of size n/k each where k = 20⌈log( 1"
MAX,0.6408977556109726,"δ )⌉.
Step 3: Feed in the k blocks to the median-of-means estimator of Lugosi & Mendelson (2019b)
to get ˆv.
Step 4: Return ˆθ = Σ−1 2 ˆv."
MAX,0.64214463840399,"Theorem 4. There is an absolute constant c > 0 such that with probability at least 1−δ, Algorithm 1
outputs ˆθ such that"
MAX,0.6433915211970075,"∥ˆθ −θθθ∗∥2
Σ ≤c · ∥θθθ∗∥2 · λmax(Σ)
 d + log( 1"
MAX,0.6446384039900249,"δ )
n

.
(14)"
MAX,0.6458852867830424,"Proof. The proof is exactly along the lines of the proof of Theorem 1 in the work of (Lugosi &
Mendelson, 2019b) that we repeat here for the sake of completeness since the original theorem is not
explicitly stated for a ﬁxed design setting. To begin with notice that"
MAX,0.6471321695760599,E[yΣ−1
MAX,0.6483790523690773,2 x] = E[(θθθ∗· x)Σ−1
MAX,0.6496259351620948,"2 x)]
(15)"
MAX,0.6508728179551122,= E[(θθθ∗· x)Σ−1
MAX,0.6521197007481296,"2 x]
(16)"
MAX,0.6533665835411472,"= Σ
1
2θθθ∗.
(17)"
MAX,0.6546134663341646,"Hence if ˆv is the output of the median of the means estimator in Step 3 of Algorithm 1, then
the least squares error of ˆθ is exactly ∥ˆv −Σ
1
2θθθ∗∥2. For convenience deﬁne µ′ = Σ
1
2θθθ∗and
Σ′ = E[x′x′⊤] −Σ
1
2θθθ∗θθθ∗⊤Σ
1
2 . Exactly as in Lugosi & Mendelson (2019b) our goal is to show that
µ′ beats any other vector v in the median of means tournament if v is far away from µ′. To quantify
this deﬁne"
MAX,0.655860349127182,"r = max
 
400 r"
MAX,0.6571072319201995,Tr(Σ′)
MAX,0.6583541147132169,"n
, 4
√ 10 r"
MAX,0.6596009975062345,"λmax(Σ′) n

."
MAX,0.6608478802992519,"For a ﬁxed vector v of length r, and block Bj, µ′ beats v if −2k n X"
MAX,0.6620947630922693,"i∈Bj
(x′
i −µ′) · v + r > 0."
MAX,0.6633416458852868,"Let us denote by σi,j ∈{0, 1} a random variable representing whether data point i is in block j or
not. By Chebychev’s inequality we get that with probability at least 9/10, k n X"
MAX,0.6645885286783042,"i∈Bj
(x′
i −µ′) · v
 ≤k n √"
SX,0.6658354114713217,"10
sX"
SX,0.6670822942643392,"i
E[σ2
i,j((x′
i −µ′) · v)2]
(18) = k n √ 10 s np 1 n X"
SX,0.6683291770573566,"i
E[((x′
i −µ′) · v)2].
(19)"
SX,0.669576059850374,Published as a conference paper at ICLR 2022
SX,0.6708229426433915,"Here p is the probability of a point belonging to block k. Noting that np = Θ(n/k) we get that with
probability at least 9/10, k n X"
SX,0.672069825436409,"i∈Bj
(x′
i −µ′) · v
 ≤
√"
R,0.6733167082294265,10r r
R,0.6745635910224439,kλmax(Σ′)
R,0.6758104738154613,"n
.
(20)"
R,0.6770573566084788,"Applying binomial tail estimates we get that with probability at least 1 −e−k/180, µ′ beats v on at
least 8/10 of the blocks. By applying the covering argument verbatim as in the proof of Theorem 1
in Lugosi & Mendelson (2019b) we get that with probability at least 1 −δ, ˆv will satisfy"
R,0.6783042394014963,"∥ˆv −Σ
1
2θθθ∗∥2 ≤c · λmax(Σ′)
 d + log( 1"
R,0.6795511221945137,"δ )
n

."
R,0.6807980049875312,"Finally, it remains to bound the spectrum of Σ′. We have"
R,0.6820448877805486,Σ′ = E[y2Σ−1
R,0.683291770573566,2 xx⊤Σ−1
R,0.6845386533665836,"2 ] −Σ
1
2θθθ∗θθθ∗⊤Σ
1
2
(21)"
R,0.685785536159601,⪯E[((θ · x)2 + θ · x)Σ−1
R,0.6870324189526185,2 xx⊤Σ−1
R,0.6882793017456359,"2 ]
(22)
⪯T1 + T2,
(23) where"
R,0.6895261845386533,"T1 = E[
 
θθθ∗· x
2Σ−1"
R,0.6907730673316709,2 xx⊤Σ−1
R,0.6920199501246883,"2 ],
(24)"
R,0.6932668329177057,"T2 = E[
 
θθθ∗· x

Σ−1"
R,0.6945137157107232,2 xx⊤Σ−1
R,0.6957605985037406,"2 ].
(25)"
R,0.6970074812967582,"To bound T1, T2 we note that for any function m(x) we have"
R,0.6982543640897756,E[m(x)Σ−1
R,0.699501246882793,2 xx⊤Σ−1
R,0.7007481296758105,"2 ] ⪯
p"
R,0.7019950124688279,"E[m2(x)]I.
(26)"
R,0.7032418952618454,"Using the above inequality and the fact that the design matrix is (O(1), 4) hyper-contractive we get
that
p"
R,0.7044887780548629,"E[(θθθ∗· x)2] = O(∥θθθ∗∥
p"
R,0.7057356608478803,"λmax(Σ))
(27)
p"
R,0.7069825436408977,"E[(θθθ∗· x)4] = O(∥θθθ∗∥2λmax(Σ)).
(28)"
R,0.7082294264339152,Combining the above we get that
R,0.7094763092269327,"Σ′ ⪯T1 + T2
(29)
⪯O(λmax(Σ))I.
(30)"
R,0.7107231920199502,"G
1-D POISSON REGRESSION"
R,0.7119700748129676,"When the covariates are one dimensional, a sharper analysis can actually be performed to show that
the MLE dominates TMO in all regimes in the Poisson regression setting considered above.
Lemma 10. There exists an absolute constant c ≥2 such that for any δ ∈[ 1"
R,0.713216957605985,"nc , 1), it holds with
probability at least 1 −δ that,"
R,0.7144638403990025,"E(ˆθls) = 1 n n
X"
R,0.71571072319202,"i=1
(yi −ˆθls)2 ≤4 · |θ∗|"
R,0.7169576059850374,"n
·
Pn
i=1 |xi|3
Pn
i=1 x2
i
log
1 δ"
R,0.7182044887780549,"
.
(31)"
R,0.7194513715710723,The bound above is also tight i.e with constant probability it holds that
R,0.7206982543640897,"E(ˆθls) = 1 n n
X"
R,0.7219451371571073,"i=1
(yi −ˆθls)2 = Ω
|θ∗|"
R,0.7231920199501247,"n ·
Pn
i=1 |xi|3
Pn
i=1 x2
i
|
{z
}"
R,0.7244389027431422,:=B(ˆθls)
R,0.7256857855361596,"
.
(32)"
R,0.726932668329177,"We provide the proofs in later in the section. Having established the bound for least squares estimator,
we next prove the following upper bound on the mean squared error achieved by the MLE."
R,0.7281795511221946,Published as a conference paper at ICLR 2022
R,0.729426433915212,Theorem 5. There exists an absolute constant c ≥2 such that for any δ ∈[ 1
R,0.7306733167082294,"nc , 1), it holds with
probability at least 1 −δ that,"
R,0.7319201995012469,"E(ˆθmle) = 1 n n
X"
R,0.7331670822942643,"i=1
(yi −ˆθmle)2 ≤4 · |θ∗|"
R,0.7344139650872819,"n
·
 Pn
i=1 x2
i
Pn
i=1 |xi|
|
{z
}"
R,0.7356608478802993,:=B(ˆθmle)
R,0.7369077306733167,"
log
2 δ"
R,0.7381546134663342,"
.
(33)"
R,0.7394014962593516,"It is easy to see that the covariate dependent term in the bound on the mean squared error achieved by
ˆθmle (deﬁned in Eq. 32) is always better the corresponding term in the bound achieved by ˆθls (deﬁned
in Eq. 33). To see this notice that,"
R,0.7406483790523691,B(ˆθls)
R,0.7418952618453866,"B(ˆθmle)
= (Pn
i=1 |xi|3)(Pn
i=1 |xi|)
(Pn
i=1 x2
i )2
≥1.
(from Cauchy-Schwarz inequality.)"
R,0.743142144638404,"Furthermore, in many cases the bound achieved by ˆθmle can be signiﬁcantly better than the one
achieved by ˆθls. As an example consider a skewed data distribution where √n of the xi’s take a large
value of √n, while the remaining data points take a value of nϵ, where ϵ is a small constant. In this
case we have that,"
R,0.7443890274314214,B(ˆθls)
R,0.7456359102244389,"B(ˆθmle)
= (Pn
i=1 x3
i )(Pn
i=1 xi)
(Pn
i=1 x2
i )2
= Ω(nϵ)."
R,0.7468827930174564,Proof of Lemma 10. Notice that
R,0.7481296758104738,E(ˆθls) = 1
R,0.7493765586034913,"n(ˆθls −θ∗)2( n
X"
R,0.7506234413965087,"i=1
x2
i ).
(34)"
R,0.7518703241895262,"Hence, it is enough to bound the parameter distance, i.e., (ˆθls −θ∗)2. In order to do that we ﬁrst
notice that yixi is a sub-exponential random variable with parameters ν2
i , α where where ν2
i = 2µix2
i ,
µi = θ∗xi, and αi = 0.56xi (Rinaldo, 2019). In other words,"
R,0.7531172069825436,"yixi ∈SE(ν2
i , α),
(35)"
R,0.7543640897755611,"Thus from the bound on a sum of independent sub-exponential variables we have that,
X"
R,0.7556109725685786,"i
yixi ∈SE(ν2 =
X"
R,0.756857855361596,"i
ν2
i , α = 0.56 max
i
xi).
(36)"
R,0.7581047381546134,"Thus we have that, P  |
X"
R,0.7593516209476309,"i
yixi −
X"
R,0.7605985037406484,"i
µixi| ≥t ! ≤"
R,0.7618453865336658,"(
2 exp

−t2"
R,0.7630922693266833,"2ν2

if t ≤ν2"
R,0.7643391521197007,"α
2 exp
 
−t"
R,0.7655860349127181,"2α

otherwise"
R,0.7668329177057357,"This means, that w.p at least 1 −2δ, |
X"
R,0.7680798004987531,"i
yixi −
X"
R,0.7693266832917706,"i
µixi| ≤2 s (
X"
R,0.770573566084788,"i
µix2
i ) log
1 δ  if
s (
X"
R,0.7718204488778054,"i
µix2
i ) log
1 δ"
R,0.773067331670823,"
≤
2 P µix2
i
0.56 maxi xi"
R,0.7743142144638404,"The condition above is satisﬁed under our assumptions on xi and δ, thereby leading to the bound"
R,0.7755610972568578,"(ˆθls −θ∗) ≤2
p"
R,0.7768079800498753,w log(1/δ)
R,0.7780548628428927,"pP x3
i
P"
R,0.7793017456359103,"i x2
i
."
R,0.7805486284289277,The bound on the MSE claimed in the lemma then follows.
R,0.7817955112219451,Published as a conference paper at ICLR 2022
R,0.7830423940149626,Now we prove the lower bound. Again it is enough to show a lower bound on |ˆθls −θ∗|. Notice that
R,0.78428927680798,"ˆθls −θ∗=
Pn
i=1(yi −µi)xi
Pn
i=1 x2
i
.
(37)"
R,0.7855361596009975,"Deﬁne the random variable Z = Pn
i=1(yi −µi)xi. We will show anti-concentration for Z by
computing the hyper-contractivity of the random variable. Recall that a random varibale Z is η-HC
(hyper-contractive) if E[Z4] ≤η4E[Z2]2. Next we have"
R,0.786783042394015,"E[Z2] = E[ n
X"
R,0.7880299251870324,"i=1
(yi −µi)xi]2 = n
X"
R,0.7892768079800498,"i=1
µix2
i .
(38)"
R,0.7905236907730673,"E[Z4] = E[ n
X"
R,0.7917705735660848,"i=1
(yi −µi)xi]2 = n
X"
R,0.7930174563591023,"i=1
µi(1 + 3µi)x4
i + 2
X"
R,0.7942643391521197,"i̸=j
µiµjx2
i x2
j"
R,0.7955112219451371,":= ∆.
(39)"
R,0.7967581047381546,Hence Z is η-HC with η4 =
R,0.7980049875311721,"
Pn
i=1 µix2
i
2 ∆
."
R,0.7992518703241895,"From anti-concentration of hyper-contractive random variables (O’Donnell, 2014) we have"
R,0.800498753117207,P(|Z| ≥1 2 p
R,0.8017456359102244,"E[Z2]) ≥Ω(η4).
(40)"
R,0.8029925187032418,"Hence we get that P  | n
X"
R,0.8042394014962594,"i=1
(yi −µi)xi| ≥1 2"
R,0.8054862842892768,"v
u
u
t n
X"
R,0.8067331670822943,"i=1
µix2
i "
R,0.8079800498753117,≥Ω(η4) ≥
R,0.8092269326683291," Pn
i=1 µix2
i
2"
R,0.8104738154613467,"∆
(41) (42)"
R,0.8117206982543641,"Next notice that since µi ≥γ for all i (Assumption 1), we have that 1 + 3µi ≤(3 + 1"
R,0.8129675810473815,γ )µi. This
R,0.814214463840399,implies that ∆≤(3 + 1
R,0.8154613466334164,"γ )
 Pn
i=1 µix2
i
2
. Hence if γ is a constant then with probability at least"
R,0.816708229426434,"1
3+ 1"
R,0.8179551122194514,γ = Ω(1) we have
R,0.8192019950124688,"E(ˆθls) = Ω
 1"
R,0.8204488778054863,"n · (Pn
i=1 x3
i )
Pn
i=1 x2
i"
R,0.8216957605985037,"
.
(43)"
R,0.8229426433915212,Proof of Theorem 5. Recall that ˆθmle is deﬁned as
R,0.8241895261845387,"ˆθmle = argmin
θ∈Θ n
X"
R,0.8254364089775561,"i=1
θxi −yi log(θxi).
(44)"
R,0.8266832917705735,"Setting the gradient of the objective to zero, we get the following closed form expression for ˆθmle."
R,0.827930174563591,"ˆθmle =
Pn
i=1 yi
Pn
i=1 xi
.
(45)"
R,0.8291770573566085,Published as a conference paper at ICLR 2022
R,0.830423940149626,"Next, we note that Z = Pn
i=1 yi is a poission random random variable with parameter µ =
Pn
i=1 θ∗xi. From tail bounds for Poisson random variables (Klar, 2000) we have that for any ϵ > 0,"
R,0.8316708229426434,P[|Z −µ| > ϵ] ≤2e−ϵ2
R,0.8329177057356608,"µ+ϵ .
(46)"
R,0.8341645885286783,Taking ϵ = c√µ log( 1
R,0.8354114713216958,"δ ), we get that with probability at least 1 −δ, | n
X"
R,0.8366583541147132,"i=1
yi −µ| ∈
1"
R,0.8379052369077307,"2, 2
r"
R,0.8391521197007481,µ log(1
R,0.8403990024937655,"δ ),
(47)"
R,0.8416458852867831,provided that log( 1
R,0.8428927680798005,"δ ) < µ (that holds for our choice δ, once n is large enough). Hence we conclude
that with probability at least 1 −δ, the mean squared error of ˆθmle is bounded by"
R,0.844139650872818,"|ˆθmle −θ∗| =

Pn
i=1 θ∗xi
Pn
i=1 xi
−
Pn
i=1 yi
Pn
i=1 xi "
R,0.8453865336658354,"= O

q"
R,0.8466334164588528,µ log( 1
R,0.8478802992518704,"δ )
Pn
i=1 xi "
R,0.8491271820448878,"= O

q"
R,0.8503740648379052,|θ∗| log( 1
R,0.8516209476309227,"δ )
pPn
i=1 xi"
R,0.8528678304239401,"
.
(48)"
R,0.8541147132169576,The bound on the mean squared error follows from the above.
R,0.8553615960099751,"H
COMPETITIVENESS FOR PARETO REGRESSION"
R,0.8566084788029925,"We verify the conditions of Theorem 2 for the Pareto regression setting.
Lemma 11. Let f(·; θ) = Qn
i=1 p(·|xi; θ) where p is deﬁned in Eq. (4). If ∥θ −θ′∥2 ≤δ then,"
R,0.85785536159601,∥f(·; θ) −f(·; θ′)∥1 ≤ s 2bnδR γ
R,0.8591022443890274,"Proof. By Pinskers’ inequality we have the following chain,"
R,0.8603491271820449,"∥f(·; θ) −f(·; θ′)∥1 ≤
p"
R,0.8615960099750624,2DKL(f(·; θ); f(·; θ′)) =
R,0.8628428927680798,"v
u
u
t2 n
X"
R,0.8640897755610972,"i=1
DKL(p(·|xi; θ); p(·|xi; θ′)) ≤"
R,0.8653366583541147,"v
u
u
t n
X"
R,0.8665835411471322,"i=1
2b| log mi −log m′
i| ≤"
R,0.8678304239401496,"v
u
u
t n
X i=1"
R,0.8690773067331671,"2b|⟨θ, xi⟩−⟨θ′, xi⟩| γ ≤ s"
R,0.8703241895261845,"2bn∥θ −θ′∥2R γ
."
R,0.871571072319202,"The second inequality follows from the fact that log x is Lipschitz with parameter L if x > 1/L. The
last inequality follows from Cauchy-Schwarz and the norm bound on xi’s."
R,0.8728179551122195,"Now we prove the rest of the conditions.
Lemma 12. We have the following smoothness bound,"
R,0.8740648379052369,"λmax(∇2
θL) ≤(b −1)"
R,0.8753117206982544,"γ2
λmax(Σ)."
R,0.8765586034912718,Published as a conference paper at ICLR 2022
R,0.8778054862842892,"Proof. We start by writing out the Hessian,"
R,0.8790523690773068,"∇2
θL = n
X i=1"
R,0.8802992518703242,(b −1)
R,0.8815461346633416,"⟨θ, xi⟩2 xixT
i := L(θ)"
R,0.8827930174563591,"Using the fact that ⟨θ, xi⟩≥γ gives us the result."
R,0.8840399002493765,"Lemma 13. We have the following strong convexity bound,"
R,0.885286783042394,"λmin(∇2
θL) ≥(b −1)"
R,0.8865336658354115,w2R2 λmin(Σ).
R,0.8877805486284289,"Proof. It follows from the expression of the Hessian in Lemma 12 and using the fact ⟨θ, xi⟩≤
∥θ∥2∥xi∥2"
R,0.8890274314214464,Proof of Corollary 2. We show that the conditions of Theorem 2 hold.
R,0.8902743142144638,"Creating Nets: First we need to create an ϵ-net over the parameter space Θ. We start by creating an
ϵ-net over the sphere with radius w. Now suppose, ϵ < γ/2R. Then we remove all centers θc if ∃i
s.t ⟨θc, xi⟩< γ/2. This is a valid ϵ-net over Θ as all net partitions that are removed do not have any
points lying in Θ. In subsequent section, we will always follow this strategy to create ϵ-nets over
subsets of Θ."
R,0.8915211970074813,"Strong Convexity and Smoothness: From Lemma 13 and 12 we have that,"
R,0.8927680798004988,"β
α = w2R2 γ2
ζ."
R,0.8940149625935162,KL Divergence: The L in Theorem 2 is bounded by 2bR/γ according to Lemma 11.
R,0.8952618453865336,"Combining the above into Theorem 2 and using the estimator in (Hsu & Sabato, 2016) we get our
result."
R,0.8965087281795511,"I
MORE ON EXPERIMENTS"
R,0.8977556109725686,We provide more experimental details in this section.
R,0.899002493765586,"I.1
METRICS"
R,0.9002493765586035,The metrics and loss functions used are as follows:
R,0.9014962593516209,"MSE: The metric is
1
n n
X"
R,0.9027431421446384,"i=1
(ˆyi −yi)2."
R,0.9039900249376559,RMSE is just the square-root of this metric.
R,0.9052369077306733,"MAE: The metric is
1
n n
X"
R,0.9064837905236908,"i=1
|ˆyi −yi|."
R,0.9077306733167082,"WAPE: The metric is
Pn
i=1|ˆyi −yi|
Pn
i=1 |yi|
."
R,0.9089775561097256,"MAPE: The metric is
1
n X"
R,0.9102244389027432,i:yi̸=0
R,0.9114713216957606,1 −ˆyi yi .
R,0.912718204488778,Published as a conference paper at ICLR 2022
R,0.9139650872817955,"Quantile Loss: The reported metrics in Table 3 are the normalized quantile losses deﬁned as, n
X i=1"
R,0.9152119700748129,"2ρ(yi −ˆyi)Iyi≥ˆyi + 2ρ(ˆyi −yi)Iyi<ˆyi
Pn
i=1 |yi|
.
(49)"
R,0.9164588528678305,During training the unnormalized version is used for quantile regression.
R,0.9177057356608479,"Huber Loss: The loss is given by,"
R,0.9189526184538653,"Lδ(y, ˆy) ="
R,0.9201995012468828,"(
1
2(y −ˆy)2,
if |y −ˆy| ≤δ
δ|y −ˆy| −δ2"
R,0.9214463840399002,"2 ,
otherwise."
R,0.9226932668329177,"I.2
MORE DETAILS ABOUT THE MIXTURE DISTRIBUTION"
R,0.9239401496259352,"We use a mixture distribution between zero, a continuous extension of negative binomial (NB)
distribution and Pareto. The continuous extension of negative binomial is such that the p.d.f at y is
proportional to,"
R,0.9251870324189526,"p(y) ∝
Γ(n + k)
Γ(k + 1)Γ(n)(1 −p)npk"
R,0.92643391521197,"given parameters n and p. That is, the p.m.f of a regular discrete NB distribution is written in terms
of Gamma functions and then we extend that to non-integral points up to proportionality, such that
the measure sums to 1. This deﬁnition of NB is the standard implementation in Tensorﬂow (Abadi
et al., 2016; Dillon et al., 2017), in order to support both discrete and continuous data . It has been
used in many regression datasets before, especially in the ﬁeld of genomics where the collected data
is a discrete continuous mixture (Robinson & Smyth, 2008; Chen et al., 2016; McCarthy et al., 2012)."
R,0.9276807980049875,"Why not use a discrete-continuous mixture of zero, discrete NB and Pareto?"
R,0.928927680798005,"A mixture of these three distributions is a discrete continuous mixture, whose CDF is well-deﬁned. It
is possible to deﬁne a likelihood in a standard manner such that it has a component proportional to
the pdf of Pareto at all points and to this we add dirac delta functions proportional to magnitude of
the negative-binomial pmf at non-negative integral points. We have an extra mass at zero to account
for the zero component. The sum of the dirac masses along with the integral of the Pareto density
sums to one. However, such a likelihood is not very useful in practice. For instance, if the data is
mostly continuous but not heavy tailed, the log-likelihood would mostly have the Pareto component
(because non-integral points have no contribution from the other components) which is not a desirable
outcome, as a NB distribution can better model sub-Gaussian and sub-Exponential data in terms of
moments."
R,0.9301745635910225,"I.3
MAPPING OF OUTPUTS FOR ZNBP"
R,0.9314214463840399,"For the ZNBP model we require an output dimension size of 6. The ﬁrst three dimensions are mapped
through a softmax layer (Goodfellow et al., 2016) to mixture weights. The fourth dimension is
mapped to the ’n’ in negative-binomial likelihood through the link function,"
R,0.9326683291770573,"φ(x) =
x + 1,
if x > 0
1/(1 −x)
otherwise."
R,0.9339152119700748,"The ﬁfth dimension is mapped to ’p’ of the negative-binomial though the sigmoid function. The last
dimension is mapped to the scale parameter for the Pareto component using the φ(x) link function
above."
R,0.9351620947630923,"I.4
HARDWARE"
R,0.9364089775561097,"We use the Tesla V100 architecture GPU for our experiments. We use Intel Xeon Silver 4216 16-Core,
32-Thread, 2.1 GHz (3.2 GHz Turbo) CPU and our post-hoc inference for MLE is parallelized over
all the cores."
R,0.9376558603491272,Published as a conference paper at ICLR 2022
R,0.9389027431421446,"I.5
MORE DETAILS ABOUT INFERENCE FOR MLE"
R,0.940149625935162,"We follow the approach of monte-carlo sampling. For each inference sample x′, we generate 10k
samples from the learnt distribution p(·|x′; ˆθθθmle). Then we compute the correct statistics. For MSE,
RMSE the statistic is just the mean and for WAPE, MAE it is the median. For a quantile, it is the
corresponding quantile from the empirical distribution. For any loss of the form,"
R,0.9413965087281796,"ℓ(y, ˆy) =
1 −
y ˆy β"
R,0.942643391521197,"the optimal statistic is the median from the distribution proportional to yβp(y|x′; ˆθθθmle) (Gneiting,
2011). Note that the MAPE falls under the above with β = −1 and relative error corresponds to
β = 1. The statistic can be computed by importance weighing the empirical samples."
R,0.9438902743142145,"I.6
MODELS, HYPERPARAMETERS AND TUNING"
R,0.9451371571072319,"LSTM
LSTM
LSTM"
R,0.9463840399002493,Encoder
R,0.9476309226932669,"LSTM
LSTM
LSTM TMO Adj. Relu Relu"
R,0.9488778054862843,FC (1 Hidden)
R,0.9501246882793017,Link Function MLE
R,0.9513715710723192,Decoder
R,0.9526184538653366,FC (1 Hidden)
R,0.9538653366583542,"Figure 1: Time-Series Seq-2-Seq models. The MLE conﬁg is shown on the left and the TMO conﬁg
is shown at the right. The main difference is the output dimension and the loss function. In order to
keep the number of parameters the same, in the TMO model we add an extra layer of size 6 with
Relu activation (shown as Adj. (adjustment))"
R,0.9551122194513716,"For the time-series datasets, the model is a seq-2-seq model with one hidden layer LSTMs for both
the encoder and the decoder. The hidden layer size is h = 256. The output layer of the LSTM is
connected to the ﬁnal output layer through one hidden layer also having h neurons. Note that h was
tuned in the set [8, 16, 32, 64, 128, 256] and for all datasets and models 256 was chosen. In order to
be keep the number of parameters exactly the same, in the TMO models we add an extra layer with
ReLU with 6 neurons before the output."
R,0.956359102244389,"We tuned the learning rate for Adam optimizer in the range [1e-5, 1e-1] in log-scale. The batch-size
was also tuned in [64, 128, 256, 512] and the Huber-δ in [2i for i in range(-8, 8)]. The learning rate
was eventually chosen as 2.77e-3 for both datasets, as it was close to the optimal values selected for
all baseline models. The batch-size was chosen to be 512 and the Huber-δ was 32 and 64 for M5 and
Favorita respectively."
R,0.9576059850374065,"For the regression datasets the model is a DNN with one hidden layer of size 32. For the Bicycle
dataset the categorical features had there own embedding layer. The features [season, month, weekday,
weathersit] had embedding layer sizes [2,4,4,2]."
R,0.9588528678304239,"We tuned the learning rate for Adam optimizer in the range [1e-5, 1e-1] in log-scale. The batch-size
was also tuned in [64, 128, 256, 512] and the Huber-δ in [2i for i in range(-8, 8)]. The learning rate
was eventually chosen as 3e-3 for the gas turbine dataset based on the best perforamce of the baseline
models and 1.98e-3 for the gas turbine dataset. The batch-size was chosen to be 512 and the Huber-δ
was 128 and 32 for Bicyle Share and Gas turbine respectively."
R,0.9600997506234414,"For the ZNBP model we also tune the α parameter in the pareto component between [3, 4, 5]. The
value of 3 was selected for all datasets, except for gas turbine where we used α = 5."
R,0.9613466334164589,Published as a conference paper at ICLR 2022
R,0.9625935162094763,Linear Layer
R,0.9638403990024937,Linear layer Relu Relu
R,0.9650872817955112,Link Function
R,0.9663341645885287,Linear
R,0.9675810473815462,"MLE
TMO"
R,0.9688279301745636,"Figure 2: Fully connected network for regression models. The MLE conﬁg is shown on the left and
the TMO conﬁg is shown at the right. The main difference is output dimension and the loss function."
R,0.970074812967581,"Dataset
n
d"
R,0.9713216957605985,"M5
1879
256
Favorita
1653
256
Bicycle
584
32
Gas Turbine
22039
32"
R,0.972568578553616,"Table 5: Note that here d refers to the dimension of the last layer of the architecture used in the
respective datasets."
R,0.9738154613466334,"We used a batched version of GP-UCB (Srinivas et al., 2009) to tune the hyper-parameters. We used
Tensorﬂow (Abadi et al., 2016) to train our models. In Table 5, we provide the number of samples
and the dimension of the last layer in each of our datasets."
R,0.9750623441396509,"I.7
DATASETS"
R,0.9763092269326683,"For the Favorita and M5 dataset we used the product hierarchy over the item-level time series. Along
with the item-level (leaf) time-series, we also add all the higher-level (parent) time-series from the
product hierarchy (i.e. we add family and class level time-series for Favorita, and department and
category level time series for M5). The time-series for a parent time-series is obtained as the mean
of the time-series of its children. This is closer to a real forecasting setting in practice where one is
interested in all levels of the hierarchy. The metrics reported are over all the time-series (both parents
and leaves) treated equally. The history length for our predictions is set to 28."
R,0.9775561097256857,"For the M5 dataset the validation scores are computed using the predictions from time steps 1886
to 1899, and test scores on steps 1900 to 1913. For the Favorita dataset the validation scores are
computed using the predictions from time steps 1660 to 1673, and test scores on steps 1674 to 1687."
R,0.9788029925187033,"The train test splits are as mentioned in the main paper. For the Gas turbine dataset we use the ofﬁcial
train test split. For the Bicycle share data there is no ofﬁcial split, but we use a randomly chosen ﬁxed
10% as the test set for all our experiments."
R,0.9800498753117207,Published as a conference paper at ICLR 2022
R,0.9812967581047382,"I.8
ADDITIONAL EXPERIMENTS AND FIGURES"
R,0.9825436408977556,"In order to show the dependency of λmax(Σ) in the bound in Corollary 1 we perform a simulated
experiment. We generate a dataset with n = 5000 and d = 10 such that each coordinate of x
is distributed i.i.d from a uniform distribution between [0, U]. In out experiment, we vary U in
{1, 2, · · · , 9}. Then y is genearted from a Poisson distribution with rate ⟨θθθ∗, x⟩for a ﬁxed θθθ∗. This
varies λmax(Σ) which is equal to U 2/12. We train and validate on 2500 samples with early stopping
and plot the squared loss achieved on the test by the ˆθθθmle based estimator in Figure 3 versus λmax(Σ).
We can clearly see a linear relationship which further validates our theoretical results."
R,0.983790523690773,"In Figure 4, we plot the average training loss as a function of training iterations for the
MLE(ZNBP)model. We can see that the loss converges to a minima."
R,0.9850374064837906,"Figure 3: Test squared error versus λmax(Σ). We can clearly see a linear relationship. Each point in
the plot is averaged over 10 runs and we plot the standard error bars."
R,0.986284289276808,"Figure 4: We plot the average training loss for the MLE(ZNBP)model as a function of training
iterations. We can observe that the training curve converges."
R,0.9875311720698254,"J
EXTENDED DISCUSSION AND LIMITATIONS"
R,0.9887780548628429,"We advocate for MLE with a suitably chosen likelihood family followed by post-hoc inference tuned
to the target metric of interest, in favor of ERM directly with the target metric. On the theory side,
we prove a general result that shows competitiveness of MLE with any estimator for a target metric
under fairly general conditions. Application of the bound in the case of MSE for Poisson regression
and Pareto regression is shown. We believe that our general result is of independent interest and can
be used as a tool to prove competitiveness of MLE for a wide variety of problems. Such applications
can be an interesting direction of future work."
R,0.9900249376558603,"On the empirical side we show that a well designed mixture likelihood like the one from Section 5 can
adapt quite well to different datasets as the mixture weights are trained. As we have mentioned before,"
R,0.9912718204488778,Published as a conference paper at ICLR 2022
R,0.9925187032418953,"the MLE log-likelihood loss in such cases can be non-convex which might lead to some limitations in
terms of optimization. However, we observed that this is usually not a problem in practice and the
solutions that can be reached by mini-batch SGD can be quite good in terms of performance."
R,0.9937655860349127,"In conclusion we would recommend the following protocol for a practitioner based on our theoretical
and empirical observations:"
R,0.9950124688279302,"If the overall problem is convex for TMO but introducing a MLE loss makes the problem non-
convex, then the gains from the MLE approach might be neutralized by the added hardness of the
non-convexity introduced. An example of such a situation is TMO for minimizing square loss on
a linear function class, which is just least-squares linear regression, but introduction of a mixture
likelihood like the one in Section 4 makes the problem non-convex. In this case it might be better
to stick with TMO or at least proceed with caution with the MLE approach. Note that if the chosen
MLE retains the convexity of the problem, for example Poisson MLE in Section 4.1, then we would
still recommend going with the MLE approach."
R,0.9962593516209476,"However, in many practical scenarios when training using a deep network, the TMO approach is
non-convex to begin with, even when the target metric itself is something simple and convex like the
square loss. In such a case we would recommend the MLE approach with a likelihood class that can
capture inductive biases about the dataset. This is because both TMO and MLE are non-convex and it
is better to capitalize on the potential gains from the MLE approach."
R,0.9975062344139651,"Finally, note that the user can always choose between TMO and even between different likelihood
classes through cross-validation in a practical setting. If the practitioner would like to forgo the
decision making in choosing the likelihood class, we recommend using a versatile likelihood like the
mixture likelihood in Sections 5."
R,0.9987531172069826,We do not anticipate this work to have any negative social or ethical impact.
