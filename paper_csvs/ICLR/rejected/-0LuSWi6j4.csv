Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.008403361344537815,"Good likelihoods do not imply great sample quality. However, the precise manner
in which models trained to achieve good likelihoods fail at sample quality remains
poorly understood. In this work, we consider the task of image generative mod-
eling with variational autoencoders and posit that the nature of high-dimensional
image data distributions poses an intrinsic challenge. In particular, much of the
entropy in these natural image distributions is attributable to visually impercepti-
ble information. This signal dominates the training objective, giving models an
easy way to achieve competitive likelihoods without successful modeling of the
visually perceptible bits. Based on this hypothesis, we decompose the task of
generative modeling explicitly into two steps: we ﬁrst prioritize the modeling of
visually perceptible information to achieve good sample quality, and then subse-
quently model the imperceptible information—the bulk of the likelihood signal—
to achieve good likelihoods. Our work highlights the well-known adage that “not
all bits are created equal” and demonstrates that this property can and should be
exploited in the design of variational autoencoders."
INTRODUCTION,0.01680672268907563,"1
INTRODUCTION"
INTRODUCTION,0.025210084033613446,"The task of generative modeling of high-dimensional image data has inspired the development of
many successful deep generative models such as autoregressive models (Uria et al., 2016; Oord et al.,
2016b), ﬂow-based models (Dinh et al., 2014; 2016), generative adversarial networks (Goodfellow
et al., 2014), variational autoencoders (Kingma & Welling, 2013; Rezende et al., 2014), and diffusion
models (Ho et al., 2020; Song & Ermon, 2019). These models have since been applied to many other
modeling tasks such as speech synthesis (Oord et al., 2016a; Donahue et al., 2018), reinforcement
learning (Zhang et al., 2019; Levine et al., 2019), and scene-understanding (Eslami et al., 2018). In
this work, we focus on the application of VAEs to image modeling. Despite the success of variational
autoencoders (VAEs) in numerous ﬁelds (Akuzawa et al., 2018; Hsu et al., 2017; G´omez-Bombarelli
et al., 2018; Sultan et al., 2018; Van Hoof et al., 2016), the application of VAEs to image modeling
has been lukewarm at best—plagued by optimization issues (Rezende & Viola, 2018; Child, 2020;
Vahdat & Kautz, 2020) and models achieving good Evidence Lower Bounds (ELBOs) but poor
sample quality. Our work examines the phenomenon that good ELBOs do not impy great sample
quality in VAEs (Theis et al., 2015). We argue that the primary cause of this issue stems from the
model being overwhelmed by the vast volume of visually-imperceptible information contained in
natural image distributions. Our contributions are thus as follows."
INTRODUCTION,0.03361344537815126,"1. We analyze the rate (the amount of information encoded in the latent space) of VAEs trained
with different rate-distortion trade-offs and show that low-rate models can achieve percep-
tually high-quality reconstructions and sampling. In contrast, the standard ELBO objective
favors high-rate models with good reconstructions but poor sampling quality."
INTRODUCTION,0.04201680672268908,"2. However, low-rate models have much worse ELBOs due to poor modeling of the visu-
ally imperceptible information. To overcome this issue, we propose a two-stage training
process that trains a secondary high-rate model on top of the low-rate model. Since the sec-
ondary model is restricted to modeling visually imperceptible information, it can improve
the ELBO signiﬁcantly with minimal impact on the sample quality achieved by the initial
low-rate model."
INTRODUCTION,0.05042016806722689,Under review as a conference paper at ICLR 2021
THE CHALLENGE OF TRAINING VARIATIONAL AUTOENCODERS,0.058823529411764705,"2
THE CHALLENGE OF TRAINING VARIATIONAL AUTOENCODERS"
THE CHALLENGE OF TRAINING VARIATIONAL AUTOENCODERS,0.06722689075630252,"Figure 1: Mean-squared error, ELBO, and gradient norm for the standard VAE with trainable vari-
ance versus a VAE targeted to achieve the same distortion. The former exhibits unstable training."
THE CHALLENGE OF TRAINING VARIATIONAL AUTOENCODERS,0.07563025210084033,"(a) Standard VAE
(b) Distortion-Targeted VAE"
THE CHALLENGE OF TRAINING VARIATIONAL AUTOENCODERS,0.08403361344537816,"Figure 2: Samples from a standard VAE with a trainable variance versus a VAE targeted to achieve
the same distortion. Both have poor sample quality despite achieving good ELBOs (Figure 1)."
THE CHALLENGE OF TRAINING VARIATIONAL AUTOENCODERS,0.09243697478991597,"In this section, we highlight the challenges of applying variational autoencoders with Gaussian ob-
servation models to continuous data. We seek to train a model pθ(x, z0:T ) with observation x and
latents z0:T . We use a hierarchical prior pθ(z0:T ) = Q
t pθ(zt | z<t) and a Gaussian observation
model pθ(x | z0:T ). As many practitioners may be aware and as demonstrated in Figures 1 and 2,
training Gaussian observation VAEs with a trainable variance presents key challenges."
THE CHALLENGE OF TRAINING VARIATIONAL AUTOENCODERS,0.10084033613445378,"Optimization difﬁculty. As past works have noted, training variational autoencoders (VAE) is
challenging (Rezende & Viola, 2018; Vahdat & Kautz, 2020; Child, 2020). It seems, however,
that VAEs with trainable-variance Gaussian observation models exacerbates the issue. Even with
modeling heuristics such as gradient skipping and precision-weighted merging that are intended to
ease the optimization landscape (Child, 2020; Sønderby et al., 2016), we observe that optimization
easily becomes unstable (Figure 1). To overcome this issue, we instead train a distortion-targeted
VAE—more precisely, we use a mean-squared error reconstruction loss but tune the reconstruction
coefﬁcient to achieve the same mean-squared error as the standard VAE."
THE CHALLENGE OF TRAINING VARIATIONAL AUTOENCODERS,0.1092436974789916,"Good ELBOs but poor sample quality. In Figure 1, we show that both the standard VAE and
distortion-targeted VAE achieve small distortion and—as we shall see demonstrate in subsequent
sections—good Evidence Lower Bound (ELBO). However, this does not translate to good sample
quality (Figure 2)."
THE CHALLENGE OF TRAINING VARIATIONAL AUTOENCODERS,0.11764705882352941,"Since distortion-targeting appears to largely resolve the optimization issue, the primary goal of our
work is to explain—and ultimately ﬁx—why VAEs achieve good ELBOs but poor sample quality."
THE CHALLENGE OF TRAINING VARIATIONAL AUTOENCODERS,0.12605042016806722,Under review as a conference paper at ICLR 2021
PERCEPTUALLY IMPORTANT INFORMATION IN VAES,0.13445378151260504,"3
PERCEPTUALLY IMPORTANT INFORMATION IN VAES"
MEAN SAMPLING IN VAES,0.14285714285714285,"3.1
MEAN SAMPLING IN VAES"
MEAN SAMPLING IN VAES,0.15126050420168066,"(a) Standard Sampling
(b) Mean Sampling"
MEAN SAMPLING IN VAES,0.15966386554621848,"Figure 3: Distortion-targeted VAE with a non-negligible distortion target γ = 16, trained on SVHN.
Standard sampling from pθ(x) versus mean sampling from µθ(Z0:T ). Standard sampling with a
Gaussian observation model is noisier since it essentially adds Gaussian noise to the mean samples."
MEAN SAMPLING IN VAES,0.16806722689075632,"A common practice for sampling from variational autoencoders involve sampling the latent code
z0:T followed by decoding the mean of pθ(x | z0:T ), as described by
z0:T ∼pθ(z0:T )
(1)
µ(z0:T ) = Epθ(x|z0:T ) [x] .
(2)
In other words, letting Z0:T denote the random variable distributed according to pθ(z0:T ), prac-
titioners often report the sampling from µ(Z0:T ) rather than actually sampling x ∼pθ(x)—
especially when the choice of the decoder makes µ(z0:T ) easy to calculate, such as in the case
when pθ(x | z0:T ) is a Gaussian observation model. The design choice to sample the mean µ(Z0:T )
instead of the full generative process pθ(x) is of technical importance, since the implicitly deﬁned
distribution µ(Z0:T ) does not necessarily have a well-deﬁned density. This is because µ(Z0:T ) is
not guaranteed to cover a non-zero measure in X."
MEAN SAMPLING IN VAES,0.17647058823529413,"Crucially, the beneﬁt of sampling from µ(Z0:T ) instead of pθ(x) comes down to how we choose to
handle the information not encoded by Z0:T . Since the goal of pθ(x | z0:T ) is to model the residual
information in the data distribution lost during the lossy encoding via qφ(z0:T | x), the mean sample
µ(Z0:T ) chooses to handle this information by averaging over it. In contrast, pθ(x | z0:T ) attempts
to explicitly model this information. In the case where pθ(x | z0:T ) is a Gaussian observation model,
the variational autoencoder is essentially attempting to crudely model the residual information via
a Gaussian distribution. In Figure 3, we compare the resulting images from sampling pθ(x) versus
µ(Z0:T ). Whereas the former yields a noisy image, the latter averages over the noise and yields a
blurry image. In cases where we are willing to accept the information loss, mean sampling offers a
more visually-appealing alternative to sampling from pθ(x). We commit to using mean sampling in
all subsequent ﬁgures."
MEAN SAMPLING IN VAES,0.18487394957983194,"3.2
HOW MUCH INFORMATION ACTUALLY MATTERS PERCEPTUALLY?"
MEAN SAMPLING IN VAES,0.19327731092436976,"In this section, we scrutinize the behavior of the mean decoding function µ(·). In particular, we are
interested in bounding the mutual information I(X ; Z) necessary for the variational autoencoder
to achieve perceptually high-quality reconstructions. Recalling that µθ(z0:T ) := Epθ(x|z0:T )[x], we
consider the objective
minimize
θ,φ
Epdata(x) [DKL(qφ(z0:T | x) ∥pθ(z0:T ))]
(3)"
MEAN SAMPLING IN VAES,0.20168067226890757,"subject to Epdata(x)Eqφ(z0:T |x)∥x −µθ(z0:T )∥2 = γ.
(4)"
MEAN SAMPLING IN VAES,0.21008403361344538,Under review as a conference paper at ICLR 2021
MEAN SAMPLING IN VAES,0.2184873949579832,"(a) SVHN Data
(b) γ = 64, rate = 1.1
(c) γ = 32, rate = 7.47"
MEAN SAMPLING IN VAES,0.226890756302521,"(d) γ = 8, rate = 37.5
(e) γ = 2, rate = 116.0
(f) γ = 0.5, rate = 355.3"
MEAN SAMPLING IN VAES,0.23529411764705882,"Figure 4: Reconstructions from distortion-targeted VAEs for various choices of γ. Choosing γ = 8
sufﬁces to achieve a high-quality reconstruction and requires encoding signiﬁcantly less information
(rate) into the latent space than γ = 0.5."
MEAN SAMPLING IN VAES,0.24369747899159663,"(a) SVHN Data
(b) γ = 64, rate = 1.1
(c) γ = 32, rate = 7.47"
MEAN SAMPLING IN VAES,0.25210084033613445,"(d) γ = 8, rate = 37.5
(e) γ = 2, rate = 116.0
(f) γ = 0.5, rate = 355.3"
MEAN SAMPLING IN VAES,0.2605042016806723,"Figure 5: Samples from distortion-targeted VAEs for various choices of γ. Lower rates result in
easier modeling but blurrier samples. Higher rates make the modeling task much more challenging,
causing poor sample quality despite good reconstructions (Figure 4)"
MEAN SAMPLING IN VAES,0.2689075630252101,"We optimize this constrained objective using a Lagrange multiplier (Zhao et al., 2018). We note that
the rate term DKL(qφ(z0:T | x) ∥pθ(z0:T )) is an upper bound on the mutual information I(X ;Z0:T )
under the distribution deﬁned by pθ(x)qφ(z0:T | x) (Alemi et al., 2018). By varying the choice of γ,
we can modulate the amount of information about x that is stored in in the latent code z0:T . Figure 4
shows the reconstructions along with the rate of the learned model. Models with high rate contain
considerably more information about x in the latent space and adds additional burden that the prior"
MEAN SAMPLING IN VAES,0.2773109243697479,Under review as a conference paper at ICLR 2021
MEAN SAMPLING IN VAES,0.2857142857142857,"Figure 6: Rate, conditional log-likelihood ln pθ(x | z0:T ), and ELBO as a function of the distortion
achieved by various VAEs on SVHN. The ELBO favors high-rate models, which has poor sample
quality (Figure 5)."
MEAN SAMPLING IN VAES,0.29411764705882354,"pθ(z0:T ) needs to model correctly. In Figure 5, we show the corresponding samples generated by
µθ(Z0:T ) when Z0:T is distributed according to pθ(z0:T ). Note that smaller the rate, the more
distributionally-similar the reconstructions (Figure 4) are to the samples (Figure 5). Furthermore,
Figure 6 shows that the ELBO objective favors high-rate models, which—when naively trained—
have poor sample quality as shown in Figure 5."
PRIORITIZING THE BITS THAT MATTER,0.3025210084033613,"4
PRIORITIZING THE BITS THAT MATTER"
PRIORITIZING THE BITS THAT MATTER,0.31092436974789917,"In contrast to naive training, which favors extremely small distortions and high rates, our exper-
iments demonstrate that only a small amount of information is required to achieve high sample-
quality reconstructions. Furthermore, by limiting the the amount of information encoded into the
latent space, the full modeling capacity of the prior pθ(z0:T ) can be dedicated to the small amount of
information necessary for perceptual quality, which in turn improves the sample quality of µ(Z0:T )
when Z0:T is drawn from the prior pθ(z0:T )."
PRIORITIZING THE BITS THAT MATTER,0.31932773109243695,"The drawback, however, is that the large amount of entropy that remains is now crudely modeled
by a Gaussian observation model, causing the model to achieve poor likelihoods. To address this
issue, we now train a second variational autoencoder—with parameters (˜φ, ˜θ) and latent variables
˜z0:T —conditional on the mean decoding ˜x := µ(z0:T ) achieved by the ﬁrst model, via the objective"
PRIORITIZING THE BITS THAT MATTER,0.3277310924369748,"minimize
˜θ, ˜φ
Eqφ(x,z0:T )
h
DKL(q ˜φ(˜z0:T | x, ˜x) ∥p˜θ(˜z0:T | ˜x))
i
(5)"
PRIORITIZING THE BITS THAT MATTER,0.33613445378151263,"subject to Eqφ(x,z0:T )Eq ˜
φ(˜z0:T |x,˜x)∥x −µ˜θ(˜z0:T , ˜x)∥2 = ˜γ.
(6)"
PRIORITIZING THE BITS THAT MATTER,0.3445378151260504,"For national simplicity, we subsumed pdata by deﬁning qφ(x, z0:T ) := pdata(x)qφ(z0:T | x)."
PRIORITIZING THE BITS THAT MATTER,0.35294117647058826,"Our two key insights are as follows. First, the reconstructions from the ﬁrst variational autoencoder
augments the initial dataset by creating the paired data of the form (x, ˜x). This paired data subse-
quently deﬁnes a new conditional density estimation problem that seeks to predict x based on the
learned reconstructions. Second, the secondary variational autoencoder can trivially recover the ini-
tial distortion ˜γ = γ by setting the decoder as µ˜θ(˜z0:T , ˜x) = ˜x. And any further improvement is
thus bounded by γ. Furthermore, since the initial distortion γ was chosen to achieve perceptually-
acceptable reconstructions, the secondary variational autoencoder is thus restricted to making small
perturbations to the existing decoding ˜x."
PRIORITIZING THE BITS THAT MATTER,0.36134453781512604,"Taken together, we argue that since the perceptible bits are already accounted for by the primary
variational autoencoder, the secondary variational autoencoder is thus restricted to modeling the
remaining imperceptible bits. As such, we hypothesize that not only will the secondary variational
autoencoder improve upon the initial ELBO by replacing the crude Gaussian observation model with
a more expressive conditional VAE observation model, it will also do so with minimal impact to the
sample quality achieved by the primary variational autoencoder. Figure 7 conﬁrms this hypothesis,
and Figure 8 further demonstrates that the secondary variational autoencoder signiﬁcantly improves
the initial ELBO without sacriﬁcing sample quality."
PRIORITIZING THE BITS THAT MATTER,0.3697478991596639,Under review as a conference paper at ICLR 2021
PRIORITIZING THE BITS THAT MATTER,0.37815126050420167,"(a) SVHN. Row 1: γ = 6.6. Rows 2-5: γ = 6.6, ˜γ = [2, 0.58, 0.17, 0.05]. Row 6: γ = 0.05."
PRIORITIZING THE BITS THAT MATTER,0.3865546218487395,"(b) CelebA. Row 1: γ = 13.8. Rows 2-5: γ = 13.8, ˜γ = [3, 2, 0.58, 0.3]. Row 6: γ = 0.3."
PRIORITIZING THE BITS THAT MATTER,0.3949579831932773,"Figure 7: Samples from two-stage training for SVHN and CelebA. The ﬁrst row shows samples from
the primary VAE. The next four rows show samples from secondary VAEs with various choices of
˜γ. The secondary VAE makes near-imperceptible visual changes to the primary VAE’s samples. The
last row shows samples from a VAE that directly targets a small distortion. In contrast to two-stage
training, directly targeting a small distortion yields poorer sample quality."
PRIORITIZING THE BITS THAT MATTER,0.40336134453781514,"(a) SVHN
(b) CelebA"
PRIORITIZING THE BITS THAT MATTER,0.4117647058823529,"Figure 8: Test set ELBO from two-stage training as a function of the ﬁnal distortion achieved.
ELBOs for single-stage training are shown as horizontal lines for comparison. For the same ﬁnal
distortion target, two-stage training achieves similar ELBO as single-stage training while also having
better sample quality (Figure 7)."
PRIORITIZING THE BITS THAT MATTER,0.42016806722689076,Under review as a conference paper at ICLR 2021
NOT ALL MEAN-SQUARED ERRORS ARE CREATED EQUAL,0.42857142857142855,"5
NOT ALL MEAN-SQUARED ERRORS ARE CREATED EQUAL"
NOT ALL MEAN-SQUARED ERRORS ARE CREATED EQUAL,0.4369747899159664,"(a) SVHN
(b) CelebA"
NOT ALL MEAN-SQUARED ERRORS ARE CREATED EQUAL,0.44537815126050423,"Figure 9: Mean-Squared Error over the ﬁrst 100000 iterations. All models quickly converge to the
targeted distortion values."
NOT ALL MEAN-SQUARED ERRORS ARE CREATED EQUAL,0.453781512605042,"(a) γ = 16, iteration = 10000
(b) γ = 16, iteration = 270000"
NOT ALL MEAN-SQUARED ERRORS ARE CREATED EQUAL,0.46218487394957986,"(c) γ = 33, iteration = 10000
(d) γ = 33, iteration = 350000"
NOT ALL MEAN-SQUARED ERRORS ARE CREATED EQUAL,0.47058823529411764,"Figure 10: Reconstructions for SVHN and CelebA as a function of the distortion target γ and train-
ing iterations. Despite achieving the same distortion (Figure 9), earlier iterations capture higher-
frequency information."
NOT ALL MEAN-SQUARED ERRORS ARE CREATED EQUAL,0.4789915966386555,"While the primary focus of our work is on prioritizing the modeling of visually perceptible informa-
tion, we also observe over the course of training a notable characteristic of how mean-squared error
relates to the visual quality of the reconstructions. Figure 9 shows characteristic distortion over time
curves when training the primary variational autoencoder on SVHN and CelebA for various choices
of γ. All models quickly achieve the targeted distortion value; the remainder of the training thus
focuses on minimizing the rate while keeping the distortion constant."
NOT ALL MEAN-SQUARED ERRORS ARE CREATED EQUAL,0.48739495798319327,Under review as a conference paper at ICLR 2021
NOT ALL MEAN-SQUARED ERRORS ARE CREATED EQUAL,0.4957983193277311,"A natural consideration, then, is how the reconstruction differs visually near the beginning versus
the end of training despite having the exact same mean-squared error. In Figure 10, we see that the
reconstructions have much greater global consistency near the end of training than at the beginning
of training. We interpret these results as indicating that the models rapidly achieve the desired distor-
tion targets by encoding high-frequency information, but gradually shifts to using lower-frequency
information (and thus achieving lower rate) over the course of training."
NOT ALL MEAN-SQUARED ERRORS ARE CREATED EQUAL,0.5042016806722689,"To draw an analogy to principal components analysis, for the bottom k eigenvectors to achieve the
same reconstruction cost as the top-k′ eigenvectors, it tends to be the case that k ≫k′ for natural
images. Similarly, while there are many encoding schemes that can achieve the reconstruction cost,
they may differ greatly in the amount of information stored in the latent space. Masking out part
of the image, for example, is an encoding scheme that still preserves signiﬁcant amount of ﬁne-
grained texture information in the image and will likely result in a much higher rate. As a result, the
constrained objective favors blurrier, but globally-consistent reconstructions over (locally) sharper
but globally-inconsistent reconstructions."
RELATED WORK,0.5126050420168067,"6
RELATED WORK"
RELATED WORK,0.5210084033613446,"Given the extensive literature on deep generative models, we highlight several works most relevant
to our paper. Theis et al. (2015) pointed out that good likelihoods do not imply good sample qual-
ity, and showed that it is possible to construct pathological distributions that exhibit such behavior
(by mixing real with data with noise data). However, such a pathology does not reﬂect the failure
mode that variational autoencoders exhibit in practice, which is the focus of our work. Meng et al.
(2021); Dai & Wipf (2019) both proposed two-stage training processes, but with different underly-
ing motivations. Meng et al. (2021) proposed to train a primary model on a noised version of the
original dataset to ease optimization in autoregressive models, whereas our two-stage training proce-
dure explicitly characterizes the relation between likelihood and sample quality. Dai & Wipf (2019)
motivated their two-stage training process to handle data manifolds with intrinsic dimensionality
lower than the dimensionality of the ambient space X. In contrast, both our primary and secondary
models are deep hierarchical VAEs whose number of latent variables exceed the dimensionality of
X. Menick & Kalchbrenner (2018) also decomposes the original image modeling problem into
bits that are more versus less signiﬁcant. However, the Subscale Pixel Network prescribes a hand-
designed encoding scheme, whereas we allow the model to learn an encoding scheme based on what
minimizes rate most for a particular distortion target. Higgins et al. (2016) bears resemblance to
our primary model, but differ in how the distortion is targeted; whereas β-VAE tunes the coefﬁ-
cients of reconstruction and rate terms, we directly target mean-squared error value, which is more
interpretable. Finally, both VDVAE and NVAE (Child, 2020; Vahdat & Kautz, 2020) highlight the
optimization challenges in VAE training; our work builds on of these works by further addressing the
challenge of building VAEs that simultaneously achieve good sample quality and good likelihoods."
CONCLUSION,0.5294117647058824,"7
CONCLUSION"
CONCLUSION,0.5378151260504201,"In this work, we critically examined why variational autoencoders tend to exhibit good likelihoods
but poor sample quality for image generative modeling. Our experiments demonstrate that only a
small amount of information in natural image distributions is pertinent to perceptual sample quality.
However, conventional ELBO optimization does not distinguish the bits that matter perceptually
from the bits that do not. And since the imperceptible bits dominate the ELBO signal, the model
does not dedicate enough modeling capacity to the perceptually relevant information, thus caus-
ing poor sample quality. In addition to demonstrating this phenomenon, we also propose a simple
two-stage training procedure that prioritizes the modeling the perceptible information. By doing so,
we can reliably train VAEs with good sample quality while still achieving ELBOs comparable to
conventionally-trained VAEs. While our work focuses on variational autoencoders, we believe that
this general phenomenon of imperceptible information dominating the likelihood signal is relevant
to other likelihood-based models for image modeling. Our work demonstrates not all bits are cre-
ated equal, and we encourage researchers and practitioners alike to explicitly prioritize the bits that
“matter” in the design of deep generative models."
CONCLUSION,0.5462184873949579,Under review as a conference paper at ICLR 2021
REFERENCES,0.5546218487394958,REFERENCES
REFERENCES,0.5630252100840336,"Kei Akuzawa, Yusuke Iwasawa, and Yutaka Matsuo. Expressive speech synthesis via modeling
expressions with variational autoencoder. arXiv preprint arXiv:1804.02135, 2018."
REFERENCES,0.5714285714285714,"Alexander Alemi, Ben Poole, Ian Fischer, Joshua Dillon, Rif A Saurous, and Kevin Murphy. Fixing
a broken elbo. In International Conference on Machine Learning, pp. 159–168. PMLR, 2018."
REFERENCES,0.5798319327731093,"Rewon Child. Very deep vaes generalize autoregressive models and can outperform them on images.
arXiv preprint arXiv:2011.10650, 2020."
REFERENCES,0.5882352941176471,"Bin Dai and David Wipf. Diagnosing and enhancing vae models. arXiv preprint arXiv:1903.05789,
2019."
REFERENCES,0.5966386554621849,"Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components esti-
mation. arXiv preprint arXiv:1410.8516, 2014."
REFERENCES,0.6050420168067226,"Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv
preprint arXiv:1605.08803, 2016."
REFERENCES,0.6134453781512605,"Chris Donahue, Julian McAuley, and Miller Puckette. Adversarial audio synthesis. arXiv preprint
arXiv:1802.04208, 2018."
REFERENCES,0.6218487394957983,"SM Ali Eslami, Danilo Jimenez Rezende, Frederic Besse, Fabio Viola, Ari S Morcos, Marta Gar-
nelo, Avraham Ruderman, Andrei A Rusu, Ivo Danihelka, Karol Gregor, et al. Neural scene
representation and rendering. Science, 360(6394):1204–1210, 2018."
REFERENCES,0.6302521008403361,"Rafael G´omez-Bombarelli, Jennifer N Wei, David Duvenaud, Jos´e Miguel Hern´andez-Lobato,
Benjam´ın S´anchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel,
Ryan P Adams, and Al´an Aspuru-Guzik. Automatic chemical design using a data-driven contin-
uous representation of molecules. ACS central science, 4(2):268–276, 2018."
REFERENCES,0.6386554621848739,"Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information
processing systems, 27, 2014."
REFERENCES,0.6470588235294118,"Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner.
beta-vae: Learning basic visual concepts with a
constrained variational framework. 2016."
REFERENCES,0.6554621848739496,"Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. arXiv preprint
arXiv:2006.11239, 2020."
REFERENCES,0.6638655462184874,"Wei-Ning Hsu, Yu Zhang, and James Glass. Learning latent representations for speech generation
and transformation. arXiv preprint arXiv:1704.04222, 2017."
REFERENCES,0.6722689075630253,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.680672268907563,"Diederik P Kingma and Max Welling.
Auto-Encoding Variational Bayes.
arXiv preprint
arXiv:1312.6114, 2013."
REFERENCES,0.6890756302521008,"Nir Levine, Yinlam Chow, Rui Shu, Ang Li, Mohammad Ghavamzadeh, and Hung Bui. Predic-
tion, consistency, curvature: Representation learning for locally-linear control. arXiv preprint
arXiv:1909.01506, 2019."
REFERENCES,0.6974789915966386,"Chenlin Meng, Jiaming Song, Yang Song, Shengjia Zhao, and Stefano Ermon. Improved autore-
gressive modeling with distribution smoothing. arXiv preprint arXiv:2103.15089, 2021."
REFERENCES,0.7058823529411765,"Jacob Menick and Nal Kalchbrenner. Generating high ﬁdelity images with subscale pixel networks
and multidimensional upscaling. arXiv preprint arXiv:1812.01608, 2018."
REFERENCES,0.7142857142857143,"Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,
Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for
raw audio. arXiv preprint arXiv:1609.03499, 2016a."
REFERENCES,0.7226890756302521,Under review as a conference paper at ICLR 2021
REFERENCES,0.7310924369747899,"Aaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Ko-
ray Kavukcuoglu.
Conditional image generation with pixelcnn decoders.
arXiv preprint
arXiv:1606.05328, 2016b."
REFERENCES,0.7394957983193278,"Danilo Jimenez Rezende and Fabio Viola. Taming vaes. arXiv preprint arXiv:1810.00597, 2018."
REFERENCES,0.7478991596638656,"Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic Backpropagation And
Approximate Inference In Deep Generative Models. arXiv preprint arXiv:1401.4082, 2014."
REFERENCES,0.7563025210084033,"Casper Kaae Sønderby, Tapani Raiko, Lars Maaløe, Søren Kaae Sønderby, and Ole Winther. Ladder
variational autoencoders. Advances in neural information processing systems, 29:3738–3746,
2016."
REFERENCES,0.7647058823529411,"Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.
arXiv preprint arXiv:1907.05600, 2019."
REFERENCES,0.773109243697479,"Mohammad M Sultan, Hannah K Wayment-Steele, and Vijay S Pande. Transferable neural networks
for enhanced sampling of protein dynamics. Journal of chemical theory and computation, 14(4):
1887–1894, 2018."
REFERENCES,0.7815126050420168,"Lucas Theis, A¨aron van den Oord, and Matthias Bethge. A note on the evaluation of generative
models. arXiv preprint arXiv:1511.01844, 2015."
REFERENCES,0.7899159663865546,"Benigno Uria, Marc-Alexandre Cˆot´e, Karol Gregor, Iain Murray, and Hugo Larochelle. Neural
autoregressive distribution estimation. The Journal of Machine Learning Research, 17(1):7184–
7220, 2016."
REFERENCES,0.7983193277310925,"Arash Vahdat and Jan Kautz. Nvae: A deep hierarchical variational autoencoder. arXiv preprint
arXiv:2007.03898, 2020."
REFERENCES,0.8067226890756303,"Herke Van Hoof, Nutan Chen, Maximilian Karl, Patrick van der Smagt, and Jan Peters. Stable rein-
forcement learning with autoencoders for tactile and visual data. In 2016 IEEE/RSJ international
conference on intelligent robots and systems (IROS), pp. 3928–3934. IEEE, 2016."
REFERENCES,0.8151260504201681,"Marvin Zhang, Sharad Vikram, Laura Smith, Pieter Abbeel, Matthew Johnson, and Sergey Levine.
Solar: Deep structured representations for model-based reinforcement learning. In International
Conference on Machine Learning, pp. 7444–7453. PMLR, 2019."
REFERENCES,0.8235294117647058,"Shengjia Zhao, Jiaming Song, and Stefano Ermon. The information autoencoding family: A la-
grangian perspective on latent variable generative models.
arXiv preprint arXiv:1806.06514,
2018."
REFERENCES,0.8319327731092437,Under review as a conference paper at ICLR 2021
REFERENCES,0.8403361344537815,"A
ARCHITECTURE"
REFERENCES,0.8487394957983193,"A.1
PRIMARY VAE"
REFERENCES,0.8571428571428571,"Our model makes use of the VDVAE architecture (Child, 2020) in conjunction with precision-
weighted merging (Sønderby et al., 2016). For SVHN, we use a model with 48 layers of stochastic-
ity, consisting of two 1×1 resolution layers, four 4×4, ten 8×8, sixteen 16×16, and sixteen 32×32
layers. For CelebA, we simply added an additional eight 64 × 64 resolution layers. Each layer of
latent variables at any n × n resolution has 16 channels. The hidden dimensionality of the residual
network is ﬁxed at 128 channels, and all bottleneck residual blocks uses a 32 channel bottleneck."
REFERENCES,0.865546218487395,"A.2
SECONDARY VAE"
REFERENCES,0.8739495798319328,"The secondary VAE has the extract same depth-structure as the primary VAE and simply conditions
on the additional image ˜x generated by the primary VAE. For the inference model, we simply con-
catenated [x, ˜x] as the input. For the generative model, we use a U-Net structure to inject information
about ˜x at all resolutions in the generative process."
REFERENCES,0.8823529411764706,"B
TRAINING INFORMATION"
REFERENCES,0.8907563025210085,"B.1
DEQUANTIZATION"
REFERENCES,0.8991596638655462,"We dequantize both SVHN and CelebA by adding uniform noise in the interval (0, 1) to the original
[0, 255] pixel intensities. We only use the dequantized images when computing ln pθ(xdequantized |
z0:T ). The inference model is always given the clean image. This practice of feeding the clean
image into the inference model still admits a valid ELBO.1"
REFERENCES,0.907563025210084,"B.2
LAGRANGIAN"
REFERENCES,0.9159663865546218,"Letting R(φ, θ) and D(φ, θ) denote the rate and distortion (as measured by mean-squared error) re-
spectively, we can follow Zhao et al. (2018) and perform γ distortion-targeting by jointly optimizing
the following two objectives with gradient descent."
REFERENCES,0.9243697478991597,"minimize
φ,θ
R(φ, θ) + D(φ, θ) + λ · (D(φ, θ) −γ)
(7)"
REFERENCES,0.9327731092436975,"maximize
λ
λ(D(φ, θ) −γ).
(8)"
REFERENCES,0.9411764705882353,"In practice, we instead perform"
REFERENCES,0.9495798319327731,"minimize
φ,θ
(1 −λ) · R(φ, θ) + λ · D(φ, θ)
(9)"
REFERENCES,0.957983193277311,"maximize
λ
λ(D(φ, θ) −γ).
(10)"
REFERENCES,0.9663865546218487,"The general intuition still holds: if D > γ, then Equation (10) is maximized by increasing λ, which
in turn encourages Equation (9) to reduce D. We restrict λ ∈[0.0001, 0.9999] via projected gradient
descent and initialize at λ = 0.9999 to mimic KL annealing Sønderby et al. (2016)."
REFERENCES,0.9747899159663865,"B.3
OPTIMIZATION"
REFERENCES,0.9831932773109243,"We use the Adam optimizer (Kingma & Ba, 2014). We use a learning rate of 2 × 10−4 for both
Equations (9) and (10). However, we use (β1, β2) = (0.9, 0.999) for Equation (9) but (β1, β2) =
(0.0, 0.999) for Equation (10)."
REFERENCES,0.9915966386554622,1see http://ruishu.io/2018/03/19/bernoulli-vae/ for the lower bound interpretation.
