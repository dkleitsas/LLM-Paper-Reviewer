Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0021141649048625794,"Graph self-supervised learning has gained increasing attention due to its capacity
to learn expressive node representations. Many pretext tasks, or loss functions
have been designed from distinct perspectives. However, we observe that differ-
ent pretext tasks affect downstream tasks differently across datasets, which sug-
gests that searching over pretext tasks is crucial for graph self-supervised learn-
ing. Different from existing works focusing on designing single pretext tasks, this
work aims to investigate how to automatically leverage multiple pretext tasks ef-
fectively. Nevertheless, evaluating representations derived from multiple pretext
tasks without direct access to ground truth labels makes this problem challeng-
ing. To address this obstacle, we make use of a key principle of many real-world
graphs, i.e., homophily, or the principle that “like attracts like,” as the guidance
to effectively search various self-supervised pretext tasks. We provide theoretical
understanding and empirical evidence to justify the ﬂexibility of homophily in this
search task. Then we propose the AUTOSSL framework to automatically search
over combinations of various self-supervised tasks. By evaluating the framework
on 8 real-world datasets, our experimental results show that AUTOSSL can sig-
niﬁcantly boost the performance on downstream tasks including node clustering
and node classiﬁcation compared with training under individual tasks."
INTRODUCTION,0.004228329809725159,"1
INTRODUCTION"
INTRODUCTION,0.006342494714587738,"Graphs are pivotal data structures describing the relationships between entities in various domains
such as social media, biology, transportation and ﬁnancial systems (Wu et al., 2019b; Battaglia et al.,
2018). Due to their prevalence and rich descriptive capacity, pattern mining and discovery on graph
data is a prominent research area with powerful implications. As the generalization of deep neural
networks on graph data, graph neural networks (GNNs) have proved to be powerful in learning
representations for graphs and associated entities (nodes, edges, subgraphs), and they have been
employed in various applications such as node classiﬁcation (Kipf & Welling, 2016a; Veliˇckovi´c
et al., 2018), node clustering (Pan et al., 2018), recommender systems (Ying et al., 2018) and drug
discovery (Duvenaud et al., 2015)."
INTRODUCTION,0.008456659619450317,"In recent years, the explosive interest in self-supervised learning (SSL) has suggested its great
potential in empowering stronger neural networks in an unsupervised manner (Chen et al., 2020;
Kolesnikov et al., 2019; Doersch et al., 2015). Many self-supervised methods have also been de-
veloped to facilitate graph representation learning (Jin et al., 2020; Xie et al., 2021; Wang et al.,
2022) such as DGI (Veliˇckovi´c et al., 2019), PAR/CLU (You et al., 2020) and MVGRL (Hassani
& Khasahmadi, 2020). Given graph and node attribute data, they construct pretext tasks, which
are called SSL tasks, based on structural and attribute information to provide self-supervision for
training graph neural networks without accessing any labeled data. For example, the pretext task of"
INTRODUCTION,0.010570824524312896,∗Work partially done while author was on internship at Snap Inc.
INTRODUCTION,0.012684989429175475,Published as a conference paper at ICLR 2022
INTRODUCTION,0.014799154334038054,"Clu
DGI
PairDis PairSim
Par
Self-Supervised Task"
INTRODUCTION,0.016913319238900635,"Citeseer
Computers
Physics
Dataset 1 2 3 4 5"
INTRODUCTION,0.019027484143763214,(a) Node Clustering
INTRODUCTION,0.021141649048625793,"Clu
DGI
PairDis PairSim
Par
Self-Supervised Task"
INTRODUCTION,0.023255813953488372,"Citeseer
Computers
Physics
Dataset 1 2 3 4 5"
INTRODUCTION,0.02536997885835095,(b) Node Classiﬁcation 0.0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1.0
INTRODUCTION,0.02748414376321353,PairSim
INTRODUCTION,0.02959830866807611,"0.0
0.05"
INTRODUCTION,0.03171247357293869,"0.1
0.15"
INTRODUCTION,0.03382663847780127,"0.2
0.25"
INTRODUCTION,0.035940803382663845,"0.3
0.35"
INTRODUCTION,0.03805496828752643,"0.4
0.45"
INTRODUCTION,0.040169133192389,"0.5
0.55"
INTRODUCTION,0.042283298097251586,"0.6
0.65"
INTRODUCTION,0.04439746300211417,"0.7
0.75"
INTRODUCTION,0.046511627906976744,"0.8
0.85"
INTRODUCTION,0.048625792811839326,"0.9
0.95 1.0"
INTRODUCTION,0.0507399577167019,PairDis 0.36 0.37 0.38 0.39 0.40 0.41 0.42 0.43 0.44
INTRODUCTION,0.052854122621564484,(c) Combining Two Tasks
INTRODUCTION,0.05496828752642706,"0
100
200
300
400
Iteration 0.0 0.2 0.4 0.6 0.8 1.0"
INTRODUCTION,0.05708245243128964,Weight values
INTRODUCTION,0.05919661733615222,"Best weights: (0.433, 0.442)"
INTRODUCTION,0.0613107822410148,NMI: 0.434
INTRODUCTION,0.06342494714587738,"PairDis
PairSim"
INTRODUCTION,0.06553911205073996,"(d) AUTOSSL
Figure 1: (a)(b): Performance of 5 SSL tasks ranked best (1) to worst (5) by color on node clustering
and classiﬁcation, showing disparate performance across datasets and tasks. (c): Clustering perfor-
mance heatmap on Citeseer when combining 2 SSL tasks, PAIRSIM and PAIRDIS, with different
weights. (d) AUTOSSL’s search trajectory for task weights, achieving near-ideal performance."
INTRODUCTION,0.06765327695560254,"PAR is to predict the graph partitions of nodes. We examine how a variety of SSL tasks including
DGI, PAR, CLU, PAIRDIS (Peng et al., 2020) and PAIRSIM (Jin et al., 2020; 2021) perform over 3
datasets. Their node clustering and node classiﬁcation performance ranks are illustrated in Figure 1a
and 1b, respectively. From these ﬁgures, we observe that different SSL tasks have distinct down-
stream performance cross datasets. This observation suggests that the success of SSL tasks strongly
depends on the datasets and downstream tasks. Learning representations with a single task naturally
leads to ignoring useful information from other tasks. As a result, searching SSL tasks is crucial,
which motivates us to study on how to automatically compose a variety of graph self-supervised
tasks to learn better node representations."
INTRODUCTION,0.06976744186046512,"However, combining multiple different SSL tasks for unlabeled representation learning is immensely
challenging. Although promising results have been achieved in multi-task self-supervised learning
for computer vision, most of them assign equal weights to SSL tasks (Doersch & Zisserman, 2017;
Ren & Lee, 2018; Zamir et al., 2018). Such combination might not always yield better performance
than a single task, as different tasks have distinct importance according to speciﬁc dataset and down-
stream tasks. To illustrate this intuition, we combine two SSL tasks, PAIRDIS and PAIRSIM, with
varied weights and illustrate the corresponding node clustering performance in Figure 1c. It clearly
indicates that different choices of weights yield different performance. To circumvent this problem,
we could plausibly search different weights for SSL tasks to optimize downstream tasks. However,
to achieve such goal, we have two obstacles. First, the search space is huge, and thus search can be
highly expensive. Hence, it is desirable to automatically learn these weights. Second, searching for
optimal task weights typically requires guidance from downstream performance, which is naturally
missing under the unsupervised setting. Thus, how to design an unsupervised surrogate evaluation
measure that can guide the search process is necessary."
INTRODUCTION,0.07188160676532769,"It is evident that many real-world graphs such as friendship networks, citation networks, co-
authorship networks and co-purchase networks (McPherson et al., 2001; Shchur et al., 2018) satisfy
the homophily assumption, i.e., “like attracts like”, or that connected nodes tend to share the same
label. This is useful prior knowledge, as it directly relates the label information of downstream tasks
to the graph structure. In this work, we explicitly take advantage of this prior knowledge and as-
sume that the predicted labels from good node embeddings should also adhere to homophily. Given
the lack of ground-truth labels during SSL, we propose a pseudo-homophily measure to evaluate
the quality of the node embeddings trained from speciﬁc combinations of SSL task. With pseudo-
homophily, we are able to design an automated framework for SSL task search, namely AUTOSSL.
Our work makes three signiﬁcant contributions:"
INTRODUCTION,0.07399577167019028,"(1) To bridge the gap between unsupervised representation and downstream labels, we propose
pseudo-homophily to measure the quality of the representation. Moreover, given graphs with
high homophily, we theoretically show that pseudo-homophily maximization can help maximize
the upper bound of mutual information between pseudo-labels and downstream labels.
(2) Based on pseudo-homophily, we propose two strategies to efﬁciently search SSL tasks, one
employing evolution algorithm and the other performing differentiable search via meta-gradient
descent. AUTOSSL is able to adjust the task weights during search as shown in Figure 1d.
(3) We evaluate the proposed AUTOSSL by composing various individual tasks on 8 real-world
datasets. Extensive experiments have demonstrated that AUTOSSL can signiﬁcantly improve"
INTRODUCTION,0.07610993657505286,Published as a conference paper at ICLR 2022
INTRODUCTION,0.07822410147991543,"the performance of individual tasks on node clustering and node classiﬁcation (e.g., up to 10.0%
relative improvement on node clustering)."
BACKGROUND AND RELATED WORK,0.080338266384778,"2
BACKGROUND AND RELATED WORK"
BACKGROUND AND RELATED WORK,0.0824524312896406,"Graph Neural Networks. Graph neural networks (GNNs) are powerful tools for extracting useful
information from graph data (Liu et al., 2021; Wu et al., 2019b; Kipf & Welling, 2016a; Veliˇckovi´c
et al., 2018; Hamilton et al., 2017; Kipf & Welling, 2016b; Pan et al., 2018; Liu et al., 2020). They
aim to learn a mapping function fθ parameterized by θ to map the input graph into a low-dimensional
space. Most graph neural networks follow a message passing scheme (Gilmer et al., 2017) where
the node representation is obtained by aggregating the representation of its neighbors.
Self-Supervised Learning in GNNs. Graph neural networks have achieved superior performance
in various applications; but they also require costly task-dependent labels to learn rich represen-
tations. To alleviate the need for the huge amount of labeled data, recent studies have employed
self-supervised learning in graph neural networks to provide additional supervision (Jin et al., 2020;
Veliˇckovi´c et al., 2019; You et al., 2020; Hassani & Khasahmadi, 2020; Hu et al., 2019; Qiu et al.,
2020; Zhu et al., 2020b). Speciﬁcally, those SSL methods construct a pre-deﬁned pretext task to
assign pseudo-labels for unlabeled nodes/graphs and then train the model on the designed pretext
task to learn representations. A recent work JOAO (You et al., 2021) on graph contrastive learn-
ing is proposed to automatically select data augmentation and focuses on graph classiﬁcation task.
Another related work is AUX-TS (Han et al., 2021), which also adaptively combines different SSL
tasks but the combination happens at the ﬁne-tuning stage and thus requires label information.
Multi-Task Self-Supervised Learning. Our work is also related to multi-task self-supervised learn-
ing (Doersch & Zisserman, 2017; Ren & Lee, 2018; Zamir et al., 2018). Most of them assume the
tasks with equal weights and perform training under the supervised setting. But our work learns
different weights for different tasks and does not require access to labeled data.
Automated Loss Function Search. Tremendous efforts have been paid to automate every aspect
of machine learning applications (Yao et al., 2018; Liu et al., 2018; Zhao et al., 2021b), such as
feature engineering, model architecture search and loss function search. Among them, our work is
highly related to loss function search (Zhao et al., 2021a; Xu et al., 2018; Wang et al., 2020; Li et al.,
2019). However, these methods are developed under the supervised setting and not applicable in self-
supervised learning. Another related work, ELo (Piergiovanni et al., 2020), evolves multiple self-
supervised losses based on Zipf distribution matching for action recognition. However, it is designed
exclusively for image data and not applicable to non-grid graph-structured data. The problem of
self-supervised loss search for graphs remains rarely explored. To bridge the gap, we propose an
automated framework for searching SSL losses towards graph data in an unsupervised manner."
AUTOMATED SELF-SUPERVISED TASK SEARCH WITH AUTOSSL,0.08456659619450317,"3
AUTOMATED SELF-SUPERVISED TASK SEARCH WITH AUTOSSL"
AUTOMATED SELF-SUPERVISED TASK SEARCH WITH AUTOSSL,0.08668076109936575,"In this section, we present the proposed framework of automated self-supervised task search, namely
AUTOSSL. Given a graph G, a GNN encoder fθ(·) and a set of n self-supervised losses (tasks)
{ℓ1, ℓ2, . . . , ℓn}, we aim at learning a set of loss weights {λ1, λ2, . . . , λn} such that fθ(·) trained
with the weighted loss combination Pn
i=1 λiℓi can extract meaningful features from the given graph
data. The key challenge is how to mathematically deﬁne “meaningful features”. If we have the
access to the labels of the downstream task, we can deﬁne “meaningful features” as the features
(node embeddings) that can have high performance on the given downstream task. Then we can
simply adopt the downstream performance as the optimization goal and formulate the problem of
automated self-supervised task search as follows:"
AUTOMATED SELF-SUPERVISED TASK SEARCH WITH AUTOSSL,0.08879492600422834,"min
λ1,··· ,λn H(fθ∗(G)),
s.t. θ∗= arg min
θ
L(fθ, {λi}, {ℓi}) = arg min
θ n
X"
AUTOMATED SELF-SUPERVISED TASK SEARCH WITH AUTOSSL,0.09090909090909091,"i=1
λiℓi(fθ(G)),
(1)"
AUTOMATED SELF-SUPERVISED TASK SEARCH WITH AUTOSSL,0.09302325581395349,"where H denotes the quality measure for the obtained node embeddings, and it can be any metric
that evaluates the downstream performance such as cross-entropy loss for the node classiﬁcation
task. However, under the self-supervised setting, we do not have the access to labeled data and thus
cannot employ the downstream performance to measure the embedding quality. Instead, we need
an unsupervised quality measure H to evaluate the quality of obtained embeddings. In a nutshell,
one challenge of automated self-supervised learning is: how to construct the goal of automated task
search without the access to label information of the downstream tasks."
AUTOMATED SELF-SUPERVISED TASK SEARCH WITH AUTOSSL,0.09513742071881606,Published as a conference paper at ICLR 2022
PSEUDO-HOMOPHILY,0.09725158562367865,"3.1
PSEUDO-HOMOPHILY"
PSEUDO-HOMOPHILY,0.09936575052854123,"Most common graphs adhere to the principle of homophily, i.e., “birds of a feather ﬂock together”
(McPherson et al., 2001), which suggests that connected nodes often belong to the same class; e.g.
connected publications in a citation network often have the same topic, and friends in social networks
often share interests (Newman, 2018). Homophily is often calculated as the fraction of intra-class
edges in a graph (Zhu et al., 2020a). Formally, it can be deﬁned as follows,
Deﬁnition 1 (Homophily). The homophily of a graph G with node label vector y is given by"
PSEUDO-HOMOPHILY,0.1014799154334038,"h(G, y) = 1 |E| X"
PSEUDO-HOMOPHILY,0.10359408033826638,"(v1,v2)∈E 1(yv1 = yv2),
(2)"
PSEUDO-HOMOPHILY,0.10570824524312897,where yvi indicates node vi’s label and 1(·) is the indicator function.
PSEUDO-HOMOPHILY,0.10782241014799154,"We calculate the homophily for seven widely used datasets as shown in Appendix A and we ﬁnd that
they all have high homophily, e.g., 0.93 in the Physics dataset. Considering the high homophily
in those datasets, intuitively the predicted labels from the extracted node features should also have
high homophily. Hence, the prior information of graph homophily in ground truth labels can serve
as strong guidance for searching combinations of self-supervised tasks. As mentioned before, in
self-supervised tasks, the ground truth labels are not available. Motivated by DeepCluster (Caron
et al., 2018) which uses the cluster assignments of learned features as pseudo-labels to train the
neural network, we propose to calculate the homophily based on the cluster assignments, which we
term as pseudo-homophily. Speciﬁcally, we ﬁrst perform k-means clustering on the obtained node
embeddings to get k clusters. Then the cluster results are used as the pseudo labels to calculate
homophily based on Eq. (2). Note that though many graphs in the real world have high homophily,
there also exist heterophily graphs (Zhu et al., 2020a; Pei et al., 2020) which have low homophily.
We include a discussion on the homophily assumption in Appendix D."
PSEUDO-HOMOPHILY,0.10993657505285412,"Theoretical analysis. In this work, we propose to achieve self-supervised task search via maxi-
mizing pseudo-homophily. To understand its rationality, we develop the following theorem to show
that pseudo-homophily maximization is related to the upper bound of mutual information between
pseudo-labels and ground truth labels.
Theorem 1. Suppose that we are given with a graph G = {V, E}, a pseudo label vector A ∈
{0, 1}N and a ground truth label vector B ∈{0, 1}N deﬁned on the node set. We denote the
homophily of A and B over G as hA and hB, respectively. If the classes in A and B are balanced and
hA < hB, the following results hold: (1) the mutual information between A and B, i.e., MI(A,B),
has an upper bound UA,B, where UA,B =
1
N

2∆log( 4"
PSEUDO-HOMOPHILY,0.11205073995771671,N ∆) + 2( N
PSEUDO-HOMOPHILY,0.11416490486257928,"2 −∆) log
  4 N ( N"
PSEUDO-HOMOPHILY,0.11627906976744186,"2 −∆)

with
∆= (hB−hA)|E|"
DMAX,0.11839323467230443,"2dmax
and dmax denoting the largest node degree in the graph; (2) if hA < hA′ < hB, we
have UA,B < UA′,B."
DMAX,0.12050739957716702,Proof. The detailed proof of this theorem can be found in Appendix B.
DMAX,0.1226215644820296,"The above theorem suggests that a larger difference between pseudo-homophily and real homophily
results in a lower upper bound of mutual information between the pseudo-labels and ground truth
labels. Thus, maximizing pseudo-homophily is to maximize the upper bound of mutual informa-
tion between pseudo-labels and ground truth labels, since we assume high homophily of the graph.
Notably, while maximizing the upper bound does not guarantee the optimality of the mutual infor-
mation, we empirically found that it works well in increasing the NMI value in different datasets,
showing that it provides the right direction to promote the mutual information."
SEARCH ALGORITHMS,0.12473572938689217,"3.2
SEARCH ALGORITHMS"
SEARCH ALGORITHMS,0.12684989429175475,"In the last subsection, we have demonstrated the importance of maximizing pseudo-homophily.
Thus in the optimization problem of Eq. (1), we can simply set H to be negative pseudo-homophily.
However, the evaluation of a speciﬁc task combination involves ﬁtting a model and evaluating its
pseudo-homophily, which can be highly expensive. Therefore, another challenge for automated self-
supervised task search is how to design an efﬁcient algorithm. In the following, we introduce the
details of the search strategies designed in this work, i.e. AUTOSSL-ES and AUTOSSL-DS."
SEARCH ALGORITHMS,0.12896405919661733,"3.2.1
AUTOSSL-ES: EVOLUTIONARY STRATEGY
Evolution algorithms are often used in automated machine learning such as hyperparameter tuning
due to their parallelism nature by design (Loshchilov & Hutter, 2016). In this work, we employ"
SEARCH ALGORITHMS,0.13107822410147993,Published as a conference paper at ICLR 2022
SEARCH ALGORITHMS,0.1331923890063425,"the covariance matrix adaptation evolution strategy (CMA-ES) (Hansen et al., 2003), a state-of-the-
art optimizer for continuous black-box functions, to evolve the combined self-supervised loss. We
name this self-supervised task search approach as AUTOSSL-ES. In each iteration of CMA-ES, it
samples a set of candidate solutions (i.e., task weights {λi}) from a multivariate normal distribution
and trains the GNN encoder under the combined loss function. The embeddings from the trained
encoder are then evaluated by H. Based on H, CMA-ES adjusts the normal distribution to give
higher probabilities to good samples that can potentially produce a lower value of H. Note that we
constrain {λi} in [0, 1] and sample 8 candidate combinations for each iteration, which is trivially
parallelizable as every candidate combination can be evaluated independently."
SEARCH ALGORITHMS,0.13530655391120508,"3.2.2
AUTOSSL-DS: DIFFERENTIABLE SEARCH VIA META-GRADIENT DESCENT"
SEARCH ALGORITHMS,0.13742071881606766,"While the aforementioned AUTOSSL-ES is parallelizable, the search cost is still expensive because
it requires to evaluate a large population of candidate combinations where every evaluation involves
ﬁtting the model in large training epochs. Thus, it is desired to develop gradient-based search meth-
ods to accelerate the search process. In this subsection, we introduce the other variant of our pro-
posed framework, AUTOSSL-DS, which performs differentiable search via meta-gradient descent.
However, pseudo-homophily is not differentiable as it is based on hard cluster assignments from
k-means clustering. Next, we will ﬁrst present how to make the clustering process differentiable and
then introduce how to perform differentiable search."
SEARCH ALGORITHMS,0.13953488372093023,"Soft Clustering. Although k-means clustering assigns hard assignments of data samples to clus-
ters, it can be viewed as a special case of Gaussian mixture model which makes soft assignments
based on the posterior probabilities (Bishop, 2006). Given a Gaussian mixture model with centroids
{c1, c2, . . . , ck} and ﬁxed variances σ2, we can calculate the posterior probability as follows:"
SEARCH ALGORITHMS,0.1416490486257928,"p (x | ci) =
1
√"
SEARCH ALGORITHMS,0.14376321353065538,"2πσ2 exp

−∥x −ci∥2 2σ2"
SEARCH ALGORITHMS,0.14587737843551796,"
,
(3)"
SEARCH ALGORITHMS,0.14799154334038056,"where x is the feature vector of data samples. By Bayes rule and considering an equal prior, i.e.,
p(c1) = p(c2) = . . . = p(ck), we can compute the probability of a feature vector x belonging to a
cluster ci as:"
SEARCH ALGORITHMS,0.15010570824524314,"p (ci | x) =
p (ci) p (x | ci)
Pk
j p (cj) p (x | cj)
=
exp −(x−ci)2"
SEARCH ALGORITHMS,0.1522198731501057,"2σ2
Pk
j=1 exp −(x−cj)2"
SEARCH ALGORITHMS,0.1543340380549683,"2σ2
.
(4)"
SEARCH ALGORITHMS,0.15644820295983086,"If σ →0, we can obtain the hard assignments as the k-means algorithm. As we can see, the
probability of each feature vector belonging to a cluster reduces to computing the distance between
them. Then we can construct our homophily loss function as follows:"
SEARCH ALGORITHMS,0.15856236786469344,"H(fθ∗(G)) =
1
k|E| k
X i=1 X"
SEARCH ALGORITHMS,0.160676532769556,"(v1,v2)∈E
ℓ(p(ci | xv1), p(ci | xv2)) ,
(5)"
SEARCH ALGORITHMS,0.16279069767441862,"where ℓis a loss function measuring the difference between the inputs. With soft assignments, the
gradient of H w.r.t. θ becomes tractable."
SEARCH ALGORITHMS,0.1649048625792812,"Search via Meta Gradient Descent.
We now detail the differentiable search process for
AUTOSSL-DS. A naive method to tackle bilevel problems is to alternatively optimize the inner
and outer problems through gradient descent. However, we cannot perform gradient descent for the
outer problem in Eq. (1) where H is not directly related to {λi}. To address this issue, we can uti-
lize meta-gradients, i.e., gradients w.r.t. hyperparameters, which have been widely used in solving
bi-level problems in meta learning (Finn et al., 2017; Z¨ugner & G¨unnemann, 2019). To obtain meta-
gradients, we need to backpropagate through the learning phase of the neural network. Concretely,
the meta-gradient of H with respect to {λi} is expressed as"
SEARCH ALGORITHMS,0.16701902748414377,"∇meta
{λi} := ∇{λi}H(fθ∗(G))
s.t. θ∗= optθ(L(fθ, {λi, ℓi})),
(6)"
SEARCH ALGORITHMS,0.16913319238900634,"where optθ stands for the inner optimization that obtains θ∗and it is typically multiple steps of
gradient descent. As an illustration, we consider optθ as T + 1 steps of vanilla gradient descent with
learning rate ϵ,
θt+1 = θt −ϵ∇θtL(fθt, {λi, ℓi}).
(7)
By unrolling the training procedure, we can express meta-gradient as"
SEARCH ALGORITHMS,0.17124735729386892,"∇meta
{λi} := ∇{λi}H(fθT (G)) = ∇fθT H(fθT (G)) · [∇{λi}fθT (G) + ∇θT fθT (G)∇{λi}θT ],
(8)"
SEARCH ALGORITHMS,0.1733615221987315,Published as a conference paper at ICLR 2022
SEARCH ALGORITHMS,0.17547568710359407,"with ∇{λi}θT = ∇{λi}θT −1 −ϵ∇{λi}∇θT −1L(fθT −1, {λi, ℓi}). Since ∇{λi}fθT (G) = 0, we have
∇meta
{λi} := ∇{λi}H(fθT (G)) = ∇fθT H(fθT (G)) · ∇θT fθT (G)∇{λi}θT .
(9)"
SEARCH ALGORITHMS,0.17758985200845667,"Note that θT −1 also depends on the task weights {λi} (see Eq. (7)). Thus, its derivative w.r.t. the
task weights chains back until θ0. By unrolling all the inner optimization steps, we can obtain the
meta-gradient ∇meta
{λi} and use it to perform gradient descent on {λi} to reduce H:"
SEARCH ALGORITHMS,0.17970401691331925,"{λi} ←−{λi} −η∇meta
{λi},
(10)
where η is the learning rate for meta-gradient descent (outer optimization)."
SEARCH ALGORITHMS,0.18181818181818182,"However, if we use the whole training trajectory θ0, θ1, . . . , θT to calculate the precise meta-
gradient, it would have an extremely high memory footprint since we need to store θ0, θ1, . . . , θT in
memory. Thus, inspired by DARTS (Liu et al., 2018), we use an online updating rule where we only
perform one step gradient descent on θ and then update {λi} in each iteration. During the process,
we constrain {λi} in [0, 1] and dynamically adjust the task weights in a differentiable manner. The
detailed algorithm for AUTOSSL-DS is summarized in Appendix C."
EXPERIMENTAL EVALUATION,0.1839323467230444,"4
EXPERIMENTAL EVALUATION"
EXPERIMENTAL EVALUATION,0.18604651162790697,"In this section, we empirically evaluate the effectiveness of the proposed AUTOSSL on self-
supervised task search on real-world datasets. We aim to answer four questions as follows. Q1:
Can AUTOSSL achieve better performance compared to training on individual SSL tasks? Q2:
How does AUTOSSL compare to other unsupervised and supervised node representation learning
baselines? Q3: Can we observe relations between AUTOSSL’s pseudo-homophily objective and
downstream classiﬁcation performance? and Q4: How do the SSL task weights, pseudo-homophily
objective, and downstream performance evolve during AUTOSSL’s training?"
EXPERIMENTAL SETTING,0.18816067653276955,"4.1
EXPERIMENTAL SETTING"
EXPERIMENTAL SETTING,0.19027484143763213,"Since our goal is to enable automated combination search and discovery of SSL tasks, we use 5
such tasks including 1 contrastive learning method and 4 predictive methods – (1) DGI (Veliˇckovi´c
et al., 2019): it is a contrastive learning method maximizing the mutual information between graph
representation and node representation; (2) CLU (You et al., 2020), it predicts partition labels from
Metis graph partition (Karypis & Kumar, 1998); (3) PAR (You et al., 2020), it predicts clustered
labels from k-means clustering on node features; (4) PAIRSIM (Jin et al., 2020; 2021), it predicts
pairwise feature similarity between node pairs and (5) PAIRDIS (Peng et al., 2020), it predicts
shortest path length between node pairs. The proposed AUTOSSL framework automatically learns
to jointly leverage the 5 above tasks and carefully mediate their inﬂuence. We also note that (1) the
recent contrastive learning method, MVGRL (Hassani & Khasahmadi, 2020), needs to deal with a
dense diffusion matrix and is prohibitively memory/time-consuming for larger graphs; thus, we only
include it as a baseline to compare as shown in Table 2; and (2) the proposed framework is general
and it is straightforward to combine other SSL tasks."
EXPERIMENTAL SETTING,0.19238900634249473,"We perform experiments on 8 real-world datasets widely used in the literature (Yang et al., 2016;
Shchur et al., 2018; Mernyei & Cangea, 2020; Hu et al., 2020), i.e., Physics, CS, Photo,
Computers, WikiCS, Citeseer, CoraFull, and ogbn-arxiv. To demonstrate the effec-
tiveness of the proposed framework, we follow (Hassani & Khasahmadi, 2020) and evaluate all
methods on two different downstream tasks: node clustering and node classiﬁcation. For the task of
node clustering, we perform k-means clustering on the obtained embeddings. We set the number of
clusters to the number of ground truth classes and report the normalized mutual information (NMI)
between the cluster results and ground truth labels. Regarding the node classiﬁcation task, we train
a logistic regression model on the obtained node embeddings and report the classiﬁcation accuracy
on test nodes. Note that labels are never used for self-supervised task search. All experiments are
performed under 5 different random seeds and results are averaged. Following DGI and MVGRL,
we use a simple one-layer GCN (Kipf & Welling, 2016a) as our encoder and set the size of hidden
dimensions to 512. We set 2σ2 = 0.001 and use L1 loss in the homophily loss function throughout
the experiments. Further details of experimental setup can be found in Appendix A."
PERFORMANCE COMPARISON WITH INDIVIDUAL TASKS,0.1945031712473573,"4.2
PERFORMANCE COMPARISON WITH INDIVIDUAL TASKS"
PERFORMANCE COMPARISON WITH INDIVIDUAL TASKS,0.19661733615221988,"To answer Q1, Table 1 summarizes the results for individual self-supervised tasks and AUTOSSL
under the two downstream tasks, i.e., node clustering and node classiﬁcation. From the table, we"
PERFORMANCE COMPARISON WITH INDIVIDUAL TASKS,0.19873150105708245,Published as a conference paper at ICLR 2022
PERFORMANCE COMPARISON WITH INDIVIDUAL TASKS,0.20084566596194503,"Table 1: Performance comparison of self-supervised tasks (losses) on node clustering and node clas-
siﬁcation. The NMI rows indicate node clustering performance; ACC rows indicate node classiﬁca-
tion accuracy (%); P-H stands for pseudo-homophily. AUTOSSL regularly outperforms individual
pretext tasks. (Bold: best in all methods; Underline: best in individual tasks). Blue and red numbers
indicate the statistically signiﬁcant improvements over the best individual task, via paired t-test at
level 0.05 and 0.1, respectively (same for Table 2 and Table 3)."
PERFORMANCE COMPARISON WITH INDIVIDUAL TASKS,0.2029598308668076,"Dataset
Metric
Self-Supervised Task
AUTOSSL"
PERFORMANCE COMPARISON WITH INDIVIDUAL TASKS,0.20507399577167018,"CLU
PAR
PAIRSIM
PAIRDIS
DGI
ES
DS"
PERFORMANCE COMPARISON WITH INDIVIDUAL TASKS,0.20718816067653276,"WikiCS
NMI
0.177±0.02
0.262±0.02
0.341±0.01
0.169±0.04
0.310±0.02
0.366±0.01
0.344±0.02
ACC
74.19±0.21
75.81±0.17
75.80±0.17
75.28±0.08
75.49±0.17
76.80±0.13
76.58±0.28
P-H
0.549
0.567
0.693
0.463
0.690
0.751
0.749"
PERFORMANCE COMPARISON WITH INDIVIDUAL TASKS,0.20930232558139536,"Citeseer
NMI
0.318±0.00
0.416±0.00
0.428±0.01
0.404±0.01
0.439±0.00
0.449±0.01
0.449±0.01
ACC
63.17±0.19
69.25±0.51
71.36±0.42
70.72±0.53
71.64±0.44
72.14±0.41
72.00±0.32
P-H
0.787
0.916
0.885
0.901
0.934
0.943
0.934"
PERFORMANCE COMPARISON WITH INDIVIDUAL TASKS,0.21141649048625794,"Computers
NMI
0.171±0.00
0.433±0.00
0.387±0.01
0.300±0.01
0.318±0.02
0.447±0.01
0.448±0.01
ACC
75.20±0.20
87.26±0.15
82.64±1.15
85.20±0.41
83.45±0.54
87.26±0.64
88.18±0.43
P-H
0.240
0.471
0.314
0.206
0.298
0.503
0.511"
PERFORMANCE COMPARISON WITH INDIVIDUAL TASKS,0.2135306553911205,"CoraFull
NMI
0.128±0.00
0.498±0.00
0.409±0.02
0.406±0.01
0.462±0.01
0.506±0.01
0.500±0.00
ACC
44.93±0.53
57.54±0.32
56.23±0.59
58.48±0.80
60.42±0.39
61.01±0.50
61.10±0.68
P-H
0.494
0.887
0.649
0.728
0.888
0.903
0.895"
PERFORMANCE COMPARISON WITH INDIVIDUAL TASKS,0.2156448202959831,"CS
NMI
0.658±0.01
0.767±0.01
0.749±0.01
0.635±0.03
0.747±0.01
0.772±0.01
0.771±0.01
ACC
88.58±0.27
92.75±0.12
92.68±0.09
89.56±1.01
90.91±0.51
93.26±0.16
93.35±0.09
P-H
0.845
0.882
0.886
0.786
0.883
0.895
0.890"
PERFORMANCE COMPARISON WITH INDIVIDUAL TASKS,0.21775898520084566,"Physics
NMI
0.692±0.00
0.704±0.00
0.674±0.00
0.420±0.05
0.670±0.00
0.725±0.00
0.726±0.00
ACC
93.60±0.07
95.07±0.06
95.05±0.10
91.69±1.02
94.03±0.15
95.57±0.02
95.13±0.36
P-H
0.911
0.913
0.905
0.821
0.906
0.921
0.923"
PERFORMANCE COMPARISON WITH INDIVIDUAL TASKS,0.21987315010570824,"Photo
NMI
0.327±0.00
0.509±0.01
0.439±0.04
0.293±0.08
0.376±0.03
0.560±0.04
0.515±0.03
ACC
90.33±0.22
91.75±0.25
91.13±0.35
91.97±0.32
92.08±0.37
92.04±0.89
92.71±0.32
P-H
0.434
0.602
0.428
0.327
0.401
0.791
0.616"
PERFORMANCE COMPARISON WITH INDIVIDUAL TASKS,0.2219873150105708,"ogbn-arxiv
NMI
0.305±0.01
0.410±0.01
0.379±0.01
0.314±0.01
0.319±0.01
0.424±0.00
0.417±0.00
ACC
66.68±0.34
67.90±0.10
67.82±0.20
67.63±0.13
67.95±0.56
68.31±0.05
69.13±0.04
P-H
0.441
0.660
0.482
0.326
0.390
0.830
0.780"
PERFORMANCE COMPARISON WITH INDIVIDUAL TASKS,0.22410147991543342,"Average Rank
NMI
6.3
3.5
4.1
6.5
4.6
1.3
1.5
ACC
6.8
3.9
4.9
5.4
3.9
1.8
1.4"
PERFORMANCE COMPARISON WITH INDIVIDUAL TASKS,0.226215644820296,"make several observations. Obs. 1: individual self-supervised tasks have different node clustering
and node classiﬁcation performance for different datasets. For example, in Photo, DGI achieves
the highest classiﬁcation accuracy while PAR achieves the highest clustering performance; CLU
performs better than PAIRDIS in both NMI and ACC on Physics while it cannot outperform
PAIRDIS in WikiCS, Citeseer, Computers and CoraFull. This observation suggests the
importance of searching suitable SSL tasks to beneﬁt downstream tasks on different datasets. Obs.
2: Most of the time, combinations of SSL tasks searched by AUTOSSL can consistently improve the
node clustering and classiﬁcation performance over the best individual task on the all datasets. For
example, the relative improvement over the best individual task on NMI from AUTOSSL-ES is 7.3%
for WikiCS and 10.0% for Photo, and its relative improvement on ACC is 1.3% for WikiCS.
These results indicate that composing multiple SSL tasks can help the model encode different types
of information and avoid overﬁtting to one single task. Obs. 3: We further note that individual
tasks resulted in different pseudo-homophily as shown in the P-H rows of Table 1. Among them,
CLU tends to result in a low pseudo-homophily and often performs much worse than other tasks in
node clustering task, which supports our theoretical analysis in Section 3.1. It also demonstrates the
necessity to increase pseudo-homophily as the two variants of AUTOSSL effectively search tasks
that lead to higher pseudo-homophily. Obs. 4: The performance of AUTOSSL-ES and AUTOSSL-
DS is close when their searched tasks lead to similar pseudo-homophily: the differences in pseudo-
homophily, NMI and ACC are relative smaller in datasets other than Photo and Computers.
It is worth noting that sometimes AUTOSSL-DS can even achieve higher pseudo-homophily than
AUTOSSL-ES. This indicates that the online updating rule for {λi} in AUTOSSL-DS not only can
greatly reduce the searching time but also can generate good task combinations. In addition to
efﬁciency, we highlight another major difference between them: AUTOSSL-ES directly ﬁnds the
best task weights while AUTOSSL-DS adjusts the task weights to generate appropriate gradients to
update model parameters. Hence, if we hope to ﬁnd the best task weights and retrain the model, we
should turn to AUTOSSL-ES. More details on their differences can be found in Appendix E.3."
PERFORMANCE COMPARISON WITH INDIVIDUAL TASKS,0.22832980972515857,Published as a conference paper at ICLR 2022
PERFORMANCE COMPARISON WITH SUPERVISED AND UNSUPERVISED BASELINES,0.23044397463002114,"4.3
PERFORMANCE COMPARISON WITH SUPERVISED AND UNSUPERVISED BASELINES"
PERFORMANCE COMPARISON WITH SUPERVISED AND UNSUPERVISED BASELINES,0.23255813953488372,"To answer Q2, we compare AUTOSSL with representative unsupervised and supervised node rep-
resentation learning baselines. Speciﬁcally, for node classiﬁcation we include 4 unsupervised base-
lines, i.e., GAE (Kipf & Welling, 2016b), VGAE (Kipf & Welling, 2016b), ARVGA (Pan et al., 2018)
and MVGRL, and 2 supervised baselines, i.e. GCN and GAT (Veliˇckovi´c et al., 2018). We also
provide the logistic regression performance on raw features and embeddings generated by a ran-
domly initialized encoder, named as Raw-Feat and Random-Init, respectively. Note that the two
supervised baselines, GCN and GAT, use label information for node representation learning in an
end-to-end manner, while other baselines and AUTOSSL do not leverage label information to learn
representations. The average performance and variances are reported in Table 2. From the table, we
ﬁnd that AUTOSSL outperforms unsupervised baselines in all datasets except Citeseer while the
performance on Citeseer is still comparable to the state-of-the-art contrastive learning method
MVGRL. When compared to supervised baselines, AUTOSSL-DS outperforms GCN and GAT in 4
out of 8 datasets, e.g., a 1.7% relative improvement over GAT on Computers. AUTOSSL-ES also
outperforms GCN and GAT in 3/4 out of 8 datasets. In other words, our unsupervised representation
learning AUTOSSL can achieve comparable performance with supervised representation learning
baselines. In addition, we use the same unsupervised baselines for node clustering and report the
results in Table 3. Both AUTOSSL-ES and AUTOSSL-DS show highly competitive clustering per-
formance. For instance, AUTOSSL-ES achieves 22.2% and 27.5% relative improvement over the
second best baseline on Physics and WikiCS; AUTOSSL-DS also achieves 22.2% and 19.8%
relative improvement on these two datasets. These results further validate that composing SSL tasks
appropriately can produce expressive and generalizable representations."
PERFORMANCE COMPARISON WITH SUPERVISED AND UNSUPERVISED BASELINES,0.2346723044397463,"Table 2: Node classiﬁcation accuracy (%). The last two rows are supervised baselines. AUTOSSL
consistently outperforms alternative self-supervised approaches, and frequently outperforms super-
vised ones. (Bold/Underline: best/runner-up among self-supervised approaches)"
PERFORMANCE COMPARISON WITH SUPERVISED AND UNSUPERVISED BASELINES,0.23678646934460887,"Model
WikiCS
Citeseer
Computers
CoraFull
CS
Physics
Photo
ogbn-arxiv
Avg. Rank"
PERFORMANCE COMPARISON WITH SUPERVISED AND UNSUPERVISED BASELINES,0.23890063424947147,"Random-Init
75.07±0.15
64.06±2.28
74.42±0.29
45.07±0.38
28.57±0.90
53.33±0.52
87.01±0.39
67.55±0.27
6.0
Raw-Feat
72.06±0.03
61.50±0.00
74.15±0.48
37.17±0.30
87.12±0.42
92.81±0.24
79.03±0.37
51.07±0.00
7.0
GAE
74.85±0.24
64.76±1.35
80.25±0.42
57.85±0.29
92.35±0.09
94.66±0.10
91.51±0.39
52.57±0.04
4.1
VGAE
74.16±0.16
67.50±0.42
81.05±0.41
53.72±0.30
92.15±0.16
94.58±0.17
88.98±1.05
52.00±0.19
4.6
ARVGA
71.64±1.03
46.88±2.15
67.61±0.92
45.20±1.33
87.26±1.07
93.84±0.13
77.74±1.16
31.57±2.96
7.1
MVGRL
75.89±0.12
72.36±0.49
84.66±0.62
60.56±0.33
90.18±0.19
94.30±0.20
92.49±0.40
OOM
3.1
AUTOSSL-ES
76.80±0.13
72.14±0.41
87.26±0.64
61.01±0.50
93.26±0.16
95.57±0.02
92.04±0.89
68.31±0.05
1.9
AUTOSSL-DS
76.58±0.28
72.00±0.32
88.18±0.43
61.10±0.68
93.35±0.09
95.13±0.36
92.71±0.32
69.13±0.04
1.5"
PERFORMANCE COMPARISON WITH SUPERVISED AND UNSUPERVISED BASELINES,0.24101479915433405,"GCN
76.42±0.02
71.26±0.15
87.53±0.21
63.77±0.37
93.04±0.09
95.66±0.15
93.09±0.11
71.74±0.29
-
GAT
77.30±0.01
71.00±0.62
86.74±0.69
63.73±0.43
92.53±0.19
95.54±0.08
92.30±0.28
71.46±0.34
-"
PERFORMANCE COMPARISON WITH SUPERVISED AND UNSUPERVISED BASELINES,0.24312896405919662,"Table 3: Clustering performance (NMI). AUTOSSL embeddings routinely exhibit superior NMI to
alternatives. (Bold: best; Underline: runner-up)."
PERFORMANCE COMPARISON WITH SUPERVISED AND UNSUPERVISED BASELINES,0.2452431289640592,"Model
WikiCS
Citeseer
Computers
CoraFull
CS
Physics
Photo
ogbn-arxiv
Avg. Rank"
PERFORMANCE COMPARISON WITH SUPERVISED AND UNSUPERVISED BASELINES,0.24735729386892177,"Random-Init
0.107±0.02
0.354±0.03
0.155±0.01
0.318±0.01
0.716±0.02
0.551±0.01
0.246±0.04
0.306±0.01
6.4
Raw-Feat
0.182±0.00
0.316±0.00
0.166±0.00
0.215±0.00
0.642±0.00
0.489±0.00
0.282±0.00
0.150±0.01
7.1
GAE
0.243±0.02
0.313±0.02
0.441±0.00
0.485±0.00
0.731±0.01
0.545±0.06
0.616±0.01
0.325±0.01
4.3
VGAE
0.261±0.01
0.364±0.01
0.423±0.00
0.453±0.01
0.733±0.00
0.563±0.02
0.530±0.04
0.311±0.01
4.0
ARVGA
0.287±0.02
0.191±0.02
0.237±0.01
0.301±0.01
0.616±0.03
0.526±0.05
0.301±0.01
0.201±0.01
6.4
MVGRL
0.263±0.01
0.452±0.01
0.244±0.00
0.400±0.01
0.740±0.01
0.594±0.00
0.344±0.04
OOM
4.3
AUTOSSL-ES
0.366±0.01
0.449±0.01
0.447±0.01
0.506±0.01
0.772±0.01
0.725±0.00
0.560±0.04
0.424±0.00
1.6
AUTOSSL-DS
0.344±0.02
0.449±0.01
0.448±0.01
0.500±0.00
0.771±0.01
0.726±0.00
0.515±0.03
0.417±0.00
2.1"
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.24947145877378435,"4.4
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY"
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.25158562367864695,"In this subsection, we investigate the relation between downstream performance and pseudo-
homophily and correspondingly answer Q3. Speciﬁcally, we use the candidate task weights sampled
in the AUTOSSL-ES searching trajectory, and illustrate their node clustering (NMI) and node clas-
siﬁcation performance (ACC) with respect to their pseudo-homophily. The results on Computers
and WikiCS are shown in Figure 2 and results for other datasets are shown in Appendix E.1. We
observe that the downstream performance tends to be better if the learned embeddings tend to have
higher pseudo-homophily. We also can observe that clustering performance has a clear relation with
pseudo-homophily for all datasets. Hence, the results empirically support our theoretical analysis in
Section 3.1 that lower pseudo-homophily leads to a lower upper bound of mutual information with
ground truth labels. While classiﬁcation accuracy has a less evident pattern, we can still observe that
higher accuracy tends to concentrate on the high pseudo-homophily regions for 5 out of 7 datasets."
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.2536997885835095,Published as a conference paper at ICLR 2022
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.2558139534883721,"0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
Pseudo-homophily 0.1 0.2 0.3 0.4"
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.25792811839323465,Node clustering NMI
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.26004228329809725,(a) Computers: NMI
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.26215644820295986,"0.64
0.66
0.68
0.70
0.72
0.74
Pseudo-homophily 0.26 0.28 0.30 0.32 0.34 0.36"
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.2642706131078224,Node clustering NMI
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.266384778012685,(b) WikiCS: NMI
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.26849894291754756,"0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
Pseudo-homophily 80 82 84 86 88"
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.27061310782241016,Node classification accuracy
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.2727272727272727,(c) Computers: ACC
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.2748414376321353,"0.64
0.66
0.68
0.70
0.72
0.74
Pseudo-homophily 75.50 75.75 76.00 76.25 76.50 76.75 77.00 77.25"
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.2769556025369979,Node classification accuracy
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.27906976744186046,(d) WikiCS: ACC
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.28118393234672306,Figure 2: Relationship between downstream performance and pseudo-homophily.
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.2832980972515856,"Clu
Par
PairSim PairDis
DGI"
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.2854122621564482,Physics Photo CS
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.28752642706131076,Wiki-CS
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.28964059196617337,Citeseer
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.2917547568710359,CoraFull
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.2938689217758985,Computers
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.2959830866807611,"0.013
0.198
0.042
0.006
0.980"
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.29809725158562367,"0.004
0.875
0.386
0.006
0.006"
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.30021141649048627,"0.004
0.423
0.411
0.002
0.954"
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.3023255813953488,"0.000
0.955
0.916
0.550
0.066"
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.3044397463002114,"0.016
0.035
0.196
0.176
0.988"
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.30655391120507397,"0.007
0.769
0.986
0.099
0.978"
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.3086680761099366,"0.002
0.996
0.054
0.090
0.574 0.2 0.4 0.6 0.8"
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.3107822410147992,(a) AUTOSSL-ES
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.3128964059196617,"0
200
400
600
800
1000
Iteration 0.0 0.2 0.4 0.6 0.8"
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.3150105708245243,Weight values
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.3171247357293869,"Clu
Par
PairSim
PairDis
DGI"
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.3192389006342495,(b) AUTOSSL-DS
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.321353065539112,Figure 3: Visualization of Task Weights.
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.32346723044397463,"0
10
20
30
40
Iteration 0.82 0.84 0.86 0.88 0.90"
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.32558139534883723,Pseudo-homophily
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.3276955602536998,(a) CoraFull
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.3298097251585624,"0
20
40
60
80
100
Iteration 0.88 0.89 0.90 0.91 0.92 0.93 0.94"
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.33192389006342493,Pseudo-homophily
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.33403805496828753,(b) Citeseer
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.3361522198731501,Figure 4: P-H change of AUTOSSL-ES
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.3382663847780127,"4.5
EVOLUTION OF SSL TASK WEIGHTS, PSEUDO-HOMOPHILY AND PERFORMANCE"
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.3403805496828753,"To answer Q4, we visualize the ﬁnal task weights searched by AUTOSSL-ES on all datasets through
the heatmap in Figure 3a. From the ﬁgure, we make three observations. Obs. 1: The searched
task weights vary signiﬁcantly from dataset to dataset. For example, the weights for PAR and DGI
are [0.198, 0.980] on Physics while they are [0.955, 0.066] on WikiCS. Obs. 2: In general,
Par beneﬁts co-purchase networks, i.e. Photo and Computers; DGI is crucial for citation/co-
authorship networks, i.e. Physics, CS, Citeseer, and CoraFull. We conjecture that local
structure information (the information that PAR captures) is essential for co-purchase networks while
both local and global information (the information that DGI captures) are necessary in citation/co-
authorship networks. Obs. 3: AUTOSSL-ES always gives very low weights to CLU, which indicates
the pseudo-labels clustered from raw features are not good supervision on the selected datasets."
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.34249471458773784,"We also provide the evolution of task weights in AUTOSSL-DS for CoraFull dataset in Figure 3b.
The weights of the 5 tasks eventually become stable: CLU and PAIRDIS are assigned with small
values while PAIRSIM, DGI and CLU are assigned with large values. Thus, both AUTOSSL-ES and
AUTOSSL-DS agree that PAIRDIS and PAR are less important for CoraFull."
RELATION BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY,0.34460887949260044,"We further investigate how pseudo-homophily changes over iterations. For AUTOSSL-ES, we il-
lustrate the mean value of resulted pseudo-homophily in each iteration (round) in Figure 4. We
only show the results on CoraFull and Citeseer while similar patterns are exhibited in other
datasets. It is clear that AUTOSSL-ES can effectively increase the pseudo-homophily and thus
search for better self-supervised task weights. The results for AUTOSSL-DS are deferred to Ap-
pendix E.2 due to the page limit."
CONCLUSION,0.346723044397463,"5
CONCLUSION"
CONCLUSION,0.3488372093023256,"Graph self-supervised learning has achieved great success in learning expressive node/graph repre-
sentations. In this work, however, we ﬁnd that SSL tasks designed for graphs perform differently on
different datasets and downstream tasks. Thus, it is worth composing multiple SSL tasks to jointly
encode multiple sources of information and produce more generalizable representations. However,
without access to labeled data, it poses a great challenge in measuring the quality of the combi-
nations of SSL tasks. To address this issue, we take advantage of graph homophily and propose
pseudo-homophily to measure the quality of combinations of SSL tasks. We then theoretically show
that maximizing pseudo-homophily can help maximize the upper bound of mutual information be-
tween the pseudo-labels and ground truth labels. Based on the pseudo-homophily measure, we
develop two automated frameworks AUTOSSL-ES and AUTOSSL-DS to search the task weights
efﬁciently. Extensive experiments have demonstrated that AUTOSSL is able to produce more gen-
eralize representations by combining various SSL tasks."
CONCLUSION,0.35095137420718814,Published as a conference paper at ICLR 2022
CONCLUSION,0.35306553911205074,ACKNOLWEDGEMENT
CONCLUSION,0.35517970401691334,"Wei Jin and Jiliang Tang are supported by the National Science Foundation (NSF) under grant num-
bers IIS1714741, CNS1815636, IIS1845081, IIS1907704, IIS1928278, IIS1955285, IOS2107215,
and IOS2035472, the Army Research Ofﬁce (ARO) under grant number W911NF-21-1-0198, the
Home Depot, Cisco Systems Inc. and SNAP Inc."
ETHICS STATEMENT,0.3572938689217759,ETHICS STATEMENT
ETHICS STATEMENT,0.3594080338266385,"To the best of our knowledge, there are no ethical issues with this paper."
REPRODUCIBILITY STATEMENT,0.36152219873150104,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.36363636363636365,"To ensure reproducibility of our experiments, we provide our source code at https://github.
com/ChandlerBang/AutoSSL. The hyper-parameters are described in details in the appendix.
We also provide a pseudo-code implementation of our framework in the appendix."
REFERENCES,0.3657505285412262,REFERENCES
REFERENCES,0.3678646934460888,"Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi,
Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al.
Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261,
2018."
REFERENCES,0.3699788583509514,"Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the
dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM
Conference on Fairness, Accountability, and Transparency, pp. 610–623, 2021."
REFERENCES,0.37209302325581395,"Christopher M Bishop. Pattern recognition and machine learning. springer, 2006."
REFERENCES,0.37420718816067655,"Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsu-
pervised learning of visual features. In Proceedings of the European Conference on Computer
Vision (ECCV), pp. 132–149, 2018."
REFERENCES,0.3763213530655391,"Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International conference on machine learning,
pp. 1597–1607. PMLR, 2020."
REFERENCES,0.3784355179704017,"Enyan Dai and Suhang Wang. Say no to the discrimination: Learning fair graph neural networks with
limited sensitive attribute information. In Proceedings of the 14th ACM International Conference
on Web Search and Data Mining, pp. 680–688, 2021."
REFERENCES,0.38054968287526425,"Carl Doersch and Andrew Zisserman. Multi-task self-supervised visual learning. In Proceedings of
the IEEE International Conference on Computer Vision, pp. 2051–2060, 2017."
REFERENCES,0.38266384778012685,"Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by
context prediction. In ICCV, 2015."
REFERENCES,0.38477801268498946,"David K Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, Rafael G´omez-Bombarelli,
Timothy Hirzel, Al´an Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs
for learning molecular ﬁngerprints. In Advances in neural information processing systems, 2015."
REFERENCES,0.386892177589852,"Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric.
arXiv preprint arXiv:1903.02428, 2019."
REFERENCES,0.3890063424947146,"Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In International Conference on Machine Learning, pp. 1126–1135. PMLR,
2017."
REFERENCES,0.39112050739957716,"Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In ICML, 2017."
REFERENCES,0.39323467230443976,Published as a conference paper at ICLR 2022
REFERENCES,0.3953488372093023,"Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In Advances in neural information processing systems, 2017."
REFERENCES,0.3974630021141649,"Xueting Han, Zhenhuan Huang, Bang An, and Jing Bai. Adaptive transfer learning on graph neural
networks. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery amp;
Data Mining, KDD ’21, pp. 565–574. Association for Computing Machinery, 2021."
REFERENCES,0.39957716701902746,"Nikolaus Hansen, Sibylle D M¨uller, and Petros Koumoutsakos. Reducing the time complexity of
the derandomized evolution strategy with covariance matrix adaptation (cma-es). Evolutionary
computation, 11(1):1–18, 2003."
REFERENCES,0.40169133192389006,"Kaveh Hassani and Amir Hosein Khasahmadi. Contrastive multi-view representation learning on
graphs. In International Conference on Machine Learning, 2020."
REFERENCES,0.40380549682875266,"Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure
Leskovec. Strategies for pre-training graph neural networks. In International Conference on
Learning Representations, 2019."
REFERENCES,0.4059196617336152,"Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta,
and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv
preprint arXiv:2005.00687, 2020."
REFERENCES,0.4080338266384778,"Wei Jin, Tyler Derr, Haochen Liu, Yiqi Wang, Suhang Wang, Zitao Liu, and Jiliang Tang.
Self-supervised learning on graphs:
Deep insights and new direction.
arXiv preprint
arXiv:2006.10141, 2020."
REFERENCES,0.41014799154334036,"Wei Jin, Tyler Derr, Yiqi Wang, Yao Ma, Zitao Liu, and Jiliang Tang. Node similarity preserving
graph convolutional networks. In Proceedings of the 14th ACM International Conference on Web
Search and Data Mining. ACM, 2021."
REFERENCES,0.41226215644820297,"George Karypis and Vipin Kumar. A fast and high quality multilevel scheme for partitioning irreg-
ular graphs. SIAM Journal on scientiﬁc Computing, 1998."
REFERENCES,0.4143763213530655,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.4164904862579281,"Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional net-
works. arXiv preprint arXiv:1609.02907, 2016a."
REFERENCES,0.4186046511627907,"Thomas N Kipf and Max Welling.
Variational graph auto-encoders.
arXiv preprint
arXiv:1611.07308, 2016b."
REFERENCES,0.42071881606765327,"Alexander Kolesnikov, Xiaohua Zhai, and Lucas Beyer. Revisiting self-supervised visual represen-
tation learning. In In CVPR, pp. 1920–1929, 2019."
REFERENCES,0.42283298097251587,"Chuming Li, Xin Yuan, Chen Lin, Minghao Guo, Wei Wu, Junjie Yan, and Wanli Ouyang. Am-lfs:
Automl for loss function search. In Proceedings of the IEEE/CVF International Conference on
Computer Vision, pp. 8410–8419, 2019."
REFERENCES,0.4249471458773784,"Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv
preprint arXiv:1806.09055, 2018."
REFERENCES,0.427061310782241,"Meng Liu, Hongyang Gao, and Shuiwang Ji. Towards deeper graph neural networks. In Proceedings
of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining.
ACM, 2020."
REFERENCES,0.42917547568710357,"Xiaorui Liu, Wei Jin, Yao Ma, Yaxin Li, Hua Liu, Yiqi Wang, Ming Yan, and Jiliang Tang. Elastic
graph neural networks. In International Conference on Machine Learning, 2021."
REFERENCES,0.4312896405919662,"Ilya Loshchilov and Frank Hutter. Cma-es for hyperparameter optimization of deep neural networks.
arXiv preprint arXiv:1604.07269, 2016."
REFERENCES,0.4334038054968288,"Miller McPherson, Lynn Smith-Lovin, and James M Cook. Birds of a feather: Homophily in social
networks. Annual review of sociology, 27(1):415–444, 2001."
REFERENCES,0.4355179704016913,Published as a conference paper at ICLR 2022
REFERENCES,0.4376321353065539,"P´eter Mernyei and C˘at˘alina Cangea. Wiki-cs: A wikipedia-based benchmark for graph neural net-
works. arXiv preprint arXiv:2007.02901, 2020."
REFERENCES,0.4397463002114165,"Mark Newman. Networks. Oxford university press, 2018."
REFERENCES,0.4418604651162791,"Shirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, Lina Yao, and Chengqi Zhang. Adversarially
regularized graph autoencoder for graph embedding. In Proceedings of the 27th International
Joint Conference on Artiﬁcial Intelligence, IJCAI’18, pp. 2609–2615. AAAI Press, 2018."
REFERENCES,0.4439746300211416,"Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric
graph convolutional networks. arXiv preprint arXiv:2002.05287, 2020."
REFERENCES,0.44608879492600423,"Zhen Peng, Yixiang Dong, Minnan Luo, Xiao-Ming Wu, and Qinghua Zheng. Self-supervised graph
representation learning via global context prediction. arXiv preprint arXiv:2003.01604, 2020."
REFERENCES,0.44820295983086683,"AJ Piergiovanni, Anelia Angelova, and Michael S Ryoo. Evolving losses for unsupervised video
representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 133–142, 2020."
REFERENCES,0.4503171247357294,"Jiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding, Kuansan Wang,
and Jie Tang. Gcc: Graph contrastive coding for graph neural network pre-training. In Pro-
ceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data
Mining, pp. 1150–1160, 2020."
REFERENCES,0.452431289640592,"Zhongzheng Ren and Yong Jae Lee. Cross-domain self-supervised multi-task feature learning us-
ing synthetic imagery. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 762–771, 2018."
REFERENCES,0.45454545454545453,"Claudia Veronica Roberts et al. Quantifying the extent to which popular pre-trained convolutional
neural networks implicitly learn high-level protected attributes. PhD thesis, Princeton University,
2018."
REFERENCES,0.45665961945031713,"Aravind Sankar, Yozen Liu, Jun Yu, and Neil Shah. Graph neural networks for friend ranking in
large-scale social platforms. In Proceedings of the Web Conference 2021, pp. 2535–2546, 2021."
REFERENCES,0.4587737843551797,"Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan G¨unnemann. Pitfalls
of graph neural network evaluation. arXiv preprint arXiv:1811.05868, 2018."
REFERENCES,0.4608879492600423,"Shubhranshu Shekhar, Neil Shah, and Leman Akoglu. Fairod: Fairness-aware outlier detection.
arXiv preprint arXiv:2012.03063, 2020."
REFERENCES,0.4630021141649049,"Susheel Suresh, Vinith Budde, Jennifer Neville, Pan Li, and Jianzhu Ma. Breaking the limit of
graph neural networks by improving the assortativity of graphs with local mixing patterns. arXiv
preprint arXiv:2106.06586, 2021."
REFERENCES,0.46511627906976744,"Xianfeng Tang, Yozen Liu, Neil Shah, Xiaolin Shi, Prasenjit Mitra, and Suhang Wang. Knowing
your fate: Friendship, action and temporal explanations for user engagement prediction on so-
cial apps. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining, pp. 2269–2279, 2020a."
REFERENCES,0.46723044397463004,"Xianfeng Tang, Huaxiu Yao, Yiwei Sun, Yiqi Wang, Jiliang Tang, Charu Aggarwal, Prasenjit Mitra,
and Suhang Wang. Investigating and mitigating degree-related biases in graph convoltuional net-
works. In Proceedings of the 29th ACM International Conference on Information & Knowledge
Management, pp. 1435–1444, 2020b."
REFERENCES,0.4693446088794926,"Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. In ICLR, 2018."
REFERENCES,0.4714587737843552,"Petar Veliˇckovi´c, William Fedus, William L. Hamilton, Pietro Li`o, Yoshua Bengio, and R Devon
Hjelm. Deep Graph Infomax. In International Conference on Learning Representations, 2019.
URL https://openreview.net/forum?id=rklz9iAcKQ."
REFERENCES,0.47357293868921774,"Xiaobo Wang, Shuo Wang, Cheng Chi, Shifeng Zhang, and Tao Mei. Loss function search for face
recognition. In International Conference on Machine Learning, pp. 10029–10038. PMLR, 2020."
REFERENCES,0.47568710359408034,Published as a conference paper at ICLR 2022
REFERENCES,0.47780126849894294,"Yu Wang, Wei Jin, and Tyler Derr. Graph neural networks: Self-supervised learning. In Graph
Neural Networks: Foundations, Frontiers, and Applications, pp. 391–420. Springer, 2022."
REFERENCES,0.4799154334038055,"Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, and Tieniu Tan. Session-based rec-
ommendation with graph neural networks. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, pp. 346–353, 2019a."
REFERENCES,0.4820295983086681,"Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S Yu. A
comprehensive survey on graph neural networks. arXiv preprint arXiv:1901.00596, 2019b."
REFERENCES,0.48414376321353064,"Yaochen Xie, Zhao Xu, Jingtun Zhang, Zhengyang Wang, and Shuiwang Ji. Self-supervised learning
of graph neural networks: A uniﬁed review. arXiv preprint arXiv:2102.10757, 2021."
REFERENCES,0.48625792811839325,"Haowen Xu, Hao Zhang, Zhiting Hu, Xiaodan Liang, Ruslan Salakhutdinov, and Eric Xing. Au-
toloss: Learning discrete schedules for alternate optimization. arXiv preprint arXiv:1810.02442,
2018."
REFERENCES,0.4883720930232558,"Zhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-supervised learning with
graph embeddings. In International conference on machine learning, pp. 40–48. PMLR, 2016."
REFERENCES,0.4904862579281184,"Quanming Yao, Mengshuo Wang, Yuqiang Chen, Wenyuan Dai, Yu-Feng Li, Wei-Wei Tu, Qiang
Yang, and Yang Yu. Taking human out of learning applications: A survey on automated machine
learning. arXiv preprint arXiv:1810.13306, 2018."
REFERENCES,0.492600422832981,"Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec.
Graph convolutional neural networks for web-scale recommender systems. In KDD. ACM, 2018."
REFERENCES,0.49471458773784355,"Yuning You, Tianlong Chen, Zhangyang Wang, and Yang Shen. When does self-supervision help
graph convolutional networks? ICML, 2020."
REFERENCES,0.49682875264270615,"Yuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. Graph contrastive learning auto-
mated. Proceedings of International Conference on Machine Learning, 2021."
REFERENCES,0.4989429175475687,"Amir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, and Silvio
Savarese. Taskonomy: Disentangling task transfer learning. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pp. 3712–3722, 2018."
REFERENCES,0.5010570824524313,"Jieyu Zhao, Tianlu Wang, Mark Yatskar, Ryan Cotterell, Vicente Ordonez, and Kai-Wei Chang.
Gender bias in contextualized word embeddings. arXiv preprint arXiv:1904.03310, 2019."
REFERENCES,0.5031712473572939,"Xiangyu Zhao, Haochen Liu, Wenqi Fan, Hui Liu, Jiliang Tang, and Chong Wang. Autoloss: Au-
tomated loss function search in recommendations. In Proceedings of the 27th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining, 2021a."
REFERENCES,0.5052854122621564,"Yue Zhao, Ryan Rossi, and Leman Akoglu. Automatic unsupervised outlier model selection. Ad-
vances in Neural Information Processing Systems, 34, 2021b."
REFERENCES,0.507399577167019,"Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai Koutra. Beyond
homophily in graph neural networks: Current limitations and effective designs. Advances in
Neural Information Processing Systems, 33, 2020a."
REFERENCES,0.5095137420718816,"Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Graph contrastive learning
with adaptive augmentation. arXiv preprint arXiv:2010.14945, 2020b."
REFERENCES,0.5116279069767442,"Daniel Z¨ugner and Stephan G¨unnemann. Adversarial attacks on graph neural networks via meta
learning. arXiv preprint arXiv:1902.08412, 2019."
REFERENCES,0.5137420718816068,Published as a conference paper at ICLR 2022
REFERENCES,0.5158562367864693,"A
EXPERIMENTAL SETUP"
REFERENCES,0.5179704016913319,"Dataset Statistics. We evaluate the proposed framework on seven real-world datasets. The dataset
statistics are shown in 4. All datasets can be loaded from PyTorch Geometric (Fey & Lenssen, 2019).
When we evaluate the node classiﬁcation performance, we need to use the training and test data. For
WikiCS (Mernyei & Cangea, 2020), ogbn-arxiv (Hu et al., 2020) and Citeseer (Yang et al.,
2016), we use the public data splits provided by the authors. For other datasets, we split the nodes
into 10%/10%/80% for training/validation/test."
REFERENCES,0.5200845665961945,Table 4: Dataset statistics.
REFERENCES,0.5221987315010571,"Dataset
Network Type
#Nodes
#Edges
#Classes
#Features
Homophily"
REFERENCES,0.5243128964059197,"WikiCS
Reference network
11,701
216,123
10
300
0.70
CS
Co-authorship network
18,333
81,894
15
6,805
0.81
Physics
Co-authorship network
34,493
247,962
5
8,415
0.93
Computers
Co-purchase network
13,381
245,778
10
767
0.78
Photo
Co-purchase network
7,487
119,043
8
745
0.83
CoraFull
Citation network
19,793
65,311
70
8,710
0.57
Citeseer
Citation network
3,327
4,732
6
3,703
0.74
ogbn-arxiv
Citation network
169,343
1,166,243
40
128
0.78"
REFERENCES,0.5264270613107822,"Hyper-parameter Settings. When calculating pseudo-homophily, we set the number of clusters to
10 for ogbn-arxiv and Computers, and 5 for other datasets. A small number of clusters can be
more efﬁcient but could be less stable. Following DGI (Veliˇckovi´c et al., 2019) and MVGRL (Has-
sani & Khasahmadi, 2020), we we use a simple one-layer GCN (Kipf & Welling, 2016a) as our
encoder. We set the size of hidden dimensions to 512, weight decay to 0, dropout rate to 0. For indi-
vidual SSL methods and AUTOSSL-ES, we set learning rate to 0.001, use Adam optimizer (Kingma
& Ba, 2014), train the models with 1000 epochs and adopt early stopping strategy. For AUTOSSL-
DS, we train the models with 1000 epochs and choose the model checkpoint that achieves the highest
pseudo-homophily. We use Adam optimizer for both inner and outer optimization. The learning
rate for outer optimization is set to 0.05. For AUTOSSL-ES, we use a population size of 8 for each
round. Due to limited computational resources, we perform 80 rounds for Citeseer, 40 rounds
for CS, Computers, CoraFull, Photo, Physics, Computers and WikiCS. We repeat the
experiments on 5 different random seeds and report the mean values and variances for downstream
performance. To ﬁt DGI into GPU memory on larger datasets and accelerate its training, instead
of using all the nodes we sample 2000 positive samples and 2000 negative samples for DGI on all
datasets except Citeseer."
REFERENCES,0.5285412262156448,"Hardware and Software Conﬁgurations. We perform experiments on one NVIDIA Tesla K80
GPU and one NVIDIA Tesla V100 GPU. Additionally, we use eight CPUs, with the model name as
Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz. The operating system we use is CentOS Linux
7 (Core)."
REFERENCES,0.5306553911205074,"B
PROOF"
REFERENCES,0.53276955602537,"Theorem 1.
Suppose that we are given with a graph G = {V, E}, a pseudo label vector A ∈
{0, 1}N and a ground truth label vector B ∈{0, 1}N deﬁned on the node set. We denote the
homophily of A and B over G as hA and hB, respectively. If the classes in A and B are balanced
and hA ¡ hB, the following results hold: (1) the mutual information between A and B, i.e., MI(A,B),
has an upper bound UA,B, where UA,B =
1
N

2∆log( 4"
REFERENCES,0.5348837209302325,N ∆) + 2( N
REFERENCES,0.5369978858350951,"2 −∆) log
  4 N ( N"
REFERENCES,0.5391120507399577,"2 −∆)

with
∆= (hB−hA)|E|"
DMAX,0.5412262156448203,"2dmax
and dmax denoting the largest node degree in the graph; (2) if hA < hA′ < hB, we
have UA,B < UA′,B."
DMAX,0.5433403805496829,"Proof. (1) We start with the proof of the ﬁrst result. The mutual information between two random
variables X and Y is expressed as"
DMAX,0.5454545454545454,"MI(X, Y ) =
X y∈Y X"
DMAX,0.547568710359408,"x∈X
p(A,B)(x, y) log
 p(X,Y )(x, y)"
DMAX,0.5496828752642706,pX(x)pY (y)
DMAX,0.5517970401691332,"
.
(11)"
DMAX,0.5539112050739958,Published as a conference paper at ICLR 2022
DMAX,0.5560253699788583,"Let Ai and Bi denote the set of nodes in the i-th class in A and B, respectively. Following the
deﬁnition in Eq. (11), the mutual information between A and B can be formulated as,"
DMAX,0.5581395348837209,"MI(A, B) ="
DMAX,0.5602536997885835,"nA−1
X i=0"
DMAX,0.5623678646934461,"nB−1
X j=0"
DMAX,0.5644820295983086,|Ai ∩Bj|
DMAX,0.5665961945031712,"N
log N |Ai ∩Bj|"
DMAX,0.5687103594080338,"|Ai| |Bj|
,
(12)"
DMAX,0.5708245243128964,"where nA, nB denote the number of classes in A and B. Since here we only consider 2 classes in A
and B, we have"
DMAX,0.572938689217759,"MI(A, B) =|A0 ∩B0|"
DMAX,0.5750528541226215,"N
log N |A0 ∩B0|"
DMAX,0.5771670190274841,"|A0| |B0|
+ |A0 ∩B1|"
DMAX,0.5792811839323467,"N
log N |A0 ∩B1|"
DMAX,0.5813953488372093,|A0| |B1|
DMAX,0.5835095137420718,+ |A1 ∩B0|
DMAX,0.5856236786469344,"N
log N |A1 ∩B0|"
DMAX,0.587737843551797,"|A1| |B0|
+ |A1 ∩B1|"
DMAX,0.5898520084566596,"N
log N |A1 ∩B1|"
DMAX,0.5919661733615222,"|A1| |B1|
.
(13)"
DMAX,0.5940803382663847,"Let |A0 ∩B0| = x, |A0| = a and |B0| = b. We then have




"
DMAX,0.5961945031712473,"


"
DMAX,0.5983086680761099,"|A0| + |A1| = N ⇒|A1| = N −a,
|B0| + |B1| = N ⇒|B1| = N −b,
|A0 ∩B0| + |A0 ∩B1| = |A0| ⇒|A0 ∩B1| = a −x,
|A0 ∩B0| + |A1 ∩B0| = |B0| ⇒|A1 ∩B0| = b −x,
|A0 ∩B1| + |A1 ∩B1| = |B1| ⇒|A1 ∩B1| = N −b −a + x. (14)"
DMAX,0.6004228329809725,"With the equations above, we rewrite MI(A, B) as follows,"
DMAX,0.6025369978858351,"MI(A, B) = 1"
DMAX,0.6046511627906976,N [x log Nx
DMAX,0.6067653276955602,ab + (a −x) log N(a −x)
DMAX,0.6088794926004228,a(N −b)
DMAX,0.6109936575052854,+ (b −x) log N(b −x)
DMAX,0.6131078224101479,(N −a)b + (N −b −a + x) log N(N −b −a + x)
DMAX,0.6152219873150105,"(N −a)(N −b) ].
(15)"
DMAX,0.6173361522198731,"Then we rewrite result (1) in the theorem as an optimization problem,"
DMAX,0.6194503171247357,"max f(x) = MI(A, B)
(16)"
DMAX,0.6215644820295984,"with constraints,




"
DMAX,0.6236786469344608,"


"
DMAX,0.6257928118393234,"0 ≤|A0 ∩B0| ≤|A0| ⇒0 ≤x ≤a,
0 ≤|A0 ∩B0| ≤|B0| ⇒0 ≤x ≤b,
|A1 ∩B1| ≥0 ⇒x ≥a + b −N,
|A0 ∩B0| + |A1 ∩B1| ≤N ⇒x ≤a+b"
DMAX,0.627906976744186,"2 ,
|A0 ∩B1| + |A1 ∩B0| ≤N ⇒x ≥a+b−N 2
, (17)"
DMAX,0.6300211416490487,"Note that the equality of |A0 ∩B0| + |A1 ∩B1| ≤N holds when A and B are the same. However,
A and B have different homophily, which indicates |A0 ∩B0| + |A1 ∩B1| cannot reach N (the
same for |A0 ∩B1|+|A1 ∩B0|). Let EA, EB denote the inter-class edges for A and B, respectively.
Thus, hA = 1 −|EA|"
DMAX,0.6321353065539113,|E| and hB = 1 −|EB|
DMAX,0.6342494714587738,"|E| . Since hA < hB, we have |EA| > |EB|. This indicates
that there are at least |EA| −|EB| edges in A connecting nodes that belong to the same ground truth
class, as shown in Figure 5."
DMAX,0.6363636363636364,Nodes in
DMAX,0.638477801268499,Nodes in
DMAX,0.6405919661733616,Intra-class edges
DMAX,0.642706131078224,Inter-class edges
DMAX,0.6448202959830867,"Figure 5: Illustration for |A0 ∩B0| + |A1 ∩B1|. The two dashed rectangles divide the nodes into
A0 and A1; red and blue nodes denote nodes in B0 and B1, respectively."
DMAX,0.6469344608879493,Let dmax denote the maximum degree in the graph and we know that at least |EA|−|EB|
DMAX,0.6490486257928119,"dmax
nodes are
“misplaced” in A, e.g., in Figure 5 the red node in A1 should be placed in A0 to achieve |A0 ∩B0|+"
DMAX,0.6511627906976745,Published as a conference paper at ICLR 2022
DMAX,0.653276955602537,|A1 ∩B1| = N. Let ∆= |EA|−|EB|
DMAX,0.6553911205073996,"2dmax
= (hB−hA)|E|"
DMAX,0.6575052854122622,"2dmax
, and we have |A0 ∩B0| + |A1 ∩B1| ≤N −2∆
and |A0 ∩B1| + |A1 ∩B0| ≤N −2∆."
DMAX,0.6596194503171248,"With the new constraints, we rewrite the optimization problem as"
DMAX,0.6617336152219874,"max f(x) = MI(A, B)
s.t."
DMAX,0.6638477801268499,"



"
DMAX,0.6659619450317125,"


"
DMAX,0.6680761099365751,"0 ≤x ≤a,
x ≤b,
x ≥a + b −N,
x ≤a+b−2∆"
DMAX,0.6701902748414377,"2
,
x ≥a+b−(N−2∆) 2
, (18)"
DMAX,0.6723044397463002,"Further, the derivative of f(x) is expressed as follows,"
DMAX,0.6744186046511628,f ′(x) = 1
DMAX,0.6765327695560254,N log x(N −b −a + x)
DMAX,0.678646934460888,"(a −x)(b −x) .
(19)"
DMAX,0.6807610993657506,"Let f ′(x) > 0, we have x >
ab
N ; let f ′(x) < 0, we have x <
ab
N .
Thus, f(x) is
monotonically decreasing at [max(0, a + b −N, a+b−(N−2∆)"
DMAX,0.6828752642706131,"2
), ab"
DMAX,0.6849894291754757,"N ] and monotonically increasing
[ ab"
DMAX,0.6871035940803383,"N , min(a, b, a+b−2∆ 2
)]."
DMAX,0.6892177589852009,"Note that in the theorem we assume the pseudo-labels and ground truth classes are balanced, i.e.,
a = b = N"
DMAX,0.6913319238900634,"2 . Then MI(A, B) becomes,"
DMAX,0.693446088794926,"MI(A, B) = f(x) = 1 N"
DMAX,0.6955602536997886,"
2x log 4"
DMAX,0.6976744186046512,N x + 2(N
DMAX,0.6997885835095138,"2 −x) log
 4 N (N"
DMAX,0.7019027484143763,"2 −x)

.
(20)"
DMAX,0.7040169133192389,"Hence, f(x) is monotonically decreasing at [∆, N"
DMAX,0.7061310782241015,"4 ] and monotonically increasing [ N 4 , N"
DMAX,0.7082452431289641,"2 −∆]. So
the maximum value of f(x) is at either x = ∆or x = N"
DMAX,0.7103594080338267,"2 −∆. Further it is easy to know that
f( N"
DMAX,0.7124735729386892,4 −x) = f( N
DMAX,0.7145877378435518,4 + x). Then we have f(∆) = f( N
DMAX,0.7167019027484144,"2 −∆), and we can get the maximum value of
f(x) as follows,"
DMAX,0.718816067653277,"UA,B = max f(x) = f(∆) = 1 N"
DMAX,0.7209302325581395,"
2∆log( 4"
DMAX,0.7230443974630021,N ∆) + 2(N
DMAX,0.7251585623678647,"2 −∆) log
 4 N (N"
DMAX,0.7272727272727273,"2 −∆)

(21)"
DMAX,0.7293868921775899,"with ∆=
(hB−hA)|E|"
DMAX,0.7315010570824524,"2dmax
. In other words, MI(A, B) reaches its upper bound when |A0 ∩B0| ="
DMAX,0.733615221987315,(hB−hA)|E|
"DMAX
OR N",0.7357293868921776,"2dmax
or N"
"DMAX
OR N",0.7378435517970402,2 −(hB−hA)|E|
DMAX,0.7399577167019028,"2dmax
."
DMAX,0.7420718816067653,(2) From the constraints x ≤a+b−2∆
DMAX,0.7441860465116279,"2
and x ≥a+b−(N−2∆)"
DMAX,0.7463002114164905,"2
in Eq (18), we have a+b−(N−2∆)"
DMAX,0.7484143763213531,"2
≤
a+b−2∆"
DMAX,0.7505285412262156,"2
⇒∆≤N"
DMAX,0.7526427061310782,"4 . Based on the discussion in (1), we know that f(∆) is monotonically decreasing
at [0, N"
DMAX,0.7547568710359408,"4 ], which means an increase of ∆leads to a decrease in f(∆), i.e., a smaller value of UA,B.
Since ∆= (hB−hA)|E|"
DMAX,0.7568710359408034,"2dmax
, a decrease in hA will lead to a increase in ∆. Then we have UA,B < UA′,B
if hA < hA′ < hB."
DMAX,0.758985200845666,"Remark on a more generalized case.
We now discuss the case where we do not have
assumptions on a and b.
As we demonstrated in the above discussion, f(x) is mono-
tonically decreasing at [max(0, a + b −N, a+b−(N−2∆)"
DMAX,0.7610993657505285,"2
), ab"
DMAX,0.7632135306553911,"N ] and monotonically increasing
[ ab"
DMAX,0.7653276955602537,"N , min(a, b, a+b−2∆"
DMAX,0.7674418604651163,"2
)].
Thus, the maximum value of f(x) should be one of the values of
f(0), f(a + b −N), f( a+b−(N−2∆)"
DMAX,0.7695560253699789,"2
), f(a), f(b) and f( a+b−2∆"
DMAX,0.7716701902748414,"2
). As our goal is to show that UA,B
would be small with low hA, to simplify the analysis, we consider a large value of ∆(or a small
value of hA) which satisﬁes ∆≥1"
DMAX,0.773784355179704,2|N −(a + b)| and ∆≥1
DMAX,0.7758985200845666,"2|a −b|. This indicates x is bounded by
[ a+b−(N−2∆)"
DMAX,0.7780126849894292,"2
, a+b−2∆"
DMAX,0.7801268498942917,"2
]. Then the maximum value of f(x), i.e., UA,B, is expressed as"
DMAX,0.7822410147991543,"UA,B = max(f(a + b −(N −2∆)"
DMAX,0.7843551797040169,"2
), f(a + b −2∆"
DMAX,0.7864693446088795,"2
)).
(22)"
DMAX,0.7885835095137421,When a+b−(N−2∆)
DMAX,0.7906976744186046,"2
≤
ab
N ≤
a+b−2∆"
DMAX,0.7928118393234672,"2
, it is easy to see that larger ∆(or smaller hA) will lead
to smaller UA,B because both a+b−(N−2∆)"
DMAX,0.7949260042283298,"2
and a+b−2∆"
WILL BE CLOSER TO THE MINIMA POINT AB,0.7970401691331924,"2
will be closer to the minima point ab"
WILL BE CLOSER TO THE MINIMA POINT AB,0.7991543340380549,"N .
When ab"
WILL BE CLOSER TO THE MINIMA POINT AB,0.8012684989429175,"N ≤
a+b−(N−2∆)"
WILL BE CLOSER TO THE MINIMA POINT AB,0.8033826638477801,"2
≤
a+b−2∆"
WILL BE CLOSER TO THE MINIMA POINT AB,0.8054968287526427,"2
, UA,B = f( a+b−2∆"
WILL BE CLOSER TO THE MINIMA POINT AB,0.8076109936575053,"2
). It decreases with the increase of
∆(or the decrease of hA) because a+b−2∆"
GETS CLOSER TO THE MINIMA POINT AB,0.8097251585623678,"2
gets closer to the minima point ab"
GETS CLOSER TO THE MINIMA POINT AB,0.8118393234672304,"N . Similarly, when
a+b−(N−2∆)"
GETS CLOSER TO THE MINIMA POINT AB,0.813953488372093,"2
≤a+b−2∆ 2
≤ab"
GETS CLOSER TO THE MINIMA POINT AB,0.8160676532769556,"N , UA,B decreases with the increase of ∆(or the decrease of hA). To
sum up, for small hA, the upper bound of MI(A, B), i.e., UA,B, decreases with the decrease of hA."
GETS CLOSER TO THE MINIMA POINT AB,0.8181818181818182,Published as a conference paper at ICLR 2022
GETS CLOSER TO THE MINIMA POINT AB,0.8202959830866807,"C
ALGORITHM"
GETS CLOSER TO THE MINIMA POINT AB,0.8224101479915433,"The detailed algorithm for AUTOSSL-ES is shown in Algorithm 1. Concretely, for each round (it-
eration) of AUTOSSL-ES, we sample K sets of task weights, i.e., K different combinations of SSL
tasks, from a multivariate normal distribution. Then we train K graph neural networks indepen-
dently on each set of task weights. Afterwards, we calculate the pseudo-homohily for each network
and adjust the mean and variance of the multivariate normal distribution through CMA-ES based on
their pseudo-homohily."
GETS CLOSER TO THE MINIMA POINT AB,0.8245243128964059,"The detailed algorithm for AUTOSSL-DS is summarized in Algorithm 2. Speciﬁcally, we ﬁrst up-
date the GNN parameter θ through one step gradient descent; then we perform k-means clustering
to obtain centroids, which are used to calculate the homophily loss H. Afterwards, we calculate the
meta-gradient ∇meta
{λi}, update {λi} through gradient descent and clip {λi} to [0, 1]."
GETS CLOSER TO THE MINIMA POINT AB,0.8266384778012685,"Algorithm 1: AUTOSSL-ES: AutoSSL with Evolutionary Strategy
for r in {0, . . . , R} do"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.828752642706131,"1. Sample K sets of tasks weights from a multivariate normal distribution
2. Train K networks w.r.t. each set of task weights from scratch
3. Calculate pseudo-homophily P-H of node embeddings from each network
4. Adjust the multivariate normal distribution through CMA-ES based on P-H
end"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.8308668076109936,"Algorithm 2: AUTOSSL-DS: AutoSSL with Differential Search
Initialize self-supervised task weights {λi} and GNN parameters θ;
for t in {0, . . . , T} do"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.8329809725158562,"1. θt+1 = θt −ϵ∇θtL(fθt, {λi, ℓi})
2. Perform k-means clustering on fθt(G) and obtain centroids {c1, c2, . . . , ck}
3. Calculate p (ci | x) according to Eq. (4)
4. Calculate homophily loss H according to Eq. (5)
5. {λi} ←−{λi} −η∇meta
{λi}
6. Clip {λi} to [0,1]
end"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.8350951374207188,"D
DISCUSSIONS ON HOMOPHILY ASSUMPTION"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.8372093023255814,"Most of the graphs in our real life satisfy the homophily assumption (McPherson et al., 2001),
such as social networks, citation networks, co-purchase networks, etc. Thus, in general, we can
treat homophily as a prior knowledge for a majority of real-world graphs. Moreover, it has been
shown in (Zhu et al., 2020a; Pei et al., 2020) that most GNNs (such as GCN, GAT, ChebyNet
and GraphSage) heavily rely on the homophily assumption and fail to generalize to low-homophily
(heterophily) graphs even with label information. Thus, following the design of most GNNs, we
focus on the homophily graphs. In addition, to apply our method on heterophily graphs, we can use
the graph transformation algorithm (Suresh et al., 2021) to increase the homophily of a given graph.
While heterophily graphs also exist in real-world applications, the research of GNNs on heterophily
graphs is still at the very early stage even in the cases where the label information is available.
Therefore, we will leave the research for heterophily graphs in the unsupervised setting as a future
work."
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.8393234672304439,"E
ADDITIONAL EXPERIMENTAL RESULTS"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.8414376321353065,"E.1
RELATIONSHIP BETWEEN DOWNSTREAM PERFORMANCE AND PSEUDO-HOMOPHILY"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.8435517970401691,"We provide more results on the relation between downstream performance and pseudo-homophily
in Figure 6. Observations are already made in Section 4.4."
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.8456659619450317,Published as a conference paper at ICLR 2022
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.8477801268498943,"0.4
0.5
0.6
0.7
0.8
0.9
Pseudo-homophily 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.8498942917547568,Node clustering NMI
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.8520084566596194,(a) CoraFull: NMI
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.854122621564482,"0.85
0.86
0.87
0.88
0.89
Pseudo-homophily 0.70 0.72 0.74 0.76 0.78 0.80"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.8562367864693446,Node clustering NMI
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.8583509513742071,(b) CS: NMI
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.8604651162790697,"0.84
0.86
0.88
0.90
0.92
0.94
Pseudo-homophily 0.34 0.36 0.38 0.40 0.42 0.44 0.46"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.8625792811839323,Node clustering NMI
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.864693446088795,(c) Citeseer: NMI
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.8668076109936576,"0.84
0.86
0.88
0.90
0.92
Pseudo-homophily 0.60 0.62 0.64 0.66 0.68 0.70 0.72"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.86892177589852,Node clustering NMI
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.8710359408033826,(d) Physics: NMI
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.8731501057082452,"0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Pseudo-homophily 0.1 0.2 0.3 0.4 0.5 0.6"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.8752642706131079,Node clustering NMI
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.8773784355179705,(e) Photo: NMI
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.879492600422833,"0.4
0.5
0.6
0.7
0.8
0.9
Pseudo-homophily 52 54 56 58 60 62"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.8816067653276956,Node classification accuracy
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.8837209302325582,(f) CoraFull: ACC
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.8858350951374208,"0.85
0.86
0.87
0.88
0.89
Pseudo-homophily 90.0 90.5 91.0 91.5 92.0 92.5 93.0 93.5"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.8879492600422833,Node classification accuracy
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.8900634249471459,(g) CS: ACC
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.8921775898520085,"0.84
0.86
0.88
0.90
0.92
0.94
Pseudo-homophily 68 69 70 71 72 73"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.8942917547568711,Node classification accuracy
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.8964059196617337,"(h)
Citeseer:
ACC"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.8985200845665962,"0.84
0.86
0.88
0.90
0.92
Pseudo-homophily 93.0 93.5 94.0 94.5 95.0 95.5 96.0"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9006342494714588,Node classification accuracy
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9027484143763214,(i) Physics: ACC
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.904862579281184,"0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Pseudo-homophily 90.5 91.0 91.5 92.0 92.5 93.0"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9069767441860465,Node classification accuracy
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9090909090909091,(j) Photo: ACC
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9112050739957717,Figure 6: Relationship between downstream performance and pseudo-homophily.
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9133192389006343,"0
200
400
600
800
1000
Iteration 0.84 0.86 0.88 0.90 0.92 0.94"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9154334038054969,Pseudo-homophily 0.36 0.38 0.40 0.42 0.44 0.46 NMI
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9175475687103594,(a) Clustering
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.919661733615222,"0
200
400
600
800
1000
Iteration 0.84 0.86 0.88 0.90 0.92 0.94"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9217758985200846,Pseudo-homophily 0.66 0.68 0.70 0.72 ACC
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9238900634249472,(b) Classiﬁcation
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9260042283298098,"0
200
400
600
800
1000
Iteration 0.84 0.86 0.88 0.90 0.92 0.94"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9281183932346723,Pseudo-homophily 0.01 0.02 0.03 0.04 0.05 0.06 Loss
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9302325581395349,"(c) Homophily loss
Figure 7: Pseudo-homophily versus NMI/ACC/Loss on Citeseer for AUTOSSL-DS. The vertical
dashed line indicates the iteration when pseudo-homophily reaches the maximum value."
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9323467230443975,"E.2
PSEUDO-HOMOPHILY OVER ITERATIONS"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9344608879492601,"We investigate how pseudo-homophily changes over iterations for AUTOSSL-DS. The changes of
pseudo-homophily, NMI, ACC and homophily loss (Eq. (5)) are plotted in Figure 7. From Figure 7a
and 7b, we can observe that pseudo-homophily ﬁrst increases and then becomes stable through
iterations. The situation is a bit different for clustering and classiﬁcation performance: NMI and
ACC ﬁrst increase with the increase of pseudo-homophily and then drop when pseudo-homophily
is relatively stable. This indicates that overtraining can hurt downstream performance as the model
will have the risk of overﬁtting on the combined SSL tasks. However, as shown in the ﬁgure, if
we stop at the iteration when pseudo-homophily reaches the maximum value we can still get a high
NMI and ACC. On a separate note, Figure 7c shows how the homophily loss used in AUTOSSL-DS
changes over iterations. We note that in the ﬁrst iterations the homophily loss is low but the pseudo-
homophily is also low. This is because the embeddings in the ﬁrst few epochs are less separable and
would lead to very close soft-assignment of clusters. As shown in the ﬁgure, however, the problem
is resolved as the embeddings become more distinguishable through iterations. Thus, we argue that
the homophily loss in Eq. (5) is still a good proxy in optimizing pseudo-homophily."
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9365750528541226,"E.3
EFFICIENCY ANALYSIS"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9386892177589852,"E.3.1
TIME COMPLEXITY ANALYSIS"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9408033826638478,"We analyze the time complexity of the proposed AUTOSSL. Here we call one set of task weights
{λi} as one candidate solution. We denote the time of training one epoch on a given set of SSL tasks
as to and the evaluation time is te. Suppose we need to train T epochs for the network. Then the time
for running one single candidate solution is Tto +te; the time of running R rounds of AUTOSSL-ES
should be RTto + Rte. For AUTOSSL-DS, the running time is Tto + Tte. As an illustration, in an
L-layer GCN with d as the number of hidden dimensions, to can be expressed as O(L|E|d+LNd2)
and te has time complexity of O(KINd) with K being the number of clusters and I being the"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9429175475687104,Published as a conference paper at ICLR 2022
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.945031712473573,"number of iterations for k-means. Hence, we also express the time complexity of AUTOSSL-ES
as O(RTL|E|d + RTLNd2 + RKINd) and that of AUTOSSL-DS as O(TL|E|d + TLNd2 +
TKINd). Both of them linearly increase with the number of nodes N when E is proportional to
N. We note that in AUTOSSL-DS, the complexity of calculating the second-order derivatives in
backward propagation has an additional factor of O(|θ||{λi}|), which can be reduced to O(|{λi}|)
with approximated Hessian-vector products. The factor can be neglected as the number of tasks
O(|{λi}|) is small."
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9471458773784355,"E.3.2
EMPIRICAL COMPARISON"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9492600422832981,"For empirical comparison, we take Citeseer, Photo, CoraFull as examples to illustrate. As
shown in Table 5, we compare the running time of different methods for training 1000 epochs on one
NVIDIA-K80 GPU. The column of 5-Tasks indicates the running time of training a combination of
5 SSL tasks, i.e. CLU, PAR, PAIRSIM, PAIRDIS and DGI, for 1000 epochs. Note that we report
the running time of AUTOSSL-ES as the multiplication between 500 and the time of 5-Tasks, i.e.,
running 500 candidate solutions. From the table, we can see that the running time of AUTOSSL-
ES depends on the number of candidate solutions and usually takes a long time to run. However,
AUTOSSL-DS signiﬁcantly reduces the running time of AUTOSSL-ES. It is worth noting the state-
of-the-art SSL task, MVGRL, takes a long time to run and suffers from the OOM issue when the
dataset gets larger."
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9513742071881607,"Table 5: Comparison of running time for training 1000 epochs on one NVIDIA-K80 GPU (12 GB
memory). OOM indicates out-of-memory on this GPU."
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9534883720930233,"DGI
MVGRL
5-Tasks
AUTOSSL-ES
AUTOSSL-DS"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9556025369978859,"Citeseer
222s
1220s
322s
322s×500
1222s
Photo
177s
1074s
507s
507s×500
1766s
CoraFull
553s
OOM
858s
858s×500
3584s"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9577167019027484,"F
COMPARISON WITH DIFFERENT STRATEGY"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.959830866807611,"In this subsection, we examine how other strategies of assigning task weights affect the quality of
learned representations. The results are summarized in Table 6. In this table, “Best SSL” indicates
the best performance achieved by the individual tasks; “Random Weight” indicates the performance
achieved by randomly assigning task weights; “Equal Weight” indicates the performance achieved
by assigning the same task weights (i.e., all 1). The values that outperform “Best SSL” are under-
lined. From the table, we make two observations. Obs 1. Unlike AUTOSSL, “Random Weight” and
“Equal Weight” would hurt both NMI and ACC on some datasets, e.g., Citeseer. This suggests
that SSL tasks might conﬂict with each other and thus harm the downstream performance. Obs 2.
In some cases like Physics, “Equal Weight” can also improve both ACC and NMI, which aligns
well with our initial motivation that combinations of SSL can help capture multiple sources of infor-
mation and beneﬁt the downstream performance. The two observations suggest that it is important
to design a clever strategy that can automatically compose graph SSL tasks."
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9619450317124736,"G
COMPARISON WITH RANDOM SEARCH"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9640591966173362,"In this subsection, we choose Citeseer to study the difference between random search and evolu-
tionary algorithm (AUTOSSL-ES), and report the result in Table 7. Speciﬁcally, the random search
method randomly generates 800 sets of tasks weights and we evaluate the pseudo-homophily from
the models trained with those task weights. Note that in AUTOSSL-ES we also evaluated 800 sets of
tasks weights in total. From the table, we can see that random search is not as efﬁcient as AUTOSSL-
ES: with the same search cost, the resulted pseudo-homophily of random search is not as high as
AUTOSSL-ES and the downstream performance is also inferior. This result suggests that search
with evolutionary algorithm can ﬁnd the optimum faster than random search."
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9661733615221987,Published as a conference paper at ICLR 2022
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9682875264270613,"Table 6: Performance comparison of different strategies of assigning task weights. The NMI rows
indicate node clustering performance; ACC rows indicate node classiﬁcation accuracy (%); P-H
stands for pseudo-homophily. (Underline: better than “Best SSL”)."
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9704016913319239,"Dataset
Metric
Best SSL
Random Weight
Equal Weight
AUTOSSL-ES
AUTOSSL-DS"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9725158562367865,"Citeseer
NMI
0.439±0.00
0.398±0.01
0.408±0.00
0.449±0.01
0.449±0.01
ACC
71.64±0.44
70.64±0.07
70.80±0.31
72.14±0.41
72.00±0.32
P-H
−
0.897
0.904
0.943
0.934"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9746300211416491,"Computers
NMI
0.433±0.00
0.341±0.03
0.290±0.00
0.447±0.01
0.448±0.01
ACC
87.26±0.15
86.86±0.25
87.24±0.38
87.26±0.64
88.18±0.43
P-H
−
0.406
0.378
0.503
0.511"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9767441860465116,"CoraFull
NMI
0.498±0.00
0.458±0.02
0.493±0.00
0.506±0.01
0.500±0.00
ACC
60.42±0.39
58.88±0.32
59.01±0.29
61.01±0.50
61.10±0.68
P-H
−
0.811
0.868
0.903
0.895"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9788583509513742,"CS
NMI
0.767±0.01
0.761±0.01
0.770±0.01
0.772±0.01
0.771±0.01
ACC
92.75±0.12
92.88±0.20
93.22±0.12
93.26±0.16
93.35±0.09
P-H
−
0.879
0.881
0.895
0.890"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9809725158562368,"Photo
NMI
0.509±0.01
0.341±0.02
0.366±0.02
0.560±0.04
0.511±0.03
ACC
92.08±0.37
92.04±0.28
92.54±0.29
92.04±0.89
92.71±0.32
P-H
−
0.412
0.472
0.791
0.626"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9830866807610994,"Physics
NMI
0.704±0.00
0.692±0.00
0.709±0.01
0.725±0.00
0.726±0.00
ACC
95.07±0.06
95.09±0.08
95.39±0.10
95.57±0.02
95.13±0.36
P-H
−
0.914
0.916
0.921
0.923"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.985200845665962,"WikiCS
NMI
0.341±0.01
0.305±0.01
0.323±0.01
0.366±0.01
0.344±0.02
ACC
75.81±0.17
76.29±0.17
76.49±0.21
76.80±0.13
76.58±0.28
P-H
−
0.675
0.690
0.751
0.749"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9873150105708245,"Table 7: Comparison with random search. The NMI indicates node clustering performance; ACC
indicates node classiﬁcation accuracy (%); P-H stands for pseudo-homophily."
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9894291754756871,"Dataset
Metric
Random Search
AUTOSSL-ES"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9915433403805497,"Citeseer
NMI
0.443±0.00
0.449±0.01
ACC
71.68±0.55
72.14±0.41
P-H
0.934
0.943"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9936575052854123,"H
BROADER IMPACT"
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9957716701902748,"Graph neural networks (GNNs) are commonly used for node and graph representation learning tasks
due to their representational power. Such models have also been recently proposed for use in large-
scale social platforms for tasks including forecasting (Tang et al., 2020a), friend ranking (Sankar
et al., 2021) and item recommendation (Ying et al., 2018; Wu et al., 2019a), which impact many end
users. Like other machine learning models, GNNs can suffer from typical unfairness issues which
may arise due to sensitive attributes, label parity issues, and more (Dai & Wang, 2021). Moreover,
GNNs can also suffer from degree-related biases (Tang et al., 2020b). Self-supervised learning (SSL)
is often used to learn high-quality representations without supervision from labeled data sources, and
is especially useful in low-resource settings or in pre-training/ﬁne-tuning scenarios. Several works
have illustrated the potential for representations learned in a self-supervised way to encode bias
unintentionally, for example in language modeling (Bender et al., 2021; Zhao et al., 2019), image
representation learning (Roberts et al., 2018) and outlier detection (Shekhar et al., 2020)."
SAMPLE K SETS OF TASKS WEIGHTS FROM A MULTIVARIATE NORMAL DISTRIBUTION,0.9978858350951374,"Our work on automated self-supervised learning with graph neural networks shares the caveats of
these two domains in terms of producing inadvertent or biased outcomes. We propose an approach to
learn self-supervised representations by utilizing multiple types of pretext tasks in conjunction with
one another. While this produces improved performance on standard tasks used for benchmarking
representation quality, it does not guarantee that these representations are fair and should be used
without typical fairness checks in industrial contexts. However, such concerns are not inherently
posed by our proposed ideas, but by the foundations it builds on in GNNs and SSL. We anticipate our
ideas will drive further research in more sophisticated and powerful self-supervised graph learning,
and do not anticipate direct negative outcomes from this work."
