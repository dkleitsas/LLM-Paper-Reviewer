Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002380952380952381,"Epistemic uncertainty is the part of out-of-sample prediction error due to the lack of
knowledge of the learner. Whereas previous work was focusing on model variance,
we propose a principled approach for directly estimating epistemic uncertainty by
learning to predict generalization error and subtracting an estimate of aleatoric
uncertainty, i.e., intrinsic unpredictability. This estimator of epistemic uncertainty
includes the effect of model bias (or misspeciﬁcation) and is useful in interactive
learning environments arising in active learning or reinforcement learning. In
addition to discussing these properties of Direct Epistemic Uncertainty Predic-
tion (DEUP), we illustrate its advantage against existing methods for uncertainty
estimation on downstream tasks including sequential model optimization and re-
inforcement learning. We also evaluate the quality of uncertainty estimates from
DEUP for probabilistic classiﬁcation of images and for estimating uncertainty
about synergistic drug combinations."
INTRODUCTION,0.004761904761904762,"1
INTRODUCTION"
INTRODUCTION,0.007142857142857143,"A remaining great challenge for machine learning research is purposeful knowledge-seeking by
learning agents, which can beneﬁt from estimation of epistemic uncertainty, i.e., a measure of lack
of knowledge, which could potentially be eliminated with enough data if the learner converges to
a Bayes-optimal predictor. Epistemic uncertainty estimation is already a key ingredient in active
learning and Bayesian optimization (Aggarwal et al., 2014; Frazier, 2018) as well as exploration
in Reinforcement Learning (Kocsis & Szepesvári, 2006; Tang et al., 2017). Epistemic uncertainty
(EU) thus tells us how much could be gained from learning around a particular area of state-space
or input data space. But how should we even quantify it? Much previous work has focused on
model variance (Gal & Ghahramani, 2016; Lakshminarayanan et al., 2017) as a proxy thereof, i.e.,
how different are the functions compatible with the learner’s preferences and the data. However,
this approach does not take into account model bias (or misspeciﬁcation), due to the learner’s ﬁnite
capacity or preferences, self-imposed to avoid overﬁtting or reduce computational costs. Recent
work (Masegosa, 2020) has also shown that Bayesian methods are suboptimal for learning predictive
models when the model is misspeciﬁed. Fig. 1 illustrates with Gaussian Processes (GP) that a model’s
posterior variance is not necessarily a good predictor of the reducible generalization error."
INTRODUCTION,0.009523809523809525,"0.5
0.0
0.5
1.0
1.5
2.0
2.5 0 2"
INTRODUCTION,0.011904761904761904,"Ground truth
GP mean"
INTRODUCTION,0.014285714285714285,"0.5
0.0
0.5
1.0
1.5
2.0
2.5
0 2 4"
INTRODUCTION,0.016666666666666666,"Epistemic uncertainty
GP variance"
INTRODUCTION,0.01904761904761905,"0.5
0.0
0.5
1.0
1.5
2.0
2.5
0 2 4"
INTRODUCTION,0.02142857142857143,"Epistemic uncertainty
DEUP's prediction"
INTRODUCTION,0.023809523809523808,"Figure 1: Top. A GP is trained on the blue points. GP uncertainty (model standard
deviation) in shaded blue. Left. Using GP variance as a proxy for EU here misses out
on important regions of the input space from which useful information can be acquired.
Right. With extra out-of-sample points, DEUP learns an estimator of the EU that
captures that information. Using this estimator to make decisions (e.g. active learning)
would focus on where more data could most reduce the loss. The experiments show
realistic use cases of DEUP in which less (or no) additional data is required to obtain
reasonable uncertainty estimators."
INTRODUCTION,0.02619047619047619,"Motivated
by
interactive
learn-
ing settings arising for example in
active learning, black-box optimiza-
tion and reinforcement learning, we
study as an alternative or complement
the quantiﬁcation of epistemic uncer-
tainty in terms of the loss function.
The total expected out-of-sample
loss at a point can be decomposed
into an epistemic (reducible) part
and an aleatoric (irreducible) part.
Aleatoric uncertainty is output noise,
the part of the error that is intrinsic
to the distribution of interest (and
independent of the choice of the"
INTRODUCTION,0.02857142857142857,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.030952380952380953,"learner), and it is also the error which may be achieved by a Bayes-optimal predictor. Consider the
case of a learning system with universal approximation properties, like modern deep learning, whose
effective capacity can be increased with more data, either via model selection, early stopping or
other means. In such cases, the part of the error which is not aleatoric is reducible with more data.
For this reason, we propose to deﬁne epistemic uncertainty as the difference between the expected
generalization error and aleatoric uncertainty. Estimating it is useful to an active or interactive learner
because it could be reduced with enough data, especially in regions of the input space we care about,
i.e., where EU is large. In order to estimate EU, we propose DEUP, for Direct Epistemic Uncertainty
Prediction, where we train a side learner (the uncertainty predictor) with an appropriate objective and
training data in order to predict this generalization error. An estimate of EU can thus be obtained by
subtracting an estimate of aleatoric uncertainty."
INTRODUCTION,0.03333333333333333,"In this paper we thus focus on two predictors: the main predictor (for the learning task of original
interest) and an error predictor which can be used to predict epistemic uncertainty of the main
predictor at given points, given some knowledge or estimate of the aleatoric uncertainty. The
proposed methodology can be seen as an end-to-end approach to epistemic uncertainty estimation,
i.e., to help estimate where a learner would acquire the most information in terms of out-of-sample
loss reduction, without relying on the brittle relationship between model variance and generalization
error, which can get murky in the presence of misspeciﬁcation (bias), as we argue below. In order
to select points with high EU, one would still need some kind of search (in the space of x), e.g., to
propose new candidates for active learning (Aggarwal et al., 2014; Nguyen et al., 2019; Bengio et al.,
2021) or sequential model optimization (Kushner, 1964; Jones et al., 1998; Snoek et al., 2012), or
exploration in RL (Kocsis & Szepesvári, 2006; Osband et al., 2016; Janz et al., 2019). S ˆf ˜f f ∗
y"
INTRODUCTION,0.03571428571428571,"bias
b ∆f"
INTRODUCTION,0.0380952380952381,≈std. dev. noise
INTRODUCTION,0.04047619047619048,ε( ˆf)
INTRODUCTION,0.04285714285714286,"Figure 2: Illustrating noise and bias. Ob-
served y is independent noise plus true
E[Y |x] = f ∗(x), itself best approxi-
mated by unknown ˜f(x) in parametric
set S (orange), e.g., using Bayesian pos-
terior distribution (grey) over parame-
ters. ˜f is the closest function in S to f ∗,
leading to a bias b(x) = f ∗(x) −˜f(x).
ε( ˆf)(x) = f ∗(x)−ˆf(x) is the reducible
error of the main predictor ˆf (e.g. pos-
terior mean), whose square corresponds
to the epistemic uncertainty or lack of
knowledge, that DEUP aims at estimat-
ing. With ˜f the unknown ideal predic-
tor and ˆf the actual (e.g. mean) pre-
dictor, the square of ∆f(x) = ˜f(x) −
ˆf(x) induces variance in the posterior.
ε( ˆf)(x) = b(x) + ∆f(x) indicates that
using the variance as a proxy for lack of
knowledge misses out a non-negligible
quantity: the bias b(x)."
INTRODUCTION,0.04523809523809524,"An interesting potential advantage of the proposed ap-
proach, compared with model variance estimators, is that
DEUP error predictors can be explicitly trained to care
about, and calibrate for estimating the generalization er-
ror for examples which may come from a distribution
different from the distribution of most of the training ex-
amples, i.e., an out-of-distribution (OOD) setting. Such
non-stationarities arise naturally in contexts like active
learning and reinforcement learning (RL) because the
learner explores new areas of the input space. In these
non-stationary settings, we typically want to retrain the
main predictor as we acquire new training data, not just
because more training data is generally better but also to
better track the changing distribution and generalize to yet
unseen but upcoming OOD inputs. This setting makes it
challenging to train the main predictor but it also entails
an even greater non-stationarity for the training data seen
by the error predictor: a large error initially made at a
point x before x is incorporated in the training set (along
with an outcome y) will typically be greatly reduced af-
ter updating the main predictor with (x, y). We propose
several strategies to cope with this non-stationarity, the
most important being to rely on additional features (such
as log-density estimates and model variance estimates at
x). These features account for variations in prediction
error arising due to updates of the main predictor, making
DEUP’s training distribution more stationary."
INTRODUCTION,0.047619047619047616,"The experiments presented here explore ﬁrst the scenario
of purely trying to estimate epistemic uncertainty in terms
of the recoverable generalization error, and second how
this can be used in contexts of sequential model optimiza-
tion (or black-box optimization) and exploration in RL
(where the issue of non-stationarity arises). The main
contributions of this paper are thus the following:"
INTRODUCTION,0.05,Under review as a conference paper at ICLR 2022
NOVEL END-TO-END APPROACH FOR DIRECTLY ESTIMATING THE EPISTEMIC UNCERTAINTY AS THE LACK OF,0.05238095238095238,"1. Novel end-to-end approach for directly estimating the epistemic uncertainty as the lack of
knowledge about the true f ∗(x) = E[Y |x], the Direct Epistemic Uncertainty Prediction (DEUP)
approach."
EXPERIMENTAL EVIDENCE THAT THIS DIRECT ESTIMATION OF EPISTEMIC UNCERTAINTY CAN LEAD TO MORE PRECISE,0.05476190476190476,"2. Experimental evidence that this direct estimation of epistemic uncertainty can lead to more precise
estimates of the lack of knowledge than variance-based methods."
A MEAN TO MITIGATE THE ISSUE OF NON-STATIONARITY ARISING IN TRAINING THE UNCERTAINTY ESTIMATOR IN,0.05714285714285714,"3. A mean to mitigate the issue of non-stationarity arising in training the uncertainty estimator in
interactive settings."
A MEAN TO MITIGATE THE ISSUE OF NON-STATIONARITY ARISING IN TRAINING THE UNCERTAINTY ESTIMATOR IN,0.05952380952380952,"4. A successful validation of the above in the contexts of (a) sequential model optimization and (b)
exploration in RL."
A MEAN TO MITIGATE THE ISSUE OF NON-STATIONARITY ARISING IN TRAINING THE UNCERTAINTY ESTIMATOR IN,0.06190476190476191,"2
ALEATORIC AND EPISTEMIC UNCERTAINTIES, MODEL VARIANCE"
A MEAN TO MITIGATE THE ISSUE OF NON-STATIONARITY ARISING IN TRAINING THE UNCERTAINTY ESTIMATOR IN,0.06428571428571428,"Consider a supervised learning algorithm (or learner) L mapping a dataset D to a predictive function
ˆf = L(D). L tries to minimize the expected value of a supervised learning loss l( ˆf(x), y) ∈R
under unknown P(Y |x). In this section, we focus on regression tasks with the squared loss l(ˆy, y) =
(ˆy −y)2, with y ∈R. In Appendix B, we provide the corresponding results for a general loss function."
A MEAN TO MITIGATE THE ISSUE OF NON-STATIONARITY ARISING IN TRAINING THE UNCERTAINTY ESTIMATOR IN,0.06666666666666667,Deﬁnition 2.1 The expected loss of a predictor ˆf at x is deﬁned as:
A MEAN TO MITIGATE THE ISSUE OF NON-STATIONARITY ARISING IN TRAINING THE UNCERTAINTY ESTIMATOR IN,0.06904761904761905,"U( ˆf, x) =
Z
( ˆf(x) −y)2dP(y|x).
(1)"
A MEAN TO MITIGATE THE ISSUE OF NON-STATIONARITY ARISING IN TRAINING THE UNCERTAINTY ESTIMATOR IN,0.07142857142857142,"Note how the expected loss is an unknown quantity, because we generally do not have access to the
true data generating distribution P(Y |x). The errors made by any predictor ˆf at x are due to both the
inherent randomness of P(Y |x) (aleatoric uncertainty) and the lack of knowledge of the predictor
that can be tackled by acquiring more information around x (epistemic uncertainty). Next, we will
deﬁne these concepts more formally. Because of this natural decomposition of the expected loss, we
will also refer to it as total uncertainty, and we will use the two terms interchangeably."
A MEAN TO MITIGATE THE ISSUE OF NON-STATIONARITY ARISING IN TRAINING THE UNCERTAINTY ESTIMATOR IN,0.07380952380952381,Bayes-optimal predictors f ∗satisfy the following equation at every x:
A MEAN TO MITIGATE THE ISSUE OF NON-STATIONARITY ARISING IN TRAINING THE UNCERTAINTY ESTIMATOR IN,0.0761904761904762,"∀˜y ∈R
Z
(f ∗(x) −y)2dP(y|x) ≤
Z
(˜y −y)2dP(y|x)."
A MEAN TO MITIGATE THE ISSUE OF NON-STATIONARITY ARISING IN TRAINING THE UNCERTAINTY ESTIMATOR IN,0.07857142857142857,"They depend on the underlying data distribution only and not on learner L or trained predictor
ˆf. Additionally, all Bayes-optimal predictors have the same total uncertainty for all x. Aleatoric
uncertainty is the irreducible expected error, i.e., that made by a Bayes-optimal predictor:"
A MEAN TO MITIGATE THE ISSUE OF NON-STATIONARITY ARISING IN TRAINING THE UNCERTAINTY ESTIMATOR IN,0.08095238095238096,"Deﬁnition 2.2 We deﬁne aleatoric uncertainty at x as the total uncertainty of any Bayes-optimal
predictor f ∗at x:
A(x) = U(f ∗, x).
(2)"
A MEAN TO MITIGATE THE ISSUE OF NON-STATIONARITY ARISING IN TRAINING THE UNCERTAINTY ESTIMATOR IN,0.08333333333333333,"and we note that by deﬁnition A(x) ≤U(f, x), ∀f ∀x."
A MEAN TO MITIGATE THE ISSUE OF NON-STATIONARITY ARISING IN TRAINING THE UNCERTAINTY ESTIMATOR IN,0.08571428571428572,"We now propose a deﬁnition of epistemic uncertainty of predictor ˆf as the gap between the error of ˆf
at x and the lowest possible error at x, i.e., the reducible part of the loss, given more knowledge."
A MEAN TO MITIGATE THE ISSUE OF NON-STATIONARITY ARISING IN TRAINING THE UNCERTAINTY ESTIMATOR IN,0.0880952380952381,"Deﬁnition 2.3 We deﬁne epistemic uncertainty E( ˆf, x) of a predictor ˆf at x as"
A MEAN TO MITIGATE THE ISSUE OF NON-STATIONARITY ARISING IN TRAINING THE UNCERTAINTY ESTIMATOR IN,0.09047619047619047,"E( ˆf, x) = U( ˆf, x) −A(x) = U( ˆf, x) −U(f ∗, x).
(3)"
A MEAN TO MITIGATE THE ISSUE OF NON-STATIONARITY ARISING IN TRAINING THE UNCERTAINTY ESTIMATOR IN,0.09285714285714286,"Using these deﬁnitions, we can present the main result about the regression setting."
A MEAN TO MITIGATE THE ISSUE OF NON-STATIONARITY ARISING IN TRAINING THE UNCERTAINTY ESTIMATOR IN,0.09523809523809523,"Proposition 1 In a regression task with ground truth P(y|x) = N(y; f ∗(x), σ2(x)),"
A MEAN TO MITIGATE THE ISSUE OF NON-STATIONARITY ARISING IN TRAINING THE UNCERTAINTY ESTIMATOR IN,0.09761904761904762,"E( ˆf, x) = ( ˆf(x) −f ∗(x))2"
A MEAN TO MITIGATE THE ISSUE OF NON-STATIONARITY ARISING IN TRAINING THE UNCERTAINTY ESTIMATOR IN,0.1,"U( ˆf, x) = Ey∼P (.|x)[( ˆf(x) −y)2] = ( ˆf(x) −f ∗(x))2 + σ2(x)"
A MEAN TO MITIGATE THE ISSUE OF NON-STATIONARITY ARISING IN TRAINING THE UNCERTAINTY ESTIMATOR IN,0.10238095238095238,A(x) = Ey∼P (.|x)[(f ∗(x) −y)2] = σ2(x)
A MEAN TO MITIGATE THE ISSUE OF NON-STATIONARITY ARISING IN TRAINING THE UNCERTAINTY ESTIMATOR IN,0.10476190476190476,Under review as a conference paper at ICLR 2022
A MEAN TO MITIGATE THE ISSUE OF NON-STATIONARITY ARISING IN TRAINING THE UNCERTAINTY ESTIMATOR IN,0.10714285714285714,"The proof is given in Appendix C. Additionally, we present in Appendix B a similar result for the
negative log-likelihood loss, with discrete or continuous outputs."
A MEAN TO MITIGATE THE ISSUE OF NON-STATIONARITY ARISING IN TRAINING THE UNCERTAINTY ESTIMATOR IN,0.10952380952380952,"We will discuss in the remainder of this section the relation between this new deﬁnition of epistemic
uncertainty and the more traditional way of modelling it through model variance or entropy."
A MEAN TO MITIGATE THE ISSUE OF NON-STATIONARITY ARISING IN TRAINING THE UNCERTAINTY ESTIMATOR IN,0.11190476190476191,"Consider a parametric model p(Y |x, θ) and a learner maintaining a distribution over parameters
θ ∈Θ, each corresponding to a predictor f in a parametric set of functions S, possibly starting from
a prior p(θ) that would lead to a posterior distribution p(θ|D). Clearly, the fact that multiple θ’s and
corresponding values of f are compatible with the data and the prior indicates lack of knowledge.
Because the lack of knowledge indicates where an interactive learner should acquire more information,
this justiﬁes the usage of dispersion measures, such as the variance or the entropy of the posterior
predictive, as proxies for epistemic uncertainty."
A MEAN TO MITIGATE THE ISSUE OF NON-STATIONARITY ARISING IN TRAINING THE UNCERTAINTY ESTIMATOR IN,0.11428571428571428,"However, the limited capacity of S or the prior p(θ) may pull the optimal ˜f in S away from the
Bayes-optimal predictor f ∗. We can refer to these self-imposed constraints as a form of bias, in
the sense that the learner is usually biased towards the prior preferences, e.g., towards smoother
predictors. In particular, this issue can arise when training neural networks with limited data, where
the network may not use all of its capacity due to implicit (and not fully understood) regularization
properties of SGD (Kale et al., 2021), explicit regularization, early stopping or a combination of these,
which can induce a preference on the functions it learns. In Fig. 2, we illustrate this gap with the bias
function b(x) = f ∗(x) −˜f(x). Because of this bias, model variance cannot be an accurate measure
of lack of knowledge E( ˆf, x) = ( ˆf(x) −f ∗(x))2 in the general case, as we have shown with the
results in Fig. 1. In Deep Ensembles (Lakshminarayanan et al., 2017) e.g., if all the networks in the
ensemble tend to fail in systematic ways, this aspect of prediction failure will not be captured by
variance. Whereas Deep Ensembles variance provide us uncertainty regarding which of the networks
we could draw is correct, this does not tell us how poor that network is even in a noise-free setting.
On the other hand, with ﬂexible models like neural networks, adding examples around x where b(x)2
is large may allow to increase capacity around x and reduce b(x)2."
A MEAN TO MITIGATE THE ISSUE OF NON-STATIONARITY ARISING IN TRAINING THE UNCERTAINTY ESTIMATOR IN,0.11666666666666667,"In section 5, we will provide more experimental evidence supporting our claim that directly estimating
the epistemic uncertainty as the reducible part of the loss can lead to better measures of lack of
knowledge, compared to variance-based methods, especially in interactive learning settings."
DIRECT EPISTEMIC UNCERTAINTY PREDICTION,0.11904761904761904,"3
DIRECT EPISTEMIC UNCERTAINTY PREDICTION"
DIRECT EPISTEMIC UNCERTAINTY PREDICTION,0.12142857142857143,"DEUP (Direct Epistemic Uncertainty Prediction) uses observed out-of-sample errors in order
to train an error predictor which can be used to estimate epistemic uncertainty elsewhere, as
suggested directly by deﬁnitions 2.1-2.3. These may be in-distribution or out-of-distribution errors,
depending on whichever we care about. The pseudo-code of DEUP in interactive settings is provided
below. The pseudo-code for the simpler version with a ﬁxed training set is given in Appendix E."
FIXED TRAINING SET,0.12380952380952381,"3.1
FIXED TRAINING SET"
FIXED TRAINING SET,0.1261904761904762,"Consider the simpler scenario with ﬁxed training set D and learning algorithm L yielding predictor
ˆf. We assume that N out-of-sample (validation) points have been set aside and can thus be used for
training an error predictor."
FIXED TRAINING SET,0.12857142857142856,"Training an error predictor e with (input, target) pairs (x, ( ˆf(x) −y)2) and squared loss yields an
estimator of the total uncertainty. To see that this is the right training set for the secondary predictor e,
we can assess what happens at the limit of inﬁnite data: the estimator is asymptotically unbiased, i.e.
limN→∞e(x) = U( ˆf, x) = E[( ˆf(x) −Y )2] for all x, if the learning algorithm ensures asymptotic
convergence to a Bayes-optimal predictor. Examples of such learners include k-nearest neighbors
with k increasing at a proper rate slower than n, and neural networks whose size and regularization
are hyper-parameters optimized on a growing validation set."
FIXED TRAINING SET,0.13095238095238096,"If A(x) = 0 then e(x) is an estimate of E( ˆf, x) as well as of U( ˆf, x). If an estimator a(x) of aleatoric
uncertainty is available, then e(x) −a(x) becomes an estimate of the epistemic uncertainty of ˆf at x.
Alternatively, one could train e, constrained to be positive (e.g. with a softplus(u) = log(exp(1 + u))
output non-linearity), with examples (x, ( ˆf(x)−y)2−a(x)) and also obtain an estimator of epistemic
uncertainty. When we have access to an oracle that samples Y given query x from the environment"
FIXED TRAINING SET,0.13333333333333333,Under review as a conference paper at ICLR 2022
FIXED TRAINING SET,0.1357142857142857,"P(Y |x) (e.g., in active learning or black-box optimization), then A(x) can be estimated using the
empirical variance of different outcomes of the oracle at the same input x; see Appendix D."
FIXED TRAINING SET,0.1380952380952381,"Note that because U and E are functions of the predictor ˆf as well (and ˆf is itself derived from the
training data D), the error predictor can also be trained on features that are informative of the pair
( ˆf, x) rather than x only. We will mainly use variance estimates (V), obtained from a separate model
(e.g. a GP), and log-density estimates (D), in addition to the input (x). The usage of these features is
particularly important in non-stationary settings, as discussed next.
3.2
DEUP IN INTERACTIVE SETTINGS
Interactive settings, in which epistemic uncertainty estimation is used in order to guide information
acquisition and iteratively gather new examples, provide a more interesting use case for DEUP (as
shown in Figures 4 and 5, where good epistemic uncertainty estimation translates in faster learning
curves). These settings are however more challenging: the main predictor is going to be retrained
multiple times, with the new examples being added to the observed dataset each time. Two kinds of
non-stationarities arise here:"
FIXED TRAINING SET,0.14047619047619048,"First, the new examples are not drawn i.i.d. (since their choice depends on the epistemic uncertainty
estimator obtained from earlier examples). This makes the stream of data seen by the main predictor
non-stationary. For example in sequential model optimization or RL, the new examples may concen-
trate more and more towards regions where the reward might be higher. Whereas it is not clear how
variance-based predictors can handle this, DEUP can in principle cope with it by virtue of having the
error predictor trained not on out-of-sample and in-distribution examples but on out-of-sample and
out-of-distribution examples of the kind we care about (e.g., as obtained from the interactive learning
process). The error predictor then learns to predict the error which would be made on typical new
examples coming from what we may call by analogy the ""frontier of knowledge"", while this frontier
continuously expands due to exploration and interactive learning: we expect such out-of-distribution
errors to be larger than in-distribution out-of-sample errors."
FIXED TRAINING SET,0.14285714285714285,"Second, the error predictor itself faces even more distributional change, even if the data stream seen
by the main predictor was i.i.d.. Indeed, after example x was selected, the main predictor will be
retrained using that additional data point, and x would now have much lower epistemic uncertainty.
This means that the targets of the error predictor also change, as the main predictor is updated with
newly acquired points."
FIXED TRAINING SET,0.14523809523809525,"Because the dataset D that is used to train the main predictor changes at each step of an active learning
process, it is necessary to see U and E as functions of the dataset as well: U( ˆf, x) = U(L(D), x),
meaning that we can no longer use (input, output) pairs (x, ˆf(x) −y)2) as mentioned in section
3.1 to train an estimator of the total uncertainty, but should in principle use (input, output) pairs
((x, D), ( ˆf(x)−y)2)), where ˆf = L(D). However, D being a very high-dimensional object, with size
growing with the number of acquired points, we may face severe overﬁtting issues when training an
uncertainty estimator using (x, D) as inputs. Hence, in this paper, we propose to use stationarizing
features of the dataset D at x, that we denote by φD(x), as inputs to the error predictor instead of
(x, D)."
FIXED TRAINING SET,0.14761904761904762,"In addition to the input x itself, we explored the use of two scalar features to stationarize the training
set of the error predictor: a model variance estimate, and a log-density estimate. Both have been used
as proxies for epistemic uncertainty in the literature. Furthermore, we considered a binary feature
s to incorporate in φD(x), that speciﬁes whether x was already used to train the main predictor (or
equivalently, x ∈D) or not."
FIXED TRAINING SET,0.15,"More formally, we use φD(x) =

x, s, log ˆq(x|D), ˆV ( ˜L, D, x)

, where ˆq(x|D) is a density estimate"
FIXED TRAINING SET,0.1523809523809524,"from data D at x, s = 1 if x ∈D otherwise 0, ˜L a learner that produces a distribution over predictors,
which can chosen to be the same as L, and ˆV ( ˜L, D, x) is an estimate of the model variance of ˜L
trained on D at x. ˆq is obtained by training a third model (besides the main predictor and the error
predictor) as a density estimator (such as a Kernel Density Estimator or a ﬂow-based deep network
(Rezende & Mohamed, 2015), in our case). Like the other predictors, the density estimator also needs
to be ﬁne-tuned when new data is added to the training set."
FIXED TRAINING SET,0.15476190476190477,"In our experiments, we found that using inputs φD(x), or even a subset of the 4 possible features, is
sufﬁcient to train an uncertainty estimator with targets ( ˆf(x) −y)2)."
FIXED TRAINING SET,0.15714285714285714,Under review as a conference paper at ICLR 2022
FIXED TRAINING SET,0.1595238095238095,"The pseudo-code is provided in Algorithm 1. Note that De is incremented twice with xacq but
φD(xacq) is different each time because xacq ﬁrst is and then is not yet in D."
KICKSTARTING THE ERROR PREDICTOR,0.1619047619047619,"3.2.1
KICKSTARTING THE ERROR PREDICTOR
In practical applications, whether in a ﬁxed dataset setting or in interactive settings, it is preferable to
use all the available data to train the main predictor, rather than keeping a subset to train the error
predictor. This would entail inaccurate uncertainty estimates once real out-of-sample unlabelled data
comes in. To tackle this, we propose a scheme inspired by standard K−fold cross-validation in
order to kickstart the error predictor with the initially available data. Multiple versions of the main
predictor, trained on different subsets of the training data D, are used to deﬁne targets for the error
predictor, whereas the subsets themselves provide the input features. The scheme is summarized in
the Optional Cross-Validation step of Algorithm 1. Note that in the case of multi-class classiﬁcation
tasks (with n classes), we obtain the folds by splitting the data based on classes, with each fold
containing ⌊n/K⌋classes. So when we train on K −1 folds, the ⌊n/K⌋classes from the remaining
fold are out-of-sample."
KICKSTARTING THE ERROR PREDICTOR,0.16428571428571428,"Algorithm 1 Training procedure for DEUP in an Active Learning setting
Data: Dinit initial training dataset with pairs (x, y) ∈X × R
a : X 7→R, estimator of aleatoric uncertainty
ˆf : X 7→R, main predictor (of y given x)
e : X 7→R, total uncertainty estimator
φ : X 7→Rk: chosen stationarizing features
acq: acquisition machinery that proposes new input points xacq from X, using the current ˆf and e
Training:
Initialize empty dataset to train the uncertainty estimator, De
Initialize D = Dinit, the dataset of training points seen so far
if kickstart_error_predictor then"
KICKSTARTING THE ERROR PREDICTOR,0.16666666666666666,"Cross-validation step: Pre-ﬁll De using training errors on initial training data Dinit:
while stopping criterion not reached do"
KICKSTARTING THE ERROR PREDICTOR,0.16904761904761906,"Split D into K random subsets D1, . . . , DK of equal size. Deﬁne ˜D = SK−1
k=1 Dk
Fit ˆf on ˜D, and ﬁt the features φ ˜
D on ˜D"
KICKSTARTING THE ERROR PREDICTOR,0.17142857142857143,De ←De ∪S
KICKSTARTING THE ERROR PREDICTOR,0.1738095238095238,"(x,y)∈D{

φ ˜
D(x), (y −ˆf(x))2
}
Fit e on De
end
end
xacq ←∅, yacq ←∅
while stopping criterion not reached do"
KICKSTARTING THE ERROR PREDICTOR,0.1761904761904762,"Optional (or every few iterations only): Fit a on D (e.g. see Appendix D)
Fit ˆf on D
Fit features in φD on D (e.g. density estimation)
De ←De ∪{

φD(xacq), (yacq −ˆf(xacq)2
}
if xacq ̸= ∅
Fit e on De
xacq ←acq(X, ˆf, e −a) (can be either a single point, or a batch of points)
Sample outcomes from the ground truth distribution: yacq ∼P( . |xacq)"
KICKSTARTING THE ERROR PREDICTOR,0.17857142857142858,"De ←De ∪{

φD(xacq), (yacq −ˆf(xacq)2
}"
KICKSTARTING THE ERROR PREDICTOR,0.18095238095238095,"D ←D ∪{(xacq, yacq)}
end"
RELATED WORK,0.18333333333333332,"4
RELATED WORK"
RELATED WORK,0.18571428571428572,"Kiureghian & Ditlevsen (2009) characterized the sources of uncertainty as aleatoric (inherent noise)
and epistemic (incomplete knowledge). Gaussian Processes or GPs (Williams & Rasmussen, 1995),
provide a way to capture epistemic uncertainty through the disagreement between the different
predictors that ﬁt the data."
RELATED WORK,0.1880952380952381,"In a deep learning context, Blundell et al. (2015); Kendall & Gal (2017); Depeweg et al. (2018) use
the posterior distribution of network weights (MacKay, 1992) in Bayesian Neural Networks (BNNs)
to capture epistemic uncertainty. Other techniques that rely on measuring the discrepancy between
different predictors as a proxy for epistemic uncertainty include MC-Dropout (Gal & Ghahramani,"
RELATED WORK,0.19047619047619047,Under review as a conference paper at ICLR 2022
RELATED WORK,0.19285714285714287,"2016), that interprets Dropout (Hinton et al., 2012) as a variational inference technique in BNNs.
These approaches, because they rely on sampling multiple sets of weights or dropout masks at
inference time, share some similarities with ensemble-based methods, that include bagging (Breiman,
1996) and boosting (Efron & Tibshirani, 1994), in which multiple predictors are trained, and used
jointly to make a prediction, although the latter measure variability due to the training set instead
of the spread of functions compatible with the given training set, as in Bayesian approaches. For
example, Shaker & Hüllermeier (2020) use Random Forests (Breiman, 2001) to estimate epistemic
uncertainty. Deep Ensembles (Lakshminarayanan et al., 2017) are closer to the Bayesian approach,
using an ensemble of neural networks that differ because of randomness in initialization and training
(as you would have with MCMC, albeit in a more heuristic way). In addition to this, several other
variants of this central idea of measuring discrepancy between different predictors have been proposed
recently (Malinin & Gales, 2018; Tagasovska & Lopez-Paz, 2019; Amini et al., 2020; Liu et al., 2020;
van Amersfoort et al., 2020; Wen et al., 2020; Antoran et al., 2020; Kirichenko et al., 2020; Malinin
et al., 2020a; van Amersfoort et al., 2021). We discuss these methods in more detail in Appendix A."
RELATED WORK,0.19523809523809524,"More closely related to DEUP, Yoo & Kweon (2019) proposed a loss prediction module for learning
to predict the value of loss function. Hu et al. (2020) also propose using a separate network that
learns to predict the variance of an ensemble. These methods, however, are trained only to capture
the in-sample error, and do not capture the out-of-sample error which is more relevant for scenarios
like active learning where we want to pick x where the reducible generalization error is large. EpiOut
(Umlauft et al., 2020; Hafner et al., 2019) propose learning a binary output that simply distinguishes
between low or high epistemic uncertainty."
EXPERIMENTS,0.1976190476190476,"5
EXPERIMENTS"
UNCERTAINTY ESTIMATION,0.2,"5.1
UNCERTAINTY ESTIMATION"
IMPORTANCE OF EPISTEMIC UNCERTAINTY CALIBRATION,0.20238095238095238,"5.1.1
IMPORTANCE OF EPISTEMIC UNCERTAINTY CALIBRATION"
IMPORTANCE OF EPISTEMIC UNCERTAINTY CALIBRATION,0.20476190476190476,"In tasks that use uncertainty estimates to make decisions, and when the resources are limited, it is
important to have well calibrated uncertainty estimates, to avoid exploring uninteresting areas of the
search space, as we will observe in the following subsections. We aim at comparing DEUP, which
presumably takes into account both bias and variance, with established methods that are based on
variance. Hence, we design a function with varying degrees of smoothness (as measured by the
absolute values of its derivatives), and use fewer training points in the less smooth regions, where
epistemic uncertainty will thus be higher. We ﬁrst consider the scenario described in Section 3.1,
where the task is to regress a known one-dimensional function."
IMPORTANCE OF EPISTEMIC UNCERTAINTY CALIBRATION,0.20714285714285716,"In Figure 1, we use a GP as a predictor, and compare the GP’s variance to the epistemic uncertainty as
per Eq. 3 (the square loss between the GP’s predicted mean and the ground truth, as per Proposition 1).
The ﬁgure shows that, naturally, in regions of the input space where little or no data is available, and
especially in the less smooth regions, the predicted variance underestimates the epistemic uncertainty
(squared error) by a higher margin compared to the other regions. We see that training another GP to
directly estimate this squared loss, using a small held out out-of-sample data set for which the ground
truth value is known, allows to close the gap between the predicted uncertainty and the true epistemic
uncertainty. Note that this second predictor can be trained using any chosen learning algorithm L."
IMPORTANCE OF EPISTEMIC UNCERTAINTY CALIBRATION,0.20952380952380953,"Naturally, if we had access to a held out set, it would be wiser to use it as part of the training set of
the main predictor, rather than the uncertainty predictor. Alternatively, we could use cross-validation
splits (Section 3.2.1), at the expense of computational efﬁciency, to avoid discarding precious data. In
active learning, it is the acquired points, before they are used to retrain the main predictor, that also act
as the out-of-sample examples to train DEUP. Using the stationarizing features introduced in Section
3.2, these acquired points should be informative enough for DEUP to generalize its uncertainty
estimates to unknown regions of the search space. In RL, because the targets (e.g. of Q-Learning) are
themselves estimates and moving, data seen at any particular point is normally out-of-sample and can
inform the uncertainty estimator, when the inputs are used with the stationarizing features.
5.1.2
EPISTEMIC UNCERTAINTY PREDICTIONS FOR REJECTING DIFFICULT EXAMPLES
Epistemic uncertainty estimates can be used to reject difﬁcult examples where the predictor might fail,
such as OOD inputs1. We thus consider a standard OOD Detection task (van Amersfoort et al., 2020;
2021), where we train a ResNet-18 (He et al., 2016) for CIFAR-10 classiﬁcation (Krizhevsky, 2009)"
IMPORTANCE OF EPISTEMIC UNCERTAINTY CALIBRATION,0.2119047619047619,"1e.g. rare but challenging inputs can be directed to a human, avoiding a costly mistake"
IMPORTANCE OF EPISTEMIC UNCERTAINTY CALIBRATION,0.21428571428571427,Under review as a conference paper at ICLR 2022
IMPORTANCE OF EPISTEMIC UNCERTAINTY CALIBRATION,0.21666666666666667,"Table 1: Spearman Rank Correlation Coefﬁcient (SRCC) between predicted uncertainty and OOD
generalization error (SVHN); Area under ROC Curve (AUROC) for OOD Detection (SVHN) with
CIFAR-10 ResNet-18 models (3 seeds). DEUP signiﬁcantly outperforms baselines."
IMPORTANCE OF EPISTEMIC UNCERTAINTY CALIBRATION,0.21904761904761905,"Model
SRCC
AUROC
MC-Dropout Gal & Ghahramani (2016)
0.287 ± 0.002
0.894 ± 0.008
Deep Ensemble Lakshminarayanan et al. (2017)
0.381 ± 0.004
0.933 ± 0.008
DUQ van Amersfoort et al. (2020)
0.376 ± 0.003
0.927 ± 0.013
DUE van Amersfoort et al. (2021)
0.378 ± 0.004
0.929 ± 0.005
DEUP (D+V)
0.426 ± 0.009
0.933 ± 0.010
and reject OOD examples using estimated uncertainty in the prediction. To facilitate rejection of
classes other than those in the training set, we use a Bernoulli Cross-Entropy Loss for each class (van
Amersfoort et al., 2020): l( ˆf(x), y) = −P
i yi log ˆfi(x) + (1 −yi) log(1 −ˆfi(x)), where y is a
one-hot vector (yi = 1 if i is the correct class, and 0 otherwise), and ˆfi(x) = predicted probability for
class i, so the target for out-of-distribution data (from other classes) is y = {0, . . . , 0}. To ascertain
how well an epistemic error estimate sorts non-training examples by the above NLL loss, we consider
the rank correlation between the predicted uncertainty and the observed OOD generalization error on
SVHN examples (Netzer et al., 2011). This metric focuses on the quality of the uncertainty estimates
rather than just their ability to simply classify in- vs out-of-distribution. We also report the AUC for
the OOD detection task; more details in Appendix F. Table 1 shows that with the variance from DUE
(van Amersfoort et al., 2021) and the density from MAF (Papamakarios et al., 2017) as DEUP inputs,
we obtain uncertainty estimates that have high rank correlation with the underlying generalization
errors and competitive AUROC, compared with the baselines. In addition, since the error predictor
is trained separately from the main predictor there is no explicit trade-off between the accuracy of
the main predictor and the quality of uncertainty estimates. We achieve competitive accuracy of
93.89% for the main predictor. We ignore the effect of aleatoric uncertainty (due to inconsistent
human labelling), which would require a human study to ascertain. We present additional results in a
distribution shift setting in Appendix F."
EPISTEMIC UNCERTAINTY ESTIMATION FOR DRUG COMBINATIONS,0.22142857142857142,"5.1.3
EPISTEMIC UNCERTAINTY ESTIMATION FOR DRUG COMBINATIONS
We validate DEUP in a real-world regression task predicting the synergy of drug combinations. While
much effort in drug discovery is spent on ﬁnding novel small molecules, a potentially cheaper method
is identifying combinations of pre-existing drugs which are synergistic (i.e., work well together).
However, every possible combination cannot be tested due to the high monetary cost and time
required to run experiments. Therefore, developing good estimates of epistemic uncertainty can help
practitioners select experiments that are both informative and promising. As shown in Table 3(c), the
out-of-sample error predicted by DEUP correlates better with residuals of the model on the test set in
comparison to several other uncertainty estimation methods. Moreover, DEUP better captured the
order of magnitude of the residuals as shown in Figure 3. Details on experiments and metrics are in
Appendix J."
EPISTEMIC UNCERTAINTY ESTIMATION FOR DRUG COMBINATIONS,0.22380952380952382,"(a) Ensemble
(b) DEUP"
EPISTEMIC UNCERTAINTY ESTIMATION FOR DRUG COMBINATIONS,0.2261904761904762,"Model
Corr. w. res.
U. Bound
Ratio
Log Likelihood
MC-Dropout
0.14 ± 0.07
0.56 ± 0.05
0.25 ± 0.12
−20.1 ± 6.8
Deep Ensemble
0.30 ± 0.09
0.59 ± 0.04
0.50 ± 0.13
−14.3 ± 4.7
DUE
0.12 ± 0.12
0.15 ± 0.03
0.80 ± 0.79
−13.0 ± 0.52
DEUP
0.47 ± 0.03
0.63 ± 0.05
0.75 ± 0.07
−3.5 ± 0.25
(c) Quality of uncertainty estimates from different methods."
EPISTEMIC UNCERTAINTY ESTIMATION FOR DRUG COMBINATIONS,0.22857142857142856,"Figure 3: Drug Combinations. Predicted mean and uncertainty on test set. 50 test examples are
ordered by increasing value of true synergy score (orange). Model predictions and uncertainties
in blue. Ensemble (a) (and MC-dropout, not shown) consistently underestimate uncertainty while
DEUP (b) captures the right order of magnitude. (c) Corr. w. res. shows correlation between model
residuals and predicted uncertainties ˆσ. A best-case Upper Bound on Corr. w. res. is obtained from
the correlation between ˆσ and true samples from N(0, ˆσ). Ratio is the ratio between col. 1 and 2
(larger is better). Log-likelihood: average over 3 seeds of per sample predictive log-likelihood."
SEQUENTIAL MODEL OPTIMIZATION,0.23095238095238096,"5.2
SEQUENTIAL MODEL OPTIMIZATION"
SEQUENTIAL MODEL OPTIMIZATION,0.23333333333333334,"Sequential model optimization or black-box optimization are forms of active learning. At each stage,
the learner chooses query examples to label, looking for examples with a high value of the unknown
oracle function. Such examples are selected so they have a high predicted value (to maximize the
unknown oracle function) and a large predicted uncertainty (offering the opportunity of discovering
yet higher values). Acquisition functions, such as Upper Conﬁdence Bound (UCB, Srinivas et al.
(2010)) and Expected Improvement (EI, Moˇckus (1975)) trade-off exploration and exploitation, and"
SEQUENTIAL MODEL OPTIMIZATION,0.2357142857142857,Under review as a conference paper at ICLR 2022
SEQUENTIAL MODEL OPTIMIZATION,0.23809523809523808,"one can select the next candidate by looking for x’s maximizing the acquisition function. We combine
UCB and EI with DEUP (DEUP-UCB and DEUP-EI) to perform active learning, treating the main
predictor and DEUP epistemic uncertainty prediction at x respectively as mean and variance of
a Gaussian distribution for the learner’s guess of the value of the oracle at x. We showcase how
using DEUP to calibrate GP variances (used as input for DEUP) allows for better performances in
higher-dimensional optimization tasks. Speciﬁcally, we compare DEUP-EI to TuRBO-EI (Eriksson
et al., 2019), a state-of-the-art method for sequential optimization, that ﬁts a collection of local GP
models instead of a global one in order to perform efﬁcient high-dimensional optimization, on the
Ackley function (Ackley, 2012) as oracle, a common benchmark for optimization algorithms. The
oracle function can be deﬁned for arbitrary dimensions, and has many local minima."
SEQUENTIAL MODEL OPTIMIZATION,0.24047619047619048,"In Figure 4, we compare different methods on the Ackley-10 function, in addition to the optimum
reached in budget-constrained optimization problems for different oracle input dimensions, and we
ﬁnd that adapting DEUP to TuRBO consistently outperforms regular TuRBO, especially in higher
dimensions. See also Appendix H for 1D and 2D black-box optimization tasks where DEUP-EI
outperforms GP-EI (Bull, 2011), as well as neural networks with MC-Dropout or Ensembles. We
ﬁnd GP-EI getting stuck in local optima whereas DEUP-EI was able to reach the global maximum
consistently. Experimental details are provided in Appendix H.3."
SEQUENTIAL MODEL OPTIMIZATION,0.24285714285714285,"0
50
100
150
200
250
Number of additional function calls 14 12 10 8 6 4 2 0"
SEQUENTIAL MODEL OPTIMIZATION,0.24523809523809523,Maximum value reached
SEQUENTIAL MODEL OPTIMIZATION,0.24761904761904763,"GP-EI
DEUP-EI
TuRBO-EI
TuRBO-DEUP-EI
True maximum"
SEQUENTIAL MODEL OPTIMIZATION,0.25,"2
5
10
20
100
Dimension of Ackley function 16 14 12 10 8 6 4 2 0"
SEQUENTIAL MODEL OPTIMIZATION,0.2523809523809524,Maximum reached after 100 iterations
SEQUENTIAL MODEL OPTIMIZATION,0.25476190476190474,"GP-EI
DEUP-EI
TuRBO-EI
TuRBO-DEUP-EI"
SEQUENTIAL MODEL OPTIMIZATION,0.2571428571428571,"Figure 4: Left.Maximum value reached by the different optimization methods, for the 10 dimensional
Ackley function. In each run, all the methods start with the same initial 20 points. The shaded areas
represent the standard error across 3 runs. Right. maximum value reached in the budget-constrained
setting, on the Ackley functions of different dimensions. The error bars represent the standard error
across 3 different runs, with different initial sets of 20 pairs. The budget is 120 function calls in total.
Higher is better and TuRBO-DEUP-EI is less hurt by dimensionality.
5.3
REINFORCEMENT LEARNING"
SEQUENTIAL MODEL OPTIMIZATION,0.25952380952380955,"Figure 5: Average regret on CartPole task. Error
bars for standard error across 5 runs."
SEQUENTIAL MODEL OPTIMIZATION,0.2619047619047619,"Similar to sequential model optimization, a key
challenge in reinforcement learning (RL) is ef-
ﬁcient exploration of the input state space. To
investigate the effectiveness of DEUP’s uncer-
tainty estimates in the context of RL, we in-
corporate epistemic uncertainties predicted by
DEUP to DQN (Mnih et al., 2013), which we
refer to as DEUP-DQN. Speciﬁcally, we train
the uncertainty predictor with the objective to
predict the TD-error, using log-density as a stationarizing input. The predicted uncertainties are then
used as an exploration bonus in the Q-values. Details of the experimental setup are in Appendix I.
We evaluate DEUP-DQN on CartPole, a classic RL task from bsuite (Osband et al., 2020), against
DQN + ϵ-greedy, DQN + MC-Dropout (Gal & Ghahramani, 2016) and Bootstrapped DQN (Osband
et al., 2016). Figure 5 shows that DEUP achieves lower regret faster, compared to all the baselines,
which demonstrates the advantage of DEUP’s uncertainty estimates for efﬁcient exploration.
6
CONCLUSION AND FUTURE WORK
Whereas standard measures of epistemic uncertainty focus on variance, we argue that bias (misspeciﬁ-
cation) can also be reduced with predictors like neural nets, e.g. using early stopping. In a regression
setup, the expected out-of-sample squared error minus aleatoric uncertainty thus captures all the
uncertainty about E[Y |x] that more data can allow to reduce. This motivates the DEUP estimator,
using a second network trained to predict the errors of the ﬁrst. In interactive settings, this nonetheless
raises non-stationarity challenges for this estimator and we propose input features to reduce this issue
and show experimentally their advantages. Future work should investigate ways to estimate aleatoric
uncertainty when one cannot simply query the oracle several times on the same x."
SEQUENTIAL MODEL OPTIMIZATION,0.2642857142857143,Under review as a conference paper at ICLR 2022
ETHICS STATEMENT,0.26666666666666666,ETHICS STATEMENT
ETHICS STATEMENT,0.26904761904761904,"The authors do not foresee any negative social impacts of this work, but of course the accumulation
of improvements in ML could be misused as it may give more power to nefarious agents."
REPRODUCIBILITY STATEMENT,0.2714285714285714,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.27380952380952384,"We provide the code to reproduce the results along with the submission. In addition to the code,
we also outline all the details of the procedure to ensure reproducibility of the results. The general
procedure for the method in the ﬁxed-data and interactive setting is provided in Algorithm 2 and
Algorithm 1 respectively. We also provide pseudocode for the speciﬁc instances of DEUP used for
the Drug-Combinations and Reinforcement Learning experiments in Algorithm 4 and Algorithm 3.
Detailed discussion about the methodology and hyperparameters are provided in the appendix in
Appendix F, Appendix H, Appendix J and Appendix I. We provide proofs for all the propositions in
Appendix C, with appropriate assumptions."
REFERENCES,0.2761904761904762,REFERENCES
REFERENCES,0.2785714285714286,"David Ackley. A connectionist machine for genetic hillclimbing, volume 28. Springer Science &amp;
Business Media, 2012."
REFERENCES,0.28095238095238095,"Charu C Aggarwal, Xiangnan Kong, Quanquan Gu, Jiawei Han, and S Yu Philip. Active learning: A
survey. In Data Classiﬁcation: Algorithms and Applications, pp. 571–605. CRC Press, 2014."
REFERENCES,0.2833333333333333,"Alexander Amini, Wilko Schwarting, Ava Soleimany, and Daniela Rus. Deep evidential regression.
In Neural Information Processing Systems (NeurIPS), 2020."
REFERENCES,0.2857142857142857,"Javier Antoran, James Allingham, and José Miguel Hernández-Lobato. Depth uncertainty in neural
networks. In Neural Information Processing Systems (NeurIPS), 2020."
REFERENCES,0.28809523809523807,"Maximilian Balandat, Brian Karrer, Daniel R. Jiang, Samuel Daulton, Benjamin Letham, An-
drew Gordon Wilson, and Eytan Bakshy. BoTorch: A Framework for Efﬁcient Monte-Carlo
Bayesian Optimization. In Advances in Neural Information Processing Systems 33, 2020. URL
http://arxiv.org/abs/1910.06403."
REFERENCES,0.2904761904761905,"Pedro J Ballester and John BO Mitchell. A machine learning approach to predicting protein–ligand
binding afﬁnity with applications to molecular docking. Bioinformatics, 26(9):1169–1175, 2010."
REFERENCES,0.29285714285714287,"Emmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, and Yoshua Bengio. Flow
network based generative models for non-iterative diverse candidate generation. NeurIPS’2021,
arXiv:2106.04399, 2021."
REFERENCES,0.29523809523809524,"Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in
neural network. In International Conference on Machine Learning, pp. 1613–1622. PMLR, 2015."
REFERENCES,0.2976190476190476,"Leo Breiman. Bagging predictors. Machine learning, 24(2):123–140, 1996."
REFERENCES,0.3,"Leo Breiman. Random forests. Machine learning, 45(1):5–32, 2001."
REFERENCES,0.30238095238095236,"Adam D Bull. Convergence rates of efﬁcient global optimization algorithms. Journal of Machine
Learning Research, 12(10), 2011."
REFERENCES,0.3047619047619048,"Youngseog Chung, Willie Neiswanger, Ian Char, and Jeff Schneider. Beyond pinball loss: Quantile
methods for calibrated uncertainty quantiﬁcation. arXiv preprint arXiv:2011.09588, 2020."
REFERENCES,0.30714285714285716,"Tomas Cihlar and Marshall Fordyce. Current status and prospects of hiv treatment. Current opinion
in virology, 18:50–56, 2016."
REFERENCES,0.30952380952380953,"Stefan Depeweg, Jose-Miguel Hernandez-Lobato, Finale Doshi-Velez, and Steffen Udluft. Decom-
position of uncertainty in bayesian deep learning for efﬁcient and risk-sensitive learning. In
International Conference on Machine Learning, pp. 1184–1193. PMLR, 2018."
REFERENCES,0.3119047619047619,Under review as a conference paper at ICLR 2022
REFERENCES,0.3142857142857143,"Michael W. Dusenberry, Ghassen Jerfel, Yeming Wen, Y. Ma, Jasper Snoek, K. Heller, Balaji
Lakshminarayanan, and Dustin Tran. Efﬁcient and scalable bayesian neural nets with rank-1
factors. In International Conference on Machine Learning (ICML), 2020."
REFERENCES,0.31666666666666665,"Bradley Efron and Robert J Tibshirani. An introduction to the bootstrap. CRC press, 1994."
REFERENCES,0.319047619047619,"David Eriksson, Michael Pearce, Jacob R Gardner, Ryan Turner, and Matthias Poloczek. Scalable
global optimization via local bayesian optimization. arXiv preprint arXiv:1910.01739, 2019."
REFERENCES,0.32142857142857145,"Sebastian Farquhar, Yarin Gal, and Tom Rainforth. On statistical bias in active learning: How and
when to ﬁx it. ICLR’2021, arXiv:2101.11665, 2021."
REFERENCES,0.3238095238095238,"Peter I. Frazier. A tutorial on bayesian optimization, 2018."
REFERENCES,0.3261904761904762,"Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. In International Conference on Machine Learning (ICML), pp.
1050–1059. PMLR, 2016."
REFERENCES,0.32857142857142857,"Danijar Hafner, Dustin Tran, Timothy Lillicrap, Alex Irpan, and James Davidson. Noise contrastive
priors for functional uncertainty. In Conference on Uncertainty in Artiﬁcial Intelligence (UAI),
2019."
REFERENCES,0.33095238095238094,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770–778, 2016."
REFERENCES,0.3333333333333333,"Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corrup-
tions and perturbations. Proceedings of the International Conference on Learning Representations,
2019."
REFERENCES,0.3357142857142857,"Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov.
Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint
arXiv:1207.0580, 2012."
REFERENCES,0.3380952380952381,"S. Hu, Nicola Pezzotti, and M. Welling. A new perspective on uncertainty quantiﬁcation of deep
ensembles. arXiv, 2020."
REFERENCES,0.3404761904761905,"Pavel Izmailov, D. Podoprikhin, T. Garipov, D. Vetrov, and A. Wilson. Averaging weights leads to
wider optima and better generalization. In Conference on Uncertainty in Artiﬁcial Intelligence
(UAI), 2018."
REFERENCES,0.34285714285714286,"David Janz, Jiri Hron, Przemysław Mazur, Katja Hofmann, José Miguel Hernández-Lobato, and Se-
bastian Tschiatschek. Successor uncertainties: Exploration and uncertainty in temporal difference
learning. In Neural Information Processing Systems (NeurIPS), 2019."
REFERENCES,0.34523809523809523,"Donald R Jones, Matthias Schonlau, and William J Welch. Efﬁcient global optimization of expensive
black-box functions. Journal of Global optimization, 13(4):455–492, 1998."
REFERENCES,0.3476190476190476,"Satyen Kale, Ayush Sekhari, and Karthik Sridharan. Sgd: The role of implicit regularization,
batch-size and multiple-epochs. In NeurIPS, 2021."
REFERENCES,0.35,"Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer
vision? In Neural Information Processing Systems (NeurIPS), 2017."
REFERENCES,0.3523809523809524,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference for Learning Representations, 2015."
REFERENCES,0.3547619047619048,"Polina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson. Why normalizing ﬂows fail to detect
out-of-distribution data. In Neural Information Processing Systems (NeurIPS), 2020."
REFERENCES,0.35714285714285715,"Armen Der Kiureghian and Ove Ditlevsen. Aleatory or epistemic? does it matter? Structural Safety,
31(2):105–112, 2009. ISSN 0167-4730. doi: https://doi.org/10.1016/j.strusafe.2008.06.020. Risk
Acceptance and Risk Communication."
REFERENCES,0.3595238095238095,Under review as a conference paper at ICLR 2022
REFERENCES,0.3619047619047619,"Levente Kocsis and Csaba Szepesvári. Bandit based monte-carlo planning. In In: ECML-06. Number
4212 in LNCS, pp. 282–293. Springer, 2006."
REFERENCES,0.36428571428571427,"Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University
of Toronto, 2009."
REFERENCES,0.36666666666666664,"Meelis Kull and Peter Flach. Novel decompositions of proper scoring rules for classiﬁcation:
Score adjustment as precursor to calibration. In Machine Learning and Knowledge Discovery in
Databases, 2015."
REFERENCES,0.36904761904761907,"H. J. Kushner. A new method of locating the maximum point of an arbitrary multipeak curve in the
presence of noise. Journal of Basic Engineering, 86:97–106, 1964."
REFERENCES,0.37142857142857144,"Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles. In Neural Information Processing Systems (NeurIPS),
2017."
REFERENCES,0.3738095238095238,"Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998."
REFERENCES,0.3761904761904762,"Jeremiah Zhe Liu, Zian Lin, Shreyas Padhy, Dustin Tran, Tania Bedrax-Weiss, and Balaji Laksh-
minarayanan. Simple and principled uncertainty estimation with deterministic deep learning via
distance awareness. In Neural Information Processing Systems (NeurIPS), 2020."
REFERENCES,0.37857142857142856,"David JC MacKay. A practical bayesian framework for backpropagation networks. Neural computa-
tion, 4(3):448–472, 1992."
REFERENCES,0.38095238095238093,"Wesley Maddox, Timur Garipov, Pavel Izmailov, Dmitry Vetrov, and Andrew Gordon Wilson. A
simple baseline for bayesian uncertainty in deep learning. In Neural Information Processing
Systems (NeurIPS), 2019."
REFERENCES,0.38333333333333336,"A Malinin and M Gales. Predictive uncertainty estimation via prior networks. In NIPS’18: Proceed-
ings of the 32nd International Conference on Neural Information Processing Systems, 2018."
REFERENCES,0.38571428571428573,"Andrey Malinin, Sergey Chervontsev, Ivan Provilkov, and Mark Gales. Regression prior networks,
2020a."
REFERENCES,0.3880952380952381,"Andrey Malinin, Bruno Mlodozeniec, and Mark Gales. Ensemble distribution distillation. In
International Conference on Learning Representations, 2020b. URL https://openreview.
net/forum?id=BygSP6Vtvr."
REFERENCES,0.3904761904761905,"Alina Malyutina, Muntasir Mamun Majumder, Wenyu Wang, Alberto Pessia, Caroline A Heckman,
and Jing Tang. Drug combination sensitivity scoring facilitates the discovery of synergistic and
efﬁcacious drug combinations in cancer. PLoS computational biology, 15(5):e1006752, 2019."
REFERENCES,0.39285714285714285,"Andres Masegosa. Learning under model misspeciﬁcation: Applications to variational and ensemble
methods. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in
Neural Information Processing Systems, 2020."
REFERENCES,0.3952380952380952,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013."
REFERENCES,0.3976190476190476,"Jonas Moˇckus. On bayesian methods for seeking the extremum. In Optimization techniques IFIP
technical conference, pp. 400–404. Springer, 1975."
REFERENCES,0.4,"Reza Bayat Mokhtari, Tina S Homayouni, Narges Baluch, Evgeniya Morgatskaya, Sushil Kumar,
Bikul Das, and Herman Yeger. Combination therapy in combating cancer. Oncotarget, 8(23):
38022, 2017."
REFERENCES,0.4023809523809524,"Harry L Morgan. The generation of a unique machine description for chemical structures-a technique
developed at chemical abstracts service. Journal of Chemical Documentation, 5(2):107–113, 1965."
REFERENCES,0.40476190476190477,Under review as a conference paper at ICLR 2022
REFERENCES,0.40714285714285714,"Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. In Advances in Neural Information
Processing Systems (NIPS), 2011."
REFERENCES,0.4095238095238095,"Vu-Linh Nguyen, Sébastien Destercke, and Eyke Hüllermeier. Epistemic uncertainty sampling. In
International Conference on Discovery Science, pp. 72–86. Springer, 2019."
REFERENCES,0.4119047619047619,"World Health Organization and Stop TB Initiative. Treatment of tuberculosis: guidelines. World
Health Organization, 2010."
REFERENCES,0.4142857142857143,"Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via
bootstrapped dqn. arXiv preprint arXiv:1602.04621, 2016."
REFERENCES,0.4166666666666667,"Ian Osband, Yotam Doron, Matteo Hessel, John Aslanides, Eren Sezener, Andre Saraiva, Katrina
McKinney, Tor Lattimore, Csaba Szepesvári, Satinder Singh, Benjamin Van Roy, Richard Sutton,
David Silver, and Hado van Hasselt. Behaviour suite for reinforcement learning. In International
Conference on Learning Representations, 2020. URL https://openreview.net/forum?
id=rygf-kSYwH."
REFERENCES,0.41904761904761906,"Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D. Sculley, Sebastian Nowozin, Joshua Dillon,
Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model's uncertainty? evaluating
predictive uncertainty under dataset shift. In Advances in Neural Information Processing Systems,
2019."
REFERENCES,0.42142857142857143,"George Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive ﬂow for density
estimation. In Neural Information Processing Systems (NeurIPS), 2017."
REFERENCES,0.4238095238095238,"Danilo Rezende and Shakir Mohamed. Variational inference with normalizing ﬂows. In International
Conference on Machine Learning (ICML), pp. 1530–1538. PMLR, 2015."
REFERENCES,0.4261904761904762,"Murat Sensoy, Lance Kaplan, and Melih Kandemir. Evidential deep learning to quantify classiﬁcation
uncertainty. In Neural Information Processing Systems (NeurIPS), pp. 3183–3193, 2018."
REFERENCES,0.42857142857142855,"Mohammad Hossein Shaker and Eyke Hüllermeier. Aleatoric and epistemic uncertainty with random
forests. In International Symposium on Intelligent Data Analysis, pp. 444–456. Springer, 2020."
REFERENCES,0.430952380952381,"Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine
learning algorithms. In Neural Information Processing Systems (NeurIPS), 2012."
REFERENCES,0.43333333333333335,"Niranjan Srinivas, Andreas Krause, Sham M Kakade, and Matthias Seeger. Gaussian process
optimization in the bandit setting: No regret and experimental design. In International Conference
on Machine Learning (ICML), 2010."
REFERENCES,0.4357142857142857,"Aravind Subramanian, Rajiv Narayan, Steven M Corsello, David D Peck, Ted E Natoli, Xiaodong
Lu, Joshua Gould, John F Davis, Andrew A Tubelli, Jacob K Asiedu, et al. A next generation
connectivity map: L1000 platform and the ﬁrst 1,000,000 proﬁles. Cell, 171(6):1437–1452, 2017."
REFERENCES,0.4380952380952381,"Natasa Tagasovska and David Lopez-Paz. Single-model uncertainties for deep learning. In Neural
Information Processing Systems (NeurIPS), 2019."
REFERENCES,0.44047619047619047,"Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen, Yan Duan, John Schulman,
Filip De Turck, and Pieter Abbeel. # exploration: A study of count-based exploration for deep
reinforcement learning. In Neural Information Processing Systems (NeurIPS), 2017."
REFERENCES,0.44285714285714284,"Jonas Umlauft, Armin Lederer, T. Beckers, and S. Hirche. Real-time uncertainty decomposition for
online learning control. ArXiv, abs/2010.02613, 2020."
REFERENCES,0.4452380952380952,"Meet P. Vadera, Adam D. Cobb, Borhan Jalaeian, and Benjamin M Marlin. Ursabench: Comprehen-
sive benchmarking of approximate bayesian inference methods for deep neural networks. ArXiv,
abs/2007.04466, 2020a."
REFERENCES,0.44761904761904764,"Meet P. Vadera, Borhan Jalaeian, and Benjamin M Marlin. Generalized bayesian posterior expectation
distillation for deep neural networks. In UAI, 2020b."
REFERENCES,0.45,Under review as a conference paper at ICLR 2022
REFERENCES,0.4523809523809524,"Joost van Amersfoort, Lewis Smith, Andrew Jesson, Oscar Key, and Yarin Gal. Improving determinis-
tic uncertainty estimation in deep learning for classiﬁcation and regression. CoRR, abs/2102.11409,
2021. URL https://arxiv.org/abs/2102.11409."
REFERENCES,0.45476190476190476,"Joost R. van Amersfoort, L. Smith, Y. Teh, and Yarin Gal. Simple and scalable epistemic uncertainty
estimation using a single deep deterministic neural network. In International Conference on
Machine Learning (ICML), 2020."
REFERENCES,0.45714285714285713,"Max Welling and Yee Whye Teh. Bayesian learning via stochastic gradient langevin dynamics.
In Proceedings of the 28th International Conference on International Conference on Machine
Learning, 2011."
REFERENCES,0.4595238095238095,"Yeming Wen, Dustin Tran, and Jimmy Ba. Batchensemble: An alternative approach to efﬁcient
ensemble and lifelong learning. In International Conference on Learning Representations, 2020."
REFERENCES,0.46190476190476193,"C. K. Williams and C. Rasmussen. Gaussian processes for regression. In Neural Information
Processing Systems (NeurIPS), 1995."
REFERENCES,0.4642857142857143,"Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P. Xing. Deep kernel learning.
In Arthur Gretton and Christian C. Robert (eds.), Proceedings of the 19th International Conference
on Artiﬁcial Intelligence and Statistics (AISTATS), volume 51 of Proceedings of Machine Learning
Research, pp. 370–378, Cadiz, Spain, 09–11 May 2016. PMLR. URL http://proceedings.
mlr.press/v51/wilson16.html."
REFERENCES,0.4666666666666667,"Donggeun Yoo and I. Kweon. Learning loss for active learning. 2019 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), pp. 93–102, 2019."
REFERENCES,0.46904761904761905,"Bulat Zagidullin, Jehad Aldahdooh, Shuyu Zheng, Wenyu Wang, Yinyin Wang, Joseph Saad, Alina
Malyutina, Mohieddin Jafari, Ziaurrehman Tanoli, Alberto Pessia, et al. Drugcomb: an integrative
cancer drug combination data portal. Nucleic acids research, 47(W1):W43–W51, 2019."
REFERENCES,0.4714285714285714,"Ruqi Zhang, Chunyuan Li, Jianyi Zhang, Changyou Chen, and Andrew Gordon Wilson. Cyclical
stochastic gradient mcmc for bayesian deep learning. In ICLR, 2020."
REFERENCES,0.4738095238095238,"Shuxing Zhang, Alexander Golbraikh, Scott Oloff, Harold Kohn, and Alexander Tropsha. A novel
automated lazy learning qsar (all-qsar) approach: method development, applications, and virtual
screening of chemical databases using validated all-qsar models. Journal of chemical information
and modeling, 46(5):1984–1995, 2006."
REFERENCES,0.47619047619047616,Under review as a conference paper at ICLR 2022
REFERENCES,0.4785714285714286,APPENDICES
REFERENCES,0.48095238095238096,"A
RELATED WORK"
REFERENCES,0.48333333333333334,"Recently, several novel deep learning-based techniques to estimate uncertainty with a single model
have been proposed. For example, Deep Evidential Regression Amini et al. (2020) is a method for
estimating epistemic uncertainty that is based on a parametric estimate of model variance. This is
in line with previous work using evidential uncertainty estimation (Malinin & Gales, 2018; Sensoy
et al., 2018). Orthonormal Certiﬁcates, a set of learned features with a suitable loss function, are used
in Tagasovska & Lopez-Paz (2019). These certiﬁcates capture the distance to the training set to learn
an estimate of the epistemic uncertainty. This is further studied in Liu et al. (2020) who formalize
distance awareness, which captures the model’s ability to quantify the distance of a test sample
from the training data manifold, as a necessary condition for uncertainty estimation. This distance
awareness can be captured with a weight normalization step in training, in addition to using a GP as
the output layer. DUE (van Amersfoort et al., 2021) is an instance of Deep Kernel Learning (Wilson
et al., 2016), which is deﬁned as a GP with a deep feature extractor inside the kernel. DUE improves
upon SNGP by using an inducing point GP, and bi-lipschitz constraints on the feature extractor, giving
better test set accuracies as well as improved training efﬁciency. DUN (Antoran et al., 2020) uses the
disagreement between the outputs from intermediate layers as a measure of uncertainty. DUQ (van
Amersfoort et al., 2020) on the other hand uses two-sided Jacobian regularization on RBF networks
(LeCun et al., 1998) for reliable uncertainty estimates."
REFERENCES,0.4857142857142857,"Wen et al. (2020) present an efﬁcient way of implementing ensembles of neural networks, by using
one shared matrix and a rank-1 matrix per member. The weights for each member are then computed
as the Hadamard product of the shared matrix and the rank-1 matrix of the member. There has also
been extensive work in scaling up Bayesian Neural Networks for high-dimensional data to capture
epistemic uncertainty. SWAG (Maddox et al., 2019) ﬁts a Gaussian distribution capturing the SWA
(Izmailov et al., 2018) mean and a covariance matrix representing the ﬁrst two moments of SGD
iterated. This distribution is then used as a posterior over the neural network weights. Dusenberry
et al. (2020) parametrize the BNN with a distribution on a rank-1 subspace for each weight matrix,
inspired by BatchEnsembles (Wen et al., 2020). Vadera et al. (2020b); Malinin et al. (2020b) propose
approaches to improve the efﬁciency of ensembles by distilling the distribution of predictions rather
than the average, thus preserving the information about the uncertainty captured by the ensemble."
REFERENCES,0.4880952380952381,"There is also a large body of work on methods for approximating samples from the Bayesian posterior
on large datasets with efﬁcient MCMC based approaches. (Welling & Teh, 2011; Zhang et al., 2020;
Vadera et al., 2020a). The variance of this posterior distribution can then be computed and used as an
uncertainty estimate. However, as we have discussed in the sections above, this does not account for
model misspeciﬁcation and thus is not an accurate estimate for the lack of knowledge of the predictor."
REFERENCES,0.49047619047619045,"Kull & Flach (2015) present several decompositions of the total expected loss, including the decom-
position into the epistemic and irreducible (aleatoric) loss. They present additive adjustments that
reduce the scoring rules like the log-loss and Brier score, but do not tackle the general problem of
uncertainty estimation."
REFERENCES,0.4928571428571429,"There are also interesting connections between the problem of out-of-distribution generalization
arising in sequential model optimization and Bayesian optimization, discussed here, and the possibility
of reweighing examples, see Farquhar et al. (2021)."
REFERENCES,0.49523809523809526,"B
EPISTEMIC UNCERTAINTY IN A GENERAL LOSS FUNCTION SETTING"
REFERENCES,0.4976190476190476,"We consider the setting presented in section 2, but with a general loss function l. We use the same
notations as in section 2."
REFERENCES,0.5,Deﬁnition B.1 The total uncertainty of f at x is deﬁned as:
REFERENCES,0.5023809523809524,"U(f, x) =
Z
l(f(x), y)dP(y|x).
(4)"
REFERENCES,0.5047619047619047,Under review as a conference paper at ICLR 2022
REFERENCES,0.5071428571428571,"Deﬁnition B.2 For a learning algorithm L which produces a distribution PL(f(x)|Dn) over possible
solutions f(x) at x, the model variance at x is deﬁned as"
REFERENCES,0.5095238095238095,"V (L, Dn, x) =
Z
l(f(x), ˆf(x))dPL(f(x)|Dn))
(5)"
REFERENCES,0.5119047619047619,"with ˆf(x) = arg min ¯
f(x)
R
l(f(x), ¯f(x))dPL(f(x)|Dn)). Note that for a loss function that is differ-
ent from the square loss, the semantics of variance (such as its non-negativity) might be lost."
REFERENCES,0.5142857142857142,"Let us consider the special cases of the negative log-likelihood loss in general (for outputs which may
be discrete or continuous) and that of the squared error loss (which ends up being a special case of
the former for normally distributed outputs). Below we see Q(Y |x) as a probability mass or density
function (over y), which is also a function of x."
REFERENCES,0.5166666666666667,"Deﬁnition B.3 The negative log-likelihood (NLL) loss takes as ﬁrst argument Q(Y |x) a probability
mass or density function and returns"
REFERENCES,0.5190476190476191,"lNLL(Q(Y |x), y) = −log Q(Y = y|x).
(6)"
REFERENCES,0.5214285714285715,"Proposition 2 For the NLL loss with ground truth P(Y |x) and predictor Q(Y |x), the total uncer-
tainty U(Q(Y |x), x) is a cross-entropy, i.e.,
U(Q(Y | . ), x) = CE(P(Y |x)||Q(Y |x))"
REFERENCES,0.5238095238095238,"= −
Z
dP(y|x) log Q(y|x)
(7)"
REFERENCES,0.5261904761904762,The proposition is shown by applying the deﬁnitions.
REFERENCES,0.5285714285714286,"Proposition 3 For the NLL loss with ground truth P(Y |x), the aleatoric uncertainty A(x) in this
setting is the entropy H[P(Y |x)] of the ground truth conditional:"
REFERENCES,0.530952380952381,"A(x) = −
Z
dP(y|x) log P(y|x) = H[P(Y |x)],
(8)"
REFERENCES,0.5333333333333333,"The proposition is shown from the cross-entropy CE(P(Y |x)||Q(Y |x)) being minimized when
Q = P."
REFERENCES,0.5357142857142857,"Proposition 4 For the NLL loss with ground truth P(Y |x) and predictor Q(Y |x), the epistemic
uncertainty E(Q(Y |x), x) is the Kullback-Liebler divergence between P and Q (with P as the
reference):
E(Q(Y | . ), x) = KL(P(Y |x)||Q(Y |x))"
REFERENCES,0.5380952380952381,"=
Z
dP(y|x) log P(y|x)"
REFERENCES,0.5404761904761904,"Q(y|x)
(9)"
REFERENCES,0.5428571428571428,"The proposition is shown by combining the above two propositions and the deﬁnition of epistemic
uncertainty."
REFERENCES,0.5452380952380952,"To move towards the MSE loss, consider the special case of NLL with a conditionally Normal output
density for both P and Q."
REFERENCES,0.5476190476190477,"Proposition 5 For the NLL loss with a conditionally Normal output density for both P and Q,
with respective means f ∗(x) and ˆf(x) and respective variances σ2
P (x) and σ2
Q(x), the epistemic
uncertainty is"
REFERENCES,0.55,"E(Q(Y | . ), x) =
1
2σ2
Q(x)lMSE( ˆf(x), f ∗(x))"
REFERENCES,0.5523809523809524,"+ KL(P(Y |x)|| ˜Q(Y |x)),
(10)"
REFERENCES,0.5547619047619048,"where ˜Q( . |x) is obtained by shifting Q( . |x) towards P( . |x) (i.e., ˜Q( . |x) is Gaussian with mean
f ∗(x) and variance σ2
Q(x) ), and the Bayes-optimal mean predictor is f ∗(x) = EP [Y |x]. Note that
if σP = σQ, then the KL term is zero."
REFERENCES,0.5571428571428572,Under review as a conference paper at ICLR 2022
REFERENCES,0.5595238095238095,"The proof is presented in Appendix C. We can compare with the MSE loss (which assumes a constant
variance σ = σP = σQ) and obtain the same result up to variance scaling."
REFERENCES,0.5619047619047619,"C
PROOFS"
REFERENCES,0.5642857142857143,"C.1
PROPOSITION 1"
REFERENCES,0.5666666666666667,"It is a well known result that, because f ∗(x) is the mean of P(.|x), it is also the minimizer of
ˆy 7→
R
(ˆy −y)2dP(y|x). f ∗is thus a Bayes Optimal predictor."
REFERENCES,0.569047619047619,By deﬁnition of the total uncertainty:
REFERENCES,0.5714285714285714,"U(f, x) =
Z
(f(x) −y)2dP(y|x) = E[(f(x) −Y )2]."
REFERENCES,0.5738095238095238,"Hence, by deﬁnition of aleatoric uncertainty:"
REFERENCES,0.5761904761904761,"A(x) = U(f ∗, x) = E[(f ∗(x) −Y )2]."
REFERENCES,0.5785714285714286,and by deﬁnition of epistemic uncertainty
REFERENCES,0.580952380952381,"E(f, x) = E

(f(x) −y)2 −(f ∗(x) −y)2"
REFERENCES,0.5833333333333334,= f(x)2 −f ∗(x)2 −2(f(x) −f ∗(x))f ∗(x)
REFERENCES,0.5857142857142857,= (f(x) −f ∗(x))2.
REFERENCES,0.5880952380952381,Which concludes the proof.
REFERENCES,0.5904761904761905,"C.2
PROPOSITION 5"
REFERENCES,0.5928571428571429,"From Equation 9, we get:"
REFERENCES,0.5952380952380952,"E(Q(Y | . ), x) = KL(P(Y |x)||Q(Y |x))"
REFERENCES,0.5976190476190476,= log σQ(x)
REFERENCES,0.6,"σP (x) + σ2
P (x) + (f(x) −f ∗(x))2"
REFERENCES,0.6023809523809524,"2σ2
Q(x)
−1 2"
REFERENCES,0.6047619047619047,"=
1
2σ2
Q(x)lMSE(f(x), f ∗(x)) + log σQ(x)"
REFERENCES,0.6071428571428571,"σP (x) + σ2
P (x) + (f ∗(x) −f ∗(x))2"
REFERENCES,0.6095238095238096,"2σ2
Q(x)
−1 2"
REFERENCES,0.611904761904762,"=
1
2σ2
Q(x)lMSE(f(x), f ∗(x)) + KL(P(Y |x)| ˜Q(Y |x))"
REFERENCES,0.6142857142857143,Which concludes the proof
REFERENCES,0.6166666666666667,"D
ESTIMATING ALEATORIC UNCERTAINTY WITH ACCESS TO AN ORACLE"
REFERENCES,0.6190476190476191,"In scenarios like active learning, one has access to an oracle from which we can obtain samples
of Y ∼P(Y |x) at any given point x. In that case, one can train an estimator a(x) of aleatoric
uncertainty by obtaining K > 1 samples y1, . . . yK at the same x, for a set of representative x’s."
REFERENCES,0.6214285714285714,"More formally, if we have multiple independent outcomes y1, . . . , yK
∼P(Y |x) for each
input point x, then training a predictor a with the squared loss on (input, target) examples

x,
K
K−1V ar(y1, . . . , yK)

, where V ar denotes the empirical variance, yields an estimator of the
aleatoric uncertainty."
REFERENCES,0.6238095238095238,"Naturally, this estimator is asymptotically unbiased, if the learning algorithm ensures asymptotic
convergence to a Bayes-Optimal predictor."
REFERENCES,0.6261904761904762,"This is due to the fact that Ey1,...,yK
h
K
K−1V ar(y1, . . . , yK)
i
= V arP (y|x)[y|x], which according to"
REFERENCES,0.6285714285714286,Proposition 1 is equal to A(x).
REFERENCES,0.6309523809523809,Under review as a conference paper at ICLR 2022
REFERENCES,0.6333333333333333,"E
PSEUDO CODES"
REFERENCES,0.6357142857142857,"Algorithm 2 illustrates the training procedure when a held-out validation set is available. We focus
on y ∈R in this paper because it makes sense for active learning applied to black-box optimization
(where we want to maximize it) but the algorithms can trivially be applied to the case where y ∈Rd
for any d. Similarly, the algorithms can be generalized to other losses besides the square loss,
following the generalized theory presented above in Appendix B."
REFERENCES,0.638095238095238,"Algorithm 2 DEUP with a ﬁxed training set: Training procedure to obtain estimates of epistemic
uncertainty
Data: D the training dataset with pairs (x, y) with x ∈X, y ∈R; Dout the out-of-sample dataset
with pairs (x, y), to train the uncertainty estimator
X, the input/search space
a : X 7→R, trained estimator of aleatoric uncertainty (
f : X 7→R, main predictor, trained on D
e : X 7→R, total uncertainty estimator (estimates error of f)
Training:
Initialize empty dataset of errors De
Cross-validation step (Optional, if using other features for the error predictor): Similar to the
corresponding step of Algorithm 1.
for every pair (x, y) in D ∪Dout do"
REFERENCES,0.6404761904761904,"De ←De ∪{(x, (y −f(x))2)}
end
Fit e on De
Evaluation: For every input x, return e(x) −a(x) as an estimator of epistemic uncertainty at x"
REFERENCES,0.6428571428571429,"F
REJECTING DIFFICULT EXAMPLES"
REFERENCES,0.6452380952380953,"We adapt the standard OOD rejection task (van Amersfoort et al., 2020; Liu et al., 2020) to measure
the Spearman Rank Correlation of the predicted uncertainty with the true generalization error, in
addition to the OOD Detection AUROC. We use MC-Dropout (Gal & Ghahramani, 2016), Deep En-
semble (Lakshminarayanan et al., 2017), DUE(van Amersfoort et al., 2021) and DUQ van Amersfoort
et al. (2020) as the baselines 2. We use these baselines as representatives for the major approaches for
uncertainty estimation in recent literature. For all the methods, including DEUP we consider two
architectures for the main predictor, ResNet-18 and ResNet-50 (He et al., 2016) (Table 2) to study the
effect of model capacity. Note that for the ResNet50 DEUP model we continue using the ResNet-18
based DUE as variance source."
REFERENCES,0.6476190476190476,"Table 2: Spearman Rank Correlation between predicted uncertainty and the true generalization error
on OOD data (SVHN) with ResNet-50 models (3 seeds) trained on CIFAR-10."
REFERENCES,0.65,"Model
ResNet-50
MC-Dropout
0.312 ± 0.003
Deep Ensemble
0.401 ± 0.004
DUQ
0.399 ± 0.003
DEUP (D+V)
0.465 ± 0.002"
REFERENCES,0.6523809523809524,"Training
The baselines were trained with the CIFAR-10 training set with 10% set aside as a
validation set for hyperparameter tuning. The hyperparameters are presented in Table 3 and Table 4.
The hyperparameters not speciﬁed are set to the default values. For DEUP, we consider the log-density,
model-variance estimate and the seen-unseen bit as the features for the error predictor. The density"
REFERENCES,0.6547619047619048,"2MC-Dropout and Deep Ensemble baselines are based on https://github.com/google/uncertainty-
baselines, DUQ based on https://github.com/y0ast/deterministic-uncertainty-quantiﬁcation and DUE based
on https://github.com/y0ast/DUE"
REFERENCES,0.6571428571428571,Under review as a conference paper at ICLR 2022
REFERENCES,0.6595238095238095,"estimator we use is Masked-Autoregressive Flows (Papamakarios et al., 2017) and the variance
estimator used is DUE (van Amersfoort et al., 2021). Note that as indicated earlier x, the input image,
is not used as a feature for the error predictor. We present those ablations in the next sub-section.
For training DEUP, the CIFAR-10 training set is divided into 5 folds, with each fold containing
8 unique classes. For each fold, we train an instance of the main predictor, density estimator and
model variance estimator on only the corresponding 8 classes. The remaining 2 classes act as the
out-of-distribution examples for training the error predictor. Using these folds we construct a dataset
for training the error predictor, a simple feed forward network. The error predictor is trained with the
log targets (i.e. log MSE between predicted and observed error). This helps since the scale of the
errors varies over multiple orders of magnitude. We then train the main predictor, density estimator
and the variance estimator on the entire CIFAR-10 dataset, for evaluation. The hyperparameters
are presented in Table 4. For all models, we train the main predictor for 75 and 125 epochs for
ResNet-18 and ResNet-50 respectively. We use SGD with Momentum (set to 0.9), with a multi-step
learning schedule with a decay of 0.2 at epochs [25, 50] and [45, 90] for ResNet-18 and ResNet-50
respectively. One complete training run for DEUP takes about 1.5-2 GPU days on a V100 GPU. In
total these set of experiments took about 31 GPU days on a Nvidia V100 GPU."
REFERENCES,0.6619047619047619,"Table 3: Left: Hyperparameters for training Deep Ensemble (Lakshminarayanan et al., 2017). Right:
Hyperparameters for training MC-Dropout (Gal & Ghahramani, 2016)."
REFERENCES,0.6642857142857143,"Parameters
Model
ResNet-18
ResNet-50
Number of members
5
5
Learning Rate
0.05
0.01"
REFERENCES,0.6666666666666666,"Parameters
Model
ResNet-18
ResNet-50
Number of samples
50
50
Dropout Rate
0.15
0.1
L2 Regularization Coefﬁcient
6e-5
8e-4
Learning Rate
0.05
0.01"
REFERENCES,0.669047619047619,"Table 4: Left: Hyperparameters for training DUQ (van Amersfoort et al., 2020). Right: Hyperpa-
rameters for training DUE (van Amersfoort et al., 2021)."
REFERENCES,0.6714285714285714,"Parameters
Model
ResNet-18
ResNet-50
Gradient Penalty
0.5
0.65
Centroid Size
512
512
Length scale
0.1
0.2
Learning Rate
0.05
0.025"
REFERENCES,0.6738095238095239,"Parameters
Model
ResNet-18
Inducing Points
50
Kernel
RBF
Lipschitz Coefﬁcient
2
BatchNorm Momentum
0.99
Learning Rate
0.05
Weight Decay
0.0005"
REFERENCES,0.6761904761904762,"Ablations
We also perform some ablation experiments to study the effect of each feature for the
error predictor. The Spearman rank correlation coefﬁcient between the generalization error and
the variance feature, V , from DUE van Amersfoort et al. (2021) alone is 37.84 ± 0.04, and the
log-density, D, from MAF Papamakarios et al. (2017) alone is 30.52 ± 0.03. With only the image
(x) the SRCC is 36.58 ± 0.16"
REFERENCES,0.6785714285714286,"Table 6 presents the results for these experiments. We observe that combining all the features performs
the best. Also note that using the log-density and variance as features to the error predictor we observe
better performance than using them directly, indicating that the error predictor perhaps captures"
REFERENCES,0.680952380952381,Table 5: Hyperparameters for training DEUP.
REFERENCES,0.6833333333333333,"Parameters
Model
ResNet-18
ResNet-50
Uncertainty Predictor Architecture
[1024] x 5
[1024] x 5
Uncertainty Predictor Epochs
100
100
Uncertainty Predictor LR
0.01
0.01
Main Predictor Learning Rate
0.05
0.01"
REFERENCES,0.6857142857142857,Under review as a conference paper at ICLR 2022
REFERENCES,0.6880952380952381,"a better target for the epistemic uncertainty. The boolean feature (B) indicating seen examples,
discussed in Section 3.2, also leads to noticeable improvments."
REFERENCES,0.6904761904761905,"Table 6: Spearman Rank Correlation between predicted uncertainty and the true generalization error
on OOD data (SVHN) with variants of DEUP with different features as input for the uncertainty
predictor. D indicates the log-density from MAF Papamakarios et al. (2017), V indicates variance
from DUQ van Amersfoort et al. (2020) and B indicates a bit indicating if the data is seen."
REFERENCES,0.6928571428571428,"Features
Model
ResNet-18
ResNet-50
D+V +B
0.426 ± 0.009
0.465 ± 0.002
D+V
0.419 ± 0.003
0.447 ± 0.003
V +B
0.401 ± 0.004
0.419 ± 0.004
D+B
0.403 ± 0.003
0.421 ± 0.002"
REFERENCES,0.6952380952380952,"F.1
PREDICTING UNCERTAINTY UNDER DISTRIBUTION SHIFT"
REFERENCES,0.6976190476190476,"We also consider the task of uncertainty estimation in the setting of shifted distributions (Ovadia et al.,
2019; Hendrycks & Dietterich, 2019). We evaluate the uncertainty predictions of models trained
with CIFAR-10, on CIFAR-10-C (Hendrycks & Dietterich, 2019) which consists of images from
CIFAR-10 distorted using 16 corruptions like gassian blur, impulse noise, among others. Figure 6
shows that even in the shifted distribution setting, the uncertainty estimates of DEUP correlate much
better with the error made by the predictor, compared to the baselines."
REFERENCES,0.7,"G
DEUP IN THE PRESENCE OF ALEATORIC UNCERTAINTY"
REFERENCES,0.7023809523809523,"In the presence of aleatoric uncertainty, we have seen that DEUP’s error predictor e is an estimate
of the total uncertainty, rather than the epistemic uncertainty. However, if we have access to to an
estimator of aleatoric uncertainty a, then e −a becomes an estimator of epistemic uncertainty. To
show the difference in behavior of DEUP when there is aleatoric uncertainty, we consider, a modiﬁed
version of the experiment in Section 5.1.1, with a non-deterministic oracle (ground truth function).
Because of the noisy training dataset, GP conﬂates epistemic and aleatoric uncertainty, which makes
the gap between the predicted epistemic uncertainty (as measured by the GP variance) and the true
epistemic uncertainty (as measured by the MSE between the GP mean and the noiseless ground truth
function) higher than in the deterministic setting of Section 5.1.1."
REFERENCES,0.7047619047619048,"Similarly, in order to train DEUP’s uncertainty estimator, more out-of-sample data is needed compared
to the noiseless setting. In ﬁgure 7, we consider the setting described in Section D, and train a separate
predictor on the targets y1, . . . , yk to estimate the aleatoric uncertainty (which boils down to the
variance of the ground-truth function). Because these targets are themselves noisy, we chose a simple
linear regressor as the estimator of aleatoric uncertainty, to avoid overﬁtting to the noise."
REFERENCES,0.7071428571428572,"A key distinction of this setting, is that DEUP’s training data (the errors of the main predictor) are
themselves noisy, which makes it important to use more out-of-sample data to obtain reasonable total
uncertainty estimates (from which we subtract the estimates of the aleatoric uncertainty)."
REFERENCES,0.7095238095238096,"H
SEQUENTIAL MODEL OPTIMIZATION EXPERIMENTS"
REFERENCES,0.7119047619047619,"We use BoTorch3 (Balandat et al., 2020) as the base framework for our experiments."
REFERENCES,0.7142857142857143,"For all our Sequential Optimization algorithms, we use Algorithm 1 to train DEUP uncertainty
estimators. We found that the optional step of pre-ﬁlling the uncertainty estimator dataset De was
important given the low number of available training points. We used half the initial training set
(randomly chosen) as in-sample examples (used to train the main predictor and an extra-feature
generator) and the other half as out-of-sample examples to provide instances of high epistemic"
REFERENCES,0.7166666666666667,3https://botorch.org/
REFERENCES,0.719047619047619,Under review as a conference paper at ICLR 2022
REFERENCES,0.7214285714285714,gaussian_noise
REFERENCES,0.7238095238095238,brightness
REFERENCES,0.7261904761904762,contrast
REFERENCES,0.7285714285714285,pixelate
REFERENCES,0.7309523809523809,speckle_noise
REFERENCES,0.7333333333333333,shot_noise
REFERENCES,0.7357142857142858,impulse_noise
REFERENCES,0.7380952380952381,defocus_blur
REFERENCES,0.7404761904761905,gaussian_blur
REFERENCES,0.7428571428571429,zoom_blur fog
REFERENCES,0.7452380952380953,elastic_transform
REFERENCES,0.7476190476190476,jpeg_compression
REFERENCES,0.75,spatter
REFERENCES,0.7523809523809524,saturate frost
REFERENCES,0.7547619047619047,Corruption Type 0.10 0.15 0.20 0.25 0.30 0.35 0.40
REFERENCES,0.7571428571428571,Spearman Rank Correlation Coefficient
REFERENCES,0.7595238095238095,"Ensembles
MC Dropout
DUE
DEUP"
REFERENCES,0.7619047619047619,"Figure 6: Spearman Rank Correlation Coefﬁcient between the predicted uncertainty and true error
for models trained with CIFAR-10, and evaluated on CIFAR-10-C. DEUP outperforms the baselines
on all types of corruptions."
REFERENCES,0.7642857142857142,"uncertainty to train an uncertainty predictor; we repeated the procedure by alternating the roles of the
two halves of the dataset. We repeated the whole procedure twice using a new random split of the
dataset, thus ending up with 4 training points in De for every initial training point in Dinit."
REFERENCES,0.7666666666666667,"The error predictor is trained with the log targets (i.e. log MSE between predicted and observed
error). This helps since the scale of the errors varies over multiple orders of magnitude."
REFERENCES,0.7690476190476191,"Computationally, the training time of DEUP-EI depends on various choices (e.g. the features used to
train the epistemic uncertainty predictor, the dimension of the input, the learning algorithms, etc..).
Additionally, the training time for the uncertainty predictor varies at each step of the optimization. In
total, the sequential optimization experiments took about 1 CPU day."
REFERENCES,0.7714285714285715,"H.1
ONE-DIMENSIONAL FUNCTION TOY EXAMPLE"
REFERENCES,0.7738095238095238,"In Figure 8, we show the results of DEUP-EI, compared to GP-EI, MCDropout-EI and Ensembles-EI,
in the task of optimizing a synthetic one-dimensional function. Because MCDropout and Ensembles
are trained on in-sample data only, they are unable to generalize their uncertainty estimates, which
makes them bad candidates for Sequential Model Optimization, because they are easily stuck in
local minima, and require many iterations before the acquisition function gives more weight to the
predicted uncertainties than the current maximum."
REFERENCES,0.7761904761904762,"For Random acquisition, we sampled for different seeds 56 points, and used the (average across the
seeds of the) maximum of the ﬁrst 6 values as the ﬁrst value in the plots (Figures 4 and 8). Note that"
REFERENCES,0.7785714285714286,Under review as a conference paper at ICLR 2022
REFERENCES,0.780952380952381,"0.5
0.0
0.5
1.0
1.5
2.0
2.5 2 0 2"
REFERENCES,0.7833333333333333,"Ground truth
GP mean"
REFERENCES,0.7857142857142857,"0.5
0.0
0.5
1.0
1.5
2.0
2.5
0 5"
"EPISTEMIC UNCERTAINTY
GP VARIANCE",0.7880952380952381,"10
Epistemic uncertainty
GP variance"
"EPISTEMIC UNCERTAINTY
GP VARIANCE",0.7904761904761904,"0.5
0.0
0.5
1.0
1.5
2.0
2.5 0 10"
"EPISTEMIC UNCERTAINTY
GP VARIANCE",0.7928571428571428,"Epistemic uncertainty
DEUP's prediction"
"EPISTEMIC UNCERTAINTY
GP VARIANCE",0.7952380952380952,"Figure 7: Top. A GP is trained to regress a function using noisy samples. GP uncertainty (model
standard deviation) is shaded in blue. Bottom left. Using GP variance as a proxy for epistemic
uncertainty misses out on more regions of the input space, when compared to Figure 1 . Bottom
right. Using additional out-of-sample data in low density regions, a second GP is trained to predict
the generalization error of the ﬁrst GP (total uncertainty). Using second samples from the oracle
for each of the training points, a linear regressor ﬁts the training pairs (x, 1"
"EPISTEMIC UNCERTAINTY
GP VARIANCE",0.7976190476190477,"2(y1 −y2)2) to estimate
the pointwise aleatoric uncertainty (constant in this case). The aleatoric uncertainty is subtracted
from DEUP’s (second GP) predictions to obtain more accurate of epistemic uncertainty. Note
that no constraint is imposed on DEUP’s outputs, which explains the predicted negative values for
uncertainties. In practice, if these predicted uncertainties were to be used, (soft) clipping should be
used."
"EPISTEMIC UNCERTAINTY
GP VARIANCE",0.8,"because the function is speciﬁcally designed to have multiple local maxima, GP-EI also required
more optimization steps, and actually performed worse than random acquistion"
"EPISTEMIC UNCERTAINTY
GP VARIANCE",0.8023809523809524,"As a stationarizing input feature, we used the variance of a GP ﬁt on the available data at every step.
We found that the binary (in-sample/out-of-sample) feature and density estimates were redundant
with the variance feature and didn’t improve the performance as captured by the number of additional
function calls. We used a GP for the DEUP uncertainty estimator. Using a neural net provided
similar results, but was computationally more expensive in this 1-D case with few datapoints. We
used a 3-hidden layer neural network, with 128 neurons per layer and a ReLU activation function,
with Adam (Kingma & Ba, 2015) and a learning rate of 10−3 (and default values for the other
hyperparameters) to train the main predictor for DEUP-EI (in order to ﬁt the available data). The
same network architecture and learning rate were used for the Dropout and Ensemble baselines. We
used 3 networks for the Ensemble baseline, and a dropout probability of 0.3 for the Dropout baseline,
with 100 test-time forward passes to compute uncertainty estimates."
"EPISTEMIC UNCERTAINTY
GP VARIANCE",0.8047619047619048,Under review as a conference paper at ICLR 2022
"EPISTEMIC UNCERTAINTY
GP VARIANCE",0.8071428571428572,"1.0
0.5
0.0
0.5
1.0
1.5
2.0
x 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 f(x)"
"EPISTEMIC UNCERTAINTY
GP VARIANCE",0.8095238095238095,"0
10
20
30
40
50
Number of additional function calls 0.4 0.5 0.6 0.7 0.8 0.9"
"EPISTEMIC UNCERTAINTY
GP VARIANCE",0.8119047619047619,Maximum value reached
"EPISTEMIC UNCERTAINTY
GP VARIANCE",0.8142857142857143,"GP-EI
Random acquisition
DEUP-EI
MCDropout-EI
Ensemble-EI
True maximum"
"EPISTEMIC UNCERTAINTY
GP VARIANCE",0.8166666666666667,"Figure 8: Left. Synthetic function to optimize. Right. Maximum value reached by the different
methods on the synthetic function. The shaded areas represent the standard error across 5 different
runs, with different initial sets of 6 pairs. For clarity, the shaded areas are omitted for the two worst
performing methods. In each run, all the methods start with the same initial set of 6 points. GP-EI
tends to get stuck in local optima and requires more than 50 steps, on average, to reach the global
maximum."
"EPISTEMIC UNCERTAINTY
GP VARIANCE",0.819047619047619,"H.2
TWO-DIMENSIONAL FUNCTION"
"EPISTEMIC UNCERTAINTY
GP VARIANCE",0.8214285714285714,"To showcase DEUP’s usefulness for Sequential Model Optimization in with a number of dimensions
greater than 1, we consider the optimization of the Levi N.13 function, a known benchmark for
optimization. The function f takes a point (x, y) in 2D space and returns:"
"EPISTEMIC UNCERTAINTY
GP VARIANCE",0.8238095238095238,"f(x, y) = −
 
sin2(3πx) + (x −1)2(1 + sin2(3πy)) + (y −1)2(1 + sin2(2πy))
"
"EPISTEMIC UNCERTAINTY
GP VARIANCE",0.8261904761904761,"We use the box [−10, 10]2 as the optimization domain. In this domain, the maximum of the function
is 0, and it is reached at (1, 1). The function has multiple local maxima, as shown in Figure 9(a)4."
"EPISTEMIC UNCERTAINTY
GP VARIANCE",0.8285714285714286,"Similar to the previous one-dimensional function, MCDropout and Ensemble provided bad perfor-
mances and are omitted from the plot in 9(b). We used the same setting and hyperparameters for
DEUP as for the previous function. DEUP-EI is again the only method that reaches the global
maximum consistently in under 56 function evaluations."
"EPISTEMIC UNCERTAINTY
GP VARIANCE",0.830952380952381,"(a) Visualization of (x, y) 7→−f(x, y)"
"EPISTEMIC UNCERTAINTY
GP VARIANCE",0.8333333333333334,"0
10
20
30
40
50
Number of additional function calls 60 50 40 30 20 10 0"
"EPISTEMIC UNCERTAINTY
GP VARIANCE",0.8357142857142857,Maximum value reached
"EPISTEMIC UNCERTAINTY
GP VARIANCE",0.8380952380952381,"GP-EI
Random acquisition
DEUP-EI
True maximum"
"EPISTEMIC UNCERTAINTY
GP VARIANCE",0.8404761904761905,(b) Comparisons with GP-EI and Random acquisition
"EPISTEMIC UNCERTAINTY
GP VARIANCE",0.8428571428571429,Figure 9: Sequential Model Optimization on the Levi N.13 function
"EPISTEMIC UNCERTAINTY
GP VARIANCE",0.8452380952380952,4Plot of the function copied from https://www.sfu.ca/ ssurjano/levy13.html
"EPISTEMIC UNCERTAINTY
GP VARIANCE",0.8476190476190476,Under review as a conference paper at ICLR 2022
"EPISTEMIC UNCERTAINTY
GP VARIANCE",0.85,"H.3
ADDITIONAL DETAILS FOR THE ACKLEY FUNCTION EXPERIMENT, FOR SYNTHETIC DATA
IN HIGHER DIMENSIONS"
"EPISTEMIC UNCERTAINTY
GP VARIANCE",0.8523809523809524,The Ackley function of dimension d is deﬁned as:
"EPISTEMIC UNCERTAINTY
GP VARIANCE",0.8547619047619047,Ackleyd : B →R
"EPISTEMIC UNCERTAINTY
GP VARIANCE",0.8571428571428571,x 7→A exp  −B
"EPISTEMIC UNCERTAINTY
GP VARIANCE",0.8595238095238096,"v
u
u
t1 d d
X"
"EPISTEMIC UNCERTAINTY
GP VARIANCE",0.861904761904762,"i=1
x2
i "
"EPISTEMIC UNCERTAINTY
GP VARIANCE",0.8642857142857143,+ exp
D,0.8666666666666667,"1
d d
X"
D,0.8690476190476191,"i=1
cos(cxi) !"
D,0.8714285714285714,−A −exp(1)
D,0.8738095238095238,"where B is a hyperrectangle of Rd. (0, . . . , 0) is the only global optimizer of Ackleyd, at which the
function is equal to 0. We use BoTorch’s default values for A, B, c, which are 20, 0.2, 2π respectively."
D,0.8761904761904762,"In our experiments, we used B = [−10, 15]d for all dimensions d."
D,0.8785714285714286,"For the TurBO baseline, we use BoTorch’s default implementation, with Expected Improvement as
an acquisition function, and a batch size of 1 (i.e. acquiring one point per step)."
D,0.8809523809523809,"For fair comparisons, for DEUP, we use a Gaussian Process as the main model, and use its variance as
the only input of the epistemic uncertainty predictor. This means that we calibrate the GP variance to
match the out-of-sample squared error, using another GP to perform the regression. TurBO-DEUP is
a combination of both, in which we perform the variance calibration task for the local GP models of
TurBO. The uncertainty predictor, i.e. the GP regressor, is trained with log targets, as in Appendix H.1,
but also with log variances as inputs."
D,0.8833333333333333,"Note that only the stationarizing feature is used as input for the uncertainty predictor. When we used
the input x as well, we found that the GP error predictor overﬁts on the x part of the input (x, v), and
it was detrimental to the ﬁnal performances. For all experiments, we used 20 initial points."
D,0.8857142857142857,"I
REINFORCEMENT LEARNING EXPERIMENTS"
D,0.888095238095238,"For RL experiments, we used bsuite (Osband et al., 2020), a collection of carefully designed RL
environments. bsuite also comes with a list of metrics which aim to evaluate RL agents from
different aspects. We compare the agents based on the basic metric and average regret as they
capture both sample complexity and ﬁnal performance. The default DQN agent is used as the base
of our experiments with a 3 layer fully-connected (FC) neural network as its Q-network. For the
Bootstrapped DQN baseline, we used the default implementation provided by bsuite. To implement
DQN + MC-Dropout, following the implementation from Gal & Ghahramani (2016), two dropout
layers with dropout probability of 0.1 are used before the second and the third FC layers. In order
take an action, the agent performs a single stochastic forward pass through the Q-network, which is
equivalent to taking a sample from the posterior over the Q-values, as done in Thompson sampling,
an alternative to ϵ−greedy exploration."
D,0.8904761904761904,"As a density estimator, we used a Kernel Density Estimator (KDE) with a Gaussian kernel and
bandwidth of 1 to map states to densities. This KDE is ﬁt after each 10000 steps (actions) with a
batch of samples from the replay buffer (which is of size 10000). The uncertainty estimator network
(E-network) has the same number of layers as the Q-network, with an additional Softplus layer at the
end. All other hyperparameters are the same as the default implementation by Osband et al. (2020).
One complete training run for the DEUP-DQN with 5 seeds experiments takes about 0.04-0.05 GPU
days on a V100 GPU. In total RL experiments took about 0.15 GPU days on a Nvidia V100 GPU."
D,0.8928571428571429,Under review as a conference paper at ICLR 2022
D,0.8952380952380953,"Algorithm 3 DEUP-DQN
Initialize replay buffer D with capacity N
Qθ(s, a): state-action value predictor
Eφ(log d): uncertainty estimator network, which takes the log-density of the states as the input
d(s): Kernel density estimator (KDE)
K: KDE ﬁtting frequency
W: Number of warm-up episodes
for episode=1 to M do"
D,0.8976190476190476,"set s0 as the initial state
for t=1 to max-steps-per-episode do"
D,0.9,"with probability ϵ: take a random action, otherwise:
if episode ≤W: a = maxaQθ(st, a), else: a = maxa

Qθ(st, a) + κ × Eφ(log d(st))(a)
"
D,0.9023809523809524,"store (st, at, rt, st+1) in D
Sample random minibatch B of transitions (sj, aj, rj, sj+1) from D
if sj is a ﬁnal state: yj = rj, else: yj = rj + γmaxaQ(st, a)
Update Q-network:
θ ←θ + αQ.∇θ E(s,a)∼B
h 
yj −Qθ(s, a)
2i"
D,0.9047619047619048,Update E-network:
D,0.9071428571428571,"φ ←φ + αE.∇φ E(s,a)∼B"
D,0.9095238095238095,"""h 
yj −Qθ(s, a)
2 −Eφ(log d(st))(a)
i2
#"
D,0.9119047619047619,"if mod(total-steps, K) = 0: ﬁt the KDE d on the states of D
end
end"
D,0.9142857142857143,"J
DRUG COMBINATION EXPERIMENTS"
D,0.9166666666666666,"To validate DEUP’s uncertainty estimates in a real-world setting, we measured its performance on a
regression task predicting the synergy of drug combinations. While much effort in drug discovery is
spent on ﬁnding novel small molecules, a potentially cheaper method is identifying combinations
of pre-existing drugs which are synergistic (i.e., work well together). Indeed, drug combinations
are the current standard-of-care for a number of diseases including HIV, tuberculosis, and some
cancers Cihlar & Fordyce (2016); Organization & Initiative (2010); Mokhtari et al. (2017)."
D,0.919047619047619,"However, due to the combinatorial nature of drug combinations, identifying pairs exhibiting synergism
is challenging. Compounding this problem is the high monetary cost of running experiments on
promising drug combinations, as well as the length of time the experiments take to complete.
Uncertainty models could be used by practitioners downstream to help accelerate drug combination
treatment discoveries and reduce involved development costs."
D,0.9214285714285714,"To test DEUP’s performance on this task we used the DrugComb and LINCS L1000 datasets Za-
gidullin et al. (2019); Subramanian et al. (2017). DrugComb is a dataset consisting of pairwise
combinations of anti-cancer compounds tested on various cancer cell lines. For each combination,
the dataset provides access to several synergy scores, each indicating whether the two drugs have
a synergistic or antagonistic effect on cancerous cell death. LINCS L1000 contains differential
gene expression proﬁles for various cell lines and drugs. Differential gene expressions measure the
difference in the amount of mRNA related to a set of inﬂuential genes before and after the application
of a drug. Because of this, gene expressions are a powerful indicator of the effect of a single drug at
the cellular level."
D,0.9238095238095239,"In our experiments, each drug is represented by its Morgan ﬁngerprint Morgan (1965)5 (with 1,024
bits and a radius of 3) as well as two differential gene expression proﬁles (each of dimension 978)
from two cell lines (PC-3 and MCF-7). In order to use gene expression features for every drug, we
only used drug pairs in DrugComb where both drugs had differential gene expression data for cell
lines PC-3 and MCF-7."
D,0.9261904761904762,"5The Morgan ﬁngerprint represents a molecule by associating with it a boolean vector specifying its
chemical structure. Morgan ﬁngerprints have been used as a signal of various molecular characteristics to great
success Ballester & Mitchell (2010); Zhang et al. (2006)."
D,0.9285714285714286,Under review as a conference paper at ICLR 2022
D,0.930952380952381,"We ﬁrst compared the quality of DEUP’s uncertainty estimations to other uncertainty estimation
methods on the task of predicting the combination sensitivity score Malyutina et al. (2019) for drug
pairs tested on the cell line PC-3 (1,385 examples). We evaluated the uncertainty methods using a
train, validation, test split of 40%, 30%, and 30%, respectively. The underlying model used by each
uncertainty estimation method consisted of a single drug fully connected neural network (2 layers
with 2048 hidden units and output of dimension 1024) and a combined drug fully connected neural
network (2 layers, with 128 hidden units). The embeddings of an input drug pair’s drugs produced by
the single drug network are summed and passed to the combined drug network, which then predicts
ﬁnal synergy. By summing the embeddings produced by the single drug network, we ensure that the
model is invariant to permutations in order of the two drugs in the pair. The models were trained with
Adam Kingma & Ba (2015), using a learning rate of 1e-4 and weight decay of 1e-5. For MC-Dropout
we used a dropout probability of 0.1 on the two layers of the combined drug network and 3 test-time
forward passes to compute uncertainty estimates. The ensemble used 3 constituent models for its
uncertainty estimates. Both Ensemble and MC-Dropout models were trained with the MSE loss."
D,0.9333333333333333,"We also compared against DUE (van Amersfoort et al., 2021) which combines a neural network
feature extractor with an approximate Gaussian process. Spectral normalization was added to all the
layers of the combined drug network and of the single drug network. Let demb denote the dimension
of the output of the combined drug network, which is also the input dimension of the approximate
Gaussian process. We conducted a grid-search over different values of demb (from 2 to 100), the
number of inducing points (from 3 to 200), the learning rate, and the kernel used by the Gaussian
process. The highest correlation of uncertainty estimates with residuals was attained with demb = 10,
100 inducing points, a learning rate of 1e-2, and the Matern12 kernel."
D,0.9357142857142857,"Algorithm 4 DEUP for Drug Combinations
Data: D dataset of pairwise drug combinations, along with synergy scores ((d1, d2), y)
Initialization:
Split training set into two halves, in-sample Din and out-of-sample Dout
fµ(d1, d2): ˆµ predictor which takes a pair of drugs as input
f in
σ (d1, d2): In-sample ˆσin error predictor
f out
σ
(d1, d2): Out-of-sample ˆσout error predictor
Training:
while training not ﬁnished do"
D,0.9380952380952381,"In-sample update
Get an in-sample batch (d1,in, d2,in, yin) ∼Din
Predict ˆµ = fµ(d1,in, d2,in) and in-sample error ˆσin = f in
σ (d1,in, d2,in)"
D,0.9404761904761905,"Compute NLL: log(ˆσ2
in)
2
+ (ˆµ−yin)2"
D,0.9428571428571428,"2ˆσ2
in
Backpropagate through fµ and f in
σ and update.
Out-of-sample update
Get an out-of-sample batch (d1,out, d2,out, yout) ∼Dout
Estimate ˆµ = fµ(d1,out, d2,out) and out-of-sample error ˆσout = f out
σ
(d1,out, d2,out)"
D,0.9452380952380952,"Compute NLL: log(ˆσ2
out)
2
+ (ˆµ−yout)2"
D,0.9476190476190476,"2ˆσ2
out
Backpropagate through f out
σ
and update.
end"
D,0.95,"The DEUP model we used outputs two heads
 ˆµ
ˆσ

and is trained with the NLL log(ˆσ2)"
D,0.9523809523809523,"2
+ (ˆµ−y)2"
D,0.9547619047619048,"2ˆσ2
in a
similar fashion as in Lakshminarayanan et al. (2017). To obtain a predictor of the out-of-sample error,
we altered our optimization procedure so that the µ and σ heads were not backpropagated through at
all times. Speciﬁcally, we ﬁrst split the training set into two halves, terming the former the in-sample
set Din and the latter the out-of-sample set Dout. We denote as f in
σ the in-sample error predictor and
f out
σ
the out-of-sample error predictor. f out
σ
is used to estimate total uncertainty. Note that in this
setting, f out
σ
predicts the square root of the epistemic uncertainty (ˆσout) rather than the epistemic
uncertainty itself (ˆσ2
out)."
D,0.9571428571428572,"In our experiments, an extra bit is added as input to the model in order to indicate whether a given
batch is from Din or Dout. Through this, the same model is used to estimate f in
σ and f out
σ
with the
model estimating f in
σ when the bit indicates an example is drawn from Din and f out
σ
otherwise. When"
D,0.9595238095238096,Under review as a conference paper at ICLR 2022
D,0.9619047619047619,"the batch is drawn from Din, both heads are trained using NLL using a single forward pass. However,
when the data is drawn from Dout only the ˆσ head is trained. To do this, we must still predict ˆµ in
order to compute the NLL. But the ˆµ predictor fµ must be agnostic to the difference between Din
and Dout. To solve this, we perform two separate forward passes. The ﬁrst pass computes ˆµ and sets
the indicator bit to 0 so fµ has no notion of Dout, while the second pass computes ˆσ, setting the bit to
1 to indicate the true source of the batch. Finally, we backpropagate through the ˆσ head only. The
training procedure is described in Algorithm 4"
D,0.9642857142857143,We report several measures for the quality of uncertainty predictions on a separate test set in Table 7.
D,0.9666666666666667,"Model
Corr. w. res.
U. Bound
Ratio
Log Likelihood
Coverage Probability
CI width
MC-Dropout
0.14 ± 0.07
0.56 ± 0.05
0.25 ± 0.12
−20.1 ± 6.8
11.4 ± 0.2
3.1 ± 0.1
Deep Ensemble
0.30 ± 0.09
0.59 ± 0.04
0.50 ± 0.13
−14.3 ± 4.7
10.8 ± 1.4
3.4 ± 0.6
DUE
0.12 ± 0.12
0.15 ± 0.03
0.80 ± 0.79
−13.0 ± 0.52
15.2 ± 1.0
3.5 ± 0.1
DEUP
0.47 ± 0.03
0.63 ± 0.05
0.75 ± 0.07
−3.5 ± 0.25
36.1 ± 2.5
13.1 ± 0.9"
D,0.969047619047619,"Table 7: Drug combinations: quality of uncertainty estimates from different methods. Corr. w. res.
shows correlation between model residuals and predicted uncertainties ˆσ. A best-case Upper Bound
on Corr. w. res. is obtained from the correlation between ˆσ and true samples from N(0, ˆσ). Ratio is
the ratio between col. 1 and 2 (larger is better). Log-likelihood: average over 3 seeds of per sample
predictive log-likelihood. Coverage Probability: Percentage of test samples which are covered by the
68% conﬁdence interval. CI width: width of the 86% conﬁdence interval."
D,0.9714285714285714,"For each model, we report the per sample predictive log-likelihood, coverage probability and conﬁ-
dence interval width, averaged over 3 seeds."
D,0.9738095238095238,"We also computed the correlation between the residuals of the model |ˆµ(xi) −yi| and the predicted
uncertainties ˆσ(xi). We noted that the different uncertainty estimation methods lead to different
distributions p(ˆσ(x)). For example, predicted uncertainties obtained with DUE always have a similar
magnitude. By contrast, DEUP yields a wide range of different predicted uncertainties."
D,0.9761904761904762,"These differences between the distributions p(ˆσ(x)) obtained with the different methods may have an
impact on the correlation metric, possibly biasing the comparison of the different methods. In order
to account for differences in the distribution p(ˆσ(x)) across methods, we report another metric which
is the ratio between the observed correlation Corr(|ˆµ(x) −y|, ˆσ(x)) and the maximum achievable
correlation given a speciﬁc distribution p(ˆσ(x))."
D,0.9785714285714285,"This maximum achievable correlation (refered to as the upper bound) is not per se a comparison metric,
and is estimated (given a speciﬁc p(ˆσ(x))) as follows: we assume that, for each example (xi, yi), the
predictive distribution of the model N(ˆµ(xi), ˆσ(xi)) corresponds exactly to the distribution of the
target, i.e. yi ∼N(ˆµ(xi), ˆσ(xi)). Under this assumption, the residual of the mean predictor follows a
distribution N(0, ˆσ(xi)). We can then estimate the upper bound by computing the correlation between
the predicted uncertainties ˆσ(xi) and samples from the corresponding Gaussians N(0, ˆσ(xi)). 5
samples were drawn from each Gaussian for our evaluation. This upper bound is reported in the
Table."
D,0.9809523809523809,"Finally, we reported our comparison metric: the ratio between the correlation Corr(|ˆµ(x) −y|, ˆσ(x))
and the upper bound. The higher the ratio is, the closer the observed correlation is to the estimated
upper bound and the better the method is doing."
D,0.9833333333333333,"It is interesting to note that the upper bound is much lower for DUE compared to other methods, as
its predicted uncertainties lie within a short range of values."
D,0.9857142857142858,"Predicted ˆµ and uncertainty estimates can be visualized in Figure 10 for different models. MC-dropout,
Ensemble and DUE consistently underestimate uncertainty, while the out-of-sample uncertainties
predicted by DEUP are much more consistent with the order of magnitude of the residuals. Moreover,
we observed that DUE predicted very similar uncertainties for all samples, resulting in a lower
upper-bound for the correlation between residuals and predicted uncertainties compared to other
methods. We observed a similar pattern when experimenting with the other kernels available in the
DUE package, including the standard Gaussian kernel."
D,0.9880952380952381,"Finally, we note that in the context of drug combination experiments, aleatoric uncertainty could be
estimated by having access to replicates of a given experiment (c.f. Section D), allowing us to subtract
the aleatoric part from the out-of-sample uncertainty, leaving us with the epistemic uncertainty only."
D,0.9904761904761905,Under review as a conference paper at ICLR 2022
D,0.9928571428571429,"(a) MC-dropout
(b) Ensemble
(c) DUE
(d) DEUP"
D,0.9952380952380953,"Figure 10: Predicted mean and uncertainty for different models on a separate test set. 50 examples
from the test set are ordered by increasing value of true synergy score (orange). Model predictions
and uncertainties are visualized in blue. MC-Dropout, Ensemble and DUE consistently underestimate
the uncertainty while DEUP seems to capture the right order of magnitude. Figures made using The
Uncertainty Toolbox (Chung et al., 2020)."
D,0.9976190476190476,"One complete training run for the drug combination experiments takes about 0.01 GPU days on a
V100 GPU. In total these set of experiments took about 0.2 GPU days on a Nvidia V100 GPU."
