Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0014684287812041115,"We describe an algorithm that learns two-layer residual units using rectiﬁed linear
unit (ReLU) activation: suppose the input x is from a distribution with support
space Rd and the ground-truth generative model is a residual unit of this type, given
by y = B∗h
(A∗x)+ + x
i
, where ground-truth network parameters A∗∈Rd×d"
ABSTRACT,0.002936857562408223,"represent a nonnegative full-rank matrix and B∗∈Rm×d is full-rank with m ≥d
and for c ∈Rd, [c+]i = max{0, ci}. We design layer-wise objectives as function-
als whose analytic minimizers express the exact ground-truth network in terms
of its parameters and nonlinearities. Following this objective landscape, learning
residual units from ﬁnite samples can be formulated using convex optimization
of a nonparametric function: for each layer, we ﬁrst formulate the corresponding
empirical risk minimization (ERM) as a positive semi-deﬁnite quadratic program
(QP), then we show the solution space of the QP can be equivalently determined by
a set of linear inequalities, which can then be efﬁciently solved by linear program-
ming (LP). We further prove the statistical strong consistency of our algorithm, and
demonstrate its robustness and sample efﬁciency through experimental results."
INTRODUCTION,0.004405286343612335,"1
INTRODUCTION"
INTRODUCTION,0.005873715124816446,"Neural networks have achieved remarkable success in various ﬁelds such as computer vision (LeCun
et al., 1998; Krizhevsky et al., 2012; He et al., 2016a) and natural language processing (Kim, 2014;
Sutskever et al., 2014). This success is largely due to the strong expressive power of neural networks
(Bengio & Delalleau, 2011), where nonlinear activation units, such as rectiﬁed linear units (ReLU)
(Nair & Hinton, 2010) and hyperbolic tangents (tanh) play a vital role to ensure the large learning
capacity of the networks (Maas et al., 2013). However, the nonlinearity of neural networks makes
them signiﬁcantly more difﬁcult to train than linear models (Livni et al., 2014). Therefore, with the
development of neural network applications, ﬁnding efﬁcient algorithms with provable properties to
train such nontrivial neural networks has become an important and a relatively new goal. A ReLU B"
INTRODUCTION,0.007342143906020558,"Residual networks, or ResNets (He et al., 2016a), are a class of deep neural networks that
adopt skip connections to feed values between nonadjacent layers, where skipped layers
may contain nonlinearities in between. Without loss of expressivity, ResNets avoid the
vanishing gradient problem by directly passing gradient information from previous layers
to current layers where otherwise gradients might vanish without skipping. In practice,
ResNets have shown strong learning efﬁciency in several tasks, e.g. achieving at least
93% test accuracy on CIFAR-10 classiﬁcation, lowering single-crop error to 20.1% on
the 1000-class ImageNet dataset (Russakovsky et al., 2015; He et al., 2016b)."
INTRODUCTION,0.00881057268722467,"Common ResNets are often aggregated by many repeated shallow networks, where each
network acts as a minimal unit with this kind of skip propagation, named a residual unit
(He et al., 2016b). Given the ﬂexibility and simplicity of residual units, much theoretical work
has been devoted to study them and develop training algorithms for them in a way that sidesteps
from the standard backpropagation regime and provides guarantees on the quality of estimation (see
subsection 1.1). In this paper, we propose algorithms that can learn a general class of single-skip two-
layer residual units with ReLU activation as shown on the right by the equation: y = B
h
(Ax)+ + x
i
,"
INTRODUCTION,0.010279001468428781,"where for a scalar c, c+ = max{0, c} (for a vector, this maximization is applied coordinate-wise), x"
INTRODUCTION,0.011747430249632892,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.013215859030837005,"is a random vector as the network input with support space Rd, and A ∈Rd×d and B ∈Rm×d are
weight matrices of layer 1 and layer 2, respectively."
INTRODUCTION,0.014684287812041116,"Compared to previous work (Ge et al., 2018; 2017; Zhang et al., 2018; Wu et al., 2019; Tian, 2017;
Du et al., 2017; Brutzkus & Globerson, 2017; Soltanolkotabi, 2017; Li & Yuan, 2017; Zhong et al.,
2017), the introduction of residual connections simpliﬁes the recovery of the network parameters by
removing the permutation and scaling invariance. However, the naive mean square error minimization
used in estimating the parameters remains nonconvex. Unlike most previous work, we do not assume
a speciﬁc input distribution nor that the distribution is symmetric (Ge et al., 2018; Du & Goel, 2018)."
INTRODUCTION,0.016152716593245228,"We show that under regularity conditions on the weights of the residual unit, the problem of learning
the unit can be formulated through quadratic programming (QP). We use nonparametric estima-
tion (Guntuboyina et al., 2018) to estimate the ReLU function values in the networks. We further
rewrite our constructed quadratic programs to linear programs (LPs). The LP formulation is simpler
to optimize and has the same solution space as the QP for the network parameters."
RELATED WORK,0.01762114537444934,"1.1
RELATED WORK"
RELATED WORK,0.01908957415565345,"Provable learning of neural networks has become an important topic of research. Arora et al. (2014)
recover a multi-layer generative network with sparse connections and Livni et al. (2014) study the
learning of multi-layer neural networks with polynomial activation. Goel et al. (2018) learns a
one-layer convolution network with a perceptron-like rule. They prove the correctness of an iterative
algorithm for exact recovery of the target network. Rabusseau et al. (2019) describe a spectral
algorithm for two-layer linear networks. Recent work has connected optimization and two-layer
network learning (Ergen & Pilanci, 2021; Sahiner et al., 2020; Ergen & Pilanci, 2020; Pilanci &
Ergen, 2020) and has showed how to optimize networks layer by layer (Belilovsky et al., 2019)."
RELATED WORK,0.020558002936857563,"For learning a one-layer ReLU network, Wu et al. (2019) optimize the norm and direction of neural
network weight vectors separately, and Zhang et al. (2018) use gradient descent with a speciﬁc
initialization. For learning a two-layer ReLU network, Ge et al. (2017) redesign the optimization
landscape such that it is more amenable to theoretical analysis and Ge et al. (2018) use a moment-
based method for estimating a neural network Janzamin et al. (2015). Many others have studied ReLU
networks in various settings (Tian, 2017; Du et al., 2017; Brutzkus & Globerson, 2017; Soltanolkotabi,
2017; Li & Yuan, 2017; Zhong et al., 2017; Goel & Klivans, 2017)."
RELATED WORK,0.022026431718061675,"The study of ReLU networks with two hidden layers has also been gaining attention (Goel & Klivans,
2017; Allen-Zhu et al., 2019), with a focus on PAC learning (Valiant, 1984). In relation to our work,
Allen-Zhu & Li (2019) examined the PAC-learnable function of a speciﬁc three-layer neural network
with residual connections. Their work differs from ours in two aspects. First, their learnable functions
include a smaller (in comparison to the student network) three-layer residual network. Second, the
assumptions they make on their three-layer model are rather different than ours."
RELATED WORK,0.023494860499265784,"In relation to nonparametric estimation, Guntuboyina et al. (2018) treat the ﬁnal output of a shape-
restricted regressor as a parameter, placing some restrictions on the type of function that can be
estimated (such as convexity). They provide solutions for estimation with isotonic regression (Brunk,
1955; Ayer et al., 1955; van Eeden, 1956), convex regression (Seijo et al., 2011), shape-restricted
additive models (Meyer, 2013; Chen & Samworth, 2016) and shape-restricted single index mod-
els (Kakade et al., 2011; Kuchibhotla et al., 2017)."
MAIN RESULTS,0.024963289280469897,"1.2
MAIN RESULTS"
MAIN RESULTS,0.02643171806167401,"We design quadratic objective functionals with a linear bounded feasible domain that take network
parameters and functions as variables to estimate the ground-truth network parameters and nonlinear-
ities. The values of the objectives are moments over the input distribution. Thm. 1.1 summarizes the
landscapes of the objectives."
MAIN RESULTS,0.027900146842878122,"Theorem 1.1 (objective landscape, informal). Suppose a ground-truth residual unit has nondegener-
ate weights in both layers and nonnegative weights in layer 1. Then there exist quadratic functionals
deﬁned in linear-constrained domains whose minimizers a) are unique, and are the exact ground-truth,
or b) are not unique, but can be adjusted to the exact ground-truth."
MAIN RESULTS,0.02936857562408223,Under review as a conference paper at ICLR 2022
MAIN RESULTS,0.030837004405286344,"In practice, the exact moments over unobserved variables are not available. We can only construct
the empirical risk minimization (ERM) of the moment-valued objectives by generated samples. With
functions as variables in moment-valued objectives being optimized nonparametrically, the empirical
objectives become quadratic functions with linear constraints (QP). We further show the convexity of
our QP which guarantees its solution in polynomial time w.r.t. sample size and dimension. With the
solution to the QP available, the strong consistency of our network learner is guaranteed:
Theorem 1.2 (strong consistency, informal). Suppose samples are generated by a ground-truth
residual unit that has nondegenerate weights in both layers and nonnegative weights in layer 1. Then
there exists an algorithm that learns a network
a.s.
−−→the exact ground-truth as sample size grows."
MAIN RESULTS,0.032305433186490456,"Roadmap: Assume A and B are student network weights of layer 1 and 2. We ﬁrst give a warm-up
vanilla linear regression approach only knowing A is in Rd×d and B is in Rm×d (section 3). We
then move to the details of our nonparametric learning for layer 2 (section 4), and similarly, layer
1 (section 5). We formalize Thm. 1.1, showing how nonparametric learning allows us to select the
values of A and B from reduced spaces that are seeded by A∗and B∗, under which LR is faster
than vanilla LR. In section 6, we describe the strong consistency of our methods for respective layers,
formalizing Thm. 1.2 using the continuous mapping theorem (Mann & Wald, 1943)."
PRELIMINARIES,0.033773861967694566,"2
PRELIMINARIES"
PRELIMINARIES,0.03524229074889868,"We describe the notations used in this paper, introduce the model and its underlying assumptions, and
state conditions which simplify the problem but which can be removed without loss of learnability."
PRELIMINARIES,0.03671071953010279,"Notation
The ReLU residual units we use take a vector x ∈Rd as input and return a vector
y ∈Rm. We use A∗∈Rd×d and B∗∈Rm×d to denote the ground-truth network parameters for
layer 1 and 2, respectively. We use circumﬂex to denote predicted terms (e.g. an empirical objective
function ˆf, estimated layer 1 weights ˆ
A). We use n to denote the number of i.i.d. samples available
for the training algorithm, {x(i), y(i)} to denote the i-th sample drawn. For an integer k, We deﬁne
[k] to be {1, 2, . . . , k}, and e(j) as the standard basis vector with a 1 at position j. All scalar-based
operators are element wise in the case of vectors or matrices unless speciﬁed otherwise. We use x
and y to refer to the input and output vectors, respectively, as random variables."
PRELIMINARIES,0.0381791483113069,"Linear Regression: Linear regression (LR) in this paper refers to the problem of estimating L
for unbiased model y = Lx. In this case, estimation is done by minimizing the empirical risk
ˆR (L) =
1
2n
P"
PRELIMINARIES,0.039647577092511016,"i∈[n]
Lx(i) −y(i)2 – using linear least squares (LLS). Its solution has a closed
form which we use as given. We refer the reader to Hamilton (1994) for more information."
PRELIMINARIES,0.041116005873715125,"Models, Assumptions and Conditions
Following previous work (Livni et al., 2014), we assume a
given neural network structure speciﬁes a hypothesis class that contains all the networks conforming
to this structure. Learning such a class means using training samples to ﬁnd a set of weights such
that the neural networks predictions generalize well to unseen samples, where both the training
and the unseen samples are drawn from an unobserved ground-truth distribution. In this paper, the
hypothesis class is given by ReLU residual units and we assume it has sufﬁcient expressive power to
ﬁt the ground-truth model. More speciﬁcally, we discuss the realizable case of learning, in which the
ground-truth model is set to be a residual unit taken from the hypothesis class with the form:"
PRELIMINARIES,0.042584434654919234,"y = B∗h
(A∗x)+ + x
i
,
(1)"
PRELIMINARIES,0.04405286343612335,"and is used to draw samples for learning. Unlike other multi-layer ReLU-afﬁne models which do not
apply skip connections, we cannot permute the weight matrices of the residual unit and retain the
same function because of skip-adding x (it breaks symmetry). This helps us circumvent issues of
identiﬁability1, and allows us to precisely estimate the ground-truth weight matrices A∗and B∗."
PRELIMINARIES,0.04552129221732746,"Our general approach for residual unit layer 2 learns a scaled ground-truth weight matrix that also
minimizes the layer 2 objective. The existence of such scaled equivalence of our layer 2 approach
comes from what is deﬁned below."
PRELIMINARIES,0.04698972099853157,"1Here, we are referring to the ability to identify the true model parameters using inﬁnite samples."
PRELIMINARIES,0.048458149779735685,Under review as a conference paper at ICLR 2022
PRELIMINARIES,0.049926578560939794,"Deﬁnition 2.1 (component-wise scale transformation). A matrix A ∈Rd×d is said to be a scale
transformation w.r.t. the j-th component if (Aj,:)⊤= Aj,j · e(j)."
PRELIMINARIES,0.0513950073421439,"Additionally, estimation is more complex when the layer 2 weights B∗is nonsquare. For simplicity
of our algorithm presentation for layer 2, we stick to Cond. 2.1 in the following sections2."
PRELIMINARIES,0.05286343612334802,"Condition 2.1 (layer 2 objective minimizer unique). A∗is not a scale transformation w.r.t. any
components and B∗is a square matrix, i.e. m = d."
PRELIMINARIES,0.05433186490455213,"3
WARM-UP: VANILLA LINEAR REGRESSION"
PRELIMINARIES,0.055800293685756244,"Consider a ground-truth two-layer residual unit. If we assume that the inputs only contain vectors
with negative entries, i.e. x < 0, the effect of the ReLU function in the residual unit then disappears
because of the nonnegativity of A∗. The residual unit turns into linear model y = B∗x. Thus, direct
LR on samples with negative inputs can learn the exact ground-truth layer 2 parameter B∗with at
least d samples, when formulating the LR as a solvable full-rank linear equation system."
PRELIMINARIES,0.05726872246696035,"On the contrary, if the inputs only contain vectors with positive entries, i.e. x > 0, all the neurons in
the hidden layer are then activated by the ReLU and the nonlinearity is eliminated. The residual unit
in this case turns into y = B∗(A∗+ Id) x. Taking the value that left-multiplies x as a single weight
matrix D∗, this is also a linear model. Direct LR on at least d samples with positive inputs by the
residual unit can learn the exact D∗. Since we have the access to the exact B∗, solving for the exact
A∗can be accomplished through solving a full-rank linear equation system B∗· ˜
A = D∗, where the
unique solution ˜
A = A∗+ Id."
PRELIMINARIES,0.05873715124816446,"While simple, this vanilla LR approach requires a large number of redundant samples, since sampled
inputs usually have a small proportion of fully negative/positive vectors. Taking random input
vectors i.i.d. with respect to each entry as an example, the probability of sampling a vector with
all negative entries is pd
−, where p−is the probability of sampling a negative vector entry, then
the expected number of samples to get one negative vector is 1/pd
−. Denoting p+ similarly, 1/pd
+
samples are expected for a positive vector. Besides, each LR step in this approach requires d such
samples respectively to make the linear equation system full-rank, which implies the expected sample
size to be d-exponential d ·
 
1/pd
−+ 1/pd
+

. For other common random vectors like Gaussian
samples, the proportions of fully negative/positive vectors in sampled inputs are also expected to
decrease exponentially as d grows, as high-dimensional random vectors like Gaussian are essentially
concentrated uniformly in a sphere (Johnstone, 2006). Technical and experimental details about
sample size expectations and the vanilla LR algorithm are further discussed in Appendix F."
PRELIMINARIES,0.06020558002936858,"4
NONPARAMETRIC LEARNING: LAYER 2"
PRELIMINARIES,0.06167400881057269,"We present how we learn a residual unit layer 2 under Cond. 2.1 (estimating B∗): We ﬁrst design an
objective functional with the arguments being a matrix and a function. The objective uses expectation
of a loss over the true distribution generating the data and is uniquely minimized by [B∗]−1 and a
rectiﬁer function (ReLU). We then formulate its ERM using nonparametric estimation as a standard
convex QP, further simpliﬁed as an LP that has the same capability as the QP to learn layer 2."
OBJECTIVE DESIGN AND LANDSCAPE,0.0631424375917768,"4.1
OBJECTIVE DESIGN AND LANDSCAPE"
OBJECTIVE DESIGN AND LANDSCAPE,0.06461086637298091,"Consider the formulation of a residual unit as in Eq. 1. It is possible to rewrite the model as equation:
C∗y = (A∗x)+ + x, where the output of the hidden neuron with skip addition is on both sides of
the equation, and C∗B∗= Id. We aim to estimate the inverse of B∗by matrix variable C and the
nonlinearity x 7→(A∗x)+ by a function variable h. The objective is formulated as risk functional by
the L2 error between values respectively computed by C and h"
OBJECTIVE DESIGN AND LANDSCAPE,0.06607929515418502,"G2 (C, h) = 1"
"EX
H",0.06754772393538913,"2Ex
h
∥h (x) + x −Cy∥2i
,
(2)"
"EX
H",0.06901615271659324,"2In Appendix G, we show that estimation of B∗remains solvable without satisfying Cond. 2.1."
"EX
H",0.07048458149779736,Under review as a conference paper at ICLR 2022
"EX
H",0.07195301027900147,"where the estimator C ∈Rd×m, the domain of h is the nonnegative3 continuous4 Rd →Rd
function space, written as C0
≥0 in shorthand. This objective is quadratic because the forward mapping
x 7→h(x) + x and the backward mapping y 7→Cy are both linear w.r.t. C and h, and the two are
linearly combined in a L2 norm. The objective in Eq. 2 is minimized by the ground-truth, i.e. C∗and
x 7→(A∗x)+, is one of its minimizers. However, it is not simple to describe other variable values
that minimize the objective if any exist. We give that detail under Cond. 2.1, the minimizer of G2 is
unique, as the exact ground-truth in the given domain.
Theorem 4.1 (objective minimizer, layer 2). Deﬁne G2(C, h) as Eq. 2, where C ∈Rd×m, h ∈C0
≥0.
Then under Cond. 2.1, G2(C, h) reaches its zero minimum iff C = [B∗]−1 and h : x 7→(A∗x)+."
"EX
H",0.07342143906020558,"Technical details are in Appendix H, where we use Lem. 4.2 (in subsection 4.3) to prove a more
general theorem which does not require Cond. 2.1 and is sufﬁcient for Thm. 4.1. In the next subsection,
we construct the ERM of G2 and present our convex QP formulation."
ERM WITH NONPARAMETRIC ESTIMATION IS CONVEX QP,0.07488986784140969,"4.2
ERM WITH NONPARAMETRIC ESTIMATION IS CONVEX QP"
ERM WITH NONPARAMETRIC ESTIMATION IS CONVEX QP,0.0763582966226138,"Consider the second layer objective (Eq. 2) with nonnegative continuous function space as the domain
of h. We follow Vapnik (1992) and deﬁne its standard empirical risk functional:"
ERM WITH NONPARAMETRIC ESTIMATION IS CONVEX QP,0.07782672540381791,"ˆG2(C, h) = 1"
N,0.07929515418502203,2n X i∈[n]
N,0.08076358296622614,"h(x(i)) + x(i) −Cy(i)
2
.
(3)"
N,0.08223201174743025,"The function variable h ∈C0
≥0 can be optimized either parametrically or nonparametrically. If we
parameterize h, and show that the nonlinearity w.r.t. its parameters would make ˆG2 lose its quadratic
form. Instead, we estimate h nonparametrically: for each sample input x(i), we introduce variables
ξ(i) that estimates mapped values by h. This avoids introducing nonlinearity to the objective and
keeps ˆG2 quadratic. On the other hand, the domain of h, i.e. nonnegative continuous function space,
turns into a set of linear inequalities as constraints when optimizing nonparametrically. In this sense,
learning the second layer of the residual unit can be formulated as the following QP:"
N,0.08370044052863436,"min
C, Ξ
ˆGNPE
2
(C, Ξ) := 1"
N,0.08516886930983847,2n X i∈[n]
N,0.08663729809104258,"ξ(i) + x(i) −Cy(i)
2
, s.t. ξ(i) ≥0, ∀i ∈[n].
(4)"
N,0.0881057268722467,"where Ξ = {ξ(i)}n
i=1 is the nonparametric estimator of x 7→(A∗x)+."
N,0.08957415565345081,"Nonparametric Estimation Validation: A solution to the ERM with nonparametric estimation, i.e.
the QP, is guaranteed to be sufﬁcient for minimizing the standard empirical risk functional (Eq. 3).
More speciﬁcally, assuming C and Ξ = {ξ(i)}n
i=1 are a solution to layer 2 QP (Eq. 4), it is clear that
C and h ∈C0
≥0 such that h(x(i)) = ξ(i) minimize the empirical risk functional Eq. 3. Conversely, a
minimizer of the standard empirical risk Eq. 3, C and h, corresponds to a solution to layer 2 QP as
we set ξ(i) = h(x(i)). Therefore, minimizing ˆG2 and solving layer 2 QP are empirically equivalent."
N,0.09104258443465492,"Convexity: The convexity of the QP: Eq. 4 is also guaranteed. First, constraints are linear. Second,
for each sample with index i ∈[n], the L2 norm wraps linearity w.r.t. C and ξ(i). Such formulation
ensures the quadratic coefﬁcient matrix is positive semideﬁnite. Thus, with the sum of convex
functions still being convex, the QP objective (Eq. 4) becomes convex. Even without the knowledge
of how samples are generated, this QP would be a convex program. Strict proofs are in Appendix I."
LP SIMPLIFICATION,0.09251101321585903,"4.3
LP SIMPLIFICATION"
LP SIMPLIFICATION,0.09397944199706314,"Consider single-sample error written as g2 (C, h; x, y) =
1
2 ∥h (x) + x −Cy∥2. If there is a
feasible C such that Cy −x ≥0 holds for all x ∈Rd, then C and h : x 7→Cy −x always
minimize g2 as zero, and thereby minimize the layer 2 objective (Eq. 2). Thus, we obtain a condition
that is equivalent to G2 reaching the minimum in Thm. 4.1 and avoids randomness (see Lem. 4.2)."
LP SIMPLIFICATION,0.09544787077826726,"3Setting h as nonnegative ensures that a) only ReLU nonlinearity minimizes G2 (see Thm. 4.1), and b) h’s
nonparametric estimator is linearly constrained (explained in subsection 4.2).
4If h is not a continuous function, only a null set of discontinuities is possible to make G2 reach zero as its
minimum. Setting h as continuous simpliﬁes our theoretical results that still strictly support empirical discussion."
LP SIMPLIFICATION,0.09691629955947137,Under review as a conference paper at ICLR 2022
LP SIMPLIFICATION,0.09838472834067548,"Algorithm 1 Learn a ReLU residual unit, layer 2."
LP SIMPLIFICATION,0.09985315712187959,"1: Input: {(x(i), y(i))}n
i=1, samples drawn by
Eq. 1.
2: Output: ˆ
B, ˆΞ, a layer 2 and x 7→(A∗x)+"
LP SIMPLIFICATION,0.1013215859030837,"estimate.
3: Go to line 4 if QP, line 5 if LP.
4: Solve QP: Eq. 4 and obtain a ˆGNPE
2
minimizer,
denoted by ˆC, ˆΞ. Go to line 6.
5: Solve LP: Eq. 5 and obtain a minimizer ˆC,
then assign ˆξ(i) ←ˆCy(i) −x(i).
6: return ˆC−1, ˆΞ."
LP SIMPLIFICATION,0.1027900146842878,"Algorithm 2 Learn a ReLU residual unit, layer 1."
LP SIMPLIFICATION,0.10425844346549193,"1: Input: {(x(i), h(i))}n
i=1, layer 1 samples.
2: Output: ˆ
A, a layer 1 estimate.
3: Solve QP: Eq. 7 or LP: Eq. 8 and obtain a
ˆGNPE
1
minimizer, denoted by ˆ
A, ˆΦ. { ˆΦ is no
longer needed.}
4: for all j ∈[d] do"
LP SIMPLIFICATION,0.10572687224669604,"5:
ˆkj ←LR{h(i)
j , ˆ
Aj,:x(i)}h(i)
j
>0."
LP SIMPLIFICATION,0.10719530102790015,"6:
ˆ
Aj,: ←ˆ
Aj,:/ˆkj. {Rescale ˆ
A.}
7: end for
8: return
ˆ
A."
LP SIMPLIFICATION,0.10866372980910426,"Lemma 4.2. G2(C, h) reaches its zero minimum iff Cy −x ≥0 holds for any x ∈Rd and its
corresponding residual unit output y, and h : x 7→Cy −x."
LP SIMPLIFICATION,0.11013215859030837,"The pointwise satisfaction of the inequality in Lem. 4.2 describes the solution space for the mini-
mization of G2. The sufﬁciency of satisfying this inequality to minimize G2 is directly obtained
by assigning h : x 7→Cy −x where C complies with Cy −x ≥0 for all x in the support space
Rd. The necessity of satisfying this inequality comes from its contraposition: If a C violates the
inequality, there must be a non-null set of x that yield the violation due to the continuity of Cy −x.
In this sense, the resulting G2 value becomes nonzero."
LP SIMPLIFICATION,0.11160058737151249,"Empirically speaking, we cannot solve an inequality that holds w.r.t. to every point in the support
space if we only observe ﬁnite samples. We can only estimate C by solving the inequality that holds
w.r.t. each sample. Following this, we formulate such estimation as to ﬁnd a feasible point in the space
deﬁned by a set of linear inequalities, each of which corresponds to a sample: Cy(i) −x(i) ≥0. Each
point in the feasibility deﬁned by the inequalities has a one-to-one correspondence in the solution
space to layer 2 QP (Eq. 4): C ↔(C, {Cy(i) −x(i)}i∈[n]). The set of inequalities can be solved by
a standard LP with a constant objective and with constraints that are inequalities, i.e."
LP SIMPLIFICATION,0.1130690161527166,"min
C
const, s.t. Cy(i) −x(i) ≥0, ∀i ∈[n].
(5)"
LP SIMPLIFICATION,0.1145374449339207,"With the one-to-one correspondences, LP: Eq. 5 and QP: Eq. 4 have equivalent solution spaces.
Moreover, LP: Eq. 5 is also a convex program. Alg. 1 summarizes the layer 2 estimator: Simply solve
the QP/LP5 and return the inverse of ˆC as the estimate of layer 2 weights and ˆΞ as x 7→(A∗x)+"
LP SIMPLIFICATION,0.11600587371512482,"estimate. Regardless of time complexity, QP and LP in Alg. 1 work equivalently since their solution
spaces are equivalent to each other."
LP SIMPLIFICATION,0.11747430249632893,"Our nonparametric learning directly ﬁnds a unique layer 2 estimate under Cond. 2.1. In the general
case without Cond. 2.1 (discussed in Appendix G), nonparametric learning essentially reduces the
possible values of B from Rm×d to a B∗scale-equivalent matrix space, where LR uses sampled data
much more efﬁciently than vanilla LR on layer 2 (section 3)."
LP SIMPLIFICATION,0.11894273127753303,"5
NONPARAMETRIC LEARNING: LAYER 1"
LP SIMPLIFICATION,0.12041116005873716,"With layer 2 learned, outputs by hidden neurons become observable. The two-layer problem is
thereby reduced to single-layer. Consider a ground-truth single-layer model: h = (A∗x)+. To
construct a learning objective for this model, we rewrite the model as a nonlinearity plus a linear
mapping by A∗: h = (−A∗x)+ + A∗x, where on both sides of the equation is the output of layer 1,
and the nonlinearity is x 7→(−A∗x)+. The objective of layer 1 is formulated as:"
LP SIMPLIFICATION,0.12187958883994127,"G1 (A, r) = 1"
"EX
H",0.12334801762114538,"2Ex
h
∥r(x) + Ax −h∥2i
,
(6)"
"EX
H",0.12481644640234948,"5We use CVX (Grant & Boyd, 2014; 2008) that calls SDPT3 (Toh et al., 1999) (a free solver under GPLv3
license) and solves our convex QP/LP in polynomial time. See Appendix C for technical details."
"EX
H",0.1262848751835536,Under review as a conference paper at ICLR 2022
"EX
H",0.1277533039647577,"where A ∈Rd×d is of the layer 1 weights estimator, the domain of r is also C0
≥0. The minimizer A
of the risk G1 falls into a matrix space such that for any matrix A in the space, each row of A is a
scaled-down version of the same row of A∗without changing the direction (Thm. 5.1).
Theorem 5.1 (objective minimizer space, layer 1). Deﬁne G1(A, r) as Eq. 6, where A ∈Rd×d,
r ∈C0
≥0. Then G1(A, r) reaches its zero minimum iff for each j ∈[d], Aj,: = kjA∗
j,: where
0 ≤kj ≤1, and r : x 7→(A∗x)+ −diag(k) · A∗x."
"EX
H",0.12922173274596183,"The scale equivalence in the solution space is derived from a ReLU inequality: (x)+ ≥kx where
0 ≤k ≤1. Let the j-th row of A be a scaled-down version of A∗, i.e. Aj,: = kjA∗
j,: where"
"EX
H",0.13069016152716592,"0 ≤kj ≤1. According to the inequality, we have
 
A∗
j,:x
+ ≥kjA∗
j,:x, which indicates (A∗x)+ ≥
diag(k) · A∗x. Thus, r minimizing G1 in Thm. 5.1 lies in feasibility C0
≥0 when A = diag(k) · A∗."
"EX
H",0.13215859030837004,"Due to the existence of scale equivalence, we must compute the scale factor to obtain the ground-truth
weights A∗. The scale factor kj is sufﬁciently obtainable with (Aj,:x)+ and
 
A∗
j,:x
+ observable:
Conditioned on nonnegative ReLU input, we have a linear model Aj,:x = kjA∗
j,:x where Aj,:x
and A∗
j,:x are observed. Thm. 5.2 summarizes layer 1 scale factor property, allowing us to correct a
minimizer of G1 to the ground-truth weights A∗by computing a scalar for each row.
Theorem 5.2 (scale factor, layer 1). Assume A is a minimizer of G1. Then for any j ∈[d],
(Aj,:x)+ /
 
A∗
j,:x
+ is always equal to the scale factor kj given that (Aj,:x)+ > 0."
"EX
H",0.13362701908957417,"The elimination of randomness for G1 follows the same pattern as layer 2. If there is a feasible A
such that (A∗x)+ −Ax ≥0 holds for all x ∈Rd, such A is a solution to our objective by Eq. 6.
We can also have the following proposition that avoids randomness.
Lemma 5.3. G1(A, r) reaches its zero minimum iff h −Ax ≥0 holds for any x ∈Rd and its
corresponding hidden output h, and r : x 7→h −Ax."
"EX
H",0.13509544787077826,"We now turn into empirical discussion. Similar to layer 2, we formulate layer 1 QP by its empirical
objective with nonparametric estimation and linear constraints representing the nonnegativity of r:"
"EX
H",0.13656387665198239,"min
A, Φ
ˆGNPE
1
(A, Φ) := 1"
N,0.13803230543318648,2n X i∈[n]
N,0.1395007342143906,"φ(i) + Ax(i) −h(i)
2
, s.t. φ(i) ≥0, ∀i ∈[n].
(7)"
N,0.14096916299559473,"where Φ = {φ(i)}n
i=1 is the function estimator of x 7→(A∗x)+ −diag(k) · A∗x where k refers
to the scaling equivalence. Similarly, the solution space of QP: Eq. 7 can be represented by a set of
linear inequalities as constraints of the following efﬁciently solvable LP"
N,0.14243759177679882,"min
A
const, s.t. h(i) −Ax(i) ≥0, ∀i ∈[n].
(8)"
N,0.14390602055800295,"Alg. 2 describes how a layer 1 is learned: First, a G1 minimizer estimate ˆ
A is obtained by solving
the QP/LP. Then for the j-th row, the scale factor kj is estimated by running LR on h(i)
j
and ˆ
Aj,:x(i)"
N,0.14537444933920704,"s.t. h(i)
j
> 0 to correct ˆ
A, as Aj,:x = kjhj is an unbiased linear model given that hj > 0. By
nonparametric learning, the value space of A is reduced from Rd×d to {diag(k) · A∗| 0 ≤kj ≤1},
where LR uses sampled data much more efﬁciently than vanilla LR layer 1 (section 3)."
FULL ALGORITHM AND ANALYSIS,0.14684287812041116,"6
FULL ALGORITHM AND ANALYSIS"
FULL ALGORITHM AND ANALYSIS,0.14831130690161526,"The full algorithm concatenates Alg. 1 and 2 in a layerwise fashion, with observations of input/output
by the ground-truth network: a) Estimates layer 2 and nonlinearity: x 7→(A∗x)+ by Alg. 1.
b) Estimates layer 1 by running Alg. 2 on input samples and the nonlinearity estimate. It has provable
guarantees. For empirical analysis, we use ˆCn to denote the estimation of C∗from n random samples.
Similar notations are applied to other estimations. First of all, our methods to solve respective layers,
Alg. 1 and 2, are strongly consistent if any convex QPs/LPs involved can be solved exactly."
FULL ALGORITHM AND ANALYSIS,0.14977973568281938,"Lemma 6.1 (layer 2 strong consistency). Under Cond. 2.1, ˆCn
a.s.
−−→C∗and ˆBn
a.s.
−−→B∗, n →∞."
FULL ALGORITHM AND ANALYSIS,0.1512481644640235,"For ˆCn: In Appendix J, we prove its more general a.s. convergence without satisfying Cond. 2.1,
where the solution space to layer 2 objective is a non-compact continuous set where all the elements"
FULL ALGORITHM AND ANALYSIS,0.1527165932452276,Under review as a conference paper at ICLR 2022
FULL ALGORITHM AND ANALYSIS,0.15418502202643172,"are scaling equivalences. We use Hausdorff distance (Rockafellar & Wets, 2009) as metric and prove
that the empirical solution space a.s. converges to the theoretical solution space. Then the more
general a.s. convergence holds with the strong consistency of layer 2 scale factor estimator where we
use LR to estimate the scale factors."
FULL ALGORITHM AND ANALYSIS,0.15565345080763582,"For ˆBn: Using the continuous mapping theorem (Mann & Wald, 1943), we directly propagate the
strong consistency of C∗estimator to its inverse B∗’s estimator."
FULL ALGORITHM AND ANALYSIS,0.15712187958883994,"According to full algorithm description, layer 1 estimation uses ˆCny(i) −x(i) as the outputs where
i ∈[n]. Thus, the strong consistency of the hidden neuron estimator is also guaranteed by the
continuous mapping theorem. Following the proof sketch of the ˆCn a.s. convergence, we obtain the
strong consistency of layer 1 estimator (See Lem. 6.2)."
FULL ALGORITHM AND ANALYSIS,0.15859030837004406,"Lemma 6.2 (layer 1 strong consistency). ˆAn
a.s.
−−→A∗, n →∞."
FULL ALGORITHM AND ANALYSIS,0.16005873715124816,"By the continuous mapping theorem, the strong consistency of the full algorithm (Thm. 6.3, formal
version of Thm. 1.2), which is commonly deﬁned by a loss function that is continuous on network
weights, is implied by the strong consistency of network weights estimators (Lem. 6.1 and 6.2). See
Appendix J for proofs of strong consistency discussions in this section."
FULL ALGORITHM AND ANALYSIS,0.16152716593245228,"Theorem 6.3 (strong consistency, formal). Deﬁne L as L2 output loss. L( ˆAn, ˆBn)
a.s.
−−→0, n →∞."
EXPERIMENTS,0.16299559471365638,"7
EXPERIMENTS"
EXPERIMENTS,0.1644640234948605,"We provide experimental analysis to demonstrate the effectiveness and robustness of our approach
in comparison to stochastic gradient descent (SGD) on L2 output loss: L(A, B) = 1"
EXPERIMENTS,0.16593245227606462,"2Ex ∥ˆy −y∥2 ,"
EXPERIMENTS,0.16740088105726872,"where we parameterize the output prediction by ˆy = B
h
(Ax)+ + x
i
. Our proposed methods
outperform SGD in terms of sample efﬁciency and robustness to different network weights and noise
strengths, which indicates a poor optimization landscape of L2 output loss for ReLU residual units."
EXPERIMENTS,0.16886930983847284,"Setup: The ground-truth weights are generated through i.i.d. folded standard Gaussian6 and standard
Gaussian for layer 1 and 2 respectively, i.e. A∗i.i.d.
∼|N| (0, 1), B∗i.i.d.
∼N(0, 1). The input distribution
is set to be an i.i.d. zero mean Gaussian-uniform equal mixture N (−0.1, 1) – U (−0.9, 1.1). SGD
is conducted on mini-batch empirical losses of L(A, B) with batch size 32 for 256 epochs in
each learning trial. We apply time-based learning rate decay η = η0/ (1 + γ · T) with initial rate
η0 = 10−3 and decay rate γ = 10−5, where T is the epoch number. The above hyperparameters are
tuned to outperform other hyperparameters in learning ReLU residual units in terms of output errors."
EXPERIMENTS,0.17033773861967694,"Evaluation: We use relative errors to measure the accuracy of our vector/matrix estimates: For a
network with weights A and B and its teacher network with weights A∗and B∗, a) layer 1 error
refers to ∥A −A∗∥/ ∥A∗∥, similar to layer 2. b) output error refers to ˆE [∥ˆy −y∥/ ∥y∥] by test
data. Due to the equivalence between the solution spaces of our QP and LP without label noise, we
choose LP in noiseless experiments, referred to as “ours”. In addition, to reduce variance, the results
of learning the same ground-truths are computed as means across 16 trials."
EXPERIMENTS,0.17180616740088106,"Sample Efﬁciency: Consider Fig. 1. It depicts the variation in the prediction errors (warmer color,
larger error) as a function of d (input dimension) and the number of samples the learning algorithm is
using. We compare SGD against our algorithm. We observe that our approach to the estimating the
neural network is more sample efﬁcient. For SGD, the estimation is relatively easy with only up to
10 dimensions. As expected, once the dimension grows, the sample size required for the same level
of error as our method is larger. Still, overall, our method is capable of learning robustly with small
sample sizes and more efﬁciently than SGD even for larger sample sizes."
EXPERIMENTS,0.17327459618208516,"Network Weight Robustness: This experiment aims to verify whether our method can learn a
broader class of residual units. In Tab. 1, our method shows a light-tailed distribution with nearly
zero means and standard deviations for layers 1 and 2, and output errors across various ground-truth
networks, whereas SGD is less robust in the same context. Our method shows strong robustness to
network weight changes, indicating its applicability across the whole hypothesis class."
EXPERIMENTS,0.17474302496328928,"6A folded Gaussian is the absolute value of a Gaussian, with p.d.f. p(|x|) where x ∼N , denoted as |N|. We
use folded Gaussian to ensure layer 1 weights are nonnegative."
EXPERIMENTS,0.1762114537444934,Under review as a conference paper at ICLR 2022 SGD
EXPERIMENTS,0.1776798825256975,160 192 224 256 288 320 352 384 8 10 12 14 16 18 20 22 Ours
EXPERIMENTS,0.17914831130690162,160 192 224 256 288 320 352 384 8 10 12 14 16 18 20 22 0.1 0.2 0.3 0.4 0.5 0.6
EXPERIMENTS,0.18061674008810572,"Figure 1:
Output errors by SGD and our
method for different dimensions and sample
sizes. For the same d, we ﬁx the ground-truth
network as sample size grows."
EXPERIMENTS,0.18208516886930984,"Table 1: Means / Standard deviations of net-
work estimate errors for different network
weights. Values are computed from the process
of learning 128 different ground-truth networks
with d = 16. 512 training samples are drawn
for each learning trial."
EXPERIMENTS,0.18355359765051396,"Layer 1
Layer 2
Output"
EXPERIMENTS,0.18502202643171806,"Mean
Std
Mean
Std
Mean
Std"
EXPERIMENTS,0.18649045521292218,"SGD
0.715
0.090
1.203
0.134
0.431
0.038
Ours
0.039
0.008
≈0
≈0
0.055
0.008"
EXPERIMENTS,0.18795888399412627,"0
0.2
0.4
0.6
0.8
1
0 0.1 0.2 0.3 0.4 0.5"
EXPERIMENTS,0.1894273127753304,"SGD
LP
QP"
EXPERIMENTS,0.19089574155653452,"0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1"
EXPERIMENTS,0.19236417033773862,"SGD
LP
QP"
EXPERIMENTS,0.19383259911894274,"0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1 1.2 1.4"
EXPERIMENTS,0.19530102790014683,"SGD
LP
QP"
EXPERIMENTS,0.19676945668135096,"Figure 2: Respective errors of layers 1 and 2 and outputs of different label noise strengths for
SGD, LP and QP. We ﬁx the ground-truth weights with d = 10 and only the noise strength varies.
512 training samples are drawn for each learning trial."
EXPERIMENTS,0.19823788546255505,"Noise Robustness: Fig. 2 conﬁrms the robustness of our methods when output noise exists. Samples
are generated by a ground-truth residual unit with output noise being i.i.d. zero-mean Gaussian in
different strengths (i.e. standard deviations). We try both QP and LP because in noisy setting the two
approaches are not equivalent w.r.t. the solution space7. First, SGD always gives larger errors than
our methods, even though it is hardly affected by tuning the noise strength. For QP/LP, all the errors
for layer 1, 2 and output grow almost linearly as noise strength increases, indicating that both QP/LP
learn the optima robustly when output noise is present, where QP slightly outperforms LP."
CONCLUSION,0.19970631424375918,"8
CONCLUSION"
CONCLUSION,0.2011747430249633,"In this paper, we address the problem of learning a general class of two-layer residual units and
propose an algorithm based on landscape design and convex optimization: We demonstrate ﬁrstly
that minimizers of our objective functionals can express the exact ground-truth network. Then, we
show that the corresponding ERM with nonparametric function estimation can be solved using convex
QP/LP, which indicates polynomial-time solvability w.r.t. sample size and dimension. Moreover, our
algorithms that are used to estimate both layers as well as the whole networks are strongly consistent,
with very weak conditions on input distributions."
CONCLUSION,0.2026431718061674,"Our work opens the door to a variety of open problems to explore. We provide a strong consis-
tency result without assuming a particular input distribution. It would be interesting to explore the
sample complexity of our algorithm when stronger assumptions are placed on input distribution.
Another extension could be to dispose of the limitations on ground-truth weights, e.g. m ≥d,
non-degeneration, nonnegativity of A∗. In addition, given that our algorithm solves ReLU by using
nonparametric estimation through convex optimization, this might provide an inspiration to solving
learning problems with other nonlinearities using nonparametric methods that improve or change the
optimization landscape."
CONCLUSION,0.20411160058737152,7See Appendix B for noisy model discussion.
CONCLUSION,0.2055800293685756,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.20704845814977973,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.20851688693098386,"The full set of assumptions of all theoretical results in this paper are stated - section 2, Appendix F
and G. The complete proofs of all theoretical results are all included - Appendix F, G, H, I and J.
Code in MATLAB is zipped as supplementary ﬁle. The numerical simulation details, including data
sizes and splits, hyperparameters and how they were chosen, clear deﬁnitions of the speciﬁc measures
or statistics used to report results, are discussed in detail - the Setup and Evaluation paragraphs
in section 7. Results with central tendency (e.g. mean) and variation (e.g. stddev) are reported -
Tab. 1. The use of external computing infrastructure, CVX, is clariﬁed, and the consent obtaining and
licensing of CVX and SDPT3 solver used in this work exactly follows the original author instructions:
http://cvxr.com/cvx/doc/citing.html - footnote 5."
REFERENCES,0.20998531571218795,REFERENCES
REFERENCES,0.21145374449339208,"Zeyuan Allen-Zhu and Yuanzhi Li. What can ResNet learn efﬁciently, going beyond kernels? In
Advances in Neural Information Processing Systems, pp. 9015–9025, 2019."
REFERENCES,0.21292217327459617,"Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized
neural networks, going beyond two layers. In Advances in neural information processing systems,
pp. 6155–6166, 2019."
REFERENCES,0.2143906020558003,"Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma. Provable bounds for learning some deep
representations. In International Conference on Machine Learning, pp. 584–592, 2014."
REFERENCES,0.21585903083700442,"Miriam Ayer, H Daniel Brunk, George M Ewing, William T Reid, and Edward Silverman. An empir-
ical distribution function for sampling with incomplete information. The annals of mathematical
statistics, pp. 641–647, 1955."
REFERENCES,0.2173274596182085,"Eugene Belilovsky, Michael Eickenberg, and Edouard Oyallon. Greedy layerwise learning can scale
to imagenet. In International conference on machine learning, pp. 583–593. PMLR, 2019."
REFERENCES,0.21879588839941264,"Yoshua Bengio and Olivier Delalleau. On the expressive power of deep architectures. In International
Conference on Algorithmic Learning Theory, pp. 18–36. Springer, 2011."
REFERENCES,0.22026431718061673,"M Émile Borel. Les probabilités dénombrables et leurs applications arithmétiques. Rendiconti del
Circolo Matematico di Palermo (1884-1940), 27(1):247–271, 1909."
REFERENCES,0.22173274596182085,"Hugh D Brunk. Maximum likelihood estimates of monotone parameters. The Annals of Mathematical
Statistics, pp. 607–616, 1955."
REFERENCES,0.22320117474302498,"Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a ConvNet with Gaussian
inputs. ArXiv, abs/1702.07966, 2017."
REFERENCES,0.22466960352422907,"Yining Chen and Richard J Samworth. Generalized additive and index models with shape constraints.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 78(4):729–754, 2016."
REFERENCES,0.2261380323054332,"Simon S. Du and Surbhi Goel. Improved learning of one-hidden-layer convolutional neural networks
with overlaps. ArXiv, abs/1805.07798, 2018."
REFERENCES,0.2276064610866373,"Simon S. Du, Jason D. Lee, Yuandong Tian, Barnabás Póczos, and Amrendra Kumar Singh. Gra-
dient descent learns one-hidden-layer cnn: Don’t be afraid of spurious local minima. ArXiv,
abs/1712.00779, 2017."
REFERENCES,0.2290748898678414,"Tolga Ergen and Mert Pilanci. Revealing the structure of deep neural networks via convex duality.
arXiv preprint arXiv:2002.09773, 2020."
REFERENCES,0.2305433186490455,"Tolga Ergen and Mert Pilanci. Implicit convex regularizers of cnn architectures: Convex optimization
of two-and three-layer networks in polynomial time. In International Conference on Learning
Representations (ICLR), 2021."
REFERENCES,0.23201174743024963,"J Farkas. Ober die theorie der einfachen ungleichungen. J. Reine Angew. Math, 124:1–24, 1902."
REFERENCES,0.23348017621145375,"Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape
design. arXiv preprint arXiv:1711.00501, 2017."
REFERENCES,0.23494860499265785,Under review as a conference paper at ICLR 2022
REFERENCES,0.23641703377386197,"Rong Ge, Rohith Kuditipudi, Zhize Li, and Xiang Wang. Learning two-layer neural networks with
symmetric inputs. arXiv preprint arXiv:1810.06793, 2018."
REFERENCES,0.23788546255506607,"Surbhi Goel and Adam R. Klivans. Learning neural networks with two nonlinear layers in polynomial
time. In COLT, 2017."
REFERENCES,0.2393538913362702,"Surbhi Goel, Adam R. Klivans, and Raghu Meka. Learning one convolutional layer with overlapping
patches. ArXiv, abs/1802.02547, 2018."
REFERENCES,0.24082232011747431,"Michael Grant and Stephen Boyd. Graph implementations for nonsmooth convex programs. In
V. Blondel, S. Boyd, and H. Kimura (eds.), Recent Advances in Learning and Control, Lecture
Notes in Control and Information Sciences, pp. 95–110. Springer-Verlag Limited, 2008. http:
//stanford.edu/~boyd/graph_dcp.html."
REFERENCES,0.2422907488986784,"Michael Grant and Stephen Boyd. CVX: Matlab software for disciplined convex programming,
version 2.1. http://cvxr.com/cvx, March 2014."
REFERENCES,0.24375917767988253,"Adityanand Guntuboyina, Bodhisattva Sen, et al. Nonparametric shape-restricted regression. Statisti-
cal Science, 33(4):568–594, 2018."
REFERENCES,0.24522760646108663,"James Douglas Hamilton. Time series analysis, volume 2. Princeton university press Princeton, NJ,
1994."
REFERENCES,0.24669603524229075,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770–778, 2016a."
REFERENCES,0.24816446402349487,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European conference on computer vision, pp. 630–645. Springer, 2016b."
REFERENCES,0.24963289280469897,"Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of non-convexity:
Guaranteed training of neural networks using tensor methods. arXiv preprint arXiv:1506.08473,
2015."
REFERENCES,0.2511013215859031,"Florian Jarre. On the convergence of the method of analytic centers when applied to convex quadratic
programs. Mathematical Programming, 49(1-3):341–358, 1990."
REFERENCES,0.2525697503671072,"Robert I Jennrich. Asymptotic properties of non-linear least squares estimators. The Annals of
Mathematical Statistics, 40(2):633–643, 1969."
REFERENCES,0.2540381791483113,"Iain M Johnstone. High dimensional statistical inference and random matrices. arXiv preprint
math/0611589, 2006."
REFERENCES,0.2555066079295154,"Sham M. Kakade, Adam Tauman Kalai, Varun Kanade, and Ohad Shamir. Efﬁcient learning of
generalized linear and single index models with isotonic regression. In NIPS, 2011."
REFERENCES,0.25697503671071953,"Narendra Karmarkar. A new polynomial-time algorithm for linear programming. In Proceedings of
the sixteenth annual ACM symposium on Theory of computing, pp. 302–311, 1984."
REFERENCES,0.25844346549192365,"Yoon Kim. Convolutional neural networks for sentence classiﬁcation. arXiv preprint arXiv:1408.5882,
2014."
REFERENCES,0.2599118942731278,"Mikhail K Kozlov, Sergei P Tarasov, and Leonid G Khachiyan. The polynomial solvability of convex
quadratic programming. USSR Computational Mathematics and Mathematical Physics, 20(5):
223–228, 1980."
REFERENCES,0.26138032305433184,"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolu-
tional neural networks. In Advances in neural information processing systems, pp. 1097–1105,
2012."
REFERENCES,0.26284875183553597,"Arun K Kuchibhotla, Rohit K Patra, and Bodhisattva Sen. Efﬁcient estimation in convex single index
models. Preprint. Available at, 2017."
REFERENCES,0.2643171806167401,"Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998."
REFERENCES,0.2657856093979442,Under review as a conference paper at ICLR 2022
REFERENCES,0.26725403817914833,"Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with ReLU activation.
ArXiv, abs/1705.09886, 2017."
REFERENCES,0.2687224669603524,"Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efﬁciency of training neural
networks. In Advances in neural information processing systems, pp. 855–863, 2014."
REFERENCES,0.2701908957415565,"David G Luenberger. Optimization by vector space methods. John Wiley & Sons, 1997."
REFERENCES,0.27165932452276065,"Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectiﬁer nonlinearities improve neural
network acoustic models. In ICML Workshop on Deep Learning for Audio, Speech and Language
Processing, 2013."
REFERENCES,0.27312775330396477,"Henry B Mann and Abraham Wald. On stochastic limit and order relationships. The Annals of
Mathematical Statistics, 14(3):217–226, 1943."
REFERENCES,0.2745961820851689,"Mary C. Meyer. A simple new algorithm for quadratic programming with applications in statistics.
Communications in Statistics - Simulation and Computation, 42:1126–1139, 2013."
REFERENCES,0.27606461086637296,"Renato DC Monteiro and Ilan Adler. Interior path following primal-dual algorithms. part ii: Convex
quadratic programming. Mathematical Programming, 44(1-3):43–66, 1989."
REFERENCES,0.2775330396475771,"Katta G Murty.
Computational complexity of parametric linear programming.
Mathematical
programming, 19(1):213–219, 1980."
REFERENCES,0.2790014684287812,"Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In
Proceedings of the 27th international conference on machine learning (ICML-10), pp. 807–814,
2010."
REFERENCES,0.28046989720998533,"Mert Pilanci and Tolga Ergen. Neural networks are convex regularizers: Exact polynomial-time
convex optimization formulations for two-layer networks. In International Conference on Machine
Learning, pp. 7695–7705. PMLR, 2020."
REFERENCES,0.28193832599118945,"Guillaume Rabusseau, Tianyu Li, and Doina Precup. Connecting weighted automata and recurrent
neural networks through spectral learning. In The 22nd International Conference on Artiﬁcial
Intelligence and Statistics, pp. 1630–1639. PMLR, 2019."
REFERENCES,0.2834067547723935,"R Tyrrell Rockafellar and Roger J-B Wets. Variational analysis, volume 317. Springer Science &
Business Media, 2009."
REFERENCES,0.28487518355359764,"Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition
challenge. International journal of computer vision, 115(3):211–252, 2015."
REFERENCES,0.28634361233480177,"Arda Sahiner, Tolga Ergen, John Pauly, and Mert Pilanci. Vector-output ReLU neural network
problems are copositive programs: Convex analysis of two layer networks and polynomial-time
algorithms. arXiv preprint arXiv:2012.13329, 2020."
REFERENCES,0.2878120411160059,"Emilio Seijo, Bodhisattva Sen, et al. Nonparametric least squares estimation of a multivariate convex
regression function. The Annals of Statistics, 39(3):1633–1657, 2011."
REFERENCES,0.28928046989721,"Pranab K Sen and Julio M Singer. Large sample methods in statistics: an introduction with applica-
tions, volume 25. CRC press, 1994."
REFERENCES,0.2907488986784141,"Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczy´nski. Lectures on stochastic program-
ming: modeling and theory. SIAM, 2014."
REFERENCES,0.2922173274596182,"Mahdi Soltanolkotabi. Learning ReLUs via gradient descent. In NIPS, 2017."
REFERENCES,0.2936857562408223,"Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
In Advances in neural information processing systems, pp. 3104–3112, 2014."
REFERENCES,0.29515418502202645,"Yuandong Tian. An analytical formula of population gradient for two-layered ReLU network and its
applications in convergence and critical point analysis. In ICML, 2017."
REFERENCES,0.2966226138032305,Under review as a conference paper at ICLR 2022
REFERENCES,0.29809104258443464,"Kim-Chuan Toh, Michael J Todd, and Reha H Tütüncü. Sdpt3—a matlab software package for
semideﬁnite programming, version 1.3. Optimization methods and software, 11(1-4):545–581,
1999."
REFERENCES,0.29955947136563876,"Leslie G. Valiant. A theory of the learnable. Commun. ACM, 27:1134–1142, 1984."
REFERENCES,0.3010279001468429,"Constance van Eeden.
Maximum likelihood estimation of ordered probabilities, 2.
Stichting
Mathematisch Centrum. Statistische Afdeling, (S 196/56), 1956."
REFERENCES,0.302496328928047,"Vladimir Vapnik. Principles of risk minimization for learning theory. In Advances in neural
information processing systems, pp. 831–838, 1992."
REFERENCES,0.3039647577092511,"Shanshan Wu, Alexandros G. Dimakis, and Sujay Sanghavi. Learning distributions generated by
one-layer ReLU networks. In NeurIPS, 2019."
REFERENCES,0.3054331864904552,"Yinyu Ye and Edison Tse. An extension of karmarkar’s projective algorithm for convex quadratic
programming. Mathematical programming, 44(1-3):157–179, 1989."
REFERENCES,0.3069016152716593,"Xiao Zhang, Yaodong Yu, Lingxiao Wang, and Quanquan Gu. Learning one-hidden-layer ReLU
networks via gradient descent. ArXiv, abs/1806.07808, 2018."
REFERENCES,0.30837004405286345,"Yin Zhang, Richard A Tapia, and John E Dennis, Jr. On the superlinear and quadratic convergence of
primal-dual interior point linear programming algorithms. SIAM Journal on Optimization, 2(2):
304–324, 1992."
REFERENCES,0.30983847283406757,"Kai Zhong, Zhao Song, Prateek Jain, Peter L. Bartlett, and Inderjit S. Dhillon. Recovery guarantees
for one-hidden-layer neural networks. In ICML, 2017."
REFERENCES,0.31130690161527164,APPENDICES
REFERENCES,0.31277533039647576,We include an outline of the appendices:
REFERENCES,0.3142437591776799,"• Appendix A: An empirical analysis of the running time of our algorithm and an analysis of
the effect of condition number on the estimation accuracy."
REFERENCES,0.315712187958884,"• Appendix B: A brief discussion of our learning algorithms in a noisy context, referring to
the LP slack variable technique that is used in our experiments with noise in the main paper.
Also, this section provides an insight to potential future direction of this work."
REFERENCES,0.31718061674008813,"• Appendix C: A detailed explanation of the computational complexity and the methods
through which the QPs/LPs in this paper are solved."
REFERENCES,0.3186490455212922,"• Appendix D: A preliminarily proposed general optimization formula for the extension of
learning multi-layer ResNets, to show the scalability of our algorithm."
REFERENCES,0.3201174743024963,"• Appendix E: A justiﬁcation of how our QP/LP approaches get rid of the exponentiality that
vanilla LR approach (section 3) has."
REFERENCES,0.32158590308370044,"• Appendix F: A formal result and an empirical validation of the example given in the main
paper in the context of the vanilla linear regression method (section 3)."
REFERENCES,0.32305433186490456,• Appendix G: A generalization of layer 2 learning to the case where Cond. 2.1 is not satisﬁed.
REFERENCES,0.3245227606461087,• Appendix H: Proofs regarding the minimizers of the objective functions we use.
REFERENCES,0.32599118942731276,• Appendix I: Proofs that justify the convexity of our QPs.
REFERENCES,0.3274596182085169,• Appendix J: Proofs that show our estimation algorithm is strongly consistent.
REFERENCES,0.328928046989721,"A
RUNNING TIME AND SENSITIVITY TO CONDITION NUMBER"
REFERENCES,0.3303964757709251,"Following the setup in section 7, we report additional experiments testing the running time of
our algorithm in comparison to SGD and the feasibility of estimating the inverse of B∗when
ill-conditioned."
REFERENCES,0.33186490455212925,Under review as a conference paper at ICLR 2022
REFERENCES,0.3333333333333333,"0
0.5
1
1.5
2
2.5
-2 -1.5 -1 -0.5 0 0.5"
REFERENCES,0.33480176211453744,"SGD-GT1
SGD-GT2
SGD-GT3
SGD-GT4
SGD-GT5"
REFERENCES,0.33627019089574156,"LP-GT1
LP-GT2
LP-GT3
LP-GT4
LP-GT5"
REFERENCES,0.3377386196769457,"0
2
4
6
8
10
12
-1 -0.8 -0.6 -0.4 -0.2 0 0.2"
REFERENCES,0.3392070484581498,"SGD-GT1
SGD-GT2
SGD-GT3
SGD-GT4
SGD-GT5"
REFERENCES,0.3406754772393539,"LP-GT1
LP-GT2
LP-GT3
LP-GT4
LP-GT5"
REFERENCES,0.342143906020558,"Figure 3: Output errors against running time by SGD and our LP algorithm for different
dimensions of networks. For each dimension size of d = 8, d = 16 and d = 32, we report the
learning curves of SGD and the ﬁnal output error of our LP algorithm against time used on 5 different
ground-truth networks. We used 512 training samples, drawn for learning each ground-truth network.
(CPU speciﬁcation: 2.8 GHz Quad-Core Intel Core i7.)"
REFERENCES,0.3436123348017621,"100
101
102
103
104
105
2 3 4 5 6 7 8 9 10 11"
SGD,0.34508076358296624,"12
SGD"
SGD,0.3465491923641703,"100
101
102
103
104
105 10-13 10-12 10-11 LP"
SGD,0.34801762114537443,"Figure 4: Layer 2 errors against layer 2 ground-truth weight condition numbers for different
dimensions of networks by SGD and our LP algorithm. Each data point is the mean across
learning 32 different ground-truth networks with the same pair of d and a condition number for B∗.
The condition number is denoted by κ(B∗). We used 512 training samples, drawn for learning each
ground-truth network."
SGD,0.34948604992657856,"A.1
RUNNING TIME EFFICIENCY VERSUS SGD"
SGD,0.3509544787077827,"In Fig. 3, we compare the running time of our algorithm against the running time of SGD for
ground-truth networks sampled at random. The running time of our algorithm is signiﬁcantly lower,
with an error level which is also signiﬁcantly lower. Furthermore, this is demonstrated across input
dimensions, where the difference between the running time of SGD until convergence and our
algorithm’s running time increases as d increases. In addition, the running time of our algorithm is
ﬁxed, and does not depend on the ground-truth network, while the running time of SGD varies based
on the network sampled."
SGD,0.3524229074889868,"A.2
LAYER 2 WEIGHTS CONDITION NUMBER ROBUSTNESS"
SGD,0.35389133627019087,"Our algorithm depends on a matrix-inversion step to obtain the estimated parameters. This affects the
estimation of B∗. Due to this, we further investigate the effect of the condition number of B∗in the
ground-truth parameters on the accuracy of estimation. For this experiment, to obtain ground-truth
network parameters with different condition number, we follow the procedure of Ge et al. (2018),
and multiply a diagonal matrix with exponentially-dropping values on the diagonal (λ−i for the i-th
element) by two random orthonormal matrices, U and V⊤, on the left and on the right respectively:
B = U diag
 
λ−1, . . . , λ−d
V⊤."
SGD,0.355359765051395,Under review as a conference paper at ICLR 2022
SGD,0.3568281938325991,"In Fig. 4, we compare the effect of the condition number of B∗on SGD and on our algorithm.
SGD turns out to be quite sensitive to the condition number, especially for lower dimensions. Our
algorithm, on the other hand, is almost not affected by the condition number, with a ﬁnal estimation
error consistently close to 0, recovering the matrix B∗."
SGD,0.35829662261380324,"B
DISCUSSION: NOISY MODEL"
SGD,0.35976505139500736,"Here, we discuss our learning methods in the noisy case. We introduce output noise to our model,
namely"
SGD,0.36123348017621143,"y = B∗h
(A∗x)+ + x
i
+ z,
(9)"
SGD,0.36270190895741555,"where the label noise z ∈Rd is an i.i.d. random vector with respect to each component, satisfying
E [z] = 0. In addition z and x are statistically independent."
SGD,0.3641703377386197,"Taking layer 2 as an example, our original objective functional does not reach zero by substituting
the ground-truth: G2(C∗, x 7→(A∗x)+) = Ez[∥C∗z∥2] = σ2 Tr C∗C∗⊤where σ is the noise
strength. However, if layer 2 is well conditioned, the ground-truth will still assign a value close
to zero to the objective. In this sense, with G2’s continuity, the ground-truth can approximately
minimize G2, which validates our QP approach in terms of learning noisy residual units."
SGD,0.3656387665198238,"Our original LP (Eq. 5) fails to give feasible solutions due to possible violation of the inequality
C∗y −x ≥0, since C∗y −x = (A∗x)+ + C∗z is not necessarily an entrywise nonnegative vector
because the term C∗z might have negative entries. So we introduce slack variables ζ(i) to soften the
constraints:"
SGD,0.3671071953010279,"min
C,Z
1
n X"
SGD,0.368575624082232,"i∈[n]
1⊤· ζ(i),
(10)"
SGD,0.3700440528634361,"s.t. Cy(i) −x(i) ≥−ζ(i), ζ(i) ≥0.
(11)"
SGD,0.37151248164464024,"For a sample (x(i), y(i)) with noise z(i), if
 
A∗x(i)+ + C∗z(i) < 0, then its L1 norm would be
added to the objective. This method remedies violations to the inequality C∗y −x ≥0. With access
to a sufﬁciently large sample and with the “stability” assumption, our solution ˆC would be close to
C∗since large deviations seldom rise and their penalties are diluted in the objective."
SGD,0.37298091042584436,"C
DISCUSSION: SOLVING CONVEX PROGRAMS"
SGD,0.3744493392070485,"The theoretical foundation of solving convex QP and LP has been driven to maturity in terms of
computational complexity (Kozlov et al., 1980; Murty, 1980) and convergence analysis (Jarre, 1990;
Zhang et al., 1992). The time complexity for a convex QP/LP is analyzed in terms of the number
of scalar variables N and the number of bits L in the input (Ye & Tse, 1989). For example, under
Cond. 2.1, N = d2 + nd for the QP because the matrix variables have d2 scalars and the function
estimators have nd scalars. Similarly we have N = d2 for the noiseless LP and N = d2 + nd for the
noisy LP. It is guaranteed that primal-dual interior point methods can solve the convex QP/LP in a
polynomial number of iterations O(
√"
SGD,0.37591776798825255,"NL), where each iteration costs at worst O(N 2.5) arithmetic
operations from the Cholesky decomposition for the needed matrix inversion (Monteiro & Adler,
1989; Karmarkar, 1984). This indicates that our QPs/LPs are guaranteed to be solvable in O(N 3L)
arithmetic operations, which is generally an O(poly(n, m, d, L)) complexity."
SGD,0.37738619676945667,"To solve convex QPs/LPs in this paper, we use CVX, a commonly used package for specifying and
solving convex programs (Grant & Boyd, 2014; 2008). For both QPs and LPs, CVX calls a solver,
SDPT3 (Toh et al., 1999), which is speciﬁed for semideﬁnite-quadratic-linear programming and
applies interior-point methods with the computational complexity mentioned above. Experimentally,
SDPT3 indeed speciﬁes and solves our programs fast and makes our numerical results robust and
stable."
SGD,0.3788546255506608,Under review as a conference paper at ICLR 2022
SGD,0.3803230543318649,"D
DISCUSSION: EXTENSION TO MULTI-LAYER"
SGD,0.38179148311306904,"In this appendix, we propose a generalized optimization formula of nonparametric learning of multi-
layer residual networks to show that our algorithm is scalable. Assume we have an L-layer network
of the form"
SGD,0.3832599118942731,r0 = x
SGD,0.38472834067547723,"rl =
 
W ∗
l−1rl−1
+ + rl−1, l = 1, . . . , L −1"
SGD,0.38619676945668135,"y = W ∗
L−1rL−1."
SGD,0.3876651982378855,"We can scale the objective to use nonparameter variables {ξ(i)
l }l∈[L−1] to estimate every ReLU
activation (W ∗
l rl)+ in this network. Generally, for the l-th ReLU we can have the outer layer
objective"
SGD,0.3891336270190896,"min
ξ(i)
[l] ,Vl"
SGD,0.39060205580029367,"ξ(i)
l
+ · · · + ξ(i)
1
+ x(i) −Vlˆh(i)
l
 , s.t. ξ(i)
[l] ≥0"
SGD,0.3920704845814978,"where hat denotes estimated values and ˆh(i)
L−1 = y(i), and its corresponding inner layer objective"
SGD,0.3935389133627019,"min
φ(i)
l
,Wl−1,ξ(i)
l"
SGD,0.39500734214390604,"φ(i)
l
+ Wl−1

ˆξ(i)
l−1 + · · · + ˆξ(i)
1
+ x(i)
−ξ(i)
l
 , s.t. φ(i)
l
≥0, ξ(i)
l
≥0."
SGD,0.3964757709251101,"We can apply techniques, e.g., tune the combinations of those objectives with respect to layers or
whether to have a hat or not, or even get them weighted, to keep its convex quadratic form and at the
same time improve its optimization landscape. Speciﬁcally when L = 2, this becomes the two-layer
ResNet learning discussed in the main paper."
SGD,0.39794419970631423,"E
SAMPLE EFFICIENCY: VANILLA LR VERSUS OURS"
SGD,0.39941262848751835,"In this appendix, we describe an example to justify how our core QP/LP approaches eliminate
the exponential sample efﬁciency compared to the vanilla LR approach. In layer 2 learning, C’s
optimization is to ﬁnd a feasible point in the space determined by n inequalities each of which
corresponds to a sample. Taking the j-th row of the inequality, i.e. Cj,:y ≥xj. Every time we get a
new sample x(i), y(i) in,"
SGD,0.4008810572687225,"• The inequality Cj,:y(i) ≥x(i)
j
eliminates the solution space of Cj,: by one of the spaces"
SGD,0.4023494860499266,"divided by plane Cj,:y(i) = x(i)
j . This property guarantees fast convergence speed at early
phase since Cj,:’s feasibility starts from Rd."
SGD,0.40381791483113066,"• In fact, when ReLU does not activate A∗x(i) at the j-th row, i.e. A∗x(i) ≤0, the theoretical
solution of Cj,: that Cj,:B∗= (0, . . . , 0, 1, 0, . . . , 0) (a 1 at index j) directly lies on the
divisive plane Cj,:y(i) = x(i)
j :"
SGD,0.4052863436123348,"Cj,:y(i) = Cj,:B∗

A∗x(i)+
+ x(i)

= x(i)
j ."
SGD,0.4067547723935389,"This property remarkably speeds up further constraints on the solution space of Cj,: to the
correct estimate and is with high probability, since it directly depends on the sign of A∗x(i)."
SGD,0.40822320117474303,"With the vanilla LR approach mentioned in section 3 all the dimensions need to have the correct
sign at the same time. This is the reason for the exponential complexity of this approach. Our main
algorithm only requires one dimension to get the correct sign for a single sample, which avoids the
exponential sample size expectation. The convergence speed is determined by the actual probability
of each dimension not getting activated by ReLU, and such probability is determined by speciﬁed
input distribution."
SGD,0.40969162995594716,Under review as a conference paper at ICLR 2022
SGD,0.4111600587371512,Algorithm 3 Learn a ReLU residual unit by LR.
SGD,0.41262848751835535,"1: Input: {(x(i), y(i))}n
i=1, n samples drawn by Eq. 1.
2: Output: ˆ
A, ˆ
B, estimated weight matrices."
SGD,0.41409691629955947,"3:
ˆ
B ←LR{(x, y) ∈{(x(i), y(i))}⌊n/2⌋
i=1
| x < 0}.
4: ˆD ←LR{(x, y) ∈{(x(i), y(i))}n
i=⌊n/2⌋+1 | x > 0}. {the estimation of B∗(A∗+ Id)}"
SGD,0.4155653450807636,"5: Solve full-rank linear equation system ˆ
B · ˜
A = ˆD
6: return
˜
A −Id, ˆ
B."
SGD,0.4170337738619677,"F
VANILLA LINEAR REGRESSION: MORE DETAILS"
SGD,0.4185022026431718,"Alg. 3 gives the full list of steps of learning two-layer residual units by LR, where we ﬁrst split the
drawn samples into two halves for the respective two key steps, then for both halves we ﬁlter negative
and positive vectors, respectively. By running LR on both ﬁltered sets we obtain an estimation of the
ground-truth network."
SGD,0.4199706314243759,"In the following sections, we formalize this intuition and describe the exponential sample complexity
of this approach under entry-wise i.i.d. setting as an example of the inefﬁciency of this method.
Theorem F.1 (exponential sample complexity, vanilla LR). Assume the input vectors are i.i.d. with
respect to each component. Then Alg. 3 learns a neural network from a ground-truth residual unit
with at least O
 
d · 2d+1
expected number of samples."
SGD,0.42143906020558003,"Proof. For each j ∈[d], let Pj be the marginal probability of xj being positive, i.e. Pj = P(xj >
0) > 0. Thus, the probability that a sample is positive P(x > 0) = Q"
SGD,0.42290748898678415,"j∈[d] Pj. Then the expected
number of sampling trials to obtain d positive samples is d/ Q
j∈[d] Pj. Similarly, the expected
number of sampling trials to obtain d negative samples is d/ Q"
SGD,0.4243759177679883,"j∈[d] [1 −Pj −P(xj = 0)]. Let n
be a random natural number s.t. with n samples the network has learned. The expected number of
samples that guarantees successful learning is the sum of the two expectations"
SGD,0.42584434654919234,"E[n] = d (
1
Q"
SGD,0.42731277533039647,"j∈[d] Pj
+
1
Q"
SGD,0.4287812041116006,"j∈[d] [1 −Pj −P(xj = 0)] ) ≥d ""
1
Q"
SGD,0.4302496328928047,"j∈[d] Pj
+
1
Q"
SGD,0.43171806167400884,j∈[d] (1 −Pj) #
SGD,0.4331864904552129,"≥d · 2
Y j∈[d]"
P,0.434654919236417,"1
p"
P,0.43612334801762115,Pj (1 −Pj)
P,0.43759177679882527,"≥d · 2
Y j∈[d]"
P,0.4390602055800294,"1
(Pj + 1 −Pj) /2"
P,0.44052863436123346,= d · 2d+1.
P,0.4419970631424376,Thus E[n] ≥O(d · 2d+1).
P,0.4434654919236417,"With an exponential sample complexity, the time complexity of the LR approach is thereby also
exponential because ﬁltering through all the samples costs time with the same complexity as the
number of samples, even though the LR itself only costs polynomial time in the number of samples.
To summarize, this vanilla LR approach learns exact ground-truth residual units but with exponential
complexity in terms of both computational cost and sample size. While this approach to learning a
residual unit such as ours is simple and intuitive, we aim to improve on this approach, making full
use of all samples available."
P,0.44493392070484583,"F.1
EXPERIMENTS: SAMPLE EFFICIENCY"
P,0.44640234948604995,"We present the results of the vanilla LR in learning the residual units. As discussed in section 3, LR
requires full-rank linear systems parameterized by samples to learn the exact ground-truth parameters."
P,0.447870778267254,Under review as a conference paper at ICLR 2022
P,0.44933920704845814,"Table 2: Learning success rate of vanilla LR on residual units with input x
i.i.d.
∼N (0, 1). The
success rates shows an exponentially decreasing trend with the same number of samples. The sample
sizes to achieve close to the same rate grows exponentially as the number of dimensions grows
linearly."
P,0.45080763582966227,"d
n
1e1
1e2
5e2
1e3
5e3
1e4
5e4
1e5"
P,0.4522760646108664,"4
0.002
0.137
0.999
1
1
1
1
1"
P,0.45374449339207046,"6
0
0
0.041
0.614
1
1
1
1"
P,0.4552129221732746,"8
0
0
0
0
0.579
0.998
1
1"
P,0.4566813509544787,"10
0
0
0
0
0
0.002
1
1"
P,0.4581497797356828,"12
0
0
0
0
0
0
0.001
0.322"
P,0.45961820851688695,"Table 3: Learning success rate of vanilla LR on residual units with input x
i.i.d.
∼N (0.1, 1). With
the non-zero Gaussian mean, the success rates show an overall decline compared with the rates shown
in Tab. 2 for zero-mean Gaussian inputs."
P,0.461086637298091,"d
n
1e1
1e2
5e2
1e3
5e3
1e4
5e4
1e5"
P,0.46255506607929514,"4
0.001
0.122
0.996
1
1
1
1
1"
P,0.46402349486049926,"6
0
0
0.030
0.346
1
1
1
1"
P,0.4654919236417034,"8
0
0
0
0
0.116
0.792
1
1"
P,0.4669603524229075,"10
0
0
0
0
0
0
0.619
1"
P,0.4684287812041116,"12
0
0
0
0
0
0
0
0.001"
P,0.4698972099853157,"However, with degenerate linear systems, LR is completely incapable of learning the parameters.
Therefore, there exists a hard threshold for the number of samples required by LR that makes the
linear equation system full-rank. With such a dichotomous constraint on LR learning, we take the
learning success rate among 1000 trials as the metric to evaluate the performance of LR with different
sample sizes and number of dimensions."
P,0.4713656387665198,"Tab. 2 shows the learning success rates with zero-mean Gaussian inputs. For each ﬁxed number of
dimensions, it appears there is a hard threshold that switches the learnability of LR. The exponential
sample complexity is also reﬂected there as the number of dimensions grows linearly. Tab. 3 shows the
rates with input mean non-zero, where an overall decline in success rates happens. This observation
is explainable because the bottleneck of LR learning is the lower value found between the probability
of sampling a positive and a negative vector. A positive mean reduces the probability of the latter,
and thereby increases the sample size required."
P,0.47283406754772395,"G
GENERALIZATION OF LAYER 2 LEARNING"
P,0.47430249632892807,"In this appendix, we discuss how layer 2 is learned without satisfying Cond. 2.1. It is a fact that
A∗being a scale transformation w.r.t. some components causes scaling equivalence to our layer 2
objective functional minimizers just as in layer 1. To be more speciﬁc, we give a general version
of Thm. 4.1 that handles the case without Cond. 2.1 and describes the scaling equivalence of G2
minimizers. See Thm. G.1 for a formal description."
P,0.47577092511013214,"Theorem G.1 (objective minimizer space, layer 2, general). Let G2 be 1"
"EX
H",0.47723935389133626,"2Ex
h
∥h (x) + x −Cy∥2i"
"EX
H",0.4787077826725404,"as a functional, where C ∈Rd×m, h ∈C0
≥0. Then G2 (C, h) reaches its zero minimum iff
CB∗= diag(k) where for each j ∈[d],"
"EX
H",0.4801762114537445,"a)
1
1+A∗
j,j ≤kj ≤1, if A∗is a scale transformation w.r.t. the j-th component.
b) kj = 1, if A∗is not a scale transformation w.r.t. the j-th component."
"EX
H",0.48164464023494863,Under review as a conference paper at ICLR 2022
"EX
H",0.4831130690161527,"As in layer 1, the scaling equivalence in layer 2 can also be obtained by assigning A∗as a scale
transformation w.r.t. some component and using the properties of the ReLU nonlinearity. See
subsection H.1 for a detailed explanation of Thm. G.1. To obtain the scale factors k and correct a
scale-equivalent G2 minimizer C to the ground-truth, we observe linear models parameterized by the
scale factors, where for each j ∈[d], [Cy]j = kjxj given that xj < 0. See Thm. G.2 and its proof
for justiﬁcations of the linear models that are used to compute k."
"EX
H",0.4845814977973568,"Theorem G.2 (scale factor, layer 2, general). Assume C is a minimizer of G1 in the context of
Thm. G.1. Then for any j ∈[d], the following three propositions are equivalent:"
"EX
H",0.48604992657856094,"a) A∗is a scale transformation w.r.t. the j-th component.
b) [Cy]j /xj is a constant cn
j given that xj < 0.
c) [Cy]j /xj is a constant cp
j given that xj > 0."
"EX
H",0.48751835535976507,"Additionally, if one of the above three propositions is true, then kj = cn
j ."
"EX
H",0.4889867841409692,"Proof. For j ∈[d], we ﬁrst prove a) =⇒b), c): From
 
A∗
j,:
⊤= A∗
j,j · e(j) we have [Cy]j xj
="
"EX
H",0.49045521292217326,"
kje(j)⊤h
(A∗x)+ + x
i xj"
"EX
H",0.4919236417033774,"=
kj
h 
A∗
j,:x
+ + xj
i"
"EX
H",0.4933920704845815,"xj
=
kj
h 
A∗
j,jxj
+ + xj
i xj"
"EX
H",0.4948604992657856,"=
kj,
xj < 0
kj
 
A∗
j,j + 1

,
xj > 0 ."
"EX
H",0.49632892804698975,"Then we prove ¬ a) =⇒¬ b), ¬ c): ¬ a) =⇒∃j′ ∈[d] and j′ ̸= j s.t. A∗
j,j′ ̸= 0. Recall [Cy]j"
"EX
H",0.4977973568281938,"xj
=
kj
h 
A∗
j,:x
+ + xj
i xj
."
"EX
H",0.49926578560939794,"The value of xj′ affects the value of [Cy]j /xj because A∗
j,j′ ̸= 0. In fact, from supp p(x) = Rd we
have supp p(xj′ | xj) = R, indicating that the value of [Cy]j /xj can never be kept as a constant
when given both xj < 0 and xj > 0 because xj′ can be any real number."
"EX
H",0.5007342143906021,"With the exact derivations above, we are able to obtain a left inverse of B∗, namely C, that satisﬁes
CB∗= Id. Consider Eq. 1 left multiplied by B∗C"
"EX
H",0.5022026431718062,"B∗Cy = y.
(12)"
"EX
H",0.5036710719530103,"Eq. 12 is also a noiseless unbiased linear model where Cy and y are observable, and thereby B∗is
computable due to the easy solvability of its linearity."
"EX
H",0.5051395007342144,"Alg. 5 describes how a residual unit second layer is learned without satisfying Cond. 2.1: We ﬁrst
solve the QP/LP and obtain a scaling equivalence to an estimated left inverse of B∗and the function
estimate, namely ˆC and ˆΞ. Then we compute the scale factor estimate ˆk by running Alg. 4, where
for each component index j ∈[d], we ﬁrst use a tolerance parameter as a threshold to determine
whether ground-truth layer 1 is a scale transformation, and if so, we run LR to estimate the model
[Cy]j = kjxj, otherwise the scale factor is directly assigned by 1. Upon correcting ˆC and ˆΞ by
ˆk, we obtain a layer 2 estimate ˆ
B by running LR to estimate the linear model Eq. 12. The strong
consistency of our results in this appendix is justiﬁed in Appendix J."
"EX
H",0.5066079295154186,"H
EXACT DERIVATION OF OBJECTIVE FUNCTIONAL MINIMIZERS"
"EX
H",0.5080763582966226,"H.1
LAYER 2"
"EX
H",0.5095447870778267,Lem. 4.2 is proved as follows.
"EX
H",0.5110132158590308,Under review as a conference paper at ICLR 2022
"EX
H",0.5124816446402349,"Algorithm 4 Rescale a ˆGNPE
2
minimizer."
"EX
H",0.5139500734214391,"1: Parameters: εtol > 0, LR objective tolerance.
2: Input: {(x(i), y(i))}n
i=1, samples drawn by Eq. 1; ˆC, a ˆGNPE
2
minimizer.
3: Output: ˆk, a layer 2 scale factor estimate.
4: for each j ∈[d] do"
"EX
H",0.5154185022026432,"5:
ˆkj ←LR
n
x(i)
j ,
 ˆCy(i) j o"
"EX
H",0.5168869309838473,"x(i)
j
<0."
"EX
H",0.5183553597650514,"6:
if the LR objective optimal ˆRj( ˆkj) > εtol then
7:
ˆkj ←1.
8:
end if
9: end for
10: return ˆk."
"EX
H",0.5198237885462555,Algorithm 5 Learn a ReLU residual unit layer 2.
"EX
H",0.5212922173274597,"1: Input: {(x(i), y(i))}n
i=1, samples drawn by Eq. 1.
2: Output: ˆ
B, ˆΞ, a layer 2 and x 7→(A∗x)+ estimate.
3: Go to line 4 if QP, line 5 if LP.
4: Solve QP: Eq. 4 and obtain a ˆGNPE
2
minimizer, denoted by ˆC, ˆΞ. Go to line 6.
5: Solve LP: Eq. 5 and obtain a minimizer ˆC, then assign ˆξ(i) ←ˆCy(i) −x(i) for each i ∈[n].
6: Run Alg. 4 on {(x(i), y(i))}i∈[n], ˆC and obtain ˆk."
"EX
H",0.5227606461086637,"7:
ˆ
B ←LR
n
diag−1(ˆk) · ˆCy(i), y(i)o"
"EX
H",0.5242290748898678,i∈[n].
"EX
H",0.5256975036710719,"8: ˆξ(i) ←diag−1(k)
ˆξ(i) + x(i)
−x(i) for each i ∈[n]. {Correct ˆΞ to the function estimation
of (A∗x)+.}
9: return
ˆ
B, ˆΞ."
"EX
H",0.527165932452276,"Proof. “ ⇐= ”: Since h(x) = Cy −x ≥0, random vector h(x) + x −Cy is always a zero vector,
which implies G2(C, h) = 0. Hence “ ⇐= ” holds."
"EX
H",0.5286343612334802,“ =⇒”: Since r.v. ∥h(x) + x −Cy∥2 ≥0 we have
"EX
H",0.5301027900146843,"G2(C, h) = 0 =⇒λ
n
x ∈Rd  ∥h(x) + x −Cy∥2 > 0
o
= 0
(13)"
"EX
H",0.5315712187958884,where λ is the Lebesgue measure on Rd.
"EX
H",0.5330396475770925,"Proof by contradiction: Assume that ∃x′ ∈Rd, ∃i ∈[d], s.t. (Cy′ −x′)i < 0, where y′ is the
corresponding network output. Let f(x) = (Cy −x)i which is continuous on Rd. Therefore for
ϵ = −f(x′) > 0, ∃δ > 0, ∀x ∈B(x; δ) i.e. ∥x −x′∥< δ,"
"EX
H",0.5345080763582967,|f(x) −f(x′)| < ϵ =⇒2f(x′) < f(x) < 0 =⇒[x −Cy]i > 0
"EX
H",0.5359765051395007,=⇒[h(x) + x −Cy]i > 0 =⇒∥h(x) + x −Cy∥2 > 0.
"EX
H",0.5374449339207048,"Since λ (B(x; δ)) =
πd/2"
"EX
H",0.5389133627019089,"Γ (d/2 + 1)δd > 0 and B(x; δ) is a subset of the measured set in Eq. 13,"
"EX
H",0.540381791483113,"we have a contradiction. Thus Cy −x ≥0 holds in a pointwise manner in Rd, indicating

x ∈Rd | h(x) ̸= Cy −x
	
must be a null set. With h’s continuity, h must be x 7→Cy −x.
Therefore “ =⇒” holds."
"EX
H",0.5418502202643172,"Lemma H.1. Cy −x ≥0 holds for any x ∈Rd and its corresponding output y only if CB∗is a
diagonal matrix."
"EX
H",0.5433186490455213,Proof. Let D = CB∗. We rewrite Cy −x ≥0 as
"EX
H",0.5447870778267254,"D
h
(A∗x)+ + x
i
−x ≥0.
(14)"
"EX
H",0.5462555066079295,Under review as a conference paper at ICLR 2022
"EX
H",0.5477239353891337,"For further use, we substitute x by −x and the resulting inequality D
h
(−A∗x)+ −x
i
+ x ≥0
still holds. Added by Eq. 14 we have"
"EX
H",0.5491923641703378,"D
 A∗
k,: · x
"
"EX
H",0.5506607929515418,"d×1 ≥0.
(15)"
"EX
H",0.5521292217327459,"Proof by contradiction: Assume D is not diagonal, then ∃i ̸= j, such that Di,j ̸= 0. Consider the
following two cases:"
"EX
H",0.55359765051395,"a) Di,j > 0. We take the i-th row of Eq. 14 as follows d
X"
"EX
H",0.5550660792951542,"k=1
Di,k
h 
A∗
k,: · x
+ + xk
i
≥xi.
(16)"
"EX
H",0.5565345080763583,"Let x−j = 0 and xj < 0. Then Pd
k=1 Di,k"
"EX
H",0.5580029368575624,"
A∗
k,: · x
+
+ xk"
"EX
H",0.5594713656387665,"
= Di,j · xj < 0 =⇒⊥."
"EX
H",0.5609397944199707,"b) Di,j < 0. We take the i-th row of Eq. 15 as follows d
X"
"EX
H",0.5624082232011748,"k=1
Di,k
A∗
k,: · x
 ≥0.
(17)"
"EX
H",0.5638766519823789,"Let x = (A∗)−1 v, where v = e(j). Then Pd
k=1 Di,k
A∗
k,: · x
 = Di,j < 0 =⇒⊥."
"EX
H",0.5653450807635829,"With Lem. 4.2 and Lem. H.1, we prove Thm. G.1 as follows."
"EX
H",0.566813509544787,"Proof. With Lem. 4.2, we only need to prove ∀x ∈Rd, Cy −x ≥0 ⇐⇒CB∗= diag(k)."
"EX
H",0.5682819383259912,“ ⇐= ”: We have
"EX
H",0.5697503671071953,"Cy −x = CB∗h
(A∗x)+ + x
i
−x = diag(k)
h
(A∗x)+ + x
i
−x.
(18)"
"EX
H",0.5712187958883994,"For the i-th row of Eq. 18, consider the following two cases:"
"EX
H",0.5726872246696035,"a) A∗
i,: · x = Ai,ixi, i.e. A∗is a scale transformation w.r.t. the i-th row. With
1
1+A∗
i,i ≤ki ≤1
we have"
"EX
H",0.5741556534508077,"[Cy −x]i = ki
h 
A∗
i,ixi
+ + xi
i
−xi = ki
 
A∗
i,ixi
+ + (ki −1) xi."
"EX
H",0.5756240822320118,"For xi ≥0, [Cy −x]i =
 
kiA∗
i,i + ki −1

xi ≥0.
For xi < 0, [Cy −x]i = (ki −1) xi ≥0.
b) A∗
i,: · x ̸= Ai,ixi. With ki = 1 we have"
"EX
H",0.5770925110132159,"[Cy −x]i = ki
h 
A∗
i,:x
+ + xi
i
−xi =
 
A∗
i,:x
+ ≥0."
"EX
H",0.57856093979442,Hence “ ⇐= ” holds.
"EX
H",0.580029368575624,"“ =⇒”: Let D = CB∗. With Lem. H.1, D is diagonal. Consider the following two cases:"
"EX
H",0.5814977973568282,"a) A∗
i,: · x = Ai,ixi. The i-th inequality can be written as"
"EX
H",0.5829662261380323,"[Cy −x]i = Di,i
h 
A∗
i,ixi
+ + xi
i
−xi"
"EX
H",0.5844346549192364,"= Di,i
 
A∗
i,ixi
+ + (Di,i −1) xi ≥0.
(19)"
"EX
H",0.5859030837004405,"Proof by contradiction: we need to ﬁnd x ∈Rd which contradicts with Eq. 19 in the
following three cases:"
"EX
H",0.5873715124816447,"a) If Di,i ≤0, then, xi > 0 =⇒Di,i
 
A∗
i,ixi
+ + (Di,i −1) xi < 0 =⇒⊥.
b) If Di,i > 1, then, xi < 0 =⇒⊥."
"EX
H",0.5888399412628488,Under review as a conference paper at ICLR 2022
"EX
H",0.5903083700440529,"c) If 0 < Di,i <
1
1+A∗
i,i , then ∃a > 0, s.t. Di,i =
1
1+A∗
i,i+a. Letting xi > 0, we have"
"EX
H",0.591776798825257,"Di,i
 
A∗
i,ixi
+ + (Di,i −1) xi ="
"EX
H",0.593245227606461," 
A∗
i,ixi
+"
"EX
H",0.5947136563876652,"1 + A∗
i,i + a −"
"EX
H",0.5961820851688693," 
A∗
i,i + a

xi
1 + A∗
i,i + a < 0 =⇒⊥."
"EX
H",0.5976505139500734,"Hence
1
1+A∗
i,i ≤Di,i ≤1.
b) A∗
i,: · x ̸= Ai,ixi, i.e. ∃j ̸= i, s.t. A∗
i,j > 0. The i-th inequality can be written as"
"EX
H",0.5991189427312775,"[Cy −x]i = Di,i
h 
A∗
i,:x
+ + xi
i
−xi"
"EX
H",0.6005873715124816,"= Di,i
 
A∗
i,:x
+ + (Di,i −1) xi ≥0.
(20)"
"EX
H",0.6020558002936858,"Proof by contradiction: we need to ﬁnd x ∈Rd which contradicts with Eq. 20 in the
following three cases:"
"EX
H",0.6035242290748899,"a) If Di,i ≤0, then, xi > 0 =⇒Di,i
 
A∗
i,:x
+ + (Di,i −1) xi < 0 =⇒⊥.
b) If Di,i > 1, then, xi < 0 ∧x−i ≤0 =⇒⊥."
"EX
H",0.604992657856094,"c) If 0 < Di,i < 1, then, xi > 0 ∧xj ≤−
A∗
i,i
A∗
i,j xi ∧xk ≤0 =⇒⊥, where k ̸= i, j.
Hence Di,i = 1."
"EX
H",0.6064610866372981,"Hence D = CB∗= diag(k), and thereby “ =⇒” holds."
"EX
H",0.6079295154185022,"H.2
LAYER 1"
"EX
H",0.6093979441997063,"The proof of Lem. 5.3 is similar to that of Lem. 4.2 because the two lemmas follow the same idea,
which is to link objective functional minimization with always-hold inequalities. With Lem. 5.3 , we
prove Thm. 5.1 as follows."
"EX
H",0.6108663729809104,"Proof. With Lem. 5.3, we only need to prove ∀x ∈Rd, (A∗x)+ −Ax ≥0
⇐⇒
∀i ∈
[d], Ai,: = kiA∗
i,:, where 0 ≤ki ≤1. This is equivalent to what it is for a single row, i.e. ∀x ∈"
"EX
H",0.6123348017621145,"Rd,

a∗⊤x
+
−a⊤x ≥0 ⇐⇒a = ka∗, where 0 ≤k ≤1."
"EX
H",0.6138032305433186,"“ ⇐= ”: The case where a∗= 0 is clear. If a∗is not a zero vector and a = ka∗, we have"
"EX
H",0.6152716593245228,"a) If a∗⊤x ≥0,

a∗⊤x
+
−a⊤x = a∗⊤x −ka∗⊤x = (1 −k) a∗⊤x ≥0."
"EX
H",0.6167400881057269,"b) If a∗⊤x < 0,

a∗⊤x
+
−a⊤x = −ka∗⊤x ≥0."
"EX
H",0.618208516886931,Hence “ ⇐= ” holds.
"EX
H",0.6196769456681351,"“ =⇒”: If a∗= 0, a must be a zero vector, otherwise let x = a, then −a⊤x < 0 =⇒⊥. If a∗is
not a zero vector, consider two cases below:"
"EX
H",0.6211453744493393,a) If a ̸= ka∗where k ≥0. Let x = ∥a∗∥
"EX
H",0.6226138032305433,"∥a∥· a −a∗, then x is not a zero vector, and"
"EX
H",0.6240822320117474,"a∗⊤x = ∥a∗∥2 (cos θ −1) < 0 ,
a⊤x = ∥a∥∥a∗∥(1 −cos θ) > 0"
"EX
H",0.6255506607929515,"
=⇒

a∗⊤x
+
−a⊤x < 0 =⇒⊥"
"EX
H",0.6270190895741556,"where θ denotes the angle between a and a∗.
b) If a = ka∗where k > 1, then let x = a∗we have

a∗⊤x
+
−a⊤x = (1 −k) ∥a∗∥2 < 0 =⇒⊥."
"EX
H",0.6284875183553598,"Hence a = ka∗where 0 ≤k ≤1 if a∗is not zero, and thereby “ =⇒” holds."
"EX
H",0.6299559471365639,"I
QP CONVEXITY"
"EX
H",0.631424375917768,"The LPs in the main paper are trivially convex. So in this appendix, we only justify the convexity of
our QPs: We ﬁrst prove the convexity of single-sample objectives, then the convexity of the empirical
objectives with nonparametric estimation, i.e. ˆGNPE
1
and ˆGNPE
2
is obtained by the convexity of convex
function summations."
"EX
H",0.6328928046989721,Under review as a conference paper at ICLR 2022
"EX
H",0.6343612334801763,Lemma I.1. Suppose f(u) = 1
"EX
H",0.6358296622613803,2 ∥T u −b∥2 where u is a real matrix. Then f is convex w.r.t. u.
"EX
H",0.6372980910425844,"Lem. I.1 is easily obtained since the Hessian f ′′(T ) = T ⊤T is positive semideﬁnite. In the following,
we demonstrate and justify the convexity of the QPs of both layers by rewriting their single-sample
objectives into the formulation of f and summing them without loss of convexity.
Theorem I.2. QP: Eq. 4, and QP: Eq. 7 are convex optimization."
"EX
H",0.6387665198237885,"Proof. First of all, constraints of both QPs are trivially linear and convex. Thus, we only need to
justify the convexity of the two empirical objectives, ˆGNPE
1
and ˆGNPE
2
(see Eq. 7 and 4). Consider
the single-sample version of ˆGNPE
1
, namely gNPE
1
(A, φ; x, h) = 1"
"EX
H",0.6402349486049926,"2 ∥φ + Ax −h∥2, which, in the
formulation of f, can be rewritten with T =  "
"EX
H",0.6417033773861968,"x⊤
1
x⊤
1
...
...
x⊤
1 "
"EX
H",0.6431718061674009,", u = "
"EX
H",0.644640234948605,
"EX
H",0.6461086637298091,"A⊤
1,:
A⊤
2,:
...
A⊤
d,:
φ "
"EX
H",0.6475770925110133,"
, b = h
(21)"
"EX
H",0.6490455212922174,"which guarantees the convexity of gNPE
1
w.r.t. A and φ by Lem. I.1. For gNPE
2
(C, ξ; x, y) =
1
2 ∥ξ + x −Cy∥2, we have T =  "
"EX
H",0.6505139500734214,"−y⊤
1
−y⊤
1
...
...
−y⊤
1 "
"EX
H",0.6519823788546255,", u = "
"EX
H",0.6534508076358296,
"EX
H",0.6549192364170338,"C⊤
1,:
C⊤
2,:
...
C⊤
m,:
ξ "
"EX
H",0.6563876651982379,"
, b = −x
(22)"
"EX
H",0.657856093979442,"which guarantees the convexity of gNPE
2
w.r.t. C and ξ by Lem. I.1. Now we consider the summation.
Taking layer 1 as an example, by deﬁnition we have
ˆGNPE
1
(A, Φ) =
X"
"EX
H",0.6593245227606461,"i∈[n]
gNPE
1
(A, φ(i); x(i), h(i)).
(23)"
"EX
H",0.6607929515418502,"For each i ∈[n], equivalently, we take Φ as a variable instead of φ(i) in gNPE
1
, but with only φ(i) ∈Φ
determining the value of gNPE
1
. In this sense, gNPE
1
is convex w.r.t. A and Φ for each i ∈[n]. Thus,
the sum ˆGNPE
1
is convex w.r.t. A and Φ. Similarly, ˆGNPE
2
is convex w.r.t. C and Ξ."
"EX
H",0.6622613803230544,"J
STRONG CONSISTENCY"
"EX
H",0.6637298091042585,"In this appendix, we justify the strong consistency of our estimators for the residual unit layer 1/2
learning and the whole network."
"EX
H",0.6651982378854625,"According to Thm. G.1 and Thm. 5.1, the solutions to our objective functionals are continuous sets.
In addition, there are theoretical intermediate results that are also represented as continuous sets,
e.g. possible left-inverse matrices for B∗in the results for layer 2. Thus, to analyze the consistency
of our learning algorithm, we deﬁne distances between sets, so that the convergence of sets can be
well deﬁned. Further point convergence results, i.e. layer 1/2 estimator strong consistency, are based
on the set convergence we deﬁne.
Deﬁnition J.1 (deviation). Let A ⊆M and B ⊆M be two non-empty sets from a metric space
(M, d). The deviation of set A from the set B, denoted by D(A, B), is
D(A, B) = sup
a∈A
d(a, B) = sup
a∈A
inf
b∈B"
"EX
H",0.6666666666666666,"d(a, b),
(24)"
"EX
H",0.6681350954478708,"where sup and inf represent supremum and inﬁmum, respectively.
Deﬁnition J.2 (Hausdorff distance). Let A ⊆M and B ⊆M be two non-empty sets from a metric
space (M, d). The Hausdorff distance between A and B, denoted by DH(A, B), is
DH(A, B) = max{D(A, B), D(B, A)}.
(25)
Remark. In the following, we use the Frobenius norm to deﬁne the distance between matrices,
i.e. d(X, Y ) = ∥X −Y ∥F."
"EX
H",0.6696035242290749,Under review as a conference paper at ICLR 2022
"EX
H",0.671071953010279,"J.1
LAYER 2"
"EX
H",0.6725403817914831,"In this subsection, we prove the strong consistency of the layer 2 estimator."
"EX
H",0.6740088105726872,"J.1.1
OBJECTIVE MINIMIZER SPACE ESTIMATOR"
"EX
H",0.6754772393538914,"For simplicity of notation, we use ˆSn to denote our layer 2 QP/LP solution space by n random
samples8 as a random set"
"EX
H",0.6769456681350955,"ˆSn := {C ∈Rd×m : Cy(i) −x(i) ≥0, ∀i ∈[n]}
(26)"
"EX
H",0.6784140969162996,and S∗to denote the value space of C in Lem. 4.2 which minimizes layer 2 objective functional
"EX
H",0.6798825256975036,"S∗:= {C ∈Rd×m : Cy −x ≥0, ∀x ∈Rd and its corresponding output y}.
(27)"
"EX
H",0.6813509544787077,"For further use, we name ˆPn as the set of n sampled inputs which deﬁne ˆSn, i.e. ˆPn := {x(i)}i∈[n]
where each x(i) is the same random variable in Eq. 26. By the deﬁnitions above, we describe the
strong consistency of our QP/LP as Lem. J.1."
"EX
H",0.6828193832599119,"Lemma J.1 (QP/LP strong consistency, layer 2). DH(ˆSn, S∗)
a.s.
−−→0 as n →∞."
"EX
H",0.684287812041116,"Proof. First we prove that DH(ˆSn, S∗)
p−→0 as n →∞. Recall Thm. G.1, Cy −x ≥0 only if
CB∗= diag(k). We inherit the notation as deﬁning D = CB∗. Theorem G.1 is based on Lem. H.1,
and we prove them by raising points that show contradiction, i.e. violate the inequality that holds in a
pointwise fashion:"
"EX
H",0.6857562408223201,"a) In the proof of Lem. H.1, we use d points: −e(i), for i ∈[d], to make
Pd
k=1 Di,k"
"EX
H",0.6872246696035242,"
A∗
k,: · x
+
+ xk"
"EX
H",0.6886930983847284,"
< xi, so that Di,j (i ̸= j) cannot be positive; and another"
"EX
H",0.6901615271659325,"d points: (A∗)−1e(i), to show Pd
k=1 Di,k
A∗
k,: · x
 < 0, so that Di,j (i ̸= j) cannot"
"EX
H",0.6916299559471366,"be negative. For each −e(i) we use here, since the violations follow strict inequalities,
we know there exists a neighborhood of −e(i), Ni = N(−e(i)), such that ∀z ∈Ni,
Pd
k=1 Di,k"
"EX
H",0.6930983847283406,"
A∗
k,: · z
+
+ zk"
"EX
H",0.6945668135095447,"
< zi. We can similarly ﬁnd such neighborhood of each"
"EX
H",0.6960352422907489,"(A∗)−1e(i) that the strict inequality holds within the neighborhood respectively. We index
them as Nd+1 to N2d.
b) In the proof of Thm. G.1, we further construct d points: for each i ∈[d], we take a point x
such that xi > 0 ∧xj ≤−
A∗
i,i
A∗
i,j xi ∧xk ≤0, where k ̸= i, j. This counterexample shows"
"EX
H",0.697503671071953,"[Cy −x]i < 0, and eliminates the possibility of 0 < Di,i < 1 when A∗
i,: is not a scale
transformation. We can similarly ﬁnd neighborhood of each point and index them as N2d+1
to N3d. Note that we omit some cases in the proof of Thm. G.1, because the ﬁrst 2d points
are sufﬁcient to use in those cases to show contradiction."
"EX
H",0.6989720998531571,"In the sampling procedure, if we sample at least one point in each neighborhood Ni, Thm. G.1
assures the solution we get ˆCn would lie in the true optimal set S∗. The probability that the sampling
procedure “omits” any of the neighborhoods is"
"EX
H",0.7004405286343612,"P

ˆPn
\
N1 = ∅or ˆPn
\
N2 = ∅or . . . or ˆPn
\
N3d = ∅

≤"
"D
X",0.7019089574155654,"3d
X"
"D
X",0.7033773861967695,"i=1
P

ˆPn
\
Ni = ∅
"
"D
X",0.7048458149779736,"≤3d[1 −min
i∈[3d] P(Ni)]n
(28)"
"D
X",0.7063142437591777,"Since the measure on each neighborhood P(Ni) =
R"
"D
X",0.7077826725403817,"x∈Ni p(x) > 0,"
"D
X",0.7092511013215859,"P

ˆPn
\
N1 = ∅or ˆPn
\
N2 = ∅or . . . or ˆPn
\
N3d = ∅

→0, as n →∞.
(29)"
"D
X",0.71071953010279,"8Here, we take samples as random variables for empirical analysis."
"D
X",0.7121879588839941,Under review as a conference paper at ICLR 2022
"D
X",0.7136563876651982,"Here we obtain DH(ˆSn, S∗)
p−→0 as n →∞. Now we take the inﬁnite sum over the both sides of
Eq. 28
X"
"D
X",0.7151248164464024,"n∈[∞]
P

ˆPn
\
N1 = ∅or ˆPn
\
N2 = ∅or . . . or ˆPn
\
N3d = ∅
 ≤3d
X n∈[∞]"
"D
X",0.7165932452276065,"
1 −min
i∈[3d] P(Ni)
n
= 3d

1
mini∈[3d] P(Ni) −1

< +∞."
"D
X",0.7180616740088106,"By the Borel-Cantelli lemma (Borel, 1909), DH(ˆSn, S∗)
a.s.
−−→0 as n →∞."
"D
X",0.7195301027900147,"J.1.2
SCALE FACTOR ESTIMATOR"
"D
X",0.7209985315712188,"To avoid ambiguity, we use nsf to denote the number of samples used in Alg. 4. The samples pairs are
{(x(i), y(i))}nsf
i=1. Without loss of generality, the following discussion focuses on some ﬁxed index
j ∈[d]. In Alg. 4, we plug in our estimator ˆCn and use LR to estimate kj given that x(i)
j
< 0"
"D
X",0.7224669603524229,"knsf( ˆCn) = arg min
k"
"D
X",0.723935389133627,"1
2nsf X"
"D
X",0.7254038179148311,i∈[nsf]
"D
X",0.7268722466960352,"[ ˆCny(i)]j −kx(i)
j

2
= P"
"D
X",0.7283406754772394,"i∈[nsf] x(i)
j [ ˆCny(i)]j
P"
"D
X",0.7298091042584435,"i∈[nsf]

x(i)
j
2
.
(30)"
"D
X",0.7312775330396476,"We ﬁrst give the strong consistency of layer 2 scale factor estimator for as nsf →∞, as described in
Lem. J.2.
Lemma J.2 (scale factor estimator strong consistency, layer 2). Suppose A∗is a scale transformation
w.r.t. the j-th component, and knsf( ˆCn) is the nsf-sample estimator of kj via LR: Eq. 30 given that
x(i)
j
< 0. Deﬁne sets"
"D
X",0.7327459618208517,"Unsf,n := {knsf(C) : C ∈ˆSn}, and U∗:="
"D
X",0.7342143906020558,"""
1
1 + A∗
j,j
, 1 #"
"D
X",0.73568281938326,".
(31)"
"D
X",0.737151248164464,"Then lim
nsf→∞lim
n→∞DH(Unsf,n, U∗)
a.s.
== 0."
"D
X",0.7386196769456681,"Proof. Following our notation, Thm. G.1 and Thm. G.2 ensure that if A∗is a scale transformation
w.r.t. the j-th component, for any C belonging to the true optimal set S∗, knsf(C) ∈
h
1
1+A∗
j,j , 1
i
."
"D
X",0.7400881057268722,"And the “iff” statement strengthens that U∗= {knsf(C) : C ∈S∗} for any nsf ∈Z+. Note that since
S∗⊂ˆSn, we have U∗⊂Unsf,n. We only need to prove D(Unsf,n, U∗)
a.s.
−−→0 as n →∞."
"D
X",0.7415565345080763,"∀ˆCn ∈ˆSn and ∀C ∈S∗,"
"D
X",0.7430249632892805,knsf( ˆCn) −knsf(C)
"D
X",0.7444933920704846,"=
1
P
i∈[nsf]

x(i)
j
2  X"
"D
X",0.7459618208516887,"i∈[nsf]
x(i)
j"
"D
X",0.7474302496328928,"
(e(j))⊤
ˆCn −C

B∗

A∗x(i)+
+ x(i)
 ≤
1
P"
"D
X",0.748898678414097,"i∈[nsf]

x(i)
j
2  X"
"D
X",0.750367107195301,i∈[nsf]
"D
X",0.7518355359765051,"x(i)
j

(e(j))⊤
ˆCn −C

B∗
2"
"D
X",0.7533039647577092,"A∗x(i)
2 +
x(i)
2 
  ≤
1
P"
"D
X",0.7547723935389133,"i∈[nsf]

x(i)
j
2  X"
"D
X",0.7562408223201175,i∈[nsf]
"D
X",0.7577092511013216,"x(i)
j

 ˆCn −C

F ∥B∗∥F (∥A∗∥F + 1)
x(i)
2   = P"
"D
X",0.7591776798825257,"i∈[nsf]
hx(i)
j
 ∥B∗∥F (∥A∗∥F + 1)
x(i)
2 i P"
"D
X",0.7606461086637298,"i∈[nsf]

x(i)
j
2
 ˆCn −C

F ."
"D
X",0.762114537444934,Under review as a conference paper at ICLR 2022 Then
"D
X",0.7635829662261381,"sup
ˆCn∈ˆSn
inf
C∈S∗"
"D
X",0.7650513950073421,"knsf( ˆCn) −knsf(C)

(32) ≤ P"
"D
X",0.7665198237885462,"i∈[nsf]
hx(i)
j
 ∥B∗∥F (∥A∗∥F + 1)
x(i)
2 i P"
"D
X",0.7679882525697503,"i∈[nsf]

x(i)
j
2
sup
ˆCn∈ˆSn
inf
C∈S∗"
"D
X",0.7694566813509545,"ˆCn −C

F
(33)"
"D
X",0.7709251101321586,which implies
"D
X",0.7723935389133627,"D(Unsf,n, U) ≤ P"
"D
X",0.7738619676945668,"i∈[nsf]
hx(i)
j
 ∥B∗∥F (∥A∗∥F + 1)
x(i)
2 i P"
"D
X",0.775330396475771,"i∈[nsf]

x(i)
j
2
D(ˆSn, S∗).
(34)"
"D
X",0.7767988252569751,Take the nsf →∞limit over both sides of Eq. 34. With the strong law of large numbers9 we have
"D
X",0.7782672540381792,"lim
nsf→∞D(Unsf,n, U) ≤
E
hx(i)
j
 ∥B∗∥F (∥A∗∥F + 1)
x(i)
2 i"
"D
X",0.7797356828193832,"E
h
x(i)
j
i2
D(ˆSn, S∗), w.p. 1.
(35)"
"D
X",0.7812041116005873,"Since D(ˆSn, S∗)
a.s.
−−→0 as n →∞, we have D(Unsf,n, U∗)
a.s.
−−→0 as n →∞then nsf →∞."
"D
X",0.7826725403817915,"J.1.3
LAYER 2 WEIGHTS ESTIMATOR"
"D
X",0.7841409691629956,"In Alg. 5, we solve B via LR. Let ˆz(i) = diag−1(ˆk) · ˆCny(i) ∈Rd, where ˆk is obtained through
Alg. 4 with input ˆCn. Assume we are using sample size of nw to do the LR. The optimization
problem is min
B X"
"D
X",0.7856093979441997,i∈[nw]
"D
X",0.7870778267254038,"y(i) −Bˆz(i)
2
(36)"
"D
X",0.788546255506608,"Now we present the strong consistency of layer 2 estimator, as described in Thm. J.3."
"D
X",0.7900146842878121,"Theorem J.3 (strong consistency, layer 2). Suppose ˆBnsf is the solution to Eq. 36. Then ˆBnw
a.s.
−−→B∗
as n, nsf, nw →∞."
"D
X",0.7914831130690162,"Proof. Let β denote vec B (ﬂattening B into a vector), then Bˆz(i) =
 ˆz(i)⊤⊗Im. Here the
operation ⊗denotes the Kronecker product. Then we can deﬁne an equivalent optimization problem"
"D
X",0.7929515418502202,"min
β
1
2nw X"
"D
X",0.7944199706314243,i∈[nw]
"D
X",0.7958883994126285,"y(i) −

ˆz(i)⊤
⊗Im 
β"
"D
X",0.7973568281938326,"2
.
(37)"
"D
X",0.7988252569750367,"Take the derivatives of β, we obtain −2
X"
"D
X",0.8002936857562408,i∈[nw]
"D
X",0.801762114537445,"""
ˆz(i)⊤
⊗Im"
"D
X",0.8032305433186491,"⊤
y(i) −

ˆz(i)⊤
⊗Im"
"D
X",0.8046989720998532,"
β
#"
"D
X",0.8061674008810573,"= 0
(38)"
"D
X",0.8076358296622613,"9Here we assume that the Kolmogorov’s strong law assumption on moments (Sen & Singer, 1994) is met as
is commonly done in empirical analysis."
"D
X",0.8091042584434655,Under review as a conference paper at ICLR 2022
"D
X",0.8105726872246696,Then the optimal solution ˆβnw of this optimization can be written in closed form:
"D
X",0.8120411160058737,ˆβnw =  X
"D
X",0.8135095447870778,i∈[nw]
"D
X",0.8149779735682819,"
ˆz(i)⊤
⊗Im"
"D
X",0.8164464023494861,"⊤
ˆz(i)⊤
⊗Im   −1  X"
"D
X",0.8179148311306902,i∈[nw]
"D
X",0.8193832599118943,"
ˆz(i)⊤
⊗Im"
"D
X",0.8208516886930984,"⊤
y(i)   =  X"
"D
X",0.8223201174743024,i∈[nw]
"D
X",0.8237885462555066,"h
ˆz(i) ⊗Im
i 
ˆz(i)⊤
⊗Im   −1  X"
"D
X",0.8252569750367107,i∈[nw]
"D
X",0.8267254038179148,"h
ˆz(i) ⊗Im
i
y(i)   =  X"
"D
X",0.8281938325991189,i∈[nw]
"D
X",0.8296622613803231,"
ˆz(i) 
ˆz(i)⊤
⊗Im   −1    X"
"D
X",0.8311306901615272,"i∈[nw]
ˆz(i) ⊗Im  y(i)   =    X"
"D
X",0.8325991189427313,"i∈[nw]
ˆz(i) 
ˆz(i)⊤
 ⊗Im   −1    X"
"D
X",0.8340675477239354,"i∈[nw]
ˆz(i) ⊗Im  y(i)  "
"D
X",0.8355359765051396,"We inherit the notation from the last two subsections. By Lem. J.1, d( ˆCn, S∗)
a.s.
−−→0 as n →∞.
Thus ∀ε > 0, ∃N such that ∀n ≥N, d( ˆCn, S∗) ≤ε w.p. 1, i.e. ∃Cn ∈S∗s.t. d( ˆCn, Cn) ≤ε
w.p. 1. Then by Lem. J.2, ∃K > 0, for ε that is small enough, ∃Nsf such that ∀nsf ≥Nsf, we
have
knsf( ˆCn) −knsf(Cn)
 ≤Kε w.p.1. For simplicity, we omit the under-script nsf of knsf in the
following discussion. In fact,"
"D
X",0.8370044052863436,"ˆz(i)
j
−z(i)
j
 = 1"
"D
X",0.8384728340675477,k( ˆCn)
"D
X",0.8399412628487518,"
e(i)⊤ˆCny(i) −
1
k(Cn)"
"D
X",0.8414096916299559,"
e(i)⊤
Cny(i) = 1"
"D
X",0.8428781204111601,k( ˆCn)k(Cn)
"D
X",0.8443465491923642,"
k(Cn)

e(i)⊤ˆCn −k( ˆCn)

e(i)⊤
Cn"
"D
X",0.8458149779735683,"
y(i) ≤ 1"
"D
X",0.8472834067547724,k( ˆCn)k(Cn) 
"D
X",0.8487518355359766,"k(Cn)

e(i)⊤ˆCny(i) −k(Cn)

e(i)⊤
Cny(i) + 1"
"D
X",0.8502202643171806,k( ˆCn)k(Cn) 
"D
X",0.8516886930983847,"k(Cn)

e(i)⊤
Cny(i) −k( ˆCn)

e(i)⊤
Cny(i) ≤"
"D
X",0.8531571218795888," 
1 + A∗
j,j
2"
"D
X",0.8546255506607929,"1 −Kε
 
1 + A∗
j,j

y(i)
 ˆCn −Cn

F +
k( ˆCn) −k(Cn)
 ∥CnB∗∥F (∥A∗∥F + 1)
x(i)
 ≤"
"D
X",0.856093979441997," 
1 + A∗
j,j
2"
"D
X",0.8575624082232012,"1 −Kε
 
1 + A∗
j,j

y(i) + dK (∥A∗∥F + 1)
x(i)

ε
(39)"
"D
X",0.8590308370044053,"Then
ˆz(i) 
ˆz(i)⊤
−z(i) 
z(i)⊤
F"
"D
X",0.8604992657856094,"=
ˆz(i) 
ˆz(i)⊤
−z(i) 
ˆz(i)⊤
+ z(i) 
ˆz(i)⊤
−z(i) 
z(i)⊤
F"
"D
X",0.8619676945668135,"≤

h
ˆz(i) −z(i)i 
ˆz(i)⊤
F
+
z(i)

ˆz(i)⊤
−

z(i)⊤
F"
"D
X",0.8634361233480177,"≤

h
ˆz(i) −z(i)i h
ˆz(i) −z(i)i⊤
F
+ 2
z(i)

ˆz(i)⊤
−

z(i)⊤
F ≤
X j∈[d]"
"D
X",0.8649045521292217,"
ˆz(i)
j
−z(i)
j
2
+ 2
z(i)
ˆz(i) −z(i)"
"D
X",0.8663729809104258,Under review as a conference paper at ICLR 2022
"D
X",0.8678414096916299,"It follows that
ˆz(i)  ˆz(i)⊤−z(i)  
z(i)⊤
F is also bounded by O(ε). With similar techniques we
can prove X"
"D
X",0.869309838472834,"i∈[nw]
ˆz(i) 
ˆz(i)⊤
−
X"
"D
X",0.8707782672540382,"i∈[nw]
z(i) 
z(i)⊤

F"
"D
X",0.8722466960352423,"≤O(ε)
(40)"
"D
X",0.8737151248164464,"and
ˆz(i) ⊗Im −z(i) ⊗Im

F ≤O(ε)
(41)"
"D
X",0.8751835535976505,"Denote
hP"
"D
X",0.8766519823788547,"i∈[nw] z(i)  
z(i)⊤i
as P and P"
"D
X",0.8781204111600588,i∈[nw] z(i) as Q. Substitute z(i) with ˆz(i) in the above
"D
X",0.8795888399412628,"expression we have ˆP and ˆQ. Hence,
 ˆβnw −β∗
F =

h
ˆP ⊗Im
i−1 h
ˆQ ⊗Im
i
y(i)
−[P ⊗Im]−1 
[Q ⊗Im] y(i)
F"
"D
X",0.8810572687224669,"≤

h
ˆP ⊗Im
i−1 h
ˆQ ⊗Im
i
y(i)
−
h
ˆP ⊗Im
i−1 
[Q ⊗Im] y(i)
F"
"D
X",0.882525697503671,"+

h
ˆP ⊗Im
i−1 
[Q ⊗Im] y(i)
−[P ⊗Im]−1 
[Q ⊗Im] y(i)
F"
"D
X",0.8839941262848752,"=

h
ˆP ⊗Im
i−1
F"
"D
X",0.8854625550660793,"h
ˆQ −Q

⊗Im
i
y(i)
F +

h
ˆP ⊗Im
i−1
−[P ⊗Im]−1

F
[Q ⊗Im] y(i)
F
In the ﬁrst part, by triangle inequality,

h
ˆP ⊗Im
i−1
F
≤

h
ˆP ⊗Im
i−1
−[P ⊗Im]−1

F
+
[P ⊗Im]−1
F
(42)"
"D
X",0.8869309838472834,"So we only need to prove

h
ˆP ⊗Im
i−1
−[P ⊗Im]−1

F
≤O(ε) to claim
 ˆβnw −β∗
F ≤O(ε)."
"D
X",0.8883994126284875,"Denote ˆP −P = ∆P . From Eq. 39, we know every entry of ∆P ⊗Im can be bounded by O(ε).
By simple calculation we have"
"D
X",0.8898678414096917,"(P + ∆P )−1 = P −1 −P −1∆P P −1 + O(ε2).
(43)"
"D
X",0.8913362701908958,"Then we have
P −1 −ˆP −1 = P −1∆P P −1 + O(ε2) = O(ε).
(44)"
"D
X",0.8928046989720999,"J.2
LAYER 1"
"D
X",0.8942731277533039,"In this subsection, we justify the strong consistency of layer 1 objective functional minimizer estimator
in detail, i.e. the layer 1 QP/LP solution space. We will omit the detailed proof of Alg. 2 line 4 to 7
strong consistency since it is similar to the proof of Lem. J.2. Additionally, we also omit the strong
consistency of the x 7→(A∗x)+ function estimator because it can be directly obtained by Lem. J.1
and J.2 and the continuous mapping theorem."
"D
X",0.895741556534508,"We use a new optimization problem equivalent to the optimization of G1. Before that, we ﬁrst deﬁne
the equivalence between two optimization problems as follows.
Deﬁnition J.3. Let opt1 and opt2 be two optimization problems, with f1, f2 as the respective
objective functions. Then opt1 and opt2 are said to be equivalent if given a feasible solution
to opt1, namely x1, a feasible solution to opt2 is uniquely corresponded, namely x2, such that
f1(x1) = f2(x2), and vice versa."
"D
X",0.8972099853157122,"The new optimization problem and its equivalence to the optimization of G1 is described in Lem. J.4.
Lemma J.4. The optimization of G1 (Eq. 6) is equivalent to"
"D
X",0.8986784140969163,"min
A f(A) = 1"
EX,0.9001468428781204,2Ex
EX,0.9016152716593245,"(Ax −h)+
2
.
(45)"
EX,0.9030837004405287,Under review as a conference paper at ICLR 2022
EX,0.9045521292217328,"Proof. To see this, suppose A1 is one optimal solution to Eq. 45, then we can construct r1(x) =
(A1x −h)+ so that G1 (A1, r1) = f(A1) and the optimality implies"
EX,0.9060205580029369,"min
A, r G1 (A, r) ≤min
A f(A)."
EX,0.9074889867841409,"On the other hand, suppose (A2, r2) is an optimum of G1. Let r3(x) = (h −A2x)+, then ∀x ∈Rd
and h be the corresponding hidden output, if [h −A2x]j ≥0, then [r3(x) + A2x −h]j = 0,
otherwise [r3(x) + A2x −h]2
j = [A2x −h]2 ≤[h2 (x) + A2x −h]2
j since h2 is nonnegative. So
that we know that"
EX,0.908957415565345,"min
A, r G1(A, r) = G1(A2, r2) = G1(A2, r3) = f(A2)"
EX,0.9104258443465492,"From the optimality, we further have"
EX,0.9118942731277533,"min
A, r G1(A, r) ≥min
A f(A)"
EX,0.9133627019089574,"From the simple calculation above, we can see that one optimal solution to Eq. 45 has a one-to-one
correspondence to an optimal solution to G1."
EX,0.9148311306901615,"Similarly, the empirical version of the two problems are equivalent, which indicates their consistency
in the empirical estimation being equivalent. In the following, we justify the strong consistency of
empirical Eq. 45 instead of G1"
EX,0.9162995594713657,"min
A
ˆfn (A) = 1"
N,0.9177679882525698,2n X i∈[n]
N,0.9192364170337739,"
Ax(i) −h(i)+"
N,0.920704845814978,"2
.
(46)"
N,0.922173274596182,"Denote T∗:= {diag(k) · A∗| 0 ≤kj ≤1, j ∈[d]} as the true optimal solution set, and ˆTn as
the optimal solution set corresponding to the n-sample problem. In the following, we justify four
conditions in a row that hold for f to derive the strong consistency of its optimal solution estimator."
N,0.9236417033773862,"Lemma J.5. Let ˆTn′ be the layer 1 QP/LP solution space by n′ samples. Then there exists a compact
set C determined by A∗, namely C(A∗), s.t. ˆTn′ ⊂C(A∗) w.p. 1 as n →∞."
N,0.9251101321585903,"Proof. ∀l ∈[d], let a∗be the l-th row of A∗, and ˆal be the l-th row of ˆAn′. We’d like ﬁrst to prove
that the set
ˆTl
n′ = {al : al is the l-th row of A, where A ∈ˆTn′}
(47)
is compact w.p.1."
N,0.9265785609397944,"Suppose n′ > d, and among the n′ samples, we classify them into two folds. To avoid ambiguity, let
u(i) be the points such that (a∗)⊤u(i) > 0, i ∈[q]; and v(j) be the points such that (a∗)⊤v(j) < 0,
j ∈[n −q]. From the analysis of Thm. 5.1, we have (a∗)⊤u(i) ≥ˆa⊤u(i), ∀i ∈[q] and ˆa⊤v(j) ≤0,
∀j ∈[n −q]. It follows that we can rewrite ˆTl
n′ as a polyhedron"
N,0.9280469897209985,"ˆTl
n′ = {a ∈Rd : a⊤u(i) ≤(a∗)⊤u(i), a⊤v(j) ≤0}
(48)"
N,0.9295154185022027,"We are going to show the polyhedron ˆTl
n′ is bounded by contradiction. If it is not bounded, then
∃d ∈Rd, d ̸= 0 and ˜a ∈ˆTl
n′, such that ∀λ > 0, ˜a + λd ∈ˆTl
n′. Then"
N,0.9309838472834068,(˜a + λd)⊤u(i) = ˜a⊤u(i) +λd⊤u(i) ≤(a∗)⊤u(i) ⇐⇒λd⊤u(i) ≤(a∗)⊤u(i) −˜a⊤u(i) (49)
N,0.9324522760646109,"similarly,
(˜a + λd)⊤v(i) = ˜a⊤v(j) + λd⊤v(j) ≤0
(50)"
N,0.933920704845815,"From the deﬁnition, λ can be arbitrarily big, then d⊤u(i) ≤0, ∀i ∈[q], and d⊤v(j) ≤0, ∀j ∈[n−q]."
N,0.9353891336270191,"Since we know span{u(i)} = Rd w.p. 1, then there ∃some i∗such that d⊤u(i∗) < 0, w.p. 1.
(Otherwise if d⊤u(i) = 0 for all i ∈[q], then either span{u(i)} ̸= Rd or d = 0.) Under our
assumption that, ˆTl
n′ is not bounded, we know the following system (w.r.t x) has a feasible solution
w.p. 1

−U
−V"
N,0.9368575624082232,"
x ≥0, x⊤u(i∗) < 0
(I)"
N,0.9383259911894273,Under review as a conference paper at ICLR 2022
N,0.9397944199706314,"where every row of U and V is
 
u(i)⊤and
 
v(j)⊤respectively. By Farkas’ lemma (Farkas, 1902),
the system
[−U ⊤, −V ⊤] · x = u(i∗), x ≥0
(II)"
N,0.9412628487518355,"is not feasible (w.p. 1). We claim that u(i∗) lies in the conic hull of −v(j)’s w.p. 1. So that the second
system actually has a feasible solution and thus it raises the contradiction."
N,0.9427312775330396,"Denote the conic hull as H = 
"
N,0.9441997063142438,"t ∈Rd : t =
X"
N,0.9456681350954479,"j∈[n−q]
λj

−v(j)
, λj ≥0 for ∀j ∈[n −q] 
 "
N,0.947136563876652,"Now suppose u(i∗) /∈H, by the supporting hyperplane theorem (Luenberger, 1997), ∃b ∈Rd, b ̸= 0,
such that b⊤u(i∗) ≤b⊤t for ∀t ∈H. Then by deﬁnition,"
N,0.9486049926578561,"−v(j) ∈{t : b⊤t ≤b⊤u(i∗)}, for ∀j ∈[n −q]"
N,0.9500734214390602,"Denote the hyperplane J = {t : b⊤t ≤b⊤u(i∗)}, then"
N,0.9515418502202643,"P

u(i∗) /∈H

≤P

−v(j) ∈J, for ∀j ∈[n −q]

= P

−v(j) ∈J
n−q"
N,0.9530102790014684,"Since we have in this case a geometric sequence, we know its inﬁnite sum is bounded. By Borel-
Cantelli lemma (Borel, 1909), we conclude that u(i∗) ∈H w.p. 1. Then system II is feasible w.p. 1.
So that ˆTl
n′ is compact w.p. 1."
N,0.9544787077826725,"Now we prove that there exists a compact set C(A∗), s.t. ˆTn′ ⊂C(A∗) w.p. 1 as n →∞. Similarly,
we focus on the analysis of one row. As discussed above, ˆTl
n′ is compact w.p. 1. Let ˆW1
n′ be the set
of all u(i) sampled in estimating ˆTn′, and ˆW2
n′ be the set of all v(j) sampled. Now for another set
ˆTn′′, similarly deﬁne sample point sets ˆW1
n′′ and ˆW2
n′′. We claim that ∀u(i) ∈ˆW1
n′, u(i) lies in the
conic hull of ˆW1
n′′. Actually, this part of the proof is very similar to the way we prove u(i∗) ∈H
w.p. 1, so we will omit it here."
N,0.9559471365638766,"∀a ∈ˆTn′′, let x(1) and x(2) be two different points in ˆW1
n′′. Then a⊤(λ1x(1) + λ2x(2)) ≤
(a∗)⊤(λ1x(1) + λ2x(2)) for ∀λ1 ≥0 and λ2 ≥0. This simple calculation reveals a⊤u(i) ≤
(a∗)⊤u(i) for ∀u(i) ∈ˆW1
n′ (from the claim we made). This implies that ˆTn′′ ⊂C(A∗) w.p. 1 as
n →∞, too."
N,0.9574155653450808,"Lemma J.6. The minimizer space of f(A), i.e. T∗= {diag(k) · A∗| 0 ≤kj ≤1, j ∈[d]}, is
contained in C(A∗).
Lemma J.7. f(A) is ﬁnite valued and continuous on C(A∗)."
N,0.9588839941262849,Lem. J.6 and J.7 are easily obtained by the formulation of f (see Eq. 45) and Lem. J.5.
N,0.960352422907489,"Lemma J.8 (uniform a.s. convergence). ˆfn(A)
a.s.
−−→f(A) as n →∞, uniformly in A ∈C(A∗)."
N,0.9618208516886931,"Proof. Name single-sample objective g(x, A) = 1"
N,0.9632892804698973,"2
(Ax −h)+
2
. The uniform a.s. convergence
is guaranteed by the uniform law of large numbers (Jennrich, 1969):"
N,0.9647577092511013,"a) By Lem. J.5, C(A∗) is a compact set.
b) g is continuous w.r.t. A by its formulation and measurable over x at each A ∈C(A∗)."
N,0.9662261380323054,"c) In fact,"
N,0.9676945668135095,"g(x, A) = 1 2"
N,0.9691629955947136,"(Ax −h)+
2"
N,0.9706314243759178,≤∥Ax∥2 + ∥A∗x∥2
N,0.9720998531571219,≤(∥A∥F + ∥A∗∥F) ∥x∥2 .
N,0.973568281938326,"Since A ∈C(A∗) is in a compact set,"
N,0.9750367107195301,"g(x, A) ≤ """
N,0.9765051395007343,"sup
A∈C(A∗)
∥A∥F + ∥A∗∥F #"
N,0.9779735682819384,"∥x∥2 .
(51)"
N,0.9794419970631424,Under review as a conference paper at ICLR 2022
N,0.9809104258443465,Thus the dominating function exists10.
N,0.9823788546255506,"By Lem. J.5, J.6, J.7 and J.8, all of the conditions are satisﬁed in (Shapiro et al., 2014, Thm. 5.3).
Thus, we have the strong consistency of layer 1 objective optima estimator as described in Lem. J.9."
N,0.9838472834067548,"Lemma J.9 (QP/LP strong consistency, layer 1). DH(ˆTn′, T∗)
a.s.
−−→0 as n →∞."
N,0.9853157121879589,"Similar to Lem. J.2, we have the strong consistency of the layer 1 scale factor estimator as described
in Lem. J.10."
N,0.986784140969163,"Lemma J.10 (scale factor estimator strong consistency, layer 1). Let kn′
sf( ˆCn) be the n′
sf-sample"
N,0.9882525697503671,"estimator of kj via LR: Alg. 2 line 5. given that h(i)
j
> 0. Deﬁne sets"
N,0.9897209985315712,"Vn′
sf,n := {kn′
sf(A) : A ∈ˆTn′}, and V∗:= [0, 1] .
(52)"
N,0.9911894273127754,"Then lim
n′
sf→∞lim
n′→∞DH(Vn′
sf,n′, V∗)
a.s.
== 0."
N,0.9926578560939795,"Remark. In case kj = 0, suppose the algorithm ﬁnds a solution over a continuous distribution with
[0, 1] as support and the probability that it ﬁnds a solution with scale factor 0 is 0."
N,0.9941262848751835,"With Thm. 5.2 and the continuous mapping theorem, the strong consistency of layer 1 estimation is
guaranteed."
N,0.9955947136563876,"Theorem J.11 (strong consistency, layer 1). Suppose ˆAn′ is scaled by ˆkn′
sf. Then ˆAn′
a.s.
−−→A∗as
n′, n′
sf →∞."
N,0.9970631424375918,"By Thm. J.3 and J.11, Thm. 6.3 is guaranteed by the continuous mapping theorem."
N,0.9985315712187959,"10Here, we assume E[∥x∥2] < +∞as is commonly done in empirical analysis."
