Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0005841121495327102,"Graph Neural Networks (GNNs) have demonstrated strong performance in graph
1"
ABSTRACT,0.0011682242990654205,"mining tasks due to their message-passing mechanism, which is aligned with the
2"
ABSTRACT,0.0017523364485981308,"homophily assumption that adjacent nodes exhibit similar behaviors. However,
3"
ABSTRACT,0.002336448598130841,"in many real-world graphs, connected nodes may display contrasting behaviors,
4"
ABSTRACT,0.0029205607476635513,"termed as heterophilous patterns, which has attracted increased interest in het-
5"
ABSTRACT,0.0035046728971962616,"erophilous GNNs (HTGNNs). Although the message-passing mechanism seems
6"
ABSTRACT,0.0040887850467289715,"unsuitable for heterophilous graphs due to the propagation of class-irrelevant infor-
7"
ABSTRACT,0.004672897196261682,"mation, it is still widely used in many existing HTGNNs and consistently achieves
8"
ABSTRACT,0.005257009345794392,"notable success. This raises the question: why does message passing remain effec-
9"
ABSTRACT,0.005841121495327103,"tive on heterophilous graphs? To answer this question, in this paper, we revisit the
10"
ABSTRACT,0.006425233644859813,"message-passing mechanisms in heterophilous graph neural networks and refor-
11"
ABSTRACT,0.007009345794392523,"mulate them into a unified heterophilious message-passing (HTMP) mechanism.
12"
ABSTRACT,0.007593457943925234,"Based on HTMP and empirical analysis, we reveal that the success of message
13"
ABSTRACT,0.008177570093457943,"passing in existing HTGNNs is attributed to implicitly enhancing the compatibility
14"
ABSTRACT,0.008761682242990653,"matrix among classes. Moreover, we argue that the full potential of the compat-
15"
ABSTRACT,0.009345794392523364,"ibility matrix is not completely achieved due to the existence of incomplete and
16"
ABSTRACT,0.009929906542056074,"noisy semantic neighborhoods in real-world heterophilous graphs. To bridge this
17"
ABSTRACT,0.010514018691588784,"gap, we introduce a new approach named CMGNN, which operates within the
18"
ABSTRACT,0.011098130841121495,"HTMP mechanism to explicitly leverage and improve the compatibility matrix. A
19"
ABSTRACT,0.011682242990654205,"thorough evaluation involving 10 benchmark datasets and comparative analysis
20"
ABSTRACT,0.012266355140186916,"against 13 well-established baselines highlights the superior performance of the
21"
ABSTRACT,0.012850467289719626,"HTMP mechanism and CMGNN method.
22"
INTRODUCTION,0.013434579439252336,"1
Introduction
23"
INTRODUCTION,0.014018691588785047,"Graph Neural Networks (GNNs) have shown remarkable performance in graph mining tasks, such
24"
INTRODUCTION,0.014602803738317757,"as social network analysis [1, 2] and recommender systems [3, 4]. The design principle of GNNs is
25"
INTRODUCTION,0.015186915887850467,"typically based on the homophily assumption [5], which assumes that nodes are inclined to exhibit
26"
INTRODUCTION,0.015771028037383176,"behaviors similar to their neighboring nodes [6]. However, this assumption does not always hold
27"
INTRODUCTION,0.016355140186915886,"in real-world graphs, where the connected nodes demonstrate a contrasting tendency known as the
28"
INTRODUCTION,0.016939252336448597,"heterophily [7]. In response to the challenges of heterophily in graphs, heterophilous GNNs (HTGNNs)
29"
INTRODUCTION,0.017523364485981307,"have attracted considerable research interest [6, 8–10], with numerous innovative approaches being
30"
INTRODUCTION,0.018107476635514017,"introduced recently [11–24]. However, the majority of these methods continue to employ a message-
31"
INTRODUCTION,0.018691588785046728,"passing mechanism, which was not originally designed for heterophilous graphs, as they tend to
32"
INTRODUCTION,0.019275700934579438,"incorporate excessive information from disparate classes. This naturally raises a question: Why does
33"
INTRODUCTION,0.01985981308411215,"message passing remain effective on heterophilous graphs?
34"
INTRODUCTION,0.02044392523364486,"Recently, a few efforts [6] have begun to investigate this question and reveal that vanilla message
35"
INTRODUCTION,0.02102803738317757,"passing can work on heterophilous graphs under certain conditions. However, the absence of a unified
36"
INTRODUCTION,0.02161214953271028,"and comprehensive understanding of message passing within existing HTGNNs has hindered the
37"
INTRODUCTION,0.02219626168224299,"creation of innovative approaches. In this paper, we first revisit the message-passing mechanisms
38"
INTRODUCTION,0.0227803738317757,"in existing HTGNNs and reformulate them into a unified heterophilous message-passing (HTMP)
39"
INTRODUCTION,0.02336448598130841,"mechanism, which extends the definition of neighborhood in various ways and simultaneously utilizes
40"
INTRODUCTION,0.02394859813084112,"the messages of multiple neighborhoods. Specifically, HTMP consists of three major steps namely
41"
INTRODUCTION,0.02453271028037383,"aggregating messages with explicit guidance, combining messages from multiple neighborhoods, and
42"
INTRODUCTION,0.02511682242990654,"fusing intermediate representations.
43"
INTRODUCTION,0.02570093457943925,"Equipped with HTMP, we further conduct empirical analysis on real-world graphs. The results reveal
44"
INTRODUCTION,0.026285046728971962,"that the success of message passing in existing HTGNNs is attributed to implicitly enhancing the
45"
INTRODUCTION,0.026869158878504672,"compatibility matrix, which exhibits the probabilities of observing edges among nodes from different
46"
INTRODUCTION,0.027453271028037383,"classes. In particular, by increasing the distinctiveness between the rows of the compatibility matrix
47"
INTRODUCTION,0.028037383177570093,"via different strategies, the node representations of different classes become more discriminative in
48"
INTRODUCTION,0.028621495327102803,"heterophilous graphs.
49"
INTRODUCTION,0.029205607476635514,"Drawing from previous observations, we contend that nodes within real-world graphs might exhibit a
50"
INTRODUCTION,0.029789719626168224,"semantic neighborhood that only reveals a fraction of the compatibility matrix, accompanied by noise.
51"
INTRODUCTION,0.030373831775700934,"This could limit the effectiveness of enhancing the compatibility matrix and result in suboptimal
52"
INTRODUCTION,0.030957943925233645,"representations. To fill this gap, we further propose a novel Compatibility Matrix-aware Graph Neural
53"
INTRODUCTION,0.03154205607476635,"Network (CMGNN) under HTMP mechanism, which utilizes the compatibility matrix to construct
54"
INTRODUCTION,0.03212616822429906,"desired neighborhood messages as supplementary for nodes and explicitly enhances the compatibility
55"
INTRODUCTION,0.03271028037383177,"matrix by a targeted constraint. We build a benchmark to fairly evaluate CMGNN and existing
56"
INTRODUCTION,0.03329439252336448,"methods, which encompasses 13 diverse baseline methods and 10 datasets that exhibit varying
57"
INTRODUCTION,0.03387850467289719,"levels of heterophily. Extensive experimental results demonstrate the superiority of CMGNN and
58"
INTRODUCTION,0.0344626168224299,"HTMP mechanism. The contributions of this paper are summarized as:
59"
INTRODUCTION,0.035046728971962614,"• We revisit the message-passing mechanisms in existing HTGNNs and reformulate them into a
60"
INTRODUCTION,0.035630841121495324,"unified heterophilous message-passing mechanism (HTMP), which not only provides a macroscopic
61"
INTRODUCTION,0.036214953271028034,"view of message passing in HTGNNs but also enables people to develop new methods flexibly.
62"
INTRODUCTION,0.036799065420560745,"• We reveal that the effectiveness of message passing on heterophilous graphs is attributed to
63"
INTRODUCTION,0.037383177570093455,"implicitly enhancing the compatibility matrix among classes, which gives us a new perspective to
64"
INTRODUCTION,0.037967289719626166,"understand the message passing in HTGNNs.
65"
INTRODUCTION,0.038551401869158876,"• Based on HTMP mechanism and empirical analysis, we propose CMGNN to unlock the potential
66"
INTRODUCTION,0.039135514018691586,"of the compatibility matrix in HTGNNs. We further build a unified benchmark that overcomes the
67"
INTRODUCTION,0.0397196261682243,"issues of current datasets for fair evaluation1. Experiments show the superiority of CMGNN.
68"
PRELIMINARIES,0.04030373831775701,"2
Preliminaries
69"
PRELIMINARIES,0.04088785046728972,"Given a graph G = (V, E, X, A, Y), V is the node set and E is the edge set. Nodes are characterized
70"
PRELIMINARIES,0.04147196261682243,"by the feature matrix X ∈RN×df , where N = |V| denotes the number of nodes, df is the features
71"
PRELIMINARIES,0.04205607476635514,"dimension. Y ∈RN×1 is the node labels with the one-hot version C ∈RN×K, where K is
72"
PRELIMINARIES,0.04264018691588785,"the number of node classes. The neighborhood of node vi is denoted as Ni. A ∈RN×N is
73"
PRELIMINARIES,0.04322429906542056,"the adjacency matrix , and D = diag(d1, ..., dn) represents the diagonal degree matrix, where
74"
PRELIMINARIES,0.04380841121495327,di = P
PRELIMINARIES,0.04439252336448598,"j Aij. ˜A = A + I represents the adjacency matrix with self-loops. Let Z ∈RN×dr be the
75"
PRELIMINARIES,0.04497663551401869,"node representations with dimension dr learned by the models. We use 1 to represent a matrix with
76"
PRELIMINARIES,0.0455607476635514,"all elements equal to 1, and 0 for a matrix with all elements equal to 0.
77"
PRELIMINARIES,0.04614485981308411,"Homophily and Heterophily. High homophily is observed in graphs where a substantial portion of
78"
PRELIMINARIES,0.04672897196261682,"connected nodes shares identical labels, while high heterophily corresponds to the opposite situation.
79"
PRELIMINARIES,0.04731308411214953,"For measuring the homophily level, two widely used metrics are edge homophily he [12] and node
80"
PRELIMINARIES,0.04789719626168224,"homophily hn [15], defined as he = |{eu,v|eu,v∈E, Yu=Yv}|"
PRELIMINARIES,0.04848130841121495,"|E|
and hn =
1
|V|
P"
PRELIMINARIES,0.04906542056074766,"v∈V
|{u|u∈Nv, Yu=Yv}|"
PRELIMINARIES,0.04964953271028037,"dv
.
81"
PRELIMINARIES,0.05023364485981308,"Both metrics have a range of [0, 1], where higher values indicate stronger homophily and lower values
82"
PRELIMINARIES,0.05081775700934579,"indicate stronger heterophily.
83"
PRELIMINARIES,0.0514018691588785,"Vanilla Message Passing (VMP). The vanilla message-passing mechanism plays a pivotal role in
84"
PRELIMINARIES,0.051985981308411214,"transforming and updating node representations based on the neighborhood [25]. Typically, the
85"
PRELIMINARIES,0.052570093457943924,1Codebase is available at the supplementary material.
PRELIMINARIES,0.053154205607476634,"Table 1: Revisiting the message passing in representative heterophilous GNNs under the perspective
of HTMP mechanism."
PRELIMINARIES,0.053738317757009345,"Method
Neighborhood Indicators
Aggregation Guidance
COMBINE
FUSE
Type
A
Type
B"
PRELIMINARIES,0.054322429906542055,GCN [1] Raw [ ˜A]
PRELIMINARIES,0.054906542056074766,DegAvg
PRELIMINARIES,0.055490654205607476,"[ ˜Bd]
/
Z = ZL"
PRELIMINARIES,0.056074766355140186,"APPNP [26]
[I, ˜A]
[I, ˜Bd]
WeightedAdd
Z = ZL"
PRELIMINARIES,0.0566588785046729,"GCNII [27]
[I, ˜A]
[I, ˜Bd]
WeightedAdd
Z = ZL"
PRELIMINARIES,0.05724299065420561,"GAT [28]
[ ˜A]
AdaWeight
[Baw]
/
Z = ZL"
PRELIMINARIES,0.05782710280373832,"GPR-GCN [20]
[ ˜A]"
PRELIMINARIES,0.05841121495327103,DegAvg
PRELIMINARIES,0.05899532710280374,"[ ˜Bd]
/
AdaAdd"
PRELIMINARIES,0.05957943925233645,"OrderedGNN [21]
[I, A]
[I, Bd]
AdaCat
Z = ZL"
PRELIMINARIES,0.06016355140186916,"ACM-GCN [18]
[I, A, ˜A]
[I, Bd, I −Bd]
AdaAdd
Z = ZL"
PRELIMINARIES,0.06074766355140187,"FAGCN [11]
[I, A]"
PRELIMINARIES,0.06133177570093458,AdaWeight
PRELIMINARIES,0.06191588785046729,"[I, Bnaw]
WeightedAdd
Z = ZLW"
PRELIMINARIES,0.0625,"GBK-GNN [24]
[I, A, A]
[I, Baw, 1 −Baw]
Add
Z = ZL"
PRELIMINARIES,0.0630841121495327,SimP-GCN [14] ReDef
PRELIMINARIES,0.06366822429906542,"[I, ˜A, Af]"
PRELIMINARIES,0.06425233644859812,DegAvg
PRELIMINARIES,0.06483644859813084,"[I, ˜Bd, Bd
f]
AdaAdd
Z = ZL"
PRELIMINARIES,0.06542056074766354,"H2GCN [12]
[A, Ah2]
[Bd, Bd
h2]
Cat
Cat"
PRELIMINARIES,0.06600467289719626,"Geom-GCN [15]
[Ac1, ..., Acr, ..., AcR]
[Bd
c1, ..., Bd
cr, ..., Bd
cR]
Cat
Z = ZL"
PRELIMINARIES,0.06658878504672897,"MixHop [16]
[I, A, Ah2, ..., Ahk]
[I, Bd, Bd
h2, ..., Bd
hk]
Cat
Z = ZL"
PRELIMINARIES,0.06717289719626168,"UGCN [13]
[˜A, ˜Ah2, Af]"
PRELIMINARIES,0.06775700934579439,AdaWeight
PRELIMINARIES,0.0683411214953271,"[ ˜Baw, ˜Baw
h2 , Baw
f ]
AdaAdd
Z = ZL"
PRELIMINARIES,0.0689252336448598,"WRGNN [22]
[Ac1, ..., Acr, ..., AcR]
[Baw
c1 , ..., Baw
cr , ..., Baw
cR]
Add
Z = ZL"
PRELIMINARIES,0.06950934579439252,"HOG-GCN [17]
[I, Ahk]"
PRELIMINARIES,0.07009345794392523,RelaEst
PRELIMINARIES,0.07067757009345794,"[I, Bre]
WeightedAdd
Z = ZL"
PRELIMINARIES,0.07126168224299065,"GloGNN [19]
[I, 1]
[I, Bre]
WeightedAdd
Z = ZL"
PRELIMINARIES,0.07184579439252337,"GGCN [23]
Dis
[I, Ap, An]
[I, Bre
p , Bre
n ]
AdaAdd
Z = ZL"
PRELIMINARIES,0.07242990654205607,"* The correspondence between the full form and the abbreviation: Raw Neighborhood (Raw), Neighborhood Redefine (ReDef), Neighborhood
Discrimination (Dis), Degree-based Averaging (DegAvg), Adaptive Weights (AdaWeight), Relation Estimation (RelaEst), Addition (Add),
Weighted Addition (WeightAdd), Adaptive Weighted Addition (AdaAdd), Concatenation (Cat), Adaptive Dimension Concatenation (AdaCat).
* More details about the notations are available in Appendix A.1."
PRELIMINARIES,0.07301401869158879,"mechanism operates iteratively and comprises two stages:
86"
PRELIMINARIES,0.07359813084112149,"eZl = AGGREGATE(A, Zl−1),
Zl = COMBINE

Zl−1, eZl
,
(1)"
PRELIMINARIES,0.0741822429906542,"where the AGGREGATE function first aggregates the input messages Zl−1 from neighborhood A
87"
PRELIMINARIES,0.07476635514018691,"into the aggregated one eZl, and subsequently, the COMBINE function combines the messages of
88"
PRELIMINARIES,0.07535046728971963,"node ego and neighborhood aggregation, resulting in updated representations Zl.
89"
PRELIMINARIES,0.07593457943925233,"3
Revisiting Message Passing in Heterophilous GNNs.
90"
PRELIMINARIES,0.07651869158878505,"To gain a thorough and unified insight into the effectiveness of message passing in HTGNNs, we
91"
PRELIMINARIES,0.07710280373831775,"revisit message passing in various notable HTGNNs [11–24] and propose a unified heterophilous
92"
PRELIMINARIES,0.07768691588785047,"message passing (HTMP) mechanism, structured as follows:
93"
PRELIMINARIES,0.07827102803738317,"eZl
r = AGGREGATE(Ar, Br, Zl−1), Zl = COMBINE({eZl
r}R
r=1), Z = FUSE({Zl}L
l=0).
(2)"
PRELIMINARIES,0.07885514018691589,"Generally, HTMP extends the definition of neighborhood in various ways and simultaneously utilize
94"
PRELIMINARIES,0.0794392523364486,"the messages of multiple neighborhoods, which is the key for better adapting to heterophily. We
95"
PRELIMINARIES,0.08002336448598131,"use R to denote the number of neighborhoods used by the model. In each message passing layer l,
96"
PRELIMINARIES,0.08060747663551401,"HTMP separately aggregates messages within R neighborhoods and combines them. The method-
97"
PRELIMINARIES,0.08119158878504673,"ological analysis of some representative HTGNNs and more details can be seen in Appendix A.
98"
PRELIMINARIES,0.08177570093457943,"Compared to the VMP mechanism, HTMP mechanism has advances in the following functions:
99"
PRELIMINARIES,0.08235981308411215,"(i) To characterize different neigborhoods, the AGGREGATE function in HTMP includes the neigh-
100"
PRELIMINARIES,0.08294392523364486,"borhood indicator Ar to indicate the neighbors within a specific neighborhood r. The adjacency
101"
PRELIMINARIES,0.08352803738317757,"matrix A in VMP is a special neighborhood indicator that marks the neighbors in the raw neigh-
102"
PRELIMINARIES,0.08411214953271028,"borhood. To further characterize the aggregation of different neighborhoods, HTMP introduces the
103"
PRELIMINARIES,0.084696261682243,"aggregation guidence Br for each neighborhood r. In VMP, the aggregation guidance is an implicit
104"
PRELIMINARIES,0.0852803738317757,"parameter of the AGGREGATE function since it only works for the raw neighborhood. A commonly
105"
PRELIMINARIES,0.08586448598130841,"used form of the AGGREGATE function is AGGREGATE(Ar, Br, Zl−1) = (Ar ⊙Br)Zl−1Wl
r,
106"
PRELIMINARIES,0.08644859813084112,"where ⊙is the Hadamard product and Wl
r is a weight matrix for message transformation. We take
107"
PRELIMINARIES,0.08703271028037383,"this as the general form of the AGGREGATE function and only analyze the neighborhood indicators
108"
PRELIMINARIES,0.08761682242990654,"and the aggregation guidance in the following.
109"
PRELIMINARIES,0.08820093457943926,"The neighborhood indicator Ar ∈{0, 1}N×N indicates neighbors associated with central nodes
110"
PRELIMINARIES,0.08878504672897196,"within neighborhood r. To describe the multiple neighborhoods in HTGNNs, neighborhood indicators
111"
PRELIMINARIES,0.08936915887850468,"can be formed as a list A = [A1, ..., Ar, ..., AR]. For the sake of simplicity, we consider the identity
112"
PRELIMINARIES,0.08995327102803738,"matrix I ∈RN×N as a special neighborhood indicator for acquiring the nodes’ ego messages. The
113"
PRELIMINARIES,0.0905373831775701,"aggregation guidance Br ∈RN×N can be viewed as pairwise aggregation weights in most cases,
114"
PRELIMINARIES,0.0911214953271028,"which has the multiple form B = [B1, ..., Br, ..., BR]. Table 1 illustrates the connection between
115"
PRELIMINARIES,0.09170560747663552,"message passing in various HTGNNs and HTMP mechanism.
116"
PRELIMINARIES,0.09228971962616822,"(ii) Considering the existence of multiple neighborhoods, the COMBINE function in HTMP need to
117"
PRELIMINARIES,0.09287383177570094,"integrate multiple messages instead of only the ego node and the raw neighborhood. Thus, the input
118"
PRELIMINARIES,0.09345794392523364,"of the COMBINE function is a set of messages eZl
r aggregated from the corresponding neighborhoods.
119"
PRELIMINARIES,0.09404205607476636,"In HTGNNs, addition and concatenation are two common approaches, each of which has variants.
120"
PRELIMINARIES,0.09462616822429906,"An effective COMBINE function is capable of simultaneously processing messages from various
121"
PRELIMINARIES,0.09521028037383178,"neighborhoods while preserving their distinct features, thereby reducing the effects of heterophily.
122"
PRELIMINARIES,0.09579439252336448,"(iii) In VMP, the final output representations are usually the one of the final layer: Z = ZL. Some
123"
PRELIMINARIES,0.0963785046728972,"HTGNNs utilize the combination of intermediate representations to leverage messages from different
124"
PRELIMINARIES,0.0969626168224299,"localities, adapting to the heterophilous structural properties in different graphs. Thus, we introduce
125"
PRELIMINARIES,0.09754672897196262,"an additional FUSE function in HTMP which integrates multiple representations Zl of different
126"
PRELIMINARIES,0.09813084112149532,"layers l into the final Z. Similarly, the FUSE function is based on addition and concatenation.
127"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.09871495327102804,"4
Why Does Message Passing Still Remain Effective in Heterophilous
128"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.09929906542056074,"Graphs?
129"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.09988317757009346,"Based on HTMP mechanism, we further dive into the motivation behind the message passing of
130"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.10046728971962617,"existing HTGNNs. Our discussion begins by examining the difference between homophilous and
131"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.10105140186915888,"heterophilous graphs. Initially, we consider the homophily ratios he and hn, as outlined in Section 2.
132"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.10163551401869159,"However, a single number is not able to indicate enough conditions of a graph. Ma et al. [6] propose
133"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.1022196261682243,"the existence of a special case of heterophily, named ""good"" heterophily, where the VMP mechanism
134"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.102803738317757,"can achieve strong performance and the homophily ratio shows no difference. Thus, to better study
135"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.10338785046728972,"the heterophily property, here we introduce the Compatibility Matrix [7] to describe graphs:
136"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.10397196261682243,"Definition 1 Compatibility Matrix (CM): The potential connection preference among classes within
137"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.10455607476635514,"a graph. It’s formatted as a matrix M ∈RK×K, where the i-th row Mi denotes the connection
138"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.10514018691588785,"probabilities between class i and all classes. It can be estimated empirically by the statistics among
139"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.10572429906542057,"nodes as follows:
140"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.10630841121495327,"M = Norm(CT Cnb),
Cnb = ˆAC,
(3)"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.10689252336448599,"where Norm(·) denotes the L1 normalization and T is the matrix transpose operation. Cnb ∈RN×K
141"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.10747663551401869,"is the semantic neighborhoods of nodes, which indicates the proportion of neighbors from each class
142"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.1080607476635514,"in nodes’ neighborhoods.
143"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.10864485981308411,"We visualize the CM of a homophilous graph Photo [29] and a heterophilous graph Amazon-
144"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.10922897196261683,"Ratings [30] in Figure 1(a) and 1(b). The CM in Photo displays an identity-like matrix, where the
145"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.10981308411214953,"diagonal elements can be viewed as the homophily level of each class. With this type of CM, the VMP
146"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.11039719626168225,"mechanism learns representations comprised mostly of messages from same the class, while messages
147"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.11098130841121495,"of other classes are diluted. Then how does HTMP mechanism work on heterophilous graphs without
148"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.11156542056074767,"an identity-like CM? The ""good"" heterophily inspires us, which we believe corresponds to a CM with
149"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.11214953271028037,"enough discriminability among classes. We conduct experiments on synthetic graphs to confirm this
150"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.11273364485981309,"idea, with details available in Appendix C. Also, we find ""good"" heterophily in real-world graphs
151"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.1133177570093458,"though it’s not as significant as imagined. Thus, we have the following observation:
152"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.11390186915887851,"Observation 1 (Connection between CM and VMP). When enough (depends on data) discriminabil-
153"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.11448598130841121,"ity exists among classes in CM, vanilla message passing can work well in heterophilous graphs.
154"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.11507009345794393,"With this observation, we have a conjecture: Is HTMP mechanism trying to enhance the discriminabil-
155"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.11565420560747663,"ity of CM? Some special designs in HTMP intuitively meet this. For example, feature-similarity-based
156"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.11623831775700935,"neighborhood indicators and neighborhood discrimination are designed to construct neighborhoods
157"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.11682242990654206,"(a) Observed CM of
Photo"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.11740654205607477,"(b) Observed CM of
Amazon-Ratings"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.11799065420560748,"(c)
New-constructed
CM of Amazon-Ratings"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.1185747663551402,"[0, 0.5, 0.5]"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.1191588785046729,"[0, 0.7, 0.3]
[0, 0.2, 0.8]"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.11974299065420561,"[0.5, 0, 0.5]"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.12032710280373832,"[0.8, 0, 0.2]"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.12091121495327103,"[0.3, 0, 0.7]"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.12149532710280374,"[0, 0, 1] ?"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.12207943925233646,"(d) Overlap of semantic
neighborhood distribution"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.12266355140186916,Figure 1: Visualizations of the compatibility matrix and the example of distribution overlap.
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.12324766355140188,"with high homophily, that is, an identity-like CM with high discriminability. We plot the CM of
158"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.12383177570093458,"feature-similarity-based neighborhood on Amazon-Ratings in Figure 1(c) to confirm it. Moreover,
159"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.1244158878504673,"we investgate two representative methods ACM-GCN [18] and GPRGNN [20], showing that they
160"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.125,"also meet this conjecture with the posterior proof in Appendix D. ACM-GCN combines the messages
161"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.12558411214953272,"of node ego, low-frequency and high-frequency with adaptive weights, which actually motifs the
162"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.1261682242990654,"edge weights and node weights to build a new CM. GPRGNN has a FUSE function with adaptive
163"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.12675233644859812,"weights while other settings are the same as GCN. It actually integrates the CMs of multiple-order
164"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.12733644859813084,"neighborhoods with adaptive weights to form a more discriminative CM. These lead to the answer to
165"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.12792056074766356,"the aforementioned question:
166"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.12850467289719625,"Observation 2 (Connection between CM and HTMP). The unified goal of various message passing
167"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.12908878504672897,"in existing HTGNNs is to utilize and enhance the discriminability of CM on heterophilous graphs.
168"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.12967289719626168,"In other words, the success of message passing in existing HTGNNs benefits from utilizing and
169"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.1302570093457944,"enhancing the discriminability of CM.
170"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.1308411214953271,"Furthermore, we notice that the power of CM is not fully released due to the incomplete and noisy
171"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.1314252336448598,"semantic neighborhoods in real-world heterophilous graphs. We use the perspective of distribution
172"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.13200934579439252,"to describe the issue more intuitively: The semantic neighborhoods of nodes from the same class
173"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.13259345794392524,"collectively form a distribution, whose mean value indicates the connection preference of that class,
174"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.13317757009345793,"i.e. Mi for class i. Influenced by factors such as degree and randomness, the semantic neighborhood
175"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.13376168224299065,"of nodes in real-world graphs may display only a fraction of CM accompanied by noise. It can
176"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.13434579439252337,"lead to the overlap between different distributions as shown in Figure 1(d), where the existence of
177"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.13492990654205608,"overlapping parts means nodes from different classes may have the same semantic neighborhood.
178"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.13551401869158877,"This brings a great challenge since the overlapping semantic neighborhood may become redundant
179"
WHY DOES MESSAGE PASSING STILL REMAIN EFFECTIVE IN HETEROPHILOUS,0.1360981308411215,"information during message passing.
180"
METHOD,0.1366822429906542,"5
Method
181"
METHOD,0.13726635514018692,"To fill this gap, we further propose a method named Compatibility Matrix-Aware GNN (CMGNN),
182"
METHOD,0.1378504672897196,"which leverages the CM to construct desired neighborhood messages as supplementary, providing
183"
METHOD,0.13843457943925233,"valuable neighborhood information for nodes to mitigate the impact of incomplete and noisy se-
184"
METHOD,0.13901869158878505,"mantic neighborhoods. The desired neighborhood message denotes the averaging message within
185"
METHOD,0.13960280373831777,"a neighborhood when a node’s semantic neighborhoods meet the CM of the corresponding class,
186"
METHOD,0.14018691588785046,"which converts the discriminability from CM into messages. CMGNN follows the HTMP mechanism
187"
METHOD,0.14077102803738317,"and constructs a supplementary neighborhood indicator along with the corresponding aggregation
188"
METHOD,0.1413551401869159,"guidance to introduce supplementary messages. Further, CMGNN introduces a simple constraint to
189"
METHOD,0.1419392523364486,"explicitly enhance the discriminability of CM.
190"
METHOD,0.1425233644859813,"Message Passing in CMGNN.
CMGNN aggregates messages from three neighborhoods for
191"
METHOD,0.143107476635514,"each node, including the ego neighborhood, raw neighborhood, and supplementary neighborhood.
192"
METHOD,0.14369158878504673,"Following the HTMP mechanism, the message passing of CMGNN cen be described as follows:
193"
METHOD,0.14427570093457945,"eZl
r = AGGREGATE(Ar, Br, Zl−1) = (Ar ⊙Br)Zl−1Wl
r,"
METHOD,0.14485981308411214,"Zl = COMBINE({eZl
r}3
r=1) = AdaWeight({eZl
r}3
r=1),"
METHOD,0.14544392523364486,"Z = FUSE({Zl}L
l=0) =
L
∥
l=0
Zl, (4)"
METHOD,0.14602803738317757,"where AdaWeight is the adaptive weighted addition implemented by an MLP with Softmax, ∥denotes
194"
METHOD,0.1466121495327103,"the concatenation. The neighborhood indicators and aggregation guidance of the three neighborhoods
195"
METHOD,0.14719626168224298,"are formatted as follows:
196"
METHOD,0.1477803738317757,"Al
1 = I, Bl
1 = I,
Al
2 = A, Bl
2 = D−11,
Al
3 = Asup, Bl
3 = Bsup,
(5)"
METHOD,0.1483644859813084,"where Asup and Bsup are described below.
197"
METHOD,0.14894859813084113,"The supplementary neighborhood indicator Asup assigns K additional virtual neighbors for each
198"
METHOD,0.14953271028037382,"node: Asup = 1 ∈RN×K. Specifically, these additional neighbors are K virtual nodes, constructed
199"
METHOD,0.15011682242990654,"as the prototypes of classes based on the labels of the training set. The attributes Xptt ∈RK×df ,
200"
METHOD,0.15070093457943926,"neighborhoods Aptt ∈RK×N and labels Yptt ∈RK×K of prototypes are defined as follows:
201"
METHOD,0.15128504672897197,"Xptt = Norm(Ctrain
T Xtrain), Aptt = 0, Yptt = I,
(6)"
METHOD,0.15186915887850466,"where Ctrain and Xtrain are the one-hot labels and attributes of nodes in the training set. Utilizing
202"
METHOD,0.15245327102803738,"class prototypes as supplementary neighborhoods can provide each node with representative messages
203"
METHOD,0.1530373831775701,"of classes, which builds the basis for desired neighborhood messages.
204"
METHOD,0.1536214953271028,"The supplementary aggregation guidance Bsup = ˆC ˆM indicates the desired semantic neighborhood
205"
METHOD,0.1542056074766355,"of nodes, i.e. the desired proportion of neighbors from each class in nodes’ neighborhoods according
206"
METHOD,0.15478971962616822,"to the probability that nodes belong to each class. ˆM is the estimated compatibility matrix described
207"
METHOD,0.15537383177570094,"in below. Using soft logits instead of one-hot pseudo labels preserves the real characteristics of nodes
208"
METHOD,0.15595794392523366,"and reduces the impact of wrong predictions. During the message aggregation in the supplementary
209"
METHOD,0.15654205607476634,"neighborhoods, the input representations Zl−1 are replaced by the representations of virtual prototype
210"
METHOD,0.15712616822429906,"nodes Zl−1
ptt , which are obtained by the same message-passing mechanism as real nodes.
211"
METHOD,0.15771028037383178,"Similar to existing methods [18, 19], we also regard topology structure as a kind of additional
212"
METHOD,0.1582943925233645,"available node features. Thus, the input representation of the first layer can be obtained in two ways:
213"
METHOD,0.1588785046728972,"Z0 = [XWX∥ˆAWA]W0, or Z0 = XW0.
(7)"
METHOD,0.1594626168224299,"Note that in practice, we use ReLU as the activation function between layers. From the perspective of
214"
METHOD,0.16004672897196262,"HTMP mechanism, our special design is to introduce an additional neighborhood indicator Asup by
215"
METHOD,0.16063084112149534,"neighborhood redefining and aggregation guidance Bsup, which can be seen as a form of relation
216"
METHOD,0.16121495327102803,"estimation along with good interpretability. Meanwhile, these designs greatly reduce the time and
217"
METHOD,0.16179906542056074,"space cost via the N × K form.
218"
METHOD,0.16238317757009346,"Compatibility Matrix Estimation.
The CM can be directly calculated via Eq 3 with full-available
219"
METHOD,0.16296728971962618,"labels. However, the label information is not entirely available in semi-supervised settings. Thus, we
220"
METHOD,0.16355140186915887,"try to estimate the CM with the help of semi-supervised and pseudo labels. Since the pseudo labels
221"
METHOD,0.1641355140186916,"predicted by the model might be wrong, which can lead to low-quality estimation, we introduce the
222"
METHOD,0.1647196261682243,"confidence g ∈RN×1 based on the information entropy to reduce the impact of wrong predictions,
223"
METHOD,0.16530373831775702,"where a high entropy means low confidence:
224"
METHOD,0.1658878504672897,"gi = log K −H( ˆCi) ∈[0, log K],
(8)"
METHOD,0.16647196261682243,"where ˆC ∈RN×K is the soft pseudo labels composed of labels from the training set and model
225"
METHOD,0.16705607476635514,"predictions. Then the nodes’ semantic neighborhoods Cnb = Norm(A(g · ˆ
C)) ∈RN×K are
226"
METHOD,0.16764018691588786,"calculated considering the confidence.
227"
METHOD,0.16822429906542055,"Further, the degrees of nodes also influence the estimation. As we mentioned in Section 4, the
228"
METHOD,0.16880841121495327,"semantic neighborhood of low-degree nodes may display incomplete CM, leading to a significant gap
229"
METHOD,0.169392523364486,"between semantic neighborhoods and corresponding CM. Thus, they deserve low weights during the
230"
METHOD,0.1699766355140187,"estimation. We manually set up two fixed thresholds and a weighting function range in [0, 1]:
231"
METHOD,0.1705607476635514,"wd
i ="
METHOD,0.1711448598130841,"(
di/2K,
di ≤K,
0.25 + di/4K,
K < di ≤3K,
1,
otherwise.
(9)"
METHOD,0.17172897196261683,"When a node’s degree di is smaller than the number of classes K, its semantic neighborhood is
232"
METHOD,0.17231308411214954,"unlikely to display complete CM, corresponding to a low weight. And when the node degree is
233"
METHOD,0.17289719626168223,"greater than 3K, we believe it can display near-complete CM, corresponding to the maximum weight.
234"
METHOD,0.17348130841121495,"Finally, we can estimate the compatibility matrix ˆM ∈RK×K as follows:
235"
METHOD,0.17406542056074767,"ˆM = Norm((wd · g · ˆC)T )Cnb.
(10)"
METHOD,0.1746495327102804,"Table 2: Node classification accuracy comparison (%). The error bar (±) denotes the standard
deviation of results over 10 trial runs. The best and second-best results in each column are highlighted
in bold font and underlined. OOM denotes out-of-memory error during the model training."
METHOD,0.17523364485981308,"Dataset
Roman-Empire
Amazon-Ratings
Chameleon-F
Squirrel-F
Actor
Flickr
BlogCatalog
Wikics
Pubmed
Photo"
METHOD,0.1758177570093458,Avg. Rank
METHOD,0.1764018691588785,"Homo.
0.05
0.38
0.25
0.22
0.22
0.24
0.4
0.65
0.8
0.83
Nodes
22,662
24,492
890
2,223
7,600
7,575
5,196
11,701
19,717
7,650
Edges
65,854
186,100
13,584
65,718
30,019
479,476
343,486
431,206
88,651
238,162
Classes
18
5
5
5
5
9
6
10
3
8"
METHOD,0.17698598130841123,"MLP
62.29 ± 1.03
42.66 ± 0.84
38.66 ± 4.02
36.74 ± 1.80
36.70 ± 0.85
89.82 ± 0.63
93.57 ± 0.55
78.94 ± 1.22
87.48 ± 0.46
89.96 ± 1.22
11
GCN
38.58 ± 2.35
45.16 ± 0.49
42.12 ± 3.82
38.47 ± 1.82
30.11 ± 0.74
68.25 ± 2.75
78.15 ± 0.95
77.53 ± 1.41
87.70 ± 0.32
94.31 ± 0.33
10.8
GAT
59.55 ± 1.45
46.90 ± 0.47
40.89 ± 3.50
38.22 ± 1.71
30.94 ± 0.95
57.22 ± 3.04
88.36 ± 1.37
76.69 ± 0.87
87.45 ± 0.53
94.59 ± 0.48
11.4
GCNII
82.53 ± 0.37
47.53 ± 0.72
41.56 ± 4.15
40.70 ± 1.80
37.51 ± 0.92
91.64 ± 0.67
96.48 ± 0.62
84.63 ± 0.66
89.96 ± 0.43
95.18 ± 0.39
4.1
H2GCN
68.61 ± 1.05
37.20 ± 0.67
42.29 ± 4.57
35.82 ± 2.20
33.32 ± 0.90
91.25 ± 0.58
96.24 ± 0.39
78.34 ± 2.01
89.32 ± 0.37
95.66 ± 0.26
8.2
MixHop
79.16 ± 0.70
47.95 ± 0.65
44.97 ± 3.12
40.43 ± 1.40
36.97 ± 0.90
91.10 ± 0.46
96.21 ± 0.42
84.19 ± 0.61
89.42 ± 0.37
95.63 ± 0.30
4.7
GBK-GNN
66.05 ± 1.44
40.20 ± 1.96
42.01 ± 4.89
36.52 ± 1.45
35.70 ± 1.12
OOM
OOM
81.07 ± 0.83
88.18 ± 0.45
93.48 ± 0.42
10.7
GGCN
OOM
OOM
41.23 ± 4.08
36.76 ± 2.19
35.68 ± 0.87
90.84 ± 0.65
95.58 ± 0.44
84.76 ± 0.65
89.04 ± 0.40
95.18 ± 0.44
8.5
GloGNN
68.63 ± 0.63
48.62 ± 0.59
40.95 ± 5.95
36.85 ± 1.97
36.66 ± 0.81
90.47 ± 0.77
94.51 ± 0.49
82.83 ± 0.52
89.60 ± 0.34
95.09 ± 0.46
8.2
HOGGCN
OOM
OOM
43.35 ± 3.66
38.63 ± 1.95
36.47 ± 0.83
90.94 ± 0.72
94.75 ± 0.65
83.74 ± 0.69
OOM
94.79 ± 0.26
7.3
GPR-GNN
71.19 ± 0.75
46.64 ± 0.52
41.84 ± 4.68
38.04 ± 1.98
36.21 ± 0.98
91.19 ± 0.47
96.37 ± 0.44
84.07 ± 0.54
89.28 ± 0.37
95.48 ± 0.24
6.7
ACM-GCN
71.15 ± 0.73
50.64 ± 0.61
45.20 ± 4.14
40.90 ± 1.74
35.88 ± 1.40
91.43 ± 0.65
96.19 ± 0.45
84.39 ± 0.43
89.99 ± 0.40
95.52 ± 0.40
4.3
OrderedGNN
83.10 ± 0.75
51.30 ± 0.61
42.07 ± 4.24
37.75 ± 2.53
37.22 ± 0.62
91.42 ± 0.79
96.27 ± 0.73
85.50 ± 0.80
90.09 ± 0.37
95.73 ± 0.33
3.3"
METHOD,0.17757009345794392,"CMGNN
84.35 ± 1.27
52.13 ± 0.55
45.70 ± 4.92
41.89 ± 2.34
36.82 ± 0.78
92.66 ± 0.46
97.00 ± 0.52
84.50 ± 0.73
89.99 ± 0.32
95.48 ± 0.29
2.1"
METHOD,0.17815420560747663,"Objective Function.
As mentioned in Sec 4, the CMs in real-world graphs don’t always have
236"
METHOD,0.17873831775700935,"significant discriminability, which may lead to low effectiveness of supplementary messages. Thus, we
237"
METHOD,0.17932242990654207,"introduce an additional discrimination loss Ldis to reduce the similarity of the desired neighborhood
238"
METHOD,0.17990654205607476,"message among different classes, which enhances the discriminability among classes in CM. The
239"
METHOD,0.18049065420560748,"overall loss consists of a CrossEntropy loss Lce and the discrimination loss Ldis:
240"
METHOD,0.1810747663551402,"L = Lce + λLdis,
Ldis =
X"
METHOD,0.1816588785046729,"i̸=j
Sim( ˆMiZptt, ˆMjZptt),
(11)"
METHOD,0.1822429906542056,"where Zptt ∈RK×dr is the representation of virtual prototypes nodes. More details about the
241"
METHOD,0.18282710280373832,"implementation of CMGNN is available in Appendix E.
242"
BENCHMARKS AND EXPERIMENTS,0.18341121495327103,"6
Benchmarks and Experiments
243"
BENCHMARKS AND EXPERIMENTS,0.18399532710280375,"In this section, we conduct comprehensive experiments to demonstrate the effectiveness of the
244"
BENCHMARKS AND EXPERIMENTS,0.18457943925233644,"proposed CMGNN with a newly organized benchmark for fair comparisons.
245"
NEW BENCHMARK,0.18516355140186916,"6.1
New Benchmark
246"
NEW BENCHMARK,0.18574766355140188,"As reported in [30], some widely adopted datasets in existing works have critical drawbacks, which
247"
NEW BENCHMARK,0.1863317757009346,"lead to unreliable results. Therefore, with a comprehensive review of existing benchmark evaluation,
248"
NEW BENCHMARK,0.18691588785046728,"we construct a new benchmark to fairly perform experimental validation. Specifically, we integrate 13
249"
NEW BENCHMARK,0.1875,"representative homophilous and heterophilous GNNs, construct a unified codebase, and evaluate their
250"
NEW BENCHMARK,0.18808411214953272,"node classification performances on 10 unified organized datasets with various heterophily levels.
251"
NEW BENCHMARK,0.1886682242990654,"Drawbacks of Existing Datasets.
Existing works mostly follow the settings and datasets used
252"
NEW BENCHMARK,0.18925233644859812,"in [15], including 6 heterophilous datasets (Cornell, Texas, Wisconsin, Actor, Chameleon, and
253"
NEW BENCHMARK,0.18983644859813084,"Squirrel) and 3 homophilous datasets (Cora, Citeseer, and Pubmed). Platonov et al. [30] pointed out
254"
NEW BENCHMARK,0.19042056074766356,"that there are serious data leakages in Chameleon and Squirrel, while Cornell, Texas, and Wisconsin
255"
NEW BENCHMARK,0.19100467289719625,"are too small with very imbalanced classes. Further, we revisit other datasets and discover new
256"
NEW BENCHMARK,0.19158878504672897,"drawbacks: (i) In the ten splits of Citeseer, there are two inconsistent ones, which have smaller
257"
NEW BENCHMARK,0.19217289719626168,"training, validation, and test sets that could cause issues with statistical results; (ii) The data split
258"
NEW BENCHMARK,0.1927570093457944,"ratios for Cora are not consistent with the expected ones. These drawbacks may lead to certain issues
259"
NEW BENCHMARK,0.1933411214953271,"with the conclusions of previous works. The detailed descriptions of dataset drawbacks are listed in
260"
NEW BENCHMARK,0.1939252336448598,"Appendix F.1.
261"
NEW BENCHMARK,0.19450934579439252,"Newly Organized Datasets.
The datasets used in the benchmark include Roman-Empire, Amazon-
262"
NEW BENCHMARK,0.19509345794392524,"Ratings, Chameleon-F, Squirrel-F, Actor, Flickr, BlogCatalog, Wikics, Pubmed, and Photo. Their
263"
NEW BENCHMARK,0.19567757009345793,"statistics are summarized in Table 2, with details in Appendix F.2. For consistency with existing meth-
264"
NEW BENCHMARK,0.19626168224299065,"ods, we randomly construct 10 splits with predefined proportions (48%/32%/20% for train/valid/test)
265"
NEW BENCHMARK,0.19684579439252337,"for each dataset and report the mean performance and standard deviation of 10 splits.
266"
NEW BENCHMARK,0.19742990654205608,"Table 3: Ablation study results (%) between CMGNN and three ablation variants, where SM denotes
supplementary messages of the desired neighborhoods and DL denotes the discrimination loss."
NEW BENCHMARK,0.19801401869158877,"Variants
Roman-Empire
Amazon-Ratings
Chameleon-F
Squirrel-F
Actor
Flickr
BlogCatalog
Wikics
Pubmed
Photo"
NEW BENCHMARK,0.1985981308411215,"CMGNN
84.35 ± 1.27
52.13 ± 0.55
45.70 ± 4.92
41.89 ± 2.34
36.82 ± 0.78
92.66 ± 0.46
97.00 ± 0.52
84.50 ± 0.73
89.99 ± 0.32
95.48 ± 0.29"
NEW BENCHMARK,0.1991822429906542,"W/O SM
83.84 ± 1.09
51.98 ± 0.61
42.35 ± 4.21
40.79 ± 1.89
36.02 ± 1.21
92.32 ± 0.83
96.52 ± 0.63
83.97 ± 0.83
89.70 ± 0.44
95.41 ± 0.40"
NEW BENCHMARK,0.19976635514018692,"W/O DL
83.68 ± 1.24
52.04 ± 0.37
44.97 ± 3.99
41.60 ± 2.43
36.28 ± 1.12
92.66 ± 0.46
97.00 ± 0.52
83.29 ± 1.83
89.99 ± 0.32
95.26 ± 0.35"
NEW BENCHMARK,0.2003504672897196,"W/O SM and DL
83.52 ± 1.91
51.58 ± 1.04
41.12 ± 2.93
40.07 ± 2.41
35.61 ± 1.48
92.32 ± 0.83
96.52 ± 0.63
81.62 ± 1.67
89.70 ± 0.44
94.66 ± 0.42"
NEW BENCHMARK,0.20093457943925233,"Baseline Methods.
As baseline methods, we choose 13 representative homophilous and het-
267"
NEW BENCHMARK,0.20151869158878505,"erophilous GNNs, including (i) shallow base model: MLP; (ii) homophilous GNNs: GCN [1],
268"
NEW BENCHMARK,0.20210280373831777,"GAT [28], GCNII [27]; (iii) heterophilous GNNs: H2GCN [12], MixHop [16], GBK-GNN [24],
269"
NEW BENCHMARK,0.20268691588785046,"GGCN [23], GloGNN [19], HOGGCN [17], GPR-GNN [20], ACM-GCN [18] and OrderedGNN [21].
270"
NEW BENCHMARK,0.20327102803738317,"For each method, we integrate its official/reproduced code into a unified codebase and search for
271"
NEW BENCHMARK,0.2038551401869159,"parameters in the space suggested by the original papers. More experimental settings can be found in
272"
NEW BENCHMARK,0.2044392523364486,"Appendix F.4 and G.1.
273"
MAIN RESULTS,0.2050233644859813,"6.2
Main Results
274"
MAIN RESULTS,0.205607476635514,"Following the constructed benchmark, we evaluate methods and report the performance in Table 2.
275"
MAIN RESULTS,0.20619158878504673,"Performance of Baseline Methods. With the new benchmarks, some interesting observations and
276"
MAIN RESULTS,0.20677570093457945,"conclusions can be found when analyzing the performance of baseline methods. First, comparing the
277"
MAIN RESULTS,0.20735981308411214,"performance of MLP and GCN, we can find ""good"" heterophily in Amazon-Ratings, Chameleon-F,
278"
MAIN RESULTS,0.20794392523364486,"and Squirrel-F. Meanwhile, when the homophily level is not high enough, ""bad"" homophily may also
279"
MAIN RESULTS,0.20852803738317757,"exist as shown in BlogCatalog and Wikics. These results once again support the observations about
280"
MAIN RESULTS,0.2091121495327103,"CMs. Therefore, homophilous GNNs can also work well in heterophilous graphs as GCNII has
281"
MAIN RESULTS,0.20969626168224298,"an average rank of 4.1, which is better than most HTGNNs. This is attributed to the initial residual
282"
MAIN RESULTS,0.2102803738317757,"connection in GCNII actually playing the role of ego/neighbor separation, which is suitable in
283"
MAIN RESULTS,0.2108644859813084,"heterophilous graphs. As for heterophilous GNNs, they are usually designed for both homophilous
284"
MAIN RESULTS,0.21144859813084113,"and heterophilous graphs. Surprisingly, MixHop, as an early method, demonstrated quite good
285"
MAIN RESULTS,0.21203271028037382,"performance. In fact, from the perspective of HTMP, it can be considered a degenerate version
286"
MAIN RESULTS,0.21261682242990654,"of OrderedGNN with no learnable dimensions. As previous SOTA methods, OrderedGNN and
287"
MAIN RESULTS,0.21320093457943926,"ACM-GCN prove their strong capabilities again.
288"
MAIN RESULTS,0.21378504672897197,"Performance of CMGNN. CMGNN achieves the best performance in 6 datasets and an average
289"
MAIN RESULTS,0.21436915887850466,"rank of 2.1, which outperforms baseline methods. This demonstrates the superiority of utilizing
290"
MAIN RESULTS,0.21495327102803738,"and enhancing the CM to handle incomplete and noisy semantic neighborhoods, especially in
291"
MAIN RESULTS,0.2155373831775701,"heterophilous graphs. Regarding the suboptimal performance in Actor, we believe that this is due
292"
MAIN RESULTS,0.2161214953271028,"to the CM in this dataset are not discriminative enough to provide valuable information via the
293"
MAIN RESULTS,0.2167056074766355,"supplementary messages and hard to enhance. In homophilous graphs, due to the identity-like CMs,
294"
MAIN RESULTS,0.21728971962616822,"the overlap between distributions is relatively less, leading to a minor contribution from supplement
295"
MAIN RESULTS,0.21787383177570094,"messages. Yet CMGNN still achieves top-level performances.
296"
ABLATION STUDY,0.21845794392523366,"6.3
Ablation Study
297"
ABLATION STUDY,0.21904205607476634,"We conduct an ablation study on two key designs of CMGNN , including the supplementary messages
298"
ABLATION STUDY,0.21962616822429906,"of the desired neighborhood (SM) and the discrimination loss (DL). The results are shown in Table 3.
299"
ABLATION STUDY,0.22021028037383178,"First of all, both SM and DL have indispensable contributions except for Flickr, BlogCatalog, and
300"
ABLATION STUDY,0.2207943925233645,"Pubmed, in which the discrimination loss has no effect. This may be due to the discriminability of
301"
ABLATION STUDY,0.2213785046728972,"desired neighborhood messages reaching the bottlenecks and can not be further improved by DL
302"
ABLATION STUDY,0.2219626168224299,"Meanwhile, the extent of their contributions varies across datasets. SM plays a more important role in
303"
ABLATION STUDY,0.22254672897196262,"most datasets except Roman-Empire, Wikics, and Photo, in which the number of nodes that need
304"
ABLATION STUDY,0.22313084112149534,"supplementary messages is relatively small and DL has great effects. Further, we notice that with
305"
ABLATION STUDY,0.22371495327102803,"SM and DL, CMGNN can reach a smaller standard deviation most of the time. This illustrates
306"
ABLATION STUDY,0.22429906542056074,"that CMGNN achieves more stable results by handling nodes with incomplete and noisy semantic
307"
ABLATION STUDY,0.22488317757009346,"neighborhoods. As for the opposite result on Chameleon-F, this may attributed to the small size of
308"
ABLATION STUDY,0.22546728971962618,"this dataset (890 nodes), which can lead to naturally unstable results.
309"
ABLATION STUDY,0.22605140186915887,"(a) Amazon-Ratings Obs
(b) Amazon-Ratings Est
(c) BlogCatalog Obs
(d) BlogCatalog Est"
ABLATION STUDY,0.2266355140186916,Figure 2: The visualization of observed (Obs) and estimated (Est) compatibility matrixes.
ABLATION STUDY,0.2272196261682243,Table 4: Node classification accuracy (%) comparison among nodes with different degrees.
ABLATION STUDY,0.22780373831775702,"Dataset
Amazon-Ratings
Flickr
BlogCatalog"
ABLATION STUDY,0.2283878504672897,"Deg. Prop.(%)
0∼20
20∼40
40∼60
60∼80
80∼100
0∼20
20∼40
40∼60
60∼80
80∼100
0∼20
20∼40
40∼60
60∼80
80∼100"
ABLATION STUDY,0.22897196261682243,"CMGNN
59.78
58.36
53.08
41.74
47.86
92.56
91.19
92.71
93.24
93.65
94.13
97.17
98.29
97.99
97.47"
ABLATION STUDY,0.22955607476635514,"ACM-GCN
57.35
56.21
51.74
41.55
46.47
90.44
91.17
92.85
93.19
89.50
92.17
96.68
97.83
97.84
96.51"
ABLATION STUDY,0.23014018691588786,"OrderedGNN
56.32
56.16
51.20
41.85
50.26
86.48
90.07
92.40
92.79
93.40
92.19
96.09
97.48
97.36
96.27"
ABLATION STUDY,0.23072429906542055,"GCNII
50.61
49.94
47.49
41.85
47.76
87.49
90.54
92.29
92.68
95.09
92.81
96.73
97.58
97.90
97.43"
VISUALIZATION OF COMPATIBILITY MATRIX ESTIMATION,0.23130841121495327,"6.4
Visualization of Compatibility Matrix Estimation
310"
VISUALIZATION OF COMPATIBILITY MATRIX ESTIMATION,0.231892523364486,"We visualize the observed and estimated CMs by CMGNN in Figure 2 with heat maps. Obviously,
311"
VISUALIZATION OF COMPATIBILITY MATRIX ESTIMATION,0.2324766355140187,"CMGNN estimates CMs that are very close to those existing in graphs. This shows that even
312"
VISUALIZATION OF COMPATIBILITY MATRIX ESTIMATION,0.2330607476635514,"with incomplete node labels, CMGNN can estimate high-quality CMs which provides valuable
313"
VISUALIZATION OF COMPATIBILITY MATRIX ESTIMATION,0.2336448598130841,"neighborhood information to nodes. Meanwhile, it can adapt to graphs with various levels of
314"
VISUALIZATION OF COMPATIBILITY MATRIX ESTIMATION,0.23422897196261683,"heterophily. More results can be seen in Appendix G.2.1.
315"
PERFORMANCE ON NODES WITH VARIOUS LEVELS OF DEGREES,0.23481308411214954,"6.5
Performance on Nodes with Various Levels of Degrees
316"
PERFORMANCE ON NODES WITH VARIOUS LEVELS OF DEGREES,0.23539719626168223,"To verify the effect of CMGNN on nodes with incomplete and noisy semantic neighborhoods, we
317"
PERFORMANCE ON NODES WITH VARIOUS LEVELS OF DEGREES,0.23598130841121495,"divide the test set nodes into 5 parts according to their degrees and report the classification accuracy
318"
PERFORMANCE ON NODES WITH VARIOUS LEVELS OF DEGREES,0.23656542056074767,"respectively. We compare CMGNN with 3 top-performance methods and show the results in Table 4.
319"
PERFORMANCE ON NODES WITH VARIOUS LEVELS OF DEGREES,0.2371495327102804,"In general, nodes with low degrees tend to have incomplete and noisy semantic neighborhoods.
320"
PERFORMANCE ON NODES WITH VARIOUS LEVELS OF DEGREES,0.23773364485981308,"Thus, our outstanding performances on the top 20% nodes with the least degree demonstrate the
321"
PERFORMANCE ON NODES WITH VARIOUS LEVELS OF DEGREES,0.2383177570093458,"effectiveness of CMGNN for providing desired neighborhood messages. Further, we can find that
322"
PERFORMANCE ON NODES WITH VARIOUS LEVELS OF DEGREES,0.2389018691588785,"OrderedGNN and GCNII are good at dealing with nodes with high degrees, while ACM-GCN is
323"
PERFORMANCE ON NODES WITH VARIOUS LEVELS OF DEGREES,0.23948598130841123,"relatively good at nodes with low degrees. And CMGNN , to a certain extent, can be adapted to both
324"
PERFORMANCE ON NODES WITH VARIOUS LEVELS OF DEGREES,0.24007009345794392,"situations at the same time.
325"
CONCLUSION AND LIMITATIONS,0.24065420560747663,"7
Conclusion and Limitations
326"
CONCLUSION AND LIMITATIONS,0.24123831775700935,"In this paper, we revisit the message passing mechanism in existing heterophilous GNNs and
327"
CONCLUSION AND LIMITATIONS,0.24182242990654207,"reformulate them into a unified heterophilous message passing (HTMP) mechanism. Based on the
328"
CONCLUSION AND LIMITATIONS,0.24240654205607476,"HTMP mechanism and empirical analysis, we reveal that the reason for message passing remaining
329"
CONCLUSION AND LIMITATIONS,0.24299065420560748,"effective is attributed to implicitly enhancing the compatibility matrix among classes. Further, we
330"
CONCLUSION AND LIMITATIONS,0.2435747663551402,"propose a novel method CMGNN to unlock the potential of the compatibility matrix by handling the
331"
CONCLUSION AND LIMITATIONS,0.2441588785046729,"incomplete and noisy semantic neighborhoods. The experimental results show the effectiveness of
332"
CONCLUSION AND LIMITATIONS,0.2447429906542056,"CMGNN and the feasibility of designing a new method following HTMP mechanism. We hope the
333"
CONCLUSION AND LIMITATIONS,0.24532710280373832,"HTMP mechanism and benchmark can further provide convenience to the community.
334"
CONCLUSION AND LIMITATIONS,0.24591121495327103,"This work mainly focuses on the message passing mechanism in existing HTGNNs under the
335"
CONCLUSION AND LIMITATIONS,0.24649532710280375,"semi-supervised setting. Thus, the other designs in HTGNNs such as objective functions are not
336"
CONCLUSION AND LIMITATIONS,0.24707943925233644,"analyzed in this paper. The proposed HTMP mechanism is suitable for only a large part of existing
337"
CONCLUSION AND LIMITATIONS,0.24766355140186916,"HTGNNs which still follow the message passing mechanism.
338"
REFERENCES,0.24824766355140188,"References
339"
REFERENCES,0.2488317757009346,"[1] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional
340"
REFERENCES,0.24941588785046728,"networks. In International Conference on Learning Representations, 2017.
341"
REFERENCES,0.25,"[2] Yanfu Zhang, Hongchang Gao, Jian Pei, and Heng Huang. Robust self-supervised struc-
342"
REFERENCES,0.2505841121495327,"tural graph neural network for social network prediction. In Proceedings of the ACM Web
343"
REFERENCES,0.25116822429906543,"Conference 2022, pages 1352–1361, 2022.
344"
REFERENCES,0.2517523364485981,"[3] Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. Neural graph collabo-
345"
REFERENCES,0.2523364485981308,"rative filtering. In Proceedings of the 42nd international ACM SIGIR conference on Research
346"
REFERENCES,0.25292056074766356,"and development in Information Retrieval, pages 165–174, 2019.
347"
REFERENCES,0.25350467289719625,"[4] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. Lightgcn:
348"
REFERENCES,0.254088785046729,"Simplifying and powering graph convolution network for recommendation. In Proceedings
349"
REFERENCES,0.2546728971962617,"of the 43rd International ACM SIGIR conference on research and development in Information
350"
REFERENCES,0.2552570093457944,"Retrieval, pages 639–648, 2020.
351"
REFERENCES,0.2558411214953271,"[5] Miller McPherson, Lynn Smith-Lovin, and James M Cook. Birds of a feather: Homophily in
352"
REFERENCES,0.2564252336448598,"social networks. Annual review of sociology, 27(1):415–444, 2001.
353"
REFERENCES,0.2570093457943925,"[6] Yao Ma, Xiaorui Liu, Neil Shah, and Jiliang Tang. Is homophily a necessity for graph neural
354"
REFERENCES,0.25759345794392524,"networks? In International Conference on Learning Representations, 2022.
355"
REFERENCES,0.25817757009345793,"[7] Jiong Zhu, Ryan A Rossi, Anup Rao, Tung Mai, Nedim Lipka, Nesreen K Ahmed, and Danai
356"
REFERENCES,0.2587616822429907,"Koutra. Graph neural networks with heterophily. In Proceedings of the AAAI conference on
357"
REFERENCES,0.25934579439252337,"artificial intelligence, volume 35, pages 11168–11176, 2021.
358"
REFERENCES,0.25992990654205606,"[8] Xin Zheng, Yixin Liu, Shirui Pan, Miao Zhang, Di Jin, and Philip S Yu. Graph neural networks
359"
REFERENCES,0.2605140186915888,"for graphs with heterophily: A survey. arXiv preprint arXiv:2202.07082, 2022.
360"
REFERENCES,0.2610981308411215,"[9] Jiong Zhu, Yujun Yan, Mark Heimann, Lingxiao Zhao, Leman Akoglu, and Danai Koutra.
361"
REFERENCES,0.2616822429906542,"Heterophily and graph neural networks: Past, present and future. IEEE Data Engineering
362"
REFERENCES,0.2622663551401869,"Bulletin, 2023.
363"
REFERENCES,0.2628504672897196,"[10] Chenghua Gong, Yao Cheng, Xiang Li, Caihua Shan, Siqiang Luo, and Chuan Shi. Towards
364"
REFERENCES,0.26343457943925236,"learning from graphs with heterophily: Progress and future. arXiv preprint arXiv:2401.09769,
365"
REFERENCES,0.26401869158878505,"2024.
366"
REFERENCES,0.26460280373831774,"[11] Deyu Bo, Xiao Wang, Chuan Shi, and Huawei Shen. Beyond low-frequency information in graph
367"
REFERENCES,0.2651869158878505,"convolutional networks. In Proceedings of the AAAI Conference on Artificial Intelligence,
368"
REFERENCES,0.2657710280373832,"volume 35, pages 3950–3957, 2021.
369"
REFERENCES,0.26635514018691586,"[12] Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai Koutra. Be-
370"
REFERENCES,0.2669392523364486,"yond homophily in graph neural networks: Current limitations and effective designs. Advances
371"
REFERENCES,0.2675233644859813,"in neural information processing systems, 33:7793–7804, 2020.
372"
REFERENCES,0.26810747663551404,"[13] Di Jin, Zhizhi Yu, Cuiying Huo, Rui Wang, Xiao Wang, Dongxiao He, and Jiawei Han.
373"
REFERENCES,0.26869158878504673,"Universal graph convolutional networks. Advances in Neural Information Processing Systems,
374"
REFERENCES,0.2692757009345794,"34:10654–10664, 2021.
375"
REFERENCES,0.26985981308411217,"[14] Wei Jin, Tyler Derr, Yiqi Wang, Yao Ma, Zitao Liu, and Jiliang Tang. Node similarity preserving
376"
REFERENCES,0.27044392523364486,"graph convolutional networks. In Proceedings of the 14th ACM international conference on
377"
REFERENCES,0.27102803738317754,"web search and data mining, pages 148–156, 2021.
378"
REFERENCES,0.2716121495327103,"[15] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geo-
379"
REFERENCES,0.272196261682243,"metric graph convolutional networks. In International Conference on Learning Representations,
380"
REFERENCES,0.2727803738317757,"2020.
381"
REFERENCES,0.2733644859813084,"[16] Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Nazanin Alipourfard, Kristina Lerman, Hrayr
382"
REFERENCES,0.2739485981308411,"Harutyunyan, Greg Ver Steeg, and Aram Galstyan. Mixhop: Higher-order graph convolutional
383"
REFERENCES,0.27453271028037385,"architectures via sparsified neighborhood mixing. In international conference on machine
384"
REFERENCES,0.27511682242990654,"learning, pages 21–29. PMLR, 2019.
385"
REFERENCES,0.2757009345794392,"[17] Tao Wang, Di Jin, Rui Wang, Dongxiao He, and Yuxiao Huang. Powerful graph convolutional
386"
REFERENCES,0.276285046728972,"networks with adaptive propagation mechanism for homophily and heterophily. In Proceedings
387"
REFERENCES,0.27686915887850466,"of the AAAI conference on artificial intelligence, volume 36, pages 4210–4218, 2022.
388"
REFERENCES,0.2774532710280374,"[18] Sitao Luan, Chenqing Hua, Qincheng Lu, Jiaqi Zhu, Mingde Zhao, Shuyuan Zhang, Xiao-Wen
389"
REFERENCES,0.2780373831775701,"Chang, and Doina Precup. Revisiting heterophily for graph neural networks. Advances in
390"
REFERENCES,0.2786214953271028,"neural information processing systems, 35:1362–1375, 2022.
391"
REFERENCES,0.27920560747663553,"[19] Xiang Li, Renyu Zhu, Yao Cheng, Caihua Shan, Siqiang Luo, Dongsheng Li, and Weining Qian.
392"
REFERENCES,0.2797897196261682,"Finding global homophily in graph neural networks when meeting heterophily. In International
393"
REFERENCES,0.2803738317757009,"Conference on Machine Learning, pages 13242–13256. PMLR, 2022.
394"
REFERENCES,0.28095794392523366,"[20] Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. Adaptive universal generalized
395"
REFERENCES,0.28154205607476634,"pagerank graph neural network. In International Conference on Learning Representations,
396"
REFERENCES,0.2821261682242991,"2021.
397"
REFERENCES,0.2827102803738318,"[21] Yunchong Song, Chenghu Zhou, Xinbing Wang, and Zhouhan Lin. Ordered GNN: Ordering
398"
REFERENCES,0.28329439252336447,"message passing to deal with heterophily and over-smoothing. In The Eleventh International
399"
REFERENCES,0.2838785046728972,"Conference on Learning Representations, 2023.
400"
REFERENCES,0.2844626168224299,"[22] Susheel Suresh, Vinith Budde, Jennifer Neville, Pan Li, and Jianzhu Ma. Breaking the limit of
401"
REFERENCES,0.2850467289719626,"graph neural networks by improving the assortativity of graphs with local mixing patterns. In
402"
REFERENCES,0.28563084112149534,"Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining,
403"
REFERENCES,0.286214953271028,"pages 1541–1551, 2021.
404"
REFERENCES,0.2867990654205608,"[23] Yujun Yan, Milad Hashemi, Kevin Swersky, Yaoqing Yang, and Danai Koutra. Two sides of the
405"
REFERENCES,0.28738317757009346,"same coin: Heterophily and oversmoothing in graph convolutional neural networks. In 2022
406"
REFERENCES,0.28796728971962615,"IEEE International Conference on Data Mining (ICDM), pages 1287–1292. IEEE, 2022.
407"
REFERENCES,0.2885514018691589,"[24] Lun Du, Xiaozhou Shi, Qiang Fu, Xiaojun Ma, Hengyu Liu, Shi Han, and Dongmei Zhang.
408"
REFERENCES,0.2891355140186916,"Gbk-gnn: Gated bi-kernel graph neural networks for modeling both homophily and heterophily.
409"
REFERENCES,0.2897196261682243,"In Proceedings of the ACM Web Conference 2022, pages 1550–1558, 2022.
410"
REFERENCES,0.290303738317757,"[25] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
411"
REFERENCES,0.2908878504672897,"message passing for quantum chemistry. In International conference on machine learning,
412"
REFERENCES,0.29147196261682246,"pages 1263–1272. PMLR, 2017.
413"
REFERENCES,0.29205607476635514,"[26] Johannes Gasteiger, Aleksandar Bojchevski, and Stephan Günnemann. Predict then propagate:
414"
REFERENCES,0.29264018691588783,"Graph neural networks meet personalized pagerank. In International Conference on Learning
415"
REFERENCES,0.2932242990654206,"Representations, 2019.
416"
REFERENCES,0.29380841121495327,"[27] Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph
417"
REFERENCES,0.29439252336448596,"convolutional networks. In International conference on machine learning, pages 1725–1735.
418"
REFERENCES,0.2949766355140187,"PMLR, 2020.
419"
REFERENCES,0.2955607476635514,"[28] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and
420"
REFERENCES,0.29614485981308414,"Yoshua Bengio. Graph attention networks. In The International Conference on Learning
421"
REFERENCES,0.2967289719626168,"Representations, 2018.
422"
REFERENCES,0.2973130841121495,"[29] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan Günnemann.
423"
REFERENCES,0.29789719626168226,"Pitfalls of graph neural network evaluation. arXiv preprint arXiv:1811.05868, 2018.
424"
REFERENCES,0.29848130841121495,"[30] Oleg Platonov, Denis Kuznedelev, Michael Diskin, Artem Babenko, and Liudmila
425"
REFERENCES,0.29906542056074764,"Prokhorenkova. A critical look at the evaluation of GNNs under heterophily: Are we re-
426"
REFERENCES,0.2996495327102804,"ally making progress? In The Eleventh International Conference on Learning Representations,
427"
REFERENCES,0.3002336448598131,"2023.
428"
REFERENCES,0.3008177570093458,"[31] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and
429"
REFERENCES,0.3014018691588785,"Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In
430"
REFERENCES,0.3019859813084112,"International conference on machine learning, pages 5453–5462. PMLR, 2018.
431"
REFERENCES,0.30257009345794394,"[32] Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks
432"
REFERENCES,0.30315420560747663,"on graphs with fast localized spectral filtering. Advances in neural information processing
433"
REFERENCES,0.3037383177570093,"systems, 29, 2016.
434"
REFERENCES,0.30432242990654207,"[33] Bingbing Xu, Huawei Shen, Qi Cao, Yunqi Qiu, and Xueqi Cheng. Graph wavelet neural
435"
REFERENCES,0.30490654205607476,"network. In International Conference on Learning Representations, 2018.
436"
REFERENCES,0.3054906542056075,"[34] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large
437"
REFERENCES,0.3060747663551402,"graphs. Advances in neural information processing systems, 30, 2017.
438"
REFERENCES,0.3066588785046729,"[35] Yao Ma, Xiaorui Liu, Tong Zhao, Yozen Liu, Jiliang Tang, and Neil Shah. A unified view on
439"
REFERENCES,0.3072429906542056,"graph neural networks as graph signal denoising. In Proceedings of the 30th ACM International
440"
REFERENCES,0.3078271028037383,"Conference on Information & Knowledge Management, pages 1202–1211, 2021.
441"
REFERENCES,0.308411214953271,"[36] Meiqi Zhu, Xiao Wang, Chuan Shi, Houye Ji, and Peng Cui. Interpreting and unifying graph
442"
REFERENCES,0.30899532710280375,"neural networks with an optimization framework. In Proceedings of the Web Conference 2021,
443"
REFERENCES,0.30957943925233644,"pages 1215–1226, 2021.
444"
REFERENCES,0.3101635514018692,"[37] Yixin Liu, Zhao Li, Shirui Pan, Chen Gong, Chuan Zhou, and George Karypis. Anomaly
445"
REFERENCES,0.3107476635514019,"detection on attributed networks via contrastive self-supervised learning. IEEE Transactions on
446"
REFERENCES,0.31133177570093457,"Neural Networks and Learning Systems, 2021.
447"
REFERENCES,0.3119158878504673,"[38] Péter Mernyei and C˘at˘alina Cangea. Wiki-cs: A wikipedia-based benchmark for graph neural
448"
REFERENCES,0.3125,"networks. arXiv preprint arXiv:2007.02901, 2020.
449"
REFERENCES,0.3130841121495327,"[39] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-
450"
REFERENCES,0.31366822429906543,"Rad. Collective classification in network data. AI magazine, 29(3):93–93, 2008.
451"
REFERENCES,0.3142523364485981,"[40] Édouard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, and Tomáš Mikolov. Learning
452"
REFERENCES,0.3148364485981308,"word vectors for 157 languages. In Proceedings of the Eleventh International Conference on
453"
REFERENCES,0.31542056074766356,"Language Resources and Evaluation (LREC 2018), 2018.
454"
REFERENCES,0.31600467289719625,"[41] Leskovec Jure. Snap datasets: Stanford large network dataset collection. Retrieved December
455"
REFERENCES,0.316588785046729,"2021 from http://snap. stanford. edu/data, 2014.
456"
REFERENCES,0.3171728971962617,"[42] Benedek Rozemberczki, Carl Allen, and Rik Sarkar. Multi-scale attributed node embedding.
457"
REFERENCES,0.3177570093457944,"Journal of Complex Networks, 9(2):cnab014, 2021.
458"
REFERENCES,0.3183411214953271,"[43] Jie Tang, Jimeng Sun, Chi Wang, and Zi Yang. Social influence analysis in large-scale networks.
459"
REFERENCES,0.3189252336448598,"In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery
460"
REFERENCES,0.3195093457943925,"and data mining, pages 807–816, 2009.
461"
REFERENCES,0.32009345794392524,"[44] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for
462"
REFERENCES,0.32067757009345793,"word representation. In Proceedings of the 2014 conference on empirical methods in natural
463"
REFERENCES,0.3212616822429907,"language processing (EMNLP), pages 1532–1543, 2014.
464"
REFERENCES,0.32184579439252337,"A
More Details of HTMP Mechanism
465"
REFERENCES,0.32242990654205606,"In this part, we list more details about the HTMP mechanism, including additional analysis about
466"
REFERENCES,0.3230140186915888,"HTMP, method-wise analysis and overall analysis.
467"
REFERENCES,0.3235981308411215,"A.1
Additional Analysis of HTMP Mechanism
468"
REFERENCES,0.3241822429906542,"A.1.1
Neighborhood Indicators
469"
REFERENCES,0.3247663551401869,"The neighborhood indicator explicitly marks the neighbors of all nodes within a specific neighbor-
470"
REFERENCES,0.3253504672897196,"hood. In existing heterophilous GNNs, neighborhood indicators typically take one of the following
471"
REFERENCES,0.32593457943925236,"forms: (i) Raw Neighborhood (Raw); (ii) Neighborhood Redefining (ReDef); and (3) Neighborhood
472"
REFERENCES,0.32651869158878505,"Discrimination (Dis).
473"
REFERENCES,0.32710280373831774,"Raw Neighborhood. Raw neighborhood, including A and ˜A, provides the basic neighborhood
474"
REFERENCES,0.3276869158878505,"information. The only difference between them lies in whether there is differential treatment of the
475"
REFERENCES,0.3282710280373832,"node’s ego messages. For example, APPNP [26] applies additional weighting to the nodes’ ego
476"
REFERENCES,0.32885514018691586,"messages compared with GCN [1]. For the sake of simplicity, we consider the identity matrix I ∈
477"
REFERENCES,0.3294392523364486,"RN×N as a special neighborhood indicator for acquiring the nodes’ ego messages. In heterophilous
478"
REFERENCES,0.3300233644859813,"GNNs, ego/neighbor separation is a common strategy that can mitigate the confusion of ego messages
479"
REFERENCES,0.33060747663551404,"with neighbor messages.
480"
REFERENCES,0.33119158878504673,"Neighborhood Redefining. Neighborhood redefining is the most commonly used technique in
481"
REFERENCES,0.3317757009345794,"heterophilous GNNs, aiming to capture additional information from new neighborhoods. As a repre-
482"
REFERENCES,0.33235981308411217,"sentative example, high-order neighborhood Ah can provide long-distance connection information
483"
REFERENCES,0.33294392523364486,"but also result in additional computational costs. Feature-similarity-based neighborhood Af is often
484"
REFERENCES,0.33352803738317754,"defined by the k-NN relationships within the feature space. Fundamentally, it only utilizes node
485"
REFERENCES,0.3341121495327103,"features and thus needs to be used in conjunction with other neighborhood indicators. Otherwise,
486"
REFERENCES,0.334696261682243,"the model will be limited by the amount of information in node features. GloGNN [19] introduces
487"
REFERENCES,0.3352803738317757,"fully-connected neighborhood 1 ∈RN×N, which can capture global neighbor information from all
488"
REFERENCES,0.3358644859813084,"nodes. However, it can also cause significant time and space consumption. Additionally, there are
489"
REFERENCES,0.3364485981308411,"some custom-defined neighborhood Ac. For example, Geom-GCN [15] redefines neighborhoods
490"
REFERENCES,0.33703271028037385,"based on the geometric relationships between node pairs. These neighborhood indicators may have
491"
REFERENCES,0.33761682242990654,"limited generality, and the effectiveness is reliant on the specific method.
492"
REFERENCES,0.3382009345794392,"Neighborhood Discrimination. Neighborhood discrimination aims to mark whether neighbors
493"
REFERENCES,0.338785046728972,"share the same label with central nodes. The neighborhoods are partitioned into positive Ap and
494"
REFERENCES,0.33936915887850466,"negative ones An, which include homophilous and heterophilous neighbors respectively. GGCN [23]
495"
REFERENCES,0.3399532710280374,"divides the raw neighborhood based on the similarity of node representations with a threshold
496"
REFERENCES,0.3405373831775701,"of 0. Explicitly distinguishing neighbors allows for targeted processing, making the model more
497"
REFERENCES,0.3411214953271028,"interpretable. However, its performance is influenced by the accuracy of the discrimination, which
498"
REFERENCES,0.34170560747663553,"may lead to the accumulation of errors.
499"
REFERENCES,0.3422897196261682,"A.1.2
Aggregation Guidance
500"
REFERENCES,0.3428738317757009,"After identifying the neighborhood, the aggregation guidance controls what type of messages to
501"
REFERENCES,0.34345794392523366,"gather from the corresponding neighbors. The existing aggregation guidance mainly includes three
502"
REFERENCES,0.34404205607476634,"kinds of approaches: (1) Degree Averaging (DegAvg), (2) Adaptive Weights (AdaWeight), and (3)
503"
REFERENCES,0.3446261682242991,"Relationship Estimation (RelaEst).
504"
REFERENCES,0.3452102803738318,"Degree Averaging. Degree averaging, formatted as Bd = D−1"
REFERENCES,0.34579439252336447,2 1D−1
REFERENCES,0.3463785046728972,"2 or Bd = D−11, is the most
505"
REFERENCES,0.3469626168224299,"common aggregation guidance, which plays the role of a low-pass filter to capture the smooth signals
506"
REFERENCES,0.3475467289719626,"and is fixed during model training. Further, combining negative degree averaging with an identity
507"
REFERENCES,0.34813084112149534,"aggregation guidance I ∈RN×N can capture the difference between central nodes and neighbors, as
508"
REFERENCES,0.348714953271028,"used in ACM-GCN [18]. Degree averaging is simple and efficient but depends on the discriminability
509"
REFERENCES,0.3492990654205608,"of corresponding neighborhoods.
510"
REFERENCES,0.34988317757009346,"Adaptive Weights. Another common strategy is allowing the model to learn the appropriate aggrega-
511"
REFERENCES,0.35046728971962615,"tion guidances Baw. GAT [28] proposes an attention mechanism to learn aggregate weights, which
512"
REFERENCES,0.3510514018691589,"guides many subsequent heterophilous methods. To better handle heterophilous graphs, FAGCN [11]
513"
REFERENCES,0.3516355140186916,"introduces negative-available attention weights Bnaw to capture the difference between central nodes
514"
REFERENCES,0.3522196261682243,"and heterophilous neighbors. Adaptive weights can personalize message aggregation for different
515"
REFERENCES,0.352803738317757,"neighbors, yet it’s difficult for models to attain the desired effect.
516"
REFERENCES,0.3533878504672897,"Relationship Estimation. Recently, some methods have tried to estimate the pair-wise relationships
517"
REFERENCES,0.35397196261682246,"Bre between nodes and use them to guide message aggregation. HOG-GCN [17] estimates the
518"
REFERENCES,0.35455607476635514,"pair-wise homophily levels between nodes as aggregation guidances based on both attribute and
519"
REFERENCES,0.35514018691588783,"topology space. GloGNN [19] treats all nodes as neighbors and estimates a coefficient matrix
520"
REFERENCES,0.3557242990654206,"as aggregation guidance based on the idea of linear subspace expression. GGCN [23] estimates
521"
REFERENCES,0.35630841121495327,"appropriate weights for message aggregation with the degrees of nodes and the similarities between
522"
REFERENCES,0.35689252336448596,"node representations. Relationship estimation usually has theoretical guidance, which brings strong
523"
REFERENCES,0.3574766355140187,"interpretability. However, it may also result in significant temporal and spatial complexity when
524"
REFERENCES,0.3580607476635514,"estimating pair-wise relations.
525"
REFERENCES,0.35864485981308414,"A.1.3
COMBINE Function
526"
REFERENCES,0.3592289719626168,"After message aggregation, the COMBINE functions integrate messages from multiple neighborhoods
527"
REFERENCES,0.3598130841121495,"into layer representations. COMBINE functions in heterophilous GNNs are commonly based on
528"
REFERENCES,0.36039719626168226,"two operations: addition and concatenation, each of which has variants. To merge several messages
529"
REFERENCES,0.36098130841121495,"together, addition (Add) is a naive idea. Further, to control the weight of messages from different
530"
REFERENCES,0.36156542056074764,"neighborhoods, weighted addition (WeightedAdd) is applied. However, it is a global setting and
531"
REFERENCES,0.3621495327102804,"cannot adapt to the differences between nodes. Thus, adaptive weighted addition (AdaAdd) is
532"
REFERENCES,0.3627336448598131,"proposed, which can learn personalized message combination weights for each node, but it will result
533"
REFERENCES,0.3633177570093458,"in additional time consumption. Although the addition is simple and efficient, some methods [12, 16]
534"
REFERENCES,0.3639018691588785,"believe that it may blur messages from different neighborhoods, which can be harmful in heterophilous
535"
REFERENCES,0.3644859813084112,"GNNs, so they employ a concatenation operation (Cat) to separate the messages. Nevertheless, such
536"
REFERENCES,0.36507009345794394,"an approach not only increases the space cost but may also retain additional redundant messages. To
537"
REFERENCES,0.36565420560747663,"address these issues, OrderedGNN [21] proposes an adaptive concatenation mechanism (AdaCat)
538"
REFERENCES,0.3662383177570093,"that can combine multiple messages with learnable dimensions. This is an innovative and worthy
539"
REFERENCES,0.36682242990654207,"further exploration practice, but the difficulty of model learning should also be considered.
540"
REFERENCES,0.36740654205607476,"A.1.4
FUSE Function
541"
REFERENCES,0.3679906542056075,"Further, the FUSE functions integrate messages from multiple layers into the final representation. For
542"
REFERENCES,0.3685747663551402,"the FUSE function, utilizing the representation of the last layer as the final representation is widely
543"
REFERENCES,0.3691588785046729,"accepted: Z = ZL. JKNet [31] proposes that the combination of representations from intermediate
544"
REFERENCES,0.3697429906542056,"layers can capture both local and global information. H2GCN [12] applies it in heterophilous graphs,
545"
REFERENCES,0.3703271028037383,"preserving messages from different localities with concatenation. Similarly, GPRGNN [20] combines
546"
REFERENCES,0.370911214953271,"the representations of multiple layers into the final representation through adaptive weighted addition.
547"
REFERENCES,0.37149532710280375,"A.1.5
AGGREGATE function
548"
REFERENCES,0.37207943925233644,"The most commonly used AGGREGATE function is AGGREGATE(Ar, Br, Zl−1
r
) = (Ar ⊙
549"
REFERENCES,0.3726635514018692,"Br)Zl−1
r
Wl
r. We take this as the fixed form of the AGGREGATE function following. Actually,
550"
REFERENCES,0.3732476635514019,"the input representations Z−1
r
and weight matrixes Wl
r also can be specially designed. Taking
551"
REFERENCES,0.37383177570093457,"the initial node representations Z0 as input is a relatively common approach as in APPNP [26],
552"
REFERENCES,0.3744158878504673,"GCNII [27], FAGCN [11] and GloGNN [19]. Further, GCNII [27] adds an identity matrix Iw to the
553"
REFERENCES,0.375,"weight matrixes to keep more original messages. However, the methods that specially design these
554"
REFERENCES,0.3755841121495327,"components are few and with a similar form. Thus, we don’t discuss them too much, but leave it for
555"
REFERENCES,0.37616822429906543,"future extensions.
556"
REFERENCES,0.3767523364485981,"A.2
Revisiting Representative GNNs with HTMP Mechanism
557"
REFERENCES,0.3773364485981308,"In this part, we utilize HTMP mechanism to revisit the representative GNNs. We start from ho-
558"
REFERENCES,0.37792056074766356,"mophilous GNNs as simple examples and further extend to heterophilous GNNs.
559"
REFERENCES,0.37850467289719625,"A.2.1
GCN
560"
REFERENCES,0.379088785046729,"Graph Convolutional Networks (GCN) [1] utilizes a low-pass filter to gather messages from neighbors
561"
REFERENCES,0.3796728971962617,"as follows:
562"
REFERENCES,0.3802570093457944,"Zl = ˆ˜AZl−1Wl.
(12)"
REFERENCES,0.3808411214953271,"It can be revisited by HTMP with the following components:
563"
REFERENCES,0.3814252336448598,"A0 = ˜A,
B0 = Bd = ˜D−1"
REFERENCES,0.3820093457943925,"2 1 ˜D−1 2 ,"
REFERENCES,0.38259345794392524,"Zl = Zl
0 = (A0 ⊙B0)Zl−1Wl = ˆ˜AZl−1Wl.
(13)"
REFERENCES,0.38317757009345793,"Specifically, GCN has a raw neighborhood indicator ˜A and a degree averaging aggregation guidance
564"
REFERENCES,0.3837616822429907,"Bd. Since there is only one neighborhood, the COMBINE function is meaningless in GCN. GCN
565"
REFERENCES,0.38434579439252337,"utilizes a naive way to fuse messages about the original neighborhood and central nodes. However, it
566"
REFERENCES,0.38492990654205606,"may confuse the representations in heterophilous graphs.
567"
REFERENCES,0.3855140186915888,"A.2.2
APPNP
568"
REFERENCES,0.3860981308411215,"PPNP [26] is also a general method whose message passing is based on Personalized PageRank
569"
REFERENCES,0.3866822429906542,"(PPR). To avoid massive consumption, APPNP is introduced as the approximate version of PPNP
570"
REFERENCES,0.3872663551401869,"with an iterative message-passing mechanism:
571"
REFERENCES,0.3878504672897196,"Zl = µZ0 + (1 −µ) ˆAZl−1.
(14)"
REFERENCES,0.38843457943925236,"It can be revisited by with the following components:
572"
REFERENCES,0.38901869158878505,"A = [A0, A1],
B = [B0, B1],"
REFERENCES,0.38960280373831774,"A0 = I,
B0 = I,
Wl
0 = I,
eZl
0 = (A0 ⊙B0)Z0Wl
0 = Z0,"
REFERENCES,0.3901869158878505,"A1 = A,
B1 = D−1"
REFERENCES,0.3907710280373832,2 1D−1
REFERENCES,0.39135514018691586,"2 ,
Wl
1 = I,
eZl
1 = (A1 ⊙B1)Zl−1Wl
1 = ˆAZl−1. (15)"
REFERENCES,0.3919392523364486,"Specifically, APPNP aggregates messages from node ego and neighborhoods separately and combines
573"
REFERENCES,0.3925233644859813,"them with a weighted addition. Compared with GCN, APPNP assigns adjustable weights to nodes,
574"
REFERENCES,0.39310747663551404,"for controlling the proportion of ego and neighbor messages during message-passing, which becomes
575"
REFERENCES,0.39369158878504673,"a worthy design in heterophilous graphs.
576"
REFERENCES,0.3942757009345794,"A.2.3
GAT
577"
REFERENCES,0.39485981308411217,"Going a step further, Graph Attention Networks (GAT) [28] allows learnable weights for each
578"
REFERENCES,0.39544392523364486,"neighbor:
579"
REFERENCES,0.39602803738317754,"Zl
i =
X"
REFERENCES,0.3966121495327103,"j∈˜
N(i)
αijZl−1
j
Wl,
(16)"
REFERENCES,0.397196261682243,"where αij is the weight for aggregating neighbor node j to center node i, whose construction process
580"
REFERENCES,0.3977803738317757,"is as follows:
581"
REFERENCES,0.3983644859813084,"αij =
exp(eij)
P"
REFERENCES,0.3989485981308411,"k∈˜
N(i) exp(eik),"
REFERENCES,0.39953271028037385,"eij = LeakyReLU
 
Zl−1
i
|Zl−1
j

a

.
(17)"
REFERENCES,0.40011682242990654,"Let PGAT be the matrix of aggregation weights in GAT:
582"
REFERENCES,0.4007009345794392,"PGAT
ij
=

αij,
˜Aij = 1,
0,
˜Aij = 0. .
(18)"
REFERENCES,0.401285046728972,"HTMP can revisit GAT with the following components:
583"
REFERENCES,0.40186915887850466,"A0 = ˜A,
B0 = Baw = PGAT ,"
REFERENCES,0.4024532710280374,"Zl = Zl
0 = (A0 ⊙B0)Zl−1Wl = PGAT Zl−1Wl,
(19)"
REFERENCES,0.4030373831775701,"which is the matrix version of Eq 16. Specifically, GAT aggregate messages from raw neighborhood
584"
REFERENCES,0.4036214953271028,"˜A with adaptive weights Baw. Aggregation guidance with adaptive weights is a nice idea, but simple
585"
REFERENCES,0.40420560747663553,"constraints are not enough for the model to learn ideal results.
586"
REFERENCES,0.4047897196261682,"A.2.4
GCNII
587"
REFERENCES,0.4053738317757009,"GCNII [27] is a novel homophilous GNN with two key designs: initial residual connection and
588"
REFERENCES,0.40595794392523366,"identity mapping, which can be formatted as follows:
589"
REFERENCES,0.40654205607476634,"Zl =

αZ0 + (1 −α) ˜D−1"
REFERENCES,0.4071261682242991,2 ˜A ˜D−1
REFERENCES,0.4077102803738318,"2 Zl−1  
βWl + (1 −β)Iw

,
(20)"
REFERENCES,0.40829439252336447,"where α and β are two predefined parameters and Iw ∈Rdr×dr is an identity matrix.
590"
REFERENCES,0.4088785046728972,"From the perspective of HTMP, it can be viewed as follows:
591"
REFERENCES,0.4094626168224299,"A = [I, ˜A],
B = [I, ˜Bd],
Wl
0 = Wl
1 =
 
βWl + (1 −β)Iw

,
eZl
0 = (I ⊙I)Z0  
βWl + (1 −β)Iw

= Z0  
βWl + (1 −β)Iw

,"
REFERENCES,0.4100467289719626,"eZl
1 = ( ˜A ⊙˜Bd)Zl−1  
βWl + (1 −β)Iw

= ˆ˜AZl−1  
βWl + (1 −β)Iw

, (21)"
REFERENCES,0.41063084112149534,"where the COMBINE function is weighted addition. Specifically, the first design of GCNII is a form
592"
REFERENCES,0.411214953271028,"of ego/neighbor separation, and the second design is a novel transformation weights matrix. This can
593"
REFERENCES,0.4117990654205608,"also be specially designed, but only GCNII does this, so we won’t analyze it too much and leave it as
594"
REFERENCES,0.41238317757009346,"a future extension.
595"
REFERENCES,0.41296728971962615,"A.2.5
Geom-GCN
596"
REFERENCES,0.4135514018691589,"Geom-GCN [15] is one of the most influential heterophilous GNNs, which employs the geometric
597"
REFERENCES,0.4141355140186916,"relationships of nodes within two kinds of neighborhoods to aggregate the messages through bi-level
598"
REFERENCES,0.4147196261682243,"aggregation:
599 Zl = "
REFERENCES,0.415303738317757,"∥
i∈{g,s}
∥
r∈R
Zl
i,r ! Wl,"
REFERENCES,0.4158878504672897,"Zl
i,r = D
−1"
REFERENCES,0.41647196261682246,"2
i,r Ai,rD
−1"
REFERENCES,0.41705607476635514,"2
i,r Zl−1, (22)"
REFERENCES,0.41764018691588783,"where ∥denotes the concatenate operator, {g, s} is the set of neighborhoods including the original
600"
REFERENCES,0.4182242990654206,"graph and the latent space. R is the set of geometric relationships. Ai,r is the corresponding adjacency
601"
REFERENCES,0.41880841121495327,"matrix in neighborhood i and relationship r.
602"
REFERENCES,0.41939252336448596,"It can be revisited by HTMP with the following components:
603"
REFERENCES,0.4199766355140187,"A = [Ai,r|i ∈{g, s}, r ∈R],
B = [Bd
i,r||i ∈{g, s}, r ∈R],"
REFERENCES,0.4205607476635514,"eZl
i,r = (Ai,r ⊙Bd
i,r)Zl−1Wl
i,r = D
−1"
REFERENCES,0.42114485981308414,"2
i,r Ai,rD
−1"
REFERENCES,0.4217289719626168,"2
i,r Zl−1Wl
i,r,
(23)"
REFERENCES,0.4223130841121495,"where the COMBINE function is concatenation and the weight matrix Wl in Eq 22 can be viewed as
604"
REFERENCES,0.42289719626168226,"the combination of multiple Wl
i,r. Specifically, Geom-GCN redefines multiple neighborhoods based
605"
REFERENCES,0.42348130841121495,"on the customized geometric relations in both raw and latent space. The messages are aggregated
606"
REFERENCES,0.42406542056074764,"from each neighborhood and combined by a concatenation. This approach may be applicable to some
607"
REFERENCES,0.4246495327102804,"datasets, yet it has weak universality.
608"
REFERENCES,0.4252336448598131,"A.2.6
H2GCN
609"
REFERENCES,0.4258177570093458,"H2GCN [12] is also an influential method with three key designs: ego- and neighbor-message
610"
REFERENCES,0.4264018691588785,"separation, higher-order neighborhoods, and the combination of intermediate representations. Its
611"
REFERENCES,0.4269859813084112,"single-layer representations are constructed as follows:
612"
REFERENCES,0.42757009345794394,"Zl =
h
ˆAZl−1 ∥ˆAh2Zl−1i
,
(24)"
REFERENCES,0.42815420560747663,"where ˆAh2 denotes the 2-order adjacency matrix with normalization.
613"
REFERENCES,0.4287383177570093,"It can be revisited by HTMP with the following components:
614"
REFERENCES,0.42932242990654207,"A = [A, Ah2],
B = [Bd, Bd
h2],
Wl
0 = Wl
1 = I,
eZl
0 = (A ⊙Bd)Zl−1I = ˆAZl−1,
eZl
1 = (Ah2 ⊙Bd
h2)Zl−1I = ˆAh2Zl−1, (25)"
REFERENCES,0.42990654205607476,"where the COMBINE function is concatenation. Meanwhile, H2GCN also uses the concatenation
615"
REFERENCES,0.4304906542056075,"as the FUSE function. Specifically, H2GCN aggregates messages from the raw and 2-order neigh-
616"
REFERENCES,0.4310747663551402,"borhoods in a layer of message passing and keeps them apart in the representations. The design
617"
REFERENCES,0.4316588785046729,"of ego/neighbor separation is first introduced by H2GCN and gradually becomes a necessity for
618"
REFERENCES,0.4322429906542056,"subsequent methods.
619"
REFERENCES,0.4328271028037383,"A.2.7
SimP-GCN
620"
REFERENCES,0.433411214953271,"SimP-GCN [14] constructs an additional graph based on the feature similarity. It has two key concepts:
621"
REFERENCES,0.43399532710280375,"(1) the information from the original graph and feature kNN graph should be balanced, and (2) each
622"
REFERENCES,0.43457943925233644,"node can adjust the contribution of its node features. Specifically, the message passing in SimP-GCN
623"
REFERENCES,0.4351635514018692,"is as follows:
624"
REFERENCES,0.4357476635514019,"Zl =

diag(sl) ˆ˜A + diag(1 −sl) ˆAf + γDl
K

Zl−1Wl,
(26)"
REFERENCES,0.43633177570093457,"where sl ∈Rn is a learnable score vector that balances the effect of the original and feature graphs,
625"
REFERENCES,0.4369158878504673,"Dl
K = diag(Kl
1, Kl
2, ..., Kl
n) is a learnable diagonal matrix.
626"
REFERENCES,0.4375,"It can be revisited by HTMP with the following components:
627"
REFERENCES,0.4380841121495327,"A = [I, ˜A, Af],
B = [I, ˜Bd, Bd
f],"
REFERENCES,0.43866822429906543,"eZl
0 = (I ⊙I)Zl−1Wl = Zl−1Wl,"
REFERENCES,0.4392523364485981,"eZl
1 = ( ˜A ⊙˜Bd)Zl−1Wl = ˆ˜AZl−1Wl,
eZl
2 = (Af ⊙Bd
f)Zl−1Wl = ˆAfZl−1Wl, (27)"
REFERENCES,0.4398364485981308,"where the COMBINE function is adaptive weighted addition. Specifically, SimP-GCN aggregates
628"
REFERENCES,0.44042056074766356,"messages from ego, raw and feature-similarity-based neighborhoods, and combines them with
629"
REFERENCES,0.44100467289719625,"node-specific learnable weights. The feature-similarity-based neighborhoods can provide more
630"
REFERENCES,0.441588785046729,"homophilous messages to enhance the discriminability of the compatibility matrix. However, it’s still
631"
REFERENCES,0.4421728971962617,"limited by the amount of information on node features.
632"
REFERENCES,0.4427570093457944,"A.2.8
FAGCN
633"
REFERENCES,0.4433411214953271,"FAGCN [11] proposes considering both low-frequency and high-frequency information simultane-
634"
REFERENCES,0.4439252336448598,"ously, and transferring them into the negative-allowable weights during message passing:
635"
REFERENCES,0.4445093457943925,"Zl
i = µZ0
i +
X j∈Ni"
REFERENCES,0.44509345794392524,"αG
ij
p"
REFERENCES,0.44567757009345793,"didj
Zl−1
j
,
(28)"
REFERENCES,0.4462616822429907,"where αG
ij can be negative as follows:
636"
REFERENCES,0.44684579439252337,"αG
ij = tanh(gT [Xi∥Xj]),
(29)"
REFERENCES,0.44742990654205606,"which can form a weight matrix:
637"
REFERENCES,0.4480140186915888,"PF AG
ij
=

αG
ij,
Aij = 1,
0,
Aij = 0.
(30)"
REFERENCES,0.4485981308411215,"It can be revisited by HTMP with the following components:
638"
REFERENCES,0.4491822429906542,"A = [I, A],
B = [I, D−1"
REFERENCES,0.4497663551401869,2 PF AGD−1
REFERENCES,0.4503504672897196,"2 ],
Wl
0 = Wl
1 = I,
eZl
0 = (I ⊙I)Z0I = Z0,
eZl
1 = (A ⊙D−1"
REFERENCES,0.45093457943925236,2 PF AGD−1
REFERENCES,0.45151869158878505,2 )Zl−1I = D−1
REFERENCES,0.45210280373831774,2 PF AGD−1
REFERENCES,0.4526869158878505,"2 Zl−1, (31)"
REFERENCES,0.4532710280373832,"where the COMBINE function is weighted addition, same as the matrix form of Eq 28. Specifically,
639"
REFERENCES,0.45385514018691586,"FAGCN aggregates messages from node ego and raw neighborhood with negative-allowable weights.
640"
REFERENCES,0.4544392523364486,"It has a similar form to GAT but allows for ego/neighbor separation and negative weights, which
641"
REFERENCES,0.4550233644859813,"means the model can capture the difference between center nodes and neighbors.
642"
REFERENCES,0.45560747663551404,"A.2.9
GGCN
643"
REFERENCES,0.45619158878504673,"GGCN [23] explicitly distinguishes between homophilous and heterophilous neighbors based on
644"
REFERENCES,0.4567757009345794,"node similarities, and assigns corresponding positive and negative weights:
645"
REFERENCES,0.45735981308411217,"Zl = αl 
βl
0 ˆZl + βl
1(Sl
pos ⊙˜Al
T )ˆZl + βl
2(Sl
neg ⊙˜Al
T )ˆZl
,
(32)"
REFERENCES,0.45794392523364486,"where ˆZl = Zl−1Wl + bl, ˜Al
T = ˜A ⊙T l is an adjacency matrix weighted by the structure property,
646"
REFERENCES,0.45852803738317754,"βl
0, βl
1 and βl
2 are learnable scalars. The neighbors are distinguished by the cosine similarity of node
647"
REFERENCES,0.4591121495327103,"representations with a threshold of 0:
648"
REFERENCES,0.459696261682243,"Sl
ij =

Cosine(Zi, Zj),
i ̸= j & Aij = 1,
0,
otherwise.
,"
REFERENCES,0.4602803738317757,"Sl
pos, ij =

Sl
ij,
Sl
ij > 0,
0,
otherwise. ,"
REFERENCES,0.4608644859813084,"Sl
neg, ij =

Sl
ij,
Sl
ij < 0,
0,
otherwise. . (33)"
REFERENCES,0.4614485981308411,"It can be revisited by HTMP with the following components:
649"
REFERENCES,0.46203271028037385,"A = [I, Ap, An],
B = [I, Sl
pos ⊙T l, Sl
neg ⊙(T)l],"
REFERENCES,0.46261682242990654,"eZl
0 = (I ⊙I)Zl−1Wl = Zl−1Wl,
eZl
1 = (Ap ⊙Sl
pos ⊙T l)Zl−1Wl = (Sl
pos ⊙T l)Zl−1Wl,"
REFERENCES,0.4632009345794392,"eZl
2 = (An ⊙Sl
neg ⊙T l)Zl−1Wl = (Sl
neg ⊙T l)Zl−1Wl, (34)"
REFERENCES,0.463785046728972,"where Ap and An are discriminated by the representation similarities:
650"
REFERENCES,0.46436915887850466,"Ap,ij =

1,
Sl
pos,ij > 0&Aij = 1,
0,
otherwise.
,"
REFERENCES,0.4649532710280374,"An,ij =

1,
Sl
neg,ij < 0&Aij = 1,
0,
otherwise.
.
(35)"
REFERENCES,0.4655373831775701,"The COMBINE function is an adaptive weighted addition. Specifically, GGCN divides the raw
651"
REFERENCES,0.4661214953271028,"neighborhood into positive and negative ones based on the similarities among node presentations.
652"
REFERENCES,0.46670560747663553,"On this basis, it aggregates messages from node ego, positive and negative neighborhoods, and
653"
REFERENCES,0.4672897196261682,"combines them with node-specific learnable weights. This approach allows for targeted processing
654"
REFERENCES,0.4678738317757009,"for homophilous and heterophilous neighbors, yet can suffer from the accuracy of discrimination,
655"
REFERENCES,0.46845794392523366,"which may lead to the accumulation of errors.
656"
REFERENCES,0.46904205607476634,"A.2.10
ACM-GCN
657"
REFERENCES,0.4696261682242991,"ACM-GCN [18] introduces 3 channels (identity, low pass and high pass) to capture different informa-
658"
REFERENCES,0.4702102803738318,"tion and mixes them with node-wise adaptive weights:
659"
REFERENCES,0.47079439252336447,"Zl = diag(αl
I)Zl−1Wl
I + diag(αl
L) ˆAZl−1Wl
L + diag(αl
H)(I −ˆA)Zl−1Wl
H,
(36)"
REFERENCES,0.4713785046728972,"where diag(αl
I), diag(αl
L), diag(αl
H) ∈RN×1 are learnable weight vectors.
660"
REFERENCES,0.4719626168224299,"It can be revisited by HTMP with the following components:
661"
REFERENCES,0.4725467289719626,"A = [I, A, A],
B = [I, Bd, I −Bd],
eZl
0 = (I ⊙I)Zl−1Wl
I = Zl−1Wl
I,
eZl
1 = (A ⊙Bd)Zl−1Wl
L = ˆAZl−1Wl
L,
eZl
2 = (A ⊙(I −Bd))Zl−1Wl
H = (I −ˆA)Zl−1Wl
H, (37)"
REFERENCES,0.47313084112149534,"where the COMBINE function is adaptive weighted addition. Specifically, ACM-GCN aggregates
662"
REFERENCES,0.473714953271028,"node ego, low-frequency, and high-frequency messages from ego and raw neighborhoods, and
663"
REFERENCES,0.4742990654205608,"combines them with node-wise adaptive weights. With simple but effective designs, ACM-GCN
664"
REFERENCES,0.47488317757009346,"achieves outstanding performance, which shows that complicated designs are not necessary.
665"
REFERENCES,0.47546728971962615,"A.2.11
OrderedGNN
666"
REFERENCES,0.4760514018691589,"OrderedGNN [21] is a SOTA method that introduces a node-wise adaptive dimension concatenation
667"
REFERENCES,0.4766355140186916,"function to combine messages from neighbors of different hops:
668"
REFERENCES,0.4772196261682243,"Zl = Pl
d ⊙Zl−1 + (1 −Pl
d) ⊙( ˆAZl−1),
(38)"
REFERENCES,0.477803738317757,"where Pd ∈RN×dr is designed to be matrix with each line Pl
d,i being a dimension indicate vector,
669"
REFERENCES,0.4783878504672897,"which starts with continuous 1s while the others be 0s. In practice, to keep the differentiability, it’s
670"
REFERENCES,0.47897196261682246,"""soften"" as follows:
671"
REFERENCES,0.47955607476635514,"ˆPl
d = cumsum←

softmax

f l
ξ

Zl−1, ˆAZl−1
,"
REFERENCES,0.48014018691588783,"Pl
d = SOFTOR(Pl−1
d
, ˆPl
d),
(39)"
REFERENCES,0.4807242990654206,"where f l
ξ is a learnable layer that fuses two messages.
672"
REFERENCES,0.48130841121495327,"It can be revisited by HTMP with the following components:
673"
REFERENCES,0.48189252336448596,"A = [I, A],
B = [I, Bd],
Wl
0 = Wl
1 = I,
eZl
0 = (I ⊙I)Zl−1 = Zl−1,
eZl
1 = (A ⊙Bd)Zl−1 = ˆAZl−1, (40)"
REFERENCES,0.4824766355140187,"where the COMBINE function is concatenation with node-wise adaptive dimensions. Specifically, in
674"
REFERENCES,0.4830607476635514,"each layer, OrderedGNN aggregates messages from node ego and raw neighborhood and concatenates
675"
REFERENCES,0.48364485981308414,"them with learnable dimensions. Combined with the multi-layer architecture, this approach can
676"
REFERENCES,0.4842289719626168,"aggregate messages from neighbors of different hops and combine them not only with adaptive
677"
REFERENCES,0.4848130841121495,"contributions but also as separately as possible.
678"
REFERENCES,0.48539719626168226,"A.3
Analysis and Advice for Designing Models
679"
REFERENCES,0.48598130841121495,"The HTMP mechanism splits the message-passing mechanism of HTGNNs into multiple modules,
680"
REFERENCES,0.48656542056074764,"establishing connections among methods. For instance, most message passing in HTGNNs have
681"
REFERENCES,0.4871495327102804,"personalized processing for nodes. Some methods [24, 11, 13, 22] utilize the learnable aggregation
682"
REFERENCES,0.4877336448598131,"guidance and some others [14, 18, 21, 23] count on learnable COMBINE functions. Though
683"
REFERENCES,0.4883177570093458,"neighborhood redefining is commonly used in HTGNNs, there are also many methods [24, 11, 18,
684"
REFERENCES,0.4889018691588785,"20, 21] using only raw neighborhoods to handle heterophily and achieve good performance. Degree
685"
REFERENCES,0.4894859813084112,"averaging, which plays the role of a low-pass filter to capture the smooth signals, can still work well
686"
REFERENCES,0.49007009345794394,"in many HTGNNs [12, 14–16, 20]. High-order neighbor information may be helpful in heterophilous
687"
REFERENCES,0.49065420560747663,"graphs. Existing HTGNNs utilize it in two ways: directly defining high-order [12, 13, 16, 17] or
688"
REFERENCES,0.4912383177570093,"even full-connected [19] neighborhood indicators and by the multi-layer architecture of message
689"
REFERENCES,0.49182242990654207,"passing [20, 21].
690"
REFERENCES,0.49240654205607476,"With the aid of HTMP, we can revisit existing methods from a unified and comprehensible perspective.
691"
REFERENCES,0.4929906542056075,"An obvious observation is that the coordination among designs is important while good combinations
692"
REFERENCES,0.4935747663551402,"with easy designs can also achieve wonderful results. For instance, in ACM-GCN [18], the separation
693"
REFERENCES,0.4941588785046729,"and adaptive addition of ego, low-frequency, and high-frequency messages can accommodate the
694"
REFERENCES,0.4947429906542056,"personalized conditions of each node. OrderedGNN’s design [21], which includes an adaptive
695"
REFERENCES,0.4953271028037383,"connection mechanism, ego/neighbor separation, and multi-layer architecture, allows discrete and
696"
REFERENCES,0.495911214953271,"adaptive combinations of messages from multi-hop neighborhoods. This advises us to take into
697"
REFERENCES,0.49649532710280375,"account all components simultaneously when designing models. As an illustration, please be cautious
698"
REFERENCES,0.49707943925233644,"about using multiple learnable components. Also, here are some additional model design tips and
699"
REFERENCES,0.4976635514018692,"considerations. Please separate the messages from node ego and neighbors. When combining them
700"
REFERENCES,0.4982476635514019,"afterward, whether by weighted addition or concatenation, this approach is at least harmless if not
701"
REFERENCES,0.49883177570093457,"beneficial, especially when dealing with heterophilous graphs. Last but not least, try to design a
702"
REFERENCES,0.4994158878504673,"model capable of personalized handling different nodes. Available components include but are not
703"
REFERENCES,0.5,"limited to, custom-defined neighborhood indicators, aggregation guidance with adaptive weights or
704"
REFERENCES,0.5005841121495327,"estimated relationships, and learnable COMBINE functions. This is to accommodate the diversity
705"
REFERENCES,0.5011682242990654,"and sparsity of neighborhoods that nodes in real-world graphs may have.
706"
REFERENCES,0.5017523364485982,"B
Related Works
707"
REFERENCES,0.5023364485981309,"Homophilous Graph Neural Networks. Graph Neural Networks (GNNs) have showcased impres-
708"
REFERENCES,0.5029205607476636,"sive capabilities in handling graph-structured data. Traditional GNNs are predominantly founded
709"
REFERENCES,0.5035046728971962,"on the assumption of homophily, broadly categorized into two classes: spectral-based GNNs and
710"
REFERENCES,0.5040887850467289,"spatial-based GNNs. Firstly, spectral-based GNNs acquire node representations through graph
711"
REFERENCES,0.5046728971962616,"convolution operations employing diverse graph filters [1, 32, 33]. Secondly, spatial-based meth-
712"
REFERENCES,0.5052570093457944,"ods gather information from neighbors and update the representation of central nodes through the
713"
REFERENCES,0.5058411214953271,"message-passing mechanism [26, 28, 34]. Moreover, for a more comprehensive understanding of
714"
REFERENCES,0.5064252336448598,"existing homophilous GNNs, several unified frameworks [35, 36] have been proposed. Ma et al. [35]
715"
REFERENCES,0.5070093457943925,"propose that the aggregation process in some representative homophilous GNNs can be regarded
716"
REFERENCES,0.5075934579439252,"as solving a graph denoising problem with a smoothness assumption. Zhu et al. [36] establishes
717"
REFERENCES,0.508177570093458,"a connection between various message-passing mechanisms and a unified optimization problem.
718"
REFERENCES,0.5087616822429907,"However, these methods have limitations, as the aggregated representations may lose discriminability
719"
REFERENCES,0.5093457943925234,"when heterophilous neighbors dominate [11, 12].
720"
REFERENCES,0.509929906542056,"Heterophilous Graph Neural Networks. Recently, some heterophilous GNNs have emerged to
721"
REFERENCES,0.5105140186915887,"tackle the heterophily problem [11–23]. Firstly, a commonly adopted strategy involves expanding the
722"
REFERENCES,0.5110981308411215,"neighborhood with higher homophily or richer messages, such as high order neighborhooods [12, 13],
723"
REFERENCES,0.5116822429906542,"feature-similarity-based neighborhoods [13, 14], and custom-defined neighborhoods [15, 22]. Sec-
724"
REFERENCES,0.5122663551401869,"ondly, some approaches [11, 17–19, 23] aim to leverage information from heterophilous neighbors,
725"
REFERENCES,0.5128504672897196,"considering that not all heterophily is detrimental et al.[6]. Thirdly, some methods [12, 16, 20, 21]
726"
REFERENCES,0.5134345794392523,"adapt to heterophily by extending the combine function in message passing, creating variations for
727"
REFERENCES,0.514018691588785,"addition and concatenation. On this basis, several works have reviewed existing heterophilous
728"
REFERENCES,0.5146028037383178,"methods. Zheng et al. [8] and Zhu et al. [9] identifies effective designs in heterophilous GNNs and
729"
REFERENCES,0.5151869158878505,"analyzes the relationship between heterophily and graph-related issues. Gong et al. [10] provide
730"
REFERENCES,0.5157710280373832,"a higher-level perspective on learning heterophilous graphs, summarizing and classifying existing
731"
REFERENCES,0.5163551401869159,"methods based on learning strategies, architectures, and applications. However, these reviews merely
732"
REFERENCES,0.5169392523364486,"classify and list methods hierarchically, lacking unified understandings and not exploring the reason
733"
REFERENCES,0.5175233644859814,"behind the effectiveness of message passing in heterophilous graphs.
734"
REFERENCES,0.518107476635514,"C
The Detail of Experiments on Synthetic Datasets
735"
REFERENCES,0.5186915887850467,"To explore the performance impact of homophily level, node degrees and compatibility matrix (CMs)
736"
REFERENCES,0.5192757009345794,"on simple GNNs, we conduct some experiments on synthetic datasets.
737"
REFERENCES,0.5198598130841121,"C.1
Synthetic Datasets
738"
REFERENCES,0.5204439252336449,"We construct synthetic graphs considering the factors of homophily, CMs and degrees. For homophily,
739"
REFERENCES,0.5210280373831776,"we set 3 levels including Lowh (0.2), Midh (0.5), and Highh (0.8). For CMs, we set two levels of
740"
REFERENCES,0.5216121495327103,"discriminability, including Easy and Hard. For degrees, we set two levels including Lowdeg (4)
741"
REFERENCES,0.522196261682243,"and Highdeg (18). Note that with a certain homophily level, we can only control the non-diagonal
742"
REFERENCES,0.5227803738317757,"elements of CMs. Thus, there are a total of 12 synthetic graphs following the above settings. These
743"
REFERENCES,0.5233644859813084,"synthetic graphs are based on the Cora dataset, which provides node features and labels, which means,
744"
REFERENCES,0.5239485981308412,"only the edges are constructed. We visualize the CMs of these graphs in Figure 3. Since there is no
745"
REFERENCES,0.5245327102803738,"significant difference in CMs between low-degree and high-degree, we only plot the high-degree
746"
REFERENCES,0.5251168224299065,"ones. Further, the edges are randomly constructed under the guidance of these CMs and degrees to
747"
REFERENCES,0.5257009345794392,"form the synthetic graphs.
748"
REFERENCES,0.5262850467289719,"C.2
Experiments on Synthetic Datasets
749"
REFERENCES,0.5268691588785047,"We use GCN to analyze the performance impact of the above factors. The semi-supervised node
750"
REFERENCES,0.5274532710280374,"classification performance of GCN is shown in Table 5 while the baseline performance of MLP (72.54
751"
REFERENCES,0.5280373831775701,"± 2.18) is the same among these datasets since their difference is only on edges. From these results,
752"
REFERENCES,0.5286214953271028,"we have some observations: (1) high homophily is not necessary, GCN can also work well on low
753"
REFERENCES,0.5292056074766355,"homophily but discriminative CM; (2) low degrees have a negative impact on performance, especially
754"
REFERENCES,0.5297897196261683,"when the CMs are relatively weak discriminative, this also indicates that nodes with lower degrees
755"
REFERENCES,0.530373831775701,"are more likely to have confused neighborhoods; and (3) when dealing with nodes with confused
756"
REFERENCES,0.5309579439252337,"neighborhoods, GCN may contaminate central nodes with their neighborhoods’ messages, which
757"
REFERENCES,0.5315420560747663,"0
1
2
3
4
5
6"
REFERENCES,0.532126168224299,"0
1
2
3
4
5
6"
REFERENCES,0.5327102803738317,"0.80
0.10
0.00
0.00
0.00
0.00
0.10"
REFERENCES,0.5332943925233645,"0.10
0.81
0.10
0.00
0.00
0.00
0.00"
REFERENCES,0.5338785046728972,"0.00
0.10
0.80
0.10
0.00
0.00
0.00"
REFERENCES,0.5344626168224299,"0.00
0.00
0.10
0.81
0.09
0.00
0.00"
REFERENCES,0.5350467289719626,"0.00
0.00
0.00
0.09
0.81
0.10
0.00"
REFERENCES,0.5356308411214953,"0.00
0.00
0.00
0.00
0.10
0.80
0.10"
REFERENCES,0.5362149532710281,"0.09
0.00
0.00
0.00
0.00
0.10
0.81"
REFERENCES,0.5367990654205608,highh_easy_highdeg 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.5373831775700935,"(a) Highh, Easy"
REFERENCES,0.5379672897196262,"0
1
2
3
4
5
6"
REFERENCES,0.5385514018691588,"0
1
2
3
4
5
6"
REFERENCES,0.5391355140186916,"0.51
0.25
0.00
0.00
0.00
0.00
0.24"
REFERENCES,0.5397196261682243,"0.25
0.50
0.25
0.00
0.00
0.00
0.00"
REFERENCES,0.540303738317757,"0.00
0.24
0.51
0.25
0.00
0.00
0.00"
REFERENCES,0.5408878504672897,"0.00
0.00
0.25
0.49
0.26
0.00
0.00"
REFERENCES,0.5414719626168224,"0.00
0.00
0.00
0.26
0.50
0.24
0.00"
REFERENCES,0.5420560747663551,"0.00
0.00
0.00
0.00
0.25
0.51
0.24"
REFERENCES,0.5426401869158879,"0.24
0.00
0.00
0.00
0.00
0.24
0.52"
REFERENCES,0.5432242990654206,midh_easy_highdeg 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.5438084112149533,"(b) Midh, Easy"
REFERENCES,0.544392523364486,"0
1
2
3
4
5
6"
REFERENCES,0.5449766355140186,"0
1
2
3
4
5
6"
REFERENCES,0.5455607476635514,"0.20
0.40
0.00
0.00
0.00
0.00
0.40"
REFERENCES,0.5461448598130841,"0.40
0.21
0.39
0.00
0.00
0.00
0.00"
REFERENCES,0.5467289719626168,"0.00
0.40
0.20
0.40
0.00
0.00
0.00"
REFERENCES,0.5473130841121495,"0.00
0.00
0.39
0.20
0.40
0.00
0.00"
REFERENCES,0.5478971962616822,"0.00
0.00
0.00
0.40
0.20
0.40
0.00"
REFERENCES,0.548481308411215,"0.00
0.00
0.00
0.00
0.40
0.20
0.41"
REFERENCES,0.5490654205607477,"0.39
0.00
0.00
0.00
0.00
0.41
0.20"
REFERENCES,0.5496495327102804,lowh_easy_highdeg 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.5502336448598131,"(c) Lowh, Easy"
REFERENCES,0.5508177570093458,"0
1
2
3
4
5
6"
REFERENCES,0.5514018691588785,"0
1
2
3
4
5
6"
REFERENCES,0.5519859813084113,"0.81
0.04
0.03
0.02
0.03
0.03
0.04"
REFERENCES,0.552570093457944,"0.04
0.80
0.04
0.03
0.03
0.03
0.03"
REFERENCES,0.5531542056074766,"0.03
0.04
0.81
0.04
0.03
0.03
0.03"
REFERENCES,0.5537383177570093,"0.03
0.04
0.04
0.81
0.04
0.03
0.03"
REFERENCES,0.554322429906542,"0.03
0.03
0.03
0.04
0.81
0.04
0.03"
REFERENCES,0.5549065420560748,"0.03
0.03
0.03
0.02
0.04
0.81
0.04"
REFERENCES,0.5554906542056075,"0.04
0.03
0.03
0.03
0.03
0.04
0.81"
REFERENCES,0.5560747663551402,highh_hard_highdeg 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.5566588785046729,"(d) Highh, Hard"
REFERENCES,0.5572429906542056,"0
1
2
3
4
5
6"
REFERENCES,0.5578271028037384,"0
1
2
3
4
5
6"
REFERENCES,0.5584112149532711,"0.50
0.11
0.07
0.07
0.07
0.07
0.11"
REFERENCES,0.5589953271028038,"0.11
0.48
0.12
0.07
0.07
0.07
0.07"
REFERENCES,0.5595794392523364,"0.07
0.11
0.50
0.10
0.07
0.07
0.07"
REFERENCES,0.5601635514018691,"0.07
0.07
0.11
0.51
0.10
0.07
0.07"
REFERENCES,0.5607476635514018,"0.07
0.07
0.07
0.10
0.51
0.10
0.07"
REFERENCES,0.5613317757009346,"0.07
0.07
0.07
0.07
0.11
0.51
0.10"
REFERENCES,0.5619158878504673,"0.11
0.07
0.07
0.07
0.07
0.10
0.51"
REFERENCES,0.5625,midh_hard_highdeg 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.5630841121495327,"(e) Midh, Hard"
REFERENCES,0.5636682242990654,"0
1
2
3
4
5
6"
REFERENCES,0.5642523364485982,"0
1
2
3
4
5
6"
REFERENCES,0.5648364485981309,"0.18
0.22
0.10
0.10
0.09
0.09
0.22"
REFERENCES,0.5654205607476636,"0.21
0.19
0.21
0.10
0.10
0.09
0.10"
REFERENCES,0.5660046728971962,"0.09
0.22
0.18
0.22
0.09
0.10
0.10"
REFERENCES,0.5665887850467289,"0.09
0.10
0.21
0.19
0.22
0.10
0.09"
REFERENCES,0.5671728971962616,"0.09
0.10
0.09
0.22
0.19
0.21
0.09"
REFERENCES,0.5677570093457944,"0.09
0.09
0.09
0.10
0.22
0.18
0.23"
REFERENCES,0.5683411214953271,"0.22
0.10
0.09
0.10
0.09
0.21
0.19"
REFERENCES,0.5689252336448598,lowh_hard_highdeg 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.5695093457943925,"(f) Lowh, Hard"
REFERENCES,0.5700934579439252,Figure 3: The visualization of compatibility matrix on synthetic graphs.
REFERENCES,0.570677570093458,Table 5: Node classification accuracy of GCN on Synthetic Datasets.
REFERENCES,0.5712616822429907,"Factors
Highh, Esay
Highh, Hard
Midh, Easy
Midh, Hard
Lowh, easy
Lowh, Hard"
REFERENCES,0.5718457943925234,"Highd
99.15 ± 0.35
99.48 ± 0.24
86.42 ± 4.13
90.52 ± 1.05
89.34 ± 2.19
39.22 ± 2.34"
REFERENCES,0.572429906542056,"Lowd
89.98 ± 1.59
91.25 ± 0.85
70.85 ± 1.59
70.20 ± 1.41
56.46 ± 2.63
40.91 ± 1.75"
REFERENCES,0.5730140186915887,"leads to performance worse than MLP. This once again remind us the importance of ego/neighbor
758"
REFERENCES,0.5735981308411215,"separation.
759"
REFERENCES,0.5741822429906542,"D
Empirical Evidence for the Conjecture about CM
760"
REFERENCES,0.5747663551401869,"In this part, we show the empirical evidence for the conjecture about CM as mentioned in Sec 4.
761"
REFERENCES,0.5753504672897196,"Specifically, we plot the observed and desired CM of ACM-GCN and GPRGNN in Figure 4. The
762"
REFERENCES,0.5759345794392523,"results show that ACM-GCN and GPRGNN have enhanced the discriminability of CM, which can be
763"
REFERENCES,0.576518691588785,"empirical evidence for the conjecture.
764"
REFERENCES,0.5771028037383178,"The desired CMs are obtained as follows: For ACM-GCN, we leverage the learned weights in the
765"
REFERENCES,0.5776869158878505,"COMBINE function to rebuild a weighted adjacency matrix Aacm based on the low-pass filter ˆA
766"
REFERENCES,0.5782710280373832,"and high-pass filter I −ˆA, then regard Aacm as the neighborhood and calculate the desired CM.
767"
REFERENCES,0.5788551401869159,"0
1
2
3
4"
REFERENCES,0.5794392523364486,"0
1
2
3
4"
REFERENCES,0.5800233644859814,"0.45
0.34
0.15
0.04
0.02"
REFERENCES,0.580607476635514,"0.24
0.45
0.22
0.06
0.02"
REFERENCES,0.5811915887850467,"0.17
0.38
0.30
0.11
0.04"
REFERENCES,0.5817757009345794,"0.13
0.32
0.31
0.17
0.08"
REFERENCES,0.5823598130841121,"0.14
0.27
0.27
0.18
0.12"
REFERENCES,0.5829439252336449,Real M 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.5835280373831776,(a) Observed
REFERENCES,0.5841121495327103,"0
1
2
3
4"
REFERENCES,0.584696261682243,"0
1
2
3
4"
REFERENCES,0.5852803738317757,"0.68
0.20
0.09
0.02
0.01"
REFERENCES,0.5858644859813084,"0.14
0.68
0.13
0.04
0.01"
REFERENCES,0.5864485981308412,"0.10
0.22
0.59
0.07
0.02"
REFERENCES,0.5870327102803738,"0.08
0.19
0.18
0.51
0.04"
REFERENCES,0.5876168224299065,"0.08
0.15
0.16
0.10
0.50 M 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.5882009345794392,(b) ACM-GCN
REFERENCES,0.5887850467289719,"0
1
2
3
4"
REFERENCES,0.5893691588785047,"0
1
2
3
4"
REFERENCES,0.5899532710280374,"0.51
0.28
0.14
0.04
0.02"
REFERENCES,0.5905373831775701,"0.20
0.51
0.19
0.07
0.03"
REFERENCES,0.5911214953271028,"0.15
0.30
0.40
0.10
0.04"
REFERENCES,0.5917056074766355,"0.13
0.26
0.25
0.29
0.07"
REFERENCES,0.5922897196261683,"0.15
0.22
0.23
0.13
0.26 M 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.592873831775701,(c) GPRGNN
REFERENCES,0.5934579439252337,Figure 4: The visualization of compatibility matrix on Amazon-Ratings.
REFERENCES,0.5940420560747663,"For GPRGNN, we utilize the leaned weights in the FUSE function to rebuild a weighted adjacency
768"
REFERENCES,0.594626168224299,"matrix Agpr based on the multi-hop adjacency matrixes [I, A, A2, ..., Ak] then regard Agpr as the
769"
REFERENCES,0.5952102803738317,"neighborhood and calculate the desired CM.
770"
REFERENCES,0.5957943925233645,"E
Additional Detailed Implementation of CMGNN
771"
REFERENCES,0.5963785046728972,"Overall Message Passing Mechanism. The overall message passing mechanism in CMGNN is
772"
REFERENCES,0.5969626168224299,"formatted as follows:
773"
REFERENCES,0.5975467289719626,"Zl = diag(αl
0)Zl−1Wl
0 + diag(αl
1) ˆAZl−1Wl
1 + diag(αl
2)(Asup ⊙Bsup)Zl−1Wl
2,"
REFERENCES,0.5981308411214953,"Z =
L
∥
l=0
Zl,
(41)"
REFERENCES,0.5987149532710281,"where diag(αl
0), diag(αl
1), diag(αl
2)RN×1 are the learned combination weights introduced below.
774"
REFERENCES,0.5992990654205608,"COMBNIE Function with Adaptive Weights. Firstly, we list the aggregated messages eZl
r from 3
775"
REFERENCES,0.5998831775700935,"neighborhoods:
776"
REFERENCES,0.6004672897196262,"eZl
0 = Zl−1Wl
0, eZl
1 = ˆAZl−1Wl
1,
eZl
2 = (Asup ⊙Bsup)Zl−1Wl
2.
(42)"
REFERENCES,0.6010514018691588,"The combination weights are learned by an MLP with Softmax:
777"
REFERENCES,0.6016355140186916,"[αl
0, αl
1, αl
2] = Softmax(Sigmoid([Zl
0∥Zl
1∥Zl
2∥d]Wl
att)Wl
mix),
(43)"
REFERENCES,0.6022196261682243,"where Wl
att ∈R(3dr+1)×3 and Wl
mix ∈R3×3 are two learnable weight matrixes, d is the node
778"
REFERENCES,0.602803738317757,"degrees which may be helpful to weights learning.
779"
REFERENCES,0.6033878504672897,"The Message Passing of Supplementary Prototypes. In practice, the virtual prototype nodes are
780"
REFERENCES,0.6039719626168224,"viewed as additional nodes, which have the same message passing mechanism as real nodes:
781"
REFERENCES,0.6045560747663551,"Zptt,l = diag(αptt,l
0
)Zptt,l−1Wl
0 + diag(αptt,l
1
) ˆApttZptt,l−1Wl
1
+ diag(αptt,l
2
)(Aptt,sup ⊙Bptt,sup)Zptt,l−1Wl
2,"
REFERENCES,0.6051401869158879,"Zptt =
L
∥
l=0
Zptt,l, (44)"
REFERENCES,0.6057242990654206,"where Asup,ptt = 1 ∈RK×K and Bsup,ptt = ˆCptt ˆM are similar with those of real nodes.
782"
REFERENCES,0.6063084112149533,"Update Strategy for the Estimation of the Compatibility Matrix. For the sake of efficiency, we do
783"
REFERENCES,0.606892523364486,"not estimate the compatibility matrix in each epoch. Instead, we save it as fixed parameters and only
784"
REFERENCES,0.6074766355140186,"update it when the evaluation performance is improved during the training.
785"
REFERENCES,0.6080607476635514,"Predition of CMGNN. CMGNN leverages the prediction of the model during message passing. For
786"
REFERENCES,0.6086448598130841,"initialization, nodes have the same probabilities belonging to each class. During the message passing,
787"
REFERENCES,0.6092289719626168,"the prediction soft label ˆC is replaced by the output of CMGNN, formatted as follow:
788"
REFERENCES,0.6098130841121495,"ˆC = CLA((Z)),
(45)"
REFERENCES,0.6103971962616822,"where CLA is a classifier implemented by an MLP and Z is the final node representations.
789"
REFERENCES,0.610981308411215,"F
More Detail about the Benchmark
790"
REFERENCES,0.6115654205607477,"In this section, we describe the details of the new benchmarks, including (i) the reason why we need
791"
REFERENCES,0.6121495327102804,"a new benchmark: drawbacks of existing datasets; (ii) detailed descriptions of new datasets; (iii)
792"
REFERENCES,0.6127336448598131,"baseline methods and the codebase; and (iv) details of obtaining benchmark performance.
793"
REFERENCES,0.6133177570093458,"F.1
Drawbacks in Existing Datasets
794"
REFERENCES,0.6139018691588785,"As mentioned in [30], the widely used datasets Cornell, Texas, and Wisconsin2 have a too small scale
795"
REFERENCES,0.6144859813084113,"for evaluation. Further, the original datasets Chameleon and Squirrel have an issue of data leakage,
796"
REFERENCES,0.615070093457944,2https://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-11/www/wwkb
REFERENCES,0.6156542056074766,"where some nodes may occur simultaneously in both training and testing sets. Then, the splitting
797"
REFERENCES,0.6162383177570093,"ratio of training, validation, and testing sets are different across various datasets, which is ignored in
798"
REFERENCES,0.616822429906542,"previous works.
799"
REFERENCES,0.6174065420560748,"Therefore, to build a comprehensive and fair benchmark for model effectiveness evaluation, we
800"
REFERENCES,0.6179906542056075,"will newly organize 10 datasets with unified splitting across various homophily values in the next
801"
REFERENCES,0.6185747663551402,"Subsection F.2.
802"
REFERENCES,0.6191588785046729,"F.2
New Datasets
803"
REFERENCES,0.6197429906542056,"In our benchmark, we adopt ten different types of publicly available datasets with a unified splitting
804"
REFERENCES,0.6203271028037384,"setting (48%/32%/20% for training/validation/testing) for fair model comparison, including Roman-
805"
REFERENCES,0.6209112149532711,"Empire [30], Amazon-Ratings [30], Chameleon-F [30], Squirrel-F [30], Actor [15], Flickr [37],
806"
REFERENCES,0.6214953271028038,"BlogCatalog [37], Wikics [38], Pubmed [39], and Photo [29]. The datasets have a variety of
807"
REFERENCES,0.6220794392523364,"homophily values from low to high. The statistics and splitting of these datasets are shown in Table 6.
808"
REFERENCES,0.6226635514018691,"The detailed description of the datasets is as follows:
809"
REFERENCES,0.6232476635514018,Table 6: Statistics and splitting of the experimental benchmark datasets.
REFERENCES,0.6238317757009346,"Dataset
Nodes
Edges
Attributes
Classes
Avg. Degree
Undirected
Homophily
Train / Valid / Test"
REFERENCES,0.6244158878504673,"Roman-Empire
22,662
65,854
300
18
2.9
!
0.05
10,877 / 7,251 / 4,534
Amazon-Ratings
24,492
186,100
300
5
7.6
!
0.38
11,756 / 7,837 / 4,899
Chameleon-F
890
13,584
2,325
5
15.3
%
0.25
427 / 284 / 179
Squirrel-F
2,223
65,718
2,089
5
29.6
%
0.22
1,067 / 711 / 445
Actor
7,600
30,019
932
5
3.9
%
0.22
3,648 / 2,432 / 1,520
Flickr
7,575
479,476
12,047
9
63.3
!
0.24
3,636 / 2,424 / 1,515
BlogCatalog
5,196
343,486
8,189
6
66.1
!
0.40
2,494 / 1,662 / 1,040
Wikics
11,701
431,206
300
10
36.9
!
0.65
5,616 / 3,744 / 2,341
Pubmed
19,717
88,651
500
3
4.5
!
0.80
9,463 / 6,310 / 3,944
Photo
7,650
238,162
745
8
31.1
!
0.83
3,672 / 2,448 / 1,530"
REFERENCES,0.625,"• Roman-Empire3 [30] is derived from the extensive article on the Roman Empire found on the
810"
REFERENCES,0.6255841121495327,"English Wikipedia, chosen for its status as one of the most comprehensive entries on the platform.
811"
REFERENCES,0.6261682242990654,"It contains 22,662 nodes and 65,854 edges between nodes. Each node represents an individual word
812"
REFERENCES,0.6267523364485982,"from the text, with the total number of nodes mirroring the length of the article. An edge between
813"
REFERENCES,0.6273364485981309,"two nodes is established under one of two conditions: the words are sequential in the text or they
814"
REFERENCES,0.6279205607476636,"are linked in the sentence’s dependency tree, indicating a grammatical relationship where one word
815"
REFERENCES,0.6285046728971962,"is syntactically dependent on the other. Consequently, the graph is structured as a chain graph,
816"
REFERENCES,0.6290887850467289,"enriched with additional edges that represent these syntactic dependencies. The graph encompasses
817"
REFERENCES,0.6296728971962616,"a total of 18 distinct node classes, with each node being equipped with 300-dimensional attributes
818"
REFERENCES,0.6302570093457944,"obtained by fastText word embeddings [40].
819"
REFERENCES,0.6308411214953271,"• Amazon-Ratings3 [30] is sourced from the Amazon product co-purchasing network metadata
820"
REFERENCES,0.6314252336448598,"dataset [41]. It contains 24,492 nodes and 186,100 edges between nodes. The nodes within
821"
REFERENCES,0.6320093457943925,"this graph represent products, encompassing a variety of categories such as books, music CDs,
822"
REFERENCES,0.6325934579439252,"DVDs, and VHS video tapes. An edge between nodes signifies that the respective products are
823"
REFERENCES,0.633177570093458,"often purchased together. The objection is to forecast the average rating assigned to a product by
824"
REFERENCES,0.6337616822429907,"reviewers, with the ratings being categorized into five distinct classes. For the purpose of node
825"
REFERENCES,0.6343457943925234,"feature representation, we have utilized the 300-dimensional mean values derived from fastText
826"
REFERENCES,0.634929906542056,"word embeddings [40], extracted from the textual descriptions of the products.
827"
REFERENCES,0.6355140186915887,"• Chameleon-F and Squirrel-F3 [30] are specialized collections of Wikipedia page-to-page net-
828"
REFERENCES,0.6360981308411215,"works [42], of which the data leakage nodes are filtered out by [30]. Within these datasets, each
829"
REFERENCES,0.6366822429906542,"node symbolizes a web page, and edges denote the mutual hyperlinks that connect them. The
830"
REFERENCES,0.6372663551401869,"node features are derived from a selection of informative nouns extracted directly from Wikipedia
831"
REFERENCES,0.6378504672897196,"articles. For the purpose of classification, nodes are categorized into five distinct groups based
832"
REFERENCES,0.6384345794392523,"on the average monthly web traffic they receive. Specifically, Chameleon-F contains 890 nodes
833"
REFERENCES,0.639018691588785,"and 13,584 edges between nodes, with each node being equipped with 2,325-dimensional features.
834"
REFERENCES,0.6396028037383178,"Squirrel-F contains 2,223 nodes and 65,718 edges between nodes, with each node being equipped
835"
REFERENCES,0.6401869158878505,"with a 2,089-dimensional feature vector.
836"
REFERENCES,0.6407710280373832,3https://github.com/yandex-research/heterophilous-graphs/tree/main/data
REFERENCES,0.6413551401869159,"• Actor4 [15] is an actor-centric induced subgraph derived from the broader film-director-actor-writer
837"
REFERENCES,0.6419392523364486,"network, as originally presented by [43]. In this refined network, each node corresponds to an
838"
REFERENCES,0.6425233644859814,"individual actor, and the edges signify the co-occurrence of these actors on the same Wikipedia
839"
REFERENCES,0.643107476635514,"page. The node features are identified through the presence of certain keywords found within
840"
REFERENCES,0.6436915887850467,"the actors’ Wikipedia entries. For the purpose of classification, the actors are organized into five
841"
REFERENCES,0.6442757009345794,"distinct categories based on the words of the actor’s Wikipedia. Statistically, it contains 7,600
842"
REFERENCES,0.6448598130841121,"nodes and 30,019 edges between nodes, with each node being equipped with a 932-dimensional
843"
REFERENCES,0.6454439252336449,"feature vector.
844"
REFERENCES,0.6460280373831776,"• Flickr and Blogcatalog5 [37] are two datasets of social networks, originating from the blog-sharing
845"
REFERENCES,0.6466121495327103,"platform BlogCatalog and the photo-sharing platform Flickr, respectively. Within these datasets,
846"
REFERENCES,0.647196261682243,"nodes symbolize the individual users of the platforms, while links signify the followship relation-
847"
REFERENCES,0.6477803738317757,"ships that exist between them. In the context of social networks, users frequently create personalized
848"
REFERENCES,0.6483644859813084,"content, such as publishing blog posts or uploading and sharing photos with accompanying tag
849"
REFERENCES,0.6489485981308412,"descriptions. These textual contents are consequently treated as attributes associated with each
850"
REFERENCES,0.6495327102803738,"node. The classification objection is to predict the interest group of each user. Specifically, Flickr
851"
REFERENCES,0.6501168224299065,"contains 7,575 nodes and 479,476 edges between nodes. The graph encompasses a total of 9
852"
REFERENCES,0.6507009345794392,"distinct node classes, with each node being equipped with a 12047-dimensional attribute vector.
853"
REFERENCES,0.6512850467289719,"BlogCatalog contains 5,196 nodes and 343,486 edges between nodes. The graph encompasses a
854"
REFERENCES,0.6518691588785047,"total of 6 distinct node classes, with each node being equipped with 8189-dimensional attributes.
855"
REFERENCES,0.6524532710280374,"• Wikics6 [38] is a dataset curated from Wikipedia, specifically designed for benchmarking the
856"
REFERENCES,0.6530373831775701,"performance of GNNs. It is meticulously constructed around 10 distinct categories that represent
857"
REFERENCES,0.6536214953271028,"various branches of computer science, showcasing a high degree of connectivity. The node features
858"
REFERENCES,0.6542056074766355,"are extracted from the text of the associated Wikipedia articles, leveraging the power of pretrained
859"
REFERENCES,0.6547897196261683,"GloVe word embeddings [44]. These features are computed as the average of the word embeddings,
860"
REFERENCES,0.655373831775701,"yielding a comprehensive 300-dimensional representation for each node. The dataset encompasses
861"
REFERENCES,0.6559579439252337,"a substantial network of 11,701 nodes interconnected by 431,206 edges.
862"
REFERENCES,0.6565420560747663,"• Pubmed7 [39] is a classical citation network consisting of 19,717 scientific publications with
863"
REFERENCES,0.657126168224299,"44,338 links between them. The text contents of each publication are treated as their node attributes,
864"
REFERENCES,0.6577102803738317,"and thus each node is assigned a 500-dimensional attribute vector. The target is to predict which of
865"
REFERENCES,0.6582943925233645,"the paper categories each node belongs to, with a total of 3 candidate classes.
866"
REFERENCES,0.6588785046728972,"• Photo8 [29] is one of the Amazon subset network from [29]. Nodes in the graph represent goods
867"
REFERENCES,0.6594626168224299,"and edges represent that two goods are frequently bought together. Given product reviews as
868"
REFERENCES,0.6600467289719626,"bag-of-words node features, each node is assigned a 745-dimensional feature vector. The task is to
869"
REFERENCES,0.6606308411214953,"map goods to their respective product category. It contains 7,650 nodes and 238,162 edges between
870"
REFERENCES,0.6612149532710281,"nodes. The graph encompasses a total of 8 distinct product categories.
871"
REFERENCES,0.6617990654205608,"F.3
Baseline Methods and the Codebase
872"
REFERENCES,0.6623831775700935,"For comprehensive comparisons, we choose 13 representative homophilous and heterophilous GNNs
873"
REFERENCES,0.6629672897196262,"as baseline methods in the benchmark, including (i) Shallow base model: MLP; (ii) Homopihlous
874"
REFERENCES,0.6635514018691588,"GNNs: GCN, GAT, GCNII; and (iii) Heterophilous GNNs: H2GCN, MixHop, GBK-GNN, GGCN,
875"
REFERENCES,0.6641355140186916,"GloGNN, HOGGCN, GPR-GNN. Detailed descriptions of some of these methods can be seen in
876"
REFERENCES,0.6647196261682243,"Appendix A.2.
877"
REFERENCES,0.665303738317757,"To explore the performance of baseline methods on new datasets and facilitate future expansions,
878"
REFERENCES,0.6658878504672897,"we collect the official/reproduced codes from GitHub and integrate them into a unified codebase.
879"
REFERENCES,0.6664719626168224,"Specifically, all methods share the same data loaders and evaluation metrics. One can easily run
880"
REFERENCES,0.6670560747663551,"different methods with only parameters changing within the codebase. The codebase is based on the
881"
REFERENCES,0.6676401869158879,"PyTorch9 framework, supporting DGL10 and PyG11. Detailed usages of the codebase are available in
882"
REFERENCES,0.6682242990654206,"the Readme file of the codebase.
883"
REFERENCES,0.6688084112149533,"4https://github.com/bingzhewei/geom-gcn/tree/master/new_data/film
5https://github.com/TrustAGI-Lab/CoLA/tree/main/raw_dataset
6https://github.com/pmernyei/wiki-cs-dataset
7https://linqs.soe.ucsc.edu/datac
8https://github.com/shchur/gnn-benchmark
9https://pytorch.org
10https://www.dgl.ai
11https://www.pyg.org"
REFERENCES,0.669392523364486,"F.4
Details of Obtaining Benchmark Performance
884"
REFERENCES,0.6699766355140186,"Following the settings in existing methods, we construct 10 random splits (48%/32%/20% for
885"
REFERENCES,0.6705607476635514,"train/valid/test) for each dataset and report the average performance among 10 runs on them along
886"
REFERENCES,0.6711448598130841,"with the standard deviation.
887"
REFERENCES,0.6717289719626168,"For all baseline methods except MLP, GCN, and GAT, we conduct parameter searches within the
888"
REFERENCES,0.6723130841121495,"search space recommended by the original papers. The searches are based on the NNI framework
889"
REFERENCES,0.6728971962616822,"with an anneal strategy. We use Adam as the optimizer for all methods. Each method has dozens
890"
REFERENCES,0.673481308411215,"of search trails according to their time costs and the best performances are reported. The currently
891"
REFERENCES,0.6740654205607477,"known optimal parameters of each method are listed in the codebase. We run these experiments
892"
REFERENCES,0.6746495327102804,"on NVIDIA GeForce RTX 3090 GPU with 24G memory. The out-of-memory error during model
893"
REFERENCES,0.6752336448598131,"training is reported as OOM in Table 2.
894"
REFERENCES,0.6758177570093458,"G
More Details about Experiments
895"
REFERENCES,0.6764018691588785,"In this section, we describe the additional details of the experiments, including experimental settings
896"
REFERENCES,0.6769859813084113,"and results.
897"
REFERENCES,0.677570093457944,"G.1
Additional Experimental Settings
898"
REFERENCES,0.6781542056074766,"Our method has the same experimental settings within the benchmark, including datasets, splits,
899"
REFERENCES,0.6787383177570093,"evaluations, hardware, optimizer and so on as in Appendix F.4.
900"
REFERENCES,0.679322429906542,"Parameters Search Space. We list the search space of parameters in Table 7, where patience is for
901"
REFERENCES,0.6799065420560748,"early stopping, nhidden is the embedding dimension of hidden layers as well as the representation
902"
REFERENCES,0.6804906542056075,"dimension dr, relu_varient decides ReLU applying before message aggregation or not as in ACM-
903"
REFERENCES,0.6810747663551402,"GCN, structure_info determines whether to use structure information as supplement node features or
904"
REFERENCES,0.6816588785046729,"not.
905"
REFERENCES,0.6822429906542056,Table 7: Parameters search space of our method.
REFERENCES,0.6828271028037384,"Parameters
Range"
REFERENCES,0.6834112149532711,"learning rate
{0.001, 0.005, 0.01, 0.05}
weight_decay
{0, 1e-7, 5e-7, 1e-6, 5e-6, 5e-5, 5e-4}
patience
{200, 400}"
REFERENCES,0.6839953271028038,"dropout
[0, 0.9]
λ
{0, 0.01, 0.1, 1, 10}
layers
{1, 2, 4, 8}
nhidden
{32, 64, 128, 256}
relu_variant
{True, False}
structure_info
{True, False}"
REFERENCES,0.6845794392523364,"Ablation Study. In the ablation study, there are three variants of our methods: without SM, without
906"
REFERENCES,0.6851635514018691,"DL, without SM and DL. For ""without SM"", we delete the supplementary messages during message
907"
REFERENCES,0.6857476635514018,"passing, using only messages from node ego and raw neighborhood for combination. For ""without
908"
REFERENCES,0.6863317757009346,"DL"", we simply set λ = 0 to delete the discrimination loss. For ""without SM and DL"", we just
909"
REFERENCES,0.6869158878504673,"combine the above two settings.
910"
REFERENCES,0.6875,"G.2
Additional Experimental Results
911"
REFERENCES,0.6880841121495327,"In this subsection, we show some additional experimental results and analysis.
912"
REFERENCES,0.6886682242990654,"G.2.1
Additional Results of CM Estimations
913"
REFERENCES,0.6892523364485982,"The additional visualizations of CM estimations are shown in Figure 5. As we can see, our method
914"
REFERENCES,0.6898364485981309,"can estimate quite accurate CMs among various homophily and class numbers, which provides a
915"
REFERENCES,0.6904205607476636,"good foundation for the construction of supplementary messages.
916"
REFERENCES,0.6910046728971962,"0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17"
REFERENCES,0.6915887850467289,"0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17"
REFERENCES,0.6921728971962616,"0.00
0.05
0.15
0.06
0.02
0.05
0.18
0.04
0.07
0.04
0.00
0.03
0.01
0.05
0.05
0.01
0.00
0.17"
REFERENCES,0.6927570093457944,"0.02
0.00
0.44
0.14
0.11
0.05
0.03
0.05
0.00
0.01
0.03
0.01
0.02
0.01
0.01
0.02
0.01
0.06"
REFERENCES,0.6933411214953271,"0.06
0.44
0.02
0.11
0.06
0.05
0.03
0.01
0.05
0.01
0.01
0.00
0.01
0.00
0.01
0.01
0.02
0.09"
REFERENCES,0.6939252336448598,"0.04
0.26
0.20
0.00
0.11
0.05
0.07
0.02
0.06
0.01
0.03
0.00
0.02
0.00
0.02
0.01
0.01
0.09"
REFERENCES,0.6945093457943925,"0.01
0.23
0.10
0.15
0.08
0.07
0.07
0.03
0.07
0.02
0.03
0.00
0.02
0.00
0.02
0.01
0.01
0.07"
REFERENCES,0.6950934579439252,"0.05
0.11
0.09
0.05
0.09
0.15
0.04
0.18
0.05
0.03
0.02
0.01
0.02
0.01
0.01
0.01
0.01
0.07"
REFERENCES,0.695677570093458,"0.20
0.09
0.06
0.11
0.09
0.05
0.02
0.02
0.02
0.03
0.03
0.03
0.03
0.00
0.00
0.01
0.10
0.12"
REFERENCES,0.6962616822429907,"0.07
0.18
0.02
0.05
0.09
0.35
0.03
0.00
0.05
0.03
0.02
0.00
0.02
0.00
0.01
0.01
0.01
0.06"
REFERENCES,0.6968457943925234,"0.11
0.00
0.18
0.12
0.11
0.09
0.02
0.04
0.00
0.01
0.02
0.01
0.01
0.00
0.00
0.03
0.07
0.17"
REFERENCES,0.697429906542056,"0.12
0.07
0.09
0.06
0.08
0.08
0.05
0.05
0.02
0.09
0.00
0.01
0.02
0.04
0.00
0.00
0.03
0.17"
REFERENCES,0.6980140186915887,"0.01
0.25
0.08
0.14
0.12
0.08
0.07
0.03
0.05
0.00
0.05
0.00
0.03
0.00
0.02
0.01
0.00
0.05"
REFERENCES,0.6985981308411215,"0.15
0.07
0.00
0.00
0.00
0.06
0.12
0.01
0.02
0.02
0.00
0.05
0.01
0.05
0.03
0.00
0.09
0.31"
REFERENCES,0.6991822429906542,"0.03
0.15
0.08
0.07
0.07
0.09
0.09
0.04
0.04
0.03
0.03
0.01
0.09
0.01
0.02
0.01
0.02
0.11"
REFERENCES,0.6997663551401869,"0.29
0.10
0.00
0.00
0.00
0.08
0.00
0.01
0.01
0.06
0.00
0.09
0.01
0.00
0.19
0.00
0.05
0.10"
REFERENCES,0.7003504672897196,"0.20
0.05
0.09
0.11
0.10
0.06
0.00
0.02
0.01
0.01
0.03
0.04
0.02
0.15
0.00
0.01
0.04
0.08"
REFERENCES,0.7009345794392523,"0.03
0.23
0.17
0.04
0.07
0.04
0.06
0.02
0.10
0.01
0.01
0.00
0.01
0.00
0.01
0.01
0.02
0.16"
REFERENCES,0.701518691588785,"0.00
0.12
0.12
0.04
0.02
0.02
0.24
0.01
0.14
0.04
0.00
0.06
0.02
0.02
0.03
0.01
0.00
0.10"
REFERENCES,0.7021028037383178,"0.10
0.10
0.13
0.08
0.05
0.05
0.05
0.02
0.06
0.04
0.01
0.04
0.03
0.01
0.01
0.03
0.02
0.18"
REFERENCES,0.7026869158878505,Real M 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.7032710280373832,(a) Roman-Empire Real
REFERENCES,0.7038551401869159,"0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17"
REFERENCES,0.7044392523364486,"0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17"
REFERENCES,0.7050233644859814,"0.03
0.06
0.15
0.06
0.02
0.06
0.19
0.05
0.06
0.05
0.00
0.03
0.01
0.06
0.03
0.01
0.00
0.15"
REFERENCES,0.705607476635514,"0.02
0.01
0.44
0.14
0.11
0.04
0.03
0.04
0.00
0.01
0.03
0.01
0.01
0.01
0.00
0.01
0.01
0.06"
REFERENCES,0.7061915887850467,"0.07
0.44
0.01
0.11
0.06
0.05
0.04
0.01
0.04
0.02
0.01
0.00
0.01
0.00
0.01
0.01
0.02
0.10"
REFERENCES,0.7067757009345794,"0.04
0.26
0.20
0.00
0.12
0.04
0.08
0.02
0.06
0.02
0.03
0.00
0.02
0.00
0.01
0.01
0.01
0.09"
REFERENCES,0.7073598130841121,"0.02
0.24
0.11
0.15
0.09
0.07
0.08
0.03
0.06
0.02
0.03
0.00
0.02
0.00
0.01
0.01
0.01
0.06"
REFERENCES,0.7079439252336449,"0.05
0.10
0.09
0.05
0.09
0.17
0.04
0.19
0.04
0.03
0.02
0.01
0.02
0.01
0.01
0.01
0.01
0.06"
REFERENCES,0.7085280373831776,"0.20
0.08
0.07
0.11
0.09
0.06
0.03
0.02
0.02
0.02
0.03
0.03
0.02
0.02
0.00
0.01
0.10
0.10"
REFERENCES,0.7091121495327103,"0.08
0.18
0.03
0.05
0.09
0.38
0.03
0.00
0.04
0.02
0.02
0.00
0.01
0.00
0.01
0.01
0.01
0.05"
REFERENCES,0.709696261682243,"0.10
0.01
0.18
0.13
0.12
0.08
0.03
0.04
0.01
0.01
0.03
0.01
0.01
0.00
0.00
0.03
0.06
0.15"
REFERENCES,0.7102803738317757,"0.13
0.08
0.11
0.06
0.08
0.08
0.05
0.04
0.02
0.10
0.00
0.01
0.01
0.04
0.00
0.00
0.03
0.16"
REFERENCES,0.7108644859813084,"0.01
0.24
0.06
0.15
0.12
0.08
0.09
0.02
0.05
0.01
0.05
0.00
0.04
0.00
0.01
0.01
0.00
0.05"
REFERENCES,0.7114485981308412,"0.19
0.07
0.01
0.00
0.00
0.06
0.16
0.01
0.02
0.02
0.00
0.06
0.01
0.07
0.02
0.00
0.07
0.23"
REFERENCES,0.7120327102803738,"0.04
0.15
0.08
0.09
0.09
0.10
0.08
0.04
0.04
0.02
0.05
0.01
0.07
0.02
0.01
0.01
0.01
0.10"
REFERENCES,0.7126168224299065,"0.32
0.10
0.01
0.00
0.00
0.06
0.10
0.01
0.00
0.06
0.00
0.08
0.02
0.00
0.09
0.00
0.04
0.10"
REFERENCES,0.7132009345794392,"0.23
0.05
0.10
0.10
0.10
0.05
0.01
0.02
0.01
0.01
0.03
0.03
0.02
0.14
0.00
0.01
0.03
0.06"
REFERENCES,0.7137850467289719,"0.04
0.21
0.17
0.04
0.07
0.04
0.08
0.02
0.09
0.01
0.01
0.00
0.01
0.00
0.00
0.02
0.02
0.16"
REFERENCES,0.7143691588785047,"0.01
0.12
0.14
0.04
0.02
0.02
0.26
0.01
0.12
0.04
0.00
0.04
0.01
0.02
0.01
0.01
0.01
0.12"
REFERENCES,0.7149532710280374,"0.10
0.11
0.15
0.08
0.05
0.05
0.06
0.02
0.06
0.04
0.01
0.02
0.02
0.01
0.01
0.03
0.02
0.16 Est M 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7155373831775701,(b) Roman-Empire Est
REFERENCES,0.7161214953271028,"0
1
2
3
4"
REFERENCES,0.7167056074766355,"0
1
2
3
4"
REFERENCES,0.7172897196261683,"0.22
0.22
0.11
0.12
0.34"
REFERENCES,0.717873831775701,"0.18
0.17
0.20
0.20
0.25"
REFERENCES,0.7184579439252337,"0.11
0.12
0.23
0.22
0.32"
REFERENCES,0.7190420560747663,"0.06
0.11
0.23
0.24
0.35"
REFERENCES,0.719626168224299,"0.07
0.08
0.20
0.24
0.41"
REFERENCES,0.7202102803738317,Real M 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.7207943925233645,(c) Chameleon-F Real
REFERENCES,0.7213785046728972,"0
1
2
3
4"
REFERENCES,0.7219626168224299,"0
1
2
3
4"
REFERENCES,0.7225467289719626,"0.28
0.16
0.10
0.10
0.36"
REFERENCES,0.7231308411214953,"0.29
0.14
0.19
0.14
0.24"
REFERENCES,0.7237149532710281,"0.14
0.10
0.27
0.18
0.31"
REFERENCES,0.7242990654205608,"0.07
0.06
0.19
0.26
0.42"
REFERENCES,0.7248831775700935,"0.11
0.06
0.20
0.25
0.38 Est M 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7254672897196262,(d) Chameleon-F Est
REFERENCES,0.7260514018691588,"0
1
2
3
4"
REFERENCES,0.7266355140186916,"0
1
2
3
4"
REFERENCES,0.7272196261682243,"0.17
0.18
0.17
0.20
0.29"
REFERENCES,0.727803738317757,"0.15
0.17
0.18
0.21
0.28"
REFERENCES,0.7283878504672897,"0.14
0.17
0.19
0.22
0.28"
REFERENCES,0.7289719626168224,"0.14
0.17
0.17
0.24
0.28"
REFERENCES,0.7295560747663551,"0.14
0.16
0.17
0.24
0.29"
REFERENCES,0.7301401869158879,Real M 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.7307242990654206,(e) Squirrel-F Real
REFERENCES,0.7313084112149533,"0
1
2
3
4"
REFERENCES,0.731892523364486,"0
1
2
3
4"
REFERENCES,0.7324766355140186,"0.25
0.14
0.12
0.22
0.28"
REFERENCES,0.7330607476635514,"0.26
0.15
0.13
0.20
0.25"
REFERENCES,0.7336448598130841,"0.27
0.16
0.13
0.20
0.24"
REFERENCES,0.7342289719626168,"0.29
0.16
0.12
0.20
0.23"
REFERENCES,0.7348130841121495,"0.28
0.15
0.13
0.20
0.25 Est M 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7353971962616822,(f) Squirrel-F Est
REFERENCES,0.735981308411215,"0
1
2
3
4"
REFERENCES,0.7365654205607477,"0
1
2
3
4"
REFERENCES,0.7371495327102804,"0.11
0.17
0.19
0.29
0.24"
REFERENCES,0.7377336448598131,"0.10
0.17
0.19
0.30
0.24"
REFERENCES,0.7383177570093458,"0.11
0.15
0.20
0.29
0.25"
REFERENCES,0.7389018691588785,"0.10
0.16
0.19
0.30
0.24"
REFERENCES,0.7394859813084113,"0.11
0.17
0.19
0.28
0.26"
REFERENCES,0.740070093457944,Real M 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.7406542056074766,(g) Actor Real
REFERENCES,0.7412383177570093,"0
1
2
3
4"
REFERENCES,0.741822429906542,"0
1
2
3
4"
REFERENCES,0.7424065420560748,"0.09
0.16
0.20
0.24
0.31"
REFERENCES,0.7429906542056075,"0.08
0.16
0.22
0.24
0.29"
REFERENCES,0.7435747663551402,"0.08
0.15
0.23
0.24
0.31"
REFERENCES,0.7441588785046729,"0.08
0.15
0.22
0.25
0.29"
REFERENCES,0.7447429906542056,"0.09
0.16
0.22
0.24
0.30 Est M 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7453271028037384,(h) Actor Est
REFERENCES,0.7459112149532711,"0
1
2
3
4
5
6
7
8"
REFERENCES,0.7464953271028038,"0
1
2
3
4
5
6
7
8"
REFERENCES,0.7470794392523364,"0.41
0.09
0.03
0.06
0.05
0.07
0.18
0.04
0.07"
REFERENCES,0.7476635514018691,"0.13
0.21
0.10
0.06
0.06
0.10
0.16
0.09
0.09"
REFERENCES,0.7482476635514018,"0.08
0.11
0.21
0.06
0.10
0.06
0.12
0.19
0.08"
REFERENCES,0.7488317757009346,"0.11
0.11
0.08
0.23
0.07
0.09
0.12
0.10
0.10"
REFERENCES,0.7494158878504673,"0.10
0.18
0.12
0.06
0.12
0.10
0.13
0.10
0.08"
REFERENCES,0.75,"0.15
0.11
0.05
0.06
0.07
0.17
0.18
0.13
0.08"
REFERENCES,0.7505841121495327,"0.17
0.11
0.05
0.06
0.06
0.09
0.29
0.09
0.09"
REFERENCES,0.7511682242990654,"0.09
0.11
0.10
0.06
0.08
0.09
0.16
0.22
0.10"
REFERENCES,0.7517523364485982,"0.10
0.10
0.06
0.06
0.06
0.08
0.13
0.08
0.33"
REFERENCES,0.7523364485981309,Real M 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.7529205607476636,(i) Flickr Real
REFERENCES,0.7535046728971962,"0
1
2
3
4
5
6
7
8"
REFERENCES,0.7540887850467289,"0
1
2
3
4
5
6
7
8"
REFERENCES,0.7546728971962616,"0.40
0.10
0.03
0.08
0.04
0.06
0.18
0.05
0.07"
REFERENCES,0.7552570093457944,"0.13
0.20
0.09
0.09
0.07
0.08
0.16
0.09
0.09"
REFERENCES,0.7558411214953271,"0.08
0.12
0.19
0.08
0.10
0.06
0.12
0.17
0.09"
REFERENCES,0.7564252336448598,"0.11
0.11
0.08
0.24
0.07
0.07
0.12
0.10
0.10"
REFERENCES,0.7570093457943925,"0.10
0.16
0.12
0.09
0.11
0.09
0.13
0.10
0.09"
REFERENCES,0.7575934579439252,"0.15
0.11
0.06
0.09
0.07
0.14
0.17
0.12
0.10"
REFERENCES,0.758177570093458,"0.17
0.11
0.05
0.08
0.05
0.08
0.28
0.09
0.09"
REFERENCES,0.7587616822429907,"0.09
0.12
0.10
0.09
0.08
0.07
0.15
0.21
0.10"
REFERENCES,0.7593457943925234,"0.11
0.10
0.06
0.08
0.06
0.08
0.12
0.08
0.31 Est M 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.759929906542056,(j) Flickr Est
REFERENCES,0.7605140186915887,"0
1
2
3
4
5
6
7
8
9"
REFERENCES,0.7610981308411215,"0
1
2
3
4
5
6
7
8
9"
REFERENCES,0.7616822429906542,"0.75
0.04
0.03
0.01
0.01
0.02
0.00
0.01
0.02
0.06"
REFERENCES,0.7622663551401869,"0.01
0.56
0.07
0.03
0.03
0.02
0.01
0.05
0.03
0.07"
REFERENCES,0.7628504672897196,"0.00
0.01
0.66
0.11
0.04
0.03
0.04
0.03
0.01
0.07"
REFERENCES,0.7634345794392523,"0.00
0.01
0.15
0.72
0.02
0.01
0.01
0.03
0.01
0.03"
REFERENCES,0.764018691588785,"0.00
0.01
0.09
0.03
0.71
0.05
0.01
0.03
0.02
0.02"
REFERENCES,0.7646028037383178,"0.00
0.01
0.10
0.03
0.09
0.64
0.01
0.04
0.03
0.03"
REFERENCES,0.7651869158878505,"0.00
0.01
0.25
0.06
0.04
0.02
0.54
0.03
0.01
0.02"
REFERENCES,0.7657710280373832,"0.00
0.04
0.11
0.07
0.06
0.04
0.02
0.51
0.03
0.06"
REFERENCES,0.7663551401869159,"0.00
0.03
0.11
0.03
0.06
0.08
0.01
0.08
0.43
0.12"
REFERENCES,0.7669392523364486,"0.00
0.03
0.12
0.05
0.02
0.02
0.01
0.02
0.02
0.70"
REFERENCES,0.7675233644859814,Real M 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.768107476635514,(k) Wikics Real
REFERENCES,0.7686915887850467,"0
1
2
3
4
5
6
7
8
9"
REFERENCES,0.7692757009345794,"0
1
2
3
4
5
6
7
8
9"
REFERENCES,0.7698598130841121,"0.73
0.04
0.05
0.02
0.02
0.01
0.00
0.01
0.03
0.09"
REFERENCES,0.7704439252336449,"0.02
0.66
0.10
0.02
0.03
0.01
0.01
0.05
0.03
0.09"
REFERENCES,0.7710280373831776,"0.00
0.01
0.74
0.08
0.04
0.02
0.03
0.02
0.01
0.06"
REFERENCES,0.7716121495327103,"0.00
0.00
0.15
0.76
0.01
0.01
0.01
0.02
0.00
0.03"
REFERENCES,0.772196261682243,"0.00
0.01
0.10
0.02
0.74
0.05
0.01
0.03
0.01
0.02"
REFERENCES,0.7727803738317757,"0.01
0.01
0.10
0.03
0.09
0.67
0.01
0.03
0.03
0.03"
REFERENCES,0.7733644859813084,"0.00
0.01
0.24
0.04
0.04
0.01
0.61
0.01
0.00
0.02"
REFERENCES,0.7739485981308412,"0.01
0.05
0.14
0.07
0.07
0.05
0.01
0.49
0.05
0.07"
REFERENCES,0.7745327102803738,"0.02
0.04
0.11
0.02
0.08
0.07
0.01
0.08
0.45
0.14"
REFERENCES,0.7751168224299065,"0.01
0.03
0.12
0.03
0.02
0.01
0.01
0.02
0.02
0.73 Est M 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7757009345794392,"(l) Wikics Est 0
1
2 0
1
2"
REFERENCES,0.7762850467289719,"0.69
0.12
0.19"
REFERENCES,0.7768691588785047,"0.05
0.85
0.10"
REFERENCES,0.7774532710280374,"0.08
0.13
0.79"
REFERENCES,0.7780373831775701,Real M 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.7786214953271028,"(m) Pubmed Real 0
1
2 0
1
2"
REFERENCES,0.7792056074766355,"0.72
0.12
0.17"
REFERENCES,0.7797897196261683,"0.04
0.88
0.07"
REFERENCES,0.780373831775701,"0.07
0.09
0.84 Est M 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7809579439252337,(n) Pubmed Est
REFERENCES,0.7815420560747663,"0
1
2
3
4
5
6
7"
REFERENCES,0.782126168224299,"0
1
2
3
4
5
6
7"
REFERENCES,0.7827102803738317,"0.89
0.04
0.00
0.02
0.00
0.00
0.04
0.00"
REFERENCES,0.7832943925233645,"0.01
0.83
0.00
0.09
0.01
0.00
0.01
0.02"
REFERENCES,0.7838785046728972,"0.00
0.01
0.95
0.00
0.02
0.00
0.00
0.00"
REFERENCES,0.7844626168224299,"0.01
0.13
0.00
0.71
0.03
0.00
0.05
0.06"
REFERENCES,0.7850467289719626,"0.00
0.03
0.01
0.05
0.79
0.00
0.10
0.02"
REFERENCES,0.7856308411214953,"0.00
0.00
0.01
0.00
0.00
0.96
0.00
0.00"
REFERENCES,0.7862149532710281,"0.01
0.01
0.00
0.02
0.03
0.00
0.86
0.05"
REFERENCES,0.7867990654205608,"0.01
0.06
0.00
0.14
0.02
0.00
0.22
0.55"
REFERENCES,0.7873831775700935,Real M 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.7879672897196262,(o) Photo Real
REFERENCES,0.7885514018691588,"0
1
2
3
4
5
6
7"
REFERENCES,0.7891355140186916,"0
1
2
3
4
5
6
7"
REFERENCES,0.7897196261682243,"0.90
0.04
0.00
0.01
0.00
0.00
0.04
0.00"
REFERENCES,0.790303738317757,"0.01
0.86
0.00
0.09
0.01
0.00
0.01
0.02"
REFERENCES,0.7908878504672897,"0.00
0.00
0.98
0.00
0.01
0.00
0.00
0.00"
REFERENCES,0.7914719626168224,"0.01
0.13
0.00
0.72
0.04
0.00
0.04
0.07"
REFERENCES,0.7920560747663551,"0.00
0.02
0.01
0.05
0.80
0.00
0.09
0.03"
REFERENCES,0.7926401869158879,"0.00
0.00
0.00
0.00
0.00
0.99
0.00
0.00"
REFERENCES,0.7932242990654206,"0.01
0.01
0.00
0.01
0.04
0.00
0.87
0.06"
REFERENCES,0.7938084112149533,"0.00
0.06
0.00
0.15
0.03
0.00
0.20
0.55 Est M 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.794392523364486,(p) Photo Est
REFERENCES,0.7949766355140186,Figure 5: The visualization of real and estimated CMs on other datasets.
REFERENCES,0.7955607476635514,"G.2.2
Additional Performance on Nodes with Various Levels of Degrees.
917"
REFERENCES,0.7961448598130841,"We show the additional performance on nodes with various degrees in Table 8. The results show that
918"
REFERENCES,0.7967289719626168,"CMGNN can achieve relatively good performance on low-degree nodes, especially on heterophilous
919"
REFERENCES,0.7973130841121495,"graphs. For the opposite results on homophilous graphs, we guess it may be due to the low-degree
920"
REFERENCES,0.7978971962616822,"nodes in homophilous graphs having a more discriminative semantic neighborhood, such as a one-hot
921"
REFERENCES,0.798481308411215,"form. On the contrary, there are relatively more high-degree nodes with confused neighborhoods due
922"
REFERENCES,0.7990654205607477,"to the randomness, which leads to the shown results on homophilous graphs.
923"
REFERENCES,0.7996495327102804,Table 8: Node classification accuracy comparison (%) among nodes with different degrees.
REFERENCES,0.8002336448598131,"Dataset
Roman-Empire
Chameleon-F
Actor"
REFERENCES,0.8008177570093458,"Deg. Prop.(%)
0∼20
20∼40
40∼60
60∼80
80∼100
0∼20
20∼40
40∼60
60∼80
80∼100
0∼20
20∼40
40∼60
60∼80
80∼100"
REFERENCES,0.8014018691588785,"Ours
88.60
87.00
85.59
86.25
74.33
40.73
45.28
56.02
46.64
39.93
35.56
37.14
38.40
36.03
36.84"
REFERENCES,0.8019859813084113,"ACM-GCN
79.00
77.87
73.52
72.09
53.77
39.51
41.21
52.25
45.80
47.09
34.48
36.58
36.27
34.63
37.46"
REFERENCES,0.802570093457944,"OrderedGNN
88.60
87.00
85.56
84.68
69.69
43.21
44.51
49.16
38.27
32.23
35.94
38.06
37.87
35.77
37.15"
REFERENCES,0.8031542056074766,"GCNII
86.79
85.14
85.20
84.75
71.09
34.84
42.56
47.50
40.45
41.84
36.89
37.20
38.53
38.02
36.99"
REFERENCES,0.8037383177570093,"Dataset
Squirrel
Pubmed
Photo"
REFERENCES,0.804322429906542,"Deg. Prop.(%)
0∼20
20∼40
40∼60
60∼80
80∼100
0∼20
20∼40
40∼60
60∼80
80∼100
0∼20
20∼40
40∼60
60∼80
80∼100"
REFERENCES,0.8049065420560748,"Ours
45.37
47.10
45.25
34.86
37.10
89.32
89.33
89.31
92.62
89.39
88.88
95.76
96.96
98.27
97.55"
REFERENCES,0.8054906542056075,"ACM-GCN
41.12
44.30
44.22
32.97
42.10
89.60
89.54
89.58
92.02
89.23
89.88
95.20
96.95
98.00
97.56"
REFERENCES,0.8060747663551402,"OrderedGNN
43.78
45.53
43.09
27.90
28.48
89.67
89.37
89.45
92.54
89.02
90.13
95.77
97.14
98.24
97.58"
REFERENCES,0.8066588785046729,"GCNII
43.08
45.55
43.65
33.07
38.05
89.77
89.50
89.24
92.45
88.86
88.89
95.36
97.12
97.83
96.64"
REFERENCES,0.8072429906542056,"G.2.3
Efficiency Study
924"
REFERENCES,0.8078271028037384,"Complexity Analysis. The number of learnable parameters in layer l of CMGNN is 3dr(dr + 1) + 9,
925"
REFERENCES,0.8084112149532711,"compared to drdr in GCN and 3dr(dr + 1) + 9 in ACM-GCN. The time complexity of layer l is
926"
REFERENCES,0.8089953271028038,"composed of 3 parts (i) AGGREGATE function: O(Ndr
2), O(Ndr
2+Mdr) and O(Ndr
2+NKdr)
927"
REFERENCES,0.8095794392523364,"for identity neighborhood, raw neighborhood and the supplementary neighborhood respectively, where
928"
REFERENCES,0.8101635514018691,"M = |E| denotes the number of edges; (ii) COMBINE function: O(3N(3dr +1)+12N) for adaptive
929"
REFERENCES,0.8107476635514018,"weights calculating and O(3N) for combination; (iii) FUSE function: O(1) for concatenations.
930"
REFERENCES,0.8113317757009346,"To this end, the time complexity of CMGNN is O(Ndr(3dr + K + 9) + Mdr + 18N + 1), or
931"
REFERENCES,0.8119158878504673,"O(Ndr
2 + Mdr) for brevity.
932"
REFERENCES,0.8125,"Experimental Running Time. we report the actual average running time (ms per epoch) of baseline
933"
REFERENCES,0.8130841121495327,"methods and CMGNN in Table 9 for comparison. The results demonstrate that CMGNN can balance
934"
REFERENCES,0.8136682242990654,"both performance effectiveness and running efficiency.
935"
REFERENCES,0.8142523364485982,"Table 9: Effiency study results of average model running time (ms/epoch). OOM denotes out-of-
memory error during the model training."
REFERENCES,0.8148364485981309,"Method
Roman-Empire
Amazon-Ratings
Chameleon-F
Squirrel-F
Actor
Flickr
BlogCatalog
Wikics
Pubmed
Photo"
REFERENCES,0.8154205607476636,"MLP
7.8
7.0
6.1
6.5
6.3
9.1
6.7
6.4
6.1
5.8
GCN
33.8
33.4
7.9
20.6
34.4
37.2
30.4
25.5
35.6
28.1
GAT
15.9
67.3
10.3
14.0
30.8
66.2
17.6
26.8
33.4
36.0
GCNII
29.4
28.4
37.3
19.6
37.7
84.2
97.6
20.7
258.0
46.9
H2GCN
20.0
31.2
17.2
32.4
55.6
415.7
165.5
332.8
39.0
87.6
MixHop
434.6
486.3
21.9
31.0
30.6
90.4
81.6
277.4
89.5
172.2
GBK-GNN
119.8
191.8
31.0
238.1
157.9
OOM
OOM
198.6
137.0
193.3
GGCN
OOM
OOM
55.7
42.1
199.8
111.2
108.7
226.6
2290.8
105.2
GloGNN
25.4
19.3
121.8
23.3
1292
562.9
30.9
1658.1
43.2
677.4
HOGGCN
OOM
OOM
25.2
54.3
1002.9
707.3
367.4
1406
OOM
655.3
GPR-GNN
15.9
12.5
22.3
23.2
16.7
15.9
14.7
49.8
13.2
13.1
ACM-GCN
56.7
56.7
26.1
29.7
22.5
60.7
31.7
42.4
37.1
40.1
OrderedGNN
86.0
110.8
49.5
60.1
67.8
107.0
88.3
116.9
88.1
78.2"
REFERENCES,0.8160046728971962,"CMGNN
51.5
93.5
62.5
64.7
19.0
52.5
69.8
44.0
102.9
20.4"
REFERENCES,0.8165887850467289,"NeurIPS Paper Checklist
936"
CLAIMS,0.8171728971962616,"1. Claims
937"
CLAIMS,0.8177570093457944,"Question: Do the main claims made in the abstract and introduction accurately reflect the
938"
CLAIMS,0.8183411214953271,"paper’s contributions and scope?
939"
CLAIMS,0.8189252336448598,"Answer: [Yes]
940"
CLAIMS,0.8195093457943925,"Justification: The contributions and scope of this paper are included in the abstract and
941"
CLAIMS,0.8200934579439252,"introduction.
942"
CLAIMS,0.820677570093458,"Guidelines:
943"
CLAIMS,0.8212616822429907,"• The answer NA means that the abstract and introduction do not include the claims
944"
CLAIMS,0.8218457943925234,"made in the paper.
945"
CLAIMS,0.822429906542056,"• The abstract and/or introduction should clearly state the claims made, including the
946"
CLAIMS,0.8230140186915887,"contributions made in the paper and important assumptions and limitations. A No or
947"
CLAIMS,0.8235981308411215,"NA answer to this question will not be perceived well by the reviewers.
948"
CLAIMS,0.8241822429906542,"• The claims made should match theoretical and experimental results, and reflect how
949"
CLAIMS,0.8247663551401869,"much the results can be expected to generalize to other settings.
950"
CLAIMS,0.8253504672897196,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
951"
CLAIMS,0.8259345794392523,"are not attained by the paper.
952"
LIMITATIONS,0.826518691588785,"2. Limitations
953"
LIMITATIONS,0.8271028037383178,"Question: Does the paper discuss the limitations of the work performed by the authors?
954"
LIMITATIONS,0.8276869158878505,"Answer: [Yes]
955"
LIMITATIONS,0.8282710280373832,"Justification: The limitations of this work are listed in the Sec 7.
956"
LIMITATIONS,0.8288551401869159,"Guidelines:
957"
LIMITATIONS,0.8294392523364486,"• The answer NA means that the paper has no limitation while the answer No means that
958"
LIMITATIONS,0.8300233644859814,"the paper has limitations, but those are not discussed in the paper.
959"
LIMITATIONS,0.830607476635514,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
960"
LIMITATIONS,0.8311915887850467,"• The paper should point out any strong assumptions and how robust the results are to
961"
LIMITATIONS,0.8317757009345794,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
962"
LIMITATIONS,0.8323598130841121,"model well-specification, asymptotic approximations only holding locally). The authors
963"
LIMITATIONS,0.8329439252336449,"should reflect on how these assumptions might be violated in practice and what the
964"
LIMITATIONS,0.8335280373831776,"implications would be.
965"
LIMITATIONS,0.8341121495327103,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
966"
LIMITATIONS,0.834696261682243,"only tested on a few datasets or with a few runs. In general, empirical results often
967"
LIMITATIONS,0.8352803738317757,"depend on implicit assumptions, which should be articulated.
968"
LIMITATIONS,0.8358644859813084,"• The authors should reflect on the factors that influence the performance of the approach.
969"
LIMITATIONS,0.8364485981308412,"For example, a facial recognition algorithm may perform poorly when image resolution
970"
LIMITATIONS,0.8370327102803738,"is low or images are taken in low lighting. Or a speech-to-text system might not be
971"
LIMITATIONS,0.8376168224299065,"used reliably to provide closed captions for online lectures because it fails to handle
972"
LIMITATIONS,0.8382009345794392,"technical jargon.
973"
LIMITATIONS,0.8387850467289719,"• The authors should discuss the computational efficiency of the proposed algorithms
974"
LIMITATIONS,0.8393691588785047,"and how they scale with dataset size.
975"
LIMITATIONS,0.8399532710280374,"• If applicable, the authors should discuss possible limitations of their approach to
976"
LIMITATIONS,0.8405373831775701,"address problems of privacy and fairness.
977"
LIMITATIONS,0.8411214953271028,"• While the authors might fear that complete honesty about limitations might be used by
978"
LIMITATIONS,0.8417056074766355,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
979"
LIMITATIONS,0.8422897196261683,"limitations that aren’t acknowledged in the paper. The authors should use their best
980"
LIMITATIONS,0.842873831775701,"judgment and recognize that individual actions in favor of transparency play an impor-
981"
LIMITATIONS,0.8434579439252337,"tant role in developing norms that preserve the integrity of the community. Reviewers
982"
LIMITATIONS,0.8440420560747663,"will be specifically instructed to not penalize honesty concerning limitations.
983"
THEORY ASSUMPTIONS AND PROOFS,0.844626168224299,"3. Theory Assumptions and Proofs
984"
THEORY ASSUMPTIONS AND PROOFS,0.8452102803738317,"Question: For each theoretical result, does the paper provide the full set of assumptions and
985"
THEORY ASSUMPTIONS AND PROOFS,0.8457943925233645,"a complete (and correct) proof?
986"
THEORY ASSUMPTIONS AND PROOFS,0.8463785046728972,"Answer: [Yes]
987"
THEORY ASSUMPTIONS AND PROOFS,0.8469626168224299,"Justification: We provide the formalization analysis in Appendix.
988"
THEORY ASSUMPTIONS AND PROOFS,0.8475467289719626,"Guidelines:
989"
THEORY ASSUMPTIONS AND PROOFS,0.8481308411214953,"• The answer NA means that the paper does not include theoretical results.
990"
THEORY ASSUMPTIONS AND PROOFS,0.8487149532710281,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
991"
THEORY ASSUMPTIONS AND PROOFS,0.8492990654205608,"referenced.
992"
THEORY ASSUMPTIONS AND PROOFS,0.8498831775700935,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
993"
THEORY ASSUMPTIONS AND PROOFS,0.8504672897196262,"• The proofs can either appear in the main paper or the supplemental material, but if
994"
THEORY ASSUMPTIONS AND PROOFS,0.8510514018691588,"they appear in the supplemental material, the authors are encouraged to provide a short
995"
THEORY ASSUMPTIONS AND PROOFS,0.8516355140186916,"proof sketch to provide intuition.
996"
THEORY ASSUMPTIONS AND PROOFS,0.8522196261682243,"• Inversely, any informal proof provided in the core of the paper should be complemented
997"
THEORY ASSUMPTIONS AND PROOFS,0.852803738317757,"by formal proofs provided in appendix or supplemental material.
998"
THEORY ASSUMPTIONS AND PROOFS,0.8533878504672897,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
999"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8539719626168224,"4. Experimental Result Reproducibility
1000"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8545560747663551,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
1001"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8551401869158879,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
1002"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8557242990654206,"of the paper (regardless of whether the code and data are provided or not)?
1003"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8563084112149533,"Answer: [Yes]
1004"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.856892523364486,"Justification: The details of the method and experimental settings are provided in Appendix.
1005"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8574766355140186,"Guidelines:
1006"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8580607476635514,"• The answer NA means that the paper does not include experiments.
1007"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8586448598130841,"• If the paper includes experiments, a No answer to this question will not be perceived
1008"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8592289719626168,"well by the reviewers: Making the paper reproducible is important, regardless of
1009"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8598130841121495,"whether the code and data are provided or not.
1010"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8603971962616822,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
1011"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.860981308411215,"to make their results reproducible or verifiable.
1012"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8615654205607477,"• Depending on the contribution, reproducibility can be accomplished in various ways.
1013"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8621495327102804,"For example, if the contribution is a novel architecture, describing the architecture fully
1014"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8627336448598131,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
1015"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8633177570093458,"be necessary to either make it possible for others to replicate the model with the same
1016"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8639018691588785,"dataset, or provide access to the model. In general. releasing code and data is often
1017"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8644859813084113,"one good way to accomplish this, but reproducibility can also be provided via detailed
1018"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.865070093457944,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
1019"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8656542056074766,"of a large language model), releasing of a model checkpoint, or other means that are
1020"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8662383177570093,"appropriate to the research performed.
1021"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.866822429906542,"• While NeurIPS does not require releasing code, the conference does require all submis-
1022"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8674065420560748,"sions to provide some reasonable avenue for reproducibility, which may depend on the
1023"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8679906542056075,"nature of the contribution. For example
1024"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8685747663551402,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
1025"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8691588785046729,"to reproduce that algorithm.
1026"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8697429906542056,"(b) If the contribution is primarily a new model architecture, the paper should describe
1027"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8703271028037384,"the architecture clearly and fully.
1028"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8709112149532711,"(c) If the contribution is a new model (e.g., a large language model), then there should
1029"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8714953271028038,"either be a way to access this model for reproducing the results or a way to reproduce
1030"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8720794392523364,"the model (e.g., with an open-source dataset or instructions for how to construct
1031"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8726635514018691,"the dataset).
1032"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8732476635514018,"(d) We recognize that reproducibility may be tricky in some cases, in which case
1033"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8738317757009346,"authors are welcome to describe the particular way they provide for reproducibility.
1034"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8744158878504673,"In the case of closed-source models, it may be that access to the model is limited in
1035"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.875,"some way (e.g., to registered users), but it should be possible for other researchers
1036"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8755841121495327,"to have some path to reproducing or verifying the results.
1037"
OPEN ACCESS TO DATA AND CODE,0.8761682242990654,"5. Open access to data and code
1038"
OPEN ACCESS TO DATA AND CODE,0.8767523364485982,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
1039"
OPEN ACCESS TO DATA AND CODE,0.8773364485981309,"tions to faithfully reproduce the main experimental results, as described in supplemental
1040"
OPEN ACCESS TO DATA AND CODE,0.8779205607476636,"material?
1041"
OPEN ACCESS TO DATA AND CODE,0.8785046728971962,"Answer: [Yes]
1042"
OPEN ACCESS TO DATA AND CODE,0.8790887850467289,"Justification: The data and code are available in the supplementary material.
1043"
OPEN ACCESS TO DATA AND CODE,0.8796728971962616,"Guidelines:
1044"
OPEN ACCESS TO DATA AND CODE,0.8802570093457944,"• The answer NA means that paper does not include experiments requiring code.
1045"
OPEN ACCESS TO DATA AND CODE,0.8808411214953271,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
1046"
OPEN ACCESS TO DATA AND CODE,0.8814252336448598,"public/guides/CodeSubmissionPolicy) for more details.
1047"
OPEN ACCESS TO DATA AND CODE,0.8820093457943925,"• While we encourage the release of code and data, we understand that this might not be
1048"
OPEN ACCESS TO DATA AND CODE,0.8825934579439252,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
1049"
OPEN ACCESS TO DATA AND CODE,0.883177570093458,"including code, unless this is central to the contribution (e.g., for a new open-source
1050"
OPEN ACCESS TO DATA AND CODE,0.8837616822429907,"benchmark).
1051"
OPEN ACCESS TO DATA AND CODE,0.8843457943925234,"• The instructions should contain the exact command and environment needed to run to
1052"
OPEN ACCESS TO DATA AND CODE,0.884929906542056,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
1053"
OPEN ACCESS TO DATA AND CODE,0.8855140186915887,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
1054"
OPEN ACCESS TO DATA AND CODE,0.8860981308411215,"• The authors should provide instructions on data access and preparation, including how
1055"
OPEN ACCESS TO DATA AND CODE,0.8866822429906542,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
1056"
OPEN ACCESS TO DATA AND CODE,0.8872663551401869,"• The authors should provide scripts to reproduce all experimental results for the new
1057"
OPEN ACCESS TO DATA AND CODE,0.8878504672897196,"proposed method and baselines. If only a subset of experiments are reproducible, they
1058"
OPEN ACCESS TO DATA AND CODE,0.8884345794392523,"should state which ones are omitted from the script and why.
1059"
OPEN ACCESS TO DATA AND CODE,0.889018691588785,"• At submission time, to preserve anonymity, the authors should release anonymized
1060"
OPEN ACCESS TO DATA AND CODE,0.8896028037383178,"versions (if applicable).
1061"
OPEN ACCESS TO DATA AND CODE,0.8901869158878505,"• Providing as much information as possible in supplemental material (appended to the
1062"
OPEN ACCESS TO DATA AND CODE,0.8907710280373832,"paper) is recommended, but including URLs to data and code is permitted.
1063"
OPEN ACCESS TO DATA AND CODE,0.8913551401869159,"6. Experimental Setting/Details
1064"
OPEN ACCESS TO DATA AND CODE,0.8919392523364486,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
1065"
OPEN ACCESS TO DATA AND CODE,0.8925233644859814,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
1066"
OPEN ACCESS TO DATA AND CODE,0.893107476635514,"results?
1067"
OPEN ACCESS TO DATA AND CODE,0.8936915887850467,"Answer: [Yes]
1068"
OPEN ACCESS TO DATA AND CODE,0.8942757009345794,"Justification: The detailed experimental settings are provided in Appendix.
1069"
OPEN ACCESS TO DATA AND CODE,0.8948598130841121,"Guidelines:
1070"
OPEN ACCESS TO DATA AND CODE,0.8954439252336449,"• The answer NA means that the paper does not include experiments.
1071"
OPEN ACCESS TO DATA AND CODE,0.8960280373831776,"• The experimental setting should be presented in the core of the paper to a level of detail
1072"
OPEN ACCESS TO DATA AND CODE,0.8966121495327103,"that is necessary to appreciate the results and make sense of them.
1073"
OPEN ACCESS TO DATA AND CODE,0.897196261682243,"• The full details can be provided either with the code, in appendix, or as supplemental
1074"
OPEN ACCESS TO DATA AND CODE,0.8977803738317757,"material.
1075"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8983644859813084,"7. Experiment Statistical Significance
1076"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8989485981308412,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
1077"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8995327102803738,"information about the statistical significance of the experiments?
1078"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9001168224299065,"Answer: [Yes]
1079"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9007009345794392,"Justification: We report the average accuracy and the standard deviation as the performance
1080"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9012850467289719,"in experiments.
1081"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9018691588785047,"Guidelines:
1082"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9024532710280374,"• The answer NA means that the paper does not include experiments.
1083"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9030373831775701,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
1084"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9036214953271028,"dence intervals, or statistical significance tests, at least for the experiments that support
1085"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9042056074766355,"the main claims of the paper.
1086"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9047897196261683,"• The factors of variability that the error bars are capturing should be clearly stated (for
1087"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.905373831775701,"example, train/test split, initialization, random drawing of some parameter, or overall
1088"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9059579439252337,"run with given experimental conditions).
1089"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9065420560747663,"• The method for calculating the error bars should be explained (closed form formula,
1090"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.907126168224299,"call to a library function, bootstrap, etc.)
1091"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9077102803738317,"• The assumptions made should be given (e.g., Normally distributed errors).
1092"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9082943925233645,"• It should be clear whether the error bar is the standard deviation or the standard error
1093"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9088785046728972,"of the mean.
1094"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9094626168224299,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
1095"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9100467289719626,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
1096"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9106308411214953,"of Normality of errors is not verified.
1097"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9112149532710281,"• For asymmetric distributions, the authors should be careful not to show in tables or
1098"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9117990654205608,"figures symmetric error bars that would yield results that are out of range (e.g. negative
1099"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9123831775700935,"error rates).
1100"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9129672897196262,"• If error bars are reported in tables or plots, The authors should explain in the text how
1101"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9135514018691588,"they were calculated and reference the corresponding figures or tables in the text.
1102"
EXPERIMENTS COMPUTE RESOURCES,0.9141355140186916,"8. Experiments Compute Resources
1103"
EXPERIMENTS COMPUTE RESOURCES,0.9147196261682243,"Question: For each experiment, does the paper provide sufficient information on the com-
1104"
EXPERIMENTS COMPUTE RESOURCES,0.915303738317757,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
1105"
EXPERIMENTS COMPUTE RESOURCES,0.9158878504672897,"the experiments?
1106"
EXPERIMENTS COMPUTE RESOURCES,0.9164719626168224,"Answer: [Yes]
1107"
EXPERIMENTS COMPUTE RESOURCES,0.9170560747663551,"Justification: We list the hardware and software resources along with the space and space
1108"
EXPERIMENTS COMPUTE RESOURCES,0.9176401869158879,"complexity in Appendix.
1109"
EXPERIMENTS COMPUTE RESOURCES,0.9182242990654206,"Guidelines:
1110"
EXPERIMENTS COMPUTE RESOURCES,0.9188084112149533,"• The answer NA means that the paper does not include experiments.
1111"
EXPERIMENTS COMPUTE RESOURCES,0.919392523364486,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
1112"
EXPERIMENTS COMPUTE RESOURCES,0.9199766355140186,"or cloud provider, including relevant memory and storage.
1113"
EXPERIMENTS COMPUTE RESOURCES,0.9205607476635514,"• The paper should provide the amount of compute required for each of the individual
1114"
EXPERIMENTS COMPUTE RESOURCES,0.9211448598130841,"experimental runs as well as estimate the total compute.
1115"
EXPERIMENTS COMPUTE RESOURCES,0.9217289719626168,"• The paper should disclose whether the full research project required more compute
1116"
EXPERIMENTS COMPUTE RESOURCES,0.9223130841121495,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
1117"
EXPERIMENTS COMPUTE RESOURCES,0.9228971962616822,"didn’t make it into the paper).
1118"
CODE OF ETHICS,0.923481308411215,"9. Code Of Ethics
1119"
CODE OF ETHICS,0.9240654205607477,"Question: Does the research conducted in the paper conform, in every respect, with the
1120"
CODE OF ETHICS,0.9246495327102804,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
1121"
CODE OF ETHICS,0.9252336448598131,"Answer: [Yes]
1122"
CODE OF ETHICS,0.9258177570093458,"Justification: The research conducted in this paper conforms, in every respect, with the
1123"
CODE OF ETHICS,0.9264018691588785,"NeurIPS Code of Ethics.
1124"
CODE OF ETHICS,0.9269859813084113,"Guidelines:
1125"
CODE OF ETHICS,0.927570093457944,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
1126"
CODE OF ETHICS,0.9281542056074766,"• If the authors answer No, they should explain the special circumstances that require a
1127"
CODE OF ETHICS,0.9287383177570093,"deviation from the Code of Ethics.
1128"
CODE OF ETHICS,0.929322429906542,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
1129"
CODE OF ETHICS,0.9299065420560748,"eration due to laws or regulations in their jurisdiction).
1130"
BROADER IMPACTS,0.9304906542056075,"10. Broader Impacts
1131"
BROADER IMPACTS,0.9310747663551402,"Question: Does the paper discuss both potential positive societal impacts and negative
1132"
BROADER IMPACTS,0.9316588785046729,"societal impacts of the work performed?
1133"
BROADER IMPACTS,0.9322429906542056,"Answer: [Yes]
1134"
BROADER IMPACTS,0.9328271028037384,"Justification: The potential positive societal impacts are provided in the Introduction while
1135"
BROADER IMPACTS,0.9334112149532711,"the potential negative societal impacts are meaningless since this work is a foundational
1136"
BROADER IMPACTS,0.9339953271028038,"research.
1137"
BROADER IMPACTS,0.9345794392523364,"Guidelines:
1138"
BROADER IMPACTS,0.9351635514018691,"• The answer NA means that there is no societal impact of the work performed.
1139"
BROADER IMPACTS,0.9357476635514018,"• If the authors answer NA or No, they should explain why their work has no societal
1140"
BROADER IMPACTS,0.9363317757009346,"impact or why the paper does not address societal impact.
1141"
BROADER IMPACTS,0.9369158878504673,"• Examples of negative societal impacts include potential malicious or unintended uses
1142"
BROADER IMPACTS,0.9375,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
1143"
BROADER IMPACTS,0.9380841121495327,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
1144"
BROADER IMPACTS,0.9386682242990654,"groups), privacy considerations, and security considerations.
1145"
BROADER IMPACTS,0.9392523364485982,"• The conference expects that many papers will be foundational research and not tied
1146"
BROADER IMPACTS,0.9398364485981309,"to particular applications, let alone deployments. However, if there is a direct path to
1147"
BROADER IMPACTS,0.9404205607476636,"any negative applications, the authors should point it out. For example, it is legitimate
1148"
BROADER IMPACTS,0.9410046728971962,"to point out that an improvement in the quality of generative models could be used to
1149"
BROADER IMPACTS,0.9415887850467289,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
1150"
BROADER IMPACTS,0.9421728971962616,"that a generic algorithm for optimizing neural networks could enable people to train
1151"
BROADER IMPACTS,0.9427570093457944,"models that generate Deepfakes faster.
1152"
BROADER IMPACTS,0.9433411214953271,"• The authors should consider possible harms that could arise when the technology is
1153"
BROADER IMPACTS,0.9439252336448598,"being used as intended and functioning correctly, harms that could arise when the
1154"
BROADER IMPACTS,0.9445093457943925,"technology is being used as intended but gives incorrect results, and harms following
1155"
BROADER IMPACTS,0.9450934579439252,"from (intentional or unintentional) misuse of the technology.
1156"
BROADER IMPACTS,0.945677570093458,"• If there are negative societal impacts, the authors could also discuss possible mitigation
1157"
BROADER IMPACTS,0.9462616822429907,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
1158"
BROADER IMPACTS,0.9468457943925234,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
1159"
BROADER IMPACTS,0.947429906542056,"feedback over time, improving the efficiency and accessibility of ML).
1160"
SAFEGUARDS,0.9480140186915887,"11. Safeguards
1161"
SAFEGUARDS,0.9485981308411215,"Question: Does the paper describe safeguards that have been put in place for responsible
1162"
SAFEGUARDS,0.9491822429906542,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
1163"
SAFEGUARDS,0.9497663551401869,"image generators, or scraped datasets)?
1164"
SAFEGUARDS,0.9503504672897196,"Answer: [NA]
1165"
SAFEGUARDS,0.9509345794392523,"Justification: This paper poses no such risk. The datasets we used are all publicly available
1166"
SAFEGUARDS,0.951518691588785,"online.
1167"
SAFEGUARDS,0.9521028037383178,"Guidelines:
1168"
SAFEGUARDS,0.9526869158878505,"• The answer NA means that the paper poses no such risks.
1169"
SAFEGUARDS,0.9532710280373832,"• Released models that have a high risk for misuse or dual-use should be released with
1170"
SAFEGUARDS,0.9538551401869159,"necessary safeguards to allow for controlled use of the model, for example by requiring
1171"
SAFEGUARDS,0.9544392523364486,"that users adhere to usage guidelines or restrictions to access the model or implementing
1172"
SAFEGUARDS,0.9550233644859814,"safety filters.
1173"
SAFEGUARDS,0.955607476635514,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
1174"
SAFEGUARDS,0.9561915887850467,"should describe how they avoided releasing unsafe images.
1175"
SAFEGUARDS,0.9567757009345794,"• We recognize that providing effective safeguards is challenging, and many papers do
1176"
SAFEGUARDS,0.9573598130841121,"not require this, but we encourage authors to take this into account and make a best
1177"
SAFEGUARDS,0.9579439252336449,"faith effort.
1178"
LICENSES FOR EXISTING ASSETS,0.9585280373831776,"12. Licenses for existing assets
1179"
LICENSES FOR EXISTING ASSETS,0.9591121495327103,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
1180"
LICENSES FOR EXISTING ASSETS,0.959696261682243,"the paper, properly credited and are the license and terms of use explicitly mentioned and
1181"
LICENSES FOR EXISTING ASSETS,0.9602803738317757,"properly respected?
1182"
LICENSES FOR EXISTING ASSETS,0.9608644859813084,"Answer: [Yes]
1183"
LICENSES FOR EXISTING ASSETS,0.9614485981308412,"Justification: The datasets and code of baseline methods are publicly available online. We
1184"
LICENSES FOR EXISTING ASSETS,0.9620327102803738,"cite the original paper and mark the URL in both papers and codebase.
1185"
LICENSES FOR EXISTING ASSETS,0.9626168224299065,"Guidelines:
1186"
LICENSES FOR EXISTING ASSETS,0.9632009345794392,"• The answer NA means that the paper does not use existing assets.
1187"
LICENSES FOR EXISTING ASSETS,0.9637850467289719,"• The authors should cite the original paper that produced the code package or dataset.
1188"
LICENSES FOR EXISTING ASSETS,0.9643691588785047,"• The authors should state which version of the asset is used and, if possible, include a
1189"
LICENSES FOR EXISTING ASSETS,0.9649532710280374,"URL.
1190"
LICENSES FOR EXISTING ASSETS,0.9655373831775701,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
1191"
LICENSES FOR EXISTING ASSETS,0.9661214953271028,"• For scraped data from a particular source (e.g., website), the copyright and terms of
1192"
LICENSES FOR EXISTING ASSETS,0.9667056074766355,"service of that source should be provided.
1193"
LICENSES FOR EXISTING ASSETS,0.9672897196261683,"• If assets are released, the license, copyright information, and terms of use in the
1194"
LICENSES FOR EXISTING ASSETS,0.967873831775701,"package should be provided. For popular datasets, paperswithcode.com/datasets
1195"
LICENSES FOR EXISTING ASSETS,0.9684579439252337,"has curated licenses for some datasets. Their licensing guide can help determine the
1196"
LICENSES FOR EXISTING ASSETS,0.9690420560747663,"license of a dataset.
1197"
LICENSES FOR EXISTING ASSETS,0.969626168224299,"• For existing datasets that are re-packaged, both the original license and the license of
1198"
LICENSES FOR EXISTING ASSETS,0.9702102803738317,"the derived asset (if it has changed) should be provided.
1199"
LICENSES FOR EXISTING ASSETS,0.9707943925233645,"• If this information is not available online, the authors are encouraged to reach out to
1200"
LICENSES FOR EXISTING ASSETS,0.9713785046728972,"the asset’s creators.
1201"
NEW ASSETS,0.9719626168224299,"13. New Assets
1202"
NEW ASSETS,0.9725467289719626,"Question: Are new assets introduced in the paper well documented and is the documentation
1203"
NEW ASSETS,0.9731308411214953,"provided alongside the assets?
1204"
NEW ASSETS,0.9737149532710281,"Answer: [Yes]
1205"
NEW ASSETS,0.9742990654205608,"Justification: We provide a public codebase along with an illustrative README file.
1206"
NEW ASSETS,0.9748831775700935,"Guidelines:
1207"
NEW ASSETS,0.9754672897196262,"• The answer NA means that the paper does not release new assets.
1208"
NEW ASSETS,0.9760514018691588,"• Researchers should communicate the details of the dataset/code/model as part of their
1209"
NEW ASSETS,0.9766355140186916,"submissions via structured templates. This includes details about training, license,
1210"
NEW ASSETS,0.9772196261682243,"limitations, etc.
1211"
NEW ASSETS,0.977803738317757,"• The paper should discuss whether and how consent was obtained from people whose
1212"
NEW ASSETS,0.9783878504672897,"asset is used.
1213"
NEW ASSETS,0.9789719626168224,"• At submission time, remember to anonymize your assets (if applicable). You can either
1214"
NEW ASSETS,0.9795560747663551,"create an anonymized URL or include an anonymized zip file.
1215"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9801401869158879,"14. Crowdsourcing and Research with Human Subjects
1216"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9807242990654206,"Question: For crowdsourcing experiments and research with human subjects, does the paper
1217"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9813084112149533,"include the full text of instructions given to participants and screenshots, if applicable, as
1218"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.981892523364486,"well as details about compensation (if any)?
1219"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9824766355140186,"Answer: [NA]
1220"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9830607476635514,"Justification: This paper does not involve crowdsourcing nor research with human subjects.
1221"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9836448598130841,"Guidelines:
1222"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9842289719626168,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1223"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9848130841121495,"human subjects.
1224"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9853971962616822,"• Including this information in the supplemental material is fine, but if the main contribu-
1225"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.985981308411215,"tion of the paper involves human subjects, then as much detail as possible should be
1226"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9865654205607477,"included in the main paper.
1227"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9871495327102804,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
1228"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9877336448598131,"or other labor should be paid at least the minimum wage in the country of the data
1229"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9883177570093458,"collector.
1230"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9889018691588785,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
1231"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9894859813084113,"Subjects
1232"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.990070093457944,"Question: Does the paper describe potential risks incurred by study participants, whether
1233"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9906542056074766,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
1234"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9912383177570093,"approvals (or an equivalent approval/review based on the requirements of your country or
1235"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.991822429906542,"institution) were obtained?
1236"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9924065420560748,"Answer: [NA]
1237"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9929906542056075,"Justification: This paper does not involve crowdsourcing nor research with human subjects.
1238"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9935747663551402,"Guidelines:
1239"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9941588785046729,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1240"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9947429906542056,"human subjects.
1241"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9953271028037384,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
1242"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9959112149532711,"may be required for any human subjects research. If you obtained IRB approval, you
1243"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9964953271028038,"should clearly state this in the paper.
1244"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9970794392523364,"• We recognize that the procedures for this may vary significantly between institutions
1245"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9976635514018691,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
1246"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9982476635514018,"guidelines for their institution.
1247"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9988317757009346,"• For initial submissions, do not include any information that would break anonymity (if
1248"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9994158878504673,"applicable), such as the institution conducting the review.
1249"
