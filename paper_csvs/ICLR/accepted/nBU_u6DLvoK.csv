Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0035335689045936395,"It is a challenging task to learn rich and multi-scale spatiotemporal semantics
from high-dimensional videos, due to large local redundancy and complex global
dependency between video frames. The recent advances in this research have
been mainly driven by 3D convolutional neural networks and vision transform-
ers. Although 3D convolution can efficiently aggregate local context to suppress
local redundancy from a small 3D neighborhood, it lacks the capability to cap-
ture global dependency because of the limited receptive field. Alternatively, vi-
sion transformers can effectively capture long-range dependency by self-attention
mechanism, while having the limitation on reducing local redundancy with blind
similarity comparison among all the tokens in each layer. Based on these ob-
servations, we propose a novel Unified transFormer (UniFormer) which seam-
lessly integrates merits of 3D convolution and spatiotemporal self-attention in a
concise transformer format, and achieves a preferable balance between computa-
tion and accuracy. Different from traditional transformers, our relation aggregator
can tackle both spatiotemporal redundancy and dependency, by learning local and
global token affinity respectively in shallow and deep layers. We conduct exten-
sive experiments on the popular video benchmarks, e.g., Kinetics-400, Kinetics-
600, and Something-Something V1&V2. With only ImageNet-1K pretraining,
our UniFormer achieves 82.9%/84.8% top-1 accuracy on Kinetics-400/Kinetics-
600, while requiring 10× fewer GFLOPs than other state-of-the-art methods. For
Something-Something V1 and V2, our UniFormer achieves new state-of-the-art
performances of 60.9% and 71.2% top-1 accuracy respectively. Code is available
at https://github.com/Sense-X/UniFormer."
INTRODUCTION,0.007067137809187279,"1
INTRODUCTION"
INTRODUCTION,0.01060070671378092,"Learning spatiotemporal representations is a fundamental task for video understanding. Basically,
there are two distinct challenges. On the one hand, videos contain large spatiotemporal redundancy,
where target motions across local neighboring frames are subtle. On the other hand, videos contain
complex spatiotemporal dependency, since target relations across long-range frames are dynamic."
INTRODUCTION,0.014134275618374558,"The advances in video classification have mostly driven by 3D convolutional neural networks (Tran
et al., 2015; Carreira & Zisserman, 2017b; Feichtenhofer et al., 2019) and spatiotemporal transform-
ers (Bertasius et al., 2021; Arnab et al., 2021). Unfortunately, each of these two frameworks focuses
on one of the aforementioned challenges. 3D convolution can capture detailed and local spatiotem-
poral features, by processing each pixel with context from a small 3D neighborhood (e.g., 3×3×3)."
INTRODUCTION,0.0176678445229682,"*Equally-contributed first authors ({kc.li, yl.wang}@siat.ac.cn)
†Corresponding author (qiaoyu@pjlab.org.cn)"
INTRODUCTION,0.02120141342756184,Published as a conference paper at ICLR 2022
INTRODUCTION,0.024734982332155476,"Figure 1: Some visualizations of TimeSformer. We respectively show the feature, spatial and
temporal attention from the 3rd layer of TimeSformer (Bertasius et al., 2021). We find that, such
transformer learns local representations with redundant global attention. For an anchor token (green
box), spatial/temporal attention compares it with all the contextual tokens for aggregation, while
only its neighboring tokens (boxes filled with red color) actually work. Hence, it wastes large
computation to encode only very local spatiotemporal representations."
INTRODUCTION,0.028268551236749116,"0
1000
2000
3000
4000
5000
6000
7000
FLOPs/Video (G) 76 77 78 79 80 81 82 83"
INTRODUCTION,0.03180212014134275,K400 Top-1 Accuracy (%)
INTRODUCTION,0.0353356890459364,S16(1)
INTRODUCTION,0.038869257950530034,S32(1)
INTRODUCTION,0.04240282685512368,S16(4)
INTRODUCTION,0.045936395759717315,B16(4)
INTRODUCTION,0.04946996466431095,B32(4) 17 ×
INTRODUCTION,0.053003533568904596,"X3D
MoViNet
Slowfast
MViT
XiT IN-21K
Swin IN-1K
TimeSformer IN-21K
Ours IN-1K"
INTRODUCTION,0.05653710247349823,"0
200
400
600
800
1000
1200
FLOPs/Video (G) 60 62 64 66 68 70"
INTRODUCTION,0.06007067137809187,Sth-SthV2 Top-1 Accuracy (%)
INTRODUCTION,0.0636042402826855,S16(1)
INTRODUCTION,0.06713780918727916,S16(3)
INTRODUCTION,0.0706713780918728,B16(3)
INTRODUCTION,0.07420494699646643,B32(3) 6 ×
INTRODUCTION,0.07773851590106007,"TEINet IN-1K
CT-Net IN-1K
TDN IN1K
MViT K600
XiT IN-21K
Swin K400
TimeSformer IN-21K
Ours K600"
INTRODUCTION,0.0812720848056537,"Figure 2: Accuracy vs. per-video GFLOPs on Kinetics-400 and Something-Something V2. B-
32(4) means we test UniFormer-B32f with 4 clips and S-16(3) means we test UniFormer-S16f with
3 crops (more testing details can be found in Section 4.3). Our UniFormer achieves the best balance
between accuracy and computation on both datasets."
INTRODUCTION,0.08480565371024736,"Hence, it can reduce spatiotemporal redundancy across adjacent frames. However, due to the lim-
ited receptive field, 3D convolution suffers from difficulty in learning long-range dependency (Wang
et al., 2018; Li et al., 2020a). Alternatively, vision transformers are good at capturing global depen-
dency, with the help of self-attention among visual tokens (Dosovitskiy et al., 2021). Recently, this
design has been introduced in video classification via spatiotemporal attention mechanism (Berta-
sius et al., 2021). However, we observe that, video transformers are often inefficient to encode local
spatiotemporal features in the shallow layers. We take the well-known and typical TimeSformer
(Bertasius et al., 2021) for illustration. As shown in Figure 1, TimeSformer indeed learns detailed
video representations in the early layers, but with very redundant spatial and temporal attention.
Specifically, spatial attention mainly focuses on the neighbor tokens (mostly in 3×3 local regions),
while learning nothing from the rest tokens in the same frame. Similarly, temporal attention mostly
only aggregates tokens in the adjacent frames, while ignoring the rest in the distant frames. More
importantly, such local representations are learned from global token-to-token similarity comparison
in all layers, requiring large computation cost. This fact clearly deteriorates computation-accuracy
balance of such video transformer (Figure 2)."
INTRODUCTION,0.08833922261484099,"To tackle these difficulties, we propose to effectively unify 3D convolution and spatiotemporal self-
attention in a concise transformer format, thus we name the network Unified transFormer (Uni-
Former), which can achieve a preferable balance between efficiency and effectiveness. More specif-
ically, our UniFormer consists of three core modules, i.e., Dynamic Position Embedding (DPE),"
INTRODUCTION,0.09187279151943463,Published as a conference paper at ICLR 2022
INTRODUCTION,0.09540636042402827,"Multi-Head Relation Aggregator (MHRA), and Feed-Forward Network (FFN). The key difference
between our UniFormer and traditional video transformers is the distinct design of our relation ag-
gregator. First, instead of utilizing a self-attention mechanism in all layers, our proposed relation
aggregator tackles video redundancy and dependency respectively. In the shallow layers, our ag-
gregator learns local relation with a small learnable parameter matrix, which can largely reduce
computation burden by aggregating context from adjacent tokens in a small 3D neighborhood. In
the deep layers, our aggregator learns global relation with similarity comparison, which can flexibly
build long-range token dependencies from distant frames in the video. Second, different from spa-
tial and temporal attention separation in the traditional transformers (Bertasius et al., 2021; Arnab
et al., 2021), our relation aggregator jointly encodes spatiotemporal context in all the layers, which
can further boost video representations in a joint learning manner. Finally, we build up our model
by progressively integrating UniFormer blocks in a hierarchical manner. In this case, we enlarge
the cooperative power of local and global UniFormer blocks for efficient spatiotemporal representa-
tion learning in videos. We conduct extensive experiments on the popular video benchmarks, e.g.,
Kineticss-400 (Carreira & Zisserman, 2017a), Kinetics-600 (Carreira et al., 2018) and Something-
Something V1&V2 (Goyal et al., 2017b). With only ImageNet-1K pretraining, our UniFormer
achieves 82.9%/84.8% top-1 accuracy on Kinetics-400/Kinetics-600, while requiring 10× fewer
GFLOPs than other comparable methods (e.g., 16.7× fewer GFLOPs than ViViT (Arnab et al.,
2021) with JFT-300M pre-training). For Something-Something V1 and V2, our UniFormer achieves
60.9% and 71.2% top-1 accuracy respectively, which are new state-of-the-art performances."
RELATED WORK,0.0989399293286219,"2
RELATED WORK"
RELATED WORK,0.10247349823321555,"Convolution-based Video Networks. 3D Convolution Neural Networks (CNNs) have been dom-
inant in video understanding (Tran et al., 2015; Feichtenhofer et al., 2019). However, they suffer
from the difficult optimization problem and large computation cost. To resolve this issue, I3D (Car-
reira & Zisserman, 2017b) inflates the pre-trained 2D convolution kernels for better optimization.
Other prior works (Tran et al., 2018; Qiu et al., 2017; Tran et al., 2019; Feichtenhofer, 2020; Wang
et al., 2020a) try to factorize 3D convolution kernel in different dimensions to reduce complexity.
Recent methods propose well-designed modules to enhance the temporal modeling ability for 2D
CNNs (Wang et al., 2016; Lin et al., 2019; Luo & Yuille, 2019; Jiang et al., 2019; Liu et al., 2020a;
Li et al., 2020b; Kwon et al., 2020; Li et al., 2020a; 2021a; Wang et al., 2020b). However, 3D
convolution struggles to capture long-range dependency, due to the limited receptive field."
RELATED WORK,0.10600706713780919,"Transformer-based Video Networks. Vision Transformers (Dosovitskiy et al., 2021; Touvron
et al., 2021a;b; Liu et al., 2021a) have been popular for vision tasks and outperform many CNNs.
Based on ViT, several prior works (Bertasius et al., 2021; Neimark et al., 2021; Sharir et al., 2021;
Li et al., 2021b; Arnab et al., 2021; Bulat et al., 2021; Patrick et al., 2021; Zha et al., 2021) propose
different variants for spatiotemporal learning, verifying the outstanding ability of the transformer to
capture long-term dependencies. To reduce high dot-product computation, MViT (Fan et al., 2021)
introduces the hierarchical structure and pooling self-attention, while Video Swin (Liu et al., 2021b)
advocates an inductive bias of locality for video. Nevertheless, the self-attention mechanism is inef-
ficient to encode low-level features, hindering their high potential. To tackle this challenge, different
from Video Swin that applies self-attention in a local 3D window, we adopt 3D convolution in a
concise transformer format to encode local features. Besides, we follow their hierarchical designs
and propose our UniFormer, achieving powerful performance for video understanding."
METHOD,0.10954063604240283,"3
METHOD"
METHOD,0.11307420494699646,"In this section, we describe our UniFormer in detail. First, we introduce the overall architecture
of the UniFormer block. Then, we explain the vital designs of our UniFormer for spatiotemporal
modeling, i.e., multi-head relation aggregator and dynamic position embedding. Finally, we hierar-
chically stack UniFormer blocks to build up our video network."
OVERVIEW OF UNIFORMER BLOCK,0.1166077738515901,"3.1
OVERVIEW OF UNIFORMER BLOCK"
OVERVIEW OF UNIFORMER BLOCK,0.12014134275618374,"To overcome problems of spatiotemporal redundancy and dependency, we propose a novel and con-
cise Unified transFormer (UniFormer) shown in Figure 3. We utilize a basic transformer format"
OVERVIEW OF UNIFORMER BLOCK,0.12367491166077739,Published as a conference paper at ICLR 2022
OVERVIEW OF UNIFORMER BLOCK,0.127208480565371,"×""#
Stage#"
OVERVIEW OF UNIFORMER BLOCK,0.13074204946996468,"3×4×4, 64
Stride 2×4×4"
OVERVIEW OF UNIFORMER BLOCK,0.13427561837455831,Stage2
OVERVIEW OF UNIFORMER BLOCK,0.13780918727915195,Stage3
OVERVIEW OF UNIFORMER BLOCK,0.1413427561837456,Stage4
OVERVIEW OF UNIFORMER BLOCK,0.14487632508833923,Stage5
OVERVIEW OF UNIFORMER BLOCK,0.14840989399293286,"1×2×2, 128
Stride 1×2×2"
OVERVIEW OF UNIFORMER BLOCK,0.1519434628975265,"1×2×2, 256
Stride 1×2×2"
OVERVIEW OF UNIFORMER BLOCK,0.15547703180212014,"1×2×2, 512
Stride 1×2×2"
OVERVIEW OF UNIFORMER BLOCK,0.15901060070671377,"3×9×:×;
64× 9 2 × : 4 × ;"
OVERVIEW OF UNIFORMER BLOCK,0.1625441696113074,"4
128× 9 2 × : 8 × ;"
OVERVIEW OF UNIFORMER BLOCK,0.16607773851590105,"8
320× 9 2 × :"
OVERVIEW OF UNIFORMER BLOCK,0.1696113074204947,16 × ;
OVERVIEW OF UNIFORMER BLOCK,0.17314487632508835,"16
512× 9 2 × :"
OVERVIEW OF UNIFORMER BLOCK,0.17667844522968199,"32 × ; 32
="
OVERVIEW OF UNIFORMER BLOCK,0.18021201413427562,"AvgPool, FC"
OVERVIEW OF UNIFORMER BLOCK,0.18374558303886926,"FFN
MHRA Norm DPE"
OVERVIEW OF UNIFORMER BLOCK,0.1872791519434629,DWConv VR AR
OVERVIEW OF UNIFORMER BLOCK,0.19081272084805653,"Concat
U
GELU
Linear
Linear
Norm
VWX
V
Y
Z"
OVERVIEW OF UNIFORMER BLOCK,0.19434628975265017,"Figure 3: Overall architecture of our Unified transFormer (UniFormer). A UniFormer block consists
of three key modules, i.e., Dynamic Position Embedding (DPE), Multi-Head Relation Aggregrator
(MHRA), and Feed Forward Network (FFN). Detailed explanations can be found in Section 3."
OVERVIEW OF UNIFORMER BLOCK,0.1978798586572438,"(Vaswani et al., 2017) but specially design it for efficient and effective spatiotemporal representa-
tion learning. Specifically, our UniFormer block consists of three key modules: Dynamic Position
Embedding (DPE), Multi-Head Relation Aggregator (MHRA), and Feed-Forward Network (FFN):
X = DPE (Xin) + Xin,
(1)"
OVERVIEW OF UNIFORMER BLOCK,0.20141342756183744,"Y = MHRA (Norm (X)) + X,
(2)
Z = FFN (Norm (Y)) + Y.
(3)"
OVERVIEW OF UNIFORMER BLOCK,0.2049469964664311,"Considering the input token tensor (frame volumes) Xin ∈RC×T ×H×W , we first introduce DPE to
dynamically integrate 3D position information into all the tokens (Eq. 1), which effectively makes
use of spatiotemporal order of the tokens for video modeling. Then, we leverage MHRA to aggregate
each token with its contextual tokens (Eq. 2). Different from the regular Multi-Head Self-Attention
(MHSA), our MHRA smartly tackles local video redundancy and global video dependency, by flex-
ible designs of token affinity learning in the shallow and deep layers. Finally, we add FFN with two
linear layers for pointwise enhancement of each token (Eq. 3)."
MULTI-HEAD RELATION AGGREGATOR,0.20848056537102475,"3.2
MULTI-HEAD RELATION AGGREGATOR"
MULTI-HEAD RELATION AGGREGATOR,0.21201413427561838,"As discussed above, we should solve large local redundancy and complex global dependency, for
efficient and effective spatiotemporal representation learning. Unfortunately, the popular 3D CNNs
and spatiotemporal transformers only focus on one of these two challenges. For this reason, we
design an alternative Relation Aggregator (RA), which can flexibly unify 3D convolution and spa-
tiotemporal self-attention in a concise transformer format, solving video redundancy and depen-
dency in the shallow layers and deep layers respectively. Specifically, our MHRA conducts token
relation learning via multi-head fusion:
Rn(X) = AnVn(X),
(4)
MHRA(X) = Concat(R1(X); R2(X); · · · ; RN(X))U.
(5)"
MULTI-HEAD RELATION AGGREGATOR,0.21554770318021202,"Given the input tensor X ∈RC×T ×H×W , we first reshape it to a sequence of tokens X ∈RL×C,
L=T×H×W. Rn(·) is the relation aggregator (RA) in the n-th head, and U ∈RC×C is a learnable
parameter matrix to integrate N heads. Moreover, each RA consists of token context encoding and
token affinity learning. Via a linear transformation, one can transform the original token into context
Vn(X) ∈RL× C"
MULTI-HEAD RELATION AGGREGATOR,0.21908127208480566,"N . Subsequently, the relation aggregator can summarize context with the guidance
of the token affinity An ∈RL×L. The key in our RA is how to learn An in videos."
MULTI-HEAD RELATION AGGREGATOR,0.2226148409893993,"Local MHRA. In the shallow layers, we aim at learning detailed video representation from the local
spatiotemporal context in small 3D neighborhoods. This coincidentally shares a similar insight
with the design of a 3D convolution filter. As a result, we design the token affinity as a learnable
parameter matrix operated in the local 3D neighborhood, i.e., given one anchor token Xi, RA learns
local spatiotemporal affinity between this token and other tokens in the small tube Ωt×h×w
i
:"
MULTI-HEAD RELATION AGGREGATOR,0.22614840989399293,"Alocal
n
(Xi, Xj) = ai−j
n
, where j ∈Ωt×h×w
i
,
(6)"
MULTI-HEAD RELATION AGGREGATOR,0.22968197879858657,Published as a conference paper at ICLR 2022
MULTI-HEAD RELATION AGGREGATOR,0.2332155477031802,"Method
Basic
Tackle Local Capture Global
Efficiency
Operation
Redundancy
Dependency
GFLOPs Top-1
X3D (Feichtenhofer, 2020)
PWConv-DWConv-PWConv
""
%
5823
80.4
TimeSformer (Bertasius et al., 2021)
Divided MHSA
%
""
7140
80.7
Our UniFormer
Joint MHRA
""
""
168
80.8
Table 1: Comparison to different methods. ‘Local Redundancy’ means the redundant computation
for capturing local features. ‘Global Dependency’ means the long-range dependency among frames."
MULTI-HEAD RELATION AGGREGATOR,0.23674911660777384,"where an ∈Rt×h×w and Xj refers to any neighbor token in Ωt×h×w
i
. (i −j) means the relative
token index that determines the aggregating weight (Appendix A shows more details). In the shal-
low layers, video contents between adjacent tokens vary subtly, it is significant to encode detailed
features with local operator to reduce redundancy. Hence, the token affinity is designed as a local
learnable parameter matrix, whose values only depend on the relative 3D position between tokens."
MULTI-HEAD RELATION AGGREGATOR,0.24028268551236748,"Comparison to 3D Convolution Block. Interestingly, we find that our local MHRA can be inter-
preted as a spatiotemporal extension of MobileNet block (Sandler et al., 2018; Tran et al., 2019; Fe-
ichtenhofer, 2020). Specifically, the linear transformation V(·) can be instantiated as pointwise con-
volution (PWConv). Furthermore, the local token affinity Alocal
n
is a spatiotemporal matrix that oper-
ated on each output channel (or head) Vn(X), thus the relation aggregator Rn(X) = Alocal
n
Vn(X)
can be explained as a depthwise convolution (DWConv). Finally, all heads are concatenated and
fused by a linear matrix U, which can also be instantiated as pointwise convolution (PWConv). As
a result, this local MHRA can be reformulated with a manner of PWConv-DWConv-PWConv in
the MobileNet block. In our experiments, we flexibly instantiate our local MHRA as such channel-
separated spatiotemporal convolution, so that our UniFormer can inherit computation efficiency for
light-weight video classification. Different from the MobileNet block, our UniFormer block is de-
signed as a generic transformer format, thus an extra FFN is inserted after MHRA, which can further
mix token context at each spatiotemporal position to boost classification accuracy."
MULTI-HEAD RELATION AGGREGATOR,0.24381625441696114,"Global MHRA. In the deep layers, we focus on capturing long-term token dependency in the global
video clip. This naturally shares a similar insight with the design of self-attention. Hence, we design
the token affinity via comparing content similarity among all the tokens in the global view:"
MULTI-HEAD RELATION AGGREGATOR,0.24734982332155478,"Aglobal
n
(Xi, Xj) =
eQn(Xi)T Kn(Xj)
P"
MULTI-HEAD RELATION AGGREGATOR,0.2508833922261484,"j′∈ΩT ×H×W eQn(Xi)T Kn(Xj′) ,
(7)"
MULTI-HEAD RELATION AGGREGATOR,0.254416961130742,"where Xj can be any token in the global 3D tube with size of T×H×W, while Qn(·) and Kn(·) are
two different linear transformations. Most video transformers apply self-attention in all stages, intro-
ducing a large amount of calculation. To reduce the dot-product computation, the prior works tend
to divide spatial and temporal attention (Bertasius et al., 2021; Arnab et al., 2021), but it deteriorates
the spatiotemporal relation among tokens. In contrast, our MHRA performs local relation aggrega-
tion in the early layers, which largely saves the computation of token comparison. Hence, instead of
factorizing spatiotemporal attention, we jointly encode spatiotemporal relation in our MHRA for all
the stages, in order to achieve a preferable computation-accuracy balance."
MULTI-HEAD RELATION AGGREGATOR,0.2579505300353357,"Comparison to Transformer Block. In the deep layers, our UniFormer block is equipped with a
global MHRA Aglobal
n
(Eq. 7). It can be instantiated as a spatiotemporal self attention, where Qn(·),
Kn(·) and Vn(·) become Query, Key and Value in the transformer (Dosovitskiy et al., 2021). Hence,
it can effectively learn long-term dependency. Instead of spatial and temporal factorization in the
previous video transformers (Bertasius et al., 2021; Arnab et al., 2021), our global MHRA is based
on joint spatiotemporal learning to generate more discriminative video representation. Moreover,
we adopt dynamic position embedding (DPE, see Section 3.3) to overcome permutation-invariance,
which can maintain translation-invariance and is friendly to different input clip lengths."
DYNAMIC POSITION EMBEDDING,0.26148409893992935,"3.3
DYNAMIC POSITION EMBEDDING"
DYNAMIC POSITION EMBEDDING,0.26501766784452296,"Since videos are both spatial and temporal variant, it is necessary to encode spatiotemporal position
information for token representations. The previous methods mainly adapt the absolute or relative
position embedding of image tasks to tackle this problem (Bertasius et al., 2021; Arnab et al., 2021).
However, when testing with longer input clips, the absolute one should be interpolated to target input
size with fine-tuning. Besides, the relative version modifies the self-attention and performs worse
due to lack of absolute position information (Islam et al., 2020). To overcome the above problems,"
DYNAMIC POSITION EMBEDDING,0.26855123674911663,Published as a conference paper at ICLR 2022
DYNAMIC POSITION EMBEDDING,0.27208480565371024,"Method
Pretrain
#Frame
GFLOPs
K400
K600
Top-1
Top-5
Top-1
Top-5
LGD(Qiu et al., 2019)
IN-1K
128×N/A
N/A
79.4
94.4
81.5
95.6
SlowFast+NL(Feichtenhofer et al., 2019)
-
16×3×10
7020
79.8
93.9
81.8
95.1
ip-CSN(Tran et al., 2019)
Sports1M
32×3×10
3270
79.2
93.8
-
-
CorrNet(Wang et al., 2020a)
Sports1M
32×3×10
6720
81.0
-
-
-
X3D-M(Feichtenhofer, 2020)
-
16×3×10
186
76.0
92.3
78.8
94.5
X3D-XL(Feichtenhofer, 2020)
-
16×3×10
1452
79.1
93.9
81.9
95.5
MoViNet-A5(Kondratyuk et al., 2021)
-
120×1×1
281
80.9
94.9
82.7
95.7
MoViNet-A6(Kondratyuk et al., 2021)
-
120×1×1
386
81.5
95.3
83.5
96.2
ViT-B-VTN (Neimark et al., 2021)
IN-21K
250×1×1
3992
78.6
93.7
-
-
TimeSformer-L(Bertasius et al., 2021)
IN-21K
96×3×1
7140
80.7
94.7
82.2
95.5
STAM (Sharir et al., 2021)
IN-21K
64×1×1
1040
79.2
-
-
-
X-ViT(Bulat et al., 2021)
IN-21K
16×3×1
850
80.2
94.7
84.5
96.3
Mformer-HR(Patrick et al., 2021)
IN-21K
16×3×10
28764
81.1
95.2
82.7
96.1
MViT-B,16×4(Fan et al., 2021)
-
16×1×5
353
78.4
93.5
82.1
95.7
MViT-B,32×3(Fan et al., 2021)
-
32×1×5
850
80.2
94.4
83.4
96.3
ViViT-L(Arnab et al., 2021)
IN-21K
16×3×4
17352
80.6
94.7
82.5
95.6
ViViT-L(Arnab et al., 2021)
JFT-300M
16×3×4
17352
82.8
95.3
84.3
96.2
Swin-T(Liu et al., 2021b)
IN-1K
32×3×4
1056
78.8
93.6
-
-
Swin-B(Liu et al., 2021b)
IN-1K
32×3×4
3384
80.6
94.6
-
-
Swin-B(Liu et al., 2021b)
IN-21K
32×3×4
3384
82.7
95.5
84.0
96.5
Our UniFormer-S
IN-1K
16×1×4
167
80.8
94.7
82.8
95.8
Our UniFormer-B
IN-1K
16×1×4
389
82.0
95.1
84.0
96.4
Our UniFormer-B
IN-1K
32×1×4
1036
82.9
95.4
84.8
96.7
Our UniFormer-B
IN-1K
32×3×4
3108
83.0
95.4
84.9
96.7
Table 2: Comparison with the state-of-the-art on Kinetics-400&600. Our UniFormer outper-
forms most of the current methods with much fewer computation cost."
DYNAMIC POSITION EMBEDDING,0.2756183745583039,"we extend the conditional position encoding (CPE) (Chu et al., 2021) to design our DPE:"
DYNAMIC POSITION EMBEDDING,0.2791519434628975,"DPE(Xin) = DWConv(Xin),
(8)"
DYNAMIC POSITION EMBEDDING,0.2826855123674912,"where DWConv means simple 3D depthwise convolution with zero paddings. Thanks to the shared
parameters and locality of convolution, DPE can overcome permutation-invariance and is friendly
to arbitrary input lengths. Moreover, it has been proven in CPE that zero paddings help the tokens
on the borders be aware of their absolute positions, thus all tokens can progressively encode their
absolute spatiotemporal position information via querying their neighbor."
MODEL ARCHITECTURE,0.2862190812720848,"3.4
MODEL ARCHITECTURE"
MODEL ARCHITECTURE,0.28975265017667845,"We hierarchically stack UniFormer blocks to build up our network for spatiotemporal learning. As
shown in Figure 3, our network consists of four stages, the channel numbers of which are 64, 128,
320 and 512 respectively. We provide two model variants depending on the number of UniFormer
blocks in these stages: {3, 4, 8, 3} for UniFormer-S and {5, 8, 20, 7} for UniFormer-B. In the first
two stages, we utilize MHRA with local token affinity (Eq. 6) to reduce the short-term spatiotempo-
ral redundancy. The tube size is set to 5×5×5 and the head number N is equal to the corresponding
channel number. In the last two stages, we apply MHRA with global token affinity (Eq. 7) to capture
long-term dependency, the head dimension of which is 64. We utilize BN (Ioffe & Szegedy, 2015)
for local MHRA and LN (Ba et al., 2016) for global MHRA. The kernel size of DPE is 3×3×3
(T×H×W) and the expand ratios of FFN in all layers are 4. We adopt a 3×4×4 convolution with
stride 2×4×4 before the first stage, which means the spatial and temporal dimensions are both
downsampled. Before other stages, we apply 1×2×2 convolutions with stride 1×2×2. Finally, the
spatiotemporal average pooling and fully connected layer are utilized to output the final predictions."
MODEL ARCHITECTURE,0.29328621908127206,"Comparison to Convolution+Transformer Network. The prior works have demonstrate that self-
attention can perform convolution (Ramachandran et al., 2019; Cordonnier et al., 2020), but they
propose to replace convolution instead of combining them. Recent works have attempted to intro-
duce convolution to vision transformers (Wu et al., 2021; Dai et al., 2021; Gao et al., 2021; Srinivas
et al., 2021), but they mainly focus on image recognition, without any spatiotemporal consideration
for video understanding. Moreover, the combination is almost straightforward in the prior video
transformers, e.g., using transformer as global attention (Wang et al., 2018) or using convolution as
patch stem (Liu et al., 2020b). In contrast, our UniFormer tackles both video redundancy and depen-
dency with an insightful unified framework (Table 1). Via local and global token affinity learning,
we can achieve a preferable computation-accuracy balance for video classification."
MODEL ARCHITECTURE,0.2968197879858657,Published as a conference paper at ICLR 2022
MODEL ARCHITECTURE,0.3003533568904594,"Method
Pretrain
#Frame
GFLOPs
SSV1
SSV2
Top-1
Top-5
Top-1
Top-5
TSN(Wang et al., 2016)
IN-1K
16×1×1
66
19.9
47.3
30.0
60.5
TSM(Lin et al., 2019)
IN-1K
16×1×1
66
47.2
77.1
-
-
GST(Luo & Yuille, 2019)
IN-1K
16×1×1
59
48.6
77.9
62.6
87.9
MSNet(Kwon et al., 2020)
IN-1K
16×1×1
101
52.1
82.3
64.7
89.4
CT-Net(Li et al., 2021a)
IN-1K
16×1×1
75
52.5
80.9
64.5
89.3
CT-NetEN(Li et al., 2021a)
IN-1K
8+12+16+24
280
56.6
83.9
67.8
91.1
TDN(Wang et al., 2020b)
IN-1K
16×1×1
72
53.9
82.1
65.3
89.5
TDNEN(Wang et al., 2020b)
IN-1K
8+16
198
56.8
84.1
68.2
91.6
TimeSformer-HR(Bertasius et al., 2021)
IN-21K
16×3×1
5109
-
-
62.5
-
X-ViT(Bulat et al., 2021)
IN-21K
32×3×1
1270
-
-
65.4
90.7
Mformer-L(Patrick et al., 2021)
K400
32×3×1
3555
-
-
68.1
91.2
ViViT-L(Arnab et al., 2021)
K400
16×3×4
11892
-
-
65.4
89.8
MViT-B,64×3(Fan et al., 2021)
K400
64×1×3
1365
-
-
67.7
90.9
MViT-B-24,32×3(Fan et al., 2021)
K600
32×1×3
708
-
-
68.7
91.5
Swin-B(Liu et al., 2021b)
K400
32×3×1
963
-
-
69.6
92.7
Our UniFormer-S
K400
16×1×1
42
53.8
81.9
63.5
88.5
Our UniFormer-S
K600
16×1×1
42
54.4
81.8
65.0
89.3
Our UniFormer-S
K400
16×3×1
125
57.2
84.9
67.7
91.4
Our UniFormer-S
K600
16×3×1
125
57.6
84.9
69.4
92.1
Our UniFormer-B
K400
16×3×1
290
59.1
86.2
70.4
92.8
Our UniFormer-B
K600
16×3×1
290
58.8
86.5
70.2
93.0
Our UniFormer-B
K400
32×3×1
777
60.9
87.3
71.2
92.8
Our UniFormer-B
K600
32×3×1
777
61.0
87.6
71.2
92.8
Table 3: Comparison with the state-of-the-art on Something-Something V1&V2. Our Uni-
Former achieves new state-of-the-art performances on both datasets."
EXPERIMENTS,0.303886925795053,"4
EXPERIMENTS"
DATASETS AND EXPERIMENTAL SETUP,0.30742049469964666,"4.1
DATASETS AND EXPERIMENTAL SETUP"
DATASETS AND EXPERIMENTAL SETUP,0.31095406360424027,"We conduct experiments on widely-used Kinetics-400 (Carreira & Zisserman, 2017a) and larger
benchmark Kinetics-600 (Carreira et al., 2018). We further verify the transfer learning performance
on temporal-related datasets Something-Something V1&V2 (Goyal et al., 2017b). For training, we
utilize the dense sampling strategy (Wang et al., 2018) for Kinetics and uniform sampling strategy
(Wang et al., 2016) for Something-Something. We adopt the same training recipe as MViT (Fan
et al., 2021) by default, but the random horizontal flip is not applied for Something-Something. To
reduce the total training cost, we inflate the 2D convolution kernels pre-trained on ImageNet for Ki-
netics (Carreira & Zisserman, 2017b). More implementation specifics are shown in Appendix C. For
testing, we explore the sampling strategies in our experiments. To obtain a preferable computation-
accuracy balance, we adopt multi-clip testing for Kinetics and multi-crop testing for Something-
Something. All scores are averaged for the final prediction."
COMPARISON TO STATE-OF-THE-ART,0.31448763250883394,"4.2
COMPARISON TO STATE-OF-THE-ART"
COMPARISON TO STATE-OF-THE-ART,0.31802120141342755,"Kinetics-400&600. Table 2 presents comparisons to the state-of-the-art methods on Kinetics-400
and Kinetics-600. The first part shows the prior works using CNN. Compared with SlowFast (Fe-
ichtenhofer et al., 2019), our UniFormer-S16f requires 42× fewer GFLOPs but obtains 1.0% per-
formance gain on both datasets. Even compared with MoViNet (Kondratyuk et al., 2021), which
is designed through extensive neural architecture search, our model achieves slightly better results
with fewer input frames (16f×4 vs. 120f). The second part lists the recent works based on vi-
sion transformers. With only ImageNet-1K pre-training, UniFormer-B16f surpasses most of the
other backbones with large dataset pre-training. For example, compared with ViViT-L pre-trained
from JFT-300M and Swin-B pre-trained from ImageNet-21K, UniFormer-B32f obtains comparable
performance with 16.7× and 3.3× fewer computation on both Kinetics-400 and Kinetics-600."
COMPARISON TO STATE-OF-THE-ART,0.3215547703180212,"Something-Something V1&V2. Results on Something-Something V1&V2 are shown in Table 3.
Since these datasets depend on temporal relation modeling, it is difficult for the CNN-based methods
to capture long-term dependencies, which leads to their worse results. In contrast, transformer-
based backbones are good at processing long sequential data and demonstrate better transfer learning
capabilities (Zhou et al., 2021). Our UniFormer pre-trained from Kinetis-600 outperforms all the
current methods under the same settings. In fact, our best model achieves the new state-of-the-art"
COMPARISON TO STATE-OF-THE-ART,0.3250883392226148,Published as a conference paper at ICLR 2022
COMPARISON TO STATE-OF-THE-ART,0.3286219081272085,"Unified
Joint
DPE
Type
ImageNet
K400 1×4
GFLOPs
#Param
Top-1
Top-5
GFLOPs
#Param
Top-1
Top-5
""
""
""
LLGG
3.6
21.5
82.9
96.2
41.8
21.4
79.3
94.3
%
""
""
LLGG
3.3
21.3
82.6
96.1
41.0
21.3
78.6
93.6
""
%
""
LLGG
3.6
21.5
82.9
96.2
36.8
27.7
78.7
94.1
""
""
%
LLGG
3.6
21.5
82.4
96.0
41.4
21.3
77.6
93.5
""
""
""
LLLL
3.7
23.3
81.9
95.9
31.6
23.7
77.2
92.9
""
""
""
LLLG
3.7
22.2
82.5
96.1
31.6
22.4
78.4
93.3
""
""
""
LGGG
3.6
21.6
82.7
96.1
39.0
21.4
79.0
94.1
""
""
""
GGGG
3.7
20.1
82.1
95.9
72.0
19.8
75.3
92.4
(a) Structure design. All models are trained for 50 epochs on Kinetics-400. To guarantee the parameters and
computation of all the models are similar, when modifying the stage types, we modify the stage numbers and
reduce the computation of self-attention as MViT (Fan et al., 2021) for LGGG and GGGG."
COMPARISON TO STATE-OF-THE-ART,0.3321554770318021,"Size
K400 1×4
GFLOPs
Top1
3
41.0
79.0
5
41.8
79.3
7
43.6
79.1
9
46.6
78.9
(b) Tube size. Our net-
work is basically robust
to the tube size."
COMPARISON TO STATE-OF-THE-ART,0.33568904593639576,"Type
Joint
GFLOPs
Pretrain
SSV1 Top-1"
COMPARISON TO STATE-OF-THE-ART,0.3392226148409894,"LLLL
""
26.1
ImageNet
49.2
K400
49.2(+0.0)"
COMPARISON TO STATE-OF-THE-ART,0.34275618374558303,"LLGG
%
36.8
ImageNet
51.9
K400
51.8(−0.1)"
COMPARISON TO STATE-OF-THE-ART,0.3462897526501767,"LLGG
""
41.8
ImageNet
52.0
K400
53.8(+1.8)
(c) Transfer learning. Jointly manner performs
better when pre-training from larger dataset."
COMPARISON TO STATE-OF-THE-ART,0.3498233215547703,"Model
Sampling
K400 Top-1
Method
1×1
1×4"
COMPARISON TO STATE-OF-THE-ART,0.35335689045936397,"Small
16×4
76.2
80.8
16×8
78.4
80.7"
COMPARISON TO STATE-OF-THE-ART,0.3568904593639576,"Base
16×4
78.1
82.0
16×8
79.3
81.7"
COMPARISON TO STATE-OF-THE-ART,0.36042402826855124,"Small
32×2
77.3
81.2
32×4
79.8
82.0
(d) Sampling method."
COMPARISON TO STATE-OF-THE-ART,0.36395759717314485,"Table 4: Ablation studies. ‘Unified’ means whether to use our local MHRA (%means to use Mo-
bileNet block). ‘Joint’ means whether to use joint attention. ‘L’/‘G’ refers to local/global MHRA."
COMPARISON TO STATE-OF-THE-ART,0.3674911660777385,"results: 61.0% top-1 accuracy on Something-Something V1 (4.2% higher than TDNEN) (Wang
et al., 2020b) and 71.2% top-1 accuracy on Something-Something V2 (1.6% higher than Swin-B
(Liu et al., 2021b)). Such results verify the capability of spatiotemporal learning for UniFormer."
ABLATION STUDIES,0.3710247349823322,"4.3
ABLATION STUDIES"
ABLATION STUDIES,0.3745583038869258,"UniFormer vs. Convolution: Does transformer-style FFN help? As mentioned in Section 3.2,
our UniFormer block in the shallow layers can be interpreted as a transformer-style spatiotemporal
MobileNet block (Tran et al., 2019) with extra FFN. Hence, we first investigate its effectiveness by
replacing our UniFormer blocks in shallow layers with MobileNet blocks (the expand ratios are set
to 3 for similar parameters). As expected, our default UniFormer outperforms such spatiotemporal
MobileNet block in Table 4a. It shows that, FFN in our UniFormer can further mix token context at
each spatiotemporal position to boost classification accuracy."
ABLATION STUDIES,0.37809187279151946,"UniFormer vs. Transformer: Is joint or divided spatiotemporal attention better? As discussed
in Section 3.2, our UniFormer block in the deep layers can be interpreted as a transformer block, but
our attention is jointly learned in a spatiotemporal manner, instead of dividing spatial and temporal
attention (Bertasius et al., 2021; Arnab et al., 2021). As shown in Table 4a, the joint version is more
powerful than the separate one, showing that joint spatiotemporal attention can learn more discrim-
inative video representations. What’s more, the joint attention is more friendly to transfer learning
with pre-training. As shown in Table 4c, when the model is gradually pre-trained from ImageNet to
Kinetics-400, the performance of our UniFormer becomes better. Such distinct characteristic is not
observed in the pure local MHRA structure (LLLL) and the splitting version. It demonstrates that
the joint learning manner is preferable for video representation learning."
ABLATION STUDIES,0.38162544169611307,"Does dynamic position embedding matter to UniFormer? With dynamic position embedding, our
UniFormer improve the top-1 accuracy by 0.5% and 1.7% on ImageNet and Kinetics-400. It shows
that via encoding the position information, our DPE can maintain spatiotemporal order, contributing
to better spatiotemporal representation learning."
ABLATION STUDIES,0.38515901060070673,"How much does local MHRA help? Since our UniFormer is equipped with local and global token
affinity respectively in the shallow and deep layers, we investigate the configuration of our network
stage by stage. As shown in Table 4a, when we only use local MHRA (LLLL), the computation
cost will be light. However, the accuracy is largely dropped, since the network lacks the capacity
of learning long-term dependency without global MHRA. When we gradually replace local MHRA
with global MHRA, the accuracy becomes better as expected. However, the accuracy is dramatically
dropped with a heavy computation load when all the layers apply global MHRA (GGGG). It is"
ABLATION STUDIES,0.38869257950530034,Published as a conference paper at ICLR 2022
ABLATION STUDIES,0.392226148409894,1 2 3 4 5 6 7 8 9 10
ABLATION STUDIES,0.3957597173144876,#Number of Clips 79 80 81 82 83
ABLATION STUDIES,0.3992932862190813,Top-1 Accuracy (%)
ABLATION STUDIES,0.4028268551236749,"K600-3crop
K600-1crop"
ABLATION STUDIES,0.40636042402826855,1 2 3 4 5 6 7 8 9 10
ABLATION STUDIES,0.4098939929328622,#Number of Clips 76 77 78 79 80 81
ABLATION STUDIES,0.4134275618374558,"K400-3crop
K400-1crop"
ABLATION STUDIES,0.4169611307420495,1 2 3 4 5 6 7 8 9 10
ABLATION STUDIES,0.4204946996466431,#Number of Clips 65 66 67 68 69 70
ABLATION STUDIES,0.42402826855123676,"SthV2-3crop
SthV2-1crop"
ABLATION STUDIES,0.4275618374558304,"1
2
3
4
5
6
7
8
9 10
#Number of Clips 54 55 56 57 58"
ABLATION STUDIES,0.43109540636042404,"SthV1-3crop
SthV1-1crop"
ABLATION STUDIES,0.43462897526501765,"Figure 4: Multi-clip/crop testing comparison on different datasets. Multi-clip testing is better
for Kinetics and multi-crop testing is better for Something-Something."
ABLATION STUDIES,0.4381625441696113,"mainly because that, without local MHRA, the network lacks the capacity of extracting detailed
video representations, leading to severe model overfitting with redundant spatiotemporal attention.
In our experiments, we choose local MHRA and global MHRA in the first two stages and the last
two stages respectively, in order to achieve a preferable computation-accuracy balance."
ABLATION STUDIES,0.4416961130742049,"Is our UniFormer more transferable? We further verify the transfer learning ability of our Uni-
Former in Table 4c. All models share the same stage numbers but the stage types are different.
Compared with pre-training from ImgeNet, pre-training from Kinetics-400 will further improve the
top-1 accuracy by 1.8%. However, such distinct characteristic is not observed in the pure local
MHRA structure and UniFormer with divided spatiotemporal attention. It demonstrates that the
joint learning manner is preferable for transfer learning."
ABLATION STUDIES,0.4452296819787986,"Empirical investigation on model parameters. We further evaluate the robustness of our Uni-
Former network to several important model parameters. (1) size of local tube: In our local token
affinity (Eq. 6), we aggregate spatiotemporal context from a small local tube. Hence, we inves-
tigate the influence of this tube by changing its 3D size (Table 4b). Our network is robust to the
tube size. We choose 5×5×5 for better accuracy. (2) sampling method: We explore the vital
sampling method shown in Table 4d. For training, 16×4 means that we sample 16 frames with
frame stride 4. For testing, 4×1 means four-clip testing. As expected, sparser sampling method
achieves a higher single-clip result. For multi-clip testing, dense sampling is slightly better when
sampling a few frames. However, when sampling more frames, sparse sampling is obviously better.
(3) testing strategy: We evaluate our network with different numbers of clips and crops for the
validation videos. As shown in Figure 4, since Kinetics is a scene-related dataset and trained with
dense sampling, multi-clip testing is preferable to cover more frames for boosting performance. Al-
ternatively, Something-Something is a temporal-related dataset and trained with uniform sampling,
so multi-crop testing is better for capturing the discriminative motion for boosting performance."
VISUALIZATION,0.44876325088339225,"4.4
VISUALIZATION"
VISUALIZATION,0.45229681978798586,"To further verify the effectiveness of UniFormer, we conduct some visualizations of different struc-
tures (see Appendix D). In Figure 5, We apply Grad-CAM (Selvaraju et al., 2019) to show the areas
of the greatest concern in the last layer. It reveals that GGGG struggles to focus on the key object,
i.e., the skateboard and the football, as it blindly compares the similarity of all tokens in all layers.
Alternatively, LLLL only performs local aggregation. Hence, its attention tends to be coarse and
inaccurate without a global view. Different from both cases, our UniFormer with LLGG can coop-
eratively learn local and global contexts in a joint manner. As a result, it can effectively capture the
most discriminative information, by paying precise attention to the skateboard and the football. In
Figure 6, we present the accuracies of different structures on Kinetics-400 (Carreira & Zisserman,
2017a). It shows that LLGG outperforms other structures in most categories, which demonstrates
that our UniFormer takes advantage of both 3D convolution and spatiotemporal self-attention."
CONCLUSION,0.4558303886925795,"5
CONCLUSION"
CONCLUSION,0.45936395759717313,"In this paper, we propose a novel UniFormer, which can effectively unify 3D convolution and spa-
tiotemporal self-attention in a concise transformer format to overcome video redundancy and depen-
dency. We adopt local MHRA in shallow layers to largely reduce computation burden and global
MHRA in deep layers to learn global token relation. Extensive experiments demonstrate that our
UniFormer achieves a preferable balance between accuracy and efficiency on popular video bench-
marks, Kinetics-400/600 and Something-Something V1/V2."
CONCLUSION,0.4628975265017668,Published as a conference paper at ICLR 2022
CONCLUSION,0.4664310954063604,ACKNOWLEDGEMENT
CONCLUSION,0.46996466431095407,"This work is partially supported bythe National Natural Science Foundation of China
(61876176,U1813218), Guangdong NSF Project (No. 2020B1515120085), the Shenzhen Research
Program(RCJC20200714114557087), the Shanghai Committee of Science and Technology, China
(Grant No. 21DZ1100100)."
REFERENCES,0.4734982332155477,REFERENCES
REFERENCES,0.47703180212014135,"A. Arnab, M. Dehghani, G. Heigold, Chen Sun, Mario Lucic, and C. Schmid. Vivit: A video vision
transformer. ArXiv, abs/2103.15691, 2021."
REFERENCES,0.48056537102473496,"Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. ArXiv, abs/1607.06450,
2016."
REFERENCES,0.4840989399293286,"Gedas Bertasius, Heng Wang, and L. Torresani.
Is space-time attention all you need for video
understanding? ArXiv, abs/2102.05095, 2021."
REFERENCES,0.4876325088339223,"Andrew Brock, Soham De, Samuel L. Smith, and K. Simonyan. High-performance large-scale
image recognition without normalization. ArXiv, abs/2102.06171, 2021."
REFERENCES,0.4911660777385159,"Adrian Bulat, Juan-Manuel P´erez-R´ua, Swathikiran Sudhakaran, Brais Mart´ınez, and Georgios Tz-
imiropoulos. Space-time mixing attention for video transformer. ArXiv, abs/2106.05968, 2021."
REFERENCES,0.49469964664310956,"Jo˜ao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics
dataset. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4724–
4733, 2017a."
REFERENCES,0.49823321554770317,"Jo˜ao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, and Andrew Zisserman. A short
note about kinetics-600. ArXiv, abs/1808.01340, 2018."
REFERENCES,0.5017667844522968,"Jo˜ao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics
dataset. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4724–
4733, 2017b."
REFERENCES,0.5053003533568905,"Xiangxiang Chu, Bo Zhang, Zhi Tian, Xiaolin Wei, and Huaxia Xia. Do we really need explicit
position encodings for vision transformers? ArXiv, abs/2102.10882, 2021."
REFERENCES,0.508833922261484,"Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between self-
attention and convolutional layers. ArXiv, abs/1911.03584, 2020."
REFERENCES,0.5123674911660777,"Zihang Dai, Hanxiao Liu, Quoc V. Le, and Mingxing Tan. Coatnet: Marrying convolution and
attention for all data sizes. ArXiv, abs/2106.04803, 2021."
REFERENCES,0.5159010600706714,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-
erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248–255. Ieee, 2009."
REFERENCES,0.519434628975265,"Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen,
and B. Guo. Cswin transformer: A general vision transformer backbone with cross-shaped win-
dows. ArXiv, abs/2107.00652, 2021."
REFERENCES,0.5229681978798587,"A. Dosovitskiy, L. Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, M. Dehghani, Matthias Minderer, G. Heigold, S. Gelly, Jakob Uszkoreit, and
N. Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. ArXiv,
abs/2010.11929, 2021."
REFERENCES,0.5265017667844523,"Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, J. Malik, and Christoph
Feichtenhofer. Multiscale vision transformers. ArXiv, abs/2104.11227, 2021."
REFERENCES,0.5300353356890459,"Christoph Feichtenhofer.
X3d: Expanding architectures for efficient video recognition.
2020
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 200–210, 2020."
REFERENCES,0.5335689045936396,Published as a conference paper at ICLR 2022
REFERENCES,0.5371024734982333,"Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video
recognition. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 6201–
6210, 2019."
REFERENCES,0.5406360424028268,"Peng Gao, Jiasen Lu, Hongsheng Li, R. Mottaghi, and Aniruddha Kembhavi. Container: Context
aggregation network. ArXiv, abs/2106.01401, 2021."
REFERENCES,0.5441696113074205,"Priya Goyal, Piotr Doll´ar, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola,
Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training ima-
genet in 1 hour. ArXiv, abs/1706.02677, 2017a."
REFERENCES,0.5477031802120141,"Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne West-
phal, Heuna Kim, Valentin Haenel, Ingo Fr¨und, Peter Yianilos, Moritz Mueller-Freitag, Florian
Hoppe, Christian Thurau, Ingo Bax, and Roland Memisevic. The “something something” video
database for learning and evaluating visual common sense. 2017 IEEE International Conference
on Computer Vision (ICCV), pp. 5843–5851, 2017b."
REFERENCES,0.5512367491166078,"Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770–778, 2016."
REFERENCES,0.5547703180212014,"Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. ArXiv, abs/1502.03167, 2015."
REFERENCES,0.558303886925795,"Md. Amirul Islam, Sen Jia, and Neil D. B. Bruce. How much position information do convolutional
neural networks encode? ArXiv, abs/2001.08248, 2020."
REFERENCES,0.5618374558303887,"Boyuan Jiang, Mengmeng Wang, Weihao Gan, Wei Wu, and Junjie Yan. Stm: Spatiotemporal and
motion encoding for action recognition. 2019 IEEE International Conference on Computer Vision
(ICCV), pp. 2000–2009, 2019."
REFERENCES,0.5653710247349824,"Zihang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Yujun Shi, Xiaojie Jin, Anran Wang, and Jiashi
Feng. All tokens matter: Token labeling for training better vision transformers. arXiv preprint
arXiv:2104.10858, 2021."
REFERENCES,0.568904593639576,"D. Kondratyuk, Liangzhe Yuan, Yandong Li, Li Zhang, Mingxing Tan, Matthew A. Brown, and
Boqing Gong.
Movinets: Mobile video networks for efficient video recognition.
ArXiv,
abs/2103.11511, 2021."
REFERENCES,0.5724381625441696,"Heeseung Kwon, Manjin Kim, Suha Kwak, and Minsu Cho. Motionsqueeze: Neural motion feature
learning for video understanding. In ECCV, 2020."
REFERENCES,0.5759717314487632,"Kunchang Li, Xianhang Li, Yali Wang, Jun Wang, and Y. Qiao. Ct-net: Channel tensorization
network for video classification. ArXiv, abs/2106.01603, 2021a."
REFERENCES,0.5795053003533569,"X. Li, Yali Wang, Zhipeng Zhou, and Yu Qiao. Smallbignet: Integrating core and contextual views
for video classification. 2020 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 1089–1098, 2020a."
REFERENCES,0.5830388692579506,"Xinyu Li, Yanyi Zhang, Chunhui Liu, Bing Shuai, Yi Zhu, Biagio Brattoli, Hao Chen, Ivan Marsic,
and Joseph Tighe. Vidtr: Video transformer without convolutions. ArXiv, abs/2104.11746, 2021b."
REFERENCES,0.5865724381625441,"Yinong Li, Bin Ji, Xintian Shi, Jianguo Zhang, Bin Kang, and Limin Wang. Tea: Temporal excitation
and aggregation for action recognition. ArXiv, abs/2004.01398, 2020b."
REFERENCES,0.5901060070671378,"Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift module for efficient video understanding.
2019 IEEE International Conference on Computer Vision (ICCV), pp. 7082–7092, 2019."
REFERENCES,0.5936395759717314,"Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, S. Lin, and B. Guo. Swin
transformer: Hierarchical vision transformer using shifted windows.
ArXiv, abs/2103.14030,
2021a."
REFERENCES,0.5971731448763251,"Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, S. Lin, and Han Hu. Video swin transformer.
ArXiv, abs/2106.13230, 2021b."
REFERENCES,0.6007067137809188,Published as a conference paper at ICLR 2022
REFERENCES,0.6042402826855123,"Zhaoyang Liu, D. Luo, Yabiao Wang, L. Wang, Ying Tai, Chengjie Wang, Jilin Li, Feiyue
Huang, and Tong Lu. Teinet: Towards an efficient architecture for video recognition. ArXiv,
abs/1911.09435, 2020a."
REFERENCES,0.607773851590106,"Zhouyong Liu, Shun Luo, Wubin Li, Jingben Lu, Yufan Wu, Chunguo Li, and Luxi Yang.
Convtransformer: A convolutional transformer network for video frame synthesis.
ArXiv,
abs/2011.10185, 2020b."
REFERENCES,0.6113074204946997,"I. Loshchilov and F. Hutter. Fixing weight decay regularization in adam. ArXiv, abs/1711.05101,
2017a."
REFERENCES,0.6148409893992933,"Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv:
Learning, 2017b."
REFERENCES,0.6183745583038869,"Chenxu Luo and Alan L. Yuille. Grouped spatial-temporal aggregation for efficient action recogni-
tion. 2019 IEEE International Conference on Computer Vision (ICCV), pp. 5511–5520, 2019."
REFERENCES,0.6219081272084805,"Daniel Neimark, Omri Bar, Maya Zohar, and Dotan Asselmann. Video transformer network. ArXiv,
abs/2102.00719, 2021."
REFERENCES,0.6254416961130742,"Mandela Patrick, Dylan Campbell, Yuki M. Asano, Ishan Misra Florian Metze, Christoph Feichten-
hofer, A. Vedaldi, and Jo˜ao F. Henriques. Keeping your eye on the ball: Trajectory attention in
video transformers. ArXiv, abs/2106.05392, 2021."
REFERENCES,0.6289752650176679,"Zhaofan Qiu, Ting Yao, and Tao Mei. Learning spatio-temporal representation with pseudo-3d
residual networks. 2017 IEEE International Conference on Computer Vision (ICCV), pp. 5534–
5542, 2017."
REFERENCES,0.6325088339222615,"Zhaofan Qiu, Ting Yao, C. Ngo, Xinmei Tian, and Tao Mei. Learning spatio-temporal representation
with local and global diffusion. 2019 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 12048–12057, 2019."
REFERENCES,0.6360424028268551,"Ilija Radosavovic, Raj Prateek Kosaraju, Ross B. Girshick, Kaiming He, and Piotr Doll´ar. Designing
network design spaces. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 10425–10433, 2020."
REFERENCES,0.6395759717314488,"Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jonathon
Shlens. Stand-alone self-attention in vision models. In NeurIPS, 2019."
REFERENCES,0.6431095406360424,"Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mo-
bilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 4510–4520, 2018."
REFERENCES,0.6466431095406361,"Ramprasaath R. Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael Cogswell, Devi Parikh,
and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based local-
ization. International Journal of Computer Vision, 128:336–359, 2019."
REFERENCES,0.6501766784452296,"Gilad Sharir, Asaf Noy, and Lihi Zelnik-Manor. An image is worth 16x16 words, what is a video
worth? ArXiv, abs/2103.13915, 2021."
REFERENCES,0.6537102473498233,"A. Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, P. Abbeel, and Ashish Vaswani. Bottle-
neck transformers for visual recognition. ArXiv, abs/2101.11605, 2021."
REFERENCES,0.657243816254417,"Mingxing Tan and Quoc V. Le. Efficientnet: Rethinking model scaling for convolutional neural
networks. ArXiv, abs/1905.11946, 2019."
REFERENCES,0.6607773851590106,"Mingxing Tan and Quoc V. Le.
Efficientnetv2: Smaller models and faster training.
ArXiv,
abs/2104.00298, 2021."
REFERENCES,0.6643109540636042,"Hugo Touvron, M. Cord, M. Douze, Francisco Massa, Alexandre Sablayrolles, and Herv’e J’egou.
Training data-efficient image transformers & distillation through attention. In ICML, 2021a."
REFERENCES,0.6678445229681979,"Hugo Touvron, M. Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herv’e J’egou. Going
deeper with image transformers. ArXiv, abs/2103.17239, 2021b."
REFERENCES,0.6713780918727915,Published as a conference paper at ICLR 2022
REFERENCES,0.6749116607773852,"Du Tran, Lubomir D. Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning
spatiotemporal features with 3d convolutional networks. 2015 IEEE International Conference on
Computer Vision (ICCV), pp. 4489–4497, 2015."
REFERENCES,0.6784452296819788,"Du Tran, Hong xiu Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer
look at spatiotemporal convolutions for action recognition. 2018 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pp. 6450–6459, 2018."
REFERENCES,0.6819787985865724,"Du Tran, Heng Wang, L. Torresani, and Matt Feiszli. Video classification with channel-separated
convolutional networks. 2019 IEEE/CVF International Conference on Computer Vision (ICCV),
pp. 5551–5560, 2019."
REFERENCES,0.6855123674911661,"Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. ArXiv, abs/1706.03762, 2017."
REFERENCES,0.6890459363957597,"Heng Wang, Du Tran, L. Torresani, and Matt Feiszli. Video modeling with correlation networks.
2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 349–358,
2020a."
REFERENCES,0.6925795053003534,"L. Wang, Yuanjun Xiong, Zhe Wang, Y. Qiao, D. Lin, X. Tang, and L. Gool. Temporal segment
networks: Towards good practices for deep action recognition. In ECCV, 2016."
REFERENCES,0.696113074204947,"Limin Wang, Zhan Tong, Bin Ji, and Gangshan Wu. Tdn: Temporal difference networks for efficient
action recognition. ArXiv, abs/2012.10071, 2020b."
REFERENCES,0.6996466431095406,"Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao.
Learning deep transformer models for machine translation. In ACL, 2019."
REFERENCES,0.7031802120141343,"Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, P. Luo,
and L. Shao. Pyramid vision transformer: A versatile backbone for dense prediction without
convolutions. ArXiv, abs/2102.12122, 2021."
REFERENCES,0.7067137809187279,"X. Wang, Ross B. Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. 2018
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7794–7803, 2018."
REFERENCES,0.7102473498233216,"Haiping Wu, Bin Xiao, N. Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt:
Introducing convolutions to vision transformers. ArXiv, abs/2103.15808, 2021."
REFERENCES,0.7137809187279152,"Li Yuan, Y. Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis E. H. Tay, Jiashi Feng, and Shuicheng
Yan.
Tokens-to-token vit: Training vision transformers from scratch on imagenet.
ArXiv,
abs/2101.11986, 2021."
REFERENCES,0.7173144876325088,"Xuefan Zha, Wentao Zhu, Tingxun Lv, Sen Yang, and Ji Liu. Shifted chunk transformer for spatio-
temporal representational learning. ArXiv, abs/2108.11575, 2021."
REFERENCES,0.7208480565371025,"Hong-Yu Zhou, Chixiang Lu, Sibei Yang, and Yizhou Yu. Convnets vs. transformers: Whose visual
representations are more transferable? ArXiv, abs/2108.05305, 2021."
REFERENCES,0.7243816254416962,Published as a conference paper at ICLR 2022
REFERENCES,0.7279151943462897,"A
MORE DETAILS ABOUT LOCAL MHRA"
REFERENCES,0.7314487632508834,"For local MHRA, it is vital to determine the neighbor tokens. Considering any token Xk (k ∈
[0, L −1]), we can calculate its index (tk, hk, wk) as follows:"
REFERENCES,0.734982332155477,"tk = ⌊
k
H × W ⌋,
(9)"
REFERENCES,0.7385159010600707,hk = ⌊k −tk × H × W
REFERENCES,0.7420494699646644,"W
⌋,
(10)"
REFERENCES,0.7455830388692579,"wk = (k −tk × H × W) mod W.
(11)"
REFERENCES,0.7491166077738516,"Therefore, for an anchor token Xi, any of its neighbor tokens Xj in Ωt×h×w
i
should satisfy"
REFERENCES,0.7526501766784452,|ti −tj| ≤t
REFERENCES,0.7561837455830389,"2,
(12)"
REFERENCES,0.7597173144876325,|hi −hj| ≤h
REFERENCES,0.7632508833922261,"2 ,
(13)"
REFERENCES,0.7667844522968198,|wi −wj| ≤w
REFERENCES,0.7703180212014135,"2 .
(14)"
REFERENCES,0.773851590106007,Thus the local spatiotemporal affinity in Eq. 6 can be calculated as follows:
REFERENCES,0.7773851590106007,"Alocal
n
(Xi, Xj) = ai−j
(15)
= a[ti −tj, hi −hj, wi −wj].
(16)"
REFERENCES,0.7809187279151943,"For other tokens not in Ωt×h×w
i
, Alocal
n
(Xi, Xj) = 0."
REFERENCES,0.784452296819788,"B
MORE DETAILS ABOUT FFN."
REFERENCES,0.7879858657243817,"We adopt the standard FFN (Eq. 3) in vision transformers (Dosovitskiy et al., 2021),"
REFERENCES,0.7915194346289752,"Z′′ = Linear2 (GELU (Linear1 (Z′))) ,
(17)"
REFERENCES,0.7950530035335689,"where GELU is a non-linear function. The channel number will be first expanded by ratio 4 and
then reduced. All token representations will be enhanced after performing FFN."
REFERENCES,0.7985865724381626,"C
ADDITIONAL IMPLEMENTATION DETAILS"
REFERENCES,0.8021201413427562,"Architecture details. As in ViT (Dosovitskiy et al., 2021), we adopt the pre-normalization configu-
ration (Wang et al., 2019) that applies norm layer at the beginning of the residual function (He et al.,
2016). Differently, we utilize BN (Ioffe & Szegedy, 2015) for local MHRA and LN (Ba et al., 2016)
for global MHRA. Moreover, we add an extra layer normalization in the downsampling layers."
REFERENCES,0.8056537102473498,"Training details. We adopt AdamW (Loshchilov & Hutter, 2017a) optimizer with cosine learning
rate schedule (Loshchilov & Hutter, 2017b) to train the entire network. The first 5 or 10 epochs are
used for warm-up (Goyal et al., 2017a) to overcome early optimization difficulty. For UniFormer-
S, the warmup epoch, total epoch, stochastic depth rate, weight decay are set to 10, 110, 0.1 and
0.05 respectively for Kinetics and 5, 50, 0.3 and 0.05 respectively for Something-Something. For
UniFormer-B, all the hyper-parameters are the same unless the stochastic depth rates are doubled.
We linearly scale the base learning rates according to the batch size, which are 1e−4 × batchsize"
AND,0.8091872791519434,"32
and
2e−4 × batchsize"
AND,0.8127208480565371,"32
for Kinetics and Something-Something."
AND,0.8162544169611308,"D
VISUALIZATION"
AND,0.8197879858657244,"We choose three structures used in our experiments (Table 4a) to make comparisons: LLGG, LLLL
and GGGG. The stage numbers of them are {3, 4, 8, 3}, {3, 5, 10, 4} and {2, 2, 7, 3} respectively."
AND,0.823321554770318,"In Figure 5, we conduct attention visualization of different structures. Part1 shows the input videos
selected from Kinetics-400 (Carreira & Zisserman, 2017a). In part2, we use Grad-CAM (Selvaraju"
AND,0.8268551236749117,Published as a conference paper at ICLR 2022
AND,0.8303886925795053,"Method
Sampling
#Frame
GFLOPs
#Param
K400
K600
stride
Top-1
Top-5
Top-1
Top-5"
AND,0.833922261484099,UniFormer-S
AND,0.8374558303886925,"4
16×1×1
41.8
21.4
76.2
92.2
79.0
93.6
16×1×4
167.2
21.4
80.8
94.7
82.8
95.8"
AND,0.8409893992932862,"8
16×1×1
41.8
21.4
78.4
92.9
80.8
94.7
16×1×4
167.2
21.4
80.8
94.4
82.7
95.7"
AND,0.8445229681978799,"2
32×1×1
109.6
21.4
77.3
92.4
-
-
32×1×4
438.4
21.4
81.2
94.7
-
-"
AND,0.8480565371024735,"4
32×1×1
109.6
21.4
79.8
93.4
-
-
32×1×4
438.4
21.4
82.0
95.1
-
-"
AND,0.8515901060070671,UniFormer-B
AND,0.8551236749116607,"4
16×1×1
96.7
49.8
78.1
92.8
80.3
94.5
16×1×4
386.8
49.8
82.0
95.1
84.0
96.4"
AND,0.8586572438162544,"8
16×1×1
96.7
49.8
79.3
93.4
81.7
95.0
16×1×4
386.8
49.8
81.7
94.8
83.4
96.0"
AND,0.8621908127208481,"4
32×1×1
259
49.8
80.9
94.0
82.7
95.7
32×1×4
1036
49.8
82.9
95.4
84.8
96.7"
AND,0.8657243816254417,Table 5: More results on Kinetics-400&600.
AND,0.8692579505300353,"et al., 2019) to generate the corresponding attention in the last layer. Since GGGG blindly com-
pares the similarity of all tokens in all, it struggles to focus on the key object, i.e., the skateboard
and the football. Alternatively, LLLL only performs local aggregation without a global view, lead-
ing to coarse and inaccurate attention. Different from both cases, our UniFormer with LLGG can
cooperatively learn local and global contexts in a joint manner. As a result, it can effectively capture
the most discriminative information, by paying precise attention to the skateboard and the football."
AND,0.872791519434629,"Additionally, in Figure 6, we show the top-1 accuracies of different structures on Kinetics-400 (Car-
reira & Zisserman, 2017a). It demonstrates that GGGG surpasses the other two structures in most
categories. Furthermore, we analyze the prediction results of several categories in Figure 7. It shows
that gargling is often misjudged as brushing teeth, while swing dancing is often misjudged as other
types of dancing. We argue that these categories are easier to be discriminated against based on the
spatial details. For example, toothbrush often exists in brushing teeth but not in gargling, and the
people’s poses are different in different dancing. Therefore, LLLL performs better than GGGG in
these categories thanks to the capacity of encoding detailed spatiotemporal features. What’s more,
playing guitar and strumming guitar are difficult to be classified, since their spatial contents are
almost the same. They require the long-range dependency between objects, e.g., the interaction be-
tween the people’s hand and the guitar, thus GGGG does better. More importantly, our UniFormer
with LLGG is competitive with the other two methods in these categories, which means it takes
advantage of both 3D convolution and spatiotemporal self-attention."
AND,0.8763250883392226,"E
ADDITIONAL RESULTS"
AND,0.8798586572438163,"E.1
MORE RESULTS ON KINETICS"
AND,0.8833922261484098,"Table 5 shows more results on Kinetics-400 (Carreira & Zisserman, 2017a) and Kinetics-600 (Car-
reira et al., 2018). The trends of the results on both datasets are similar. When sampling with a large
frame stride, the corresponding single-clip testing result will be better. It is mainly because sparser
sampling covers a larger time range. For multi-clip testing, sampling with frame stride 4 always
performs better, thus we adopt frame stride 4 by default."
AND,0.8869257950530035,"E.2
MORE RESULTS ON SOMETHING-SOMETHING"
AND,0.8904593639575972,"Table 6 presents more results on Something-Something V1&V2 (Goyal et al., 2017b).
For
UniFormer-S, pre-training with Kinetics-600 is better than pre-training with Kinetics-400, improv-
ing the top-1 accuracy by approximately 1.5%. However, for UniFormer-B, the improvement is not
obvious. We claim that the small model is difficult to fit, thus larger dataset pre-training can help it
fit better. Besides, UniFormer-B with 16 frames performs better than UniFormer-S with 32 frames."
AND,0.8939929328621908,Published as a conference paper at ICLR 2022
AND,0.8975265017667845,(a) hoverboarding.
AND,0.901060070671378,(b) passing American football (not in game).
AND,0.9045936395759717,"Figure 5: Attention visualization of different structures. Videos are chosen from Kinetics-400 (Car-
reira & Zisserman, 2017a)."
AND,0.9081272084805654,Published as a conference paper at ICLR 2022
AND,0.911660777385159,Figure 6: Accuracy of different structures on Kinetics-400.
AND,0.9151943462897526,Figure 7: Prediction comparisons of different structures.
AND,0.9187279151943463,"Method
Pretrain
#Frame
GFLOPs
#Param
SSV1
SSV2
Top-1
Top-5
Top-1
Top-5"
AND,0.9222614840989399,UniFormer-S K400
AND,0.9257950530035336,"16×1×1
41.8
21.3
53.8
81.9
63.5
88.5
16×3×1
125.4
21.3
57.2
84.9
67.7
91.4
16×3×2
250.8
21.3
57.3
85.1
68.1
91.7 K600"
AND,0.9293286219081273,"16×1×1
41.8
21.3
54.4
81.8
65.0
89.3
16×3×1
125.4
21.3
57.6
84.9
69.4
92.1
16×3×2
250.8
21.3
57.8
84.9
69.5
92.2 K400"
AND,0.9328621908127208,"32×1×1
109.6
21.3
55.8
83.6
64.9
89.2
32×3×1
328.8
21.3
58.8
86.4
69.0
91.7
32×3×2
657.6
21.3
58.9
86.6
69.2
91.8 K600"
AND,0.9363957597173145,"32×1×1
109.6
21.3
56.9
83.8
66.4
90.2
32×3×1
328.8
21.3
59.9
86.2
70.4
93.1
32×3×2
657.6
21.3
59.9
86.3
70.5
92.9"
AND,0.9399293286219081,UniFormer-B K400
AND,0.9434628975265018,"16×1×1
96.7
49.7
55.4
82.9
65.8
89.9
16×3×1
290.1
49.7
59.1
86.2
70.4
92.8
16×3×2
580.2
49.7
59.3
86.4
70.7
92.9 K600"
AND,0.9469964664310954,"16×1×1
96.7
49.7
55.7
83.3
66.1
90.0
16×3×1
290.1
49.7
58.8
86.5
70.2
93.0
16×3×2
580.2
49.7
59.1
86.5
70.7
92.9 K400"
AND,0.950530035335689,"32×1×1
259
49.7
58.1
84.9
67.2
90.2
32×3×1
777
49.7
60.9
87.3
71.2
92.8
32×3×2
1554
49.7
61.0
87.3
71.4
92.8 K600"
AND,0.9540636042402827,"32×1×1
259
49.7
58.0
84.9
67.5
90.2
32×3×1
777
49.7
61.0
87.6
71.2
92.8
32×3×2
1554
49.7
61.2
87.6
71.3
92.8"
AND,0.9575971731448764,Table 6: More results on Something-Something V1&V2.
AND,0.9611307420494699,Published as a conference paper at ICLR 2022
AND,0.9646643109540636,"E.3
COMPARSION TO STATE-OF-THE-ART ON IMAGENET"
AND,0.9681978798586572,"Table 7 compares our method with the state-of-the-art ImageNet (Deng et al., 2009). We design four
model variants as follows:"
AND,0.9717314487632509,"• UniFormer-S: channel numbers={64, 128, 320, 512}, stage numbers={3, 4, 8, 3}"
AND,0.9752650176678446,"• UniFormer-S†: channel numbers={64, 128, 320, 512}, stage numbers={3, 5, 9, 3}"
AND,0.9787985865724381,"• UniFormer-B: channel numbers={64, 128, 320, 512}, stage numbers={5, 8, 20, 7}"
AND,0.9823321554770318,"• UniFormer-L: channel numbers={128, 192, 448, 640}, stage numbers={5, 10, 24, 7}"
AND,0.9858657243816255,"All the other model parameters are the same as we mention in Section 3.4. For UniFormer-S†,
we adopt overlapped convolutional patch embedding. All the training hyper-parameters are the
same as DeiT (Touvron et al., 2021a) by defaults. When training our models with Token Labeling,
we follow the settings used in LV-ViT (Jiang et al., 2021). It shows that our models outperform
other methods with similar parameters/FLOPs on ImageNet, especially when training with Token
Labeling. Moreover, our model surpasses those models combining CNN with Transformer, e.g.,
CvT (Wu et al., 2021) and CoAtNet (Dai et al., 2021), which reflects our UniFormer can unify
convolution and self-attention better for preferable accuracy-computation balance."
AND,0.9893992932862191,Published as a conference paper at ICLR 2022
AND,0.9929328621908127,"Method
Architecture
#Param
GFLOPs
Train
Test
ImageNet
Size
Size
Top-1
RegNetY-4G (Radosavovic et al., 2020)
CNN
21
4.0
224
224
80.0
EffcientNet-B5 (Tan & Le, 2019)
CNN
30
9.9
456
456
83.6
EfficientNetV2-S (Tan & Le, 2021)
CNN
22
8.5
384
384
83.9
DeiT-S (Touvron et al., 2021a)
Trans
22
4.6
224
224
79.9
PVT-S (Wang et al., 2021)
Trans
25
3.8
224
224
79.8
T2T-14 (Yuan et al., 2021)
Trans
22
5.2
224
224
80.7
Swin-T (Liu et al., 2021a)
Trans
29
4.5
224
224
81.3
CSwin-T ↑384 (Dong et al., 2021)
Trans
23
14.0
224
384
84.3
LV-ViT-S (Jiang et al., 2021)
Trans
26
6.6
224
224
83.3
LV-ViT-S ↑384 (Jiang et al., 2021)
Trans
26
22.2
224
384
84.4
CvT-13 (Wu et al., 2021)
CNN+Trans
20
4.5
224
224
81.6
CvT-13 ↑384 (Wu et al., 2021)
CNN+Trans
20
16.3
224
384
83.0
CoAtNet-0 (Dai et al., 2021)
CNN+Trans
25
4.2
224
224
81.6
CoAtNet-0 ↑384 (Dai et al., 2021)
CNN+Trans
20
13.4
224
384
83.9
Container (Gao et al., 2021)
CNN+Trans
22
8.1
224
224
82.7
UniFormer-S
CNN+Trans
22
3.6
224
224
82.9
UniFormer-S+TL
CNN+Trans
22
3.6
224
224
83.4
UniFormer-S+TL ↑384
CNN+Trans
22
11.9
224
384
84.6
UniFormer-S†
CNN+Trans
24
4.2
224
224
83.4
UniFormer-S†+TL
CNN+Trans
24
4.2
224
224
83.9
UniFormer-S†+TL ↑384
CNN+Trans
24
13.7
224
384
84.9
RegNetY-8G (Radosavovic et al., 2020)
CNN
39
8.0
224
224
81.7
EffcientNet-B7 (Tan & Le, 2019)
CNN
66
39.2
600
600
84.3
EfficientNetV2-M (Tan & Le, 2021)
CNN
54
25.0
480
480
85.1
PVT-L (Wang et al., 2021)
Trans
61
9.8
224
224
81.7
T2T-24 (Yuan et al., 2021)
Trans
64
13.2
224
224
82.2
Swin-S (Liu et al., 2021a)
Trans
50
8.7
224
224
83.0
CSwin-S ↑384 (Dong et al., 2021)
Trans
35
22.0
224
384
85.0
LV-ViT-M (Jiang et al., 2021)
Trans
56
16.0
224
224
84.1
LV-ViT-M ↑384 (Jiang et al., 2021)
Trans
56
42.2
224
384
85.4
CvT-21 (Wu et al., 2021)
CNN+Trans
32
7.1
224
224
82.5
CvT-21 ↑384 (Wu et al., 2021)
CNN+Trans
32
24.9
224
384
83.3
CoAtNet-1 (Dai et al., 2021)
CNN+Trans
42
8.4
224
224
83.3
CoAtNet-1 ↑384 (Dai et al., 2021)
CNN+Trans
42
27.4
224
384
85.1
UniFormer-B
CNN+Trans
50
8.3
224
224
83.9
UniFormer-B+TL
CNN+Trans
50
8.3
224
224
85.1
UniFormer-B+TL ↑384
CNN+Trans
50
27.2
224
384
86.0
RegNetY-16G (Radosavovic et al., 2020)
CNN
84
16.0
224
224
82.9
EfficientNetV2-L (Tan & Le, 2021)
CNN
121
53
480
480
85.7
NFNet-F4 (Brock et al., 2021)
CNN
316
215.3
384
512
85.9
NFNet-F5 (Brock et al., 2021)
CNN
377
289.8
416
544
86.0
DeiT-B Touvron et al. (2021a)
Trans
86
17.5
224
224
81.8
Swin-B (Liu et al., 2021a)
Trans
88
15.4
224
224
83.3
CSwin-B ↑384 (Dong et al., 2021)
Trans
78
47.0
224
384
85.4
LV-ViT-L (Jiang et al., 2021)
Trans
150
59.0
288
288
85.3
LV-ViT-L ↑448 (Jiang et al., 2021)
Trans
150
157.2
288
448
85.9
CaiT-S48 ↑384 (Touvron et al., 2021b)
Trans
89
63.8
224
384
85.1
CaiT-M36 ↑448Υ (Touvron et al., 2021b)
Trans
271
247.8
224
448
86.3
BoTNet-T7 (Srinivas et al., 2021)
CNN+Trans
79
19.3
256
256
84.2
BoTNet-T7 ↑384 (Srinivas et al., 2021)
CNN+Trans
79
45.8
256
384
84.7
CoAtNet-3 (Dai et al., 2021)
CNN+Trans
168
34.7
224
224
84.5
CoAtNet-3 ↑384 (Dai et al., 2021)
CNN+Trans
168
107.4
224
384
85.8
UniFormer-L+TL
CNN+Trans
100
12.6
224
224
85.6
UniFormer-L+TL ↑384
CNN+Trans
100
39.2
224
384
86.3"
AND,0.9964664310954063,"Table 7: Comparison with the state-of-the-art on ImageNet. ‘Train Size’ and ‘Test Size’ refer to
resolutions used in training and fine-tuning respectively. ‘TL’ means token labeling proposed in
LV-ViT (Jiang et al., 2021). We group the models based on their parameters."
