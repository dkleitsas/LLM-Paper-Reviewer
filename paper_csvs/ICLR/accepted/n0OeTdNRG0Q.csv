Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002932551319648094,"Overparametrized Deep Neural Networks (DNNs) often achieve astounding per-
formances, but may potentially result in severe generalization error. Recently,
the relation between the sharpness of the loss landscape and the generalization
error has been established by Foret et al. (2020), in which the Sharpness Aware
Minimizer (SAM) was proposed to mitigate the degradation of the generaliza-
tion. Unfortunately, SAM’s computational cost is roughly double that of base
optimizers, such as Stochastic Gradient Descent (SGD). This paper thus proposes
Efﬁcient Sharpness Aware Minimizer (ESAM), which boosts SAM’s efﬁciency at
no cost to its generalization performance. ESAM includes two novel and efﬁ-
cient training strategies—Stochastic Weight Perturbation and Sharpness-Sensitive
Data Selection. In the former, the sharpness measure is approximated by per-
turbing a stochastically chosen set of weights in each iteration; in the latter, the
SAM loss is optimized using only a judiciously selected subset of data that is
sensitive to the sharpness. We provide theoretical explanations as to why these
strategies perform well. We also show, via extensive experiments on the CIFAR
and ImageNet datasets, that ESAM enhances the efﬁciency over SAM from re-
quiring 100% extra computational overhead to 40% vis-`a-vis base optimizers,
while test accuracies are preserved or even improved. Our codes are avaliable
at https://github.com/dydjw9/Efficient_SAM."
INTRODUCTION,0.005865102639296188,"1
INTRODUCTION"
INTRODUCTION,0.008797653958944282,"Deep learning has achieved astounding performances in many ﬁelds by relying on larger numbers of
parameters and increasingly sophisticated optimization algorithms. However, DNNs with far more
parameters than training samples are more prone to poor generalization. Generalization is arguably
the most fundamental and yet mysterious aspect of deep learning."
INTRODUCTION,0.011730205278592375,"Several studies have been conducted to better understand the generalization of DNNs and to train
DNNs that generalize well across the natural distribution (Keskar et al., 2017; Neyshabur et al., 2017;
Chaudhari et al., 2019; Zhang et al., 2019; Wu et al., 2020; Foret et al., 2020; Zhang et al., 2021). For
example, Keskar et al. (2017) investigate the effect of batch size on neural networks’ generalization
ability. Zhang et al. (2019); Zhou et al. (2021) propose optimizers for training DNNs with improved
generalization ability. Speciﬁcally, Hochreiter & Schmidhuber (1995), Li et al. (2018) and Dinh
et al. (2017) argue that the geometry of the loss landscape affects generalization and DNNs with a
ﬂat minimum can generalize better. The recent work by Foret et al. (2020) proposes an effective
training algorithm Sharpness Aware Minimizer (SAM) for obtaining a ﬂat minimum. SAM employs
a base optimizer such as Stochastic Gradient Descent (Nesterov, 1983) or Adam (Kingma & Ba,
2015) to minimize both the vanilla training loss and the sharpness. The sharpness, which describes"
INTRODUCTION,0.01466275659824047,∗corresponding author
INTRODUCTION,0.017595307917888565,Published as a conference paper at ICLR 2022
INTRODUCTION,0.020527859237536656,"the ﬂatness of a minimum, is characterized using eigenvalues of the Hessian matrix by Keskar et al.
(2017). SAM quantiﬁes the sharpness as the maximized change of training loss when a constraint
perturbation is added to current weights. As a result, SAM leads to a ﬂat minimum and signiﬁcantly
improves the generalization ability of the trained DNNs. SAM and its variants have been shown
to outperform the state-of-the-art across a variety of deep learning benchmarks (Kwon et al., 2021;
Chen et al., 2021; Galatolo et al., 2021; Zheng et al., 2021). Regrettably though, SAM and its vari-
ants achieve such remarkable performance at the expense of doubling the computational overhead
of the given base optimizers, which minimize the training loss with a single forward and backward
propagation step. SAM requires an additional propagation step compared to the base optimizers to
resolve the weight perturbation for quantifying the sharpness. The extra propagation step requires
the same computational overhead as the single propagation step used by base optimizers, resulting
in SAM’s computational overhead being doubled (2×). As demonstrated in Figure 1, SAM achieves
higher test accuracy (i.e., 84.46% vs. 81.89%) at the expense of sacriﬁcing half of the training speed
of the base optimizer (i.e., 276 imgs/s vs. 557 imgs/s)."
INTRODUCTION,0.02346041055718475,"SGD
SAM
ESAM
80 81 82 83 84 85 86"
INTRODUCTION,0.026392961876832845,Accuracy (%) 100 200 300 400 500 600 700
INTRODUCTION,0.02932551319648094,Training Speed (images/s)
INTRODUCTION,0.03225806451612903,"Accuracy
Training Speed"
INTRODUCTION,0.03519061583577713,"Figure 1: Training Speed vs. Accuracy of
SGD, SAM and ESAM evaluated by Pyra-
midNet on CIFAR100. ESAM improves the
efﬁciency with better accuracy compared to
SAM."
INTRODUCTION,0.03812316715542522,"In this paper, we aim to improve the efﬁciency of
SAM but preserve its superior performance in general-
ization.
We propose Efﬁcient Sharpness Aware Mini-
mizer (ESAM), which consists of two training strategies
Stochastic Weight Perturbation (SWP) and Sharpness-
sensitive Data Selection (SDS), both of which reduce
computational overhead and preserve the performance of
SAM. On the one hand, SWP approximates the sharp-
ness by searching weight perturbation within a stochasti-
cally chosen neighborhood of the current weights. SWP
preserves the performance by ensuring that the expected
weight perturbation is identical to that solved by SAM.
On the other hand, SDS improves efﬁciency by ap-
proximately optimizing weights based on the sharpness-
sensitive subsets of batches.
These subsets consist of
samples whose loss values increase most w.r.t. the weight
perturbation and consequently can better quantify the
sharpness of DNNs. As a result, the sharpness calculated over the subsets can serve as an upper
bound of the SAM’s sharpness, ensuring that SDS’s performance is comparable to that of SAM’s."
INTRODUCTION,0.04105571847507331,"We verify the effectiveness of ESAM on the CIFAR10, CIFAR100 (Krizhevsky et al., 2009) and Im-
ageNet (Deng et al., 2009) datasets with ﬁve different DNN architectures. The experimental results
demonstrate that ESAM obtains ﬂat minima at a cost of only 40% (vs. SAM’s 100%) extra com-
putational overhead over base optimizers. More importantly, ESAM achieves better performance in
terms of the test accuracy compared to SAM. In a nutshell, our contributions are as follows:"
INTRODUCTION,0.04398826979472141,"• We propose two novel and effective training strategies Stochastic Weight Perturbation
(SWP) and Sharpness-sensitive Data Selection (SDS). Both strategies are designed to im-
prove efﬁciency without sacriﬁcing performance. The empirical results demonstrate that
both of the proposed strategies can improve both the efﬁciency and effectiveness of SAM."
INTRODUCTION,0.0469208211143695,"• We introduce the ESAM, which integrates SWP and SDS. ESAM improves the generaliza-
tion ability of DNNs with marginally additional computational cost compared to standard
training."
INTRODUCTION,0.04985337243401759,"The rest of this paper is structured in this way. Section 2.1 introduces SAM and its computational
issues. Section 2.2 and Section 2.3 discuss how the two proposed training strategies SWP and SDS
are designed respectively. Section 3 veriﬁes the effectiveness of ESAM across a variety of datasets
and DNN architectures. Section 4 presents the related work and Section 5 concludes this paper."
METHODOLOGY,0.05278592375366569,"2
METHODOLOGY"
METHODOLOGY,0.05571847507331378,"We start with recapitulating how SAM achieves a ﬂat minimum with small sharpness, which is
quantiﬁed by resolving a maximization problem. To compute the sharpness, SAM requires addi-
tional forward and backward propagation and results in the doubling of the computational overhead"
METHODOLOGY,0.05865102639296188,Published as a conference paper at ICLR 2022
METHODOLOGY,0.06158357771260997,Algorithm 1 Efﬁcient SAM (ESAM)
METHODOLOGY,0.06451612903225806,"Input: Network fθ with parameters θ = (θ1, θ2, . . . , θN); Training set S; Batch size b; Learning
rate η > 0; Neighborhood size ρ > 0; Number of iterations A; SWP hyperparameter β; SDS
hyperparameter γ.
Output: A ﬂat minimum solution ˆθ."
METHODOLOGY,0.06744868035190615,"1: for a = 1 to A do
2:
Sample a mini-batch B ⊂S with size b.
3:
for n = 1 to N do
4:
if θn is chosen by probability β then
5:
ϵn ←
ρ
1−β ∇θnLB(fθ)
▷SWP in B1
6:
else
7:
ϵn ←0
8:
ˆϵ ←(ϵ1, ..., ϵN)
▷Assign Weight Perturbation
9:
Compute ℓ(fθ+ˆϵ, xi, yi) and construct B+ with selection ratio γ (Equation 6)
10:
Compute gradients g = ∇θLB+(fθ+ˆϵ)
▷SDS in B2
11:
Update weights θ ←θ −ηg"
METHODOLOGY,0.07038123167155426,"compared to base optimizers. Following that, we demonstrate how we derive and propose ESAM,
which integrates SWP and SDS, to maximize efﬁciency while maintaining the performance. We in-
troduce SWP and SDS in Sections 2.2 and 2.3 respectively. Algorithm 1 shows the overall proposed
ESAM algorithm."
METHODOLOGY,0.07331378299120235,"Throughout this paper, we denote a neural network f with weight parameters θ as fθ. The weights
are contained in the vector θ = (θ1, θ2, . . . , θN), where N is the number of weight units in the
neural network. Given a training dataset S that contains samples i.i.d. drawn from a distribution D,
the network is trained to obtain optimal weights ˆθ via empirical risk minimization (ERM), i.e.,"
METHODOLOGY,0.07624633431085044,"ˆθ = arg min
θ"
METHODOLOGY,0.07917888563049853,"
LS(fθ) = 1 |S| X"
METHODOLOGY,0.08211143695014662,"(xi,yi)∈S
ℓ(fθ, xi, yi)

.
(1)"
METHODOLOGY,0.08504398826979472,"where ℓcan be an arbitrary loss function. We take ℓto be the cross entropy loss in this paper.
The population loss is deﬁned as LD(fθ) ≜E(xi,yi)∼D

ℓ(fθ, xi, yi)

. In each training iteration,
optimizers sample a mini-batch B ⊂S with size b to update parameters."
SHARPNESS-AWARE MINIMIZATION AND ITS COMPUTATIONAL DRAWBACK,0.08797653958944282,"2.1
SHARPNESS-AWARE MINIMIZATION AND ITS COMPUTATIONAL DRAWBACK"
SHARPNESS-AWARE MINIMIZATION AND ITS COMPUTATIONAL DRAWBACK,0.09090909090909091,"To improve the generalization capability of DNNs, Foret et al. (2020) proposed the SAM training
strategy for searching ﬂat minima. SAM trains DNNs by solving the following min-max optimiza-
tion problem,
min
θ
max
ϵ:∥ϵ∥2≤ρ LS(fθ+ϵ).
(2)"
SHARPNESS-AWARE MINIMIZATION AND ITS COMPUTATIONAL DRAWBACK,0.093841642228739,"Given θ, the inner optimization attempts to ﬁnd a weight perturbation ϵ in Euclidean ball with radius
ρ that maximizes the empirical loss. The maximized loss at weights θ is the sum of the empirical
loss and the sharpness, which is deﬁned to be RS(fθ) = maxϵ:∥ϵ∥2<ρ[LS(fθ+ϵ) −LS(fθ)]. This
sharpness is quantiﬁed by the maximal change of empirical loss when a perturbation ϵ (whose norm
is constrained by ρ) is added to θ. The min-max problem encourages SAM to ﬁnd ﬂat minima."
SHARPNESS-AWARE MINIMIZATION AND ITS COMPUTATIONAL DRAWBACK,0.0967741935483871,"For a certain set of weights θ, Foret et al. (2020) theoretically justiﬁes that the population loss of
DNNs can be upper-bounded by the sum of sharpness, empirical loss, and a regularization term on
the norm of weights (refer to Equation 3). Thus, by minimizing the sharpness together with the
empirical loss, SAM produces optimized solutions for DNNs with ﬂat minima, and the resultant
models can thus generalize better (Foret et al., 2020; Chen et al., 2021; Kwon et al., 2021). Indeed,
we have
LD(fθ) ≤RS(fθ) + LS(fθ) + λ∥θ∥2
2 =
max
ϵ:∥ϵ∥2≤ρ LS(fθ+ϵ) + λ∥θ∥2
2.
(3)"
SHARPNESS-AWARE MINIMIZATION AND ITS COMPUTATIONAL DRAWBACK,0.09970674486803519,"In practice, SAM ﬁrst approximately solves the inner optimization by means of a single-step gradient
descent method, i.e.,
ˆϵ = arg max
ϵ:∥ϵ∥2<ρ
LS(fθ+ϵ) ≈ρ∇θLS(fθ).
(4)"
SHARPNESS-AWARE MINIMIZATION AND ITS COMPUTATIONAL DRAWBACK,0.10263929618768329,Published as a conference paper at ICLR 2022
SHARPNESS-AWARE MINIMIZATION AND ITS COMPUTATIONAL DRAWBACK,0.10557184750733138,"The sharpness at weights θ is approximated by RS(fθ) = LS(fθ+ˆϵ) −LS(fθ). Then, a base opti-
mizer, such as SGD (Nesterov, 1983) or Adam (Kingma & Ba, 2015), updates the DNNs’ weights to
minimize LS(fθ+ˆϵ). We refer to LS(fθ+ˆϵ) as the SAM loss. Overall, SAM requires two forward and
two backward operations to update weights once. We refer to the forward and backward propagation
for approximating ˆϵ as F1 and B1 and those for updating weights by base optimizers as F2 and B2
respectively. Although SAM can effectively improve the generalization of DNNs, it additionally re-
quires one forward and one backward operation (F1 and B1) in each training iteration. Thus, SAM
results in a doubling of the computational overhead compared to the use of base optimizers."
SHARPNESS-AWARE MINIMIZATION AND ITS COMPUTATIONAL DRAWBACK,0.10850439882697947,"To improve the efﬁciency of SAM, we propose ESAM, which consists of two strategies—SWP
and SDS, to accelerate the sharpness approximation phase and the weight updating phase. Specif-
ically, on the one hand, when estimating ˆϵ around weight vector θ, SWP efﬁciently approximates
ˆϵ by randomly selecting each parameter with a given probability to form a subset of weights to
be perturbed. The reduction of the number of perturbed parameters results in lower computational
overhead during the backward propagation. SWP rescales the resultant weight perturbation so as
to assure that the expected weight perturbation equals to ˆϵ, and the generalization capability thus
will not be signiﬁcantly degraded. On the other hand, when updating weights via base optimizers,
instead of computing the upper bound LB(fθ+ˆϵ) over a whole batch of samples, SDS selects a subset
of samples, B+, whose loss values increase the most with respect to the perturbation ˆϵ. Optimizing
the weights based on a fewer number of samples decreases the computational overhead (in a linear
fashion). We further justify that LB(fθ+ˆϵ) can be upper bounded by LB+(fθ+ˆϵ) and consequently
the generalization capability can be preserved. In general, ESAM works much more efﬁciently and
performs as well as SAM in terms of the generalization capability."
STOCHASTIC WEIGHT PERTURBATION,0.11143695014662756,"2.2
STOCHASTIC WEIGHT PERTURBATION"
STOCHASTIC WEIGHT PERTURBATION,0.11436950146627566,"This section elaborates on the ﬁrst efﬁciency enhancement strategy, SWP, and explains why SWP
can effectively reduce computational overhead while preserving the generalization capability."
STOCHASTIC WEIGHT PERTURBATION,0.11730205278592376,"To efﬁciently approximate ˆϵ(θ, S) during the sharpness estimation phase, SWP randomly chooses a
subset ˜θ = {θI1, θI2, . . .} from the original set of weights θ = (θ1, . . . , θN) to perform backprop-
agation B1. Each parameter is selected to be in the subvector ˜θ with some probability β, which
can be tuned as a hyperparameter. SWP approximates the weight perturbation with ρ∇˜θLS(fθ)."
STOCHASTIC WEIGHT PERTURBATION,0.12023460410557185,"To be formal, we introduce a gradient mask m = (m1, . . . , mN) where mi
i.i.d.
∼
Bern(β) for all
i ∈{1, . . . , N}. Then, we have ρ∇˜θLS(fθ) = m⊤ˆϵ(θ, B). To ensure the expected weight per-
turbation of SWP equals to ˆϵ, we scale ρ∇˜θLS(fθ) by a factor of 1"
STOCHASTIC WEIGHT PERTURBATION,0.12316715542521994,"β . Finally, SWP produces an
approximate solution of the inner maximization as"
STOCHASTIC WEIGHT PERTURBATION,0.12609970674486803,"a(θ, B) = m⊤ˆϵ(θ, B)"
STOCHASTIC WEIGHT PERTURBATION,0.12903225806451613,"β
.
(5)"
STOCHASTIC WEIGHT PERTURBATION,0.13196480938416422,"Computation
Ideally, SWP reduces the overall computational overhead in proportion to 1 −β
in B1. However, there exists some parameters not included in ˜θ that are still required to be updated
in the backpropagation step. This additional computational overhead is present due to the use of the
chain rule, which calculates the entire set of gradients with respect to the parameters along a prop-
agation path. This additional computational overhead slightly increases in deeper neural networks.
Thus, the amount of reduction in the computational overhead is positively correlated to 1 −β. In
practice, β is tuned to maximize SWP’s efﬁciency while maintaining a generalization performance
comparable to SAM’s."
STOCHASTIC WEIGHT PERTURBATION,0.1348973607038123,"Generalization
We will next argue that SWP’s generalization performance can be preserved when
compared to SAM by showing that the expected weight perturbation a(θ, B) of SWP equals to
the original SAM’s perturbation ˆϵ(θ, B) in the sense of the ℓ2 norm and direction. We denote the
expected SWP perturbation by ¯a(θ, B), where"
STOCHASTIC WEIGHT PERTURBATION,0.1378299120234604,"¯a(θ, B)[i] = E[a(θ, B)[i]] = 1"
STOCHASTIC WEIGHT PERTURBATION,0.14076246334310852,"β · βˆϵ(θ, B)[i] = ˆϵ(θ, B)[i],"
STOCHASTIC WEIGHT PERTURBATION,0.1436950146627566,"for i ∈{1, . . . , N}. Thus, it holds that"
STOCHASTIC WEIGHT PERTURBATION,0.1466275659824047,"∥¯a(θ, B)∥2 = ∥ˆϵ(θ, B)∥2
and
CosSim

¯a(θ, B), ˆϵ(θ, B)

= 1,"
STOCHASTIC WEIGHT PERTURBATION,0.1495601173020528,Published as a conference paper at ICLR 2022
STOCHASTIC WEIGHT PERTURBATION,0.15249266862170088,showing that the expected weight perturbation of SWP is the same as that of SAM’s.
SHARPNESS-SENSITIVE DATA SELECTION,0.15542521994134897,"2.3
SHARPNESS-SENSITIVE DATA SELECTION"
SHARPNESS-SENSITIVE DATA SELECTION,0.15835777126099707,"L
+(f
+ )"
SHARPNESS-SENSITIVE DATA SELECTION,0.16129032258064516,"L
(f
+ ) = 0 0.40 0.80 1.20 1.60 2.00 2.40 2.80 3.20 3.60"
SHARPNESS-SENSITIVE DATA SELECTION,0.16422287390029325,"Figure 2:
Illustration on the loss
changes of samples in B+ and B−"
SHARPNESS-SENSITIVE DATA SELECTION,0.16715542521994134,"along the weight perturbation ˆϵ. The
average loss of samples in B+ in-
creases the most along the perturbation
direction ˆϵ."
SHARPNESS-SENSITIVE DATA SELECTION,0.17008797653958943,"In this section, we introduce the second efﬁciency enhance-
ment technique, SDS, which reduces computational overhead
of SAM linearly as the number of selected samples decreases.
We also explain why the generalization capability of SAM is
preserved by SDS."
SHARPNESS-SENSITIVE DATA SELECTION,0.17302052785923755,"In the sharpness estimation phase, we obtain the approximate
solution ˆϵ of the inner maximization. Perturbing weights along
this direction signiﬁcantly increases the average loss over a
batch B. To improve the efﬁciency but still control the upper
bound LB(fθ+ˆϵ), we select a subset of samples from the whole
batch. The loss values of this subset of samples increase most
when the weights are perturbed by ˆϵ. To be speciﬁc, SDS splits
the mini-batch B into the following two subsets"
SHARPNESS-SENSITIVE DATA SELECTION,0.17595307917888564,"B+ :=

(xi, yi) ∈B : ℓ(fθ+ˆϵ, xi, yi) −ℓ(fθ, xi, yi) > α
	
,"
SHARPNESS-SENSITIVE DATA SELECTION,0.17888563049853373,"B−:=

(xi, yi) ∈B : ℓ(fθ+ˆϵ, xi, yi) −ℓ(fθ, xi, yi) < α
	
,
(6)"
SHARPNESS-SENSITIVE DATA SELECTION,0.18181818181818182,"where B+ is termed as the sharpness-sensitive subset and the
threshold α controls the size of B+. We let γ = |B+|/|B| be the ratio of the number of selected
samples with respect to the batch size. In practice, γ determines the exact value of α and serves as
a predeﬁned hyperparameter of SDS. As illustrated in Figure 2, when α = 0, the gradient of the
weights evaluated on B+ aligns with the direction of ˆϵ and the loss values of the samples in B+ will
increase with respect to the weight perturbation ˆϵ."
SHARPNESS-SENSITIVE DATA SELECTION,0.18475073313782991,"Computation
SDS reduces the computational overhead in F2 and B2. The reduction is linear
in 1 −γ. The hyperparameter γ can be tuned to meet up distinct requirements in efﬁciency and
performance. SDS is conﬁgured the same as SWP for maximizing efﬁciency with comparable per-
formance to SAM."
SHARPNESS-SENSITIVE DATA SELECTION,0.187683284457478,"Generalization
For the generalization capability, we now justify that the SAM loss computing
over the batch B, LB(fθ+ˆϵ), can be approximately upper bounded by the corresponding loss evalu-
ated only on B+, LB+(fθ+ˆϵ). From Equation 3, we have"
SHARPNESS-SENSITIVE DATA SELECTION,0.1906158357771261,"LB(fθ+ˆϵ) = γLB+(fθ+ˆϵ) + (1 −γ)LB−(fθ+ˆϵ)
= LB+(fθ+ˆϵ) + (1 −γ)[LB−(fθ+ˆϵ) −LB+(fθ+ˆϵ)]
= LB+(fθ+ˆϵ) + (1 −γ)[RB−(fθ) + LB−(fθ) −RB+(fθ) −LB+(fθ)].
(7)"
SHARPNESS-SENSITIVE DATA SELECTION,0.1935483870967742,"On the one hand, since RB(fθ) =
1
|B|
P"
SHARPNESS-SENSITIVE DATA SELECTION,0.19648093841642228,"(xi,yi)∈B[ℓ(fθ+ˆϵ, xi, yi) −ℓ(fθ, xi, yi)] represents the aver-
age sharpness of the batch B, by Equation 6, we have RB−(fθ) ≤RB(fθ) ≤RB+(fθ), and"
SHARPNESS-SENSITIVE DATA SELECTION,0.19941348973607037,"RB−(fθ) −RB+(fθ) ≤0.
(8)"
SHARPNESS-SENSITIVE DATA SELECTION,0.20234604105571846,"On the other hand, B+ and B−are constructed by sorting ℓ(fθ+ˆϵ, xi, yi) −ℓ(fθ, xi, yi), which is
positively correlated to l(fθ, xi, yi) (Li et al., 2019) (more details can be found in Appendix A.2).
Thus, we have
LB−(fθ) −LB+(fθ) ≤0.
(9)"
SHARPNESS-SENSITIVE DATA SELECTION,0.20527859237536658,"Therefore, by Equation 8 and Equation 9, we have"
SHARPNESS-SENSITIVE DATA SELECTION,0.20821114369501467,"LB(fθ+ˆϵ) ≤LB+(fθ+ˆϵ).
(10)"
SHARPNESS-SENSITIVE DATA SELECTION,0.21114369501466276,"Experimental results in Figure 5 corroborate that RB−(fθ)−RB+(fθ) < 0 and LB−(fθ)−LB+(fθ) <
0. Besides, Figure 6 veriﬁes that the selected batch B+ is sufﬁciently representative to mimic the
gradients of B since B+ has a signiﬁcantly higher cosine similarity with B compared to B−in terms
of the computed gradients. According to Equation 10, one can utilize LB+(fθ+ˆϵ) as a proxy to the
real objective to minimize of the overall loss LB(fθ+ˆϵ) with a smaller number of samples. As a
result, SDS improves SAM’s efﬁciency without performance degradation."
SHARPNESS-SENSITIVE DATA SELECTION,0.21407624633431085,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.21700879765395895,"3
EXPERIMENTS"
EXPERIMENTS,0.21994134897360704,"This section demonstrates the effectiveness of our proposed ESAM algorithm. We conduct experi-
ments on several benchmark datasets: CIFAR-10 (Krizhevsky et al., 2009), CIFAR-100 (Krizhevsky
et al., 2009) and ImageNet (Deng et al., 2009), using various model architectures: ResNet (He et al.,
2016), Wide ResNet (Zagoruyko & Komodakis, 2016), and PyramidNet (Han et al., 2017). We
demonstrate the proposed ESAM improves the efﬁciency of vanilla SAM by speeding up to 40.3%
computational overhead with better generalization performance. We report the main results in Ta-
ble 1 and Table 2. Besides, we perform an ablation study on the two proposed strategies of ESAM
(i.e., SWP and SDS). The experimental results in Table 3 and Figure 3 indicate that both strategies
improve SAM’s efﬁciency and performance."
RESULTS,0.22287390029325513,"3.1
RESULTS"
RESULTS,0.22580645161290322,"CIFAR10 and CIFAR100 We start from evaluating ESAM on the CIFAR-10 and CIFAR-100 image
classiﬁcation datasets. The evaluation is carried out on three different model architectures: ResNet-
18 (He et al., 2016), WideResNet-28-10 (Zagoruyko & Komodakis, 2016) and PyramidNet-110
(Han et al., 2017). We set all the training settings, including the maximum number of training
epochs, iterations per epoch, and data augmentations, the same for fair comparison among SGD,
SAM and ESAM. Additionally, the other hyperparameters of SGD, SAM and ESAM have been
tuned separately for optimal test accuracies using grid search."
RESULTS,0.2287390029325513,"We train all the models with 3 different random seeds using a batch size of 128, weight decay 10−4
and cosine learning rate decay (Loshchilov & Hutter, 2017). The training epochs are set to be 200
for ResNet-18 (He et al., 2016), WideResNet-28-10 (Zagoruyko & Komodakis, 2016), and 300 for
PyramidNet-110 (Han et al., 2017). We set β = 0.6 and γ = 0.5 for ResNet-18 and PyramidNet-
110 models; and set β = 0.5 and γ = 0.5 for WideResNet-28-10. The above-mentioned β and γ
are optimal for efﬁciency with comparable performance compared to SAM. The details of training
setting are listed in Appendix A.7. We record the best test accuracies obtained by SGD, SAM and
ESAM in Table 1."
RESULTS,0.2316715542521994,"The experimental results indicate that our proposed ESAM can increase the training speed by up to
40.30% in comparison with SAM. Concerning the performance, ESAM outperforms SAM in the six
sets of experiments. The best efﬁciency of ESAM is reported in CIFAR10 trained with ResNet-18
(Training speed 140.3% vs. SAM 100%). The best accuracy is reported in CIFAR100 trained with
PyramidNet110 (Accuracy 85.56% vs. SAM 84.46%). ESAM improves efﬁciency and achieves
better performance compared to SAM in CIFAR10/100 benchmarks."
RESULTS,0.23460410557184752,"Table 1: Classiﬁcation accuracies and training speed on the CIFAR-10 and CIFAR-100 datasets. Compu-
tational overhead is quantiﬁed by #images processed per second (images/s). The numbers in parentheses (·)
indicate the ratio of ESAM’s training speed w.r.t. SAM."
RESULTS,0.2375366568914956,"CIFAR-10
CIFAR-100"
RESULTS,0.2404692082111437,"ResNet-18
Accuracy
images/s
Accuracy
images/s"
RESULTS,0.2434017595307918,"SGD
95.41± 0.03
3,387
78.17± 0.05
3,483
SAM
96.52± 0.13
1,717(100.0%)
80.17± 0.17
1,730 (100.0%)
ESAM
96.56± 0.08
2,409 (140.3%)
80.41± 0.10
2,423 (140.0%)"
RESULTS,0.24633431085043989,"Wide-28-10
Accuracy
images/s
Accuracy
images/s"
RESULTS,0.24926686217008798,"SGD
96.34± 0.12
801
81.56± 0.13
792
SAM
97.27± 0.11
396 (100.0%)
83.42± 0.04
391 (100.0%)
ESAM
97.29± 0.11
550 (138.9%)
84.51± 0.01
545 (139.4%)"
RESULTS,0.25219941348973607,"PyramidNet-110
Accuracy
images/s
Accuracy
images/s"
RESULTS,0.25513196480938416,"SGD
96.62± 0.10
580
81.89± 0.17
555
SAM
97.30 ± 0.10
289 (100.0%)
84.46± 0.04
276 (100.0%)
ESAM
97.81± 0.01
401 (138.7%)
85.56± 0.05
381 (137.9%)"
RESULTS,0.25806451612903225,"ImageNet To evaluate ESAM’s effectiveness on a large-scale benchmark dataset, we conduct ex-
periments on ImageNet Datasets.The 1000 class ImageNet dataset contains roughly 1.28 million"
RESULTS,0.26099706744868034,Published as a conference paper at ICLR 2022
RESULTS,0.26392961876832843,"Table 2: Classiﬁcation accuracies and training speed on the ImageNet dataset. The numbers in parentheses (·)
indicate the ratio of ESAM’s training speed w.r.t. SAM’s. Results with * are referred to Chen et al. (2021)"
RESULTS,0.2668621700879765,"ResNet-50
ResNet-101"
RESULTS,0.2697947214076246,"ImageNet
Accuracy
images/s
Accuracy
images/s"
RESULTS,0.2727272727272727,"SGD
76.00*
1,327
77.80*
891
SAM
76.70*
654 (100.0%)
78.60*
438 (100.0%)
ESAM
77.05
846 (129.3%)
79.09
564 (128.7%)"
RESULTS,0.2756598240469208,"training images and 50, 000 validation images with 469 × 387 averaged resolution. The ImageNet
dataset is more representative (of real-world scenarios) and persuasive (of a method’s effectiveness)
than CIFAR datasets. We resize the images on ImageNet to 224 × 224 resolution to train ResNet-50
and ResNet-101 models. We train 90 epochs and set the optimal hyperparameters for SGD, SAM
and ESAM as suggested by Chen et al. (2021), and the details are listed in appendix A.7. We use
β = 0.6 and γ = 0.7 for ResNet-50 and ResNet-101. We employ the m-sharpness strategy for both
SAM and ESAM with m = 128, which is the same as that suggested in Zheng et al. (2021)."
RESULTS,0.2785923753665689,"The experimental results are reported in Table 2. The results indicate that the performance of ESAM
on large-scale datasets is consistent with the two (smaller) CIFAR datasets. ESAM outperforms
SAM by 0.35% to 0.49% in accuracy and, more importantly, enjoys 28.7% faster training speed
compared to SAM. As the γ we used here is larger than the one used in CIFAR datasets, the training
speed of ESAM here is slightly slower than that in the CIFAR datasets."
RESULTS,0.28152492668621704,"These experiments demonstrate that ESAM outperforms SAM on a variety of benchmark datasets
for widely-used DNNs’ architectures in terms of training speed and classiﬁcation accuracies."
ABLATION AND PARAMETER STUDIES,0.2844574780058651,"3.2
ABLATION AND PARAMETER STUDIES"
ABLATION AND PARAMETER STUDIES,0.2873900293255132,"To better understand the effectiveness of SWP and SDS in improving the performance and efﬁciency
compared to SAM, we conduct four sets of ablation studies on CIFAR-10 and CIFAR-100 datasets
using ResNet-18 and WideResNet-28-10 models, respectively. We consider two variants of ESAM:
(i) only with SWP, (ii) only with SDS. The rest of the experimental settings are identical to the
settings described in Section 3.1. We conduct grid search over the interval [0.3, 0.9] for β and the
interval [0.3, 0.9] for γ, with a same step size of 0.1. We report the grid search results in Figure 3.
We use β = 0.6,γ = 0.5 for ResNet-18; and set β = 0.5, γ = 0.5 for WideResNet-28-10 in the
four sets of ablation studies. The ablation study results are reported in Table 3."
ABLATION AND PARAMETER STUDIES,0.2903225806451613,"Table 3: Ablation Study of ESAM on CIFAR-10 and CIFAR100. The numbers in brackets [·] represent the
accuracy improvement in comparison to SGD. The numbers in parentheses (·) indicate the ratio of ESAM’s
training speed to SAM’s. Green color indicates improvement compared to SAM, whereas red color suggests a
degradation."
ABLATION AND PARAMETER STUDIES,0.2932551319648094,"CIFAR-10
CIFAR-100
ResNet-18
Accuracy
images/s
Accuracy
images/s"
ABLATION AND PARAMETER STUDIES,0.2961876832844575,"SGD
95.41
3,387
78.17
3,438
SAM
96.52 [+1.11]
1,717 (100.0%)
80.17 [+2.00]
1,730 (100.0%)"
ABLATION AND PARAMETER STUDIES,0.2991202346041056,"+ ESAM-SWP
96.74 [+1.33]
1,896 (110.5%)
80.53 [+2.36]
1,887 (109.1%)
+ ESAM-SDS
96.45 [+1.04]
2,105 (122.6%)
80.38 [+2.21]
2,103 (121.5%)
ESAM
96.56 [+1.15]
2,409 (140.3%)
80.41 [+2.24]
2,423 (140.9%)"
ABLATION AND PARAMETER STUDIES,0.3020527859237537,"Wide-28-10
Accuracy
images/s
Accuracy
images/s"
ABLATION AND PARAMETER STUDIES,0.30498533724340177,"SGD
96.34
801
81.56
792
SAM
97.27 [+0.93]
396 (100.0%)
83.42 [+1.86]
391 (100.0%)"
ABLATION AND PARAMETER STUDIES,0.30791788856304986,"+ ESAM-SWP
97.37 [+1.03]
430 (108.5%)
84.44 [+2.88]
423 (108.3%)
+ ESAM-SDS
97.24 [+0.90]
495 (124.8%)
84.46 [+2.90]
492 (125.8%)
ESAM
97.29 [+0.95]
551 (138.9%)
84.51 [+2.95]
545 (139.4%)"
ABLATION AND PARAMETER STUDIES,0.31085043988269795,Published as a conference paper at ICLR 2022
ABLATION AND PARAMETER STUDIES,0.31378299120234604,"14
16
18
20
22
24
26
28
30
Elapsed Time Per Epoch 95.25 95.50 95.75 96.00 96.25 96.50 96.75 97.00 97.25"
ABLATION AND PARAMETER STUDIES,0.31671554252199413,Accuracy
ABLATION AND PARAMETER STUDIES,0.3196480938416422,β = 0.6
ABLATION AND PARAMETER STUDIES,0.3225806451612903,β = 0.3
ABLATION AND PARAMETER STUDIES,0.3255131964809384,γ = 0.5
ABLATION AND PARAMETER STUDIES,0.3284457478005865,γ = 0.3
ABLATION AND PARAMETER STUDIES,0.3313782991202346,Resnet18 on CIFAR10
ABLATION AND PARAMETER STUDIES,0.3343108504398827,"SGD
SAM
ESAM-SWP
ESAM-SDS
ESAM"
ABLATION AND PARAMETER STUDIES,0.33724340175953077,"14
16
18
20
22
24
26
28
30
Elapsed Time Per Epoch 78.0 78.5 79.0 79.5 80.0 80.5 81.0"
ABLATION AND PARAMETER STUDIES,0.34017595307917886,Accuracy
ABLATION AND PARAMETER STUDIES,0.34310850439882695,β = 0.6
ABLATION AND PARAMETER STUDIES,0.3460410557184751,β = 0.3
ABLATION AND PARAMETER STUDIES,0.3489736070381232,γ = 0.5
ABLATION AND PARAMETER STUDIES,0.3519061583577713,γ = 0.3
ABLATION AND PARAMETER STUDIES,0.3548387096774194,Resnet18 on CIFAR100
ABLATION AND PARAMETER STUDIES,0.35777126099706746,"SGD
SAM
ESAM-SWP
ESAM-SDS
ESAM"
ABLATION AND PARAMETER STUDIES,0.36070381231671556,"60
70
80
90
100
110
120
130
Elapsed Time Per Epoch 96.4 96.6 96.8 97.0 97.2 97.4 97.6"
ABLATION AND PARAMETER STUDIES,0.36363636363636365,Accuracy
ABLATION AND PARAMETER STUDIES,0.36656891495601174,β = 0.5
ABLATION AND PARAMETER STUDIES,0.36950146627565983,β = 0.3
ABLATION AND PARAMETER STUDIES,0.3724340175953079,γ = 0.5
ABLATION AND PARAMETER STUDIES,0.375366568914956,γ = 0.3
ABLATION AND PARAMETER STUDIES,0.3782991202346041,Wide-28-10 on CIFAR10
ABLATION AND PARAMETER STUDIES,0.3812316715542522,"SGD
SAM
ESAM-SWP
ESAM-SDS
ESAM"
ABLATION AND PARAMETER STUDIES,0.3841642228739003,"60
70
80
90
100
110
120
130
Elapsed Time Per Epoch 81 82 83 84 85"
ABLATION AND PARAMETER STUDIES,0.3870967741935484,Accuracy
ABLATION AND PARAMETER STUDIES,0.39002932551319647,β = 0.5
ABLATION AND PARAMETER STUDIES,0.39296187683284456,β = 0.3
ABLATION AND PARAMETER STUDIES,0.39589442815249265,γ = 0.5
ABLATION AND PARAMETER STUDIES,0.39882697947214074,γ = 0.3
ABLATION AND PARAMETER STUDIES,0.40175953079178883,Wide-28-10 on CIFAR100
ABLATION AND PARAMETER STUDIES,0.4046920821114369,"SGD
SAM
ESAM-SWP
ESAM-SDS
ESAM"
ABLATION AND PARAMETER STUDIES,0.40762463343108507,"Figure 3: Parameter study of SWP and SDS. The connected dots refer to SWP and SDS with different param-
eters; the isolated dots refer to the ﬁnal results of SGD, SAM, and ESAM."
ABLATION AND PARAMETER STUDIES,0.41055718475073316,"ESAM-SWP As shown in Table 3, SWP improves SAM’s training speed by 8.3% to 10.5%, and
achieves better performance at the same time. SWP can further improve the efﬁciency by using a
smaller β. The best performance of SWP is obtained when β = 0.6 for ResNet-18 and β = 0.5
for WideResNet-28-10. The four sets of experiments indicate that β is consistent among different
architectures and datasets. Therefore, we set β = 0.6 for PyramidNet on CIFAR10/100 datasets and
ResNet on ImageNet datasets."
ABLATION AND PARAMETER STUDIES,0.41348973607038125,"ESAM-SDS SDS also signiﬁcantly improves the efﬁciency by 21.5% to 25.8% compared to SAM.
It outperforms SAM’s performance on CIFAR100 datasets, and achieves comparable performance
on CIFAR10 datasets. SDS can outperform SAM on both datasets with both architectures with little
degradation to the efﬁciency, as demonstrated in Figure 3. Across all experiments, γ = 0.5 is the
smallest value that is optimal for efﬁciency while maintaining comparable performance to SAM."
ABLATION AND PARAMETER STUDIES,0.41642228739002934,"Visualization of Loss Landscapes To visualize the sharpness of the ﬂat minima obtained by ESAM,
we plot the loss landscapes trained with SGD, SAM and ESAM on the ImageNet dataset. We display
the loss landscapes in Figure 4, following the plotting algorithm in Li et al. (2018). The x- and y-
axes represent two random sampled orthogonal Gaussian perturbations. We sampled 100 × 100
points for 10 groups random Gaussian perturbations. The displayed loss landscapes are the results
we obtained by averaging over ten groups of random perturbations. It can be clearly seen that both
SAM and ESAM improve the sharpness signiﬁcantly in comparison to SGD."
ABLATION AND PARAMETER STUDIES,0.41935483870967744,"To summarize, SWP and SDS both reduce the computational overhead and accelerate training com-
pared to SAM. Most importantly, both these strategies achieve a comparable or better performance
than SAM. In practice, by conﬁguring the β and γ, ESAM can meet a variety of user-deﬁned efﬁ-
ciency and performance requirements."
RELATED WORK,0.4222873900293255,"4
RELATED WORK"
RELATED WORK,0.4252199413489736,"The concept of regularizing sharpness for better generalization dates back to (Hochreiter & Schmid-
huber, 1995). By using an MDL-based argument, which clariﬁes that a statistical model with fewer"
RELATED WORK,0.4281524926686217,Published as a conference paper at ICLR 2022
RELATED WORK,0.4310850439882698,"1.0
0.5
0.0
0.5
1.0
1.0
0.5
0.0
0.5
1.0"
RELATED WORK,0.4340175953079179,Training Loss 0 1 2 3 4 5 SGD
RELATED WORK,0.436950146627566,"1.0
0.5
0.0
0.5
1.0
1.0
0.5
0.0
0.5
1.0"
RELATED WORK,0.4398826979472141,Training Loss 0 1 2 3 4 5 SAM
RELATED WORK,0.44281524926686217,"1.0
0.5
0.0
0.5
1.0
1.0
0.5
0.0
0.5
1.0"
RELATED WORK,0.44574780058651026,Training Loss 0 1 2 3 4 5 ESAM 1 2 3 4
RELATED WORK,0.44868035190615835,"Figure 4: Cross-entropy loss landscapes of the ResNet50 model on the ImageNet dataset trained with SGD,
SAM, and ESAM."
RELATED WORK,0.45161290322580644,"bits to describe can have better generalization ability, Hochreiter & Schmidhuber (1995) claim that
a ﬂat minimum can alleviate overﬁtting issues. Following that, more studies were proposed to in-
vestigate the connection between the ﬂat minima with the generalization abilities (Keskar et al.,
2017; Dinh et al., 2017; Liu et al., 2020; Li et al., 2018; Dziugaite & Roy, 2017; Jiang et al., 2019;
Moosavi-Dezfooli et al., 2019). Keskar et al. (2017) starts by investigating the phenomenon that
training with a larger batch size results in worse generalization ability. The authors found that the
sharpness of the minimum is critical in accounting for the observed phenomenon. Keskar et al.
(2017) and Dinh et al. (2017) both argue that the sharpness can be characterized using the eigenval-
ues of the Hessian. Although they also deﬁne speciﬁc notions and methods to quantify sharpness,
they do not propose complete training strategies to ﬁnd minima that are relative “ﬂat”."
RELATED WORK,0.45454545454545453,"SAM (Foret et al., 2020) leverages the connection between “ﬂat” minima and the generalization error
to train DNNs that generalize well across the natural distribution. Inspired by Keskar et al. (2017)
and Dinh et al. (2017), SAM ﬁrst proposes the quantiﬁcation of the sharpness, which is achieved by
solving a maximization problem. Then, SAM proposes a complete training algorithm to improve the
generalization abilities of DNNs. SAM is demonstrated to achieve state-of-the-art performance in
a variety of deep learning benchmarks, including image classiﬁcation, natural language processing,
and noisy learning (Foret et al., 2020; Chen et al., 2021; Kwon et al., 2021; Pham et al., 2021; Yuan
et al., 2021; Jia et al., 2021)."
RELATED WORK,0.4574780058651026,"A series of SAM-related works has been proposed. A work that was done contemporaneously SAM
(Wu et al., 2020) also regularizes the sharpness term in adversarial training and achieves much more
robust generalization performance against adversarial attacks. Many works focus on combining
SAM with other training strategies or architectures (Chen et al., 2021; Wang et al., 2022; Tseng
et al., 2021), or apply SAM on other tasks (Zheng et al., 2021; Damian et al., 2021; Galatolo et al.,
2021). Kwon et al. (2021) improves SAM’s sharpness by adaptively scaling the size of the nearby
search space ρ in relation to the size of parameters. Liu et al. (2022) leverages the past calculated
weight perturbations to save SAM’s computations. However, most of these works overlook the fact
that SAM improves generalization at the expense of the doubling the computational overhead. As
a result, most of the SAM-related works suffer from the same efﬁciency drawback as SAM. This
computational cost prevents SAM from being widely used in large-scale datasets and architectures,
particularly in real-world applications, which motivates us to propose ESAM to efﬁciently improve
the generalization ability of DNNs."
CONCLUSION,0.4604105571847507,"5
CONCLUSION"
CONCLUSION,0.4633431085043988,"In this paper, we propose the Efﬁcient Sharpness Aware Minimizer (ESAM) to enhance the efﬁciency
of vanilla SAM. The proposed ESAM integrates two novel training strategies, namely, SWP and
SDS, both of which are derived based on theoretical underpinnings and are evaluated over a variety
of datasets and DNN architectures. Both SAM and ESAM are two-step training strategies consisting
of sharpness estimation and weight updating. In each step, gradient back-propagation is performed
to compute the weight perturbation or updating. In future research, we will explore how to combine
the two steps into one by utilizing the information of gradients in previous iterations so that the
computational overhead of ESAM can be reduced to the same as base optimizers."
CONCLUSION,0.4662756598240469,Published as a conference paper at ICLR 2022
CONCLUSION,0.46920821114369504,ACKNOWLEDGEMENT
CONCLUSION,0.47214076246334313,"Jiawei Du and Joey Tianyi Zhou are suppored by Joey Tianyi Zhou’s A*STAR SERC Central Re-
search Fund."
CONCLUSION,0.4750733137829912,"Hanshu Yan and Vincent Tan are funded by a Singapore National Research Foundation (NRF) Fel-
lowship (R-263-000-D02-281) and a Singapore Ministry of Education AcRF Tier 1 grant (R-263-
000-E80-114)."
CONCLUSION,0.4780058651026393,"We would like to express our special thanks of gratitude to Dr. Yuan Li for helping us conduct
experiments on ImageNet, and Dr. Wang Yangzihao for helping us implement Distributed Data
Parallel codes."
REFERENCES,0.4809384164222874,REFERENCES
REFERENCES,0.4838709677419355,"Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian
Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-SGD: Biasing gradi-
ent descent into wide valleys. Journal of Statistical Mechanics: Theory and Experiment, 2019
(12):124018, 2019."
REFERENCES,0.4868035190615836,"Xiangning Chen, Cho-Jui Hsieh, and Boqing Gong. When vision transformers outperform resnets
without pretraining or strong data augmentations. arXiv preprint arXiv:2106.01548, 2021."
REFERENCES,0.4897360703812317,"Alex Damian, Tengyu Ma, and Jason D Lee. Label noise sgd provably prefers ﬂat global minimizers.
Advances in Neural Information Processing Systems, 34, 2021."
REFERENCES,0.49266862170087977,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-
erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248–255. Ieee, 2009."
REFERENCES,0.49560117302052786,"Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize
for deep nets. In International Conference on Machine Learning, pp. 1019–1028. PMLR, 2017."
REFERENCES,0.49853372434017595,"Jiawei Du, Hu Zhang, Joey Tianyi Zhou, Yi Yang, and Jiashi Feng. Query-efﬁcient meta attack to
deep neural networks. In International Conference on Learning Representations, 2019."
REFERENCES,0.501466275659824,"Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. arXiv preprint
arXiv:1703.11008, 2017."
REFERENCES,0.5043988269794721,"Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimiza-
tion for efﬁciently improving generalization. In International Conference on Learning Represen-
tations, 2020."
REFERENCES,0.5073313782991202,"Alessio Galatolo, Alfred Nilsson, Roderick Karlemstrand, and Yineng Wang. Using early-learning
regularization to classify real-world noisy data. arXiv preprint arXiv:2105.13244, 2021."
REFERENCES,0.5102639296187683,"Dongyoon Han, Jiwhan Kim, and Junmo Kim. Deep pyramidal residual networks. In Proceedings
of the IEEE conference on computer vision and pattern recognition, pp. 5927–5935, 2017."
REFERENCES,0.5131964809384164,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778, 2016."
REFERENCES,0.5161290322580645,"Sepp Hochreiter and J¨urgen Schmidhuber. Simplifying neural nets by discovering ﬂat minima. In
Advances in neural information processing systems, pp. 529–536, 1995."
REFERENCES,0.5190615835777126,"Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan
Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning
with noisy text supervision. In International Conference on Machine Learning, pp. 4904–4916.
PMLR, 2021."
REFERENCES,0.5219941348973607,Published as a conference paper at ICLR 2022
REFERENCES,0.5249266862170088,"Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantas-
tic generalization measures and where to ﬁnd them. In International Conference on Learning
Representations, 2019."
REFERENCES,0.5278592375366569,"Nitish Shirish Keskar, Jorge Nocedal, Ping Tak Peter Tang, Dheevatsa Mudigere, and Mikhail
Smelyanskiy. On large-batch training for deep learning: Generalization gap and sharp minima.
In 5th International Conference on Learning Representations, ICLR 2017, 2017."
REFERENCES,0.530791788856305,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR (Poster),
2015."
REFERENCES,0.533724340175953,"Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. CIFAR-10 and CIFAR-100 datasets. URl:
https://www. cs. toronto. edu/kriz/cifar. html, 6(1):1, 2009."
REFERENCES,0.5366568914956011,"Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. Asam: Adaptive sharpness-
aware minimization for scale-invariant learning of deep neural networks. In International Con-
ference on Machine Learning, pp. 5905–5914. PMLR, 2021."
REFERENCES,0.5395894428152492,"Buyu Li, Yu Liu, and Xiaogang Wang. Gradient harmonized single-stage detector. In Proceedings
of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pp. 8577–8584, 2019."
REFERENCES,0.5425219941348973,"Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss land-
scape of neural nets. Advances in neural information processing systems, 31, 2018."
REFERENCES,0.5454545454545454,"Chen Liu, Mathieu Salzmann, Tao Lin, Ryota Tomioka, and Sabine S¨usstrunk. On the loss landscape
of adversarial training: Identifying challenges and how to overcome them. Advances in Neural
Information Processing Systems, 33:21476–21487, 2020."
REFERENCES,0.5483870967741935,"Yong Liu, Siqi Mai, Xiangning Chen, Cho-Jui Hsieh, and Yang You. Towards efﬁcient and scalable
sharpness-aware minimization. arXiv preprint arXiv:2203.02714, 2022."
REFERENCES,0.5513196480938416,Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. 2017.
REFERENCES,0.5542521994134897,"Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Jonathan Uesato, and Pascal Frossard. Robust-
ness via curvature regularization, and vice versa. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 9078–9086, 2019."
REFERENCES,0.5571847507331378,"Yurii E Nesterov. A method for solving the convex programming problem with convergence rate
O(1/k2). In Dokl. akad. nauk Sssr, volume 269, pp. 543–547, 1983."
REFERENCES,0.5601173020527859,"Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring general-
ization in deep learning. Advances in neural information processing systems, 30, 2017."
REFERENCES,0.5630498533724341,"Hieu Pham, Zihang Dai, Qizhe Xie, and Quoc V Le. Meta pseudo labels. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11557–11568, 2021."
REFERENCES,0.5659824046920822,"Chongli Qin, James Martens, Sven Gowal, Dilip Krishnan, Krishnamurthy Dvijotham, Alhussein
Fawzi, Soham De, Robert Stanforth, and Pushmeet Kohli. Adversarial robustness through local
linearization. Advances in Neural Information Processing Systems, 32, 2019."
REFERENCES,0.5689149560117303,"Ching-Hsun Tseng, Shin-Jye Lee, Jia-Nan Feng, Shengzhong Mao, Yu-Ping Wu, Jia-Yu Shang,
Mou-Chung Tseng, and Xiao-Jun Zeng. Upanets: Learning from the universal pixel attention
networks. arXiv preprint arXiv:2103.08640, 2021."
REFERENCES,0.5718475073313783,"Pichao Wang, Xue Wang, Hao Luo, Jingkai Zhou, Zhipeng Zhou, Fan Wang, Hao Li, and Rong Jin.
Scaled relu matters for training vision transformers. 2022."
REFERENCES,0.5747800586510264,"Dongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust gener-
alization. Advances in Neural Information Processing Systems, 33:2958–2969, 2020."
REFERENCES,0.5777126099706745,"Hanshu Yan, Jiawei Du, Vincent Tan, and Jiashi Feng. On robustness of neural ordinary differential
equations. In International Conference on Learning Representations, 2019."
REFERENCES,0.5806451612903226,Published as a conference paper at ICLR 2022
REFERENCES,0.5835777126099707,"Hanshu Yan, Jingfeng Zhang, Gang Niu, Jiashi Feng, Vincent YF Tan, and Masashi Sugiyama.
CIFS: Improving adversarial robustness of CNNs via channel-wise importance-based feature se-
lection. International Conference on Machine Learning, 2021."
REFERENCES,0.5865102639296188,"Li Yuan, Qibin Hou, Zihang Jiang, Jiashi Feng, and Shuicheng Yan. Volo: Vision outlooker for
visual recognition. arXiv preprint arXiv:2106.13112, 2021."
REFERENCES,0.5894428152492669,"Sergey Zagoruyko and Nikos Komodakis.
Wide residual networks.
In British Machine Vision
Conference 2016. British Machine Vision Association, 2016."
REFERENCES,0.592375366568915,"Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107–
115, 2021."
REFERENCES,0.5953079178885631,"Michael Zhang, James Lucas, Jimmy Ba, and Geoffrey E Hinton. Lookahead optimizer: k steps
forward, 1 step back. Advances in Neural Information Processing Systems, 32, 2019."
REFERENCES,0.5982404692082112,"Yaowei Zheng, Richong Zhang, and Yongyi Mao. Regularizing neural networks via adversarial
model perturbation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 8156–8165, 2021."
REFERENCES,0.6011730205278593,"Pan Zhou, Hanshu Yan, Xiaotong Yuan, Jiashi Feng, and Shuicheng Yan. Towards understanding
why lookahead generalizes better than SGD and beyond.
In Proc. Conf. Neural Information
Processing Systems, 2021."
REFERENCES,0.6041055718475073,Published as a conference paper at ICLR 2022
REFERENCES,0.6070381231671554,"A
APPENDIX"
REFERENCES,0.6099706744868035,"A.1
THE ALGORITHM OF SAM"
REFERENCES,0.6129032258064516,The algorithm of SAM is demonstrated in Algorithm 2.
REFERENCES,0.6158357771260997,Algorithm 2 SGD vs. SAM
REFERENCES,0.6187683284457478,"Input: Network fθ, θ = (θ1, θ1, . . . , θN), Training set S, Batch size b, Learning rate η, Neighbor-
hood size ρ, Iterations A.
Output: A minimum solution ˜θ."
REFERENCES,0.6217008797653959,"1: for a = 1 to A do
2:
Sample B with size b that B ⊂S,
3:
if SGD then
4:
ˆϵ ←0
▷No addtional computational overhead for ˆϵ
5:
else if SAM then
6:
ˆϵ ←∇θLB(fθ)
▷additional F1 and B1 to compute ˆϵ
7:
Compute g = ∇θLB(fθ+ˆϵ)
▷F2 and B2
8:
update the weights θ ←θ −ηg"
REFERENCES,0.624633431085044,"A.2
OPTIMIZING OVER SUBSET B+ IS REPRESENTATIVE"
REFERENCES,0.6275659824046921,"The sharpness-sensitive subset B+ is constructed by sorting ℓ(fθ+ˆϵ, xi, yi) −ℓ(fθ, xi, yi), which is
positively correlated to l(fθ, xi, yi). By a ﬁrst-order Taylor series approximation,"
REFERENCES,0.6304985337243402,"ℓ(fθ+ˆϵ, xi, yi) −ℓ(fθ, xi, yi) = ˆϵ · ∇θℓ(fθ, xi, yi) + o(∥ˆϵ∥)."
REFERENCES,0.6334310850439883,"By Equation 4, ˆϵ is the aggregated gradients of each instance in the complete dataset B, i.e.,"
REFERENCES,0.6363636363636364,"ˆϵ = arg max
ϵ:∥ϵ∥2<ρ
LS(fθ+ϵ) ≈ρ∇θLS(fθ) = |B|
X"
REFERENCES,0.6392961876832844,"i=1
∇θℓ(fθ, xi, yi),"
REFERENCES,0.6422287390029325,"which indicates that ℓ(fθ+ˆϵ, xi, yi) −ℓ(fθ, xi, yi) is positively correlated to the gradient
∇θℓ(fθ, xi, yi). Li et al. (2019) claims that the difﬁcult examples in deep learning (the training sam-
ples with high training loss) produce gradients with larger magnitudes. Therefore, ℓ(fθ+ˆϵ, xi, yi) −
ℓ(fθ, xi, yi) is positively correlated to l(fθ, xi, yi). We also demonstrate the correlation empirically."
REFERENCES,0.6451612903225806,"We conduct experiments to verify Equation 9 and Equation 10. In Figure 5, We plot the four losses,
LB+(fθ), LB−(fθ), LB+(fθ+ˆϵ), and LB−(fθ+ˆϵ) w.r.t the epochs. The experimental results verify
that Equation 9 and Equation 10 hold for every training epoch."
REFERENCES,0.6480938416422287,"Moreover, we conduct experiments to demonstrate that optimizing over the subset B+ is much more
representative than the subset B−. We compare the updating gradients computed from B+,B−and a
random subset Brand that |Brand| = |B+| = |B−| to those computed from B by calculating the cosine
similarity inspired by (Du et al., 2019), i.e."
REFERENCES,0.6510263929618768,"CosSim(∇θLB+(fθ+ˆϵ), ∇θLB(fθ+ˆϵ)),"
REFERENCES,0.6539589442815249,"CosSim(∇θLB−(fθ+ˆϵ), ∇θLB(fθ+ˆϵ)),"
REFERENCES,0.656891495601173,"CosSim(∇θLBrand(fθ+ˆϵ), ∇θLB(fθ+ˆϵ))."
REFERENCES,0.6598240469208211,"In Figure 6, we plot the cosine similarities in each training epoch evaluated with ResNet-18, Wide-
28-10 on CIFAR10. In terms of the computed gradients, the experimental results show that B+ has
the highest cosine similarities with B than B−and the random set Brand."
REFERENCES,0.6627565982404692,"A.3
LINEARITY MEASUREMENT OF SWP"
REFERENCES,0.6656891495601173,"As the experimental results in Figure 3 demonstrated, SWP can also improve the accuracy of ESAM
compared to SAM. We will investigate the advantage of SWP in terms of generalization in the"
REFERENCES,0.6686217008797654,Published as a conference paper at ICLR 2022
REFERENCES,0.6715542521994134,"0
25
50
75
100
125
150
175
200
Epochs 0.0 0.5 1.0 1.5 2.0 2.5 3.0"
REFERENCES,0.6744868035190615,Training Loss
REFERENCES,0.6774193548387096,Resnet18 on CIFAR10
REFERENCES,0.6803519061583577,"L+(fθ)
L−(fθ)
L+(fθ + ̂ε)
L−(fθ + ̂ε))"
REFERENCES,0.6832844574780058,"0
25
50
75
100
125
150
175
200
Epochs 0 1 2 3 4"
REFERENCES,0.6862170087976539,Training Loss
REFERENCES,0.6891495601173021,Resnet18 on CIFAR100
REFERENCES,0.6920821114369502,"L+(fθ)
L−(fθ)
L+(fθ + ̂ε)
L−(fθ + ̂ε))"
REFERENCES,0.6950146627565983,"0
25
50
75
100
125
150
175
200
Epochs 0.0 0.5 1.0 1.5 2.0"
REFERENCES,0.6979472140762464,Training Loss
REFERENCES,0.7008797653958945,Wide-28-10 on CIFAR10
REFERENCES,0.7038123167155426,"L+(fθ)
L−(fθ)
L+(fθ + ̂ε)
L−(fθ + ̂ε))"
REFERENCES,0.7067448680351907,"0
25
50
75
100
125
150
175
200
Epochs 0 1 2 3 4"
REFERENCES,0.7096774193548387,Training Loss
REFERENCES,0.7126099706744868,Wide-28-10 on CIFAR100
REFERENCES,0.7155425219941349,"L+(fθ)
L−(fθ)
L+(fθ + ̂ε)
L−(fθ + ̂ε))"
REFERENCES,0.718475073313783,"Figure 5: The SAM loss and the empirical loss calculated over the selected subsets B+, B−, w.r.t the epochs,
as evaluated with ResNet-18, Wide-28-10 on CIFAR10 and CIFAR100. The subset B+ selected by SDS has
much higher SAM loss and empirical loss than B−among all the four groups of experiments."
REFERENCES,0.7214076246334311,"0
25
50
75
100
125
150
175
200
Epochs −0.2 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7243401759530792,Cosine Similarit
REFERENCES,0.7272727272727273,Resnet18 on CIFAR10
REFERENCES,0.7302052785923754,"CosSim(∇θL+(fθ +
̂ε̂, ∇θL(fθ +
̂ε̂)"
REFERENCES,0.7331378299120235,"CosSim(∇θL−(fθ +
̂ε̂, ∇θL(fθ +
̂ε̂)"
REFERENCES,0.7360703812316716,"CosSim(∇θLrand(fθ +
̂ε̂, ∇θL(fθ +
̂ε̂)"
REFERENCES,0.7390029325513197,"0
25
50
75
100
125
150
175
200
Epochs −0.2 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7419354838709677,Cosine Similarit
REFERENCES,0.7448680351906158,Resnet18 on CIFAR100
REFERENCES,0.7478005865102639,"CosSim(∇θL+(fθ +
̂ε̂, ∇θL(fθ +
̂ε̂)"
REFERENCES,0.750733137829912,"CosSim(∇θL−(fθ +
̂ε̂, ∇θL(fθ +
̂ε̂)"
REFERENCES,0.7536656891495601,"CosSim(∇θLrand(fθ +
̂ε̂, ∇θL(fθ +
̂ε̂)"
REFERENCES,0.7565982404692082,"0
25
50
75
100
125
150
175
200
Epochs −0.2 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7595307917888563,Cosine Simila ity
REFERENCES,0.7624633431085044,Wide-28-10 on CIFAR10
REFERENCES,0.7653958944281525,"CosSim(∇θL+(fθ +
̂ε̂, ∇θL(fθ +
̂ε̂)"
REFERENCES,0.7683284457478006,"CosSim(∇θL−(fθ +
̂ε̂, ∇θL(fθ +
̂ε̂)"
REFERENCES,0.7712609970674487,"CosSim(∇θLrand(fθ +
̂ε̂, ∇θL(fθ +
̂ε̂)"
REFERENCES,0.7741935483870968,"0
25
50
75
100
125
150
175
200
Epochs −0.2 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7771260997067448,Cosine Simila ity
REFERENCES,0.7800586510263929,Wide-28-10 on CIFAR100
REFERENCES,0.782991202346041,"CosSim(∇θL+(fθ +
̂ε̂, ∇θL(fθ +
̂ε̂)"
REFERENCES,0.7859237536656891,"CosSim(∇θL−(fθ +
̂ε̂, ∇θL(fθ +
̂ε̂)"
REFERENCES,0.7888563049853372,"CosSim(∇θLrand(fθ +
̂ε̂, ∇θL(fθ +
̂ε̂)"
REFERENCES,0.7917888563049853,"Figure 6: The cosine similarity between the gradients computed from subsets B+, B−, and Brand with B,
as evaluated with ResNet-18, Wide-28-10 on CIFAR10 and CIFAR100. The gradients from the subset B+"
REFERENCES,0.7947214076246334,"selected by SDS has much higher cosine similarity with the gradients from B than the gradients from B−and
Brand among all the four groups of experiments."
REFERENCES,0.7976539589442815,Published as a conference paper at ICLR 2022
REFERENCES,0.8005865102639296,"future research. Here we provide a discussion about the accuracy improvement contributed by SWP.
A plausible reason for such improvement is that SWP leads to a better inner maximization solved
in equation 2. The current solution of ˆϵ is approximated by assuming LS(fθ) is a linear function.
Therefore, the ˆϵ would result in a better inner maximization if LS(fθ) is “more linear” with respect
to θ. Inspired by (Qin et al., 2019; Yan et al., 2019; 2021), we measure the linearity of the loss
function by
ζ(ϵ, B) = |LB(fθ+ϵ) −LB(fθ) −ϵ⊤∇θLB(fθ)|.
We conduct experiments on the CIFAR10 dataset with ResNet-18 model to verify that SWP can
improve the linearity ζ(ϵ, B) of the loss function LS(fθ). We compare the linearity of ESAM with
the β ranging from {0.2, 0.3, ..., 0.9} to the SAM. The results are demonstrated in Figure 7. It can"
REFERENCES,0.8035190615835777,"0
25
50
75
100
125
150
175
200
Epochs 0.016 0.018 0.020 0.022 0.024 0.026 0.028 0.030"
REFERENCES,0.8064516129032258,Linearity
REFERENCES,0.8093841642228738,Linearity Measure of SWP
REFERENCES,0.8123167155425219,"SAM
SWP,
= 0.9"
REFERENCES,0.8152492668621701,"SWP,
= 0.8
SWP,
= 0.7"
REFERENCES,0.8181818181818182,"SWP,
= 0.6
SWP,
= 0.5
SWP,
= 0.4
SWP,
= 0.3"
REFERENCES,0.8211143695014663,"SWP,
= 0.2"
REFERENCES,0.8240469208211144,"Figure 7: The linearity measurement of SWP evaluated with the ResNet18 model on the CIFAR10 dataset
compared with SAM. The experimental results indicate that a smaller β can result in better linearity."
REFERENCES,0.8269794721407625,"be shown that SWP will result in a better linearity as the β decreases. However, decreasing β will
also reduce the magnitude of ˆϵ and thus result in a worse inner maximization in equation 2. We
observe that β = {0.5, 0.6} is optimal to balance the accuracy and efﬁciency of ESAM."
REFERENCES,0.8299120234604106,"A.4
REDUCED COMPUTATIONAL OVERHEAD CONTRIBUTED BY SWP"
REFERENCES,0.8328445747800587,"Formulation We formulate the saved computational overhead contributed by SWP here. We dis-
cuss and examine the computational overhead in the PyTorch framework. Suppose that the NN we
discussed here has N layers. The most unit of parameters (D × C × H × W) is the entire param-
eters of a layer in NN, where D is the number of kernels, C is the number of channels, H and W
are the height and width of the input. SWP select each basic parameter unit to compute gradients
(i.e. requries grad = True) with probability β. We use g(N, β) to measure the saved computational
overhead in terms of percentage contributed by SWP compared to the vanilla SAM."
REFERENCES,0.8357771260997068,"The computational overhead g(N, β) is reduced not just by calculating gradients, but also by the
storing and hooking gradients. Because the storing and hooking operations of each parameter’s
gradients are independent to each other, the computational overhead saved from storing and hooking
are proportional to 1 −β and irrelevant to the depth N of NN. In addition, the storing and hooking
gradients are the dominant factor that result in the reduced computational overhead according to our
toy example in the following."
REFERENCES,0.8387096774193549,"Next we discuss the saved computational overhead that stems from the calculation operations in the
general case. Some layers in the DNNs with complicated architectures such as DenseNet and ViT,
may have multiple basic parameter units in the same layer and be connected across any other layers.
Suppose the nth layer has K(n) basic parameter units, let pn be the calculation-free rate of a certain
parameter unit in nth layer, we have pn = (1−β)K(n) ·pn−1. Therefore, pn = (1−β)
Pn
j=1 K(j) ≈"
REFERENCES,0.841642228739003,Published as a conference paper at ICLR 2022
REFERENCES,0.844574780058651,"(1 −β)n ¯
K, where ¯K = 1"
REFERENCES,0.8475073313782991,"N
PN
j=1 K(j). Assumed that the computations of each layer are the same,
by summing up the saved computational overhead of all parameters, we have"
REFERENCES,0.8504398826979472,"g(N, β) = k1(1 −β) + k2 N
X n=i pn N"
REFERENCES,0.8533724340175953,"≈k1(1 −β) +
k2(1 −β)
N[1 −(1 −β) ¯
K]"
REFERENCES,0.8563049853372434,"≈k1(1 −β) + k′
2(1 −β)"
REFERENCES,0.8592375366568915,"Nβ
.
(11)"
REFERENCES,0.8621700879765396,"where k1, k2 are determined by the computing time of calculation, k′
2 =
k2β
[1−(1−β) ¯
K]."
REFERENCES,0.8651026392961877,"However, the commonly used DNNs’ architectures such as ResNet only have one basic parameter
unit in each layer. Besides, each layer of them is connected in serial with the next layer. Then, we
have pn = (1 −β) · pn−1. Therefore, pn = (1 −β)n. The saved calculation contributed by the
parameter unit is
1
N pn. By summing up the saved computational overhead of all parameters, we
have"
REFERENCES,0.8680351906158358,"g(N, β) = k1(1 −β) + k2 N
X n=i pn N"
REFERENCES,0.8709677419354839,= k1(1 −β) + k2
REFERENCES,0.873900293255132,"N
1 −β −(1 −β)N β"
REFERENCES,0.8768328445747801,≈k1(1 −β) + k2(1 −β)
REFERENCES,0.8797653958944281,"Nβ
.
(12)"
REFERENCES,0.8826979472140762,"where k1, k2 are determined by the computing time of calculation, storing and hooking gradients."
REFERENCES,0.8856304985337243,"Toy Example We conducted a toy example on the CIFAR10 dataset with two MLPs. Each fully
connected layer of MLP is the same with a size of 3, 000 for both in and out features. The ﬁrst mlp is
for the special case that ¯K = 1, where each layer has only one basic parameter unit and is connected
in serial with the next layer. We examined N = {50, 75, 100, 125} and β = {0.1, ..., 0.9, 1.0} to
record the saved computational overhead in percentage. Part of the results are reported in Table A.4.
By linear regression, we have k1 = 0.3185, k2 = 0.1310, and the returned R2 = 0.9983. The
second mlp is for the general case that ¯K > 1, where each layer may have multiple basic parameter
units in the same layer and be connected across any other layers. We examined N = {35, 50, 65, 75}
and β = {0.1, ..., 0.9, 1.0} to record the saved computational overhead in percentage. Part of the
results are reported in Table A.4. By linear regression, we have k1 = 0.3143, k2 = 0.0737, and
the returned R2 = 0.9989. The above experimental results verify the formulation of the reduced
computational overhead contributed by SWP in equation 11 and equation 12."
REFERENCES,0.8885630498533724,"N
β
g(N, β)
50
0.9
3.42%
50
0.5
16.28%
50
0.1
30.08%
75
0.9
3.44%
75
0.5
15.79%
75
0.1
30.42%
100
0.9
3.37%
100
0.5
15.86%
100
0.1
29.56%
125
0.9
2.94%
125
0.5
14.87%
125
0.1
29.42%"
REFERENCES,0.8914956011730205,"Table 4: The special case that ¯K = 1. By linear
regression, k1 = 0.3185, k2 = 0.1310, and the
returned R2 = 0.9983."
REFERENCES,0.8944281524926686,"N
β
g(N, β)
25
0.9
2.88%
25
0.5
15.54%
25
0.1
30.03%
50
0.9
2.34%
50
0.5
15.10%
50
0.1
29.83%
65
0.9
2.23%
65
0.5
14.95%
65
0.1
29.22%
75
0.9
2.22%
75
0.5
15.10%
75
0.1
28.68%"
REFERENCES,0.8973607038123167,"Table 5: The general case that ¯K > 1. By linear
regression, k1 = 0.3143, k
′
2 = 0.0737, and the
returned R2 = 0.9989."
REFERENCES,0.9002932551319648,Published as a conference paper at ICLR 2022
REFERENCES,0.9032258064516129,"A.5
VISUALIZATION OF LOSS LANDSCAPES WITH RESPECT TO ADVERSARIAL WEIGHT
PERTURBATIONS"
REFERENCES,0.906158357771261,"We visualize the sharpness of the ﬂat minima with respect to adversarial weight perturbations of
SGD,SAM and ESAM on the Cifar10 dataset. The x- and y-axes represent two orthogonal ad-
versarial weight perturbations, which are η∇θLBx(fθ) and η∇θLBy(fθ) respectively, where η is
the learning rate during training. Bx and By are the randomly sampled subsets of batch B, and
|Bx| = |By| = 1"
REFERENCES,0.9090909090909091,"2|B|, Bx ∪By = B. We display the loss landscape in Figure 8, which demonstrates
that both SAM and ESAM improve the sharpness signiﬁcantly in comparison to SGD."
REFERENCES,0.9120234604105572,"1.0
0.5
0.0
0.5
1.0
1.0
0.5
0.0
0.5
1.0"
REFERENCES,0.9149560117302052,Training Loss 0.00 0.02 0.04 0.06 0.08 0.10 SGD
REFERENCES,0.9178885630498533,"1.0
0.5
0.0
0.5
1.0
1.0
0.5
0.0
0.5
1.0"
REFERENCES,0.9208211143695014,Training Loss 0.00 0.02 0.04 0.06 0.08 0.10 SAM
REFERENCES,0.9237536656891495,"1.0
0.5
0.0
0.5
1.0
1.0
0.5
0.0
0.5
1.0"
REFERENCES,0.9266862170087976,Training Loss 0.00 0.02 0.04 0.06 0.08 0.10 ESAM 0.02 0.04 0.06 0.08 0.10
REFERENCES,0.9296187683284457,"Figure 8: Cross-entropy loss landscapes of the ResNet18 model respect to adversarial weight perturbations on
the CIFAR10 dataset trained with SGD, SAM, and ESAM."
REFERENCES,0.9325513196480938,"A.6
EVALUATION OF ESAM ON VIT-S/16"
REFERENCES,0.9354838709677419,"SAM has also been demonstrated to be effective on the new vision Transformer(ViT) architecture
(Chen et al., 2021). Therefore, we also evaluate ESAM with ViT-S/16 on ImageNet Datasets. We
use β = 0.5 and γ = 0.7 for ViT-S/16, which share the same hyperparameters as ResNet-50 and
ResNet-101 in section 3.1. The results are reported in Table 6, which indicate that ESAM can still be
effective to improve efﬁciency in ViT-S/16 architecture. In particular, ESAM-SWP achieves much
better accuracy than SAM (80.88% v.s. 80.34%)."
REFERENCES,0.9384164222873901,Table 6: Classiﬁcation accuracies and training speed of ViT-S/16 on the ImageNet dataset.
REFERENCES,0.9413489736070382,ViT-S/16
REFERENCES,0.9442815249266863,"ImageNet
Accuracy
images/s"
REFERENCES,0.9472140762463344,"SGD
79.72
1,133
SAM
80.34
581
ESAM-SWP
80.88
616
ESAM-SDS
79.97
693
ESAM
80.46
734"
REFERENCES,0.9501466275659824,"A.7
TRAINING DETAILS"
REFERENCES,0.9530791788856305,"We tune the training parameters of SGD, SAM, and ESAM, by using grid searches. The learning rate
is chosen from the set {0.01, 0.05, 0.1, 0.2}, the weight decay from the set {5 × 10−4, 1 × 10−3},
and the batch size from the set {64, 128, 256}. This is done to attain the best accuracies. The
exact training hyperparameters are reported in Table 7. On the ImageNet datasets, limited by the
computing resource, we follow and slightly modify the optimal hyperparameters as suggested by
Chen et al. (2021) for SGD, SAM and ESAM. The exact training hyperparameters are reported in
Table 8."
REFERENCES,0.9560117302052786,Published as a conference paper at ICLR 2022
REFERENCES,0.9589442815249267,Table 7: Hyperparameters for training from scratch on CIFAR10 and CIFAR100
REFERENCES,0.9618768328445748,"CIFAR-10
CIFAR-100
ResNet-18
SGD
SAM
ESAM
SGD
SAM
ESAM"
REFERENCES,0.9648093841642229,"Epoch
200
200
Batch size
128
128
Data augmentation
Basic
Basic
Peak learning rate
0.05
0.05
Learning rate decay
Cosine
Cosine
Weight decay
5 × 10−4
1 × 10−3
1 × 10−3
5 × 10−4
1 × 10−3
1 × 10−3"
REFERENCES,0.967741935483871,"ρ
-
0.05
0.05
-
0.05
0.05"
REFERENCES,0.9706744868035191,"Wide-28-10
SGD
SAM
ESAM
SGD
SAM
ESAM"
REFERENCES,0.9736070381231672,"Epoch
200
200
Batch size
256
256
Data augmentation
Basic
Basic
Peak learning rate
0.05
0.05
Learning rate decay
Cosine
Cosine
Weight decay
5 × 10−4
1 × 10−3
1 × 10−3
5 × 10−4
1 × 10−3
1 × 10−3"
REFERENCES,0.9765395894428153,"ρ
-
0.1
0.1
-
0.1
0.1"
REFERENCES,0.9794721407624634,"PyramidNet-110
SGD
SAM
ESAM
SGD
SAM
ESAM"
REFERENCES,0.9824046920821115,"Epoch
300
300
Batch size
256
256
Data augmentation
Basic
Basic
Peak learning rate
0.1
0.1
Learning rate decay
Cosine
Cosine
Weight decay
5 × 10−4
5 × 10−4"
REFERENCES,0.9853372434017595,"ρ
-
0.2
0.2
-
0.2
0.2"
REFERENCES,0.9882697947214076,Table 8: Hyperparameters for training from scratch on ImageNet
REFERENCES,0.9912023460410557,"ResNet-50
ResNet-110
ImageNet
SGD
SAM
ESAM
SGD
SAM
ESAM"
REFERENCES,0.9941348973607038,"Epoch
90
90
Batch size
512
512
Data augmentation
Inception-style
Inception-style
Peak learning rate
0.2
0.2
Learning rate decay
Cosine
Cosine
Weight decay
1 × 10−4
1 × 10−4"
REFERENCES,0.9970674486803519,"ρ
-
0.05
0.05
-
0.05
0.05
Input resolution
224 × 224
224 × 224"
