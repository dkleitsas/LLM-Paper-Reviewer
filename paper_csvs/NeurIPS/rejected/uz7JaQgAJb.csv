Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001763668430335097,"McKean-Vlasov stochastic differential equations (MV-SDEs) provide a mathe-
1"
ABSTRACT,0.003527336860670194,"matical description of the behavior of an inﬁnite number of interacting particles
2"
ABSTRACT,0.005291005291005291,"by imposing a dependence on the particle density. These processes differ from
3"
ABSTRACT,0.007054673721340388,"standard Itô-SDEs to the extent that MV-SDEs include distributional information
4"
ABSTRACT,0.008818342151675485,"in their individual particle parameterization. As such, we study the inﬂuence of
5"
ABSTRACT,0.010582010582010581,"explicitly including distributional information in the parameterization of the SDE.
6"
ABSTRACT,0.012345679012345678,"We ﬁrst propose a series of semi-parametric methods for representing MV-SDEs,
7"
ABSTRACT,0.014109347442680775,"and then propose corresponding estimators for inferring parameters from data
8"
ABSTRACT,0.015873015873015872,"based on the underlying properties of the MV-SDE. By analyzing the properties of
9"
ABSTRACT,0.01763668430335097,"the different architectures and estimators, we consider their relationship to standard
10"
ABSTRACT,0.019400352733686066,"Itô-SDEs and consider their applicability in relevant machine learning problems.
11"
ABSTRACT,0.021164021164021163,"We empirically compare the performance of the different architectures on a series
12"
ABSTRACT,0.02292768959435626,"of real and synthetic datasets for time series and probabilistic modeling. The results
13"
ABSTRACT,0.024691358024691357,"suggest that including the distributional dependence in MV-SDEs is an effective
14"
ABSTRACT,0.026455026455026454,"modeling framework for temporal data under an exchangeability assumption while
15"
ABSTRACT,0.02821869488536155,"maintaining strong performance for standard Itô-SDE problems due to the richer
16"
ABSTRACT,0.029982363315696647,"class of probability ﬂows associated with MV-SDEs.
17"
INTRODUCTION,0.031746031746031744,"1
Introduction
18"
INTRODUCTION,0.03350970017636684,"(a) Itô-SDE
(b) MV-SDE"
INTRODUCTION,0.03527336860670194,"Figure 1: SDE sample paths of a double-well po-
tential, where the particles (a) do not interact and
(b) exhibit complex phase transitions as a result
only of interaction via weak attraction."
INTRODUCTION,0.037037037037037035,"Stochastic differential equations (SDEs) model
19"
INTRODUCTION,0.03880070546737213,"the evolution of a stochastic process through two
20"
INTRODUCTION,0.04056437389770723,"functions known as the drift and diffusion func-
21"
INTRODUCTION,0.042328042328042326,"tions. Beginning with Itô-SDEs, where individ-
22"
INTRODUCTION,0.04409171075837742,"ual sample paths are assumed to be independent,
23"
INTRODUCTION,0.04585537918871252,"neural representations of the drift and diffusion
24"
INTRODUCTION,0.047619047619047616,"have achieved high performance in many appli-
25"
INTRODUCTION,0.04938271604938271,"cations, such as time series and generative mod-
26"
INTRODUCTION,0.05114638447971781,"eling [Song et al., 2020, Tashiro et al., 2021].
27"
INTRODUCTION,0.05291005291005291,"On the other hand, interacting particle systems
28"
INTRODUCTION,0.054673721340388004,"are also used to model stochastic processes using
29"
INTRODUCTION,0.0564373897707231,"many of the same characteristics as an Itô-SDE,
30"
INTRODUCTION,0.0582010582010582,"but they additionally dictate an interaction between the different sample paths [Liggett, 1997]. When
31"
INTRODUCTION,0.059964726631393295,"the number of particles approaches inﬁnity, these processes generalize Itô-SDEs to nonlinear SDEs
32"
INTRODUCTION,0.06172839506172839,"known as McKean-Vlasov SDEs (MV-SDEs). The nonlinearity arises from the individual particle
33"
INTRODUCTION,0.06349206349206349,"dependence on the whole particle density, often in the form of a mean-ﬁeld term represented by an
34"
INTRODUCTION,0.06525573192239859,"expectation with respect to the particle density. This distributional dependence allows for greater
35"
INTRODUCTION,0.06701940035273368,"ﬂexibility in the time marginal distributions that the MV-SDE can represent versus the Itô-SDE. An
36"
INTRODUCTION,0.06878306878306878,"example of the differences between the two frameworks is illustrated in Figure 1 where Figure 1a
37"
INTRODUCTION,0.07054673721340388,"depicts an Itô-SDE where the sample paths are independent and Figure 1b depicts a MV-SDE where
38"
INTRODUCTION,0.07231040564373897,"the sample paths interact through distributional dependence. While these models appear in a variety
39"
INTRODUCTION,0.07407407407407407,"of disciplines such as in ﬁnance [Feinstein and Søjmark, 2021], biology [Keller and Segel, 1971], and
40"
INTRODUCTION,0.07583774250440917,"social sciences [Carrillo et al., 2020], relatively few works have considered the problem of estimating
41"
INTRODUCTION,0.07760141093474426,"parameters from observations or their application in machine learning tasks.
42"
INTRODUCTION,0.07936507936507936,"This brings us to a motivating question:
43"
INTRODUCTION,0.08112874779541446,"(Q1) Can we develop theoretically justiﬁed neural architectures to represent MV-SDEs?
44"
INTRODUCTION,0.08289241622574955,"To answer (Q1), we use the relationship between the approximation capabilities of neural networks
45"
INTRODUCTION,0.08465608465608465,"and properties of MV-SDEs. We consider two ideas: (i) expressing a layer in a neural network as an
46"
INTRODUCTION,0.08641975308641975,"expectation with respect to a density and (ii) using generative models to capture distributions and
47"
INTRODUCTION,0.08818342151675485,"generate samples.
48"
INTRODUCTION,0.08994708994708994,"Our second question relates the theoretical generality of MV-SDEs to Itô-SDEs:
49"
INTRODUCTION,0.09171075837742504,"(Q2) Does including explicit distributional dependence empirically affect modeling capabilities?
50"
INTRODUCTION,0.09347442680776014,"We discuss a few theoretical properties that motivate this question and answer the question empirically
51"
INTRODUCTION,0.09523809523809523,"by comparing different architectures for applications in time series and in probabilistic modeling.
52"
RELATED WORK,0.09700176366843033,"1.1
Related work
53"
RELATED WORK,0.09876543209876543,"Methods that estimate MV-SDEs from observations often assume known interaction kernels and
54"
RELATED WORK,0.10052910052910052,"drift parameters. They then rely on a large number of samples at regularly spaced time intervals
55"
RELATED WORK,0.10229276895943562,"to empirically approximate the expectation in the mean-ﬁeld term [Messenger and Bortz, 2022,
56"
RELATED WORK,0.10405643738977072,"Della Maestra and Hoffmann, 2022, Yao et al., 2022, Della Maestra and Hoffmann, 2023]. In Pavliotis
57"
RELATED WORK,0.10582010582010581,"and Zanoni [2022], the authors describe a method of moments estimator for the parameters of the
58"
RELATED WORK,0.10758377425044091,"MV-SDE. Other approaches concerned analyzing the partial differential equation (PDE) associated
59"
RELATED WORK,0.10934744268077601,"with MV-SDEs as in Gomes et al. [2019]. In our work, we are primarily concerned with inference
60"
RELATED WORK,0.1111111111111111,"in regions where we have limited time-marginal data and the number of samples is not large. Other
61"
RELATED WORK,0.1128747795414462,"applications of MV-SDEs in machine learning topics include estimating optimal trajectories in scRNA-
62"
RELATED WORK,0.1146384479717813,"Seq data [Chizat et al., 2022] and stochastic control problems relating to mean-ﬁeld games [Han et al.,
63"
RELATED WORK,0.1164021164021164,"2022]. Ruthotto et al. [2020] considered a machine learning approach for solving certain kinds of
64"
RELATED WORK,0.11816578483245149,"mean ﬁeld games and mean ﬁeld control problems. Inverse problems can also be solved by deriving
65"
RELATED WORK,0.11992945326278659,"an appropriate MV-SDE as the authors describe in Crucinio et al. [2022]. Extensive analysis of the
66"
RELATED WORK,0.12169312169312169,"dynamics of the parameters of a neural network under stochastic gradient descent has been conducted
67"
RELATED WORK,0.12345679012345678,"using the theory from MV-SDEs, e.g. [Hu et al., 2021]. These methods use a pre-described form of
68"
RELATED WORK,0.12522045855379188,"the drift to conduct their analyses whereas we’re interested in learning a representation of the drift.
69"
RELATED WORK,0.12698412698412698,"Our Contributions To address the lack of non-parametric MV-SDE estimators in the existing litera-
70"
RELATED WORK,0.12874779541446207,"ture, this paper contributes the following: First, we present two neural architectures for representing
71"
RELATED WORK,0.13051146384479717,"MV-SDEs based on learned measures and generative networks; then, we present three estimators,
72"
RELATED WORK,0.13227513227513227,"based on maximum likelihood, used in conjunction with the architectures without prior knowledge
73"
RELATED WORK,0.13403880070546736,"on the structure of the drift; next, we characterize the properties of implicit regularization and richer
74"
RELATED WORK,0.13580246913580246,"probability ﬂows of these architectures; ﬁnally, we empirically demonstrate the applicability of the
75"
RELATED WORK,0.13756613756613756,"architectures on time series and generative modeling.
76"
PROPERTIES OF MV-SDES,0.13932980599647266,"2
Properties of MV-SDEs
77"
PROPERTIES OF MV-SDES,0.14109347442680775,"We begin by describing the background and properties of the transition densities of MV-SDEs.
78"
PROPERTIES OF MV-SDES,0.14285714285714285,"Figure 2 illustrates some of these concepts qualitatively where we ﬁrst consider non-local dynamics
79"
PROPERTIES OF MV-SDES,0.14462081128747795,"and then consider jumps in the sample paths.
80"
BACKGROUND,0.14638447971781304,"2.1
Background
81"
BACKGROUND,0.14814814814814814,"Consider a domain D ⇢Rd and let Pk(D) be the space of all probability distributions supported on
82"
BACKGROUND,0.14991181657848324,"D with ﬁnite kth moment. Let Wt 2 Rd be a d-dimensional Wiener process and let Xt 2 Rd be a
83"
BACKGROUND,0.15167548500881833,"solution to the following MV-SDE
84"
BACKGROUND,0.15343915343915343,"dXt = b(Xt, pt, t)dt + p"
BACKGROUND,0.15520282186948853,"⌃(Xt, pt, t)dWt
(1)"
BACKGROUND,0.15696649029982362,where pt denotes the law of Xt at time t and p
BACKGROUND,0.15873015873015872,"⌃denotes the Cholesky decomposition of ⌃. We
85"
BACKGROUND,0.16049382716049382,"assume that the drift vector b : Rd ⇥P2(D) ⇥R+ ! Rd and the diffusion matrix ⌃: Rd ⇥P2(D) ⇥
86"
BACKGROUND,0.16225749559082892,"R+ ! SPD(Rd⇥d) are globally Lipshitz for the existence and uniqueness of the solution, with SPD
87"
BACKGROUND,0.164021164021164,"denoting the space of symmetric, positive deﬁnite matrices.
88"
BACKGROUND,0.1657848324514991,"We focus on the case where the diffusion coefﬁcient is a known constant, σ, and focus on estimating
89"
BACKGROUND,0.1675485008818342,"the drift, b, from data. In addition, for simplicity in analysis, we suppose that b factors linearly
90"
BACKGROUND,0.1693121693121693,"into a non-interacting component, and an interacting component, where the mean-ﬁeld term with
91"
BACKGROUND,0.1710758377425044,"dependence on pt is often written in terms of an expectation, speciﬁcally
92"
BACKGROUND,0.1728395061728395,"dXt = f(Xt, t)dt + Eyt⇠pt [' (Xt, yt)]dt + σdWt
(2)"
BACKGROUND,0.1746031746031746,"where f : Rd ⇥R+ ! Rd can be seen as the Itô drift, the expectation as the mean-ﬁeld drift, and
93"
BACKGROUND,0.1763668430335097,"' : Rd ⇥Rd ! Rk as the interaction function describing the interaction between particles, e.g.
94"
BACKGROUND,0.1781305114638448,"attraction with '(x, y) = −(x −y) in Figure 1b and the left side of Figure 2. We also assume that
95"
BACKGROUND,0.17989417989417988,"all coefﬁcients exhibit sufﬁcient regularity such that the empirical law converges to the true law of
96"
BACKGROUND,0.18165784832451498,"the system (i.e.
1
N PN"
BACKGROUND,0.18342151675485008,i=1 δX(i)
BACKGROUND,0.18518518518518517,"t
!N!1 pt(Xt)), i.e. propagation of chaos holds [Méléard, 1996].
97"
BACKGROUND,0.18694885361552027,"As mentioned, unlike Itô-SDEs which only consider dependence on Xt and t, MV-SDEs also depend
98"
BACKGROUND,0.18871252204585537,"on the marginal time distribution pt. By introducing a dependence on the marginal law, the transition
99"
BACKGROUND,0.19047619047619047,"density of the process satisﬁes a richer class of functions.
100"
NON-LOCALITY OF THE TRANSITION DENSITY,0.19223985890652556,"2.2
Non-locality of the transition density
101"
NON-LOCALITY OF THE TRANSITION DENSITY,0.19400352733686066,"Following the background, we describe a favorable property of the MV-SDE that induces non-local
102"
NON-LOCALITY OF THE TRANSITION DENSITY,0.19576719576719576,"dependencies in the state space. The transition density of (2) can be written as the non-linear PDE
103"
NON-LOCALITY OF THE TRANSITION DENSITY,0.19753086419753085,@tpt(x) = −r · 0
NON-LOCALITY OF THE TRANSITION DENSITY,0.19929453262786595,"B
B
@ ptf(x)"
NON-LOCALITY OF THE TRANSITION DENSITY,0.20105820105820105,| {z }
NON-LOCALITY OF THE TRANSITION DENSITY,0.20282186948853614,Itô Drift + pt Z
NON-LOCALITY OF THE TRANSITION DENSITY,0.20458553791887124,"'(x −yt)pt(yt)dyt
|
{z
}
Non-Local Interactions −σ2"
RPT,0.20634920634920634,"2 rpt
| {z }"
RPT,0.20811287477954143,Diffusion 1
RPT,0.20987654320987653,"C
C
A .
(3)"
RPT,0.21164021164021163,"This non-local behavior has a variety of implications. For example, the distribution of particles “far
104"
RPT,0.21340388007054673,"away” from a reference particle can affect the behavior of the reference particle. This property is
105"
RPT,0.21516754850088182,"illustrated in the left side of Figure 2 with an example from the mean-ﬁeld FitzHugh-Nagumo model
106"
RPT,0.21693121693121692,"used to model spikes in neuron activation, leading to interactions between distinct spikes [Crevat
107"
RPT,0.21869488536155202,"et al., 2019]. Notably, this is not possible when considering only the Itô drift, since that operator acts
108"
RPT,0.2204585537918871,"locally on the density.
109"
DISCONTINUOUS SAMPLE PATHS,0.2222222222222222,"2.3
Discontinuous sample paths
110"
DISCONTINUOUS SAMPLE PATHS,0.2239858906525573,"Figure 2: MV-SDE sample paths with non-local
dynamics (left) and discontinuities (right)."
DISCONTINUOUS SAMPLE PATHS,0.2257495590828924,"The richer class of densities modeled by MV-
111"
DISCONTINUOUS SAMPLE PATHS,0.2275132275132275,"SDEs has direct inﬂuence on individual sample
112"
DISCONTINUOUS SAMPLE PATHS,0.2292768959435626,"paths. In a modeling scenario, we may wish to
113"
DISCONTINUOUS SAMPLE PATHS,0.2310405643738977,"approximate a process that exhibits jumps. For
114"
DISCONTINUOUS SAMPLE PATHS,0.2328042328042328,"example, in ﬁnance, a number of related entities
115"
DISCONTINUOUS SAMPLE PATHS,0.2345679012345679,"may have common exposure and experience fail-
116"
DISCONTINUOUS SAMPLE PATHS,0.23633156966490299,"ure simultaneously [Nadtochiy and Shkolnikov,
117"
DISCONTINUOUS SAMPLE PATHS,0.23809523809523808,"2019, Feinstein and Søjmark, 2021]. Similarly,
118"
DISCONTINUOUS SAMPLE PATHS,0.23985890652557318,"in neuroscience, a number of neurons spiking
119"
DISCONTINUOUS SAMPLE PATHS,0.24162257495590828,"simultaneously results in discontinuities in the
120"
DISCONTINUOUS SAMPLE PATHS,0.24338624338624337,"sample paths [Carrillo et al., 2013]. The fact
121"
DISCONTINUOUS SAMPLE PATHS,0.24514991181657847,"that the interaction of many particles can cause blowups leads to a remarkable property of MV-SDEs
122"
DISCONTINUOUS SAMPLE PATHS,0.24691358024691357,"that allows discontinuous paths. The major beneﬁt of this property is that we do not need to consider
123"
DISCONTINUOUS SAMPLE PATHS,0.24867724867724866,"an additional jump noise process – we only need to specify a particular interaction between the
124"
DISCONTINUOUS SAMPLE PATHS,0.25044091710758376,"particles to induce the jump behavior. A simple proof for the case of positive feedback is given
125"
DISCONTINUOUS SAMPLE PATHS,0.25220458553791886,"in Hambly et al. [2019, Theorem 1.1].
126"
DISCONTINUOUS SAMPLE PATHS,0.25396825396825395,"Having described the theoretical advantages of MV-SDEs as compared to Itô-SDEs, we will proceed
127"
DISCONTINUOUS SAMPLE PATHS,0.25573192239858905,"to discuss the neural architectures for representing these processes.
128"
DISCONTINUOUS SAMPLE PATHS,0.25749559082892415,Implicit Measure (IM)
NW,0.25925925925925924,"1
nw Pnw"
NW,0.26102292768959434,"i=1 '( · , W (i)"
NW,0.26278659611992944,0 ) dPt dP0
NW,0.26455026455026454,Empirical Measure (EM)
N,0.26631393298059963,"1
N PN"
N,0.26807760141093473,"i=1 '( · , X(i) t )
n X(i) t oN"
N,0.2698412698412698,i=1 ⇠Pt
N,0.2716049382716049,Marginal Law (ML)
NP,0.27336860670194,"1
np Pnp"
NP,0.2751322751322751,"i=1 '( · , ˆX(i) t )
n ˆX(i) t onp"
NP,0.2768959435626102,i=1 ⇠ˆPt
NP,0.2786596119929453,"Explicit pt
Implicit pt"
NP,0.2804232804232804,"Figure 3: Schematic comparing neural architectures for modeling MV-SDEs. Implicit measure (IM)
architecture uses a mean-ﬁeld layer that represents particles as learned weights; the empirical measure
(EM) architecture computes the expectation with the observed particles; the marginal law (ML)
estimates the particle density, computing the expectation with samples from the estimated density."
NP,0.2821869488536155,"3
Mean-ﬁeld Architectures
129"
NP,0.2839506172839506,"We now describe methods for representing the mean-ﬁeld drift of a MV-SDE in (2). We ﬁrst consider
130"
NP,0.2857142857142857,"a modiﬁcation of the cylindrical architecture [Pham and Warin, 2022] that empirically computes
131"
NP,0.2874779541446208,"the expectation using observations, and denote it as the empirical measure (EM) architecture. We
132"
NP,0.2892416225749559,"then propose two architectures – an architecture based on representing a learned measure with neural
133"
NP,0.291005291005291,"weights, denoted as the implicit measure (IM) architecture, and a generative architecture based on
134"
NP,0.2927689594356261,"representing the marginal law of the samples (ML). Figure 3 provides a schematic of the different
135"
NP,0.2945326278659612,"architectures and mean-ﬁeld representations. We denote a function f parameterized by parameters ✓
136"
NP,0.2962962962962963,"as f(·; ✓).
137"
EMPIRICAL MEASURE ARCHITECTURE,0.2980599647266314,"3.1
Empirical measure architecture
138"
EMPIRICAL MEASURE ARCHITECTURE,0.2998236331569665,Suppose we observe N particles at each time t given by {X(i) t }N
EMPIRICAL MEASURE ARCHITECTURE,0.30158730158730157,"i=1 and denote the discrete measure
139"
EMPIRICAL MEASURE ARCHITECTURE,0.30335097001763667,associated with these observations as pδ t = 1 N PN
EMPIRICAL MEASURE ARCHITECTURE,0.30511463844797176,i=1 δX(i)
EMPIRICAL MEASURE ARCHITECTURE,0.30687830687830686,"t . Then, we can use pδ"
EMPIRICAL MEASURE ARCHITECTURE,0.30864197530864196,"t to approximate the
140"
EMPIRICAL MEASURE ARCHITECTURE,0.31040564373897706,"expectation in (2) as
141"
EMPIRICAL MEASURE ARCHITECTURE,0.31216931216931215,"Eyt⇠pt [' (Xt, yt)] ⇡Eyt⇠pδ"
EMPIRICAL MEASURE ARCHITECTURE,0.31393298059964725,"t [' (Xt, yt; ✓)] = 1 N N
X i=1 ' ⇣"
EMPIRICAL MEASURE ARCHITECTURE,0.31569664902998235,"Xt, X(i) t ; ✓ ⌘ (4)"
EMPIRICAL MEASURE ARCHITECTURE,0.31746031746031744,"for a neural network '(·, ·; ✓) describing the interaction function between the particles [Pham and
142"
EMPIRICAL MEASURE ARCHITECTURE,0.31922398589065254,"Warin, 2022]. Suppose the non-mean ﬁeld component f is also represented with a neural network
143"
EMPIRICAL MEASURE ARCHITECTURE,0.32098765432098764,"f(·, t; ✓). Assuming that ' and f are well learned, this architecture can represent the true MV-SDE
144"
EMPIRICAL MEASURE ARCHITECTURE,0.32275132275132273,"drift in the limit as the number of observations N ! 1. We refer to this architecture as the
145"
EMPIRICAL MEASURE ARCHITECTURE,0.32451499118165783,"empirical measure (EM) architecture since at each time step the expectation is taken with respect to
146"
EMPIRICAL MEASURE ARCHITECTURE,0.3262786596119929,"the empirical measure derived from the observations.
147"
IMPLICIT MEASURE ARCHITECTURE,0.328042328042328,"3.2
Implicit measure architecture
148"
IMPLICIT MEASURE ARCHITECTURE,0.3298059964726631,"While the EM architecture in (4) explicitly deﬁnes the relationship between the law pt and the
149"
IMPLICIT MEASURE ARCHITECTURE,0.3315696649029982,"interaction ', it relies on obtaining the empirical measure at each time point. This may be difﬁcult in
150"
IMPLICIT MEASURE ARCHITECTURE,0.3333333333333333,"practice for a variety of reasons such as having few samples or the lack of data at some time points.
151"
IMPLICIT MEASURE ARCHITECTURE,0.3350970017636684,"Instead, let us ﬁrst recall that a single layer in a multilayer perceptron (MLP) can be written in terms
152"
IMPLICIT MEASURE ARCHITECTURE,0.3368606701940035,"of an expectation as
153"
IMPLICIT MEASURE ARCHITECTURE,0.3386243386243386,"MLPW,b(x) = Z"
IMPLICIT MEASURE ARCHITECTURE,0.3403880070546737,"σ (Wx + b) d⌫(W, b)
(5)"
IMPLICIT MEASURE ARCHITECTURE,0.3421516754850088,"where the expectation is taken over ⌫(·), a measure over the space of parameters y = (W, b), and σ
154"
IMPLICIT MEASURE ARCHITECTURE,0.3439153439153439,"is an activation function.
155"
IMPLICIT MEASURE ARCHITECTURE,0.345679012345679,"When ⌫=
1
N PN"
IMPLICIT MEASURE ARCHITECTURE,0.3474426807760141,"i=1 δy(i), a discrete measure with N particles, the expectation is exactly a single
156"
IMPLICIT MEASURE ARCHITECTURE,0.3492063492063492,"layer of width N, suggesting a correspondence between an empirical measure with N samples and a
157"
IMPLICIT MEASURE ARCHITECTURE,0.3509700176366843,"single layer of width N. Building on this correspondence, we propose a mean-ﬁeld layer:
158"
IMPLICIT MEASURE ARCHITECTURE,0.3527336860670194,"Deﬁnition 3.1 (Mean-ﬁeld Layer). Deﬁne the weight of the mean-ﬁeld layer with width n as the
159"
IMPLICIT MEASURE ARCHITECTURE,0.3544973544973545,matrix W0 2 Rn⇥d and denote its ith row as W (i)
IMPLICIT MEASURE ARCHITECTURE,0.3562610229276896,"0 . The mean-ﬁeld layer then is deﬁned by the
160"
IMPLICIT MEASURE ARCHITECTURE,0.35802469135802467,"operation
161"
IMPLICIT MEASURE ARCHITECTURE,0.35978835978835977,"MF(n)('(Xt)) := 1 n n
X i=1"
IMPLICIT MEASURE ARCHITECTURE,0.36155202821869487,"'(Xt, W (i)"
IMPLICIT MEASURE ARCHITECTURE,0.36331569664902996,"0 ) dPt dP0 .
(6)"
IMPLICIT MEASURE ARCHITECTURE,0.36507936507936506,"The mean-ﬁeld layer (MF) can be thought of as another layer within the network architecture that
162"
IMPLICIT MEASURE ARCHITECTURE,0.36684303350970016,approximates the law pt. Each row W (i)
IMPLICIT MEASURE ARCHITECTURE,0.36860670194003525,"0
is of size Rd, corresponding to the dimensions of X(i)"
IMPLICIT MEASURE ARCHITECTURE,0.37037037037037035,"t
2 Rd.
163"
IMPLICIT MEASURE ARCHITECTURE,0.37213403880070545,"The activation function of the mean-ﬁeld layer is the average over the augmented dimension over
164"
IMPLICIT MEASURE ARCHITECTURE,0.37389770723104054,which MF operates. The change of measure dPt
IMPLICIT MEASURE ARCHITECTURE,0.37566137566137564,"dP0 can be learned as part of the estimator of the
165"
IMPLICIT MEASURE ARCHITECTURE,0.37742504409171074,"interaction function, '(·, ·, t; ✓). Importantly, the above representation allows modeling mean-ﬁeld
166"
IMPLICIT MEASURE ARCHITECTURE,0.37918871252204583,"interactions without the need for a full set of observations at each time point and without the need to
167"
IMPLICIT MEASURE ARCHITECTURE,0.38095238095238093,"explicitly represent the distribution pt at each time point. Assuming that ' and MF are well learned,
168"
IMPLICIT MEASURE ARCHITECTURE,0.38271604938271603,"this architecture can represent the true MV-SDE drift in the limit as the width n ! 1. We note
169"
IMPLICIT MEASURE ARCHITECTURE,0.3844797178130511,"empirically that a ﬁnite n is sufﬁcient and we provide examples of ablations in the appendix.
170"
IMPLICIT MEASURE ARCHITECTURE,0.3862433862433862,"A similar analysis can be made for the standard MLP architecture. However, the explicit separation
171"
IMPLICIT MEASURE ARCHITECTURE,0.3880070546737213,"of f and ' is not enforced in this case. This leads us to the following remark:
172"
IMPLICIT MEASURE ARCHITECTURE,0.3897707231040564,"Remark 3.2 (Itô-SDEs with drift represented using MLPs can model MV-SDEs). From the above
173"
IMPLICIT MEASURE ARCHITECTURE,0.3915343915343915,"discussion, the expectation with respect to the law pt may be implicitly represented by a MLP.
174"
IMPLICIT MEASURE ARCHITECTURE,0.3932980599647266,"Our motivation is then concerned with how a relatively more explicit distribution dependence with
175"
IMPLICIT MEASURE ARCHITECTURE,0.3950617283950617,"' and MF affect modeling capabilities. This explicit structure lends to an implicit regularization
176"
IMPLICIT MEASURE ARCHITECTURE,0.3968253968253968,"that promotes a smaller norm of the mean-ﬁeld component under a maximum likelihood estimation
177"
IMPLICIT MEASURE ARCHITECTURE,0.3985890652557319,"framework, which we detail later in Section 5.1.
178"
MARGINAL LAW ARCHITECTURE,0.400352733686067,"3.3
Marginal law architecture
179"
MARGINAL LAW ARCHITECTURE,0.4021164021164021,"A solution to the MV-SDE is the pair (X, p) such that pt = Law(Xt). In addition, if p is a solution
180"
MARGINAL LAW ARCHITECTURE,0.4038800705467372,"to the SDE in (2), it is also a weak solution to the PDE in (3), and the converse holds. For this reason,
181"
MARGINAL LAW ARCHITECTURE,0.4056437389770723,"p is often itself the main object of study. In the marginal law (ML) architecture, in conjunction with
182"
MARGINAL LAW ARCHITECTURE,0.4074074074074074,"the drift, we introduce a generative model for representing the time-varying density. In this case, we
183"
MARGINAL LAW ARCHITECTURE,0.4091710758377425,"approximate the expectation in (2) as
184"
MARGINAL LAW ARCHITECTURE,0.4109347442680776,"Eyt⇠pt [' (Xt, yt)] ⇡Eyt⇠ˆ
Pt [' (Xt, yt; ✓)] = 1 n n
X i=1 ' ⇣"
MARGINAL LAW ARCHITECTURE,0.4126984126984127,"Xt, ˆX(i) t ; ✓ ⌘ (7)"
MARGINAL LAW ARCHITECTURE,0.4144620811287478,where the expectation is taken with respect to the discrete measure derived from samples { ˆX(i) t }n
MARGINAL LAW ARCHITECTURE,0.41622574955908287,"i=1
185"
MARGINAL LAW ARCHITECTURE,0.41798941798941797,"from the generative model ˆPt. The parameter estimation problem then requires optimizing both the
186"
MARGINAL LAW ARCHITECTURE,0.41975308641975306,"generative model ˆPt and the networks f and ' representing the drift, while ensuring consistency
187"
MARGINAL LAW ARCHITECTURE,0.42151675485008816,"between the two. Using knowledge of the PDE in (3), we regularize ˆPt such that it matches the ﬂow
188"
MARGINAL LAW ARCHITECTURE,0.42328042328042326,"relating to the drift. Additional details regarding the PDE and its relationship to the ML architecture
189"
MARGINAL LAW ARCHITECTURE,0.42504409171075835,"are in the appendix.
190"
PARAMETER ESTIMATION,0.42680776014109345,"4
Parameter Estimation
191"
PARAMETER ESTIMATION,0.42857142857142855,"Having presented the relevant architectures, we now describe the procedures for estimating the
192"
PARAMETER ESTIMATION,0.43033509700176364,"parameters of the different architectures. We ﬁrst describe the likelihood function for use in cases
193"
PARAMETER ESTIMATION,0.43209876543209874,"with regularly sampled data. We then describe a bridge estimator for cases of irregularly sampled
194"
PARAMETER ESTIMATION,0.43386243386243384,"data. Finally, we describe an estimator for the generative architecture based on both the likelihood
195"
PARAMETER ESTIMATION,0.43562610229276894,"function and the transition density. For this section, we assume that we observe multiple paths, i.e.,
196
n"
PARAMETER ESTIMATION,0.43738977072310403,{Xtj}(i)
PARAMETER ESTIMATION,0.43915343915343913,j=1...K o
PARAMETER ESTIMATION,0.4409171075837742,"i=1...N. Full details of all algorithms are in the appendix.
197"
MAXIMUM LIKELIHOOD ESTIMATION,0.4426807760141093,"4.1
Maximum likelihood estimation
198"
MAXIMUM LIKELIHOOD ESTIMATION,0.4444444444444444,"We use an estimator based on the path-wise likelihood derived from Girsanov’s theorem and an
199"
MAXIMUM LIKELIHOOD ESTIMATION,0.4462081128747795,"Euler-Maruyama discretization for the likelihood, considered in Sharrock et al. [2021]. The likelihood
200"
MAXIMUM LIKELIHOOD ESTIMATION,0.4479717813051146,"function is given as
201"
MAXIMUM LIKELIHOOD ESTIMATION,0.4497354497354497,"L(✓; t1, tK) := exp ✓1 σ2 Z tK t1"
MAXIMUM LIKELIHOOD ESTIMATION,0.4514991181657848,"b (Xs, ps, s; ✓) dXs −
1
2σ2 Z tK t1"
MAXIMUM LIKELIHOOD ESTIMATION,0.4532627865961199,"b (Xs, ps, s; ✓)2 ds ◆ .
(8)"
MAXIMUM LIKELIHOOD ESTIMATION,0.455026455026455,"Following discretization, with the approximations ∆Xtj = Xtj+1 −Xtj and ∆tj = tj+1 −tj, the
202"
MAXIMUM LIKELIHOOD ESTIMATION,0.4567901234567901,"log-likelihood is approximated by
203"
MAXIMUM LIKELIHOOD ESTIMATION,0.4585537918871252,"log L(✓; t1, tK) ⇡ K−1
X j=1 b 5"
MAXIMUM LIKELIHOOD ESTIMATION,0.4603174603174603,"Xtj, ptj, tj; ✓ 6"
MAXIMUM LIKELIHOOD ESTIMATION,0.4620811287477954,"(Xtj+1 −Xtj) −1 2 K−1
X j=1 b 5"
MAXIMUM LIKELIHOOD ESTIMATION,0.4638447971781305,"Xtj, ptj, tj; ✓"
MAXIMUM LIKELIHOOD ESTIMATION,0.4656084656084656,62 (tj+1 −tj).
MAXIMUM LIKELIHOOD ESTIMATION,0.4673721340388007,"If the time interval ∆t is large, then this likelihood loses accuracy, as is a property of the Euler-
204"
MAXIMUM LIKELIHOOD ESTIMATION,0.4691358024691358,"Maruyama discretization. Optimization is performed using standard gradient based optimizers with
205"
MAXIMUM LIKELIHOOD ESTIMATION,0.4708994708994709,"the drift b represented as one of the presented architectures.
206"
ESTIMATION WITH BROWNIAN BRIDGES,0.47266313932980597,"4.2
Estimation with Brownian bridges
207"
ESTIMATION WITH BROWNIAN BRIDGES,0.47442680776014107,"Often data are not collected at uniform intervals in time, but rather, the time marginals may be
208"
ESTIMATION WITH BROWNIAN BRIDGES,0.47619047619047616,"collected at irregular intervals. In that case, we consider an interpolation approach to maximizing the
209"
ESTIMATION WITH BROWNIAN BRIDGES,0.47795414462081126,"likelihood following the results of Lavenant et al. [2021] and Cameron et al. [2021] in the Itô-SDE
210"
ESTIMATION WITH BROWNIAN BRIDGES,0.47971781305114636,"case. We can write the likelihood conditioned on the set of observations (dropping the particle index
211"
ESTIMATION WITH BROWNIAN BRIDGES,0.48148148148148145,"for ease of notation) as
212"
ESTIMATION WITH BROWNIAN BRIDGES,0.48324514991181655,LBB(✓) = EQ 2
Y,0.48500881834215165,"4
Y"
Y,0.48677248677248675,j=1...K−1
Y,0.48853615520282184,"1{Ztj+1 −Xtj+1}L(✓; tj, tj+1) 3 5"
Y,0.49029982363315694,"where {Zs : s 2 [tj, tj+1]} is a Brownian bridge from Xtj to Xtj+1 and Q is the Wiener measure.
213"
Y,0.49206349206349204,"Brownian bridges can easily be sampled and reused for computing the expectation, which reduces
214"
Y,0.49382716049382713,"the variance of the estimator. By applying Jensen’s inequality, we can write an evidence lower bound
215"
Y,0.49559082892416223,"(ELBO) as
216"
Y,0.4973544973544973,log LBB ≥EQ 2
X,0.4991181657848324,"4
X"
X,0.5008818342151675,j=1...K−1
X,0.5026455026455027,"log L(✓; tj, tj+1) <<<< ="
X,0.5044091710758377,Ztj = Xtj K j=1 3
X,0.5061728395061729,"5 .
(9)"
X,0.5079365079365079,"The ELBO in this case aims to ﬁt the observed marginal distributions exactly while penalizing
217"
X,0.5097001763668431,"deviations in regions without data that deviate from the Brownian bridge paths.
218"
X,0.5114638447971781,"4.3
Estimation with explicit marginal law ˆPt
219"
X,0.5132275132275133,"Returning to the ML architecture described in Section 3.3, where we explicitly model the density pt
220"
X,0.5149911816578483,"with a generative network ˆPt, our estimator should enforce the regularity of pt through its PDE in (3).
221"
X,0.5167548500881834,"Let the parameters of the drift be ✓and the parameters of the generative model be φ, then we solve
222"
X,0.5185185185185185,"the optimization problem
223 max ✓,φ
E ⇥"
X,0.5202821869488536,"L(✓, φ | {Xtj}j=1...K) ⇤"
X,0.5220458553791887,"s.t.
(10)
Z tj+1 tj"
X,0.5238095238095238,AAA ˆPs(x; φ) −E h
X,0.5255731922398589,ˆPtj+1 ⇣
X,0.527336860670194,ˆXtj+1; φ ⌘
X,0.5291005291005291,| ˆXs = x
X,0.5308641975308642,"iAAA ds = 0
(11)"
X,0.5326278659611993,"for time intervals indexed by j = 1 . . . K −1, the state space x 2 supp(Xt), and where the
224"
X,0.5343915343915344,"trajectories of ˆXt follow the dynamics of the ML architecture, speciﬁcally
225"
X,0.5361552028218695,"d ˆXt = f( ˆXt, t; ✓)dt + Eyt⇠ˆ
Pt(·;φ) h ' ⇣"
X,0.5379188712522046,"ˆXt, yt; ✓ ⌘i"
X,0.5396825396825397,"dt + σdWt.
(12)"
X,0.5414462081128748,"The likelihood at the observed margins is ﬁrst maximized in (10). In (11), the marginals at previous
226"
X,0.5432098765432098,"times are regularized using the correspondence between the PDE and its associated SDE via the
227"
X,0.544973544973545,"nonlinear Kolomogorov backwards equation [Buckdahn et al., 2017], which describes pt as an
228"
X,0.54673721340388,"expectation of trajectories at a terminal time, i.e. pt(x) = E[pT (XT )|Xt = x] for t < T.
229"
MODELING PROPERTIES,0.5485008818342152,"5
Modeling Properties
230"
MODELING PROPERTIES,0.5502645502645502,"Having discussed the architectures and estimators, we now discuss speciﬁc properties of the modeling
231"
MODELING PROPERTIES,0.5520282186948854,"framework, which follow from the theoretical discussion presented in Section 2. We ﬁrst discuss how
232"
MODELING PROPERTIES,0.5537918871252204,"the factorization into ' and MF lends to an implicit regularization of the IM architecture. We then
233"
MODELING PROPERTIES,0.5555555555555556,"compare the gradient ﬂows of Itô-SDEs and MV-SDEs.
234"
IMPLICIT REGULARIZATION OF THE IMPLICIT MEASURE ARCHITECTURE,0.5573192239858906,"5.1
Implicit regularization of the implicit measure architecture
235"
IMPLICIT REGULARIZATION OF THE IMPLICIT MEASURE ARCHITECTURE,0.5590828924162258,"Closely related to the IM architecture are neural Itô-SDEs, where we previously remarked can model
236"
IMPLICIT REGULARIZATION OF THE IMPLICIT MEASURE ARCHITECTURE,0.5608465608465608,"MV-SDEs. On the other hand, the factorization of the IM architecture into ' and MF leads to a type
237"
IMPLICIT REGULARIZATION OF THE IMPLICIT MEASURE ARCHITECTURE,0.562610229276896,"of implicit regularization when the parameters are estimated using gradient descent.
238"
IMPLICIT REGULARIZATION OF THE IMPLICIT MEASURE ARCHITECTURE,0.564373897707231,"Proposition 5.1 (Implicit Regularization). Suppose f, ' known and ﬁxed. Further, assume that ' is
239"
IMPLICIT REGULARIZATION OF THE IMPLICIT MEASURE ARCHITECTURE,0.5661375661375662,"twice differentiable. Then, for each time step t, the minimizing ﬁnite width MF with weight matrix
240"
IMPLICIT REGULARIZATION OF THE IMPLICIT MEASURE ARCHITECTURE,0.5679012345679012,W0 2 Rn⇥d and ith row W (i)
IMPLICIT REGULARIZATION OF THE IMPLICIT MEASURE ARCHITECTURE,0.5696649029982364,"0
under gradient descent satisﬁes the following optimization problem
241 min W0 X"
IMPLICIT REGULARIZATION OF THE IMPLICIT MEASURE ARCHITECTURE,0.5714285714285714,i=1...n X
IMPLICIT REGULARIZATION OF THE IMPLICIT MEASURE ARCHITECTURE,0.5731922398589065,j=1...d
IMPLICIT REGULARIZATION OF THE IMPLICIT MEASURE ARCHITECTURE,0.5749559082892416,"'(Xt, W (i)"
IMPLICIT REGULARIZATION OF THE IMPLICIT MEASURE ARCHITECTURE,0.5767195767195767,"0 )j
s.t.
E 1"
IMPLICIT REGULARIZATION OF THE IMPLICIT MEASURE ARCHITECTURE,0.5784832451499118,"2∆t kXt+∆t −Xt −b(Xt, pt, t)k2 E = 0."
IMPLICIT REGULARIZATION OF THE IMPLICIT MEASURE ARCHITECTURE,0.5802469135802469,"Proof. We follow the blueprint in Belabbas [2020] and give full details in the appendix.
242"
IMPLICIT REGULARIZATION OF THE IMPLICIT MEASURE ARCHITECTURE,0.582010582010582,"Proposition 5.1 effectively says that the mean-ﬁeld system approximated is the one that has the least
243"
IMPLICIT REGULARIZATION OF THE IMPLICIT MEASURE ARCHITECTURE,0.5837742504409171,"inﬂuence from the other particles under perfectly matched marginals. In the case where ' can be
244"
IMPLICIT REGULARIZATION OF THE IMPLICIT MEASURE ARCHITECTURE,0.5855379188712522,"decomposed as a norm, this amounts to ﬁnding the drift parameterized by weight W0 with smallest
245"
IMPLICIT REGULARIZATION OF THE IMPLICIT MEASURE ARCHITECTURE,0.5873015873015873,"norm while still matching the marginals.
246"
IMPLICIT REGULARIZATION OF THE IMPLICIT MEASURE ARCHITECTURE,0.5890652557319224,"5.2
Gradient ﬂows of the MV-SDE
247"
IMPLICIT REGULARIZATION OF THE IMPLICIT MEASURE ARCHITECTURE,0.5908289241622575,"To illustrate the difference between the MV-SDE and Itô-SDE particle ﬂows, we invoke the analysis
248"
IMPLICIT REGULARIZATION OF THE IMPLICIT MEASURE ARCHITECTURE,0.5925925925925926,"in Santambrogio [2017, Section 4.6] to describe the functionals that are minimized by each.
249"
IMPLICIT REGULARIZATION OF THE IMPLICIT MEASURE ARCHITECTURE,0.5943562610229277,"Remark 5.2 (Functional Minimizer). Consider two drifts B = rf(X) and BMF = B +
250"
IMPLICIT REGULARIZATION OF THE IMPLICIT MEASURE ARCHITECTURE,0.5961199294532628,E[r'(X −y)]. Consider a functional F[p] = R
IMPLICIT REGULARIZATION OF THE IMPLICIT MEASURE ARCHITECTURE,0.5978835978835979,log pdp+ R
IMPLICIT REGULARIZATION OF THE IMPLICIT MEASURE ARCHITECTURE,0.599647266313933,"f(X)dp for some measure p absolutely
251"
IMPLICIT REGULARIZATION OF THE IMPLICIT MEASURE ARCHITECTURE,0.6014109347442681,"continuous with respect to the Lebesgue measure. Then, the gradient ﬂow satisfying the linear
252"
IMPLICIT REGULARIZATION OF THE IMPLICIT MEASURE ARCHITECTURE,0.6031746031746031,"Fokker-Planck equation with drift B minimizes F. On the other hand, the nonlinear Fokker-Planck
253"
IMPLICIT REGULARIZATION OF THE IMPLICIT MEASURE ARCHITECTURE,0.6049382716049383,associated with drift BMF minimizes the functional FMF[p] = F[p] + R
IMPLICIT REGULARIZATION OF THE IMPLICIT MEASURE ARCHITECTURE,0.6067019400352733,"'(X −Y )dp(X)dp(Y ).
254"
IMPLICIT REGULARIZATION OF THE IMPLICIT MEASURE ARCHITECTURE,0.6084656084656085,"This has an important implication, for example, if we take '(·) = 2k · k dq"
IMPLICIT REGULARIZATION OF THE IMPLICIT MEASURE ARCHITECTURE,0.6102292768959435,"dp −k · k2 −k · k2 ⇣ dq
dp ⌘2 255"
IMPLICIT REGULARIZATION OF THE IMPLICIT MEASURE ARCHITECTURE,0.6119929453262787,"then the functional is minimizing the squared energy distance between a target measure q as well as
256"
IMPLICIT REGULARIZATION OF THE IMPLICIT MEASURE ARCHITECTURE,0.6137566137566137,"the entropy. We use this example to motivate some of the experiments on probabilistic modeling.
257"
NUMERICAL EXPERIMENTS,0.6155202821869489,"6
Numerical Experiments
258"
NUMERICAL EXPERIMENTS,0.6172839506172839,"We discussed Q1 on modeling and inferring distributional dependence. We now wish to answer
259"
NUMERICAL EXPERIMENTS,0.6190476190476191,"Q2 and quantify the effect of distributional dependence in machine learning tasks. To do this, we
260"
NUMERICAL EXPERIMENTS,0.6208112874779541,"test the methods on synthetic and real data for time series estimation and sample generation. The
261"
NUMERICAL EXPERIMENTS,0.6225749559082893,"main goal is to determine the difference between standard Neural Itô-SDE and the proposed Neural
262"
NUMERICAL EXPERIMENTS,0.6243386243386243,"MV-SDEs under different modeling scenarios. In that sense, the baseline we consider is the Itô-SDE
263"
NUMERICAL EXPERIMENTS,0.6261022927689595,"parameterized using an MLP. However, we also consider other deep learning based methods for
264"
NUMERICAL EXPERIMENTS,0.6278659611992945,"comparison in a broader context. We abbreviate the different architectures as the Empirical Measure
265"
NUMERICAL EXPERIMENTS,0.6296296296296297,"(EM) in Section 3.1, Implicit Measure (IM) in Section 3.2, and Marginal Law (ML) in Section 3.3.
266"
NUMERICAL EXPERIMENTS,0.6313932980599647,"Full descriptions of the models, baselines, and datasets are given in the appendix.
267"
NUMERICAL EXPERIMENTS,0.6331569664902998,"Synthetic data experiments
Motivated by the application of MV-SDEs in physical, biological,
268"
NUMERICAL EXPERIMENTS,0.6349206349206349,"social, and ﬁnancial settings, we benchmark the proposed methods on 4 canonical MV-SDEs: the
269"
NUMERICAL EXPERIMENTS,0.63668430335097,"Kuramoto model which describes synchronizing oscillators [Sonnenschein and Schimansky-Geier,
270"
NUMERICAL EXPERIMENTS,0.6384479717813051,"2013], the mean-ﬁeld FitzHugh-Nagumo model which characterizes spikes in neuron activations
271"
NUMERICAL EXPERIMENTS,0.6402116402116402,"Figure 4: Top row: sample paths from the different synthetic datasets. Bottom row: mean squared
error (MSE) of different architectures’ performance on drift estimation, under the effect of different
levels of observation noise. Reported value is an average of 10 runs."
NUMERICAL EXPERIMENTS,0.6419753086419753,"[Mischler et al., 2016], the opinion dynamic model on the formation of opinion groups [Sharrock
272"
NUMERICAL EXPERIMENTS,0.6437389770723104,"et al., 2021], and the mean-ﬁeld atlas model for pricing equity markets [Jourdain and Reygner,
273"
NUMERICAL EXPERIMENTS,0.6455026455026455,"2015]. We additionally benchmark the proposed methods on two Itô-SDEs: an Ornstein–Uhlenbeck
274"
NUMERICAL EXPERIMENTS,0.6472663139329806,"(OU) process and a circular motion equation to determine the performance on Itô-SDEs. Finally, to
275"
NUMERICAL EXPERIMENTS,0.6490299823633157,"understand the performance on discontinuous paths, we benchmark the proposed methods on an OU
276"
NUMERICAL EXPERIMENTS,0.6507936507936508,"process with jumps. We focus on recovering the drift from observations.
277"
NUMERICAL EXPERIMENTS,0.6525573192239859,"Figure 5: Left: Average paths of true and estimated OU
process with 4 jumps. Right: Energy distance between
true and generated paths."
NUMERICAL EXPERIMENTS,0.654320987654321,"Since the true drifts of the synthetic data are
278"
NUMERICAL EXPERIMENTS,0.656084656084656,"known, we directly compare the estimated
279"
NUMERICAL EXPERIMENTS,0.6578483245149912,"drifts to the true drifts. The performance
280"
NUMERICAL EXPERIMENTS,0.6596119929453262,"on ﬁve different datasets with three differ-
281"
NUMERICAL EXPERIMENTS,0.6613756613756614,"ent levels of added observational noise is
282"
NUMERICAL EXPERIMENTS,0.6631393298059964,"presented in Figure 4. The proposed mean-
283"
NUMERICAL EXPERIMENTS,0.6649029982363316,"ﬁeld architectures outperform the standard
284"
NUMERICAL EXPERIMENTS,0.6666666666666666,"MLP in modeling MV-SDEs; moreover,
285"
NUMERICAL EXPERIMENTS,0.6684303350970018,"our experiments on OU and circular pro-
286"
NUMERICAL EXPERIMENTS,0.6701940035273368,"cess suggest that incorporating explicit dis-
287"
NUMERICAL EXPERIMENTS,0.671957671957672,"tributional depedence does not diminish the
288"
NUMERICAL EXPERIMENTS,0.673721340388007,"performance in estimating non-interacting
289"
NUMERICAL EXPERIMENTS,0.6754850088183422,"Itô-SDEs. When modeling processes with
290"
NUMERICAL EXPERIMENTS,0.6772486772486772,"jump discontinuities, Figure 5 highlights the ﬂexibility of the proposed methods, IM, ML, to match
291"
NUMERICAL EXPERIMENTS,0.6790123456790124,"such models. The EM likely does not perform as well due to the high variance of the empirical
292"
NUMERICAL EXPERIMENTS,0.6807760141093474,"measure, leading to difﬁculties in learning. Additionally, the MLP does not have an explicit decompo-
293"
NUMERICAL EXPERIMENTS,0.6825396825396826,"sition between the MV and Itô components, resulting in issues when estimating the feedback between
294"
NUMERICAL EXPERIMENTS,0.6843033509700176,"the particles inducing jumps.
295"
NUMERICAL EXPERIMENTS,0.6860670194003528,"Real data experiments
Extending from the synthetic examples, we consider two real examples:
296"
NUMERICAL EXPERIMENTS,0.6878306878306878,"brain activity recorded by electroencephalograms (EEG), which is closely related to the Kuramoto
297"
NUMERICAL EXPERIMENTS,0.689594356261023,"model [Nguyen et al., 2020]; and chemically stimulated movement of organisms (chemotaxis), which
298"
NUMERICAL EXPERIMENTS,0.691358024691358,"can be modeled by the Keller-Segel model [Tomaševi´c, 2021, Keller and Segel, 1971].
299"
NUMERICAL EXPERIMENTS,0.6931216931216931,"We evaluate the proposed architectures in these modeling tasks by comparing the goodness-of-ﬁt
300"
NUMERICAL EXPERIMENTS,0.6948853615520282,"of generated path samples to the observed path samples. We compute the Continuous Ranked
301"
NUMERICAL EXPERIMENTS,0.6966490299823633,"Probability Score (CRPS) deﬁned in Gneiting and Raftery [2007] (see appendix for details) for
302"
NUMERICAL EXPERIMENTS,0.6984126984126984,"the 1-dimensional EEG data, and the normalized MSE (normalized with sample variance) for the
303"
NUMERICAL EXPERIMENTS,0.7001763668430335,"3-dimensional chemotaxis data with respect to the held out data. We also benchmark against the
304"
NUMERICAL EXPERIMENTS,0.7019400352733686,"DeepAR probabilistic time series forecaster [Salinas et al., 2020] with RNN, GRU, LSTM, and
305"
NUMERICAL EXPERIMENTS,0.7037037037037037,"Transformer (TR) backbones as another baseline model to compare the goodness-of-ﬁt.
306"
NUMERICAL EXPERIMENTS,0.7054673721340388,"The performances of different architectures are presented in Table 1. For EEG, the proposed
307"
NUMERICAL EXPERIMENTS,0.7072310405643739,"architectures generally perform better than the baselines in generating paths within the training time
308"
NUMERICAL EXPERIMENTS,0.708994708994709,"steps, and on par with the DeepAR architectures for forecasting (full results presented in appendix).
309"
NUMERICAL EXPERIMENTS,0.7107583774250441,"For chemotaxis data, the MV-SDE based architectures all outperform the DeepAR baselines.
310"
NUMERICAL EXPERIMENTS,0.7125220458553791,"Table 1: Time series estimation on held out trajectories. NA/A stands
for non-alcoholics/alcoholics. Bolded values and italic values are best
and second best respectively."
NUMERICAL EXPERIMENTS,0.7142857142857143,"CRPS #
MSE #"
NUMERICAL EXPERIMENTS,0.7160493827160493,"NA-EEG
A-EEG
C.Cres
E.Coli"
NUMERICAL EXPERIMENTS,0.7178130511463845,"MLP (Itô)
5.52 (1.40)
4.33 (1.14)
0.096 (0.002)
0.080 (0.003)
IM
5.23 (1.24)
4.30 (1.21)
0.094 (0.003)
0.080 (0.001)
ML
5.10 (1.22)
4.05 (1.12)
0.093 (0.002)
0.084 (0.002)
EM
5.35 (1.22)
4.09 (1.11)
0.093 (0.004)
0.086 (0.004)"
NUMERICAL EXPERIMENTS,0.7195767195767195,"LSTM
6.27 (2.02)
5.68 (2.56)
1.159 (0.234)
0.585 (0.350)
RNN
6.22 (2.07)
4.64 (1.38)
1.563 (1.070)
0.773 (0.092)
GRU
6.35 (2.01)
6.18 (2.73)
0.826 (0.289)
0.568 (0.301)
TR
5.95 (1.45)
4.29 (1.36)
1.503 (0.212)
1.204 (0.212)"
NUMERICAL EXPERIMENTS,0.7213403880070547,"Figure 6: ELBO of generated
paths from standard Gaussian
to eight Gaussian mixture (in
increasing dimension) evalu-
ated against OT mapping. 311"
NUMERICAL EXPERIMENTS,0.7231040564373897,"Generative modeling experiments
We focus on applying the bridge estimator discussed in Sec-
312"
NUMERICAL EXPERIMENTS,0.7248677248677249,"tion 4.2 to map between a Gaussian and a target distribution. We are interested in two aspects: 1) the
313"
NUMERICAL EXPERIMENTS,0.7266313932980599,"properties of the learned mapping, and 2) the generated trajectories. We ﬁrst study the properties of
314"
NUMERICAL EXPERIMENTS,0.7283950617283951,"the learned mapping using a synthetic eight Gaussian mixture with increasing dimensionality. We
315"
NUMERICAL EXPERIMENTS,0.7301587301587301,"compare the performance of different architectures through the ELBO of the sample paths generated
316"
NUMERICAL EXPERIMENTS,0.7319223985890653,"by the optimal transport (OT) mapping between the initial distribution and held out target samples.
317"
NUMERICAL EXPERIMENTS,0.7336860670194003,"We next evaluate the generated trajectories through the energy distance (see appendix for details)
318"
NUMERICAL EXPERIMENTS,0.7354497354497355,"between generated and held-out data for 5 real data density estimation experiments. In addition,
319"
NUMERICAL EXPERIMENTS,0.7372134038800705,"we compare to common density estimators of variational autoencoder (VAE) [Kingma and Welling,
320"
NUMERICAL EXPERIMENTS,0.7389770723104057,"2013], Wasserstein generative adversarial network (W-GAN) [Gulrajani et al., 2017], masked autore-
321"
NUMERICAL EXPERIMENTS,0.7407407407407407,"gresive ﬂow (MAF) [Papamakarios et al., 2017] and score-based generative modeling through SDEs,
322"
NUMERICAL EXPERIMENTS,0.7425044091710759,"which corresponds to a constrained form of the MLP [Song et al., 2020]. The MV-SDE architectures
323"
NUMERICAL EXPERIMENTS,0.7442680776014109,"not only outperform the Itô architecture for all dimensions in the eight Gaussian experiment, as
324"
NUMERICAL EXPERIMENTS,0.746031746031746,"shown in Figure 6, but also for the 5 real data density estimation experiments, as shown in Table 2,
325"
NUMERICAL EXPERIMENTS,0.7477954144620811,"while outperforming common baselines. All sampling is performed using standard Euler-Maruyama,
326"
NUMERICAL EXPERIMENTS,0.7495590828924162,"with full details of the sampling and inference algorithms in the appendix. This again suggests the
327"
NUMERICAL EXPERIMENTS,0.7513227513227513,"MV-SDE provides a more amenable probability ﬂow for modeling compared with the Itô case.
328"
NUMERICAL EXPERIMENTS,0.7530864197530864,"Table 2: Density estimation: Energy distance between observed samples and generated samples of
different methods. Bolded values and italic values are best and second best correspondingly."
NUMERICAL EXPERIMENTS,0.7548500881834215,"POWER
MINIBOONE
HEPMASS
GAS
CORTEX"
NUMERICAL EXPERIMENTS,0.7566137566137566,"MLP (Itô)
0.342 (0.096)
0.674 (0.048)
0.537 (0.052)
0.405 (0.08)
0.742 (0.062)
IM
0.292 (0.078)
0.395 (0.045)
0.405 (0.025)
0.287 (0.082)
0.53 (0.026)
ML
0.282 (0.083)
0.443 (0.034)
0.366 (0.03)
0.305 (0.063)
0.568 (0.03)
EM
0.328 (0.116)
0.455 (0.036)
0.429 (0.046)
0.298 (0.036)
0.577 (0.037)"
NUMERICAL EXPERIMENTS,0.7583774250440917,"VAE
1.19 (0.024)
2.117 (0.148)
1.763 (0.031)
1.516 (0.023)
2.412 (0.197)
W-GAN
1.248 (0.017)
2.079 (0.003)
1.819 (0.013)
1.3 (0.016)
2.19 (0.011)
MAF
0.288 (0.041)
0.467 (0.009)
0.308 (0.017)
0.519 (0.033)
0.532 (0.026)
Score-Based
0.302 (0.049)
0.499 (0.019)
0.324 (0.028)
0.562 (0.043)
0.582 (0.020)"
DISCUSSION,0.7601410934744268,"7
Discussion
329"
DISCUSSION,0.7619047619047619,"In this paper we discuss an alternative viewpoint of the standard Itô-SDE parameterization. In
330"
DISCUSSION,0.763668430335097,"particular, we focus on MV-SDEs and discuss how neural networks can represent a process that
331"
DISCUSSION,0.7654320987654321,"depends on the distribution, and we describe ways of making this dependence more explicit. We
332"
DISCUSSION,0.7671957671957672,"demonstrated the efﬁcacy of the proposed architectures on a number of synthetic and real benchmarks.
333"
DISCUSSION,0.7689594356261023,"The results suggest that the proposed architectures provide an improvement over baselines in certain
334"
DISCUSSION,0.7707231040564374,"generative modeling and time series applications.
335"
DISCUSSION,0.7724867724867724,"Limitations We only studied the implicit regularization of the IM architecture under gradient descent,
336"
DISCUSSION,0.7742504409171076,"but the extension of the analysis to the other proposed architectures is important to understand the
337"
DISCUSSION,0.7760141093474426,"corresponding regularization. Additionally, computing expectations incurs additional computational
338"
DISCUSSION,0.7777777777777778,"cost. Improving the computational accuracy using a multilevel scheme as proposed in Szpruch et al.
339"
DISCUSSION,0.7795414462081128,"[2019] could improve the performance of the methods.
340"
REFERENCES,0.781305114638448,"References
341"
REFERENCES,0.783068783068783,"Mohamed Ali Belabbas. On implicit regularization: Morse functions and applications to matrix
342"
REFERENCES,0.7848324514991182,"factorization. arXiv preprint arXiv:2001.04264, 2020.
343"
REFERENCES,0.7865961199294532,"Rainer Buckdahn, Juan Li, Shige Peng, and Catherine Rainer. Mean-ﬁeld stochastic differential
344"
REFERENCES,0.7883597883597884,"equations and associated pdes. The Annals of Probability, 45(2):824–878, 2017. ISSN 00911798,
345"
REFERENCES,0.7901234567901234,"2168894X. URL http://www.jstor.org/stable/44245559.
346"
REFERENCES,0.7918871252204586,"Scott Cameron, Tyron Cameron, Arnu Pretorius, and Stephen Roberts. Robust and scalable sde
347"
REFERENCES,0.7936507936507936,"learning: A functional perspective. arXiv preprint arXiv:2110.05167, 2021.
348"
REFERENCES,0.7954144620811288,"JA Carrillo, RS Gvalani, GA Pavliotis, and A Schlichting. Long-time behaviour and phase transitions
349"
REFERENCES,0.7971781305114638,"for the mckean–vlasov equation on the torus. Archive for Rational Mechanics and Analysis, 235
350"
REFERENCES,0.798941798941799,"(1):635–690, 2020.
351"
REFERENCES,0.800705467372134,"José A Carrillo, María d M González, Maria P Gualdani, and Maria E Schonbek. Classical solutions
352"
REFERENCES,0.8024691358024691,"for a nonlinear fokker-planck equation arising in computational neuroscience. Communications in
353"
REFERENCES,0.8042328042328042,"Partial Differential Equations, 38(3):385–409, 2013.
354"
REFERENCES,0.8059964726631393,"Lénaïc Chizat, Stephen Zhang, Matthieu Heitz, and Geoffrey Schiebinger. Trajectory inference
355"
REFERENCES,0.8077601410934744,"via mean-ﬁeld langevin in path space. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and
356"
REFERENCES,0.8095238095238095,"Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL
357"
REFERENCES,0.8112874779541446,"https://openreview.net/forum?id=Mftcm8i4sL.
358"
REFERENCES,0.8130511463844797,"Joachim Crevat, Grégory Faye, and Francis Filbet. Rigorous derivation of the nonlocal reaction-
359"
REFERENCES,0.8148148148148148,"diffusion ﬁtzhugh–nagumo system. SIAM Journal on Mathematical Analysis, 51(1):346–373,
360"
REFERENCES,0.8165784832451499,"2019.
361"
REFERENCES,0.818342151675485,"Francesca R Crucinio, Valentin De Bortoli, Arnaud Doucet, and Adam M Johansen. Solving fredholm
362"
REFERENCES,0.8201058201058201,"integral equations of the ﬁrst kind via wasserstein gradient ﬂows. arXiv preprint arXiv:2209.09936,
363"
REFERENCES,0.8218694885361552,"2022.
364"
REFERENCES,0.8236331569664903,"Laetitia Della Maestra and Marc Hoffmann. Nonparametric estimation for interacting particle systems:
365"
REFERENCES,0.8253968253968254,"Mckean–vlasov models. Probability Theory and Related Fields, 182(1):551–613, 2022.
366"
REFERENCES,0.8271604938271605,"Laetitia Della Maestra and Marc Hoffmann. The lan property for mckean-vlasov models in a
367"
REFERENCES,0.8289241622574955,"mean-ﬁeld regime. Stochastic Processes and their Applications, 155:109–146, 2023. ISSN 0304-
368"
REFERENCES,0.8306878306878307,"4149. doi: https://doi.org/10.1016/j.spa.2022.10.002. URL https://www.sciencedirect.com/
369"
REFERENCES,0.8324514991181657,"science/article/pii/S0304414922002113.
370"
REFERENCES,0.8342151675485009,"Kai Du, Yifan Jiang, and Jinfeng Li. Empirical approximation to invariant measures for mckean–
371"
REFERENCES,0.8359788359788359,"vlasov processes: mean-ﬁeld interaction vs self-interaction. arXiv preprint arXiv:2112.14112,
372"
REFERENCES,0.8377425044091711,"2021.
373"
REFERENCES,0.8395061728395061,"Zachary Feinstein and Andreas Søjmark. Dynamic default contagion in heterogeneous interbank
374"
REFERENCES,0.8412698412698413,"systems. SIAM Journal on Financial Mathematics, 12(4):SC83–SC97, 2021.
375"
REFERENCES,0.8430335097001763,"Tilmann Gneiting and Adrian E Raftery.
Strictly proper scoring rules, prediction, and esti-
376"
REFERENCES,0.8447971781305115,"mation.
Journal of the American Statistical Association, 102(477):359–378, 2007.
doi:
377"
REFERENCES,0.8465608465608465,"10.1198/016214506000001437. URL https://doi.org/10.1198/016214506000001437.
378"
REFERENCES,0.8483245149911817,"Susana N Gomes, Andrew M Stuart, and Marie-Therese Wolfram. Parameter estimation for macro-
379"
REFERENCES,0.8500881834215167,"scopic pedestrian dynamics models from microscopic data. SIAM Journal on Applied Mathematics,
380"
REFERENCES,0.8518518518518519,"79(4):1475–1500, 2019.
381"
REFERENCES,0.8536155202821869,"Will Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, and David Duvenaud. Scalable reversible
382"
REFERENCES,0.855379188712522,"generative models with free-form continuous dynamics. In International Conference on Learning
383"
REFERENCES,0.8571428571428571,"Representations, 2019. URL https://openreview.net/forum?id=rJxgknCcK7.
384"
REFERENCES,0.8589065255731922,"Marianne Grognot and Katja M Taute. A multiscale 3d chemotaxis assay reveals bacterial navigation
385"
REFERENCES,0.8606701940035273,"mechanisms. Communications biology, 4(1):1–8, 2021.
386"
REFERENCES,0.8624338624338624,"Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville.
387"
REFERENCES,0.8641975308641975,"Improved training of wasserstein gans. Advances in neural information processing systems, 30,
388"
REFERENCES,0.8659611992945326,"2017.
389"
REFERENCES,0.8677248677248677,"Ben Hambly, Sean Ledger, and Andreas Søjmark. A mckean–vlasov equation with positive feedback
390"
REFERENCES,0.8694885361552028,"and blow-ups. The Annals of Applied Probability, 29(4):2338–2373, 2019.
391"
REFERENCES,0.8712522045855379,"Jiequn Han, Ruimeng Hu, and Jihao Long. Learning high-dimensional mckean-vlasov forward-
392"
REFERENCES,0.873015873015873,"backward stochastic differential equations with general distribution dependence. arXiv preprint
393"
REFERENCES,0.8747795414462081,"arXiv:2204.11924, 2022.
394"
REFERENCES,0.8765432098765432,"Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4(2):
395"
REFERENCES,0.8783068783068783,"251–257, 1991.
396"
REFERENCES,0.8800705467372134,"Kaitong Hu, Zhenjie Ren, David Šiška, and Łukasz Szpruch. Mean-ﬁeld langevin dynamics and
397"
REFERENCES,0.8818342151675485,"energy landscape of neural networks. In Annales de l’Institut Henri Poincaré, Probabilités et
398"
REFERENCES,0.8835978835978836,"Statistiques, volume 57, pages 2043–2065. Institut Henri Poincaré, 2021.
399"
REFERENCES,0.8853615520282186,"Chin-Wei Huang, Jae Hyun Lim, and Aaron C Courville. A variational perspective on diffusion-based
400"
REFERENCES,0.8871252204585538,"generative models and score matching. Advances in Neural Information Processing Systems, 34:
401"
REFERENCES,0.8888888888888888,"22863–22876, 2021.
402"
REFERENCES,0.890652557319224,"Benjamin Jourdain and Julien Reygner. Capital distribution and portfolio performance in the mean-
403"
REFERENCES,0.892416225749559,"ﬁeld atlas model. Annals of Finance, 11(2):151–198, 2015.
404"
REFERENCES,0.8941798941798942,"Evelyn F. Keller and Lee A. Segel. Model for chemotaxis. Journal of Theoretical Biology, 30(2):
405"
REFERENCES,0.8959435626102292,"225–234, 1971.
406"
REFERENCES,0.8977072310405644,"Diederik P Kingma and Max Welling.
Auto-encoding variational bayes.
arXiv preprint
407"
REFERENCES,0.8994708994708994,"arXiv:1312.6114, 2013.
408"
REFERENCES,0.9012345679012346,"Hugo Lavenant, Stephen Zhang, Young-Heon Kim, and Geoffrey Schiebinger. Towards a mathemati-
409"
REFERENCES,0.9029982363315696,"cal theory of trajectory inference. arXiv preprint arXiv:2102.09204, 2021.
410"
REFERENCES,0.9047619047619048,"Thomas M. Liggett. Stochastic models of interacting systems. The Annals of Probability, 25(1):1 –
411"
REFERENCES,0.9065255731922398,"29, 1997. doi: 10.1214/aop/1024404276. URL https://doi.org/10.1214/aop/1024404276.
412"
REFERENCES,0.908289241622575,"Sylvie Méléard. Asymptotic behaviour of some interacting particle systems; mckean-vlasov and
413"
REFERENCES,0.91005291005291,"boltzmann models. Probabilistic models for nonlinear partial differential equations, pages 42–95,
414"
REFERENCES,0.9118165784832452,"1996.
415"
REFERENCES,0.9135802469135802,"Daniel A. Messenger and David M. Bortz. Learning mean-ﬁeld equations from particle data using
416"
REFERENCES,0.9153439153439153,"wsindy. Physica D: Nonlinear Phenomena, 439:133406, 2022. ISSN 0167-2789. doi: https:
417"
REFERENCES,0.9171075837742504,"//doi.org/10.1016/j.physd.2022.133406. URL https://www.sciencedirect.com/science/
418"
REFERENCES,0.9188712522045855,"article/pii/S0167278922001543.
419"
REFERENCES,0.9206349206349206,"Stéphane Mischler, Cristóbal Quininao, and Jonathan Touboul. On a kinetic ﬁtzhugh–nagumo model
420"
REFERENCES,0.9223985890652557,"of neuronal network. Communications in mathematical physics, 342(3):1001–1042, 2016.
421"
REFERENCES,0.9241622574955908,"Sergey Nadtochiy and Mykhaylo Shkolnikov. Particle systems with singular interaction through
422"
REFERENCES,0.9259259259259259,"hitting times: application in systemic risk modeling. The Annals of Applied Probability, 2019.
423"
REFERENCES,0.927689594356261,"Phuong Thi Mai Nguyen, Yoshikatsu Hayashi, Murilo Da Silva Baptista, and Toshiyuki Kondo.
424"
REFERENCES,0.9294532627865961,"Collective almost synchronization-based model to extract and predict features of EEG signals. Sci-
425"
REFERENCES,0.9312169312169312,"entiﬁc Reports, 10(1):16342, 2020. URL https://doi.org/10.1038/s41598-020-73346-z.
426"
REFERENCES,0.9329805996472663,"Bernt Karsten Øksendal and Agnes Sulem. Applied stochastic control of jump diffusions, volume
427"
REFERENCES,0.9347442680776014,"498. Springer, 2007.
428"
REFERENCES,0.9365079365079365,"George Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive ﬂow for density
429"
REFERENCES,0.9382716049382716,"estimation. Advances in neural information processing systems, 30, 2017.
430"
REFERENCES,0.9400352733686067,"Grigorios A Pavliotis and Andrea Zanoni. A method of moments estimator for interacting particle
431"
REFERENCES,0.9417989417989417,"systems and their mean ﬁeld limit. arXiv preprint arXiv:2212.00403, 2022.
432"
REFERENCES,0.9435626102292769,"Huyên Pham and Xavier Warin. Mean-ﬁeld neural networks: learning mappings on wasserstein
433"
REFERENCES,0.9453262786596119,"space. arXiv preprint arXiv:2210.15179, 2022.
434"
REFERENCES,0.9470899470899471,"Lars Ruthotto, Stanley J Osher, Wuchen Li, Levon Nurbekyan, and Samy Wu Fung. A machine
435"
REFERENCES,0.9488536155202821,"learning framework for solving high-dimensional mean ﬁeld game and mean ﬁeld control problems.
436"
REFERENCES,0.9506172839506173,"Proceedings of the National Academy of Sciences, 117(17):9183–9193, 2020.
437"
REFERENCES,0.9523809523809523,"David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. Deepar: Probabilistic
438"
REFERENCES,0.9541446208112875,"forecasting with autoregressive recurrent networks. International Journal of Forecasting, 36(3):
439"
REFERENCES,0.9559082892416225,"1181–1191, 2020.
440"
REFERENCES,0.9576719576719577,"Filippo Santambrogio. {Euclidean, metric, and Wasserstein} gradient ﬂows: an overview. Bulletin of
441"
REFERENCES,0.9594356261022927,"Mathematical Sciences, 7(1):87–154, 2017.
442"
REFERENCES,0.9611992945326279,"Louis Sharrock, Nikolas Kantas, Panos Parpas, and Grigorios A Pavliotis. Parameter estimation for
443"
REFERENCES,0.9629629629629629,"the mckean-vlasov stochastic differential equation. arXiv preprint arXiv:2106.13751, 2021.
444"
REFERENCES,0.9647266313932981,"Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben
445"
REFERENCES,0.9664902998236331,"Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint
446"
REFERENCES,0.9682539682539683,"arXiv:2011.13456, 2020.
447"
REFERENCES,0.9700176366843033,"Bernard Sonnenschein and Lutz Schimansky-Geier. Approximate solution to the stochastic kuramoto
448"
REFERENCES,0.9717813051146384,"model. Physical Review E, 88(5), nov 2013.
449"
REFERENCES,0.9735449735449735,"Lukasz Szpruch, Shuren Tan, and Alvin Tse. Iterative multilevel particle approximation for mckean–
450"
REFERENCES,0.9753086419753086,"vlasov sdes. The Annals of Applied Probability, 29(4):2230–2265, 2019.
451"
REFERENCES,0.9770723104056437,"Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. Csdi: Conditional score-based diffu-
452"
REFERENCES,0.9788359788359788,"sion models for probabilistic time series imputation. Advances in Neural Information Processing
453"
REFERENCES,0.9805996472663139,"Systems, 34:24804–24816, 2021.
454"
REFERENCES,0.982363315696649,"Milica Tomaševi´c. A new mckean–vlasov stochastic interpretation of the parabolic-parabolic keller–
455"
REFERENCES,0.9841269841269841,"segel model: The two-dimensional case. The Annals of Applied Probability, 31(1):432–459,
456"
REFERENCES,0.9858906525573192,"2021.
457"
REFERENCES,0.9876543209876543,"Rentian Yao, Xiaohui Chen, and Yun Yang. Mean-ﬁeld nonparametric estimation of interacting
458"
REFERENCES,0.9894179894179894,"particle systems. In Po-Ling Loh and Maxim Raginsky, editors, Proceedings of Thirty Fifth
459"
REFERENCES,0.9911816578483245,"Conference on Learning Theory, volume 178 of Proceedings of Machine Learning Research, pages
460"
REFERENCES,0.9929453262786596,"2242–2275. PMLR, 02–05 Jul 2022. URL https://proceedings.mlr.press/v178/yao22a.
461"
REFERENCES,0.9947089947089947,"html.
462"
REFERENCES,0.9964726631393298,"Xiao Lei Zhang, Henri Begleiter, Bernice Porjesz, Wenyu Wang, and Ann Litke. Event related
463"
REFERENCES,0.9982363315696648,"potentials during object recognition tasks. Brain Research Bulletin, 38(6):531–538, 1995.
464"
