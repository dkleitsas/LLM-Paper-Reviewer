Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002336448598130841,"Brain encoding models aim to predict brain voxel-wise responses to stimuli images,
1"
ABSTRACT,0.004672897196261682,"replicating brain signals captured by neuroimaging techniques. There is a large
2"
ABSTRACT,0.007009345794392523,"volume of publicly available data, but training a comprehensive brain encoding
3"
ABSTRACT,0.009345794392523364,"model is challenging. The main difficulties stem from a) diversity within individual
4"
ABSTRACT,0.011682242990654205,"brain, with functional heterogeneous brain regions; b) diversity of brains from
5"
ABSTRACT,0.014018691588785047,"different subjects, due to genetic and developmental differences; c) diversity of
6"
ABSTRACT,0.016355140186915886,"imaging modalities and processing pipelines. We use this diversity to our advantage
7"
ABSTRACT,0.018691588785046728,"by introducing the All-for-One training recipe, which divides the challenging one-
8"
ABSTRACT,0.02102803738317757,"big-model problem into multiple small models, with the small models aggregating
9"
ABSTRACT,0.02336448598130841,"the knowledge while preserving the distinction between the different functional
10"
ABSTRACT,0.02570093457943925,"regions. Agnostic of the training recipe, we use biological knowledge of the brain,
11"
ABSTRACT,0.028037383177570093,"specifically retinotopy, to introduce inductive bias to learn a 3D brain-to-image
12"
ABSTRACT,0.030373831775700934,"mapping that ensures a) each neuron knows which image regions and semantic
13"
ABSTRACT,0.03271028037383177,"levels to gather information, and b) no neurons are left behind in the model.
14"
ABSTRACT,0.035046728971962614,"We pre-trained a brain encoding model using over one million data points from five
15"
ABSTRACT,0.037383177570093455,"public datasets spanning three imaging modalities. To the best of our knowledge,
16"
ABSTRACT,0.0397196261682243,"this is the most comprehensive brain encoding model to the date. We demonstrate
17"
ABSTRACT,0.04205607476635514,"the effectiveness of the pre-trained model as a drop-in replacement for commonly
18"
ABSTRACT,0.04439252336448598,"used vision backbone models. Furthermore, we demonstrate the application of the
19"
ABSTRACT,0.04672897196261682,"model to brain decoding. Code and the model checkpoint will be made available.
20"
ABSTRACT,0.04906542056074766,"0.36
0.39
0.42
0.45
0.48
In-Distribution Dataset Pearson's R 0.17 0.18 0.19 0.20 0.21 0.22"
HOLDOUT DATASETS,0.0514018691588785,"2 Holdout Datasets
Pearson's R AFO"
HOLDOUT DATASETS,0.053738317757009345,"CLIP-RN50x4
DiNOv2-ViTB"
HOLDOUT DATASETS,0.056074766355140186,IM-RN50
HOLDOUT DATASETS,0.05841121495327103,IM-ViTB
HOLDOUT DATASETS,0.06074766355140187,"-0.2
0.0
0.2
0.4
0.6
0.8 0.18 0.20 0.22"
HOLDOUT DATASETS,0.0630841121495327,"2 Holdout Datasets 
Pearson's R"
HOLDOUT DATASETS,0.06542056074766354,"0.36
0.39
0.42
0.45
0.48
In-Distribution Dataset Pearson's R -0.3"
HOLDOUT DATASETS,0.06775700934579439,"0.0
0.3
0.6
0.9"
HOLDOUT DATASETS,0.07009345794392523,"Figure 1: All-for-One recipe pre-trained backbone model evaluated by linear probing brain encoding.
All models remain frozen, the dimension of latent image features are reduced using PCA to a
consistent size. Subsequently, a linear regression is conducted for each voxel. The in-distribution
dataset comprises one subject from NSD, the holdout datasets consist of two subjects from BOLD5000
and ThingsfMRI1. Violin plot show distribution of score over voxels."
HOLDOUT DATASETS,0.07242990654205607,AFO Backbone
HOLDOUT DATASETS,0.07476635514018691,DiNOv2
HOLDOUT DATASETS,0.07710280373831775,layer2
HOLDOUT DATASETS,0.0794392523364486,layer5
HOLDOUT DATASETS,0.08177570093457943,layer8
HOLDOUT DATASETS,0.08411214953271028,layer11
HOLDOUT DATASETS,0.08644859813084112,ConvBlocks B1 B2 B3 B4 x y z u1 u2 x y z w2
HOLDOUT DATASETS,0.08878504672897196,softmax
HOLDOUT DATASETS,0.0911214953271028,TopyNeck w1 w3 w4 w1 w2 w3 w4 x x x x +
HOLDOUT DATASETS,0.09345794392523364,RetinaMapper
HOLDOUT DATASETS,0.09579439252336448,LayerSelector
HOLDOUT DATASETS,0.09813084112149532,"one
voxel"
HOLDOUT DATASETS,0.10046728971962617,RetinaMap
HOLDOUT DATASETS,0.102803738317757,fMRI EEG MEG
HOLDOUT DATASETS,0.10514018691588785,RetinaGrid tanh
HOLDOUT DATASETS,0.10747663551401869,"1
2
3
4
5
6
7
8
9
10 11 12 13 14 15 16 17 18 veROI"
HOLDOUT DATASETS,0.10981308411214953,NSD_01
HOLDOUT DATASETS,0.11214953271028037,NSD_02
HOLDOUT DATASETS,0.11448598130841121,NSD_03
HOLDOUT DATASETS,0.11682242990654206,NSD_04
HOLDOUT DATASETS,0.1191588785046729,NSD_05
HOLDOUT DATASETS,0.12149532710280374,NSD_06
HOLDOUT DATASETS,0.12383177570093458,NSD_07
HOLDOUT DATASETS,0.1261682242990654,NSD_08 ALG HCP EEG MEG
HOLDOUT DATASETS,0.12850467289719625,"0
9000"
HOLDOUT DATASETS,0.1308411214953271,"Voxel Counts
NSD_01
veROI"
HOLDOUT DATASETS,0.13317757009345793,"1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18"
HOLDOUT DATASETS,0.13551401869158877,"Figure 2: The proposed brain encoding model consists of three main components: the backbone, the
TopyNeck, and the linear regression head. The backbone is trainable convolution blocks attached to a
frozen DiNOv2-ViT-B model. TopyNeck selects one-dimensional features for each voxel based on
its physical coordinates. TopyNeck composes of RetinaMapper that maps the voxel to a 2D image
grid (RetinaGrid), and LayerSelector that combine feature vectors obtained from backbone layers.
Each dot in RetinaMap is a voxel, and color corresponds to argmax of LayerSelector. Finally, a
no-weight-sharing linear regression is conducted for each voxel. Voxel-wise encoding ROI (veROI),
is a novel brain parcellation that unifies multi-modal subjects."
INTRODUCTION,0.1378504672897196,"1
Introduction
21"
INTRODUCTION,0.14018691588785046,"There is a growing body of research in neuroscience that utilizes brain encoding models. The model
22"
INTRODUCTION,0.1425233644859813,"predicts voxel-wise brain response to visual stimuli, and it can be depicted as a multi-task regression
23"
INTRODUCTION,0.14485981308411214,"problem where each voxel is a task. The brain encoding model serves as a computational counterpart
24"
INTRODUCTION,0.14719626168224298,"to biological brains Wen et al. (2018). The common practice for building brain encoding models
25"
INTRODUCTION,0.14953271028037382,"is to use pre-trained models from image classification Deng et al. (2009), text-to-image alignment
26"
INTRODUCTION,0.15186915887850466,"Radford et al. (2021), or self-supervised tasks Oquab et al. (2023). These pre-trained models may
27"
INTRODUCTION,0.1542056074766355,"excel at their benchmarked task; however, Schrimpf et al. (2018) show that the image-classification
28"
INTRODUCTION,0.15654205607476634,"benchmark score does not align with prediction performance in brain encoding.
29"
INTRODUCTION,0.1588785046728972,"Building a model from all data sources poses a significant challenge due to heterogeneity in data:
30"
INTRODUCTION,0.16121495327102803,"a) diversity in functional sub-modules within each brain, b) genetic and developmental differences
31"
INTRODUCTION,0.16355140186915887,"across subjects, c) inconsistent imaging techniques and pre-processing pipelines. The current best
32"
INTRODUCTION,0.1658878504672897,"practice is to build Region-of-Interest (ROI)1 models over subjects from the same dataset Cichy et al.
33"
INTRODUCTION,0.16822429906542055,"(2021) Willeke et al. (2022) Allen et al. (2022), where ROIs are predefined by well-studied anatomical
34"
INTRODUCTION,0.1705607476635514,"and functional properties of the brain voxels. However, the ROI-model approach lacks the potential
35"
INTRODUCTION,0.17289719626168223,"benefits for ROIs to aggregate knowledge and collaborate. This issue can be mitigated to some extent
36"
ROI REFERS TO BRAIN ATLAS PARCELLATIONS,0.17523364485981308,1ROI refers to brain atlas parcellations
ROI REFERS TO BRAIN ATLAS PARCELLATIONS,0.17757009345794392,"by adjusting the granularity of ROIs. This work proposes a multi-stage All-for-One (AFO) training
37"
ROI REFERS TO BRAIN ATLAS PARCELLATIONS,0.17990654205607476,"recipe that explicitly lets ROIs aggregate knowledge while keeping the main training objective less
38"
ROI REFERS TO BRAIN ATLAS PARCELLATIONS,0.1822429906542056,"challenging than training for one all-ROI model. Borrowing the idea of ‘Dark knowledge’ distillation
39"
ROI REFERS TO BRAIN ATLAS PARCELLATIONS,0.18457943925233644,"Hinton et al. (2015), we use denoising to ensure the aggregated knowledge is clean.
40"
ROI REFERS TO BRAIN ATLAS PARCELLATIONS,0.18691588785046728,"Biological domain knowledge of the brain, specifically retinotopy, can be explored to design a
41"
ROI REFERS TO BRAIN ATLAS PARCELLATIONS,0.18925233644859812,"better model Lurz et al. (2021). The retina cells are physically wired through the optic nerve to the
42"
ROI REFERS TO BRAIN ATLAS PARCELLATIONS,0.19158878504672897,"lateral geniculate nucleus, which connects to the visual cortex. Thus, visual cortex cells preserve the
43"
ROI REFERS TO BRAIN ATLAS PARCELLATIONS,0.1939252336448598,"topological structure of images projected to the retina. This study explicitly defines a RetinaMapper
44"
ROI REFERS TO BRAIN ATLAS PARCELLATIONS,0.19626168224299065,"function that replicates retinotopic mapping. An obvious solution is learning a forward mapping that
45"
ROI REFERS TO BRAIN ATLAS PARCELLATIONS,0.1985981308411215,"transforms 2D RetinaGrid into a neuron in a 3D brain location. However, such forward mapping
46"
ROI REFERS TO BRAIN ATLAS PARCELLATIONS,0.20093457943925233,"can not guarantee to be surjective: every 3D neuron location is the mapped from at least one 2D
47"
ROI REFERS TO BRAIN ATLAS PARCELLATIONS,0.20327102803738317,"RetinaGrid. Our solution is to model the RetinaMapper from the inverse perspective, mapping 3D
48"
ROI REFERS TO BRAIN ATLAS PARCELLATIONS,0.205607476635514,"neuron to 2D RetinaGrid. RetinaMapper is learned without ground-truth supervision, but still exhibits
49"
ROI REFERS TO BRAIN ATLAS PARCELLATIONS,0.20794392523364486,"retinotopic behavior, as shown in our results.
50"
ROI REFERS TO BRAIN ATLAS PARCELLATIONS,0.2102803738317757,"A well-reported phenomenon is that neuron voxels are mapped to shallow to deep layers of a feed-
51"
ROI REFERS TO BRAIN ATLAS PARCELLATIONS,0.21261682242990654,"forward neuron network Takagi and Nishimoto (2022). This motivates the common practice of
52"
ROI REFERS TO BRAIN ATLAS PARCELLATIONS,0.21495327102803738,"selecting the best layers for each voxel. But per-voxel hyper-parameter tuning is highly noisy and
53"
ROI REFERS TO BRAIN ATLAS PARCELLATIONS,0.21728971962616822,"prone to overfitting; previous studies overcome this by choosing the same layers for each ROI. In this
54"
ROI REFERS TO BRAIN ATLAS PARCELLATIONS,0.21962616822429906,"study, we propose a LayerSelector module that enforces spatial proximity, thus allowing a flexible
55"
ROI REFERS TO BRAIN ATLAS PARCELLATIONS,0.2219626168224299,"and robust selection of layers.
56"
RELATED WORK,0.22429906542056074,"2
Related work
57"
RELATED WORK,0.2266355140186916,"The field of computational neuroscience has been actively exploring the task of brain encoding,
58"
RELATED WORK,0.22897196261682243,"highlighting from Kay et al. (2008) Naselaris et al. (2011), surveyed by Wen et al. (2018). There
59"
RELATED WORK,0.23130841121495327,"are several initiatives and benchmarks: The brain-score Schrimpf et al. (2018) initiative compares
60"
RELATED WORK,0.2336448598130841,"frozen image backbone models using a PCA and linear regression pipeline. The PCA approach
61"
RELATED WORK,0.23598130841121495,"allows for a fair comparison of vision models with different latent dimensions. Additionally, Conwell
62"
RELATED WORK,0.2383177570093458,"et al. (2022) utilized a similar frozen PCA pipeline to benchmark various vision models on the NSD
63"
RELATED WORK,0.24065420560747663,"dataset. The Algonauts challenge Cichy et al. (2021) benchmarks end-to-end trained model without
64"
RELATED WORK,0.24299065420560748,"the constraint of frozen model and PCA dimension reduction. The Sensorium benchmark Willeke
65"
RELATED WORK,0.24532710280373832,"et al. (2022) worked on invasive mouse V1 imaging data. The Things initiative Hebart et al. (2023)
66"
RELATED WORK,0.24766355140186916,"provides fine-grid image captions which can be used for hypotheses testing. These datasets and
67"
RELATED WORK,0.25,"benchmarks cover a wide range of imaging modalities, and preprocessing and denoising pipelines
68"
RELATED WORK,0.2523364485981308,"Kay et al. (2013) Prince et al. (2022). The All-for-One training recipe aims to leverage all of these
69"
RELATED WORK,0.2546728971962617,"diverse data sources to pre-train a comprehensive brain encoding model.
70"
RELATED WORK,0.2570093457943925,"The neuroscience community has extensively applied brain encoding models to unravel the biological
71"
RELATED WORK,0.25934579439252337,"mechanisms underlying brain function. St-Yves et al. (2022) employed transfer learning techniques
72"
RELATED WORK,0.2616822429906542,"with brain encoding models to investigate the hierarchical organization of the brain. Franke et al.
73"
RELATED WORK,0.26401869158878505,"(2022) applied the model to study color coding in mouse neurons. The NeuroGen framework Gu et al.
74"
RELATED WORK,0.26635514018691586,"(2022) combined brain encoding models with image generation models, they utilize gradient-based
75"
RELATED WORK,0.26869158878504673,"methods to manipulate stimulus images. Bashivan et al. (2019) generated maximally excited images
76"
RELATED WORK,0.27102803738317754,"for populations of neurons and presented these images to subjects to validate the conclusions. On the
77"
RELATED WORK,0.2733644859813084,"other hand, there are fruitful studies of brain decoding2 without a brain encoding model Takagi and
78"
RELATED WORK,0.2757009345794392,"Nishimoto (2022) Gu et al. (2023) Lu et al. (2023) Gu et al. (2023). Their framework is to take a
79"
RELATED WORK,0.2780373831775701,"pre-trained text-conditioned image generation model Ho et al. (2020) Rombach et al. (2022), then
80"
RELATED WORK,0.2803738317757009,"train a mapping function that aligns brain patterns to the text-condition embeddings space. However,
81"
RELATED WORK,0.2827102803738318,"we argue that decoding without a pre-trained encoding model is less efficient: Firstly, this pipeline
82"
RELATED WORK,0.2850467289719626,"is tightly linked to the pre-trained image generation model. Also, this pipeline face challenges in
83"
RELATED WORK,0.28738317757009346,"effectively utilizing heterogeneous data from various imaging modalities. We argue that decoding
84"
RELATED WORK,0.2897196261682243,"with a frozen encoding model is more efficient as this approach is agnostic to the specific image
85"
RELATED WORK,0.29205607476635514,"generation model.
86"
RELATED WORK,0.29439252336448596,"Previous studies also explored incorporating retinotopy into the brain encoding model. Allen et al.
87"
RELATED WORK,0.2967289719626168,"(2022) fits Gabor filters of various sizes and locations for each voxel. Lurz et al. (2021) also employed
88"
RELATED WORK,0.29906542056074764,"2We use the term encoding for mapping from stimuli image to brain voxels, decoding for the reverse."
RELATED WORK,0.3014018691588785,"the RetinaMapper, but their work focuses on training with the same imaging modality and one single
89"
RELATED WORK,0.3037383177570093,"ROI. In contrast, our approach tries to model the whole visual brain with diverse data sources.
90"
METHOD,0.3060747663551402,"3
Method
91"
METHOD,0.308411214953271,"The voxel-wise encoding model (Fig 2) comprises three main components: Firstly, the backbone
92"
METHOD,0.3107476635514019,"processes the input image and extracts latent image features from its intermediate layers. Next, the
93"
METHOD,0.3130841121495327,"neck component compresses the feature vector for each voxel. Finally, the head applies a linear
94"
METHOD,0.31542056074766356,regression model to fit a prediction for each voxel. Let M l ∈RD× H k × W
METHOD,0.3177570093457944,"k be the feature map output
95"
METHOD,0.32009345794392524,"from the frozen backbone, where l is the layer index, k is the down-scale factor, we refer the H k × W k
96"
METHOD,0.32242990654205606,"grid as RetinaGrid. The brain encoding model can be formulated as learning a mapping function F
97"
METHOD,0.3247663551401869,"(Eq 1), where N depends on the imaging modality3. NMRI := (X × Y × Z) × 1, NEEG := C × T,
98"
METHOD,0.32710280373831774,"NMEG := (X × Y × Z) × T
99"
METHOD,0.3294392523364486,F : R(L×D)× H k × W
METHOD,0.3317757009345794,"k →RN
(1)"
TOPYNECK,0.3341121495327103,"3.1
TopyNeck
100"
TOPYNECK,0.3364485981308411,"RetinaMapper
The biological retinotopy process is mapping f : R
H k × W"
TOPYNECK,0.338785046728972,"k →RX×Y ×Z. Riti-
101"
TOPYNECK,0.3411214953271028,"naMapper aims to replicate this mapping. However, f can not guarantee to be surjective: every 3D
102"
TOPYNECK,0.34345794392523366,"neuron location is the mapped from at least one 2D RetinaGrid. Instead of the forward mapping f,
103"
TOPYNECK,0.34579439252336447,"we learn a reverse injective mapping f ′ : RX×Y ×Z →R
H k × W"
TOPYNECK,0.34813084112149534,"k and use tanh activation function to
104"
TOPYNECK,0.35046728971962615,"guarantee the output 2D coordinates lies within the RetinaGrid. The RetinaMapper is formulated as
105"
TOPYNECK,0.352803738317757,"u = tanh(MLP(PE(p)))
(2)"
TOPYNECK,0.35514018691588783,"where p ∈RN×3 is the voxel’s spatial coordinate, PE is sinusoidal positional encoding function,
106"
TOPYNECK,0.3574766355140187,"u ∈RN×2 is coordinates in the RetinaGrid. During training, a small non-trainable variance σ
107"
TOPYNECK,0.3598130841121495,"is introduced u′ ∼N(u, σ). At inference time σ is set to 0. At each u′, linear interpolation is
108"
TOPYNECK,0.3621495327102804,"performed to obtain a 1-D feature vector ml ∈RN ×D for each layer l. Furthermore, Another 1-D
109"
TOPYNECK,0.3644859813084112,"feature vector ql = MLP(GlobalAvgPool(M l), GlobalMaxPool(M l)) is added to ml. Parameters
110"
TOPYNECK,0.36682242990654207,"of RetinaMapper is shared for all layers. Figure 2 and 4 show examples of such mapping. The color
111"
TOPYNECK,0.3691588785046729,"dots in RetinaGrid indicate which 3D neuron layers it is from. The blank area indicates image regions
112"
TOPYNECK,0.37149532710280375,"that are unused for prediction.
113"
TOPYNECK,0.37383177570093457,"LayerSelector
Early visual to downstream regions have growing receptive field sizes and neurons’
114"
TOPYNECK,0.37616822429906543,"latent representation of the stimuli image grows abstract. This motivates matching voxels to layers in
115"
TOPYNECK,0.37850467289719625,"feed-forward neuron networks. But selecting the best or top layers for each voxel is suspected to be
116"
TOPYNECK,0.3808411214953271,"overfitting. LayerSelector enforce spatial proximity formulated as
117"
TOPYNECK,0.38317757009345793,"η = softmax(MLP(PE(p)))
(3)
where η ∈RN×L. The 1-D feature vectors sampled from various layers at RetinaGrid is reduced as
118"
TOPYNECK,0.3855140186915888,"m∗
i = P"
TOPYNECK,0.3878504672897196,"L ηl
iml
i. Regularization loss lent = P"
TOPYNECK,0.3901869158878505,"L ηl log ηl is applied to prevent converging to a local
119"
TOPYNECK,0.3925233644859813,"minimum that only selects one single layer.
120"
ALL-FOR-ONE TRAINING RECIPE,0.39485981308411217,"3.2
All-for-One training recipe
121"
ALL-FOR-ONE TRAINING RECIPE,0.397196261682243,"Dividing neuron voxels into ROIs loses ROIs’ potential to aggregate knowledge and collaborate.
122"
ALL-FOR-ONE TRAINING RECIPE,0.39953271028037385,"Mixing can also negatively affect individual voxel performance, making learning more challenging.
123"
ALL-FOR-ONE TRAINING RECIPE,0.40186915887850466,"The AFO recipe aims to gather the benefits from both dividing and mixing. Multiple stages models
124"
ALL-FOR-ONE TRAINING RECIPE,0.40420560747663553,"are trained (Figure 3): In stage one, each ROI model is trained separately. In stage two, each ROI
125"
ALL-FOR-ONE TRAINING RECIPE,0.40654205607476634,"model is trained to distill the dark knowledge Hinton et al. (2015) from all other ROIs, but the
126"
ALL-FOR-ONE TRAINING RECIPE,0.4088785046728972,"ground truth loss is only applied on the target ROI, other ROIs are helpers, and their parameters were
127"
ALL-FOR-ONE TRAINING RECIPE,0.411214953271028,"discarded after training. Model checkpointing and early stopping are conditioned only on the target
128"
ALL-FOR-ONE TRAINING RECIPE,0.4135514018691589,"ROI. In stage three, the final model is trained with all ROIs as outputs, with dark knowledge and
129"
ALL-FOR-ONE TRAINING RECIPE,0.4158878504672897,"ground truth loss. The final product is one comprehensive all-ROI model.
130"
ALL-FOR-ONE TRAINING RECIPE,0.4182242990654206,3We use a unified term voxel to refer to a single smallest element in N.
ALL-FOR-ONE TRAINING RECIPE,0.4205607476635514,"GT: Grount Truth
DK: Dark Knowledge"
ALL-FOR-ONE TRAINING RECIPE,0.42289719626168226,Stage1
ALL-FOR-ONE TRAINING RECIPE,0.4252336448598131,"Model1
ROI1"
ALL-FOR-ONE TRAINING RECIPE,0.42757009345794394,Modeln
ALL-FOR-ONE TRAINING RECIPE,0.42990654205607476,Stage2
ALL-FOR-ONE TRAINING RECIPE,0.4322429906542056,Model1
ALL-FOR-ONE TRAINING RECIPE,0.43457943925233644,"GT
loss"
ALL-FOR-ONE TRAINING RECIPE,0.4369158878504673,"GT
loss"
ALL-FOR-ONE TRAINING RECIPE,0.4392523364485981,"Helper
ROI loss"
ALL-FOR-ONE TRAINING RECIPE,0.441588785046729,"DK
loss"
ALL-FOR-ONE TRAINING RECIPE,0.4439252336448598,"GT
loss"
ALL-FOR-ONE TRAINING RECIPE,0.4462616822429907,Modeln
ALL-FOR-ONE TRAINING RECIPE,0.4485981308411215,discarded after training
ALL-FOR-ONE TRAINING RECIPE,0.45093457943925236,Stage3 Model ROI1
ALL-FOR-ONE TRAINING RECIPE,0.4532710280373832,"DK
loss"
ALL-FOR-ONE TRAINING RECIPE,0.45560747663551404,"GT
loss ROIn"
ALL-FOR-ONE TRAINING RECIPE,0.45794392523364486,"DK
loss"
ALL-FOR-ONE TRAINING RECIPE,0.4602803738317757,"GT
loss
ROIn DK ROI1"
ALL-FOR-ONE TRAINING RECIPE,0.46261682242990654,"Figure 3: All-for-One training recipe involves training multiple stage of models using dark knowledge
distillation. In Stage1, a separate model is trained for each ROI. In Stage2, each model is an all-ROI
model that leverages the dark knowledge from all other models as helpers, the parameters of these
helper models are discarded after training. In Stage3, a single all-ROI model is trained."
ALL-FOR-ONE TRAINING RECIPE,0.4649532710280374,"Table 1: Brain encoding datasets. The term Datapoints refers to the number of image stimulus
presentations, including repeated presentation of the same image."
ALL-FOR-ONE TRAINING RECIPE,0.4672897196261682,"Training Datasets
Holdout Datasets
NSD
HCP
MOVIE
Algonauts
2021
Things
MEG1
Things
EEG2
BOLD
5000
Things
fMRI1"
ALL-FOR-ONE TRAINING RECIPE,0.4696261682242991,"Datapoints
240K
441K
30K
88K
640K
20K
24K
Subjects
8
184
10
4
10
4
3
Voxels
315K
29K
13K
60K
17K
9K
19K"
ALL-FOR-ONE TRAINING RECIPE,0.4719626168224299,"Modality
7T fMRI
7T fMRI
3T fMRI
MEG
EEG
3T fMRI
3T fMRI"
VOXEL-WISE ENCODING ROI,0.4742990654205608,"3.3
Voxel-wise encoding ROI
131"
VOXEL-WISE ENCODING ROI,0.4766355140186916,"We need a unified ROI parcellation that is defined for all subjects from various imaging modalities. To
132"
VOXEL-WISE ENCODING ROI,0.47897196261682246,"generate such a unified ROI, we utilize the final linear regression weight, which is extracted from an
133"
VOXEL-WISE ENCODING ROI,0.48130841121495327,"average of 10 all-ROI models. We start by performing Euclidean distance k-means clustering on the
134"
VOXEL-WISE ENCODING ROI,0.48364485981308414,"weights to reduce the dimension of voxel counts. Subsequently, Ward’s method applies hierarchical
135"
VOXEL-WISE ENCODING ROI,0.48598130841121495,"clustering to find the cluster centroids. This hierarchical clustering results in a dendrogram. We cut
136"
VOXEL-WISE ENCODING ROI,0.4883177570093458,"the dendrogram at a hand-picked threshold to identify the veROIs. By adjusting this threshold, we
137"
VOXEL-WISE ENCODING ROI,0.49065420560747663,"can control the granularity of the veROIs.
138"
EXPERIMENTS,0.4929906542056075,"4
Experiments
139"
DATASETS,0.4953271028037383,"4.1
Datasets
140"
DATASETS,0.4976635514018692,"We utilize 7 publicly available datasets for our experiments (Table 1). Details are provided in Allen
141"
DATASETS,0.5,"et al. (2022) Van Essen et al. (2012) Cichy et al. (2021) Hebart et al. (2023) Gifford et al. (2022)
142"
DATASETS,0.5023364485981309,"Chang et al. (2019). We use only voxels from the visual brain. Each dataset was divided into training,
143"
DATASETS,0.5046728971962616,"validation, and test sets with a ratio around 90 : 6 : 4. For the Things datasets, we use repeatedly
144"
DATASETS,0.5070093457943925,"represented images as the test set. All the experiment results are reported from the test set unless
145"
DATASETS,0.5093457943925234,"specified. The HCP video was split into chunks of 20 seconds to ensure no data leak, and a time
146"
DATASETS,0.5116822429906542,"delay of 4 seconds between video frames and fMRI frames was applied Khosla et al. (2021), blank
147"
DATASETS,0.514018691588785,"resting-state segments are not discarded. For video stimulus, we extracted frames at a rate of one
148"
DATASETS,0.5163551401869159,"frame per second. We only use one frame for the ALG dataset.
149"
DATASETS,0.5186915887850467,"Notably, except for the NSD dataset, all subjects from other datasets viewed the same set of images.
150"
DATASETS,0.5210280373831776,"As a compromise for computation intensity, we concatenated the voxels from ALG EEG MEG
151"
DATASETS,0.5233644859813084,"subjects into each single large brain, voxel’s spatial coordinates are placed in an evenly spaced grid.
152"
DATASETS,0.5257009345794392,"For the HCP dataset, a group average was performed due to the large number of subjects and the
153"
DATASETS,0.5280373831775701,"lower SNR in each individual subject. All datasets have spatial coordinates for voxels except the
154"
DATASETS,0.530373831775701,"EEG dataset, EEG voxel’s spatial coordinates are generated from dummy sequential numbers.
155"
TOPYNECK PROBING,0.5327102803738317,"4.2
TopyNeck probing
156"
TOPYNECK PROBING,0.5350467289719626,"RetinaMapper
In Figure 4, for NSD subjects, early visual voxels were mapped to span most of the
157"
TOPYNECK PROBING,0.5373831775700935,"RetinaGrid, while downstream-region voxels remained concentrated in the center. The ablation study
158"
TOPYNECK PROBING,0.5397196261682243,"presented in Table 2 further demonstrates the outstanding importance of the RetinaMapper for early
159"
TOPYNECK PROBING,0.5420560747663551,"visual voxels in NSD subjects. This alignment with retinotopy design motivation. However, for other
160"
TOPYNECK PROBING,0.544392523364486,"low SNR datasets, no clear retinotopic mapping was observed, suggesting that the RetinaMapper
161"
TOPYNECK PROBING,0.5467289719626168,"may not be necessary in such cases, and a constant mapping to the center could be sufficient.
162"
TOPYNECK PROBING,0.5490654205607477,"LayerSelector
In Figure 5, for subject NSD_01, a smooth transition from shallow to deep layers
163"
TOPYNECK PROBING,0.5514018691588785,"was observed. This alignment with the design motivation. Ablation study in Table 2 also indicates
164"
TOPYNECK PROBING,0.5537383177570093,"significant improvement for NSD subjects compared to un-weighted averaging layers or selecting a
165"
TOPYNECK PROBING,0.5560747663551402,"single layer. However, for low SNR datasets, the trend was to select only the last layer (Figure 4),
166"
TOPYNECK PROBING,0.5584112149532711,"suggesting that the LayerSelector module may not be necessary in such cases.
167 x y z x' y'"
TOPYNECK PROBING,0.5607476635514018,RetinaMapper
TOPYNECK PROBING,0.5630841121495327,RetinaMap
TOPYNECK PROBING,0.5654205607476636,RetinaGrid
TOPYNECK PROBING,0.5677570093457944,"Virtual
MapBack"
TOPYNECK PROBING,0.5700934579439252,"NSD_01
NSD_02
NSD_03
NSD_04"
TOPYNECK PROBING,0.572429906542056,"NSD_05
NSD_06
NSD_07
NSD_08"
TOPYNECK PROBING,0.5747663551401869,"ALG
HCP
EEG
MEG"
TOPYNECK PROBING,0.5771028037383178,"layer2
layer5
layer8
layer11"
TOPYNECK PROBING,0.5794392523364486,"Figure 4: RetinaMapper maps voxels to RetinaGrid. Each dot on RetinaMap is a voxel colored by
argmax of the LayerSelector, colors indicate selection of layers."
TOPYNECK PROBING,0.5817757009345794,"layer2
layer5
layer8
layer11"
TOPYNECK PROBING,0.5841121495327103,"Figure 5: LayerSelector re-weights backbone layers, outputs for all layers sum to 1. Results are
showed for subject NSD_01."
ALL-FOR-ONE RECIPE RESULTS,0.5864485981308412,"4.3
All-for-One recipe results
168"
ALL-FOR-ONE RECIPE RESULTS,0.5887850467289719,"In Table 3, a significant performance gap between the S1 and S2 models indicates the effectiveness
169"
ALL-FOR-ONE RECIPE RESULTS,0.5911214953271028,"of aggregating knowledge among ROIs. We also study a randROI that has the exact same number of
170"
ALL-FOR-ONE RECIPE RESULTS,0.5934579439252337,"ROIs and number of voxels inside each ROI. S1 and S2 gap is not observed in the randROI approach,
171"
ALL-FOR-ONE RECIPE RESULTS,0.5957943925233645,"as randROI already covers all types of voxels in every ROI. Furthermore, the model trained with
172"
ALL-FOR-ONE RECIPE RESULTS,0.5981308411214953,"ground truth (NoDK) as helpers shows little to no improvement over the S1 model. This suggests
173"
ALL-FOR-ONE RECIPE RESULTS,0.6004672897196262,"that the quality of the helper ROI is critical for the AFO recipe, as involving noisy helpers makes the
174"
ALL-FOR-ONE RECIPE RESULTS,0.602803738317757,"training process unnecessarily challenging. In this context, dark knowledge plays a crucial role as
175"
ALL-FOR-ONE RECIPE RESULTS,0.6051401869158879,"denoising. However, solely dark knowledge distillation doesn’t have a great impact as can be inferred
176"
ALL-FOR-ONE RECIPE RESULTS,0.6074766355140186,"from the small gap between randROI S1 and S2 models.
177"
ALL-FOR-ONE RECIPE RESULTS,0.6098130841121495,"Table 2: TopyNeck ablation study. The reported numbers are the average Pearson correlation
coefficient across all voxels. Results are averaged over three runs. FrozenRM maps every voxel to the
center, FrozenLS outputs uniform weight for each layer. NoRegLS selects a single layer."
ALL-FOR-ONE RECIPE RESULTS,0.6121495327102804,"Subject
NSD_01
NSD_08
EEG"
ALL-FOR-ONE RECIPE RESULTS,0.6144859813084113,"ROI
all
early
late
mid
all
early
late
mid
all"
ALL-FOR-ONE RECIPE RESULTS,0.616822429906542,"FullTopyNeck
0.462
0.515
0.435
0.470
0.291
0.304
0.285
0.292
0.228
FrozenRM
0.441
0.476
0.422
0.452
0.274
0.261
0.280
0.272
0.226
w/o GlobalPool
0.457
0.513
0.428
0.467
0.293
0.303
0.289
0.295
0.230
FrozenLS
0.451
0.512
0.419
0.466
0.280
0.300
0.270
0.279
0.224
NoRegLS
0.447
0.505
0.417
0.464
0.287
0.299
0.282
0.284
0.229"
ALL-FOR-ONE RECIPE RESULTS,0.6191588785046729,"Table 3: All-for-One training recipe ablation study. The reported numbers are the average Pearson
correlation coefficient across all voxels, NSD(NC) is the median of noise-normalized score. NaiveMix
train one all-ROI model. NoDK use ground truth as helpers. randROI and veROI has the exact same
size. S2+1 indicates one extra iteration of stage2. b is number of parameters in the convolution
blocks, n is number of voxels, d is feature dimension, r is number of ROIs."
ALL-FOR-ONE RECIPE RESULTS,0.6214953271028038,"Method
# Params
Dataset(s)
NSD
EEG
MEG
HCP
ALG
ALL
NSD
(NC)"
ALL-FOR-ONE RECIPE RESULTS,0.6238317757009346,"NaiveMix
b + nd
0.422
0.212
0.180
0.340
0.256
0.367
0.560
veROIS1
rb + nd
0.425
0.212
0.194
0.346
0.265
0.371
0.567
veROIS2
rb + nd
0.433
0.222
0.209
0.365
0.266
0.380
0.588
veROIS3
b + nd
0.435
0.225
0.210
0.366
0.267
0.382
0.593
veROIS2+1
rb + nd
0.432
0.226
0.211
0.362
0.264
0.380
0.586
NoDK
rb + nd
0.426
0.216
0.186
0.349
0.256
0.371
0.569
randROIS1
rb + nd
0.431
0.216
0.207
0.343
0.258
0.377
0.584
randROIS2
rb + nd
0.432
0.220
0.207
0.348
0.259
0.378
0.586"
VEROI RESULTS,0.6261682242990654,"4.4
veROI results
178"
VEROI RESULTS,0.6285046728971962,"Figure 6 shows veROI on cortex across all NSD subjects, early visual areas is centered around
179"
VEROI RESULTS,0.6308411214953271,"veROI_5 (blue) and downstream areas centered around veROI_9 (green), voxels that drop out from
180"
VEROI RESULTS,0.633177570093458,"the field of view in early visual areas are centered around veROI_16 (red). The score for each veROI
181"
VEROI RESULTS,0.6355140186915887,"for subject NSD_01 can be found in Figure 8, where veROI_12 onward is mainly for the low SNR
182"
VEROI RESULTS,0.6378504672897196,"voxels. From the heatmap in Figure 2 we can also observe that veROI_12 onward is mainly HCP,
183"
VEROI RESULTS,0.6401869158878505,"EEG, and MEG subjects.
184"
BRAIN DECODING,0.6425233644859814,"4.5
Brain decoding
185"
BRAIN DECODING,0.6448598130841121,"Methods
In this study, brain decoding refers to the task of ranking and retrieving candidate images
186"
BRAIN DECODING,0.647196261682243,"from a candidate set, retrieved images are to match a given brain response pattern. The decoding
187"
BRAIN DECODING,0.6495327102803738,"pipeline involves forwarding each candidate image through the brain encoding model and measuring
188"
BRAIN DECODING,0.6518691588785047,"Pearson’s correlation coefficient between the model’s prediction and the ground truth.
189"
BRAIN DECODING,0.6542056074766355,"Results
The experiments are conducted on 500 validation images as candidate images. As a
190"
BRAIN DECODING,0.6565420560747663,"qualitative analysis, Figure 7 and Figure 9 demonstrate that when conditioning on the early visual
191"
BRAIN DECODING,0.6588785046728972,"area or veROI_5, texture and orientation are more preserved in the decoded images. Conversely,
192"
BRAIN DECODING,0.6612149532710281,"when conditioning on downstream ROIs, semantic concepts are more preserved. Additionally, Figure
193"
BRAIN DECODING,0.6635514018691588,"8 shows that image retrieval achieves high accuracy when conditioned on early ROIs. Quantitative
194"
BRAIN DECODING,0.6658878504672897,"exploration of the functional roles of ROIs is beyond the scope of this study. Future work may involve
195"
BRAIN DECODING,0.6682242990654206,"1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18"
BRAIN DECODING,0.6705607476635514,"NSD_01
NSD_02
NSD_03
NSD_04"
BRAIN DECODING,0.6728971962616822,"NSD_05
NSD_06
NSD_07
NSD_08"
BRAIN DECODING,0.6752336448598131,"Figure 6: veROI cluster voxels into ROIs by hierarchical clustering. ROIs are identified by cutting
the linkage at a manually selected threshold value(dashed line). The feature used for clustering is the
linear regression weight associated with each voxel."
BRAIN DECODING,0.677570093457944,"investigating semantic concepts with image generation models. Furthermore, the gradient of the
196"
BRAIN DECODING,0.6799065420560748,"encoding model can be utilized to facilitate image generation and manipulation.
197"
BRAIN DECODING,0.6822429906542056,"GT
Top1
Top2
Top3
Top4
Top5"
BRAIN DECODING,0.6845794392523364,(a) Match all voxels early
BRAIN DECODING,0.6869158878504673,"GT
Top1
Top2
Top3
Top4
Top5"
BRAIN DECODING,0.6892523364485982,"mid
late"
BRAIN DECODING,0.6915887850467289,(b) Match one ROI
BRAIN DECODING,0.6939252336448598,"Figure 7: Image retrieval to match brain response pattern. Images are ranked by Pearson’s r of
captured biological brain pattern and model output. Results are for subject NSD_01."
BRAIN DECODING,0.6962616822429907,"all
early mid
late
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
0.0 0.2 0.4 0.6 0.8 1.0"
BRAIN DECODING,0.6985981308411215,"Image Retrieval 
Accuracy ROI"
BRAIN DECODING,0.7009345794392523,"top1
top5
r 0.0 0.2 0.4 0.6 0.8 1.0"
BRAIN DECODING,0.7032710280373832,Performance Score
BRAIN DECODING,0.705607476635514,Pearson's R
BRAIN DECODING,0.7079439252336449,"Figure 8: Performance of image retrieval(blue and orange) conditioned on ROIs. The integer numbers
are the indices of the veROIs. Performance scores of brain encoding(green) are the average value of
the voxels within each ROI, standard error is in black. Results are for subject NSD_01."
IMPLEMENTATION DETAILS,0.7102803738317757,"4.6
Implementation details
198"
IMPLEMENTATION DETAILS,0.7126168224299065,"We use smooth L1 loss with β = 0.01, regulirazation loss lent is scaled down by λ = 0.00003.
199"
IMPLEMENTATION DETAILS,0.7149532710280374,"AdaBelief optimizer Zhuang et al. (2020) is employed with lr = 0.003, batchsize = 128,
200"
IMPLEMENTATION DETAILS,0.7172897196261683,"weight_decay = 0.0001, (β1, β2) = (0.9, 0.999). Notably, we mix subjects in one mini-batch,
201"
IMPLEMENTATION DETAILS,0.719626168224299,"and the effective batch size for each subject is less than the total. Due to memory constrain, we
202"
IMPLEMENTATION DETAILS,0.7219626168224299,"GT
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18"
IMPLEMENTATION DETAILS,0.7242990654205608,"Figure 9: Image retrieval conditioned on veROIs. The numerical numbers are the indices of veROIs.
The top four images are placed from the top left to the bottom right."
IMPLEMENTATION DETAILS,0.7266355140186916,"randomly sample up to 8000 voxels for each training datapoint, there is 436,715 voxels totaling
203"
IMPLEMENTATION DETAILS,0.7289719626168224,"all subjects. Early stopping is configured with patience = 20 epochs, we define one epoch as
204"
IMPLEMENTATION DETAILS,0.7313084112149533,"10% of the total training data. Greedy Model Soup Wortsman et al. (2022) is applied at the top 10
205"
IMPLEMENTATION DETAILS,0.7336448598130841,"validation checkpoints. Backbone is kept frozen except LayerNorm running statistics is updated.
206"
IMPLEMENTATION DETAILS,0.735981308411215,"Input resolution is 224 × 224 and the feature from backbone layers are all of the size 768 × 16 × 16.
207"
IMPLEMENTATION DETAILS,0.7383177570093458,"The attached trainable convolution block is three zero-padded 5x5 convolutions with skip connection
208"
IMPLEMENTATION DETAILS,0.7406542056074766,"and LayerNorm, C = 768. The last convolution layer reduces the dimension to D = 256. We trained
209"
IMPLEMENTATION DETAILS,0.7429906542056075,"all models on single NVIDIA RTX 2080 Ti 12GB GPUs at a reduced clock speed of 1140Mhz,
210"
IMPLEMENTATION DETAILS,0.7453271028037384,"single-subject all-ROI models consume half to 1 GPU hour, all-subject single-ROI models consume
211"
IMPLEMENTATION DETAILS,0.7476635514018691,"3 to 5 GPU hours, all-subject all-ROI models consume 10 GPU hours. The complete AFO recipe
212"
IMPLEMENTATION DETAILS,0.75,"total around 300 GPU hours. Models are trained Pytorch Lightning Falcon (2019) mixed precision
213"
IMPLEMENTATION DETAILS,0.7523364485981309,"FP16. To boost training speed, MLPs in RetinaMapper and LayerSelector are pre-optimized by a
214"
IMPLEMENTATION DETAILS,0.7546728971962616,"single-subject all-ROI model, they are loaded and kept frozen in the AFO recipe, this gives 2 times
215"
IMPLEMENTATION DETAILS,0.7570093457943925,"faster convergence speed.
216"
CONCLUSION AND LIMITATIONS,0.7593457943925234,"5
Conclusion and Limitations
217"
CONCLUSION AND LIMITATIONS,0.7616822429906542,"We proposed the AFO recipe alongside veROI to address the issue of heterogeneity in publicly
218"
CONCLUSION AND LIMITATIONS,0.764018691588785,"available datasets. To the best of our knowledge, our pre-trained model constructed with over 1
219"
CONCLUSION AND LIMITATIONS,0.7663551401869159,"million data points is the most comprehensive brain encoding model to date. The model shows
220"
CONCLUSION AND LIMITATIONS,0.7686915887850467,"superior performance when transferred to small hold-out datasets. As demonstrated by our brain
221"
CONCLUSION AND LIMITATIONS,0.7710280373831776,"decoding experiments, the pre-trained model could facilitate further neuroscience research.
222"
CONCLUSION AND LIMITATIONS,0.7733644859813084,"We also designed TopyNeck inspired by retinotopy, which showed retinotopic behavior despite having
223"
CONCLUSION AND LIMITATIONS,0.7757009345794392,"no ground truth supervision for the retinotopic mapping function. However, the retinotopic behavior
224"
CONCLUSION AND LIMITATIONS,0.7780373831775701,"diminishes when the target dataset SNR is low, e.g. EEG, MEG. This suggests a simple alternative
225"
CONCLUSION AND LIMITATIONS,0.780373831775701,"approach is sufficient in such a case.
226"
REFERENCES,0.7827102803738317,"References
227"
REFERENCES,0.7850467289719626,"Allen, E. J., St-Yves, G., Wu, Y., Breedlove, J. L., Prince, J. S., Dowdle, L. T., Nau, M., Caron,
228"
REFERENCES,0.7873831775700935,"B., Pestilli, F., Charest, I., Hutchinson, J. B., Naselaris, T., and Kay, K. (2022). A massive 7T
229"
REFERENCES,0.7897196261682243,"fMRI dataset to bridge cognitive neuroscience and artificial intelligence. Nature Neuroscience,
230"
REFERENCES,0.7920560747663551,"25(1):116–126. Number: 1 Publisher: Nature Publishing Group.
231"
REFERENCES,0.794392523364486,"Bashivan, P., Kar, K., and DiCarlo, J. J. (2019). Neural population control via deep image synthesis.
232"
REFERENCES,0.7967289719626168,"Science, 364(6439):eaav9436. Publisher: American Association for the Advancement of Science.
233"
REFERENCES,0.7990654205607477,"Chang, N., Pyles, J. A., Marcus, A., Gupta, A., Tarr, M. J., and Aminoff, E. M. (2019). BOLD5000,
234"
REFERENCES,0.8014018691588785,"a public fMRI dataset while viewing 5000 visual images. Scientific Data, 6(1):49. Number: 1
235"
REFERENCES,0.8037383177570093,"Publisher: Nature Publishing Group.
236"
REFERENCES,0.8060747663551402,"Cichy, R. M., Dwivedi, K., Lahner, B., Lascelles, A., Iamshchinina, P., Graumann, M., Andonian, A.,
237"
REFERENCES,0.8084112149532711,"Murty, N. A. R., Kay, K., Roig, G., and Oliva, A. (2021). The Algonauts Project 2021 Challenge:
238"
REFERENCES,0.8107476635514018,"How the Human Brain Makes Sense of a World in Motion. arXiv:2104.13714 [cs, q-bio].
239"
REFERENCES,0.8130841121495327,"Conwell, C., Prince, J. S., Alvarez, G. A., and Konkle, T. (2022). Large-Scale Benchmarking
240"
REFERENCES,0.8154205607476636,"of Diverse Artificial Vision Models in Prediction of 7T Human Neuroimaging Data. Pages:
241"
REFERENCES,0.8177570093457944,"2022.03.28.485868 Section: New Results.
242"
REFERENCES,0.8200934579439252,"Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). ImageNet: A large-
243"
REFERENCES,0.822429906542056,"scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern
244"
REFERENCES,0.8247663551401869,"Recognition, pages 248–255. ISSN: 1063-6919.
245"
REFERENCES,0.8271028037383178,"Falcon, W. A. (2019). Pytorch lightning. GitHub, 3.
246"
REFERENCES,0.8294392523364486,"Franke, K., Willeke, K. F., Ponder, K., Galdamez, M., Zhou, N., Muhammad, T., Patel, S., Froudarakis,
247"
REFERENCES,0.8317757009345794,"E., Reimer, J., Sinz, F. H., and Tolias, A. S. (2022). State-dependent pupil dilation rapidly shifts
248"
REFERENCES,0.8341121495327103,"visual feature selectivity. Nature, 610(7930):128–134. Number: 7930 Publisher: Nature Publishing
249"
REFERENCES,0.8364485981308412,"Group.
250"
REFERENCES,0.8387850467289719,"Gifford, A. T., Dwivedi, K., Roig, G., and Cichy, R. M. (2022). A large and rich EEG dataset for
251"
REFERENCES,0.8411214953271028,"modeling human visual object recognition. NeuroImage, 264:119754.
252"
REFERENCES,0.8434579439252337,"Gu, Z., Jamison, K., Kuceyeski, A., and Sabuncu, M. (2023). Decoding natural image stimuli from
253"
REFERENCES,0.8457943925233645,"fMRI data with a surface-based convolutional network. arXiv:2212.02409 [cs, q-bio].
254"
REFERENCES,0.8481308411214953,"Gu, Z., Jamison, K. W., Khosla, M., Allen, E. J., Wu, Y., St-Yves, G., Naselaris, T., Kay, K., Sabuncu,
255"
REFERENCES,0.8504672897196262,"M. R., and Kuceyeski, A. (2022). NeuroGen: Activation optimized image synthesis for discovery
256"
REFERENCES,0.852803738317757,"neuroscience. NeuroImage, 247:118812.
257"
REFERENCES,0.8551401869158879,"Hebart, M. N., Contier, O., Teichmann, L., Rockter, A. H., Zheng, C. Y., Kidder, A., Corriveau, A.,
258"
REFERENCES,0.8574766355140186,"Vaziri-Pashkam, M., and Baker, C. I. (2023). THINGS-data, a multimodal collection of large-scale
259"
REFERENCES,0.8598130841121495,"datasets for investigating object representations in human brain and behavior. eLife, 12:e82580.
260"
REFERENCES,0.8621495327102804,"Publisher: eLife Sciences Publications, Ltd.
261"
REFERENCES,0.8644859813084113,"Hinton, G., Vinyals, O., and Dean, J. (2015). Distilling the Knowledge in a Neural Network.
262"
REFERENCES,0.866822429906542,"arXiv:1503.02531 [cs, stat].
263"
REFERENCES,0.8691588785046729,"Ho, J., Jain, A., and Abbeel, P. (2020). Denoising Diffusion Probabilistic Models. arXiv:2006.11239
264"
REFERENCES,0.8714953271028038,"[cs, stat].
265"
REFERENCES,0.8738317757009346,"Kay, K. N., Naselaris, T., Prenger, R. J., and Gallant, J. L. (2008). Identifying natural images from
266"
REFERENCES,0.8761682242990654,"human brain activity. Nature, 452(7185):352–355.
267"
REFERENCES,0.8785046728971962,"Kay, K. N., Rokem, A., Winawer, J., Dougherty, R. F., and Wandell, B. A. (2013). GLMdenoise: a
268"
REFERENCES,0.8808411214953271,"fast, automated technique for denoising task-based fMRI data. Frontiers in Neuroscience, 7.
269"
REFERENCES,0.883177570093458,"Khosla, M., Ngo, G. H., Jamison, K., Kuceyeski, A., and Sabuncu, M. R. (2021). Cortical re-
270"
REFERENCES,0.8855140186915887,"sponse to naturalistic stimuli is largely predictable with deep neural networks. Science Advances,
271"
REFERENCES,0.8878504672897196,"7(22):eabe7547.
272"
REFERENCES,0.8901869158878505,"Lu, Y., Du, C., Wang, D., and He, H. (2023). MindDiffuser: Controlled Image Reconstruction from
273"
REFERENCES,0.8925233644859814,"Human Brain Activity with Semantic and Structural Diffusion. arXiv:2303.14139 [cs].
274"
REFERENCES,0.8948598130841121,"Lurz, K.-K., Bashiri, M., Willeke, K., Jagadish, A., Wang, E., Walker, E. Y., Cadena, S. A., Muham-
275"
REFERENCES,0.897196261682243,"mad, T., Cobos, E., Tolias, A. S., Ecker, A. S., and Sinz, F. H. (2021). Generalization in data-driven
276"
REFERENCES,0.8995327102803738,"models of primary visual cortex.
277"
REFERENCES,0.9018691588785047,"Naselaris, T., Kay, K. N., Nishimoto, S., and Gallant, J. L. (2011). Encoding and decoding in fMRI.
278"
REFERENCES,0.9042056074766355,"NeuroImage, 56(2):400–410.
279"
REFERENCES,0.9065420560747663,"Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza,
280"
REFERENCES,0.9088785046728972,"D., Massa, F., El-Nouby, A., Assran, M., Ballas, N., Galuba, W., Howes, R., Huang, P.-Y., Li,
281"
REFERENCES,0.9112149532710281,"S.-W., Misra, I., Rabbat, M., Sharma, V., Synnaeve, G., Xu, H., Jegou, H., Mairal, J., Labatut,
282"
REFERENCES,0.9135514018691588,"P., Joulin, A., and Bojanowski, P. (2023). DINOv2: Learning Robust Visual Features without
283"
REFERENCES,0.9158878504672897,"Supervision. arXiv:2304.07193 [cs].
284"
REFERENCES,0.9182242990654206,"Prince, J. S., Charest, I., Kurzawski, J. W., Pyles, J. A., Tarr, M. J., and Kay, K. N. (2022). Improving
285"
REFERENCES,0.9205607476635514,"the accuracy of single-trial fMRI response estimates using GLMsingle. eLife, 11:e77599.
286"
REFERENCES,0.9228971962616822,"Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A.,
287"
REFERENCES,0.9252336448598131,"Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. (2021). Learning Transferable Visual Models
288"
REFERENCES,0.927570093457944,"From Natural Language Supervision. arXiv:2103.00020 [cs].
289"
REFERENCES,0.9299065420560748,"Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. (2022). High-Resolution Image
290"
REFERENCES,0.9322429906542056,"Synthesis with Latent Diffusion Models. arXiv:2112.10752 [cs].
291"
REFERENCES,0.9345794392523364,"Schrimpf, M., Kubilius, J., Hong, H., Majaj, N. J., Rajalingham, R., Issa, E. B., Kar, K., Bashivan, P.,
292"
REFERENCES,0.9369158878504673,"Prescott-Roy, J., Schmidt, K., Yamins, D. L. K., and DiCarlo, J. J. (2018). Brain-Score: Which
293"
REFERENCES,0.9392523364485982,"Artificial Neural Network for Object Recognition is most Brain-Like? Pages: 407007 Section:
294"
REFERENCES,0.9415887850467289,"New Results.
295"
REFERENCES,0.9439252336448598,"St-Yves, G., Allen, E. J., Wu, Y., Kay, K., and Naselaris, T. (2022). Brain-optimized neural networks
296"
REFERENCES,0.9462616822429907,"learn non-hierarchical models of representation in human visual cortex. Pages: 2022.01.21.477293
297"
REFERENCES,0.9485981308411215,"Section: New Results.
298"
REFERENCES,0.9509345794392523,"Takagi, Y. and Nishimoto, S. (2022). High-resolution image reconstruction with latent diffusion
299"
REFERENCES,0.9532710280373832,"models from human brain activity. Pages: 2022.11.18.517004 Section: New Results.
300"
REFERENCES,0.955607476635514,"Van Essen, D. C., Ugurbil, K., Auerbach, E., Barch, D., Behrens, T. E. J., Bucholz, R., Chang,
301"
REFERENCES,0.9579439252336449,"A., Chen, L., Corbetta, M., Curtiss, S. W., Della Penna, S., Feinberg, D., Glasser, M. F., Harel,
302"
REFERENCES,0.9602803738317757,"N., Heath, A. C., Larson-Prior, L., Marcus, D., Michalareas, G., Moeller, S., Oostenveld, R.,
303"
REFERENCES,0.9626168224299065,"Petersen, S. E., Prior, F., Schlaggar, B. L., Smith, S. M., Snyder, A. Z., Xu, J., Yacoub, E., and WU-
304"
REFERENCES,0.9649532710280374,"Minn HCP Consortium (2012). The Human Connectome Project: a data acquisition perspective.
305"
REFERENCES,0.9672897196261683,"NeuroImage, 62(4):2222–2231.
306"
REFERENCES,0.969626168224299,"Wen, H., Shi, J., Zhang, Y., Lu, K.-H., Cao, J., and Liu, Z. (2018). Neural Encoding and Decoding
307"
REFERENCES,0.9719626168224299,"with Deep Learning for Dynamic Natural Vision. Cerebral Cortex, 28(12):4136–4160.
308"
REFERENCES,0.9742990654205608,"Willeke, K. F., Fahey, P. G., Bashiri, M., Pede, L., Burg, M. F., Blessing, C., Cadena, S. A., Ding, Z.,
309"
REFERENCES,0.9766355140186916,"Lurz, K.-K., Ponder, K., Muhammad, T., Patel, S. S., Ecker, A. S., Tolias, A. S., and Sinz, F. H.
310"
REFERENCES,0.9789719626168224,"(2022). The Sensorium competition on predicting large-scale mouse primary visual cortex activity.
311"
REFERENCES,0.9813084112149533,"arXiv:2206.08666 [cs, q-bio].
312"
REFERENCES,0.9836448598130841,"Wortsman, M., Ilharco, G., Gadre, S. Y., Roelofs, R., Gontijo-Lopes, R., Morcos, A. S., Namkoong,
313"
REFERENCES,0.985981308411215,"H., Farhadi, A., Carmon, Y., Kornblith, S., and Schmidt, L. (2022). Model soups: averaging
314"
REFERENCES,0.9883177570093458,"weights of multiple fine-tuned models improves accuracy without increasing inference time.
315"
REFERENCES,0.9906542056074766,"arXiv:2203.05482 [cs].
316"
REFERENCES,0.9929906542056075,"Zhuang, J., Tang, T., Ding, Y., Tatikonda, S., Dvornek, N., Papademetris, X., and Duncan, J. S. (2020).
317"
REFERENCES,0.9953271028037384,"AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients. arXiv:2010.07468
318"
REFERENCES,0.9976635514018691,"[cs, stat].
319"
