Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0009852216748768472,"Deep neural networks (DNNs) have shown great success in many machine learn-
ing tasks. Their training is challenging since the loss surface of the network ar-
chitecture is generally non-convex, or even non-smooth. How and under what as-
sumptions is guaranteed convergence to a global minimum possible? We propose
a reformulation of the minimization problem allowing for a new recursive algo-
rithmic framework. By using bounded style assumptions, we prove convergence
to an ε-(global) minimum using ˜O(1/ε3) gradient computations. Our theoretical
foundation motivates further study, implementation, and optimization of the new
algorithmic framework and further investigation of its non-standard bounded style
assumptions. This new direction broadens our understanding of why and under
what circumstances training of a DNN converges to a global minimum."
INTRODUCTION,0.0019704433497536944,"1
INTRODUCTION"
INTRODUCTION,0.002955665024630542,"In recent years, deep neural networks (DNNs) have shown a great success in many machine learn-
ing tasks. However, training these neural networks is challenging since the loss surface of network
architecture is generally non-convex, or even non-smooth. Thus, there have been a long-standing
question on how optimization algorithms may converge to a global minimum. Many previous work
have investigated Gradient Descent algorithm and its stochastic version for over-parameterized set-
ting (Arora et al., 2018; Soudry et al., 2018; Allen-Zhu et al., 2019; Du et al., 2019a; Zou & Gu,
2019). Although these works have shown promising convergence results under certain assumptions,
there is still a lack of new efﬁcient methods that can guarantee global convergence for machine
learning optimization. In this paper, we address this problem using a different perspective. Instead
of analyzing the traditional ﬁnite-sum formulation, we adopt a new composite formulation that ex-
actly depicts the structure of machine learning where a data set is used to learn a common classiﬁer."
INTRODUCTION,0.003940886699507389,"Representation. Let

(x(i), y(i))
	n
i=1 be a given training set with x(i) ∈Rm, y(i) ∈Rc, we
investigate the following novel representation for deep learning tasks:"
INTRODUCTION,0.0049261083743842365,"min
w∈Rd ("
INTRODUCTION,0.005911330049261084,"F(w) = 1 n n
X"
INTRODUCTION,0.006896551724137931,"i=1
φi(h(w; i)) ) ,
(1)"
INTRODUCTION,0.007881773399014778,"where h(·; i) : Rd →Rc, i ∈[n] = {1, . . . , n}, is the classiﬁer for each input data x(i); and
φi : Rc →R, i ∈[n], is the loss function corresponding to each output data y(i). Our composite for-
mulation (1) is a special case of the ﬁnite-sum problem minw∈Rd

F(w) = 1"
INTRODUCTION,0.008866995073891626,"n
Pn
i=1 f(w; i)
	
where
each individual function f(·; i) is a composition of the loss function φi and the classiﬁer h(·; i). This
problem covers various important applications in machine learning, including logistic regression and
neural networks. The most common approach for the ﬁnite-sum problem is using ﬁrst-order meth-
ods such as (stochastic) gradient algorithms and making assumptions on the component functions
f(·; i). As an alternative, we further investigate the structure of the loss function φi and narrow
our assumption on the classiﬁer h(·; i). For the purpose of this work, we ﬁrst consider convex and
Lipschitz-smooth loss functions while the classiﬁers can be non-convex. Using this representation,
we propose a new framework followed by two algorithms that guarantee global convergence for the
minimization problem."
INTRODUCTION,0.009852216748768473,"Algorithmic Framework. Representation (1) admits a new perspective. Our key insight is to (A)
deﬁne z(t)
i
= h(w(t); i), where t is an iteration count of the outer loop in our algorithmic framework."
INTRODUCTION,0.01083743842364532,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.011822660098522168,"Next (B), we want to approximate the change z(t+1)
i
−z(t)
i
in terms of a step size times the gradient"
INTRODUCTION,0.012807881773399015,"∇φi(z(t)
i ) = (∂φi(z)/∂za)a∈[c]

z=z(t)
i ,"
INTRODUCTION,0.013793103448275862,and (C) we approximate the change h(w(t+1); i) −h(w(t); i) in terms of the ﬁrst order derivative
INTRODUCTION,0.014778325123152709,"H(t)
i
= (∂ha(w; i)/∂wb)a∈[c],b∈[d]

w=w(t)."
INTRODUCTION,0.015763546798029555,"Finally, we combine (A), (B), and (C) to equate the approximations of z(t+1)
i
−z(t)
i
and
h(w(t+1); i) −h(w(t); i). This leads to a recurrence on w(t) of the form w(t+1) = w(t) −η(t)v(t),
where η(t) is a step size and which involves computing v(t) by solving a convex quadratic sub-
problem, see the details in Section 4. We explain two methods for approximating a solution for the
derived subproblem. We show how to approximate the subproblem by transforming it into a strongly
convex problem by adding a regularizer which can be solved in closed form. And we show how to
use Gradient Descent (GD) on the subproblem to ﬁnd an approximation v(t) of its solution."
INTRODUCTION,0.016748768472906402,"Convergence Analysis. Our analysis introduces non-standard bounded style assumptions. Intu-
itively, we assume that our convex and quadratic subproblem has a bounded solution. This allows
us to prove a total complexity of ˜O( 1"
INTRODUCTION,0.017733990147783252,"ε3 ) to ﬁnd an ε-(global) solution that satisﬁes F( ˆw) −F∗≤ε,
where F∗is the global minimizer of F. Our analysis applies to a wide range of applications in
machine learning: Our results hold for squared loss and softmax cross-entropy loss and applicable
for a range of activation functions in DNN as we only assume that the h(·; i) are twice continuously
differentiable and their Hessian matrices (second order derivatives) as well as their gradients (ﬁrst
order derivatives) are bounded."
INTRODUCTION,0.0187192118226601,Contributions and Outline. Our contributions in this paper can be summarized as follows.
INTRODUCTION,0.019704433497536946,"• We propose a new representation (1) for analyzing the machine learning minimization prob-
lem. Our formulation utilizes the structure of machine learning tasks where a training data
set of inputs and outputs is used to learn a common classiﬁer. Related work in Section 2
shows how (1) is different from the classical ﬁnite-sum problem.
• Based on the new representation we propose a novel algorithm framework. The algorith-
mic framework approximates a solution to a subproblem for which we show two distinct
approaches.
• For general DNNs and based on bounded style assumptions, we prove a total complexity
of ˜O( 1"
INTRODUCTION,0.020689655172413793,"ε3 ) to ﬁnd an ε-(global) solution that satisﬁes F( ˆw)−F∗≤ε, where F∗is the global
minimizer of F."
INTRODUCTION,0.02167487684729064,"We emphasize that our focus is on developing a new theoretical foundation and that a translation
to a practical implementation with empirical results is for future work. Our theoretical foundation
motivates further study, implementation, and optimization of the new algorithmic framework and
further investigation of its non-standard bounded style assumptions. This new direction broadens
our understanding of why and under what circumstances training of a DNN converges to a global
minimum."
INTRODUCTION,0.022660098522167486,"The rest of this paper is organized as follows. Section 2 discusses related work. Section 3 describes
our setting and deep learning representation. Section 4 explains our key insight and derives our
Framework 1. Section 5 presents our algorithms and their global convergence. All technical proofs
are deferred to the Appendix."
RELATED WORK,0.023645320197044337,"2
RELATED WORK"
RELATED WORK,0.024630541871921183,"Formulation for Machine Learning Problems. The ﬁnite-sum problem is one of the most im-
portant and fundamental problems in machine learning. Analyzing this model is the most popular
approach in the machine learning literature and it has been studied intensively throughout the years
(Bottou et al., 2018; Reddi et al., 2016; Duchi et al., 2011b). Our new formulation (1) is a spe-
cial case of the ﬁnite-sum problem, however, it is much more complicated than the previous model
since it involves the data index i both inside the classiﬁers h(·; i) and the loss functions φi. For a
comparison, previous works only consider a common loss function l(ˆy, y) for the predicted value"
RELATED WORK,0.02561576354679803,Under review as a conference paper at ICLR 2022
RELATED WORK,0.026600985221674877,"ˆy and output data y (Zou et al., 2018; Soudry et al., 2018). Our modiﬁed version of loss function
φi is a natural setting for machine learning. We note that when h(w; i) is the output produced by
a model, our goal is to match this output with the corresponding target y(i). For that reason, the
loss function for each output has a dependence on the output data y(i), and is denoted by φi. This
fact reﬂects the natural setting of machine learning where the outputs are designed to ﬁt different
targets, and the optimization process depends on both outer function φi and inner functions h(·; i).
This complication may potentially bring a challenge to theoretical analysis. However, with separate
loss functions, we believe this model will help to exploit better the structure of machine learning
problems and gain more insights on the neural network architecture."
RELATED WORK,0.027586206896551724,"Other related composite optimization models are also investigated thoroughly in (Lewis & Wright,
2016; Zhang & Xiao, 2019; Tran-Dinh et al., 2020). Our model is different from these works as
it does not have a common function wrapping outside the ﬁnite-sum term, as in (Lewis & Wright,
2016). Note that a broad class of variance reduction algorithms (e.g. SAG (Le Roux et al., 2012),
SAGA (Defazio et al., 2014), SVRG (Johnson & Zhang, 2013), SARAH (Nguyen et al., 2017)) is
designed speciﬁcally for the ﬁnite-sum formulation and is known to have certain beneﬁts over Gra-
dient Descent. In addition, the multilevel composite problem considered in (Zhang & Xiao, 2021)
also covers empirical risk minimization problem. However our formulation does not match their
work since our inner function h(w; i) is not an independent expectation over some data distribution,
but a speciﬁc function that depends on the current data."
RELATED WORK,0.02857142857142857,"Global Convergence for Neural Networks. A recent popular line of research is studying the dy-
namics of optimization methods on some speciﬁc neural network architectures. There are some early
works that show the global convergence of Gradient Descent (GD) for simple linear network and
two-layer network (Brutzkus et al., 2018; Soudry et al., 2018; Arora et al., 2019; Du et al., 2019b).
Some further works extend these results to deep learning architectures (Allen-Zhu et al., 2019; Du
et al., 2019a; Zou & Gu, 2019). These theoretical guarantees are generally proved for the case when
the last output layer is ﬁxed, which is not standard in practice. A recent work (Nguyen & Mondelli,
2020) prove the global convergence for GD when all layers are trained with some initial conditions.
However, these results are for neural networks without bias neurons and it is unclear how these anal-
yses can be extended to handle the bias terms of deep networks with different activations. Our novel
framework and algorithms do not exclude learning bias layers as in (Nguyen & Mondelli, 2020)."
RELATED WORK,0.029556650246305417,"Using a different algorithm, Brutzkus et al. (2018) investigate Stochastic Gradient Descent (SGD)
for two-layer networks in a restricted linearly separable data setting. This line of research continues
with the works from Allen-Zhu et al. (2019); Zou et al. (2018) and later with Zou & Gu (2019). They
justify the global convergence of SGD for deep neural networks for some probability depending on
the number of input data and the initialization process."
RELATED WORK,0.030541871921182268,"Over-Paramaterized Settings and other Assumptions for Machine Learning. Most of the mod-
ern learning architectures are over-parameterized, which means that the number of parameters are
very large and often far more than the number of input data. Some recent works prove the global
convergence of Gradient Descent when the number of neurons are extensively large, e.g. (Zou &
Gu, 2019) requires Ω(n8) neurons for every hidden layer, and (Nguyen & Mondelli, 2020) im-
proves this number to Ω(n3). If the initial point satisﬁes some special conditions, then they can
show a better dependence of Ω(n). In Allen-Zhu et al. (2019), the authors initialize the weights
using a random Gaussian distribution where the variance depends on the dimension of the problem.
In non-convex setting, they prove the convergence of SGD using the assumption that the dimension
depends inversely on the tolerance ϵ. We will discuss how these over-paramaterized settings might
be a necessary condition to develop our theory."
RELATED WORK,0.03152709359605911,"Other standard assumptions for machine learning include the bounded gradient assumption (Ne-
mirovski et al., 2009; Shalev-Shwartz et al., 2007; Reddi et al., 2016; Tran et al., 2021). It is also
common to assume all the iterations of an algorithm stays in a bounded domain (Duchi et al., 2011a;
Levy et al., 2018; G¨urb¨uzbalaban et al., 2019; Reddi et al., 2018; Vaswani et al., 2021). Since we
are analyzing a new composite formulation, it is understandable that our assumptions may also not
be standard. However, we believe that there is a strong connection between our assumptions and the
traditional setting of machine learning. We will discuss this point more clearly in Section 4."
RELATED WORK,0.03251231527093596,Under review as a conference paper at ICLR 2022
BACKGROUND,0.033497536945812804,"3
BACKGROUND"
BACKGROUND,0.034482758620689655,"In this section, we discuss our formulation and notations in detail. Although this paper focuses on
deep neural networks, our framework and theoretical analysis are general and applicable for other
learning architectures."
BACKGROUND,0.035467980295566505,"Deep Learning Representation. Let {(x(i), y(i))}n
i=1 be a training data set where x(i) ∈Rm is a
training input and y(i) ∈Rc is a training output. We consider a fully-connected neural network with
L layers, where the l-th layer, l ∈{0, 1, . . . , L}, has nl neurons. We represent layer 0-th and L-th
layer as input and output layers, respectively, that is, n0 = d and nL = c. For l ∈{1, . . . , L}, let
W (l) ∈Rnl−1×nl and b(l) ∈Rnl, where {(W (l), b(l))L
l=1} represent the parameters of the neural
network. A classiﬁer h(w; i) is formulated as"
BACKGROUND,0.03645320197044335,"h(w; i) = W (L)⊤σL−1(W (L−1)⊤σL−2(. . . σ1(W (1)⊤x(i) + b(1)) . . . ) + b(L−1)) + b(L),"
BACKGROUND,0.0374384236453202,"where w = vec({W (1), b(1), . . . , W (L), b(L)}) ∈Rd is the vectorized weight and {σl}L−1
l=1 are some
activation functions. The most common choices for machine learning are ReLU, sigmoid, hyperbolic
tangent and softplus. For j ∈[c], hj(·; i) : Rd →R denotes the component function of the output
h(·; i), for each data i ∈[n] respectively. Moreover, we deﬁne h∗
i = arg minz∈Rc φi(z), i ∈[n]."
BACKGROUND,0.03842364532019704,"Loss Functions.
The well-known loss functions in neural networks for solving classiﬁcation and
regression problems are softmax cross-entropy loss and square loss, respectively:"
BACKGROUND,0.03940886699507389,(Softmax) Cross-Entropy Loss: F(w) = 1
BACKGROUND,0.04039408866995074,"n
Pn
i=1 f(w; i) with"
BACKGROUND,0.041379310344827586,"f(w; i) = −y(i)⊤log(softmax(h(w; i))).
(2)"
BACKGROUND,0.042364532019704436,Squared Loss: F(w) = 1
BACKGROUND,0.04334975369458128,"n
Pn
i=1 f(w; i) with"
BACKGROUND,0.04433497536945813,f(w; i) = 1
BACKGROUND,0.04532019704433497,"2∥h(w; i) −y(i)∥2.
(3)"
BACKGROUND,0.04630541871921182,We provide some basic deﬁnitions in optimization theory to support our theory.
BACKGROUND,0.04729064039408867,"Deﬁnition 1 (L-smooth). Function φ : Rc →R is Lφ-smooth if there exists a constant Lφ > 0
such that, ∀x1, x2 ∈Rc,"
BACKGROUND,0.04827586206896552,"∥∇φ(x1) −∇φ(x2)∥≤Lφ∥x1 −x2∥.
(4)"
BACKGROUND,0.04926108374384237,"Deﬁnition 2 (Convex). Function φ : Rc →R is convex if ∀x1, x2 ∈Rc,"
BACKGROUND,0.05024630541871921,"φ(x1) −φ(x2) ≥⟨∇φ(x2), x1 −x2⟩.
(5)"
BACKGROUND,0.05123152709359606,The following corollary shows the properties of softmax cross-entropy loss (2) and squared loss (3).
BACKGROUND,0.052216748768472904,"Corollary 1. For softmax cross-entropy loss (2) and squared loss (3), there exist functions h(·; i) :
Rd →Rc and φi : Rc →R such that, for i ∈[n], φi(z) is convex and Lφ-smooth with Lφ = 1, and"
BACKGROUND,0.053201970443349754,"f(w; i) = φi(h(w; i)) = φi(z)

z=h(w;i).
(6)"
NEW ALGORITHM FRAMEWORK,0.054187192118226604,"4
NEW ALGORITHM FRAMEWORK"
KEY INSIGHT,0.05517241379310345,"4.1
KEY INSIGHT"
KEY INSIGHT,0.0561576354679803,"We assume f(w; i) = φi(h(w; i)) with φi convex and Lφ-smooth. Our goal is to utilize the con-
vexity of the outer function φi. In order to simplify notation, we write ∇zφi(h(w(t); i)) instead of
∇zφi(z)

z=h(w(t);i) and denote z(t)
i
= h(w(t); i). Starting from the current weight w(t), we would"
KEY INSIGHT,0.05714285714285714,like to ﬁnd the next point w(t+1) that satisﬁes the following approximation for all i ∈[n]:
KEY INSIGHT,0.05812807881773399,"h(w(t+1); i) = z(t+1)
i
≈z(t)
i
−α(t)
i ∇zφi(z(t)
i ) = h(w(t); i) −α(t)
i ∇zφi(h(w(t); i)).
(7)"
KEY INSIGHT,0.059113300492610835,Under review as a conference paper at ICLR 2022
KEY INSIGHT,0.060098522167487685,"We can see that this approximation is a “noisy” version of a gradient descent update for every
function φi, simultaneously for all i ∈[n]. In order to do this, we use the following update"
KEY INSIGHT,0.061083743842364535,"w(t+1) = w(t) −η(t)v(t),
(8)"
KEY INSIGHT,0.06206896551724138,"where η(t) > 0 is a learning rate and v(t) is a search direction that helps us approximate equation
(7). If the update term η(t)v(t) is small enough, and if h(·; i) has some nice smooth properties, then
from basic calculus we have the following approximation:"
KEY INSIGHT,0.06305418719211822,"h(w(t+1); i) = h(w(t) −η(t)v(t); i) ≈h(w(t); i) −H(t)
i
 
η(t)v(t)
,
(9)"
KEY INSIGHT,0.06403940886699508,"where H(t)
i
is a matrix in Rc×d with ﬁrst-order derivatives. Motivated by approximations (7) and
(9), we consider the following optimization problem:"
KEY INSIGHT,0.06502463054187192,"v(t)
∗
= arg min
v∈Rd
1
2
1
n n
X"
KEY INSIGHT,0.06600985221674877,"i=1
∥H(t)
i
 
η(t)v

−α(t)
i ∇zφi(h(w(t); i))∥2.
(10)"
KEY INSIGHT,0.06699507389162561,"Hence, by solving for the solution v(t)
∗
of problem (10) we are able to ﬁnd a search direction for the
key approximation (7). This yields our new algorithmic Framework 1, see below."
KEY INSIGHT,0.06798029556650247,Framework 1 New Algorithm Framework
KEY INSIGHT,0.06896551724137931,"Initialization: Choose an initial point w(0) ∈Rd;
for t = 0, 1, · · · , T −1 do"
KEY INSIGHT,0.06995073891625615,"Solve for an approximation v(t) of the solution v(t)
∗
of the problem in (10)"
KEY INSIGHT,0.07093596059113301,"v(t)
∗
= arg min
v∈Rd
1
2
1
n n
X"
KEY INSIGHT,0.07192118226600985,"i=1
∥η(t)H(t)
i v −α(t)
i ∇zφi(h(w(t); i))∥2"
KEY INSIGHT,0.0729064039408867,"Update w(t+1) = w(t) −η(t)v(t)
end for"
TECHNICAL ASSUMPTIONS,0.07389162561576355,"4.2
TECHNICAL ASSUMPTIONS"
TECHNICAL ASSUMPTIONS,0.0748768472906404,"Assumption 1. The loss function φi is convex and Lφ-smooth for i ∈[n]. Moreover, we assume
that it is lower bounded, i.e. infz∈Rc φi(z) > −∞for i ∈[n]."
TECHNICAL ASSUMPTIONS,0.07586206896551724,"We have shown the convexity and smoothness of squared loss and softmax cross-entropy loss in
Section 3. The bounded property of φi is required in any algorithm for the well-deﬁnedness of (1).
Now, in order to use the Taylor series approximation, we need the following assumption on the
neural network architecture h:"
TECHNICAL ASSUMPTIONS,0.07684729064039408,"Assumption 2. We assume that h(·; i) is twice continuously differentiable for all i ∈[n] (i.e. the
second-order partial derivatives of all scalars hj(·; i) are continuous for all j ∈[c] and i ∈[n]),
and that their Hessian matrices are bounded, that is, there exists a G > 0 such that for all w ∈Rd,
i ∈[n] and j ∈[c],"
TECHNICAL ASSUMPTIONS,0.07783251231527094,"∥Mi,j(w)∥= ∥Jw (∇whj(w; i))∥≤G,
(11)"
TECHNICAL ASSUMPTIONS,0.07881773399014778,where Jw denotes the Jacobian1.
TECHNICAL ASSUMPTIONS,0.07980295566502463,"Remark 1 (Relation to second-order methods). Although our analysis requires an assumption on
the Hessian matrices of h(w; i), our algorithms do not use any second order information or try to
approximate this information. Our theoretical analysis focused on the approximation of the clas-
siﬁer and the gradient information, therefore is not related to the second order type algorithms.
It is currently unclear how to apply second order methods into our problem, however, this is an
interesting research question to expand the scope of this work."
TECHNICAL ASSUMPTIONS,0.08078817733990148,"1For a continuously differentiable function g(w) : Rd →Rc we deﬁne the Jacobian Jw(g(w)) as the matrix
(∂ga(w)/∂wb)a∈[c],b∈[d]."
TECHNICAL ASSUMPTIONS,0.08177339901477833,Under review as a conference paper at ICLR 2022
TECHNICAL ASSUMPTIONS,0.08275862068965517,"Assumption 2 allows us to apply a Taylor approximation of each function hj(·; i) with which we
prove the following Lemma that bounds the error in equation (9):"
TECHNICAL ASSUMPTIONS,0.08374384236453201,"Lemma 1. Suppose that Assumption 2 holds for the classiﬁer h. Then for all i ∈[n] and 0 ≤t < T,"
TECHNICAL ASSUMPTIONS,0.08472906403940887,"h(w(t+1); i) = h(w(t) −η(t)v(t); i) = h(w(t); i) −η(t)H(t)
i v(t) + ϵ(t)
i ,
(12)"
TECHNICAL ASSUMPTIONS,0.08571428571428572,"where
H(t)
i
= Jw(h(w; i))|w=w(t) ∈Rc×d
(13)"
TECHNICAL ASSUMPTIONS,0.08669950738916256,"is deﬁned as the Jacobian matrix of h(w; i) at w(t) and entries ϵ(t)
i,j, j ∈[c], of vector ϵ(t)
i
satisfy"
TECHNICAL ASSUMPTIONS,0.08768472906403942,"|ϵ(t)
i,j| ≤1"
TECHNICAL ASSUMPTIONS,0.08866995073891626,"2(η(t))2∥v(t)∥2G.
(14)"
TECHNICAL ASSUMPTIONS,0.0896551724137931,"In order to approximate (7) combined with (9), that is, to make sure the right hand sides of (7) and
(9) are close to one another, we consider the optimization problem (10):"
TECHNICAL ASSUMPTIONS,0.09064039408866995,"v(t)
∗
= arg min
v∈Rd
1
2
1
n n
X"
TECHNICAL ASSUMPTIONS,0.0916256157635468,"i=1
∥η(t)H(t)
i v −α(t)
i ∇zφi(h(w(t); i))∥2."
TECHNICAL ASSUMPTIONS,0.09261083743842365,"The optimal value of problem (10) is equal to 0 if there exists a vector v(t)
∗
satisfying η(t)H(t)
i v(t)
∗
=
α(t)
i ∇zφi(h(w(t); i)) for every i ∈[n]. Since the solution v(t)
∗
is in Rd and ∇zφi(h(w(t); i)) is in
Rc, this condition is equivalent to a linear system with n · c constraints and d variables. In the over-
parameterized setting where dimension d is sufﬁciently large (d ≫n · c) and there are no identical
data, there exists almost surely a vector v(t)
∗
that interpolates all the training set, see the Appendix
for details."
TECHNICAL ASSUMPTIONS,0.09359605911330049,"Let us note that an approximation of v(t)
∗
serves as the search direction for Framework 1. For this
reason, the solution v(t)
∗
of problem (10) plays a similar role as a gradient in the search direction
of (stochastic) gradient descent method. It is standard to assume a bounded gradient in the ma-
chine learning literature (Nemirovski et al., 2009; Shalev-Shwartz et al., 2007; Reddi et al., 2016).
Motivated by these facts, we assume the following Assumption 3, which implies the existence of a
near-optimal bounded solution of (10):"
TECHNICAL ASSUMPTIONS,0.09458128078817735,"Assumption 3. We consider an over-parameterized setting where dimension d is sufﬁciently large
enough to interpolate all the data and the tolerance ε. We assume that there exists a bound V > 0
such that for ε > 0 and 0 ≤t < T as in Framework 1, there exists a vector ˆv(t)
∗ε with ∥ˆv(t)
∗ε ∥2 ≤V
so that"
TECHNICAL ASSUMPTIONS,0.09556650246305419,"1
2
1
n n
X"
TECHNICAL ASSUMPTIONS,0.09655172413793103,"i=1
∥η(t)H(t)
i
ˆv(t)
∗ε −α(t)
i ∇zφi(h(w(t); i))∥2 ≤ε2."
TECHNICAL ASSUMPTIONS,0.09753694581280788,"Our Assumption 3 requires a nice dependency on the tolerance ε for the gradient matrices H(t)
i
and
∇zφi(h(w(t); i)). We note that at the starting point t = 0, these matrices may depend on ε due to
the initialization process and the dependence of d on ε. This setting is similar to previous works,
e.g. Allen-Zhu et al. (2019)."
NEW ALGORITHMS AND CONVERGENCE RESULTS,0.09852216748768473,"5
NEW ALGORITHMS AND CONVERGENCE RESULTS"
APPROXIMATING THE SOLUTION USING REGULARIZER,0.09950738916256158,"5.1
APPROXIMATING THE SOLUTION USING REGULARIZER"
APPROXIMATING THE SOLUTION USING REGULARIZER,0.10049261083743842,"Since problem (10) is convex and quadratic, we consider the following regularized problem:"
APPROXIMATING THE SOLUTION USING REGULARIZER,0.10147783251231528,"min
v∈Rd ("
APPROXIMATING THE SOLUTION USING REGULARIZER,0.10246305418719212,"Ψ(v) = 1 2
1
n n
X"
APPROXIMATING THE SOLUTION USING REGULARIZER,0.10344827586206896,"i=1
∥η(t)H(t)
i v −α(t)
i ∇zφi(h(w(t); i))∥2 + ε2"
APPROXIMATING THE SOLUTION USING REGULARIZER,0.10443349753694581,"2 ∥v∥2
)"
APPROXIMATING THE SOLUTION USING REGULARIZER,0.10541871921182266,",
(15)"
APPROXIMATING THE SOLUTION USING REGULARIZER,0.10640394088669951,Under review as a conference paper at ICLR 2022
APPROXIMATING THE SOLUTION USING REGULARIZER,0.10738916256157635,"for some small ε > 0 and t ≥0. It is widely known that problem (15) is strongly convex, and has a
unique minimizer v(t)
∗reg. The global minimizer satisﬁes ∇vΨ(v(t)
∗reg) = 0. We have"
APPROXIMATING THE SOLUTION USING REGULARIZER,0.10837438423645321,"∇vΨ(v) = 1 n n
X"
APPROXIMATING THE SOLUTION USING REGULARIZER,0.10935960591133005,"i=1
[η(t)H(t)
i
⊤H(t)
i η(t)v −α(t)
i η(t)H(t)
i
⊤∇zφi(h(w(t); i))] + ε2 · v ="
N,0.1103448275862069,"1
n n
X"
N,0.11133004926108374,"i=1
η(t)H(t)
i
⊤H(t)
i η(t) + ε2I ! v −"
N,0.1123152709359606,"1
n n
X"
N,0.11330049261083744,"i=1
α(t)
i η(t)H(t)
i
⊤∇zφi(h(w(t); i)) ! ."
N,0.11428571428571428,"Therefore,"
N,0.11527093596059114,"v(t)
∗reg ="
N,0.11625615763546798,"1
n n
X"
N,0.11724137931034483,"i=1
η(t)H(t)
i
⊤H(t)
i η(t) + ε2I"
N,0.11822660098522167,"!−1  
1
n n
X"
N,0.11921182266009853,"i=1
α(t)
i η(t)H(t)
i
⊤∇zφi(h(w(t); i)) !"
N,0.12019704433497537,".
(16)"
N,0.12118226600985221,"If ε2 is small enough, then v(t)
∗reg is a close approximation of the solution v(t)
∗
for problem (10). Our
ﬁrst algorithm updates Framework 1 based on this approximation."
N,0.12216748768472907,Algorithm 1 Solve for the exact solution of the regularized problem
N,0.12315270935960591,"Initialization: Choose an initial point w(0) ∈Rd, tolerance ε > 0;
for t = 0, 1, · · · , T −1 do"
N,0.12413793103448276,"Update the search direction v(t) as the solution v(t)
∗reg of problem in (15):"
N,0.12512315270935961,"v(t) = v(t)
∗reg ="
N,0.12610837438423644,"1
n n
X"
N,0.1270935960591133,"i=1
η(t)H(t)
i
⊤H(t)
i η(t) + ε2I"
N,0.12807881773399016,"!−1  
1
n n
X"
N,0.129064039408867,"i=1
α(t)
i η(t)H(t)
i
⊤∇zφi(h(w(t); i)) !"
N,0.13004926108374384,"Update w(t+1) = w(t) −η(t)v(t)
end for"
N,0.1310344827586207,"The following Lemma shows the relation between the regularized solution v(t)
∗reg and the optimal
solution of the original convex problem ˆv(t)
∗ε .
Lemma 2. For given ε > 0, suppose that Assumption 3 holds for bound V > 0. Then, for iteration
0 ≤t < T, the optimal solution v(t)
∗reg of problem (15) satisﬁes ∥v(t)
∗reg∥2 ≤2 + V and"
N,0.13201970443349753,"1
2
1
n n
X"
N,0.1330049261083744,"i=1
∥η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))∥2 ≤(1 + V"
N,0.13399014778325122,"2 )ε2.
(17)"
N,0.13497536945812807,"Based on Lemma 2, we guarantee the global convergence of Algorithm 1 and prove our ﬁrst theorem.
Since it is currently expensive to solve for the exact solution of problem (15), our algorithm serves
as a theoretical method to obtain the global convergence for the ﬁnite-sum minimization."
N,0.13596059113300493,"Theorem 1. Let w(t) be generated by Algorithm 1 where we use the closed form solution for the
search direction. We execute Algorithm 1 for T =
β"
N,0.13694581280788176,"ε outer loops for some constant β > 0. We
assume Assumption 1 holds. Suppose that Assumption 2 holds for G > 0 and Assumption 3 holds
for V > 0. We set the step size equal to η(t) = D√ε for some D > 0 and choose a learning rate
α(t)
i
= (1 + ε)α(t−1)
i
= (1 + ε)tα(0)
i . Based on β, we deﬁne α(0)
i
=
α
eβLφ with α ∈(0, 1"
N,0.13793103448275862,"3). Let F∗
be the global minimizer of F, and h∗
i = arg minz∈Rc φi(z), i ∈[n]. Then"
T,0.13891625615763548,"1
T"
T,0.1399014778325123,"T −1
X"
T,0.14088669950738916,"t=0
[F(w(t)) −F∗] ≤eβLφ(1 + ε)"
T,0.14187192118226602,"2(1 −3α)αβ · 1 n n
X"
T,0.14285714285714285,"i=1
∥h(w(0); i) −h∗
i ∥2 · ε"
T,0.1438423645320197,+ eβLφ(3ε + 2)
T,0.14482758620689656,"8α(1 −3α)

c(4 + (V + 2)GD2)2 + 8 + 4V

· ε.
(18)"
T,0.1458128078817734,"We note that β is a constant for the purpose of choosing the number of iterations T. The analysis
can be simpliﬁed by choosing β = 1 with T = 1"
T,0.14679802955665025,ε. Notice that the common convergence criteria for
T,0.1477832512315271,Under review as a conference paper at ICLR 2022
T,0.14876847290640394,ﬁnding a stationary point for non-convex problems is 1
T,0.1497536945812808,"T
PT
t=1 ||∇F(wt)||2 ≤O(ε). This criteria
has been widely used in the existing literature for non-convex optimization problems. Our conver-
gence criteria 1"
T,0.15073891625615762,"T
PT
t=1[F(wt) −F∗] ≤O(ε) is slightly different, in order to ﬁnd a global solution
for non-convex problems."
T,0.15172413793103448,"Our proof for Theorem 1 is novel and insightful. It is originally motivated by the Gradient Descent
update (7) and the convexity of the loss functions φi. For this reason it may not be a surprise that
Algorithm 1 can ﬁnd an ε-global solution after O
  1"
T,0.15270935960591134,"ε

iterations. However, computing the exact
solution in every iteration might be extremely challenging, especially when the number of samples
n is large. Therefore, we present a different approach to this problem in the following section."
APPROXIMATION USING GRADIENT DESCENT,0.15369458128078817,"5.2
APPROXIMATION USING GRADIENT DESCENT"
APPROXIMATION USING GRADIENT DESCENT,0.15467980295566502,"In this section, we use Gradient Descent (GD) algorithm to solve the strongly convex problem (15).
It is well-known that if ψ(x) −µ"
APPROXIMATION USING GRADIENT DESCENT,0.15566502463054188,"2 ∥x∥2 is convex for ∀x ∈Rc, then ψ(x) is µ-strongly convex (see
e.g. Nesterov (2004)). Hence Ψ(·) is ε2-strongly convex. For each iteration t, we use GD to ﬁnd a
search direction v(t) which is sufﬁciently close to the optimal solution v(t)
∗reg in that"
APPROXIMATION USING GRADIENT DESCENT,0.1566502463054187,"∥v(t) −v(t)
∗reg∥≤ε.
(19)"
APPROXIMATION USING GRADIENT DESCENT,0.15763546798029557,Our Algorithm 2 is described as follows.
APPROXIMATION USING GRADIENT DESCENT,0.15862068965517243,Algorithm 2 Solve the regularized problem using Gradient Descent
APPROXIMATION USING GRADIENT DESCENT,0.15960591133004925,"Initialization: Choose an initial point w(0) ∈Rd, tolerance ε > 0;
for t = 0, 1, · · · , T −1 do"
APPROXIMATION USING GRADIENT DESCENT,0.1605911330049261,Use Gradient Descent algorithm to solve Problem (15) and ﬁnd a solution v(t) that satisﬁes
APPROXIMATION USING GRADIENT DESCENT,0.16157635467980297,"∥v(t) −v(t)
∗reg∥≤ε"
APPROXIMATION USING GRADIENT DESCENT,0.1625615763546798,"Update w(t+1) = w(t) −η(t)v(t)
end for"
APPROXIMATION USING GRADIENT DESCENT,0.16354679802955666,"Since Algorithm 2 can only approximate a solution within some ε-preciseness, we need a supple-
mental assumption for the analysis of our next Theorem 2:"
APPROXIMATION USING GRADIENT DESCENT,0.16453201970443349,"Assumption 4. Let H(t)
i
be the Jacobian matrix deﬁned in Lemma 1. We assume that there exists
some constant H > 0 such that, for i ∈[n], ε > 0, and 0 ≤t < T as in Algorithm 2,"
APPROXIMATION USING GRADIENT DESCENT,0.16551724137931034,"∥H(t)
i ∥≤H
√ε.
(20)"
APPROXIMATION USING GRADIENT DESCENT,0.1665024630541872,"Assumption 4 requires a mild condition on the bounded Jacobian of h(w; i), and the upper bound
may depend on ε. This ﬂexibility allows us to accommodate a good dependence of ε for the theo-
retical analysis. We are now ready to present our convergence theorem for Algorithm 2."
APPROXIMATION USING GRADIENT DESCENT,0.16748768472906403,"Theorem 2. Let w(t) be generated by Algorithm 2 where v(t) satisﬁes (19). We execute Algorithm
2 for T =
β"
APPROXIMATION USING GRADIENT DESCENT,0.1684729064039409,"ε outer loops for some constant β > 0. We assume Assumption 1 holds. Suppose
that Assumption 2 holds for G > 0, Assumption 3 holds for V > 0 and Assumption 4 holds for
H > 0. We set the step size equal to η(t) = D√ε for some D > 0 and choose a learning rate
α(t)
i
= (1 + ε)α(t−1)
i
= (1 + ε)tα(0)
i . Based on β, we deﬁne α(0)
i
=
α
eβLφ with α ∈(0, 1"
APPROXIMATION USING GRADIENT DESCENT,0.16945812807881774,"4). Let F∗
be the global minimizer of F, and h∗
i = arg minz∈Rc φi(z), i ∈[n]. Then"
T,0.17044334975369457,"1
T"
T,0.17142857142857143,"T −1
X"
T,0.1724137931034483,"t=0
[F(w(t)) −F∗] ≤eβLφ(1 + ε)"
T,0.17339901477832512,"2(1 −4α)αβ · 1 n n
X"
T,0.17438423645320197,"i=1
∥h(w(0); i) −h∗
i ∥2 · ε"
T,0.17536945812807883,+ eβLφ(4ε + 3)
T,0.17635467980295566,"2α(1 −4α)

D2H2 + c(2 + (V + ε2 + 2)GD2)2 + 2 + V

· ε."
T,0.17733990147783252,Under review as a conference paper at ICLR 2022
T,0.17832512315270935,"Theorem 2 implies Corollary 2 which provides the computational complexity for Algorithm 2. Note
that for (Stochastic) Gradient Descent, we derive the complexity in terms of component gradient
calculations for the ﬁnite-sum problem (1). As an alternative, for Algorithm 2 we compare the
number of component gradients in problem (15). Such individual gradient has the following form:"
T,0.1793103448275862,"∇vψi(v) = η(t)H(t)
i
⊤H(t)
i η(t)v −α(t)
i η(t)H(t)
i
⊤∇zφi(h(w(t); i))."
T,0.18029556650246306,"In machine learning applications, the gradient of f(·; i) is calculated using automatic differentiation
(i.e. backpropagation). Since f(·; i) is the composition of the network structure h(·; i) and loss func-
tion φi(·), this process also computes the Jacobian matrix H(t)
i
and the gradient ∇zφi(h(w(t); i)) at
a speciﬁc weight w(t). Since matrix-vector multiplication computation is not expensive, the cost for
computing the component gradient of problem (15) is similar to problem (1)."
T,0.1812807881773399,"Corollary 2. Suppose that the conditions in Theorem 2 hold with η(t) = D
√ ˆε
√"
T,0.18226600985221675,"N for some D > 0 and
0 < ˆε ≤N (that is, we set ε = ˆε/N), where"
T,0.1832512315270936,"N = eβLφ
Pn
i=1 ∥h(w(0);i)−h∗
i ∥2"
T,0.18423645320197043,"n(1−4α)αβ
+
7eβLφ[D2H2+c(2+(V +3)GD2)2+2+V ]"
T,0.1852216748768473,"2α(1−4α)
."
T,0.18620689655172415,"Then, the total complexity to guarantee min0≤t≤T −1[F(w(t))−F∗] ≤1"
T,0.18719211822660098,"T
PT −1
t=0 [F(w(t))−F∗] ≤ˆε"
T,0.18817733990147784,"is O

n N 3β"
T,0.1891625615763547,ˆε3 (D2H2 + (ˆε2/N)) log( N
T,0.19014778325123152,"ˆε )

."
T,0.19113300492610838,"Remark 2. Corollary 2 shows that O (1/ˆε) outer loop iterations are needed in order to reach an
ˆε-global solution, and it proves that each iteration needs the equivalent of O
  n"
T,0.1921182266009852,ˆε2 log( 1
T,0.19310344827586207,"ˆε)

gradient
computations for computing an approximate solution. In total, Algorithm 2 has total complexity
O
  n"
T,0.19408866995073892,ˆε3 log( 1
T,0.19507389162561575,"ˆε)

for ﬁnding an ˆε-global solution."
T,0.1960591133004926,"For a comparison, Stochastic Gradient Descent uses a total of O( 1"
T,0.19704433497536947,"ε2 ) gradient computations to
ﬁnd a stationary point satisfying E[∥∇F( ˆw)∥2] ≤ε for non-convex problems (Ghadimi & Lan,
2013). Gradient Descent has a better complexity in terms of ε, i.e. O( n"
T,0.1980295566502463,"ε ) such that ∥∇F( ˆw)∥2 ≤ε
(Nesterov, 2004). However, both methods may not be able to reach a global solution of (1). In order
to guarantee global convergence for nonconvex settings, one may resort to use Polyak-Lojasiewicz
(PL) inequality (Karimi et al., 2016; Gower et al., 2021). This assumption is widely known to be
strong, which implies that every stationary point is also a global minimizer."
FURTHER DISCUSSION AND CONCLUSIONS,0.19901477832512315,"6
FURTHER DISCUSSION AND CONCLUSIONS"
FURTHER DISCUSSION AND CONCLUSIONS,0.2,"This paper presents an alternative composite formulation for solving the ﬁnite-sum optimization
problem. Our formulation allows a new way of exploiting the structure of machine learning prob-
lems and the convexity of squared loss and softmax cross entropy loss, and leads to a novel algorith-
mic framework that guarantees global convergence (when the outer loss functions are convex and
Lipschitz-smooth). Our analysis is general and can be applied to various different learning architec-
tures, in particular, our analysis and assumptions match practical neural networks; in recent years,
there has been a great interest in the structure of deep learning architectures for over-parameterized
settings (Arora et al., 2018; Allen-Zhu et al., 2019; Nguyen & Mondelli, 2020). Algorithm 2 demon-
strates a gradient method to solve the regularized problem, however, other methods can be applied
to our framework (e.g. conjugate gradient descent)."
FURTHER DISCUSSION AND CONCLUSIONS,0.20098522167487684,"Our theoretical foundation motivates further study, implementation, and optimization of the new
algorithmic framework and further investigation of its non-standard bounded style assumptions.
Possible research directions include more practical algorithm designs based on our Framework 1,
and different related methods to solve the regularized problem and approximate the solution. This
potentially leads to a new class of efﬁcient algorithms for machine learning problems. This paper
presents a new perspective to the research community."
FURTHER DISCUSSION AND CONCLUSIONS,0.2019704433497537,Under review as a conference paper at ICLR 2022
ETHICS STATEMENT,0.20295566502463055,ETHICS STATEMENT
ETHICS STATEMENT,0.20394088669950738,This paper does not contain ethics concerns.
REFERENCES,0.20492610837438424,REFERENCES
REFERENCES,0.20591133004926107,"Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the
36th International Conference on Machine Learning, volume 97 of Proceedings of Machine
Learning Research, pp. 242–252. PMLR, 09–15 Jun 2019. URL http://proceedings.
mlr.press/v97/allen-zhu19a.html."
REFERENCES,0.20689655172413793,"Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. In Jennifer Dy and Andreas Krause (eds.), Proceedings of
the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
Learning Research, pp. 244–253. PMLR, 10–15 Jul 2018. URL http://proceedings.
mlr.press/v80/arora18a.html."
REFERENCES,0.20788177339901479,"Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. In Kamalika
Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference
on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 322–332.
PMLR, 09–15 Jun 2019.
URL http://proceedings.mlr.press/v97/arora19a.
html."
REFERENCES,0.20886699507389161,"L´eon Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. SIAM Review, 60(2):223–311, 2018. doi: 10.1137/16M1080173."
REFERENCES,0.20985221674876847,"Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz.
SGD learns over-
parameterized networks that provably generalize on linearly separable data.
In International
Conference on Learning Representations, 2018. URL https://openreview.net/forum?
id=rJ33wwxRb."
REFERENCES,0.21083743842364533,"Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method
with support for non-strongly convex composite objectives. In Advances in Neural Information
Processing Systems, pp. 1646–1654, 2014."
REFERENCES,0.21182266009852216,"Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent ﬁnds global
minima of deep neural networks.
In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.),
Proceedings of the 36th International Conference on Machine Learning, volume 97 of Pro-
ceedings of Machine Learning Research, pp. 1675–1685. PMLR, 09–15 Jun 2019a.
URL
http://proceedings.mlr.press/v97/du19c.html."
REFERENCES,0.21280788177339902,"Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations,
2019b. URL https://openreview.net/forum?id=S1eK3i09YQ."
REFERENCES,0.21379310344827587,"John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(61):2121–2159, 2011a. URL
http://jmlr.org/papers/v12/duchi11a.html."
REFERENCES,0.2147783251231527,"John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12:2121–2159, 2011b."
REFERENCES,0.21576354679802956,"S. Ghadimi and G. Lan. Stochastic ﬁrst-and zeroth-order methods for nonconvex stochastic pro-
gramming. SIAM J. Optim., 23(4):2341–2368, 2013."
REFERENCES,0.21674876847290642,"Robert Gower, Othmane Sebbouh, and Nicolas Loizou. Sgd for structured nonconvex functions:
Learning rates, minibatching and interpolation. In Arindam Banerjee and Kenji Fukumizu (eds.),
Proceedings of The 24th International Conference on Artiﬁcial Intelligence and Statistics, volume
130 of Proceedings of Machine Learning Research, pp. 1315–1323. PMLR, 13–15 Apr 2021.
URL https://proceedings.mlr.press/v130/gower21a.html."
REFERENCES,0.21773399014778325,Under review as a conference paper at ICLR 2022
REFERENCES,0.2187192118226601,"M. G¨urb¨uzbalaban, A. Ozdaglar, and P. A. Parrilo. Convergence rate of incremental gradient and
incremental newton methods. SIAM Journal on Optimization, 29(4):2542–2565, 2019. doi: 10.
1137/17M1147846. URL https://doi.org/10.1137/17M1147846."
REFERENCES,0.21970443349753693,"Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In Advances in Neural Information Processing Systems, pp. 315–323, 2013."
REFERENCES,0.2206896551724138,"Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-
gradient methods under the Polyak-Łojasiewicz condition. In Paolo Frasconi, Niels Landwehr,
Giuseppe Manco, and Jilles Vreeken (eds.), Machine Learning and Knowledge Discovery in
Databases, pp. 795–811, Cham, 2016. Springer International Publishing."
REFERENCES,0.22167487684729065,"Nicolas Le Roux, Mark Schmidt, and Francis Bach. A stochastic gradient method with an exponen-
tial convergence rate for ﬁnite training sets. In NIPS, pp. 2663–2671, 2012."
REFERENCES,0.22266009852216748,"Kﬁr Y. Levy, Alp Yurtsever, and Volkan Cevher.
Online adaptive methods, universality and
acceleration.
In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran As-
sociates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
b0169350cd35566c47ba83c6ec1d6f82-Paper.pdf."
REFERENCES,0.22364532019704433,"Adrian S. Lewis and Stephen J. Wright. A proximal method for composite minimization. Mathe-
matical Programming, 158:501–546, 2016."
REFERENCES,0.2246305418719212,"A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to
stochastic programming. SIAM J. on Optimization, 19(4):1574–1609, 2009."
REFERENCES,0.22561576354679802,"Yurii Nesterov. Introductory lectures on convex optimization : a basic course. Applied optimization.
Kluwer Academic Publ., Boston, Dordrecht, London, 2004. ISBN 1-4020-7553-7."
REFERENCES,0.22660098522167488,"Lam M Nguyen, Jie Liu, Katya Scheinberg, and Martin Tak´aˇc. Sarah: A novel method for machine
learning problems using stochastic recursive gradient. In Proceedings of the 34th International
Conference on Machine Learning-Volume 70, pp. 2613–2621. JMLR. org, 2017."
REFERENCES,0.22758620689655173,"Quynh N Nguyen and Marco Mondelli.
Global convergence of deep networks with one wide
layer followed by pyramidal topology. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Bal-
can, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
11961–11972. Curran Associates, Inc., 2020.
URL https://proceedings.neurips.
cc/paper/2020/file/8abfe8ac9ec214d68541fcb888c0b4c3-Paper.pdf."
REFERENCES,0.22857142857142856,"Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola. Stochastic variance
reduction for nonconvex optimization. In Maria Florina Balcan and Kilian Q. Weinberger (eds.),
Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceed-
ings of Machine Learning Research, pp. 314–323, New York, New York, USA, 20–22 Jun 2016.
PMLR. URL https://proceedings.mlr.press/v48/reddi16.html."
REFERENCES,0.22955665024630542,"Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In
International Conference on Learning Representations, 2018. URL https://openreview.
net/forum?id=ryQu7f-RZ."
REFERENCES,0.23054187192118228,"Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. Pegasos: Primal estimated sub-gradient
solver for svm. Association for Computing Machinery, 2007. doi: 10.1145/1273496.1273598.
URL https://doi.org/10.1145/1273496.1273598."
REFERENCES,0.2315270935960591,"Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The im-
plicit bias of gradient descent on separable data. J. Mach. Learn. Res., 19(1):2822–2878, January
2018. ISSN 1532-4435."
REFERENCES,0.23251231527093597,"Trang H Tran, Lam M Nguyen, and Quoc Tran-Dinh. Smg: A shufﬂing gradient-based method
with momentum. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International
Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research,
pp. 10379–10389. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/
v139/tran21b.html."
REFERENCES,0.2334975369458128,Under review as a conference paper at ICLR 2022
REFERENCES,0.23448275862068965,"Quoc Tran-Dinh, Nhan Pham, and Lam Nguyen. Stochastic Gauss-Newton algorithms for noncon-
vex compositional optimization. In Hal Daum´e III and Aarti Singh (eds.), Proceedings of the 37th
International Conference on Machine Learning, volume 119 of Proceedings of Machine Learn-
ing Research, pp. 9572–9582. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.
press/v119/tran-dinh20a.html."
REFERENCES,0.2354679802955665,"Sharan Vaswani, Issam Laradji, Frederik Kunstner, Si Yi Meng, Mark Schmidt, and Simon Lacoste-
Julien. Adaptive gradient methods converge faster with over-parameterization (but you should do
a line-search), 2021."
REFERENCES,0.23645320197044334,"Junyu Zhang and Lin Xiao.
A stochastic composite gradient method with incremental vari-
ance reduction.
In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran As-
sociates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
a68259547f3d25ab3c0a5c0adb4e3498-Paper.pdf."
REFERENCES,0.2374384236453202,"Junyu Zhang and Lin Xiao. Multilevel composite stochastic optimization via nested variance reduc-
tion. SIAM Journal on Optimization, 31(2):1131–1157, 2021. doi: 10.1137/19M1285457. URL
https://doi.org/10.1137/19M1285457."
REFERENCES,0.23842364532019705,"Difan Zou and Quanquan Gu.
An improved analysis of training over-parameterized deep
neural networks.
In Hanna M. Wallach,
Hugo Larochelle,
Alina Beygelzimer,
Flo-
rence d’Alch´e-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural In-
formation Processing Systems 32:
Annual Conference on Neural Information Process-
ing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp.
2053–2062, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/
6a61d423d02a1c56250dc23ae7ff12f3-Abstract.html."
REFERENCES,0.23940886699507388,"Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep relu networks, 2018."
REFERENCES,0.24039408866995074,Under review as a conference paper at ICLR 2022
REFERENCES,0.2413793103448276,APPENDIX
REFERENCES,0.24236453201970443,"A
TABLE OF NOTATIONS"
REFERENCES,0.24334975369458128,"Notation
Meaning"
REFERENCES,0.24433497536945814,"F∗
Global minimization function of F in (1)"
REFERENCES,0.24532019704433497,F∗= minw∈Rd F(w)
REFERENCES,0.24630541871921183,"h∗
i
h∗
i = arg minz∈Rc φi(z), i ∈[n]"
REFERENCES,0.24729064039408866,"v(t)
∗
Solution of the convex problem in (10)"
REFERENCES,0.2482758620689655,minv∈Rd 1
REFERENCES,0.24926108374384237,"2
1
n
Pn
i=1 ∥η(t)H(t)
i v −α(t)
i ∇zφi(h(w(t); i))∥2"
REFERENCES,0.25024630541871923,"v(t)
An approximation of v(t)
∗
which is used as the search direction in Framework 1"
REFERENCES,0.2512315270935961,"ˆv(t)
∗ε
A vector that satisﬁes"
REFERENCES,0.2522167487684729,"1
2
1
n
Pn
i=1 ∥η(t)H(t)
i v −α(t)
i ∇zφi(h(w(t); i))∥2 ≤ε2"
REFERENCES,0.25320197044334974,"for some ε > 0 and ∥ˆv(t)
∗ε ∥2 ≤V , for some V > 0."
REFERENCES,0.2541871921182266,"v(t)
∗reg
Solution of the strongly convex problem in (15)"
REFERENCES,0.25517241379310346,"minv∈Rd
n
1
2
1
n
Pn
i=1 ∥η(t)H(t)
i v −α(t)
i ∇zφi(h(w(t); i))∥2 + ε2"
REFERENCES,0.2561576354679803,2 ∥v∥2o
REFERENCES,0.2571428571428571,"B
USEFUL RESULTS"
REFERENCES,0.258128078817734,"The following lemmas provide key tools for our results.
Lemma 3 (Squared loss). Let b ∈Rc and deﬁne φ(z) = 1"
REFERENCES,0.25911330049261083,"2∥z −b∥2 for z ∈Rc. Then φ is convex
and Lφ-smooth with Lφ = 1.
Lemma 4 (Softmax cross-entropy loss). Let index a ∈[c] and deﬁne"
REFERENCES,0.2600985221674877,"φ(z) = log "" c
X"
REFERENCES,0.26108374384236455,"k=1
exp(zk −za) # = log "" c
X"
REFERENCES,0.2620689655172414,"k=1
exp(w⊤
k z) # ,"
REFERENCES,0.2630541871921182,"for z = (z1, . . . , zc)⊤∈Rc, where wk = ek−ea with ei representing the i-th unit vector (containing
1 at the i-th position and 0 elsewhere). Then φ is convex and Lφ-smooth with Lφ = 1."
REFERENCES,0.26403940886699506,"The following lemma is a standard result in (Nesterov, 2004).
Lemma 5 ((Nesterov, 2004)). If φ is Lφ-smooth and convex, then for ∀z ∈Rc,"
REFERENCES,0.2650246305418719,"∥∇φ(z)∥2 ≤2Lφ(φ(z) −φ(z∗)),
(21)"
REFERENCES,0.2660098522167488,where z∗= arg minz φ(z).
REFERENCES,0.26699507389162563,"The following useful derivations could be used later in our theoretical analysis. Since φi is convex,
by Deﬁnition 2 we have"
REFERENCES,0.26798029556650244,"φi(h(w; i)) ≥φi(h(w′; i)) +

∇zφi(z)

z=h(w′;i), h(w; i) −h(w′; i)

.
(22)"
REFERENCES,0.2689655172413793,"If φi is convex and Lφ-smooth, then by Lemma 5
∇zφi(z)

z=h(w;i) "
REFERENCES,0.26995073891625615,"2
≤2Lφ [φi(h(w; i)) −φi(h∗
i )] ,
(23)"
REFERENCES,0.270935960591133,"where h∗
i = arg minz∈Rc φi(z)."
REFERENCES,0.27192118226600986,We compute gradients of f(w; i) in term of φi(h(w; i)).
REFERENCES,0.2729064039408867,Under review as a conference paper at ICLR 2022
REFERENCES,0.2738916256157635,• Gradient of softmax cross-entropy loss:
REFERENCES,0.2748768472906404,"∇φi(z)

z=h(w;i) =
∂φi(z) ∂z1"
REFERENCES,0.27586206896551724,"z=h(w;i), . . . , ∂φi(z) ∂zc"
REFERENCES,0.2768472906403941,"z=h(w;i) ⊤
,"
REFERENCES,0.27783251231527095,"where for j ∈[c],"
REFERENCES,0.2788177339901478,∂φi(z) ∂zj
REFERENCES,0.2798029556650246,z=h(w;i) =
REFERENCES,0.28078817733990147,"


 

"
REFERENCES,0.2817733990147783,"exp

[h(w;i)]j−[h(w;i)]I(y(i)) "
REFERENCES,0.2827586206896552,"Pc
k=1 exp

[h(w;i)]k−[h(w;i)]I(y(i))"
REFERENCES,0.28374384236453204,"
, j ̸= I(y(i)) − P"
REFERENCES,0.28472906403940884,"k̸=I(y(i)) exp

[h(w;i)]k−[h(w;i)]I(y(i)) "
REFERENCES,0.2857142857142857,"Pc
k=1 exp

[h(w;i)]k−[h(w;i)]I(y(i))"
REFERENCES,0.28669950738916256,"
, j = I(y(i))
.
(24)"
REFERENCES,0.2876847290640394,• Gradient of squared loss:
REFERENCES,0.28866995073891627,"∇φi(z)

z=h(w;i) = h(w; i) −y(i).
(25)"
REFERENCES,0.2896551724137931,"C
ADDITIONAL DISCUSSION"
REFERENCES,0.29064039408866993,"C.1
ABOUT ASSUMPTION 2"
REFERENCES,0.2916256157635468,"We make a formal assumption for the case h(·; i) is closely approximated by k(·; i).
Assumption 5. We assume that for all i ∈[n] there exists some approximations k(w; i) : Rd →Rc
such that"
REFERENCES,0.29261083743842364,"|kj(w; i) −hj(w; i)| ≤ε, ∀w ∈Rd, i ∈[n] and j ∈[c],
(26)"
REFERENCES,0.2935960591133005,"where k(·; i) are twice continuously differentiable (i.e. the second-order partial derivatives of all
scalars kj(·; i) are continuous for all i ∈[n]), and that their Hessian matrices are bounded:"
REFERENCES,0.29458128078817736,"∥Mi,j(w)∥= ∥Jw (∇wkj(w; i))∥≤G, ∀w ∈Rd, i ∈[n] and j ∈[c].
(27)"
REFERENCES,0.2955665024630542,"Assumption 5 allows us to prove the following Lemma that bound the error in equation (9):
Lemma 6. Suppose that Assumption 5 holds for the classiﬁer h. Then for all i ∈[n] and 0 ≤t < T,
we have:"
REFERENCES,0.296551724137931,"h(w(t+1); i) = h(w(t) −η(t)v(t); i) = h(w(t); i) −η(t)H(t)
i v(t) + ϵ(t)
i ,
(28)"
REFERENCES,0.2975369458128079,"where H(t)
i
is deﬁned to be the Jacobian matrix of the approximation k(w; i) at w(t):"
REFERENCES,0.29852216748768473,"H(t)
i
:= Jwk(w; i)|w=w(t) =  "
REFERENCES,0.2995073891625616,∂k1(w;i)
REFERENCES,0.30049261083743845,"∂w1
. . .
∂k1(w;i)"
REFERENCES,0.30147783251231525,"∂wd
. . .
. . .
. . ."
REFERENCES,0.3024630541871921,∂kc(w;i)
REFERENCES,0.30344827586206896,"∂w1
. . .
∂kc(w;i) ∂wd  "
REFERENCES,0.3044334975369458,"w=w(t)
∈Rc×d.
(29)"
REFERENCES,0.3054187192118227,"Additionally we have,"
REFERENCES,0.30640394088669953,"|ϵ(t)
i,j| ≤1"
REFERENCES,0.30738916256157633,"2(η(t))2∥v(t)∥2G + 2ε, j ∈[c].
(30)"
REFERENCES,0.3083743842364532,"Note that these result recover the case when h(·; i) is itself smooth. Hence we analyze our algorithms
using the result of Lemma 6, which generalizes the result from Lemma 1."
REFERENCES,0.30935960591133005,"C.2
ABOUT ASSUMPTION 3"
REFERENCES,0.3103448275862069,"In this section, we justify the existence of the search direction in Assumption 3 (almost surely). We
argue that there exists a vector ˆv(t)
∗ε satisfying"
REFERENCES,0.31133004926108376,"1
2
1
n n
X"
REFERENCES,0.31231527093596056,"i=1
∥η(t)H(t)
i
ˆv(t)
∗ε −α(t)
i ∇zφi(h(w(t); i))∥2 ≤ε2."
REFERENCES,0.3133004926108374,Under review as a conference paper at ICLR 2022
REFERENCES,0.3142857142857143,It is sufﬁcient to ﬁnd a vector v satisfying that
REFERENCES,0.31527093596059114,"η(t)H(t)
i v = α(t)
i ∇zφi(h(w(t); i)) for every i ∈[n]."
REFERENCES,0.316256157635468,"Since the solution v is in Rd and ∇zφi(h(w(t); i)) is in Rc, this condition is equivalent to a linear
system with n·c constraints and d variables. Let A and b be the following stacked matrix and vector: A =  "
REFERENCES,0.31724137931034485,"H(t)
1 η(t) . . ."
REFERENCES,0.31822660098522165,"H(t)
n η(t) "
REFERENCES,0.3192118226600985,"∈Rn·c×d, and b =  "
REFERENCES,0.32019704433497537,"α(t)
1 ∇zφ1(h(w(t); i)) . . ."
REFERENCES,0.3211822660098522,"α(t)
n ∇zφn(h(w(t); i)) "
REFERENCES,0.3221674876847291,"∈Rn·c,"
REFERENCES,0.32315270935960594,"then the problem reduce to ﬁnding the solution of the equation Av = b. In the over-parameterized
setting where dimension d is sufﬁciently large (d ≫n · c), then rank A = n · c almost surely and
there exists almost surely a vector v that interpolates all the training set."
REFERENCES,0.32413793103448274,"To demonstrate this fact easier, we consider a simple neural network where the classiﬁer h(w; i) is
formulated as"
REFERENCES,0.3251231527093596,"h(w; i) = W (2)⊤σ(W (1)⊤x(i)),"
REFERENCES,0.32610837438423645,"where c = 1, W (1) ∈Rm×l and W (2) ∈Rl×1, w = vec({W (1), W (2)}) ∈Rd is the vectorized
weight where d = l(m + 1) and σ is sigmoid activation function."
REFERENCES,0.3270935960591133,"H(t)
i
is deﬁned to be the Jacobian matrix of h(w; i) at w(t):"
REFERENCES,0.32807881773399017,"H(t)
i
:= Jwh(w; i)|w=w(t) =
h
∂h(w;i)"
REFERENCES,0.32906403940886697,"∂w1
. . .
∂h(w;i) ∂wd"
REFERENCES,0.33004926108374383,"i 
w=w(t)
∈R1×d, then"
REFERENCES,0.3310344827586207,A = η(t)  
REFERENCES,0.33201970443349754,"H(t)
1
. . ."
REFERENCES,0.3330049261083744,"H(t)
n "
REFERENCES,0.33399014778325126,= η(t)  
REFERENCES,0.33497536945812806,∂h(w;1)
REFERENCES,0.3359605911330049,"∂w1
. . .
∂h(w;1)"
REFERENCES,0.3369458128078818,"∂wd
. . .
. . .
. . ."
REFERENCES,0.33793103448275863,∂h(w;n)
REFERENCES,0.3389162561576355,"∂w1
. . .
∂h(w;n) ∂wd "
REFERENCES,0.3399014778325123,∈Rn×d.
REFERENCES,0.34088669950738915,"We want to show that A has full rank, almost surely. We consider the over-parameterized setting
where the last layer has at least n neuron (i.e. l = n and the simple version when c = 1. We argue
that rank of matrix A is greater than or equal to rank of the submatrix B created by the weights of
the last layer W (2) ∈Rn: B =  "
REFERENCES,0.341871921182266,∂h(w;1)
REFERENCES,0.34285714285714286,"∂W (2)
1
. . .
∂h(w;1)"
REFERENCES,0.3438423645320197,"∂W (2)
n
. . .
. . .
. . ."
REFERENCES,0.3448275862068966,∂h(w;n)
REFERENCES,0.3458128078817734,"∂W (2)
1
. . .
∂h1(w;n)"
REFERENCES,0.34679802955665023,"∂W (2)
n "
REFERENCES,0.3477832512315271,∈Rn×n.
REFERENCES,0.34876847290640395,"Note that h(·, i) is a linear function of the last weight layers (in this simple case W (2) ∈Rn and
σ(W (1)⊤x(i)) ∈Rn), we can compute the partial derivatives as follows:"
REFERENCES,0.3497536945812808,∂h(w; i)
REFERENCES,0.35073891625615766,"∂W (2)
= σ(W (1)⊤x(i)); i ∈[n]. Hence B =  "
REFERENCES,0.35172413793103446,σ(W (1)⊤x(1)) . . .
REFERENCES,0.3527093596059113,σ(W (1)⊤x(n)) 
REFERENCES,0.3536945812807882,∈Rn×n.
REFERENCES,0.35467980295566504,"Assuming that there are no identical data, and σ is the sigmoid activation, the set of weights W (1)
that make matrix B degenerate has measure zero. Hence B has full rank almost surely, and we have
the same conclusion for A. Therefore we are able to prove the almost surely existence of a solution
v of the linear equation Av = b for simple two layers network. Using the same argument, this result
can be generalized for larger neural networks where the dimension d is sufﬁciently large (d ≫nc)."
REFERENCES,0.3556650246305419,Under review as a conference paper at ICLR 2022
REFERENCES,0.3566502463054187,"C.3
INITIALIZATION EXAMPLE"
REFERENCES,0.35763546798029555,"Our Assumption 3 requires a nice dependency on the tolerance ε for the gradient matrices H(0)
i
and
∇zφi(h(w(0); i)). We note that at the starting point t = 0, these matrices may depend on ε due
to the initialization process and the dependence of d on ε. In order to accommodate the choice of
learning rate η(0) = D√ε in our theorems, in this section we describe a network initialization that
satisﬁes ∥H(0)
i
∥= Θ

1
√ε

where the gradient norm ∥∇zφi(h(w(0); i))∥is at most constant order
with respect to ε. To simplify the problem, we only consider small-dimension data and networks
without activation."
REFERENCES,0.3586206896551724,"About the target vector: We choose φi to be the softmax cross-entropy loss. By Lemma 7 (see
below), we have that the gradient norm is upper bounded by a constant c, where c is the output
dimension of the problem and is not dependent on ε. Note that when we stack all gradients for n
data points, then the size of new vector is still not dependent on ε."
REFERENCES,0.35960591133004927,"About the network architecture: For simplicity, we consider the following classiﬁcation problem
where"
REFERENCES,0.3605911330049261,"• The input data is in R2. There are only two data points {x(1), x(2)}. Input data is bounded
and non-degenerate (we will clarify this property later)."
REFERENCES,0.361576354679803,"• The output data is (categorical) in R2: {y(1) = (1, 0), y(2) = (0, 1)}."
REFERENCES,0.3625615763546798,"We want to have an over-parameterized setting where the dimension of weight vector is at least
nc = 4. We consider a simple network with two layers, no biases and no activation functions. Let the
number of neurons in the hidden layer be m. The ﬂow of this network is (in) R2 →Rm →R2 (out).
First, we consider the case where m = 1."
REFERENCES,0.36354679802955664,"• The ﬁrst layer has 2 parameters (w1, w2) and only 1 neuron that outputs z(i) = w1x(i)
1
+
w2x(i)
2
(the subscript is for the coordinate of input data x(i)).
• The second layer has 2 parameters (w3, w4). The ﬁnal output is"
REFERENCES,0.3645320197044335,"h(w, i) = [w3(w1x(i)
1 + w2x(i)
2 ), w4(w1x(i)
1 + w2x(i)
2 )]⊤∈R2,"
REFERENCES,0.36551724137931035,"with w = [w1, w2, w3, w4]⊤∈R4. This network satisﬁes that the Hessian matrices of h(w; i) are
bounded. Let Q and b be the following stacked matrix and vector: Q ="
REFERENCES,0.3665024630541872,"""
H(0)
1
H(0)
2 #"
REFERENCES,0.367487684729064,"∈R4×4, and b ="
REFERENCES,0.36847290640394087,"""
∇zφ1(h(w(0); 1))"
REFERENCES,0.3694581280788177,"∇zφ2(h(w(0); 2)) # ∈R4,"
REFERENCES,0.3704433497536946,Then we have the following:
REFERENCES,0.37142857142857144,Q = Q(w) =
REFERENCES,0.3724137931034483,"""
H(0)
1
H(0)
2 # = "
REFERENCES,0.3733990147783251,
REFERENCES,0.37438423645320196,"∇w[w3(w1x(1)
1
+ w2x(1)
2 )]"
REFERENCES,0.3753694581280788,"∇w[w4(w1x(1)
1
+ w2x(1)
2 )]"
REFERENCES,0.37635467980295567,"∇w[w3(w1x(2)
1
+ w2x(2)
2 )]"
REFERENCES,0.37733990147783253,"∇w[w4(w1x(2)
1
+ w2x(2)
2 )] "
REFERENCES,0.3783251231527094, = 
REFERENCES,0.3793103448275862,
REFERENCES,0.38029556650246304,"w3x(1)
1
w3x(1)
2
w1x(1)
1
+ w2x(1)
2
0"
REFERENCES,0.3812807881773399,"w4x(1)
1
w4x(1)
2
0
w1x(1)
1
+ w2x(1)
2
w3x(2)
1
w3x(2)
2
w1x(2)
1
+ w2x(2)
2
0"
REFERENCES,0.38226600985221676,"w4x(2)
1
w4x(2)
2
0
w1x(2)
1
+ w2x(2)
2 "
REFERENCES,0.3832512315270936,"
."
REFERENCES,0.3842364532019704,"The determinant of this matrix is a polynomial of the weight w and the input data. Under some mild
non-degenerate condition of the input data, we can choose some base point w′ that made this matrix
invertible (note that if this condition is not satisﬁed, we can rescale/add a very small noise to the
data - which is the common procedure in machine learning)."
REFERENCES,0.3852216748768473,Under review as a conference paper at ICLR 2022
REFERENCES,0.38620689655172413,Hence the system Qu = b always has a solution. Now we consider the following two initializations:
REFERENCES,0.387192118226601,"1. We choose to initialize the starting point at w(0) =
1
√εw′ and note that Q(w) is a linear function"
REFERENCES,0.38817733990147785,"of w and Q(w′) is independent of ε. Then the norm of matrix Q(w(0)) has the same scale with
1
√ε."
REFERENCES,0.3891625615763547,"2. Instead of choosing m = 1, we consider an over-parameterized network where m = 1"
REFERENCES,0.3901477832512315,"ε (recall
that m is the number of neurons in the hidden layer). The hidden layer in this case is: z = 

 
"
REFERENCES,0.39113300492610836,"z(i)
1
= w(1)
1,1x(i)
1 + w(1)
2,1x(i)
2
. . .
z(i)
m
= w(1)
1,mx(i)
1 + w(1)
2,mx(i)
2 ."
REFERENCES,0.3921182266009852,"The output layer is:
(
y(i)
1
= z(i)
1 w(2)
1,1 + · · · + z(i)
m w(2)
m,1 = (w(1)
1,1x(i)
1 + w(1)
2,1x(i)
2 )w(2)
1,1 + · · · + (w(1)
1,mx(i)
1 + w(1)
2,mx(i)
2 )w(2)
m,1
y(i)
2
= z(i)
1 w(2)
1,2 + · · · + z(i)
m w(2)
m,2 = (w(1)
1,1x(i)
1 + w(1)
2,1x(i)
2 )w(2)
1,2 + · · · + (w(1)
1,mx(i)
1 + w(1)
2,mx(i)
2 )w(2)
m,2"
REFERENCES,0.3931034482758621,"with w = [w(1)
1,1, . . . , w(1)
1,m, w(1)
2,1, . . . , w(1)
2,m, w(2)
1,1, w(2)
1,2, . . . , w(2)
m,1, w(2)
m,2]⊤∈R4m."
REFERENCES,0.39408866995073893,"Hence,"
REFERENCES,0.39507389162561574,Q(w) = 
REFERENCES,0.3960591133004926,
REFERENCES,0.39704433497536945,"w(2)
1,1x(1)
1
. . . w(2)
m,1x(1)
1
w(2)
1,1x(1)
2
. . .
w(2)
m,1x(1)
2
z(1)
1
0
. . . z(1)
m
0"
REFERENCES,0.3980295566502463,"w(2)
1,2x(1)
1
. . . w(2)
m,2x(1)
1
w(2)
1,2x(1)
2
. . .
w(2)
m,2x(1)
2
0
z(1)
1
. . . 0
z(1)
m"
REFERENCES,0.39901477832512317,"w(2)
1,1x(2)
1
. . . w(2)
m,1x(2)
1
w(2)
1,1x(2)
2
. . .
w(2)
m,1x(2)
2
z(2)
1
0
. . . z(2)
m
0"
REFERENCES,0.4,"w(2)
1,2x(2)
1
. . . w(2)
m,2x(2)
1
w(2)
1,2x(2)
2
. . .
w(2)
m,2x(2)
2
0
z(2)
1
. . . 0
z(2)
m "
REFERENCES,0.4009852216748768,"
."
REFERENCES,0.4019704433497537,"Hence, the number of (possibly) non-zero elements in each row is 3m = 3 ε."
REFERENCES,0.40295566502463054,"For matrix A of rank r, we have ∥A∥2 ≤∥A∥F ≤√r∥A∥2. Since the rank of Q(w) is at most 4
(nc = 4, independent of ε), we only need to ﬁnd the Frobenius norm of Q(w). We have"
REFERENCES,0.4039408866995074,∥Q(w)∥F =
REFERENCES,0.40492610837438425,"v
u
u
t"
X,0.4059113300492611,"4
X i=1"
"M
X",0.4068965517241379,"4m
X"
"M
X",0.40788177339901477,"j=1
|qij|2."
"M
X",0.4088669950738916,"Let qmin and qmax be the element with smallest/largest magnitude of Q(w). Suppose that x(i) ̸=
(0, 0) and choose w ̸= 0 such that z ̸= 0, qmin > 0 and independent of ε. Hence,
√"
"M
X",0.4098522167487685,"8
√ε|qmin| ≤"
"M
X",0.41083743842364534,"∥Q(w)∥F ≤
√"
"M
X",0.41182266009852214,"12
√ε |qmax|."
"M
X",0.412807881773399,"Hence, ∥Q(w)∥= Θ

1
√ε

. Therefore this simple network initialization supports the dependence
on ε for our Assumption 3. We note that a similar setting is found in (Allen-Zhu et al., 2019), where
the authors initialize the weights using a random Gaussian distribution with a variance depending
on the dimension of the problem. In non-convex setting, they prove the convergence of SGD using
the assumption that the number of neurons m depends inversely on the tolerance ε."
"M
X",0.41379310344827586,"Lemma 7. For softmax cross-entropy loss, and x = h(w; i) ∈Rc, for ∀w ∈Rd and i ∈[n], we
have
∇zφi(x)

x=h(w;i) "
"M
X",0.4147783251231527,"2
≤c.
(31)"
"M
X",0.41576354679802957,"Proof. By (24), we have for i = 1, . . . , n,"
"M
X",0.41674876847290643,Under review as a conference paper at ICLR 2022
"M
X",0.41773399014778323,• For j ̸= I(y(i)):
"M
X",0.4187192118226601,∂φi(x) ∂xj
"M
X",0.41970443349753694,"x=h(w;i) 2
="
"M
X",0.4206896551724138,"exp
 
[h(w; i)]j −[h(w; i)]I(y(i))
"
"M
X",0.42167487684729066,"Pc
k=1 exp
 
[h(w; i)]k −[h(w; i)]I(y(i))
 !2 ="
"M
X",0.42266009852216746,"exp
 
[h(w; i)]j −[h(w; i)]I(y(i))
 1 + P"
"M
X",0.4236453201970443,"k̸=I(y(i)) exp
 
[h(w; i)]k −[h(w; i)]I(y(i))
 !2 ≤1."
"M
X",0.4246305418719212,• For j = I(y(i)):
"M
X",0.42561576354679803,∂φi(x) ∂xj
"M
X",0.4266009852216749,"x=h(w;i) 2
= P"
"M
X",0.42758620689655175,"k̸=I(y(i)) exp
 
[h(w; i)]k −[h(w; i)]I(y(i))
"
"M
X",0.42857142857142855,"Pc
k=1 exp
 
[h(w; i)]k −[h(w; i)]I(y(i))
 !2 = P"
"M
X",0.4295566502463054,"k̸=I(y(i)) exp
 
[h(w; i)]k −[h(w; i)]I(y(i))
 1 + P"
"M
X",0.43054187192118226,"k̸=I(y(i)) exp
 
[h(w; i)]k −[h(w; i)]I(y(i))
 !2 ≤1"
"M
X",0.4315270935960591,"Hence, for i = 1, . . . , n,
∇zφi(x)

x=h(w;i)  2
= c
X j=1"
"M
X",0.432512315270936,∂φi(x) ∂xj
"M
X",0.43349753694581283,x=h(w;i)
"M
X",0.43448275862068964,"2
≤c."
"M
X",0.4354679802955665,This completes the proof.
"M
X",0.43645320197044335,"D
PROOFS OF LEMMAS AND COROLLARY 1"
"M
X",0.4374384236453202,PROOF OF LEMMA 1
"M
X",0.43842364532019706,"Proof. Since h(·; i) are twice continuously differentiable for all i ∈[n], we have the following
Taylor approximation for each component outputs hj(·; i) where j ∈[c] and i ∈[n]:"
"M
X",0.43940886699507387,hj(w(t+1); i) = hj(w(t) −η(t)v(t); i)
"M
X",0.4403940886699507,= hj(w(t); i) −Jwhj(w; i)|w=w(t)η(t)v(t) + 1
"M
X",0.4413793103448276,"2(η(t)v(t))⊤Mi,j( ˜w(t))(η(t)v(t)), (32)"
"M
X",0.44236453201970444,"where Mi,j( ˜w(t)) is the Hessian matrices of hj(·; i)at ˜w(t) and ˜w(t) = αw(t) + (1 −α)w(t+1) for
some α ∈[0, 1]. This leads to our desired statement:"
"M
X",0.4433497536945813,"h(w(t+1); i) = h(w(t) −η(t)v(t); i) = h(w(t); i) −η(t)H(t)
i v(t) + ϵ(t)
i , where"
"M
X",0.44433497536945815,"ϵ(t)
i,j = 1"
"M
X",0.44532019704433495,"2(η(t)v(t))⊤Mi,j( ˜w(t))(η(t)v(t)), j ∈[c],"
"M
X",0.4463054187192118,Hence we get the ﬁnal bound:
"M
X",0.44729064039408867,"|ϵ(t)
i,j| ≤1 2"
"M
X",0.4482758620689655,"(η(t)v(t))⊤Mi,j( ˜w(t))(η(t)v(t)) ≤1"
"M
X",0.4492610837438424,"2(η(t))2∥v(t)∥2 · ∥Mi,j( ˜w(t))∥"
"M
X",0.45024630541871924,"(11)
≤1"
"M
X",0.45123152709359604,"2(η(t))2∥v(t)∥2G, j ∈[c]."
"M
X",0.4522167487684729,Under review as a conference paper at ICLR 2022
"M
X",0.45320197044334976,PROOF OF LEMMA 2
"M
X",0.4541871921182266,"Proof. From Assumption 3, we know that there exists ˆv(t)
∗ε so that"
"M
X",0.45517241379310347,"1
2
1
n n
X"
"M
X",0.45615763546798027,"i=1
∥η(t)H(t)
i
ˆv(t)
∗ε −α(t)
i ∇zφi(h(w(t); i))∥2 ≤ε2,"
"M
X",0.45714285714285713,"and ∥ˆv(t)
∗ε ∥2 ≤V , for some V > 0. Hence,"
"M
X",0.458128078817734,"1
2
1
n n
X"
"M
X",0.45911330049261084,"i=1
∥η(t)H(t)
i
ˆv(t)
∗ε −α(t)
i ∇zφi(h(w(t); i))∥2 + ε2"
"M
X",0.4600985221674877,"2 ∥ˆv(t)
∗ε ∥2 ≤ε2 + ε2"
"M
X",0.46108374384236456,2 V = (1 + V
"M
X",0.46206896551724136,2 )ε2.
"M
X",0.4630541871921182,"Since v(t)
∗reg is the optimal solution of the problem in (15) for 0 ≤t < T, we have"
"M
X",0.4640394088669951,"1
2
1
n n
X"
"M
X",0.46502463054187193,"i=1
∥η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))∥2 + ε2"
"M
X",0.4660098522167488,"2 ∥v(t)
∗reg∥2 ≤(1 + V"
"M
X",0.4669950738916256,2 )ε2.
"M
X",0.46798029556650245,"Therefore, we have (17) and ∥v(t)
∗reg∥2 ≤2 + V for 0 ≤t < T."
"M
X",0.4689655172413793,PROOF OF LEMMA 3
"M
X",0.46995073891625616,"Proof. 1. We want to show that for any α ∈[0, 1]"
"M
X",0.470935960591133,"φ(αz1 + (1 −α)z2) ≤αφ(z1) + (1 −α)φ(z2), ∀z1, z2 ∈Rc,
(33)"
"M
X",0.4719211822660099,"in order to have the convexity of φ with respect to z (see (Nesterov, 2004))."
"M
X",0.4729064039408867,"For any α ∈[0, 1], we have for ∀z1, z2 ∈Rc,"
"M
X",0.47389162561576353,α∥z1 −b∥2 + (1 −α)∥z2 −b∥2 −∥α(z1 −b) + (1 −α)(z2 −b)∥2
"M
X",0.4748768472906404,= α∥z1 −b∥2 + (1 −α)∥z2 −b∥2 −α2∥z1 −b∥2 −(1 −α)2∥z2 −b∥2
"M
X",0.47586206896551725,"−2α(1 −α)⟨z1 −b, z2 −b⟩"
"M
X",0.4768472906403941,≥α(1 −α)∥z1 −b∥2 + (1 −α)α∥z2 −b∥2 −2α(1 −α)∥z1 −b∥· ∥z2 −b∥
"M
X",0.47783251231527096,"= α(1 −α) (∥z1 −b∥−∥z2 −b∥)2 ≥0,"
"M
X",0.47881773399014776,"where the ﬁrst inequality follows according to Cauchy-Schwarz inequality ⟨a, b⟩≤∥a∥·∥b∥. Hence,"
"M
X",0.4798029556650246,"1
2∥αz1 + (1 −α)z2 −b∥2 ≤α"
"M
X",0.4807881773399015,2 ∥z1 −b∥2 + (1 −α)
"M
X",0.48177339901477834,"2
∥z2 −b∥2."
"M
X",0.4827586206896552,"Therefore, (33) implies the convexity of φ with respect to z."
"M
X",0.483743842364532,2. We want to show that ∃Lφ > 0 such that
"M
X",0.48472906403940885,"∥∇φ(z1) −∇φ(z2)∥≤Lφ∥z1 −z2∥, ∀z1, z2 ∈Rc.
(34)"
"M
X",0.4857142857142857,"Notice that ∇φ(z) = z −b, then clearly ∀z1, z2 ∈Rc,"
"M
X",0.48669950738916257,∥∇φ(z1) −∇φ(z2)∥= ∥z1 −z2∥.
"M
X",0.4876847290640394,"Therefore, (34) implies the Lφ-smoothness of φ with respect to z with Lφ = 1."
"M
X",0.4886699507389163,PROOF OF LEMMA 4
"M
X",0.4896551724137931,"Proof. 1. For ∀z1, z2 ∈Rc and 1 ≤k ≤c, denote uk,1 = exp(w⊤
k z1) and uk,2 = exp(w⊤
k z2) and
using Holder inequality c
X"
"M
X",0.49064039408866994,"k=1
ak · bk ≤ c
X"
"M
X",0.4916256157635468,"k=1
|ak|p
! 1"
"M
X",0.49261083743842365,"p  c
X"
"M
X",0.4935960591133005,"k=1
|bk|q
! 1"
"M
X",0.4945812807881773,"q
, where 1 p + 1"
"M
X",0.49556650246305417,"q = 1,
(35)"
"M
X",0.496551724137931,Under review as a conference paper at ICLR 2022
"M
X",0.4975369458128079,we have
"M
X",0.49852216748768474,"φ(αz1 + (1 −α)z2) = log "" c
X"
"M
X",0.4995073891625616,"k=1
exp(w⊤
k (αz1 + (1 −α)z2)) # = log "" c
X"
"M
X",0.5004926108374385,"k=1
uα
k,1 · u(1−α)
k,2 #"
"M
X",0.5014778325123153,"(35)
≤log   c
X"
"M
X",0.5024630541871922,"k=1
u
α· 1 α
k,1"
"M
X",0.503448275862069,"!α  c
X"
"M
X",0.5044334975369458,"k=1
u
(1−α)·
1
(1−α)
k,2 !1−α "
"M
X",0.5054187192118227,"= α log "" c
X"
"M
X",0.5064039408866995,"k=1
exp(w⊤
k z1) #"
"M
X",0.5073891625615764,"+ (1 −α) log "" c
X"
"M
X",0.5083743842364532,"k=1
exp(w⊤
k z2) #"
"M
X",0.50935960591133,"= αφ(z1) + (1 −α)φ(z2),"
"M
X",0.5103448275862069,"where the ﬁrst inequality since log(x) is an increasing function for ∀x > 0 and exp(v) > 0 for
∀v ∈R. Therefore, (33) implies the convexity of φ with respect to z."
"M
X",0.5113300492610837,"2. Note that ∥∇2φ(z)∥≤Lφ if and only if φ(z) is Lφ-smooth (see (Nesterov, 2004)). First, we
compute gradient of φ(z):"
"M
X",0.5123152709359606,• For i ̸= a: ∂φ(z)
"M
X",0.5133004926108374,"∂zi
=
exp(zi −za)
Pc
k=1 exp(zk −za)."
"M
X",0.5142857142857142,• For i = a: ∂φ(z)
"M
X",0.5152709359605911,"∂zi
=
−P"
"M
X",0.516256157635468,"k̸=a exp(zk −za)
Pc
k=1 exp(zk −za)
= −Pc
k=1 exp(zk −za) + 1
Pc
k=1 exp(zk −za)"
"M
X",0.5172413793103449,"= −1 +
1
Pc
k=1 exp(zk −za) = −1 +
exp(zi −za)
Pc
k=1 exp(zk −za)."
"M
X",0.5182266009852217,We then calculate ∂2φ(z)
"M
X",0.5192118226600986,"∂zj∂zi =
∂
∂zj"
"M
X",0.5201970443349754,"
∂φ(z) ∂zi "
"M
X",0.5211822660098522,• For i = j:
"M
X",0.5221674876847291,"∂2φ(z)
∂zj∂zi
= exp(zi −za)[Pc
k=1 exp(zk −za)] −exp(zi −za) exp(zi −za)"
"M
X",0.5231527093596059,"[Pc
k=1 exp(zk −za)]2"
"M
X",0.5241379310344828,"= exp(zi −za)[Pc
k=1 exp(zk −za) −exp(zi −za)]
[Pc
k=1 exp(zk −za)]2
."
"M
X",0.5251231527093596,• For i ̸= j:
"M
X",0.5261083743842364,"∂2φ(z)
∂zj∂zi
= −exp(zj −za) exp(zi −za)"
"M
X",0.5270935960591133,"[Pc
k=1 exp(zk −za)]2
."
"M
X",0.5280788177339901,"Denote that yi = exp(zi −za) ≥0, i ∈[c], we have:"
"M
X",0.529064039408867,"• For i = j:

∂2φ(z)
∂zj∂zi"
"M
X",0.5300492610837438,"=

yi(Pc
k=1 yk −yi)
(Pc
k=1 yk)2 ."
"M
X",0.5310344827586206,"• For i ̸= j:

∂2φ(z)
∂zj∂zi"
"M
X",0.5320197044334976,"=
|yiyj|
(Pc
k=1 yk)2 ."
"M
X",0.5330049261083744,Under review as a conference paper at ICLR 2022
"M
X",0.5339901477832513,"Recall that for matrix A = (aij) ∈Rc×c: ∥A∥2 ≤∥A∥2
F = Pc
i=1
Pc
j=1 |aij|2. We have: c
X j=1"
"M
X",0.5349753694581281,"∂2φ(z)
∂zj∂zi "
"M
X",0.5359605911330049,"2
≤
1
(Pc
k=1 yk)4 "
"M
X",0.5369458128078818,"y2
i ( c
X"
"M
X",0.5379310344827586,"k=1
yk −yi)2 +
X"
"M
X",0.5389162561576355,"j̸=i
(yiyj)2  "
"M
X",0.5399014778325123,"=
1
(Pc
k=1 yk)4 "
"M
X",0.5408866995073892,"y2
i ( c
X"
"M
X",0.541871921182266,"k=1
yk)2 −2y2
i c
X"
"M
X",0.5428571428571428,"k=1
yk.yi + y4
i +
X"
"M
X",0.5438423645320197,"j̸=i
(yiyj)2  "
"M
X",0.5448275862068965,"=
1
(Pc
k=1 yk)4 """
"M
X",0.5458128078817734,"y2
i ( c
X"
"M
X",0.5467980295566502,"k=1
yk)2 −2y3
i c
X"
"M
X",0.547783251231527,"k=1
yk + y2
i c
X"
"M
X",0.548768472906404,"k=1
y2
k #"
"M
X",0.5497536945812808,"Therefore,"
"M
X",0.5507389162561577,"∥∇2φ(z)∥2 ≤ c
X i=1 c
X j=1"
"M
X",0.5517241379310345,"∂2φ(z)
∂zj∂zi  2"
"M
X",0.5527093596059113,"≤
1
(Pc
k=1 yk)4 "" ( c
X"
"M
X",0.5536945812807882,"i=1
y2
i )( c
X"
"M
X",0.554679802955665,"k=1
yk)2 −2( c
X"
"M
X",0.5556650246305419,"i=1
y3
i )( c
X"
"M
X",0.5566502463054187,"k=1
yk) + ( c
X"
"M
X",0.5576354679802956,"i=1
y2
i )( c
X"
"M
X",0.5586206896551724,"k=1
y2
k) #"
"M
X",0.5596059113300492,"≤(Pc
i=1 y2
i )(Pc
k=1 yk)2"
"M
X",0.5605911330049261,"(Pc
k=1 yk)4
≤(Pc
k=1 yk)4"
"M
X",0.5615763546798029,"(Pc
k=1 yk)4 = 1,"
"M
X",0.5625615763546798,"where the last inequality holds since ( c
X"
"M
X",0.5635467980295567,"i=1
y2
i )( c
X"
"M
X",0.5645320197044335,"k=1
y2
k) ≤( c
X"
"M
X",0.5655172413793104,"i=1
y3
i )( c
X"
"M
X",0.5665024630541872,"k=1
yk) ⇔( c
X"
"M
X",0.5674876847290641,"k=1
y2
k) ≤"
"M
X",0.5684729064039409,"v
u
u
t( c
X"
"M
X",0.5694581280788177,"i=1
y3
i )( c
X"
"M
X",0.5704433497536946,"k=1
yk),"
"M
X",0.5714285714285714,"which follows by the application of Holder inequality (35) with p = 2, q = 2, ak = y3/2
k
, and
bk = y1/2
k
(Note that yk ≥0, k ∈[c]). Hence, ∥∇2φ(z)∥≤Lφ with Lφ = 1 which is equivalent to
Lφ-smoothness of φ."
"M
X",0.5724137931034483,PROOF OF LEMMA 6
"M
X",0.5733990147783251,"Proof. Since k(·; i) are twice continuously differentiable for all i ∈[n], we have the following
Taylor approximation for each component outputs kj(·; i) where j ∈[c] and i ∈[n]:"
"M
X",0.574384236453202,kj(w(t+1); i) = kj(w(t) −η(t)v(t); i)
"M
X",0.5753694581280788,= kj(w(t); i) −Jwkj(w; i)|w=w(t)η(t)v(t) + 1
"M
X",0.5763546798029556,"2(η(t)v(t))⊤Mi,j( ˜w(t))(η(t)v(t)), (36)"
"M
X",0.5773399014778325,"where Mi,j( ˜w(t)) is the Hessian matrices of kj(·; i)at ˜w(t) and ˜w(t) = αw(t) + (1 −α)w(t+1) for
some α ∈[0, 1]."
"M
X",0.5783251231527093,Shifting this back to the original function hj(·; i) we have:
"M
X",0.5793103448275863,hj(w(t+1); i) = kj(w(t+1); i) + (hj(w(t+1); i) −kj(w(t+1); i))
"M
X",0.5802955665024631,"(36)
= kj(w(t); i) −Jwkj(w; i)|w=w(t)η(t)v(t) + 1"
"M
X",0.5812807881773399,"2(η(t)v(t))⊤Mi,j( ˜w(t))(η(t)v(t))"
"M
X",0.5822660098522168,"+ (hj(w(t+1); i) −kj(w(t+1); i)),"
"M
X",0.5832512315270936,= hj(w(t); i) −Jwkj(w; i)|w=w(t)η(t)v(t) + 1
"M
X",0.5842364532019705,"2(η(t)v(t))⊤Mi,j( ˜w(t))(η(t)v(t))"
"M
X",0.5852216748768473,"+ (hj(w(t+1); i) −kj(w(t+1); i)) + (kj(w(t); i) −hj(w(t); i)),"
"M
X",0.5862068965517241,which leads to our desired statement:
"M
X",0.587192118226601,"h(w(t+1); i) = h(w(t) −η(t)v(t); i) = h(w(t); i) −η(t)H(t)
i v(t) + ϵ(t)
i ,"
"M
X",0.5881773399014778,Under review as a conference paper at ICLR 2022 where
"M
X",0.5891625615763547,"ϵ(t)
i,j = 1"
"M
X",0.5901477832512315,"2(η(t)v(t))⊤Mi,j( ˜w(t))(η(t)v(t))"
"M
X",0.5911330049261084,"+ (hj(w(t+1); i) −kj(w(t+1); i)) + (kj(w(t); i) −hj(w(t); i)), j ∈[c],"
"M
X",0.5921182266009852,Hence we get the ﬁnal bound:
"M
X",0.593103448275862,"|ϵ(t)
i,j| ≤1 2"
"M
X",0.594088669950739,"(η(t)v(t))⊤Mi,j( ˜w(t))(η(t)v(t))"
"M
X",0.5950738916256157,+ |hj(w(t+1); i) −kj(w(t+1); i)| + |kj(w(t); i) −hj(w(t); i)|
"M
X",0.5960591133004927,"(26)
≤1 2"
"M
X",0.5970443349753695,"(η(t)v(t))⊤Mi,j( ˜w(t))(η(t)v(t))
 + 2ε, ≤1"
"M
X",0.5980295566502463,"2(η(t))2∥v(t)∥2 · ∥Mi,j( ˜w(t))∥+ 2ε"
"M
X",0.5990147783251232,"(11)
≤1"
"M
X",0.6,"2(η(t))2∥v(t)∥2G + 2ε, j ∈[c]."
"M
X",0.6009852216748769,PROOF OF COROLLARY 1
"M
X",0.6019704433497537,Proof. The proof of this corollary follows directly by the applications of Lemmas 3 and 4.
"M
X",0.6029556650246305,"E
TECHNICAL PROOFS FOR THEOREM 1"
"M
X",0.6039408866995074,"Lemma 8. Suppose that Assumption 2 holds for G > 0 and Assumption 3 holds for V > 0, and
v(t) = v(t)
∗reg. Consider η(t) = D√ε for some D > 0 and ε > 0. For i ∈[n] and 0 ≤t < T, we
have"
"M
X",0.6049261083743842,"∥ϵ(t)
i ∥2 ≤1"
"M
X",0.6059113300492611,"4c(4 + (V + 2)GD2)2ε2.
(37)"
"M
X",0.6068965517241379,"Proof. From (14), for i ∈[n], j ∈[c], and for 0 ≤t < T, by Lemma 1 and Lemma 6 we have"
"M
X",0.6078817733990147,"|ϵ(t)
i,j| ≤1"
"M
X",0.6088669950738916,2(η(t))2∥v(t)∥2G + 2ε ≤1
"M
X",0.6098522167487684,2(V + 2)GD2ε + 2ε = 1
"M
X",0.6108374384236454,"2ε(4 + (V + 2)GD2),"
"M
X",0.6118226600985222,"where the last inequality follows by the fact ∥v(t)∥2 = ∥v(t)
∗reg∥2 ≤2 + V of Lemma 2 and η(t) =
D√ε. Hence,"
"M
X",0.6128078817733991,"∥ϵ(t)
i ∥2 = c
X"
"M
X",0.6137931034482759,"j=1
|ϵ(t)
i,j|2 ≤1"
"M
X",0.6147783251231527,4c(4 + (V + 2)GD2)2ε2.
"M
X",0.6157635467980296,"Lemma 9. Let w(t) be generated by Algorithm 1 where we use the closed form solution for the
search direction. We execute Algorithm 1 for T =
β"
"M
X",0.6167487684729064,"ε outer loops for some constant β > 0. We
assume Assumption 1 holds. Suppose that Assumption 2 holds for G > 0 and Assumption 3 holds
for V > 0. We set the step size equal to η(t) = D√ε for some D > 0 and choose a learning rate
α(t)
i
≤
α
Lφ , for some α ∈(0, 1"
"M
X",0.6177339901477833,"3). For i ∈[n] and 0 ≤t < T, we have"
"M
X",0.6187192118226601,"∥h(w(t+1); i) −h∗
i ∥2 ≤(1 + ε)∥h(w(t); i) −h∗
i ∥2 −2(1 −3α)α(t)
i [φi(h(w(t); i)) −φi(h∗
i )]"
"M
X",0.6197044334975369,+ (3ε + 2)
"M
X",0.6206896551724138,"4
c(4 + (V + 2)GD2)2 · ε"
"M
X",0.6216748768472906,+ 3ε + 2
"M
X",0.6226600985221675,"ε
∥η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))∥2
(38)"
"M
X",0.6236453201970443,Under review as a conference paper at ICLR 2022
"M
X",0.6246305418719211,"Proof. Note that we have the optimal solution v(t)
∗reg for the optimization problem (15) for 0 ≤t <
T. From (12), we have, for i ∈[n],"
"M
X",0.625615763546798,"h(w(t+1); i) = h(w(t) −η(t)v(t)
∗reg; i)"
"M
X",0.6266009852216748,"= h(w(t); i) −η(t)H(t)
i v(t)
∗reg + ϵ(t)
i
= h(w(t); i) −α(t)
i ∇zφi(h(w(t); i)) + ϵ(t)
i
−[η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))]."
"M
X",0.6275862068965518,"Hence, we have"
"M
X",0.6285714285714286,"∥h(w(t+1); i) −h∗
i ∥2"
"M
X",0.6295566502463055,"= ∥h(w(t); i) −h∗
i −α(t)
i ∇zφi(h(w(t); i)) + ϵ(t)
i
−[η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))]∥2"
"M
X",0.6305418719211823,"= ∥h(w(t); i) −h∗
i ∥2 + (α(t)
i )2∥∇zφi(h(w(t); i))∥2"
"M
X",0.6315270935960591,"+ ∥ϵ(t)
i ∥2 + ∥η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))∥2"
"M
X",0.632512315270936,"−2 · ⟨h(w(t); i) −h∗
i , α(t)
i ∇zφi(h(w(t); i))⟩"
"M
X",0.6334975369458128,"+ 2 · ⟨h(w(t); i) −h∗
i , ϵ(t)
i ⟩"
"M
X",0.6344827586206897,"−2 · ⟨h(w(t); i) −h∗
i , η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))⟩"
"M
X",0.6354679802955665,"−2 · ⟨α(t)
i ∇zφi(h(w(t); i)), ϵ(t)
i ⟩"
"M
X",0.6364532019704433,"+ 2 · ⟨α(t)
i ∇zφi(h(w(t); i)), η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))⟩"
"M
X",0.6374384236453202,"−2 · ⟨ϵ(t)
i , η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))⟩,"
"M
X",0.638423645320197,"where we expand the square term. Now applying Young’s inequalities: 2|⟨u, v⟩| ≤∥u∥2"
"M
X",0.6394088669950739,ε/2 +(ε/2)∥v∥2
"M
X",0.6403940886699507,"for ε > 0 and 2|⟨u, v⟩| ≤∥u∥2 + ∥v∥2 we have:"
"M
X",0.6413793103448275,"∥h(w(t+1); i) −h∗
i ∥2"
"M
X",0.6423645320197044,"= ∥h(w(t); i) −h∗
i ∥2 + (α(t)
i )2∥∇zφi(h(w(t); i))∥2"
"M
X",0.6433497536945813,"+ ∥ϵ(t)
i ∥2 + ∥η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))∥2"
"M
X",0.6443349753694582,"−2α(t)
i ⟨h(w(t); i) −h∗
i , ∇zφi(h(w(t); i))⟩ + ε"
"M
X",0.645320197044335,"2∥h(w(t); i) −h∗
i ∥2 + 2"
"M
X",0.6463054187192119,"ε∥ϵ(t)
i ∥2 + ε"
"M
X",0.6472906403940887,"2∥h(w(t); i) −h∗
i ∥2 + 2"
"M
X",0.6482758620689655,"ε∥η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))∥2"
"M
X",0.6492610837438424,"+ 2(α(t)
i )2∥∇zφi(h(w(t); i))∥2 + 2∥ϵ(t)
i ∥2"
"M
X",0.6502463054187192,"+ 2∥η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))∥2"
"M
X",0.6512315270935961,"(22)
≤(1 + ε)∥h(w(t); i) −h∗
i ∥2 + 3(α(t)
i )2∥∇zφi(h(w(t); i))∥2"
"M
X",0.6522167487684729,"+

3 + 2 ε"
"M
X",0.6532019704433497,"
∥ϵ(t)
i ∥2 +

3 + 2 ε"
"M
X",0.6541871921182266,"
∥η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))∥2"
"M
X",0.6551724137931034,"−2α(t)
i [φi(h(w(t); i)) −φi(h∗
i )]"
"M
X",0.6561576354679803,"Note that from (23) we get that ∥∇zφi(h(w(t); i))∥2 ≤2Lφ[φi(h(w(t); i))−φi(h∗
i )]. Applying this
and using the fact that α(t)
i
≤
α
Lφ , for some α ∈(0, 1"
"M
X",0.6571428571428571,"3), we are able to derive:"
"M
X",0.6581280788177339,"∥h(w(t+1); i) −h∗
i ∥2"
"M
X",0.6591133004926109,"≤(1 + ε)∥h(w(t); i) −h∗
i ∥2 −2(1 −3α)α(t)
i [φi(h(w(t); i)) −φi(h∗
i )]"
"M
X",0.6600985221674877,+ 3ε + 2
"M
X",0.6610837438423646,"ε
∥ϵ(t)
i ∥2 + 3ε + 2"
"M
X",0.6620689655172414,"ε
∥η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))∥2"
"M
X",0.6630541871921182,"≤(1 + ε)∥h(w(t); i) −h∗
i ∥2 −2(1 −3α)α(t)
i [φi(h(w(t); i)) −φi(h∗
i )]"
"M
X",0.6640394088669951,Under review as a conference paper at ICLR 2022
"M
X",0.6650246305418719,+ 3ε + 2
"M
X",0.6660098522167488,"ε
1
4c(4 + (V + 2)GD2)2ε2 + 3ε + 2"
"M
X",0.6669950738916256,"ε
∥η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))∥2"
"M
X",0.6679802955665025,where the last inequality follows by Lemma 8.
"M
X",0.6689655172413793,"Lemma 10. Let w(t) be generated by Algorithm 1 where we use the closed form solution for the
search direction. We execute Algorithm 1 for T =
β"
"M
X",0.6699507389162561,"ε outer loops for some constant β > 0. We
assume Assumption 1 holds. Suppose that Assumption 2 holds for G > 0 and Assumption 3 holds
for V > 0. We set the step size equal to η(t) = D√ε for some D > 0 and choose a learning rate
α(t)
i
= (1 + ε)α(t−1)
i
= (1 + ε)tα(0)
i . Based on β, we deﬁne α(0)
i
=
α
eβLφ with α ∈(0, 1 3)."
"M
X",0.670935960591133,We have
T,0.6719211822660098,"1
T"
T,0.6729064039408867,"T −1
X t=0"
N,0.6738916256157635,"1
n n
X"
N,0.6748768472906403,"i=1
[f(w(t); i) −φi(h∗
i )] ≤eβLφ(1 + ε)"
N,0.6758620689655173,"2(1 −3α)αβ · 1 n n
X"
N,0.6768472906403941,"i=1
∥h(w(0); i) −h∗
i ∥2 · ε"
N,0.677832512315271,"+
eβLφ
8α(1 −3α)(3ε + 2)

c(4 + (V + 2)GD2)2 + 8 + 4V

· ε. (39)"
N,0.6788177339901478,"Proof. Rearranging the terms in Lemma 9, we have"
N,0.6798029556650246,"φi(h(w(t); i)) −φi(h∗
i ) ≤
1
2(1 −3α)"
N,0.6807881773399015,(1 + ε)
N,0.6817733990147783,"α(t)
i
∥h(w(t); i) −h∗
i ∥2 −
1"
N,0.6827586206896552,"α(t)
i
∥h(w(t+1); i) −h∗
i ∥2
!"
N,0.683743842364532,"+
1
8(1 −3α) ·
1"
N,0.6847290640394089,"α(t)
i
· ε(3ε + 2)c(4 + (V + 2)GD2)2"
N,0.6857142857142857,"+
1
2(1 −3α) ·
1"
N,0.6866995073891625,"α(t)
i
· 3ε + 2"
N,0.6876847290640394,"ε
∥η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))∥2"
N,0.6886699507389162,"≤
1
2(1 −3α)"
N,0.6896551724137931,(1 + ε)
N,0.69064039408867,"α(t)
i
∥h(w(t); i) −h∗
i ∥2 −(1 + ε)"
N,0.6916256157635468,"α(t+1)
i
∥h(w(t+1); i) −h∗
i ∥2
!"
N,0.6926108374384237,"+
eβLφ
8α(1 −3α) · ε(3ε + 2)c(4 + (V + 2)GD2)2"
N,0.6935960591133005,"+
eβLφ
2α(1 −3α) · 3ε + 2"
N,0.6945812807881774,"ε
∥η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))∥2."
N,0.6955665024630542,"The last inequality follows because the learning rate satisﬁes α(0)
i
=
α
eβLφ ≤
α
Lφ and for t ="
N,0.696551724137931,"1, . . . , T = β"
N,0.6975369458128079,ε for some β > 0
N,0.6985221674876847,"α(t)
i
= (1 + ε)α(t−1)
i
= (1 + ε)tα(0)
i
≤(1 + ε)T α(0)
i
= (1 + ε)β/ε
α
eβLφ
≤α Lφ
,"
N,0.6995073891625616,"since (1 + x)1/x ≤e, x > 0. Moreover, we have
1
α(t)
i
≤
1
α(0)
i
= eβLφ"
N,0.7004926108374384,"α , t = 0, . . . , T −1."
N,0.7014778325123153,"Taking the average sum from t = 0, . . . , T −1, we have"
T,0.7024630541871921,"1
T"
T,0.7034482758620689,"T −1
X"
T,0.7044334975369458,"t=0
[φi(h(w(t); i)) −φi(h∗
i )] ≤
1
2(1 −3α)T · (1 + ε)"
T,0.7054187192118226,"α(0)
i
∥h(w(0); i) −h∗
i ∥2"
T,0.7064039408866996,"+
eβLφ
8α(1 −3α) · ε(3ε + 2)c(4 + (V + 2)GD2)2"
T,0.7073891625615764,"+
eβLφ
2α(1 −3α) · 3ε + 2 ε
1
T"
T,0.7083743842364532,"T −1
X"
T,0.7093596059113301,"t=0
∥η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))∥2"
T,0.7103448275862069,= eβLφ(1 + ε)
T,0.7113300492610838,"2(1 −3α)αβ ε · ∥h(w(0); i) −h∗
i ∥2"
T,0.7123152709359606,Under review as a conference paper at ICLR 2022
T,0.7133004926108374,"+
eβLφ
8α(1 −3α) · ε(3ε + 2)c(4 + (V + 2)GD2)2"
T,0.7142857142857143,"+
eβLφ
2α(1 −3α) · 3ε + 2 ε
1
T"
T,0.7152709359605911,"T −1
X"
T,0.716256157635468,"t=0
∥η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))∥2."
T,0.7172413793103448,"Taking the average sum from i = 1, . . . , n, we have"
T,0.7182266009852217,"1
T"
T,0.7192118226600985,"T −1
X t=0"
N,0.7201970443349753,"1
n n
X"
N,0.7211822660098522,"i=1
[φi(h(w(t); i)) −φi(h∗
i )]"
N,0.722167487684729,≤eβLφ(1 + ε)
N,0.723152709359606,"2(1 −3α)αβ ε · 1 n n
X"
N,0.7241379310344828,"i=1
∥h(w(0); i) −h∗
i ∥2"
N,0.7251231527093596,"+
eβLφ
8α(1 −3α) · ε(3ε + 2)c(4 + (V + 2)GD2)2"
N,0.7261083743842365,"+
eβLφ
2α(1 −3α) · 3ε + 2 ε
1
T"
N,0.7270935960591133,"T −1
X t=0"
N,0.7280788177339902,"1
n n
X"
N,0.729064039408867,"i=1
∥η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))∥2"
N,0.7300492610837438,"(17)
≤
eβLφ(1 + ε)
2(1 −3α)αβ ε · 1 n n
X"
N,0.7310344827586207,"i=1
∥h(w(0); i) −h∗
i ∥2"
N,0.7320197044334975,"+
eβLφ
8α(1 −3α) · ε(3ε + 2)c(4 + (V + 2)GD2)2"
N,0.7330049261083744,"+
eβLφ
2α(1 −3α) · 3ε + 2"
N,0.7339901477832512,"ε
(2 + V )ε2.
(40)"
N,0.734975369458128,Note that
T,0.7359605911330049,"1
T"
T,0.7369458128078817,"T −1
X t=0"
N,0.7379310344827587,"1
n n
X"
N,0.7389162561576355,"i=1
[φi(h(w(t); i)) −φi(h∗
i )] = 1 T"
N,0.7399014778325124,"T −1
X t=0"
N,0.7408866995073892,"1
n n
X"
N,0.741871921182266,"i=1
[f(w(t); i) −φi(h∗
i )].
(41)"
N,0.7428571428571429,"Therefore, applying (41) to (40), we have"
T,0.7438423645320197,"1
T"
T,0.7448275862068966,"T −1
X t=0"
N,0.7458128078817734,"1
n n
X"
N,0.7467980295566502,"i=1
[f(w(t); i) −φi(h∗
i )]"
N,0.7477832512315271,≤eβLφ(1 + ε)
N,0.7487684729064039,"2(1 −3α)αβ · 1 n n
X"
N,0.7497536945812808,"i=1
∥h(w(0); i) −h∗
i ∥2 · ε"
N,0.7507389162561576,"+
eβLφ
8α(1 −3α)(3ε + 2)

c(4 + (V + 2)GD2)2 + 8 + 4V

· ε."
N,0.7517241379310344,which is our desired result.
N,0.7527093596059113,PROOF OF THEOREM 1
N,0.7536945812807881,Proof. We have
N,0.7546798029556651,"F∗= min
w∈Rd F(w) = min
w∈Rd"
N,0.7556650246305419,"1
n n
X"
N,0.7566502463054188,"i=1
fi(w) ! = 1"
N,0.7576354679802956,"n min
w∈Rd n
X"
N,0.7586206896551724,"i=1
fi(w) ! ≥1 n n
X"
N,0.7596059113300493,"i=1
min
w∈Rd (fi(w)) = 1 n n
X"
N,0.7605911330049261,"i=1
f ∗
i ≥1 n n
X"
N,0.761576354679803,"i=1
φi(h∗
i ).
(42)"
N,0.7625615763546798,Hence F∗−1
N,0.7635467980295566,"n
Pn
i=1 φi(h∗
i ) ≥0. Therefore"
T,0.7645320197044335,"1
T T
X"
T,0.7655172413793103,"t=1
[F(w(t)) −F∗] = 1 T T
X t=1"
N,0.7665024630541872,"1
n n
X"
N,0.767487684729064,"i=1
[f(w(t); i) −φi(h∗
i )] − "" F∗−1 n n
X"
N,0.7684729064039408,"i=1
φi(h∗
i ) #!"
N,0.7694581280788177,"Under review as a conference paper at ICLR 2022 ≤1 T T
X t=1"
N,0.7704433497536946,"1
n n
X"
N,0.7714285714285715,"i=1
[f(w(t); i) −φi(h∗
i )]"
N,0.7724137931034483,"(39)
≤
eβLφ(1 + ε)
2(1 −3α)αβ · 1 n n
X"
N,0.7733990147783252,"i=1
∥h(w(0); i) −h∗
i ∥2 · ε"
N,0.774384236453202,+ eβLφ(3ε + 2)
N,0.7753694581280788,"8α(1 −3α)

c(4 + (V + 2)GD2)2 + 8 + 4V

· ε."
N,0.7763546798029557,"F
TECHNICAL PROOFS FOR THEOREM 2"
N,0.7773399014778325,"Lemma 11. For 0 ≤t < T, suppose that Assumption 3 holds for V ≥0 and v(t) satisﬁes (19).
Then"
N,0.7783251231527094,∥v(t)∥2 ≤2(ε2 + V + 2).
N,0.7793103448275862,"Proof. From ∥v(t) −v(t)
∗reg∥≤ε. Using ∥a∥2 ≤2∥a −b∥2 + 2∥b∥2, we have"
N,0.780295566502463,"∥v(t)∥2 ≤2∥v(t) −v(t)
∗reg∥2 + 2∥v(t)
∗reg∥2 (19)
≤2ε2 + 4 + 2V."
N,0.7812807881773399,"where the last inequality follows since ∥v(t)
∗reg∥2 ≤2 + V for some V > 0 in Lemma 2."
N,0.7822660098522167,"Lemma 12. Suppose that Assumption 2 holds for G > 0 and Assumption 3 holds for V > 0.
Consider η(t) = D√ε for some D > 0 and ε > 0. For i ∈[n] and 0 ≤t < T, we have"
N,0.7832512315270936,"∥ϵ(t)
i ∥2 ≤c(2 + (V + ε2 + 2)GD2)2ε2.
(43)"
N,0.7842364532019704,"Proof. From (14), for i ∈[n], j ∈[c], and for 0 ≤t < T, by Lemma 1 and Lemma 6 we have"
N,0.7852216748768472,"|ϵ(t)
i,j| ≤1"
N,0.7862068965517242,2(η(t))2∥v(t)∥2G + 2ε ≤1
N,0.787192118226601,"22(ε2 + V + 2)GD2ε + 2ε = ε(2 + (V + ε2 + 2)GD2),"
N,0.7881773399014779,"where the last inequality follows by the application of Lemma 11 and η(t) = D√ε. Hence,"
N,0.7891625615763547,"∥ϵ(t)
i ∥2 = c
X"
N,0.7901477832512315,"j=1
|ϵ(t)
i,j|2 ≤c(2 + (V + ε2 + 2)GD2)2ε2."
N,0.7911330049261084,"Lemma 13. Let w(t) be generated by Algorithm 2 where v(t) satisﬁes (19). We execute Algorithm
2 for T = β"
N,0.7921182266009852,"ε outer loops for some constant β > 0. We assume Assumption 1 holds. Suppose that
Assumption 2 holds for G > 0, Assumption 3 holds for V > 0 and Assumption 4 holds for H > 0.
We set the step size equal to η(t) = D√ε for some D > 0 and choose a learning rate α(t)
i
≤
α
Lφ ,
for some α ∈(0, 1"
N,0.7931034482758621,"4). For i ∈[n] and 0 ≤t < T, we have"
N,0.7940886699507389,"∥h(w(t+1); i) −h∗
i ∥2 ≤(1 + ε)∥h(w(t); i) −h∗
i ∥2 −2(1 −4α)α(t)
i [φi(h(w(t); i)) −φi(h∗
i )]"
N,0.7950738916256158,"+ ε(4ε + 3)

D2H2 + c(2 + (V + ε2 + 2)GD2)2"
N,0.7960591133004926,+ 4ε + 3
N,0.7970443349753694,"ε
∥η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))∥2
(44)"
N,0.7980295566502463,Under review as a conference paper at ICLR 2022
N,0.7990147783251231,"Proof. Note that v(t) is obtained from the optimization problem (15) for 0 ≤t < T. From (9), we
have, for i ∈[n],"
N,0.8,h(w(t+1); i) = h(w(t) −η(t)v(t); i)
N,0.8009852216748768,"= h(w(t); i) −η(t)H(t)
i v(t) + ϵ(t)
i
= h(w(t); i) −η(t)H(t)
i (v(t) −v(t)
∗reg) −α(t)
i ∇zφi(h(w(t); i)) + ϵ(t)
i
−[η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))]."
N,0.8019704433497536,"Hence, we have"
N,0.8029556650246306,"∥h(w(t+1); i) −h∗
i ∥2"
N,0.8039408866995074,"= ∥h(w(t); i) −h∗
i −η(t)H(t)
i (v(t) −v(t)
∗reg) −α(t)
i ∇zφi(h(w(t); i))"
N,0.8049261083743843,"+ ϵ(t)
i
−[η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))]∥2"
N,0.8059113300492611,"= ∥h(w(t); i) −h∗
i ∥2 + ∥η(t)H(t)
i (v(t) −v(t)
∗reg)∥2 + (α(t)
i )2∥∇zφi(h(w(t); i))∥2"
N,0.8068965517241379,"+ ∥ϵ(t)
i ∥2 + ∥η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))∥2"
N,0.8078817733990148,"−2 · ⟨h(w(t); i) −h∗
i , η(t)H(t)
i (v(t) −v(t)
∗reg)⟩"
N,0.8088669950738916,"−2 · ⟨h(w(t); i) −h∗
i , α(t)
i ∇zφi(h(w(t); i))⟩"
N,0.8098522167487685,"+ 2 · ⟨h(w(t); i) −h∗
i , ϵ(t)
i ⟩"
N,0.8108374384236453,"−2 · ⟨h(w(t); i) −h∗
i , η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))⟩"
N,0.8118226600985222,"+ 2 · ⟨η(t)H(t)
i (v(t) −v(t)
∗reg), α(t)
i ∇zφi(h(w(t); i))⟩"
N,0.812807881773399,"−2 · ⟨η(t)H(t)
i (v(t) −v(t)
∗reg), ϵ(t)
i ⟩"
N,0.8137931034482758,"+ 2 · ⟨η(t)H(t)
i (v(t) −v(t)
∗reg), η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))⟩"
N,0.8147783251231527,"−2 · ⟨α(t)
i ∇zφi(h(w(t); i)), ϵ(t)
i ⟩"
N,0.8157635467980295,"+ 2 · ⟨α(t)
i ∇zφi(h(w(t); i)), η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))⟩"
N,0.8167487684729065,"−2 · ⟨ϵ(t)
i , η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))⟩,"
N,0.8177339901477833,"where we expand the square term. Now applying Young’s inequalities: 2|⟨u, v⟩| ≤∥u∥2"
N,0.81871921182266,ε/3 +(ε/3)∥v∥2
N,0.819704433497537,"for ε > 0 and 2|⟨u, v⟩| ≤∥u∥2 + ∥v∥2 we have:"
N,0.8206896551724138,"∥h(w(t+1); i) −h∗
i ∥2"
N,0.8216748768472907,"= ∥h(w(t); i) −h∗
i ∥2 + ∥η(t)H(t)
i (v(t) −v(t)
∗reg)∥2 + (α(t)
i )2∥∇zφi(h(w(t); i))∥2"
N,0.8226600985221675,"+ ∥ϵ(t)
i ∥2 + ∥η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))∥2 + ε"
N,0.8236453201970443,"3∥h(w(t); i) −h∗
i ∥2 + 3"
N,0.8246305418719212,"ε∥η(t)H(t)
i (v(t) −v(t)
∗reg)∥2"
N,0.825615763546798,"−2α(t)
i ⟨h(w(t); i) −h∗
i , ∇zφi(h(w(t); i))⟩ + ε"
N,0.8266009852216749,"3∥h(w(t); i) −h∗
i ∥2 + 3"
N,0.8275862068965517,"ε∥ϵ(t)
i ∥2 + ε"
N,0.8285714285714286,"3∥h(w(t); i) −h∗
i ∥2 + 3"
N,0.8295566502463054,"ε∥η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))∥2"
N,0.8305418719211822,"+ 3(η(t))2∥H(t)
i (v(t) −v(t)
∗reg)∥2 + 3(α(t)
i )2∥∇zφi(h(w(t); i))∥2 + 3∥ϵ(t)
i ∥2"
N,0.8315270935960591,"+ 3∥η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))∥2"
N,0.8325123152709359,"(22)
≤(1 + ε)∥h(w(t); i) −h∗
i ∥2 + 4(α(t)
i )2∥∇zφi(h(w(t); i))∥2"
N,0.8334975369458129,"+

4 + 3 ε"
N,0.8344827586206897,"
∥η(t)H(t)
i (v(t) −v(t)
∗reg)∥2 +

4 + 3 ε"
N,0.8354679802955665,"
∥ϵ(t)
i ∥2"
N,0.8364532019704434,Under review as a conference paper at ICLR 2022
N,0.8374384236453202,"+

4 + 3 ε"
N,0.8384236453201971,"
∥η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))∥2"
N,0.8394088669950739,"−2α(t)
i [φi(h(w(t); i)) −φi(h∗
i )]"
N,0.8403940886699507,"Note that from (23) we get that ∥∇zφi(h(w(t); i))∥2 ≤2Lφ[φi(h(w(t); i))−φi(h∗
i )]. Applying this
and using the fact that α(t)
i
≤
α
Lφ , for some α ∈(0, 1"
N,0.8413793103448276,"4), we are able to derive:"
N,0.8423645320197044,"∥h(w(t+1); i) −h∗
i ∥2"
N,0.8433497536945813,"≤(1 + ε)∥h(w(t); i) −h∗
i ∥2 −2(1 −4α)α(t)
i [φi(h(w(t); i)) −φi(h∗
i )]"
N,0.8443349753694581,+ 4ε + 3
N,0.8453201970443349,"ε
∥η(t)H(t)
i (v(t) −v(t)
∗reg)∥2 + 4ε + 3"
N,0.8463054187192118,"ε
∥ϵ(t)
i ∥2"
N,0.8472906403940886,+ 4ε + 3
N,0.8482758620689655,"ε
∥η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))∥2"
N,0.8492610837438423,"(a)
≤(1 + ε)∥h(w(t); i) −h∗
i ∥2 −2(1 −4α)α(t)
i [φi(h(w(t); i)) −φi(h∗
i )]"
N,0.8502463054187193,+ 4ε + 3
N,0.8512315270935961,"ε
D2εH2"
N,0.8522167487684729,"ε ∥v(t) −v(t)
∗reg∥2 + 4ε + 3"
N,0.8532019704433498,"ε
∥ϵ(t)
i ∥2"
N,0.8541871921182266,+ 4ε + 3
N,0.8551724137931035,"ε
∥η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))∥2"
N,0.8561576354679803,"(b)
≤(1 + ε)∥h(w(t); i) −h∗
i ∥2 −2(1 −4α)α(t)
i [φi(h(w(t); i)) −φi(h∗
i )]"
N,0.8571428571428571,+ 4ε + 3
N,0.858128078817734,"ε
D2H2 · ε2 + 4ε + 3"
N,0.8591133004926108,"ε
· c(2 + (V + ε2 + 2)GD2)2ε2"
N,0.8600985221674877,+ 4ε + 3
N,0.8610837438423645,"ε
∥η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))∥2"
N,0.8620689655172413,"= (1 + ε)∥h(w(t); i) −h∗
i ∥2 −2(1 −4α)α(t)
i [φi(h(w(t); i)) −φi(h∗
i )]"
N,0.8630541871921182,"+ ε(4ε + 3)

D2H2 + c(2 + (V + ε2 + 2)GD2)2"
N,0.864039408866995,+ 4ε + 3
N,0.865024630541872,"ε
∥η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))∥2"
N,0.8660098522167488,"where (a) follows by using matrix vector inequality ∥Hv∥≤∥H∥∥v∥, where H ∈Rc×d and
v ∈Rd and Assumption 4 in (20) and η(t) = D√ε for some D > 0 and ε > 0; (b) follows by the
fact that ∥v(t) −v(t)
∗reg∥2 ≤ε2 in (19) and Lemma 12."
N,0.8669950738916257,"Lemma 14. Let w(t) be generated by Algorithm 2 where v(t) satisﬁes (19). We execute Algorithm
2 for T =
β"
N,0.8679802955665025,"ε outer loops for some constant β > 0. We assume Assumption 1 holds. Suppose
that Assumption 2 holds for G > 0, Assumption 3 holds for V > 0 and Assumption 4 holds for
H > 0.We set the step size equal to η(t) = D√ε for some D > 0 and choose a learning rate
α(t)
i
= (1 + ε)α(t−1)
i
= (1 + ε)tα(0)
i . Based on β, we deﬁne α(0)
i
=
α
eβLφ with α ∈(0, 1 4)."
N,0.8689655172413793,We have
T,0.8699507389162562,"1
T"
T,0.870935960591133,"T −1
X t=0"
N,0.8719211822660099,"1
n n
X"
N,0.8729064039408867,"i=1
[f(w(t); i) −φi(h∗
i )]"
N,0.8738916256157635,≤eβLφ(1 + ε)
N,0.8748768472906404,"2(1 −4α)αβ · 1 n n
X"
N,0.8758620689655172,"i=1
∥h(w(0); i) −h∗
i ∥2 · ε"
N,0.8768472906403941,+ eβLφ(4ε + 3)
N,0.8778325123152709,"2α(1 −4α)

D2H2 + c(2 + (V + ε2 + 2)GD2)2 + 2 + V

· ε.
(45)"
N,0.8788177339901477,"Proof. Rearranging the terms in Lemma 13, we have"
N,0.8798029556650246,"φi(h(w(t); i)) −φi(h∗
i ) ≤
1
2(1 −4α)"
N,0.8807881773399014,(1 + ε)
N,0.8817733990147784,"α(t)
i
∥h(w(t); i) −h∗
i ∥2 −
1"
N,0.8827586206896552,"α(t)
i
∥h(w(t+1); i) −h∗
i ∥2
!"
N,0.8837438423645321,Under review as a conference paper at ICLR 2022
N,0.8847290640394089,"+
1
2(1 −4α) ·
1"
N,0.8857142857142857,"α(t)
i
· ε(4ε + 3)

D2H2 + c(2 + (V + ε2 + 2)GD2)2"
N,0.8866995073891626,"+
1
2(1 −4α) ·
1"
N,0.8876847290640394,"α(t)
i
· 4ε + 3"
N,0.8886699507389163,"ε
∥η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))∥2"
N,0.8896551724137931,"≤
1
2(1 −4α)"
N,0.8906403940886699,(1 + ε)
N,0.8916256157635468,"α(t)
i
∥h(w(t); i) −h∗
i ∥2 −(1 + ε)"
N,0.8926108374384236,"α(t+1)
i
∥h(w(t+1); i) −h∗
i ∥2
!"
N,0.8935960591133005,"+
eβLφ
2α(1 −4α) · ε(4ε + 3)

D2H2 + c(2 + (V + ε2 + 2)GD2)2"
N,0.8945812807881773,"+
eβLφ
2α(1 −4α) · 4ε + 3"
N,0.8955665024630541,"ε
∥η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))∥2. (46)"
N,0.896551724137931,"The last inequality follows because the learning rate satisﬁes α(0)
i
=
α
eβLφ ≤
α
Lφ and for t ="
N,0.8975369458128079,"1, . . . , T = β"
N,0.8985221674876848,ε for some β > 0
N,0.8995073891625616,"α(t)
i
= (1 + ε)α(t−1)
i
= (1 + ε)tα(0)
i
≤(1 + ε)T α(0)
i
= (1 + ε)β/ε
α
eβLφ
≤α Lφ
,"
N,0.9004926108374385,"since (1 + x)1/x ≤e, x > 0. Moreover, we have
1
α(t)
i
≤
1
α(0)
i
= eβLφ"
N,0.9014778325123153,"α , t = 0, . . . , T −1."
N,0.9024630541871921,"Taking the average sum from t = 0, . . . , T −1, we have"
T,0.903448275862069,"1
T"
T,0.9044334975369458,"T −1
X"
T,0.9054187192118227,"t=0
[φi(h(w(t); i)) −φi(h∗
i )] ≤
1
2(1 −4α)T · (1 + ε)"
T,0.9064039408866995,"α(0)
i
∥h(w(0); i) −h∗
i ∥2"
T,0.9073891625615763,"+
eβLφ
2α(1 −4α) · ε(4ε + 3)

D2H2 + c(2 + (V + ε2 + 2)GD2)2"
T,0.9083743842364532,"+
eβLφ
2α(1 −4α) · 4ε + 3 ε
1
T"
T,0.90935960591133,"T −1
X"
T,0.9103448275862069,"t=0
∥η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))∥2"
T,0.9113300492610837,= eβLφ(1 + ε)
T,0.9123152709359605,"2(1 −4α)αβ ε · ∥h(w(0); i) −h∗
i ∥2"
T,0.9133004926108375,"+
eβLφ
2α(1 −4α) · ε(4ε + 3)

D2H2 + c(2 + (V + ε2 + 2)GD2)2"
T,0.9142857142857143,"+
eβLφ
2α(1 −4α) · 4ε + 3 ε
1
T"
T,0.9152709359605912,"T −1
X"
T,0.916256157635468,"t=0
∥η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))∥2."
T,0.9172413793103448,"Taking the average sum from i = 1, . . . , n, we have"
T,0.9182266009852217,"1
T"
T,0.9192118226600985,"T −1
X t=0"
N,0.9201970443349754,"1
n n
X"
N,0.9211822660098522,"i=1
[φi(h(w(t); i)) −φi(h∗
i )]"
N,0.9221674876847291,≤eβLφ(1 + ε)
N,0.9231527093596059,"2(1 −4α)αβ ε · 1 n n
X"
N,0.9241379310344827,"i=1
∥h(w(0); i) −h∗
i ∥2"
N,0.9251231527093596,"+
eβLφ
2α(1 −4α) · ε(4ε + 3)

D2H2 + c(2 + (V + ε2 + 2)GD2)2"
N,0.9261083743842364,"+
eβLφ
2α(1 −4α) · 4ε + 3 ε
1
T"
N,0.9270935960591133,"T −1
X t=0"
N,0.9280788177339901,"1
n n
X"
N,0.929064039408867,"i=1
∥η(t)H(t)
i v(t)
∗reg −α(t)
i ∇zφi(h(w(t); i))∥2"
N,0.9300492610837439,"(17)
≤
eβLφ(1 + ε)
2(1 −4α)αβ ε · 1 n n
X"
N,0.9310344827586207,"i=1
∥h(w(0); i) −h∗
i ∥2"
N,0.9320197044334976,Under review as a conference paper at ICLR 2022
N,0.9330049261083744,"+
eβLφ
2α(1 −4α) · ε(4ε + 3)

D2H2 + c(2 + (V + ε2 + 2)GD2)2"
N,0.9339901477832512,"+
eβLφ
2α(1 −4α) · 4ε + 3"
N,0.9349753694581281,"ε
(2 + V )ε2.
(47)"
N,0.9359605911330049,Note that
T,0.9369458128078818,"1
T"
T,0.9379310344827586,"T −1
X t=0"
N,0.9389162561576355,"1
n n
X"
N,0.9399014778325123,"i=1
[φi(h(w(t); i)) −φi(h∗
i )] = 1 T"
N,0.9408866995073891,"T −1
X t=0"
N,0.941871921182266,"1
n n
X"
N,0.9428571428571428,"i=1
[f(w(t); i) −φi(h∗
i )].
(48)"
N,0.9438423645320198,"Therefore, applying (48) to (47), we have"
T,0.9448275862068966,"1
T"
T,0.9458128078817734,"T −1
X t=0"
N,0.9467980295566503,"1
n n
X"
N,0.9477832512315271,"i=1
[f(w(t); i) −φi(h∗
i )]"
N,0.948768472906404,≤eβLφ(1 + ε)
N,0.9497536945812808,"2(1 −4α)αβ · 1 n n
X"
N,0.9507389162561576,"i=1
∥h(w(0); i) −h∗
i ∥2 · ε"
N,0.9517241379310345,"+
eβLφ
2α(1 −4α)(4ε + 3)

D2H2 + c(2 + (V + ε2 + 2)GD2)2 + 2 + V

· ε."
N,0.9527093596059113,PROOF OF THEOREM 2
N,0.9536945812807882,Proof. From (42) we have F∗−1
N,0.954679802955665,"n
Pn
i=1 φi(h∗
i ) ≥0. This leads to"
T,0.9556650246305419,"1
T T
X"
T,0.9566502463054187,"t=1
[F(w(t)) −F∗] = 1 T T
X t=1"
N,0.9576354679802955,"1
n n
X"
N,0.9586206896551724,"i=1
[f(w(t); i) −φi(h∗
i )] − "" F∗−1 n n
X"
N,0.9596059113300492,"i=1
φi(h∗
i ) #!"
N,0.9605911330049262,"(42)
≤
1
T T
X t=1"
N,0.961576354679803,"1
n n
X"
N,0.9625615763546798,"i=1
[f(w(t); i) −φi(h∗
i )]"
N,0.9635467980295567,"(45)
≤
eβLφ(1 + ε)
2(1 −4α)αβ · 1 n n
X"
N,0.9645320197044335,"i=1
∥h(w(0); i) −h∗
i ∥2 · ε"
N,0.9655172413793104,+ eβLφ(4ε + 3)
N,0.9665024630541872,"2α(1 −4α)

D2H2 + c(2 + (V + ε2 + 2)GD2)2 + 2 + V

· ε. (49)"
N,0.967487684729064,PROOF OF COROLLARY 2
N,0.9684729064039409,"Proof. For each iteration 0 ≤t < T, we need to ﬁnd v(t) satisfying the following criteria:"
N,0.9694581280788177,"∥v(t) −v(t)
∗reg∥2 ≤ε2,"
N,0.9704433497536946,for some ε > 0. Using Gradient Descent we need O(n L
N,0.9714285714285714,µ log( 1
N,0.9724137931034482,ε2 )) = O(2n L
N,0.9733990147783251,µ log( 1
N,0.9743842364532019,"ε)) number of
gradient evaluations (Nesterov, 2004), where L and µ = ε2 are the smooth and strongly convex
constants, respectively, of Ψ. Let"
N,0.9753694581280788,ψi(v) = 1
N,0.9763546798029556,"2∥η(t)H(t)
i v −α(t)
i ∇zφi(h(w(t); i))∥2, i ∈[n].
(50)"
N,0.9773399014778326,"Then, for any v ∈Rc"
N,0.9783251231527094,"∇vψi(v) = η(t)H(t)
i
⊤[η(t)H(t)
i v −α(t)
i ∇zφi(h(w(t); i))], i ∈[n].
(51)"
N,0.9793103448275862,Under review as a conference paper at ICLR 2022
N,0.9802955665024631,"Consider η(t) = D√ε for some D > 0 and ε > 0, we have for i ∈[n] and 0 ≤t < T"
N,0.9812807881773399,"∥∇2
vψi(v)∥= (η(t))2∥H(t)
i
⊤H(t)
i ∥≤(η(t))2∥H(t)
i ∥· ∥H(t)
i ∥
(20)
≤D2H2."
N,0.9822660098522168,"Hence, ∥∇2
vΦ(v)∥≤∥∇2
vψi(v)∥+ ε2 for any v ∈Rc which implies that L = D2H2 + ε2 (Nes-
terov (2004)) and L"
N,0.9832512315270936,"µ =
D2H2+ε2"
N,0.9842364532019704,"ε2
. Therefore, the complexity to ﬁnd v(t) for each iteration t is"
N,0.9852216748768473,O(2n D2H2+ε2
N,0.9862068965517241,"ε2
log( 1 ε))."
N,0.987192118226601,"Let us choose 0 < ε ≤1. From (49), we have"
T,0.9881773399014778,"1
T"
T,0.9891625615763546,"T −1
X"
T,0.9901477832512315,"t=0
[F(w(t)) −F∗] ≤
eβLφ
(1 −4α)αβ · 1 n n
X"
T,0.9911330049261083,"i=1
∥h(w(0); i) −h∗
i ∥2 · ε"
T,0.9921182266009853,"+
7eβLφ
2α(1 −4α)

D2H2 + c(2 + (V + 3)GD2)2 + 2 + V

· ε = Nε, where"
T,0.993103448275862,"N =
eβLφ
(1−4α)αβ
1
n n
X"
T,0.994088669950739,"i=1
∥h(w(0); i) −h∗
i ∥2 +
7eβLφ
2α(1−4α)

D2H2 + c(2 + (V + 3)GD2)2 + 2 + V

."
T,0.9950738916256158,"Let ˆε = Nε with 0 < ˆε ≤N.
Then, we need T =
Nβ"
T,0.9960591133004926,"ˆε
for some β > 0 to guarantee
min0≤t≤T −1[F(w(t)) −F∗] ≤
1
T
PT −1
t=0 [F(w(t)) −F∗] ≤ˆε. Hence, the total complexity is"
T,0.9970443349753695,"O

n N3β"
T,0.9980295566502463,ˆε3 (D2H2 + (ˆε2/N)) log( N
T,0.9990147783251232,"ˆε )

."
