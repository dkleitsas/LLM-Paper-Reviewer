Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.00398406374501992,"Discovering successful coordinated behaviors is a central challenge in Multi-Agent
Reinforcement Learning (MARL) since it requires exploring a joint action space
that grows exponentially with the number of agents. In this paper, we propose
a mechanism for achieving sufﬁcient exploration and coordination in a team of
agents. Speciﬁcally, agents are rewarded for contributing to a more diversiﬁed team
behavior by employing proper intrinsic motivation functions. To learn meaningful
coordination protocols, we structure agents’ interactions by introducing a novel
framework, where at each timestep, an agent simulates counterfactual rollouts
of its policy and, through a sequence of computations, assesses the gap between
other agents’ current behaviors and their targets. Actions that minimize the gap
are considered highly inﬂuential and are rewarded. We evaluate our approach on a
set of challenging tasks with sparse rewards and partial observability that require
learning complex cooperative strategies under a proper exploration scheme, such
as the StarCraft Multi-Agent Challenge. Our methods show signiﬁcantly improved
performances over different baselines across all tasks."
INTRODUCTION,0.00796812749003984,"1
INTRODUCTION"
INTRODUCTION,0.01195219123505976,"Deep Reinforcement Learning (DRL) has been applied to solve various challenging problems, where
an agent typically learns to maximize the expected sum of extrinsic rewards gathered as a result of its
actions performed in the environment (Sutton et al., 1998). Multi-Agent Reinforcement Learning
(MARL) refers to the task of training a set of agents to maximize collective and/or individual rewards,
while existing in the same environment and interacting with each other."
INTRODUCTION,0.01593625498007968,"Recent works have shown that agents with coordinated behaviors learn remarkably faster (Roy et al.,
2019) since coordination helps the discovery of effective policies in cooperative tasks. Nevertheless,
achieving coordination among agents still remains a central challenge in MARL (Jaques et al.,
2019). Prominent works often resort to a popular learning paradigm called Centralized Training
with Decentralized Execution (CTDE) (Lowe et al., 2017; Foerster et al., 2018), where each agent is
evaluated using a centralized critic and has access to extra information about the policies of other
learning agents during training. At the time of execution, policies’ actions are restricted to local
information only (i.e. their own observations). Our work is primarily motivated by the following
natural questions:"
INTRODUCTION,0.0199203187250996,"In multi-agent settings, how can we quantify the effect of an agent (or player) on other teammates’
behaviors (particularly on their performance)? And to what degree exploiting this quantity can lead
to coordinated behaviors and consequently better overall performances?"
INTRODUCTION,0.02390438247011952,"To that end, we propose a novel approach that aims at promoting coordination for cooperative tasks
by augmenting CTDE MARL main return-maximization objective with an additional multi-agent
objective that acts as a policy regularizer; we refer to the latter objective as the influence function.
To build intuition, a chosen agent, which we call the “inﬂuencer”, assesses the progress that other
agents are making given its current policy and consequently learns behaviors that will result in an
improved performance of its teammates. Concretely, we formulate the inﬂuence of an inﬂuencer π as
an estimation of the dissimilarity between other agents’ behaviors and their targets given the current
behavior of π. The inﬂuencer is encouraged to learn behaviors that are expected to minimize that"
INTRODUCTION,0.027888446215139442,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.03187250996015936,"dissimilarity. We also propose two approaches to estimate the inﬂuence and empirically show that
they yield unbiased estimates of the true value."
INTRODUCTION,0.035856573705179286,"Agents acting upon the proposed coordination paradigm learn to efﬁciently exploit the observed joint
action space using available information. However, and since the joint space grows exponentially
with the number of agents, it is highly unlikely that agents will have access to sufﬁcient information
to learn optimal behaviors to solve the task at hand; this problem arises in many scenarios such as
sparse-reward environments, thus a proper exploration scheme is often required. However, many
existing multi-agent deep reinforcement learning algorithms still use mostly noise-based techniques
(Liu et al., 2021; Rashid et al., 2018; Yang et al., 2018). Moreover, independent exploration proved
to be inefﬁcient in cooperative settings (Roy et al., 2019). Recently, this challenge was addressed
through Intrinsic Motivation (IM) (Jaques et al., 2019; Du et al., 2019; Zhou et al., 2020b). Many
approaches employ IM to encourage exploration of state-space (Han et al., 2020; Burda et al., 2019)
or state-action space (Fayad & Ibrahim, 2021) by identifying novel conﬁgurations and rewarding an
agent for visiting them. We provide an extension of these ideas into multi-agent settings and further
build connection between reward shaping and coordinated behavior learning, where we choose an
agent to act as an inﬂuencer (i.e. regularize its standard objective using the inﬂuence function) while
other agents learn to maximize the expected sum of both extrinsic and intrinsic rewards."
INTRODUCTION,0.0398406374501992,"To sum up, our main contributions are threefold: 1) developing an inﬂuence function to promote
learning coordinated behaviors and improve team performance; 2) extending exploration via random
network distillation to multi-agent settings by crafting a ""novelty"" function that rewards under-
explored behaviors; 3) formulating a novel intrinsic incentive to promote learning diverse team
behaviors to help uncover complex behaviors in a collaborative way."
INTRODUCTION,0.043824701195219126,"We demonstrate the effectiveness of our methods on a comprehensive set of challenging tasks which
include, but not limited to, the StarCraft Multi-Agent Challenges (SMAC) (Samvelyan et al., 2019)
and the Multi-Agent Particle Environments (MAPE) (Mordatch & Abbeel, 2018; Lowe et al., 2017).
Empirical results show a signiﬁcant improvement over a wide variety of state-of-the-art MARL
approaches. We also conduct insightful ablation studies to understand the relative importance of each
component of the approach individually."
BACKGROUND,0.04780876494023904,"2
BACKGROUND"
MARKOV GAMES,0.05179282868525897,"2.1
MARKOV GAMES"
MARKOV GAMES,0.055776892430278883,"Markov games (Littman, 1994; Agarwal et al., 2020; Daskalakis et al., 2021) are a superset of Markov
decision process (MDPs) and matrix games, including both multiple agents and multiple states.
Formally, we consider a setting with N > 2 agents ([N] := {1, 2 . . . , N}) who repeatedly select
actions in a shared MDP. The goal of each agent is to maximize their respective action value function.
Formally, an MDP is deﬁned as a tuple G = (S, N, {Ai, ri}i∈[N], T, γ, ρ), where:"
MARKOV GAMES,0.05976095617529881,"• S is the ﬁnite states space, where the initial states are determined by a distribution ρ : S →
[0, 1]. Since we consider partially observable testing environments, we denote by si the
information observed by agent i about the global state s ∈S.
• Ai is a ﬁnite action space for agent i with generic element ai ∈Ai. Using common
conventions, we will write A = Πj∈[N]Aj and A−i = Πj̸=iAj to denote the joint action
spaces of all agents and of all agents other than i with generic elements a = (ai)i∈[N] and
a−i = (aj)i̸=j∈[N], respectively. According to this notation, we have that a = (ai, a−i) ∀i."
MARKOV GAMES,0.06374501992031872,"• ri : S × A →R is the individual reward function of agent i, i.e., ri(s, ai, a−i) is the
instantaneous reward of agent i when agent i takes action ai and all other agents take actions
a−i at state s ∈S.
• T is the transition probability function, for which T(s′|s, a) is the probability of transitioning
from s to s′ when a ∈A is the action proﬁle chosen by the agents.
• γ is a discount factor for future rewards of the MDP, shared by all agents."
MARKOV GAMES,0.06772908366533864,"In a Markov game, each agent is independently choosing actions and receiving rewards. Convention-
ally, an agent k aims to maximize its own total expected return Rk = PT
t=0 γtrk,t where T is the
time horizon."
MARKOV GAMES,0.07171314741035857,Under review as a conference paper at ICLR 2022
MULTI-AGENT DEEP DETERMINISTIC POLICY GRADIENT,0.07569721115537849,"2.2
MULTI-AGENT DEEP DETERMINISTIC POLICY GRADIENT"
MULTI-AGENT DEEP DETERMINISTIC POLICY GRADIENT,0.0796812749003984,"MADDPG (Lowe et al., 2017) is a multi-agent extension of the DDPG algorithm (Lillicrap et al.,
2015). It adapts the CTDE paradigm, where each agent i possesses its own deterministic policy µ(i)"
MULTI-AGENT DEEP DETERMINISTIC POLICY GRADIENT,0.08366533864541832,"for action selection and critic Q(i) for state-action value estimation, respectively parameterized by θ(i)"
MULTI-AGENT DEEP DETERMINISTIC POLICY GRADIENT,0.08764940239043825,"and φ(i). All parametric models are trained off-policy from previous transitions ζt := (st, at, rt, st+1)
uniformly sampled from a replay buffer D. Each centralized critic is trained to estimate the expected
return for a particular agent i from the Q-learning loss:"
MULTI-AGENT DEEP DETERMINISTIC POLICY GRADIENT,0.09163346613545817,L(i)(φ(i)) = Eζt∼D
MULTI-AGENT DEEP DETERMINISTIC POLICY GRADIENT,0.09561752988047809,"Q(i)(st, at; φ(i)) −

ri,t + γQ(i)
target(st+1,µµµ(st+1))

2
(1)"
MULTI-AGENT DEEP DETERMINISTIC POLICY GRADIENT,0.099601593625498,Each policy is updated to maximize the expected discounted return of the corresponding agent i :
MULTI-AGENT DEEP DETERMINISTIC POLICY GRADIENT,0.10358565737051793,"J(i)
P G(θ(i)) = ED
h
Q(i)(st, at)
i
(2)"
MULTI-AGENT DEEP DETERMINISTIC POLICY GRADIENT,0.10756972111553785,"Notice that while optimizing an agent’s policy, all agents’ observation-action pairs are taken into
consideration. By that, the value functions of all agents are trained in a centralized, stationary
environment, despite happening in a multi-agent setting. Moreover, this procedure allows for the
learning of coordinated strategies, yet needs to be augmented with efﬁcient exploration methods that
reward novel action conﬁgurations which may lead to the discovery of higher-return behaviors."
METHODS,0.11155378486055777,"3
METHODS"
BASIC INFLUENCE,0.11553784860557768,"3.1
BASIC INFLUENCE"
BASIC INFLUENCE,0.11952191235059761,"Intuitively, one can deﬁne coordination in a team of agents as the behavior of each individual agent
being informed by other agents. Furthermore, agents’ behaviors can be inter-affected either directly
through communication for example or indirectly through task-speciﬁc shared goals and/or rewards
or the dynamics of the environment. We hypothesize that when agents learn in a cooperative setting,
they tend to affect each other’s exploitation processes, we conﬁrm the hypothesis throughout the paper
and build on that to formalize a general method to foster inﬂuential interactions and learn meaningful
coordination protocols. Speciﬁcally, consider a setting where N agents with policies π1, π2, . . . , πN.
We assume that the transition dynamics are dependent on agents’ actions (i.e. T(s′|s, a) ̸= T(s|s′)),
and use tasks that satisfy this assumption to validate our methods. We will write πππ,πππ−j to denote the
joint policy of all agents and of all agents other than agent j, respectively. For two ﬁxed agents i and
j, the inﬂuence of the behavior of j’s policy on i’s performance (expected return) can be quantiﬁed by
""marginalizing"" out all effects induced by other agents on the expected outcomes of i. More precisely,
if the desired quantity is qπj
i , then for all (s, aj) ∈S × Aj:"
BASIC INFLUENCE,0.12350597609561753,"qπj
i (s, aj) =
E
a−j∼πππ−j[ri(s, a)] + γ
X"
BASIC INFLUENCE,0.12749003984063745,"s′
p(s′|s, aj) ·
E
a′
j∼πj(.|s′
j)"
BASIC INFLUENCE,0.13147410358565736,"
qπj
i (s′, a′
j)
"
BASIC INFLUENCE,0.13545816733067728,"p(s′|s, aj) =
X"
BASIC INFLUENCE,0.1394422310756972,"a−j
T(s′|s, a)
Y"
BASIC INFLUENCE,0.14342629482071714,"k̸=j
πk(ak|s)
(3)"
BASIC INFLUENCE,0.14741035856573706,"One can note that, analogously to the action-value function Qi, qπj
i
measures tuple values w.r.t i
where the learning dynamics of i conditioned only on the behavior of j. Naturally, in expectation,
qπj
i
represents the sum of rewards that an agent i is expected to receive when inﬂuenced by πj.
Based on above, we say that an agent is an optimal inﬂuencer when agents only inﬂuenced by its
policy learn optimal behaviors. To that end, if we ﬁx an agent (say 1), then its inﬂuence, F(π1), on
the rest of the team is deﬁned as follows:"
BASIC INFLUENCE,0.15139442231075698,"F(π1) =
X"
BASIC INFLUENCE,0.1553784860557769,"2≤i≤N
αi · Es,a1∼π1
 
qπ1
i (s, a1) −max
u
Qi(s, u)
2
(4)"
BASIC INFLUENCE,0.1593625498007968,"where (αi)2≤i≤N are positive scaling factors that sum up to 1 and Qi is the action value function
of agent i. Intuitively, since F(π1) is the difference between the expected rewards of other agents
conditioned on the current behavior of π1 and the maximum returns that can be achieved by other
agents in the given task, minimizing F will encourage π1 to take actions that lead other agents’"
BASIC INFLUENCE,0.16334661354581673,Under review as a conference paper at ICLR 2022
BASIC INFLUENCE,0.16733067729083664,"conditioned returns (qπ1
i )2≤i≤N closer to their maximum unrestricted returns (maxu Qi(s, u)),
which matches our deﬁnition of an optimal inﬂuencer. Hence, the objective functions of agents
i ∈[N] are: Ji(θi) = Eπππ
 P"
BASIC INFLUENCE,0.17131474103585656,"0≤t≤T γtri,t

−λF(π1)1i=1, where θi are the parameters of πi and λ
is a hyperparameter called the inﬂuence importance temperature.
Theorem 1. (Inﬂuence Gradient)"
BASIC INFLUENCE,0.1752988047808765,"∇θ1F(π1) =
X"
BASIC INFLUENCE,0.17928286852589642,"2≤i≤N
αiEs,π1

∇θ1 log π1(a1|s)g(s, a1)2"
BASIC INFLUENCE,0.18326693227091634,"+ 2g(s, a1)Es,π1[∇θ1 log π1(a1|s)qπ1
i (s, a1)]

(5)"
BASIC INFLUENCE,0.18725099601593626,"where g(T) = g(s, a1) = qπ1
i (s, a1) −maxu Qi(s, u)."
BASIC INFLUENCE,0.19123505976095617,"It has been shown empirically that the existence of one inﬂuencer can greatly improve the overall
team performance. Thus, we only consider one inﬂuencer throughout our practical sections and upon
that, we call our method ""Asymmetric Learning for Inﬂuencing a Team of Agents (ALITA)""."
INFLUENCE WITH SINGLE ESTIMATOR,0.1952191235059761,"3.1.1
INFLUENCE WITH SINGLE ESTIMATOR"
INFLUENCE WITH SINGLE ESTIMATOR,0.199203187250996,"Practically, we quantify the inﬂuence F(π1) by initializing a network Qcen : S × A →RN−1 with
parameters φcen; the i-th component of Qcen(.; φcen) estimates qπ1
i
by minimizing the following
loss at each timestep t:"
INFLUENCE WITH SINGLE ESTIMATOR,0.20318725099601595,"L(φcen) = E(x,a,r,x′)∼Dt

∥Qcen(x, a; φcen) −y∥2 
(6)"
INFLUENCE WITH SINGLE ESTIMATOR,0.20717131474103587,"where y ∈RN−1, yi = ri(x, a) + γQ(i)
target(x′,πππ(x′)); Q(i)
target is the target critic of agent πi, and
Dt = {T | T = (x, a, r, x′) ⊂B and x1 = s1,t, a1 = a1,t}, where B is a buffer storing all agents’
transitions. The reason for choosing such deﬁnition of D is that when ﬁxing the current information
of π (i.e. at timestep t) while updating Qcen
i
, we practically marginalize out all agents’ experience
(other than 1 and i). Later in this section, we show that this trick yields a fairly good approximation
of qπ1
i . After training Qcen, we compute the inﬂuence F(π1):"
INFLUENCE WITH SINGLE ESTIMATOR,0.21115537848605578,"F(π1) = E(x,.,r,x′)∼B

∥Qcen(x,πππ(x)) −z∥2 
(7)"
INFLUENCE WITH SINGLE ESTIMATOR,0.2151394422310757,"where z = [maxu Q2(x′, u), . . . , maxu QN(x′, u)]T . Note that the second term in the expectation
(i.e. the target vector z) is set to be undifferentiable with respect to π1’s parameters and thus does not
propagate through its network."
INFLUENCE WITH MULTIPLE INDIVIDUAL ESTIMATORS,0.21912350597609562,"3.1.2
INFLUENCE WITH MULTIPLE INDIVIDUAL ESTIMATORS"
INFLUENCE WITH MULTIPLE INDIVIDUAL ESTIMATORS,0.22310756972111553,"As seen earlier, agent π1 estimates the gap between each agent’s value and its target value by
employing a single network. Another desirable approach is to additively decompose the centralized
estimator Qcen, i.e. to use multiple estimators where each estimator, namely Q(i)
clone, individually
calculates a fairly good approximation of qπ1
i . To reduce computational costs and arrive at better
estimates, each estimator’s network is initialized with the parameters of the corresponding critic
network at each episode (i.e. Q(i)
clone ←Qi). The training is carried out similarly to that of the single
estimator setting,
L(φ(i)) = E(x,a,r,x′)∼Dt

||Q(i)
clone(x, a; φ(i)) −yi||2
(8)"
INFLUENCE WITH MULTIPLE INDIVIDUAL ESTIMATORS,0.22709163346613545,The inﬂuence function could be expressed as:
INFLUENCE WITH MULTIPLE INDIVIDUAL ESTIMATORS,0.23107569721115537,"F(π1) = E(x,.,.,x′)∼B
1
N −1 N−1
X i=1"
INFLUENCE WITH MULTIPLE INDIVIDUAL ESTIMATORS,0.2350597609561753,"Q(i)
clone(x,πππ(x)) −zi

2
(9)"
INFLUENCE WITH MULTIPLE INDIVIDUAL ESTIMATORS,0.23904382470119523,"Which approach yields better estimates of the true value of the inﬂuence? The inﬂuence of an
agent π on a team of agents T was deﬁned as a measure of the improvement in the performance of
T given the current behavior of π. However, measuring the inﬂuence using function approximators
might result in inaccurate estimates. To resolve this concern, we plot the inﬂuence estimates of the
two prior approaches over time while they learn on the Cooperative Navigation MAPE task (Mordatch"
INFLUENCE WITH MULTIPLE INDIVIDUAL ESTIMATORS,0.24302788844621515,Under review as a conference paper at ICLR 2022
INFLUENCE WITH MULTIPLE INDIVIDUAL ESTIMATORS,0.24701195219123506,"& Abbeel, 2018), where the number of agents is N = 6. In Figure (1), we graph the average inﬂuence
estimates over 40000 episodes and compare it to the true value. The latter is averaged over 1000
episodes following the current policies of agents and is reported every 5000 episodes. The plots show
a relatively small bias of both methods during learning. However, as Figure (1) suggests, measuring
inﬂuence using multiple individual estimators yielded more accurate values after enough training
which substantiates its superiority over the shared network approach. Note that, although conﬁrms
our prior hypothesis, this experiment does not reﬂect the importance of employing the inﬂuence on
the ﬁnal performance of the agents as we will discuss that in Section (4)."
INTRINSIC MOTIVATION FOR DIVERSIFIED TEAM BEHAVIOR,0.250996015936255,"3.2
INTRINSIC MOTIVATION FOR DIVERSIFIED TEAM BEHAVIOR"
INTRINSIC MOTIVATION FOR DIVERSIFIED TEAM BEHAVIOR,0.2549800796812749,"Figure 1: Empirical evaluation of the
bias in the proposed methods of mea-
suring inﬂuence. The inﬂuence values
estimated by single and multiple esti-
mators are compared to the true values
of F. The results for the estimated
values are averaged across 8 runs."
INTRINSIC MOTIVATION FOR DIVERSIFIED TEAM BEHAVIOR,0.2589641434262948,"In this section, we introduce a framework for achieving coop-
erative exploration by ensuring that agents are consistently
tilted towards visiting under-explored state-action conﬁgu-
rations; we start by providing a simple demonstration which
shows that the number of environment steps required for
all agents to randomly traverse all possible action conﬁgu-
rations increases at least exponentially with the number of
agents."
INTRINSIC MOTIVATION FOR DIVERSIFIED TEAM BEHAVIOR,0.26294820717131473,"Proposition 1. Consider an L-action setting of n agents.
In expectation, the number of steps T needed to visit all
Ln action conﬁgurations at least once without coordinated
exploration grows at least exponentially with the number of
agents. More concretely, E[T] = Ω(nLn)."
INTRINSIC MOTIVATION FOR DIVERSIFIED TEAM BEHAVIOR,0.26693227091633465,"To mitigate this issue, we assign agent π1 a prediction error
as an intrinsic reward to facilitate recognizing and learning
novel behaviors:"
INTRINSIC MOTIVATION FOR DIVERSIFIED TEAM BEHAVIOR,0.27091633466135456,"r(π1)
t
= rext
1,t + λπ ∥φ(s1,t, a1,t) −(s1,t, a1,t)∥2
|
{z
}
ψ(s1,t,a1,t) (10)"
INTRINSIC MOTIVATION FOR DIVERSIFIED TEAM BEHAVIOR,0.2749003984063745,"Where φ is an autoencoder network regularly trained on data generated by the policy π1 and λπ is a
hyper-parameter that balances the extrinsic and intrinsic reward terms. ψ’s expression stems from the
observation that when an autoencoder is trained on data from a particular distribution, it will be good
at reconstructing data from that distribution, while it will perform poorly if the data is from a different
distribution. Thus, by employing ψ as an intrinsic bonus, π1 rewards states’ observations and actions
that do not belong to the data generated by it. In practice, φ is designed to be a relatively large
network since we want it to be slightly overﬁtted to the training data so that it will not accidentally
generalize to behaviors that we may deem novel (Fayad & Ibrahim, 2021; Zhang et al., 2019)."
INTRINSIC MOTIVATION FOR DIVERSIFIED TEAM BEHAVIOR,0.2788844621513944,"Nevertheless, assigning each agent a ψ is not sufﬁcient as it makes the case equivalent to independent
exploration approaches. Thus, we propose a coordinated exploration method that takes into account
other agents’ behaviors, encouraging agents to diversify team behavior while maintaining good
performance."
INTRINSIC MOTIVATION FOR DIVERSIFIED TEAM BEHAVIOR,0.28286852589641437,"Speciﬁcally, we assign agents {πi}N
i=2 intrinsic penalty deﬁned as:"
INTRINSIC MOTIVATION FOR DIVERSIFIED TEAM BEHAVIOR,0.2868525896414343,"rint
πi(st, at) = −exp
 
−ωiψ(s1,t, a1,t)

∥πi(si,t) −ai,t∥2
(11)"
INTRINSIC MOTIVATION FOR DIVERSIFIED TEAM BEHAVIOR,0.2908366533864542,"This reward term aims at teaching the agents to recognize previous behaviors and synchronously
select novel conﬁgurations. To build intuition, consider a case where N = 2. Whenever (π, µ) select
an action tuple in the neighborhood of a frequently-visited tuple (a1, a2) in a global state s, ψ will be
relatively small and the penalty, rint
µ , will be large. Conversely, if (π, µ) encounter a novel tuple, say
(a′
1, a′
2) in s, the small penalty of µ (Eq. (11)) together with a large reward for π (Eq. (10)) will drive
both agents to further explore this encounter."
INTRINSIC MOTIVATION FOR DIVERSIFIED TEAM BEHAVIOR,0.2948207171314741,"In all, Fig. (2) shows how this framework can be augmented with the basic inﬂuence introduced
earlier to reinforce learning and discovering coordinated behaviors."
INTRINSIC MOTIVATION FOR DIVERSIFIED TEAM BEHAVIOR,0.29880478087649404,Under review as a conference paper at ICLR 2022
INTRINSIC MOTIVATION FOR DIVERSIFIED TEAM BEHAVIOR,0.30278884462151395,Figure 2: Architecture of the general proposed method
EMPIRICAL EVALUATION & ANALYSIS,0.30677290836653387,"4
EMPIRICAL EVALUATION & ANALYSIS"
EMPIRICAL EVALUATION & ANALYSIS,0.3107569721115538,"The goals of our experiments are to: a) verify the performance of our method on a comprehensive set
of multi-agent challenges (SMAC, MAPE, sparse-reward settings, and continuous control environ-
ments); b) perform ablations to examine which particular components of the proposed framework are
important for good performance."
COOPERATIVE & MIXED GAMES,0.3147410358565737,"4.1
COOPERATIVE & MIXED GAMES"
STARCRAFT MULTI-AGENT CHALLENGE,0.3187250996015936,"4.1.1
STARCRAFT MULTI-AGENT CHALLENGE"
STARCRAFT MULTI-AGENT CHALLENGE,0.32270916334661354,"StarCraft provides a rich set of heterogeneous units each with diverse actions, allowing for ex-
tremely complex cooperative behaviors among agents. We thus evaluate our method on several SC
micromanagement tasks from the SMAC1 benchmark (Samvelyan et al., 2019), where a group of
mixed-typed units controlled by decentralized agents needs to cooperate to defeat another group of
mixed-typed enemy units controlled by built-in heuristic rules with “difﬁcult” setting; the battles
can be both symmetric (same units in both groups) or asymmetric. Each agent observes its own
status and, within its ﬁeld of view, it also observes other units’ statistics such as health, location,
and unit type (partial observability); agents can only attack enemies within their shooting range.
A shared reward is received on battle victory as well as damaging or killing enemy units. Each
battle has step limits set by SMAC and may end early. We consider 5 battle maps grouped into
Easy (2s3z), Hard (5m_vs_6m, 3s_vs_5z), and Super Hard (corridor, 3s5z_vs_3s6z) against 8
baseline methods using their open-source implementations based on PyMARL (Samvelyan et al.,
2019): IQL (Tan, 1993), VDN (Sunehag et al., 2018), QMIX (Rashid et al., 2018), LIIR (Individual
Intrinsic Rewards) (Du et al., 2019), LICA (Implicit Credit Assignment) (Zhou et al., 2020b), and
MAVEN (Variational Exploration) (Mahajan et al., 2019), EDTI (Decision-Theoretic Exploration) ,
EITI (Information-Theoretic Exploration) (Wang et al., 2019)."
STARCRAFT MULTI-AGENT CHALLENGE,0.32669322709163345,"The corridor map, in which 6 Zealots face 24 enemy Zerglings, requires agents to make effective
use of the terrain features and block enemy attacks from different directions. A properly coordinated
exploration scheme applied to this map would help the agents discover a suitable unit positioning
quickly and improve performance, while 2s3z requires agents to learn “focus ﬁre"" and interception.
For the asymmetric 5m_vs_6m, basic agent coordination alone such as “focus ﬁring” no longer
sufﬁces (Du et al., 2019) and consistent success requires extended exploration to uncover complex
cooperative strategies such as pulling back units with low health during combat. The 3s_vs_5z
scenario features three allied Stalkers against ﬁve enemy Zealots. Since Zealots counter Stalkers, the
only winning strategy for the allied units is to kite the enemy around the map and kill them one after
another, causing the failure of independent learning algorithms to learn good policies in this task.
The 3s5z_vs_3s6z is an extended scenario of 3s_vs_5z where 3 Stalkers and 5 Zealots battle against
3 Stalkers and 6 Zealots—The extra enemy makes this scenario far more challenging. For this task,
we randomly chose 1 S and 1 Z to act as inﬂuencers because of the heterogeneity of the team. In the
rest, agents are symmetric, hence the inﬂuencer is randomly chosen at the beginning of training."
STARCRAFT MULTI-AGENT CHALLENGE,0.33067729083665337,"For most of these scenarios, ALITA consistently shows the best performance with signiﬁcant learning
speed which conﬁrms the effectiveness of our proposed methods. Detailed results are reported in
Figure (3) as they present the median win rate during the training across 12 random runs."
STARCRAFT MULTI-AGENT CHALLENGE,0.3346613545816733,1https://github.com/oxwhirl/smac
STARCRAFT MULTI-AGENT CHALLENGE,0.3386454183266932,Under review as a conference paper at ICLR 2022
STARCRAFT MULTI-AGENT CHALLENGE,0.3426294820717131,"(a) 2s3z Easy
(b) 3s_vs_5z Hard
(c) 5m_vs_6m Hard"
STARCRAFT MULTI-AGENT CHALLENGE,0.3466135458167331,"(d) corridor SuperHard
(e) 3s5z_vs_3s6z SuperHard"
STARCRAFT MULTI-AGENT CHALLENGE,0.350597609561753,Figure 3: The median test win % of various methods across the SMAC scenarios.
SPARSE-REWARD SETTINGS,0.3545816733067729,"4.1.2
SPARSE-REWARD SETTINGS"
SPARSE-REWARD SETTINGS,0.35856573705179284,"We test on two additional tasks to show the effectiveness our method on sparse-reward settings and
compare it to famous inﬂuence-based coordinated exploration algorithms (Table (1)): EDTI, EITI
(Wang et al., 2019), and Social Inﬂuence (Jaques et al., 2019)."
SPARSE-REWARD SETTINGS,0.36254980079681276,"Sparse Push-Box: A 15 × 15 room is populated with 2 agents and 1 box. Agents need to push the
box to the wall in 300 environment steps to get a reward of 1000. Moreover, both can observe the
coordinates of their teammate and the location of the box. However, the box is so heavy that only
when two agents push it in the same direction at the same time can it be moved a grid. Agents need
to coordinate their positions and actions for multiple steps to earn a reward."
SPARSE-REWARD SETTINGS,0.3665338645418327,"Sparse Secret Room: A 25 × 25 grid is divided into three small rooms on the right and one large
room on the left where 2 agents are initially spawned. There is one door between each small room
and the large room. A switch in the large room controls all three doors. A switch also exists in each
small room which only controls the room’s door. The agents need to navigate to one of the three small
rooms, i.e. the target room, to receive positive reward. The task is considered solved if both agents
are in the target room. The state vector contains (x, y) locations of all agents and binary variables to
indicate if doors are open."
SPARSE-REWARD SETTINGS,0.3705179282868526,Table 1: Results on the Push-Box and Secret Room tasks after 20000 updates across 10 runs.
SPARSE-REWARD SETTINGS,0.3745019920318725,"Push-Box
Secret Room
Agents
Team Performance
Performance std
Avg Success Rate
Success Rate std
ALITA
146.66
34.13
0.68
0.04
EDTI
135.84
45.20
0.34
0.02
Social Inﬂuence
86.67
65.81
0.25
0.10
EITI
75.09
78.54
0.46
0.06"
SPARSE-REWARD SETTINGS,0.3784860557768924,"A notable reason for the good performance of ALITA on the two tasks is that through the intrinsic
rewards, agents tend to explore the majority of the possible encounters at the beginning of learning
which is crucial for estimating the value of the inﬂuence. Exploiting the latter, agents tend to reinforce
learning jointly rewarding conﬁgurations."
MULTI-AGENT PARTICLE ENVIRONMENTS,0.38247011952191234,"4.1.3
MULTI-AGENT PARTICLE ENVIRONMENTS"
MULTI-AGENT PARTICLE ENVIRONMENTS,0.38645418326693226,"To understand how the proposed method helps agents achieve cooperative behavior in nonstation-
ary settings, we conduct experiments on the grounded communication environment 2 proposed in"
MULTI-AGENT PARTICLE ENVIRONMENTS,0.3904382470119522,2https://github.com/openai/multiagent-particle-envs
MULTI-AGENT PARTICLE ENVIRONMENTS,0.3944223107569721,Under review as a conference paper at ICLR 2022
MULTI-AGENT PARTICLE ENVIRONMENTS,0.398406374501992,"(Mordatch & Abbeel, 2018; Lowe et al., 2017). The chosen tasks are the Cooperative Navigation,
Cooperative Communication, and Physical Deception.3 We trained with 10 random seeds and
reported results in Tables (2, 3, 4)."
CONTINUOUS ENVIRONMENTS,0.40239043824701193,"4.2
CONTINUOUS ENVIRONMENTS"
CONTINUOUS ENVIRONMENTS,0.4063745019920319,"To conﬁrm the scalability of our algorithm to large continuous settings, we measure the performance
of our algorithm on a suite of PyBullet (Tan et al., 2018) continuous control tasks, interfaced through
OpenAI Gym (Brockman et al., 2016). Gym environments, however, are mainly single-agent settings,
thus to evaluate our approach, we reframe the problem by introducing an additional learning agent
that acts as an auxiliary agent. Crucially, both agents work collaboratively in order to ﬁnd a region of
the solution space where an agent accumulates higher rewards. We use TD3 (Fujimoto et al., 2018)
as our learning model and test it against state-of-the-art algorithms in 5 gym environments. Our
algorithm outperforms all baselines across all different environments (e.g. our method attains 131%
return of SAC ﬁnal performance on Humanoid-v3). For detailed results, see Appendix C."
ABLATIONS,0.4103585657370518,"4.3
ABLATIONS"
ABLATIONS,0.41434262948207173,"Figure 4: Ablations for different compo-
nents of our framework on 2s3z scenario."
ABLATIONS,0.41832669322709165,"We further investigate the signiﬁcance of each component
along with a symmetric extension of the proposed frame-
work. Speciﬁcally, we consider the three cases: 1) No F:
where the inﬂuence function does not contribute to the
update rule to any of the policies; 2) No IM: where a ran-
domly selected agent maximizes both the expected sum
of extrinsic rewards along with the inﬂuence function,
and other agents’ policies are learned using the DDPG; 3)
Symmetric: where all agents simultaneously play the role
of an inﬂuencer and inﬂuencee: they learn to maximize
an augmented reward function (extrinsic and intrinsic)
along with the inﬂuence function."
ABLATIONS,0.42231075697211157,"Results of the experiments conducted on the 2s3z SMAC
scenario show that, in the absence of the intrinsic rewards
(No IM), the agents experience a slightly decreased overall performance when compared to the
signiﬁcant decline induced by detaching the inﬂuence function (No F). In Figure (4), we observe that
the agents following the Symmetric approach learn faster, and achieve a signiﬁcantly higher median
win rate. This approach, however, doubles the computational costs which restricts its applicability in
larger settings."
RELATED WORK,0.4262948207171315,"5
RELATED WORK"
RELATED WORK,0.4302788844621514,"We discuss recently developed methods for exploration in RL using intrinsic motivation, coordination
in multi-agent RL, and inﬂuence-based coordinated exploration methods subsequently."
RELATED WORK,0.4342629482071713,"Intrinsic motivation (IM) has been increasingly used both in single-agent RL and multi-agent RL. A
core idea of IM is to encourage the agent to take new actions or visit new states, thus exploring the
environment and obtaining more diverse behaviors. One common approach is to approximate state
or state-action visitation frequency and add a reward bonus to states the agent rarely covers (Tang
et al., 2017; Bellemare et al., 2016; Martin et al., 2017). A more related IM approach is to evaluate
state visitation novelty (Klissarov et al., 2019; Han et al., 2020; Burda et al., 2019) or state-action
visitation novelty (Fayad & Ibrahim, 2021). Inspired by the latter, we provided a natural extension for
this approach to the MARL settings by the learning of a ""novelty"" function. Other works make use of
single-agent IM to construct their multi-agent intrinsic reward (Du et al., 2019; Iqbal & Sha, 2019).
Each agent in (Du et al., 2019) learns a distinct intrinsic reward so that the agents are stimulated
differently, even when the environment only feedbacks a team reward. This reward helps distinguish
the contributions of the agents when the environment only returns a collective reward. In (Iqbal &
Sha, 2019), each agent has a novelty function that assesses how novel an observation is to it, based"
FULL DESCRIPTION IN APPENDIX B,0.43824701195219123,3Full description in Appendix B
FULL DESCRIPTION IN APPENDIX B,0.44223107569721115,Under review as a conference paper at ICLR 2022
FULL DESCRIPTION IN APPENDIX B,0.44621513944223107,"on its past experience. Their multi-agent intrinsic reward is deﬁned based on how novel all agents
consider an agent’s observation. A recent work (Liu et al., 2021) assigns agents a common goal while
exploring. The goal is selected from multiple projected state spaces via a normalized entropy-based
technique. Then, agents are trained to reach this goal in a coordinated manner."
FULL DESCRIPTION IN APPENDIX B,0.450199203187251,"Many works studied the cooperative settings in MARL; a straightforward approach is to use inde-
pendent learning agents (fully decentralized learning). This approach, however, is shown to perform
inadequately both with Q-learning (Matignon et al., 2012) and with policy gradient (Lowe et al.,
2017). Therefore, we considered the CTDE paradigm, where each agent’s policy takes its individual
observation as many real life applications dictate, while the centralized critic permit for sharing
of information during training. Policy gradient methods have been commonly used along with the
CTDE paradigm in MARL, either by implementing a single centralized critic for all agents (Foerster
et al., 2018), or one centralized critic for each agent (Lowe et al., 2017). Adopting the latter, we
enable agents with different reward functions to learn in competitive and mixed scenarios as well."
FULL DESCRIPTION IN APPENDIX B,0.4541832669322709,"Some other works encouraged cooperative interactions between agents by sharing useful information
(Yang et al., 2020; Hostallero et al., 2020). In Hostallero et al. (2020), each agent broadcasts a signal
that represents an assessment of the effect of the joint actions that all agents take on its expected
reward. Different from our approach, this signal encourages agents to behave as is expected of them
and does not beneﬁt exploration. As for Yang et al. (2020), each agent learns an incentive function
that rewards other agents based on their actions. Each agent’s function aims to alter other agents’
behavior to maximize its extrinsic rewards. To accomplish this, each agent requires access to every
other agent’s policy, incentive function, and return making this approach difﬁcult to scale and execute.
Additionally, Roy et al. (2019) proposed two policy regularizers approaches to promote coordination
in a team of agent, one of which assumes that an agent must be able to predict the behavior of
its teammates in order to coordinate with them, while the other supposes that coordinated agents
collectively recognize different situations and synchronously switch to different sub-policies to react
to them."
FULL DESCRIPTION IN APPENDIX B,0.4581673306772908,"Similarly to our work, (Jaques et al., 2019) proposed a similar idea of rewarding an agent for having a
casual inﬂuence on other agents’ actions. Their method showed interesting results in terms of learning
coordinated behavior. However, this casual inﬂuence is designed to reward policies for inﬂuencing
other policies’ actions without considering the ""quality"" of this inﬂuence. Barton et al. (2018) propose
causal inﬂuence as a way to measure coordination between agents, speciﬁcally using Convergence
Cross Mapping (CCM) to analyze the degree of dependence between two agents’ policies. Our
method also draws inspiration from the work of (Wang et al., 2019), as they deﬁne an inﬂuence-based
intrinsic exploration bonus, called Value of Interaction (VoI), by the expected difference between the
action-value function of one agent and its counterfactual action-value function without considering
the state and action of the other agent. Particularly, the latter measures the expected behavior of an
agent in a situation where it is not inﬂuenced by the other agent. Consequently, by maximizing the
VoI, agents tend to explore meaningful interaction points as the distance between their action-value
functions conditioned on other agents and their independent action-value (only conditioned on self
behavior) functions is maximized. The primary difference from ALITA lies in our deﬁnition of the
inﬂuence."
CONCLUSIONS & FUTURE WORK,0.46215139442231074,"6
CONCLUSIONS & FUTURE WORK"
CONCLUSIONS & FUTURE WORK,0.46613545816733065,"We introduced a novel multi-agent RL algorithm for achieving coordination through assessing the
inﬂuence an agent has on other agents’ behaviors. Additionally, we proposed to learn an intrinsic
reward for each agent to promote coordinated team exploration. We tested our algorithm on a
wide variety of tasks with many challenges, such as partial observability, sparse rewards, and large
spaces; these tasks include, but not limited to, SMAC, MAPE, as well as OpenAI gym continuous
environments. Our methods achieved noticeable improvement over prominent algorithms on all tasks.
One promising extension of our algorithm is to use Graph Attention Networks (Veliˇckovi´c et al.,
2017; Zhou et al., 2020a) to learn the importance of the inﬂuencer in determining the inﬂuencees’
policies and to establish a message-passing architecture in networked systems. The investigation of
the effectiveness of these methods is left for future works."
CONCLUSIONS & FUTURE WORK,0.4701195219123506,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.47410358565737054,"7
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.47808764940239046,"We have provided an illustration of the proposed algorithm in Fig. (1) along with implementation
details and hyperparameters selection in Appendix D. Furthermore, code is submitted with Sup-
plementary Material and each algorithm is evaluated at least 10 times using random seeds on all
environments."
REFERENCES,0.4820717131474104,REFERENCES
REFERENCES,0.4860557768924303,"Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. Optimality and approximation
with policy gradient methods in markov decision processes. In Conference on Learning Theory,
pp. 64–66. PMLR, 2020."
REFERENCES,0.4900398406374502,"Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016."
REFERENCES,0.4940239043824701,"Sean L Barton, Nicholas R Waytowich, and Derrik E Asher. Coordination-driven learning in multi-
agent problem spaces. arXiv preprint arXiv:1809.04918, 2018."
REFERENCES,0.49800796812749004,"Marc G. Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Rémi
Munos. Unifying count-based exploration and intrinsic motivation. In 30th Conference on Neural
Information Processing Systems, 2016."
REFERENCES,0.50199203187251,"Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016."
REFERENCES,0.5059760956175299,"Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network
distillation. In International Conference on Learning Representations, 2019. URL https:
//openreview.net/forum?id=H1lJJnR5Ym."
REFERENCES,0.5099601593625498,"Gang Chen and Yiming Peng. Off-policy actor-critic in an ensemble: Achieving maximum general
entropy and effective environment exploration in deep reinforcement learning. arXiv preprint
arXiv:1902.05551, 2019."
REFERENCES,0.5139442231075697,"Constantinos Daskalakis, Dylan J Foster, and Noah Golowich. Independent policy gradient methods
for competitive reinforcement learning. arXiv preprint arXiv:2101.04233, 2021."
REFERENCES,0.5179282868525896,"Yali Du, Lei Han, Meng Fang, Ji Liu, Tianhong Dai, and Dacheng Tao. Liir: Learning individual
intrinsic reward in multi-agent reinforcement learning. Advances in Neural Information Processing
Systems, 32:4403–4414, 2019."
REFERENCES,0.5219123505976095,"Ammar Fayad and Majd Ibrahim. Behavior-guided actor-critic: Improving exploration via learning
policy behavior representation for deep reinforcement learning. arXiv preprint arXiv:2104.04424,
2021."
REFERENCES,0.5258964143426295,"Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual multi-agent policy gradients. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, volume 32, 2018."
REFERENCES,0.5298804780876494,"Scott Fujimoto, Herke Van Hoof, and David Meger. Addressing function approximation error in
actor-critic methods. arXiv preprint arXiv:1802.09477, 2018."
REFERENCES,0.5338645418326693,"Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artiﬁcial intelligence and
statistics, pp. 249–256. JMLR Workshop and Conference Proceedings, 2010."
REFERENCES,0.5378486055776892,"Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International conference
on machine learning, pp. 1861–1870. PMLR, 2018."
REFERENCES,0.5418326693227091,"Gao-Jie Han, Xiao-Fang Zhang, Hao Wang, and Chen-Guang Mao. Curiosity-driven variational
autoencoder for deep q network. In Paciﬁc-Asia Conference on Knowledge Discovery and Data
Mining, pp. 764–775. Springer, 2020."
REFERENCES,0.545816733067729,Under review as a conference paper at ICLR 2022
REFERENCES,0.549800796812749,"Matthew Hausknecht and Peter Stone. Deep recurrent q-learning for partially observable mdps. In
2015 aaai fall symposium series, 2015."
REFERENCES,0.5537848605577689,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing
human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE international
conference on computer vision, pp. 1026–1034, 2015."
REFERENCES,0.5577689243027888,"David Earl Hostallero, Daewoo Kim, Sangwoo Moon, Kyunghwan Son, Wan Ju Kang, and Yung
Yi. Inducing cooperation through reward reshaping based on peer evaluations in deep multi-agent
reinforcement learning. In Proceedings of the 19th International Conference on Autonomous
Agents and MultiAgent Systems, pp. 520–528, 2020."
REFERENCES,0.5617529880478087,"Shariq Iqbal and Fei Sha. Coordinated exploration via intrinsic rewards for multi-agent reinforcement
learning. arXiv preprint arXiv:1905.12127, 2019."
REFERENCES,0.5657370517928287,"Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro Ortega, DJ Strouse,
Joel Z Leibo, and Nando De Freitas. Social inﬂuence as intrinsic motivation for multi-agent deep
reinforcement learning. In International Conference on Machine Learning, pp. 3040–3049. PMLR,
2019."
REFERENCES,0.5697211155378487,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.5737051792828686,"Martin Klissarov, Riashat Islam, Khimya Khetarpal, and Doina Precup. Variational state encoding as
intrinsic motivation in reinforcement learning. In Task-Agnostic Reinforcement Learning Workshop
at Proceedings of the International Conference on Learning Representations, 2019."
REFERENCES,0.5776892430278885,"Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015."
REFERENCES,0.5816733067729084,"Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In
Machine learning proceedings 1994, pp. 157–163. Elsevier, 1994."
REFERENCES,0.5856573705179283,"Iou-Jen Liu, Unnat Jain, Raymond A Yeh, and Alexander Schwing. Cooperative exploration for
multi-agent deep reinforcement learning. In International Conference on Machine Learning, pp.
6826–6836. PMLR, 2021."
REFERENCES,0.5896414342629482,"Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-
critic for mixed cooperative-competitive environments. arXiv preprint arXiv:1706.02275, 2017."
REFERENCES,0.5936254980079682,"Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson. Maven: Multi-agent
variational exploration. arXiv preprint arXiv:1910.07483, 2019."
REFERENCES,0.5976095617529881,"Jarryd Martin, Suraj Narayanan Sasikumar, Tom Everitt, and Marcus Hutter. Count-based exploration
in feature space for reinforcement learning. arXiv preprint arXiv:1706.08090, 2017."
REFERENCES,0.601593625498008,"Laetitia Matignon, Guillaume J Laurent, and Nadine Le Fort-Piat. Independent reinforcement learners
in cooperative markov games: a survey regarding coordination problems. Knowledge Engineering
Review, 27(1):1–31, 2012."
REFERENCES,0.6055776892430279,"Igor Mordatch and Pieter Abbeel. Emergence of grounded compositional language in multi-agent
populations. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 32, 2018."
REFERENCES,0.6095617529880478,"Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shi-
mon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement
learning. In International Conference on Machine Learning, pp. 4295–4304. PMLR, 2018."
REFERENCES,0.6135458167330677,"Julien Roy, Paul Barde, Félix G Harvey, Derek Nowrouzezahrai, and Christopher Pal. Promoting
coordination through policy regularization in multi-agent deep reinforcement learning. arXiv
preprint arXiv:1908.02269, 2019."
REFERENCES,0.6175298804780877,"Mikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nantas Nardelli,
Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The
starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019."
REFERENCES,0.6215139442231076,Under review as a conference paper at ICLR 2022
REFERENCES,0.6254980079681275,"John Schulman, Sergey Levine, Pieter Abbeel, Michael I. Jordan, and Philipp Moritz. Trust region
policy optimization. In 32nd International Conference on Machine Learning, 2015."
REFERENCES,0.6294820717131474,"John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017."
REFERENCES,0.6334661354581673,"Peter Sunehag, Guy Lever, A. Gruslys, Wojciech Czarnecki, V. Zambaldi, Max Jaderberg, Marc
Lanctot, Nicolas Sonnerat, Joel Z. Leibo, K. Tuyls, and T. Graepel. Value-decomposition networks
for cooperative multi-agent learning. ArXiv, abs/1706.05296, 2018."
REFERENCES,0.6374501992031872,"Richard S Sutton, Andrew G Barto, et al. Introduction to reinforcement learning, volume 135. MIT
press Cambridge, 1998."
REFERENCES,0.6414342629482072,"Jie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen, Yunfei Bai, Danijar Hafner, Steven Bohez, and
Vincent Vanhoucke. Sim-to-real: Learning agile locomotion for quadruped robots. arXiv preprint
arXiv:1804.10332, 2018."
REFERENCES,0.6454183266932271,"Ming Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents. In In Proceedings
of the Tenth International Conference on Machine Learning, pp. 330–337. Morgan Kaufmann,
1993."
REFERENCES,0.649402390438247,"Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen, Yan Duan, John Schulman,
Filip De Turck, and Pieter Abbeel. #exploration: A study of count-based exploration for deep
reinforcement learning. In 31st Conference on Neural Information Processing Systems, 2017."
REFERENCES,0.6533864541832669,"Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026–5033.
IEEE, 2012."
REFERENCES,0.6573705179282868,"Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017."
REFERENCES,0.6613545816733067,"Tonghan Wang, Jianhao Wang, Yi Wu, and Chongjie Zhang. Inﬂuence-based multi-agent exploration.
arXiv preprint arXiv:1910.05512, 2019."
REFERENCES,0.6653386454183267,"Yuhuai Wu, Elman Mansimov, Roger B. Grosse, Shun Liao, and Jimmy Ba.
Scalable trust-
region method for deep reinforcement learning using kronecker-factored approximation. ArXiv,
abs/1708.05144, 2017."
REFERENCES,0.6693227091633466,"Jiachen Yang, Ang Li, Mehrdad Farajtabar, Peter Sunehag, Edward Hughes, and Hongyuan Zha.
Learning to incentivize other learning agents. arXiv preprint arXiv:2006.06051, 2020."
REFERENCES,0.6733067729083665,"Yaodong Yang, Rui Luo, Minne Li, Ming Zhou, Weinan Zhang, and Jun Wang. Mean ﬁeld multi-
agent reinforcement learning. In International Conference on Machine Learning, pp. 5571–5580.
PMLR, 2018."
REFERENCES,0.6772908366533864,"Yunbo Zhang, Wenhao Yu, and Greg Turk. Learning novel policies for tasks. In International
Conference on Machine Learning, pp. 7483–7492. PMLR, 2019."
REFERENCES,0.6812749003984063,"Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, and Maosong Sun. Graph neural
networks: A review of methods and applications. AI Open, 1:57–81, 2020a."
REFERENCES,0.6852589641434262,"Meng Zhou, Ziyu Liu, Pengwei Sui, Yixuan Li, and Yuk Ying Chung. Learning implicit credit
assignment for cooperative multi-agent reinforcement learning. arXiv preprint arXiv:2007.02529,
2020b."
REFERENCES,0.6892430278884463,Under review as a conference paper at ICLR 2022
REFERENCES,0.6932270916334662,"A
MATHEMATICAL DETAILS"
REFERENCES,0.6972111553784861,Theorem 1. (Inﬂuence Gradient)
REFERENCES,0.701195219123506,"∇θ1F(π1) =
X"
REFERENCES,0.7051792828685259,"2≤i≤N
αiEs,π1

∇θ1 log π1(a1|s)g(s, a1)2 + 2g(s, a1)Es,π1[∇θ1 log π1(a1|s)qπ1
i (s, a1)]
"
REFERENCES,0.7091633466135459,"where g(T) = g(s, a1) = qπ1
i (s, a1) −maxu Qi(s, u)."
REFERENCES,0.7131474103585658,"Proof. We start by ﬁnding ∇θ1qπ1
i :"
REFERENCES,0.7171314741035857,"∇θ1qπ1
i (s, a1) = γ
E
s′∼p(.|s,a1)
a′
1∼π1(.|s′
1)"
REFERENCES,0.7211155378486056,"
∇θ1 log π1(a′
1|s′)qπ1
i (s′, a′
1) + ∇θ1qπ1
i (s′, a′
1)
"
REFERENCES,0.7250996015936255,"Let T = (s, a1) ∈S × A1, and φ(T) = γEs′,a′
1[∇θ1 log π1(a′
1|s′)qπ1
i (s′, a′
1)] where the distribution
of s′ is conditional on s and a1. Thus, write ∇θ1qπ1
i (T) as:"
REFERENCES,0.7290836653386454,"φ(T) + γEs′,a′
1[∇θ1qπ1
i (T ′)]
(T ′ = (s′, a′
1))"
REFERENCES,0.7330677290836654,"= φ(T) + γEs′,a′
1[φ(T ′) + γEs′′,a′′
1 [∇θ1qπ1
i (T ′′)]]
(new tuples T ′′are conditioned on previous ones)
...
(repeatedly unroll ∇qπ1
i ) ∗=
X T ′ ∞
X"
REFERENCES,0.7370517928286853,"k=0
Pr(T →T ′; k)γkφ(T ′) ∝
X"
REFERENCES,0.7410358565737052,"T ′
lim
t→∞Pr(St = s′|T, γπ1)
|
{z
}
dT (s′)"
REFERENCES,0.7450199203187251,"φ(T ′)
(normalize the series term) =
X"
REFERENCES,0.749003984063745,"T ′
dT (s′)Es′′,a′′
1 [∇θ1 log π1(a′′
1|s′′)qπ1
i (s′′, a′′
1)]
(dT is the discounted visitation distribution induced by π1)"
REFERENCES,0.7529880478087649,"=
E
s′∼dT (.)
a′
1∼π1(.|s′)"
REFERENCES,0.7569721115537849,"[∇θ1 log π1(a′
1|s′)qπ1
i (s′, a′
1)]"
REFERENCES,0.7609561752988048,"=
E
T ′∼ρ(.|T )[∇θ1 log π1(a′
1|s′)qπ1
i (T ′)]
(use T ′ to simplify notation)"
REFERENCES,0.7649402390438247,"Since g(T) = qπ1
i (s, a1) −maxu Qi(s, u), the gradient of F(π1) can be expressed as:"
REFERENCES,0.7689243027888446,"∇θ1F(π1) =
X"
REFERENCES,0.7729083665338645,"2≤i≤N
αiET

∇θ1 log π1(a1|s)g(T)2
|
{z
}
A"
REFERENCES,0.7768924302788844,"+2g(T)∇θ1qπ1
i (T)] =
X"
REFERENCES,0.7808764940239044,"2≤i≤N
αiET

A + 2g(T)
E
T ′∼ρ(.|T )[∇θ1 log π1(a′
1|s′)qπ1
i (T ′)]
 =
X"
REFERENCES,0.7848605577689243,"2≤i≤N
αiET

A + 2g(T) · ET ′[∇θ1 log π1(a′
1|s′)qπ1
i (T ′)]
"
REFERENCES,0.7888446215139442,"In all,"
REFERENCES,0.7928286852589641,"∇θ1F(π1) =
X"
REFERENCES,0.796812749003984,"2≤i≤N
αiEs,π1

∇θ1 log π1(a1|s)g(s, a1)2+2g(s, a1)Es,π1[∇θ1 log π1(a1|s)qπ1
i (s, a1)]
"
REFERENCES,0.8007968127490039,"Proposition 1. Consider an L-action setting of n agents. In expectation, the number of steps T
needed to visit all Ln action conﬁgurations at least once without coordinated exploration grows at
least exponentially with the number of agents. More concretely, E[T] = Ω(nLn)."
REFERENCES,0.8047808764940239,"Proof. Let M = Ln. Since agents tend to visit different action conﬁgurations with no coordinated
behavior, one can equivalently say that agents uniformly pick a conﬁguration out of all Ln possible"
REFERENCES,0.8087649402390438,Under review as a conference paper at ICLR 2022
REFERENCES,0.8127490039840638,"conﬁgurations at each step. Let Tk be the number of steps to visit the k-th distinct conﬁguration after
covering k −1 distinct action tuples. Observe that:"
REFERENCES,0.8167330677290837,"E[T] = M
X"
REFERENCES,0.8207171314741036,"k=1
E[Tk]
(12)"
REFERENCES,0.8247011952191236,"Now, Pr[Tk = i] =
  k−1"
REFERENCES,0.8286852589641435,"M
i−1 
1 −k−1"
REFERENCES,0.8326693227091634,"M

meaning that Tk follows a geometric distribution. Thus,"
REFERENCES,0.8366533864541833,"E[Tk] = ∞
X i=1 k −1 M"
REFERENCES,0.8406374501992032,i−1M −k + 1 M
REFERENCES,0.8446215139442231,"
i =
M
M −k + 1
(13)"
REFERENCES,0.848605577689243,"Getting back to Eq. (12),"
REFERENCES,0.852589641434263,"E[T] = M M
X k=1"
REFERENCES,0.8565737051792829,"1
M −k + 1 = M M
X k=1"
REFERENCES,0.8605577689243028,"1
k > M
Z M 1"
REFERENCES,0.8645418326693227,"1
xdx = M ln M = nLn ln L
(14)"
REFERENCES,0.8685258964143426,And the conclusion follows.
REFERENCES,0.8725099601593626,"B
DETAILS OF MAPE TASKS"
REFERENCES,0.8764940239043825,"Cooperative Navigation: In this environment, N agents must collaborate to reach a set of N
landmarks with known positions. Agents are rewarded based on how far any agent is from each
landmark, meaning that the agents learn to spread with each agent covering one landmark. The
agents, which occupy a signiﬁcant physical space, are aware of their relative positions to each other
and are further penalized when colliding with each other."
REFERENCES,0.8804780876494024,"Table 2: Avg # of collisions per episode and avg agent distance from a landmark in the cooperative
navigation task, after 25000 episodes. we achieved optimal performance in the N = 3 case, with
near-optimal performance in the N = 6 case, as agents focused more on not colliding with each other
(lowest collision average)"
REFERENCES,0.8844621513944223,"N = 3
N = 6
Agent πππ
Average dist.
# collisions
Average dist.
# collisions
ALITA
1.559
0.185
3.349
1.294
MADDPG
1.767
0.209
3.345
1.366
DDPG
1.858
0.375
3.350
1.585"
REFERENCES,0.8884462151394422,"Cooperative Communication: Here, a stationary speaker must guide a listener in an environment
consisting of three landmarks of differing colors. At each episode, one landmark of a particular color
is set as a goal for the listener to be reached, however, only the speaker can observe which landmark
the listener must navigate to. Moreover, The speaker can produce a communication output at each
time step which is observed by the listener. The latter must navigate the environment to reach the
correct landmark. Agents are collectively rewarded at the end of an episode based on the listener’s
distance from the correct landmark."
REFERENCES,0.8924302788844621,"Physical Deception: This environment consists of N agents and N landmarks, with one landmark
as the target of all agents. The agents are rewarded based on the distance of the closest agent to the
target landmark, making it sufﬁcient for only one agent to reach it. An adversary agent also tries to
reach the target landmark, while the agents are penalized as it gets closer to the target. The adversary,
however, does not know which landmark is the target and must deduce it from the agents’ behavior.
For that reason, agents must cooperate to trick the adversary by learning to cover all the landmarks.
This task shows that our algorithm is applicable not only to cooperative interactions but to mixed
environments as well."
REFERENCES,0.896414342629482,"In the cooperative communication and physical deception tasks, ALITA obtained the highest success
rate across all baselines."
REFERENCES,0.900398406374502,Under review as a conference paper at ICLR 2022
REFERENCES,0.9043824701195219,"Table 3: Percentage of episodes where the agent reached the target landmark and average distance
from the target in the cooperative communication environment, after 25000 episodes."
REFERENCES,0.9083665338645418,"Agent
Target reach %
Average distance
ALITA
90.3%
0.093
MADDPG
84.0%
0.133
DDPG
32.0%
0.456
DQN
24.8%
0.754
Actor-Critic
17.2%
2.071
TRPO
20.6%
1.573
REINFORCE
13.6%
3.333"
REFERENCES,0.9123505976095617,"Table 4: Results on the physical deception task, with N = 2 cooperative agents/landmarks. Success
(succ %) for agents (AG) and adversaries (ADV) is if they are within a small distance from the target
landmark."
REFERENCES,0.9163346613545816,"Agent πππ
Adversary πππ
AG succ %
ADV succ %
∆succ %
ALITA
MADDPG
95.2%
45.1%
50.1%
MADDPG
MADDPG
94.4%
39.2%
55.2%
MADDPG
DDPG
92.2%
16.4%
75.8%
DDPG
MADDPG
68.9%
59.0%
9.9%
DDPG
DDPG
74.7%
38.6%
36.1%"
REFERENCES,0.9203187250996016,"C
ADDITIONAL EXPERIMENTS ON CONTINUOUS ENVIRONMENTS"
REFERENCES,0.9243027888446215,"Since the formulation of F needs a shared buffer, SAC and TD3 stand as the best off-policy
candidates to be incorporated with our framework, as they have shown great performances on many
benchmarks. SAC, however, uses stochastic policies in general which makes it infeasible to combine
with the formulation of rint. Therefore, we use TD3 as our learning model to measure its performance
on a suite of PyBullet (Tan et al., 2018) continuous control tasks, interfaced through OpenAI Gym
(Brockman et al., 2016). While many previous works utilized the Mujoco (Todorov et al., 2012)
physics engine to simulate the system dynamics of these tasks, we found it better to evaluate our
method on benchmark problems powered by PyBullet simulator since it is widely reported that
PyBullet problems are harder to solve (Tan et al., 2018) when compared to Mujoco. Also, Pybullet is
license-free, unlike Mujoco that is only available to its license holders."
REFERENCES,0.9282868525896414,"(a) Humanoid-v3
(b) Ant
(c) Walker2D"
REFERENCES,0.9322709163346613,"(d) HalfCheetah
(e) Reacher"
REFERENCES,0.9362549800796812,"Figure 5: Learning curves for the OpenAI gym continuous control tasks. The shaded region represents
quarter a standard deviation of the average evaluation. Curves are smoothed for visual clarity."
REFERENCES,0.9402390438247012,"We compare our method to the original twin delayed deep deterministic policy gradients (TD3)
(Fujimoto et al., 2018); soft actor critic (SAC) (Haarnoja et al., 2018); proximal policy optimization
(PPO) (Schulman et al., 2017), a stable and efﬁcient on-policy policy gradient algorithm; deep
deterministic policy gradient (DDPG); trust region policy optimization (TRPO) (Schulman et al.,"
REFERENCES,0.9442231075697212,Under review as a conference paper at ICLR 2022
REFERENCES,0.9482071713147411,"2015); Tsallis actor-critic (TAC) (Chen & Peng, 2019), a recent off-policy algorithm for learning
maximum entropy policies, where we use the implementation of the authors4 5; and Actor-Critic
using Kronecker-Factored Trust Region (ACKTR) (Wu et al., 2017), as implemented by OpenAI’s
baselines repository 6. Each task is run for at least 1 million time steps and the average return of 15
episodes is reported every 5000 time steps. To enable reproducibility, each experiment is conducted
on 10 random seeds of Gym simulator and network initialization. Results of the best performing
agent of the two across different methods are reported in Figure (5)."
REFERENCES,0.952191235059761,"D
TRAINING DETAILS"
REFERENCES,0.9561752988047809,"D.1
GENERAL CONFIGURATIONS"
REFERENCES,0.9601593625498008,"We use a buffer-size of 106 entries and a batch-size of 1024. We collect 100 transitions by interacting
with the environment for each learning update. For all tasks in our hyper-parameter searches, we
train the agents for 15, 000 episodes of 100 steps and then re-train the best conﬁguration for each
algorithm-environment pair for twice as long (30, 000 episodes) to ensure full convergence for the
ﬁnal evaluation. We use a discount factor γ of 0.95, an inﬂuence importance temperature λ of 0.1,
and a gradient clipping threshold of 0.5 in all experiments unless otherwise speciﬁed. Each cloned
critic is updated 4 time per step."
REFERENCES,0.9641434262948207,"D.2
SPARSE PUSH BOX AND SPARSE SECRET ROOM, MAPE, & GYM"
REFERENCES,0.9681274900398407,"We use the Adam optimizer (Kingma & Ba, 2014) to perform parameter updates. All models (actors,
critics and proxy critics) are parametrized by feedforward networks containing two hidden layers of
128 units excpet for the autoencoder network where we use 7 hidden layers with dimensions (128,
64, 12, 3, 12, 64, 128), respectively. All models’ parameters are initialized using Glorot Initialization
method (Glorot & Bengio, 2010); while the autoencoder’s parameters are initialized using Kaiming
method (He et al., 2015). We employ the Rectiﬁed Linear Unit (ReLU) as activation function and
layer normalization (Ba et al., 2016) on the pre-activations unit to stabilize the learning."
REFERENCES,0.9721115537848606,"Table 5: Best found hyper-parameters for the Sparse-reward tasks, MAPE, & Gym environments"
REFERENCES,0.9760956175298805,"HYPER-PARAMETER
PUSH BOX
SECRET ROOM
MAPE
HUMANOID-V3
GYM (EXCEPT FOR HUMANOID-V3)
λπ
0.10
0.10
0.01
0.10
0.10
{ωi}n−1
1
0.10
0.10
0.01
0.10
0.10
β
0.15
0.10
0.10
0.15
0.1"
REFERENCES,0.9800796812749004,"D.3
SMAC"
REFERENCES,0.9840637450199203,"The architecture of all agent networks is a DRQN (Hausknecht & Stone, 2015) with a recurrent layer
comprised of a GRU with a 64-dimensional hidden state, with a fully-connected layer before and
after. All neural networks are trained using RMSprop (α = 0.99 with no weight decay or momentum)
with learning rate 5 × 10−4."
REFERENCES,0.9880478087649402,Table 6: Best found hyper-parameters for the SMAC environments
REFERENCES,0.9920318725099602,"Hyper-parameter
Corridor
5m_vs_6m
3s_vs_5z
2s3z
λπ
0.09
0.03
0.01
0.01
{ωi}n−1
1
0.03
0.03
0.01
0.01
β
0.15
0.15
0.10
0.10"
REFERENCES,0.9960159362549801,"4https://github.com/haarnoja/sac
5https://github.com/yimingpeng/sac-master
6https://github.com/openai/baselines"
