Section,Section Appearance Order,Paragraph
INTRODUCTION,0.0,"1
INTRODUCTION
Ofï¬‚ine Reinforcement Learning (RL) is one of the important areas of RL where the learner is pre-
sented with a static dataset consisting of transition-related information (state, action, reward, and
next state) collected by some behavior policy, and needs to learn purely from the ofï¬‚ine data without
any future online interaction with the environment. Ofï¬‚ine RL is used in a number of applications
where online random experimentation is costly or dangerous such as health care (Kosorok & Laber,
2019), digital marketing (Chen et al., 2019) and robotics (Levine et al., 2020)."
INTRODUCTION,0.0010384215991692627,"The performance guarantees of ofï¬‚ine RL often rely on two quantities: the coverage of the ofï¬‚ine
data and the property of the function approximation used in the algorithms. For instance, for the
classic Fitted-Q-iteration (FQI) algorithm (Ernst et al., 2005; Munos & SzepesvÂ´ari, 2008), it requires
(a) full coverage in the ofï¬‚ine data, i.e., max(s,a) dÏ€(s, a)/Ï(s, a) < âˆfor any stochastic policies
Ï€ including history-dependent non-Markovian policies, where dÏ€(s, a) is a state-action occupancy
distribution of a policy Ï€ and Ï(s, a) is an ofï¬‚ine distribution, (b) realizability in a Q function
class, i.e., the optimal Q function belongs to the function class, and (c) Bellman completeness, i.e.,
applying the Bellman operator on any function in the function class results in a new function that
also belongs to the function class (see the ï¬rst row in Table 1). Among these three assumptions,
the full coverage and the Bellman completeness are particularly strong. The full coverage means
that the behavior policy needs to be exploratory enough, although ï¬guring out an exploratory policy
itself is an extremely hard problem for large-scale MDPs. The Bellman completeness assumption
does not have a monotonic property, i.e., even starting with a function class that originally permits
Bellman completeness, slightly increasing the capacity of the function class could result in a new
class that does not have Bellman completeness anymore. Thus, we aim to relax the assumptions on
the ofï¬‚ine data and the function class. Particularly, we are interested in the following question:"
INTRODUCTION,0.0020768431983385254,"Given a realizable function class and an ofï¬‚ine distribution that only provides
partial coverage, can we learn a policy that is able to compete with any policy that
is covered by the ofï¬‚ine distribution?"
INTRODUCTION,0.003115264797507788,"We study this question from a model-based learning perspective and provide an afï¬rmative answer
to the question. More speciï¬cally, different from FQI, we start with a realizable model class, i.e.,
the ground truth transition falls into the model class. We further abandon the strong full coverage
assumption, and instead, assume partial coverage which means the ofï¬‚ine data distribution only
covers a state-action distribution of some high-quality comparator policy Ï€âˆ—(Ï€âˆ—is not necessarily"
INTRODUCTION,0.004153686396677051,Published as a conference paper at ICLR 2022
INTRODUCTION,0.005192107995846314,"Methods
Type
Coverage
Additional Structures
FQI (Munos & SzepesvÂ´ari, 2008)
F
Full: maxs,a
dÏ€(s,a)"
INTRODUCTION,0.006230529595015576,"Ï(s,a) < âˆ, âˆ€Ï€
Bellman complete"
INTRODUCTION,0.007268951194184839,"Minimax Way (Uehara et al., 2020)
F
Full: maxs,a
dÏ€(s,a)"
INTRODUCTION,0.008307372793354102,"Ï(s,a) < âˆ, âˆ€Ï€
Realizability in density ratio
Duan et al. (2020)
F
Full: Es,aâˆ¼ÏÏ†(s, a)Ï†(s, a)âŠ¤is PSD
Linear Bellman complete
Xie & Jiang (2020)
F
Full: maxs,a,sâ€² P âˆ—(sâ€²|s,a)"
INTRODUCTION,0.009345794392523364,"Ï(sâ€²)
< âˆ
None"
INTRODUCTION,0.010384215991692628,"Liu et al. (2020)
F
Partialâ€  : maxs,a
dÏ€âˆ—(s,a)"
INTRODUCTION,0.01142263759086189,"Ï(s,a)
< âˆ
Bellman / Policy class complete"
INTRODUCTION,0.012461059190031152,"Rashidinejad et al. (2021)
F
Partial: maxs,a
dÏ€âˆ—(s,a)"
INTRODUCTION,0.013499480789200415,"Ï(s,a)
< âˆ
Tabular MDP"
INTRODUCTION,0.014537902388369679,"Jin et al. (2020b); Zhang et al. (2021b)
F
Partialâ€ â€ : maxx
xâŠ¤Es,aâˆ¼dÏ€âˆ—Ï†(s,a)(Ï†(s,a))âŠ¤x"
INTRODUCTION,0.01557632398753894,"xâŠ¤Es,aâˆ¼ÏÏ†(s,a)(Ï†(s,a))âŠ¤x
< âˆ
Linear MDP (Jin et al., 2020a)"
INTRODUCTION,0.016614745586708203,"Xie et al. (2021)
F
Partial: maxf
âˆ¥fâˆ’T fâˆ¥2
dÏ€âˆ—
âˆ¥fâˆ’T fâˆ¥2Âµ
< âˆ
Bellman complete"
INTRODUCTION,0.017653167185877467,"Zanette et al. (2021)
F
Partial : maxx
xâŠ¤Es,aâˆ¼dÏ€âˆ—Ï†(s,a)(Ï†(s,a))âŠ¤x"
INTRODUCTION,0.018691588785046728,"xâŠ¤Es,aâˆ¼ÏÏ†(s,a)(Ï†(s,a))âŠ¤x
< âˆ
Linear Bellman complete"
INTRODUCTION,0.01973001038421599,"Batch (Ross & Bagnell, 2012)
B
Full: maxs,a
dÏ€(s,a)"
INTRODUCTION,0.020768431983385256,"Ï(s,a) < âˆ, âˆ€Ï€
None"
INTRODUCTION,0.021806853582554516,"Milo (Chang et al., 2021)
B
Partial: maxx
xâŠ¤Es,aâˆ¼dÏ€âˆ—Ï†(s,a)(Ï†(s,a))âŠ¤x"
INTRODUCTION,0.02284527518172378,"xâŠ¤Es,aâˆ¼ÏÏ†(s,a)(Ï†(s,a))âŠ¤x
< âˆ
KNR / GP"
INTRODUCTION,0.023883696780893044,"Partialâ€ â€ â€ : maxs,a
dÏ€âˆ—(s,a)"
INTRODUCTION,0.024922118380062305,"Ï(s,a)
< âˆ
None"
INTRODUCTION,0.02596053997923157,"Partial: maxx
xâŠ¤Es,aâˆ¼dÏ€âˆ—Ï†(s,a)(Ï†(s,a))âŠ¤x"
INTRODUCTION,0.02699896157840083,"xâŠ¤Es,aâˆ¼ÏÏ†(s,a)(Ï†(s,a))âŠ¤x
< âˆ
Linear MDP /KNR / GP
CPPO (Ours)
B"
INTRODUCTION,0.028037383177570093,"Partial: maxx
xâŠ¤Es,aâˆ¼dÏ€âˆ—Ï†âˆ—(s,a)(Ï†âˆ—(s,a))âŠ¤x"
INTRODUCTION,0.029075804776739357,"xâŠ¤Es,aâˆ¼ÏÏ†âˆ—(s,a)(Ï†âˆ—(s,a))âŠ¤x
< âˆ
Low-rank MDP (unknown Ï†âˆ—)"
INTRODUCTION,0.030114226375908618,"Table 1: Comparison among existing works regarding their type, coverage, and additional structural
assumptions on the function class or MDPs. Type F means model-free and type B means model-
based. Partial coverage means 2that the ofï¬‚ine distribution Ï covers a state-action distribution of
a comparator policy Ï€âˆ—. â€  means it assumes an accurate density estimator for Ï(s, a). â€ â€  means
although the analysis in Jin et al. (2020a) is done under the full coverage for linear MDPs, based on
the argument (Zhang et al., 2021b), we can show the algorithm has the PAC guarantee under partial
coverage in terms of the relative condition number for linear MDPs. â€  â€  â€  means that we can reï¬ne
it to a more adaptive quantity using the model class (i.e., Deï¬nition 1). All the methods in the table
require realizability in the function class."
INTRODUCTION,0.03115264797507788,"the optimal policy, and Ï€âˆ—could be non-Markovian), i.e., maxs,a dÏ€âˆ—(s, a)/Ï(s, a) < âˆ, We design
an algorithm â€” Constrained Pessimistic Policy Optimization (CPPO), which can learn a policy that
is as good as any comparator policy Ï€âˆ—that is covered by the ofï¬‚ine data. The fact that CPPO can
learn to compete against history-dependent policies is meaningful in ofï¬‚ine RL when the ofï¬‚ine
data does not cover the optimal policy."
INTRODUCTION,0.032191069574247146,"While one could assume density ratio based concentrability coefï¬cient (maxs,a dÏ€âˆ—(s, a)/Ï(s, a))
to be under control for small size MDPs, in large-scale MDPs (e.g. continuous state space), the den-
sity ratio could quickly become an extremely large quantity which makes the performance guarantee
vacuous. When applying CPPO to MDPs with additional structural assumptions, we can seamlessly
reï¬ne the density ratio based concentrability coefï¬cient to more natural and tighter quantities. No-
tably, we consider the ofï¬‚ine representation learning setting where the underlying MDPs permit a
low-rank structure (unlikely linear MDPs (Jin et al., 2020a; Yang & Wang, 2020), we do not assume
the ground truth state-action feature representation Ï†â‹†is known, and instead we need to learn Ï†â‹†)
and we show that we can reï¬ne the density ratio to a relative condition number that is deï¬ned us-
ing the unknown true state-action feature representation Ï†â‹†. Intuitively this means that as long as
there exists a high-quality comparator policy that only visits the subspace (deï¬ned using the true
representation Ï†) that is covered by the ofï¬‚ine data, CPPO can compete against such a policy, even
without knowing the true Ï†â‹†. Such bounded relative condition number assumption is much weaker
than the bounded density ratio assumption.3 While the concept of relative condition number was
originally introduced in the online RL setting (e.g., Agarwal et al. (2020c;a) with a known linear
feature Ï†), and later was introduced in ofï¬‚ine RL (Zhang et al. (2021b); Chang et al. (2021)), these
prior works all rely on the fact that the feature representation Ï† is known to the learner a priori
(see Table 1 for the comparison). Another interesting example is factored MDPs (Kearns & Koller,
1999) where we show CPPO reï¬nes the density ratios to be density ratio associated with individual
factors, which leverages the factored structure and is provably tighter. We also give examples on
linear MDPs (Yang & Wang, 2020), kernelized nonlinear regulator (KNRs) (Kakade et al., 2020),
where we again show that CPPO enjoys problem speciï¬c quantities for measuring the coverage."
INTRODUCTION,0.033229491173416406,"Our contributions.
Our contributions are two folds, which we summarize below:"
INTRODUCTION,0.03426791277258567,"3Strictly speaking, in Jin et al. (2020b); Rashidinejad et al. (2021), a comparator policy is restricted to the
optimal policy. In Chang et al. (2021); Zanette et al. (2021); Xie et al. (2021) and CPPO, a comparator policy
can be any policy."
INTRODUCTION,0.035306334371754934,Published as a conference paper at ICLR 2022
INTRODUCTION,0.036344755970924195,"1. We show that in the model-based setting, realizability and partial coverage is enough to
learn a high-quality comparator policy (Theorem 1). Notably, (1) this result holds for any
MDPs with realizable model classes, (2) we can compete against even history-dependent
policies. This is in sharp contrast to the state-of-art provable model-free ofï¬‚ine RL results:
see Table 1 on page 2 for detailed comparisons to prior works."
INTRODUCTION,0.037383177570093455,"2. Under additional structural assumptions (e.g., KNRs, linear MDPs (Yang & Wang, 2020),
linear mixture MDPs (Ayoub et al., 2020), low-rank MDPs, factored MDPs), we show that
we can seamlessly reï¬ne the density ratio based concentrability coefï¬cients to problem spe-
ciï¬c quantities. This ï¬‚exibility to adapt to problem speciï¬c coverage measuring quantities
is in sharp contrast to other model-free ofï¬‚ine RL algorithms such as minimax based ap-
proaches (Uehara et al., 2020) which, to the best of our knowledge, cannot leverage MDPâ€™s
structures (e.g., linear MDPs) to reï¬ne its density ratio based concentrability coefï¬cients."
INTRODUCTION,0.03842159916926272,"While we focus on the model-based setting and have demonstrated advantages of our approach over
model-free ones (i.e., no more Bellman completeness assumption on function classes, being able to
compete against a larger pool of policies, and the ability to seamlessly adapt to problem dependent
structures), it is worth noting that realizability in the model-based setting is usually considered
stronger than the one in the model-free setting. On the empirical side, model-based ofï¬‚ine RL
algorithms are the state-of-art (e.g., Yu et al. (2020); Kidambi et al. (2020); Matsushima et al. (2020);
Cang et al. (2021); Chang et al. (2021)). Our theoretical results provide a sharp contrast between
model-based and model-free approaches in ofï¬‚ine RL."
RELATED WORK,0.03946002076843198,"2
RELATED WORK"
RELATED WORK,0.040498442367601244,"We discuss two families of related works of ofï¬‚ine RL. In Appendix C, we discuss related works
about representation learning in RL."
RELATED WORK,0.04153686396677051,"Insufï¬cient coverage of the dataset due to the lack of online exploration is known as the main
challenge in ofï¬‚ine RL (Wang et al., 2020). To deal with this problem, a number of methods have
been recently proposed from both model-free (Wu et al., 2019; Touati et al., 2020; Kumar et al.,
2020; Liu et al., 2020; Rezaeifar et al., 2021; Fujimoto et al., 2019; Fakoor et al., 2021; Ghasemipour
et al., 2021; Buckman et al., 2020) and model-based perspectives (Yu et al., 2020; Kidambi et al.,
2020; Matsushima et al., 2020; Yin et al., 2021). More or less, their methods rely on the idea
of pessimism and its variants in the sense that the learned policy can avoid uncertain regions not
covered by ofï¬‚ine data. As a theoretical side, Munos & SzepesvÂ´ari (2008); Duan et al. (2020;
2021); Fan et al. (2020) proved FQI has a PAC (probably approximately correct) guarantee under
realizability, the global coverage, and Bellman completeness conditions. Other ofï¬‚ine model-free
RL methods such as minimax ofï¬‚ine RL methods also require realizability and the global coverage
(Chen & Jiang, 2019; Antos et al., 2008; Uehara et al., 2021; Duan et al., 2021; Zhang et al., 2020;
Nachum et al., 2019). Recently, by leveraging the aforementioned the pessimism idea, Jin et al.
(2020a); Rajaraman et al. (2020) showed that pessimistic FQI can be applied to partial coverage
setting for linear and tabular MDPs. Comparing to their works, our analysis focuses on model-
based approaches with general function approximation. The ofï¬‚ine model-based method is known
to have a PAC guarantee under the realizability and the global coverage (Ross & Bagnell, 2012;
Chen & Jiang, 2019). As the most closely related work, Chang et al. (2021) proved a model-based
method with an additional penalty term can weaken the assumption from the global coverage to the
partial coverage for structured MDPs such as KNRs and Gaussian Processes models (Deisenroth &
Rasmussen, 2011). In this work, we consider arbitrary MDPs with a realizable model class and aim
for PAC bounds under a partial coverage condition."
PRELIMINARIES,0.04257528556593977,"3
PRELIMINARIES"
PRELIMINARIES,0.04361370716510903,"We consider a Markov Decision process (MDP) M = {S, A, P, Î³, r, d0} where P : S Ã—A â†’âˆ†(S)
is the transition, r : S Ã— A â†’[0, 1] is the reward function, Î³ âˆˆ[0, 1) is the discount factor, and
d0 âˆˆâˆ†(S) is the initial state distribution. A policy Ï€ maps from state (or history) to distribution
over actions. Given a policy Ï€ and a transition distribution P, V Ï€
P denotes the expected cumulative
reward of Ï€ under P, d0 and r. Similarly, QÏ€
P : S Ã— A â†’R, AÏ€
P : S Ã— A â†’R are a Q-
function and advantage-function under P and Ï€. Given a transition P, we denote Ï€(P) as the
optimal policy associated with model P under reward r. We also denote dÏ€
P âŠ‚âˆ†(S Ã— A) as the
average state-action distribution of Ï€ under the transition model P, i.e, dÏ€
P = (1 âˆ’Î³) Pâˆ
t=0 Î³tdÏ€
P,t,"
PRELIMINARIES,0.0446521287642783,Published as a conference paper at ICLR 2022
PRELIMINARIES,0.04569055036344756,"where dÏ€
P,t âˆˆâˆ†(S Ã— A) is the distribution of (st, at) under Ï€ and P at a time-step t. We denote the
true transition distribution as P â‹†, which we do not know in advance. For simplicity, we suppose r
is known. The extension to the unknown reward is straightforward."
PRELIMINARIES,0.04672897196261682,"In the ofï¬‚ine RL setting, we have an ofï¬‚ine distribution Ï âˆˆâˆ†(S Ã— A), and an ofï¬‚ine dataset
D = {s(i), a(i), r(i), sâ€²(i)}n
i=1 which is sampled in the following way: s, a âˆ¼Ï, r = r(s, a), sâ€² âˆ¼
P â‹†(Â·|s, a). We hope to obtain Ï€(P â‹†) = arg maxÏ€ V Ï€
P â‹†from this ofï¬‚ine dataset without any further
interaction with the environment. We often denote ED[f(s, a, sâ€²)] = 1/n P"
PRELIMINARIES,0.04776739356178609,"(s,a,sâ€²)âˆˆD f(s, a, sâ€²).
Our goal is to construct an ofï¬‚ine RL algorithm Alg, which maps from D to Ï€ so that the subopti-
mality gap V Ï€âˆ—
P â‹†âˆ’V Alg(D)
P â‹†
for any comparator policy Ï€âˆ—âˆˆÎ  is minimized, where Î  in this work
can be an unrestricted policy class (e.g., including non-Markovian policies). Hereafter, c, c1, c2, Â· Â· Â·
are always universal constants."
PRELIMINARIES,0.04880581516095535,"Partial coverage.
Throughout this work, we do not assume Ï has global coverage. The global cov-
erage in this work means that the density ratio based concentrability coefï¬cient dÏ€
P â‹†(s, a)/Ï(s, a)
is upper-bounded by some constant C âˆˆR+ for all polices Ï€ âˆˆÎ  , or the feature covariance matrix
corresponding to the ofï¬‚ine distribution Es,aâˆ¼ÏÏ†(s, a)Ï†(s, a)âŠ¤(Ï† âˆˆS Ã— A â†’R is a feature
representation) is full rank and has a non-zero minimum eigenvalue, which are commonly used
assumptions in ofï¬‚ine RL (Munos, 2005; Antos et al., 2008; Chen & Jiang, 2019; Duan et al., 2020).
Under the full coverage, they show the output policy can compete with the globally optimal policy
Ï€(P â‹†). However, this assumption may not be true in practice as computing an exploratory policy
itself is a challenging task for large-scale RL problems. Instead, we are interested in the partial
coverage setting such as dÏ€âˆ—
P â‹†(s, a)/Ï(s, a) â‰¤C, which means the state-action occupancy measure
under some comparator policy Ï€âˆ—is covered by the ofï¬‚ine dataset. We want to design an algorithm
that can compete against any policy Ï€âˆ—that is covered by the ofï¬‚ine data. This assumption is much
weaker than the global coverage."
PESSIMISTIC MODEL-BASED OFFLINE RL,0.04984423676012461,"4
PESSIMISTIC MODEL-BASED OFFLINE RL"
PESSIMISTIC MODEL-BASED OFFLINE RL,0.05088265835929388,"We ï¬rst introduce a general model-based algorithm that has a PAC guarantee of the suboptimality
gap under partial coverage deï¬ned with a newly introduced concentrability coefï¬cient. The algo-
rithm takes a realizable model class as input and outputs a policy that is as good as any comparator
policy that is covered by the ofï¬‚ine data in the sense of the bounded concentrability coefï¬cient."
PESSIMISTIC MODEL-BASED OFFLINE RL,0.05192107995846314,"Our algorithm, Constrained Pessimistic Policy Optimization (CPPO) (Algorithm 1), takes a realiz-
able hypothesis class M (with P â‹†âˆˆM) consisting of |M| candidate models as input, computes
the maximum likelihood estimator (MLE) bPMLE using the given ofï¬‚ine data D = {s, a, sâ€²}. It then
forms a min-max objective subject to a constraint. The min-max objective introduces pessimism via
searching for the least favorable model P (in terms of its policyâ€™s value V Ï€
P ) that is feasible with re-
spect to the constraint. We can also express the constrained optimization procedure using a version
space MD and a policy optimization procedure deï¬ned below:"
PESSIMISTIC MODEL-BASED OFFLINE RL,0.0529595015576324,"max
Ï€âˆˆÎ 
min
P âˆˆMD V Ï€
P , where MD =
n
P | P âˆˆM, ED
h
TV( bPMLE(Â·|s, a), P(Â·|s, a))2i
â‰¤Î¾
o
,
(1)"
PESSIMISTIC MODEL-BASED OFFLINE RL,0.05399792315680166,"where TV(P1, P2) is a total variation (TV) distance between two distributions P1 and P2. The
version space MD contains models that are not far away from bPMLE in terms of the average TV
distance under D. The version space is constructed such that with high probability P â‹†âˆˆMD."
PESSIMISTIC MODEL-BASED OFFLINE RL,0.055036344755970926,"Below we state the algorithmâ€™s performance guarantee. Assuming for now that P â‹†âˆˆMD holds
with high probability, then, Ë†V Ï€ := minP âˆˆMD V Ï€
P is a pessimistic policy evaluation estimator, which
satisï¬es Ë†V Ï€ â‰¤V Ï€
P â‹†for all Ï€ âˆˆÎ . Using the idea of pessimism, we have the following observation:"
PESSIMISTIC MODEL-BASED OFFLINE RL,0.056074766355140186,"V Ï€âˆ—
P â‹†âˆ’V Ë†Ï€
P â‹†= V Ï€âˆ—
P â‹†âˆ’Ë†V Ï€âˆ—+ Ë†V Ï€âˆ—âˆ’V Ë†Ï€
P â‹†â‰¤V Ï€âˆ—
P â‹†âˆ’Ë†V Ï€âˆ—+ Ë†V Ë†Ï€ âˆ’V Ë†Ï€
P â‹†â‰¤V Ï€âˆ—
P â‹†âˆ’Ë†V Ï€âˆ—,"
PESSIMISTIC MODEL-BASED OFFLINE RL,0.05711318795430945,"where the ï¬rst inequality uses Ë†Ï€ = arg maxÏ€âˆˆÎ  Ë†V Ï€ and the second inequality uses Ë†V Ï€ â‰¤V Ï€
P â‹†for
all Ï€ âˆˆÎ . Thus, the ï¬nal error only incurs the policy evaluation error for the comparator policy Ï€âˆ—,
which leads to the error only depending on the concentrability coefï¬cient for the comparator policy."
PESSIMISTIC MODEL-BASED OFFLINE RL,0.058151609553478714,We deï¬ne the following new concentrability coefï¬cient that uses the model class M :
PESSIMISTIC MODEL-BASED OFFLINE RL,0.059190031152647975,Published as a conference paper at ICLR 2022
PESSIMISTIC MODEL-BASED OFFLINE RL,0.060228452751817235,Algorithm 1 Constrained Pessimistic Policy Optimization (CPPO)
PESSIMISTIC MODEL-BASED OFFLINE RL,0.0612668743509865,"1: Require: Models M, dataset D, parameter Î¾, policy class Î  (note Î  could be unrestricted)
2: Obtain the estimator Ë†PMLE by MLE: bPMLE = arg maxP âˆˆM ED[ln P(sâ€² | s, a)].
3: Constrained policy optimization:"
PESSIMISTIC MODEL-BASED OFFLINE RL,0.06230529595015576,"Ë†Ï€ = arg maxÏ€âˆˆÎ  minP âˆˆM V Ï€
P , s.t., ED
h
TV( bPMLE(Â·|s, a), P(Â·|s, a))2i
â‰¤Î¾."
PESSIMISTIC MODEL-BASED OFFLINE RL,0.06334371754932502,4: Return Ë†Ï€
PESSIMISTIC MODEL-BASED OFFLINE RL,0.06438213914849429,"Deï¬nition 1 (Model-based Concentrability Coefï¬cient). For a comparator policy Ï€âˆ—, we deï¬ne the
concentrability coefï¬cient Câ€ 
Ï€âˆ—as follows:"
PESSIMISTIC MODEL-BASED OFFLINE RL,0.06542056074766354,"Câ€ 
Ï€âˆ—= supP â€²âˆˆM"
PESSIMISTIC MODEL-BASED OFFLINE RL,0.06645898234683281,"E(s,a)âˆ¼dÏ€âˆ—
P â‹†[TV(P â€²(Â·|s,a),P â‹†(Â·|s,a))2]"
PESSIMISTIC MODEL-BASED OFFLINE RL,0.06749740394600208,"E(s,a)âˆ¼Ï[TV(P â€²(Â·|s,a),P â‹†(Â·|s,a))2] ."
PESSIMISTIC MODEL-BASED OFFLINE RL,0.06853582554517133,"The following theorem shows CPPO learns a policy that competes against Ï€âˆ—when Câ€ 
Ï€âˆ—< âˆ."
PESSIMISTIC MODEL-BASED OFFLINE RL,0.0695742471443406,"Theorem 1 (PAC Bound for CPPO with general function class). Assume P â‹†âˆˆM. We set Î¾ =
c1
ln(c2|M|/Î´)"
PESSIMISTIC MODEL-BASED OFFLINE RL,0.07061266874350987,"n
. Then, with probability 1 âˆ’Î´, for any comparator policy Ï€âˆ—âˆˆÎ  (Î  can be the
unrestricted policy class containing non-Markovian policies),"
PESSIMISTIC MODEL-BASED OFFLINE RL,0.07165109034267912,"V Ï€âˆ—
P â‹†âˆ’V Ë†Ï€
P â‹†â‰¤c3(1 âˆ’Î³)âˆ’2
q"
PESSIMISTIC MODEL-BASED OFFLINE RL,0.07268951194184839,"Câ€ 
Ï€âˆ—ln(c2|M|/Î´) n
."
PESSIMISTIC MODEL-BASED OFFLINE RL,0.07372793354101766,"To the best of our knowledge, this is the ï¬rst algorithm that achieves a PAC guarantee for any
MDPs under the partial coverage assumption Câ€ 
Ï€âˆ—< âˆwith only a realizable hypothesis class.
We emphasize that the inequality in the above uniformly holds for all policies with probability
1 âˆ’Î´ including history-dependent non-Markovian policies (see Remark 2). Note that the ability
to compete against non-Markovian policies in ofï¬‚ine RL is meaningful when the ofï¬‚ine data does
not cover the optimal policy Ï€â‹†(i.e., there could be a high-quality history-dependent policy that
is covered by the ofï¬‚ine data against which we want to compete). In model-free approaches, this
type of result generally cannot be obtained. Indeed, the model-free approach from Xie et al. (2021)
requires Î  to be a restricted Markovian policy class, since their bound contains poly(ln(|Î |))
dependence. For the detailed discussion, refer to Remark 1."
PESSIMISTIC MODEL-BASED OFFLINE RL,0.07476635514018691,"The quantity Câ€ 
Ï€âˆ—adaptively captures the discrepancy between the ofï¬‚ine data and the state-action
occupancy measure under a comparator policy Ï€âˆ—depending on the model class M. For example,
Câ€ 
Ï€âˆ—can be reduced to a relative condition number in KNRs. Besides, it is always upper bounded by
the density ratio based concentrability coefï¬cient:"
PESSIMISTIC MODEL-BASED OFFLINE RL,0.07580477673935618,"CÏ€âˆ—,âˆ:= sup(s,a)
dÏ€âˆ—
P â‹†(s,a)"
PESSIMISTIC MODEL-BASED OFFLINE RL,0.07684319833852545,"Ï(s,a) ."
PESSIMISTIC MODEL-BASED OFFLINE RL,0.0778816199376947,"One extreme case is that functions in M are all the same, which implies Câ€ 
Ï€âˆ—= 1 regardless."
PESSIMISTIC MODEL-BASED OFFLINE RL,0.07892004153686397,"Theorem 1 consider the case where the hypothesis class M is ï¬nite. When the hypothesis class is
inï¬nite, we can still obtain the PAC guarantee by utilizing the generalized result in Section A for any
realizable model class with valid statistical complexity (e.g., localized Rademacher complexity)."
PESSIMISTIC MODEL-BASED OFFLINE RL,0.07995846313603323,"Prior works that achieve PAC guarantees with only realizable model classes rely on much stronger
global coverage supÏ€ CÏ€,âˆ< âˆ(Chen & Jiang, 2019). Even when the comparator policy is
the optimal policy Ï€(P â‹†), the partial coverage condition CÏ€(P â‹†),âˆ< âˆis weaker.
Existing
pessimistic model-based algorithms and their theoretical results (Chang et al., 2021) often assume
that a point-wise model uncertainty measure is given as a by-product of model ï¬tting, which limits
the applicability to special linear models such as KNRs/GPs. CPPO can work for any MDPs with
the realizable function class having a valid statistical complexity such that the MLE properly works."
PESSIMISTIC MODEL-BASED OFFLINE RL,0.08099688473520249,"Remark 1 (Comparison to the model-free approach from Xie et al. (2021); Zanette et al. (2021)
). Xie et al. (2021) study the model-free setting where the function class Q models Q functions
assumed to be Bellman complete for any Markovian policy in Î . While directly comparing model-
based approaches to model-free approaches is hard as they use different inductive biases in function
classes, we can leverage the approach from Chen & Jiang (2019, Corollary 6) to convert a model"
PESSIMISTIC MODEL-BASED OFFLINE RL,0.08203530633437175,Published as a conference paper at ICLR 2022
PESSIMISTIC MODEL-BASED OFFLINE RL,0.08307372793354102,"class M to a pair of Q and Î  class. Speciï¬cally, we can convert a model class M to a pair of Q
class and Î  class such that Q will be realizable and also Bellman complete with respect to all Ï€ âˆˆÎ .
After such conversion from the model-based setting to the model-free setting, running the algorithm
from Xie et al. (2021) using Q and Î  achieves V Ï€âˆ—
P â‹†âˆ’V Ë†Ï€
P â‹†=
p"
PESSIMISTIC MODEL-BASED OFFLINE RL,0.08411214953271028,"Câ‹„ln(|M||Î |/n), âˆ€Ï€âˆ—âˆˆÎ , where
Câ‹„is some concentrability coefï¬cient. For the detailed derivation, we refer readers to Appendix
D. Since the suboptimality gap from such conversion incurs log |Î |, a policy class Î  cannot be too
large. Especially, unlike our results, it cannot take the unrestricted policy class as Î . This restriction
cannot be ï¬xed even if we use natural policy gradient (NPG) algorithms unless models have special
structures (Xie et al., 2021; Zanette et al., 2021). The details are given in Section D."
PESSIMISTIC MODEL-BASED OFFLINE RL,0.08515057113187954,"Thus, our theorem indicates two advantages of model-based approaches: (1) realizability in function
class is enough to ensure a PAC guarantee under a partial coverage condition, (2) it can compete
against a larger pool of candidate policies including history-dependent non-Markovian policies,
which is a meaningful property when the ofï¬‚ine data does not cover the globally optimal policy.
Next, we demonstrate another key advantage of our approach which is its ï¬‚exibility to be seam-
lessly applied to MDPs with special structures."
EXAMPLES WITH REFINED CONCENTRABILITY COEFFICIENTS,0.08618899273104881,"5
EXAMPLES WITH REFINED CONCENTRABILITY COEFFICIENTS"
EXAMPLES WITH REFINED CONCENTRABILITY COEFFICIENTS,0.08722741433021806,"In the previous section, our results apply to any MDP as long as its true transition belongs to a
function class M. In this section, we consider several concrete MDPs with additional structural
conditions. We show that by leveraging the additional structural conditions, we can reï¬ne the model-
based concentrability coefï¬cient to more natural quantities. The examples that we discuss here are:
(1) linear mixture MDPs which generalize linear MDPs from Yang & Wang (2020) and tabular
MDPs, (2) KNRs which generalize LQRs, (3) low-rank MDPs, and (4) factored MDPs."
EXAMPLES WITH REFINED CONCENTRABILITY COEFFICIENTS,0.08826583592938733,"Before proceeding, we clarify CPPO cannot capture linear MDPs in Jin et al. (2020a) that is differ-
ent from the one (Yang & Wang, 2020) we use, and linear Bellman-complete MDPs (Duan et al.,
2020) without any modiï¬cation since MLE-based model learning is no longer applicable to them.
However, other objective functions for learning models could be applied to these models (e.g., see
the nonparametric model-based learning approach from Lykouris et al. (2021); Neu & Pike-Burke
(2020) in the online setting), which we leave it as a future work."
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.0893042575285566,"5.1
TABULAR MDPS AND LINEAR MIXTURE MDPS
Tabular MDPs
Tabular MDPs are MDPs where the state and action spaces are ï¬nite. Although the
corresponding hypothesis class for tabular MDPs is inï¬nite, we can still run MLE, that is, estimating
P â‹†by the empirical distribution. Then, Algorithm 1 has the following guarantee."
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.09034267912772585,"Corollary 1 (PAC bound for tabular MDP). We set Î¾ = c1
|S|2|A| ln(n|S|A|c2/Î´)"
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.09138110072689512,"n
. Then with proba-
bility 1 âˆ’Î´, for all Ï€âˆ—âˆˆÎ ,"
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.09241952232606439,"V Ï€âˆ—
P â‹†âˆ’V Ë†Ï€
P â‹†â‰¤c3(1 âˆ’Î³)âˆ’2
q"
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.09345794392523364,"CÏ€âˆ—,âˆ|S|2|A| ln(n|S||A|c4/Î´) n 
."
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.09449636552440291,"Here, for tabular MDPs with M = {P : P(Â·|s, a) âˆˆâˆ†(S), âˆ€s, a}, the model-based concentrability
coefï¬cient in Deï¬nition 1 is equal to the density ratio based concentrability coefï¬cient CÏ€âˆ—,âˆwhich
is the right quantity for small-size tabular MDPs."
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.09553478712357218,"Linear mixture MDPs
We deï¬ne linear mixture MDPs (Ayoub et al., 2020; Modi et al., 2020).
Deï¬nition 2 (Linear mixture MDPs). Given a feature vector Ïˆ : (S, A, S) â†’Rd, a linear mixture
MDP is an MDP where the ground truth transition is P â‹†(sâ€²|s, a) := Î¸â‹†âŠ¤Ïˆ(s, a, sâ€²), Î¸â‹†âˆˆRd."
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.09657320872274143,"By setting, Ïˆ(s, a, sâ€²) = Âµ(sâ€²) N Ï†(s, a) (âŠ—denotes the Kronecker product), linear mixture MDPs
include the following linear MDPs (Yang & Wang, 2020):"
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.0976116303219107,"Deï¬nition 3 (Linear MDPs). Linear MDP has P â‹†(sâ€²|s, a) := Pd1
i=1
Pd2
j=1 M â‹†
ijÂµi(sâ€²)Ï†j(s, a) with
Âµ : S â†’Rd1 and Ï† : S Ã— A â†’Rd2 are known features, and M â‹†âŠ‚Rd1Ã—d2."
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.09865005192107996,We use CPPO to learn on linear mixture MDPs. The corresponding M is
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.09968847352024922,"MMix =

Î¸âŠ¤Ïˆ(s, a, sâ€²) | Î¸ âˆˆÎ˜ âŠ‚Rd,
R
Î¸âŠ¤Ïˆ(s, a, sâ€²)d(sâ€²) = 1
âˆ€(s, a)
	
."
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.10072689511941849,Published as a conference paper at ICLR 2022
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.10176531671858775,"Given a function V
: S â†’R, deï¬ne the state-action feature indexed by V as ÏˆV (s, a) :=
R
Ïˆ(s, a, sâ€²)V (sâ€²)d(sâ€²), we have the following PAC guarantee."
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.102803738317757,"Corollary 2 (PAC bound for linear mixture MDPs). Suppose infs,a,sâ€² P â‹†(sâ€² | s, a) â‰¥c3 > 0,
Î˜ = {Î¸ : âˆ¥Î¸âˆ¥2 â‰¤R}, âˆ¥ÏˆV (s, a)âˆ¥2 â‰¤1, âˆ€V âˆˆS â†’[0, 1] and P â‹†âˆˆMMix. We set Î¾ =
c1d ln2(c2nR/Î´)/n. Then, with probability 1 âˆ’Î´, for any Ï€âˆ—in Î  (again Î  can be the unrestricted
policy class), CPPO outputs a policy Ë†Ï€ such that:"
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.10384215991692627,"V Ï€âˆ—
P â‹†âˆ’V Ë†Ï€
P â‹†â‰¤c4(1 âˆ’Î³)âˆ’2 s"
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.10488058151609553,"min(dCâ€ 
Ï€âˆ—, d2 Â¯CÏ€âˆ—,mix)ln2(c5nR/Î´)"
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.1059190031152648,"n
,
(2)"
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.10695742471443406,"where the concentrability coefï¬cient Â¯CÏ€âˆ—,mix is deï¬ned as:"
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.10799584631360332,"Â¯CÏ€âˆ—,mix :=
sup
P âˆˆZP â‹†
sup
xâˆˆRd ï£«"
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.10903426791277258,"ï£­
xâŠ¤Î£Ï€âˆ—,ÏˆV Ï€âˆ—
P x"
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.11007268951194185,"xâŠ¤Î£Ï,ÏˆV Ï€âˆ—
P x ï£¶ ï£¸"
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.1111111111111111,"with the localized class ZP â‹†:= {P : E(s,a)âˆ¼Ï[TV(P(Â· | s, a), P â‹†(Â· | s, a))2] â‰¤Î¾}, Î£Ï,ÏˆV Ï€âˆ—
P
="
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.11214953271028037,"E(s,a)âˆ¼Ï[ÏˆV Ï€âˆ—
P (s, a)ÏˆV Ï€âˆ—
P (s, a)âŠ¤], and Î£Ï€âˆ—,ÏˆV Ï€âˆ—
P
= Es,aâˆ¼dÏ€âˆ—
P â‹†[ÏˆV Ï€âˆ—
P (s, a)ÏˆV Ï€âˆ—
P (s, a)âŠ¤]."
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.11318795430944964,"When specializing to linear MDPs, the above bound still holds with Â¯CÏ€âˆ—,mix being replaced by the
relative condition number Â¯CÏ€âˆ—:"
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.1142263759086189,"Â¯CÏ€âˆ—:= sup
xâˆˆRd
xT Î£Ï€âˆ—x
xâŠ¤Î£Ï, x, where Î£Ï = E(s,a)âˆ¼Ï[Ï†(s, a)Ï†(s, a)âŠ¤], Î£Ï€âˆ—= E(s,a)âˆ¼dÏ€âˆ—
P â‹†[Ï†(s, a)Ï†(s, a)âŠ¤]."
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.11526479750778816,"This is the ï¬rst PAC-guarantee result in the ofï¬‚ine setting under partial coverage Â¯CÏ€âˆ—,mix < âˆfor
linear mixture MDPs. Â¯CÏ€âˆ—,mix is a newly-introduced concentrability coefï¬cient for linear mixture
MDPs. This coefï¬cient is measured on the integrated feature vectors Ï†V (s, a) for V : S â†’[0, 1].
Note the class of V is localized, i.e., we consider state-value functions V Ï€âˆ—
P (s) for all P centered
around P â‹†under data distribution Ï (i.e., P âˆˆZP â‹†). Such localization property ensures that
Â¯CÏ€âˆ—,mix â‰¤Câ€ 
Ï€â‹†(see Lemma 10 in Section F)."
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.11630321910695743,"Note that these relative condition number based quantiï¬ers are always tighter than the density ratio
based concentrability coefï¬cients (i.e., max{ Â¯CÏ€âˆ—, Â¯CÏ€âˆ—,mix} â‰¤CÏ€âˆ—,âˆ). For the special case where
Ï†(s, a) is a one-hot encoding vector, then they are reduced to the density ratio based concentrability
coefï¬cient. In a non-tabular setting, even if when the density ratio is inï¬nite, the relative condition
number can be still ï¬nite. Intuitively, the bounded relative condition number implies that the ofï¬‚ine
data covers the subspace that the comparator policy Ï€âˆ—visits."
"TABULAR MDPS AND LINEAR MIXTURE MDPS
TABULAR MDPS",0.11734164070612668,"We remark P â‹†(sâ€² | s, a) â‰¥c3 > 0 in Corollary 2 is a technical condition that allows us to calculate
the entropy integral of the hypothesis class easily. It can be potentially discarded by a more careful
argument following (van de Geer, 2000, Chapter 7). The norm assumption âˆ¥ÏˆV (s, a)âˆ¥2 < 1 is
commonly assumed in the online setting (Zhou et al., 2021)."
KERNELIZED NOLINEAR REGULATORS,0.11838006230529595,"5.2
KERNELIZED NOLINEAR REGULATORS"
KERNELIZED NOLINEAR REGULATORS,0.11941848390446522,"We consider the example of KNRs in this section. A kernelized Nonlinear Regulator (KNR) (Kakade
et al., 2020) is a model where the ground truth transition P â‹†(sâ€²|s, a) is deï¬ned as sâ€² = W â‹†Ï†(s, a)+Ïµ,
Ïµ âˆ¼N(0, Î¶2I), with Ï† : S Ã— A â†’Rd being a possibly nonlinear feature mapping. We denote
the corresponding model on W by P(W). We can apply Algorithm 1 and obtain its guarantee.
Especially, since TV(P(W)(Â· | s, a), P(W â‹†)(Â· | s, a))2 = Î˜(âˆ¥(W âˆ’W â‹†)Ï†(s, a)âˆ¥2
2) (Devroye
et al., 2018), Câ€ 
Ï€âˆ—is upper-bounded by the relative condition number Â¯CÏ€âˆ—."
KERNELIZED NOLINEAR REGULATORS,0.12045690550363447,"Then, we can also recover the result of Chang et al. (2021) which proposes a reward penalty-based
pessimistic ofï¬‚ine RL algorithm. The detail is given in Section B. In summary, we can show"
KERNELIZED NOLINEAR REGULATORS,0.12149532710280374,"V Ï€âˆ—
P â‹†âˆ’V Ë†Ï€
P â‹†â‰¤c1(1 âˆ’Î³)âˆ’2 min(d1/2, Â¯R)
âˆšÂ¯R
q"
KERNELIZED NOLINEAR REGULATORS,0.122533748701973,"dS Â¯
CÏ€âˆ—ln(1+n) n
, ."
KERNELIZED NOLINEAR REGULATORS,0.12357217030114226,where Â¯R := rank[Î£Ï]{rank[Î£Ï] + ln(c2/Î´)} and dS is the dimension of the state.
KERNELIZED NOLINEAR REGULATORS,0.12461059190031153,Published as a conference paper at ICLR 2022
KERNELIZED NOLINEAR REGULATORS,0.1256490134994808,"This implies CPPO can learn a policy that can compete against Ï€âˆ—with partial coverage Â¯CÏ€âˆ—< âˆ.
Note that the condition Â¯CÏ€âˆ—< âˆdoes not require Î£Ï to be full-rank. Also the bound uses rank[Î£Ï]
instead of d, which means that our bound is distribution dependent and is still valid even when
d = âˆas long as the ofï¬‚ine data only concentrate on a low-dimensional subspace."
LOW-RANK MDPS WITH REPRESENTATION LEARNING,0.12668743509865005,"5.3
LOW-RANK MDPS WITH REPRESENTATION LEARNING
We consider the representation learning in ofï¬‚ine RL. Following FLAMBE (Agarwal et al., 2020b),
we study low-rank MDPs but in the ofï¬‚ine setting. Note that low-rank MDPs here are a more
generalized model of the aforementioned linear MDPs (Yang & Wang, 2020) since the true feature
representation Ï†â‹†in a low-rank MDP is unknown.
Deï¬nition 4 (Low rank MDPs). The ground-truth model P â‹†admits a low rank decomposition with
a dimension d if there exists two embedding functions Âµâˆ—: S â†’Rd, Ï†âˆ—: S Ã— A â†’Rd s.t.
P â‹†(sâ€² | s, a) = Âµâˆ—(sâ€²)âŠ¤Ï†âˆ—(s, a). Neither Âµâˆ—nor Ï†âˆ—is known to the learner."
LOW-RANK MDPS WITH REPRESENTATION LEARNING,0.1277258566978193,"One interesting special case of a low-rank MDP is the following latent variable model (see Agarwal
et al. (2020b) for more details).
Deï¬nition 5 (Latent variable models). There exists a latent space Z along with functions Âµâˆ—: Z â†’
âˆ†(S) and Ï†âˆ—: S Ã— A â†’âˆ†(Z) s.t. P â‹†(Â· | s, a) = P"
LOW-RANK MDPS WITH REPRESENTATION LEARNING,0.12876427829698858,"zâˆˆZ Âµâˆ—(Â· | z)Ï†âˆ—(z | s, a)."
LOW-RANK MDPS WITH REPRESENTATION LEARNING,0.12980269989615784,"To tackle representation learning under partial coverage on low-rank MDPs, we setup function
classes as follows: given two function classes Î¨ âŠ‚S â†’Rd, Î¦ âŠ‚S Ã— A â†’Rd (both are re-
alizable in the sense that Âµâˆ—âˆˆÎ¨ and Ï†âˆ—âˆˆÎ¦), we consider a hypothesis class {Âµ(sâ€²)âŠ¤Ï†(s, a); Âµ âˆˆ
Î¨, Ï† âˆˆÎ¦}. Then, CPPO (Algorithm 1) and Theorem 1 still work under this setting. Note that this
function class setup is exactly the same as the one from FLAMBE."
LOW-RANK MDPS WITH REPRESENTATION LEARNING,0.1308411214953271,"Here we show that by leveraging the low-rankness, we can reï¬ne the concentrability coefï¬cient to
a relative condition number deï¬ned by the unknown true representation Ï†âˆ—. We emphasize that this
does not depend on the other features. Particularly, given a comparator policy Ï€âˆ—, we deï¬ne Â¯CÏ€âˆ—,Ï†â‹†:"
LOW-RANK MDPS WITH REPRESENTATION LEARNING,0.13187954309449637,"Â¯CÏ€âˆ—,Ï†â‹†= sup
xâˆˆRd
xâŠ¤Î£Ï€âˆ—x"
LOW-RANK MDPS WITH REPRESENTATION LEARNING,0.13291796469366562,"xâŠ¤Î£Ïx ,
Î£Ï€âˆ—:= Es,aâˆ¼dÏ€âˆ—
P â‹†Ï†âˆ—(s, a)Ï†âˆ—(s, a)âŠ¤,
Î£Ï := Es,aâˆ¼ÏÏ†âˆ—(s, a)Ï†âˆ—(s, a)âŠ¤."
LOW-RANK MDPS WITH REPRESENTATION LEARNING,0.13395638629283488,"We can show CPPO learns a policy that can compete against Ï€âˆ—as long as Â¯CÏ€âˆ—,Ï†â‹†< âˆ."
LOW-RANK MDPS WITH REPRESENTATION LEARNING,0.13499480789200416,"Theorem 2 (PAC bound for low-rank MDP). We set Î¾
=
c1
ln(|Î¦||Î¨|c2/Î´)"
LOW-RANK MDPS WITH REPRESENTATION LEARNING,0.1360332294911734,"n
.
Suppose (a):
âˆ¥Ï†(s, a)âˆ¥2 â‰¤1, âˆ€(s, a) âˆˆS Ã—A, âˆ€Ï† âˆˆÎ¦,
R
Âµ(sâ€²)âŠ¤Ï†(s, a)d(sâ€²) = 1 and
R
âˆ¥Âµ(s)âˆ¥2ds â‰¤
âˆš"
LOW-RANK MDPS WITH REPRESENTATION LEARNING,0.13707165109034267,"d, âˆ€Âµ âˆˆ
Î¨, Ï† âˆˆÎ¦, (b) Ï(s, a) = dÏ€b
P â‹†(s, a), (c) P â‹†(sâ€²|s, a) = Âµâˆ—(sâ€²)âŠ¤Ï†âˆ—(s, a) for some Âµâˆ—âˆˆÎ¨, Ï†âˆ—âˆˆÎ¦.
With probability at least 1 âˆ’Î´, for all Ï€âˆ—âˆˆÎ  (again Î  can be an unrestricted policy class), CPPO
(Algorithm 1) ï¬nds Ë†Ï€ such that:"
LOW-RANK MDPS WITH REPRESENTATION LEARNING,0.13811007268951195,"V Ï€âˆ—
P â‹†âˆ’V Ë†Ï€
P â‹†â‰¤c3
q"
LOW-RANK MDPS WITH REPRESENTATION LEARNING,0.1391484942886812,"Â¯CÏ€âˆ—,Ï†â‹†Ï‰Ï€âˆ—rank(Î£Ï) ln(|Î¨||Î¦|c4/Î´)"
LOW-RANK MDPS WITH REPRESENTATION LEARNING,0.14018691588785046,"(1âˆ’Î³)4n
, Ï‰Ï€âˆ—=

max(s,a)
Ï€âˆ—(a|s)
Ï€b(a|s)

(3)"
LOW-RANK MDPS WITH REPRESENTATION LEARNING,0.14122533748701974,"To the best of our knowledge, this is the ï¬rst established PAC result under the partial coverage
condition Â¯CÏ€âˆ—,Ï†â‹†< âˆ, Ï‰Ï€âˆ—< âˆfor low-rank MDPs in the ofï¬‚ine setting. We also emphasize
that our bound in Theorem 2 is distribution dependent, i.e., it depends on rank(Î£Ï) rather than
the exact rank d. Note that rank(Î£Ï) â‰¤d, and rank(Î£Ï) could be much smaller than d when the
ofï¬‚ine distribution only concentrates on a low-dimensional subspace (deï¬ned using Ï†âˆ—). Note that
the assumption that Ï‰Ï€âˆ—< âˆdoes not imply the state-action density ratio CÏ€âˆ—,âˆis small. Indeed,
Ï‰Ï€âˆ—< âˆis much weaker than CÏ€âˆ—,âˆ< âˆ."
FACTORED MDPS,0.142263759086189,"5.4
FACTORED MDPS
The last example we include is the factored MDP (Kearns & Koller, 1999) deï¬ned as follows:
Deï¬nition 6 (Factored MDPs). Let d âˆˆN+ and O being a small ï¬nite set. The state space S = Od,
and for each state s, we denote s[i] âˆˆO as the i-th variable of the state s. For each i âˆˆ[1, Â· Â· Â· , d],
the parents of i, pai âŠ‚[1, Â· Â· Â· , d], is the subset of state variables that directly inï¬‚uences i, i.e., the
transition is deï¬ned as follows:"
FACTORED MDPS,0.14330218068535824,"âˆ€s, a, sâ€² : P â‹†(sâ€²|s, a) = Qd
i=1 P â‹†
i (sâ€²[i]|s[pai], a)."
FACTORED MDPS,0.14434060228452752,"We will denote Si = O|pai|, and given s âˆˆS, we will have s[pai] âˆˆSi"
FACTORED MDPS,0.14537902388369678,Published as a conference paper at ICLR 2022
FACTORED MDPS,0.14641744548286603,"Due to the factorization, the transition operator P â‹†can be described with L := Pd
i=1 |A||O|1+|pai|"
FACTORED MDPS,0.1474558670820353,"many parameters. In contrast, the non-factored transition will need O(|O|d) parameters. When
|pai| â‰ªd âˆ€i, it is expected that we can learn this model with lower sample complexity by leveraging
the factorization which has been demonstrated in the online setting (Kearns & Koller, 1999). We
remark a factored MDP is an example where model-based approaches are necessary as neither the
optimal policy nor the Q functions are factored (Koller & Parr, 2000)."
FACTORED MDPS,0.14849428868120457,"We will slightly modify Algorithm 1 to take the factorization into consideration. First, we perform
MLE for model learning: each factor P â‹†
i is independently learned via MLE:"
FACTORED MDPS,0.14953271028037382,"âˆ€i âˆˆ[d], bPMLE,i = arg maxP ED[ln P(sâ€²[i]|s[pai], a)],
bP = Q"
FACTORED MDPS,0.1505711318795431,"i bPMLE,i.
Next, the constrained policy optimization procedure is deï¬ned as"
FACTORED MDPS,0.15160955347871236,"Ë†Ï€ = arg max
Ï€
min
P :=Q"
FACTORED MDPS,0.1526479750778816,"i Pi V Ï€
P , s.t., ED[TV(Pi(Â· | s, a), bPMLE,i(Â· | s, a))2] â‰¤Î¾i (âˆ€i âˆˆ[1, Â· Â· Â· , d])."
FACTORED MDPS,0.1536863966770509,"Note that in the above objective, there is no restriction on the policy, i.e., the arg max operator
searches over all possible policies including non-Markovian ones."
FACTORED MDPS,0.15472481827622014,"To analyze the performance of the above modiï¬ed CPPO, we introduce a specialized con-
centration coefï¬cient for factored MDPs that utilizes the factored structure.
We focus on
density ratio based concentrability coefï¬cients since in a factored MDP with the function class
M := {P = Q
i Pi : Pi âˆˆSi Ã— A â†’âˆ†(O)}, the concentrability coefï¬cient associated with
M in Deï¬nition 1 will be reduced to the density ratio. For any Ï€âˆ—, we deï¬ne the concentrability
coefï¬cients for the factored MDP as follows:"
FACTORED MDPS,0.1557632398753894,"Â¨CÏ€âˆ—,âˆ:=
max
jâˆˆ[1,Â·Â·Â· ,d]
max
sjâˆˆSj,aâˆˆA
dÏ€âˆ—
P â‹†(sj, a)
Ï(sj, a) ,"
FACTORED MDPS,0.15680166147455868,"where for sj âˆˆSj, we denote Î½(sj, a) := P"
FACTORED MDPS,0.15784008307372793,"sâˆˆS:s[paj]=sj Î½(s, a) for any distribution Î½ âˆˆâˆ†(SÃ—A)."
FACTORED MDPS,0.1588785046728972,"Comparing to CÏ€âˆ—,âˆdeï¬ned on the original state space S, here Â¨CÏ€âˆ—,âˆis deï¬ned over each state
space Sj associated with each factor j. Note that when |paj| = Î˜(1), |Sj| is exponentially smaller
than |S|. One can verify that Â¨CÏ€âˆ—,âˆâ‰¤CÏ€âˆ—,âˆ(see Appendix E.7), where CÏ€âˆ—,âˆignores the
factored structure and treat S as a whole single space. This formally demonstrates the beneï¬t of the
factored structure in terms of the coverage condition in ofï¬‚ine RL."
FACTORED MDPS,0.15991692627206647,"With the new deï¬nition of the concentrability coefï¬cients, now we are ready to state the PAC bound
of CPPO for factored MDPs. Recall L := Pd
i=1 Li, Li = |A||O|1+|pai|."
FACTORED MDPS,0.16095534787123572,"Theorem 3 (PAC bound for factored MDP). We set Î¾i = c1
Li ln(Lic2d/Î´)"
FACTORED MDPS,0.16199376947040497,"n
. Then with probability
1 âˆ’Î´, CPPO ï¬nds a policy Ë†Ï€ such that for all comparator policy Ï€âˆ—âˆˆÎ  (Î  can be unrestricted),"
FACTORED MDPS,0.16303219106957426,"V Ï€âˆ—
P â‹†âˆ’V Ë†Ï€
P â‹†â‰¤c3(1 âˆ’Î³)âˆ’2
q"
FACTORED MDPS,0.1640706126687435,"d Â¨
CÏ€âˆ—,âˆLÂ·ln(nLc4d/Î´) n
."
FACTORED MDPS,0.16510903426791276,"Note that our sub-optimality gap scales polynomially with respect to L, i.e., the complexity of the
factored MDP, rather than |S| which can be â„¦(exp(d))."
CONCLUSION,0.16614745586708204,"6
CONCLUSION"
CONCLUSION,0.1671858774662513,"We study model-based ofï¬‚ine RL with function approximation under partial coverage. We show that
for the model-based setting, realizability in function class and partial coverage together are enough
to learn a policy that is comparable to any policies (including history-dependent policies) covered by
the ofï¬‚ine distribution. Our result demonstrates a sharp contrast to model-free ofï¬‚ine RL approaches
which often require additional structural conditions in the function class (e.g., Bellman completion)
and have restrictions on the pool of candidate policies that they can compete against."
CONCLUSION,0.16822429906542055,"Some readers might wonder whether CPPO is computationally efï¬cient. The minimax optimiza-
tion problem arg maxÏ€âˆˆÎ  minP âˆˆM V Ï€
P ï¬ts into a framework of planning on robust MDPs (Nilim
& El Ghaoui, 2005; Iyengar, 2005). By introducing a robust Bellman equation, they proposed value
iteration and policy iteration algorithms, and showed that algorithms are practically tractable in the
tabular setting. In the non-tabular setting, Lim & Autef (2019); Tamar et al. (2014) propose the ex-
tension using function approximation. Thus, we can apply their methods to approximately solve the
minimax optimization problem in a model-free fashion. We leave the formal theoretical justiï¬cation
when using these approximation planning algorithms as an important direction for future work."
CONCLUSION,0.16926272066458983,Published as a conference paper at ICLR 2022
CONCLUSION,0.1703011422637591,ACKNOWLEDGEMENT
CONCLUSION,0.17133956386292834,"The authors would like to thank Nan Jiang, Tengyang Xie for valuable feedback."
CONCLUSION,0.17237798546209762,Masatoshi Ueharra is partially supported by Masason foundation.
REFERENCES,0.17341640706126688,REFERENCES
REFERENCES,0.17445482866043613,"Alekh Agarwal, Mikael Henaff, Sham Kakade, and Wen Sun. Pc-pg: Policy cover directed ex-
ploration for provable policy gradient learning. In Advances in Neural Information Processing
Systems, volume 33, pp. 13399â€“13412, 2020a."
REFERENCES,0.1754932502596054,"Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. Flambe: Structural complex-
ity and representation learning of low rank mdps. In Advances in Neural Information Processing
Systems, volume 33, pp. 20095â€“20107, 2020b."
REFERENCES,0.17653167185877466,"Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. Optimality and approxima-
tion with policy gradient methods in markov decision processes. In Proceedings of Thirty Third
Conference on Learning Theory, volume 125 of Proceedings of Machine Learning Research, pp.
64â€“66, 2020c."
REFERENCES,0.17757009345794392,"AndrÂ´as Antos, Csaba SzepesvÂ´ari, and RÂ´emi Munos. Learning near-optimal policies with bellman-
residual minimization based ï¬tted policy iteration and a single sample path. Machine Learning,
71:89â€“129, 2008."
REFERENCES,0.1786085150571132,"Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. Model-based reinforcement
learning with value-targeted regression. In International Conference on Machine Learning, pp.
463â€“474. PMLR, 2020."
REFERENCES,0.17964693665628245,"Jacob Buckman, Carles Gelada, and Marc G Bellemare. The importance of pessimism in ï¬xed-
dataset policy optimization. arXiv preprint arXiv:2009.06799, 2020."
REFERENCES,0.1806853582554517,"Catherine Cang, Aravind Rajeswaran, Pieter Abbeel, and Michael Laskin. Behavioral priors and
dynamics models: Improving performance and domain transfer in ofï¬‚ine rl.
arXiv preprint
arXiv:2106.09119, 2021."
REFERENCES,0.181723779854621,"Jonathan D Chang, Masatoshi Uehara, Dhruv Sreenivas, Rahul Kidambi, and Wen Sun. Mitigating
covariate shift in imitation learning via ofï¬‚ine data without great coverage. 2021."
REFERENCES,0.18276220145379024,"Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning.
In Proceedings of the 36th International Conference on Machine Learning, volume 97, pp. 1042â€“
1051, 2019."
REFERENCES,0.1838006230529595,"Minmin Chen, Alex Beutel, Paul Covington, Sagar Jain, Francois Belletti, and Ed Chi.
Top-k
off-policy correction for a reinforce recommender system. In Proceedings of the Twelfth ACM
International Conference on web search and data mining, WSDM â€™19, pp. 456â€“464, 2019."
REFERENCES,0.18483904465212878,"Marc Deisenroth and Carl E Rasmussen. Pilco: A model-based and data-efï¬cient approach to policy
search. In Proceedings of the 28th International Conference on machine learning (ICML-11), pp.
465â€“472. Citeseer, 2011."
REFERENCES,0.18587746625129803,"Luc Devroye, Abbas Mehrabian, and Tommy Reddad. The total variation distance between high-
dimensional gaussians. arXiv preprint arXiv:1810.08693, 2018."
REFERENCES,0.18691588785046728,"Yaqi Duan, Zeyu Jia, and Mengdi Wang. Minimax-optimal off-policy evaluation with linear func-
tion approximation. In Proceedings of the 37th International Conference on Machine Learning,
volume 119 of Proceedings of Machine Learning Research, pp. 2701â€“2709, 2020."
REFERENCES,0.18795430944963656,"Yaqi Duan, Chi Jin, and Zhiyuan Li. Risk bounds and rademacher complexity in batch reinforcement
learning. arXiv preprint arXiv:2103.13883, 2021."
REFERENCES,0.18899273104880582,"Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning.
Journal of Machine Learning Research, 6:503â€“556, 2005."
REFERENCES,0.19003115264797507,Published as a conference paper at ICLR 2022
REFERENCES,0.19106957424714435,"Rasool Fakoor, Jonas Mueller, Pratik Chaudhari, and Alexander J Smola. Continuous doubly con-
strained batch reinforcement learning. arXiv preprint arXiv:2102.09225, 2021."
REFERENCES,0.1921079958463136,"Jianqing Fan, Zhaoran Wang, Yuchen Xie, and Zhuoran Yang. A theoretical analysis of deep q-
learning. In Proceedings of the 2nd Conference on Learning for Dynamics and Control, volume
120 of Proceedings of Machine Learning Research, pp. 486â€“489, 2020."
REFERENCES,0.19314641744548286,"Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning, pp. 2052â€“2062. PMLR, 2019."
REFERENCES,0.19418483904465214,"Seyed Kamyar Seyed Ghasemipour, Dale Schuurmans, and Shixiang Shane Gu. Emaq: Expected-
max q-learning operator for simple yet effective ofï¬‚ine and online rl. In International Conference
on Machine Learning, pp. 3682â€“3691. PMLR, 2021."
REFERENCES,0.1952232606438214,"Botao Hao, Yaqi Duan, Tor Lattimore, Csaba SzepesvÂ´ari, and Mengdi Wang. Sparse feature selec-
tion makes batch reinforcement learning more sample efï¬cient. In International Conference on
Machine Learning, pp. 4063â€“4073. PMLR, 2021."
REFERENCES,0.19626168224299065,"Garud N Iyengar. Robust dynamic programming. Mathematics of Operations Research, 30(2):
257â€“280, 2005."
REFERENCES,0.19730010384215993,"Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efï¬cient reinforcement learn-
ing with linear function approximation. In Proceedings of Thirty Third Conference on Learning
Theory, volume 125 of Proceedings of Machine Learning Research, pp. 2137â€“2143, 2020a."
REFERENCES,0.19833852544132918,"Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efï¬cient for ofï¬‚ine rl? arXiv
preprint arXiv:2012.15085, 2020b."
REFERENCES,0.19937694704049844,"Sham Kakade, Akshay Krishnamurthy, Kendall Lowrey, Motoya Ohnishi, and Wen Sun. Infor-
mation theoretic regret bounds for online nonlinear control. In Advances in Neural Information
Processing Systems, volume 33, pp. 15312â€“15325, 2020."
REFERENCES,0.20041536863966772,"Michael Kearns and Daphne Koller. Efï¬cient reinforcement learning in factored mdps. In IJCAI,
volume 16, pp. 740â€“747, 1999."
REFERENCES,0.20145379023883697,"Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-
based ofï¬‚ine reinforcement learning. In Advances in Neural Information Processing Systems,
volume 33, pp. 21810â€“21823. Curran Associates, Inc., 2020."
REFERENCES,0.20249221183800623,"Daphne Koller and Ronald Parr. Policy iteration for factored mdps. In Proceedings of the Sixteenth
conference on Uncertainty in artiï¬cial intelligence, pp. 326â€“334, 2000."
REFERENCES,0.2035306334371755,"Michael R. Kosorok and Eric B. Laber. Precision medicine. 6:263â€“286, 2019."
REFERENCES,0.20456905503634476,"Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for ofï¬‚ine
reinforcement learning. arXiv preprint arXiv:2006.04779, 2020."
REFERENCES,0.205607476635514,"Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Ofï¬‚ine reinforcement learning: Tuto-
rial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020."
REFERENCES,0.2066458982346833,"Shiau Hong Lim and Arnaud Autef. Kernel-based reinforcement learning in robust markov decision
processes. In International Conference on Machine Learning, pp. 3973â€“3981. PMLR, 2019."
REFERENCES,0.20768431983385255,"Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Provably good batch off-policy
reinforcement learning without great exploration. In Advances in Neural Information Processing
Systems, volume 33, pp. 1264â€“1274, 2020."
REFERENCES,0.2087227414330218,"Thodoris Lykouris, Max Simchowitz, Alex Slivkins, and Wen Sun. Corruption-robust exploration
in episodic reinforcement learning. In Conference on Learning Theory, pp. 3242â€“3245. PMLR,
2021."
REFERENCES,0.20976116303219106,"Tatsuya Matsushima, Hiroki Furuta, Yutaka Matsuo, Oï¬r Nachum, and Shixiang Gu. Deployment-
efï¬cient reinforcement learning via model-based ofï¬‚ine optimization. ICLR, 2020."
REFERENCES,0.21079958463136034,Published as a conference paper at ICLR 2022
REFERENCES,0.2118380062305296,"Aditya Modi, Nan Jiang, Ambuj Tewari, and Satinder Singh. Sample complexity of reinforcement
learning using linearly combined model ensembles. In International Conference on Artiï¬cial
Intelligence and Statistics, pp. 2010â€“2020. PMLR, 2020."
REFERENCES,0.21287642782969884,"Aditya Modi, Jinglin Chen, Akshay Krishnamurthy, Nan Jiang, and Alekh Agarwal. Model-free
representation learning and exploration in low-rank mdps. arXiv preprint arXiv: 2102.07035,
2021."
REFERENCES,0.21391484942886813,"RÂ´emi Munos. Error bounds for approximate value iteration. In Proceedings of the National Confer-
ence on Artiï¬cial Intelligence, volume 20, pp. 1006. Menlo Park, CA; Cambridge, MA; London;
AAAI Press; MIT Press; 1999, 2005."
REFERENCES,0.21495327102803738,"RÂ´emi Munos and Csaba SzepesvÂ´ari. Finite-time bounds for ï¬tted value iteration. Journal of Machine
Learning Research, 9(May):815â€“857, 2008."
REFERENCES,0.21599169262720663,"Oï¬r Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. Algaedice:
Policy gradient from arbitrary experience. arXiv preprint arXiv:1912.02074, 2019."
REFERENCES,0.21703011422637591,"Gergely Neu and Ciara Pike-Burke. A unifying view of optimism in episodic reinforcement learning.
arXiv preprint arXiv:2007.01891, 2020."
REFERENCES,0.21806853582554517,"Chengzhuo Ni, Anru Zhang, Yaqi Duan, and Mengdi Wang. Learning good state and action repre-
sentations via tensor decomposition. arXiv preprint arXiv:2105.01136, 2021."
REFERENCES,0.21910695742471442,"Arnab Nilim and Laurent El Ghaoui. Robust control of markov decision processes with uncertain
transition matrices. Operations Research, 53(5):780â€“798, 2005."
REFERENCES,0.2201453790238837,"Matteo Papini, Andrea Tirinzoni, Marcello Restelli, Alessandro Lazaric, and Matteo Pirotta. Lever-
aging good representations in linear contextual bandits. arXiv preprint arXiv:2104.03781, 2021."
REFERENCES,0.22118380062305296,"Nived Rajaraman, Lin F Yang, Jiantao Jiao, and Kannan Ramachandran. Toward the fundamental
limits of imitation learning. arXiv preprint arXiv:2009.05990, 2020."
REFERENCES,0.2222222222222222,"Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging ofï¬‚ine rein-
forcement learning and imitation learning: A tale of pessimism. arXiv preprint arXiv:2103.12021,
2021."
REFERENCES,0.2232606438213915,"Shideh Rezaeifar, Robert Dadashi, Nino Vieillard, LÂ´eonard Hussenot, Olivier Bachem, Olivier
Pietquin, and Matthieu Geist. Ofï¬‚ine reinforcement learning as anti-exploration. arXiv preprint
arXiv:2106.06431, 2021."
REFERENCES,0.22429906542056074,"StÂ´ephane Ross and J Andrew Bagnell. Agnostic system identiï¬cation for model-based reinforcement
learning. In Proceedings of the 29th International Coference on International Conference on
Machine Learning, pp. 1905â€“1912, 2012."
REFERENCES,0.22533748701973,"Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Model-based
rl in contextual decision processes: Pac bounds and exponential improvements over model-free
approaches. In Proceedings of the Thirty-Second Conference on Learning Theory, volume 99 of
Proceedings of Machine Learning Research, pp. 2898â€“2933, 2019."
REFERENCES,0.22637590861889928,"Aviv Tamar, Shie Mannor, and Huan Xu. Scaling up robust mdps using function approximation. In
International conference on machine learning, pp. 181â€“189. PMLR, 2014."
REFERENCES,0.22741433021806853,"Ahmed Touati, Amy Zhang, Joelle Pineau, and Pascal Vincent. Stable policy optimization via off-
policy divergence regularization. arXiv preprint arXiv:2003.04108, 2020."
REFERENCES,0.2284527518172378,"Masatoshi Uehara, Jiawei Huang, and Nan Jiang. Minimax weight and q-function learning for off-
policy evaluation. In Proceedings of the 37th International Conference on Machine Learning, pp.
9659â€“9668, 2020."
REFERENCES,0.22949117341640707,"Masatoshi Uehara, Masaaki Imaizumi, Nan Jiang, Nathan Kallus, Wen Sun, and Tengyang Xie.
Finite sample analysis of minimax ofï¬‚ine reinforcement learning: Completeness, fast rates and
ï¬rst-order efï¬ciency. arXiv preprint arXiv:2102.02981, 2021."
REFERENCES,0.23052959501557632,Published as a conference paper at ICLR 2022
REFERENCES,0.23156801661474558,"S van de Geer. Empirical Processes in M-Estimation. Cambridge Series in Statistical and Proba-
bilistic Mathematics. Cambridge University Press, 2000."
REFERENCES,0.23260643821391486,"Martin J Wainwright.
High-Dimensional Statistics : A Non-Asymptotic Viewpoint.
Cambridge
University Press, New York, 2019."
REFERENCES,0.2336448598130841,"Ruosong Wang, Dean P. Foster, and Sham M. Kakade. What are the statistical limits of ofï¬‚ine rl
with linear function approximation?. arXiv preprint arXiv:2010.11895, 2020."
REFERENCES,0.23468328141225336,"Yifan Wu, George Tucker, and Oï¬r Nachum. Behavior regularized ofï¬‚ine reinforcement learning.
arXiv preprint arXiv:1911.11361, 2019."
REFERENCES,0.23572170301142265,"Tengyang Xie and Nan Jiang. Batch value-function approximation with only realizability. arXiv
preprint arXiv:2008.04990, 2020."
REFERENCES,0.2367601246105919,"Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent
pessimism for ofï¬‚ine reinforcement learning. arXiv preprint arXiv:2106.06926, 2021."
REFERENCES,0.23779854620976115,"Lin Yang and Mengdi Wang. Reinforcement learning in feature space: Matrix bandit, kernels, and
regret bound. In Proceedings of the 37th International Conference on Machine Learning, pp.
10746â€“10756, 2020."
REFERENCES,0.23883696780893043,"Ming Yin, Yu Bai, and Yu-Xiang Wang. Near-optimal ofï¬‚ine reinforcement learning via double
variance reduction. arXiv preprint arXiv:2102.01748, 2021."
REFERENCES,0.2398753894080997,"Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea
Finn, and Tengyu Ma. Mopo: Model-based ofï¬‚ine policy optimization. In Advances in Neural
Information Processing Systems, volume 33, pp. 14129â€“14142, 2020."
REFERENCES,0.24091381100726894,"Andrea Zanette, Martin J Wainwright, and Emma Brunskill. Provable beneï¬ts of actor-critic meth-
ods for ofï¬‚ine reinforcement learning. arXiv preprint arXiv:2108.08812, 2021."
REFERENCES,0.24195223260643822,"Ruiyi Zhang, Bo Dai, Lihong Li, and Dale Schuurmans. Gendice: Generalized ofï¬‚ine estimation of
stationary values. In International Conference on Learning Representations, 2020."
REFERENCES,0.24299065420560748,"Weitong Zhang, Jiafan He, Dongruo Zhou, Amy Zhang, and Quanquan Gu.
Provably efï¬cient
representation learning in low-rank markov decision processes. arXiv preprint arXiv:2106.11935,
2021a."
REFERENCES,0.24402907580477673,"Xuezhou Zhang, Yiding Chen, Jerry Zhu, and Wen Sun. Corruption-robust ofï¬‚ine reinforcement
learning. arXiv preprint arXiv:2106.06630, 2021b."
REFERENCES,0.245067497403946,"Dongruo Zhou, Quanquan Gu, and Csaba Szepesvari. Nearly minimax optimal reinforcement learn-
ing for linear mixture markov decision processes. In Conference on Learning Theory, pp. 4532â€“
4576. PMLR, 2021."
REFERENCES,0.24610591900311526,Published as a conference paper at ICLR 2022
REFERENCES,0.24714434060228452,"A
GENERALIZATION OF THEOREM 1"
REFERENCES,0.2481827622014538,"We present the generalized version of Theorem 1 when the hypothesis class is inï¬nite. We deï¬ne
the modiï¬ed function class of M: H = (r"
REFERENCES,0.24922118380062305,P + P âˆ—
REFERENCES,0.2502596053997923,"2
| P âˆˆM ) ."
REFERENCES,0.2512980269989616,"Given a function class F, let N[](Î´, F, d) be the bracketing number of F w.r.t the metric d(a, b)
given by"
REFERENCES,0.2523364485981308,"d(a, b) = E(s,a)âˆ¼Ï"
REFERENCES,0.2533748701973001,"Z
(a(sâ€² | s, a) âˆ’b(sâ€² | s, a))2d(sâ€²)
1/2
."
REFERENCES,0.2544132917964694,"Then, the entropy integral of F is given by"
REFERENCES,0.2554517133956386,"JB(Î´, F, d) = max Z Î´"
REFERENCES,0.2564901349948079,"Î´2/2
(log N[](u, F, d))1/2du, Î´ ! .
(4)"
REFERENCES,0.25752855659397716,We also deï¬ne the localized class of H:
REFERENCES,0.2585669781931464,"H(Î´) = {h âˆˆH : E(s,a)âˆ¼Ï[h2(P(Â· | s, a)âˆ¥P â‹†(Â· | s, a))] â‰¤Î´2},"
REFERENCES,0.25960539979231567,"where h(P(Â· | s, a)âˆ¥P â‹†(Â· | s, a)) denotes Hellinger distance deï¬ned by

0.5
Z
{
p"
REFERENCES,0.26064382139148495,"P(sâ€² | s, a) âˆ’
p"
REFERENCES,0.2616822429906542,"P â‹†(sâ€² | s, a)}2d(sâ€²)
1/2
."
REFERENCES,0.26272066458982346,"Based on Theorem 7.4 (van de Geer, 2000), the MLE has the following guarantee."
REFERENCES,0.26375908618899274,"Theorem 4 (MLE guarantee with general function approximation). We take a function G(Ïµ) :
[0, 1] â†’R s.t. G(Ïµ) â‰¥JB[Ïµ, H(Ïµ), d] and G(Ïµ)/Ïµ2 is a non-increasing function w.r.t Ïµ. Then,
letting Î¾n be a solution to âˆšnÏµ2 â‰¥cG(Ïµ) w.r.t Ïµ. With probability 1 âˆ’Î´, we have"
REFERENCES,0.26479750778816197,"E(s,a)âˆ¼Ï[âˆ¥Ë†PMLE(Â· | s, a) âˆ’P(Â· | s, a)âˆ¥2
1] â‰¤c1
n
Î¾n +
p"
REFERENCES,0.26583592938733125,"log(c2/Î´)/n
o2
."
REFERENCES,0.26687435098650053,"We remark that the original guarantee in (van de Geer, 2000) is given for the estimation of uncondi-
tional distributions. The adaption to the conditional case is straightforward. For more details, refer
to Section A.1. Besides, when we assume the convexity of the function class, the entropy integral
with bracketing (4) can be replaced with the entropy integral with covering number (Wainwright,
2019, Chapter 14)."
REFERENCES,0.26791277258566976,"By using the above notation and the MLE guarantee, we can generalize Theorem 1."
REFERENCES,0.26895119418483904,"Theorem 5 (Finite sample error bound of CPPO with an inï¬nite hypothesis class). Assume P â‹†âˆˆ
M. Let f(P)(s, a) = TV(P(Â· | s, a), P â‹†(Â· | s, a))2. Deï¬ne"
REFERENCES,0.2699896157840083,"M1 =

P : E(s,a)âˆ¼Ï [f(P)(s, a)] â‰¤c

Î¾2
n + ln(c/Î´) n 
,"
REFERENCES,0.27102803738317754,"M2 =

P : E(s,a)âˆ¼D[f(P)(s, a)] â‰¤c

G(M1) + Î¾2
n + ln(c/Î´) n 
,"
REFERENCES,0.2720664589823468,"G(M1) = E[ sup
P âˆˆM1
|(ED âˆ’EÏ)[f(P)]|], G(M2) = E[ sup
P âˆˆMD
|(ED âˆ’EÏ)[f(P)]|]."
REFERENCES,0.2731048805815161,"Here, in G(M1) and G(M2), the expectation is taken over the data. We set Î¾ = cG(M1) + cÎ¾2
n +"
REFERENCES,0.27414330218068533,"c

ln(c/Î´)"
REFERENCES,0.2751817237798546,"n

. Then, for all Ï€âˆ—âˆˆÎ , we have"
REFERENCES,0.2762201453790239,"V Ï€âˆ—
P â‹†âˆ’V Ë†Ï€
P â‹†â‰¤(1 âˆ’Î³)âˆ’2c1
q Câ€ 
Ï€âˆ— r"
REFERENCES,0.2772585669781931,"G(M2) + G(M1) + Î¾2n + ln(c/Î´) n
."
REFERENCES,0.2782969885773624,Published as a conference paper at ICLR 2022
REFERENCES,0.2793354101765317,"This theorem shows once we can calculate GM1, GM2 and Î¾n, we can obtain the tight rate. Im-
portantly, GM1 and GM2 are upper-bounded by the localized versions of Rademacher complexities
based on symmetrization argument. Hence, their rates are faster than the ones of the nonlocalized
versions."
REFERENCES,0.2803738317757009,"For example, when |M| is ï¬nite, we ï¬rst have Î¾n =
p"
REFERENCES,0.2814122533748702,"ln(|M|c/Î´)/n. Then, from Bernsteinâ€™s
inequality (Wainwright, 2019, Exercise 2.8) and union bound, G(M1) is upper-bounded by"
REFERENCES,0.2824506749740395,"G(M1) â‰²
Î¾n
|{z}
Variance term
Ã—
p"
REFERENCES,0.2834890965732087,"ln(|M|c/Î´)/n
|
{z
}
Union bound"
REFERENCES,0.284527518172378,"â‰²Î¾2
n."
REFERENCES,0.28556593977154726,"Similarly, from empirical Bernsteinâ€™s inequality,"
REFERENCES,0.2866043613707165,"G(M2) â‰²
Î¾n
|{z}
Variance term
Ã—
p"
REFERENCES,0.28764278296988577,"ln(|M|c/Î´)/n
|
{z
}
Union bound"
REFERENCES,0.28868120456905505,"â‰²Î¾2
n."
REFERENCES,0.2897196261682243,"Then, we can obtain the result of Theorem 1:"
REFERENCES,0.29075804776739356,"V Ï€âˆ—
P â‹†âˆ’V Ë†Ï€
P â‹†â‰¤(1 âˆ’Î³)âˆ’2c1
q Câ€ 
Ï€âˆ— r"
REFERENCES,0.29179646936656284,ln(|M|c/Î´)
REFERENCES,0.29283489096573206,"n
,
âˆ€Ï€âˆ—âˆˆÎ ."
REFERENCES,0.29387331256490135,"We stress if we use Hoeffedingâ€™s inequality above, we would immediately get the slower rate
O(nâˆ’1/4). To calculate G(M1) and G(M2) in a tight manner, we need to leverage the knowl-
edge that the variance of each element in M1 and M2 is controlled from the restriction P âˆˆM1 or
P âˆˆM2."
REFERENCES,0.2949117341640706,"A.1
RATE OF CONVERGENCE OF MAXIMUM LIKELIHOOD ESTIMATION WITH INFINITE
HYPOTHESIS CLASS"
REFERENCES,0.29595015576323985,"We aim for obtaining a PAC guarantee of MLE following (van de Geer, 2000). We explain how we
should modify the proof of van de Geer (2000) for unconditional density estimation to conditional
density estimation. For simplicity, we assume P â‹†> 0."
REFERENCES,0.29698857736240913,We ï¬rst introduce the notation:
REFERENCES,0.2980269989615784,"Â¯P = (P + P â‹†)/2, gP = 0.5 log
Â¯P
P â‹†, ( Â¯
M)1/2 =
np"
REFERENCES,0.29906542056074764,"Â¯P | P âˆˆM
o
."
REFERENCES,0.3001038421599169,Recall
REFERENCES,0.3011422637590862,"h2(P1(Â· | s, a), P2(Â· | s, a)) =

0.5
Z
P 1/2
1
(sâ€² | s, a) âˆ’P 1/2(sâ€² | s, a)d(sâ€²)
0.5
."
REFERENCES,0.30218068535825543,"Here, from Lemma 4.2 (van de Geer, 2000), the following holds:
Lemma 1 (Some property of Hellinger distance)."
REFERENCES,0.3032191069574247,"E(s,a)âˆ¼Ï[h2( Â¯P1(Â· | s, a), Â¯P2(Â· | s, a))] â‰¤0.5E(s,a)âˆ¼Ï[h2(P1(Â· | s, a), P2(Â· | s, a))],"
REFERENCES,0.304257528556594,"E(s,a)âˆ¼Ï[h2(P(Â· | s, a), P â‹†(Â· | s, a))] â‰¤E(s,a)âˆ¼Ï[16h2( Â¯P(Â· | s, a), P â‹†(Â· | s, a))]."
REFERENCES,0.3052959501557632,"We also recall Hellinger distance is stronger than TV distance:
Lemma 2 (Relation of Hellinger distance and TV distance )."
REFERENCES,0.3063343717549325,"TV(P1(Â· | s, a), P2(Â· | s, a)) â‰¤
âˆš"
REFERENCES,0.3073727933541018,"2h(P1(Â· | s, a), P2(Â· | s, a))."
REFERENCES,0.308411214953271,"The following lemma is useful to connect the log-loss and the Hellinger distance.
Lemma 3 (Basic Inequality for MLE)."
REFERENCES,0.3094496365524403,"E(s,a)âˆ¼Ï[h2( Ë†PMLE(Â· | s, a), P â‹†(Â· | s, a))] â‰¤(ED âˆ’E(s,a)âˆ¼Ï,sâ€²âˆ¼P â‹†(Â·|s,a))[gP (s, a, sâ€²)]."
REFERENCES,0.31048805815160957,"This is proved by Lemma 4.1 (van de Geer, 2000). To simplify the notation, we deï¬ne"
REFERENCES,0.3115264797507788,"H2(P1, P2) = E(s,a)âˆ¼Ï[h2(P1(Â· | s, a), P2(Â· | s, a))]."
REFERENCES,0.3125649013499481,Published as a conference paper at ICLR 2022
REFERENCES,0.31360332294911736,"Here, our goal is showing with probability 1 âˆ’Î´,"
REFERENCES,0.3146417445482866,"H2( Ë†PMLE, P â‹†) â‰¤{Î¾n +
p"
REFERENCES,0.31568016614745587,log(c2/Î´)/n}2.
REFERENCES,0.31671858774662515,"This is proved by showing for x â‰¥Î¾n,"
REFERENCES,0.3177570093457944,"P(H2( Ë†PMLE, P â‹†) â‰¥x2) â‰¤c exp(âˆ’nx2/c2)."
REFERENCES,0.31879543094496365,"This corresponds to the statement in Theorem 7.4 (van de Geer, 2000). To prove the above, we ï¬rst
use"
REFERENCES,0.31983385254413293,"P(H2( Ë†PMLE, P â‹†) â‰¥x2) â‰¤P(16H2( Â¯Ë†PMLE, P â‹†) â‰¥x2),"
REFERENCES,0.32087227414330216,"from Lemma 1. Then, from Lemma 3, this is upper-bounded by"
REFERENCES,0.32191069574247144,"P(
sup
P âˆˆM,H2( Â¯
P ,P â‹†)â‰¥x2/16
Î½n(gP ) âˆ’âˆšnH2(P, P â‹†) â‰¥0)
(5)"
REFERENCES,0.3229491173416407,"where Î½n = âˆšn(ED âˆ’E(s,a)âˆ¼Ï,sâ€²âˆ¼P â‹†(Â·|s,a)). To prove the term 5 is less than c exp(âˆ’nÎ´2/c2),
we use Theorem 5.11 (van de Geer, 2000), that is, some uniform inequality based on entropy with
bracketing. The rest of the proof is the same as Theorem 7.4 (van de Geer, 2000). In summary, the
only difference is we use the distance H(a, b) tailored to the conditional density estimation instead
of unconditional density estimation."
REFERENCES,0.32398753894080995,"B
MORE DETAILS FOR KNRS"
REFERENCES,0.32502596053997923,"We explain the algorithm and present the PAC guaranteed for KNRs. Here, we denote the dimension
of S by dS."
REFERENCES,0.3260643821391485,"We tailor Algorithm 1 to KNRs as follows to obtain a tighter guarantee. First, MLE procedure is
replaced with Ë†WMLE by regularized MLE:"
REFERENCES,0.32710280373831774,"Ë†WMLE = arg min
W âˆˆRdS Ã—d ED[âˆ¥WÏ†(s, a) âˆ’sâ€²âˆ¥2
2] + Î»âˆ¥Wâˆ¥2
F ,"
REFERENCES,0.328141225337487,"where âˆ¥Â· âˆ¥F is a Frobenius norm. Then, the ï¬nal policy optimization procedure is"
REFERENCES,0.3291796469366563,"Ë†Ï€ = arg maxÏ€âˆˆÎ  minW âˆˆWD V Ï€
P (W ), s.t., WD = {W âˆˆRdSÃ—d : âˆ¥( Ë†WMLE âˆ’W)(Î£n)1/2âˆ¥2 â‰¤Î¾}"
REFERENCES,0.3302180685358255,"where Î£n = Pn
i=1 Ï†(si, ai)Ï†âŠ¤(si, ai). We state the theoretical guarantee for KNRs below."
REFERENCES,0.3312564901349948,"Corollary 3 (PAC bound for KNRs). Assume âˆ¥Ï†(s, a)âˆ¥2 â‰¤1, âˆ€(s, a) âˆˆS Ã— A. We set Î¾ =
q"
REFERENCES,0.3322949117341641,"2Î»âˆ¥W â‹†âˆ¥2
2 + 8Î¶2  
dS ln(5) + ln(1/Î´) + Â¯In

,
Â¯In = ln (det(Î£n)/ det(Î»I)) ."
REFERENCES,0.3333333333333333,"Suppose the KNR model is well-speciï¬ed. By letting âˆ¥W â‹†âˆ¥2
2 = O(1), Î¶2 = O(1), Î» = O(1), with
probability 1 âˆ’Î´, for all Ï€âˆ—, we have"
REFERENCES,0.3343717549325026,"V Ï€âˆ—
P â‹†âˆ’V Ë†Ï€
P â‹†â‰¤c1H2 min(d1/2, Â¯R)
p Â¯R r"
REFERENCES,0.3354101765316719,"dS Â¯CÏ€âˆ—,P â‹†ln(1 + n)"
REFERENCES,0.3364485981308411,"n
,
where Â¯R := rank[Î£Ï]{rank[Î£Ï] + ln(c2/Î´)}."
REFERENCES,0.3374870197300104,The proof is deferred to Section E.5.
REFERENCES,0.33852544132917967,"C
MORE RELATED WORKS"
REFERENCES,0.3395638629283489,We discuss literature related to representation learning in RL.
REFERENCES,0.3406022845275182,"Representation learning for low-rank MDPs (ground truth feature representation is unknown) in
online learning is studied from a model-based perspective (Agarwal et al., 2020b) and model-free
perspective (Modi et al., 2021). In the online setting, Zhang et al. (2021a); Papini et al. (2021)
also study representation learning under different model assumptions. Comparing with these works,
since our setting is ofï¬‚ine, the algorithm and analysis are totally different."
REFERENCES,0.34164070612668745,Published as a conference paper at ICLR 2022
REFERENCES,0.3426791277258567,"In the ofï¬‚ine setting, Ni et al. (2021) study dimensionality reduction in a given kernel space, and Hao
et al. (2021) study feature selection in sparse linear MDPs. Their focus is different as they do not
study PAC guarantees under partial coverage. Ni et al. (2021) assumes the transition operator can be
properly embedded into predeï¬ned Reproducing Kernel Hilbert Spaces and learns low-dimensional
state-action representations via kernelized embedding and low-rank tensor decomposition. However,
they did not study the errors for policy optimization after using these learned features. Regarding
ofï¬‚ine distribution coverage, Ni et al. (2021) assumes that the feature covariance matrix (feature
associated with the pre-deï¬ned kernel) of the ofï¬‚ine distribution is full rank. Hao et al. (2021) studies
an OPE problem on sparse linear Bellman complete MDPs in the ofï¬‚ine learning setting where
they assume all covariance matrices (covariance matrices that correspond to all possible subsets of
features) under the ofï¬‚ine distribution are full rank as well. We study policy optimization in low-
rank MDPs (with unknown feature representation), and we do not assume full coverage, i.e., we do
not assume the feature covariance matrix is full rank, and indeed our result is distribution-dependent
since it scales with respect to the rank of the covariance matrix that is deï¬ned using the ground truth
feature representation."
REFERENCES,0.34371754932502596,"D
COMPARISON TO XIE ET AL. (2021)"
REFERENCES,0.34475597092419524,"We compare a result in (Xie et al., 2021) to our result in detail. Let F be a function class for Q-
functions. Here, we consider a more general version of their algorithm by replacing the original
E(f, Ï€; D) in their algorithm with"
REFERENCES,0.34579439252336447,"E(f, Ï€; D) := L(f, f; Ï€, D) âˆ’min
gâˆˆG L(g, f; Ï€, D)."
REFERENCES,0.34683281412253375,"In their original algorithm, they set G = F. Here, we consider the version such that a discriminator
class G can be different from F."
REFERENCES,0.34787123572170303,"They show the PAC result under partial coverage as follows. Here, T Ï€
P â‹†is a Bellman operator under
Ï€ and P â‹†:"
REFERENCES,0.34890965732087226,"T Ï€
P â‹†: {S Ã— A â†’R} âˆ‹f 7â†’r(s, a) + EP âˆ—(sâ€²|s,a)[f(sâ€², Ï€)] âˆˆ{S Ã— A â†’R}."
REFERENCES,0.34994807892004154,"Theorem 6 (Extension of Result in (Xie et al., 2021) ). Suppose realizaibility QÏ€
P â‹†âˆˆF, âˆ€Ï€ âˆˆÎ 
and closeness maxfâˆˆF mingâˆˆG Es,aâˆ¼Ï[(g âˆ’T Ï€
P â‹†f)2(s, a)] = 0, âˆ€Ï€ âˆˆÎ . Then, with 1 âˆ’Î´, for any
Ï€âˆ—âˆˆÎ , we have"
REFERENCES,0.3509865005192108,"V Ï€âˆ—
P â‹†âˆ’V Ë†Ï€
P â‹†= O(
p"
REFERENCES,0.35202492211838005,"Câ‹„ln(|Î ||F||G|/Î´)/n),
Câ‹„= sup
fâˆˆF"
REFERENCES,0.3530633437175493,"E(s,a)âˆ¼dÏ€âˆ—
P â‹†[(f âˆ’T f)2(s, a)]"
REFERENCES,0.3541017653167186,"E(s,a)âˆ¼Ï[(f âˆ’T f)2(s, a)] ."
REFERENCES,0.35514018691588783,"By combining this result with the conversion from model-free results to model-based results in
(Chen & Jiang, 2019, Corollary 6), we can obtain the following result under partial coverage."
REFERENCES,0.3561786085150571,"Theorem 7 (PAC guarantee from the direct application of (Xie et al., 2021) to mode-based RL ).
Assume P â‹†âˆˆM. Then, there exists an algorithm s.t. with 1 âˆ’Î´, for any policy Ï€â‹†âˆˆÎ ,"
REFERENCES,0.3572170301142264,"V Ï€âˆ—
P â‹†âˆ’V Ë†Ï€
P â‹†= O(
p"
REFERENCES,0.3582554517133956,Câ‹„ln(|Î ||M|/Î´)/n).
REFERENCES,0.3592938733125649,"As we mentioned, this is worse than our result since it includes |Î |. Besides, the algorithm can
only compete against policies restricted in Î , while our algorithm works for the unrestricted policy
class Î  which could even include history dependent policies. For completeness, we give the proof
as follows."
REFERENCES,0.3603322949117342,"We remark that their results (Theorem 4.1) with NPG that can possibly compete with any stochastic
policies, are not applicable here. This is because they need an assumption that the comparator policy
Ï€âˆ—needs to satisfy QÏ€âˆ—
P â‹†âˆˆF and maxfâˆˆF mingâˆˆG Es,aâˆ¼Ï[(g âˆ’T Ï€âˆ—
P â‹†f)2(s, a)] = 0, which does not
hold for the corresponding Q-function class F after the conversion. As a notable exception, when
the model is a linear Bellman-complete MDP (Zanette et al., 2021), any stochastic policies satisfy
the Bellman completeness for the linear Q-function class; then, their algorithms can learn policies
that can compete with any stochastic policies satisfying partial coverage."
REFERENCES,0.3613707165109034,Published as a conference paper at ICLR 2022
REFERENCES,0.3624091381100727,"Proof of Theorem 7. Given a model class M, consider the following reduction. We deï¬ne a Q-
function class:
F = {qÏ€
P | Ï€ âˆˆÎ , P âˆˆM}.
Then, we deï¬ne a discriminator class G:"
REFERENCES,0.363447559709242,"G = {T Ï€â€²
P â€² qÏ€
P | Ï€ âˆˆÎ , Ï€â€² âˆˆÎ , P âˆˆM, P â€² âˆˆM}."
REFERENCES,0.3644859813084112,"The above satisï¬es the realizability QÏ€
P â‹†âˆˆF, âˆ€Ï€ âˆˆÎ  and the closedness T Ï€
P â‹†F âŠ‚G, âˆ€Ï€ âˆˆÎ .
Thus, the assumptions in Theorem 6 are satisï¬ed. Then, we have"
REFERENCES,0.3655244029075805,"V Ï€âˆ—
P â‹†âˆ’V Ë†Ï€
P â‹†= O(
p"
REFERENCES,0.36656282450674976,Câ‹„ln(|Î ||F||G|/Î´)/n)
REFERENCES,0.367601246105919,"= O(
p"
REFERENCES,0.36863966770508827,"Câ‹„ln(|Î ||M|/Î´)/n),"
REFERENCES,0.36967808930425755,noting |F| = |Î ||M| and |G| = |Î |2|M|2.
REFERENCES,0.3707165109034268,"E
MISSING PROOFS"
REFERENCES,0.37175493250259606,"Below we use c, c1, c2, Â· Â· Â· to denote universal constants. For a d-dimensional vector a and a matrix
A âˆˆRdÃ—d, we denote âˆ¥aâˆ¥2
A = aâŠ¤Aa. Here, a â‰²B means a â‰¤cB for some universal constant. c"
REFERENCES,0.37279335410176534,"E.1
PROOFS FOR GENERAL FUNCTION APPROXIMATION"
REFERENCES,0.37383177570093457,"Proof of Theorem 1. From Lemma 6, the MLE guarantee gives us the following generalization
bound: with probability 1 âˆ’Î´,"
REFERENCES,0.37487019730010385,"Es,aâˆ¼Ï[TV( bPMLE(Â· | s, a), P â‹†(Â· | s, a))2] â‰²ln(|M|/Î´)"
REFERENCES,0.37590861889927313,"n
.
(6)"
REFERENCES,0.37694704049844235,"Letting
A(P) := |Es,aâˆ¼Ï[TV(P(Â· | s, a), P â‹†(Â· | s, a))2] âˆ’ED[TV(P(Â· | s, a), P â‹†(Â· | s, a))2]|.
with probability 1 âˆ’Î´, from union bound and Bernsteinâ€™s inequality, we also have"
REFERENCES,0.37798546209761164,A(P) â‰¤ r
REFERENCES,0.3790238836967809,"c1var(s,a)âˆ¼Ï[TV(P(Â· | s, a), P â‹†(Â· | s, a))2] ln(|M|/Î´)"
REFERENCES,0.38006230529595014,"n
+ c2 ln(|M|/Î´)"
REFERENCES,0.3811007268951194,"n
, âˆ€P âˆˆM."
REFERENCES,0.3821391484942887,"(7)
Hereafter, we condition on the above two events. Recall that we construct the version space using
D and bPMLE as follows:"
REFERENCES,0.38317757009345793,"MD :=
n
P âˆˆM : ED[TV(P(Â· | s, a), Ë†PMLE(Â· | s, a))2] â‰¤Î¾
o
."
REFERENCES,0.3842159916926272,"First Step: Show P â‹†âˆˆMD in high-probability.
We set Î¾ = c ln(|M|/Î´)"
REFERENCES,0.3852544132917965,"n
. Conditioning on the
above two events equations (6) and (7), we prove P â‹†âˆˆMD. This is proved by"
REFERENCES,0.3862928348909657,"ED[TV( bPMLE(Â· | s, a), P â‹†(Â· | s, a))2]"
REFERENCES,0.387331256490135,"= ED[TV( bPMLE(Â· | s, a), P â‹†(Â· | s, a))2] âˆ’E(s,a)âˆ¼Ï[TV( bPMLE(Â· | s, a), P â‹†(Â· | s, a))2]"
REFERENCES,0.3883696780893043,"+ E(s,a)âˆ¼Ï[TV( bPMLE(Â· | s, a), P â‹†(Â· | s, a))2]"
REFERENCES,0.3894080996884735,"= ED[TV( bPMLE(Â· | s, a), P â‹†(Â· | s, a))2] âˆ’E(s,a)âˆ¼Ï[TV( bPMLE(Â· | s, a), P â‹†(Â· | s, a))2] + c1 ln(|M|/Î´) n â‰² s"
REFERENCES,0.3904465212876428,"var(s,a)âˆ¼Ï[TV( bPMLE(Â· | s, a), P â‹†(Â· | s, a))2] ln(|M|/Î´)"
REFERENCES,0.39148494288681207,"n
+ ln(|M|/Î´)"
REFERENCES,0.3925233644859813,"n
(From (7)) â‰² s"
REFERENCES,0.3935617860851506,"E(s,a)âˆ¼Ï[TV( bPMLE(Â· | s, a), P â‹†(Â· | s, a))2] ln(|M|/Î´)"
REFERENCES,0.39460020768431986,"n
+ ln(|M|/Î´)"
REFERENCES,0.3956386292834891,"n
(TV( bPMLE(Â· | s, a), P â‹†(Â· | s, a))2 â‰¤4) â‰²1"
REFERENCES,0.39667705088265837,"n ln(|M|/Î´).
(Plug in MLE guarantee)"
REFERENCES,0.39771547248182765,Published as a conference paper at ICLR 2022
REFERENCES,0.3987538940809969,"Second Step: Show Es,aâˆ¼Ï[TV(P(Â· | s, a), P â‹†(Â· | s, a))2] â‰¤cÎ¾,
âˆ€P âˆˆMD in high probabil-
ity.
We show for any P âˆˆMD, the distance between P â‹†is sufï¬ciently controlled in terms of TV
distance. More concretely (conditioning on the above two events (6) and (7) ), we show"
REFERENCES,0.39979231568016615,"Es,aâˆ¼Ï[TV(P(Â· | s, a), P â‹†(Â· | s, a))2] â‰²Î¾,
âˆ€P âˆˆMD."
REFERENCES,0.40083073727933544,"In order to observe this, for any P âˆˆMD, we have"
REFERENCES,0.40186915887850466,"ED[TV(P(Â· | s, a), P â‹†(Â· | s, a))2]"
REFERENCES,0.40290758047767394,"â‰¤2ED[TV( bPMLE(Â· | s, a), P(Â· | s, a))2] + 2ED[TV( bPMLE(Â· | s, a), P â‹†(Â· | s, a))2] â‰¤4Î¾
(From (a + b)2 â‰¤2a2 + 2b2.)"
REFERENCES,0.4039460020768432,"Thus, we have:"
REFERENCES,0.40498442367601245,"Es,aâˆ¼Ï[TV(P(Â· | s, a), P â‹†(Â· | s, a))2]"
REFERENCES,0.40602284527518173,"= Es,aâˆ¼Ï[TV(P(Â· | s, a), P â‹†(Â· | s, a))2] âˆ’ED[TV(P(Â· | s, a), P â‹†(Â· | s, a))2] + ED[TV(P(Â· | s, a), P â‹†(Â· | s, a))2]
â‰¤A(P) + cÎ¾.
(8)"
REFERENCES,0.407061266874351,"Here, from (7), we have"
REFERENCES,0.40809968847352024,A(P) â‰¤ r
REFERENCES,0.4091381100726895,"c1var(s,a)âˆ¼Ï[TV(P(Â· | s, a), P â‹†(Â· | s, a))2]] ln(|M|/Î´)"
REFERENCES,0.4101765316718588,"n
+ c2 ln(|M|/Î´)"
REFERENCES,0.411214953271028,"n
, âˆ€P âˆˆMD."
REFERENCES,0.4122533748701973,"Then, for any P âˆˆMD, we have"
REFERENCES,0.4132917964693666,A(P) â‰¤ r
REFERENCES,0.4143302180685358,"c1E(s,a)âˆ¼Ï[TV(P(Â· | s, a), P â‹†(Â· | s, a))4] ln(|M|/Î´)"
REFERENCES,0.4153686396677051,"n
+ c2 ln(|M|/Î´) n â‰¤ r"
REFERENCES,0.4164070612668744,"4c1E(s,a)âˆ¼Ï[TV(P(Â· | s, a), P â‹†(Â· | s, a))2] ln(|M|/Î´)"
REFERENCES,0.4174454828660436,"n
+ c2 ln(|M|/Î´)"
REFERENCES,0.4184839044652129,"n
([TV(P(Â· | s, a), P â‹†(Â· | s, a))2] â‰¤4.) â‰¤ r"
REFERENCES,0.4195223260643821,4c1(A(P) + cÎ¾) ln(|M|/Î´)
REFERENCES,0.4205607476635514,"n
+ c2 ln(|M|/Î´) n
."
REFERENCES,0.4215991692627207,"From (a + b)2 â‰¤2a2 + 2b2,"
REFERENCES,0.4226375908618899,A2(P) â‰² r
REFERENCES,0.4236760124610592,c(A(P) + Î¾) ln(|M|/Î´)
REFERENCES,0.42471443406022846,"n
+ c ln(|M|/Î´) n !2"
REFERENCES,0.4257528556593977,â‰²(A(P) + Î¾) ln(|M|/Î´)
REFERENCES,0.42679127725856697,"n
+
c ln(|M|/Î´) n 2"
REFERENCES,0.42782969885773625,â‰²(A(P) + Î¾) ln(|M|/Î´)
REFERENCES,0.4288681204569055,"n
(Î¾ includes ln(|M|/Î´))"
REFERENCES,0.42990654205607476,"â‰²(A(P) + 1/n ln(|M|/Î´)) ln(|M|/Î´) n
."
REFERENCES,0.43094496365524404,"Then, we have"
REFERENCES,0.43198338525441327,"A2(P) âˆ’B1A(P) âˆ’B2 â‰¤0,
B1 = c ln(|M|/Î´)/n,
B2 = c(1/n)2 ln(|M|/Î´)2."
REFERENCES,0.43302180685358255,This concludes
REFERENCES,0.43406022845275183,"0 â‰¤A(P) â‰¤B1 +
p"
REFERENCES,0.43509865005192105,"B2
1 + 4B2
2
â‰¤c(B1 +
p"
REFERENCES,0.43613707165109034,"B2) â‰¤cln(|M|/Î´) n
â‰²Î¾."
REFERENCES,0.4371754932502596,"Thus, by using the above A(P) â‰²Î¾(P âˆˆMD) and (8), with probability 1 âˆ’Î´, we have:"
REFERENCES,0.43821391484942884,"Es,aâˆ¼Ï[TV(P(Â· | s, a), P â‹†(Â· | s, a))2] â‰¤A(P) + cÎ¾ â‰²Î¾,
P âˆˆMD."
REFERENCES,0.4392523364485981,"Third Step: Calculate the ï¬nal error bound taking the distribution shift into account
For
any P âˆˆMD, we prove"
REFERENCES,0.4402907580477674,"V Ï€âˆ—
P â‹†âˆ’V Ï€âˆ—
P
â‰¤(1 âˆ’Î³)âˆ’2c
q Câ€ 
Ï€âˆ— r"
REFERENCES,0.44132917964693663,ln(|M|/Î´)
REFERENCES,0.4423676012461059,"n
.
(9)"
REFERENCES,0.4434060228452752,Published as a conference paper at ICLR 2022
REFERENCES,0.4444444444444444,"For any P âˆˆMD, this is proved as follows:"
REFERENCES,0.4454828660436137,"V Ï€âˆ—
P â‹†âˆ’V Ï€âˆ—
P
â‰¤(1 âˆ’Î³)âˆ’2E(s,a)âˆ¼dÏ€âˆ—
P â‹†[TV(P(Â· | s, a), P â‹†(Â· | s, a))]"
REFERENCES,0.446521287642783,"(Simulation lemma, Lemma 5)"
REFERENCES,0.4475597092419522,â‰¤(1 âˆ’Î³)âˆ’2q
REFERENCES,0.4485981308411215,"E(s,a)âˆ¼dÏ€âˆ—
P â‹†[TV(P(Â· | s, a), P â‹†(Â· | s, a))2]"
REFERENCES,0.44963655244029077,"â‰¤(1 âˆ’Î³)âˆ’2
q"
REFERENCES,0.45067497403946,"Câ€ 
Ï€âˆ—E(s,a)âˆ¼Ï[TV(P(Â· | s, a), P â‹†(Â· | s, a))2]"
REFERENCES,0.4517133956386293,"â‰¤c(1 âˆ’Î³)âˆ’2
q Câ€ 
Ï€âˆ— r"
REFERENCES,0.45275181723779856,ln(|M|/Î´)
REFERENCES,0.4537902388369678,"n
.
(Based on the consequence of the second step)"
REFERENCES,0.45482866043613707,"Combining all things together, with probability 1 âˆ’Î´, for any Ï€âˆ—âˆˆÎ , we have"
REFERENCES,0.45586708203530635,"V Ï€âˆ—
P â‹†âˆ’V Ë†Ï€
P â‹†â‰¤V Ï€âˆ—
P â‹†âˆ’min
P âˆˆMD V Ï€âˆ—
P
+ min
P âˆˆMD V Ï€âˆ—
P
âˆ’V Ë†Ï€
P â‹†"
REFERENCES,0.4569055036344756,"â‰¤V Ï€âˆ—
P â‹†âˆ’min
P âˆˆMD V Ï€âˆ—
P
+ min
P âˆˆMD V Ë†Ï€
P âˆ’V Ë†Ï€
P â‹†
(deï¬nition of Ë†Ï€)"
REFERENCES,0.45794392523364486,"â‰¤V Ï€âˆ—
P â‹†âˆ’min
P âˆˆMD V Ï€âˆ—
P
(Fist step, P â‹†âˆˆMD)"
REFERENCES,0.45898234683281414,"â‰²(1 âˆ’Î³)âˆ’2c1
q Câ€ 
Ï€âˆ— r"
REFERENCES,0.46002076843198336,ln(|M|c2/Î´)
REFERENCES,0.46105919003115264,"n
.
(From (9))"
REFERENCES,0.4620976116303219,"Remark 2 (To compete with all history-dependent polices). Consider the case where Î  is all
Markovian polices. We want to show we can compete with all history-dependent non-Markovian
polices: Â¯Î  = ( âˆ
Y"
REFERENCES,0.46313603322949115,"i=1
Ï€i | Ï€i âˆˆ"
REFERENCES,0.46417445482866043,""" iâˆ’1
Y"
REFERENCES,0.4652128764278297,"k=1
S Ã— A ! â†’âˆ†(A) #) ."
REFERENCES,0.46625129802699894,"We take an element Ï€âˆ—from Â¯Î . Then, V Ï€âˆ—
P â‹†and dÏ€âˆ—
P â‹†are still well-deï¬ned. Then, every step in the
proof still holds. The only step we need to check carefully is this line:"
REFERENCES,0.4672897196261682,"V Ï€âˆ—
P â‹†âˆ’V Ë†Ï€
P â‹†â‰¤V Ï€âˆ—
P â‹†âˆ’min
P âˆˆMD V Ï€âˆ—
P
+ min
P âˆˆMD V Ï€âˆ—
P
âˆ’V Ë†Ï€
P â‹†"
REFERENCES,0.4683281412253375,"â‰¤V Ï€âˆ—
P â‹†âˆ’min
P âˆˆMD V Ï€âˆ—
P
+ min
P âˆˆMD V Ë†Ï€
P âˆ’V Ë†Ï€
P â‹†."
REFERENCES,0.46936656282450673,"This is proved by maxÏ€âˆˆÂ¯Î  V Ï€
P = maxÏ€âˆˆÎ  V Ï€
P for any P."
REFERENCES,0.470404984423676,"E.2
PROOFS FOR GENERAL FUNCTION APPROXIMATION WITH INFINITE HYPOTHESIS CLASS"
REFERENCES,0.4714434060228453,"Proof of Theorem 5. From Theorem 4, the MLE guarantee gives us the following generalization
bound: with probability 1 âˆ’Î´,"
REFERENCES,0.4724818276220145,"E(s,a)âˆ¼Ï[TV( bPMLE(Â· | s, a), P â‹†(Â· | s, a))2] â‰²

Î¾2
n + ln(c/Î´) n"
REFERENCES,0.4735202492211838,"
.
(10)"
REFERENCES,0.4745586708203531,We deï¬ne M1 = (
REFERENCES,0.4755970924195223,"P âˆˆM : E(s,a)âˆ¼Ï

TV(P(Â· | s, a), P â‹†(Â· | s, a))2
â‰¤c "
REFERENCES,0.4766355140186916,"Î¾2
n + r"
REFERENCES,0.47767393561786087,ln(c/Î´) n !) .
REFERENCES,0.4787123572170301,"for some large c. From a functional Bernsteinâ€™s inequality (Lemma 12), by deï¬ning"
REFERENCES,0.4797507788161994,"f(P)(s, a) = TV(P(Â· | s, a), P â‹†(Â· | s, a))2, G(M1) = E[ sup
P âˆˆM1
|(ED âˆ’EÏ)[f(P)]|]."
REFERENCES,0.48078920041536866,Published as a conference paper at ICLR 2022
REFERENCES,0.4818276220145379,"with probability 1 âˆ’Î´, we have"
REFERENCES,0.48286604361370716,"sup
P âˆˆM1
|(ED âˆ’EÏ)[f(P)]| â‰²G(M1) + {G(M1) + sup
P âˆˆM1
EÏ[f(P)2]}1/2
r"
REFERENCES,0.48390446521287644,log(c1/Î´)
REFERENCES,0.48494288681204567,"n
+ log(c1/Î´) n"
REFERENCES,0.48598130841121495,"â‰²G(M1) + {G(M1) + sup
P âˆˆM1
EÏ[f(P)]}1/2
r"
REFERENCES,0.48701973001038423,log(c1/Î´)
REFERENCES,0.48805815160955346,"n
+ log(c1/Î´) n"
REFERENCES,0.48909657320872274,"â‰²G(M1) + Î¾2
n +
ln(c/Î´) n"
REFERENCES,0.490134994807892,"
.
(11)"
REFERENCES,0.49117341640706125,"Similarly, by deï¬ning"
REFERENCES,0.49221183800623053,"M2 =

P : E(s,a)âˆ¼D[TV(P(Â· | s, a), P â‹†(Â· | s, a))2] â‰¤cG(M1) + cÎ¾2
n + c
ln(c/Î´) n 
,"
REFERENCES,0.4932502596053998,"G(M2) = E[ sup
P âˆˆMD
|(ED âˆ’EÏ)[f(P)]|],"
REFERENCES,0.49428868120456904,"Zn = sup
P âˆˆM2
E(s,a)âˆ¼Ï[f(P)(s, a)]."
REFERENCES,0.4953271028037383,"from a functional Bernsteinâ€™s inequality, with probability 1 âˆ’Î´, we have"
REFERENCES,0.4963655244029076,"sup
P âˆˆM2
|(ED âˆ’EÏ)[f(P)]| â‰²G(M2) + {G(M2) + Zn}1/2
r"
REFERENCES,0.4974039460020768,log(c1/Î´)
REFERENCES,0.4984423676012461,"n
+ log(c1/Î´) n"
REFERENCES,0.4994807892004154,â‰²G(M2) + r
REFERENCES,0.5005192107995846,"Zn
ln(c/Î´)"
REFERENCES,0.5015576323987538,"n
+ ln(c/Î´)"
REFERENCES,0.5025960539979232,"n
(12)"
REFERENCES,0.5036344755970924,"Hereafter, we condition on the above three events:(10), (11) and (12)."
REFERENCES,0.5046728971962616,"First step: Show P âˆ—âˆˆMD in high probability.
From (10), we have"
REFERENCES,0.505711318795431,"E(s,a)âˆ¼Ï[f( Ë†PMLE)(s, a)] â‰¤Î¾2
n + ln(c/Î´) n
."
REFERENCES,0.5067497403946002,"Thus, P â‹†âˆˆM1. Then, from (11) and (10),"
REFERENCES,0.5077881619937694,"E(s,a)âˆ¼D[f( Ë†PMLE)(s, a)]"
REFERENCES,0.5088265835929388,"= |E(s,a)âˆ¼D[f( Ë†PMLE)(s, a)] âˆ’E(s,a)âˆ¼Ï[f( Ë†PMLE)(s, a)]| + E(s,a)âˆ¼Ï[f( Ë†PMLE)(s, a)]"
REFERENCES,0.509865005192108,"â‰²G(M1) + Î¾2
n + ln(c/Î´) n
."
REFERENCES,0.5109034267912772,"Thus, P â‹†âˆˆMD recalling the deï¬nition of MD (we set Î¾ = G(M1) + Î¾2
n + ln(c/Î´) n
)."
REFERENCES,0.5119418483904465,"Second step: Show the upper bound of E(s,a)âˆ¼Ï[f(P)(s, a)] for any P âˆˆMD.
For any
P âˆˆMD, as the proof of Theorem 1, we can prove P âˆˆM2. Thus,"
REFERENCES,0.5129802699896158,"sup
P âˆˆMD
E(s,a)âˆ¼Ï[f(P)(s, a)] â‰¤Zn."
REFERENCES,0.514018691588785,"Thus, we will hereafter analyze Zn. Here from (12), for any P âˆˆM2, we have"
REFERENCES,0.5150571131879543,"E(s,a)âˆ¼Ï[f(P)(s, a)] â‰¤sup
P âˆˆM2"
REFERENCES,0.5160955347871236," 
|(E(s,a)âˆ¼D âˆ’E(s,a)âˆ¼Ï)[f(P)(s, a)]|

+ sup
P âˆˆM2
E(s,a)âˆ¼D[f(P)(s, a)]"
REFERENCES,0.5171339563862928,â‰²G(M2) + r
REFERENCES,0.5181723779854621,"Zn
ln(c/Î´)"
REFERENCES,0.5192107995846313,"n
+ G(M1) + Î¾2
n + ln(c/Î´) n
."
REFERENCES,0.5202492211838006,"Hence,"
REFERENCES,0.5212876427829699,Zn â‰¤G(M2) + r
REFERENCES,0.5223260643821391,"Zn
ln(c/Î´)"
REFERENCES,0.5233644859813084,"n
+ G(M1) + Î¾2
n + ln(c/Î´) n
."
REFERENCES,0.5244029075804777,This shows
REFERENCES,0.5254413291796469,"Zn â‰¤G(M2) + G(M1) + Î¾2
n + ln(c/Î´) n
."
REFERENCES,0.5264797507788161,Published as a conference paper at ICLR 2022
REFERENCES,0.5275181723779855,"Third step: Calculate the ï¬nal error bound.
Following the proof of Theorem 1, we can prove"
REFERENCES,0.5285565939771547,"V Ï€âˆ—
P â‹†âˆ’V Ë†Ï€
P â‹†â‰¤(1 âˆ’Î³)âˆ’2c1
q Câ€ 
Ï€âˆ— r"
REFERENCES,0.5295950155763239,"G(M2) + G(M1) + Î¾2n + ln(c/Î´) n
."
REFERENCES,0.5306334371754933,"E.3
PROOFS FOR TABULAR MDPS"
REFERENCES,0.5316718587746625,Proof of Corollary 1. We prove in a similar way as Theorem 1.
REFERENCES,0.5327102803738317,"First step
We set Î¾ = c |S|2|A| ln(n|S|A|c2/Î´)}"
REFERENCES,0.5337487019730011,"n
. Then, from Lemma 7, with probability 1 âˆ’Î´, we
can show P â‹†âˆˆMD since"
REFERENCES,0.5347871235721703,"Es,aâˆ¼D
h
TV( bPMLE(Â· | s, a), P â‹†(Â· | s, a))2i
â‰¤Î¾."
REFERENCES,0.5358255451713395,"Hereafter, we condition on the above event."
REFERENCES,0.5368639667705088,"Second step.
Following the second step in the proof of Theorem 1 based on (8), for any P âˆˆMD,
we have"
REFERENCES,0.5379023883696781,"Es,aâˆ¼Ï

TV(P(Â· | s, a), P â‹†(Â· | s, a))2
â‰¤cÎ¾ + A(P)
(13) where"
REFERENCES,0.5389408099688473,"A(P) := |Es,aâˆ¼Ï[TV(P(Â· | s, a), P â‹†(Â· | s, a))2] âˆ’ED[TV(P(Â· | s, a), P â‹†(Â· | s, a))2]|."
REFERENCES,0.5399792315680166,"Our goal here is showing with probability 1 âˆ’Î´,"
REFERENCES,0.5410176531671859,"A(P) â‰²Î¾, âˆ€P âˆˆMD.
(14)"
REFERENCES,0.5420560747663551,"To prove (14), consider an Ïµ-net {P1(s, a), Â· Â· Â· , PK(s, a)} covering a simplex in terms of âˆ¥Â·âˆ¥1 4 for
each ï¬xed pair (s, a) âˆˆS Ã— A. We take Ïµ = 1/n. Since the covering number K is upper-bounded
by (c/Ïµ)|S| (Wainwright, 2019, Lemma 5.7), we can obtain Â¯
M = {P1, Â· Â· Â· , PK|S|Ã—|A|} s.t. for any
possible P âŠ‚S Ã— A â†’âˆ†(S), there exists Pi s.t."
REFERENCES,0.5430944963655244,"TV(Pi(Â· | s, a), P(Â· | s, a)) â‰¤Ïµ, âˆ€(s, a)."
REFERENCES,0.5441329179646937,"This implies for any P âŠ‚S Ã— A â†’âˆ†(S), there exists Pi(Â· | s, a) s.t. âˆ€(s, a),"
REFERENCES,0.5451713395638629,"|TV(P(Â· | s, a), P â‹†(Â· | s, a))2 âˆ’TV(Pi(Â· | s, a), P â‹†(Â· | s, a))2|"
REFERENCES,0.5462097611630322,"â‰¤4|TV(P(Â· | s, a), P â‹†(Â· | s, a)) âˆ’TV(Pi(Â· | s, a), P â‹†(Â· | s, a))|
(a2 âˆ’b2 = (a âˆ’b)(a + b))
â‰¤4TV(PÂ· | s, a), Pi(Â· | s, a))
(|âˆ¥aâˆ¥âˆ’âˆ¥bâˆ¥| â‰¤âˆ¥a âˆ’bâˆ¥)
â‰¤4Ïµ.
(15)"
REFERENCES,0.5472481827622014,We often use this property (15) hereafter.
REFERENCES,0.5482866043613707,"Next, we deï¬ne Mâ€² âŠ‚Â¯
M so that it covers MD. Concretely, we deï¬ne Mâ€²:"
REFERENCES,0.54932502596054,"Mâ€² = {P âˆˆÂ¯
M : âˆƒP â€²â€² âˆˆMD, TV(P(Â· | s, a), P â€²â€²(Â· | s, a)) â‰¤Ïµ
âˆ€(s, a)}.
(16)"
REFERENCES,0.5503634475597092,"The construction is illustrated in Figure 1. Here, from the deï¬nition, for any P âˆˆMD , we can also
ï¬nd P â€² âˆˆMâ€² s.t.
TV(P(Â· | s, a), P â€²(Â· | s, a)) â‰¤Ïµ, âˆ€(s, a).
This is because from the deï¬nition of Â¯
M, we can always ï¬nd P âˆˆÂ¯
M satisfying the above. Such P
belongs to Mâ€² from the deï¬nition of Mâ€². We use this fact later."
REFERENCES,0.5514018691588785,"Then, from (16) and recalling (13), we have"
REFERENCES,0.5524402907580478,"Es,aâˆ¼Ï

TV(P(Â· | s, a), P â‹†(Â· | s, a))2
â‰²Î¾ + A(P),
âˆ€P âˆˆMâ€².
(17)"
REFERENCES,0.553478712357217,"4In the tabular setting, since the state space is countable, it is equivalent to L1 distance."
REFERENCES,0.5545171339563862,Published as a conference paper at ICLR 2022
REFERENCES,0.5555555555555556,"Figure 1: MD is colored in gray. Mâ€² corresponds to the set of black dots. Orange dots correspond
to Â¯
M, which do not belong to Mâ€²."
REFERENCES,0.5565939771547248,because
REFERENCES,0.557632398753894,"Es,aâˆ¼Ï

TV(P(Â· | s, a), P â‹†(Â· | s, a))2"
REFERENCES,0.5586708203530634,"â‰¤Es,aâˆ¼Ï

TV(P(Â· | s, a), P â€²â€²(Â· | s, a))2
+ Es,aâˆ¼Ï

TV(P â€²â€²(Â· | s, a), P â‹†(Â· | s, a))2"
REFERENCES,0.5597092419522326,"â‰¤Es,aâˆ¼Ï

TV(P â€²â€²(Â· | s, a), P â‹†(Â· | s, a))2
+ Ïµ2
(Take some P â€²â€² âˆˆMD noting (16))"
REFERENCES,0.5607476635514018,"â‰¤cÎ¾ + A(P).
(From (13))"
REFERENCES,0.5617860851505712,"Then, with probability 1 âˆ’Î´, from Bernsteinâ€™s inequality, we have"
REFERENCES,0.5628245067497404,A(P) â‰¤ r
REFERENCES,0.5638629283489096,"cvar[TV(P(Â· | s, a), P â‹†(Â· | s, a))2] ln(K|S|Ã—|A|/Î´)"
REFERENCES,0.564901349948079,"n
+ c ln(K|S|Ã—|A|/Î´)"
REFERENCES,0.5659397715472482,"n
, âˆ€P âˆˆM."
REFERENCES,0.5669781931464174,"Hereafter, we condition on the above event. Based on (17), we can state"
REFERENCES,0.5680166147455867,"var[TV(P(Â· | s, a), P â‹†(Â· | s, a))2] â‰²E[TV(P(Â· | s, a), P â‹†(Â· | s, a))2] â‰²Î¾ + A(P),
âˆ€P âˆˆMâ€²,"
REFERENCES,0.569055036344756,"with probability 1 âˆ’Î´. Following the argument of Theorem 1, for P âˆˆMâ€², we have"
REFERENCES,0.5700934579439252,"A2(P) âˆ’A(P)B1 âˆ’B2 â‰¤0,
B1 = ln(K|S|Ã—|A|/Î´)"
REFERENCES,0.5711318795430945,"n
,
B2 = Î¾ ln(K|S|Ã—|A|/Î´)"
REFERENCES,0.5721703011422637,"n
+
ln(K|S|Ã—|A|/Î´) n 2
."
REFERENCES,0.573208722741433,"Then, with probability 1 âˆ’Î´, we have"
REFERENCES,0.5742471443406023,"A(P) â‰¤ln(K|S|Ã—|A|/Î´) n
+ r"
REFERENCES,0.5752855659397715,ln(K|S|Ã—|A|/Î´)
REFERENCES,0.5763239875389408,"n
Î¾1/2 â‰²Î¾,
âˆ€P âˆˆMâ€².
(18)"
REFERENCES,0.5773624091381101,"This shows for any P âˆˆMD, we have"
REFERENCES,0.5784008307372793,"|{ED âˆ’E(s,a)âˆ¼Ï}[TV(P(Â· | s, a), P â‹†(Â· | s, a))2]|"
REFERENCES,0.5794392523364486,"â‰¤|{ED âˆ’E(s,a)âˆ¼Ï}[TV(P â€²(Â· | s, a), P(Â· | s, a))2 + TV(P â€²(Â· | s, a), P â‹†(Â· | s, a))2]|
(We take P â€² âˆˆMâ€² such that (16))"
REFERENCES,0.5804776739356179,"â‰¤|{ED âˆ’E(s,a)âˆ¼Ï}[TV(P â€²(Â· | s, a), P â‹†(Â· | s, a))2] + 8Ïµ
(From the deï¬nition of Mâ€²)"
REFERENCES,0.5815160955347871,"â‰²Î¾.
(From (18) and P â€² âˆˆMâ€²)"
REFERENCES,0.5825545171339563,"Thus, (14) is proved."
REFERENCES,0.5835929387331257,"Third step.
We follow the third step of Theorem 1:"
REFERENCES,0.5846313603322949,"V Ï€âˆ—
P â‹†âˆ’V Ë†Ï€
P â‹†â‰²(1 âˆ’Î³)âˆ’2
q"
REFERENCES,0.5856697819314641,"Câ€ 
Ï€âˆ—Î¾."
REFERENCES,0.5867082035306335,Published as a conference paper at ICLR 2022
REFERENCES,0.5877466251298027,"E.4
PROOFS FOR LINEAR MIXTURE MDPS"
REFERENCES,0.5887850467289719,"Proof of Corollary 2. Here, letting P(Î¸) = Î¸âŠ¤Ïˆ(s, a, sâ€²), recall"
REFERENCES,0.5898234683281413,"MMix =

P(Î¸) | Î¸ âˆˆÎ˜ âŠ‚Rd,
Z
Î¸âŠ¤Ïˆ(s, a, sâ€²)d(sâ€²) = 1
âˆ€(s, a)

, H = (r"
REFERENCES,0.5908618899273105,P + P â‹†
REFERENCES,0.5919003115264797,"2
| P âˆˆMMix ) ."
REFERENCES,0.592938733125649,"Upper-bounding E(s,a)âˆ¼Ï[TV(P(Î¸â‹†)(Â· | s, a), P(Ë†Î¸MLE)(Â· | s, a))2].
By invoking Theorem 4, we ï¬rst show"
REFERENCES,0.5939771547248183,"E(s,a)âˆ¼Ï[TV(P(Î¸â‹†)(Â· | s, a), P(Ë†Î¸MLE)(Â· | s, a))2] â‰¤c{(d/n) ln2(nR) + ln(c/Î´)/n}."
REFERENCES,0.5950155763239875,"To do that, we calculate the entropy integral with bracketing. First, we have
N[](Ïµ, H, d) â‰¤N[](Ïµ, MMix, dâ€²).
(19)
where"
REFERENCES,0.5960539979231568,"dâ€²(a, b) = E(s,a)âˆ¼Ï"
REFERENCES,0.5970924195223261,"Z
(a(s, a, sâ€²) âˆ’b(s, a, sâ€²))2d(sâ€²)
1/2
,
(20)"
REFERENCES,0.5981308411214953,"d(a, b) = E(s,a)âˆ¼Ï"
REFERENCES,0.5991692627206646,"Z
(
p"
REFERENCES,0.6002076843198338,"a(s, a, sâ€²) âˆ’
p"
REFERENCES,0.6012461059190031,"b(s, a, sâ€²))2d(sâ€²)
1/2
.
(21)"
REFERENCES,0.6022845275181724,"Here, we use two observations. The ï¬rst observation is d2
 r"
REFERENCES,0.6033229491173416,"P(Î¸â€²) + P â‹† 2
, r"
REFERENCES,0.6043613707165109,P(Î¸â€²â€²) + P â‹† 2 !
REFERENCES,0.6053997923156802,"â‰¤c1dâ€²2(P(Î¸â€²), P(Î¸â€²â€²))"
REFERENCES,0.6064382139148494,"due to the mean-value theoremâˆša âˆ’
âˆš"
REFERENCES,0.6074766355140186,"b â‰¤max(1/âˆša, 1/
âˆš"
REFERENCES,0.608515057113188,"b)(a âˆ’b)
and assumption P â‹†(sâ€² | s, a) â‰¥c0 > 0. The second observation is when we have P â€² < g < P â€²â€², we
also have
p"
REFERENCES,0.6095534787123572,"(P â€² + P â‹†)/2 <
p"
REFERENCES,0.6105919003115264,"(g + P â‹†)/2 <
p"
REFERENCES,0.6116303219106958,"(P â€²â€² + P â‹†)/2. Then, (19) is concluded."
REFERENCES,0.612668743509865,"Next, by letting Î¸(1), Â· Â· Â· , Î¸(K) be an Ïµ-cover of the d-dimensional ball with a radius R, i.e, Bd(R),
we have the brackets {[P(Î¸(i)) âˆ’Ïµ, P(Î¸(i)) + Ïµ]}K
i=1 which cover Mmix. This is because for any
P(Î¸) âˆˆMmix, we can take Î¸(i) s.t. âˆ¥Î¸ âˆ’Î¸(i)âˆ¥2 â‰¤Ïµ, then,"
REFERENCES,0.6137071651090342,"P(Î¸(i)) âˆ’Ïµ < P(Î¸) < P(Î¸(i)) + Ïµ,
âˆ€(s, a, sâ€²)
noting"
REFERENCES,0.6147455867082036,"|P(Î¸)(s, a, sâ€²) âˆ’P(Î¸(i))(s, a, sâ€²)| â‰¤âˆ¥Î¸ âˆ’Î¸(i)âˆ¥2 â‰¤Ïµ,
âˆ€(s, a, sâ€²)
(22)
The last equality is from Lemma 10."
REFERENCES,0.6157840083073728,"The brackets above are size of Ïµ. Therefore, we have
N[](Ïµ, Mmix, âˆ¥Â· âˆ¥2) â‰¤N(Ïµ, Bd(cR), âˆ¥Â· âˆ¥2),"
REFERENCES,0.616822429906542,"where N(Ïµ, Bd(cR), âˆ¥Â· âˆ¥2) is a covering number of Bd(cR) w.r.t âˆ¥Â· âˆ¥2. This is upper-bounded by
(cR/Ïµ)d (Wainwright, 2019, Lemma 5.7). Thus, we can calculate the upper bound of the entropy
integral JB(Î´, Mmix, âˆ¥Â· âˆ¥2):
Z Î´"
REFERENCES,0.6178608515057114,"0
d1/2 ln1/2(cR/u)du â‰¤
Z Î´"
REFERENCES,0.6188992731048806,"0
d1/2 ln(1/u)du + Î´d1/2 ln(c1R)"
REFERENCES,0.6199376947040498,= cd1/2(Î´ + Î´ ln(1/Î´)) + Î´d1/2 ln(cR)
REFERENCES,0.6209761163032191,â‰¤cd1/2Î´ ln(cR/Î´).
REFERENCES,0.6220145379023884,"By taking G(x) = d1/2x ln(cR/x) in Theorem 4, Î´n = (d/n)1/2 ln(nR) satisï¬es the critical in-
equality
âˆšnÎ´2
n â‰¥d1/2Î´n ln(cR/Î´n).
Finally, with probability 1 âˆ’Î´"
REFERENCES,0.6230529595015576,"E(s,a)âˆ¼Ï[TV(P(Î¸â‹†)(Â· | s, a), P(Ë†Î¸MLE)(Â· | s, a))2] â‰¤Î¾â€²,
Î¾â€² := {(d/n) ln2(nR) + ln(c/Î´)/n}.
(23)
Hereafter, we condition on this event."
REFERENCES,0.6240913811007269,Published as a conference paper at ICLR 2022
REFERENCES,0.6251298026998962,"Upper bounding ED[TV(P(Î¸â‹†)(Â· | s, a), P(Ë†Î¸MLE)(Â· | s, a))2].
We take an Ïµ-cover of the ball
Bd(R) in terms of âˆ¥Â·âˆ¥2, i.e., Â¯
M = {Î¸(1), Â· Â· Â· , Î¸(K)}, where K = (cR/Ïµ)d. We take Ïµ = 1/n. Then,
for any Î¸ âˆˆBd(R), there exists Î¸(i) s.t. âˆ€(s, a),"
REFERENCES,0.6261682242990654,"|TV(P(Î¸)(Â· | s, a), P(Ë†Î¸MLE)(Â· | s, a))2 âˆ’TV(P(Î¸(i))(Â· | s, a), P(Ë†Î¸MLE)(Â· | s, a))2|"
REFERENCES,0.6272066458982347,"â‰¤4|TV(P(Î¸)(Â· | s, a), P(Ë†Î¸MLE)(Â· | s, a)) âˆ’TV(P(Î¸(i))(Â· | s, a), P(Ë†Î¸MLE)(Â· | s, a))|
(a2 âˆ’b2 = (a âˆ’b)(a + b))"
REFERENCES,0.6282450674974039,"â‰¤4TV(P(Î¸)(Â· | s, a), P(Î¸(i))(Â· | s, a))
(|âˆ¥aâˆ¥âˆ’âˆ¥bâˆ¥| â‰¤âˆ¥a âˆ’bâˆ¥)"
REFERENCES,0.6292834890965732,"â‰¤4âˆ¥Î¸ âˆ’Î¸(i)âˆ¥2
(From Lemma 10.)
â‰¤4Ïµ.
(24)"
REFERENCES,0.6303219106957425,"Hereafter, we condition on the event:"
REFERENCES,0.6313603322949117,"|(ED âˆ’E(s,a)âˆ¼Ï)[TV(P(Î¸)(Â· | s, a), P(Î¸â‹†)(Â· | s, a))2]|
(25) â‰² r"
REFERENCES,0.632398753894081,"var(s,a)âˆ¼Ï[TV(P(Î¸)(Â· | s, a), P(Î¸â‹†)(Â· | s, a))2] ln(K/Î´)"
REFERENCES,0.6334371754932503,"n
+ ln(K/Î´)"
REFERENCES,0.6344755970924195,"n
,
âˆ€Î¸ âˆˆÂ¯
M."
REFERENCES,0.6355140186915887,This event holds with probability 1 âˆ’Î´ from Bernsteinâ€™s inequality.
REFERENCES,0.6365524402907581,"Here, note for Ë†Î¸MLE, we have Î¸(i) s.t. âˆ¥Ë†Î¸MLE âˆ’Î¸(i)âˆ¥2 â‰¤Ïµ. Then, following the ï¬rst step of
Theorem 1,"
REFERENCES,0.6375908618899273,"E(s,a)âˆ¼DTV(P(Î¸â‹†)(Â· | s, a), P(Ë†Î¸MLE)(Â· | s, a))2"
REFERENCES,0.6386292834890965,"â‰²E(s,a)âˆ¼DTV(P(Î¸â‹†)(Â· | s, a), P(Î¸(i))(Â· | s, a))2 + Ïµ
(From (24))"
REFERENCES,0.6396677050882659,"â‰²(ED âˆ’E(s,a)âˆ¼Ï)TV(P(Î¸â‹†)(Â· | s, a), P(Î¸(i))(Â· | s, a))2 + Ïµ + E(s,a)âˆ¼ÏTV(P(Î¸â‹†)(Â· | s, a), P(Î¸(i))(Â· | s, a))2 â‰² s"
REFERENCES,0.6407061266874351,"var(s,a)âˆ¼Ï[TV(P(Î¸â‹†)(Â· | s, a), P(Î¸(i))(Â· | s, a))2] ln(K/Î´)"
REFERENCES,0.6417445482866043,"n
+ ln(K/Î´) n"
REFERENCES,0.6427829698857737,"+ Ïµ + E(s,a)âˆ¼ÏTV(P(Î¸â‹†)(Â· | s, a), P(Î¸(i))(Â· | s, a))2
(From (25)) â‰² s"
REFERENCES,0.6438213914849429,"E(s,a)âˆ¼Ï[TV(P(Î¸â‹†)(Â· | s, a), P(Î¸(i))(Â· | s, a))2] ln(K/Î´)"
REFERENCES,0.6448598130841121,"n
+ ln(K/Î´) n"
REFERENCES,0.6458982346832814,"+ Ïµ + E(s,a)âˆ¼Ï[TV(P(Î¸â‹†)(Â· | s, a), P(Î¸(i))(Â· | s, a))2].
(TV(P(Î¸â‹†)(Â· | s, a), P(Î¸(i))(Â· | s, a))2 â‰¤4)"
REFERENCES,0.6469366562824507,"Then, we have"
REFERENCES,0.6479750778816199,"ED[TV(P(Î¸â‹†)(Â· | s, a), P(Ë†Î¸MLE)(Â· | s, a))2] â‰² s"
REFERENCES,0.6490134994807892,"{E(s,a)âˆ¼Ï[TV(P(Î¸â‹†)(Â· | s, a), P(Ë†Î¸MLE)(Â· | s, a))2] + Ïµ} ln(K/Î´)"
REFERENCES,0.6500519210799585,"n
+ ln(K/Î´) n"
REFERENCES,0.6510903426791277,"+ Ïµ + E(s,a)âˆ¼ÏTV(P(Î¸â‹†)(Â· | s, a), P(Ë†Î¸MLE)(Â· | s, a))2 â‰² r"
REFERENCES,0.652128764278297,{Î¾â€² + Ïµ} ln(K/Î´)
REFERENCES,0.6531671858774662,"n
+ ln(K/Î´)"
REFERENCES,0.6542056074766355,"n
+ Ïµ + Î¾â€².
(From (23))"
REFERENCES,0.6552440290758048,"In the end, by taking Ïµ = 1/n, we have with probability 1 âˆ’Î´,"
REFERENCES,0.656282450674974,"EDTV(P(Î¸â‹†)(Â· | s, a), P(Ë†Î¸MLE)(Â· | s, a))2 â‰¤Î¾,
Î¾ = c{(d/n) ln2(nR) + ln(c/Î´)/n}."
REFERENCES,0.6573208722741433,"This also implies with probability 1 âˆ’Î´, P â‹†âˆˆMD."
REFERENCES,0.6583592938733126,"Show E(s,a)âˆ¼ÏTV(P(Î¸â‹†)(Â· | s, a), P(Î¸)(Â· | s, a))2 â‰²Î¾, âˆ€P(Î¸) âˆˆMD.
We show for any P âˆˆMD, the distance between P â‹†is controlled in terms of TV distance. Our
goal is showing"
REFERENCES,0.6593977154724818,"E(s,a)âˆ¼ÏTV(P(Î¸â‹†)(Â· | s, a), P(Î¸)(Â· | s, a))2 â‰²Î¾,
âˆ€P(Î¸) âˆˆMD."
REFERENCES,0.660436137071651,Published as a conference paper at ICLR 2022
REFERENCES,0.6614745586708204,"First, following the second step of Theorem 1 based on equation 8, we have
E(s,a)âˆ¼ÏTV(P(Î¸â‹†)(Â· | s, a), P(Î¸)(Â· | s, a))2 â‰¤A(Î¸) + cÎ¾,
âˆ€P(Î¸) âˆˆMD
(26)
where
A(Î¸) := |(ED âˆ’E(s,a)âˆ¼Ï)TV(P(Î¸â‹†)(Â· | s, a), P(Î¸)(Â· | s, a))2|."
REFERENCES,0.6625129802699896,"From now on, we again consider an Ïµ-cover of the ball Bd(R) in terms of âˆ¥Â· âˆ¥2, i.e.,
Â¯
M =
{Î¸(1), Â· Â· Â· , Î¸(K)}, where K = (c1R/Ïµ)d (Ïµ = 1/n). This also covers the space MD. We take
Mâ€² = {Î¸(i1), Î¸(i2) Â· Â· Â· , } âŠ‚M which covers MD, that is,"
REFERENCES,0.6635514018691588,"Mâ€² =

Î¸ âˆˆÂ¯
M | âˆƒÎ¸â€²s.t.P(Î¸â€²) âˆˆMD, âˆ¥Î¸ âˆ’Î¸â€²âˆ¥2 â‰¤Ïµ
	
."
REFERENCES,0.6645898234683282,"Recall Figure 1, which illustrates this deï¬nition. Here, for any Î¸ s.t. âˆ€P(Î¸) âˆˆMD, we can take
Î¸â€² âˆˆMâ€² s.t. âˆ¥Î¸ âˆ’Î¸â€²âˆ¥2 â‰¤1/n. This is because we can take Î¸ âˆˆÂ¯
M s.t. âˆ¥Î¸ âˆ’Î¸â€²âˆ¥2 â‰¤Ïµ noting Â¯
M is
an Ïµ-net, but such Î¸ belongs to Mâ€² from the deï¬nition of Mâ€²."
REFERENCES,0.6656282450674974,"Then, we have
E(s,a)âˆ¼ÏTV(P(Î¸â‹†)(Â· | s, a), P(Î¸)(Â· | s, a))2 â‰¤A(Î¸) + cÎ¾,
âˆ€Î¸ âˆˆMâ€².
(27)"
REFERENCES,0.6666666666666666,"This is because for any Î¸(i) âˆˆMâ€², we can take P(Î¸) âˆˆMD such that"
REFERENCES,0.667705088265836,"E(s,a)âˆ¼ÏTV(P(Î¸â‹†)(Â· | s, a), P(Î¸(i))(Â· | s, a))2"
REFERENCES,0.6687435098650052,"â‰¤E(s,a)âˆ¼Ï[TV(P(Î¸â‹†)(Â· | s, a), P(Î¸(i))(Â· | s, a))2 âˆ’TV(P(Î¸â‹†)(Â· | s, a), P(Î¸)(Â· | s, a))2]"
REFERENCES,0.6697819314641744,"+ E(s,a)âˆ¼Ï[TV(P(Î¸â‹†)(Â· | s, a), P(Î¸)(Â· | s, a))2]"
REFERENCES,0.6708203530633438,"â‰¤4Ïµ + E(s,a)âˆ¼Ï[TV(P(Î¸â‹†)(Â· | s, a), P(Î¸)(Â· | s, a))2]
(âˆ¥Î¸ âˆ’Î¸(i)âˆ¥2 â‰¤Ïµ and from (24))"
REFERENCES,0.671858774662513,"â‰²A(Î¸) + Î¾.
( From (26))"
REFERENCES,0.6728971962616822,"Here, from (25), we have"
REFERENCES,0.6739356178608515,A(Î¸) â‰¤ r
REFERENCES,0.6749740394600208,"cvar(s,a)âˆ¼Ï[TV(P(Î¸â‹†)(Â· | s, a), P(Î¸)(Â· | s, a))2] ln(K/Î´)"
REFERENCES,0.67601246105919,"n
+ c ln(K/Î´)"
REFERENCES,0.6770508826583593,"n
,
âˆ€Î¸ âˆˆMâ€²"
REFERENCES,0.6780893042575286,"Based on the construction of Mâ€² and (27), we have
var(s,a)âˆ¼Ï[TV(P(Î¸â‹†)(Â· | s, a), P(Î¸)(Â· | s, a))2] â‰²A(Î¸) + Î¾,
âˆ€Î¸ âˆˆMâ€²."
REFERENCES,0.6791277258566978,"Then, following the second step of Theorem 1, A(Î¸) satisï¬es"
REFERENCES,0.6801661474558671,"A2(Î¸) âˆ’A(Î¸)B1 âˆ’B2 â‰¤0,
B1 = ln(K/Î´)"
REFERENCES,0.6812045690550363,"n
, B2 = Î¾ ln(K/Î´)"
REFERENCES,0.6822429906542056,"n
+
ln(K/Î´) n 2
."
REFERENCES,0.6832814122533749,"Then, we have"
REFERENCES,0.6843198338525441,A(Î¸) â‰¤ln(K/Î´)
REFERENCES,0.6853582554517134,"n
+ Î¾1/2
r"
REFERENCES,0.6863966770508827,ln(K/Î´)
REFERENCES,0.6874350986500519,"n
â‰²Î¾,
âˆ€Î¸ âˆˆMâ€².
(28)"
REFERENCES,0.6884735202492211,"We combine all steps. Recall for any âˆ€P(Î¸) âˆˆMD, we can take Î¸â€² âˆˆMâ€² s.t. âˆ¥Î¸ âˆ’Î¸â€²âˆ¥2 â‰¤1/n.
Then, for any P(Î¸) âˆˆMD, we have"
REFERENCES,0.6895119418483905,"A(Î¸) = |(ED âˆ’E(s,a)âˆ¼Ï)TV(P(Î¸)(Â· | s, a), P(Î¸â‹†)(Â· | s, a))2"
REFERENCES,0.6905503634475597,"â‰¤|(ED âˆ’E(s,a)âˆ¼Ï)[TV(P(Î¸)(Â· | s, a), P(Î¸â‹†)(Â· | s, a))2 âˆ’TV(P(Î¸â€²)(Â· | s, a), P(Î¸â‹†)(Â· | s, a))2]"
REFERENCES,0.6915887850467289,"+ (ED âˆ’E(s,a)âˆ¼Ï)[TV(P(Î¸â€²)(Â· | s, a), P(Î¸â‹†)(Â· | s, a))2]"
REFERENCES,0.6926272066458983,"â‰¤8Ïµ + |(ED âˆ’E(s,a)âˆ¼Ï)[TV(P(Î¸â€²)(Â· | s, a), P(Î¸â‹†)(Â· | s, a))2]
(From equation 24)"
REFERENCES,0.6936656282450675,"â‰²Î¾.
(From equation 28 and Î¸â€² âˆˆMâ€²)
Then, we have with probability 1 âˆ’Î´,
A(Î¸) â‰²Î¾,
âˆ€P(Î¸) âˆˆMD.
(29)
Finally, for any P(Î¸) âˆˆMD, with probability 1 âˆ’Î´, we have"
REFERENCES,0.6947040498442367,"E(s,a)âˆ¼Ï[TV(P(Î¸â‹†)(Â· | s, a), P(Î¸)(Â· | s, a))2] â‰¤A(Î¸) + cÎ¾
(From equation 26)"
REFERENCES,0.6957424714434061,"â‰²Î¾.
(From equation 29)"
REFERENCES,0.6967808930425753,Published as a conference paper at ICLR 2022
REFERENCES,0.6978193146417445,"Distribution shift part
Here, for P âˆˆMD we prove"
REFERENCES,0.6988577362409139,"V Ï€âˆ—
P â‹†âˆ’V Ï€âˆ—
P
â‰²(1 âˆ’Î³)âˆ’2p"
REFERENCES,0.6998961578400831,"dCÏ€âˆ—,mixÎ¾,
(30)"
REFERENCES,0.7009345794392523,"V Ï€âˆ—
P â‹†âˆ’V Ï€âˆ—
P
â‰²(1 âˆ’Î³)âˆ’2
q"
REFERENCES,0.7019730010384216,"Câ€ 
Ï€âˆ—Î¾.
(31)"
REFERENCES,0.7030114226375909,"Following the third step of the proof of Theorem 5, this immediately concludes the bound"
REFERENCES,0.7040498442367601,"V Ï€âˆ—
P â‹†âˆ’V Ë†Ï€
P â‹†â‰²(1 âˆ’Î³)âˆ’2p"
REFERENCES,0.7050882658359294,"dCÏ€âˆ—,mixÎ¾,"
REFERENCES,0.7061266874350987,"V Ï€âˆ—
P â‹†âˆ’V Ë†Ï€
P â‹†â‰²(1 âˆ’Î³)âˆ’2
q"
REFERENCES,0.7071651090342679,"Câ€ 
Ï€âˆ—Î¾."
REFERENCES,0.7082035306334372,"Since (31) is obvious from simulation lemma, we only prove (30). To prove (30), we take a distri-
bution P(Î¸) âˆˆMD. First, recall for P(Î¸) âˆˆMD, we have"
REFERENCES,0.7092419522326064,"E(s,a)âˆ¼ÏTV(P(Î¸â‹†)(Â· | s, a), P(Î¸)(Â· | s, a))2 â‰²Î¾."
REFERENCES,0.7102803738317757,"From the third statement of Lemma 10, for any V : S â†’[0, 1], we have"
REFERENCES,0.711318795430945,"E(s,a)âˆ¼Ï[|(Î¸ âˆ’Î¸âˆ—)âŠ¤ÏˆV (s, a)|2] â‰²Î¾. Thus,"
REFERENCES,0.7123572170301142,"âˆ€V : S â†’[0, 1],
(Î¸ âˆ’Î¸âˆ—)âŠ¤Î£Ï,V (Î¸ âˆ’Î¸âˆ—) â‰²Î¾,
Î£Ï,V = E(s,a)âˆ¼Ï[ÏˆV (s, a)ÏˆâŠ¤
V (s, a)]."
REFERENCES,0.7133956386292835,"Here, we have"
REFERENCES,0.7144340602284528,"V Ï€âˆ—
P â‹†âˆ’V Ï€âˆ—
P
â‰¤(1 âˆ’Î³)âˆ’1
E(s,a)âˆ¼dÏ€âˆ—
P â‹†"
REFERENCES,0.715472481827622,"Z
{P(sâ€² | s, a) âˆ’P â‹†(sâ€² | s, a)}V Ï€âˆ—
P (sâ€²)d(sâ€²)
"
REFERENCES,0.7165109034267912,"(Simulation lemma, Lemma 5)"
REFERENCES,0.7175493250259606,"â‰¤(1 âˆ’Î³)âˆ’1 E(s,a)âˆ¼dÏ€âˆ—
P â‹†"
REFERENCES,0.7185877466251298,"h
(Î¸ âˆ’Î¸âˆ—)ÏˆV Ï€âˆ—
P (s, a)
i"
REFERENCES,0.719626168224299,"(Recall ÏˆV =
R
Ïˆ(s, a, sâ€²)V Ï€âˆ—
P (sâ€²)d(sâ€²))"
REFERENCES,0.7206645898234684,"â‰¤(1 âˆ’Î³)âˆ’1 âˆ¥Î¸ âˆ’Î¸âˆ—âˆ¥Î»I+Î£Ï,V Ï€âˆ—
P
|
{z
}
(a)"
REFERENCES,0.7217030114226376,"E(s,a)âˆ¼dÏ€âˆ—
P â‹†"
REFERENCES,0.7227414330218068,"
âˆ¥ÏˆV Ï€âˆ—
P (s, a)âˆ¥(Î£Ï,V Ï€âˆ—
P
+Î»I)âˆ’1
"
REFERENCES,0.7237798546209762,"|
{z
}
(b) ."
REFERENCES,0.7248182762201454,(CS inequality)
REFERENCES,0.7258566978193146,"The ï¬rst term (a) is upper-bounded by
p"
REFERENCES,0.726895119418484,"{(1 âˆ’Î³)âˆ’2Î¾ + Î»R2} noting 0 â‰¤V Ï€âˆ—
P
â‰¤(1 âˆ’Î³)âˆ’1. The
term (b) is upper-bounded by"
REFERENCES,0.7279335410176532,"E(s,a)âˆ¼dÏ€âˆ—
P â‹†"
REFERENCES,0.7289719626168224,"
âˆ¥ÏˆV Ï€âˆ—
P (s, a)âˆ¥(Î£Ï,V Ï€âˆ—
P
+Î»I)âˆ’1

â‰¤E(s,a)âˆ¼dÏ€âˆ—
P â‹†"
REFERENCES,0.7300103842159917,"
âˆ¥ÏˆV Ï€âˆ—
P (s, a)âˆ¥2
(Î£Ï,V Ï€âˆ—
P
+Î»I)âˆ’1
1/2"
REFERENCES,0.731048805815161,"(Jensenâ€™s inequality) =
q"
REFERENCES,0.7320872274143302,"Tr(Î£dÏ€âˆ—
P â‹†,V Ï€âˆ—
P (Î»I + Î£Ï,V Ï€âˆ—
P )âˆ’1) â‰¤
q"
REFERENCES,0.7331256490134995,"CÏ€âˆ—,mix Tr(Î£Ï,V Ï€âˆ—
P (Î»I + Î£Ï,V Ï€âˆ—
P )âˆ’1)"
REFERENCES,0.7341640706126688,"(From Lemma 11) â‰¤
q"
REFERENCES,0.735202492211838,"CÏ€âˆ—,mixrank(Î£Ï,V Ï€âˆ—
P ) â‰¤
p"
REFERENCES,0.7362409138110073,"CÏ€âˆ—,mixd."
REFERENCES,0.7372793354101765,"By taking Î» s.t. Î»R2 â‰²(1 âˆ’Î³)âˆ’2Î¾, (30) is proved."
REFERENCES,0.7383177570093458,"For linear MDPs, from the fourth statement of Lemma 10, CÏ€âˆ—,mix â‰¤Â¯CÏ€âˆ—. Then, the statement is
concluded."
REFERENCES,0.7393561786085151,"E.5
PROOFS FOR KNRS"
REFERENCES,0.7403946002076843,"Proof of Corollary 3.
We prove in a similar way as Theorem 1."
REFERENCES,0.7414330218068536,Published as a conference paper at ICLR 2022
REFERENCES,0.7424714434060229,"First Step
Recall Î¾ =
q"
REFERENCES,0.7435098650051921,"2Î»âˆ¥W â‹†âˆ¥2
2 + 8Î¶2  
dS ln(5) + ln(1/Î´) + Â¯In

,
Â¯In = ln (det(Î£n)/ det(Î»I)) ."
REFERENCES,0.7445482866043613,"Thus, from Lemma 8, with probability 1 âˆ’Î´, we can show W âˆ—âˆˆWD since


c
WMLE âˆ’W â‹†
(Î£n)1/2
2 â‰¤Î¾."
REFERENCES,0.7455867082035307,"Hereafter, we condition on this event."
REFERENCES,0.7466251298026999,"Second step
For any W âˆˆWD, with probability 1 âˆ’Î´, we have
(W âˆ’W â‹†) (Î£n)1/2
2 â‰¤


W âˆ’c
W

(Î£n)1/2
2 +


W âˆ—âˆ’c
W

(Î£n)1/2
2 â‰¤Î¾."
REFERENCES,0.7476635514018691,"Third step
Note P â‹†= P(W âˆ—). Then,"
REFERENCES,0.7487019730010385,"V Ï€âˆ—
P â‹†âˆ’V Ë†Ï€
P â‹†â‰¤V Ï€âˆ—
P â‹†âˆ’min
W âˆˆWD V Ï€âˆ—
P (W ) + min
W âˆˆWD V Ï€âˆ—
P (W ) âˆ’V Ë†Ï€
P â‹†"
REFERENCES,0.7497403946002077,"â‰¤V Ï€âˆ—
P â‹†âˆ’min
W âˆˆWD V Ï€âˆ—
P (W ) + min
W âˆˆWD V Ë†Ï€
P (W ) âˆ’V Ë†Ï€
P â‹†
(deï¬nition of Ë†Ï€)"
REFERENCES,0.7507788161993769,"â‰¤V Ï€âˆ—
P â‹†âˆ’min
W âˆˆWD V Ï€âˆ—
P (W ).
(Fist step, W âˆ—âˆˆWD)"
REFERENCES,0.7518172377985463,"Then, by setting W â€² = arg minW âˆˆMD V Ï€âˆ—
P (W ), we have"
REFERENCES,0.7528556593977155,"V Ï€âˆ—
P â‹†âˆ’V Ë†Ï€
P â‹†â‰¤(1 âˆ’Î³)âˆ’2E(s,a)âˆ¼dÏ€âˆ—
P â‹†[âˆ¥P â€²(s, a) âˆ’P â‹†(s, a)âˆ¥TV]"
REFERENCES,0.7538940809968847,â‰¤(1 âˆ’Î³)âˆ’2
REFERENCES,0.754932502596054,"Î¶
E(s,a)âˆ¼dÏ€âˆ—
P â‹†[âˆ¥(W â€² âˆ’W â‹†)Ï†(s, a)âˆ¥2]
(Lemma 9)"
REFERENCES,0.7559709241952233,â‰¤(1 âˆ’Î³)âˆ’2
REFERENCES,0.7570093457943925,"Î¶
E(s,a)âˆ¼dÏ€âˆ—
P â‹†"
REFERENCES,0.7580477673935618,"h(W â€² âˆ’W â‹†)(Î£n)1/2
2 âˆ¥Ï†(s, a)âˆ¥Î£âˆ’1
n"
REFERENCES,0.7590861889927311,"i
(CS inequality)"
REFERENCES,0.7601246105919003,â‰¤(1 âˆ’Î³)âˆ’2
REFERENCES,0.7611630321910696,"Î¶
Î¾E(s,a)âˆ¼dÏ€âˆ—
P â‹†[âˆ¥Ï†(s, a)âˆ¥Î£âˆ’1
n ]
(Second step)"
REFERENCES,0.7622014537902388,"From Chang et al. (2021, Theorem 20), with probability 1 âˆ’Î´, we have"
REFERENCES,0.7632398753894081,"Î¾ â‰¤c1
q"
REFERENCES,0.7642782969885774,"âˆ¥W âˆ—âˆ¥2 + dS min(rank(Î£Ï){rank(Î£Ï) + ln(c2/Î´)}, d) ln(1 + n)."
REFERENCES,0.7653167185877466,"In addition, from Chang et al. (2021, Theorem 21), with probability 1 âˆ’Î´, we also have"
REFERENCES,0.7663551401869159,"E(s,a)âˆ¼dÏ€âˆ—
P â‹†[âˆ¥Ï†(s, a)âˆ¥Î£âˆ’1
n ] â‰¤c1"
REFERENCES,0.7673935617860852,"r Â¯CÏ€âˆ—rank[Î£Ï]{rank[Î£Ï] + ln(c2/Î´)} n
."
REFERENCES,0.7684319833852544,"Finally, by combining all things, we have"
REFERENCES,0.7694704049844237,"V Ï€âˆ—
P â‹†âˆ’V Ë†Ï€
P â‹†â‰¤c1(1 âˆ’Î³)âˆ’2 min(d1/2, Â¯R)
p Â¯R r"
REFERENCES,0.770508826583593,dS Â¯CÏ€âˆ—ln(1 + n)
REFERENCES,0.7715472481827622,"n
, Â¯R = rank[Î£Ï]{rank[Î£Ï] + ln(c2/Î´)}."
REFERENCES,0.7725856697819314,"E.6
PROOFS FOR LOW-RANK MDPS"
REFERENCES,0.7736240913811008,"Proof of Theorem 2. Until the second step, we can perform the same analysis as Theorem 1. More
concretely, with probability 1 âˆ’Î´, we have P â‹†âˆˆMD and"
REFERENCES,0.77466251298027,"Es,aâˆ¼Ï[TV(P(Â· | s, a), P â‹†(Â· | s, a))2] â‰¤Î¾,
âˆ€P âˆˆMD, Î¾ := cln(|M|/Î´)"
REFERENCES,0.7757009345794392,"n
.
(32)"
REFERENCES,0.7767393561786086,"Hereafter, we condition on the above event."
REFERENCES,0.7777777777777778,Published as a conference paper at ICLR 2022
REFERENCES,0.778816199376947,"Letting f(s, a) = TV(P(Â· | s, a), P â‹†(Â· | s, a)), we use Lemma 4. Then,"
REFERENCES,0.7798546209761164,"E(s,a)âˆ¼dÏ€
P â‹†[f(s, a)] â‰¤E(s,a)âˆ¼dÏ€
P â‹†[âˆ¥Ï†â‹†(s, a)âˆ¥Î£âˆ’1
Ï,Ï†â‹†]
q"
REFERENCES,0.7808930425752856,"nÎ³Ï‰Ï€EÏ[f 2(s, a)] + 4Î³2Î»d +
q"
REFERENCES,0.7819314641744548,"(1 âˆ’Î³)Ï‰Ï€EÏ[f 2(s, a)]"
REFERENCES,0.7829698857736241,"where Î£ = nEÏ[Ï†â‹†Ï†â‹†âŠ¤] + Î»I. We consider how to bound E(s,a)âˆ¼dÏ€
P â‹†[âˆ¥Ï†â‹†(s, a)âˆ¥Î£âˆ’1
Ï,Ï†â‹†]. This is
upper-bounded by"
REFERENCES,0.7840083073727934,"E(s,a)âˆ¼dÏ€
P â‹†[âˆ¥Ï†â‹†(s, a)âˆ¥Î£âˆ’1
Ï,Ï†â‹†] â‰¤
q"
REFERENCES,0.7850467289719626,"tr(E(s,a)âˆ¼dÏ€
P â‹†[Ï†â‹†Ï†â‹†âŠ¤]Î£âˆ’1
Ï,Ï†â‹†) â‰¤
q"
REFERENCES,0.7860851505711319,"Â¯CÏ€âˆ—,Ï†â‹†tr(E(s,a)âˆ¼Ï[Ï†â‹†Ï†â‹†âŠ¤]Î£âˆ’1
Ï,Ï†â‹†)
(From Lemma 11) â‰¤
q"
REFERENCES,0.7871235721703012,"Â¯CÏ€âˆ—,Ï†â‹†rank(Î£Ï)/n."
REFERENCES,0.7881619937694704,"Here, in the last line, by letting the SVD of Î£Ï = EÏ[Ï†Ï†âŠ¤] be U ËœÎ£ÏU âŠ¤where ËœÎ£Ï is a dÃ—d diagonal
matrix and U is a d Ã— d orthogonal matrix , we use"
REFERENCES,0.7892004153686397,"tr

Î£ÏÎ£âˆ’1
Ï,Ï†â‹†

= tr(U ËœÎ£ÏU âŠ¤{nU ËœÎ£ÏU âŠ¤+ Î»I}âˆ’1) = tr(ËœÎ£ÏU âŠ¤{nU ËœÎ£ÏU âŠ¤+ Î»I}âˆ’1U)"
REFERENCES,0.7902388369678089,= tr(ËœÎ£ÏU âŠ¤{U{nËœÎ£Ï + Î»I}U âŠ¤}âˆ’1U)
REFERENCES,0.7912772585669782,= tr(ËœÎ£ÏU âŠ¤U{nËœÎ£Ï + Î»I}âˆ’1U âŠ¤U)
REFERENCES,0.7923156801661475,= tr(ËœÎ£Ï{nËœÎ£Ï + Î»I})âˆ’1 â‰¤rank(Î£Ï)/n.
REFERENCES,0.7933541017653167,"Hence, when P âˆˆMD, by setting Î» s.t. Î»d â‰²nÏ‰Ï€Î¾, we have"
REFERENCES,0.794392523364486,"E(s,a)âˆ¼dÏ€
P â‹†[f(s, a)] â‰¤ r"
REFERENCES,0.7954309449636553,"Î³ Â¯CÏ€âˆ—,Ï†â‹†rank(Î£Ï)Ï‰Ï€ ln(|M|/Î´) n
+ r"
REFERENCES,0.7964693665628245,"(1 âˆ’Î³)Ï‰Ï€ ln(|M|/Î´) n
."
REFERENCES,0.7975077881619937,We use (32) here.
REFERENCES,0.7985462097611631,"Finally,"
REFERENCES,0.7995846313603323,"V Ï€âˆ—
P â‹†âˆ’V Ë†Ï€
P â‹†"
REFERENCES,0.8006230529595015,"â‰¤V Ï€âˆ—
P â‹†âˆ’min
P âˆˆMD V Ï€âˆ—
P
(Recall the proof of the third step in the proof of Theorem 1)"
REFERENCES,0.8016614745586709,"â‰¤(1 âˆ’Î³)âˆ’2Es,aâˆ¼dÏ€â‹†
P â‹†TV(P â€²(s, a), P â‹†(Â· | s, a))
(P â€² = arg minP âˆˆMD V Ï€âˆ—
P ) â‰² s"
REFERENCES,0.8026998961578401,"Â¯CÏ€âˆ—,Ï†â‹†rank(Î£Ï)Ï‰Ï€âˆ—ln(|M|/Î´)"
REFERENCES,0.8037383177570093,"n(1 âˆ’Î³)4
."
REFERENCES,0.8047767393561787,"The following inequality is an important lemma to connect E(s,a)âˆ¼dÏ€
P â‹†{f(s, a)} with an elliptical
potential E(Ëœs,Ëœa)âˆ¼dÏ€
P â‹†âˆ¥Ï†â‹†(Ëœs, Ëœa)âˆ¥Î£âˆ’1
Ï,Ï†â‹†."
REFERENCES,0.8058151609553479,"Lemma 4 (One-step back inequality). Take any f âŠ‚S Ã— A â†’R s.t. âˆ¥fâˆ¥âˆâ‰¤B and 0 < Î» âˆˆR.
Letting Ï‰ = maxs,a(Ï€(a | s)/Ï€b(a | s)), for any policy Ï€, we have"
REFERENCES,0.8068535825545171,"|E(s,a)âˆ¼dÏ€
P â‹†{f(s, a)} | â‰¤E(Ëœs,Ëœa)âˆ¼dÏ€
P â‹†âˆ¥Ï†â‹†(Ëœs, Ëœa)âˆ¥Î£âˆ’1
q
nÏ‰Ï€Î³E(s,a)âˆ¼Ï [f 2(s, a)]
	
+ Î³2Î»dB2 +
q"
REFERENCES,0.8078920041536864,"(1 âˆ’Î³)Ï‰Ï€E(s,a)âˆ¼Ï [f 2(s, a)]."
REFERENCES,0.8089304257528557,"where Î£ = nE(s,a)âˆ¼Ï[Ï†â‹†(s, a)Ï†â‹†âŠ¤(s, a)] + Î»I."
REFERENCES,0.8099688473520249,"Proof of Lemma 4. First, we have an equality:"
REFERENCES,0.8110072689511942,"E(s,a)âˆ¼dÏ€
P â‹†{f(s, a)} = Î³E(Ëœs,Ëœa)âˆ¼dÏ€
P â‹†,sâˆ¼P â‹†(Ëœs,Ëœa) {f(s, a)} + (1 âˆ’Î³)Esâˆ¼d0,aâˆ¼Ï€(s0) {f(s, a)} .
(33)"
REFERENCES,0.8120456905503635,Published as a conference paper at ICLR 2022
REFERENCES,0.8130841121495327,The second term in (33) is upper-bounded by
REFERENCES,0.814122533748702,"Esâˆ¼d0,aâˆ¼Ï€(s0) {f(s, a)} â‰¤Esâˆ¼d0,aâˆ¼Ï€(s0)

f 2(s, a)
	
}1/2 =
q"
REFERENCES,0.8151609553478713,"Ï‰Ï€E(s,a)âˆ¼Ï [f 2(s, a)] /(1 âˆ’Î³)."
REFERENCES,0.8161993769470405,"Next we consider the ï¬rst term in (33). By CS inequality, we have"
REFERENCES,0.8172377985462098,"E(Ëœs,Ëœa)âˆ¼dÏ€
P â‹†,sâˆ¼P â‹†(Ëœs,Ëœa) {f(s, a)}
 =
E(Ëœs,Ëœa)âˆ¼dÏ€
P â‹†Ï†â‹†(Ëœs, Ëœa)âŠ¤
Z
Ë†Âµ(s)Ï€(a | s)f(s, a)d(s, a)"
REFERENCES,0.818276220145379,"â‰¤E(Ëœs,Ëœa)âˆ¼dÏ€
P â‹†âˆ¥Ï†â‹†(Ëœs, Ëœa)âˆ¥Î£âˆ’1
Ï,Ï†â‹†âˆ¥
Z
Ë†Âµ(s)Ï€(a | s)f(s, a)d(s, a)âˆ¥Î£Ï,Ï†â‹†. Then,"
REFERENCES,0.8193146417445483,"âˆ¥
Z
Ë†Âµ(s)Ï€(a | s)f(s, a)d(s, a)âˆ¥2
Î£Ï,Ï†â‹†"
REFERENCES,0.8203530633437176,"â‰¤
Z
Ë†Âµ(s)Ï€(a | s)f(s, a)d(s, a)
âŠ¤n
nE(s,a)âˆ¼Ï[Ï†â‹†Ï†â‹†âŠ¤] + Î»I
o Z
Ë†Âµ(s)Ï€(a | s)f(s, a)d(s, a)
"
REFERENCES,0.8213914849428868,"â‰¤n

E(Ëœs,Ëœa)âˆ¼Ï"
REFERENCES,0.822429906542056,"Z
Ë†Âµ(s)âŠ¤Ï†â‹†(Ëœs, Ëœa)Ï€(a | s)f(s, a)d(s, a)
2
+ B2Î»d"
REFERENCES,0.8234683281412254,"(Use the assumption âˆ¥f(s, a)âˆ¥âˆâ‰¤B and âˆ¥
R
Ë†Âµ(s)d(s)âˆ¥2 â‰¤
âˆš d)"
REFERENCES,0.8245067497403946,"= n

E(Ëœs,Ëœa)âˆ¼Ï,sâˆ¼P â‹†(Ëœs,Ëœa),aâˆ¼Ï€(s) [f(s, a)]
	2 + B2Î»d"
REFERENCES,0.8255451713395638,"â‰¤n

E(Ëœs,Ëœa)âˆ¼Ï,sâˆ¼P â‹†(Ëœs,Ëœa),aâˆ¼Ï€(s)

f 2(s, a)
	
+ B2Î»d.
(Jensen)"
REFERENCES,0.8265835929387332,"Finally, the the ï¬rst term in (33) is upper-bounded by"
REFERENCES,0.8276220145379024,"n

E(Ëœs,Ëœa)âˆ¼Ï,sâˆ¼P â‹†(Ëœs,Ëœa),aâˆ¼Ï€(s)

f 2(s, a)
	
+ Î»dB2"
REFERENCES,0.8286604361370716,"â‰¤nÏ‰Ï€

E(Ëœs,Ëœa)âˆ¼Ï,sâˆ¼P â‹†(Ëœs,Ëœa),aâˆ¼Ï€b(s)

f 2(s, a)
	
+ Î»dB2
(Importance sampling) â‰¤nÏ‰Ï€  1"
REFERENCES,0.829698857736241,"Î³ E(s,a)âˆ¼Ï

f 2(s, a)

+ Î»dB2.
(Deï¬nition of Ï)"
REFERENCES,0.8307372793354102,The ï¬nal statement is immediately concluded.
REFERENCES,0.8317757009345794,"E.7
PROOFS FOR FACTORED MDPS"
REFERENCES,0.8328141225337488,"Proof of Theorem 3. We denote the constrained set as MD: MD = ( P =
Y"
REFERENCES,0.833852544132918,"i
Pi | ED
h
TV( bPMLE,i(Â· | s[pai], a), Pi(Â· | s[pai], a))2i
â‰¤Î¾i, âˆ€i âˆˆ[1, Â· Â· Â· , d] ) ."
REFERENCES,0.8348909657320872,"Following the ï¬rst step in the proof of Corollary 1, with probability 1 âˆ’Î´, the product Q"
REFERENCES,0.8359293873312564,"i P â‹†
i is in
MD, i.e.,"
REFERENCES,0.8369678089304258,"Es,aâˆ¼D
h
TV( bPMLE,i(Â· | s[pai], a), P â‹†
i (Â· | s[pai], a))2i
â‰¤Î¾i, âˆ€i âˆˆ[1, Â· Â· Â· , d],
Î¾i =
q"
REFERENCES,0.838006230529595,"Li log(Lid/Î´) n
."
REFERENCES,0.8390446521287642,"Note d comes from the union bound. Besides, following the second step in the proof of Corollary 1,
for any P âˆˆMD, with probability 1 âˆ’Î´,"
REFERENCES,0.8400830737279336,"Es,aâˆ¼Ï
h
TV ( bPi(Â· | s[pai], a), P â‹†
i (Â· | s[pai], a))2i
â‰¤Î¾i, âˆ€i âˆˆ[1, Â· Â· Â· , d]."
REFERENCES,0.8411214953271028,Published as a conference paper at ICLR 2022
REFERENCES,0.842159916926272,"After conditioning on the above two events, then, for any P âˆˆMD and Ï€â‹†âˆˆÎ , we have"
REFERENCES,0.8431983385254413,"V Ï€âˆ—
P â‹†âˆ’V Ï€âˆ—
P
â‰¤(1 âˆ’Î³)âˆ’2E(s,a)âˆ¼dÏ€âˆ—
P â‹†[TV(P(Â· | s, a), P â‹†(Â· | s, a))]"
REFERENCES,0.8442367601246106,"(Simulation lemma, Lemma 5)"
REFERENCES,0.8452751817237798,"â‰¤(1 âˆ’Î³)âˆ’2E(s,a)âˆ¼dÏ€âˆ—
P â‹†[
X"
REFERENCES,0.8463136033229491,"i
TV(Pi(Â· | s[pai], a), P â‹†
i (Â· | s[pai], a))]"
REFERENCES,0.8473520249221184,â‰¤(1 âˆ’Î³)âˆ’2 X i
REFERENCES,0.8483904465212876,"v
u
u
tE(s,a)âˆ¼Ï"
REFERENCES,0.8494288681204569,"""dÏ€âˆ—
P â‹†(s[pai], a)
Ï(s[pai], a) 2#"
REFERENCES,0.8504672897196262,"E(s,a)âˆ¼Ï[TV(Pi(Â· | s[pai], a), P â‹†
i (Â· | s[pai], a))2]"
REFERENCES,0.8515057113187954,(CS inequality)
REFERENCES,0.8525441329179647,â‰¤(1 âˆ’Î³)âˆ’2 X i q
REFERENCES,0.8535825545171339,"Â¨CÏ€âˆ—,âˆE(s,a)âˆ¼Ï[TV(Pi(Â· | s, a), P â‹†
i (Â· | s, a))2] â‰¤(1 âˆ’Î³)âˆ’2 X i q"
REFERENCES,0.8546209761163032,"Â¨CÏ€âˆ—,âˆÎ¾i"
REFERENCES,0.8556593977154725,"â‰¤(1 âˆ’Î³)âˆ’2
s"
REFERENCES,0.8566978193146417,"d Â¨CÏ€âˆ—,âˆ
X"
REFERENCES,0.857736240913811,"i
Î¾i
(CS inequality)"
REFERENCES,0.8587746625129803,"â‰¤c(1 âˆ’Î³)âˆ’2
r"
REFERENCES,0.8598130841121495,"d Â¨CÏ€âˆ—,âˆ
L ln(Lnd/Î´) n
."
REFERENCES,0.8608515057113187,"Here, recall"
REFERENCES,0.8618899273104881,"Â¨CÏ€âˆ—,âˆ=
max
iâˆˆ[1,Â·Â·Â· ,d] E(s,a)âˆ¼Ï"
REFERENCES,0.8629283489096573,"""dÏ€âˆ—
P â‹†(s[pai], a)
Ï(s[pai], a) 2# ."
REFERENCES,0.8639667705088265,"Following the third step in the proof of Corollary 1, the statement is concluded."
REFERENCES,0.8650051921079959,"Next, we show that Â¨CÏ€âˆ—,âˆâ‰¤CÏ€âˆ—,P â‹†= maxs,a
dÏ€âˆ—
P â‹†(s,a)"
REFERENCES,0.8660436137071651,"Ï(s,a) ."
REFERENCES,0.8670820353063343,"Proposition 1 (Comparison of Lâˆ-density-ratio based concentrabiliity coefï¬cient between factored
MDPs and non-factored MDPs ). For any Ï€âˆ—, we have:
Â¨CÏ€âˆ—,âˆâ‰¤CÏ€âˆ—,âˆ.
(34)"
REFERENCES,0.8681204569055037,"Proof. From now on, for any i âˆˆ[1, Â· Â· Â· , d], by deï¬ning Sâ€²
i s.t. S = Si Ã— Sâ€²
i, we prove"
REFERENCES,0.8691588785046729,"max
siâˆˆSi,aâˆˆA
dÏ€âˆ—
P â‹†(si, a)
Ï(si, a)
â‰¤
max
sâˆˆSi,sâ€²
iâˆˆSi,aâˆˆA
dÏ€âˆ—
P â‹†(si, sâ€²
i, a)
Ï(si, sâ€²
i, a)
= CÏ€âˆ—,âˆ."
REFERENCES,0.8701973001038421,"Then, (34) is easily proved."
REFERENCES,0.8712357217030114,"First, for any si âˆˆSi, a âˆˆA, we have"
REFERENCES,0.8722741433021807,"max
sâ€²
i"
REFERENCES,0.8733125649013499,"dÏ€âˆ—
P â‹†(si, sâ€²
i, a)
Ï(si, sâ€²
i, a)
= max
sâ€²
i"
REFERENCES,0.8743509865005192,"dÏ€âˆ—
P â‹†(si, a)dÏ€âˆ—
P â‹†(sâ€²
i | si, a)
Ï(si, a)Ï(sâ€²
i | si, a)
= dÏ€âˆ—
P â‹†(si, a)
Ï(si, a)
max
sâ€²
i"
REFERENCES,0.8753894080996885,"dÏ€âˆ—
P â‹†(sâ€²
i | si, a)
Ï(sâ€²
i | si, a)
â‰¥dÏ€âˆ—
P â‹†(si, a)
Ï(si, a) ."
REFERENCES,0.8764278296988577,"(35)
Here, we use"
REFERENCES,0.877466251298027,"1 â‰¤max
sâ€²
i"
REFERENCES,0.8785046728971962,"dÏ€âˆ—
P â‹†(sâ€²
i | si, a)
Ï(sâ€²
i | si, a) ,"
REFERENCES,0.8795430944963655,"which is proved by the contradiction argument, that is, if 1 > maxsâ€²
i
dÏ€âˆ—
P â‹†(sâ€²
i|si,a)
Ï(sâ€²
i|si,a) , both Ï and dÏ€âˆ—
P â‹†
cannot be probability mass functions since we would get 1 =
X"
REFERENCES,0.8805815160955348,"sâ€²
i
dÏ€âˆ—
P â‹†(sâ€²
i | si, a) â‰¤max
sâ€²
i"
REFERENCES,0.881619937694704,"dÏ€âˆ—
P â‹†(sâ€²
i | si, a)
Ï(sâ€²
i | si, a)  X"
REFERENCES,0.8826583592938733,"sâ€²
i
Ï(sâ€²
i | si, a) <
X"
REFERENCES,0.8836967808930426,"sâ€²
i
Ï(sâ€²
i | si, a)."
REFERENCES,0.8847352024922118,"Then, by taking the maximum over si âˆˆSi, a âˆˆA for both sides on (35), we have"
REFERENCES,0.885773624091381,"max
si,a
dÏ€âˆ—
P â‹†(si, a)
Ï(si, a)
â‰¤max
si,sâ€²
i,a
dÏ€âˆ—
P â‹†(si, sâ€²
i, a)
Ï(s1, sâ€²
i, a) ."
REFERENCES,0.8868120456905504,Published as a conference paper at ICLR 2022
REFERENCES,0.8878504672897196,"F
AUXILIARY LEMMAS"
REFERENCES,0.8888888888888888,"Lemma 5 (Simulation Lemma). Consider any two transitions P and bP, and any policy Ï€ : S â†’
âˆ†(A). We have:"
REFERENCES,0.8899273104880582,"|V Ï€
P âˆ’V Ï€
b
P | â‰¤|(1 âˆ’Î³)âˆ’1Es,aâˆ¼dÏ€
P [Esâ€²âˆ¼P (s,a)[V Ï€
b
P (sâ€²)] âˆ’Esâ€²âˆ¼P (s,a)[V Ï€
b
P (sâ€²)]]|"
REFERENCES,0.8909657320872274,"â‰¤(1 âˆ’Î³)âˆ’2Es,aâˆ¼dÏ€
P
h
TV(P(Â·|s, a), bP(Â·|s, a))
i
."
REFERENCES,0.8920041536863966,"Proof. Such simulation lemma is standard in model-based RL literature and the derivation can be
found, for instance, in the proof of Lemma 10 from Sun et al. (2019)."
REFERENCES,0.893042575285566,"Lemma 6 (MLE guarantee). Given a set of models M = {P : S Ã— A â†’âˆ†(S)} with P â‹†âˆˆM,
and a dataset D = {si, ai, sâ€²
i}n
i=1 with si, ai âˆ¼Ï, and sâ€²
i âˆ¼P â‹†(si, ai), let bPMLE be"
REFERENCES,0.8940809968847352,"bPMLE = arg min
P âˆˆM n
X"
REFERENCES,0.8951194184839044,"i=1
âˆ’ln P(sâ€²
i|si, ai)."
REFERENCES,0.8961578400830738,"With probability at least 1 âˆ’Î´, we have:"
REFERENCES,0.897196261682243,"Es,aâˆ¼ÏTV( bPMLE(Â·|s, a), P â‹†(Â·|s, a))2 â‰²ln(|M|/Î´) n
."
REFERENCES,0.8982346832814122,"Proof. Refer to (Agarwal et al., 2020b, Section E)"
REFERENCES,0.8992731048805815,Lemma 7 (MLE guarantee for tabular models).
REFERENCES,0.9003115264797508,"ED
h
TV(P(Â·|s, a), bPMLE(Â·|s, a))2i
â‰¤|S|A|{|S| ln 2 + ln(2|S||A|/Î´)}"
N,0.90134994807892,"2n
."
N,0.9023883696780893,"Proof. From Chang et al. (2021, Lemma 12) , with probability 1 âˆ’Î´,"
N,0.9034267912772586,"TV(P(Â·|s, a), bPMLE(Â·|s, a))2 â‰¤|S| ln 2 + ln(2|S||A|/Î´)"
N,0.9044652128764278,"2N(s, a)
âˆ€(s, a) âˆˆS Ã— A,"
N,0.9055036344755971,"where N(s, a) is the number of visiting times for (s, a). Then,"
N,0.9065420560747663,"ED
h
TV(P(Â·|s, a), bPMLE(Â·|s, a))2i
â‰¤ED"
N,0.9075804776739356,|S| ln 2 + ln(2|S||A|/Î´)
N,0.9086188992731049,"2N(s, a)  â‰¤
X (s,a)"
N,0.9096573208722741,|S| ln 2 + ln(2|S||A|/Î´)
N,0.9106957424714434,2n 
N,0.9117341640706127,= |S|A|{|S| ln 2 + ln(2|S||A|/Î´)}
N,0.9127725856697819,"2n
."
N,0.9138110072689511,Lemma 8 (MLE guarantee for KNRs).
N,0.9148494288681205,"
c
WMLE âˆ’W â‹†
(Î£n)1/2
2 â‰¤Î²n."
N,0.9158878504672897,"Proof. The proof directly follows the conï¬dence ball construction and proof from (Kakade et al.,
2020)."
N,0.9169262720664589,"Lemma 9 (â„“1 Distance between two Gaussians). Consider two Gaussian distributions P1 :=
N(Âµ1, Î¶2I) and P2 := N(Âµ2, Î¶2I). We have:"
N,0.9179646936656283,"TV(P1, P2) â‰¤1"
N,0.9190031152647975,Î¶ âˆ¥Âµ1 âˆ’Âµ2âˆ¥2 .
N,0.9200415368639667,Published as a conference paper at ICLR 2022
N,0.9210799584631361,"Proof. This lemma is proved by Pinskerâ€™s inequality and the closed-form of the KL divergence
between P1 and P2. Refer to (Kakade et al., 2020)."
N,0.9221183800623053,"Lemma 10 (Property of linear mixture MDPs). Let P(Î¸) = Î¸âŠ¤Ïˆ(s, a, sâ€²). Suppose P(Î¸) âˆˆS Ã—
A â†’âˆ†(S). For any function V âˆˆS â†’[0, 1], letting ÏˆV (s, a) =
R
Ïˆ(s, a, sâ€²)V (sâ€²)d(sâ€²), we
suppose âˆ¥ÏˆV (s, a)âˆ¥2 â‰¤1. The following theorems hold:"
N,0.9231568016614745,"1. For any (s, a, sâ€²), we have |P(Î¸)(s, a, sâ€²) âˆ’P(Î¸â€²)(s, a, sâ€²)| â‰¤âˆ¥Î¸ âˆ’Î¸â€²âˆ¥2."
N,0.9241952232606438,"2. For any (s, a), we have TV(P(Î¸)(s, a, Â·), P(Î¸â€²)(s, a, Â·)) â‰¤âˆ¥Î¸ âˆ’Î¸â€²âˆ¥2. Besides, for any
V : S â†’[0, 1], we have"
N,0.9252336448598131,"|(Î¸ âˆ’Î¸â€²)ÏˆV (s, a)| â‰¤TV(P(Î¸)(s, a, Â·), P(Î¸â€²)(s, a, Â·)). 3."
N,0.9262720664589823,"Câ€ 
Ï€â‹†,P â‹†= sup
x"
N,0.9273104880581516,"xâŠ¤E(s,a)âˆ¼dÏ€âˆ—
P â‹†[ÏˆV(s,a,x)(s, a)ÏˆâŠ¤
V(s,a,x)(s, a)]x"
N,0.9283489096573209,"xâŠ¤E(s,a)âˆ¼Ï[ÏˆV(s,a,x)(s, a)ÏˆâŠ¤
V(s,a,x)(s, a)]x ,"
N,0.9293873312564901,"V(s,a,x) = arg max
V :Sâ†’[0,1]"
N,0.9304257528556594,"xâŠ¤
Z
Ï†(s, a, sâ€²)V (sâ€²)d(sâ€²)
 ."
N,0.9314641744548287,"4. In linear MDPs (i.e., Ïˆ(s, a, sâ€²) = Ï†(s, a) âŠ—Âµ(sâ€²)), we have"
N,0.9325025960539979,"sup
V âˆˆ{Sâ†’[0,1]}
sup
x"
N,0.9335410176531672,"xâŠ¤E(s,a)âˆ¼dÏ€âˆ—
P â‹†[ÏˆV (s, a)ÏˆâŠ¤
V (s, a)]x"
N,0.9345794392523364,"xâŠ¤E(s,a)âˆ¼Ï[ÏˆV (s, a)ÏˆâŠ¤
V (s, a)]x
= sup
x"
N,0.9356178608515057,"xâŠ¤EdÏ€âˆ—
P â‹†[Ï†(s, a)Ï†(s, a)âŠ¤]x"
N,0.936656282450675,"xâŠ¤EÏ[Ï†(s, a)Ï†(s, a)âŠ¤]x ."
N,0.9376947040498442,Proof. We prove the ï¬rst statement. This is proved by
N,0.9387331256490135,"|P(Î¸) âˆ’P(Î¸â€²)| = |(Î¸ âˆ’Î¸â€²)Ïˆ(s, a, sâ€²)| â‰¤âˆ¥Î¸ âˆ’Î¸â€²âˆ¥2âˆ¥Ïˆ(s, a, sâ€²)âˆ¥2 â‰¤âˆ¥Î¸ âˆ’Î¸â€²âˆ¥2,"
N,0.9397715472481828,"Here, we use âˆ¥Ïˆ(s, a, sâ€²)âˆ¥2 â‰¤1 which is proved by the assumption by setting V (s) = I(sâ€² = s)
for any sâ€²."
N,0.940809968847352,"Next, we prove the second statement. For ï¬xed Î¸ âˆˆRd and (s, a) âˆˆS Ã— A, we have"
N,0.9418483904465212,"TV(P(Î¸)(s, a, Â·), P(Î¸â‹†)(s, a, Â·)) =
sup
V :Sâ†’[0,1]
|
Z
(Î¸ âˆ’Î¸â‹†)âŠ¤Ïˆ(s, a, sâ€²)V (sâ€²)d(sâ€²)|"
N,0.9428868120456906,"=
sup
V :Sâ†’[0,1]
|(Î¸ âˆ’Î¸â‹†)âŠ¤
Z
Ïˆ(s, a, sâ€²)V (sâ€²)d(sâ€²)|"
N,0.9439252336448598,"= |(Î¸ âˆ’Î¸â‹†)âŠ¤
Z
Ïˆ(s, a, sâ€²)V(s,a,Î¸)(sâ€²)d(sâ€²)|"
N,0.944963655244029,"= |(Î¸ âˆ’Î¸â‹†)âŠ¤ÏˆV(s,a,Î¸)(s, a)|."
N,0.9460020768431984,"In the third line, we deï¬ne V (s, a, Î¸) = arg maxV :Sâ†’[0,1] |(Î¸ âˆ’Î¸â‹†)âŠ¤R
Ïˆ(s, a, sâ€²)V (sâ€²)d(sâ€²)|."
N,0.9470404984423676,"Then, from CS inequality,"
N,0.9480789200415368,"TV(P(Î¸)(s, a, Â·), P(Î¸â‹†)(s, a, Â·)) â‰¤âˆ¥(Î¸ âˆ’Î¸â‹†âˆ¥2âˆ¥ÏˆV(s,a,Î¸)(s, a)|âˆ¥2 â‰¤âˆ¥Î¸ âˆ’Î¸â‹†âˆ¥2."
N,0.9491173416407062,"We use the assumption âˆ¥ÏˆV(s,a,Î¸)(s, a)âˆ¥2 â‰¤1. This concludes the second statement. Besides, for
any V : S â†’[0, 1], we have"
N,0.9501557632398754,"|(Î¸ âˆ’Î¸â€²)ÏˆV (s, a)| â‰¤|(Î¸ âˆ’Î¸â‹†)âŠ¤ÏˆV(s,a,Î¸)(s, a)|"
N,0.9511941848390446,"â‰¤TV(P(Î¸)(s, a, Â·), P(Î¸â€²)(s, a, Â·))."
N,0.952232606438214,The third statement is immediately concluded by
N,0.9532710280373832,"E(s,a)âˆ¼dÏ€âˆ—
P â‹†[TV(P(Î¸)(s, a, Â·), P(Î¸â‹†)(s, a, Â·))2]"
N,0.9543094496365524,"E(s,a)âˆ¼Ï[TV(P(Î¸)(s, a, Â·), P(Î¸â‹†)(s, a, Â·))2]
=
E(s,a)âˆ¼dÏ€âˆ—
P â‹†[|(Î¸ âˆ’Î¸â‹†)âŠ¤ÏˆV(s,a,Î¸)(s, a)|2]"
N,0.9553478712357217,"E(s,a)âˆ¼Ï[|(Î¸ âˆ’Î¸â‹†)âŠ¤ÏˆV(s,a,Î¸)(s, a)|2] .
(36)"
N,0.956386292834891,Published as a conference paper at ICLR 2022
N,0.9574247144340602,"Finally, we prove the fourth statement. Suppose Ïˆ(s, a, sâ€²) = Ï†(s, a) âŠ—Âµ(sâ€²) (âŠ—denotes kronerker
product). Then, Ï†V (s, a, sâ€²) = Ï†(s, a) âŠ—
R
Âµ(sâ€²)V (sâ€²)d(sâ€²). Then, by deï¬ning a vector Âµ(V ) =
R
Âµ(sâ€²)V (sâ€²)d(sâ€²), we immediately have"
N,0.9584631360332295,"xâŠ¤E(s,a)âˆ¼dÏ€âˆ—
P â‹†[ÏˆV (s, a)ÏˆâŠ¤
V (s, a)]x"
N,0.9595015576323987,"xâŠ¤E(s,a)âˆ¼Ï[ÏˆV (s, a)ÏˆâŠ¤
V (s, a)]x
= sup
x"
N,0.960539979231568,"xâŠ¤E(s,a)âˆ¼dÏ€âˆ—
P â‹†[(Ï†(s, a) âŠ—Âµ(V ))(Ï†(s, a) âŠ—Âµ(V ))âŠ¤]x"
N,0.9615784008307373,"xâŠ¤E(s,a)âˆ¼Ï[(Ï†(s, a) âŠ—Âµ(V ))(Ï†(s, a) âŠ—Âµ(V ))âŠ¤]x . (37)"
N,0.9626168224299065,"Here, we have"
N,0.9636552440290758,"EÏ[(Ï†(s, a) âŠ—Âµ(V ))(Ï†(s, a) âŠ—Âµ(V ))âŠ¤] = EÏ[(Ï†(s, a) âŠ—Âµ(V ))(Ï†(s, a)âŠ¤âŠ—Âµ(V )âŠ¤)]"
N,0.9646936656282451,"= EÏ[(Ï†(s, a)Ï†(s, a)âŠ¤)] âŠ—(Âµ(V )Âµ(V )âŠ¤)."
N,0.9657320872274143,We notice
N,0.9667705088265836,"{EÏ[(Ï†(s, a)Ï†(s, a)âŠ¤)] âŠ—(Âµ(V )Âµ(V )âŠ¤)}1/2 = EÏ[Ï†(s, a)Ï†(s, a)âŠ¤]1/2 âŠ—(Âµ(V )Âµ(V )âŠ¤)1/2."
N,0.9678089304257529,"This is because the square root of a matrix is unique and we have (A1/2 âŠ—B1/2)(A1/2 âŠ—B1/2) =
AB for symmetric matrices A and B. Then, by denoting FÏ = EÏ[Ï†(s, a)Ï†(s, a)âŠ¤], FdÏ€
P â‹†=
EdÏ€
P â‹†[Ï†(s, a)Ï†(s, a)âŠ¤] and denoting the pseudo inverse of F as F +, we can see (37) is equal to"
N,0.9688473520249221,"{F 1/2
Ï
âŠ—(Âµ(V )Âµ(V )âŠ¤)1/2}+{FdÏ€
P â‹†âŠ—(Âµ(V )Âµ(V )âŠ¤)}{F 1/2
Ï
âŠ—(Âµ(V )Âµ(V )âŠ¤)1/2}+"
N,0.9698857736240913,"= {F âˆ’1/2
Ï
âŠ—(Âµ(V )Âµ(V )âŠ¤)âˆ’1/2}{FdÏ€
P â‹†âŠ—(Âµ(V )Âµ(V )âŠ¤)}{F âˆ’1/2
Ï
âŠ—(Âµ(V )Âµ(V )âŠ¤)âˆ’1/2}"
N,0.9709241952232607,"= {F âˆ’1/2
Ï
FdÏ€
P â‹†F âˆ’1/2
Ï
} âŠ—{(Âµ(V )Âµ(V )âŠ¤)âˆ’1/2(Âµ(V )Âµ(V )âŠ¤)(Âµ(V )Âµ(V )âŠ¤)âˆ’1/2}"
N,0.9719626168224299,"= {F âˆ’1/2
Ï
FdÏ€
P â‹†F âˆ’1/2
Ï
} âŠ—Ik (k = rank(Âµ(V )Âµ(V )âŠ¤))."
N,0.9730010384215991,"Here, Ik is a diagonal matrix s.t. k âˆˆN+ values in the diagonal entries are 1 and the rest of
values are 0. Then, the maximum singular value of {F âˆ’1/2
Ï
FdÏ€
P â‹†F âˆ’1/2
Ï
} âŠ—Ik is equal to the one of"
N,0.9740394600207685,"{F âˆ’1/2
Ï
FdÏ€
P â‹†F âˆ’1/2
Ï
}. This is equal to sup
x"
N,0.9750778816199377,"xâŠ¤FdÏ€
P â‹†x xâŠ¤FÏx"
N,0.9761163032191069,"Hence, the fourth statement is concluded."
N,0.9771547248182763,"Lemma 11 (Distribution shift lemma). Suppose A1, A2, A3 are semipositive deï¬nite matrices:"
N,0.9781931464174455,"Tr(A1A2) â‰¤Ïƒmax(Aâˆ’1/2
3
A1Aâˆ’1/2
3
) Tr(A3A2). Note"
N,0.9792315680166147,"Ïƒmax(Aâˆ’1/2
3
A1Aâˆ’1/2
3
) = sup
xâˆˆRd
xâŠ¤A1x
xâŠ¤A3x."
N,0.980269989615784,Proof.
N,0.9813084112149533,"Tr(A1A2) = Tr(A1/2
1
A2A1/2
1
) = Tr(A1/2
1
Aâˆ’1/2
3
A1/2
3
A2A1/2
3
Aâˆ’1/2
3
A1/2
1
)"
N,0.9823468328141225,"= Tr(Aâˆ’1/2
3
A1Aâˆ’1/2
3
A1/2
3
A2A1/2
3
)."
N,0.9833852544132918,"In addition, for any semipositive deï¬nite matrices A, B we have"
N,0.9844236760124611,"Tr(AB) = Tr(UÎ›U âŠ¤B) = Tr(Î›U âŠ¤BU) â‰¤Ïƒmax(Î›) Tr(U âŠ¤BU) = Ïƒmax(A) Tr(B),"
N,0.9854620976116303,where UÎ›U âŠ¤is the SVD decomoposition of A. This concludes that
N,0.9865005192107996,"Tr(A1A2) â‰¤Ïƒmax(Aâˆ’1/2
3
A1Aâˆ’1/2
3
) Tr(A3A2)."
N,0.9875389408099688,Published as a conference paper at ICLR 2022
N,0.9885773624091381,"The following lemma is useful to obtain the generalized result of Theorem 1. The proof is given in
(Wainwright, 2019, Theorem 3.27). We ï¬rst deï¬ne"
N,0.9896157840083074,"Z = sup
fâˆˆF
|{ED âˆ’EÏ}[f]"
N,0.9906542056074766,"Î£2 = sup
fâˆˆF
ED[{f(s, a) âˆ’EÏ[f(s, a)]}2], Ïƒ2 = sup
fâˆˆF
var[f(s, a)]."
N,0.9916926272066459,"Lemma 12 (Functional Bernsteinâ€™s inequality: Talagrand concentration inequality for empirical
process). Suppose âˆ¥fâˆ¥âˆâ‰¤B. With probability 1 âˆ’Î´,"
N,0.9927310488058152,"|Z âˆ’E[Z]| â‰¤Î£2
r"
N,0.9937694704049844,log(c/Î´)
N,0.9948078920041536,"n
+ B log(c/Î´) n
."
N,0.995846313603323,"As an immediate corollary,"
N,0.9968847352024922,|Z âˆ’E[Z]| â‰¤{Ïƒ2 + BE[Z]} r
N,0.9979231568016614,log(c/Î´)
N,0.9989615784008308,"n
+ B log(c/Î´) n
."
