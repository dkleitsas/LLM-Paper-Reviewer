Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0007507507507507507,"Recent empirical advances show that training deep models with large learning rate
often improves generalization performance. However, theoretical justiÔ¨Åcations on
the beneÔ¨Åts of large learning rate are highly limited, due to challenges in analysis. In
this paper, we consider using Gradient Descent (GD) with a large learning rate on a
homogeneous matrix factorization problem, i.e., minX,Y ‚à•A ‚àíXY ‚ä§‚à•2
F. We prove
a convergence theory for constant large learning rates well beyond 2/L, where L
is the largest eigenvalue of Hessian at the initialization. Moreover, we rigorously
establish an implicit bias of GD induced by such a large learning rate, termed
‚Äòbalancing‚Äô, meaning that magnitudes of X and Y at the limit of GD iterations
will be close even if their initialization is signiÔ¨Åcantly unbalanced. Numerical
experiments are provided to support our theory."
INTRODUCTION,0.0015015015015015015,"1
INTRODUCTION"
INTRODUCTION,0.0022522522522522522,"Training machine learning models such as deep neural networks involves optimizing highly nonconvex
functions. Empirical results indicate an intimate connection between training algorithms and the
performance of trained models (Le et al., 2011; Bottou et al., 2018; Zhang et al., 2021; Soydaner,
2020; Zhou et al., 2020). Especially for widely used Ô¨Årst-order training algorithms (e.g., GD and
SGD), the learning rate is of essential importance and has received extensive focus from researchers
(Smith, 2017; Jastrzebski et al., 2017; Smith, 2018; Gotmare et al., 2018; Liu et al., 2019; Li & Arora,
2019). A recent perspective is that large learning rates often lead to improved testing performance
compared to the counterpart trained with small learning rates (Smith & Topin, 2019; Yue et al., 2020).
Towards explaining the better performance, a common belief is that large learning rates encourage
the algorithm to search for Ô¨Çat minima, which often generalize better and are more robust than sharp
ones (Seong et al., 2018; Lewkowycz et al., 2020)."
INTRODUCTION,0.003003003003003003,"Despite abundant empirical observations, theoretical understandings of the beneÔ¨Åts of large learning
rate are still limited for non-convex functions, partly due to challenges in analysis. For example, the
convergence (of GD or SGD) under large learning rate is not guaranteed. Even for globally smooth
functions, very few general results exist if the learning rate exceeds certain threshold (Kong & Tao,
2020). Besides, popular regularity assumptions such as global smoothness for simpliÔ¨Åed analyses are
often absent in homogeneous models, including commonly used ReLU neural networks."
INTRODUCTION,0.0037537537537537537,This paper theoretically studies the beneÔ¨Åts of large learning rate in a matrix factorization problem
INTRODUCTION,0.0045045045045045045,"min
X,Y
1
2"
INTRODUCTION,0.005255255255255256,"A ‚àíXY ‚ä§2"
INTRODUCTION,0.006006006006006006,"F ,
where A ‚ààRn√ón, X, Y ‚ààRn√ód.
(1)"
INTRODUCTION,0.006756756756756757,"We consider Gradient Descent (GD) for solving (1): at the k-th iteration, we have"
INTRODUCTION,0.0075075075075075074,"Xk+1 = Xk + h(A ‚àíXkY ‚ä§
k )Yk
and
Yk+1 = Yk + h(A‚ä§‚àíYkX‚ä§
k )Xk,"
INTRODUCTION,0.008258258258258258,"where h is the learning rate. Despite its simple formula, problem (1) serves as an important foundation
of a variety of problems, including matrix sensing (Chen & Wainwright, 2015; Bhojanapalli et al.,
2016; Tu et al., 2016), matrix completion (Keshavan et al., 2010; Hardt, 2014), and linear neural
networks (Ji & Telgarsky, 2018; Gunasekar et al., 2018)."
INTRODUCTION,0.009009009009009009,Published as a conference paper at ICLR 2022
INTRODUCTION,0.00975975975975976,"Problem (1) possesses several intriguing properties. Firstly, the objective function is non-convex,
and critical points are either global minima or saddles (see e.g., Baldi & Hornik (1989); Li et al.
(2019b); Valavi et al. (2020a); Chen et al. (2018)). Secondly, problem (1) is homogeneous in X
and Y , meaning that rescaling X, Y to aX, a‚àí1Y for any a Ã∏= 0 will not change the objective‚Äôs
value. This property is shared by commonly used ReLU neural networks. A direct consequence of
homogeneity is that global minima of (1) are non-isolated and can be unbounded. The curvatures at
these global minima are highly dependent on the magnitudes of X, Y . When X, Y have comparable
magnitudes, the largest eigenvalue of Hessian is small, and this corresponds to a Ô¨Çat minimum; on
the contrary, unbalanced X and Y give a sharp minimum. Last but not the least, the homogeneity
impairs smoothness conditions of (1), rendering the gradient being not Lipschitz continuous unless
X, Y are bounded. See a formal discussion in Section 2."
INTRODUCTION,0.010510510510510511,"Existing approaches for solving (1) often uses explicit regularization (Ge et al., 2017; Tu et al., 2016;
Cabral et al., 2013; Li et al., 2019a), or inÔ¨Ånitesimal (or diminishing) learning rates for controlling
the magnitudes of X, Y (Du et al., 2018; Ye & Du, 2021). In this paper, we go beyond the scope of
aforementioned works, and analyze GD with a large learning rate for solving (1). In particular, we
allow the learning rate h to be as large as approximately 4/L (see more explanation in Section 2),
where L denotes the largest eigenvalue of Hessian at GD initialization. In connection to empirical
observations, we provide positive answers to the following two questions:"
INTRODUCTION,0.01126126126126126,"Does GD with large learning rate converge at least for some cases of (1)?
Does larger learning rate biases toward Ô¨Çatter minima (i.e., X, Y with comparable magnitudes)?"
INTRODUCTION,0.012012012012012012,"We theoretically show the convergence of GD with large learning rate for the two situations n =
1, d ‚ààN+ or d = 1, n ‚ààN+ with isotropic A. We also observe a, perhaps surprising, ‚Äúbalancing
effect‚Äù for general matrix factorization (i.e., any d, n, and A), meaning that when h is sufÔ¨Åciently
large, the difference between X and Y shrinks signiÔ¨Åcantly at the convergence of GD compared to
its initial, even if the initial point is close to an unbalanced global minimum. In fact, with a proper
large learning rate h, ‚à•Xk ‚àíYk‚à•2
F may decrease by an arbitrary factor at its limit. The following is a
simple example of our theory for n = 1 (i.e. scalar factorization), and more general results will be
presented later with a precise bound for h depending on the initial condition and A."
INTRODUCTION,0.012762762762762763,"Theorem 1.1 (Informal version of Thm.3.1 & 3.2). Given scalar A and initial condition X0, Y0 ‚àà
R1√ód chosen almost everywhere, with learning rate h ‚â≤4/L, GD converges to a global minimum
(X, Y ) satisfying ‚à•X‚à•2
F + ‚à•Y ‚à•2
F ‚â§
2
h. Consequently, its extent of balancing is quantiÔ¨Åed by
‚à•X ‚àíY ‚à•2
F ‚â§2"
INTRODUCTION,0.013513513513513514,h ‚àí2A.
INTRODUCTION,0.014264264264264264,"We remark that having a learning rate h ‚âà4/L is far beyond the commonly analyzed regime in
optimization. Even for globally L-smooth objective, traditional theory requires h < 2/L for GD
convergence and h = 1/L is optimal for convex functions (Boyd et al., 2004), not to mention that
our problem (1) is never globally L-smooth due to homogeneity. ModiÔ¨Åed equation provides a tool
for probing intermediate learning rates (see Hairer et al. (2006, Chapter 9) for a general review, and
Kong & Tao (2020, Appendix A) for the speciÔ¨Åc setup of GD), but the learning rate here is too large
for modiÔ¨Åed equation to work (see Appendix C). In fact, besides blowing up, GD with large learning
rate may have a zoology of limiting behaviors (see e.g., Appendix B for convergence to periodic
orbits under our setup, and Kong & Tao (2020) for convergence to chaotic attractors)."
INTRODUCTION,0.015015015015015015,"Our analyses (of convergence and balancing) leverage various mathematical tools, including a
proper partition of state space and its dynamical transition (speciÔ¨Åcally invented for this problem),
stability theory of discrete time dynamical systems (Alligood et al., 1996), and geometric measure
theory (Federer, 2014)."
INTRODUCTION,0.015765765765765764,"The rest of the paper is organized as: Section 2 provides the background of studying (1) and discusses
related works; Section 3 presents convergence and balancing results for scalar factorization problems;
Section 4 generalizes the theory to rank-1 matrix approximation; Section 5 studies problem (1) with
arbitrary A and its arbitrary-rank approximation; Section 6 summarizes the paper and discusses
broadly related topics and future directions."
INTRODUCTION,0.016516516516516516,Published as a conference paper at ICLR 2022
BACKGROUND AND RELATED WORK,0.017267267267267267,"2
BACKGROUND AND RELATED WORK"
BACKGROUND AND RELATED WORK,0.018018018018018018,Notations. ‚à•v‚à•is ‚Ñì2 norm of a column or row vector v. ‚à•M‚à•F is the Frobenius norm of a matrix M.
BACKGROUND AND RELATED WORK,0.01876876876876877,"Sharp and Ô¨Çat minima in (1)
We discuss the curvatures at global minima of (1). To ease the
presentation, consider simpliÔ¨Åed versions of (1) with either n = 1 or d = 1. In this case, X and
Y become vectors and we denote them as x, y, respectively. We show the following proposition
characterizing the spectrum of Hessian at a global minimum.
Proposition 2.1. When n = 1, d ‚ààN+ or d = 1, n ‚ààN+ in (1), the largest eigenvalue of Hessian
at a global minimum (x, y) is ‚à•x‚à•2 + ‚à•y‚à•2 and the smallest eigenvalue is 0."
BACKGROUND AND RELATED WORK,0.01951951951951952,"Homogenity implies global minimizers of (1) are not isolated, which is consistent with the 0 eigen-
value. On the other hand, if the largest eigenvalue ‚à•x‚à•2 + ‚à•y‚à•2 is large (or small), then the curvature
at such a global minimum is sharp (or Ô¨Çat), in the direction of the leading eigenvector. Meanwhile,
note this sharpness/Ô¨Çatness is an indication of the balancedness between magnitudes of x, y at a
global minimum. To see this, singular value decomposition (SVD) yields that at a global minimum,
(x, y) satisÔ¨Åes
xy‚ä§2
F = œÉ2
max(A). Therefore, large ‚à•x‚à•2 + ‚à•y‚à•2 is obtained when |‚à•x‚à•‚àí‚à•y‚à•| is
large, i.e., x and y magnitudes are unbalanced, and small ‚à•x‚à•2 + ‚à•y‚à•2 is obtained when balanced."
BACKGROUND AND RELATED WORK,0.02027027027027027,"Large learning rate
We study smoothness properties of (1) and demonstrate that our learning rate
is well beyond conventional optimization theory. We Ô¨Årst deÔ¨Åne the smoothness of a function.
DeÔ¨Ånition 2.2 (L-smooth). A function f ‚ààC1 deÔ¨Åned on RN is L-smooth if for all u1, u2 ‚ààRN,"
BACKGROUND AND RELATED WORK,0.021021021021021023,"‚à•‚àáf(u1) ‚àí‚àáf(u2)‚à•‚â§L‚à•u1 ‚àíu2‚à•.
(2)"
BACKGROUND AND RELATED WORK,0.02177177177177177,"If further f ‚ààC2, then ‚àá2f ‚™ØLI. Moreover, if we have (2) for u1, u2 ‚ààX ‚äÜRN, we call it locally
L-smooth."
BACKGROUND AND RELATED WORK,0.02252252252252252,"In traditional optimization (Nesterov, 2003; Polyak, 1987; Nesterov, 1983; Polyak, 1964; Beck &
Teboulle, 2009), most analyzed objective functions often satisfy (i) (some relaxed form of) convexity
or strong convexity, and (ii) L-smoothness. Choosing a step size h < 2/L guarantees the convergence
of GD to a minimum by the existing theory (reviewed in Appendix F.3). Our choice of learning rate
h ‚âà4/L (more precisely, 4/L0; see below) goes beyond the classical analyses."
BACKGROUND AND RELATED WORK,0.023273273273273273,"Besides, in our problem (1), the regularity is very different. Even simpliÔ¨Åed versions of (1), i.e., with
either n = 1 or d = 1, suffer from (i) non-convexity and (ii) unbounded eigenvalues of Hessian, i.e.,
no global smoothness (see Appendix F.2 for more details). As shown in Du et al. (2018); Ye & Du
(2021); Ma et al. (2021), decaying or inÔ¨Ånitesimal learning rate ensures that the GD trajectory stays in
a locally smooth region. However, the gap between the magnitudes of X, Y can only be maintained
in that case. We show, however, that larger learning rate can shrink this gap. More precisely, if initial
condition is well balanced, there is no need to use large learning rate; otherwise, we can use learning
rate as large as approximately 4/L, and within this range, larger h provides smaller gap between X
and Y at the limit (i.e. inÔ¨Ånitely many GD iterations)."
BACKGROUND AND RELATED WORK,0.024024024024024024,"Related work
Matrix factorization problems in various forms have been extensively studied in
the literature. The version of (1) is commonly known as the low-rank factorization, although here d
can arbitrary and we consider both d ‚â§rank(A) and d > rank(A) cases. Baldi & Hornik (1989);
Li et al. (2019b); Valavi et al. (2020b) provide landscape analysis of (1). Ge et al. (2017); Tu et al.
(2016) propose to penalize the Frobenius norm of
X‚ä§X ‚àíY ‚ä§Y
2
F to mitigate the homogeneity
and establish global convergence guarantees of GD for solving (1). Cabral et al. (2013); Li et al.
(2019a) instead penalize individual Frobenius norms of ‚à•X‚à•2
F + ‚à•Y ‚à•2
F. We remark that penalizing
‚à•X‚à•2
F + ‚à•Y ‚à•2
F is closely related to nuclear norm regularization, since the variational formula for
nuclear norm ‚à•Z‚à•2
‚àó= minZ=XY ‚ä§‚à•X‚à•2
F + ‚à•Y ‚à•2
F. More recently, Liu et al. (2021) consider using
injected noise as regularization to GD and establish a global convergence (see also Zhou et al. (2019);
Liu et al. (2022). More speciÔ¨Åcally, by perturbing the GD iterate (Xk, Yk) with Gaussian noise, GD
will converge to a Ô¨Çat global optimum. On the other hand, Du et al. (2018); Ye & Du (2021); Ma et al.
(2021) show that even without explicit regularization, when the learning rate of GD is inÔ¨Ånitesimal,"
BACKGROUND AND RELATED WORK,0.024774774774774775,Published as a conference paper at ICLR 2022
BACKGROUND AND RELATED WORK,0.025525525525525526,"i.e., GD approximating gradient Ô¨Çow, X and Y maintain the gap in their magnitudes. Such an effect
is more broadly recognized as implicit bias of learning algorithms (Neyshabur et al., 2014; Gunasekar
et al., 2018; Soudry et al., 2018; Li et al., 2020; 2021). Built upon this implicit bias, Du et al. (2018)
further prove that GD with diminishing learning rates converges to a bounded global minimum of (1),
and this conclusion is recently extended to the case of a constant small learning rate (Ye & Du, 2021).
Our work goes beyond the scopes of these milestones and considers matrix factorization with much
larger learning rates."
BACKGROUND AND RELATED WORK,0.026276276276276277,"Additional results exist that demonstrate large learning rate can improve performance in various
learning problems. Most of them involve non-constant learn rates. SpeciÔ¨Åcally, Li et al. (2019c)
consider a two-layer neural network setting, where using learning rate annealing (initially large,
followed by small ones) can improve classiÔ¨Åcation accuracy compared to training with small learning
rates. Nakkiran (2020) shows that the observation in Li et al. (2019c) even exists in convex problems.
Lewkowycz et al. (2020) study constant large learning rates, and demonstrate distinct algorithmic
behaviors of large and small learning rates, as well as empirically illustrate large learning rate yields
better testing performance on neural networks. Their analysis is built upon the neural tangent kernel
perspective (Jacot et al., 2018), with a focus on the kernel spectrum evolution under large learning
rate. Worth noting is, Kong & Tao (2020) also study constant large learning rates, and show that large
learning rate provides a mechanism for GD to escape local minima, alternative to noisy escapes due
to stochastic gradients."
OVERPARAMETERIZED SCALAR FACTORIZATION,0.02702702702702703,"3
OVERPARAMETERIZED SCALAR FACTORIZATION"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.027777777777777776,"In order to provide intuition before directly studying the most general problem, we begin with a
simple special case, namely factorizing a scalar by two vectors. It corresponds to (1) with n = 1 and
d ‚ààN+, and this overparameterized problem is written as"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.028528528528528527,"min
x,y‚ààR1√ód
1
2(¬µ ‚àíxy‚ä§)2,
(3)"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.02927927927927928,"where ¬µ is assumed without loss of generality to be a positive scalar. Problem (3) can be viewed as
univariate regression using a linear two-layer neural network with the quadratic loss, which is studied
in Lewkowycz et al. (2020) with atomic data distribution. Yet our analysis in the sequel can be used
to study arbitrary univariate data distributions; see details in Section 6."
OVERPARAMETERIZED SCALAR FACTORIZATION,0.03003003003003003,"Although simpliÔ¨Åed, problem (3) is still nonconvex and exhibits the same homogeneity as (1). The
convergence of its large learning rate GD optimization was previously not understood, let alone
balancing. Many results that we will obtain for (3) will remain true for more general problems."
OVERPARAMETERIZED SCALAR FACTORIZATION,0.03078078078078078,"We Ô¨Årst prove that GD converges despite of > 2/L learning rate and for almost all initial conditions:
Theorem 3.1 (Convergence). Given (x‚ä§
0 , y‚ä§
0 ) ‚àà(Rd√óRd)\B where B is some Lebesgue measure-0
set, when the learning rate h satisÔ¨Åes"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.03153153153153153,"h ‚â§min

4
‚à•x0‚à•2 + ‚à•y0‚à•2 + 4¬µ,
1
3¬µ 
,"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.03228228228228228,GD converges to a global minimum.
OVERPARAMETERIZED SCALAR FACTORIZATION,0.03303303303303303,"Theorem 3.1 says that choosing a constant learning rate depending on GD initialization guarantees
the convergence for almost every starting point in the whole space. This result is even stronger
than the already nontrivial convergence under small learning rate with high probability over random
initialization (Ye & Du, 2021). Furthermore, the upper bound on h is sufÔ¨Åciently large: on the one
hand, suppose GD initialization (x0, y0) is close to an unbalanced global minimum. By Proposi-
tion 2.1, we can check that the largest eigenvalue L(x0, y0) of Hessian ‚àá2f(x0, y0) is approximately
‚à•x0‚à•2 + ‚à•y0‚à•2. Consequently, our upper bound of h is almost 4/L, which is beyond 2/L (see
Section 2 for more details). On the other hand, we observe numerically that the 4/L upper bound
is actually very close to the stability limit of GD when initialized away from the origin (see more
details in Appendix E)."
OVERPARAMETERIZED SCALAR FACTORIZATION,0.033783783783783786,"The convergence in Theorem 3.1 has an interesting searching-to-converging transition as depicted in
Figure 1, where we observe two phases. In Phase 1, large learning rate drives GD to search for Ô¨Çat"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.03453453453453453,Published as a conference paper at ICLR 2022
OVERPARAMETERIZED SCALAR FACTORIZATION,0.03528528528528529,Figure 1: The dynamics of GD under different learning rate h
OVERPARAMETERIZED SCALAR FACTORIZATION,0.036036036036036036,"regions, escaping from the attraction of sharp minima. After some iterations, the algorithm enters the
vicinity of a global minimum with more balanced magnitudes in x, y. Then in Phase 2, GD converges
to the found balanced global minimum. We remark that the searching-to-converging transition also
appears in Lewkowycz et al. (2020). However, the algorithmic behaviors are not the same. In fact, in
our searching phase (phase 1), the objective function does not exhibit the blow-up phenomenon. In
our convergence phase (phase 2), the analysis relies on a detailed state space partition (see Line 192)
due to nonconvex nature of (3), while the analysis in Lewkowycz et al. (2020) is akin to monotone
convergence in a convex problem."
OVERPARAMETERIZED SCALAR FACTORIZATION,0.03678678678678678,"In comparison with the dynamics of small learning rate, we note that the searching phase (Phase 1) is
vital to the convergence analysis. Meanwhile, the searching phase induces a balancing effect of large
learning rate. The following theorem explicitly quantiÔ¨Åes the extent of balancing.
Theorem 3.2 (Balancing). Under the same initial condition and learning rate h as Theorem 3.1, GD
for (3) converges to a global minimizer (x, y) satisfying"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.03753753753753754,‚à•x‚à•2 + ‚à•y‚à•2 ‚â§2 h.
OVERPARAMETERIZED SCALAR FACTORIZATION,0.038288288288288286,"Consequently, its extent of balancing is quantiÔ¨Åed by"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.03903903903903904,‚à•x ‚àíy‚à•2 ‚â§2
OVERPARAMETERIZED SCALAR FACTORIZATION,0.03978978978978979,h ‚àí2¬µ.
OVERPARAMETERIZED SCALAR FACTORIZATION,0.04054054054054054,"One special case of Theorem 3.2 is the following theorem, which states that no matter how close to a
global minimum does GD start, if this minimum does not correspond to well-balanced norms, a large
learning rate will take the iteration to a more balanced limit. We also demonstrate a sharp shrinkage
in the distance between x and y.
Corollary 3.3 (From ‚Äòunbalanced‚Äô to ‚Äòbalanced‚Äô). For any Œ¥ ‚àà(0, ¬µ), let the GD initialization satisfy"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.04129129129129129,"(x0, y0) ‚àà

(u, v) : |uv‚ä§‚àí¬µ| < Œ¥, ‚à•u‚à•2 + ‚à•v‚à•2 > 8¬µ
	
\B,"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.042042042042042045,"where B is some Lebesgue measure-0 set. When the learning rate h satisÔ¨Åes h =
4
‚à•x0‚à•2+‚à•y0‚à•2+4¬µ,
the extent of balancing at the limiting point (x, y) of GD obeys"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.04279279279279279,‚à•x ‚àíy‚à•2 < 1
OVERPARAMETERIZED SCALAR FACTORIZATION,0.04354354354354354,2‚à•x0 ‚àíy0‚à•2 + 2¬µ.
OVERPARAMETERIZED SCALAR FACTORIZATION,0.044294294294294295,"Both Theorem 3.2 and Corollary 3.3 suggest that larger learning rate yields better balancing effect, as
‚à•x ‚àíy‚à•2 at the limit of GD may decrease a lot and is controlled by the learning rate. We remark that
the balancing effect is a consequence of large learning rate, as small learning rate can only maintain
the difference in magnitudes of x, y (Du et al., 2018)."
OVERPARAMETERIZED SCALAR FACTORIZATION,0.04504504504504504,"In addition, the actual balancing effect can be quite strong with ‚à•xk ‚àíyk‚à•2 decreasing to be almost 0
at its limit under a proper choice of large learning rate. Figure 2 illustrates an almost perfect balancing
case when h =
4
‚à•x0‚à•2+‚à•y0‚à•2+4 ‚âà0.0122 is chosen as the upper bound. The difference ‚à•xk ‚àíyk‚à•
decreases from approximately 17.9986 to 0.0154 at its limit. Additional experiments with various
learning rates and initializations can be found in Appendix A."
OVERPARAMETERIZED SCALAR FACTORIZATION,0.0457957957957958,"Technical Overview
We sketch the main ideas behind Theorem 3.1, which lead to the balancing
effect in Theorem 3.2. Full proof is deferred to Appendix G."
OVERPARAMETERIZED SCALAR FACTORIZATION,0.046546546546546545,Published as a conference paper at ICLR 2022
OVERPARAMETERIZED SCALAR FACTORIZATION,0.0472972972972973,"0
100
200
300
number of iterations 10-10 10-5 100 105"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.04804804804804805,"0
100
200
300
number of iterations 0 5 10 15 20"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.048798798798798795,"Figure 2: The objective function is (1 ‚àíxy‚ä§)2/2, where x‚ä§, y‚ä§‚ààR10. Highly unbalanced initial
condition is uniformly randomized, with the norms to be ‚à•x0‚à•= 18, ‚à•y0‚à•= 0.09."
OVERPARAMETERIZED SCALAR FACTORIZATION,0.04954954954954955,"The convergence is proved by handling Phase 1 and 2 separately (see Fig.1). In Phase 1, we prove that
‚à•xk‚à•2 +‚à•yk‚à•2 has a decreasing trend as GD searches for Ô¨Çat minimum. We show that ‚à•xk‚à•2 +‚à•yk‚à•2"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.0503003003003003,"may not be monotone, i.e., it either decreases every iteration or decreases every other iteration."
OVERPARAMETERIZED SCALAR FACTORIZATION,0.05105105105105105,"In Phase 2, we carefully partition the state space and show GD at each partition will eventually enter
a monotone convergence region. Note that the partition is based on detailed understanding of the
dynamics and is highly nontrivial. Attentive readers may refer to Appendix G for more details. The
combination of Phase 1 & 2 is brieÔ¨Çy summarized as a proof Ô¨Çow chart in Figure 3."
OVERPARAMETERIZED SCALAR FACTORIZATION,0.0518018018018018,"Figure 3: Proof overview of Theorem 3.1. At the k-th iteration, we denote (xk, yk) as the iterate and
sk is deÔ¨Åned as xk+1y‚ä§
k+1 ‚àí¬µ = sk(xky‚ä§
k ‚àí¬µ)."
OVERPARAMETERIZED SCALAR FACTORIZATION,0.052552552552552555,"4
RANK-1 APPROX. OF ISOTROPIC A (AN UNDER-PARAMETERIZED CASE)"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.0533033033033033,"Given insights from scalar factorization, we consider rank-1 factorization of an isotropic matrix A,
i.e., A = ¬µIn√ón with ¬µ > 0, d = 1, and n ‚ààN+. The corresponding optimization problem is"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.05405405405405406,"min
x,y‚ààRn√ó1
1
2"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.054804804804804805,"¬µIn√ón ‚àíxy‚ä§2"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.05555555555555555,"F .
(4)"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.05630630630630631,"Although similar at an uncareful glance, Problems (4) and (3) are rather different unless n = d = 1.
First of all, Problem (4) is under-parameterized for n > 1, while (3) is overparameterized. More
importantly, we‚Äôll show that, when (x, y) is a global minimum of (4), x, y must be aligned, i.e., x = ‚Ñìy
for some ‚Ñì> 0. In the scalar factorization problem, however, no such alignment is required. As a
result, the set of global minima of (4) is an n-dimensional submanifold embedded in a 2n-dimensional
space, while in the scalar factorization problem the set of global minimum is a (2d ‚àí1)-dimensional"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.057057057057057055,Published as a conference paper at ICLR 2022
OVERPARAMETERIZED SCALAR FACTORIZATION,0.05780780780780781,"submanifold ‚Äî one rank deÔ¨Åcient ‚Äî in a 2d-dimensional space. We expect the convergence in (4) is
more complicated than that in (3), since searching in (4) is demanding."
OVERPARAMETERIZED SCALAR FACTORIZATION,0.05855855855855856,"To prove the convergence of large learning rate GD for (4), our theory consists of two steps: (i) show
the convergence of the alignment between x and y (this is new); (ii) use that to prove the convergence
of the full iterates (i.e., x & y). Step (i) Ô¨Årst:
Theorem 4.1 (Alignment). Given (x0, y0) ‚àà(Rn √ó Rn)\B, where B is some Lebesgue measure-0
set, when learning rate h ‚â§min
n
4
‚à•x0‚à•2+‚à•y0‚à•2+4
‚àö"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.05930930930930931,"7¬µ,
1
2
‚àö 7¬µ"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.06006006006006006,"o
, the iterator (xk, yk) of GD at the"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.060810810810810814,"k-th iteration satisÔ¨Åes | cos(‚à†(xk, yk))| ‚Üí1 as k ‚Üí‚àû."
OVERPARAMETERIZED SCALAR FACTORIZATION,0.06156156156156156,"Proof sketch. A sufÔ¨Åcient condition for the convergence of | cos(‚à†(xk, yk))| is ‚à•xk‚à•2 ‚à•yk‚à•2 ‚àí
(x‚ä§
k yk)2 ‚Üí0. To ease the presentation, let Uk = x‚ä§
k yk, Vk = x‚ä§
k xk, and Wk = y‚ä§
k yk. By the GD
update and some algebraic manipulation, we derive"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.06231231231231231,"Vk+1Wk+1 ‚àíU 2
k+1 = rk ¬∑ (VkWk ‚àíU 2
k),"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.06306306306306306,"where rk = (1 ‚àíh(Vk + Wk) + h2(VkWk ‚àí¬µ2))2. When k is sufÔ¨Åciently large, we can show a
uniform upper bound on rk < 1‚àíc for some constant c > 0. In this way, we deduce that VkWk ‚àíU 2
k
will exponentially decay and converge to 0. More details are provided in Appendix H."
OVERPARAMETERIZED SCALAR FACTORIZATION,0.06381381381381382,"Theorem 4.1 indicates that GD iterations will converge to the neighbourhood of {(x, y) : x =
‚Ñìy, for some ‚Ñì‚ààR\{0}}. This helps establish the global convergence as stated in Step (ii).
Theorem 4.2 (Convergence). Under the same initial conditions and learning rate h as Theorem 4.1,
GD for (4) converges to a global minimum."
OVERPARAMETERIZED SCALAR FACTORIZATION,0.06456456456456457,"Similar to the over-parametrized scalar case, this convergence can also be split into two phases where
phase 1 motivates the balancing behavior with the decrease of ‚à•xk‚à•2 + ‚à•yk‚à•2, and phase 2 ensures
the convergence. The following balancing theorem is thus obtained.
Theorem 4.3 (Balancing). Under the same initial conditions and learning rate h as Theorem 4.1,
GD for (4) converges to a global minimizer that obeys"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.06531531531531531,"‚à•x‚à•2 + ‚à•y‚à•2 ‚â§2 h,"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.06606606606606606,and its extent of balancing is quantiÔ¨Åed by
OVERPARAMETERIZED SCALAR FACTORIZATION,0.06681681681681682,‚à•x ‚àíy‚à•2 ‚â§2
OVERPARAMETERIZED SCALAR FACTORIZATION,0.06756756756756757,h ‚àí2¬µ.
OVERPARAMETERIZED SCALAR FACTORIZATION,0.06831831831831832,"This conclusion is the same as the one in Section 3. A quantitatively similar corollary like Corol-
lary 3.3 can also be obtained from the above theorem, namely, if x0 and y0 start from an unbalanced
point near a minimum, the limit will be a more balanced one."
GENERAL MATRIX FACTORIZATION,0.06906906906906907,"5
GENERAL MATRIX FACTORIZATION"
GENERAL MATRIX FACTORIZATION,0.06981981981981981,"In this section, we consider problem (1) with an arbitrary matrix A ‚ààRn√ón. We replicate the
problem formulation here for convenience,"
GENERAL MATRIX FACTORIZATION,0.07057057057057058,"min
X,Y ‚ààRn√ód
1
2"
GENERAL MATRIX FACTORIZATION,0.07132132132132132,"A ‚àíXY ‚ä§2"
GENERAL MATRIX FACTORIZATION,0.07207207207207207,"F .
(5)"
GENERAL MATRIX FACTORIZATION,0.07282282282282282,"Note this is the most general case with n, d ‚ààN+ and any square matrix A. Due to this generalization,
we no longer utilize the convergence analysis and instead, establish the balancing theory via stability
analysis of GD as a discrete time dynamical system."
GENERAL MATRIX FACTORIZATION,0.07357357357357357,"Let ¬µ1 ‚â•¬µ2 ‚â•¬∑ ¬∑ ¬∑ ‚â•¬µn ‚â•0 be the singular values of A. Assume for technical convenience
‚à•A‚à•2
F = Pn
i=1 ¬µ2
i being independent of d and n. We denote the singular value decomposition of A
as A = UDV ‚ä§, where U, D, V ‚ààRn√ón, U, V are orthogonal matrices and D is diagonal. Then we
establish the following balancing effect."
GENERAL MATRIX FACTORIZATION,0.07432432432432433,Published as a conference paper at ICLR 2022
GENERAL MATRIX FACTORIZATION,0.07507507507507508,"Theorem 5.1. Given almost all the initial conditions, for any learning rate h such that GD for (5)
converges to a point (X, Y ), there exists c = c(n, d) > c0 with constant c0 > 0 independent of h, n,
and d, such that (X, Y ) satisÔ¨Åes"
GENERAL MATRIX FACTORIZATION,0.07582582582582582,"c(‚à•X‚à•2
F + ‚à•Y ‚à•2
F) < 2 h,"
GENERAL MATRIX FACTORIZATION,0.07657657657657657,"and the extent of balancing is quantiÔ¨Åed by
X ‚àí(UV ‚ä§)Y
2
F <
2
ch ‚àí2 Pmin{d,n}
i=1
¬µi, which means"
GENERAL MATRIX FACTORIZATION,0.07732732732732733,"‚à•X‚à•F ‚àí‚à•Y ‚à•F
2 < 2 ch ‚àí2"
GENERAL MATRIX FACTORIZATION,0.07807807807807808,"v
u
u
t"
GENERAL MATRIX FACTORIZATION,0.07882882882882883,"min{d,n}
X"
GENERAL MATRIX FACTORIZATION,0.07957957957957958,"i=1
¬µ2
i ."
GENERAL MATRIX FACTORIZATION,0.08033033033033032,"In particular, when d = 1, i.e., rank-1 factorization of an arbitrary A, the constant c equals 1."
GENERAL MATRIX FACTORIZATION,0.08108108108108109,"We observe that the extent of balancing can be quantiÔ¨Åed under some rotation of Y . This is necessary,
since for factorizing a general matrix A (which can be asymmetric), at a global minimum, X, Y may
only align after a rotation (which is however Ô¨Åxed by A, independent of initial or Ô¨Ånal conditions).
Figure 4 illustrates an example of the balancing effect under different learning rates. Evidently,
larger learning rate leads to a more balanced global minimizer. Additional experiments with various
dimensions, learning rates, and initializations can be found in Appendix A; a similar balancing effect
is also shown for additional problems including matrix sensing and matrix completion there."
GENERAL MATRIX FACTORIZATION,0.08183183183183183,"0
100
200
number of iterations 10-10 100"
GENERAL MATRIX FACTORIZATION,0.08258258258258258,"0
100
200
number of iterations 0 5 10 F F"
GENERAL MATRIX FACTORIZATION,0.08333333333333333,"0
100
200
number of iterations 10-10 100"
GENERAL MATRIX FACTORIZATION,0.08408408408408409,"0
100
200
number of iterations 0 5 10 F F"
GENERAL MATRIX FACTORIZATION,0.08483483483483484,"Figure 4: Balancing effect of general matrix factorization. We independently generate elements in
A ‚ààR6√ó6 from a Gaussian distribution. We choose X, Y ‚ààR6√ó100 and randomly pick a pair of
initial point (X0, Y0) with ‚à•X0‚à•F = 1 and ‚à•Y0‚à•F = 9."
GENERAL MATRIX FACTORIZATION,0.08558558558558559,"Different from previous sections, Theorem 5.1 builds on stability analysis by viewing GD as a
dynamical system in discrete time. More precisely, the proof of Theorem 5.1 (see Appendix I for
details) consists of two parts: (i) the establishment of an easier but equivalent problem via the rotation
of X and Y , (ii) stability analysis of the equivalent problem."
GENERAL MATRIX FACTORIZATION,0.08633633633633633,"For (i), by singular value decomposition (SVD), A = UDV ‚ä§, where U, D, V ‚ààRn√ón, U and V are
orthogonal matrices, and D is a non-negative diagonal matrix. Let Xk = URk, Yk = V Sk. Then

Xk+1 = Xk + h(A ‚àíXkY ‚ä§
k )Yk
Yk+1 = Yk + h(A ‚àíXkY ‚ä§
k )‚ä§Xk"
GENERAL MATRIX FACTORIZATION,0.08708708708708708,"‚áî

URk+1 = URk + h(UDV ‚ä§‚àíURkS‚ä§
k V ‚ä§)V Sk
V Sk+1 = V Sk + h(UDV ‚ä§‚àíURkS‚ä§
k V ‚ä§)‚ä§URk"
GENERAL MATRIX FACTORIZATION,0.08783783783783784,"‚áî

Rk+1 = Rk + h(D ‚àíRkS‚ä§
k )Sk
Sk+1 = Sk + h(D ‚àíRkS‚ä§
k )‚ä§Rk
."
GENERAL MATRIX FACTORIZATION,0.08858858858858859,"Therefore, GD for problem (5) is equivalent to GD for the following problem"
GENERAL MATRIX FACTORIZATION,0.08933933933933934,"min
R,S‚ààRn√ód
1
2"
GENERAL MATRIX FACTORIZATION,0.09009009009009009,"D ‚àíRS‚ä§2 F"
GENERAL MATRIX FACTORIZATION,0.09084084084084085,and it thus sufÔ¨Åces to work with diagonal non-negative A.
GENERAL MATRIX FACTORIZATION,0.0915915915915916,"For (ii), here is a brief description of the idea of stability analysis: consider each iteration of GD as a
mapping œà from uk ‚àº(Xk, YK) to uk+1 ‚àº(Xk+1, Yk+1), where matrices X and Y are Ô¨Çattened"
GENERAL MATRIX FACTORIZATION,0.09234234234234234,Published as a conference paper at ICLR 2022
GENERAL MATRIX FACTORIZATION,0.09309309309309309,"and concatenated into a vector so that œà is a closed map on vector space R2dn. GD iteration is thus a
discrete time dynamical system on state space R2dn given by"
GENERAL MATRIX FACTORIZATION,0.09384384384384384,"uk+1 = œà(uk) = uk ‚àíh‚àáf(uk),"
GENERAL MATRIX FACTORIZATION,0.0945945945945946,where f is the objective function f(uk) = 1
GENERAL MATRIX FACTORIZATION,0.09534534534534535,"2
A ‚àíXkY ‚ä§
k
2
F, and gradient returns a vector that
collects all component-wise partial derivatives."
GENERAL MATRIX FACTORIZATION,0.0960960960960961,"It‚Äôs easy to see that any stationary point of f, denoted by u‚àó, is a Ô¨Åxed point of œà, i.e., u‚àó= œà(u‚àó).
What Ô¨Åxed point will the iterations of œà converge to? For this, the following notions are helpful:
Proposition 5.2. Consider a Ô¨Åxed point u‚àóof œà. If all the eigenvalues of Jacobian matrix ‚àáœà(u‚àó)
are of complex modulus less than 1, it is a stable Ô¨Åxed point.
Proposition 5.3. Consider a Ô¨Åxed point u‚àóof œà. If at least one eigenvalue of Jacobian matrix
‚àáœà(u‚àó) is of complex modulus greater than 1, it is an unstable Ô¨Åxed point."
GENERAL MATRIX FACTORIZATION,0.09684684684684684,"Roughly put, the stable set of an unstable Ô¨Åxed point is of negligible size when compared to that
of a stable Ô¨Åxed point, and thus what GD converges to is a stable Ô¨Åxed point for almost all initial
conditions (Alligood et al., 1996). Thus, we investigate the stability of each global minimum of f
(each saddle of f is an unstable Ô¨Åxed point of œà and thus is irrelevant). By a detailed evaluation of
‚àáœà‚Äôs eigenvalues (Appendix I), we see that a global minimum (X, Y ) of f corresponds to a stable"
GENERAL MATRIX FACTORIZATION,0.09759759759759759,"Ô¨Åxed point of GD iteration if
1 ‚àích

‚à•X‚à•2
F + ‚à•Y ‚à•2
F
 < 1, i.e., it is balanced as in Thm. 5.1."
CONCLUSION AND DISCUSSION,0.09834834834834835,"6
CONCLUSION AND DISCUSSION"
CONCLUSION AND DISCUSSION,0.0990990990990991,"In this paper, we demonstrate an implicit regularization effect of large learning rate on the homo-
geneous matrix factorization problem solved by GD. More precisely, a phenomenon termed as
‚Äúbalancing‚Äù is theoretically illustrated, which says the difference between the two factors X and Y
may decrease signiÔ¨Åcantly at the limit of GD, and the extent of balancing can increase as learning
rate increases. In addition, we provide theoretical analysis of the convergence of GD to the global
minimum, and this is with large learning rate that can exceed the typical limit of 2/L, where L is the
largest eigenvalue of Hessian at GD initialization."
CONCLUSION AND DISCUSSION,0.09984984984984985,"For the matrix factorization problem analyzed here, large learning rate avoids bad regularities induced
by the homogeneity between X and Y . We feel it is possible that such balancing behavior can also be
seen in problems with similar homogeneous properties, for example, in tensor decomposition (Kolda
& Bader, 2009), matrix completion (Keshavan et al., 2010; Hardt, 2014), generalized phase retrieval
(Candes et al., 2015; Sun et al., 2018), and neural networks with homogeneous activation functions
(e.g., ReLU). Besides the balancing effect, the convergence analysis under large learning rate may be
transplanted to other non-convex problems and help discover more implicit regularization effects."
CONCLUSION AND DISCUSSION,0.1006006006006006,"In addition, factorization problems studied here are closely related to two-layer linear neural networks.
For example, one-dimensional regression via a two-layer linear neural network can be formulated as
the scalar factorization problem (3): Suppose we have a collection of data points (xi, yi) ‚ààR √ó R
for i = 1, . . . , n. We aim to train a linear neural network y = (u‚ä§v)x with u, v ‚ààRd for Ô¨Åtting the
data. We optimize u, v by minimizing the quadratic loss,"
CONCLUSION AND DISCUSSION,0.10135135135135136,"(u‚àó, v‚àó) ‚ààarg min
u,v
1
n n
X i=1"
CONCLUSION AND DISCUSSION,0.1021021021021021," 
yi ‚àí(u‚ä§v)xi
2 = arg min
u,v
1
n"
CONCLUSION AND DISCUSSION,0.10285285285285285,"Pn
i=1 xiyi
Pn
i=1 x2
i
‚àíu‚ä§v
2
.
(6)"
CONCLUSION AND DISCUSSION,0.1036036036036036,"As can be seen, taking ¬µ ="
CONCLUSION AND DISCUSSION,0.10435435435435435,"Pn
i=1 xiyi
Pn
i=1 x2
i
recovers (3). In this regard, our theory indicates that training
of u, v by GD with large learning rate automatically balances u, v, and the obtained minimum is Ô¨Çat.
This may provide some initial understanding of the improved performance brought by large learning
rates in practice. Note that (6) generalizes to arbitrary data distribution of training a two-layer linear
network with atomic data (i.e., x = 1 and y = 0) in Lewkowycz et al. (2020)."
CONCLUSION AND DISCUSSION,0.10510510510510511,"It is important to clarify, however, that there is a substantial gap between this demonstration and
extensions to general neural networks, including deep linear and nonlinear networks. Although we
suspect that large learning rate leads to similar balancing effect of weight matrices in the network,
rigorous theoretical analysis is left as a future direction."
CONCLUSION AND DISCUSSION,0.10585585585585586,Published as a conference paper at ICLR 2022
CONCLUSION AND DISCUSSION,0.1066066066066066,ACKNOWLEDGMENTS
CONCLUSION AND DISCUSSION,0.10735735735735735,"We thank anonymous reviewers and area chair for suggestions that improved the quality of this
paper. The authors are grateful for partial supports from NSF DMS-1847802 (YW and MT) and
ECCS-1936776 (MT)."
REFERENCES,0.10810810810810811,REFERENCES
REFERENCES,0.10885885885885886,"Kathleen T Alligood, Tim D Sauer, and James A Yorke. Chaos: an introduction to dynamical systems.
Springer, 1996."
REFERENCES,0.10960960960960961,"Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from
examples without local minima. Neural networks, 2(1):53‚Äì58, 1989."
REFERENCES,0.11036036036036036,"Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse
problems. SIAM journal on imaging sciences, 2(1):183‚Äì202, 2009."
REFERENCES,0.1111111111111111,"Srinadh Bhojanapalli, Anastasios Kyrillidis, and Sujay Sanghavi. Dropping convexity for faster
semi-deÔ¨Ånite optimization. In Conference on Learning Theory, pp. 530‚Äì582. PMLR, 2016."
REFERENCES,0.11186186186186187,"L¬¥eon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. Siam Review, 60(2):223‚Äì311, 2018."
REFERENCES,0.11261261261261261,"Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge
university press, 2004."
REFERENCES,0.11336336336336336,"Ricardo Cabral, Fernando De la Torre, JoÀúao P Costeira, and Alexandre Bernardino. Unifying nuclear
norm and bilinear factorization approaches for low-rank matrix decomposition. In Proceedings of
the IEEE International Conference on Computer Vision, pp. 2488‚Äì2495, 2013."
REFERENCES,0.11411411411411411,"Emmanuel J Candes, Xiaodong Li, and Mahdi Soltanolkotabi. Phase retrieval via wirtinger Ô¨Çow:
Theory and algorithms. IEEE Transactions on Information Theory, 61(4):1985‚Äì2007, 2015."
REFERENCES,0.11486486486486487,"Yudong Chen and Martin J Wainwright. Fast low-rank estimation by projected gradient descent:
General statistical and algorithmic guarantees. arXiv preprint arXiv:1509.03025, 2015."
REFERENCES,0.11561561561561562,"Zhehui Chen, Xingguo Li, Lin F Yang, Jarvis Haupt, and Tuo Zhao.
On landscape of la-
grangian functions and stochastic search for constrained nonconvex optimization. arXiv preprint
arXiv:1806.05151, 2018."
REFERENCES,0.11636636636636637,"Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous
models: Layers are automatically balanced. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems,
volume 31. Curran Associates, Inc., 2018."
REFERENCES,0.11711711711711711,"Herbert Federer. Geometric measure theory. Springer, 2014."
REFERENCES,0.11786786786786786,"Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A
uniÔ¨Åed geometric analysis. In International Conference on Machine Learning, pp. 1233‚Äì1242.
PMLR, 2017."
REFERENCES,0.11861861861861862,"Akhilesh Gotmare, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. A closer look
at deep learning heuristics: Learning rate restarts, warmup and distillation.
arXiv preprint
arXiv:1810.13243, 2018."
REFERENCES,0.11936936936936937,"Suriya Gunasekar, Blake Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nathan Srebro.
Implicit regularization in matrix factorization. In 2018 Information Theory and Applications
Workshop (ITA), pp. 1‚Äì10. IEEE, 2018."
REFERENCES,0.12012012012012012,"Ernst Hairer, Marlis Hochbruck, Arieh Iserles, and Christian Lubich. Geometric numerical integration.
Oberwolfach Reports, 3(1):805‚Äì882, 2006."
REFERENCES,0.12087087087087087,Published as a conference paper at ICLR 2022
REFERENCES,0.12162162162162163,"Moritz Hardt. Understanding alternating minimization for matrix completion. In 2014 IEEE 55th
Annual Symposium on Foundations of Computer Science, pp. 651‚Äì660. IEEE, 2014."
REFERENCES,0.12237237237237238,"Arthur Jacot, Franck Gabriel, and Cl¬¥ement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. arXiv preprint arXiv:1806.07572, 2018."
REFERENCES,0.12312312312312312,"Stanislaw Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio,
and Amos Storkey. Three factors inÔ¨Çuencing minima in sgd. arXiv preprint arXiv:1711.04623,
2017."
REFERENCES,0.12387387387387387,"Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. arXiv
preprint arXiv:1810.02032, 2018."
REFERENCES,0.12462462462462462,"Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from a few
entries. IEEE transactions on information theory, 56(6):2980‚Äì2998, 2010."
REFERENCES,0.12537537537537538,"Tamara G Kolda and Brett W Bader. Tensor decompositions and applications. SIAM review, 51(3):
455‚Äì500, 2009."
REFERENCES,0.12612612612612611,"Lingkai Kong and Molei Tao. Stochasticity of deterministic gradient descent: Large learning rate for
multiscale objective function. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin
(eds.), Advances in Neural Information Processing Systems, volume 33, pp. 2625‚Äì2638. Curran
Associates, Inc., 2020."
REFERENCES,0.12687687687687688,"Quoc V Le, Jiquan Ngiam, Adam Coates, Ahbik Lahiri, Bobby Prochnow, and Andrew Y Ng. On
optimization methods for deep learning. In ICML, 2011."
REFERENCES,0.12762762762762764,"Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari. The large
learning rate phase of deep learning: the catapult mechanism. 2020."
REFERENCES,0.12837837837837837,"Qiuwei Li, Zhihui Zhu, and Gongguo Tang. The non-convex geometry of low-rank matrix optimiza-
tion. Information and Inference: A Journal of the IMA, 8(1):51‚Äì96, 2019a."
REFERENCES,0.12912912912912913,"Xingguo Li, Junwei Lu, Raman Arora, Jarvis Haupt, Han Liu, Zhaoran Wang, and Tuo Zhao.
Symmetry, saddle points, and global optimization landscape of nonconvex matrix factorization.
IEEE Transactions on Information Theory, 65(6):3489‚Äì3514, 2019b."
REFERENCES,0.12987987987987987,"Yan Li, Ethan X.Fang, Huan Xu, and Tuo Zhao. Implicit bias of gradient descent based adversarial
training on separable data. In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=HkgTTh4FDH."
REFERENCES,0.13063063063063063,"Yan Li, Caleb Ju, Ethan X Fang, and Tuo Zhao. Implicit regularization of bregman proximal point
algorithm and mirror descent on separable data. arXiv preprint arXiv:2108.06808, 2021."
REFERENCES,0.1313813813813814,"Yuanzhi Li, Colin Wei, and Tengyu Ma. Towards explaining the regularization effect of initial large
learning rate in training neural networks. arXiv preprint arXiv:1907.04595, 2019c."
REFERENCES,0.13213213213213212,"Zhiyuan Li and Sanjeev Arora. An exponential learning rate schedule for deep learning. arXiv
preprint arXiv:1910.07454, 2019."
REFERENCES,0.13288288288288289,"Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei
Han. On the variance of the adaptive learning rate and beyond. arXiv preprint arXiv:1908.03265,
2019."
REFERENCES,0.13363363363363365,"Tianyi Liu, Yan Li, Song Wei, Enlu Zhou, and Tuo Zhao. Noisy gradient descent converges to Ô¨Çat
minima for nonconvex matrix factorization. 2021."
REFERENCES,0.13438438438438438,"Tianyi Liu, Yan Li, Enlu Zhou, and Tuo Zhao. Noise regularizes over-parameterized rank one matrix
recovery, provably. arXiv preprint arXiv:2202.03535, 2022."
REFERENCES,0.13513513513513514,"Cong Ma, Yuanxin Li, and Yuejie Chi. Beyond procrustes: Balancing-free gradient descent for
asymmetric low-rank matrix sensing. IEEE Transactions on Signal Processing, 69:867‚Äì877, 2021."
REFERENCES,0.13588588588588588,"Preetum Nakkiran. Learning rate annealing can provably help generalization, even for convex
problems. 2020."
REFERENCES,0.13663663663663664,Published as a conference paper at ICLR 2022
REFERENCES,0.1373873873873874,"Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2003."
REFERENCES,0.13813813813813813,"Yurii E Nesterov. A method for solving the convex programming problem with convergence rate o
(1/kÀÜ 2). In Dokl. akad. nauk Sssr, volume 269, pp. 543‚Äì547, 1983."
REFERENCES,0.1388888888888889,"Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the
role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014."
REFERENCES,0.13963963963963963,"Boris T Polyak. Some methods of speeding up the convergence of iteration methods. Ussr computa-
tional mathematics and mathematical physics, 4(5):1‚Äì17, 1964."
REFERENCES,0.1403903903903904,"Boris T Polyak. Introduction to optimization. optimization software. Inc., Publications Division,
New York, 1, 1987."
REFERENCES,0.14114114114114115,"Sihyeon Seong, Yegang Lee, Youngwook Kee, Dongyoon Han, and Junmo Kim. Towards Ô¨Çatter loss
surface via nonmonotonic learning rate scheduling. In UAI, pp. 1020‚Äì1030, 2018."
REFERENCES,0.14189189189189189,"Leslie N Smith. Cyclical learning rates for training neural networks. In 2017 IEEE winter conference
on applications of computer vision (WACV), pp. 464‚Äì472. IEEE, 2017."
REFERENCES,0.14264264264264265,"Leslie N Smith. A disciplined approach to neural network hyper-parameters: Part 1‚Äìlearning rate,
batch size, momentum, and weight decay. arXiv preprint arXiv:1803.09820, 2018."
REFERENCES,0.14339339339339338,"Leslie N Smith and Nicholay Topin. Super-convergence: Very fast training of neural networks using
large learning rates. In ArtiÔ¨Åcial Intelligence and Machine Learning for Multi-Domain Operations
Applications, volume 11006, pp. 1100612. International Society for Optics and Photonics, 2019."
REFERENCES,0.14414414414414414,"Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit
bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):
2822‚Äì2878, 2018."
REFERENCES,0.1448948948948949,"Derya Soydaner. A comparison of optimization algorithms for deep learning. International Journal
of Pattern Recognition and ArtiÔ¨Åcial Intelligence, 34(13):2052013, 2020."
REFERENCES,0.14564564564564564,"Ju Sun, Qing Qu, and John Wright. A geometric analysis of phase retrieval. Foundations of
Computational Mathematics, 18(5):1131‚Äì1198, 2018."
REFERENCES,0.1463963963963964,"Stephen Tu, Ross Boczar, Max Simchowitz, Mahdi Soltanolkotabi, and Ben Recht. Low-rank
solutions of linear matrix equations via procrustes Ô¨Çow. In International Conference on Machine
Learning, pp. 964‚Äì973. PMLR, 2016."
REFERENCES,0.14714714714714713,"Hossein Valavi, Sulin Liu, and Peter Ramadge. Revisiting the landscape of matrix factorization.
In Silvia Chiappa and Roberto Calandra (eds.), Proceedings of the Twenty Third International
Conference on ArtiÔ¨Åcial Intelligence and Statistics, volume 108 of Proceedings of Machine
Learning Research, pp. 1629‚Äì1638. PMLR, 26‚Äì28 Aug 2020a."
REFERENCES,0.1478978978978979,"Hossein Valavi, Sulin Liu, and Peter J Ramadge. The landscape of matrix factorization revisited.
arXiv preprint arXiv:2002.12795, 2020b."
REFERENCES,0.14864864864864866,"Tian Ye and Simon S Du. Global convergence of gradient descent for asymmetric low-rank matrix
factorization. arXiv preprint arXiv:2106.14289, 2021."
REFERENCES,0.1493993993993994,"Xubo Yue, Maher Nouiehed, and Raed Al Kontar. Salr: Sharpness-aware learning rates for improved
generalization. arXiv preprint arXiv:2011.05348, 2020."
REFERENCES,0.15015015015015015,"Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep
learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107‚Äì115,
2021."
REFERENCES,0.15090090090090091,"Mo Zhou, Tianyi Liu, Yan Li, Dachao Lin, Enlu Zhou, and Tuo Zhao. Toward understanding the
importance of noise in training neural networks. In International Conference on Machine Learning,
pp. 7594‚Äì7602. PMLR, 2019."
REFERENCES,0.15165165165165165,"Pan Zhou, Jiashi Feng, Chao Ma, Caiming Xiong, Steven Hoi, et al. Towards theoretically under-
standing why sgd generalizes better than adam in deep learning. arXiv preprint arXiv:2010.05627,
2020."
REFERENCES,0.1524024024024024,Published as a conference paper at ICLR 2022
REFERENCES,0.15315315315315314,"Supplementary Materials for ‚ÄúLarge Learning Rate Tames
Homogeneity: Convergence and Balancing Effect‚Äù"
REFERENCES,0.1539039039039039,"A
ADDITIONAL EXPERIMENTS"
REFERENCES,0.15465465465465467,"A.1
MORE RESULTS FOR MATRIX FACTORIZATION"
REFERENCES,0.1554054054054054,"In this section, we present more experiments with different choices of n, d, various initializations
and scalings, and a broader range of learning rates. All these experiments verify our claim on the
balancing effect of large learning rate, i.e., the shrinkage of the gap between the magnitudes of X
and Y exists for general matrix factorization problems, i.e., for any choice of n, d ‚ààN+."
REFERENCES,0.15615615615615616,"We Ô¨Årst provide examples of scalar factorization in Figure 5, 6, and 7 to numerically justify our
theory in Section 3. In the three Ô¨Ågures, the initial conditions randomly generated, respectively with
(‚à•x0‚à•, ‚à•y0‚à•) = (9, 1), (‚à•x0‚à•, ‚à•y0‚à•) = (19, 1), and (‚à•x0‚à•, ‚à•y0‚à•) = (99, 1); the learning rates are
chosen within the range of Theorem 3.1 from large to small as h0, 6"
REFERENCES,0.1569069069069069,"7h0, 5"
REFERENCES,0.15765765765765766,"7h0, 4"
REFERENCES,0.15840840840840842,"7h0, 3"
REFERENCES,0.15915915915915915,"7h0, 2"
REFERENCES,0.15990990990990991,"7h0 for the
1st-6th columns respectively where h0 = 4/(‚à•x0‚à•2 + ‚à•y0‚à•2 + 8). The learning rates of the left three
columns are larger than 2/L (L is the local Lipschitz constant of gradient near the initial condition),
where we can see the decrease in the gap between ‚à•xk‚à•and ‚à•yk‚à•and larger learning rate leads to
smaller gap; the right three correspond to h < 2/L where there are almost no changes in ‚à•xk‚à•and
‚à•yk‚à•as k increases. Moreover, the loss does not decrease monotonically at the beginning of the
iterations for all the h > 2/L cases while in the later iterations GD shows monotone convergence; for
the right three columns (h < 2/L cases), we can see monotone decrease of the loss. This validates
our two-phase pattern of convergence (see e.g. Figure 1 and Section 3 for detailed explanation)."
REFERENCES,0.16066066066066065,"0
50
100
0 2 4 6 8 10"
REFERENCES,0.1614114114114114,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.16216216216216217,"0
50
100
0 2 4 6 8 10"
REFERENCES,0.1629129129129129,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.16366366366366367,"0
50
100
0 2 4 6 8 10"
REFERENCES,0.16441441441441443,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.16516516516516516,"0
50
100
0 2 4 6 8 10"
REFERENCES,0.16591591591591592,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.16666666666666666,"0
50
100
0 2 4 6 8 10"
REFERENCES,0.16741741741741742,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.16816816816816818,"0
50
100
0 2 4 6 8 10"
REFERENCES,0.16891891891891891,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.16966966966966968,"Figure 5: Scalar factorization with ‚à•x0‚à•= 9, ‚à•y0‚à•= 1. The x‚àíaxis represents the number of
iterations k; the y‚àíaxis represents the value for the norm of xk and yk in the Ô¨Årst row, and the value
for loss in the second row; the learning rate h for each column is the same."
REFERENCES,0.1704204204204204,"Then we experiment with the general matrix factorization as a supplement of our theory in Sec-
tion 5 where we only rigorously prove the balancing effect given the convergence of GD. In the
following examples, we show that there indeed exist large learning rates that trigger the shrinkage
of the gap between ‚à•Xk‚à•F and ‚à•Yk‚à•F and at the same time guarantee the convergence of GD. Fig-
ure 8, 9, and 10 correspond to the over-parameterized version of matrix factorization problem (5)
with A ‚ààR6√ó6 (asymmetric, generated by i.i.d. Gaussian) and X, Y ‚ààR6√ó100. The initial con-
ditions are randomly generated with (‚à•X0‚à•F , ‚à•Y0‚à•F) = (9, 1), (‚à•X0‚à•F , ‚à•Y0‚à•F) = (19, 1), and"
REFERENCES,0.17117117117117117,Published as a conference paper at ICLR 2022
REFERENCES,0.17192192192192193,"0
50
100
0 4 8 12 16 20"
REFERENCES,0.17267267267267267,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.17342342342342343,"0
50
100
0 4 8 12 16 20"
REFERENCES,0.17417417417417416,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.17492492492492492,"0
50
100
0 4 8 12 16 20"
REFERENCES,0.17567567567567569,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.17642642642642642,"0
50
100
0 4 8 12 16 20"
REFERENCES,0.17717717717717718,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.17792792792792791,"0
50
100
0 4 8 12 16 20"
REFERENCES,0.17867867867867868,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.17942942942942944,"0
50
100
0 4 8 12 16 20"
REFERENCES,0.18018018018018017,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.18093093093093093,"Figure 6: Scalar factorization with ‚à•x0‚à•= 19, ‚à•y0‚à•= 1. The x‚àíaxis represents the number of
iterations k; the y‚àíaxis represents the value for the norm of xk and yk in the Ô¨Årst row, and the value
for loss in the second row; the learning rate h for each column is the same."
REFERENCES,0.1816816816816817,"0
50
100
0 20 40 60 80 100"
REFERENCES,0.18243243243243243,"0
50
100
10-15 10-10 10-5"
REFERENCES,0.1831831831831832,"100
105"
REFERENCES,0.18393393393393392,"0
50
100
0 20 40 60 80 100"
REFERENCES,0.18468468468468469,"0
50
100
10-15 10-10 10-5"
REFERENCES,0.18543543543543545,"100
105"
REFERENCES,0.18618618618618618,"0
50
100
0 20 40 60 80 100"
REFERENCES,0.18693693693693694,"0
50
100
10-15 10-10 10-5"
REFERENCES,0.18768768768768768,"100
105"
REFERENCES,0.18843843843843844,"0
50
100
0 20 40 60 80 100"
REFERENCES,0.1891891891891892,"0
50
100
10-15 10-10 10-5"
REFERENCES,0.18993993993993993,"100
105"
REFERENCES,0.1906906906906907,"0
50
100
0 20 40 60 80 100"
REFERENCES,0.19144144144144143,"0
50
100
10-15 10-10 10-5"
REFERENCES,0.1921921921921922,"100
105"
REFERENCES,0.19294294294294295,"0
50
100
0 20 40 60 80 100"
REFERENCES,0.19369369369369369,"0
50
100
10-15 10-10 10-5"
REFERENCES,0.19444444444444445,"100
105"
REFERENCES,0.19519519519519518,"Figure 7: Scalar factorization with ‚à•x0‚à•= 99, ‚à•y0‚à•= 1. The x‚àíaxis represents the number of
iterations k; the y‚àíaxis represents the value for the norm of xk and yk in the Ô¨Årst row, and the value
for loss in the second row; the learning rate h for each column is the same."
REFERENCES,0.19594594594594594,"(‚à•X0‚à•F , ‚à•Y0‚à•F) = (99, 1) respectively. Similarly, the learning rates are chosen from large to small
as h0, 6"
REFERENCES,0.1966966966966967,"7h0, 5"
REFERENCES,0.19744744744744744,"7h0, 4"
REFERENCES,0.1981981981981982,"7h0, 3"
REFERENCES,0.19894894894894896,"7h0, 2"
REFERENCES,0.1996996996996997,7h0 for the 1st-6th columns respectively where 6
REFERENCES,0.20045045045045046,"7h0 (the 2nd column) is
picked near the stability limit. We also similarly provide two examples of the under-parameterized
version in Figure 11 and 12. The two examples correspond to problem (5) with A ‚ààR100√ó100
(asymmetric, similarly generated as the previous one) and X, Y ‚ààR100√ó3. Here we use a shifted
loss which is to subtract the global minimum error."
REFERENCES,0.2012012012012012,"As is shown in Figure 8, 9, 10, 11, and 12, we can observe the similar phenomenon as the scalar
case, in the sense that (1) larger learning rate gives a smaller the gap between ‚à•Xk‚à•F and ‚à•Yk‚à•F in
the limit, except for the overly large h that causes GD to diverge; (2) when the learning rate becomes
sufÔ¨Åciently big, a two-phase pattern of convergence appears, which does not manifest in traditional
optimization analysis and thus indicates that the learning rate is already larger than that permitted by
traditional theory, and yet one still has convergence."
REFERENCES,0.20195195195195195,Published as a conference paper at ICLR 2022
REFERENCES,0.20270270270270271,"More precisely, the 2nd-4th columns are the large learning rate cases, where one observes a shrinkage
of the unbalancedness and non-monotonicity of the loss at the beginning, while the right two columns
corresponds to the small learning rates for which ‚à•Xk‚à•F and ‚à•Yk‚à•F barely change as k increases,
and the decrease of loss is monotone. These are all evidence of the consistency between the most
general case of matrix factorization in Section 5 and the special cases in Section 3 and 4."
REFERENCES,0.20345345345345345,"0
5
10
0 1 2 3"
REFERENCES,0.2042042042042042,"4
10198 F F"
REFERENCES,0.20495495495495494,"0
5
10
100 10200"
REFERENCES,0.2057057057057057,"0
50
100
0 2 4 6 8 10"
REFERENCES,0.20645645645645647,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.2072072072072072,"0
50
100
0 2 4 6 8 10"
REFERENCES,0.20795795795795796,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.2087087087087087,"0
50
100
0 2 4 6 8 10"
REFERENCES,0.20945945945945946,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.21021021021021022,"0
50
100
0 2 4 6 8 10"
REFERENCES,0.21096096096096095,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.21171171171171171,"0
50
100
0 2 4 6 8 10"
REFERENCES,0.21246246246246248,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.2132132132132132,"Figure 8: General over-parameterized matrix factorization with ‚à•X0‚à•F = 9 ‚à•Y0‚à•F = 1. The x‚àíaxis
represents the number of iterations k; the y‚àíaxis represents the value for the norm of Xk and Yk in
the Ô¨Årst row, and the value for loss in the second row; the learning rate h for each column is the same."
REFERENCES,0.21396396396396397,"0
5
10
0 5"
REFERENCES,0.2147147147147147,"10
10153 F F"
REFERENCES,0.21546546546546547,"0
5
10
100 10100 10200"
REFERENCES,0.21621621621621623,"0
50
100
0 4 8 12 16 20"
REFERENCES,0.21696696696696696,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.21771771771771772,"0
50
100
0 4 8 12 16 20"
REFERENCES,0.21846846846846846,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.21921921921921922,"0
50
100
0 4 8 12 16 20"
REFERENCES,0.21996996996996998,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.22072072072072071,"0
50
100
0 4 8 12 16 20"
REFERENCES,0.22147147147147148,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.2222222222222222,"0
50
100
0 4 8 12 16 20"
REFERENCES,0.22297297297297297,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.22372372372372373,"Figure 9: General over-parameterized matrix factorization with ‚à•X0‚à•F = 19 ‚à•Y0‚à•F = 1. The
x‚àíaxis represents the number of iterations k; the y‚àíaxis represents the value for the norm of Xk
and Yk in the Ô¨Årst row, and the value for loss in the second row; the learning rate h for each column
is the same."
REFERENCES,0.22447447447447447,"A.2
MATRIX SENSING AND MATRIX COMPLETION"
REFERENCES,0.22522522522522523,"As we demonstrated, the balancing effect is a nontrivial implicit bias created by large learning rate in
GD, and this was rigorously established for the problem of matrix factorization. Matrix factorization
already corresponds to a class of important problems as we consider arbitrary n and d; however, we
feel balancing is an effect even more general, and thus we now demonstrate it empirically on two
related additional problems, namely matrix sensing and matrix completion."
REFERENCES,0.22597597597597596,Published as a conference paper at ICLR 2022
REFERENCES,0.22672672672672672,"0
5
10
0 1 2 3 4 10244 F F"
REFERENCES,0.22747747747747749,"0
5
10
100 10100"
REFERENCES,0.22822822822822822,"0
50
100
0 20 40 60 80 100"
REFERENCES,0.22897897897897898,"0
1000 2000
10-15 10-10 10-5 100 105"
REFERENCES,0.22972972972972974,"0
50
100
0 20 40 60 80 100"
REFERENCES,0.23048048048048048,"0
1000 2000
10-15 10-10 10-5 100 105"
REFERENCES,0.23123123123123124,"0
50
100
0 20 40 60 80 100"
REFERENCES,0.23198198198198197,"0
1000 2000
10-15 10-10 10-5 100 105"
REFERENCES,0.23273273273273273,"0
50
100
0 20 40 60 80 100"
REFERENCES,0.2334834834834835,"0
1000 2000
10-15 10-10 10-5 100 105"
REFERENCES,0.23423423423423423,"0
50
100
0 20 40 60 80 100"
REFERENCES,0.234984984984985,"0
1000 2000
10-15 10-10 10-5 100 105"
REFERENCES,0.23573573573573572,"Figure 10: General over-parameterized matrix factorization with ‚à•X0‚à•F = 99 ‚à•Y0‚à•F = 1. The
x‚àíaxis represents the number of iterations k; the y‚àíaxis represents the value for the norm of Xk
and Yk in the Ô¨Årst row, and the value for loss in the second row; the learning rate h for each column
is the same. Note the x‚àíaxis range of the 1st row is shortened to better show the changes of ‚à•Xk‚à•F
and ‚à•Yk‚à•F at the beginning of the iterations."
REFERENCES,0.23648648648648649,"0
5
10
0 5 10"
REFERENCES,0.23723723723723725,"15
10138 F F"
REFERENCES,0.23798798798798798,"0
5
10
100 10100 10200"
REFERENCES,0.23873873873873874,"0
50
100
0 2 4 6 8 10 0
1
2 104 10-10 10-5 100 105"
REFERENCES,0.23948948948948948,"0
50
100
0 2 4 6 8 10 0
1
2 104 10-10 10-5 100 105"
REFERENCES,0.24024024024024024,"0
50
100
0 2 4 6 8 10 0
1
2 104 10-10 10-5 100 105"
REFERENCES,0.240990990990991,"0
50
100
0 2 4 6 8 10 0
1
2 104 10-10 10-5 100 105"
REFERENCES,0.24174174174174173,"0
50
100
0 2 4 6 8 10 0
1
2 104 10-10 10-5 100 105"
REFERENCES,0.2424924924924925,"Figure 11: General under-parameterized matrix factorization with ‚à•X0‚à•F = 9 ‚à•Y0‚à•F = 1. The
x‚àíaxis represents the number of iterations k; the y‚àíaxis represents the value for the norm of Xk
and Yk in the Ô¨Årst row, and the value for loss in the second row; the learning rate h for each column
is the same. Note the x‚àíaxis range of the 1st row is shortened to better show the changes of ‚à•Xk‚à•F
and ‚à•Yk‚à•F at the beginning of the iterations."
REFERENCES,0.24324324324324326,"In Figure 13, we consider matrix sensing corresponding to the problem"
REFERENCES,0.243993993993994,"min
X,Y ‚ààRn√ód
1
2m m
X"
REFERENCES,0.24474474474474475,"i=1
(bi ‚àí‚ü®Ai, XY ‚ä§‚ü©)2."
REFERENCES,0.24549549549549549,"Here Ai ‚ààR100√ó100 are generated from element-wise i.i.d. Gaussian; bi ‚ààR are generated
from uniform distribution [0, 1]; m = 10; X, Y ‚ààR100√ó6; ‚ü®U, V ‚ü©= tr(V ‚ä§U). The problem is
solved via GD. Experiments in the Ô¨Ågure correspond to learning rates chosen from large to small
as h0, 4"
REFERENCES,0.24624624624624625,"5h0, 3"
REFERENCES,0.246996996996997,"5h0, 2"
REFERENCES,0.24774774774774774,"5h0 where h0 (the 1st column) is picked near the stability limit. We can see from
the Ô¨Ågure that matrix sensing exhibits a similar balancing effect with matrix factorization that larger
learning rate leads to more balanced norms between X and Y ."
REFERENCES,0.2484984984984985,Published as a conference paper at ICLR 2022
REFERENCES,0.24924924924924924,"0
5
10
0 2 4 6"
REFERENCES,0.25,"8
10176 F F"
REFERENCES,0.25075075075075076,"0
5
10
100 10100 10200"
REFERENCES,0.2515015015015015,"0
50
100
0 4 8 12 16 20"
REFERENCES,0.25225225225225223,"0
1.5
3 105 10-10 10-5 100 105"
REFERENCES,0.253003003003003,"0
50
100
0 4 8 12 16 20"
REFERENCES,0.25375375375375375,"0
1.5
3 105 10-10 10-5 100 105"
REFERENCES,0.2545045045045045,"0
50
100
0 4 8 12 16 20"
REFERENCES,0.2552552552552553,"0
1.5
3 105 10-10 10-5 100 105"
REFERENCES,0.256006006006006,"0
50
100
0 4 8 12 16 20"
REFERENCES,0.25675675675675674,"0
1.5
3 105 10-10 10-5 100 105"
REFERENCES,0.2575075075075075,"0
50
100
0 4 8 12 16 20"
REFERENCES,0.25825825825825827,"0
1.5
3 105 10-10 10-5 100 105"
REFERENCES,0.25900900900900903,"Figure 12: General under-parameterized matrix factorization with ‚à•X0‚à•F = 19 ‚à•Y0‚à•F = 1. The
x‚àíaxis represents the number of iterations k; the y‚àíaxis represents the value for the norm of Xk
and Yk in the Ô¨Årst row, and the value for loss in the second row; the learning rate h for each column
is the same. Note the x‚àíaxis range of the 1st row is shortened to better show the changes of ‚à•Xk‚à•F
and ‚à•Yk‚à•F at the beginning of the iterations."
REFERENCES,0.25975975975975973,"In Figure 14, we consider matrix completion corresponding to the problem"
REFERENCES,0.2605105105105105,"min
X,Y ‚ààRn√ód
1
2"
REFERENCES,0.26126126126126126,"P‚Ñ¶(A ‚àíXY T )
2
F ,"
REFERENCES,0.262012012012012,"where A ‚ààR10√ó10 is a low-rank matrix with rank 2; X, Y ‚ààR10√ó2; P‚Ñ¶(U) = (Uij)(i,j)‚àà‚Ñ¶+
(0)(i,j)/‚àà‚Ñ¶with sparsity‚àí0.6 where sparsity=(number of non-zero elements)/(number of all elements).
The problem is solved via GD. In the above mentioned Ô¨Ågure, the learning rates are similarly chosen
as the above matrix sensing example. Likewise, the decrease of learning rate results in the increase in
the gap between the norms of X and Y ."
REFERENCES,0.2627627627627628,"Both the matrix sensing and matrix completion above hold the same homogeneity property between
X and Y . The ‚Äúbalancing effect‚Äù that we proved for matrix factorization is also observed when
and only when the learning rate h is large, in which case the norms of X and Y become close at
the convergence of GD (and yes, GD still converges even though h is large enough such that the
convergence is not monotone."
REFERENCES,0.2635135135135135,"We feel the techniques invented and employed in this paper can extend to these two cases, but that is
beyond the scope of this paper."
REFERENCES,0.26426426426426425,"B
GD CONVERGING TO PERIODIC ORBIT"
REFERENCES,0.265015015015015,"Consider the objective (1 ‚àíxy)2/2. Take step size h = 1.9. Then GD can converge to periodic orbits
with period 2, 3 and 4 respectively in Figure 15."
REFERENCES,0.26576576576576577,"C
MODIFIED EQUATION FAILS FOR LARGE LEARNING RATES"
REFERENCES,0.26651651651651653,"Consider the objective function f(x, y) = (1 ‚àíxy)2/2. Then the GD update is the following"
REFERENCES,0.2672672672672673,"xk+1 = xk + h(1 ‚àíxkyk)yk
yk+1 = yk + h(1 ‚àíykxk)xk
‚áí

xk+1
yk+1"
REFERENCES,0.268018018018018,"
=

xk
yk"
REFERENCES,0.26876876876876876,"
+ h(‚àí‚àáf(xk, yk)).
(7)"
REFERENCES,0.2695195195195195,Published as a conference paper at ICLR 2022
REFERENCES,0.2702702702702703,"0
50
100
0 5 10 15 20 25 F F"
REFERENCES,0.27102102102102105,"0
50
100
10-20 10-10 100"
REFERENCES,0.27177177177177175,"0
50
100
0 5 10 15 20 25"
REFERENCES,0.2725225225225225,"0
50
100
10-20 10-10 100"
REFERENCES,0.2732732732732733,"0
50
100
0 5 10 15 20 25"
REFERENCES,0.27402402402402404,"0
50
100
10-20 10-10 100"
REFERENCES,0.2747747747747748,"0
50
100
0 5 10 15 20 25"
REFERENCES,0.2755255255255255,"0
50
100
10-20 10-10 100"
REFERENCES,0.27627627627627627,"Figure 13: Matrix sensing. The x‚àíaxis represents the number of iterations k; the y‚àíaxis represents
the value for the norm of Xk and Yk in the Ô¨Årst row, and the value for loss in the second row; the
learning rate h for each column is the same."
REFERENCES,0.27702702702702703,"0
50
100
0 5 10 15 20 F F"
REFERENCES,0.2777777777777778,"0
5
10 105 10-5 100"
REFERENCES,0.27852852852852855,"0
50
100
0 5 10 15 20"
REFERENCES,0.27927927927927926,"0
5
10 105 10-5 100"
REFERENCES,0.28003003003003,"0
50
100
0 5 10 15 20"
REFERENCES,0.2807807807807808,"0
5
10 105 10-5 100"
REFERENCES,0.28153153153153154,"0
50
100
0 5 10 15 20"
REFERENCES,0.2822822822822823,"0
5
10 105 10-5 100"
REFERENCES,0.283033033033033,"Figure 14: Matrix completion. The x‚àíaxis represents the number of iterations k; the y‚àíaxis
represents the value for the norm of Xk and Yk in the Ô¨Årst row, and the value for loss in the second
row; the learning rate h for each column is the same. Note the x‚àíaxis range of the 1st row is
shortened to better show the changes of ‚à•Xk‚à•F and ‚à•Yk‚à•F at the beginning of the iterations."
REFERENCES,0.28378378378378377,"By backward error analysis (Hairer et al., 2006, Chapter 9), the modiÔ¨Åed equation can better
approximate GD than gradient Ô¨Çow and is deÔ¨Åned as follows"
REFERENCES,0.28453453453453453,"
Àôx
Àôy"
REFERENCES,0.2852852852852853,"
= ‚àí‚àáf(x, y) ‚àíh"
REFERENCES,0.28603603603603606,"2 ‚àá2f(x, y)‚àáf(x, y) + O(h2).
(8)"
REFERENCES,0.28678678678678676,"Figure 16 shows the trajectories of the 1st order modiÔ¨Åed equation of (8) and GD (7) with initial
condition x = 4, y = 10 and h = 0.026. The x-axis represents the time t and for GD, the time point
for kth step is kh. We compare the absolute values of x and y of both methods due to the symmetry
of the global minima xy = 1. As is shown in the Ô¨Ågure, even if GD almost converges to the most
balanced minimizer, the solutions of modiÔ¨Åed equation are still far away from each other, x ‚âà0.1 and
y ‚âà9. Actually, large learning rate h fall outside the convergence domain of the modiÔ¨Åed equation
which thus is not an appropriate tool for the analysis of norm balancing."
REFERENCES,0.2875375375375375,Published as a conference paper at ICLR 2022
REFERENCES,0.2882882882882883,"0
1
2
3
4
5 0 1 2 3 4 5 y"
REFERENCES,0.28903903903903905,"0
1
2
3
4
5
x 0 1 2 3 4 5"
REFERENCES,0.2897897897897898,"0
1
2
3
4
5 0 1 2 3 4 5"
REFERENCES,0.2905405405405405,"Figure 15: Three orbits of period 2, 3, and 4. The blue line are the orbits; the red line is a reference
line of the global minima xy = 1."
REFERENCES,0.2912912912912913,"0
2
4
6
8
10
12
Time t 0 2 4 6 8 10"
REFERENCES,0.29204204204204204,Absolute values of x and y
REFERENCES,0.2927927927927928,"modified equation x
modified equation y
GD x
GD y"
REFERENCES,0.29354354354354356,Figure 16: Trajectories: modiÔ¨Åed equation vs GD
REFERENCES,0.29429429429429427,"D
PROOF OF PROPOSITION 2.1"
REFERENCES,0.29504504504504503,Proof of Proposition 2.1. This proposition is a direct consequence of Theorem F.2 and I.3.
REFERENCES,0.2957957957957958,"E
OVER-PARAMETRIZED SCALAR DECOMPOSITION: STABILITY LIMIT"
REFERENCES,0.29654654654654655,"Consider the objective (1 ‚àíxy)2/2. Choose the initial condition to be x0 = 20, y0 = 0.07 and use
GD update. The upper bound of h in Theorem 3.1 is
4
x2
0+y2
0+4¬µ, where ¬µ = 1. In Figure 17, when"
REFERENCES,0.2972972972972973,"h =
4
x2
0+y2
0+4 (the left one), GD converges; however, when h is slightly larger than this bound, it
blows up. Hence our restriction for h is very close to the stability limit."
REFERENCES,0.2980480480480481,"F
OVERPARAMETRIZED OBJECTIVE: LARGE LEARNING RATE"
REFERENCES,0.2987987987987988,"In this section, we use column vector instead of row vector for sake of better understanding, i.e., our
objective function is (¬µ ‚àíx‚ä§y)2/2."
REFERENCES,0.29954954954954954,"F.1
EIGENVALUES OF HESSIAN"
REFERENCES,0.3003003003003003,"Lemma F.1 (Matrix determinant lemma). Suppose A is an invertible n √ó n matrix and u, v ‚ààRn
are column vectors. Then"
REFERENCES,0.30105105105105107,det(A + uv‚ä§) = (1 + v‚ä§A‚àí1u) det A.
REFERENCES,0.30180180180180183,Published as a conference paper at ICLR 2022
REFERENCES,0.30255255255255253,"0
500
1000
1500
2000
Number of iterations 10-20"
REFERENCES,0.3033033033033033,"100
loss h=0.0099009"
REFERENCES,0.30405405405405406,"0
5
10
15
20
Number of iterations 100 10100 10200"
REFERENCES,0.3048048048048048,loss h=0.01
REFERENCES,0.3055555555555556,Figure 17: The loss with slightly different h near its upper bound
REFERENCES,0.3063063063063063,"Theorem F.2. The eigenvalues of the Hessian of (¬µ ‚àíx‚ä§y)2/2 are ¬±(¬µ ‚àíx‚ä§y) repeated n ‚àí1
times and 1"
REFERENCES,0.30705705705705705,"2
 
‚à•x‚à•2 + ‚à•y‚à•2 ¬±
p"
REFERENCES,0.3078078078078078,"(‚à•x‚à•2 + ‚à•y‚à•2)2 + 4(¬µ ‚àíx‚ä§y)2 ‚àí8(¬µ ‚àíx‚ä§y)x‚ä§y

. Especially, at
x‚ä§y = ¬µ, the eigenvalues are ‚à•x‚à•2 + ‚à•y‚à•2 and 0 repeated 2n ‚àí1 times."
REFERENCES,0.30855855855855857,"Proof. Consider the objective f(x, y) = (¬µ ‚àíx‚ä§y)2/2. Its Hessian is the following"
REFERENCES,0.30930930930930933,"H =

yy‚ä§
(x‚ä§y ‚àí¬µ)In + yx‚ä§"
REFERENCES,0.31006006006006004,"(x‚ä§y ‚àí¬µ)In + xy‚ä§
xx‚ä§ "
REFERENCES,0.3108108108108108,"=

0
(x‚ä§y ‚àí¬µ)In
(x‚ä§y ‚àí¬µ)In
0"
REFERENCES,0.31156156156156156,"
+

y
x"
REFERENCES,0.3123123123123123," 
y
x ‚ä§
."
REFERENCES,0.3130630630630631,"Then to calculate the eigenvalues, we also need"
REFERENCES,0.3138138138138138,"ŒªI2n ‚àíH =

ŒªIn
(¬µ ‚àíx‚ä§y)In
(¬µ ‚àíx‚ä§y)In
ŒªIn"
REFERENCES,0.31456456456456455,"
‚àí

y
x"
REFERENCES,0.3153153153153153," 
y
x"
REFERENCES,0.3160660660660661,"‚ä§
‚àÜ= B ‚àí

y
x"
REFERENCES,0.31681681681681684," 
y
x ‚ä§
."
REFERENCES,0.31756756756756754,"By Lemma F.1, we have for invertible B"
REFERENCES,0.3183183183183183,"det

B ‚àí

y
x"
REFERENCES,0.31906906906906907," 
y
x"
REFERENCES,0.31981981981981983,"‚ä§
=

1 ‚àí

y
x"
REFERENCES,0.3205705705705706,"‚ä§
B‚àí1

y
x"
REFERENCES,0.3213213213213213," 
det B."
REFERENCES,0.32207207207207206,"Since (¬µ ‚àíx‚ä§y)In and ŒªIn commute, we have"
REFERENCES,0.3228228228228228,det B = det((Œª2 ‚àí(¬µ ‚àíx‚ä§y)2)In) = (Œª2 ‚àí(¬µ ‚àíx‚ä§y)2)n.
REFERENCES,0.3235735735735736,"By the formula for inversion of block matrix, we have for Œª2 ‚àí(¬µ ‚àíx‚ä§y)2 Ã∏= 0, B‚àí1 ="
REFERENCES,0.32432432432432434,"Œª
Œª2‚àí(¬µ‚àíx‚ä§y)2 In
‚àí
¬µ‚àíx‚ä§y
Œª2‚àí(¬µ‚àíx‚ä§y)2 In
‚àí
¬µ‚àíx‚ä§y
Œª2‚àí(¬µ‚àíx‚ä§y)2 In
Œª
Œª2‚àí(¬µ‚àíx‚ä§y)2 In ! ."
REFERENCES,0.32507507507507505,"Then combining all these and by the continuity of characteristic polynomial, we obtain the following
expression for all Œª"
REFERENCES,0.3258258258258258,det(ŒªI2n ‚àíH) = (Œª2 ‚àí(¬µ ‚àíx‚ä§y)2)n‚àí1(Œª2 ‚àíŒª(x‚ä§x + y‚ä§y) ‚àí(¬µ ‚àíx‚ä§y)2 + 2(¬µ ‚àíx‚ä§y)x‚ä§y).
REFERENCES,0.32657657657657657,"When x‚ä§y = ¬µ, it becomes"
REFERENCES,0.32732732732732733,det(ŒªI2n ‚àíH) = Œª2n‚àí1(Œª ‚àí(x‚ä§x + y‚ä§y)).
REFERENCES,0.3280780780780781,"F.2
LOCAL NON-CONVEXITY NEAR THE GLOBAL MINIMUM"
REFERENCES,0.32882882882882886,"Consider the region D = {Œ¥ > x‚ä§y‚àí¬µ > 0}, a small neighbourhood of the global minima ¬µ = x‚ä§y
for some Œ¥ > 0. Consider two points [x, y], [¬Øx, ¬Øy] ‚ààD with x = c¬Øx and y = ¬Øy/c for c > 0 a constant."
REFERENCES,0.32957957957957956,Published as a conference paper at ICLR 2022 Then
REFERENCES,0.3303303303303303,"f(x, y) + ‚ü®‚àáf(x, y), [¬Øx ‚àíx, ¬Øy ‚àíy]‚ü©= (¬µ ‚àíx‚ä§y)2/2 + (¬µ ‚àíx‚ä§y)
 
y‚ä§(x ‚àí¬Øx) + x‚ä§(y ‚àí¬Øy)
"
REFERENCES,0.3310810810810811,= (¬µ ‚àí¬Øx‚ä§¬Øy)2/2 + (2 ‚àíc ‚àí1/c)(¬µ ‚àí¬Øx‚ä§¬Øy)¬Øx‚ä§¬Øy
REFERENCES,0.33183183183183185,"‚â•(¬µ ‚àí¬Øx‚ä§¬Øy)2/2 = f(¬Øx, ¬Øy)."
REFERENCES,0.3325825825825826,This contradicts the deÔ¨Ånition of convexity. Therefore the objective is not locally convex.
REFERENCES,0.3333333333333333,"F.3
A REVIEW OF TRADITIONAL CONVERGENCE ANALYSIS OF GD UNDER L-SMOOTHNESS"
REFERENCES,0.3340840840840841,"For general function f, GD is deÔ¨Åned as follows"
REFERENCES,0.33483483483483484,"xk+1 = xk ‚àíh‚àáf(xk).
(9)"
REFERENCES,0.3355855855855856,"Theorem F.3. If f : RN ‚ÜíR is L‚àísmooth, then with h < 2"
REFERENCES,0.33633633633633636,"L, GD converges to a stationary point."
REFERENCES,0.33708708708708707,Proof. Let min f = f ‚àó.
REFERENCES,0.33783783783783783,"f(xk+1) ‚â§f(xk) + ‚ü®‚àáf(xk), xk+1 ‚àíxk‚ü©+ L"
REFERENCES,0.3385885885885886,2 ‚à•xk+1 ‚àíxk‚à•2
REFERENCES,0.33933933933933935,= f(xk) ‚àíh(1 ‚àíL
REFERENCES,0.3400900900900901,"2 h)‚à•‚àáf(xk)‚à•2. Then N
X"
REFERENCES,0.3408408408408408,"k=1
‚à•‚àáf(xk)‚à•2 ‚â§
1
h(1 ‚àíL"
REFERENCES,0.3415915915915916,2 h)(f(x0) ‚àíf(xN))
REFERENCES,0.34234234234234234,"‚â§
1
h(1 ‚àíL"
REFERENCES,0.3430930930930931,2 h)(f(x0) ‚àíf ‚àó).
REFERENCES,0.34384384384384387,"Therefore limk‚Üí‚àû‚à•‚àáf(xk)‚à•2 = 0, i.e., GD converges to a stationary point."
REFERENCES,0.34459459459459457,"G
PROOF OF THEOREM 3.1, THEOREM 3.2, AND COROLLARY 3.3"
REFERENCES,0.34534534534534533,"In this section, we use column vectors for x and y instead of row vectors for better understanding.
Then the objective function is 1"
REFERENCES,0.3460960960960961,2(¬µ ‚àíx‚ä§y)2.
REFERENCES,0.34684684684684686,"The following Theorem G.1 and G.2 are the main theorems of the over-parametrized scalar case.
Next, for sake of convenience, let u2
k = ‚à•xk‚à•2 + ‚à•yk‚à•2."
REFERENCES,0.3475975975975976,"Theorem G.1 (Main 1). Let h =
4
u2
0+4c¬µ. Assume c ‚â•1 and u2
0 > 8¬µ. Then GD converges to a"
REFERENCES,0.3483483483483483,"point in {(x, y) : ‚à•x‚à•2 + ‚à•y‚à•2 ‚â§2"
REFERENCES,0.3490990990990991,"h, x‚ä§y = ¬µ} except for a Lebesgue measure-0 set of (x0, y0)."
REFERENCES,0.34984984984984985,"Proof. By Lemma G.7 and Lemma G.11, the theorem holds."
REFERENCES,0.3506006006006006,"Theorem G.2 (Main 2). Let h =
4
8¬µ+4c¬µ =
1
(2+c)¬µ. Assume c ‚â•1 and u2
0 ‚â§8¬µ. Then GD
converges to a point in {(x, y) : ‚à•x‚à•2 + ‚à•y‚à•2 ‚â§2"
REFERENCES,0.35135135135135137,"h, x‚ä§y = ¬µ} except for a Lebesgue measure-0 set
of (x0, y0)."
REFERENCES,0.3521021021021021,"Proof. For the measure-0 set, by Lemma G.5, the set of points converging to {‚à•x‚à•2 + ‚à•y‚à•2 > 2"
REFERENCES,0.35285285285285284,"h}
is measure-0; by the proof of Lemma G.11, the set of points converging to the origin is measure-0;
also {u2
0 = ‚à•x‚à•2 + ‚à•y‚à•2 = 0} is a hyperplane and thus is measure-0. Hence the set of all the initial
conditions not converging to {‚à•x‚à•2 + ‚à•y‚à•2 > 2"
REFERENCES,0.3536036036036036,"h, x‚ä§y = ¬µ} is measure-0."
REFERENCES,0.35435435435435436,"Since u2
0 ‚â§8¬µ, if u2
0 >
2
h, by Lemma G.7, it will decrease to u2
k ‚â§
2
h for some k. Then by
Lemma G.11, we have the convergence."
REFERENCES,0.3551051051051051,"With the above two theorems, we can prove all the theorems and corollary in Section 3."
REFERENCES,0.35585585585585583,Published as a conference paper at ICLR 2022
REFERENCES,0.3566066066066066,"Proof of Theorem 3.1 and Theorem 3.2. From Theorem G.1, h =
4
u2
0+4c¬µ for c ‚â•1 which implies"
REFERENCES,0.35735735735735735,"h ‚â§
4
u2
0+4¬µ for all u2
0 > 8¬µ. Similarly, for Theorem G.2, h ‚â§
1
3¬µ. When u2
0 = 8¬µ,
4
u2
0+4¬µ ="
REFERENCES,0.3581081081081081,"4
8¬µ+4¬µ =
1
3¬µ; when u2
0 > 8¬µ,
4
u2
0+4¬µ <
1
3¬µ; when u2
0 < 8¬µ,
4
u2
0+4¬µ >
1
3¬µ. Also, all the limit"
REFERENCES,0.3588588588588589,"points are in {‚à•x‚à•2 + ‚à•y‚à•2 ‚â§
2
h}. Then we can get Theorem 3.1 and 3.2 where the second
inequality of Theorem 3.2 is because at the global minimum x‚ä§y = ¬µ and also ‚à•x ‚àíy‚à•2 =
‚à•x‚à•2 + ‚à•y‚à•2 ‚àí2x‚ä§y."
REFERENCES,0.35960960960960964,"Proof of Corollary 3.3. For |x‚ä§
0 y0 ‚àí¬µ| < Œ¥, ‚à•x0‚à•2 + ‚à•y0‚à•2 > 8¬µ, we have"
REFERENCES,0.36036036036036034,"x‚ä§
0 y0 < ¬µ + Œ¥ ‚áí‚à•x0 ‚àíy0‚à•2 = ‚à•x0‚à•2 + ‚à•y0‚à•2 ‚àí2x‚ä§
0 y0 > 4"
REFERENCES,0.3611111111111111,h ‚àí4¬µ ‚àí2(¬µ + Œ¥) = 4
REFERENCES,0.36186186186186187,h ‚àí6¬µ ‚àí2Œ¥.
REFERENCES,0.36261261261261263,"Also, from Theorem 3.2, ‚à•x ‚àíy‚à•2 ‚â§2"
REFERENCES,0.3633633633633634,h ‚àí2¬µ and 2
REFERENCES,0.3641141141141141,h ‚àí2¬µ < 4
REFERENCES,0.36486486486486486,h ‚àí6¬µ ‚àí2Œ¥ < 4
REFERENCES,0.3656156156156156,"h ‚àí8¬µ. We then obtain
‚à•x ‚àíy‚à•2 ‚â§1"
REFERENCES,0.3663663663663664,2 ‚à•x0 ‚àíy0‚à•2 + 2¬µ.
REFERENCES,0.36711711711711714,We will divide the proof of the two main theorems into two phases: (1) when 2
REFERENCES,0.36786786786786785,"h < x2
k + y2
k < 4"
REFERENCES,0.3686186186186186,"h, we
would like to show that GD escapes to a smaller ball x2
k + y2
k ‚â§2"
REFERENCES,0.36936936936936937,"hexcept for a measure-0 set; (2)
once GD enters x2
k + y2
k ‚â§2"
REFERENCES,0.37012012012012013,"h, it will converge to the global minimum inside this region except for a
measure-0 set."
REFERENCES,0.3708708708708709,"G.1
PHASE 1: 2"
REFERENCES,0.3716216216216216,"h < u2
k < 4 h"
REFERENCES,0.37237237237237236,"We Ô¨Årst deal with the situation where GD just converges in this region by showing that such points
form a null set. This is stated in Theorem G.5."
REFERENCES,0.3731231231231231,"Theorem G.3 and Corollary G.4 are the preliminary of proving Theorem G.5. Also Corollary G.4 is a
direct result of Theorem G.3."
REFERENCES,0.3738738738738739,"Theorem G.3. Let f : RN ‚ÜíRM and f ‚ààC1. If the set of critical points of f is a null-set, i,e.,"
REFERENCES,0.37462462462462465,"L({x ‚ààRN : ‚àáf(x) is not invertible}) = 0,"
REFERENCES,0.37537537537537535,then L(f ‚àí1(B)) = 0 for any null-set B.
REFERENCES,0.3761261261261261,"Proof. Let G = {|‚àáf| Ã∏= 0} and G is an open set, where |‚àáf| denotes the determinant of ‚àáf. By
implicit function theorem, we have G ‚à©f ‚àí1(z) is a (N ‚àí1)-submanifold in C1.
Consider G ‚à©{f ‚ààB} = S"
REFERENCES,0.3768768768768769,"z‚ààB G ‚à©f ‚àí1(z), where L(G ‚à©f ‚àí1(z)) = 0 from the above discussion.
Hence, if B is countable, L(G ‚à©{f ‚ààB}) = 0."
REFERENCES,0.37762762762762764,"When B is uncountable, using co-area formula from geometric measure theory, we have for any
bounded open ball Br,
Z"
REFERENCES,0.3783783783783784,"G‚à©Br
g|‚àáf|dL =
Z R Z"
REFERENCES,0.3791291291291291,"f ‚àí1(z)‚à©G‚à©Br
g(x)dLN‚àí1(x)dz =
Z B Z"
REFERENCES,0.37987987987987987,"f ‚àí1(z)‚à©G‚à©Br
g(x)dLN‚àí1(x)dz."
REFERENCES,0.38063063063063063,"Let g be the indicator of S
z‚ààB G ‚à©Br ‚à©f ‚àí1(z). Then since B is a null-set, we have
Z B Z"
REFERENCES,0.3813813813813814,"f ‚àí1(z)‚à©G‚à©Br
g(x)dLN‚àí1(x)dz =
Z B Z"
REFERENCES,0.38213213213213215,"f ‚àí1(z)‚à©G‚à©Br
dLN‚àí1(x)dz = 0 ‚áí
Z"
REFERENCES,0.38288288288288286,"G‚à©Br
g|‚àáf|dL = 0."
REFERENCES,0.3836336336336336,"Then g|‚àáf| = 0 a.e. in G ‚à©Br. Also, |‚àáf| Ã∏= 0 a.e.. Therefore, g = 0 a.e. in G ‚à©Br, i.e., L
 ["
REFERENCES,0.3843843843843844,"z‚ààB
G ‚à©Br ‚à©f ‚àí1(z)

=
Z
gdL = 0."
REFERENCES,0.38513513513513514,Published as a conference paper at ICLR 2022
REFERENCES,0.3858858858858859,"Since we can Ô¨Ånd a sequence of bounded open ball Br, s.t., RN = S‚àû
r=1 Br, and also L(Gc) = 0,
we have"
REFERENCES,0.3866366366366366,"L(f ‚àí1(B)) ‚â§L
 ["
REFERENCES,0.38738738738738737,"z‚ààB
G ‚à©f ‚àí1(z)

+ L(Gc) = L
 ‚àû
[ r=1 ["
REFERENCES,0.38813813813813813,"z‚ààB
G ‚à©Br ‚à©f ‚àí1(z)

+ L(Gc) = 0."
REFERENCES,0.3888888888888889,"Corollary G.4. Let œà : R2d ‚ÜíR2d be the GD iteration map, i.e., œà(x, y) = [x + h(¬µ ‚àíx‚ä§y)y, y +
h(¬µ ‚àíx‚ä§y)x]‚ä§. Then if"
REFERENCES,0.38963963963963966,"L({det(Dœà) = 0}) = 0,"
REFERENCES,0.39039039039039036,then L(œà‚àí1(B)) = 0 for any null-set B.
REFERENCES,0.3911411411411411,"Lemma G.5. Given h =
4
u2
0+4c¬µ and u2
k > 2"
REFERENCES,0.3918918918918919,"h, GD will not converge to the points in {‚à•x‚à•2 +‚à•y‚à•2 >"
REFERENCES,0.39264264264264265,"2
h, x‚ä§y = ¬µ}, except for a measure-0 set."
REFERENCES,0.3933933933933934,"Proof. For d = 1,"
REFERENCES,0.39414414414414417,"det(Dœà) = 1 ‚àíhx2 ‚àíhy2 ‚àí3h2x2y2 + 4h2xy¬µ ‚àíh2¬µ2 = p1(x, y) ¬∑ ¬∑ ¬∑ pm(x, y) = 0,"
REFERENCES,0.3948948948948949,"where pi(x, y) is irreducible polynomial and pi(x, y) = 0 is a co-dimensional-1 manifold. Then
{(x, y) : det(Dœà) = 0} = Sm
i=1{pi(x, y) = 0} are measure zero, i.e., L({det(Dœà) = 0}) = 0.
Similarly for d > 1, we also have L({det(Dœà) = 0}) = 0."
REFERENCES,0.39564564564564564,"From the GD iteration, we have"
REFERENCES,0.3963963963963964,"x‚ä§
k+1yk+1 ‚àí¬µ = (x‚ä§
k yk ‚àí¬µ) ¬∑ (1 ‚àíh(‚à•xk‚à•2 + ‚à•yk‚à•2) ‚àíh2x‚ä§
k yk(¬µ ‚àíx‚ä§
k yk))."
REFERENCES,0.39714714714714716,"Then let B = {(x, y) : 1 ‚àíh(‚à•x‚à•2 + ‚à•y‚à•2) ‚àíh2x‚ä§y(¬µ ‚àíx‚ä§y) = 0}. For any (x, y) ‚ààB, let
[x+, y+]‚ä§= œà(x, y). Then x+y+ = ¬µ. Similarly, L(B) = 0. By Corollary G.4 L(œà‚àí1(B)) = 0
and then L(œà‚àín(B)) = L(œà‚àí1 ‚ó¶¬∑ ¬∑ ¬∑ ‚ó¶œà‚àí1(B)) = 0. Let œà‚àí0(B) = B and G = S‚àû
i=0 œà‚àín(B).
Hence"
REFERENCES,0.3978978978978979,"L(G) = L ‚àû
["
REFERENCES,0.39864864864864863,"i=0
œà‚àín(B) ! ‚â§ ‚àû
X"
REFERENCES,0.3993993993993994,"i=0
L(œà‚àín(B)) = 0."
REFERENCES,0.40015015015015015,"Moreover, for any 0 < œµ < ¬µ, assume |x‚ä§
k yk ‚àí¬µ| < œµ. When u2
k ‚â•2"
REFERENCES,0.4009009009009009,"h + œµh(¬µ + œµ),"
REFERENCES,0.4016516516516517,"|x‚ä§
k+1yk+1 ‚àí¬µ| = |x‚ä§
k yk ‚àí¬µ| ¬∑ |1 ‚àíhu2
k ‚àíh2x‚ä§
k yk(¬µ ‚àíx‚ä§
k yk)|"
REFERENCES,0.4024024024024024,"= |x‚ä§
k yk ‚àí¬µ| ¬∑ (‚àí1 + hu2
k + h2x‚ä§
k yk(¬µ ‚àíx‚ä§
k yk))"
REFERENCES,0.40315315315315314,"‚â•|x‚ä§
k yk ‚àí¬µ| ¬∑

‚àí1 + h
 2"
REFERENCES,0.4039039039039039,"h + œµh(¬µ + œµ)

‚àíh2(¬µ + œµ)œµ
"
REFERENCES,0.40465465465465467,"> |x‚ä§
k yk ‚àí¬µ|."
REFERENCES,0.40540540540540543,"Hence, {‚à•x‚à•2 + ‚à•y‚à•2 > 2"
REFERENCES,0.40615615615615613,"h, x‚ä§y = ¬µ} is not the limit of this GD map œà, except for the measure-0
set G."
REFERENCES,0.4069069069069069,Next we show that GD will be bounded inside {‚à•x‚à•2 + ‚à•y‚à•2 < 4 h}.
REFERENCES,0.40765765765765766,"Lemma G.6. Given h =
4
u2
0+4c¬µ, then for 0 ‚â§k < min{k : u2
k ‚â§2"
REFERENCES,0.4084084084084084,"h}, we have u2
k ‚â§4"
REFERENCES,0.4091591591591592,h ‚àí3¬µ < 4
REFERENCES,0.4099099099099099,"h
for all k."
REFERENCES,0.41066066066066065,"Proof. First u2
0 =
4
h ‚àí4c¬µ ‚â§
4
h ‚àí4¬µ <
4
h. Then if x‚ä§
0 y0 > ¬µ or x‚ä§
0 y0 < ‚àíh¬µu2
0
4‚àíhu2
0 , from"
REFERENCES,0.4114114114114114,"Lemma G.8, u2
1 < u2
0 < 4"
REFERENCES,0.41216216216216217,"h. If ‚àíh¬µu2
0
4‚àíhu2
0 ‚â§x‚ä§
0 y0 < ¬µ, from Lemma G.9 and its proof, u2
2 < u2
0 < 4"
REFERENCES,0.41291291291291293,"h
and u1 ‚â§u2
0 + ¬µ"
REFERENCES,0.41366366366366364,"c ‚â§u2
0 + ¬µ ‚â§4"
REFERENCES,0.4144144144144144,h ‚àí3¬µ < 4
REFERENCES,0.41516516516516516,"h. Therefore, iteratively we have u2
k < 4 h."
REFERENCES,0.4159159159159159,Published as a conference paper at ICLR 2022
REFERENCES,0.4166666666666667,"Therefore, without loss of generality, we can just assume u2
k ‚â§u2
0 for a Ô¨Åxed kth iteration that we
need to analyze because for every two step there exists an ith iteration such that u2
i ‚â§u2
0 and we can
choose k = i to do the analysis."
REFERENCES,0.4174174174174174,"Lemma G.8 and G.9 describe one-step or two-step decay of u2
k, which lead to the primary result,
Lemma G.7, in phase one. Moreover, the proof of Lemma G.7 contains a Ô¨Åner characterization of u2
k
making it possible to end phase one and enter phase two.
Lemma G.7. Given h =
4
u2
0+4c¬µ for c ‚â•1, GD will enter {‚à•x‚à•2 + ‚à•y‚à•2 ‚â§
2
h} except for a
measure-0 set of initial conditions."
REFERENCES,0.41816816816816815,"Proof. By Lemma G.6 and its discussion, assume without loss of generality 2"
REFERENCES,0.4189189189189189,"h < u2
k ‚â§u2
0. From
Lemma G.8 and G.9, the region where the decrease of u2
k may be small is when xk, yk are close to"
REFERENCES,0.4196696696696697,"x‚ä§
k yk = ¬µ or x‚ä§
k yk = ‚àíh¬µu2
k
4‚àíhu2
k . Since"
REFERENCES,0.42042042042042044,"x‚ä§
k+1yk+1 ‚àí¬µ = (x‚ä§
k yk ‚àí¬µ)(1 ‚àíhu2
k ‚àíh2x‚ä§
k yk(¬µ ‚àíx‚ä§
k yk)),"
REFERENCES,0.42117117117117114,"consider sk = 1 ‚àíhu2
k ‚àíh2x‚ä§
k yk(¬µ ‚àíx‚ä§
k yk). From the proof of Lemma G.9, we know when"
REFERENCES,0.4219219219219219,"x‚ä§
k yk = ‚àíh¬µu2
k
4‚àíhu2
k , sk < ‚àí1+2h2¬µ2 < 0. Therefore, there exists Œ¥ > 0, s.t., for all xk, yk ‚àà{|x‚ä§
k yk+"
REFERENCES,0.42267267267267267,"h¬µu2
k
4‚àíhu2
k | < Œ¥, 2"
REFERENCES,0.42342342342342343,"h < u2
k ‚â§u2
0}, sk < 0. Then x‚ä§
k+1yk+1 > ¬µ. Hence, when ‚àíŒ¥ ‚â§x‚ä§
k yk +
h¬µu2
k
4‚àíhu2
k < 0,"
REFERENCES,0.4241741741741742,"we will skip this step and consider the decrease of the next step with x‚ä§
k+1yk+1 > ¬µ. Also, there"
REFERENCES,0.42492492492492495,"exist Œ≤1 = Œ≤1(Œ¥) > 0, s.t., when x‚ä§
k yk +
h¬µu2
k
4‚àíhu2
k < ‚àíŒ¥,"
REFERENCES,0.42567567567567566,"u2
k+1 ‚àíu2
k = h(¬µ ‚àíx‚ä§
k yk)((4 ‚àíhu2
k)x‚ä§
k yk + hu2
k¬µ) < ‚àíŒ≤1."
REFERENCES,0.4264264264264264,"From the proof of Lemma G.9, when ‚àíh¬µu2
k
4‚àíhu2
k ‚â§x‚ä§
k yk ‚â§0, for Œ≤2 = max{h¬µ2(4 ‚àíhu2
0)(1 ‚àí"
REFERENCES,0.4271771771771772,"2h2¬µ2)2, h¬µ2(8 ‚àíh(2u2
0 + ¬µ"
REFERENCES,0.42792792792792794,"c ))} > 0,"
REFERENCES,0.4286786786786787,"u2
k+2 ‚àíu2
k < max{‚àíh(¬µ ‚àíx‚ä§
k yk)¬µ(4 ‚àíhu2
k+1)(1 ‚àí2h2¬µ2)2, ‚àíh(¬µ ‚àíx‚ä§
k yk)2(8 ‚àíh(u2
k + u2
k+1))} ‚â§‚àíŒ≤2."
REFERENCES,0.4294294294294294,"Fix an small œµ in 0 < œµ < ¬µ. When x‚ä§
k yk ‚â•¬µ + œµ, from the proof of Lemma G.8, we have for
Œ≤3 = 4h¬µœµ > 0,"
REFERENCES,0.43018018018018017,"u2
k+1 ‚àíu2
k ‚â§4h¬µ(¬µ ‚àíx‚ä§
k yk) ‚â§‚àí4h¬µœµ = ‚àíŒ≤3."
REFERENCES,0.43093093093093093,"When 0 < x‚ä§
k yk ‚â§¬µ ‚àíœµ with œµ > 0, from the proof of Lemma G.9, we have for Œ≤4 = hœµ2(8 ‚àí
h(2u2
0 + ¬µ"
REFERENCES,0.4316816816816817,"c )) > 0,"
REFERENCES,0.43243243243243246,"u2
k+2 ‚àíu2
k ‚â§‚àíh(¬µ ‚àíx‚ä§
k yk)2(8 ‚àíh(u2
k + u2
k+1)) ‚â§‚àíŒ≤4."
REFERENCES,0.43318318318318316,"When |x‚ä§
k yk ‚àí¬µ| < œµ, assume |x‚ä§
k yk ‚àí¬µ| = œµk. In this case, we have sk < 0 meaning GD oscillates
around x‚ä§y = ¬µ. Also, if 0 < x‚ä§
k yk < ¬µ and u2
k > 2"
REFERENCES,0.4339339339339339,"h, sk = 1 ‚àíhu2
k ‚àíh2x‚ä§y(¬µ ‚àíx‚ä§y) < ‚àí1,
i.e., if GD is in 0 < x‚ä§y < ¬µ, then we have |x‚ä§
k+1yk+1 ‚àí¬µ| > |x‚ä§
k yk ‚àí¬µ|. Hence we only
need to focus on the other side which is x‚ä§
k yk > ¬µ. From Lemma G.5, if u2
k ‚â•2"
REFERENCES,0.4346846846846847,"h + œµh(¬µ + œµ),
|x‚ä§
k+1yk+1 ‚àí¬µ| > |x‚ä§
k yk ‚àí¬µ|. Then within Ô¨Ånite steps (note all these steps satisfy either one-step or
two-step decrease of u2
k; we ignore these steps only because the decrease maybe small), we will have
either |x‚ä§
k yk ‚àí¬µ| keeps increasing or u2
K < 2"
REFERENCES,0.43543543543543545,"h + œµKh(¬µ + œµK) (here we also consider x‚ä§
KyK > ¬µ).
For the former one, the decrease of u2
k for each step will be lower bounded away from 0; for the latter
one, from the discussion above, u2
K+1 ‚àíu2
K ‚â§‚àí4h¬µœµK ‚áíu2
K+1 ‚â§2 h."
REFERENCES,0.4361861861861862,"From Lemma G.5, we know GD will not terminate in Ô¨Ånite steps except for measure-0 set. From all
the discussion above, we have that u2
k decreases by a constant for either one-step or two-step except
and hence GD will enter ‚à•x‚à•2 + ‚à•y‚à•2 ‚â§2"
REFERENCES,0.4369369369369369,h except for a measure-0 set.
REFERENCES,0.4376876876876877,Published as a conference paper at ICLR 2022
REFERENCES,0.43843843843843844,"Lemma G.8. Given h =
4
u2
0+4c¬µ and u2
k <
4
h, when x‚ä§
k yk > ¬µ or x‚ä§
k yk < ‚àíh¬µu2
k
4‚àíhu2
k , we have"
REFERENCES,0.4391891891891892,"u2
k+1 ‚àíu2
k < 0."
REFERENCES,0.43993993993993996,Proof.
REFERENCES,0.44069069069069067,"u2
k+1 ‚àíu2
k = h(¬µ ‚àíx‚ä§
k yk)((4 ‚àíhu2
k)x‚ä§
k yk + hu2
k¬µ) = h(¬µ ‚àíx‚ä§
k yk)(4x‚ä§
k yk + hu2
k(¬µ ‚àíx‚ä§
k yk))"
REFERENCES,0.44144144144144143,"If x‚ä§
k yk > ¬µ, by u2
k < 4 h,"
REFERENCES,0.4421921921921922,"u2
k+1 ‚àíu2
k = h(¬µ ‚àíx‚ä§
k yk)(4x‚ä§
k yk + hu2
k(¬µ ‚àíx‚ä§
k yk)) ‚â§4h¬µ(¬µ ‚àíx‚ä§
k yk) < 0."
REFERENCES,0.44294294294294295,"If x‚ä§
k yk < ‚àíh¬µu2
k
4‚àíhu2
k ‚áíx‚ä§
k yk < ¬µ, then,"
REFERENCES,0.4436936936936937,"u2
k+1 ‚àíu2
k = h(¬µ ‚àíx‚ä§
k yk)((4 ‚àíhu2
k)x‚ä§
k yk + hu2
k¬µ) < 0."
REFERENCES,0.4444444444444444,"Lemma G.9. Given h =
4
u2
0+4c¬µ , c > max{ 1"
REFERENCES,0.4451951951951952,"2, 2h¬µ}, c > 0, and u2
k ‚â§u2
0, when ‚àíh¬µu2
k
4‚àíhu2
k ‚â§"
REFERENCES,0.44594594594594594,"x‚ä§
k yk < ¬µ, then u2
k+2 ‚àíu2
k < 0."
REFERENCES,0.4466966966966967,"Proof. For every u2
k, there exist a constant ck > 0, s.t. h =
4
u2
k+4ck¬µ. Since 2"
REFERENCES,0.44744744744744747,"h < u2
k ‚â§u2
0 < 4 h, we"
REFERENCES,0.44819819819819817,"have c ‚â§ck <
1
2h¬µ. Since"
REFERENCES,0.44894894894894893,"x‚ä§
k+1yk+1 ‚àí¬µ = (x‚ä§
k yk ‚àí¬µ)(1 ‚àíhu2
k ‚àíh2x‚ä§
k yk(¬µ ‚àíx‚ä§
k yk)) = (x‚ä§
k yk ‚àí¬µ)sk,"
REFERENCES,0.4496996996996997,"then sk = 1 ‚àíhu2
k ‚àíh2x‚ä§
k yk(¬µ ‚àíx‚ä§
k yk) is bounded by the value at x‚ä§
k yk = ‚àíh¬µu2
k
4‚àíhu2
k or x‚ä§
k yk = ¬µ,
i.e.,"
REFERENCES,0.45045045045045046,"sk = 1 ‚àíhu2
k ‚àíh2x‚ä§
k yk(¬µ ‚àíx‚ä§
k yk) ‚â§max

1 ‚àíhu2
k +
4h3u2
k¬µ2"
REFERENCES,0.4512012012012012,"(4 ‚àíhu2
k)2 , 1 ‚àíhu2
k "
REFERENCES,0.4519519519519519,"Since u2
k > 2"
REFERENCES,0.4527027027027027,"h, we have 1 ‚àíhu2
k < ‚àí1. For the other one, since u2
k > 2"
REFERENCES,0.45345345345345345,"h and ck <
1
2h¬µ,"
REFERENCES,0.4542042042042042,"1 ‚àíhu2
k +
4h3u2
k¬µ2"
REFERENCES,0.45495495495495497,"(4 ‚àíhu2
k)2 = (1 ‚àí3c2
k)u2
k + 4c3
k¬µ
c2
k(u2
k + 4ck¬µ)
< ‚àí1 + 2h2¬µ2,"
REFERENCES,0.45570570570570573,"where c >
h¬µ+‚àö"
REFERENCES,0.45645645645645644,2‚àíh2¬µ2
REFERENCES,0.4572072072072072,"2(1‚àíh2¬µ2)
. This is because either u2
0 > 8¬µ ‚áíh¬µ <
1
3¬µ or h ‚â§
1
3¬µ and then h¬µ+‚àö"
REFERENCES,0.45795795795795796,2‚àíh2¬µ2
REFERENCES,0.4587087087087087,"2(1‚àíh2¬µ2)
< 1 ‚â§c. Also note here this is c not ck; this value ‚àí1 + 2h2¬µ2 is achieved when"
REFERENCES,0.4594594594594595,"u2
k = 2/h and ck =
h¬µ+‚àö"
REFERENCES,0.4602102102102102,2‚àíh2¬µ2
REFERENCES,0.46096096096096095,"2(1‚àíh2¬µ2)
or
1
2h¬µ."
REFERENCES,0.4617117117117117,"Also, when 0 ‚â§x‚ä§
k yk < ¬µ,"
REFERENCES,0.4624624624624625,"sk = 1 ‚àíhu2
k ‚àíh2x‚ä§
k yk(¬µ ‚àíx‚ä§
k yk) ‚â§1 ‚àíhu2
k < ‚àí1."
REFERENCES,0.46321321321321324,"We then prove 4 ‚àíhu2
k+1 > 0. When x‚ä§
k yk < ¬µ,"
REFERENCES,0.46396396396396394,"u2
k+1 ‚àíu2
k = h(¬µ ‚àíx‚ä§
k yk)((4 ‚àíhu2
k)x‚ä§
k yk + hu2
k¬µ)."
REFERENCES,0.4647147147147147,"The maximum is achieved at x‚ä§
k yk = 2‚àíhu2
k
4‚àíhu2
k ¬µ, i.e., u2
k+1 ‚àíu2
k ‚â§¬µ ck ‚â§¬µ"
REFERENCES,0.46546546546546547,"c . Hence 4 ‚àíhu2
k+1 > 0
when c > 1/2."
REFERENCES,0.46621621621621623,"Since x‚ä§
k yk < ¬µ and sk ‚â§0,"
REFERENCES,0.466966966966967,"u2
k+2 ‚àíu2
k = u2
k+2 ‚àíu2
k+1 + u2
k+1 ‚àíu2
k
= h(¬µ ‚àíx‚ä§
k+1yk+1)((4 ‚àíhu2
k+1)x‚ä§
k+1yk+1 + hu2
k+1¬µ)"
REFERENCES,0.4677177177177177,"+ h(¬µ ‚àíx‚ä§
k yk)((4 ‚àíhu2
k)x‚ä§
k yk + hu2
k¬µ)"
REFERENCES,0.46846846846846846,"= h(¬µ ‚àíx‚ä§
k yk)(((4 ‚àíhu2
k)x‚ä§
k yk + hu2
k¬µ) + sk(hu2
k+1¬µ + (4 ‚àíhu2
k+1)(sk(x‚ä§
k yk ‚àí¬µ) + ¬µ)))"
REFERENCES,0.4692192192192192,"= h(¬µ ‚àíx‚ä§
k yk)
 
x‚ä§
k yk(4 ‚àíhu2
k) + s2
k(4 ‚àíhu2
k+1)(x‚ä§
k yk ‚àí¬µ) + hu2
k¬µ + sk(hu2
k+1¬µ + (4 ‚àíhu2
k+1)¬µ)

."
REFERENCES,0.46996996996997,Published as a conference paper at ICLR 2022
REFERENCES,0.47072072072072074,"When 0 ‚â§x‚ä§
k yk < ¬µ, s < ‚àí1, then"
REFERENCES,0.47147147147147145,"u2
k+2 ‚àíu2
k ‚â§‚àíh(¬µ ‚àíx‚ä§
k yk)2(8 ‚àíh(u2
k + u2
k+1)) < 0"
REFERENCES,0.4722222222222222,"When ‚àíh¬µu2
k
4‚àíhu2
k ‚â§x‚ä§
k yk < 0, sk ‚â§0, then for c ‚â•2h¬µ,"
REFERENCES,0.47297297297297297,"u2
k+2 ‚àíu2
k < h(¬µ ‚àíx‚ä§
k yk)¬µ(‚àí4 + hu2
k + 8h2¬µ2 ‚àí(4 ‚àíhu2
k+1)(1 ‚àí2h2¬µ2)2)"
REFERENCES,0.47372372372372373,"< ‚àíh(¬µ ‚àíx‚ä§
k yk)¬µ(4 ‚àíhu2
k+1)(1 ‚àí2h2¬µ2)2 < 0"
REFERENCES,0.4744744744744745,"where this bound is achieved by taking sk = ‚àí1 + 2h2¬µ2 and x‚ä§
k yk = 0."
REFERENCES,0.4752252252252252,"G.2
PHASE 2: u2
k ‚â§2 h"
REFERENCES,0.47597597597597596,"In this part, we will show the convergence of GD in Lemma G.11."
REFERENCES,0.4767267267267267,"There are two convergence patterns: (i) transversal convergence, i.e., oscillating around the valley;
(ii) unilateral convergence, i.e., converging from one side of the valley."
REFERENCES,0.4774774774774775,"The key point of the proof of pattern (i) is to analyze the change of sk = 1‚àíhu2
k‚àíh2x‚ä§
k yk(¬µ‚àíx‚ä§
k yk).
We know x‚ä§
k+1yk+1 ‚àí¬µ = sk(x‚ä§
k yk ‚àí¬µ), i.e., sk measures the change of the loss. If for some
constant K > 0 we have |sk| < 1 ‚àÄk ‚â•K such that |x‚ä§
k yk ‚àí¬µ| is guaranteed to decrease to 0, then
the convergence follows. Moreover, we will need to analyze s2k and s2k+1 separately because in this
case sk < 0 and x‚ä§
k yk ‚àí¬µ changes sign at each step."
REFERENCES,0.47822822822822825,"For pattern (ii), we will show that the trajectory of GD is bounded in a subset of this region |sk| < 1
that guarantees the decrease of the loss."
REFERENCES,0.47897897897897895,"Before presenting our main result in phase 2, we Ô¨Årst show a boundedness theorem when GD enters
this phase."
REFERENCES,0.4797297297297297,Lemma G.10. Once GD enters {‚à•x‚à•2 + ‚à•y‚à•2 ‚â§2
REFERENCES,0.4804804804804805,"h}, it will stay bounded inside {‚à•x‚à•2 + ‚à•y‚à•2 ‚â§
2
h + 2¬µ} and re-enter {‚à•x‚à•2 + ‚à•y‚à•2 ‚â§2"
REFERENCES,0.48123123123123124,"h} within Ô¨Ånite steps where the number of such steps do not
depend on the number of iteration."
REFERENCES,0.481981981981982,"Proof. If at step K, u2
K ‚â§2"
REFERENCES,0.4827327327327327,"h, then from Lemma G.15, u2
K+1 ‚â§2"
REFERENCES,0.48348348348348347,"h + ¬µ and it returns to phase 1.
From the proof of Lemma G.9, u2
K+2 ‚â§u2
K+1 + ¬µ ‚â§2"
REFERENCES,0.48423423423423423,"h + 2¬µ and we know either u2
K+2 < u2
K+1 or
u2
K+3 < u2
K+1 and so on until it re-enters u2
i ‚â§2"
REFERENCES,0.484984984984985,"h for some i ‚â•K. Therefore, all the u2
k ‚â§2"
REFERENCES,0.48573573573573575,"h + 2¬µ
once GD enters {‚à•x‚à•2 + ‚à•y‚à•2 ‚â§2 h}."
REFERENCES,0.4864864864864865,"Then by Lemma G.10, we can always pick a kth iteration such that u2
k ‚â§2"
REFERENCES,0.4872372372372372,"h. Also, when u2
0 > 8¬µ,
we have h¬µ < 1"
REFERENCES,0.487987987987988,"3. Together with the choice of h =
1
(2+c)¬µ when 0 < u2
0 ‚â§8¬µ, we have h¬µ ‚â§1"
REFERENCES,0.48873873873873874,"3.
Then we obtain the following convergence of GD inside phase 2.
Lemma G.11. Given h¬µ ‚â§1"
REFERENCES,0.4894894894894895,"3, if GD enters {‚à•x‚à•2 + ‚à•y‚à•2 ‚â§2"
REFERENCES,0.49024024024024027,"h}, it converge to x‚ä§y = ¬µ inside this
region except for a measure-0 set of initial conditions."
REFERENCES,0.49099099099099097,"Proof. First, from Corollary G.4, we know the set of points converging to (0, 0) in Ô¨Ånite steps is
measure 0. For all the other points, we have the following discussion."
REFERENCES,0.49174174174174173,"If sk = 0, then x‚ä§
k+1yk+1 = ¬µ, i.e., it converges."
REFERENCES,0.4924924924924925,"If x‚ä§
k yk < ‚àí¬µ and 1"
REFERENCES,0.49324324324324326,"h + 2h¬µ2 < u2
k ‚â§2"
REFERENCES,0.493993993993994,"h, then by Lemma G.14,"
REFERENCES,0.4947447447447447,"u2
k+1 ‚àíu2
k = h(¬µ ‚àíx‚ä§
k yk)((4 ‚àíhu2
k)x‚ä§
k yk + hu2
k¬µ) ‚â§2h(¬µ2 ‚àí(x‚ä§
k yk)2) < 0."
REFERENCES,0.4954954954954955,"Hence for Œ¥1 > 0, when ‚àí¬µ ‚àíŒ¥1 < x‚ä§
k yk < ‚àí¬µ, we have x‚ä§
k+1yk+1 > ¬µ and we will skip this step
and directly consider the (k+1)th iteration; when x‚ä§
k yk ‚â§‚àí¬µ‚àíŒ¥1, u2
k+1‚àíu2
k ‚â§‚àí2h((¬µ+Œ¥1)2‚àí¬µ2).
Namely when x‚ä§
k yk < ‚àí¬µ, either u2
k+1 is some constant away from u2
k or we can ignore the decrease
in this step and directly look at the next one with x‚ä§
k+1yk+1 > ¬µ."
REFERENCES,0.49624624624624625,Published as a conference paper at ICLR 2022
REFERENCES,0.496996996996997,"If x‚ä§
k yk < ‚àí¬µ and u2
k ‚â§1"
REFERENCES,0.49774774774774777,"h + 2h¬µ2, then"
REFERENCES,0.4984984984984985,"u2
k+1 ‚àíu2
k = h(¬µ ‚àíx‚ä§
k yk)(4x‚ä§
k yk + hu2
k(¬µ ‚àíx‚ä§
k yk))"
REFERENCES,0.49924924924924924,"‚â§h(¬µ ‚àíx‚ä§
k yk)(4x‚ä§
k yk + (1 + 2h2¬µ2)(¬µ ‚àíx‚ä§
k yk))"
REFERENCES,0.5,‚â§‚àí4h¬µ2(1 ‚àí2h2¬µ2) < 0.
REFERENCES,0.5007507507507507,"If x‚ä§
k yk > 3¬µ, then u2
k+1 ‚àíu2
k ‚â§2h(¬µ2 ‚àíx2
ky2
k) < ‚àí16h¬µ2. Also, when u2
k ‚â§6¬µ, x‚ä§
k yk ‚â§3¬µ."
REFERENCES,0.5015015015015015,"Next, for the rest of the region, we Ô¨Årst consider 1"
REFERENCES,0.5022522522522522,"h + 2h¬µ2 < u2
k ‚â§2"
REFERENCES,0.503003003003003,"h (by Lemma G.10, we are
always able to Ô¨Ånd u2
k ‚â§
2
h). In this region, by Lemma G.18, we know sk < 0, i.e., (x‚ä§
k yk ‚àí
¬µ)(x‚ä§
k+1yk+1 ‚àí¬µ) < 0. We divide the whole region into two parts: x‚ä§
k yk > ¬µ and x‚ä§
k yk < ¬µ.
Therefore all the xk+2i should be on the same side for i such that 1"
REFERENCES,0.5037537537537538,"h + 2h¬µ2 < u2
k+2i ‚â§2 h."
REFERENCES,0.5045045045045045,"If ‚àí¬µ ‚â§x‚ä§
k yk < ¬µ and sk ‚â§‚àí1, by Lemma G.16, we have u2
k+2 ‚àíu2
k ‚â§‚àí4h(1 ‚àíh¬µ)(¬µ ‚àí
x‚ä§
k yk)2 < 0. Hence when GD does not converge, i.e., {(xk+2n, yk+2n)}‚àû
n=0 does not converge , u2
k
will keep decreasing with u2
k+2i ‚â§2"
REFERENCES,0.5052552552552553,"h for all K+2i with i ‚â•0 such that sk+2i ‚â§‚àí1. Moreover, there
exists N > 0 s.t. sk+2N > ‚àí1 because sk = 1‚àíhu2
k‚àíh2x‚ä§
k yk(¬µ‚àíx‚ä§
k yk) and if xk+2i, yk+2i is the
Ô¨Årst iteration to leave this region, then it has to satisfy x‚ä§
k+2iyk+2i < ‚àí¬µ which implies sk+2i > ‚àí1,
and then it follows from Lemma G.13 and G.12 just as the following two paragraphs of discussion; if
it never leaves this region and GD is not converging, i.e., |¬µ ‚àíx‚ä§
k yk| has a lower bound, then u2
k+2i
will keep decreasing until sk+2N > ‚àí1 (from the proof of Lemma G.12, if ¬µ ‚àíx‚ä§
k yk is very small,
then sk+1 < sk meaning both sides are blowing up and therefore |¬µ ‚àíx‚ä§
k yk| has a lower bound if
sk ‚â§‚àí1)."
REFERENCES,0.506006006006006,"If ‚àí¬µ ‚â§x‚ä§
k yk < ¬µ and sk > ‚àí1, from Lemma G.12 and Lemma G.13, similar to the previous
discussion, there exists N1 > 0, s.t., for all k > N1, sk > min{‚àí1 + c0, ‚àí1 + c1(¬µ ‚àíx‚ä§
k yk)2} for
some c0, c1 > 0, namely |¬µ ‚àíx‚ä§
k yk| strictly decreases. Then it will either converge in 1"
REFERENCES,0.5067567567567568,"h + 2h¬µ2 <
‚à•x‚à•2 + ‚à•y‚à•2 ‚â§2"
REFERENCES,0.5075075075075075,h or enter ‚à•x‚à•2 + ‚à•y‚à•2 ‚â§1
REFERENCES,0.5082582582582582,h + 2h¬µ2.
REFERENCES,0.509009009009009,"If ¬µ < x‚ä§
k yk ‚â§3¬µ, from the deÔ¨Ånition of sk, we have sk > ‚àí1. Also by Lemma G.13, we know
sm > min{‚àí1 + c0, ‚àí1 + c1(¬µ ‚àíx‚ä§
k yk)2} for all m > k. Hence similarly, it will either converge
or enter u2
k ‚â§1"
REFERENCES,0.5097597597597597,h + 2h¬µ2.
REFERENCES,0.5105105105105106,"Then we consider u2
k ‚â§
1
h + 2h¬µ2. From Lemma G.17 and Lemma G.20, we know |sk| < 1
and will be away from 1 by a constant in this area. We will show that GD stays bounded in the
converging domain. If sk ‚â§0, it follows from the previous discussion and the proof of Lemma G.17
and Lemma G.20. If sk > 0, when the trajectory is in {|x‚ä§y| > ¬µ}, from Lemma G.14, we have
u2
k+1 < u2
k; when it is in {|x‚ä§y| < ¬µ}, then |xk+1 ‚àíyk+1| < |xk ‚àíyk| from Lemma G.19. Hence
the trajectory will stay inside the monotone decreasing region. Next, since |¬µ ‚àíx‚ä§
k yk| monotonically
decreases and is lower bounded by 0, we have that |¬µ ‚àíx‚ä§
k yk| converges. If |¬µ ‚àíx‚ä§
k yk| converges to
C > 0, then min{‚àí1 + c0, ‚àí1 + c1(¬µ ‚àíx‚ä§
k yk)2} < sk < 1 ‚àíc2 for some c0, c1, c2 > 0 meaning
|¬µ ‚àíx‚ä§
k yk| does not converges to C. Contradiction. Therefore, |¬µ ‚àíx‚ä§
k yk| converges to 0."
REFERENCES,0.5112612612612613,"Lemma G.12. When ‚àí¬µ ‚â§x‚ä§
k yk < ¬µ and 1"
REFERENCES,0.512012012012012,"h + 2h¬µ2 < u2
k ‚â§
2
h + 2¬µ, if sk > ‚àí1, then"
REFERENCES,0.5127627627627628,sk+2 > min{‚àí3
REFERENCES,0.5135135135135135,4 ‚àíh2¬µ2
REFERENCES,0.5142642642642643,"4 , ‚àí5h2¬µ2"
REFERENCES,0.515015015015015,"2
, ‚àí1 + h2(¬µ ‚àíx‚ä§
k yk)2} > ‚àí1, for 9"
REFERENCES,0.5157657657657657,2h2¬µ2 < 1.
REFERENCES,0.5165165165165165,"Proof. From previous statement, sk < 0 when u2
k > 1"
REFERENCES,0.5172672672672672,"h + 2h¬µ2. For sk = 1 ‚àíhu2
k ‚àíh2x‚ä§
k yk(¬µ ‚àí
x‚ä§
k yk), let œµ = ¬µ ‚àíx‚ä§
k yk. Then x‚ä§
k+1yk+1 ‚àí¬µ = ‚àískœµ."
REFERENCES,0.5180180180180181,"sk+1 = 1 ‚àíhu2
k+1 ‚àíh2x‚ä§
k+1yk+1(¬µ ‚àíx‚ä§
k+1yk+1) = sk ‚àíh2œµ¬µ(3 + sk) + h2œµ2(3 + s2
k ‚àíhu2
k)."
REFERENCES,0.5187687687687688,"Hence if œµ is very small, we have sk+1 < sk. Also, xk+2yk+2 ‚àí¬µ = sk+1(x‚ä§
k+1yk+1 ‚àí¬µ) =
‚àísk+1skœµ. Moreover, u2
k+1 ‚â§2"
REFERENCES,0.5195195195195195,h + 2¬µ by Lemma G.6.
REFERENCES,0.5202702702702703,Published as a conference paper at ICLR 2022
REFERENCES,0.521021021021021,"sk+2 = 1 ‚àíhu2
k+2 ‚àíh2x‚ä§
k+2yk+2(¬µ ‚àíx‚ä§
k+2yk+2)"
REFERENCES,0.5217717717717718,"= sk ‚àíh2œµ¬µ(3 + 4sk + sksk+1) + h2œµ2(3 + s2
k ‚àíhu2
k + s2
k(3 + s2
k+1 ‚àíhu2
k+1)))"
REFERENCES,0.5225225225225225,"‚â•sk ‚àíh2œµ¬µ(3 + 4sk + sksk+1) + h2œµ2(1 + (1 + s2
k+1)s2
k)"
REFERENCES,0.5232732732732732,"sk+1=¬µ/(2skœµ)
‚â•
sk ‚àíh2¬µ2"
REFERENCES,0.524024024024024,"4
‚àíh2œµ(3 + 4sk) + h2œµ2(1 + s2
k)"
REFERENCES,0.5247747747747747,"If 3 + 4sk ‚â•0, then"
REFERENCES,0.5255255255255256,sk+2 ‚â•sk ‚àíh2¬µ2
REFERENCES,0.5262762762762763,"4
‚àíh2œµ(3 + 4sk) + h2œµ2(1 + s2
k)"
REFERENCES,0.527027027027027,"œµ=(3+4sk)¬µ/(2(1+s2
k))
‚â•
sk ‚àíh2¬µ2"
REFERENCES,0.5277777777777778,"4
‚àíh2¬µ2(3 + 4sk)2"
REFERENCES,0.5285285285285285,"4(1 + s2
k)"
REFERENCES,0.5292792792792793,"sk=0 or sk=‚àí3/4
‚â•
min{‚àí3"
REFERENCES,0.53003003003003,4 ‚àíh2¬µ2
REFERENCES,0.5307807807807807,"4
, ‚àí5h2¬µ2"
REFERENCES,0.5315315315315315,"2
} > ‚àí1."
REFERENCES,0.5322822822822822,"If 3 + 4sk < 0, i.e., ‚àí1 < sk < ‚àí3"
REFERENCES,0.5330330330330331,"4, then"
REFERENCES,0.5337837837837838,"sk+2 = sk ‚àíh2œµ¬µ(3 + 4sk + sksk+1) + h2œµ2(3 + s2
k ‚àíhu2
k + s2
k(3 + s2
k+1 ‚àíhu2
k+1))"
REFERENCES,0.5345345345345346,"= sk ‚àíh2œµ¬µ(3 + 4sk + sk(sk ‚àíh2œµ¬µ(3 + sk) + h2œµ2(3 + s2 ‚àíhu2
k)))"
REFERENCES,0.5352852852852853,"+ h2œµ2(3 + s2
k ‚àíhu2
k + s2
k(3 + s2
k+1 ‚àíhu2
k+1))"
REFERENCES,0.536036036036036,"= sk ‚àíh2¬µœµ(sk(3 + sk) + 3 + sk) ‚àíh4¬µœµ3sk(3 + s2
k ‚àíhu2
k)"
REFERENCES,0.5367867867867868,"+ h2œµ2(3 + s2
k ‚àíhu2
k + s2
k(3 + s2
k+1 ‚àíhu2
k+1) + h2¬µ2sk(3 + sk))."
REFERENCES,0.5375375375375375,"When ‚àí1 < sk <
1‚àí3h2œµ¬µ"
REFERENCES,0.5382882882882883,"h2œµ¬µ
, sk ‚àíh2¬µœµ(sk(3 + sk) + 3 + sk) > ‚àí1. Since œµ ‚â§2¬µ, we have"
REFERENCES,0.539039039039039,1‚àí3h2œµ¬µ
REFERENCES,0.5397897897897898,"h2œµ¬µ
‚â•1‚àí6h2¬µ2"
REFERENCES,0.5405405405405406,"2h2¬µ2
‚â•‚àí3"
REFERENCES,0.5412912912912913,4 for 9
REFERENCES,0.5420420420420421,"2h2¬µ2 < 1. Also,"
REFERENCES,0.5427927927927928,"‚àíh4¬µœµ3sk(3 + s2
k ‚àíhu2
k) > 3"
REFERENCES,0.5435435435435435,"4h4¬µœµ3 > 0,"
REFERENCES,0.5442942942942943,"h2œµ2(3 + s2
k ‚àíhu2
k + s2
k(3 + s2
k+1 ‚àíhu2
k+1) + h2¬µ2sk(3 + sk)) > h2œµ2 > 0. Hence"
REFERENCES,0.545045045045045,"sk+2 > ‚àí1 ‚àíh4¬µœµ3sk(3 + s2
k ‚àíhu2
k)"
REFERENCES,0.5457957957957958,"+ h2œµ2(3 + s2
k ‚àíhu2
k + s2
k(3 + s2
k+1 ‚àíhu2
k+1) + h2¬µ2sk(3 + sk)) > ‚àí1 + h2œµ2"
REFERENCES,0.5465465465465466,"Lemma G.13. When x‚ä§
k yk > ¬µ and 1"
REFERENCES,0.5472972972972973,"h + 2h¬µ2 < u2
k ‚â§2"
REFERENCES,0.5480480480480481,"h + 2¬µ, if sk > ‚àí1, then sk+1 > sk and
sk+2 > min{‚àí1 + h2(¬µ ‚àíx‚ä§
k yk)2( 1 2 ‚àí3"
REFERENCES,0.5487987987987988,"2h2¬µ2), ‚àí1 2 ‚àí3"
REFERENCES,0.5495495495495496,"2h4(¬µ ‚àíx‚ä§
k yk)2¬µ2} > ‚àí1, for 8h2¬µ2 < 1.
Moreover, if u2
k+1 ‚â§2"
REFERENCES,0.5503003003003003,"h, sk ‚â§‚àí1, and sk+1 > ‚àí1, then sk+2 > sk + h2(¬µ ‚àíx‚ä§
k yk)2(1 ‚àí8h2¬µ2)."
REFERENCES,0.551051051051051,"Proof. From previous statement, sk < 0 when u2
k > 1"
REFERENCES,0.5518018018018018,"h + 2h¬µ2. Without loss of generality, we can
just assume u2
k < 3"
REFERENCES,0.5525525525525525,"h (otherwise u2
k+2 < 3"
REFERENCES,0.5533033033033034,"h and we can use this as our starting point). For x‚ä§
k yk > ¬µ,
sk = 1 ‚àíhu2
k ‚àíh2x‚ä§
k yk(¬µ ‚àíx‚ä§
k yk). Let x‚ä§
k yk ‚àí¬µ = œµ. Then"
REFERENCES,0.5540540540540541,"sk+1 = 1 ‚àíhu2
k+1 ‚àíh2x‚ä§
k+1yk+1(¬µ ‚àíx‚ä§
k+1yk+1) = sk + h2œµ¬µ(3 + sk) + h2œµ2(3 + s2
k ‚àíhu2
k) > sk,"
REFERENCES,0.5548048048048048,"where sk > ‚àí3 in this region. Also, x‚ä§
k+2yk+2 ‚àí¬µ = sk+1skœµ."
REFERENCES,0.5555555555555556,"Then for sk ‚â§‚àí1, u2
k+1 ‚â§2"
REFERENCES,0.5563063063063063,"h, and sk+1 > ‚àí1,"
REFERENCES,0.5570570570570571,"sk+2 = sk + h2œµ¬µ(3 + 4sk + sksk+1) + h2œµ2(3 + s2
k ‚àíhu2
k + s2
k(3 + s2
k+1 ‚àíhu2
k+1))"
REFERENCES,0.5578078078078078,"= sk + h2œµ¬µ(3 + 4sk+1 + s2
k+1) + h2œµ2
(3 + s2
k ‚àíhu2
k)(1 ‚àí4h2œµ¬µ ‚àíh2œµ¬µsk+1)"
REFERENCES,0.5585585585585585,"+ s2
k(2 ‚àíhu2
k+1) + s2
k(1 + s2
k+1) ‚àíh2¬µ2(4 + sk+1)(3 + sk)
"
REFERENCES,0.5593093093093093,> sk + h2œµ2(1 ‚àí8h2¬µ2)
REFERENCES,0.56006006006006,Published as a conference paper at ICLR 2022
REFERENCES,0.5608108108108109,"where the inequality is achieved by sk+1 > ‚àí1, u2
k ‚â§4"
REFERENCES,0.5615615615615616,"h, u2
k+1 ‚â§2"
REFERENCES,0.5623123123123123,"h, 8h2¬µ2 < 1, œµ ‚â§2¬µ."
REFERENCES,0.5630630630630631,"For sk > ‚àí1,"
REFERENCES,0.5638138138138138,"sk+2 = sk + h2œµ¬µ(3 + 4sk + s2
k) + h2œµ2
(3 + s2
k ‚àíhu2
k)(1 + skh2œµ¬µ)"
REFERENCES,0.5645645645645646,"+ s2
k(3 + s2
k+1 ‚àíhu2
k+1) + h2¬µ2sk(3 + sk)
"
REFERENCES,0.5653153153153153,"> sk + h2œµ¬µ(3 + 4sk + s2
k) + h2œµ2
s2
k(1 + 2skh2¬µ2) + s2
k + h2¬µ2sk(3 + sk)
"
REFERENCES,0.566066066066066,"= sk + h2œµ¬µ(3 + 4sk + s2
k) + h2œµ2
2s2
k + h2¬µ2sk(3 + 2s2
k + sk)
"
REFERENCES,0.5668168168168168,"When sk > ‚àí1, we have sk + h2œµ¬µ(3 + 4sk + s2
k) > ‚àí1 and h2œµ¬µ(3 + 4sk + s2
k) > 0. Since
8h2¬µ2 < 1, 2s2
k + h2¬µ2sk(3 + 2s2
k + sk) ‚â•1 2 ‚àí3"
REFERENCES,0.5675675675675675,2h2¬µ2 > 0 for ‚àí1 < sk ‚â§‚àí1
REFERENCES,0.5683183183183184,2. For ‚àí1
REFERENCES,0.5690690690690691,"2 <
sk ‚â§0, 2s2
k + h2¬µ2sk(3 + 2s2
k + sk) ‚â•h2¬µ2sk(3 + 2s2
k + sk) ‚â•‚àí3"
REFERENCES,0.5698198198198198,2h2¬µ2 > ‚àí1
HENCE,0.5705705705705706,"2. Hence
sk+2 > min{‚àí1 + h2œµ2( 1 2 ‚àí3"
HENCE,0.5713213213213213,"2h2¬µ2), ‚àí1 2 ‚àí3"
HENCE,0.5720720720720721,2h4œµ2¬µ2} > ‚àí1.
HENCE,0.5728228228228228,"Lemma G.14. Given u2
k ‚â§2"
HENCE,0.5735735735735735,"h, when |x‚ä§
k yk| > ¬µ, u2
k+1 ‚àíu2
k < 0."
HENCE,0.5743243243243243,Proof.
HENCE,0.575075075075075,"u2
k+1 ‚àíu2
k = h(¬µ ‚àíx‚ä§
k yk)(4x‚ä§
k yk + hu2
k(¬µ ‚àíx‚ä§
k yk))"
HENCE,0.5758258258258259,"= h(¬µ ‚àíx‚ä§
k yk)4x‚ä§
k yk + h2u2
k(¬µ ‚àíx‚ä§
k yk)2"
HENCE,0.5765765765765766,"‚â§h(¬µ ‚àíx‚ä§
k yk)4x‚ä§
k yk + 2h(¬µ ‚àíx‚ä§
k yk)2"
HENCE,0.5773273273273273,"= 2h(¬µ2 ‚àí(x‚ä§
k yk)2) < 0"
HENCE,0.5780780780780781,"Lemma G.15. Given u2
k ‚â§2"
HENCE,0.5788288288288288,"h, u2
k+1 ‚àíu2
k ‚â§¬µ."
HENCE,0.5795795795795796,Proof.
HENCE,0.5803303303303303,"u2
k+1 ‚àíu2
k = h(¬µ ‚àíx‚ä§
k yk)(4x‚ä§
k yk + hu2
k(¬µ ‚àíx‚ä§
k yk)) ‚â§
4h¬µ2"
HENCE,0.581081081081081,"4 ‚àíhu2
k
,"
HENCE,0.5818318318318318,"where the maximum is achieved at x‚ä§
k yk =
2‚àíhu2
k
4‚àíhu2
k ¬µ. When h =
4
u2
0+4c¬µ, from the proof of"
HENCE,0.5825825825825826,"Lemma G.9, uk+1 ‚àíu2
k ‚â§¬µ. When h =
1
(2+c)¬µ and in this case u2
k ‚â§2"
HENCE,0.5833333333333334,h = (1 + c
HENCE,0.5840840840840841,"2¬µ), uk+1 ‚àíu2
k ‚â§"
HENCE,0.5848348348348348,"8
7(2+c)¬µ < ¬µ."
HENCE,0.5855855855855856,"Lemma G.16. When ‚àí¬µ ‚â§x‚ä§
k yk < ¬µ, if sk = 1‚àíhu2
k ‚àíh2x‚ä§
k yk(¬µ ‚àíx‚ä§
k yk) ‚â§‚àí1, u2
k+2 ‚àíu2
k ‚â§
‚àíh(4 ‚àí4h¬µ)(¬µ ‚àíx‚ä§
k yk)2 < 0."
HENCE,0.5863363363363363,Proof.
HENCE,0.5870870870870871,"u2
k+2 ‚àíu2
k = h(¬µ ‚àíx‚ä§
k yk)(((4 ‚àíhu2
k)x‚ä§
k yk + hu2
k¬µ) + s(hu2
k+1¬µ + (4 ‚àíhu2
k+1)(s(x‚ä§
k yk ‚àí¬µ) + ¬µ)))"
HENCE,0.5878378378378378,"= h(¬µ ‚àíx‚ä§
k yk)
 
‚àí(4 ‚àíhu2
k + s2(4 ‚àíhu2
k+1))(¬µ ‚àíx‚ä§
k yk) + 4(1 + s)¬µ
"
HENCE,0.5885885885885885,"‚â§‚àíh(8 ‚àíh(u2
k + u2
k+1))(¬µ ‚àíx‚ä§
k yk)2"
HENCE,0.5893393393393394,"‚â§‚àíh(4 ‚àí4h¬µ)(¬µ ‚àíx‚ä§
k yk)2 < 0"
HENCE,0.5900900900900901,"where the Ô¨Årst inequality is because when sk = ‚àí1 it has the maximum value; the second is from
Lemma G.10."
HENCE,0.5908408408408409,"Lemma G.17. When u2
k ‚â§2"
HENCE,0.5915915915915916,"h, then sk < 1 and will be away from 1 by a constant."
HENCE,0.5923423423423423,Published as a conference paper at ICLR 2022
HENCE,0.5930930930930931,Proof.
HENCE,0.5938438438438438,"sk = 1 ‚àíhu2
k ‚àíh2x‚ä§
k yk(¬µ ‚àíx‚ä§
k yk) ‚â§1 ‚àíhu2
k + h2 u2
k
2"
HENCE,0.5945945945945946,"
¬µ + u2
k
2 "
HENCE,0.5953453453453453,"where the equality is achieved when x‚ä§
k yk = ‚àíu2
k
2 ."
HENCE,0.5960960960960962,"Since 0 < u2
k ‚â§2 h,"
HENCE,0.5968468468468469,"sk ‚â§1 ‚àíhu2
k + h2 u2
k
2"
HENCE,0.5975975975975976,"
¬µ + u2
k
2"
HENCE,0.5983483483483484,"
‚â§max{1, h¬µ} < 1."
HENCE,0.5990990990990991,"Actually this sk will be away from 1 by a constant. This is because when u2
k are close to 0, sk will be
close to 1; however, u2
k will be increasing because of the decrease of |¬µ ‚àíx‚ä§
k yk| and thus sk will
decrease accordingly."
HENCE,0.5998498498498499,"Lemma G.18. When u2
k > 1"
HENCE,0.6006006006006006,"h + 2h¬µ2 and ‚àí¬µ ‚â§x‚ä§
k yk ‚â§3¬µ, (x‚ä§
k+1yk+1 ‚àí¬µ)(x‚ä§
k yk ‚àí¬µ) < 0,
meaning it oscillates around x‚ä§y = ¬µ."
HENCE,0.6013513513513513,Proof.
HENCE,0.6021021021021021,"1 ‚àíhu2
k ‚àíh2x‚ä§
k yk(¬µ ‚àíx‚ä§
k yk) < 1 ‚àíh( 1"
HENCE,0.6028528528528528,h + 2h¬µ2) + 2h2¬µ2 = 0
HENCE,0.6036036036036037,"Lemma G.19. When ‚àí¬µ ‚â§x‚ä§
k yk < ¬µ and h¬µ ‚â§1"
HENCE,0.6043543543543544,"3, we have |xk+1 ‚àíyk+1| < |xk ‚àíyk|."
HENCE,0.6051051051051051,Proof.
HENCE,0.6058558558558559,"|xk+1 ‚àíyk+1| = |xk ‚àíyk| |1 ‚àíh(¬µ ‚àíx‚ä§
k yk)|."
HENCE,0.6066066066066066,"Since h¬µ < 1, then"
HENCE,0.6073573573573574,"‚àí1 < 1 ‚àí2h¬µ ‚â§1 ‚àíh(¬µ ‚àíx‚ä§
k yk) < 1."
HENCE,0.6081081081081081,"Lemma G.20. When u2
k ‚â§1"
HENCE,0.6088588588588588,h + 2h¬µ2 and h¬µ ‚â§1
HENCE,0.6096096096096096,"3, sk ‚â•‚àí1 4."
HENCE,0.6103603603603603,"Proof. By h¬µ ‚â§1 3,"
HENCE,0.6111111111111112,"1 ‚àíhu2
k ‚àíh2x‚ä§
k yk(¬µ ‚àíx‚ä§
k yk) ‚â•‚àí2h2¬µ2 ‚àíh2¬µ2"
HENCE,0.6118618618618619,"4
‚â•‚àí9h2¬µ2 4
‚â•‚àí1 4."
HENCE,0.6126126126126126,"H
PROOF OF THEOREM 4.1, THEOREM 4.2, AND THEOREM 4.3"
HENCE,0.6133633633633634,"The proof in this section mainly follows the strategy of the scalar case. More precisely, all the
expressions, e.g., u2
k+1 and x‚ä§
k+1yk+1 ‚àí¬µ, in this rank-1 approximation case contains two parts: the
one that can be handled using the technique in scalar case, and the other one measuring the extent of
alignment between xk and yk. Again, let u2
k = ‚à•xk‚à•2 + ‚à•yk‚à•2."
HENCE,0.6141141141141141,"Theorem H.1 (Main 1). Let h =
4
u2
0+4c¬µ. Assume c ‚â•
‚àö"
HENCE,0.6148648648648649,"7 and u2
0 > 4
‚àö"
HENCE,0.6156156156156156,7¬µ. Then GD converges to
HENCE,0.6163663663663663,"a point in {(x, y) : ‚à•x‚à•2 + ‚à•y‚à•2 ‚â§2"
HENCE,0.6171171171171171,"h, x‚ä§y = ¬µ} except for a Lebesgue measure-0 set of (x0, y0)."
HENCE,0.6178678678678678,Proof. The proof follows from Lemma H.3 and Lemma H.8.
HENCE,0.6186186186186187,Published as a conference paper at ICLR 2022
HENCE,0.6193693693693694,"Theorem H.2 (Main 2). Let h =
4
4
‚àö"
HENCE,0.6201201201201201,"7¬µ+4c¬µ =
1
(
‚àö"
HENCE,0.6208708708708709,"7+c)¬µ. Assume c ‚â•
‚àö"
HENCE,0.6216216216216216,"7 and u2
0 ‚â§4
‚àö"
HENCE,0.6223723723723724,7¬µ. Then
HENCE,0.6231231231231231,"GD converges to a point in {(x, y) : ‚à•x‚à•2 + ‚à•y‚à•2 ‚â§2"
HENCE,0.6238738738738738,"h, x‚ä§y = ¬µ} except for a Lebesgue measure-0
set of (x0, y0)."
HENCE,0.6246246246246246,"Proof. Since c ‚â•
‚àö"
HENCE,0.6253753753753754,"7, in this case we have u2
0 ‚â§4
‚àö"
HENCE,0.6261261261261262,"7¬µ ‚â§2(
‚àö"
HENCE,0.6268768768768769,"7 + c)¬µ =
2
h. Therefore, the
convergence follows from Lemma H.8."
HENCE,0.6276276276276276,Hence we can show the proof of the theorems in Section 4.
HENCE,0.6283783783783784,"Proof of Theorem 4.1. By Lemma H.3 and H.7, the proof follows from Lemma H.9."
HENCE,0.6291291291291291,"Proof of Theorem 4.2 and 4.3. Similar to scalar case, h =
4
u2
0+4c¬µ for c ‚â•
‚àö"
HENCE,0.6298798798798799,"7 implies h ‚â§
4
u2
0+4
‚àö"
HENCE,0.6306306306306306,"7¬µ
and h =
1
(
‚àö"
HENCE,0.6313813813813813,"7+c)¬µ implies h ‚â§
1
2
‚àö"
HENCE,0.6321321321321322,"7. Therefore, the convergence and balancing of GD follow from"
HENCE,0.6328828828828829,"Theorem H.1 and H.2. The second inequality of Theorem 4.3 follows from x‚ä§y = ¬µ at the global
minimum because of diagonal and non-negative A and the best rank-1 approximation in SVD."
HENCE,0.6336336336336337,"H.1
PHASE 1"
HENCE,0.6343843843843844,The main theorem in phase 1 is the following.
HENCE,0.6351351351351351,"Lemma H.3. Given h =
4
u2
0+4c¬µ for c ‚â•
‚àö"
HENCE,0.6358858858858859,"7, GD will enter {‚à•x‚à•2 + ‚à•y‚à•2 ‚â§
2
h} except for a
measure-0 set of initial conditions."
HENCE,0.6366366366366366,"Proof. Since the expressions, u2
k+2‚àíu2
k and u2
k+2‚àíu2
k, of the rank-1 version have an additional term
upper bounded by c0((x‚ä§
k yk)2 ‚àíx‚ä§
k xky‚ä§
k yk) for some constant c0 > 0 which is non-positive, we
have u2
k decreases by a constant for either one- or two-step if (x‚ä§
k yk)2 ‚àíx‚ä§
k xky‚ä§
k yk < ‚àíc1 for some
constant c1 > 0 when 2"
HENCE,0.6373873873873874,"h < u2
k < 4"
HENCE,0.6381381381381381,"h. Therefore, we can pick a K > 0 s.t. (x‚ä§
k yk)2 ‚àíx‚ä§
k xky‚ä§
k yk <
‚àíc1 for k ‚â§K. If GD enters {‚à•x‚à•2 + ‚à•y‚à•2 ‚â§2"
HENCE,0.6388888888888888,"h} within K steps, then we are done. Otherwise,
we can just assume that for all the k > K, ‚àíc1 ‚â§(x‚ä§
k yk)2 ‚àíx‚ä§
k xky‚ä§
k yk ‚â§0. Then again, like"
HENCE,0.6396396396396397,"the scalar case, we only need to consider when xk, yk are close to x‚ä§
k yk = ¬µ and x‚ä§
k yk = ‚àíh¬µu2
k
4‚àíhu2
k ."
HENCE,0.6403903903903904,"For the region near x‚ä§
k yk = ‚àíh¬µu2
k
4‚àíhu2
k , we can always choose a c1 s.t. there exists a Œ¥ > 0, when"
HENCE,0.6411411411411412,"‚àíŒ¥ ‚â§x‚ä§
k yk +
h¬µu2
k
4‚àíhu2
k < 0, we have both x‚ä§
k+1yk+1 > ¬µ and sk < 0. Then all the discussion just"
HENCE,0.6418918918918919,"follows the scalar case. For the region near x‚ä§
k yk = ¬µ, i.e., |x‚ä§
k yk ‚àí¬µ| < œµ for some 0 < œµ < ¬µ,
consider"
HENCE,0.6426426426426426,"x‚ä§
k+1yk+1 ‚àí¬µ = (x‚ä§
k yk ‚àí¬µ)sk + h2(2¬µ ‚àíx‚ä§
k yk)((x‚ä§
k yk)2 ‚àíx‚ä§
k xky‚ä§
k yk)."
HENCE,0.6433933933933934,"Similar to scalar decomposition, we only need to consider the case when sk ‚â§‚àí1 because if sk > ‚àí1
which can only happen when x‚ä§
k yk > ¬µ, we have u2
k ‚â§2"
HENCE,0.6441441441441441,"h + œµkh(¬µ + œµ) and then u2
k+1 ‚â§2"
HENCE,0.6448948948948949,"h. If
sk ‚â•‚àí1 for all the following iterations, together with the bounded second term in x‚ä§
k+1yk+1 ‚àí¬µ,
i.e., ‚àí2h2¬µc1 ‚â§h2(2¬µ ‚àíx‚ä§
k yk)((x‚ä§
k yk)2 ‚àíx‚ä§
k xky‚ä§
k yk) ‚â§0, we have that there exists K > 0 s.t.
|x‚ä§
KyK ‚àí¬µ| ‚â•œµ1 for some œµ1 > 0 unless it leaves this region |x‚ä§
k yk ‚àí¬µ| < œµ. Therefore, by the
same discussion in scalar case, u2
k will decrease either in one step or two steps by at least a constant."
HENCE,0.6456456456456456,"Moreover, by Lemma H.4, GD will not terminate inside { 2"
HENCE,0.6463963963963963,h < ‚à•x‚à•2 + ‚à•y‚à•2 < 4
HENCE,0.6471471471471472,"h} in Ô¨Ånite step.
Thus, GD is guaranteed to enter {‚à•x‚à•2 + ‚à•y‚à•2 ‚â§2"
HENCE,0.6478978978978979,h} in Ô¨Ånite steps.
HENCE,0.6486486486486487,Lemma H.4. GD will not converge to any Ô¨Åxed points in {‚à•x‚à•2 +‚à•y‚à•2 > 2
HENCE,0.6493993993993994,"h} except for a measure-0
set."
HENCE,0.6501501501501501,"Proof. Similar to the discussion in scalar case, the set of points converging to x‚ä§y = ¬µ in Ô¨Ånite step
is measure-0."
HENCE,0.6509009009009009,Published as a conference paper at ICLR 2022
HENCE,0.6516516516516516,"Also, since ¬µx‚ä§y = x‚ä§xy‚ä§y = ¬µ2, the Ô¨Åxed points of GD map requires x‚ä§xy‚ä§y = (x‚ä§y)2. This
can be seen in the following expression"
HENCE,0.6524024024024024,"x‚ä§
k+1yk+1 ‚àí¬µ = (x‚ä§
k yk ‚àí¬µ)(1 ‚àíhu2
k ‚àíh2x‚ä§
k yk(¬µ ‚àíx‚ä§
k yk)) + h2(2¬µ ‚àíx‚ä§
k yk)((x‚ä§
k yk)2 ‚àíx‚ä§
k xky‚ä§
k yk),"
HENCE,0.6531531531531531,"where the convergence of x‚ä§
k yk ‚àí¬µ requires the convergence of x‚ä§
k xky‚ä§
k yk ‚àí(x‚ä§
k yk)2."
HENCE,0.6539039039039038,"Hence if x‚ä§
k xky‚ä§
k yk ‚àí(x‚ä§
k yk)2 does not converge, then GD will not converge. If x‚ä§
k xky‚ä§
k yk ‚àí
(x‚ä§
k yk)2 converges, then c ‚â•x‚ä§
k xky‚ä§
k yk ‚àí(x‚ä§
k yk)2 ‚â•0 for all the rest of the iterations with some
c > 0, and we further assume |x‚ä§
k yk ‚àí¬µ| < œµ for any 0 < œµ < ¬µ. As is shown in scalar case, when
u2
k ‚â•2"
HENCE,0.6546546546546547,"h + œµh(¬µ + œµ), we have sk = 1 ‚àíhu2
k ‚àíh2x‚ä§
k yk(¬µ ‚àíx‚ä§
k yk) ‚â•‚àí1. Then"
HENCE,0.6554054054054054,"x‚ä§
k+1yk+1 ‚àí¬µ = (x‚ä§
k yk ‚àí¬µ)sk + h2(2¬µ ‚àíx‚ä§
k yk)((x‚ä§
k yk)2 ‚àíx‚ä§
k xky‚ä§
k yk)"
HENCE,0.6561561561561562,will never converge (because sk ‚â§‚àí1 and the second term is bounded).
HENCE,0.6569069069069069,"Lemma H.5. When x‚ä§
k yk > ¬µ or x‚ä§
k yk < ‚àíh¬µu2
k
4‚àíhu2
k , we have u2
k+1 ‚àíu2
k < 0."
HENCE,0.6576576576576577,Proof.
HENCE,0.6584084084084084,"u2
k+1 ‚àíu2
k = h(¬µ ‚àíx‚ä§
k yk)((4 ‚àíhu2
k)x‚ä§
k yk + hu2
k¬µ) + h(4 ‚àíhu2
k)((x‚ä§
k yk)2 ‚àíx‚ä§
k xky‚ä§
k yk)"
HENCE,0.6591591591591591,"‚â§h(¬µ ‚àíx‚ä§
k yk)((4 ‚àíhu2
k)x‚ä§
k yk + hu2
k¬µ)."
HENCE,0.6599099099099099,Hence the proof follows 1D case.
HENCE,0.6606606606606606,"Lemma H.6. When ‚àíh¬µu2
k
4‚àíhu2
k ‚â§x‚ä§
k yk < ¬µ, we have u2
k+2 ‚àíu2
k < 0."
HENCE,0.6614114114114115,"Proof. Let sk = 1 ‚àíhu2
k ‚àíh2x‚ä§
k yk(¬µ ‚àíx‚ä§
k yk), the same as 1D."
HENCE,0.6621621621621622,"x‚ä§
k+1yk+1 ‚àí¬µ = (x‚ä§
k yk ‚àí¬µ)(1 ‚àíhu2
k ‚àíh2x‚ä§
k yk(¬µ ‚àíx‚ä§
k yk)) + h2(2¬µ ‚àíx‚ä§
k yk)((x‚ä§
k yk)2 ‚àíx‚ä§
k xky‚ä§
k yk)"
HENCE,0.6629129129129129,"‚àÜ= (x‚ä§
k yk ‚àí¬µ)sk + M."
HENCE,0.6636636636636637,"u2
k+2 ‚àíu2
k = u2
k+1 ‚àíu2
k + u2
k+2 ‚àíu2
k+1
= h(¬µ ‚àíx‚ä§
k yk)((4 ‚àíhu2
k)x‚ä§
k yk + hu2
k¬µ) + h(4 ‚àíhu2
k)((x‚ä§
k yk)2 ‚àíx‚ä§
k xky‚ä§
k yk)"
HENCE,0.6644144144144144,"+ h(¬µ ‚àíx‚ä§
k+1yk+1)((4 ‚àíhu2
k+1)x‚ä§
k+1yk+1 + hu2
k+1¬µ)"
HENCE,0.6651651651651652,"+ h(4 ‚àíhu2
k+1)((x‚ä§
k+1yk+1)2 ‚àíx‚ä§
k+1xk+1y‚ä§
k+1yk+1)"
HENCE,0.6659159159159159,"=

h(¬µ ‚àíx‚ä§
k yk)(((4 ‚àíhu2
k)x‚ä§
k yk + hu2
k¬µ) + sk(hu2
k+1¬µ + (4 ‚àíhu2
k+1)(sk(x‚ä§
k yk ‚àí¬µ) + ¬µ)))
"
HENCE,0.6666666666666666,"+

h(4 ‚àíhu2
k)((x‚ä§
k yk)2 ‚àíx‚ä§
k xky‚ä§
k yk) + hM 2(‚àí4 + hu2
k+1)"
HENCE,0.6674174174174174,"+ hM(‚àíhu2
k+1¬µ + (‚àí4 + hu2
k+1)¬µ + 2(4 ‚àíhu2
k+1)(¬µ ‚àíx‚ä§
k yk)sk)
"
HENCE,0.6681681681681682,‚àÜ= I + II.
HENCE,0.668918918918919,"I is the same as 1D u2
k+2 ‚àíu2
k and hence (1) < 0. For II,"
HENCE,0.6696696696696697,"II = hM 2(‚àí4 + hu2
k+1) + h((x‚ä§
k yk)2 ‚àíx‚ä§
k xky‚ä§
k yk)"
HENCE,0.6704204204204204,"√ó (4 ‚àíhu2 + h2(2¬µ ‚àíx‚ä§
k yk)(‚àíhu2
k+1¬µ + (‚àí4 + hu2
k+1)¬µ + 2(4 ‚àíhu2
k+1)(¬µ ‚àíx‚ä§
k yk)sk)
|
{z
}
III )."
HENCE,0.6711711711711712,"From 1D results, sk < 0. Then, III achieves its lower bound at x‚ä§
k yk = ¬µ or x‚ä§
k yk = ‚àíh¬µu2
k
4‚àíhu2
k . For"
HENCE,0.6719219219219219,"x‚ä§
k yk = ¬µ,"
HENCE,0.6726726726726727,"III = 4 ‚àíhu2
k ‚àí4h2¬µ2 ‚â•4h¬µ(c ‚àíh¬µ) > 0,"
HENCE,0.6734234234234234,Published as a conference paper at ICLR 2022
HENCE,0.6741741741741741,"where the inequality is from u2
k ‚â§u2
0 = 4"
HENCE,0.674924924924925,h ‚àí4c¬µ and c > h¬µ.
HENCE,0.6756756756756757,"For x‚ä§
k yk = ‚àíh¬µu2
k
4‚àíhu2
k ,"
HENCE,0.6764264264264265,"III = 4 ‚àíhu2
k ‚àí4h2(8 ‚àíhu2
k)(4 ‚àíhu2
k ‚àí2sk(4 ‚àíhu2
k+1))¬µ2"
HENCE,0.6771771771771772,"(4 ‚àíhu2
k)2"
HENCE,0.6779279279279279,"‚â•4 ‚àíhu2
k ‚àí4h2(8 ‚àíhu2
k)(4 ‚àíhu2
k ‚àí2sk(4 ‚àíhu2
k))¬µ2"
HENCE,0.6786786786786787,"(4 ‚àíhu2
k)2"
HENCE,0.6794294294294294,"= 4 ‚àíhu2
k ‚àí4h2(8 ‚àíhu2
k)(1 ‚àí2sk)¬µ2"
HENCE,0.6801801801801802,"4 ‚àíhu2
k"
HENCE,0.6809309309309309,"u2
k‚â§u2
0
‚â•
4 ‚àíhu2
k ‚àí4h2(4 + 4ch¬µ)(1 ‚àí2sk)¬µ2 4ch¬µ"
HENCE,0.6816816816816816,"u2
k‚â§u2
0
‚â•
4h¬µ"
HENCE,0.6824324324324325,c (‚àí1 + c2 + 2sk + ch(‚àí1 + 2sk)¬µ)
HENCE,0.6831831831831832,"‚àó
‚â•c2(1 + 8h2¬µ2) + c(h¬µ ‚àíh3¬µ3"
HENCE,0.683933933933934,"2
) ‚àí7 ‚àíh2¬µ2"
HENCE,0.6846846846846847,"2
‚àó‚àó
‚â•0,
where ‚àófollows from
sk = 1 ‚àíhu2
k ‚àíh2x‚ä§
k yk(¬µ ‚àíx‚ä§
k yk)"
HENCE,0.6854354354354354,"x‚ä§
k yk=¬µ/2
‚â•
1 ‚àíhu2
k ‚àíh2¬µ2 4"
HENCE,0.6861861861861862,"u2
k‚â§u2
0
‚â•
‚àí3 + 4ch¬µ ‚àíh2¬µ2 4
,"
HENCE,0.6869369369369369,"and ‚àó‚àófollows from c ‚â•
‚àö 7."
HENCE,0.6876876876876877,"From 1D discussion, ‚àí4 + hu2
k+1 < 0. Also, (x‚ä§
k yk)2 ‚àíx‚ä§
k xky‚ä§
k yk < 0. Hence we have (3) ‚â•0."
HENCE,0.6884384384384384,"Overall, (2) ‚â§0 and therefore the whole discussion of (1) is the same as 1D."
HENCE,0.6891891891891891,"H.2
PHASE 2"
HENCE,0.68993993993994,"The proof in phase 2 is partly different from the scalar case due to the alignment. Before presenting
the main convergence lemma, we Ô¨Årst state the boundedness of GD in the following lemma.
Lemma H.7. Once GD enters {‚à•x‚à•2 + ‚à•y‚à•2 ‚â§2"
HENCE,0.6906906906906907,"h}, it will stay bounded inside {‚à•x‚à•2 + ‚à•y‚à•2 ‚â§
2
h + 2h¬µ2(1 +
1
1‚àíh2¬µ2 )} and re-enter {‚à•x‚à•2 + ‚à•y‚à•2 ‚â§2"
HENCE,0.6914414414414415,"h} within Ô¨Ånite steps where the number of
such steps do not depend on the number of iteration."
HENCE,0.6921921921921922,"Proof. Assume u2
k ‚â§2"
HENCE,0.6929429429429429,h. Then
HENCE,0.6936936936936937,"u2
k+1 ‚àíu2
k = h(¬µ ‚àíx‚ä§
k yk)((4 ‚àíhu2
k)x‚ä§
k yk + hu2
k¬µ) + h(4 ‚àíhu2
k)((x‚ä§
k yk)2 ‚àíx‚ä§
k xky‚ä§
k yk)"
HENCE,0.6944444444444444,"‚â§2h(¬µ2 ‚àíx‚ä§
k xky‚ä§
k yk) ‚â§2h¬µ2,"
HENCE,0.6951951951951952,"where the Ô¨Årst inequality follows from u2
k ‚â§2"
HENCE,0.6959459459459459,"h. If further u2
k+1 ‚â§2"
HENCE,0.6966966966966966,"h + 2h¬µ2, then by x‚ä§
k+1yk+1 ‚â• ‚àí
q"
HENCE,0.6974474474474475,"x‚ä§
k+1xk+1y‚ä§
k+1yk+1 and ‚àí1 + h2¬µ2 < 0, we have"
HENCE,0.6981981981981982,"u2
k+2 ‚àíu2
k+1 = h(¬µ ‚àíx‚ä§
k+1yk+1)((4 ‚àíhu2
k+1)x‚ä§
k+1yk+1 + hu2
k+1¬µ)"
HENCE,0.698948948948949,"+ h(4 ‚àíhu2
k+1)((x‚ä§
k+1yk+1)2 ‚àíx‚ä§
k+1xk+1y‚ä§
k+1yk+1)"
HENCE,0.6996996996996997,"‚â§2h(x‚ä§
k+1xk+1y‚ä§
k+1yk+1(‚àí1 + h2¬µ2) + ¬µ2(1 ‚àí2x‚ä§
k+1yk+1h2¬µ + h2¬µ2))"
HENCE,0.7004504504504504,"‚â§2h(x‚ä§
k+1xk+1y‚ä§
k+1yk+1(‚àí1 + h2¬µ2) + ¬µ2(1 + 2
q"
HENCE,0.7012012012012012,"x‚ä§
k+1xk+1y‚ä§
k+1yk+1h2¬µ + h2¬µ2))"
HENCE,0.7019519519519519,"‚â§
2h¬µ2"
HENCE,0.7027027027027027,1 ‚àíh2¬µ2 .
HENCE,0.7034534534534534,Published as a conference paper at ICLR 2022
HENCE,0.7042042042042042,"The Ô¨Årst inequality is from u2
k+1 ‚â§2"
HENCE,0.704954954954955,h + 2h¬µ2; the second inequality follows from the two conditions
HENCE,0.7057057057057057,"mentioned above; the third inequality follows from choosing
q"
HENCE,0.7064564564564565,"x‚ä§
k+1xk+1y‚ä§
k+1yk+1 =
h2¬µ3"
HENCE,0.7072072072072072,1‚àíh2¬µ2 .
HENCE,0.7079579579579579,"From previous proof, we know when u2
k >
2
h, by Lemma H.3, we have GD will enter {‚à•x‚à•2 +
‚à•y‚à•2 ‚â§
2
h} in Ô¨Ånite steps with the decrease of u2
k in either one step or two steps. Then, u2
i ‚â§
2
h + 2h¬µ2(1 +
1
1‚àíh2¬µ2 ) for all i ‚â•k."
HENCE,0.7087087087087087,"Let Uk = x‚ä§
k yk, Vk = x‚ä§
k xk, Wk = y‚ä§
k yk. Then we can rewrite the iteration for the three variables
Ô£±
Ô£≤ Ô£≥"
HENCE,0.7094594594594594,"Uk+1 = h(1 ‚àíhVk)¬µWk + h¬µVk(1 ‚àíhWk) + Uk(1 + h2VkWk ‚àíh(Vk + Wk) + h2¬µ2)
Vk+1 = 2h¬µUk + Vk ‚àí2hVkWk + h2(¬µ2Wk ‚àí2¬µUkWk + VkW 2
k )
Wk+1 = 2h¬µUk + Wk ‚àí2hVkWk + h2(¬µ2Vk ‚àí2¬µUkVk + V 2
k Wk)
. (10)"
HENCE,0.7102102102102102,"When h =
4
u2
0+4c¬µ or h =
1
2
‚àö"
HENCE,0.710960960960961,"7¬µ, we have h¬µ ‚â§
1
2
‚àö"
HENCE,0.7117117117117117,7. The following is the main lemma in phase 2.
HENCE,0.7124624624624625,"Lemma H.8. Given h¬µ ‚â§
1
2
‚àö"
HENCE,0.7132132132132132,"7, if GD enters {‚à•x‚à•2 +‚à•y‚à•2 ‚â§2"
HENCE,0.713963963963964,"h}, it converges to x‚ä§y = ‚à•x‚à•‚à•y‚à•=
¬µ, i.e., the global minimum, inside this region except for a measure-0 set of initial conditions."
HENCE,0.7147147147147147,"Proof. This proof follows the same method as the scalar decomposition. We need to prove that all
the conclusions in scalar case also holds in this rank one decomposition."
HENCE,0.7154654654654654,"Consider u2
k+1 ‚àíu2
k when x‚ä§
k yk < ‚àí¬µ or x‚ä§
k yk > ¬µ,"
HENCE,0.7162162162162162,"u2
k+1 ‚àíu2
k = h(¬µ ‚àíx‚ä§
k yk)((4 ‚àíhu2
k)x‚ä§
k yk + hu2
k¬µ) + h(4 ‚àíhu2
k)((x‚ä§
k yk)2 ‚àíx‚ä§
k xky‚ä§
k yk)"
HENCE,0.7169669669669669,"= h(¬µ ‚àíx‚ä§
k yk)((4 ‚àíhu2
k)x‚ä§
k yk + hu2
k¬µ) + h(4 ‚àíhu2
k)(U 2
k ‚àíVkWk)"
HENCE,0.7177177177177178,"‚â§h(¬µ ‚àíx‚ä§
k yk)((4 ‚àíhu2
k)x‚ä§
k yk + hu2
k¬µ),"
HENCE,0.7184684684684685,"where the last expression is the same as scalar case and therefore under the conditions such that
u2
k+1 ‚àíu2
k < 0 in scalar case, we also have u2
k+1 ‚àíu2
k < 0 in rank-one approximation."
HENCE,0.7192192192192193,"As in scalar case, we also denote sk = 1 ‚àíhu2
k ‚àíh2x‚ä§
k yk(¬µ ‚àíx‚ä§
k yk). First consider 1"
HENCE,0.71996996996997,"h + 2h¬µ2 <
u2
k ‚â§2"
HENCE,0.7207207207207207,"h. If ‚àí¬µ ‚â§x‚ä§
k yk < ¬µ, u2
k+1 ‚â•u2
k, and sk ‚â§‚àí1, consider u2
k+2 ‚àíu2
k. From Lemma H.6, the
extra terms of u2
k+2 ‚àíu2
k compared to scalar case is"
HENCE,0.7214714714714715,"hM 2(‚àí4 + hu2
k+1) + h((x‚ä§
k yk)2 ‚àíx‚ä§
k xky‚ä§
k yk)"
HENCE,0.7222222222222222,"√ó (4 ‚àíhu2 + h2(2¬µ ‚àíx‚ä§
k yk)(‚àíhu2
k+1¬µ + (‚àí4 + hu2
k+1)¬µ + 2(4 ‚àíhu2
k+1)(¬µ ‚àíx‚ä§
k yk)sk)
|
{z
}
III )"
HENCE,0.722972972972973,"where M = h2(2¬µ ‚àíx‚ä§
k yk)((x‚ä§
k yk)2 ‚àíx‚ä§
k xky‚ä§
k yk). Then we will prove that III is positive and
then all the extra terms will be less than 0. By rewriting III in terms of x‚ä§
k yk, we get"
HENCE,0.7237237237237237,"III = 2(x‚ä§
k yk)2h2sk(4 ‚àíhu2
k+1) + x‚ä§
k yk(h3u2
k+1¬µ ‚àí6h2sk(4 ‚àíhu2
k+1)¬µ ‚àíh2(‚àí4 + hu2
k+1)¬µ)"
HENCE,0.7244744744744744,"‚àí2h3u2
k+1¬µ2 + 4h2sk(4 ‚àíhu2
k+1)¬µ2 + 2h2(‚àí4 + hu2
k+1)¬µ2 + 4 ‚àíhu2
k
‚â•4 ‚àíhu2
k + 12h2(‚àí1 + 4sk)¬µ2 ‚àí12h3sku2
k+1¬µ2"
HENCE,0.7252252252252253,"‚â•4 ‚àíhu2
k + 12h2(‚àí1 + 4sk)¬µ2 ‚àí12h3sku2
k¬µ2"
HENCE,0.725975975975976,"= 4 ‚àíhu2
k + 12h2(‚àí1 + sk(4 ‚àíhu2
k))¬µ2 > 0,"
HENCE,0.7267267267267268,"where the Ô¨Årst inequality is because it obtains the minimum at x‚ä§
k yk = ‚àí¬µ; the second inequality
follows from u2
k+1 ‚â•u2
k; the last inequality is from the range of u2
k and sk. Therefore the extra
terms of u2
k+2 ‚àíu2
k is always negative and then we have the same conclusion as scalar case, i.e.,
u2
k+2 ‚àíu2
k < ‚àíC(¬µ ‚àíx‚ä§
k yk)2 for some constant C > 0."
HENCE,0.7274774774774775,Published as a conference paper at ICLR 2022
HENCE,0.7282282282282282,"Next, consider sk+1 when ‚àí¬µ ‚â§x‚ä§
k yk ‚â§3¬µ and 1"
HENCE,0.728978978978979,"h + 2h¬µ2 < u2
k ‚â§2"
HENCE,0.7297297297297297,h which implies ‚àí1 ‚àíh2¬µ2
HENCE,0.7304804804804805,"4
‚â§
sk < 0. The extra terms of sk+1 compared to scalar case is"
HENCE,0.7312312312312312,"h2(U 2
k ‚àíVkWk)

‚àí(4 ‚àíhu2
k) + (‚àíx‚ä§
k yk + 2¬µ)(2h2sk(x‚ä§
k yk ‚àí¬µ) + h2¬µ) + h4x(‚àíx‚ä§
k yk + 2¬µ)2"
HENCE,0.7319819819819819,"|
{z
}
IV ,"
HENCE,0.7327327327327328,"where U 2
k ‚àíVkWk ‚â§0. Then consider the upper bound of IV"
HENCE,0.7334834834834835,"IV = ‚àí4 ‚àí2(x‚ä§
k yk)2h2sk + x‚ä§
k yk(‚àíh2¬µ + 6h2s¬µ) + hu2
k + 2h2¬µ2 ‚àí4h2sk¬µ2"
HENCE,0.7342342342342343,"‚â§‚àí4 + hu2
k + (‚àíh2 ‚àí4h2sk)¬µ2 < 0,"
HENCE,0.734984984984985,"where the Ô¨Årst inequality follows from x‚ä§
k yk ‚â§3¬µ. Therefore the extra terms is non-negative, namely
all the lower bound for sk+1 in the scalar case also holds for rank-one approximation."
HENCE,0.7357357357357357,"Also we need to consider sk+2 when sk > ‚àí1, ‚àí¬µ ‚â§x‚ä§
k yk ‚â§3¬µ, and 1"
HENCE,0.7364864864864865,"h + 2h¬µ2 < u2
k ‚â§2"
HENCE,0.7372372372372372,"h. For
scalar case, from Lemma G.13 with œµ = x‚ä§
k+1yk+1 ‚àí¬µ, we have"
HENCE,0.737987987987988,"sk+2 = sk+1 + (h2(3 + s2
k+1) ‚àíh3u2
k)œµ2 + h2œµ(‚àí3¬µ ‚àísk+1¬µ)"
HENCE,0.7387387387387387,"= 3h2œµ2 + h2s2
k+1œµ2 ‚àíh3u2
kœµ2 ‚àí3h2œµ¬µ + sk+1(1 ‚àíh2œµ¬µ)."
HENCE,0.7394894894894894,Since the minimum point w.r.t. sk+1 is ‚àí(1‚àíh2œµ¬µ)
HENCE,0.7402402402402403,"2h2œµ2
< ‚àí1 ‚àíh2¬µ2"
HENCE,0.740990990990991,"4 , we have if sk+1 is larger, then sk+2
is also larger. Given the same value for sk for both scalar and rank-one cases, sk+1 is larger from
the above discussion and thus sk+2 is also larger, namely, the lower bound for sk+2 also holds in
rank-one cases."
HENCE,0.7417417417417418,"Next consider u2
k ‚â§1"
HENCE,0.7424924924924925,"h + 2h¬µ2 where |sk| < 1 and we need the iteration to have certain restriction
such that it will be bounded in a region. If sk < 0, then it follows the same from the above
discussion. If sk > 0 and x‚ä§
k yk > ¬µ or x‚ä§
k yk < ‚àí¬µ, then by u2
k+1 ‚àíu2
k < 0, it will stay in
{x‚ä§
k yk > ¬µ, u2
k ‚â§1"
HENCE,0.7432432432432432,"h + 2h¬µ2} until convergence. If sk > 0 and ‚àí¬µ ‚â§x‚ä§
k yk < ¬µ, similar to scalar
case, we consider |x‚ä§
k xk ‚àíy‚ä§
k yk| = |Vk ‚àíWk| and"
HENCE,0.743993993993994,"|Vk+1 ‚àíWk+1| = |Vk ‚àíWk| ¬∑ |1 ‚àíh2VkWk + 2h2Uk¬µ ‚àíh2¬µ2|,"
HENCE,0.7447447447447447,"where Uk = x‚ä§
k yk and U 2
k ‚â§VkWk. Then"
HENCE,0.7454954954954955,"1 ‚àíh2VkWk + 2h2Uk¬µ ‚àíh2¬µ2 ‚â§1 ‚àíh2U 2
k + 2h2Uk¬µ ‚àíh2¬µ2 < 1,"
HENCE,0.7462462462462462,"1 ‚àíh2VkWk + 2h2Uk¬µ ‚àíh2¬µ2 ‚â•1 ‚àíh2 u4
k
4 ‚àí2h2¬µ ‚àíh2¬µ2 ‚â•1 ‚àí1"
HENCE,0.746996996996997,4(1 + 2h2¬µ2)2 ‚àí3h2¬µ2 > 0.
HENCE,0.7477477477477478,"Therefore if ‚àí¬µ ‚â§x‚ä§
k yk < ¬µ and u2
k ‚â§
1
h + 2h¬µ2, we have |Vk+1 ‚àíWk+1| < |Vk ‚àíWk|,
meaning together with 0 ‚â§¬µ ‚àíx‚ä§
k+1yk+1 < ¬µ ‚àíx‚ä§
k yk, xk+1 and yk+1 is bounded in the monotone
convergence region."
HENCE,0.7484984984984985,"So far we retrieve all the conditions for convergence. Therefore, from the proof of the convergence in
scalar case, we have that there exists a constant K > 0, s.t., |sk| < 1 and 0 < 2¬µ ‚àíx‚ä§
k yk < 2¬µ for
all k > K. Then let Sk = h2(2¬µ ‚àíx‚ä§
k yk) and we have"
HENCE,0.7492492492492493,"x‚ä§
k+1yk+1 ‚àí¬µ = (x‚ä§
k yk ‚àí¬µ)(1 ‚àíhu2
k ‚àíh2x‚ä§
k yk(¬µ ‚àíx‚ä§
k yk))"
HENCE,0.75,"+ h2(2¬µ ‚àíx‚ä§
k yk)((x‚ä§
k yk)2 ‚àíx‚ä§
k xky‚ä§
k yk)"
HENCE,0.7507507507507507,"= (x‚ä§
k yk ‚àí¬µ)sk + Sk(U 2
k ‚àíVkWk)"
HENCE,0.7515015015015015,"In fact x‚ä§
k yk ‚àí¬µ ‚Üí0 as k ‚Üí0. First, since Sk(U 2
k ‚àíVkWk) ‚Üí0 as k ‚Üí0 by Lemma H.9 and
|sk| < 1, we have x‚ä§
k yk ‚àí¬µ is bounded. Also, from previous discussion, |sk| ‚â§max{C2, 1 ‚àí
C3(x‚ä§
k yk ‚àí¬µ)2} for some constant C2, C3 > 0. For any œµ0 > 0, there exists K1 > 0 and œµ1 ‚â™œµ0
such that |Sk(U 2
k ‚àíVkWk)| < œµ1 for all k > K1 and C2 ‚â§1 ‚àíC3œµ2
1. Therefore x‚ä§
k yk ‚àí¬µ will"
HENCE,0.7522522522522522,Published as a conference paper at ICLR 2022
HENCE,0.753003003003003,decreases to O(œµ1). This is because
HENCE,0.7537537537537538,"|x‚ä§
k+1yk+1 ‚àí¬µ| ‚â§(1 ‚àíC3œµ2
1)|x‚ä§
k yk ‚àí¬µ| + 2¬µ|U 2
K1 ‚àíVK1WK1|C2(k‚àíK1)
0
‚â§|(x‚ä§
K1yK1 ‚àí¬µ)|(1 ‚àíC3œµ2
1)k‚àíK1"
HENCE,0.7545045045045045,"+ 2h2¬µ(VK1WK1 ‚àíU 2
K1) k
X"
HENCE,0.7552552552552553,"i=K1
C2(i‚àíK1)
0
(1 ‚àíC3œµ2
1)k‚àíi"
HENCE,0.756006006006006,"‚â§|(x‚ä§
K1yK1 ‚àí¬µ)|(1 ‚àíC3œµ2
1)k‚àíK1 + 2h2¬µ(VK1WK1 ‚àíU 2
K1)(k ‚àíK1)Ck‚àíK1
4"
HENCE,0.7567567567567568,"where C4 = max{C0, 1 ‚àíC3œµ2
1} < 1 and we have kCk
4 ‚Üí0 as k ‚Üí‚àû. Therefore there exists
K2 ‚â•K1 s.t. x‚ä§
K2yK2 ‚àí¬µ = O(œµ1). Then we will show for all k > K2, |x‚ä§
k yk ‚àí¬µ| < œµ0. As is
shown before there exists K3 > K2 and œµ2 ‚â™œµ1 s.t. |Sk(U 2
k ‚àíVkWk)| < œµ2. Since |sk| < 1, the
increase of |x‚ä§
k yk ‚àí¬µ| is also O(œµ1) and thus |x‚ä§
k yk ‚àí¬µ| = O(œµ1) < œµ0 for k > K2. Then we have
the convergence of |x‚ä§
k yk ‚àí¬µ| ‚Üí0."
HENCE,0.7575075075075075,"In general, we have x‚ä§
k yk ‚Üí¬µ and | cos(‚à†(xk, yk))| ‚Üí0, which implies GD converges to the global
minimum x‚ä§y = ‚à•x‚à•‚à•y‚à•= ¬µ."
HENCE,0.7582582582582582,"For the proof of alignment, we only need to consider when ‚à•xk‚à•2 +‚à•yk‚à•2 ‚â§2"
HENCE,0.759009009009009,"h +2h¬µ2(1+
1
1‚àíh2¬µ2 ).
Also, we have VkWk ‚àíU 2
k ‚â•0 and"
HENCE,0.7597597597597597,"Vk+1Wk+1 ‚àíU 2
k+1 = (VkWk ‚àíU 2
k)(1 ‚àíh(Vk + Wk) + h2(VkWk ‚àí¬µ2))2."
HENCE,0.7605105105105106,"Thus we denote lk = 1 ‚àíh(Vk + Wk) + h2(VkWk ‚àí¬µ2) which characterizes the change of
VkWk ‚àíU 2
k."
HENCE,0.7612612612612613,Lemma H.9. If ‚à•xk‚à•2 + ‚à•yk‚à•2 ‚â§2
HENCE,0.762012012012012,"h + 2h¬µ2(1 +
1
1‚àíh2¬µ2 ), then VkWk ‚àíU 2
k converges to 0 and
also | cos(‚à†(xk, yk))| converges to 1."
HENCE,0.7627627627627628,Proof. By 0 ‚â§VkWk ‚â§( Vk+Wk
HENCE,0.7635135135135135,"2
)2 and the boundedness theorem, we have"
HENCE,0.7642642642642643,Vk + Wk ‚â§2
HENCE,0.765015015015015,"h + 2h¬µ2(1 +
1
1 ‚àíh2¬µ2 ), and"
HENCE,0.7657657657657657,"lk = 1 ‚àíh(Vk + Wk) + h2(VkWk ‚àí¬µ2) ‚â•‚àí1 ‚àíh2¬µ2(3 +
2
1 ‚àíh2¬µ2 ),
(11)"
HENCE,0.7665165165165165,lk = 1 ‚àíh(Vk + Wk) + h2(VkWk ‚àí¬µ2) ‚â§h2
HENCE,0.7672672672672672,4 (Vk + Wk)2 ‚àíh(Vk + Wk) + 1 ‚àíh2¬µ2
HENCE,0.7680180180180181,‚â§1 ‚àíh2¬µ2 < 1.
HENCE,0.7687687687687688,"We will show that there exists K > 0 and a constant C > 0 only depending on h¬µ, s.t. when k > K,
|lk| < 1 ‚àíC."
HENCE,0.7695195195195195,If Vk + Wk < 2
HENCE,0.7702702702702703,"h ‚àíh¬µ2, then"
HENCE,0.771021021021021,lk = 1 ‚àíh(Vk + Wk) + h2(VkWk ‚àí¬µ2) > ‚àí1 + h2VkWk > ‚àí1.
HENCE,0.7717717717717718,"(If VkWk = 0, then VkWk ‚àíU 2
k = 0 meaning it just converge.)"
HENCE,0.7725225225225225,"Therefore when Vk + Wk <
2
h ‚àíh¬µ2, we have |lk| < 1. We only need to consider lk+1 when
2
h ‚àíh¬µ2 ‚â§Vk + Wk ‚â§2"
HENCE,0.7732732732732732,"h + 2h¬µ2(1 +
1
1‚àíh2¬µ2 ). Next we replace lk, Vk, Wk, Uk by l, V, W, U for"
HENCE,0.774024024024024,Published as a conference paper at ICLR 2022
HENCE,0.7747747747747747,simplicity.
HENCE,0.7755255255255256,lk+1 = 1 ‚àíh(Vk+1 + Wk+1) + h2(Vk+1Wk+1 ‚àí¬µ2)
HENCE,0.7762762762762763,"= 1 ‚àíh(Vk+1 + Wk+1) + h2(Vk+1Wk+1 ‚àíU 2
k+1) + h2(U 2
k+1 ‚àí¬µ2)"
HENCE,0.777027027027027,"= 1 ‚àíh(Vk+1 + Wk+1) + h2l2(V W ‚àíU 2) + h2(U 2
k+1 ‚àí¬µ2)"
HENCE,0.7777777777777778,"(10)
= l + h2(3 + l2 ‚àíh(1 + 4h2¬µ2)(V + W))(V W ‚àíU 2)"
HENCE,0.7785285285285285,+ h2U 2(3 + (l + 2h2¬µ2)2 ‚àíh(1 + 4h2¬µ2)(V + W))
HENCE,0.7792792792792793,+ h2¬µ2(h2(W + V )2 ‚àíh(W + V ) + 4h4V 2W 2)
HENCE,0.78003003003003,"+ 2h2U¬µ((1 ‚àí(l + 2h2¬µ2))h(W + V ) ‚àí2 + 2(l + 2h2¬µ2)(1 ‚àíl ‚àíh2¬µ2))
(12)"
HENCE,0.7807807807807807,"If l ‚â•‚àí4h2¬µ2, we have"
HENCE,0.7815315315315315,lk+1 = l + h2(3 + l2 ‚àíh(1 + 4h2¬µ2)(V + W))(V W ‚àíU 2)
HENCE,0.7822822822822822,+ h2U 2(3 + (l + 2h2¬µ2)2 ‚àíh(1 + 4h2¬µ2)(V + W))
HENCE,0.7830330330330331,+ h2¬µ2(h2(W + V )2 ‚àíh(W + V ) + 4h4V 2W 2)
HENCE,0.7837837837837838,+ 2h2U¬µ((1 ‚àí(l + 2h2¬µ2))h(W + V ) ‚àí2 + 2(l + 2h2¬µ2)(1 ‚àíl ‚àíh2¬µ2))
HENCE,0.7845345345345346,‚â•l + 2h2U¬µ((1 ‚àí(l + 2h2¬µ2))h(W + V ) ‚àí2 + 2(l + 2h2¬µ2)(1 ‚àíl ‚àíh2¬µ2))
HENCE,0.7852852852852853,‚â•l ‚àíh2¬µ(V + W)(5h2¬µ2 + (9h4¬µ4)/8)
HENCE,0.786036036036036,"‚â•‚àí4h2¬µ2 ‚àíh¬µ(2 + 2h2¬µ2(1 +
1
1 ‚àíh2¬µ2 ))(5h2¬µ2 + (9h4¬µ4)/8) > ‚àí1,"
HENCE,0.7867867867867868,where the Ô¨Årst inequality is because all the removed terms are positive when 2
HENCE,0.7875375375375375,"h ‚àíh¬µ2 ‚â§V + W ‚â§
2
h + 2h¬µ2(1 +
1
1‚àíh2¬µ2 ); the second inequality follows from recollecting l and take l from quadratic
formula with the h(V + W) taken at the bound, and U > ‚àíV +W"
HENCE,0.7882882882882883,"2
; the third inequality follows from
the bounds of h(V + W) and l ; the last inequality is from h¬µ ‚â§
1
2
‚àö 7."
HENCE,0.789039039039039,If l < ‚àí4h2¬µ2 ‚áíl + h2¬µ2 < 0. By rewriting lk+1 in the following way
HENCE,0.7897897897897898,lk+1 = l + h2(4h4V 2W 2 ‚àíh(V + W) + h2(V + W)2)¬µ2 + 4h4U 2¬µ2(l + h2¬µ2)
HENCE,0.7905405405405406,+ 2h2U¬µ(‚àí2 ‚àíh(V + W)(‚àí1 + l + 2h2¬µ2) ‚àí2(‚àí1 + l + h2¬µ2)(l + 2h2¬µ2))
HENCE,0.7912912912912913,"+ h2V W(3 + l2 ‚àíh(V + W)(1 + 4h2¬µ2)),"
HENCE,0.7920420420420421,"we have that the minimum of lk+1 is achieved when U = ¬±
‚àö"
HENCE,0.7927927927927928,"V W. Thus we will consider two cases:
U ‚â•0 and U < 0. Note we always have U 2 < V W (otherwise V W ‚àíU 2 = 0 just converges) and
hence lk+1 is larger than the corresponding value at U = ¬±
‚àö"
HENCE,0.7935435435435435,"V W. Let q = h|U|, v = h¬µ, q = cv
for c ‚â•0. Then we have"
HENCE,0.7942942942942943,"q2 = c2v2 ‚â§h2V W = h(V + W) ‚àí1 + l ‚àív2 < 1,
i.e.,
c < 1"
HENCE,0.795045045045045,"v .
(13)"
HENCE,0.7957957957957958,"Next, let"
HENCE,0.7965465465465466,"l‚Ä≤
k+1 = lk+1 ‚àíh2(3 + l2 ‚àíh(1 + 4h2¬µ2)(V + W))(V W ‚àíU 2) < lk+1 ‚àíh2(1"
HENCE,0.7972972972972973,2 + l2)(V W ‚àíU 2).
HENCE,0.7980480480480481,"First let U ‚â•0 and q = hU < h
‚àö"
HENCE,0.7987987987987988,"V W. Choose q = hU = h
‚àö"
HENCE,0.7995495495495496,V W. Then by
HENCE,0.8003003003003003,"h(V + W) = 1 ‚àíl + h2(V W ‚àí¬µ2) = 1 ‚àíl + c2v2 ‚àív2,"
HENCE,0.801051051051051,we have the minimum value of lk+1 when U ‚â•0
HENCE,0.8018018018018018,"lk+1 ‚â•l‚Ä≤
k+1 = l + (‚àí1 + c)(‚àí((‚àí1 + l)l) + c(2 + l + l2))v2
(14)"
HENCE,0.8025525525525525,‚àí(‚àí1 + c)2(1 + c2 ‚àí2l + 2cl)v4 + (‚àí1 + c)4v6.
HENCE,0.8033033033033034,"1) When ‚àí1 < l < ‚àí4h2¬µ2 and c = 1, we have lk+1 = l‚Ä≤
k+1 = l = lk."
HENCE,0.8040540540540541,Published as a conference paper at ICLR 2022
HENCE,0.8048048048048048,"2) When ‚àí1 < l < ‚àí4h2¬µ2 and c > 1, the derivative of (14) with respect to c is"
HENCE,0.8055555555555556,"d
dcl‚Ä≤
k+1 = (1 ‚àíl)lv2 + (‚àí2 ‚àíl ‚àíl2)v2 + 2v4 ‚àí6lv4 ‚àí4v6 + 3c2(2v4 ‚àí2lv4 ‚àí4v6)"
HENCE,0.8063063063063063,"+ 4c3(‚àív4 + v6) + 2c((2 + l + l2)v2 ‚àí2v4 + 6lv4 + 6v6),"
HENCE,0.8070570570570571,"which is a third order polynominal with negative coefÔ¨Åcient of the highest degree term and has a
root in [0, 1]. Also, when c = 1, d"
HENCE,0.8078078078078078,"dcl‚Ä≤
k+1 = 2v2(1 + l) > 0. Therefore, when 1 < c < 1"
HENCE,0.8085585585585585,"v, l‚Ä≤
k+1 either
increases or Ô¨Årst increases then decreases, namely, the minimum of l‚Ä≤
k+1 is achieved at c = 1 or
c = 1"
HENCE,0.8093093093093093,"v. If c = 1, by the previous discussion, l‚Ä≤
k+1 = l. If c = 1 v,"
HENCE,0.81006006006006,"l‚Ä≤
k+1 = 1 + l2(‚àí1 + v)2 ‚àív2 ‚àí2v3 + 5v4 ‚àí4v5 + v6 + l(2 ‚àí2v + 5v2 ‚àí6v3 + 2v4)"
HENCE,0.8108108108108109,‚â•v2(‚àí24 + 44v ‚àí25v2 + 4v3)
HENCE,0.8115615615615616,"4(‚àí1 + v)2
> ‚àí1,"
HENCE,0.8123123123123123,"where the Ô¨Årst inequality follows from the property of quadratic function by taking l
=
‚àí(2‚àí2v+5v2‚àí6v3+2v4)"
HENCE,0.8130630630630631,"2(‚àí1+v)2
. Therefore when ‚àí1 < l < ‚àí4h2¬µ2 and c > 1, lk+1 > ‚àí1."
HENCE,0.8138138138138138,"3) When ‚àí1 < l < ‚àí4h2¬µ2 and c < 1, rewrite l‚Ä≤
k+1 with respect to l and we have"
HENCE,0.8145645645645646,"l‚Ä≤
k+1 = 2(‚àí1 + c)cv2 ‚àí(‚àí1 + c)2v4 ‚àí(‚àí1 + c)2c2v4 + (‚àí1 + c)4v6 + l2((1 ‚àíc)v2"
HENCE,0.8153153153153153,"+ (‚àí1 + c)cv2) + l(1 + (‚àí1 + c)v2 + (‚àí1 + c)cv2 + 2(‚àí1 + c)2v4 ‚àí2(‚àí1 + c)2cv4),"
HENCE,0.816066066066066,where the minimum point is
HENCE,0.8168168168168168,"l = ‚àí
1
2(‚àí1 + c)2v2 ‚àí1"
HENCE,0.8175675675675675,2 ‚àí(1 ‚àíc)v2 < ‚àí1
HENCE,0.8183183183183184,2v2 ‚àí1
HENCE,0.8190690690690691,2 < ‚àí1.
HENCE,0.8198198198198198,"Therefore the minimum of l‚Ä≤
k+1 is obtained at l = ‚àí1, i.e.,"
HENCE,0.8205705705705706,"l‚Ä≤
k+1 ‚â•‚àí1 + (‚àí1 + c)2v2(2 ‚àí(3 ‚àí2c + c2)v2 + (‚àí1 + c)2v4)"
HENCE,0.8213213213213213,"‚â•‚àí1 + (‚àí1 + c)2v2(2 ‚àí3v2 + (‚àí1 + c)2v4) > ‚àí1,"
HENCE,0.8220720720720721,"where the inequality is from 0 ‚â§c < 1. Therefore when ‚àí1 < l < ‚àí4h2¬µ2 and c < 1, lk+1 > ‚àí1."
HENCE,0.8228228228228228,4) When l ‚â§‚àí1 and c ‚â•5
HENCE,0.8235735735735735,"4, consider l‚Ä≤
k+1 ‚àíl (15) and take the derivative d"
HENCE,0.8243243243243243,"dc(l‚Ä≤
k+1 ‚àíl) =
d
dcl‚Ä≤
k+1.
From previous discussion, consider d"
HENCE,0.825075075075075,"dcl‚Ä≤
k+1 at c = 5"
HENCE,0.8258258258258259,"4, i.e.,"
HENCE,0.8265765765765766,"d
dcl‚Ä≤
k+1 = 1/16v2(48 + 8l2 ‚àí23v2 + v4 + l(40 ‚àí6v2)) > v2 ‚àí(161v4)/16 + (325v6)/16 > 0,"
HENCE,0.8273273273273273,"where the Ô¨Årst inequality follows from l > ‚àí1 ‚àí6v2 by (11). Therefore the minimum of l‚Ä≤
k+1 ‚àíl is
at c = 5"
HENCE,0.8280780780780781,4 or c = 1
HENCE,0.8288288288288288,"v. If c = 5 4,"
HENCE,0.8295795795795796,"l‚Ä≤
k+1 ‚àíl = v2"
HENCE,0.8303303303303303,256(160 + 16l2 ‚àí41v2 + v4 ‚àí8l(‚àí18 + v2)) > v2
HENCE,0.831081081081081,256(32 ‚àí705v2 + 625v4) > 0
HENCE,0.8318318318318318,"where the Ô¨Årst inequality is from l > ‚àí1 ‚àí6v2 by (11). If c = 1 v,"
HENCE,0.8325825825825826,"l‚Ä≤
k+1 ‚àíl = l2(‚àí1 + v)2 + l(‚àí1 + v)(‚àí1 + v ‚àí4v2 + 2v3) + (‚àí1 + v)(‚àí1 ‚àív + 2v3 ‚àí3v4 + v5)"
HENCE,0.8333333333333334,‚â•(1 ‚àív)3(1 + 3v + v2 ‚àív3) > 0
HENCE,0.8340840840840841,"where the Ô¨Årst inequality is from l ‚â§‚àí1. Therefore when l ‚â§‚àí1 and c ‚â•
5
4, lk+1 ‚àílk >
min{ v2"
HENCE,0.8348348348348348,"256(32 ‚àí705v2 + 625v4), (1 ‚àív)3(1 + 3v + v2 ‚àív3)} > 0."
HENCE,0.8355855855855856,In the next two cases c ‚â§1 and 1 < c < 5
HENCE,0.8363363363363363,"4, we no longer assume q = h
‚àö"
HENCE,0.8370870870870871,"V W but want the same
lower bound of l‚Ä≤
k+1. Since q = hU < h
‚àö"
HENCE,0.8378378378378378,"V W, we have h(V + W) > 1 ‚àíl + c2v2 ‚àív2 > 0. By
rewriting l‚Ä≤
k+1 to be"
HENCE,0.8385885885885885,"l‚Ä≤
k+1 =l ‚àí4cv2 + 3c2v2 + 4c4v6 + 4cv2(1 ‚àíl ‚àív2)(l + 2v2) + v2h2(V + W)2"
HENCE,0.8393393393393394,"+ c2v2(l + 2v2)2 + (‚àív2 + 2cv2(1 ‚àíl ‚àí2v2) ‚àíc2v2(1 + 4v2))h(V + W),"
HENCE,0.8400900900900901,Published as a conference paper at ICLR 2022
HENCE,0.8408408408408409,it can be seen from the minimum point w.r.t. h(V + W) that when h(V + W) > 0 and 0 ‚â§c < 5
HENCE,0.8415915915915916,"4,
l‚Ä≤
k+1 decreases as h(V + W) decreases. Therefore by h(V + W) > 1 ‚àíl + c2v2 ‚àív2, we have the
same expression for the lower bound of l‚Ä≤
k+1 but with U 2 < V W, i.e.,"
HENCE,0.8423423423423423,"lk+1 ‚àíl‚Ä≤
k+1 = h2(3 + l2 ‚àíh(1 + 4h2¬µ2)(V + W))(V W ‚àíU 2) > h2(1"
HENCE,0.8430930930930931,"2 + l2)(V W ‚àíU 2),"
HENCE,0.8438438438438438,"l‚Ä≤
k+1 > l + (‚àí1 + c)(‚àí((‚àí1 + l)l) + c(2 + l + l2))v2"
HENCE,0.8445945945945946,‚àí(‚àí1 + c)2(1 + c2 ‚àí2l + 2cl)v4 + (‚àí1 + c)4v6.
HENCE,0.8453453453453453,"5) When l ‚â§‚àí1 and c ‚â§1, consider l‚Ä≤
k+1 ‚àíl"
HENCE,0.8460960960960962,"l‚Ä≤
k+1 ‚àíl > (‚àí1 + c)(‚àí((‚àí1 + l)l) + c(2 + l + l2))v2
(15)"
HENCE,0.8468468468468469,‚àí(‚àí1 + c)2(1 + c2 ‚àí2l + 2cl)v4 + (‚àí1 + c)4v6
HENCE,0.8475975975975976,= (‚àí1 + c)2l2v2 + (‚àí1 + c)lv2(1 + c ‚àí2(1 ‚àíc)v2 + 2(1 ‚àíc)cv2)
HENCE,0.8483483483483484,+ (‚àí1 + c)v2(2c + (1 ‚àíc)v2 + (1 ‚àíc)c2v2 + (‚àí1 + c)3v4)
HENCE,0.8490990990990991,where the minimum point is l = ‚àí1
HENCE,0.8498498498498499,2 + 1‚àí(1‚àíc)v2+(1‚àíc)cv2
HENCE,0.8506006006006006,"1‚àíc
> ‚àí1. By l ‚â§‚àí1 we have"
HENCE,0.8513513513513513,"l‚Ä≤
k+1 ‚àíl > v2(2(‚àí1 + c)2 ‚àí(‚àí1 + c)2(3 ‚àí2c + c2)v2 + (‚àí1 + c)4v4)"
HENCE,0.8521021021021021,= v2((‚àí1 + c)2(2 ‚àí(3 ‚àí2c + c2)v2) + (‚àí1 + c)4v4)
HENCE,0.8528528528528528,‚â•v2((‚àí1 + c)2(2 ‚àí3v2) + (‚àí1 + c)4v4) ‚â•0.
HENCE,0.8536036036036037,"Therefore when l ‚â§‚àí1 and c ‚â§1, lk+1 ‚àílk > h2( 1"
HENCE,0.8543543543543544,2 + l2)(V W ‚àíU 2) ‚â•3
HENCE,0.8551051051051051,2h2(V W ‚àíU 2).
HENCE,0.8558558558558559,6) When l ‚â§‚àí1 and 1 < c < 5
HENCE,0.8566066066066066,"4, consider Uk+1. By h(V + W) ‚àíh2V W = 1 ‚àíl ‚àív2, we have"
HENCE,0.8573573573573574,Uk+1 = h(1 ‚àíhV )¬µW + h¬µV (1 ‚àíhW) + U(1 + h2V W ‚àíh(V + W) + h2¬µ2)
HENCE,0.8581081081081081,= ¬µ((c ‚àí1)(l + v2) + 1 + cv2 ‚àíh2V W).
HENCE,0.8588588588588588,Thus denote hUk+1 = ck+1h¬µ = ck+1v and we have
HENCE,0.8596096096096096,ck+1 = (c ‚àí1)(l + v2) + 1 + cv2 ‚àíh2V W
HENCE,0.8603603603603603,< (c ‚àí1)(l + v2) + 1 + cv2 ‚àíc2v2 < 1.
HENCE,0.8611111111111112,"Then if lk+1 ‚àíl ‚â•0, then lk+2 ‚àíl = lk+2 ‚àílk+1 + lk+1 ‚àíl > lk+2 ‚àílk+1 > 3"
HENCE,0.8618618618618619,"2h2(V W ‚àíU 2).
Otherwise, we have lk+1 < l and consider lk+2. Also, the distance between 1 and ck+1 is larger than
that of 1 and c, i.e.,"
HENCE,0.8626126126126126,"1 ‚àíck+1 ‚àí(c ‚àí1) > (1 ‚àí(c ‚àí1)(l + v2) + 1 + cv2 ‚àíc2v2) ‚àí(c ‚àí1)
(16)"
HENCE,0.8633633633633634,= 1 + l + v2 + c2v2 + c(‚àí1 ‚àíl ‚àí2v2) > 0.
HENCE,0.8641141141141141,"From previous discussion, l‚Ä≤
k+1 ‚àíl increases as l decreases. Also d"
HENCE,0.8648648648648649,"dcl‚Ä≤
k+1 < 0 when 0 ‚â§c < 1, so
l‚Ä≤
k+1 ‚àíl increases as c decreases. By (16), we have ck+1 < 2 ‚àíc. Then"
HENCE,0.8656156156156156,"l‚Ä≤
k+2 ‚àíl = l‚Ä≤
k+2 ‚àílk+1 + lk+1 ‚àíl > l‚Ä≤
k+2 ‚àílk+1 + l‚Ä≤
k+1 ‚àíl"
HENCE,0.8663663663663663,> (‚àí1 + c)(‚àí((‚àí1 + l)l) + c(2 + l + l2))v2
HENCE,0.8671171171171171,‚àí(‚àí1 + c)2(1 + c2 ‚àí2l + 2cl)v4 + (‚àí1 + c)4v6
HENCE,0.8678678678678678,+ (1 ‚àíc)v2(‚àí((‚àí1 + l)l) ‚àí(‚àí2 + c)(2 + l + l2)
HENCE,0.8686186186186187,+ (‚àí1 + c)(5 + c2 + 2l ‚àí2c(2 + l))v2 ‚àí(‚àí1 + c)3v4)
HENCE,0.8693693693693694,= 2v2(‚àí1 + c)2((2 + l + l2) + (‚àí2 ‚àí(‚àí1 + c)2)v2 + (‚àí1 + c)2v4)
HENCE,0.8701201201201201,"> 2v2(‚àí1 + c)2((2 ‚àí3v2) + (‚àí1 + c)2v4) > 0,"
HENCE,0.8708708708708709,"where the second inequality follows from ck+1 < 2 ‚àíc and lk+1 < l; the last inequality is by
l ‚â§‚àí1 and 1 < c < 5"
HENCE,0.8716216216216216,4. Therefore when l ‚â§‚àí1 and 1 < c < 5
HENCE,0.8723723723723724,"4, lk+2 ‚àílk > 3"
HENCE,0.8731231231231231,"2h2(V W ‚àíU 2) or
2v2(‚àí1 + c)2((2 ‚àí3v2) + (‚àí1 + c)2v4)."
HENCE,0.8738738738738738,Published as a conference paper at ICLR 2022
HENCE,0.8746246246246246,"Second, let U < 0 and q = ‚àíhU < h
‚àö"
HENCE,0.8753753753753754,"V W. Choose q = ‚àíhU = h
‚àö"
HENCE,0.8761261261261262,"V W. Then similarly we
have the minimum value of lk+1 when U < 0"
HENCE,0.8768768768768769,"l‚Ä≤
k+1 = l + (1 + c)((‚àí1 + l)l + c(2 + l + l2))v2 ‚àí(1 + c)2(1 + c2
(17)"
HENCE,0.8776276276276276,‚àí2l ‚àí2cl)v4 + (1 + c)4v6
HENCE,0.8783783783783784,= (1 + c)2l2v2 + l(1 + (‚àí1 + c2)v2 + 2(1 + c)3v4)
HENCE,0.8791291291291291,+ (1 + c)v2(v2(‚àí1 + v2) + c3v2(‚àí1 + v2) + c2v2(‚àí1 + 3v2) + c(2 ‚àív2 + 3v4))
HENCE,0.8798798798798799,"‚â•‚àí1 + (‚àí4 + 4c + 3c2)v2 + (15 + 16c ‚àí5c2 ‚àí8c3 ‚àí2c4)v4
(18)"
HENCE,0.8806306306306306,"+ (‚àí5 ‚àí4c + 2c2 + c3)2v6,"
HENCE,0.8813813813813813,"where the inequality is because when l > ‚àí1 ‚àí6v2 + c2v2 (from (11)), l‚Ä≤
k+1 increases as l increases."
HENCE,0.8821321321321322,"1) When c ‚â•1, since (‚àí5 ‚àí4c + 2c2 + c3)2v6 ‚â•0, we consider (‚àí4 + 4c + 3c2)v2 + (15 + 16c ‚àí
5c2 ‚àí8c3 ‚àí2c4)v4 denoted to be lpart
k+1"
HENCE,0.8828828828828829,"d
dclpart
k+1 = (4 + 6c)v2 + (16 ‚àí10c ‚àí24c2 ‚àí8c3)v4."
HENCE,0.8836336336336337,"It has a root between -2 and 0. Also when c = 1, d"
HENCE,0.8843843843843844,"dclpart
k+1 > 0; when c = 1 v, d"
HENCE,0.8851351351351351,"dclpart
k+1 < 0. Thus, the
minimum of lpart
k+1 is achieved at c = 1 or c = 1"
HENCE,0.8858858858858859,"v, i.e., lpart
k+1 ‚â•v2(3 + 16v2). Then"
HENCE,0.8866366366366366,"l‚Ä≤
k+1 ‚â•‚àí1 + v2(3 + 16v2) + (‚àí5 ‚àí4c + 2c2 + c3)2v6 > 0."
HENCE,0.8873873873873874,"Therefore when c ‚â•1, lk+1 > ‚àí1."
HENCE,0.8881381381381381,"2) When 0 ‚â§c < 1 and l > ‚àí1, from previous discussion of l‚Ä≤
k+1 we have"
HENCE,0.8888888888888888,"l‚Ä≤
k+1 ‚â•‚àí1 + 2(1 + c)2v2 ‚àí(1 + c)2(3 + 2c + c2)v4 + (1 + c)4v6"
HENCE,0.8896396396396397,‚â•‚àí1 + (1 + c)2v2(2 ‚àí6v2) + (1 + c)4v6
HENCE,0.8903903903903904,‚â•‚àí1 + v2(2 ‚àí6v2) + v6 > ‚àí1.
HENCE,0.8911411411411412,"Therefore when 0 ‚â§c < 1 and l > ‚àí1, lk+1 > ‚àí1."
HENCE,0.8918918918918919,"3) When 0 ‚â§c < 1 and l ‚â§‚àí1, consider l‚Ä≤
k+1 ‚àíl. By (17),"
HENCE,0.8926426426426426,"l‚Ä≤
k+1 ‚àíl = (1 + c)2l2v2 + l((‚àí1 + c2)v2 + 2(1 + c)3v4)"
HENCE,0.8933933933933934,+ (1 + c)v2(v2(‚àí1 + v2) + c3v2(‚àí1 + v2) + c2v2(‚àí1 + 3v2) + c(2 ‚àív2 + 3v4))
HENCE,0.8941441441441441,‚â•2(1 + c)2v2 ‚àí(1 + c)2(3 + 2c + c2)v4 + (1 + c)4v6
HENCE,0.8948948948948949,"‚â•v2(2 ‚àí6v2) + v6,"
HENCE,0.8956456456456456,"where the Ô¨Årst inequality is because the minimum point of l is greater than -1. Therefore when
0 ‚â§c < 1 and l ‚â§‚àí1, lk+1 ‚àílk > v2(2 ‚àí6v2) + v6."
HENCE,0.8963963963963963,"Actually all the proofs of lk > ‚àí1 ‚áílk+1 > ‚àí1 + C for some C > 0 are valid for all 0 <
Vk + Wk ‚â§2"
HENCE,0.8971471471471472,"h + 2h¬µ2(1 +
1
1‚àíh2¬µ2 )."
HENCE,0.8978978978978979,"In general, if lk > ‚àí1, then there exists a constant C > 0 only depending on h¬µ s.t. li > ‚àí1 + C for
all i > k. If lk ‚â§‚àí1, then either (1) lk+1 increases at least by a Ô¨Åxed value only depending on h¬µ, or
(2) lk+2 or lk+1 increases by 3"
HENCE,0.8986486486486487,"2h2(VkWk ‚àíU 2
k). If it keeps staying in case (2), then since lk ‚â§‚àí1,
VkWk ‚àíU 2
k will be larger and larger until li > ‚àí1 for some i. Therefore, there exists a K > 0, s.t.
when k > K, lk > ‚àí1+C. Then because lk ‚â§1‚àíh2¬µ2, let 0 < C0 = max{1‚àíC, 1‚àíh2¬µ2} < 1
and we have |lk| ‚â§C0 < 1 for all k > K. Further,"
HENCE,0.8993993993993994,"VkWk ‚àíU 2
k ‚â§C2(k‚àíK)
0
(VKWK ‚àíU 2
K) ‚Üí0
as k ‚Üí‚àû,"
HENCE,0.9001501501501501,"i.e., 1 ‚àí
U 2
k
VkWk ‚Üí0 ‚áí| cos(‚à†(xk, yk))| ‚Üí1."
HENCE,0.9009009009009009,"Otherwise, there exists a K > 0, s.t., VKWK ‚àíU 2
K = 0, then VkWk ‚àíU 2
k = 0 for all k ‚â•K.
There are two cases. (i) xK and yK are already aligned. (ii) one of VK and WK is 0, WOLG assume"
HENCE,0.9016516516516516,Published as a conference paper at ICLR 2022
HENCE,0.9024024024024024,"VK = 0. Then from the GD update,

xk+1 = xk + h(¬µI ‚àíxky‚ä§
k )yk
yk+1 = yk + h(¬µI ‚àíykx‚ä§
k )xk
,"
HENCE,0.9031531531531531,"xK+1 and yK+1 will be aligned for both cases. Then for all k ‚â•K + 1, xk and yk is aligned, i.e.,
| cos(‚à†(xk, yk))| = 1."
HENCE,0.9039039039039038,"I
PROOF OF THEOREM 5.1"
HENCE,0.9046546546546547,"I.1
GENERAL RANK 1 APPROXIMATION FOR NON-NEGATIVE DIAGONAL MATRIX"
HENCE,0.9054054054054054,"We Ô¨Årst discuss an easy case where A is a non-negative diagonal matrix (later on we will see this is
actually the canonical case) and consider"
HENCE,0.9061561561561562,"min
x,y‚ààRn ‚à•A ‚àíxy‚ä§‚à•2
F /2.
(19)"
HENCE,0.9069069069069069,"Assume A = Ô£´ Ô£¨
Ô£≠"
HENCE,0.9076576576576577,"¬µ1In1
...
¬µmInm Ô£∂"
HENCE,0.9084084084084084,"Ô£∑
Ô£∏, where Ini is an identity matrix of dimension ni √ó ni,"
HENCE,0.9091591591591591,"Pm
i=1 ni = n, ¬µi ‚â•0, ¬µi Ã∏= ¬µj for i Ã∏= j. Let Si = {1, 2, ¬∑ ¬∑ ¬∑ , ni} + Pi‚àí1
j=1 nj, i.e., an index
set describing the positional indices of Ini. Let ¬µmax = max{¬µ1, ¬∑ ¬∑ ¬∑ , ¬µm} and Smax be the
corresponding index set. Let [x‚àû, y‚àû] be a Ô¨Åxed point of the GD map. Let xs,‚àûand ys,‚àûbe the sth
element of x‚àûand y‚àû."
HENCE,0.9099099099099099,The following theorem characterizes the Ô¨Åxed points of the GD map.
HENCE,0.9106606606606606,Theorem I.1. The Ô¨Åxed points of GD map satisfy P
HENCE,0.9114114114114115,"s‚ààSi x2
s,‚àû¬∑P"
HENCE,0.9121621621621622,"s‚ààSi y2
s,‚àû= ¬µ2
i , xs,‚àû= ys,‚àû= 0
for s /‚ààSi, ‚àÄi = 1, ¬∑ ¬∑ ¬∑ , n. Thus ‚à•x‚àû‚à•‚à•y‚àû‚à•= ¬µi, ‚àÄi = 1, ¬∑ ¬∑ ¬∑ , n."
HENCE,0.9129129129129129,"Proof. To solve the Ô¨Åxed points, we have ‚àÄi = 1, ¬∑ ¬∑ ¬∑ , n and x ‚ààSi

(A ‚àíx‚àûy‚ä§
‚àû)y‚àû= Ay‚àû‚àí(y‚ä§
‚àûy‚àû)x‚àû= 0
(A ‚àíx‚àûy‚ä§
‚àû)‚ä§x‚àû= A‚ä§x‚àû‚àí(x‚ä§
‚àûx‚àû)y‚àû= 0"
HENCE,0.9136636636636637,"i.e.,

¬µiys,‚àû= (y‚ä§
‚àûy‚àû)xs,‚àû
¬µixs,‚àû= (x‚ä§
‚àûx‚àû)ys,‚àû
‚áí¬µ2
i xs,‚àûys,‚àû= (x‚ä§
‚àûx‚àû)(y‚ä§
‚àûy‚àû)xs,‚àûys,‚àû"
HENCE,0.9144144144144144,"Obviously, x‚àû= y‚àû= 0 is a Ô¨Åxed point. If xs,‚àûÃ∏= 0, then ys,‚àûÃ∏= 0 and we have ¬µ2
i =
(x‚ä§
‚àûx‚àû)(y‚ä§
‚àûy‚àû). Since ¬µi Ã∏= ¬µj for i Ã∏= j, there exists at most one i, s.t. ¬µ2
i = (x‚ä§
‚àûx‚àû)(y‚ä§
‚àûy‚àû)
and all the xs,‚àû, ys,‚àûs.t. s ‚ààSj, j Ã∏= i is zero."
HENCE,0.9151651651651652,"This theorem identiÔ¨Åes that each of these Ô¨Åxed points is concentrated in one of the positions of
the eigenvalue blocks. Note the Ô¨Åxed points contain both stable and unstable ones, i.e., all the
global minima and saddles are of the above form. Moreover, the global minima are the points
obeying P
s‚ààSmax x2
s,‚àû¬∑ P"
HENCE,0.9159159159159159,"s‚ààSmax y2
s,‚àû= ¬µ2
max, xs,‚àû= ys,‚àû= 0 for s /‚ààSmax and also
‚à•x‚àû‚à•‚à•y‚àû‚à•= ¬µmax. The saddles are the ones with ¬µi Ã∏= ¬µmax, ‚àÄi = 1, ¬∑ ¬∑ ¬∑ , n."
HENCE,0.9166666666666666,"Remark I.2 (The alignment of x‚àûand y‚àûat the global minimum). At the global minimum of the
objective (19) with diagonal and non-negative A deÔ¨Åned above, x‚àûy‚ä§
‚àûis the rank-1 approximation
of A with the largest eigenvalue, i.e., we have ¬µmax = tr(x‚àûy‚ä§
‚àû) = x‚ä§
‚àûy‚àû. Also, from the above
theorem, ‚à•x‚àû‚à•‚à•y‚àû‚à•= ¬µmax. Therefore for each global minimum, there exists a scalar l > 0, s.t.,
x‚àû= ly‚àû, i.e., x‚àûand y‚àûare aligned at the global minimum."
HENCE,0.9174174174174174,"Based on the analytical form of eigenvalues of the Jacobian of GD map (19) in Theorem I.1, we can
establish the following relation between x‚àûand y‚àûat the global minimum via stability analysis."
HENCE,0.9181681681681682,Published as a conference paper at ICLR 2022
HENCE,0.918918918918919,"Theorem I.3. For almost all initial conditions, if GD converges to a minimum, then x‚ä§
‚àûy‚àû= ¬µmax
and"
HENCE,0.9196696696696697,‚à•x‚àû‚à•2 + ‚à•y‚àû‚à•2 < 2
HENCE,0.9204204204204204,"h;
(20)"
HENCE,0.9211711711711712,the extent of balancing is quantiÔ¨Åed by
HENCE,0.9219219219219219,‚à•x‚àû‚àíy‚àû‚à•2 < 2
HENCE,0.9226726726726727,"h ‚àí2¬µmax.
(21)"
HENCE,0.9234234234234234,"Proof. Since each unstable Ô¨Åxed point has its stable set with negligible size when compared to the
stable set of stable Ô¨Åxed point, then for almost all initial conditions, if GD converges to a global
minimum, this minimum is a stable Ô¨Åxed point of the GD map."
HENCE,0.9241741741741741,"From the above theorem, we know the Ô¨Åxed points are ¬µ2
i = (x‚ä§
‚àûx‚àû)(y‚ä§
‚àûy‚àû). Assume ¬µ2 =
(x‚ä§
‚àûx‚àû)(y‚ä§
‚àûy‚àû)."
HENCE,0.924924924924925,"For the Jacobian J = I2n +

‚àíh(y‚ä§
‚àûy‚àû)In
hA ‚àí2hx‚àûy‚ä§
‚àû
hA‚ä§‚àí2hy‚àûx‚ä§
‚àû
‚àíh(x‚ä§
‚àûx‚àû)In"
HENCE,0.9256756756756757,"
= I2n + ÀúA, since the lower two"
HENCE,0.9264264264264265,"blocks commute, we have"
HENCE,0.9271771771771772,"det(ŒªI ‚àíÀúA) = det(Œª2In + Œªh(x‚ä§
‚àûx‚àû+ y‚ä§
‚àûy‚àû)In + h2¬µ2In ‚àíh2A2"
HENCE,0.9279279279279279,"+ h2(2Ay‚àûx‚ä§
‚àû+ 2x‚àûy‚ä§
‚àûA ‚àí4x‚àûy‚ä§
‚àûy‚àûx‚ä§
‚àû))"
HENCE,0.9286786786786787,"= det(Œª2In + Œªh(x‚ä§
‚àûx‚àû+ y‚ä§
‚àûy‚àû)In + h2¬µ2In ‚àíh2A2"
HENCE,0.9294294294294294,"+ h2(2(A ‚àíx‚àûy‚ä§
‚àû)y‚àûx‚ä§
‚àû+ 2x‚àûy‚ä§
‚àû(A ‚àíy‚àûx‚ä§
‚àû)))"
HENCE,0.9301801801801802,"= det(Œª2In + Œªh(x‚ä§
‚àûx‚àû+ y‚ä§
‚àûy‚àû)In + h2¬µ2In ‚àíh2A2) = n
Y"
HENCE,0.9309309309309309,"i=1
(Œª2 + h(x‚ä§
‚àûx‚àû+ y‚ä§
‚àûy‚àû)Œª + h2(¬µ2 ‚àí¬µ2
i ))."
HENCE,0.9316816816816816,"Hence the two values, 1 and 1 ‚àíh(x‚ä§
‚àûx‚àû+ y‚ä§
‚àûy‚àû), are eigenvalues of J at each nonzero Ô¨Åxed
point."
HENCE,0.9324324324324325,"If ¬µ = max{¬µi}, then |1 ‚àíh(x‚ä§
‚àûx‚àû+ y‚ä§
‚àûy‚àû)| < 1 ‚áíall the eigenvalues are bounded by 1. Hence
x‚ä§
‚àûx‚àû+ y‚ä§
‚àûy‚àû< 2 h."
HENCE,0.9331831831831832,"If ¬µ Ã∏= max{¬µi}, then 1+ 1"
HENCE,0.933933933933934,"2(‚àíh(x‚ä§
‚àûx‚àû+y‚ä§
‚àûy‚àû)+
p"
HENCE,0.9346846846846847,"h2(x‚ä§
‚àûx‚àû+ y‚ä§
‚àûy‚àû)2 + 4h2((¬µ2
i ‚àí¬µ2))) >
1 for ¬µi ‚â•¬µ. It has at least one unstable direction. It‚Äôs stable set is measure 0."
HENCE,0.9354354354354354,"The second inequality of the theorem follows from x‚ä§
‚àûy‚àû= ¬µ at the global minimum because of
diagonal and non-negative A and SVD."
HENCE,0.9361861861861862,"I.2
GENERAL MATRIX FACTORIZATION FOR NON-NEGATIVE DIAGONAL MATRIX"
HENCE,0.9369369369369369,"In this section, we consider problem (5) via GD iteration but with A ‚ààRn√ón to be an arbitrary
non-negative diagonal matrix."
HENCE,0.9376876876876877,"Let ¬µ1 ‚â•¬µ2 ‚â•¬∑ ¬∑ ¬∑ ‚â•¬µn ‚â•0 be the diagonal elements of A. Let r = rank(A) which of course
satiÔ¨Åes r ‚â§n. Also assume that ‚à•A‚à•F does not scale with n or d (without loss of generality we can
assume ‚à•A‚à•F = O(1)). Then we will have our balancing result in the following.
Theorem I.4. For almost all initial conditions, if GD for (5) converges to a global minimum, then
there exists c = c(n, d) > c0 with constant c0 > 0 independent of n, d, and learning rate h, such
that, (X, Y ) satisÔ¨Åes"
HENCE,0.9384384384384384,"c(‚à•X‚àû‚à•2
F + ‚à•Y‚àû‚à•2
F) < 2 h,"
HENCE,0.9391891891891891,and the extent of balancing is quantiÔ¨Åed by
HENCE,0.93993993993994,"‚à•X‚àû‚àíY‚àû‚à•2
F < 2 ch ‚àí2"
HENCE,0.9406906906906907,"min{d,r}
X"
HENCE,0.9414414414414415,"i=1
¬µi."
HENCE,0.9421921921921922,Published as a conference paper at ICLR 2022
HENCE,0.9429429429429429,"Proof. Let r =rank(A) ‚â§n. Since each unstable Ô¨Åxed point has its stable set with size negligible
when compared to the stable set of stable Ô¨Åxed point, then for almost all initial conditions, if GD
converges to a global minimum, this minimum is a stable Ô¨Åxed point of the GD map."
HENCE,0.9436936936936937,"For matrix X = (x1 x2 ¬∑ ¬∑ ¬∑ xd) ‚ààRn√ód, where xi ‚ààRn is the ith column vector, we vectorize the"
HENCE,0.9444444444444444,"variable and get vec(X) = Ô£´ Ô£¨
Ô£¨
Ô£≠"
HENCE,0.9451951951951952,"x1
x2
...
xd Ô£∂"
HENCE,0.9459459459459459,"Ô£∑
Ô£∑
Ô£∏. Then we can rewrite the GD iteration,"
HENCE,0.9466966966966966,"
Xk+1 = Xk + h(A ‚àíXkY ‚ä§
k )Yk
Yk+1 = Yk + h(A‚ä§‚àíYkX‚ä§
k )Xk"
HENCE,0.9474474474474475,"‚áí

vec(Xk+1) = vec(Xk) + h(I ‚äóA ‚àíT(vec(Yk)vec(Xk)‚ä§))vec(Yk)
vec(Yk+1) = vec(Yk) + h(I ‚äóA ‚àíT(vec(Xk)vec(Yk)‚ä§))vec(Xk) ,"
HENCE,0.9481981981981982,"where ‚äóis the Kronecker product, and T : Rnd√ónd ‚ÜíRnd√ónd is a linear operator, s.t. T Ô£´ Ô£¨
Ô£≠ Ô£´ Ô£¨
Ô£≠"
HENCE,0.948948948948949,"M11
¬∑ ¬∑ ¬∑
M1d
...
...
Md1
¬∑ ¬∑ ¬∑
Mdd Ô£∂ Ô£∑
Ô£∏ Ô£∂ Ô£∑
Ô£∏= Ô£´ Ô£¨
Ô£≠"
HENCE,0.9496996996996997,"M ‚ä§
11
¬∑ ¬∑ ¬∑
M ‚ä§
1d
...
...
M ‚ä§
d1
¬∑ ¬∑ ¬∑
M ‚ä§
dd Ô£∂ Ô£∑
Ô£∏,"
HENCE,0.9504504504504504,"then
T(vec(Y )vec(X)‚ä§) = Ô£´"
HENCE,0.9512012012012012,"Ô£¨
Ô£¨
Ô£¨
Ô£≠"
HENCE,0.9519519519519519,"x1y‚ä§
1
x2y‚ä§
1
¬∑ ¬∑ ¬∑
xdy‚ä§
1
x1y‚ä§
2
x2y‚ä§
2
¬∑ ¬∑ ¬∑
xdy‚ä§
2
...
...
...
x1y‚ä§
d
x2y‚ä§
d
¬∑ ¬∑ ¬∑
xdy‚ä§
d Ô£∂"
HENCE,0.9527027027027027,"Ô£∑
Ô£∑
Ô£∑
Ô£∏."
HENCE,0.9534534534534534,"The Jacobian of the vectorized GD map is (replace Xk and Yk by X and Y ) I2nd ‚àíhM, where"
HENCE,0.9542042042042042,"M =

Y ‚ä§Y ‚äóIn
T(vec(Y )vec(X)‚ä§) + I ‚äó(XY ‚ä§‚àíA)
T(vec(X)vec(Y )‚ä§) + I ‚äó(Y X‚ä§‚àíA)
X‚ä§X ‚äóIn 
."
HENCE,0.954954954954955,"We would like to obtain an estimation of the eigenvalues of I ‚àíhM for stability analysis. Since M is
the Hessian of the objective at the global minimum deÔ¨Åned, all the eigenvalues are non-negative. Also,
since ‚à•A‚à•F is independent of h, n and d, we can just assume without loss of generality ‚à•A‚à•F = O(1)
and then take ‚à•X‚à•F and ‚à•Y ‚à•F to be O(1) at each global minimum. Then"
HENCE,0.9557057057057057,"tr(M) = n(‚à•X‚à•2
F + ‚à•Y ‚à•2
F) = O(n)."
HENCE,0.9564564564564565,"Next consider tr(M 2). We only need the diagonal blocks of M 2. The two diagonal blocks are the
following"
HENCE,0.9572072072072072,M1 = (Y ‚ä§Y ‚äóIn)2
HENCE,0.9579579579579579,"+ [T(vec(Y )vec(X)‚ä§) + I ‚äó(XY ‚ä§‚àíA)][T(vec(X)vec(Y )‚ä§) + I ‚äó(Y X‚ä§‚àíA)],"
HENCE,0.9587087087087087,M2 = (X‚ä§X ‚äóIn)2
HENCE,0.9594594594594594,+ [T(vec(X)vec(Y )‚ä§) + I ‚äó(Y X‚ä§‚àíA)][T(vec(Y )vec(X)‚ä§) + I ‚äó(XY ‚ä§‚àíA)].
HENCE,0.9602102102102102,"Since we evaluate this matrix at the minimum, we have the gradient equals 0, i.e.,"
HENCE,0.960960960960961,"(A ‚àíXY ‚ä§)Y = 0, (A ‚àíY X‚ä§)X = 0."
HENCE,0.9617117117117117,"By the mixed-product property of Kronecker product, we have"
HENCE,0.9624624624624625,M1 = (Y ‚ä§Y )2 ‚äóIn + T(vec(Y )vec(X)‚ä§)T(vec(X)vec(Y )‚ä§)
HENCE,0.9632132132132132,"+ Id ‚äó[(A ‚àíXY ‚ä§)(A ‚àíY X‚ä§)],"
HENCE,0.963963963963964,M2 = (X‚ä§X)2 ‚äóIn + T(vec(X)vec(Y )‚ä§)T(vec(Y )vec(X)‚ä§)
HENCE,0.9647147147147147,+ Id ‚äó[(A ‚àíY X‚ä§)(A ‚àíXY ‚ä§)].
HENCE,0.9654654654654654,Published as a conference paper at ICLR 2022 Then
HENCE,0.9662162162162162,"tr(M 2) = 2 ‚à•X‚à•2
F ‚à•Y ‚à•2
F + n(
X‚ä§X
2"
HENCE,0.9669669669669669,"F +
Y ‚ä§Y
2"
HENCE,0.9677177177177178,"F) + d
A ‚àíXY ‚ä§2 F ."
HENCE,0.9684684684684685,"Also, this is the global minimum, meaning"
HENCE,0.9692192192192193,"A ‚àíXY ‚ä§2"
HENCE,0.96996996996997,"F = tr((A ‚àíXY ‚ä§)(A ‚àíY X‚ä§)) =
Pr
i=d+1 ¬µ2
i ,
d < r
0,
d ‚â•r"
HENCE,0.9707207207207207,"Therefore, when d ‚â•r, we have"
HENCE,0.9714714714714715,"tr(M 2) = 2 ‚à•X‚à•2
F ‚à•Y ‚à•2
F + n(
X‚ä§X
2"
HENCE,0.9722222222222222,"F +
Y ‚ä§Y
2"
HENCE,0.972972972972973,F) = O(n).
HENCE,0.9737237237237237,"When d < r, we have"
HENCE,0.9744744744744744,"tr(M 2) = 2 ‚à•X‚à•2
F ‚à•Y ‚à•2
F + n(
X‚ä§X
2"
HENCE,0.9752252252252253,"F +
Y ‚ä§Y
2"
HENCE,0.975975975975976,"F) + d
A ‚àíXY ‚ä§2 F"
HENCE,0.9767267267267268,"< 2 ‚à•X‚à•2
F ‚à•Y ‚à•2
F + n(
X‚ä§X
2"
HENCE,0.9774774774774775,"F +
Y ‚ä§Y
2"
HENCE,0.9782282282282282,"F) + n
A ‚àíXY ‚ä§2"
HENCE,0.978978978978979,F = O(n).
HENCE,0.9797297297297297,"Therefore, the number of non-zero eigenvalues is N = O(n). Let Œªmax be the largest eigenvalue of
M. Then there exist a constant c0 > 0"
HENCE,0.9804804804804805,"Œªmax ‚â•tr(M)/N ‚â•c0(‚à•X‚à•2
F + ‚à•Y ‚à•2
F)."
HENCE,0.9812312312312312,"From stability analysis, we need the absolute values of the eigenvalues of I ‚àíhM to be bounded by
1, i.e.,"
HENCE,0.9819819819819819,"1 ‚àíhc0(‚à•X‚à•2
F + ‚à•Y ‚à•2
F) ‚â•1 ‚àíhŒªmax ‚â•‚àí1 ‚áíc0(‚à•X‚à•2
F + ‚à•Y ‚à•2
F) ‚â§2 h."
HENCE,0.9827327327327328,"Obviously, if we pick a constant c = c(n, d) for each n and d instead of c0, the above inequality also
holds with c(n, d) > c0."
HENCE,0.9834834834834835,"For the derivation of the second inequality, since at the global minimum tr(X‚àûY ‚ä§
‚àû)
=
Pmin{d,r}
i=1
¬µi = Pmin{d,n}
i=1
¬µi because for non-negative diagonal A, its SVD of the non-zero part
satisÔ¨Åes Unon‚àízero = Vnon‚àízero , we have"
HENCE,0.9842342342342343,"‚à•X‚àû‚àíY‚àû‚à•2
F = ‚à•X‚àû‚à•2
F + ‚à•Y‚àû‚à•2
F ‚àí2 tr(X‚àûY ‚ä§
‚àû) ‚â§2 ch ‚àí2"
HENCE,0.984984984984985,"min{d,n}
X"
HENCE,0.9857357357357357,"i=1
¬µi."
HENCE,0.9864864864864865,"I.3
FROM DIAGONAL MATRICES TO GENERAL MATRICES"
HENCE,0.9872372372372372,"Proof of Theorem 5.1. First all the minima of this problem are global minima and all the saddles are
unstable Ô¨Åxed points. Since each unstable Ô¨Åxed point has its stable set with size negligible when
compared to the stable set of stable Ô¨Åxed point, then for almost all initial conditions, if GD converges
to a point, for almost all situations, this point is a stable Ô¨Åxed point of the GD map."
HENCE,0.987987987987988,"By singular value decomposition (SVD), A = UDV ‚ä§, where U, D, V ‚ààRn√ón, U and V are
orthogonal matrices, and D is a non-negative diagonal matrix. Let Xk = URk, Yk = V Sk. Then

Xk+1 = Xk + h(A ‚àíXkY ‚ä§
k )Yk
Yk+1 = Yk + h(A ‚àíXkY ‚ä§
k )‚ä§Xk"
HENCE,0.9887387387387387,"‚áî

URk+1 = URk + h(UDV ‚ä§‚àíURkS‚ä§
k V ‚ä§)V Sk
V Sk+1 = V Sk + h(UDV ‚ä§‚àíURkS‚ä§
k V ‚ä§)‚ä§URk"
HENCE,0.9894894894894894,"‚áî

Rk+1 = Rk + h(D ‚àíRkS‚ä§
k )Sk
Sk+1 = Sk + h(D ‚àíRkS‚ä§
k )‚ä§Rk
."
HENCE,0.9902402402402403,"Therefore, problem (5) is equivalent to the following problem solved by GD"
HENCE,0.990990990990991,"min
R,S‚ààRn√ód
D ‚àíRS‚ä§2"
HENCE,0.9917417417417418,"F /2.
(22)"
HENCE,0.9924924924924925,Published as a conference paper at ICLR 2022
HENCE,0.9932432432432432,"From Theorem I.3 and I.4, we obtain: if GD for (22) converges to a global minimum (R, S), then
there exists a constant c > 0 independent of n, r, d, and h, s.t., the limiting minimum satisÔ¨Åes"
HENCE,0.993993993993994,"c(‚à•R‚à•2
F + ‚à•S‚à•2
F) ‚â§2 h,"
HENCE,0.9947447447447447,and the extent of balancing is quantiÔ¨Åed by
HENCE,0.9954954954954955,"‚à•R ‚àíS‚à•2
F ‚â§2 ch ‚àí2"
HENCE,0.9962462462462462,"min{d,r}
X"
HENCE,0.996996996996997,"i=1
¬µi."
HENCE,0.9977477477477478,"Especially, when d = 1, n ‚ààN+, c = 1."
HENCE,0.9984984984984985,"Since X = UR, Y = V S ‚áíR = U ‚ä§X, S = V ‚ä§Y , we have ‚à•R‚à•F = ‚à•X‚à•F, ‚à•S‚à•F = ‚à•Y ‚à•F,
and ‚à•R ‚àíS‚à•F =
U ‚ä§X ‚àíV ‚ä§Y

F =
X ‚àí(UV ‚ä§)Y

F. The we obtain the Ô¨Årst and second"
HENCE,0.9992492492492493,"inequalities in Theorem 5.1. Also, by SVD,
XY ‚ä§2
F = Pmin{d,n}
i=1
¬µ2
i ‚â§‚à•X‚à•2
F ‚à•Y ‚à•2
F, we obtain
the third inequality."
