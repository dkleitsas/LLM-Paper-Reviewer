Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0016638935108153079,"Statistical properties such as mean and variance often change over time in time
series, i.e., time-series data suffer from a distribution shift problem. This change
in temporal distribution is one of the main challenges that prevent accurate time-
series forecasting. To address this issue, we propose a simple yet effective nor-
malization method called reversible instance normalization (RevIN), a generally-
applicable normalization-and-denormalization method with learnable afﬁne trans-
formation. The proposed method is symmetrically structured to remove and re-
store the statistical information of a time-series instance, leading to signiﬁcant
performance improvements in time-series forecasting, as shown in Fig. 1. We
demonstrate the effectiveness of RevIN via extensive quantitative and qualitative
analyses on various real-world datasets, addressing the distribution shift problem."
ABSTRACT,0.0033277870216306157,∗Both authors contributed equally. The order of the ﬁrst authors was determined by coin ﬂip.
ABSTRACT,0.004991680532445923,Published as a conference paper at ICLR 2022
INTRODUCTION,0.0066555740432612314,"1
INTRODUCTION"
INTRODUCTION,0.008319467554076539,"Time-series forecasting plays a signiﬁcant role in addressing various daily problems, including
health care, economics, and trafﬁc data analyses (Kim et al., 2021a; Ahmadi et al., 2019; Park
et al., 2020). Recently, time-series forecasting models have achieved outstanding performance on
these problems, overcoming several challenges, such as long-term forecasting (Zhou et al., 2021;
Liu et al., 2021) and missing value imputation (Zhang et al., 2021; Kim et al., 2021b). However, the
time-series forecasting models often suffer badly from a unique characteristic in time-series data:
their statistical properties, e.g., mean and variance, can change over time. This is widely known as
the distribution shift problem, and it can yield discrepancies between the distributions of the training
and test data of the forecasting models. In time-series forecasting tasks, the training and test data
are usually divided from the original data based on a speciﬁc point in time. Accordingly, they often
hardly overlap, which is a common reason for model performance degradation. Furthermore, the in-
put sequences to the model can have different underlying distributions as well. We can assume that
the discrepancy between different input sequences can signiﬁcantly degrade the model performance."
INTRODUCTION,0.009983361064891847,"Under this assumption, if we remove non-stationary information from the input sequences, speciﬁ-
cally, the mean and standard deviation of the instances, the discrepancy in the data distributions will
be reduced, thereby improving model performance. However, applying such normalization to the
model input can cause another problem since it can prevent the model from capturing the original
data distribution. It removes non-stationary information that can be important to predict future values
in the forecasting task. The model would need to reconstruct the original distribution only using the
normalized input, which degrades its forecasting performance due to the inherent limitation. Thus,
if we explicitly return the information removed by input normalization back to the model, the model
will not have to rebuild the original distribution by itself while keeping the advantage of normalizing
the input. To accomplish this, we propose to reverse the normalization applied to the input data in
the output layer, i.e., to denormalize the model output using the normalization statistics."
INTRODUCTION,0.011647254575707155,"Inspired by this, we propose a simple yet effective normalization-and-denormalization method, re-
versible instance normalization (RevIN), which ﬁrst normalizes the input sequences and then de-
normalizes the model output sequences to solve the time-series forecasting problems against distri-
bution shift. RevIN is symmetrically structured to return the original distribution information to the
model output by scaling and shifting the output in the denormalization layer in an amount equivalent
to the shifting and scaling of the input data in the normalization layer. To verify the effectiveness of
RevIN, we conduct extensive quantitative evaluations using several state-of-the-art time-series fore-
casting methods as the baselines: Informer (Zhou et al., 2021), N-BEATS (Oreshkin et al., 2020),
and SCINet (Liu et al., 2021). We also provide an in-depth analysis of the behavior of the proposed
approach, including veriﬁcation of the assumptions on reversible instance normalization."
INTRODUCTION,0.013311148086522463,"RevIN is a ﬂexible, end-to-end trainable layer that can be applied to any arbitrarily chosen layers,
effectively suppressing non-stationary information (mean and variance of the instance) in one layer
and restoring it in another layer at a virtually symmetric position, e.g., input and output layers. De-
spite its remarkable performance, there has been no work on generalizing and expanding instance-
wise normalization-and-denormalization as a ﬂexibly applicable, trainable layer in the time-series
domain. Recently, deep learning-based time-series forecasting approaches, such as Informer (Zhou
et al., 2021) and N-BEATS (Oreshkin et al., 2020), have shown outstanding performance in time-
series forecasting. However, they have overlooked the importance of normalization, merely using
simple global preprocessing of the model input without further exploration and expecting their end-
to-end deep learning model to replace the role. Despite the simplicity of our method, there have
been no cases of using such techniques in modern deep-learning-based time-series forecasting ap-
proaches (Zhou et al., 2021; Liu et al., 2021; Oreshkin et al., 2020). In this sense, we introduce the
importance of an appropriate normalization method for deep-learning-based time-series approaches.
We propose a carefully designed, deep-learning-friendly module for time-series forecasting by com-
bining the method with the learnable afﬁne transformation, which has been widely accepted in recent
deep-learning-based normalization work (Ulyanov et al., 2016)."
INTRODUCTION,0.014975041597337771,"In summary, our contributions are as follows:"
INTRODUCTION,0.016638935108153077,"• We propose a simple yet effective normalization-and-denormalization method for time-
series, called RevIN, which is symmetrically structured to remove and restore the statisti-"
INTRODUCTION,0.018302828618968387,Published as a conference paper at ICLR 2022
INTRODUCTION,0.019966722129783693,"cal information of a time-series instance. The proposed method is generally applicable to
arbitrary deep neural networks with negligible cost."
INTRODUCTION,0.021630615640599003,"• By adding RevIN to the baseline, we achieve state-of-the-art performance on seven large-
scale real-world datasets by a signiﬁcant margin."
INTRODUCTION,0.02329450915141431,"• We conduct extensive evaluations of RevIN using quantitative analysis and qualitative vi-
sualizations to verify its effectiveness, addressing the distribution shift problem."
RELATED WORK,0.024958402662229616,"2
RELATED WORK"
RELATED WORK,0.026622296173044926,"Time-series forecasting.
Time-series forecasting methods are mainly categorized into three dis-
tinct approaches: (1) statistical methods, (2) hybrid methods, and (3) deep learning-based methods.
Statistical models are theoretically well guaranteed and have several advantages, including inter-
pretability. As an example of the statistical models, exponential smoothing forecasting (Holt, 2004;
Winters, 1960) is a well-established benchmark for predicting future values. To further boost perfor-
mance, recent work proposed a hybrid model (Smyl, 2020) that incorporates a deep learning module
with a statistical model. It achieved better performance than statistical methods in the M4 time-
series forecasting competition. The deep learning-based method basically follows the sequence-
to-sequence framework to model the time-series forecasting. Initially, deep learning-based models
utilized variations of recurrent neural networks (RNNs). However, to overcome the limitation of the
limited receptive ﬁeld, several studies utilized advanced techniques, such as the dilatation and at-
tention module. For instance, SCINet (Liu et al., 2021) and Informer (Zhou et al., 2021) modiﬁed
the sequence-to-sequence-based model to improve performance for long sequences. However, most
previous deep learning-based models are hard to interpret compared to statistical models. Thus, in-
spired by statistical models, N-BEATS (Oreshkin et al., 2020) designed an interpretable layer for
time-series forecasting by encouraging the model to learn trend, seasonality explicitly, and residual
components. This model shows superior performance on the M4 competition dataset."
RELATED WORK,0.028286189683860232,"Distribution shift.
Although there are various models for time-series forecasting, they often suf-
fer from non-stationary time-series, where the data distribution changes over time. Domain adapta-
tion (Tzeng et al., 2017; Ganin et al., 2016; Wang et al., 2018) and domain generalization (Wang
et al., 2021; Li et al., 2018; Muandet et al., 2013) are common ways to alleviate the distribution
shift. A domain adaptation algorithm attempts to reduce the distribution gap between source and
target domains. A domain generalization algorithm only relies on the source domain and hopes to
generalize on the target domain. Both domain adaptation and generalization have a common ob-
jective, which bridges the gap between source and target distributions. However, deﬁning a domain
is not straightforward in non-stationary time series since the data distribution shifts over time. Re-
cently, Du et al. (Du et al., 2021) proposed Adaptive RNNs to handle the distribution shift problems
of non-stationary time-series data. It ﬁrst characterizes the distribution information by splitting the
training data into periods. Then, it matches the distributions of the discovered periods to general-
ize the model. However, unlike Adaptive RNNs, which is costly, RevIN is simple yet effective and
model-agnostic. The method can be easily adopted to any deep neural network."
PROPOSED METHOD,0.029950083194675542,"3
PROPOSED METHOD"
PROPOSED METHOD,0.03161397670549085,"This section proposes reversible instance normalization to alleviate the distribution shift problem in
time-series, which is known to cause a substantial discrepancy between the training and test data
distributions. Section 3.1 describes the proposed method in detail, and Section 3.2 discusses how
our approach mitigates the distribution discrepancy in time-series data."
REVERSIBLE INSTANCE NORMALIZATION,0.033277870216306155,"3.1
REVERSIBLE INSTANCE NORMALIZATION"
REVERSIBLE INSTANCE NORMALIZATION,0.03494176372712146,"Given a set of input X = {x(i)}N
i=1 and the corresponding target Y = {y(i)}N
i=1, we consider a mul-
tivariate time-series forecasting task in discrete time, where N denotes the number of sequences.
Let K, Tx, and Ty denote the number of variables, the input sequence length, and the model predic-
tion length, respectively. Given an input sequence x(i) ∈RK×Tx, we aim to solve the time-series
forecasting problem, which is to predict the subsequent values y(i) ∈RK×Ty. In RevIN, the input"
REVERSIBLE INSTANCE NORMALIZATION,0.036605657237936774,"Published as a conference paper at ICLR 2022 0 0
0 (b-2) (b-1) 0 0 0 0 (b-4) (b-3) (a-1)"
REVERSIBLE INSTANCE NORMALIZATION,0.03826955074875208,"Instance 
normalization 0 0"
REVERSIBLE INSTANCE NORMALIZATION,0.03993344425956739,"RevIN
RevIN (a-2)"
REVERSIBLE INSTANCE NORMALIZATION,0.04159733777038269,"𝜃𝜃
Source distribution
Target distribution
0 0 0"
REVERSIBLE INSTANCE NORMALIZATION,0.04326123128119801,"Denormalization
(a-3) Non-stationary information"
REVERSIBLE INSTANCE NORMALIZATION,0.04492512479201331,"𝝁𝝁, 𝝈𝝈, 𝜷𝜷, 𝜸𝜸"
REVERSIBLE INSTANCE NORMALIZATION,0.04658901830282862,"Target distribution
Source distribution"
REVERSIBLE INSTANCE NORMALIZATION,0.048252911813643926,"Figure 2: Overview of the proposed method. We illustrate an example of a univariate case, where
x(i) ∈R1×Tx; the input data x(i) is actually multivariate (See Section 3.1). In RevIN, the (a-1)
instance normalization and (a-2) denormalization are symmetrically structured to remove (a-3) non-
stationary information from one layer and restore it on the other layer. Here, RevIN is applied to
the input and output layers. The (a-3) non-stationary information includes statistical properties from
the input data: mean µ, variance σ2, and learnable afﬁne parameters γ, β. The normalization layer
transforms the (b-1) original data distribution into a (b-2) mean-centered distribution, where the
distribution discrepancy between different instances is reduced. Using ˆx, the model predicts the
future values ˜y following the (b-3) distribution where non-stationary information is eliminated. To
restore it (b-4), RevIN reverses the instance normalization in the output layer."
REVERSIBLE INSTANCE NORMALIZATION,0.04991680532445923,"sequence length Tx and the prediction length Ty can be different since the observations are normal-
ized and denormalized across the temporal dimension, as will be explained below. Our proposed
method, RevIN, consists of symmetrically structured normalization-and-denormalization layers, as
illustrated in Fig. 2. First, we normalize the input data x(i) using its instance-speciﬁc mean and stan-
dard deviation, which is widely accepted as instance normalization (Ulyanov et al., 2016). The mean
and standard deviation are computed for every instance x(i)"
REVERSIBLE INSTANCE NORMALIZATION,0.051580698835274545,k· ∈RTx of the input data (Fig. 2(a-3)) as
REVERSIBLE INSTANCE NORMALIZATION,0.05324459234608985,Et[x(i)
REVERSIBLE INSTANCE NORMALIZATION,0.05490848585690516,"kt ] = 1 Tx Tx
X"
REVERSIBLE INSTANCE NORMALIZATION,0.056572379367720464,"j=1
x(i)"
REVERSIBLE INSTANCE NORMALIZATION,0.05823627287853577,"kj
and
Var[x(i)"
REVERSIBLE INSTANCE NORMALIZATION,0.059900166389351084,"kt ] = 1 Tx Tx
X j=1"
REVERSIBLE INSTANCE NORMALIZATION,0.06156405990016639," 
x(i)"
REVERSIBLE INSTANCE NORMALIZATION,0.0632279534109817,kj −Et[x(i)
REVERSIBLE INSTANCE NORMALIZATION,0.064891846921797,"kt ]
2.
(1)"
REVERSIBLE INSTANCE NORMALIZATION,0.06655574043261231,"Using these statistics, we normalize the input data x(i) (Fig. 2(a-1)) as ˆx(i)"
REVERSIBLE INSTANCE NORMALIZATION,0.06821963394342762,kt = γk x(i)
REVERSIBLE INSTANCE NORMALIZATION,0.06988352745424292,kt −Et[x(i)
REVERSIBLE INSTANCE NORMALIZATION,0.07154742096505824,"kt ]
q"
REVERSIBLE INSTANCE NORMALIZATION,0.07321131447587355,Var[x(i)
REVERSIBLE INSTANCE NORMALIZATION,0.07487520798668885,kt ] + ϵ !
REVERSIBLE INSTANCE NORMALIZATION,0.07653910149750416,"+ βk,
(2)"
REVERSIBLE INSTANCE NORMALIZATION,0.07820299500831947,"where γ, β ∈RK are learnable afﬁne parameter vectors. The normalized sequences can have a
more consistent mean and variance, where the non-stationary information is reduced. As a result, the
normalization layer allows the model to accurately predict the local dynamics within the sequence
while receiving inputs of consistent distributions in terms of the mean and variance."
REVERSIBLE INSTANCE NORMALIZATION,0.07986688851913477,"The model then receives the transformed data ˆx(i) as input and forecasts their future values. How-
ever, the input data have different statistics than the original distribution, and by observing only
the normalized input ˆx(i), it is difﬁcult to capture the original distribution of the input x(i). Thus,
to make this easier for the model, we explicitly return the non-stationary properties removed from
the input data to the model output by reversing the normalization step at a symmetric position, the
output layer. A denormalization step can return the model output to the original time-series value as
well (Ogasawara et al., 2010). Accordingly, we denormalize the model output ˜y(i) by applying the"
REVERSIBLE INSTANCE NORMALIZATION,0.08153078202995008,Published as a conference paper at ICLR 2022
REVERSIBLE INSTANCE NORMALIZATION,0.08319467554076539,"ETTh1
ECL"
REVERSIBLE INSTANCE NORMALIZATION,0.08485856905158069,"Density
Density
Density
ETTm1"
REVERSIBLE INSTANCE NORMALIZATION,0.08652246256239601,"(a) original input
(b) RevIN-normalized 
(c) model output
(d) RevIN-denormalized
→
→"
REVERSIBLE INSTANCE NORMALIZATION,0.08818635607321132,"Figure 3: Effect of RevIN on distribution discrepancy between training and test data. From left
to right columns, we compare the training and test data distributions of a variable on each step of the
sequential process in RevIN: (a) the original input x, (b) the input ˆx normalized by RevIN, (c) the
model prediction output ˜y, and (d) the output ˆy denormalized by RevIN, the ﬁnal prediction. The
analysis is conducted on the ETT and ECL datasets using SCINet (Liu et al., 2021) as the baseline."
REVERSIBLE INSTANCE NORMALIZATION,0.08985024958402663,reciprocal of the normalization in Eq. 2 (Fig. 2(a-3)) as ˆy(i)
REVERSIBLE INSTANCE NORMALIZATION,0.09151414309484193,"kt =
q"
REVERSIBLE INSTANCE NORMALIZATION,0.09317803660565724,Var[x(i)
REVERSIBLE INSTANCE NORMALIZATION,0.09484193011647254,kt ] + ϵ · ˜y(i)
REVERSIBLE INSTANCE NORMALIZATION,0.09650582362728785,kt −βk γk !
REVERSIBLE INSTANCE NORMALIZATION,0.09816971713810316,+ Et[x(i)
REVERSIBLE INSTANCE NORMALIZATION,0.09983361064891846,"kt ].
(3)"
REVERSIBLE INSTANCE NORMALIZATION,0.10149750415973377,"The same statistics used in the normalization step in Eq. 2 are used for the scaling and shifting. Now,
ˆy(i) is the ﬁnal prediction of the model instead of ˜y(i)."
REVERSIBLE INSTANCE NORMALIZATION,0.10316139767054909,"Simply added to virtually symmetric positions in a network, RevIN can effectively alleviate dis-
tribution discrepancy in time-series data, as a generally-applicable trainable normalization layer to
arbitrary deep neural networks. Indeed, the proposed method is a ﬂexible, end-to-end trainable layer
that can be applied to any arbitrarily chosen layers, even to several layers. We verify its effectiveness
as a ﬂexible layer by adding it to the intermediate layers in the model in Table 7 in Appendix A.4.
Nevertheless, RevIN is most effective when applied to virtually symmetric layers of encoder-decoder
structure. In a typical time-series forecasting model, the boundary between the encoder and the de-
coder is often unclear. Thus, we apply RevIN to the input and output layers of a model as they can
be interpreted as an encoder-decoder structure, generating subsequent values, given input data."
EFFECT OF REVERSIBLE INSTANCE NORMALIZATION ON DISTRIBUTION SHIFT,0.1048252911813644,"3.2
EFFECT OF REVERSIBLE INSTANCE NORMALIZATION ON DISTRIBUTION SHIFT"
EFFECT OF REVERSIBLE INSTANCE NORMALIZATION ON DISTRIBUTION SHIFT,0.1064891846921797,"This section veriﬁes that RevIN can alleviate the distribution discrepancy problem by removing
non-stationary information in the input layer and then restoring it in the output layer. We analyze the
distributions of the training and test data at each step of the proposed approach, as shown in Fig. 3."
EFFECT OF REVERSIBLE INSTANCE NORMALIZATION ON DISTRIBUTION SHIFT,0.10815307820299501,"When comparing the distribution of training and test data in each example (Fig. 3(a-b)), we can
observe that RevIN signiﬁcantly reduces their discrepancy. To be speciﬁc, in the original input
(Fig. 3(a)), the training and test data distributions hardly overlap (especially ETTm1), which is
caused by the distribution shift problem. Also, each data distribution has multiple peaks (especially
the test data of ETTh1 and ECL), implying that sequences in the data might have severe discrepan-
cies in their distributions. However, in the proposed approach, the normalization step transforms
each data distribution into mean-centered distributions (Fig. 3(b)). This result supports that the
original multimodal distributions (Fig. 3(a)) are caused by discrepancies in distributions between
different sequences in the data. Even more, the proposed approach makes training and test data dis-
tributions overlapped. This veriﬁes that the normalization step of RevIN can alleviate the distribution
shift problem, reducing the distribution discrepancy between training and test data."
EFFECT OF REVERSIBLE INSTANCE NORMALIZATION ON DISTRIBUTION SHIFT,0.10981697171381032,Published as a conference paper at ICLR 2022
EFFECT OF REVERSIBLE INSTANCE NORMALIZATION ON DISTRIBUTION SHIFT,0.11148086522462562,"Taking the normalized data as the input, the model can retain aligned training and test data distri-
butions in the prediction output (Fig. 3(c)). As expected, these are then returned back to the orig-
inal distribution by the denormalization step of RevIN (Fig. 3(d)). Without denormalization, the
model needs to reconstruct the values that follow the original distributions (Fig. 3(d)) using only
the normalized input that follows the transformed distributions where non-stationary information is
removed (Fig. 3(b)). Additionally, we hypothesize that the distribution discrepancy will be reduced
in the intermediate layers of the model as well, when RevIN is applied at the input and output layers
only, which will be discussed in Section 4.2.3. As a result, this RevIN procedure can be considered
to ﬁrst make problems easier, and then restore them back to the original state, rather than directly
solving the challenging problem where the distribution shift problem exists."
EXPERIMENTS,0.11314475873544093,"4
EXPERIMENTS"
EXPERIMENTS,0.11480865224625623,This section describes the experimental setup and provides extensive experimental results of RevIN.
EXPERIMENTAL SETUP,0.11647254575707154,"4.1
EXPERIMENTAL SETUP"
EXPERIMENTAL SETUP,0.11813643926788686,"Datasets. We evaluate our methods mainly on four large-scale real-world time-series datasets. Ad-
ditionally, we provide experimental results on three more datasets, including the air quality and Nas-
daq datasets taken from the UCI repository and M4 competition dataset (Makridakis et al., 2020)
in Appendix A.1. (i) Electricity transformer temperature (ETT)1 data consists of seven features,
including power load features and oil temperature. It is collected from two different regions in China
for two years. Following the same protocol as Informer (Zhou et al., 2021), we split the data into
three datasets: ETTh1, ETTh2, and ETTm1. The ETTh1 and ETTh2 datasets are hourly data ob-
tained from different regions. The ETTm1 dataset has a value every 15 minutes. For each dataset,
we split the ﬁrst 12 months, the middle four months, and the last four months as training, validation,
and test data, respectively. (ii) Electricity Consuming Load (ECL)2 data contains the electricity
consumption (kWh) collected from 321 clients. Following the prior work (Zhou et al., 2021), data
from each client is used as a variable on an hourly basis in the multivariate forecasting setting. For
the ECL dataset, we use 15, 3, and 4 months as training, validation, and test data, respectively."
EXPERIMENTAL SETUP,0.11980033277870217,"Experimental details. We set the prediction lengths to be one day (1d), 2d, 7d, 14d, 30d, and 40d
for the hourly-basis datasets, ETTh1, ETTh2, and ECL. For the ETTm1 dataset, we chose six hours
(6h), 12h, 3d, 7d, and 14d as the prediction window lengths. We evaluate the time-series forecasting
performance on the mean squared error (MSE) and mean absolute error (MAE). Following the same
evaluation procedure used in the previous study (Zhou et al., 2021), we compute the MSE and MAE
on z-score normalized data to measure different variables on the same scale. More details on exper-
imental settings, including training details and hyperparameters, are provided in Appendix A.11."
EXPERIMENTAL SETUP,0.12146422628951747,"Baselines compared. RevIN is a model-agnostic method, generally applicable to any deep neural
network. In this paper, we verify the effectiveness of RevIN by adopting it to three state-of-the-art
time-series forecasting models: Informer (Zhou et al., 2021), N-BEATS (Oreshkin et al., 2020), and
SCINet (Liu et al., 2021). These are non-autoregressive forecasting models. The reproduction details
for the baselines are provided in Appendix A.12. Unless stated otherwise, we compare RevIN and
the baselines under the same hyperparameter settings, including the input and prediction lengths."
RESULTS AND ANALYSES,0.12312811980033278,"4.2
RESULTS AND ANALYSES"
RESULTS AND ANALYSES,0.12479201331114809,"This section provides the quantitative analysis and qualitative visualization results of RevIN in com-
parison with the state-of-the-art time-series forecasting baselines."
"EFFECTIVENESS OF REVERSIBLE INSTANCE NORMALIZATION ON VARIOUS
TIME-SERIES FORECASTING MODELS",0.1264559068219634,"4.2.1
EFFECTIVENESS OF REVERSIBLE INSTANCE NORMALIZATION ON VARIOUS
TIME-SERIES FORECASTING MODELS"
"EFFECTIVENESS OF REVERSIBLE INSTANCE NORMALIZATION ON VARIOUS
TIME-SERIES FORECASTING MODELS",0.1281198003327787,"Table 1 compares the forecasting accuracy of the baselines and RevIN. The results show that RevIN
consistently outperforms all three baselines, Informer, N-BEATS, and SCINet, by a large margin,"
"EFFECTIVENESS OF REVERSIBLE INSTANCE NORMALIZATION ON VARIOUS
TIME-SERIES FORECASTING MODELS",0.129783693843594,"1https://github.com/zhouhaoyi/ETDataset
2https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014"
"EFFECTIVENESS OF REVERSIBLE INSTANCE NORMALIZATION ON VARIOUS
TIME-SERIES FORECASTING MODELS",0.1314475873544093,Published as a conference paper at ICLR 2022
"EFFECTIVENESS OF REVERSIBLE INSTANCE NORMALIZATION ON VARIOUS
TIME-SERIES FORECASTING MODELS",0.13311148086522462,"Table 1: Comparison of forecasting errors between the baselines and RevIN. The analysis on the
four datasets, ETTh1, ETTh2, ETTm1, and ECL, is conducted by increasing the prediction length
from 24 to 960/1344. We report the average errors for ﬁve runs. The complete results are provided
in Appendix A.14, including standard deviation and the originally reported values for the baselines."
"EFFECTIVENESS OF REVERSIBLE INSTANCE NORMALIZATION ON VARIOUS
TIME-SERIES FORECASTING MODELS",0.13477537437603992,"Method
Informer
+ RevIN
N-BEATS
+ RevIN
SCINet
+ RevIN"
"EFFECTIVENESS OF REVERSIBLE INSTANCE NORMALIZATION ON VARIOUS
TIME-SERIES FORECASTING MODELS",0.13643926788685523,"Metric
MSE
MAE
MSE
MAE
MSE
MAE
MSE
MAE
MSE
MAE
MSE
MAE ETTh1"
"EFFECTIVENESS OF REVERSIBLE INSTANCE NORMALIZATION ON VARIOUS
TIME-SERIES FORECASTING MODELS",0.13810316139767054,"24
0.550
0.536
0.504
0.472
0.478
0.505
0.330
0.373
0.338
0.373
0.308
0.347
48
0.772
0.668
0.646
0.547
0.536
0.542
0.372
0.400
0.436
0.459
0.365
0.389
168
1.138
0.853
0.655
0.561
1.005
0.782
0.466
0.452
0.459
0.461
0.406
0.416
336
1.278
0.909
1.058
0.758
0.932
0.743
0.515
0.483
0.527
0.513
0.467
0.471
720
1.357
0.945
0.926
0.717
1.389
0.926
0.576
0.534
0.596
0.571
0.507
0.505
960
1.470
0.990
0.902
0.715
1.383
0.932
0.678
0.575
0.604
0.574
0.545
0.526 ETTh2"
"EFFECTIVENESS OF REVERSIBLE INSTANCE NORMALIZATION ON VARIOUS
TIME-SERIES FORECASTING MODELS",0.13976705490848584,"24
0.450
0.520
0.238
0.325
0.403
0.472
0.192
0.276
0.199
0.295
0.180
0.263
48
2.171
1.200
0.361
0.404
1.330
0.918
0.254
0.320
0.350
0.422
0.231
0.302
168
8.157
2.558
0.859
0.649
7.174
2.329
0.410
0.418
0.559
0.518
0.337
0.378
336
4.746
1.844
0.890
0.673
4.859
1.863
0.449
0.447
0.664
0.583
0.357
0.403
720
3.190
1.529
0.576
0.546
5.656
2.012
0.496
0.482
1.546
0.944
0.411
0.445
960
2.972
1.441
0.600
0.570
6.408
2.077
0.471
0.481
1.862
1.066
0.438
0.462 ETTm1"
"EFFECTIVENESS OF REVERSIBLE INSTANCE NORMALIZATION ON VARIOUS
TIME-SERIES FORECASTING MODELS",0.14143094841930118,"24
0.330
0.382
0.309
0.352
0.443
0.437
0.403
0.392
0.130
0.231
0.106
0.196
48
0.499
0.486
0.390
0.391
0.453
0.472
0.328
0.371
0.155
0.262
0.135
0.222
96
0.605
0.554
0.405
0.411
0.603
0.581
0.379
0.406
0.195
0.291
0.162
0.247
288
0.906
0.738
0.563
0.502
0.849
0.702
0.451
0.445
0.361
0.419
0.265
0.321
672
0.943
0.760
0.663
0.550
0.860
0.726
0.555
0.511
1.020
0.756
0.357
0.380
1344
1.095
0.823
0.824
0.632
14.613
1.948
0.631
0.556
1.841
1.044
0.412
0.422 ECL"
"EFFECTIVENESS OF REVERSIBLE INSTANCE NORMALIZATION ON VARIOUS
TIME-SERIES FORECASTING MODELS",0.14309484193011648,"24
0.250
0.358
0.148
0.257
0.279
0.372
0.176
0.285
0.138
0.246
0.112
0.207
48
0.300
0.386
0.171
0.279
0.309
0.388
0.194
0.301
0.163
0.265
0.126
0.222
168
0.345
0.423
0.261
0.354
0.333
0.410
0.218
0.320
0.177
0.281
0.153
0.249
336
0.429
0.473
0.356
0.414
0.326
0.406
0.241
0.337
0.202
0.308
0.162
0.262
720
0.851
0.719
0.834
0.700
0.420
0.467
0.303
0.383
0.234
0.333
0.183
0.281
960
0.930
0.750
0.894
0.741
0.399
0.455
0.325
0.398
0.235
0.330
0.200
0.292"
"EFFECTIVENESS OF REVERSIBLE INSTANCE NORMALIZATION ON VARIOUS
TIME-SERIES FORECASTING MODELS",0.1447587354409318,"Table 2: Comparison of long sequence forecasting performance. We analyze the forecasting error
of the baselines and RevIN by increasing the prediction length from 48 to 960 while the input length
is ﬁxed to 48. The experiment is conducted on ETTh1. The average errors for ﬁve runs are reported,
and the complete results, including standard deviation, are provided in Appendix A.14."
"EFFECTIVENESS OF REVERSIBLE INSTANCE NORMALIZATION ON VARIOUS
TIME-SERIES FORECASTING MODELS",0.1464226289517471,"Prediction length
48
168
336
720
960"
"EFFECTIVENESS OF REVERSIBLE INSTANCE NORMALIZATION ON VARIOUS
TIME-SERIES FORECASTING MODELS",0.1480865224625624,"Metric
MSE
MAE
MSE
MAE
MSE
MAE
MSE
MAE
MSE
MAE"
"EFFECTIVENESS OF REVERSIBLE INSTANCE NORMALIZATION ON VARIOUS
TIME-SERIES FORECASTING MODELS",0.1497504159733777,"Informer
0.687
0.628
0.982
0.795
1.212
0.893
1.157
0.863
1.203
0.888
+ RevIN
0.540
0.481
0.680
0.574
0.939
0.696
1.021
0.752
1.061
0.775"
"EFFECTIVENESS OF REVERSIBLE INSTANCE NORMALIZATION ON VARIOUS
TIME-SERIES FORECASTING MODELS",0.15141430948419302,"N-BEATS
0.512
0.523
0.804
0.690
1.001
0.773
1.022
0.765
0.901
0.728
+ RevIN
0.365
0.389
0.454
0.438
0.526
0.477
0.568
0.514
0.638
0.544"
"EFFECTIVENESS OF REVERSIBLE INSTANCE NORMALIZATION ON VARIOUS
TIME-SERIES FORECASTING MODELS",0.15307820299500832,"SCINet
0.376
0.396
0.600
0.556
0.841
0.695
0.875
0.721
0.900
0.737
+ RevIN
0.349
0.370
0.445
0.426
0.509
0.461
0.533
0.494
0.557
0.510"
"EFFECTIVENESS OF REVERSIBLE INSTANCE NORMALIZATION ON VARIOUS
TIME-SERIES FORECASTING MODELS",0.15474209650582363,"achieving state-of-the-art performance on the four datasets. Moreover, the effectiveness of RevIN
is more evident for the long sequence prediction, where it remarkably reduces the errors of the
baselines. RevIN shows a stable performance in contrast to the baselines, which show a high increase
in error as prolonging the prediction length. For example, when the prediction length increases from
24 to 960 on the ETTh2 dataset, the forecasting error of N-BEATS signiﬁcantly increases from
0.403 to 6.408. In contrast, RevIN shows a much slight increase in error, i.e., from 0.192 to 0.471.
A similar tendency appears with the other prediction lengths, datasets, and baseline models as well.
These results demonstrate that RevIN makes the baseline model more robust to prediction length."
"EFFECTIVENESS OF REVERSIBLE INSTANCE NORMALIZATION ON VARIOUS
TIME-SERIES FORECASTING MODELS",0.15640599001663893,Published as a conference paper at ICLR 2022
"EFFECTIVENESS OF REVERSIBLE INSTANCE NORMALIZATION ON VARIOUS
TIME-SERIES FORECASTING MODELS",0.15806988352745424,"Figure 4: Forecasting error for each time step. We compare the error of predicting 1∼960 steps
ahead between the baselines and RevIN on ETTh1 when the prediction length is 960 (40 days)."
"EFFECTIVENESS OF REVERSIBLE INSTANCE NORMALIZATION ON VARIOUS
TIME-SERIES FORECASTING MODELS",0.15973377703826955,"Table 3: Comparison with classical and state-of-the-art normalization methods. The mean
squared errors are compared on the four datasets, using N-BEATS as the baseline for all experiments.
Here, every normalization method is applied to the input data. DAIN, deep adaptive input normal-
ization (Passalis et al., 2019); RevBN, the reversible batch normalization, i.e., the modiﬁed version
of RevIN. Complete results, including different prediction lengths, are provided in Appendix A.7."
"EFFECTIVENESS OF REVERSIBLE INSTANCE NORMALIZATION ON VARIOUS
TIME-SERIES FORECASTING MODELS",0.16139767054908485,"Dataset
ETTh1
ETTh2
ETTm1
ECL"
"EFFECTIVENESS OF REVERSIBLE INSTANCE NORMALIZATION ON VARIOUS
TIME-SERIES FORECASTING MODELS",0.16306156405990016,"Prediction length
168
960
168
960
96
1344
168
960"
"EFFECTIVENESS OF REVERSIBLE INSTANCE NORMALIZATION ON VARIOUS
TIME-SERIES FORECASTING MODELS",0.16472545757071547,"Min-max norm
1.074
1.224
2.987
3.308
1.035
1.320
0.374
0.387
z-score norm
0.953
1.043
3.329
3.087
1.016
1.274
0.335
0.377
Layer norm
0.871
1.303
4.092
5.822
0.502
2.488
0.343
0.379
DAIN
0.996
1.032
1.982
2.802
0.672
1.348
0.347
0.381"
"EFFECTIVENESS OF REVERSIBLE INSTANCE NORMALIZATION ON VARIOUS
TIME-SERIES FORECASTING MODELS",0.16638935108153077,"Batch norm
0.851
1.691
6.206
7.755
0.505
1.147
0.320
0.422
RevBN
0.717
0.779
0.729
2.148
0.601
0.991
0.327
0.414"
"EFFECTIVENESS OF REVERSIBLE INSTANCE NORMALIZATION ON VARIOUS
TIME-SERIES FORECASTING MODELS",0.16805324459234608,"Instance norm
0.946
1.090
3.240
3.145
1.021
1.329
0.333
0.386
RevIN (ours)
0.515
0.697
0.419
0.465
0.388
0.602
0.220
0.329"
"EFFECTIVENESS OF REVERSIBLE INSTANCE NORMALIZATION ON VARIOUS
TIME-SERIES FORECASTING MODELS",0.16971713810316139,"We further quantitatively analyze the effect of RevIN on long sequence prediction in Table 2. When
the model prediction length is increased from 48 to 960, RevIN reduces the prediction error com-
pared to the baseline, showing robust performance against the prediction length. The difference in
the forecasting error between SCINet and RevIN is relatively small when the prediction length is
short (e.g., 48), but RevIN remarkably surpasses SCINet by a signiﬁcant margin when the prediction
length is long (e.g., 336, 720, and 960). These results substantiate that adopting RevIN can make a
model robust to the prediction length."
"EFFECTIVENESS OF REVERSIBLE INSTANCE NORMALIZATION ON VARIOUS
TIME-SERIES FORECASTING MODELS",0.1713810316139767,"Additionally, to study how RevIN can perform well in long sequence prediction, we visualize
the forecasting error for each time step in Fig. 4. The error at the t-th time step is computed as
MSE-t =
1
N
PN
i=1
1
K
PK
k=1(ˆy(i)
kt −y(i)
kt )2. Overall, RevIN shows superior performance compared
to the baselines; the performance degradation is signiﬁcantly slower, showing low error even when
forecasting 960 steps ahead. Speciﬁcally, for N-BEATS and SCINet, the error extremely increases
as predicting the distant future values. RevIN alleviates this signiﬁcant increase in error, showing
remarkable performance compared to the baselines. Informer also shows unstable performance for
the different time steps. The error is substantial in the early steps and becomes relatively small in
the distant steps. RevIN allows the models to have consistently minor errors in every time step, even
where the baselines originally show high error (early steps for Informer, distant steps for N-BEATS
and SCINet). The results demonstrate the effectiveness of RevIN when forecasting long sequences."
COMPARISON WITH EXISTING NORMALIZATION METHODS,0.17304492512479203,"4.2.2
COMPARISON WITH EXISTING NORMALIZATION METHODS"
COMPARISON WITH EXISTING NORMALIZATION METHODS,0.17470881863560733,"We compare RevIN with classical and state-of-the-art normalization methods, including min-
max normalization, z-score normalization, layer normalization (Ba et al., 2016), batch normaliza-
tion (Ioffe & Szegedy, 2015), instance normalization (Ulyanov et al., 2016), and deep adaptive input"
COMPARISON WITH EXISTING NORMALIZATION METHODS,0.17637271214642264,"Published as a conference paper at ICLR 2022 +
+"
COMPARISON WITH EXISTING NORMALIZATION METHODS,0.17803660565723795,"featrue divergence 
Train-test data +"
COMPARISON WITH EXISTING NORMALIZATION METHODS,0.17970049916805325,"Layer-1
Layer-2
Layer-1
Layer-2
Layer-1
Layer-2
Layer-1
Layer-2 +"
COMPARISON WITH EXISTING NORMALIZATION METHODS,0.18136439267886856,"Figure 5: Feature divergence between the training and test data in the intermediate layers of the
model The feature divergences are computed on the ETTh1, ETTh2, ETTm1, and ECL datasets using
the features obtained from the ﬁrst (Layer-1) and the second (Layer-2) encoder layers in Informer."
COMPARISON WITH EXISTING NORMALIZATION METHODS,0.18302828618968386,"normalization (DAIN) (Passalis et al., 2019) in Table 3. Here, we compute the statistics for min-max
and z-score normalization methods for every input instance, not for the entire data. Additionally, we
attempt to use batch normalization as the input normalization method in RevIN, named reversible
batch normalization (RevBN). Layer normalization cannot be used in a similar manner since it is
not reversible when the input and prediction lengths are different, as in our experimental settings."
COMPARISON WITH EXISTING NORMALIZATION METHODS,0.18469217970049917,"As a result, RevIN shows outstanding performance compared to the other normalization methods,
especially on ETTh2 and ETTm1 datasets. Also, RevBN improves forecasting performance of batch
normalization. Speciﬁcally, the error considerably decreases in the long sequence prediction, such as
960 and 1344. This result supports that the denormalization step of RevIN is essential as a key com-
ponent of the proposed method for improving long sequence forecasting. However, batch normal-
ization applies identical normalization to all the input sequences, using the global statistics obtained
from the entire training data; it can not reduce the discrepancy between the training and test data
distributions. Consequently, RevIN, which transforms the data in the instance level, outperforms
RevBN by a signiﬁcant margin, demonstrating that successfully reducing the discrepancy between
distributions of different input sequences can effectively improve performance. Moreover, RevIN not
only shows the best performance but also has the advantage of being lightweight compared to the
baselines. For example, when K is the number of variables, DAIN requires at least 3K2 additional
parameters, whereas RevIN only requires 2K additional parameters."
ANALYSIS OF DISTRIBUTION SHIFT IN THE INTERMEDIATE LAYERS,0.18635607321131448,"4.2.3
ANALYSIS OF DISTRIBUTION SHIFT IN THE INTERMEDIATE LAYERS"
ANALYSIS OF DISTRIBUTION SHIFT IN THE INTERMEDIATE LAYERS,0.18801996672212978,"In Fig. 5, we analyze the feature divergence between the training and test data to verify that RevIN
can reduce the distribution shift at the intermediate feature level as well. We conduct the experiment
using Informer as the baseline; it comprises two encoder layers and one decoder layer. Thus, we an-
alyze the features of the ﬁrst (Layer-1) and the second (Layer-2) encoder layers. Following the prior
work (Pan et al., 2018), we compute the average feature divergence using symmetric KL divergence
(See Appendix A.10). The results show that RevIN signiﬁcantly reduces the feature divergence be-
tween the training and test data in both layers, demonstrating that the proposed approach, when
added only to the input and output layers, successfully alleviates the distribution shift problem in the
intermediate layers. Moreover, this strengthens RevIN as a generally-applicable ﬂexible layer. An
arbitrary model can adopt RevIN by adding it to input and output layers without any architectural
modiﬁcations. Note that our approach still can be added to any arbitrarily chosen layers, signiﬁcantly
improving the model performance, as shown in Appendix A.4."
CONCLUSION,0.1896838602329451,"5
CONCLUSION"
CONCLUSION,0.1913477537437604,"This paper aims to address the distribution shift problem in time series, proposing a simple yet ef-
fective normalization-and-denormalization method, reversible instance normalization (RevIN). The
proposed approach effectively alleviates the discrepancy between training and test data distribu-
tions, leading to signiﬁcant performance improvements in time-series forecasting. As a generally-
applicable layer to arbitrary deep neural networks, the proposed approach achieves state-of-the-art
performance on seven real-world time-series datasets by a signiﬁcant margin. The extensive quanti-
tative and qualitative experiments with in-depth analysis demonstrate the effectiveness of RevIN for
accurate time-series forecasting against the distribution shift problem."
CONCLUSION,0.1930116472545757,Published as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.194675540765391,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.19633943427620631,"To ensure reproducibility, we provide the source code of our method publicly, including the pre-
trained model weights. In the main manuscript, Section 4.1 describes how we conduct data prepro-
cessing on the datasets used in the experiments. Appendix A.11 explains the experimental details,
including random seed values for the experiments. Appendix A.12 provides a detailed explanation
of hyperparameter conﬁgurations with the reproduction details of the baselines."
REPRODUCIBILITY STATEMENT,0.19800332778702162,ACKNOWLEDGMENTS
REPRODUCIBILITY STATEMENT,0.19966722129783693,"This work was supported by the Institute of Information & communications Technology Planning
& Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2018-0-00219, Space-
time complex artiﬁcial intelligence blue-green algae prediction technology based on direct-readable
water quality complex sensor and hyperspectral image, and No.2019-0-00075, Artiﬁcial Intelligence
Graduate School Program (KAIST))."
REFERENCES,0.20133111480865223,REFERENCES
REFERENCES,0.20299500831946754,"Mohsen Ahmadi, Saeid Jafarzadeh-Ghoushchi, Rahim Taghizadeh, and Abbas Shariﬁ. Presentation
of a new hybrid approach for forecasting economic growth using artiﬁcial intelligence approaches.
Neural Computing and Applications, 31(12):8661–8680, 2019."
REFERENCES,0.20465890183028287,"Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016."
REFERENCES,0.20632279534109818,"Yuntao Du, Jindong Wang, Wenjie Feng, Sinno Pan, Tao Qin, Renjun Xu, and Chongjun Wang.
Adarnn: Adaptive learning and forecasting of time series. Proc. the International Conference on
Information and Knowledge Management (CIKM), 2021."
REFERENCES,0.2079866888519135,"Laura Fr´ıas-Paredes, Ferm´ın Mallor, Mart´ın Gast´on-Romeo, and Teresa Le´on. Assessing energy
forecasting inaccuracy by simultaneously considering temporal and absolute errors. Energy Con-
version and Management, 142:533–546, 2017."
REFERENCES,0.2096505823627288,"Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc¸ois Lavi-
olette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks.
The journal of machine learning research (JMLR), 17(1):2096–2030, 2016."
REFERENCES,0.2113144758735441,"Charles C Holt.
Forecasting seasonals and trends by exponentially weighted moving averages.
International journal of forecasting, 20(1):5–10, 2004."
REFERENCES,0.2129783693843594,"Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In Proc. the International Conference on Machine Learning
(ICML), pp. 448–456. PMLR, 2015."
REFERENCES,0.2146422628951747,"Jinhee Kim, Taesung Kim, Jang-Ho Choi, and Jaegul Choo. End-to-end multi-task learning of miss-
ing value imputation and forecasting in time-series data. In 2020 25th International Conference
on Pattern Recognition (ICPR), pp. 8849–8856. IEEE, 2021a."
REFERENCES,0.21630615640599002,"Taesung Kim, Jinhee Kim, Wonho Yang, Hunjoo Lee, and Jaegul Choo. Missing value imputation
of time-series air-quality data via deep neural networks. International journal of environmental
research and public health, 18(22):12213, 2021b."
REFERENCES,0.21797004991680533,"Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long- and short-term
temporal patterns with deep neural networks. CoRR, 2017."
REFERENCES,0.21963394342762063,"Vincent Le Guen and Nicolas Thome. Shape and time distortion loss for training deep time series
forecasting models. In Proc. the Advances in Neural Information Processing Systems (NeurIPS),
volume 4191, 2019."
REFERENCES,0.22129783693843594,"Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot. Domain generalization with adversarial
feature learning. In Proc. of the IEEE conference on computer vision and pattern recognition
(CVPR), 2018."
REFERENCES,0.22296173044925124,Published as a conference paper at ICLR 2022
REFERENCES,0.22462562396006655,"Minhao Liu, Ailing Zeng, Qiuxia Lai, and Qiang Xu. Time series is a special sequence: Forecasting
with sample convolution and interaction. arXiv preprint arXiv:2106.09305, 2021."
REFERENCES,0.22628951747088186,"Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos.
The m4 competition:
100,000 time series and 61 forecasting methods. International Journal of Forecasting, 36(1):
54–74, 2020."
REFERENCES,0.22795341098169716,"Krikamol Muandet, David Balduzzi, and Bernhard Sch¨olkopf. Domain generalization via invari-
ant feature representation. In Proc. the International Conference on Machine Learning (ICML).
PMLR, 2013."
REFERENCES,0.22961730449251247,"Eduardo Ogasawara, Leonardo C Martinez, Daniel De Oliveira, Geraldo Zimbr˜ao, Gisele L Pappa,
and Marta Mattoso.
Adaptive normalization: A novel data normalization approach for non-
stationary time series. In The International Joint Conference on Neural Networks (IJCNN), pp.
1–8. IEEE, 2010."
REFERENCES,0.23128119800332778,"Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-beats: Neural basis
expansion analysis for interpretable time series forecasting. Proc. the International Conference
on Learning Representations (ICLR), 2020."
REFERENCES,0.23294509151414308,"Xingang Pan, Ping Luo, Jianping Shi, and Xiaoou Tang. Two at once: Enhancing learning and
generalization capacities via ibn-net. In Proc. of the European Conference on Computer Vision
(ECCV), 2018."
REFERENCES,0.23460898502495842,"Cheonbok Park, Chunggi Lee, Hyojin Bahng, Yunwon Tae, Seungmin Jin, Kihwan Kim, Sungahn
Ko, and Jaegul Choo. St-grat: A novel spatio-temporal graph attention network for accurately
forecasting dynamically changing road speed. In Proc. the ACM Conference on Information and
Knowledge Management (CIKM), 2020."
REFERENCES,0.23627287853577372,"Nikolaos Passalis, Anastasios Tefas, Juho Kanniainen, Moncef Gabbouj, and Alexandros Iosiﬁdis.
Deep adaptive input normalization for time series forecasting. IEEE transactions on neural net-
works and learning systems, 31(9):3760–3765, 2019."
REFERENCES,0.23793677204658903,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-
performance deep learning library. Proc. the Advances in Neural Information Processing Systems
(NeurIPS), 32, 2019."
REFERENCES,0.23960066555740434,"Slawek Smyl. A hybrid method of exponential smoothing and recurrent neural networks for time
series forecasting. International Journal of Forecasting, 36(1):75–85, 2020."
REFERENCES,0.24126455906821964,"Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain
adaptation. In Proc. of the IEEE conference on computer vision and pattern recognition (CVPR),
2017."
REFERENCES,0.24292845257903495,"Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing in-
gredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016."
REFERENCES,0.24459234608985025,"Jindong Wang, Wenjie Feng, Yiqiang Chen, Han Yu, Meiyu Huang, and Philip S Yu. Visual do-
main adaptation with manifold embedded distribution alignment. In Proc. the ACM international
conference on Multimedia, pp. 402–410, 2018."
REFERENCES,0.24625623960066556,"Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Wenjun Zeng, and Tao Qin. Generalizing
to unseen domains: A survey on domain generalization. arXiv preprint arXiv:2103.03097, 2021."
REFERENCES,0.24792013311148087,"Peter R Winters. Forecasting sales by exponentially weighted moving averages. Management sci-
ence, 6(3):324–342, 1960."
REFERENCES,0.24958402662229617,"Ying Zhang, Baohang Zhou, Xiangrui Cai, Wenya Guo, Xiaoke Ding, and Xiaojie Yuan. Miss-
ing value imputation in multivariate time series with end-to-end generative adversarial networks.
Information Sciences, 551:67–82, 2021."
REFERENCES,0.2512479201331115,"Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.
Informer: Beyond efﬁcient transformer for long sequence time-series forecasting. In Proc. the
AAAI Conference on Artiﬁcial Intelligence (AAAI), 2021."
REFERENCES,0.2529118136439268,Published as a conference paper at ICLR 2022
REFERENCES,0.2545757071547421,"A
APPENDIX"
REFERENCES,0.2562396006655574,"This section provides additional information, visualizations, and experimental results that support
the main manuscript. Section A.1 shows the experimental results on additional real-world datasets
along with a qualitative analysis on one of the datasets to verify the effectiveness of our method
on obvious non-stationary time series. Section A.2 addresses the potential of RevIN on solving the
cross-domain time-series forecasting task. Section A.3 and Section A.4 present the hyperparame-
ter sensitivity analysis and the ablation study on the proposed method, respectively. Section A.5
evaluates our method using complementary metrics, which measure the similarity between two se-
quences. Section A.6 and Section A.7 compare the forecasting results using the proposed method
and existing normalization methods. Section A.8, Section A.9, and Section A.10 provide the algo-
rithm for RevIN, the theoretical justiﬁcation of RevIN, and the calculation details of the feature
divergence used in Section 4.2.3, respectively. Section A.11 and Section A.12 describe additional
implementation details and the reproduction details of the baselines where RevIN is applied, respec-
tively. Section A.13 illustrates additional quantitative results for RevIN and the baselines. Lastly,
Section A.14 provides complete quantitative results, including the standard deviation values for ﬁve
experiments, which can not be included in the main manuscript due to the lack of space."
REFERENCES,0.2579034941763727,"A.1
EXPERIMENTAL RESULTS ON ADDITIONAL REAL-WORLD TIME-SERIES DATASETS"
REFERENCES,0.259567387687188,"Table 4: Forecasting performance on the air quality, Nasdaq, and M4 competition datasets.
The results on the M4 dataset (*) are multiplied by ten for readability. The average value and
the standard deviation value for ﬁve runs are reported."
REFERENCES,0.2612312811980033,"Methods
Informer
+ RevIN
N-BEATS
+ RevIN
SCINet
+ RevIN"
REFERENCES,0.2628951747088186,"Metric
MSE
MAE
MSE
MAE
MSE
MAE
MSE
MAE
MSE
MAE
MSE
MAE"
REFERENCES,0.26455906821963393,Air quality
REFERENCES,0.26622296173044924,"24
0.802
± 0.178
0.671
± 0.084
0.585
± 0.033
0.539
± 0.023"
REFERENCES,0.26788685524126454,"0.698
± 0.064
0.626
± 0.029
0.527
± 0.005
0.498
± 0.003"
REFERENCES,0.26955074875207985,"0.512
± 0.029
0.514
± 0.019
0.490
± 0.006
0.474
± 0.004"
REFERENCES,0.27121464226289516,"48
0.966
± 0.054
0.761
± 0.023
0.859
± 0.086
0.668
± 0.041"
REFERENCES,0.27287853577371046,"0.955
± 0.106
0.740
± 0.035
0.705
± 0.019
0.600
± 0.009"
REFERENCES,0.27454242928452577,"0.712
± 0.091
0.627
± 0.047
0.659
± 0.013
0.566
± 0.007"
REFERENCES,0.2762063227953411,"168
1.328
± 0.107
0.923
± 0.040
1.036
± 0.056
0.761
± 0.020"
REFERENCES,0.2778702163061564,"1.079
± 0.108
0.818
± 0.046
0.789
± 0.008
0.660
± 0.005"
REFERENCES,0.2795341098169717,"0.957
± 0.067
0.737
± 0.031
0.794
± 0.025
0.645
± 0.014"
REFERENCES,0.281198003327787,"336
1.278
± 0.074
0.901
± 0.032
1.145
± 0.032
0.801
± 0.010"
REFERENCES,0.28286189683860236,"1.105
± 0.052
0.835
± 0.021
0.860
± 0.017
0.685
± 0.006"
REFERENCES,0.28452579034941766,"0.989
± 0.111
0.760
± 0.046
0.854
± 0.029
0.676
± 0.010"
REFERENCES,0.28618968386023297,"720
2.028
± 0.216
1.104
± 0.061
1.161
± 0.028
0.810
± 0.009"
REFERENCES,0.2878535773710483,"1.538
± 0.419
0.968
± 0.110
0.842
± 0.015
0.686
± 0.008"
REFERENCES,0.2895174708818636,"1.228
± 0.048
0.858
± 0.021
0.839
± 0.024
0.680
± 0.013"
REFERENCES,0.2911813643926789,Nasdaq
REFERENCES,0.2928452579034942,"30
5.318
± 0.052
1.093
± 0.017
1.273
± 0.078
0.630
± 0.009"
REFERENCES,0.2945091514143095,"5.500
± 0.647
1.254
± 0.086
1.023
± 0.034
0.577
± 0.007"
REFERENCES,0.2961730449251248,"1.742
± 0.111
0.739
± 0.028
0.985
± 0.018
0.564
± 0.005"
REFERENCES,0.2978369384359401,"60
5.525
± 0.022
1.098
± 0.016
1.573
± 0.098
0.666
± 0.011"
REFERENCES,0.2995008319467554,"5.226
± 0.424
1.236
± 0.032
1.207
± 0.044
0.617
± 0.009"
REFERENCES,0.3011647254575707,"2.304
± 0.062
0.790
± 0.010
1.161
± 0.021
0.601
± 0.003"
REFERENCES,0.30282861896838603,"120
5.793
± 0.140
1.090
± 0.012
2.648
± 0.186
0.762
± 0.016"
REFERENCES,0.30449251247920134,"6.023
± 0.382
1.197
± 0.034
1.959
± 0.062
0.714
± 0.006"
REFERENCES,0.30615640599001664,"3.227
± 0.236
0.853
± 0.007
1.869
± 0.037
0.697
± 0.003 M4∗"
REFERENCES,0.30782029950083195,"average
0.099
± 0.002
0.258
± 0.020
0.008
± 0.005
0.074
± 0.005"
REFERENCES,0.30948419301164726,"2.241
± 0.037
2.065
± 0.029
2.082
± 0.014
1.974
± 0.006"
REFERENCES,0.31114808652246256,"2.180
± 1.943
1.943
± 0.011
2.079
± 0.011
1.892
± 0.004"
REFERENCES,0.31281198003327787,"We evaluate the proposed method on the four large-scale real-world time-series datasets, the ETTh1,
ETTh2, ETTm1, and ECL datasets in the main manuscript. Additionally, this section provides ex-
perimental results on three more datasets, including two real-world datasets taken from the UCI
repository, the air quality dataset and the Nasdaq dataset, and the M4 competition dataset (Makri-
dakis et al., 2020). In total, our proposed method is evaluated on seven datasets in this paper."
REFERENCES,0.3144758735440932,"Air quality3 dataset contains hourly averaged responses collected from ﬁve metal oxide chemical
sensors located in Italy. The data consist of 13 variables of length 9537. We set the prediction length
as {24, 48, 168, 336, 720} and the corresponding input length as {48, 96, 168, 168, 360} so that
their ratios become {2x, 2x, 1x, 0.5x, 0.5x}."
REFERENCES,0.3161397670549085,"Nasdaq4 dataset consists of 82 variables, including important indices of markets around the world,
the price of major companies in the U.S. market, treasury bill rates, etc. It is measured daily, having
a total of 1984 data samples for each variable. We prolong the prediction length as {30, 60, 120}
and set the corresponding input length as 60 for all."
REFERENCES,0.3178036605657238,"3https://archive.ics.uci.edu/ml/datasets/Air+Quality
4https://archive.ics.uci.edu/ml/datasets/CNNpred%3A+CNN-based+stock+market+prediction+using+
a+diverse+set+of+variables"
REFERENCES,0.3194675540765391,Published as a conference paper at ICLR 2022
REFERENCES,0.3211314475873544,"Figure 6: Prediction results on the Nasdaq dataset. The results on three variables in the data,
Close, DTB6, and DE1, are shown. The prediction length is 60 days, and the seventh value is illus-
trated to show the results on the entire test set. We compare RevIN with N-BEATS."
REFERENCES,0.3227953410981697,"M45 competition dataset consists of six different hourly, daily, weekly, monthly, quarterly, and
yearly sets, containing 100,000 test data. We follow the original experimental protocol of the M4
competition (Makridakis et al., 2020). For evaluation, we measure the micro-averaged mean ab-
solute error and mean squared error: we ﬁrst compute the metric independently for each set, i.e.,
hourly, daily, weekly, monthly, quarterly, and yearly sets, and then calculate the weighted average of
the metrics using the contributions of each set as the weights."
REFERENCES,0.324459234608985,"As shown in Table 4, RevIN signiﬁcantly improves the forecasting performance of the baselines on
all three datasets. Notably, RevIN shows outstanding performance on the Nasdaq dataset, reducing
the prediction errors by more than half compared to the baselines."
REFERENCES,0.3261231281198003,"Additionally, we conduct a qualitative analysis on the Nasdaq dataset to verify the effectiveness
of our method on obvious non-stationary time series. As shown in Fig. 6, the Nasdaq index (the
variable ’Close’) has steadily increased since 2010. Accordingly, when data are divided into the
training and test data based on a speciﬁc point in time (vertical dashed line in Fig. 6), the test data
values tend to be higher than the training data values. In other words, the data severely suffers
from the distribution shift problem, where the training and test data show a discrepancy in their
distribution. As a result, even existing state-of-the-art models often cannot predict the future values
appropriately, as shown in Fig. 6. The baseline fails to keep up with the trend in data, whose mean
value continues to increase, and thus the prediction results are shifted. However, RevIN mitigates
this distribution discrepancy and remarkably increases the prediction performance of the baseline."
REFERENCES,0.3277870216306156,5https://mofc.unic.ac.cy/m4/
REFERENCES,0.32945091514143093,Published as a conference paper at ICLR 2022
REFERENCES,0.33111480865224624,"Similarly, RevIN shows superior performance on the other rapidly increasing data (’DTB6’ in Fig. 6)
and the decreasing test data (‘DE1’ in Fig. 6), accurately predicting the changing mean of the data."
REFERENCES,0.33277870216306155,"A.2
CROSS-DOMAIN TIME-SERIES FORECASTING"
REFERENCES,0.33444259567387685,"Table 5: Cross-domain time-series forecasting results. We conduct a cross-domain evaluation on
the ETT datasets, ETTh1, ETTh2, and ETTm1. We train RevIN using SCINet as the baseline. We
report the average errors and the standard deviation values for ﬁve runs."
REFERENCES,0.33610648918469216,"Train
ETTh1
ETTh2
ETTm1
Test
ETTh2
ETTm1
ETTh1
ETTm1
ETTh1
ETTh2
Prediction length
336
960
336
960
336
960
336
960
288
1344
288
1344"
REFERENCES,0.33777038269550747,SCINet
REFERENCES,0.33943427620632277,"MSE
0.471
± 0.034
0.741
± 0.056
0.471
± 0.043
0.765
± 0.029"
REFERENCES,0.3410981697171381,"0.614
± 0.020
0.671
± 0.061
0.608
± 0.049
1.668
± 0.110"
REFERENCES,0.3427620632279534,"0.659
± 0.032
0.654
± 0.026
0.518
± 0.009
1.813
± 0.166"
REFERENCES,0.34442595673876875,"MAE
0.450
± 0.024
0.640
± 0.027
0.427
± 0.032
0.636
± 0.013"
REFERENCES,0.34608985024958405,"0.507
± 0.014
0.607
± 0.035
0.487
± 0.031
1.004
± 0.046"
REFERENCES,0.34775374376039936,"0.562
± 0.015
0.608
± 0.016
0.509
± 0.004
1.027
± 0.078"
REFERENCES,0.34941763727121466,+ RevIN
REFERENCES,0.35108153078202997,"MSE
0.350
± 0.010
0.419
± 0.007
0.346
± 0.008
0.452
± 0.004"
REFERENCES,0.3527454242928453,"0.501
± 0.006
0.586
± 0.015
0.321
± 0.009
0.441
± 0.001"
REFERENCES,0.3544093178036606,"0.478
± 0.003
0.542
± 0.013
0.387
± 0.005
0.506
± 0.010"
REFERENCES,0.3560732113144759,"MAE
0.383
± 0.006
0.451
± 0.004
0.331
± 0.005
0.449
± 0.001"
REFERENCES,0.3577371048252912,"0.449
± 0.004
0.542
± 0.007
0.331
± 0.004
0.445
± 0.001"
REFERENCES,0.3594009983361065,"0.477
± 0.002
0.531
± 0.006
0.417
± 0.002
0.503
± 0.006"
REFERENCES,0.3610648918469218,"As RevIN can alleviate the distribution discrepancy between the training and test data, we investigate
the ability of RevIN in mitigating distribution discrepancy between different domains through cross-
domain time-series forecasting task. In time series, a domain can be a location of a sensor where
the data is collected. We use the ETT datasets for the experiment since they have the same feature
categories. However, their distributions can exhibit signiﬁcant discrepancy because the ETTh1 and
ETTh2 datasets are collected from different locations, and the ETTh and ETTm1 datasets have dif-
ferent measurement time intervals. Thus, we alternately use each ETT dataset as a source domain
for training and a target domain for testing. The goal of cross-domain time-series forecasting is to
alleviate the data distribution discrepancy between the source and target domains, e.g., between the
ETTh1 and ETTh2 datasets."
REFERENCES,0.3627287853577371,"In Table 5, despite the difference in the data distributions, RevIN shows remarkable performance in
cross-domain time-series forecasting. In particular, RevIN outperforms SCINet by a large margin
when the model needs to reduce the discrepancy between the ETTh2 and ETTm1 datasets. The
results demonstrate that RevIN successfully solves the distribution shift problem by alleviating data
distribution discrepancy between different domains, leading to better generalization performance."
REFERENCES,0.3643926788685524,"A.3
HYPERPARAMETER SENSITIVITY ANALYSIS"
REFERENCES,0.36605657237936773,"We analyze the hyperparameter sensitivity of the proposed method compared with the baseline mod-
els. Input sequence length can be a crucial hyperparameter to RevIN since the method computes the
mean and the standard deviation across the entire input sequence and then uses the statistics at its"
REFERENCES,0.36772046589018303,"0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0"
REFERENCES,0.36938435940099834,"48
168
336
480
720
960"
REFERENCES,0.37104825291181365,Input sequence length
REFERENCES,0.37271214642262895,"N-BEATS MSE
N-BEATS MAE
+RevIN MSE
+RevIN MAE 0.5 0.7 0.9 1.1 1.3 1.5 1.7 1.9"
REFERENCES,0.37437603993344426,"48
168
336
480
720
960"
REFERENCES,0.37603993344425957,Input sequence length
REFERENCES,0.3777038269550749,"Informer MSE
Informer MAE
+RevIN MSE
+RevIN MAE
±. "
REFERENCES,0.3793677204658902,"0.5
0.6
0.7
0.8
0.9
1.0
1.1
1.2
1.3"
REFERENCES,0.3810316139767055,"48
168
336
480
720
960"
REFERENCES,0.3826955074875208,Input sequence length
REFERENCES,0.3843594009983361,"SCINet MSE
SCINet MAE
+RevIN MSE
+RevIN MAE"
REFERENCES,0.3860232945091514,"Figure 7: Impacts of the input sequence length on RevIN compared with the baselines. We
prolong the input length from 48 (two days) to 960 (40 days) when the prediction length is set as
960 (40 days) on the ETTh1 dataset. The average errors for ﬁve runs, with standard deviation values,
are reported. In N-BEATS, the standard deviation value for the mean squared error is too large to
visualize when the prediction length is 960; we write the value as “±4.955” instead."
REFERENCES,0.3876871880199667,Published as a conference paper at ICLR 2022
REFERENCES,0.389351081530782,"normalization and denormalization steps. Thus, the input sequence length would play a crucial role
in the prediction accuracy and stability of the training process. Accordingly, we prolong the model
input sequence length to see its impact on the forecasting performance of RevIN, as shown in Fig. 7.
As a result, RevIN consistently outperforms the baseline for various input sequence lengths. More
importantly, RevIN makes the baseline models more robust to the input length. In other words,
RevIN shows stable performance for various input lengths in contrast to the baseline models, which
shows the high variance in their performance according to the input length. Notably, in N-BEATS,
the average error, as well as the standard deviation of the error, signiﬁcantly increase as prolonging
the input length. This is because the trend in data is expressed as a linear function in N-BEATS.
As the input length becomes prolonged, there is a chance that the variance (or non-stationarity) in
the time-series values will become higher, decreasing the accuracy of the linear trend to ﬁt the data
unless the data is monotonically increasing or decreasing. Also, the mispredicted trends linearly
increase the error in future values. However, when N-BEATS adopts the RevIN layer, it removes
the non-stationary statistics from the input, and thus, the model shows robust performance against
the input length. Removing the variability, i.e., normalizing its mean and standard deviation, before
feeding it to the model and returning it to the output makes the model learning stable."
REFERENCES,0.3910149750415973,"A.4
ABLATION STUDY"
REFERENCES,0.39267886855241263,"Table 6: Ablation study results. We ablate the afﬁne transformation (afﬁne.) from RevIN and evalu-
ate forecasting performance on the six datasets. N-BEATS is used as the baseline for all experiments.
We report the average and the standard deviation values for the ﬁve runs."
REFERENCES,0.39434276206322794,"Method
+ RevIN w/o afﬁne.
+ RevIN w/ afﬁne. (ours)"
REFERENCES,0.39600665557404324,"Metric
MSE
MAE
MSE
MAE"
REFERENCES,0.39767054908485855,"ETTh1
48
0.370 ± 0.006
0.393 ± 0.003
0.363 ± 0.005
0.389 ± 0.003
960
0.675 ± 0.038
0.576 ± 0.014
0.638 ± 0.035
0.559 ± 0.017"
REFERENCES,0.39933444259567386,"ETTh2
48
0.257 ± 0.003
0.322 ± 0.002
0.255 ± 0.008
0.321 ± 0.005
960
0.483 ± 0.012
0.487 ± 0.007
0.471 ± 0.015
0.481 ± 0.008"
REFERENCES,0.40099833610648916,"ETTm1
96
0.384 ± 0.013
0.408 ± 0.009
0.378 ± 0.011
0.406 ± 0.007
1344
0.664 ± 0.085
0.567 ± 0.039
0.631 ± 0.061
0.556 ± 0.020"
REFERENCES,0.40266222961730447,"ECL
48
0.197 ± 0.002
0.302 ± 0.002
0.195 ± 0.002
0.301 ± 0.001
960
0.347 ± 0.034
0.415 ± 0.026
0.325 ± 0.019
0.398 ± 0.015"
REFERENCES,0.4043261231281198,"Air
quality"
REFERENCES,0.4059900166389351,"48
0.707 ± 0.009
0.601 ± 0.002
0.705 ± 0.019
0.600 ± 0.009
720
0.852 ± 0.025
0.689 ± 0.013
0.842 ± 0.015
0.686 ± 0.008"
REFERENCES,0.40765391014975044,"Nasdaq
30
0.983 ± 0.017
0.565 ± 0.005
0.981 ± 0.017
0.564 ± 0.005
60
1.163 ± 0.010
0.610 ± 0.002
1.155 ± 0.020
0.608 ± 0.005"
REFERENCES,0.40931780366056575,"We ablate the afﬁne transformation from RevIN to analyze its impact on forecasting performance.
We conduct the analysis on the ETTh1, ETTh2, ETTm1, ECL, Nasdaq, and air quality datasets using
N-BEATS as the baseline. The results in Table 6 show that the afﬁne transformation consistently
contributes to performance improvement on a variety of datasets. As mentioned earlier, a model
can add RevIN in an intermediate layer, even to several layers. While existing approaches are a
preprocessing-and-postprocessing method applied outside of the main prediction model, RevIN is
an end-to-end trainable layer that can be added to any layer in the model as batch normalization (Ioffe
& Szegedy, 2015) and instance normalization (Ulyanov et al., 2016), which are recently proposed
deep learning-based normalization layers. Thus, we verify that adopting RevIN in the intermediate
layers instead of the input and output layers can improve the forecasting performance as well. We
add RevIN to the ﬁrst stack of N-BEATS and SCINet and evaluate their performance on the six
datasets. The results in Table 7 demonstrate that even when added to the intermediate layers, RevIN
improves the performance of the baselines, as a learnable normalization layer. We mainly focus on
adding RevIN to the input and output of a model since it shows robust performance on average.
Nevertheless, the model adopting RevIN in the intermediate layers consistently outperforms the
baseline without RevIN, frequently achieving the best performance among all. This performance
is even better than the dynamic normalization methods, LSTNet∗and ES-RNN∗, when they are"
REFERENCES,0.41098169717138106,Published as a conference paper at ICLR 2022
REFERENCES,0.41264559068219636,"adopted to N-BEATS as well (See Table 9 in Appendix A.6). For example, when the prediction
length is 960, the mean squared errors of LSTNet∗, ES-RNN∗, RevIN (inter.), and RevIN (i/o) are
5.627, 1.338, 0.523, and 0.471, on average. In conclusion, RevIN is a ﬂexible, end-to-end trainable"
REFERENCES,0.41430948419301167,"Table 7: Effectiveness of RevIN when added to the intermediate layers in the model. We add
RevIN to the ﬁrst stack of N-BEATS and SCINet and evaluate their performance on the six datasets.
We report the average value and standard deviation of ﬁve experiments. RevIN (inter.) indicates
the model where RevIN is added to the intermediate layers of the baseline network. RevIN (i/o)
indicates the model where RevIN is added to the input and output layer of the baseline network."
REFERENCES,0.415973377703827,"Method
N-BEATS
+ RevIN (inter.)
+ RevIN (i/o)
SCINet
+ RevIN (inter.)
+ RevIN (i/o)"
REFERENCES,0.4176372712146423,"Metric
MSE
MAE
MSE
MAE
MSE
MAE
MSE
MAE
MSE
MAE
MSE
MAE ETTh1"
REFERENCES,0.4193011647254576,"24
0.478
± 0.022
0.505
± 0.012
0.347
± 0.006
0.389
± 0.004
0.330
± 0.006
0.373
± 0.004"
REFERENCES,0.4209650582362729,"0.338
± 0.012
0.373
± 0.009
0.306
± 0.004
0.347
± 0.004
0.308
± 0.003
0.347
± 0.002"
REFERENCES,0.4226289517470882,"48
0.536
± 0.060
0.542
± 0.041
0.375
± 0.008
0.407
± 0.005
0.372
± 0.001
0.400
± 0.002"
REFERENCES,0.4242928452579035,"0.436
± 0.025
0.459
± 0.021
0.363
± 0.004
0.394
± 0.004
0.365
± 0.005
0.389
± 0.003"
REFERENCES,0.4259567387687188,"168
1.005
± 0.146
0.782
± 0.064
0.495
± 0.086
0.481
± 0.062
0.466
± 0.030
0.452
± 0.014"
REFERENCES,0.4276206322795341,"0.459
± 0.015
0.461
± 0.013
0.415
± 0.001
0.424
± 0.002
0.406
± 0.003
0.416
± 0.003"
REFERENCES,0.4292845257903494,"336
0.932
± 0.079
0.743
± 0.042
0.538
± 0.043
0.508
± 0.027
0.515
± 0.013
0.483
± 0.008"
REFERENCES,0.43094841930116473,"0.527
± 0.010
0.513
± 0.006
0.552
± 0.002
0.516
± 0.001
0.467
± 0.005
0.471
± 0.003"
REFERENCES,0.43261231281198004,"720
1.389
± 0.230
0.926
± 0.066
0.608
± 0.016
0.572
± 0.009
0.576
± 0.035
0.534
± 0.018"
REFERENCES,0.43427620632279534,"0.596
± 0.015
0.571
± 0.013
0.560
± 0.007
0.550
± 0.005
0.507
± 0.006
0.505
± 0.004"
REFERENCES,0.43594009983361065,"960
1.383
± 0.380
0.932
± 0.120
0.664
± 0.033
0.604
± 0.015
0.678
± 0.019
0.575
± 0.009"
REFERENCES,0.43760399334442596,"0.604
± 0.017
0.574
± 0.014
0.619
± 0.005
0.582
± 0.002
0.545
± 0.010
0.526
± 0.005 ETTh2"
REFERENCES,0.43926788685524126,"24
0.403
± 0.185
0.472
± 0.101
0.199
± 0.002
0.291
± 0.002
0.192
± 0.003
0.276
± 0.002"
REFERENCES,0.44093178036605657,"0.199
± 0.026
0.295
± 0.027
0.186
± 0.003
0.272
± 0.001
0.180
± 0.004
0.263
± 0.002"
REFERENCES,0.4425956738768719,"48
1.330
± 0.240
0.918
± 0.073
0.263
± 0.007
0.335
± 0.005
0.254
± 0.011
0.320
± 0.008"
REFERENCES,0.4442595673876872,"0.350
± 0.025
0.422
± 0.027
0.313
± 0.045
0.373
± 0.032
0.231
± 0.006
0.302
± 0.006"
REFERENCES,0.4459234608985025,"168
7.174
± 0.449
2.329
± 0.049
0.425
± 0.015
0.434
± 0.010
0.410
± 0.010
0.418
± 0.005"
REFERENCES,0.4475873544093178,"0.559
± 0.044
0.518
± 0.025
0.338
± 0.003
0.380
± 0.001
0.337
± 0.007
0.378
± 0.003"
REFERENCES,0.4492512479201331,"336
4.859
± 0.268
1.863
± 0.043
0.446
± 0.007
0.456
± 0.005
0.449
± 0.011
0.447
± 0.006"
REFERENCES,0.4509151414309484,"0.664
± 0.073
0.583
± 0.030
0.422
± 0.001
0.443
± 0.001
0.357
± 0.003
0.403
± 0.002"
REFERENCES,0.4525790349417637,"720
5.656
± 1.053
2.012
± 0.186
0.505
± 0.022
0.501
± 0.013
0.496
± 0.008
0.482
± 0.002"
REFERENCES,0.454242928452579,"1.546
± 0.378
0.944
± 0.141
0.634
± 0.010
0.564
± 0.005
0.411
± 0.003
0.445
± 0.002"
REFERENCES,0.4559068219633943,"960
6.408
± 2.039
2.077
± 0.242
0.523
± 0.040
0.522
± 0.025
0.471
± 0.015
0.481
± 0.008"
REFERENCES,0.45757071547420963,"1.862
± 0.153
1.066
± 0.055
0.734
± 0.014
0.603
± 0.005
0.438
± 0.007
0.462
± 0.004 ETTm1"
REFERENCES,0.45923460898502494,"24
0.443
± 0.043
0.437
± 0.035
0.387
± 0.018
0.391
± 0.012
0.403
± 0.006
0.392
± 0.005"
REFERENCES,0.46089850249584025,"0.130
± 0.003
0.231
± 0.003
0.108
± 0.002
0.203
± 0.004
0.106
± 0.002
0.196
± 0.001"
REFERENCES,0.46256239600665555,"48
0.453
± 0.034
0.472
± 0.018
0.341
± 0.008
0.388
± 0.007
0.328
± 0.010
0.371
± 0.007"
REFERENCES,0.46422628951747086,"0.155
± 0.004
0.262
± 0.004
0.142
± 0.011
0.241
± 0.013
0.135
± 0.003
0.222
± 0.002"
REFERENCES,0.46589018302828616,"96
0.603
± 0.051
0.581
± 0.027
0.401
± 0.007
0.428
± 0.004
0.379
± 0.011
0.406
± 0.007"
REFERENCES,0.46755407653910147,"0.195
± 0.012
0.291
± 0.013
0.192
± 0.016
0.285
± 0.017
0.162
± 0.001
0.247
± 0.001"
REFERENCES,0.46921797004991683,"288
0.849
± 0.095
0.702
± 0.051
0.502
± 0.032
0.483
± 0.018
0.451
± 0.016
0.445
± 0.008"
REFERENCES,0.47088186356073214,"0.361
± 0.008
0.419
± 0.004
0.264
± 0.002
0.323
± 0.001
0.265
± 0.003
0.321
± 0.002"
REFERENCES,0.47254575707154745,"672
0.860
± 0.057
0.726
± 0.026
0.553
± 0.020
0.512
± 0.009
0.555
± 0.011
0.511
± 0.008"
REFERENCES,0.47420965058236275,"1.020
± 0.040
0.756
± 0.025
0.663
± 0.081
0.583
± 0.033
0.357
± 0.004
0.380
± 0.002"
REFERENCES,0.47587354409317806,"1344
14.613
± 26.108
1.948
± 1.655
0.722
± 0.064
0.594
± 0.027
0.631
± 0.061
0.556
± 0.020"
REFERENCES,0.47753743760399336,"1.841
± 0.242
1.044
± 0.100
0.989
± 0.211
0.717
± 0.081
0.412
± 0.008
0.422
± 0.003 ECL"
REFERENCES,0.47920133111480867,"24
0.279
± 0.007
0.372
± 0.003
0.182
± 0.001
0.300
± 0.001
0.176
± 0.002
0.285
± 0.001"
REFERENCES,0.480865224625624,"0.138
± 0.004
0.246
± 0.005
0.111
± 0.000
0.207
± 0.001
0.112
± 0.001
0.207
± 0.001"
REFERENCES,0.4825291181364393,"48
0.309
± 0.007
0.388
± 0.004
0.207
± 0.003
0.318
± 0.002
0.194
± 0.001
0.301
± 0.001"
REFERENCES,0.4841930116472546,"0.163
± 0.007
0.265
± 0.007
0.124
± 0.001
0.221
± 0.001
0.126
± 0.001
0.222
± 0.001"
REFERENCES,0.4858569051580699,"168
0.333
± 0.016
0.410
± 0.012
0.237
± 0.007
0.340
± 0.004
0.218
± 0.002
0.320
± 0.001"
REFERENCES,0.4875207986688852,"0.177
± 0.003
0.281
± 0.005
0.154
± 0.002
0.248
± 0.001
0.153
± 0.003
0.249
± 0.002"
REFERENCES,0.4891846921797005,"336
0.326
± 0.004
0.406
± 0.001
0.245
± 0.011
0.348
± 0.007
0.241
± 0.005
0.337
± 0.002"
REFERENCES,0.4908485856905158,"0.202
± 0.004
0.308
± 0.004
0.161
± 0.002
0.261
± 0.002
0.162
± 0.001
0.262
± 0.001"
REFERENCES,0.4925124792013311,"720
0.420
± 0.094
0.467
± 0.058
0.308
± 0.019
0.393
± 0.016
0.303
± 0.012
0.383
± 0.011"
REFERENCES,0.49417637271214643,"0.234
± 0.006
0.333
± 0.004
0.184
± 0.003
0.283
± 0.003
0.183
± 0.003
0.281
± 0.002"
REFERENCES,0.49584026622296173,"960
0.399
± 0.022
0.455
± 0.017
0.335
± 0.018
0.413
± 0.015
0.325
± 0.019
0.398
± 0.015"
REFERENCES,0.49750415973377704,"0.235
± 0.011
0.330
± 0.008
0.196
± 0.005
0.295
± 0.005
0.200
± 0.003
0.292
± 0.002"
REFERENCES,0.49916805324459235,Air quality
REFERENCES,0.5008319467554077,"24
0.698
± 0.064
0.626
± 0.029
0.558
± 0.011
0.537
± 0.008
0.527
± 0.005
0.498
± 0.003"
REFERENCES,0.502495840266223,"0.512
± 0.029
0.514
± 0.019
0.488
± 0.006
0.486
± 0.009
0.490
± 0.006
0.474
± 0.004"
REFERENCES,0.5041597337770383,"48
0.955
± 0.106
0.740
± 0.035
0.722
± 0.013
0.629
± 0.005
0.705
± 0.019
0.600
± 0.009"
REFERENCES,0.5058236272878536,"0.712
± 0.091
0.627
± 0.047
0.651
± 0.032
0.578
± 0.023
0.659
± 0.013
0.566
± 0.007"
REFERENCES,0.5074875207986689,"168
1.079
± 0.108
0.818
± 0.046
0.819
± 0.007
0.691
± 0.004
0.789
± 0.008
0.660
± 0.005"
REFERENCES,0.5091514143094842,"0.957
± 0.067
0.737
± 0.031
0.787
± 0.020
0.648
± 0.012
0.794
± 0.025
0.645
± 0.014"
REFERENCES,0.5108153078202995,"336
1.105
± 0.052
0.835
± 0.021
0.902
± 0.018
0.721
± 0.010
0.860
± 0.017
0.685
± 0.006"
REFERENCES,0.5124792013311148,"0.989
± 0.111
0.760
± 0.046
0.870
± 0.022
0.695
± 0.011
0.854
± 0.029
0.676
± 0.010"
REFERENCES,0.5141430948419301,"720
1.538
± 0.419
0.968
± 0.110
0.945
± 0.030
0.757
± 0.012
0.842
± 0.015
0.686
± 0.008"
REFERENCES,0.5158069883527454,"1.228
± 0.048
0.858
± 0.021
0.939
± 0.064
0.730
± 0.025
0.839
± 0.024
0.680
± 0.013"
REFERENCES,0.5174708818635607,Nasdaq
REFERENCES,0.519134775374376,"30
5.500
± 0.647
1.254
± 0.086
0.940
± 0.055
0.581
± 0.037
1.023
± 0.034
0.577
± 0.007"
REFERENCES,0.5207986688851913,"1.742
± 0.111
0.739
± 0.028
1.111
± 0.095
0.599
± 0.020
0.985
± 0.018
0.564
± 0.005"
REFERENCES,0.5224625623960066,"60
5.226
± 0.424
1.236
± 0.032
0.989
± 0.025
0.578
± 0.008
1.207
± 0.044
0.617
± 0.009"
REFERENCES,0.5241264559068219,"2.304
± 0.062
0.790
± 0.010
1.280
± 0.023
0.630
± 0.004
1.161
± 0.021
0.601
± 0.003"
REFERENCES,0.5257903494176372,"120
6.023
± 0.382
1.197
± 0.034
1.166
± 0.014
0.615
± 0.003
1.959
± 0.062
0.714
± 0.006"
REFERENCES,0.5274542429284526,"3.227
± 0.236
0.853
± 0.007
2.585
± 0.374
0.776
± 0.031
1.869
± 0.037
0.697
± 0.003"
REFERENCES,0.5291181364392679,Published as a conference paper at ICLR 2022
REFERENCES,0.5307820299500832,"layer that can signiﬁcantly increase the performance of a model in time series forecasting, applied
to any arbitrarily chosen layers."
REFERENCES,0.5324459234608985,"A.5
PERFORMANCE EVALUATION ON SIMILARITY METRICS FOR TIME SERIES"
REFERENCES,0.5341098169717138,"We evaluate the forecasting performance of RevIN mainly on the mean squared error and the mean
absolute error. Additionally, we use complementary metrics that measure the similarity between two
sequences, dynamic time warping (DTW) and temporal distortion index (TDI) (Le Guen & Thome,
2019; Fr´ıas-Paredes et al., 2017). Table 8 shows that our approach signiﬁcantly improves the base-
line models across all datasets in terms of the DTW and TDI. Notably, RevIN exhibits outstanding
performance by a large margin compared to the baselines for long prediction length. For example,
when RevIN is added, the average DTW decreases from 38.348 to 15.240 for Informer, from 53.148
to 12.766 for N-BEATS, and from 20.498 to 11.080 for SCINet when the prediction length is 960 on
ETTh2. There are a few cases where the proposed method predicts a less similar sequence than the
baseline. However, the margin is minimal in terms of either DTW or TDI compared to the signiﬁcant
margin found when our method outperforms the baseline. These results demonstrate that adopting
RevIN can generate a sequence more similar to the groundtruth than the baseline, especially showing
better prediction accuracy on the longer sequences."
REFERENCES,0.5357737104825291,"Table 8: Comparison results on similarity metrics for time series. We assess the model forecast-
ing results in terms of the shape and temporal errors using the DTW and the TDI, respectively (the
lower, the better). The experiments are conducted for the four datasets, using the three baselines. We
report the average value and standard deviation of ﬁve experiments."
REFERENCES,0.5374376039933444,"Method
Informer
+ RevIN
N-BEATS
+ RevIN
SCINet
+ RevIN"
REFERENCES,0.5391014975041597,"Metric
TDI
DTW
TDI
DTW
TDI
DTW
TDI
DTW
TDI
DTW
TDI
DTW ETTh1"
REFERENCES,0.540765391014975,"24
1.602
± 0.093
2.515
± 0.121
1.207
± 0.037
2.206
± 0.097"
REFERENCES,0.5424292845257903,"1.309
± 0.147
2.361
± 0.048
1.031
± 0.021
1.830
± 0.012"
REFERENCES,0.5440931780366056,"1.136
± 0.113
1.784
± 0.037
0.969
± 0.017
1.672
± 0.007"
REFERENCES,0.5457570715474209,"48
3.932
± 0.796
4.178
± 0.333
2.762
± 0.190
3.401
± 0.063"
REFERENCES,0.5474209650582362,"2.430
± 0.386
3.564
± 0.230
1.592
± 0.055
2.745
± 0.012"
REFERENCES,0.5490848585690515,"2.418
± 0.297
2.873
± 0.117
1.456
± 0.045
2.508
± 0.012"
REFERENCES,0.5507487520798668,"168
31.569
± 4.652
10.384
± 0.420
9.459
± 1.173
6.532
± 0.227"
REFERENCES,0.5524126455906821,"19.369
± 3.679
8.937
± 0.490
6.190
± 1.033
5.756
± 0.161"
REFERENCES,0.5540765391014975,"4.913
± 1.085
5.344
± 0.170
3.791
± 0.075
5.017
± 0.008"
REFERENCES,0.5557404326123128,"336
72.959
± 6.769
14.850
± 0.514
45.215
± 17.098
11.605
± 1.321"
REFERENCES,0.5574043261231281,"47.961
± 8.500
11.782
± 0.531
14.302
± 2.127
8.532
± 0.162"
REFERENCES,0.5590682196339434,"6.120
± 0.329
7.723
± 0.087
6.973
± 0.292
7.319
± 0.025"
REFERENCES,0.5607321131447587,"720
167.254
± 13.008
23.295
± 0.244
98.648
± 18.571
18.483
± 0.346"
REFERENCES,0.562396006655574,"143.832
± 28.984
20.494
± 0.952
26.265
± 8.439
12.383
± 0.458"
REFERENCES,0.5640599001663894,"20.351
± 3.588
11.550
± 0.246
11.337
± 0.296
10.511
± 0.056"
REFERENCES,0.5657237936772047,"960
182.008
± 16.076
28.848
± 1.217
128.152
± 9.174
22.018
± 0.204"
REFERENCES,0.56738768718802,"148.671
± 48.669
23.810
± 2.418
40.774
± 13.985
15.107
± 0.744"
REFERENCES,0.5690515806988353,"24.067
± 3.383
13.551
± 0.270
14.148
± 0.461
12.508
± 0.063 ETTh2"
REFERENCES,0.5707154742096506,"24
2.016
± 0.152
2.481
± 0.353
1.528
± 0.142
1.601
± 0.029"
REFERENCES,0.5723793677204659,"1.476
± 0.230
2.307
± 0.488
1.128
± 0.084
1.409
± 0.007"
REFERENCES,0.5740432612312812,"1.210
± 0.128
1.350
± 0.095
1.088
± 0.022
1.250
± 0.015"
REFERENCES,0.5757071547420965,"48
4.462
± 0.567
8.194
± 0.397
4.802
± 0.342
2.806
± 0.081"
REFERENCES,0.5773710482529119,"4.244
± 0.470
6.131
± 0.542
2.529
± 0.165
2.207
± 0.010"
REFERENCES,0.5790349417637272,"3.368
± 0.380
2.362
± 0.183
2.374
± 0.058
1.906
± 0.019"
REFERENCES,0.5806988352745425,"168
24.016
± 3.355
32.496
± 1.563
24.516
± 1.705
6.950
± 0.313"
REFERENCES,0.5823627287853578,"42.270
± 1.708
28.487
± 0.642
14.759
± 1.149
5.242
± 0.104"
REFERENCES,0.5840266222961731,"12.307
± 1.637
4.899
± 0.229
10.205
± 0.533
4.225
± 0.083"
REFERENCES,0.5856905158069884,"336
74.168
± 9.015
29.549
± 2.453
55.817
± 3.180
9.572
± 0.270"
REFERENCES,0.5873544093178037,"95.645
± 6.432
30.837
± 1.115
42.654
± 3.450
8.063
± 0.155"
REFERENCES,0.589018302828619,"28.823
± 2.681
7.447
± 0.350
15.831
± 0.141
5.975
± 0.022"
REFERENCES,0.5906821963394343,"720
129.440
± 22.426
35.908
± 2.230
111.842
± 18.634
13.109
± 0.391"
REFERENCES,0.5923460898502496,"185.321
± 17.061
47.461
± 6.122
92.521
± 33.126
11.953
± 0.977"
REFERENCES,0.5940099833610649,"142.118
± 36.524
16.428
± 2.748
26.577
± 0.987
8.989
± 0.035"
REFERENCES,0.5956738768718802,"960
177.336
± 18.126
38.348
± 1.665
170.218
± 14.187
15.240
± 0.246"
REFERENCES,0.5973377703826955,"281.720
± 75.629
53.148
± 9.408
100.142
± 29.742
12.766
± 0.734"
REFERENCES,0.5990016638935108,"218.477
± 27.365
20.498
± 1.979
43.028
± 2.007
11.080
± 0.108 ETTm1"
REFERENCES,0.6006655574043261,"24
2.343
± 0.130
1.730
± 0.098
2.478
± 0.094
1.590
± 0.040"
REFERENCES,0.6023294509151415,"3.109
± 0.135
2.062
± 0.178
3.368
± 0.067
1.882
± 0.026"
REFERENCES,0.6039933444259568,"2.618
± 0.096
1.194
± 0.023
2.134
± 0.034
0.995
± 0.007"
REFERENCES,0.6056572379367721,"48
4.818
± 0.122
3.133
± 0.097
4.116
± 0.093
2.457
± 0.038"
REFERENCES,0.6073211314475874,"5.223
± 0.340
3.036
± 0.083
4.400
± 0.084
2.426
± 0.061"
REFERENCES,0.6089850249584027,"4.176
± 0.355
1.661
± 0.077
3.317
± 0.045
1.470
± 0.015"
REFERENCES,0.610648918469218,"96
8.550
± 0.671
4.813
± 0.228
5.837
± 0.098
3.586
± 0.085"
REFERENCES,0.6123128119800333,"9.686
± 1.050
5.220
± 0.162
6.803
± 0.271
3.803
± 0.090"
REFERENCES,0.6139767054908486,"5.479
± 0.349
2.409
± 0.093
4.782
± 0.045
2.180
± 0.009"
REFERENCES,0.6156405990016639,"288
32.918
± 2.223
11.054
± 0.341
17.231
± 0.759
7.403
± 0.122"
REFERENCES,0.6173044925124792,"42.809
± 3.710
10.706
± 0.513
15.942
± 0.531
7.285
± 0.064"
REFERENCES,0.6189683860232945,"22.650
± 2.713
5.365
± 0.072
15.457
± 0.273
4.436
± 0.033"
REFERENCES,0.6206322795341098,"672
80.546
± 17.739
16.365
± 0.749
38.265
± 6.209
12.030
± 0.455"
REFERENCES,0.6222961730449251,"111.531
± 13.075
16.601
± 0.649
45.064
± 5.369
12.482
± 0.311"
REFERENCES,0.6239600665557404,"149.483
± 10.180
13.980
± 0.350
44.721
± 1.867
8.040
± 0.126"
REFERENCES,0.6256239600665557,"1344
161.539
± 16.808
23.822
± 1.203
77.844
± 7.652
17.293
± 0.606"
REFERENCES,0.627287853577371,"444.199
± 59.765
69.125
± 63.858
161.725
± 99.382
19.490
± 3.115"
REFERENCES,0.6289517470881864,"397.100
± 39.675
27.319
± 2.824
98.890
± 4.495
12.779
± 0.156 ECL"
REFERENCES,0.6306156405990017,"24
0.517
± 0.007
1.610
± 0.015
0.339
± 0.005
1.234
± 0.006"
REFERENCES,0.632279534109817,"0.506
± 0.004
1.702
± 0.017
0.384
± 0.002
1.368
± 0.010"
REFERENCES,0.6339434276206323,"0.368
± 0.017
1.171
± 0.016
0.281
± 0.001
1.058
± 0.053"
REFERENCES,0.6356073211314476,"48
0.677
± 0.029
2.375
± 0.038
0.383
± 0.006
1.806
± 0.012"
REFERENCES,0.6372712146422629,"0.676
± 0.025
2.444
± 0.028
0.446
± 0.008
1.972
± 0.011"
REFERENCES,0.6389351081530782,"0.466
± 0.034
1.752
± 0.038
0.318
± 0.006
1.527
± 0.004"
REFERENCES,0.6405990016638935,"168
1.338
± 0.017
4.364
± 0.071
0.785
± 0.024
3.709
± 0.054"
REFERENCES,0.6422628951747088,"1.738
± 0.166
4.576
± 0.087
0.839
± 0.013
3.828
± 0.016"
REFERENCES,0.6439267886855241,"0.955
± 0.077
3.379
± 0.033
0.730
± 0.029
3.122
± 0.026"
REFERENCES,0.6455906821963394,"336
2.276
± 0.046
6.358
± 0.148
1.495
± 0.044
5.640
± 0.126"
REFERENCES,0.6472545757071547,"3.555
± 0.562
6.448
± 0.049
1.661
± 0.083
5.702
± 0.038"
REFERENCES,0.64891846921797,"2.336
± 0.209
5.103
± 0.033
1.194
± 0.034
4.591
± 0.012"
REFERENCES,0.6505823627287853,"720
23.481
± 15.963
16.561
± 5.353
22.117
± 12.615
17.227
± 3.790"
REFERENCES,0.6522462562396006,"12.379
± 4.196
10.841
± 1.204
5.583
± 1.060
9.239
± 0.230"
REFERENCES,0.653910149750416,"4.576
± 0.806
7.913
± 0.102
2.830
± 0.092
7.162
± 0.056"
REFERENCES,0.6555740432612313,"960
32.548
± 18.405
21.065
± 3.649
28.328
± 8.781
21.717
± 3.002"
REFERENCES,0.6572379367720466,"15.866
± 3.309
12.128
± 0.377
9.198
± 1.400
11.140
± 0.347"
REFERENCES,0.6589018302828619,"6.972
± 0.678
9.255
± 0.108
4.502
± 0.233
8.560
± 0.064"
REFERENCES,0.6605657237936772,Published as a conference paper at ICLR 2022
REFERENCES,0.6622296173044925,"Table 9: Forecasting performance of RevIN in comparison with existing dynamic normaliza-
tion methods. LSTNet∗indicates the model where the autoregressive linear bypass module of LST-
Net is added to the baseline network. ES-RNN∗indicates the model where the exponential smooth-
ing of ES-RNN is added to the baseline network. The experiments are conducted on the ETTh1,
ETTh2, ETTm1, ECL, and the M4 datasets using N-BEATS as the baseline. The missing perfor-
mances in the table are where the model fails to converge."
REFERENCES,0.6638935108153078,"Methods
N-BEATS
+ LSTNet∗
+ ESRNN∗
+ RevIN"
REFERENCES,0.6655574043261231,"Metric
MSE
MAE
MSE
MAE
MSE
MAE
MSE
MAE ETTh1"
REFERENCES,0.6672212978369384,"24
0.478
± 0.022
0.505
± 0.012"
REFERENCES,0.6688851913477537,"0.462
± 0.047
0.497
± 0.035"
REFERENCES,0.670549084858569,"0.547
± 0.031
0.515
± 0.021"
REFERENCES,0.6722129783693843,"0.330
± 0.006
0.373
± 0.004"
REFERENCES,0.6738768718801996,"48
0.536
± 0.060
0.542
± 0.041"
REFERENCES,0.6755407653910149,"0.587
± 0.063
0.576
± 0.043"
REFERENCES,0.6772046589018302,"0.662
± 0.027
0.567
± 0.012"
REFERENCES,0.6788685524126455,"0.372
± 0.001
0.400
± 0.002"
REFERENCES,0.6805324459234608,"168
1.005
± 0.146
0.782
± 0.064"
REFERENCES,0.6821963394342762,"1.031
± 0.099
0.795
± 0.054"
REFERENCES,0.6838602329450915,"0.698
± 0.044
0.600
± 0.018"
REFERENCES,0.6855241264559068,"0.466
± 0.030
0.452
± 0.014"
REFERENCES,0.6871880199667221,"336
0.932
± 0.079
0.743
± 0.042"
REFERENCES,0.6888519134775375,"0.964
± 0.047
0.760
± 0.023"
REFERENCES,0.6905158069883528,"0.768
± 0.041
0.640
± 0.022"
REFERENCES,0.6921797004991681,"0.515
± 0.013
0.483
± 0.008"
REFERENCES,0.6938435940099834,"720
1.389
± 0.230
0.926
± 0.066"
REFERENCES,0.6955074875207987,"1.549
± 0.061
0.994
± 0.022"
REFERENCES,0.697171381031614,"0.966
± 0.084
0.742
± 0.022"
REFERENCES,0.6988352745424293,"0.576
± 0.035
0.534
± 0.018"
REFERENCES,0.7004991680532446,"960
1.383
± 0.380
0.932
± 0.120"
REFERENCES,0.7021630615640599,"1.293
± 0.059
0.897
± 0.026
-
-
0.678
± 0.019
0.575
± 0.009 ETTh2"
REFERENCES,0.7038269550748752,"24
0.403
± 0.185
0.472
± 0.101"
REFERENCES,0.7054908485856906,"0.394
± 0.099
0.485
± 0.068"
REFERENCES,0.7071547420965059,"0.614
± 0.010
0.522
± 0.004"
REFERENCES,0.7088186356073212,"0.192
± 0.003
0.276
± 0.002"
REFERENCES,0.7104825291181365,"48
1.330
± 0.240
0.918
± 0.073"
REFERENCES,0.7121464226289518,"1.261
± 0.214
0.907
± 0.075"
REFERENCES,0.7138103161397671,"0.654
± 0.009
0.543
± 0.007"
REFERENCES,0.7154742096505824,"0.254
± 0.011
0.320
± 0.008"
REFERENCES,0.7171381031613977,"168
7.174
± 0.449
2.329
± 0.049"
REFERENCES,0.718801996672213,"7.053
± 0.428
2.290
± 0.095"
REFERENCES,0.7204658901830283,"0.962
± 0.129
0.696
± 0.054"
REFERENCES,0.7221297836938436,"0.410
± 0.010
0.418
± 0.005"
REFERENCES,0.7237936772046589,"336
4.859
± 0.268
1.863
± 0.043"
REFERENCES,0.7254575707154742,"5.070
± 0.336
1.914
± 0.083"
REFERENCES,0.7271214642262895,"1.204
± 0.158
0.789
± 0.050"
REFERENCES,0.7287853577371048,"0.449
± 0.011
0.447
± 0.006"
REFERENCES,0.7304492512479202,"720
5.656
± 1.053
2.012
± 0.186"
REFERENCES,0.7321131447587355,"6.311
± 2.057
2.049
± 0.225"
REFERENCES,0.7337770382695508,"1.284
± 0.145
0.810
± 0.033"
REFERENCES,0.7354409317803661,"0.496
± 0.008
0.482
± 0.002"
REFERENCES,0.7371048252911814,"960
6.408
± 2.039
2.077
± 0.242"
REFERENCES,0.7387687188019967,"5.627
± 1.670
1.965
± 0.314"
REFERENCES,0.740432612312812,"1.338
± 0.535
0.809
± 0.127"
REFERENCES,0.7420965058236273,"0.471
± 0.015
0.481
± 0.008 ETTm1"
REFERENCES,0.7437603993344426,"24
0.443
± 0.043
0.437
± 0.035"
REFERENCES,0.7454242928452579,"0.412
± 0.026
0.426
± 0.024"
REFERENCES,0.7470881863560732,"0.564
± 0.015
0.477
± 0.009"
REFERENCES,0.7487520798668885,"0.403
± 0.006
0.392
± 0.005"
REFERENCES,0.7504159733777038,"48
0.453
± 0.034
0.472
± 0.018"
REFERENCES,0.7520798668885191,"0.420
± 0.028
0.455
± 0.018"
REFERENCES,0.7537437603993344,"0.615
± 0.093
0.531
± 0.049"
REFERENCES,0.7554076539101497,"0.328
± 0.010
0.371
± 0.007"
REFERENCES,0.757071547420965,"96
0.603
± 0.051
0.581
± 0.027"
REFERENCES,0.7587354409317804,"0.572
± 0.039
0.553
± 0.030"
REFERENCES,0.7603993344425957,"0.668
± 0.031
0.555
± 0.015"
REFERENCES,0.762063227953411,"0.379
± 0.011
0.406
± 0.007"
REFERENCES,0.7637271214642263,"288
0.849
± 0.095
0.702
± 0.051"
REFERENCES,0.7653910149750416,"0.789
± 0.069
0.677
± 0.039"
REFERENCES,0.7670549084858569,"0.795
± 0.070
0.623
± 0.032"
REFERENCES,0.7687188019966722,"0.451
± 0.016
0.445
± 0.008"
REFERENCES,0.7703826955074875,"672
0.860
± 0.057
0.726
± 0.026"
REFERENCES,0.7720465890183028,"0.958
± 0.183
0.758
± 0.076"
REFERENCES,0.7737104825291181,"1.657
± 1.116
0.890
± 0.290"
REFERENCES,0.7753743760399334,"0.555
± 0.011
0.511
± 0.008"
REFERENCES,0.7770382695507487,"1344
14.613
± 26.108
1.948
± 1.655"
REFERENCES,0.778702163061564,"5.592
± 7.032
1.497
± 0.671
-
-
0.631
± 0.061
0.556
± 0.020 ECL"
REFERENCES,0.7803660565723793,"24
0.279
± 0.007
0.372
± 0.003"
REFERENCES,0.7820299500831946,"0.198
± 0.005
0.310
± 0.003"
REFERENCES,0.78369384359401,"0.242
± 0.005
0.332
± 0.006"
REFERENCES,0.7853577371048253,"0.176
± 0.002
0.285
± 0.001"
REFERENCES,0.7870216306156406,"48
0.309
± 0.007
0.388
± 0.004"
REFERENCES,0.7886855241264559,"0.245
± 0.009
0.343
± 0.007"
REFERENCES,0.7903494176372712,"0.275
± 0.007
0.352
± 0.006"
REFERENCES,0.7920133111480865,"0.194
± 0.001
0.301
± 0.001"
REFERENCES,0.7936772046589018,"168
0.333
± 0.016
0.410
± 0.012"
REFERENCES,0.7953410981697171,"0.285
± 0.006
0.375
± 0.004
-
-
0.218
± 0.002
0.320
± 0.001"
REFERENCES,0.7970049916805324,"336
0.326
± 0.004
0.406
± 0.001"
REFERENCES,0.7986688851913477,"0.304
± 0.019
0.393
± 0.013
-
-
0.241
± 0.005
0.337
± 0.002"
REFERENCES,0.800332778702163,"720
0.420
± 0.094
0.467
± 0.058"
REFERENCES,0.8019966722129783,"0.378
± 0.083
0.443
± 0.056
-
-
0.303
± 0.012
0.383
± 0.011"
REFERENCES,0.8036605657237936,"960
0.399
± 0.022
0.455
± 0.017"
REFERENCES,0.8053244592346089,"0.360
± 0.037
0.433
± 0.027
-
-
0.325
± 0.019
0.398
± 0.015 M4"
REFERENCES,0.8069883527454242,"average
0.224
± 0.004
0.207
± 0.003"
REFERENCES,0.8086522462562395,"0.223
± 0.004
0.206
± 0.003"
REFERENCES,0.8103161397670549,"0.223
± 0.001
0.204
± 0.001"
REFERENCES,0.8119800332778702,"0.208
± 0.001
0.197
± 0.001"
REFERENCES,0.8136439267886856,Published as a conference paper at ICLR 2022
REFERENCES,0.8153078202995009,"(a) model input
(b) normalized input 
(c) model output
(d) denormalized output"
REFERENCES,0.8169717138103162,"ESRNN*
RevIN
LSTNet*"
REFERENCES,0.8186356073211315,"Density
Density
Density"
REFERENCES,0.8202995008319468,"Figure 8: Effect of RevIN on distribution discrepancy on training and test data compared to
existing dynamic normalization methods. We compare RevIN with LSTNet∗, which adds the
autoregressive linear bypass module of LSTNet to the baseline and ES-RNN∗, which adds the expo-
nential smoothing of ES-RNN to the baseline. The analysis is conducted on the ETTh2 dataset with
a prediction length of 960 using N-BEATS as the baseline. From left to right, the columns compare
the training and test data distributions of each step of the sequential process in each method."
REFERENCES,0.8219633943427621,"A.6
COMPARISON WITH EXISTING DYNAMIC NORMALIZATION METHODS"
REFERENCES,0.8236272878535774,"We compare RevIN with the dynamic normalization methods proposed in LSTNet (Lai et al., 2017)
and ES-RNN (Smyl, 2020). Similar to adding RevIN to the baseline model, we add the autoregres-
sive linear bypass module of LSTNet and the modiﬁed Holt-Winters exponential smoothing of ES-
RNN to the baseline model, respectively. As shown in Table 9, RevIN consistently achieves the best
performance among the normalization methods adopted on N-BEATS by a signiﬁcant margin. When
we replace RevIN with the other normalization methods, the autoregressive linear bypass module of
LSTNet (LSTNet∗) also consistently reduces the prediction error compared to the baseline. How-
ever, the performance improvement is smaller than our method. For example, when the prediction
length is 960 on the ETTh2 dataset, N-BEATS shows an average error of 6.408, and LSTNet∗re-
duces the error to 5.627. But this is still much worse than RevIN, which reduces the error to 0.471.
Similarly, when the prediction length is 1344 on the ETTm1 dataset, the baseline shows an average
error of 14.613 and LSTNet∗largely decreases the error to 5.592, but RevIN more signiﬁcantly de-
creases the error to 0.631. In the case of ES-RNN∗, the training of the model is unstable, failing to
converge for several cases. Also, ES-RNN∗often degrades the baseline performance, e.g., when the
prediction length is either 24 or 48 on the ETTh1 and ETTm1 datasets. It signiﬁcantly reduces the
error for long prediction length much better than LSTNet∗, but still worse than RevIN, for example,
when the prediction length is 960 on the ETTh2 dataset."
REFERENCES,0.8252911813643927,"Additionally, we further analyze the data distributions of the dynamic normalization methods on the
ETTh2 dataset, as shown in Fig. 8. We compare the training and test data distributions of each step
of the sequential process in each method."
REFERENCES,0.826955074875208,"ES-RNN∗shows the distributions of (a) the original model input, (b) the normalized input where the
level and seasonality are removed by its proposed method, (c) the model prediction output, (d) the
denormalized output where the level and seasonality is multiplied back to the original distribution."
REFERENCES,0.8286189683860233,"LSTNet∗shows the distributions of (a) the original model input, (b) the same original input since
the method does not transform the input data before feeding them to the main prediction model, the
model prediction output (c) before, and (d) after adding the output of the autoregressive network."
REFERENCES,0.8302828618968386,"RevIN shows the distributions of (a) the original model input, (b) the normalized input by RevIN,
(c) the model prediction output, and (d) the denormalized output by RevIN."
REFERENCES,0.831946755407654,Published as a conference paper at ICLR 2022
REFERENCES,0.8336106489184693,"In Fig. 8(a), the original training and test data show a discrepancy in their distributions. Also, they
have several peaks, not being centered on the mean. This implies that sequences in the data will
have different mean values. In Fig. 8(b), both RevIN and ES-RNN∗transform data distributions
into mean-centered distributions. Particularly, ES-RNN∗extremely concentrates the distribution on
the mean, with only a small variance. Both RevIN and ES-RNN∗result in data sequences with
similar statistics, thereby alleviating the distribution shift problem in the input data. This leads to
outstanding performance on long prediction sequences, in contrast to LSTNet∗. LSTNet∗cannot
resolve the distribution discrepancy because it does not have any module that can change the input
statistics. Also, in LSTNet∗, the distributions of the model output (Fig. 8(c)) completely differ from
the input data distributions (Fig. 8(b)). The method cannot make the input and output distribution
to be consistent. In addition, its proposed autoregressive model barely affects the model output
distributions, as shown in Fig. 8(c-d); there is almost no difference between the model output and
the ﬁnal output distributions."
REFERENCES,0.8352745424292846,"Most importantly, the distributions of the ﬁnal output (Fig. 8(d)) signiﬁcantly differ from the original
data (Fig. 8(a)) in LSTNet∗. Similarly, although ES-RNN∗alleviates the distribution discrepancy
in the input data, it fails to return the model output (Fig. 8(d)) back to the original distribution
(Fig. 8(a)), especially with the test data. These results imply that LSTNet∗and ES-RNN∗fail to
learn the appropriate data distribution, and this could be the main reason why their prediction error
is higher than RevIN. On the other hand, in RevIN, the distributions of the ﬁnal output (Fig. 8(d)) are
successfully returned to the original distributions (Fig. 8(a)). Also, with RevIN, the input and output
of the model maintain consistent distributions, as well as the training and test data be overlapped.
As a result, RevIN shows superior performance than the other dynamic normalization methods."
REFERENCES,0.8369384359400999,"Table 10: Additional results on the comparison with classical and state-of-the-art normaliza-
tion methods in Table 3 in the main manuscript. The mean squared error is measured on the
ETTh1, ETTh2, ETTm1, and ECL datasets. Ty indicates the prediction length. RevBN is the modi-
ﬁed version of RevIN, where the input normalization is replaced by batch normalization."
REFERENCES,0.8386023294509152,"Dataset
Ty
Min-max
norm
z-score
norm
Layer
norm
DAIN
Batch
norm
RevBN
Instance
norm
RevIN
(Ours) ETTh1"
REFERENCES,0.8402662229617305,"24
0.885
0.959
0.472
0.652
0.451
0.574
0.989
0.322
48
1.010
0.898
0.741
1.389
0.557
0.649
0.999
0.373
168
1.074
0.953
0.871
0.996
0.851
0.717
0.946
0.515
336
1.083
0.969
0.827
0.979
0.828
0.775
1.078
0.509
720
1.226
0.978
1.184
1.014
0.916
0.705
0.986
0.567
960
1.224
1.043
1.303
1.032
1.691
0.779
1.090
0.697 ETTh2"
REFERENCES,0.8419301164725458,"24
2.659
3.152
0.478
1.437
0.336
0.550
2.976
0.192
48
2.772
3.232
1.335
1.476
1.018
1.058
3.175
0.244
168
2.987
3.329
4.092
1.982
6.206
0.729
3.240
0.419
336
2.914
3.288
4.207
2.631
5.422
0.546
3.186
0.452
720
3.092
3.031
5.822
2.954
7.062
1.552
3.079
0.492
960
3.308
3.087
5.204
2.802
7.755
2.148
3.145
0.465 ETTm1"
REFERENCES,0.8435940099833611,"24
0.981
0.930
0.515
0.431
0.477
0.680
0.926
0.395
48
0.998
1.005
0.555
0.747
0.489
0.531
1.005
0.337
96
1.035
1.016
0.502
0.672
0.505
0.601
1.021
0.388
288
0.974
0.988
0.773
0.877
0.677
0.656
1.056
0.444
672
1.157
1.029
0.795
1.043
0.620
0.670
1.157
0.549
1344
1.320
1.274
2.488
1.348
1.147
0.991
1.329
0.602 ECL"
REFERENCES,0.8452579034941764,"24
0.370
0.313
0.294
0.348
0.301
0.304
0.307
0.174
48
0.334
0.326
0.310
0.387
0.319
0.331
0.313
0.194
168
0.374
0.335
0.343
0.347
0.320
0.327
0.333
0.220
336
0.378
0.338
0.358
0.357
0.337
0.369
0.335
0.244
720
0.746
0.417
0.371
0.366
0.374
0.440
0.378
0.294
960
0.387
0.377
0.379
0.381
0.422
0.414
0.386
0.329"
REFERENCES,0.8469217970049917,Published as a conference paper at ICLR 2022
REFERENCES,0.848585690515807,"A.7
ADDITIONAL RESULTS ON COMPARISON WITH EXISTING NORMALIZATION METHODS"
REFERENCES,0.8502495840266223,"This section provides complete results that compare with existing normalization methods, which are
not included in the main manuscript due to lack of space. The forecasting error of RevIN and existing
normalization methods are evaluated on the ETTh1, ETTh2, ETTm1, and ECL datasets in Table 10.
RevIN consistently outperforms the other normalization methods across all datasets. Interestingly,
when the denormalization step is added to batch normalization (RevBN) as RevIN, the model better
forecasts long sequences than batch normalization (Batch norm), e.g., when the prediction length is
960. The denormalization step of RevIN plays a critical role in improving model performance by
restoring the model prediction to the original distribution. However, a denormalization step cannot
be added to DAIN since it has the Hadamard multiplication operation in the last step, which is not
reversible when the input and prediction sequence lengths are different. These differences could
be the reason for its worse performance compared to RevIN despite that DAIN requires higher
computational costs and a larger amount of model parameters."
REFERENCES,0.8519134775374376,"A.8
ALGORITHM OF REVERSIBLE INSTANCE NORMALIZATION"
REFERENCES,0.8535773710482529,"Algorithm 1 summarizes the procedure of the proposed approach. Reversible instance normalization
consists of the normalization (line 3-4) and denormalization layers (line 6-7). It transforms the input
and output of a model using identical statistics. As RevIN is generally applicable, gθ in Algorithm 1
(line 5) can be any arbitrary deep neural network."
REFERENCES,0.8552412645590682,"Algorithm 1: RevIN, applied to input x and output y of a module in the model."
REFERENCES,0.8569051580698835,"Input : Tx ∈R1, the input sequence length; x(i)
kt ∈R1, the k-th feature at time step t
of the i-th item in a mini-batch; γ, β ∈RK, learnable parameters for RevIN;
gθ, a module in the model parameterized by θ.
Output: γ, β, θ."
COMPUTE,0.8585690515806988,"1
Compute
µT ←
1
Tx
PTx
j=1 x(i)
kj
▷instance mean"
COMPUTE,0.8602329450915142,"2
Compute
σ2
T ←
1
Tx
PTx
j=1(x(i)
kj −µT )2
▷instance variance"
NORMALIZE,0.8618968386023295,"3
Normalize
ˆx(i)
kt ←x(i)
kt −µT
√"
NORMALIZE,0.8635607321131448,"σ2
T +ϵ
▷normalization"
TRANSFORM,0.8652246256239601,"4
Transform
ˆx(i)
kt ←γk · ˆx(i)
kt + βk ≡RevINn
γ,β(x(i)
kt )
▷scale and shift"
PREDICT,0.8668885191347754,"5
Predict
˜y ←gθ(ˆx)
▷forward propagation"
RETRANSFORM,0.8685524126455907,"6
Retransform
ˆy(i)
kt ←˜y(i)
kt −βk"
RETRANSFORM,0.870216306156406,"γk
▷reverse scale and shift"
DENORMALIZE,0.8718801996672213,"7
Denormalize
ˆy(i)
kt ←µT + ˆy(i)
kt
p"
DENORMALIZE,0.8735440931780366,"σ2
T + ϵ ≡RevINdn
γ,β(˜y(i)
kt )
▷denormalization"
DENORMALIZE,0.8752079866888519,"A.9
THEORETICAL JUSTIFICATION OF REVIN AGAINST DISTRIBUTION SHIFT"
DENORMALIZE,0.8768718801996672,"Let x(i) ∈RK×Tx denote a time series comprising K variables of length Tx. Consider a univariate
case where K = 1 without the loss of generality. Then, x(i) ∈RTx denotes the i-th time series
in the data. Consider training and test data, whose distributions are denoted as Ptra and Ptst, re-
spectively. We consider a distribution shift problem where the training and test data have different
distributions (Du et al., 2021). That is,"
DENORMALIZE,0.8785357737104825,"Ptra(x) ̸= Ptst(x).
(4)"
DENORMALIZE,0.8801996672212978,"In our work, we consider the distribution shift problem in terms of the mean and the variance. Then,
the distribution shift problem can be redeﬁned as"
DENORMALIZE,0.8818635607321131,"E[xtra] ̸= E[xtst] or Var[xtra] ̸= Var[xtst], for xtra ∼Ptra, xtst ∼Ptst.
(5)"
DENORMALIZE,0.8835274542429284,"Let’s assume that the given training and test data suffer from the distribution shift problem in terms
of the mean and variance. In order to solve this problem, RevIN ﬁrst normalizes a training sample"
DENORMALIZE,0.8851913477537438,Published as a conference paper at ICLR 2022
DENORMALIZE,0.8868552412645591,"x(i) ∼Ptra. Mathematically, a training sample is transformed as"
DENORMALIZE,0.8885191347753744,"ˆx(i) = x(i) −E[x(i)]
p"
DENORMALIZE,0.8901830282861897,"Var[x(i)]
· γ + β.
(6)"
DENORMALIZE,0.891846921797005,"By the laws of expectation and variance,"
DENORMALIZE,0.8935108153078203,"E[ˆx(i)] = β and Var[ˆx(i)] = γ2, for all i.
(7)"
DENORMALIZE,0.8951747088186356,"By symmetry, this also holds for x(i) ∼Ptst, for all i. Therefore, the mean and variance of the
training and test data distributions become identical. Thus, by deﬁnition (Eq. 5), the distribution
shift problem for the training and test data is solved by the ﬁrst step of RevIN."
DENORMALIZE,0.8968386023294509,"Given the normalized time-series data, the forecasting model parameterized by θ, fθ : RTx →RTy,
predicts the corresponding subsequent future values, ˜y = fθ(ˆx). Then, the denormalization step of
RevIN returns the non-stationary information of the original data, i.e., E[x(i)] and Var[x(i)] in Eq. 6,
to the prediction output so that model does not have to reconstruct them from the normalized input.
In summary, the model prediction ˜y(i) is denormalized as"
DENORMALIZE,0.8985024958402662,"ˆy(i) = ˜y(i) −β γ
·
q"
DENORMALIZE,0.9001663893510815,"Var[x(i)] + E[x(i)].
(8)"
DENORMALIZE,0.9018302828618968,"By the laws of expectation and variance, the mean and variance of ˆy(i) can be expressed as"
DENORMALIZE,0.9034941763727121,"E[ˆy(i)] = ∆+ E[x(i)] and Var[ˆy(i)] = λ · Var[x(i)].
(9)"
DENORMALIZE,0.9051580698835274,"The denormalization step allows the mean and variance of the ﬁnal prediction values to be expressed
as the difference from the input statistics. Here, since the input data x(i) and the groundtruth future
values are consecutive sequences, we can assume that their difference in the mean and variance
can be expressed as Eq. 9 as well. Under this assumption, the model adopting RevIN only needs to
capture the difference from the input statistics, ∆and λ, to accurately predict the statistics of the
future values. In conclusion, through the normalization and denormalization steps of RevIN, a model
can focus on learning the offset from the input distribution to the output distribution by removing
their common non-stationary statistics."
DENORMALIZE,0.9068219633943427,"A.10
CALCULATION DETAILS ON FEATURE DIVERGENCE"
DENORMALIZE,0.908485856905158,"This section explains how the feature divergence is computed in Section 4.2.3. Following the pre-
vious work (Pan et al., 2018), we calculate the average feature divergence between the training and
test data using symmetric KL divergence, assuming that the output features of the model layer will
follow a Gaussian distribution with mean µ and variance σ2. Then, the equation for the feature
divergence of the k-th feature fk can be expressed as"
DENORMALIZE,0.9101497504159733,"D(f train
k
||f test
k ) = KL(f train
k
||f test
k ) + KL(f test
k ||f train
k
),
(10)"
DENORMALIZE,0.9118136439267887,"where
KL(f A
k ||f B
k ) = log σB
k
σA
k
+ σA
k
2 + (µA
k −µB
k )2"
DENORMALIZE,0.913477537437604,"2σB
k
2
−1"
DENORMALIZE,0.9151414309484193,"2.
(11)"
DENORMALIZE,0.9168053244592346,"A.11
ADDITIONAL EXPERIMENTAL DETAILS"
DENORMALIZE,0.9184692179700499,"We train and evaluate the models using the following seeds: 12, 22, 32, 42, and 52. The experiments
using N-BEATS and Informer as the baseline are performed on NVIDIA TITAN RTX, and the
experiments using SCINet are conducted on NVIDIA TITAN Xp. Following the multivariate time-
series forecasting settings of the previous studies (Zhou et al., 2021; Liu et al., 2021), we select
input sequence length from two days (2d), 4d, 7d, 14d, 15d, 20d, 28d, 30d for the hourly datasets,
i.e., the ETTh1, ETTh2, and ECL datasets, and from half day, 1d, 3.5d, 7d for the ETTm1 dataset.
Particularly, we set the ratio of input length to prediction length to be smaller from 2.0 to 0.35 as the
prediction length becomes longer. In the case of SCINet, when the prediction length is 720, we set
its input length to be 736, unlike the other baselines. It is because the original paper of the method
requires its input sequence length to meet a speciﬁc condition. To be speciﬁc, the input length needs
to be a multiple of 32 due to its hierarchical architecture (Liu et al., 2021)."
DENORMALIZE,0.9201331114808652,Published as a conference paper at ICLR 2022
DENORMALIZE,0.9217970049916805,"A.12
REPRODUCTION DETAILS FOR BASELINE MODELS"
DENORMALIZE,0.9234608985024958,"This section describes the implementation details of the baselines, Informer, N-BEATS, and SCINet.
Note that we compare each baseline model and RevIN using the same hyperparameters except for
the presence of RevIN. We exactly follow the experimental settings of the baseline models by using
their ofﬁcial code to conduct experiments, except for N-BEATS that have no ofﬁcially released code.
We reproduce the model and set hyperparameters as stated in the original N-BEATS paper."
DENORMALIZE,0.9251247920133111,"Informer. We use the ofﬁcial open-source code of Informer 6. If provided, we follow the same hy-
perparameter settings in training the network, e.g., hidden dimension of the network or the learning
rate. For the ECL dataset, detailed hyperparameter settings are not ofﬁcially provided in Informer;
we use the same settings with the ETTh1 dataset."
DENORMALIZE,0.9267886855241264,"N-BEATS. We reproduce N-BEATS using the PyTorch framework (Paszke et al., 2019). We follow
the same hyperparameter settings of the N-BEATS-I model in the original paper. We train N-BEATS
to minimize the mean squared error between the model prediction and groundtruth values. For a fair
comparison with the other baselines, we use a single model instead of using the ensemble method
originally proposed in the N-BEATS paper. Since N-BEATS is a model tailored to univariate time-
series forecasting, we ﬂatten each multivariate input sequence into a univariate sequence having a
single dimension for the feature before feeding it to the model. Additionally, we conduct a grid
search for the learning rate of N-BEATS with the range of [1e-5, 1e-3] and train the model using the
weight decay with the factor of 0.001 to stabilize training."
DENORMALIZE,0.9284525790349417,SCINet. We follow the experimental settings provided in the ofﬁcial code 7 of SCINet.
DENORMALIZE,0.930116472545757,"A.13
ADDITIONAL QUALITATIVE RESULTS"
DENORMALIZE,0.9317803660565723,"In Fig. 9, we illustrate the additional results comparing the predictions of RevIN and the baselines.
Overall, the prediction results of the baselines are inaccurately scaled and shifted. However, RevIN
shows remarkable performance, consistently improving the baselines to predict more precise results.
With RevIN, the forecasting results are better aligned with the groundtruth."
DENORMALIZE,0.9334442595673876,"A.14
COMPLETE QUANTITATIVE RESULTS"
DENORMALIZE,0.9351081530782029,"Table 11 provides the standard deviation values of ﬁve independent experiments to compare long
sequence forecasting performance in Table 2 in the main manuscript. Table 12 shows the complete
results of the comparison of the forecasting errors between the baselines and RevIN in Table 1 in the
main manuscript. They include the standard deviation of ﬁve runs and the performance reported in
the original papers of the baselines. RevIN shows signiﬁcant performance improvement compared
to the state-of-the-art forecasting baselines."
DENORMALIZE,0.9367720465890182,"Table 11: Standard deviation values of the ﬁve runs for the comparison of long sequence fore-
casting performance in Table 2 in the main manuscript."
DENORMALIZE,0.9384359400998337,"Prediction length
48
168
336
720
960"
DENORMALIZE,0.940099833610649,"Metric
MSE
MAE
MSE
MAE
MSE
MAE
MSE
MAE
MSE
MAE"
DENORMALIZE,0.9417637271214643,"Informer
0.056
0.035
0.052
0.024
0.085
0.031
0.037
0.024
0.034
0.022
+ RevIN
0.030
0.008
0.051
0.030
0.073
0.026
0.067
0.033
0.041
0.022"
DENORMALIZE,0.9434276206322796,"N-BEATS
0.042
0.030
0.056
0.027
0.081
0.037
0.072
0.029
0.067
0.030
+ RevIN
0.006
0.003
0.014
0.007
0.011
0.006
0.041
0.020
0.027
0.012"
DENORMALIZE,0.9450915141430949,"SCINet
0.008
0.007
0.063
0.042
0.115
0.064
0.033
0.022
0.049
0.029
+ RevIN
0.002
0.001
0.013
0.007
0.022
0.010
0.028
0.017
0.015
0.008"
DENORMALIZE,0.9467554076539102,"6https://github.com/zhouhaoyi/Informer2020
7https://github.com/cure-lab/SCINet"
DENORMALIZE,0.9484193011647255,Published as a conference paper at ICLR 2022
DENORMALIZE,0.9500831946755408,"ETTm1
ECL
ETTh
ETTh
1
2"
DENORMALIZE,0.9517470881863561,"Figure 9: Additional multivariate time-series forecasting results comparing RevIN and state-
of-the-art baselines. The analysis is conducted on the ETTh1, ETTh2, ETTm1, and ECL datasets.
We set the prediction length as 168 for the hour datasets and 288 for the ETTm1 dataset."
DENORMALIZE,0.9534109816971714,Published as a conference paper at ICLR 2022
DENORMALIZE,0.9550748752079867,"Table 12: Complete results for the comparison of the forecasting errors in Table 1 in the main manuscript. The mean and standard deviation of the ﬁve
independent experiments are recorded. † denotes the reported performances for the baselines in their original paper."
DENORMALIZE,0.956738768718802,"Methods
Informer†
Informer
Informer +RevIN
N-BEATS
N-BEATS+RevIN
SCINet†
SCINet
SCINet+RevIN"
DENORMALIZE,0.9584026622296173,"Metric
MSE
MAE
MSE
MAE
MSE
MAE
MSE
MAE
MSE
MAE
MSE
MAE
MSE
MAE
MSE
MAE ETTh1"
DENORMALIZE,0.9600665557404326,"24
0.577
0.549
0.550
± 0.041
0.536
± 0.025
0.504
± 0.049
0.472
± 0.025
0.478
± 0.022
0.505
± 0.012
0.330
± 0.006
0.373
± 0.004
0.311
0.348
0.338
± 0.012
0.373
± 0.009
0.308
± 0.003
0.347
± 0.002"
DENORMALIZE,0.961730449251248,"48
0.685
0.625
0.772
± 0.122
0.668
± 0.055
0.646
± 0.039
0.547
± 0.015
0.536
± 0.060
0.542
± 0.041
0.372
± 0.001
0.400
± 0.002
0.364
0.388
0.436
± 0.025
0.459
± 0.021
0.365
± 0.005
0.389
± 0.003"
DENORMALIZE,0.9633943427620633,"168
0.931
0.752
1.138
± 0.096
0.853
± 0.045
0.655
± 0.055
0.561
± 0.024
1.005
± 0.146
0.782
± 0.064
0.466
± 0.030
0.452
± 0.014
0.497
0.491
0.459
± 0.015
0.461
± 0.013
0.406
± 0.003
0.416
± 0.003"
DENORMALIZE,0.9650582362728786,"336
1.128
0.873
1.278
± 0.129
0.909
± 0.058
1.058
± 0.119
0.758
± 0.059
0.932
± 0.079
0.743
± 0.042
0.515
± 0.013
0.483
± 0.008
0.491
0.494
0.527
± 0.010
0.513
± 0.006
0.467
± 0.005
0.471
± 0.003"
DENORMALIZE,0.9667221297836939,"720
1.215
0.896
1.357
± 0.056
0.945
± 0.009
0.926
± 0.057
0.717
± 0.036
1.389
± 0.230
0.926
± 0.066
0.576
± 0.035
0.534
± 0.018
0.612
0.582
0.596
± 0.015
0.571
± 0.013
0.507
± 0.006
0.505
± 0.004"
DENORMALIZE,0.9683860232945092,"960
·
·
1.470
± 0.124
0.990
± 0.052
0.902
± 0.033
0.715
± 0.025
1.383
± 0.380
0.932
± 0.120
0.678
± 0.019
0.575
± 0.009
·
·
0.604
± 0.017
0.574
± 0.014
0.545
± 0.010
0.526
± 0.005 ETTh2"
DENORMALIZE,0.9700499168053245,"24
0.720
0.665
0.450
± 0.099
0.520
± 0.071
0.238
± 0.010
0.325
± 0.006
0.403
± 0.185
0.472
± 0.101
0.192
± 0.003
0.276
± 0.002
0.183
0.271
0.199
± 0.026
0.295
± 0.027
0.180
± 0.004
0.263
± 0.002"
DENORMALIZE,0.9717138103161398,"48
1.457
1.001
2.171
± 0.094
1.200
± 0.048
0.361
± 0.023
0.404
± 0.014
1.330
± 0.240
0.918
± 0.073
0.254
± 0.011
0.320
± 0.008
0.259
0.341
0.350
± 0.025
0.422
± 0.027
0.231
± 0.006
0.302
± 0.006"
DENORMALIZE,0.9733777038269551,"168
3.489
1.515
8.157
± 0.631
2.558
± 0.113
0.859
± 0.072
0.649
± 0.026
7.174
± 0.449
2.329
± 0.049
0.410
± 0.010
0.418
± 0.005
0.528
0.509
0.559
± 0.044
0.518
± 0.025
0.337
± 0.007
0.378
± 0.003"
DENORMALIZE,0.9750415973377704,"336
2.723
1.340
4.746
± 0.455
1.844
± 0.102
0.890
± 0.057
0.673
± 0.023
4.859
± 0.268
1.863
± 0.043
0.449
± 0.011
0.447
± 0.006
0.648
0.608
0.664
± 0.073
0.583
± 0.030
0.357
± 0.003
0.403
± 0.002"
DENORMALIZE,0.9767054908485857,"720
3.467
1.473
3.190
± 0.326
1.529
± 0.085
0.576
± 0.044
0.546
± 0.025
5.656
± 1.053
2.012
± 0.186
0.496
± 0.008
0.482
± 0.002
1.074
0.761
1.546
± 0.378
0.944
± 0.141
0.411
± 0.003
0.445
± 0.002"
DENORMALIZE,0.978369384359401,"960
·
·
2.972
± 0.183
1.441
± 0.035
0.600
± 0.033
0.570
± 0.018
6.408
± 2.039
2.077
± 0.242
0.471
± 0.015
0.481
± 0.008
·
·
1.862
± 0.153
1.066
± 0.055
0.438
± 0.007
0.462
± 0.004 ETTm1"
DENORMALIZE,0.9800332778702163,"24
0.323
0.369
0.330
± 0.021
0.382
± 0.017
0.309
± 0.020
0.352
± 0.010
0.443
± 0.043
0.437
± 0.035
0.403
± 0.006
0.392
± 0.005
0.127
0.226
0.130
± 0.003
0.231
± 0.003
0.106
± 0.002
0.196
± 0.001"
DENORMALIZE,0.9816971713810316,"48
0.494
0.503
0.499
± 0.024
0.486
± 0.012
0.390
± 0.008
0.391
± 0.006
0.453
± 0.034
0.472
± 0.018
0.328
± 0.010
0.371
± 0.007
0.150
0.261
0.155
± 0.004
0.262
± 0.004
0.135
± 0.003
0.222
± 0.002"
DENORMALIZE,0.9833610648918469,"96
0.678
0.614
0.605
± 0.033
0.554
± 0.027
0.405
± 0.013
0.411
± 0.006
0.603
± 0.051
0.581
± 0.027
0.379
± 0.011
0.406
± 0.007
0.190
0.291
0.195
± 0.012
0.291
± 0.013
0.162
± 0.001
0.247
± 0.001"
DENORMALIZE,0.9850249584026622,"288
1.056
0.786
0.906
± 0.039
0.738
± 0.028
0.563
± 0.024
0.502
± 0.015
0.849
± 0.095
0.702
± 0.051
0.451
± 0.016
0.445
± 0.008
0.417
0.462
0.361
± 0.008
0.419
± 0.004
0.265
± 0.003
0.321
± 0.002"
DENORMALIZE,0.9866888519134775,"672
1.192
0.926
0.943
± 0.062
0.760
± 0.034
0.663
± 0.082
0.550
± 0.031
0.860
± 0.057
0.726
± 0.026
0.555
± 0.011
0.511
± 0.008
0.554
0.527
1.020
± 0.040
0.756
± 0.025
0.357
± 0.004
0.380
± 0.002"
DENORMALIZE,0.9883527454242929,"1344
·
·
1.095
± 0.065
0.823
± 0.040
0.824
± 0.039
0.632
± 0.019
14.613
± 26.108
1.948
± 1.655
0.631
± 0.061
0.556
± 0.020
·
·
1.841
± 0.242
1.044
± 0.100
0.412
± 0.008
0.422
± 0.003 ECL"
DENORMALIZE,0.9900166389351082,"24
·
·
0.250
± 0.005
0.358
± 0.005
0.148
± 0.001
0.257
± 0.001
0.279
± 0.007
0.372
± 0.003
0.176
± 0.002
0.285
± 0.001
·
·
0.138
± 0.004
0.246
± 0.005
0.112
± 0.001
0.207
± 0.001"
DENORMALIZE,0.9916805324459235,"48
0.344
0.393
0.300
± 0.010
0.386
± 0.005
0.171
± 0.004
0.279
± 0.003
0.309
± 0.007
0.388
± 0.004
0.194
± 0.001
0.301
± 0.001
·
·
0.163
± 0.007
0.265
± 0.007
0.126
± 0.001
0.222
± 0.001"
DENORMALIZE,0.9933444259567388,"168
0.368
0.424
0.345
± 0.012
0.423
± 0.010
0.261
± 0.010
0.354
± 0.007
0.333
± 0.016
0.410
± 0.012
0.218
± 0.002
0.320
± 0.001
·
·
0.177
± 0.003
0.281
± 0.005
0.153
± 0.003
0.249
± 0.002"
DENORMALIZE,0.9950083194675541,"336
0.381
0.431
0.429
± 0.041
0.473
± 0.023
0.356
± 0.026
0.414
± 0.015
0.326
± 0.004
0.406
± 0.001
0.241
± 0.005
0.337
± 0.002
·
·
0.202
± 0.004
0.308
± 0.004
0.162
± 0.001
0.262
± 0.001"
DENORMALIZE,0.9966722129783694,"720
0.406
0.443
0.851
± 0.088
0.719
± 0.072
0.834
± 0.122
0.700
± 0.076
0.420
± 0.094
0.467
± 0.058
0.303
± 0.012
0.383
± 0.011
·
·
0.234
± 0.006
0.333
± 0.004
0.183
± 0.003
0.281
± 0.002"
DENORMALIZE,0.9983361064891847,"960
0.460
0.548
0.930
± 0.075
0.750
± 0.042
0.894
± 0.047
0.741
± 0.031
0.399
± 0.022
0.455
± 0.017
0.325
± 0.019
0.398
± 0.015
·
·
0.235
± 0.011
0.330
± 0.008
0.200
± 0.003
0.292
± 0.002"
