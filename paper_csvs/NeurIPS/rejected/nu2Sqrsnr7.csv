Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002173913043478261,"This paper explores the optimal imposition of hard constraints, strategic sampling
1"
ABSTRACT,0.004347826086956522,"of PDEs, and computational domain scaling for solving the acoustic wave equation
2"
ABSTRACT,0.006521739130434782,"within a specified computational budget. First, we derive a formula to systemat-
3"
ABSTRACT,0.008695652173913044,"ically enforce hard boundary and initial conditions in Physics-Informed Neural
4"
ABSTRACT,0.010869565217391304,"Networks (PINNs), employing continuous functions within the PINN ansatz to
5"
ABSTRACT,0.013043478260869565,"ensure that these conditions are satisfied. We demonstrate that optimally selecting
6"
ABSTRACT,0.015217391304347827,"these functions significantly enhances the convergence of the solution. Secondly,
7"
ABSTRACT,0.017391304347826087,"we introduce a Dynamic Amplitude-Focused Sampling (DAFS) method that opti-
8"
ABSTRACT,0.01956521739130435,"mizes the efficiency of hard-constraint PINNs under a fixed number of sampling
9"
ABSTRACT,0.021739130434782608,"points. Leveraging these strategies, we develop an algorithm to determine the opti-
10"
ABSTRACT,0.02391304347826087,"mal computational domain size, given a computational budget. Our approach offers
11"
ABSTRACT,0.02608695652173913,"a practical framework for domain decomposition in large-scale implementation of
12"
ABSTRACT,0.02826086956521739,"acoustic wave equation systems.
13"
INTRODUCTION,0.030434782608695653,"1
Introduction
14"
INTRODUCTION,0.03260869565217391,"The concept of using artificial neural networks to solve differential equations was first explored in the
15"
INTRODUCTION,0.034782608695652174,"1990s by Lagaris et al. [1998]. In the work of Lagaris et al. [1998], they developed an ansatz solution
16"
INTRODUCTION,0.03695652173913044,"that inherently satisfies the boundary conditions (BC) and the initial conditions (IC) of differential
17"
INTRODUCTION,0.0391304347826087,"equations. More recently, the advent of physics-informed neural networks (PINNs) was marked by
18"
INTRODUCTION,0.041304347826086954,"the influential study of Raissi et al. [2019]. This work leverages modern deep neural networks to solve
19"
INTRODUCTION,0.043478260869565216,"forward and inverse problems involving nonlinear partial differential equations (PDEs), incorporating
20"
INTRODUCTION,0.04565217391304348,"BCs and ICs through soft constraints in loss functions.
21"
INTRODUCTION,0.04782608695652174,"Subsequent research has introduced various modifications to PINNs to enhance their accuracy,
22"
INTRODUCTION,0.05,"efficiency, and scalability [Lu et al., 2021a]. There are a couple of drawbacks for many PINNs with
23"
INTRODUCTION,0.05217391304347826,"soft constraints for BCs and ICs. The selection of weights and samples for BCs and ICs cannot
24"
INTRODUCTION,0.05434782608695652,"certainly be determined and requires many trial-and-error tests. Even when the loss function is
25"
INTRODUCTION,0.05652173913043478,"minimized, the BCs and ICs are not strictly satisfied. To target the scaling problems of general PDEs
26"
INTRODUCTION,0.058695652173913045,"and take advantage of parallel computing, XPINNs and FBPINNs have been developed based on
27"
INTRODUCTION,0.06086956521739131,"domain decomposition methods [Jagtap and Karniadakis, 2020, Shukla et al., 2021, Moseley et al.,
28"
INTRODUCTION,0.06304347826086956,"2023].
29"
INTRODUCTION,0.06521739130434782,"There are a few key points that these previous reseearches missed. First, how to formulate ansatz
30"
INTRODUCTION,0.06739130434782609,"solutions satisfying BCs and ICs, specifically the function multiplier of NN. Second, if BC and IC
31"
INTRODUCTION,0.06956521739130435,"are inheriently satisfied by constructing the ansatz solution, how to optimally sample the PDEs in the
32"
INTRODUCTION,0.07173913043478261,"training process. Furthermore, for the existing PINNs handling scaling problems, how to decompose
33"
INTRODUCTION,0.07391304347826087,"the domain to save the overall compute budget.
34"
INTRODUCTION,0.07608695652173914,"In this paper, we set up a 1D wave equation problem and investigate the optimal sampling and
35"
INTRODUCTION,0.0782608695652174,"constraint imposing method given a compute budget.
36"
INTRODUCTION,0.08043478260869565,"The contributions of this paper are as follows.
37"
INTRODUCTION,0.08260869565217391,"• We systematically derived the implementation of hard BC and IC constraints in PINNs to
38"
INTRODUCTION,0.08478260869565217,"solve acoustic wave equations. We give a strategy to select basic functions in the PINN
39"
INTRODUCTION,0.08695652173913043,"ansatz solution that guarantee the satisfaction of BCs and ICs. We find that optimal selection
40"
INTRODUCTION,0.0891304347826087,"of the basic function in the PINN ansatz can improve the convergence of PINNs.
41"
INTRODUCTION,0.09130434782608696,"• We developed a Dynamic Amplitude-Focused Sampling (DAFS) algorithm to improve the
42"
INTRODUCTION,0.09347826086956522,"convergence of hard-constraint PINNs for wave equations given a fixed number of sampling
43"
INTRODUCTION,0.09565217391304348,"points.
44"
INTRODUCTION,0.09782608695652174,"• With the hard constraint and importance sampling strategies, we propose an algorithm
45"
INTRODUCTION,0.1,"to find the optimal size of the computational given a compute budget. This domain size
46"
INTRODUCTION,0.10217391304347827,"optimization algorithm can help the domain decomposition-based PINNs for large-scale
47"
INTRODUCTION,0.10434782608695652,"problems save computational cost.
48"
RELATED WORK,0.10652173913043478,"2
Related Work
49"
RELATED WORK,0.10869565217391304,"Hard constraint
Hard constraint PINNs can guarantee the satisfaction of BCs, ICs, symmetries,
50"
RELATED WORK,0.1108695652173913,"and/or conservation laws. There are comprehensive studies of embedding BCs in PINNs. Lu
51"
RELATED WORK,0.11304347826086956,"et al. [2021b] demontrated various ansatz equations to strictly meet Dirichlet and periodic BCs,
52"
RELATED WORK,0.11521739130434783,"and proposed the penalty method and the augamented Lagrangian method to impose inequality
53"
RELATED WORK,0.11739130434782609,"constraints as hard constraints. Liu et al. [2022] developed a unified ansatz formula to enforce the
54"
RELATED WORK,0.11956521739130435,"Dirichlet, Neumann, and Robin boundary conditions for high-dimensional and geometrically complex
55"
RELATED WORK,0.12173913043478261,"domains. Moseley et al. [2023] implemented the hard Dirichlet in the subdomain using a tanh2 (ωx)
56"
RELATED WORK,0.12391304347826088,"function as the multiplier function of the neural networks in their FBPINN ansatz solution. However,
57"
RELATED WORK,0.12608695652173912,"studies on how to impose both hard BC and IC constraints in PINNs for acoustic wave equations
58"
RELATED WORK,0.1282608695652174,"that have a second-order time dirivative term are still limited. Alkhadhr and Almekkawy [2023]
59"
RELATED WORK,0.13043478260869565,"compared the accuracy and performance of PINNs with a combination of hard-BC/soft-BC and
60"
RELATED WORK,0.13260869565217392,"hard-IC/soft-IC for solving a 1D wave equation with a time-dependent point source function. This
61"
RELATED WORK,0.13478260869565217,"implementation of the hard-IC only considers the satisfaction of the wavefield values at the initial
62"
RELATED WORK,0.13695652173913042,"time u(x, t = 0), but neglects the hard constraint of the first-order time derivative of the wavefield
63"
RELATED WORK,0.1391304347826087,"u(x, t), i.e., ∂tu(x, t = 0). Brecht et al. [2023] proposed improved physics-informed DeepONets
64"
RELATED WORK,0.14130434782608695,"with hard constraints, and presented a numerical example of a 1D standing wave equation with
65"
RELATED WORK,0.14347826086956522,"Dirichlet BCs. The DeepONet framework used in the paper has an inherent satisfaction of the initial
66"
RELATED WORK,0.14565217391304347,"wavefield, but ∂tu(x, t = 0) is also neglected. This neglection does not affect the numerical results
67"
RELATED WORK,0.14782608695652175,"for the 1D standing wave equation in their paper, since they simply assume ∂tu(x, t = 0) = 0.
68"
RELATED WORK,0.15,"Strategic Sampling
Many sampling algorithms have been developed to improve the training effi-
69"
RELATED WORK,0.15217391304347827,"ciency, mitigating failure modes of PINNs. [Wu et al., 2023] provided a comprehensive comparison of
70"
RELATED WORK,0.15434782608695652,"ten sampling methods, including non-adaptive and residual-based adaptive methods. Daw et al. [2023]
71"
RELATED WORK,0.1565217391304348,"proposed a Retain-Resample-Release (R3) Sampling algorithm to mitigate the failure propagation
72"
RELATED WORK,0.15869565217391304,"during the training processes of PINNs. [Gao et al., 2023a,b] developed failure informed adamptive
73"
RELATED WORK,0.1608695652173913,"sampling for PINNs, with the extentions of combining re-sampling and subset simulation. Yang et al.
74"
RELATED WORK,0.16304347826086957,"[2023] introduced a Dynamic Mesh-Based Importance Sampling (DMIS) method to enhance the
75"
RELATED WORK,0.16521739130434782,"training of PINNs. Additionally, [Zhang et al., 2024] proposed an annealed adaptive importance
76"
RELATED WORK,0.1673913043478261,"sampling method for solving high-dimensional partial differential equations using PINNs.
77"
RELATED WORK,0.16956521739130434,"Domain Scaling
Computational domain scaling is a key issue to apply PINNs to real-world large
78"
RELATED WORK,0.17173913043478262,"spatial-temporal scale applications. [Jagtap and Karniadakis, 2020] proposed a generalized space-
79"
RELATED WORK,0.17391304347826086,"time domain decomposition framework for PINNs, named extended PINNs (XPINNs), which can
80"
RELATED WORK,0.17608695652173914,"handle nonlinear PDEs on complex-geometry domains. XPINNs provide large representation and
81"
RELATED WORK,0.1782608695652174,"parallelization capacity by deploying multiple neural networks in smaller subdomains, offering both
82"
RELATED WORK,0.18043478260869567,"space and time parallelization to reduce training costs effectively. Shukla et al. [2021] developed
83"
RELATED WORK,0.1826086956521739,"a distributed framework for PINNs based on two extensions: conservative PINNs (cPINNs) and
84"
RELATED WORK,0.18478260869565216,"XPINNs. These methods employ domain decomposition in space and time-space, respectively,
85"
RELATED WORK,0.18695652173913044,"enhancing the parallelization capacity, representation capacity, and efficient hyperparameter tuning of
86"
RELATED WORK,0.1891304347826087,"PINNs. The framework allows for optimizing all hyperparameters of each neural network separately
87"
RELATED WORK,0.19130434782608696,"in each subdomain, providing significant advantages for multi-scale and multi-physics problems. They
88"
RELATED WORK,0.1934782608695652,"demonstrated the efficiency of cPINNs and XPINNs through various forward problems, highlighting
89"
RELATED WORK,0.1956521739130435,"that cPINNs are more communication-efficient while XPINNs offer greater flexibility for handling
90"
RELATED WORK,0.19782608695652174,"complex subdomains. Moseley et al. [2023] addressed the limitations of PINNs in solving large
91"
RELATED WORK,0.2,"domains and multi-scale solutions by proposing Finite Basis PINNs (FBPINNs). FBPINNs use neural
92"
RELATED WORK,0.20217391304347826,"networks to learn basis functions defined over small, overlapping subdomains, inspired by classical
93"
RELATED WORK,0.20434782608695654,"finite element methods. This approach mitigates the spectral bias of neural networks and reduces the
94"
RELATED WORK,0.20652173913043478,"complexity of the optimization problem by using smaller neural networks in a parallel, divide-and-
95"
RELATED WORK,0.20869565217391303,"conquer approach. Their experiments showed that FBPINNs outperform standard PINNs in accuracy
96"
RELATED WORK,0.2108695652173913,"and computational efficiency for both small and large, multi-scale problems. Chalapathi et al. [2024]
97"
RELATED WORK,0.21304347826086956,"introduced a scalable approach to enforce hard physical constraints using Mixture-of-Experts (MoE)
98"
RELATED WORK,0.21521739130434783,"in neural network architectures. This method imposes constraints over smaller decomposed domains,
99"
RELATED WORK,0.21739130434782608,"with each domain solved by an expert through differentiable optimization. The independence of each
100"
RELATED WORK,0.21956521739130436,"expert allows for parallelization across multiple GPUs, improving accuracy, training stability, and
101"
RELATED WORK,0.2217391304347826,"computational efficiency for predicting the dynamics of complex nonlinear systems. The optimal
102"
RELATED WORK,0.22391304347826088,"decomposition of subdomains is critical to the effectiveness of these scaling methods, given a fixed
103"
RELATED WORK,0.22608695652173913,"compute budget. Our work focuses on finding the maximum subdomain size that even a 64x2 small
104"
RELATED WORK,0.22826086956521738,"PINN can handle within a compute budget.
105"
METHODOLOGY,0.23043478260869565,"3
Methodology
106"
METHODOLOGY,0.2326086956521739,"In this section, we outline our approach to effectively implement hard constraints, strategically
107"
METHODOLOGY,0.23478260869565218,"sampling partial differential equations (PDEs), and optimizing the scaling of computational domains.
108"
METHODOLOGY,0.23695652173913043,"These methods are utilized to solve the acoustic wave equation within a specified computational
109"
METHODOLOGY,0.2391304347826087,"budget.
110"
METHODOLOGY,0.24130434782608695,"We focus on an acoustic wave equation defined by:
111"
METHODOLOGY,0.24347826086956523,"D[u(x, t); c(x)] = f(x, t),
x ∈Ω,
t ∈[t0, T],
Bi[u(x, t)] = Ui(x, t),
x ∈∂Ωi,
t ∈[t0, T],
Ij[u(x, t0)] = Vj(x),
x ∈Ω,
(1)"
METHODOLOGY,0.24565217391304348,"where:
112"
METHODOLOGY,0.24782608695652175,"• D represents the differential operator. For a simplified one-dimensional acoustic wave
113"
METHODOLOGY,0.25,"equation, D = ∂tt −c2(x)∇2, indicating the second temporal derivative minus the spatial
114"
METHODOLOGY,0.25217391304347825,"derivative scaled by the square of the local speed of sound, c(x).
115"
METHODOLOGY,0.2543478260869565,"• Bi denotes the boundary condition operator applied at x ∈∂Ωi.
116"
METHODOLOGY,0.2565217391304348,"• Ij signifies the initial condition operator, defining the state of the system at t = t0 across
117"
METHODOLOGY,0.25869565217391305,"the domain Ω.
118"
HARD CONSTRAINT IMPOSING,0.2608695652173913,"3.1
Hard constraint imposing
119"
HARD CONSTRAINT IMPOSING,0.26304347826086955,"A prevalent ansatz employed in prior studies on hard-constraint PINNs for 1D wave equations is
120"
HARD CONSTRAINT IMPOSING,0.26521739130434785,"expressed as:
121"
HARD CONSTRAINT IMPOSING,0.2673913043478261,"u(x, t) = τ(t)˜u(x, t) + (1 −τ(t))u(x, 0),
(2)
where ˜u(x, t) represents the neural network output with inputs x and t, and τ(t) is a function that
122"
HARD CONSTRAINT IMPOSING,0.26956521739130435,"satisfies τ(0) = 0. This design ensures that the initial condition u(x, 0) is met precisely when t = 0.
123"
HARD CONSTRAINT IMPOSING,0.2717391304347826,"To accommodate boundary conditions (BCs) at x = 0 and x = L, the ansatz is often modified to:
124"
HARD CONSTRAINT IMPOSING,0.27391304347826084,"u(x, t) = x(L −x)˜u(x, t) + Ui(x, t),
(3)
ensuring that u(xi, t) = Ui(xi, t) for x ∈∂Ωi.
125"
HARD CONSTRAINT IMPOSING,0.27608695652173915,"A more comprehensive form,
126"
HARD CONSTRAINT IMPOSING,0.2782608695652174,"u(x, t) =x(L −x)τ(t)˜u(x, t) + (1 −τ(t))u(x, 0) +L −x"
HARD CONSTRAINT IMPOSING,0.28043478260869564,"L
(u(0, t) −(1 −τ(t))u(0, 0)) + x"
HARD CONSTRAINT IMPOSING,0.2826086956521739,"L(u(L, t) −(1 −τ(t))u(L, 0)), (4)"
HARD CONSTRAINT IMPOSING,0.2847826086956522,"can ensure both Dirichlet BCs and the initial condition u(x, t)|t=0 = u(x, 0). However, this ansatz
127"
HARD CONSTRAINT IMPOSING,0.28695652173913044,"does not account for ∂tu(x, t)|t=0, unless it is assumed to be zero.
128"
HARD CONSTRAINT IMPOSING,0.2891304347826087,"We propose a more general hard constraint imposition formula:
129"
HARD CONSTRAINT IMPOSING,0.29130434782608694,"u(x, t) =x(L −x)τ(t)˜u(x, t) + ((1 −τ(t)) + t∂t)u(x, 0) +L −x"
HARD CONSTRAINT IMPOSING,0.29347826086956524,"L
(u(0, t) −((1 −τ(t)) + t∂t)u(0, 0)) + x"
HARD CONSTRAINT IMPOSING,0.2956521739130435,"L(u(L, t) −((1 −τ(t)) + t∂t)u(L, 0)), (5)"
HARD CONSTRAINT IMPOSING,0.29782608695652174,"which guarantees satisfaction of the conditions:
130"
HARD CONSTRAINT IMPOSING,0.3,"u(x, t) = Ui(x, t),
x ∈∂Ωi,
u(x, t)|t=0 = Vj(x),
x ∈Ω,
∂tu(x, t)|t=0 = Wj(x),
x ∈Ω,
(6)"
HARD CONSTRAINT IMPOSING,0.30217391304347824,"where Ui(x, t), Vj(x), Wj(x) are the specified functions in BCs and ICs, and τ(t) is an arbitrary
131"
HARD CONSTRAINT IMPOSING,0.30434782608695654,"function satisfying τ(0) = dtτ(0) = 0.
132"
HARD CONSTRAINT IMPOSING,0.3065217391304348,"It is straightforward to demonstrate that the proposed ansatz correctly imposes all BCs and ICs as
133"
HARD CONSTRAINT IMPOSING,0.30869565217391304,"required:
134



 

"
HARD CONSTRAINT IMPOSING,0.3108695652173913,"u(x, t)|x=0
= u(0, t),
u(x, t)|x=L
= u(L, t),
u(x, t)|t=0
= u(x, 0),
∂tu(x, t)|t=0
= ∂tu(x, 0). (7)"
HARD CONSTRAINT IMPOSING,0.3130434782608696,"In Section 4.2, we will explore numerical tests to optimize the selection of τ(t) by evaluating
135"
HARD CONSTRAINT IMPOSING,0.31521739130434784,"convergence rates and mean absolute errors (MAE).
136"
HARD CONSTRAINT IMPOSING,0.3173913043478261,"The primary advantage of employing hard constraints in our model is the elimination of the need to
137"
HARD CONSTRAINT IMPOSING,0.31956521739130433,"fine-tune the weights of PDE, BC, and IC loss terms typically required in soft-constraint PINNs.
138"
SAMPLING STRATEGY,0.3217391304347826,"3.2
Sampling strategy
139"
SAMPLING STRATEGY,0.3239130434782609,"Sampling is crucial for efficient training of PINNs, ensuring rapid convergence and mitigating
140"
SAMPLING STRATEGY,0.32608695652173914,"potential failure modes. To enhance the computational efficiency of our hard-constraint PINNs,
141"
SAMPLING STRATEGY,0.3282608695652174,"we introduce the Dynamic Amplitude-Focused Sampling (DAFS) method. This strategy optimally
142"
SAMPLING STRATEGY,0.33043478260869563,"selects the number of points, Npde, used in the training.
143"
SAMPLING STRATEGY,0.33260869565217394,"Initially, we segmented the computational domain to identify regions with high-amplitude acoustic
144"
SAMPLING STRATEGY,0.3347826086956522,"wave fields, based on low-resolution finite difference (FD) simulations. These high-amplitude regions
145"
SAMPLING STRATEGY,0.33695652173913043,"are defined by a threshold δ, which determines the intensity level above which areas are considered
146"
SAMPLING STRATEGY,0.3391304347826087,"to be of high amplitude. Within these identified regions, we uniformly sampled αNpde points. This
147"
SAMPLING STRATEGY,0.34130434782608693,"was supplemented by uniformly sampling (1 −α)Npde points in the remaining areas of the domain.
148"
SAMPLING STRATEGY,0.34347826086956523,"Both and α are parameters crucial to the sampling process and are optimally chosen to balance the
149"
SAMPLING STRATEGY,0.3456521739130435,"computational budget and the accuracy of the simulations. By adjusting these parameters, we can
150"
SAMPLING STRATEGY,0.34782608695652173,"tailor the distribution of sample points to areas that are most influential in the wave dynamics, thereby
151"
SAMPLING STRATEGY,0.35,"improving the efficiency of our PINN training.
152"
SAMPLING STRATEGY,0.3521739130434783,"The pseudocode for the DAFS algorithm is provided in Algorithm 1.
153"
SAMPLING STRATEGY,0.35434782608695653,"This sampling strategy, characterized by its focus on dynamically identified regions of interest based
154"
SAMPLING STRATEGY,0.3565217391304348,"on wave amplitude, significantly optimizes the efficiency of the computation during the PINN training
155"
SAMPLING STRATEGY,0.358695652173913,"phase. The numerical tests for DAFS are in Section 4.3.
156"
EXPERIMENTS,0.36086956521739133,"4
Experiments
157"
PROBLEM SETUP,0.3630434782608696,"4.1
Problem setup
158"
PROBLEM SETUP,0.3652173913043478,"We applied our method to three numerical examples for three different types of 1D acoustic wave
159"
PROBLEM SETUP,0.3673913043478261,"equations — standing waves, string waves, and traveling waves. The ground truth wavefields are
160"
PROBLEM SETUP,0.3695652173913043,"shown in Figure 1.
161"
PROBLEM SETUP,0.3717391304347826,Algorithm 1 Dynamic Amplitude-Focused Sampling (DAFS)
PROBLEM SETUP,0.3739130434782609,"Require: Npde, α, domain, FD results (low-resolution Finite Difference results indicating amplitude)
Ensure: Sampled points for training"
PROBLEM SETUP,0.3760869565217391,"1: Initialize points ←[]
2: Identify high-amplitude regions from FD results
3: Nhigh ←αNpde
▷Number of points in high-amplitude regions
4: Nlow ←(1 −α)Npde
▷Number of points in low-amplitude regions
5: Uniformly sample Nhigh points in high-amplitude regions and add to points
6: Uniformly sample Nlow points in the remaining areas of the domain and add to points
return points"
PROBLEM SETUP,0.3782608695652174,(a) standing waves
PROBLEM SETUP,0.3804347826086957,(b) string waves
PROBLEM SETUP,0.3826086956521739,(c) Gaussian traveling waves
PROBLEM SETUP,0.3847826086956522,"Figure 1: Ground truth wavefields for (a) standing waves, (b) string waves, and (c) traveling waves
with k = 1, 2, 3."
PROBLEM SETUP,0.3869565217391304,"Standing waves for Dirichlet BCs
Our first numerical example is a standing wave solution for the
162"
PROBLEM SETUP,0.38913043478260867,"following 1D wave equation with Dirichlet BCs:
163"
PROBLEM SETUP,0.391304347826087,"∂2u(x, t)"
PROBLEM SETUP,0.3934782608695652,"∂t2
−c2 ∂2u"
PROBLEM SETUP,0.39565217391304347,"∂x2 = 0, x ∈(0, L)"
PROBLEM SETUP,0.3978260869565217,"B.C.: u(0, t) = u(L, t) = 0,"
PROBLEM SETUP,0.4,"I.C.: u(x, 0) = U(x), ∂u"
PROBLEM SETUP,0.40217391304347827,"∂t (x, 0) = V (x). (8)"
PROBLEM SETUP,0.4043478260869565,"The analytical solution u(x, t) for Equation 8 is
164"
PROBLEM SETUP,0.40652173913043477,"u(x, t) = ∞
X"
PROBLEM SETUP,0.40869565217391307,"n=1
An sin
nπx L"
PROBLEM SETUP,0.4108695652173913,"
cos
nπct L"
PROBLEM SETUP,0.41304347826086957,"
+ Bn sin
nπx L"
PROBLEM SETUP,0.4152173913043478,"
sin
nπct L"
PROBLEM SETUP,0.41739130434782606,"
.
(9)"
PROBLEM SETUP,0.41956521739130437,"A standing wave solution
165"
PROBLEM SETUP,0.4217391304347826,"u(x, t) = sin
kπx L"
PROBLEM SETUP,0.42391304347826086,"
cos
kπct L"
PROBLEM SETUP,0.4260869565217391,"
, k ∈Z+
(10)"
PROBLEM SETUP,0.4282608695652174,"can be achieved if we assume U(x) = sin
  kπx"
PROBLEM SETUP,0.43043478260869567,"L

and V (x) = 0. We show the solutions for k = 1, 2, 3
166"
PROBLEM SETUP,0.4326086956521739,"in Figure 1(a).
167"
PROBLEM SETUP,0.43478260869565216,"String waves for time-dependent BCs
Our third example is a string wave solution for time-
168"
PROBLEM SETUP,0.4369565217391304,"dependent BCs shown in Equation 11. The ground truth solutions in Figuer 1(b) are achieved by
169"
PROBLEM SETUP,0.4391304347826087,"finite different simulation.
170"
PROBLEM SETUP,0.44130434782608696,"∂2u(x, t)"
PROBLEM SETUP,0.4434782608695652,"∂t2
−c2 ∂2u"
PROBLEM SETUP,0.44565217391304346,"∂x2 = 0, x ∈(0, L)"
PROBLEM SETUP,0.44782608695652176,"B.C.: u(0, t) = u(L, t) = sin(2πt),"
PROBLEM SETUP,0.45,"I.C.: u(x, 0) = 0, ∂u"
PROBLEM SETUP,0.45217391304347826,"∂t (x, 0) = 2π cos
2kπx L"
PROBLEM SETUP,0.4543478260869565,"
(11)"
PROBLEM SETUP,0.45652173913043476,"Traveling waves for Gaussian source time functions
Our third example is a traveling wave
171"
PROBLEM SETUP,0.45869565217391306,"solution for initial conditions of Gaussian source time functions shown in Equation 12. The ground
172"
PROBLEM SETUP,0.4608695652173913,"truth solutions in Figuer 1(c) are computed by finite different simulation.
173"
PROBLEM SETUP,0.46304347826086956,"∂2u(x, t)"
PROBLEM SETUP,0.4652173913043478,"∂t2
−c2 ∂2u"
PROBLEM SETUP,0.4673913043478261,"∂x2 = 0, x ∈(0, L)"
PROBLEM SETUP,0.46956521739130436,"B.C.: u(0, t) = u(L, t) = 0,"
PROBLEM SETUP,0.4717391304347826,"I.C.: u(x, 0) =
1
σ
√"
PROBLEM SETUP,0.47391304347826085,"2π exp

−(x −µ)2 2σ2"
PROBLEM SETUP,0.47608695652173916,"
, ∂u"
PROBLEM SETUP,0.4782608695652174,"∂t (x, 0) = 0 (12)"
PROBLEM SETUP,0.48043478260869565,"4.2
Optimal τ(t) selection for hard constraints
174"
PROBLEM SETUP,0.4826086956521739,"We selected six candidate functions for τ(t) to construct PINNs with a network configuration of only
175"
PROBLEM SETUP,0.48478260869565215,"64x2 neurons. Figures 2 through 4 illustrate the L2 loss and L1 error as functions of training epochs.
176"
PROBLEM SETUP,0.48695652173913045,"Our findings suggest that τ(t) significantly influences both the convergence rate and the emergence of
177"
PROBLEM SETUP,0.4891304347826087,"failure modes. In general, t2,
2t2
1+t2 performs better in general, especially for higher modes k = 2, 3.
178"
PROBLEM SETUP,0.49130434782608695,"We show a few training dynmaics in Appendix C.
179"
PROBLEM SETUP,0.4934782608695652,"Our analysis indicates that the frequency characteristics of τ(t) and the corresponding wavefields may
180"
PROBLEM SETUP,0.4956521739130435,"be critical for selecting an appropriate τ(t). Matching these characteristics can potentially enhance
181"
PROBLEM SETUP,0.49782608695652175,"the model’s efficiency by aligning τ(t)’s influence on the neural network’s learning dynamics with
182"
PROBLEM SETUP,0.5,"the physical properties of the wave phenomena being modeled.
183"
PROBLEM SETUP,0.5021739130434782,(a) L2 loss
PROBLEM SETUP,0.5043478260869565,(b) L1 error
PROBLEM SETUP,0.5065217391304347,"Figure 2: L2 loss and L1 error for standing waves with PINNs constructed using six canditate τ(t)
functions."
PROBLEM SETUP,0.508695652173913,(a) L2 loss
PROBLEM SETUP,0.5108695652173914,(b) L1 error
PROBLEM SETUP,0.5130434782608696,"Figure 3: L2 loss and L1 error for string waves with PINNs constructed using six canditate τ(t)
functions."
PROBLEM SETUP,0.5152173913043478,(a) L2 loss
PROBLEM SETUP,0.5173913043478261,(b) L1 error
PROBLEM SETUP,0.5195652173913043,"Figure 4: L2 loss and L1 error for travelling Gaussian waves with PINNs constructed using six
canditate τ(t) functions."
DYNAMIC AMPLITUDE-FOCUSED SAMPLING,0.5217391304347826,"4.3
Dynamic Amplitude-Focused Sampling
184"
DYNAMIC AMPLITUDE-FOCUSED SAMPLING,0.5239130434782608,"We demonstrate the efficacy of our proposed Dynamic Amplitude-Focused Sampling (DAFS) in
185"
DYNAMIC AMPLITUDE-FOCUSED SAMPLING,0.5260869565217391,"enhancing both the convergence and accuracy of Physics-Informed Neural Networks (PINNs).
186"
DYNAMIC AMPLITUDE-FOCUSED SAMPLING,0.5282608695652173,"Experiments varying α from 0 to 0.5 to 1 indicate that optimal results are typically achieved when α
187"
DYNAMIC AMPLITUDE-FOCUSED SAMPLING,0.5304347826086957,"is around 0.5.
188"
DYNAMIC AMPLITUDE-FOCUSED SAMPLING,0.532608695652174,"This suggests a balanced sampling strategy, where a significant portion of the samples is concentrated
189"
DYNAMIC AMPLITUDE-FOCUSED SAMPLING,0.5347826086956522,"in regions of higher amplitude. However, exclusively focusing on these high-amplitude areas can
190"
DYNAMIC AMPLITUDE-FOCUSED SAMPLING,0.5369565217391304,"hinder information transfer from boundary conditions to the interior of the domain, potentially leading
191"
DYNAMIC AMPLITUDE-FOCUSED SAMPLING,0.5391304347826087,"to failure modes. Figures 5 and 6 illustrate these dynamics, showing the L2 loss and L1 error across
192"
DYNAMIC AMPLITUDE-FOCUSED SAMPLING,0.5413043478260869,"different values of α, and the impact on the predicted wavefield and its accuracy.
193"
DYNAMIC AMPLITUDE-FOCUSED SAMPLING,0.5434782608695652,Figure 5: L2 loss and L1 error with varied α from 0 to 1.
DYNAMIC AMPLITUDE-FOCUSED SAMPLING,0.5456521739130434,(a) α = 0.00
DYNAMIC AMPLITUDE-FOCUSED SAMPLING,0.5478260869565217,(b) α = 0.50
DYNAMIC AMPLITUDE-FOCUSED SAMPLING,0.55,(c) α = 1.00
DYNAMIC AMPLITUDE-FOCUSED SAMPLING,0.5521739130434783,"Figure 6: Visualizations for α = 0.00, 0.50, and 1.00 (top to bottom): Left - Predicted wavefield,
Middle - Difference between the prediction and ground truth, Right - Sampling distribution."
OPTIMAL SUBDOMAIN,0.5543478260869565,"4.4
Optimal subdomain
194"
OPTIMAL SUBDOMAIN,0.5565217391304348,"We then propose an optimal subdomain selection method shown in a flow chart in Figure 7. This
195"
OPTIMAL SUBDOMAIN,0.558695652173913,"method will automatically determine the optimal k our 64x2 small PINNs can handle, given a
196"
OPTIMAL SUBDOMAIN,0.5608695652173913,"compute budget.
197"
LIMITATIONS AND TRAINING DYNAMICS,0.5630434782608695,"5
Limitations and Training Dynamics
198"
LIMITATIONS AND TRAINING DYNAMICS,0.5652173913043478,"While our proposed methods significantly enhance the functionality and efficiency of PINNs, the
199"
LIMITATIONS AND TRAINING DYNAMICS,0.5673913043478261,"determination of the optimal function τ(t) presents certain limitations. The choice of τ(t) is crucial
200"
LIMITATIONS AND TRAINING DYNAMICS,0.5695652173913044,"as it directly affects the model’s ability to satisfy boundary and initial conditions rigidly. However,
201"
LIMITATIONS AND TRAINING DYNAMICS,0.5717391304347826,"finding an ideal τ(t) that adapts across different problems and boundary conditions without extensive
202"
LIMITATIONS AND TRAINING DYNAMICS,0.5739130434782609,"trial and error remains challenging. The training dynamics are also sensitive to the form of τ(t), where
203"
LIMITATIONS AND TRAINING DYNAMICS,0.5760869565217391,"inappropriate selections can lead to slower convergence or even divergence in some cases. These
204"
LIMITATIONS AND TRAINING DYNAMICS,0.5782608695652174,"issues underscore the need for a more automated, perhaps adaptive, approach to selecting τ(t) that
205"
LIMITATIONS AND TRAINING DYNAMICS,0.5804347826086956,"can dynamically adjust based on the evolving training characteristics and the specific requirements of
206"
LIMITATIONS AND TRAINING DYNAMICS,0.5826086956521739,"the PDE being solved.
207"
LIMITATIONS AND TRAINING DYNAMICS,0.5847826086956521,Figure 7: The flow chart of optimal subdomain determination.
CONCLUSION,0.5869565217391305,"6
Conclusion
208"
CONCLUSION,0.5891304347826087,"This work presented a comprehensive approach to improving the effectiveness and efficiency of
209"
CONCLUSION,0.591304347826087,"Physics-Informed Neural Networks (PINNs) for solving acoustic wave equations. By integrating
210"
CONCLUSION,0.5934782608695652,"a well-formulated hard constraint imposition strategy and the novel Dynamic Amplitude-Focused
211"
CONCLUSION,0.5956521739130435,"Sampling (DAFS) method, we have significantly enhanced both the accuracy and convergence of
212"
CONCLUSION,0.5978260869565217,"PINNs.
213"
CONCLUSION,0.6,"Our methodological innovations include:
214"
CONCLUSION,0.6021739130434782,"• A systematic derivation of hard boundary and initial conditions in PINNs that ensures these
215"
CONCLUSION,0.6043478260869565,"constraints are inherently satisfied, leading to better convergence and stability of the solution.
216"
CONCLUSION,0.6065217391304348,"• The introduction of DAFS, which optimally allocates computational resources by focus-
217"
CONCLUSION,0.6086956521739131,"ing sampling in regions of high amplitude while ensuring adequate coverage across the
218"
CONCLUSION,0.6108695652173913,"computational domain to prevent information isolation.
219"
CONCLUSION,0.6130434782608696,"• Development of a domain size optimization algorithm that assists in domain decompo-
220"
CONCLUSION,0.6152173913043478,"sition, enabling efficient scaling of PINNs for large-scale applications while managing
221"
CONCLUSION,0.6173913043478261,"computational costs.
222"
CONCLUSION,0.6195652173913043,"These contributions mark a significant step forward in the practical deployment of PINNs, especially
223"
CONCLUSION,0.6217391304347826,"in fields requiring the simulation of complex physical phenomena over large scales. Future work will
224"
CONCLUSION,0.6239130434782608,"focus on extending these strategies to other types of partial differential equations and exploring the
225"
CONCLUSION,0.6260869565217392,"integration of our methods with other deep learning frameworks to further enhance the adaptability
226"
CONCLUSION,0.6282608695652174,"and efficiency of PINNs in diverse applications, for example, we will explore the integration of our
227"
CONCLUSION,0.6304347826086957,"methods with existing PINNs frameworks that employ domain decomposition techniques, such as
228"
CONCLUSION,0.6326086956521739,"XPINNs and FBPINNs, to further enhance their scalability and adaptability. We aim to make PINNs
229"
CONCLUSION,0.6347826086956522,"more adaptable and efficient for a broader range of applications, particularly in complex systems
230"
CONCLUSION,0.6369565217391304,"where traditional numerical methods struggle. By advancing these strategies, we can significantly
231"
CONCLUSION,0.6391304347826087,"contribute to the deployment of PINNs in real-world scenarios, tackling large-scale and multi-scale
232"
CONCLUSION,0.6413043478260869,"challenges effectively.
233"
REFERENCES,0.6434782608695652,"References
234"
REFERENCES,0.6456521739130435,"Isaac E Lagaris, Aristidis Likas, and Dimitrios I Fotiadis. Artificial neural networks for solving ordinary and
235"
REFERENCES,0.6478260869565218,"partial differential equations. IEEE transactions on neural networks, 9(5):987–1000, 1998.
236"
REFERENCES,0.65,"Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learning
237"
REFERENCES,0.6521739130434783,"framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal
238"
REFERENCES,0.6543478260869565,"of Computational physics, 378:686–707, 2019.
239"
REFERENCES,0.6565217391304348,"Lu Lu, Xuhui Meng, Zhiping Mao, and George Em Karniadakis. Deepxde: A deep learning library for solving
240"
REFERENCES,0.658695652173913,"differential equations. SIAM review, 63(1):208–228, 2021a.
241"
REFERENCES,0.6608695652173913,"Ameya D Jagtap and George Em Karniadakis. Extended physics-informed neural networks (xpinns): A
242"
REFERENCES,0.6630434782608695,"generalized space-time domain decomposition based deep learning framework for nonlinear partial differential
243"
REFERENCES,0.6652173913043479,"equations. Communications in Computational Physics, 28(5):2002–2041, 2020.
244"
REFERENCES,0.6673913043478261,"Khemraj Shukla, Ameya D Jagtap, and George Em Karniadakis. Parallel physics-informed neural networks via
245"
REFERENCES,0.6695652173913044,"domain decomposition. Journal of Computational Physics, 447:110683, 2021.
246"
REFERENCES,0.6717391304347826,"Ben Moseley, Andrew Markham, and Tarje Nissen-Meyer. Finite basis physics-informed neural networks
247"
REFERENCES,0.6739130434782609,"(fbpinns): a scalable domain decomposition approach for solving differential equations.
Advances in
248"
REFERENCES,0.6760869565217391,"Computational Mathematics, 49(4):62, 2023.
249"
REFERENCES,0.6782608695652174,"Lu Lu, Raphaël Pestourie, Wenjie Yao, Zhicheng Wang, Francesc Verdugo, and Steven G. Johnson. Physics-
250"
REFERENCES,0.6804347826086956,"Informed Neural Networks with Hard Constraints for Inverse Design. SIAM Journal on Scientific Computing,
251"
REFERENCES,0.6826086956521739,"43(6):B1105–B1132, January 2021b. ISSN 1064-8275, 1095-7197. doi: 10.1137/21M1397908. URL
252"
REFERENCES,0.6847826086956522,"https://epubs.siam.org/doi/10.1137/21M1397908.
253"
REFERENCES,0.6869565217391305,"Songming Liu, Zhongkai Hao, Chengyang Ying, Hang Su, Jun Zhu, and Ze Cheng. A Unified Hard-Constraint
254"
REFERENCES,0.6891304347826087,"Framework for Solving Geometrically Complex PDEs. Advances in Neural Information Processing Systems,
255"
REFERENCES,0.691304347826087,"35:20287–20299, 2022.
256"
REFERENCES,0.6934782608695652,"Shaikhah Alkhadhr and Mohamed Almekkawy. Wave Equation Modeling via Physics-Informed Neural Networks:
257"
REFERENCES,0.6956521739130435,"Models of Soft and Hard Constraints for Initial and Boundary Conditions. Sensors, 23(5):2792, March 2023.
258"
REFERENCES,0.6978260869565217,"ISSN 1424-8220. doi: 10.3390/s23052792. URL https://www.mdpi.com/1424-8220/23/5/2792.
259"
REFERENCES,0.7,"Rüdiger Brecht, Dmytro R. Popovych, Alex Bihlo, and Roman O. Popovych.
Improving physics-
260"
REFERENCES,0.7021739130434783,"informed DeepONets with hard constraints, September 2023. URL http://arxiv.org/abs/2309.07899.
261"
REFERENCES,0.7043478260869566,"arXiv:2309.07899 [physics].
262"
REFERENCES,0.7065217391304348,"Chenxi Wu, Min Zhu, Qinyang Tan, Yadhu Kartha, and Lu Lu. A comprehensive study of non-adaptive and
263"
REFERENCES,0.7086956521739131,"residual-based adaptive sampling for physics-informed neural networks. Computer Methods in Applied
264"
REFERENCES,0.7108695652173913,"Mechanics and Engineering, 403:115671, 2023.
265"
REFERENCES,0.7130434782608696,"Arka Daw, Jie Bu, Sifan Wang, Paris Perdikaris, and Anuj Karpatne. Mitigating propagation failures in physics-
266"
REFERENCES,0.7152173913043478,"informed neural networks using retain-resample-release (r3) sampling. In International Conference on
267"
REFERENCES,0.717391304347826,"Machine Learning, pages 7264–7302. PMLR, 2023.
268"
REFERENCES,0.7195652173913043,"Zhiwei Gao, Liang Yan, and Tao Zhou. Failure-informed adaptive sampling for pinns. SIAM Journal on Scientific
269"
REFERENCES,0.7217391304347827,"Computing, 45(4):A1971–A1994, 2023a.
270"
REFERENCES,0.7239130434782609,"Zhiwei Gao, Tao Tang, Liang Yan, and Tao Zhou. Failure-informed adaptive sampling for pinns, part ii: combin-
271"
REFERENCES,0.7260869565217392,"ing with re-sampling and subset simulation. Communications on Applied Mathematics and Computation,
272"
REFERENCES,0.7282608695652174,"pages 1–22, 2023b.
273"
REFERENCES,0.7304347826086957,"Zijiang Yang, Zhongwei Qiu, and Dongmei Fu. Dmis: Dynamic mesh-based importance sampling for train-
274"
REFERENCES,0.7326086956521739,"ing physics-informed neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence,
275"
REFERENCES,0.7347826086956522,"volume 37, pages 5375–5383, 2023.
276"
REFERENCES,0.7369565217391304,"Zhengqi Zhang, Jing Li, and Bin Liu. Annealed adaptive importance sampling method in pinns for solving high
277"
REFERENCES,0.7391304347826086,"dimensional partial differential equations. arXiv preprint arXiv:2405.03433, 2024.
278"
REFERENCES,0.741304347826087,"Nithin Chalapathi, Yiheng Du, and Aditi Krishnapriyan. Scaling physics-informed hard constraints with
279"
REFERENCES,0.7434782608695653,"mixture-of-experts. arXiv preprint arXiv:2402.13412, 2024.
280"
REFERENCES,0.7456521739130435,"A
Phase diagrams of loss weights
281"
REFERENCES,0.7478260869565218,Figure 8: Phase diagrams
REFERENCES,0.75,"B
Seed
282"
REFERENCES,0.7521739130434782,"C
Training dynmaics
283"
REFERENCES,0.7543478260869565,"mono:
284"
REFERENCES,0.7565217391304347,"string: increase Npde to 104, we have converged solution(each 104 steps):
285"
REFERENCES,0.758695652173913,(a) standing waves
REFERENCES,0.7608695652173914,(b) string waves
REFERENCES,0.7630434782608696,(c) Gaussian traveling waves
REFERENCES,0.7652173913043478,"Figure 9: t2,
t2
t2+1,
2t2
t2+1, tanh2(t),

tanh(t)
tanh(1)
2"
REFERENCES,0.7673913043478261,"Figure 10: 0, 1000, 2000, and the last(converged)"
REFERENCES,0.7695652173913043,"NeurIPS Paper Checklist
286"
REFERENCES,0.7717391304347826,"The checklist is designed to encourage best practices for responsible machine learning research, addressing
287"
REFERENCES,0.7739130434782608,"issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The
288"
REFERENCES,0.7760869565217391,"papers not including the checklist will be desk rejected. The checklist should follow the references and follow
289"
REFERENCES,0.7782608695652173,"the (optional) supplemental material. The checklist does NOT count towards the page limit.
290"
REFERENCES,0.7804347826086957,"Please read the checklist guidelines carefully for information on how to answer these questions. For each
291"
REFERENCES,0.782608695652174,"question in the checklist:
292"
REFERENCES,0.7847826086956522,"• You should answer [Yes] , [No] , or [NA] .
293"
REFERENCES,0.7869565217391304,"• [NA] means either that the question is Not Applicable for that particular paper or the relevant
294"
REFERENCES,0.7891304347826087,"information is Not Available.
295"
REFERENCES,0.7913043478260869,"• Please provide a short (1–2 sentence) justification right after your answer (even for NA).
296"
REFERENCES,0.7934782608695652,"The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area
297"
REFERENCES,0.7956521739130434,"chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions)
298"
REFERENCES,0.7978260869565217,"with the final version of your paper, and its final version will be published with the paper.
299"
REFERENCES,0.8,"The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While
300"
REFERENCES,0.8021739130434783,"""[Yes] "" is generally preferable to ""[No] "", it is perfectly acceptable to answer ""[No] "" provided a proper
301"
REFERENCES,0.8043478260869565,"justification is given (e.g., ""error bars are not reported because it would be too computationally expensive"" or
302"
REFERENCES,0.8065217391304348,"""we were unable to find the license for the dataset we used""). In general, answering ""[No] "" or ""[NA] "" is not
303"
REFERENCES,0.808695652173913,"grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is
304"
REFERENCES,0.8108695652173913,"often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting
305"
REFERENCES,0.8130434782608695,"evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer
306"
REFERENCES,0.8152173913043478,"[Yes] to a question, in the justification please point to the section(s) where related material for the question can
307"
REFERENCES,0.8173913043478261,"be found.
308"
REFERENCES,0.8195652173913044,"Figure 11: 0, 10000, 20000, and the last(converged)"
REFERENCES,0.8217391304347826,"IMPORTANT, please:
309"
REFERENCES,0.8239130434782609,"• Delete this instruction block, but keep the section heading “NeurIPS paper checklist"",
310"
REFERENCES,0.8260869565217391,"• Keep the checklist subsection headings, questions/answers and guidelines below.
311"
REFERENCES,0.8282608695652174,"• Do not modify the questions and only use the provided macros for your answers.
312"
CLAIMS,0.8304347826086956,"1. Claims
313"
CLAIMS,0.8326086956521739,"Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s
314"
CLAIMS,0.8347826086956521,"contributions and scope?
315"
CLAIMS,0.8369565217391305,"Answer: [Yes]
316"
CLAIMS,0.8391304347826087,"Justification: NA
317"
LIMITATIONS,0.841304347826087,"2. Limitations
318"
LIMITATIONS,0.8434782608695652,"Question: Does the paper discuss the limitations of the work performed by the authors?
319"
LIMITATIONS,0.8456521739130435,"Answer: [Yes]
320"
LIMITATIONS,0.8478260869565217,"Justification: NA
321"
THEORY ASSUMPTIONS AND PROOFS,0.85,"3. Theory Assumptions and Proofs
322"
THEORY ASSUMPTIONS AND PROOFS,0.8521739130434782,"Question: For each theoretical result, does the paper provide the full set of assumptions and a complete
323"
THEORY ASSUMPTIONS AND PROOFS,0.8543478260869565,"(and correct) proof?
324"
THEORY ASSUMPTIONS AND PROOFS,0.8565217391304348,"Answer: [TODO]
325"
THEORY ASSUMPTIONS AND PROOFS,0.8586956521739131,"Justification: [TODO]
326"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8608695652173913,"4. Experimental Result Reproducibility
327"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8630434782608696,"Question: Does the paper fully disclose all the information needed to reproduce the main experimental
328"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8652173913043478,"results of the paper to the extent that it affects the main claims and/or conclusions of the paper
329"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8673913043478261,"(regardless of whether the code and data are provided or not)?
330"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8695652173913043,"Answer: [TODO]
331"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8717391304347826,"Justification: [TODO]
332"
OPEN ACCESS TO DATA AND CODE,0.8739130434782608,"5. Open access to data and code
333"
OPEN ACCESS TO DATA AND CODE,0.8760869565217392,"Question: Does the paper provide open access to the data and code, with sufficient instructions to
334"
OPEN ACCESS TO DATA AND CODE,0.8782608695652174,"faithfully reproduce the main experimental results, as described in supplemental material?
335"
OPEN ACCESS TO DATA AND CODE,0.8804347826086957,"Answer: [TODO]
336"
OPEN ACCESS TO DATA AND CODE,0.8826086956521739,"Justification: [TODO]
337"
OPEN ACCESS TO DATA AND CODE,0.8847826086956522,"6. Experimental Setting/Details
338"
OPEN ACCESS TO DATA AND CODE,0.8869565217391304,"Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters,
339"
OPEN ACCESS TO DATA AND CODE,0.8891304347826087,"how they were chosen, type of optimizer, etc.) necessary to understand the results?
340"
OPEN ACCESS TO DATA AND CODE,0.8913043478260869,"Answer: [TODO]
341"
OPEN ACCESS TO DATA AND CODE,0.8934782608695652,"Justification: [TODO]
342"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8956521739130435,"7. Experiment Statistical Significance
343"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8978260869565218,"Question: Does the paper report error bars suitably and correctly defined or other appropriate informa-
344"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9,"tion about the statistical significance of the experiments?
345"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9021739130434783,"Answer: [TODO]
346"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9043478260869565,"Justification: [TODO]
347"
EXPERIMENTS COMPUTE RESOURCES,0.9065217391304348,"8. Experiments Compute Resources
348"
EXPERIMENTS COMPUTE RESOURCES,0.908695652173913,"Question: For each experiment, does the paper provide sufficient information on the computer
349"
EXPERIMENTS COMPUTE RESOURCES,0.9108695652173913,"resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?
350"
EXPERIMENTS COMPUTE RESOURCES,0.9130434782608695,"Answer: [TODO]
351"
EXPERIMENTS COMPUTE RESOURCES,0.9152173913043479,"Justification: [TODO]
352"
CODE OF ETHICS,0.9173913043478261,"9. Code Of Ethics
353"
CODE OF ETHICS,0.9195652173913044,"Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code
354"
CODE OF ETHICS,0.9217391304347826,"of Ethics https://neurips.cc/public/EthicsGuidelines?
355"
CODE OF ETHICS,0.9239130434782609,"Answer: [TODO]
356"
CODE OF ETHICS,0.9260869565217391,"Justification: [TODO]
357"
BROADER IMPACTS,0.9282608695652174,"10. Broader Impacts
358"
BROADER IMPACTS,0.9304347826086956,"Question: Does the paper discuss both potential positive societal impacts and negative societal impacts
359"
BROADER IMPACTS,0.9326086956521739,"of the work performed?
360"
BROADER IMPACTS,0.9347826086956522,"Answer: [TODO]
361"
BROADER IMPACTS,0.9369565217391305,"Justification: [TODO]
362"
SAFEGUARDS,0.9391304347826087,"11. Safeguards
363"
SAFEGUARDS,0.941304347826087,"Question: Does the paper describe safeguards that have been put in place for responsible release of
364"
SAFEGUARDS,0.9434782608695652,"data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or
365"
SAFEGUARDS,0.9456521739130435,"scraped datasets)?
366"
SAFEGUARDS,0.9478260869565217,"Answer: [TODO]
367"
SAFEGUARDS,0.95,"Justification: [TODO]
368"
LICENSES FOR EXISTING ASSETS,0.9521739130434783,"12. Licenses for existing assets
369"
LICENSES FOR EXISTING ASSETS,0.9543478260869566,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper,
370"
LICENSES FOR EXISTING ASSETS,0.9565217391304348,"properly credited and are the license and terms of use explicitly mentioned and properly respected?
371"
LICENSES FOR EXISTING ASSETS,0.9586956521739131,"Answer: [Yes]
372"
LICENSES FOR EXISTING ASSETS,0.9608695652173913,"Justification: NA
373"
NEW ASSETS,0.9630434782608696,"13. New Assets
374"
NEW ASSETS,0.9652173913043478,"Question: Are new assets introduced in the paper well documented and is the documentation provided
375"
NEW ASSETS,0.967391304347826,"alongside the assets?
376"
NEW ASSETS,0.9695652173913043,"Answer: [No]
377"
NEW ASSETS,0.9717391304347827,"Justification: NA
378"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9739130434782609,"14. Crowdsourcing and Research with Human Subjects
379"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9760869565217392,"Question: For crowdsourcing experiments and research with human subjects, does the paper include
380"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9782608695652174,"the full text of instructions given to participants and screenshots, if applicable, as well as details about
381"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9804347826086957,"compensation (if any)?
382"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9826086956521739,"Answer: [No]
383"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9847826086956522,"Justification: NA.
384"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9869565217391304,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects
385"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9891304347826086,"Question: Does the paper describe potential risks incurred by study participants, whether such
386"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.991304347826087,"risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an
387"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9934782608695653,"equivalent approval/review based on the requirements of your country or institution) were obtained?
388"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9956521739130435,"Answer: [No]
389"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9978260869565218,"Justification: NA
390"
