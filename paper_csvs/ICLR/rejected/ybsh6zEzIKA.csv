Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003389830508474576,"We present a simple and yet effective interpolation-based regularization technique
to improve the generalization of Graph Neural Networks (GNNs). We leverage the
recent advances in Mixup regularizer for vision and text, where random sample
pairs and their labels are interpolated to create synthetic samples for training.
Unlike images or natural sentences, which embrace a grid or linear sequence
format, graphs have arbitrary structure and topology, which play a vital role on the
semantic information of a graph. Consequently, even simply deleting or adding
one edge from a graph can dramatically change its semantic meanings. This
makes interpolating graph inputs very challenging because mixing random graph
pairs may naturally create graphs with identical structure but with different labels,
causing the manifold intrusion issue. To cope with this obstacle, we propose the
ﬁrst input mixing schema for Mixup on graph. We theoretically prove that our
mixing strategy can recover the source graphs from the mixed graph, and guarantees
that the mixed graphs are manifold intrusion free. We also empirically show that
our method can effectively regularize the graph classiﬁcation learning, resulting in
superior predictive accuracy over popular graph augmentation baselines."
INTRODUCTION,0.006779661016949152,"1
INTRODUCTION"
INTRODUCTION,0.010169491525423728,"Graph Neural Networks (GNNs) (Kipf & Welling, 2017; Veliˇckovi´c et al., 2018; Xu et al., 2019)
have recently shown its profound successes in many challenging applications, including predicting
molecule properties for drug and material discovery (Gilmer et al., 2017; Wu et al., 2018), forecasting
protein functions for biological networks (Alvarez & Yan, 2012; Jiang et al., 2017), and estimating
circuit functionality in modern circuit design (Zhang et al., 2019). Nevertheless, like other successfully
deployed deep networks such as those for image classiﬁcation (Krizhevsky et al., 2012), speech
recognition (Graves et al., 2013) and machine translation (Sutskever et al., 2014; Bahdanau et al.,
2014), GNNs are also suffering from the data-hungry issue due to their high modeling freedom.
Consequently, researchers have been actively seeking effective regularization techniques for GNNs,
aiming to power their learning but to avoid over-smoothing (Li et al., 2018; Wu et al., 2019) and
over-ﬁtting (Goodfellow et al., 2016; Zhang et al., 2021) for better model generalization. To this end,
data augmentation schemes for regularizing GNNs mostly involve edge and node manipulation (e.g.,
deletion and addition) on a single graph (Rong et al., 2020; Zhou et al., 2020; You et al., 2020)."
INTRODUCTION,0.013559322033898305,original graph: class 1
INTRODUCTION,0.01694915254237288,original graph: class 2
INTRODUCTION,0.020338983050847456,original graph: class 3
INTRODUCTION,0.023728813559322035,drop node: class 1
INTRODUCTION,0.02711864406779661,drop edge: class 1
INTRODUCTION,0.030508474576271188,intrusion
INTRODUCTION,0.03389830508474576,intrusion
INTRODUCTION,0.03728813559322034,"Figure 1: Manifold intrusion caused by deleting a node
or an edge (gray dot lines) from the left graph. The two
synthetic graphs (middle) have the same structures as the
two original graphs on the right but with different labels."
INTRODUCTION,0.04067796610169491,"In this paper, we look into a very success-
ful pairwise data augmentation technique
for image recognition (Zhang et al., 2018a;
Verma et al., 2018; Guo et al., 2019a; Kim
et al., 2020) and natural text classiﬁca-
tion (Guo et al., 2019b; Guo, 2020; Jin-
dal et al., 2020), called Mixup. Mixup
was originally introduced by Zhang et al.
(2018a) as an interpolation-based regular-
izer for image classiﬁcation. It regularizes
the learning of deep classiﬁcation models
by training with synthetic samples, which
are created by linearly interpolating a pair
of randomly selected training samples as
well as their modeling targets. Neverthe-
less, unlike images or natural sentences, which embrace a grid or linear sequence format, graph"
INTRODUCTION,0.04406779661016949,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.04745762711864407,"data have arbitrary structure and topology, which play a critical role on the semantics of a graph,
and consequently even simply removing or adding one edge can dramatically change the semantic
meaning of a graph. As a result, mixing a graph pair is challenging and may naturally cause the
manifold intrusion issue in Mixup as identiﬁed by Guo et al. (2019a)."
INTRODUCTION,0.05084745762711865,"Manifold intrusion results from conﬂicts between the synthetic labels of the mixed-up samples or
between the synthetic labels and the labels of the original training data. For example, consider the
graph on the left of Figure 1, and its application of the popular graph perturbation action of node
and edge deletion (gray lines in the ﬁgure). In this case, the resulting two graphs in the middle will
have the same structure as the two on the right from the original training set, but with different labels
(indicated by the color bars under the graphs). Note: an illustration of manifold intrusion when
mixing two graphs is presented in A.5. As discussed in (Guo et al., 2019a), when such intrusions
occur, regularization using these synthetic graphs will contradict with the original training data. This
essentially induces a form of under-ﬁtting, resulting in the degradation of the model performance."
INTRODUCTION,0.05423728813559322,"𝑣=𝜆𝑣𝐴+ 1 −𝜆𝑣𝐵
 𝑒=𝜆𝑒𝐴+ 1 −𝜆𝑒𝐵"
INTRODUCTION,0.0576271186440678,𝑦=𝜆𝑦𝐴+ 1 −𝜆𝑦𝐵
INTRODUCTION,0.061016949152542375,"Graph A: (𝑣𝐴, 𝑒𝐴, 𝑦𝐴) 𝜆"
INTRODUCTION,0.06440677966101695,"Graph B: (𝑣B, 𝑒𝐵, 𝑦B)"
INTRODUCTION,0.06779661016949153,"mixed graph: ( 𝑣,  𝑒,  𝑦)"
INTRODUCTION,0.0711864406779661,"Figure 2:
The proposed mixing
schema. Top subﬁgure depicts the
source graph pair, and the bottom is
the resulting mixed graph for train-
ing with mixing ratio λ ∈[0, 1]."
INTRODUCTION,0.07457627118644068,"To address the aforementioned challenge, we propose the ﬁrst
input mixing schema for Mixup on graph learning, coined
ifMixup (intrusion-free Mixup). As illustrated in Figure 2,
ifMixup ﬁrst samples a random graph pair (top subﬁgure) from
the training data. For a given mixing ratio λ sampled from
a Beta distribution, ifMixup then creates a synthetic graph
(bottom subﬁgure), for each sample pair. ifMixup treats a
graph pair as two sets of ordered nodes in the same size (middle
subﬁgure, where the unﬁlled node depicts an added dummy
node). As a result, it can linearly interpolate the node features
and the edges of the input pair. The newly generated graphs,
which have a much larger number of graphs with changing
local neighborhood properties than the original training dataset,
are then used for training to regularize the GNNs learning.
Theoretically, we prove that our mixing strategy can recover
the source graphs from the mixed graph, and such invertibility
property guarantees that the mixed graphs are intrusion free.
That is, our method eliminates the possibility for the graphs
resulting from mixing to coincide with any other graph in the
training set or with a mixed graph from any other graph pairs.
Experimentally, we show, using eight benchmarking tasks
from different domains, that our strategy effectively regularizes
the graph classiﬁcation to improve its predictive performance,
outperforming popular graph augmentation approaches and
existing pair-wise graph mixing methods 1."
INTRODUCTION,0.07796610169491526,"Our key contributions are as follows: 1) To the best of our knowledge, we are the ﬁrst to propose
an input Mixup for graph classiﬁcation. 2) We prove that our Mixup schema is free of manifold
intrusion, which can signiﬁcantly degrade a Mixup-like model’s predictive accuracy. 3) We obtain
the SOTA performance among popular graph perturbation baselines."
RELATED WORK,0.08135593220338982,"2
RELATED WORK"
RELATED WORK,0.0847457627118644,"GNNs have been shown to be very effective for graph classiﬁcation in a variety of domains (Kipf
& Welling, 2017; Ying et al., 2018; Veliˇckovi´c et al., 2018; Klicpera et al., 2019; Xu et al., 2019;
Bianchi et al., 2020). One of the key challenges of these successes is to leverage strong regularization
techniques such as data augmentation to regularize those GNNs models with high modeling freedom.
Nonetheless, data augmentation is still a less touched area in graph data due to the arbitrary structure
and topology. Most of the graph augmentation strategies are for node classiﬁcation tasks, and
heavily focus on perturbing nodes and edges in one given graph (Hamilton et al., 2017; Zhang et al.,
2018b; Rong et al., 2020; Chen et al., 2020; Zhou et al., 2020; Qiu et al., 2020; You et al., 2020;
Wang et al.; Fu et al., 2020; Wang et al., 2020; Song et al., 2021; Zhao et al., 2021). For example,
DropEdge (Rong et al., 2020) randomly removes a set of edges of a given graph. GAUG (Zhao
et al., 2021) learns to perturb graph edges for node classiﬁcation. DropNode, representing node"
RELATED WORK,0.08813559322033898,1Our PyTorch code will be released upon the acceptance of the paper.
RELATED WORK,0.09152542372881356,Under review as a conference paper at ICLR 2022
RELATED WORK,0.09491525423728814,"sampling based methods (Hamilton et al., 2017; Chen et al., 2018; Huang et al., 2018), samples a
set of nodes from a given graph. Unlike these approaches, our proposed strategy leverages a pair of
graphs, instead of one graph, to augment the learning of graph level classiﬁcation."
RELATED WORK,0.09830508474576272,"Despite its great success in augmenting data for image recognition and text processing (Zhang et al.,
2018a; Verma et al., 2018; Guo et al., 2019a; Guo, 2020; Jindal et al., 2020; Kim et al., 2020), Mixup
has been less explored for graph learning. To the best of our knowledge, there are two methods that
apply Mixup to GNNs. GraphMix (Verma et al., 2019) leverages the idea of mixing on the embedding
layer, with an additional fully-connected network to share parameters with the GNNs, for graph node
classiﬁcation in semi-supervised learning. MixupGraph (Wang et al., 2021) also leverages a simple
way to avoid dealing with the arbitrary structure in the input space for mixing a graph pair, through
mixing the graph representation resulting from passing the graph through the GNNs. Our paper here
introduces the ﬁrst input mixing method for Mixup to augment training data for graph classiﬁcation.
Furthermore, our method guarantees that the mixed graphs are manifold intrusion free for Mixup."
INTRUSION-FREE GRAPH MIXING,0.1016949152542373,"3
INTRUSION-FREE GRAPH MIXING"
GRAPH CLASSIFICATION AND MIXUP INTERPOLATION,0.10508474576271186,"3.1
GRAPH CLASSIFICATION AND MIXUP INTERPOLATION
Graph Classiﬁcation In the graph classiﬁcation problem we consider, the input G is a graph labelled
with node features, or a “node-featured graph”. Speciﬁcally, assume that the graph has n nodes, each
identiﬁed with an integer ID in [n] := {1, 2, . . . , n}. The set of edges E of the graph is a collection of
unordered pairs (i, j)’s of node-IDs, as the current paper considers only undirected graphs (although
there is no difﬁculty to extend the setting to directed graphs). Associated with node i, there is a
feature vector v(i) of dimension d. We will use v to denote the collection of all feature vectors and
one may simply regards v as a matrix of dimension n × d. Thus, each input node featured graph G is
essentially speciﬁed via the pair (v, E). The output y is a class label in ﬁnite set Y := {1, 2, . . . , C},
which will be expressed as a (one-hot) probability vector over the label set Y. The classiﬁcation
problem is to ﬁnd a mapping that predicts the label y for a node-featured graph G. The training data
G of this learning task is a collection of such (G, y) pairs ."
GRAPH CLASSIFICATION AND MIXUP INTERPOLATION,0.10847457627118644,"Modern GNNs use the graph structure and node features to learn a distributed vector to represent
a graph. The learning follows the “message passing” mechanism for neighborhood aggregation. It
iteratively updates the embedding of a node hv by aggregating representations/embeddings of its
neighbors. The entire graph representation hG is then obtained through a READOUT function, which
aggregates embeddings from all nodes of the graph. Formally, representation hk
i of node i at the k-th
layer of a GNN is deﬁned as:"
GRAPH CLASSIFICATION AND MIXUP INTERPOLATION,0.11186440677966102,"hk
i = AGGREGATE(hk−1
i
, hk−1
j
|j ∈N(i), W k),
(1)"
GRAPH CLASSIFICATION AND MIXUP INTERPOLATION,0.1152542372881356,"where W k denotes the trainable weights at layer k, N(i) denotes the set of all nodes adjacent to i,
and AGGREGATE is an aggregation function implemented by the speciﬁc GNN model (popular ones
include Max, Mean, Summation pooling operations), and h0
i is typically initialized as the input node
feature v(i) . The graph representation hG aggregates node representations hv using the READOUT
graph pooling function:
hG = READOUT(hk
i |i ∈[n]).
(2)"
GRAPH CLASSIFICATION AND MIXUP INTERPOLATION,0.11864406779661017,"This graph representation is then mapped to label y using a standard classiﬁcation network (for
example, a softmax layer)."
GRAPH CLASSIFICATION AND MIXUP INTERPOLATION,0.12203389830508475,"Mixup Interpolation Mixup was introduced by (Zhang et al., 2018a) as an interpolation-based
regularizer for image classiﬁcation. It regularizes the learning of deep classiﬁcation models by
training with synthetic samples, which are created by linearly interpolating a pair of randomly
selected training samples as well as their modeling targets. In detail, let (xA, yA) and (xB, yB) be
two training instances, in which xA and xB refer to the input images and yA and yB refer to their
corresponding labels. For a randomly chosen such training pair, Mixup generates a synthetic sample
as follows.
ex = λxA + (1 −λ)xB,
(3)"
GRAPH CLASSIFICATION AND MIXUP INTERPOLATION,0.12542372881355932,"ey = λyA + (1 −λ)yB,
(4)"
GRAPH CLASSIFICATION AND MIXUP INTERPOLATION,0.1288135593220339,"where λ is a scalar mixing ratio, sampled from a Beta(α, β) distribution with hyper-parameters α and
β. Such synthetic instances (ex, ey)’s are then used for training."
GRAPH CLASSIFICATION AND MIXUP INTERPOLATION,0.13220338983050847,Under review as a conference paper at ICLR 2022
GRAPH CLASSIFICATION AND MIXUP INTERPOLATION,0.13559322033898305,"Motivated by the effectiveness of Mixup in regularizing image classiﬁcation models, we are naturally
motivated to design a similar “Mixup” scheme for graph data, in particular, the node-featured graphs,
as are the interest of this paper. When this is possible, we may use the synthetic instances ( eG, ey)’s to
learn the modele parameter θ by minimizing the loss L:"
GRAPH CLASSIFICATION AND MIXUP INTERPOLATION,0.13898305084745763,"min
θ
E(GA,yA)∼G,(GB,yB)∼G,λ∼Beta(α,β)⌈L( eG, ey)⌉.
(5)"
GRAPH CLASSIFICATION AND MIXUP INTERPOLATION,0.1423728813559322,"To mix (GA, yA) and (GB, yB), it is straight-forward to apply Equation (4) to obtain the mixed label
ey. The key question of investigation is how to mix GA and GB to obtain eG."
GRAPH CLASSIFICATION AND MIXUP INTERPOLATION,0.14576271186440679,"It is worth noting that, unlike images or natural sentences which embrace a rigid structure of spatial
coordinates or time axis, the underlying “coordinate system” of graph data may have different and
arbitrary topology across different instances. Consequently even simply deleting or adding one edge
can invalid the semantic meaning of a graph. One simple way to avoid dealing with the arbitrary
structure in the input space for mixing a graph pair is to mix their ﬁxed-size graph representation
that results from the READOUT function as depicted in Equation 2, namely mixing the two graphs
by eG = λhGA + (1 −λ)hGB, as proposed by Wang et al. (2021). We here propose to directly mix
the graph inputs with arbitrary sizes for Mixup. Our mixing strategy can recover the source graphs
from the mixed graph, and such invertibility guarantees that the mixed graphs are free of manifold
intrusion (Guo et al., 2019a), which can cause severe underﬁtting for Mixup learning."
INVERTIBLE GRAPH MIXING SCHEMA,0.14915254237288136,"3.2
INVERTIBLE GRAPH MIXING SCHEMA"
INVERTIBLE GRAPH MIXING SCHEMA,0.15254237288135594,"Now we propose a simple approach, ifMixup, for generating mixed node-featured graph eG from a
pair of such graphs GA and GB. In the nutshell, ifMixup simply adopts a different representation for
each node-featured graph."
INVERTIBLE GRAPH MIXING SCHEMA,0.15593220338983052,"Speciﬁcally, given a node featured graph G = (v, E), we represent E as a binary matrix e with n
rows and n columns, in which e(i, j) = 1 if (i, j) ∈E, and e(i, j) = 0 otherwise. Thus intead of
expressing G as (v, E) we express it as (v, e). The mixing of GA = (vA, eA) with GB = (vB, eB)
to obtain eG = (ev, ee), can simply be done as follow,"
INVERTIBLE GRAPH MIXING SCHEMA,0.15932203389830507,"ee = λeA + (1 −λ)eB.
(6)"
INVERTIBLE GRAPH MIXING SCHEMA,0.16271186440677965,"ev = λvA + (1 −λ)vB.
(7)"
INVERTIBLE GRAPH MIXING SCHEMA,0.16610169491525423,"In order for the above mixing rule to be well deﬁned, we need the two graphs to have the same
number of nodes. For this purpose we deﬁne n = max(nA, nB), where nA and nB are the number
of nodes in instances A and B respectively. If GA or GB has less than n nodes, we simply introduce
dummy node to the graph and make them disconnected from the existing nodes. The feature vectors
for the dummy nodes are set to the all-zero vector."
INVERTIBLE GRAPH MIXING SCHEMA,0.1694915254237288,"This mixing process is illustrated in Figure 2, where the top subﬁgure is the source graph pair, and
the middle depicts the added dummy node (i.e., the node with unﬁlled circle). The bottom is the
resulting mixed graph, where a mixed color indicates that the resulting node or edge is mixed by
existing nodes or edges from both source graphs, and the blue color denotes a node that is mixed
with a dummy node or an edge that is mixed with a zero-weighted edge."
INVERTIBLE GRAPH MIXING SCHEMA,0.17288135593220338,"It is worth noting that the resulting mixed graphs, through Equations 6 and 7, contain edges with
weights between [0, 1]. As a result, during training, this will require the GNN networks be able to
take the edge weights into account for message passing. Below we will discuss how the two popular
GNN networks, namely GCN (Kipf & Welling, 2017) and GIN (Xu et al., 2019), cope with the
weighted edges in graphs, namely how they implement Equation 1 to generate node representations."
INVERTIBLE GRAPH MIXING SCHEMA,0.17627118644067796,"GCN handles edge weights naturally by enabling adjacency matrix to have values between zero and
one (Kipf & Welling, 2017), instead of either zero or one, representing edge weights:"
INVERTIBLE GRAPH MIXING SCHEMA,0.17966101694915254,"hk
i = σ "
INVERTIBLE GRAPH MIXING SCHEMA,0.18305084745762712,"W k ·  
X"
INVERTIBLE GRAPH MIXING SCHEMA,0.1864406779661017,j∈N(i)∪{i}
INVERTIBLE GRAPH MIXING SCHEMA,0.18983050847457628,"e(i, j)
q"
INVERTIBLE GRAPH MIXING SCHEMA,0.19322033898305085,"ˆdj ˆdi
hk−1
j   "
INVERTIBLE GRAPH MIXING SCHEMA,0.19661016949152543,",
(8)"
INVERTIBLE GRAPH MIXING SCHEMA,0.2,Under review as a conference paper at ICLR 2022
INVERTIBLE GRAPH MIXING SCHEMA,0.2033898305084746,where ˆdi = 1 + P
INVERTIBLE GRAPH MIXING SCHEMA,0.20677966101694914,"j∈N(i) e(i, j); W k stands for the trainable weights at layer k; σ denotes the
non-linearity transformation, i.e. the ReLu function."
INVERTIBLE GRAPH MIXING SCHEMA,0.21016949152542372,"To enable GIN to handle soft edge weight, we replace the sum operation of the isomorphism operator
in GIN with a weighted sum calculation. That is, the GIN updates node representations as:"
INVERTIBLE GRAPH MIXING SCHEMA,0.2135593220338983,"hk
i = MLPk "
INVERTIBLE GRAPH MIXING SCHEMA,0.21694915254237288,"(1 + ϵk) · hk−1
i
+
X"
INVERTIBLE GRAPH MIXING SCHEMA,0.22033898305084745,"j∈N(i)
e(i, j) · hk−1
j "
INVERTIBLE GRAPH MIXING SCHEMA,0.22372881355932203,",
(9)"
INVERTIBLE GRAPH MIXING SCHEMA,0.2271186440677966,where ϵk is a learnable parameter.
INVERTIBILITY AND INTRUSION-FREENESS,0.2305084745762712,"3.3
INVERTIBILITY AND INTRUSION-FREENESS"
INVERTIBILITY AND INTRUSION-FREENESS,0.23389830508474577,"We now show that such a simple mixing scheme in fact makes the original two node-featured graphs
GA and GB recoverable from the mixed graph eG under a mild assumption and hence avoids manifold
intrusion."
INVERTIBILITY AND INTRUSION-FREENESS,0.23728813559322035,"To see this, we ﬁrst show that the graph topology and node features of the two original instances can
both be recovered from the mixed instance."
INVERTIBILITY AND INTRUSION-FREENESS,0.24067796610169492,"Lemma 1 (Edge Invertibility) Let ee be constructed using Equation 6 with λ ̸= 0.5. Consider
equation
se + (1 −s)e′ = ee
with unknowns s, e and e′, where s is a scalar and e and e′ are binary (i.e., {0, 1}-valued) n × n
matrices. There are exactly two solutions to this equation:

s = λ,
e = eA,
e′ = eB, or
s = 1 −λ,
e = eB,
e′ = eA
Note: the proof of this lemma is in Section A.1."
INVERTIBILITY AND INTRUSION-FREENESS,0.2440677966101695,"By this lemma, we see that if the mixing coefﬁcient λ ̸= 0.5, from the mixed edge representation ee,
we can always recover eA and eB (and hence EA and EB) and their corresponding weights used for
mixing. Note that if λ is drawn from a continuous distribution over (0, 1), the probability it takes
value 0.5 is zero. That is, the connectivity of original two graphs can be perfectly recovered from the
mixed edge representation ee."
INVERTIBILITY AND INTRUSION-FREENESS,0.24745762711864408,"Lemma 2 (Node Feature Invertibility) Suppose that the node feature vectors for all instances in
the task take values from a ﬁnite set V ⊂Rd and that V is linearly independent. Let ev be cosntructed
using Equation 7. Let V ∗= V ∪{0}, where 0 denotes the zero vector in Rd. Consider equation
ev = sv + (1 −s)v′"
INVERTIBILITY AND INTRUSION-FREENESS,0.25084745762711863,"in which n×d matricess v and v′ are unknowns with rows taking value in V ∗. For any ﬁxed s ∈(0, 1),
there is exactly one solution of (v, v′) for this equation."
INVERTIBILITY AND INTRUSION-FREENESS,0.2542372881355932,Note: the proof of this lemma is in Section A.2.
INVERTIBILITY AND INTRUSION-FREENESS,0.2576271186440678,"The node feature invertibility in this lemma requires that the node feature vectors are linear inde-
pendent. Note that if the feature dimension d is larger than the size |V | of V and for each vector in
V , its elements are drawn at random, then the linear independence property of V is satisﬁed with
high probability. Thus, if we have the modeling freedom in designing the dimension d of the feature
vectors (for example, in designing the embedding dimension), choosing a large d will make the linear
independence condition of V satisﬁed. There are however cases in which feature vectors are given
and d < |V |. In this case, we establish the following result which requires a much weaker condition."
INVERTIBILITY AND INTRUSION-FREENESS,0.26101694915254237,"To that end, suppose that the span SPAN(V ) of V is an m−dimensional space and m < d. Let B
be an m × d matrix whose rows form a basis of SPAN(V ). Any node feature matrix v can then be
expressed as
v = TB
for some matrix T with size n × m. In this case, we may identify a node-featured graph as (TB, e).
Let T denote the collection of all T matrices for all instances in the training set. That is,
T := {T : (TB, e) ∈G}"
INVERTIBILITY AND INTRUSION-FREENESS,0.26440677966101694,Under review as a conference paper at ICLR 2022
INVERTIBILITY AND INTRUSION-FREENESS,0.2677966101694915,"Lemma 3 (Node Feature Invertibility) Let ev be constructed using Equation 7 and suppose that T
is linearly independent. Consider equation"
INVERTIBILITY AND INTRUSION-FREENESS,0.2711864406779661,ev = sv + (1 −s)v′
INVERTIBILITY AND INTRUSION-FREENESS,0.2745762711864407,"in which n × d matrices v and v′ are unknowns where v = TB and v′ = T ′B for some T and T ′ in
T . For any ﬁxed s ∈(0, 1), there is exactly one solution of (v, v′) for this equation."
INVERTIBILITY AND INTRUSION-FREENESS,0.27796610169491526,Note: the proof of this lemma is in Section A.3.
INVERTIBILITY AND INTRUSION-FREENESS,0.28135593220338984,"Note that since each T has size n × m, usually a large number, it is much easier for the linear
independence condition of T to get satisﬁed in practice."
INVERTIBILITY AND INTRUSION-FREENESS,0.2847457627118644,"Theorem 1 (Intrusion-Freeness) Suppose that λ ̸= 1/2 and that either the condition for Lemma
2 is satisﬁed or the condition for Lemma 3 is satisﬁed. Then for any mixed node-featured graph
eG = (ev, ee) constructed using Equations 6 and 7, the two original node-feature graph GA and GB
can be uniquely recovered."
INVERTIBILITY AND INTRUSION-FREENESS,0.288135593220339,"Proof: Since λ ̸= 0.5, we can recover eA, eB and λ from ee. Given λ and either the condition for
Lemma 2 or the condition for Lemma 3, we can recover vA and vB from ev.
□"
INVERTIBILITY AND INTRUSION-FREENESS,0.29152542372881357,"By this theorem, there is no other pair (G′
A, G′
B) from the training set C that can be mixed into eG
using any λ. Thus, manifold intrusion does not occur under the mild condition of the theorem."
INVERTIBILITY AND INTRUSION-FREENESS,0.29491525423728815,"We note the intrusion-freeness of the proposed ifMixup scheme relies on the fact that input graphs do
not have soft (weighted) edges. We believe however that there is a potential to extend the scheme to
graphs with weighted edges. Promising directions include a combination of the following techniques.
First, instead of recovering edges and node features in tandem (as shown in the proof Theorem 1),
we may consider jointly recover both. Second we may quantize the edge weights to a set of discrete
values and consider a judiciously designed distribution for the mixing coefﬁcient λ. Third, we may
insist the ordering of nodes in a graph to reﬂect certain semantics or graph topology of instance,
whereby only allowing a restricted family of alignment schemes of the two graphs before mixing
them. It is our interest to investigate in these directions further."
EXPERIMENTS,0.2983050847457627,"4
EXPERIMENTS"
SETTINGS,0.3016949152542373,"4.1
SETTINGS
Datasets We evaluate our method with eight graph classiﬁcation tasks from the graph benchmark
datasets collection TUDatasets (Morris et al., 2020): PTC MR, NCI109, NCI1, and MUTAG for small
molecule classiﬁcation, ENZYMES and PROTEINS for protein categorization, and IMDB-M and
IMDB-B for social networks classiﬁcation. These datasets have been widely used for benchmarking
such as in Xu et al. (2019) and can be downloaded directly using PyTorch Geometric (Fey & Lenssen,
2019)’s build-in function online 2. The social networks datasets IMDB-M and IMDB-B have no
node features, and we use the node degrees as feature as in (Xu et al., 2019). Data statistics of these
datasets are shown in Table 3, including the number graphs, the average node number per graph, the
average edge number per graph, the number of node features, and the number of classes."
SETTINGS,0.3050847457627119,"Comparison Baselines We compare our method with four baselines: MixupGraph (Wang et al.,
2021), DropEdge (Rong et al., 2020), DropNode (Hamilton et al., 2017; Chen et al., 2018; Huang
et al., 2018), and Baseline. For the Baseline model, we use two popular GNNs network architectures:
GCN (Kipf & Welling, 2017) and GIN (Xu et al., 2019)."
SETTINGS,0.30847457627118646,"MixupGraph is the only available approach for applying Mixup on graph classiﬁcation. It leverages
a simple way to avoid dealing with the arbitrary structure for mixing a graph pair, through mixing
the entire graph representation resulting from the READOUT function of the GNNs. DropEdge and
DropNode are two widely used graph perturbation strategies for graph augmentation. DropEdge
randomly removes a set of existing edges from a given graph. DropNode randomly deletes a portion
of nodes and their connected edges."
SETTINGS,0.31186440677966104,"GCN and GIN are two popular GNN architectures and have been widely adopted for graph classi-
ﬁcation. GCN leverages spectral-based convolutional operation to learn spectral features of graph"
SETTINGS,0.3152542372881356,2https://chrsmrrs.github.io/datasets/docs/datasets
SETTINGS,0.31864406779661014,Under review as a conference paper at ICLR 2022
SETTINGS,0.3220338983050847,"through aggregation, beneﬁting from a normalized adjacency matrix, while GIN leverages the nodes’
spatial relations to aggregate neighborhood features, representing the state-of-the-art GNN network
architecture. We use their implementations in the PyTorch Geometric platform 3. Note that, for the
GCN, we use the GCN with Skip Connection (He et al., 2016) as that in (Li et al., 2019), This Skip
Connection powers the GCN to beneﬁt from deeper layers in GNN networks."
SETTINGS,0.3254237288135593,"Detail Settings We follow the evaluation protocol and hyperparameters search of GIN (Xu et al.,
2019) and DropEdge (Rong et al., 2020). We evaluate the models using 10-fold cross validation, and
report the mean and standard deviation of three runs on a NVidia V100 GPU with 32 GB memory.
Each fold is trained with 350 epochs with AdamW optimizer (Kingma & Ba, 2015), and the initial
learning rate is reduced by half every 50 epochs. The hyper-parameters we search for all models on
each dataset are as follows: (1) initial learning rate ∈{0.01, 0.0005}; (2) hidden unit of size ∈{64,
128}; (3) batch size ∈{32, 128}; (4) dropout ratio after the dense layer ∈{0, 0.5}; (5) DropNode and
DropEdge drop ratio ∈{20%, 40%}; (6) number of layers in GNNs ∈{5, 8}; (7) Beta distribution
for ifMixup, MixupGraph and Manifold Mixup ∈{Beta(1, 1), Beta(2, 2), Beta(20, 1)}. Following
GIN (Xu et al., 2019) and DropEdge (Rong et al., 2020), we report the case giving the best 10-fold
average cross-validation accuracy."
RESULTS OF USING GCN AS BASELINE,0.3288135593220339,"4.2
RESULTS OF USING GCN AS BASELINE
The accuracy obtained by the GCN (Kipf & Welling, 2017) baseline, ifMixup, MixupGraph, DropE-
dge, and DropNode with GCN on the eight datasets are presented in Table 1 (best results in Bold)."
RESULTS OF USING GCN AS BASELINE,0.33220338983050846,"GCN Baseline
ifMixup
MixupGraph
DropEdge
DropNode
Rel. Impr.
PTC MR
0.621±0.018
0.654±0.003
0.633±0.012
0.653±0.007
0.648±0.018
5.31%
NCI109
0.803±0.001
0.820±0.005
0.801±0.005
0.801±0.001
0.793±0.015
2.12%
NCI1
0.804±0.005
0.819±0.004
0.808±0.004
0.811±0.002
0.805±0.019
1.87%
MUTAG
0.850±0.011
0.879±0.003
0.860±0.006
0.855±0.008
0.829±0.006
3.41%
ENZYMES
0.541±0.001
0.570±0.014
0.551±0.016
0.566±0.006
0.532±0.006
5.36%
PROTEINS
0.742±0.003
0.753±0.008
0.742±0.003
0.750±0.003
0.748±0.001
1.48%
IMDB-M
0.515±0.002
0.523±0.004
0.513±0.003
0.514±0.00.
0.512±0.003
1.55%
IMDB-B
0.758±0.004
0.763±0.003
0.759±0.002
0.762±0.004
0.761±0.005
0.66%"
RESULTS OF USING GCN AS BASELINE,0.33559322033898303,"Table 1: Accuracy of the testing methods with GCN networks as baseline. We report mean accuracy
over 3 runs of 10-fold cross validation with standard deviations (denoted ±). The relative improvement
of ifMixup over the baseline GCN is provided in the last row of the table. Best results are in Bold."
RESULTS OF USING GCN AS BASELINE,0.3389830508474576,"Results in Table 1 show that ifMixup outperformed all the four comparison models against all the
eight datasets. For example, when comparing with the GCN baseline, ifMixup obtained a relative
accuracy improvement of 5.36%, 5.31%, and 3.41% on the ENZYMES, PTC MR, and MUTAG
datasets, respectively. When considering the comparison with the Mixup-like approach MixupGraph,
ifMixup also obtained superior accuracy on all the eight datasets. For example, ifMixup was able to
improve the accuracy over MixupGraph from 80.1%, 80.8%, 63.3%, and 51.3% to 82.0%, 81.9%,
65.4%, and 52.3% on the NCI109, NCI1, PTC MR and IMDB-M datasets, respectively."
RESULTS OF USING GCN AS BASELINE,0.3423728813559322,"Furthermore, as shown Table 1, unlike all the other augmentation methods (namely MixupGraph,
DropEdge, and DropNode), which can degrade the predictive performance of the baseline models,
our method never degraded the baseline models’ predictive accuracy."
RESULTS OF USING GCN AS BASELINE,0.34576271186440677,"4.2.1
MANIFOLD INTRUSION: MIXING RATIOS FOR GRAPH PAIRS"
RESULTS OF USING GCN AS BASELINE,0.34915254237288135,"0.0
0.2
0.4
0.6
0.8
1.0"
RESULTS OF USING GCN AS BASELINE,0.3525423728813559,"0.0
0.5
1.0
1.5
2.0"
RESULTS OF USING GCN AS BASELINE,0.3559322033898305,density
RESULTS OF USING GCN AS BASELINE,0.3593220338983051,"Beta(1, 1)
Beta(2,2)
Beta(5, 1)
Beta(10, 1)
Beta(20,1)"
RESULTS OF USING GCN AS BASELINE,0.36271186440677966,"Figure 3: probability density
function of Beta distribution."
RESULTS OF USING GCN AS BASELINE,0.36610169491525424,"In this ablation study, we evaluate the sensitivity of the graph mixing
ratio to the two Mixup-like approaches: ifMixup and MixupGraph.
We present the accuracy obtained by these two methods with Beta
distribution as Beta(1, 1), Beta(2, 2), Beta(5, 1), Beta(10, 1) and
Beta(20, 1) on the ﬁrst six datasets of Table 3. Results are pre-
sented in Figure 4. Note: the density functions of these ﬁve Beta
distributions are depicted in Figure 3."
RESULTS OF USING GCN AS BASELINE,0.3694915254237288,"Results in Figure 4 show that both MixupGraph and ifMixup ob-
tained superior results on the six testing datasets with Beta(20, 1).
Nevertheless, MixupGraph seemed to very sensitive to the mixing
ratio distribution. For example, when Beta distributions were (1, 1)"
RESULTS OF USING GCN AS BASELINE,0.3728813559322034,3https://github.com/pyg-team/pytorch geometric
RESULTS OF USING GCN AS BASELINE,0.376271186440678,Under review as a conference paper at ICLR 2022
RESULTS OF USING GCN AS BASELINE,0.37966101694915255,"and (2, 2) (ﬁrst two bars in Figure 4), MixupGraph signiﬁcantly degraded its accuracy on all the six
tasks (except for PTC MR). In contrast, ifMixup was robust to the ﬁve Beta distributions we tested. 0.62 0.63 0.64 0.65 0.66 0.67"
RESULTS OF USING GCN AS BASELINE,0.38305084745762713,"MixupGraph
ifMixup"
RESULTS OF USING GCN AS BASELINE,0.3864406779661017,"PTC_MR
beta(1,1)
beta(2,2)
beta(5, 1)"
RESULTS OF USING GCN AS BASELINE,0.3898305084745763,"beta(10, 1)
beta(20,1) 0.62 0.67 0.72 0.77 0.82 0.87"
RESULTS OF USING GCN AS BASELINE,0.39322033898305087,"MixupGraph
ifMixup"
RESULTS OF USING GCN AS BASELINE,0.39661016949152544,NCI109 0.62 0.67 0.72 0.77 0.82 0.87
RESULTS OF USING GCN AS BASELINE,0.4,"MixupGraph
ifMixup NCI1 0.79 0.81 0.83 0.85 0.87 0.89 0.91"
RESULTS OF USING GCN AS BASELINE,0.4033898305084746,"MixupGraph
ifMixup MUTAG 0.4 0.45 0.5 0.55 0.6"
RESULTS OF USING GCN AS BASELINE,0.4067796610169492,"MixupGraph
ifMixup"
RESULTS OF USING GCN AS BASELINE,0.4101694915254237,ENEYMES 0.67 0.69 0.71 0.73 0.75 0.77 0.79
RESULTS OF USING GCN AS BASELINE,0.4135593220338983,"MixupGraph
ifMixup"
RESULTS OF USING GCN AS BASELINE,0.41694915254237286,PROTEINS
RESULTS OF USING GCN AS BASELINE,0.42033898305084744,"Figure 4: MixupGraph and ifMixup with mixing ratios from
Beta(1, 1), Beta(2, 2), Beta(5, 1), Beta(10, 1) and Beta(20, 1)."
RESULTS OF USING GCN AS BASELINE,0.423728813559322,"We here conjecture that, the de-
crease of MixupGraph’s accu-
racy obtained with Beta(1, 1) and
Beta(2, 2) was due to the man-
ifold intrusion issue. The mix-
ing ratios sampled from Beta(1,
1) follow an uniform distribu-
tion between [0, 1], and those
sampled from Beta(2, 2) fol-
low a Bell-Shaped distribution
between [0, 1] (see Figure 3).
Those mixing ratios have a wide
range, and thus may aggravate
the creation of mixed embeddings with conﬂict labels for MixupGraph. On the other hand, ratios
being sampled from Beta(5, 1), Beta(10, 1) and Beta(20, 1) mostly fall in the range of [0.8, 1].
Such conservative mixing ratios may alleviate creating conﬂict training samples for MixupGraph.
Promisingly, due to the intrusion-free nature, ifMixup did not suffer from the manifold intrusion
problem, showing less sensitivity to the mixing ratios as in Figure 4."
RESULTS OF USING GCN AS BASELINE,0.4271186440677966,"4.2.2
OVER-SMOOTHING: IMPACT OF GNNS LAYERS"
RESULTS OF USING GCN AS BASELINE,0.43050847457627117,"In this ablation study, we also evaluate the accuracy obtained by GCN, ifMixup and MixupGraph on
the ﬁrst six datasets of Table 3, when varying the number of layers of the GCN networks."
RESULTS OF USING GCN AS BASELINE,0.43389830508474575,"The results for all the six datasets are depicted in Figure 5. Results in this ﬁgure show that when
increasing the GCN networks from 5 layers (blue bars) to 8 layers (red bars), both GCN and
MixupGraph seemed to degrade its performance on all the six datasets. For example, for the NCI109
and NCI1 datasets, MixupGraph resulted in about 10% of accuracy drop when increasing the number
of layers in GCNs (with Skip Connection) from 5 to 8. On the contrary, ifMixup was able to increase
the accuracy on all the six test datasets. 0.58 0.6 0.62 0.64 0.66 0.68"
RESULTS OF USING GCN AS BASELINE,0.43728813559322033,"Baseline
ifMixup
MixupGraph"
RESULTS OF USING GCN AS BASELINE,0.4406779661016949,"PTC_MR
Layer5
Layer8 0.57 0.62 0.67 0.72 0.77 0.82 0.87"
RESULTS OF USING GCN AS BASELINE,0.4440677966101695,"Baseline
ifMixup
MixupGraph"
RESULTS OF USING GCN AS BASELINE,0.44745762711864406,"NCI109
Layer5
Layer8 0.73 0.75 0.77 0.79 0.81 0.83"
RESULTS OF USING GCN AS BASELINE,0.45084745762711864,"Baseline
ifMixup
MixupGraph"
RESULTS OF USING GCN AS BASELINE,0.4542372881355932,"NCI1
Layer5
Layer8 0.82 0.83 0.84 0.85 0.86 0.87 0.88 0.89"
RESULTS OF USING GCN AS BASELINE,0.4576271186440678,"Baseline
ifMixup
MixupGraph"
RESULTS OF USING GCN AS BASELINE,0.4610169491525424,"MUTAG
Layer5
Layer8 0.5 0.52 0.54 0.56 0.58"
RESULTS OF USING GCN AS BASELINE,0.46440677966101696,"Baseline
ifMixup
MixupGraph"
RESULTS OF USING GCN AS BASELINE,0.46779661016949153,"ENZYMES
Layer5
Layer8 0.71 0.72 0.73 0.74 0.75 0.76"
RESULTS OF USING GCN AS BASELINE,0.4711864406779661,"Baseline
ifMixup
MixupGraph"
RESULTS OF USING GCN AS BASELINE,0.4745762711864407,"PROTEIN
Layer5
Layer8"
RESULTS OF USING GCN AS BASELINE,0.47796610169491527,"Figure 5: Varying the depth for GCN, ifMixup, and MixupGraph."
RESULTS OF USING GCN AS BASELINE,0.48135593220338985,"Such decrease of accuracy ob-
tained by GCN and MixupGraph
may due to the over-smoothing
problem (Li et al., 2018; Wu et al.,
2019). That is, with deeper net-
works architectures (i.e., more lay-
ers), the representations of all
nodes of a graph may converge
to a subspace that makes these
representations unrelated to the in-
put. This negative effect is mainly
caused by the fact that the mes-
sage passing between adjacent nodes is conducted along edge paths in GCNs. That is, each graph
convolutional layer in the GCNs keeps pushing the representations of adjacent nodes to blend with
each other, based on the ﬁxed connections between nodes. Through generating new adjacency
matrices for each training step by randomly deleting a portion of edges of the input graphs, DropE-
dge (Rong et al., 2020) has show its effectiveness on mitigating over-smoothing. Similar to DropEdge,
ifMixup also creates graphs with changing node connections as inputs to the GCNs in each training
step. That is, each mixed graph in ifMixup will provide a new adjacency matrix to the networks,
making node connections very random and diverse as that in DropEdge. These changing local
neighborhood properties in the mixed graphs thus help ifMixup alleviate the over-smoothing problem
when GCNs goes deeper."
RESULTS OF USING GCN AS BASELINE,0.4847457627118644,"4.2.3
OVER-FITTING: REGULARIZATION EFFECT"
RESULTS OF USING GCN AS BASELINE,0.488135593220339,"In this ablation study, we evaluate the over-ﬁtting effect of our method. We plot the training loss and
validation accuracy of ifMixup, GCN, and MixupGraph methods across the 350 training epochs on
both the NCI109 and NCI1 datasets in Figure 6."
RESULTS OF USING GCN AS BASELINE,0.4915254237288136,Under review as a conference paper at ICLR 2022 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
RESULTS OF USING GCN AS BASELINE,0.49491525423728816,"0
50
100
150
200
250
300"
RESULTS OF USING GCN AS BASELINE,0.49830508474576274,"IfMixup
GCN
MixupGraph Epoch Loss"
RESULTS OF USING GCN AS BASELINE,0.5016949152542373,NCI109 0.65 0.70 0.75 0.80 0.85
RESULTS OF USING GCN AS BASELINE,0.5050847457627119,"1
51
101
151
201
251
301"
RESULTS OF USING GCN AS BASELINE,0.5084745762711864,"IfMixup
GCN
MixupGraph Epoch"
RESULTS OF USING GCN AS BASELINE,0.511864406779661,Accuracy
RESULTS OF USING GCN AS BASELINE,0.5152542372881356,NCI109 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
RESULTS OF USING GCN AS BASELINE,0.5186440677966102,"1
51
101
151
201
251
301"
RESULTS OF USING GCN AS BASELINE,0.5220338983050847,"IfMixup
GCN
MixupGraph Epoch Loss NCI1 0.65 0.70 0.75 0.80 0.85"
RESULTS OF USING GCN AS BASELINE,0.5254237288135594,"1
51
101
151
201
251
301"
RESULTS OF USING GCN AS BASELINE,0.5288135593220339,"IfMixup
GCN
MixupGraph Epoch NCI1"
RESULTS OF USING GCN AS BASELINE,0.5322033898305085,Accuracy
RESULTS OF USING GCN AS BASELINE,0.535593220338983,"Figure 6: Training loss (left) and validation accuracy
(right)."
RESULTS OF USING GCN AS BASELINE,0.5389830508474577,"Figure 6 shows that the training loss of
ifMixup (left subﬁgures) maintained a rel-
atively high level, when compared to GCN
and MixupGraph. For GCN, the training
loss reduced to near zero after 300 training
epochs. Compared to GCN, MixupGraph
and ifMixup remained higher training loss,
although the loss of MixupGraph kept de-
creasing after 300 epochs."
RESULTS OF USING GCN AS BASELINE,0.5423728813559322,"The relative high loss here allows the mod-
els to keep tuning. Such high loss is due
to the much larger space of the synthetic
graphs as random and diverse inputs to the
networks, thus preventing the model from
being over-ﬁtted by limited number of graph samples in the original training set. As a result, as
shown in the right subﬁgure, even training for a long time, the ifMixup models were not overﬁtting."
RESULTS OF USING GCN AS BASELINE,0.5457627118644067,Note: a 2D visualization of the learned representations of the training graphs is presented in A.6.
RESULTS OF USING GIN AS BASELINE,0.5491525423728814,"4.3
RESULTS OF USING GIN AS BASELINE"
RESULTS OF USING GIN AS BASELINE,0.5525423728813559,"We also evaluate our method using the GIN (Xu et al., 2019) network architecture. The accuracy
obtained by the GIN baseline, ifMixup, MixupGraph, DropEdge, and DropNode using GIN as
baseline on the eight test datasets are presented in Table 2, where best results are in Bold."
RESULTS OF USING GIN AS BASELINE,0.5559322033898305,"GIN Baseline
ifMixup
MixupGraph
DropEdge
DropNode
Rel. Impr.
PTC MR
0.644±0.007
0.672±0.005
0.631±0.005
0.669±0.003
0.663±0.006
4.35%
NCI109
0.820±0.002
0.837±0.004
0.822±0.008
0.792±0.002
0.796±0.002
2.07%
NCI1
0.818±0.009
0.839±0.004
0.822±0.001
0.791±0.005
0.785±0.003
2.57%
MUTAG
0.886±0.011
0.890±0.006
0.884±0.009
0.854±0.003
0.859±0.003
0.45%
ENZYMES
0.526±0.014
0.543±0.005
0.521±0.007
0.488±0.015
0.528±0.002
3.23%
PROTEINS
0.745±0.003
0.754±0.002
0.744±0.005
0.749±0.002
0.751±0.005
1.21%
IMDB-M
0.519±0.001
0.532±0.001
0.518±0.004
0.517±0.003
0.516±0.002
2.50%
IMDB-B
0.762±0.004
0.765±0.005
0.761±0.001
0.762±0.005
0.764±0.006
0.39%"
RESULTS OF USING GIN AS BASELINE,0.559322033898305,"Table 2: Accuracy of the testing methods with GIN networks as baseline. We report mean scores over
3 runs of 10-fold cross validation with standard deviations (denoted ±). The relative improvement of
ifMixup over the baseline GIN is provided in the last row of the table. Best results are in Bold."
RESULTS OF USING GIN AS BASELINE,0.5627118644067797,"Results in Table 2 show that, similar to the GCN case, the ifMixup with GIN as baseline outperformed
all the four comparison models against all the eight datasets. For example, when comparing with GIN,
ifMixup obtained a relative accuracy improvement of 4.35%, 3.23%, and 2.57% on the PTC MR,
ENZYMES, and NCI1 datasets, respectively. When comparing with the Mixup-like approach
MixupGraph, ifMixup also obtained higher accuracy on all the eight datasets. For example, ifMixup
was able to improve the accuracy over MixupGraph from 82.2%, 82.2%, 63.1%, and 51.8% to 83.7%,
83.9%, 67.2%, and 53.2% on the NCI109, NCI1, PTC MR, and IMDB-M datasets, respectively."
CONCLUSIONS AND FUTURE WORK,0.5661016949152542,"5
CONCLUSIONS AND FUTURE WORK"
CONCLUSIONS AND FUTURE WORK,0.5694915254237288,"We proposed the ﬁrst input mixing schema for Mixup on graph classiﬁcation. We proved that our
mixing strategy can recover the source graphs from the mixed graph, and such invertibility in turn
guarantees that the mixed graphs are free of manifold intrusion, a form of under-ﬁtting which can
signiﬁcantly degrade a Mixup-like model’s predictive accuracy. We showed, using eight benchmark
graph classiﬁcation tasks from different domains, that our strategy obtained superior predictive
accuracy over popular graph augmentation approaches and existing pair-wise graph mixing methods."
CONCLUSIONS AND FUTURE WORK,0.5728813559322034,"In the future, we will extend our method for node classiﬁcation in graph. Also, we will study the
potential of our graph mixing schema on semantic-persevering graph mutation."
CONCLUSIONS AND FUTURE WORK,0.576271186440678,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.5796610169491525,"6
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.5830508474576271,"Our results are easily reproducible by following the experimental settings in Section 4.1 since our
implementations used the PyTorch Geometric platform and the standard datasets from TUDatasets."
REPRODUCIBILITY STATEMENT,0.5864406779661017,"Also, we will make our PyTorch code publicly available upon the acceptance of the paper."
REFERENCES,0.5898305084745763,REFERENCES
REFERENCES,0.5932203389830508,"Marco A. Alvarez and Changhui Yan. A new protein graph model for function prediction. Computa-
tional Biology and Chemistry, 37:6–10, 2012."
REFERENCES,0.5966101694915255,"Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. CoRR, abs/1409.0473, 2014."
REFERENCES,0.6,"Filippo Maria Bianchi, Daniele Grattarola, and Cesare Alippi. Spectral clustering with graph neural
networks for graph pooling. In Proceedings of the 37th international conference on Machine
learning, pp. 2729–2738. ACM, 2020."
REFERENCES,0.6033898305084746,"Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and relieving the over-
smoothing problem for graph neural networks from the topological view. In The Thirty-Fourth
AAAI Conference on Artiﬁcial Intelligence, pp. 3438–3445. AAAI Press, 2020."
REFERENCES,0.6067796610169491,"Jie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: Fast learning with graph convolutional networks via
importance sampling. CoRR, 2018."
REFERENCES,0.6101694915254238,"Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In
ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019."
REFERENCES,0.6135593220338983,"Kun Fu, Tingyun Mao, Yang Wang, Daoyu Lin, Y. Zhang, Junjian Zhan, Xi an Sun, and F. Li.
Ts-extractor: large graph exploration via subgraph extraction based on topological and semantic
information. Journal of Visualization, pp. 1 – 18, 2020."
REFERENCES,0.6169491525423729,"Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural
message passing for quantum chemistry. In International Conference on Machine Learning, 2017."
REFERENCES,0.6203389830508474,"Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016."
REFERENCES,0.6237288135593221,"Alex Graves, Abdel-rahman Mohamed, and Geoffrey E. Hinton. Speech recognition with deep
recurrent neural networks. CoRR, abs/1303.5778, 2013."
REFERENCES,0.6271186440677966,"Hongyu Guo. Nonlinear mixup: Out-of-manifold data augmentation for text classiﬁcation. Proceed-
ings of the AAAI Conference on Artiﬁcial Intelligence, pp. 4044–4051, Apr. 2020."
REFERENCES,0.6305084745762712,"Hongyu Guo, Yongyi Mao, and Richong Zhang. Mixup as locally linear out-of-manifold regulariza-
tion. In AAAI2019, 2019a."
REFERENCES,0.6338983050847458,"Hongyu Guo, Yongyi Mao, and Richong Zhang. Augmenting data with mixup for sentence classiﬁca-
tion: An empirical study. CoRR, abs/1905.08941, 2019b."
REFERENCES,0.6372881355932203,"William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs.
NIPS’17, 2017."
REFERENCES,0.6406779661016949,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
2016."
REFERENCES,0.6440677966101694,"Wenbing Huang, Tong Zhang, Yu Rong, and Junzhou Huang. Adaptive sampling towards fast graph
representation learning. 2018."
REFERENCES,0.6474576271186441,"Biaobin Jiang, Kyle Kloster, David F. Gleich, and Michael Gribskov. Aptrank: an adaptive pagerank
model for protein function prediction on bi-relational graphs. Bioinformatics, 33(12):1829–1836,
2017."
REFERENCES,0.6508474576271186,Under review as a conference paper at ICLR 2022
REFERENCES,0.6542372881355932,"Amit Jindal, Arijit Ghosh Chowdhury, Aniket Didolkar, Di Jin, Ramit Sawhney, and Rajiv Ratn
Shah. Augmenting NLP models using latent feature interpolations. In Proceedings of the 28th
International Conference on Computational Linguistics, December 2020."
REFERENCES,0.6576271186440678,"Jang-Hyun Kim, Wonho Choo, and Hyun Oh Song. Puzzle mix: Exploiting saliency and local
statistics for optimal mixup. In International Conference on Machine Learning (ICML), 2020."
REFERENCES,0.6610169491525424,"Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio
and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015,
San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015."
REFERENCES,0.6644067796610169,"Thomas N. Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks.
In International Conference on Learning Representations (ICLR), 2017."
REFERENCES,0.6677966101694915,"Johannes Klicpera, Stefan Weißenberger, and Stephan G¨unnemann. Diffusion improves graph
learning. In Conference on Neural Information Processing Systems (NeurIPS), 2019."
REFERENCES,0.6711864406779661,"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep convolu-
tional neural networks. In NIPS, pp. 1097–1105, 2012."
REFERENCES,0.6745762711864407,"Guohao Li, Matthias M¨uller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as
cnns? In The IEEE International Conference on Computer Vision (ICCV), 2019."
REFERENCES,0.6779661016949152,"Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for
semi-supervised learning. In Sheila A. McIlraith and Kilian Q. Weinberger (eds.), AAAI, 2018."
REFERENCES,0.6813559322033899,"Christopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion
Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. In ICML
2020 Workshop on Graph Representation Learning and Beyond (GRL+ 2020), 2020. URL
www.graphlearning.io."
REFERENCES,0.6847457627118644,"Jiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding, Kuansan Wang,
and Jie Tang. GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training, pp.
1150–1160. 2020."
REFERENCES,0.688135593220339,"Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convo-
lutional networks on node classiﬁcation. In International Conference on Learning Representations,
2020."
REFERENCES,0.6915254237288135,"Rui Song, Fausto Giunchiglia, Ke Zhao, and Hao Xu. Topological regularization for graph neural
networks augmentation. CoRR, abs/2104.02478, 2021."
REFERENCES,0.6949152542372882,"Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks.
CoRR, abs/1409.3215, 2014."
REFERENCES,0.6983050847457627,"Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine
Learning Research, 9(86):2579–2605, 2008."
REFERENCES,0.7016949152542373,"Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua
Bengio. Graph Attention Networks. International Conference on Learning Representations, 2018."
REFERENCES,0.7050847457627119,"Vikas Verma, Alex Lamb, Christopher Beckham, Aaron Courville, Ioannis Mitliagkas, and Yoshua
Bengio. Manifold mixup: Encouraging meaningful on-manifold interpolation as a regularizer.
CoRR, 2018."
REFERENCES,0.7084745762711865,"Vikas Verma, Meng Qu, Alex Lamb, Yoshua Bengio, Juho Kannala, and Jian Tang. Graphmix:
Regularized training of graph neural networks for semi-supervised learning. CoRR, abs/1909.11715,
2019."
REFERENCES,0.711864406779661,"Yiwei Wang, Wei Wang, Yuxuan Liang, Yujun Cai, Juncheng Liu, and Bryan Hooi. Nodeaug:
Semi-supervised node classiﬁcation with data augmentation. In Rajesh Gupta, Yan Liu, Jiliang
Tang, and B. Aditya Prakash (eds.), KDD ’20, pp. 207–217."
REFERENCES,0.7152542372881356,"Yiwei Wang, Wei Wang, Yuxuan Liang, Yujun Cai, and Bryan Hooi. Graphcrop: Subgraph cropping
for graph classiﬁcation. CoRR, abs/2009.10564, 2020."
REFERENCES,0.7186440677966102,Under review as a conference paper at ICLR 2022
REFERENCES,0.7220338983050848,"Yiwei Wang, Wei Wang, Yuxuan Liang, Yujun Cai, and Bryan Hooi. Mixup for node and graph
classiﬁcation. In Proceedings of the Web Conference 2021, pp. 3663–3674, 2021."
REFERENCES,0.7254237288135593,"Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S
Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning.
Chemical science, 9(2):513–530, 2018."
REFERENCES,0.7288135593220338,"Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S. Yu. A
comprehensive survey on graph neural networks. CoRR, 2019."
REFERENCES,0.7322033898305085,"Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? In International Conference on Learning Representations, 2019."
REFERENCES,0.735593220338983,"Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, William L. Hamilton, and Jure Leskovec.
Hierarchical graph representation learning with differentiable pooling. In Advances in Neural
Information Processing Systems 31: Annual Conference on Neural Information Processing Systems
2018, NeurIPS 2018, December 3-8, 2018, Montr´eal, Canada, pp. 4805–4815, 2018."
REFERENCES,0.7389830508474576,"Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph
contrastive learning with augmentations. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan,
and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 5812–5823.
Curran Associates, Inc., 2020."
REFERENCES,0.7423728813559322,"Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning (still) requires rethinking generalization. 2021."
REFERENCES,0.7457627118644068,"Guo Zhang, Hao He, and Dina Katabi. Circuit-gnn: Graph neural networks for distributed circuit
design. In International Conference on Machine Learning, 2019."
REFERENCES,0.7491525423728813,"Hongyi Zhang, Moustapha Ciss´e, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical
risk minimization. In ICLR2018, 2018a."
REFERENCES,0.752542372881356,"Yingxue Zhang, Soumyasundar Pal, Mark Coates, and Deniz ¨Ustebay. Bayesian graph convolutional
neural networks for semi-supervised classiﬁcation. 2018b."
REFERENCES,0.7559322033898305,"Tong Zhao, Yozen Liu, Leonardo Neves, Oliver Woodford, Meng Jiang, and Neil Shah. Data
augmentation for graph neural networks. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, volume 35, pp. 11015–11023, 2021."
REFERENCES,0.7593220338983051,"Jiajun Zhou, Jie Shen, and Qi Xuan. Data augmentation for graph classiﬁcation. In Proceedings of the
29th ACM International Conference on Information and Knowledge Management, pp. 2341–2344,
2020."
REFERENCES,0.7627118644067796,Under review as a conference paper at ICLR 2022
REFERENCES,0.7661016949152543,"A
APPENDIX"
REFERENCES,0.7694915254237288,"A.1
PROOF OF LEMMA 1"
REFERENCES,0.7728813559322034,"Proof: First note that the values in matrix ee can only take values in {0, λ, 1 −λ, 1}."
REFERENCES,0.7762711864406779,The set [n] × [n] of all node pairs can be partitioned into four sets:
REFERENCES,0.7796610169491526,"M00
:=
{(i, j) ∈[n] × [n] : eA(i, j) = 0, eB(i, j) = 0}
M01
:=
{(i, j) ∈[n] × [n] : eA(i, j) = 0, eB(i, j) = 1}
M10
:=
{(i, j) ∈[n] × [n] : eA(i, j) = 1, eB(i, j) = 0}
M11
:=
{(i, j) ∈[n] × [n] : eA(i, j) = 1, eB(i, j) = 1}"
REFERENCES,0.7830508474576271,It is clear that
REFERENCES,0.7864406779661017,"ee(i, j) = 

 
"
REFERENCES,0.7898305084745763,"0,
if (i, j) ∈M00
1 −λ,
if (i, j) ∈M01
λ,
if (i, j) ∈M10
1,
if (i, j) ∈M11
Let e, e′ and s be the solution of the equation in the lemma. On M00∪M11, we must have e = e′ = ee.
We only need to determine e and e′ on M01 and M10. When λ ̸= 0.5, we must have either"
REFERENCES,0.7932203389830509,"s = λ, e(i, j) =

1,
(i, j) ∈M10
0,
(i, j) ∈M01
and e′(i, j) =

0,
(i, j) ∈M10
1,
(i, j) ∈M01
or"
REFERENCES,0.7966101694915254,"s = 1 −λ, e(i, j) =

0,
(i, j) ∈M10
1,
(i, j) ∈M01
and e′(i, j) =

1,
(i, j) ∈M10
0,
(i, j) ∈M01
Comparing such solutions with eA and eB, we prove the lemma.
□"
REFERENCES,0.8,"A.2
PROOF OF LEMMA 2"
REFERENCES,0.8033898305084746,"Proof: We will prove the lemma by showing that for any i ∈[n], based on v(i), we can uniquely
recover v(i) and v′(i)."
REFERENCES,0.8067796610169492,Case 1: ev(i) = 0. It is obvious v(i) = v′(i) = 0.
REFERENCES,0.8101694915254237,"Case 2: ev(i) /∈V but ev = cu for some u ∈V and some scalar c. In this case, c must be either s or
1 −s. If c = s, then v(i) = u, v′(i) = 0. If c = 1 −s, then v(i) = 0, v′(i) = u."
REFERENCES,0.8135593220338984,"Case 3: ev(i) /∈V and ev ̸= cu for any u ∈V and any scalar c ̸= 0. For any two u, u′ ∈V , let
SPAN(u, u′) denote the vector space spanned u and u′. Since V is a linearly independent set, it is
clear every choice of (u, u′) gives a different space SPAN(u, u′), and ev(i) must live in one and only
one such space. After identifying this space, we can identify (u, u′). With the knowledge of s, we
can precisely correspond u and u′ with v(i) and v′(i) since either u = v(i) and u′ = v′(i) are true,
or u = v′(i) and u = v(i) are true, but both can not be true at the same time."
REFERENCES,0.8169491525423729,"Thus we have enumerated all possible cases, and in each case, there is a unique solution to the
equation of interest.
□"
REFERENCES,0.8203389830508474,"A.3
PROOF OF LEMMA 3"
REFERENCES,0.823728813559322,"Proof: Since the rows of B are linearly independent, there is a unique eT for which"
REFERENCES,0.8271186440677966,ev = eTB.
REFERENCES,0.8305084745762712,"We can recover eT by projecting the rows of ev on the basis B. It is clear eT = sT +(1−s)T ′. Then we
only need to recover T and T ′ from eT. But since T is linearly independent and T, T ′ ∈T , following
a similar argument as in Case 3 of the proof for Lemma 2, we see that T and T ′ can be uniquely
recovered.
□."
REFERENCES,0.8338983050847457,Under review as a conference paper at ICLR 2022
REFERENCES,0.8372881355932204,"A.4
STATISTICS OF BENCHMARK DATASETS"
REFERENCES,0.8406779661016949,Table 3 details the statistics of the 8 benchmark datasets used in the paper.
REFERENCES,0.8440677966101695,"Name
graphs
nodes
edges
features
classes
PTC MR
334
14.3
29.4
18
2
NCI109
4127
29.7
64.3
38
2
NCI1
4110
29.9
64.6
37
2
MUTAG
188
17.9
39.6
7
2
ENZYMES
600
32.6
124.3
3
6
PROTEINS
1113
39.1
145.6
3
2
IMDB-M
1500
13.0
65.9
N/A
3
IMDB-B
1000
19.8
96.5
N/A
2"
REFERENCES,0.847457627118644,Table 3: Statistics of the graph classiﬁcation benchmark datasets.
REFERENCES,0.8508474576271187,"A.5
ILLUSTRATION OF MANIFOLD INTRUSION FROM MIXING GRAPH PAIRS"
REFERENCES,0.8542372881355932,original graph: 1.0 class 3
REFERENCES,0.8576271186440678,original graph: 1.0 class 1
REFERENCES,0.8610169491525423,"0.5 class1, 0.5 class 2"
REFERENCES,0.864406779661017,intrusion
REFERENCES,0.8677966101694915,original graph : 1.0 class 2
REFERENCES,0.8711864406779661,"Figure 7: Manifold intrusion caused by connecting a graph pair. The synthetic graph in the middle is
created by connecting the two graphs from the left, but assigning a soft label. This synthetic graph
(with soft label) has the same structure as the right graph with one-hot label."
REFERENCES,0.8745762711864407,"Figure 7 depicts an intrusion caused by connecting a graph pair. The synthetic graph in the middle
is created by connecting the two graphs from the left, but assigning a soft label (i.e., 50% of class1
and 50% of class 2). This synthetic graph has the same structure as the right graph from the original
training set, with an one-hot label (i.e., 100% of class 3)."
REFERENCES,0.8779661016949153,"A.6
VISUALIZATION OF THE LEARNED REPRESENTATIONS"
REFERENCES,0.8813559322033898,"In Figure 8, we also visualize the ﬁnal-layer representations formed by the GCN baseline, Mixup-
Graph, and ifMixup on the original training graphs of the NCI109 and NCI1 datasets. We project
these embeddings to 2D using t-SNE (van der Maaten & Hinton, 2008)."
REFERENCES,0.8847457627118644,"The upper row of Figure 8 shows that for NCI109, both GCN and MixupGraph were not able to
separate the two classes, while ifMixup completely separated the training graphs with different labels.
Similarly, when considering the bottom row of Figure 8 as for NCI1, both GCN and MixupGraph did
not completely divide the two classes, while ifMixup attained a perfect separation for the two."
REFERENCES,0.888135593220339,"A.7
UNEXPECTED RESULTS"
REFERENCES,0.8915254237288136,We here also report an unexpected result.
REFERENCES,0.8949152542372881,"We randomly shufﬂe the node order of one of the graphs in the graph pair before mixing for ifMixup.
Such shufﬂing is able to signiﬁcantly increase the number of synthetic graphs used for training for
ifMixup, and we expect this would further improve the model’s predicative accuracy. The accuracy"
REFERENCES,0.8983050847457628,Under review as a conference paper at ICLR 2022
REFERENCES,0.9016949152542373,Figure 8: 2D visualization of the learned representations of the training graphs in NCI109 and NCI1.
REFERENCES,0.9050847457627119,"obtained over the ﬁrst six datasets of Table 3 obtained by ifMixup with GCNs as baseline is presented
in Table 4."
REFERENCES,0.9084745762711864,"Unexpectedly, results in Table 4 show that leveraging node order shufﬂing to increase the training data
size did not help in terms of accuracy obtained. We hypothesis that this is may due to the modeling
capability of the GCN networks. We will further investigate this hypothesis in our future work."
REFERENCES,0.911864406779661,"ifMixup
without Shufﬂing
with Shufﬂing
PTC MR
0.654±0.003
0.650±0.004
NCI109
0.820±0.005
0.816±0.001
NCI1
0.819±0.004
0.817±0.001
MUTAG
0.879±0.003
0.864±0.006
ENZYMES
0.570±0.014
0.579±0.008
PROTEINS
0.753±0.008
0.741±0.003"
REFERENCES,0.9152542372881356,"Table 4: Accuracy of ifMixup with and without randomly shufﬂing the graph node order before
mixing, with GCN networks as baseline."
REFERENCES,0.9186440677966101,"A.8
A VARIANT OF MIXUPGRAPH"
REFERENCES,0.9220338983050848,"MixupGraph
Manifold Mixup
PTC MR
0.631±0.005
0.655±0.025
NCI109
0.822±0.008
0.820±0.007
NCI1
0.822±0.001
0.824±0.005
MUTAG
0.884±0.009
0.887±0.008
ENZYMES
0.521±0.007
0.505±0.028
PROTEINS
0.744±0.005
0.747±0.008
IMDB-M
0.518±0.004
0.521±0.002
IMDB-B
0.761±0.001
0.764±0.004
Table 5: Accuracy of the MixupGraph and Man-
ifold Mixup with GIN networks as baseline. We
report mean scores over 3 runs of 10-fold cross
validation with standard deviations (denoted ±)."
REFERENCES,0.9254237288135593,"For GIN, the ﬁnal-layer representation of a
graph is the concatenation of all the represen-
tations of each layer of the networks. In this
sense, we can implement the idea of mixing on
a random embedding layer as that in the Man-
ifold Mixup (Verma et al., 2018) for vision."
REFERENCES,0.9288135593220339,"We compare Manifold Mixup with the Mixup-
Graph, and the results are shown in Table 5.
Results in the table show that MixupGraph
and Manifold Mixup obtained similar accu-
racy on all the eight datasets. For example,
for the PTC MR and IMDB-M datasets, Man-
ifold Mixup obtained higher accuracy, while on
the ENZYMES and NCI109 datasets Manifold
Mixup obtained lower accuracy than MixupGraph. For the other four datasets, the accuracy obtained
by the two methods are comparable."
REFERENCES,0.9322033898305084,Under review as a conference paper at ICLR 2022
REFERENCES,0.9355932203389831,"A.9
SHALLOW GCN AND GIN"
REFERENCES,0.9389830508474576,"We also tested the performance of a 3-layer GCN and a 3-layer GIN baseline models. Results are
presented in Table 6. As can be seen in the table, both GCN and GIN baselines obtained inferior"
REFERENCES,0.9423728813559322,"GCN Baseline
GIN Baseline
PTC MR
0.619± 0.006
0.617± 0.003
NCI109
0.791± 0.004
0.810± 0.002
NCI1
0.796± 0.002
0.814± 0.001
MUTAG
0.827± 0.003
0.883± 0.009
ENZYMES
0.508± 0.015
0.497± 0.003
PROTEINS
0.738± 0.005
0.742± 0.007
IMDB-M
0.510± 0.008
0.511± 0.008
IMDB-B
0.748± 0.008
0.758± 0.002"
REFERENCES,0.9457627118644067,"Table 6: Accuracy of GCN and GIN with 3 layers. We report mean scores over 3 runs of 10-fold
cross validation with standard deviations (denoted ±)."
REFERENCES,0.9491525423728814,"accuracy than a deeper GNN networks, namely 5 or 8 layers as used in the experiments in the main
paper."
REFERENCES,0.9525423728813559,"A.10
ILLUSTRATION OF MIXING AND GRAPH STRUCTURE RECOVERING"
REFERENCES,0.9559322033898305,"In Figure 9, we illustrate how the mixed graph structure is formed and how the structures of the two
source graphs can be recovered from the mixed graph."
REFERENCES,0.9593220338983051,𝜆= 0.75 1.0 1.0
REFERENCES,0.9627118644067797,"1.0
1.0
1.0"
REFERENCES,0.9661016949152542,"𝑒=1.0
0.75"
REFERENCES,0.9694915254237289,𝑒=𝜆𝑒𝐴+ 1 −𝜆𝑒𝐵 0.75 0.25
REFERENCES,0.9728813559322034,For each edge  𝑒in the mixed graph:
REFERENCES,0.976271186440678,"1) if  𝑒= 1.0 𝑡ℎ𝑒𝑛𝑒𝐴and 𝑒𝐵both exist
2) if  𝑒> 0.5, 𝑡ℎ𝑒𝑛𝑒𝐴exists only
3) if  𝑒< 0.5, then 𝑒𝐵exists only 1.0 1.0"
REFERENCES,0.9796610169491525,"1.0
1.0
1.0"
REFERENCES,0.9830508474576272,"𝑒=1.0
0.75 0.75 0.25"
REFERENCES,0.9864406779661017,"forming mixed graph
recovering source graphs"
REFERENCES,0.9898305084745763,"Graph A: (𝑣𝐴, 𝑒𝐴)
Graph B: (𝑣B, 𝑒𝐵)
Graph A: (𝑣𝐴, 𝑒𝐴)
Graph B: (𝑣B, 𝑒𝐵)"
REFERENCES,0.9932203389830508,"mixed graph
mixed graph"
REFERENCES,0.9966101694915255,Figure 9: Illustration of mixing and recovering in ifMixup.
