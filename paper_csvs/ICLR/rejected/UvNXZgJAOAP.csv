Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.001984126984126984,"Attention mechanism has been widely applied to tasks that output some sequence
1"
ABSTRACT,0.003968253968253968,"from an input image. Its success comes from the ability to align relevant parts of
2"
ABSTRACT,0.005952380952380952,"the encoded image with the target output. However, most of the existing methods
3"
ABSTRACT,0.007936507936507936,"fail to build clear alignment because the aligned parts are unable to well represent
4"
ABSTRACT,0.00992063492063492,"the target. In this paper we seek clear alignment in attention mechanism through
5"
ABSTRACT,0.011904761904761904,"a sharpener module. Since it deliberately locates the target in an image region
6"
ABSTRACT,0.013888888888888888,"and reﬁnes representation to be target-speciﬁc, the alignment and interpretability
7"
ABSTRACT,0.015873015873015872,"of attention can be signiﬁcantly improved. Experiments on synthetic handwritten
8"
ABSTRACT,0.017857142857142856,"digit as well as real-world scene text recognition datasets show that our approach
9"
ABSTRACT,0.01984126984126984,"outperforms the mainstream ones such as soft and hard attention.
10"
INTRODUCTION,0.021825396825396824,"1
INTRODUCTION
11"
INTRODUCTION,0.023809523809523808,"In modern sequence to sequence learning, attention mechanism has become a key building block,
12"
INTRODUCTION,0.025793650793650792,"because it helps identify relevant parts of the input sequence and align them with the target output at
13"
INTRODUCTION,0.027777777777777776,"each time step. Such alignment resembles ﬁxation in human vision, where only the object of interest
14"
INTRODUCTION,0.02976190476190476,"falls on the fovea in the retina, leading the visual scene outside to be largely ignored. Thanks to such
15"
INTRODUCTION,0.031746031746031744,"ability to select perceptual information, attention mechanism has been successfully applied to many
16"
INTRODUCTION,0.03373015873015873,"visual tasks, such as scene text recognition (Shi et al., 2019) and image captioning (Xu et al., 2015).
17"
INTRODUCTION,0.03571428571428571,"Although a variety of attention mechanisms have been proposed to build alignment, most of them
18"
INTRODUCTION,0.037698412698412696,"fail to achieve clear alignment. Soft attention (Bahdanau et al., 2015; Luong et al., 2015), the most
19"
INTRODUCTION,0.03968253968253968,"popular one, aligns a weighted average of the input sequence with the target output throughout the
20"
INTRODUCTION,0.041666666666666664,"time. Since the weights are never zeros, irrelevant parts are inevitably involved in the alignment and
21"
INTRODUCTION,0.04365079365079365,"may introduce distraction. For distinct alignment, hard attention (Xu et al., 2015) enforces exactly
22"
INTRODUCTION,0.04563492063492063,"one input part is employed, regardless of whether it represents the target or not, and thus may still
23"
INTRODUCTION,0.047619047619047616,"suffer from irrelevant parts. Besides, existing attention mechanisms often regard the input sequence
24"
INTRODUCTION,0.0496031746031746,"as ﬁxed during alignment establishment. If the target representation given by the selected part(s)
25"
INTRODUCTION,0.051587301587301584,"is poor, there is no way to ﬁx it. This is especially the case in visual sequence learning, where
26"
INTRODUCTION,0.05357142857142857,"features are precomputed by a convolutional neural network (CNN) before being fed into attention
27"
INTRODUCTION,0.05555555555555555,"mechanisms. As each feature only characterises a local ﬁxed image region (i.e. the receptive ﬁeld), it
28"
INTRODUCTION,0.057539682539682536,"hardly covers the appearance of the target exactly, thus leading to noisy representation. See Fig. 1(b)
29"
INTRODUCTION,0.05952380952380952,"for an example.
30"
INTRODUCTION,0.061507936507936505,"In this work we address the construction of clear alignment in attention mechanism. This is achieved
31"
INTRODUCTION,0.06349206349206349,"by aligning the target output with image regions instead of features, which is a more natural approach
32"
INTRODUCTION,0.06547619047619048,"to alignment for visual sequence learning. A sharpener module is then used to make the aligned
33"
INTRODUCTION,0.06746031746031746,"region as speciﬁc as possible to the target, essentially a clear alignment. While it can take any form,
34"
INTRODUCTION,0.06944444444444445,"the module explored in this paper consists of a localiser and an encoder. The former locates the
35"
INTRODUCTION,0.07142857142857142,"target in the region, while the latter extracts features from the result for alignment. It is such accurate
36"
INTRODUCTION,0.07341269841269842,"and speciﬁc representation that makes attention mechanism able to pay close attention to the target,
37"
INTRODUCTION,0.07539682539682539,"leading to improved alignment and thus interpretability. The sharpener can be trained along with
38"
INTRODUCTION,0.07738095238095238,"any sequence-to-sequence (Seq2Seq) model through back-propagation without extra supervision.
39"
INTRODUCTION,0.07936507936507936,"Nonetheless, it is also possible to guide its training to further improve alignment quality if auxiliary
40"
INTRODUCTION,0.08134920634920635,"information is available (see Sect. 4.1). Therefore, the sharpener naturally lends itself to direct
41"
INTRODUCTION,0.08333333333333333,"attention manipulation, which is yet not available in most of the existing attention mechanisms.
42"
INTRODUCTION,0.08531746031746032,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.0873015873015873,softmax tanh
INTRODUCTION,0.08928571428571429,"+
φ(X, αt)
ct fc
fc"
INTRODUCTION,0.09126984126984126,"ht−1
x1 · · · · · · · · · xM
X"
INTRODUCTION,0.09325396825396826,"αt1 · · · · · · · · · αtM
αt X"
INTRODUCTION,0.09523809523809523,"W
f
t
(a)
(b)"
INTRODUCTION,0.09722222222222222,"Figure 1: (a) An outline of the attention mechanism. A query vector ht−1 is compared with an input
sequence X to ﬁgure out where to look via a multilayer perceptron (see the grey box), leading to
a set of weights αt. A context vector ct is then computed for alignment by a function φ, where
attention mechanisms generally differ. Soft attention computes ct as a weighted average of X while
hard attention does it by randomly sampling an element from X. (b) Poor target representation in
attention mechanism. Each element of X only represents a ﬁxed region deﬁned by the receptive ﬁeld
(see yellow boxes). It is hard to accurately represent the target without the distraction of redundant or
missing parts (see letters ‘f’ and ‘W’ for examples)."
SHARP ATTENTION,0.0992063492063492,"2
SHARP ATTENTION
43"
ALIGNMENT,0.10119047619047619,"2.1
ALIGNMENT
44"
ALIGNMENT,0.10317460317460317,"Let X = {x1, . . . , xM} be an input sequence of length M, where xi ∈RD is a feature vector
45"
ALIGNMENT,0.10515873015873016,"representing a region of an input image. Usually, X comes from the last convolutional layer of a
46"
ALIGNMENT,0.10714285714285714,"backbone network, and xi delineates a region of ﬁxed size given by the receptive ﬁeld of that layer.
47"
ALIGNMENT,0.10912698412698413,"Similarly, we denote the output sequence of length N as Y = {y1, . . . , yN}, where yt ∈RK is
48"
ALIGNMENT,0.1111111111111111,"a one-of-K encoded vector indicating a discrete token in a vocabulary of size K. Our goal is to
49"
ALIGNMENT,0.1130952380952381,"learn a model that can accurately predict yt by choosing appropriate xi in a sequential process.
50"
ALIGNMENT,0.11507936507936507,"This implicitly asks for building an alignment between X and Y at each time step. To facilitate the
51"
ALIGNMENT,0.11706349206349206,"construction, we introduce a latent variable A = {a1, . . . , aN}, where at ∈RM is a one-of-M
52"
ALIGNMENT,0.11904761904761904,"encoded vector indicating the index of the selected feature vector at some time. For example, ati = 1
53"
ALIGNMENT,0.12103174603174603,"refers to the i-th element of X (i.e. xi) being chosen to predict yt at time t. Usually, the selected
54"
ALIGNMENT,0.12301587301587301,"feature is called context vector and denoted as ct.
55"
ALIGNMENT,0.125,"To learn the model, we maximise a conditional probability
56"
ALIGNMENT,0.12698412698412698,"p(Y |X) =
X"
ALIGNMENT,0.12896825396825398,"A
p(Y, A|X) =
X A N
Y"
ALIGNMENT,0.13095238095238096,"t=1
p(yt, A| y1, . . . , yt−1
|
{z
}
y<t"
ALIGNMENT,0.13293650793650794,", X) = N
Y t=1 X"
ALIGNMENT,0.1349206349206349,"at
p(yt, at|y<t, X)
(1) = N
Y t=1 X"
ALIGNMENT,0.13690476190476192,"at
p(at|y<t, X)p(yt|y<t, X, at) ≡ N
Y t=1 M
X"
ALIGNMENT,0.1388888888888889,"i=1
p(ati = 1|y<t, X)p(yt|y<t, xi)(2)"
ALIGNMENT,0.14087301587301587,"where the last two terms in (1) are obtained by applying chain rule on Y , and by using the assumption
57"
ALIGNMENT,0.14285714285714285,"that yt only depends on at at time t, respectively. Equation (2) clearly deﬁnes two major components
58"
ALIGNMENT,0.14484126984126985,"to compute p(Y |X). One is the chance of selecting each element of X, and the other is the likelihood
59"
ALIGNMENT,0.14682539682539683,"of the target token given the selection. However, the computation of the latter is impractical when M
60"
ALIGNMENT,0.1488095238095238,"is large because every element of X has to be considered. Two typical approximations to (2) are thus
61"
ALIGNMENT,0.15079365079365079,"proposed, leading to the soft and hard attention mechanisms.
62"
ALIGNMENT,0.1527777777777778,"By using the ﬁrst order Taylor expansion,1 we obtain the loss function for soft attention,
63"
ALIGNMENT,0.15476190476190477,"log p(Y |X) = N
X"
ALIGNMENT,0.15674603174603174,"t=1
log M
X"
ALIGNMENT,0.15873015873015872,"i=1
αtip(yt|y<t, xi) ! ≈ N
X"
ALIGNMENT,0.16071428571428573,"t=1
log p "
ALIGNMENT,0.1626984126984127,"yt|y<t, M
X"
ALIGNMENT,0.16468253968253968,"i=1
αtixi ! ,
(3)"
ALIGNMENT,0.16666666666666666,"1Let f(·) be a function of some random variable. By using Taylor’s theorem, the ﬁrst-order approximation to
the expectation E[f(·)] is given by E[f(·)] ≈f(E[·])."
ALIGNMENT,0.16865079365079366,Under review as a conference paper at ICLR 2022
ALIGNMENT,0.17063492063492064,"where we deﬁne αti ≡p(ati = 1|y<t, X) to simplify the notation. In (3), the context vector ct is
64"
ALIGNMENT,0.17261904761904762,"given by PM
i=1 αtixi, which means that yt is no longer predicted by a single element of X but rather
65"
ALIGNMENT,0.1746031746031746,"a weighted average of X, thus leading to the break in alignment. To pursue the alignment such that
66"
ALIGNMENT,0.1765873015873016,"each target token only depends on one element of X, hard attention instead estimates a variational
67"
ALIGNMENT,0.17857142857142858,"lower bound on log p(Y |X) using Jensen’s inequality,
68"
ALIGNMENT,0.18055555555555555,"log p(Y |X) = N
X"
ALIGNMENT,0.18253968253968253,"t=1
log M
X"
ALIGNMENT,0.18452380952380953,"i=1
αtip(yt|y<t, xi) ! ⩾ N
X t=1 M
X"
ALIGNMENT,0.1865079365079365,"i=1
αti log p(yt|y<t, xi).
(4)"
ALIGNMENT,0.1884920634920635,"Now the optimisation of log p(Y |X) can be thought as repeating the following steps until happy:
69"
ALIGNMENT,0.19047619047619047,"(i) estimating p(at|y<t, X) by ﬁxing all model parameters; (ii) modifying the parameters to maximise
70"
ALIGNMENT,0.19246031746031747,"log p(yt|y<t, xi) using each xi. The underlying idea is to increase log p(Y |X) by iteratively raising
71"
ALIGNMENT,0.19444444444444445,"the lower bound. Since it is infeasible to consider every xi as aforementioned, approximation is often
72"
ALIGNMENT,0.19642857142857142,"adopted and achieved by Monte Carlo sampling (See Sect. 2.4 for details), thus leading ct to be the
73"
ALIGNMENT,0.1984126984126984,"sampled feature vector for hard attention.
74"
LOSS FUNCTION,0.2003968253968254,"2.2
LOSS FUNCTION
75"
LOSS FUNCTION,0.20238095238095238,"Intuitively, if each xi is an accurate representation of the target token when optimising (4), particularly
76"
LOSS FUNCTION,0.20436507936507936,"in the second step, there would be a tight gap between log p(Y |X) and its lower bound. In contrast,
77"
LOSS FUNCTION,0.20634920634920634,"if any poor xi occurs, the gap may become large and thus result in performance degradation. Subject
78"
LOSS FUNCTION,0.20833333333333334,"to the ﬁxed local region, it is unlikely for xi to well characterise the target token, which makes hard
79"
LOSS FUNCTION,0.21031746031746032,"attention hardly achieve clear alignment (see Fig. 1(b)). This motivates us to reformulate (4) for more
80"
LOSS FUNCTION,0.2123015873015873,"ﬂexible representation of the target tokens.
81"
LOSS FUNCTION,0.21428571428571427,"Suppose we can break down the input image into a set of M local regions, each of which is sufﬁciently
82"
LOSS FUNCTION,0.21626984126984128,"large to cover objects of interest. We would like to maximise the marginal log-likelihood log p(Y |R),
83"
LOSS FUNCTION,0.21825396825396826,"where R = {r1, . . . , rM} is the set of regions. Similarly, its lower bound ℓis given by
84"
LOSS FUNCTION,0.22023809523809523,"log p(Y |R) ⩾ N
X t=1 X"
LOSS FUNCTION,0.2222222222222222,"at
p(at|y<t, R) log p(yt|y<t, R, at) ≡ℓ,
(5)"
LOSS FUNCTION,0.22420634920634921,"where at is still a one-of-M encoded vector but now refers to the index of the selected region at time
85"
LOSS FUNCTION,0.2261904761904762,"t. By working with (5), we are not restricted to the representation given by X any more. Consider
86"
LOSS FUNCTION,0.22817460317460317,"x to be a function of r parameterised by the backbone network, e.g., x = fg(r; θg), where θg
87"
LOSS FUNCTION,0.23015873015873015,"denotes all the weights in the network. By plugging X = {fg(r1; θg), . . . , fg(rM; θg)} into (4), it
88"
LOSS FUNCTION,0.23214285714285715,"is easy to see that the lower bound of log p(Y |X) is equivalent to that of log p(Y |R) when partially
89"
LOSS FUNCTION,0.23412698412698413,"parameterising the two terms p(at|y<t, R) and log p(yt|y<t, R, at) using θg. As ℓis valid for all
90"
LOSS FUNCTION,0.2361111111111111,"model parameters, it does not rely on any speciﬁc modelling. This allows us to separately model the
91"
LOSS FUNCTION,0.23809523809523808,"use of R in each term. For example, we may leverage the same backbone network for the ﬁrst term to
92"
LOSS FUNCTION,0.2400793650793651,"get a rough idea on where to look, while deliberately design a network for the second term to sharpen
93"
LOSS FUNCTION,0.24206349206349206,"the focus. It is such ﬂexible parameterisation that makes the construction of clear alignment possible.
94"
LOSS FUNCTION,0.24404761904761904,"Below we elaborate the modelling of each term in (5).
95"
MODELLING,0.24603174603174602,"2.3
MODELLING
96"
MODELLING,0.24801587301587302,"We use a variant of VGG (Shi et al., 2017) as the backbone network to not only create the set of
97"
MODELLING,0.25,"regions but also process it in p(at|y<t, R). When the backbone network is a CNN, we may take
98"
MODELLING,0.251984126984127,"advantage of the implicitly deﬁned sliding widow for region generation. For example, our backbone
99"
MODELLING,0.25396825396825395,"network effectively divides an input image of size 100×32 into 24 86×46 regions when extracting
100"
MODELLING,0.25595238095238093,"features from the ﬁnal layer (Araujo et al., 2019). Note that the generation of R is arbitrary and we
101"
MODELLING,0.25793650793650796,"just use the sliding window for simplicity. To emphasise clear alignment with log p(yt|y<t, R, at),
102"
MODELLING,0.25992063492063494,"we leverage a sharpener module that consists of a localiser and an encoder. The former seeks the
103"
MODELLING,0.2619047619047619,"target in the selected region while the latter extracts features from the result. While a natural choice
104"
MODELLING,0.2638888888888889,"of the localiser is object detectors, we instead resort to spatial transformer networks (STNs, Jaderberg
105"
MODELLING,0.26587301587301587,"et al. (2015)) for both computational and labelling efﬁciency. STN is a lightweight CNN that is able
106"
MODELLING,0.26785714285714285,"to crop and transform an image region. Its training does not need expensive annotations such as
107"
MODELLING,0.2698412698412698,"bounding boxes, which are usually unavailable in sequential learning tasks. The encoder can take
108"
MODELLING,0.2718253968253968,"any form and we use the same CNN to the backbone to simplify the implementation. Let Z be the
109"
MODELLING,0.27380952380952384,Under review as a conference paper at ICLR 2022
MODELLING,0.2757936507936508,"fc
softmax
ˆyt"
MODELLING,0.2777777777777778,"Input
Backbone
Attention
Sharpener
RNN"
MODELLING,0.27976190476190477,"r1
· · · rM X αt ht−1 ri ct"
MODELLING,0.28174603174603174,"ht−1
yt−1 ht ht"
MODELLING,0.2837301587301587,"Figure 2: A generic Seq2Seq learning architecture with sharp attention. Given an input image, a
backbone network breaks down it into a set of regions and extracts features from each region, leading
to a sequence of feature vectors X. An attention mechanism (see Fig. 1(a)) uses X and a hidden state
vector ht−1 to compute a categorical distribution αt, based on which a region is randomly chosen
and fed into a sharpener module to compute the context vector ct for clear alignment. A recurrent
neural network (RNN) takes in ht−1, ct and yt−1 (the token at previous time step) to update its
internal state, and then outputs current ht for token prediction and next iteration (dashed line)."
MODELLING,0.2857142857142857,"encoder output, which is also a sequence of feature vectors similar to X with length dependent on the
110"
MODELLING,0.2876984126984127,"size of the localisation result. The output of the sharpener is the context vector, whose computation
111"
MODELLING,0.2896825396825397,"will be detailed in Sect. 2.5.
112"
MODELLING,0.2916666666666667,"The dependency of the current token yt on all previous ones y<t over the time is often modelled by
113"
MODELLING,0.29365079365079366,"an RNN, e.g., long short-term memory (LSTM, Hochreiter & Schmidhuber (1997)). Speciﬁcally, it is
114"
MODELLING,0.29563492063492064,"deﬁned by2
115"
MODELLING,0.2976190476190476,"ht = fr(ht−1, yt−1, ct; θr),
h0 ≡fi"
M,0.2996031746031746,"1
M M
X"
M,0.30158730158730157,"i=1
xi; θi !"
M,0.30357142857142855,",
y0 ≡0,
(6)"
M,0.3055555555555556,"where fr(·) refers to the non-linear function deﬁned by LSTM with parameters θr, ht is a hidden
116"
M,0.30753968253968256,"state vector at time t that summarises the history tokens y<t and is initialised by a fully connected
117"
M,0.30952380952380953,"layer fi(·) that takes an average of X as the input, and a zero vector is used as the initial token y0.
118"
M,0.3115079365079365,"Now we are ready to deﬁne the two terms in (5). As we use the backbone network to process R in
119"
M,0.3134920634920635,"p(at|y<t, R), the probability of selecting a particular region is given by
120"
M,0.31547619047619047,"αti ≡p(ati = 1|y<t, R) ≡p(ati = 1|ht−1, X) = softmax(fa(xi, ht−1; θa)),
(7)"
M,0.31746031746031744,"where fa(·) is the attention function as described in Bahdanau et al. (2015) (Fig. 1(a)). The probability
121"
M,0.3194444444444444,"of the output token ˆyt given all previous ones as well as the selected region is computed by
122"
M,0.32142857142857145,"p(yt = ˆyt|y<t, R, at) ≡p(yt = ˆyt|ht−1, ct) = softmax(fe(yt−1, ht; θe)),
(8)"
M,0.32341269841269843,"where fe(·) is a fully connected layer. An overview of the whole architecture is given in Fig. 2.
123"
OPTIMISATION,0.3253968253968254,"2.4
OPTIMISATION
124"
OPTIMISATION,0.3273809523809524,"The differentiation of ℓw.r.t. all model parameters yields the following learning rule3
125"
OPTIMISATION,0.32936507936507936,"∂ℓ
∂θ = N
X t=1 X"
OPTIMISATION,0.33134920634920634,"at
p(at|y<t, R)
∂log p(yt|y<t, R, at)"
OPTIMISATION,0.3333333333333333,"∂θ
+ log p(yt|y<t, R, at)∂log p(at|y<t, R) ∂θ 
,"
OPTIMISATION,0.3353174603174603,"where θ is a collection of model parameters, e.g., θ = {θg, θs, θr, θi, θa, θe} (θs for the sharpener
126"
OPTIMISATION,0.3373015873015873,"module). To reduce the computational cost as explained in Sect. 2.1, the derivative at time t is often
127"
OPTIMISATION,0.3392857142857143,"numerically approximated by the Monte Carlo method as follows
128 ∂ℓt ∂θ ≈1 S S
X"
OPTIMISATION,0.3412698412698413,"s=1
p(ˆas
t|y<t, R)
∂log p(yt|y<t, R, ˆas
t)
∂θ
+ log p(yt|y<t, R, ˆas
t)∂log p(ˆas
t|y<t, R)
∂θ 
,"
OPTIMISATION,0.34325396825396826,"2Strictly speaking, the alignment in this modelling is no longer conditionally independent throughout the
time as assumed in Sect. 2.1. In fact, this is the modelling used in both soft and hard attention mechanisms
(Bahdanau et al., 2015; Xu et al., 2015). However, the discussion on why these mechanisms fail to achieve clear
alignment still applies.
3We use the trick ∇θp(x; θ) = p(x; θ)∇θ log p(x; θ) in the derivation."
OPTIMISATION,0.34523809523809523,Under review as a conference paper at ICLR 2022
OPTIMISATION,0.3472222222222222,"where S is the number of samples ˆas
t drawn from a categorical distribution deﬁned by p(at|y<t, R)
129"
OPTIMISATION,0.3492063492063492,"(Xu et al., 2015). Similar to Mnih et al. (2014) and Ba et al. (2015), this approximation yields a hybrid
130"
OPTIMISATION,0.35119047619047616,"loss function asking for different optimisation strategies for the two terms in the square brackets.
131"
OPTIMISATION,0.3531746031746032,"The former is optimised by a cross entropy loss together with the ground-truth token at time t and
132"
OPTIMISATION,0.3551587301587302,"gradient back-propagation. By regarding the accumulated sum of log p(yt|y<t, R, ˆas
t) over the time
133"
OPTIMISATION,0.35714285714285715,"as a reward, the latter is achieved by the REINFORCE algorithm (Williams, 1992). To reduce the
134"
OPTIMISATION,0.35912698412698413,"high variance in gradient estimate caused by the unbounded log p(yt|y<t, R, ˆas
t) (Ba et al., 2015),
135"
OPTIMISATION,0.3611111111111111,"we follow Xu et al. (2015) to introduce a moving average baseline
136"
OPTIMISATION,0.3630952380952381,"bj = 0.9 × bj−1 + 0.1 ×
1
NS S
X s=1 N
X"
OPTIMISATION,0.36507936507936506,"t=1
log p(yt|y<t, R, ˆas
t),"
OPTIMISATION,0.36706349206349204,"where j is the index of the mini-batch. Finally, we use the following learning rule for optimisation
137"
OPTIMISATION,0.36904761904761907,"∂ℓ
∂θ ≈1 S S
X s=1 N
X"
OPTIMISATION,0.37103174603174605,"t=1
p(ˆas
t|y<t, R)
∂log p(yt|y<t, R, ˆas
t)
∂θ
+ λr(log p(yt|y<t, R, ˆas
t) −b)∂log p(ˆas
t|y<t, R)
∂θ 
,"
OPTIMISATION,0.373015873015873,"(9)
where λr is a learning hyper-parameter. We do not add entropy H[at] to (9) to further reduce gradient
138"
OPTIMISATION,0.375,"variance as in Xu et al. (2015), because it encourages a uniform distribution and breaks the alignment.
139"
CONTEXT VECTOR,0.376984126984127,"2.5
CONTEXT VECTOR
140"
CONTEXT VECTOR,0.37896825396825395,"Rather than compute c from X like both soft and hard attention do, the proposed sharp attention
141"
CONTEXT VECTOR,0.38095238095238093,"leverages the encoder output. Speciﬁcally, we explore three ways to compute ct given Zt, the encoder
142"
CONTEXT VECTOR,0.38293650793650796,"output at time t. The ﬁrst one is pooling, where an average pooling of Zt is used for ct. Inspired
143"
CONTEXT VECTOR,0.38492063492063494,"by the glimpse idea (Mnih et al., 2014), we combine the result from pooling with ˆxs
t, the feature
144"
CONTEXT VECTOR,0.3869047619047619,"vector associated with the sampled region, to incorporate both ﬁne and coarse representation of the
145"
CONTEXT VECTOR,0.3888888888888889,"target token, leading to the second way—chain. Alternatively, we may compute a weighted average
146"
CONTEXT VECTOR,0.39087301587301587,"of the feature set {Zt, ˆxs
t} for better mixed representation. With ht−1, the weights can be learned in
147"
CONTEXT VECTOR,0.39285714285714285,"a similar way to the soft attention as shown in Fig. 1(a). We call this last approach to ct weighting.
148"
RELATED WORK,0.3948412698412698,"3
RELATED WORK
149"
RELATED WORK,0.3968253968253968,"In Xu et al. (2015), the alignment is treated as a latent variable to help deﬁne a loss function, which is
150"
RELATED WORK,0.39880952380952384,"optimised by gradually increasing a variational lower bound on marginal log-likelihood of the target
151"
RELATED WORK,0.4007936507936508,"output given the input sequence. Later, a variety of variational inference techniques (Lawson et al.,
152"
RELATED WORK,0.4027777777777778,"2018; Deng et al., 2018; Bahuleyan et al., 2018) are proposed to further reduce the gap between the
153"
RELATED WORK,0.40476190476190477,"lower bound and the marginal log-likelihood. Alternative approaches to approximating the marginal
154"
RELATED WORK,0.40674603174603174,"log-likelihood can be found in Shankar et al. (2018) and Shankar & Sarawagi (2019). The former
155"
RELATED WORK,0.4087301587301587,"cherry-picks a set of alignments, computes the log-likelihood conditioned on each alignment and
156"
RELATED WORK,0.4107142857142857,"averages the results, while the latter extends the idea by enforcing Markov property on adjacent
157"
RELATED WORK,0.4126984126984127,"alignments. Instead of approximation, Wu et al. (2018) attempted to compute exact marginal log-
158"
RELATED WORK,0.4146825396825397,"likelihood by assuming that each alignment is conditionally independent across time steps. All of the
159"
RELATED WORK,0.4166666666666667,"above methods regard the input sequence as ﬁxed in optimisation and thus cannot tailor the input
160"
RELATED WORK,0.41865079365079366,"part(s) to clear alignment.
161"
RELATED WORK,0.42063492063492064,"The idea of using relevant parts to improve attention has been explored in various tasks. Mei et al.
162"
RELATED WORK,0.4226190476190476,"(2016) adjusted the weights of the input parts resulting from soft attention to highlight the most
163"
RELATED WORK,0.4246031746031746,"relevant ones for selective generation. A similar work can be found in Nallapati et al. (2016), where
164"
RELATED WORK,0.42658730158730157,"keywords are interwoven with the sentences in which they lie for text summarisation by applying soft
165"
RELATED WORK,0.42857142857142855,"attention to sentences and words respectively and rescaling word weights. Instead of reweighting,
166"
RELATED WORK,0.4305555555555556,"Cheng et al. (2017) used character-level masks to guide the selection of useful parts for scene text
167"
RELATED WORK,0.43253968253968256,"recognition. None of these methods build clear alignment due to the use of soft attention.
168"
RELATED WORK,0.43452380952380953,"Our work is closely related to Xu et al. (2015) and Ba et al. (2015). We generalise the former’s
169"
RELATED WORK,0.4365079365079365,"mathematical formulation on hard attention by introducing ﬂexible representation of objects of interest
170"
RELATED WORK,0.4384920634920635,"via a sharpener module. While the generalisation appears similar to the latter, we use it to tackle the
171"
RELATED WORK,0.44047619047619047,"alignment issue in attention mechanisms rather than develop a new Seq2Seq model. Our modelling
172"
RELATED WORK,0.44246031746031744,"also differs. Instead of seeking desired objects within the whole image, a divide-and-conquer scheme
173"
RELATED WORK,0.4444444444444444,Under review as a conference paper at ICLR 2022
RELATED WORK,0.44642857142857145,"is used to gradually narrow the search range for accurate localisation. Another difference in modelling
174"
RELATED WORK,0.44841269841269843,"is that in Ba et al. (2015) prediction will not happen until a series of localisation across predeﬁned
175"
RELATED WORK,0.4503968253968254,"time steps whereas in our work that immediately follows localisation at each time.
176"
EXPERIMENTS,0.4523809523809524,"4
EXPERIMENTS
177"
EXPERIMENTS,0.45436507936507936,"We demonstrate the efﬁcacy of the proposed method in two different scenarios of increasing difﬁculty:
178"
EXPERIMENTS,0.45634920634920634,"(i) synthetic handwritten digit recognition and (ii) real-world scene text recognition. Rather than strive
179"
EXPERIMENTS,0.4583333333333333,"for state-of-the-art results, the focus here is to highlight (i) the performance of Seq2Seq models can
180"
EXPERIMENTS,0.4603174603174603,"be boosted if attention mechanism really yields clear alignment, that is, paying attention to the target
181"
EXPERIMENTS,0.4623015873015873,"object, and (ii) the proposed sharp attention is an effective approach to reaching the goal. Therefore,
182"
EXPERIMENTS,0.4642857142857143,"our vanilla system is built upon off-the-shelf modules and was trained without sophisticated parameter
183"
EXPERIMENTS,0.4662698412698413,"tuning schemes. Below we describe some common choices for all scenarios.
184"
EXPERIMENTS,0.46825396825396826,"Implementation
All images are converted to grey scale and resized to 100×32. A variant of VGG
185"
EXPERIMENTS,0.47023809523809523,"(Shi et al., 2017) is then used as the backbone to extract a 24×1 feature map from each resized
186"
EXPERIMENTS,0.4722222222222222,"image as well as create the set of regions, whose height is clipped to 32. The input sequence X
187"
EXPERIMENTS,0.4742063492063492,"is obtained by splitting the feature map along its width, leading the dimensionality of each xi to
188"
EXPERIMENTS,0.47619047619047616,"be the feature map depth (i.e. 512). Before feeding it into the attention mechanism, we follow
189"
EXPERIMENTS,0.4781746031746032,"Shi et al. (2017) to further process X to capture long-range contextual information with a 2-layer
190"
EXPERIMENTS,0.4801587301587302,"bidirectional LSTM, where each layer has a forward LSTM and a backward LSTM, each having 256
191"
EXPERIMENTS,0.48214285714285715,"hidden units. The depth of the attention mechanism is set to 256 and so is the number of the hidden
192"
EXPERIMENTS,0.48412698412698413,"units of the associated LSTM, which runs over some time to predict the output sequence Y . The
193"
EXPERIMENTS,0.4861111111111111,"number of time steps is set to the maximum transcription length in each scenario. As in Sutskever
194"
EXPERIMENTS,0.4880952380952381,"et al. (2014), an end-of-sequence token is used to indicate the ﬁnish of prediction. The STN in the
195"
EXPERIMENTS,0.49007936507936506,"sharpener is composed of a localisation network, a grid generator and a sampler. Given a region,
196"
EXPERIMENTS,0.49206349206349204,"the localisation network, achieved by the one described in Liu et al. (2016), uses it to estimate an
197"
EXPERIMENTS,0.49404761904761907,"afﬁne transformation, which is then used by the grid generator to place a set of control points on the
198"
EXPERIMENTS,0.49603174603174605,"region. By sampling the intensity value at each control point in a way similar to Shi et al. (2019), the
199"
EXPERIMENTS,0.498015873015873,"sampler produces a patch of given size as the STN output. When multiple STNs are used, the output
200"
EXPERIMENTS,0.5,"of the previous one is used as the input to the next one. The output of the last STN is plugged into
201"
EXPERIMENTS,0.501984126984127,"the encoder in the sharpener to compute Z. The whole system was implemented using TensorFlow
202"
EXPERIMENTS,0.503968253968254,"(Abadi et al., 2016) and the code will be released in the near future.
203"
EXPERIMENTS,0.5059523809523809,"Training
Three kinds of Seq2Seq models were trained from scratch in terms of the attention
204"
EXPERIMENTS,0.5079365079365079,"mechanism used. Speciﬁcally, the soft and hard models were learned via corresponding attention
205"
EXPERIMENTS,0.5099206349206349,"mechanisms respectively. Unlike the previous two baseline models, the sharp models were obtained
206"
EXPERIMENTS,0.5119047619047619,"by the sharpener with the context vector schemes described in Sect. 2.5. A stochastic gradient descent
207"
EXPERIMENTS,0.5138888888888888,"method, ADADELTA (Zeiler, 2012), was used to learn the model parameters until certain number
208"
EXPERIMENTS,0.5158730158730159,"of iterations in different scenarios. The learning rate was constant and set to 1.0 and the decay rate
209"
EXPERIMENTS,0.5178571428571429,"was 0.95. In addition, all model parameters were regularised by an L2 norm with a weight decay
210"
EXPERIMENTS,0.5198412698412699,"of 4×10-5. All experiments were done with a batch size of 192 (per GPU) on a workstation of 4
211"
EXPERIMENTS,0.5218253968253969,"NVIDIA GEFORCE RTX 2080 Ti GPUs. The number of samples S was set to the batch size.
212"
EXPERIMENTS,0.5238095238095238,"Evaluation
A prediction is correct if the predicted transcription matches ground truth. We reported
213"
EXPERIMENTS,0.5257936507936508,"the proportion of correct predictions on each testing dataset. As in Shi et al. (2017), all transcriptions
214"
EXPERIMENTS,0.5277777777777778,"were converted to lower cases and had punctuations ruled out before evaluation if applicable.
215"
HANDWRITTEN DIGIT RECOGNITION,0.5297619047619048,"4.1
HANDWRITTEN DIGIT RECOGNITION
216"
HANDWRITTEN DIGIT RECOGNITION,0.5317460317460317,"We randomly chose l images from the MNIST dataset (Lecun et al., 1998), resized them to 32×32
217"
HANDWRITTEN DIGIT RECOGNITION,0.5337301587301587,"and concatenated them horizontally, leading to an image of l handwritten digits. For each l in {5,
218"
HANDWRITTEN DIGIT RECOGNITION,0.5357142857142857,"7, 9, 11, 13}, we created 20,000 images for training and 10,000 for testing by selecting from the
219"
HANDWRITTEN DIGIT RECOGNITION,0.5376984126984127,"MNIST training and testing datasets respectively, leading to a normal handwritten digit dataset. To
220"
HANDWRITTEN DIGIT RECOGNITION,0.5396825396825397,"introduce some distortion, we repeated the above procedure for a rotated dataset by randomly turning
221"
HANDWRITTEN DIGIT RECOGNITION,0.5416666666666666,"the selected images around y-axis within [-30◦, 30◦] before concatenation. Examples of the generated
222"
HANDWRITTEN DIGIT RECOGNITION,0.5436507936507936,"images are given in Fig. 3. We trained all models with the resulting datasets for 30,000 iterations
223"
HANDWRITTEN DIGIT RECOGNITION,0.5456349206349206,"by setting λr = 1.0 when applicable. For better localisation, we upsampled the set of regions along
224"
HANDWRITTEN DIGIT RECOGNITION,0.5476190476190477,Under review as a conference paper at ICLR 2022
HANDWRITTEN DIGIT RECOGNITION,0.5496031746031746,"4
6
2
1
4
1
1
6
8
2
5
4
6
5
9
6 HARD"
HANDWRITTEN DIGIT RECOGNITION,0.5515873015873016,"7
5
2
1
4
1
1
6
9
2
3
4
6
5
9
6 SHARP"
HANDWRITTEN DIGIT RECOGNITION,0.5535714285714286,"4
6
2
1
4
1
1
6
8
2
3
4
6
5
9
6
SHARP+
REFERENCE"
HANDWRITTEN DIGIT RECOGNITION,0.5555555555555556,"4
6
2
1
4
1
1
6
8
2
5
4
6
5
9
6"
HANDWRITTEN DIGIT RECOGNITION,0.5575396825396826,"Normal (7 digits)
Rotation (9 digits)"
HANDWRITTEN DIGIT RECOGNITION,0.5595238095238095,"GROUND
TRUTH"
HANDWRITTEN DIGIT RECOGNITION,0.5615079365079365,"Figure 3: From top to bottom, we show examples of the created handwritten digit images and
transcriptions (row 2), and patches for context vector computation as well as predicted digits (failures
shown in red) at each time step (rows 3-5). Note that the patches are the receptive ﬁelds for the hard
model and the output of the STN for the pooling based sharp models. The reference images are given
below the patches in the last row."
HANDWRITTEN DIGIT RECOGNITION,0.5634920634920635,Table 1: Recognition accuracy of all models for the handwritten digit datasets.
HANDWRITTEN DIGIT RECOGNITION,0.5654761904761905,"Model
Normal
Rotation
Mean
Acc.
5
7
9
11
13
5
7
9
11
13"
HANDWRITTEN DIGIT RECOGNITION,0.5674603174603174,"Baseline
SOFT
97.2
96.6
95.0
91.9
82.2
96.3
95.4
93.6
89.1
78.3
91.6
HARD
97.2
96.4
94.1
91.6
81.7
96.6
95.4
93.5
89.2
78.6
91.4"
HANDWRITTEN DIGIT RECOGNITION,0.5694444444444444,"Sharp
POOLING
97.8
97.5
96.6
94.9
92.8
97.1
96.4
95.0
92.8
89.6
95.1
CHAIN
97.5
97.3
96.3
95.0
93.7
97.3
96.7
95.5
94.1
91.7
95.5
WEIGHTING 95.0
95.4
95.0
92.6
90.2
94.3
94.9
94.6
91.7
87.9
93.2"
HANDWRITTEN DIGIT RECOGNITION,0.5714285714285714,"Pooling-based Sharp+Reference
AFFINE
98.1
97.7
96.8
96.0
94.4
98.0
97.1
96.1
95.2
93.0
96.2"
HANDWRITTEN DIGIT RECOGNITION,0.5734126984126984,"the width with a scale factor of 1.8 before plugging them into the sharpener, which was efﬁciently
225"
HANDWRITTEN DIGIT RECOGNITION,0.5753968253968254,"achieved by running region generation with 180×32 images. The patch output by the STN had a size
226"
HANDWRITTEN DIGIT RECOGNITION,0.5773809523809523,"of 24×32.
227"
HANDWRITTEN DIGIT RECOGNITION,0.5793650793650794,"Table 1 shows that all sharp models signiﬁcantly outperform the baseline, demonstrating the efﬁcacy
228"
HANDWRITTEN DIGIT RECOGNITION,0.5813492063492064,"of clear alignment in attention mechanism. This can be easily seen from the pooling based model
229"
HANDWRITTEN DIGIT RECOGNITION,0.5833333333333334,"whose context vector purely results from the sharpener. Figure 3 also illustrates how attention can
230"
HANDWRITTEN DIGIT RECOGNITION,0.5853174603174603,"beneﬁt from the sharpener. Take the rotation case for example. To predict digit ‘8’ (the second
231"
HANDWRITTEN DIGIT RECOGNITION,0.5873015873015873,"column from left), hard attention chose a feature corresponding to a region ﬁlled with four digits. Due
232"
HANDWRITTEN DIGIT RECOGNITION,0.5892857142857143,"to the distraction of irrelevant digits (e.g., ‘6’, ‘2’ and ‘5’), the feature failed to precisely represent
233"
HANDWRITTEN DIGIT RECOGNITION,0.5912698412698413,"‘8’, thus giving wrong result. Although sharp attention selected the same region, it avoided most
234"
HANDWRITTEN DIGIT RECOGNITION,0.5932539682539683,"of the distraction by deliberately locating ‘8’ in that region, thus leading to more accurate and
235"
HANDWRITTEN DIGIT RECOGNITION,0.5952380952380952,"speciﬁc representation as well as correct prediction. Besides, the resulting attention also has better
236"
HANDWRITTEN DIGIT RECOGNITION,0.5972222222222222,"interpretability since it is more focused. The above results testify that the performance of Seq2Seq
237"
HANDWRITTEN DIGIT RECOGNITION,0.5992063492063492,"models can be largely improved when attention mechanism is really focused on the object of interest.
238"
HANDWRITTEN DIGIT RECOGNITION,0.6011904761904762,"To show that the sharpener allows for external supervision, a set of 24×32 reference images for each
239"
HANDWRITTEN DIGIT RECOGNITION,0.6031746031746031,"digit (see Fig. 3) was created with the Roboto Bold typeface of font size 36.4 The supervision was
240"
HANDWRITTEN DIGIT RECOGNITION,0.6051587301587301,"achieved by introducing an L2 image similarity loss of a weight of 1.0 to (9) to minimise the intensity
241"
HANDWRITTEN DIGIT RECOGNITION,0.6071428571428571,"difference between the patch and the reference. By showing what a desired digit would look like, the
242"
HANDWRITTEN DIGIT RECOGNITION,0.6091269841269841,4The font is available at https://fonts.google.com/specimen/Roboto. We used Pygame for rendering.
HANDWRITTEN DIGIT RECOGNITION,0.6111111111111112,Under review as a conference paper at ICLR 2022
HANDWRITTEN DIGIT RECOGNITION,0.6130952380952381,Table 2: Recognition accuracy of all models for the scene text recognition datasets.
HANDWRITTEN DIGIT RECOGNITION,0.6150793650793651,"Model
IIIT
SVT
IC03
IC13
IC15
SP
Mean
Acc.
3000
647
867
857
1811
645"
HANDWRITTEN DIGIT RECOGNITION,0.6170634920634921,"Baseline
SOFT
77.9
78.8
87.8
86.1
61.4
62.5
75.8
HARD
77.6
78.8
89.2
86.0
59.9
63.9
75.9
Sharp+One STN
CHAIN
76.9
78.1
88.7
88.8
61.1
65.6
76.5
Sharp+Two STNs
POOLING
73.9
72.6
83.6
83.3
54.2
60.2
71.3
CHAIN
77.9
80.1
89.0
87.7
62.0
65.1
77.0
WEIGHTING
77.7
78.5
89.0
87.4
61.5
64.0
76.4"
HANDWRITTEN DIGIT RECOGNITION,0.6190476190476191,"c
o
t
t
a
g
e"
HANDWRITTEN DIGIT RECOGNITION,0.621031746031746,"a
t
m
o
s
p
h
e
r
e"
HANDWRITTEN DIGIT RECOGNITION,0.623015873015873,"f
i
n
n
e
s
s"
HANDWRITTEN DIGIT RECOGNITION,0.625,"s
o
r
a
g
e"
HANDWRITTEN DIGIT RECOGNITION,0.626984126984127,"(a) Success
(b) Failures"
HANDWRITTEN DIGIT RECOGNITION,0.628968253968254,"Figure 4: Examples of recognition results for scene text recognition. All real-world testing images
are shown as is without rescaling and grey scale conversion. We leverage the chain based sharp model
to generate the patches localised by the sharpener and predicted tokens (failures highlighted in red)
across time steps. The results from other sharp models look similar to what has been shown here."
HANDWRITTEN DIGIT RECOGNITION,0.6309523809523809,"pooling based sharpener gives better localisation where all digit shape is well preserved with little
243"
HANDWRITTEN DIGIT RECOGNITION,0.6329365079365079,"distraction, compared to its counterpart without such guidance (see Fig. 3). This, in turn, improves
244"
HANDWRITTEN DIGIT RECOGNITION,0.6349206349206349,"the representation of the context vector for alignment, leading to a boost of the model performance.
245"
HANDWRITTEN DIGIT RECOGNITION,0.6369047619047619,"Table 1 shows that such improvement is the most remarkable when handling images of more digits
246"
HANDWRITTEN DIGIT RECOGNITION,0.6388888888888888,"(e.g., 13). This is not surprising because the receptive ﬁelds in such case have more digits ﬁlled and
247"
HANDWRITTEN DIGIT RECOGNITION,0.6408730158730159,"thus make it difﬁcult for the baseline models to decide where to look. Even if compared with the best
248"
HANDWRITTEN DIGIT RECOGNITION,0.6428571428571429,"one amongst all sharp models trained without external supervision, i.e. the chain based model, the
249"
HANDWRITTEN DIGIT RECOGNITION,0.6448412698412699,"improvement is still noticeable, further demonstrating the importance of accurate and target-speciﬁc
250"
HANDWRITTEN DIGIT RECOGNITION,0.6468253968253969,"representation in attention mechanism.
251"
SCENE TEXT RECOGNITION,0.6488095238095238,"4.2
SCENE TEXT RECOGNITION
252"
SCENE TEXT RECOGNITION,0.6507936507936508,"To further show the effectiveness of the proposed sharp attention, we applied it to a real-world visual
253"
SCENE TEXT RECOGNITION,0.6527777777777778,"sequence learning task, scene text recognition. The training datasets include MJSynth (Jaderberg et al.,
254"
SCENE TEXT RECOGNITION,0.6547619047619048,"2014) and SynthText (Jaderberg et al., 2016), while the testing ones consist of IIIT5K-Words (Mishra
255"
SCENE TEXT RECOGNITION,0.6567460317460317,"et al., 2012), Street View Text (Wang et al., 2011), ICDAR2003 (Lucas et al., 2003), ICDAR2013
256"
SCENE TEXT RECOGNITION,0.6587301587301587,"(Karatzas et al., 2013), ICDAR2015 (Karatzas et al., 2015) and SVT Perspective (Phan et al., 2013),
257"
SCENE TEXT RECOGNITION,0.6607142857142857,"which are short for IIIT, SVT, IC03, IC13, IC15 and SP respectively. There are 8.9 million images
258"
SCENE TEXT RECOGNITION,0.6626984126984127,"in the MJSynth dataset and 5.5 million in SynthText by cropping text regions and ruling out non-
259"
SCENE TEXT RECOGNITION,0.6646825396825397,"alphanumeric characters. The number of images in each testing dataset is detailed in Table 2, where
260"
SCENE TEXT RECOGNITION,0.6666666666666666,"the three datasets, IC03, IC13 and IC15, were prepared by following the protocol in Baek et al. (2019).
261"
SCENE TEXT RECOGNITION,0.6686507936507936,"The total number of testing images is 7, 827. Note that some of these datasets (e.g., IC15 and SP) are
262"
SCENE TEXT RECOGNITION,0.6706349206349206,"quite challenging due to various nuisance factors, such as poor lighting and geometry change (Fig. 4).
263"
SCENE TEXT RECOGNITION,0.6726190476190477,"For fast evaluation of various model conﬁgurations, we randomly sampled 2 million labelled images
264"
SCENE TEXT RECOGNITION,0.6746031746031746,"from the MJSynth dataset. The sampling was done by following the distribution (i.e. histogram of
265"
SCENE TEXT RECOGNITION,0.6765873015873016,"the transcription length) of the original dataset. We set the maximum transcription length to 16 to
266"
SCENE TEXT RECOGNITION,0.6785714285714286,"enable a large batch size (i.e. 192) for robust training. For sharp models, we upsampled the regions
267"
SCENE TEXT RECOGNITION,0.6805555555555556,"along the width with a scale factor of 2.0 for better localisation. To explore the effects of using
268"
SCENE TEXT RECOGNITION,0.6825396825396826,"multiple STNs for localisation, we ﬁrst used two STNs to train all sharp models, and then just used
269"
SCENE TEXT RECOGNITION,0.6845238095238095,Under review as a conference paper at ICLR 2022
SCENE TEXT RECOGNITION,0.6865079365079365,"the second one to train a chain based model for comparison. To see whether localisation beneﬁts
270"
SCENE TEXT RECOGNITION,0.6884920634920635,"from a coarse-to-ﬁne search strategy, the ﬁrst STN was designed to estimate a simple transformation
271"
SCENE TEXT RECOGNITION,0.6904761904761905,"(i.e. x-direction translation) but output a large patch (i.e. 64×32), while the second STN had the same
272"
SCENE TEXT RECOGNITION,0.6924603174603174,"conﬁguration as used in Sect. 4.1. All models were trained for 400,000 iterations with the sampled
273"
SCENE TEXT RECOGNITION,0.6944444444444444,"dataset, and no reference images were used. To train the hard and sharp models we used λr = 0.1.
274"
SCENE TEXT RECOGNITION,0.6964285714285714,Table 3: Mean entropy of different attention mechanisms for the scene text recognition datasets.
SCENE TEXT RECOGNITION,0.6984126984126984,"Model
IIIT
SVT
IC03
IC13
IC15
SP
Mean"
SCENE TEXT RECOGNITION,0.7003968253968254,"SOFT
1.06
1.01
1.06
1.00
1.15
1.12
1.07
HARD
0.63
0.61
0.67
0.63
0.63
0.62
0.63
SHARP
0.42
0.36
0.42
0.39
0.38
0.38
0.39"
SCENE TEXT RECOGNITION,0.7023809523809523,Table 4: Sharp attention vs. soft attention in a published scene text recognition work.
SCENE TEXT RECOGNITION,0.7043650793650794,"Model
IIIT
SVT
IC03
IC13
IC15
SP
Mean Acc."
SCENE TEXT RECOGNITION,0.7063492063492064,"SOFT(Baek et al., 2019)
84.3
83.8
93.1
91.9
70.8
71.9
82.6
SHARP
83.9
85.8
94.3
92.2
71.3
73.6
83.5"
SCENE TEXT RECOGNITION,0.7083333333333334,"From Table 2, we see that the chain based one again works the best amongst all sharp models of two
275"
SCENE TEXT RECOGNITION,0.7103174603174603,"STNs. It beats the baseline models remarkably on most of the datasets, whereas the other two either
276"
SCENE TEXT RECOGNITION,0.7123015873015873,"moderately outperform or lag behind the baseline. It also beats its counterpart of single STN, even
277"
SCENE TEXT RECOGNITION,0.7142857142857143,"though the latter is slightly better than the baseline as well. The result from the winning sharp model
278"
SCENE TEXT RECOGNITION,0.7162698412698413,"further testiﬁes our hypothesis on clear alignment in large-scale real-world datasets, which is also
279"
SCENE TEXT RECOGNITION,0.7182539682539683,"revealed by Fig. 4. Whenever prediction is successfully performed, there is sensible localisation of
280"
SCENE TEXT RECOGNITION,0.7202380952380952,"the target token. In fact, the sharpener attempts to highlight the token by placing it in the patch centre
281"
SCENE TEXT RECOGNITION,0.7222222222222222,"(see Fig. 4 for examples surrounded by blue boxes). This is an encouraging result given that the
282"
SCENE TEXT RECOGNITION,0.7242063492063492,"sharpener was trained in a data-driven manner with merely sequential labelling (i.e. transcriptions).
283"
SCENE TEXT RECOGNITION,0.7261904761904762,"However, the localisation is by no means satisfactory since all patches have some sort of distractions,
284"
SCENE TEXT RECOGNITION,0.7281746031746031,"such as skewed target tokens and irrelevant parts of adjacent tokens. Both this observation and the
285"
SCENE TEXT RECOGNITION,0.7301587301587301,"beneﬁt of multi-STN suggest a potential increase in accuracy if the sharpener is properly designed
286"
SCENE TEXT RECOGNITION,0.7321428571428571,"such that it can produce good localisation as shown in Fig. 3, which is beyond the scope of this paper.
287"
SCENE TEXT RECOGNITION,0.7341269841269841,"In Fig. 3, we have shown sharp attention is more focused and yields better interpretability. This can
288"
SCENE TEXT RECOGNITION,0.7361111111111112,"be evaluated by H[at], entropy of the categorical distribution deﬁned by αt (Shankar & Sarawagi,
289"
SCENE TEXT RECOGNITION,0.7380952380952381,"2019). It measures attention uncertainty, that is, the lower entropy, the better alignment and thus
290"
SCENE TEXT RECOGNITION,0.7400793650793651,"interpretability. Averaging H[at] across all valid time steps leads to the entropy for an image. We
291"
SCENE TEXT RECOGNITION,0.7420634920634921,"reported the mean of such entropy on each testing dataset for various attention mechanisms in
292"
SCENE TEXT RECOGNITION,0.7440476190476191,"Table 3. The results clearly show that sharp attention indeed boosts interpretability since entropy is a
293"
SCENE TEXT RECOGNITION,0.746031746031746,"logarithmic metric. We only reported the entropy from the best sharp model in Table 2.
294"
SCENE TEXT RECOGNITION,0.748015873015873,"Finally, we used the full datasets (i.e. MJSynth & SynthText) to train a sharp model with the same
295"
SCENE TEXT RECOGNITION,0.75,"conﬁguration to the best one in Table 2. To fairly compare the proposed attention with other attention
296"
SCENE TEXT RECOGNITION,0.751984126984127,"mechanisms in existing scene text recognition works, we reported the performance of the sharp model
297"
SCENE TEXT RECOGNITION,0.753968253968254,"and a model (i.e. VGG+BiLSTM+Attn) based on soft attention trained with the same datasets by
298"
SCENE TEXT RECOGNITION,0.7559523809523809,"Baek et al. (2019) in Table 4. The two models share the same backbone and RNN decoder. They only
299"
SCENE TEXT RECOGNITION,0.7579365079365079,"differ in attention mechanism. Table 4 further shows the superiority of our method.
300"
CONCLUSION,0.7599206349206349,"5
CONCLUSION
301"
CONCLUSION,0.7619047619047619,"We have described a novel attention mechanism that is able to build clear alignment between relevant
302"
CONCLUSION,0.7638888888888888,"regions in the input image and the target output. This is achieved by a generic sharpener module that
303"
CONCLUSION,0.7658730158730159,"computes accurate representation of the targets across time steps. Experimental results show that a
304"
CONCLUSION,0.7678571428571429,"vanilla implementation of our method can signiﬁcantly beat soft and hard attention on both synthetic
305"
CONCLUSION,0.7698412698412699,"and real-world datasets in terms of performance and interpretability, without bells and whistles such
306"
CONCLUSION,0.7718253968253969,"as the auxiliary model in Ba et al. (2015) and sophisticated training schemes in Xu et al. (2015). We
307"
CONCLUSION,0.7738095238095238,"plan to apply our method to more visual sequence learning tasks in the future.
308"
CONCLUSION,0.7757936507936508,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.7777777777777778,"6
REPRODUCIBILITY STATEMENT
309"
REPRODUCIBILITY STATEMENT,0.7797619047619048,"The implementation details have been given in lines 185–202. Although most of the training
310"
REPRODUCIBILITY STATEMENT,0.7817460317460317,"parameters have been described in lines 207–212, some task-speciﬁc setting can be found in lines
311"
REPRODUCIBILITY STATEMENT,0.7837301587301587,"223–227 and 266–274 respectively. The generation of synthetic handwritten digit dataset has been
312"
REPRODUCIBILITY STATEMENT,0.7857142857142857,"detailed in lines 217–222, while the preparation of real-world scene text recognition datasets has
313"
REPRODUCIBILITY STATEMENT,0.7876984126984127,"been elaborated in lines 254–261.
314"
REFERENCES,0.7896825396825397,"REFERENCES
315"
REFERENCES,0.7916666666666666,"Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu
316"
REFERENCES,0.7936507936507936,"Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg,
317"
REFERENCES,0.7956349206349206,"Rajat Monga, Sherry Moore, Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete
318"
REFERENCES,0.7976190476190477,"Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: A system for large-scale
319"
REFERENCES,0.7996031746031746,"machine learning. In Proceedings of the 12th USENIX Conference on Operating Systems Design
320"
REFERENCES,0.8015873015873016,"and Implementation, pp. 265–283, 2016.
321"
REFERENCES,0.8035714285714286,"André Araujo, Wade Norris, and Jack Sim. Computing receptive ﬁelds of convolutional neural
322"
REFERENCES,0.8055555555555556,"networks. Distill, 2019. doi: 10.23915/distill.00021. https://distill.pub/2019/computing-
323"
REFERENCES,0.8075396825396826,"receptive-fields.
324"
REFERENCES,0.8095238095238095,"Jimmy Ba, Volodymyr Mnih, and Koray Kavukcuoglu. Multiple object recognition with visual
325"
REFERENCES,0.8115079365079365,"attention. In Proceedings of the International Conference on Learning Representations, 2015.
326"
REFERENCES,0.8134920634920635,"Jeonghun Baek, Geewook Kim, Junyeop Lee, Sungrae Park, Dongyoon Han, Sangdoo Yun,
327"
REFERENCES,0.8154761904761905,"Seong Joon Oh, and Hwalsuk Lee. What is wrong with scene text recognition model com-
328"
REFERENCES,0.8174603174603174,"parisons? Dataset and model analysis. In Proceedings of IEEE Conference on International
329"
REFERENCES,0.8194444444444444,"Conference on Computer Vision, pp. 4715–4723, 2019.
330"
REFERENCES,0.8214285714285714,"Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
331"
REFERENCES,0.8234126984126984,"learning to align and translate. In Yoshua Bengio and Yann LeCun (eds.), Proceedings of the
332"
REFERENCES,0.8253968253968254,"International Conference on Learning Representations, 2015.
333"
REFERENCES,0.8273809523809523,"Hareesh Bahuleyan, Lili Mou, Olga Vechtomova, and Pascal Poupart. Variational attention for
334"
REFERENCES,0.8293650793650794,"sequence-to-sequence models. In Proceedings of the International Conference on Computational
335"
REFERENCES,0.8313492063492064,"Linguistics, pp. 1672–1682, 2018.
336"
REFERENCES,0.8333333333333334,"Zhanzhan Cheng, Fan Bai, Yunlu Xu, Gang Zheng, Shiliang Pu, and Shuigeng Zhou. Focusing
337"
REFERENCES,0.8353174603174603,"attention: Towards accurate text recognition in natural images. In Proceedings of IEEE Conference
338"
REFERENCES,0.8373015873015873,"on International Conference on Computer Vision, pp. 5086–5094, 2017.
339"
REFERENCES,0.8392857142857143,"Yuntian Deng, Yoon Kim, Justin Chiu, Demi Guo, and Alexander M. Rush. Latent alignment and
340"
REFERENCES,0.8412698412698413,"variational attention. In Proceedings of Advances in Neural Information Processing Systems, pp.
341"
REFERENCES,0.8432539682539683,"9735–9747, 2018.
342"
REFERENCES,0.8452380952380952,"Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):
343"
REFERENCES,0.8472222222222222,"1735–1780, 1997.
344"
REFERENCES,0.8492063492063492,"Max Jaderberg, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Synthetic data and
345"
REFERENCES,0.8511904761904762,"artiﬁcial neural networks for natural scene text recognition. In Proceedings of NIPS Workshop on
346"
REFERENCES,0.8531746031746031,"Deep Learning, 2014.
347"
REFERENCES,0.8551587301587301,"Max Jaderberg, Karen Simonyan, Andrew Zisserman, and Koray Kavukcuoglu. Spatial transformer
348"
REFERENCES,0.8571428571428571,"networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett (eds.), Proceed-
349"
REFERENCES,0.8591269841269841,"ings of Advances in Neural Information Processing Systems, pp. 2017–2025, 2015.
350"
REFERENCES,0.8611111111111112,"Max Jaderberg, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Reading text in the wild
351"
REFERENCES,0.8630952380952381,"with convolutional neural networks. International Journal of Computer Vision, 116:1–20, 2016.
352"
REFERENCES,0.8650793650793651,"Dimosthenis Karatzas, Faisal Shafait, Seiichi Uchida, Masakazu Iwamura, Lluis Gomez i Bigorda,
353"
REFERENCES,0.8670634920634921,"Sergi Robles Mestre, Joan Mas, David Fernandez Mota, Jon Almazàn Almazàn, and Lluís Pere
354"
REFERENCES,0.8690476190476191,"de las Heras. ICDAR 2013 robust reading competition. In Proceedings of the International
355"
REFERENCES,0.871031746031746,"Conference on Document Analysis and Recognition, pp. 1484–1493, 2013.
356"
REFERENCES,0.873015873015873,Under review as a conference paper at ICLR 2022
REFERENCES,0.875,"Dimosthenis Karatzas, Lluis Gomez-Bigorda, Anguelos Nicolaou, Suman Ghosh, Andrew Bagdanov,
357"
REFERENCES,0.876984126984127,"Masakazu Iwamura, Jiri Matas, Lukas Neumann, Vijay Ramaseshan Chandrasekhar, Shijian
358"
REFERENCES,0.878968253968254,"Lu, Faisal Shafait, Seiichi Uchida, and Ernest Valveny. ICDAR 2015 competition on robust
359"
REFERENCES,0.8809523809523809,"reading. In Proceedings of the International Conference on Document Analysis and Recognition,
360"
REFERENCES,0.8829365079365079,"pp. 1156–1160, 2015.
361"
REFERENCES,0.8849206349206349,"Dieterich Lawson, Chung-Cheng Chiu, George Tucker, Colin Raffel, Kevin Swersky, and Navdeep
362"
REFERENCES,0.8869047619047619,"Jaitly. Learning hard alignments with variational inference. In Proceedings of the International
363"
REFERENCES,0.8888888888888888,"Conference on Acoustics, Speech and Signal Processing, pp. 5799–5803, 2018.
364"
REFERENCES,0.8908730158730159,"Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
365"
REFERENCES,0.8928571428571429,"recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.
366"
REFERENCES,0.8948412698412699,"Wei Liu, Chaofeng Chen, Kwan-Yee K. Wong, Zhizhong Su, and Junyu Han. STAR-Net: A spatial
367"
REFERENCES,0.8968253968253969,"attention residue network for scene text recognition. In Edwin R. Hancock Richard C. Wilson and
368"
REFERENCES,0.8988095238095238,"William A. P. Smith (eds.), Proceedings of British Machine Vision Conference, pp. 43.1–43.13,
369"
REFERENCES,0.9007936507936508,"2016.
370"
REFERENCES,0.9027777777777778,"S.M. Lucas, A. Panaretos, L. Sosa, A. Tang, S. Wong, and R. Young. ICDAR 2003 robust read-
371"
REFERENCES,0.9047619047619048,"ing competitions. In Proceedings of the International Conference on Document Analysis and
372"
REFERENCES,0.9067460317460317,"Recognition, pp. 682–687, 2003.
373"
REFERENCES,0.9087301587301587,"Minh-Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention-
374"
REFERENCES,0.9107142857142857,"based neural machine translation. In Proceedings of the International Conference on Empirical
375"
REFERENCES,0.9126984126984127,"Methods in Natural Language Processing, pp. 1412–1421, 2015.
376"
REFERENCES,0.9146825396825397,"Hongyuan Mei, Mohit Bansal, and Matthew R. Walter. What to talk about and how? Selective
377"
REFERENCES,0.9166666666666666,"generation using LSTMs with coarse-to-ﬁne alignment. In Proceedings of the Conference of the
378"
REFERENCES,0.9186507936507936,"North American Chapter of the Association for Computational Linguistics: Human Language
379"
REFERENCES,0.9206349206349206,"Technologies, pp. 720–730, 2016.
380"
REFERENCES,0.9226190476190477,"Anand Mishra, Karteek Alahari, and C. V. Jawahar. Scene text recognition using higher order
381"
REFERENCES,0.9246031746031746,"language priors. In Richard Bowden, John P. Collomosse, and Krystian Mikolajczyk (eds.),
382"
REFERENCES,0.9265873015873016,"Proceedings of British Machine Vision Conference, pp. 1–11, 2012.
383"
REFERENCES,0.9285714285714286,"Volodymyr Mnih, Nicolas Heess, Alex Graves, and Koray Kavukcuoglu. Recurrent models of visual
384"
REFERENCES,0.9305555555555556,"attention. In Proceedings of Advances in Neural Information Processing Systems, pp. 2204–2212,
385"
REFERENCES,0.9325396825396826,"2014.
386"
REFERENCES,0.9345238095238095,"Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, Ça˘glar G˙ulçehre, and Bing Xiang. Abstractive
387"
REFERENCES,0.9365079365079365,"text summarization using sequence-to-sequence RNNs and beyond. In Proceedings of the SIGNLL
388"
REFERENCES,0.9384920634920635,"Conference on Computational Natural Language Learning, pp. 280–290, 2016.
389"
REFERENCES,0.9404761904761905,"Trung Quy Phan, Palaiahnakote Shivakumara, Shangxuan Tian, and Chew Lim Tan. Recognizing text
390"
REFERENCES,0.9424603174603174,"with perspective distortion in natural scenes. In Proceedings of IEEE Conference on International
391"
REFERENCES,0.9444444444444444,"Conference on Computer Vision, pp. 569–576, 2013.
392"
REFERENCES,0.9464285714285714,"Shiv Shankar and Sunita Sarawagi. Posterior attention models for sequence to sequence learning. In
393"
REFERENCES,0.9484126984126984,"Proceedings of the International Conference on Learning Representations, 2019.
394"
REFERENCES,0.9503968253968254,"Shiv Shankar, Siddhant Garg, and Sunita Sarawagi. Surprisingly easy hard-attention for sequence
395"
REFERENCES,0.9523809523809523,"to sequence learning. In Proceedings of the International Conference on Empirical Methods in
396"
REFERENCES,0.9543650793650794,"Natural Language Processing, pp. 640–645, 2018.
397"
REFERENCES,0.9563492063492064,"Baoguang Shi, Xiang Bai, and Cong Yao. An end-to-end trainable neural network for image-based
398"
REFERENCES,0.9583333333333334,"sequence recognition and its application to scene text recognition. IEEE Transactions on Pattern
399"
REFERENCES,0.9603174603174603,"Analysis and Machine Intelligence, 39(11):2298–2304, 2017.
400"
REFERENCES,0.9623015873015873,"Baoguang Shi, Mingkun Yang, Xinggang Wang, Pengyuan Lyu, Cong Yao, and Xiang Bai. ASTER:
401"
REFERENCES,0.9642857142857143,"An attentional scene text recognizer with ﬂexible rectiﬁcation. IEEE Transactions on Pattern
402"
REFERENCES,0.9662698412698413,"Analysis and Machine Intelligence, 41(9):2035–2048, 2019.
403"
REFERENCES,0.9682539682539683,Under review as a conference paper at ICLR 2022
REFERENCES,0.9702380952380952,"Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks.
404"
REFERENCES,0.9722222222222222,"In Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger (eds.), Proceedings
405"
REFERENCES,0.9742063492063492,"of Advances in Neural Information Processing Systems, pp. 3104–3112, 2014.
406"
REFERENCES,0.9761904761904762,"Kai Wang, Boris Babenko, and Serge Belongie. End-to-end scene text recognition. In Proceedings of
407"
REFERENCES,0.9781746031746031,"IEEE Conference on International Conference on Computer Vision, pp. 1457–1464, 2011.
408"
REFERENCES,0.9801587301587301,"Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
409"
REFERENCES,0.9821428571428571,"learning. Machine Learning, 8(3–4):229–256, 1992.
410"
REFERENCES,0.9841269841269841,"Shijie Wu, Pamela Shapiro, and Ryan Cotterell. Hard non-monotonic attention for character-level
411"
REFERENCES,0.9861111111111112,"transduction. In Proceedings of the International Conference on Empirical Methods in Natural
412"
REFERENCES,0.9880952380952381,"Language Processing, pp. 4425–4438, 2018.
413"
REFERENCES,0.9900793650793651,"Kelvin Xu, Jimmy Lei Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov,
414"
REFERENCES,0.9920634920634921,"Richard S. Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation
415"
REFERENCES,0.9940476190476191,"with visual attention. In Proceedings of the International Conference on Machine Learning, pp.
416"
REFERENCES,0.996031746031746,"2048–2057, 2015.
417"
REFERENCES,0.998015873015873,"Matthew D. Zeiler. Adadelta: An adaptive learning rate method, 2012.
418"
