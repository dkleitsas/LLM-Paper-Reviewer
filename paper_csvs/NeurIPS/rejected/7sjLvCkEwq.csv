Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0021413276231263384,"Maximization of mutual information between the model’s input and output is
1"
ABSTRACT,0.004282655246252677,"formally related to “decisiveness” and “fairness” of the softmax predictions Bridle
2"
ABSTRACT,0.006423982869379015,"et al. (1991), motivating such unsupervised entropy-based losses for discriminative
3"
ABSTRACT,0.008565310492505354,"models. Recent self-labeling methods based on such losses represent the state of
4"
ABSTRACT,0.010706638115631691,"the art in deep clustering. First, we discuss a number of general properties of such
5"
ABSTRACT,0.01284796573875803,"entropy clustering methods, including their relation to K-means and unsupervised
6"
ABSTRACT,0.014989293361884369,"SVM-based techniques. Disproving some earlier published claims, we point out
7"
ABSTRACT,0.017130620985010708,"fundamental differences with K-means. On the other hand, we show similarity
8"
ABSTRACT,0.019271948608137045,"with SVM-based clustering allowing us to link explicit margin maximization to
9"
ABSTRACT,0.021413276231263382,"entropy clustering. Finally, we observe that the common form of cross-entropy is
10"
ABSTRACT,0.023554603854389723,"not robust to pseudo-label errors. Our new loss addresses the problem and leads to
11"
ABSTRACT,0.02569593147751606,"a new EM algorithm improving the state of the art on many standard benchmarks.
12"
INTRODUCTION,0.027837259100642397,"1
Introduction
13"
INTRODUCTION,0.029978586723768737,"Discriminative entropy-based loss functions, e.g. decisiveness and fairness, were proposed for
14"
INTRODUCTION,0.032119914346895075,"network training Bridle et al. (1991); Krause et al. (2010) and regularization Grandvalet & Bengio
15"
INTRODUCTION,0.034261241970021415,"(2004) and are commonly used for unsupervised and weakly-supervised classification problems
16"
INTRODUCTION,0.03640256959314775,"Ghasedi Dizaji et al. (2017); Hu et al. (2017); Ji et al. (2019); Asano et al. (2020); Jabi et al. (2021).
17"
INTRODUCTION,0.03854389721627409,"In particular, the state-of-the-art in unsupervised classification Asano et al. (2020); Jabi et al. (2021)
18"
INTRODUCTION,0.04068522483940043,"is achieved by self-labeling methods using extensions of decisiveness and fairness.
19"
INTRODUCTION,0.042826552462526764,"Section 1.1 reviews the entropy-based clustering with soft-max models and introduces the necessary
20"
INTRODUCTION,0.044967880085653104,"notation. Then, Section 1.2 reviews the corresponding self-labeling formulations. Section 1.3
21"
INTRODUCTION,0.047109207708779445,"summarizes our main contributions and outlines the structure of the main parts of the paper.
22"
INTRODUCTION,0.04925053533190578,"1.1
Discriminative entropy clustering: background and notation
23"
INTRODUCTION,0.05139186295503212,"Consider neural networks using probability-type outputs, e.g. softmax σ : RK →∆K mapping
24"
INTRODUCTION,0.05353319057815846,"K logits lk ∈R to K-class probabilities σk =
exp lk
P"
INTRODUCTION,0.055674518201284794,"c exp lc forming a categorical distribution σ =
25"
INTRODUCTION,0.057815845824411134,"(σ1, . . . , σK) ∈∆K often interpreted as a posterior. We reserve superscripts to indicate classes or
26"
INTRODUCTION,0.059957173447537475,"categories. For shortness, this paper uses the same symbol for functions or mappings and examples of
27"
INTRODUCTION,0.06209850107066381,"their output, e.g. specific predictions σ. If necessary, subscript i can indicate values, e.g. prediction
28"
INTRODUCTION,0.06423982869379015,"σi or logit lk
i , corresponding to any specific input example Xi in the training dataset {Xi}N
i=1.
29"
INTRODUCTION,0.06638115631691649,"The mutual information (MI) loss, proposed by Bridle et al. (1991) for unsupervised discriminative
30"
INTRODUCTION,0.06852248394004283,"training of softmax models, trains the model output to keep as much information about the input
31"
INTRODUCTION,0.07066381156316917,"as possible. They derived MI estimate as the difference between the average entropy of the output
32"
INTRODUCTION,0.0728051391862955,"H(σ) = 1 N
P"
INTRODUCTION,0.07494646680942184,"i H(σi) and the entropy of the average output σ = 1 N
P"
INTRODUCTION,0.07708779443254818,"i σi, which is a distribution of
33"
INTRODUCTION,0.07922912205567452,"class predictions over the whole dataset
34"
INTRODUCTION,0.08137044967880086,"\
la
bel {e q:
m
i} L _ {mi}\;\;:=\;\;- MI(C,X)\;\;\;\;\approx \;\;\;\;\overline {H(\sigma )} \;-\; H(\overline {\sigma }) 
(1)"
INTRODUCTION,0.0835117773019272,"where C is a random variable representing the class prediction for input X. Besides the motivating
35"
INTRODUCTION,0.08565310492505353,"information-theoretic interpretation of the loss, the right-hand side in (1) has a clear discriminative
36"
INTRODUCTION,0.08779443254817987,"interpretation that stands on its own: H(σ) encourages “fair” predictions with a balanced support
37"
INTRODUCTION,0.08993576017130621,"of all categories across the whole training dataset, while H(σ) encourages confident or “decisive”
38"
INTRODUCTION,0.09207708779443255,"prediction at each data point suggesting that decision boundaries are away from the training examples
39"
INTRODUCTION,0.09421841541755889,"Grandvalet & Bengio (2004). Our paper refers to unsupervised training of discriminative soft-max
40"
INTRODUCTION,0.09635974304068523,"models using predictions’ entropies, e.g. see (1), as discriminative entropy clustering. This should
41"
INTRODUCTION,0.09850107066381156,"not be confused with generative entropy clustering methods where the entropy is used as a measure
42"
INTRODUCTION,0.1006423982869379,"of compactness for clusters’ density functions1.
43"
INTRODUCTION,0.10278372591006424,"Discriminative clustering loss (1) can be applied to deep or shallow models. For clarity, this
44"
INTRODUCTION,0.10492505353319058,"paper distinguishes parameters w of the representation layers of the network computing features
45"
INTRODUCTION,0.10706638115631692,"fw(X) ∈RM for any input X. We separate the linear classifier parameters v in the output layer
46"
INTRODUCTION,0.10920770877944326,"computing K-logit vector l = v⊤f for any feature f ∈RM. As mentioned earlier, this paper uses
47"
INTRODUCTION,0.11134903640256959,"the same notation for mapping f(·) and its values or (deep) features f produced by the representation
48"
INTRODUCTION,0.11349036402569593,"layers. For shortness, we assume a “homogeneous” representation of the linear classifier so that v⊤f
49"
INTRODUCTION,0.11563169164882227,"includes the bias. The overall network model is defined as
50"
INTRODUCTION,0.11777301927194861,"\label {eq:postmodel_deep} \sigma (\wc ^\top f_\wf (X)). 
(2)"
INTRODUCTION,0.11991434689507495,"A special “shallow” case of the model in (2) is a basic linear discriminator
51"
INTRODUCTION,0.12205567451820129,"\label {eq:postmodel_shallow} \sigma (\wc ^\top X) 
(3)"
INTRODUCTION,0.12419700214132762,"directly operating on given input features f(X) = X. In this case, M represents the input dimensions.
52"
INTRODUCTION,0.12633832976445397,"Optimization of the loss (1) for the shallow model (3) is done only over linear classifier parameters v,
53"
INTRODUCTION,0.1284796573875803,"but the deeper network model (2) is optimized over all network parameters [v, w]. Typically, this is
54"
INTRODUCTION,0.13062098501070663,"done via gradient descent or backpropagation Rumelhart et al. (1986); Bridle et al. (1991).
55"
INTRODUCTION,0.13276231263383298,"In the context of deep models (2), the decision boundaries between the clusters of data points {Xi}
56"
INTRODUCTION,0.1349036402569593,"can be arbitrarily complex since the network learns high-dimensional non-linear representation map
57"
INTRODUCTION,0.13704496788008566,"or embedding fw(X). In this case, loss (1) is optimized with respect to both representation w and
58"
INTRODUCTION,0.139186295503212,"classification v parameters. To avoid overly complex clustering of the training data and to improve
59"
INTRODUCTION,0.14132762312633834,"generality, it is common to use self-augmentation techniques Hu et al. (2017). For example, Ji et al.
60"
INTRODUCTION,0.14346895074946467,"(2019) maximize the mutual information between class predictions for input X and its augmentation
61"
INTRODUCTION,0.145610278372591,"counterpart X′ encouraging deep features invariant to augmentation.
62"
INTRODUCTION,0.14775160599571735,"To reduce the model’s complexity, Krause et al. (2010) combine entropy-based loss (1) with regular-
63"
INTRODUCTION,0.14989293361884368,ization of all network parameters interpreted as their isotropic Gaussian prior
INTRODUCTION,0.15203426124197003,"\nonumber
 
L_{m i
+dec
a y}\; \;\;"
INTRODUCTION,0.15417558886509636,"\;
 = &  \;\; \ ;\ ; \;\;  \overline {H(\sigma )} \;\;-\; \;\; H(\overline {\sigma }) \;\;\;\;\;\;+\; \| [\wc ,\wf ] \|^2 \\ \label {eq:mi+decay} \eqc & \;\;\;\;\;\;\overline {H(\sigma )} \;\;+\; KL(\overline {\sigma }\,\|\,u) \;\;+\; \|[\wc ,\wf ]\|^2
(4)"
INTRODUCTION,0.15631691648822268,"where
c= represents equality up to an additive constant and u is a uniform distribution over K classes.
65"
INTRODUCTION,0.15845824411134904,"The second loss formulation in (4) uses KL divergence motivated in Krause et al. (2010) by the
66"
INTRODUCTION,0.16059957173447537,"possibility to generalize the fairness to any target balancing distribution different from the uniform.
67"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.16274089935760172,"1.2
Self-labeling methods for entropy clustering
68"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.16488222698072805,"Optimization of losses (1) or (4) during network training is mostly done with standard gradient descent
69"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.1670235546038544,"or backpropagation Bridle et al. (1991); Krause et al. (2010); Hu et al. (2017). However, the difference
70"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.16916488222698073,"between the two entropy terms implies non-convexity, which makes such losses challenging for
71"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.17130620985010706,"gradient descent. This motivates alternative formulations and optimization approaches. For example,
72"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.1734475374732334,"it is common to extend the loss by incorporating auxiliary or hidden variables y representing pseudo-
73"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.17558886509635974,"labels for unlabeled data points X, which are to be estimated jointly with optimization of the network
74"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.1777301927194861,"parameters Ghasedi Dizaji et al. (2017); Asano et al. (2020); Jabi et al. (2021). Typically, such
75"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.17987152034261242,"self-labeling approaches to unsupervised network training iterate optimization of the loss over pseudo-
76"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.18201284796573874,"labels and network parameters, similarly to Lloyd’s algorithm for K-means or EM algorithm for
77"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.1841541755888651,"Gaussian mixtures Bishop (2006). While the network parameters are still optimized via gradient
78"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.18629550321199143,"descent, the pseudo-labels can be optimized via more powerful algorithms.
79"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.18843683083511778,"1E.g., K-means minimizes cluster variances, whose logs are cluster’s density entropies, assuming Gaussianity."
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.1905781584582441,"For example, Asano et al. (2020) formulate self-labeling using the following constrained optimization
80"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.19271948608137046,"problem with discrete pseudo-labels y tied to predictions by cross entropy function H(y, σ)"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.1948608137044968,"\la
b
ely{ eq
:vid
a l di
} L
_{c
e}  \;\;=\;\; \overline {H(y,\sigma )} \;\;\;\;\;\;\;\; s.t.\;\;\; y\in \Delta ^K_{0,1} \;\;\;and\;\;\; \bar {y}=u
(5)"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.19700214132762311,"where ∆K
0,1 are one-hot distributions, i.e. corners of the probability simplex ∆K. Training of the
82"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.19914346895074947,"network is done by minimizing cross entropy H(y, σ), which is convex w.r.t. σ, assuming fixed
83"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.2012847965738758,"pseudo-labels y. Then, model predictions get fixed and cross-entropy is minimized w.r.t variables
84"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.20342612419700215,"y. Note that cross-entropy H(y, σ) is linear with respect to y, and its minimum over simplex ∆K
85"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.20556745182012848,"is achieved by one-hot distribution for a class label corresponding to arg max(σ) at each training
86"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.20770877944325483,"example. However, the balancing constraint ¯y = u converts minimization of cross-entropy over all
87"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.20985010706638116,"data points into a non-trivial integer programming problem that can be approximately solved via
88"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.21199143468950749,"optimal transport Cuturi (2013). The cross-entropy in (5) encourages the network predictions σ to
89"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.21413276231263384,"approximate the estimated one-hot target distributions y, which implies the decisiveness.
90"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.21627408993576017,"Self-labeling methods for unsupervised clustering can also use soft pseudo-labels y ∈∆K as
91"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.21841541755888652,"target distributions inside H(y, σ). In general, soft targets y are commonly used with cross-entropy
92"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.22055674518201285,"functions H(y, σ), e.g. in the context of noisy labels Tanaka et al. (2018); Song et al. (2022). Softened
93"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.22269807280513917,"targets y can also assist network calibration Guo et al. (2017); Müller et al. (2019) and improve
94"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.22483940042826553,"generalization by reducing over-confidence Pereyra et al. (2017). In the context of unsupervised
95"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.22698072805139186,"clustering, cross entropy H(y, σ) with soft pseudo-labels y approximates the decisiveness since it
96"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.2291220556745182,"encourages σ ≈y implying H(y, σ) ≈H(y) ≈H(σ) where the latter is the decisiveness term in (1).
97"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.23126338329764454,"Inspired by (4), instead of the hard constraint ¯y = u used in (5), self-labeling losses can represent
98"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.2334047109207709,"the fairness using KL divergence KL(¯y ∥u), as in Ghasedi Dizaji et al. (2017); Jabi et al. (2021). In
99"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.23554603854389722,"particular, Jabi et al. (2021) formulates the following entropy-based self-labeling loss
  \"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.23768736616702354,"label 
{
eq:i sm
a il} L _ {ce+kl} \;\;= &\;\;\;\;\;\;\overline {H(y,\sigma )} \;\;\;\;\;+ \;\; KL(\Bar {y}\,\|\,u)
(6)"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.2398286937901499,"encouraging decisiveness and fairness, as discussed. Similarly to (5), the network parameters in loss
101"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.24197002141327623,"(6) are trained by the standard cross-entropy term. Optimization over relaxed pseudo-labels y ∈∆K
102"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.24411134903640258,"is relatively easy since KL divergence is convex and cross-entropy is linear w.r.t. y. While there
103"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.2462526766595289,"is no closed-form solution, the authors offer an efficient approximate solver for y. Iterating steps
104"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.24839400428265523,"that estimate pseudo-labels y and optimize the model parameters resemble the Lloyd’s algorithm for
105"
SELF-LABELING METHODS FOR ENTROPY CLUSTERING,0.2505353319057816,"K-means. Jabi et al. (2021) also establish a formal relation with K-means objective.
106"
SUMMARY OF OUR CONTRIBUTIONS,0.25267665952890794,"1.3
Summary of our contributions
107"
SUMMARY OF OUR CONTRIBUTIONS,0.25481798715203424,"Our work is closely related to self-labeling loss (6) and the corresponding ADM algorithm proposed
108"
SUMMARY OF OUR CONTRIBUTIONS,0.2569593147751606,"in Jabi et al. (2021). Their inspiring approach is a good reference point for our self-labeling loss
109"
SUMMARY OF OUR CONTRIBUTIONS,0.25910064239828695,"formulation (13). It also helps to illuminate the limits in a general understanding of entropy clustering.
110"
SUMMARY OF OUR CONTRIBUTIONS,0.26124197002141325,"Our paper provides conceptual and algorithmic contributions. First of all, we examine the relations of
111"
SUMMARY OF OUR CONTRIBUTIONS,0.2633832976445396,"discriminative entropy clustering to K-means and SVM. In particular, we disprove the main theoretical
112"
SUMMARY OF OUR CONTRIBUTIONS,0.26552462526766596,"claim (in the title) of a recent TPAMI paper Jabi et al. (2021) wrongly stating the equivalence between
113"
SUMMARY OF OUR CONTRIBUTIONS,0.2676659528907923,"the standard K-means objective and the entropy-based clustering losses. Our Figure 1 provides a
114"
SUMMARY OF OUR CONTRIBUTIONS,0.2698072805139186,"simple counterexample to the claim, but we also show specific technical errors in their proof. We
115"
SUMMARY OF OUR CONTRIBUTIONS,0.27194860813704497,"highlight fundamental differences with a broader generative group of clustering methods, which
116"
SUMMARY OF OUR CONTRIBUTIONS,0.2740899357601713,"includes K-means, GMM, etc. On the other hand, we find stronger similarities between entropy
117"
SUMMARY OF OUR CONTRIBUTIONS,0.2762312633832976,"clustering and discriminative SVM-based clustering. In particular, this helps to formally show the
118"
SUMMARY OF OUR CONTRIBUTIONS,0.278372591006424,"soft margin maximization effect when decisiveness is combined with a norm regularization term.
119"
SUMMARY OF OUR CONTRIBUTIONS,0.28051391862955033,"This paper also proposes a new self-labeling algorithm for entropy-based clustering. In the context
120"
SUMMARY OF OUR CONTRIBUTIONS,0.2826552462526767,"of relaxed pseudo-labels y, we observe that the standard formulation of decisiveness H(y, σ) is
121"
SUMMARY OF OUR CONTRIBUTIONS,0.284796573875803,"sensitive to pseudo-label uncertainty/errors. We motivate the reverse cross-entropy formulation,
122"
SUMMARY OF OUR CONTRIBUTIONS,0.28693790149892934,"which we demonstrate is significantly more robust to label noise. We also propose a zero-avoiding
123"
SUMMARY OF OUR CONTRIBUTIONS,0.2890792291220557,"form of KL-divergence as a strong fairness term. Unlike standard fairness, it does not tolerate highly
124"
SUMMARY OF OUR CONTRIBUTIONS,0.291220556745182,"unbalanced clusters. Our new self-labeling loss allows an efficient EM algorithm for estimating
125"
SUMMARY OF OUR CONTRIBUTIONS,0.29336188436830835,"pseudo-labels. We derive closed-form E and M steps. Our new algorithm improves the state-of-the-art
126"
SUMMARY OF OUR CONTRIBUTIONS,0.2955032119914347,"on many standard benchmarks for deep clustering, which empirically validates our technical insights.
127"
SUMMARY OF OUR CONTRIBUTIONS,0.29764453961456105,"Our paper is organized as follows. Section 2 discusses the relation of entropy clustering to K-means
128"
SUMMARY OF OUR CONTRIBUTIONS,0.29978586723768735,"and SVM. Section 3 motivates our self-labeling loss and derives an EM algorithm for estimating
129"
SUMMARY OF OUR CONTRIBUTIONS,0.3019271948608137,"pseudo-labels. The experimental results for our entropy clustering algorithm are in Section 4.
130"
SUMMARY OF OUR CONTRIBUTIONS,0.30406852248394006,TWO LINEAR DECISION FUNCTIONS OVER 2D FEATURES X ∈R2
SUMMARY OF OUR CONTRIBUTIONS,0.30620985010706636,"kµ(X) = arg mink ∥X −µk∥
σv(X) = soft-max(v⊤X)"
SUMMARY OF OUR CONTRIBUTIONS,0.3083511777301927,"(a) variance clustering
(b) entropy clustering"
SUMMARY OF OUR CONTRIBUTIONS,0.31049250535331907,"Figure 1: K-means vs entropy clustering - binary example (K = 2) for 2D data {Xi} ⊂RM (M = 2)
comparing linear methods of similar parametric complexity: (a) K-means [µk ∈RM] and (b) entropy
clustering based on a linear classifier using K-columns linear discriminator matrix v = [vk ∈RM]
and soft-max predictions. Red and green colors in (a) and (b) illustrate optimal linear decision
regions over X ∈R2 produced by the decision functions kµ(X), σv(X) for parameters µ and v
minimizing two losses: (a) compactness/variance of clusters P"
SUMMARY OF OUR CONTRIBUTIONS,0.31263383297644537,"i ∥Xi −µki∥2 where ki = kµ(Xi)
and (b) decisiveness and fairness of predictions H(σ) −H(¯σ) where H(·) is entropy function and
H(σ) = avg{H(σi)}, ¯σ = avg{σi} for σi = σv(Xi). The decision function kµ(X) is hard (a) and
σv(X) is soft, particularly near the linear decision boundary (b). The optimal results in (a,b) are
analyzed in Sec.2.1. The result in (b) may require a margin maximization term ∥v∥2, see Sec.2.2."
RELATION TO DISCRIMINATIVE AND GENERATIVE CLUSTERING METHODS,0.3147751605995717,"2
Relation to discriminative and generative clustering methods
131"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.3169164882226981,"2.1
Entropy-based clustering versus K-means
132"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.31905781584582443,"Discriminative entropy clustering (1) is not as widely known as K-means, but for no good reason.
133"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.32119914346895073,"With linear models (3), entropy clustering (1) is as simple as K-means, e.g. it produces linear cluster
134"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.3233404710920771,"boundaries. Both approaches have good approximate optimization algorithms for their non-convex
135"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.32548179871520344,"(1) or NP-hard Mahajan et al. (2012) objectives. Two methods also generalize to non-linear clustering
136"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.32762312633832974,"using more complex representations, e.g. learned fw(X) or implicit (kernel K-means).
137"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.3297644539614561,"There is a limited general understanding of how entropy clustering relates to more popular methods,
138"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.33190578158458245,"such as K-means. The prior work, including Bridle et al. (1991), mainly discusses entropy clustering
139"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.3340471092077088,"in the context of neural networks. K-means is also commonly used with deep features, but it is
140"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.3361884368308351,"hard to understand the differences in such complex settings. An illustrative 2D example of entropy
141"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.33832976445396146,"clustering in Krause et al. (2010) (Fig.1) is helpful, but it looks like a typical textbook example for
142"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.3404710920770878,"K-means where it would work perfectly. Interestingly, Jabi et al. (2021) make a theoretical claim
143"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.3426124197002141,"about algebraic equivalence between K-means objective and a regularized entropy clustering loss.
144"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.34475374732334046,"Here we show significant differences between K-means and entropy clustering. First, we disprove
145"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.3468950749464668,"the claim by Jabi et al. (2021). We provide a simple counterexample in Figure 1 where the optimal
146"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.3490364025695932,"solutions are different in a basic linear setting. Moreover, we point out a critical technical error in their
147"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.3511777301927195,"Proposition 2 - its proof ignores normalization inside softmax. Symbol ∝hides it in their equation (5),
148"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.3533190578158458,"which is later treated as equality in the proof of Proposition 2. Equations in their proof do not work
149"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.3554603854389722,"with normalization, which is critical for softmax models. The extra regularization term ∥v∥2 in their
150"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.3576017130620985,"entropy loss is also important. Without softmax normalization, ln σ inside cross-entropy H(y, σ)
151"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.35974304068522484,"turns into a linear term w.r.t. logits v⊤x and adding ∥v∥2 creates a quadratic form resembling squared
152"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.3618843683083512,"errors (x −v)2 in K-means. In contrast, Section 2.2 shows that regularization ∥v∥2 corresponds to
153"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.3640256959314775,"the margin maximization controlling the width of the soft gap between the clusters, see our Fig.1(b).
154"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.36616702355460384,"In general, Figure 1 highlights fundamental differences between generative and discriminative
155"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.3683083511777302,"approaches to clustering using two basic linear methods of similar parametric complexity (about
156"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.37044967880085655,"K × M parameters). K-means (a) seeks balanced compact clusters of the least variance (squared
157"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.37259100642398285,"errors). This can be interpreted “generatively” Kearns et al. (1997) as MLE fitting of two (isotropic)
158"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.3747323340471092,"Gaussian densities, which also explains why K-means fails on highly anisotropic clusters (a). To fix
159"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.37687366167023556,"this “generatively”, one should use non-isotropic Gaussian densities. In particular, 2-mode GMM
160"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.37901498929336186,"would produce soft clusters as in (b). But, this increases parametric complexity (two extra covariance
161"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.3811563169164882,"matrices) and leads to quadratic decision boundaries. In contrast, discriminative entropy clustering
162"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.38329764453961457,"in (b) simply looks for the best linear decision boundary giving balanced (“fair”) clusters with data
163"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.3854389721627409,"points away from the boundary (“decisiveness”), regardless of the data density model complexity.
164"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.3875802997858672,"2.2
Entropy-based clustering and SVM: margin maximization
165"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.3897216274089936,"This section discusses similarities between entropy clustering with soft-max models and unsupervised
166"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.39186295503211993,"SVM methods Ben-Hur et al. (2001); Xu et al. (2004). First, consider the fully supervised setting,
167"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.39400428265524623,"where the relationship between SVMs Vapnik (1995) and logistic regression is known. Assuming
168"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.3961456102783726,"binary classification with target labels t = ±1, one standard soft-margin SVM loss formulation
169"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.39828693790149894,"combines a margin-maximization term with the hinge loss penalizing margin violations, e.g. see
170"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.4004282655246253,"Bishop (2006)
171"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.4025695931477516,"\l a bel { e q:soft - S V M} L_{svm}\;=\; \gamma \|\wc \|^2 \;+\; \overline {\max \{0,1-t\,\wc ^\top f\}} 
(7)
where the linear classifier norm ∥v∥(excluding bias!) is the reciprocal of the decision margin and
172"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.40471092077087795,"γ is the relative weight of the margin maximization term. For shortness and consistently with the
173"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.4068522483940043,"notation introduced in Sec.1.1, logits v⊤f include the bias using “homogeneous” representations of
174"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.4089935760171306,"v and features f, and the “bar” operator represents averaging over all training data points.
175"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.41113490364025695,"Instead of the hinge loss, soft-margin maximization (7) can use the logistic regression as an alternative
176"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.4132762312633833,"soft penalty for margin violations, see Section 7.1.2 and Figure 7.5 in Bishop (2006),
177"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.41541755888650966,"\l a bel { e q:
l
o g istic} L_
{
log}\ ; =\; \gamma \|\wc \|^2 \;+\; \overline {\ln \left (1+\exp ^{-t\wc ^\top f}\right )} \;\;\equiv \;\; \gamma \|\wc \|^2 \;+\; \overline {H(y,\sigma )} 
(8)
where the second binary cross-entropy formulation in (8) replaces integer targets t ∈{±1} with
178"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.41755888650963596,"one-hot target distributions y ∈{(1, 0), (0, 1)} consistent with our general terminology in Sec.1.2.
179"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.4197002141327623,"Our second formulation in (8) uses soft-max σ = {
exp l1
exp l1+exp l2 ,
exp l2
exp l1+exp l2 } with logits l1 = 1"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.42184154175588867,"2v⊤f
180"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.42398286937901497,and l2 = −1
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.4261241970021413,"2v⊤f; its one advantage is a trivial multi-class generalization. The difference between
181"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.4282655246252677,"the soft-margin maximization losses (7) and (8) is that the flat region of the hinge loss leads to a
182"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.430406852248394,"sparse set of support vectors for the maximum margin solution, see Section 7.1.2 in Bishop (2006).
183"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.43254817987152033,"Now, consider the standard SVM-based self-labeling formulation of maximum margin clustering by
184"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.4346895074946467,"Xu et al. (2004). They combine loss (7) with a linear fairness constraint −ϵ ≤¯t ≤ϵ
185"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.43683083511777304,"\ l abel { eq:mm}  L _ {mm}\
;=\;
 \ g am m a \|\wc \|^2 \;+\; \overline {\max \{0,1-t\,\wc ^\top f\}}, \quad \text {s.t.}\quad -\epsilon \leq \bar {t}\leq \epsilon 
(9)
and treat labels t as optimization variables in addition to model parameters. Note that the hinge loss
186"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.43897216274089934,"encourages consistency between the pseudo labels t ∈{±1} and the sign of the logits v⊤f. Besides,
187"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.4411134903640257,"loss (9) still encourages maximum margin between the clusters. Keeping data points away from the
188"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.44325481798715205,"decision boundary is similar to the motivation for the decisiveness in entropy-based clustering.
189"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.44539614561027835,"It is easy to connect (9) to self-labeling entropy clustering. Similarly to (7) and (8), one can replace
190"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.4475374732334047,"the hinge loss by cross-entropy as an alternative margin-violation penalty. As before, the main
191"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.44967880085653106,"difference is that the margin may not be defined by a sparse subset of support vectors. We can also
192"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.4518201284796574,"replace the linear balancing constraint in (9) by an entropy-based fairness term. Then, we get
193"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.4539614561027837,"\la b el {e q :sem m}  L_{semm}\;=\; \gamma \|\wc \|^2 \;+\; \overline {H(y,\sigma )} \; - \;H(\bar {y}) y
(10)
which is a self-labeling surrogate for the entropy-based maximum-margin clustering loss
194"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.45610278372591007,"\l a bel { e q:em m } L_{emm}\;=\; \gamma \|\wc \|^2 \;+\; \overline {H(\sigma )} \; - \;H(\bar {\sigma }). 
(11)
Losses (11) and (10) are examples of general clustering losses for K ≥2 combining decisiveness and
195"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.4582441113490364,"fairness as in Sections 1.1, 1.2. The first term can be seen as a special case of the norm regularization
196"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.4603854389721627,"in (4). However, instead of a generic model simplicity argument used to justify (4), the specific
197"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.4625267665952891,"combination of cross-entropy with regularizer ∥v∥2 (excluding bias) in (11) and (10) is explicitly
198"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.46466809421841543,"linked to margin maximization where
1
∥v∥corresponds to the margin’s width2.
199"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.4668094218415418,"It was known that “for a poorly regularized classifier” the combination of decisiveness and fairness
200"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.4689507494646681,"“alone will not necessarily lead to good solutions to unsupervised classification” (Bridle et al. (1991))
201"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.47109207708779444,"and that decision boundary can tightly pass between the data points (Fig.1 in Krause et al. (2010)). The
202"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.4732334047109208,"formal relation to margin maximization above complements such prior knowledge. Our supplementary
203"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.4753747323340471,"material (A) shows the empirical effect of parameter γ in (11) on the inter-cluster gaps.
204"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.47751605995717344,"2The entropy clustering loss (6) is also appended with regularization ∥v∥2 in Jabi et al. (2021), where it is
incorrectly used for proving K-means connection, see Sec.2.1. They do not discuss margin maximization."
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.4796573875802998,"(a) strong fairness KL(u∥¯σ)
(b) reverse cross-entropy H(σ, y)"
ENTROPY-BASED CLUSTERING VERSUS K-MEANS,0.4817987152034261,"Figure 2: “Forward” vs “reverse”: (a) KL-divergence and (b) cross-entropy. Assuming binary
classification K = 2, probability distributions σ or ¯σ are represented as points on [0,1]. The solid
curves in (a) illustrate the forward KL-divergence KL(u∥¯σ) for average predictions ¯σ. We show
two examples of volumetric prior u1 = (0.9, 0.1) (blue) and u2 = (0.5, 0.5) (red). The reverse
KL-divergence KL(¯σ∥u) (dashed curves), commonly representing fairness in prior work, tolerates
extremely unbalanced clustering, i.e. the end points of the interval [0,1]. The solid curves in (b) are
the reverse cross-entropy H(σ, y) for predictions σ. The dashed curves are the forward cross-entropy
H(y, σ). The plots in (b) show examples for two fixed pseudo-labels y1 = (0.9, 0.1) (blue) and
y2 = (0.5, 0.5) (red). Our loss H(σ, y) weakens the training (reduces gradients) on data points with
higher label uncertainty (compare blue and red curves). In contrast, the standard loss H(y, σ) trains
the network to copy this uncertainty, see the optimum σ on the dashed curves. The boundedness of
H(σ, y) also represents robustness to errors in y."
OUR SELF-LABELING ENTROPY CLUSTERING METHOD,0.48394004282655245,"3
Our self-labeling entropy clustering method
205"
OUR SELF-LABELING ENTROPY CLUSTERING METHOD,0.4860813704496788,"The conceptual properties discussed in the previous section may improve the general understanding
206"
OUR SELF-LABELING ENTROPY CLUSTERING METHOD,0.48822269807280516,"of entropy clustering, but their new practical benefits are limited. For example, margin maximization
207"
OUR SELF-LABELING ENTROPY CLUSTERING METHOD,0.49036402569593146,"implicitly happens in prior entropy methods since norm regularization (weight-decay) is omnipresent.
208"
OUR SELF-LABELING ENTROPY CLUSTERING METHOD,0.4925053533190578,"This section addresses some specific limitations of prior entropy clustering formulations that do
209"
OUR SELF-LABELING ENTROPY CLUSTERING METHOD,0.49464668094218417,"affect the practical performance. We focus on self-labeling (Sec.1.2) and observe that the standard
210"
OUR SELF-LABELING ENTROPY CLUSTERING METHOD,0.49678800856531047,"cross-entropy formulation of decisiveness is sensitive to pseudo-label errors. Section 3.1 introduces
211"
OUR SELF-LABELING ENTROPY CLUSTERING METHOD,0.4989293361884368,"our new self-labeling loss using the reverse cross-entropy, which we show is more robust to label
212"
OUR SELF-LABELING ENTROPY CLUSTERING METHOD,0.5010706638115632,"noise. We also propose strong fairness. Section 3.2 derives an efficient EM algorithm for minimizing
213"
OUR SELF-LABELING ENTROPY CLUSTERING METHOD,0.5032119914346895,"our loss w.r.t. pseudo-labels, which is a critical step of our self-labeling algorithm.
214"
OUR SELF-LABELING LOSS FORMULATION,0.5053533190578159,"3.1
Our self-labeling loss formulation
215"
OUR SELF-LABELING LOSS FORMULATION,0.5074946466809421,"We start from the maximum-margin entropy clustering (10) where the entropy fairness can be replaced
216"
OUR SELF-LABELING LOSS FORMULATION,0.5096359743040685,"by an equivalent KL-divergence term explicitly expressing the target balance distribution u. This
217"
OUR SELF-LABELING LOSS FORMULATION,0.5117773019271948,"gives a self-labeling variant of the loss (4) in Krause et al. (2010) similar to (6) in Jabi et al. (2021)
218"
OUR SELF-LABELING LOSS FORMULATION,0.5139186295503212,"\la
be
l {e q: s emm2} L_{semm} \;\;\eqc \;\; \overline {H(y,\sigma )} \; + \; KL(\overline {y}\,\|\,u) \;\;+\; \gamma \, \|\wc \|^2. y
(12)"
OUR SELF-LABELING LOSS FORMULATION,0.5160599571734475,"We propose two changes to this loss based on several numerical insights leading to a significant
219"
OUR SELF-LABELING LOSS FORMULATION,0.5182012847965739,"performance improvement over Krause et al. (2010) and Jabi et al. (2021). First, we reverse the order
220"
OUR SELF-LABELING LOSS FORMULATION,0.5203426124197003,"of the cross-entropy arguments, see Fig.2(b). This improves the robustness of network predictions σ
221"
OUR SELF-LABELING LOSS FORMULATION,0.5224839400428265,"to errors in estimated pseudo-labels y, as confirmed by our experiment in Figure 3. This reversal also
222"
OUR SELF-LABELING LOSS FORMULATION,0.5246252676659529,"works for estimating pseudo-labels y as the second argument in cross-entropy is a standard position
223"
OUR SELF-LABELING LOSS FORMULATION,0.5267665952890792,"for an “estimated” distribution. Second, we also observe that the standard fairness term in (12,4,6)
224"
OUR SELF-LABELING LOSS FORMULATION,0.5289079229122056,"is the reverse KL divergence w.r.t. cluster volumes, i.e. the average predictions ¯σ. It can tolerate
225"
OUR SELF-LABELING LOSS FORMULATION,0.5310492505353319,"highly unbalanced solutions where ¯σk = 0 for some cluster k, see the dashed curves in Fig.2(a). We
226"
OUR SELF-LABELING LOSS FORMULATION,0.5331905781584583,"propose the forward, a.k.a. zero-avoiding, KL divergence KL(u ∥σ), see the solid curves Fig.2(a),
227"
OUR SELF-LABELING LOSS FORMULATION,0.5353319057815846,"which assigns infinite penalties to highly unbalanced clusters. We refer to this as strong fairness.
228"
OUR SELF-LABELING LOSS FORMULATION,0.5374732334047109,"The two changes above modify the clustering loss (12) into our formulation of self-labeling loss
  \"
OUR SELF-LABELING LOSS FORMULATION,0.5396145610278372,"labe
l 
{eq: se m m _our } yL _ { our}\;\; := & \;\;\;\;\;\;\overline {H(\sigma ,y)} \;\;+\; \lambda \, KL(u\,\|\,\overline {y}) \;\;+\; \gamma \, \|\wc \|^2.
(13)"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.5417558886509636,"3.2
Our EM algorithm for pseudo-labels
230"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.5438972162740899,"0.0
0.2
0.4
0.6
0.8
corruption level 25% 35% 45% 55% 65% 75% 85% 95%"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.5460385438972163,accuracy
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.5481798715203426,"forward CE: H(y,
)"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.550321199143469,"forward CE: H(y,
)"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.5524625267665952,"reverse CE: H( , y)"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.5546038543897216,"Figure 3: Robustness to noisy labels: reverse
H(σ, y) vs standard cross-entropy H(y, σ).
We train ResNet-18 on fully-supervised Nat-
ural Scene dataset [NSD] where we corrupted
some labels. The horizontal axis shows the
corruption level, i.e. percentage η of training
images where correct ground truth labels were
replaced by a random label. We use soft target
distributions ˜y = η∗u+(1−η)∗y that is a mix-
ture of one-hot distribution y for the observed
corrupt label and the uniform distribution u, as
in Müller et al. (2019). The vertical axis shows
the test accuracy. Reverse cross-entropy im-
proves robustness to high labeling errors."
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.556745182012848,"Minimization of a self-supervised loss w.r.t pseudo-
231"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.5588865096359743,"labels y for given predictions σ is a critical opera-
232"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.5610278372591007,"tion in iterative self-labeling techniques Asano et al.
233"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.563169164882227,"(2020); Jabi et al. (2021), see Sec.1.2. Besides well-
234"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.5653104925053534,"motivated numerical properties of our new loss (13),
235"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.5674518201284796,"in practice it also matters that it has an efficient
236"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.569593147751606,"solver for pseudo-labels. While (13) is convex w.r.t.
237"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.5717344753747323,"y, optimization is done over a probability simplex
238"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.5738758029978587,"and a good practical solver is not a given. Note
239"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.576017130620985,"that H(σ, y) works as a log barrier for the con-
240"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.5781584582441114,"straint y ∈∆K. This could be problematic for the
241"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.5802997858672377,"first-order methods, but a basic Newton’s method
242"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.582441113490364,"is a good match, e.g. Kelley (1995). The overall
243"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.5845824411134903,"convergence rate of such second-order methods is
244"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.5867237687366167,"fast, but computing the Hessian’s inverse is costly,
245"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.588865096359743,"see Table 1. Instead, we derive a more efficient
246"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.5910064239828694,"expectation-maximization (EM) algorithm.
247"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.5931477516059958,"Assume that model parameters and predictions in
248"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.5952890792291221,"(13) are fixed, i.e. v and σ. Following variational
249"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.5974304068522484,"inference Bishop (2006), we introduce K auxiliary
250"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.5995717344753747,"latent variables, distributions Sk ∈∆N represent-
251"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6017130620985011,"ing normalized support of each cluster k over N
252"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6038543897216274,"data points. In contrast, N distributions yi ∈∆K
253"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6059957173447538,"show support for each class at every point Xi. We
254"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6081370449678801,"refer to each vector Sk as a normalized cluster k. Note that here we focus on individual data points
255"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6102783725910065,"and explicitly index them by i ∈{1, . . . , N}. Thus, we use yi ∈∆K and σi ∈∆K. Individual
256"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6124197002141327,"components of distribution Sk ∈∆N corresponding to data point Xi is denoted by scalar Sk
i .
257"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6145610278372591,"First, we expand our loss (13) using our new latent variables Sk ∈∆N
  \"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6167023554603854,"labe
l 
{eq: EM  i niti al}  L _{ou
r}\;"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6188436830835118,"\
;&\e qc  \
;"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6209850107066381,"\
;\ ov
e"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6231263383297645,"r
li
n
e 
{
H(
\ s i g ma ,"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6252676659528907,"y
)}+\ la m b
d a "
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6274089935760171,"\
,H(u
, \b
ar
 
{y
} ) + \ gamma \,\|\wc \|^2 \\\nonumber &=\;\;\overline {H(\sigma ,y)}-\lambda \,\sum _{k}u^k\ln {\sum _{i}S_i^k\frac {y_i^k}{S_i^k N}}+\gamma \,\|\wc \|^2\\ &\leq \;\;\overline {H(\sigma ,y)}-\lambda \,\sum _{k}\sum _{i}u^k S_i^k \ln {\frac {y_i^k}{S_i^k N}}+\gamma \,\|\wc \|^2 \label {eq:EM derivation}
(15)"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6295503211991434,"Due to the convexity of negative log, we apply Jensen’s inequality to derive an upper bound, i.e. (15),
259"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6316916488222698,"to Lour. Such a bound becomes tight when:
260"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6338329764453962,"\lab e
l 
{ e
q:
E"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6359743040685225,"s te
p} \text {E-step}:\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;S_i^k=\frac {y_i^k}{\sum _{j}y_j^k}\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; (16)"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6381156316916489,"Then, we fix Sk
i as (16) and solve the Lagrangian of (15) with simplex constraint to update y as:
261"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6402569593147751,"\cen tering \label {eq:M step} \text {M-step}:\;\;\;\;\;\;\;\;\;\;\;\;y_i^k=\frac {\sigma _i^k+\lambda N u^k S_i^k}{1+\lambda N\sum _{c}u^c S_i^c}\;\;\;\;\;\;\;\;\;\;\;\;\;\; 
y
(17)"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6423982869379015,"We run these two steps until convergence with respect to some predefined tolerance.
Note
262"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6445396145610278,"that the minimum y is guaranteed to be globally optimal since (14) is convex w.r.t.
y.
263"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6466809421841542,"number of iterations
running time in sec.
(to convergence)
(to convergence)"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6488222698072805,"K
2
20
200
2
20
200"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6509635974304069,"Newton
3
3
4
2.8e−2
3.3e−2
1.7e−1"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6531049250535332,"EM
2
2
2
9.9e−4
2.0e−3
4.0e−3"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6552462526766595,"Table 1: Our EM algorithm vs Newton’s
methods Kelley (1995)."
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6573875802997858,"The empirical convergence rate is within 15 steps on
264"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6595289079229122,"MNIST. The comparison of computation speed on syn-
265"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6616702355460385,"thetic data is shown in Table 1. While the number of
266"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6638115631691649,"iterations to convergence is roughly the same as Newton’s
267"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6659528907922913,"methods, our EM algorithm is much faster in terms of
268"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6680942184154176,"running time and is extremely easy to implement using
269"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6702355460385439,"the highly optimized built-in functions from the standard
270"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6723768736616702,"PyTorch library that supports GPU.
271"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6745182012847966,"Inspired by Springenberg (2015); Hu et al. (2017), we also adapted our EM algorithm to allow
272"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6766595289079229,"for updating y within each batch. In fact, the mini-batch approximation of (14) is an upper bound.
273"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6788008565310493,"Considering the first two terms of (14), we can use Jensen’s inequality to get:
274"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6809421841541756,"\label {eq:batch approx} \overline {H(\sigma ,y)}+\lambda \,H(u,\bar {y})\;\;\leq \;\;\mathbb {E}_B[\overline {H_B(\sigma ,y)}+\lambda \,H(u,\bar {y}_B)] y
(18)"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.683083511777302,"where B is the batch randomly sampled from the whole dataset. Now, we can apply our EM algorithm
275"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6852248394004282,"to update y in each batch, which is even more efficient. Compared to other methods Ghasedi Dizaji
276"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6873661670235546,"et al. (2017); Asano et al. (2020); Jabi et al. (2021) which also use the auxiliary variable y, we can
277"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6895074946466809,"efficiently update y on the fly while they only update once or just a few times per epoch due to the
278"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6916488222698073,"inefficiency to update y for the whole dataset per iteration. Interestingly, we found that it is actually
279"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6937901498929336,"important to update y on the fly, which makes convergence faster and improves the performance
280"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.69593147751606,"significantly (see supplementary material). We use this “batch version” EM throughout all the
281"
OUR EM ALGORITHM FOR PSEUDO-LABELS,0.6980728051391863,"experiments. Our full algorithm for the loss (13) is summarized in supplementary material.
282"
EXPERIMENTAL RESULTS,0.7002141327623126,"4
Experimental results
283"
EXPERIMENTAL RESULTS,0.702355460385439,"Our experiments start from clustering on fixed features to joint training with feature learning. We test
284"
EXPERIMENTAL RESULTS,0.7044967880085653,"our approach on standard benchmark datasets with different network architectures. We also provide
285"
EXPERIMENTAL RESULTS,0.7066381156316917,"the comparison of different losses under weakly-supervised settings (see supplementary material).
286"
EXPERIMENTAL RESULTS,0.708779443254818,"Dataset
For the clustering problem, we use four standard benchmark datasets: MNIST Lecun et al.
287"
EXPERIMENTAL RESULTS,0.7109207708779444,"(1998), CIFAR10/100 Torralba et al. (2008) and STL10 Coates et al. (2011). We follow Ji et al.
288"
EXPERIMENTAL RESULTS,0.7130620985010707,"(2019) to use the whole dataset for training and testing unless otherwise specified.
289"
EXPERIMENTAL RESULTS,0.715203426124197,"Evaluation
As for the evaluation on clustering, we set the number of clusters to the number
290"
EXPERIMENTAL RESULTS,0.7173447537473233,"of ground-truth category labels and adopt the standard method Kuhn (1955) by finding the best
291"
EXPERIMENTAL RESULTS,0.7194860813704497,"one-to-one mapping between clusters and labels.
292"
CLUSTERING WITH FIXED FEATURES,0.721627408993576,"4.1
Clustering with fixed features
293"
CLUSTERING WITH FIXED FEATURES,0.7237687366167024,"We compare our method against the state-of-the-art methods using fixed deep features generated by
294"
CLUSTERING WITH FIXED FEATURES,0.7259100642398287,"pre-trained (ImageNet) ResNet-50 He et al. (2016). We use a one-layer linear classifier for all losses
295"
CLUSTERING WITH FIXED FEATURES,0.728051391862955,"except for K-means. We set λ in our loss to 100. We use stochastic gradient descent with learning rate
296"
CLUSTERING WITH FIXED FEATURES,0.7301927194860813,"0.1 to optimize the loss for 10 epochs. The batch size was set to 250. The coefficients for the margin
297"
CLUSTERING WITH FIXED FEATURES,0.7323340471092077,"maximization terms are set to 0.001, 0.02, 0.009, and 0.02 for MNIST, CIFAR10, CIFAR100 and
298"
CLUSTERING WITH FIXED FEATURES,0.734475374732334,"STL10 respectively. As stated in Section 2.2, such coefficient is important for the optimal decision
299"
CLUSTERING WITH FIXED FEATURES,0.7366167023554604,"boundary, especially when features are fixed. If we simultaneously learn the representation/feature
300"
CLUSTERING WITH FIXED FEATURES,0.7387580299785867,"and cluster the data, we observed that the results are less sensitive to such coefficient.
301"
CLUSTERING WITH FIXED FEATURES,0.7408993576017131,"STL10
CIFAR10
CIFAR100 (20)
MNIST"
CLUSTERING WITH FIXED FEATURES,0.7430406852248393,"K-means
85.20%(5.9)
67.78%(4.6)
42.99%(1.3)
47.62%(2.1)
MI-GD Bridle et al. (1991); Krause et al. (2010)
89.56%(6.4)
72.32%(5.8)
43.59%(1.1)
52.92%(3.0)
SeLa Asano et al. (2020)
90.33%(4.8)
63.31%(3.7)
40.74%(1.1)
52.38%(5.2)
MI-ADM Jabi et al. (2021)
81.28%(7.2)
56.07%(5.5)
36.70%(1.1)
47.15%(3.7)
MI-ADM⋆Jabi et al. (2021)
88.64%(7.1)
60.57%(3.3)
41.2%(1.4)
50.61%(1.3)"
CLUSTERING WITH FIXED FEATURES,0.7451820128479657,"Our
92.2%(6.2)
73.48%(6.2)
43.8%(1.1)
58.2%(3.1)"
CLUSTERING WITH FIXED FEATURES,0.7473233404710921,"Table 2: Comparison of different methods using fixed features. The numbers are the average accuracy
and the standard deviation over 6 trials. ⋆: our “batch version"" implementation of their method."
JOINT CLUSTERING AND FEATURE LEARNING,0.7494646680942184,"4.2
Joint clustering and feature learning
302"
JOINT CLUSTERING AND FEATURE LEARNING,0.7516059957173448,"In this section, we train a deep network to jointly learn the features and cluster the data. We test our
303"
JOINT CLUSTERING AND FEATURE LEARNING,0.7537473233404711,"method on both a small architecture (VGG4) and a large one (ResNet-18). The only extra standard
304"
JOINT CLUSTERING AND FEATURE LEARNING,0.7558886509635975,"technique we add here is the self-augmentation, following Hu et al. (2017); Ji et al. (2019); Asano
305"
JOINT CLUSTERING AND FEATURE LEARNING,0.7580299785867237,"et al. (2020). The experimental settings and more details are given in the supplementary material.
306"
JOINT CLUSTERING AND FEATURE LEARNING,0.7601713062098501,"To train the VGG4, we use random initialization for network parameters. From Table 3, it can
307"
JOINT CLUSTERING AND FEATURE LEARNING,0.7623126338329764,"be seen that our approach consistently achieves the most competitive results in terms of accuracy
308"
JOINT CLUSTERING AND FEATURE LEARNING,0.7644539614561028,"(ACC). Most of the methods we compared in our work (including our method) are general concepts
309"
JOINT CLUSTERING AND FEATURE LEARNING,0.7665952890792291,"applicable to single-stage end-to-end training. To be fair, we tested all of them on the same simple
310"
JOINT CLUSTERING AND FEATURE LEARNING,0.7687366167023555,"architecture. But, these general methods can be easily integrated into other more complex systems.
311"
JOINT CLUSTERING AND FEATURE LEARNING,0.7708779443254818,"STL10
CIFAR10
CIFAR100 (20)
MNIST"
JOINT CLUSTERING AND FEATURE LEARNING,0.7730192719486081,"MI-D⋆Hu et al. (2017)
25.28%(0.5)
21.4%(0.5)
14.39%(0.7)
92.90%(6.3)
IIC⋆Ji et al. (2019)
24.12%(1.7)
21.3%(1.4)
12.58%(0.6)
82.51%(2.3)"
JOINT CLUSTERING AND FEATURE LEARNING,0.7751605995717344,"SeLa§ Asano et al. (2020)
23.99%(0.9)
24.16%(1.5)
15.34%(0.3)
52.86%(1.9)"
JOINT CLUSTERING AND FEATURE LEARNING,0.7773019271948608,"MI-ADM§ Jabi et al. (2021)
17.37%(0.9)
17.27%(0.6)
11.02%(0.5)
17.75%(1.3)"
JOINT CLUSTERING AND FEATURE LEARNING,0.7794432548179872,"MI-ADM⋆,§ Jabi et al. (2021)
23.37%(0.9)
23.26%(0.6)
14.02%(0.5)
78.88%(3.3)"
JOINT CLUSTERING AND FEATURE LEARNING,0.7815845824411135,"Our⋆,§
25.33%(1.4)
24.16%(0.8)
15.09%(0.5)
93.58%(4.8)"
JOINT CLUSTERING AND FEATURE LEARNING,0.7837259100642399,"Table 3: Quantitative results of accuracy for unsupervised clustering methods with VGG4. We
only use the 20 coarse categories for CIFAR100. We reuse the code published by Ji et al. (2019);
Asano et al. (2020); Hu et al. (2017) and implemented the optimization for loss of Jabi et al. (2021)
according to the paper. ⋆: all variables are updated for each batch. §: loss formula has pseudo-label."
JOINT CLUSTERING AND FEATURE LEARNING,0.7858672376873662,"As for the training of ResNet-18, we found that random initialization does not work well when we only
312"
JOINT CLUSTERING AND FEATURE LEARNING,0.7880085653104925,"use self-augmentation. We may need more training tricks such as auxiliary over-clustering, multiple
313"
JOINT CLUSTERING AND FEATURE LEARNING,0.7901498929336188,"heads, and more augmentations Ji et al. (2019). In the mean time, the authors from Van Gansbeke
314"
JOINT CLUSTERING AND FEATURE LEARNING,0.7922912205567452,"et al. (2020) proposed a three-stage approach for the unsupervised classification and we found that
315"
JOINT CLUSTERING AND FEATURE LEARNING,0.7944325481798715,"the pre-trained weight from their first stage is beneficial to us. For a fair comparison, we followed
316"
JOINT CLUSTERING AND FEATURE LEARNING,0.7965738758029979,"their experimental settings and compared ours to their second-stage results. Note that they split the
317"
JOINT CLUSTERING AND FEATURE LEARNING,0.7987152034261242,"data into training and testing. We also report two additional evaluation metrics, i.e. NMI and ARI.
318"
JOINT CLUSTERING AND FEATURE LEARNING,0.8008565310492506,"In Table 4, we show the results using their pretext-trained network (stage one) as initialization for
319"
JOINT CLUSTERING AND FEATURE LEARNING,0.8029978586723768,"our entropy clustering. We use only our clustering loss together with the self-augmentation (one
320"
JOINT CLUSTERING AND FEATURE LEARNING,0.8051391862955032,"augmentation per image this time) to reach higher numbers than SCAN, as shown in the table below.
321"
JOINT CLUSTERING AND FEATURE LEARNING,0.8072805139186295,"CIFAR10
CIFAR100 (20)
STL10"
JOINT CLUSTERING AND FEATURE LEARNING,0.8094218415417559,"ACC
NMI
ARI
ACC
NMI
ARI
ACC
NMI
ARI"
JOINT CLUSTERING AND FEATURE LEARNING,0.8115631691648822,"SCAN Van Gansbeke et al. (2020)
81.8
(0.3)
71.2
(0.4)
66.5
(0.4)
42.2
(3.0)
44.1
(1.0)
26.7
(1.3)
75.5
(2.0)
65.4
(1.2)
59.0
(1.6)"
JOINT CLUSTERING AND FEATURE LEARNING,0.8137044967880086,"Our
83.09
(0.2)
71.65
(0.1)
68.05
(0.1)
46.79
(0.3)
43.27
(0.1)
28.51
(0.1)
77.67
(0.1)
67.66
(0.3)
61.26
(0.4)"
JOINT CLUSTERING AND FEATURE LEARNING,0.815845824411135,Table 4: Quantitative comparison using network ResNet-18.
CONCLUSIONS,0.8179871520342612,"5
Conclusions
322"
CONCLUSIONS,0.8201284796573876,"Our paper proposed a new self-labeling algorithm for discriminative entropy clustering, but we
323"
CONCLUSIONS,0.8222698072805139,"also clarify several important conceptual properties of this general methodology. For example, we
324"
CONCLUSIONS,0.8244111349036403,"disproved a theoretical claim in a recent TPAMI paper stating the equivalence between variance
325"
CONCLUSIONS,0.8265524625267666,"clustering (K-means) and discriminative entropy-based clustering. We also demonstrate that standard
326"
CONCLUSIONS,0.828693790149893,"formulations of entropy clustering losses may lead to narrow decision margins. Unlike prior work on
327"
CONCLUSIONS,0.8308351177730193,"discriminative entropy clustering, we show that classifier norm regularization is important for margin
328"
CONCLUSIONS,0.8329764453961456,"maximization.
329"
CONCLUSIONS,0.8351177730192719,"We also discussed several limitations of the existing self-labeling formulations of entropy clustering
330"
CONCLUSIONS,0.8372591006423983,"and propose a new loss addressing such limitations. In particular, we replace the standard (forward)
331"
CONCLUSIONS,0.8394004282655246,"cross-entropy by the reverse cross-entropy that we show is significantly more robust to errors in esti-
332"
CONCLUSIONS,0.841541755888651,"mated soft pseudo-labels. Our loss also uses a strong formulation of the fairness constraint motivated
333"
CONCLUSIONS,0.8436830835117773,"by a zero-avoiding version of KL divergence. Moreover, we designed an efficient EM algorithm
334"
CONCLUSIONS,0.8458244111349036,"minimizing our loss w.r.t. pseudo-labels; it is significantly faster than standard alternatives, e.g
335"
CONCLUSIONS,0.8479657387580299,"Newton’s method. Our empirical results improved the state-of-the-art on many standard benchmarks
336"
CONCLUSIONS,0.8501070663811563,"for deep clustering.
337"
REFERENCES,0.8522483940042827,"References
338"
REFERENCES,0.854389721627409,"Asano, Y. M., Rupprecht, C., and Vedaldi, A.
Self-labelling via simultaneous clustering and
339"
REFERENCES,0.8565310492505354,"representation learning. In International Conference on Learning Representations, 2020.
340"
REFERENCES,0.8586723768736617,"Ben-Hur, A., Horn, D., Siegelman, H., and Vapnik, V. Support vector clustering. Journal of Machine
341"
REFERENCES,0.860813704496788,"Learning Research, 2:125 – 137, 2001.
342"
REFERENCES,0.8629550321199143,"Bishop, C. M. Pattern Recognition and Machine Learning. Springer, 2006.
343"
REFERENCES,0.8650963597430407,"Bridle, J. S., Heading, A. J. R., and MacKay, D. J. C. Unsupervised classifiers, mutual information
344"
REFERENCES,0.867237687366167,"and ’phantom targets’. In NIPS, pp. 1096–1101, 1991.
345"
REFERENCES,0.8693790149892934,"Coates, A., Ng, A., and Lee, H. An analysis of single-layer networks in unsupervised feature learning.
346"
REFERENCES,0.8715203426124197,"In Proceedings of the fourteenth international conference on artificial intelligence and statistics,
347"
REFERENCES,0.8736616702355461,"pp. 215–223. JMLR Workshop and Conference Proceedings, 2011.
348"
REFERENCES,0.8758029978586723,"Cuturi, M. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural
349"
REFERENCES,0.8779443254817987,"information processing systems, 26, 2013.
350"
REFERENCES,0.880085653104925,"Ghasedi Dizaji, K., Herandi, A., Deng, C., Cai, W., and Huang, H. Deep clustering via joint
351"
REFERENCES,0.8822269807280514,"convolutional autoencoder embedding and relative entropy minimization. In Proceedings of the
352"
REFERENCES,0.8843683083511777,"IEEE international conference on computer vision, pp. 5736–5745, 2017.
353"
REFERENCES,0.8865096359743041,"Grandvalet, Y. and Bengio, Y. Semi-supervised learning by entropy minimization. Advances in
354"
REFERENCES,0.8886509635974305,"neural information processing systems, 17, 2004.
355"
REFERENCES,0.8907922912205567,"Guo, C., Pleiss, G., Sun, Y., and Weinberger, K. Q. On calibration of modern neural networks. In
356"
REFERENCES,0.892933618843683,"International conference on machine learning, pp. 1321–1330. PMLR, 2017.
357"
REFERENCES,0.8950749464668094,"He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings
358"
REFERENCES,0.8972162740899358,"of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.
359"
REFERENCES,0.8993576017130621,"Hu, W., Miyato, T., Tokui, S., Matsumoto, E., and Sugiyama, M. Learning discrete representations
360"
REFERENCES,0.9014989293361885,"via information maximizing self-augmented training. In International conference on machine
361"
REFERENCES,0.9036402569593148,"learning, pp. 1558–1567. PMLR, 2017.
362"
REFERENCES,0.9057815845824411,"Jabi, M., Pedersoli, M., Mitiche, A., and Ayed, I. B. Deep clustering: On the link between discrimi-
363"
REFERENCES,0.9079229122055674,"native models and k-means. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43
364"
REFERENCES,0.9100642398286938,"(6):1887–1896, 2021.
365"
REFERENCES,0.9122055674518201,"Ji, X., Henriques, J. F., and Vedaldi, A. Invariant information clustering for unsupervised image
366"
REFERENCES,0.9143468950749465,"classification and segmentation. In Proceedings of the IEEE/CVF International Conference on
367"
REFERENCES,0.9164882226980728,"Computer Vision, pp. 9865–9874, 2019.
368"
REFERENCES,0.9186295503211992,"Kearns, M., Mansour, Y., and Ng, A. Y. An information-theoretic analysis of hard and soft assignment
369"
REFERENCES,0.9207708779443254,"methods for clustering. In UAI ’97: Proceedings of the Thirteenth Conference on Uncertainty in
370"
REFERENCES,0.9229122055674518,"Artificial Intelligence, Brown University, Providence, Rhode Island, USA, August 1-3, 1997, pp.
371"
REFERENCES,0.9250535331905781,"282–293. Morgan Kaufmann, 1997.
372"
REFERENCES,0.9271948608137045,"Kelley, C. T. Iterative methods for linear and nonlinear equations. SIAM, 1995.
373"
REFERENCES,0.9293361884368309,"Krause, A., Perona, P., and Gomes, R. Discriminative clustering by regularized information maxi-
374"
REFERENCES,0.9314775160599572,"mization. Advances in neural information processing systems, 23, 2010.
375"
REFERENCES,0.9336188436830836,"Kuhn, H. W. The hungarian method for the assignment problem. Naval research logistics quarterly,
376"
REFERENCES,0.9357601713062098,"2(1-2):83–97, 1955.
377"
REFERENCES,0.9379014989293362,"Lecun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document
378"
REFERENCES,0.9400428265524625,"recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.
379"
REFERENCES,0.9421841541755889,"Mahajan, M., Nimbhorkar, P., and Varadarajan, K.
The planar K-means problem is NP-hard.
380"
REFERENCES,0.9443254817987152,"Theoretical Computer Science, 442:13–21, 2012.
381"
REFERENCES,0.9464668094218416,"Müller, R., Kornblith, S., and Hinton, G. E. When does label smoothing help? Advances in neural
382"
REFERENCES,0.9486081370449678,"information processing systems, 32, 2019.
383"
REFERENCES,0.9507494646680942,"NSD.
Natural
Scenes
Dataset
[NSD].
https://www.kaggle.com/datasets/
384"
REFERENCES,0.9528907922912205,"nitishabharathi/scene-classification, 2020.
385"
REFERENCES,0.9550321199143469,"Pereyra, G., Tucker, G., Chorowski, J., Kaiser, Ł., and Hinton, G. Regularizing neural networks by
386"
REFERENCES,0.9571734475374732,"penalizing confident output distributions. ICLR workshop, arXiv:1701.06548, 2017.
387"
REFERENCES,0.9593147751605996,"Rumelhart, D. E., Hinton, G. E., and Williams, R. J. Learning representations by back-propagating
388"
REFERENCES,0.961456102783726,"errors. Nature, 323(6088):533–536, 1986.
389"
REFERENCES,0.9635974304068522,"Song, H., Kim, M., Park, D., Shin, Y., and Lee, J.-G. Learning from noisy labels with deep neural
390"
REFERENCES,0.9657387580299786,"networks: A survey. IEEE Transactions on Neural Networks and Learning Systems, 2022.
391"
REFERENCES,0.9678800856531049,"Springenberg, J. T. Unsupervised and semi-supervised learning with categorical generative adversarial
392"
REFERENCES,0.9700214132762313,"networks. In International Conference on Learning Representations, 2015.
393"
REFERENCES,0.9721627408993576,"Tanaka, D., Ikami, D., Yamasaki, T., and Aizawa, K. Joint optimization framework for learning with
394"
REFERENCES,0.974304068522484,"noisy labels. In Proceedings of the IEEE conference on computer vision and pattern recognition,
395"
REFERENCES,0.9764453961456103,"pp. 5552–5560, 2018.
396"
REFERENCES,0.9785867237687366,"Torralba, A., Fergus, R., and Freeman, W. T. 80 million tiny images: A large data set for nonparametric
397"
REFERENCES,0.9807280513918629,"object and scene recognition. IEEE transactions on pattern analysis and machine intelligence, 30
398"
REFERENCES,0.9828693790149893,"(11):1958–1970, 2008.
399"
REFERENCES,0.9850107066381156,"Van Gansbeke, W., Vandenhende, S., Georgoulis, S., Proesmans, M., and Van Gool, L. Scan: Learning
400"
REFERENCES,0.987152034261242,"to classify images without labels. In Computer Vision–ECCV 2020: 16th European Conference,
401"
REFERENCES,0.9892933618843683,"Glasgow, UK, August 23–28, 2020, Proceedings, Part X, pp. 268–285. Springer, 2020.
402"
REFERENCES,0.9914346895074947,"Vapnik, V. The Nature of Statistical Learning Theory. Springer, 1995.
403"
REFERENCES,0.9935760171306209,"Xu, L., Neufeld, J., Larson, B., and Schuurmans, D. Maximum margin clustering. In Saul, L., Weiss,
404"
REFERENCES,0.9957173447537473,"Y., and Bottou, L. (eds.), Advances in Neural Information Processing Systems, volume 17. MIT
405"
REFERENCES,0.9978586723768736,"Press, 2004.
406"
