Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0018115942028985507,"Arguably the most fundamental question in the theory of generative adversarial
networks (GANs) is to understand when GANs can actually learn the underlying
distribution. Theoretical and empirical evidence (see e.g. (Arora et al., 2018)) sug-
gests local optimality of the empirical training objective is insufﬁcient. Yet, it does
not rule out the possibility that achieving a true population minimax optimal solu-
tion might imply distribution learning. In this paper, we show that standard cryp-
tographic assumptions imply this stronger condition is still insufﬁcient. Namely,
we show that if local pseudorandom generators (PRGs) exist, then for a large fam-
ily of natural target distributions, there are ReLU network generators of constant
depth and poly size which take Gaussian random seeds so that (i) the output is
far in Wasserstein distance from the target distribution, but (ii) no polynomially
large Lipschitz discriminator ReLU network can detect this. This implies that even
achieving a population minimax optimal solution to the Wasserstein GAN objec-
tive is likely insufﬁcient for distribution learning in the usual statistical sense. Our
techniques reveal a deep connection between GANs and PRGs, which we believe
will lead to further insights into the computational landscape of GANs."
INTRODUCTION,0.0036231884057971015,"1
INTRODUCTION"
INTRODUCTION,0.005434782608695652,"When will a generative adversarial network (GAN) trained with samples from a distribution D actu-
ally output samples from a distribution that is close to D? This question is one of the most founda-
tional questions in GAN theory—indeed, it was raised since the original paper introducing GANs.
However, despite signiﬁcant interest, this question still remains to be fully understood for general
classes of generators and discriminators."
INTRODUCTION,0.007246376811594203,"A signiﬁcant literature has developed discussing the role of the training dynamics (Liu et al., 2017;
Li et al., 2018; Arora et al., 2018; Berard et al., 2019; Wiatrak et al., 2019; Thanh-Tung & Tran,
2020; Allen-Zhu & Li, 2021), as well as the generalization error of the GAN objective (Zhang et al.,
2017; Arora et al., 2017; Thanh-Tung et al., 2019). In most cases, researchers have demonstrated
that given sufﬁcient training data, GANs are able to learn some speciﬁc form of distributions after
successful training. Underlying these works appears to be a tacit belief that if we are able to achieve
the minimax optimal solution to the population-level GAN objective, then the GAN should be able
to learn the target distribution. In this work, we take a closer look at this assumption."
INTRODUCTION,0.009057971014492754,"What does it mean to learn the target distribution? As a starting point, we must ﬁrst formally
deﬁne what we mean by learning a distribution; more concretely, what do we mean when we say that
two distributions are close? The original paper of (Goodfellow et al., 2020) proposed to measure
closeness with KL divergence. However, learning the target distribution in KL divergence is quite
unlikely to be satisﬁed for real-world distributions. This is because learning distributions in KL
divergence also requires us to exactly recover the support of the target distribution, which we cannot
really hope to do if the distribution lies in an unknown (complicated) low-dimensional manifold. To
rectify this, one may instead consider learning in Wasserstein distance, as introduced in the context
of GANs by (Arjovsky et al., 2017), which has no such “trivial” barriers. Recall that the Wasserstein
distance between two distributions D1, D2 over Rd is given by"
INTRODUCTION,0.010869565217391304,"W1(D1, D2) =
sup
Lip(f)≤1
ED1[f] −ED2[f] ,"
INTRODUCTION,0.012681159420289856,Published as a conference paper at ICLR 2022
INTRODUCTION,0.014492753623188406,"where for any f : Rd →R, we let Lip(f) denote the Lipschitz constant of f. That is, two densities
are close in Wasserstein distance if no Lipschitz function can distinguish between them. In this
work we will focus on Wasserstein distance as it is the most standard notion of distance between
probability distributions considered in the context of GANs."
INTRODUCTION,0.016304347826086956,"Note that if the class of discriminators contains sufﬁciently large neural networks, then minimax
optimality of the GAN objective does imply learning in Wasserstein distance. This is because we
can approximate any Lipschitz function arbitrarily well, with an exponentially large network with
one hidden layer (see e.g. (Poggio et al., 2017)). Thus, in this case, minimizing the population GAN
objective is actually equivalent to learning in Wasserstein distance. Of course in practice, however,
we are limited to polynomially large networks for both the generator and the discriminator. This
raises the natural question:"
INTRODUCTION,0.018115942028985508,"Does achieving small error against all poly-size neural network discriminators imply that the
poly-size generator has learned the distribution in Wasserstein distance?"
INTRODUCTION,0.019927536231884056,"One might conjecture that this claim is true, since the generator is only of poly-size. Thus, using
a (larger) poly-size discriminator (as opposite to the class of all 1-Lipschitz functions) might still
be sufﬁcient to minimize the actual Wasserstein distance. In this paper, however, we provide strong
evidence to the contrary. We demonstrate that widely accepted cryptographic assumptions imply
that this is false, even if the generator is of constant depth:
Theorem 1.1 (Informal, see Theorem 3.1). For any n ∈N, let γn be the standard Gaussian mea-
sure over Rn. Assuming local pseudorandom generators exist, the following holds for any sufﬁ-
ciently large m ∈Z, d, r ≤poly(m), and any diverse1 target distribution D∗over [0, 1]d given
by the pushforward of the uniform distribution Ir on [0, 1]r by a constant depth ReLU network of
polynomial size/Lipschitzness:2"
INTRODUCTION,0.021739130434782608,"There exist generators G : Rm →Rd computed by (deterministic) ReLU networks of constant
depth and polynomial size for which no ReLU network of polynomial depth/size/Lipschitzness can
tell apart the distributions G(γm) and D∗, yet G(γm) and D∗are Ω(1)-far in Wasserstein distance."
INTRODUCTION,0.02355072463768116,"While Theorem 1.1 pertains to the practically relevant setting of continuous seed and output dis-
tributions, we also give guarantees for the discrete setting. In fact, if we replace D∗and γm by
the uniform distributions over {±1}d and {±1}m, we show this holds for generators whose output
coordinates are given by constant-size networks (see Theorem 3.2)."
INTRODUCTION,0.025362318840579712,"We defer the formal deﬁnition of local pseudorandom generators (PRGs) to Section 2.2. We pause
to make a number of remarks about this theorem."
INTRODUCTION,0.02717391304347826,"First, our theorem talks about the population loss of the GAN objective; namely, it says that the
true population GAN objective is small for this generator G, meaning that for every ReLU network
discriminator f of polynomial depth/size/Lipschitzness, we have that"
INTRODUCTION,0.028985507246376812,"|E[f(D∗)] −E[f(G(γm))]| ≤
1
dω(1) ."
INTRODUCTION,0.030797101449275364,"In other words, our theorem states that even optimizing the true population minimax objective is
insufﬁcient for distribution learning. In fact, we show this even when the target distribution can be
represented perfectly by some other generative model."
INTRODUCTION,0.03260869565217391,"Second, notice that our generator is extremely simple: notably, it is only constant depth. On the other
hand, the discriminator is allowed to be much more complex, namely any ReLU network of poly-
nomial complexity. This discriminator class thus constitutes the most powerful family of functions
we could hope to use in practice. Despite this, we show that the discriminators are still not powerful
enough to distinguish the output of the (much simpler) generator from the target distribution."
INTRODUCTION,0.034420289855072464,"Third, our conclusions hold both for d ≥m and d ≤m, so long as the input and output dimensions
are related by polynomial factors."
INTRODUCTION,0.036231884057971016,"1See Deﬁnition 6. In the discussion proceeding this deﬁnition, we give a number of examples making clear
that this is a mild and practically relevant assumption to make.
2When we say “polynomial,” we are implicitly referring to the dependence on the parameter m, though
because d, r are bounded by poly(m), “polynomial” could equivalently refer to the dependence on those pa-
rameters if they exceeded m."
INTRODUCTION,0.03804347826086957,Published as a conference paper at ICLR 2022
INTRODUCTION,0.03985507246376811,"Finally, we formally deﬁne the class of “diverse” target distributions for which our conclusions hold
in Section 2.3. We note that this class is quite general: for instance, it includes pushforwards of the
uniform distribution under random leaky ReLU networks (see Lemma 2.3)."
INTRODUCTION,0.041666666666666664,"Empirical results. To complement these theoretical results, we also perform some empirical vali-
dations of our ﬁndings (see Section 4). Our theorem is constructive; that is, given a local PRG, we
give an explicit generator which satisﬁes the theorem. We instantiate this construction with Goldre-
ich’s PRG with the “Tri-Sum-And” (TSA) predicate (Goldreich, 2011), which is an explicit function
which is believed to satisfy the local PRG property. We then demonstrate that a neural network dis-
criminator trained via standard methods empirically cannot distinguish between the output of this
generator and the uniform distribution. While of course we cannot guarantee that we achieve the
truly optimal discriminator using these methods, this still demonstrates that our construction leads
to a function which does appear to be hard to distinguish in practice."
INTRODUCTION,0.043478260869565216,"GANs, PRGs, and circuit lower bounds. At a high level, our results and techniques demonstrate
surprising and deep connections between GANs and more “classical” problems in cryptography
and complexity theory. Theorem 1.1 already shows that cryptographic assumptions may pose a
fundamental barrier to the most basic question in GAN theory. In addition to this, we also show
a connection between this question and circuit lower bounds. In the supplementary material, we
show that if we are able to unconditionally exhibit generators which can fool polynomially large
ReLU network discriminators, then we would obtain breakthrough circuit lower bounds against
TC0 (see Theorem C.2 and Remark C.3). This complements Theorem 1.1, as it says that if we can
unconditionally construct generators which fool realistic discriminators, then we make progress on
long-standing questions in circuit complexity. We believe that exploring these connections may be
crucial to achieving a deeper understanding of what GANs can and cannot learn."
RELATED WORK,0.04528985507246377,"1.1
RELATED WORK"
RELATED WORK,0.04710144927536232,"GANs and Distribution Learning
The literature on GAN theory is vast and we cannot hope to
do it full justice here. For a more extensive review, see e.g. (Gui et al., 2020). Besides the previously
mentioned work on understanding GAN dynamics and generalization, we only mention the most
relevant papers here. One closely related line of work derives concrete bounds on when minimax
optimality of the GAN objective implies distribution learning (Bai et al., 2018; Liang, 2018; Singh
et al., 2018; Uppal et al., 2019; Chen et al., 2020a; Schreuder et al., 2021). However, the rates they
achieve scale poorly with the dimensionality of the data, and/or require strong assumptions on the
class of generators and discriminators, such as invertability. Another line of work has demonstrated
that ﬁrst order methods can learn very simple GAN architectures in polynomial time (Feizi et al.,
2017; Daskalakis et al., 2017; Gidel et al., 2019; Lei et al., 2020). However, these results do not
cover many of the generators used in practice, such as ReLU networks with > 1 hidden layers."
RELATED WORK,0.04891304347826087,"Local PRGs and Learning
PRGs have had a rich history of study in cryptography and complex-
ity theory (see e.g. (Vadhan, 2012)). From this literature, the object most relevant to the present
work is the notion of a local PRG. These are part of a broader research program of building constant
parallel-time cryptography (Applebaum et al., 2006). One popular local PRG candidate was sug-
gested in (Goldreich, 2011). By now there is compelling evidence that this candidate is a valid PRG,
as a rich family of algorithms including sum-of-squares (ODonnell & Witmer, 2014) and statistical
query algorithms (Feldman et al., 2018) provably cannot break it."
RELATED WORK,0.050724637681159424,"Finally, we remark that Goldreich’s PRG and, more generally, hardness of refuting random CSPs
have been used in a number of works showing hardness for various supervised learning problems
(Daniely & Vardi, 2021; Daniely et al., 2014; Daniely, 2016; Daniely & Shalev-Shwartz, 2016;
Applebaum et al., 2006; Applebaum & Raykov, 2016). We consider a very different setting, and our
techniques are very different from the aforementioned papers."
TECHNICAL PRELIMINARIES,0.05253623188405797,"2
TECHNICAL PRELIMINARIES"
TECHNICAL PRELIMINARIES,0.05434782608695652,"Notation
Denote by Un the uniform distribution over {±1}n, by γn the standard n-dimensional
Gaussian measure, and by In the uniform measure on [0, 1]n. Given distribution D over Rm and
measurable function G : Rm →Rd, let G(D) denote the distribution over Rd given by the push-"
TECHNICAL PRELIMINARIES,0.05615942028985507,Published as a conference paper at ICLR 2022
TECHNICAL PRELIMINARIES,0.057971014492753624,"forward of D under G—to sample from the pushforward G(D), sample x from D and output G(x).
Given σ : R →R and vector v ∈Rd, denote by σ(v) ∈Rd the result of applying σ entrywise to v.
To avoid dealing with issues of real-valued computation, let Rτ ⊂R be the set of multiples of 2−τ
bounded in magnitude by 2τ."
GANS AND PSEUDORANDOM GENERATORS,0.059782608695652176,"2.1
GANS AND PSEUDORANDOM GENERATORS"
GANS AND PSEUDORANDOM GENERATORS,0.06159420289855073,"In this section we review basic notions about generative models and PRGs.
Deﬁnition 1 (ReLU Networks). Let CL,S,d denote the family of ReLU networks F : Rd →R of
depth L and size S. Formally, F ∈CL,S,d if there exist weight matrices W1 ∈Rk1×d, W2 ∈
Rk2×k1, . . . , WL ∈R1×kL−1 and biases b1 ∈Rk1, b2 ∈Rk2 . . . , bL ∈R such that"
GANS AND PSEUDORANDOM GENERATORS,0.06340579710144928,"F(x) ≜WLφ (WL−1φ (· · · φ(W1x + b1) · · · ) + bL−1) + bL,"
GANS AND PSEUDORANDOM GENERATORS,0.06521739130434782,"and PL−1
i=1 ki = S, where φ(z) ≜max(0, z) is the ReLU activation. Additionally, we let Cτ,Λ
L,S,d
be the subset of such networks which are additionally Λ-Lipschitz and whose weight matrices and
biases have entries in Rτ– we will refer to τ as the bit complexity of the network."
GANS AND PSEUDORANDOM GENERATORS,0.06702898550724638,"The following allows us to control the complexity of compositions of ReLU networks:
Lemma 2.1. Let J : Rs →Rr be a function each of whose output coordinates is computed by some
network in Cτ1,Λ1
L1,S1,s, and let f ∈Cτ2,Λ2
L2,S2,r. Then f ◦J ∈Cτ,Λ
L,S,s for τ = max(τ1, τ2), Λ = Λ1Λ2
√r,
L = L1 + L2, and S = (S1 + 1)r + S2. Furthermore, for the network in Cτ,Λ
L,S,s realizing f ◦J, the
bias and weight vector entries in the output layer lie in Rτ2."
GANS AND PSEUDORANDOM GENERATORS,0.06884057971014493,"Next, we formalize the probability metric we will work with.
Deﬁnition 2 (IPM). Given a family F of functions, deﬁne the F-integral probability metric between
two distributions p, q by WF(p, q) = supf∈F|Ey∼p[f(y)] −Ey∼q[f(y)]|. When F consists of the
family of 1-Lipschitz functions, this is the standard Wasserstein-1 metric, which we denote by W1."
GANS AND PSEUDORANDOM GENERATORS,0.07065217391304347,"In the context of GANs, we will focus on discriminators given by ReLU networks of polynomial
size, depth, Lipschitzness, and bit complexity:
Deﬁnition 3 (Discriminators). F∗denotes the set of all sequences of discriminators fd : Rd →R,
indexed by d ∈N, whose size, depth, Lipschitzness, bit complexity grow at most polynomially in d."
GANS AND PSEUDORANDOM GENERATORS,0.07246376811594203,"We now formalize the deﬁnition of GANs, which closely parallels the deﬁnition of PRGs.
Deﬁnition 4 (GANs/PRGs). Let ϵ : N →[0, 1] be an arbitrary function, and let {d(m)}m∈N be
some sequence of positive integers. Given a sequence of seed distributions {Dm}m over Rm, a
sequence of target distributions {D∗
d(m)} over Rd(m), a family F of discriminators f : Rd(m) →R,
and a sequence of generators Gm : Rm →Rd(m), we say that {Gm} ϵ-fools F relative to {D∗
d(m)}
with seed {Dm} if for all sufﬁciently large m,
E[f(Gm(Dm))] −E[f(D∗
d(m))]
 ≤ϵ(m) ∀f ∈F, f : Rd(m) →R.
(1)"
GANS AND PSEUDORANDOM GENERATORS,0.07427536231884058,"Note that in the notation of Deﬁnition 2, (1) is equivalent to WF(Gm(Dm), D∗
d(m)) ≤ϵ(m).3"
GANS AND PSEUDORANDOM GENERATORS,0.07608695652173914,"In this deﬁnition, if the discriminators and generators were instead Boolean functions, we would
refer to {Gm} as pseudorandom generators.
Remark 2.2. It will often be cumbersome to refer to sequences of target/seed distributions and
discriminators/generators as in Deﬁnitions 3 and 4, so occasionally we will refer to a single choice
of m and d even though we implicitly mean that m and d are parameters that increase towards
inﬁnity. In this vein, we will often say that a single network f is in F∗, though we really mean that
f belongs to a sequence of networks which lies in F∗. And for distributions p, q which implicitly
belong to sequences {pd}, {qd}, when we refer to bounds on WF∗(p, q) we really mean that for any
sequence of discriminators fd ∈F∗, |E[fd(pd)] −E[fd(qd)]| is bounded."
GANS AND PSEUDORANDOM GENERATORS,0.07789855072463768,"3Here we are slightly abusing notation as F contains functions which have domain not equal to Rd(m), and
we ignore these functions in the supremum."
GANS AND PSEUDORANDOM GENERATORS,0.07971014492753623,Published as a conference paper at ICLR 2022
LOCAL PSEUDORANDOM GENERATORS,0.08152173913043478,"2.2
LOCAL PSEUDORANDOM GENERATORS"
LOCAL PSEUDORANDOM GENERATORS,0.08333333333333333,"In complexity theory, the role of neural network discriminators is played by Boolean circuits, which
one can roughly think of as networks which take in Boolean strings as input and whose activations
are logical operations (e.g. AND/OR/NOT) rather than ReLUs (see Section A.2 in the supplement)."
LOCAL PSEUDORANDOM GENERATORS,0.08514492753623189,"It is widely believed that there exist so-called local PRGs capable of fooling all polynomial-sized
Boolean circuits and all of whose output coordinates are functions of a constant number of input
coordinates (Applebaum et al., 2006). One prominent candidate is Goldreich’s PRG:
Deﬁnition 5 ((Goldreich, 2011)). Let H be a collection of d subsets S1, . . . , Sd of {1, . . . , m}, each
of size k and each sampled independently from the uniform distribution over subsets of {1, . . . , m}
of size k. Let P : {±1}k →{±1} be a Boolean function; P is often referred to as a predicate."
LOCAL PSEUDORANDOM GENERATORS,0.08695652173913043,"Let GP,H : {±1}m →{±1}d denote the Boolean function whose ℓ-th output coordinate is com-
puted by evaluating P on the coordinates of the input indexed by subset Sℓin H."
LOCAL PSEUDORANDOM GENERATORS,0.08876811594202899,"The following is a standard assumption in cryptography (see (Applebaum, 2016)), namely that the
incredibly simple functions in Deﬁnition 5 fool all Boolean circuits of polynomial size:
Assumption 1. For any c > 1, there exists k ∈N and P : {±1}k →{±1} such that for m
sufﬁciently large, with probability 1 −om(1) over the randomness of H in Deﬁnition 5, the function
G = GP,H negl(m)-fools all polynomial-size Boolean circuits relative to Umc with seed Um for
some negligible function negl : N →[0, 1].4"
DIVERSE DISTRIBUTIONS,0.09057971014492754,"2.3
DIVERSE DISTRIBUTIONS"
DIVERSE DISTRIBUTIONS,0.09239130434782608,"Recall that the main result of this paper is to construct generators that look indistinguishable from
natural target distributions D∗according to any poly-sized neural network, but which are far from
D∗in Wasserstein. In this section we describe in greater detail the properties that these D∗satisfy.
Deﬁnition 6. A distribution µ over Rd is (N, β)-diverse if for any discrete distribution ν on Rd
supported on at most N points, W1(µ, ν) ≥β."
DIVERSE DISTRIBUTIONS,0.09420289855072464,"The main fact we use about (N, β)-diverse distributions is that they cannot be approximated by
pushforwards of Ulog2 N (see Lemma A.12 in the supplement for a formal statement)."
DIVERSE DISTRIBUTIONS,0.09601449275362318,"Note that Deﬁnition 6 is a very mild assumption that simply requires that the distribution not be
tightly concentrated around a few points. Distributions that satisfy Deﬁnition 6 are both practically
relevant and highly expressive. For starters, any reasonable real-world image distribution will be
diverse as it will not be concentrated around a few unique images. One can also show that distri-
butions like uniform distributions over well-separated discrete point sets and over [0, 1]d are diverse
(Lemma A.13 and Lemma A.15 in supplement). Deﬁnition 6 also captures random expansive neural
networks with leaky ReLU activations:
Lemma 2.3 (Random expansive leaky ReLU networks). For k0, . . . , kL ∈N satisfying ki ≥
1.1ki−1 for all i ∈[L], let W1 ∈Rk1×k0, W2 ∈Rk2×k1, . . . , WL ∈RkL×kL−1 be ran-
dom weight matrices, where every entry of Wi is an independent draw from N(0, 1/ki). For
the function F : Rk0 →RkL given by F(x) ≜WLψλ (WL−1ψλ (· · · ψλ(W1x) · · · )), where
ψλ(z) = ψλ(z) = z/2 + (1/2 −λ)|z| is the leaky ReLU activation, F(Ik0) is (2m, β)-diverse for
m = (k0/2) log(k0/2) −kL/1.1 −1 and β = Θ(λ)L with probability at least 1 −exp(−Ω(d))."
DIVERSE DISTRIBUTIONS,0.09782608695652174,"For example, if λ, L = Θ(1), then F(Ik0) is (2Ω(k0 log k0), Ω(1))-diverse for k0 sufﬁciently large."
DIVERSE DISTRIBUTIONS,0.09963768115942029,"It is easy to see that the networks in Lemma 2.3 can be implemented as ReLU networks, so our main
theorem applies to target distributions given by pushing the uniform distribution over the continuous
cube through a random expansive leaky ReLU network of constant depth."
FOOLING RELU NETWORK DISCRIMINATORS DOES NOT SUFFICE,0.10144927536231885,"3
FOOLING RELU NETWORK DISCRIMINATORS DOES NOT SUFFICE"
FOOLING RELU NETWORK DISCRIMINATORS DOES NOT SUFFICE,0.10326086956521739,"In this section we will show that even though a generative model looks indistinguishable from some
target distribution D∗according to any ReLU network in F∗, it can be quite far from D∗in Wasser-"
FOOLING RELU NETWORK DISCRIMINATORS DOES NOT SUFFICE,0.10507246376811594,"4g : N →R≥0 is negligible if for every polynomial p, g(n) < |1/p(n)| for all sufﬁciently large n."
FOOLING RELU NETWORK DISCRIMINATORS DOES NOT SUFFICE,0.1068840579710145,Published as a conference paper at ICLR 2022
FOOLING RELU NETWORK DISCRIMINATORS DOES NOT SUFFICE,0.10869565217391304,"stein. We begin by describing a simple version of this result over discrete domains in Section 3.1.
In Section 3.2 we extend this to target distributions over continuous domains, but where the gen-
erator still takes in a discrete-valued seed. Finally, in Section 3.3 we give a simple reduction that
extends these results to give generators that take in a continuous-valued random (Gaussian) seed,
culminating in the following main result:"
FOOLING RELU NETWORK DISCRIMINATORS DOES NOT SUFFICE,0.1105072463768116,"Theorem 3.1. Let {Hm}m be a sequence of generators Hm
:
Rr(m)
→
Rd(m) for
r(m), d(m) ≤poly(m) whose output coordinates are computable by networks in Cτ ′(m),Λ′(m)
L′(m),S′(m),r(m)
for τ ′(m), Λ′(m) ≤poly(m). Suppose that Hm(Ir(m)) is (2m, Ω(1))-diverse."
FOOLING RELU NETWORK DISCRIMINATORS DOES NOT SUFFICE,0.11231884057971014,"Fix any ϵ : N →[0, 1] satisfying ϵ(m) ≥max(negl(m), exp(−O(m)). Under Assumption 1, there
is a sequence of generators Gm : Rm →Rd(m) such that for all m sufﬁciently large:"
FOOLING RELU NETWORK DISCRIMINATORS DOES NOT SUFFICE,0.11413043478260869,"1. Every output coordinate of Gm is computable by a network in Cτ,Λ
L,S,m for"
FOOLING RELU NETWORK DISCRIMINATORS DOES NOT SUFFICE,0.11594202898550725,"τ = max(O(log(Λ′(m)·m·d(m)/ϵ(m))), τ ′(m), O(1)),
Λ = O(Λ′(m)2poly(m)/ϵ(m)),"
FOOLING RELU NETWORK DISCRIMINATORS DOES NOT SUFFICE,0.11775362318840579,"L = L′(m) + O(1),
S = O(r(m) log(1/ϵ(m))) + 3m + S′(m)."
FOOLING RELU NETWORK DISCRIMINATORS DOES NOT SUFFICE,0.11956521739130435,"2. WF∗(Gm(γm), Hm(Ir(m))) ≤ϵ(m)·poly(m),
3. W1(Gm(γm), Hm(Ir(m))) ≥Ω(1)."
FOOLING RELU NETWORK DISCRIMINATORS DOES NOT SUFFICE,0.1213768115942029,Note that a natural choice of parameters for Hm would be
FOOLING RELU NETWORK DISCRIMINATORS DOES NOT SUFFICE,0.12318840579710146,"τ ′(m) ≤O(log m), Λ′(m), S′(m), 1/ϵ(m) ≤poly(m), L′(m) ≤O(1)."
FOOLING RELU NETWORK DISCRIMINATORS DOES NOT SUFFICE,0.125,"(In fact, the poly(m) factor in bullet point 2 simply comes from the Lipschitzness of Hm, so ϵ(m)
only needs to scale inversely in this quantity for WF∗to be small.) Altogether, we conclude that
Gm’s output coordinates are computable by constant-depth ReLU networks with polynomial size
and Lipschitzness and logarithmic bit complexity τ."
STRETCHING BITS TO BITS,0.12681159420289856,"3.1
STRETCHING BITS TO BITS"
STRETCHING BITS TO BITS,0.1286231884057971,"As a warmup, in this subsection we prove the following special case of Theorem 3.1 when the target
distribution and seed distribution are discrete.
Theorem 3.2. Under Assumption 1, for any constant c > 1, there is a sequence of generators
Gm : Rm →Rd(m) for d(m) ≥mc such that for all m,"
STRETCHING BITS TO BITS,0.13043478260869565,"1. Every output coordinate of Gm is computable by a network in Cτ,Λ
L,S,m for τ, Λ, L, S = Oc(1)."
STRETCHING BITS TO BITS,0.1322463768115942,"2. WF∗(Gm(Um), Ud(m)) ≤negl(m),
3. W1(Gm(Um), Ud(m)) ≥Ω(1)."
STRETCHING BITS TO BITS,0.13405797101449277,"We emphasize that in this discrete setting, our quantitative guarantees are even stronger: all parame-
ters τ, Λ, L, S of the generator are constant, and no polynomial-sized ReLU network can distinguish
between Gm(Um) and Ud(m) with even non-negligible advantage."
STRETCHING BITS TO BITS,0.1358695652173913,"As discussed in the introduction, a basic but important building block in the proof of Theorem 3.2
is the connection between Goldreich’s PRG and generative models computed by neural networks of
constant depth/size/Lipschitzness. We begin by elaborating on this connection and showing that any
predicate {±1}k →{±1} can be implemented as a network in Cτ,Λ
L,S,d where τ, Λ, L, S = Ok(1).
As a consequence of known constructions for implementing Boolean functions with ReLU networks
(see e.g. Lemma A.2 of (Chen et al., 2020b)), we can show:
Corollary 3.3. For any k ∈N and any H and P as in Deﬁnition 5, the coordinates of the output of
GP,H are computed by networks in Cτ,Λ
L,S,m for τ = O(k), Λ = exp(O(k)), L = k, S = O(2kk)."
STRETCHING BITS TO BITS,0.13768115942028986,"Before we use this to prove Theorem 3.2, we need an extra technical ingredient to formalize the
fact that a discriminator given by a ReLU network of polynomially bounded complexity yields a
discriminator computable by a polynomial-sized Boolean circuit. The idea is that if WF∗is large
so that there exists some ReLU network discriminator, then because the input to the discriminator is
sufﬁciently well concentrated, some afﬁne threshold of the ReLU network can distinguish between
the two distributions:"
STRETCHING BITS TO BITS,0.13949275362318841,Published as a conference paper at ICLR 2022
STRETCHING BITS TO BITS,0.14130434782608695,"Lemma 3.4. Given independent X and Y such that E[Y ]−E[X] = α and for which X −E[X] and
Y −E[Y ] are σ2-sub-Gaussian, there exists a threshold t ∈[E[X] −O(σ
p"
STRETCHING BITS TO BITS,0.1431159420289855,"log(σ/|α|)), E[Y ] + O(σ
p"
STRETCHING BITS TO BITS,0.14492753623188406,"log(σ/|α|))] for which | P[X > t] −P[Y > t]| ≥min

1/2, eΩ(|α|/σ)

."
STRETCHING BITS TO BITS,0.14673913043478262,"We can now sketch the proof of Theorem 3.2, deferring a formal argument to the supplement."
STRETCHING BITS TO BITS,0.14855072463768115,"Proof sketch of Theorem 3.2. The parameter m will be clear from context in the following discus-
sion, so for convenience we will refer to d(m) and Gm as d and G. Let k, P, G, negl(·) be such that
the outcome of Assumption 1 holds. By Corollary 3.3, every output coordinate of G is computable
by a network in Cτ,Λ
L,S,m for τ = O(k), Λ = exp(O(k)), L = k, S = O(2kk)."
STRETCHING BITS TO BITS,0.1503623188405797,"The fact that W1(G(Um), Ud) > 1/3 follows from the fact that G(Um) is uniform over 2m points
on the hypercube (with multiplicity), and Ud is (2m, 1/3)-diverse."
STRETCHING BITS TO BITS,0.15217391304347827,"It remains to check that G fools F∗relative to Ud. Suppose to the contrary that there exists some
f : Rd →R in F∗for which |E[f(G(Um))] −E[f(Ud)]| > d−a for some constant a > 0. Note
that for any threshold t ∈Rτ, there is a Turing machine Mτ : {±1}d →{±1} that computes y 7→
sgn(f(y) −t) using τ bits of advice. By applying Lemma 3.4 to random variables X = f(G(Um))
and Y = f(Ud), we get that there exists a threshold t ∈Rpoly(d) for which |E[Mτ(G(Um))] −
E[Mτ(Ud)]| > 1/poly(d), contradicting Assumption 1."
FROM BINARY OUTPUTS TO CONTINUOUS OUTPUTS,0.1539855072463768,"3.2
FROM BINARY OUTPUTS TO CONTINUOUS OUTPUTS"
FROM BINARY OUTPUTS TO CONTINUOUS OUTPUTS,0.15579710144927536,"In this section we show how to extend Theorem 3.2 to the setting where the target distribution D∗is a
pushforward of the uniform distribution on [0, 1]r. At a high level, the idea will be to post-process the
output of the generator constructed in Theorem 3.2. Roughly speaking, we take weighted averages
of clusters of output coordinates from the generator in Theorem 3.2 and pass these averages through
the pushforward map deﬁning D∗. Formally, we show:
Theorem 3.5. Let {Hm}m be a sequence of generators Hm
:
Rr(m)
→
Rd(m) for
r(m), d(m) ≤poly(m) whose output coordinates are computable by networks in Cτ ′(m),Λ′(m)
L′(m),S′(m),r(m)
for τ ′(m), Λ′(m) ≤poly(m). Suppose that Hm(Ir(m)) is (2m, Ω(1))-diverse."
FROM BINARY OUTPUTS TO CONTINUOUS OUTPUTS,0.15760869565217392,"Fix any ϵ : N →[0, 1] satisfying ϵ(m) ≥max(negl(m), exp(−O(m)). Under Assumption 1, there
is a sequence of generators Gm : Rm →Rd(m) such that for all m sufﬁciently large:"
FROM BINARY OUTPUTS TO CONTINUOUS OUTPUTS,0.15942028985507245,"1. Every output coordinate of Gm is computable by a network in Cτ,Λ
L,S,m for"
FROM BINARY OUTPUTS TO CONTINUOUS OUTPUTS,0.161231884057971,"τ = max(O(log(1/ϵ(m))), τ ′(m), O(1)),
Λ = O(Λ′(m)) · poly(m),
L = L′(m) + O(1),
S = O(r(m) · log(1/ϵ(m))) + S′(m)."
FROM BINARY OUTPUTS TO CONTINUOUS OUTPUTS,0.16304347826086957,"2. WF∗(Gm(Um), Hm(Ir(m))) ≤ϵ(m) · poly(m),
3. W1(Gm(Um), Hm(Ir(m))) ≥Ω(1)."
FROM BINARY OUTPUTS TO CONTINUOUS OUTPUTS,0.16485507246376813,"Theorem 3.5 retains many of the nice properties of Theorem 3.2, e.g. it can tolerate distinguishing
advantage ϵ(m) which is negligible, at the mild cost of an extra logarithmic dependence on 1/ϵ(m)
in the bit complexity τ. And as with Theorem 3.1, if the networks Hm are of constant depth, the
resulting generators Gm are also of constant depth."
FROM BINARY OUTPUTS TO CONTINUOUS OUTPUTS,0.16666666666666666,"To prove Theorem 3.5, we begin by showing that for any pair of distributions which are close under
the WF metric for some family of neural networks F, their pushforwards under a simple generative
model will still be close under the WF′ metric for some slightly weaker family of networks F′."
FROM BINARY OUTPUTS TO CONTINUOUS OUTPUTS,0.16847826086956522,"Lemma 3.6. Fix parameters Λ, Λ′ > 1. Let F = Cτ,Λ
L,S,s. If WF(p, q) ≤ϵ for some distributions
p, q on Rs, then for any J : Rs →Rr each of whose output coordinates is computed by a function
in Cτ,Λ′"
FROM BINARY OUTPUTS TO CONTINUOUS OUTPUTS,0.17028985507246377,"L′,S′,s for some L′ < L and S′ ≤
S−r−1"
FROM BINARY OUTPUTS TO CONTINUOUS OUTPUTS,0.1721014492753623,"r
, we have WF′(J(p), J(q)) ≤2ϵΛ′√r for F′ ="
FROM BINARY OUTPUTS TO CONTINUOUS OUTPUTS,0.17391304347826086,"Cτ ′,Λ
L−L′,S′′,r where τ ′ = τ −⌈log2 Λ√r⌉and S′′ = S −r(S′ + 1)."
FROM BINARY OUTPUTS TO CONTINUOUS OUTPUTS,0.17572463768115942,"Now recall that in Theorem 3.2, we exhibited a GAN which is close in WF∗to Us.
Using
Lemma 3.6, we can show that a certain simple pushforward of this GAN will be close in WF∗to the
uniform distribution over [0, 1]r for r slightly smaller than s. The starting point is the following:"
FROM BINARY OUTPUTS TO CONTINUOUS OUTPUTS,0.17753623188405798,Published as a conference paper at ICLR 2022
FROM BINARY OUTPUTS TO CONTINUOUS OUTPUTS,0.1793478260869565,"Fact 3.7. For any 0 < ϵ < 1 and n ≥log2(1/ϵ), let h : Rn →R be given by h(x) = ⟨w, x + 1⟩
for w =
 
1/4, 1/8, . . . , (1/2)n+1
and 1 the all-1’s vector. Then W1(h(Un), I1) ≤ϵ."
FROM BINARY OUTPUTS TO CONTINUOUS OUTPUTS,0.18115942028985507,"By leveraging Lemma 3.6 and Fact 3.7, we can get an approximation to the uniform distribution
over [0, 1]r out of the uniform distribution over {±1}s:"
FROM BINARY OUTPUTS TO CONTINUOUS OUTPUTS,0.18297101449275363,"Lemma 3.8. Suppose ϵ > 0 satisﬁes log(1/ϵ) ≤poly(s). If a distribution eD over Rs satisﬁes
WF∗( eD, Us) ≤ϵ, then for r ≜s/⌈log(1/ϵ)⌉,5 there is a function J : Rs →Rr each of whose output
coordinates is computed by a function in CO(log(1/ϵ)),O(1)
1,0,s
such that WF∗(J( eD), Ir) ≤ϵ · poly(r)."
FROM BINARY OUTPUTS TO CONTINUOUS OUTPUTS,0.18478260869565216,"By further combining Lemma 3.6 and Lemma 3.8, we can thus extend the latter from the uniform
distribution on [0, 1]r to simple pushforwards thereof.
Lemma 3.9. Under the hypotheses of Lemma 3.8, for any d ≤poly(s) and any function H :
Rr →Rd each of whose output coordinates is computed by a function in Cτ ′,Λ′"
FROM BINARY OUTPUTS TO CONTINUOUS OUTPUTS,0.18659420289855072,"L′,S′,r for τ ′ ≤poly(s),
there is a function J′ : Rs →Rd each of whose output coordinates is computed by a function in
Cmax(O(log(1/ϵ)),τ ′),O(Λ′√"
FROM BINARY OUTPUTS TO CONTINUOUS OUTPUTS,0.18840579710144928,"d)
L′+1,r+S′,s
such that WF∗(J′( eD), H(Ir)) ≤ϵΛ′ · poly(s)."
FROM BINARY OUTPUTS TO CONTINUOUS OUTPUTS,0.19021739130434784,"So for any pushforward H(Ir) of the uniform distribution on [0, 1]r, Lemma 3.9 lets us take the
GAN given by Theorem 3.2 and slightly post-process its output so that it is close in WF∗to H(Ir).
We can now sketch Theorem 3.5’s proof, deferring the full argument to the supplement."
FROM BINARY OUTPUTS TO CONTINUOUS OUTPUTS,0.19202898550724637,"Proof of Theorem 3.5. Condition 3 follows because G(Um) is a uniform distribution on 2m points
(w/ multiplicity) and H(Ir) is (2m, Ω(1))-diverse. Next, let s = r · ⌈log(1/ϵ)⌉. As we are as-
suming ϵ ≥exp(−O(m)), s ≤r · m = mc for some constant c > 1. Take G′ to be the gen-
erator G : Rm →Rs constructed in Theorem 3.2. By applying Lemma 3.9 to eD = G′(Um),
we get a function J′ : Rs →Rd each of whose output coordinates is computed by a function in"
FROM BINARY OUTPUTS TO CONTINUOUS OUTPUTS,0.19384057971014493,"Cmax(O(log(1/ϵ)),τ ′),O(Λ′√"
FROM BINARY OUTPUTS TO CONTINUOUS OUTPUTS,0.1956521739130435,"d)
L′+1,r+S′,s
such that WFd(J′(G′(Um)), H(Ir)) ≤ϵΛ′ · poly(m) ≤ϵ · poly(m),
so we get condition 2 of the theorem for G ≜J′ ◦G′. Finally, by Lemma 2.1, every output
coordinate of G can be realized by a network in Cτ,Λ
L,S,m for τ = max(O(log(1/ϵ)), τ ′, O(1)),
Λ = O(Λ′√"
FROM BINARY OUTPUTS TO CONTINUOUS OUTPUTS,0.19746376811594202,"ds) = O(Λ′) · poly(m), L = L′ + O(1), and S = O(s) + r + S′ = O(s) + S′ (where
we used the fact that r = s/⌈log(1/ϵ)⌉< s). This establishes condition 1 of the theorem."
FROM BINARY INPUTS TO CONTINUOUS INPUTS,0.19927536231884058,"3.3
FROM BINARY INPUTS TO CONTINUOUS INPUTS"
FROM BINARY INPUTS TO CONTINUOUS INPUTS,0.20108695652173914,"In Theorem 3.5, we have shown how to go from Um to any simple pushforward Hm(Ir(m)) of the
uniform distribution over [0, 1]r(m). Here we complete the proof of our main result, Theorem 3.1,
by giving a simple reduction showing how to use Gaussian seed γm instead of Um. At a high level,
the idea will be to pre-process the inputs to the generator constructed in Theorem 3.5 by appending
appropriate activations at the input layer. We will need the following elementary construction.
Lemma 3.10. For any ξ for which 1/ξ ∈Rτ, the piecewise linear function hξ which outputs −1 for
inputs x ≤−ξ, 1 for x ≥ξ, and x/ξ otherwise, lies in Cτ,1/ξ
2,2,1 ."
FROM BINARY INPUTS TO CONTINUOUS INPUTS,0.2028985507246377,"The function hξ will let us approximately convert from γm to Um. Speciﬁcally, the following
says that if we want to approximate the output distribution of the generator in Theorem 3.5 using
Gaussians instead of bits as seed, it sufﬁces to attach entrywise applications of hξ at the input layer:"
FROM BINARY INPUTS TO CONTINUOUS INPUTS,0.20471014492753623,"Lemma 3.11. Let G0 : Rm →Rd have output coordinates computable by networks in Cτ ′′,Λ′′"
FROM BINARY INPUTS TO CONTINUOUS INPUTS,0.20652173913043478,"L′′,S′′,m′′."
FROM BINARY INPUTS TO CONTINUOUS INPUTS,0.20833333333333334,"For any ϵ > 0, let ξ′ ≜ϵ/

Λ′′p"
FROM BINARY INPUTS TO CONTINUOUS INPUTS,0.21014492753623187,"(2/π)m3d

and let ξ bethe multiplicative inverse of ⌈1/ξ′⌉."
FROM BINARY INPUTS TO CONTINUOUS INPUTS,0.21195652173913043,"The function G : Rm →Rd given by G(x) = G0(hξ(x)), where hξ(x) denotes entrywise applica-
tion of hξ deﬁned in Lemma 3.10, satisﬁes that 1) each of the output coordinates of G is computable
by a network in Cτ,Λ
L,S,m for the parameters τ = max(τ ′′, O(log(Λ′′md/ϵ))), Λ = O(Λ′′2m2√"
FROM BINARY INPUTS TO CONTINUOUS INPUTS,0.213768115942029,"d/ϵ),
L = L′′ + 2, and S = 3m + S′′, and 2) W1(G0(Um), G(γm)) ≤ϵ."
FROM BINARY INPUTS TO CONTINUOUS INPUTS,0.21557971014492755,"5We will assume for simplicity that this is an integer, though it is not hard to handle the case where
⌈log(1/ϵ)⌉does not divide s."
FROM BINARY INPUTS TO CONTINUOUS INPUTS,0.21739130434782608,Published as a conference paper at ICLR 2022
FROM BINARY INPUTS TO CONTINUOUS INPUTS,0.21920289855072464,"Remark 3.12. Note the only fact we use about γm in the proof of Lemma 3.11 is that outside of an
event with probability O(mξ), hξ(γm) is uniform over {±1}m. In particular, the same would hold
for any product measure each of whose coordinates is symmetric and anticoncentrated around zero.
For instance, up to a constant factor in ξ′, Lemma 3.11 also holds with γm replaced by Im."
FROM BINARY INPUTS TO CONTINUOUS INPUTS,0.2210144927536232,"Proof of Theorem 3.1. Substitute the generator constructed in Theorem 3.5, call it G0, into
Lemma 3.11; let G be the generator resulting from the lemma. Recall from Theorem 3.5 that
each output coordinate of G0 lies in Cτ ′′,Λ′′"
FROM BINARY INPUTS TO CONTINUOUS INPUTS,0.22282608695652173,"L′′,S′′,m for τ ′′ = max(O(log(1/ϵ)), τ ′, O(1)), Λ′′ =
O(Λ′poly(m)), L′′ = L′ + O(1), S′′ = O(r log(1/ϵ)) + S′. So by Lemma 3.11, every out-
put coordinate of G is computable by a network in Cτ,Λ
L,S,m for τ = max(τ ′′, O(log(Λ′md/ϵ))) =
max(O(log(Λmd/ϵ)), τ ′, O(1)), Λ = O(Λ′′2m2√"
FROM BINARY INPUTS TO CONTINUOUS INPUTS,0.2246376811594203,"d/ϵ) = O(Λ′2poly(m)/ϵ), L = L′′ + 2 =
L′ + O(1), and S = 3m + S′′ = O(r log(1/ϵ)) + 3m + S′′."
EXPERIMENTAL RESULTS,0.22644927536231885,"4
EXPERIMENTAL RESULTS"
EXPERIMENTAL RESULTS,0.22826086956521738,"To empirically demonstrate the existence of a constant depth generator that can fool polynomially-
bounded discriminators, we evaluated the generator G given by Goldreich’s PRG (Goldreich, 2011)
with input dimension m = 50, output dimension d = 200, and predicate P : {±1}5 →{±1} given
by the popular TSA predicate, namely P(x1, . . . , x5) = x1 · x2 · x3 · (x4 ∧x5). This is the smallest
predicate under which Goldreich’s candidate construction is believed to be secure."
EXPERIMENTAL RESULTS,0.23007246376811594,"The target distribution D∗is the uniform distribution U200 over {±1}200.
As we prove in
Lemma A.13, Ud is sufﬁciently diverse that W1(G(Um), Ud) ≥Ω(1). We trained four differ-
ent discriminators given respectively by 1, 2, 3, 4 hidden-layer ReLU networks, where each hid-
den layer is fully connected with dimensions 200 × 200, to discriminate the output of the gener-
ator G(Um) from the target distribution Ud. We used the Adam optimizer with step size 0.001
over the DCGAN training objective, with batch-size 128. As we can see in Figure 1, the test loss
E[−log(D(X))] + E[−log(1 −D(G(z)))] −2 log(2) stays consistently above zero, indicating that
the discriminator can not discriminate the true distribution from the generator output, even though
the Wasserstein distance between these two distributions is provably large."
EXPERIMENTAL RESULTS,0.2318840579710145,"Figure 1: Test loss (E[−log(D(X))]+E[−log(1−D(G(z)))]−2 log(2)) over course of training. Discrim-
inators cannot distinguish generator output from true distribution, though Wasserstein provably large."
CONCLUSIONS,0.23369565217391305,"5
CONCLUSIONS"
CONCLUSIONS,0.23550724637681159,"In light of the obstructions presented in this paper, what are natural next steps for the theory of
GANs? Here, we offer a couple of thoughts and possible future directions."
CONCLUSIONS,0.23731884057971014,"One limitation of our lower bound is that it holds for a speciﬁc generator. Of course, it is quite
unlikely that we will ever encounter such a generator through natural GAN training. One way to
circumvent our lower bound is to argue that the training dynamics of the generator may have some
regularization effect which allows us to avoid these troublesome generators, and which allows GANs
to learn distributions in polynomial time."
CONCLUSIONS,0.2391304347826087,"Another orthogonal perspective is that our results suggest that perhaps statistical learning is too
strong of a goal. If our GAN is indeed indistinguishable from the target distribution to all polynomial
time algorithms, then not only should the output of the GAN be sufﬁcient for humans, but it should
also be sufﬁcient for all downstream applications, which presumably run in polynomial time. This
raises the intriguing possibility that the correct metric for measuring closeness between distances in
the context of GANs should inherently involve some computational component (e.g. in the sense of
Dwork et al. (2021)) as opposed to the purely statistical metrics generally considered in the literature."
CONCLUSIONS,0.24094202898550723,Published as a conference paper at ICLR 2022
CONCLUSIONS,0.2427536231884058,"Acknowledgments
The authors would like to thank Boaz Barak, Adam Klivans, and Alex Lom-
bardi for enlightening discussions about local PRGs. SC was supported by NSF Award 2103300.
This work was done while the ﬁrst, second, and fourth authors were visiting the Simons Institute for
the Theory of Computing."
REFERENCES,0.24456521739130435,REFERENCES
REFERENCES,0.2463768115942029,"Zeyuan Allen-Zhu and Yuanzhi Li. Forward super-resolution: How can gans learn hierarchical
generative models for real-world distributions. arXiv preprint arXiv:2106.02619, 2021."
REFERENCES,0.24818840579710144,"Benny Applebaum. Cryptographic hardness of random local functions. Computational complexity,
25(3):667–722, 2016."
REFERENCES,0.25,"Benny Applebaum and Pavel Raykov. Fast pseudorandom functions based on expander graphs. In
Theory of Cryptography Conference, pp. 27–56. Springer, 2016."
REFERENCES,0.25181159420289856,"Benny Applebaum, Yuval Ishai, and Eyal Kushilevitz. Cryptography in NC0. SIAM Journal on
Computing, 36(4):845–888, 2006."
REFERENCES,0.2536231884057971,"Martin Arjovsky, Soumith Chintala, and L´eon Bottou. Wasserstein generative adversarial networks.
In International conference on machine learning, pp. 214–223. PMLR, 2017."
REFERENCES,0.2554347826086957,"Sanjeev Arora and Boaz Barak. Computational complexity: a modern approach. Cambridge Uni-
versity Press, 2009."
REFERENCES,0.2572463768115942,"Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium
in generative adversarial nets (gans). In International Conference on Machine Learning, pp. 224–
232. PMLR, 2017."
REFERENCES,0.25905797101449274,"Sanjeev Arora, Andrej Risteski, and Yi Zhang. Do gans learn the distribution? some theory and
empirics. In International Conference on Learning Representations, 2018."
REFERENCES,0.2608695652173913,"Yu Bai, Tengyu Ma, and Andrej Risteski. Approximability of discriminators implies diversity in
gans. In International Conference on Learning Representations, 2018."
REFERENCES,0.26268115942028986,"Hugo Berard, Gauthier Gidel, Amjad Almahairi, Pascal Vincent, and Simon Lacoste-Julien. A
closer look at the optimization landscapes of generative adversarial networks. arXiv preprint
arXiv:1906.04848, 2019."
REFERENCES,0.2644927536231884,"Lijie Chen and Roei Tell. Bootstrapping results for threshold circuits “just beyond” known lower
bounds. In Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing,
pp. 34–41, 2019."
REFERENCES,0.266304347826087,"Minshuo Chen, Wenjing Liao, Hongyuan Zha, and Tuo Zhao. Statistical guarantees of generative
adversarial networks for distribution estimation. arXiv preprint arXiv:2002.03938, 2020a."
REFERENCES,0.26811594202898553,"Ruiwen Chen, Rahul Santhanam, and Srikanth Srinivasan.
Average-Case Lower Bounds and
Satisﬁability Algorithms for Small Threshold Circuits.
In Ran Raz (ed.), 31st Conference
on Computational Complexity (CCC 2016), volume 50 of Leibniz International Proceedings
in Informatics (LIPIcs), pp. 1:1–1:35, Dagstuhl, Germany, 2016. Schloss Dagstuhl–Leibniz-
Zentrum fuer Informatik. ISBN 978-3-95977-008-8. doi: 10.4230/LIPIcs.CCC.2016.1. URL
http://drops.dagstuhl.de/opus/volltexte/2016/5844."
REFERENCES,0.26992753623188404,"Sitan Chen, Adam R Klivans, and Raghu Meka. Learning deep relu networks is ﬁxed-parameter
tractable. arXiv preprint arXiv:2009.13512, 2020b."
REFERENCES,0.2717391304347826,"Amit Daniely. Complexity theoretic limitations on learning halfspaces. In Proceedings of the forty-
eighth annual ACM symposium on Theory of Computing, pp. 105–117, 2016."
REFERENCES,0.27355072463768115,"Amit Daniely and Shai Shalev-Shwartz. Complexity theoretic limitations on learning dnf’s. In
Conference on Learning Theory, pp. 815–830. PMLR, 2016."
REFERENCES,0.2753623188405797,"Amit Daniely and Gal Vardi. From local pseudorandom generators to hardness of learning. arXiv
preprint arXiv:2101.08303, 2021."
REFERENCES,0.27717391304347827,Published as a conference paper at ICLR 2022
REFERENCES,0.27898550724637683,"Amit Daniely, Nati Linial, and Shai Shalev-Shwartz. From average case complexity to improper
learning complexity. In Proceedings of the forty-sixth annual ACM symposium on Theory of
computing, pp. 441–448, 2014."
REFERENCES,0.2807971014492754,"Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training gans with
optimism. arXiv preprint arXiv:1711.00141, 2017."
REFERENCES,0.2826086956521739,"Cynthia Dwork, Michael P Kim, Omer Reingold, Guy N Rothblum, and Gal Yona. Outcome in-
distinguishability. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of
Computing, pp. 1095–1108, 2021."
REFERENCES,0.28442028985507245,"Soheil Feizi, Farzan Farnia, Tony Ginart, and David Tse. Understanding gans: the lqg setting. arXiv
preprint arXiv:1710.10793, 2017."
REFERENCES,0.286231884057971,"Vitaly Feldman, Will Perkins, and Santosh Vempala. On the complexity of random satisﬁability
problems with planted solutions. SIAM Journal on Computing, 47(4):1294–1338, 2018."
REFERENCES,0.28804347826086957,"Gauthier Gidel, Reyhane Askari Hemmat, Mohammad Pezeshki, R´emi Le Priol, Gabriel Huang, Si-
mon Lacoste-Julien, and Ioannis Mitliagkas. Negative momentum for improved game dynamics.
In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pp. 1802–1811.
PMLR, 2019."
REFERENCES,0.2898550724637681,"Mikael Goldmann and Marek Karpinski. Simulating threshold circuits by majority circuits. SIAM
Journal on Computing, 27(1):230–246, 1998."
REFERENCES,0.2916666666666667,"Mikael Goldmann, Johan H˚astad, and Alexander Razborov. Majority gates vs. general weighted
threshold gates. Computational Complexity, 2(4):277–300, 1992."
REFERENCES,0.29347826086956524,"Oded Goldreich. Candidate one-way functions based on expander graphs. In Studies in Complexity
and Cryptography. Miscellanea on the Interplay between Randomness and Computation, pp. 76–
87. Springer, 2011."
REFERENCES,0.29528985507246375,"Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the
ACM, 63(11):139–144, 2020."
REFERENCES,0.2971014492753623,"Jie Gui, Zhenan Sun, Yonggang Wen, Dacheng Tao, and Jieping Ye. A review on generative adver-
sarial networks: Algorithms, theory, and applications. arXiv preprint arXiv:2001.06937, 2020."
REFERENCES,0.29891304347826086,"Russell Impagliazzo, Ramamohan Paturi, and Michael E Saks. Size–depth tradeoffs for threshold
circuits. SIAM Journal on Computing, 26(3):693–707, 1997."
REFERENCES,0.3007246376811594,"Qi Lei, Jason Lee, Alex Dimakis, and Constantinos Daskalakis. SGD learns one-layer networks in
wgans. In International Conference on Machine Learning, pp. 5799–5808. PMLR, 2020."
REFERENCES,0.302536231884058,"Jerry Li, Aleksander Madry, John Peebles, and Ludwig Schmidt. On the limitations of ﬁrst-order
approximation in gan dynamics. In International Conference on Machine Learning, pp. 3005–
3013. PMLR, 2018."
REFERENCES,0.30434782608695654,"Tengyuan Liang.
How well generative adversarial networks learn distributions.
arXiv preprint
arXiv:1811.03179, 2018."
REFERENCES,0.3061594202898551,"Shuang Liu, Olivier Bousquet, and Kamalika Chaudhuri. Approximation and convergence proper-
ties of generative adversarial learning. arXiv preprint arXiv:1705.08991, 2017."
REFERENCES,0.3079710144927536,"Ester Mariucci and Markus Reiß. Wasserstein and total variation distance between marginals of l´evy
processes. Electronic Journal of Statistics, 12(2):2482–2514, 2018."
REFERENCES,0.30978260869565216,"Ryan ODonnell and David Witmer. Goldreich’s prg: evidence for near-optimal polynomial stretch.
In 2014 IEEE 29th Conference on Computational Complexity (CCC), pp. 1–12. IEEE, 2014."
REFERENCES,0.3115942028985507,"Tomaso Poggio, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao. Why
and when can deep-but not shallow-networks avoid the curse of dimensionality: a review. Inter-
national Journal of Automation and Computing, 14(5):503–519, 2017."
REFERENCES,0.3134057971014493,Published as a conference paper at ICLR 2022
REFERENCES,0.31521739130434784,"Mark Rudelson and Roman Vershynin. Smallest singular value of a random rectangular matrix.
Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of
Mathematical Sciences, 62(12):1707–1739, 2009."
REFERENCES,0.3170289855072464,"Nicolas Schreuder, Victor-Emmanuel Brunel, and Arnak Dalalyan. Statistical guarantees for gen-
erative models without domination. In Algorithmic Learning Theory, pp. 1051–1071. PMLR,
2021."
REFERENCES,0.3188405797101449,"Shashank Singh, Ananya Uppal, Boyue Li, Chun-Liang Li, Manzil Zaheer, and Barnab´as P´oczos.
Nonparametric density estimation under adversarial losses. In NeurIPS, 2018."
REFERENCES,0.32065217391304346,"Michael Sipser. Introduction to the theory of computation. ACM Sigact News, 27(1):27–29, 1996."
REFERENCES,0.322463768115942,"Hoang Thanh-Tung and Truyen Tran. Catastrophic forgetting and mode collapse in gans. In 2020
International Joint Conference on Neural Networks (IJCNN), pp. 1–10. IEEE, 2020."
REFERENCES,0.3242753623188406,"Hoang Thanh-Tung, Truyen Tran, and Svetha Venkatesh. Improving generalization and stability of
generative adversarial networks. arXiv preprint arXiv:1902.03984, 2019."
REFERENCES,0.32608695652173914,"Ananya Uppal, Shashank Singh, and Barnabas Poczos. Nonparametric density estimation & conver-
gence rates for gans under besov ipm losses. Advances in Neural Information Processing Systems,
32:9089–9100, 2019."
REFERENCES,0.3278985507246377,"Salil P Vadhan. Pseudorandomness, volume 7. Now Delft, 2012."
REFERENCES,0.32971014492753625,"Emanuele Viola. The sum of d small-bias generators fools polynomials of degree d. Computational
Complexity, 18(2):209–217, 2009."
REFERENCES,0.33152173913043476,"Ingo Wegener. The complexity of Boolean functions. John Wiley & Sons, Inc., 1987."
REFERENCES,0.3333333333333333,"Maciej Wiatrak, Stefano V Albrecht, and Andrew Nystrom. Stabilizing generative adversarial net-
works: A survey. arXiv preprint arXiv:1910.00927, 2019."
REFERENCES,0.3351449275362319,"Pengchuan Zhang, Qiang Liu, Dengyong Zhou, Tao Xu, and Xiaodong He. On the discrimination-
generalization tradeoff in gans. arXiv preprint arXiv:1711.02771, 2017."
REFERENCES,0.33695652173913043,Published as a conference paper at ICLR 2022
REFERENCES,0.338768115942029,"Roadmap for Supplement
In Section A we provide some additional technical preliminaries, in-
cluding background on Boolean circuits. In Section B we give the deferred proofs from Section 3 for
our main result, Theorem 3.1. In Section C.1 we give background on circuit lower bounds for TC0,
and in Section C we prove Theorem C.2 which effectively says that if one could exhibit genera-
tors with even logarithmic stretch which can fool constant-depth, sufﬁciently superlinear-size ReLU
networks, then this would imply breakthrough circuit lower bounds."
REFERENCES,0.34057971014492755,"A
ADDITIONAL TECHNICAL PRELIMINARIES"
REFERENCES,0.3423913043478261,"More Notation and Miscellaneous Tools
Let 1n denote the all-ones vector in n dimensions;
when n is clear from context, we denote this by 1."
REFERENCES,0.3442028985507246,"Given a vector v ∈Rd, we let ∥v∥denote its Euclidean norm. Given r > 0, let B(v, r) ⊂Rd denote
the Euclidean ball of radius r with center v. Given a matrix W, we let ∥W∥denote its operator
norm. Let σmin(W) denote its minimum singular value."
REFERENCES,0.34601449275362317,"Given a distribution p, let p⊗n denote the product measure given by drawing n independent samples
from p."
REFERENCES,0.34782608695652173,"Deﬁne the function sgn(x) ≜
1
x ≥0
−1
x < 0."
REFERENCES,0.3496376811594203,"Let φ : R →R denote the ReLU activation φ(z) ≜max(0, z). Let ψλ : R →R denote the leaky
ReLU activation ψλ(z) = z/2 + (1/2 −λ)|z|. Note that"
REFERENCES,0.35144927536231885,ψλ(z) = (1 −λ)φ(z) −λφ(−z).
REFERENCES,0.3532608695652174,We will need the following well-known result:
REFERENCES,0.35507246376811596,"Theorem A.1 (Kirszbraun extension). Given an arbitrary subset S ⊂Rd and f : S →R which is
L-Lipschitz, there exists an L-Lipschitz extension ef : Rd →R for which ef(y) = f(y) for all y ∈S."
REFERENCES,0.35688405797101447,We will need the following basic fact about composing Lipschitz functions:
REFERENCES,0.358695652173913,"Fact A.2. If g1, . . . , gr : Rd →R are Λ-Lipschitz and h : Rr →R is Λ′-Lipschitz, then the function"
REFERENCES,0.3605072463768116,"x 7→h(g1(x), . . . , gr(x))"
REFERENCES,0.36231884057971014,is ΛΛ′√r-Lipschitz.
REFERENCES,0.3641304347826087,"Proof. For any x, x′, we have |gi(x) −gi(x′)| ≤Λ∥x −x′∥, so
 Pr
i=1(gi(x) −gi(x′))21/2 ≤
Λ√r∥x −x′∥. This implies that |h(g1(x), . . . , gr(x)) −h(g1(x′), . . . , gr(x′))| ≤ΛΛ′√r∥x −x′∥
as desired."
REFERENCES,0.36594202898550726,Fact A.3. The volume of a d-dimensional Euclidean ball of radius 1 is at most (18/d)d/2.
REFERENCES,0.3677536231884058,Proof. It is a standard fact that the volume of the ball can be expressed as 2d · (π/2)⌊d/2⌋
REFERENCES,0.3695652173913043,"d!!
. If d is"
REFERENCES,0.3713768115942029,"even, then d!! = 2d/2 · (d/2)! ≥2d/2 · e
  d"
E,0.37318840579710144,"2e
d/2 ≥(d/e)d/2. If d is odd, then d!! =
d!
⌊d/2⌋!!·2⌊d/2⌋≥"
E,0.375,e(d/e)d
E,0.37681159420289856,e(d/2e)d/2·2d/2 = (d/e)d/2. We conclude that the volume is at most (2πe/d)d/2 ≤(18/d)d/2.
E,0.3786231884057971,"A.1
CONCENTRATION OF MEASURE"
E,0.3804347826086957,We will use the following consequence of McDiarmid’s inequality:
E,0.3822463768115942,"Lemma A.4 (McDiarmid’s Inequality). Suppose F : {±1}n →{±1} is such that for any x, x′ ∈
{±1}n differing on exactly one coordinate, |F(x) −F(x′)| ≤c. Then"
E,0.38405797101449274,"P
x∼{±1}n[|f(x) −E[f(x)]| > s] ≤exp

−2s2 nc2 
."
E,0.3858695652173913,Published as a conference paper at ICLR 2022
E,0.38768115942028986,"Corollary A.5. Given F : {±1}n →{±1} which is Λ-Lipschitz, deﬁne the random variable
X ≜F(Un). Then X −E[X] is Λ
√"
E,0.3894927536231884,2n-sub-Gaussian.
E,0.391304347826087,"Proof. Because F is Lipschitz, it satisﬁes the hypothesis of Lemma A.4 with c = Λ, so the corollary
follows by the deﬁnition of sub-Gaussianity."
E,0.39311594202898553,"Theorem A.6 (Theorem 1.1, (Rudelson & Vershynin, 2009)). For n, d ∈N with n ≥d, let W ∈
Rn×d be a random matrix whose entries are independent draws from N(0, 1). Then for every ϵ > 0,"
E,0.39492753623188404,"P
h
σmin(W) ≤ϵ(√n −
√"
E,0.3967391304347826,"d −1)
i
≤(Cϵ)n−d+1 + e−cn"
E,0.39855072463768115,"for absolute constants C, c > 0."
E,0.4003623188405797,"A.2
BOOLEAN CIRCUITS"
E,0.40217391304347827,"In the context of pseudorandom generators, the set of all polynomial-sized Boolean circuits is the
canonical family of discriminator functions to consider when formalizing what it means for a gen-
erator to fool all polynomial-time algorithms."
E,0.40398550724637683,"Here we review some basics about Boolean circuits; for a more thorough introduction to these
concepts, we refer the reader to any of the standard textbooks on complexity theory, e.g. (Arora &
Barak, 2009; Sipser, 1996)."
E,0.4057971014492754,"Deﬁnition 7 (Boolean circuits). Fix a set G of logical gates, e.g. ∧, ∨, ¬. A Boolean circuit C
is a Boolean function {±1}n →{±1} given by a directed acyclic graph with n input nodes with
in-degree zero and an output node with out-degree zero, where each node that isn’t an input node is
labeled by some logical gate in G. Unless otherwise speciﬁed, we will take G to be {∧, ∨, ¬}."
E,0.4076086956521739,"The size S of the circuit is the number of nodes in the graph, and the depth D is given by the length
of the longest directed path in the graph. The value of C on input x ∈{±1}n is deﬁned in an
inductive fashion: the value at a node v in the graph is deﬁned to be the evaluation of the gate at
v on the in-neighbors of v (as the graph is acyclic, this is well-deﬁned), and the value of C on x is
then the value of the output node."
E,0.40942028985507245,"We will occasionally also be interested in the number W of wires in the circuit, i.e. the number of
edges in the graph. Note that trivially
S ≤W + 1.
(2)"
E,0.411231884057971,"Deﬁnition 8 (P/poly). Given T : N →N, let SIZE(T(n)) denote the family of sequences of Boolean
functions {fn : {±1}n →{±1}} for which there exist Boolean circuits {Cn} with sizes {Sn} that
compute {fn} and such that Sn ≤T(n)."
E,0.41304347826086957,Let P/poly ≜S
E,0.4148550724637681,"c>1 SIZE(nc). We refer to (sequences of) functions in P/poly as functions com-
putable by polynomial-sized circuits."
E,0.4166666666666667,"The following standard fact about bounded-depth Boolean circuits will make it convenient to trans-
late between them and neural networks."
E,0.41847826086956524,"Lemma A.7 (See Theorem 1.1 in Section 12.1 of (Wegener, 1987)). For any Boolean circuit C of
size S and depth D with gate set G, there is another circuit C′ of size D · S and depth D with gate
set G which computes the same function as C but with the additional property that for any gate in
C′, all paths from an input to the gate are of the same length."
E,0.42028985507246375,"The upshot of Lemma A.7 is that for any length ℓ, we can think of the gates of C′ at distance ℓfrom
the inputs as comprising a “layer” in the circuit."
E,0.4221014492753623,"A less combinatorial way of formulating the complexity class captured by polynomial-sized circuits
is in terms of Turing machines with advice strings."
E,0.42391304347826086,"Fact A.8 (See e.g. Theorem 6.11 in (Arora & Barak, 2009)). A sequence of Boolean functions
{fn : {±1}n →{±1}} is in P/poly if and only if there exists a sequence of advice strings {αn},
where αn ∈{±1}n for an ≤poly(n), and a Turing machine M which runs for at most poly(n)
steps and, for any n ∈N, takes as input any x ∈{±1}n and the advice string αn and outputs
M(x, αn) = fn(x)."
E,0.4257246376811594,Published as a conference paper at ICLR 2022
E,0.427536231884058,"This fact will be useful for translating discriminators computed by neural networks into discrimina-
tors given by polynomial-sized Boolean circuits."
E,0.42934782608695654,"A.3
MORE ON GANS AND PRGS"
E,0.4311594202898551,"In this section we ﬁll in some additional details regarding the contents of Section 2.1. We begin with
a simple remark about Deﬁnition 1.
Remark A.9. In Deﬁnition 1, if L = 1, then S = 0 and the deﬁnition specializes to linear functions.
That is, Cτ,Λ
1,0,d is simply the class of afﬁne linear functions F(x) = ⟨w, x⟩+ b for w ∈Rd
τ and
b ∈Rτ satisfying ∥w∥≤Λ."
E,0.4329710144927536,"Next, we ﬁll in the proof of Lemma 2.1, which we restate below for the reader’s convenience."
E,0.43478260869565216,"Lemma A.10. Let J : Rs →Rr be a function each of whose output coordinates is computed
by some network in Cτ1,Λ1
L1,S1,s, and let f ∈Cτ2,Λ2
L2,S2,r. Then f ◦J ∈Cτ,Λ
L,S,s for τ = max(τ1, τ2),
Λ = Λ1Λ2
√r, L = L1 + L2, and S = (S1 + 1)r + S2. Furthermore, for the network in Cτ,Λ
L,S,s
realizing f ◦J, the bias and weight vector entries in the output layer lie in Rτ2."
E,0.4365942028985507,Proof. Suppose that the i-th output coordinate of J is computed by a neural network with weight
E,0.4384057971014493,"matrices W(i)
1
∈Rk(i)
1 ×s, . . . , W(i)
L1 ∈R1×k(i)
L1−1 and biases b(i)
1
∈Rk(i)
1 , . . . , b(i)
L1 ∈R."
E,0.44021739130434784,"Deﬁne the (Pr
i=1 k(i)
1 ) × s weight matrix W1 by vertically concatenating the weight matrices
W(1)
1 , . . . , W(r)
1 . For every 1 < j < L1 deﬁne the (Pr
i=1 k(i)
j ) × (Pr
i=1 k(i)
j−1) weight ma-"
E,0.4420289855072464,"trix Wj by diagonally concatenating the weight matrices W(1)
j , . . . , W(r)
j . Similarly, deﬁne the"
E,0.4438405797101449,"r × (Pr
i=1 k(i)
L1) matrix WL1 by diagonally concatening the column vectors W(1)
L1 , . . . , W(r)
L1 . For
the bias vectors in these layers, for every 1 ≤j ≤L1 deﬁne bj to be the vector given by concate-
nating b(1)
j , . . . , b(r)
j ."
E,0.44565217391304346,"Now suppose that f is computed by a neural network with weight matrices WL1+1 ∈RkL1+1×r,
. . . , WL1+L2 ∈R1×kL1+L2−1 and biases bL1+1 ∈RkL1+1, . . . , bL1+L2 ∈R. Then by design, for
any y ∈Rs we have"
E,0.447463768115942,f(J(y)) = WL1+L2φ(WL1+L2−1φ(· · · φ(W1y + b1) · · · ) + bL1+L2−1) + bL1+L2.
E,0.4492753623188406,"This network has depth L1 + L2 and size
 "
E,0.45108695652173914,"L1−1
X j=1 r
X"
E,0.4528985507246377,"i=1
k(i)
j "
E,0.45471014492753625,+ r +
E,0.45652173913043476,"L1+L2
X"
E,0.4583333333333333,"j=L1+1
kj = r · S1 + r + S2 = S."
E,0.4601449275362319,"The bit complexity of the entries of the weight matrices and biases are obviously bounded by
max(τ1, τ2), and the Lipschitzness of the network is bounded by Λ1Λ2
√r by Fact A.2."
E,0.46195652173913043,"Finally, we will also use the following standard tensorization property of Wasserstein distance later:"
E,0.463768115942029,"Fact A.11 (See e.g. Lemma 3 in (Mariucci & Reiß, 2018)). If p, q satisfy W1(p, q) ≤ϵ, then
W1(p⊗n, q⊗n) ≤ϵ√n."
E,0.46557971014492755,"A.4
DIVERSE DISTRIBUTIONS: MISSING PROOFS"
E,0.4673913043478261,"Here we ﬁll in some missing details from Section 2.3. We ﬁrst show that diverse distributions cannot
be approximated by pushforwards of Um if m is insufﬁciently large. This follows immediately from
the deﬁnition of diversity:"
E,0.4692028985507246,"Lemma A.12. For any 0 < β < 1, if D∗is a (2m, β)-diverse distribution over Rd, then for any
function G : {±1}m →Rd, W1(G(Um), D∗) ≥β."
E,0.47101449275362317,"Proof. G(Um) is a uniform distribution on 2m points, with multiplicity if there are multiple points in
{±1}m that map to the same point in Rd under G, so the claim follows by deﬁnition of diversity."
E,0.47282608695652173,Published as a conference paper at ICLR 2022
E,0.4746376811594203,Below we give some simple examples of diverse distributions.
E,0.47644927536231885,"Lemma A.13 (Discrete, well-separated distributions). For any α > 0 and any N, N ′ ∈N satisfying
N ≤N ′. Let Ω⊆Rd be a set of points such that for any z, z′ ∈Ω, ∥z −z′∥≥α. Then the uniform
distribution µ on any N ′ points from Ωis (N, β)-diverse for β = α(1 −N/N ′)."
E,0.4782608695652174,"Proof. Take any discrete distribution ν supported on at most N points y1, . . . , yN in Rd. Consider
the function f : Rd →R: for any y in the support of ν, let f(y) = 0, and for any y not in the
support of ν, let f(y) = 1. As a function from Ωto R, where Ωinherits the Euclidean metric, f is
clearly 1/α-Lipschitz over Ω. By Theorem A.1, there exists a 1/α-Lipschitz extension ef : Rd →R
of f, and we have
|E[f(µ)] −E[f(ν)]| = |E[f(µ)]| ≥1 −N/N ′,"
E,0.48007246376811596,"so W1(µ, ν) ≥1 −N/N ′ as desired."
E,0.48188405797101447,"We now turn to examples of continuous distributions which are diverse. We ﬁrst observe that a
distribution is (N, β)-diverse if it satisﬁes certain small-ball probability bounds."
E,0.483695652173913,"Deﬁnition 9. For a distribution D over Rd, deﬁne the L´evy concentration function QD(r) ≜
supx′∈Rd Px∼D[∥x −x′∥≤r]."
E,0.4855072463768116,"Lemma A.14. If a distribution D over Rd satisﬁes QD(r) ≤α, then D is (N, r(1 −Nα))-diverse."
E,0.48731884057971014,"Proof. Take any N points z1, . . . , zN
∈Rd.
By the bound on QD(r), the union S of the
balls of radius r around these points has Lebesgue measure at most Nα.
Deﬁne the function
f : {z1, . . . , zN} ∪(Rd\S) →{0, 1} to be zero on {z1, . . . , zN} and one on Rd\S. This func-
tion is 1/r-Lipschitz on its domain, so by Theorem A.1 there is an extension f ′ : Rd →R of f
which remains 1/r-Lipschitz on its domain. Deﬁne the function f ∗(x) ≜|f(x)|. Note that for µ
the uniform distribution on {z1, . . . , zN},"
E,0.4891304347826087,"|E[f(µ)] −E[f(D)]| = |E[f(D)]| ≥1 −Nα,"
E,0.49094202898550726,"so we conclude that W1(µ, D) ≥r(1 −Nα)."
E,0.4927536231884058,"Lemma A.15 (Uniform distribution on box). Id is (N, 1/2)-diverse for N ≤1"
E,0.4945652173913043,2(d/18)d/2.
E,0.4963768115942029,"Proof. By Fact A.3, QId(r) ≤(18r2/d)d/2. Taking r = 1 and applying Lemma A.14 allows us to
conclude that W1(µ, Id) ≥1 −N(18/d)d/2, from which the lemma follows."
E,0.49818840579710144,"As we stated in Lemma 2.3, a large family of pushforwards of the uniform distribution over [0, 1]d
are similarly diverse. Below we restate this lemma and provide a complete proof."
E,0.5,"Lemma A.16 (Random expansive leaky ReLU networks). Let γ > 0. For k0, . . . , kL ∈N satisfying
ki ≥(1 + γ)ki−1 for all i ∈[L], let W1 ∈Rk1×k0, W2 ∈Rk2×k1, . . . , WL ∈RkL×kL−1 be
random weight matrices, where every entry of Wi is an independent draw from N(0, 1/ki). For the
function F : Rk0 →RkL given by"
E,0.5018115942028986,"F(x) ≜WLψλ (WL−1ψλ (· · · ψλ(W1x) · · · )) ,"
E,0.5036231884057971,"we have that F(Ik0) is (2m, β)-diverse for"
E,0.5054347826086957,m = (k0/2) log(k0/2) −kL/γ −1 β = 1 6
E,0.5072463768115942,"
λ
2e1/γ L
."
E,0.5090579710144928,"So for example, if γ, λ, L are constants, then F(Ik0) is (2Ω(k0 log k0), Ω(1))-diverse for k0 sufﬁciently
large."
E,0.5108695652173914,"We will prove this inductively by ﬁrst arguing that pushing anticoncentrated distributions through
leaky ReLU (Lemma A.17) or through mildly “expansive” random linear functions (Lemma A.18)
preserves anticoncentration to some extent:"
E,0.5126811594202898,Published as a conference paper at ICLR 2022
E,0.5144927536231884,"Lemma A.17. Let 0 < λ ≤1/2. If a distribution D over Rd satisﬁes QD(r) ≤α, then the
pushforward D′ ≜ψλ(D) also satisﬁes QD′(λr) ≤2dα, where here ψλ(· · · ) denotes entrywise
application of the leaky ReLU activation."
E,0.5163043478260869,"Proof. Consider any ball B(ν, λr) in Rd. Take any orthant KS of Rd, given by points whose i-th
coordinates are nonnegative for i ∈S and negative for i ̸∈S. Let BS be the intersection of B with
this orthant. Then ψ−1
λ (BS) consists of points z ∈KS for which
X"
E,0.5181159420289855,"i∈S
(λzi −νi)2 +
X"
E,0.519927536231884,"i̸∈S
((1 −λ)zi −νi)2 ≤λ2r2.
(3)"
E,0.5217391304347826,We can rewrite the left-hand side of (3) as λ2 X
E,0.5235507246376812,"i∈S
(zi −νiλ)2 + (1 −λ)2 X"
E,0.5253623188405797,"i̸∈S
(zi −νi/(1 −λ))2 ≥λ2∥z −ν(S)∥2,"
E,0.5271739130434783,where in the last step we used λ ≤1/2 and deﬁne the vector νS ∈Rd by
E,0.5289855072463768,"νS
i =
νS
i /λ
i ∈S
νS
i /(1 −λ)
i ̸∈S ."
E,0.5307971014492754,"In other words, ψ−1
λ (BS) is contained in KS ∩B(ν(S), r). In particular,"
E,0.532608695652174,"ψ−1
λ (B) ⊂
["
E,0.5344202898550725,"S
KS ∩B(ν(S), r),"
E,0.5362318840579711,so Px∼D′[x ∈B] ≤2d · α by a union bound.
E,0.5380434782608695,"Lemma A.18. Suppose n, d ∈N satisfy n ≥(1 + γ)d for some γ > 0. Let W ∈Rn×d be a matrix
whose entries are independent draws from N(0, 1/n). If a distribution D over Rd satisﬁes QD(r) ≤"
E,0.5398550724637681,"α, then for the linear map f : x 7→Wx, the pushforward D′ ≜f(D) satisﬁes QD′

γr
2(1+γ)

≤α"
E,0.5416666666666666,with probability at least 1 −exp(−Ω(γd)).
E,0.5434782608695652,"Proof. By Theorem A.6, for any ϵ > 0 we have that σmin(W) ≥ϵ ·

1 −
q d−1 n"
E,0.5452898550724637,"
with probability"
E,0.5471014492753623,"at least 1 −(Cϵ)n−d+1 −e−cn. Taking ϵ = 1/2C and noting that 1 −
q d−1"
E,0.5489130434782609,"n
≥
γ
1+γ , we conclude
that P"
E,0.5507246376811594,"
σmin(W) ≥
γ
2(1 + γ)"
E,0.552536231884058,"
≥1 −exp(−Ω(γd))."
E,0.5543478260869565,"Condition on this event. Now for any ν ∈Rn, if we write ν as Wµ + µ⊥where µ⊥is orthogonal to
the column span of W, then ∥Wx−ν∥2 = ∥W(x−µ)∥2+∥µ⊥∥2. So ∥Wx−ν∥≤
γr
2(1+γ) implies
that ∥W(x −µ)∥≤
γr
2(1+γ). But because σmin(W) ≥
γ
2(1+γ), we conclude that ∥x −µ∥≤r, from
which the lemma follows."
E,0.5561594202898551,We are now ready to prove Lemma 2.3:
E,0.5579710144927537,"Proof of Lemma 2.3. By Lemma A.14 it sufﬁces to bound the L´evy concentration function. We will
induct on the layers of F. For i ∈[L], let F (i) denote the sub-network"
E,0.5597826086956522,"Wiψλ (WL−1ψλ (· · · ψλ(W1x) · · · )) ,"
E,0.5615942028985508,"and let Di denote the pushforward F (i)(Ik0), which is a distribution over Rki. We would like to
apply Lemma A.18 to each of the weight matrices W1, . . . , WL, so condition on the event that the
lemma holds for these matrices, which happens with probability at least 1 −L exp(−Ω(γd))."
E,0.5634057971014492,"Recalling from Fact A.3 that QIk0(r) ≤(18r2/k0)k0/2 for any r > 0, we get from Lemma A.18"
E,0.5652173913043478,"applied to W1 that QD1

γr
2(1+γ)

≤(18r2/k0)k0/2."
E,0.5670289855072463,Published as a conference paper at ICLR 2022
E,0.5688405797101449,"Suppose inductively that we have shown that QDi(ri) ≤αi for some ri, α > 0.
Then by
Lemma A.17 and Lemma A.18 applied to weight matrix Wi+1, we conclude that"
E,0.5706521739130435,"QDi+1(ri+1) ≤αi+1 for ri+1 =
λγri
2(1 + γ), αi+1 = 2kiαi.
(4)"
E,0.572463768115942,"Unrolling the recursion (4), we conclude that QDL(rL) ≤αL for"
E,0.5742753623188406,"rL = rλL−1

γ
2(1 + γ)"
E,0.5760869565217391,"L
≥r ·

λγ
2(1 + γ)"
E,0.5778985507246377,"L
≥r ·

λ
2 · e1/γ L"
E,0.5797101449275363,"αL = 2k1+···+kL−1(18r2/k0)k0/2 ≤2kL/γ(18r2/k0)k0/2,
(5)"
E,0.5815217391304348,"where the inequality in (5) follows from the fact that k1 +· · ·+kL−1 ≤kL−1(1+1/γ) ≤kL/γ. By
Lemma A.14, F(Ik0) = DL is (N, rL(1 −NαL))-diverse. The lemma follows by taking r = 1/3
and 2m = N = 1/2αL."
E,0.5833333333333334,"B
DEFERRED PROOFS FROM SECTION 3"
E,0.5851449275362319,"B.1
PROOF OF COROLLARY 3.3"
E,0.5869565217391305,"Corollary 3.3 is an immediate consequence of the following which appeared in (Chen et al., 2020b):"
E,0.5887681159420289,"Lemma B.1. For any function P : {±1}k →{±1}, there is a collection of k weight matrices
W1, . . . , Wk with entries in RO(k) for which"
E,0.5905797101449275,"P(x) = Wkφ(· · · φ(W1x) · · · )
(6)"
E,0.592391304347826,"for all x ∈{±1}k, and for which ∥Wi∥≤O(1). Furthermore, the size of the network on the
right-hand side of (6) is at most O(2k · k)."
E,0.5942028985507246,We give a proof for completeness to make explicit the dependence of the parameters on k.
E,0.5960144927536232,Proof. Consider the Fourier expansion F(x) = P
E,0.5978260869565217,S⊆[k] bF[S] Q
E,0.5996376811594203,"i∈S xi. We show how to represent
each Fourier basis function Q"
E,0.6014492753623188,"i∈S xi as a ReLU network with at most k layers. Observe that for any
x1, x2 ∈{±1},"
E,0.6032608695652174,"x1 · x2 = φ(x1 + x2) + φ(−x1 −x2) −φ(x2) −φ(−x2),
(7)"
E,0.605072463768116,"which is a two-layer neural network of size 4 whose two weight matrices have operator norm at
most 3. Suppose inductively that for some 1 ≤m < n, there exist weight matrices W′
1, . . . , W′
m
for which Qm
i=1 xi = W′
mφ(· · · φ(W′
1x) · · · ) for all x ∈{±1}k, that this network has size 4m,
and that Qm
i=1∥W′
i∥≤6m."
E,0.6068840579710145,"We now show how to compute Qm+1
i=1 xi. Deﬁne W′′
1 by adding the m-th standard basis vector as a
new row at the bottom of W′
1. For every 1 < i ≤m, deﬁne W′′
i to be the matrix given by appending
a column of zeros to the right of W′′
i and then a new row at the bottom consisting of zeros except
in the rightmost entry. Note that ∥W′′∥1 = max(1, ∥W′
i∥). Deﬁne the network Fm : Rk →R2 by
Fm(x) = W′′
mφ(· · · φ(W′′
1x) · · · )."
E,0.6086956521739131,"Letting v, e ∈R2 be the vectors (1, 1) and (0, 1), we can use (7) to conclude that m+1
Y"
E,0.6105072463768116,"i=1
xi = φ m
Y"
E,0.6123188405797102,"i=1
xi + xm+1 ! + φ  − m
Y"
E,0.6141304347826086,"i=1
xi −xm+1 !"
E,0.6159420289855072,−φ(xm+1) −φ(−xm+1)
E,0.6177536231884058,= φ(v⊤Fm(x)) + φ(−v⊤Fm(x)) −φ(e⊤Fm(x)) −φ(−e⊤Fm(x)).
E,0.6195652173913043,"We can thus write Qm+1
i=1 xi as the ReLU network m+1
Y"
E,0.6213768115942029,"i=1
xi = W′′′
m+1φ(· · · φ(W′′′
1 x) · · · )
(8)"
E,0.6231884057971014,Published as a conference paper at ICLR 2022 where
E,0.625,"W′′′
m+1 = (1, 1, −1, −1), W′′′
m =  

"
E,0.6268115942028986,"v⊤W′′
m
−v⊤W′′
m
e⊤W′′
m
−e⊤W′′
m "
E,0.6286231884057971,"

, W′′′
i = W′′
i
∀1 ≤i < m."
E,0.6304347826086957,"Note that the entries of any W′′′
i are in {0, ±1} and thus have bit complexity at most 2. Additionally,
∥W′′′
m+1∥≤2, W′′′
m ≤3∥W′′
m∥= 3 · max(1, ∥W′
m∥), and ∥W′′
i ∥= max(1, ∥W′
i∥) for all
1 ≤i < m, so Qm+1
i=1 ∥W′′′
i ∥≤6m+1. Furthermore, the size of the network in (8) is 4m + 4.
This completes the inductive step and we conclude that any Fourier basis function Q"
E,0.6322463768115942,"i∈S xi can
be implemented by an |S|-layer ReLU network with size 4|S| and the product of whose weight
matrices’ operator norms is at most 6|S|."
E,0.6340579710144928,"In particular, as the biases in the network are zero, we can rescale the weight matrices so they have
equal operator norm, in which case they each have operator norm at most O(1) and entries in RO(k)"
E,0.6358695652173914,Finally note that because the Fourier coefﬁcients are given by E[F(x) Q
E,0.6376811594202898,"i∈S xi], they are all multi-
ples of 1/2k and thus have bit complexity O(k). The proof follows from applying Lemma B.2 to
these Fourier basis functions and λ given by the Fourier coefﬁcients of P, as ∥λ∥= ∥P∥= 1."
E,0.6394927536231884,The above proof required the following basic fact:
E,0.6413043478260869,"Lemma B.2. Let τ, τ ′ ∈N, and let λ ∈Rr
τ. Given neural networks F1, . . . , Fr : Rd →R each
with L layers and whose weight matrices {W(1)
i
}, . . . , {W(r)
i
} have operator norm bounded by
some R > 0 and entries in Rτ ′, their linear combination P"
E,0.6431159420289855,"i λiFi is a neural network with L layers,
size given by the sum of the sizes of F1, . . . , Fr, and weight matrices W1, . . . , WL with entries in
RO(τ+τ ′) and satisfying ∥W1∥≤R√r, ∥WL∥≤R∥λ∥, and ∥Wi∥≤R for all 1 < i < L. Here
λ ∈Rr is the vector with entries λi."
E,0.644927536231884,"Proof. Denote the i-th weight matrix of Fj by W(j)
i . Deﬁne W1 to be the vertical concatenation
of W(1)
1 , . . . , W(r)
1 , and for every 1 < i < L, deﬁne Wi to be the block diagonal concatenation of
W(1)
i
, . . . , W(r)
i
. Finally, deﬁne WL to be the row vector given by the product λ⊤ "
E,0.6467391304347826,"



"
E,0.6485507246376812,"W(1)
L
0
· · ·
0
0
W(2)
L
· · ·
0
...
0
...
0
0
0
· · ·
W(r)
L "
E,0.6503623188405797,"



"
E,0.6521739130434783,"For all 1 < i < L, ∥Wi∥≤maxj∈[r]∥W(i)∥, and additionally ∥W1∥2 ≤Pr
j=1∥W(j)
1 ∥2 and"
E,0.6539855072463768,"∥WL∥≤∥λ∥maxj∈[r]∥W(j)
L ∥."
E,0.6557971014492754,"B.2
PROOF OF LEMMA 3.4"
E,0.657608695652174,"We will need the following helper lemma about means of truncations of sub-Gaussian random vari-
ables:"
E,0.6594202898550725,"Lemma B.3. If Z is σ2-sub-Gaussian and mean zero, then for any interval I = [a, b] with a ≤0 ≤
b, we have |E[Z · 1[Z ̸∈I]]| ≤O(b −a + σ) · exp(−min(−a, b)2/2σ2)."
E,0.6612318840579711,Published as a conference paper at ICLR 2022
E,0.6630434782608695,"Proof. Deﬁne the random variable Z′ = Z · 1[Z ̸∈I]. Then by integration by parts,"
E,0.6648550724637681,"E[Z′] ≤E[Z · 1[Z > b]] =
Z ∞"
E,0.6666666666666666,"0
P[Z′ > t]dt"
E,0.6684782608695652,"= b P[Z > b] +
Z ∞"
E,0.6702898550724637,"b
P[Z > t]dt"
E,0.6721014492753623,≤b exp(−b2/2σ2) + O(σ · exp(−b2/2σ2))
E,0.6739130434782609,≤O(b + σ) · exp(−b2/2σ2).
E,0.6757246376811594,"and similarly, E[Z′] ≥E[Z · 1[Z < −a]] ≥O(a −σ) · exp(−b2/2σ2), completing the proof."
E,0.677536231884058,We now complete the proof of Lemma 3.4.
E,0.6793478260869565,"Proof of Lemma 3.4. Without loss of generality we can assume that E[X] = 0 and E[Y ] = α. If
α ≥cσ for some sufﬁciently large absolute constant, then we can simply take t = α/2 and get that
| P[X > t]−P[Y > t]| ≥1/2. Now suppose α < cσ, and let I = [−r, r+α] for r = σ
p"
E,0.6811594202898551,"log(Cσ/α)
for some large constant C > 0. Note that by this choice of r,"
E,0.6829710144927537,"r exp(−r2/2σ2) ≤O(α),"
E,0.6847826086956522,"where the constant factor can be made arbitrarily small by picking C sufﬁciently lage. Deﬁne the
random variables X′ ≜X · 1[X ∈I] and Y ′ ≜Y · 1[Y ∈I]. Then"
E,0.6865942028985508,"α = E[Y ] −E[X] = E[Y ′] −E[X′] + E[Y · 1[Y ̸∈I]] −E[X · 1[X ̸∈I]].
(9)"
E,0.6884057971014492,"By Lemma B.3,"
E,0.6902173913043478,E[X · 1[X ̸∈I]] ≤O(2r + α + σ) · exp(−(r + α)2/2σ2) ≤O(r) · exp(−r2/2σ2) ≤O(α) (10)
E,0.6920289855072463,and similarly
E,0.6938405797101449,E[Y · 1[Y ̸∈I]] ≤E[Y ] · P[Y ̸∈I] + O(2r + α + σ) · exp(−(r + α)2/2σ2).
E,0.6956521739130435,"≤2α exp(−r2/2σ2) + O(r) · exp(−(r + α)2/2σ2) ≤O(α).
(11)"
E,0.697463768115942,"Additionally, we have"
E,0.6992753623188406,"E[X′] −E[Y ′] =
Z α+r"
E,0.7010869565217391,"0
(ΦY ′(z) −ΦX′(z))dz −
Z 0"
E,0.7028985507246377,"−α
(ΦX′(z) −ΦY ′(z))dz
(12)"
E,0.7047101449275363,"where ΦZ(z) denotes the cdf at z of random variable Z. Putting (9), (10), (11), (12) together, we
conclude that"
E,0.7065217391304348,"min
Z α+r"
E,0.7083333333333334,"0
(ΦY ′(z) −ΦX′(z))dz,
Z 0"
E,0.7101449275362319,"−α
(ΦX′(z) −ΦY ′(z))dz

≥Ω(α),"
E,0.7119565217391305,"where the constant factor can be made arbitrarily close to 1/2 by making C sufﬁciently small. By
averaging, we conclude that there exists t ∈[−α, α + r] for which"
E,0.7137681159420289,| P[X′ > t] −P[Y ′ > t]| ≥Ω(α/r).
E,0.7155797101449275,"But P[X ̸∈I], P[Y ̸∈I] ≤O(exp(−r2/2σ2)) ≤O(α/r), where the absolute constant can be made
arbitrarily small by making C sufﬁciently small. The claim follows by a union bound, recalling the
deﬁnition of X′, Y ′."
E,0.717391304347826,"B.3
THRESHOLDS OF NETWORKS AS CIRCUITS"
E,0.7192028985507246,"In the proof of Theorem 3.2, we also need the following basic fact that signs of ReLU networks can
be computed in P/poly.
Lemma B.4. For any f ∈F∗, there is a Turing machine that, given any input y, outputs sgn(f(y))
after poly(d) steps."
E,0.7210144927536232,Published as a conference paper at ICLR 2022
E,0.7228260869565217,"Proof. Recall that the weight matrices W1, . . . , WL of f have entries in Rτ for τ = poly(d). So
for any 1 ≤ℓ≤L, diagonal matrices D1 ∈{0, 1}k1×k1, . . . , Dℓ−1 ∈{0, 1}kℓ−1×kℓ−1, and vector
y ∈{±1}d, every entry of the vector"
E,0.7246376811594203,"WℓDℓ−1(Wℓ−1Dℓ−2(· · · (W1y + b1) · · · ) + bℓ−1) + bℓ
has bit complexity bounded by log2 "
E,0.7264492753623188,"ℓ· 2O(ℓτ)
ℓ−1
Y"
E,0.7282608695652174,"i=1
ki !"
E,0.730072463768116,"= O(ℓτ + S) = poly(d),"
E,0.7318840579710145,"where in the second step we used that log(ki) ≤ki for all i ∈[ℓ−1]. So for any input to f, every
intermediate activation has poly(d) bit complexity."
E,0.7336956521739131,"The Turing machine we exhibit for computing sgn(f(y)) will compute the activations in the network
layer by layer. The entries of W1y + b1 can readily be computed in poly(d) time. Now given the
vector of activations
v = Wℓφ(· · · φ(W1y + b1) · · · ) + bℓ
for some ℓ≥1 (where v is represented on a tape of the Turing machine as a bitstring of length
poly(d)), we need to compute Wℓ+1φ(v) + bℓ+1. The ReLU activation can be readily computed
in poly(d) time, so in poly(d) additional steps we can form this new vector of activations at the
(ℓ+ 1)-layer. So within S · poly(d) = poly(d) steps the Turing machine will have written down
f(y) (represented as a bitstring of length poly(d)) on one of its tapes, after which it will return the
sign of this quantity."
E,0.7355072463768116,"B.4
PROOF OF THEOREM 3.2"
E,0.7373188405797102,We now give a complete proof of Theorem 3.2:
E,0.7391304347826086,"Proof. The parameter m will be clear from context in the following discussion, so for convenience
we will refer to d(m) and Gm as d and G. Let k, P, G be such that the outcome of Assumption 1
holds, and negl(·) denote the function indicating the extent to which G fools poly-sized circuits.
By Corollary 3.3, every output coordinate of G is computable by a network in Cτ,Λ
L,S,m for τ =
O(k), Λ = exp(O(k)), L = k, S = O(2kk)."
E,0.7409420289855072,"We ﬁrst check that W1(G(Um), Ud) > 1/3.
Note that G(Um) has support of size 2m.
In
Lemma A.13 we can take µ = Ud and conclude that µ is (2m, 2(1 −2m−d))-diverse, so
W1(G(Um), Ud) ≥2(1 −2m−d) = 2(1 −2m−mc) ≥1."
E,0.7427536231884058,"It remains to check that G fools F∗relative to Ud. Suppose to the contrary that there exists some
f ∈F∗and absolute constant a > 0 for which |E[f(G(Um))] −E[f(Ud)]| > 1/da. We will argue
that this implies there is a poly-sized circuit C : {±1}d →{±1} distinguishing G(Um) from Ud."
E,0.7445652173913043,"First note that for any threshold t ∈Rτ, by Lemma B.4 there is a Turing machine Mτ : {±1}d →
{±1} that computes y 7→sgn(f(y) −t) with τ bits of advice. So if there existed a threshold t ∈Rτ
for which
|E[Mτ(G(Um))] −E[Mτ(Ud)]| > 1/da′,
(13)
for some constant a′ > 0, then by Fact A.8, there would exist a Boolean circuit C distinguishing
G(Um) from Ud with non-negligible advantage, contradicting Assumption 1 and concluding the
proof."
E,0.7463768115942029,"We will apply Lemma 3.4 to show the existence of such a threshold t. Speciﬁcally, deﬁne random
variables X = f(G(Um)) and Y = f(Ud). By Corollary A.5 applied to the poly(d)-Lipschitz
function f : {±1}d →{±1}, Y −E[Y ] is poly(d)-sub-Gaussian. And recalling that G ∈Cτ,Λ
L,S,m
for Λ = exp(O(k)), we can apply Corollary A.5 to the poly(d) · Ok(1)-Lipschitz function f ◦G :
{±1}m →{±1} to conclude that X −E[X] is σ2-sub-Gaussian for σ ≜poly(m) · exp(O(k)) =
poly(d). By Lemma 3.4, there exists a threshold t for which the left-hand side of (13) exceeds
min(1/2, eΩ(n−a/σ)), which is not negligible."
E,0.7481884057971014,"It remains to verify that t has bit complexity at most poly(d). As the entries in the weight matrices
and biases in f all have bit complexity poly(d) and f has size and depth poly(d), f(y) has bit"
E,0.75,Published as a conference paper at ICLR 2022
E,0.7518115942028986,"complexity poly(d) for any y ∈{±1}d. Similarly, the entries in the weight matrices and biases
in G all have bit complexity O(k) = O(1), so f(G(x)) has bit complexity poly(d) for any x ∈
{±1}m. By the bound on t in Lemma 3.4 and our bound on σ above, t therefore also has poly(d)
bit complexity."
E,0.7536231884057971,"B.5
PROOF OF LEMMA 3.6"
E,0.7554347826086957,Proof. Suppose to the contrary that there existed some function f ∈F′ for which
E,0.7572463768115942,|E[f(J(p))] −E[f(J(q))]| > 2ϵ · Λ′.
E,0.7590579710144928,"By Lemma 2.1 and our choice of S′′, the composition f ◦J : Rs →R can be computed by a
network in Cτ,ΛΛ′√r
L,S,s
whose bias and weight vector entries in the output layer lie in Rτ ′."
E,0.7608695652173914,We ﬁrst show why this would lead to a contradiction. Consider the function h ≜1
E,0.7626811594202898,C · f ◦J for
E,0.7644927536231884,"C = 2⌈log2 Λ′√r⌉∈[Λ′, 2Λ′),"
E,0.7663043478260869,"which can be computed by taking the network computing f◦J and scaling the bias and weight vector
in the output layer by C. Note that this scaling results in bias and weight vector entries in the output
layer for h with bit complexity τ ′ + ⌈log2 Λ′√r⌉= τ. Furthermore, h is ΛΛ′√r/C ≤Λ-Lipschitz,
so h ∈Cτ,Λ
L,S,s. On the other hand, we would have"
E,0.7681159420289855,"|E[h(p)] −E[h(q)]| > 2ϵΛ′√r/C ≥ϵ,"
E,0.769927536231884,"yielding the desired contradiction of the assumption that WF(p, q) ≤ϵ."
E,0.7717391304347826,"B.6
PROOF OF FACT 3.7"
E,0.7735507246376812,"Proof. Note that h(Un) is the uniform distribution over multiples of 1/2n in the interval [0, 1).
Given any such multiple z, let pz denote the uniform distribution over [z, z + 1/2n). One way of
sampling from I1 is thus to sample z from h(Un) and then sample from pz."
E,0.7753623188405797,"Now consider any 1-Lipschitz function f : R →R. Note that for any z′ in the support of pz,
|f(z) −f(z′)| ≤1/2n ≤ϵ. We have"
E,0.7771739130434783,"|E[f(h(Un))] −E[f(I1)]| =

E
z∼h(Un) "
E,0.7789855072463768,"E
z′∼pz
[f(z) −f(z′)]
 ≤ϵ"
E,0.7807971014492754,as desired.
E,0.782608695652174,"B.7
PROOF OF LEMMA 3.8"
E,0.7844202898550725,"Proof. Let n ≜⌈log(1/ϵ)⌉. For every i ∈[r], deﬁne Si ≜{(i −1) · n + 1, . . . , i · n}. Take J to
be the linear function where for every i ∈[r], the i-th output coordinate of J is the linear function
which maps y ∈Rs to ⟨wi, y + 1⟩where wi is zero outside of Si and, over coordinates indexed by
Si, equal to the vector (1/4, . . . , 1/2n+1). Note that each output coordinate of J is computed by a
function in Cn+1,O(1)
1,0,s
."
E,0.7862318840579711,"By Fact 3.7 and Fact A.11,"
E,0.7880434782608695,"WF∗(J(Us), Ir) ≤poly(r) · W1(J(Us), Ir) ≤poly(r) · ϵ."
E,0.7898550724637681,"On the other hand, by Lemma 3.6 and the fact that the union of Cτ−⌈log2 Λ√r⌉,Λ
L−1,S−r,r
over τ, Λ, L, S =
poly(s) is still F∗, we conclude that"
E,0.7916666666666666,"WF∗(J( eD), J(Us)) ≤O(ϵ√r),"
E,0.7934782608695652,from which the lemma follows by triangle inequality.
E,0.7952898550724637,Published as a conference paper at ICLR 2022
E,0.7971014492753623,"B.8
PROOF OF LEMMA 3.9"
E,0.7989130434782609,"Proof. Let J : Rs →Rr be given by Lemma 3.8. We know that WF∗(J( eD), Ir) ≤ϵ · poly(r).
By Lemma 3.6 applied to these two distributions and the generator function H, together with the"
E,0.8007246376811594,"fact that the union of Cτ ′−⌈log2 Λ
√"
E,0.802536231884058,"d⌉,Λ
L−L′,S−d(S′+1),d over Λ, L, S = poly(s) is still F∗, we thus have that"
E,0.8043478260869565,"WF∗(H(J( eD)), H(Ir)) ≤O(ϵΛ′√"
E,0.8061594202898551,d) · poly(r) = ϵΛ′ · poly(s).
E,0.8079710144927537,"We will thus take J′ in the lemma to be H ◦J. By Lemma 2.1, each output coordinate of J′ is"
E,0.8097826086956522,"computed by a function in Cmax(O(log(1/ϵ)),τ ′),O(Λ′√"
E,0.8115942028985508,"d)
L′+1,r+S′,s
as claimed."
E,0.8134057971014492,"B.9
PROOF OF THEOREM 3.5"
E,0.8152173913043478,"Proof of Theorem 3.5. The parameter m will be clear from context in the following discussion, so
for convenience we will refer to r(m), d(m), ϵ(m), Hm, Gm as r, d, ϵ, H, G, and similarly for the
network parameters τ ′, Λ′, L′, S′."
E,0.8170289855072463,"It is easy to verify condition 3 before we even deﬁne G: because G(Um) is a uniform distribution on
2m points (with multiplicity) and H(Ir) is (2m, Ω(1))-diverse, W1(G(Um), Id) ≥Ω(1) as claimed."
E,0.8188405797101449,"Let s = r · ⌈log(1/ϵ)⌉. As we are assuming ϵ ≥exp(−O(m)), s ≤r · m = mc for some
constant c > 1. If s ≤m, then deﬁne G′ : Rm →Rs to be the map given by projecting to
the ﬁrst s coordinates so that G′(Um) and Us are identical as distributions. Otherwise, take G′ to
be the generator G : Rm →Rs constructed in Theorem 3.2, recalling that WF∗(G′(Um), Us) ≤
negl(m) ≤ϵ."
E,0.8206521739130435,"Next, by applying Lemma 3.9 to eD = G′(Um), we get a function J′ : Rs →Rd each of whose"
E,0.822463768115942,"output coordinates is computed by a function in Cmax(O(log(1/ϵ)),τ ′),O(Λ′√"
E,0.8242753623188406,"d)
L′+1,r+S′,s
such that"
E,0.8260869565217391,"WFd(J′(G′(Um)), H(Ir)) ≤ϵΛ′ · poly(m) ≤ϵ · poly(m),
(14)"
E,0.8278985507246377,where the second step follows by our assumption on Λ′.
E,0.8297101449275363,"We will take G ≜J′ ◦G′. (14) establishes condition 2 of the theorem. Finally, by Lemma 2.1, every
output coordinate of G can be realized by a network in Cτ,Λ
L,S,m for τ = max(O(log(1/ϵ)), τ ′, O(1)),
Λ = O(Λ′√"
E,0.8315217391304348,"ds) = O(Λ′) · poly(m), L = L′ + O(1), and S = O(s) + r + S′ = O(s) + S′ (where
we used the fact that r = s/⌈log(1/ϵ)⌉< s). This establishes condition 1 of the theorem."
E,0.8333333333333334,"B.10
PROOF OF LEMMA 3.10"
E,0.8351449275362319,"Proof. Note that
hξ(x) = φ(x/ξ + 1) −φ(x/ξ −1) −1,
(15)"
E,0.8369565217391305,so we can take weight matrices
E,0.8387681159420289,"W1 =

1/ξ
1/ξ"
E,0.8405797101449275,"
W2 = (1
−1)"
E,0.842391304347826,"and biases b1 = (1, −1) and b2 = −1. Note that hξ is 1/ξ-Lipschitz. We conclude that hξ ∈
Cτ,1/ξ
2,2,1 ."
E,0.8442028985507246,"B.11
PROOF OF LEMMA 3.11"
E,0.8460144927536232,"Proof. We ﬁrst verify that W1(G0(Um), G(γm)) ≤ϵ. Take any 1-Lipschitz function f. Note that
we can sample from Um by sampling a vector g from γm, applying hξ entrywise to g, and replacing
each resulting entry of hξ(g) by its sign; importantly, the last step only affects entries i ∈[m] for
which |gi| < ξ."
E,0.8478260869565217,"We will deﬁne E to be the event that |gi| ≥ξ for all i ∈[m], noting that"
E,0.8496376811594203,"P[E] ≥1 −m ·
P
g∼N(0,1)[|g| < ξ] ≥1 −mξ
p 2/π."
E,0.8514492753623188,Published as a conference paper at ICLR 2022
E,0.8532608695652174,We can thus write
E,0.855072463768116,|E[f(G0(Um))] −E[f(G(γm))]|
E,0.8568840579710145,"=
 E
g∼γm[(f(G0(hξ(g))) −f(G(g))) · 1[E] + (f(G0(sgn(hξ(g)))) −f(G(g))) · 1[Ec]]"
E,0.8586956521739131,"=
 E
g∼γm[(f(G0(sgn(hξ(g)))) −f(G0(hξ(g)))) · 1[Ec]]
.
(16)"
E,0.8605072463768116,"By Fact A.2, f ◦G0 is Λ′′√"
E,0.8623188405797102,"d-Lipschitz. Furthermore, because hξ(g) ∈[−1, 1]m, ∥sgn(hξ(g)) −
hξ(g)∥≤√m. We can thus upper bound (16) by ≤Λ′′√"
E,0.8641304347826086,"md · P[Ec] ≤Λ′′ξ
p"
E,0.8659420289855072,(2/π)m3d ≤ϵ
E,0.8677536231884058,"so W1(G0(Um), G(γm)) ≤ϵ as desired."
E,0.8695652173913043,"It remains to bound the complexity of G. For any i ∈[d], we can apply Lemma 2.1 with f given by
the i-th output coordinate of G0 and J given by the map which applies hξ to every entry of the input.
We thus conclude that G ∈Cτ,Λ
L,S,m for τ = max(τ ′′, log2(1/ξ)) = max(τ ′′, O(log(Λ′′md/ϵ))),
Λ = Λ′′√m/ξ = O(Λ′′2m2√"
E,0.8713768115942029,"d/ϵ), L = L′′ + 2, S = 3m + S′′ as claimed."
E,0.8731884057971014,"C
FOOLING RELU NETWORKS WOULD IMPLY NEW CIRCUIT LOWER
BOUNDS"
E,0.875,"In this section we show that even exhibiting generators with logarithmic stretch that can fool all
ReLU network discriminators of constant depth and slightly superlinear size would yield break-
through circuit lower bounds."
E,0.8768115942028986,"First, in Section C.1 we review basics about average-case hardness and recall the state-of-the-art for
lower bounds against TC0. Then in Section C.2 we present and prove the main result of this section,
Theorem C.2."
E,0.8786231884057971,"C.1
AVERAGE-CASE HARDNESS AND TC0"
E,0.8804347826086957,"One of the most common notions of hardness for a class of functions F is worst-case hardness, that
is, the existence of functions which cannot be computed by functions in F.
Deﬁnition 10 (Worst-case hardness). Given a class of Boolean functions F, a sequence of functions
fn : {±1}n →{±1} is worst-case-hard for F if for every f : {±1}n →{±1} in F, there is some
input x ∈{±1}n for which f(x) ̸= fn(x)."
E,0.8822463768115942,"A more robust notion of hardness is that of average-case hardness, which implies worst-case hard-
ness. For any fn ∈F, rather than simply require that there is some input on which f and fn
disagree, we would like that over some ﬁxed distribution over possible inputs, the probability that f
and fn output the same value is small. Typically, this ﬁxed distribution is the uniform distribution
over {±1}n, but in many situations even showing average-case hardness with respect to less natural
distributions is open.
Deﬁnition 11 (Average-case hardness). Given a class of Boolean functions F, a function ϵ : N →
[0, 1/2), and a sequence of distributions {Dn}n over {±1}n, a sequence of functions fn : {±1}n →
{±1} is (1/2+ϵ(n))-average-case-hard for F with respect to {Dn} if for every f : {±1}n →{±1}
in F,"
E,0.8840579710144928,"P
x∼Dn
[f(x) = fn(x)] ≤1"
E,0.8858695652173914,2 + ϵ(n).
E,0.8876811594202898,"By a counting argument, for any reasonably constrained class F there must exist functions which
are worst/average-case hard for F. A central challenge in complexity theory has been to exhibit
explicit hard functions for natural complexity classes. In the context of this work, by explicit we
simply mean that there is a polynomial-time algorithm for evaluating the function."
E,0.8894927536231884,"The complexity class we will focus on in this section is TC0, the class of constant-depth linear
threshold circuits of polynomial size:"
E,0.8913043478260869,Published as a conference paper at ICLR 2022
E,0.8931159420289855,"Deﬁnition 12 (Linear threshold circuits). A linear threshold circuit of size S and depth D is any
Boolean circuit of size S and depth D whose gates come from the set G of all linear threshold
functions mapping x ∈{±1}n to sgn(⟨w, x⟩−b) for some arity n ∈N, vector w ∈Rn, and bias
b ∈R. TC0 is the set of all linear threshold circuits of size poly(n) and depth O(1).6"
E,0.894927536231884,"The best-known worst-case hardness result for TC0 is that of (Impagliazzo et al., 1997) who showed:
Theorem C.1 ((Impagliazzo et al., 1997)). Let fn : {±1}n →{±1} be the parity function on n
bits. For any depth D ≥1, any linear threshold circuit of depth D must have at least n1+cθ−D"
E,0.8967391304347826,"wires, where c > 0 and θ > 1 are absolute constants."
E,0.8985507246376812,"In (Chen et al., 2016) this worst-case hardness result was upgraded to an average-case hardness result
with respect to the uniform distribution over the hypercube. Remarkably, this slightly superlinear
lower bound from (Impagliazzo et al., 1997) has not been improved upon in over two decades!"
E,0.9003623188405797,"We remark that the discussion about lower bounds for threshold circuits is a very limited snapshot
of a rich line of work over many decades. We refer to the introduction in (Chen & Tell, 2019) for a
more detailed overview of this literature."
E,0.9021739130434783,"C.2
HARDNESS VERSUS RANDOMNESS FOR GANS"
E,0.9039855072463768,"For convenience, given sequences of parameters D(m), S(m) ∈N (these will eventually correspond
to the depth and size of the linear threshold circuits against which we wish to show lower bounds)
let
Cd(D, S) ≜Cpoly(d),poly(d)
Θ(D),3d+Θ(S),d.
This will comprise the family of ReLU network discriminators that we will focus on. We now
show that if one could exhibit generators that can provably fool discriminators in Cd(D, S), then
this would translate to average-case hardness against linear threshold circuits of depth D and size
D. Formally, we show the following:
Theorem C.2. There is an absolute constant c > 0 for which the following holds.
Fix se-
quences of parameters D(m), S(m) ∈N. Suppose there is an explicit7 sequence of generators
Gm : Rm →Rd(m) for d(m) ≥cm log m such that WCd(m)(D(m),S(m))(Gm(Im), Id(m)) ≤ϵ(m)
for some ϵ(m) ≥1/poly(m) and such that each output coordinate of Gm is computable by a net-
work in F∗, then there exists a sequence of functions hd(m) : {±1}d(m) →{±1} in NP which are
(1/2+ϵ(m)/2+m−Ω(m))-average-case-hard with respect to some sequence of explicit distributions
{Dm} for linear threshold circuits of depth D(m) and size S(m).
Remark C.3. In particular, this shows that if we could exhibit explicit generators fooling all discrimi-
nators given by neural networks of polynomial Lipschitzness/bit complexity of depth D(m) and size
O(d(m)1+exp(−D(m).99)), then by (2) we would get new average-case circuit lower bounds for TC0.
In fact it was shown by (Chen & Tell, 2019) that such a result would imply TC0 ̸= NC1, which would
be a major breakthrough in complexity. This can be interpreted in one of two ways: 1) it would be
extraordinarily difﬁcult to show that a particular generative model truly fools all constant-depth,
barely-superlinear-size ReLU network discriminators, or 2) gives a learning-theoretic motivation
for trying to prove circuit lower bounds."
E,0.9057971014492754,"Regarding the proof of Theorem C.2, note that the statement is closely related to existing well-
studied connections between hardness and randomness in the study of pseudorandom generators. In
fact, readers familiar with this literature will observe that Theorem C.2 is the GAN analogue of the
“easy” direction of the equivalence between hardness and randomness: an explicit pseudorandom
generator that fools some class of functions implies average-case-hardness for that class."
E,0.907608695652174,"In order to leverage this connection however, we need to formalize the link between GANs (over
continuous domains) and pseudorandom generators (over discrete domains) in the next lemma. It"
E,0.9094202898550725,"6Sometimes TC0 is deﬁned with the gate set taken to consist of {∧, ∨, ¬} and majority gates, though these
two classes are equivalent up to polynomial overheads (Goldmann et al., 1992; Goldmann & Karpinski, 1998).
Moreover, because a circuit of size S and depth D using the latter gate set is clearly implementable by a circuit
of size S and depth D using the former gate set, so our lower bounds against the former gate set immediately
translate to ones against the latter.
7By explicit, we mean that we are provided a way to evaluate these functions in polynomial time."
E,0.9112318840579711,Published as a conference paper at ICLR 2022
E,0.9130434782608695,"turns out that in the preceding sections we already developed most of the ingredients for establishing
this connection."
E,0.9148550724637681,"Lemma C.4. Suppose there is an explicit sequence of generators Gm : Rm →Rd(m) such that
WCd(m)(D(m),S(m))(Gm(Im), Id(m)) ≤ϵ(m) for some ϵ(m) = 1/poly(m) and such that each
output coordinate of Gm is computable by a network in F∗. Then there is an explicit sequence of
pseudorandom generators G′
m : {±1}n(m) →{±1}d(m) for n(m) = Θ(m log m) that 2ϵ(m)-fool
linear threshold circuits of depth D(m) and size S(m)."
E,0.9166666666666666,"Proof. As in the proofs of the theorems from Section 3, the parameter m will be clear from context,
so we will drop m from subscripts and parenthetical references."
E,0.9184782608695652,"Recall the function hξ from Lemma 3.10; we will take ξ = ϵ/poly(m).
Also deﬁne n ≜
Θ(log(m/ϵ)) and recall from the proof of Lemma 3.8 the deﬁnition of the linear function J :
Rmn →Rm: for every i ∈[m], the i-th output coordinate of J is the linear function which maps
x ∈Rmn to ⟨wi, x + 1⟩, where wi is zero outside of indices {(i −1) · n + 1, . . . , i · n} and equal
to the vector (1/4, 1/8, . . . , 1/2n+1) on those indices."
E,0.9202898550724637,"Given generator G fooling Cd, we will show that the Boolean function G′ : {±1}mn →{±1}d
given by
G′ = hξ ◦G ◦J"
E,0.9221014492753623,"is a pseudorandom generator that fools TC0 circuits. To that end, suppose there was a TC0 circuit
f : {±1}d →{±1} for which |E[f(G′(Umn))] −E[f(Ud)]| > 2ϵ. We will show that this implies
the existence of a ReLU network f ′ ∈Cd(D, S) for which |E[f ′(G(Im))] −E[f ′(Id)]| > ϵ."
E,0.9239130434782609,Our proof proceeds in three steps: argue that
E,0.9257246376811594,"1. f ◦hξ ∈Cd(D, S)"
E,0.927536231884058,2. E[f(Ud)] ≈E[f(hξ(Id))]
E,0.9293478260869565,3. E[G′(Umn)] ≈E[f(hξ(G(Im)))]
E,0.9311594202898551,"Note that 2 and 3, together with the fact that f is a discriminator for G′, imply that f ′ ≜f ◦hξ
is a discriminator for G. 1 then ensures that this discriminator is a ReLU network with the right
complexity bounds, yielding the desired contradiction."
E,0.9329710144927537,"To show step 1, we will show that f can be computed by a network in Cpoly(d),poly(d)
O(1),poly(d),d and then apply
Lemma 2.1 and Lemma 3.10. Suppose the threshold circuit computing f has depth D, where D is
some constant. Recall from Lemma A.7 that we may assume, up to an additional blowup in size by
D, that the constant-depth threshold circuit C computing f is comprised of layers S1, . . . , SD such
that Si consists of all gates in C for which any path from the inputs to the gate is of length i."
E,0.9347826086956522,"Let ki denote the number of gates in Si (where kD = 1), and for each j ∈[ki], suppose the linear
threshold function computed by the j-th gate in Si is given by sgn(⟨wi,j, ·⟩−bi,j) for wi,j ∈Rki−1.
As each linear threshold takes at most poly(d) bits as input, we can assume without loss of generality
that bi,j and the entries of wi,j lie in Rτ for τ = poly(d). For this τ, note that for any w ∈Rk
τ, b ∈
Rτ, x ∈{±1}k,
sgn(⟨w, x⟩−b) = hξ′ (⟨w, x⟩−b) ,"
E,0.9365942028985508,"for some ξ′ = 1/poly(d), where h1/poly(d)(·) is the function deﬁned in Lemma 3.10, and recall
from the proof of Lemma 3.10 that it can be represented as a two-layer ReLU network via (15). For
every i ∈[D], we can thus deﬁne two weight matrices W(1)
i
∈R2ki×ki−1 and W(2)
i
∈Rki×2ki by"
E,0.9384057971014492,"W(1)
i
= 1 ξ′ · "
E,0.9402173913043478,"



"
E,0.9420289855072463,"wi,1
wi,1
...
...
...
wi,ki
wi,ki "
E,0.9438405797101449,"




W(2)
i
=  

"
E,0.9456521739130435,"1
−1
0
0
· · ·
0
0
0
0
1
−1
· · ·
0
0
...
...
...
...
...
...
...
0
0
0
0
· · ·
1
−1  

"
E,0.947463768115942,Published as a conference paper at ICLR 2022
E,0.9492753623188406,"and biases b(1)
i
∈R2ki and b(2)
i
∈Rki by"
E,0.9510869565217391,"b(1)
i
= (1, −1, 1, −1, . . . , 1, −1)
b(2)
i
= (−1, . . . , −1)"
E,0.9528985507246377,"so that for all x ∈{±1}d,"
E,0.9547101449275363,"f(x) = W(2)
D φ

W(1)
D φ

· · · φ

W(2)
1 φ

W(1)
1 x + b(1)
1

+ b(2)
1

· · ·

+ b(1)
D

+ b(2)
D
(17)"
E,0.9565217391304348,"The entries of the weight matrices and bias vectors are clearly in Rpoly(d), and because each hξ′
is poly(d)-Lipschitz and there are D = O(1) layers in the circuit, the function in (17) is poly(d)-
Lipschitz as a function over Rd. The size and depth of the network are within a constant factor of
the size S and depth D of the circuit. Lemma 2.1 and Lemma 3.10 then imply that f ◦hξ has depth
Θ(D) and size 3d + Θ(S), as well as Lipshitzness and bit complexity polynomial in m because
ϵ ≥1/poly(m) so that ξ ≥1/poly(m). Therefore, f ◦hξ ∈Cd(D, S)."
E,0.9583333333333334,"To show step 2,
recall from Lemma 3.11 and Remark 3.12 that W1(Um, hξ(Im))
≤
ϵ/poly(m). Recalling that f is poly(d) = poly(m)-Lipschitz, we obtain the desired inequality
|E[f(Ud)] −E[f(hξ(Id))]| ≤ϵ/2. Here the factor of 1/2 is an arbitrary small constant coming
from taking the poly(m) in the deﬁnition of ξ sufﬁciently large."
E,0.9601449275362319,"Finally, to show step 3, recall by Fact 3.7 that W1(J(Umn), Im) ≤ϵ2/poly(m) by our choice of
n = Θ(log(m/ϵ)) (the ϵ2 comes from taking the constant factor in the deﬁnition of n sufﬁciently
large). By applying Fact A.2 to f ◦hξ and G, we know that the composition f ◦hξ ◦G is poly(m)/ϵ-
Lipschitz. It follows that |E[G′(Umn)] −E[f(hξ(G(Im)))]| ≤ϵ/2. The factor of 1/2 is an arbitrary
small constant coming from taking the constant factor in the deﬁnition of n sufﬁciently large."
E,0.9619565217391305,"Putting everything together, we conclude by triangle inequality that
|E[f(G(Im))] −E[f(Id)]| > ϵ,
a contradiction."
E,0.9637681159420289,"The following lemma gives the standard transformation from pseudorandom generators to average-
case hardness. We include a proof for completeness.
Lemma C.5 (Prop. 5 of (Viola, 2009)). Suppose the sequence of functions Gm : {±1}m →
{±1}d(m) ϵ(m)-fools a class of Boolean functions F. Deﬁne the function hd(m) : {±1}d(m) →
{±1} by"
E,0.9655797101449275,"hd(m)(x) =
1
exists y ∈{±1}m such that G(y) = x
−1
otherwise
."
E,0.967391304347826,"Let Dd(m) be the distribution over {±1}d(m) given by the uniform mixture between Ud(m) and
G(Um)."
E,0.9692028985507246,"Then the sequence of functions {hd(m)} is (1/2 + ϵ′(m))-average-case-hard for F with respect to
{Dd(m)} for ϵ′(m) = ϵ(m)/4 + 2m−d(m)−1."
E,0.9710144927536232,"Proof. As usual, we will omit most subscripts/parentheses referring to the parameter m. Let f :
{±1}d →{±1} be any function in F. Then"
E,0.9728260869565217,P[f(D) = hd(D)] = 1
E,0.9746376811594203,2 P[f(Ud) = hd(Ud)] + 1
E,0.9764492753623188,2 P[f(G(Um)) = hd(G(Um))] ≤1
E,0.9782608695652174,2 (P[f(Ud) = 0] + P[hd(Ud) = 1]) + 1
E,0.980072463768116,2 P[f(G(Um)) = 1] ≤1
E,0.9818840579710145,"2
 
P[f(Ud) = 0] + 2m−d
+ 1"
E,0.9836956521739131,2 P[f(G(Um)) = 1] ≤1
E,0.9855072463768116,"2
 
P[f(Ud) = 0] + 2m−d
+ 1"
E,0.9873188405797102,2 (P[f(Ud) = 1] + ϵ/2) = 1 2 + ϵ
E,0.9891304347826086,"4 + 2m−d−1,"
E,0.9909420289855072,"where in the second step we used a union bound and the fact that h(G(Um)) is deterministically 1
by construction, in the third step we used the fact that P[hd(Ud)] ≤2m−d because there are at most
2m elements in the range of G, and in the fourth step we used the fact that G ϵ-fools functions in
F."
E,0.9927536231884058,Published as a conference paper at ICLR 2022
E,0.9945652173913043,We are now ready to prove Theorem C.2.
E,0.9963768115942029,"Proof of Theorem C.2. By Lemma C.4, we can construct out of the generators Gm an explicit se-
quence of pseudorandom generators that stretch Θ(m log m) bits to d(m) ≥c · m log m bits and
2ϵ(m)-fool linear threshold circuits of size S(m) and depth D(m). The theorem follows upon
substituting this into Lemma C.5, which implies (1/2 + ϵ′(m))-average-case-hardness for such
circuits with respect to the explicit distributions Dd(m) deﬁned in Lemma C.5, where ϵ′(m) =
ϵ(m)/2+2Θ(m log m)−d(m)−1 = ϵ(m)/2+m−Ω(m), provided the absolute constant c is sufﬁciently
large."
E,0.9981884057971014,"Finally, note that the average-case-hard functions hd(m) we get from Lemma C.5 are in NP because
given an input x and a certiﬁcate y, one can easily verify whether G(y) = x."
