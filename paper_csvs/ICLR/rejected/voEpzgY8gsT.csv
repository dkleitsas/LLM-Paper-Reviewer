Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0018484288354898336,"We present the Additive Poisson Process (APP), a novel framework that can model
the higher-order interaction effects of the intensity functions in Poisson processes
using projections into lower-dimensional space. Our model combines the tech-
niques from information geometry to model higher-order interactions on a statis-
tical manifold and in generalized additive models to use lower-dimensional pro-
jections to overcome the effects from the curse of dimensionality. Our approach
solves a convex optimization problem by minimizing the KL divergence from a
sample distribution in lower-dimensional projections to the distribution modeled
by an intensity function in the Poisson process. Our empirical results show that
our model is able to use samples observed in the lower dimensional space to esti-
mate the higher-order intensity function with extremely sparse observations."
INTRODUCTION,0.0036968576709796672,"1
INTRODUCTION"
INTRODUCTION,0.005545286506469501,"The Poisson process is a counting process used in a wide range of disciplines such as spatial-
temporal sequential data in transportation (Zhou et al., 2018; 2021), ﬁnance (Ilalan, 2016) and ecol-
ogy (Thompson, 1955) to model the arrival rate by learning an intensity function. For a given time
interval, the integral of the intensity function represents the average number of events occurring in
that interval."
INTRODUCTION,0.0073937153419593345,"The intensity function can be generalized to multiple dimensions. However, for most practical appli-
cations, learning the multi-dimensional intensity function becomes a challenge because of the sparse
observations. Despite the recent advances of Poisson processes, most current Poisson process mod-
els are unable to learn the intensity function of a multi-dimensional Poisson process. Our research
question is, “Are there any good ways of approximating the high dimensional intensity function?”
Our proposed model, the Additive Poisson Process (APP), provides a novel solution to this problem."
INTRODUCTION,0.009242144177449169,"Throughout this paper, we will use a running example in a spatial-temporal setting. Say we want
to learn the intensity function for a taxi to pick up customers at a given time and location. For this
setting, each event is multi-dimensional; that is, (xi, yi, Wi), where a pair of xi and yi represents
two spatial coordinates and Wi represents the day of the week. In addition, observation time is
associated with each component of the event, which is represented as ti = (txi, tyi, tWi). In this
example, it is reasonable to assume txi = tyi = tWi as they are observed simultaneously. For any
given location or time, we can expect at most a few pick-up events, which makes it difﬁcult for any
model to learn the low-valued intensity function. Figure 2b visualizes this problem. In this problem
setup, if we would like to learn the intensity function at a given location (x, y) and day of the week
W, the na¨ıve approach would be to learn the intensity at (x, y, W) directly from observations. This
is extremely difﬁcult because there could be only few events for a given location and day."
INTRODUCTION,0.011090573012939002,"However, there is useful information in lower-dimensional space; for example, the marginalized ob-
servations at the location (xi, yi) across all days of the week, or on the day Wi at all locations. This
information can be included into the model to improve the estimation of the joint intensity function.
Using the information in lower-dimensional space provides a structured approach to include prior
information based on the location or day of the week to improve the estimation of the joint intensity
function. For example, a given location could be a shopping center or a hotel, where it is common
for taxis to pick up passengers, and therefore we expect more passengers at this location. There"
INTRODUCTION,0.012939001848428836,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.014787430683918669,"could also be additional patterns that could be uncovered based on the day of the week. We can then
use the observations of events to update our knowledge of the intensity function."
INTRODUCTION,0.0166358595194085,"Previous approaches, such as kernel density estimation (KDE) (Rosenblatt, 1956), are able to learn
the joint intensity function by using information in lower dimensions. However, KDE suffers from
the curse of dimensionality, which means that it requires a large size of samples to build an accurate
model. In addition, the complexity of the model expands exponentially with respect to the number of
dimensions, which makes it infeasible to compute. Bayesian approaches, such as using a mixture of
beta distributions with a Dirichlet prior (Kottas, 2006; Kottas & Sans´o, 2007) and Reproducing Ker-
nel Hilbert Space (RKHS) (Flaxman et al., 2017), have been proposed to quantify the uncertainty for
the intensity function. However, these approaches are often non-convex, making it difﬁcult to obtain
the globally optimal solution. Besides, if observations are sparse, it is hard for these approaches to
learn a reasonable intensity function. Additional related work about Bayesian inference for Poisson
processes and Poisson factorization can be found in Appendix A."
INTRODUCTION,0.018484288354898338,"In this paper, we propose a novel framework to learn the higher-order interaction effects of intensity
functions in Poisson processes. Our model combines the techniques introduced by Luo & Sugiyama
(2019) to model higher-order interactions between Poisson processes and by Friedman & Stuetzle
(1981) in generalized additive models to learn the joint intensity function using samples in a lower
dimensional space. Our proposed approach is to decompose a multi-dimensional Poisson process
into lower-dimensional representations. For example, we have points (xi)N
i=1 in the x-dimension
and (yi)N
i=1 in the y-dimension. Such data in the lower-dimensional space can be used to improve
the estimation of the joint intensity function. This is different from the traditional approaches where
only the joint occurrence of events is used to learn the joint intensity."
INTRODUCTION,0.02033271719038817,"We ﬁrst show the connection between generalized additive models and Poisson processes, and then
provide the connection between generalized additive models and the log-linear model (Agresti,
2012), which has a well-established theoretical background in information geometry (Amari, 2016).
We draw parallels between the formulation of the generalized additive models and the binary log-
linear model on a partially ordered set (poset) (Sugiyama et al., 2017). The learning process in our
model is formulated as a convex optimization problem to arrive at a unique optimal solution using
natural gradient, which minimizes the Kullback-Leibler (KL) divergence from the sample distri-
bution in a lower-dimensional space to the distribution modeled by the learned intensity function.
This connection provides remarkable properties to our model: the ability to learn higher-order inten-
sity functions using lower-dimensional projections, thanks to the Kolmogorov-Arnold representation
theorem. This property makes it advantageous to use our proposed approach for cases where there
are no observations, missing samples, or low event rates. Our model is ﬂexible because it can cap-
ture the interaction effects between events in a Poisson process as a partial order structure in the
log-linear model and the parameters of the model are fully customizable to meet the requirements of
the application. Our empirical results show that our model effectively uses samples projected onto
a lower dimensional space to estimate the higher-order intensity function. More importantly, our
model is also robust to various sample sizes."
FORMULATION,0.022181146025878003,"2
FORMULATION"
FORMULATION,0.024029574861367836,"We start this section by introducing the technical background in the Poisson process and its extension
to a multi-dimensional Poisson process. We then introduce the Generalized Additive Model (GAM)
and its connection to the Poisson process. This is followed by presenting our novel framework,
called Additive Poisson Process (APP), which is our main technical contribution and has a tight link
to the Poisson process modeled by GAMs. We show that the learning of APP can be achieved via
convex optimization using natural gradient."
FORMULATION,0.025878003696857672,"The Poisson process is characterized by an intensity function λ : R →R. An inhomogeneous
Poisson process is an extension of a homogeneous Poisson process, where the arrival rate changes
with time. The process with time-changing intensity λ(t) is deﬁned as a counting process N(t),
which has an independent increment property. For any time t ≥0 and inﬁnitesimal interval δ ≥0,
the probability of events count is p(N(t+δ)−N(t) = 0) = 1−δλ(t)+o(δ), p(N(t+δ)−N(t) = 1) =
δλ(t)+o(δ), and p(N(t+δ)−N(t) ≥2) = o(δ), where o(·) denotes little-o notation (Daley & Vere-
Jones, 2007). This formulation can be generalized into the (inhomogeneous) multi-dimensional
Poisson process, which can be used to learn the intensity function λ : RD →R from a realization"
FORMULATION,0.027726432532347505,Under review as a conference paper at ICLR 2022
FORMULATION,0.029574861367837338,"of timestamps t1, t2, . . . , tN with ti ∈[0, T]D. Each ti is the time of occurrence for the i-th event
across D dimensions and T is the observation duration. The likelihood for the multi-dimensional
Poisson process (Daley & Vere-Jones, 2007) is given by"
FORMULATION,0.031423290203327174,"p

{ti}N
i=1 | λ (t)

= exp

−
Z
λ(t)dt
 N
Y"
FORMULATION,0.033271719038817,"i=1
λ (ti) ,
(1)"
FORMULATION,0.03512014787430684,"where t = (t(1), . . . , t(D)) ∈RD. We deﬁne the functional prior on λ(t) as"
FORMULATION,0.036968576709796676,"λ(t) := g (f(t)) = exp (f(t)) .
(2)"
FORMULATION,0.038817005545286505,"The function g(·) is a positive function to guarantee the non-negativity of the intensity which we
choose to be the exponential function, and our objective is to learn the function f(·). The log-
likelihood of the multi-dimensional Poisson process with the functional prior is described as"
FORMULATION,0.04066543438077634,"log p

{ti}N
i=1 |λ (t)

= N
X"
FORMULATION,0.04251386321626617,"i=1
f(ti) −
Z
exp (f (t)) dt.
(3)"
FORMULATION,0.04436229205175601,"In the following sections, we introduce the generalized additive models and propose to model it by
the log-linear model to learn f(t) and the normalizing term (
R
exp (f (t)) dt)."
GENERALIZED ADDITIVE MODEL,0.04621072088724584,"2.1
GENERALIZED ADDITIVE MODEL"
GENERALIZED ADDITIVE MODEL,0.04805914972273567,"In this section, we present the connection between Poisson processes and the Generalized Additive
Model (GAM) proposed by Friedman & Stuetzle (1981). The GAM projects higher-dimensional
features into lower-dimensional space to apply smoothing functions to build a restricted class of
non-parametric regression models.
GAM is less affected by the curse of dimensionality com-
pared to directly using smoothing in a higher-dimensional space. For a given set of processes
J ⊆[D] = {1, . . . , D}, the traditional GAM using one-dimensional projections is deﬁned as
log λJ(t) = P"
GENERALIZED ADDITIVE MODEL,0.04990757855822551,j∈J fj(t(j)) −βJ with some smoothing function fj.
GENERALIZED ADDITIVE MODEL,0.051756007393715345,"In this paper, we extend it to include higher-order interactions between features in GAM by introduc-
ing terms that represent the multiplicative product between events. The k-th order GAM is deﬁned
as"
GENERALIZED ADDITIVE MODEL,0.053604436229205174,"log λJ(t) =
X"
GENERALIZED ADDITIVE MODEL,0.05545286506469501,"j∈J
f{j}(t(j)) +
X"
GENERALIZED ADDITIVE MODEL,0.05730129390018484,"j1,j2∈J
f{j1,j2}(t(j1), t(j2)) + · · · +
X"
GENERALIZED ADDITIVE MODEL,0.059149722735674676,"j1,...,jk∈J
f{j1,...,jk}(t(j1), . . . , t(jk)) −βJ =
X"
GENERALIZED ADDITIVE MODEL,0.06099815157116451,"I⊆J, |I|≤k
fI(t(I)) −βJ,
(4)"
GENERALIZED ADDITIVE MODEL,0.06284658040665435,"where t(I) ∈R|I| denotes the subvector (t(j))j∈I of t with respect to I ⊆[D]. The function fI :
R|I| →R is a smoothing function to ﬁt the data, and the normalization constant βJ for the intensity
function is obtained as βJ =
R
λJ(t)dt =
R
exp( P"
GENERALIZED ADDITIVE MODEL,0.06469500924214418,"I⊆J, |I|≤k fI(t(I)) )dt. The deﬁnition of the
additive model is in the same form as Equation (3). In particular, if we compare Equation (3) and
Equation (4), we can see that the smoothing function f in Equation (3) is realized as the summation
over lower-dimensional projections in Equation (4)."
GENERALIZED ADDITIVE MODEL,0.066543438077634,"Learning of a continuous function using lower-dimensional projections is well known because of
the Kolmogorov-Arnold representation theorem, which states that:"
GENERALIZED ADDITIVE MODEL,0.06839186691312385,"Theorem 1 (Kolmogorov–Arnold Representation Theorem (Braun & Griebel, 2009; Kol-
mogorov, 1957)). Any multivariate continuous function can be represented as a superposition of
one–dimensional functions; that is, f (t1, . . . , tn) = P2n+1
q=1 fq
Pn
p=1 gq,p (tp)

."
GENERALIZED ADDITIVE MODEL,0.07024029574861368,"Braun (2009) showed that the GAM is an approximation to the general form presented in
Kolmogorov-Arnold representation theorem by replacing the range q ∈{1, . . . , 2n + 1} with
I
⊆J and the inner function gq,p by the identity if q = p and zero otherwise, yielding
f(t) = P"
GENERALIZED ADDITIVE MODEL,0.07208872458410351,I⊆J fI(t(I)).
GENERALIZED ADDITIVE MODEL,0.07393715341959335,Under review as a conference paper at ICLR 2022
GENERALIZED ADDITIVE MODEL,0.07578558225508318,"τ = 1
τ = 2"
GENERALIZED ADDITIVE MODEL,0.07763401109057301,Order of interaction
GENERALIZED ADDITIVE MODEL,0.07948243992606285,between processes Time
GENERALIZED ADDITIVE MODEL,0.08133086876155268,"(Ω, ≺) τ = 0 τ = M"
GENERALIZED ADDITIVE MODEL,0.08317929759704251,"{1}
{2}
{3}"
GENERALIZED ADDITIVE MODEL,0.08502772643253234,"{1,2}
{1,3}{2,3}"
GENERALIZED ADDITIVE MODEL,0.08687615526802218,"{1,2,3}"
GENERALIZED ADDITIVE MODEL,0.08872458410351201,"{1}
{2}
{3}"
GENERALIZED ADDITIVE MODEL,0.09057301293900184,"{1,2}
{1,3}{2,3}"
GENERALIZED ADDITIVE MODEL,0.09242144177449169,"{1,2,3} ⊥"
GENERALIZED ADDITIVE MODEL,0.09426987060998152,"Figure 1: Partial order structured sam-
ple space (Ω, ⪯) with D = 3. Each
node represents a state and and the di-
rected edge represents the direction of
the partial ordering."
GENERALIZED ADDITIVE MODEL,0.09611829944547134,"(a)
(b)
Figure 2: (a) A visualisation of the truncated parame-
ter space to approximate the joint intensity function with
D = 4 and k = 2. (b) A visualization of the input datasets,
where the blue points represent events with two spatial di-
mensions and one time dimension.
Interestingly, the canonical form for additive models in Equation (4) can be rearranged to be in the
same form as Kolmogorov-Arnold representation theorem. By letting f(t) = P
I⊆J fI(t(I)) =
g−1(λ(t)) and g(·) = exp(·), we have"
GENERALIZED ADDITIVE MODEL,0.09796672828096119,"λJ(t) =
1
exp (βJ) exp
X"
GENERALIZED ADDITIVE MODEL,0.09981515711645102,"I⊆J fI

t(I)
∝exp
X"
GENERALIZED ADDITIVE MODEL,0.10166358595194085,"I⊆J fI

t(I)
,
(5)"
GENERALIZED ADDITIVE MODEL,0.10351201478743069,"where we assume fI(t(I)) = 0 if |I| > k for the k-th order model and 1/ exp(βJ) is the normal-
ization term for the intensity function. Based on the Kolmogorov-Arnold representation theorem,
generalized additive models are able to learn the intensity of the higher-order interaction between
Poisson processes by using projections into lower dimensional spaces. The log-likelihood function
for a kth-order model is obtained by substituting λJ(t) in Equation (4) into λ(t) in Equation (3),"
GENERALIZED ADDITIVE MODEL,0.10536044362292052,"log p
 
{t}N
i=1|λ (t)

= N
X i=1 X"
GENERALIZED ADDITIVE MODEL,0.10720887245841035,"I⊆J, |I|≤k
fI

t(I)
−β′,
(6)"
GENERALIZED ADDITIVE MODEL,0.10905730129390019,"where β′ is a constant given by β′ =
R
λ(t)dt+P"
GENERALIZED ADDITIVE MODEL,0.11090573012939002,"I⊆J βJ. In the following subsection we introduce
a log-linear formulation equipped with partially ordered sample space, which aligns with the GAM
formulation in Equation (5)."
ADDITIVE POISSON PROCESS,0.11275415896487985,"2.2
ADDITIVE POISSON PROCESS"
ADDITIVE POISSON PROCESS,0.11460258780036968,"We introduce our key technical contribution in this section. We introduce a log-linear formulation
called the additive Poisson process to estimate the parameters for the higher-order interactions in
equation 6. We begin by discretizing the time window [0, T] into M bins and treat each bin as a
natural number τ ∈[M] = {1, 2, . . . , M} for each process. The discretization avoids the need
to compute the intractable integral in the likelihood function in Equation (6). The discretization
approach tends to perform better in high-dimension compare to the alternative approaches such
as variational inference. We assume that M is predetermined by the user. First we introduce a
structured space for the Poisson process to incorporate interactions between processes. Let Ω=
{ (J, τ) | J ∈2[D] \ ∅, τ ∈[M] } ∪{(⊥, 0)}. We deﬁne the partial order ⪯(Davey & Priestley,
2002) on Ωas
ω = (J, τ) ⪯ω′ = (J′, τ ′) ⇐⇒J ⊆J′ and τ ≤τ ′,
for each ω, ω′ ∈Ω,
(7)
and (⊥, 0) ⪯ω for all ω ∈Ω, which is illustrated in Figure 1. The relation J ⊆J′ is used to
model any-order interactions between Poisson processes (Luo & Sugiyama, 2019) (Amari, 2016,
Section 6.8.4) and each τ in (J, τ) represents the auto-regressive component (“time”) in our model.
The symbol ⊥denotes the least element in the partial order structure, which is required to normalize
probabilities in the log-linear model. Each node ω in the partially ordered set (poset) 1 represents the
state of the sample space and the arrows in Figure 1 represent the partial order relationship between
two nodes2; that is, if ω →ω′, then ω ⪯ω′."
ADDITIVE POISSON PROCESS,0.11645101663585952,"1In information geometry, this corresponds to hypergraphs which have simplicial complex structure Ay et al.
(2018, Section 2.9).
2This graph structure should not be confused with the graph structure studied in graphical models, where the
nodes typically represent a random variable and the arrows represent the relationship between the two random
variables."
ADDITIVE POISSON PROCESS,0.11829944547134935,Under review as a conference paper at ICLR 2022
ADDITIVE POISSON PROCESS,0.12014787430683918,Algorithm 1 Additive Poisson Process (APP)
ADDITIVE POISSON PROCESS,0.12199630314232902,"1: Function APP({ti}N
i=1, S, M, h):
2: Initialize Ωwith the number M of bins
3: Apply Gaussian Kernel with bandwidth h
on {ti}N
i=1 to compute ˆp
4: Compute ˆη = (ˆηs)s∈S from ˆp
5: Initialize θ = (θs)s∈S (randomly or θs =
0)
6: repeat
7:
Compute p using the current θ
=
(θs)s∈S
8:
Compute η = (ηs)s∈S from p
9:
∆η ←η −ˆη
10:
Compute the Fisher information matrix
G using Equation (11)
11:
θ ←θ −G−1∆η
12: until convergence of θ = (θs)s∈S
13: End Function"
ADDITIVE POISSON PROCESS,0.12384473197781885,"Intuitively, the greatest node for each τ
∈
[M], which is ({1, 2, 3}, τ) in Figure 1, rep-
resents the multi-dimensional Poisson process.
Other nodes represent projections onto lower-
dimensional space that correspond to the marginal-
ized observations; for example, {{1}, {2}, {3}}
and {{1, 2}, {1, 3}, {2, 3}} represent the ﬁrst- and
second-order processes. Using our example in In-
troduction, where we wanted to estimate the in-
tensity function of a pick-up event of a taxi, {1}
and {2} correspond to spacial coordinates x and
y, respectively, and {3} to the day of the week
W, and τ represents the (discretized) observation
time.
We can then update our belief to model
the second-order intensity function using obser-
vations of the second order events. For example,
{1, 2}, {1, 3}, {2, 3} represents an event occurring
at {x, y}, {x, W}, and {y, W}. We can then con-
tinue this process to an arbitrary order of interac-
tions. Later on in this section we introduce the mathematics to estimate the higher-order function
using a restricted number of lower-dimensional projections."
ADDITIVE POISSON PROCESS,0.1256931608133087,"The domain of τ can be generalized from [M] to [M]D to take different time stamps into account,
while in the following we assume that observed time stamps are always the same across processes
for simplicity. Our experiments in the next section demonstrate that we can still accurately estimate
the density of processes. Our model can be applied not only to time-series data, but to any sequential
data."
ADDITIVE POISSON PROCESS,0.12754158964879853,"On any set equipped with a partial order, we can introduce a log-linear model (Sugiyama et al.,
2017). Let us assume that a parameter domain S ⊆Ωis given. For a partially ordered set (Ω, ⪯),
the log-linear model with parameters (θs)s∈S is introduced as"
ADDITIVE POISSON PROCESS,0.12939001848428835,"log p(ω; θ) =
X"
ADDITIVE POISSON PROCESS,0.13123844731977818,"s∈S 1[s⪯ω]θs −ψ(θ)
(8)"
ADDITIVE POISSON PROCESS,0.133086876155268,"for each ω ∈Ω, where 1[·] = 1 if the statement in [·] is true and 0 otherwise, and ψ(θ) ∈R is the par-
tition function uniquely obtained as ψ(θ) = log P"
ADDITIVE POISSON PROCESS,0.13493530499075784,ω∈Ωexp( P
ADDITIVE POISSON PROCESS,0.1367837338262477,"s∈S 1[s⪯ω]θs ) = −θ(⊥,0). A special
case of this formulation coincides with the density function of the Boltzmann machines (Sugiyama
et al., 2018; Luo & Sugiyama, 2019)."
ADDITIVE POISSON PROCESS,0.13863216266173753,"Here there is a clear correspondence between the log-linear formulation and that in the form of
Kolmogorov-Arnold representation theorem in Equation (5) if we rewrite Equation (8) as"
ADDITIVE POISSON PROCESS,0.14048059149722736,"p(ω; θ) =
1
exp ψ(θ) exp
X"
ADDITIVE POISSON PROCESS,0.1423290203327172,"s∈S 1[s⪯ω]θs

∝exp
X"
ADDITIVE POISSON PROCESS,0.14417744916820702,"s∈S 1[s⪯ω]θs

.
(9)"
ADDITIVE POISSON PROCESS,0.14602587800369685,"We call this model with (Ω, ⪯) deﬁned in Equation (7) the additive Poisson process, which rep-
resents the intensity λ as the joint distribution across all possible states. The intensity λ of the
multi-dimensional Poisson process given via the GAM in Equation (5) is fully modeled (parameter-
ized) by Equation (8) and each intensity fI(·) is obtained as θ(I,·). To consider the k-th order model,
we consistently use the parameter domain S, given as S = { (J, τ) ∈Ω| |J| ≤k }, where k is an
input parameter to the model that speciﬁes the upper bound of the order of interactions. This means
that θs = 0 for all s /∈S. Note that our model is well-deﬁned for any subset S ⊆Ωand the user can
use an arbitrary domain in applications. A visualization of the truncated parameter space is shown
in Figure 2a."
ADDITIVE POISSON PROCESS,0.1478743068391867,"For a given J ⊆[D] and each bin τ with ω = (J, τ), the empirical probability ˆp(ω) of input
observations is given as"
ADDITIVE POISSON PROCESS,0.14972273567467653,ˆp(ω) = 1 Z X
ADDITIVE POISSON PROCESS,0.15157116451016636,"I⊆J
σI(τ),
Z =
X"
ADDITIVE POISSON PROCESS,0.1534195933456562,"ω∈Ω
ˆp(ω),
σI(τ) :=
1
NhI N
X i=1
K"
ADDITIVE POISSON PROCESS,0.15526802218114602,"τ (I) −t(I)
i
hI ! (10)"
ADDITIVE POISSON PROCESS,0.15711645101663585,"for each discretized state ω = (J, τ), where τ = (τ, . . . , τ) ∈RD. The function σI performs
smoothing on time stamps t1, . . . , tN, which is the kernel smoother proposed by Buja et al. (1989)."
ADDITIVE POISSON PROCESS,0.1589648798521257,Under review as a conference paper at ICLR 2022
ADDITIVE POISSON PROCESS,0.16081330868761554,"The function K is a kernel and hI is the bandwidth for each projection I ⊆[D]. We use the Gaussian
kernel as K to ensure that probability is always nonzero, meaning that the deﬁnition of the kernel
smoother coincides with the kernel estimator of the intensity function proposed by Sch¨abe (1993)."
OPTIMIZATION,0.16266173752310537,"2.3
OPTIMIZATION"
OPTIMIZATION,0.1645101663585952,"Given an empirical distribution ˆp deﬁned in Equation (10), the task is to learn the parameter (θs)s∈S
such that the distribution via the log-linear model in Equation (8) is as close to ˆp as much as possible.
Let us deﬁne SS = {p | θs = 0 if s ̸∈S}, which is the set of distributions that can be represented
by the log-linear model using the parameter domain S. Then the objective function is given as
minp∈SS DKL(ˆp, p), where DKL(ˆp, p) = P"
OPTIMIZATION,0.16635859519408502,"ω∈Ωˆp log(ˆp/p) is the KL divergence from ˆp to p. In
this optimization, let p∗be the learned distribution from the sample with inﬁnitely large sample size
and let p be the learned distribution for each sample. Then we can lower bound the uncertainty
(variance) E[DKL(p∗, p)] by |S|/2N (Barron & Hengartner, 1998)."
OPTIMIZATION,0.16820702402957485,"Thanks to the well-developed theory of information geometry (Amari, 2016) for the log-linear
model (Amari, 2001), it is known that this problem can be solved by e-projection, which coincides
with the maximum likelihood estimation and is always convex optimization (Amari, 2016, Chapter
2.8.3). The gradient with respect to each parameter θs is obtained by (∂/∂θs)DKL(ˆp, p) = ηs −ˆηs,
where ηs = P"
OPTIMIZATION,0.17005545286506468,"ω∈Ω1[ω⪰s]p(ω). The value ηs is known as the expectation parameter (Sugiyama
et al., 2017) and ˆηs is obtained by replacing p with ˆp in the above equation. If ˆηs = 0 for some
s ∈S, we remove s from S to ensure that the model is well-deﬁned."
OPTIMIZATION,0.17190388170055454,"Let S = {s1, . . . , s|S|} and θ = [θs1, . . . , θs|S|]T , η = [ηs1, . . . , ηs|S|]T . We can always use
the natural gradient (Amari, 1998) as the closed form solution of the Fisher information matrix is
always available (Sugiyama et al., 2017). The update step is θnext = θ −G−1(η −ˆη), where the
Fisher information matrix G is obtained as"
OPTIMIZATION,0.17375231053604437,"gij =
∂
∂θsi∂θsj
DKL(ˆp, p) =
X"
OPTIMIZATION,0.1756007393715342,"ω∈Ω
1[ω⪰si]1[ω⪰sj]p(ω) −ηsiηsj.
(11)"
OPTIMIZATION,0.17744916820702403,"Theoretically the Fisher information matrix is numerically stable to perform a matrix inversion.
However, computationally, ﬂoating point errors may cause the matrix to become indeﬁnite. To over-
come this issue, a small positive value is added along the main diagonal of the matrix. This technique
is known as jitter and it is used in areas like Gaussian processes to ensure that the covariance matrix
is computationally positive semi-deﬁnite (Neal, 1999)."
OPTIMIZATION,0.17929759704251386,"The pseudocode for APP is shown in Algorithm 1. The time complexity of computing line 7 is
O(|Ω||S|). This means when implementing the model using gradient descent, the time complexity
of the model is O(|Ω||S|2) to update the parameters in S for each iteration. For natural gradient,
the cost of inverting the Fisher information matrix G is O(|S|3); therefore, the time complexity to
update the parameters in S is O(|S|3 + |Ω||S|) for each iteration. The time complexity for natural
gradient is signiﬁcantly higher higher because of the requirement to invert the ﬁsher information
matrix; if the number of parameters is small, it is more efﬁcient to use natural gradient because it
requires signiﬁcantly fewer iterations. However, if the number of parameters is large, it is more
efﬁcient to use gradient descent."
EXPERIMENTS,0.18114602587800369,"3
EXPERIMENTS"
EXPERIMENTS,0.18299445471349354,"We perform experiments using two-dimensional synthetic data, higher-dimensional synthetic data,
and real-world data to evaluate the performance of our proposed approach. Our code is imple-
mented in Python 3.7.5 with NumPy version 1.8.2 and the experiments are run on Ubuntu 18.04
LTS with an Intel i7-8700 6c/12t with 16GB of memory 3. In experiments with synthetic data, we
simulate random events using Equation (1). We generate an intensity function using a mixture of
Gaussians, where the mean is drawn from a uniform distribution and the covariance is drawn from
an inverted Wishart distribution. The intensity function is then the density function multiplied by
the sample size. The synthetic data is generated by directly drawing a sample from the probability
density function . An arbitrary number of samples is drawn from the mixture of Gaussians. We"
EXPERIMENTS,0.18484288354898337,3The code is available in the supplementary material and will be publicly available online.
EXPERIMENTS,0.1866913123844732,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.18853974121996303,"(a) Dense observations N = 100, 000."
EXPERIMENTS,0.19038817005545286,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
Kernel Bandwid h 10−3"
EXPERIMENTS,0.1922365988909427,KL Divergence
EXPERIMENTS,0.19408502772643252,Process: [1]
EXPERIMENTS,0.19593345656192238,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
Kernel Bandwid h 10−3 10−2"
EXPERIMENTS,0.1977818853974122,KL Divergence
EXPERIMENTS,0.19963031423290203,Process: [2]
EXPERIMENTS,0.20147874306839186,"Order: 1
Order: 2
KDE"
EXPERIMENTS,0.2033271719038817,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
Kernel Bandwid h 10−2 10−1"
EXPERIMENTS,0.20517560073937152,KL Divergence
EXPERIMENTS,0.20702402957486138,"Process: [1, 2]"
EXPERIMENTS,0.2088724584103512,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
Kernel Bandwid h 10−2 10−1"
EXPERIMENTS,0.21072088724584104,KL Divergence
EXPERIMENTS,0.21256931608133087,To al KL Divergence
EXPERIMENTS,0.2144177449168207,"(b) Sparse observations, N = 1000.
Figure 3: KL Divergence for second-order Poisson process. The order of the model (color of the
line) represents the k-th order model, i.e., k = 1 (blue) and k = 2 (orange)."
EXPERIMENTS,0.21626617375231053,"0
2
4
6
8
10
Time 0 500 1000 1500"
EXPERIMENTS,0.21811460258780038,Intensity
EXPERIMENTS,0.2199630314232902,Process: [1]
EXPERIMENTS,0.22181146025878004,"0
2
4
6
8
10
Time 0 500 1000 1500"
EXPERIMENTS,0.22365988909426987,Intensity
EXPERIMENTS,0.2255083179297597,Process: [2]
EXPERIMENTS,0.22735674676524953,"Ground Truth
Order: 1
Order: 2
KDE
RKHS
DP-beta"
EXPERIMENTS,0.22920517560073936,"0
2
4
6
8
10
Time 0 5 10 15 20 25 30"
EXPERIMENTS,0.23105360443622922,Intensity
EXPERIMENTS,0.23290203327171904,"Process: [1, 2]"
EXPERIMENTS,0.23475046210720887,"(a) Dense observations, N = 1000, h = 0.4."
EXPERIMENTS,0.2365988909426987,"0
2
4
6
8
10
Time 0 5 10 15 20"
EXPERIMENTS,0.23844731977818853,Intensity
EXPERIMENTS,0.24029574861367836,Process: [1]
EXPERIMENTS,0.24214417744916822,"0
2
4
6
8
10
Time 0 5 10 15 20"
EXPERIMENTS,0.24399260628465805,Intensity
EXPERIMENTS,0.24584103512014788,Process: [2]
EXPERIMENTS,0.2476894639556377,"Ground Truth
Order: 1
Order: 2
KDE
RKHS
DP-beta"
EXPERIMENTS,0.24953789279112754,"0
2
4
6
8
10
Time 0.0 0.1 0.2 0.3 0.4"
EXPERIMENTS,0.2513863216266174,Intensity
EXPERIMENTS,0.2532347504621072,"Process: [1, 2]"
EXPERIMENTS,0.25508317929759705,"(b) Sparse observations, N = 100, 000, h = 0.2.
Figure 4: Intensity function of two dimensional processes. Dots represent observations. Left: Repre-
sents marginalized observation of the ﬁrst dimension. Middle: Represents marginalized observation
of the second dimension. Right: The joint observation of dimension 1 and 2. The order of the model
(color of the line) represents the k-th order model, i.e., k = 1 (blue) and k = 2 (orange).
Table 1: The lowest KL divergence from the
ground truth distribution to the obtained distri-
bution on two types of single processes ([1] and
[2]) and joint process of them ([1,2]). APP-
# represents the order of the Additive Poisson
Process. Missing values mean that the compu-
tation did not ﬁnish within two days."
EXPERIMENTS,0.25693160813308685,"Process
APP-1
APP-2
KDE
RKHS
DP-beta Dense"
EXPERIMENTS,0.2587800369685767,"[1]
4.98e-5
4.98e-5
2.81e-4
-
-
[2]
2.83e-5
2.83e-5
1.17e-4
-
-
[1,2]
2.98e-2
1.27e-3
6.33e-4
4.09e-2
4.54e-2"
EXPERIMENTS,0.26062846580406657,Sparse
EXPERIMENTS,0.26247689463955637,"[1]
7.26e-4
7.26e-4
8.83e-4
1.96e-2
2.62e-3
[2]
2.28e-4
2.28e-4
2.76e-4
2.35e-3
2.49e-3
[1,2]
2.88e-2
1.77e-2
3.67e-3
1.84e-2
3.68e-2"
EXPERIMENTS,0.2643253234750462,"Table 2: Negative test log-likelihood for the
New York Taxi data. Single processes ([T] and
[W]) and joint process of them ([T,W]). APP-
# represents the order of the Additive Poisson
Process."
EXPERIMENTS,0.266173752310536,"Process
APP-1
APP-2
KDE
RKHS
DP-beta Jan"
EXPERIMENTS,0.2680221811460259,"[T]
714.07
714.07
713.77
728.13
731.01
[W]
745.60
745.60
745.23
853.42
790.04
[T,W]
249.60
246.05
380.22
259.29
260.30 Feb"
EXPERIMENTS,0.2698706099815157,"[T]
713.43
713.43
755.71
795.61
765.76
[W]
738.66
738.66
773.65
811.34
792.10
[T,W]
328.84
244.21
307.86
334.31
326.52 Mar"
EXPERIMENTS,0.27171903881700554,"[T]
716.72
716.72
733.74
755.48
741.28
[W]
738.06
738.06
816.99
853.33
832.43
[T,W]
291.20
246.19
289.69
328.47
300.36
then run our models and compare with Kernel Density Estimation (KDE) (Rosenblatt, 1956), an
inhomogeneous Poisson process whose intensity is estimated by a reproducing kernel Hilbert space
formulation (RKHS) (Flaxman et al., 2017), and a Dirichlet process mixture of Beta distributions
(DP-beta) (Kottas, 2006; Kottas & Sans´o, 2007). The hyper-parameters M and h in our proposed
model are selected using grid search and cross-validation. For situations where a validation set is
not available, then h could be selected using a rule of thumb approach such as Scott’s Rule (Scott,
2015) and M could be selected empirically from the input data by computing the time interval of
the joint observation."
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.2735674676524954,"3.1
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES"
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.2754158964879852,"For our experiment, we use 20 Gaussian components and simulate a dense case with 100,000 obser-
vations and a sparse case with 1,000 observations within the time frame of 10 seconds. We consider
that a joint event occurs if the two events occur 0.1 seconds apart. Figure 3a and Figure 3b com-
pare the KL divergence between the ﬁrst- and second-order models and plots in Figure 4 are the
corresponding intensity functions. In the ﬁrst-order processes, both ﬁrst- and second-order models"
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.27726432532347506,Under review as a conference paper at ICLR 2022
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.27911275415896486,"0.0
0.5
1.0
1.5
2.0
Kernel Bandwidth 102 103 104 105"
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.2809611829944547,KL Divergence
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.2828096118299446,Process: [1]
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.2846580406654344,"0.0
0.5
1.0
1.5
2.0
Kernel Bandwidth 102 103 104"
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.28650646950092423,KL Divergence
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.28835489833641403,"Process: [1, 2]"
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.2902033271719039,"0.0
0.5
1.0
1.5
2.0
Kernel Bandwidth 102 103"
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.2920517560073937,KL Divergence
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.29390018484288355,"Process: [1, 2, 3]"
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.2957486136783734,"0.0
0.5
1.0
1.5
2.0
Kernel Bandwidth 101 102"
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.2975970425138632,KL Divergence
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.29944547134935307,"Process: [1, 2, 3, 4]"
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.30129390018484287,"0.0
0.5
1.0
1.5
2.0
Kernel Bandwidth 103 104 105"
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.3031423290203327,KL Divergence
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.3049907578558225,Total KL Divergence
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.3068391866913124,"Order: 1
Order: 2"
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.30868761552680224,"Order: 3
Order: 4"
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.31053604436229204,"(a) Dense observations, N = 107."
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.3123844731977819,"0.0
0.5
1.0
1.5
2.0
Kernel Bandwidth 102 103"
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.3142329020332717,KL Divergence
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.31608133086876156,Process: [1]
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.3179297597042514,"0.0
0.5
1.0
1.5
2.0
Kernel Bandwidth 101 102"
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.3197781885397412,KL Divergence
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.32162661737523107,"Process: [1, 2]"
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.3234750462107209,"0.0
0.5
1.0
1.5
2.0
Kernel Bandwidth 100 101"
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.32532347504621073,KL Divergence
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.32717190388170053,"Process: [1, 2, 3]"
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.3290203327171904,"0.0
0.5
1.0
1.5
2.0
Kernel Bandwidth 100 101"
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.33086876155268025,KL Divergence
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.33271719038817005,"Process: [1, 2, 3, 4]"
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.3345656192236599,"0.0
0.5
1.0
1.5
2.0
Kernel Bandwidth 102 103"
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.3364140480591497,KL Divergence
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.33826247689463956,Total KL Divergence
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.34011090573012936,"Order: 1
Order: 2"
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.3419593345656192,"Order: 3
Order: 4"
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.3438077634011091,"(b) Sparse observations, N = 105.
Figure 5: KL Divergence for fourth-order Poisson process. We selected four representative examples
for our experimental results, full results available in the supplementary material. The line color
signiﬁes the order of the model, i.e., k = 1 (blue), k = 2 (orange), k = 3 (green) and k = 4 (red)."
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.3456561922365989,"0
2
4
6
8
10
Time 0 50000"
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.34750462107208874,100000
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.34935304990757854,150000
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.3512014787430684,Intensity
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.35304990757855825,Process: [1]
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.35489833641404805,"0
2
4
6
8
10
Time 0 1000 2000 3000"
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.3567467652495379,Intensity
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.3585951940850277,"Process: [1, 2]"
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.36044362292051757,"0
2
4
6
8
10
Time 0 20 40 60 80 100"
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.36229205175600737,Intensity
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.36414048059149723,"Process: [1, 2, 3]"
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.3659889094269871,"Order: 1
Order: 2"
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.3678373382624769,"Order: 3
Order: 4"
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.36968576709796674,Ground Truth
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.37153419593345655,"0
2
4
6
8
10
Time 0 1 2 3 4 5"
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.3733826247689464,Intensity
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.3752310536044362,"Process: [1, 2, 3, 4]"
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.37707948243992606,"(a) Dense observations, N = 107."
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.3789279112754159,"0
2
4
6
8
10
Time 0 500 1000 1500"
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.3807763401109057,Intensity
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.3826247689463956,Process: [1]
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.3844731977818854,"0
2
4
6
8
10
Time 0 10 20 30"
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.38632162661737524,Intensity
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.38817005545286504,"Process: [1, 2]"
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.3900184842883549,"0
2
4
6
8
10
Time 0.0 0.2 0.4 0.6 0.8"
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.39186691312384475,Intensity
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.39371534195933455,"Process: [1, 2, 3]"
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.3955637707948244,"Order: 1
Order: 2"
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.3974121996303142,"Order: 3
Order: 4"
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.39926062846580407,Ground Truth
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.4011090573012939,"0
2
4
6
8
10
Time 0.00 0.02 0.04 0.06 0.08"
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.4029574861367837,Intensity
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.4048059149722736,"Process: [1, 2, 3, 4]"
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.4066543438077634,"(b) Sparse observations, N = 105.
Figure 6: Intensity function of higher dimensional processes. Dots represent observations. We
have selected four representative examples for our experimental results, full results available in the
supplementary material. The order of the model (color of the line) represents the k-th order model,
i.e., k = 1 (blue), k = 2 (orange), k = 3 (green) and k = 4 (red)."
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.40850277264325324,"have the same performance. This is expected, as both of the models can treat ﬁrst-order interactions
and are able to learn the empirical intensity function exactly, which is the superposition of the one-
dimensional projection of the Gaussian kernels on each observation. For the second-order process,
the second-order model performs better than the ﬁrst-order model because it is able to directly learn
the intensity function from the projection onto the two-dimensional space. In contrast, the ﬁrst-order
model must approximate the second-order process using the observations from the ﬁrst-order pro-
cesses. In the sparse case, the second-order model performs better when the correct bandwidth is
selected."
EXPERIMENTS ON TWO-DIMENSIONAL PROCESSES,0.41035120147874304,"Table 1 compares our approach APP with other state-of-the-art approaches. APP performs the best
for ﬁrst-order processes in both the sparse and dense experiments. Experiments for RKHS and
DP-beta were unable to complete running within two days for the dense experiment. In the second-
order process, our approach was outperformed by KDE, while the second-order APP was able to
outperform both RKHS and DP-beta process for both sparse and dense experiments. Figures 3a
and 3b show that KDE is sensitive to changes in bandwidth, which means that, for any practical
implementation of the model, second-order APP with a less sensitive bandwidth is more likely to
learn a more accurate intensity function when the ground truth is unknown."
EXPERIMENTS ON HIGHER-DIMENSIONAL PROCESSES,0.4121996303142329,"3.2
EXPERIMENTS ON HIGHER-DIMENSIONAL PROCESSES"
EXPERIMENTS ON HIGHER-DIMENSIONAL PROCESSES,0.41404805914972276,"We generate a fourth-order process to simulate the behavior of the model in higher dimensions. The
model is generalizable to higher dimensions, but it is difﬁcult to demonstrate results for processes
higher than fourth order. For our experiment, we generate an intensity function using 50 Gaussian
components and draw a sample with the size of 107 for the dense case and that with the size of 105
for the sparse case. We consider the joint event to be the time frame of 0.1 seconds."
EXPERIMENTS ON HIGHER-DIMENSIONAL PROCESSES,0.41589648798521256,"We were not able to run comparison experiments with other models because they are unable to learn
when there are no or few joint observations in third- and fourth-order processes. In addition, the time
complexity is too high to learn from joint observations in ﬁrst- and second-order processes because"
EXPERIMENTS ON HIGHER-DIMENSIONAL PROCESSES,0.4177449168207024,Under review as a conference paper at ICLR 2022
EXPERIMENTS ON HIGHER-DIMENSIONAL PROCESSES,0.4195933456561922,"all the other models have their time complexity proportional to the number of observations. The time
complexity for KDE is O(N D) for the dimensionality with D, while DP-beta is O(N 2K), where
K is the number of clusters, and RKHS is O(N 2) for each iteration with respect to the sample size
N, where DP-beta and RKHS are applied directly on the joint observation as they cannot use the
projections in lower-dimensional space. KDE is able to make an estimation of the intensity function
using projections in lower-dimensional space, but it was too computationally expensive to complete
running the experiment. By contrast, our model is more efﬁcient because the time complexity is
proportional to the number of bins in our model. The time complexity of APP for each iteration is
O(|Ω||S|), where |Ω| = M D and |S| = Pk
c=1
 D
c

. Our model scales combinatorially with respect
to the number of dimensions. However, this is unavoidable for any model that directly takes into
account the high-order interactions. For practical applications, the number of dimensions D and the
order of the model k is often small, making it feasible to compute."
EXPERIMENTS ON HIGHER-DIMENSIONAL PROCESSES,0.4214417744916821,"In Figure 5a, we observe similar behavior in the model, where the ﬁrst-order processes ﬁt precisely
to the empirical distribution generated by the Gaussian kernels. The third-order model is able to
predict better on the fourth-order process. This is because the observation shown in Figure 6a
is largely sparse and learning from the observations directly may overﬁt. A lower-dimensional
approximation is able to provide a better result in the third-order model. Similar trends can be seen
in the sparse case, as shown in Figure 5b, where a second-order model is able to produce better
estimation in third- and fourth-order processes. The observations are extremely sparse, as seen in
Figure 6b, where there are only a few observations or no observations at all to learn the intensity
function."
UNCOVERING COMMON PATTERNS IN THE NEW YORK TAXI DATASET,0.4232902033271719,"3.3
UNCOVERING COMMON PATTERNS IN THE NEW YORK TAXI DATASET"
UNCOVERING COMMON PATTERNS IN THE NEW YORK TAXI DATASET,0.42513863216266173,"We demonstrate the capability of our model on the 2016 Green Taxi Trip dataset4, which is a open
source dataset with a CC0: Public Domain licences. We are interested in ﬁnding the common
pick-up patterns across Tuesdays and Wednesdays. We deﬁne a common pick-up time to be within
1-minute intervals of each other between the two days. We have chosen to learn an intensity function
using the Poisson process for Tuesday and Wednesday and a joint process for both of them. The
joint process uncovers the common pick-up patterns between the two days. We have selected to use
the ﬁrst two Tuesdays and Wednesdays in January 2016 as our training and validation sets and the
Tuesday and Wednesday of the third week of January 2016 as our testing set. We repeat the same
experiment for February and March."
UNCOVERING COMMON PATTERNS IN THE NEW YORK TAXI DATASET,0.4269870609981516,"We show our results in Table 2, where we use the negative test log-likelihood as an evaluation
measure. APP-2 has consistently outperformed all the other approaches for the joint process between
Tuesday and Wednesday. In addition, for the individual process, APP-1 and -2 also showed the best
result for February and March. These results demonstrate the effectiveness of our model in capturing
higher-order interactions between processes, which is difﬁcult for the other existing approaches."
CONCLUSION,0.4288354898336414,"4
CONCLUSION"
CONCLUSION,0.43068391866913125,"We have proposed a novel framework, called Additive Poisson Process (APP), to learn the intensity
function of the higher-order interaction between Poisson processes using observations projected into
lower-dimensional spaces. We formulated our proposed model using the log-linear model and opti-
mize it using information geometric structure of the distribution space. We drew parallels between
our proposed model and generalized additive model and showed the ability to learn from lower
dimensional projections via the Kolmogorov-Arnold representation theorem. Our empirical results
show the superiority of our method when learning the higher-order interactions between Poisson
processes and when there are no or extremely sparse joint observations. Our model is also robust to
varying sample sizes. Our approach provides a novel formulation to learn the joint intensity function
which typically has extremely low intensity. There is enormous potential to apply APP to real-world
applications, where higher order interaction effects need to be modeled such as transportation, ﬁ-
nance and ecology."
CONCLUSION,0.43253234750462105,4https://data.cityofnewyork.us/Transportation/2016-Green-Taxi-Trip-Data/hvrh-b6nb
CONCLUSION,0.4343807763401109,Under review as a conference paper at ICLR 2022
ETHICS STATEMENT,0.43622920517560076,ETHICS STATEMENT
ETHICS STATEMENT,0.43807763401109057,"This paper is intended to be written in a neutral tone. Any suggestion of any application that may be
considered as unethical or have a negative social impact is only coincidental and not our intention.
The results of this work should only be used to have a positive impact in society."
REPRODUCIBILITY STATEMENT,0.4399260628465804,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.4417744916820702,"Our model is implemented in Python 3.7.5 with NumPy version 1.8.2. The code to reproduce
our experiments is available in the supplementary material and will be made publicly available on
GitHub after the paper is accepted."
REFERENCES,0.4436229205175601,REFERENCES
REFERENCES,0.4454713493530499,"Alan Agresti. Categorical Data Analysis. Wiley, 3 edition, 2012."
REFERENCES,0.44731977818853974,"Shun-Ichi Amari. Natural gradient works efﬁciently in learning. Neural Computation, 10(2):251–
276, 1998."
REFERENCES,0.4491682070240296,"Shun-Ichi Amari. Information geometry on hierarchy of probability distributions. IEEE Transac-
tions on Information Theory, 47(5):1701–1711, 2001."
REFERENCES,0.4510166358595194,"Sun-Ichi Amari. Information Geometry and Its Applications. Springer, 2016."
REFERENCES,0.45286506469500926,"Nihat Ay, Paolo Gibilisco, and F Matus. Information Geometry and Its Applications. Springer, 2018."
REFERENCES,0.45471349353049906,"A. Barron and N. Hengartner. Information theory and superefﬁciency. The Annals of Statistics, 26
(5):1800–1825, 1998."
REFERENCES,0.4565619223659889,"J¨urgen Braun. An application of Kolmogorov’s superposition theorem to function reconstruction in
higher dimensions. PhD thesis, Universit¨ats-und Landesbibliothek Bonn, 2009."
REFERENCES,0.4584103512014787,"J¨urgen Braun and Michael Griebel. On a constructive proof of Kolmogorov’s superposition theorem.
Constructive Approximation, 30(3):653, 2009."
REFERENCES,0.4602587800369686,"Andreas Buja, Trevor Hastie, and Robert Tibshirani. Linear smoothers and additive models. The
Annals of Statistics, 17(2):453–510, 1989."
REFERENCES,0.46210720887245843,"Eric C Chi and Tamara G Kolda. On tensors, sparsity, and nonnegative factorizations. SIAM Journal
on Matrix Analysis and Applications, 33(4):1272–1299, 2012."
REFERENCES,0.46395563770794823,"Daryl J Daley and David Vere-Jones. An Introduction to the Theory of Point Processes: Volume II:
General Theory and Structure. Springer, 2007."
REFERENCES,0.4658040665434381,"Brian A Davey and Hilary A Priestley. Introduction to Lattices and Order. Cambridge University
Press, 2002."
REFERENCES,0.4676524953789279,"Seth Flaxman, Yee Whye Teh, and Dino Sejdinovic. Poisson intensity estimation with reproducing
kernels. Electronic Journal of Statistics, 11(2):5081–5104, 2017."
REFERENCES,0.46950092421441775,"Jerome H Friedman and Werner Stuetzle. Projection pursuit regression. Journal of the American
Statistical Association, 76(376):817–823, 1981."
REFERENCES,0.4713493530499076,"Deniz Ilalan. A poisson process with random intensity for modeling ﬁnancial stability. The Spanish
Review of Financial Economics, 14(2):43–50, 2016."
REFERENCES,0.4731977818853974,"Andrei Nikolaevich Kolmogorov. On the representation of continuous functions of many variables
by superposition of continuous functions of one variable and addition. Doklady Akademii Nauk,
114(5):953–956, 1957."
REFERENCES,0.47504621072088726,"Athanasios Kottas. Dirichlet process mixtures of beta distributions, with applications to density
and intensity estimation. In Workshop on Learning with Nonparametric Bayesian Methods, 23rd
International Conference on Machine Learning (ICML), volume 47, 2006."
REFERENCES,0.47689463955637706,Under review as a conference paper at ICLR 2022
REFERENCES,0.4787430683918669,"Athanasios Kottas and Bruno Sans´o. Bayesian mixture modeling for spatial poisson process inten-
sities, with applications to extreme value analysis. Journal of Statistical Planning and Inference,
137(10):3151–3163, 2007."
REFERENCES,0.4805914972273567,"Chris Lloyd, Tom Gunter, Michael Osborne, and Stephen Roberts. Variational inference for gaussian
process modulated poisson processes. In International Conference on Machine Learning, pp.
1814–1822. PMLR, 2015."
REFERENCES,0.4824399260628466,"Simon Luo and Mahito Sugiyama. Bias-variance trade-off in hierarchical probabilistic models us-
ing higher-order feature interactions. In Proceedings of the 33rd AAAI Conference on Artiﬁcial
Intelligence, pp. 4488–4495, 2019."
REFERENCES,0.48428835489833644,"Radford M Neal. Regression and classiﬁcation using gaussian process priors. Bayesian Statistics,
6:475–501, 1999."
REFERENCES,0.48613678373382624,"Krzysztof Nowicki and Tom A B Snijders. Estimation and prediction for stochastic blockstructures.
Journal of the American statistical association, 96(455):1077–1087, 2001."
REFERENCES,0.4879852125693161,"Yosihiko Ogata. On Lewis’ simulation method for point processes. IEEE Transactions on Informa-
tion Theory, 27(1):23–31, 1981."
REFERENCES,0.4898336414048059,"Murray Rosenblatt. Remarks on some nonparametric estimates of a density function. The Annals of
Mathematical Statistics, pp. 832–837, 1956."
REFERENCES,0.49168207024029575,"Yves-Laurent Kom Samo and Stephen Roberts. Scalable nonparametric bayesian inference on point
processes with gaussian processes. In International Conference on Machine Learning, pp. 2227–
2236. PMLR, 2015."
REFERENCES,0.49353049907578556,"H Sch¨abe. Nonparametric estimation of intensities of nonhomogeneous poisson processes. Statisti-
cal Papers, 34(1):113–131, 1993."
REFERENCES,0.4953789279112754,"Aaron Schein, Mingyuan Zhou, David Blei, and Hanna Wallach. Bayesian poisson tucker decompo-
sition for learning the structure of international relations. In International Conference on Machine
Learning, pp. 2810–2819. PMLR, 2016."
REFERENCES,0.49722735674676527,"David W Scott. Multivariate density estimation: theory, practice, and visualization. John Wiley &
Sons, 2015."
REFERENCES,0.49907578558225507,"Mahito Sugiyama, Hiroyuki Nakahara, and Koji Tsuda. Tensor balancing on statistical manifold. In
Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceed-
ings of Machine Learning Research, pp. 3270–3279, 2017."
REFERENCES,0.5009242144177449,"Mahito Sugiyama, Hiroyuki Nakahara, and Koji Tsuda. Legendre decomposition for tensors. In
Advances in Neural Information Processing Systems 31, pp. 8825–8835, 2018."
REFERENCES,0.5027726432532348,"HR Thompson. Spatial point processes, with applications to ecology. Biometrika, 42(1/2):102–115,
1955."
REFERENCES,0.5046210720887245,"Feng Zhou, Zhidong Li, Xuhui Fan, Yang Wang, Arcot Sowmya, and Fang Chen. A reﬁned MISD
algorithm based on Gaussian process regression. In Paciﬁc-Asia Conference on Knowledge Dis-
covery and Data Mining, pp. 584–596. Springer, 2018."
REFERENCES,0.5064695009242144,"Feng Zhou, Simon Luo, Zhidong Li, Xuhui Fan, Yang Wang, Arcot Sowmya, and Fang Chen.
Efﬁcient em-variational inference for nonparametric hawkes process. Statistics and Computing,
31(4):1–11, 2021."
REFERENCES,0.5083179297597042,Under review as a conference paper at ICLR 2022
REFERENCES,0.5101663585951941,APPENDIX
REFERENCES,0.512014787430684,"A
RELATED WORK"
REFERENCES,0.5138632162661737,"A.1
BAYESIAN INFERENCE FOR POISSON PROCESS"
REFERENCES,0.5157116451016636,"Learning an intensity function from sparse high-dimensional datasets is a challenging problem. It
often comes a trade-off between computational complexity and numerical accuracy. For applications
which are sparse with higher dimensionality, numerical accuracy is often sacriﬁced to approximate
the intensity function."
REFERENCES,0.5175600739371534,"The na¨ıve approach using MCMC has cubic complexity O(n3) for each dimension and increases
exponentially with respect with additional input dimension, that is, O((n3)d), where n refers to the
number of observations and d is the number of dimensions. One example of this approach is using
a Dirichlet process mixture of Beta distribution as a prior for the intensity function of the Poisson
process (Kottas, 2006; Kottas & Sans´o, 2007). The solution from MCMC is often accurate and
asymtopically converges to the true posterior intensity function. However, due to its computational
complexity, it is infeasible to estimate any high-dimensional intensity function."
REFERENCES,0.5194085027726433,"More recent techniques have attempted to scale up these approaches by using Gaussian processes as
a functional prior to the intensity function (Samo & Roberts, 2015). However, the model complexity
is O(n2k) for each dimension and is exponential with respect to the number of input dimensions,
which is still infeasible to estimate any high-dimensional intensity function."
REFERENCES,0.5212569316081331,"Variational inference (Lloyd et al., 2015) approaches that can be used to make Bayesian inference of
Poisson process much more efﬁcient scaling it up to the linear complexity O(n) for each dimension.
However variational inference is not guaranteed to asymptotically converge to the true posterior
distribution."
REFERENCES,0.5231053604436229,"Our approach uses the discretization approach to scale up the model to higher dimensions. We use
a graph (partial order) structure to allow the ﬂexibility for domain expertise to speciﬁc how each
dimension is treated and which interaction effects should be included into the model."
REFERENCES,0.5249537892791127,"A.2
POISSON FACTORIZATION"
REFERENCES,0.5268022181146026,"Our work is closely related to Poisson Factorization (Chi & Kolda, 2012), where random variables
in a tensor are represented with a Poisson distributed or Poisson process likelihood. The tensor is
usually used to represent some high-dimensional dataset such as contingency tables or other collec-
tion of counting datasets, which are often large and sparse. The objective of Poison Factorization
is to decompose the high-dimensional sparse matrices into lower dimensional space, where we can
ﬁnd some meaningful latent structure."
REFERENCES,0.5286506469500925,"The effectiveness of Poisson factorization for high dimensional datasets makes it ideal to analyze
spatial-temporal problems consisting of sparse count data. One example of this work is Bayesian
Poisson Tucker decomposition (BPTD) (Schein et al., 2016), where a dataset of interaction events
is represented as a set of N events, each of which consists of a pair of a token ei that encodes certain
features and time, that is, (ei, ti). BPTD uses an MCMC inference algorithm to learn the latent
structure, which is based on an extension of stochastic block models (SBM) (Nowicki & Snijders,
2001) with a Poisson likelihood."
REFERENCES,0.5304990757855823,"Our approach provides a generalization of this idea of Poisson Factorization by using Legendre
tensor decomposition (Sugiyama et al., 2018) and demonstrating its ability on a spatial-temporal
problem. Our optimization is much more efﬁcient as it is guided by gradients to minimize the KL-
divergence. Our approach also contains a graph structure which allows domain experts to encode
certain properties into the model."
REFERENCES,0.532347504621072,Under review as a conference paper at ICLR 2022
REFERENCES,0.5341959334565619,Algorithm 2 Thinning Algorithm for non-homogenous Poisson Process
REFERENCES,0.5360443622920518,"1: Function Thinning Algorithm (λ (t), T):
2: n = m = 0, t0 = s0 = 0, ¯λ = sup0≤t≤T λ (t)
3: repeat
4:
u ∼uniform (0, 1)
5:
w = −1"
REFERENCES,0.5378927911275416,"¯λ ln u {w ∼exponential(¯λ)}
6:
sm+1 = sm + w
7:
D ∼uniform (0, 1)"
REFERENCES,0.5397412199630314,"8:
if D ≤λ(sm+1)"
REFERENCES,0.5415896487985212,"¯λ
then
9:
tn+1 = sm+1
10:
n = n + 1
11:
else
12:
m = m + 1
13:
end if
14:
if tn ≤T then
15:
return {tk}k=1,2,...,n
16:
else
17:
return {tk}k=1,2,...,n−1
18:
end if
19: until sm ≤T
20: End Function"
REFERENCES,0.5434380776340111,"B
ADDITIONAL EXPERIMENTS"
REFERENCES,0.5452865064695009,"B.1
BANDWIDTH SENSITIVITY ANALYSIS"
REFERENCES,0.5471349353049908,"Our ﬁrst experiment is to demonstrate the ability for our proposed model to learn an intensity func-
tion from samples. We generate a Bernoulli process with probably of p = 0.1 to generate samples
for every 1 seconds for 100 seconds to create a toy problem for our model. This experiment is to
observe the behaviour of varying the bandwidth in our model. In Figure 7a, we observe that apply-
ing no kernel, we learn the deltas of each individual observation. When we apply a Gaussian kernel,
the output of the model for the intensity function is much more smooth. Increasing the bandwidth
of the kernel will provide a wider and much smoother function. Between the 60 seconds and 80
seconds mark, it can be seen when two observations have overlapping kernels, the intensity function
becomes larger in magnitude."
REFERENCES,0.5489833641404805,"B.2
ONE DIMENSIONAL POISSON PROCESS"
REFERENCES,0.5508317929759704,"A one dimensional experiment is simulated using Ogata’s thinning algorithm (Ogata, 1981). We gen-
erate two experiments use the standard sinusoidal benchmark intensity function with a frequency of
20π. The dense experiment has troughs with 0 intensity and peaks at 201 and the sparse experiment
has troughs with 0 intensity and peaks at 2. Figure 7d shows the experimental results of the dense
case, our model has no problem learning the intensity function. We compare our results using KL
divergence between the underlying intensity function used to generate the samples to the intensity
function generated by the model. Figure 7b shows that the optimal bandwidth is h = 1."
REFERENCES,0.5526802218114603,Under review as a conference paper at ICLR 2022
REFERENCES,0.5545286506469501,"0
20
40
60
80
100
Time 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.55637707948244,Intensity
REFERENCES,0.5582255083179297,"No Kernel
Gaussian Kernel, h=1
Gaussian Kernel, h=2
Gaussian Kernel, h=5
Gaussian Kernel, h=10"
REFERENCES,0.5600739371534196,(a) Toy Example
REFERENCES,0.5619223659889094,"0
2
4
6
8
10
12
Kernel Bandwith 0.000 0.025 0.050 0.075 0.100 0.125"
REFERENCES,0.5637707948243993,KL Divergence
REFERENCES,0.5656192236598891,"(b) KL divergence of dense experi-
ment"
REFERENCES,0.5674676524953789,"0
2
4
6
8
10
12
Kernel Bandwith −0.2 −0.1 0.0 0.1 0.2 0.3"
REFERENCES,0.5693160813308688,KL Divergence
REFERENCES,0.5711645101663586,"(c) KL divergence of sparse experi-
ment"
REFERENCES,0.5730129390018485,(d) Ogata’s thinning algorithm with high intensity
REFERENCES,0.5748613678373382,"0
20
40
60
80
100
Time 0 2 4"
REFERENCES,0.5767097966728281,Intensity h=0.3
REFERENCES,0.5785582255083179,"0
20
40
60
80
100
Time 0 1 2 3"
REFERENCES,0.5804066543438078,Intensity h=1
REFERENCES,0.5822550831792976,"0
20
40
60
80
100
Time 0 1 2"
REFERENCES,0.5841035120147874,Intensity h=2
REFERENCES,0.5859519408502772,"0
20
40
60
80
100
Time 0 1 2"
REFERENCES,0.5878003696857671,Intensity h=4
REFERENCES,0.589648798521257,"0
20
40
60
80
100
Time 0 1 2"
REFERENCES,0.5914972273567468,Intensity h=5
REFERENCES,0.5933456561922366,"0
20
40
60
80
100
Time 0 1 2"
REFERENCES,0.5951940850277264,Intensity h=8
REFERENCES,0.5970425138632163,"IGPP
Ground Truth"
REFERENCES,0.5988909426987061,"0
20
40
60
80
100
Time 0 1 2"
REFERENCES,0.600739371534196,Intensity h=10
REFERENCES,0.6025878003696857,"0
20
40
60
80
100
Time 0 1 2"
REFERENCES,0.6044362292051756,Intensity h=50
REFERENCES,0.6062846580406654,(e) Ogata’s thinning algorithm with low intensity
REFERENCES,0.6081330868761553,Figure 7: One dimensional experiments
REFERENCES,0.609981515711645,Under review as a conference paper at ICLR 2022
REFERENCES,0.6118299445471349,"0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Kernel Bandwidth 10−6 10−5 10−4 10−3"
REFERENCES,0.6136783733826248,KL Divergence
REFERENCES,0.6155268022181146,Proce  : [1]
REFERENCES,0.6173752310536045,"0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Kernel Bandwidth 10−6 10−5 10−4 10−3"
REFERENCES,0.6192236598890942,KL Divergence
REFERENCES,0.6210720887245841,Proce  : [2]
REFERENCES,0.6229205175600739,"0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Kernel Bandwidth 10−6 10−5 10−4 10−3"
REFERENCES,0.6247689463955638,KL Divergence
REFERENCES,0.6266173752310537,Proce  : [3]
REFERENCES,0.6284658040665434,"0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Kernel Bandwidth 10−6 10−5 10−4 10−3"
REFERENCES,0.6303142329020333,KL Divergence
REFERENCES,0.6321626617375231,Proce  : [4]
REFERENCES,0.634011090573013,"0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Kernel Bandwidth 10−4 10−3 10−2"
REFERENCES,0.6358595194085028,KL Divergence
REFERENCES,0.6377079482439926,"Proce  : [1, 2]"
REFERENCES,0.6395563770794824,"0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Kernel Bandwidth 10−4 10−3 10−2"
REFERENCES,0.6414048059149723,KL Divergence
REFERENCES,0.6432532347504621,"Proce  : [1, 3]"
REFERENCES,0.6451016635859519,"0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Kernel Bandwidth 10−4 10−3 10−2"
REFERENCES,0.6469500924214417,KL Divergence
REFERENCES,0.6487985212569316,"Proce  : [1, 4]"
REFERENCES,0.6506469500924215,"0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Kernel Bandwidth 10−4 10−3 10−2"
REFERENCES,0.6524953789279113,KL Divergence
REFERENCES,0.6543438077634011,"Proce  : [2, 3]"
REFERENCES,0.6561922365988909,"0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Kernel Bandwidth 10−4 10−3 10−2"
REFERENCES,0.6580406654343808,KL Divergence
REFERENCES,0.6598890942698706,"Proce  : [2, 4]"
REFERENCES,0.6617375231053605,"0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Kernel Bandwidth 10−4 10−3 10−2"
REFERENCES,0.6635859519408502,KL Divergence
REFERENCES,0.6654343807763401,"Proce  : [3, 4]"
REFERENCES,0.66728280961183,"0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Kernel Bandwidth 10−3 10−2"
REFERENCES,0.6691312384473198,KL Divergence
REFERENCES,0.6709796672828097,"Proce  : [1, 2, 3]"
REFERENCES,0.6728280961182994,"0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Kernel Bandwidth 10−3 10−2 10−1"
REFERENCES,0.6746765249537893,KL Divergence
REFERENCES,0.6765249537892791,"Proce  : [1, 2, 4]"
REFERENCES,0.678373382624769,"0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Kernel Bandwidth 10−3 10−2 10−1"
REFERENCES,0.6802218114602587,KL Divergence
REFERENCES,0.6820702402957486,"Proce  : [1, 3, 4]"
REFERENCES,0.6839186691312384,"0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Kernel Bandwidth 10−3 10−2 10−1"
REFERENCES,0.6857670979667283,KL Divergence
REFERENCES,0.6876155268022182,"Proce  : [2, 3, 4]"
REFERENCES,0.6894639556377079,"Order: 1
Order: 2
Order: 3
Order: 4"
REFERENCES,0.6913123844731978,"0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Kernel Bandwidth 10−2 10−1"
REFERENCES,0.6931608133086876,KL Divergence
REFERENCES,0.6950092421441775,"Proce  : [1, 2, 3, 4]"
REFERENCES,0.6968576709796673,"0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Kernel Bandwidth 10−2 10−1 100"
REFERENCES,0.6987060998151571,KL Divergence
REFERENCES,0.7005545286506469,Total KL Divergence
REFERENCES,0.7024029574861368,(a) Dense observations.
REFERENCES,0.7042513863216266,"0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Kernel Bandwid h 10−4 10−3"
REFERENCES,0.7060998151571165,KL Divergence
REFERENCES,0.7079482439926063,Process: [1]
REFERENCES,0.7097966728280961,"0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Kernel Bandwid h 10−4 10−3"
REFERENCES,0.711645101663586,KL Divergence
REFERENCES,0.7134935304990758,Process: [2]
REFERENCES,0.7153419593345656,"0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Kernel Bandwid h 10−4 10−3"
REFERENCES,0.7171903881700554,KL Divergence
REFERENCES,0.7190388170055453,Process: [3]
REFERENCES,0.7208872458410351,"0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Kernel Bandwid h 10−4 10−3"
REFERENCES,0.722735674676525,KL Divergence
REFERENCES,0.7245841035120147,Process: [4]
REFERENCES,0.7264325323475046,"0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Kernel Bandwid h 10−3 10−2"
REFERENCES,0.7282809611829945,KL Divergence
REFERENCES,0.7301293900184843,"Process: [1, 2]"
REFERENCES,0.7319778188539742,"0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Kernel Bandwid h 10−3 10−2"
REFERENCES,0.7338262476894639,KL Divergence
REFERENCES,0.7356746765249538,"Process: [1, 3]"
REFERENCES,0.7375231053604436,"0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Kernel Bandwid h 10−3 10−2"
REFERENCES,0.7393715341959335,KL Divergence
REFERENCES,0.7412199630314233,"Process: [1, 4]"
REFERENCES,0.7430683918669131,"0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Kernel Bandwid h 10−3 10−2"
REFERENCES,0.744916820702403,KL Divergence
REFERENCES,0.7467652495378928,"Process: [2, 3]"
REFERENCES,0.7486136783733827,"0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Kernel Bandwid h 10−3 10−2"
REFERENCES,0.7504621072088724,KL Divergence
REFERENCES,0.7523105360443623,"Process: [2, 4]"
REFERENCES,0.7541589648798521,"0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Kernel Bandwid h 10−2"
REFERENCES,0.756007393715342,KL Divergence
REFERENCES,0.7578558225508318,"Process: [3, 4]"
REFERENCES,0.7597042513863216,"0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Kernel Bandwid h 10−2 10−1"
REFERENCES,0.7615526802218114,KL Divergence
REFERENCES,0.7634011090573013,"Process: [1, 2, 3]"
REFERENCES,0.7652495378927912,"0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Kernel Bandwid h 10−1"
REFERENCES,0.767097966728281,KL Divergence
REFERENCES,0.7689463955637708,"Process: [1, 2, 4]"
REFERENCES,0.7707948243992606,"0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Kernel Bandwid h 10−2 10−1"
REFERENCES,0.7726432532347505,KL Divergence
REFERENCES,0.7744916820702403,"Process: [1, 3, 4]"
REFERENCES,0.7763401109057301,"0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Kernel Bandwid h 10−1"
REFERENCES,0.7781885397412199,KL Divergence
REFERENCES,0.7800369685767098,"Process: [2, 3, 4]"
REFERENCES,0.7818853974121996,"Order: 1
Order: 2
Order: 3
Order: 4"
REFERENCES,0.7837338262476895,"0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Kernel Bandwid h 10−1 100"
REFERENCES,0.7855822550831792,KL Divergence
REFERENCES,0.7874306839186691,"Process: [1, 2, 3, 4]"
REFERENCES,0.789279112754159,"0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Kernel Bandwid h 10−1 100"
REFERENCES,0.7911275415896488,KL Divergence
REFERENCES,0.7929759704251387,To al KL Divergence
REFERENCES,0.7948243992606284,(b) Sparse observations.
REFERENCES,0.7966728280961183,Figure 8: KL Divergence for four-order Poisson process.
REFERENCES,0.7985212569316081,Under review as a conference paper at ICLR 2022
REFERENCES,0.800369685767098,"0
2
4
6
8
10
Time 0"
REFERENCES,0.8022181146025879,100000
REFERENCES,0.8040665434380776,Intensity
REFERENCES,0.8059149722735675,Process: [1]
REFERENCES,0.8077634011090573,"0
2
4
6
8
10
Time 0"
REFERENCES,0.8096118299445472,100000
REFERENCES,0.8114602587800369,Intensity
REFERENCES,0.8133086876155268,Process: [2]
REFERENCES,0.8151571164510166,"0
2
4
6
8
10
Time 0"
REFERENCES,0.8170055452865065,100000
REFERENCES,0.8188539741219963,Intensity
REFERENCES,0.8207024029574861,Process: [3]
REFERENCES,0.822550831792976,"0
2
4
6
8
10
Time 0 50000"
REFERENCES,0.8243992606284658,100000
REFERENCES,0.8262476894639557,Intensity
REFERENCES,0.8280961182994455,Process: [4]
REFERENCES,0.8299445471349353,"0
2
4
6
8
10
Time 0 2000"
REFERENCES,0.8317929759704251,Intensity
REFERENCES,0.833641404805915,"Process: [1, 2]"
REFERENCES,0.8354898336414048,"0
2
4
6
8
10
Time 0 2000 4000"
REFERENCES,0.8373382624768947,Intensity
REFERENCES,0.8391866913123844,"Process: [1, 3]"
REFERENCES,0.8410351201478743,"0
2
4
6
8
10
Time 0 1000 2000"
REFERENCES,0.8428835489833642,Intensity
REFERENCES,0.844731977818854,"Process: [1, 4]"
REFERENCES,0.8465804066543438,"0
2
4
6
8
10
Time 0 1000 2000"
REFERENCES,0.8484288354898336,Intensity
REFERENCES,0.8502772643253235,"Process: [2, 3]"
REFERENCES,0.8521256931608133,"0
2
4
6
8
10
Time 0 1000 2000"
REFERENCES,0.8539741219963032,Intensity
REFERENCES,0.8558225508317929,"Process: [2, 4]"
REFERENCES,0.8576709796672828,"0
2
4
6
8
10
Time 0 1000 2000"
REFERENCES,0.8595194085027726,Intensity
REFERENCES,0.8613678373382625,"Process: [3, 4]"
REFERENCES,0.8632162661737524,"0
2
4
6
8
10
Time 0 50 100"
REFERENCES,0.8650646950092421,Intensity
REFERENCES,0.866913123844732,"Process: [1, 2, 3]"
REFERENCES,0.8687615526802218,"0
2
4
6
8
10
Time 0 25 50"
REFERENCES,0.8706099815157117,Intensity
REFERENCES,0.8724584103512015,"Process: [1, 2, 4]"
REFERENCES,0.8743068391866913,"0
2
4
6
8
10
Time 0 50"
REFERENCES,0.8761552680221811,Intensity
REFERENCES,0.878003696857671,"Process: [1, 3, 4]"
REFERENCES,0.8798521256931608,"0
2
4
6
8
10
Time 0 50"
REFERENCES,0.8817005545286506,Intensity
REFERENCES,0.8835489833641405,"Process: [2, 3, 4]"
REFERENCES,0.8853974121996303,"0
2
4
6
8
10
Time 0 2 4"
REFERENCES,0.8872458410351202,Intensity
REFERENCES,0.88909426987061,"Process: [1, 2, 3, 4]"
REFERENCES,0.8909426987060998,Order: 1
REFERENCES,0.8927911275415896,Order: 2
REFERENCES,0.8946395563770795,Order: 3
REFERENCES,0.8964879852125693,Order: 4
REFERENCES,0.8983364140480592,Ground Truth
REFERENCES,0.9001848428835489,(a) Dense observations.
REFERENCES,0.9020332717190388,Under review as a conference paper at ICLR 2022
REFERENCES,0.9038817005545287,"0
2
4
6
8
10
Time 0 1000"
REFERENCES,0.9057301293900185,Intensity
REFERENCES,0.9075785582255084,Process: [1]
REFERENCES,0.9094269870609981,"0
2
4
6
8
10
Time 0 1000"
REFERENCES,0.911275415896488,Intensity
REFERENCES,0.9131238447319778,Process: [2]
REFERENCES,0.9149722735674677,"0
2
4
6
8
10
Time 0 1000"
REFERENCES,0.9168207024029574,Intensity
REFERENCES,0.9186691312384473,Process: [3]
REFERENCES,0.9205175600739371,"0
2
4
6
8
10
Time 0 500 1000"
REFERENCES,0.922365988909427,Intensity
REFERENCES,0.9242144177449169,Process: [4]
REFERENCES,0.9260628465804066,"0
2
4
6
8
10
Time 0 20"
REFERENCES,0.9279112754158965,Intensity
REFERENCES,0.9297597042513863,"Process: [1, 2]"
REFERENCES,0.9316081330868762,"0
2
4
6
8
10
Time 0 20 40"
REFERENCES,0.933456561922366,Intensity
REFERENCES,0.9353049907578558,"Process: [1, 3]"
REFERENCES,0.9371534195933456,"0
2
4
6
8
10
Time 0 10 20"
REFERENCES,0.9390018484288355,Intensity
REFERENCES,0.9408502772643254,"Process: [1, 4]"
REFERENCES,0.9426987060998152,"0
2
4
6
8
10
Time 0 10 20"
REFERENCES,0.944547134935305,Intensity
REFERENCES,0.9463955637707948,"Process: [2, 3]"
REFERENCES,0.9482439926062847,"0
2
4
6
8
10
Time 0 10 20"
REFERENCES,0.9500924214417745,Intensity
REFERENCES,0.9519408502772643,"Process: [2, 4]"
REFERENCES,0.9537892791127541,"0
2
4
6
8
10
Time 0 10 20"
REFERENCES,0.955637707948244,Intensity
REFERENCES,0.9574861367837338,"Process: [3, 4]"
REFERENCES,0.9593345656192237,"0
2
4
6
8
10
Time 0.0 0.5"
REFERENCES,0.9611829944547134,Intensity
REFERENCES,0.9630314232902033,"Process: [1, 2, 3]"
REFERENCES,0.9648798521256932,"0
2
4
6
8
10
Time 0.0 0.2 0.4"
REFERENCES,0.966728280961183,Intensity
REFERENCES,0.9685767097966729,"Process: [1, 2, 4]"
REFERENCES,0.9704251386321626,"0
2
4
6
8
10
Time 0.00 0.25 0.50"
REFERENCES,0.9722735674676525,Intensity
REFERENCES,0.9741219963031423,"Process: [1, 3, 4]"
REFERENCES,0.9759704251386322,"0
2
4
6
8
10
Time 0.0 0.5 1.0"
REFERENCES,0.977818853974122,Intensity
REFERENCES,0.9796672828096118,"Process: [2, 3, 4]"
REFERENCES,0.9815157116451017,"0
2
4
6
8
10
Time 0.00 0.05"
REFERENCES,0.9833641404805915,Intensity
REFERENCES,0.9852125693160814,"Process: [1, 2, 3, 4]"
REFERENCES,0.9870609981515711,Order: 1
REFERENCES,0.988909426987061,Order: 2
REFERENCES,0.9907578558225508,Order: 3
REFERENCES,0.9926062846580407,Order: 4
REFERENCES,0.9944547134935305,Ground Truth
REFERENCES,0.9963031423290203,(a) Sparse observations.
REFERENCES,0.9981515711645101,Figure 10: Intensity function of higher dimensional processes. Dots represent observations.
