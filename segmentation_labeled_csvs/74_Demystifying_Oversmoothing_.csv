Section,Section Appearance Order,Paragraph
ABSTRACT,0.0033783783783783786,Abstract
ABSTRACT,0.006756756756756757,"Oversmoothing in Graph Neural Networks (GNNs) refers to the phenomenon
where increasing network depth leads to homogeneous node representations.
While previous work has established that Graph Convolutional Networks (GCNs)
exponentially lose expressive power, it remains controversial whether the graph
attention mechanism can mitigate oversmoothing. In this work, we provide a
definitive answer to this question, by viewing attention-based GNNs as nonlinear
time-varying dynamical systems and incorporating tools and techniques from the
theory of products of inhomogeneous matrices and the joint spectral radius. We
establish that, contrary to popular belief, the graph attention mechanism cannot
prevent oversmoothing and loses expressive power exponentially. The proposed
framework extends the existing results on oversmoothing for symmetric GCNs
to a significantly broader class of GNN models, including random walk GCNs,
Graph Attention Networks (GATs) and (graph) transformers. In particular, our
analysis accounts for asymmetric, state-dependent and time-varying aggregation
operators and a wide range of common nonlinear activation functions, such as
ReLU, LeakyReLU, GELU and SiLU."
INTRODUCTION,0.010135135135135136,"1
Introduction"
INTRODUCTION,0.013513513513513514,"Graph neural networks (GNNs) have emerged as a powerful framework for learning with graph-
structured data [1–7]. Most GNN models follow the message-passing paradigm [8], where the
representation of each node is computed by recursively aggregating and transforming the representa-
tions of its neighboring nodes."
INTRODUCTION,0.016891891891891893,"One notable drawback of repeated message-passing is oversmoothing, which refers to the phenomenon
that stacking message-passing GNN layers makes node representations of the same connected
component converge to the same vector [6, 9–14]. As a result, most GNNs used in practice remain
relatively shallow and often only have few layers [6, 7, 15]. On the theory side, while previous works
have shown that the symmetric Graph Convolution Networks (GCNs) with ReLU and LeakyReLU
nonlinearities exponentially lose expressive power, analyzing the oversmoothing phenomenon in
other types of GNNs is still an open question [10, 11]. In particular, the question of whether the
graph attention mechanism can prevent oversmoothing has not been settled yet. Motivated by the
capacity of graph attention to distinguish the importance of different edges in the graph, some works
claim that oversmoothing is alleviated in Graph Attention Networks (GATs), heuristically crediting
to GATs’ ability to learn adaptive node-wise aggregation operators via the attention mechanism [16].
On the other hand, it has been empirically observed that similar to the case of GCNs, oversmoothing
seems inevitable for attention-based GNNs such as GATs or (graph) transformers [14, 17]."
INTRODUCTION,0.02027027027027027,"In this work, we provide a definitive answer to this question — attention-based GNNs also lose
expressive power exponentially, albeit potentially at a slower exponential rate compared to GCNs.
Given that attention-based GNNs can be viewed as nonlinear time-varying dynamical systems, our
analysis is built on the theory of products of inhomogeneous matrices [18, 19] and the concept
of joint spectral radius [20], as these methods have been long proved effective in the analysis of"
INTRODUCTION,0.02364864864864865,Demystifying Oversmoothing in Attention-Based Graph Neural Networks
INTRODUCTION,0.02702702702702703,"time-inhomogeneous markov chains and ergodicity of dynamical systems [18, 19, 21]. Our approach
generalizes the existing results on oversmoothing for symmetric GCNs to a significantly broader
class of GNN models with asymmetric, state-dependent and time-varying aggregation operators
and nonlinear activation functions under general conditions. In particular, our analysis accounts for
asymmetric, state-dependent and time-varying aggregation operators and a wide range of common
nonlinearities such as ReLU, LeakyReLU, and even non-monotone ones like GELU and SiLU."
IMPLEMENTATION/METHODS,0.030405405405405407,"2
Problem Setup"
IMPLEMENTATION/METHODS,0.033783783783783786,"2.1
Graph attention mechanism and attention-based GNNs"
IMPLEMENTATION/METHODS,0.037162162162162164,"Let G be a graph with N nodes. Given node representation vectors Xi, Xj ∈Rd, we use an attention
function Ψ : Rd′ × Rd′ →R to compute a raw attention coefficient eij = Ψ(W ⊤Xi, W ⊤Xj), W ∈
Rd×d′ that indicates the importance of node j’s features to node i. Then the graph structure is injected
into the mechanism by performing masked attention normalized using the softmax function, where for
each node i, we only compute its attention to its neighbors: Pij = softmaxj(eij) =
exp(eij)
P"
IMPLEMENTATION/METHODS,0.04054054054054054,k∈Ni exp(eik) .
IMPLEMENTATION/METHODS,0.04391891891891892,"The matrix P, where the ijth entry is Pij, is a row stochastic matrix. We refer to P as an aggregation
operator in message-passing."
IMPLEMENTATION/METHODS,0.0472972972972973,"The update rule of a single graph attentional layer can be written as X′ = σ(PXW) , where σ(·) is
a pointwise nonlinearity function, and the aggregation operator P is a function of XW. As a result,
the output of the tth graph attentional layers can be written as X(t+1) = σ(P (t)X(t)W (t)), where
X(0) = X ∈RN×d is the input node features, W (t) ∈Rd′×d′ for t ∈N and W (0) ∈Rd×d′. For the
rest of this work, without loss of generality, we assume that d = d′."
IMPLEMENTATION/METHODS,0.05067567567567568,"2.2
Measure of Oversmoothing"
IMPLEMENTATION/METHODS,0.05405405405405406,"We establish our results on oversmoothing for attention-based GNNs using the following node
similarity measure µ : RN×d →R≥0, satisfying the criteria proposed in Rusch et al. [14]: µ(X) :=
∥X −1γX∥F , where 1 ∈RN is the all-one vector, γX = 1⊤X"
IMPLEMENTATION/METHODS,0.057432432432432436,"N
. In particular, µ(X) = 0 if and only
if all node representations converge to the same vector. Then oversmoothing with respect to µ is
defined as the layer-wise exponential convergence of the node-similarity measure µ to zero, i.e. for
t ∈N, with constants C1, C2 > 0,
µ(X(t)) ≤C1e−C2t.
(1)
We note that our analysis directly applies to any Lipschitz node similarity measure, including the
popular Dirichlet energy [11, 14]."
IMPLEMENTATION/METHODS,0.060810810810810814,"2.3
Assumptions"
IMPLEMENTATION/METHODS,0.06418918918918919,"We make the following assumptions (in fact, quite minimal) in deriving our results:"
IMPLEMENTATION/METHODS,0.06756756756756757,"A1 The graph G is connected and has a self-loop at each node.
A2 The attention function Ψ(·, ·) is continuous.
A3 The sequence {∥Qk
t=0 |W (t)|∥max}∞
k=0 is bounded.
A4 The point-wise nonlinear activation function σ(·) satisfies 0 ≤σ(x)"
IMPLEMENTATION/METHODS,0.07094594594594594,"x
≤1 for x ̸= 0 and σ(0) = 0."
IMPLEMENTATION/METHODS,0.07432432432432433,"We note that all of these assumptions are either standard or quite general. The assumptions on the
GNN architecture A2 and A4 can be easily verified for commonly used GNN designs. For example,
the attention functions used in the GAT [7], GATv2 [22], and transformers [23] are specific cases
that all satisfy A2. As for A4, one way to satisfy it is to have σ be 1-Lipschitz and σ(x) ≤0 for
x < 0 and σ(x) ≥0 for x > 0. Then it is easy to verify that most of the commonly used nonlinear
activation functions such as ReLU, LeakyReLU, GELU, SiLU, ELU, tanh all satisfy A4."
RESULTS/EXPERIMENTS,0.0777027027027027,"3
Main Results"
RESULTS/EXPERIMENTS,0.08108108108108109,"3.1
Common connectivity structure among aggregation operators across different layers"
RESULTS/EXPERIMENTS,0.08445945945945946,Through writing the ith column of X(t+1) as
RESULTS/EXPERIMENTS,0.08783783783783784,"X(t+1)
·i
=
X"
RESULTS/EXPERIMENTS,0.09121621621621621,"jt+1=i, (jt,...,j0)∈[d]t+1 tY"
RESULTS/EXPERIMENTS,0.0945945945945946,"k=0
W (k)
jkjk+1 !"
RESULTS/EXPERIMENTS,0.09797297297297297,"D(t)
jt+1P (t)...D(0)
j1 P (0)X(0)
j0 ,
(2)"
RESULTS/EXPERIMENTS,0.10135135135135136,Demystifying Oversmoothing in Attention-Based Graph Neural Networks
RESULTS/EXPERIMENTS,0.10472972972972973,"where D(t)
i
is a diagonal matrix representing the effect of σ(·) to the ith column of P (t)X(t)W (t), we
can show the boundedness of the node representations’ trajectories X(t) for all t ≥0. We define D
to be the set of all possible diagonal matrices D(t)
i
satisfying A4: D := {diag(d) : d ∈RN, 0 ≤ew
d ≤ew 1} . Then we establish the following key lemma suggesting that the graph attention mechanism
cannot fundamentally change the connectivity pattern of the graph.
Lemma 1. Under A2-A4, there exists ϵ > 0 such that for all t ≥0 and for any nodes i, j that are
connected, we have P (t)
ij ≥ϵ."
RESULTS/EXPERIMENTS,0.10810810810810811,"We define the family of row-stochastic matrices satisfying Lemma 1 below.
Definition 1. Let ϵ > 0. We define PG,ϵ to be the set of row-stochastic matrices satisfying the
following conditions: 1. ϵ ≤Pij ≤1, for i, j connected; 2. Pij = 0, for i, j not connected."
RESULTS/EXPERIMENTS,0.11148648648648649,"3.2
Ergodicity of infinite products of matrices"
RESULTS/EXPERIMENTS,0.11486486486486487,"Ergodicity, in its most general form, deals with the long-term behavior of dynamical systems. The
oversmoothing phenomenon in GNNs defined in the sense of (1) concerns the convergence of all
rows of X(t) to a common vector at an exponential rate. To this end, we define ergodicity in our
analysis as the convergence of infinite matrix products to a rank-one matrix with identical rows.
Definition 2 (Ergodicity). Let B ∈R(N−1)×N be the orthogonal projection onto the space or-
thogonal to span{1}. A sequence of matrices {M (n)}∞
n=0 is ergodic if lim
t→∞B Qt
n=0 M (n) = 0 ."
RESULTS/EXPERIMENTS,0.11824324324324324,"We will take advantage of the following properties of the projection matrix B already established in
Blondel et al. [21]:1. B1 = 0; 2. ∥Bx∥2 = ∥x∥2 for x ∈RN if x⊤1 = 0; 3. For M ∈RN×N, there
exists a unique matrix ˜
M ∈R(N−1)×(N−1) such that BM = ˜
MB ."
RESULTS/EXPERIMENTS,0.12162162162162163,"Let MG,ϵ := {DP : D ∈D, P ∈PG,ϵ} . Then any infinite product of matrices in MG,ϵ is ergodic.
Lemma 2. Any sequence {D(t)P (t)}∞
t=0 in MG,ϵ is ergodic."
RESULTS/EXPERIMENTS,0.125,"3.3
Joint spectral radius"
RESULTS/EXPERIMENTS,0.12837837837837837,"Finally, we make use of the concept of the joint spectral radius for a set of matrices [20] and employ
it to deduce exponential convergence of node representations from our ergodicity result, Lemma 2.
Definition 3 (Joint Spectral Radius). For a collection of matrices M, the joint spectral radius
JSR(M) is defined to be"
RESULTS/EXPERIMENTS,0.13175675675675674,"JSR(M) = lim sup
k→∞
sup
M1,M2,...,Mk∈M
∥M1M2...Mk∥
1
k ,"
RESULTS/EXPERIMENTS,0.13513513513513514,and it is independent of the norm used.
RESULTS/EXPERIMENTS,0.13851351351351351,"To analyze the convergence rate of products of matrices in MG,ϵ to a rank-one matrix with identical
rows, we investigate the dynamics induced by the matrices on the subspace orthogonal to span{1}.
More precisely, with the notion of ergodicity in Definition 2 and the goal of studying the convergence
rate of a matrix product BM1M2 . . . Mk where each Mi ∈MG,ϵ, we use the third property of the
orthogonal projection B to write"
RESULTS/EXPERIMENTS,0.14189189189189189,"BM1M2 . . . Mk = ˜
M1 ˜
M2... ˜
MkB ,"
RESULTS/EXPERIMENTS,0.14527027027027026,"where each ˜
Mi is the unique matrix in R(N−1)×(N−1) that satisfies BMi =
˜
MiB. To analyze
products of such matrices ˜
Mi, let us define ˜
MG,ϵ := { ˜
M : BM = ˜
MB, M ∈MG,ϵ}. We can use
the ergodictiy result developed in Lemma 2 to show that the joint spectral radius of ˜
MG,ϵ is strictly
less than 1.
Lemma 3. Let 0 < ϵ < 1. Under assumptions A1-A4, JSR( ˜
MG,ϵ) < 1."
RESULTS/EXPERIMENTS,0.14864864864864866,"It follows from the definition of the joint spectral radius that if JSR( ˜
MG,ϵ) < 1, for any
JSR( ˜
MG,ϵ) < q < 1, there exists a C for which"
RESULTS/EXPERIMENTS,0.15202702702702703,"∥˜
M1 ˜
M2... ˜
Mky∥≤Cqk∥y∥
(3)"
RESULTS/EXPERIMENTS,0.1554054054054054,"for all y ∈RN−1 and ˜
M1, ˜
M2, ..., ˜
Mk ∈˜
MG,ϵ."
RESULTS/EXPERIMENTS,0.15878378378378377,Demystifying Oversmoothing in Attention-Based Graph Neural Networks
RESULTS/EXPERIMENTS,0.16216216216216217,"3.4
Main Theorem"
RESULTS/EXPERIMENTS,0.16554054054054054,"Applying (3) to the recursive expansion of X(t+1)
·i
in (2) using the 2-norm, we can prove the
exponential convergence of µ(X(t)) to zero for the similarity measure µ(·) defined in (1), which in
turn implies the convergence of node representations to a common representation at an exponential
rate. This completes the proof of the main result of this paper, which states that oversmoothing
defined in (1) is unavoidable for attention-based GNNs.
Theorem 1 (Oversmoothing happens exponentially in attention-based GNNs). Under assump-
tions A1-A4, JSR( ˜
MG,ϵ) < 1 and for any q satisfying JSR( ˜
MG,ϵ) < q < 1, there exists C1(q) > 0
such that
µ(X(t)) ≤C1qt , ∀t ≥0 ,"
RESULTS/EXPERIMENTS,0.16891891891891891,where µ(X) = ∥X −11⊤X
RESULTS/EXPERIMENTS,0.17229729729729729,"N
∥F . As a result, node representations X(t) exponentially converge to the
same value as the model depth t →∞."
RESULTS/EXPERIMENTS,0.17567567567567569,"3.5
Comparison with the GCN"
RESULTS/EXPERIMENTS,0.17905405405405406,"Computing or approximating the joint spectral radius for a given set of matrices is known to be hard
in general [24], yet it is straightforward to lower bound JSR( ˜
MG,ϵ)."
RESULTS/EXPERIMENTS,0.18243243243243243,"Proposition 1. Let λ be the second largest eigenvalue of D−1/2
deg AD−1/2
deg . Then under assumptions
A1-A4, it holds that λ ≤JSR( ˜
MG,ϵ).
A direct consequence of the above result is that the upper bound q on the convergence rate that we get
for graph attention in Theorem 1 is at least as large as λ. On the other hand, previous work has already
established that in the graph convolution case, the convergence rate of µ(X(t)) is O(λt) [10, 11]. It
is thus natural to expect attention-based GNNs to potentially have better expressive power at finite
depth than GCNs, even though they both inevitably suffer from oversmoothing. This is also evident
from the numerical experiments that we present in the next section."
RESULTS/EXPERIMENTS,0.1858108108108108,"4
Numerical Experiments"
RESULTS/EXPERIMENTS,0.1891891891891892,"We validate our theoretical results on three real-world datasets: Cora, CiteSeer and PubMed [25]
with two attention-based GNN architectures (GAT [7] and random walk GCN (constant attention
function) [12, 13]) and five common nonlinearities. We ran each experiment 10 times. Figure 1
shows the evolution of µ(X(t)) in log-log scale on the largest connected component of each graph as
we forward pass the input X into a trained model. The solid curve is the average over 10 runs and the
band indicates one standard deviation around the average. We observe that oversmoothing happens
exponentially in both GCNs and GATs with the rates varying depending on the choice of activation
function. Notably, GCNs demonstrate faster rates of oversmoothing compared to GATs."
RESULTS/EXPERIMENTS,0.19256756756756757,"Figure 1: Evolution of µ(X(t)) (in log-log scale) on the largest connected component of three
benchmark datasets: Cora, Citeseer, and PubMed."
RESULTS/EXPERIMENTS,0.19594594594594594,Demystifying Oversmoothing in Attention-Based Graph Neural Networks
RESULTS/EXPERIMENTS,0.19932432432432431,Acknowledgements
RESULTS/EXPERIMENTS,0.20270270270270271,"This research has been supported in part by ARO MURI W911NF-19-0217, ONR N00014-20-1-2394,
and the MIT-IBM Watson AI Lab."
REFERENCES,0.20608108108108109,References
REFERENCES,0.20945945945945946,"[1] M. Gori, G. Monfardini, and F. Scarselli. A new model for learning in graph domains. In
IJCNN, 2005. 1
[2] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.
The graph neural network model. IEEE Transactions on Neural Networks, 20:61–80, 2009.
[3] Joan Bruna, Wojciech Zaremba, Arthur D. Szlam, and Yann LeCun. Spectral networks and
locally connected networks on graphs. In ICLR, 2014.
[4] David Kristjanson Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, Rafael Gómez-
Bombarelli, Timothy D. Hirzel, Alán Aspuru-Guzik, and Ryan P. Adams. Convolutional
networks on graphs for learning molecular fingerprints. In NeurIPS, 2015.
[5] Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks
on graphs with fast localized spectral filtering. In NeurIPS, 2016.
[6] Thomas Kipf and Max Welling. Semi-supervised classification with graph convolutional
networks. In ICLR, 2017. 1
[7] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua
Bengio. Graph attention networks. In ICLR, 2018. 1, 2, 4, 13
[8] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl.
Neural message passing for quantum chemistry. In ICML, 2017. 1
[9] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks
for semi-supervised learning. In AAAI, 2018. 1
[10] Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for
node classification. In ICLR, 2020. 1, 4
[11] Chen Cai and Yusu Wang. A note on over-smoothing for graph neural networks. In ICML
Graph Representation Learning and Beyond (GRL+) Workshop, 2020. 1, 2, 4
[12] Nicolas Keriven. Not too little, not too much: a theoretical analysis of graph (over)smoothing.
In NeurIPS, 2022. 4
[13] Xinyi Wu, Zhengdao Chen, William Wang, and Ali Jadbabaie. A non-asymptotic analysis of
oversmoothing in graph neural networks. In ICLR, 2023. 4
[14] T.Konstantin Rusch, Michael M. Bronstein, and Siddhartha Mishra. A survey on oversmoothing
in graph neural networks. ArXiv, abs/2303.10993, 2023. 1, 2
[15] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S. Yu. A
comprehensive survey on graph neural networks. IEEE Transactions on Neural Networks and
Learning Systems, 32:4–24, 2019. 1
[16] Yimeng Min, Frederik Wenkel, and Guy Wolf. Scattering gcn: Overcoming oversmoothness in
graph convolutional networks. In NeurIPS, 2020. 1
[17] Han Shi, Jiahui Gao, Hang Xu, Xiaodan Liang, Zhenguo Li, Lingpeng Kong, Stephen M. S.
Lee, and James Tin-Yau Kwok. Revisiting over-smoothing in bert from the perspective of graph.
In ICLR, 2022. 1
[18] Darald J. Hartfiel. Nonhomogeneous Matrix Products. 2002. 1, 2, 8
[19] Eugene Seneta. Non-negative Matrices and Markov Chains. 2008. 1, 2, 8
[20] Gian-Carlo Rota and W. Gilbert Strang. A note on the joint spectral radius. 1960. 1, 3
[21] Vincent D. Blondel, Julien M. Hendrickx, Alexander Olshevsky, and John N. Tsitsiklis. Con-
vergence in multiagent coordination, consensus, and flocking. Proceedings of the 44th IEEE
Conference on Decision and Control, pages 2996–3000, 2005. 2, 3
[22] Shaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks? In ICLR,
2022. 2"
REFERENCES,0.21283783783783783,Demystifying Oversmoothing in Attention-Based Graph Neural Networks
REFERENCES,0.21621621621621623,"[23] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.
Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 2"
REFERENCES,0.2195945945945946,"[24] John N. Tsitsiklis and Vincent D. Blondel. The Lyapunov exponent and joint spectral radius of
pairs of matrices are hard—when not impossible—to compute and to approximate. Mathematics
of Control, Signals and Systems, 10:31–40, 1997. 4"
REFERENCES,0.22297297297297297,"[25] Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning
with graph embeddings. In ICML, 2016. 4"
REFERENCES,0.22635135135135134,"[26] David A. Levin, Yuval Peres, and Elizabeth L. Wilmer. Markov Chains and Mixing Times. 2008. 8"
REFERENCES,0.22972972972972974,"[27] Jacques Theys. Joint spectral radius: theory and approximations. Ph. D. dissertation, 2005. 11"
REFERENCES,0.23310810810810811,[28] Peter D. Lax. Functional Analysis. 2002. 13
REFERENCES,0.23648648648648649,"[29] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style,
high-performance deep learning library. In NeurIPS, 2019. 13"
REFERENCES,0.23986486486486486,"[30] Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric.
In ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019. 13"
OTHER,0.24324324324324326,"A
Basic Facts about Matrix Norms"
OTHER,0.24662162162162163,"In this section, we list some basic facts about matrix norms that will be helpful in comprehending the
subsequent proofs."
OTHER,0.25,"A.1
Matrix norms induced by vector norms"
OTHER,0.2533783783783784,"Suppose a vector norm ∥·∥α on Rn and a vector norm ∥·∥β on Rm are given. Any matrix M ∈Rm×n
induces a linear operator from Rn to Rm with respect to the standard basis, and one defines the
corresponding induced norm or operator norm by"
OTHER,0.25675675675675674,"∥M∥α,β = sup
∥Mv∥β"
OTHER,0.26013513513513514,"∥v∥α
, v ∈Rn, v ̸= 0

."
OTHER,0.2635135135135135,"If the p-norm for vectors (1 ≤p ≤∞) is used for both spaces Rn and Rm, then the corresponding
operator norm is"
OTHER,0.2668918918918919,"∥M∥p = sup
v̸=0 ∥Mv∥p"
OTHER,0.2702702702702703,"∥v∥p
."
OTHER,0.27364864864864863,The matrix 1-norm and ∞-norm can be computed by
OTHER,0.27702702702702703,"∥M∥1 = max
1≤j m
X"
OTHER,0.28040540540540543,"i=1
|Mij| ,"
OTHER,0.28378378378378377,"that is, the maximum absolute column sum of the matrix M;"
OTHER,0.28716216216216217,"∥M∥∞= max
1≤m n
X"
OTHER,0.2905405405405405,"j=1
|Mij| ,"
OTHER,0.2939189189189189,"that is, the maximum absolute row sum of the matrix M."
OTHER,0.2972972972972973,"Remark.
In the special case of p = 2, the induced matrix norm ∥· ∥2 is called the spectral norm,
and is equal to the largest singular value of the matrix."
OTHER,0.30067567567567566,"For square matrices, we note that the name “spectral norm"" does not imply the quantity is directly
related to the spectrum of a matrix, unless the matrix is symmetric."
OTHER,0.30405405405405406,Demystifying Oversmoothing in Attention-Based Graph Neural Networks
OTHER,0.30743243243243246,"Example.
We give the following example of a stochastic matrix P, whose spectral radius is 1, but
its spectral norm is greater than 1."
OTHER,0.3108108108108108,"P =

0.9
0.1
0.25
0.75"
OTHER,0.3141891891891892,"
∥P∥2 ≈1.0188"
OTHER,0.31756756756756754,"A.2
Matrix (p, q)-norms"
OTHER,0.32094594594594594,The Frobenius norm of a matrix M ∈Rm×n is defined as
OTHER,0.32432432432432434,∥M∥F =
OTHER,0.3277027027027027,"v
u
u
t n
X j=1 m
X"
OTHER,0.3310810810810811,"i=1
|Mij|2 ,"
OTHER,0.3344594594594595,"and it belongs to a family of entry-wise matrix norms: for 1 ≤p, q ≤∞, the matrix (p, q)-norm is
defined as"
OTHER,0.33783783783783783,"∥M∥p,q =  
n
X j=1 m
X"
OTHER,0.34121621621621623,"i=1
|Mij|p
!q/p  1/q ."
OTHER,0.34459459459459457,"The special case p = q = 2 is the Frobenius norm ∥· ∥F , and p = q = ∞yields the max norm
∥· ∥max."
OTHER,0.34797297297297297,"A.3
Equivalence of norms"
OTHER,0.35135135135135137,"For any two matrix norms ∥· ∥α and ∥· ∥β, we have that for all matrices M ∈Rm×n,
r∥M∥α ≤∥M∥β ≤s∥M∥α
for some positive numbers r and s. In particular, the following inequality holds for the 2-norm ∥· ∥2
and the ∞-norm ∥· ∥∞:
1
√n∥M∥∞≤∥M∥2 ≤√m∥M∥∞."
OTHER,0.3547297297297297,"B
Proof of Lemma 1"
OTHER,0.3581081081081081,"We can use the formulation in (2) to show the boundedness of the node representations’ trajectories
X(t) for all t ∈N≥0, which in turn implies the boundedness of the input to graph attention in each
layer, X(t)W (t).
Lemma 4. Under assumptions A3-A4, there exists C > 0 such that ∥X(t)∥max ≤C for all t ∈N≥0."
OTHER,0.3614864864864865,Proof of Lemma 4. According to the formulation (2):
OTHER,0.36486486486486486,"X(t+1)
·i
=
X"
OTHER,0.36824324324324326,"jt+1=i, (jt,...,j0)∈[d]t+1 tY"
OTHER,0.3716216216216216,"k=0
W (k)
jkjk+1 !"
OTHER,0.375,"D(t)
jt+1P (t)...D(0)
j1 P (0)X(0)
j0 ,"
OTHER,0.3783783783783784,we thus obtain that
OTHER,0.38175675675675674,"∥X(t+1)
·i
∥∞=  X"
OTHER,0.38513513513513514,"jt+1=i ,(jt,...,j0)∈[d]t+1 tY"
OTHER,0.3885135135135135,"k=0
W (k)
jkjk+1 !"
OTHER,0.3918918918918919,"D(t)
jt+1P (t)...D(0)
j1 P (0)X(0)
j0 ∞ ≤
X"
OTHER,0.3952702702702703,"jt+1=i ,(jt,...,j0)∈[d]t+1 tY k=0"
OTHER,0.39864864864864863,"W (k)
jkjk+1 "
OTHER,0.40202702702702703,"! 


D(t)
jt+1P (t)...D(0)
j1 P (0)


∞"
OTHER,0.40540540540540543,"X(0)
j0 ∞ ≤
X"
OTHER,0.40878378378378377,"jt+1=i ,(jt,...,j0)∈[d]t+1 tY k=0"
OTHER,0.41216216216216217,"W (k)
jkjk+1 "
OTHER,0.4155405405405405,"! 


X(0)
j0 ∞ ≤C0  
X"
OTHER,0.4189189189189189,"jt+1=i ,(jt,...,j0)∈[d]t+1 tY k=0"
OTHER,0.4222972972972973,"W (k)
jkjk+1  ! "
OTHER,0.42567567567567566,"= C0∥(|W (0)|...|W (t)|)·i∥1 ,"
OTHER,0.42905405405405406,Demystifying Oversmoothing in Attention-Based Graph Neural Networks
OTHER,0.43243243243243246,where C0 equals the maximal entry in |X(0)|.
OTHER,0.4358108108108108,"The assumption A3 implies that there exists C′ > 0 such that for all t ∈N≥0 and i ∈[d],"
OTHER,0.4391891891891892,∥(|W (0)|...|W (t)|)·i∥1 ≤C′N .
OTHER,0.44256756756756754,"Hence there exists C′′ > 0 such that for all t ∈N≥0 and i ∈[d], we have"
OTHER,0.44594594594594594,"∥X(t)
·i ∥∞≤C′′ ,"
OTHER,0.44932432432432434,proving the existence of C > 0 such that ∥X(t)∥max ≤C for all t ∈N≥0.
OTHER,0.4527027027027027,"For a continuous Ψ(·, ·)1, the following lemma is a direct consequence of Lemma 4, suggesting that
the graph attention mechanism cannot fundamentally change the connectivity pattern of the graph."
OTHER,0.4560810810810811,"C
Proof of Lemma 2"
OTHER,0.4594594594594595,"C.1
Ergodicity of infinite products of matrices in PG,ϵ"
OTHER,0.46283783783783783,"In this section, we make use the existing results on the ergodicity of infinite products of inhomoge-
neous stochastic matrices [18, 19] to show that any sequence of matrices in PG,ϵ is ergodic.
Lemma 5. Fix ϵ > 0. Consider a sequence of matrices {P (t)}∞
t=0 in PG,ϵ. That is, P (t) ∈PG,ϵ for
all t ∈N≥0. Then {P (t)}∞
t=0 is ergodic."
OTHER,0.46621621621621623,"Proof of Lemma 5. The following sufficient condition guarantees the ergodicity of the infinite prod-
ucts of row-stochastic matrices."
OTHER,0.46959459459459457,"Lemma 6 (Corollary 5.1 [18]). Consider a sequence of row-stochastic matrices {S(t)}∞
t=0. Let at
and bt be the smallest and largest entries in S(t), respectively. If P∞
t=0
at
bt = ∞, then {S(t)}∞
t=0 is
ergodic."
OTHER,0.47297297297297297,"In order to make use of the above result, we first show that long products of P (t)’s from PG,ϵ will
eventually become strictly positive. For t0 ≤t1, we denote"
OTHER,0.47635135135135137,P (t1:t0) = P (t1) . . . P (t0) .
OTHER,0.4797297297297297,"Lemma 7. Under the assumption A1, there exist T ∈N and c > 0 such that for all t0 ≥0,"
OTHER,0.4831081081081081,"c ≤P (t0+T :t0)
ij
≤1 , ∀1 ≤i, j ≤N ."
OTHER,0.4864864864864865,"Proof of Lemma 7. Fix any T ∈N≥0. Since ∥P (t)∥∞≤1 for any P (t) ∈PG,ϵ, it follows that
∥P (t0+T :t0)∥∞≤1 and hence P (t0+T :t0)
ij
≤1, for all 1 ≤i, j ≤N."
OTHER,0.48986486486486486,"To show the lower bound, without loss of generality, we will show that there exist T ∈N and c > 0
such that
P (T :0)
ij
≥c , ∀1 ≤i, j ≤N ."
OTHER,0.49324324324324326,"Since each P (t) has the same connectivity pattern as the original graph G, it follows from the
assumption A1 that there exists T ∈N such that P (T :0) is a positive matrix, following a similar
argument as the one for Proposition 1.7 in [26]: For each pair of nodes i, j, since we assume that the
graph G is connected, there exists r(i, j) such that P (r(i,j):0)
ij
> 0. on the other hand, since we also"
OTHER,0.4966216216216216,"assume each node has a self-loop, P (t:0)
ii
> 0 for all t ≥0 and hence for t ≥r(i, j),"
OTHER,0.5,"P (t:0)
ij
≥P (t−r(i,j))
ii
P (r(i,j):0)
ij
> 0 ."
OTHER,0.5033783783783784,"1More generally, for Ψ(·, ·) that outputs bounded attention scores for bounded inputs."
OTHER,0.5067567567567568,Demystifying Oversmoothing in Attention-Based Graph Neural Networks
OTHER,0.5101351351351351,"For t ≥t(i) := max
j∈G r(i, j), we have P (t:0)
ij
> 0 for all node j in G. Finally, if t ≥T := max
i∈G t(i),"
OTHER,0.5135135135135135,"then P (t:0)
ij
> 0 for all pairs of nodes i, j in G. Notice that P (T :0)
ij
is a weighted sum of walks of"
OTHER,0.5168918918918919,"length T between nodes i and j, and hence P (T :0)
ij
> 0 if and only if there exists a walk of length"
OTHER,0.5202702702702703,"T between nodes i and j. Since for all t ∈N≥0, P (t)
ij
≥ϵ if (i, j) ∈E(G), we conclude that"
OTHER,0.5236486486486487,"P (T :0)
ij
≥ϵT := c."
OTHER,0.527027027027027,"Given the sequence {P (t)}∞
t=0, we use T ∈N from Lemma 7 and define"
OTHER,0.5304054054054054,¯P (k) := P ((k+1)T :kT ) .
OTHER,0.5337837837837838,"Then {P (t)}∞
t=0 is ergodic if and only if { ¯P (k)}∞
k=0 is ergodic. Notice that by Lemma 7, for all
k ∈N≥0, there exists c > 0 such that c ≤¯P (k)
ij
≤1 , ∀1 ≤i, j ≤N. Then Lemma 5 is a direct
consequence of Lemma 6."
OTHER,0.5371621621621622,"C.2
Notations and auxiliary results"
OTHER,0.5405405405405406,"Consider a sequence {D(t)P (t)}∞
t=0 in MG,ϵ. For t0 ≤t1, define"
OTHER,0.543918918918919,"Qt0,t1 := D(t1)P (t1)...D(t0)P (t0)"
OTHER,0.5472972972972973,"and
δt = ∥D(t) −IN∥∞,
where IN denotes the N × N identity matrix. It is also useful to define"
OTHER,0.5506756756756757,"ˆQt0,t1 :=P (t1)Qt0,t1−1
:=P (t1)D(t1−1)P (t1−1)...D(t0)P (t0)."
OTHER,0.5540540540540541,"We start by proving the following key lemma, which states that long products of matrices in MG,ϵ
eventually become a contraction in ∞-norm.
Lemma 8. There exist 0 < c < 1 and T ∈N such that for all t0 ≤t1,"
OTHER,0.5574324324324325,"∥ˆQt0,t1+T ∥∞≤(1 −cδt1)∥ˆQt0,t1∥∞."
OTHER,0.5608108108108109,"Proof of Lemma 8. First observe that for every T ≥0,"
OTHER,0.5641891891891891,"∥ˆQt0,t1+T ∥∞≤∥P (t1+T )D(t1+T −1)P (t1+T −1)...D(t1+1)P (t1+1)D(t1)∥∞∥ˆQt0,t1∥∞"
OTHER,0.5675675675675675,"≤∥P (t1+T )P (t1+T −1)...P (t1+1)D(t1)∥∞∥ˆQt0,t1∥∞,"
OTHER,0.5709459459459459,where the second inequality is based on the following element-wise inequality:
OTHER,0.5743243243243243,P (t1+T )P (t1+T −1)...D(t1+1)P (t1+1) ≤ew P (t1+T )P (t1+T −1)...P (t1+1) .
OTHER,0.5777027027027027,"By Lemma 7, there exist T ∈N and 0 < c < 1 such that"
OTHER,0.581081081081081,"(P (t1+T )...P (t1+1))ij ≥c, ∀1 ≤i, j ≤N ."
OTHER,0.5844594594594594,"Since the matrix product P (t1+T )P (t1+T −1)...P (t1+1) is row-stochastic, multiplying it with the
diagonal matrix D(t1) from right decreases the row sums by at least c(1 −D(t1)
min) = cδt1, where
D(t1)
min here denotes the smallest diagonal entry of the diagonal matrix D(t1). Hence,"
OTHER,0.5878378378378378,∥P (t1+T )P (t1+T −1)...P (t1+1)D(t1)∥∞≤1 −cδt1 .
OTHER,0.5912162162162162,"Now define βk := Qk
t=0(1 −cδt) and let β := lim
k→∞βk. Note that β is well-defined because the"
OTHER,0.5945945945945946,"partial product is non-increasing and bounded from below. Then we present the following result,
which is stated as Lemma 9 in the main paper and from which the ergodicity of any sequence in
MG,ϵ is an immediate result."
OTHER,0.597972972972973,Demystifying Oversmoothing in Attention-Based Graph Neural Networks
OTHER,0.6013513513513513,"Lemma 9. Let βk := Qk
t=0(1 −cδt) and β := lim
k→∞βk."
OTHER,0.6047297297297297,"1. If β = 0, then lim
k→∞Q0,k = 0 ;"
OTHER,0.6081081081081081,"2. If β > 0, then lim
k→∞BQ0,k = 0 ."
OTHER,0.6114864864864865,Proof of Lemma 9. We will prove the two cases separately.
OTHER,0.6148648648648649,"[Case β = 0].
We will show that β = 0 implies
lim
k→∞∥ˆQ0,k∥∞= 0, and as a result,"
OTHER,0.6182432432432432,"lim
k→∞∥Q0,k∥∞= 0. For 0 ≤j ≤T −1, let us define βj := ∞
Y"
OTHER,0.6216216216216216,"k=0
(1 −δj+kT ) ."
OTHER,0.625,"Then by Lemma 8, we get that"
OTHER,0.6283783783783784,"lim
k→∞∥ˆQ0,kT ∥∞≤βj∥ˆQ0,j∥∞."
OTHER,0.6317567567567568,"By construction, β = ΠT −1
j=0 βj. Hence, if β = 0 then βj0 = 0 for some 0 ≤j0 ≤T −1, which
yields lim
k→∞∥ˆQ0,k∥∞= 0. Consequently, lim
k→∞∥Q0,k∥∞= 0 implies that lim
k→∞Q0,k = 0."
OTHER,0.6351351351351351,"[Case β > 0].
First observe that if β > 0, then ∀0 < η < 1, there exist m ∈N≥0 such that ∞
Y"
OTHER,0.6385135135135135,"t=m
(1 −cδt) > 1 −η .
(4)"
OTHER,0.6418918918918919,"Using 1 −x ≤e−x for all x ∈R, we deduce ∞
Y"
OTHER,0.6452702702702703,"t=m
e−cδt > 1 −η ."
OTHER,0.6486486486486487,"It also follows from (4) that 1 −cδt > 1 −η, or equivalently δt < η"
OTHER,0.652027027027027,c for t ≥m. Choosing η < c
OTHER,0.6554054054054054,"2
thus ensures that δt < 1"
OTHER,0.6587837837837838,"2 for t ≥m. Putting this together with the fact that, there exists2 b > 0 such
that 1 −x ≥e−bx for all x ∈[0, 1"
OTHER,0.6621621621621622,"2], we obtain ∞
Y"
OTHER,0.6655405405405406,"t=m
(1 −δt) ≥ ∞
Y"
OTHER,0.668918918918919,"t=m
e−bδt > (1 −η)
b
c := 1 −η′ .
(5)"
OTHER,0.6722972972972973,"Define the product of row-stochastic matrices P (M:m) := P (M) . . . P (m). It is easy to verify the
following element-wise inequality:
 M
Y"
OTHER,0.6756756756756757,"t=m
(1 −cδt) !"
OTHER,0.6790540540540541,"P (M:m) ≤ew Qm,M ≤ew P (M:m) ,"
OTHER,0.6824324324324325,which together with (5) leads to
OTHER,0.6858108108108109,"(1 −η′)P (M:m) ≤ew Qm,M ≤ew P (M:m) .
(6)"
OTHER,0.6891891891891891,"Therefore,"
OTHER,0.6925675675675675,"∥BQm,M∥∞= ∥B(Qm,M −P (M:m)) + BP (M:m)∥∞"
OTHER,0.6959459459459459,"≤∥B(Qm,M −P (M:m))∥∞+ ∥BP (M:m)∥∞"
OTHER,0.6993243243243243,"= ∥B(Qm,M −P (M:m))∥∞"
OTHER,0.7027027027027027,"≤∥B∥∞∥Qm,M −P (M:m)∥∞
≤η′∥B∥∞ ≤η′√ N ,"
OTHER,0.706081081081081,"2Choose, e.g., b = 2 log 2."
OTHER,0.7094594594594594,Demystifying Oversmoothing in Attention-Based Graph Neural Networks
OTHER,0.7128378378378378,"where the last inequality is due to the fact that ∥B∥2 = 1. By definition, Q0,M = Qm,MQ0,m−1,
and hence"
OTHER,0.7162162162162162,"∥BQ0,M∥∞≤∥BQm,M∥∞∥Q0,m−1∥∞≤∥BQm,M∥∞≤η′√"
OTHER,0.7195945945945946,"N .
(7)
The above inequality (7) holds when taking M →∞. Then taking η →0 implies η′ →0 and
together with (7), we conclude that
lim
M→∞∥BQ0,M∥∞= 0 ,"
OTHER,0.722972972972973,"and therefore,
lim
M→∞BQ0,M = 0 ."
OTHER,0.7263513513513513,"C.3
Proof of Lemma 2"
OTHER,0.7297297297297297,"Notice that both cases β = 0 and β > 0 in Lemma 9 imply the ergodicity of {D(t)P (t)}∞
t=0. Hence
the statement is a direct corollary of Lemma 9."
OTHER,0.7331081081081081,"D
Proof of Lemma 3"
OTHER,0.7364864864864865,"In order to show that JSR( ˜
MG,ϵ) < 1, we start by making the following observation.
Lemma 10. A sequence {M (n)}∞
n=0 is ergodic if and only if Qt
n=0 ˜
M (n) converges to the zero
matrix."
OTHER,0.7398648648648649,"Proof of Lemma 10. For any t ∈N≥0, it follows from the third property of the orthogonal projection
B (see, Page 6 of the main paper) that B tY"
OTHER,0.7432432432432432,"n=0
M (n) = tY"
OTHER,0.7466216216216216,"n=0
˜
M (n)B . Hence"
OTHER,0.75,"{M (n)}∞
n=0 is ergodic ⇐⇒lim
t→∞B tY"
OTHER,0.7533783783783784,"n=0
M (n) = 0"
OTHER,0.7567567567567568,"⇐⇒lim
t→∞ tY"
OTHER,0.7601351351351351,"n=0
˜
M (n)B = 0"
OTHER,0.7635135135135135,"⇐⇒lim
t→∞ tY"
OTHER,0.7668918918918919,"n=0
˜
M (n) = 0 ."
OTHER,0.7702702702702703,"Next, we utilize the following result, as a means to ensure a joint spectral radius strictly less than 1
for a bounded set of matrices.
Lemma 11 (Proposition 3.2 in [27]). For any bounded set of matrices M, JSR(M) < 1 if and only
if for any sequence {M (n)}∞
n=0 in M, Qt
n=0 M (n) converges to the zero matrix."
OTHER,0.7736486486486487,"Here, “bounded"" means that there exists an upper bound on the norms of the matrices in the set. Note
that MG,ϵ is bounded because ∥DP∥∞≤1, DP ∈MG,ϵ. To show that ˜
MG,ϵ is also bounded, let
˜
M ∈˜
MG,ϵ, then by definition, we have
˜
MB = BM, M ∈MG,ϵ ⇒˜
M = BMBT ,"
OTHER,0.777027027027027,"since BBT = IN−1. As a result,"
OTHER,0.7804054054054054,"∥˜
M∥2 = ∥BMBT ∥2 ≤∥M∥2 ≤
√"
OTHER,0.7837837837837838,"N ,
where the first inequality is due to ∥B∥2 = ∥B⊤∥2 = 1, and the second ineuality follows from
∥M∥∞≤1."
OTHER,0.7871621621621622,"Combining Lemma 2, Lemma 10 and Lemma 11, we conclude that JSR( ˜
MG,ϵ) < 1."
OTHER,0.7905405405405406,Demystifying Oversmoothing in Attention-Based Graph Neural Networks
OTHER,0.793918918918919,"E
Proof of Theorem 1"
OTHER,0.7972972972972973,"Recall the formulation of X(t+1)
·i
in (2):"
OTHER,0.8006756756756757,"X(t+1)
·i
= σ(P (t)(X(t)W (t))·i) =
X"
OTHER,0.8040540540540541,"jt+1=i, (jt,...,j0)∈[d]t+1 tY"
OTHER,0.8074324324324325,"k=0
W (k)
jkjk+1 !"
OTHER,0.8108108108108109,"D(t)
jt+1P (t)...D(0)
j1 P (0)X(0)
j0 ."
OTHER,0.8141891891891891,Then it follows that
OTHER,0.8175675675675675,"∥BX(t+1)
·i
∥2 =  X"
OTHER,0.8209459459459459,"jt+1=i, (jt,...,j0)∈[d]t+1 tY"
OTHER,0.8243243243243243,"k=0
W (k)
jkjk+1 !"
OTHER,0.8277027027027027,"BD(t)
jt+1P (t)...D(0)
j1 P (0)X(0)
j0 2 ≤
X"
OTHER,0.831081081081081,"jt+1=i, (jt,...,j0)∈[d]t+1 tY k=0"
OTHER,0.8344594594594594,"W (k)
jkjk+1 "
OTHER,0.8378378378378378,"! 


BD(t)
jt+1P (t)...D(0)
j1 P (0)X(0)
j0 2 =
X"
OTHER,0.8412162162162162,"jt+1=i, (jt,...,j0)∈[d]t+1 tY k=0"
OTHER,0.8445945945945946,"W (k)
jkjk+1 "
OTHER,0.847972972972973,"! 


 ˜D(t)
jt+1 ˜P (t)... ˜D(0)
j1 ˜P (0)BX(0)
j0 2 ≤
X"
OTHER,0.8513513513513513,"jt+1=i, (jt,...,j0)∈[d]t+1 tY k=0"
OTHER,0.8547297297297297,"W (k)
jkjk+1  !"
OTHER,0.8581081081081081,"Cqt+1 


BX(0)
j0 2"
OTHER,0.8614864864864865,"≤C′qt+1  
X"
OTHER,0.8648648648648649,"jt+1=i, (jt,...,j0)∈[d]t+1 tY k=0"
OTHER,0.8682432432432432,"W (k)
jkjk+1  ! "
OTHER,0.8716216216216216,"= C′qt+1∥(|W (0)|...|W (t)|)·i∥1 ,"
OTHER,0.875,"where C′ = Cmax
j∈[d]∥BX(0)
j
∥2 and ∥· ∥1 denotes the 1-norm. Specifically, the first inequality follows"
OTHER,0.8783783783783784,"from the triangle inequality, and the second inequality is due to the property of the joint spectral
radius in (3), where JSR( ˜
MG,ϵ) < q < 1."
OTHER,0.8817567567567568,"Since ∥Bx∥2 = ∥x∥2 if x⊤1 = 0 for x ∈RN, we also have that if X⊤1 = 0 for X ∈RN×d, then"
OTHER,0.8851351351351351,"∥BX∥F = ∥X∥F ,"
OTHER,0.8885135135135135,using which we obtain that
OTHER,0.8918918918918919,µ(X(t+1)) = ∥X(t+1) −1γX(t+1)∥F = ∥BX(t+1)∥F =
OTHER,0.8952702702702703,"v
u
u
t d
X"
OTHER,0.8986486486486487,"i=1
∥BX(t+1)
·i
∥2
2"
OTHER,0.902027027027027,≤C′qt+1
OTHER,0.9054054054054054,"v
u
u
t d
X"
OTHER,0.9087837837837838,"i=1
∥(|W (0)|...|W (t)|)·i∥2
1"
OTHER,0.9121621621621622,≤C′qt+1
OTHER,0.9155405405405406,"v
u
u
t d
X"
OTHER,0.918918918918919,"i=1
∥|(W (0)|...|W (t)|)·i∥1 !2"
OTHER,0.9222972972972973,"= C′qt+1∥|(W (0)|...|W (t)|∥1,1 ,"
OTHER,0.9256756756756757,"where ∥· ∥1,1 denotes the matrix (1, 1)-norm (recall from Section A.2 that for a matrix M ∈Rm×n,
we have ∥M∥1,1 = Pm
i=1
Pn
j=1 |Mij|). The assumption A3 implies that there exists C′′ such that
for all t ∈N≥0,
∥(|W (0)|...|W (t)|)∥1,1 ≤C′′d2 ."
OTHER,0.9290540540540541,"Thus we conclude that there exists C1 such that for all t ∈N≥0,"
OTHER,0.9324324324324325,µ(X(t)) ≤C1qt .
OTHER,0.9358108108108109,Demystifying Oversmoothing in Attention-Based Graph Neural Networks
OTHER,0.9391891891891891,"F
Proof of Proposition 1"
OTHER,0.9425675675675675,"Since D−1
degA is similar to D−1/2
deg AD−1/2
deg , they have the same spectrum. For D−1
degA, the smallest
nonzero entry has value 1/dmax, where dmax is the maximum node degree in G. On the other hand,
it follows from the definition of PG,ϵ that"
OTHER,0.9459459459459459,ϵdmax ≤1 .
OTHER,0.9493243243243243,"Therefore, ϵ ≤1/dmax and thus D−1
degA ∈PG,ϵ."
OTHER,0.9527027027027027,"We proceed by proving the following result.
Lemma 12. For any M in M, the spectral radius of M denoted by ρ(M), satisfies"
OTHER,0.956081081081081,ρ(M) ≤JSR(M) .
OTHER,0.9594594594594594,"Proof of Lemma 12. Gelfand’s formula states that ρ(M) =
lim
k→∞∥M k∥
1
k , where the quantity is"
OTHER,0.9628378378378378,"independent of the norm used [28]. Then comparing with the definition of the joint spectral radius,
we can immediately conclude the statement."
OTHER,0.9662162162162162,"Let B(D−1
degA) = ˜PB. By definition, ˜P ∈˜
MG,ϵ since D−1
degA ∈PG,ϵ as shown before the lemma.
Moreover, the spectrum of ˜P is the spectrum of D−1
degA after reducing the multiplicity of eigenvalue 1
by one. Under the assumption A1, the eigenvalue 1 of D−1
degA has multiplicity 1, and hence ρ( ˜P) = λ,
where λ is the second largest eigenvalue of D−1
degA. Putting this together with Lemma 12, we conclude
that
λ ≤JSR( ˜
MG,ϵ)
as desired."
OTHER,0.9695945945945946,"G
Numerical Experiments"
OTHER,0.972972972972973,"Here we provide more details on the numerical experiments. All models were implemented with
PyTorch [29] and PyTorch Geometric [30]."
OTHER,0.9763513513513513,"Datasets.
We used torch_geometric.datasets.planetoid provided in PyTorch Geometric
for all the three datasets: Cora, CiteSeer, and PubMed with their default training and test splits."
OTHER,0.9797297297297297,Model details.
OTHER,0.9831081081081081,"• For GAT, we consider the architecture proposed in Veliˇckovi´c et al. [7] with each attentional
layer sharing the parameter a in LeakyReLU(a⊤[W ⊤Xi||W ⊤Xj]), a ∈R2d′ to compute the
attention scores.
• For GCN, we consider the standard random walk graph convolution D−1
degA. That is, the update
rule of each graph convolutional layer can be written as"
OTHER,0.9864864864864865,"X′ = D−1
degAXW ,"
OTHER,0.9898648648648649,"where X and X′ are the input and output node representations, respectively, and W is the shared
learnable weight matrix in the layer."
OTHER,0.9932432432432432,"Compute.
We trained all of our models on a Telsa V100 GPU."
OTHER,0.9966216216216216,"Training details.
In all experiments, we used the Adam optimizer using a learning rate of 0.00001
and 0.0005 weight decay and trained for 1000 epoch."
