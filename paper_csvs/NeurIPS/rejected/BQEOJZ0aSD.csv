Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0011641443538998836,"Training a diverse ensemble of models has several practical application scenarios,
1"
ABSTRACT,0.002328288707799767,"such as model selection for out-of-distribution (OOD) generalization and the
2"
ABSTRACT,0.0034924330616996507,"detection of OOD samples via Bayesian principles. Previous approaches to diverse
3"
ABSTRACT,0.004656577415599534,"ensemble training have relied on the framework of letting the models make the
4"
ABSTRACT,0.005820721769499418,"correct predictions for the given in-distribution (ID) data while letting them come up
5"
ABSTRACT,0.006984866123399301,"with different hypotheses for the OOD data. As such, they require well-separated
6"
ABSTRACT,0.008149010477299184,"ID and OOD datasets to ensure a performant and diverse ensemble and have
7"
ABSTRACT,0.009313154831199068,"only been verified in smaller-scale lab environments where such a separation is
8"
ABSTRACT,0.010477299185098952,"readily available. In this work, we propose a framework, Scalable Ensemble
9"
ABSTRACT,0.011641443538998836,"Diversification (SED), for scaling up existing diversification methods to large-scale
10"
ABSTRACT,0.012805587892898719,"datasets and tasks (e.g. ImageNet), where the ID-OOD separation may not be
11"
ABSTRACT,0.013969732246798603,"available. SED automatically identifies OOD samples within the large-scale ID
12"
ABSTRACT,0.015133876600698487,"dataset on the fly and encourages the ensemble to make diverse hypotheses on
13"
ABSTRACT,0.01629802095459837,"them. To make SED more suitable for large-scale applications, we propose an
14"
ABSTRACT,0.017462165308498253,"algorithm to speed up the expensive pairwise disagreement computation. We verify
15"
ABSTRACT,0.018626309662398137,"the resulting diversification of the ensemble on ImageNet and demonstrate the
16"
ABSTRACT,0.01979045401629802,"benefit of diversification on the OOD generalization and OOD detection tasks.
17"
ABSTRACT,0.020954598370197905,"In particular, for OOD detection, we propose a novel uncertainty score estimator
18"
ABSTRACT,0.02211874272409779,"based on the diversity of ensemble hypotheses, which lets SED surpass all the
19"
ABSTRACT,0.023282887077997673,"considered baselines in OOD detection task. Code will be available soon.
20"
INTRODUCTION,0.024447031431897557,"1
Introduction
21"
INTRODUCTION,0.025611175785797437,"Training a diverse ensemble of models is useful in multiple applications. Diverse ensembles are used
22"
INTRODUCTION,0.02677532013969732,"to enhance out-of-distribution (OOD) generalization, where strong spurious features learned from
23"
INTRODUCTION,0.027939464493597205,"the in-distribution (ID) training data hinder generalization [30, 31, 28, 23]. By learning multiple
24"
INTRODUCTION,0.02910360884749709,"hypotheses, the ensemble is given a chance to learn causal features that are otherwise overshadowed
25"
INTRODUCTION,0.030267753201396973,"by the prominent spurious features [39, 4]. In Bayesian machine learning, diversification of the
26"
INTRODUCTION,0.03143189755529686,"posterior samples has been studied as a means to improve the precision and efficiency of sample
27"
INTRODUCTION,0.03259604190919674,"uncertainty estimates [5, 37].
28"
INTRODUCTION,0.033760186263096625,"A common strategy to train a diverse ensemble is to introduce two objectives: one for the main
29"
INTRODUCTION,0.034924330616996506,"task and one for diversification [29, 5, 28, 23]. The main task loss, such as the cross-entropy loss
30"
INTRODUCTION,0.03608847497089639,"for classification, encourages the hypotheses to solve the task on the labeled ID training set. The
31"
INTRODUCTION,0.037252619324796274,"diversification loss encourages the hypotheses to diversify the responses on an unlabelled OOD
32"
INTRODUCTION,0.03841676367869616,"dataset [28, 23] (Figure 1). The datasets for the objectives are separated to avoid contradictory
33"
INTRODUCTION,0.03958090803259604,"objectives: prediction diversification on the ID set will encourage wrong answers if there is only one
34"
INTRODUCTION,0.04074505238649592,"correct label.
35"
INTRODUCTION,0.04190919674039581,"This strategy, however, requires a separate OOD dataset where the hypotheses may make diverse
36"
INTRODUCTION,0.04307334109429569,"predictions without harming the main task performance on the ID training samples. Previous work
37"
INTRODUCTION,0.04423748544819558,"has thus been tested on hypothetical lab settings where the spurious and causal features can easily be
38"
INTRODUCTION,0.04540162980209546,"controlled to secure separate ID and OOD datasets for diverse ensemble training. It is not clear yet
39"
INTRODUCTION,0.046565774155995346,"how one could diversify an ensemble of models for realistic, uncontrolled, and large-scale applications
40"
INTRODUCTION,0.047729918509895226,"(e.g. ImageNet scale) where collecting a separate OOD dataset can be very costly, if not impossible.
41"
INTRODUCTION,0.048894062863795114,Existing work assumes existence of ID and OOD datasets.
INTRODUCTION,0.050058207217694994,Our Scalable Ensemble Diversiﬁcation (SED) only requires a single ID dataset.
INTRODUCTION,0.051222351571594875,Diverse model ensemble
INTRODUCTION,0.05238649592549476,Classify
"DATASETS
TRAINING OBJECTIVES",0.05355064027939464,"2 datasets
training objectives"
"DATASETS
TRAINING OBJECTIVES",0.05471478463329453,Disagree
"DATASETS
TRAINING OBJECTIVES",0.05587892898719441,ID dataset
"DATASETS
TRAINING OBJECTIVES",0.0570430733410943,OOD dataset
"DATASETS
TRAINING OBJECTIVES",0.05820721769499418,"Diverse model ensemble
1 dataset
training objectives"
"DATASETS
TRAINING OBJECTIVES",0.059371362048894066,Classify
"DATASETS
TRAINING OBJECTIVES",0.06053550640279395,Disagree
"DATASETS
TRAINING OBJECTIVES",0.06169965075669383,Identify OOD samples
"DATASETS
TRAINING OBJECTIVES",0.06286379511059371,ID dataset
"DATASETS
TRAINING OBJECTIVES",0.0640279394644936,OOD samples
"DATASETS
TRAINING OBJECTIVES",0.06519208381839348,"Figure 1: Existing diversification work vs SED. Unlike previous
diversification approaches that require a separate OOD dataset on
which the models are trained to diverge, our Scalable Ensemble
Diversification (SED) operates on a single ID dataset where OOD
samples are dynamically identified and are used to let the ensemble
members diverge."
"DATASETS
TRAINING OBJECTIVES",0.06635622817229336,"To address the scalability challenge,
42"
"DATASETS
TRAINING OBJECTIVES",0.06752037252619325,"we propose a novel diversification
43"
"DATASETS
TRAINING OBJECTIVES",0.06868451688009314,"framework, Scalable Ensemble Diver-
44"
"DATASETS
TRAINING OBJECTIVES",0.06984866123399301,"sification (SED, Figure 1). We intro-
45"
"DATASETS
TRAINING OBJECTIVES",0.0710128055878929,"duce three ingredients. (1) OOD sam-
46"
"DATASETS
TRAINING OBJECTIVES",0.07217694994179279,"ples are dynamically selected from the
47"
"DATASETS
TRAINING OBJECTIVES",0.07334109429569266,"ID training samples, on which the mod-
48"
"DATASETS
TRAINING OBJECTIVES",0.07450523864959255,"els are trained to make different predic-
49"
"DATASETS
TRAINING OBJECTIVES",0.07566938300349244,"tions. (2) At each iteration, a subset of
50"
"DATASETS
TRAINING OBJECTIVES",0.07683352735739232,"model pairs are stochastically selected
51"
"DATASETS
TRAINING OBJECTIVES",0.0779976717112922,"to construct the disagreement objec-
52"
"DATASETS
TRAINING OBJECTIVES",0.07916181606519208,"tive, rather than the full list of model
53"
"DATASETS
TRAINING OBJECTIVES",0.08032596041909197,"pairs. (3) Deep networks are trained
54"
"DATASETS
TRAINING OBJECTIVES",0.08149010477299184,"to diversify only a few layers at the
55"
"DATASETS
TRAINING OBJECTIVES",0.08265424912689173,"end, rather than the full networks. This
56"
"DATASETS
TRAINING OBJECTIVES",0.08381839348079162,"framework allows scaling up existing
57"
"DATASETS
TRAINING OBJECTIVES",0.08498253783469151,"ensemble diversification methods. In
58"
"DATASETS
TRAINING OBJECTIVES",0.08614668218859138,"this work, we focus on scaling up the
59"
"DATASETS
TRAINING OBJECTIVES",0.08731082654249127,"Agree to Disagree (A2D) method [28].
60"
"DATASETS
TRAINING OBJECTIVES",0.08847497089639116,"We verify that SEDdiversifies a model
61"
"DATASETS
TRAINING OBJECTIVES",0.08963911525029103,"ensemble trained on ImageNet. We
62"
"DATASETS
TRAINING OBJECTIVES",0.09080325960419092,"demonstrate the benefit of diversifica-
63"
"DATASETS
TRAINING OBJECTIVES",0.0919674039580908,"tion on OOD generalization and OOD
64"
"DATASETS
TRAINING OBJECTIVES",0.09313154831199069,"detection tasks. For the former, we showcase the usage of SED-diversified ensemble in three variants:
65"
"DATASETS
TRAINING OBJECTIVES",0.09429569266589057,"(a) vanilla ensemble of prediction probabilities [22], (b) an average of the model weights through
66"
"DATASETS
TRAINING OBJECTIVES",0.09545983701979045,"model soup [38], and (c) the oracle selection of the individual models for each OOD test set [23, 30].
67"
"DATASETS
TRAINING OBJECTIVES",0.09662398137369034,"In all three cases, SEDachieves a superior generalization to OOD datasets like ImageNet-A/R/C,
68"
"DATASETS
TRAINING OBJECTIVES",0.09778812572759023,"OpenImages, and iNaturalist.
69"
"DATASETS
TRAINING OBJECTIVES",0.0989522700814901,"For OOD detection, we seek multiple ways to use the SED-diversified ensemble: (a) treating them as
70"
"DATASETS
TRAINING OBJECTIVES",0.10011641443538999,"samples of the Bayesian posterior and (b) using our novel OODness estimate of Predictive Diversity
71"
"DATASETS
TRAINING OBJECTIVES",0.10128055878928988,"Score (PDS) that measures the diversity of predictions from an ensemble. We show that PDS provides
72"
"DATASETS
TRAINING OBJECTIVES",0.10244470314318975,"a superior detection of OOD samples like ImageNet-A/R/C, OpenImages, and iNaturalist.
73"
"DATASETS
TRAINING OBJECTIVES",0.10360884749708964,"Our contributions are
74"
"DATASETS
TRAINING OBJECTIVES",0.10477299185098952,"1. Scalable Ensemble Diversification (SED) framework that scales up existing ensemble
75"
"DATASETS
TRAINING OBJECTIVES",0.10593713620488941,"methods;
76"
"DATASETS
TRAINING OBJECTIVES",0.10710128055878929,"2. Predictive Diversity Score (PDS) that computes the OODness score for samples based on
77"
"DATASETS
TRAINING OBJECTIVES",0.10826542491268917,"ensemble prediction diversity;
78"
FIRST DEMONSTRATION OF THE ENSEMBLE DIVERSIFICATION AND ITS APPLICATION TO OOD GENERALIZATION,0.10942956926658906,"3. First demonstration of the ensemble diversification and its application to OOD generalization
79"
FIRST DEMONSTRATION OF THE ENSEMBLE DIVERSIFICATION AND ITS APPLICATION TO OOD GENERALIZATION,0.11059371362048893,"and detection at ImageNet level.
80"
FIRST DEMONSTRATION OF THE ENSEMBLE DIVERSIFICATION AND ITS APPLICATION TO OOD GENERALIZATION,0.11175785797438882,"The code will be released with the next versions of the manuscript.
81"
RELATED WORK,0.11292200232828871,"2
Related work
82"
RELATED WORK,0.1140861466821886,"In this section, we give a short overview of ensembling methods. At first, we speak about ensembles
83"
RELATED WORK,0.11525029103608847,"in general and the role of diversity in them (§ 2.1), then we focus on ensembling methods for neural
84"
RELATED WORK,0.11641443538998836,"networks and separate them into two big groups. The first group includes algorithms that use loss
85"
RELATED WORK,0.11757857974388825,"regularizers (§ 2.2) and the second group covers works that do not modify the training loss (§ 2.3).
86"
ENSEMBLES AS A TECHNIQUE,0.11874272409778813,"2.1
Ensembles as a technique
87"
ENSEMBLES AS A TECHNIQUE,0.119906868451688,"Ensembling is a powerful technique of aggregating the outputs of multiple models to make more
88"
ENSEMBLES AS A TECHNIQUE,0.1210710128055879,"accurate predictions and it has been around for decades [12, 21, 18, 2, 3]. It is well known that
89"
ENSEMBLES AS A TECHNIQUE,0.12223515715948778,"diversity in ensemble members’ outputs leads to better performance of the ensemble compared to the
90"
ENSEMBLES AS A TECHNIQUE,0.12339930151338765,"performance of a single model [21] because ensemble members make independent errors [12, 11].
91"
ENSEMBLES AS A TECHNIQUE,0.12456344586728754,"Therefore, one way to reduce DNNs’ reliance on spurious correlations is to train multiple models
92"
ENSEMBLES AS A TECHNIQUE,0.12572759022118743,"on the same task and make them diverse in terms of errors they make so that their ensemble is less
93"
ENSEMBLES AS A TECHNIQUE,0.12689173457508732,"dependent on such correlations.
94"
NEURAL NETWORK ENSEMBLES THAT PROMOTE DIVERSITY THROUGH LOSS REGULARIZERS,0.1280558789289872,"2.2
Neural network ensembles that promote diversity through loss regularizers
95"
NEURAL NETWORK ENSEMBLES THAT PROMOTE DIVERSITY THROUGH LOSS REGULARIZERS,0.12922002328288706,"Diversity in models can be induced by supplying training loss with a suitable regularizer.
96"
NEURAL NETWORK ENSEMBLES THAT PROMOTE DIVERSITY THROUGH LOSS REGULARIZERS,0.13038416763678695,"Such regularizers can diversify models’ weights [5, 7, 34, 6], features [39, 4], input gradients
97"
NEURAL NETWORK ENSEMBLES THAT PROMOTE DIVERSITY THROUGH LOSS REGULARIZERS,0.13154831199068684,"[29, 30, 31, 33] and outputs [25, 5, 28, 23].
98"
NEURAL NETWORK ENSEMBLES THAT PROMOTE DIVERSITY THROUGH LOSS REGULARIZERS,0.13271245634458673,"Notably, in [5] authors showed that regularizer of a certain structure that repulses ensemble members’
99"
NEURAL NETWORK ENSEMBLES THAT PROMOTE DIVERSITY THROUGH LOSS REGULARIZERS,0.13387660069848661,"weights or outputs leads to ensembles that provide a better approximation of Bayesian Model
100"
NEURAL NETWORK ENSEMBLES THAT PROMOTE DIVERSITY THROUGH LOSS REGULARIZERS,0.1350407450523865,"Averaging. This idea was later extended by works that repulse ensemble members’ features [39] and
101"
NEURAL NETWORK ENSEMBLES THAT PROMOTE DIVERSITY THROUGH LOSS REGULARIZERS,0.1362048894062864,"input gradients [33].
102"
NEURAL NETWORK ENSEMBLES THAT PROMOTE DIVERSITY THROUGH LOSS REGULARIZERS,0.13736903376018628,"Since the ensemble performs better due to the diversity of errors that ensemble members make
103"
NEURAL NETWORK ENSEMBLES THAT PROMOTE DIVERSITY THROUGH LOSS REGULARIZERS,0.13853317811408614,"[21] we want those members to give pairwise different outputs for the same inputs. Unfortunately,
104"
NEURAL NETWORK ENSEMBLES THAT PROMOTE DIVERSITY THROUGH LOSS REGULARIZERS,0.13969732246798602,"diversity in weights space, input gradient space, or features space does not guarantee such property
105"
NEURAL NETWORK ENSEMBLES THAT PROMOTE DIVERSITY THROUGH LOSS REGULARIZERS,0.1408614668218859,"without additional assumptions due to functional symmetry which means that models can be different
106"
NEURAL NETWORK ENSEMBLES THAT PROMOTE DIVERSITY THROUGH LOSS REGULARIZERS,0.1420256111757858,"in terms of their weights or feature maps and input gradients they produce but still give the same
107"
NEURAL NETWORK ENSEMBLES THAT PROMOTE DIVERSITY THROUGH LOSS REGULARIZERS,0.14318975552968569,"outputs for a given input. That is why we are focused on methods that diversify models’ outputs,
108"
NEURAL NETWORK ENSEMBLES THAT PROMOTE DIVERSITY THROUGH LOSS REGULARIZERS,0.14435389988358557,"specifically [28, 23] which are state-of-the-art according to [1] and use regularizer of repulsive nature
109"
NEURAL NETWORK ENSEMBLES THAT PROMOTE DIVERSITY THROUGH LOSS REGULARIZERS,0.14551804423748546,"conceptually similar to [5].
110"
NEURAL NETWORK ENSEMBLES THAT PROMOTE DIVERSITY WITHOUT MODIFYING LOSS,0.14668218859138532,"2.3
Neural network ensembles that promote diversity without modifying loss
111"
NEURAL NETWORK ENSEMBLES THAT PROMOTE DIVERSITY WITHOUT MODIFYING LOSS,0.1478463329452852,"In addition to loss regularizers, there were an uncountable number of different ways to induce diversity
112"
NEURAL NETWORK ENSEMBLES THAT PROMOTE DIVERSITY WITHOUT MODIFYING LOSS,0.1490104772991851,"in ensembles of neural networks that did not modify the training loss. The most straightforward
113"
NEURAL NETWORK ENSEMBLES THAT PROMOTE DIVERSITY WITHOUT MODIFYING LOSS,0.15017462165308498,"approach of independently training multiple models of the same architecture by changing only random
114"
NEURAL NETWORK ENSEMBLES THAT PROMOTE DIVERSITY WITHOUT MODIFYING LOSS,0.15133876600698487,"seeds is called Deep Ensemble [22] which was extended from the Bayesian perspective in [37].
115"
NEURAL NETWORK ENSEMBLES THAT PROMOTE DIVERSITY WITHOUT MODIFYING LOSS,0.15250291036088476,"Another solution is to construct an ensemble from models trained with different hyperparameters [36],
116"
NEURAL NETWORK ENSEMBLES THAT PROMOTE DIVERSITY WITHOUT MODIFYING LOSS,0.15366705471478465,"augmentations [24], or architectures [40]. More computationally efficient direction allows training
117"
NEURAL NETWORK ENSEMBLES THAT PROMOTE DIVERSITY WITHOUT MODIFYING LOSS,0.1548311990686845,"only one base model inducing diversity by ensembling either checkpoints saved in different local
118"
NEURAL NETWORK ENSEMBLES THAT PROMOTE DIVERSITY WITHOUT MODIFYING LOSS,0.1559953434225844,"minima along the training trajectory of this base model [19] or models produced by the base model
119"
NEURAL NETWORK ENSEMBLES THAT PROMOTE DIVERSITY WITHOUT MODIFYING LOSS,0.15715948777648428,"after applying dropout [10] or masking [9] to it. The mixture of experts paradigm can also be viewed
120"
NEURAL NETWORK ENSEMBLES THAT PROMOTE DIVERSITY WITHOUT MODIFYING LOSS,0.15832363213038417,"as an ensemble diversification technique [41] where diversification happens due to assigning different
121"
NEURAL NETWORK ENSEMBLES THAT PROMOTE DIVERSITY WITHOUT MODIFYING LOSS,0.15948777648428406,"training samples to different ensemble members.
122"
NEURAL NETWORK ENSEMBLES THAT PROMOTE DIVERSITY WITHOUT MODIFYING LOSS,0.16065192083818394,"Despite their conceptual simplicity Deep Ensembles [22] and ensembles of models trained with
123"
NEURAL NETWORK ENSEMBLES THAT PROMOTE DIVERSITY WITHOUT MODIFYING LOSS,0.16181606519208383,"different hyperparameters [36] are strong baselines for OOD detection [27] and OOD generalization
124"
NEURAL NETWORK ENSEMBLES THAT PROMOTE DIVERSITY WITHOUT MODIFYING LOSS,0.1629802095459837,"tasks, especially when combined with model souping techniques [38]. That is why we selected them
125"
NEURAL NETWORK ENSEMBLES THAT PROMOTE DIVERSITY WITHOUT MODIFYING LOSS,0.16414435389988358,"as baselines for our experiments.
126"
METHOD,0.16530849825378346,"3
Method
127"
METHOD,0.16647264260768335,"We present our main technical contributions, Scalable Ensemble Diversification (SED, §3.2) and the
128"
METHOD,0.16763678696158324,"Predictive Diversity Score (PDS, §3.3).
129"
PRELIMINARIES,0.16880093131548313,"3.1
Preliminaries
130"
PRELIMINARIES,0.16996507566938301,"We cover background materials before introducing our main technical contributions. We work with
131"
PRELIMINARIES,0.17112922002328287,"a training set D := {xn, yn}N
n=1, which we refer to as the in-distribution (ID) dataset. For prior
132"
PRELIMINARIES,0.17229336437718276,"diversification methods, we also assume the existence of a separate, unlabeled out-of-distribution
133"
PRELIMINARIES,0.17345750873108265,"(OOD) dataset Dood := {xood
n }Nood
n=1. We write f(·, θ) for a deep neural network classifier parametrized
134"
PRELIMINARIES,0.17462165308498254,"by θ. f (x; θ) ∈RC indicates the logit outputs for C classes for input x. We write p(x) :=
135"
PRELIMINARIES,0.17578579743888242,"Softmax(f(x)) ∈[0, 1]C for the probability outputs. We consider an ensemble {f 1, · · · , f M} of M
136"
PRELIMINARIES,0.1769499417927823,"models.
137"
EXISTING ENSEMBLE DIVERSIFICATION APPROACH,0.1781140861466822,"3.1.1
Existing ensemble diversification approach
138"
EXISTING ENSEMBLE DIVERSIFICATION APPROACH,0.17927823050058206,"We introduce an existing approach for diversifying an ensemble of models [28, 23]. Two objectives
139"
EXISTING ENSEMBLE DIVERSIFICATION APPROACH,0.18044237485448195,"are imposed upon the ensemble of models: the main task loss and the diversification regularization.
140"
EXISTING ENSEMBLE DIVERSIFICATION APPROACH,0.18160651920838183,"For the main task, the community has focused on the classification task. The cross-entropy loss
141"
EXISTING ENSEMBLE DIVERSIFICATION APPROACH,0.18277066356228172,"−log py(x; θ) is used to train the model ensemble {f 1, · · · , f M} on the ID dataset D:
142"
EXISTING ENSEMBLE DIVERSIFICATION APPROACH,0.1839348079161816,"Lmain =
1
MN X n X"
EXISTING ENSEMBLE DIVERSIFICATION APPROACH,0.1850989522700815,"m
−log pm
yn(xn; θ).
(1)"
EXISTING ENSEMBLE DIVERSIFICATION APPROACH,0.18626309662398138,"This encourages each member of the ensemble to behave similarly on the ID dataset.
143"
EXISTING ENSEMBLE DIVERSIFICATION APPROACH,0.18742724097788127,"Different diversification schemes use different diversification regularization loss Ldiv applied on pairs
144"
EXISTING ENSEMBLE DIVERSIFICATION APPROACH,0.18859138533178113,"(f m, f l) of ensemble members. The diversification objective is commonly optimized on the OOD
145"
EXISTING ENSEMBLE DIVERSIFICATION APPROACH,0.18975552968568102,"dataset Dood to encourage the training of multiple hypotheses on the OOD samples while avoiding
146"
EXISTING ENSEMBLE DIVERSIFICATION APPROACH,0.1909196740395809,"clashes with the main task objective. In this work, we focus on the Agree to Disagree [28] method.
147"
EXISTING ENSEMBLE DIVERSIFICATION APPROACH,0.1920838183934808,"The diversification loss for a pair (pm, pl) is defined as:
148"
EXISTING ENSEMBLE DIVERSIFICATION APPROACH,0.19324796274738068,"A2D(pm(x), pl(x)) = −log

pm
ˆy (x) · (1 −pl
ˆy(x)) + (1 −pm
ˆy (x)) · pl
ˆy(x)

(2)"
EXISTING ENSEMBLE DIVERSIFICATION APPROACH,0.19441210710128057,"where ˆy := arg maxc pm
c (x) is the predicted class for the first model pm. One may symmetrically
149"
EXISTING ENSEMBLE DIVERSIFICATION APPROACH,0.19557625145518046,"define ˆy to be the prediction for the second model pl; in practice, it does not make a difference [28].
150"
EXISTING ENSEMBLE DIVERSIFICATION APPROACH,0.19674039580908032,"Note that the diversification loss favors pl to predict a lower likelihood for the prediction by pm,
151"
EXISTING ENSEMBLE DIVERSIFICATION APPROACH,0.1979045401629802,"pl
ˆy(x), and vice versa. For M models in an ensemble, A2D is applied on the OOD dataset Dood for
152"
EXISTING ENSEMBLE DIVERSIFICATION APPROACH,0.1990686845168801,"every pair of models (pm, pl):
153"
EXISTING ENSEMBLE DIVERSIFICATION APPROACH,0.20023282887077998,"Ldiv =
1
N ood · M(M −1) X n X"
EXISTING ENSEMBLE DIVERSIFICATION APPROACH,0.20139697322467986,"m<l
A2D(pm(xood
n ), pl(xood
n )).
(3)"
EXISTING ENSEMBLE DIVERSIFICATION APPROACH,0.20256111757857975,"3.2
Scalable Ensemble Diversification (SED)
154"
EXISTING ENSEMBLE DIVERSIFICATION APPROACH,0.20372526193247964,"We present Scalable Ensemble Diversification (SED) that addresses the limitation of the existing
155"
EXISTING ENSEMBLE DIVERSIFICATION APPROACH,0.2048894062863795,"ensemble diversification framework that requires a separate OOD dataset. We introduce two main
156"
EXISTING ENSEMBLE DIVERSIFICATION APPROACH,0.2060535506402794,"components of SED: dynamic selection of OOD samples within the ID dataset (§3.2.1) and the
157"
EXISTING ENSEMBLE DIVERSIFICATION APPROACH,0.20721769499417927,"stochastic selection of pairs to diverge in the optimization iterations (§3.2.2).
158"
DYNAMIC SELECTION OF OOD SAMPLES,0.20838183934807916,"3.2.1
Dynamic selection of OOD samples
159"
DYNAMIC SELECTION OF OOD SAMPLES,0.20954598370197905,"If only the ID training dataset is present, it is difficult to induce diversity in ensemble members,
160"
DYNAMIC SELECTION OF OOD SAMPLES,0.21071012805587894,"as they are uniformly incentivized to solve the main task objective: given x, predict y. Hence,
161"
DYNAMIC SELECTION OF OOD SAMPLES,0.21187427240977882,"previous approaches have introduced a qualitatively disjoint unlabeled set, which we refer to as
162"
DYNAMIC SELECTION OF OOD SAMPLES,0.21303841676367868,"the OOD dataset, where the ensemble members are encouraged to disagree with each other. The
163"
DYNAMIC SELECTION OF OOD SAMPLES,0.21420256111757857,"clear separation of ID and OOD datasets for the two objectives matters for ensuring a good balance
164"
DYNAMIC SELECTION OF OOD SAMPLES,0.21536670547147846,"between the main task performance and the diversity of hypotheses.
165"
DYNAMIC SELECTION OF OOD SAMPLES,0.21653084982537835,"Previous works like Pagliardini et al. [28], Lee et al. [23] have performed experiments on small-scale
166"
DYNAMIC SELECTION OF OOD SAMPLES,0.21769499417927823,"datasets where factors are well-controlled and clean versions of OOD datasets are readily available.
167"
DYNAMIC SELECTION OF OOD SAMPLES,0.21885913853317812,"Examples include Waterbirds, Camelyon17, CelebA, MultiNLI, C-MNIST, and the Office-Home
168"
DYNAMIC SELECTION OF OOD SAMPLES,0.220023282887078,"datasets. For example, for Waterbirds, the ID dataset is set as the cases where the bird’s habitat
169"
DYNAMIC SELECTION OF OOD SAMPLES,0.22118742724097787,"matches with the visual background and the OOD dataset corresponds to the complementary case.
170"
DYNAMIC SELECTION OF OOD SAMPLES,0.22235157159487776,"While conceptually desirable, collecting a separate OOD dataset can be highly cumbersome and
171"
DYNAMIC SELECTION OF OOD SAMPLES,0.22351571594877764,"expensive. For a large-scale dataset like ImageNet, it is highly non-obvious how one could build a
172"
DYNAMIC SELECTION OF OOD SAMPLES,0.22467986030267753,"corresponding OOD dataset where the underlying feature-label correlations are different from the ID
173"
DYNAMIC SELECTION OF OOD SAMPLES,0.22584400465657742,"training dataset.
174"
DYNAMIC SELECTION OF OOD SAMPLES,0.2270081490104773,"To address this challenge, we consider dynamically identifying an OOD subset of the ID dataset and
175"
DYNAMIC SELECTION OF OOD SAMPLES,0.2281722933643772,"letting the ensemble diverge on this subset. The desiderata for the identification of OOD samples
176"
DYNAMIC SELECTION OF OOD SAMPLES,0.22933643771827705,"within the ID dataset are twofold: (a) we wish to discriminate samples where the ensemble members
177"
DYNAMIC SELECTION OF OOD SAMPLES,0.23050058207217694,"make mistakes and (b) we only trust the ensemble prediction for the OOD sample identification when
178"
DYNAMIC SELECTION OF OOD SAMPLES,0.23166472642607683,"the ensemble is sufficiently trained.
179"
DYNAMIC SELECTION OF OOD SAMPLES,0.23282887077997672,"We define the sample-wise weight αn on each ID sample (xn, yn) ∈D that satisfy the two conditions:
180"
DYNAMIC SELECTION OF OOD SAMPLES,0.2339930151338766,"αn :=
CE(f 1, · · · , f M; xn, yn)

1
|B|
P"
DYNAMIC SELECTION OF OOD SAMPLES,0.2351571594877765,"b∈B CE(f 1, · · · , f M; xb, yb)
2
(4)"
DYNAMIC SELECTION OF OOD SAMPLES,0.23632130384167638,"where CE(f 1, · · · , f M; xn, yn) := CE( 1 M
P"
DYNAMIC SELECTION OF OOD SAMPLES,0.23748544819557627,"m f m(xn), yn) is the loss on the logit-averaged pre-
181"
DYNAMIC SELECTION OF OOD SAMPLES,0.23864959254947612,"diction and B is a minibatch that contains the sample (xn, yn). αn is a weight proportional to the
182"
DYNAMIC SELECTION OF OOD SAMPLES,0.239813736903376,"ensemble loss on the sample; we thus meet the condition (a). The normalization is designed to handle
183"
DYNAMIC SELECTION OF OOD SAMPLES,0.2409778812572759,"the condition (b). To see this, consider the batch-wise weight
184"
DYNAMIC SELECTION OF OOD SAMPLES,0.2421420256111758,"αB :=
1
|B| X"
DYNAMIC SELECTION OF OOD SAMPLES,0.24330616996507567,"b∈B
αb =
1
1
|B|
P"
DYNAMIC SELECTION OF OOD SAMPLES,0.24447031431897556,"b CE(f 1, · · · , f M; xb, yb).
(5)"
DYNAMIC SELECTION OF OOD SAMPLES,0.24563445867287545,"Note that αB is now inversely proportional to the average cross-entropy loss of the ensemble on
185"
DYNAMIC SELECTION OF OOD SAMPLES,0.2467986030267753,"the batch B. Thus, the overall level of αn for n ∈B is lower for earlier iterations of the ensemble
186"
DYNAMIC SELECTION OF OOD SAMPLES,0.2479627473806752,"training, where the predictions from the models are not trustworthy yet.
187"
DYNAMIC SELECTION OF OOD SAMPLES,0.24912689173457508,"With this definition of sample-wise weight αn for the diversification objective, we define the SED
188"
DYNAMIC SELECTION OF OOD SAMPLES,0.25029103608847497,"objective with the A2D loss for the diversification kernel:
189"
DYNAMIC SELECTION OF OOD SAMPLES,0.25145518044237486,"LSED := Lmain +
λ
NM(M −1) X n X"
DYNAMIC SELECTION OF OOD SAMPLES,0.25261932479627475,"m<l
stopgrad(αn) · A2D(pm(xn), pl(xn)),
(6)"
DYNAMIC SELECTION OF OOD SAMPLES,0.25378346915017463,"where λ > 0 controls the overall weight of the diversification term. Note that, compared to Equation
190"
DYNAMIC SELECTION OF OOD SAMPLES,0.2549476135040745,"3, this formulation does not rely on the OOD dataset Dood. Instead, all ID samples are treated as
191"
DYNAMIC SELECTION OF OOD SAMPLES,0.2561117578579744,"potential OOD samples, where their OODness is softly determined via αn. This enables a seamless
192"
DYNAMIC SELECTION OF OOD SAMPLES,0.2572759022118743,"adaptation of existing ensemble diversification methods to a relaxed setting where a separate OOD
193"
DYNAMIC SELECTION OF OOD SAMPLES,0.25844004656577413,"dataset is unavailable.
194"
FURTHER TRICKS FOR SCALABILITY,0.259604190919674,"3.2.2
Further tricks for scalability
195"
FURTHER TRICKS FOR SCALABILITY,0.2607683352735739,Model 1
FURTHER TRICKS FOR SCALABILITY,0.2619324796274738,"Iteration K: 
randomly select 1 and 3 Input"
FURTHER TRICKS FOR SCALABILITY,0.2630966239813737,Forward pass
FURTHER TRICKS FOR SCALABILITY,0.26426076833527357,Model 2
FURTHER TRICKS FOR SCALABILITY,0.26542491268917345,Model 3
FURTHER TRICKS FOR SCALABILITY,0.26658905704307334,Model 1
FURTHER TRICKS FOR SCALABILITY,0.26775320139697323,"Iteration K + 1: 
randomly select 1 and 2"
FURTHER TRICKS FOR SCALABILITY,0.2689173457508731,Model 2
FURTHER TRICKS FOR SCALABILITY,0.270081490104773,Model 3
FURTHER TRICKS FOR SCALABILITY,0.2712456344586729,"Ensemble diversification algorithms are often based on pairwise
196"
FURTHER TRICKS FOR SCALABILITY,0.2724097788125728,"similarities of the members. Pairwise similarity computation scales
197"
FURTHER TRICKS FOR SCALABILITY,0.27357392316647267,"quadratically with the size of the ensemble M. The second term of
198"
FURTHER TRICKS FOR SCALABILITY,0.27473806752037255,"Equation 6 is an example of this. This is potentially a hurdle when
199"
FURTHER TRICKS FOR SCALABILITY,0.2759022118742724,"ensemble diversification is to be applied to M ≥10, and the data
200"
FURTHER TRICKS FOR SCALABILITY,0.2770663562281723,"and parameter sizes are in the order of millions (e.g. ImageNet).
201"
FURTHER TRICKS FOR SCALABILITY,0.27823050058207216,"We address this computational challenge by computing the summa-
202"
FURTHER TRICKS FOR SCALABILITY,0.27939464493597205,"tion of pairwise distances as a stochastic sum. For every minibatch B
203"
FURTHER TRICKS FOR SCALABILITY,0.28055878928987193,"of SGD iterations, we uniformly-iid sample a subset I of {1, · · · , M}
204"
FURTHER TRICKS FOR SCALABILITY,0.2817229336437718,"to compute the diversification term in Equation 6. The procedure is
205"
FURTHER TRICKS FOR SCALABILITY,0.2828870779976717,"illustrated in the figure on the right.
206"
FURTHER TRICKS FOR SCALABILITY,0.2840512223515716,"To further speed up the SED training, we consider diversifying only
207"
FURTHER TRICKS FOR SCALABILITY,0.2852153667054715,"a subset of layers, while freezing the other layers. In our experiments,
208"
FURTHER TRICKS FOR SCALABILITY,0.28637951105937137,"ensemble members share the same frozen feature extractor of Deit3b
209"
FURTHER TRICKS FOR SCALABILITY,0.28754365541327126,"[32] pretrained on ImageNet-21k [8] and we diversify only the last
210"
FURTHER TRICKS FOR SCALABILITY,0.28870779976717115,"two layers of the models.
211"
FURTHER TRICKS FOR SCALABILITY,0.28987194412107103,"3.3
Predictive Diversity Score (PDS) for OOD Detection
212"
FURTHER TRICKS FOR SCALABILITY,0.2910360884749709,"We demonstrate several benefits of the diversified ensembles in §4. One of them is the possibility of
213"
FURTHER TRICKS FOR SCALABILITY,0.29220023282887075,"using them for detecting OOD samples through the notion of epistemic uncertainty [13]. Given an
214"
FURTHER TRICKS FOR SCALABILITY,0.29336437718277064,"ensemble of models, a simple baseline for OOD detection is to compute the predictive uncertainty of
215"
FURTHER TRICKS FOR SCALABILITY,0.29452852153667053,"the Bayesian Model Averaging (BMA) by treating the ensemble members as samples of the posterior
216"
FURTHER TRICKS FOR SCALABILITY,0.2956926658905704,"p(θ|D) [22, 37]:
217"
FURTHER TRICKS FOR SCALABILITY,0.2968568102444703,"ηBMA := max
c
1
M X"
FURTHER TRICKS FOR SCALABILITY,0.2980209545983702,"m
pm
c (x).
(7)"
FURTHER TRICKS FOR SCALABILITY,0.2991850989522701,"This notion of epistemic uncertainty does not directly exploit the potential diversity in individual
218"
FURTHER TRICKS FOR SCALABILITY,0.30034924330616997,"models of the ensemble because it averages out the predictions along the model index m.
219"
FURTHER TRICKS FOR SCALABILITY,0.30151338766006985,"We propose a novel measure for epistemic uncertainty, Predictive Diversity Score (PDS), that directly
220"
FURTHER TRICKS FOR SCALABILITY,0.30267753201396974,"measures the prediction diversity of the individual members. The formulation is given below:
221"
FURTHER TRICKS FOR SCALABILITY,0.30384167636786963,ηPDS := 1 C X
FURTHER TRICKS FOR SCALABILITY,0.3050058207217695,"c
max
m pm
c (x).
(8)"
FURTHER TRICKS FOR SCALABILITY,0.3061699650756694,"PDS is a continuous relaxation of the number of unique argmax predictions within an ensemble
222"
FURTHER TRICKS FOR SCALABILITY,0.3073341094295693,"of models. To see this, consider the special case where pm ∈{0, 1} are one-hot vectors. Then,
223"
FURTHER TRICKS FOR SCALABILITY,0.3084982537834691,"maxm pm
c (x) is 1 if any of m predicts c and 0 otherwise. Thus, P"
FURTHER TRICKS FOR SCALABILITY,0.309662398137369,"c maxm pm
c (x) computes the
224"
FURTHER TRICKS FOR SCALABILITY,0.3108265424912689,"number of classes that at least one of the ensemble members predicts. We show that, with our diverse
225"
FURTHER TRICKS FOR SCALABILITY,0.3119906868451688,"ensembles, PDS outperforms the DE baseline for the OOD detection task (§4.4).
226"
EXPERIMENTS,0.3131548311990687,"4
Experiments
227"
EXPERIMENTS,0.31431897555296856,"We verify our contributions, Scalable Ensemble Diversification (SED, §3.2) and Predictive Diversity
228"
EXPERIMENTS,0.31548311990686845,"Score (PDS, §3.3), on ImageNet-scale tasks and datasets. We first verify that SED diversifies the
229"
EXPERIMENTS,0.31664726426076834,"ensemble (§4.2). Then, we demonstrate the application of diversified ensemble to OOD generalization
230"
EXPERIMENTS,0.3178114086146682,"(§4.3) and OOD detection (§4.4) tasks.
231"
EXPERIMENTAL SETUP,0.3189755529685681,"4.1
Experimental setup
232"
EXPERIMENTAL SETUP,0.320139697322468,"We task the ensemble with the OOD generalization and OOD detection tasks.
233"
EXPERIMENTAL SETUP,0.3213038416763679,"Training settings. For both tasks, we train an ensemble of models with the SED framework with
234"
EXPERIMENTAL SETUP,0.3224679860302678,"the A2D [28] diversity regularization using AdamW optimizer [26]. We use the default settings of a
235"
EXPERIMENTAL SETUP,0.32363213038416766,"batch size of 16, learning rate 10−3, weight decay 0.01, and the number of epochs 10. The overall
236"
EXPERIMENTAL SETUP,0.32479627473806755,"diversity weight λ is set to 0.1 and the stochastic pairing is done for |I| = 2 models for each SGD
237"
EXPERIMENTAL SETUP,0.3259604190919674,"batch. We use Deit3b [32] network pretrained on ImageNet21k [8] for all the experiments. Following
238"
EXPERIMENTAL SETUP,0.32712456344586727,"the speed-up trick in §3.2.2, we use only the last 2 layers of the network. For the in-distribution
239"
EXPERIMENTAL SETUP,0.32828870779976715,"(ID) dataset where the ensemble is trained to diversify, we use the training split of ImageNet with
240"
EXPERIMENTAL SETUP,0.32945285215366704,"|D| = 1, 281, 167. All experiments were ran on RTX2080Ti GPUs with 12GB vRAM and 40GB
241"
EXPERIMENTAL SETUP,0.33061699650756693,"RAM, each experiment took from 2 to 12 hours depending on the complexity of the training.
242"
EXPERIMENTAL SETUP,0.3317811408614668,"Baselines. For naive ensemble training, we consider the deep ensemble [22] where each ensemble
243"
EXPERIMENTAL SETUP,0.3329452852153667,"member independently with different random seeds that control the weight initialization and SGD
244"
EXPERIMENTAL SETUP,0.3341094295692666,"batch shuffling. To match the resource usage of our SED, where we diversify only the last 2 layers
245"
EXPERIMENTAL SETUP,0.3352735739231665,"of the network, we consider the shallow ensemble variant, which is the deep ensemble where only
246"
EXPERIMENTAL SETUP,0.33643771827706637,"the last 2 layers are trained. We further consider a viable diversification scheme that performs deep
247"
EXPERIMENTAL SETUP,0.33760186263096625,"ensemble with varying hyperparameters [36]. In addition to that, we reimplement A2D [28] and
248"
EXPERIMENTAL SETUP,0.33876600698486614,"DivDis [23] algorithms and apply them without stochastic model sampling to do classification on
249"
EXPERIMENTAL SETUP,0.33993015133876603,"labeled samples from ImageNet-Train and disagreement on unlabeled samples from ImageNet-R.
250"
EXPERIMENTAL SETUP,0.3410942956926659,"For A2D we use frozen feature extractor and a parallel variant of their method which means that all
251"
EXPERIMENTAL SETUP,0.34225844004656575,"ensemble members are trained simultaneously and not sequentially. The computational complexity
252"
EXPERIMENTAL SETUP,0.34342258440046564,"of both these approaches scales quadratically with ensemble size which is why they are called Naive
253"
EXPERIMENTAL SETUP,0.3445867287543655,"A2D and Naive DivDis respectively.
254"
EXPERIMENTAL SETUP,0.3457508731082654,"Evaluation benchmarks. The generalization performances of the ImageNet-trained ensembles are
255"
EXPERIMENTAL SETUP,0.3469150174621653,"measured on multiple test datasets, ranging from the in-distribution validation split of ImageNet with
256"
EXPERIMENTAL SETUP,0.3480791618160652,"50,000 samples to OOD datasets like ImageNet-A (A [17], 7.5k images & 200 classes), ImageNet-R
257"
EXPERIMENTAL SETUP,0.3492433061699651,"(A [16], 30k images, 200 classes), ImageNet-C (C-i for corruption strength i [14], 50k images, 1k
258"
EXPERIMENTAL SETUP,0.35040745052386496,"classes). OpenImages-O (OI [35], 17k images, unlabeled), and iNaturalist (iNat [20], 10k images,
259"
EXPERIMENTAL SETUP,0.35157159487776485,"unlabeled). For OOD detection, we task the ensemble with the detection of the above OOD datasets
260"
EXPERIMENTAL SETUP,0.35273573923166474,"against the ImageNet validation split.
261"
EXPERIMENTAL SETUP,0.3538998835855646,"Evaluation metrics. For OOD generalization, we use the accuracy. For OOD detection, we use the
262"
EXPERIMENTAL SETUP,0.3550640279394645,"area under the ROC curve, following [15].
263"
EXPERIMENTAL SETUP,0.3562281722933644,"GT
Cowboy hat
Sea lion
Scuba diver
Great shark
Weimaraner"
EXPERIMENTAL SETUP,0.3573923166472643,"SED
Cowboy hat
Sea lion
Scuba diver
Great shark
Weimaraner
Comic book
Otter
Jellyfish
Killer whale
Vizsla
PDS
0.300
0.300
0.294
0.292
0.292"
EXPERIMENTAL SETUP,0.3585564610011641,"GT
Pomegranate
Zebra
Pomegranate
Pomegranate
Hummingbird"
EXPERIMENTAL SETUP,0.359720605355064,"SED
Pomegranate
Zebra
Pomegranate
Pomegranate
Hummingbird
PDS
0.216
0.216
0.216
0.216
0.216"
EXPERIMENTAL SETUP,0.3608847497089639,"Figure 2: ImageNet-R examples leading to the greatest and least disagreement. We show the 5 most
divergent and 5 least divergent samples according to the SED ensemble. We measure the prediction diversity
with the Prediction Diversity Score (PDS) in §3.3. GT refers to the ground truth category. Ensemble predictions
are shown in bold, in cases when ensemble members predict classes different from the ensemble prediction we
provide them on the next line with standard font."
DIVERSIFICATION,0.3620488940628638,"4.2
Diversification
264"
DIVERSIFICATION,0.36321303841676367,"We start with the question of whether Scalable Ensemble Diversification (SED) truly diversify the
265"
DIVERSIFICATION,0.36437718277066355,"ensemble at the ImageNet scale. To measure the diversity of the ensemble, we compute the number
266"
DIVERSIFICATION,0.36554132712456344,"of unique predictions for each sample for the committee of models (#unique).
267"
DIVERSIFICATION,0.36670547147846333,"Method
C-1
C-5
iNat
OI"
DIVERSIFICATION,0.3678696158323632,"Deep ensemble
1.09
1.19
1.31
1.23
+Diverse hyperparams
1.11
1.32
1.48
1.33"
DIVERSIFICATION,0.3690337601862631,"Naive DivDis
1.04
1.14
1.19
1.16
Naive A2D
1.04
1.15
1.19
1.91"
DIVERSIFICATION,0.370197904540163,"SED-A2D
5.00
5.00
4.68
4.11"
DIVERSIFICATION,0.3713620488940629,"Table 1: #unique for ensembles.
We report the
#unique on OOD datasets (see §4.1 for the datasets).
The ensemble size M is 5 for all methods; it is the max
possible #unique value."
DIVERSIFICATION,0.37252619324796277,"Table 1 shows the #unique values for the IN-Val
268"
DIVERSIFICATION,0.37369033760186265,"as well as multiple OOD datasets. We observe
269"
DIVERSIFICATION,0.37485448195576254,"that the deep ensemble baseline does not increase
270"
DIVERSIFICATION,0.3760186263096624,"the diversity dramatically (e.g. 1.09 for C-1) be-
271"
DIVERSIFICATION,0.37718277066356226,"yond no-diversity values (1.0). Diversification
272"
DIVERSIFICATION,0.37834691501746215,"tricks like hyperparameter diversification (1.11
273"
DIVERSIFICATION,0.37951105937136204,"for C-1) or Naive A2D (1.04 for C-1) and DivDis
274"
DIVERSIFICATION,0.3806752037252619,"(1.04 for C-1) do not improve the prediction di-
275"
DIVERSIFICATION,0.3818393480791618,"versity dramatically. On the other hand, our SED
276"
DIVERSIFICATION,0.3830034924330617,"increases the prediction diversity across the board
277"
DIVERSIFICATION,0.3841676367869616,"(e.g. 5.00 for C-1).
278"
DIVERSIFICATION,0.3853317811408615,"Qualitative results on ImageNet-R further verify the ability of SED to diversify the ensemble (Fig-
279"
DIVERSIFICATION,0.38649592549476136,"ure 2). As a measure for diversity, we use the Predictive Diversity Score (PDS) in §3.3. We observe
280"
DIVERSIFICATION,0.38766006984866125,"that the samples inducing the highest diversity (high PDS scores) are indeed ambiguous: for the
281"
DIVERSIFICATION,0.38882421420256114,"first image, where the “cowboy hat” is the ground truth category, we observe that “comic book” is
282"
DIVERSIFICATION,0.389988358556461,"also a valid label for the image style. On the other hand, samples with low PDS exhibit clearer
283"
DIVERSIFICATION,0.3911525029103609,"image-to-category relationship.
284"
OOD GENERALIZATION,0.39231664726426074,"4.3
OOD Generalization
285"
OOD GENERALIZATION,0.39348079161816063,"We examine the first application of diversified ensembles: OOD generalization. We hypothesize that
286"
OOD GENERALIZATION,0.3946449359720605,"the superior diversification ability verified in §4.2 leads to greater OOD generalization due to the
287"
OOD GENERALIZATION,0.3958090803259604,"consideration of more robust hypotheses that do not rely on obvious spurious correlations.
288"
OOD GENERALIZATION,0.3969732246798603,"Ensemble aggregation for OOD generalization. As a means to exploit such robust hypothe-
289"
OOD GENERALIZATION,0.3981373690337602,"ses, we consider 3 aggregation strategies.
(1) Oracle selection: the best-performing individ-
290"
OOD GENERALIZATION,0.39930151338766007,"ual model is chosen from an ensemble [28, 30]. Final prediction is given by f(x; θm⋆) where
291"
OOD GENERALIZATION,0.40046565774155995,"Oracle selection
Prediction ensemble
Uniform soup"
OOD GENERALIZATION,0.40162980209545984,"Method
M
Val IN-A IN-R C-1
C-5
Val IN-A IN-R C-1
C-5
Val IN-A IN-R C-1
C-5"
OOD GENERALIZATION,0.40279394644935973,"Single model
1
85.4 37.9
44.7 75.6 38.5 85.4 37.9
44.7 75.6 38.5 85.4 37.9
44.7 75.6 38.5"
OOD GENERALIZATION,0.4039580908032596,"Deep ensemble
5
85.4 37.9
44.9 75.7 38.6 85.4 39.9
46.3 75.7 38.6 85.3 36.7
44.6 75.5 38.3
+Diverse HPs
5
85.4 38.5
45.4 77.4 40.7 85.4 39.9
46.5 76.0 39.0 85.3 35.3
44.1 75.9 38.7
Naive DivDis
5
85.2 35.8
40.8 77.2 40.2 85.1 36.3
41.8 77.2 40.2 84.8 40.7
42.5 76.2 38.9
Naive A2D
5
85.2 36.6
44.3 77.3 40.4 85.1 37.8
45.2 77.2 40.3 84.5 39.3
45.1 75.5 39.1
SED-A2D
5
85.1 38.3
45.3 77.2 40.4 85.3 42.4
48.1 77.3 40.6 85.3 40.3
46.1 77.3 40.6"
OOD GENERALIZATION,0.4051222351571595,"Deep ensemble 50 85.5 38.1
45.2 75.7 38.6 85.5 38.8
45.8 75.6 38.5 85.4 37.5
45.0 75.5 38.4
+Diverse HPs
50 85.5 38.5
45.6 77.5 40.8 85.5 42.5
48.5 76.0 39.0 85.4 36.4
44.8 75.9 38.8
SED-A2D
50 82.6 39.0
45.8 74.4 38.3 83.5 50.9
54.4 75.8 39.3 83.5 39.2
46.5 75.8 39.3"
OOD GENERALIZATION,0.4062863795110594,"Table 2: OOD generalization of ensembles. Models are trained on the ImageNet training split. M is the
ensemble size. For Naive DivDis and A2D, we use the ImageNet-R as the OOD datasets where the respective
diversification objectives are applied."
OOD GENERALIZATION,0.4074505238649593,"m⋆:= arg maxm Acc(f m, Dood). (2) Prediction ensemble is a vanilla prediction ensemble where
292"
OOD GENERALIZATION,0.4086146682188591,"the logit values are averaged:
1
M
P"
OOD GENERALIZATION,0.409778812572759,"m f m(x) [38]. (3) Uniform soup [38] averages the weights
293"
OOD GENERALIZATION,0.4109429569266589,themselves. Final prediction is given by f(x; 1
OOD GENERALIZATION,0.4121071012805588,"M
P
m θm).
294"
OOD GENERALIZATION,0.41327124563445866,"SED improves OOD generalization for ensembles. We show the OOD generalization performances
295"
OOD GENERALIZATION,0.41443538998835855,"of ensembles in Table 2, for the three ensemble prediction aggregation strategies described above. We
296"
OOD GENERALIZATION,0.41559953434225844,"observe that our SED framework (SED-A2D) results in superior OOD generalization performances
297"
OOD GENERALIZATION,0.4167636786961583,"for all three strategies. SED-A2D is particularly strong in prediction ensemble (e.g. 48.1% for M = 5
298"
OOD GENERALIZATION,0.4179278230500582,"and 54.4% for M = 50 on ImageNet-R) and uniform soup (e.g. 46.1% for M = 5 and 46.5%
299"
OOD GENERALIZATION,0.4190919674039581,"for M = 50 on ImageNet-R). We contend that the increased ensemble diversity contributes to the
300"
OOD GENERALIZATION,0.420256111757858,"improvements in OOD generalization. We also remark that the SED framework (SED-A2D) envelops
301"
OOD GENERALIZATION,0.4214202561117579,"the performance of Naive A2D in this ImageNet-scale experiment. Together with the superiority of
302"
OOD GENERALIZATION,0.42258440046565776,"computational efficiency (as discussed at the end of § 4.4) of SED-A2D over the Naive A2D, this
303"
OOD GENERALIZATION,0.42374854481955765,"demonstrates that SED fulfills its purpose of scaling up ensemble diversification methods like A2D.
304"
OOD GENERALIZATION,0.42491268917345754,"Deep ensemble is a strong baseline. We also note that deep ensemble, particularly with diverse
305"
OOD GENERALIZATION,0.42607683352735737,"hyperparameters, provides a strong baseline, outperforming dedicated diversification methodologies
306"
OOD GENERALIZATION,0.42724097788125726,"under the oracle selection strategy when M = 5. It also provides a good balance between ID
307"
OOD GENERALIZATION,0.42840512223515714,"(ImageNet validation split) and OOD generalization.
308"
OOD DETECTION,0.42956926658905703,"4.4
OOD Detection
309"
OOD DETECTION,0.4307334109429569,"Method
η
C-1
C-5
iNat
OI"
OOD DETECTION,0.4318975552968568,"Single model
BMA
0.615
0.833
0.958
0.909"
OOD DETECTION,0.4330616996507567,"Deep Ensemble
BMA
0.619
0.835
0.958
0.911
+Diverse HPs
BMA
0.642
0.861
0.969
0.923
Naive DivDis
BMA
0.598
0.843
0.966
0.922
Naive A2D
BMA
0.594
0.835
0.966
0.916
SED-A2D
BMA
0.641
0.845
0.960
0.915"
OOD DETECTION,0.4342258440046566,"Deep Ensemble
PDS
0.565
0.625
0.592
0.589
+Diverse HPs
PDS
0.643
0.849
0.926
0.889
Naive DivDis
PDS
0.600
0.851
0.969
0.939
Naive A2D
PDS
0.599
0.850
0.971
0.939
SED-A2D
PDS
0.686
0.896
0.977
0.941"
OOD DETECTION,0.43538998835855647,"Table 3: OOD detection via ensembles. For each OOD
dataset (C-1, C-5, iNat, and OI), the ensembles are tasked
to detect the respective OOD samples among ID samples
(ImageNet validation split). We show the AUROC scores for
the OOD detection task. Ensemble size is fixed at M = 5.
η refers to the epistemic uncertainty computation framework
discussed in §3.3."
OOD DETECTION,0.43655413271245636,"We study the impact of ensemble diversifi-
310"
OOD DETECTION,0.43771827706635624,"cation on OOD detection capabilities of an
311"
OOD DETECTION,0.43888242142025613,"ensemble. Once an ensemble is trained, we
312"
OOD DETECTION,0.440046565774156,"compute the epistemic uncertainty, or like-
313"
OOD DETECTION,0.4412107101280559,"lihood of the sample being OOD, following
314"
OOD DETECTION,0.44237485448195574,"two schemes, ηBMA and ηPDS introduced in
315"
OOD DETECTION,0.4435389988358556,"§3.3.
316"
OOD DETECTION,0.4447031431897555,"SED and PDS together lead to superior
317"
OOD DETECTION,0.4458672875436554,"OOD detection performances. We show
318"
OOD DETECTION,0.4470314318975553,"the OOD detection results in Table 3. For
319"
OOD DETECTION,0.4481955762514552,"the BMA scores, deep ensemble remains a
320"
OOD DETECTION,0.44935972060535506,"strong baseline. In particular, when the hy-
321"
OOD DETECTION,0.45052386495925495,"perparameters are varied (“+Diverse HPs”),
322"
OOD DETECTION,0.45168800931315484,"the detection AUROC reaches the maximal
323"
OOD DETECTION,0.4528521536670547,"performances among the ensembles using
324"
OOD DETECTION,0.4540162980209546,"the BMA scores. The quality of PDS is
325"
OOD DETECTION,0.4551804423748545,"more sensitive to the ensemble diversity, as
326"
OOD DETECTION,0.4563445867287544,"seen in the jump from the deep ensemble
327"
OOD DETECTION,0.4575087310826543,"(e.g. 0.589 for OI) to the diverse-HP vari-
328"
OOD DETECTION,0.4586728754365541,"ant (0.889). However, when the ensemble
329"
OOD DETECTION,0.459837019790454,"is sufficiently diverse, such as when trained
330"
OOD DETECTION,0.4610011641443539,"with SED-A2D, the PDS leads to high-quality OODness scores. SED-A2D with PDS achieves the
331"
OOD DETECTION,0.46216530849825377,"best AUROC across the board, including the BMA variants.
332"
OOD DETECTION,0.46332945285215366,"Figure 3: Impact of diversity regulariser on OOD detection. We show the model answer diversity, measured
by PDS, and the OOD detection performance, measured by AUROC, against λ values, the loss weight for the
disagreement regularizer term."
OOD DETECTION,0.46449359720605354,"Impact of diversification parameter λ. We further study the impact of ensemble diversification
333"
OOD DETECTION,0.46565774155995343,"on the OOD detection with the PDS estimator. In Figure 3, we observe that strengthening the
334"
OOD DETECTION,0.4668218859138533,"diversification objective (higher λ) indeed leads to greater diversity (higher PDS), with a jump at
335"
OOD DETECTION,0.4679860302677532,"around λ ∈[10−1, 101]. This range corresponds to the jump in the OOD detection performance
336"
OOD DETECTION,0.4691501746216531,"(higher AUROC).
337"
OOD DETECTION,0.470314318975553,Figure 4: Impact of ensemble size on OOD detection.
OOD DETECTION,0.47147846332945287,"Influence of ensemble size. How ensemble size
338"
OOD DETECTION,0.47264260768335276,"influences performance of our method? We can
339"
OOD DETECTION,0.47380675203725264,"see that increasing ensemble size helps to im-
340"
OOD DETECTION,0.47497089639115253,"prove AUROC for OOD detection on C-1 (Fig-
341"
OOD DETECTION,0.47613504074505236,"ure 4).
Increasing ensemble size marginally
342"
OOD DETECTION,0.47729918509895225,"helps, but using 5 models provides already a
343"
OOD DETECTION,0.47846332945285214,"significant improvement over the smallest pos-
344"
OOD DETECTION,0.479627473806752,"sible ensemble of size 2. It is also important to
345"
OOD DETECTION,0.4807916181606519,"mention, that SED framework is computationally
346"
OOD DETECTION,0.4819557625145518,"more efficient w.r.t. ensemble size M than Naive
347"
OOD DETECTION,0.4831199068684517,"A2D and Naive DivDis: since we train ensembles for the fixed number of epochs, training complexity
348"
OOD DETECTION,0.4842840512223516,"for SED is O(1) thanks to stochastic model pairs selection, while for Naive A2D and Naive DivDis it
349"
OOD DETECTION,0.48544819557625146,"is O(M 2).
350"
CONCLUSION,0.48661233993015135,"5
Conclusion
351"
CONCLUSION,0.48777648428405124,"Ensemble diversification has many implications for treating one of the ultimate goals of machine learn-
352"
CONCLUSION,0.4889406286379511,"ing, handling out-of-distribution (OOD) samples. By training a large number of plausible hypotheses
353"
CONCLUSION,0.490104772991851,"on an in-distribution (ID) dataset, an OOD-generalizable hypothesis may appear. Moreover, the
354"
CONCLUSION,0.4912689173457509,"diversity of hypotheses lets us distinguish ID samples from OOD samples by measuring the degree of
355"
CONCLUSION,0.49243306169965073,"divergence in ensemble members’ predictions. Despite conceptual benefits, diverse-ensemble training
356"
CONCLUSION,0.4935972060535506,"has previously remained a lab-bound concept for several reasons. First, previous approaches required
357"
CONCLUSION,0.4947613504074505,"a separate OOD dataset that may nurture diverse hypotheses. Second, computational complexities of
358"
CONCLUSION,0.4959254947613504,"previous pairwise diversification objectives increase quadratically with the ensemble size.
359"
CONCLUSION,0.4970896391152503,"We have addressed the challenges through the novel Scalable Ensemble Diversification (SED)
360"
CONCLUSION,0.49825378346915017,"framework. SED identifies the OOD-like samples from a single dataset, bypassing the need to
361"
CONCLUSION,0.49941792782305006,"prepare a separate OOD dataset. SED also employs a stochastic pair selection algorithm which
362"
CONCLUSION,0.5005820721769499,"reduces the quadratic complexity of previous approaches to a constant cost per SGD iteration. We
363"
CONCLUSION,0.5017462165308498,"have demonstrated good performances by SED on the OOD generalization and detection tasks, both
364"
CONCLUSION,0.5029103608847497,"at the ImageNet scale, a largely underexplored regime in the ensemble diversification community.
365"
CONCLUSION,0.5040745052386496,"In particular, for OOD detection, our novel diversity measure of Predictive Diversity Score (PDS)
366"
CONCLUSION,0.5052386495925495,"amplifies the benefits of diverse ensembles for OOD detection. The code to reproduce the results of
367"
CONCLUSION,0.5064027939464494,"our experiments will provided with the next revision of the manuscript.
368"
CONCLUSION,0.5075669383003493,"Limitations
369"
CONCLUSION,0.5087310826542492,"We do not provide theoretical justification for the method. Our experiments were conducted on
370"
CONCLUSION,0.509895227008149,"models with a frozen feature extractor.
371"
REFERENCES,0.5110593713620489,"References
372"
REFERENCES,0.5122235157159488,"[1] H. L. Benoit, L. Jiang, A. Atanov, O. F. Kar, M. Rigotti, and A. Zamir. Unraveling the key compo-
373"
REFERENCES,0.5133876600698487,"nents of OOD generalization via diversification. In The Twelfth International Conference on Learning
374"
REFERENCES,0.5145518044237486,"Representations, 2024. URL https://openreview.net/forum?id=Lvf7GnaLru.
375"
REFERENCES,0.5157159487776485,"[2] L. Breiman. Bagging predictors. Machine Learning, 24(2):123–140, Aug 1996. ISSN 1573-0565. doi:
376"
REFERENCES,0.5168800931315483,"10.1007/BF00058655. URL https://doi.org/10.1007/BF00058655.
377"
REFERENCES,0.5180442374854481,"[3] L. Breiman. Random forests. Machine Learning, 45(1):5–32, Oct 2001. ISSN 1573-0565. doi: 10.1023/A:
378"
REFERENCES,0.519208381839348,"1010933404324. URL https://doi.org/10.1023/A:1010933404324.
379"
REFERENCES,0.5203725261932479,"[4] A. S. Chen, Y. Lee, A. Setlur, S. Levine, and C. Finn. Project and probe: Sample-efficient domain
380"
REFERENCES,0.5215366705471478,"adaptation by interpolating orthogonal features. arXiv preprint arXiv:2302.05441, 2023.
381"
REFERENCES,0.5227008149010477,"[5] F. D’Angelo and V. Fortuin. Repulsive deep ensembles are bayesian. Advances in Neural Information
382"
REFERENCES,0.5238649592549476,"Processing Systems, 34:3451–3465, 2021.
383"
REFERENCES,0.5250291036088475,"[6] A. de Mathelin, F. Deheeger, M. Mougeot, and N. Vayatis. Maximum weight entropy. arXiv preprint
384"
REFERENCES,0.5261932479627474,"arXiv:2309.15704, 2023.
385"
REFERENCES,0.5273573923166472,"[7] A. de Mathelin, F. Deheeger, M. Mougeot, and N. Vayatis. Deep anti-regularized ensembles provide
386"
REFERENCES,0.5285215366705471,"reliable out-of-distribution uncertainty quantification, 2023.
387"
REFERENCES,0.529685681024447,"[8] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image
388"
REFERENCES,0.5308498253783469,"database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248–255, 2009.
389"
REFERENCES,0.5320139697322468,"doi: 10.1109/CVPR.2009.5206848.
390"
REFERENCES,0.5331781140861467,"[9] N. Durasov, T. Bagautdinov, P. Baque, and P. Fua. Masksembles for uncertainty estimation. In Proceedings
391"
REFERENCES,0.5343422584400466,"of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13539–13548, 2021.
392"
REFERENCES,0.5355064027939465,"[10] Y. Gal and Z. Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep
393"
REFERENCES,0.5366705471478463,"learning. In international conference on machine learning, pages 1050–1059. PMLR, 2016.
394"
REFERENCES,0.5378346915017462,"[11] I. Goodfellow, Y. Bengio, and A. Courville.
Deep Learning.
MIT Press, 2016.
http://www.
395"
REFERENCES,0.5389988358556461,"deeplearningbook.org.
396"
REFERENCES,0.540162980209546,"[12] L. Hansen and P. Salamon. Neural network ensembles. IEEE Transactions on Pattern Analysis and
397"
REFERENCES,0.5413271245634459,"Machine Intelligence, 12(10):993–1001, 1990. doi: 10.1109/34.58871.
398"
REFERENCES,0.5424912689173458,"[13] J. C. Helton, J. D. Johnson, and W. L. Oberkampf. An exploration of alternative approaches to the
399"
REFERENCES,0.5436554132712457,"representation of uncertainty in model predictions. Reliability Engineering & System Safety, 85(1-3):
400"
REFERENCES,0.5448195576251456,"39–71, 2004.
401"
REFERENCES,0.5459837019790454,"[14] D. Hendrycks and T. Dietterich.
Benchmarking neural network robustness to common corruptions
402"
REFERENCES,0.5471478463329453,"and perturbations. In International Conference on Learning Representations, 2019. URL https://
403"
REFERENCES,0.5483119906868452,"openreview.net/forum?id=HJz6tiCqYm.
404"
REFERENCES,0.5494761350407451,"[15] D. Hendrycks and K. Gimpel. A baseline for detecting misclassified and out-of-distribution examples
405"
REFERENCES,0.5506402793946449,"in neural networks. In International Conference on Learning Representations, 2017. URL https:
406"
REFERENCES,0.5518044237485448,"//openreview.net/forum?id=Hkg4TI9xl.
407"
REFERENCES,0.5529685681024447,"[16] D. Hendrycks, S. Basart, N. Mu, S. Kadavath, F. Wang, E. Dorundo, R. Desai, T. Zhu, S. Parajuli, M. Guo,
408"
REFERENCES,0.5541327124563445,"et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings
409"
REFERENCES,0.5552968568102444,"of the IEEE/CVF international conference on computer vision, pages 8340–8349, 2021.
410"
REFERENCES,0.5564610011641443,"[17] D. Hendrycks, K. Zhao, S. Basart, J. Steinhardt, and D. Song. Natural adversarial examples. In Proceedings
411"
REFERENCES,0.5576251455180442,"of the IEEE/CVF conference on computer vision and pattern recognition, pages 15262–15271, 2021.
412"
REFERENCES,0.5587892898719441,"[18] T. K. Ho. Random decision forests. In Proceedings of 3rd International Conference on Document Analysis
413"
REFERENCES,0.559953434225844,"and Recognition, volume 1, pages 278–282 vol.1, 1995. doi: 10.1109/ICDAR.1995.598994.
414"
REFERENCES,0.5611175785797439,"[19] G. Huang, Y. Li, G. Pleiss, Z. Liu, J. E. Hopcroft, and K. Q. Weinberger. Snapshot ensembles: Train 1, get
415"
REFERENCES,0.5622817229336438,"m for free. arXiv preprint arXiv:1704.00109, 2017.
416"
REFERENCES,0.5634458672875436,"[20] R. Huang and Y. Li. Mos: Towards scaling out-of-distribution detection for large semantic space. In
417"
REFERENCES,0.5646100116414435,"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8710–8719,
418"
REFERENCES,0.5657741559953434,"2021.
419"
REFERENCES,0.5669383003492433,"[21] A. Krogh and J. Vedelsby.
Neural network ensembles, cross validation, and active learning.
In
420"
REFERENCES,0.5681024447031432,"G. Tesauro, D. Touretzky, and T. Leen, editors, Advances in Neural Information Processing Systems,
421"
REFERENCES,0.5692665890570431,"volume 7. MIT Press, 1994. URL https://proceedings.neurips.cc/paper_files/paper/1994/
422"
REFERENCES,0.570430733410943,"file/b8c37e33defde51cf91e1e03e51657da-Paper.pdf.
423"
REFERENCES,0.5715948777648429,"[22] B. Lakshminarayanan, A. Pritzel, and C. Blundell. Simple and scalable predictive uncertainty estimation
424"
REFERENCES,0.5727590221187427,"using deep ensembles. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,
425"
REFERENCES,0.5739231664726426,"and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran As-
426"
REFERENCES,0.5750873108265425,"sociates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/
427"
REFERENCES,0.5762514551804424,"9ef2ed4b7fd2c810847ffa5fa85bce38-Paper.pdf.
428"
REFERENCES,0.5774155995343423,"[23] Y. Lee, H. Yao, and C. Finn.
Diversify and disambiguate: Out-of-distribution robustness via dis-
429"
REFERENCES,0.5785797438882422,"agreement.
In The Eleventh International Conference on Learning Representations, 2023.
URL
430"
REFERENCES,0.5797438882421421,"https://openreview.net/forum?id=RVTOp3MwT3n.
431"
REFERENCES,0.580908032596042,"[24] Z. Li, I. Evtimov, A. Gordo, C. Hazirbas, T. Hassner, C. C. Ferrer, C. Xu, and M. Ibrahim. A whac-a-mole
432"
REFERENCES,0.5820721769499418,"dilemma: Shortcuts come in multiples where mitigating one amplifies others. In Proceedings of the
433"
REFERENCES,0.5832363213038417,"IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20071–20082, 2023.
434"
REFERENCES,0.5844004656577415,"[25] Y. Liu and X. Yao. Simultaneous training of negatively correlated neural networks in an ensemble. IEEE
435"
REFERENCES,0.5855646100116414,"Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 29(6):716–725, 1999.
436"
REFERENCES,0.5867287543655413,"[26] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In International Conference on
437"
REFERENCES,0.5878928987194412,"Learning Representations, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7.
438"
REFERENCES,0.5890570430733411,"[27] Y. Ovadia, E. Fertig, J. Ren, Z. Nado, D. Sculley, S. Nowozin, J. Dillon, B. Lakshminarayanan, and
439"
REFERENCES,0.590221187427241,"J. Snoek. Can you trust your model’s uncertainty? evaluating predictive uncertainty under dataset shift.
440"
REFERENCES,0.5913853317811408,"Advances in neural information processing systems, 32, 2019.
441"
REFERENCES,0.5925494761350407,"[28] M. Pagliardini, M. Jaggi, F. Fleuret, and S. P. Karimireddy. Agree to disagree: Diversity through disagree-
442"
REFERENCES,0.5937136204889406,"ment for better transferability. In The Eleventh International Conference on Learning Representations,
443"
REFERENCES,0.5948777648428405,"2023. URL https://openreview.net/forum?id=K7CbYQbyYhY.
444"
REFERENCES,0.5960419091967404,"[29] A. Ross, W. Pan, L. Celi, and F. Doshi-Velez. Ensembles of locally independent prediction models. In
445"
REFERENCES,0.5972060535506403,"Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 5527–5536, 2020.
446"
REFERENCES,0.5983701979045402,"[30] D. Teney, E. Abbasnejad, S. Lucey, and A. van den Hengel. Evading the simplicity bias: Training a diverse
447"
REFERENCES,0.59953434225844,"set of models discovers solutions with superior ood generalization. In Proceedings of the IEEE/CVF
448"
REFERENCES,0.6006984866123399,"Conference on Computer Vision and Pattern Recognition (CVPR), pages 16761–16772, June 2022.
449"
REFERENCES,0.6018626309662398,"[31] D. Teney, M. Peyrard, and E. Abbasnejad. Predicting is not understanding: Recognizing and addressing
450"
REFERENCES,0.6030267753201397,"underspecification in machine learning. In S. Avidan, G. Brostow, M. Cissé, G. M. Farinella, and T. Hassner,
451"
REFERENCES,0.6041909196740396,"editors, Computer Vision – ECCV 2022, pages 458–476, Cham, 2022. Springer Nature Switzerland. ISBN
452"
REFERENCES,0.6053550640279395,"978-3-031-20050-2.
453"
REFERENCES,0.6065192083818394,"[32] H. Touvron, M. Cord, and H. Jégou. Deit iii: Revenge of the vit. In European conference on computer
454"
REFERENCES,0.6076833527357393,"vision, pages 516–533. Springer, 2022.
455"
REFERENCES,0.6088474970896391,"[33] T. Trinh, M. Heinonen, L. Acerbi, and S. Kaski. Input-gradient space particle inference for neural network
456"
REFERENCES,0.610011641443539,"ensembles. In International Conference on Learning Representations, 2024.
457"
REFERENCES,0.6111757857974389,"[34] H. Wang and Q. Ji. Diversity-enhanced probabilistic ensemble for uncertainty estimation. In R. J. Evans
458"
REFERENCES,0.6123399301513388,"and I. Shpitser, editors, Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence,
459"
REFERENCES,0.6135040745052387,"volume 216 of Proceedings of Machine Learning Research, pages 2214–2225. PMLR, 31 Jul–04 Aug
460"
REFERENCES,0.6146682188591386,"2023. URL https://proceedings.mlr.press/v216/wang23c.html.
461"
REFERENCES,0.6158323632130385,"[35] H. Wang, Z. Li, L. Feng, and W. Zhang. Vim: Out-of-distribution with virtual-logit matching. In
462"
REFERENCES,0.6169965075669382,"Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4921–4930,
463"
REFERENCES,0.6181606519208381,"2022.
464"
REFERENCES,0.619324796274738,"[36] F. Wenzel, J. Snoek, D. Tran, and R. Jenatton. Hyperparameter ensembles for robustness and uncertainty
465"
REFERENCES,0.6204889406286379,"quantification. Advances in Neural Information Processing Systems, 33:6514–6527, 2020.
466"
REFERENCES,0.6216530849825378,"[37] A. G. Wilson and P. Izmailov. Bayesian deep learning and a probabilistic perspective of generalization.
467"
REFERENCES,0.6228172293364377,"Advances in neural information processing systems, 33:4697–4708, 2020.
468"
REFERENCES,0.6239813736903376,"[38] M. Wortsman, G. Ilharco, S. Y. Gadre, R. Roelofs, R. Gontijo-Lopes, A. S. Morcos, H. Namkoong,
469"
REFERENCES,0.6251455180442375,"A. Farhadi, Y. Carmon, S. Kornblith, and L. Schmidt. Model soups: averaging weights of multiple
470"
REFERENCES,0.6263096623981373,"fine-tuned models improves accuracy without increasing inference time. In K. Chaudhuri, S. Jegelka,
471"
REFERENCES,0.6274738067520372,"L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors, Proceedings of the 39th International Conference
472"
REFERENCES,0.6286379511059371,"on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 23965–23998.
473"
REFERENCES,0.629802095459837,"PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/wortsman22a.html.
474"
REFERENCES,0.6309662398137369,"[39] S. Yashima, T. Suzuki, K. Ishikawa, I. Sato, and R. Kawakami. Feature space particle inference for neural
475"
REFERENCES,0.6321303841676368,"network ensembles. In International Conference on Machine Learning, pages 25452–25468. PMLR, 2022.
476"
REFERENCES,0.6332945285215367,"[40] S. Zaidi, A. Zela, T. Elsken, C. C. Holmes, F. Hutter, and Y. Teh. Neural ensemble search for uncertainty
477"
REFERENCES,0.6344586728754366,"estimation and dataset shift. Advances in Neural Information Processing Systems, 34:7898–7911, 2021.
478"
REFERENCES,0.6356228172293364,"[41] T. Zhou, S. Wang, and J. A. Bilmes.
Diverse ensemble evolution: Curriculum data-model mar-
479"
REFERENCES,0.6367869615832363,"riage.
In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Gar-
480"
REFERENCES,0.6379511059371362,"nett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Asso-
481"
REFERENCES,0.6391152502910361,"ciates, Inc., 2018.
URL https://proceedings.neurips.cc/paper_files/paper/2018/file/
482"
REFERENCES,0.640279394644936,"3070e6addcd702cb58de5d7897bfdae1-Paper.pdf.
483"
REFERENCES,0.6414435389988359,"NeurIPS Paper Checklist
484"
CLAIMS,0.6426076833527358,"1. Claims
485"
CLAIMS,0.6437718277066357,"Question: Do the main claims made in the abstract and introduction accurately reflect the
486"
CLAIMS,0.6449359720605355,"paper’s contributions and scope?
487"
CLAIMS,0.6461001164144354,"Answer: [Yes]
488"
CLAIMS,0.6472642607683353,"Justification: Please refer to § 4
489"
CLAIMS,0.6484284051222352,"Guidelines:
490"
CLAIMS,0.6495925494761351,"• The answer NA means that the abstract and introduction do not include the claims
491"
CLAIMS,0.6507566938300349,"made in the paper.
492"
CLAIMS,0.6519208381839348,"• The abstract and/or introduction should clearly state the claims made, including the
493"
CLAIMS,0.6530849825378346,"contributions made in the paper and important assumptions and limitations. A No or
494"
CLAIMS,0.6542491268917345,"NA answer to this question will not be perceived well by the reviewers.
495"
CLAIMS,0.6554132712456344,"• The claims made should match theoretical and experimental results, and reflect how
496"
CLAIMS,0.6565774155995343,"much the results can be expected to generalize to other settings.
497"
CLAIMS,0.6577415599534342,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
498"
CLAIMS,0.6589057043073341,"are not attained by the paper.
499"
LIMITATIONS,0.660069848661234,"2. Limitations
500"
LIMITATIONS,0.6612339930151339,"Question: Does the paper discuss the limitations of the work performed by the authors?
501"
LIMITATIONS,0.6623981373690337,"Answer: [Yes]
502"
LIMITATIONS,0.6635622817229336,"Justification: Please refer to § 5
503"
LIMITATIONS,0.6647264260768335,"Guidelines:
504"
LIMITATIONS,0.6658905704307334,"• The answer NA means that the paper has no limitation while the answer No means that
505"
LIMITATIONS,0.6670547147846333,"the paper has limitations, but those are not discussed in the paper.
506"
LIMITATIONS,0.6682188591385332,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
507"
LIMITATIONS,0.6693830034924331,"• The paper should point out any strong assumptions and how robust the results are to
508"
LIMITATIONS,0.670547147846333,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
509"
LIMITATIONS,0.6717112922002328,"model well-specification, asymptotic approximations only holding locally). The authors
510"
LIMITATIONS,0.6728754365541327,"should reflect on how these assumptions might be violated in practice and what the
511"
LIMITATIONS,0.6740395809080326,"implications would be.
512"
LIMITATIONS,0.6752037252619325,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
513"
LIMITATIONS,0.6763678696158324,"only tested on a few datasets or with a few runs. In general, empirical results often
514"
LIMITATIONS,0.6775320139697323,"depend on implicit assumptions, which should be articulated.
515"
LIMITATIONS,0.6786961583236322,"• The authors should reflect on the factors that influence the performance of the approach.
516"
LIMITATIONS,0.6798603026775321,"For example, a facial recognition algorithm may perform poorly when image resolution
517"
LIMITATIONS,0.681024447031432,"is low or images are taken in low lighting. Or a speech-to-text system might not be
518"
LIMITATIONS,0.6821885913853318,"used reliably to provide closed captions for online lectures because it fails to handle
519"
LIMITATIONS,0.6833527357392316,"technical jargon.
520"
LIMITATIONS,0.6845168800931315,"• The authors should discuss the computational efficiency of the proposed algorithms
521"
LIMITATIONS,0.6856810244470314,"and how they scale with dataset size.
522"
LIMITATIONS,0.6868451688009313,"• If applicable, the authors should discuss possible limitations of their approach to
523"
LIMITATIONS,0.6880093131548312,"address problems of privacy and fairness.
524"
LIMITATIONS,0.689173457508731,"• While the authors might fear that complete honesty about limitations might be used by
525"
LIMITATIONS,0.6903376018626309,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
526"
LIMITATIONS,0.6915017462165308,"limitations that aren’t acknowledged in the paper. The authors should use their best
527"
LIMITATIONS,0.6926658905704307,"judgment and recognize that individual actions in favor of transparency play an impor-
528"
LIMITATIONS,0.6938300349243306,"tant role in developing norms that preserve the integrity of the community. Reviewers
529"
LIMITATIONS,0.6949941792782305,"will be specifically instructed to not penalize honesty concerning limitations.
530"
THEORY ASSUMPTIONS AND PROOFS,0.6961583236321304,"3. Theory Assumptions and Proofs
531"
THEORY ASSUMPTIONS AND PROOFS,0.6973224679860303,"Question: For each theoretical result, does the paper provide the full set of assumptions and
532"
THEORY ASSUMPTIONS AND PROOFS,0.6984866123399301,"a complete (and correct) proof?
533"
THEORY ASSUMPTIONS AND PROOFS,0.69965075669383,"Answer: [NA]
534"
THEORY ASSUMPTIONS AND PROOFS,0.7008149010477299,"Justification: The paper contains no theoretical results.
535"
THEORY ASSUMPTIONS AND PROOFS,0.7019790454016298,"Guidelines:
536"
THEORY ASSUMPTIONS AND PROOFS,0.7031431897555297,"• The answer NA means that the paper does not include theoretical results.
537"
THEORY ASSUMPTIONS AND PROOFS,0.7043073341094296,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
538"
THEORY ASSUMPTIONS AND PROOFS,0.7054714784633295,"referenced.
539"
THEORY ASSUMPTIONS AND PROOFS,0.7066356228172294,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
540"
THEORY ASSUMPTIONS AND PROOFS,0.7077997671711292,"• The proofs can either appear in the main paper or the supplemental material, but if
541"
THEORY ASSUMPTIONS AND PROOFS,0.7089639115250291,"they appear in the supplemental material, the authors are encouraged to provide a short
542"
THEORY ASSUMPTIONS AND PROOFS,0.710128055878929,"proof sketch to provide intuition.
543"
THEORY ASSUMPTIONS AND PROOFS,0.7112922002328289,"• Inversely, any informal proof provided in the core of the paper should be complemented
544"
THEORY ASSUMPTIONS AND PROOFS,0.7124563445867288,"by formal proofs provided in appendix or supplemental material.
545"
THEORY ASSUMPTIONS AND PROOFS,0.7136204889406287,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
546"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7147846332945286,"4. Experimental Result Reproducibility
547"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7159487776484285,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
548"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7171129220023282,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
549"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7182770663562281,"of the paper (regardless of whether the code and data are provided or not)?
550"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.719441210710128,"Answer: [Yes]
551"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7206053550640279,"Justification: Please refer to § 4
552"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7217694994179278,"Guidelines:
553"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7229336437718277,"• The answer NA means that the paper does not include experiments.
554"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7240977881257276,"• If the paper includes experiments, a No answer to this question will not be perceived
555"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7252619324796274,"well by the reviewers: Making the paper reproducible is important, regardless of
556"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7264260768335273,"whether the code and data are provided or not.
557"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7275902211874272,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
558"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7287543655413271,"to make their results reproducible or verifiable.
559"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.729918509895227,"• Depending on the contribution, reproducibility can be accomplished in various ways.
560"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7310826542491269,"For example, if the contribution is a novel architecture, describing the architecture fully
561"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7322467986030268,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
562"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7334109429569267,"be necessary to either make it possible for others to replicate the model with the same
563"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7345750873108265,"dataset, or provide access to the model. In general. releasing code and data is often
564"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7357392316647264,"one good way to accomplish this, but reproducibility can also be provided via detailed
565"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7369033760186263,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
566"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7380675203725262,"of a large language model), releasing of a model checkpoint, or other means that are
567"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7392316647264261,"appropriate to the research performed.
568"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.740395809080326,"• While NeurIPS does not require releasing code, the conference does require all submis-
569"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7415599534342259,"sions to provide some reasonable avenue for reproducibility, which may depend on the
570"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7427240977881258,"nature of the contribution. For example
571"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7438882421420256,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
572"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7450523864959255,"to reproduce that algorithm.
573"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7462165308498254,"(b) If the contribution is primarily a new model architecture, the paper should describe
574"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7473806752037253,"the architecture clearly and fully.
575"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7485448195576252,"(c) If the contribution is a new model (e.g., a large language model), then there should
576"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7497089639115251,"either be a way to access this model for reproducing the results or a way to reproduce
577"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7508731082654249,"the model (e.g., with an open-source dataset or instructions for how to construct
578"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7520372526193247,"the dataset).
579"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7532013969732246,"(d) We recognize that reproducibility may be tricky in some cases, in which case
580"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7543655413271245,"authors are welcome to describe the particular way they provide for reproducibility.
581"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7555296856810244,"In the case of closed-source models, it may be that access to the model is limited in
582"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7566938300349243,"some way (e.g., to registered users), but it should be possible for other researchers
583"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7578579743888242,"to have some path to reproducing or verifying the results.
584"
OPEN ACCESS TO DATA AND CODE,0.7590221187427241,"5. Open access to data and code
585"
OPEN ACCESS TO DATA AND CODE,0.760186263096624,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
586"
OPEN ACCESS TO DATA AND CODE,0.7613504074505238,"tions to faithfully reproduce the main experimental results, as described in supplemental
587"
OPEN ACCESS TO DATA AND CODE,0.7625145518044237,"material?
588"
OPEN ACCESS TO DATA AND CODE,0.7636786961583236,"Answer: [Yes]
589"
OPEN ACCESS TO DATA AND CODE,0.7648428405122235,"Justification: Code will be available soon, please refer to § 4.1.
590"
OPEN ACCESS TO DATA AND CODE,0.7660069848661234,"Guidelines:
591"
OPEN ACCESS TO DATA AND CODE,0.7671711292200233,"• The answer NA means that paper does not include experiments requiring code.
592"
OPEN ACCESS TO DATA AND CODE,0.7683352735739232,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
593"
OPEN ACCESS TO DATA AND CODE,0.7694994179278231,"public/guides/CodeSubmissionPolicy) for more details.
594"
OPEN ACCESS TO DATA AND CODE,0.770663562281723,"• While we encourage the release of code and data, we understand that this might not be
595"
OPEN ACCESS TO DATA AND CODE,0.7718277066356228,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
596"
OPEN ACCESS TO DATA AND CODE,0.7729918509895227,"including code, unless this is central to the contribution (e.g., for a new open-source
597"
OPEN ACCESS TO DATA AND CODE,0.7741559953434226,"benchmark).
598"
OPEN ACCESS TO DATA AND CODE,0.7753201396973225,"• The instructions should contain the exact command and environment needed to run to
599"
OPEN ACCESS TO DATA AND CODE,0.7764842840512224,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
600"
OPEN ACCESS TO DATA AND CODE,0.7776484284051223,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
601"
OPEN ACCESS TO DATA AND CODE,0.7788125727590222,"• The authors should provide instructions on data access and preparation, including how
602"
OPEN ACCESS TO DATA AND CODE,0.779976717112922,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
603"
OPEN ACCESS TO DATA AND CODE,0.7811408614668219,"• The authors should provide scripts to reproduce all experimental results for the new
604"
OPEN ACCESS TO DATA AND CODE,0.7823050058207218,"proposed method and baselines. If only a subset of experiments are reproducible, they
605"
OPEN ACCESS TO DATA AND CODE,0.7834691501746216,"should state which ones are omitted from the script and why.
606"
OPEN ACCESS TO DATA AND CODE,0.7846332945285215,"• At submission time, to preserve anonymity, the authors should release anonymized
607"
OPEN ACCESS TO DATA AND CODE,0.7857974388824214,"versions (if applicable).
608"
OPEN ACCESS TO DATA AND CODE,0.7869615832363213,"• Providing as much information as possible in supplemental material (appended to the
609"
OPEN ACCESS TO DATA AND CODE,0.7881257275902211,"paper) is recommended, but including URLs to data and code is permitted.
610"
OPEN ACCESS TO DATA AND CODE,0.789289871944121,"6. Experimental Setting/Details
611"
OPEN ACCESS TO DATA AND CODE,0.7904540162980209,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
612"
OPEN ACCESS TO DATA AND CODE,0.7916181606519208,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
613"
OPEN ACCESS TO DATA AND CODE,0.7927823050058207,"results?
614"
OPEN ACCESS TO DATA AND CODE,0.7939464493597206,"Answer: [Yes]
615"
OPEN ACCESS TO DATA AND CODE,0.7951105937136205,"Justification: please refer to § 4.1.
616"
OPEN ACCESS TO DATA AND CODE,0.7962747380675204,"Guidelines:
617"
OPEN ACCESS TO DATA AND CODE,0.7974388824214202,"• The answer NA means that the paper does not include experiments.
618"
OPEN ACCESS TO DATA AND CODE,0.7986030267753201,"• The experimental setting should be presented in the core of the paper to a level of detail
619"
OPEN ACCESS TO DATA AND CODE,0.79976717112922,"that is necessary to appreciate the results and make sense of them.
620"
OPEN ACCESS TO DATA AND CODE,0.8009313154831199,"• The full details can be provided either with the code, in appendix, or as supplemental
621"
OPEN ACCESS TO DATA AND CODE,0.8020954598370198,"material.
622"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8032596041909197,"7. Experiment Statistical Significance
623"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8044237485448196,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
624"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8055878928987195,"information about the statistical significance of the experiments?
625"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8067520372526193,"Answer: [No]
626"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8079161816065192,"Justification: Error bars are not reported because their magnitude was below the rounding
627"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8090803259604191,"error or roughly around it for the majority of experiments.
628"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.810244470314319,"Guidelines:
629"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8114086146682189,"• The answer NA means that the paper does not include experiments.
630"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8125727590221188,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
631"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8137369033760187,"dence intervals, or statistical significance tests, at least for the experiments that support
632"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8149010477299186,"the main claims of the paper.
633"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8160651920838184,"• The factors of variability that the error bars are capturing should be clearly stated (for
634"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8172293364377182,"example, train/test split, initialization, random drawing of some parameter, or overall
635"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8183934807916181,"run with given experimental conditions).
636"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.819557625145518,"• The method for calculating the error bars should be explained (closed form formula,
637"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8207217694994179,"call to a library function, bootstrap, etc.)
638"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8218859138533178,"• The assumptions made should be given (e.g., Normally distributed errors).
639"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8230500582072177,"• It should be clear whether the error bar is the standard deviation or the standard error
640"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8242142025611175,"of the mean.
641"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8253783469150174,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
642"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8265424912689173,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
643"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8277066356228172,"of Normality of errors is not verified.
644"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8288707799767171,"• For asymmetric distributions, the authors should be careful not to show in tables or
645"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.830034924330617,"figures symmetric error bars that would yield results that are out of range (e.g. negative
646"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8311990686845169,"error rates).
647"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8323632130384168,"• If error bars are reported in tables or plots, The authors should explain in the text how
648"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8335273573923166,"they were calculated and reference the corresponding figures or tables in the text.
649"
EXPERIMENTS COMPUTE RESOURCES,0.8346915017462165,"8. Experiments Compute Resources
650"
EXPERIMENTS COMPUTE RESOURCES,0.8358556461001164,"Question: For each experiment, does the paper provide sufficient information on the com-
651"
EXPERIMENTS COMPUTE RESOURCES,0.8370197904540163,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
652"
EXPERIMENTS COMPUTE RESOURCES,0.8381839348079162,"the experiments?
653"
EXPERIMENTS COMPUTE RESOURCES,0.8393480791618161,"Answer: [Yes]
654"
EXPERIMENTS COMPUTE RESOURCES,0.840512223515716,"Justification: please refer to § 4.1.
655"
EXPERIMENTS COMPUTE RESOURCES,0.8416763678696159,"Guidelines:
656"
EXPERIMENTS COMPUTE RESOURCES,0.8428405122235157,"• The answer NA means that the paper does not include experiments.
657"
EXPERIMENTS COMPUTE RESOURCES,0.8440046565774156,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
658"
EXPERIMENTS COMPUTE RESOURCES,0.8451688009313155,"or cloud provider, including relevant memory and storage.
659"
EXPERIMENTS COMPUTE RESOURCES,0.8463329452852154,"• The paper should provide the amount of compute required for each of the individual
660"
EXPERIMENTS COMPUTE RESOURCES,0.8474970896391153,"experimental runs as well as estimate the total compute.
661"
EXPERIMENTS COMPUTE RESOURCES,0.8486612339930152,"• The paper should disclose whether the full research project required more compute
662"
EXPERIMENTS COMPUTE RESOURCES,0.8498253783469151,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
663"
EXPERIMENTS COMPUTE RESOURCES,0.8509895227008148,"didn’t make it into the paper).
664"
CODE OF ETHICS,0.8521536670547147,"9. Code Of Ethics
665"
CODE OF ETHICS,0.8533178114086146,"Question: Does the research conducted in the paper conform, in every respect, with the
666"
CODE OF ETHICS,0.8544819557625145,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
667"
CODE OF ETHICS,0.8556461001164144,"Answer: [Yes]
668"
CODE OF ETHICS,0.8568102444703143,"Justification: we followed the Code to the best of our knowledge.
669"
CODE OF ETHICS,0.8579743888242142,"Guidelines:
670"
CODE OF ETHICS,0.8591385331781141,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
671"
CODE OF ETHICS,0.860302677532014,"• If the authors answer No, they should explain the special circumstances that require a
672"
CODE OF ETHICS,0.8614668218859138,"deviation from the Code of Ethics.
673"
CODE OF ETHICS,0.8626309662398137,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
674"
CODE OF ETHICS,0.8637951105937136,"eration due to laws or regulations in their jurisdiction).
675"
BROADER IMPACTS,0.8649592549476135,"10. Broader Impacts
676"
BROADER IMPACTS,0.8661233993015134,"Question: Does the paper discuss both potential positive societal impacts and negative
677"
BROADER IMPACTS,0.8672875436554133,"societal impacts of the work performed?
678"
BROADER IMPACTS,0.8684516880093132,"Answer: [NA]
679"
BROADER IMPACTS,0.869615832363213,"Justification: We believe that this work has no societal impact.
680"
BROADER IMPACTS,0.8707799767171129,"Guidelines:
681"
BROADER IMPACTS,0.8719441210710128,"• The answer NA means that there is no societal impact of the work performed.
682"
BROADER IMPACTS,0.8731082654249127,"• If the authors answer NA or No, they should explain why their work has no societal
683"
BROADER IMPACTS,0.8742724097788126,"impact or why the paper does not address societal impact.
684"
BROADER IMPACTS,0.8754365541327125,"• Examples of negative societal impacts include potential malicious or unintended uses
685"
BROADER IMPACTS,0.8766006984866124,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
686"
BROADER IMPACTS,0.8777648428405123,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
687"
BROADER IMPACTS,0.8789289871944121,"groups), privacy considerations, and security considerations.
688"
BROADER IMPACTS,0.880093131548312,"• The conference expects that many papers will be foundational research and not tied
689"
BROADER IMPACTS,0.8812572759022119,"to particular applications, let alone deployments. However, if there is a direct path to
690"
BROADER IMPACTS,0.8824214202561118,"any negative applications, the authors should point it out. For example, it is legitimate
691"
BROADER IMPACTS,0.8835855646100116,"to point out that an improvement in the quality of generative models could be used to
692"
BROADER IMPACTS,0.8847497089639115,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
693"
BROADER IMPACTS,0.8859138533178114,"that a generic algorithm for optimizing neural networks could enable people to train
694"
BROADER IMPACTS,0.8870779976717112,"models that generate Deepfakes faster.
695"
BROADER IMPACTS,0.8882421420256111,"• The authors should consider possible harms that could arise when the technology is
696"
BROADER IMPACTS,0.889406286379511,"being used as intended and functioning correctly, harms that could arise when the
697"
BROADER IMPACTS,0.8905704307334109,"technology is being used as intended but gives incorrect results, and harms following
698"
BROADER IMPACTS,0.8917345750873108,"from (intentional or unintentional) misuse of the technology.
699"
BROADER IMPACTS,0.8928987194412107,"• If there are negative societal impacts, the authors could also discuss possible mitigation
700"
BROADER IMPACTS,0.8940628637951106,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
701"
BROADER IMPACTS,0.8952270081490105,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
702"
BROADER IMPACTS,0.8963911525029103,"feedback over time, improving the efficiency and accessibility of ML).
703"
SAFEGUARDS,0.8975552968568102,"11. Safeguards
704"
SAFEGUARDS,0.8987194412107101,"Question: Does the paper describe safeguards that have been put in place for responsible
705"
SAFEGUARDS,0.89988358556461,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
706"
SAFEGUARDS,0.9010477299185099,"image generators, or scraped datasets)?
707"
SAFEGUARDS,0.9022118742724098,"Answer: [NA]
708"
SAFEGUARDS,0.9033760186263097,"Justification: We believe that our paper does not pose such risks as we train models for
709"
SAFEGUARDS,0.9045401629802096,"ImageNet classification.
710"
SAFEGUARDS,0.9057043073341094,"Guidelines:
711"
SAFEGUARDS,0.9068684516880093,"• The answer NA means that the paper poses no such risks.
712"
SAFEGUARDS,0.9080325960419092,"• Released models that have a high risk for misuse or dual-use should be released with
713"
SAFEGUARDS,0.9091967403958091,"necessary safeguards to allow for controlled use of the model, for example by requiring
714"
SAFEGUARDS,0.910360884749709,"that users adhere to usage guidelines or restrictions to access the model or implementing
715"
SAFEGUARDS,0.9115250291036089,"safety filters.
716"
SAFEGUARDS,0.9126891734575088,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
717"
SAFEGUARDS,0.9138533178114087,"should describe how they avoided releasing unsafe images.
718"
SAFEGUARDS,0.9150174621653085,"• We recognize that providing effective safeguards is challenging, and many papers do
719"
SAFEGUARDS,0.9161816065192084,"not require this, but we encourage authors to take this into account and make a best
720"
SAFEGUARDS,0.9173457508731082,"faith effort.
721"
LICENSES FOR EXISTING ASSETS,0.9185098952270081,"12. Licenses for existing assets
722"
LICENSES FOR EXISTING ASSETS,0.919674039580908,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
723"
LICENSES FOR EXISTING ASSETS,0.9208381839348079,"the paper, properly credited and are the license and terms of use explicitly mentioned and
724"
LICENSES FOR EXISTING ASSETS,0.9220023282887078,"properly respected?
725"
LICENSES FOR EXISTING ASSETS,0.9231664726426076,"Answer: [No]
726"
LICENSES FOR EXISTING ASSETS,0.9243306169965075,"Justification: we were unable to find the license for the dataset we used.
727"
LICENSES FOR EXISTING ASSETS,0.9254947613504074,"Guidelines:
728"
LICENSES FOR EXISTING ASSETS,0.9266589057043073,"• The answer NA means that the paper does not use existing assets.
729"
LICENSES FOR EXISTING ASSETS,0.9278230500582072,"• The authors should cite the original paper that produced the code package or dataset.
730"
LICENSES FOR EXISTING ASSETS,0.9289871944121071,"• The authors should state which version of the asset is used and, if possible, include a
731"
LICENSES FOR EXISTING ASSETS,0.930151338766007,"URL.
732"
LICENSES FOR EXISTING ASSETS,0.9313154831199069,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
733"
LICENSES FOR EXISTING ASSETS,0.9324796274738067,"• For scraped data from a particular source (e.g., website), the copyright and terms of
734"
LICENSES FOR EXISTING ASSETS,0.9336437718277066,"service of that source should be provided.
735"
LICENSES FOR EXISTING ASSETS,0.9348079161816065,"• If assets are released, the license, copyright information, and terms of use in the
736"
LICENSES FOR EXISTING ASSETS,0.9359720605355064,"package should be provided. For popular datasets, paperswithcode.com/datasets
737"
LICENSES FOR EXISTING ASSETS,0.9371362048894063,"has curated licenses for some datasets. Their licensing guide can help determine the
738"
LICENSES FOR EXISTING ASSETS,0.9383003492433062,"license of a dataset.
739"
LICENSES FOR EXISTING ASSETS,0.9394644935972061,"• For existing datasets that are re-packaged, both the original license and the license of
740"
LICENSES FOR EXISTING ASSETS,0.940628637951106,"the derived asset (if it has changed) should be provided.
741"
LICENSES FOR EXISTING ASSETS,0.9417927823050058,"• If this information is not available online, the authors are encouraged to reach out to
742"
LICENSES FOR EXISTING ASSETS,0.9429569266589057,"the asset’s creators.
743"
NEW ASSETS,0.9441210710128056,"13. New Assets
744"
NEW ASSETS,0.9452852153667055,"Question: Are new assets introduced in the paper well documented and is the documentation
745"
NEW ASSETS,0.9464493597206054,"provided alongside the assets?
746"
NEW ASSETS,0.9476135040745053,"Answer: [NA]
747"
NEW ASSETS,0.9487776484284052,"Justification: the paper does not release new assets.
748"
NEW ASSETS,0.9499417927823051,"Guidelines:
749"
NEW ASSETS,0.9511059371362048,"• The answer NA means that the paper does not release new assets.
750"
NEW ASSETS,0.9522700814901047,"• Researchers should communicate the details of the dataset/code/model as part of their
751"
NEW ASSETS,0.9534342258440046,"submissions via structured templates. This includes details about training, license,
752"
NEW ASSETS,0.9545983701979045,"limitations, etc.
753"
NEW ASSETS,0.9557625145518044,"• The paper should discuss whether and how consent was obtained from people whose
754"
NEW ASSETS,0.9569266589057043,"asset is used.
755"
NEW ASSETS,0.9580908032596042,"• At submission time, remember to anonymize your assets (if applicable). You can either
756"
NEW ASSETS,0.959254947613504,"create an anonymized URL or include an anonymized zip file.
757"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9604190919674039,"14. Crowdsourcing and Research with Human Subjects
758"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9615832363213038,"Question: For crowdsourcing experiments and research with human subjects, does the paper
759"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9627473806752037,"include the full text of instructions given to participants and screenshots, if applicable, as
760"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9639115250291036,"well as details about compensation (if any)?
761"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9650756693830035,"Answer: [NA]
762"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9662398137369034,"Justification: the paper does not involve crowdsourcing nor research with human subjects.
763"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9674039580908033,"Guidelines:
764"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9685681024447031,"• The answer NA means that the paper does not involve crowdsourcing nor research with
765"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.969732246798603,"human subjects.
766"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9708963911525029,"• Including this information in the supplemental material is fine, but if the main contribu-
767"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9720605355064028,"tion of the paper involves human subjects, then as much detail as possible should be
768"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9732246798603027,"included in the main paper.
769"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9743888242142026,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
770"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9755529685681025,"or other labor should be paid at least the minimum wage in the country of the data
771"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9767171129220024,"collector.
772"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9778812572759022,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
773"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9790454016298021,"Subjects
774"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.980209545983702,"Question: Does the paper describe potential risks incurred by study participants, whether
775"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9813736903376019,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
776"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9825378346915018,"approvals (or an equivalent approval/review based on the requirements of your country or
777"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9837019790454016,"institution) were obtained?
778"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9848661233993015,"Answer: [NA]
779"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9860302677532014,"Justification: the paper does not involve crowdsourcing nor research with human subjects.
780"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9871944121071012,"Guidelines:
781"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9883585564610011,"• The answer NA means that the paper does not involve crowdsourcing nor research with
782"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.989522700814901,"human subjects.
783"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9906868451688009,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
784"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9918509895227008,"may be required for any human subjects research. If you obtained IRB approval, you
785"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9930151338766007,"should clearly state this in the paper.
786"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9941792782305006,"• We recognize that the procedures for this may vary significantly between institutions
787"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9953434225844005,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
788"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9965075669383003,"guidelines for their institution.
789"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9976717112922002,"• For initial submissions, do not include any information that would break anonymity (if
790"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9988358556461001,"applicable), such as the institution conducting the review.
791"
