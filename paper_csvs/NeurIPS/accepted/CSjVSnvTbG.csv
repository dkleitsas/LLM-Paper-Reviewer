Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0012345679012345679,"We propose a new framework for formulating optimal transport distances between
Markov chains. Previously known formulations studied couplings between the en-
tire joint distribution induced by the chains, and derived solutions via a reduction to
dynamic programming (DP) in an appropriately defined Markov decision process.
This formulation has, however, not led to particularly efficient algorithms so far,
since computing the associated DP operators requires fully solving a static optimal
transport problem, and these operators need to be applied numerous times during
the overall optimization process. In this work, we develop an alternative perspective
by considering couplings between a “flattened” version of the joint distributions
that we call discounted occupancy couplings, and show that calculating optimal
transport distances in the full space of joint distributions can be equivalently formu-
lated as solving a linear program (LP) in this reduced space. This LP formulation
allows us to port several algorithmic ideas from other areas of optimal transport
theory. In particular, our formulation makes it possible to introduce an appropriate
notion of entropy regularization into the optimization problem, which in turn en-
ables us to directly calculate optimal transport distances via a Sinkhorn-like method
we call Sinkhorn Value Iteration (SVI). We show both theoretically and empirically
that this method converges quickly to an optimal coupling, essentially at the same
computational cost of running vanilla Sinkhorn in each pair of states. Along the
way, we point out that our optimal transport distance exactly matches the common
notion of bisimulation metrics between Markov chains, and thus our results also ap-
ply to computing such metrics, and in fact our algorithm turns out to be significantly
more efficient than the best known methods developed so far for this purpose."
INTRODUCTION,0.0024691358024691358,"1
Introduction"
INTRODUCTION,0.003703703703703704,"Measuring distances between structured objects and sequences is an important problem in a variety
of areas of science. The more structured the objects become, the harder it gets to define appropriate
notions of distances, as good notions of proximity need to take into account the possibly complex
relationships between the constituent parts of each object. The possibility that the objects in question
may be random further complicates the picture, and in such cases it becomes more natural to measure
distances between the underlying joint probability distributions. Within the specific context of
comparing stochastic processes, two natural notions of distance have emerged over the past decades:
the notion of probabilistic bisimulation metrics that takes its root in modal logic and theoretical
computer science [Sangiorgi, 2009, Abate, 2013], and the notion of optimal-transport distances that
originates from probability theory [Villani, 2009, Peyré and Cuturi, 2019]. In this paper, we show
that bisimulation metrics are in fact optimal-transport distances, and we make use of this observation
to derive efficient algorithms for computing distances between stochastic processes."
INTRODUCTION,0.0049382716049382715,"The two distance notions have found strikingly different applications. Bisimulation emerged within the
area of theoretical computer science as one of the most important important concepts in concurrency
theory and formal verification of computer systems [Park, 1981, Milner, 1989], and has been extended
to probabilistic transition systems by Larsen and Skou [1989]. Within machine learning, bisimulation
metrics have become especially popular in the context of reinforcement learning (RL) due to the work
of Ferns et al. [2004], and have become one of the few standard tools of representation learning [Jiang,
2018, 2024]. In particular, the work of Ferns et al. [2004] advocates for using bisimilarity as a basis
for state aggregation, measuring similarities of states in terms of similarities of two chains MX and
MY that only differ in their initial state. While this approach has inspired numerous follow-up works
[Gelada et al., 2019, Castro, 2020, Agarwal et al., 2021b, Zhang et al., 2021, Hansen-Estruch et al.,
2022, Castro et al., 2022], ultimately this line of work has failed to discover efficient algorithms for
computing bisimulation metrics and has largely resorted to heuristics for computing similarity metrics."
INTRODUCTION,0.006172839506172839,"On the other hand, optimal transport (OT) has found numerous applications in areas as diverse as
economics [Galichon, 2016], signal processing [Kolouri et al., 2017], or genomics [Schiebinger
et al., 2019]. Within machine learning, it has been used for the similarly diverse areas of domain
adaptation [Courty et al., 2016], generative modeling [Arjovsky et al., 2017, Song et al., 2020, Shi
et al., 2024], representation learning [Courty et al., 2018], and, perhaps most relevant to our work, as
a way of measuring distances between graphs [Titouan et al., 2019, Chen et al., 2022, Chuang and
Jegelka, 2022]. The recent works of Yi et al. [2021], Brugère et al. [2024] propose to define graph
distances via studying the behavior of random walks defined on the graph, thus reducing the problem
of comparing graphs to comparing stochastic processes—exactly the subject of the present paper.
Other applications of OT between stochastic processes include generative modeling for sequential
data [Xu et al., 2020], pricing and hedging in mathematical finance [Backhoff-Veraguas et al., 2017],
and analyzing multistage stochastic optimization problems [Pflug, 2010, Bartl and Wiesel, 2022].
It appears however that the literature on optimal transport for stochastic processes has apparently
not yet discovered connections with bisimulation metrics and the rich intellectual history behind it.
Also, applications of optimal transport for representation learning within the context of reinforcement
learning appear to be nonexistent."
INTRODUCTION,0.007407407407407408,"In this paper we observe that, despite their apparent differences, bisimulation metrics and optimal
transport distances are one and the same. Furthermore, we provide a new perspective on both OT
distances and bisimulation metrics by formulating the distance metric as the solution of a linear
program (LP) in the space of “occupancy couplings”, a finite-dimensional projection of the infinite-
dimensional process laws. Building on tools from computational optimal transport [Peyré and Cuturi,
2019] and entropy-regularized Markov decision processes [Neu et al., 2017, Geist et al., 2019], we
design an algorithm that effectively combines Sinkhorn’s algorithm [Sinkhorn and Knopp, 1967,
Cuturi, 2013] with an entropy-regularized version of the classic Value Iteration algorithm [Bellman,
1957, Neu et al., 2017]. Building on recent work on computational optimal transport [Altschuler
et al., 2017, Ballu and Berthet, 2023], we provide theoretical guarantees for the resulting algorithm
(called Sinkhorn Value Iteration) and perform numerical studies that demonstrate its effectiveness for
computing distances between Markov chains."
INTRODUCTION,0.008641975308641974,"Notations.
For a finite set S, we use ∆S to denote the set of all probability distributions over
S. We will denote infinite sequences by x = (x0, x1, . . . ) and the corresponding subsequences as
xn = (x0, x1, . . . , xn). For two sets X and Y, we will often write XY to abbreviate the direct-
product notation X × Y, and for two indices x and y and a function f : XY →Z, we will often
write f(xy) instead of f(x, y) to save space. Also, we will denote scalar products by ⟨·, ·⟩and use
∥·∥p to denote the ℓp-norm."
PRELIMINARIES,0.009876543209876543,"2
Preliminaries"
PRELIMINARIES,0.011111111111111112,"We study the problem of measuring distances between pairs of finite Markov chains. Specifically, we
consider two stationary Markov processes MX = (X, PX , ν0,X ) and MY = (Y, PY, ν0,Y), where"
PRELIMINARIES,0.012345679012345678,"• X and Y are the finite state spaces with cardinalities m = |X| and n = |Y |,
• PX : X →∆X and PY : Y →∆Y are the transition kernels that determine the evolution of the
states as PX (x′|x) = P [Xt+1 =x′| Xt =x] and PY(y′|y) = P [Yt+1 =y′| Yt =y] for all t, and
• ν0,X and ν0,Y are the initial-state distributions with X0 ∼ν0,X and Y0 ∼ν0,Y."
PRELIMINARIES,0.013580246913580247,"Without significant loss of generality, we will suppose that the initial states are fixed almost surely
as X0 = x0 and Y0 = y0, and refer to their corresponding joint distribution as ν0 = δx0,y0. These
objects together define a sequence of joint distributions P [(X0, X1, . . . , Xn) = (x0, x1, . . . , xn)]
and P [(Y0, Y1, . . . , Yn) = (y0, y1, . . . , yn)] for each n, which together define respective the laws
of the infinite sequences X = (X1, X2, . . . ) and Y = (Y1, Y2, . . . ) via Kolmogorov’s extension
theorem. With a slight abuse of notation, we will use MX and MY to denote the corresponding
measures that satisfy MX (xn) = P
"
PRELIMINARIES,0.014814814814814815,"Xn = xn

and MY(yn) = P
"
PRELIMINARIES,0.016049382716049384,"Y n = yn

for any x ∈X ∞,
y ∈Y∞and n. The corresponding conditional distributions are denoted as MX (xn|xn−1) =
P

Xn = xn|Xn−1 = xn−1

and MY(yn|yn−1) = P

Yn = yn|Y n−1 = yn−1

."
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS,0.01728395061728395,"2.1
Optimal transport between Markov chains"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS,0.018518518518518517,"Our main object of interest in this work is a notion of optimal transport distance between infinite-
horizon Markov chains. Several previous works have studied such distances (which are discussed
in detail in Appendix A), and our precise definition we give below is closest to Moulos [2021],
O’Connor et al. [2022] and Brugère et al. [2024]. We consider Markov chains on state spaces where
a “ground metric” (or “ground cost”) c : X × Y →R+ is available to measure distances between any
two individual states x ∈X and y ∈Y, with the distance denoted as c(x, y). For any two sequences
x = (x1, x2, . . . ) and y = (y1, y2, . . . ), we define the discounted total cost"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS,0.019753086419753086,"cγ(x, y) = ∞
X"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS,0.020987654320987655,"t=0
γtc(xt, yt),
(1)"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS,0.022222222222222223,"where γ ∈(0, 1) is the discount factor that expresses the preference that two sequences be considered
further apart if they exhibit differences at earlier times in terms of the ground cost c. Following the
optimal-transport literature, we will consider distances between the stochastic processes MX and MY
via the notion of couplings. To this end, we define a coupling of MX and MY as a stochastic process
evolving on the joint space XY, with its law defined for all n as MXY(xnyn) = P
"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS,0.02345679012345679,"Xn = xn, Y n =
yn

, required to satisfy P"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS,0.024691358024691357,yn∈Yn MXY(xnyn) = MX (xn) and P
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS,0.025925925925925925,"xn∈X n MXY(xnyn) = MY(yn).
We will define the set of all such couplings as Π."
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS,0.027160493827160494,"The notion of couplings defined above does not respect the temporal structure of the Markov chains
MX and MY appropriately: while by definition the distribution of state Xn may only be causally
influenced by past states Xk with k < n, the general notion of coupling above allows the state Xn to
be influenced by future states Yk with k ≥n as well. To rule out this possibility (and following past
works mentioned in the introduction), we will introduce the notion of bicausal couplings. A coupling
MXY is called bicausal if and only if it satisfies
X"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS,0.028395061728395062,"yn
MXY(xnyn|xn−1yn−1) = MX (xn|xn−1) and
X"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS,0.02962962962962963,"xn
MXY(xnyn|xn−1yn−1) = MY(yn|yn−1)"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS,0.030864197530864196,"for all sequences x, y ∈X ∞× Y∞and all n. Denoting the set of all bicausal couplings by Πbc, we
define our optimal transport distance as"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS,0.03209876543209877,"Wγ(MX , MY; c, x0, y0) =
inf
MXY∈Πbc"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS,0.03333333333333333,"Z
cγ(X, Y ) dMXY(X, Y ),
(2)"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS,0.0345679012345679,"where we emphasize the dependence of the distance on x0, y0 explicitly with our notation."
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS,0.03580246913580247,"By noticing that the optimization problem outlined above can be reformulated as a Markov decision
process (MDP), Moulos [2021] has shown that the infimum in (2) is achieved within the family of
Markovian couplings that satisfy
X"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS,0.037037037037037035,"yn
MXY(xnyn|xn−1yn−1) = PX (xn|xn−1) and
X"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS,0.03827160493827161,"xn
MXY(xnyn|xn−1yn−1) = PY(yn|yn−1)"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS,0.03950617283950617,"for all sequences of state pairs and all values of n. Furthermore, it can be seen that Markovian
couplings are fully specified in terms of transition couplings of the form π : XY →∆XY, with
π(x′y′|xy) standing for P [(Xt+1, Yt+1) = (x′, y′)| (Xt, Yt) = (xt, yt)] under the law induced by
the coupling. We say that a transition coupling is valid if it satisfies the marginal constraints
P
y′ π(x′y′|xy) = PX (x′|x) and P
x′ π(x′y′|xy) = PY(y′|y). Defining the set of such valid tran-
sition couplings in state pair xy by Πxy =

p ∈∆XY : P"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS,0.040740740740740744,"y′ p(x′y′) = PX (x′|x), P"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS,0.04197530864197531,x′ p(x′y′) =
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS,0.043209876543209874,"PY(y′|y)
	
, Moulos [2021] introduces an MDP M with an infinite action set corresponding to picking
the joint next-state couplings in Πxy. An optimal transition coupling can be found by solving the
following Bellman optimality equations of the MDP M:"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS,0.044444444444444446,"V ∗(xy) = c(xy) + γ inf
p∈Πxy X"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS,0.04567901234567901,"x′y′
p(x′y′)V ∗(x′y′).
(3)"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS,0.04691358024691358,"The infimum on the right-hand side is achieved by an optimal transition coupling π∗(·|xy) =
arg minp∈Πxy
P"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS,0.04814814814814815,"x′y′ p(x′y′)V ∗(x′y′). The solution V ∗is unique and can be shown to satisfy
V ∗(xy) = Wγ(MX , MY; c, x, y) for all xy ∈XY. For completeness, we include the precise
definition of the MDP M and the proofs of these results in Appendix B."
BISIMULATION METRICS,0.04938271604938271,"2.2
Bisimulation metrics"
BISIMULATION METRICS,0.050617283950617285,"The notion of bisimulation metrics has been introduced by Desharnais et al. [1999, 2004] and
van Breugel and Worrell [2001], with the purpose of defining distances between Markov chains,
using a methodology rooted in modal logic that at first may appear entirely different from the
optimal-transport framework described above. We only give a very high-level overview of the classic
logic-based characterization here (as the fine details are irrelevant to the final conclusion that this
section is headed to), and refer the reader to the additional discussion in Appendix A for further
reading. Desharnais et al. [1999, 2004] considered labeled Markov chains where a labeling function
r : X ∪Y →R assigns labels to each state, and defined bisimulation metrics via"
BISIMULATION METRICS,0.05185185185185185,"dγ(MX , MY; r, x0, y0) = sup
f∈Fγ
|fMX (x0) −fMY(y0)| ,
(4)"
BISIMULATION METRICS,0.05308641975308642,"where Fγ is a family of functional expressions generated by a certain grammar, and fMX : X →R
and fMY : Y →R are the respective “interpretations” of each f ∈F on the Markov chains MX and
MY. Building on this formulation, van Breugel and Worrell [2001] have shown that the distance
metric can be equivalently characterized by the solution of a fixed-point equation, whose expression
was subsequently used by Ferns et al. [2004] to define bisimulation metrics for Markov decision
processes where r takes the role of a reward function, and the evolution of states may be influenced
by actions. The case of having no actions available corresponds to our setting, where their definition
of a bisimulation metric simplifies to the solution of the fixed-point equation"
BISIMULATION METRICS,0.05432098765432099,"U ∗(xy) = (1 −γ)|r(x) −r(y)| + γ inf
p∈Πxy X"
BISIMULATION METRICS,0.05555555555555555,"x′y′
p(x′y′)U ∗(x′y′).
(5)"
BISIMULATION METRICS,0.056790123456790124,"The solution to this system is unique and satisfies U ∗(xy) = dγ(MX , MY; r, x, y). Putting this
result side-by-side with Equation (3), one can immediately realize that bisimulation metrics and
our notion of optimal transport distances coincide when picking the ground cost function c(xy) =
(1 −γ)|r(x) −r(y)|. To our knowledge, this remarkable observation has not been publicly made
anywhere in either the optimal-transport or the bisimulation-metric literature. This connection has
several important implications, which we have already discussed at some length in Section 1. We
relegate further discussion of these metrics in the light of this observation to Appendix A."
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS AS A LINEAR PROGRAM,0.05802469135802469,"3
Optimal transport between Markov chains as a linear program"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS AS A LINEAR PROGRAM,0.05925925925925926,"Optimal transport problems can typically be formulated as linear programs (LPs), since couplings
can be characterized as joint distributions satisfying a set of easily-expressed linear constraints (see,
e.g., Chapter 3 in Peyré and Cuturi 2019). Our problem is no exception, and in fact the original
problem statement of Equation 2 can be expressed in this form: the constraints defining Πbc are
all linear in MXY. However, MXY is an infinite-dimensional object and thus this formulation is
not instructive for developing computationally tractable algorithms. We address this problem in
this section, where we define an equivalent LP formulation that replaces the infinite-dimensional
optimization variable with an appropriate low-dimensional projection. Our framework builds on
the classic LP formulation of optimal control in MDPs first proposed in the 1960’s [Manne, 1960,
de Ghellinck, 1960, d’Epenoux, 1963, Denardo, 1970], and covered thoroughly in several standard
textbooks (e.g., Section 6.9 of Puterman 1994)."
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS AS A LINEAR PROGRAM,0.06049382716049383,"In order to set things up, we need to start with some important definitions. We say that a transition
coupling π : XY →∆XY generates a trajectory (X0, Y0, X1, Y1, . . . ) if (X0, Y0) ∼ν0 and the"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS AS A LINEAR PROGRAM,0.06172839506172839,"subsequent state-pairs are drawn independently from the transition coupling as (Xt+1, Yt+1) ∼
π(·|Xt, Yt) for all t. Then, we define the occupancy coupling µπ associated with this process as a
distribution over XY × XY, with each of its entries xy, x′y′ defined as"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS AS A LINEAR PROGRAM,0.06296296296296296,"µπ(xy, x′y′) = (1 −γ)Eπ "" ∞
X"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS AS A LINEAR PROGRAM,0.06419753086419754,"t=0
γtI{(Xt,Yt)=(x,y),(Xt+1,Yt+1)=(x′,y′)} # ,"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS AS A LINEAR PROGRAM,0.0654320987654321,"where Eπ [·] emphasizes that the trajectory of state-pairs has been generated by π. In words,
µπ(xy, x′y′) is the discounted number of times that the quadruple (xy, x′y′) is visited by the process.
With this definition, it is easy to notice that the objective optimized in Equation (2) can be rewritten
as a linear function of µπ. Indeed, suppose that MXY is the law of the process generated by π as
described above, so that we can write
Z
cγ(X, Y ) dMXY(X, Y ) = Eπ "" ∞
X"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS AS A LINEAR PROGRAM,0.06666666666666667,"t=0
γtc(Xt, Yt) # = Eπ "" ∞
X"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS AS A LINEAR PROGRAM,0.06790123456790123,"t=0
γt X"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS AS A LINEAR PROGRAM,0.0691358024691358,"xy
I{(Xt,Yt)=(x,y)}c(xy) # =
X xy
Eπ "" ∞
X"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS AS A LINEAR PROGRAM,0.07037037037037037,"t=0
γtI{(Xt,Yt)=(x,y)} #"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS AS A LINEAR PROGRAM,0.07160493827160494,"c(xy) =
1
1 −γ X"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS AS A LINEAR PROGRAM,0.0728395061728395,"xy,x′y′
µπ(xy, x′y′)c(xy) = ⟨µπ, c⟩"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS AS A LINEAR PROGRAM,0.07407407407407407,1 −γ .
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS AS A LINEAR PROGRAM,0.07530864197530865,"We say that an occupancy coupling µ is valid if it is generated by a valid transition coupling. It is easy
to verify that every valid occupancy coupling µ ∈∆XY×XY satisfies the following three constraints:
X"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS AS A LINEAR PROGRAM,0.07654320987654321,"x′y′
µ(xy, x′y′) = γ
X"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS AS A LINEAR PROGRAM,0.07777777777777778,"x′y′
µ(x′y′, xy) + (1 −γ)ν0(xy)
(∀xy ∈XY),
(6) X"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS AS A LINEAR PROGRAM,0.07901234567901234,"y′
µ(xy, x′y′) =
X"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS AS A LINEAR PROGRAM,0.08024691358024691,"x′′,y′′
µ(xy, x′′y′′)PX (x′|x)
(∀x, x′ ∈X × X),
(7) X"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS AS A LINEAR PROGRAM,0.08148148148148149,"x′
µ(xy, x′y′) =
X"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS AS A LINEAR PROGRAM,0.08271604938271605,"x′′,y′′
µ(xy, x′′y′′)PY(y′|y)
(∀y, y′ ∈Y × Y).
(8)"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS AS A LINEAR PROGRAM,0.08395061728395062,"We refer to the first equality constraint as the flow constraint and the second and third ones as the
transition coherence constraints for MX and MY, respectively1. We show in the following lemma
that the above conditions uniquely characterize the set of valid occupancy couplings.
Lemma 1. A distribution µ ∈∆XY×XY is a valid occupancy coupling associated with some
transition coupling π : XY →∆XY if and only if it satisfies Equations (6)–(8)."
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS AS A LINEAR PROGRAM,0.08518518518518518,"One important consequence of this result is that for each transition coupling, one can associate a
transition coupling πµ with µπµ = µ, with entries satisfying P"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS AS A LINEAR PROGRAM,0.08641975308641975,"x′′y′′ µ(xy, x′′y′′)πµ(x′y′|xy) =
µ(xy, x′y′). A proof is provided in Appendix B.2. Having established in the previous section that
stationary occupancy couplings are sufficient to achieve the supremum in the definition of the OT
distance of Equation (2), this result immediately implies the following.
Theorem 1. Let B, SX and SY respectively stand for the sets of all distributions µ ∈∆XY×XY that
satisfy equations (6), (7), and (8). Then,"
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS AS A LINEAR PROGRAM,0.08765432098765433,"Wγ(MX , MY; c, x0, y0) =
1
1 −γ ·
inf
µ∈B∩SX ∩SY ⟨µ, c⟩."
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS AS A LINEAR PROGRAM,0.08888888888888889,"Since the constraints on µ in the above reformulation are all linear, the optimization problem
stated above is obviously a linear program. Notably, the resulting LP is not the standard dual to
the LP associated with the MDP formulation introduced in Section 2.1, which would result in an
infinite-dimensional LP with one constraint associated with each of the continuously-valued actions.
Such an infinite-dimensional reformulation was previously considered by Chen et al. [2012] in the
context of computing bisimulation metrics, who used it as a tool for analysis rather than algorithm
design. Notably, our formulation results in a finite-dimensional LP with |X|2 |Y|2 variables and
|X|2 + |X| |Y| + |Y|2 constraints. Building on the celebrated result of Ye [2011] (and similarly to
the work of Chen et al., 2012 mentioned above), one can show that our LP can be solved in strongly
polynomial time via an appropriate adaptation of the simplex method or the classic policy iteration
method of Howard [1960]. We propose an alternative methodology in the next section."
OPTIMAL TRANSPORT BETWEEN MARKOV CHAINS AS A LINEAR PROGRAM,0.09012345679012346,"1When γ = 1, the first condition is known as detailed balance within statistical physics. Altogether, the
constraints closely resemble what are often called the “Bellman flow constraints” in today’s RL literature."
SINKHORN VALUE ITERATION,0.09135802469135802,"4
Sinkhorn Value Iteration"
SINKHORN VALUE ITERATION,0.09259259259259259,"Solving optimal-transport problems via standard LP solvers (like variations of the simplex method or
interior-point methods) is known to be empirically hard, and thus we seek alternatives to this approach
towards optimizing our own LP defined in the previous section. In computational optimal transport, a
paradigm shift was initiated by Cuturi [2013] who successfully applied entropic regularization to
the classic LP objective of optimal transport, and solved the resulting optimization method through
an iterative algorithm called the Sinkhorn–Knopp method (sometimes simply called Sinkhorn’s
algorithm, due to Sinkhorn and Knopp, 1967). This method is based on finding a feasible point in
a two-constraint problem by alternately satisfying one and the other, and has resulted in practical
algorithms that were orders of magnitude faster than all previously studied methods. Entropy regular-
ization and Sinkhorn’s algorithm have thus became the most important cornerstones of computational
optimal transport. Drawing on the same principles as well as the theory of entropy-regularized
Markov decision processes [Neu et al., 2017, Geist et al., 2019], we develop a computationally
effective algorithm for computing optimal transport distances between Markov chains below."
SINKHORN VALUE ITERATION,0.09382716049382717,"4.1
Formal definition: Mirror Sinkhorn on the space of occupancy couplings"
SINKHORN VALUE ITERATION,0.09506172839506173,"Our method is an adaptation of a version of Sinkhorn’s algorithm called Mirror Sinkhorn, first
proposed and analyzed by Ballu and Berthet [2023]. This method combines mirror-descent-style
updates [Nemirovski and Yudin, 1983, Beck and Teboulle, 2003] with alternating projections to
two convex sets whose intersection corresponds to the feasible set we seek to optimize over. In our
adaptation, we choose the two sets as BX = 
"
SINKHORN VALUE ITERATION,0.0962962962962963,"µ :
X"
SINKHORN VALUE ITERATION,0.09753086419753086,"y′
µ(xy, x′y′) =  γ
X"
SINKHORN VALUE ITERATION,0.09876543209876543,"x′′y′′
µ(x′′y′′, xy) + (1 −γ)ν0(xy) "
SINKHORN VALUE ITERATION,0.1,"PX (x′|x)
(∀xy, x′) 
 ,"
SINKHORN VALUE ITERATION,0.10123456790123457,"that can be seen to be the set of distributions µ that satisfy both Equations (6) and (7), and BY = 
"
SINKHORN VALUE ITERATION,0.10246913580246914,"µ :
X"
SINKHORN VALUE ITERATION,0.1037037037037037,"x′
µ(xy, x′y′) =  γ
X"
SINKHORN VALUE ITERATION,0.10493827160493827,"x′′y′′
µ(x′′y′′, xy) + (1 −γ)ν0(xy) "
SINKHORN VALUE ITERATION,0.10617283950617284,"PY(y′|y)
(∀xy, y′) 
 "
SINKHORN VALUE ITERATION,0.10740740740740741,"which is the set of distributions µ that satisfy both Equations (6) and (8). Naturally, the intersection
of the two sets corresponds to valid occupancy couplings. It remains to define an appropriate notion
of entropy for the purpose of regularization. Following Neu et al. [2017], we will use the conditional
relative entropy defined between two joint distributions µ, µ′ ∈∆XYXY as"
SINKHORN VALUE ITERATION,0.10864197530864197,"H (µ∥µ′) =
X"
SINKHORN VALUE ITERATION,0.10987654320987654,"xy,x′y′
µ(xy, x′y′) log
µ(xy, x′y′)/ P"
SINKHORN VALUE ITERATION,0.1111111111111111,"x′′y′′ µ(xy, x′′y′′)"
SINKHORN VALUE ITERATION,0.11234567901234568,"µ′(xy, x′y′)/ P"
SINKHORN VALUE ITERATION,0.11358024691358025,"x′′y′′ µ′(xy, x′′y′′) =
X"
SINKHORN VALUE ITERATION,0.11481481481481481,"xy,x′y′
µ(xy, x′y′) log πµ(x′y′|xy)"
SINKHORN VALUE ITERATION,0.11604938271604938,πµ′(x′y′|xy).
SINKHORN VALUE ITERATION,0.11728395061728394,"It is an easy exercise to show that H is a Bregman divergence that is convex in its first argument (see,
e.g., Appendix A.1 of Neu et al., 2017). Note however that H (µ∥µ′) can be zero even if µ ̸= µ′ and
thus it is not strongly convex in µ."
SINKHORN VALUE ITERATION,0.11851851851851852,"With these ingredients, we symbolically define our algorithm as calculating the sequence of updates"
SINKHORN VALUE ITERATION,0.11975308641975309,"µk+1 = arg min
µ∈Bk"
SINKHORN VALUE ITERATION,0.12098765432098765,"
⟨µ, c⟩+ 1"
SINKHORN VALUE ITERATION,0.12222222222222222,"η H (µ∥µk)

(9)"
SINKHORN VALUE ITERATION,0.12345679012345678,"for each k = 1, . . . , K −1, where µ1 is the occupancy coupling associated to the trivial coupling
π1(·|xy) = PX (·|x) ⊗PY(·|y) for each state pair xy ∈XY, η > 0 is a stepsize (or learning-rate)
parameter and Bk is chosen to be BX in odd rounds and BY in even rounds. By adapting tools from
the theory of entropy-regularized Markov decision processes, the updates can be computed in closed
form by solving a system of equations closely resembling the regularized Bellman equations. In
particular, we define the Bellman–Sinkhorn operators for a given transition coupling π as the operators"
SINKHORN VALUE ITERATION,0.12469135802469136,"T π
X : RXY×X →RXY×X and T π
Y : RXY×Y →RXY×Y acting on functions VX ∈RXY×X and
VY ∈RXY×Y respectively as"
SINKHORN VALUE ITERATION,0.1259259259259259,"(T π
X VX ) (xy, x′) = −1"
SINKHORN VALUE ITERATION,0.1271604938271605,"η log
X y′"
SINKHORN VALUE ITERATION,0.12839506172839507,π(x′y′|xy)
SINKHORN VALUE ITERATION,0.12962962962962962,PX (x′|x) exp  −η 
SINKHORN VALUE ITERATION,0.1308641975308642,"c(xy) + γ
X"
SINKHORN VALUE ITERATION,0.13209876543209875,"x′′
PX (x′′|x′)VX (x′y′, x′′) !! , and"
SINKHORN VALUE ITERATION,0.13333333333333333," 
T π
Y VY

(xy, y′) = −1"
SINKHORN VALUE ITERATION,0.1345679012345679,"η log
X x′"
SINKHORN VALUE ITERATION,0.13580246913580246,π(x′y′|xy)
SINKHORN VALUE ITERATION,0.13703703703703704,PY(y′|y) exp  −η 
SINKHORN VALUE ITERATION,0.1382716049382716,"c(xy) + γ
X"
SINKHORN VALUE ITERATION,0.13950617283950617,"y′′
PY(y′′|y′)VY(x′y′, y′′)    ."
SINKHORN VALUE ITERATION,0.14074074074074075,"Then, for odd rounds, the updates can be calculated by solving the fixed-point equations Vk = T πk
X Vk,
defining the shorthand Qk(xy, x′y′) = c(xy) + γ P"
SINKHORN VALUE ITERATION,0.1419753086419753,"x′′ PX (x′′|x′)Vk(x′y′, x′′), and subsequently
updating the transition coupling πk multiplicatively as"
SINKHORN VALUE ITERATION,0.14320987654320988,"πk+1(x′y′|xy) =
πk(x′y′|xy) exp (−ηQk(xy, x′y′))
P"
SINKHORN VALUE ITERATION,0.14444444444444443,"y′′ πk(x′y′′|xy) exp (−ηQk(xy, x′y′′))PX (x′|x).
(10)"
SINKHORN VALUE ITERATION,0.145679012345679,It is easy to verify that this transition coupling satisfies P
SINKHORN VALUE ITERATION,0.1469135802469136,"y′ πk+1(x′y′|xy) = P(x′|x). The updates
for even rounds are computed analogously with the roles of X and Y swapped. We respectively refer
to the fixed-point equations Vk = T πk
X Vk and Vk = T πk
Y Vk as the Bellman–Sinkhorn equations for
MX and MY, and the functions Vk and Qk as value functions. The following proposition (proved in
Appendix E.1) formally establishes the equivalence between the two update rules."
SINKHORN VALUE ITERATION,0.14814814814814814,"Proposition 1. Let µk+1 and πk+1 be specified for each k as in Equations (9) and (10), respectively.
Then, µk+1 = µπk+1 holds for all k."
PRACTICAL IMPLEMENTATION,0.14938271604938272,"4.2
Practical implementation"
PRACTICAL IMPLEMENTATION,0.1506172839506173,"Algorithm 1: Sinkhorn Value Iteration
Input: PX , PY, c, η, γ, K, m
Initialise: π1 ←PX ⊗PY;
for k = 1, ..., K −1 do"
PRACTICAL IMPLEMENTATION,0.15185185185185185,if k is odd then
PRACTICAL IMPLEMENTATION,0.15308641975308643,"VX ←(T πk
X )m VX ; {BX projection.}
else"
PRACTICAL IMPLEMENTATION,0.15432098765432098,"VY ←
 
T πk
Y
m VY; {BY projection.}
end
πk+1 ←update(πk);
{Equation 10}
end
µK ←1"
PRACTICAL IMPLEMENTATION,0.15555555555555556,"K
PK
k=1(µπk);
πout ←round(πµK);
V πout ←evaluate(πout);
Output: πout, V πout
{Final coupling}"
PRACTICAL IMPLEMENTATION,0.15679012345679014,"The algorithm described above can be seen as
performing online Mirror Sinkhorn updates in
each state pair xy with a sequence of cost func-
tions Qk, which are computed via solving the
Bellman–Sinkhorn equations. Since T π
X and
T π
Y are easily seen to be contractive with re-
spect to the supremum norm with contraction
factor γ (as shown by a standard calculation in-
cluded in Appendix E.3), these equations can
be solved by an adaptation of the classic Value
Iteration method of Bellman [1957]. Concretely,
we repeatedly apply the Bellman–Sinkhorn op-
erators until the fixed point is reached up to
sufficient precision (controlled by the number of
update steps m). We call the resulting method
Sinkhorn Value Iteration (SVI), and provide its
pseudocode as Algorithm 1."
PRACTICAL IMPLEMENTATION,0.1580246913580247,"Notably, while SVI is defined in its abstract form as a sequence of updates in the space of occupancy
couplings µk, its implementation only works with transition couplings πk. The final output of
SVI is a transition coupling πout, obtained by computing the average µK =
1
K
PK
k=1 µπk of all
occupancy couplings, computing πµK and then rounding the result to a valid transition coupling.
In particular we apply a simple rounding procedure due to Altschuler et al. [2017] individually on
πµK(·|xy) for each state-pair xy—for the full details, see Appendix E.2. Besides πout, SVI also
outputs an estimate of V ∗in the form of the value function V πout, as defined in Equation (11) in
Appendix B.1. This function can be computed efficiently by solving the linear system of Bellman
equations V πout(xy) = c(xy) + γ P"
PRACTICAL IMPLEMENTATION,0.15925925925925927,x′y′ πout(x′y′|xy)V πout(x′y′).
PRACTICAL IMPLEMENTATION,0.16049382716049382,"A number of small simplifying steps can be made to make the algorithm easier to implement. First,
instead of obtaining πµK via the computationally expensive procedure described above, one can
simply run the rounding procedure on the final transition coupling πK and output the result. Second,"
PRACTICAL IMPLEMENTATION,0.1617283950617284,"while theoretical analysis suggests setting m = ∞in order to make sure that all projection steps
are perfect, such exact computation may be unnecessary and inefficient in practice, and thus (much)
smaller values can be used instead. Third, for small values of η the softmax function used in the
definition of the Bellman–Sinkhorn operator can be accurately approximated by an average with
respect to πk(x′y′|xy)/PX (x′|x), which suggests a simple alternative to the projection steps. This
approximates SVI similarly as to how the Mirror Descent Modified Policy Iteration method of Geist
et al. [2019] approximates the mirror descent method of Neu et al. [2017] (see also Azar et al., 2012).
The resulting method (that we refer to as Sinkhorn Policy Iteration, or SPI) is presented in detail
along with its theoretical analysis in Appendix D. We study effects of these implementation choices
via a sequence of experiments in Section 5."
CONVERGENCE GUARANTEES,0.16296296296296298,"4.3
Convergence guarantees"
CONVERGENCE GUARANTEES,0.16419753086419753,"The following theorem establishes a guarantee on the number of iterations necessary for V πout(xy) be
an ε-accurate approximation of the transport cost Wγ(MX , MY; c, x, y) for any x, y.
Theorem 2. Suppose that Sinkhorn Value Iteration is run for K steps with regularization pa-"
CONVERGENCE GUARANTEES,0.1654320987654321,"rameter η =
1
4∥c∥∞ q"
CONVERGENCE GUARANTEES,0.16666666666666666,(1−γ)3 log |X||Y|
CONVERGENCE GUARANTEES,0.16790123456790124,"K
, and initialized with the uniform coupling defined for each"
CONVERGENCE GUARANTEES,0.16913580246913582,"xy, x′y′ as π1(x′y′|xy) =
1
|X||Y|. Then, for any x0y0 ∈XY, the output satisfies V πout(x0y0) ≤
Wγ(MX , MY; c, x0, y0) + ε if the number of iterations is at least"
CONVERGENCE GUARANTEES,0.17037037037037037,"K ≥324 ∥c∥2
∞log |X||Y|
(1 −γ)5ε2
."
CONVERGENCE GUARANTEES,0.17160493827160495,"The proof is relegated to Section C, and we present a similar performance guarantee for SPI in
Appendix D. Importantly, these guarantees technically only hold when setting m = ∞, which is a
limitation we discuss in more detail in Section 6. The condition that π1 is chosen as the uniform
coupling is not necessary and simply made to make the statement easier to state. A more detailed
statement of the bound is provided in Appendix C.4."
EXPERIMENTS,0.1728395061728395,"5
Experiments"
EXPERIMENTS,0.17407407407407408,"We have conducted a range of experiments on some simple environments with the purpose of
illustrating the numerical properties of our algorithms and some aspects of the distance metrics we
studied. Due to space restrictions, we only report a very limited subsample of the results below, and
refer the reader to Appendix F for the complete suite2."
EXPERIMENTS,0.17530864197530865,"One set of experiments we report here addresses the biggest open question left behind our theory:
the effect of the choice of m on the quality of the updates. For this experiment, we use the classic
“4-rooms” environment first studied by Sutton et al. [1999], and run both SVI (Algorithm 1) and SPI
(Algorithm 2) for a range of different choices of m, and a fixed γ = 0.95. The results of this study
are shown in Figure 1. The plots indicate that the estimates produced by both algorithms converge
towards the true distance at a rate that is basically unaffected by m, and in particular even a value
of m = 1 remains competitive. This observation is consistent across all of our experiments. Also,
the output of SPI appears to converge slightly more slowly towards the optimum in this experiment,
but this observation is not entirely consistent and can be likely ascribed to the fact that the learning
rate was not optimized to favor either algorithm in this experiment. In most experiments, the two
algorithms performed very similarly, up to some small occasional differences."
EXPERIMENTS,0.1765432098765432,"We have also conducted a number of experiments to illustrate the potential of optimal-transport
distances for comparing Markov chains of different sizes and transition functions. In the experiment
we show here, we compare two Markov chains illustrated in Figure 2. The first Markov chain MX
is a simple, nine-state “gridworld” environment, which has its initial state in the upper left corner
(denoted as s0) and a reward of +1 in the lower left corner (shown in blue). The second, MY, is an
instance of the 4-rooms environment, where each room is a rotation of the aforementioned small grid.
The transition kernels in both environments are uniform distributions over the adjacent cells in the
four principal directions. The plot shows the distances between the two chains as a function of the
initial state of MY, revealing an intuitive pattern of similarities that captures the symmetries of MY."
EXPERIMENTS,0.17777777777777778,2The code is available at https://github.com/SergioCalo/SVI
EXPERIMENTS,0.17901234567901234,"(a) SVI (Algorithm 1)
(b) SPI (Algorithm 2)"
EXPERIMENTS,0.18024691358024691,"Figure 1: Estimated transport cost as a function k, for various choices of m and η = 1."
EXPERIMENTS,0.1814814814814815,Figure 2: Visual representation of the distances computed between the chains MX and MY.
DISCUSSION,0.18271604938271604,"6
Discussion"
DISCUSSION,0.18395061728395062,We discuss some further aspects of our framework and results below.
DISCUSSION,0.18518518518518517,"Representation learning for reinforcement learning.
Among the numerous applications listed in
Section 1 and Appendix A, the most interesting for us is using our metrics for representation learning
in RL. As mentioned earlier, bisimulation metrics have been extensively used for this purpose in the
past. In particular, almost all such work uses bisimulation metrics to compare states within the same
MDP and use the resulting similarity metrics for merging states that are at low distance (an approach
called “state aggregation”). As our results highlight, this is a rather narrow view of what bisimulation
metrics are capable of: they can define similarity metrics between processes that live on potentially
different state spaces, which in particular can be used to select representations by minimizing the
distance between a high-dimensional process and a set of low-dimensional representations. Curiously,
our LP formulation may allow differentiating the distances with respect to the transition kernels, which
we believe will be an important property for future developments in representation learning for RL."
DISCUSSION,0.18641975308641975,"Limitations of the theory.
In their current form, our theoretical guarantees in Theorems 2 and 6
only apply to perfect projection and evaluation steps, corresponding to setting m = ∞. We conjecture
that this limitation can be addressed with a more careful analysis, and results similar to those of
Theorems 2 and 6 can be shown, potentially at the price of a worse dependence on the effective
horizon 1/(1 −γ) [Scherrer et al., 2012, 2015], by making use of the techniques of Geist et al. [2019]
and Moulin and Neu [2023] for analyzing regularized dynamic-programming algorithms."
DISCUSSION,0.18765432098765433,"From dynamic programming to learning from data.
This paper focuses on computing distances
between known Markov processes via dynamic-programming-style methods. In the most interesting
applications however, the transition kernels are unknown, which requires the development of new
tools. We are confident that our framework can serve as a solid basis for such developments, and in
particular that one can port many ideas from the field of reinforcement learning that is essentially all
about turning dynamic-programming methods into algorithms that can learn from interaction data.
Additionally, we believe that our LP formulation in Section 3 makes it much easier to import further
ideas from computational optimal transport, and in particular that stochastic optimization methods
like those of Genevay et al. [2016] can be adapted to solving our linear programs."
DISCUSSION,0.18888888888888888,Acknowledgments.
DISCUSSION,0.19012345679012346,"G. Neu would like to thank Marin Ballu and Quentin Berthet for clarifying a number of details of
their analysis of Mirror Sinkhorn, Tristan Brugère for pointing us to their publicly available code, and
Anna Korba and Stefan Schrott for kindly helping out with some references about optimal transport
between stochastic processes. G. Neu was supported by the European Research Council (ERC)
under the European Union’s Horizon 2020 research and innovation programme (Grant agreement
No. 950180). Anders Jonsson is partially supported by the EU ICT-48 2020 project TAILOR (No.
952215), AGAUR SGR, and the Spanish grant PID2019-108141GB-I00. Javier Segovia-Aguas is
supported by MCIN/AEI /10.13039/501100011033 under the Maria de Maeztu Units of Excellence
Programme (CEX2021-001195-M)."
REFERENCES,0.19135802469135801,References
REFERENCES,0.1925925925925926,"A. Abate. Approximation metrics based on probabilistic bisimulations for general state-space Markov
processes: a survey. Electronic Notes in Theoretical Computer Science, 297:3–25, 2013."
REFERENCES,0.19382716049382717,"Y. Abbasi-Yadkori, P. Bartlett, K. Bhatia, N. Lazic, C. Szepesvari, and G. Weisz. POLITEX: Regret
bounds for policy iteration using expert prediction. In International Conference on Machine
Learning (ICML), pages 3692–3702, 2019."
REFERENCES,0.19506172839506172,"P. Aczel. Non-well-founded sets. CSLI lecture notes, no. 14, 1988."
REFERENCES,0.1962962962962963,"A. Agarwal, S. M. Kakade, J. D. Lee, and G. Mahajan. On the theory of policy gradient methods:
Optimality, approximation, and distribution shift. Journal of Machine Learning Research, 22(98):
1–76, 2021a."
REFERENCES,0.19753086419753085,"R. Agarwal, M. C. Machado, P. S. Castro, and M. G. Bellemare. Contrastive behavioral similarity
embeddings for generalization in reinforcement learning. In International Conference on Learning
Representations (ICLR), 2021b."
REFERENCES,0.19876543209876543,"J. Altschuler, J. Niles-Weed, and P. Rigollet. Near-linear time approximation algorithms for optimal
transport via Sinkhorn iteration. Advances in Neural Information Processing Systems, 30, 2017."
REFERENCES,0.2,"M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. In International
Conference on Machine Learning (ICML), pages 214–223, 2017."
REFERENCES,0.20123456790123456,"M. G. Azar, V. Gómez, and H. J. Kappen. Dynamic policy programming. Journal of Machine
Learning Research, 13(Nov):3207–3245, 2012."
REFERENCES,0.20246913580246914,"J. Backhoff-Veraguas, M. Beiglbock, Y. Lin, and A. Zalashko. Causal transport in discrete time and
applications. SIAM Journal on Optimization, 27(4):2528–2562, 2017."
REFERENCES,0.2037037037037037,"J. Backhoff-Veraguas, D. Bartl, M. Beiglböck, and M. Eder. All adapted topologies are equal.
Probability Theory and Related Fields, 178:1125–1172, 2020."
REFERENCES,0.20493827160493827,"M. Ballu and Q. Berthet. Mirror Sinkhorn: Fast online optimization on transport polytopes. In
International Conference on Machine Learning (ICML), 2023."
REFERENCES,0.20617283950617285,"D. Bartl and J. Wiesel. Sensitivity of multiperiod optimization problems in adapted wasserstein
distance. arXiv preprint arXiv:2208.05656, 2022."
REFERENCES,0.2074074074074074,"E. Bayraktar and B. Han. Fitted value iteration methods for bicausal optimal transport. arXiv preprint
arXiv:2306.12658, 2023."
REFERENCES,0.20864197530864198,"A. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for convex
optimization. Operations Research Letters, 31(3):167–175, 2003."
REFERENCES,0.20987654320987653,"R. Bellman. Dynamic Programming. Princeton University Press, Princeton, New Jersey, 1957."
REFERENCES,0.2111111111111111,"D. P. Bertsekas. Monotone mappings with application in dynamic programming. SIAM Journal on
Control and Optimization, 15(3):438–464, 1977. doi: 10.1137/0315031."
REFERENCES,0.2123456790123457,"G. Bian and A. Abate. On the relationship between bisimulation and trace equivalence in an
approximate probabilistic context. In Foundations of Software Science and Computation Structures
(FoSSaCS), pages 321–337, 2017."
REFERENCES,0.21358024691358024,"T. Brugère, Z. Wan, and Y. Wang. Distances for Markov chains, and their differentiation. In
International Conference on Algorithmic Learning Theory (ALT), pages 282–336, 2024."
REFERENCES,0.21481481481481482,"X.-R. Cao. Single sample path-based optimization of Markov chains. Journal of optimization theory
and applications, 100:527–548, 1999."
REFERENCES,0.21604938271604937,"P. S. Castro. Scalable methods for computing state similarity in deterministic Markov decision
processes. In AAAI Conference on Artificial Intelligence (AAAI), pages 10069–10076, 2020."
REFERENCES,0.21728395061728395,"P. S. Castro, T. Kastner, P. Panangaden, and M. Rowland. A kernel perspective on behavioural metrics
for Markov decision processes. Transactions on Machine Learning Research, 2022."
REFERENCES,0.21851851851851853,"D. Chen, F. van Breugel, and J. Worrell. On the complexity of computing probabilistic bisimilarity.
In Foundations of Software Science and Computation Structure (FoSSaCS), pages 437–451, 2012."
REFERENCES,0.21975308641975308,"S. Chen, S. Lim, F. Mémoli, Z. Wan, and Y. Wang. Weisfeiler–Lehman meets Gromov–Wasserstein.
In International Conference on Machine Learning (ICML), pages 3371–3416, 2022."
REFERENCES,0.22098765432098766,"C.-Y. Chuang and S. Jegelka. Tree mover’s distance: Bridging graph metrics and stability of graph
neural networks. Advances in Neural Information Processing Systems, 35:2944–2957, 2022."
REFERENCES,0.2222222222222222,"N. Courty, R. Flamary, D. Tuia, and A. Rakotomamonjy. Optimal transport for domain adaptation.
IEEE transactions on pattern analysis and machine intelligence, 39(9):1853–1865, 2016."
REFERENCES,0.2234567901234568,"N. Courty, R. Flamary, and M. Ducoffe. Learning Wasserstein embeddings. In International
Conference on Learning Representations (ICLR), pages 1–13, 2018."
REFERENCES,0.22469135802469137,"M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in Neural
Information Processing Systems, 26, 2013."
REFERENCES,0.22592592592592592,"G. de Ghellinck. Les problèmes de décisions séquentielles. Cahiers du Centre d’Études de Recherche
Opérationnelle, 2:161–179, 1960."
REFERENCES,0.2271604938271605,"E. V. Denardo. On linear programming in a Markov decision problem. Management Science, 16(5):
281–288, 1970."
REFERENCES,0.22839506172839505,"F. d’Epenoux. A probabilistic production and inventory problem. Management Science, 10(1):
98–108, 1963."
REFERENCES,0.22962962962962963,"J. Desharnais, V. Gupta, R. Jagadeesan, and P. Panangaden. Metrics for labeled Markov systems. In
International Conference on Concurrency Theory (CONCUR), pages 258–273, 1999."
REFERENCES,0.2308641975308642,"J. Desharnais, R. Jagadeesan, V. Gupta, and P. Panangaden. The metric analogue of weak bisimulation
for probabilistic processes. In IEEE Symposium on Logic in Computer Science (LICS), pages
413–422, 2002."
REFERENCES,0.23209876543209876,"J. Desharnais, V. Gupta, R. Jagadeesan, and P. Panangaden. Metrics for labelled Markov processes.
Theoretical Computer Science, 318(3):323–354, 2004."
REFERENCES,0.23333333333333334,"S. Eckstein and G. Pammer. Computational methods for adapted optimal transport. The Annals of
Applied Probability, 34(1A):675–713, 2024."
REFERENCES,0.2345679012345679,"E. Even-Dar, S. M. Kakade, and Y. Mansour. Online Markov decision processes. Mathematics of
Operations Research, 34(3):726–736, 2009."
REFERENCES,0.23580246913580247,"N. Ferns, P. Panangaden, and D. Precup. Metrics for finite Markov decision processes. In Uncertainty
in Artificial Intelligence (UAI), pages 162–169, 2004."
REFERENCES,0.23703703703703705,"N. Ferns, P. S. Castro, D. Precup, and P. Panangaden. Methods for computing state similarity in
Markov decision processes. In Uncertainty in Artificial Intelligence (UAI), pages 174–181, 2006."
REFERENCES,0.2382716049382716,"M. Forti and F. Honsell. Set theory with free construction principles. Annali Scuola Normale
Superiore, Pisa, Serie IV X(3):493–522, 1983."
REFERENCES,0.23950617283950618,"A. Galichon. Optimal Transport Methods in Economics. Princeton University Press, 2016."
REFERENCES,0.24074074074074073,"M. Geist, B. Scherrer, and O. Pietquin. A theory of regularized Markov decision processes. In
International Conference on Machine Learning (ICML), pages 2160–2169, 2019."
REFERENCES,0.2419753086419753,"C. Gelada, S. Kumar, J. Buckman, O. Nachum, and M. G. Bellemare. Learning continuous latent
space models for representation learning. In International Conference on Machine Learning
(ICML), 2019."
REFERENCES,0.24320987654320989,"A. Genevay, M. Cuturi, G. Peyré, and F. Bach. Stochastic optimization for large-scale optimal
transport. Advances in neural information processing systems, 29, 2016."
REFERENCES,0.24444444444444444,"A. Giacalone, C.-C. Jou, and S. A. Smolka. Algebraic reasoning for probabilistic concurrent systems.
In IFIP Conference on Programming Concepts and Methods, pages 443–458, 1990."
REFERENCES,0.24567901234567902,"R. Givan, T. Dean, and M. Greig. Equivalence notions and model minimization in Markov decision
processes. Artificial Intelligence, 147:163–223, 2003."
REFERENCES,0.24691358024691357,"P. Hansen-Estruch, A. Zhang, A. Nair, P. Yin, and S. Levine. Bisimulation makes analogies in goal-
conditioned reinforcement learning. In International Conference on Machine Learning (ICML),
pages 8407–8426, 2022."
REFERENCES,0.24814814814814815,"R. A. Howard. Dynamic programming and Markov processes. John Wiley, 1960."
REFERENCES,0.24938271604938272,"N. Jiang. Notes on state abstractions, 2018."
REFERENCES,0.2506172839506173,"N. Jiang. A note on loss functions and error compounding in model-based reinforcement learning.
arXiv preprint arXiv:2404.09946, 2024."
REFERENCES,0.2518518518518518,"B. Jonsson and K. G. Larsen. Specification and refinement of probabilistic processes. In IEEE
Symposium on Logic in Computer Science (LICS), pages 167–183, 1991."
REFERENCES,0.25308641975308643,"S. Kakade. A natural policy gradient. Advances in Neural Information Processing Systems, 14:
1531–1538, 2001."
REFERENCES,0.254320987654321,"S. Kakade and J. Langford. Approximately optimal approximate reinforcement learning. In Interna-
tional Conference on Machine Learning (ICML), pages 267–274, 2002."
REFERENCES,0.25555555555555554,"L. V. Kantorovich. On the translocation of masses. In Dokl. Akad. Nauk. USSR (NS), volume 37,
pages 199–201, 1942."
REFERENCES,0.25679012345679014,"M. Kemertas and A. Jepson. Approximate policy iteration with bisimulation metrics. arXiv preprint
arXiv:2202.02881, 2022a."
REFERENCES,0.2580246913580247,"M. Kemertas and A. Jepson. Approximate policy iteration with bisimulation metrics. Transactions of
Machine Learning Research, 2022b."
REFERENCES,0.25925925925925924,"S. Kolouri, S. R. Park, M. Thorpe, D. Slepcev, and G. K. Rohde. Optimal mass transport: signal
processing and machine-learning applications. IEEE Signal Processing Magazine, 34(4):43–59,
2017."
REFERENCES,0.26049382716049385,"J. B. Kruskal. Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis.
Psychometrika, 29(1):1–27, 1964."
REFERENCES,0.2617283950617284,"K. G. Larsen and A. Skou. Bisimulation through probabilistic testing. In ACM Symposium on
Principles of Programming Languages (POPL), pages 344–352, 1989."
REFERENCES,0.26296296296296295,"R. Lassalle. Causal Transport Plans and Their Monge–Kantorovich Problems. Taylor & Francis,
2018."
REFERENCES,0.2641975308641975,"T. Lattimore and C. Szepesvári. Bandit algorithms. Cambridge University Press, 2020."
REFERENCES,0.2654320987654321,"A. S. Manne. Linear programming and sequential decisions. Management Science, 6(3):259–267,
1960."
REFERENCES,0.26666666666666666,"R. Milner. Communication and Concurrency. Prentice Hall, 1989."
REFERENCES,0.2679012345679012,"A. Moulin and G. Neu. Optimistic planning via regularized dynamic programming. In International
Conference on Machine Learning (ICML), 2023."
REFERENCES,0.2691358024691358,"V. Moulos. Bicausal optimal transport for Markov chains via dynamic programming. In IEEE
International Symposium on Information Theory (ISIT), pages 1688–1693, 2021."
REFERENCES,0.27037037037037037,"A. Nemirovski and D. Yudin. Problem Complexity and Method Efficiency in Optimization. Wiley
Interscience, 1983."
REFERENCES,0.2716049382716049,"G. Neu, A. Jonsson, and V. Gómez. A unified view of entropy-regularized Markov decision processes.
arXiv preprint arXiv:1705.07798, 2017."
REFERENCES,0.27283950617283953,"K. O’Connor, K. McGoff, and A. B. Nobel. Optimal transport for stationary Markov chains via
policy iteration. Journal of Machine Learning Research, 23(1):2175–2226, 2022."
REFERENCES,0.2740740740740741,"D. M. R. Park. Concurrency and automata on infinite sequences. In GI Symposium on Theoretical
Computer Science, volume 104 of Lecture Notes in Computer Science, pages 167–183. Springer,
1981."
REFERENCES,0.27530864197530863,"G. Peyré and M. Cuturi. Computational optimal transport. Foundations and Trends in Machine
Learning, 11(5-6):355–607, 2019."
REFERENCES,0.2765432098765432,"G. C. Pflug. Version-independence and nested distributions in multistage stochastic optimization.
SIAM Journal on Optimization, 20(3):1406–1420, 2010."
REFERENCES,0.2777777777777778,"G. C. Pflug and A. Pichler. A distance for multistage stochastic optimization models. SIAM Journal
on Optimization, 22(1):1–23, 2012."
REFERENCES,0.27901234567901234,"M. L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. Wiley-
Interscience, April 1994."
REFERENCES,0.2802469135802469,"D. Sangiorgi. On the origins of bisimulation and coinduction. ACM Transactions on Programming
Languages and Systems (TOPLAS), 31(4):1–41, 2009."
REFERENCES,0.2814814814814815,"B. Scherrer. On the performance bounds of some policy search dynamic programming algorithms.
arXiv preprint arXiv:1306.0539, 2013."
REFERENCES,0.28271604938271605,"B. Scherrer, V. Gabillon, M. Ghavamzadeh, and M. Geist. Approximate modified policy iteration. In
International Conference on Machine Learning (ICML), pages 1207–1214, 2012."
REFERENCES,0.2839506172839506,"B. Scherrer, M. Ghavamzadeh, V. Gabillon, B. Lesner, and M. Geist. Approximate modified policy
iteration and its application to the game of Tetris. Journal of Machine Learning Research, 16:
1629–1676, 2015."
REFERENCES,0.2851851851851852,"G. Schiebinger, J. Shu, M. Tabaka, B. Cleary, V. Subramanian, A. Solomon, J. Gould, S. Liu, S. Lin,
P. Berube, L. Lee, J. Chen, J. Brumbaug, P. Rigollet, K. Hochedlinger, R. Jaenisch, A. Regev, and
E. S. Lander. Optimal-transport analysis of single-cell gene expression identifies developmental
trajectories in reprogramming. Cell, 176(4):928–943, 2019."
REFERENCES,0.28641975308641976,"Y. Shi, V. De Bortoli, A. Campbell, and A. Doucet. Diffusion Schrödinger bridge matching. Advances
in Neural Information Processing Systems, 36, 2024."
REFERENCES,0.2876543209876543,"R. Sinkhorn and P. Knopp. Concerning nonnegative matrices and doubly stochastic matrices. Pacific
Journal of Mathematics, 21(2):343–348, 1967."
REFERENCES,0.28888888888888886,"Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-based generative
modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020."
REFERENCES,0.29012345679012347,"R. S. Sutton, D. Precup, and S. Singh. Between MDPs and semi-MDPs: A framework for temporal
abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181–211, 1999."
REFERENCES,0.291358024691358,"V. Titouan, N. Courty, R. Tavenard, and R. Flamary. Optimal transport for structured data with
application on graphs. In International Conference on Machine Learning (ICML), pages 6275–
6284, 2019."
REFERENCES,0.29259259259259257,"J. van Benthem. Modal Logic and Classical Logic. Bibliopolis, 1983."
REFERENCES,0.2938271604938272,"F. van Breugel and J. Worrell. An algorithm for quantitative verification of probabilistic transition
systems. In International Conference on Concurrency Theory (CONCUR), pages 336–350, 2001."
REFERENCES,0.29506172839506173,"C. Villani. Optimal transport: old and new, volume 338. Springer, 2009."
REFERENCES,0.2962962962962963,"T. Xu, L. K. Wenliang, M. Munn, and B. Acciaio. COT-GAN: Generating sequential data via causal
optimal transport. Advances in neural information processing systems, 33:8798–8809, 2020."
REFERENCES,0.2975308641975309,"Y. Ye. The simplex and policy-iteration methods are strongly polynomial for the Markov decision
problem with a fixed discount rate. Mathematics of Operations Research, 36(4):593–603, 2011."
REFERENCES,0.29876543209876544,"B. Yi, K. O’Connor, K. McGoff, and A. B. Nobel. Alignment and comparison of directed networks
via transition couplings of random walks. arXiv preprint arXiv:2106.07106, 2021."
REFERENCES,0.3,"A. Zhang, R. McAllister, R. Calandra, Y. Gal, and S. Levine. Invariant representations for reinforce-
ment learning without reconstruction. In International Conference on Learning Representations
(ICLR), 2021."
REFERENCES,0.3012345679012346,"A
Extended discussion of related work"
REFERENCES,0.30246913580246915,"In this appendix we include a discussion of related work that could not be accommodated in the main
text due to space limitations."
REFERENCES,0.3037037037037037,"A.1
Bisimulation"
REFERENCES,0.30493827160493825,"The concept of bisimulation originated independently in modal logic [van Benthem, 1983], computer
science [Park, 1981, Milner, 1989] and set theory [Forti and Honsell, 1983, Aczel, 1988], with firm
roots in fixed-point theory. Bisimulation was originally devised as a tool for determining whether
or not two processes are behaviorally equivalent in the sense that no test can distinguish between
the labels they generate. Being a much less demanding notion of equivalence than isomorphism
(which is generally NP-hard to verify), the notion of bisimulation has had significant impact in
concurrency theory and formal verification of computer systems, and has become a standard tool for
model checking. We refer the reader to the very enjoyable paper of Sangiorgi [2009] for a detailed
history of bisimulation and related concepts."
REFERENCES,0.30617283950617286,"Larsen and Skou [1989] developed a theory of probabilistic bisimulation between stochastic processes.
Their approach is based on comparing interpretations of logical formulas on stochastic processes,
roughly saying that two processes are probabilistically bisimilar if all formulas acting on the sequence
of labels encountered along the corresponding random trajectories follow the same probability
distribution. Their definition can be most simply presented when the two processes live on the same
state space and follow the same transition kernel, but are initialized at two different states. In this
setup, bisimulation reduces to a relation between individual states, which can be described formally
as follows. Given a stationary Markov process MX = (X, PX , ν0,X ) as defined in Section 2 and a
label function L : X →R, a probabilistic bisimulation R is a relation on X × X that satisfies the
following property: two states x and x′ are bisimilar (denoted xRx′) if and only if L(x) = L(x′)
and for each subset C in the partition X \ R induced by R, it holds that
X"
REFERENCES,0.3074074074074074,"x′′∈C
PX (x′′|x) =
X"
REFERENCES,0.30864197530864196,"x′′∈C
PX (x′′|x′)."
REFERENCES,0.30987654320987656,"Jonsson and Larsen [1991] define another similarity notion called “satisfaction relation” based on
couplings, and prove that probabilistic bisimilarity and satisfaction are equivalent notions of similarity.
These works show that bisimulation is indeed an equivalence relation, and that when two processes
are initialized from two states within the same equivalence class (i.e., they are bisimilar), then they
will not only transition to bisimilar states in the next step but will in fact continue to evolve in a way
that is indistinguishable based on the labels (in the sense that they will produce the same distribution
over sequences of labes)."
REFERENCES,0.3111111111111111,"Giacalone et al. [1990], Desharnais et al. [1999, 2004] and van Breugel and Worrell [2001] relax the
restrictive notion of exact probabilistic bisimulation and introduce real-valued pseudometrics that
measure the degree of bisimilarity between two states. These notions rely on real-valued labeling
functions. Giacalone et al. [1990] gives a notion of ε-bisimulation which relaxes the condition
L(x) = L(x′) in the definition of the hard bisimulation relation given above, and only requires
equality to hold up to some ε > 0. Desharnais et al. [1999, 2004] go further and define a genuinely
real-valued extension of bisimulation relations by defining bisimulation metrics as described in the
main text (and particularly Equation (4)). A fixed-point characterization of these bisimulation metrics
was established by van Breugel and Worrell [2001] and Desharnais et al. [2002]. These works show
that the resulting distance notion is in fact a pseudometric, and two processes are bisimilar if and
only if they are at distance zero. This justifies seeing bisimilarity metrics as “soft” extensions of the
binary relation of bisimilarity."
REFERENCES,0.31234567901234567,"Interestingly, the first bisimulation metrics all make use of concepts from optimal transport in some
way or another: Desharnais et al. [1999, 2002] already note that their definition is inspired by the
Wasserstein distance (which they call the “Hutchinson metric”), and the fixed-point characterization of
van Breugel and Worrell [2001] also make use of this distance to compare transition kernels (cf. Equa-
tion 5). Precisely, their definition that we recalled as Equation (4) is admittedly inspired by the Kan-
torovich dual representation of the Wasserstein distance between probability distributions over metric
spaces. To our knowledge, this connection with optimal transport has not been explored further in the
literature, and in particular no “primal” counterpart based on couplings has been discovered so far."
REFERENCES,0.3135802469135803,"Givan et al. [2003] adapt probabilistic bisimulation to Markov decision processes (MDPs), requiring
states (or state-action pairs) to have identical rewards and transition probabilities for two MDPs to be
bisimilar. Ferns et al. [2004, 2006] introduce bisimulation metrics for MDPs, essentially using the
fixed-point characterization of van Breugel and Worrell [2001] as a starting point for their definition.
Once again, their definition makes use of the Wasserstein distance between the transition kernels
(called Kantorovich metric in the paper), but no deeper connection between bisimulation metrics and
optimal transport is discussed."
REFERENCES,0.3148148148148148,"Castro [2020] defines bisimulation metrics for MDPs with respect to a given policy π, which thus
falls back to the standard definition of bisimulation metrics for Markov chains as studied in the works
of Desharnais et al. [1999, 2004] and van Breugel and Worrell [2001]. Kemertas and Jepson [2022b]
adjusts the definition of Ferns et al. [2004, 2006] by replacing the Wasserstein distance with the
entropy-regularized Wasserstein distance proposed by Cuturi [2013], which allows them to apply the
Sinkhorn algorithm to compute bisimulation metrics for MDPs. The resulting approach can be seen
to be nearly identical to the methods proposed by O’Connor et al. [2022] and Brugère et al. [2024]
for approximately solving the fixed-point equations (3) in the context of optimal transport—see
Section A.2 for further discussion of these works."
REFERENCES,0.3160493827160494,"Chen et al. [2012] study the computational complexity of computing bisimulation metrics. Similar
to our work, the authors formulate a linear program that characterizes bisimulation metrics in an
equivalent way to other common definitions, though the linear program has one constraint per
next-state coupling, which makes their LP intractable as stated. They use their LP formulation
as an analytic tool to show the existence of a polynomial-time algorithm to solve the fixed-point
equations (5) (which essentially amounts to Bellman’s value iteration algorithm implemented in the
MDP we describe in Appendix B). Each step of the resulting algorithm solves one optimal-transport
problem per state pair via the network simplex algorithm, which is known to be impractical for
this purpose in comparison with Sinkhorn-style methods [Cuturi, 2013]. In the context of optimal
transport, a closely related linear program has been discovered by Backhoff-Veraguas et al. [2017],
whose framework is more general in that it mostly focuses on general (potentially non-Markovian)
stochastic processes, but with the limitation that only finite horizons are considered. We discuss
further developments on this topic in Section A.2."
REFERENCES,0.3172839506172839,"Finally, Bian and Abate [2017] show that ε-bisimilar Markov processes generate distributions over
finite-length trajectories that are close in total variation distance. Since this distance is a special case
of the Wasserstein distance (with the Hamming metric over sequences as ground metric), this result
can be seen to establish some relation between OT distances and bisimulation, but the link is rather
weak in the sense that no equivalence is shown between the two notions. Indeed, these results only
imply that nearly-bisimilar processes generate nearly-identical trajectory distributions, but the reverse
implication is not shown to hold."
REFERENCES,0.31851851851851853,"A.2
Optimal Transport"
REFERENCES,0.3197530864197531,"Optimal transport [Villani, 2009] studies the problem of transporting mass between two density
functions p and q, given a cost function that measures the transport distance between any pair of
points. In the classic formulation of Kantorovich [1942], the vehicle used to transport mass is
a coupling, that is, a joint probability distribution whose marginals equal p and q. The problem
of finding an optimal coupling that minimizes the total transport distance can be formulated as
a linear program. Historically, the resulting LPs have been solved via standard solvers like the
network simplex method or interior-point methods, which lead to algorithms with polynomial runtime
guarantees but rather poor empirical performance. Cuturi [2013] successfully advocated for adding
entropy regularization to the standard LP objective, which enabled algorithms that are orders of
magnitude faster than previously proposed methods."
REFERENCES,0.32098765432098764,"In this work we consider a problem of optimal transport between stochastic processes with a temporal
dimension. This topic has recently started to receive attention in the OT literature, mostly focusing
on stochastic processes with finite horizon [Pflug and Pichler, 2012, Backhoff-Veraguas et al., 2017,
Lassalle, 2018]. In this setting (often called “adapted transport”, “causal transport”, or “bicausal
transport”), the problem is to transport mass between joint distributions of sequences of elements,
which can be formulated as an optimization problem over the set of causal couplings (i.e., the
set of couplings over joint distributions over sequences that respect the temporal order inherent
in the process). Backhoff-Veraguas et al. [2017] have observed that, due to the linearity of the"
REFERENCES,0.32222222222222224,"causality constraints, this optimization problem can be phrased as a linear program, which however is
infinite-dimensional and thus intractable to solve directly. They complement this view by providing
dynamic-programming principles for characterizing the structure of the optimal coupling, which,
in the special case of Markov processes, boils down to the finite-horizon version of the fixed point
equations (3). This development essentially mirrors the LP formulation and dynamic-programming
principles put forth by Chen et al. [2012] in the context of computing bisimulation metrics (cf. the
discussion in Section A.1)."
REFERENCES,0.3234567901234568,"Still on the front of computing optimal transport distances, a notable contribution is due to Eckstein
and Pammer [2024], who propose and analyze a version of Sinkhorn’s algorithm for optimal transport
on the space of stochastic processes. When specialized to Markov processes, their algorithm can
be seen to be very closely related to ours, the technical explanation being that in finite-horizon
Markov processes the entropy of path distributions that they use as regularization can be seen to be
equal to the conditional entropy that our method uses for the same purpose. The resulting algorithm
performs iterative Bregman projections via backward recursion over the finite time horizon, with
computational steps that are essentially identical to applying our Bellman–Sinkhorn operators. That
said, their analysis relies very heavily on the finite-horizon structure of the problem and as such it is
not applicable in our considerably more challenging infinite-horizon problem setting."
REFERENCES,0.32469135802469135,"The more recent works of Moulos [2021], O’Connor et al. [2022], Bayraktar and Han [2023] and
Brugère et al. [2024] have investigated optimal-transport distances between infinite-horizon Markov
chains. O’Connor et al. [2022] considered the undiscounted version of our problem and proposed to
compute optimal transition couplings via an adaptation of approximate policy iteration (cf. Scherrer
2013) to an appropriately adjusted version of the MDP we describe in Appendix B. Their key
algorithmic idea is approximating the greedy policy update steps by running Sinkhorn’s algorithm
for each pair of states. Essentially the same idea was used by Brugère et al. [2024] to solve the
discounted problem that is the subject of the present paper, with the difference that their method takes
approximate value iteration as its starting point. Both of these approaches are closely related to the
alternative fixed-point definition of bisimulation metrics using Sinkhorn divergences due to Kemertas
and Jepson [2022b], as mentioned in Section A.1. While these approaches are nearly as effective as
our Sinkhorn Value Iteration method in practice, their black-box use of Sinkhorn’s algorithm make
them difficult to analyze theoretically, and difficult to build further theory on."
REFERENCES,0.32592592592592595,"To wrap up, let us mention some results that in a sense have already foreshadowed our observation
about the relation of OT distances and bisimulation metrics. First, we note that Yi et al. [2021],
Brugère et al. [2024] proposed to study optimal transport distances of Markov chains defined over
graphs as a means of studying the similarity of the underlying graphs. The purpose of these works
was to define a notion of distance that is less demanding than isomorphism, but is still grounded in
fundamental theory and can be computed effectively—which is precisely the reason that the notion
of bisimulation was originally introduced in the 1980s in the context of formal verification by Park
[1981] and Milner [1989]. Finally, the work of Backhoff-Veraguas et al. [2020] has established that
“all adapted topologies are equal” on the space of laws of stochastic processes, understood in the sense
that a large number of topologies (including the one induced by optimal-transport metrics defined in
terms of bicausal couplings) are in fact identical. While one may argue with their sweeping claim
that all such topologies are equal, it may not be surprising in light of their results that the topology
induced by bisimulation metrics is also identical to these well-studied topologies (as revealed to be
true by our observations in this paper)."
REFERENCES,0.3271604938271605,"B
Optimal Transport as a Markov Decision Process"
REFERENCES,0.32839506172839505,"Consider the two Markov processes MX and MY with initial states x0 and y0, respectively. To
compute the optimal transport cost Wγ(MX , MY; c, x0, y0) and the optimal transition coupling π∗,
we can introduce a Markov decision process"
REFERENCES,0.3296296296296296,"M = (XY, A, q, γ, c, ν0) where:"
REFERENCES,0.3308641975308642,"• XY is the state space, defined as the set of joint states of MX and MY,"
REFERENCES,0.33209876543209876,"• A(xy) = Πxy is the set of applicable actions in state xy ∈XY, corresponding to the set of
valid couplings of PX (·|x) and PY(·|y),"
REFERENCES,0.3333333333333333,"• q(·|xy, a) = a is the transition probability distribution, which is fully determined by the
action a ∈Πxy,"
REFERENCES,0.3345679012345679,"• γ is the discount factor,"
REFERENCES,0.3358024691358025,"• c : XY →[0, ∞) is the cost function that maps joint states to positive real numbers,"
REFERENCES,0.337037037037037,• ν0 = δx0y0 is the initial state distribution.
REFERENCES,0.33827160493827163,"The objective of the agent in this MDP is to select its sequence of actions A0, A1, . . . in a way that
minimizes the total discounted cost E [P∞
t=0 γtc(Xt, Yt)], where each state pair is drawn according
to the action taken by the agent as (Xt, Yt) ∼At. The sequence of actions is generated by a sequence
of history dependent policies πt ∈ΠHD : Ht →∆A(Xt,Yt) where Ht = (X0, Y0, . . . , Xt, Yt).
A first remark is that we can restrict ourselves to deterministic policies. Indeed, the action set is
convex at every time step and both the reward and the transition probability distributions are linear
in the action. A second remark is that bicausal couplings correspond exactly to history-dependent
deterministic policies. Indeed, by bicausality, the bicausal coupling MXY is generated by the policy
MXY(xn, yn|¯xn−1¯yn−1). We will denote this policy by πMXY."
REFERENCES,0.3395061728395062,"Of special interest are stationary deterministic (or Markovian) policies of the form π : XY →A,
mapping joint states (Xt, Yt) to actions in A(Xt, Yt) as At = π(Xt, Yt). Such policies correspond
exactly with transition couplings as defined in the main text as mappings π : XY →∆XY of the same
type. Accordingly, we will sometimes write π(·|xy) to refer to the distribution π(xy) ∈A(xy) below."
REFERENCES,0.34074074074074073,"B.1
Value functions, optimal policies, and sufficiency of transition couplings"
REFERENCES,0.3419753086419753,"Each policy π induces a value function V π, defined in each state xy ∈XY as"
REFERENCES,0.3432098765432099,"V π(xy) = Eπ "" ∞
X"
REFERENCES,0.34444444444444444,"t=0
γtc(XtYt)"
REFERENCES,0.345679012345679,X0Y0 = xy #
REFERENCES,0.3469135802469136,",
(11)"
REFERENCES,0.34814814814814815,"where the expectation is taken with respect to the stochastic process induced by the policy π. Here,
Xt and Yt are random variables representing the state of the two processes at time t. In particular, we
have that for any bicausal coupling MXY ∈Πbc,
Z
cγ( ¯X, ¯Y )dM ¯
X, ¯Y =
Z
∞
X"
REFERENCES,0.3493827160493827,"t=0
γtc(Xt, Yt)dMXY( ¯X, ¯Y )"
REFERENCES,0.3506172839506173,"= EπMXY "" ∞
X"
REFERENCES,0.35185185185185186,"t=0
γtc(Xt, Yt)"
REFERENCES,0.3530864197530864,X0Y0 = x0y0 #
REFERENCES,0.35432098765432096,"= V πMXY (x0, y0)."
REFERENCES,0.35555555555555557,And as a result we can relate the optimal transport cost to the optimal value function of the MDP
REFERENCES,0.3567901234567901,"Wγ(MX , MY; c, x0, y0) =
inf
MXY∈Πbc"
REFERENCES,0.35802469135802467,"Z
cγ(X, Y ) dMXY(X, Y )"
REFERENCES,0.3592592592592593,"=
inf
MXY∈Πbc V πMXY (x0y0)"
REFERENCES,0.36049382716049383,"=
inf
π∈ΠHD V π(x0y0)."
REFERENCES,0.3617283950617284,"Building on classic results of MDP theory, it can be shown that there exists an optimal Markovian
policy π∗whose value function V ∗= V π∗satisfies V ∗(xy) ≤V π(xy) for all policies π and joint
states xy, and said optimal value function V ∗satisfies the Bellman optimality equations"
REFERENCES,0.362962962962963,"V ∗(xy) = T V ∗(xy),"
REFERENCES,0.36419753086419754,where T is the Bellman operator acting on a function V ∈RXY as
REFERENCES,0.3654320987654321,"T V (xy) = c(xy) + γ inf
p∈Πxy X"
REFERENCES,0.36666666666666664,"x′y′
p(x′y′)V (x′y′)
(∀xy) ."
REFERENCES,0.36790123456790125,"This is precisely the set of equations in Equation (3). Since V ∗is optimal, V ∗(xy) equals the optimal
transport cost Wγ(MX , MY; c, x, y) for each state xy. An optimal policy π∗achieves the infimum
in each state xy, with associated value function V π∗= V ∗. These claims are summarized in the
following theorem, stated in nearly identical form by Moulos [2021].
Theorem 3 (cf. Theorem 1 in Moulos 2021). Under the above conditions, the following hold:"
REFERENCES,0.3691358024691358,"• There exists an optimal Markovian transition coupling π∗such that for any policy π ∈ΠHD
and any joint states xy, V π∗(xy) ≤V π(xy)."
REFERENCES,0.37037037037037035,"• The value function of π∗satisfies Wγ(MX , MY; c, x, y) = V π∗(xy)."
REFERENCES,0.37160493827160496,• There exists a unique solution to the Bellman optimality equation (3) denoted V ∗∈RXY.
REFERENCES,0.3728395061728395,• We have that V ∗= V π∗.
REFERENCES,0.37407407407407406,"The above theorem justifies considering Markovian transition couplings when computing the optimal
transport cost Wγ(MX , MY; c, x0, y0)."
REFERENCES,0.37530864197530867,"Proof. One can straightforwardly check that our setting is an instance of optimal control prob-
lems with additive cost functional studied in Bertsekas [1977].
In particular, the contraction
assumption (Assumption C in the paper mentioned above) is satisfied.
We define pointwise
V ∗(xy) = infπ∈ΠHD V π(xy), and Proposition 1 of Bertsekas [1977] shows that V ∗is the unique
solution to the Bellman optimality equation."
REFERENCES,0.3765432098765432,"Now, for the existence of an optimal stationary policy or transition coupling in our terminology,
an additional technical condition on the action set must be carefully verified. For every xy ∈XY,
ℓ∈[0, ∞) and k, we define the set"
REFERENCES,0.37777777777777777,"Uk(xy, λ) =

a ∈A(xy) : c(x, y) + γ
Z
T kV (x′y′)da(x′y′) ≤λ
"
REFERENCES,0.3790123456790123,"This set is compact as the intersection of the compact set A(xy) with a closed set, the preimage of
a closed set by a continuous application. Knowing that Uk(xy, λ) is compact, we can then apply
Proposition 14 of Bertsekas [1977] and that gives us the existence of an optimal Markovian transition
coupling π∗that satisfies V π∗= V ∗. In particular, for any policy π ∈ΠHD and joint state xy, we
have that V π∗(xy) = V ∗(xy) ≤V π(xy) by definition of V ∗."
REFERENCES,0.3802469135802469,"B.2
Occupancy measures and occupancy couplings"
REFERENCES,0.3814814814814815,"In a finite Markov decision process, occupancy measures express the discounted number of times that
a given state and action are visited on expectation by the controlled stochastic process. This notion is
not meaningfully applicable in the MDP formulation of our optimal-transport problem, given that the
action space is infinite. However, the closely related notion of occupancy coupling can be seen to
play a similar role in that it allows expressing the total-discounted-cost objective as a linear function,
and that the set of valid occupancy couplings can be fully characterized in terms of a finite number of
linear constraints. In what follows, we prove this latter key property of occupancy couplings, stated
as Lemma 1 in the main text."
REFERENCES,0.38271604938271603,"Proof of Lemma 1. We begin by showing that the occupancy coupling µπ associated with any transi-
tion coupling π ∈ΠXY satisfies the following system of equations (sometimes called the “Bellman"
REFERENCES,0.38395061728395063,flow equations”):
REFERENCES,0.3851851851851852,"µπ(xy, x′y′) = π(x′y′|xy)  γ
X"
REFERENCES,0.38641975308641974,"x′′y′′
µπ(x′′y′′, xy) + (1 −γ)ν0(xy) "
REFERENCES,0.38765432098765434,".
(12)"
REFERENCES,0.3888888888888889,"Indeed, this can be shown to follow from the definition of occupancy couplings as"
REFERENCES,0.39012345679012345,"µπ(xy, x′y′) = (1 −γ) ∞
X"
REFERENCES,0.391358024691358,"t=0
γtPπ [XtYt = xy, Xt+1Yt+1 = x′y′]"
REFERENCES,0.3925925925925926,"= (1 −γ) ∞
X"
REFERENCES,0.39382716049382716,"t=0
γtπ(x′y′|xy)Pπ [XtYt = xy]"
REFERENCES,0.3950617283950617,= π(x′y′|xy) 
REFERENCES,0.3962962962962963,"(1 −γ)ν0(xy) + (1 −γ) ∞
X"
REFERENCES,0.39753086419753086,"t=1
γtPπ [XtYt = xy] !"
REFERENCES,0.3987654320987654,= π(x′y′|xy) 
REFERENCES,0.4,"(1 −γ)ν0(xy) + γ
X"
REFERENCES,0.4012345679012346,"x′′y′′
(1 −γ) ∞
X"
REFERENCES,0.4024691358024691,"t=1
γt−1Pπ [Xt−1Yt−1 = x′′y′′, XtYt = xy]  "
REFERENCES,0.40370370370370373,= π(x′y′|xy) 
REFERENCES,0.4049382716049383,"(1 −γ)ν0(xy) + γ
X"
REFERENCES,0.40617283950617283,"x′′y′′
µπ(x′′y′′, xy)  ,"
REFERENCES,0.4074074074074074,"where in the first step we used the stationarity of the transition coupling π, then the definition of ν0,
followed by the law of total probability, and finally the stationarity of the Markov chain that allowed
us to recognize µπ(x′′y′′, xy) in the last step. Now, summing both sides of Equation (12) for all
x′y′, we can confirm that µπ indeed satisfies Equation (6). Furthermore, summing the two sides of
Equation (12) over all x′, we get X"
REFERENCES,0.408641975308642,"x′
µπ(xy, x′y′) =
X"
REFERENCES,0.40987654320987654,"x′
π(x′y′|xy) "
REFERENCES,0.4111111111111111,"(1 −γ)ν0(xy) + γ
X"
REFERENCES,0.4123456790123457,"x′′y′′
µπ(x′′y′′, xy)   =
X"
REFERENCES,0.41358024691358025,"x′
π(x′y′|xy)
X"
REFERENCES,0.4148148148148148,"x′′y′′
µπ(xy, x′′y′′)"
REFERENCES,0.4160493827160494,"= PY(y′|y)
X"
REFERENCES,0.41728395061728396,"x′′y′′
µπ(xy, x′′y′′),"
REFERENCES,0.4185185185185185,"where the first step in the second line follows from Equation 6, and the 4th line comes from the fact
that π(·|xy) is a coupling of PX (·|x) and PY(·|y). This verifies that µπ satisfies Equation (8), and
the same reasoning can be used to verify that it also satisfies Equation 7."
REFERENCES,0.41975308641975306,"Conversely, suppose that µ ∈RXY×XY
+
satisfies Equations (6), (7), and (8). Define νµ(xy) =
P"
REFERENCES,0.42098765432098767,"x′y′ µ(xy, x′y′) and let"
REFERENCES,0.4222222222222222,πµ(x′y′|xy) =
REFERENCES,0.42345679012345677,"(
µ(xy,x′y′)"
REFERENCES,0.4246913580246914,"νµ(xy)
if νµ(xy) ̸= 0,
PX (x′|x)PY(y′|y)
otherwise ."
REFERENCES,0.42592592592592593,"We will verify that πµ defines a valid Markovian coupling and that µ is the state action occupancy
measure of πµ. If νµ(xy) = 0, then P"
REFERENCES,0.4271604938271605,x′ πµ(x′y′|xy) = PY(y′|y) P
REFERENCES,0.4283950617283951,"x′ PX (x′|x) = PY(y′|y). If
νµ(xy) ̸= 0, then we have
X"
REFERENCES,0.42962962962962964,"x′
πµ(x′y′|xy) =
1
νµ(xy) X"
REFERENCES,0.4308641975308642,"x′
µ(xy, x′y′) =
1
νµ(xy) X"
REFERENCES,0.43209876543209874,"x′y′′
µ(xy, x′y′′)PY(y′|y)"
REFERENCES,0.43333333333333335,"=
1
νµ(xy)νµ(xy)PY(y′|y) = PY(y′|y),"
REFERENCES,0.4345679012345679,"where we have used that µ satisfies the constraint of Equation (8). In any case, we have that
P"
REFERENCES,0.43580246913580245,"x′ πµ(x′y′|xy) = PY(y′|y). By symmetry, using that µ satisfies Equation (7), we also have"
REFERENCES,0.43703703703703706,"P
y′ µ(x′y′|xy) = PX (x′|x). Hence we have demonstrated that πµ is a valid Markovian coupling of
PX and PY. To proceed, observe that the occupancy coupling associated with πµ satisfies
X"
REFERENCES,0.4382716049382716,"x′y′
µπµ(xy, x′y′) = γ
X"
REFERENCES,0.43950617283950616,"x′′y′′
µπµ(x′′y′′, xy) + (1 −γ)ν0(xy)."
REFERENCES,0.44074074074074077,"We will verify that µπµ = µ by showing that this system of equations has a unique solution. In order
to see this, let us recall the definition of νµ and reorder Equation (6) as"
REFERENCES,0.4419753086419753,"(1 −γ)ν0(xy) = νµ(xy) −γ
X"
REFERENCES,0.44320987654320987,"x′′y′′
νµ(x′′y′′)πµ(xy|x′′y′′)."
REFERENCES,0.4444444444444444,"Introducing the matrix Z ∈R|X||Y|×|X||Y| with entries Z(xy, x′y′) = πµ(x′y′|xy) and representing
the functions ν0 and νµ in matrix form, this system of equations can be written as"
REFERENCES,0.445679012345679,(1 −γ)ν0 = (I −γZ)νµ.
REFERENCES,0.4469135802469136,"Now, thanks to the Perron–Frobenius theorem, the stochastic matrix Z has spectral radius 1, and thus
(I −γZ) is invertible, meaning that there is a unique solution νµ = (1 −γ)(I −γZ)−1ν0. This in
turn implies that µ = µπµ, thus verifying that µ is indeed an occupancy coupling induced by a valid
transition coupling πµ if it satisfies Equations (6)-(8). This concludes the proof."
REFERENCES,0.44814814814814813,"C
The proof of Theorem 2"
REFERENCES,0.44938271604938274,"The proof is composed of two main parts: showing a bound on the regret of the iterates µ1, µ2, . . . , µK,
and then accounting for the errors incurred when rounding the average iterate µK =
1
K
PK
k=1 µk
to a feasible occupancy coupling. We start by stating some general results that will be useful
throughout the analysis, and then study the two sources of error mentioned above separately. For any
µ ∈RXY×XY, we define Eµ(xy) = P"
REFERENCES,0.4506172839506173,"x′,y′ µ(x′y′, xy), ETµ(xy) = P"
REFERENCES,0.45185185185185184,"x′,y′ µ(xy, x′y′) and"
REFERENCES,0.45308641975308644,πµ(x′y′|xy) =
REFERENCES,0.454320987654321,"(
µ(xy,x′y′)"
REFERENCES,0.45555555555555555,"ETµ(xy) if ETµ(xy) ̸= 0,
PX (x′|x)PY(y′|y) otherwise."
REFERENCES,0.4567901234567901,"In particular, we always have that µ(xy, x′y′) = ETµ(xy)πµ(x′y′|xy). Furthermore, for any π :
XY →∆XY, we will denote by ˜π = ρ(π) the rounded transition coupling obtained by adapting
the rounding procedure of Altschuler et al. [2017], and we will let ρ(µ) denote the corresponding
occupancy coupling µ˜πµ (cf. Section E.2 for the description and analysis of this method). In particular,
˜π will always be a valid transition coupling associated with PX , PY. Finally we define πout = ρ
 
πµK
"
REFERENCES,0.4580246913580247,"and µout = µπout, and we let µ∗be an optimal occupancy coupling achieving the minimum in the
problem formulation of Theorem 1."
REFERENCES,0.45925925925925926,"With these notations, we decompose the overall error of the output as"
REFERENCES,0.4604938271604938,"⟨µout −µ∗, c⟩= ⟨µout −µK, c⟩+ ⟨µK −µ∗, c⟩= ⟨µout −µK, c⟩+ 1 K K
X"
REFERENCES,0.4617283950617284,"k=1
⟨µk −µ∗, c⟩."
REFERENCES,0.46296296296296297,"The first sum on the right-hand side corresponds to the rounding error, and the second one to the so-
called regret of the sequence of iterates µk. This latter sum can be controlled by adapting arguments
from the classic analysis of mirror-descent methods [Nemirovski and Yudin, 1983, Beck and Teboulle,
2003], with some ideas adopted from the Mirror Sinkhorn analysis of Ballu and Berthet [2023] that
will also come in handy for analyzing the rounding errors. We state these tools first below, and then
analyze the two terms in the above decomposition separately."
REFERENCES,0.4641975308641975,"C.1
General tools"
REFERENCES,0.4654320987654321,"We begin with a version of the classic “three-point identity” for mirror-descent methods (e.g.,
Lemma 4.1 of Beck and Teboulle, 2003) adapted to our specific setting that involves alternating
projections to the sets BX and BY. The result is similar to Lemma A.3 of Ballu and Berthet [2023],
which we reprove here with a more standard methodology (as used for proving, e.g., Theorem 28.4
of Lattimore and Szepesvári, 2020)."
REFERENCES,0.4666666666666667,"Lemma 2. Let µ∗∈BX ∩BY be arbitrary. Then,"
REFERENCES,0.4679012345679012,"⟨µk+1 −µ∗, c⟩≤H (µ∗∥µk) −H (µ∗∥µk+1) −H (µk+1∥µk) η
."
REFERENCES,0.4691358024691358,"Proof. We first recall that H is the Bregman divergence associated with the conditional entropy
function C(µ) = P
xy,x′y′ µ(xy, x′y′) log
µ(xy,x′y′)
P"
REFERENCES,0.4703703703703704,"x′′y′′ µ(xy,x′′y′′) (cf. Appendix A.1 in Neu et al. 2017).
For the actual proof, let us consider the case of k odd when µk+1 ∈BX . Then, by definition, we have
that µk+1 is the minimizer of Ψk+1(µ) = η ⟨µ, c⟩+ H (µ∥µk) on this set. Noticing that the gradient
of Ψk+1 at µ is written as ∇Ψk+1(µ) = ηc + ∇C(µ) −∇C(µk), the first-order optimality condition
over the convex set BX implies that the following inequality holds for any µ ∈BX :"
REFERENCES,0.47160493827160493,"⟨ηc + ∇C(µk+1) −∇C(µk), µ −µk+1⟩≥0."
REFERENCES,0.4728395061728395,"In particular, using this result for µ = µ∗(which is indeed in BX ), the claim follows from using the
standard three-point identity of Bregman divergences that states"
REFERENCES,0.4740740740740741,"⟨∇C(µk+1) −∇C(µk), µ∗−µk+1⟩= H (µ∗∥µk) −H (µ∗∥µk+1) −H (µk+1∥µk) ."
REFERENCES,0.47530864197530864,"Repeating the same argument for even rounds (and noticing that the comparator also satisfies µ∗∈BY
as needed for that case) completes the proof."
REFERENCES,0.4765432098765432,"The following standard lemma will also be useful for studying various notions of distances between
occupancy couplings: the total variation distance ∥µ −µ′∥1 = P"
REFERENCES,0.4777777777777778,"xy,x′y′ |µ(xy, x′y′)−µ′(xy, x′y′)|,"
REFERENCES,0.47901234567901235,"the relative entropy D (µ∥µ′) = P
xy,x′y′ µ(xy, x′y′) log µ(xy,x′y′)"
REFERENCES,0.4802469135802469,"µ′(xy,x′y′), and the conditional relative
entropy introduced earlier."
REFERENCES,0.48148148148148145,"Lemma 3. For any two occupancy couplings µ and µ′, we have"
REFERENCES,0.48271604938271606,"1
2 ∥µ −µ′∥2
1 ≤D (µ∥µ′) ≤H (µ∥µ′)"
REFERENCES,0.4839506172839506,"1 −γ
."
REFERENCES,0.48518518518518516,"Proof. The first inequality is Pinsker’s. For proving the second inequality, we let ν = ET µ and
ν′ = ET µ′ be the state occupancies associated with µ and µ′, respectively. Then, we write the
following:"
REFERENCES,0.48641975308641977,"D (µ∥µ′) = D (ν∥ν′) + H (µ∥µ′)
(by the chain rule of the relative entropy)"
REFERENCES,0.4876543209876543,"= D ((1 −γ)ν0 + γEµ∥(1 −γ)ν0 + γEµ′) + H (µ∥µ′)
(using that µ and µ′ are valid occupancy couplings)"
REFERENCES,0.4888888888888889,"≤(1 −γ)D (ν0∥ν0) + γD (Eµ∥Eµ′) + H (µ∥µ′)
(using the joint convexity of the relative entropy)"
REFERENCES,0.4901234567901235,"≤γD (µ∥µ′) + H (µ∥µ′) ,"
REFERENCES,0.49135802469135803,"where the final step follows from using the data-processing inequality for the relative entropy.
Reordering the terms concludes the proof."
REFERENCES,0.4925925925925926,"C.2
Constraint violations"
REFERENCES,0.49382716049382713,"Let us begin with some definitions. First of all we introduce a quantity that measures the extent to
which an occupancy coupling µ violates the transition coherence constraints. Specifically, we will
measure the violations of the X-constraints by"
REFERENCES,0.49506172839506174,"δX (µ) =
X xyx′"
REFERENCES,0.4962962962962963,"νµ(xy)PX (x′|x) −
X"
REFERENCES,0.49753086419753084,"y′
µ(xy, x′y′) "
REFERENCES,0.49876543209876545,and the violations of the Y-constraints by
REFERENCES,0.5,"δY(µ) =
X xyx′"
REFERENCES,0.5012345679012346,"νµ(xy)PY(y′|y) −
X"
REFERENCES,0.5024691358024691,"x′
µ(xy, x′y′) ,"
REFERENCES,0.5037037037037037,and the overall constraint violations will be written as
REFERENCES,0.5049382716049383,δ(µ) = δX (µ) + δY(µ).
REFERENCES,0.5061728395061729,"Note that we have δX (µk) = 0 for odd rounds and δY(µk) = 0 for even rounds by definition of the
updates. We also define the rounding error associated with an occupancy coupling µ as the average
total variation distance between the transition coupling πµ and its rounded counterpart eπµ = ρ(πµ):"
REFERENCES,0.5074074074074074,"∆(µ) =
X"
REFERENCES,0.508641975308642,"xy,x′y′
µ(xy, x′y′) ∥eπµ(·|xy) −πµ(·|xy))∥1 ."
REFERENCES,0.5098765432098765,"The first statement establishes a link between the quality of an occupancy coupling and the occupancy
coupling obtained after rounding the transition coupling to satisfy the constraints."
REFERENCES,0.5111111111111111,"Lemma 4. For any µ satisfying Equation (6), we have"
REFERENCES,0.5123456790123457,"⟨ρ(µ) −µ, c⟩≤∥c∥∞
∆(µ)
(1 −γ)."
REFERENCES,0.5135802469135803,"Proof. Let ˜πµ = ρ(πµ), ˜µ = ρ(µ), νµ = ETµ, and νeµ = ET eµ. Furthermore, let us define the
shorthand notation ν ◦π to denote the composition of the state-pair distribution ν with the transition
coupling π as (ν ◦π)(xy, x′y′) = π(x′y′|xy)ν(xy). Then, we have"
REFERENCES,0.5148148148148148,"∥eµ −µ∥1 = ∥νeµ ◦eπµ −νµ ◦πµ∥1 = ∥νeµ ◦eπµ + νµ ◦eπµ −νµ ◦eπµ −νµ ◦πµ∥1
= ∥(νeµ −νµ) ◦eπµ + νµ ◦(eπµ −πµ)∥1
≤∥(νeµ −νµ) ◦eπµ∥1 + ∥νµ ◦(eπµ −πµ)∥1
(using the triangle inequality)
= ∥νeµ −νµ∥1 + ∆(µ)"
REFERENCES,0.5160493827160494,"(using the definition of ∆)
= γ ∥Eeµ −Eµ∥1 + ∆(µ)
(using νµ = (1 −γ)ν0 + γEµ)
≤γ ∥eµ −µ∥1 + ∆(µ),"
REFERENCES,0.5172839506172839,where in the last step we used that E is non-expansive with respect to the ℓ1-norm. Reordering gives
REFERENCES,0.5185185185185185,∥eµ −µ∥1 ≤∆(µ)
REFERENCES,0.519753086419753,"1 −γ ,"
REFERENCES,0.5209876543209877,and putting everything together proves the claim of the lemma.
REFERENCES,0.5222222222222223,The next result relates the rounding errors to the constraint violations.
REFERENCES,0.5234567901234568,"Lemma 5. For any µ ∈∆XY×XY, we have"
REFERENCES,0.5246913580246914,"1
2∆(µ) ≤δ(µ)"
REFERENCES,0.5259259259259259,"The proof of this result builds on the error analysis of the rounding procedure of Altschuler et al.
[2017], and can be found in Appendix E.2. Finally, the last technical lemma (inspired by Lemma A.4
of Ballu and Berthet, 2023) bounds the rounding errors in terms of the change rate of the occupancy
couplings."
REFERENCES,0.5271604938271605,"Lemma 6. For any k ≥1,"
REFERENCES,0.528395061728395,"δ(µk) ≤2 min(∥µk −µk+1∥1 , ∥µk −µk−1∥1)."
REFERENCES,0.5296296296296297,"Proof. We study the case where the kth update is a BY projection. For this proof, it will be convenient
to introduce the following notation. We define IX : RXY×XY →RXY×X and (with some abuse
of notation), PX : RXY×XY →RXY×X as the linear operators that respectively act on µ via the
assignment (IX µ)(xy, x′) = P
y′ µ(xy, x′y′) and (PX µ)(xy, x′) = P
x′′,y′′ µ(xy, x′′y′′)PX (x′|x).
This allows us to write δX (µ) = ∥(IX −PX)µ∥1, so that we have the expression"
REFERENCES,0.5308641975308642,"δ(µk) = δX (µk) + δY(µk) = ∥IX µk −PX µk∥1 ,"
REFERENCES,0.5320987654320988,"where we have also used δY(µk) = 0 that holds thanks to the fact that k is even. Moreover, the
(k + 1)st update is a BX -projection and thus we have IX µk+1 = PX µk+1. Hence,"
REFERENCES,0.5333333333333333,"δ(µk) = ∥IX µk −PX µk∥1
= ∥IX µk −IX µk+1 + PX µk+1 −PX µk∥1
≤∥IX (µk −µk+1)∥1 + ∥PX (µk+1 −µk)∥1
≤2 ∥µk −µk+1∥1 ,"
REFERENCES,0.5345679012345679,"where we have used the fact that both IX and PX are non-expansions for the ℓ1-norm in the last line,
which follows from the data-processing inequality for the total variation distance. We then conclude
the analysis for the even rounds by replacing µk+1 with µk−1 in the argument above, and repeating
the same reasoning for odd rounds completes the overall proof."
REFERENCES,0.5358024691358024,"Having established these elementary results, we now turn to addressing the main technical hurdle:
bounding the cumulative rounding errors."
REFERENCES,0.5370370370370371,"Theorem 4. The cumulative constraint violations of the iterates produced by Sinkhorn Value Iteration
satisfy K
X"
REFERENCES,0.5382716049382716,"k=1
δ(µk) ≤(1 −γ)H (µ∗∥µ1)"
REFERENCES,0.5395061728395062,"2η ∥c∥∞
+ 16η ∥c∥∞K"
REFERENCES,0.5407407407407407,"(1 −γ)2
."
REFERENCES,0.5419753086419753,"Proof. We start by applying Lemma 6 to show δ(µk) ≤2 ∥µk −µk+1∥1, which reduces our task to
bounding PK
k=1 ∥µk −µk+1∥1. We do this as follows, for any fixed α > 0: K
X"
REFERENCES,0.5432098765432098,"k=1
∥µk −µk+1∥1 ≤αK 2
+ K
X k=1"
REFERENCES,0.5444444444444444,"∥µk −µk+1∥2
1
2α"
REFERENCES,0.5456790123456791,"(by the inequality of arithmetic and geometric means) ≤αK 2
+ K
X k=1"
REFERENCES,0.5469135802469136,D (µk+1∥µk) α
REFERENCES,0.5481481481481482,"(Pinsker’s inequality) ≤αK 2
+ K
X k=1"
REFERENCES,0.5493827160493827,H (µk+1∥µk)
REFERENCES,0.5506172839506173,(1 −γ)α
REFERENCES,0.5518518518518518,"(Lemma 3) ≤αK 2
+ K
X k=1"
REFERENCES,0.5530864197530864,H (µ∗∥µk) −H (µ∗∥µk+1)
REFERENCES,0.554320987654321,"(1 −γ)α
+
η
α (1 −γ) K
X"
REFERENCES,0.5555555555555556,"k=1
⟨c, µ∗−µk+1⟩"
REFERENCES,0.5567901234567901,(Lemma 2) ≤αK
REFERENCES,0.5580246913580247,"2
+ H (µ∗∥µ1) −H (µ∗∥µK+1)"
REFERENCES,0.5592592592592592,"α(1 −γ)
+
η
α (1 −γ) K
X"
REFERENCES,0.5604938271604938,"k=1
⟨c, ρ(µk+1) −µk+1⟩"
REFERENCES,0.5617283950617284,"(since ⟨µ∗, c⟩≤⟨ρ(µk+1), c⟩) ≤αK"
REFERENCES,0.562962962962963,"2
+ H (µ∗∥µ1)"
REFERENCES,0.5641975308641975,"α(1 −γ) +
η ∥c∥∞
α(1 −γ)2 K
X"
REFERENCES,0.5654320987654321,"k=1
∆(µk+1)"
REFERENCES,0.5666666666666667,(Lemma 4) ≤αK
REFERENCES,0.5679012345679012,"2
+ H (µ∗∥µ1)"
REFERENCES,0.5691358024691358,α(1 −γ) + 2η ∥c∥∞
REFERENCES,0.5703703703703704,"α(1 −γ)2 K
X"
REFERENCES,0.571604938271605,"k=1
δ(µk+1)"
REFERENCES,0.5728395061728395,(Lemma 5) ≤αK
REFERENCES,0.5740740740740741,"2
+ H (µ∗∥µ1)"
REFERENCES,0.5753086419753086,α(1 −γ) + 4η ∥c∥∞
REFERENCES,0.5765432098765432,"α(1 −γ)2 K
X"
REFERENCES,0.5777777777777777,"k=1
∥µk −µk+1∥1 ,"
REFERENCES,0.5790123456790124,"where we have finally used that δ(µk+1) ≤2 ∥µk −µk+1∥1 (Lemma 6). Now, we need to make sure
that 4η∥c∥∞"
REFERENCES,0.5802469135802469,"α(1−γ)2 ≤1 to turn this into a meaningful result. In particular, setting α = 8η∥c∥∞"
REFERENCES,0.5814814814814815,"(1−γ)2 guarantees
that the constant in question equals 1"
REFERENCES,0.582716049382716,"2, so that we can reorder the terms to obtain K
X"
REFERENCES,0.5839506172839506,"k=1
∥µk −µk+1∥1 ≤αK + 2H (µ∗∥µ1)"
REFERENCES,0.5851851851851851,"α(1 −γ)
= 8η ∥c∥∞K"
REFERENCES,0.5864197530864198,"(1 −γ)2
+ (1 −γ)H (µ∗∥µ1)"
REFERENCES,0.5876543209876544,"4η ∥c∥∞
."
REFERENCES,0.5888888888888889,This concludes the proof.
REFERENCES,0.5901234567901235,"C.3
Regret analysis"
REFERENCES,0.591358024691358,"In this section, we bound the regret of the iterates produced by Sinkhorn Value Iteration."
REFERENCES,0.5925925925925926,"Theorem 5. The regret of the mirror Sinkhorn procedure satisfies K
X"
REFERENCES,0.5938271604938271,"k=1
⟨µk −µ∗, c⟩≤H (µ∗∥µ1)"
REFERENCES,0.5950617283950618,"η
+ 2 ∥c∥∞."
REFERENCES,0.5962962962962963,Proof. We first apply Lemma 2 to obtain the bound
REFERENCES,0.5975308641975309,"⟨µk −µ∗, c⟩≤H (µ∥µk) −H (µ∥µk+1) −H (µk+1∥µk)"
REFERENCES,0.5987654320987654,"η
+ ⟨c, µk+1 −µk⟩.
(13)"
REFERENCES,0.6,"Adding up both sides for all k = 1, 2, . . . , K, we get K
X"
REFERENCES,0.6012345679012345,"k=1
⟨µk −µ∗, c⟩≤H (µ∥µ1) −H (µ∥µK+1) −PK
k=1 H (µk+1∥µk)
η
+ ⟨c, µK+1 −µ1⟩"
REFERENCES,0.6024691358024692,≤H (µ∥µ1)
REFERENCES,0.6037037037037037,"η
+ 2 ∥c∥∞."
REFERENCES,0.6049382716049383,This concludes the proof.
REFERENCES,0.6061728395061728,"C.4
The proof of Theorem 2"
REFERENCES,0.6074074074074074,"The proof follows from applying the above results to bounding the rounding errors and the regret.
The first of these is handled as follows:"
REFERENCES,0.608641975308642,"⟨µout −µK, c⟩≤∥c∥∞
∆(µK)"
REFERENCES,0.6098765432098765,"1 −γ ≤2 ∥c∥∞
δ(µK) 1 −γ"
REFERENCES,0.6111111111111112,≤2 ∥c∥∞
REFERENCES,0.6123456790123457,"PK
k=1 δ(µk)
K(1 −γ)
≤H (µ∗∥µ1)"
REFERENCES,0.6135802469135803,"Kη
+ 32η ∥c∥2
∞
(1 −γ)3 ,"
REFERENCES,0.6148148148148148,"where the first and second inequalities respectively come from Lemmas 4 and 5, the third one from
the convexity of δ, and the last inequality comes from the application of Theorem 4."
REFERENCES,0.6160493827160494,The regret is then bounded using Theorem 5 as
REFERENCES,0.6172839506172839,"⟨µK −µ∗, c⟩= 1 K K
X"
REFERENCES,0.6185185185185185,"k=1
⟨µk −µ∗, c⟩≤H (µ∗∥µ1)"
REFERENCES,0.6197530864197531,"Kη
+ 2"
REFERENCES,0.6209876543209877,K ∥c∥∞.
REFERENCES,0.6222222222222222,"Putting both bounds together, we obtain"
REFERENCES,0.6234567901234568,"⟨µout −µ∗, c⟩= ⟨µout −µK, c⟩+ ⟨µK −µ∗, c⟩"
REFERENCES,0.6246913580246913,≤2H (µ∗∥µ1)
REFERENCES,0.6259259259259259,"Kη
+ 32η ∥c∥2
∞
(1 −γ)3 + 2 ∥c∥∞ K"
REFERENCES,0.6271604938271605,"Now, to prove the actual claim of the theorem, we now let π1 be the uniform transition coupling and
µ1 be the associated occupancy coupling. In this case, the conditional relative entropy can be upper
bounded as H (µ∗∥µ1) ≤log |X||Y|, and we can further bound"
REFERENCES,0.6283950617283951,"⟨µout −µ∗, c⟩≤2 log |X||Y|"
REFERENCES,0.6296296296296297,"Kη
+ 32η ∥c∥2
∞
(1 −γ)3 + 2 ∥c∥∞ K"
REFERENCES,0.6308641975308642,≤16 ∥c∥∞ s
REFERENCES,0.6320987654320988,"log |X||Y|
K(1 −γ)3 + 2 ∥c∥∞ K"
REFERENCES,0.6333333333333333,≤18 ∥c∥∞ s
REFERENCES,0.6345679012345679,"log |X||Y|
K(1 −γ)3 ,"
REFERENCES,0.6358024691358025,"where the second to last line is obtained by picking η =
1
4∥c∥∞ q"
REFERENCES,0.6370370370370371,(1−γ)3 log |X||Y|
REFERENCES,0.6382716049382716,"K
and the last line is"
REFERENCES,0.6395061728395062,"obtained by noticing that 1 K ≤
q"
REFERENCES,0.6407407407407407,log |X||Y|
REFERENCES,0.6419753086419753,"K(1−γ)3 holds whenever K ≥1, |X| ≥2, |Y| ≥2 and γ ∈(0, 1).
Finally, note that"
REFERENCES,0.6432098765432098,"V πout(x0y0) −Wγ(MX , MY; c, x0, y0) =
1
1 −γ ⟨µout −µ∗, c⟩≤18 ∥c∥∞ s"
REFERENCES,0.6444444444444445,"log |X||Y|
K(1 −γ)5 ."
REFERENCES,0.645679012345679,"Now, it can be directly verified that the right-hand side is indeed at most ε whenever K is greater
than the expression given in the theorem, thus concluding the proof."
REFERENCES,0.6469135802469136,"D
Sinkhorn Policy Iteration"
REFERENCES,0.6481481481481481,"We describe here a simple alternative to Sinkhorn Value Iteration called Sinkhorn Policy Iteration
(SPI). After introducing this method heuristically, we provide a formal performance analysis, and
finally explain its relation to SVI."
REFERENCES,0.6493827160493827,"The core concept underlying the definition of SPI is the notion of Q-functions, defined analogously
to action-value functions in an MDP. The Q-function associated with a transition coupling π is a
function Qπ : XY × XY →R, with each of its entries defined as"
REFERENCES,0.6506172839506172,"Qπ(xy, x′y′) = E. "" ∞
X"
REFERENCES,0.6518518518518519,"t=0
γtc(Xt, Yt)"
REFERENCES,0.6530864197530865,"(X0, Y0) = (x, y), (X1, Y1) = (x′y′) #"
REFERENCES,0.654320987654321,"Analogously to the results presented in Section B.1, it is possible to show that the Q-function of a
given transition coupling π satisfies the Bellman equations"
REFERENCES,0.6555555555555556,"Qπ(xy, x′y′) = c(xy) + γ
X"
REFERENCES,0.6567901234567901,"x′′y′′
π(x′′y′′|x′y′)Qπ(x′y′, x′′, y′′),"
REFERENCES,0.6580246913580247,"and that the Q-value function of the optimal transition coupling Q∗= Qπ∗satisfies the Bellman
optimality equations"
REFERENCES,0.6592592592592592,"Q∗(xy, x′y′) = c(xy) + γ
inf
p∈Πx′y′ X"
REFERENCES,0.6604938271604939,"x′′y′′
p(x′′y′′)Q∗(x′y′, x′′, y′′)."
REFERENCES,0.6617283950617284,"These are respectively the fixed points of the Bellman operator T π : RXY×XY →RXY×XY defined
via
(T πf)(xy, x′y′) = c(xy) + γ
X"
REFERENCES,0.662962962962963,"x′′y′′
π(x′′y′′|x′y′)f(x′y′, x′′, y′′)"
REFERENCES,0.6641975308641975,and the Bellman optimality operator T : RXY×XY →RXY×XY defined via
REFERENCES,0.6654320987654321,"(T f)(xy, x′y′) = c(xy) + γ
inf
p∈Πx′y′ X"
REFERENCES,0.6666666666666666,"x′′y′′
p(x′′y′′)f(x′y′, x′′, y′′)."
REFERENCES,0.6679012345679012,"The system of equations T Q∗= Q∗is essentially as hard to solve (or even harder) than the Bellman
optimality equations for V ∗stated earlier. However, one can develop an algorithmic approach toward
finding an optimal transition coupling by drawing inspiration from the literature on regularized
dynamic programming."
REFERENCES,0.6691358024691358,"In particular, we develop below an analogue of an entropy-regularized policy iteration scheme that
is known under many names in the RL literature: Natural Policy Gradients by Kakade [2001] and
Agarwal et al. [2021a], MDP-Expert by Even-Dar et al. [2009], Mirror-Descent Policy Iteration by
Geist et al. [2019], POLITEX by Abbasi-Yadkori et al. [2019], Policy Mirror Descent by Agarwal
et al. [2021a], and the list goes on. This method can be directly adapted to our setting as follows.
Starting from an arbitrary initial transition coupling π1, SPI performs the following sequence of
updates for each k = 1, 2, . . . , K:"
REFERENCES,0.6703703703703704,"• Round the transition coupling πk to eπk = ρ(πk),"
REFERENCES,0.671604938271605,"• update the Q-function by solving the fixed-point equation Qk = T eπkQk,
• if k is odd, then update the transition coupling as"
REFERENCES,0.6728395061728395,"πk+1(x′y′|xy) =
πk(x′y′|xy) exp(−ηQk(xy, x′y′))
P"
REFERENCES,0.674074074074074,"y′′ πk(x′y′′|xy) exp(−ηQk(xy, x′y′′))PX (x′|x),"
REFERENCES,0.6753086419753086,"• else if k is even, then update the transition coupling as"
REFERENCES,0.6765432098765433,"πk+1(x′y′|xy) =
πk(x′y′|xy) exp(−ηQk(xy, x′y′))
P
x′′ πk(x′′y′|xy) exp(−ηQk(xy, x′′y′))PY(y′|y)."
REFERENCES,0.6777777777777778,"As in the case of SVI, it is easy to verify that these transition couplings satisfy the required marginal
constraints. Furthermore, one can verify that the updates defined above exactly correspond to running
an instance of Mirror Sinkhorn [Ballu and Berthet, 2023] in each state-pair xy with the sequence
of cost functions Q1(xy, ·), Q2(xy, ·), . . . , QK(xy, ·), similarly how the entropy-regularized policy
iteration methods run an instance of entropic mirror descent in each state. We discuss various aspects
of this algorithm below."
REFERENCES,0.6790123456790124,"D.1
Practical implementation"
REFERENCES,0.6802469135802469,"Algorithm 2: Sinkhorn Policy Iteration
Input: PX , PY, c, η, γ, K, m
Initialise: π1 ←PX ⊗PY;
for k = 1, ..., K −1 do"
REFERENCES,0.6814814814814815,"eπk ←ρ(πk);
Q ←
 
T eπkm Q;
πk+1 ←update(πk, Q);
{Equation 10}
end
µout ←1"
REFERENCES,0.682716049382716,"K
PK
k=1 eµk;
πout ←πµout;
V πout ←evaluate(πout);
Output: πout, V πout
{Final coupling}"
REFERENCES,0.6839506172839506,"Like SVI, this method can be seen as perform-
ing online Mirror Sinkhorn updates in each state
pair xy with a sequence of cost functions Qk,
which are computed via solving the linear sys-
tem of Bellman equations Qk = T eπkQk. The
occupancy couplings produced by SPI are de-
noted by eµk = µeπk, and the final output of the
method is produced by averaging these occupan-
cies as µout = 1"
REFERENCES,0.6851851851851852,"K
PK
k=1 eµk, and then extracting
the transition coupling πout = πµout. Notably,
there is no need to round this transition coupling
since each eµk satisfies all constraints by con-
struction, and so does their average µout due to
the linearity of the constraints."
REFERENCES,0.6864197530864198,"One advantage of SPI over SVI is that finding an exact solution for the fixed-point equation Qk =
T πkQk is easier than computing the fixed points required by SVI, thanks to the fact that this is a
linear system of equations. A downside of the method is that it requires to run the rounding procedure
after each update, at least for the theoretical guarantees to remain valid. The impact of these steps
may however be negligible in practical implementations. Similarly to SVI, the ideal updates of SPI
can be approximated by applying the Bellman operator to the Q-functions only a small number of
times m, with m = ∞corresponding to the ideal implementation analyzed above. We present a
pseudocode for SPI as Algorithm 2."
REFERENCES,0.6876543209876543,"D.2
Convergence guarantees"
REFERENCES,0.6888888888888889,"In what follows, we show the following performance guarantee for SPI."
REFERENCES,0.6901234567901234,Theorem 6. Suppose that Sinkhorn Policy Iteration is run for K steps with regularization pa-
REFERENCES,0.691358024691358,"rameter η =
1−γ
3∥c∥∞ q"
REFERENCES,0.6925925925925925,8 log |X||Y|
REFERENCES,0.6938271604938272,"K
, and initialized with the uniform coupling defined for each"
REFERENCES,0.6950617283950618,"xy, x′y′ as π1(x′y′|xy) =
1
|X||Y|. Then, for any x0y0 ∈XY, the output satisfies V πout(x0y0) ≤
Wγ(MX , MY; c, x0, y0) + ε if the number of iterations is at least"
REFERENCES,0.6962962962962963,"K ≥5 ∥c∥2
∞log |X||Y|
(1 −γ)4ε2
."
REFERENCES,0.6975308641975309,"As the analyses of all regularized policy iteration methods listed above, this one also starts with
establishing the following claim that corresponds to the classic performance difference lemma (often
attributed to Kakade and Langford, 2002, but proposed much earlier in works like Cao, 1999 and
even Howard, 1960). To state the result, we define Vk(xy) = P"
REFERENCES,0.6987654320987654,"x′y′ eπk(x′y′|xy)Qk(xy, x′y′) and
the operators E+ : RXY×XY →RXY and E−: RXY×XY →RXY with each element given by
(E−V )(xy, x′y′) = V (xy) and (E+V )(xy, x′y′) = V (x′y′). Then, the following bound holds on
the instantaneous regret of SPI in round k."
REFERENCES,0.7,"Lemma 7. ⟨eµk −µ∗, c⟩= ⟨µ∗, E−Vk −Qk⟩."
REFERENCES,0.7012345679012346,"Proof. The proof follows from elementary properties of occupancy couplings. First, note that by the
definition of the value function Vk, we have"
REFERENCES,0.7024691358024692,"⟨eµk, c⟩= (1 −γ) ⟨ν0, Vk⟩."
REFERENCES,0.7037037037037037,"Furthermore, by multiplying both sides of the Bellman equations Qk = c + γE+Vk with µ∗and
using the flow constraints ET
+µ∗= γET
−µ∗+ (1 −γ)ν0, we obtain"
REFERENCES,0.7049382716049383,"⟨µ∗, Qk⟩= ⟨µ∗, c + γE+Vk⟩= ⟨µ∗, c + E−Vk⟩−(1 −γ) ⟨ν0, Vk⟩."
REFERENCES,0.7061728395061728,The result follows after reordering the terms.
REFERENCES,0.7074074074074074,"Given the above lemma, we can readily express the regret of SPI as follows: K
X"
REFERENCES,0.7086419753086419,"k=1
⟨eµk −µ∗, c⟩= K
X"
REFERENCES,0.7098765432098766,"k=1
⟨µ∗, E−Vk −Qk⟩ =
X"
REFERENCES,0.7111111111111111,"xy
ν∗(xy) K
X"
REFERENCES,0.7123456790123457,"k=1
⟨π∗(·|xy) −eπk(·|xy), Qk(xy, ·)⟩,"
REFERENCES,0.7135802469135802,"where we can recognize the regret of Mirror Sinkhorn in each state pair xy ∈XY. Thus, applying
the bound of Theorem 3.1 of Ballu and Berthet [2023] to each of these terms (while noting that
∥Qk∥∞≤∥c∥∞/ (1 −γ) holds for all k) gives K
X"
REFERENCES,0.7148148148148148,"k=1
⟨π∗(·|xy) −eπk(·|xy), Qk(xy, ·)⟩≤DKL (π∗(·|xy)∥π1(·|xy))"
REFERENCES,0.7160493827160493,"η
+ 9η ∥c∥2
∞K
8(1 −γ)2 ,"
REFERENCES,0.717283950617284,and thus putting the bounds together we obtain
REFERENCES,0.7185185185185186,"⟨µout −µ∗, c⟩= 1 K K
X"
REFERENCES,0.7197530864197531,"k=1
⟨eµk −µ∗, c⟩≤H (µ∗∥µ1)"
REFERENCES,0.7209876543209877,"ηK
+ 9η ∥c∥2
∞
8(1 −γ)2"
REFERENCES,0.7222222222222222,"Now, setting π1 as the uniform coupling, we can further upper bound the conditional relative entropy"
REFERENCES,0.7234567901234568,"as H (µ∗∥µ1) ≤log |X||Y|, and after setting η =
1−γ
3∥c∥∞ q"
REFERENCES,0.7246913580246913,8 log |X||Y|
REFERENCES,0.725925925925926,"K
, the bound becomes"
REFERENCES,0.7271604938271605,"⟨µout −µ∗, c⟩≤6 ∥c∥∞ 1 −γ r"
REFERENCES,0.7283950617283951,log |X||Y|
K,0.7296296296296296,"8K
."
K,0.7308641975308642,"Finally, note that"
K,0.7320987654320987,"V πout(x0y0) −Wγ(MX , MY; c, x0, y0) =
1
1 −γ ⟨µout −µ∗, c⟩≤6 ∥c∥∞"
K,0.7333333333333333,(1 −γ)2 r
K,0.7345679012345679,log |X||Y|
K,0.7358024691358025,"8K
."
K,0.737037037037037,Using a crude upper bound 36/8 ≤5 verifies the claim of Theorem 6.
K,0.7382716049382716,"D.3
Relation to Sinkhorn Value Iteration"
K,0.7395061728395061,"While on the surface, SPI may seem only loosely related to SVI, a closer connection can be drawn
by making the following observations. First, observe that the transition-coupling updates exactly
match the updates of Sinkhorn Value Iteration, although there is an apparent difference in how the
Q-functions are defined. To expose the similarity between the two methods better, let us consider
an even round in which πk satisfies the X-marginal conditions P"
K,0.7407407407407407,"y′ πk(x′y′|xy) = P(x′|x). Thus,"
K,0.7419753086419754,"introducing the notation VX (xy, x′) = P"
K,0.7432098765432099,"y′
πk(x′y′|xy)"
K,0.7444444444444445,"P (x′|x)
Qk(xy, x′y′), we can multiply the Bellman
equations by πk(x′y′|xy)/P(x′|x) and sum them up to obtain"
K,0.745679012345679,"VX (xy, x′) =
X y′"
K,0.7469135802469136,πk(x′y′|xy)
K,0.7481481481481481,P(x′|x) 
K,0.7493827160493827,"c(xy) + γ
X"
K,0.7506172839506173,"x′′y′′
πk(x′′y′′|x′y′)Qk(x′y′, x′′y′′)   =
X y′"
K,0.7518518518518519,πk(x′y′|xy)
K,0.7530864197530864,P(x′|x) 
K,0.754320987654321,"c(xy) + γ
X"
K,0.7555555555555555,"x′′
P(x′′|x′)VX (x′y′, x′′) ! ≈−1"
K,0.7567901234567901,"η log
X y′"
K,0.7580246913580246,πk(x′y′|xy)
K,0.7592592592592593,"P(x′|x)
exp  −η "
K,0.7604938271604939,"c(xy) + γ
X"
K,0.7617283950617284,"x′′
P(x′′|x′)VX (x′y′, x′′) !! ,"
K,0.762962962962963,"where the approximation is accurate as η approaches zero. Thus, for small values of η, the Sinkhorn–
Bellman operator used by Sinkhorn Value Iteration is an accurate approximation of the Bellman opera-
tor used by Sinkhorn Policy Iteration, and thus one may reasonably expect their respective fixed points
to be close as well (this intuition may, however easily fail and is not necessary for our analysis above)."
K,0.7641975308641975,"E
Auxiliary proofs and technical results"
K,0.7654320987654321,In this appendix we prove several technical results from the main text.
K,0.7666666666666667,"E.1
Proof of Proposition 1"
K,0.7679012345679013,"We prove the claim by showing that the solution of the constrained optimization problem of Equa-
tion (9) is equivalent to the transition-coupling update rule specified in Equation (10). To this end, we
study the Lagrangian of the optimization problem (9) corresponding to the update for odd rounds, BX ,
and note that the update rule for even rounds, BY, can be worked out analogously. By introducing
Lagrange multipliers VX (xy, x′) for each constraint in BX , we obtain the Lagrangian"
K,0.7691358024691358,"L(µ; VX ) = ⟨µ, c⟩+ 1"
K,0.7703703703703704,"η H (µ∥µk) +
X"
K,0.7716049382716049,"xy,x′
VX (xy, x′)    γ
X"
K,0.7728395061728395,"x′′y′′
µ(x′′y′′, xy) + (1 −γ)ν0(xy) "
K,0.774074074074074,"PX (x′|x) −
X"
K,0.7753086419753087,"y′
µ(xy, x′y′)   =
X"
K,0.7765432098765432,"xy,x′y′
µ(xy, x′y′) "
K,0.7777777777777778,"c(xy) + γ
X"
K,0.7790123456790123,"x′′
PX (x′′|x′)VX (x′y′, x′′) −VX (xy, x′) !"
K,0.7802469135802469,"+ (1 −γ)
X"
K,0.7814814814814814,"xy,x′
ν0(xy)PX (x′|x)VX (xy, x′) + 1"
K,0.782716049382716,η H (µ∥µk) .
K,0.7839506172839507,"A quick calculation (cf. Appendix A.1 of Neu et al., 2017) shows that the derivative of H (µ∥µk)
satisfies"
K,0.7851851851851852,"∂H (µ∥µk)
∂µ(xy, x′y′) = log πµ(x′y′|xy) −log πµk(x′y′|xy) = log πµ(x′y′|xy) −log πk(x′y′|xy),"
K,0.7864197530864198,"where we have used that πµk = πk holds by definition of µk and πk. To proceed, for a fixed VX , we
set the gradient of the Lagrangian to zero and solve for the transition coupling πk+1, which gives"
K,0.7876543209876543,πk+1(x′y′|xy) = πk(x′y′|xy) exp  −η 
K,0.7888888888888889,"c(xy) + γ
X"
K,0.7901234567901234,"x′′
PX (x′′|x′)VX (x′y′, x′′) −VX (xy, x′) !! ."
K,0.7913580246913581,"Then, the correct choice of VX has to be such that the constraint P"
K,0.7925925925925926,"y′ πk+1(x′y′|xy) = PX (x′|x) is
satisfied. To see this, suppose that this condition is indeed verified and that µk+1 is the occupancy
coupling associated with πk+1. We need to show that µk+1 indeed verifies the condition defining BX .
To this end, notice that µk+1 satisfies Equation (6), and thus the condition can be simply written as X"
K,0.7938271604938272,"y′
µk+1(xy, x′y′) =  X"
K,0.7950617283950617,"x′′y′′
µk+1(xy, x′′y′′) "
K,0.7962962962962963,"PX (x′|x),"
K,0.7975308641975308,"which, after recalling the relation πk+1(x′y′|xy) =
µ(xy,x′y′)
P"
K,0.7987654320987654,"x′′y′′ µ(xy,x′′y′′) can be indeed seen to hold if
P"
K,0.8,y′ πk+1(x′y′|xy) = PX (x′|x) is true.
K,0.8012345679012346,"Enforcing this constraint gives the following expression for VX (xy, x′):"
K,0.8024691358024691,"VX (xy, x′) = −1"
K,0.8037037037037037,"η log
X y′"
K,0.8049382716049382,πk(x′y′|xy)
K,0.8061728395061728,"PX (x′|x)
exp  −η "
K,0.8074074074074075,"c(xy) + γ
X"
K,0.808641975308642,"x′′
PX (x′′|x′)VX (x′y′, x′′) !! ."
K,0.8098765432098766,"To conclude, we need to ensure that this system of equations has a unique solution. In order to do
this, we recall the definition of the Bellman–Sinkhorn operator T πk
X
: RXY×X →RXY×X , acting on
a function f as"
K,0.8111111111111111,"(T πk
X f)(x′y′, x′′) = −1"
K,0.8123456790123457,"η log
X y′"
K,0.8135802469135802,πk(x′y′|xy)
K,0.8148148148148148,"PX (x′|x)
exp  −η "
K,0.8160493827160494,"c(xy) + γ
X"
K,0.817283950617284,"x′′
PX (x′′|x′)f(x′y′, x′′) !! ."
K,0.8185185185185185,"With this notation, we can directly verify that VX satisfies T πk
X VX = VX . Furthermore, it can
be shown that the operator T πk
X
is a γ-contraction in supremum norm (cf. Lemma 8), and thus
it has a unique fixed point by the Banach fixed-point theorem. We finally note that defining
Qk(xy, x′y′) = (c(xy) + γ P"
K,0.8197530864197531,"x′′ PX (x′′|x′)VX (x′y′, x′′)), the transition-coupling update derived
above can be rewritten as"
K,0.8209876543209876,"πk+1(x′y′|xy) =
πk(x′y′|xy) exp (−ηQk(xy, x′y′))
P"
K,0.8222222222222222,"y′ πk(x′y′′|xy) exp (−ηQk(xy, x′y′′))PX (x′|x).
(14)"
K,0.8234567901234567,This concludes the proof.
K,0.8246913580246914,"E.2
Rounding procedure and proof of Lemma 5"
K,0.825925925925926,"Algorithm 3: Rounding procedure for couplings
Input: approximate coupling F, margins p, q
X ←diag(min(p/(F · 1), 1));
F ′ ←XF;
Y ←diag(min(q/(F ′⊤· 1), 1));
F ′′ ←F ′Y ;
errp = p −F ′′ · 1, errq = q −F ′′⊤· 1;
Output: G ←F ′′ + errperr⊤
q / ∥errp∥1"
K,0.8271604938271605,"We adapt the rounding procedure stated as Al-
gorithm 2 of Altschuler et al. [2017] and repro-
duced here as Algorithm 3 (where / denotes
element-wise division in the pseudocode). For-
mally, for two probability distributions p ∈
∆(X) and q ∈∆(Y), the set of valid cou-
plings is Up,q = {P ∈RXY
+
: P · 1 = p;
P T · 1 = q}. For a nonnegative matrix F ∈
RXY
+ , the rounding procedure outputs a valid
coupling ρ(F, p, q) ∈Up,q which, by Lemma 7
of Altschuler et al. [2017], satisfies
∥ρ(F, p, q) −F∥1 ≤2
 
∥F · 1 −p∥1 +
F T · 1 −q

1

."
K,0.828395061728395,"We will now define the rounding procedure for a (not necessarily valid) transition coupling π ∈
RXY×XY by using the aforementioned rounding procedure at each state pair as
˜π(·|xy) = ρ(π(·|xy), PX (·|x), PY(·|y)).
With some abuse of notation, we will write the resulting transition coupling as eπ = ρ(π) and the
associated occupancy coupling as eµ = µeπ = ρ(µ). Because of the correctness of the original
rounding procedure, this transition coupling is valid, and so is the associated occupancy coupling.
We can now proceed to the proof of Lemma 5."
K,0.8296296296296296,"Proof of Lemma 5. Let µ ∈RXY×XY
+
, and as before define νµ(xy) = P"
K,0.8308641975308642,"x′y′ µ(xy, x′y′) and"
K,0.8320987654320988,πµ(x′y′|xy) =
K,0.8333333333333334,"(
µ(xy,x′y′)"
K,0.8345679012345679,"νµ(xy)
if νµ(xy) ̸= 0,
PX (x′|x)PY(y′|y) otherwise."
K,0.8358024691358025,"For arbitrary state pairs xy, we use Lemma 7 of Altschuler et al. [2017] to obtain that"
K,0.837037037037037,∥˜πµ(·|xy) −πµ(·|xy)∥1 ≤2  X x′
K,0.8382716049382716,"PX (x′|x) −
X"
K,0.8395061728395061,"y′
πµ(x′y′|xy) +
X y′"
K,0.8407407407407408,"PY(y′|y) −
X"
K,0.8419753086419753,"x′
πµ(x′y′|xy)   ."
K,0.8432098765432099,"Now, multiplying by νµ(xy) and summing over xy, we get
X"
K,0.8444444444444444,"xy
νµ(xy) ∥˜πµ(·|xy) −πµ(·|xy)∥1 ≤2  X"
K,0.845679012345679,"xyx′
νµ(xy)"
K,0.8469135802469135,"PX (x′|x) −
X"
K,0.8481481481481481,"y′
πµ(x′y′|xy) +
X"
K,0.8493827160493828,"xyy′
νµ(xy)"
K,0.8506172839506173,"PY(y′|y) −
X"
K,0.8518518518518519,"x′
πµ(x′y′|xy)    = 2  X xyx′"
K,0.8530864197530864,"νµ(xy)PX (x′|x) −
X"
K,0.854320987654321,"y′
µ(xy, x′y′) +
X xyy′"
K,0.8555555555555555,"νµ(xy)PY(y′|y) −
X"
K,0.8567901234567902,"x′
µ(xy, x′y′)   "
K,0.8580246913580247,"= 2δX (µ) + δY(µ) = 2δ(µ),
where we used the fact that µ(xy, x′y′) = νµ(xy)πµ(x′y′|xy) for any xy, x′y′, and the definitions δX ,
δY and δ. Finally, we notice that ∆(µ) = P"
K,0.8592592592592593,"xy νµ(xy) ∥˜πµ(·|xy) −πµ(·|xy)∥1, which concludes
the proof."
K,0.8604938271604938,"E.3
The contraction property of the Bellman–Sinkhorn operator"
K,0.8617283950617284,"Lemma 8. Let π : XY →∆XY be arbitrary and consider the associated Bellman–Sinkhorn
operator T π acting on a function f : XY × Y →R as"
K,0.8629629629629629,"(T πf)(x′y′, x′′) = −1"
K,0.8641975308641975,"η log
X y′"
K,0.8654320987654321,π(x′y′|xy)
K,0.8666666666666667,"P(x′|x)
exp  −η "
K,0.8679012345679012,"c(xy) + γ
X"
K,0.8691358024691358,"x′′
P(x′′|x′)f(x′y′, x′′) !! ."
K,0.8703703703703703,"Then, T π is a γ-contraction for the supremum norm ∥·∥∞, that is, for any two functions f1, f2 :
XY × X, we have
∥T πf1 −T π(f2)∥∞≤γ ∥f1 −f2∥∞."
K,0.8716049382716049,"Proof. The claim easily follows from using the standard fact that the function gp(z)
=
log P"
K,0.8728395061728395,"y′ p(y′)ez(y′) is 1-smooth with respect to the supremum norm, so that for any two vec-
tors z, we have |gp(z) −gp(z)| ≤∥z −z′∥∞.
To apply this result, we define q(y′|xy) =
π(x′y′|xy)/P(x′|x) and z1(xy, x′y′) = c(xy) + γ P"
K,0.8740740740740741,"x′′ P(x′′|x′)f1(x′y′, x′′) and z2(xy, x′y′) =
c(xy) + γ P"
K,0.8753086419753087,"x′′ P(x′′|x′)f2(x′y′, x′′), so that we can write"
K,0.8765432098765432,"∥T πf1 −T πf2∥∞= max
xy,x′
gq(·|xy)(z1(xy, x′·)) −gq(·|xy)(z2(xy, x′·))"
K,0.8777777777777778,"≤max
xy,x′ ∥z1(xy, x′·) −z2(xy, x′·)∥∞= ∥z1 −z2∥∞≤γ ∥f1 −f2∥∞,"
K,0.8790123456790123,where the last step follows from the straightforward calculation
K,0.8802469135802469,"∥z1 −z2∥∞= sup
xy,x′y′ X"
K,0.8814814814814815,"x′′
P(x′′|x′) |f1(x′y′, x′′) −f2(x′y′, x′′)| ≤∥f1 −f2∥∞."
K,0.8827160493827161,This concludes the proof.
K,0.8839506172839506,"F
Additional experimental results"
K,0.8851851851851852,In this appendix we present the results of additional experiments not included in the main text.
K,0.8864197530864197,"F.1
Impact of the regularization parameter η"
K,0.8876543209876543,"Besides the parameter m that we have already studied experimentally in Section 5, the only tuning
parameter of SVI and SPI is the regularization parameter η, which takes the role of a learning rate. In
this experiment, we study two random walks run on two separate 4-room environments [Sutton et al.,
1999], with two separate reward functions rX and rY that together define the ground cost function
c(x, y) = |rX (x) −rY(y)| for each state pair. The results of this study for K = 2 · 104 iterations are
shown in Figure 3. The error is computed as the difference between the distance estimate produced by
the algorithm and a near-optimal distance obtained by running Algorithm 1 for a very small value of
η and a large number of iterations. We observe that higher values of η lead to faster error reduction in
the initial steps, but eventually prevent convergence to the true solution. In contrast, choosing smaller
learning rates enables convergence to better solutions, at the cost of making slower progress initially.
An intuitive explanation for this is that for larger values of η, the iterates converge rapidly to the broad
proximity of an optimal solution, but then reach a cycle where they continue to perform large updates
to the transition coupling which results in large constraint violations, which necessitate large updates,
and so on. This is formally verified by the fact that our bound on the constraint violation terms in
Theorem 4 are increasing for large values of η. Notably, employing a time-dependent learning-rate
schedule (inspired by Ballu and Berthet [2023]) with ηk ∼1/
√"
K,0.8888888888888888,"k leads to the best performance. This
strategy leverages faster convergence initially and, for sufficiently large number of iterations, also
achieves near-optimal solutions."
K,0.8901234567901235,"(a) Algorithm 1
(b) Algorithm 2"
K,0.891358024691358,"Figure 3: Error of estimated transport cost as a function k, for various choices of η."
K,0.8925925925925926,"F.2
Comparison with alternative methods"
K,0.8938271604938272,"We now turn to studying the computational complexity of our algorithms, and compare them empiri-
cally to some existing methods that have been proposed for computing optimal transport distances
and bisimulation metrics between Markov chains. Specifically, we will focus here on two previous
methods: the method of [O’Connor et al., 2022] that we refer to as “EntripicOTC” and Algorithm 2
of [Brugère et al., 2024] that we call “dWL”. We adapt both of these methods with some minor
changes to our setting. First, we remove one scaling factor from the transport cost in the definition
of the distance defined by Brugère et al. [2024] so that it matches ours. Second, the algorithm of
O’Connor et al. [2022] is originally defined for the infinite-horizon average-cost case, and thus we
made appropriate changes to adapt it to the discounted case by replacing their approximate policy
evaluation step by T applications of the discounted Bellman evaluation operator. As pointed out in
Appendix A.2, the resulting methods are closely related, and can be regarded as approximate dynamic
programming methods for solving the MDP formulation of our optimal transport problem presented
in Appendix B. We also recall that the algorithm proposed by Kemertas and Jepson [2022a] for the
purpose of computing bisimulation metrics also falls into the same class of approximate dynamic
programming methods, and nearly matches the method of [Brugère et al., 2024]. The comparison"
K,0.8950617283950617,"Figure 4: Comparison of the computational time of the different methods proposed to obtain a
near-optimal solution for different values of γ. For each Markov chain size, the results obtained in
5 randomly generated instances are compared, showing the standard deviation in the plot. Data is
displayed on a log-log scale."
K,0.8962962962962963,"below is based on the original Python implementation3 of Brugère et al. [2024] and our own Python
adaptation of the MATLAB code4 of O’Connor et al. [2022]."
K,0.8975308641975308,"One difficulty that we had to face in these experiments is having to tune various hyperparameters of
each method (such as number of iterations and regularization parameter), which can each influence
the quality of the solution and the computation time. For a fair comparison between the methods,
we have adopted the following procedure to obtain our results. First, we estimate a ground truth
obtained by running one of the algorithms for a very high number of iterations and a very low level
of regularization, and then use this ground truth as a comparator to adjust the hyperparameters of
the algorithms so that they are close to this value in as little wall-clock time as possible. While all
the algorithms perform similar operations, their total runtimes turn out to be rather different and
heavily dependent on problem parameters such as the discount factor γ. The comparison between
the resulting runtimes of each method is shown on Figure 4 as a function of the size of the Markov
chains, and for a three different choices of the discount factor, for a set of randomly generated MDPs
(following the setup described in Section 7.1 of O’Connor et al., 2022)."
K,0.8987654320987655,"First, we observe that EntropicOTC is a policy-iteration-like method, and as such it needs fewer
iterations than the rest of the algorithms we tested to converge to the true cost, but each iteration
requires running an expensive policy evaluation subroutine until convergence. This computational
cost eventually adds up in a way that this algorithm has always ended up being the slowest among
all that we have tested, although its performance has proved notably robust to changes in the discount
factor γ. Second, we note that the updates of dWL are much cheaper to compute, especially for large
regularization parameters. However, the errors of this value-iteration-like method compound much
more rapidly than in the case of EntropicOTC, which makes it especially hard to tune the hyperpa-
rameters of this method. This problem is especially pronounced when the discount factor is large,
which is the most interesting regime as it leads to distances with much stronger discrimination power."
K,0.9,"The plots shown on Figure 4 indicate that our methods find optimal couplings consistently more
efficiently than the other methods in the regime we studied, leading to up to 10 times faster runtimes.
For the case of sufficiently small values of γ, the algorithm of Brugère et al. [2024] sometimes
performs competitively, but the hyperparameters leading to good performance are much harder to find
than in the case of our methods. In our experience, the massive speedup achieved by our methods
can be largely ascribed to maintaining the transition couplings πk between iterations, as opposed
to computing these afresh by running Sinkhorn’s algorithm from scratch for each update as done
by all other competing methods. Adjusting these other algorithms by maintaining the couplings in
memory and using them to warm-start the subsequent updates makes them competitive with our
methods, and in fact doing so makes them quite similar to SVI and SPI. Our algorithms use such
warm-starts as a primary design choice as opposed to an obscure implementation detail, which is
ultimately responsible for the computational efficiency of all these dynamic-programming methods."
K,0.9012345679012346,"3https://github.com/yusulab/ot_markov_distances
4https://github.com/oconnor-kevin/OTC"
K,0.9024691358024691,"The computational time per iteration of each of these methods grows roughly at a rate of n4, with n
being the number of states of the Markov chains. In the case of our methods, this is easily explained by
noting that applying the Bellman–Sinkhorn operators and updating the transition couplings requires
|X|2|Y|2 operations in total. This matches the runtime necessary for running the regularized policy
improvement subroutines employed by O’Connor et al. [2022] and Brugère et al. [2024], which
consists of running an instance of Sinkhorn’s algorithm in each pair of states. The computational cost
of all these methods can be improved by leveraging the sparsity of transition kernels: in particular, if
at most S states are reachable with positive probability in both of the chains, the complexity of the
updates can be trivially improved to |X||Y|S2. We did not pursue this direction in our experiments
as our goal was to compare the basic versions of each studied method, and we believe that our
conclusions would not be altered if we were to implement this improvement for all methods."
K,0.9037037037037037,"F.3
Optimal transport distances as similarity metrics"
K,0.9049382716049382,"We finally provide a range of experiments that illustrate how the optimal-transport distances we
studied in this paper can be used to capture relationships and symmetries in groups of varying Markov
chains. To this end, we have generated 35 different “4-room” instances and computed their pairwise
distances. Each instance differs in its initial state and position of the obstacles (amounting to changes
in the transition kernel), while maintaining a fixed reward function, with one reward located in each
room except for the upper left room. Crossing a door between each room results in a negative reward.
For each instance, we have studied the Markov chain induced by the corresponding optimal policy
(which amounts to taking the shortest path toward the closest positive reward, modulo the additional
randomness inherent to the transitions). Figure 5 presents a 3D visualization (where the z-dimension
is represented by a color gradient) generated using Multidimensional Scaling (MDS) [Kruskal, 1964]
based on the computed pairwise distances. One can observe a clear clustered structure, where
instances with similar behaviors are grouped closely together. As the figure highlights, the resulting
metrics capture the intuitive similarities and symmetries between each process, which indicates the
potential usefulness of optimal transport distances and bisimulation metrics for comparing Markov
chains under minimal structural assumptions made on the state spaces and the transition functions."
K,0.9061728395061729,"Figure 5: Result of applying MDS to the pairwise distances between the set of 4-room instances
studied. On the plot, the first two coordinates of the MDS embedding are used as the spatial
coordinates, and the third coordinate is encoded via the color bar provided on the left-hand side of
the axes. It can be observed how the elements in the same cluster present common features that
differentiate them from those in another cluster. In the examples shown in the figure we can see how
the instances in which the closest reward involves crossing a door are concentrated in one cluster,
while the instances in which the reward and the initial state are located in the same room belong to a
different cluster. The remaining clusters correspond to having to cross two doors for a reward (set
of green points on the top), or having no reward that is accessible from the initial state (set of blue
points in the middle, with large negative z-coordinates)."
K,0.9074074074074074,NeurIPS Paper Checklist
CLAIMS,0.908641975308642,1. Claims
CLAIMS,0.9098765432098765,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?"
CLAIMS,0.9111111111111111,Answer: [Yes]
CLAIMS,0.9123456790123456,Justification:
CLAIMS,0.9135802469135802,Guidelines:
CLAIMS,0.9148148148148149,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper."
LIMITATIONS,0.9160493827160494,2. Limitations
LIMITATIONS,0.917283950617284,Question: Does the paper discuss the limitations of the work performed by the authors?
LIMITATIONS,0.9185185185185185,Answer: [Yes]
LIMITATIONS,0.9197530864197531,Justification:
LIMITATIONS,0.9209876543209876,Guidelines:
LIMITATIONS,0.9222222222222223,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations."
THEORY ASSUMPTIONS AND PROOFS,0.9234567901234568,3. Theory Assumptions and Proofs
THEORY ASSUMPTIONS AND PROOFS,0.9246913580246914,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?"
THEORY ASSUMPTIONS AND PROOFS,0.9259259259259259,Answer: [Yes]
THEORY ASSUMPTIONS AND PROOFS,0.9271604938271605,"Justification:
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.928395061728395,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4. Experimental Result Reproducibility"
THEORY ASSUMPTIONS AND PROOFS,0.9296296296296296,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification:
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9308641975308642,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5. Open access to data and code"
THEORY ASSUMPTIONS AND PROOFS,0.9320987654320988,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
THEORY ASSUMPTIONS AND PROOFS,0.9333333333333333,Answer: [Yes]
THEORY ASSUMPTIONS AND PROOFS,0.9345679012345679,Justification:
THEORY ASSUMPTIONS AND PROOFS,0.9358024691358025,Guidelines:
THEORY ASSUMPTIONS AND PROOFS,0.937037037037037,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted."
THEORY ASSUMPTIONS AND PROOFS,0.9382716049382716,6. Experimental Setting/Details
THEORY ASSUMPTIONS AND PROOFS,0.9395061728395062,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
THEORY ASSUMPTIONS AND PROOFS,0.9407407407407408,Answer: [Yes]
THEORY ASSUMPTIONS AND PROOFS,0.9419753086419753,Justification:
THEORY ASSUMPTIONS AND PROOFS,0.9432098765432099,Guidelines:
THEORY ASSUMPTIONS AND PROOFS,0.9444444444444444,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.945679012345679,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9469135802469136,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9481481481481482,Answer: [NA]
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9493827160493827,"Justification: There is no randomness in any of the experiments that could be meaningfully
assessed via error bars."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9506172839506173,Guidelines:
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9518518518518518,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors)."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9530864197530864,"• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text."
EXPERIMENTS COMPUTE RESOURCES,0.9543209876543209,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.9555555555555556,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.9567901234567902,Answer: [Yes]
EXPERIMENTS COMPUTE RESOURCES,0.9580246913580247,Justification:
EXPERIMENTS COMPUTE RESOURCES,0.9592592592592593,Guidelines:
EXPERIMENTS COMPUTE RESOURCES,0.9604938271604938,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper)."
CODE OF ETHICS,0.9617283950617284,9. Code Of Ethics
CODE OF ETHICS,0.9629629629629629,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?"
CODE OF ETHICS,0.9641975308641976,Answer: [Yes]
CODE OF ETHICS,0.9654320987654321,Justification:
CODE OF ETHICS,0.9666666666666667,Guidelines:
CODE OF ETHICS,0.9679012345679012,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction)."
BROADER IMPACTS,0.9691358024691358,10. Broader Impacts
BROADER IMPACTS,0.9703703703703703,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?"
BROADER IMPACTS,0.971604938271605,Answer: [NA]
BROADER IMPACTS,0.9728395061728395,Justification: There is no societal impact of the work performed.
BROADER IMPACTS,0.9740740740740741,Guidelines:
BROADER IMPACTS,0.9753086419753086,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations."
BROADER IMPACTS,0.9765432098765432,"• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML)."
SAFEGUARDS,0.9777777777777777,11. Safeguards
SAFEGUARDS,0.9790123456790123,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?"
SAFEGUARDS,0.980246913580247,Answer: [NA]
SAFEGUARDS,0.9814814814814815,Justification:
SAFEGUARDS,0.9827160493827161,Guidelines:
SAFEGUARDS,0.9839506172839506,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort."
LICENSES FOR EXISTING ASSETS,0.9851851851851852,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.9864197530864197,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
LICENSES FOR EXISTING ASSETS,0.9876543209876543,Answer: [Yes]
LICENSES FOR EXISTING ASSETS,0.9888888888888889,Justification:
LICENSES FOR EXISTING ASSETS,0.9901234567901235,Guidelines:
LICENSES FOR EXISTING ASSETS,0.991358024691358,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided."
LICENSES FOR EXISTING ASSETS,0.9925925925925926,"• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13. New Assets"
LICENSES FOR EXISTING ASSETS,0.9938271604938271,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: There are no new assets introduced in the paper.
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9950617283950617,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14. Crowdsourcing and Research with Human Subjects"
LICENSES FOR EXISTING ASSETS,0.9962962962962963,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9975308641975309,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: the paper does not involve crowdsourcing nor research with human subjects.
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9987654320987654,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
