Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0017889087656529517,"Training neural PDE solver in an unsupervised manner is essential in scenarios
1"
ABSTRACT,0.0035778175313059034,"with limited available or high-quality data. However, the performance and effi-
2"
ABSTRACT,0.005366726296958855,"ciency of existing methods are limited by the properties of numerical algorithms
3"
ABSTRACT,0.007155635062611807,"integrated during the training stage (like FDM and PSM), which require careful
4"
ABSTRACT,0.008944543828264758,"spatiotemporal discretization to obtain reasonable accuracy, especially in cases with
5"
ABSTRACT,0.01073345259391771,"high-frequency components and long periods. To overcome these limitations, we
6"
ABSTRACT,0.012522361359570662,"propose Monte Carlo Neural PDE Solver (MCNP Solver) for training unsupervised
7"
ABSTRACT,0.014311270125223614,"neural solvers via a Monte Carlo view, which regards macroscopic phenomena as
8"
ABSTRACT,0.016100178890876567,"ensembles of random particles. MCNP Solver naturally inherits the advantages of
9"
ABSTRACT,0.017889087656529516,"the Monte Carlo method (MCM), which is robust against spatial-temporal varia-
10"
ABSTRACT,0.01967799642218247,"tions and can tolerate coarse time steps compared to other unsupervised methods.
11"
ABSTRACT,0.02146690518783542,"In practice, we develop one-step rollout and Fourier Interpolation techniques that
12"
ABSTRACT,0.023255813953488372,"help reduce computational costs or errors arising from time and space, respec-
13"
ABSTRACT,0.025044722719141325,"tively. Furthermore, we design a multi-scale framework to improve performance
14"
ABSTRACT,0.026833631484794274,"in long-time simulation tasks. In theory, we characterize the approximation error
15"
ABSTRACT,0.028622540250447227,"and robustness of the MCNP Solver on convection-diffusion equations. Numerical
16"
ABSTRACT,0.03041144901610018,"experiments on diffusion and Navier-Stokes equations demonstrate significant
17"
ABSTRACT,0.03220035778175313,"accuracy improvements compared to other unsupervised baselines in cases with
18"
ABSTRACT,0.03398926654740608,"highly variable fields and long-time simulation settings.
19"
INTRODUCTION,0.03577817531305903,"1
Introduction
20"
INTRODUCTION,0.03756708407871199,"Neural PDE solvers, which leverage neural networks as surrogate models to approximate the solutions
21"
INTRODUCTION,0.03935599284436494,"of PDEs, are emerging as a new paradigm for simulating physical systems with the development of
22"
INTRODUCTION,0.04114490161001789,"deep learning [33, 31, 23, 12]. Along this direction, several studies have proposed diverse network
23"
INTRODUCTION,0.04293381037567084,"architectures for neural PDE solvers [30, 33, 5]. These solvers can be trained using supervised [33, 30]
24"
INTRODUCTION,0.044722719141323794,"or unsupervised approaches [59, 54, 32], employing pre-generated data or PDE information to
25"
INTRODUCTION,0.046511627906976744,"construct training targets, respectively. The unsupervised training approach is essential for AI-based
26"
INTRODUCTION,0.04830053667262969,"PDE solvers, particularly in scenarios with limited available or high-quality data. To address this,
27"
INTRODUCTION,0.05008944543828265,"some studies [54, 32] borrow techniques from classical numerical solvers to construct training targets.
28"
INTRODUCTION,0.0518783542039356,"For instance, the low-rank decomposition network (LordNet) [54] and physics-informed neural
29"
INTRODUCTION,0.05366726296958855,"operator (PINO) [32] integrate finite difference method (FDM) and pseudo-spectral methods (PSM)
30"
INTRODUCTION,0.055456171735241505,"with neural networks during the training stage, respectively. However, FDM and PSM require fine
31"
INTRODUCTION,0.057245080500894455,"meshes or time steps for stable simulations in general. Therefore, the performance and efficiency of
32"
INTRODUCTION,0.059033989266547404,"these neural PDE solvers are also limited by the discretization of time and space, particularly when
33"
INTRODUCTION,0.06082289803220036,"handling highly spatial-temporal variations and simulating physical systems over long periods.
34"
INTRODUCTION,0.0626118067978533,"To this end, we propose Monte Carlo Neural PDE Solver (MCNP Solver) for training neural solvers
35"
INTRODUCTION,0.06440071556350627,"from a Monte Carlo perspective, which regards macroscopic phenomena as ensembles of random
36"
INTRODUCTION,0.06618962432915922,"movements of microscopic particles [62]. Consequently, for a PDE system with probabilistic repre-
37"
INTRODUCTION,0.06797853309481217,"sentation, MCNP Solver constructs its solutions as training targets via Monte Carlo approximation.
38"
INTRODUCTION,0.06976744186046512,"Compared to other unsupervised neural solvers, such as LordNet [54] and PINO [32], MCNP Solver
39"
INTRODUCTION,0.07155635062611806,"naturally inherits the advantages of MCM. On the one hand, MCNP Solver can tolerate coarse
40"
INTRODUCTION,0.07334525939177101,"time steps [11, 39], thereby reducing training costs and accumulated errors arising from temporal
41"
INTRODUCTION,0.07513416815742398,"discretization. On the other hand, it can efficiently handle high-frequency spatial fields due to the
42"
INTRODUCTION,0.07692307692307693,"derivative-free property of MCM [37, 1]. Moreover, the boundary conditions are automatically
43"
INTRODUCTION,0.07871198568872988,"encoded into the stochastic process of particles [2, 34], eliminating the need to introduce extra loss
44"
INTRODUCTION,0.08050089445438283,"terms to satisfy such constraints. In addition to inheriting the benefits of MCM, we also develop
45"
INTRODUCTION,0.08228980322003578,"one-step rollout and Fourier Interpolation techniques to improve performance and efficiency from
46"
INTRODUCTION,0.08407871198568873,"the perspective of time and space. Furthermore, we design a multi-scale framework to improve the
47"
INTRODUCTION,0.08586762075134168,"accuracy and robustness of the MCNP Solver in long-time simulation tasks.
48"
INTRODUCTION,0.08765652951699464,"Compared to traditional MCM, MCNP Solver enjoys a significantly faster inference speed once
49"
INTRODUCTION,0.08944543828264759,"trained. Additionally, traditional MCM requires sampling excess particles to achieve high-precision
50"
INTRODUCTION,0.09123434704830054,"results, which can lead to severe computational and memory issues. However, thanks to the involve-
51"
INTRODUCTION,0.09302325581395349,"ment of neural networks, the MCNP Solver does not necessitate sampling as many particles per epoch
52"
INTRODUCTION,0.09481216457960644,"during training. According to our experimental observations, the model can converge as expected
53"
INTRODUCTION,0.09660107334525939,"using gradient descent with only a few particles.
54"
INTRODUCTION,0.09838998211091235,"In this paper, we conduct in-depth analyses of the MCNP Solver‚Äôs performance theoretically and
55"
INTRODUCTION,0.1001788908765653,"experimentally. In summary, we make the following contributions:
56"
INTRODUCTION,0.10196779964221825,"1. We introduce MCNP Solver, a novel Monte Carlo-based unsupervised approach for training neural
57"
INTRODUCTION,0.1037567084078712,"solvers applicable to PDE systems that allow probabilistic representation. Additionally, we develop
58"
INTRODUCTION,0.10554561717352415,"several techniques to enhance performance and efficiency, such as Fourier Interpolation, one-step
59"
INTRODUCTION,0.1073345259391771,"rollout, and multi-scale prediction.
60"
INTRODUCTION,0.10912343470483005,"2. Theoretically, we compare the approximation error and robustness of two kinds of neural PDE
61"
INTRODUCTION,0.11091234347048301,"solvers concerning variations in spatial conditions, temporal discretization steps, and diffusive
62"
INTRODUCTION,0.11270125223613596,"coefficients. Our theoretical results reveal that MCNP Solver is more robust against the spatial-
63"
INTRODUCTION,0.11449016100178891,"temporal variants when solving convection-diffusion equations.
64"
INTRODUCTION,0.11627906976744186,"3. Our experiments on the diffusion and Navier-Stokes equation (NSE) show significant improvements
65"
INTRODUCTION,0.11806797853309481,"in accuracy compared to other unsupervised neural solvers for simulating tasks with complex spatial-
66"
INTRODUCTION,0.11985688729874776,"temporal variants and long-time simulation. Furthermore, the MCNP Solver can obtain comparable
67"
INTRODUCTION,0.12164579606440072,"or even better results than supervised neural solvers.
68"
RELATED WORK,0.12343470483005367,"2
Related Work
69"
RELATED WORK,0.1252236135957066,"Neural PDE Solver
Neural PDE solvers have been proposed to learn mappings between functional
70"
RELATED WORK,0.12701252236135957,"spaces, such as mapping a PDE‚Äôs initial condition to its solution [33]. Works like DeepONet [33] and
71"
RELATED WORK,0.12880143112701253,"its variants [15, 52, 57, 26] encode the initial conditions and queried locations using branch and trunk
72"
RELATED WORK,0.13059033989266547,"networks, respectively. Additionally, Fourier Neural Operator (FNO) [31] and its variants [29, 45, 56]
73"
RELATED WORK,0.13237924865831843,"explore learning the operator in Fourier space, an efficient approach for handling different frequency
74"
RELATED WORK,0.13416815742397137,"components. Several studies have employed graph neural networks [30, 5] or transformers [6, 28]
75"
RELATED WORK,0.13595706618962433,"as the backbone models of neural solvers to adapt to complex geometries. However, these methods
76"
RELATED WORK,0.13774597495527727,"require the supervision of ground-truth data generated via accurate numerical solvers, which can be
77"
RELATED WORK,0.13953488372093023,"time-consuming in general. To this end, some studies aim to train the neural PDE solvers without
78"
RELATED WORK,0.1413237924865832,"the supervision of data [59, 32, 54, 19]. For example, [59] proposed PI-DeepONets, which utilize
79"
RELATED WORK,0.14311270125223613,"the PDE residuals to train DeepONets in an unsupervised way. Similarly, [19] proposed Meta-
80"
RELATED WORK,0.1449016100178891,"Auto-Decoder, a meta-learning approach to learn families of PDEs in the unsupervised regime.
81"
RELATED WORK,0.14669051878354203,"Furthermore, LordNet [54] and PINO [32] borrow techniques from FDM and PSM, and utilize the
82"
RELATED WORK,0.148479427549195,"corresponding residuals as training loss, respectively. Compared to these unsupervised methods, the
83"
RELATED WORK,0.15026833631484796,"MCNP Solver incorporates physics information through the Feynman-Kac law, representing a Monte
84"
RELATED WORK,0.1520572450805009,"Carlo perspective. This approach allows the solver to efficiently manage diffusion terms, exhibit
85"
RELATED WORK,0.15384615384615385,"robustness against spatial-temporal variants, and be suitable for long-time simulations.
86"
RELATED WORK,0.1556350626118068,"Physics-Informed Neural Networks (PINNs)
PINNs have been proposed to solve PDE systems
87"
RELATED WORK,0.15742397137745975,"by approximating solutions using the PDE residuals, which involve point-to-point mapping between
88"
RELATED WORK,0.1592128801431127,"spatial-temporal points and solution values. They are widely employed for solving forward or inverse
89"
RELATED WORK,0.16100178890876565,"problems [46, 8, 22, 66]. Recently, PINNs have made significant progress in addressing scientific
90"
RELATED WORK,0.16279069767441862,"problems based on PDEs, including NSEs [47, 20, 36], Schr√∂dinger equations [18, 27], Allen Cahn
91"
RELATED WORK,0.16457960644007155,"equations [38, 21], and more. Instead of constructing the loss function directly via the PDE residuals,
92"
RELATED WORK,0.16636851520572452,"some works utilize the probabilistic representation to train neural networks [17, 14, 63], which can
93"
RELATED WORK,0.16815742397137745,"efficiently handle high-dimensional or fractional PDEs [16, 50, 14, 49, 41]. Furthermore, some
94"
RELATED WORK,0.16994633273703041,"studies design loss functions based on other numerical methods, such as the finite volume method [4],
95"
RELATED WORK,0.17173524150268335,"finite element method [40, 42], and energy-based method [61]. Notably, the aforementioned PINN
96"
RELATED WORK,0.1735241502683363,"methods require retraining neural networks when encountering a PDE with new initial conditions,
97"
RELATED WORK,0.17531305903398928,"which can be time-consuming. Moreover, the studies [3, 48] consider PDE families with varying
98"
RELATED WORK,0.1771019677996422,"initial conditions while requiring corresponding conditions can be represented by a low-dimensional
99"
RELATED WORK,0.17889087656529518,"vector. In this paper, we aim to learn operators between functional spaces that can generalize to
100"
RELATED WORK,0.1806797853309481,"different PDE conditions over a distribution. When applying Feynman-Kac laws to this new scenario,
101"
RELATED WORK,0.18246869409660108,"we encounter several computational challenges arising from corresponding tasks, such as higher
102"
RELATED WORK,0.18425760286225404,"generalization requirements, long-time simulations, and the non-linearity of PDEs. Therefore, we
103"
RELATED WORK,0.18604651162790697,"propose Fourier Interpolation, one-step rollout, and multi-scale prediction to overcome these issues.
104"
RELATED WORK,0.18783542039355994,"More detailed discussions of these Feynman-Kac-based PINNs can be seen in Appendix D.
105"
METHODOLOGY,0.18962432915921287,"3
Methodology
106"
PRELIMINARY,0.19141323792486584,"3.1
Preliminary
107"
PRELIMINARY,0.19320214669051877,"In this paper, we consider the general convection-diffusion equation defined as follows:
108 ‚àÇu"
PRELIMINARY,0.19499105545617174,"‚àÇt = Œ≤[u](x, t) ¬∑ ‚àáu + Œ∫‚àÜu + f(x, t),
u(x, 0) = u0(x),
(1)"
PRELIMINARY,0.1967799642218247,"where x ‚àà‚Ñ¶‚äÇRd and t denote the d-dimensional spatial variable and the time variable, respectively,
109"
PRELIMINARY,0.19856887298747763,"Œ≤[u](x, t) ‚ààRd is a vector-valued mapping from u to Rd, Œ∫ ‚ààR+ is the diffusion parameter, and
110"
PRELIMINARY,0.2003577817531306,"f(x, t) ‚ààR denotes the force term. Many well-known PDEs, such as Burgers‚Äô equation, NSE, can
111"
PRELIMINARY,0.20214669051878353,"be viewed as a special form of Eq. 1.
112"
PRELIMINARY,0.2039355992844365,"For such PDEs with the form as Eq. 1, the Feynman-Kac formula provides the relationship between
113"
PRELIMINARY,0.20572450805008943,"the PDEs and corresponding probabilistic representation [43, 44, 16]. In detail, we can use the time
114"
PRELIMINARY,0.2075134168157424,"inversion (i.e., Àúu(x, t) = u(x, T ‚àít), Àúf(x, t) = f(x, T ‚àít)) to the PDE as:
115 ‚àÇÀúu"
PRELIMINARY,0.20930232558139536,"‚àÇt = ‚àíŒ≤[Àúu](x, t) ¬∑ ‚àáÀúu ‚àíŒ∫‚àÜÀúu ‚àíÀúf(x, t),
Àúu(x, T) = u0(x).
(2)"
PRELIMINARY,0.2110912343470483,"Applying the Feynman-Kac formula [35] to the terminal value problem Eq. 2, we have
116"
PRELIMINARY,0.21288014311270126,"Àúu0(x) = E """
PRELIMINARY,0.2146690518783542,"ÀúuT (ÀúŒæT ) +
Z T"
PRELIMINARY,0.21645796064400716,"0
Àúf(ÀúŒæs, s)ds # ,
(3)"
PRELIMINARY,0.2182468694096601,"where ÀúŒæs ‚ààRd is a random process starting at x, and moving from 0 to T, which satisfies:
117"
PRELIMINARY,0.22003577817531306,"dÀúŒæs = Œ≤[Àúu](ÀúŒæs, s)ds +
‚àö"
PRELIMINARY,0.22182468694096602,"2Œ∫dBs,
ÀúŒæ0 = x,
(4)"
PRELIMINARY,0.22361359570661896,"where Bs is the d-dimensional standard Brownian motion. Applying time inversion t ‚ÜíT ‚àít to
118"
PRELIMINARY,0.22540250447227192,"Eq. 3 and letting Œæ be the inversion of ÀúŒæ, we have
119"
PRELIMINARY,0.22719141323792486,"uT (x) = E """
PRELIMINARY,0.22898032200357782,"u0(Œæ0) +
Z T"
PRELIMINARY,0.23076923076923078,"0
f(Œæs, s)ds # .
(5)"
PRELIMINARY,0.23255813953488372,"Furthermore, apart from Eq. 1, some other PDEs can also be handled via the Feynman-Kac formula
120"
PRELIMINARY,0.23434704830053668,"after certain processing, like wave equations [9] and spatially varying diffusion equations [51].
121"
MONTE CARLO NEURAL PDE SOLVER,0.23613595706618962,"3.2
Monte Carlo Neural PDE Solver
122"
MONTE CARLO NEURAL PDE SOLVER,0.23792486583184258,"Given a PDE with the form of Eq. 1 and a distribution of the initial conditions D0, the target of MCNP
123"
MONTE CARLO NEURAL PDE SOLVER,0.23971377459749552,"Solver is to learn a functional mapping GŒ∏ with parameter Œ∏ which can simulate the subsequent fields
124"
MONTE CARLO NEURAL PDE SOLVER,0.24150268336314848,"for all initial fields u0 ‚àºD0 at time t ‚àà[0, T]. In detail, the inputs and outputs of GŒ∏ are given as:
125"
MONTE CARLO NEURAL PDE SOLVER,0.24329159212880144,"GŒ∏ : D0 √ó [0, T] ‚ÜíD[0,T ],"
MONTE CARLO NEURAL PDE SOLVER,0.24508050089445438,"(u0, t) 7‚Üíut,
(6) ùíñùíñùüéùüé"
MONTE CARLO NEURAL PDE SOLVER,0.24686940966010734,ùíïùíï+ ùúüùúüùúüùúü ùíïùíï ùíñùíñùíïùíï
MONTE CARLO NEURAL PDE SOLVER,0.24865831842576028,ùíñùíñùíïùíï+ùúüùúüùúüùúü
MONTE CARLO NEURAL PDE SOLVER,0.2504472271914132,"Fourier 
Interpolation ‡∑ùùíñùíñùíïùíï MCNP C A
B"
MONTE CARLO NEURAL PDE SOLVER,0.2522361359570662,"A: Random walk
B: Project
C: Query and Average MCNP"
MONTE CARLO NEURAL PDE SOLVER,0.25402504472271914,"Figure 1: Illustration of the neural Monte Carlo loss. We construct the training loss via the relationship
between ut and ut+‚àÜt given by the Feynman-Kac law. A: random walk according to Eq. 11, and denote the M
particles starting at the grid point x as {Œæm
s }M
m=1; B: when Œæm
s moving from t + ‚àÜt to t, project each Œæm
t to
the nearest coordinate point ÀÜŒæ
m
t in the high resolution coordinate system; C: query the value of each ÀÜŒæ
m
t via ÀÜut
and average ÀÜut(ÀÜŒæ
m
t ) as PM
m=1 ÀÜut(ÀÜŒæ
m
t ). Please note that the high-resolution ÀÜut is obtained from ut via Fourier
interpolation. Then, the neural Monte Carlo loss at x is given by: ‚à•GŒ∏(u0, t)(x) ‚àíPM
m=1 ÀÜut(ÀÜŒæ
m
t )‚à•2
2."
MONTE CARLO NEURAL PDE SOLVER,0.2558139534883721,"where D[0,T ] denotes the joint distribution of the field after t = 0. Unlike other supervised operator
126"
MONTE CARLO NEURAL PDE SOLVER,0.25760286225402507,"learning algorithms [27, 33, 5], MCNP Solver aims to learn the operator in an unsupervised way,
127"
MONTE CARLO NEURAL PDE SOLVER,0.259391771019678,"i.e., only utilize the physics information provided by PDEs. To this end, MCNP Solver considers
128"
MONTE CARLO NEURAL PDE SOLVER,0.26118067978533094,"training the solver via the relationship between ut and ut+‚àÜt (where 0 ‚â§t < t + ‚àÜt ‚â§T) derived
129"
MONTE CARLO NEURAL PDE SOLVER,0.2629695885509839,"by the aforementioned probabilistic representation. Considering Eq. 5, an expected neural operator
130"
MONTE CARLO NEURAL PDE SOLVER,0.26475849731663686,"GŒ∏ should satisfy the following equation:
131"
MONTE CARLO NEURAL PDE SOLVER,0.26654740608228983,"GŒ∏(u0, t + ‚àÜt)(x) = EŒæ """
MONTE CARLO NEURAL PDE SOLVER,0.26833631484794274,"GŒ∏(u0, t)(Œæt) +
Z t+‚àÜt"
MONTE CARLO NEURAL PDE SOLVER,0.2701252236135957,"t
f(Œæs, s)ds # ,
(7)"
MONTE CARLO NEURAL PDE SOLVER,0.27191413237924866,"where Œæs(s ‚àà[t, t + ‚àÜt]) is the inverse version of stochastic process in Eq. 4 as follows:
132"
MONTE CARLO NEURAL PDE SOLVER,0.2737030411449016,"dŒæs = ‚àíŒ≤[u](Œæs, s)ds ‚àí
‚àö"
MONTE CARLO NEURAL PDE SOLVER,0.27549194991055453,"2Œ∫dBs,
Œæt+‚àÜt = x.
(8)"
MONTE CARLO NEURAL PDE SOLVER,0.2772808586762075,"Regarding Eq. 7 as the optimization objective, the neural Monte Carlo loss can be written as follows:
133"
MONTE CARLO NEURAL PDE SOLVER,0.27906976744186046,"LMC(GŒ∏|u0, t, ‚àÜt) ="
MONTE CARLO NEURAL PDE SOLVER,0.2808586762075134,"GŒ∏(u0, t + ‚àÜt)(x) ‚àíEŒæ """
MONTE CARLO NEURAL PDE SOLVER,0.2826475849731664,"GŒ∏(u0, t)(Œæt) +
Z t+‚àÜt"
MONTE CARLO NEURAL PDE SOLVER,0.2844364937388193,"t
f(Œæs, s)ds # 2"
MONTE CARLO NEURAL PDE SOLVER,0.28622540250447226,"2
.
(9)"
MONTE CARLO NEURAL PDE SOLVER,0.2880143112701252,"Equipped with the loss function Eq. 9, we sample the initial states u0 from D0 and the time t from
134"
MONTE CARLO NEURAL PDE SOLVER,0.2898032200357782,"[0, T] each epoch, and the MCNP loss LMCNP is given as follows:
135"
MONTE CARLO NEURAL PDE SOLVER,0.29159212880143115,"LMCNP = Eu0‚àºD0[Linit(GŒ∏|u0) + ŒªEt‚àº[0,T ][LMC(GŒ∏|u0, t, ‚àÜt)]],
(10)"
MONTE CARLO NEURAL PDE SOLVER,0.29338103756708406,"where Œª ‚ààR+ is a hyper-parameter, and Linit(GŒ∏|u0) ‚âú‚à•GŒ∏(u0, 0) ‚àíu0‚à•2
2 denotes the loss at t = 0.
136"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.295169946332737,"3.3
Implementation Details of MCNP Solver
137"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.29695885509839,"In this section, we introduce some important implementation details for MCNP Solver. We illustrate
138"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.29874776386404295,"the framework and training process of MCNP Solver in Fig. 1 and the overall algorithm in Appendix
139"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.3005366726296959,"A. We design one-step rollout and Fourier Interpolation trick to reduce the computational cost and
140"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.3023255813953488,"error from the perspectives of time and space, respectively. Moreover, we conduct the multi-scale
141"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.3041144901610018,"framework to improve the long-time simulation ability of MCNP Solver.
142"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.30590339892665475,"Temporal Discretization and One-Step Rollout
When simulating the stochastic process in Eq. 8,
143"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.3076923076923077,"we utilize the classical Euler‚ÄìMaruyama method [58] to approximate corresponding SDEs, .i.e,
144"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.3094812164579606,"Œæt = Œæt+‚àÜt + Œ≤[u](Œæt+‚àÜt, t + ‚àÜt)‚àÜt +
‚àö"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.3112701252236136,"2Œ∫‚àÜBt,
Œæt+‚àÜt = x.
(11)"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.31305903398926654,"The stochastic integral of the force f in Eq. 7 is approximated via the Euler method, which aligns
145"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.3148479427549195,"with [16]. Unlike other Feynman-Kac-based methods [16, 41] conducting random walks in Eq. 8
146"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.31663685152057247,"with multi-steps, we utilize one-step rollout technique to simulate SDEs, i.e., at each t + ‚àÜt, MCNP
147"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.3184257602862254,"Solver generates new particles from x, and moves them back to t according to Eq. 11. The one-step
148"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.32021466905187834,"rollout trick can enforce all Œæt+‚àÜt starting at x share the same Œ≤[u](x, t + ‚àÜt) during the simulation
149"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.3220035778175313,"of SDEs and thus, reduce the computational cost, especially for the scenario when the calculation
150"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.32379248658318427,"cost of Œ≤ is expensive. For instance, when the drift Œ≤ term depends on solution u, we have to utilize
151"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.32558139534883723,"MCNP Solver to calculate Œ≤ accordingly. Moreover, in the NSE conducted in this paper, the mapping
152"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.32737030411449014,"u ‚ÜíŒ≤ represents the transformation from the vorticity field to the velocity field, which involves a
153"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.3291592128801431,"numerical integration over an entire domain.
154"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.33094812164579607,"Random Walks and Boundary Conditions
Eq. 3 and Eq. 4 describe the random walks driven by
155"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.33273703041144903,"stochastic processes of corresponding PDEs. For PDEs with periodical boundary conditions, particles
156"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.334525939177102,"should be pulled back according to the periodical law when walking out of the domain ‚Ñ¶. For
157"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.3363148479427549,"Dirichlet boundary conditions, the random walk of particles should stop once they reach the boundary.
158"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.33810375670840787,"Compared to other unsupervised neural PDE solvers, MCNP Solver encodes the boundary conditions
159"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.33989266547406083,"naturally into the random walks of particles and thus does not need additional soft constraints in the
160"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.3416815742397138,"loss function. Furthermore, for PDEs with the fractional Laplacian ‚àí(‚àí‚àÜ)Œ±u, where Œ± ‚àà(0, 2), we
161"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.3434704830053667,"only need to replace the Brownian motion with the Œ±-stable L√©vy process [24, 65, 64].
162"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.34525939177101966,"Spatial Discretization and Fourier Interpolation
In this paper, we are interested in the evolution
163"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.3470483005366726,"of PDEs at fixed grids {xp}P
p=1 ‚àà‚Ñ¶. Consequently, the inputs and outputs of the solver GŒ∏ are
164"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.3488372093023256,"solution values at P coordinate points. Please note that in Eq .7, the particles Œæt need to query the
165"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.35062611806797855,"value of GŒ∏(u0, t) when approximating GŒ∏(u0, t + ‚àÜt). To efficiently obtain the querying results, we
166"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.35241502683363146,"project the locations of particles Œæt to their nearest neighbor grids in practice. To reduce projection
167"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.3542039355992844,"errors, we utilize the Fourier transform to interpolate the fields ut = GŒ∏(u0, t) to the high-resolution
168"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.3559928443649374,"one ÀÜut before the projection. It is worth mentioning that the Fourier Interpolation technique can
169"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.35778175313059035,"help the neural solver achieve high-accuracy training signals without the calls of solvers on the
170"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.3595706618962433,"high-resolution PDE fields, thereby reducing the training cost.
171"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.3613595706618962,"Multi-Scale Framework for Long-Time Simulation
When handling tasks with long temporal
172"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.3631484794275492,"intervals, we design the following multi-scale framework to make the training process more robust.
173"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.36493738819320215,"In detail, we divide the long-time interval [0, T] into K coarse subintervals, i.e., {[Tk, Tk+1]}K‚àí1
k=0 ,
174"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.3667262969588551,"with T0 = 0, TK = T and Tk+1 ‚àíTk = ‚àÜT. Accordingly, we adopt K neural solvers {GŒ∏k}K‚àí1
k=0
175"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.3685152057245081,"with independent parameter Œ∏k to approximate the solution in [Tk, Tk+1], respectively. In the training
176"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.370304114490161,"stage, the loss function for long-time simulation is given as follows:
177"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.37209302325581395,"LLong
MCNP = Eu0‚àºD0"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.3738819320214669,"""K‚àí1
X"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.3756708407871199,"k=0
Linit(GŒ∏k|uTk) + Œª K‚àí1
X"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.3774597495527728,"k=0
Et‚àº[Tk,Tk+1][LMC(GŒ∏k|uTk, t, ‚àÜt)] #"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.37924865831842575,".
(12)"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.3810375670840787,"Here, uTk = GŒ∏k‚àí1(uTk‚àí1, ‚àÜT) can be calculated recursively with uT0 = u0, and Linit(GŒ∏k|uTk) ‚âú
178"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.3828264758497317,"‚à•GŒ∏k(uTk, 0)‚àísg[uTk]‚à•2
2 denotes the initialization loss for GŒ∏k, where sg[¬∑] denotes the stop-gradient
179"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.38461538461538464,"operator. In the inference stage, when predicting the PDE field with the initialization u0 at t =
180"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.38640429338103754,"Tk + ‚àÜt(0 < ‚àÜt < ‚àÜT), we first rollout with coarse step ‚àÜT to obtain uTk, and then adopt the
181"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.3881932021466905,"finer step to give the prediction of ut as GŒ∏k(uTk, ‚àÜt). Due to the independent parameterization and
182"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.38998211091234347,"stop-gradient operator, the proposed multi-scale framework can prevent the prediction at time t‚Ä≤ from
183"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.39177101967799643,"producing harmful effects on the former time t < t‚Ä≤ in the optimization stage. Our experiments reveal
184"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.3935599284436494,"that it can improve the performance on long-time simulation tasks where the PDE fields change
185"
IMPLEMENTATION DETAILS OF MCNP SOLVER,0.3953488372093023,"dramatically over time (e.g., turbulent flow simulation).
186"
THEORETICAL RESULTS,0.39713774597495527,"4
Theoretical Results
187"
THEORETICAL RESULTS,0.39892665474060823,"In this section, we study the theoretical properties of MCNP Solver when simulating the convection-
188"
THEORETICAL RESULTS,0.4007155635062612,"diffusion equation, and the proof can be seen in Appendix B. In detail, we consider the periodical
189"
THEORETICAL RESULTS,0.40250447227191416,"convection-diffusion equation defined as follows:
190 ‚àÇu"
THEORETICAL RESULTS,0.40429338103756707,"‚àÇt = Œ∫‚àÜu + Œ≤t,
x ‚àà[0, 2œÄ], t ‚àà[0, T], Œ≤ ‚ààR.
(13)"
THEORETICAL RESULTS,0.40608228980322003,"In the following main theorem, we consider the error of one-step rollout targets provided in PSM and
191"
THEORETICAL RESULTS,0.407871198568873,"MCM when training neural PDE solvers, respectively.
192"
THEORETICAL RESULTS,0.40966010733452596,"Theorem 4.1 Let ut(x) be solution of the convection-diffusion equation in the form of Eq. 13,
193"
THEORETICAL RESULTS,0.41144901610017887,"and assume the exact solution at time t can be expressed by the Fourier basis, i.e., ut(x) =
194
PN
n=1 an sin(nx). Let GŒ∏ be the neural PDE solver, and its prediction on ut(x) can be written
195"
THEORETICAL RESULTS,0.41323792486583183,"as GŒ∏(u0, t)(x) = PN
n=1(an + Œ¥n) sin(nx), where Œ¥n denotes the residual of coefficient on each
196"
THEORETICAL RESULTS,0.4150268336314848,"Fourier basis. Let H and M denote the gird size after Fourier Interpolation and sampling num-
197"
THEORETICAL RESULTS,0.41681574239713776,"bers in neural Monte Carlo loss. Let uPSM
t+‚àÜt(x) and uMCM
t+‚àÜt (x) be the one-step labels starting from
198"
THEORETICAL RESULTS,0.4186046511627907,"GŒ∏(u0, t)(x), given by PSM and MCM, respectively. Assume ‚àÜtu and ut(x) are Lipschitz functions
199"
THEORETICAL RESULTS,0.4203935599284436,"with respect to t and x, respectively, i.e.:
200"
THEORETICAL RESULTS,0.4221824686940966,"|‚àÜt1u(x) ‚àí‚àÜt2u(x)| ‚â§Lt
‚àÜu|t1 ‚àít2|,
|ut(x1) ‚àíut(x2)| ‚â§Lx
u|x1 ‚àíx2|.
(14)
Then, we have
201"
THEORETICAL RESULTS,0.42397137745974955,"1)
uPSM
t+‚àÜt(x) ‚àíut+‚àÜt(x)
 ‚â§Œ∫Lt
‚àÜu‚àÜt2"
THEORETICAL RESULTS,0.4257602862254025,"2
|
{z
}
EPSM
1 + N
X"
THEORETICAL RESULTS,0.4275491949910555,"n=1
|Œ¥n(Œ∫n2‚àÜt ‚àí1)|"
THEORETICAL RESULTS,0.4293381037567084,"|
{z
}
EPSM
2 ;
202"
THEORETICAL RESULTS,0.43112701252236135,"2) With probability at least 1 ‚àí(2Lx
u)2Œ∫‚àÜt
Mœµ2
, we have
203"
THEORETICAL RESULTS,0.4329159212880143,"uMCM
t+‚àÜt (x) ‚àíut+‚àÜt(x)
 ‚â§
1
2H N
X"
THEORETICAL RESULTS,0.4347048300536673,"n=1
|nan|"
THEORETICAL RESULTS,0.4364937388193202,"|
{z
}
EMCM
1 + N
X"
THEORETICAL RESULTS,0.43828264758497315,"n=1
|Œ¥n|"
THEORETICAL RESULTS,0.4400715563506261,"| {z }
EMCM
2 +
œµ"
THEORETICAL RESULTS,0.4418604651162791,"|{z}
EMCM
3 (15)"
THEORETICAL RESULTS,0.44364937388193204,"In the PSM, error terms EPSM
1
and EPSM
2
arise from the temporal discretization and the perturbation
204"
THEORETICAL RESULTS,0.44543828264758495,"of GŒ∏(u0, t), respectively. Additionally, the error term EPSM
2
increases with the rate of n2, where
205"
THEORETICAL RESULTS,0.4472271914132379,"n2 comes from the second order derivative of sin(nx). To mitigate the error induced by the PSM,
206"
THEORETICAL RESULTS,0.4490161001788909,"one has to decrease ‚àÜt, which inevitably necessitates additional calls to classical or neural solvers.
207"
THEORETICAL RESULTS,0.45080500894454384,"Conversely, for MCM, the error term EMCM
1
originates from the Fourier Interpolation trick, which
208"
THEORETICAL RESULTS,0.4525939177101968,"can be controlled by increasing the interpolation rate. This operation does not consume much time
209"
THEORETICAL RESULTS,0.4543828264758497,"because it does not require extra solver calls. Moreover, the error caused by the residual Œ¥n (EMCM
2
)
210"
THEORETICAL RESULTS,0.4561717352415027,"remains stable as n grows due to the derivative-free property of MCM. It is worth noting that while
211"
THEORETICAL RESULTS,0.45796064400715564,"EMCM
3
can be controlled by the number of samples M, an excessive number of particles is not
212"
THEORETICAL RESULTS,0.4597495527728086,"required in practice. Unlike deterministic biases introduced by other error terms, EMCM
3
stems from
213"
THEORETICAL RESULTS,0.46153846153846156,"the variance of random processes and can be regarded as a type of stochastic label noise. Some
214"
THEORETICAL RESULTS,0.46332737030411447,"studies [7, 10] have found that such stochastic label noise can aid generalization and even counteract
215"
THEORETICAL RESULTS,0.46511627906976744,"inherent biases. Therefore, we assert that, compared to PSM, the neural Monte Carlo method can
216"
THEORETICAL RESULTS,0.4669051878354204,"tolerate coarser time steps and spatial variations when solving convection-diffusion equations.
217"
EXPERIMENTS,0.46869409660107336,"5
Experiments
218"
EXPERIMENTS,0.47048300536672627,"In this section, we conduct numerical experiments to evaluate the proposed MCNP Solver on two
219"
EXPERIMENTS,0.47227191413237923,"tasks: 1D diffusion equations and 2D NSEs. Implementation details are introduced in Appendix E.
220"
EXPERIMENTS,0.4740608228980322,"We utilize the FNO [31] as the backbone network, with more detailed discussions in Appendix C. We
221"
EXPERIMENTS,0.47584973166368516,"evaluate the model performance for all tasks via the relative ‚Ñì2 error on 200 test PDE samples. We
222"
EXPERIMENTS,0.4776386404293381,"repeat each experiment with three random seeds in {0, 1, 2} and report the mean value and variance.
223"
EXPERIMENTS,0.47942754919499103,"All experiments are implemented on an NVIDIA A100 GPU.
224"
EXPERIMENTS,0.481216457960644,"5.1
1D Diffusion Equation
225"
EXPERIMENTS,0.48300536672629696,"In this section, we conduct experiments on periodical 1D diffusion equation defined as follows:
226"
EXPERIMENTS,0.4847942754919499,"‚àÇu(x, t)"
EXPERIMENTS,0.4865831842576029,"‚àÇt
= Œ∫‚àÜu(x, t), x ‚àà[0, 1], t ‚àà[0, 5].
(16)"
EXPERIMENTS,0.4883720930232558,"The initial states u(x, 0) are generated from the functional space FN ‚âú{PN
n=1 an sin(2œÄnx) :
227"
EXPERIMENTS,0.49016100178890876,"an ‚àºU(0, 1)}, where U(0, 1) denotes the uniform distribution over (0, 1), and N represents the
228"
EXPERIMENTS,0.4919499105545617,"maximum frequency of the functional space.
229"
EXPERIMENTS,0.4937388193202147,"Table 1: 1D diffusion equation with varying N and Œ∫. Relative errors (%) and computational costs for
baseline methods and MCNP Solver."
EXPERIMENTS,0.49552772808586765,"Model
Œ∫ = 0.01
Œ∫ = 0.02
Time
Params"
EXPERIMENTS,0.49731663685152055,"N = 6
N = 12
N = 6
N = 12
Train (H)
Infer (S)
# (M)"
EXPERIMENTS,0.4991055456171735,"PSM
NAN*
NAN
NAN
NAN
‚Äì
0.028
‚Äì
PSM+
0.000448
0.00132
NAN
NAN
‚Äì
0.554
‚Äì
MCM
5.574¬± 0.009
12.615¬± 0.056
29.991¬± 0.183
83.442¬± 0.234
‚Äì
0.034
‚Äì
FNO
1.125¬± 0.183
5.930¬± 7.468
3.662¬± 0.265
23.926¬± 14.775
0.194
0.00145
0.152
PINO
1.075¬± 0.208
3.563¬± 0.684
5.275¬± 2.328
26.735¬± 17.878
0.206
0.00145
0.152
PI-DeepONet
16.224¬± 1.165
112.630¬± 18.945
113.212¬± 25.875
NAN
2.451
0.00126
0.153
MCNP
1.056¬± 0.194
1.511¬± 0.090
3.727¬± 1.587
6.575¬± 1.948
0.116
0.00145
0.152"
EXPERIMENTS,0.5008944543828264,* Here we unitize NAN to represent the results whose relative error is larger than 200%.
EXPERIMENTS,0.5026833631484794,"Experimental Settings
In this setting, Œ∫ represents the heat transfer rate, with larger Œ∫ values
230"
EXPERIMENTS,0.5044722719141324,"indicating faster temporal variation rates. N can be regarded as a measure of spatial complexity,
231"
EXPERIMENTS,0.5062611806797853,"where larger values correspond to a higher proportion of high-frequency signals. We select two
232"
EXPERIMENTS,0.5080500894454383,"different Œ∫ in {0.01, 0.02} and N in {6, 12}, respectively, to evaluate the performance of different
233"
EXPERIMENTS,0.5098389982110912,"methods in handling temporal-spatial variations. We divide the spatial domain [0, 1] into 64 grid
234"
EXPERIMENTS,0.5116279069767442,"elements for all experiments.
235"
EXPERIMENTS,0.5134168157423972,"Baselines
We introduce the baselines conducted on 1D diffusion equations, including: i). PSM: A
236"
EXPERIMENTS,0.5152057245080501,"traditional numerical methods. We divide the time interval into 100 uniform lattices and utilize the
237"
EXPERIMENTS,0.516994633273703,"2nd Runge-Kutta method for temporal revolution. ii). PSM+: PSM with a fine step size. We divide
238"
EXPERIMENTS,0.518783542039356,"the time interval into 2000 uniform lattices. iii). MCM: a traditional numerical method based on
239"
EXPERIMENTS,0.5205724508050089,"the probabilistic representation of PDEs. We set the sampling numbers as 105. iv). FNO: Training
240"
EXPERIMENTS,0.5223613595706619,"with 1000 pre-generated data, calculating from the analytic solution of Eq. 16. v). PINO [32]: An
241"
EXPERIMENTS,0.5241502683363148,"unsupervised neural operator based on PSM. We divide the time interval into 100 uniform lattices.
242"
EXPERIMENTS,0.5259391771019678,"vi). PI-DeepONet [59]: an unsupervised neural operator based on PINN loss and DeepONets. For
243"
EXPERIMENTS,0.5277280858676208,"MCNP Solver, we set the sampling numbers and the time step ‚àÜt as 64 and 0.2, respectively. We
244"
EXPERIMENTS,0.5295169946332737,"interpolate the spatial domain into 1024 elements in the Fourier Interpolation trick.
245"
EXPERIMENTS,0.5313059033989267,"Results
Table 1 presents each method‚Äôs performance and computational cost on the 1D diffusion
246"
EXPERIMENTS,0.5330948121645797,"equation. Among all unsupervised neural PDE solvers, including PI-DeepONet and PINO, the MCNP
247"
EXPERIMENTS,0.5348837209302325,"Solver performs best on all tasks, particularly for cases with large spatial or temporal variations.
248"
EXPERIMENTS,0.5366726296958855,"Despite PINO obtaining comparable results on the simplest tasks (i.e., Œ∫ = 0.01 and N = 6), its error
249"
EXPERIMENTS,0.5384615384615384,"rapidly increases on tasks with Œ∫ = 0.02 or N = 12, which is consistent with our theoretical results.
250"
EXPERIMENTS,0.5402504472271914,"The results of PI-DeepONet indicate that the PINN loss cannot efficiently handle high-frequency
251"
EXPERIMENTS,0.5420393559928444,"components, which has also been observed in previous literature [25, 60]. Compared to the supervised
252"
EXPERIMENTS,0.5438282647584973,"method FNO, MCNP Solver obtains comparable results on the tasks when N = 6 while significantly
253"
EXPERIMENTS,0.5456171735241503,"outperforming it when N = 12, which indicates that more data is required for FNO when handling
254"
EXPERIMENTS,0.5474060822898033,"complex spatial variants. As for classical solvers, PSM fails on all tasks because it requires a fine
255"
EXPERIMENTS,0.5491949910554562,"grid to prevent blowing up, which explains why MCNP Solver can beat PINO. Although PSM+
256"
EXPERIMENTS,0.5509838998211091,"achieves spectral accuracy on the tasks with Œ∫ = 0.01, it still fails to achieve meaningful results when
257"
EXPERIMENTS,0.552772808586762,"Œ∫ = 0.02. Moreover, it is more than 380 times slower than other neural solvers due to the refined
258"
EXPERIMENTS,0.554561717352415,"step size, highlighting one of the main motivations for AI-based PDE studies. MCM‚Äôs performance is
259"
EXPERIMENTS,0.556350626118068,"limited by the variance inherent in Monte Carlo simulation, even sampling 105 particles. However,
260"
EXPERIMENTS,0.5581395348837209,"this stochastic label noise arising from the Monte Carlo simulation does not cause apparent harm
261"
EXPERIMENTS,0.5599284436493739,"to the MCNP Solver due to the involvement of neural networks, which is in line with the studies of
262"
EXPERIMENTS,0.5617173524150268,"label noise [7, 10]. In practice, the sampling numbers in MCNP Solver are only set as 64 per epoch,
263"
EXPERIMENTS,0.5635062611806798,"and the neural network can converge as expected with gradient descent during training.
264"
EXPERIMENTS,0.5652951699463328,"5.2
2D Navier-Stokes Equation
265"
EXPERIMENTS,0.5670840787119857,"In this experiment, we simulate the vorticity field for 2D incompressible flows in a periodic domain
266"
EXPERIMENTS,0.5688729874776386,"‚Ñ¶= [0, 1] √ó [0, 1], whose vortex equation is given as follows:
267 ‚àÇœâ"
EXPERIMENTS,0.5706618962432916,"‚àÇt = ‚àí(u ¬∑ ‚àá)œâ + ŒΩ‚àÜœâ + f(x),
œâ = ‚àá√ó u,
(17)"
EXPERIMENTS,0.5724508050089445,"Table 2: 2D NSE with varying ŒΩ and T. Relative errors (%) and computational costs for baseline methods and
MCNP Solver."
EXPERIMENTS,0.5742397137745975,"Model
Varying ŒΩ
Time
Params"
EXPERIMENTS,0.5760286225402504,"ŒΩ = 10‚àí3
ŒΩ = 10‚àí4
ŒΩ = 10‚àí5
Train (H)
Infer (S)
# (M)"
EXPERIMENTS,0.5778175313059034,T = 10
EXPERIMENTS,0.5796064400715564,"PSM
0.309
NAN
NAN
‚Äì
0.039
‚Äì
PSM+
0.103
0.136
1.521
‚Äì
0.758
‚Äì
FNO
1.421¬± 0.068
5.155¬± 0.290
7.594¬± 0.091
0.934
0.00255
5.319
PINO
1.192¬± 0.043
5.730¬± 0.046
8.952¬± 0.125
0.958
0.00255
5.319
MCNP
1.773¬± 0.117
4.440¬± 0.157
6.539¬± 0.384
0.964
0.00432
4.730"
EXPERIMENTS,0.5813953488372093,T = 15
EXPERIMENTS,0.5831842576028623,"PSM
0.389
NAN
NAN
‚Äì
0.058
‚Äì
PSM+
0.137
0.168
NAN
‚Äì
1.133
‚Äì
FNO
1.391¬± 0.054
5.407¬± 0.103
8.429¬± 0.048
1.636
0.00258
7.238
PINO
2.161¬± 0.193
19.655¬± 5.971
24.185¬± 3.947
1.703
0.00258
7.238
MCNP
2.195¬± 0.142
6.553¬± 0.384
8.677¬± 0.350
1.458
0.00635
7.095 A
B"
EXPERIMENTS,0.5849731663685152,"Figure 2: Simulation of 2D NSE. The ground-truth solution versus the prediction of a learned MCNP Solver
for an example in the test set at t = 10, with the viscosity terms ŒΩ = 10‚àí3 (A) and ŒΩ = 10‚àí5 (B), respectively."
EXPERIMENTS,0.5867620751341681,"where f(x) = 0.1 sin (2œÄ (x1 + x2)) + 0.1 cos (2œÄ (x1 + x2)) is the forcing function, and ŒΩ ‚ààR+
268"
EXPERIMENTS,0.5885509838998211,"represents the viscosity term. The initial vorticity is generated from the Gaussian random field
269"
EXPERIMENTS,0.590339892665474,"N
 
0, 73/2(‚àí‚àÜ+ 49I)‚àí2.5
with periodic boundaries.
270"
EXPERIMENTS,0.592128801431127,"Experimental Setups
The viscosity term ŒΩ can be regarded as a measure of the temporal-spatial
271"
EXPERIMENTS,0.59391771019678,"complexity of NSE. As ŒΩ decreases, the nonlinear term (u ¬∑ ‚àá)œâ gradually governs the motion of
272"
EXPERIMENTS,0.5957066189624329,"fluids, increasing the difficulty of simulation. To evaluate the performance of handling different
273"
EXPERIMENTS,0.5974955277280859,"degrees of turbulence, we conduct the experiments with ŒΩ in {10‚àí3, 10‚àí4, 10‚àí5}, respectively. We
274"
EXPERIMENTS,0.5992844364937389,"choose two different T in {10, 15} to test the long-time simulation ability of each method. We divide
275"
EXPERIMENTS,0.6010733452593918,"the domain ‚Ñ¶into 64 √ó 64 grid elements.
276"
EXPERIMENTS,0.6028622540250447,"Baselines
We introduce the baselines conducted on 2D NSEs, including:1 i). PSM: We divide the
277"
EXPERIMENTS,0.6046511627906976,"time interval into 100 (150) uniform lattices for T = 10 (15) and utilize the Crank‚ÄìNicolson scheme
278"
EXPERIMENTS,0.6064400715563506,"for temporal revolution. ii). PSM+: We divide the time interval into 2000 (3000) uniform lattices
279"
EXPERIMENTS,0.6082289803220036,"for T = 10 (15). iii). FNO: Training with 1000 pre-generated data, taking 0.624 hours for data
280"
EXPERIMENTS,0.6100178890876565,"generation. iv). PINO: We divide the time interval into 100 and 150 uniform lattices for T = 10 and
281"
EXPERIMENTS,0.6118067978533095,"15, respectively. For MCNP Solver, we set the sampling numbers and step size ‚àÜt to 16 and 0.1,
282"
EXPERIMENTS,0.6135957066189625,"respectively. We interpolate the spatial domain into 256 √ó 256 elements in the Fourier Interpolation
283"
EXPERIMENTS,0.6153846153846154,"trick. The ‚àÜT in the multi-scale framework is set to 5 for all tasks.
284"
EXPERIMENTS,0.6171735241502684,"Results
Table 2 presents each method‚Äôs performance and computational cost on the 2D NSEs. As
285"
EXPERIMENTS,0.6189624329159212,"the viscosity term ŒΩ decreases, simulating the flow becomes more challenging for all methods due to
286"
EXPERIMENTS,0.6207513416815742,"increased turbulence, as shown in Fig. 2. Compared to PINO, MCNP Solver achieves comparable
287"
EXPERIMENTS,0.6225402504472272,"results on ŒΩ = 10‚àí3 while outperforming it when ŒΩ = 10‚àí4 and 10‚àí5, indicating that MCNP Solver
288"
EXPERIMENTS,0.6243291592128801,"is more accurate on turbulent flow simulation. Furthermore, MCNP Solver has advantages and
289"
EXPERIMENTS,0.6261180679785331,"disadvantages compared to the supervised baseline FNO. On the one hand, MCNP Solver can learn
290"
EXPERIMENTS,0.627906976744186,"from more training samples due to its data-free regime. On the other hand, the FNO directly uses
291"
EXPERIMENTS,0.629695885509839,"1For PI-DeepONets [59], they only conduct experiments on time-independent PDE in 2D situations in their
paper. Furthermore, MCM cannot directly simulate the nonlinear NSE because the unknown velocity ut+‚àÜt is
required during the simulation of SDE trajectories Œæt+‚àÜt ‚ÜíŒæt."
EXPERIMENTS,0.631484794275492,"the ground-truth data as training labels for all t ‚àà[0, T], thus avoiding accumulated errors arising
292"
EXPERIMENTS,0.6332737030411449,"from the calls of the solver during the training stage like other unsupervised methods. As a result,
293"
EXPERIMENTS,0.6350626118067979,"MCNP Solver and FNO achieve better results on most tasks when T = 10 and 15, respectively. As
294"
EXPERIMENTS,0.6368515205724508,"for classical solvers, PSM only obtains meaningful results when ŒΩ = 10‚àí3, confirming that both
295"
EXPERIMENTS,0.6386404293381037,"PSM and PINO are not robust to coarser time steps. PSM+ achieves the lowest error rate on most
296"
EXPERIMENTS,0.6404293381037567,"tasks but requires almost 180 ‚àº300 times more inference time than other neural solvers.
297"
ABLATION STUDY,0.6422182468694096,"5.3
Ablation Study
298"
ABLATION STUDY,0.6440071556350626,"We performed several ablation studies of MCNP Solver on NSE (ŒΩ = 10‚àí5, T = 15) to understand
299"
ABLATION STUDY,0.6457960644007156,"the contribution of each model component. MCNP-OR replaces the one-step rollout technique with
300"
ABLATION STUDY,0.6475849731663685,"two-step when simulating the SDEs. MCNP-FI and MCNP-MS represent the MCNP Solver without
301"
ABLATION STUDY,0.6493738819320215,"the Fourier Interpolation and multi-scale trick, respectively. MCNP-MC replaces the neural Monte
302"
ABLATION STUDY,0.6511627906976745,"Carlo loss with the PSM loss, which aligns with the loss function in PINO. Table 3 reports the results
303"
ABLATION STUDY,0.6529516994633273,"and training costs. MCNP-OR obtains comparable results with MCNP while spending 44% additional
304"
ABLATION STUDY,0.6547406082289803,"training time. Compared to MCNP with MCNP-FI, the Fourier Interpolation trick can significantly
305"
ABLATION STUDY,0.6565295169946332,"improve the accuracy of MCNP while introducing little extra computational cost. The reason is that
306"
ABLATION STUDY,0.6583184257602862,"the rate-determining step in the training stage is the optimization of neural solvers, and the Fourier
307"
ABLATION STUDY,0.6601073345259392,"Interpolation trick does not involve any calls of solvers. Compared to MCNP with MCNP-MS, we
308"
ABLATION STUDY,0.6618962432915921,"can see that the multi-scale framework plays a vital role in improving the long-time simulation ability
309"
ABLATION STUDY,0.6636851520572451,"of MCNP. Additionally, this architecture can reduce the training time because each sub-network is
310"
ABLATION STUDY,0.6654740608228981,"relatively lightweight. Finally, the gap between MCNP and MCNP-MC reveals the advantages of
311"
ABLATION STUDY,0.667262969588551,"Monte Carlo loss compared to the PSM loss, which is more robust against spatial-temporal variations
312"
ABLATION STUDY,0.669051878354204,in turbulence simulation tasks.
ABLATION STUDY,0.6708407871198568,"Table 3: Ablation Studies of each model component in MCNP Solver. Relative error (%) and training time
for each method on the NSE tasks with ŒΩ = 10‚àí5 and T = 15."
ABLATION STUDY,0.6726296958855098,"MCNP
MCNP-OR
MCNP-FI
MCNP-MS
MCNP-MC"
ABLATION STUDY,0.6744186046511628,"Error (%)
8.677¬± 0.350
8.874¬± 0.150
15.561¬± 0.596
24.107¬± 1.104
14.110¬± 1.789
Time (H)
1.458
2.097
1.431
2.164
1.072 313"
ADDITIONAL NUMERICAL RESULTS,0.6762075134168157,"5.4
Additional Numerical Results
314"
ADDITIONAL NUMERICAL RESULTS,0.6779964221824687,"We also conduct experiments to evaluate the MCNP Solver‚Äôs ability to handle different boundary
315"
ADDITIONAL NUMERICAL RESULTS,0.6797853309481217,"conditions, fractional Laplacian, and irregular grids, as detailed in Appendix C.
316"
CONCLUSION AND DISCUSSION,0.6815742397137746,"6
Conclusion and Discussion
317"
CONCLUSION AND DISCUSSION,0.6833631484794276,"Conclusion In this paper, we propose the MCNP Solver, which leverages the Feynman-Kac formula
318"
CONCLUSION AND DISCUSSION,0.6851520572450805,"to train neural PDE solvers in an unsupervised manner. Theoretically, we characterize the approxima-
319"
CONCLUSION AND DISCUSSION,0.6869409660107334,"tion error and robustness of the MCNP Solver on convection-diffusion equations. Numerical analyses
320"
CONCLUSION AND DISCUSSION,0.6887298747763864,"demonstrate the MCNP Solver‚Äôs ability to adapt to complex spatiotemporal variations and long-time
321"
CONCLUSION AND DISCUSSION,0.6905187835420393,"simulations on diffusion equations and NSEs.
322"
CONCLUSION AND DISCUSSION,0.6923076923076923,"Limitations This paper has several limitations: (1) The theoretical results are lacking when Œ≤ is not
323"
CONCLUSION AND DISCUSSION,0.6940966010733453,"constant, and the gradient flow of the MCNP Solver during the training stage requires further analysis.
324"
CONCLUSION AND DISCUSSION,0.6958855098389982,"(2) Some PDEs are not suitable for the Feynman-Kac formula and therefore do not fall within the
325"
CONCLUSION AND DISCUSSION,0.6976744186046512,"scope of the MCNP Solver, such as third or higher-order PDEs (involving high-order operators like
326"
CONCLUSION AND DISCUSSION,0.6994633273703041,"uxxx). (3) The accuracy of the MCNP Solver cannot outperform numerical solvers when disregarding
327"
CONCLUSION AND DISCUSSION,0.7012522361359571,"inference time, which is also a major drawback for other existing neural solvers [55, 13]. As discussed
328"
CONCLUSION AND DISCUSSION,0.7030411449016101,"in [55], AI-based methods lack precision compared to classical methods while achieving reasonable
329"
CONCLUSION AND DISCUSSION,0.7048300536672629,"accuracy and offering great potential for efficient parameter studies.
330"
CONCLUSION AND DISCUSSION,0.7066189624329159,"Future Work In addition to addressing the limitations, we suggest several directions for future
331"
CONCLUSION AND DISCUSSION,0.7084078711985689,"research: (1) Extend the proposed MCNP Solver to broader scenarios, such as high-dimensional PDEs
332"
CONCLUSION AND DISCUSSION,0.7101967799642218,"and optimal control problems; (2) Utilize techniques from out-of-distribution generalization [53] to
333"
CONCLUSION AND DISCUSSION,0.7119856887298748,"improve the generalization ability of MCNP Solver.
334"
REFERENCES,0.7137745974955277,"References
335"
REFERENCES,0.7155635062611807,"[1] Juan A. Acebr√≥n and Marco A. Ribeiro. A monte carlo method for solving the one-dimensional telegraph
336"
REFERENCES,0.7173524150268337,"equations with boundary conditions. Journal of Computational Physics, 305:29‚Äì43, 2016.
337"
REFERENCES,0.7191413237924866,"[2] W. F. Bauer. The monte carlo method. Journal of the Society for Industrial and Applied Mathematics,
338"
REFERENCES,0.7209302325581395,"6(4):438‚Äì451, 1958.
339"
REFERENCES,0.7227191413237924,"[3] Julius Berner, Markus Dablander, and Philipp Grohs. Numerically solving parametric families of high-
340"
REFERENCES,0.7245080500894454,"dimensional kolmogorov partial differential equations via deep learning. Advances in Neural Information
341"
REFERENCES,0.7262969588550984,"Processing Systems, 33:16615‚Äì16627, 2020.
342"
REFERENCES,0.7280858676207513,"[4] Deniz A Bezgin, Steffen J Schmidt, and Nikolaus A Adams. A data-driven physics-informed finite-volume
343"
REFERENCES,0.7298747763864043,"scheme for nonclassical undercompressive shocks. Journal of Computational Physics, 437:110324, 2021.
344"
REFERENCES,0.7316636851520573,"[5] Johannes Brandstetter, Daniel E. Worrall, and Max Welling. Message passing neural PDE solvers. In
345"
REFERENCES,0.7334525939177102,"International Conference on Learning Representations, 2022.
346"
REFERENCES,0.7352415026833632,"[6] Shuhao Cao. Choose a transformer: Fourier or galerkin. In A. Beygelzimer, Y. Dauphin, P. Liang, and
347"
REFERENCES,0.7370304114490162,"J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021.
348"
REFERENCES,0.738819320214669,"[7] Pengfei Chen, Guangyong Chen, Junjie Ye, Jingwei Zhao, and Pheng-Ann Heng. Noise against noise:
349"
REFERENCES,0.740608228980322,"stochastic label noise helps combat inherent label noise.
In International Conference on Learning
350"
REFERENCES,0.7423971377459749,"Representations, 2021.
351"
REFERENCES,0.7441860465116279,"[8] Zhao Chen, Yang Liu, and Hao Sun. Physics-informed learning of governing equations from scarce data.
352"
REFERENCES,0.7459749552772809,"Nature communications, 12(1):1‚Äì13, 2021.
353"
REFERENCES,0.7477638640429338,"[9] Robert Dalang, Carl Mueller, and Roger Tribe.
A feynman-kac-type formula for the deterministic
354"
REFERENCES,0.7495527728085868,"and stochastic wave equations and other pde‚Äôs. Transactions of the American Mathematical Society,
355"
REFERENCES,0.7513416815742398,"360(9):4681‚Äì4703, 2008.
356"
REFERENCES,0.7531305903398927,"[10] Alex Damian, Tengyu Ma, and Jason D Lee. Label noise sgd provably prefers flat global minimizers.
357"
REFERENCES,0.7549194991055456,"Advances in Neural Information Processing Systems, 34:27449‚Äì27461, 2021.
358"
REFERENCES,0.7567084078711985,"[11] Francis X Giraldo and Beny Neta. A comparison of a family of eulerian and semi-lagrangian finite element
359"
REFERENCES,0.7584973166368515,"methods for the advection-diffusion equation. WIT Transactions on The Built Environment, 30, 1997.
360"
REFERENCES,0.7602862254025045,"[12] Somdatta Goswami, Aniruddha Bora, Yue Yu, and George Em Karniadakis. Physics-informed neural
361"
REFERENCES,0.7620751341681574,"operators. arXiv preprint arXiv:2207.05748, 2022.
362"
REFERENCES,0.7638640429338104,"[13] Tamara G. Grossmann, Urszula Julia Komorowska, Jonas Latz, and Carola-Bibiane Sch√∂nlieb. Can
363"
REFERENCES,0.7656529516994633,"physics-informed neural networks beat the finite element method?, 2023.
364"
REFERENCES,0.7674418604651163,"[14] Ling Guo, Hao Wu, Xiaochen Yu, and Tao Zhou. Monte carlo fpinns: Deep learning method for forward
365"
REFERENCES,0.7692307692307693,"and inverse problems involving high dimensional fractional partial differential equations. Computer
366"
REFERENCES,0.7710196779964222,"Methods in Applied Mechanics and Engineering, 400:115523, 2022.
367"
REFERENCES,0.7728085867620751,"[15] Patrik Simon Hadorn. Shift-deeponet: Extending deep operator networks for discontinuous output functions.
368"
REFERENCES,0.774597495527728,"ETH Zurich, Seminar for Applied Mathematics, 2022.
369"
REFERENCES,0.776386404293381,"[16] Jiequn Han, Arnulf Jentzen, and Weinan E. Solving high-dimensional partial differential equations using
370"
REFERENCES,0.778175313059034,"deep learning. Proceedings of the National Academy of Sciences, 115(34):8505‚Äì8510, 2018.
371"
REFERENCES,0.7799642218246869,"[17] Jihun Han, Mihai Nica, and Adam R Stinchcombe. A derivative-free method for solving elliptic partial
372"
REFERENCES,0.7817531305903399,"differential equations with deep neural networks. Journal of Computational Physics, 419:109672, 2020.
373"
REFERENCES,0.7835420393559929,"[18] Jan Hermann, Zeno Sch√§tzle, and Frank No√©. Deep-neural-network solution of the electronic schr√∂dinger
374"
REFERENCES,0.7853309481216458,"equation. Nature Chemistry, 12(10):891‚Äì897, 2020.
375"
REFERENCES,0.7871198568872988,"[19] Xiang Huang, Zhanhong Ye, Hongsheng Liu, Shi Bei Ji, Zidong Wang, Kang Yang, Yang Li, Min Wang,
376"
REFERENCES,0.7889087656529516,"Haotian CHU, Fan Yu, Bei Hua, Lei Chen, and Bin Dong. Meta-auto-decoder for solving parametric
377"
REFERENCES,0.7906976744186046,"partial differential equations. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
378"
REFERENCES,0.7924865831842576,"editors, Advances in Neural Information Processing Systems, 2022.
379"
REFERENCES,0.7942754919499105,"[20] Xiaowei Jin, Shengze Cai, Hui Li, and George Em Karniadakis. Nsfnets (navier-stokes flow nets): Physics-
380"
REFERENCES,0.7960644007155635,"informed neural networks for the incompressible navier-stokes equations. Journal of Computational
381"
REFERENCES,0.7978533094812165,"Physics, 426:109951, 2021.
382"
REFERENCES,0.7996422182468694,"[21] Matthias Karlbauer, Timothy Praditia, Sebastian Otte, Sergey Oladyshkin, Wolfgang Nowak, and Martin V
383"
REFERENCES,0.8014311270125224,"Butz. Composing partial differential equations with physics-aware neural networks. In International
384"
REFERENCES,0.8032200357781754,"Conference on Machine Learning, pages 10773‚Äì10801. PMLR, 2022.
385"
REFERENCES,0.8050089445438283,"[22] George Em Karniadakis, Ioannis G Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang.
386"
REFERENCES,0.8067978533094812,"Physics-informed machine learning. Nature Reviews Physics, 3(6):422‚Äì440, 2021.
387"
REFERENCES,0.8085867620751341,"[23] Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew
388"
REFERENCES,0.8103756708407871,"Stuart, and Anima Anandkumar. Neural operator: Learning maps between function spaces with applications
389"
REFERENCES,0.8121645796064401,"to pdes. Journal of Machine Learning Research, 24(89):1‚Äì97, 2023.
390"
REFERENCES,0.813953488372093,"[24] Tomasz J Kozubowski, Mark M Meerschaert, and Krzysztof Podgorski. Fractional laplace motion.
391"
REFERENCES,0.815742397137746,"Advances in applied probability, 38(2):451‚Äì464, 2006.
392"
REFERENCES,0.817531305903399,"[25] Aditi Krishnapriyan, Amir Gholami, Shandian Zhe, Robert Kirby, and Michael W Mahoney. Characterizing
393"
REFERENCES,0.8193202146690519,"possible failure modes in physics-informed neural networks. Advances in Neural Information Processing
394"
REFERENCES,0.8211091234347049,"Systems, 34:26548‚Äì26560, 2021.
395"
REFERENCES,0.8228980322003577,"[26] Jae Yong Lee, SungWoong CHO, and Hyung Ju Hwang.
HyperdeepONet: learning operator with
396"
REFERENCES,0.8246869409660107,"complex target function space using the limited resources via hypernetwork. In The Eleventh International
397"
REFERENCES,0.8264758497316637,"Conference on Learning Representations, 2023.
398"
REFERENCES,0.8282647584973166,"[27] Hong Li, Qilong Zhai, and Jeff ZY Chen. Neural-network-based multistate solver for a static schr√∂dinger
399"
REFERENCES,0.8300536672629696,"equation. Physical Review A, 103(3):032405, 2021.
400"
REFERENCES,0.8318425760286225,"[28] Zijie Li, Kazem Meidani, and Amir Barati Farimani. Transformer for partial differential equations‚Äô operator
401"
REFERENCES,0.8336314847942755,"learning. arXiv preprint arXiv:2205.13671, 2022.
402"
REFERENCES,0.8354203935599285,"[29] Zongyi Li, Daniel Zhengyu Huang, Burigede Liu, and Anima Anandkumar. Fourier neural operator with
403"
REFERENCES,0.8372093023255814,"learned deformations for pdes on general geometries. arXiv preprint arXiv:2207.05209, 2022.
404"
REFERENCES,0.8389982110912343,"[30] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew
405"
REFERENCES,0.8407871198568873,"Stuart, and Anima Anandkumar. Neural operator: Graph kernel network for partial differential equations.
406"
REFERENCES,0.8425760286225402,"arXiv preprint arXiv:2003.03485, 2020.
407"
REFERENCES,0.8443649373881932,"[31] Zongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya,
408"
REFERENCES,0.8461538461538461,"Andrew M. Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential
409"
REFERENCES,0.8479427549194991,"equations. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria,
410"
REFERENCES,0.8497316636851521,"May 3-7, 2021. OpenReview.net, 2021.
411"
REFERENCES,0.851520572450805,"[32] Zongyi Li, Hongkai Zheng, Nikola Kovachki, David Jin, Haoxuan Chen, Burigede Liu, Kamyar Aziz-
412"
REFERENCES,0.853309481216458,"zadenesheli, and Anima Anandkumar. Physics-informed neural operator for learning partial differential
413"
REFERENCES,0.855098389982111,"equations. arXiv preprint arXiv:2111.03794, 2021.
414"
REFERENCES,0.8568872987477638,"[33] Lu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, and George Em Karniadakis. Learning nonlinear
415"
REFERENCES,0.8586762075134168,"operators via deeponet based on the universal approximation theorem of operators. Nature Machine
416"
REFERENCES,0.8604651162790697,"Intelligence, 3(3):218‚Äì229, 2021.
417"
REFERENCES,0.8622540250447227,"[34] Sylvain Maire and Etienne Tanr√©. Monte carlo approximations of the neumann problem. In Monte Carlo
418"
REFERENCES,0.8640429338103757,"Methods Appl., 2012.
419"
REFERENCES,0.8658318425760286,"[35] Xuerong Mao. Stochastic differential equations and applications. Elsevier, 2007.
420"
REFERENCES,0.8676207513416816,"[36] Nils Margenberg, Dirk Hartmann, Christian Lessig, and Thomas Richter. A neural network multigrid
421"
REFERENCES,0.8694096601073346,"solver for the navier-stokes equations. Journal of Computational Physics, 460:110983, 2022.
422"
REFERENCES,0.8711985688729875,"[37] Guillermo Marshall. Monte carlo methods for the solution of nonlinear partial differential equations.
423"
REFERENCES,0.8729874776386404,"Computer Physics Communications, 56(1):51‚Äì61, 1989.
424"
REFERENCES,0.8747763864042933,"[38] Revanth Mattey and Susanta Ghosh. A novel sequential method to train physics informed neural networks
425"
REFERENCES,0.8765652951699463,"for allen cahn and cahn hilliard equations. Computer Methods in Applied Mechanics and Engineering,
426"
REFERENCES,0.8783542039355993,"390:114474, 2022.
427"
REFERENCES,0.8801431127012522,"[39] Chlo√© Mimeau and Iraj Mortazavi. A review of vortex methods and their applications: From creation to
428"
REFERENCES,0.8819320214669052,"recent advances. Fluids, 6(2):68, 2021.
429"
REFERENCES,0.8837209302325582,"[40] Sebastian K Mitusch, Simon W Funke, and Miroslav Kuchta. Hybrid fem-nn models: Combining artificial
430"
REFERENCES,0.8855098389982111,"neural networks with the finite element method. Journal of Computational Physics, 446:110651, 2021.
431"
REFERENCES,0.8872987477638641,"[41] Nikolas N√ºsken and Lorenz Richter. Interpolating between bsdes and pinns‚Äìdeep learning for elliptic and
432"
REFERENCES,0.889087656529517,"parabolic boundary value problems. arXiv preprint arXiv:2112.03749, 2021.
433"
REFERENCES,0.8908765652951699,"[42] Panos Pantidis and Mostafa E Mobasher. Integrated finite element neural network (i-fenn) for non-local
434"
REFERENCES,0.8926654740608229,"continuum damage mechanics. Computer Methods in Applied Mechanics and Engineering, 404:115766,
435"
REFERENCES,0.8944543828264758,"2023.
436"
REFERENCES,0.8962432915921288,"[43] Etienne Pardoux and Shige Peng. Backward stochastic differential equations and quasilinear parabolic
437"
REFERENCES,0.8980322003577818,"partial differential equations. In Stochastic partial differential equations and their applications, pages
438"
REFERENCES,0.8998211091234347,"200‚Äì217. Springer, 1992.
439"
REFERENCES,0.9016100178890877,"[44] Etienne Pardoux and Shanjian Tang. Forward-backward stochastic differential equations and quasilinear
440"
REFERENCES,0.9033989266547406,"parabolic pdes. Probability Theory and Related Fields, 114(2):123‚Äì150, 1999.
441"
REFERENCES,0.9051878354203936,"[45] Md Ashiqur Rahman, Zachary E Ross, and Kamyar Azizzadenesheli. U-NO: U-shaped neural operators.
442"
REFERENCES,0.9069767441860465,"Transactions on Machine Learning Research, 2023.
443"
REFERENCES,0.9087656529516994,"[46] Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep
444"
REFERENCES,0.9105545617173524,"learning framework for solving forward and inverse problems involving nonlinear partial differential
445"
REFERENCES,0.9123434704830053,"equations. Journal of Computational physics, 378:686‚Äì707, 2019.
446"
REFERENCES,0.9141323792486583,"[47] Maziar Raissi, Alireza Yazdani, and George Em Karniadakis. Hidden fluid mechanics: Learning velocity
447"
REFERENCES,0.9159212880143113,"and pressure fields from flow visualizations. Science, 367(6481):1026‚Äì1030, 2020.
448"
REFERENCES,0.9177101967799642,"[48] Carl Remlinger, Joseph Mikael, and Romuald Elie. Robust Operator Learning to Solve PDE. working
449"
REFERENCES,0.9194991055456172,"paper or preprint, April 2022.
450"
REFERENCES,0.9212880143112702,"[49] Lorenz Richter and Julius Berner. Robust sde-based variational formulations for solving linear pdes via
451"
REFERENCES,0.9230769230769231,"deep learning. In International Conference on Machine Learning, pages 18649‚Äì18666. PMLR, 2022.
452"
REFERENCES,0.924865831842576,"[50] Lorenz Richter, Leon Sallandt, and Nikolas N√ºsken. Solving high-dimensional parabolic pdes using the
453"
REFERENCES,0.9266547406082289,"tensor train format. In International Conference on Machine Learning, pages 8998‚Äì9009. PMLR, 2021.
454"
REFERENCES,0.9284436493738819,"[51] Rohan Sawhney, Dario Seyb, Wojciech Jarosz, and Keenan Crane. Grid-free monte carlo for pdes with
455"
REFERENCES,0.9302325581395349,"spatially varying coefficients. ACM Transactions on Graphics (TOG), 41(4):1‚Äì17, 2022.
456"
REFERENCES,0.9320214669051878,"[52] Jacob H Seidman, Georgios Kissas, Paris Perdikaris, and George J. Pappas. NOMAD: Nonlinear manifold
457"
REFERENCES,0.9338103756708408,"decoders for operator learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
458"
REFERENCES,0.9355992844364938,"editors, Advances in Neural Information Processing Systems, 2022.
459"
REFERENCES,0.9373881932021467,"[53] Zheyan Shen, Jiashuo Liu, Yue He, Xingxuan Zhang, Renzhe Xu, Han Yu, and Peng Cui. Towards
460"
REFERENCES,0.9391771019677997,"out-of-distribution generalization: A survey. arXiv preprint arXiv:2108.13624, 2021.
461"
REFERENCES,0.9409660107334525,"[54] Wenlei Shi, Xinquan Huang, Xiaotian Gao, Xinran Wei, Jia Zhang, Jiang Bian, Mao Yang, and Tie-Yan
462"
REFERENCES,0.9427549194991055,"Liu. Lordnet: Learning to solve parametric partial differential equations without simulated data. arXiv
463"
REFERENCES,0.9445438282647585,"preprint arXiv:2206.09418, 2022.
464"
REFERENCES,0.9463327370304114,"[55] Derick Nganyu Tanyu, Jianfeng Ning, Tom Freudenberg, Nick Heilenk√∂tter, Andreas Rademacher, Uwe
465"
REFERENCES,0.9481216457960644,"Iben, and Peter Maass. Deep learning methods for partial differential equations and related parameter
466"
REFERENCES,0.9499105545617174,"identification problems, 2022.
467"
REFERENCES,0.9516994633273703,"[56] Alasdair Tran, Alexander Mathews, Lexing Xie, and Cheng Soon Ong. Factorized fourier neural operators.
468"
REFERENCES,0.9534883720930233,"In The Eleventh International Conference on Learning Representations, 2023.
469"
REFERENCES,0.9552772808586762,"[57] Simone Venturi and Tiernan Casey. Svd perspectives for augmenting deeponet flexibility and interpretability.
470"
REFERENCES,0.9570661896243292,"Computer Methods in Applied Mechanics and Engineering, 403:115718, 2023.
471"
REFERENCES,0.9588550983899821,"[58] J. Vom Scheidt. Kloeden, p. e.; platen, e., numerical solution of stochastic differential equations. berlin etc.,
472"
REFERENCES,0.960644007155635,"springer-verlag 1992. xxxvi, 632 pp., 85 figs., dm 118,oo. isbn 3-540-54062-8 (applications of mathematics
473"
REFERENCES,0.962432915921288,"23). ZAMM - Journal of Applied Mathematics and Mechanics / Zeitschrift f√ºr Angewandte Mathematik
474"
REFERENCES,0.964221824686941,"und Mechanik, 74(8):332‚Äì332, 1994.
475"
REFERENCES,0.9660107334525939,"[59] Sifan Wang, Hanwen Wang, and Paris Perdikaris. Learning the solution operator of parametric partial
476"
REFERENCES,0.9677996422182469,"differential equations with physics-informed deeponets. Science advances, 7(40):eabi8605, 2021.
477"
REFERENCES,0.9695885509838998,"[60] Sifan Wang, Xinling Yu, and Paris Perdikaris. When and why pinns fail to train: A neural tangent kernel
478"
REFERENCES,0.9713774597495528,"perspective. Journal of Computational Physics, 449:110768, 2022.
479"
REFERENCES,0.9731663685152058,"[61] Yizheng Wang, Jia Sun, Wei Li, Zaiyuan Lu, and Yinghua Liu. Cenn: Conservative energy method
480"
REFERENCES,0.9749552772808586,"based on neural networks with subdomains for solving variational problems involving heterogeneous and
481"
REFERENCES,0.9767441860465116,"complex geometries. Computer Methods in Applied Mechanics and Engineering, 400:115491, 2022.
482"
REFERENCES,0.9785330948121646,"[62] Li-Ming Yang. Kinetic theory of diffusion in gases and liquids. i. diffusion and the brownian motion.
483"
REFERENCES,0.9803220035778175,"Proceedings of the Royal Society of London. Series A, Mathematical and Physical Sciences, pages 94‚Äì116,
484"
REFERENCES,0.9821109123434705,"1949.
485"
REFERENCES,0.9838998211091234,"[63] Rui Zhang, Peiyan Hu, Qi Meng, Yue Wang, Rongchan Zhu, Bingguang Chen, Zhi-Ming Ma, and Tie-Yan
486"
REFERENCES,0.9856887298747764,"Liu. Drvn (deep random vortex network): A new physics-informed machine learning method for simulating
487"
REFERENCES,0.9874776386404294,"and inferring incompressible fluid flows. Physics of Fluids, 34(10):107112, 2022.
488"
REFERENCES,0.9892665474060823,"[64] Xicheng Zhang. Stochastic functional differential equations driven by l√©vy processes and quasi-linear
489"
REFERENCES,0.9910554561717353,"partial integro-differential equations. The Annals of Applied Probability, 22(6):2505‚Äì2538, 2012.
490"
REFERENCES,0.9928443649373881,"[65] Xicheng Zhang. Stochastic lagrangian particle approach to fractal navier-stokes equations. Communications
491"
REFERENCES,0.9946332737030411,"in Mathematical Physics, 311(1):133‚Äì155, 2012.
492"
REFERENCES,0.9964221824686941,"[66] Qingqing Zhao, David B. Lindell, and Gordon Wetzstein. Learning to solve pde-constrained inverse
493"
REFERENCES,0.998211091234347,"problems with graph networks. In ICML, 2022.
494"
