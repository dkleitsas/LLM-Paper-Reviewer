Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0018796992481203006,"Training very deep neural networks is still an extremely challenging task. The
common solution is to use shortcut connections and normalization layers, which
are both crucial ingredients in the popular ResNet architecture. However, there is
strong evidence to suggest that ResNets behave more like ensembles of shallower
networks than truly deep ones. Recently, it was shown that deep vanilla networks
(i.e. networks without normalization layers or shortcut connections) can be trained
as fast as ResNets by applying certain transformations to their activation functions.
However, this method (called Deep Kernel Shaping) isn’t fully compatible with
ReLUs, and produces networks that overﬁt signiﬁcantly more than ResNets on
ImageNet. In this work, we rectify this situation by developing a new type of
transformation that is fully compatible with a variant of ReLUs – Leaky ReLUs.
We show in experiments that our method, which introduces negligible extra com-
putational cost, achieves validation accuracies with deep vanilla networks that are
competitive with ResNets (of the same width/depth), and signiﬁcantly higher than
those obtained with the Edge of Chaos (EOC) method. And unlike with EOC, the
validation accuracies we obtain do not get worse with depth."
INTRODUCTION,0.0037593984962406013,"1
INTRODUCTION"
INTRODUCTION,0.005639097744360902,"0
20000
40000
60000
80000
100000
Iterations 0.50 0.55 0.60 0.65 0.70 0.75"
INTRODUCTION,0.007518796992481203,Validation acc
LAYERS WITH TAT,0.009398496240601503,"50 layers with TAT
50 layers with EOC
101 layers with TAT
101 layers with EOC
200 layers with TAT
200 layers with EOC"
LAYERS WITH TAT,0.011278195488721804,"Figure 1: Top-1 ImageNet validation accuracy
of vanilla deep networks initialized using either
EOC (with ReLU) or TAT (with LReLU) and
trained with K-FAC."
LAYERS WITH TAT,0.013157894736842105,"Thanks to many architectural and algorithmic innova-
tions, the recent decade has witnessed the unprecedented
success of deep learning in various high-proﬁle chal-
lenges, e.g., the ImageNet recognition task (Krizhevsky
et al., 2012), the challenging board game of Go (Sil-
ver et al., 2017) and human-like text generation (Brown
et al., 2020). Among them, shortcut connections (He
et al., 2016a; Srivastava et al., 2015) and normalization
layers (Ioffe & Szegedy, 2015; Ba et al., 2016) are two
architectural components of modern networks that are
critically important for achieving fast training at very
high depths, and feature prominently in the ubiquitous
ResNet architecture of He et al. (2016b)."
LAYERS WITH TAT,0.015037593984962405,"Despite the success of ResNets, there is signiﬁcant evidence to suggest that the primary reason they
work so well is that they resemble ensembles of shallower networks during training (Veit et al., 2016),
which lets them avoid the common pathologies associated with very deep networks (e.g. Hochreiter
et al., 2001; Duvenaud et al., 2014). Moreover, ResNets without normalization layers could lose
expressivity as the depth goes to inﬁnity (Hayou et al., 2021). In this sense, the question of whether
truly deep networks can be efﬁcient and effectively trained on challenging tasks remains an open one."
LAYERS WITH TAT,0.016917293233082706,"As argued by Oyedotun et al. (2020) and Ding et al. (2021), the multi-branch topology of ResNets
also has certain drawbacks. For example, it is memory-inefﬁcient at inference time, as the input to
every residual block has to be kept in memory until the ﬁnal addition. In particular, the shortcut
branches in ResNet-50 account for about 40% of the memory usage by feature maps. Also, the
classical interpretation of why deep networks perform well – because of the hierarchical feature
representations they produce – does not strictly apply to ResNets, due to their aforementioned
tendency to behave like ensembles of shallower networks. Beyond the drawbacks of ResNets,
training vanilla deep neural networks (which we deﬁne as networks without shortcut connections or"
LAYERS WITH TAT,0.018796992481203006,Published as a conference paper at ICLR 2022
LAYERS WITH TAT,0.020676691729323307,"normalization layers) is an interesting research problem in its own right, and ﬁnding a solution could
open the path to discovering new model architectures. However, recent progress in this direction has
not fully succeeded in matching the generalization performance of ResNets."
LAYERS WITH TAT,0.022556390977443608,"Schoenholz et al. (2017) used a mean-ﬁeld analysis of deep MLPs to choose variances for the initial
weights and bias parameters, and showed that the resulting method – called Edge of Chaos (EOC)
– allowed vanilla networks to be trained at very high depths on small datasets. Building on EOC,
and incorporating dynamical isometry theory, Xiao et al. (2018) was able to train vanilla networks
with Tanh units1 at depths of up to 10,000. While impressive, these EOC-initialized networks
trained signiﬁcantly slower than standard ResNets of the same depth, and also exhibited signiﬁcantly
worse generalization performance. Qi et al. (2020) proposed to enforce the convolution kernels to
be near isometric, but the gaps with ResNets are still signiﬁcant on ImageNet. While Oyedotun
et al. (2020) was able to narrow the generalization gap between vanilla networks and ResNets, their
experiments were limited to networks with only 30 layers, and their networks required many times
more parameters. More recently, Martens et al. (2021) introduced a method called Deep Kernel
Shaping (DKS) for initializing and transforming networks based on an analysis of their initialization-
time kernel properties. They showed that their approach enabled vanilla networks to train faster
than previous methods, even matching the speed of similarly sized ResNets when combined with
stronger optimizers like K-FAC (Martens & Grosse, 2015) or Shampoo (Anil et al., 2020). However,
their method isn’t fully compatible with ReLUs, and in their experiments (which focused on training
speed) their networks exhibited signiﬁcantly more overﬁtting than ResNets."
LAYERS WITH TAT,0.02443609022556391,"Inspired by both DKS and the line of work using mean-ﬁeld theory, we propose a new method
called Tailored Activation Transformation (TAT). TAT inherits the main advantages of DKS, while
working particularly well with the “Leaky ReLU” activation function. TAT enables very deep vanilla
neural networks to be trained on ImageNet without the use of any additional architectural elements,
while only introducing negligible extra computational cost. Using TAT, we demonstrate for the ﬁrst
time that a 50-layer vanilla deep network can nearly match the validation accuracy of its ResNet
counterpart when trained on ImageNet. And unlike with the EOC method, validation accuracy we
achieve does not decrease with depth (see Figure 1). Furthermore, TAT can also be applied to ResNets
without normalization layers, allowing them to match or even exceed the validation accuracy of
standard ResNets of the same width/depth. A multi-framework open source implementation of DKS
and TAT is available at https://github.com/deepmind/dks."
BACKGROUND,0.02631578947368421,"2
BACKGROUND"
BACKGROUND,0.02819548872180451,"Our main tool of analysis will be kernel functions for neural networks (Neal, 1996; Cho & Saul,
2009; Daniely et al., 2016) and the related Q/C maps (Saxe et al., 2013; Poole et al., 2016; Martens
et al., 2021). In this section, we introduce our notation and some key concepts used throughout."
KERNEL FUNCTION APPROXIMATION FOR WIDE NETWORKS,0.03007518796992481,"2.1
KERNEL FUNCTION APPROXIMATION FOR WIDE NETWORKS
For simplicity, we start with the kernel function approximation for feedforward fully-connected
networks, and discuss its extensions to convolutional networks and non-feedforward networks later.
In particular, we will assume a network that is deﬁned by a sequence of L combined layers (each of
which is an afﬁne transformation followed by the elementwise activation function φ) as follows:
xl+1 = φ
 
Wlxl + bl

∈Rdl+1,
(1)"
KERNEL FUNCTION APPROXIMATION FOR WIDE NETWORKS,0.03195488721804511,"with weights Wl ∈Rdl+1×dl initialized as Wl
iid∼N(0, 1/dl) (or scale-corrected uniform orthogonal
matrices (Martens et al., 2021)), and biases bl ∈Rdl+1 initialized to zero. Due to the randomness
of the initial parameters θ, the network can be viewed as random feature model f l
θ(x) ≜xℓat each
layer l (with x0 = x) at initialization time. This induces a random kernel deﬁned as follows:
κl
f(x1, x2) = f l
θ(x1)⊤f l
θ(x2)/dl.
(2)"
KERNEL FUNCTION APPROXIMATION FOR WIDE NETWORKS,0.03383458646616541,"Given these assumptions, as the width of each layer goes to inﬁnity, κl
f(x1, x2) converges in proba-
bility (see Theorem 3) to a deterministic kernel ˜κl
f(x1, x2) that can be computed layer by layer:"
KERNEL FUNCTION APPROXIMATION FOR WIDE NETWORKS,0.03571428571428571,"Σl+1 = Ez∼N(0,Σl)

φ(z)φ(z)⊤
, with Σl =
˜κl
f(x1, x1)
˜κl
f(x1, x2)
˜κl
f(x1, x2)
˜κl
f(x2, x2)"
KERNEL FUNCTION APPROXIMATION FOR WIDE NETWORKS,0.03759398496240601,"
,
(3)"
KERNEL FUNCTION APPROXIMATION FOR WIDE NETWORKS,0.039473684210526314,"where ˜κ0
f(x1, x2) = κ0
f(x1, x2) = x⊤
1 x2/d0."
KERNEL FUNCTION APPROXIMATION FOR WIDE NETWORKS,0.041353383458646614,"1Dynamical isometry is unavailable for ReLU (Pennington et al., 2017), even with orthogonal weights."
KERNEL FUNCTION APPROXIMATION FOR WIDE NETWORKS,0.043233082706766915,Published as a conference paper at ICLR 2022
KERNEL FUNCTION APPROXIMATION FOR WIDE NETWORKS,0.045112781954887216,"2.2
LOCAL Q/C MAPS"
KERNEL FUNCTION APPROXIMATION FOR WIDE NETWORKS,0.046992481203007516,"By equation 3, any diagonal entry ql+1
i
of Σl+1 only depends on the corresponding diagonal entry ql
i
of Σl. Hence, we obtain the following recursion for these diagonal entries, which we call q values:"
KERNEL FUNCTION APPROXIMATION FOR WIDE NETWORKS,0.04887218045112782,"ql+1
i
= Q(ql
i) = Ez∼N(0,ql
i)[φ(z)2] = Ez∼N(0,1)"
KERNEL FUNCTION APPROXIMATION FOR WIDE NETWORKS,0.05075187969924812,"
φ(
q"
KERNEL FUNCTION APPROXIMATION FOR WIDE NETWORKS,0.05263157894736842,"ql
iz)2

, with q0
i = ∥xi∥2/d0
(4)"
KERNEL FUNCTION APPROXIMATION FOR WIDE NETWORKS,0.05451127819548872,"where Q is the local Q map. We note that ql
i is an approximation of κl
f(xi, xi). Analogously, one
can write the recursion for the normalized off-diagonal entries, which we call c values, as:"
KERNEL FUNCTION APPROXIMATION FOR WIDE NETWORKS,0.05639097744360902,"cl+1 = C(cl, ql
1, ql
2) =
E[ z1
z2 ]∼N(0,Σl) [φ(z1)φ(z2)] p"
KERNEL FUNCTION APPROXIMATION FOR WIDE NETWORKS,0.05827067669172932,"Q(ql
1)Q(ql
2)
, with Σl =

ql
1
√"
KERNEL FUNCTION APPROXIMATION FOR WIDE NETWORKS,0.06015037593984962,"ql
1ql
2cl
√"
KERNEL FUNCTION APPROXIMATION FOR WIDE NETWORKS,0.06203007518796992,"ql
1ql
2cl
ql
2"
KERNEL FUNCTION APPROXIMATION FOR WIDE NETWORKS,0.06390977443609022,"
,
(5)"
KERNEL FUNCTION APPROXIMATION FOR WIDE NETWORKS,0.06578947368421052,"where C is the local C map and c0 = x⊤
1 x2/d0. We note that cl is an approximation of the cosine
similarity between f l
θ(x1) and f l
θ(x2). Because C is a three dimensional function, it is difﬁcult to
analyze, as the associated q values can vary wildly for distinct inputs. However, by scaling the inputs
to have norm √d0, and rescaling φ so that Q(1) = 1, it follows that ql
i = 1 for all l. This allows us
to treat C as a one dimensional function from [−1, 1] to [−1, 1] satisfying C(1) = 1. Additionally, it
can be shown that C possesses special structure as a positive deﬁnite function (see Appendix A.4 for
details). Going forward, we will thus assume that q0
i = 1, and that φ is scaled so that Q(1) = 1."
EXTENSIONS TO CONVOLUTIONAL NETWORKS AND MORE COMPLEX TOPOLOGIES,0.06766917293233082,"2.3
EXTENSIONS TO CONVOLUTIONAL NETWORKS AND MORE COMPLEX TOPOLOGIES"
EXTENSIONS TO CONVOLUTIONAL NETWORKS AND MORE COMPLEX TOPOLOGIES,0.06954887218045112,"As argued in Martens et al. (2021), Q/C maps can also be deﬁned for convolutional networks if one
adopts a Delta initialization (Balduzzi et al., 2017; Xiao et al., 2018), in which all weights except those
in the center of the ﬁlter are initialized to zero. Intuitively, this makes convolutional networks behave
like a collection of fully-connected networks operating independently over feature map locations. As
such, the Q/C map computations for a feed-forward convolutional network are the same as above.
Martens et al. (2021) also gives formulas to compute q and c values for weighted sum operations
between the outputs of multiple layers (without nonlinearities), thus allowing more complex network
topologies. In particular, the sum operation’s output q value is given by q = Pn
i=1 w2
i qi, and its
output c value is given by 1"
EXTENSIONS TO CONVOLUTIONAL NETWORKS AND MORE COMPLEX TOPOLOGIES,0.07142857142857142,"q
Pn
i=1 w2
i qici. In order to maintain the property that all q values are 1 in
the network, we will assume that sum operations are normalized in the sense that Pn
i=1 w2
i = 1."
EXTENSIONS TO CONVOLUTIONAL NETWORKS AND MORE COMPLEX TOPOLOGIES,0.07330827067669173,"Following Martens et al. (2021), we will extend the deﬁnition of Q/C maps to include global Q/C
maps, which describe the behavior of entire networks. Global maps, denoted by Qf and Cf for a
given network f, can be computed by applying the above rules for each layer in f. For example, the
global C map of a three-layer network f is simply Cf(c) = C ◦C ◦C(c). Like the local C map, global
C maps are positive deﬁnite functions (see Appendix A.4). In this work, we restrict our attention to
the family of networks comprising of combined layers, and normalized sums between the output of
multiple afﬁne layers, for which we can compute global Q/C maps. And all of our formal results will
implicitly assume this family of networks."
EXTENSIONS TO CONVOLUTIONAL NETWORKS AND MORE COMPLEX TOPOLOGIES,0.07518796992481203,"2.4
Q/C MAPS FOR RESCALED RESNETS
ResNets consist of a sequence of residual blocks, each of which computes the sum of a residual
branch (which consists of a small multi-layer convolutional network) and a shortcut branch (which
copies the block’s input). In order to analyze ResNets we will consider the modiﬁed version used
in Shao et al. (2020) and Martens et al. (2021) which removes the normalization layers found in
the residual branches, and replaces the sum at the end of each block with a normalized sum. These
networks, which we will call rescaled ResNets, are deﬁned by the following recursion:"
EXTENSIONS TO CONVOLUTIONAL NETWORKS AND MORE COMPLEX TOPOLOGIES,0.07706766917293233,"xl+1 = wxl +
p"
EXTENSIONS TO CONVOLUTIONAL NETWORKS AND MORE COMPLEX TOPOLOGIES,0.07894736842105263,"1 −w2R(xl),
(6)"
EXTENSIONS TO CONVOLUTIONAL NETWORKS AND MORE COMPLEX TOPOLOGIES,0.08082706766917293,"where R is the residual branch, and w is the shortcut weight (which must be in [−1, 1]). Applying
the previously discussed rules for computing Q/C maps, we get ql
i = 1 for all l and"
EXTENSIONS TO CONVOLUTIONAL NETWORKS AND MORE COMPLEX TOPOLOGIES,0.08270676691729323,"cl+1 = w2cl + (1 −w2)CR(cl).
(7)"
EXISTING SOLUTIONS AND THEIR LIMITATIONS,0.08458646616541353,"3
EXISTING SOLUTIONS AND THEIR LIMITATIONS"
EXISTING SOLUTIONS AND THEIR LIMITATIONS,0.08646616541353383,"Global Q/C maps can be intuitively understood as a way of characterizing signal propagation through
the network f at initialization time. The q value approximates the squared magnitude of the activation"
EXISTING SOLUTIONS AND THEIR LIMITATIONS,0.08834586466165413,Published as a conference paper at ICLR 2022
EXISTING SOLUTIONS AND THEIR LIMITATIONS,0.09022556390977443,"vector, so that Qf describe the contraction or expansion of this magnitude through the action of f.
On the other hand, the c value approximates the cosine similarity of the function values for different
inputs, so that Cf describes how well f preserves this cosine similarity from its input to its output."
EXISTING SOLUTIONS AND THEIR LIMITATIONS,0.09210526315789473,"Standard initializations methods (LeCun et al., 1998; Glorot & Bengio, 2010; He et al., 2015) are
motivated through an analysis of how the variance of the activations evolves throughout the network.
This can be viewed as a primitive form of Q map analysis, and from that perspective, these methods
are trying to ensure that q values remain stable throughout the network by controlling the local Q map.
This is necessary for trainability, since very large or tiny q values can cause numerical issues, saturated
activation functions (which have implications for C maps), and problems with scale-sensitive losses.
However, as was ﬁrst observed by Schoenholz et al. (2017), a well-behaved C map is also necessary
for trainability. When the global C map is close to a constant function (i.e. degenerate) on (−1, 1),
which easily happens in deep networks (as discussed in Appendix A.2), this means that the network’s
output will appear either constant or random looking, and won’t convey any useful information about
the input. Xiao et al. (2020) and Martens et al. (2021) give more formal arguments for why this leads
to slow optimization and/or poor generalization under gradient descent."
EXISTING SOLUTIONS AND THEIR LIMITATIONS,0.09398496240601503,"−1.00 −0.75 −0.50 −0.25
0.00
0.25
0.50
0.75
1.00
input c value 0.0 0.2 0.4 0.6 0.8 1.0"
EXISTING SOLUTIONS AND THEIR LIMITATIONS,0.09586466165413533,output c value
EXISTING SOLUTIONS AND THEIR LIMITATIONS,0.09774436090225563,"ReLU with depth 10
ReLU with depth 50
TReLU with depth 2
TReLU with depth 10
TReLU with depth 50
TReLU with depth 1000"
EXISTING SOLUTIONS AND THEIR LIMITATIONS,0.09962406015037593,"Figure 2: Global C maps for ReLU networks
(EOC) and TReLU networks (Cf(0) = 0.5).
The global C map of a TReLU network con-
verges to a well-behavior function as depth
increases (proved in Proposition 3)."
EXISTING SOLUTIONS AND THEIR LIMITATIONS,0.10150375939849623,"Several previous works (Schoenholz et al., 2017; Yang &
Schoenholz, 2017; Hayou et al., 2019) attempt to achieve
a well-behaved global C map by choosing the variance
of the initial weights and biases in each layer such that
C′(1) = 1 – a procedure which is referred to as Edge of
Chaos (EOC). However, this approach only slows down
the convergence (with depth) of the c values from exponen-
tial to sublinear (Hayou et al., 2019), and does not solve the
fundamental issue of degenerate global C maps for very
deep networks. In particular, the global C map of a deep
network with ReLU and EOC initialization rapidly concen-
trates around 1 as depth increases (see Figure 2). While
EOC allows very deep vanilla networks to be trained, the
training speed and generalization performance is typically
much worse than for comparable ResNets. Klambauer et al. (2017) applied an afﬁne transformation
to the output of activation functions to achieve Q(1) = 1 and C(0) = 0, while Lu et al. (2020) applied
them to achieve Q(1) = 1 and C′(1) = 1, although the effect of both approaches is similar to EOC."
EXISTING SOLUTIONS AND THEIR LIMITATIONS,0.10338345864661654,"To address these problems, Martens et al. (2021) introduced DKS, which enforces the conditions
Cf(0) = 0 and C′
f(1) = ζ (for some modest constant ζ > 1) directly on the network’s global C map
Cf. They show that these conditions, along with the positive deﬁniteness of C maps, cause Cf to be
close to the identity and thus well-behaved. In addition to these C map conditions, DKS enforces
that Q(1) = 1 and Q′(1) = 1, which lead to constant q values of 1 in the network, and lower kernel
approximation error (respectively). DKS enforces these Q/C map conditions by applying a model
class-preserving transformation ˆφ(x) = γ(φ(αx + β) + δ). with non-trainable parameters α, β, γ
and δ. The hyperparameter ζ is chosen to be sufﬁciently greater than 1 (e.g. 1.5) in order to prevent
the transformed activation functions from looking “nearly linear” (as they would be exactly linear if
ζ = 1), which Martens et al. (2021) argue makes it hard for the network to achieve nonlinear behavior
during training. Using DKS, they were able to match the training speed of ResNets on ImageNet with
vanilla networks using K-FAC. However, DKS is not fully compatible with ReLUs, and the networks
in their experiments fell substantially short of ResNets in terms of generalization performance."
EXISTING SOLUTIONS AND THEIR LIMITATIONS,0.10526315789473684,"4
TAILORED ACTIVATION TRANSFORMATION (TAT)"
EXISTING SOLUTIONS AND THEIR LIMITATIONS,0.10714285714285714,"The reason why DKS is not fully compatible with ReLUs is that they are positive homogeneous,
i.e. φ(αx) = αφ(x) for α ≥0. This makes the γ parameter of the transformed activation function
redundant, thus reducing the degrees of freedom with which to enforce DKS’s four Q/C map
conditions. Martens et al. (2021) attempt to circumvent this issue by dropping the condition Q′(1) =
1, which leads to vanilla deep networks that are trainable, but slower to optimize compared to using
DKS with other activation functions. This is a signiﬁcant drawback for DKS, as the best generalizing
deep models often use ReLU-family activations. We therefore set out to investigate other possible
remedies – either in the form of different activation functions, new Q/C map conditions, or both. To
this end, we adopt a ReLU-family activation function with an extra degree of freedom (known as
“Leaky ReLU”), and modify the Q/C map conditions in order to preserve certain desirable properties"
EXISTING SOLUTIONS AND THEIR LIMITATIONS,0.10902255639097744,Published as a conference paper at ICLR 2022
EXISTING SOLUTIONS AND THEIR LIMITATIONS,0.11090225563909774,Table 1: Comparison of different methods applied to a network f.
EXISTING SOLUTIONS AND THEIR LIMITATIONS,0.11278195488721804,"EOC (smooth)
EOC (LReLU)
DKS
TAT (smooth)
TAT (LReLU)"
EXISTING SOLUTIONS AND THEIR LIMITATIONS,0.11466165413533834,"q∞exists
C′(1, q∞,q∞) = 1
Q(q) = q
C′(1) = 1"
EXISTING SOLUTIONS AND THEIR LIMITATIONS,0.11654135338345864,"Q(1) = 1
Q′(1) = 1
Cf(0) = 0
C′
f(1) = ζ"
EXISTING SOLUTIONS AND THEIR LIMITATIONS,0.11842105263157894,"Q(1) = 1
Q′(1) = 1
C′
f(1) = 1
C′′
f (1) = τ"
EXISTING SOLUTIONS AND THEIR LIMITATIONS,0.12030075187969924,"Q(q) = q
C′
f(1) = 1
Cf(0) = η"
EXISTING SOLUTIONS AND THEIR LIMITATIONS,0.12218045112781954,"of this choice. The resulting method, which we name Tailored Activation Transformation (TAT)
achieves competitive generalization performance with ResNets in our experiments."
TAILORED ACTIVATION TRANSFORMATION FOR LEAKY RELUS,0.12406015037593984,"4.1
TAILORED ACTIVATION TRANSFORMATION FOR LEAKY RELUS"
TAILORED ACTIVATION TRANSFORMATION FOR LEAKY RELUS,0.12593984962406016,"One way of addressing the issue of DKS’s partial incompatibility with ReLUs is to consider a slightly
different activation function – namely the Leaky ReLU (LReLU) (Maas et al., 2013):"
TAILORED ACTIVATION TRANSFORMATION FOR LEAKY RELUS,0.12781954887218044,"φα(x) = max{x, 0} + α min{x, 0},
(8)"
TAILORED ACTIVATION TRANSFORMATION FOR LEAKY RELUS,0.12969924812030076,"where α is the negative slope parameter. While using LReLUs with α ̸= 0 in place of ReLUs changes
the model class, it doesn’t limit the model’s expressive capabilities compared to ReLU, as assuming
α ̸= ±1, one can simulate a ReLU network with a LReLU network of the same depth by doubling
the number of neurons (see Proposition 4). Rather than using a ﬁxed value for α, we will use it as
an extra parameter to satisfy our desired Q/C map conditions. Deﬁne ˜φα(x) =
q"
TAILORED ACTIVATION TRANSFORMATION FOR LEAKY RELUS,0.13157894736842105,"2
1+α2 φα(x). By
Lemma 1, the local Q and C maps for this choice of activation function are:"
TAILORED ACTIVATION TRANSFORMATION FOR LEAKY RELUS,0.13345864661654136,"Q(q) = q
and
C(c) = c +
(1−α)2"
TAILORED ACTIVATION TRANSFORMATION FOR LEAKY RELUS,0.13533834586466165,"π(1+α2)
p"
TAILORED ACTIVATION TRANSFORMATION FOR LEAKY RELUS,0.13721804511278196,"1 −c2 −c cos−1(c)

.
(9)"
TAILORED ACTIVATION TRANSFORMATION FOR LEAKY RELUS,0.13909774436090225,"Note that the condition Q(q) = q is actually stronger than DKS’s Q map conditions (Q(1) = 1 and
Q′(1) = 1), and has the potential to reduce kernel approximation errors in ﬁnite width networks
compared to DKS, as it provides a better guarantee on the stability of Qf w.r.t. random perturbations
of the q values at each layer. Additionally, because the form of C does not depend on either of the
layer’s input q values, it won’t be affected by such perturbations at all. (Notably, if one uses the
negative slope parameter to transform LReLUs with DKS, these properties will not be achieved.) In
support of these intuitions is the fact that better bounds on the kernel approximation error exist for
ReLU networks than for general smooth ones (as discussed in Appendix A.1)."
TAILORED ACTIVATION TRANSFORMATION FOR LEAKY RELUS,0.14097744360902256,"Another consequence of using ˜φα(x) for our activation function is that we have C′(1) = 1 as in EOC.
If combined with the condition C(0) = 0 (which is used to achieve Cf(0) = 0 in DKS) this would
imply by Theorem 1 that C is the identity function, which by equation 9 is only true when α = 1,
thus resulting in a linear network. In order to avoid this situation, and the closely related one where
˜φα appears “nearly linear”, we instead choose the value of α so that Cf(0) = η, for a hyperparameter
0 ≤η ≤1. As shown in the following theorem, η controls how close Cf is to the identity, thus
allowing us to achieve a well-behaved global C map without making ˜φα nearly linear:"
TAILORED ACTIVATION TRANSFORMATION FOR LEAKY RELUS,0.14285714285714285,"Theorem 1. For a network f with ˜φα(x) as its activation function (with α ≥0), we have"
TAILORED ACTIVATION TRANSFORMATION FOR LEAKY RELUS,0.14473684210526316,"max
c∈[−1,1] |Cf(c) −c| ≤min {4Cf(0), 1 + Cf(0)} ,
max
c∈[−1,1]"
TAILORED ACTIVATION TRANSFORMATION FOR LEAKY RELUS,0.14661654135338345,"C′
f(c) −1
 ≤min {4Cf(0), 1}
(10)"
TAILORED ACTIVATION TRANSFORMATION FOR LEAKY RELUS,0.14849624060150377,Another motivation for using ˜φα(x) as an activation function is given by the following proposition:
TAILORED ACTIVATION TRANSFORMATION FOR LEAKY RELUS,0.15037593984962405,"Proposition 1. The global C map of a feedforward network with ˜φα(x) as its activation function
is equal to that of a rescaled ResNet of the same depth (see Section 2.4) with normalized ReLU
activation φ(x) =
√"
TAILORED ACTIVATION TRANSFORMATION FOR LEAKY RELUS,0.15225563909774437,"2 max(x, 0), shortcut weight
q"
TAILORED ACTIVATION TRANSFORMATION FOR LEAKY RELUS,0.15413533834586465,"α
1+α2 , and residual branch R consisting of a"
TAILORED ACTIVATION TRANSFORMATION FOR LEAKY RELUS,0.15601503759398497,combined layer (or just a normalized ReLU activation) followed by an afﬁne layer.
TAILORED ACTIVATION TRANSFORMATION FOR LEAKY RELUS,0.15789473684210525,"This result implies that at initialization, a vanilla network using ˜φα behaves similarly to a ResNet, a
property that is quite desirable given the success that ResNets have already demonstrated."
TAILORED ACTIVATION TRANSFORMATION FOR LEAKY RELUS,0.15977443609022557,"In summary, we have the following three conditions:"
TAILORED ACTIVATION TRANSFORMATION FOR LEAKY RELUS,0.16165413533834586,"Q(q) = q,
C′
f(1) = 1,
Cf(0) = η,
(11)"
TAILORED ACTIVATION TRANSFORMATION FOR LEAKY RELUS,0.16353383458646617,"which we achieve by picking the negative slope parameter α so that Cf(0) = η. We deﬁne the
Tailored Rectiﬁer (TReLU) to be ˜φα with α chosen in this way. Note that the ﬁrst two conditions are"
TAILORED ACTIVATION TRANSFORMATION FOR LEAKY RELUS,0.16541353383458646,Published as a conference paper at ICLR 2022
TAILORED ACTIVATION TRANSFORMATION FOR LEAKY RELUS,0.16729323308270677,"also true when applying the EOC method to LReLUs, and its only the third which sets TReLU apart.
While this might seem like a minor difference, it actually matters a lot to the behavior of the global C
map. This can be seen in Figure 2 where the c value quickly converges towards 1 with depth under
EOC, resulting in a degenerate global C map. By contrast, the global C map of TReLU for a ﬁxed
η converges rapidly to a nice function, suggesting a very deep vanilla network with TReLU has the
same well-behaved global C map as a shallow network. We prove this in Proposition 3 by showing
the local C map in equation 9 converges to an ODE as we increase the depth. For direct comparison
of all Q/C map conditions, we refer the readers to Table 1."
TAILORED ACTIVATION TRANSFORMATION FOR LEAKY RELUS,0.16917293233082706,"For the hyperparameter 0 ≤η ≤1, we note that a value very close to 0 will produce a network that
is “nearly linear”, while a value very close to 1 will give rise to a degenerate C map. In practice we
use η = 0.9 or 0.95, which seems to work well in most settings. Once we decide on η, we can solve
the value α using binary search by exploiting the closed-form form of C in equation 9 to efﬁciently
compute Cf(0). For instance, if f is a 100 layer vanilla network, one can compute Cf(0) as follows:"
TAILORED ACTIVATION TRANSFORMATION FOR LEAKY RELUS,0.17105263157894737,Cf(0) =
"TIMES
Z",0.17293233082706766,"100 times
z
}|
{
C ◦C · · · C ◦C(0),
(12)"
"TIMES
Z",0.17481203007518797,"which is a function of α. This approach can be generalized to more advanced architectures, such as
rescaled ResNets, as discussed in Appendix B."
TAILORED ACTIVATION TRANSFORMATION FOR SMOOTH ACTIVATION FUNCTIONS,0.17669172932330826,"4.2
TAILORED ACTIVATION TRANSFORMATION FOR SMOOTH ACTIVATION FUNCTIONS"
TAILORED ACTIVATION TRANSFORMATION FOR SMOOTH ACTIVATION FUNCTIONS,0.17857142857142858,"Unlike LReLU, most activation functions don’t have closed-form formulas for their local C maps. As
a result, the computation of Cf(0) involves the numerical approximation of many two-dimensional
integrals to high precision (as in equation 5), which can be quite expensive. One alternative way to
control how close Cf is to the identity, while maintaining the condition C′
f(1) = 1, is to modulate its
second derivative C′′
f (1). The validity of this approach is established by the following theorem:"
TAILORED ACTIVATION TRANSFORMATION FOR SMOOTH ACTIVATION FUNCTIONS,0.18045112781954886,"Theorem 2. Suppose f is a network with a smooth activation function. If C′
f(1) = 1, then we have"
TAILORED ACTIVATION TRANSFORMATION FOR SMOOTH ACTIVATION FUNCTIONS,0.18233082706766918,"max
c∈[−1,1] |Cf(c) −c| ≤2C′′
f (1),
max
c∈[−1,1]"
TAILORED ACTIVATION TRANSFORMATION FOR SMOOTH ACTIVATION FUNCTIONS,0.18421052631578946,"C′
f(c) −1
 ≤2C′′
f (1)
(13)"
TAILORED ACTIVATION TRANSFORMATION FOR SMOOTH ACTIVATION FUNCTIONS,0.18609022556390978,"Given C(1) = 1 and C′(1) = 1, a straightforward computation shows that C′′
f (1) = LC′′(1) if f is an
L-layer vanilla network. (See Appendix B for a discussion of how to do this computation for more
general architectures.) From this we obtain the following four local Q/C map conditions:"
TAILORED ACTIVATION TRANSFORMATION FOR SMOOTH ACTIVATION FUNCTIONS,0.18796992481203006,"Q(1) = 1,
Q′(1) = 1,
C′′(1) = τ/L,
C′(1) = 1.
(14)"
TAILORED ACTIVATION TRANSFORMATION FOR SMOOTH ACTIVATION FUNCTIONS,0.18984962406015038,"To achieve these we adopt the same activation transformation as DKS: ˆφ(x) = γ(φ(αx + β) + δ) for
non-trainable scalars α, β, δ, and γ. We emphasize that these conditions cannot be used with LReLU,
as LReLU networks have C′′(1) = ∞. By equation 4 and basic properties of expectations, we have"
TAILORED ACTIVATION TRANSFORMATION FOR SMOOTH ACTIVATION FUNCTIONS,0.19172932330827067,"1 = Q(1) = Ez∼N(0,1)
h
ˆφ(z)2i
= γ2Ez∼N(0,1)

(φ(αz + β) + δ)2
(15)"
TAILORED ACTIVATION TRANSFORMATION FOR SMOOTH ACTIVATION FUNCTIONS,0.19360902255639098,"so that γ = Ez∼N(0,1)

(φ(αz + β) + δ)2−1/2. To obtain the values for α, β and δ, we can treat
the remaining conditions as a three-dimensional nonlinear system, which can be written as follows:"
TAILORED ACTIVATION TRANSFORMATION FOR SMOOTH ACTIVATION FUNCTIONS,0.19548872180451127,"Ez∼N(0,1)
h
ˆφ(z)ˆφ′(z)z
i
= Q′(1) = 1,"
TAILORED ACTIVATION TRANSFORMATION FOR SMOOTH ACTIVATION FUNCTIONS,0.19736842105263158,"Ez∼N(0,1)
h
ˆφ′′(z)2i
= C′′(1) = τ/L, Ez∼N(0,1)
h
ˆφ′(z)2i
= C′(1) = 1.
(16)"
TAILORED ACTIVATION TRANSFORMATION FOR SMOOTH ACTIVATION FUNCTIONS,0.19924812030075187,"We do not have a closed-form solution of this system. However, each expectation is a one dimensional
integral, and so can be quickly evaluated to high precision using Gaussian quadrature. One can then
use black-box nonlinear equation solvers, such as modiﬁed Powell’s method (Powell, 1964), to obtain
a solution. See https://github.com/deepmind/dks for a complete implementation."
EXPERIMENTS,0.20112781954887218,"5
EXPERIMENTS"
EXPERIMENTS,0.20300751879699247,"Our main experimental evaluation of TAT and competing approaches is on training deep convolutional
networks for ImageNet classiﬁcation (Deng et al., 2009). The goal of these experiments is not to
achieve state-of-the-art, but rather to compare TAT as fairly as possible with existing methods, and
standard ResNets in particular. To this end, we use ResNet V2 (He et al., 2016b) as the main reference"
EXPERIMENTS,0.20488721804511278,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.20676691729323307,"architecture, from which we obtain rescaled ResNets (by removing normalization layers and weighing
the branches as per equation 6), and vanilla networks (by further removing shortcuts). For networks
without batch normalization, we add dropout to the penultimate layer for regularization, as was done
in Brock et al. (2021b). We train the models with 90 epochs and a batch size of 1024, unless stated
otherwise. For TReLU, we obtain η by grid search in {0.9, 0.95}. The weight initialization used
for all methods is the Orthogonal Delta initialization, with an extra multiplier given by σw. We
initialize biases iid from N(0, σ2
b). We use (σw, σb) = (1, 0) in all experiments (unless explicitly
stated otherwise), with the single exception that we use (σw, σb) = (
√"
EXPERIMENTS,0.20864661654135339,"2, 0) in standard ResNets, as
per standard practice (He et al., 2015). For all other details see Appendix D."
TOWARDS REMOVING BATCH NORMALIZATION,0.21052631578947367,"5.1
TOWARDS REMOVING BATCH NORMALIZATION"
TOWARDS REMOVING BATCH NORMALIZATION,0.212406015037594,"Two crucial components for the successful training of very deep neural networks are shortcut
connections and batch normalization (BN) layers. As argued in De & Smith (2020) and Shao et al.
(2020), BN implicitly biases the residual blocks toward the identity function, which makes the
network better behaved at initialization time, and thus easier to train. This suggests that one can
compensate for the removal of BN layers, at least in terms of their effect on the behaviour of the
network at initialization time, by down-scaling the residual branch of each residual block. Arguably,
almost all recent work on training deep networks without normalization layers (Zhang et al., 2018;
Shao et al., 2020; Bachlechner et al., 2020; Brock et al., 2021a;b) has adopted this idea by introducing
multipliers on the residual branches (which may or may not be optimized during training)."
TOWARDS REMOVING BATCH NORMALIZATION,0.21428571428571427,"Table 2: Top-1 validation accuracy of rescaled ResNet50 with
varying shortcut weights. We set η = 0.9 for TReLU."
TOWARDS REMOVING BATCH NORMALIZATION,0.2161654135338346,"Optimizer
Standard
ResNet
Activation
Rescaled ResNet (w)"
TOWARDS REMOVING BATCH NORMALIZATION,0.21804511278195488,"0.0
0.5
0.8
0.9"
TOWARDS REMOVING BATCH NORMALIZATION,0.2199248120300752,"K-FAC
76.4
ReLU
72.6
74.5
75.6
75.9
TReLU
74.6
75.5
76.4
75.9"
TOWARDS REMOVING BATCH NORMALIZATION,0.22180451127819548,"SGD
76.3
ReLU
63.7
72.4
73.9
75.0
TReLU
71.0
72.6
76.0
74.8"
TOWARDS REMOVING BATCH NORMALIZATION,0.2236842105263158,"In Table 2, we show that one can close
most of the gap with standard ResNets
by simply adopting the modiﬁcation
in equation 6 without using BN lay-
ers. By further replacing ReLU with
TReLU, we can exactly match the per-
formance of standard ResNets. With
K-FAC as the optimizer, the rescaled
ResNet with shortcut weight w = 0.9
is only 0.5 shy of the validation accuracy (76.4) of the standard ResNet. Further replacing ReLU
with TReLU, we match the performance of standard ResNet with shortcut weight w = 0.8."
THE DIFFICULTY OF REMOVING SHORTCUT CONNECTIONS,0.22556390977443608,"5.2
THE DIFFICULTY OF REMOVING SHORTCUT CONNECTIONS"
THE DIFFICULTY OF REMOVING SHORTCUT CONNECTIONS,0.2274436090225564,"Table 3: ImageNet top-1 validation accuracies of
shortcut-free networks on ImageNet."
THE DIFFICULTY OF REMOVING SHORTCUT CONNECTIONS,0.22932330827067668,"Depth
Optimizers
vanilla
BN
LN"
K-FAC,0.231203007518797,"50
K-FAC
72.6
72.8
72.7
SGD
63.7
72.6
58.1"
K-FAC,0.23308270676691728,"101
K-FAC
71.8
67.6
72.0
SGD
41.6
43.4
28.6"
K-FAC,0.2349624060150376,"While the aforementioned works have shown that it
is possible to achieve competitive results without nor-
malization layers, they all rely on the use of shortcut
connections to make the network look more linear at
initialization. A natural question to ask is whether
normalization layers could compensate for the re-
moval of shortcut connections. We address this ques-
tion by training shortcut-free networks with either
BN or Layer Normalization (LN) layers. As shown in Table 3, these changes do not seem to make a
signiﬁcant difference, especially with strong optimizers like K-FAC. These ﬁndings are in agreement
with the analyses of Yang et al. (2019) and Martens et al. (2021), who respectively showed that deep
shortcut-free networks with BN layers still suffer from exploding gradients, and deep shortcut-free
networks with LN layers still have degenerate C maps."
TRAINING DEEP NEURAL NETWORKS WITHOUT SHORTCUTS,0.23684210526315788,"5.3
TRAINING DEEP NEURAL NETWORKS WITHOUT SHORTCUTS"
TRAINING DEEP NEURAL NETWORKS WITHOUT SHORTCUTS,0.2387218045112782,"The main motivation for developing TAT is to help deep vanilla networks achieve generalization
performance similar to standard ResNets. In our investigations we include rescaled ResNets with
a shortcut weight of either 0 (i.e. vanilla networks) or 0.8. In Table 4 we can see that with a strong
optimizer like K-FAC, we can reduce the gap on the 50 layer network to only 1.8% accuracy when
training for 90 epochs, and further down to 0.6% when training for 180 epochs. For 101 layers, the
gaps are 3.6% and 1.7% respectively, which we show can be further reduced with wider networks
(see Table 9). To our knowledge, this is the ﬁrst time that a deep vanilla network has been trained
to such a high validation accuracy on ImageNet. In addition, our networks have fewer parameters
and run faster than standard ResNets, and use less memory at inference time due to the removal of"
TRAINING DEEP NEURAL NETWORKS WITHOUT SHORTCUTS,0.24060150375939848,Published as a conference paper at ICLR 2022
TRAINING DEEP NEURAL NETWORKS WITHOUT SHORTCUTS,0.2424812030075188,"Table 4: ImageNet top-1 validation accuracy. For rescaled ResNets (w = 0.0 or w = 0.8), we do not include
any normalization layer. For standard ResNets, batch normalization is included. By default, ReLU activation is
used for standard ResNet while we use TReLU for rescaled networks."
TRAINING DEEP NEURAL NETWORKS WITHOUT SHORTCUTS,0.24436090225563908,"Depth
Optimizer
90 epochs
180 epochs"
TRAINING DEEP NEURAL NETWORKS WITHOUT SHORTCUTS,0.2462406015037594,"ResNet
w = 0.0
w = 0.8
ResNet
w = 0.0
w = 0.8"
K-FAC,0.24812030075187969,"50
K-FAC
76.4
74.6
76.4
76.6
76.0
77.0
SGD
76.3
71.0
76.0
76.6
72.3
76.8"
K-FAC,0.25,"101
K-FAC
77.8
74.2
77.8
77.6
75.9
78.4
SGD
77.9
70.0
77.3
77.6
73.8
77.4"
K-FAC,0.2518796992481203,"shortcut connections and BN layers. The gaps when using SGD as the optimizer are noticeably larger,
which we further explore in Section 5.5. Lastly, using rescaled ResNets with a shortcut weight of 0.8
and TReLU, we can exactly match or even surpass the performance of standard ResNets."
COMPARISONS WITH EXISTING APPROACHES,0.25375939849624063,"5.4
COMPARISONS WITH EXISTING APPROACHES"
COMPARISONS WITH EXISTING APPROACHES,0.2556390977443609,"Table 5: ImageNet top-1 validation accuracy compari-
son between EOC and TAT on deep vanilla networks."
COMPARISONS WITH EXISTING APPROACHES,0.2575187969924812,"Depth
Optimizer
Method
(L)ReLU
Tanh"
"K-FAC
EOC",0.2593984962406015,"50
K-FAC
EOC
72.6
70.6
TAT
74.6
73.1"
"K-FAC
EOC",0.26127819548872183,"SGD
EOC
63.7
55.7
TAT
71.0
69.5"
"K-FAC
EOC",0.2631578947368421,"101
K-FAC
EOC
71.8
69.2
TAT
74.2
72.8"
"K-FAC
EOC",0.2650375939849624,"SGD
EOC
41.6
54.0
TAT
70.0
69.0"
"K-FAC
EOC",0.2669172932330827,"Comparison with EOC. Our ﬁrst comparison
is between TAT and EOC on vanilla deep net-
works. For EOC with ReLUs we set (σw, σb) =
(
√"
"K-FAC
EOC",0.26879699248120303,"2, 0) to achieve Q(1) = 1 as in He et al.
(2015), since ReLU networks always satisfy
C′(1) = 1 whenever σb = 0. For Tanh acti-
vations, a comprehensive comparison with EOC
is more difﬁcult, as there are inﬁnitely many
choices of (σw, σb) that achieve C′(1) = 1.
Here we use (σw, σb) = (1.302, 0.02)2, as sug-
gested in Hayou et al. (2019). In Table 5, we can
see that in all the settings, networks constructed
with TAT outperform EOC-initialized networks by a signiﬁcant margin, especially when using SGD.
Another observation is that the accuracy of EOC-initialized networks drops as depth increases."
"K-FAC
EOC",0.2706766917293233,"Comparison with DKS. The closest approach to TAT in the existing literature is DKS, whose
similarity and drawbacks are discussed in Section 4. We compare TAT to DKS on both LReLUs3,
and smooth functions like the SoftPlus and Tanh. For smooth activations, we perform a grid search
over {0.2, 0.3, 0.5} for τ in TAT, and {1.5, 10.0, 100.0} for ζ in DKS, and report only the best
performing one. From the results shown in Table 7, we observe that TAT, together with LReLU
(i.e. TReLU), performs the best in nearly all settings we tested, and that its advantage becomes larger
when we remove dropout. One possible reason for the superior performance of TReLU networks is
the stronger Q/C map conditions that they satisfy compared to other activations (i.e. Q(q) = q for all
q vs Q(1) = 1 and Q′(1) = 1, and invariance of C to the input q value), and the extra resilience to
kernel approximation error that these stronger conditions imply. In practice, we found that TReLU
indeed has smaller kernel approximation error (compared to DKS with smooth activation functions,
see Appendix E.1) and works equally well with Gaussian initialization (see Appendix E.7)."
"K-FAC
EOC",0.2725563909774436,Table 6: Comparison with PReLU.
"K-FAC
EOC",0.2744360902255639,"Depth
Optimizer
TReLU
PReLU0.0
PReLU0.25"
K-FAC,0.27631578947368424,"50
K-FAC
74.6
72.5
73.6"
K-FAC,0.2781954887218045,"SGD
71.0
66.7
67.9"
K-FAC,0.2800751879699248,"101
K-FAC
74.2
71.9
72.8"
K-FAC,0.2819548872180451,"SGD
70.0
54.3
66.3"
K-FAC,0.28383458646616544,"Comparison with PReLU. The Parametric
ReLU (PReLU) introduced in He et al. (2015)
differs from LReLU by making the negative
slope a trainable parameter. Note that this is
distinct from what we are doing with TReLU,
since there we compute the negative slope pa-
rameter ahead of time and ﬁx it during training.
In our comparisons with PReLU we consider
two different initializations: 0 (which recovers the standard ReLU), and 0.25, which was used in
He et al. (2015). We report the results on deep vanilla networks in Table 6 (see Appendix E.6 for
results on rescaled ResNets). For all settings, our method outperforms PReLU by a large margin,
emphasizing the importance of the initial negative slope value. In principle, these two methods can
be combined together (i.e. we could ﬁrst initialize the negative slope parameter with TAT, and then
optimize it during training), however we did not see any beneﬁt from doing this in our experiments."
K-FAC,0.2857142857142857,"2We also ran experiments with (σw, σb) = (1.0, 0.0), and the scheme described in Pennington et al. (2017)
and Xiao et al. (2018) for dynamical isometry. The results were worse than those reported in the table.
3For DKS, we set the negative slope as a parameter and adopt the transformation ˆφ(x) = γ(φα(x + β) + δ)."
K-FAC,0.287593984962406,Published as a conference paper at ICLR 2022
K-FAC,0.2894736842105263,"Table 7: Comparisons between TAT and DKS. The numbers on the right hand of / are results without dropout.
The methods with ∗are introduced in this paper."
K-FAC,0.29135338345864664,"Depth
Optimizer
Shortcut
Weight
TAT
DKS"
K-FAC,0.2932330827067669,"LReLU∗
SoftPlus∗
Tanh∗
LReLU∗
SoftPlus
Tanh 50"
K-FAC,0.2951127819548872,"K-FAC
w = 0.0
74.6/74.2
74.4/74.2
73.1/72.9
74.3/74.3
74.3/73.7
72.9/72.9
w = 0.8
76.4/75.9
76.4/75.0
74.8/74.4
76.2/76.2
76.3/75.1
74.7/74.5"
K-FAC,0.29699248120300753,"SGD
w = 0.0
71.1/71.1
70.2/70.0
69.5/69.5
70.4/70.4
71.8/71.4
69.2/69.2
w = 0.8
76.0/75.8
74.3/73.8
72.4/72.2
73.4/73.0
75.2/74.1
72.8/72.8 101"
K-FAC,0.29887218045112784,"K-FAC
w = 0.0
74.2/74.2
74.1/73.4
72.8/72.5
73.5/73.5
73.9/73.1
72.5/72.4
w = 0.8
77.8/77.0
76.6/75.7
75.8/75.1
76.8/76.7
76.8/75.6
75.9/75.7"
K-FAC,0.3007518796992481,"SGD
w = 0.0
70.0/70.0
70.3/68.8
69.0/67.8
68.3/68.3
68.3/68.3
69.8/69.8
w = 0.8
77.3/76.0
75.3/75.3
73.8/73.5
74.9/74.6
76.3/75.1
74.6/74.6"
THE ROLE OF THE OPTIMIZER,0.3026315789473684,"5.5
THE ROLE OF THE OPTIMIZER
Table 8: Batch size scaling."
THE ROLE OF THE OPTIMIZER,0.30451127819548873,"Optimizer
Batch size"
THE ROLE OF THE OPTIMIZER,0.30639097744360905,"128
256
512
1024
2048
4096"
THE ROLE OF THE OPTIMIZER,0.3082706766917293,"K-FAC
74.5
74.4
74.5
74.6
74.2
72.0
SGD
72.7
72.6
72.7
71.0
69.3
62.0
LARS
72.4
72.3
72.6
71.8
71.3
70.2"
THE ROLE OF THE OPTIMIZER,0.3101503759398496,"One interesting phenomenon we observed
in our experiments, which echoes the ﬁnd-
ings of Martens et al. (2021), is that a strong
optimizer such as K-FAC signiﬁcantly out-
performs SGD on vanilla deep networks in
terms of training speed. One plausible expla-
nation is that K-FAC works better than SGD in the large-batch setting, and our default batch size of
1024 is already beyond SGD’s “critical batch size”, at which scaling efﬁciency begins to drop. Indeed,
it was shown by Zhang et al. (2019) that optimization algorithms that employ preconditioning, such
as Adam and K-FAC, result in much larger critical batch sizes."
THE ROLE OF THE OPTIMIZER,0.31203007518796994,"0
1
2
3
4
Cases processed
1e8 0.55 0.60 0.65 0.70 0.75 0.80"
THE ROLE OF THE OPTIMIZER,0.31390977443609025,Validation acc
THE ROLE OF THE OPTIMIZER,0.3157894736842105,"KFAC with 90 epochs
SGD with 90 epochs
KFAC with 180 epochs
SGD with 180 epochs
KFAC with 360 epochs
SGD with 360 epochs"
THE ROLE OF THE OPTIMIZER,0.3176691729323308,"Figure 3: Training speed comparison be-
tween K-FAC and SGD on 50 layer vanilla
TReLU network."
THE ROLE OF THE OPTIMIZER,0.31954887218045114,"To investigate this further, we tried batch sizes between
128 and 4096 for training 50-layer vanilla TReLU net-
works. As shown in Table 8, K-FAC performs equally
well for all different batch sizes except 4096 (where we
see increased overﬁtting), while the performance of SGD
starts to drop when we increase the batch size past 512.
Surprisingly, we observe a similar trend for the LARS
optimizer (You et al., 2019), which was designed for large-
batch training. Even at the smallest batch size we tested
(128), K-FAC still outperforms SGD by a gap of 1.8%
within our standard epoch budget. We conjecture the rea-
son behind this to be that vanilla networks without normalization and shortcuts give rise to loss
landscapes with worse curvature properties compared to ResNets, and that this slows down simpler
optimizers like SGD. To investigate further, we also ran SGD (with a batch size of 512) and K-FAC
for up to 360 epochs with a “one-cycle” cosine learning rate schedule (Loshchilov & Hutter, 2016)
that decreases the learning rate to to 0 by the ﬁnal epoch. As shown in Figure 3, SGD does indeed
eventually catch up with K-FAC (using cosine scheme), requiring just over double the number
of epochs to achieve the same validation accuracy. While one may argue that K-FAC introduces
additional computational overhead at each step, thus making a head-to-head comparison versus SGD
unfair, we note that this overhead can amortized by not updating K-FAC’s preconditioner matrix at
every step. In our experiments we found that this strategy allowed K-FAC to achieve a similar per-step
runtime to SGD, while retaining its optimization advantage on vanilla networks. (See Appendix E.3.)"
CONCLUSIONS,0.32142857142857145,"6
CONCLUSIONS"
CONCLUSIONS,0.3233082706766917,"In this work we considered the problem of training and generalization in vanilla deep neural networks
(i.e. those without shortcut connections and normalization layers). To address this we developed a
novel method that modiﬁes the activation functions in a way tailored to the speciﬁc architecture, and
which enables us to achieve generalization performance on par with standard ResNets of the same
width/depth. Unlike the most closely related approach (DKS), our method is fully compatible with
ReLU-family activation functions, and in fact achieves its best performance with them. By obviating
the need for shortcut connections, we believe our method could enable further research into deep
models and their representations. In addition, our method may enable new architectures to be trained
for which existing techniques, such as shortcuts and normalization layers, are insufﬁcient."
CONCLUSIONS,0.325187969924812,Published as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.32706766917293234,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.32894736842105265,"Here we discuss our efforts to facilitate the reproducibility of this paper. Firstly, we have made an
open Python implementation of DKS and TAT, supporting multiple tensor programming frameworks,
available at https://github.com/deepmind/dks. Secondly, we have given all important
details of our experiments in Appendix D."
REFERENCES,0.3308270676691729,REFERENCES
REFERENCES,0.33270676691729323,"Rohan Anil, Vineet Gupta, Tomer Koren, Kevin Regan, and Yoram Singer. Scalable second order
optimization for deep learning. arXiv preprint arXiv:2002.09018, 2020."
REFERENCES,0.33458646616541354,"Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.
Layer normalization.
arXiv preprint
arXiv:1607.06450, 2016."
REFERENCES,0.33646616541353386,"Jimmy Ba, Roger Grosse, and James Martens. Distributed second-order optimization using kronecker-
factored approximations. In International Conference on Learning Representations, 2017."
REFERENCES,0.3383458646616541,"Thomas Bachlechner, Bodhisattwa Prasad Majumder, Huanru Henry Mao, Garrison W Cottrell,
and Julian McAuley. Rezero is all you need: Fast convergence at large depth. arXiv preprint
arXiv:2003.04887, 2020."
REFERENCES,0.34022556390977443,"David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, and Brian McWilliams.
The shattered gradients problem: If resnets are the answer, then what is the question?
In
International Conference on Machine Learning, pp. 342–350. PMLR, 2017."
REFERENCES,0.34210526315789475,"James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and
Qiao Zhang.
JAX: composable transformations of Python+NumPy programs, 2018.
URL
http://github.com/google/jax."
REFERENCES,0.34398496240601506,"Andrew Brock, Soham De, and Samuel L Smith. Characterizing signal propagation to close the per-
formance gap in unnormalized resnets. In International Conference on Learning Representations,
2021a."
REFERENCES,0.3458646616541353,"Andrew Brock, Soham De, Samuel L Smith, and Karen Simonyan. High-performance large-scale
image recognition without normalization. arXiv preprint arXiv:2102.06171, 2021b."
REFERENCES,0.34774436090225563,"Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv preprint arXiv:2005.14165, 2020."
REFERENCES,0.34962406015037595,"Sam Buchanan, Dar Gilboa, and John Wright. Deep networks and the multiple manifold problem. In
International Conference on Learning Representations, 2020."
REFERENCES,0.35150375939849626,"Youngmin Cho and Lawrence Saul. Kernel methods for deep learning. Advances in Neural Informa-
tion Processing Systems, 22:342–350, 2009."
REFERENCES,0.3533834586466165,"Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks: The
power of initialization and a dual view on expressivity. Advances In Neural Information Processing
Systems, 29:2253–2261, 2016."
REFERENCES,0.35526315789473684,"Soham De and Sam Smith. Batch normalization biases residual blocks towards the identity function
in deep networks. Advances in Neural Information Processing Systems, 33, 2020."
REFERENCES,0.35714285714285715,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248–255. Ieee, 2009."
REFERENCES,0.35902255639097747,"Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han, Guiguang Ding, and Jian Sun. Repvgg:
Making vgg-style convnets great again. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 13733–13742, 2021."
REFERENCES,0.3609022556390977,Published as a conference paper at ICLR 2022
REFERENCES,0.36278195488721804,"David Duvenaud, Oren Rippel, Ryan Adams, and Zoubin Ghahramani. Avoiding pathologies in very
deep networks. In Artiﬁcial Intelligence and Statistics, pp. 202–210. PMLR, 2014."
REFERENCES,0.36466165413533835,"Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artiﬁcial intelligence and
statistics, pp. 249–256. JMLR Workshop and Conference Proceedings, 2010."
REFERENCES,0.36654135338345867,"Souﬁane Hayou, Arnaud Doucet, and Judith Rousseau. On the impact of the activation function on
deep neural networks training. In International conference on machine learning, pp. 2672–2680.
PMLR, 2019."
REFERENCES,0.3684210526315789,"Souﬁane Hayou, Eugenio Clerico, Bobby He, George Deligiannidis, Arnaud Doucet, and Judith
Rousseau. Stable resnet. In International Conference on Artiﬁcial Intelligence and Statistics, pp.
1324–1332. PMLR, 2021."
REFERENCES,0.37030075187969924,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing
human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE international
conference on computer vision, pp. 1026–1034, 2015."
REFERENCES,0.37218045112781956,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770–778, 2016a."
REFERENCES,0.37406015037593987,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European conference on computer vision, pp. 630–645. Springer, 2016b."
REFERENCES,0.37593984962406013,"Tom Hennigan, Trevor Cai, Tamara Norman, and Igor Babuschkin. Haiku: Sonnet for JAX, 2020.
URL http://github.com/deepmind/dm-haiku."
REFERENCES,0.37781954887218044,"Matteo Hessel, David Budden, Fabio Viola, Mihaela Rosca, Eren Sezener, and Tom Hennigan. Optax:
composable gradient transformation and optimisation, in jax!, 2020. URL http://github.
com/deepmind/optax."
REFERENCES,0.37969924812030076,"Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, J¨urgen Schmidhuber, et al. Gradient ﬂow in
recurrent nets: the difﬁculty of learning long-term dependencies, 2001."
REFERENCES,0.3815789473684211,"Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International conference on machine learning, pp. 448–456.
PMLR, 2015."
REFERENCES,0.38345864661654133,Hassan K. Khalil. Nonlinear systems third edition. 2008.
REFERENCES,0.38533834586466165,"G¨unter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing
neural networks. Advances in neural information processing systems, 30, 2017."
REFERENCES,0.38721804511278196,"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolu-
tional neural networks. Advances in neural information processing systems, 2012."
REFERENCES,0.3890977443609023,Alex Krizhevsky et al. Learning multiple layers of features from tiny images. 2009.
REFERENCES,0.39097744360902253,"Yann A LeCun, L´eon Bottou, Genevieve B Orr, and Klaus-Robert M¨uller. Efﬁcient backprop. In
Neural networks: Tricks of the trade. Springer, 1998."
REFERENCES,0.39285714285714285,"Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. arXiv
preprint arXiv:1608.03983, 2016."
REFERENCES,0.39473684210526316,"Yao Lu, Stephen Gould, and Thalaiyasingam Ajanthan.
Bidirectional self-normalizing neural
networks. arXiv preprint arXiv:2006.12169, 2020."
REFERENCES,0.3966165413533835,"Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectiﬁer nonlinearities improve neural network
acoustic models. In International Conference on Machine Learning, 2013."
REFERENCES,0.39849624060150374,"James Martens. On the validity of kernel approximations for orthogonally-initialized neural networks.
arXiv preprint arXiv:2104.05878, 2021."
REFERENCES,0.40037593984962405,Published as a conference paper at ICLR 2022
REFERENCES,0.40225563909774437,"James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate
curvature. In International conference on machine learning, pp. 2408–2417. PMLR, 2015."
REFERENCES,0.4041353383458647,"James Martens, Andy Ballard, Guillaume Desjardins, Grzegorz Swirszcz, Valentin Dalibard, Jascha
Sohl-Dickstein, and Samuel S Schoenholz. Rapid training of deep neural networks without skip
connections or normalization layers using deep kernel shaping. arXiv preprint arXiv:2110.01765,
2021."
REFERENCES,0.40601503759398494,"Radford M Neal. Bayesian learning for neural networks. Lecture notes in statistics, 118, 1996."
REFERENCES,0.40789473684210525,"Oyebade K Oyedotun, Djamila Aouada, Bj¨orn Ottersten, et al. Going deeper with neural networks
without skip connections. In 2020 IEEE International Conference on Image Processing (ICIP), pp.
1756–1760. IEEE, 2020."
REFERENCES,0.40977443609022557,"Jeffrey Pennington, Samuel S Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep
learning through dynamical isometry: theory and practice. In Proceedings of the 31st International
Conference on Neural Information Processing Systems, pp. 4788–4798, 2017."
REFERENCES,0.4116541353383459,"Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential
expressivity in deep neural networks through transient chaos. Advances in neural information
processing systems, 29:3360–3368, 2016."
REFERENCES,0.41353383458646614,"Michael JD Powell. An efﬁcient method for ﬁnding the minimum of a function of several variables
without calculating derivatives. The Computer Journal, 7(2):155–162, 1964."
REFERENCES,0.41541353383458646,"Haozhi Qi, Chong You, Xiaolong Wang, Yi Ma, and Jitendra Malik. Deep isometric learning for
visual recognition. In International Conference on Machine Learning, pp. 7824–7835. PMLR,
2020."
REFERENCES,0.41729323308270677,"Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics
of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013."
REFERENCES,0.4191729323308271,"Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information
propagation. In International Conference on Learning Representations, 2017."
REFERENCES,0.42105263157894735,"Jie Shao, Kai Hu, Changhu Wang, Xiangyang Xue, and Bhiksha Raj. Is normalization indispensable
for training deep neural network? Advances in Neural Information Processing Systems, 33, 2020."
REFERENCES,0.42293233082706766,"David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without
human knowledge. nature, 550(7676):354–359, 2017."
REFERENCES,0.424812030075188,"Rupesh Kumar Srivastava, Klaus Greff, and J¨urgen Schmidhuber. Highway networks. arXiv preprint
arXiv:1505.00387, 2015."
REFERENCES,0.4266917293233083,"Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking
the inception architecture for computer vision. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 2818–2826, 2016."
REFERENCES,0.42857142857142855,"Andreas Veit, Michael J Wilber, and Serge Belongie. Residual networks behave like ensembles of
relatively shallow networks. Advances in neural information processing systems, 29:550–558,
2016."
REFERENCES,0.43045112781954886,"Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel Schoenholz, and Jeffrey Penning-
ton. Dynamical isometry and a mean ﬁeld theory of cnns: How to train 10,000-layer vanilla
convolutional neural networks. In International Conference on Machine Learning, 2018."
REFERENCES,0.4323308270676692,"Lechao Xiao, Jeffrey Pennington, and Samuel Schoenholz. Disentangling trainability and generaliza-
tion in deep neural networks. In International Conference on Machine Learning, pp. 10462–10472.
PMLR, 2020."
REFERENCES,0.4342105263157895,"Greg Yang and Samuel Schoenholz. Mean ﬁeld residual networks: On the edge of chaos. In Advances
in Neural Information Processing Systems, volume 30, 2017."
REFERENCES,0.43609022556390975,Published as a conference paper at ICLR 2022
REFERENCES,0.43796992481203006,"Greg Yang, Jeffrey Pennington, Vinay Rao, Jascha Sohl-Dickstein, and Samuel S. Schoenholz. A
mean ﬁeld theory of batch normalization. ArXiv, abs/1902.08129, 2019."
REFERENCES,0.4398496240601504,"Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan
Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep
learning: Training bert in 76 minutes. arXiv preprint arXiv:1904.00962, 2019."
REFERENCES,0.4417293233082707,"Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In British Machine Vision
Conference 2016. British Machine Vision Association, 2016."
REFERENCES,0.44360902255639095,"Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George Dahl, Chris
Shallue, and Roger B Grosse. Which algorithmic choices matter at which batch sizes? insights
from a noisy quadratic model. Advances in neural information processing systems, 2019."
REFERENCES,0.44548872180451127,"Hongyi Zhang, Yann N Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without
normalization. In International Conference on Learning Representations, 2018."
REFERENCES,0.4473684210526316,Published as a conference paper at ICLR 2022
REFERENCES,0.4492481203007519,"A
BACKGROUND"
REFERENCES,0.45112781954887216,"A.1
KERNEL FUNCTION APPROXIMATION ERROR BOUNDS"
REFERENCES,0.45300751879699247,"In Section 2.1, we claimed that the kernel deﬁned in equation 2 would converge to a deterministic
kernel, as the width of each layer goes to inﬁnity. To be speciﬁc, one has the following result bounding
the kernel approximation error.
Theorem 3 (Adapted from Theorem 2 of Daniely et al. (2016)). Consider a fully-connected network
of depth L with weights initialized independently using a standard Gaussian fan-in initialization.
Further suppose that the activation function φ is C-bounded (i.e. ∥φ∥∞≤C, ∥φ′∥∞≤C and
∥φ′′∥∞≤C for some constant C) and satisﬁes Ez∼N(0,1)[φ(z)2] = 1, and that the width of each
layer is greater than or equal to (4C4)L log(8L/δ)/ϵ2. Then at initialization time, for inputs x1 and
x2 satisfying ∥x1∥2 = ∥x2∥2 = dim(x1), we have that
κL
f (x1, x2) −˜κL
f (x1, x2)
 ≤ϵ"
REFERENCES,0.4548872180451128,with probability at least 1 −δ.
REFERENCES,0.4567669172932331,"The bound in Theorem 3 predicts an exponential dependence on the depth L of the minimum required
width of each layer. However, for a network with ReLU activations, this dependence is only quadratic
in L, as is established in the following theorem:
Theorem 4 (Adapted from Theorem 3 of Daniely et al. (2016)). Consider a fully-connected network
of depth L with ReLU activations and weights initialized independently using a He initialization (He
et al., 2015), and suppose that the width of each layer is greater than or equal to L2 log(L/δ)/ϵ2.
Then at initialization time, for inputs x1 and x2 satisfying ∥x1∥2 = ∥x2∥2 = dim(x1), and ϵ ≲1"
REFERENCES,0.45864661654135336,"L,
we have that
κL
f (x1, x2) −˜κL
f (x1, x2)
 ≤ϵ
with probability at least 1 −δ."
REFERENCES,0.4605263157894737,"According to Lemma D.1 of Buchanan et al. (2020), the requirement of the width for ReLU networks
could further be reduced to linear in the depth L, but with a worse dependency on δ."
REFERENCES,0.462406015037594,"Although Theorems 3 and 4 are only applicable to Gaussian initializations, a similar bound has
been given by Martens (2021) for scaled uniform orthogonal initializations in the case that L = 1.
Moreover, Martens (2021) conjectures that their result could be extended to general values of L."
REFERENCES,0.4642857142857143,"A.2
DEGENERATE C MAPS FOR VERY DEEP NETWORKS"
REFERENCES,0.46616541353383456,"Daniely et al. (2016), Poole et al. (2016), and Martens et al. (2021) have shown that without very
careful interventions, C maps inevitably become “degenerate” in deep networks, tending rapidly
towards constant functions on (−1, 1) as depth increases. The following proposition is a restatement
of Claim 1 from Daniely et al. (2016):
Proposition 2. Suppose f is a deep network consisting of a composition of L combined layers. Then
for all c ∈(−1, 1) we have
lim
L→∞Cf(c) = c∗,"
REFERENCES,0.4680451127819549,"for some c∗∈[0, 1]."
REFERENCES,0.4699248120300752,"While the above result doesn’t characterize the rate of convergence Cf(c) to a constant function,
Poole et al. (2016) show that if C′(1) ̸= 1, it happens exponentially fast as a function of L in the
asymptotic limit of large L. Martens et al. (2021) gives a similar result which holds uniformly for all
L, and for networks with more general repeated structures."
REFERENCES,0.4718045112781955,"A.3
C MAP DERIVATIVE"
REFERENCES,0.47368421052631576,"Poole et al. (2016) gave the following nice formula for the derivative of C map of a combined layer
with activation function φ:"
REFERENCES,0.4755639097744361,"C′(c, q1, q2) =
√q1q2
p"
REFERENCES,0.4774436090225564,"Q(q1)Q(q2)
Ez1,z2∼N(0,1)
h
φ′ (√q1z1) φ′ √q2

cz1 +
p"
REFERENCES,0.4793233082706767,"1 −c2z2
i
.
(17)"
REFERENCES,0.48120300751879697,Published as a conference paper at ICLR 2022
REFERENCES,0.4830827067669173,For a rigorous proof of this result we refer the reader to Martens et al. (2021).
REFERENCES,0.4849624060150376,One can iterate this formula to obtain a similar equation for higher-order derivatives:
REFERENCES,0.4868421052631579,"C(i)(c, q1, q2) =
(q1q2)(i/2)
p"
REFERENCES,0.48872180451127817,"Q(q1)Q(q2)
Ez1,z2∼N(0,1)
h
φ(i) (√q1z1) φ(i) √q2

cz1 +
p"
REFERENCES,0.4906015037593985,"1 −c2z2
i
. (18)"
REFERENCES,0.4924812030075188,"A.4
SOME USEFUL PROPERTIES OF C MAPS"
REFERENCES,0.4943609022556391,In this section we will assume that q1 = q2 = 1.
REFERENCES,0.49624060150375937,"Observe that C(1) = Ez∼N(0,1)
h
φ (z)2i
= 1 and that C maps [−1, 1] to [−1, 1] (which follows from
its interpretation as computing cosine similarities for inﬁnitely wide networks). Moreover, C is a
positive deﬁnite function, which means that it can be written as P∞
n=0 bncn for bn ≥0 (Daniely
et al., 2016; Martens et al., 2021). Note that for smooth activation functions, positive deﬁniteness can
be easily veriﬁed by Taylor-expanding C(c) about c = 0 and using"
REFERENCES,0.4981203007518797,"C(i)(0) = Ez∼N(0,1)
h
φ(i) (z)
i2
≥0.
(19)"
REFERENCES,0.5,"As discussed in Section 2.3, global C maps are computed by recursively taking compositions and
weighted averages (with non-negative weights), starting from C. Because all of the above properties
are preserved under these operations, it follows that global C maps inherit them from C."
REFERENCES,0.5018796992481203,"B
ADDITIONAL DETAILS AND PSEUDOCODE FOR ACTIVATION FUNCTION
TRANSFORMATIONS"
REFERENCES,0.5037593984962406,"B.1
TAKING ALL SUBNETWORKS INTO ACCOUNT"
REFERENCES,0.5056390977443609,"In the main text of this paper we have used the condition C′
f(1) = ζ in DKS, Cf(0) = η in TAT for
Leaky ReLUs, and C′′
f (1) = τ in TAT for smooth activation functions. However, the condition used
by Martens et al. (2021) in DKS was actually µ1
f(C′(1)) = ζ, where µ1
f is the so-called “maximal
slope function”:"
REFERENCES,0.5075187969924813,"µ1
f(C′(1)) = max
g:g⊆f C′
g(1),"
REFERENCES,0.5093984962406015,"where “g ⊆f” denotes that g is a subnetwork4 of f. (That C′
g(1) is fully determined by C′(1) follows
from the fact that Cg can be written in terms of compositions, weighted average operations, and
applications of C, and that C maps always preserve the value 1. Using the chain rule, and the linearity
of derivatives, these facts allow one to write C′
g(1) as a polynomial function of C′(1).)"
REFERENCES,0.5112781954887218,"The motivation given by Martens et al. (2021) for looking at C′
g(1) over all subnetworks g ⊆f
(instead of just C′
f(1)) is that we want all layers of f, in all of its subnetworks, to be readily trainable.
For example, a very deep and untrainable MLP could be made to have a reasonable global C map
simply by adding a skip connection from its input to its output, but this won’t do anything to address
the untrainability of the layers being “skipped around” (which form a subnetwork)."
REFERENCES,0.5131578947368421,"In the main text we ignored this complication in the interest of a shorter presentation, and because
we happened to have µ1
f(C′(1)) = C′
f(1) for the simple network architectures focused on in this
work. To remedy this, in the current section we will discuss how to modify the conditions Cf(0) = η
and C′′
f (1) = τ used in TAT so that they take into account all subnetworks. This will be done
using a natural generalization of the maximal slope function from DKS. We will then address the
computational challenges that result from doing this."
REFERENCES,0.5150375939849624,"4A subnetwork of f is deﬁned as a (non-strict) connected subset of the layers in f that constitute a neural
network with a singular input and output layer. So for example, layers 3, 4 and 5 of a 10 layer MLP form a
subnetwork, while layers 3, 4, and 6 do not."
REFERENCES,0.5169172932330827,Published as a conference paper at ICLR 2022
REFERENCES,0.518796992481203,"To begin, we will replace the condition Cf(0) = η (used in TAT for Leaky ReLUs) by the condition
µ0
f(0) = η, where we deﬁne the maximal c value function µ0
f of f by"
REFERENCES,0.5206766917293233,"µ0
f(α) = max
g:g⊆f Cg(0),"
REFERENCES,0.5225563909774437,"where α is the negative slope parameter (which determines C in LReLU networks [via φα] and thus
each Cg)."
REFERENCES,0.5244360902255639,"We will similarly replace the condition C′′
f (1) = τ (used in TAT for smooth activations) by the
condition µ2
f(C′′(1)) = τ, where we deﬁne the maximal curvature function µ2
f of f by"
REFERENCES,0.5263157894736842,"µ2
f(C′′(1)) = max
g:g⊆f C′′
g (1),"
REFERENCES,0.5281954887218046,"where each C′′
g (1) is determined by C′′(1). That each C′′
g (1) is a well-deﬁned function of C′′(1) follows
from the fact that C maps always map the value 1 to 1, the aforementioned relationship between Cg and
C, and the fact that we have C′(1) = 1 under TAT (so that C′
h(1) = 1 for all subnetworks h). These
facts allow us to write C′′
g (1) as a constant multiple of C′′(1) using the linearity of 2nd derivatives
and the 2nd-order chain rule (which is given by (a ◦b)′′(x) = a′′(b(x))b′(x)2 + a′(b(x))b′′(x))."
REFERENCES,0.5300751879699248,"B.2
COMPUTING µ0
f AND µ2
f IN GENERAL"
REFERENCES,0.5319548872180451,"Given these new conditions for TAT, it remains to compute their left hand sides so that we may ulti-
mately solve for the required quantities (α or C′′(1)). In Section 2.3 we discussed how a (sub)network
f’s C map Cf can be computed in terms of the local C map C by a series of composition and non-
negative weighted sum operations. We can deﬁne a generalized version of this construction Uf,r
which replaces C with an arbitrary non-decreasing function r, so that Uf,C(c) = Cf(c). A recipe for
computing Uf,r is given in Appendix B.4."
REFERENCES,0.5338345864661654,"Given Uf,r, we deﬁne the subnetwork maximizing function M by"
REFERENCES,0.5357142857142857,"Mf,r(x) = max
g:g⊆f Ug,r(x)."
REFERENCES,0.5375939849624061,"With this deﬁnition, it is not hard to see that if r0(x) = C(x), r1(x) = C′(1)x, and r2(x) = C′′(1)+x,
then µ0
f(α) = Mf,r0(0) (where the dependence on α is implicit through the dependence of C on
φα), µ1
f(C′(1)) = Mf,r1(1), and µ2
f(C′′(1)) = Mf,r2(0). Thus, it sufﬁces to derive a scheme for
computing (and inverting) Mf,r for general networks f and non-decreasing functions r."
REFERENCES,0.5394736842105263,"Naively, computing Mf,r could involve a very large maximization and be quite computationally
expensive. But analogously to the maximal slope function computation described in Martens et al.
(2021), the computation of Mf,r can simpliﬁed substantially, so that we rarely have to maximize
over more than a few possible subnetworks. In particular, since Ug,r(x) is a non-decreasing function
of x for all g (which follows from the fact that r is non-decreasing), and Ug◦h,r = Ug,r ◦Uh,r, it
thus follows that Ug◦h,r(x) ≥Ug,r(x), Uh,r(x) for all x. This means that for the purposes of the
maximization, we can ignore any subnetwork in f which composes with another subnetwork (not
necessarily in f) to form a strictly larger subnetwork isomorphic to one in f. This will typically be
the vast majority of them. Note that this does not therefore imply that Mf,r = Uf,r, since not all
subnetworks compose in this way. For example, a sufﬁciently deep residual branch of a residual
block in a rescaled ResNet won’t compose with any subnetwork to form a larger one."
REFERENCES,0.5413533834586466,"B.3
SOLVING FOR α AND C′′(1)"
REFERENCES,0.543233082706767,"Having shown how to efﬁciently compute Mf,r, and thus both of µ0
f and µ2
f, it remains to show how
we can invert them to ﬁnd solutions for α and C′′(1) (respectively). Fortunately, this turns out to
be easy, as both functions are strictly monotonic in their arguments (α and C′′(1)), provided that
f contains at least one nonlinear layer. Thus, we may apply a simple 1-dimensional root-ﬁnding
approach, such as binary search."
REFERENCES,0.5451127819548872,"To see that µ0
f(α) is a strictly decreasing function of α (or in other words, a strictly increasing
function of −α), we observe that it is a maximum over terms of the form Ug,C(0), which are all either
strictly decreasing non-negative functions of α, or are identically zero. These properties of Ug,C(0)"
REFERENCES,0.5469924812030075,Published as a conference paper at ICLR 2022
REFERENCES,0.5488721804511278,"follow from the fact that it involves only applications of C, along with compositions and non-negative
weighted averages, and that C(c) is a strictly decreasing function of α for all c ∈[−1, 1] (in Leaky
ReLU networks). A similar argument can be used to show that µ2
f(C′′(1)) is a strictly increasing
function of C′′(1) (and is in fact equal to a non-negative multiple of C′′(1))."
REFERENCES,0.5507518796992481,"B.4
RECIPE FOR COMPUTING Uf,r"
REFERENCES,0.5526315789473685,"As deﬁned, Uf,r is computed from f by taking the computational graph for Cf and replacing the
local C map C with r wherever the former appears. So in particular, one can obtain a computational
graph for Uf,r(x) from f’s computational graph by recursively applying the following rules:"
REFERENCES,0.5545112781954887,"1. Composition g ◦h of two subnetworks g and h maps to Ug,r ◦Uh,r.
2. Afﬁne layers map to the identity function.
3. Nonlinear layers map to r.
4. Normalized sums with weights w1, w2, ..., wn over the outputs of subnetworks g1, g2, ..., gn,
map to the function"
REFERENCES,0.556390977443609,"w2
1Ug1,r(x1) + w2
2Ug2,r(x2) + · · · + w2
nUgn,r(xn),"
REFERENCES,0.5582706766917294,"where x1, x2, ..., xn are the respective inputs to the Ugi,r’s.
5. f’s input layer maps to x."
REFERENCES,0.5601503759398496,"In the special case of computing Uf,r2(0), one gets the following simpliﬁed list of rules:"
REFERENCES,0.5620300751879699,"1. Composition g ◦h of two subnetworks g and h maps to Ug,r2(0) + Uh,r2(0)
2. Afﬁne layers map to 0.
3. Nonlinear layers map to C′′(1).
4. Normalized sums with weights w1, w2, ..., wn over the outputs of subnetworks g1, g2, ..., gn,
map to the function"
REFERENCES,0.5639097744360902,"w2
1Ug1,r2(0) + w2
2Ug2,r2(0) + · · · + w2
nUgn,r2(0)."
REFERENCES,0.5657894736842105,5. f’s input layer maps to x.
REFERENCES,0.5676691729323309,"Note that this second procedure will always produce a non-negative multiple of C′′(1), provided that
f contains at least one nonlinear layer."
REFERENCES,0.5695488721804511,"B.5
RESCALED RESNET EXAMPLE"
REFERENCES,0.5714285714285714,"In this subsection we will demonstrate how to apply the above rules to compute the maximal curvature
function µ2
f for a rescaled ResNet f with shortcut weight w and residual branch R (as deﬁned in
equation 6). We note that this computation also handles the case of a vanilla network by simply
taking w = 0."
REFERENCES,0.5733082706766918,"First, we observe that all subnetworks in f compose to form larger ones in f, except for f
itself, and for the residual branches of its residual blocks.
We thus have that µ2
f(C′′(1)) =
max{Uf,r2(0), UR,r2(0)}."
REFERENCES,0.575187969924812,"Because each residual branch has a simple feedforward structure with three nonlinear layers, it
follows that UR,r2(0) = 3C′′(1). And because each shortcut branch S has no nonlinear layers, it
follows that US,r2(0) = 0. Applying the rule for weighted averages to the output of each block B we
thus have that UB,r2(0) = w2US,r2(0) + (1 −w2)UR,r2(0) = 3(1 −w2)C′′(1). Given a network
with L nonlinear layers, we have L/3 blocks, and since the blocks compose in a feedforward manner
it thus follows that Uf,r2(0) = (L/3) · 3(1 −w2)C′′(1) = L(1 −w2)C′′(1). We therefore conclude
that µ2
f(C′′(1)) = max{3, L(1 −w2)}C′′(1)."
REFERENCES,0.5770676691729323,"The rescaled ResNets used in our experiments have a slightly more complex structure (based on the
ResNet-50 and ResNet-101 architectures), with a nonlinear layer appearing after the sequence of
residual blocks, and with a four of their blocks being “transition blocks”, whose shortcut branches"
REFERENCES,0.5789473684210527,Published as a conference paper at ICLR 2022
REFERENCES,0.5808270676691729,"contain a nonlinear layer. In these networks, the total number of residual blocks is given by (L−2)/3.
Following a similar argument to the one above we have that"
REFERENCES,0.5827067669172933,"Uf,r2(0) =
L −2"
REFERENCES,0.5845864661654135,"3
−4

· 3(1 −w2)C′′(1) + 4 · (w2 + 3(1 −w2))C′′(1) + C′′(1)"
REFERENCES,0.5864661654135338,"= [(L −2)(1 −w2) + 4w2 + 1]C′′(1) = [(L −6)(1 −w2) + 5]C′′(1),"
REFERENCES,0.5883458646616542,and thus
REFERENCES,0.5902255639097744,"µ2
f(C′′(1)) = max{[(L −6)(1 −w2) + 5]C′′(1), 3(1 −w2)C′′(1)}"
REFERENCES,0.5921052631578947,= [(L −6)(1 −w2) + 5]C′′(1).
REFERENCES,0.5939849624060151,"B.6
PSEUDOCODE"
REFERENCES,0.5958646616541353,Algorithm 1 TAT for LReLU.
REFERENCES,0.5977443609022557,"Require: The target value η for µ0
f(α)
1: Use the steps from Subsection B.2 to construct a procedure for computing the maximal c value
function µ0
f(α) for general α ≥0. Note that the local C map C, on which µ0
f(α) depends, can be
computed efﬁciently for (transformed) LReLUs using equation 9.
2: Perform a binary search to ﬁnd the negative slope α such that µ0
f(α) = η."
REFERENCES,0.599624060150376,"3: Using the found α, output the transformed activation function given by ˜φα(x) =
q"
REFERENCES,0.6015037593984962,"2
1+α2 φα(x)."
REFERENCES,0.6033834586466166,Algorithm 2 TAT for smooth activations.
REFERENCES,0.6052631578947368,"Require: The target value τ of C′′
f (1)
Require: The original activation function φ(x)"
REFERENCES,0.6071428571428571,"1: Use the steps from Subsection B.2 to construct a procedure for computing the maximal curvature
function µ2
f(C′′(1)) for general C′′(1) ≥0.
2: Perform a binary search to ﬁnd C′′(1) such that µ2
f(C′′(1)) = τ.
3: Using a numerical solver, solve the three-dimensional nonlinear system in equation 16 (but with
the value of C′′(1) found above instead of τ/L) to obtain values for α, β, γ, and δ.
4: Using the solution from the last step, output the transformed activation function given by
ˆφ(x) = γ(φ(αx + β) + δ)."
REFERENCES,0.6090225563909775,"C
TECHNICAL RESULTS AND PROOFS"
REFERENCES,0.6109022556390977,"Lemma 1. For networks using the activation function ˜φα(x) =
q"
REFERENCES,0.6127819548872181,"2
1+α2 φα(x), the local Q and C
maps are given by"
REFERENCES,0.6146616541353384,"Q(q) = q
and
C(c) = c + (1 −α)2"
REFERENCES,0.6165413533834586,π(1 + α2) p
REFERENCES,0.618421052631579,"1 −c2 −c cos−1(c)

.
(20)"
REFERENCES,0.6203007518796992,"Proof. In this proof we will use the notation Qφ and Cφ to denote the local Q and C maps for
networks that use a given activation function φ."
REFERENCES,0.6221804511278195,"First, we note that LReLU is basically the weighted sum of identity and ReLU. In particular, we have
the following equation:"
REFERENCES,0.6240601503759399,"φα(x) = αx + (1 −α)φ0(x) = max{x, 0} + α min{x, 0}."
REFERENCES,0.6259398496240601,"Second, we have that Qφα(q) = Ez∼N(0,1)

qz2I[z ≥0]

+ α2Ez∼N(0,1)

qz2I[z < 0]

= 1+α2"
Q,0.6278195488721805,"2
q
(from which Q ˜φα(q) = q immediately follows)."
Q,0.6296992481203008,Published as a conference paper at ICLR 2022
Q,0.631578947368421,"It then follows from equation 5, and the fact that local C maps are invariant to multiplication of the
activation function by a constant, that"
Q,0.6334586466165414,"C ˜φα(c) = Cφα(c) =
2
1 + α2 Ez1,z2∼N(0,1)
h
φα (z1) φα

cz1 +
p"
Q,0.6353383458646616,"1 −c2z2
i"
Q,0.6372180451127819,"=
2
1 + α2

α2c + (1 −α)2Cφ0(c)Qφ0(1)
"
Q,0.6390977443609023,"+
2
1 + α2"
Q,0.6409774436090225,"h
2α(1 −α)Ez1,z2∼N(0,1)
h
(cz1 +
p"
Q,0.6428571428571429,"1 −c2z2)φ0 (z1)
ii
(21)"
Q,0.6447368421052632,From Daniely et al. (2016) we have that
Q,0.6466165413533834,Cφ0(c) = √
Q,0.6484962406015038,1 −c2 + (π −cos−1(c))c
Q,0.650375939849624,"π
,
(22)"
Q,0.6522556390977443,and for the last part of equation 21 we have
Q,0.6541353383458647,"Ez1,z2∼N(0,1)
h
(cz1 +
p"
Q,0.6560150375939849,"1 −c2z2)φ0 (z1)
i
= Ez1∼N(0,1)

cz2
11z1>0

= c"
Q,0.6578947368421053,"2.
(23)"
Q,0.6597744360902256,"Plugging equation 22 and equation 23 back into equation 21, we get"
Q,0.6616541353383458,"C ˜φα(c) =
2
1 + α2 """
Q,0.6635338345864662,α2c + (1 −α)2 2 √
Q,0.6654135338345865,1 −c2 + (π −cos−1(c))c
Q,0.6672932330827067,"π
+ α(1 −α)c #"
Q,0.6691729323308271,= (1 −α)2  √
Q,0.6710526315789473,"1 −c2 + c(π −cos−1(c))

+ 2παc
(1 + α2)π
. (24)"
Q,0.6729323308270677,Rearranging this gives the claimed formula.
Q,0.674812030075188,"Proposition 1. The global C map of a feedforward network with ˜φα(x) as its activation function
is equal to that of a rescaled ResNet of the same depth (see Section 2.4) with normalized ReLU
activation φ(x) =
√"
Q,0.6766917293233082,"2 max(x, 0), shortcut weight
q"
Q,0.6785714285714286,"α
1+α2 , and residual branch R consisting of a"
Q,0.6804511278195489,combined layer (or just a normalized ReLU activation) followed by an afﬁne layer.
Q,0.6823308270676691,"Proof. By equation 7, the C map for a residual block B of the hypothesized rescaled ResNet is given
by
CB(c) = w2c + (1 −w2)Cφ0(c).
(25)"
Q,0.6842105263157895,"The global C map of this network is given by L compositions of this function, while the global C
map of the hypothesized feedforward network is given by L compositions of C ˜φα(c). So to prove the
claim it sufﬁces to show that CB(c) = C ˜φα(c)."
Q,0.6860902255639098,"Taking w =
q"
Q,0.6879699248120301,"2α
1+α2 , one obtains the following"
Q,0.6898496240601504,"CB(c) =
2α
1 + α2 + (1 −α2)"
Q,0.6917293233082706,1 + α2 √
Q,0.693609022556391,1 −c2 + c(π −cos−1(c))
Q,0.6954887218045113,"π
,
(26)"
Q,0.6973684210526315,which is exactly the same as C ˜φα(c) as given in Lemma 1. This concludes the proof.
Q,0.6992481203007519,"Proposition 3. Suppose f is vanilla network consisting of L combined layers with the TReLU
activation function (so that Cf(0) = η ∈(0, 1)). Then Cf converges to a limiting map on (−1, 1) as
L goes to inﬁnity. In particular,
lim
L→∞Cf(c) = ψ(c, T),
(27)"
Q,0.7011278195488722,"where T is such that ψ(0, T) = η, and where ψ is the solution of the following ordinary differential
equation (ODE) with the ﬁrst argument being the initial condition (i.e. ψ(c, 0) = c), and the second
argument being time:
dx(t)"
Q,0.7030075187969925,"dt
=
p"
Q,0.7048872180451128,"1 −x(t)2 −x(t) cos−1(x(t)).
(28)"
Q,0.706766917293233,Published as a conference paper at ICLR 2022
Q,0.7086466165413534,"Proof. First, we notice that the local C map for TReLU networks can be written as a difference
equation:"
Q,0.7105263157894737,C(c) = c + (1 −α)2
Q,0.7124060150375939,π(1 + α2) p
Q,0.7142857142857143,"1 −c2 −c cos−1(c)

.
(29)"
Q,0.7161654135338346,"Importantly, C is a monotonically increasing function of c, whose derivative goes to zero only as
α ∈[0, 1] goes to 1. Thus, to achieve Cf(0) = η in the limit of large L, we require that
(1−α)2"
Q,0.7180451127819549,"π(1+α2) goes
to 0. This implies that the above difference equation converges to the ODE in equation 28."
Q,0.7199248120300752,"Because the function
√"
Q,0.7218045112781954,"1 −x2−x cos−1(x) is continuously differentiable in [−1, 1], and its derivative
−cos−1(x) is bounded, one can immediately show that it is globally Lipschitz, and the ODE has a
unique solution ψ(c0, t) according to Theorem 3.2 of Khalil (2008)."
Q,0.7236842105263158,"Now, we are only left to ﬁnd the time T such that C∞
f (0) = ψ(0, T) = η. To that end, we notice that"
Q,0.7255639097744361,"g(x) =
p"
Q,0.7274436090225563,"1 −x2 −x cos−1(x) > 0, for x ∈(−1, 1)
(30)"
Q,0.7293233082706767,"because g(1) = 0 and g′(x) = −cos−1(x) < 0 on (−1, 1). This implies that the ψ(0, t) is a
monotonically increasing continuous function of t. Since ψ(0, 0) = 0, to establish the existence of T
it sufﬁces to show that ψ(0, ∞) ≥1."
Q,0.731203007518797,"To this end we ﬁrst observe that
g(x) ≥2
√"
Q,0.7330827067669173,"2
3 (1 −x)3/2,
(31)"
Q,0.7349624060150376,"which follows by deﬁning h(x) = g(x) −2
√"
Q,0.7368421052631579,"2
3 (1 −x)3/2 and observing that h(1) = 0 and h′(x) =
−cos−1(x)+
√"
Q,0.7387218045112782,"2(1−x)1/2 < 0 on (−1, 1). Given this, it is sufﬁcient to show that the solution ˜ψ for
the ODE ˙x = 2
√"
Q,0.7406015037593985,"2
3 (1 −x)3/2 satisﬁes ˜ψ(0, ∞) = 1. The solution ˜ψ turns out to have a closed-form
of ˜ψ(0, t) = 1 −(
3
√"
Q,0.7424812030075187,"2t+3)2, and thus ψ(0, ∞) ≥˜ψ(0, ∞) = 1. This completes the proof."
Q,0.7443609022556391,"Theorem 1. For a network f with ˜φα(x) as its activation function (with α ≥0), we have"
Q,0.7462406015037594,"max
c∈[−1,1] |Cf(c) −c| ≤min {4Cf(0), 1 + Cf(0)} ,
max
c∈[−1,1]"
Q,0.7481203007518797,"C′
f(c) −1
 ≤min {4Cf(0), 1}
(10)"
Q,0.75,"Proof. Because Cf is a positive deﬁnite function (by Section A.4) we have that it can be written as
Cf(c) = P∞
n bncn for bn ≥0. Given Cf(1) = C′
f(1) = 1, we have ∞
X"
Q,0.7518796992481203,"n=0
bn = ∞
X"
Q,0.7537593984962406,"n=1
nbn = 1
=⇒
b0 = ∞
X"
Q,0.7556390977443609,"n=2
(n −1)bn
=⇒
2b0 + b1 ≥1.
(32)"
Q,0.7575187969924813,"Hence, 1 −C′
f(0) = 1 −b1 ≤2b0 = 2Cf(0). Now we are ready to bound the deviation of Cf(c)
from identity:"
Q,0.7593984962406015,"max
c∈[−1,1] |Cf(c) −c| =
max
c∈[−1,1] b0 + ∞
X"
Q,0.7612781954887218,"n=2
bncn −(1 −b1)c "
Q,0.7631578947368421,"≤
max
c∈[−1,1] "" b0 + ∞
X"
Q,0.7650375939849624,"n=2
bn|c|n + (1 −b1)|c| #"
Q,0.7669172932330827,"= b0 + ∞
X"
Q,0.768796992481203,"n=2
bn + 1 −b1 = 2(1 −b1)"
Q,0.7706766917293233,"= 2(1 −C′
f(0)) ≤4Cf(0). (33)"
Q,0.7725563909774437,Using equation 20 we have that
Q,0.7744360902255639,C′(c) = 1 −(1 −α)2
Q,0.7763157894736842,(1 + α2)π cos−1(c).
Q,0.7781954887218046,"From our assumption that α ≥0 it follows that 0 ≤C′(c) ≤1 for all c ∈[−1, 1]. Since the property
of having a derivative bounded between 0 and 1 is closed under functional composition and positive"
Q,0.7800751879699248,Published as a conference paper at ICLR 2022
Q,0.7819548872180451,"weighted averages, it thus follows that 0 ≤Cf(c) ≤1 for all c ∈[−1, 1]. An immediate consequence
of this is that Cf(c) is non-decreasing, and that"
Q,0.7838345864661654,"max
c∈[−1,1] |Cf(c) −c| = Cf(−1) + 1 ≤Cf(0) + 1.
(34)"
Q,0.7857142857142857,"Next, we bound the deviation of C′
f(c) from 1:"
Q,0.7875939849624061,"max
c∈[−1,1]"
Q,0.7894736842105263,"C′
f(c) −1
 =
max
c∈[−1,1]  ∞
X"
Q,0.7913533834586466,"n=2
nbncn−1 −(1 −b1) "
Q,0.793233082706767,"≤
max
c∈[−1,1] "" ∞
X"
Q,0.7951127819548872,"n=2
nbn|c|n−1 + (1 −b1) # = ∞
X"
Q,0.7969924812030075,"n=2
nbn + 1 −b1 = 2(1 −b1)"
Q,0.7988721804511278,"= 2(1 −C′
f(0)) ≤4Cf(0). (35)"
Q,0.8007518796992481,"From the previous fact that 0
≤
C′
f(c)
≤
1 for all c
∈
[−1, 1] we also have that"
Q,0.8026315789473685,"maxc∈[−1,1]
C′
f(c) −1
 ≤1. This completes the proof."
Q,0.8045112781954887,"Theorem 2. Suppose f is a network with a smooth activation function. If C′
f(1) = 1, then we have"
Q,0.806390977443609,"max
c∈[−1,1] |Cf(c) −c| ≤2C′′
f (1),
max
c∈[−1,1]"
Q,0.8082706766917294,"C′
f(c) −1
 ≤2C′′
f (1)
(13)"
Q,0.8101503759398496,"Proof. Cf is a positive deﬁnite function by Section A.4. So by the fact that positive deﬁnite functions
are non-negative, non-decreasing, and convex on the non-negative part of their domain, we obtain
that C′
f(0) ≥C′
f(1) −C′′
f (1) = 1 −C′′
f (1). By equation 33, we have"
Q,0.8120300751879699,"max
c∈[−1,1] |Cf(c) −c| ≤2(1 −C′
f(0)) ≤2C′′
f (1).
(36)"
Q,0.8139097744360902,"Further by equation 35, we also have"
Q,0.8157894736842105,"max
c∈[−1,1]"
Q,0.8176691729323309,"C′
f(c) −1
 ≤2(1 −C′
f(0)) ≤2C′′
f (1).
(37)"
Q,0.8195488721804511,This completes the proof.
Q,0.8214285714285714,"Proposition 4. Suppose f is some function computed by a neural network with the ReLU activation.
Then for any negative slope parameter α ̸= ±1, we can compute f using an LReLU neural network
of the same structure and double the width of the original network."
Q,0.8233082706766918,"Proof. The basic intuition behind this proof is that a ReLU unit can always be “simulated” by two
LReLU units as long as α ̸= ±1, due to the following formula:"
Q,0.825187969924812,"φ0(x) =
1
1 −α2 (φα(x) + αφα(−x)) ."
Q,0.8270676691729323,"We will begin by proving the claim in the case of a network with one hidden layer. In particular, we
assume the ReLU network has m hidden units:"
Q,0.8289473684210527,"f(w, b, a, x) = m
X"
Q,0.8308270676691729,"r=1
arφ0(w⊤
r x + br),
(38)"
Q,0.8327067669172933,"where x ∈Rd is the input, and w ∈Rmd, b ∈Rm and a ∈Rm are weights, biases of the input layer
and weights of output layer, respectively. For LReLU with negative slope α, one can construct the
following network"
Q,0.8345864661654135,"f(w′, b′, a′, x) ="
"M
X",0.8364661654135338,"2m
X"
"M
X",0.8383458646616542,"r=1
a′
rφα(w′
r
⊤x + b′
r).
(39)"
"M
X",0.8402255639097744,Published as a conference paper at ICLR 2022
"M
X",0.8421052631578947,"If we choose w′
r = wr = −w′
r+m, b′
r = br = −b′
r+m, a′
r =
1
1−α2 ar and a′
r+m =
α
1−α2 ar, we have"
"M
X",0.8439849624060151,"a′
rφα(w′
r
⊤x + b′
r) + a′
r+mφα(w′
r+m
⊤x + b′
r+m)"
"M
X",0.8458646616541353,"=
1
1 −α2 arφα(w⊤
r x + br) −
α2"
"M
X",0.8477443609022557,1 −α2 arφ 1
"M
X",0.849624060150376,"α (w⊤
r x + br) = arφ(w⊤
r x + br),
(40)"
"M
X",0.8515037593984962,"This immediately suggests that f(w′, b′, a′, x) = f(w, b, a, x)."
"M
X",0.8533834586466166,"Since deeper networks, and one with more complex topologies, can be constructed by composing
and summing shallower ones, the general claim follows."
"M
X",0.8552631578947368,"D
EXPERIMENT DETAILS"
"M
X",0.8571428571428571,"For input preprocessing on ImageNet we perform a random crop of size 224×224 to each image, and
apply a random horizontal ﬂip. In all experiments, we applied L2 regularization only to the weights
(and not the biases or batch normalization parameters). We selected the L2 constant by grid search
from {0.00005, 0.00002, 0.0}. For networks without batch normalization layers we applied dropout
to the penultimate layer, with the dropout rate chosen by grid search from {0.2, 0.0}. In addition, we
used label smoothing (Szegedy et al., 2016) with a value of 0.1."
"M
X",0.8590225563909775,"For each optimizer we used a standard learning rate warm-up scheme which linearly increases
the learning rate from 0 to the “initial learning rate” in the ﬁrst 5 epochs, and then decays the
learning rate by a factor of 10 at 4/9 and 7/9 of the total epoch budget5, unless speciﬁed other-
wise. The initial learning rate was chosen by grid search from {1.0, 0.3, 0.1, 0.03, 0.01} for SGD,
{0.003, 0.001, 0.0003, 0.0001, 0.00003} for K-FAC, and {10.0, 3.0, 1.0, 0.3, 0.1} for LARS. For all
optimizers we set the momentum constant to 0.9. For K-FAC, we used a ﬁxed damping value of
0.001, and a norm constraint value of 0.001 (see Ba et al. (2017) for a description of this parameter).
We also updated the Fisher matrix approximation every iteration, and computed the Fisher inverse
every 50 iterations, unless stated otherwise. For LARS, we set the “trust” coefﬁcient to 0.001. For
networks with batch normalization layers, we set the decay value for the statistics to 0.9."
"M
X",0.8609022556390977,"For initialization of the weights we used the scale-corrected uniform orthogonal (SUO) distribu-
tion (Martens et al., 2021) for all methods/models, unless stated otherwise. For a m × k matrix
(with k being the input dimension), samples from this distribution can be generated by computing
 
XX⊤−1/2 X, where X is an m × k matrix with entries sampled independently from N(0, 1).
When m > k, we may apply the same procedure but with k and m reversed, and then transpose the
result. The resulting matrix is further multiplied by the scaling factor max{
p"
"M
X",0.8627819548872181,"m/k, 1}, which will
have an effect only when k ≤m. For convolutional networks, we initialize only the weights in the
center of each ﬁlter to non-zero values, which is a technique known as Delta initialization (Balduzzi
et al., 2017; Xiao et al., 2018), or Orthogonal Delta initialization when used with orthogonal weights
(as we do in this work)."
"M
X",0.8646616541353384,"We implemented all methods/models with JAX (Bradbury et al., 2018) and Haiku (Hennigan et al.,
2020). We used the implementation of SGD and LARS from Optax (Hessel et al., 2020). We used the
JAX implementation of K-FAC available at https://github.com/deepmind/kfac_jax."
"M
X",0.8665413533834586,"E
ADDITIONAL EXPERIMENTAL RESULTS"
"M
X",0.868421052631579,"E.1
EMPIRICAL C VALUES FOR FINITE-WIDTH NETWORKS"
"M
X",0.8703007518796992,"The computation of cosine similarities performed by C maps is only an approximation for ﬁnite
width networks, and it is natural to ask how large the approximation error is. To answer this question,
we compare the theoretical predictions with the empirical simulations on fully-connect networks of
different depths and widths. In particular, we use a ﬁxed η = 0.9 for TReLU and we compute the"
"M
X",0.8721804511278195,"l-th “empirical c value” ˆcl =
xl
1
⊤xl
2
∥xl
1∥∥xl
2∥for each layer index l, where x0
1 and x0
2 are random vectors"
"M
X",0.8740601503759399,"5We later found that cosine learning rate annealing (Loshchilov & Hutter, 2016) is slightly better for most
settings, but this did not change our conclusions."
"M
X",0.8759398496240601,Published as a conference paper at ICLR 2022
"M
X",0.8778195488721805,"0
20
40
60
80
100
depth 0.0 0.2 0.4 0.6 0.8 1.0"
"M
X",0.8796992481203008,c value computed on i-layer
"M
X",0.881578947368421,"width = 1
width = 30
width = 100
width = 300"
"M
X",0.8834586466165414,"(a) TReLU, Gaussian"
"M
X",0.8853383458646616,"0
20
40
60
80
100
depth 0.0 0.2 0.4 0.6 0.8 1.0"
"M
X",0.8872180451127819,c value computed on i-layer
"M
X",0.8890977443609023,"width = 30
width = 100
width = 300"
"M
X",0.8909774436090225,"(b) DKS + SoftPlus, Gaussian"
"M
X",0.8928571428571429,"0
20
40
60
80
100
depth 0.0 0.2 0.4 0.6 0.8 1.0"
"M
X",0.8947368421052632,c value computed on i-layer
"M
X",0.8966165413533834,"width = 1
width = 30
width = 100
width = 300"
"M
X",0.8984962406015038,"(c) TReLU, Orthogonal"
"M
X",0.900375939849624,"0
20
40
60
80
100
depth 0.0 0.2 0.4 0.6 0.8 1.0"
"M
X",0.9022556390977443,c value computed on i-layer
"M
X",0.9041353383458647,"width = 30
width = 100
width = 300"
"M
X",0.9060150375939849,"(d) DKS + Softplus, Orthogonal
Figure 4: Empirical c values for TAT and DKS, which are averaged over 100 pairs of inputs and 50 different
randomly-inialized networks. We include the results for both Gaussian fan-in and Orthogonal initialization.
Vertical lines indicate the standard deviation. TReLU has smaller kernel approximation error and is robust to
Gaussian initialization. For TReLU, we also plot the evolution of the c values (black dashed line) as predicted
by the C map (which we can compute analytically for TReLU)."
"M
X",0.9078947368421053,"chosen so that ∥x0
1∥2 = ∥x0
2∥= d0 and x0
1
⊤x0
2 = 0 (so that ˆc0 = 0). As shown in Figure 4a and 4c,
the approximation error is relatively small even for networks with width 30."
"M
X",0.9097744360902256,"We also included the results for networks using DKS (with ζ = 10) and the SoftPlus activation
function. Figure 4b and 4d reports empirical c values as a function of layer index l, with x0
1 and
x0
2 chosen so that ˆc0 = 0.8. With Gaussian initialization, the standard deviations are much larger
than TReLU, and the average values for widths 30 and 100 deviate signiﬁcantly from the theoretical
predictions. (The DKS conditions implies C(c) ≤c for any c ∈[0, 1], which suggests the c value
should decrease monotonically.) By comparison, the error seems to be much smaller for orthogonal
initialization, which is consistent with the better performance of orthogonal initialization reported by
Martens et al. (2021). (By contrast, we show in Appendix E.7 that Gaussian initialization performs
on par with orthogonal initialization for TReLU.) In addition, we note that the standard deviations
increase along with the depth for both Gaussian and orthogonal initializations."
"M
X",0.9116541353383458,"E.2
RESULTS ON CIFAR-10"
"M
X",0.9135338345864662,"0
2500
5000
7500
10000 12500 15000 17500 20000
Iterations 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95"
"M
X",0.9154135338345865,Validation acc
LAYERS WITH TAT,0.9172932330827067,"100 layers with TAT
100 layers with EOC
304 layers with TAT
304 layers with EOC"
LAYERS WITH TAT,0.9191729323308271,"Figure 5:
CIFAR-10 validation accuracy of
ResNets with ReLU activation function initialized
using either EOC or TAT (ours)."
LAYERS WITH TAT,0.9210526315789473,"In addition to our main results on the ImageNet
dataset, we also compared TAT to EOC on CIFAR-
10 (Krizhevsky et al., 2009) using vanilla networks
derived from a Wide ResNet reference architecture
(Zagoruyko & Komodakis, 2016). In particular, we
start with a Wide ResNet with a widening factor of
2, and remove all the batch normalization layers and
shortcut connections. We trained these networks with
the K-FAC optimizer for 200 epochs using a standard
piecewise constant learning rate schedule. To be spe-
ciﬁc, we decay the learning rate by a factor of 10 at
75 and 150 epochs. For K-FAC, we set the damping
value to 0.01 and norm constraint value to 0.0001. For data preprocessing we include basic data"
LAYERS WITH TAT,0.9229323308270677,Published as a conference paper at ICLR 2022
LAYERS WITH TAT,0.924812030075188,"augmentations such as random crop and horizontal ﬂip during training. As shown in Figure 5, TAT
outperforms EOC signiﬁcantly. As we increase the depth from 100 to 304, the accuracy of EOC
network drops dramatically while the accuracy of the TAT network remains roughly unchanged."
LAYERS WITH TAT,0.9266917293233082,"E.3
REDUCING THE OVERHEAD OF K-FAC"
LAYERS WITH TAT,0.9285714285714286,"0
50000
100000
150000
200000
Iterations 0.550 0.575 0.600 0.625 0.650 0.675 0.700 0.725 0.750"
LAYERS WITH TAT,0.9304511278195489,Validation acc
LAYERS WITH TAT,0.9323308270676691,"Depth=50, K-FAC w/ inv_period=50
Depth=50, K-FAC w/ cov_period=10, inv_period=200
Depth=50, SGD
Depth=50, SGD w/ 180 epochs"
LAYERS WITH TAT,0.9342105263157895,"0
10000
20000
30000
40000
50000
Wall-clock time (seconds) 0.550 0.575 0.600 0.625 0.650 0.675 0.700 0.725 0.750"
LAYERS WITH TAT,0.9360902255639098,Validation acc
LAYERS WITH TAT,0.9379699248120301,"Depth=50, K-FAC w/ inv_period=50
Depth=50, K-FAC w/ cov_period=10, inv_period=200
Depth=50, SGD
Depth=50, SGD w/ 180 epochs"
LAYERS WITH TAT,0.9398496240601504,"Figure 6: Top-1 validation accuracy on ImageNet as a function of number of iterations (left) or wall-clock time
(right) with K-FAC optimizer. One can reduce the computational overhead signiﬁcantly by updating curvature
matrix approximation and its inverse less frequently."
LAYERS WITH TAT,0.9417293233082706,"In our main experiments the per-step wall-clock time of K-FAC was roughly 2.5× that of SGD.
However, this gap can be decreased signiﬁcantly by reducing the frequency of the updates of K-FAC’s
approximate curvature matrix and its inverse. For example, if we update the curvature approximation
every 10 steps, and the inverses every 200 steps, the average per-step wall-clock time of K-FAC
reduces by half to a mere 1.25× that of SGD. Importantly, as can be seen on Figure 6, this does not
appear to signiﬁcantly affect optimization performance."
LAYERS WITH TAT,0.943609022556391,"E.4
DISENTANGLING TRAINING AND GENERALIZATION"
LAYERS WITH TAT,0.9454887218045113,"0
20000
40000
60000
80000
100000
Iterations 0.60 0.65 0.70 0.75 0.80 0.85 0.90"
LAYERS WITH TAT,0.9473684210526315,Training acc
LAYERS WITH TAT,0.9492481203007519,"50 layers with TAT
50 layers with EOC
101 layers with TAT
101 layers with EOC
200 layers with TAT
200 layers with EOC"
LAYERS WITH TAT,0.9511278195488722,"Figure 7: ImageNet training accuracy of deep
vanilla networks with either EOC-initialized
ReLU networks or TReLU networks."
LAYERS WITH TAT,0.9530075187969925,"In our main experiments we only reported validation
accuracy on ImageNet, making it hard to tell whether
the superior performance of TAT vs EOC is due to
improved ﬁtting/optimization speed, or improved gen-
eralization. Here, we compare training accuracies of
EOC-initialized networks (with ReLU) and networks
with TReLU, in exactly the same experimental setting
as Figure 1. We train each network on ImageNet us-
ing K-FAC for 90 epochs. For each setting, we plot
the training accuracy for the hyperparameter combi-
nation that gave the highest ﬁnal validation accuracy.
As shown in Figure 7, the EOC-initialized networks
achieve competitive (if not any better) training accu-
racy, suggesting that the use of TReLU improves the
generalization performance and not optimization performance."
LAYERS WITH TAT,0.9548872180451128,"E.5
CLOSING THE REMAINING GAP USING WIDER NETWORKS"
LAYERS WITH TAT,0.956766917293233,"Table 9: The effect of increasing width on Im-
ageNet validation accuracy. We use vanilla net-
works for EOC and TAT (ours)."
LAYERS WITH TAT,0.9586466165413534,"Depth
Width
EOC
TAT
ResNets"
LAYERS WITH TAT,0.9605263157894737,"50
1×
72.0
76.0
76.7
2×
73.5
77.3
77.9"
LAYERS WITH TAT,0.9624060150375939,"101
1×
62.4
76.5
77.9
2×
66.5
77.6
78.6"
LAYERS WITH TAT,0.9642857142857143,"In all of our main experiments we used networks
derived from standard ResNets (by removing nor-
malization layers and/or shortcut connections). By
construction, these have the same layer widths as stan-
dard ResNets. A natural question to ask is whether
using wider networks would change our results. For
example, it’s possible that vanilla networks with TAT
would beneﬁt more than ResNets from increased
width, since higher width would make the kernel
approximations more accurate, and could also help
compensate for the minor loss of expressive power due to the removal of shortcut connections."
LAYERS WITH TAT,0.9661654135338346,Published as a conference paper at ICLR 2022
LAYERS WITH TAT,0.9680451127819549,"With layers double the width of standard ResNets, it becomes too expensive to store and invert
Kronecker factors used in K-FAC. Therefore, we only train these wider networks with SGD. In order
to mitigate the slower convergence of SGD for vanilla networks (see Section 5.5), we train them for
360 epochs at a batch size of 512. Note that due to increased overﬁtting we observed in ResNets
after 360 epochs (resulting in lower validation accuracy) we only trained them for 90 epochs. As
shown in Table 9, doubling the width does indeed narrow the remaining validation accuracy gap
between ResNets and vanilla TAT networks. In particular, the gap goes from 0.7% to 0.6% for depth
50 networks, and from 1.4% to 1% for depth 101 networks."
LAYERS WITH TAT,0.9699248120300752,"E.6
COMPARISON WITH PRELU ON RESCALED RESNETS"
LAYERS WITH TAT,0.9718045112781954,"Table 10:
Comparison with PReLU with rescaled
ResNets (w = 0.8)."
LAYERS WITH TAT,0.9736842105263158,"Depth
Optimizer
TReLU
PReLU0.0
PReLU0.25"
K-FAC,0.9755639097744361,"50
K-FAC
76.4
75.7
73.6"
K-FAC,0.9774436090225563,"SGD
76.0
71.5
71.5"
K-FAC,0.9793233082706767,"101
K-FAC
77.8
76.4
76.8"
K-FAC,0.981203007518797,"SGD
77.3
73.1
73.4"
K-FAC,0.9830827067669173,"In Table 6 of the main text we compare PReLU
and TReLU on deep vanilla networks. Here
we extend this comparison to rescaled ResNets
with a shortcut weight of w = 0.8. For PReLU,
we again include two different initializations:
one with 0 negative slope (effectively ReLU),
and another with 0.25 negative slope (which
was used in He et al. (2015)). We report the
full results in Table 10. For all settings, TAT
outperforms PReLU by a large margin, suggesting that a better-initialized negative slope is crucial
for both rescaled ResNets and deep vanilla networks."
K-FAC,0.9849624060150376,"E.7
COMPARISON OF DIFFERENT INITIALIZATIONS"
K-FAC,0.9868421052631579,"Table 11: Comparison of Orthogonal Delta and Gaussian fan-in
initialization."
K-FAC,0.9887218045112782,"Depth
Optimizer
Init
ResNet
EOC
TAT"
"K-FAC
ORTH DELTA",0.9906015037593985,"50
K-FAC
Orth Delta
76.4
72.6
74.6
Gaussian
76.5
72.5
74.8"
"K-FAC
ORTH DELTA",0.9924812030075187,"SGD
Orth Delta
76.3
63.7
71.0
Gaussian
76.6
63.1
68.7"
"K-FAC
ORTH DELTA",0.9943609022556391,"101
K-FAC
Orth Delta
77.8
71.8
74.2
Gaussian
77.8
72.3
74.1"
"K-FAC
ORTH DELTA",0.9962406015037594,"SGD
Orth Delta
77.9
41.6
70.0
Gaussian
78.0
41.1
68.7"
"K-FAC
ORTH DELTA",0.9981203007518797,"In all of our experiments we use the
Orthogonal Delta initialization intro-
duced by Balduzzi et al. (2017) and
Xiao et al. (2018). This is because
it’s technically required in order to
apply the extended Q/C map analy-
sis of Martens et al. (2021) (which
underlies DKS and TAT) to convo-
lutional networks, and because it is
generally thought to be beneﬁcial.
In this subsection we examine this
choice more closely by comparing it
to a traditional Gaussian fan-in ini-
tialization (with σ2
w = 2 for ReLUs).
We consider standard ResNets and
deep vanilla networks using either EOC (with ReLUs) or TAT with (with LReLU). Surprisingly, it
turns out that the Orthogonal Delta initialization does not have any clear advantage over the Gaussian
fan-in approach, at least in terms of validation accuracy after 90 epochs."
