Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.004405286343612335,"We scale perceived distances of the core-set algorithm by a factor of uncertainty
and search for low-conﬁdence conﬁgurations, ﬁnding signiﬁcant improvements
in sample efﬁciency across CIFAR10/100 and SVHN image classiﬁcation, espe-
cially in larger acquisition sizes. We show the necessity of our modiﬁcations and
explain how the improvement is due to a probabilistic quadratic speed-up in the
convergence of core-set loss, under assumptions about the relationship of model
uncertainty and misclassiﬁcation."
INTRODUCTION,0.00881057268722467,"1
INTRODUCTION"
INTRODUCTION,0.013215859030837005,"Active learning aims to identify the most informative data to label and include in supervised train-
ing. Often, these algorithms focus on reducing model variance, representing distributional densities,
maximizing expected model change, or minimizing expected generalization error (Kirsch et al.,
2019; Sener & Savarese, 2018; Settles, 2009; Shen et al., 2018; Sinha et al., 2019). A unifying
theme is efﬁcient data collection, which is measured by the rate in improvement as more data are la-
belled. This is important when we want to identify only the most promising samples to be labelled,
but also for tasks that require slow or expensive labelling (Ducoffe & Precioso, 2018; Ma et al.,
2020; Settles, 2009)."
INTRODUCTION,0.01762114537444934,"We describe active learning with the same notation as Sener & Savarese (2018). Suppose we wish
to classify elements of a compact space X into labels Y = {1, . . . , C}. We collect n data points
{xi, yi}i∈[n] ∼PX×Y, but only have access to the labels of m of these, denoted by their indices
s = {s(i) ∈[n]}i∈[m]. We use the learning algorithm As on the labelled set s to return the optimized
parameters of the classiﬁer, and measure performance with the loss function l(·, ·; As) : X ×Y →R.
The goal of active learning is to produce a set of indices s+ whose cardinality is limited by the la-
belling budget b, such that expected loss is minimized upon labelling and training on these elements:
arg mins+:|s+|≤b
Ex,y∼PX×Y

l(x, y; As∪s+)

(Sener & Savarese, 2018). In practice, we use the
test set {xi, yi}i∈[t] ∼PX×Y to approximate the expectation. The typical way to assess the data-
efﬁciency of any particular active learning algorithm is to compare its trend of test performance
across increasing labels compared to random and other sampling baselines (Kirsch et al., 2019;
Sener & Savarese, 2018; Settles, 2009; Shen et al., 2018; Sinha et al., 2019; Ducoffe & Precioso,
2018; Ma et al., 2020)."
INTRODUCTION,0.022026431718061675,"Sener & Savarese (2018) suggested that we can improve data-efﬁciency by minimizing the core-set
radius, δ, deﬁned as the maximum distance of any unlabelled point from its nearest labelled point:
δ = maxi∈[n] minj∈s δ(xi, xj). Given the generalization error ζn of all labelled and unlabelled
data, and zero training error, expected error converges linearly with respect to δ (Sener & Savarese,
2018):"
INTRODUCTION,0.02643171806167401,"E
x,y∼PX×Y
[l(x, y; As)] ≤ζn + 1 n X"
INTRODUCTION,0.030837004405286344,"i∈[n]
l(xi, yi) ≤ζn + O (Cδ) + O r"
N,0.03524229074889868,"1
n ! (1)"
N,0.039647577092511016,"Sener & Savarese (2018) argued that generalization error for neural networks has well-deﬁned
bounds, so optimizing the rest of Equation 1, referred to as core-set loss, is critical for active learn-
ing. Indeed, their algorithms for optimizing core-sets consistently improved over their baselines
(Sener & Savarese, 2018)."
N,0.04405286343612335,Under review as a conference paper at ICLR 2022
N,0.048458149779735685,"Uncertainty-based sampling is particularly valuable for identifying support vectors, leading to ﬁner
classiﬁcation boundaries (Kirsch et al., 2019; Settles, 2009). However, these methods may catas-
trophically concentrate their labelling budget on difﬁcult, noisy regions between classes, as shown
in Figure 1."
N,0.05286343612334802,"Figure 1: A toy demonstration of catastrophic concentration from least conﬁdence acquisition functions.
Columns 2, 3 and 4 show the labelling requests made by three different active learning strategies over 5 it-
erations on the dataset shown in column 1. Each strategy has a request budget of 50 labels per iteration. Test
results show the mean and 1 standard deviation of 3 random initializations. Whereas least conﬁdence wastes its
labelling budget on difﬁcult regions between clusters and performs worse than random acquisition, the greedy
core-set strategy covers the input space effectively and achieves superior label efﬁciency."
N,0.05726872246696035,We present a two-part solution for incorporating uncertainty into core-sets:
N,0.06167400881057269,"1. Scale distances between points by doubt (I) (Settles, 2009; Shen et al., 2018) before com-
puting core-set radii:"
N,0.06607929515418502,"ˆδi = δi I(xi), where I(x) = 1 −max
y
P(y|x)
(2)"
N,0.07048458149779736,"2. Apply beam search to greedily identify the core-set conﬁguration among K-candidates
with the lowest maximum log-conﬁdence to reduce the variance of core-set trajectories."
BACKGROUND,0.07488986784140969,"2
BACKGROUND"
BACKGROUND,0.07929515418502203,"Greedy versus optimal core-set for active learning.
Core-set radius δ is the maximum
of all distances between each data point in xu
=
{xi
:
∀i
∈
[n]} and its closest la-
belled point in xl
=
{xi
:
∀i
∈
s} (Sener & Savarese, 2018).
The optimal core-
set achieves linear convergence of core-set loss in respect to δ by ﬁnding the acquisition
set s+ with optimal core-set radii δOP T shown in Equation 3 (Sener & Savarese, 2018).
Sener and Savarese used l2-norm between activations of the last layer of VGG16 as ∆."
BACKGROUND,0.08370044052863436,"δOP T = min
s+
max
i∈[n]
min
j∈s+∪s ∆(xi, xj)
(3)
δg = max
i∈[n]
min
j∈ˆ
s+∪s ∆(xi, xj) ≤2 δOP T
(4)"
BACKGROUND,0.0881057268722467,"Since this problem is NP-hard (Cook
et al., 1998), Sener & Savarese (2018)
proposed a greedy version shown in Al-
gorithm 1 with acquisitions ˆ
s+ bounded
above by Equation 4. It returns a selec-
tion mask over the data pool to signal la-
belling requests for elements that greedily
minimize the maximum distance between
any point and its nearest labelled point."
BACKGROUND,0.09251101321585903,"Algorithm
1:
Greedy
core-set
(Sener
&
Savarese, 2018)"
BACKGROUND,0.09691629955947137,"1 def greedy core-set(xu, xl, budget):"
BACKGROUND,0.1013215859030837,"2
selection = [0 : ∀i ∈[|xu|]];"
BACKGROUND,0.10572687224669604,"3
for t = 0, . . . , budget do"
BACKGROUND,0.11013215859030837,"4
i =
arg maxi∈[|xu|] minj∈[|xl|] ∆(x(i)
u , x(j)
l
);"
BACKGROUND,0.1145374449339207,"5
selection(i) = 1;"
BACKGROUND,0.11894273127753303,"6
return selection;"
BACKGROUND,0.12334801762114538,Under review as a conference paper at ICLR 2022
BACKGROUND,0.1277533039647577,"Figure 2 shows how δ varies compared to closest-K-means core-sets, where the core-set consists of
the closest points to optimized K-means."
BACKGROUND,0.13215859030837004,"Figure 2: Core-set radii (δ) differs between optimal, greedy and closest-K-means core-sets (K = 4). Crosses
and dots represent the labelled and unlabelled set, respectively, and translucent circles show the δ-cover. Sener
& Savarese (2018) found a δ-linear upper bound on core-set loss convergence."
BACKGROUND,0.13656387665198239,"Related techniques for batched acquisition.
Batch active learning by diverse gradient embed-
dings acquires batches in two steps. First, we compute loss gradients in respect to the parameters
of the last layer of the classiﬁer for each unlabelled point and its most probable label (Ash et al.,
2020). Then, we sample from clusters of these gradients using, for instance, K-means++ to avoid
catastrophic concentration (Ash et al., 2020). We share similar intuitions about classiﬁer conﬁdence
and intra-batch diversity being important sources of information that may enhance active learning,
but differ in that we do not optimize for model change and use core-sets for diversiﬁcation because
of its theoretical foundations."
BACKGROUND,0.14096916299559473,"BatchBALD acquires batches that maximize the mutual information between the joint data and
model parameters and was intended to overcome redundant sampling of repeated BALD (Kirsch
et al., 2019). We tried combining this with a probabilistic technique of estimating likely core-set
locations (see Appendix A.2), but it appeared that core-sets mainly require δ minimization for core-
set loss convergence."
METHODS,0.14537444933920704,"3
METHODS"
METHODS,0.14977973568281938,"Algorithm 2 and its dependence on Algorithm 3 implement doubt-weighted greedy core-set to run
on GPU. To incorporate uncertainty information, we make two key changes to the original greedy
core-set algorithm. First, we compute core-sets in a warped space where distances originating from
any unlabelled point diminish to zero with classiﬁcation conﬁdence."
METHODS,0.15418502202643172,"Given inputs xu and xl, which repre-
sent the unlabelled and labelled data,
Line 2 of Algorithm 2 calls Algorithm
3 to pre-compute distances of each
unlabelled datum to their nearest la-
belled data. We scale these distances
by the doubt of the classiﬁer on the
respective unlabelled data on Lines 3
and 11. Each acquisition is removed
from the existing unlabelled pool, and
Line 12 updates new core-set radii.
Figure 3 illustrates how core-sets in
these spaces preferentially cover re-
gions of low conﬁdence."
METHODS,0.15859030837004406,Algorithm 2: Doubt-weighted core-set
METHODS,0.16299559471365638,"1 def doubted core-set(xu, xl, batch size, budget):"
METHODS,0.16740088105726872,"2
min δ = compute min δ(xu, xl, batch size);"
METHODS,0.17180616740088106,"3
min ˆδ = [min δ(i) · I(x(i)
u ) : ∀i ∈[|xu|]]];"
METHODS,0.1762114537444934,"4
xu = memory-copy(xu);"
METHODS,0.18061674008810572,"5
index = [i : ∀i ∈[|xu|]];"
METHODS,0.18502202643171806,"6
selection = [0 : ∀i ∈[|xu|]];"
METHODS,0.1894273127753304,"7
for t = 0, . . . , budget do"
METHODS,0.19383259911894274,"8
i = arg max min ˆδ(i);"
METHODS,0.19823788546255505,"9
selection(index[i]) = 1;"
METHODS,0.2026431718061674,"10
x = x(i)
u ;
splice out: index(i), x(i)
u , min δ(i)"
METHODS,0.20704845814977973,"11
ˆδ = ∆(x, xu) · I(x);"
METHODS,0.21145374449339208,"12
min ˆδ =
h
min{min ˆδ(k), ˆδ(k)} : ∀k ∈[|xu|]
i"
METHODS,0.21585903083700442,"13
return selection;"
METHODS,0.22026431718061673,Under review as a conference paper at ICLR 2022
METHODS,0.22466960352422907,"Figure 3: Quadrant classiﬁcation of x ∈[−1, 1]2, represented as translucent dots. The ﬁrst column shows the
initial dataset and model conﬁdence. Columns progress from left to right at a rate of 20 label acquisitions per
step. Solid squares represent current acquisitions, while translucent squares represent previous acquisitions.
Row 1 shows that the core-sets maximize uniform coverage of the feature space. In contrast, rows 2 and 3 show
preferential coverage of class boundaries when we apply the same algorithm to point-wise distances that are
scaled by the doubt of the model. Rows 4 and 5 show how this new method maximizes uniform coverage in the
uniquely warped space perceived from candidate points in an uncertain (origin) versus high-conﬁdence (-1, 1)
region. Note how the furthest distances from the origin are along the axes (i.e., the furthest distances from (-1,
1) occur along the boundary of the red quadrant with the green and orange quadrants, rather than (1, -1))."
METHODS,0.2290748898678414,"We choose the same ∆as Sener & Savarese (2018), which is l2-norm between activations of the last
layer of VGG16. Given unlabelled data of size U = |xu|, labelled data of size L = |xl|, feature size
∀i ∈[U], ∀j ∈[L], D = dim x(i)
u
= dim x(j)
l , batch size B ≪min{U, L} and labelling budget b,
Algorithm 3 costs Θ (ULD) steps and Θ
 
B2D

memory with the bottleneck on line 5. Excluding
line 2, Algorithm 2 costs O (bUD) in both computation and memory with the bottleneck on line
11. Since both the original core-set algorithm and our modiﬁcation requires ﬁne-tuning VGG16
per addition to the training set, and computing the class probabilities requires only a single linear
transformation, the ﬁnal computational complexity is the same as the original core-set search. For
core-set sizes 5k to 15k, compute time scales linearly from 25 s to 50 s on a NVIDIA Titan GPU."
METHODS,0.23348017621145375,Under review as a conference paper at ICLR 2022
METHODS,0.23788546255506607,Algorithm 3: Memory-efﬁcient core-set radii
METHODS,0.2422907488986784,"1 def compute min δ(xu, xl, b):"
METHODS,0.24669603524229075,"2
min δ = [∞: ∀k ∈[|xu|]];"
METHODS,0.2511013215859031,"3
for i = 0, . . . , |xu| b
do"
METHODS,0.2555066079295154,"4
for j = 0, . . . , |xl| b
do"
METHODS,0.2599118942731278,"5
˜δi:i+b,j:j+b = ∆(x(i:i+b)
u
, x(j:j+b)
l
);"
METHODS,0.2643171806167401,"6
min ˜δi:i+b =

min
c∈[j,j+b]
˜δ(k,c)
i:i+b,j:j+b : ∀k ∈[i, i + b]

;"
METHODS,0.2687224669603524,"7
min δ(i:i+b) =
h
min
n
min δ(k), min ˜δ(k)
i:i+b
o
: ∀k ∈[i, i + b]
i"
METHODS,0.27312775330396477,"8
return min δ;"
METHODS,0.2775330396475771,"Second,
we
use
beam
search
to
greedily
prune
and
keep
track
of
the
top
result-
ing core-set conﬁgurations with the lowest overall conﬁdence.
Since there is no guar-
antee for the optimality of greedy core-sets (Sener & Savarese, 2018),
we seek an
orientation with the most points near classiﬁcation regions of high uncertainty at the
cost of increasing compute and memory complexity by a factor of the beam width.
We modify maximum normalized log probability
(Shen et al., 2018) to rank overall classiﬁer uncertainty
U of core-set s:
U(s) = −1 |s| X"
METHODS,0.28193832599118945,"x∈s
log(1 −I(x))
(5)"
METHODS,0.28634361233480177,Figure 4 shows a sample ranking of the conﬁgurations found during beam search with width K = 4.
METHODS,0.2907488986784141,"Figure 4: A sample acquisition from the toy quadrant dataset in Figure 3. Beam search for the top greedy
core-sets ranks the conﬁgurations by increasing overall conﬁdence (i.e., decreasing negative log conﬁdence).
Regions of high uncertainty occur near the x1 = 0 and x2 = 0 axes in this dataset. The left-most conﬁguration
(beam rank 1) should be selected for its dual optimality of core-set radii and low overall conﬁdence."
METHODS,0.29515418502202645,"Active learning pipeline.
For each active learning experiment, we start by randomly partitioning
the full training set into an initial pool of labelled data and an unlabelled pool of features. We ﬁne-
tune the parameters of a ImageNet-1-pretrained VGG16 on this initial dataset. For each training
batch size of 64, we optimize for cross-entropy loss using Adam (Kingma & Ba, 2015) under default
hyperparameters from PyTorch (Paszke et al., 2019) and a learning rate of 0.01 for CIFAR10/100
and 0.005 for SVHN. We then use either random acquisition, the original greedy core-set algorithm,
or variations of Algorithm 2 with the trained model to produce a selection mask over the unlabelled
data. We enforce that the number of selected elements is equal to the labelling budget per iteration.
The selected features and their labels join the training set, the model retrains with a re-initialized
optimizer, and the process is repeated until the number of the labelled data reaches the speciﬁed
ceiling for the experiment."
METHODS,0.29955947136563876,"Note that we do not compare with the other baselines used in the original core-set experiments. Since
the original core-set algorithm improved signiﬁcantly from those baselines, we expect improvement
over the original core-set algorithm to imply similar or greater improvement as well."
METHODS,0.3039647577092511,Under review as a conference paper at ICLR 2022
METHODS,0.30837004405286345,"Table 1 shows the iterations that we found were
necessary to roughly meet zero training error
on the initial dataset (“First-pass”) and all addi-
tions to the dataset per active learning iteration
(“Thereafter”). Note that validation error is not
required to satisfy the convergence requirement
of core-sets, so we ignore it in our experiments."
METHODS,0.31277533039647576,"Table 1:
Epochs of optimization required to
consistently surpass 99% training accuracy."
METHODS,0.31718061674008813,"Dataset
Training epochs
First-pass
Thereafter
CIFAR10
30
12
CIFAR100
80
20
SVHN
50
20"
METHODS,0.32158590308370044,"For the ablation studies, we tune hyperparameters and conduct ablation studies on CIFAR10
(Krizhevsky, 2009) using a budget of 400 labels per active learning iteration and an initial dataset
size of 1000 samples. We use the same hyperparameters as the ablation studies in the main exper-
iments on CIFAR10/100 (Krizhevsky, 2009) and SVHN (Netzer et al., 2011), which uses a budget
of 5000 labels per iteration and a starting dataset size of 5000 samples."
RESULTS,0.32599118942731276,"4
RESULTS"
RESULTS,0.3303964757709251,"Figure 5 shows the results of ablation studies on the small-scale version of the main experiments,
where we observe that beam search for the core-set conﬁguration with the lowest log conﬁdence
yields signiﬁcant improvements over greedy core-set only if core-set radii are scaled by the uncer-
tainty of each unlabelled sample."
RESULTS,0.33480176211453744,"Figure 5: Ablation results on CIFAR10 showing the mean and 1 standard deviation derived from 5 random
initializations. Left: using beam search (beam=20) to greedily ﬁnd the core-set conﬁguration with the lowest
log conﬁdence has no effect. Middle: weighing perceived distances by uncertainty (pweighted) results in better
performance and beam search reduces variance, resulting in the highest ﬁnal scores. Note that using a single
beam in “pweighted” resulted in occasionally deteriorating performance, which was avoided using 20 beams.
Right: performance improvement over the original greedy core-set algorithm appears signiﬁcant."
RESULTS,0.3392070484581498,"Figure 6 shows how our contributions signiﬁcantly improve the label efﬁciency of greedy core-set
on CIFAR10 and SVHN on large-scale active learning experiments under the same hyperparame-
ters from the ablation studies. Our contributions increase absolute label efﬁciency above random
acquisition."
RESULTS,0.3436123348017621,"Theoretical rationale for improved label efﬁciency using conﬁdence-weighted distances.
Sener & Savarese (2018) showed that the softmax function over c classes is Lipschitz continuous
and we denote its constant as λc. We deﬁne conﬁdence to be the max of the softmax output and
assume that the conﬁdence of any training point is 1. Consider an unspeciﬁed unlabelled point xu
that is located r distance away from its closest labelled point xi in the training set s. Equation 6
shows the bound on doubt I (i.e., 1 minus conﬁdence) as a function of distance r from the nearest
training example. We interpret this to be rising minimum uncertainty with increasing distance from
the nearest training example, capped at 1."
RESULTS,0.34801762114537443,"I(r) ≤min{1, rλc} , where r = min
i∈s ∆(xi, xu)
(6)"
RESULTS,0.3524229074889868,Under review as a conference paper at ICLR 2022
RESULTS,0.3568281938325991,"Figure 6: Means and 1 standard deviation derived from 5 random initializations on image classiﬁcation us-
ing the same acquisition budgets as Sener & Savarese (2018). Weighing perceived distances by uncertainty
(pweighted) and using beam search (beam=10) to ﬁnd the core-set conﬁguration with the lowest log conﬁdence
causes signiﬁcant active learning improvement over vanilla core-sets."
RESULTS,0.36123348017621143,"Recall that we scale δ by doubt to obtain a new radius, ˆδ. Originally, δ was the minimum distance
between each unlabelled point and its nearest labelled point; in our case, we deﬁne ˆδ to be the
minimum distance between each unlabelled point and its nearest unlabelled point with 0 error
that we know exists with probability β. Then, Equation 7 holds with probability β."
RESULTS,0.3656387665198238,"ˆδ = δI(δ) ≤
δ
if δλc ≥1
δ2λc
if δλc < 1
(7)"
RESULTS,0.3700440528634361,"To clarify, we assume that for any unlabelled point, with probability β there exists another nearby
unlabelled point that behaves as if it exists in the training set already. Instead of using the distance
from the nearest labelled point, the core-set loss can use the distance from this unlabelled point
instead. Equation 8 shows that with probability β, the convergence of the core-set loss for all datasets
where δλc < 1 now depends on a quadratic factor of δ versus the linear relationship from before."
N,0.3744493392070485,"1
n X"
N,0.3788546255506608,"i∈[n]
l(xi, yi; As) ≤O

Cˆδ

+ O r"
N,0.3832599118942731,"1
n !"
N,0.3876651982378855,"= O
 
Cδ2λc

+ O r"
N,0.3920704845814978,"1
n !"
N,0.3964757709251101,", only if δλc < 1
(8)"
N,0.4008810572687225,"Recall that the greedy core-set algorithm minimizes δ towards 0 per optimization step. This means
that larger initial training sets should beneﬁt our algorithm more, since the smaller δ will more likely
yield a quadratic convergence of core-set loss."
N,0.4052863436123348,"Next, we analyze the probability β of such an unlabelled point with 0 error existing in a radius ˆδ
around each unlabelled point. In order to do this, we make 2 key assumptions about the relationship
between model conﬁdence and empirical misclassiﬁcation. Assume that the probability of incorrect
classiﬁcation is equal to the product of doubt with ϵ, which represents the error rate given doubt. We
further assume that ϵ equals 0 starting at any training point and is λϵ-Lipschitz continuous for any
distance extending away the closest training point, as shown in Equation 9. To deduce the probabil-
ity β of at least one unlabelled point with 0 error existing between the given unlabelled point and a
distance r0 from its nearest labelled point, we subtract from 1 the probability of non-zero error occur-
ring in all these unlabelled points, which Equation 10 bounds (see proof in Appendix A.1, Claim 1)."
N,0.40969162995594716,"Perr(r) = I(r) · ϵ(r) ≤λcλϵr2
(9)
β = 1− δ
Y"
N,0.41409691629955947,"r0
Perr(r)dr ≥1−(λcλϵ)δ−r0"
N,0.4185022026431718,"exp(δ −r0)2  δδ rr0
0"
N,0.42290748898678415,"2
(10)"
N,0.42731277533039647,"Now suppose r0 = δz, where z ∈[0, 1] is a scaling factor of δ. Then:"
N,0.43171806167400884,β ≥1 −
N,0.43612334801762115,δ√λcλϵ e
N,0.44052863436123346,1−z 1 zz !2δ
N,0.44493392070484583,"(see Appendix A.1: Claim 4)
(11)"
N,0.44933920704845814,Under review as a conference paper at ICLR 2022
N,0.45374449339207046,"We will have no information on whether our algorithm improves upon vanilla core-sets when β ≥0,
or when we rely on random chance that there exists an unlabelled point with 0 error within ˆδ distance
from any unlabelled point of interest. Equation 12 shows the minimum distance δ∗such a point
would have to be from the nearest labelled point:"
N,0.4581497797356828,β ≥0 = 1 −
N,0.46255506607929514,δ∗√λcλϵ e
N,0.4669603524229075,1−z 1 zz !2δ
N,0.4713656387665198,"−→
δ∗≥e z
z
1−z
√λcλϵ
(12)"
N,0.47577092511013214,"Figure 7 shows slices of the lower bound on β from Equation 11. When conﬁdence is high (i.e.,
low z) for an unspeciﬁed unlabelled point located δ from its closest labelled point, we expect higher
probabilities for an unlabelled point with 0 error to exist between δz and δ. The ﬁgure also illustrates
an increase in decay rate of this probability with decreasing conﬁdence. This makes sense because
we assumed that conﬁdence, to some degree, indicates correctness of the model on unlabelled data."
N,0.4801762114537445,"Figure 7: We ﬁx λc = λϵ = 1 and observe the probability β of our algorithm performing better than the
original greedy core-set algorithm across a range of distances δ from the nearest training point at different
slices of doubt z. All colored regions represent possible values of β. Orange regions occur when δ < 1/λc,
which represent the probability of quadratic convergence of core-set loss in respect to δ (see Equation 8). All
blue regions indicate values of β in which our algorithm performs at least as well as greedy core-set search.
Pale blue regions occur when δ > δ∗(see Equation 12), which represent areas where we cannot reason about
β."
N,0.4845814977973568,"There is less information to extract about the surroundings of points with low conﬁdence, so the
lower bound on β naturally ﬂattens sooner, resulting in shorter δ∗and a larger space in which we
cannot infer any beneﬁt of our algorithm over vanilla greedy core-sets. The decay of the lower
bound close to the origin was also expected, since the space between δz and δ rapidly diminishes
into 0 when δ shrinks, which does not allow much opportunity for an unlabelled point with 0 error
to appear."
DISCUSSION,0.4889867841409692,"5
DISCUSSION"
DISCUSSION,0.4933920704845815,"Sener & Savarese (2018) suspected the potential of incorporating uncertainty information to improve
core-sets for active learning and we successfully conﬁrm this in our implementation of greedy core-
set search on doubt-scaled distances. Our ablation studies show that doubt-scaling is critical for fast
core-set loss convergence while beam search for the core-sets with low overall conﬁdence stabilizes
acquisition variance. Assuming that doubt acts as a cheap but noisy estimate of the distance to the
nearest point with zero error, the theoretical results show that our empirical improvements is caused
by a probabilistic quadratic-minimization of δ improving the linear order from before."
DISCUSSION,0.4977973568281938,"The difference between core-set loss convergence of the ablation studies versus full experiments
on CIFAR10/100 and SVHN is explained by differences in their dataset and budget sizes. Larger
core-sets, whose quality consists of the diversity of the initial, uniformly-sampled dataset and future
acquisitions, have smaller δ because the maximal distance between any unlabelled point and its
nearest labelled point is naturally minimized as the goal of any core-set algorithm. Since δ-quadratic
convergence of core-set error can only occur when δ is sufﬁciently small (i.e., δ < 1/λc), it is
expected that our contributions improved the difference in core-set loss using the larger core-sets of
the full experiments. The smaller initial datasets and acquisitions of the ablation studies would have"
DISCUSSION,0.5022026431718062,Under review as a conference paper at ICLR 2022
DISCUSSION,0.5066079295154186,"large δ that may exceed the threshold-criteria for δ-quadratic convergence or even δ∗, for which we
will have no guarantee of improvement above vanilla greedy core-set search."
DISCUSSION,0.5110132158590308,"Our theoretical results also explain why the rates of performance improvement from our method
appear to diminish faster with more data. When the labelled set saturates its coverage of the full dis-
tribution, δ diminishes towards 0. The lower bound on the probability of δ-quadratic convergence
diminishes to 0 regardless of model conﬁdence in this region (see Figure 7), preventing us from rea-
soning about the beneﬁt of our algorithm. Intuitively, when the training set is sufﬁciently large and
varied such that it already covers the vast majority of the input distribution, conﬁdence estimations
may be too similar to distinguish a signal about δ from noise."
CONCLUSION,0.5154185022026432,"6
CONCLUSION"
CONCLUSION,0.5198237885462555,"Greedy core-set search in doubt-scaled space empirically and theoretically improves upon the origi-
nal algorithm in active learning. We show that the magnitude of improvement is greatest for datasets
that are not too small or already comprehensive of the input distribution, which maximizes the prob-
ability of quadratic convergence of core-set loss with respect to core-set radii minimization. Even in
cases where the performance of our contribution equates to that of the original core-set algorithm,
there is no additional computational cost."
CONCLUSION,0.5242290748898678,"The value of our algorithm is a strict improvement over the original core-set, such that active learning
performance improves with more labels. We suspect that our algorithm would most beneﬁt online,
large-scale active learning systems."
CONCLUSION,0.5286343612334802,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.5330396475770925,"7
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.5374449339207048,See supplementary code to replicate all results.
REFERENCES,0.5418502202643172,REFERENCES
REFERENCES,0.5462555066079295,"Jordan T. Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal. Deep
batch active learning by diverse, uncertain gradient lower bounds. International Conference for
Learning Representations, 2020."
REFERENCES,0.5506607929515418,"William J. Cook, William H. Cunningham, William R. Pulleyblank, and Alexander Schrijver. Com-
binatorial optimization, volume 605. Springer, 1998."
REFERENCES,0.5550660792951542,"Melanie Ducoffe and Frederic Precioso. Adversarial active learning for deep networks: a margin
based approach. International Conference on Machine Learning, 2018."
REFERENCES,0.5594713656387665,"Diederik P. Kingma and Jimmy Ba. Adam: a method for stochastic optimization. International
Conference for Learning Representations, 2015."
REFERENCES,0.5638766519823789,"Andreas Kirsch, Joost van Amersfoort, and Yarin Gal. Batchbald: Efﬁcient and diverse batch acqui-
sition for deep bayesian active learning. Neural Information Processing Systems, 32:7026–7037,
2019."
REFERENCES,0.5682819383259912,"Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University
of Toronto, 2009."
REFERENCES,0.5726872246696035,"Lin Ma, Bailu Ding, Sudipto Das, and Adith Swaminathan.
Active learning for ML enhanced
database systems. ACM SIGMOID International Conference on Management of Data, pp. 175–
191, 2020."
REFERENCES,0.5770925110132159,"Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading
digits in natural images with unsupervised feature learning. NIPS Workshop on Deep Learning
and Unsupervised Feature Learning, 2011."
REFERENCES,0.5814977973568282,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style, High-Performance
Deep Learning Library. Curran Associates, Inc., 2019."
REFERENCES,0.5859030837004405,"Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set
approach. International Conference on Learning Representations, 2018."
REFERENCES,0.5903083700440529,"Burr Settles. Active learning literature survey. Technical Report Computer Sciences Technical
Report 1648, University of Wisconsin-Madison, 2009."
REFERENCES,0.5947136563876652,"Yanyao Shen, Hyokun Yun, Zachary C. Lipton, Yakov Kronrod, and Animashree Anandkumar.
Deep active learning for named entity recognition. International Conference on Learning Repre-
sentations, 2018."
REFERENCES,0.5991189427312775,"Samarth Sinha, Sayna Ebrahimi, and Trevor Darrell. Variational adversarial active learning. arXiv
preprint:1904.00370, 2019."
REFERENCES,0.6035242290748899,Under review as a conference paper at ICLR 2022
REFERENCES,0.6079295154185022,"A
APPENDIX"
REFERENCES,0.6123348017621145,"A.1
ROUGH WORK FOR CLAIMS"
REFERENCES,0.6167400881057269,Claim 1.
REFERENCES,0.6211453744493393,β ≥1 −(λcλϵ)δ−r0
REFERENCES,0.6255506607929515,"exp(δ −r0)2  δδ rr0
0"
REFERENCES,0.6299559471365639,"2
(13)"
REFERENCES,0.6343612334801763,Proof.
REFERENCES,0.6387665198237885,"β = 1 − δ
Y"
REFERENCES,0.6431718061674009,"r0
Perror(r)dr = 1 −exp Z δ"
REFERENCES,0.6475770925110133,"r0
ln Perror(r)dr ! (14)"
REFERENCES,0.6519823788546255,≥1 −exp Z δ
REFERENCES,0.6563876651982379,"r0
ln λcλϵr2dr !"
REFERENCES,0.6607929515418502,"Lower bound from Eq. 9
(15)"
REFERENCES,0.6651982378854625,"= 1 −exp

r ln(λcλϵr2) −2r

δ r0"
REFERENCES,0.6696035242290749,"
(see Appendix A.1: Claim 2) (16)"
REFERENCES,0.6740088105726872,= 1 −(λcλϵ)δ−r0
REFERENCES,0.6784140969162996,"exp(δ −r0)2  δδ rr0
0"
REFERENCES,0.6828193832599119,"2
(see Appendix A.1: Claim 3) (17)"
REFERENCES,0.6872246696035242,"Claim 2.
Z
ln(ax2) dx = x ln(ax2) −2x + C"
REFERENCES,0.6916299559471366,Proof. Use integration by parts. Let:
REFERENCES,0.6960352422907489,f(x) = ax2
REFERENCES,0.7004405286343612,f ′(x) = 2ax
REFERENCES,0.7048458149779736,u = ln f(x)
REFERENCES,0.7092511013215859,du = f ′(x)/f(x)dx
REFERENCES,0.7136563876651982,"dv = dx
v = x"
REFERENCES,0.7180616740088106,"Also note:
x f ′(x)"
REFERENCES,0.7224669603524229,"f(x)
= 2ax2"
REFERENCES,0.7268722466960352,ax2 = 2
REFERENCES,0.7312775330396476,"So the original problem can be integrated by parts:
Z
ln(ax2)dx =
Z
u dv"
REFERENCES,0.73568281938326,"= uv −
Z
v du"
REFERENCES,0.7400881057268722,"= x ln f(x) −
Z x f ′(x)"
REFERENCES,0.7444933920704846,f(x) dx
REFERENCES,0.748898678414097,= x ln(ax2) −2x + C
REFERENCES,0.7533039647577092,Claim 3.
REFERENCES,0.7577092511013216,"exp

r ln(λcλϵr2) −2r

δ r0"
REFERENCES,0.762114537444934,"
= (λcλϵ)δ−r0"
REFERENCES,0.7665198237885462,"exp(δ −r0)2  δδ rr0
0 2"
REFERENCES,0.7709251101321586,Under review as a conference paper at ICLR 2022
REFERENCES,0.775330396475771,Proof.
REFERENCES,0.7797356828193832,"exp

r ln(λcλϵr2) −2r

δ r0"
REFERENCES,0.7841409691629956,"
= exp
 
δ ln(λcλϵδ2) −r0 ln(λcλϵr2
0) −2δ + 2r0
"
REFERENCES,0.788546255506608,"= (λcλϵδ2)δ(λcλϵr2
0)−r0 exp (−2(δ −r0))"
REFERENCES,0.7929515418502202,= (λcλϵ)δ
REFERENCES,0.7973568281938326,"(λcλϵ)r0
δ2δ"
REFERENCES,0.801762114537445,"r2r0
0 1"
REFERENCES,0.8061674008810573,exp (δ −r0)2
REFERENCES,0.8105726872246696,= (λcλϵ)δ−r0
REFERENCES,0.8149779735682819,"exp(δ −r0)2  δδ rr0
0 2"
REFERENCES,0.8193832599118943,Claim 4.
REFERENCES,0.8237885462555066,(λcλϵ)δ(1−z)
REFERENCES,0.8281938325991189,"exp(1 −z)2δ 
δδ"
REFERENCES,0.8325991189427313,"δδzzδz 2
="
REFERENCES,0.8370044052863436,δ√λcλϵ e
REFERENCES,0.8414096916299559,1−z 1 zz !2δ
REFERENCES,0.8458149779735683,Proof.
REFERENCES,0.8502202643171806,(λcλϵ)δ(1−z)
REFERENCES,0.8546255506607929,"exp(1 −z)2δ 
δδ"
REFERENCES,0.8590308370044053,δδzzδz
REFERENCES,0.8634361233480177,"2
=
√λcλϵ e"
REFERENCES,0.8678414096916299,2δ(1−z) δδ(1−z) zδz 2
REFERENCES,0.8722466960352423,"=
δ√λcλϵ e"
REFERENCES,0.8766519823788547,"2δ(1−z)
1
z2δz ="
REFERENCES,0.8810572687224669,δ√λcλϵ e
REFERENCES,0.8854625550660793,1−z 1 zz !2δ
REFERENCES,0.8898678414096917,"A.2
NEGATIVE RESULT: PROBABILISTIC CORE-SETS"
REFERENCES,0.8942731277533039,"Instead of using a deterministic algorithm to compute core-sets, we score random batches on their
likelihood of being a subset of the optimal core-set. The goal is for the concatenation of the best-
scoring batches with the training set to result in a set of elements that are spread out on the feature
space and occur in dense regions, minimizing δ by deﬁnition."
REFERENCES,0.8986784140969163,"Suppose we are interested in classifying whether a real number is positive or not. Figure 8 shows the
unlabelled data and the result of learning a Gaussian mixture model (GMM) over the features. We
then use the trained GMM to estimate feature probabilities, which are required for computing mod-
iﬁed batch-BALD scores (see Appendix A.3). Figure 9 shows that higher scores indicate features
that are likely spread apart. For random batches sampled from this toy dataset, Figure 10 shows that
the distribution of scores form a long right tail that contains the most likely core-set centers."
REFERENCES,0.9030837004405287,"Figure 8: Left: collected data with one feature being the value along the real axis. Middle: the true distribution
of the input features. Right: Gaussian mixture with 32 components ﬁt to the collected data. We purposefully
overﬁt the GMM because the distribution of features is not known a priori and we would like high resolution
for computing joint information later."
REFERENCES,0.9074889867841409,Under review as a conference paper at ICLR 2022
REFERENCES,0.9118942731277533,"Figure 9: Given the trained Gaussian mixture model from Figure 8, we estimate the probability per component
for each element. We use the probabilities to compute a modiﬁed batch-BALD joint information score (M-
bBALD), which is positively correlated with entropy across the components. The batches with the highest
scores occur at dense regions but are spread out across the feature distribution. We want to avoid redundant
labelling of elements in the left column. Top row: each ﬁgure contains eight dotted lines that represent the
locations of elements in three different batches. Bottom row: corresponding probabilities per GMM component
for each element."
REFERENCES,0.9162995594713657,"Figure 10: The distribution of modiﬁed batch-BALD scores for randomly sampled batches have a long right
tail from which we mine for likely core-set elements. Range of scores depends on the number of components
in the Gaussian mixture model."
REFERENCES,0.920704845814978,Under review as a conference paper at ICLR 2022
REFERENCES,0.9251101321585903,"In the subsequent iterations, we ﬁrst construct a new unlabelled data pool that contains features
that have low probability to have appeared in the labelled pool, according to a ﬁtted GMM on the
labelled pool. Then, we ﬁt a new GMM on this modiﬁed unlabelled pool and repeat the selection
algorithm to search for the batch with the highest modiﬁed batch-BALD score when combined with
the existing training data. We also experiment with interpolating the modiﬁed batch-BALD score
with the least conﬁdence acquisition metric."
REFERENCES,0.9295154185022027,"We plot test accuracy versus number of labelled points for random acquisition (random), maxi-
mum entropy (max-entropy), least certainty (min-max-probs), probabilistic core-set (probabilistic-
coreset) and an exploitative version of probabilistic core-set that interpolates with least certainty
at a 9:1 ratio (probabilistic-coreset-exploitive-0.1). Figures 11 and 12 show that there is modest
improvement of the core-set variants from the random baseline in both toy datasets, although its
signiﬁcance is unknown. The entropy and least certainty methods performed poorly."
REFERENCES,0.933920704845815,"Figure 11: In the toy experiment with 0.5 standard deviation, clusters were mostly separable and core-set
variants dominated all baselines. Shaded area represents one standard deviation."
REFERENCES,0.9383259911894273,"Figure 13 shows examples of data points acquired in the toy experiments by probabilistic core-set
versus the points evaluated to be informative by maximum entropy (Figure 14) and least conﬁdence
(Figure 15). Whereas the core-set variants prioritized covering the input space, the uncertainty-based
methods focused on areas of overlapping clusters, which are prone to error and hard to classify."
REFERENCES,0.9427312775330396,Figure 13: Batch-BALD effectively maximized the distance between elements of selected batches.
REFERENCES,0.947136563876652,Under review as a conference paper at ICLR 2022
REFERENCES,0.9515418502202643,"Figure 12: In the toy experiment with 1 standard deviation, clusters overlapped substantially and probabilistic
core-set methods formed a modest upper bound in accuracy over all baselines. Shaded area represents one
standard deviation."
REFERENCES,0.9559471365638766,Figure 14: Maximizing entropy resulted in concentrated sampling in the most uncertain regions.
REFERENCES,0.960352422907489,Figure 15: Least conﬁdence also concentrates sampling along uncertain regions.
REFERENCES,0.9647577092511013,"The poor performance of entropy and uncertainty methods for large batch acquisition in a noisy
classiﬁcation dataset agrees with existing work Settles (2009); Kirsch et al. (2019); Sener & Savarese
(2018). The cause of this is wasteful labelling requests in uncertain regions of features that turned
out to be inseparable. In contrast, core-set variants and random acquisition are successful because
they covered the majority of the input space."
REFERENCES,0.9691629955947136,"The effect of increasingly difﬁcult separability on acquisition function efﬁciency is clear in the toy
data with 0.5 versus 1 standard deviation. When multiple class distributions overlap substantially,
their joint distribution density is sampled more frequently under the core-set variants, which is harm-"
REFERENCES,0.973568281938326,Under review as a conference paper at ICLR 2022
REFERENCES,0.9779735682819384,"ful because those samples do not improve test accuracy for noisy class boundaries. This suggests
that class inseparability may play some role in the poor performance of the core-set variants."
REFERENCES,0.9823788546255506,"Overall, probabilistic core-sets barely improved from random acquisitions and cost more computa-
tion than Algorithm 2. Like Sener and Savarese Sener & Savarese (2018), we also conclude with
the belief that any method that depends on distributional density sampling will have difﬁculty ex-
ceeding random sampling at an unknown test because of the obvious fact that i.i.d. samples are
already well-represented in the target distribution. Then, the main beneﬁcial effect of these density
sampling techniques is to reduce redundancy, but this may be a rare phenomenon in the typical high
dimensional representations of under-determined and nonlinear classiﬁcation tasks."
REFERENCES,0.986784140969163,"A.3
BATCH-BALD EVALUATES THE MUTUAL INFORMATION OF BATCHES OF DATA"
REFERENCES,0.9911894273127754,"Given a distribution of model parameters, Bayesian active learning by disagreement (BALD) evalu-
ates the information of a single data point as its marginal entropy penalized with the average entropy
across the parameter distribution Kirsch et al. (2019). Intuitively, this selects for samples that elicit
low overall certainty from the Bayesian model, but high individual certainty from the competing
hypotheses sampled from its parameter distribution. Naive application of BALD to a batch of data
may lead to the overestimation of mutual information between elements within the batch Kirsch
et al. (2019). On the other hand, Batch-BALD scores their joint information Kirsch et al. (2019)."
REFERENCES,0.9955947136563876,"The Batch-BALD information metric is useful for identifying likely and different core-set centers in
two important but different ways from its original setting. First, we ﬁt a GMM and sample its means
θ from P(θ), which we assume to be uniform. We use these Gaussian means to estimate P(y|x, θ).
Second, since there may exist multiple means that cover the same peak, optimizing for batch BALD
identiﬁes peaks with high overall certainty that have low likelihood of intersecting with other peaks."
