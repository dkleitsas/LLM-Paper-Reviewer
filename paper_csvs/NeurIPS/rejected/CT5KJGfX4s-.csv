Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002320185614849188,"While a broad range of techniques have been proposed to tackle distribution shift,
1"
ABSTRACT,0.004640371229698376,"the simple baseline of training on an undersampled dataset often achieves close
2"
ABSTRACT,0.0069605568445475635,"to state-of-the-art-accuracy across several popular benchmarks. This is rather
3"
ABSTRACT,0.009280742459396751,"surprising, since undersampling algorithms discard excess majority group data. To
4"
ABSTRACT,0.01160092807424594,"understand this phenomenon, we ask if learning is fundamentally constrained by a
5"
ABSTRACT,0.013921113689095127,"lack of minority group samples. We prove that this is indeed the case in the setting
6"
ABSTRACT,0.016241299303944315,"of nonparametric binary classiﬁcation. Our results show that in the worst case,
7"
ABSTRACT,0.018561484918793503,"an algorithm cannot outperform undersampling unless there is a high degree of
8"
ABSTRACT,0.02088167053364269,"overlap between the train and test distributions (which is unlikely to be the case
9"
ABSTRACT,0.02320185614849188,"in real-world datasets), or if the algorithm leverages additional structure about
10"
ABSTRACT,0.025522041763341066,"the distribution shift. In particular, in the case of label shift we show that there is
11"
ABSTRACT,0.027842227378190254,"always an undersampling algorithm that is minimax optimal. While in the case
12"
ABSTRACT,0.030162412993039442,"of group-covariate shift we show that there is an undersampling algorithm that is
13"
ABSTRACT,0.03248259860788863,"minimax optimal when the overlap between the group distributions is small. We
14"
ABSTRACT,0.03480278422273782,"also perform an experimental case study on a label shift dataset and ﬁnd that in line
15"
ABSTRACT,0.037122969837587005,"with our theory the test accuracy of robust neural network classiﬁers is constrained
16"
ABSTRACT,0.03944315545243619,"by the number of minority samples.
17"
INTRODUCTION,0.04176334106728538,"1
Introduction
18"
INTRODUCTION,0.04408352668213457,"A key challenge facing the machine learning community is to design models that are robust to
19"
INTRODUCTION,0.04640371229698376,"distribution shift. When there is a mismatch between the train and test distributions, current models
20"
INTRODUCTION,0.048723897911832945,"are often brittle and perform poorly on rare examples [11, 2, 20, 10, 1]. In this paper, our focus is on
21"
INTRODUCTION,0.05104408352668213,"group-structured distribution shifts. In the training set, we have many samples from a majority group
22"
INTRODUCTION,0.05336426914153132,"and relatively few samples from the minority group, while during test time we are equally likely to
23"
INTRODUCTION,0.05568445475638051,"get a sample from either group.
24"
INTRODUCTION,0.058004640371229696,"To tackle such distribution shifts, a naïve algorithm is one that ﬁrst undersamples the training data by
25"
INTRODUCTION,0.060324825986078884,"discarding excess majority group samples [14, 23] and then trains a model on this resulting dataset (see
26"
INTRODUCTION,0.06264501160092807,"Figure 1 for an illustration of this algorithm). The samples that remain in this undersampled dataset
27"
INTRODUCTION,0.06496519721577726,"constitute i.i.d. draws from the test distribution. Therefore, while a classiﬁer trained on this pruned
28"
INTRODUCTION,0.06728538283062645,"dataset cannot suffer biases due to distribution shift, this algorithm is clearly wasteful, as it discards
29"
INTRODUCTION,0.06960556844547564,"training samples.
30"
INTRODUCTION,0.07192575406032482,"No Undersampling
Undersampling"
INTRODUCTION,0.07424593967517401,"Learned Boundary
Predicted Majority"
INTRODUCTION,0.0765661252900232,"Predicted Minority
Majority Samples"
INTRODUCTION,0.07888631090487239,"Minority Samples
True Boundary"
INTRODUCTION,0.08120649651972157,Dropped Samples
INTRODUCTION,0.08352668213457076,"Figure 1: Example with linear models and linearly separable data. On the left we have the maximum
margin classiﬁer over the entire dataset, while on the right we have the maximum margin classiﬁer
over the undersampled dataset. The undersampled classiﬁer is less biased and aligns more closely
with the true boundary."
INTRODUCTION,0.08584686774941995,"This perceived inefﬁciency of undersampling has led to the design of several algorithms to combat
31"
INTRODUCTION,0.08816705336426914,"such distribution shift [6, 15, 18, 5, 17, 29, 13, 24]. In spite of this algorithmic progress, the simple
32"
INTRODUCTION,0.09048723897911833,"baseline of training models on an undersampled dataset remains competitive. In the case of label
33"
INTRODUCTION,0.09280742459396751,"shift, where one class label is overrepresented in the training data, this has been observed by Cui et al.
34"
INTRODUCTION,0.0951276102088167,"[7], Cao et al. [5], and Yang and Xu [28]. While in the case of group-covariate shift, a study by Idrissi
35"
INTRODUCTION,0.09744779582366589,"et al. [12] showed that the empirical effectiveness of these more complicated algorithms is limited.
36"
INTRODUCTION,0.09976798143851508,"For example, Idrissi et al. [12] showed that on the group-covariate shift CelebA dataset the worst-
37"
INTRODUCTION,0.10208816705336426,"group accuracy of a ResNet-50 model on the undersampled CelebA dataset which discards 97% of the
38"
INTRODUCTION,0.10440835266821345,"available training data is as good as methods that use all of available data such as importance-weighted
39"
INTRODUCTION,0.10672853828306264,"ERM [19], Group-DRO [18] and Just-Train-Twice [16]. In Table 1, we report the performance of
40"
INTRODUCTION,0.10904872389791183,"the undersampled classiﬁer compared to the state-of-the-art-methods in the literature across several
41"
INTRODUCTION,0.11136890951276102,"label shift and group-covariate shift datasets. We ﬁnd that, although undersampling isn’t always
42"
INTRODUCTION,0.1136890951276102,"the optimal robustness algorithm, it is typically a very competitive baseline and within 1–4% the
43"
INTRODUCTION,0.11600928074245939,"performance of the best method.
44"
INTRODUCTION,0.11832946635730858,"Table 1: Performance of undersampled classiﬁer compared to the best classiﬁer across several popular
label shift and group-covariate shift datasets. When reporting worst-group accuracy we denote it by a
⋆. When available, we report the 95% conﬁdence interval. We ﬁnd that the undersampled classiﬁer is
always within 1–4% of the best performing robustness algorithm, except on the MultiNLI dataset."
INTRODUCTION,0.12064965197215777,"Shift Type
Dataset/Paper
Test Accuracy/Worst-Group Accuracy⋆"
INTRODUCTION,0.12296983758700696,"Best
Undersampled"
INTRODUCTION,0.12529002320185614,"Label
Imb. CIFAR10 (step 10) [5]
87.81
84.59"
INTRODUCTION,0.12761020881670534,"Imb. CIFAR100 (step 10) [5]
58.71
55.06"
INTRODUCTION,0.12993039443155452,"CelebA [12]
86.9 ± 1.1⋆
85.6 ± 2.3⋆"
INTRODUCTION,0.13225058004640372,"Waterbirds [12]
87.6 ± 1.6⋆
89.1 ± 1.1⋆
Group-Covariate
MultiNLI [12]
78.0 ± 0.7⋆
68.9 ± 0.8⋆"
INTRODUCTION,0.1345707656612529,"CivilComments [12]
72.0 ± 1.9⋆
71.8 ± 1.4⋆"
INTRODUCTION,0.1368909512761021,"Inspired by the strong performance of undersampling in these experiments, we ask:
45"
INTRODUCTION,0.13921113689095127,"Is the performance of a model under distribution shift fundamentally
46"
INTRODUCTION,0.14153132250580047,"constrained by the lack of minority group samples?
47"
INTRODUCTION,0.14385150812064965,"To answer this question we analyze the minimax excess risk. We lower bound the minimax excess risk
48"
INTRODUCTION,0.14617169373549885,"to prove that the performance of any algorithm is lower bounded only as a function of the minority
49"
INTRODUCTION,0.14849187935034802,"samples (nmin). This shows that even if a robust algorithm optimally trades off between the bias and
50"
INTRODUCTION,0.15081206496519722,"the variance, it is fundamentally constrained by the variance on the minority group which decreases
51"
INTRODUCTION,0.1531322505800464,"only with nmin.
52"
INTRODUCTION,0.1554524361948956,"For our study, we consider the well-studied setting of nonparametric binary classiﬁcation [21]. By
53"
INTRODUCTION,0.15777262180974477,"operating in this nonparametric regime we are able to study the properties of undersampling in rich
54"
INTRODUCTION,0.16009280742459397,"data distributions, but are able to circumvent the complications that arise due to the optimization
55"
INTRODUCTION,0.16241299303944315,"and implicit bias of parametric models. We explore two distribution shift scenarios: label shift and
56"
INTRODUCTION,0.16473317865429235,"group-covariate shift. Under label shift, one of the labels is overrepresented in the training data,
57"
INTRODUCTION,0.16705336426914152,"Ptrain(y = 1) ≥Ptrain(y = −1), whereas the test samples are equally likely to come from either
58"
INTRODUCTION,0.16937354988399073,"class. Here the class-conditional distribution P(x | y) is Lipschitz in x. Under group-covariate shift,
59"
INTRODUCTION,0.1716937354988399,"we have two groups {a, b} and in the training data we have more samples from the distribution Pa(x)
60"
INTRODUCTION,0.1740139211136891,"than from Pb(x). Whereas during test time, it is equiprobable to receive samples from either group.
61"
INTRODUCTION,0.17633410672853828,"In this case, the distribution P(y | x) is Lipschitz in x.
62"
INTRODUCTION,0.17865429234338748,"Our Contributions.
We show that in the label shift setting there is a fundamental constraint, and
63"
INTRODUCTION,0.18097447795823665,"that the minimax excess risk of any robust learning method is lower bounded by 1/nmin1/3. That is,
64"
INTRODUCTION,0.18329466357308585,"minority group samples fundamentally constrain performance under distribution shift. Furthermore,
65"
INTRODUCTION,0.18561484918793503,"by leveraging previous results about nonparametric density estimation [9] we show a matching upper
66"
INTRODUCTION,0.18793503480278423,"bound on the excess risk of a standard binning estimator trained on an undersampled dataset to
67"
INTRODUCTION,0.1902552204176334,"demonstrate that undersampling is optimal.
68"
INTRODUCTION,0.1925754060324826,"In the case of group-covariate shift, we show that when the overlap (deﬁned in terms of total variation
69"
INTRODUCTION,0.19489559164733178,"distance) between the group distribution Pa and Pb is small, a similar result holds and the minimax
70"
INTRODUCTION,0.19721577726218098,"excess risk of any robust learning algorithm is lower bounded by 1/nmin1/3. We show that this lower
71"
INTRODUCTION,0.19953596287703015,"bound is tight, by proving an upper bound on the excess risk of the binning estimator acting on the
72"
INTRODUCTION,0.20185614849187936,"undersampled dataset.
73"
INTRODUCTION,0.20417633410672853,"Finally, we experimentally show in a label shift dataset (Imbalanced Binary CIFAR10) that the
74"
INTRODUCTION,0.20649651972157773,"accuracy of popular classiﬁers generally follow the trends predicted by our theory. When the minority
75"
INTRODUCTION,0.2088167053364269,"samples are increased, the accuracy of these classiﬁers increases drastically, whereas when the number
76"
INTRODUCTION,0.2111368909512761,"of majority samples are increased the gains in the accuracy are marginal at best.
77"
INTRODUCTION,0.21345707656612528,"Taken together, our results underline the need to move beyond designing “general-purpose” robustness
78"
INTRODUCTION,0.21577726218097448,"algorithms (like importance-weighting [5, 17, 13, 24], g-DRO [18], JTT [16], SMOTE [6], etc.)
79"
INTRODUCTION,0.21809744779582366,"that are agnostic to the structure in the distribution shift. Our worst case analysis highlights that to
80"
INTRODUCTION,0.22041763341067286,"successfully beat undersampling, an algorithm must leverage additional structure in the distribution
81"
INTRODUCTION,0.22273781902552203,"shift.
82"
RELATED WORK,0.22505800464037123,"2
Related Work
83"
RELATED WORK,0.2273781902552204,"On several group-covariate shift benchmarks (CelebA, CivilComments, Waterbirds), Idrissi et al. [12]
84"
RELATED WORK,0.2296983758700696,"showed that training ResNet classiﬁers on an undersampled dataset either outperforms or performs
85"
RELATED WORK,0.23201856148491878,"as well as other popular reweighting methods like Group-DRO [18], reweighted ERM, and Just-
86"
RELATED WORK,0.23433874709976799,"Train-Twice [16]. They ﬁnd Group-DRO performs comparably to undersampling, while both tend to
87"
RELATED WORK,0.23665893271461716,"outperform methods that don’t utilize group information.
88"
RELATED WORK,0.23897911832946636,"One classic method to tackle distribution shift is importance weighting [19], which reweights the loss
89"
RELATED WORK,0.24129930394431554,"of the minority group samples to yield an unbiased estimate of the loss. However, recent work [3, 27]
90"
RELATED WORK,0.24361948955916474,"has demonstrated the ineffectiveness of such methods when applied to overparameterized neural
91"
RELATED WORK,0.2459396751740139,"networks. Many followup papers [5, 29, 17, 13, 24] have introduced methods that modify the loss
92"
RELATED WORK,0.2482598607888631,"function in various ways to address this. However, despite this progress undersampling remains a
93"
RELATED WORK,0.2505800464037123,"competitive alternative to these importance weighted classiﬁers.
94"
RELATED WORK,0.2529002320185615,"Our theory draws from the rich literature on non-parametric classiﬁcation [21]. Apart from borrowing
95"
RELATED WORK,0.2552204176334107,"this setting of nonparametric classiﬁcation, we also utilize upper bounds on the estimation error of
96"
RELATED WORK,0.25754060324825984,"the simple histogram estimator [9, 8] to prove our upper bounds in the label shift case. Finally, we
97"
RELATED WORK,0.25986078886310904,"note that to prove our minimax lower bounds we proceed by using the general recipe of reducing
98"
RELATED WORK,0.26218097447795824,"from estimation to testing [22, Chapter 15]. One difference from this standard framework is that our
99"
RELATED WORK,0.26450116009280744,"training samples shall be drawn from a different distribution than the test samples used to deﬁne the
100"
RELATED WORK,0.2668213457076566,"risk.
101"
SETTING,0.2691415313225058,"3
Setting
102"
SETTING,0.271461716937355,"In this section, we shall introduce our problem setup and deﬁne the types of distribution shift that we
103"
SETTING,0.2737819025522042,"consider.
104"
PROBLEM SETUP,0.27610208816705334,"3.1
Problem Setup
105"
PROBLEM SETUP,0.27842227378190254,"The setting for our study is nonparametric binary classiﬁcation with Lipschitz data distributions.
106"
PROBLEM SETUP,0.28074245939675174,"We are given n training datapoints S := {(x1, y1), . . . , (xn, yn)} ∈([0, 1] × {−1, 1})n that are all
107"
PROBLEM SETUP,0.28306264501160094,"drawn from a train distribution Ptrain. During test time, the data shall be drawn from a different
108"
PROBLEM SETUP,0.2853828306264501,"distribution Ptest. To present a clean analysis, we study the case where the features x are bounded
109"
PROBLEM SETUP,0.2877030162412993,"scalars, however, it is easy to extend our results to the high-dimensional setting.
110"
PROBLEM SETUP,0.2900232018561485,"Given a classiﬁer f : R →{−1, 1}, we shall be interested in the test error (risk) of this classiﬁer
111"
PROBLEM SETUP,0.2923433874709977,"under the test distribution Ptest:
112"
PROBLEM SETUP,0.29466357308584684,"R(f; Ptest) := E(x,y)∼Ptest [1(f(x) ̸= y)] ."
TYPES OF DISTRIBUTION SHIFT,0.29698375870069604,"3.2
Types of Distribution Shift
113"
TYPES OF DISTRIBUTION SHIFT,0.29930394431554525,"We assume that Ptrain consists of a mixture of two groups of unequal size, and Ptest contains equal
114"
TYPES OF DISTRIBUTION SHIFT,0.30162412993039445,"numbers of samples from both groups. Given a majority group distribution Pmaj and a minority group
115"
TYPES OF DISTRIBUTION SHIFT,0.3039443155452436,"distribution Pmin, the learner has access to nmaj majority group samples and nmin minority group
116"
TYPES OF DISTRIBUTION SHIFT,0.3062645011600928,"samples:
117"
TYPES OF DISTRIBUTION SHIFT,0.308584686774942,"Smaj ∼Pnmaj
maj
and
Smin ∼Pnmin
min ."
TYPES OF DISTRIBUTION SHIFT,0.3109048723897912,"Here nmaj > n/2 and nmin < n/2 with nmaj + nmin = n. The full training dataset is S =
118"
TYPES OF DISTRIBUTION SHIFT,0.31322505800464034,"Smaj ∪Smin = {(x1, y1), . . . , (xn, yn)}. We assume that the learner has access to the knowledge
119"
TYPES OF DISTRIBUTION SHIFT,0.31554524361948955,"whether a particular sample (xi, yi) comes from the majority or minority group.
120"
TYPES OF DISTRIBUTION SHIFT,0.31786542923433875,The test samples will be drawn from Ptest = 1
TYPES OF DISTRIBUTION SHIFT,0.32018561484918795,2Pmaj + 1
TYPES OF DISTRIBUTION SHIFT,0.3225058004640371,"2Pmin, a uniform mixture over Pmaj and Pmin.
121"
TYPES OF DISTRIBUTION SHIFT,0.3248259860788863,"Thus, the training dataset is an imbalanced draw from the distributions Pmaj and Pmin, whereas the
122"
TYPES OF DISTRIBUTION SHIFT,0.3271461716937355,"test samples are balanced draws. We let ρ := nmaj/nmin > 1 denote the imbalance ratio in the
123"
TYPES OF DISTRIBUTION SHIFT,0.3294663573085847,"training data.
124"
TYPES OF DISTRIBUTION SHIFT,0.33178654292343385,"We focus on two-types of distribution shifts: label shift and group-covariate shift that we describe
125"
TYPES OF DISTRIBUTION SHIFT,0.33410672853828305,"below.
126"
LABEL SHIFT,0.33642691415313225,"3.2.1
Label Shift
127"
LABEL SHIFT,0.33874709976798145,"In this setting, the imbalance in the training data comes from there being more samples from one
class over another. Without loss of generality, we shall assume that the class y = 1 is the majority
class. Then, we deﬁne the majority and the minority class distributions as"
LABEL SHIFT,0.34106728538283065,"Pmaj(x, y) = P1(x)1(y = 1)
and
Pmin = P−1(x)1(y = −1),"
LABEL SHIFT,0.3433874709976798,"where P1, P−1 are class-conditional distributions over the interval [0, 1]. We assume that class-
128"
LABEL SHIFT,0.345707656612529,"conditional distributions Pi have densities on [0, 1] and that they are 1-Lipschitz: for any x, x′ ∈[0, 1],
129"
LABEL SHIFT,0.3480278422273782,|Pi(x) −Pi(x′)| ≤|x −x′|.
LABEL SHIFT,0.3503480278422274,"We denote the class of pairs of distributions (Pmaj, Pmin) that satisfy these conditions by PLS.
130"
GROUP-COVARIATE SHIFT,0.35266821345707655,"3.2.2
Group-Covariate Shift
131"
GROUP-COVARIATE SHIFT,0.35498839907192575,"In this setting, we have two groups {a, b}, and corresponding to each of these groups is a distribution
(with densities) over the features Pa(x) and Pb(x). We let a correspond to the majority group and b
correspond to the minority group. Then, we deﬁne"
GROUP-COVARIATE SHIFT,0.35730858468677495,"Pmaj(x, y) = Pa(x)P(y | x)
and
Pmin(x, y) = Pb(x)P(y | x)."
GROUP-COVARIATE SHIFT,0.35962877030162416,"We assume that for y ∈{−1, 1}, for all x, x′ ∈[0, 1]:
132"
GROUP-COVARIATE SHIFT,0.3619489559164733,"P(y | x) −P(y | x′)
 ≤|x −x′|,"
GROUP-COVARIATE SHIFT,0.3642691415313225,"that is, the distribution of the label given the feature is 1-Lipschitz, and it varies slowly over the
133"
GROUP-COVARIATE SHIFT,0.3665893271461717,"domain.
134"
GROUP-COVARIATE SHIFT,0.3689095127610209,"To quantify the shift between the train and test distribution, we deﬁne a notion of overlap between the
135"
GROUP-COVARIATE SHIFT,0.37122969837587005,"group distributions Pa and Pb as follows:
136"
GROUP-COVARIATE SHIFT,0.37354988399071926,"Overlap(Pa, Pb) := 1 −TV(Pa, Pb)."
GROUP-COVARIATE SHIFT,0.37587006960556846,"Notice that when Pa and Pb have disjoint supports, TV(Pa, Pb) = 1 and therefore Overlap(Pa, Pb) =
137"
GROUP-COVARIATE SHIFT,0.37819025522041766,"0. On the other hand when Pa = Pb, TV(Pa, Pb) = 0 and Overlap(Pa, Pb) = 1. When the overlap
138"
GROUP-COVARIATE SHIFT,0.3805104408352668,"is 1, the majority and minority distributions are identical and hence we have no shift between train
139"
GROUP-COVARIATE SHIFT,0.382830626450116,"and test. Observe that Overlap(Pa, Pb) = Overlap(Pmaj, Pmin) since P(y | x) is shared across Pmaj
140"
GROUP-COVARIATE SHIFT,0.3851508120649652,"and Pmin.
141"
GROUP-COVARIATE SHIFT,0.3874709976798144,"Given a level of overlap τ ∈[0, 1] we denote the class of pairs of distributions (Pmaj, Pmin) with
142"
GROUP-COVARIATE SHIFT,0.38979118329466356,"overlap at least τ by PGS(τ). It is easy to check that, PGS(τ) ⊆PGS(0) at any overlap level τ ∈[0, 1].
143"
LOWER BOUNDS ON THE MINIMAX EXCESS RISK,0.39211136890951276,"4
Lower Bounds on the Minimax Excess Risk
144"
LOWER BOUNDS ON THE MINIMAX EXCESS RISK,0.39443155452436196,"In this section, we shall prove our lower bounds that show that the performance of any algorithm is
145"
LOWER BOUNDS ON THE MINIMAX EXCESS RISK,0.39675174013921116,"constrained by the number of minority samples nmin. Before we state our lower bounds, we need to
146"
LOWER BOUNDS ON THE MINIMAX EXCESS RISK,0.3990719257540603,"introduce the notion of excess risk and minimax excess risk.
147"
LOWER BOUNDS ON THE MINIMAX EXCESS RISK,0.4013921113689095,"Excess Risk and Minimax Excess Risk.
We measure the performance of an algorithm A through
148"
LOWER BOUNDS ON THE MINIMAX EXCESS RISK,0.4037122969837587,"its excess risk deﬁned in the following way. Given an algorithm A that takes as input a dataset S
149"
LOWER BOUNDS ON THE MINIMAX EXCESS RISK,0.4060324825986079,"and returns a classiﬁer AS, and a pair of distributions (Pmaj, Pmin) with Ptest = 1"
LOWER BOUNDS ON THE MINIMAX EXCESS RISK,0.40835266821345706,2Pmaj + 1
LOWER BOUNDS ON THE MINIMAX EXCESS RISK,0.41067285382830626,"2Pmin, the
150"
LOWER BOUNDS ON THE MINIMAX EXCESS RISK,0.41299303944315546,"expected excess risk is given by
151"
LOWER BOUNDS ON THE MINIMAX EXCESS RISK,0.41531322505800466,"Excess Risk[A; (Pmaj, Pmin)] := ES∼P
nmaj
maj ×P
nmin
min

R(AS; Ptest)) −R(f ⋆(Ptest); Ptest)

,
(1)"
LOWER BOUNDS ON THE MINIMAX EXCESS RISK,0.4176334106728538,"where f ⋆(Ptest) is the Bayes classiﬁer that minimizes the risk R(·; Ptest). The ﬁrst term corresponds
152"
LOWER BOUNDS ON THE MINIMAX EXCESS RISK,0.419953596287703,"to the expected risk for the algorithm when given nmaj samples from Pmaj and nmin samples from
153"
LOWER BOUNDS ON THE MINIMAX EXCESS RISK,0.4222737819025522,"Pmin, whereas the second term corresponds to the Bayes error for the problem.
154"
LOWER BOUNDS ON THE MINIMAX EXCESS RISK,0.4245939675174014,"Excess risk does not let us characterize the inherent difﬁculty of a problem, since for any particular
155"
LOWER BOUNDS ON THE MINIMAX EXCESS RISK,0.42691415313225056,"data distribution (Pmaj, Pmin) the best possible algorithm A to minimize the excess risk would be the
156"
LOWER BOUNDS ON THE MINIMAX EXCESS RISK,0.42923433874709976,"trivial mapping AS = f ⋆(Ptest). Therefore, to prove meaningful lower bounds on the performance
157"
LOWER BOUNDS ON THE MINIMAX EXCESS RISK,0.43155452436194897,"of algorithms we need to deﬁne the notion of minimax excess risk [see 22, Chapter 15]. Given a class
158"
LOWER BOUNDS ON THE MINIMAX EXCESS RISK,0.43387470997679817,"of pairs of distributions P deﬁne
159"
LOWER BOUNDS ON THE MINIMAX EXCESS RISK,0.4361948955916473,"Minimax Excess Risk(P) := inf
A
sup
(Pmaj,Pmin)∈P
Excess Risk[A; (Pmaj, Pmin)],
(2)"
LOWER BOUNDS ON THE MINIMAX EXCESS RISK,0.4385150812064965,"where the inﬁmum is over all measurable estimators A. The minimax excess risk is the excess risk of
160"
LOWER BOUNDS ON THE MINIMAX EXCESS RISK,0.4408352668213457,"the “best” algorithm in the worst case over the class of problems deﬁned by P.
161"
LABEL SHIFT LOWER BOUNDS,0.4431554524361949,"4.1
Label Shift Lower Bounds
162"
LABEL SHIFT LOWER BOUNDS,0.44547563805104406,"We demonstrate the hardness of the label shift problem in general by establishing a lower bound
163"
LABEL SHIFT LOWER BOUNDS,0.44779582366589327,"on the minimax excess risk. Below we let c > 0 be an absolute constant independent of problem
164"
LABEL SHIFT LOWER BOUNDS,0.45011600928074247,"parameters like nmaj and nmin.
165"
LABEL SHIFT LOWER BOUNDS,0.45243619489559167,"Theorem 4.1. Consider the label shift setting described in Section 3.2.1. Recall that PLS is the class
166"
LABEL SHIFT LOWER BOUNDS,0.4547563805104408,"of pairs of distributions (Pmaj, Pmin) that satisfy the assumptions in that section. The minimax excess
167"
LABEL SHIFT LOWER BOUNDS,0.45707656612529,"risk over this class is lower bounded as follows:
168"
LABEL SHIFT LOWER BOUNDS,0.4593967517401392,"Minimax Excess Risk(PLS) = inf
A
sup
(Pmaj,Pmin)∈PLS
Excess Risk[A; (Pmaj, Pmin)] ≥
c
nmin1/3 .
(3)"
LABEL SHIFT LOWER BOUNDS,0.4617169373549884,"We establish this result in Appendix B.
169"
LABEL SHIFT LOWER BOUNDS,0.46403712296983757,"We show that rather surprisingly, the lower bound on the minimax excess risk scales only with the
170"
LABEL SHIFT LOWER BOUNDS,0.46635730858468677,"number of minority class samples nmin1/3, and does not depend on nmaj. Intuitively, this is because
171"
LABEL SHIFT LOWER BOUNDS,0.46867749419953597,"any learner must predict which class-conditional distribution (P(x | 1) or P(x | −1)) assigns higher
172"
LABEL SHIFT LOWER BOUNDS,0.4709976798143852,"likelihood at that x. To interpret this result, consider the extreme scenario where nmaj →∞but nmin
173"
LABEL SHIFT LOWER BOUNDS,0.4733178654292343,"is ﬁnite. In this case, the learner has full information about the majority class distribution. However,
174"
LABEL SHIFT LOWER BOUNDS,0.4756380510440835,"the learning task continues to be challenging since any learner would be uncertain about whether
175"
LABEL SHIFT LOWER BOUNDS,0.4779582366589327,"the minority class distribution assigns higher or lower likelihood at any given x. This uncertainty
176"
LABEL SHIFT LOWER BOUNDS,0.4802784222737819,"underlies the reason why the minimax rate of classiﬁcation is constrained by the number of minority
177"
LABEL SHIFT LOWER BOUNDS,0.48259860788863107,"samples nmin.
178"
LABEL SHIFT LOWER BOUNDS,0.48491879350348027,"We also note that the theorem can be trivially extended to higher dimensions. In this case the
179"
LABEL SHIFT LOWER BOUNDS,0.4872389791183295,"exponents degrade to 1/3d rather than 1/3 as is to be expected in nonparametric classiﬁcation.
180"
GROUP-COVARIATE SHIFT LOWER BOUNDS,0.4895591647331787,"4.2
Group-Covariate Shift Lower Bounds
181"
GROUP-COVARIATE SHIFT LOWER BOUNDS,0.4918793503480278,"Next, we shall state our lower bound on the minimax excess risk that demonstrates the hardness of the
182"
GROUP-COVARIATE SHIFT LOWER BOUNDS,0.494199535962877,"group-covariate shift problem. In the theorem below c > 0 shall be an absolute constant independent
183"
GROUP-COVARIATE SHIFT LOWER BOUNDS,0.4965197215777262,"of nmaj, nmin and τ.
184"
GROUP-COVARIATE SHIFT LOWER BOUNDS,0.4988399071925754,"Theorem 4.2. Consider the group shift setting described in Section 3.2.2. Given any overlap
185"
GROUP-COVARIATE SHIFT LOWER BOUNDS,0.5011600928074246,"τ ∈[0, 1] recall that PGS(τ) is the class of distributions such that Overlap(Pmaj, Pmin) ≥τ. The
186"
GROUP-COVARIATE SHIFT LOWER BOUNDS,0.5034802784222738,"minimax excess risk in this setting is lower bounded as follows:
187"
GROUP-COVARIATE SHIFT LOWER BOUNDS,0.505800464037123,"Minimax Excess Risk(PGS(τ)) = inf
A
sup
(Pmaj,Pmin)∈PGS(τ)
Excess Risk[A; (Pmaj, Pmin)]"
GROUP-COVARIATE SHIFT LOWER BOUNDS,0.5081206496519721,"≥
c
(nmin · (2 −τ) + nmaj · τ)1/3 ≥
c
nmin1/3(ρ · τ + 2)1/3 ,
(4)"
GROUP-COVARIATE SHIFT LOWER BOUNDS,0.5104408352668214,"where ρ = nmaj/nmin > 1.
188"
GROUP-COVARIATE SHIFT LOWER BOUNDS,0.5127610208816705,"We prove this theorem in Appendix C.
189"
GROUP-COVARIATE SHIFT LOWER BOUNDS,0.5150812064965197,"We see that in the low overlap setting (τ ≪1/ρ), the minimax excess risk is lower bounded by
190"
GROUP-COVARIATE SHIFT LOWER BOUNDS,0.5174013921113689,"1/nmin1/3, and we are fundamentally constrained by the number of samples in minority group. To
191"
GROUP-COVARIATE SHIFT LOWER BOUNDS,0.5197215777262181,"see why this is the case, consider the extreme example with τ = 0 where Pa has support [0, 0.5]
192"
GROUP-COVARIATE SHIFT LOWER BOUNDS,0.5220417633410673,"and Pb has support [0.5, 1]. The nmaj majority group samples from Pa provide information about
193"
GROUP-COVARIATE SHIFT LOWER BOUNDS,0.5243619489559165,"the correct label predict in the interval [0, 0.5] (the support of Pa). However, since the distribution
194"
GROUP-COVARIATE SHIFT LOWER BOUNDS,0.5266821345707656,"P(y | x) is 1-Lipschitz in the worst case these samples provide very limited information about the
195"
GROUP-COVARIATE SHIFT LOWER BOUNDS,0.5290023201856149,"correct predictions in [0.5, 1] (the support of Pb). Thus, predicting on the support of Pb requires
196"
GROUP-COVARIATE SHIFT LOWER BOUNDS,0.531322505800464,"samples from the minority group and this results in the nmin dependent rate. In fact, in this extreme
197"
GROUP-COVARIATE SHIFT LOWER BOUNDS,0.5336426914153132,"case (τ = 0) even if nmaj →∞, the minimax excess risk is still bounded away from zero. This
198"
GROUP-COVARIATE SHIFT LOWER BOUNDS,0.5359628770301624,"intuition also carries over to the case when the overlap is small but non-zero and our lower bound
199"
GROUP-COVARIATE SHIFT LOWER BOUNDS,0.5382830626450116,"shows that minority samples are much more valuable than majority samples at reducing the risk.
200"
GROUP-COVARIATE SHIFT LOWER BOUNDS,0.5406032482598608,"On the other hand, when the overlap is high (τ ≫1/ρ) the minimax excess risk is lower bounded
201"
GROUP-COVARIATE SHIFT LOWER BOUNDS,0.54292343387471,"by 1/(nmin(2 −τ) + nmajτ)1/3 and the extra majority samples are quite beneﬁcial. This is roughly
202"
GROUP-COVARIATE SHIFT LOWER BOUNDS,0.5452436194895591,"because the supports of Pa and Pb have large overlap and hence samples from the majority group
203"
GROUP-COVARIATE SHIFT LOWER BOUNDS,0.5475638051044084,"are useful in helping make predictions even in regions where Pb is large. In the extreme case when
204"
GROUP-COVARIATE SHIFT LOWER BOUNDS,0.5498839907192575,"τ = 1, we have that Pa = Pb and therefore recover the classic i.i.d. setting with no distribution shift.
205"
GROUP-COVARIATE SHIFT LOWER BOUNDS,0.5522041763341067,"Here, the lower bound scales with 1/n1/3, as one might expect.
206"
GROUP-COVARIATE SHIFT LOWER BOUNDS,0.5545243619489559,"Identical to the label shift case, the theorem can be extended to hold in higher dimensions with the
207"
GROUP-COVARIATE SHIFT LOWER BOUNDS,0.5568445475638051,"exponents being 1/3d rather than 1/3.
208"
UPPER BOUNDS ON THE EXCESS RISK FOR THE UNDERSAMPLED BINNING,0.5591647331786543,"5
Upper Bounds on the Excess Risk for the Undersampled Binning
209"
UPPER BOUNDS ON THE EXCESS RISK FOR THE UNDERSAMPLED BINNING,0.5614849187935035,"Estimator
210"
UPPER BOUNDS ON THE EXCESS RISK FOR THE UNDERSAMPLED BINNING,0.5638051044083526,"We will show that an undersampled estimator matches the rates in the previous section showing
211"
UPPER BOUNDS ON THE EXCESS RISK FOR THE UNDERSAMPLED BINNING,0.5661252900232019,"that undersampling is an optimal robustness intervention. We start by deﬁning the undersampling
212"
UPPER BOUNDS ON THE EXCESS RISK FOR THE UNDERSAMPLED BINNING,0.568445475638051,"procedure and the undersampling binning estimator.
213"
UPPER BOUNDS ON THE EXCESS RISK FOR THE UNDERSAMPLED BINNING,0.5707656612529002,"Undersampling Procedure.
Given training data S := {(x1, y1), . . . , (xn, yn)}, generate a new
214"
UPPER BOUNDS ON THE EXCESS RISK FOR THE UNDERSAMPLED BINNING,0.5730858468677494,"undersampled dataset SUS by
215"
UPPER BOUNDS ON THE EXCESS RISK FOR THE UNDERSAMPLED BINNING,0.5754060324825986,"• including all nmin samples from Smin and,
216"
UPPER BOUNDS ON THE EXCESS RISK FOR THE UNDERSAMPLED BINNING,0.5777262180974478,"• including nmin samples from Smaj by sampling uniformly at random without replacement.
217"
UPPER BOUNDS ON THE EXCESS RISK FOR THE UNDERSAMPLED BINNING,0.580046403712297,"This procedure ensures that in the undersampled dataset SUS, the groups are balanced, and that
218"
UPPER BOUNDS ON THE EXCESS RISK FOR THE UNDERSAMPLED BINNING,0.5823665893271461,"|SUS| = 2nmin.
219"
UPPER BOUNDS ON THE EXCESS RISK FOR THE UNDERSAMPLED BINNING,0.5846867749419954,"The undersampling binning estimator deﬁned next will ﬁrst run this undersampling procedure to
220"
UPPER BOUNDS ON THE EXCESS RISK FOR THE UNDERSAMPLED BINNING,0.5870069605568445,"obtain SUS and just uses these samples to output a classiﬁer.
221"
UPPER BOUNDS ON THE EXCESS RISK FOR THE UNDERSAMPLED BINNING,0.5893271461716937,"Undersampled Binning Estimator
The undersampled binning estimator AUSB takes as input a
222"
UPPER BOUNDS ON THE EXCESS RISK FOR THE UNDERSAMPLED BINNING,0.5916473317865429,"dataset S and a positive integer K corresponding to the number of bins, and returns a classiﬁer
223"
UPPER BOUNDS ON THE EXCESS RISK FOR THE UNDERSAMPLED BINNING,0.5939675174013921,"AS,K
USB : [0, 1] →{−1, 1}. This estimator is deﬁned as follows:
224"
UPPER BOUNDS ON THE EXCESS RISK FOR THE UNDERSAMPLED BINNING,0.5962877030162413,"1. First, we compute the undersampled dataset SUS.
225"
UPPER BOUNDS ON THE EXCESS RISK FOR THE UNDERSAMPLED BINNING,0.5986078886310905,"2. Given this dataset SUS, let n1,j be the number of points with label +1 that lie in the interval
226"
UPPER BOUNDS ON THE EXCESS RISK FOR THE UNDERSAMPLED BINNING,0.6009280742459396,"Ij = [ j−1 K , j"
UPPER BOUNDS ON THE EXCESS RISK FOR THE UNDERSAMPLED BINNING,0.6032482598607889,"K ]. Also, deﬁne n−1,j analogously. Then set
227"
UPPER BOUNDS ON THE EXCESS RISK FOR THE UNDERSAMPLED BINNING,0.605568445475638,"Aj =
1
if n1,j > n−1,j,
−1
otherwise."
UPPER BOUNDS ON THE EXCESS RISK FOR THE UNDERSAMPLED BINNING,0.6078886310904872,"3. Deﬁne the classiﬁer AS,K
USB such that if x ∈Ij then
228"
UPPER BOUNDS ON THE EXCESS RISK FOR THE UNDERSAMPLED BINNING,0.6102088167053364,"AS,K
USB(x) = Aj.
(5)"
UPPER BOUNDS ON THE EXCESS RISK FOR THE UNDERSAMPLED BINNING,0.6125290023201856,"Essentially in each bin Ij, we set the prediction to be the majority label among the samples
229"
UPPER BOUNDS ON THE EXCESS RISK FOR THE UNDERSAMPLED BINNING,0.6148491879350348,"that fall in this bin.
230"
UPPER BOUNDS ON THE EXCESS RISK FOR THE UNDERSAMPLED BINNING,0.617169373549884,"Whenever the number of bins K is clear from the context we shall denote AS,K
USB by AS
USB. Below we
231"
UPPER BOUNDS ON THE EXCESS RISK FOR THE UNDERSAMPLED BINNING,0.6194895591647331,"establish upper bounds on the excess risk of this simple estimator.
232"
LABEL SHIFT UPPER BOUNDS,0.6218097447795824,"5.1
Label Shift Upper Bounds
233"
LABEL SHIFT UPPER BOUNDS,0.6241299303944315,"We now establish an upper bound on the excess risk of AUSB in the label shift setting (see Sec-
234"
LABEL SHIFT UPPER BOUNDS,0.6264501160092807,"tion 3.2.1). Below we let c, C > 0 be absolute constants independent of problem parameters like
235"
LABEL SHIFT UPPER BOUNDS,0.62877030162413,"nmaj and nmin.
236"
LABEL SHIFT UPPER BOUNDS,0.6310904872389791,"Theorem 5.1. Consider the label shift setting described in Section 3.2.1. For any (Pmaj, Pmin) ∈PLS
237"
LABEL SHIFT UPPER BOUNDS,0.6334106728538283,"the expected excess risk of the Undersampling Binning Estimator (Eq. (5)) with number of bins with
238"
LABEL SHIFT UPPER BOUNDS,0.6357308584686775,"K = c⌈nmin1/3⌉is upper bounded by
239"
LABEL SHIFT UPPER BOUNDS,0.6380510440835266,"Excess Risk[AUSB; (Pmaj, Pmin)] = ES∼P
nmaj
maj ×P
nmin
min

R(AS
USB; Ptest) −R(f ⋆; Ptest)

≤
C
nmin1/3 ."
LABEL SHIFT UPPER BOUNDS,0.6403712296983759,"We prove this result in Appendix B. This upper bound combined with the lower bound in Theorem 4.1
240"
LABEL SHIFT UPPER BOUNDS,0.642691415313225,"shows that an undersampling approach is minimax optimal up to constants in the presence of label
241"
LABEL SHIFT UPPER BOUNDS,0.6450116009280742,"shift.
242"
LABEL SHIFT UPPER BOUNDS,0.6473317865429234,"We note that our analysis leaves open the possibility of better algorithms when the learner has
243"
LABEL SHIFT UPPER BOUNDS,0.6496519721577726,"additional information about the structure of the label shift beyond Lipschitz continuity.
244"
GROUP-COVARIATE SHIFT UPPER BOUNDS,0.6519721577726219,"5.2
Group-Covariate Shift Upper Bounds
245"
GROUP-COVARIATE SHIFT UPPER BOUNDS,0.654292343387471,"Next, we present our upper bounds on the excess risk of the undersampled binning estimator in the
246"
GROUP-COVARIATE SHIFT UPPER BOUNDS,0.6566125290023201,"group-covariate shift setting (see Section 3.2.2). In the theorem below, C > 0 is an absolute constant
247"
GROUP-COVARIATE SHIFT UPPER BOUNDS,0.6589327146171694,"independent of the problem parameters nmaj, nmin and τ.
248"
GROUP-COVARIATE SHIFT UPPER BOUNDS,0.6612529002320185,"Theorem 5.2. Consider the group shift setting described in Section 3.2.2. For any overlap τ ∈[0, 1]
249"
GROUP-COVARIATE SHIFT UPPER BOUNDS,0.6635730858468677,"and for any (Pmaj, Pmin) ∈PGS(τ) the expected excess risk of the Undersampling Binning Estimator
250"
GROUP-COVARIATE SHIFT UPPER BOUNDS,0.665893271461717,"(Eq. (5)) with number of bins with K = ⌈nmin1/3⌉is
251"
GROUP-COVARIATE SHIFT UPPER BOUNDS,0.6682134570765661,"Excess Risk[AUSB; (Pmaj, Pmin)] = ES∼P
nmaj
maj ×P
nmin
min

R(AS
USB; Ptest)) −R(f ⋆; Ptest)

≤
C
nmin1/3 ."
GROUP-COVARIATE SHIFT UPPER BOUNDS,0.6705336426914154,"We provide a proof for this theorem in Appendix C. Compared to the lower bound established in
252"
GROUP-COVARIATE SHIFT UPPER BOUNDS,0.6728538283062645,"Theorem 4.2 which scales as 1/ ((2 −τ)nmin + nmajτ)1/3, the upper bound for the undersampled
253"
GROUP-COVARIATE SHIFT UPPER BOUNDS,0.6751740139211136,"binning estimator always scales with 1/nmin1/3 since it operates on the undersampled dataset (SUS).
254"
GROUP-COVARIATE SHIFT UPPER BOUNDS,0.6774941995359629,"Thus, we have shown that in the absence of overlap (τ ≪1/ρ = nmin/nmaj) there is an under-
255"
GROUP-COVARIATE SHIFT UPPER BOUNDS,0.679814385150812,"sampling algorithm that is minimax optimal up to constants. However when there is high overlap
256"
GROUP-COVARIATE SHIFT UPPER BOUNDS,0.6821345707656613,"(τ ≫1/ρ) there is a non-trivial gap between the upper and lower bounds:
257"
GROUP-COVARIATE SHIFT UPPER BOUNDS,0.6844547563805105,"Upper Bound
Lower Bound = c(ρ · τ + 2)1/3."
MINORITY SAMPLE DEPENDENCE IN PRACTICE,0.6867749419953596,"6
Minority Sample Dependence in Practice
258"
MINORITY SAMPLE DEPENDENCE IN PRACTICE,0.6890951276102089,"Figure 2: Convolutional neural network classiﬁers trained on the Imbalanced Binary CIFAR10 dataset
with a 5:1 label imbalance. (Top) Models trained using the importance weighted cross entropy loss
with early stopping. (Bottom) Models trained using the importance weighted VS loss [13] with early
stopping. We report the average test accuracy calculated on a balanced test set over 5 random seeds.
We start off with 2500 cat examples and 500 dog examples in the training dataset. We ﬁnd that in
accordance with our theory, for both of the classiﬁers adding only minority class samples (red) leads
to large gain in accuracy (∼6%), while adding majority class samples (blue) leads to little or no
gain. In fact, adding majority samples sometimes hurts test accuracy due to the added bias. When
we add majority and minority samples in a 5:1 ratio (green), the gain is largely due to the addition
of minority samples and is only marginally higher (< 2%) than adding only minority samples. The
green curves correspond to the same classiﬁers in both the left and right panels."
MINORITY SAMPLE DEPENDENCE IN PRACTICE,0.691415313225058,"Inspired by our worst-case theoretical predictions in nonparametric classiﬁcation, we ask: how does
259"
MINORITY SAMPLE DEPENDENCE IN PRACTICE,0.6937354988399071,"the accuracy of neural network classiﬁers trained using robust algorithms evolve as a function of the
260"
MINORITY SAMPLE DEPENDENCE IN PRACTICE,0.6960556844547564,"majority and minority samples?
261"
MINORITY SAMPLE DEPENDENCE IN PRACTICE,0.6983758700696056,"To explore this question, we conduct a small case study using the imbalanced binary CIFAR10
262"
MINORITY SAMPLE DEPENDENCE IN PRACTICE,0.7006960556844548,"dataset [3, 24] that is constructed using the “cat” and “dog” classes. The test set consists of all
263"
MINORITY SAMPLE DEPENDENCE IN PRACTICE,0.703016241299304,"of the 1000 cat and 1000 dog test examples. To form our initial train and validation sets, we take
264"
MINORITY SAMPLE DEPENDENCE IN PRACTICE,0.7053364269141531,"2500 cat examples but only 500 dog examples from the ofﬁcial train set, corresponding to a 5:1
265"
MINORITY SAMPLE DEPENDENCE IN PRACTICE,0.7076566125290024,"label imbalance. We then use 80% of those examples for training and the rest for validation. In our
266"
MINORITY SAMPLE DEPENDENCE IN PRACTICE,0.7099767981438515,"experiment, we either (a) add only minority samples; (b) add only majority samples; (c) add both
267"
MINORITY SAMPLE DEPENDENCE IN PRACTICE,0.7122969837587007,"majority and minority samples in a 5:1 ratio. We consider competitive robust classiﬁers proposed
268"
MINORITY SAMPLE DEPENDENCE IN PRACTICE,0.7146171693735499,"in the literature that are convolutional neural networks trained either by using (i) the importance
269"
MINORITY SAMPLE DEPENDENCE IN PRACTICE,0.7169373549883991,"weighted cross entropy loss, or (ii) the importance weighted VS loss [13]. We early stop using the
270"
MINORITY SAMPLE DEPENDENCE IN PRACTICE,0.7192575406032483,"importance weighted validation loss in both cases. The additional experimental details are presented
271"
MINORITY SAMPLE DEPENDENCE IN PRACTICE,0.7215777262180975,"in Appendix D.
272"
MINORITY SAMPLE DEPENDENCE IN PRACTICE,0.7238979118329466,"Our results in Figure 2 are generally consistent with our theoretical predictions. By adding only
273"
MINORITY SAMPLE DEPENDENCE IN PRACTICE,0.7262180974477959,"minority class samples the test accuracy of both classiﬁers increases by a great extent (6%), while by
274"
MINORITY SAMPLE DEPENDENCE IN PRACTICE,0.728538283062645,"adding only majority class samples the test accuracy remains constant or in some cases even decreases
275"
MINORITY SAMPLE DEPENDENCE IN PRACTICE,0.7308584686774942,"owing to the added bias of the classiﬁers. When we add samples to both groups proportionately, the
276"
MINORITY SAMPLE DEPENDENCE IN PRACTICE,0.7331786542923434,"increase in the test accuracy appears to largely to be due to the increase in the number of minority
277"
MINORITY SAMPLE DEPENDENCE IN PRACTICE,0.7354988399071926,"class samples and on the left panels, we see that the difference between adding only extra minority
278"
MINORITY SAMPLE DEPENDENCE IN PRACTICE,0.7378190255220418,"group samples (red) and both minority and majority group samples (green) is small. Thus, we ﬁnd
279"
MINORITY SAMPLE DEPENDENCE IN PRACTICE,0.740139211136891,"that the accuracy for these neural network classiﬁers is also constrained by the number of minority
280"
MINORITY SAMPLE DEPENDENCE IN PRACTICE,0.7424593967517401,"class samples.
281"
DISCUSSION,0.7447795823665894,"7
Discussion
282"
DISCUSSION,0.7470997679814385,"We showed that undersampling is an optimal robustness intervention in nonparametric classiﬁcation
283"
DISCUSSION,0.7494199535962877,"in the absence of signiﬁcant overlap between group distributions or without additional structure
284"
DISCUSSION,0.7517401392111369,"beyond Lipschitz continuity.
285"
DISCUSSION,0.7540603248259861,"At a high level our results highlight the need to reason about the speciﬁc structure in the distribution
286"
DISCUSSION,0.7563805104408353,"shift and design algorithms that are tailored to take advantage of this structure. This would require
287"
DISCUSSION,0.7587006960556845,"us to step away from the common practice in robust machine learning where the focus is to design
288"
DISCUSSION,0.7610208816705336,"“universal” robustness interventions that are agnostic to the structure in the shift. Alongside this,
289"
DISCUSSION,0.7633410672853829,"our results also dictate the need for datasets and benchmarks with the propensity for transfer from
290"
DISCUSSION,0.765661252900232,"training time to test time.
291"
REFERENCES,0.7679814385150812,"References
292"
REFERENCES,0.7703016241299304,"[1] M. A. Alcorn, Q. Li, Z. Gong, C. Wang, L. Mai, W.-S. Ku, and A. Nguyen. Strike (with) a pose:
293"
REFERENCES,0.7726218097447796,"Neural networks are easily fooled by strange poses of familiar objects. In Computer Vision and
294"
REFERENCES,0.7749419953596288,"Pattern Recognition (CVPR), 2019.
295"
REFERENCES,0.777262180974478,"[2] S. L. Blodgett, L. Green, and B. T. O’Connor. Demographic dialectal variation in social
296"
REFERENCES,0.7795823665893271,"media: A case study of african-american english. In Empirical Methods in Natural Language
297"
REFERENCES,0.7819025522041764,"Processing (EMNLP), 2016.
298"
REFERENCES,0.7842227378190255,"[3] J. Byrd and Z. Lipton. What is the effect of importance weighting in deep learning?
In
299"
REFERENCES,0.7865429234338747,"International Conference on Machine Learning (ICML), 2019.
300"
REFERENCES,0.7888631090487239,"[4] C. L. Canonne.
A short note on an inequality between KL and TV.
arXiv preprint
301"
REFERENCES,0.7911832946635731,"arXiv:2202.07198, 2022.
302"
REFERENCES,0.7935034802784223,"[5] K. Cao, C. Wei, A. Gaidon, N. Arechiga, and T. Ma. Learning imbalanced datasets with
303"
REFERENCES,0.7958236658932715,"label-distribution-aware margin loss. Advances in Neural Information Processing Systems
304"
REFERENCES,0.7981438515081206,"(NeurIPS), 2019.
305"
REFERENCES,0.8004640371229699,"[6] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer. Smote: Synthetic minority
306"
REFERENCES,0.802784222737819,"over-sampling technique. Journal of Artiﬁcial Intelligence Research, 2002.
307"
REFERENCES,0.8051044083526682,"[7] Y. Cui, M. Jia, T.-Y. Lin, Y. Song, and S. Belongie. Class-balanced loss based on effective
308"
REFERENCES,0.8074245939675174,"number of samples. In Computer Vision and Pattern Recognition (CVPR), 2019.
309"
REFERENCES,0.8097447795823666,"[8] L. Devroye and L. Györﬁ. Nonparametric density estimation: the L1 view. Wiley Series in
310"
REFERENCES,0.8120649651972158,"Probability and Mathematical Statistics, 1985.
311"
REFERENCES,0.814385150812065,"[9] D. Freedman and P. Diaconis. On the histogram as a density estimator: L2 theory. Zeitschrift
312"
REFERENCES,0.8167053364269141,"für Wahrscheinlichkeitstheorie und verwandte Gebiete, 1981.
313"
REFERENCES,0.8190255220417634,"[10] T. Hashimoto, M. Srivastava, H. Namkoong, and P. Liang. Fairness without demographics in
314"
REFERENCES,0.8213457076566125,"repeated loss minimization. In International Conference on Machine Learning (ICML), 2018.
315"
REFERENCES,0.8236658932714617,"[11] D. Hovy and A. Søgaard. Tagging performance correlates with author age. In Association for
316"
REFERENCES,0.8259860788863109,"Computational Linguistics (ACL), 2015.
317"
REFERENCES,0.8283062645011601,"[12] B. Y. Idrissi, M. Arjovsky, M. Pezeshki, and D. Lopez-Paz. Simple data balancing achieves
318"
REFERENCES,0.8306264501160093,"competitive worst-group-accuracy. In Causal Learning and Reasoning, 2022.
319"
REFERENCES,0.8329466357308585,"[13] G. R. Kini, O. Paraskevas, S. Oymak, and C. Thrampoulidis. Label-imbalanced and group-
320"
REFERENCES,0.8352668213457076,"sensitive classiﬁcation under overparameterization. Advances in Neural Information Processing
321"
REFERENCES,0.8375870069605569,"Systems (NeurIPS), 2021.
322"
REFERENCES,0.839907192575406,"[14] M. Kubat, S. Matwin, et al. Addressing the curse of imbalanced training sets: one-sided
323"
REFERENCES,0.8422273781902552,"selection. In International Conference on Machine Learning (ICML), 1997.
324"
REFERENCES,0.8445475638051044,"[15] Z. Lipton, Y.-X. Wang, and A. Smola. Detecting and correcting for label shift with black box
325"
REFERENCES,0.8468677494199536,"predictors. In International Conference on Machine Learning (ICML), 2018.
326"
REFERENCES,0.8491879350348028,"[16] E. Z. Liu, B. Haghgoo, A. S. Chen, A. Raghunathan, P. W. Koh, S. Sagawa, P. Liang, and
327"
REFERENCES,0.851508120649652,"C. Finn. Just train twice: Improving group robustness without training group information. In
328"
REFERENCES,0.8538283062645011,"International Conference on Machine Learning (ICML), 2021.
329"
REFERENCES,0.8561484918793504,"[17] A. K. Menon, S. Jayasumana, A. S. Rawat, H. Jain, A. Veit, and S. Kumar. Long-tail learning
330"
REFERENCES,0.8584686774941995,"via logit adjustment. In International Conference on Learning Representations (ICLR), 2020.
331"
REFERENCES,0.8607888631090487,"[18] S. Sagawa, P. W. Koh, T. B. Hashimoto, and P. Liang. Distributionally robust neural networks.
332"
REFERENCES,0.8631090487238979,"In International Conference on Learning Representations (ICLR), 2020.
333"
REFERENCES,0.8654292343387471,"[19] H. Shimodaira. Improving predictive inference under covariate shift by weighting the log-
334"
REFERENCES,0.8677494199535963,"likelihood function. Journal of Statistical Planning and Inference, 2000.
335"
REFERENCES,0.8700696055684455,"[20] R. Tatman. Gender and dialect bias in youtube’s automatic captions. In ACL Workshop on
336"
REFERENCES,0.8723897911832946,"Ethics in Natural Language Processing, 2017.
337"
REFERENCES,0.8747099767981439,"[21] A. B. Tsybakov. Introduction to Nonparametric Estimation. Springer, 2010.
338"
REFERENCES,0.877030162412993,"[22] M. J. Wainwright. High-dimensional statistics: A non-asymptotic viewpoint. Cambridge
339"
REFERENCES,0.8793503480278422,"University Press, 2019.
340"
REFERENCES,0.8816705336426914,"[23] B. C. Wallace, K. Small, C. E. Brodley, and T. A. Trikalinos. Class imbalance, redux. In
341"
REFERENCES,0.8839907192575406,"International Conference on Data Mining (ICDM, 2011.
342"
REFERENCES,0.8863109048723898,"[24] K. A. Wang, N. S. Chatterji, S. Haque, and T. Hashimoto. Is importance weighting incompatible
343"
REFERENCES,0.888631090487239,"with interpolating classiﬁers? In International Conference on Learning Representations (ICLR),
344"
REFERENCES,0.8909512761020881,"2022.
345"
REFERENCES,0.8932714617169374,"[25] L. Wasserman. Lecture notes in nonparametric classiﬁcation. URL https://www.stat.cmu.
346"
REFERENCES,0.8955916473317865,"edu/~larry/=sml/nonparclass.pdf. [Online; accessed 12-May-2022].
347"
REFERENCES,0.8979118329466357,"[26] Wikipedia contributors. Poisson binomial distribution — Wikipedia, the free encyclopedia,
348"
REFERENCES,0.9002320185614849,"2022.
URL https://en.wikipedia.org/w/index.php?title=Poisson_binomial_
349"
REFERENCES,0.9025522041763341,"distribution&oldid=1071847908. [Online; accessed 5-May-2022].
350"
REFERENCES,0.9048723897911833,"[27] D. Xu, Y. Ye, and C. Ruan. Understanding the role of importance weighting for deep learning.
351"
REFERENCES,0.9071925754060325,"In International Conference on Learning Representations (ICLR), 2020.
352"
REFERENCES,0.9095127610208816,"[28] Y. Yang and Z. Xu. Rethinking the value of labels for improving class-imbalanced learning.
353"
REFERENCES,0.9118329466357309,"Advances in Neural Information Processing Systems (NeurIPS), 2020.
354"
REFERENCES,0.91415313225058,"[29] H.-J. Ye, H.-Y. Chen, D.-C. Zhan, and W.-L. Chao. Identifying and compensating for feature
355"
REFERENCES,0.9164733178654292,"deviation in imbalanced deep learning. arXiv preprint arXiv:2001.01385, 2020.
356"
REFERENCES,0.9187935034802784,"Checklist
357"
REFERENCES,0.9211136890951276,"1. For all authors...
358"
REFERENCES,0.9234338747099768,"(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s
359"
REFERENCES,0.925754060324826,"contributions and scope? [Yes]
360"
REFERENCES,0.9280742459396751,"(b) Did you describe the limitations of your work? [Yes]
361"
REFERENCES,0.9303944315545244,"(c) Did you discuss any potential negative societal impacts of your work? [Yes]
362"
REFERENCES,0.9327146171693735,"(d) Have you read the ethics review guidelines and ensured that your paper conforms to
363"
REFERENCES,0.9350348027842227,"them? [Yes]
364"
REFERENCES,0.9373549883990719,"2. If you are including theoretical results...
365"
REFERENCES,0.9396751740139211,"(a) Did you state the full set of assumptions of all theoretical results? [Yes]
366"
REFERENCES,0.9419953596287703,"(b) Did you include complete proofs of all theoretical results? [Yes]
367"
REFERENCES,0.9443155452436195,"3. If you ran experiments...
368"
REFERENCES,0.9466357308584686,"(a) Did you include the code, data, and instructions needed to reproduce the main experi-
369"
REFERENCES,0.9489559164733179,"mental results (either in the supplemental material or as a URL)? [Yes]
370"
REFERENCES,0.951276102088167,"(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
371"
REFERENCES,0.9535962877030162,"were chosen)? [Yes]
372"
REFERENCES,0.9559164733178654,"(c) Did you report error bars (e.g., with respect to the random seed after running experi-
373"
REFERENCES,0.9582366589327146,"ments multiple times)? [Yes]
374"
REFERENCES,0.9605568445475638,"(d) Did you include the total amount of compute and the type of resources used (e.g., type
375"
REFERENCES,0.962877030162413,"of GPUs, internal cluster, or cloud provider)? [Yes]
376"
REFERENCES,0.9651972157772621,"4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
377"
REFERENCES,0.9675174013921114,"(a) If your work uses existing assets, did you cite the creators? [Yes]
378"
REFERENCES,0.9698375870069605,"(b) Did you mention the license of the assets? [N/A]
379"
REFERENCES,0.9721577726218097,"(c) Did you include any new assets either in the supplemental material or as a URL? [N/A]
380 381"
REFERENCES,0.974477958236659,"(d) Did you discuss whether and how consent was obtained from people whose data you’re
382"
REFERENCES,0.9767981438515081,"using/curating? [N/A]
383"
REFERENCES,0.9791183294663574,"(e) Did you discuss whether the data you are using/curating contains personally identiﬁable
384"
REFERENCES,0.9814385150812065,"information or offensive content? [N/A]
385"
REFERENCES,0.9837587006960556,"5. If you used crowdsourcing or conducted research with human subjects...
386"
REFERENCES,0.9860788863109049,"(a) Did you include the full text of instructions given to participants and screenshots, if
387"
REFERENCES,0.988399071925754,"applicable? [N/A]
388"
REFERENCES,0.9907192575406032,"(b) Did you describe any potential participant risks, with links to Institutional Review
389"
REFERENCES,0.9930394431554525,"Board (IRB) approvals, if applicable? [N/A]
390"
REFERENCES,0.9953596287703016,"(c) Did you include the estimated hourly wage paid to participants and the total amount
391"
REFERENCES,0.9976798143851509,"spent on participant compensation? [N/A]
392"
