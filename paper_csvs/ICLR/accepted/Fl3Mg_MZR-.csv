Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.005,"The lottery ticket hypothesis questions the role of overparameterization in super-
vised deep learning. But how is the performance of winning lottery tickets af-
fected by the distributional shift inherent to reinforcement learning problems? In
this work, we address this question by comparing sparse agents who have to ad-
dress the non-stationarity of the exploration-exploitation problem with supervised
agents trained to imitate an expert. We show that feed-forward networks trained
with behavioural cloning compared to reinforcement learning can be pruned to
higher levels of sparsity without performance degradation. This suggests that in
order to solve the RL problem agents require more degrees of freedom. Using
a set of carefully designed baseline conditions, we ﬁnd that the majority of the
lottery ticket effect in both learning paradigms can be attributed to the identiﬁed
mask rather than the weight initialization. The input layer mask selectively prunes
entire input dimensions that turn out to be irrelevant for the task at hand. At
a moderate level of sparsity the mask identiﬁed by iterative magnitude pruning
yields minimal task-relevant representations, i.e., an interpretable inductive bias.
Finally, we propose a simple initialization rescaling which promotes the robust
identiﬁcation of sparse task representations in low-dimensional control tasks."
INTRODUCTION,0.01,"1
INTRODUCTION"
INTRODUCTION,0.015,"Recent research on the lottery ticket hypothesis (LTH, Frankle & Carbin, 2019; Frankle et al., 2019)
in deep learning has demonstrated the existence of very sparse neural networks that train to per-
formance levels comparable to those of their dense counterparts. These results challenge the role
of overparameterization in supervised learning and provide a new perspective on the emergence of
stable learning dynamics (Frankle et al., 2020a;b). Recently these results have been extended to
various domains beyond supervised image classiﬁcation. These include self-supervised learning
(Chen et al., 2020a), natural language processing (Yu et al., 2019; Chen et al., 2020b) and semantic
segmentation (Girish et al., 2020). But how does the lottery ticket ticket phenomenon transfer to
reinforcement learning agents? One key challenge may be the inherent non-stationarity of the opti-
mization problem in deep reinforcement learning (DRL): The data-generation process is not static,
but depends on the changing state of the neural network. Furthermore, a weight may serve different
roles at different stages of learning (e.g. during exploration and exploitation). It is not obvious how
a simple weight magnitude-based pruning heuristic acting on a well-performing policy shapes the
learning process of the agent. In this work, we therefore investigate winning tickets in reinforce-
ment learning and their underlying contributing factors. We compare supervised behavioral cloning
with DRL, putting a special emphasis on the resulting input representations used for prediction and
control. Thereby, we connect the statistical perspective of sparse structure discovery (e.g. Hastie
et al., 2019) with the iterative magnitude pruning (IMP, Han et al., 2015) procedure in the context of
Markov decision processes (MDPs). The contributions of this work are summarized as follows:"
INTRODUCTION,0.02,1. We show that winning tickets exist in both high-dimensional visual and control tasks (con-
INTRODUCTION,0.025,"tinuous/discrete). A positive lottery ticket effect is robustly observed for both off-policy
DRL algorithms, including Deep-Q-Networks (DQN, Mnih et al., 2015) and on-policy"
INTRODUCTION,0.03,∗Authors contributed equally. RTL is the corresponding author (robert.t.lange@tu-berlin.de).
INTRODUCTION,0.035,Published as a conference paper at ICLR 2022
INTRODUCTION,0.04,"Figure 1: Representation compression, disentangling baselines & lottery ticket effects in continuous
control tasks. Left. IMP successively prunes task-irrelevant outer rim pixels in a MNIST digit-
classiﬁcation task and for an IMP-masked agent solving a visual navigation task in DRL. The chan-
nel encoding the patrolling enemy is pruned up to the point where only potential enemy locations
are considered. Middle. To disentangle the contributions of mask, initialization and layer-wise
pruning ratio to the winning lottery ticket (mask/weights condition), we compare three baselines:
After each IMP iteration, we permute either only the remaining initial weights (mask/permuted) or
also the sparsity mask (permuted/permuted). The third baseline is created by randomly sampling a
sparse mask and random re-initialization of the weights (random/re-init). Right. Avg. normalized
performance of policies at different sparsity levels and for different baseline conditions and four
PyBullet (Ellenberger, 2018) tasks. For both behavioral cloning and PPO agents most of the ticket
effect can be attributed to tasks IMP-derived mask as compared to the weight initialization. Agents
trained with supervision can be pruned to higher sparsity levels before performance deteriorates."
INTRODUCTION,0.045,"policy-gradient methods (PPO, Schulman et al., 2015; 2017), providing evidence that the
lottery ticket effect is a universal phenomenon across optimization formulations in DRL."
INTRODUCTION,0.05,"2. By comparing RL to supervised behavioral cloning (BC), we show that networks trained"
INTRODUCTION,0.055,"with explicit supervision can be pruned to higher sparsity levels before performance starts
to degrade, indicating that the RL problem requires a larger amount of parameters to ad-
dress exploration, distribution shift & quality of the credit assignment signal (section 3.1)."
INTRODUCTION,0.06,"3. By introducing a set of lottery ticket baselines (section 3; ﬁgure 1), we disentangle the con-"
INTRODUCTION,0.065,"tributions of the mask, weight initialization and layer-wise pruning ratio. We demonstrate
that the mask explains most of the ticket effect in behavioral cloning and reinforcement
learning for MLP-based agents, whereas the associated weight initialization is less impor-
tant (section 3.2 and 3.3). For CNN-based agents the weight initialization contributes more."
INTRODUCTION,0.07,"4. By visualizing the sparsiﬁed weights for each layer, we ﬁnd that early network layers are"
INTRODUCTION,0.075,"pruned more. Entire input dimensions can be rendered invisible to MLP-based agents by
the pruning procedure. By this mechanism, IMP compresses the input representation of the
MDP (e.g. ﬁgure 1, left column, bottom row) and reveals a minimal task representation for
the underlying control problems (section 4)."
INTRODUCTION,0.08,"5. The IMP input layer mask not only eliminates obviously redundant dimensions, but also"
INTRODUCTION,0.085,"identiﬁes complex relationships between input features and the task of the agent (section
4.1), e.g. the proximity of an enemy or the speed of an approaching object. We show that
the input masking can be transferred to train dense agents with sparse inputs at lower costs."
WE SHOW THAT THE WEIGHT INITIALIZATION SCHEME IS IMPORTANT FOR DISCOVERING MINIMAL REP-,0.09,6. We show that the weight initialization scheme is important for discovering minimal rep-
WE SHOW THAT THE WEIGHT INITIALIZATION SCHEME IS IMPORTANT FOR DISCOVERING MINIMAL REP-,0.095,"resentations.
Depending on the input size of different layers of the network, global
magnitude-based pruning can introduce a strong layer-speciﬁc pruning bias. We com-
pare initializations and show that a suitable initialization scheme enables the removal of
task-irrelevant dimensions (section 4.2)."
WE SHOW THAT THE WEIGHT INITIALIZATION SCHEME IS IMPORTANT FOR DISCOVERING MINIMAL REP-,0.1,Published as a conference paper at ICLR 2022
BACKGROUND AND RELATED WORK,0.105,"2
BACKGROUND AND RELATED WORK"
BACKGROUND AND RELATED WORK,0.11,"Iterative Magnitude Pruning. We use the iterative pruning procedure outlined in Frankle & Carbin
(2019) to identify winning tickets. We train DRL agents for a previously calibrated number of
transitions and track the best performing network checkpoint throughout. Performance is measured
by the average return on a set of evaluation episodes. Afterwards, we prune 20% of the weights
with smallest magnitude globally (across all layers). The remaining weights are reset to their initial
values and we iterate this procedure (train →prune →reset).1 The lottery ticket effect refers to the
performance gap between the sparse network obtained via IMP and a randomly initialized network
with sparsity-matched random pruning mask."
BACKGROUND AND RELATED WORK,0.115,"Lottery Tickets in Deep Reinforcement Learning. Yu et al. (2019) previously demonstrated the
existence of tickets in DRL that outperform parameter-matched random initializations. They ob-
tained tickets for a distributed on-policy actor-critic agent on a subset of environments in the ALE
benchmark (Bellemare et al., 2013) as well as a set of discrete control tasks. While they provide em-
pirical evidence for the existence of lottery tickets in DRL, they did not investigate the underlying
mechanisms. Here, we aim to unravel these mechanisms. To this end, we focus on a diverse set of
environments and provide a detailed comparison between supervised behavioral cloning and on-/off-
policy Deep RL with a set of carefully designed ticket baselines. We analyze the resulting masked
representations that the agent learns to act upon and the impact of speciﬁc weight initializations on
the resulting sparse networks."
BACKGROUND AND RELATED WORK,0.12,"Lottery Tickets with Non-Stationary Data Distributions. Desai et al. (2019) investigated whether
trained lottery tickets overﬁt the training data distribution under which they were obtained. Using
transfer learning tasks on natural language data, they showed that lottery tickets provide general
inductive biases. Similar ticket transfer results were reported by Morcos et al. (2019) and Mehta
(2019) in the context of optimizers and vision datasets. Unlike our work, these studies do not
investigate within-training covariate shift, but instead focus on transferring ticket initializations after
a full IMP run. Chen et al. (2021), on the other hand, investigate the ticket phenomenon in the context
of lifelong learning and class-incremental image classiﬁcation. They propose new pruning strategies
to overcome the sequential nature of tasks and need for increased model capacity. Compared to the
DRL setting, the covariate shift is here determined by the curriculum schedule of tasks and not the
exploration behaviour of the network-parameterized agent."
BACKGROUND AND RELATED WORK,0.125,"Deep Reinforcement Learning Background. In our off-policy DRL experiments, we train Deep-
Q-Networks (DQN, Mnih et al., 2015) with double Q-learning loss (Van Hasselt et al., 2016) and
prioritized experience replay (Schaul et al., 2015). As a representative on-policy algorithm, we chose
Proximal Policy Optimization (PPO, Schulman et al., 2015; 2017). PPO is a baseline-corrected pol-
icy gradient algorithm which uses a clipping strategy to approximate a computationally expensive
trust-region optimization method. For illustrative purposes, we train DQN agents on a visual navi-
gation task, in which an agent has to collect coins in a grid while avoiding poison and two patrollers
that are moving in restricted parts of the grid (ﬁgure 1, left column, bottom row; SI B). We scale
our results to four PyBullet (Ellenberger, 2018) continuous control and a subset of ALE benchmark
(Bellemare et al., 2013) environments. Due to computational considerations we limit each individ-
ual IMP iteration for the ATARI environments to 2.5 million frames. All other tasks were trained
for a pre-calibrated generous amount of transitions. We focus on feedforward value estimators and
policies (MLP & CNN) and used default hyperparameters with little tuning (SI C)."
BACKGROUND AND RELATED WORK,0.13,"Supervised Behavioral Cloning. While most supervised learning relies on a stationary data dis-
tribution provided by a static dataset, reinforcement learning agents have to acquire their training
data in an action-perception loop. Since the agent’s behavioural policy is learned over time, the
data distribution used in optimization undergoes covariate shift. To study how the covariate shift,
additional exploration problem and different credit assignment signal inﬂuence winning tickets, we
mimic the supervised learning case by training agents via supervised policy distillation (Rusu et al.,
2015; Schmitt et al., 2018). We roll out a pre-trained expert policy and train the student agent by
minimizing the KL divergence between the student’s and teacher’s policies."
BACKGROUND AND RELATED WORK,0.135,"1In supervised learning, the pruning mask is often constructed based on an early stopping criterion and the
ﬁnal network. We instead track the best performing agent. Thereby, we reduce noise introduced by unstable
learning dynamics and exploit that the agent is trained and evaluated on the same environment. We found that
late rewinding to a later checkpoint (Frankle et al., 2019) is not necessary for obtaining tickets (SI ﬁgure 14)."
BACKGROUND AND RELATED WORK,0.14,Published as a conference paper at ICLR 2022
DISENTANGLING TICKET CONTRIBUTIONS IN BC AND DEEP RL,0.145,"3
DISENTANGLING TICKET CONTRIBUTIONS IN BC AND DEEP RL"
DISENTANGLING TICKET CONTRIBUTIONS IN BC AND DEEP RL,0.15,Examined Sparsity-Generating IMP Variants
DISENTANGLING TICKET CONTRIBUTIONS IN BC AND DEEP RL,0.155,"Retain
weights"
DISENTANGLING TICKET CONTRIBUTIONS IN BC AND DEEP RL,0.16,"Retain
mask"
DISENTANGLING TICKET CONTRIBUTIONS IN BC AND DEEP RL,0.165,"Retain layer
pruning ratio
mask/weights
✓
✓
✓
mask/permuted
✗
✓
✓
permuted/permuted
✗
✗
✓
random/re-init
✗
✗
✗"
DISENTANGLING TICKET CONTRIBUTIONS IN BC AND DEEP RL,0.17,Table 1: Baselines for Disentangling Ticket Contributions
DISENTANGLING TICKET CONTRIBUTIONS IN BC AND DEEP RL,0.175,"There are two contributing fac-
tors to the lottery ticket ef-
fect: The IMP-identiﬁed binary
mask and the preserved initial-
ized weights that remain after
pruning (mask/weights). We aim
to disentangle the contributions
by introducing a set of counter-
factual baselines, which modify
the original IMP procedure (ﬁg-
ure 1, middle column; table 1).
A ﬁrst baseline estimates how
much of the performance of the ticket can be attributed to the initial weights, by means of a layer-
speciﬁc permutation of the weights that remain after masking (mask/permuted). A second, weaker
baseline estimates the contribution of the mask, by also permuting the layer-speciﬁc masks (per-
muted/permuted). Finally, we consider the standard random/re-init baseline, which samples random
binary masks – discarding layer-speciﬁc pruning ratios – and re-initializes all weights at each IMP
iteration. Throughout the next sections we use these baselines to analyze and compare the factors
that give rise to the lottery ticket effect in different control settings."
COMPARING WINNING TICKETS IN SUPERVISED BEHAVIORAL CLONING AND DEEP RL,0.18,"3.1
COMPARING WINNING TICKETS IN SUPERVISED BEHAVIORAL CLONING AND DEEP RL"
COMPARING WINNING TICKETS IN SUPERVISED BEHAVIORAL CLONING AND DEEP RL,0.185,"Does the covariate shift in DRL affect the existence and nature of lottery tickets? Weights pruned
for their small magnitude at the end of the learning process might be needed at earlier stages, e.g.,
during exploration. If this were the case, the performance of the ticket in DRL should degrade for
lower levels of sparsity than in a corresponding supervised task. To investigate this question, we turn
to a behavioral cloning setting, in which a student agent is trained to imitate the stochastic policy
of a pre-trained expert and apply magnitude pruning to the student network after each training run
iteration. By collecting transitions based on the static behavioral policy of the expert, we avoid
the need for exploration and the effect of an otherwise non-stationary data distribution. For both
the discrete action space and all continuous control tasks we ﬁnd that agents trained with RL and
the supervised settings start to degrade in performance at different sparsity levels (ﬁgure 2). More
speciﬁcally, the maximal return of agents that were trained with reinforcement learning starts to
drop at signiﬁcantly earlier stages of the IMP procedure. Hence, the covariate shift inherent to the
RL problem increases the minimal required size of a winning ticket in the tasks studied here: More
parameters are required for an agent that has to solve the additional exploration problem."
DISENTANGLING TICKET CONTRIBUTIONS IN SUPERVISED BEHAVIORAL CLONING,0.19,"3.2
DISENTANGLING TICKET CONTRIBUTIONS IN SUPERVISED BEHAVIORAL CLONING"
DISENTANGLING TICKET CONTRIBUTIONS IN SUPERVISED BEHAVIORAL CLONING,0.195,"To disentangle the contributions of the initial weights and the weight mask to the ticket effect in su-
pervised behavioral cloning, we next compared the three baselines to the full non-randomized ticket
(ﬁgure 1, top, right and 3). For most agents, training performance does not degrade substantially
when the weights are permuted but the mask is kept intact (no signiﬁcant gap between red and green
curves). But we observe a strong decrease in the ability to prune the policy to high sparsity levels
if one additionally permutes the IMP-derived weight mask (gap between green and blue curves).
This observation holds for the cloning of expert policies in both discrete and continuous control
tasks. Hence, the ticket effect can be mainly attributed to the mask rather than the weights. Fi-
nally, the traditional random/re-init baseline performs worse already for moderate levels of sparsity.
The resulting performance gap (blue and yellow curves) indicates the contribution of the layer-wise
pruning ratio to the mask effect. These insights emphasize the importance of strong and nuanced
baselines to understand the contributions to the full lottery ticket effect."
DISENTANGLING TICKET CONTRIBUTIONS IN DEEP REINFORCEMENT LEARNING,0.2,"3.3
DISENTANGLING TICKET CONTRIBUTIONS IN DEEP REINFORCEMENT LEARNING"
DISENTANGLING TICKET CONTRIBUTIONS IN DEEP REINFORCEMENT LEARNING,0.205,"The observation that ticket information is mostly carried by the mask rather than the initial weights
carries over to the full DRL setups (ﬁgure 1, bottom, right and ﬁgure 4)."
DISENTANGLING TICKET CONTRIBUTIONS IN DEEP REINFORCEMENT LEARNING,0.21,Published as a conference paper at ICLR 2022
DISENTANGLING TICKET CONTRIBUTIONS IN DEEP REINFORCEMENT LEARNING,0.215,"Figure 2: Comparing lottery tickets in DRL and supervised behavioral cloning. Networks trained
with explicit supervision can be pruned to higher sparsity levels before performance starts to de-
grade. Results are averaged over 15 runs for both the Cart-Pole and Acrobot and 10 runs for PyBullet
environments. We plot mean best performance and one standard deviation."
DISENTANGLING TICKET CONTRIBUTIONS IN DEEP REINFORCEMENT LEARNING,0.22,"Figure 3: Disentangling baselines for lottery tickets in supervised behavioral cloning. The gap be-
tween the ticket (mask/weights) and weight-permuted baseline (mask/permuted) is small, indicating
a strong contribution of the mask. Results are averaged over 15 runs for both Cart-Pole and Acrobot
and 10 runs for PyBullet environments. We plot mean best performance and one standard deviation."
DISENTANGLING TICKET CONTRIBUTIONS IN DEEP REINFORCEMENT LEARNING,0.225,"When preserving the information contained in the mask, agents can be pruned to much higher lev-
els of sparsity as compared to the case in which the mask information is distorted (permuted or
resampled). This effect holds for both PPO- and DQN-based agents and across a wide range of
qualitatively different tasks.2 Interestingly, ticket weights are more important for CNN-based agents
(e.g., see ATARI plots). We provide further experimental evidence for this observation on four Mi-
nAtar games (Young & Tian, 2019) in the supplementary material (SI ﬁgure 10), where we compare
MLP- and CNN-based agents."
DISENTANGLING TICKET CONTRIBUTIONS IN DEEP REINFORCEMENT LEARNING,0.23,"Unlike the behavioral cloning experiments, for some tasks the permuted/permuted baseline performs
worse than the random/reinit conﬁguration. This observation only occurs for the PyBullet control"
DISENTANGLING TICKET CONTRIBUTIONS IN DEEP REINFORCEMENT LEARNING,0.235,"2For further experiments investigating the role of different network capacities (SI ﬁgures 11, 12), state
encoding (SI ﬁgure 13) and regularization (SI ﬁgure 14) we refer the interested reader to the SI."
DISENTANGLING TICKET CONTRIBUTIONS IN DEEP REINFORCEMENT LEARNING,0.24,Published as a conference paper at ICLR 2022
DISENTANGLING TICKET CONTRIBUTIONS IN DEEP REINFORCEMENT LEARNING,0.245,"Figure 4: Tickets in on- and off-policy deep reinforcement learning. The disentangling baselines for
tickets in on-policy (PPO) and off-policy (DQN) DRL for a set of continuous control, a visual nav-
igation and a subset of ATARI environments reveal the consistent importance of the IMP-extracted
mask. Results are averaged over 5 independent runs on the Gridworld and ATARI and 10 runs on
the continuous control environments. We plot mean best performance and one standard deviation."
DISENTANGLING TICKET CONTRIBUTIONS IN DEEP REINFORCEMENT LEARNING,0.25,"tasks in which both the separate critic and actor networks were pruned (ACP). If instead, one only
prunes the actor network (AP), as was done for the PPO agents trained on Cart-Pole, Acrobot and
ATARI, the permuted/permuted baseline performs stronger (see SI ﬁgure 15). This result highlights
the importance of differences in IMP-derived pruning ratios between agent modules, i.e. separate
actor and critic networks."
MINIMALLY TASK-SUF F ICIENT REPRESENTATIONS VIA IMP,0.255,"4
MINIMALLY TASK-SUF F ICIENT REPRESENTATIONS VIA IMP"
MINIMALLY TASK-SUF F ICIENT REPRESENTATIONS VIA IMP,0.26,"The previous results establish the importance of the information contained in the mask for the suc-
cess of winning tickets. To better understand this phenomenon and the resulting inductive biases,
we analyzed the sparsiﬁed weight matrices, speciﬁcally between the input and the units in layer one.
The next section sets out to answer the following questions: 1) The derived mask can be thought of
as a pair of goggles which guide the processing of state information. What do the masked MLP-
based agents observe? 2) Can we transfer the input layer mask and re-use it as an inductive bias for
otherwise dense agents? 3) How much inﬂuence do weight initialization and input dimensionality
have on the discovered input layer mask?"
MINIMAL TASK REPRESENTATIONS IN HIGH-DIMENSIONAL VISUAL TASKS,0.265,"4.1
MINIMAL TASK REPRESENTATIONS IN HIGH-DIMENSIONAL VISUAL TASKS"
MINIMAL TASK REPRESENTATIONS IN HIGH-DIMENSIONAL VISUAL TASKS,0.27,"In the visual navigation experiments, the pruning primarily affects the input layer (ﬁgure 5, top
right column). When visualizing the cumulative absolute weights of the input layer for an IMP-
derived DQN agent, we ﬁnd that IMP deletes entire dimensions from the observation vector (ﬁgure
5, left column) by removing all connections between those dimensions and the ﬁrst hidden layer.
These eliminated input dimensions are not task-relevant. A fully-connected network trained on
the remaining dimensions learns the task as quickly as when trained on the complete set of input
dimensions (ﬁgure 5, bottom right column). Hence, the IMP-derived mask provides a compressed
representation of a high-dimensional observation space, enabling successful training even at high
levels of sparsity."
MINIMAL TASK REPRESENTATIONS IN HIGH-DIMENSIONAL VISUAL TASKS,0.275,Published as a conference paper at ICLR 2022
MINIMAL TASK REPRESENTATIONS IN HIGH-DIMENSIONAL VISUAL TASKS,0.28,"Figure 5: IMP eliminates task-irrelevant observation dimensions for a high-dimensional visual nav-
igation task (ot ∈R6×10×20). Left. Channel-/pixel-wise cumulative weight magnitudes. IMP
successively prunes redundant input pixels which are not necessary to solve the navigation task. All
of the pruned enemy channel pixels encode locations which the patrolling enemy cannot access.
Right, Top. Pruning affects the layers and object channels differentially. The input layer is pruned
most strongly, while the agent channel is pruned the least. Right, Bottom. The input layer pruning
mask (at moderate sparsity levels) can be used as an inductive bias for training a dense network."
MINIMAL TASK REPRESENTATIONS IN HIGH-DIMENSIONAL VISUAL TASKS,0.285,"This result extends to masked input layers of MLP-agents trained on the MinAtar environments
(ﬁgure 6 and SI). For the Freeway task we ﬁnd that the IMP mask encodes the notion of velocity
of moving cars. The input layer weights corresponding to the binary observation channel which
encodes the slowest moving car are pruned the most, maintaining only the pixels needed to react
in time. The same principle applies to the enemy bullet channel in the SpaceInvaders task. The
agent only needs to know about a potential collision at the next time step in order to avoid being hit.
Therefore, IMP prunes all information about the enemy bullet that is more than one step away from
the agent. For all visualized MinAtar tasks, only the actionable row/column of the agent channel
is preserved. In Breakout, additionally the bottom row of the ball and trail channel is pruned since
the game terminates once the ball or trail reach it. There is no action-relevant information encoded
and it can be discarded. In summary, we ﬁnd that IMP-derived input layer mask provides a visually
interpretable and physically meaningful compression of the observation space."
"MINIMAL REPRESENTATIONS IN LOW-DIMENSIONAL CONTROL TASKS AND THE ISSUE
OF INITIALIZATION SCALES",0.29,"4.2
MINIMAL REPRESENTATIONS IN LOW-DIMENSIONAL CONTROL TASKS AND THE ISSUE
OF INITIALIZATION SCALES"
"MINIMAL REPRESENTATIONS IN LOW-DIMENSIONAL CONTROL TASKS AND THE ISSUE
OF INITIALIZATION SCALES",0.295,"The continuous and discrete control tasks, on the other hand, rely on low-dimensional state repre-
sentations. At ﬁrst sight, they do not contain obviously uninformative input dimensions that lend
themselves to be pruned. We would expect the cumulative weight strength of each input dimension
to decrease at equal speed over the course of iterative pruning. Contrary to this initial intuition,
IMP still aggressively prunes core dimensions while yielding trainable agents (ﬁgure 7, 8). For a
subset of continuous control tasks, we ﬁnd that one can prune 20 and up to 50 percent of the input
dimensions, while still being able to train the agent to the performance level of a dense counterpart
(ﬁgure 7). The exact amount depends on the considered environment indicating a varying degree
of observation over-speciﬁcation across task formulations. For example, the Walker2D environment
can train to full performance with only 11 out of 22 observation dimensions, while the Ant environ-
ment requires 22 out of 28 dimensions. We note an initial positive effect of the weight pruning on
the performance of all agents indicating the effectiveness of the implied regularization."
"MINIMAL REPRESENTATIONS IN LOW-DIMENSIONAL CONTROL TASKS AND THE ISSUE
OF INITIALIZATION SCALES",0.3,"In the Cart-Pole task the cart position and its velocity are pruned (ﬁgure 8, top). Newton’s laws are
invariant with regard to the choice of the inertial frame of reference, so the input channels encod-
ing the cart position and velocity do not provide additional information over the pole’s angle and
velocity. The Acrobot task requires the agent to swing-up a two-link pendulum across a line as fast"
"MINIMAL REPRESENTATIONS IN LOW-DIMENSIONAL CONTROL TASKS AND THE ISSUE
OF INITIALIZATION SCALES",0.305,Published as a conference paper at ICLR 2022
"MINIMAL REPRESENTATIONS IN LOW-DIMENSIONAL CONTROL TASKS AND THE ISSUE
OF INITIALIZATION SCALES",0.31,"Figure 6: IMP eliminates task-irrelevant observation dimensions for a selection of MinAtar environ-
ments (Young & Tian, 2019). The environment depicting ﬁgures were adapted from Young & Tian
(2019). Left. Freeway. IMP provides an inductive bias by differentially pruning object channels
with different velocities (e.g. car speeds). Middle. SpaceInvaders. IMP only preserves pixels of en-
emy objects which encode actionable proximity information (e.g. bullets being close to the agent).
Right. Breakout. IMP discards pixels which only change when it is too late to act (e.g. when the
ball has left the display and the episode terminates)."
"MINIMAL REPRESENTATIONS IN LOW-DIMENSIONAL CONTROL TASKS AND THE ISSUE
OF INITIALIZATION SCALES",0.315,"Figure 7: IMP eliminates task-irrelevant observation dimensions for a set of continuous control
tasks. For all three tasks there exists a moderate sparsity level (approximately 10% non-sparse
weights) at which entire input units (columns of the ﬁrst linear layer) are pruned while the agents
still train to the performance level of their dense counterparts. The extent of this observation strongly
depends on the considered environment indicating a varying extent of observation over-speciﬁcation."
"MINIMAL REPRESENTATIONS IN LOW-DIMENSIONAL CONTROL TASKS AND THE ISSUE
OF INITIALIZATION SCALES",0.32,"as possible. Here, IMP eliminates all sine and cosine transformations of the rotational angles of the
links (ﬁgure 8, middle row, right column). Since the swing-up can be achieved by coordinating and
increasing the angular velocities of the two links, only these two dimensions have to be preserved
from pruning. These experiments demonstrate that IMP can yield fundamental insights into what
physical information is sufﬁcient to solve a task and leverages them to prune the input represen-
tation without impairing performance. During our analysis we discovered that the core dimension
discovery of IMP crucially depends on the initialization of the input layer weights. Popular weight
initialization heuristics such as the Kaiming family (He et al., 2015) aim to preserve activation mag-
nitudes across network layers. As a result, layers that receive high-dimensional input are initialized
at lower values and are therefore prone to more aggressive pruning by IMP. This bias does not occur
for Xavier initializations (Glorot & Bengio, 2010, ﬁgure 8, top rows, right column). Furthermore,
we experimented with different approaches to combat this initialization bias. We ﬁnd that down-
scaling the Kaiming initialization of the input layer by a factor of ten leads to a robust discovery
after few IMP iterations (ﬁgure 8, top rows, middle column). While smaller rescaling factors lead
to a slower separation of dimensions, larger factors result in overly aggressive pruning of the input
layer. In summary, we have shown that the input layer initialization can shape the resulting lottery
ticket mask and provide a ﬁrst initialization heuristic promoting efﬁcient minimal task representation
discovery."
"MINIMAL REPRESENTATIONS IN LOW-DIMENSIONAL CONTROL TASKS AND THE ISSUE
OF INITIALIZATION SCALES",0.325,Published as a conference paper at ICLR 2022
"MINIMAL REPRESENTATIONS IN LOW-DIMENSIONAL CONTROL TASKS AND THE ISSUE
OF INITIALIZATION SCALES",0.33,"Figure 8: IMP eliminates task-irrelevant observation dimensions for low-dimensional control tasks.
Suitable initialization heuristics can promote and accelerate the discovery of minimal representa-
tions. Top. IMP identiﬁes that the Cart-Pole task can be solved with only two dimensions: The pole
angle and angular velocity. Middle. IMP identiﬁes that the Acrobot task can be solved with only
two dimensions: The angular velocities of the two pendulum links. Bottom. Agents can success-
fully be trained on the subset of IMP-derived dimensions alone. We plot mean performance and one
standard deviation across 10 independent runs."
CONCLUSION,0.335,"5
CONCLUSION"
CONCLUSION,0.34,"Summary. This work has investigated the mechanisms underlying the lottery ticket effect in be-
havioral cloning and DRL. We provide evidence that agents which are trained on a stationary data
distribution and do not face the exploration problem, can be pruned to higher levels of sparsity while
successfully performing their task. For both setups we ﬁnd that the effect can be attributed to the
identiﬁed mask rather than to the corresponding weight initialization. We have shown that the mask
compresses the input representation by removing task-irrelevant information in the form of visual
object pixels or entire control observation dimensions. Finally, we found that the pruning process is
sensitive to the layer-speciﬁc scale of the weight initialization. The interpretability of the resulting
minimal representations can be enhanced by heuristically re-scaling the weights in different layers."
CONCLUSION,0.345,"Limitations. While our observations are qualitatively robust, they quantitatively vary across learn-
ing paradigms, algorithms, architectures and different tasks. Compared to our results, weight values
have been claimed to play a more central role in the context of lottery tickets in supervised image
classiﬁcation (Frankle et al., 2020a). Future work will need to investigate under which conditions
the initialization of the weight values signiﬁcantly contributes to the ticket effect in RL. Further-
more, most of our experimental results have been obtained for on-policy PPO and policy-based BC
agents. Whether the interpretation of the mask effect as a representation regularizer also holds for
intermediate and higher layer masks is hard to assess and remains an open question. Finally, this
study is empirical in its nature and will require further theoretical guidance and foundations."
CONCLUSION,0.35,"Future Work. In future work we want to investigate layer-wise effects, e.g., by studying layer-
speciﬁc pruning ratios, normalization schemes and ﬁne-grained baseline analyses, in the hope of
identifying sparser, more efﬁcient and interpretable winning tickets. Further work needs to be done
to disentangle the contributions of the additional exploration problem, distribution shift and credit
assignment signal in RL. Our proposed baseline analysis can be applied more generally to all stud-
ies considering the lottery ticket procedure. Finally, we believe that there are many opportunities
to modify the original IMP procedure to simplify the discovery of minimal representations. For
example, pruning entire units of intermediate layers may improve the interpretability of hidden rep-
resentations."
CONCLUSION,0.355,Published as a conference paper at ICLR 2022
CONCLUSION,0.36,ACKNOWLEDGMENTS
CONCLUSION,0.365,"We thank Jonathan Frankle for initial discussions and feedback on the ﬁrst manuscript draft. Further-
more, we thank Joram Keijser for reviewing the manuscript. This work is funded by the Deutsche
Forschungsgemeinschaft (DFG, German Research Foundation) under Germanys Excellence Strat-
egy - EXC 2002/1 Science of Intelligence - project number 390523135."
REFERENCES,0.37,REFERENCES
REFERENCES,0.375,"Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environ-"
REFERENCES,0.38,"ment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research, 47:
253–279, 2013."
REFERENCES,0.385,"Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and"
REFERENCES,0.39,"Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016."
REFERENCES,0.395,"Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Michael Carbin, and"
REFERENCES,0.4,"Zhangyang Wang. The lottery tickets hypothesis for supervised and self-supervised pre-training
in computer vision models. arXiv preprint arXiv:2012.06908, 2020a."
REFERENCES,0.405,"Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, and"
REFERENCES,0.41,"Michael Carbin.
The lottery ticket hypothesis for pre-trained bert networks.
arXiv preprint
arXiv:2007.12223, 2020b."
REFERENCES,0.415,"Tianlong Chen, Zhenyu Zhang, Sijia Liu, Shiyu Chang, and Zhangyang Wang. Long live the lottery:"
REFERENCES,0.42,"The existence of winning tickets in lifelong learning. In International Conference on Learning
Representations, 2021a. URL https://openreview. net/forum, 2021."
REFERENCES,0.425,"Shrey Desai, Hongyuan Zhan, and Ahmed Aly. Evaluating lottery tickets under distributional shifts."
REFERENCES,0.43,"arXiv preprint arXiv:1910.12708, 2019."
REFERENCES,0.435,"Benjamin
Ellenberger.
Pybullet
gymperium.
https://github.com/benelot/
pybullet-gym, 2018."
REFERENCES,0.44,"Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural"
REFERENCES,0.445,"networks. arXiv preprint arXiv:1803.03635, 2019."
REFERENCES,0.45,"Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M Roy, and Michael Carbin. Stabilizing the"
REFERENCES,0.455,"lottery ticket hypothesis. arXiv preprint arXiv:1903.01611, 2019."
REFERENCES,0.46,"Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M Roy, and Michael Carbin. Linear mode"
REFERENCES,0.465,"connectivity and the lottery ticket hypothesis. arXiv preprint arXiv:1912.05671, 2020a."
REFERENCES,0.47,"Jonathan Frankle, David J Schwab, and Ari S Morcos. The early phase of neural network training."
REFERENCES,0.475,"arXiv preprint arXiv:2002.10365, 2020b."
REFERENCES,0.48,"Sharath Girish, Shishira R Maiya, Kamal Gupta, Hao Chen, Larry Davis, and Abhinav Shrivastava."
REFERENCES,0.485,"The lottery ticket hypothesis for object recognition. arXiv preprint arXiv:2012.04643, 2020."
REFERENCES,0.49,Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward neural
REFERENCES,0.495,"networks. In Proceedings of the thirteenth international conference on artiﬁcial intelligence and
statistics, pp. 249–256. JMLR Workshop and Conference Proceedings, 2010."
REFERENCES,0.5,"Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for"
REFERENCES,0.505,"efﬁcient neural networks. arXiv preprint arXiv:1506.02626, 2015."
REFERENCES,0.51,"Charles R Harris, K Jarrod Millman, St´efan J van der Walt, Ralf Gommers, Pauli Virtanen, David"
REFERENCES,0.515,"Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J Smith, et al. Array program-
ming with numpy. Nature, 585(7825):357–362, 2020."
REFERENCES,0.52,"Trevor Hastie, Robert Tibshirani, and Martin Wainwright. Statistical learning with sparsity: the"
REFERENCES,0.525,"lasso and generalizations. Chapman and Hall/CRC, 2019."
REFERENCES,0.53,Published as a conference paper at ICLR 2022
REFERENCES,0.535,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing"
REFERENCES,0.54,"human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE international
conference on computer vision, pp. 1026–1034, 2015."
REFERENCES,0.545,"John D Hunter. Matplotlib: A 2d graphics environment. IEEE Annals of the History of Computing,"
REFERENCES,0.55,"9(03):90–95, 2007."
REFERENCES,0.555,"Rahul Mehta. Sparse transfer learning via winning lottery tickets. arXiv preprint arXiv:1905.07785, 2019."
REFERENCES,0.56,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-"
REFERENCES,0.565,"mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. nature, 518(7540):529–533, 2015."
REFERENCES,0.57,"Ari Morcos, Haonan Yu, Michela Paganini, and Yuandong Tian. One ticket to win them all: gener-"
REFERENCES,0.575,"alizing lottery ticket initializations across datasets and optimizers. In Advances in Neural Infor-
mation Processing Systems, pp. 4933–4943, 2019."
REFERENCES,0.58,"Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,"
REFERENCES,0.585,"Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
Automatic differentiation in
pytorch. 2017."
REFERENCES,0.59,"Andrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins, James Kirk-"
REFERENCES,0.595,"patrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell. Policy distil-
lation. arXiv preprint arXiv:1511.06295, 2015."
REFERENCES,0.6,"Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv"
REFERENCES,0.605,"preprint arXiv:1511.05952, 2015."
REFERENCES,0.61,"Simon Schmitt, Jonathan J Hudson, Augustin Zidek, Simon Osindero, Carl Doersch, Wojciech M"
REFERENCES,0.615,"Czarnecki, Joel Z Leibo, Heinrich Kuttler, Andrew Zisserman, Karen Simonyan, et al. Kickstart-
ing deep reinforcement learning. arXiv preprint arXiv:1803.03835, 2018."
REFERENCES,0.62,"John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region"
REFERENCES,0.625,"policy optimization. In International conference on machine learning, pp. 1889–1897. PMLR,
2015."
REFERENCES,0.63,"John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy"
REFERENCES,0.635,"optimization algorithms. arXiv preprint arXiv:1707.06347, 2017."
REFERENCES,0.64,Adam Stooke and Pieter Abbeel. rlpyt: A research code base for deep reinforcement learning in
REFERENCES,0.645,"pytorch. arXiv preprint arXiv:1909.01500, 2019."
REFERENCES,0.65,"Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-"
REFERENCES,0.655,"learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 30, 2016."
REFERENCES,0.66,"Michael L Waskom. Seaborn: statistical data visualization. Journal of Open Source Software, 6"
REFERENCES,0.665,"(60):3021, 2021."
REFERENCES,0.67,Kenny Young and Tian Tian. Minatar: An atari-inspired testbed for thorough and reproducible
REFERENCES,0.675,"reinforcement learning experiments. arXiv preprint arXiv:1903.03176, 2019."
REFERENCES,0.68,"Haonan Yu, Sergey Edunov, Yuandong Tian, and Ari S Morcos. Playing the lottery with rewards"
REFERENCES,0.685,"and multiple languages: lottery tickets in rl and nlp. arXiv preprint arXiv:1906.02768, 2019."
REFERENCES,0.69,Published as a conference paper at ICLR 2022
REFERENCES,0.695,"SUPPLEMENTARY INFORMATION: ON LOTTERY TICKETS AND MINIMAL
TASK REPRESENTATIONS IN DEEP REINFORCEMENT LEARNING"
REFERENCES,0.7,"A
ADDITIONAL RESULTS"
REFERENCES,0.705,"A.1
MINATAR DISTILLATION AND DQN RESULTS - MLP AND CNN-BASED AGENTS"
REFERENCES,0.71,"To test the robustness of the lottery ticket phenomenon to different architectures and diverse tasks,
we repeat the baseline comparison distillation experiments for the MinAtar environments (see ﬁgure
9). We trained MLP- and CNN-based agents to distill experts’ value estimators and using the same
architecture and hyperparameters for all considered games. For MLP agents the mask consistently
contributes most to the the ticket. For CNN-based agents and selected games (Asterix and Space
Invaders) the weight initialization contributes more."
REFERENCES,0.715,"Figure 9: Lottery tickets in supervised policy distillation (MinAtar environments, Young & Tian,
2019). We ﬁnd evidence for a strong contribution of the IMP-identiﬁed mask to the overall ticket
effect. The qualitative baseline comparison generalizes from MLP- to CNN-based agents. Top.
Disentangling ticket baselines for MLP-based value function estimators across four MinAtar games.
Bottom. Disentangling ticket baselines for CNN-based value function estimators across four Mi-
nAtar games. The results are averaged over 5 independent runs for all MinAtar environments. We
plot mean best performance and one standard deviation."
REFERENCES,0.72,"Figure 10: Tickets in off-policy deep reinforcement learning (MinAtar environments, Young & Tian,
2019). We ﬁnd evidence for a strong contribution of the IMP-identiﬁed mask to the overall ticket
effect. The qualitative baseline comparison generalizes from MLP- to CNN-based agents. Top.
Disentangling ticket baselines for MLP-based value function estimators across four MinAtar games.
Bottom. Disentangling ticket baselines for CNN-based value function estimators across four Mi-
nAtar games. The results are averaged over 5 independent runs for all MinAtar environments. We
plot mean best performance and one standard deviation."
REFERENCES,0.725,"Strengthening an observation in Yu et al. (2019), we observe that the performance deteriorates at
different levels of network sparsity depending on the considered game. Freeway agents keep per-"
REFERENCES,0.73,Published as a conference paper at ICLR 2022
REFERENCES,0.735,"forming well even for high levels of sparsity, while agents trained on Breakout and Space Invaders
continually get worse as the sparsity level increases. In general we ﬁnd that the qualitative results
obtained for MLP agents generalize well to CNN-based agents. The only major difference is that
unlike the Asterix CNN agent, the MLP agent improves their performance at moderate levels of
sparsity. In summary, we provide further consistent evidence for the strong contribution of the mask
to the lottery ticket effect in DRL (both on-policy and off-policy algorithms). The results gener-
alize between different architectures indicating that the strength of the overall lottery ticket effect
is mainly dictated by the combination of the task-speciﬁcation of the environment and the DRL
algorithm."
REFERENCES,0.74,"A.2
THE EF FECT OF NETWORK CAPACITY ON THE ABSOLUTE SIZE OF WINNING TICKETS"
REFERENCES,0.745,"Figure 11: Effect of network size on lottery ticket effect in supervised behavioral cloning and deep
reinforcement learning. Top. The initial network size has no inﬂuence on relative performance of
tickets and permuted/permuted baselines for supervised behavioral cloning. Larger networks do not
yield tickets that outperform those generated from smaller networks for a given absolute number of
remaining weights. Bottom. Initial network size comparison for tickets in on- and off-policy DRL.
Again, larger initial network size does not lead to more performant tickets. The results are averaged
over 5 independent runs on the GridMaze environment and 15 independent runs on the Cart-Pole
and Acrobot environments. We plot mean best performance and one standard deviation."
REFERENCES,0.75,"The lottery ticket hypothesis suggests that using a larger original network size increases the number
of sub-networks which may turn out to be winning tickets (Frankle & Carbin, 2019). To investigate
this hypothesis for the case of policy distillation, we analyzed the effect of the initial network size on
the lottery ticket effect (ﬁgure 2, right column). Against this initial intuition, we observe that smaller
dense networks are capable of maintaining strong performance at higher levels of absolute sparsity
as compared to their larger counterparts. Furthermore, the initial network size does not have a
strong effect on the relative performance gap between the ticket conﬁguration (mask/weights) and the
baseline (permuted/permuted). We suspect that larger networks can not realize their combinatorial
potential due to a an unfavorable layer-wise pruning bias introduced by initialization schemes such
as the Kaiming family (He et al., 2015). An imbalance between input size and hidden layer size can
have strong impact on which weights are targeted by IMP. We further investigate this relationship in
section 4.2."
REFERENCES,0.755,Published as a conference paper at ICLR 2022
REFERENCES,0.76,"Figure 12: In the main text, agents were trained on GridMaze using the DQN algorithm and on
Cart-Pole using PPO. Here, we report the performance of PPO-trained agents on the GridMaze task
(left) and of DQN-trained agents on the cart-pole task (right) for different network sizes."
REFERENCES,0.765,"A.3
THE EF FECT OF DIF FERENT REPRESENTATIONS ON LOTTERY TICKETS IN DRL"
REFERENCES,0.77,"Figure 13: Performance of agents trained on an RGB-encoded GridMaze task (left) and on a ran-
domly projected, entangled representation (right). The derived mask robustly contributes most to
the ticket. More information can be found in section B."
REFERENCES,0.775,"A.4
THE EF FECT OF REGULARIZATION AND LATE REWINDING ON LOTTERY TICKETS IN
DRL"
REFERENCES,0.78,"Figure 14: Left. Lottery ticket plot with and without L2 weight decay (λ = 0.1). Using weight
decay does not impair the ticket phenomenon for a DQN agent. Middle. Lottery ticket plot with
and without dropout in all layers (p = 0.1). Dropout deteriorates overall performance at all levels
of sparsity, but does not impair the ticket effect for a DQN agent. Right. Late rewinding (Frankle
et al., 2019) to different stages of training (0, 100k, 400k, 4000k environment steps)."
REFERENCES,0.785,Published as a conference paper at ICLR 2022
REFERENCES,0.79,"A.5
PYBULLET DEEP RL PPO EXPERIMENTS WITHOUT PRUNING OF THE CRITIC"
REFERENCES,0.795,"Figure 15: Tickets in on-policy deep reinforcement learning and when pruning only the actor mod-
ule. The random re-sampling baseline in this case outperforms permuting both the weights and the
mask. The results are averaged over 10 runs. We plot mean best performance and one standard
deviation."
REFERENCES,0.8,"B
MAZEGRID ENVIRONMENT"
REFERENCES,0.805,"The MazeGrid is a visual navigation task: The agent navigates a grid environment, which is ten
pixels high and twenty pixels wide. Each location holds a single unique object. There are six types
of objects: empty background (black), walls (grey), the agent (cyan), two moving enemies (magenta
and green), as well as 42 coins (yellow) and twelve poisons (brown) (visualized in ﬁgure 1, bottom
row, left column). The layout of the game is the same in every episode. The agent can walk in four
directions (up, down, left, right). The enemies patrol horizontally inside the gaps in the wall. A full
game in motion can be watched in the project repository, which will be released after publication.
A game terminates after 200 timesteps, or if the agent collects (walks over) all coins, or if the
agent is in the same location as an enemy. Each collected coin yields a reward of plus one, each
collected poison yields minus one, presented immediately to the agent. To ensure our results do not
depend on the speciﬁc encoding of the environment, we compared three different representation of
the MazeGrid environment (section A.3):"
REFERENCES,0.81,• The object-map encoding consists of separate one-hot maps for each type of object. Empty
REFERENCES,0.815,"space is encoded explicitly by a separate map, walls however do not have their own map
and are only implicitly represented by the lack of any other object. The representation thus
contains six maps, ten by twenty pixels each for a total of 1200 binary values for each state."
REFERENCES,0.82,"• The RGB encoding consists of the canonical three color channels for each location, result-"
REFERENCES,0.825,"ing in 600 integer values in range [0, 255]. Losing information due to occlusion is not an
issue in this environment since any location can only hold a single object at a time."
REFERENCES,0.83,• The entangled encoding is derived from the object-map encoding by ﬂattening all values
REFERENCES,0.835,"into a vector and multiplying it with a pseudo-random 1200 by 1200 matrix. The matrix’s
values are sampled independently from U(−1, 1). The matrix remains constant across all
IMP iterations, every seed has its own matrix."
REFERENCES,0.84,Published as a conference paper at ICLR 2022
REFERENCES,0.845,"All experiments in the main text were conducted on the object-map environment. Figure 13 provides
a comprehensive overview over the baselines to demonstrate that tickets do rely on one speciﬁc
encoding.The importance of the mask is even further highlighted by our results on RGB speciﬁcally."
REFERENCES,0.85,"C
HYPERPARAMETER SETTINGS FOR REPRODUCTION"
REFERENCES,0.855,"All simulations were implemented in Python using the rlpyt DRL training package (Stooke &
Abbeel, 2019, MIT License) and PyTorch pruning utilities (Paszke et al., 2017). The environments
were implented by the OpenAI gym (Brockman et al., 2016, MIT License), MinAtar (Young &
Tian, 2019, GPL-3.0 License), PyBullet gym (Ellenberger, 2018) packages and the ALE benchmark
environment (Bellemare et al., 2013). Furthermore, all visualizations were done using Matplotlib
(Hunter, 2007) and Seaborn (Waskom, 2021, BSD-3-Clause License). Finally, the numerical analy-
sis was supported by NumPy (Harris et al., 2020, BSD-3-Clause License). We will release the code
after the publication of the paper. The simulations were conducted on a CPU cluster and no GPUs
were used. Each individual IMP run required between 8 (Cart-Pole and Acrobot), 10 (MazeGrid,
MinAtar) and 20 cores (PyBullet and ATARI environments). Depending on the setting, a for lottery
ticket experiment of 20 to 30 iterations lasts between 2 hours (Cart-Pole) and 5 (ATARI games) days
of training time."
REFERENCES,0.86,"C.1
CART-POLE - BEHAVIORAL CLONING & PPO"
REFERENCES,0.865,"Parameter
Value
Student Network Size
128,128 units and 256,256 units
Teacher Network Size
64,64 units and 128,128 units
Learning Rate
0.001 (Adam)
Training Environment Steps
10.000
Number of workers
4
Distillation Loss
Cross-entropy expert-student policies"
REFERENCES,0.87,"Table 2: Hyperparameters for the BC algorithm on Cart-Pole. Results reported in ﬁg. 2, 3 and 11."
REFERENCES,0.875,"Parameter
Value
Parameter
Value
Optimizer
Adam
Value Loss Coeff.
0.5
Learning Rate
0.0005
Entropy Loss Coeff.
0.001
Temporal Discount Factor
0.99
Likelihood Ratio Clip
0.2
Training Environment Steps
80.000
Number of workers
4
GAE λ
0.8
Number of epochs
4"
REFERENCES,0.88,"Table 3: Hyperparameters for the PPO algorithm on Cart-Pole. Results reported in ﬁg. 2, 8 and 11."
REFERENCES,0.885,"C.2
ACROBOT - BEHAVIORAL CLONING & PPO"
REFERENCES,0.89,"Parameter
Value
Student Network Size
128,64 and 256,128 and 512,256 units
Teacher Network Size
128,64 units
Learning Rate
0.0005 (Adam)
Training Environment Steps
200.000
Number of workers
4
Distillation Loss
Cross-entropy expert-student policies"
REFERENCES,0.895,"Table 4: Hyperparameters for the BC algorithm on Acrobot. Results reported in ﬁg. 2, 3 and 11."
REFERENCES,0.9,Published as a conference paper at ICLR 2022
REFERENCES,0.905,"Parameter
Value
Parameter
Value
Optimizer
Adam
Value Loss Coeff.
0.5
Learning Rate
0.0005
Entropy Loss Coeff.
0.01
Temporal Discount Factor
0.99
Likelihood Ratio Clip
0.2
Training Environment Steps
500.000
Number of workers
4
GAE λ
0.95
Number of epochs
4"
REFERENCES,0.91,"Table 5: Hyperparameters for the PPO algorithm on Acrobot. Results reported in ﬁg. 2, 8 and 11."
REFERENCES,0.915,"C.3
PYBULLET CONTINUOUS CONTROL - BEHAVIORAL CLONING & PPO"
REFERENCES,0.92,"Parameter
Value
Student Network Size
64,64 Actor and Critic
Teacher Network Size
64,64 units
Learning Rate
0.0005 (Adam)
Training Environment Steps
500.000
Number of workers
10
Distillation Loss
KL divergence expert-student policies"
REFERENCES,0.925,"Table 6: Hyperparameters for the BC algorithm on PyBullet. Results reported in ﬁg. 2, and 3."
REFERENCES,0.93,"Parameter
Value
Parameter
Value
Optimizer
Adam
Value Loss Coeff.
0.5
Learning Rate
0.0005
Entropy Loss Coeff.
0.001
Temporal Discount Factor
0.99
Likelihood Ratio Clip
0.2
Training Environment Steps
1.500.000
Number of workers
10
GAE λ
0.98
Number of epochs
10"
REFERENCES,0.935,Table 7: Hyperparameters for the PPO algorithm on PyBullet. Results reported in ﬁg. 4 and 7.
REFERENCES,0.94,"C.4
GRIDMAZE - DQN"
REFERENCES,0.945,"Parameter
Value
Parameter
Value
Optimizer
Adam
Replay Buffer Size
100.000
Learning Rate
0.0005
Replay Buffer α
0.6
Temporal Discount Factor
0.99
Replay Buffer β (init.)
0.4
Batch Size
256
Replay Buffer β (ﬁnal)
1
Huber Loss δ
1.0
Data Replay Ratio
4
Clip Grad. Norm
10
Training Environment Steps
5.000.000
ϵstart
1
ϵfinal
0.01
#ϵ Annealing Frames
1.000.000
ϵeval
0.001"
REFERENCES,0.95,Table 8: Hyperparameters for the DQN algorithm on GridMaze. Results reported in ﬁg. 4.
REFERENCES,0.955,Published as a conference paper at ICLR 2022
REFERENCES,0.96,"C.5
ATARI - PPO"
REFERENCES,0.965,"Parameter
Value
Parameter
Value
Optimizer
Adam
Value Loss Coeff.
0.5
Learning Rate
0.0005
Entropy Loss Coeff.
0.001
Temporal Discount Factor
0.99
Likelihood Ratio Clip
0.2
Training Environment Steps
2.500.000
Number of workers
10
GAE λ
0.98
Number of epochs
10"
REFERENCES,0.97,Table 9: Hyperparameters for the PPO algorithm on ATARI. Results reported in ﬁg. 8.
REFERENCES,0.975,"C.6
MINATAR - BEHAVIORAL CLONING & DQN FOR MLP/CNN AGENTS"
REFERENCES,0.98,"Parameter
Value
Student Network Size
1024,512 units (MLP) and Conv2D-16x3, 128-64 (CNN)
Teacher Network Size
512,256 units (MLP) and Conv2D-16x3, 128-64 (CNN)
Learning Rate
0.0005
Training Environment Steps
10.000.000 (MLP) and 5.000.000 (CNN)
Independent Seeds
5
Distillation Loss
Huber loss of expert-student value estimates"
REFERENCES,0.985,Table 10: Hyperparameters for the BC algorithm on MinAtar. Results reported in ﬁg. 9.
REFERENCES,0.99,"Parameter
Value
Parameter
Value
Optimizer
Adam
Replay Buffer Size
100.000
Learning Rate
0.00025
Replay Buffer α
0.6
Temporal Discount Factor
0.99
Replay Buffer β (init.)
0.4
Batch Size
256
Replay Buffer β (ﬁnal)
1
Huber Loss δ
1.0
Data Replay Ratio
4
Clip Grad. Norm
10
Training Environment Steps
15.000.00
ϵstart
1
ϵfinal
0.05
#ϵ Annealing Frames
7.500.000
ϵeval
0.001"
REFERENCES,0.995,Table 11: Hyperparameters for the DQN algorithm on MinAtar. Results reported in ﬁg. 10.
