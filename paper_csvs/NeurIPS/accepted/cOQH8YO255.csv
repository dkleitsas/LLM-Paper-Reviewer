Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0033222591362126247,"This paper investigates model robustness in reinforcement learning (RL) via the
framework of distributionally robust Markov decision processes (RMDPs). Despite
recent efforts, the sample complexity of RMDPs is much less understood regardless
of the uncertainty set in use; in particular, there exist large gaps between existing
upper and lower bounds, and it is unclear if distributional robustness bears any
statistical implications when benchmarked against standard RL. In this paper,
assuming access to a generative model, we derive the sample complexity of RMDPs
— when the uncertainty set is measured via either total variation or χ2 divergence
over the full range of uncertainty levels — using a model-based algorithm called
distributionally robust value iteration, and develop minimax lower bounds to
benchmark its tightness. Our results not only strengthen the prior art in both
directions of upper and lower bounds, but also deliver surprising messages that
learning RMDPs is not necessarily easier or more difficult than standard MDPs. In
the case of total variation, we establish the minimax-optimal sample complexity
of RMDPs which is always smaller than that of standard MDPs. In the case of
χ2 divergence, we establish the sample complexity of RMDPs that is tight up to
polynomial factors of the effective horizon, and grows linearly with respect to the
uncertainty level when it approaches infinity."
INTRODUCTION,0.006644518272425249,"1
Introduction"
INTRODUCTION,0.009966777408637873,"Reinforcement learning (RL) deals with the problem of learning to make sequential decisions based on
trial-and-error interactions with some unknown environment. As a fast-growing subfield of artificial
intelligence, it has achieved significant success in a variety of domains such as large language model
alignment (OpenAI, 2023; Ziegler et al., 2019), healthcare (Liu et al., 2019; Fatemi et al., 2021),
robotics and control (Kober et al., 2013; Mnih et al., 2013). Due to the high dimensionality of the
state-action space, achieving sample efficiency lies at the core of modern RL practice, especially
in various sample-starved applications. As a result, a large portion of efforts in RL has been put in
designing sample-efficient algorithms and understanding the fundamental statistical difficulty for
diverse RL problems (Azar et al., 2013; Li et al., 2020)."
INTRODUCTION,0.013289036544850499,"While standard RL has been heavily invested recently, its use can be significantly hampered in
practice due to the sim-to-real gap, where an agent trained in an ideal, nominal environment might
be extremely sensitive and fail catastrophically when the deployed environment is subject to small
changes in task objectives or unexpected perturbations (Zhang et al., 2020a; Klopp et al., 2017;
Mahmood et al., 2018). Consequently, in addition to maximizing the long-term cumulative reward,"
INTRODUCTION,0.016611295681063124,"∗Department of Computing Mathematical Sciences, California Institute of Technology, CA, USA.
†Department of Statistics, The Chinese University of Hong Kong, Hong Kong, China.
‡Department of Statistics and Data Science, Wharton School, University of Pennsylvania, PA, USA.
§Google Research, Brain Team, Paris, France.
¶Department of Electrical and Computer Engineering, Carnegie Mellon University, PA, USA."
INTRODUCTION,0.019933554817275746,"robustness becomes another critical goal for an RL agent, especially in high-stake applications such
as robotics, autonomous driving, clinical trials, financial investments, and so on. To address this,
distributionally robust RL (Iyengar, 2005; Nilim and El Ghaoui, 2005), which leverages insights from
distributionally robust optimization and supervised learning (Rahimian and Mehrotra, 2019; Gao,
2020; Bertsimas et al., 2018; Duchi and Namkoong, 2018; Blanchet and Murthy, 2019), becomes a
natural and versatile framework with the goal of learning a policy that performs well even when the
deployed environment deviates from the nominal one in the face of environment uncertainty."
INTRODUCTION,0.023255813953488372,"In this paper, we are particularly interested in understanding whether, and how, the choice of
distributional robustness bears statistical implications in learning the desired policy, by studying
the sample complexity in the widely-used generative model (Kearns and Singh, 1999). Suppose
that one has access to a generative model which draws samples from a Markov decision processes
(MDP) with a nominal transition kernel. Standard RL aims to learn the optimal policy tailored
for the nominal kernel based on this set of samples, where the sample complexity has been well
understood with matching upper and lower bounds developed recently (Azar et al., 2013; Li et al.,
2020). In contrast, distributionally robust RL — leveraging the same set of samples — aims to learn
the optimal robust policy whose worst-case performance is maximized when the transition kernel is
from some prescribed uncertainty set around the nominal kernel, a setting that is referred to as the
robust MDP (RMDPs).6 Clearly, this ensures that the performance of the learned policy is robust
and does not fail catastrophically as long as the sim-to-real gap is not too large. It is then natural
to wonder how the robustness consideration impacts the RL performance: should we always prefer
to learn a robust policy for a given set of samples? Is there a statistical premium when asking for
additional robustness?"
INTRODUCTION,0.026578073089700997,"Compared with standard MDPs, RMDPs is a richer class of models since one additionally needs
to prescribe the shape and size of the uncertainty set, which is usually hand-picked as a small ball
around the nominal kernel measured with respect to some distance measure ρ and uncertainty level σ.
To ensure the tractability of solving RMDPs, the uncertainty set is usually assumed to obey certain
structures, where the uncertainty set can be decomposed as a product of independent uncertainty
subsets over each state or state-action pair (Zhou et al., 2021; Wiesemann et al., 2013), denoted as
the s- and (s, a)-rectangular rectangularity respectively; in particular, our paper adopts the second
choice by assuming the uncertainty set satisfies the (s, a)-rectangularity. An additional challenge
with RMDPs arises from distribution shift, where the transition kernel drawn from the uncertainty set
can be different from the nominal kernel, leading to complicated nonlinearity and nested optimization
in the problem structure not present in standard MDPs."
PRIOR ART AND OPEN QUESTIONS,0.029900332225913623,"1.1
Prior art and open questions
In this paper, we focus on understanding the sample complexity of learning the optimal robust policy
of RMDPs in the infinite-horizon setting assuming access to a generative model, when the uncertainty
set is measured using one of the f-divergence: total variation (TV) distance and χ2 divergence. These
two choices are motivated by their practical appeal: easy to implement, and already adopted by
empirical RL (Lee et al., 2021; Pan et al., 2023)."
PRIOR ART AND OPEN QUESTIONS,0.03322259136212625,"A popular learning approach is model-based, which first estimates the nominal transition kernel using
a plug-in estimator based on the collected samples, and then runs a planning algorithm such as a
robust variant of value iteration on top of the estimated RMDP. Despite the surge of recent activities,
however, existing statistical guarantees for the above paradigm remain highly inadequate, as we shall
elaborate momentarily (see Table 1 and Table 2 respectively for a summary). For concreteness, let
S be the size of the state space, A be the size of the action space, γ be the discount factor (and the
effective horizon
1
1−γ ), and σ be the uncertainty level. We are interested in the sample complexity
— the number of samples needed for an algorithm to output a policy whose robust value function
(the worst-case value over all the transition kernels in the uncertainty set) is at most ε away from the
optimal robust one — with respect to all salient problem parameters."
PRIOR ART AND OPEN QUESTIONS,0.036544850498338874,"• Large gaps between existing upper and lower bounds. There remained large gaps between the
sample complexity upper and lower bounds established in prior literature, regardless of the divergence
metric in use. Specifically, considering the cases using either TV distance or χ2 divergence, the
state-of-the-art upper bounds (Panaganti and Kalathil, 2022) scales quadratically with the size S of the"
PRIOR ART AND OPEN QUESTIONS,0.03986710963455149,"6While it is straightforward to incorporate additional uncertainty of the reward in our framework, we do not
consider it here for simplicity, since the key challenge is to deal with the uncertainty of the transition kernel."
PRIOR ART AND OPEN QUESTIONS,0.04318936877076412,"Result type
Reference"
PRIOR ART AND OPEN QUESTIONS,0.046511627906976744,Sample complexity
PRIOR ART AND OPEN QUESTIONS,0.04983388704318937,"0 < σ ≲1 −γ
1 −γ ≲σ < 1"
PRIOR ART AND OPEN QUESTIONS,0.053156146179401995,Upper bound
PRIOR ART AND OPEN QUESTIONS,0.05647840531561462,"Yang et al. (2022)
S2A(2+σ)2"
PRIOR ART AND OPEN QUESTIONS,0.059800664451827246,σ2(1−γ)4ε2
PRIOR ART AND OPEN QUESTIONS,0.06312292358803986,"Panaganti and Kalathil (2022)
S2A
(1−γ)4ε2"
PRIOR ART AND OPEN QUESTIONS,0.0664451827242525,"Ours
SA
(1−γ)3ε2"
PRIOR ART AND OPEN QUESTIONS,0.06976744186046512,"SA
(1−γ)2σε2"
PRIOR ART AND OPEN QUESTIONS,0.07308970099667775,Lower bound
PRIOR ART AND OPEN QUESTIONS,0.07641196013289037,"Yang et al. (2022)
SA
(1−γ)3ε2
SA(1−γ) σ4ε2"
PRIOR ART AND OPEN QUESTIONS,0.07973421926910298,"Ours
SA
(1−γ)3ε2"
PRIOR ART AND OPEN QUESTIONS,0.08305647840531562,"SA
(1−γ)2σε2"
PRIOR ART AND OPEN QUESTIONS,0.08637873754152824,"Table 1: Comparisons between our results and prior arts for finding an ε-optimal robust policy in the
infinite-horizon RMDPs with an uncertainty set measured with respect to the TV distance, where
we ignore logarithmic factors in the sample complexities. Here, S, A, γ, and σ ∈(0, 1) are the state
space size, the action space size, the discount factor, and the uncertainty level, respectively."
PRIOR ART AND OPEN QUESTIONS,0.08970099667774087,Upper & minimax lower bound
PRIOR ART AND OPEN QUESTIONS,0.09302325581395349,(this work)
PRIOR ART AND OPEN QUESTIONS,0.09634551495016612,"0
O(1)
σ"
PRIOR ART AND OPEN QUESTIONS,0.09966777408637874,"Standard MDPs
upper & minimax lower bound"
PRIOR ART AND OPEN QUESTIONS,0.10299003322259136,Sample complexity
PRIOR ART AND OPEN QUESTIONS,0.10631229235880399,"SA
(1 −γ)3""2"
PRIOR ART AND OPEN QUESTIONS,0.10963455149501661,"SA
(1 −γ)2""2"
PRIOR ART AND OPEN QUESTIONS,0.11295681063122924,"SA(1 −γ) ""2"
PRIOR ART AND OPEN QUESTIONS,0.11627906976744186,Lower bound [Yang et al.]
PRIOR ART AND OPEN QUESTIONS,0.11960132890365449,"SA(1 −γ) σ4""2"
PRIOR ART AND OPEN QUESTIONS,0.12292358803986711,"SA
(1 −γ)2""2σ"
PRIOR ART AND OPEN QUESTIONS,0.12624584717607973,"1
O(1 −γ)"
PRIOR ART AND OPEN QUESTIONS,0.12956810631229235,"Upper bound
[Panaganti and Kalathil]
S2A
(1 −γ)4""2"
PRIOR ART AND OPEN QUESTIONS,0.132890365448505,"Lower bound [Yang et al.] 0
σ"
PRIOR ART AND OPEN QUESTIONS,0.1362126245847176,Sample complexity
PRIOR ART AND OPEN QUESTIONS,0.13953488372093023,"SA
(1 −γ)3""2"
PRIOR ART AND OPEN QUESTIONS,0.14285714285714285,"SA
(1 −γ)4""2"
PRIOR ART AND OPEN QUESTIONS,0.1461794019933555,Lower bound
PRIOR ART AND OPEN QUESTIONS,0.14950166112956811,(this work)
PRIOR ART AND OPEN QUESTIONS,0.15282392026578073,"SA
(1 −γ)2""2σ"
PRIOR ART AND OPEN QUESTIONS,0.15614617940199335,Upper bound
PRIOR ART AND OPEN QUESTIONS,0.15946843853820597,(this work)
PRIOR ART AND OPEN QUESTIONS,0.16279069767441862,"SA
(1 −γ)""2"
PRIOR ART AND OPEN QUESTIONS,0.16611295681063123,"O(1 −γ)
O(1)
O !"
PRIOR ART AND OPEN QUESTIONS,0.16943521594684385,"1/(1 −γ) """
PRIOR ART AND OPEN QUESTIONS,0.17275747508305647,"Standard MDPs
upper & minimax lower bound SAσ ""2"
PRIOR ART AND OPEN QUESTIONS,0.1760797342192691,"SAσ
(1 −γ)4""2"
PRIOR ART AND OPEN QUESTIONS,0.17940199335548174,"SAσ
(1 −γ)4(1 + σ)4"
PRIOR ART AND OPEN QUESTIONS,0.18272425249169436,"S2A
(1 −γ)4""2"
PRIOR ART AND OPEN QUESTIONS,0.18604651162790697,"Upper bound
[Panaganti and Kalathil]"
PRIOR ART AND OPEN QUESTIONS,0.1893687707641196,"S2Aσ
(1 −γ)4""2"
PRIOR ART AND OPEN QUESTIONS,0.19269102990033224,"(a) TV distance
(b) χ2 distance"
PRIOR ART AND OPEN QUESTIONS,0.19601328903654486,"Figure 1: Illustrations of the obtained sample complexity upper and lower bounds for learning
RMDPs with comparisons to state-of-the-art and the sample complexity of standard MDPs, where
the uncertainty set is specified using the TV distance (a) and the χ2 distance (b)."
PRIOR ART AND OPEN QUESTIONS,0.19933554817275748,"state space, while the lower bound (Yang et al., 2022) exhibits only linear scaling with S. Moreover,
in the χ2 divergence case, the state-of-the-art upper bound grows linearly with the uncertainty level σ
when σ ≳1,7 while the lower bound (Yang et al., 2022) is inversely proportional to σ. These lead to
unbounded gaps between the upper and lower bounds as σ grows. Can we hope to close these gaps
for RMDPs?"
PRIOR ART AND OPEN QUESTIONS,0.2026578073089701,"• Benchmarking with standard MDPs. Perhaps a more pressing issue is that, past works failed to
provide an affirmative answer regarding how to benchmark the sample complexity of RMDPs with
that of standard MDPs regardless of the chosen shape (determined by ρ) or size (determined by σ) of
the uncertainty set, given the large unresolved gaps mentioned above. Specifically, existing sample
complexity upper (resp. lower) bounds are all larger (resp. smaller) than the sample size requirement
for standard MDPs. As a consequence, it remains mostly unclear whether learning RMDPs is harder
or easier than learning standard MDPs."
MAIN CONTRIBUTIONS,0.2059800664451827,"1.2
Main contributions
To address these questions, we have developed new upper bounds on learning RMDPs with TV
distance and χ2 distance in the infinite-horizon setting using the model-based approach called
distributionally robust value iteration (DRVI), as well as minimax lower bounds to help gauge their"
MAIN CONTRIBUTIONS,0.20930232558139536,"7Let X :=
 
S, A,
1
1−γ , σ, 1 ε, 1"
MAIN CONTRIBUTIONS,0.21262458471760798,"δ
. The notation f(X) = O(g(X)) or f(X) ≲g(X) indicates that there
exists a universal constant C1 > 0 such that f ≤C1g, the notation f(X) ≳g(X) indicates that g(X) =
O(f(X)), and the notation f(X) ≍g(X) indicates that f(X) ≲g(X) and f(X) ≳g(X) hold simultaneously.
Additionally, the notation eO(·) is defined in the same way as O(·) except that it hides logarithmic factors."
MAIN CONTRIBUTIONS,0.2159468438538206,"Result type
Reference"
MAIN CONTRIBUTIONS,0.21926910299003322,Sample complexity
MAIN CONTRIBUTIONS,0.22259136212624583,"0 < σ ≲1 −γ
1 −γ ≲σ ≲
1
1−γ
σ ≳
1
1−γ"
MAIN CONTRIBUTIONS,0.22591362126245848,Upper bound
MAIN CONTRIBUTIONS,0.2292358803986711,"Panaganti and Kalathil (2022)
S2A(1+σ)"
MAIN CONTRIBUTIONS,0.23255813953488372,(1−γ)4ε2
MAIN CONTRIBUTIONS,0.23588039867109634,"Yang et al. (2022)
S2A(1+σ)2"
MAIN CONTRIBUTIONS,0.23920265780730898,(√1+σ−1)2(1−γ)4ε2
MAIN CONTRIBUTIONS,0.2425249169435216,"Ours
SA(1+σ)
(1−γ)4ε2"
MAIN CONTRIBUTIONS,0.24584717607973422,Lower bound
MAIN CONTRIBUTIONS,0.24916943521594684,"Yang et al. (2022)
SA
(1−γ)3ε2
SA
(1−γ)2σε2"
MAIN CONTRIBUTIONS,0.25249169435215946,"Ours
SA
(1−γ)3ε2
SAσ
(1−γ)4(1+σ)4ε2
SAσ ε2"
MAIN CONTRIBUTIONS,0.2558139534883721,"Table 2: Comparisons between our results and prior arts for finding an ε-optimal robust policy in the
infinite-horizon RMDPs with an uncertainty set measured with respect to the χ2 distance, where we
ignore logarithmic factors in the sample complexities. Here, S, A, γ, and σ ∈(0, ∞) are the state
space size, the action space size, the discount factor, and the uncertainty level, respectively."
MAIN CONTRIBUTIONS,0.2591362126245847,"tightness and provide benchmarking with standard MDPs. As shall be outlined, these new analyses
lead to new insights on the interplay between the geometry of uncertainty sets and the statistical
sample complexity."
MAIN CONTRIBUTIONS,0.26245847176079734,"Sample complexity of RMDPs under the TV distance. We summarize the results and comparisons
to prior works in Table 1; see Figure 1(a) for an illustration."
MAIN CONTRIBUTIONS,0.26578073089701,"• Minimax-optimal sample complexity. We prove that DRVI reaches ε accuracy as soon as the sample
complexity is on the order of"
MAIN CONTRIBUTIONS,0.2691029900332226,"eO

SA
(1 −γ)2ε2 min

1
1 −γ , 1 σ "
MAIN CONTRIBUTIONS,0.2724252491694352,"for all σ ∈(0, 1), assuming that ε is small enough. In addition, a matching minimax lower bound
(modulo some logarithmic factor) is established to guarantee the tightness of the upper bound. To the
best of our knowledge, this is the first minimax-optimal sample complexity for RMDPs, which was
previously unavailable regardless of the divergence metric and uncertainty level in use and is over the
full range of the uncertainty level."
MAIN CONTRIBUTIONS,0.2757475083056478,"• RMDPs are easier than standard MDPs under the TV distance. Given the sample complexity
O

SA
(1−γ)3ε2

of standard MDPs, it can be seen that RMDPs under the TV distance is never harder
than standard MDPs, where it matches that of standard MDPs when σ ≲1 −γ, and becomes simpler
by a factor of σ/(1 −γ) when 1 −γ ≲σ < 1. Therefore, in this case, the robustness comes almost
for free since we do not need to collect more samples to reach the same accuracy."
MAIN CONTRIBUTIONS,0.27906976744186046,"Sample complexity of RMDPs under the χ2 distance. We summarize the results and comparisons
to prior works in Table 2; see Figure 1(b) for an illustration."
MAIN CONTRIBUTIONS,0.2823920265780731,"• Near-optimal sample complexity. It is established that DRVI reaches ε accuracy as soon as the
sample complexity is on the order of eO

SA(1+σ)
(1−γ)4ε2

for all σ ∈(0, ∞), which is the first sample
complexity that scales linearly with respect to the size of the state space S in the infinite-horizon
setting, breaking the quadratic scaling bottleneck presented in prior works (Panaganti and Kalathil,
2022; Yang et al., 2022). We have also developed a strengthened lower bound that is optimized by
leveraging the geometry of the uncertainty set under different ranges of σ. Comparing the two bounds,
they match at eO

SA
(1−γ)4ε2

when σ ≍1, and have a bounded gap no larger than a polynomial factor
of the effective horizon 1/(1 −γ) over the entire range of the uncertainty level, again significantly
improving prior art that exhibits an unbounded gap."
MAIN CONTRIBUTIONS,0.2857142857142857,"• RMDPs can be harder than standard MDPs under the χ2 distance. Somewhat surprisingly, the
new lower bound suggests that RMDPs in this case can be much harder than standard MDPs, at least
for certain ranges of the uncertainty level. We single out two regimes of particular interest. First,
when σ ≍1, the sample size requirement of RMDPs is eO

SA
(1−γ)4ε2

, which is provably harder than"
MAIN CONTRIBUTIONS,0.28903654485049834,"standard MDPs by a factor of the effective horizon
1
1−γ . Second, when σ ≳
1
(1−γ)3 , the lower bound
exceeds the sample complexity of standard MDPs and keeps growing linearly with the uncertainty
level σ."
MAIN CONTRIBUTIONS,0.292358803986711,"In sum, our sample complexity bounds not only strengthen the prior art in both directions of upper
and lower bounds, but also reveal new insights on how the additional consideration of distributional
robustness fundamentally changes the sample complexity of RL in a surprising manner. It turns
out that RMDPs are not necessarily harder nor easier than standard MDPs, but the answer is far
more nuanced and highly dependent on both the size and shape of the uncertainty set, which are not
elucidated in prior analyses."
PROBLEM FORMULATION,0.2956810631229236,"2
Problem formulation
In this section, we introduce the model of distributionally robust Markov decision processes (RMDPs)
focusing on the discounted infinite-horizon setting, the sampling mechanism, as well as our goal."
PROBLEM FORMULATION,0.29900332225913623,"Standard MDPs. To begin, we first introduce the standard Markov decision process (MDP), which
facilitates the understanding of RMDPs. A discounted infinite-horizon MDP is represented by
M =
 
S, A, P, γ, r

, where S = {1, · · · , S} and A = {1, · · · , A} are the finite state and action
spaces, respectively, γ ∈[0, 1) is the discount factor, P : S × A →∆(S) denotes the probability
transition kernel, and r : S × A →[0, 1] is the immediate reward function which is assumed to be
deterministic. A policy is denoted as π : S →∆(A), which specifies the action selection probability
over the action space in any state. When the policy is deterministic, we overload the notation and
refer to π(s) as the action selected by policy π in state s. To characterize the long term cumulative
reward, the value function V π,P for any policy π under the transition kernel P is defined by"
PROBLEM FORMULATION,0.3023255813953488,"∀s ∈S :
V π,P (s) := Eπ,P "" ∞
X"
PROBLEM FORMULATION,0.30564784053156147,"t=0
γtr
 
st, at
  s0 = s # ,
(1)"
PROBLEM FORMULATION,0.3089700996677741,"where the expectation is taken over the randomness of the trajectory {st, at, rt}∞
t=0 generated by
executing policy π under the transition kernel P, namely, at ∼π(st), and st+1 ∼P(· | st, at).
Similarly, the Q-function Qπ,P associated with any policy π under the transition kernel P is defined as"
PROBLEM FORMULATION,0.3122923588039867,"∀(s, a) ∈S × A :
Qπ,P (s, a) := Eπ,P "" ∞
X"
PROBLEM FORMULATION,0.31561461794019935,"t=0
γtr
 
st, at
  s0 = s, a0 = a # ,
(2)"
PROBLEM FORMULATION,0.31893687707641194,where the expectation is again taken over the randomness of the trajectory.
PROBLEM FORMULATION,0.3222591362126246,"Distributionally robust MDPs. In this work, we focus on the discounted infinite-horizon distri-
butionally robust MDP (RMDP), denoted as Mrob = {S, A, Uσ
ρ (P 0), γ, r}, where S, A, γ, r are
defined the same as those in the above standard MDP. A key distinction from the standard MDP,
is that rather than assuming a fixed transition kernel P, it postulates the transition kernel lies in
an uncertainty set Uσ
ρ (P 0) centered around a nominal kernel P 0 : S × A →∆(S), where the
uncertainty set is specified using some distance metric ρ of radius σ > 0. In particular, given the
nominal transition kernel P 0 and some uncertainty level σ, the uncertainty set—with divergence
ρ : ∆(S) × ∆(S) →R+—is specified as"
PROBLEM FORMULATION,0.32558139534883723,"Uσ
ρ (P 0) := ⊗Uσ
ρ (P 0
s,a),
Uσ
ρ (P 0) :=

Ps,a ∈∆(S) : ρ
 
Ps,a, P 0
s,a

≤σ
	
,
(3)"
PROBLEM FORMULATION,0.3289036544850498,"where we denote a vector of the transition kernel P or P 0 at state-action pair (s, a) respectively as"
PROBLEM FORMULATION,0.33222591362126247,"Ps,a := P(· | s, a) ∈R1×S,
P 0
s,a := P 0(· | s, a) ∈R1×S.
(4)"
PROBLEM FORMULATION,0.33554817275747506,"In other words, the uncertainty is imposed in a separate manner for each state-action pair, obeying
the so-called (s, a)-rectangularity (Zhou et al., 2021; Wiesemann et al., 2013). In RMDPs, we are
interested in the worst-case performance of a policy π over all the possible transition kernels in the
uncertainty set. This is measured by the robust value function V π,σ and the robust Q-function Qπ,σ
in Mrob, defined respectively as"
PROBLEM FORMULATION,0.3388704318936877,"∀(s, a) ∈S × A :
V π,σ(s) :=
inf
P ∈Uσ
ρ (P 0) V π,P (s),
Qπ,σ(s, a) :=
inf
P ∈Uσ
ρ (P 0) Qπ,P (s, a)."
PROBLEM FORMULATION,0.34219269102990035,"Optimal robust policy and robust Bellman operator. Generalizing standard MDPs, it is well-
known that there exists at least one deterministic policy that maximizes the robust value function and"
PROBLEM FORMULATION,0.34551495016611294,"the robust Q-function simultaneously (Iyengar, 2005; Nilim and El Ghaoui, 2005). Therefore, we
denote the optimal robust value function (resp. optimal robust Q-function) as V ⋆,σ (resp. Q⋆,σ), and
the optimal robust policy as π⋆, which satisfies"
PROBLEM FORMULATION,0.3488372093023256,"∀s ∈S :
V ⋆,σ(s) := V π⋆,σ(s) = max
π
V π,σ(s),
(5a)"
PROBLEM FORMULATION,0.3521594684385382,"∀(s, a) ∈S × A :
Q⋆,σ(s, a) := Qπ⋆,σ(s, a) = max
π
Qπ,σ(s, a).
(5b)"
PROBLEM FORMULATION,0.3554817275747508,"The robust Bellman operator (Iyengar, 2005; Nilim and El Ghaoui, 2005) is denoted as T σ(·) :
RSA →RSA, which is defined as follows: for all (s, a) ∈S × A,"
PROBLEM FORMULATION,0.3588039867109635,"T σ(Q)(s, a) := r(s, a) + γ
inf
P∈Uσ
ρ (P 0
s,a) PV,
with
V (s) := max
a
Q(s, a).
(6)"
PROBLEM FORMULATION,0.36212624584717606,"Given that Q⋆,σ is the unique fixed point of T σ, one can recover the optimal robust value function
and Q-function using a procedure termed distributionally robust value iteration—which generalizes
the standard value iteration—by recursively applying the robust Bellman operator from some fixed
initialization. In addition, this procedure converges rather fast due to the nice γ-contraction property
of T σ (Iyengar, 2005; Nilim and El Ghaoui, 2005) with respect to the ℓ∞norm."
PROBLEM FORMULATION,0.3654485049833887,"Specification of the divergence ρ. We consider two popular choices of the uncertainty set measured
in terms of the f-divergence: total variation and χ2 divergence, given respectively by"
PROBLEM FORMULATION,0.3687707641196013,"ρTV
 
Ps,a, P 0
s,a
 := 1 2"
PROBLEM FORMULATION,0.37209302325581395,"Ps,a −P 0
s,a

1 = 1 2 X"
PROBLEM FORMULATION,0.3754152823920266,"s′∈S
P 0(s′ | s, a)
1 −P(s′ | s, a)"
PROBLEM FORMULATION,0.3787375415282392,"P 0(s′ | s, a) ,
(7)"
PROBLEM FORMULATION,0.38205980066445183,"ρχ2  
Ps,a, P 0
s,a
 :=
X"
PROBLEM FORMULATION,0.3853820598006645,"s′∈S
P 0(s′ | s, a)

1 −P(s′ | s, a)"
PROBLEM FORMULATION,0.38870431893687707,"P 0(s′ | s, a)"
PROBLEM FORMULATION,0.3920265780730897,"2
.
(8)"
PROBLEM FORMULATION,0.3953488372093023,"Note that ρTV
 
Ps,a, P 0
s,a

∈[0, 1] and ρχ2  
Ps,a, P 0
s,a

∈[0, ∞) in general. As we shall see, the
two choices convey drastically different messages in the statistical complexity of RMDPs."
PROBLEM FORMULATION,0.39867109634551495,"Sampling mechanism: a generative model. Following Zhou et al. (2021); Panaganti and Kalathil
(2022), we assume the access to a generative model or a simulator (Kearns and Singh, 1999), which
allows us to collect N independent samples from the nominal kernel P 0 at each state-action pair:"
PROBLEM FORMULATION,0.4019933554817276,"∀(s, a) ∈S × A,
si,s,a
i.i.d
∼P 0(· | s, a),
i = 1, 2, · · · , N.
(9)"
PROBLEM FORMULATION,0.4053156146179402,The total sample size therefore is NSA.
PROBLEM FORMULATION,0.40863787375415284,"Goal. Given the collected samples, the task is to learn the robust optimal policy for the RMDP
with some uncertainty set Uσ
ρ (P 0) around the nominal kernel accurately using as few samples as
possible. Specifically, given some accuracy level ε > 0, the goal is to seek an ε-optimal robust policy
bπ obeying V ⋆,σ(s) −V bπ,σ(s) ≤ε for all s ∈S."
PROBLEM FORMULATION,0.4119601328903654,"3
Model-based algorithm: distributionally robust value iteration"
PROBLEM FORMULATION,0.4152823920265781,"We consider a model-based strategy, which first constructs an empirical nominal transition kernel
based on the collected samples, and then applies distributionally robust value iteration (DRVI) to
recover the optimal robust policy."
PROBLEM FORMULATION,0.4186046511627907,"Empirical nominal kernel. The empirical nominal transition kernel bP 0 ∈RSA×S can be constructed
using the empirical frequency of visits, i.e."
PROBLEM FORMULATION,0.4219269102990033,"∀(s, a) ∈S × A :
bP 0(s′ | s, a) := 1 N N
X"
PROBLEM FORMULATION,0.42524916943521596,"i=1
1

si,s,a = s′	
,
(10)"
PROBLEM FORMULATION,0.42857142857142855,"which leads to an empirical RMDP c
Mrob = {S, A, Uσ
ρ ( bP 0), γ, r}. Analogously, we can define
the corresponding robust value function (resp. robust Q-function) of policy π in c
Mrob as bV π,σ"
PROBLEM FORMULATION,0.4318936877076412,"(resp. bQπ,σ) (cf. (5)). In addition, we denote the corresponding optimal robust policy as bπ⋆and the
optimal robust value function (resp. optimal robust Q-function) as bV ⋆,σ (resp. bQ⋆,σ) (cf. (5)), which
satisfies the robust Bellman optimality equation:"
PROBLEM FORMULATION,0.43521594684385384,"∀(s, a) ∈S × A :
bQ⋆,σ(s, a) = r(s, a) + γ
inf
P∈Uσ
ρ ( b
P 0
s,a)
P bV ⋆,σ.
(11)"
PROBLEM FORMULATION,0.43853820598006643,Algorithm 1: Distributionally robust value iteration (DRVI) for infinite-horizon RMDPs.
PROBLEM FORMULATION,0.4418604651162791,"1 input: empirical nominal transition kernel bP 0; reward function r; uncertainty level σ; number of
iterations T."
PROBLEM FORMULATION,0.44518272425249167,"2 initialization: bQ0(s, a) = 0, bV0(s) = 0 for all (s, a) ∈S × A."
PROBLEM FORMULATION,0.4485049833887043,"3 for t = 1, 2, · · · , T do"
PROBLEM FORMULATION,0.45182724252491696,"4
for s ∈S, a ∈A do"
PROBLEM FORMULATION,0.45514950166112955,"5
Set bQt(s, a) according to (13);"
PROBLEM FORMULATION,0.4584717607973422,"6
for s ∈S do"
PROBLEM FORMULATION,0.46179401993355484,"7
Set bVt(s) = maxa bQt(s, a);"
PROBLEM FORMULATION,0.46511627906976744,"8 output: bQT , bVT and bπ obeying bπ(s) := arg maxa bQT (s, a)."
PROBLEM FORMULATION,0.4684385382059801,"Equipped with bP 0, define the empirical robust Bellman operator bT σ as"
PROBLEM FORMULATION,0.4717607973421927,"∀(s, a) ∈S × A : bT σ(Q)(s, a) := r(s, a) + γ
inf
P∈Uσ
ρ ( b
P 0
s,a)
PV, with V (s) := max
a
Q(s, a).
(12)"
PROBLEM FORMULATION,0.4750830564784053,"DRVI: distributionally robust value iteration.
To solve for the fixed point of bT σ, we introduce
distributionally robust value iteration (DRVI), which is summarized in Algorithm 1. Starting from
some initialization bQ0 = 0, the update rule at the t-th (t ≥1) step can be formulated as:"
PROBLEM FORMULATION,0.47840531561461797,"∀(s, a) ∈S × A :
bQt(s, a) = bT σ 
bQt−1

(s, a) = r(s, a) + γ
inf
P∈Uσ
ρ ( b
P 0
s,a)
P bVt−1,
(13)"
PROBLEM FORMULATION,0.48172757475083056,"where bVt−1(s) = maxa bQt−1(s, a) for all s ∈S. However, directly solving (13) is computationally
prohibitive since it involves optimization over an S-dimensional probability simplex at each iteration,
especially when the dimension of the state space S is prohibitive. Fortunately, in view of strong
duality (Iyengar, 2005), (13) can be equivalently solved using its dual problem, which concerns
optimizing of a scalar dual variable and thus can be solved efficiently. The specific form of the dual
problem depends on the choice of the divergence ρ, which we discuss in a more detailed version."
PROBLEM FORMULATION,0.4850498338870432,"4
Theoretical guarantees: sample complexity analyses"
PROBLEM FORMULATION,0.4883720930232558,"We now present our main results, which concern the sample complexities of learning RMDPs when
the uncertainty set is specified using the TV distance or the χ2 divergence. Surprisingly, the choice
of the uncertainty set can lead to dramatic consequence in the sample size requirement."
PROBLEM FORMULATION,0.49169435215946844,"4.1
The case of TV distance: RMDP is easier than standard MDP"
PROBLEM FORMULATION,0.4950166112956811,"We start with the case when the uncertainty set is measured via the TV distance, where Theorem 1
presents the sample complexity upper bound above which DRVI is able to find an ε-optimal robust
policy in a small number of iterations; the key challenge of the analysis is to carefully control the
robust value function V π,σ as a function of uncertainty level σ.
Theorem 1 (Upper bound using TV distance). Fix the uncertainty set Uσ
ρ (·) = Uσ
TV(·) using the TV
distance in (7). Consider any discount factor γ ∈
 1"
PROBLEM FORMULATION,0.4983388704318937,"4, 1

, uncertainty level σ ∈(0, 1), and δ ∈(0, 1).
With probability at least 1 −δ, the output bπ from Algorithm 1 with at most T = C1 log
 
N(1 −γ)
"
PROBLEM FORMULATION,0.5016611295681063,"iterations yields V ⋆,σ(s) −V bπ,σ(s) ≤ε for any ε ∈

0,
p"
PROBLEM FORMULATION,0.5049833887043189,"1/ max{1 −γ, σ}
i
, as long as the total"
PROBLEM FORMULATION,0.5083056478405316,"number of samples obeys NSA ≥
C2SA
(1−γ)2 max{1−γ,σ}ε2 log

SAN
(1−γ)δ

. Here, C1, C2 > 0 are some
large enough universal constants."
PROBLEM FORMULATION,0.5116279069767442,"Before discussing the implications of Theorem 1, we present a matching minimax lower bound
that confirms the optimality of the upper bound, which in turn pins down the sample complexity
requirement for learning RMDPs with TV distance.
Theorem 2 (Lower bound using TV distance). Consider any tuple (S, A, γ, σ, ε) obeying σ ∈(0, 1−
c0] with 0 < c0 ≤1"
PROBLEM FORMULATION,0.5149501661129569,"8 being any small enough positive constant, γ ∈
 1"
PROBLEM FORMULATION,0.5182724252491694,"2, 1

, and ε ∈
 
0,
c0
256(1−γ)

. We"
PROBLEM FORMULATION,0.521594684385382,"can construct two infinite-horizon RMDPs M0, M1 defined by the uncertainty set Uσ
ρ (·) = Uσ
TV(·),
an initial state distribution φ, and a dataset with N independent samples for each state-action pair
over the nominal transition kernel (for M0 and M1 respectively), such that"
PROBLEM FORMULATION,0.5249169435215947,"inf
bπ max
n
P0
 
V ⋆,σ(φ) −V bπ,σ(φ) > ε

, P1
 
V ⋆,σ(φ) −V bπ,σ(φ) > ε
o
≥1 8,"
PROBLEM FORMULATION,0.5282392026578073,"provided that NSA ≤
c0SA log 2
8192(1−γ)2 max{1−γ,σ}ε2 . The infimum is taken over all estimators bπ, and P0
(resp. P1) denotes the probability when the RMDP is M0 (resp. M1)."
PROBLEM FORMULATION,0.53156146179402,"Below, we interpret the above theorems and highlight several key implications about the sample
complexity requirements for learning RMDPs with TV distance."
PROBLEM FORMULATION,0.5348837209302325,"Near minimax-optimal sample complexity.
Theorem 1 shows that the total number of samples
required for DRVI to yield ε-accuracy is"
PROBLEM FORMULATION,0.5382059800664452,"eO

SA
(1 −γ)2 max{1 −γ, σ}ε2"
PROBLEM FORMULATION,0.5415282392026578,"
.
(14)"
PROBLEM FORMULATION,0.5448504983388704,"Taking together with the minimax lower bound asserted in Theorem 2, this confirms the near minimax-
optimality of the sample complexity up to some logarithmic factor almost over the full range of the
uncertainty level σ, which scales linearly with respect to the size of the state-action space."
PROBLEM FORMULATION,0.5481727574750831,"RMDPs is easier than standard MDPs with TV distance.
Recall that the sample complex-
ity requirement for standard MDP (Agarwal et al., 2020; Li et al., 2020) to yield ε accuracy is
eO

SA
(1−γ)3ε2

. Comparing with the sample complexity requirement in (14) for RMDPs with TV
distance, this confirms that the latter is at least as easy as—if not easier—than standard MDPs. In
particular, when σ ≲1 −γ is small, the sample complexity of RMDPs is the same as the standard
MDPs, which is expected since the RMDP reduces to the standard MDP when σ = 0. On the other
hand, when 1 −γ ≲σ < 1, the sample complexity of RMDPs becomes eO

SA
(1−γ)2σε2

, which is"
PROBLEM FORMULATION,0.5514950166112956,smaller than that of standard MDPs by a factor of σ/(1 −γ).
PROBLEM FORMULATION,0.5548172757475083,"Comparison with state-of-the-art bounds.
For the upper bound, our results (cf. Theorem 1)
significantly improves over the prior art eO

S2A
(1−γ)4ε2

of Panaganti and Kalathil (2022) by at least a"
PROBLEM FORMULATION,0.5581395348837209,"factor of
S
1−γ and even
S
(1−γ)2 when the uncertainty level 1 −γ ≲σ < 1 is large. Turning to the
lower bound side, Yang et al. (2022) developed a lower bound for RMDPs under the TV distance,
which scales as eO

SA(1−γ)"
PROBLEM FORMULATION,0.5614617940199336,"ε2
min
n
1
(1−γ)4 , 1"
PROBLEM FORMULATION,0.5647840531561462,"σ4
o
. Clearly, this is worse than ours by a factor of"
PROBLEM FORMULATION,0.5681063122923588,"σ3
(1−γ)3 ∈
 
1,
1
(1−γ)3

in the regime where 1 −γ ≲σ < 1."
PROBLEM FORMULATION,0.5714285714285714,"4.2
The case of χ2 distance: RMDP can be harder than standard MDP"
PROBLEM FORMULATION,0.574750830564784,"We now move onto the case when the uncertainty set is measured via the χ2 distance, where Theorem 3
presents the sample complexity upper bound above which DRVI is able to find an ε-optimal robust
policy in a small number of iterations.
Theorem 3 (Upper bound using χ2 distance). Fix the uncertainty set Uσ
ρ (·) = Uσ
χ2(·) using the χ2"
PROBLEM FORMULATION,0.5780730897009967,"distance in (8). Consider any uncertainty level σ ∈(0, ∞), and δ ∈(0, 1). With probability at
least 1 −δ, the output policy bπ from Algorithm 1 with at most T = c1 log (N(1 −γ)) iterations
yields V ⋆,σ(s) −V bπ,σ(s) ≤ε for any ε ∈
 
0,
1
1−γ

, as long as the total number of samples obeying"
PROBLEM FORMULATION,0.5813953488372093,NSA ≥c2SA(1+σ)
PROBLEM FORMULATION,0.584717607973422,"(1−γ)4ε2 log
  SAN"
PROBLEM FORMULATION,0.5880398671096345,"δ

. Here, c1, c2 > 0 are some large enough universal constants."
PROBLEM FORMULATION,0.5913621262458472,"In addition, in order to gauge the tightness of Theorem 3, and understand the minimal sample
complexity requirement for learning RMDPs with χ2 divergence, we further develop a minimax
lower bound as follows.
Theorem 4 (Lower bound using χ2 divergence). Consider any (S, A, γ, σ, ε) obeying γ ∈[ 3"
PROBLEM FORMULATION,0.5946843853820598,"4, 1),
σ ∈(0, ∞), and ε ≤c3"
PROBLEM FORMULATION,0.5980066445182725,"(
1
1−γ
if σ ∈
 
0, 1−γ 4

,"
PROBLEM FORMULATION,0.6013289036544851,"max
n
1
σ(1−γ), 1
o
if σ ∈
 1−γ"
PROBLEM FORMULATION,0.6046511627906976,"4 , ∞

,
(15)"
PROBLEM FORMULATION,0.6079734219269103,"for some small universal constant c3 > 0. Then we can construct two infinite-horizon RMDPs
M0, M1 defined by the uncertainty set Uσ
ρ (·) = Uσ
χ2(·), an initial state distribution ρ, and a dataset
with N independent samples for each (s, a) pair over the nominal transition kernel (for M0 and M1
respectively), such that"
PROBLEM FORMULATION,0.6112956810631229,"inf
bπ max
n
P0
 
V ⋆,σ(ρ) −V bπ,σ(ρ) > ε

, P1
 
V ⋆,σ(ρ) −V bπ,σ(ρ) > ε
o
≥1"
PROBLEM FORMULATION,0.6146179401993356,"8,
(16)"
PROBLEM FORMULATION,0.6179401993355482,provided that the total number of samples
PROBLEM FORMULATION,0.6212624584717608,NSA ≤c4
PROBLEM FORMULATION,0.6245847176079734,"(
SA
(1−γ)3ε2
if σ ∈
 
0, 1−γ"
PROBLEM FORMULATION,0.627906976744186,"4

,
σSA
min{1,(1−γ)4(1+σ)4}ε2
if σ ∈
 1−γ"
PROBLEM FORMULATION,0.6312292358803987,"4 , ∞

(17)"
PROBLEM FORMULATION,0.6345514950166113,for some universal constant c4 > 0.
PROBLEM FORMULATION,0.6378737541528239,"We are now positioned to highlight some key implications of the above theorems about the sample
complexity requirements for learning RMDPs with χ2 divergence."
PROBLEM FORMULATION,0.6411960132890365,"Nearly tight sample complexity. To achieve ε-accuracy for RMDPs with χ2 distance, Theorem 3
shows that a total number of samples on the order of eO

SA(1+σ)
(1−γ)4ε2

is sufficient for DRVI. Taking
it together with the minimax lower bound in Theorem 4 confirms that the sample complexity is
near-optimal up to a polynomial factor of the effective horizon 1/(1 −γ) over the entire range of"
PROBLEM FORMULATION,0.6445182724252492,"the uncertainty level σ. In particular, when σ ≍1, our sample complexity eO

SA
(1−γ)4ε2

is tight and"
PROBLEM FORMULATION,0.6478405315614618,"matches with the lower bound; when σ ≳
1
(1−γ)3 , our sample complexity correctly predicts the linear
dependency with σ, suggesting that more samples are needed when one plans for larger χ2-based
uncertainty sets."
PROBLEM FORMULATION,0.6511627906976745,"RMDPs can be much harder than standard MDPs with χ2 divergence. The minimax lower bound
developed in Theorem 4 exhibits a surprising non-monotonic behavior of the sample size requirement
over the entire range of the uncertainty level σ ∈(0, ∞) when the uncertainty set is measured via the"
PROBLEM FORMULATION,0.654485049833887,"χ2 divergence. When σ ≲1 −γ, the lower bound reduces to eO

SA
(1−γ)3ε2

, which matches with
that of standard MDPs, as σ = 0 corresponds to standard MDP. However, two additional regimes are
worth calling out:"
PROBLEM FORMULATION,0.6578073089700996,"1 −γ ≲σ ≲
1
(1 −γ)1/3 : eO

SA
(1 −γ)4ε2 min

σ, 1 σ3"
PROBLEM FORMULATION,0.6611295681063123,"
, and σ ≳
1
(1 −γ)3 : eO
SAσ ε2 
,"
PROBLEM FORMULATION,0.6644518272425249,"both of which are greater than that of standard MDPs, indicating learning RMDPs with χ2 divergence
can be much harder."
PROBLEM FORMULATION,0.6677740863787376,"Comparison with state-of-the-art bounds. Our upper bound significantly improves over the prior art
eO

S2A(1+σ)"
PROBLEM FORMULATION,0.6710963455149501,"(1−γ)4ε2

of Panaganti and Kalathil (2022) by a factor of S, and provides the first finite-sample
complexity that scales linearly with respect to S for discounted infinite-horizon RMDPs, which
typically exhibit more complicated statistical dependencies than the finite-horizon counterpart. On
the other hand, Yang et al. (2022) established a lower bound on the order of eO

SA
(1−γ)2σε2

when
σ ≳1 −γ, which is always smaller than the requirement of standard MDPs, and diminishes when
σ grows. Consequently, Yang et al. (2022) does not lead to the rigorous justification that RMDPs
can be harder than standard MDPs, nor the correct linear scaling of the sample size when σ grows
towards infinity."
OTHER RELATED WORKS,0.6744186046511628,"5
Other related works"
OTHER RELATED WORKS,0.6777408637873754,"Finite-sample guarantees for standard RL.
There has been a considerable amount of research
into non-asymptotic sample analysis of standard RL for a variety of settings; partial examples include,
but are not limited to, the works via probably approximately correct (PAC) bounds for the generative
model setting (Kearns and Singh, 1999; Beck and Srikant, 2012; Li et al., 2022a; Chen et al., 2020;
Azar et al., 2013; Sidford et al., 2018; Agarwal et al., 2020; Li et al., 2023, 2020; Wainwright, 2019)
and the offline setting (Rashidinejad et al., 2021; Xie et al., 2021; Yin et al., 2021; Shi et al., 2022; Li"
OTHER RELATED WORKS,0.6810631229235881,"et al., 2022b; Jin et al., 2021; Yan et al., 2022), as well as the online setting via both regret-based and
PAC-base analyses (Jin et al., 2018; Bai et al., 2019; Li et al., 2021; Zhang et al., 2020b; Dong et al.,
2019; Jin et al., 2020; Li et al., 2022a; Jafarnia-Jahromi et al., 2020; Yang et al., 2021; Woo et al.,
2023)."
OTHER RELATED WORKS,0.6843853820598007,"Robustness in RL.
To address the challenges of deployed environment uncertainty, an emerging
line of works begin to address robustness of RL algorithms with respect to the uncertainty or
perturbation over different components of MDPs — state, action, reward, and the transition kernel;
see Moos et al. (2022) for a recent review. Besides the framework of distributionally robust MDPs
(RMDPs) (Iyengar, 2005) adopted by this work, to promote robustness in RL, there exist various
other works including but not limited to Zhang et al. (2020a, 2021); Han et al. (2022); Qiaoben et al.
(2021); Sun et al. (2021); Xiong et al. (2022) investigating the robustness w.r.t. state uncertainty.
Besides, Tessler et al. (2019); Tan et al. (2020) considered the robustness w.r.t. the uncertainty of the
action, and Ding et al. (2023) tackles robustness against spurious correlations."
OTHER RELATED WORKS,0.6877076411960132,"Distributionally robust RL.
Rooted in the literature of distributionally robust optimization, which
has primarily been investigated in the context of supervised learning (Rahimian and Mehrotra,
2019; Gao, 2020; Bertsimas et al., 2018; Duchi and Namkoong, 2018; Blanchet and Murthy, 2019),
distributionally robust dynamic programming and RMDPs have attracted considerable attention
recently (Wolff et al., 2012; Kaufman and Schaefer, 2013; Ho et al., 2018; Smirnova et al., 2019; Ho
et al., 2021; Goyal and Grand-Clement, 2022; Derman and Mannor, 2020; Tamar et al., 2014). In the
context of RMDPs, both empirical and theoretical studies have been widely conducted, although most
prior theoretical analyses focus on planning with an exact knowledge of the uncertainty set (Iyengar,
2005; Xu and Mannor, 2012; Tamar et al., 2014), or are asymptotic in nature (Roy et al., 2017)."
OTHER RELATED WORKS,0.6910299003322259,"Resorting to the tools of high-dimensional statistics, various recent works begin to shift attention
to understand the finite-sample performance of provable robust RL algorithms, under diverse data
generating mechanisms and forms of the uncertainty set over the transition kernel. Besides the
infinite-horizon setting, finite-sample complexity bounds for RMDPs with the TV distance and
the χ2 divergence are also developed for the finite-horizon setting in Xu et al. (2023); Dong et al.
(2022). In addition, many other forms of uncertainty sets have been considered associated with
different divergence function including but not limited to Wasserstein distance, R-contamination, KL
divergence, Wang and Zou (2021); Yang et al. (2022); Panaganti and Kalathil (2022); Zhou et al.
(2021); Shi and Chi (2022); Xu et al. (2023); Wang et al. (2023a); Blanchet et al. (2023); Liu et al.
(2022); Wang et al. (2023c); Liang et al. (2023); Xu et al. (2023); Badrinath and Kalathil (2021);
Ramesh et al. (2023); Panaganti et al. (2022); Ma et al. (2022). Moreover, various other related
problems or issues have been explored such as the difference of various uncertainty types (Wang
et al., 2023b), the iteration complexity of the policy-based methods (Li et al., 2022c; Kumar et al.,
2023), the cases when the uncertainty level is instance-dependent small enough (Clavier et al., 2023),
and regularization-based robust RL (Yang et al., 2023; Zhang et al., 2023)."
DISCUSSIONS,0.6943521594684385,"6
Discussions"
DISCUSSIONS,0.6976744186046512,"This work studies sample complexity bounds for learning RMDPs when the uncertainty set is
measured via the TV distance and the χ2 divergence under the generative model. Our sample
complexity bounds not only strengthen the prior art in both directions of upper and lower bounds, but
also reveal new insights on how the additional consideration of distributional robustness fundamentally
changes the sample complexity of RL in a surprising manner. It turns out that RMDPs are not
necessarily harder nor easier than standard MDPs, but the answer is far more nuanced and highly
dependent on both the size and shape of the uncertainty set under consideration. These findings could
help to guide the practice of RMDPs, by raising awareness that the choice of the uncertainty set
not only represents a preference in robustness, but also influences the statistical complexity of the
problem."
DISCUSSIONS,0.7009966777408638,Acknowledgments and Disclosure of Funding
DISCUSSIONS,0.7043189368770764,"The work of L. Shi and Y. Chi is supported in part by the grants ONR N00014-19-1-2404, NSF
CCF-2106778, DMS-2134080, and CNS-2148212. L. Shi is also gratefully supported by the Leo
Finzi Memorial Fellowship, Wei Shen and Xuehong Zhang Presidential Fellowship, and Liang"
DISCUSSIONS,0.707641196013289,"Ji-Dian Graduate Fellowship at Carnegie Mellon University. The work of Y. Wei is supported in part
by the the NSF grants DMS-2147546/2015447, CAREER award DMS-2143215, CCF-2106778, and
the Google Research Scholar Award. The work of Y. Chen is supported in part by the Alfred P. Sloan
Research Fellowship, the Google Research Scholar Award, the AFOSR grant FA9550-22-1-0198, the
ONR grant N00014-22-1-2354, and the NSF grants CCF-2221009 and CCF-1907661. The authors
also acknowledge Zuxin Liu and He Wang for valuable discussions."
REFERENCES,0.7109634551495017,References
REFERENCES,0.7142857142857143,"Agarwal, A., Kakade, S., and Yang, L. F. (2020). Model-based reinforcement learning with a
generative model is minimax optimal. In Conference on Learning Theory, pages 67–83. PMLR."
REFERENCES,0.717607973421927,"Azar, M. G., Munos, R., and Kappen, H. J. (2013). Minimax PAC bounds on the sample complexity
of reinforcement learning with a generative model. Machine learning, 91(3):325–349."
REFERENCES,0.7209302325581395,"Badrinath, K. P. and Kalathil, D. (2021). Robust reinforcement learning using least squares policy
iteration with provable performance guarantees. In International Conference on Machine Learning,
pages 511–520. PMLR."
REFERENCES,0.7242524916943521,"Bai, Y., Xie, T., Jiang, N., and Wang, Y.-X. (2019). Provably efficient Q-learning with low switching
cost. arXiv preprint arXiv:1905.12849."
REFERENCES,0.7275747508305648,"Beck, C. L. and Srikant, R. (2012). Error bounds for constant step-size Q-learning. Systems & control
letters, 61(12):1203–1208."
REFERENCES,0.7308970099667774,"Bertsimas, D., Gupta, V., and Kallus, N. (2018). Data-driven robust optimization. Mathematical
Programming, 167(2):235–292."
REFERENCES,0.7342192691029901,"Blanchet, J., Lu, M., Zhang, T., and Zhong, H. (2023). Double pessimism is provably efficient
for distributionally robust offline reinforcement learning: Generic algorithm and robust partial
coverage. arXiv preprint arXiv:2305.09659."
REFERENCES,0.7375415282392026,"Blanchet, J. and Murthy, K. (2019). Quantifying distributional model risk via optimal transport.
Mathematics of Operations Research, 44(2):565–600."
REFERENCES,0.7408637873754153,"Chen, Z., Maguluri, S. T., Shakkottai, S., and Shanmugam, K. (2020). Finite-sample analysis of
stochastic approximation using smooth convex envelopes. arXiv preprint arXiv:2002.00874."
REFERENCES,0.7441860465116279,"Clavier, P., Pennec, E. L., and Geist, M. (2023). Towards minimax optimality of model-based robust
reinforcement learning. arXiv preprint arXiv:2302.05372v1."
REFERENCES,0.7475083056478405,"Derman, E. and Mannor, S. (2020). Distributional robustness and regularization in reinforcement
learning. arXiv preprint arXiv:2003.02894."
REFERENCES,0.7508305647840532,"Ding, W., Shi, L., Chi, Y., and Zhao, D. (2023). Seeing is not believing: Robust reinforcement learning
against spurious correlation. In Thirty-seventh Conference on Neural Information Processing
Systems."
REFERENCES,0.7541528239202658,"Dong, J., Li, J., Wang, B., and Zhang, J. (2022). Online policy optimization for robust MDP. arXiv
preprint arXiv:2209.13841."
REFERENCES,0.7574750830564784,"Dong, K., Wang, Y., Chen, X., and Wang, L. (2019). Q-learning with UCB exploration is sample
efficient for infinite-horizon MDP. arXiv preprint arXiv:1901.09311."
REFERENCES,0.760797342192691,"Duchi, J. and Namkoong, H. (2018). Learning models with uniform performance via distributionally
robust optimization. arXiv preprint arXiv:1810.08750."
REFERENCES,0.7641196013289037,"Fatemi, M., Killian, T. W., Subramanian, J., and Ghassemi, M. (2021). Medical dead-ends and
learning to identify high-risk states and treatments. Advances in Neural Information Processing
Systems, 34:4856–4870."
REFERENCES,0.7674418604651163,"Gao, R. (2020).
Finite-sample guarantees for wasserstein distributionally robust optimization:
Breaking the curse of dimensionality. arXiv preprint arXiv:2009.04382."
REFERENCES,0.770764119601329,"Goyal, V. and Grand-Clement, J. (2022). Robust markov decision processes: Beyond rectangularity.
Mathematics of Operations Research."
REFERENCES,0.7740863787375415,"Han, S., Su, S., He, S., Han, S., Yang, H., and Miao, F. (2022). What is the solution for state
adversarial multi-agent reinforcement learning? arXiv preprint arXiv:2212.02705."
REFERENCES,0.7774086378737541,"Ho, C. P., Petrik, M., and Wiesemann, W. (2018). Fast bellman updates for robust MDPs. In
International Conference on Machine Learning, pages 1979–1988. PMLR."
REFERENCES,0.7807308970099668,"Ho, C. P., Petrik, M., and Wiesemann, W. (2021). Partial policy iteration for l1-robust markov
decision processes. Journal of Machine Learning Research, 22(275):1–46."
REFERENCES,0.7840531561461794,"Iyengar, G. N. (2005). Robust dynamic programming. Mathematics of Operations Research,
30(2):257–280."
REFERENCES,0.7873754152823921,"Jafarnia-Jahromi, M., Wei, C.-Y., Jain, R., and Luo, H. (2020). A model-free learning algorithm for
infinite-horizon average-reward MDPs with near-optimal regret. arXiv preprint arXiv:2006.04354."
REFERENCES,0.7906976744186046,"Jin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M. I. (2018). Is Q-learning provably efficient? In
Advances in Neural Information Processing Systems, pages 4863–4873."
REFERENCES,0.7940199335548173,"Jin, C., Krishnamurthy, A., Simchowitz, M., and Yu, T. (2020).
Reward-free exploration for
reinforcement learning. In International Conference on Machine Learning, pages 4870–4879.
PMLR."
REFERENCES,0.7973421926910299,"Jin, Y., Yang, Z., and Wang, Z. (2021). Is pessimism provably efficient for offline RL? In International
Conference on Machine Learning, pages 5084–5096."
REFERENCES,0.8006644518272426,"Kaufman, D. L. and Schaefer, A. J. (2013). Robust modified policy iteration. INFORMS Journal on
Computing, 25(3):396–410."
REFERENCES,0.8039867109634552,"Kearns, M. J. and Singh, S. P. (1999). Finite-sample convergence rates for Q-learning and indirect
algorithms. In Advances in neural information processing systems, pages 996–1002."
REFERENCES,0.8073089700996677,"Klopp, O., Lounici, K., and Tsybakov, A. B. (2017). Robust matrix completion. Probability Theory
and Related Fields, 169(1-2):523–564."
REFERENCES,0.8106312292358804,"Kober, J., Bagnell, J. A., and Peters, J. (2013). Reinforcement learning in robotics: A survey. The
International Journal of Robotics Research, 32(11):1238–1274."
REFERENCES,0.813953488372093,"Kumar, N., Derman, E., Geist, M., Levy, K., and Mannor, S. (2023). Policy gradient for s-rectangular
robust markov decision processes. arXiv preprint arXiv:2301.13589."
REFERENCES,0.8172757475083057,"Lee, J., Jeon, W., Lee, B., Pineau, J., and Kim, K.-E. (2021). Optidice: Offline policy optimization
via stationary distribution correction estimation. In International Conference on Machine Learning,
pages 6120–6130. PMLR."
REFERENCES,0.8205980066445183,"Li, G., Cai, C., Chen, Y., Wei, Y., and Chi, Y. (2023). Is Q-learning minimax optimal? a tight sample
complexity analysis. Operations Research."
REFERENCES,0.8239202657807309,"Li, G., Chi, Y., Wei, Y., and Chen, Y. (2022a). Minimax-optimal multi-agent rl in markov games with
a generative model. Advances in Neural Information Processing Systems, 35:15353–15367."
REFERENCES,0.8272425249169435,"Li, G., Shi, L., Chen, Y., Chi, Y., and Wei, Y. (2022b). Settling the sample complexity of model-based
offline reinforcement learning. arXiv preprint arXiv:2204.05275."
REFERENCES,0.8305647840531561,"Li, G., Shi, L., Chen, Y., Gu, Y., and Chi, Y. (2021). Breaking the sample complexity barrier to
regret-optimal model-free reinforcement learning. Advances in Neural Information Processing
Systems, 34."
REFERENCES,0.8338870431893688,"Li, G., Wei, Y., Chi, Y., Gu, Y., and Chen, Y. (2020). Breaking the sample size barrier in model-based
reinforcement learning with a generative model. Advances in neural information processing
systems, 33:12861–12872."
REFERENCES,0.8372093023255814,"Li, Y., Zhao, T., and Lan, G. (2022c). First-order policy optimization for robust markov decision
process. arXiv preprint arXiv:2209.10579."
REFERENCES,0.840531561461794,"Liang, Z., Ma, X., Blanchet, J., Zhang, J., and Zhou, Z. (2023). Single-trajectory distributionally
robust reinforcement learning. arXiv preprint arXiv:2301.11721."
REFERENCES,0.8438538205980066,"Liu, S., Ngiam, K. Y., and Feng, M. (2019). Deep reinforcement learning for clinical decision support:
a brief survey. arXiv preprint arXiv:1907.09475."
REFERENCES,0.8471760797342193,"Liu, Z., Bai, Q., Blanchet, J., Dong, P., Xu, W., Zhou, Z., and Zhou, Z. (2022). Distributionally robust
q-learning. In International Conference on Machine Learning, pages 13623–13643. PMLR."
REFERENCES,0.8504983388704319,"Ma, X., Liang, Z., Blanchet, J., Liu, M., Xia, L., Zhang, J., Zhao, Q., and Zhou, Z. (2022). Distribu-
tionally robust offline reinforcement learning with linear function approximation. arXiv preprint
arXiv:2209.06620."
REFERENCES,0.8538205980066446,"Mahmood, A. R., Korenkevych, D., Vasan, G., Ma, W., and Bergstra, J. (2018). Benchmarking
reinforcement learning algorithms on real-world robots. In Conference on robot learning, pages
561–591. PMLR."
REFERENCES,0.8571428571428571,"Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M.
(2013). Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602."
REFERENCES,0.8604651162790697,"Moos, J., Hansel, K., Abdulsamad, H., Stark, S., Clever, D., and Peters, J. (2022). Robust reinforce-
ment learning: A review of foundations and recent advances. Machine Learning and Knowledge
Extraction, 4(1):276–315."
REFERENCES,0.8637873754152824,"Nilim, A. and El Ghaoui, L. (2005). Robust control of Markov decision processes with uncertain
transition matrices. Operations Research, 53(5):780–798."
REFERENCES,0.867109634551495,OpenAI (2023). Gpt-4 technical report.
REFERENCES,0.8704318936877077,"Pan, Y., Chen, Y., and Lin, F. (2023). Adjustable robust reinforcement learning for online 3d bin
packing. arXiv preprint arXiv:2310.04323."
REFERENCES,0.8737541528239202,"Panaganti, K. and Kalathil, D. (2022). Sample complexity of robust reinforcement learning with
a generative model. In International Conference on Artificial Intelligence and Statistics, pages
9582–9602. PMLR."
REFERENCES,0.8770764119601329,"Panaganti, K., Xu, Z., Kalathil, D., and Ghavamzadeh, M. (2022). Robust reinforcement learning
using offline data. Advances in neural information processing systems, 35:32211–32224."
REFERENCES,0.8803986710963455,"Qiaoben, Y., Zhou, X., Ying, C., and Zhu, J. (2021). Strategically-timed state-observation attacks on
deep reinforcement learning agents. In ICML 2021 Workshop on Adversarial Machine Learning."
REFERENCES,0.8837209302325582,"Rahimian, H. and Mehrotra, S. (2019). Distributionally robust optimization: A review. arXiv preprint
arXiv:1908.05659."
REFERENCES,0.8870431893687708,"Ramesh, S. S., Sessa, P. G., Hu, Y., Krause, A., and Bogunovic, I. (2023). Distributionally robust
model-based reinforcement learning with large state spaces. arXiv preprint arXiv:2309.02236."
REFERENCES,0.8903654485049833,"Rashidinejad, P., Zhu, B., Ma, C., Jiao, J., and Russell, S. (2021). Bridging offline reinforcement
learning and imitation learning: A tale of pessimism. Neural Information Processing Systems
(NeurIPS)."
REFERENCES,0.893687707641196,"Roy, A., Xu, H., and Pokutta, S. (2017). Reinforcement learning under model mismatch. Advances
in neural information processing systems, 30."
REFERENCES,0.8970099667774086,"Shi, L. and Chi, Y. (2022). Distributionally robust model-based offline reinforcement learning with
near-optimal sample complexity. arXiv preprint arXiv:2208.05767."
REFERENCES,0.9003322259136213,"Shi, L., Li, G., Wei, Y., Chen, Y., and Chi, Y. (2022). Pessimistic Q-learning for offline reinforcement
learning: Towards optimal sample complexity. In Proceedings of the 39th International Conference
on Machine Learning, volume 162, pages 19967–20025. PMLR."
REFERENCES,0.9036544850498339,"Sidford, A., Wang, M., Wu, X., Yang, L., and Ye, Y. (2018). Near-optimal time and sample
complexities for solving Markov decision processes with a generative model. In Advances in
Neural Information Processing Systems, pages 5186–5196."
REFERENCES,0.9069767441860465,"Smirnova, E., Dohmatob, E., and Mary, J. (2019). Distributionally robust reinforcement learning.
arXiv preprint arXiv:1902.08708."
REFERENCES,0.9102990033222591,"Sun, K., Liu, Y., Zhao, Y., Yao, H., Jui, S., and Kong, L. (2021). Exploring the training robust-
ness of distributional reinforcement learning against noisy state observations. arXiv preprint
arXiv:2109.08776."
REFERENCES,0.9136212624584718,"Tamar, A., Mannor, S., and Xu, H. (2014). Scaling up robust MDPs using function approximation. In
International conference on machine learning, pages 181–189. PMLR."
REFERENCES,0.9169435215946844,"Tan, K. L., Esfandiari, Y., Lee, X. Y., and Sarkar, S. (2020). Robustifying reinforcement learning
agents via action space adversarial training. In 2020 American control conference (ACC), pages
3959–3964. IEEE."
REFERENCES,0.920265780730897,"Tessler, C., Efroni, Y., and Mannor, S. (2019). Action robust reinforcement learning and applications
in continuous control. In International Conference on Machine Learning, pages 6215–6224.
PMLR."
REFERENCES,0.9235880398671097,"Wainwright, M. J. (2019). Stochastic approximation with cone-contractive operators: Sharp ℓ∞-
bounds for Q-learning. arXiv preprint arXiv:1905.06265."
REFERENCES,0.9269102990033222,"Wang, S., Si, N., Blanchet, J., and Zhou, Z. (2023a). A finite sample complexity bound for distribu-
tionally robust q-learning. arXiv preprint arXiv:2302.13203."
REFERENCES,0.9302325581395349,"Wang, S., Si, N., Blanchet, J., and Zhou, Z. (2023b). On the foundation of distributionally robust
reinforcement learning. arXiv preprint arXiv:2311.09018."
REFERENCES,0.9335548172757475,"Wang, S., Si, N., Blanchet, J., and Zhou, Z. (2023c). Sample complexity of variance-reduced
distributionally robust Q-learning. arXiv preprint arXiv:2305.18420."
REFERENCES,0.9368770764119602,"Wang, Y. and Zou, S. (2021). Online robust reinforcement learning with model uncertainty. Advances
in Neural Information Processing Systems, 34."
REFERENCES,0.9401993355481728,"Wiesemann, W., Kuhn, D., and Rustem, B. (2013). Robust markov decision processes. Mathematics
of Operations Research, 38(1):153–183."
REFERENCES,0.9435215946843853,"Wolff, E. M., Topcu, U., and Murray, R. M. (2012). Robust control of uncertain markov decision
processes with temporal logic specifications. In 2012 IEEE 51st IEEE Conference on Decision
and Control (CDC), pages 3372–3379. IEEE."
REFERENCES,0.946843853820598,"Woo, J., Joshi, G., and Chi, Y. (2023). The blessing of heterogeneity in federated Q-learning: Linear
speedup and beyond. arXiv preprint arXiv:2305.10697."
REFERENCES,0.9501661129568106,"Xie, T., Jiang, N., Wang, H., Xiong, C., and Bai, Y. (2021). Policy finetuning: Bridging sample-
efficient offline and online reinforcement learning. Advances in neural information processing
systems, 34."
REFERENCES,0.9534883720930233,"Xiong, Z., Eappen, J., Zhu, H., and Jagannathan, S. (2022). Defending observation attacks in deep
reinforcement learning via detection and denoising. arXiv preprint arXiv:2206.07188."
REFERENCES,0.9568106312292359,"Xu, H. and Mannor, S. (2012). Distributionally robust Markov decision processes. Mathematics of
Operations Research, 37(2):288–300."
REFERENCES,0.9601328903654485,"Xu, Z., Panaganti, K., and Kalathil, D. (2023). Improved sample complexity bounds for distribution-
ally robust reinforcement learning. arXiv preprint arXiv:2303.02783."
REFERENCES,0.9634551495016611,"Yan, Y., Li, G., Chen, Y., and Fan, J. (2022). The efficacy of pessimism in asynchronous Q-learning.
arXiv preprint arXiv:2203.07368."
REFERENCES,0.9667774086378738,"Yang, K., Yang, L., and Du, S. (2021). Q-learning with logarithmic regret. In International Conference
on Artificial Intelligence and Statistics, pages 1576–1584. PMLR."
REFERENCES,0.9700996677740864,"Yang, W., Wang, H., Kozuno, T., Jordan, S. M., and Zhang, Z. (2023). Avoiding model estimation in
robust markov decision processes with a generative model. arXiv preprint arXiv:2302.01248."
REFERENCES,0.973421926910299,"Yang, W., Zhang, L., and Zhang, Z. (2022). Toward theoretical understandings of robust Markov
decision processes: Sample complexity and asymptotics. The Annals of Statistics, 50(6):3223–
3248."
REFERENCES,0.9767441860465116,"Yin, M., Bai, Y., and Wang, Y.-X. (2021). Near-optimal offline reinforcement learning via double
variance reduction. arXiv preprint arXiv:2102.01748."
REFERENCES,0.9800664451827242,"Zhang, H., Chen, H., Boning, D., and Hsieh, C.-J. (2021). Robust reinforcement learning on state
observations with learned optimal adversary. arXiv preprint arXiv:2101.08452."
REFERENCES,0.9833887043189369,"Zhang, H., Chen, H., Xiao, C., Li, B., Liu, M., Boning, D., and Hsieh, C.-J. (2020a). Robust deep
reinforcement learning against adversarial perturbations on state observations. Advances in Neural
Information Processing Systems, 33:21024–21037."
REFERENCES,0.9867109634551495,"Zhang, R., Hu, Y., and Li, N. (2023). Regularized robust mdps and risk-sensitive mdps: Equivalence,
policy gradient, and sample complexity. arXiv preprint arXiv:2306.11626."
REFERENCES,0.9900332225913622,"Zhang, Z., Zhou, Y., and Ji, X. (2020b). Almost optimal model-free reinforcement learning via
reference-advantage decomposition. Advances in Neural Information Processing Systems, 33."
REFERENCES,0.9933554817275747,"Zhou, Z., Bai, Q., Zhou, Z., Qiu, L., Blanchet, J., and Glynn, P. (2021). Finite-sample regret bound
for distributionally robust offline tabular reinforcement learning. In International Conference on
Artificial Intelligence and Statistics, pages 3331–3339. PMLR."
REFERENCES,0.9966777408637874,"Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano, P.,
and Irving, G. (2019). Fine-tuning language models from human preferences. arXiv preprint
arXiv:1909.08593."
