Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0021691973969631237,"We study efÔ¨Åcient algorithms for reinforcement learning in Markov decision pro-
cesses, whose complexity is independent of the number of states. This formulation
succinctly captures large scale problems, but is also known to be computationally
hard in its general form. Previous approaches attempt to circumvent the compu-
tational hardness by assuming structure in either transition function or the value
function, or by relaxing the solution guarantee to a local optimality condition.
We consider the methodology of boosting, borrowed from supervised learning, for
converting weak learners into an effective policy. The notion of weak learning we
study is that of sampled-based approximate optimization of linear functions over
policies. Under this assumption of weak learnability, we give an efÔ¨Åcient algo-
rithm that is capable of improving the accuracy of such weak learning methods
iteratively. We prove sample complexity and running time bounds on our method,
that are polynomial in the natural parameters of the problem: approximation guar-
antee, discount factor, distribution mismatch and number of actions. In particular,
our bound does not explicitly depend on the number of states.
A technical difÔ¨Åculty in applying previous boosting results, is that the value func-
tion over policy space is not convex. We show how to use a non-convex variant of
the Frank-Wolfe method, coupled with recent advances in gradient boosting that
allow incorporating a weak learner with multiplicative approximation guarantee,
to overcome the non-convexity and attain global optimality guarantees."
INTRODUCTION,0.004338394793926247,"1
INTRODUCTION"
INTRODUCTION,0.006507592190889371,"The Ô¨Åeld of reinforcement learning, formally modelled as learning in Markov decision processes
(MDP), models the mechanism of learning from rewards, as opposed to examples. Although the
case of tabular MDPs is well understood, the main difÔ¨Åculty in applying RL to practice is the size
of the state space."
INTRODUCTION,0.008676789587852495,"Various techniques have been suggested and applied to cope with very large MDPs. The most
common of which is function approximation of either the value or the transition function of the
underlying MDP, many times using deep neural networks. Training deep neural networks in the
supervised learning model is known to be computationally hard. Therefore reinforcement learning
with neural function approximation is also computationally hard in general, and for this reason lacks
provable guarantees."
INTRODUCTION,0.010845986984815618,"This challenge of Ô¨Ånding efÔ¨Åcient and provable algorithms for MDPs with large state space is the
focus of our study. Previous approaches can be categorized in terms of the structural assumptions
made on the MDP to circumvent the computational hardness. Some studies focus on structured
dynamics, whereas others on structured value function or policy classes w.r.t. to the dynamics."
INTRODUCTION,0.013015184381778741,"In this paper we study another methodology to derive provable algorithms for reinforcement learn-
ing: ensemble methods for aggregating weak or approximate algorithms into substantially more
accurate solutions. Our method can be thought of as extending the methodology of boosting from
supervised learning (Schapire & Freund, 2012) to reinforcement learning. Interestingly, however,
our resulting aggregation of weak learners is not linear."
INTRODUCTION,0.015184381778741865,"In order to circumvent the computational hardness of solving general MDPs with function approx-
imation, we assumes access to a weak learner: an efÔ¨Åcient sample-based procedure that is capable"
INTRODUCTION,0.01735357917570499,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.019522776572668113,"of generating an approximate solution to any linear optimization objective over the space of poli-
cies. We describe an algorithm that iteratively calls this procedure on carefully constructed new
objectives, and aggregates the solution into a single policy. We prove that after sufÔ¨Åciently many
iterations, our resulting policy is provably near-optimal."
CHALLENGES AND TECHNIQUES,0.021691973969631236,"1.1
CHALLENGES AND TECHNIQUES"
CHALLENGES AND TECHNIQUES,0.02386117136659436,"Reinforcement learning is quite different from supervised learning and several difÔ¨Åculties have to be
circumvented for boosting to work. Amongst the challenges that the reinforcement learning setting
presents, consider the following,"
CHALLENGES AND TECHNIQUES,0.026030368763557483,"(a) The value function is not a convex or concave function of the policy. This is true even in
the tabular case, and even more so if we use a parameterized policy class."
CHALLENGES AND TECHNIQUES,0.028199566160520606,"(b) The transition matrix is unknown, or prohibitively large to manipulate for large state spaces.
This means that even evaluation of a policy cannot be exact, and can only be computed
approximately."
CHALLENGES AND TECHNIQUES,0.03036876355748373,"(c) It is unrealistic to expect a weak learner that attains near-optimal value for a given linear
objective over the policy class. At most one can hope for a multiplicative and/or additive
approximation of the overall value."
CHALLENGES AND TECHNIQUES,0.03253796095444685,"Our approach overcomes these challenges by applied several new as well as recently developed
techniques. To overcome the nonconvexity of the value function, we use a novel variant of the
Frank-Wolfe optimization algorithm that simultaneously delivers on two guarantees. First, it Ô¨Ånds a
Ô¨Årst order stationary point with near-optimal rate. Secondly, if the objective happens to admit a cer-
tain gradient domination property, an important generalization of convexity, it also guarantees near
optimal value. The application of the nonconvex Frank-Wolfe method is justiÔ¨Åed due to previous
recent investigation of the policy gradient algorithm (Agarwal et al., 2019; 2020a), which identiÔ¨Åed
conditions under which the value function is gradient dominated."
CHALLENGES AND TECHNIQUES,0.03470715835140998,"The second information-theoretic challenge of the unknown transition function is overcome by care-
ful algorithmic design: our boosting algorithm requires only samples of the transitions and rewards.
These are obtained by rollouts on the MDP."
CHALLENGES AND TECHNIQUES,0.0368763557483731,"The third challenge is perhaps the most difÔ¨Åcult to overcome. Thus far, the use of the Frank-Wolfe
method in reinforcement learning did not include a multiplicative approximation, which is critical
for our application. Luckily, recent work in the area of online convex optimization (Hazan & Singh,
2021) studies boosting with a multiplicative weak learner. We make critical use of this new technique
which includes a non-linear aggregation (using a 2-layer neural network) of the weak learners. This
aspect is perhaps of general interest to boosting algorithm design, which is mostly based on linear
aggregation."
OUR CONTRIBUTIONS,0.039045553145336226,"1.2
OUR CONTRIBUTIONS"
OUR CONTRIBUTIONS,0.04121475054229935,"Our main contribution is a novel efÔ¨Åcient boosting algorithm for reinforcement learning. The input
to this algorithm is a weak learning method capable of approximately optimizing a linear function
over a certain policy class."
OUR CONTRIBUTIONS,0.04338394793926247,"The output of the algorithm is a policy which does not belong to the original class considered. It is
rather a non-linear aggregation of policies from the original class, according to a two-layer neural
network. This is a result of the two-tier structure of our algorithm: an outer loop of non-convex
Frank-Wolfe method, and an inner loop of online convex optimization boosting. The Ô¨Ånal policy
comes with provable guarantees against the class of all possible policies."
OUR CONTRIBUTIONS,0.0455531453362256,"Our algorithm and guarantees come in four Ô¨Çavors, depending on the mode of accessing the MDP
(two options), and the boosting methodology for the inner online convex optimization problem (two
options)."
OUR CONTRIBUTIONS,0.04772234273318872,"It is important to point out that we study the question from an optimization perspective, and hence,
assume the availability of an efÔ¨Åcient exploration scheme ‚Äì either via access to a reset distribution
that has some overlap with the state distribution of the optimal policy, or constraining the policy"
OUR CONTRIBUTIONS,0.049891540130151846,Under review as a conference paper at ICLR 2022
OUR CONTRIBUTIONS,0.052060737527114966,"Supervised weak learner
Online weak learner"
OUR CONTRIBUTIONS,0.05422993492407809,"Episodic model
C6
‚àû(Œ†)/Œ±4Œµ5
C4
‚àû(Œ†)/Œ±2Œµ3
C‚àû= maxœÄ‚ààŒ†
 dœÄ‚àó"
OUR CONTRIBUTIONS,0.05639913232104121,"dœÄ

‚àû"
OUR CONTRIBUTIONS,0.05856832971800434,"Rollouts w. ŒΩ-resets
D6
‚àû/Œ±4Œµ6
D4
‚àû/Œ±2Œµ4
D‚àû=
 dœÄ‚àó"
OUR CONTRIBUTIONS,0.06073752711496746,"ŒΩ

‚àû"
OUR CONTRIBUTIONS,0.06290672451193059,"Table 1: Sample complexity of the proposed algorithms for different Œ±-weak learning models (su-
pervised & online) and modes of accessing the MDP (rollouts & rollouts with reset distribution ŒΩ),
suppressing polynomial factors in |A|, 1/(1 ‚àíŒ≥). See Theorem 11 for details."
OUR CONTRIBUTIONS,0.0650759219088937,"class to policies that explore sufÔ¨Åciently. Such considerations also arise when reducing reinforce-
ment learning to a sequence of supervised learning problems, e.g. Conservative Policy Iteration
(Kakade & Langford, 2002) assumes the former. One contribution we make here is to quantita-
tively differentiate between these two modes of exploration in terms of the rates of convergence they
enable for the boosting setting."
RELATED WORK,0.06724511930585683,"1.3
RELATED WORK"
RELATED WORK,0.06941431670281996,"To cope with prohibitively large MDPs, the method of choice to approximate the policy and transi-
tion space are deep neural networks, dubbed ‚Äúdeep reinforcement learning"". Deep RL gave rise to
beyond human performance in games such as Go, protein folding, as well as near-human level au-
tonomous driving. In terms of provable methods for deep RL, there are two main lines of work. The
Ô¨Årst is a robust analysis of the policy gradient algorithm (Agarwal et al., 2019; 2020a). Importantly,
the gradient domination property of the value function established in this work is needed in order to
achieve global convergence guarantees of our boosting method."
RELATED WORK,0.07158351409978309,"The other line of work for provable approaches is policy iteration, which uses a restricted policy
class, making incremental updates, such as Conservative Policy Iteration (CPI) (Kakade & Langford,
2002; Scherrer & Geist, 2014), and Policy Search by Dynamic Programming (PSDP)(Bagnell et al.,
2003)."
RELATED WORK,0.0737527114967462,"Our boosting approach for provable deep RL builds on the vast literature of boosting for supervised
learning (Schapire & Freund, 2012), and recently online learning (Leistner et al., 2009; Chen et al.,
2012; 2014; Beygelzimer et al., 2015; Jung et al., 2017; Jung & Tewari, 2018). One of the crucial
techniques important for our application is the extension of boosting to the online convex optimiza-
tion setting, with bandit information (Brukhim & Hazan, 2021), and critically with a multiplicative
weak learner (Hazan & Singh, 2021). This latter technique implies a non-linear aggregation of the
weak learners. Non-linear boosting was only recently investigated in the context of classiÔ¨Åcation
(Alon et al., 2020), where it was shown to potentially enable signiÔ¨Åcantly more efÔ¨Åcient boosting."
RELATED WORK,0.07592190889370933,"Perhaps the closest work to ours is boosting in the context of control of dynamical systems (Agar-
wal et al., 2020b). However, this work critically requires knowledge of the underlying dynamics
(transitions), which we do not, and cannot cope with a multiplicative approximate weak learner."
RELATED WORK,0.07809110629067245,"The Frank-Wolfe algorithm is extensively used in machine learning, see e.g. (Jaggi, 2013), refer-
ences therein, and recent progress in stochastic Frank-Wolfe methods (Hassani et al., 2017; Mokhtari
et al., 2018; Chen et al., 2018; Xie et al., 2019). Recent literature has applied a variant of this algo-
rithm to reinforcement learning in the context of state space exploration (Hazan et al., 2019)."
PRELIMINARIES,0.08026030368763558,"2
PRELIMINARIES"
PRELIMINARIES,0.0824295010845987,"Optimization.
We say that a differentiable function f : K 7‚ÜíR over some domain K is L-smooth
with respect to some norm ‚à•¬∑ ‚à•‚àóif for every x, y ‚ààK we have"
PRELIMINARIES,0.08459869848156182,"f(y) ‚àíf(x) ‚àí‚àáf(x)‚ä§(y ‚àíx)
 ‚â§L"
PRELIMINARIES,0.08676789587852494,"2 ‚à•x ‚àíy‚à•2
‚àó."
PRELIMINARIES,0.08893709327548807,Under review as a conference paper at ICLR 2022
PRELIMINARIES,0.0911062906724512,"For constrained optimization (such as over ‚àÜA), the projection Œì : R|A| ‚Üí‚àÜA of a point x to onto
a domain ‚àÜA is
Œì[x] = arg min
y‚àà‚àÜA
‚à•x ‚àíy‚à•."
PRELIMINARIES,0.09327548806941431,"An important generalization of convex function we use henceforth is that of gradient domination,
DeÔ¨Ånition 1 (Gradient Domination). A function f : K ‚ÜíR is said to be (Œ∫, œÑ, K1, K2)-locally
gradient dominated (around K1 by K2) if for all x ‚ààK1, it holds that"
PRELIMINARIES,0.09544468546637744,"max
y‚ààK f(y) ‚àíf(x) ‚â§Œ∫ √ó max
y‚ààK2"
PRELIMINARIES,0.09761388286334056,"
‚àáf(x)‚ä§(y ‚àíx)
	
+ œÑ."
PRELIMINARIES,0.09978308026030369,"Markov decision process.
An inÔ¨Ånite-horizon discounted Markov Decision Process (MDP) M =
(S, A, P, r, Œ≥, d0) is speciÔ¨Åed by: a state space S, an action space A, a transition model P where
P(s‚Ä≤|s, a) denotes the probability of immediately transitioning to state s‚Ä≤ upon taking action a at
state s, a reward function r : S √ó A ‚Üí[0, 1] where r(s, a) is the immediate reward associated with
taking action a at state s, a discount factor Œ≥ ‚àà[0, 1); a starting state distribution d0 over S. For any
inÔ¨Ånite-length state-action sequence (hereafter, called a trajectory), we assign the following value"
PRELIMINARIES,0.1019522776572668,"V (œÑ = (s0, a0, s1, a1, . . . )) = ‚àû
X"
PRELIMINARIES,0.10412147505422993,"t=0
Œ≥tr(st, at)."
PRELIMINARIES,0.10629067245119306,"The agent interacts with the MDP through the choice of stochastic policy œÄ : S ‚Üí‚àÜA it exe-
cutes, where ‚àÜA denotes the probability simplex over A. The execution of such a policy induces a
distribution over trajectories œÑ = (s0, a0, . . . ) as"
PRELIMINARIES,0.10845986984815618,"P(œÑ|œÄ) = d0(s0) ‚àû
Y"
PRELIMINARIES,0.11062906724511931,"t=0
(P(st+1|st, at)œÄ(at|st)).
(1)"
PRELIMINARIES,0.11279826464208242,"Using this description we can associate a state V œÄ(s) and state-action QœÄ(s, a) value function with
any policy œÄ. For an arbitrary distrbution d over S, deÔ¨Åne:"
PRELIMINARIES,0.11496746203904555,"QœÄ(s) = E "" ‚àû
X"
PRELIMINARIES,0.11713665943600868,"t=0
Œ≥tr(st, at)
 œÄ, s0 = s, a0 = a # ,"
PRELIMINARIES,0.1193058568329718,"V œÄ(s) = Ea‚àºœÄ(¬∑|s) [QœÄ(s, a)|œÄ, s] ,
V œÄ
d = Es0‚àºd [V œÄ(s)|œÄ] .
Here the expectation is with respect to the randomness of the trajectory induced by œÄ in M. When
convenient, we shall use V œÄ to denote V œÄ
d0, and V ‚àóto denote maxœÄ V œÄ."
PRELIMINARIES,0.12147505422993492,"Similarly, to any policy œÄ, one may ascribe a (discounted) state-visitation distribution dœÄ = dœÄ
d0."
PRELIMINARIES,0.12364425162689804,"dœÄ
d(s) = (1 ‚àíŒ≥) ‚àû
X"
PRELIMINARIES,0.12581344902386118,"t=0
Œ≥t X"
PRELIMINARIES,0.1279826464208243,"œÑ:st=s
P(œÑ|œÄ, s0 ‚àºd)"
PRELIMINARIES,0.1301518438177874,"Modes of Accessing the MDP.
We henceforth consider two modes of accessing the MDP, that are
standard in the reinforcement learning literature, and provide different results for each."
PRELIMINARIES,0.13232104121475055,"The Ô¨Årst natural access model is called the episodic rollout setting. This mode of interaction allows
us to execute a policy, stop and restart at any point, and do this multiple times."
PRELIMINARIES,0.13449023861171366,"Another interaction model we consider is called rollout with ŒΩ-restarts. This is similar to the
episodic setting, but here the agent may draw from the MDP a trajectory seeded with an initial state
distribution ŒΩ Ã∏= d0. This interaction model was considered in prior work on policy optimization
Kakade & Langford (2002); Agarwal et al. (2019). The motivation for this model is two-fold:
Ô¨Årst, ŒΩ can be used to incorporate priors (or domain knowledge) about the state coverage of the
optimal policy; second, ŒΩ provides a mechanism to incorporate exploration into policy optimization
procedures."
PRELIMINARIES,0.13665943600867678,"3
SETTING: POLICY AGGREGATION AND WEAK LEARNING"
PRELIMINARIES,0.13882863340563992,"Our boosting algorithms henceforth call upon weak learners to generate weak policies, and aggre-
gate these policies in a way that guarantees eventual convergence to optimality. In this section we
formalize both components."
PRELIMINARIES,0.14099783080260303,"Under review as a conference paper at ICLR 2022 ¬ØœÄ Œì[Œª2] Œª1 œÄ1 w1 œÄ2
œÄ3 w‚Ä≤
1 Œì[Œª2] Œª2"
PRELIMINARIES,0.14316702819956617,"œÄ4
œÄ5
œÄ6 w6 w‚Ä≤
2"
PRELIMINARIES,0.14533622559652928,"A Shrub
Œªt ‚ààŒõ(Œ†, N)"
PRELIMINARIES,0.1475054229934924,"A Policy Tree
¬ØœÄ ‚ààŒ†(Œ†, N, T)"
PRELIMINARIES,0.14967462039045554,"Figure 1: The Ô¨Ågure illustrates a Policy Tree hierarchy (see DeÔ¨Ånition 5), obtained by setting N = 3
on the inner loop, and T = 2 on the outer loop, to overall get all base policies œÄ1, ..., œÄ6 ‚ààŒ†W on
the lower level. The middle level holds T = 2 Policy Shurbs (see DeÔ¨Ånition 4), where each Shrub
Œªt ‚ààŒõ(Œ†, N) is an aggregation of base policies. The top level is an weighted aggregation of the
projected shrubs Œì[Œªt] , which forms the overall Policy Tree ¬ØœÄ ‚ààŒ†(Œ†, N, T)."
POLICY AGGREGATION,0.15184381778741865,"3.1
POLICY AGGREGATION"
POLICY AGGREGATION,0.1540130151843818,"For a base class of policies Œ†W , our algorithm incrementally builds a more expressive policy class
by aggregating base policies via both linear combinations and non-linear transformations. In effect,
the algorithm produces a Ô¨Ånite-width depth-2 circuit over some subset of the base policy class. We
start with the simpler linear aggregation."
POLICY AGGREGATION,0.1561822125813449,"DeÔ¨Ånition 2 (Function Aggregation). Given some N0 ‚ààZ+, w ‚ààRN0, (f1, . . . fN0) ‚àà(S ‚Üí
R|A|)‚äóN0, we deÔ¨Åne f = PN0
n=1 wnfn to be the unique function f : S ‚ÜíRA for which simultane-
ously for all s ‚ààS, it holds"
POLICY AGGREGATION,0.15835140997830802,"f(s) = N0
X"
POLICY AGGREGATION,0.16052060737527116,"n=1
wnf(s)."
POLICY AGGREGATION,0.16268980477223427,"Next, the projection operation below may be viewed as a non-linear activation, such as ReLU, in
deep learning terms. Note that the projection of any function from S to R|A| produces a policy, i.e.
a mapping from states to distributions over actions."
POLICY AGGREGATION,0.1648590021691974,"DeÔ¨Ånition 3 (Policy Projection). Given a function f : S ‚ÜíR|A|, deÔ¨Åne a projected policy œÄ = Œì[f]
to be a policy such that simultaneously for all s ‚ààS, it holds that œÄ(¬∑|s) = Œì [f(s)] ."
POLICY AGGREGATION,0.16702819956616052,"The next deÔ¨Ånition deÔ¨Ånes the class of functions represented by circuits of depth 1 over a base
policy class. Note that these function do not necessarily represent policies since they take an afÔ¨Åne
(vs. convex) combination of policies."
POLICY AGGREGATION,0.16919739696312364,"DeÔ¨Ånition 4 (Shrub). For an arbitrary base policy class Œ† ‚äÜS ‚Üí‚àÜA, deÔ¨Åne Œõ(Œ†, N) to be a set
such that Œª ‚ààŒõ(Œ†, N) if and only if there exists N0 ‚â§N, w ‚ààRN0, (œÄ1, . . . œÄN0) ‚ààŒ†‚äóN0 such
that Œª = PN0
n=1 wnœÄn."
POLICY AGGREGATION,0.17136659436008678,The Ô¨Ånal deÔ¨Ånition describes the set of possible outputs of the boosting procedure.
POLICY AGGREGATION,0.1735357917570499,"DeÔ¨Ånition 5 (Policy Tree). For an arbitrary base policy class Œ† ‚äÜS ‚Üí‚àÜA, deÔ¨Åne Œ†(Œ†, N, T) to be
a policy class such that œÄ ‚ààŒ†(Œ†, N, T) if and only if there exists T0 ‚â§T, w ‚àà‚àÜT0, (Œª1, . . . ŒªT0) ‚àà
Œõ(Œ†, N)‚äóT0 such that œÄ = PT0
t=1 wtŒì[Œªt]."
POLICY AGGREGATION,0.175704989154013,"It is important that the policy that the boosting algorithm outputs can be evaluated efÔ¨Åciently. In the
appendix we show it is indeed the case (see Lemma 15)."
POLICY AGGREGATION,0.17787418655097614,Under review as a conference paper at ICLR 2022
MODELS OF WEAK LEARNING,0.18004338394793926,"3.2
MODELS OF WEAK LEARNING"
MODELS OF WEAK LEARNING,0.1822125813449024,"We consider two types of weak learners, and give different end results based on the different as-
sumptions: weak supervised and weak online learners. In the discussion below, let œÄr be a uniformly
random policy, i.e. ‚àÄ(s, a) ‚ààS √ó A, œÄr(a|s) = 1/|A|."
MODELS OF WEAK LEARNING,0.1843817787418655,"Supervised Learning.
The natural way to deÔ¨Åne weak learning is an algorithm whose perfor-
mance is always slight better than that of random policy, one that chooses an action uniformly at
random at any given state. However, in general no learner can outperform a random learner over
all label distributions (this is called the ‚Äúno free lunch"" theorem). This motivates the literature on
agnostic boosting (Kanade & Kalai, 2009; Brukhim et al., 2020; Hazan & Singh, 2021) that deÔ¨Ånes
a weak learner as one that can approximate the best policy in a given policy class.
DeÔ¨Ånition 6 (Weak Supervised Learner). Let Œ± ‚àà(0, 1). Consider a class L of linear loss functions
‚Ñì: RA ‚ÜíR, and D a family of distributions that are supported over S √ó L, policy classes Œ†W , Œ†.
A weak supervised learning algorithm, for every Œµ, Œ¥ > 0, given m(Œµ, Œ¥) samples Dm from any
distribution D ‚ààD outputs a policy W(Dm) ‚ààŒ†W such that with probability 1 ‚àíŒ¥,"
MODELS OF WEAK LEARNING,0.18655097613882862,"E(s,‚Ñì)‚àºD

‚Ñì(W(Dm))

‚â•Œ± max
œÄ‚àó‚ààŒ† E(s,‚Ñì)‚àºD

‚Ñì(œÄ‚àó(s))

+ (1 ‚àíŒ±)E(s,‚Ñì)‚àºD

‚Ñì(œÄr(s))

‚àíŒµ."
MODELS OF WEAK LEARNING,0.18872017353579176,"Note that the weak learner outputs a policy in Œ†W which is approximately competitive against
the class Œ†. As an additional relaxation, instead of requiring that the weak learning guarantee
holds for all distributions, in our setup, it will be sufÔ¨Åcient that the weak learning assumption holds
over natural distributions. We deÔ¨Åne these below. Hereafter, we refer to Œ†(Œ†W , N, T) as Œ† for
N, T = O(poly(|A|, (1 ‚àíŒ≥)‚àí1, Œµ‚àí1, Œ±‚àí1, log Œ¥‚àí1)) speciÔ¨Åed later.
Assumption 1 (Weak Supervised Learning). The booster has access to a weak supervised learning
oracle (DeÔ¨Ånition 6) over the policy class Œ†, for some Œ± ‚àà(0, 1). Furthermore, the weak learning
condition holds only for a class of natural distributions D ‚Äì D ‚ààD if and only if there exists some
œÄ ‚ààŒ† such that"
MODELS OF WEAK LEARNING,0.19088937093275488,"DS(s) =
Z"
MODELS OF WEAK LEARNING,0.19305856832971802,"‚Ñì
D(s, ‚Ñì)d¬µ(‚Ñì) = dœÄ(s)."
MODELS OF WEAK LEARNING,0.19522776572668113,"In particular, while a natural distribution may have arbitrary distribution over labels, its marginal
distribution over states must be realizable as the state distribution of some policy in Œ† over the MDP
M. Therefore, the complexity of weak learning adapts to the complexity of the MDP itself. As an
extreme example, in stochastic contextual bandits where policies do not affect the distribution of
states (say d0), it is sufÔ¨Åcient that the weak learning condition holds with respect to all couplings of
a single distribution d0."
MODELS OF WEAK LEARNING,0.19739696312364424,"Online Learning.
The second model of weak learning we consider requires a stronger assumption,
but will give us better sample and oracle complexity bounds henceforth.
DeÔ¨Ånition 7 (Weak Online Learner). Let Œ± ‚àà(0, 1). Consider a class L of linear loss functions
‚Ñì: RA ‚ÜíR. A weak online learning algorithm, for every M > 0, incrementally for each timestep
computes a policy Wm ‚ààŒ†W and then observes the state-loss pair (s, ‚Ñìt) ‚ààS √ó L such that M
X"
MODELS OF WEAK LEARNING,0.19956616052060738,"m=1
‚Ñìm(Wm(sm)) ‚â•Œ± max
œÄ‚àó‚ààŒ† M
X"
MODELS OF WEAK LEARNING,0.2017353579175705,"m=1
‚Ñìm(œÄ‚àó(sm)) + (1 ‚àíŒ±) M
X"
MODELS OF WEAK LEARNING,0.2039045553145336,"m=1
‚Ñìm(œÄr(sm)) ‚àíRW(M)."
MODELS OF WEAK LEARNING,0.20607375271149675,"Assumption 2 (Weak Online Learning). The booster has access to a weak online learning oracle
(DeÔ¨Ånition 7) over the policy class Œ†, for some Œ± ‚àà(0, 1).
Remark 8. A similar remark about natural distributions applies to the online weak learner. In par-
ticular, it is sufÔ¨Åcient the guarantee in 7 holds for arbitrary sequence of loss functions with high
probability over the sampling of the state from dœÄ for some œÄ ‚ààŒ†. Although stronger than su-
pervised weak learning, this oracle can be interpreted as a relaxation of the online weak learning
oracle considered in (Brukhim et al., 2020; Brukhim & Hazan, 2021; Hazan & Singh, 2021). A sim-
ilar model of hybrid adversarial-stochastic online learning was considered in (Rakhlin et al., 2011;
Lazaric & Munos, 2009; Beygelzimer et al., 2011). In particular, it is known (Lazaric & Munos,
2009) that unlike online learning, the capacity of a hypothesis class for this model is governed by its
VC dimension (vs. Littlestone dimension)."
MODELS OF WEAK LEARNING,0.20824295010845986,Under review as a conference paper at ICLR 2022
ALGORITHM & MAIN RESULTS,0.210412147505423,"4
ALGORITHM & MAIN RESULTS"
ALGORITHM & MAIN RESULTS,0.21258134490238612,"In this section we describe our RL boosting algorithm. Here we focus on the case where a supervised
weak learning is provided. The online weak learners variant of our result is detailed in the appendix.
We next deÔ¨Åne several deÔ¨Ånitions and algorithmic subroutines required for our method."
ALGORITHM & MAIN RESULTS,0.21475054229934923,"The Extension Operator.
The extension operator (Hazan & Singh, 2021) operate overs functions
and modiÔ¨Åes their value outside and near the boundary of the convex set ‚àÜA to aid the boosting
algorithm."
ALGORITHM & MAIN RESULTS,0.21691973969631237,"FG,Œ≤[f](x) = min
y‚àà‚àÜA"
ALGORITHM & MAIN RESULTS,0.21908893709327548,"
f(y) + G min
z‚àà‚àÜA ‚à•y ‚àíz‚à•+ 1"
ALGORITHM & MAIN RESULTS,0.22125813449023862,"2Œ≤ ‚à•x ‚àíy‚à•2
"
ALGORITHM & MAIN RESULTS,0.22342733188720174,"To state the results, we need the following deÔ¨Ånitions. The Ô¨Årst generalizes the policy completeness
notion from (Scherrer & Geist, 2014). It may be seen as the policy-equivalent analogue of inherent
bellman error (Munos & Szepesv√°ri, 2008). Intuitively, it measures the degree to which a policy in Œ†
can best approximate the bellman operator in an average sense with respect to the state distribution
induced by a policy from Œ†.
DeÔ¨Ånition 9 (Policy Completeness). For any initial state distribution ¬µ, deÔ¨Åne"
ALGORITHM & MAIN RESULTS,0.22559652928416485,"E¬µ(Œ†, Œ†) = max
œÄ‚ààŒ† min
œÄ‚àó‚ààŒ† Es‚àºdœÄ
¬µ"
ALGORITHM & MAIN RESULTS,0.227765726681128,"
max
a‚ààA QœÄ(s, a) ‚àíQœÄ(s, ¬∑)‚ä§œÄ‚àó(¬∑|s)

."
ALGORITHM & MAIN RESULTS,0.2299349240780911,"The following notion of the distribution mismatch coefÔ¨Åcient is often useful to characterize the
exploration problem faced by policy optimization algorithms.
DeÔ¨Ånition 10 (Distribution Mismatch). Let œÄ‚àó= arg maxœÄ V œÄ, and ŒΩ a Ô¨Åxed initial state distribu-
tion (see section 2). DeÔ¨Åne the following distribution mismatch coefÔ¨Åcients:1"
ALGORITHM & MAIN RESULTS,0.23210412147505424,"C‚àû(Œ†) = max
œÄ‚ààŒ† dœÄ‚àó dœÄ"
ALGORITHM & MAIN RESULTS,0.23427331887201736,"‚àû
,
D‚àû=

dœÄ‚àó ŒΩ ‚àû
."
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.23644251626898047,"4.1
RL BOOSTING VIA WEAK SUPERVISED LEARNING"
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.2386117136659436,"We give the main RL boosting algorithm, assuming supervised weak learners. We use a simple
sub-routine for choosing a step size, provided in the appendix."
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.24078091106290672,Algorithm 1 RL Boosting via Weak Supervised Learning
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.24295010845986983,"1: Input parameters T, N, M, P, ¬µ. Initialize a policy œÄ0 ‚ààŒ†W arbitrarily.
2: for t = 1 to T do
3:
Set œÅt,0 to be an arbitrary policy in Œ†W .
4:
for n = 1 to N do
5:
Execute œÄt‚àí1 for M episodes with initial state distribution ¬µ via Algorithm 2, to get"
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.24511930585683298,"Dt,n = {(si, c
Qi)m
i=1}."
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.2472885032537961,"6:
Modify Dt,n to produce a new dataset D‚Ä≤
t,n = {(si, fi)}m
i=1, such that for all i ‚àà[m]:"
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.24945770065075923,"fi = ‚àí‚àáFG,Œ≤[‚àíbQi](œÅt,n(¬∑|si))"
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.25162689804772237,".
7:
Let At,n be the policy chosen by the weak learning oracle when given data set D‚Ä≤
t,n.
8:
Update œÅt,n = (1 ‚àíŒ∑2,n)œÅt,n‚àí1 + Œ∑2,n"
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.25379609544468545,"Œ± At,n ‚àíŒ∑2,n
  1"
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.2559652928416486,"Œ± ‚àí1

œÄr.
9:
end for
10:
Declare œÄ‚Ä≤
t = Œì [œÅt,N].
11:
Choose Œ∑1,t = min{1, 2C‚àû"
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.25813449023861174,"t
} if ¬µ = d0 else Œ∑1,t = StepChooser(œÄt‚àí1, œÄ‚Ä≤
t, ¬µ, P).
12:
Update œÄt = (1 ‚àíŒ∑1,t)œÄt‚àí1 + Œ∑1,tœÄ‚Ä≤
t.
13: end for
14: Output ¬ØœÄ=œÄT if ¬µ = d0 else output œÄt‚àí1 with the smallest Œ∑t."
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.2603036876355748,"1For brevity, We use the shorthand C‚àûwhere clear from context."
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.26247288503253796,Under review as a conference paper at ICLR 2022
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.2646420824295011,"Theorem 11. Algorithm 1 samples T(MN + P) episodes of length ÀúO(
1
1‚àíŒ≥ ) with probability 1 ‚àíŒ¥."
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.2668112798264642,"In the episodic model, for T = O

C2
‚àû
(1‚àíŒ≥)3Œµ

, N =

16|A|C‚àû
(1‚àíŒ≥)2Œ±œµ
2
, M = m

(1‚àíŒ≥)2Œ±Œµ"
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.26898047722342733,"C‚àû|A| ,
Œ¥
NT

,¬µ = d0,
P = 0, with probability 1 ‚àíŒ¥,"
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.27114967462039047,"V ‚àó‚àíV œÄ ‚â§C‚àû
E(Œ†, Œ†)"
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.27331887201735355,"1 ‚àíŒ≥
+ Œµ."
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.2754880694143167,"In the ŒΩ-reset model, for T
=
8D2
‚àû
(1‚àíŒ≥)6Œµ2 , N
=

16|A|D‚àû
(1‚àíŒ≥)3Œ±œµ
2
, P
=
ÀúO( 200|A|2D2
‚àû
(1‚àíŒ≥)6Œµ2 ), M
="
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.27765726681127983,"m

(1‚àíŒ≥)3Œ±Œµ"
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.279826464208243,"8|A|D‚àû,
Œ¥
2NT

,¬µ = ŒΩ, with probability 1 ‚àíŒ¥,"
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.28199566160520606,"V ‚àó‚àíV œÄ ‚â§D‚àû
EŒΩ(Œ†, Œ†)"
RL BOOSTING VIA WEAK SUPERVISED LEARNING,0.2841648590021692,(1 ‚àíŒ≥)2 + Œµ.
TRAJECTORY SAMPLER,0.28633405639913234,"4.2
TRAJECTORY SAMPLER"
TRAJECTORY SAMPLER,0.2885032537960954,"In Algorithm 2 we describe an episodic sampling procedure, that is used in our sample-based RL
boosting algorithms described above. For a Ô¨Åxed initial state distribution ¬µ, and any given policy
œÄ, we apply the following sampling procedure: start at an initial state s0 ‚àº¬µ, and continue to
act thereafter in the MDP according to any policy œÄ, until termination. With this process, it is
straightforward to both sample from the state visitation distribution s ‚àºdœÄ, and to obtain unbiased
samples of QœÄ(s, ¬∑); see Algorithm 2 for the detailed process."
TRAJECTORY SAMPLER,0.29067245119305857,"Algorithm 2 Trajectory Sampler: s ‚àºdœÄ, unbiased estimate of QœÄ
s
1: Sample state s0 ‚àº¬µ, and action a‚Ä≤ ‚àºU(A) uniformly.
2: Sample s ‚àºdœÄ as follows: at every timestep h, with probability Œ≥, act according to œÄ; else,
accept sh as the sample and proceed to Step 3.
3: Take action a‚Ä≤ at state sh, then continue to execute œÄ, and use a termination probability of 1‚àíŒ≥.
Upon termination, set R(sh, a‚Ä≤) as the undiscounted sum of rewards from time h onwards."
TRAJECTORY SAMPLER,0.2928416485900217,"4: DeÔ¨Åne the vector d
QœÄsh, such that for all a ‚ààA, d
QœÄsh(a) = |A| ¬∑ R(sh, a‚Ä≤) ¬∑ Ia=a‚Ä≤."
TRAJECTORY SAMPLER,0.2950108459869848,"5: return (sh, d
QœÄsh)."
TRAJECTORY SAMPLER,0.29718004338394793,"5
ANALYSIS ‚Äì PROOF SKETCH"
TRAJECTORY SAMPLER,0.2993492407809111,"We sketch the high-level ideas of the proof of our main result, stated in Theorem 11, and refer the
reader to the appendix for the formal proof. Throughout the analysis, we use the notation ‚àáœÄV œÄ to
denote the gradient of the value function with respect to the |S| √ó |A|-sized representation of the
policy œÄ, namely the functional gradient of V œÄ."
TRAJECTORY SAMPLER,0.30151843817787416,"We establish an equivalence between the outlined algorithm and an abstraction of the Frank-Wolfe
algorithm (Algorithm D) from optimization theory. This variant of the Frank-Wolfe (FW) algo-
rithm operates over non-convex and gradient dominated functions to obtain the following novel
convergence guarantees. We establish the necessary gradient domination results from the policy
completeness results.
Theorem 12. Let f : K ‚ÜíR be L-smooth in some norm ‚à•¬∑ ‚à•‚àó, H-bounded, and the diameter of K
in ‚à•¬∑ ‚à•‚àóbe D. Then, for a (œµ0, K2)-linear optimization oracle, the output ¬Øx of Algorithm D satisÔ¨Åes"
TRAJECTORY SAMPLER,0.3036876355748373,"max
u‚ààK2 ‚àáf(¬Øx)‚ä§(u ‚àí¬Øx) ‚â§ r 2HLD2"
TRAJECTORY SAMPLER,0.30585683297180044,"T
+ 3œµ + œµ0."
TRAJECTORY SAMPLER,0.3080260303687636,"Furthermore, if f is (Œ∫, œÑ, K1, K2)-locally gradient-dominated and x0, . . . xT ‚ààK1, then it holds"
TRAJECTORY SAMPLER,0.31019522776572667,"max
x‚àó‚ààK f(x‚àó) ‚àíf(¬Øx) ‚â§2Œ∫2 max{LD2, H}"
TRAJECTORY SAMPLER,0.3123644251626898,"T
+ œÑ + Œ∫œµ0."
TRAJECTORY SAMPLER,0.31453362255965295,"The Frank-Wolfe algorithm utilizes an inner gradient optimization oracle as a subroutine. To imple-
ment this oracle using approximate optimizers, we utilize yet another variant of the FW method as
‚Äúinternal-boosting‚Äù for the weak learners (by employing an adapted analysis of Theorem 13)."
TRAJECTORY SAMPLER,0.31670281995661603,Under review as a conference paper at ICLR 2022
INTERNAL-BOOSTING WEAK LEARNERS,0.3188720173535792,"5.1
INTERNAL-BOOSTING WEAK LEARNERS"
INTERNAL-BOOSTING WEAK LEARNERS,0.3210412147505423,"We utilize a variant of the Frank-Wolfe method as a form ‚Äúinternal-boosting‚Äù for the weak learners,
by employing an adapted analysis of previous work that is stated below."
INTERNAL-BOOSTING WEAK LEARNERS,0.3232104121475054,"Note that bQœÄ(s, ¬∑) produced by Algorithm 2 satisÔ¨Åes ‚à•bQœÄ(s, ¬∑)‚à•=
|A|
1‚àíŒ≥ . We can now borrow the
following result on boosting for statistical learning from (Hazan & Singh, 2021), specializing the
decision set to be ‚àÜA. Let Dt be the distribution induced by the trajectory sampler in round t."
INTERNAL-BOOSTING WEAK LEARNERS,0.32537960954446854,"Theorem 13 ((Hazan & Singh, 2021)). Let Œ≤ =
q"
INTERNAL-BOOSTING WEAK LEARNERS,0.3275488069414317,"1
Œ±N , and Œ∑2,n = min{ 2"
INTERNAL-BOOSTING WEAK LEARNERS,0.3297180043383948,"n, 1}. Then, for any t, œÄ‚Ä≤
t
produced by Algorithm 1 satisÔ¨Åes with probability 1 ‚àíŒ¥ that"
INTERNAL-BOOSTING WEAK LEARNERS,0.3318872017353579,"max
œÄ‚ààŒ† E(s,Q)‚àºDt

Q‚ä§œÄ(s)

‚àíE(s,Q)‚àºDt

Q‚ä§œÄ‚Ä≤
t(s)

‚â§
2|A|
(1 ‚àíŒ≥)Œ±  2
‚àö"
INTERNAL-BOOSTING WEAK LEARNERS,0.33405639913232105,"N
+ Œµ
"
FROM WEAK LEARNING TO LINEAR OPTIMIZATION,0.3362255965292842,"5.2
FROM WEAK LEARNING TO LINEAR OPTIMIZATION"
FROM WEAK LEARNING TO LINEAR OPTIMIZATION,0.3383947939262473,"In the following Lemma, we give an important observation which allows us to re-state the guarantee
in the previous subsection in terms of linear optimization over functional gradients.
Lemma 14. Applying Algorithm 2 for any given policy œÄ, yields an unbiased estimate of the gradi-
ent, such that for any œÄ‚Ä≤,"
FROM WEAK LEARNING TO LINEAR OPTIMIZATION,0.3405639913232104,"(‚àáœÄV œÄ
¬µ )‚ä§œÄ‚Ä≤ =
1
1 ‚àíŒ≥ E(s, c
QœÄ(s,¬∑))‚àºD
h
c
QœÄ(s, ¬∑)‚ä§œÄ‚Ä≤(¬∑|s)
i
,
(2)"
FROM WEAK LEARNING TO LINEAR OPTIMIZATION,0.34273318872017355,"where œÄ‚Ä≤(¬∑|s) ‚àà‚àÜA, and D is the distribution induced on the outputs of Algorithm 2, for a given
policy œÄ and initial state distribution ¬µ."
FROM WEAK LEARNING TO LINEAR OPTIMIZATION,0.34490238611713664,"Proof. Recall ‚àáœÄV œÄ denotes the gradient with respect to the |S| √ó |A|-sized representation of the
policy œÄ ‚Äì the functional gradient. Then, using the policy gradient theorem (Williams, 1992; Sutton
et al., 2000), it is given by,
‚àÇV œÄ
¬µ
‚àÇœÄ(a|s) =
1
1 ‚àíŒ≥ dœÄ
¬µ(s)QœÄ(s, a).
(3)"
FROM WEAK LEARNING TO LINEAR OPTIMIZATION,0.3470715835140998,"The following sources of randomness are at play in the sampling algorithm (Algorithm 2): the
distribution dœÄ (which encompasses the discount-factor-based random termination, the transition
probability, and the stochasticity of œÄ), and the uniform sampling over A. For a Ô¨Åxed s, œÄ, denote by
QœÄ
s as the distribution over c
QœÄ(s, ¬∑) ‚ààRA, induced by all the aforementioned randomness sources.
To conclude the claim, observe that by construction"
FROM WEAK LEARNING TO LINEAR OPTIMIZATION,0.3492407809110629,"EQœÄ(s,¬∑)[ c
QœÄ(s, ¬∑)|œÄ, s] = QœÄ(s, ¬∑).
(4)"
CONCLUSIONS,0.351409978308026,"6
CONCLUSIONS"
CONCLUSIONS,0.35357917570498915,"Building on recent advances in boosting for online convex optimization and bandits, we have de-
scribed a boosting algorithm for reinforcement learning over large state spaces with provable guaran-
tees. We see this as a Ô¨Årst attempt at using a tried-and-tested methodology from supervised learning
in RL, and many challenges remain."
CONCLUSIONS,0.3557483731019523,"First and foremost, our notion of weak learner optimizes a linear function over policy space. A more
natural weak learner would be an RL agent with multiplicative optimality guarantee, and it would
be interesting to extend our methodology to this notion of weak learnability."
CONCLUSIONS,0.3579175704989154,"Another important aspect that is not discussed in our paper is that of state-space exploration. Poten-
tially boosting can be combined with state-space exploration techniques to give stronger guarantees
independent of distribution mismatch C‚àû, D‚àûfactors."
CONCLUSIONS,0.3600867678958785,"Finally, a feature of our method is that it produces nonlinear aggregations of weak learners as per a
two layer neural network. Are simpler aggregations with provable guarantees possible?"
CONCLUSIONS,0.36225596529284165,Under review as a conference paper at ICLR 2022
REFERENCES,0.3644251626898048,REFERENCES
REFERENCES,0.3665943600867679,"Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy gradi-
ent methods: Optimality, approximation, and distribution shift. arXiv preprint arXiv:1908.00261,
2019."
REFERENCES,0.368763557483731,"Alekh Agarwal, Mikael Henaff, Sham Kakade, and Wen Sun. Pc-pg: Policy cover directed explo-
ration for provable policy gradient learning. arXiv preprint arXiv:2007.08459, 2020a."
REFERENCES,0.37093275488069416,"Naman Agarwal, Nataly Brukhim, Elad Hazan, and Zhou Lu. Boosting for control of dynamical
systems. In International Conference on Machine Learning, pp. 96‚Äì103. PMLR, 2020b."
REFERENCES,0.37310195227765725,"Noga Alon, Alon Gonen, Elad Hazan, and Shay Moran. Boosting simple learners. arXiv preprint
arXiv:2001.11704, 2020."
REFERENCES,0.3752711496746204,"J Andrew Bagnell, Sham Kakade, Andrew Y Ng, and Jeff G Schneider. Policy search by dynamic
programming. In Advances in Neural Information Processing Systems, 2003."
REFERENCES,0.3774403470715835,"Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandit
algorithms with supervised learning guarantees. In Proceedings of the Fourteenth International
Conference on ArtiÔ¨Åcial Intelligence and Statistics, pp. 19‚Äì26. JMLR Workshop and Conference
Proceedings, 2011."
REFERENCES,0.3796095444685466,"Alina Beygelzimer, Satyen Kale, and Haipeng Luo. Optimal and adaptive algorithms for online
boosting. In International Conference on Machine Learning, pp. 2323‚Äì2331, 2015."
REFERENCES,0.38177874186550975,"Nataly Brukhim and Elad Hazan. Online boosting with bandit feedback. In Algorithmic Learning
Theory, pp. 397‚Äì420. PMLR, 2021."
REFERENCES,0.3839479392624729,"Nataly Brukhim, Xinyi Chen, Elad Hazan, and Shay Moran. Online agnostic boosting via regret
minimization. In Advances in Neural Information Processing Systems, 2020."
REFERENCES,0.38611713665943603,"Lin Chen, Christopher Harshaw, Hamed Hassani, and Amin Karbasi. Projection-free online opti-
mization with stochastic gradient: From convexity to submodularity. In International Conference
on Machine Learning, pp. 814‚Äì823, 2018."
REFERENCES,0.3882863340563991,"Shang-Tse Chen, Hsuan-Tien Lin, and Chi-Jen Lu. An online boosting algorithm with theoretical
justiÔ¨Åcations. In Proceedings of the 29th International Coference on International Conference on
Machine Learning, pp. 1873‚Äì1880, 2012."
REFERENCES,0.39045553145336226,"Shang-Tse Chen, Hsuan-Tien Lin, and Chi-Jen Lu. Boosting with online binary learners for the
multiclass bandit problem. In International Conference on Machine Learning, pp. 342‚Äì350, 2014."
REFERENCES,0.3926247288503254,"John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra. EfÔ¨Åcient projections onto
the l 1-ball for learning in high dimensions. In Proceedings of the 25th international conference
on Machine learning, pp. 272‚Äì279, 2008."
REFERENCES,0.3947939262472885,"Hamed Hassani, Mahdi Soltanolkotabi, and Amin Karbasi. Gradient methods for submodular max-
imization. In Advances in Neural Information Processing Systems, pp. 5841‚Äì5851, 2017."
REFERENCES,0.3969631236442516,"Elad Hazan. Introduction to online convex optimization. arXiv preprint arXiv:1909.05207, 2019."
REFERENCES,0.39913232104121477,"Elad Hazan and Karan Singh.
Boosting for online convex optimization.
arXiv preprint
arXiv:2102.09305, 2021."
REFERENCES,0.40130151843817785,"Elad Hazan, Sham Kakade, Karan Singh, and Abby Van Soest. Provably efÔ¨Åcient maximum entropy
exploration. In International Conference on Machine Learning, pp. 2681‚Äì2691. PMLR, 2019."
REFERENCES,0.403470715835141,"Martin Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In International
Conference on Machine Learning, pp. 427‚Äì435. PMLR, 2013."
REFERENCES,0.40563991323210413,"Young Hun Jung and Ambuj Tewari. Online boosting algorithms for multi-label ranking. In Inter-
national Conference on ArtiÔ¨Åcial Intelligence and Statistics, pp. 279‚Äì287, 2018."
REFERENCES,0.4078091106290672,Under review as a conference paper at ICLR 2022
REFERENCES,0.40997830802603036,"Young Hun Jung, Jack Goetz, and Ambuj Tewari. Online multiclass boosting. In Advances in neural
information processing systems, pp. 919‚Äì928, 2017."
REFERENCES,0.4121475054229935,"Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In
In Proc. 19th International Conference on Machine Learning. Citeseer, 2002."
REFERENCES,0.41431670281995664,"Varun Kanade and Adam Kalai. Potential-based agnostic boosting. In Advances in neural informa-
tion processing systems, pp. 880‚Äì888, 2009."
REFERENCES,0.4164859002169197,"Alessandro Lazaric and R√©mi Munos. Hybrid stochastic-adversarial on-line learning. In Conference
on Learning Theory, 2009."
REFERENCES,0.41865509761388287,"Christian Leistner, Amir Saffari, Peter M Roth, and Horst Bischof.
On robustness of on-line
boosting-a competitive study. In IEEE 12th International Conference on Computer Vision Work-
shops, ICCV Workshops, pp. 1362‚Äì1369. IEEE, 2009."
REFERENCES,0.420824295010846,"Aryan Mokhtari, Hamed Hassani, and Amin Karbasi. Stochastic conditional gradient methods:
From convex minimization to submodular maximization. arXiv preprint arXiv:1804.09554, 2018."
REFERENCES,0.4229934924078091,"R√©mi Munos and Csaba Szepesv√°ri. Finite-time bounds for Ô¨Åtted value iteration. Journal of Machine
Learning Research, 9(5), 2008."
REFERENCES,0.42516268980477223,"Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Online learning: Stochastic and con-
strained adversaries. arXiv preprint arXiv:1104.5070, 2011."
REFERENCES,0.42733188720173537,"Robert E Schapire and Yoav Freund. Boosting: Foundations and Algorithms. MIT Press, 2012."
REFERENCES,0.42950108459869846,"Bruno Scherrer and Matthieu Geist. Local policy search in a convex space and conservative pol-
icy iteration as boosted policy search. In Joint European Conference on Machine Learning and
Knowledge Discovery in Databases, pp. 35‚Äì50. Springer, 2014."
REFERENCES,0.4316702819956616,"Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour.
Policy gradi-
ent methods for reinforcement learning with function approximation.
In S. Solla, T. Leen,
and K. M√ºller (eds.), Advances in Neural Information Processing Systems, volume 12.
MIT Press, 2000.
URL https://proceedings.neurips.cc/paper/1999/file/
464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf."
REFERENCES,0.43383947939262474,"Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229‚Äì256, 1992."
REFERENCES,0.4360086767895879,"Jiahao Xie, Zebang Shen, Chao Zhang, Hui Qian, and Boyu Wang. Stochastic recursive gradient-
based methods for projection-free online learning. arXiv preprint arXiv:1910.09396, 2019."
REFERENCES,0.43817787418655096,Under review as a conference paper at ICLR 2022
REFERENCES,0.4403470715835141,"A
APPENDIX"
REFERENCES,0.44251626898047725,"It is important that the policy that the boosting algorithm outputs can be evaluated efÔ¨Åciently. To-
wards that end, we give the following claim."
REFERENCES,0.44468546637744033,"Claim 15. For any œÄ ‚ààŒ†(Œ†, N, T), œÄ(¬∑|s) for any s ‚ààS can be evaluated using TN base policy
evaluations and O(T √ó (NA + A log A)) arithmetic and logical operations."
REFERENCES,0.44685466377440347,"Proof. Since œÄ ‚ààŒ†(Œ†, N, T), it is composed of TN base policies. Producing each aggregated
function takes NA additions and multiplications; there are T of these. Each projection takes time
equivalent to sorting |A| numbers, due to a water-Ô¨Ålling algorithm (Duchi et al., 2008); these are
also T in number. The Ô¨Ånal linear transformation takes an additional TA operations."
REFERENCES,0.4490238611713666,"B
STEP-SIZE SUBROUTINE"
REFERENCES,0.4511930585683297,"Below we give an algorithm for choosing step sizes used in both of the RL boosting methods (for
online, and supervised, weak learners)."
REFERENCES,0.45336225596529284,"Algorithm 3 StepChooser(œÄt‚àí1, œÄ‚Ä≤
t, ¬µ, P)"
REFERENCES,0.455531453362256,"1: Execute œÄt‚àí1 for P episodes with initial state distribution ¬µ via Algorithm 2, to get"
REFERENCES,0.45770065075921906,"D = {(si, c
Qi)P
i=1}."
REFERENCES,0.4598698481561822,"2: For any policy œÄ, let c
GœÄ = 1"
REFERENCES,0.46203904555314534,"P
PP
p=1 c
Qi
‚ä§œÄ(¬∑|si).
3: Return"
REFERENCES,0.4642082429501085,"Œ∑1,t = clip[0,1]"
REFERENCES,0.46637744034707157,(1 ‚àíŒ≥)2 2
REFERENCES,0.4685466377440347,"d
GœÄ‚Ä≤
t ‚àí\
GœÄt‚àí1
"
REFERENCES,0.47071583514099785,"C
RL BOOSTING VIA WEAK ONLINE LEARNING"
REFERENCES,0.47288503253796094,Algorithm 4 RL Boosting via Weak Online Learning
REFERENCES,0.4750542299349241,"1: Initialize a policy œÄ0 ‚ààŒ†W arbitrarily.
2: for t = 1 to T do
3:
Initialize online weak learners W1, . . . WN.
4:
for m = 1 to M do
5:
Execute œÄt‚àí1 once with initial state distribution ¬µ via Algorithm 2, to get (st,m, bQt,m).
6:
Choose œÅt,m,0 ‚ààŒ†W arbitrarily.
7:
for n = 1 to N do
8:
Set œÅt,m,n = (1 ‚àíŒ∑2,n)œÅt,m,n‚àí1 + Œ∑2,n"
REFERENCES,0.4772234273318872,"Œ± Wn ‚àíŒ∑2,n
  1"
REFERENCES,0.4793926247288503,"Œ± ‚àí1

œÄr.
9:
end for
10:
Pass to each Wn the following loss linear ft,m,n:"
REFERENCES,0.48156182212581344,"ft,m,n = ‚àí‚àáFG,Œ≤[‚àíbQt,m](œÅt,m,n(¬∑|si))"
REFERENCES,0.4837310195227766,"11:
end for
12:
Declare œÄ‚Ä≤
t =
1
M
PM
m=1 Œì [œÅt,m,N]."
REFERENCES,0.48590021691973967,"13:
Choose Œ∑1,t = min{1, 2C‚àû(Œ†)"
REFERENCES,0.4880694143167028,"t
} if ¬µ = d0 else set Œ∑1,t = StepChooser(œÄt‚àí1, œÄ‚Ä≤
t, ¬µ, P).
14:
Update œÄt = (1 ‚àíŒ∑1,t)œÄt‚àí1 + Œ∑1,tœÄ‚Ä≤
t.
15: end for
16: Output ¬ØœÄ= œÄT if ¬µ = d0 else output œÄt‚àí1 with the smallest Œ∑t."
REFERENCES,0.49023861171366595,"Theorem 16. Algorithm 4 samples T(M +P) episodes of length
1
1‚àíŒ≥ log T (M+P )"
REFERENCES,0.4924078091106291,"Œ¥
with probability"
REFERENCES,0.4945770065075922,"1‚àíŒ¥. In the episodic model, Algorithm 4 guarantees as long as T = 16C2
‚àû(Œ†)
(1‚àíŒ≥)3Œµ , N =

16|A|C‚àû(Œ†)"
REFERENCES,0.4967462039045553,"(1‚àíŒ≥)2Œ±œµ
2
,"
REFERENCES,0.49891540130151846,Under review as a conference paper at ICLR 2022
REFERENCES,0.5010845986984815,"M = max
n
1000|A|2C2
‚àû(Œ†)
(1‚àíŒ≥)4Œµ2Œ±2
log2 TŒ¥, 8|A|C‚àû(Œ†)RW(M)"
REFERENCES,0.5032537960954447,"(1‚àíŒ≥)2Œ±Œµ
o
,¬µ = d0, we have with probability 1 ‚àíŒ¥"
REFERENCES,0.5054229934924078,"V ‚àó‚àíV œÄ ‚â§C‚àû(Œ†)E(Œ†, Œ†)"
REFERENCES,0.5075921908893709,"1 ‚àíŒ≥
+ Œµ"
REFERENCES,0.5097613882863341,"In the ŒΩ-reset model, Algorithm 1 guarantees as long as T =
100D2
‚àû
(1‚àíŒ≥)6Œµ2 , N =

20|A|D‚àû
(1‚àíŒ≥)3Œ±œµ
2
,"
REFERENCES,0.5119305856832972,"P = 250D2
‚àû|A|2"
REFERENCES,0.5140997830802603,(1‚àíŒ≥)6Œµ2 log2 T
REFERENCES,0.5162689804772235,"Œ¥ , M = max

40|A|D‚àû
(1‚àíŒ≥)3Œ±Œµ log T"
REFERENCES,0.5184381778741866,"Œ¥
2
, 10|A|D‚àûRW(M)"
REFERENCES,0.5206073752711496,(1‚àíŒ≥)3Œ±Œµ
REFERENCES,0.5227765726681128,"
,¬µ = ŒΩ, we have with"
REFERENCES,0.5249457700650759,probability 1 ‚àíŒ¥
REFERENCES,0.527114967462039,"V ‚àó‚àíV œÄ ‚â§D‚àû
EŒΩ(Œ†, Œ†)"
REFERENCES,0.5292841648590022,(1 ‚àíŒ≥)2 + Œµ
REFERENCES,0.5314533622559653,"If RW(M) =
p"
REFERENCES,0.5336225596529284,"M log |W| for some measure of weak learning complexity |W|, the algorithm"
REFERENCES,0.5357917570498916,"samples ÀúO

C4
‚àû(Œ†)|A|2 log |W|"
REFERENCES,0.5379609544468547,"(1‚àíŒ≥)7Œ±2Œµ3

episodes in the episodic model, and ÀúO

D4
‚àû|A|2 log |W|
(1‚àíŒ≥)12Œ±2Œµ4

in the ŒΩ-
reset model."
REFERENCES,0.5401301518438177,"D
NON-CONVEX FRANK-WOLFE"
REFERENCES,0.5422993492407809,"In this section, we give an abstract high-level procedural template that the previously introduced RL
boosters operate in. This is based on a variant of the Frank-Wolfe optimization technique, adapted
to non-convex and gradient dominated function classes (see DeÔ¨Ånition 1)."
REFERENCES,0.544468546637744,"The Frank-Wolfe (FW) method assumes oracle access to a black-box linear optimizer, denoted O,
and utilizes it by iteratively making oracle calls with modiÔ¨Åed objectives, in order to solve the
harder task of convex optimization. Analogously, boosting algorithms often assume oracle access
to a ‚Äùweak‚Äù learner, which are utilized by iteratively making oracle calls with modiÔ¨Åed objective,
in order to obtain a ‚Äùstrong‚Äù learner, with boosted performance. In the RL setting, the objective is
in fact non-convex, but exhibits gradient domination. By adapting Frank-Wolfe technique to this
setting, we will in subsequent section obtain guarantees for the algorithms given in Section 4."
REFERENCES,0.5466377440347071,"Setting.
Denote by O a black-box oracle to an (œµ0, K2)-approximate linear optimizer over a con-
vex set K ‚äÜRd such that for any given v ‚ààRd, we have"
REFERENCES,0.5488069414316703,"v‚ä§O(v) ‚â•max
u‚ààK2 v‚ä§u ‚àíœµ0."
REFERENCES,0.5509761388286334,Algorithm 5 Non-convex Frank-Wolfe
REFERENCES,0.5531453362255966,"1: Input: T > 0, objective f, linear optimization oracle O
2: Choose x0 arbitrarily.
3: for t = 1, . . . , T do
4:
Call zt = O(‚àát‚àí1), where ‚àát‚àí1 = ‚àáf(xt‚àí1).
5:
Choose Œ∑t = min{1, 2Œ∫"
REFERENCES,0.5553145336225597,"t } in the gradient-dominated case, else choose Œ∑t so that"
REFERENCES,0.5574837310195228,"|LD2Œ∑t ‚àí‚àá‚ä§
t‚àí1(zt ‚àíxt‚àí1)| ‚â§œµ."
REFERENCES,0.559652928416486,"6:
Set xt = (1 ‚àíŒ∑t)xt‚àí1 + Œ∑tzt.
7: end for
8: return ¬Øx = xT in the gradient-dominated case, else xt‚àí1 with the smallest Œ∑t."
REFERENCES,0.561822125813449,"Theorem 17. Let f : K ‚ÜíR be L-smooth in some norm ‚à•¬∑ ‚à•‚àó, H-bounded, and the diameter of K
in ‚à•¬∑ ‚à•‚àóbe D. Then, for a (œµ0, K2)-linear optimization oracle, the output ¬Øx of Algorithm D satisÔ¨Åes"
REFERENCES,0.5639913232104121,"max
u‚ààK2 ‚àáf(¬Øx)‚ä§(u ‚àí¬Øx) ‚â§ r 2HLD2"
REFERENCES,0.5661605206073753,"T
+ 3œµ + œµ0."
REFERENCES,0.5683297180043384,"Furthermore, if f is (Œ∫, œÑ, K1, K2)-locally gradient-dominated and x0, . . . xT ‚ààK1, then it holds"
REFERENCES,0.5704989154013015,"max
x‚àó‚ààK f(x‚àó) ‚àíf(¬Øx) ‚â§2Œ∫2 max{LD2, H}"
REFERENCES,0.5726681127982647,"T
+ œÑ + Œ∫œµ0."
REFERENCES,0.5748373101952278,Under review as a conference paper at ICLR 2022
REFERENCES,0.5770065075921909,"E
ANALYSIS FOR BOOSTING WITH SUPERVISED LEARNING (PROOF OF
THEOREM 11)"
REFERENCES,0.579175704989154,"Theorem (Formal version of Theorem 11). Algorithm 1 samples T(MN + P) episodes of length
1
1‚àíŒ≥ log T (MN+P )"
REFERENCES,0.5813449023861171,"Œ¥
with probability 1 ‚àíŒ¥. In the episodic model, Algorithm 1 guarantees as long as"
REFERENCES,0.5835140997830802,"T = 16C2
‚àû(Œ†)
(1‚àíŒ≥)3Œµ , N =

16|A|C‚àû(Œ†)"
REFERENCES,0.5856832971800434,"(1‚àíŒ≥)2Œ±œµ
2
, M = m

(1‚àíŒ≥)2Œ±Œµ
8C‚àû(Œ†)|A|,
Œ¥
NT

,¬µ = d0, we have with probability
1 ‚àíŒ¥"
REFERENCES,0.5878524945770065,"V ‚àó‚àíV œÄ ‚â§C‚àû(Œ†)E(Œ†, Œ†)"
REFERENCES,0.5900216919739696,"1 ‚àíŒ≥
+ Œµ"
REFERENCES,0.5921908893709328,"In the ŒΩ-reset model, Algorithm 1 guarantees as long as T =
8D2
‚àû
(1‚àíŒ≥)6Œµ2 , N =

16|A|D‚àû
(1‚àíŒ≥)3Œ±œµ
2
, P ="
REFERENCES,0.5943600867678959,"200|A|2D2
‚àû
(1‚àíŒ≥)6Œµ2 log 2T N"
REFERENCES,0.596529284164859,"Œ¥ , M = m

(1‚àíŒ≥)3Œ±Œµ"
REFERENCES,0.5986984815618221,"8|A|D‚àû,
Œ¥
2NT

,¬µ = ŒΩ, we have with probability 1 ‚àíŒ¥"
REFERENCES,0.6008676789587852,"V ‚àó‚àíV œÄ ‚â§D‚àû
EŒΩ(Œ†, Œ†)"
REFERENCES,0.6030368763557483,(1 ‚àíŒ≥)2 + Œµ
REFERENCES,0.6052060737527115,"If m(Œµ, Œ¥) = log |W|"
REFERENCES,0.6073752711496746,"Œµ2
log 1"
REFERENCES,0.6095444685466378,"Œ¥ for some measure of weak learning complexity |W|, the algorithm samples
ÀúO

C6
‚àû(Œ†)|A|4 log |W|"
REFERENCES,0.6117136659436009,"(1‚àíŒ≥)11Œ±4Œµ5

episodes in the episodic model, and ÀúO

D6
‚àû|A|4 log |W|
(1‚àíŒ≥)18Œ±4Œµ6

in the ŒΩ-reset model."
REFERENCES,0.613882863340564,"Proof of Theorem 11. The broad scheme here is to utilize an equivalence between Algorithm 1 and
Algorithm D on the function V œÄ (or V œÄ
ŒΩ in the ŒΩ-reset model), to which Theorem 17 applies."
REFERENCES,0.6160520607375272,"To this end, Ô¨Årstly, note V œÄ is
1
1‚àíŒ≥ -bounded. DeÔ¨Åne a norm ‚à•¬∑ ‚à•‚àû,1 : R|S|√ó|A| ‚ÜíR as ‚à•x‚à•1,‚àû=
maxs‚ààS
P"
REFERENCES,0.6182212581344902,"a‚ààA |xs,a|. Further, observe that for any policy œÄ : S ‚Üí‚àÜA, ‚à•œÄ‚à•‚àû,1 = 1. The
following lemma speciÔ¨Åes the smoothness of V œÄ in this norm."
REFERENCES,0.6203904555314533,"Lemma 18. V œÄ is
2Œ≥
(1‚àíŒ≥)3 -smooth in the ‚à•¬∑ ‚à•‚àû,1 norm."
REFERENCES,0.6225596529284165,"To be able to interpret Algorithm 1 as an instantiation of the algorithmic template Algorithm D
presents, we advance two claims: one, the step-size choices of the two algorithms conincide; two,
œÄ‚Ä≤
t (Line 3-10) serves as an approximate linear optimizers for ‚àáV œÄt‚àí1. Together, these imply that
the iterates produced by the two algorithms conincide. The Ô¨Årst of these, which provides a value of
œµ to use in the statement of Theorem 17, is established below."
REFERENCES,0.6247288503253796,"Claim 19. Upon every invocation of StepChooser, the output Œ∑1,t satisÔ¨Åes with probability 1‚àíŒ¥

2Œ∑1,t
(1 ‚àíŒ≥)3 ‚àí(‚àáV œÄt‚àí1
¬µ
)‚ä§(œÄ‚Ä≤
t ‚àíœÄt‚àí1)
 ‚â§
16|A|
(1 ‚àíŒ≥)2‚àö"
REFERENCES,0.6268980477223427,"P
log 1 Œ¥"
REFERENCES,0.6290672451193059,"Next, we move onto the linear optimization equivalence. Indeed, Claim 20 demonstrates that œÄ‚Ä≤
t
serves a linear optimizer over gradients of the function V œÄ; the suboptimality speciÔ¨Åes œµ0."
REFERENCES,0.631236442516269,"Claim 20. Let Œ≤ =
q"
REFERENCES,0.6334056399132321,"1
Œ±N , and Œ∑2,n = min{ 2"
REFERENCES,0.6355748373101953,"n, 1}. Then, for any t, œÄ‚Ä≤
t produced by Algorithm 1
satisÔ¨Åes with probability 1 ‚àíŒ¥"
REFERENCES,0.6377440347071583,"max
œÄ‚ààŒ†(‚àáV œÄt‚àí1
¬µ
)‚ä§(œÄ ‚àíœÄ‚Ä≤
t) ‚â§
2|A|
(1 ‚àíŒ≥)2Œ±  2
‚àö"
REFERENCES,0.6399132321041214,"N
+ ŒµW "
REFERENCES,0.6420824295010846,"Finally, observe that it is by construction that œÄt ‚ààŒ†. Therefore, in terms of the previous section, K
is the class of all policies, K1 = Œ†, K2 = Œ†."
REFERENCES,0.6442516268980477,"In the episodic model, we wish to invoke the second part of Theorem 17. The next lemma establishes
gradient-domination properties of V œÄ to support this."
REFERENCES,0.6464208242950108,"Lemma 21. V œÄ is

C‚àû(Œ†),
1
1‚àíŒ≥ C‚àû(Œ†)E(Œ†, Œ†), Œ†, Œ†

-gradient dominated, i.e. for any œÄ ‚ààŒ†:"
REFERENCES,0.648590021691974,"V ‚àó‚àíV œÄ ‚â§C‚àû(Œ†)

1
1 ‚àíŒ≥ E(Œ†, Œ†) + max
œÄ‚Ä≤‚ààŒ†(‚àáV œÄ)‚ä§(œÄ‚Ä≤ ‚àíœÄ)
"
REFERENCES,0.6507592190889371,Under review as a conference paper at ICLR 2022
REFERENCES,0.6529284164859002,"Deriving Œ∫, œÑ from the above lemma along with œµ0 from Claim 20 and œµ from Claim 19, as a conse-
quence of the second part of Theorem 17, we have with probability 1 ‚àíNTŒ¥"
REFERENCES,0.6550976138828634,"V ‚àó‚àíV ¬ØœÄ ‚â§C‚àû(Œ†)E(Œ†, Œ†)"
REFERENCES,0.6572668112798264,"1 ‚àíŒ≥
+ 4C2
‚àû(Œ†)
(1 ‚àíŒ≥)3T +
4|A|C‚àû(Œ†)
(1 ‚àíŒ≥)2Œ±
‚àö"
REFERENCES,0.6594360086767896,"N
+ 2|A|C‚àû(Œ†)"
REFERENCES,0.6616052060737527,(1 ‚àíŒ≥)2Œ± ŒµW .
REFERENCES,0.6637744034707158,"Similarly, in the ŒΩ-reset model, the Ô¨Årst part of Theorem 17 provides a local-optimality guarantee for
V œÄ
ŒΩ . Lemma 22 provides a bound on the function-value gap (on V œÄ) provided such local-optimality
conditions."
REFERENCES,0.665943600867679,"Lemma 22. For any œÄ ‚ààŒ†, we have"
REFERENCES,0.6681127982646421,"V ‚àó‚àíV œÄ ‚â§
1
1 ‚àíŒ≥ D‚àû"
REFERENCES,0.6702819956616052,"
1
1 ‚àíŒ≥ EŒΩ(Œ†, Œ†) + max
œÄ‚Ä≤‚ààŒ†(‚àáV œÄ
ŒΩ )‚ä§(œÄ‚Ä≤ ‚àíœÄ)
"
REFERENCES,0.6724511930585684,"Again, using the bound on maxœÄ‚Ä≤‚ààŒ†(‚àáV ¬ØœÄ
ŒΩ )‚ä§(œÄ‚Ä≤ ‚àí¬ØœÄ) Theorem 17 provides, we have that with
probability 1 ‚àí2NTŒ¥"
REFERENCES,0.6746203904555315,"V ‚àó‚àíV ¬ØœÄ ‚â§D‚àûEŒΩ(Œ†, Œ†)"
REFERENCES,0.6767895878524945,"(1 ‚àíŒ≥)2
+
2D‚àû
(1 ‚àíŒ≥)3‚àö"
REFERENCES,0.6789587852494577,"T
+ 2|A|D‚àû"
REFERENCES,0.6811279826464208,"(1 ‚àíŒ≥)3Œ±  2
‚àö"
REFERENCES,0.6832971800433839,"N
+ ŒµW"
REFERENCES,0.6854663774403471,"
+
48|A|D‚àû
(1 ‚àíŒ≥)3‚àö"
REFERENCES,0.6876355748373102,"P
log 1 Œ¥"
REFERENCES,0.6898047722342733,"F
ANALYSIS FOR BOOSTING WITH ONLINE LEARNING (PROOF OF
THEOREM 16)"
REFERENCES,0.6919739696312365,"Proof of Theorem 16. Similar to the proof of Theorem 11, we establish an equivalence between
Algorithm 1 and Algorithm D on the function V œÄ (or V œÄ
ŒΩ in the ŒΩ-reset model), to which Theorem 17
applies provided smoothness (see Lemma 18)."
REFERENCES,0.6941431670281996,"Indeed, Claim 23 demonstrates œÄ‚Ä≤
t serves a linear optimizer over gradients of the function V œÄ, and
provides a bound on œµ0. Claim 19 ensures that that the step size choices (and hence iterates) of the
two algorithms coincide. As before, observe that it is by construction that œÄt ‚ààŒ†."
REFERENCES,0.6963123644251626,"Claim 23. Let Œ≤ =
q"
REFERENCES,0.6984815618221258,"1
Œ±N , and Œ∑2,n = min{ 2"
REFERENCES,0.7006507592190889,"n, 1}. Then, for any t, œÄ‚Ä≤
t produced by Algorithm 4
satisÔ¨Åes with probability 1 ‚àíŒ¥"
REFERENCES,0.702819956616052,"max
œÄ‚ààŒ†(‚àáV œÄt‚àí1
¬µ
)‚ä§(œÄ ‚àíœÄ‚Ä≤
t) ‚â§
2|A|
(1 ‚àíŒ≥)2Œ± 2
‚àö"
REFERENCES,0.7049891540130152,"N
+ RW(M) M
+ r"
REFERENCES,0.7071583514099783,16 log Œ¥‚àí1 M !
REFERENCES,0.7093275488069414,"In the episodic model, one may combine the second part of Theorem 17, which provides a bound
on function-value gap for gradient dominated functions, which Lemma 21 guarantees, to conclude
with probability 1 ‚àíTŒ¥"
REFERENCES,0.7114967462039046,"V ‚àó‚àíV ¬ØœÄ ‚â§C‚àû(Œ†)E(Œ†, Œ†)"
REFERENCES,0.7136659436008677,"1 ‚àíŒ≥
+ 4C2
‚àû(Œ†)
(1 ‚àíŒ≥)3T +
4|A|C‚àû(Œ†)
(1 ‚àíŒ≥)2Œ±
‚àö"
REFERENCES,0.7158351409978309,"N
+ 2|A|C‚àû(Œ†)"
REFERENCES,0.7180043383947939,"(1 ‚àíŒ≥)2Œ±
RW(M)"
REFERENCES,0.720173535791757,"M
+ 8|A|C‚àû(Œ†) log Œ¥‚àí1"
REFERENCES,0.7223427331887202,"(1 ‚àíŒ≥)2Œ±
‚àö M
."
REFERENCES,0.7245119305856833,"Similarly, in the ŒΩ-reset model, Lemma 22 provides a bound on the function-value gap provided
local-optimality conditions, which the Ô¨Årst part of Theorem 17 provides for. Again, with probability
1 ‚àíTŒ¥"
REFERENCES,0.7266811279826464,"V ‚àó‚àíV ¬ØœÄ ‚â§D‚àûEŒΩ(Œ†, Œ†)"
REFERENCES,0.7288503253796096,"(1 ‚àíŒ≥)2
+
2D‚àû
(1 ‚àíŒ≥)3  1
‚àö"
REFERENCES,0.7310195227765727,"T
+ |A| Œ±  2
‚àö"
REFERENCES,0.7331887201735358,"N
+ RW(M)"
REFERENCES,0.735357917570499,"M
+ 4 log Œ¥‚àí1 ‚àö M"
REFERENCES,0.737527114967462,"
+ 24|A|
‚àö"
REFERENCES,0.7396963123644251,"P
log 1 Œ¥ "
REFERENCES,0.7418655097613883,Under review as a conference paper at ICLR 2022
REFERENCES,0.7440347071583514,"G
PROOFS OF SUPPORTING CLAIMS"
REFERENCES,0.7462039045553145,"G.1
NON-CONVEX FRANK-WOLFE METHOD (THEOREM 17)"
REFERENCES,0.7483731019522777,"Proof of Theorem 17. Non-convex case. Note that for any timestep t, it holds due to smoothness
that
f(xt) = f(xt‚àí1 + Œ∑t(zt ‚àíxt‚àí1))"
REFERENCES,0.7505422993492408,"‚â•f(xt‚àí1) + Œ∑t‚àá‚ä§
t‚àí1(zt ‚àíxt‚àí1) ‚àíŒ∑2
t
L 2 D2"
REFERENCES,0.7527114967462039,"= f(xt‚àí1) ‚àí
1
2LD2
 
LD2Œ∑t ‚àí‚àá‚ä§
t‚àí1(zt ‚àíxt‚àí1)
2 + (‚àá‚ä§
t‚àí1(zt ‚àíxt‚àí1))2"
REFERENCES,0.754880694143167,"2LD2
Using the step-size deÔ¨Ånition to bound on the middle term, and telescoping this inequality over
function-value differences across successive iterates, we have"
REFERENCES,0.7570498915401301,"min
t (‚àá‚ä§
t‚àí1(zt ‚àíxt‚àí1))2 ‚â§1 T T
X"
REFERENCES,0.7592190889370932,"t=1
(‚àá‚ä§
t‚àí1(zt ‚àíxt‚àí1))2 ‚â§2LD2H"
REFERENCES,0.7613882863340564,"T
+ œµ2"
REFERENCES,0.7635574837310195,"Let t‚Ä≤ = arg mint Œ∑t and t‚àó= arg mint(‚àá‚ä§
t‚àí1(zt ‚àíxt‚àí1))2. Then"
REFERENCES,0.7657266811279827,"‚àá‚ä§
t‚Ä≤‚àí1(zt‚Ä≤ ‚àíxt‚Ä≤‚àí1) ‚â§LD2Œ∑t‚Ä≤ + œµ ‚â§LD2Œ∑t‚àó+ œµ"
REFERENCES,0.7678958785249458,"‚â§‚àá‚ä§
t‚àó‚àí1(zt‚àó‚àíxt‚àó‚àí1) + 2œµ ‚â§ r 2LD2H"
REFERENCES,0.7700650759219089,"T
+ œµ2 + 2œµ"
REFERENCES,0.7722342733188721,"To conclude the claim for the non-convex part, observe
‚àö"
REFERENCES,0.7744034707158352,"a + b ‚â§‚àöa +
‚àö"
REFERENCES,0.7765726681127982,"b for a, b > 0, and that
since zt‚Ä≤ = O(‚àát‚Ä≤‚àí1), it follows by oracle deÔ¨Ånition that"
REFERENCES,0.7787418655097614,"max
u‚ààK2 ‚àá‚ä§
t‚Ä≤‚àí1u ‚â§‚àá‚ä§
t‚Ä≤‚àí1zt‚Ä≤ + œµ0."
REFERENCES,0.7809110629067245,"Gradient-Dominated Case.
DeÔ¨Åne x‚àó= arg maxx‚ààK f(x) and ht = f(x‚àó) ‚àíf(xt)."
REFERENCES,0.7830802603036876,"ht ‚â§ht‚àí1 ‚àíŒ∑t‚àá‚ä§
t‚àí1(zt ‚àíxt‚àí1) + Œ∑2
t
L"
REFERENCES,0.7852494577006508,"2 D2
smoothness"
REFERENCES,0.7874186550976139,"‚â§ht‚àí1 ‚àíŒ∑t max
y‚ààK2 Œ∑t‚àá‚ä§
t‚àí1(y ‚àíxt‚àí1) + Œ∑2
t
L"
REFERENCES,0.789587852494577,"2 D2 + Œ∑tœµ0
oracle"
REFERENCES,0.7917570498915402,‚â§ht‚àí1 ‚àíŒ∑t
REFERENCES,0.7939262472885033,"Œ∫ (f(x‚àó) ‚àíf(xt‚àí1)) + Œ∑2
t
L"
REFERENCES,0.7960954446854663,"2 D2 + Œ∑t

œµ0 + œÑ Œ∫"
REFERENCES,0.7982646420824295,"
gradient domination"
REFERENCES,0.8004338394793926,"=

1 ‚àíŒ∑t Œ∫"
REFERENCES,0.8026030368763557,"
ht‚àí1 + Œ∑2
t
L"
REFERENCES,0.8047722342733189,"2 D2 + Œ∑t

œµ0 + œÑ Œ∫ "
REFERENCES,0.806941431670282,The theorem now follows from the following claim.
REFERENCES,0.8091106290672451,Claim 24. Let C ‚â•1. Let gt be a H-bounded positive sequence such that
REFERENCES,0.8112798264642083,"gt ‚â§

1 ‚àíœÉt C"
REFERENCES,0.8134490238611713,"
gt‚àí1 + œÉ2
t D + œÉtE."
REFERENCES,0.8156182212581344,"Then choosing œÉt = min{1, 2C"
REFERENCES,0.8177874186550976,"t } implies gt ‚â§
2C2 max{2D,H}"
REFERENCES,0.8199566160520607,"t
+ CE."
REFERENCES,0.8221258134490239,"G.2
SMOOTHNESS OF VALUE FUNCTION (LEMMA 18)"
REFERENCES,0.824295010845987,"Proof of Lemma 18. Consider any two policies œÄ, œÄ‚Ä≤. Using the Performance Difference Lemma
(Lemma 3.2 in (Agarwal et al., 2019), e.g.) and Equation 2, we have"
REFERENCES,0.8264642082429501,|V œÄ‚Ä≤ ‚àíV œÄ ‚àí‚àáV œÄ(œÄ‚Ä≤ ‚àíœÄ)|
REFERENCES,0.8286334056399133,"=
1
1 ‚àíŒ≥"
REFERENCES,0.8308026030368764,"Es‚àºdœÄ‚Ä≤ 
QœÄ(¬∑|s)‚ä§(œÄ‚Ä≤(¬∑|s) ‚àíœÄ(¬∑|s)

‚àíEs‚àºdœÄ 
QœÄ(¬∑|s)‚ä§(œÄ‚Ä≤(¬∑|s) ‚àíœÄ(¬∑|s)
"
REFERENCES,0.8329718004338394,"‚â§
1
(1 ‚àíŒ≥)2 ‚à•dœÄ‚Ä≤ ‚àídœÄ‚à•1‚à•œÄ‚Ä≤ ‚àíœÄ‚à•‚àû,1"
REFERENCES,0.8351409978308026,Under review as a conference paper at ICLR 2022
REFERENCES,0.8373101952277657,"The last inequality uses the fact that maxs,a QœÄ(s, a)
‚â§
1
1‚àíŒ≥ .
It sufÔ¨Åces to show ‚à•dœÄ‚Ä≤ ‚àí
dœÄ‚à•1
‚â§
Œ≥
1‚àíŒ≥ ‚à•œÄ‚Ä≤ ‚àíœÄ‚à•‚àû,1.
To establish this, consider the Markov operator P œÄ(s‚Ä≤|s) =
P"
REFERENCES,0.8394793926247288,"a‚ààA P(s‚Ä≤|s, a)œÄ(a|s) induced by a policy œÄ on MDP M. For any distribution d supported on
S, we have"
REFERENCES,0.841648590021692,"‚à•(P œÄ‚Ä≤ ‚àíP œÄ)d‚à•1 =
X s‚Ä≤  X"
REFERENCES,0.8438177874186551,"s,a
P(s‚Ä≤|s, a)d(s)(œÄ‚Ä≤(a|s) ‚àíœÄ(a|s)  ‚â§
X"
REFERENCES,0.8459869848156182,"s‚Ä≤
P(s‚Ä≤|s, a)‚à•d‚à•1‚à•œÄ‚Ä≤ ‚àíœÄ‚à•‚àû,1 ‚â§‚à•œÄ‚Ä≤ ‚àíœÄ‚à•‚àû,1"
REFERENCES,0.8481561822125814,"Using sub-additivity of the l1 norm and applying the above observation t times, we have for any t"
REFERENCES,0.8503253796095445,"‚à•((P œÄ‚Ä≤)t ‚àí(P œÄ)t)d‚à•1 ‚â§t‚à•œÄ‚Ä≤ ‚àíœÄ‚à•‚àû,1."
REFERENCES,0.8524945770065075,"Finally, observe that"
REFERENCES,0.8546637744034707,"‚à•dœÄ‚Ä≤ ‚àídœÄ‚à•1 ‚â§(1 ‚àíŒ≥) ‚àû
X"
REFERENCES,0.8568329718004338,"t=0
Œ≥t‚à•((P œÄ‚Ä≤)t ‚àí(P œÄ)t)d0‚à•1"
REFERENCES,0.8590021691973969,"‚â§‚à•œÄ‚Ä≤ ‚àíœÄ‚à•‚àû,1(1 ‚àíŒ≥) ‚àû
X"
REFERENCES,0.8611713665943601,"t=0
tŒ≥t =
Œ≥
1 ‚àíŒ≥ ‚à•œÄ‚Ä≤ ‚àíœÄ‚à•‚àû,1"
REFERENCES,0.8633405639913232,"G.3
STEP-SIZE GUARANTEE (CLAIM 19)"
REFERENCES,0.8655097613882863,"Proof of Claim 19. Let D be the distribution induced by Algorithm 2 upon being given œÄt‚àí1. Due
to Lemma 14, it sufÔ¨Åces to demonstrate that for any œÄ ‚àà{œÄ‚Ä≤
t, œÄt‚àí1} the following claim holds with
probability 1 ‚àíŒ¥"
REFERENCES,0.8676789587852495,"2. The claim in turn follows from Hoeffding‚Äôs inequality, while noting \
QœÄt‚àí1(s, ¬∑)
is
|A|
1‚àíŒ≥ -bounded in the l‚àûnorm.
 c
GœÄ ‚àíE(s, \
QœÄt‚àí1(s,¬∑))‚àºD"
REFERENCES,0.8698481561822126,"h
\
QœÄt‚àí1(s, ¬∑)‚ä§œÄ(¬∑|s)
i ‚â§
8|A|
(1 ‚àíŒ≥)
‚àö"
REFERENCES,0.8720173535791758,"P
log 1 2Œ¥"
REFERENCES,0.8741865509761388,"G.4
GRADIENT DOMINATION (LEMMA 21 AND LEMMA 22)"
REFERENCES,0.8763557483731019,"Proof of Lemma 21. Invoking Lemma 4.1 from (Agarwal et al., 2019) with ¬µ = d0, we have"
REFERENCES,0.8785249457700651,"V ‚àó‚àíV œÄ ‚â§

dœÄ‚àó dœÄ"
REFERENCES,0.8806941431670282,"‚àû
max
œÄ0 (‚àáV œÄ)‚ä§(œÄ0 ‚àíœÄ)"
REFERENCES,0.8828633405639913,"‚â§C‚àû(Œ†)(max
œÄ0 (‚àáV œÄ)‚ä§œÄ0 ‚àímax
œÄ‚Ä≤‚ààŒ†(‚àáV œÄ)‚ä§œÄ‚Ä≤ + max
œÄ‚Ä≤‚ààŒ†(‚àáV œÄ)‚ä§(œÄ‚Ä≤ ‚àíœÄ))"
REFERENCES,0.8850325379609545,"Finally, with the aid of Equation 2, observe that"
REFERENCES,0.8872017353579176,"max
œÄ0 (‚àáV œÄ)‚ä§œÄ0 ‚àímax
œÄ‚Ä≤‚ààŒ†(‚àáV œÄ)‚ä§œÄ‚Ä≤ = min
œÄ‚Ä≤‚ààŒ†
1
1 ‚àíŒ≥ Es‚àºdœÄ
h
max
a
QœÄ(s, a) ‚àíQœÄ(¬∑|s)‚ä§œÄ‚Ä≤i"
REFERENCES,0.8893709327548807,"‚â§
1
1 ‚àíŒ≥ E(Œ†, Œ†)"
REFERENCES,0.8915401301518439,"Proof of Lemma 22. Invoking Lemma 4.1 from (Agarwal et al., 2019) with ¬µ = ŒΩ, we have"
REFERENCES,0.8937093275488069,"V ‚àó‚àíV œÄ ‚â§
1
1 ‚àíŒ≥ dœÄ‚àó ŒΩ"
REFERENCES,0.89587852494577,"‚àû
max
œÄ0 (‚àáV œÄ
ŒΩ )‚ä§(œÄ0 ‚àíœÄ)"
REFERENCES,0.8980477223427332,"‚â§
1
1 ‚àíŒ≥ D‚àû(max
œÄ0 (‚àáV œÄ
ŒΩ )‚ä§œÄ0 ‚àímax
œÄ‚Ä≤‚ààŒ†(‚àáV œÄ
ŒΩ )‚ä§œÄ‚Ä≤ + max
œÄ‚Ä≤‚ààŒ†(‚àáV œÄ
ŒΩ )‚ä§(œÄ‚Ä≤ ‚àíœÄ))"
REFERENCES,0.9002169197396963,Under review as a conference paper at ICLR 2022
REFERENCES,0.9023861171366594,"Again, with the aid of Equation 2, observe that"
REFERENCES,0.9045553145336226,"max
œÄ0 (‚àáV œÄ
ŒΩ )‚ä§œÄ0 ‚àímax
œÄ‚Ä≤‚ààŒ†(‚àáV œÄ
ŒΩ )‚ä§œÄ‚Ä≤ = min
œÄ‚Ä≤‚ààŒ†
1
1 ‚àíŒ≥ Es‚àºdœÄ
ŒΩ
h
max
a
QœÄ(s, a) ‚àíQœÄ(¬∑|s)‚ä§œÄ‚Ä≤i"
REFERENCES,0.9067245119305857,"‚â§
1
1 ‚àíŒ≥ EŒΩ(Œ†, Œ†)"
REFERENCES,0.9088937093275488,"G.5
SUPERVISED LINEAR OPTIMIZATION GUARANTEES (CLAIM 20)"
REFERENCES,0.911062906724512,"Proof of Claim 20. The subroutine presented in lines 3-10 (which culminate in œÄ‚Ä≤
t) is an instantiation
of Algorithm 3 from (Hazan & Singh, 2021), specializing the decision set to be ‚àÜA. To note the
equivalence, note that in (Hazan & Singh, 2021) the algorithm is stated assuming that the center-of-
mass of the decision set is at the origin (after a coordinate transform); correspondingly, the update
rule in Algorithm 1 can be written as"
REFERENCES,0.913232104121475,"(œÅt,n ‚àíœÄr) = (1 ‚àíŒ∑2,n)(œÅt,n‚àí1 ‚àíœÄr) + Œ∑2,n"
REFERENCES,0.9154013015184381,"Œ± (At,n ‚àíœÄr)."
REFERENCES,0.9175704989154013,"For any state s, œÄr(¬∑|s) =
1
A1|A| corresponds to the center-of-masss of ‚àÜA. Finally, note that
maximizing f ‚ä§x over x ‚ààK is equivalent to minimizing (‚àíf)‚ä§x over the same domain. Therefore,
we can borrow the following result on boosting for statistical learning from (Hazan & Singh, 2021)
(Theorem 13). Note that c
QœÄ(s, ¬∑) produced by Algorithm 2 satisÔ¨Åes ‚à•c
QœÄ(s, ¬∑)‚à•=
|A|
1‚àíŒ≥ . Let Dt be
the distribution induced by the trajectory sampler in round t."
REFERENCES,0.9197396963123644,"Theorem 25 ((Hazan & Singh, 2021)). Let Œ≤ =
q"
REFERENCES,0.9219088937093276,"1
Œ±N , and Œ∑2,n = min{ 2"
REFERENCES,0.9240780911062907,"n, 1}. Then, for any t, œÄ‚Ä≤
t
produced by Algorithm 1 satisÔ¨Åes with probability 1 ‚àíŒ¥ that"
REFERENCES,0.9262472885032538,"max
œÄ‚ààŒ† E(s,Q)‚àºDt

Q‚ä§œÄ(s)

‚àíE(s,Q)‚àºDt

Q‚ä§œÄ‚Ä≤
t(s)

‚â§
2|A|
(1 ‚àíŒ≥)Œ±  2
‚àö"
REFERENCES,0.928416485900217,"N
+ Œµ
"
REFERENCES,0.93058568329718,"Lemma 14 allows us to restate the guarantees in the previous subsection in terms of linear optimiza-
tion over functional gradients. The conclusion thus follows immediately by combining Lemma 14
and Theorem 25."
REFERENCES,0.9327548806941431,"G.6
ONLINE LINEAR OPTIMIZATION GUARANTEES (CLAIM 23)"
REFERENCES,0.9349240780911063,"Proof of Claim 23. In a similar vein to the proof of Claim 20, here we state the a result on boosting
for online convex optimization (OCO) from (Hazan & Singh, 2021) (Theorem 6), the counterpart of
Theorem 13 for the online weak learning case."
REFERENCES,0.9370932754880694,"Theorem 26 ((Hazan & Singh, 2021)). Let Œ≤ =
q"
REFERENCES,0.9392624728850325,"1
Œ±N , and Œ∑2,n = min{ 2"
REFERENCES,0.9414316702819957,"n, 1}. Then, for any t,
Œì[œÅt,m,N] produced by Algorithm 4 satisÔ¨Åes"
REFERENCES,0.9436008676789588,"max
œÄ‚ààŒ† M
X m=1"
REFERENCES,0.9457700650759219,"h
ÀÜQ‚ä§
t,mœÄ(st,m)
i
‚àí M
X m=1"
REFERENCES,0.9479392624728851,"h
ÀÜQ‚ä§
t,mŒì[œÅt,m,N](st,m)
i
‚â§
2|A|
(1 ‚àíŒ≥)Œ±"
REFERENCES,0.9501084598698482," 2M
‚àö"
REFERENCES,0.9522776572668112,"N
+ RW(M)
"
REFERENCES,0.9544468546637744,"Next we invoke online-to-batch conversions. Note that in Algorithm 4, (st,m, ÀÜQt,m) for any Ô¨Åxed
t is sampled i.i.d. from the same distribution. Therefore, we can apply online-to-batch results, i.e.
Theorem 9.5 in (Hazan, 2019), on Theorem 26 to get"
REFERENCES,0.9566160520607375,"max
œÄ‚ààŒ† E(s,Q)‚àºDt

Q‚ä§œÄ(s)

‚àíE(s,Q)‚àºDt

Q‚ä§œÄ‚Ä≤
t(s)

‚â§
2|A|
(1 ‚àíŒ≥)Œ± 2
‚àö"
REFERENCES,0.9587852494577006,"N
+ RW(M) M
+ r"
REFERENCES,0.9609544468546638,16 log Œ¥‚àí1 M !
REFERENCES,0.9631236442516269,We Ô¨Ånally invoke Lemma 14.
REFERENCES,0.96529284164859,Under review as a conference paper at ICLR 2022
REFERENCES,0.9674620390455532,"G.7
REMAINING PROOFS (CLAIM 24)"
REFERENCES,0.9696312364425163,"Proof of Claim 24. Let T ‚àó= arg maxt{t : t ‚â§2C}. For any t ‚â§T ‚àó, we have œÉt = 1 and
gt ‚â§H ‚â§
2C2H"
REFERENCES,0.9718004338394793,"t
. For t ‚â•T ‚àó, we proceed by induction. The base case (t = T ‚àó) is true by the"
REFERENCES,0.9739696312364425,"previous display. Now, assume gt‚àí1 ‚â§
2C2 max{2D,H}"
REFERENCES,0.9761388286334056,"t‚àí1
+ CE for some t > T ‚àó."
REFERENCES,0.9783080260303688,"gt ‚â§

1 ‚àí2 t"
REFERENCES,0.9804772234273319," 2C2 max{2D, H}"
REFERENCES,0.982646420824295,"t ‚àí1
+ CE

+ 4C2D"
REFERENCES,0.9848156182212582,"t2
+ 2CE t"
REFERENCES,0.9869848156182213,"‚â§CE + 2C2 max{2D, H}

1
t ‚àí1"
REFERENCES,0.9891540130151844,"
1 ‚àí2 t 
+ 1 t2 "
REFERENCES,0.9913232104121475,"= CE + 2C2 max{2D, H}t2 ‚àí2t + t ‚àí1"
REFERENCES,0.9934924078091106,t2(t ‚àí1)
REFERENCES,0.9956616052060737,"‚â§CE + 2C2 max{2D, H} t(t ‚àí1)"
REFERENCES,0.9978308026030369,t2(t ‚àí1)
